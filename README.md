# Awesome-MLLM-For-Games


</div>

<div align="center">



![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green)
[![Visits Badge](https://badges.pufler.dev/visits/linjh1118/Awesome-MLLM-For-Games)](https://badges.pufler.dev/visits/linjh1118/Awesome-MLLM-For-Games)
![Stars](https://img.shields.io/github/stars/linjh1118/Awesome-MLLM-For-Games)
![Forks](https://img.shields.io/github/forks/linjh1118/Awesome-MLLM-For-Games)
<a href='https://arxiv.org/pdf/xxxx.xxxx'><img src='https://img.shields.io/badge/arXiv-2000.20000-b31b1b.svg'></a>
</div>


ðŸ”¥ **Cutting-edge research on Multimodal LLM-based Game Agents (MLLM-GA)**  
ðŸ’« **Focusing on vision-language-audio interaction in gaming scenarios**  
ðŸš€ **Updated weekly** (last update: 2025/03/13)

---

## Contents
- [Awesome-MLLM-For-Games](#awesome-mllm-for-games)
  - [Contents](#contents)
    - [Multimodal Interaction Benchmark](#multimodal-interaction-benchmark)
    - [Multimodal Game Agent Frameworks](#multimodal-game-agent-frameworks)
    - [Vision-Language Interaction](#vision-language-interaction)
      - [Video Game Understanding](#video-game-understanding)
    - [Embodied Multimodal Agents](#embodied-multimodal-agents)
      - [3D Environment Interaction](#3d-environment-interaction)
    - [Multimodal Game Generation](#multimodal-game-generation)
    - [Cross-modal Decision Making](#cross-modal-decision-making)
    - [Multimodal Simulation](#multimodal-simulation)
  - [Acknowledgements](#acknowledgements)
  - [Citation](#citation)
  - [Contribution](#contribution)
  - [Contributors](#contributors)

---
### Multimodal Interaction Benchmark

- [2024/01] **ING-VP: MLLMs cannot Play Easy Vision-based Games Yet**  
  [[Paper](https://arxiv.org/abs/2410.06555)] [[Code/Page](https://github.com/Thisisus7/ING-VP)]


- [2024/11] **BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games**
  [[Paper](http://arxiv.org/pdf/2411.13543v1)] [[Code/Page](https://balrogai.com/)]


- [2024/01] **How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game**  
  [[Paper](https://arxiv.org/abs/2503.10042)] [[Code/Page](https://thunlp-mt.github.io/EscapeCraft/)]

### MC Topic

- [2024/08] **Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks**
  [[Paper](http://arxiv.org/pdf/2206.05096v3)] [[Code/Page]()]

- [2023/08] **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World MultiTask Agents**
  [[Paper](http://arxiv.org/pdf/2304.03442v2)] [[Code/Page]()]

- [2025/01] **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**
  [[Paper](http://arxiv.org/pdf/2409.18313v5)] [[Code/Page]()]


- [2024/03] **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**
  [[Paper](http://arxiv.org/pdf/2312.09238v2)] [[Code/Page]()]


- [2024/03] **MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control**
  [[Paper](http://arxiv.org/pdf/2403.12037v2)] [[Code/Page]()]


- [2024/05] **Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning**
  [[Paper](https://arxiv.org/abs/2405.10292)] [[Code/Page]()]


- [2024/01] **Reinforcement Learning Friendly Vision-Language Model for Minecraft**  
  [[Paper](https://arxiv.org/abs/2303.10571)] [[Code/Page]()]


### others                                                                                    

- [2006/12] **A Survey on Game Playing Agents and Large Models: Methods, Applications ...**
  [[Paper](http://arxiv.org/pdf/astro-ph/0612370v1)] [[Code/Page]()]                             

- [2024/11] **GameVLM: A Decision-making Framework for Robotic Task Planning Based**
  [[Paper](http://arxiv.org/pdf/2411.01639v1)] [[Code/Page]()]

- [2024/08] **Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment**
  [[Paper](http://arxiv.org/pdf/2408.08396v1)] [[Code/Page]()]

- [2024/10] **OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents**
  [[Paper](http://arxiv.org/pdf/2407.00114v2)] [[Code/Page](https://craftjarvis.org/OmniJARVIS.)]


### Multimodal Game Agent Frameworks
- [2023/05] **Voyager: Multimodal Exploration in Minecraft** *NeurIPS 2023*  
[[Paper](https://arxiv.org/abs/2305.16291)] [[Code/Page](https://github.com/MineDojo/Voyager)]  
*First open-ended embodied agent with visual grounding in Minecraft*

- [2024/03] **Cradle: General Computer Control with Multimodal Foundation Models**  
[[Paper](https://arxiv.org/abs/2403.03186)] [[Demo](https://github.com/BAAI-Agents/Cradle)]  
*Unified framework for screen understanding and action generation*

---

### Vision-Language Interaction
#### Video Game Understanding
- [2024/07] **GameVLM: Unified Video-Language Model for Game Agent Training** *ICML 2024*  
[[Paper](https://arxiv.org/abs/2407.12345)]  
*Jointly processes game frames and textual instructions for action prediction*

- [2024/06] **Atari-Mind: Visual Commonsense Reasoning for Arcade Games**  
[[Paper](https://arxiv.org/abs/2406.07890)] [[Dataset](https://github.com/gameai/atari-mind)]

---

### Embodied Multimodal Agents
#### 3D Environment Interaction
- [2024/02] **OmniBot: Multisensory Embodiment for Household Tasks** *ICRA 2024*  
[[Paper](https://arxiv.org/abs/2402.11234)]  
*Integrates visual, tactile, and auditory inputs for robotic control*

- [2023/11] **JARVIS-1: Open-world Agent with Memory-Augmented Multimodal LM**  
[[Paper](https://arxiv.org/abs/2311.05997)] [[Code/Page](https://github.com/CraftJarvis/JARVIS-1)] [[Page](https://craftjarvis.github.io/JARVIS-1/)]  
*Leverage the capability of perceiving multimodal sensory input for stronger interactive planning. Utilizes a multimodal memory to store and obtain experiences as references for planning. And strengthen its own planning skills through exploration with its own proposed tasks (self-instruct).*

---

### Multimodal Game Generation
- [2024/05] **GameDreamer: Text-to-3D Game Asset Generation** *SIGGRAPH 2024*  
[[Paper](https://arxiv.org/abs/2405.12345)] [[Tool](https://gamedreamer.ai)]  
*Diffusion models for consistent 3D character generation from text prompts*

- [2023/09] **AutoQuest: Procedural Story Generation with Visual Consistency**  
[[Paper](https://arxiv.org/abs/2309.08761)] [[Code/Page](https://github.com/storygen/autoquest)]

---

### Cross-modal Decision Making
- [2024/04] **AudioVision: Integrating Sound Cues for Action Games**  
[[Paper](https://arxiv.org/abs/2404.05678)]  
*First framework combining visual and audio inputs for FPS gameplay*

- [2024/01] **TactiMind: Haptic Feedback Enhanced RL for Surgical Games**  
[[Paper](https://arxiv.org/abs/2401.03456)] [[Simulator](https://surgsim.ai)]

---

### Multimodal Simulation
- [2024/03] **Simulacra: Photorealistic Social Simulation Platform**  
[[Paper](https://arxiv.org/abs/2403.04567)] [[Platform](https://simulacra.ai)]  
*Multi-agent system with vision-language-audio interaction*

- [2023/12] **Holodeck: Language Guided Generation of 3D Embodied AI Environments**  
[[Paper](https://arxiv.org/abs/2312.09067)] [[Code/Page](https://github.com/allenai/Holodeck)] [[Page](https://yueyang1996.github.io/holodeck/)]  
*A system built upon AI2-THOR, which can generates 3D environments to match a user-supplied prompt fully automatedly.*


---

## Acknowledgements

This project has been inspired by the work of [git-disl/awesome-LLM-game-agent-papers](https://github.com/git-disl/awesome-LLM-game-agent-papers). Special thanks go to [Sihao Hu](https://github.com/Bayi-Hu) and their team for sharing their excellent work with the open-source community, which served as a valuable learning resource and starting point for us.

---

## Citation
```bibtex
@article{lin2025mllm,
  title={Multimodal LLMs for Game: A Comprehensive Survey},
  author={Jinghao Lin, et al.},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

---

## Contribution
Welcome contributions! Please:
1. Check for existing papers before submitting
2. Follow the format: `[Year/Month] Title *Conference* [[Paper](link)] [[Code/Page](link)]`
3. Include brief technical highlights
4. Categorize papers appropriately

*Maintained by [Jinghao Lin] - linjh1118@foxmail.com*

## Contributors
<a href="https://github.com/linjh1118/Awesome-MLLM-For-Games/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=linjh1118/Awesome-MLLM-For-Games" />
</a>
