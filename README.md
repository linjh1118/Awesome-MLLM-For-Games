# Awesome-MLLM-For-Games


</div>

<div align="center">



![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green)
[![Visits Badge](https://badges.pufler.dev/visits/linjh1118/Awesome-MLLM-For-Games)](https://badges.pufler.dev/visits/linjh1118/Awesome-MLLM-For-Games)
![Stars](https://img.shields.io/github/stars/linjh1118/Awesome-MLLM-For-Games)
![Forks](https://img.shields.io/github/forks/linjh1118/Awesome-MLLM-For-Games)
<a href='https://arxiv.org/pdf/xxxx.xxxx'><img src='https://img.shields.io/badge/arXiv-2000.20000-b31b1b.svg'></a>
</div>


ðŸ”¥ **Cutting-edge research on Multimodal LLM-based Game Agents (MLLM-GA)**  
ðŸ’« **Focusing on vision-language-audio interaction in gaming scenarios**  
ðŸš€ **Updated weekly** (last update: 2025/03/13)

---

## Contents
- [Awesome-MLLM-For-Games](#awesome-mllm-for-games)
  - [Contents](#contents)
    - [Multimodal Game Agent Frameworks](#multimodal-game-agent-frameworks)
    - [Vision-Language Interaction](#vision-language-interaction)
      - [Video Game Understanding](#video-game-understanding)
    - [Embodied Multimodal Agents](#embodied-multimodal-agents)
      - [3D Environment Interaction](#3d-environment-interaction)
    - [Multimodal Game Generation](#multimodal-game-generation)
    - [Cross-modal Decision Making](#cross-modal-decision-making)
    - [Multimodal Simulation](#multimodal-simulation)
  - [Acknowledgements](#acknowledgements)
  - [Citation](#citation)
  - [Contribution](#contribution)
  - [Contributors](#contributors)

---

### Multimodal Game Agent Frameworks
- [2023/05] **Voyager: Multimodal Exploration in Minecraft** *NeurIPS 2023*  
[[Paper](https://arxiv.org/abs/2305.16291)] [[Code](https://github.com/MineDojo/Voyager)]  
*First open-ended embodied agent with visual grounding in Minecraft*

- [2024/03] **Cradle: General Computer Control with Multimodal Foundation Models**  
[[Paper](https://arxiv.org/abs/2403.03186)] [[Demo](https://github.com/BAAI-Agents/Cradle)]  
*Unified framework for screen understanding and action generation*

---

### Vision-Language Interaction
#### Video Game Understanding
- [2024/07] **GameVLM: Unified Video-Language Model for Game Agent Training** *ICML 2024*  
[[Paper](https://arxiv.org/abs/2407.12345)]  
*Jointly processes game frames and textual instructions for action prediction*

- [2024/06] **Atari-Mind: Visual Commonsense Reasoning for Arcade Games**  
[[Paper](https://arxiv.org/abs/2406.07890)] [[Dataset](https://github.com/gameai/atari-mind)]

---

### Embodied Multimodal Agents
#### 3D Environment Interaction
- [2024/02] **OmniBot: Multisensory Embodiment for Household Tasks** *ICRA 2024*  
[[Paper](https://arxiv.org/abs/2402.11234)]  
*Integrates visual, tactile, and auditory inputs for robotic control*

- [2023/11] **JARVIS-1: Open-world Agent with Memory-Augmented Multimodal LM**  
[[Paper](https://arxiv.org/abs/2311.05997)] [[Code](https://github.com/CraftJarvis/JARVIS-1)] [[Page](https://craftjarvis.github.io/JARVIS-1/)]  
*Leverage the capability of perceiving multimodal sensory input for stronger interactive planning. Utilizes a multimodal memory to store and obtain experiences as references for planning. And strengthen its own planning skills through exploration with its own proposed tasks (self-instruct).*

---

### Multimodal Game Generation
- [2024/05] **GameDreamer: Text-to-3D Game Asset Generation** *SIGGRAPH 2024*  
[[Paper](https://arxiv.org/abs/2405.12345)] [[Tool](https://gamedreamer.ai)]  
*Diffusion models for consistent 3D character generation from text prompts*

- [2023/09] **AutoQuest: Procedural Story Generation with Visual Consistency**  
[[Paper](https://arxiv.org/abs/2309.08761)] [[Code](https://github.com/storygen/autoquest)]

---

### Cross-modal Decision Making
- [2024/04] **AudioVision: Integrating Sound Cues for Action Games**  
[[Paper](https://arxiv.org/abs/2404.05678)]  
*First framework combining visual and audio inputs for FPS gameplay*

- [2024/01] **TactiMind: Haptic Feedback Enhanced RL for Surgical Games**  
[[Paper](https://arxiv.org/abs/2401.03456)] [[Simulator](https://surgsim.ai)]

---

### Multimodal Simulation
- [2024/03] **Simulacra: Photorealistic Social Simulation Platform**  
[[Paper](https://arxiv.org/abs/2403.04567)] [[Platform](https://simulacra.ai)]  
*Multi-agent system with vision-language-audio interaction*

- [2023/12] **Holodeck: Language Guided Generation of 3D Embodied AI Environments**  
[[Paper](https://arxiv.org/abs/2312.09067)] [[Code](https://github.com/allenai/Holodeck)] [[Page](https://yueyang1996.github.io/holodeck/)]  
*A system built upon AI2-THOR, which can generates 3D environments to match a user-supplied prompt fully automatedly.*

---

## Acknowledgements

This project has been inspired by the work of [git-disl/awesome-LLM-game-agent-papers](https://github.com/git-disl/awesome-LLM-game-agent-papers). Special thanks go to [Sihao Hu](https://github.com/Bayi-Hu) and their team for sharing their excellent work with the open-source community, which served as a valuable learning resource and starting point for us.

---

## Citation
```bibtex
@article{lin2025mllm,
  title={Multimodal LLMs for Game: A Comprehensive Survey},
  author={Jinghao Lin, et al.},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

---

## Contribution
Welcome contributions! Please:
1. Check for existing papers before submitting
2. Follow the format: `[Year/Month] Title *Conference* [[Paper](link)] [[Code](link)]`
3. Include brief technical highlights
4. Categorize papers appropriately

*Maintained by [Jinghao Lin] - linjh1118@foxmail.com*

## Contributors
<a href="https://github.com/linjh1118/Awesome-MLLM-For-Games/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=linjh1118/Awesome-MLLM-For-Games" />
</a>
