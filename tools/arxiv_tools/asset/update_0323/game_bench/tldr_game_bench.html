
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments</td>
      <td>Large Language Model~(LLM) based agents have been increasingly popular in<br>solving complex and dynamic tasks, which requires proper evaluation systems to<br>assess their capabilities. Nevertheless, existing benchmarks usually either<br>focus on single-objective tasks or use overly broad assessing metrics, failing<br>to provide a comprehensive inspection of the actual capabilities of LLM-based<br>agents in complicated decision-making tasks. To address these issues, we<br>introduce DSGBench, a more rigorous evaluation platform for strategic<br>decision-making. Firstly, it incorporates six complex strategic games which<br>serve as ideal testbeds due to their long-term and multi-dimensional<br>decision-making demands and flexibility in customizing tasks of various<br>difficulty levels or multiple targets. Secondly, DSGBench employs a<br>fine-grained evaluation scoring system which examines the decision-making<br>capabilities by looking into the performance in five specific dimensions and<br>offering a comprehensive assessment in a well-designed way. Furthermore,<br>DSGBench also incorporates an automated decision-tracking mechanism which<br>enables in-depth analysis of agent behaviour patterns and the changes in their<br>strategies. We demonstrate the advances of DSGBench by applying it to multiple<br>popular LLM-based agents and our results suggest that DSGBench provides<br>valuable insights in choosing LLM-based agents as well as improving their<br>future development. DSGBench is available at<br>https://github.com/DeciBrain-Group/DSGBench.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DSGBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å†³ç­–ç¯å¢ƒä¸­çš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚å’ŒåŠ¨æ€ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„å®é™…èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ç³»ç»Ÿé€šå¸¸åªå…³æ³¨å•ä¸€ç›®æ ‡ä»»åŠ¡æˆ–ä½¿ç”¨è¿‡äºå®½æ³›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMæ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„å®é™…èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDSGBenchæ˜¯ä¸€ä¸ªæ›´ä¸¥æ ¼çš„è¯„ä¼°å¹³å°ï¼Œç”¨äºè¯„ä¼°æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚å®ƒåŒ…å«äº†å…­ä¸ªå¤æ‚çš„æˆ˜ç•¥æ¸¸æˆï¼Œè¿™äº›æ¸¸æˆå› å…¶é•¿æœŸå’Œå¤šç»´åº¦çš„å†³ç­–éœ€æ±‚ä»¥åŠå®šåˆ¶å„ç§éš¾åº¦çº§åˆ«æˆ–å¤šä¸ªç›®æ ‡çš„ä»»åŠ¡çš„çµæ´»æ€§è€Œæˆä¸ºç†æƒ³çš„æµ‹è¯•å¹³å°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDSGBenché‡‡ç”¨äº†ä¸€ç§ç»†ç²’åº¦çš„è¯„ä¼°è¯„åˆ†ç³»ç»Ÿï¼Œé€šè¿‡è€ƒå¯Ÿåœ¨äº”ä¸ªç‰¹å®šç»´åº¦ä¸­çš„è¡¨ç°æ¥æ£€æŸ¥å†³ç­–èƒ½åŠ›ï¼Œå¹¶ä»¥ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„æ–¹å¼æä¾›å…¨é¢çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼ŒDSGBenchè¿˜åŒ…å«ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å†³ç­–è·Ÿè¸ªæœºåˆ¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æä»£ç†çš„è¡Œä¸ºæ¨¡å¼å’Œç­–ç•¥çš„å˜åŒ–ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>DSGBenché€šè¿‡åº”ç”¨äºå¤šä¸ªæµè¡Œçš„LLMæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨é€‰æ‹©LLMæ¨¡å‹ä»¥åŠæ”¹è¿›å…¶æœªæ¥å‘å±•æ–¹é¢çš„ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSGBenchèƒ½å¤Ÿæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMæ¨¡å‹åœ¨ä¸åŒå†³ç­–ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DSGBenchä¸ºè¯„ä¼°LLMæ¨¡å‹åœ¨å¤æ‚å†³ç­–ç¯å¢ƒä¸­çš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ã€‚å…¶ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡å’Œå†³ç­–è·Ÿè¸ªæœºåˆ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ·±å…¥äº†è§£LLMæ¨¡å‹çš„è¡Œä¸ºæ¨¡å¼å’Œç­–ç•¥å˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°æ”¹è¿›æ¨¡å‹çš„è®¾è®¡å’Œå¼€å‘ã€‚æ­¤å¤–ï¼ŒDSGBenchçš„çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¯„ä¼°éœ€æ±‚ï¼Œä¸ºLLMæ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests</td>
      <td>We examine three evaluation paradigms: large question-answering benchmarks<br>(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and<br>cognitive tests (e.g., for working memory or theory of mind). First, we<br>investigate which of the former two-benchmarks or games-is most effective at<br>discriminating LLMs of varying quality. Then, inspired by human cognitive<br>assessments, we compile a suite of targeted tests that measure cognitive<br>abilities deemed essential for effective language use, and we investigate their<br>correlation with model performance in benchmarks and games. Our analyses reveal<br>that interactive games are superior to standard benchmarks in discriminating<br>models. Causal and logical reasoning correlate with both static and interactive<br>tests, while differences emerge regarding core executive functions and<br>social/emotional skills, which correlate more with games. We advocate the<br>development of new interactive benchmarks and targeted cognitive tasks inspired<br>by assessing human abilities but designed specifically for LLMs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡åŸºå‡†æµ‹è¯•ã€æ¸¸æˆå’Œè®¤çŸ¥æµ‹è¯•æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡çš„é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œä½†è¿™äº›æµ‹è¯•å¾€å¾€å­˜åœ¨æ•°æ®æ±¡æŸ“å’Œæ¨¡å‹å¯¹æç¤ºæ ¼å¼æ•æ„Ÿçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æµ‹è¯•å¿½ç•¥äº†å®é™…çš„è¯­è¨€ä½¿ç”¨åœºæ™¯ï¼Œå³åœ¨ç¤¾ä¼šå’Œä»»åŠ¡å¯¼å‘çš„å¤šè½®äº¤äº’ä¸­ä½¿ç”¨è¯­è¨€ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢æ›´æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°ç†è§£LLMsçš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸‰ç§è¯„ä¼°èŒƒå¼ï¼šå¤§è§„æ¨¡é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMLUå’ŒBBHï¼‰ã€äº¤äº’å¼æ¸¸æˆï¼ˆå¦‚ä¿¡å·æ¸¸æˆæˆ–ç¦å¿Œæ¸¸æˆï¼‰å’Œè®¤çŸ¥æµ‹è¯•ï¼ˆå¦‚å·¥ä½œè®°å¿†æˆ–å¿ƒæ™ºç†è®ºæµ‹è¯•ï¼‰ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›èŒƒå¼ï¼Œç ”ç©¶å›¢é˜Ÿæ—¨åœ¨æ‰¾å‡ºæœ€æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå—äººç±»è®¤çŸ¥è¯„ä¼°çš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹æ€§çš„æµ‹è¯•ï¼Œä»¥è¡¡é‡å¯¹æœ‰æ•ˆè¯­è¨€ä½¿ç”¨è‡³å…³é‡è¦çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMsåœ¨å› æœæ¨ç†ã€é€»è¾‘æ¨ç†ã€å·¥ä½œè®°å¿†ã€æƒ…ç»ªæ™ºåŠ›ç­‰æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶åˆ†æè¿™äº›èƒ½åŠ›ä¸æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•å’Œæ¸¸æˆä¸­çš„è¡¨ç°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼æ¸¸æˆåœ¨åŒºåˆ†ä¸åŒè´¨é‡çš„LLMsæ–¹é¢ä¼˜äºä¼ ç»Ÿçš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚å› æœæ¨ç†å’Œé€»è¾‘æ¨ç†ä¸é™æ€å’Œäº¤äº’å¼æµ‹è¯•éƒ½ç›¸å…³ï¼Œè€Œæ ¸å¿ƒæ‰§è¡ŒåŠŸèƒ½å’Œç¤¾äº¤/æƒ…æ„ŸæŠ€èƒ½åˆ™æ›´å¤šåœ°ä¸æ¸¸æˆç›¸å…³ã€‚ç‰¹åˆ«æ˜¯ï¼Œå·¥ä½œè®°å¿†å’Œæƒ…ç»ªæ™ºåŠ›ä»…ä¸æ¸¸æˆä¸­çš„è¡¨ç°æ˜¾è‘—ç›¸å…³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼æ¸¸æˆæ˜¯ä¸€ç§æ›´æœ‰æ•ˆçš„è¯„ä¼°LLMsçš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¨¡å‹åœ¨å®é™…è¯­è¨€ä½¿ç”¨åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ€§çš„è®¤çŸ¥æµ‹è¯•å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMsçš„å†…åœ¨èƒ½åŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æä¾›æŒ‡å¯¼ã€‚å› æ­¤ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºLLMsçš„è¯„ä¼°å’Œå¼€å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents</td>
      <td>Large Language Model (LLM) agents frameworks often employ modular<br>architectures, incorporating components such as planning, reasoning, action<br>execution, and reflection to tackle complex tasks. However, quantifying the<br>contribution of each module to overall system performance remains a significant<br>challenge, impeding optimization and interpretability. To address this, we<br>introduce CapaBench (Capability-level Assessment Benchmark), an evaluation<br>framework grounded in cooperative game theory's Shapley Value, which<br>systematically measures the marginal impact of individual modules and their<br>interactions within an agent's architecture. By replacing default modules with<br>test variants across all possible combinations, CapaBench provides a principle<br>method for attributing performance contributions. Key contributions include:<br>(1) We are the first to propose a Shapley Value-based methodology for<br>quantifying the contributions of capabilities in LLM agents; (2) Modules with<br>high Shapley Values consistently lead to predictable performance gains when<br>combined, enabling targeted optimization; and (3) We build a multi-round<br>dataset of over 1,500 entries spanning diverse domains and practical task<br>scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench<br>bridges the gap between component-level evaluation and holistic system<br>assessment, providing actionable insights for optimizing modular LLM agents and<br>advancing their deployment in complex, real-world scenarios.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è°æ˜¯MVPï¼ŸåŸºäºåšå¼ˆè®ºçš„LLM Agentæ¨¡å—åŒ–è¯„ä¼°åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼ŒLLM Agentæ¡†æ¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é€šå¸¸é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼ŒåŒ…æ‹¬è§„åˆ’ã€æ¨ç†ã€è¡ŒåŠ¨æ‰§è¡Œå’Œåæ€ç­‰ç»„ä»¶ã€‚ç„¶è€Œï¼Œå¦‚ä½•é‡åŒ–æ¯ä¸ªæ¨¡å—å¯¹æ•´ä½“ç³»ç»Ÿæ€§èƒ½çš„è´¡çŒ®ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™é˜»ç¢äº†ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CapaBenchï¼ˆèƒ½åŠ›çº§è¯„ä¼°åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåˆä½œåšå¼ˆè®ºçš„Shapley Valueçš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¡¡é‡äº†å•ä¸ªæ¨¡å—åŠå…¶åœ¨Agentæ¶æ„ä¸­çš„äº¤äº’çš„è¾¹é™…å½±å“ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡æå‡ºåŸºäºShapley Valueçš„æ–¹æ³•æ¥é‡åŒ–LLM Agentä¸­èƒ½åŠ›çš„è´¡çŒ®ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…·æœ‰é«˜Shapley Valueçš„æ¨¡å—åœ¨ç»„åˆæ—¶å§‹ç»ˆå¯¼è‡´å¯é¢„æµ‹çš„æ€§èƒ½æå‡ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡1,500ä¸ªæ¡ç›®çš„å¤šè½®æ•°æ®é›†ï¼Œæ¶µç›–å„ç§é¢†åŸŸå’Œå®é™…ä»»åŠ¡åœºæ™¯ï¼Œä½¿Agentèƒ½åŠ›çš„å…¨é¢è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹ä¹ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæ¶µç›–äº†äº”ä¸ªä¸»è¦ä»»åŠ¡ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚å’Œç‹¬ç‰¹çš„æ¨¡å—è´¡çŒ®æ¨¡å¼ã€‚ç»“æœè¡¨æ˜ï¼Œå…·æœ‰é«˜Shapley Valueçš„æ¨¡å—åœ¨ç»„åˆæ—¶å§‹ç»ˆå¯¼è‡´å¯é¢„æµ‹çš„æ€§èƒ½æå‡ï¼Œè¿™ä¸ºå¼€å‘äººå‘˜æä¾›äº†æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–æŒ‡å¯¼ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>CapaBenchæ¡†æ¶ä¸ºä¼˜åŒ–æ¨¡å—åŒ–LLM Agentå’Œæ¨è¿›å…¶åœ¨å¤æ‚ã€ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°å’Œé‡åŒ–æ¯ä¸ªæ¨¡å—çš„è´¡çŒ®ï¼Œå¼€å‘äººå‘˜å¯ä»¥æ›´å¥½åœ°ç†è§£Agentçš„æ€§èƒ½ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜LLM Agentåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ•ˆæœã€‚</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research</td>
      <td>After the great achievement of solving two-player zero-sum games, more and<br>more AI researchers focus on solving multiplayer games. To facilitate the<br>development of designing efficient learning algorithms for solving multiplayer<br>games, we propose a multiplayer game platform for solving Urban Network<br>Security Games (\textbf{UNSG}) that model real-world scenarios. That is,<br>preventing criminal activity is a highly significant responsibility assigned to<br>police officers in cities, and police officers have to allocate their limited<br>security resources to interdict the escaping criminal when a crime takes place<br>in a city. This interaction between multiple police officers and the escaping<br>criminal can be modeled as a UNSG. The variants of UNSGs can model different<br>real-world settings, e.g., whether real-time information is available or not,<br>and whether police officers can communicate or not. The main challenges of<br>solving this game include the large size of the game and the co-existence of<br>cooperation and competition. While previous efforts have been made to tackle<br>UNSGs, they have been hampered by performance and scalability issues.<br>Therefore, we propose an open-source UNSG platform (\textbf{GraphChase}) for<br>designing efficient learning algorithms for solving UNSGs. Specifically,<br>GraphChase offers a unified and flexible game environment for modeling various<br>variants of UNSGs, supporting the development, testing, and benchmarking of<br>algorithms. We believe that GraphChase not only facilitates the development of<br>efficient algorithms for solving real-world problems but also paves the way for<br>significant advancements in algorithmic development for solving general<br>multiplayer games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GraphChaseï¼šè§£å†³åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆçš„AIå¹³å°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½åœ¨è§£å†³ä¸¤äººé›¶å’Œåšå¼ˆæ–¹é¢å–å¾—å·¨å¤§æˆå°±ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜å¼€å§‹å…³æ³¨è§£å†³å¤šäººæ¸¸æˆã€‚åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆï¼ˆUNSGï¼‰ä½œä¸ºä¸€ç§æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯çš„å¤šç©å®¶æ¸¸æˆï¼Œå¯¹äºç ”ç©¶å¤šäººåšå¼ˆç®—æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼ŒUNSGçš„è§£å†³é¢ä¸´ç€æ¸¸æˆè§„æ¨¡åºå¤§ã€åˆä½œä¸ç«äº‰å…±å­˜ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰çš„ç®—æ³•åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGraphChaseå¹³å°<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGraphChaseçš„å¼€æºUNSGå¹³å°ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ã€çµæ´»çš„æ¸¸æˆç¯å¢ƒï¼Œç”¨äºæ¨¡æ‹Ÿå„ç§UNSGå˜ä½“ï¼Œå¹¶æ”¯æŒç®—æ³•çš„å¼€å‘ã€æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ã€‚GraphChaseå¹³å°å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š<br>- **çµæ´»çš„æ¸¸æˆç¯å¢ƒ**ï¼šç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰æ¸¸æˆå‚æ•°ï¼ŒåŒ…æ‹¬å›¾ç»“æ„ã€åˆå§‹ä½ç½®ã€æ—¶é—´èŒƒå›´ç­‰ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒçš„UNSGåœºæ™¯ã€‚<br>- **å¤šç§ç®—æ³•æ”¯æŒ**ï¼šGraphChaseå¹³å°æ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚CFR-MIXã€NSG-NFSPã€NSGZeroã€Pretrained PSROå’ŒGrasperï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œç®—æ³•æ¯”è¾ƒå’Œè¯„ä¼°ã€‚<br>- **æ¨¡å—åŒ–è®¾è®¡**ï¼šGraphChaseå¹³å°é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…æ‹¬æ¸¸æˆæ¨¡å—ã€ä»£ç†æ¨¡å—å’Œæ±‚è§£å™¨æ¨¡å—ï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œè‡ªå®šä¹‰å’Œæ‰©å±•ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºå‡†æµ‹è¯•<br>æœ¬æ–‡åœ¨GraphChaseå¹³å°ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯„ä¼°äº†ç°æœ‰ç®—æ³•çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ç°æœ‰ç®—æ³•åœ¨ç®€å•åœºæ™¯ä¸‹èƒ½å¤Ÿå–å¾—åˆç†æ€§èƒ½ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸‹ä»ç„¶å­˜åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚è¿™è¡¨æ˜ï¼Œå¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„ç®—æ³•æ¥è§£å†³ç°å®ä¸–ç•Œçš„UNSGä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphChaseå¹³å°èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹ŸUNSGåœºæ™¯ï¼Œå¹¶ä¸”èƒ½å¤ŸåŠ é€Ÿç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œç°æœ‰ç®—æ³•åœ¨è§£å†³å¤æ‚åœºæ™¯å’Œå¤§è§„æ¨¡æ¸¸æˆæ—¶å­˜åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GraphChaseå¹³å°ä¸ºç ”ç©¶UNSGå’Œå¤šäººåšå¼ˆç®—æ³•æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾åœ°æ¨¡æ‹Ÿä¸åŒçš„åœºæ™¯å’Œæµ‹è¯•ä¸åŒçš„ç®—æ³•ã€‚æ­¤å¤–ï¼ŒGraphChaseå¹³å°è¿˜å¯ä»¥ä½œä¸ºå…¶ä»–å¤šäººåšå¼ˆé—®é¢˜çš„æµ‹è¯•å¹³å°ï¼Œä¾‹å¦‚å¯¹æŠ—æ€§å›¢é˜Ÿæ¸¸æˆå’Œè¿½é€ƒæ¸¸æˆã€‚<br><br>## ğŸ“š æ€»ç»“<br>GraphChaseå¹³å°ä¸ºè§£å†³åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆæä¾›äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼Œå¹¶ä¸ºç ”ç©¶å¤šäººåšå¼ˆç®—æ³•æä¾›äº†æ–°çš„æ€è·¯ã€‚éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒGraphChaseå¹³å°æœ‰æœ›åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚</td>
    </tr>
    <tr>
      <th>4</th>
      <td>How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games</td>
      <td>The deployment of large language models (LLMs) in diverse applications<br>requires a thorough understanding of their decision-making strategies and<br>behavioral patterns. As a supplement to a recent study on the behavioral Turing<br>test, this paper presents a comprehensive analysis of five leading LLM-based<br>chatbot families as they navigate a series of behavioral economics games. By<br>benchmarking these AI chatbots, we aim to uncover and document both common and<br>distinct behavioral patterns across a range of scenarios. The findings provide<br>valuable insights into the strategic preferences of each LLM, highlighting<br>potential implications for their deployment in critical decision-making roles.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡Œä¸ºæ¨¡å¼<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç†è§£è¿™äº›æ¨¡å‹çš„å†³ç­–ç­–ç•¥å’Œè¡Œä¸ºæ¨¡å¼å˜å¾—è‡³å…³é‡è¦ã€‚è¿™ä¸ä»…æœ‰åŠ©äºä¼˜åŒ–å®ƒä»¬åœ¨ç‰¹å®šåº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œè¿˜èƒ½æ›´å¥½åœ°è¯„ä¼°å®ƒä»¬çš„å¯é æ€§å’Œå¯é¢„æµ‹æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠé‡å¤§è´£ä»»çš„æƒ…å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºä¸åŒLLMsçš„è¡Œä¸ºæ¨¡å¼çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡¨ç°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€šè¿‡è¡Œä¸ºç»æµå­¦æ¸¸æˆè¯„ä¼°LLMs<br>æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—ç»å…¸çš„è¡Œä¸ºç»æµå­¦æ¸¸æˆï¼Œå¦‚ç‹¬è£è€…æ¸¸æˆã€æœ€åé€šç‰’æ¸¸æˆã€ä¿¡ä»»æ¸¸æˆã€å…¬å…±ç‰©å“æ¸¸æˆã€ç‚¸å¼¹é£é™©æ¸¸æˆå’Œå›šå¾’å›°å¢ƒæ¸¸æˆï¼Œå¯¹äº”ç§é¢†å…ˆçš„LLM-basedèŠå¤©æœºå™¨äººè¿›è¡Œäº†å…¨é¢åˆ†æã€‚è¿™äº›æ¸¸æˆæ—¨åœ¨æµ‹è¯•LLMsåœ¨ä¿¡ä»»ã€å…¬å¹³ã€é£é™©è§„é¿ã€åˆ©ä»–ä¸»ä¹‰å’Œåˆä½œç­‰æ–¹é¢çš„è¡Œä¸ºæ¨¡å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥è¡Œä¸ºå›¾çµæµ‹è¯•å’Œåˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•<br>ä¸ºäº†è¯„ä¼°LLMsçš„è¡Œä¸ºæ¨¡å¼ï¼Œæœ¬æ–‡å¼•å…¥äº†è¡Œä¸ºå›¾çµæµ‹è¯•å’Œåˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•ã€‚è¡Œä¸ºå›¾çµæµ‹è¯•é€šè¿‡æ¯”è¾ƒLLMså’Œäººç±»çš„è¡Œä¸ºåˆ†å¸ƒï¼Œè¯„ä¼°LLMsæ˜¯å¦èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¡Œä¸ºã€‚åˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•åˆ™ä½¿ç”¨Wassersteinè·ç¦»æ¥è¡¡é‡LLMsçš„è¡Œä¸ºåˆ†å¸ƒä¸äººç±»è¡Œä¸ºåˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æµ‹è¯•çš„èŠå¤©æœºå™¨äººéƒ½èƒ½æˆåŠŸåœ°æ•æ‰åˆ°ç‰¹å®šçš„äººç±»è¡Œä¸ºæ¨¡å¼ï¼Œå¯¼è‡´å†³ç­–åˆ†å¸ƒé«˜åº¦é›†ä¸­ã€‚å°½ç®¡æ——èˆ°èŠå¤©æœºå™¨äººè¡¨ç°å‡ºæ˜¾è‘—çš„é€šè¿‡å›¾çµæµ‹è¯•çš„æ¦‚ç‡ï¼Œä½†å®ƒä»¬åªèƒ½äº§ç”Ÿä¸äººç±»ç›¸ä¼¼çš„è¡Œä¸ºåˆ†å¸ƒã€‚ä¸äººç±»ç›¸æ¯”ï¼ŒèŠå¤©æœºå™¨äººåœ¨æ”¶ç›Šåå¥½ä¸Šæ›´å¼ºè°ƒå…¬å¹³æ€§ã€‚èŠå¤©æœºå™¨äººåœ¨ä¸åŒæ¸¸æˆä¸­çš„æ”¶ç›Šåå¥½å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚ä¸åŒçš„èŠå¤©æœºå™¨äººåœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ä»¥é€šè¿‡å›¾çµæµ‹è¯•ç»“æœã€æ”¶ç›Šåå¥½å’Œè¡Œä¸ºçš„è¿è´¯æ€§è¿›ä¸€æ­¥åŒºåˆ†ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºç†è§£LLMsçš„è¡Œä¸ºæ¨¡å¼æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥åœ¨AIè¡Œä¸ºç§‘å­¦é¢†åŸŸçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¨¡ä»¿äººç±»è¡Œä¸ºæ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥æé«˜LLMsåœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨ä¸åŒæ¸¸æˆä¸­çš„è¡Œä¸ºä¸ä¸€è‡´æ€§ä¹Ÿè¡¨æ˜ï¼Œéœ€è¦å¼€å‘æ›´é€šç”¨çš„åå¥½å’Œç›®æ ‡ï¼Œä»¥ä¾¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”å„ç§æƒ…å¢ƒã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games</td>
      <td>Decision-making in large-scale games is an essential research area in<br>artificial intelligence (AI) with significant real-world impact. However, the<br>limited access to realistic large-scale game environments has hindered research<br>progress in this area. In this paper, we present AuctionNet, a benchmark for<br>bid decision-making in large-scale ad auctions derived from a real-world online<br>advertising platform. AuctionNet is composed of three parts: an ad auction<br>environment, a pre-generated dataset based on the environment, and performance<br>evaluations of several baseline bid decision-making algorithms. More<br>specifically, the environment effectively replicates the integrity and<br>complexity of real-world ad auctions through the interaction of several<br>modules: the ad opportunity generation module employs deep generative networks<br>to bridge the gap between simulated and real-world data while mitigating the<br>risk of sensitive data exposure; the bidding module implements diverse<br>auto-bidding agents trained with different decision-making algorithms; and the<br>auction module is anchored in the classic Generalized Second Price (GSP)<br>auction but also allows for customization of auction mechanisms as needed. To<br>facilitate research and provide insights into the environment, we have also<br>pre-generated a substantial dataset based on the environment. The dataset<br>contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over<br>500 million auction records. Performance evaluations of baseline algorithms<br>such as linear programming, reinforcement learning, and generative models for<br>bid decision-making are also presented as a part of AuctionNet. We believe that<br>AuctionNet is applicable not only to research on bid decision-making in ad<br>auctions but also to the general area of decision-making in large-scale games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AuctionNetï¼šå¤§å‹æ¸¸æˆä¸­å†³ç­–åˆ¶å®šçš„æ–°åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹æ¸¸æˆä¸­çš„å†³ç­–åˆ¶å®šæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œå®ƒå¯¹ç°å®ä¸–ç•Œæœ‰ç€æ·±è¿œçš„å½±å“ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯¹çœŸå®å¤§å‹æ¸¸æˆç¯å¢ƒçš„è®¿é—®ï¼Œè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿›å±•å—åˆ°äº†é™åˆ¶ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿç¯å¢ƒå¾€å¾€ä¸çœŸå®ç¯å¢ƒå­˜åœ¨è¾ƒå¤§å·®è·ï¼Œæ— æ³•å®Œå…¨åæ˜ ç°å®ä¸–ç•Œä¸­çš„å¤šæ™ºèƒ½ä½“åŠ¨æ€ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuctionNetç¯å¢ƒ<br>AuctionNetæ˜¯ä¸€ä¸ªåŸºäºçœŸå®åœ¨çº¿å¹¿å‘Šå¹³å°çš„å¤§å‹å¹¿å‘Šæ‹å–å†³ç­–åˆ¶å®šåŸºå‡†ã€‚å®ƒç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šå¹¿å‘Šæ‹å–ç¯å¢ƒã€åŸºäºç¯å¢ƒçš„é¢„ç”Ÿæˆæ•°æ®é›†ä»¥åŠå‡ ä¸ªåŸºçº¿æŠ•æ ‡å†³ç­–åˆ¶å®šç®—æ³•çš„æ€§èƒ½è¯„ä¼°ã€‚è¯¥ç¯å¢ƒé€šè¿‡å‡ ä¸ªæ¨¡å—çš„äº¤äº’æœ‰æ•ˆåœ°å¤åˆ¶äº†ç°å®ä¸–ç•Œå¹¿å‘Šæ‹å–çš„å®Œæ•´æ€§å’Œå¤æ‚æ€§ï¼š<br>- å¹¿å‘Šæœºä¼šç”Ÿæˆæ¨¡å—ï¼šä½¿ç”¨æ·±åº¦ç”Ÿæˆç½‘ç»œæ¥å¼¥åˆæ¨¡æ‹Ÿæ•°æ®å’Œç°å®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„å·®è·ï¼ŒåŒæ—¶é™ä½æ•æ„Ÿæ•°æ®æ³„éœ²çš„é£é™©ã€‚<br>- æŠ•æ ‡æ¨¡å—ï¼šå®ç°äº†å¤šç§è‡ªåŠ¨æŠ•æ ‡ä»£ç†ï¼Œè¿™äº›ä»£ç†ä½¿ç”¨ä¸åŒçš„å†³ç­–åˆ¶å®šç®—æ³•è¿›è¡Œè®­ç»ƒã€‚<br>- æ‹å–æ¨¡å—ï¼šä»¥ç»å…¸çš„å¹¿ä¹‰ç¬¬äºŒä»·æ ¼ï¼ˆGSPï¼‰æ‹å–ä¸ºåŸºç¡€ï¼Œä½†ä¹Ÿå…è®¸æ ¹æ®éœ€è¦å®šåˆ¶æ‹å–æœºåˆ¶ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢„ç”Ÿæˆæ•°æ®é›†<br>ä¸ºäº†ä¿ƒè¿›ç ”ç©¶å’Œæä¾›å¯¹ç¯å¢ƒçš„æ´å¯Ÿï¼ŒAuctionNetè¿˜åŸºäºç¯å¢ƒé¢„ç”Ÿæˆäº†ä¸€ä¸ªåºå¤§çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«1000ä¸‡ä¸ªå¹¿å‘Šæœºä¼šã€48ç§ä¸åŒçš„è‡ªåŠ¨æŠ•æ ‡ä»£ç†å’Œè¶…è¿‡5äº¿æ¡æ‹å–è®°å½•ã€‚è¿™äº›æ•°æ®å¯ä»¥ç”¨äºå»ºæ¨¡ç¯å¢ƒï¼Œå¹¶æœ‰æ•ˆåœ°è®­ç»ƒè‡ªåŠ¨æŠ•æ ‡ä»£ç†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>AuctionNetå·²ç»ä¸ºNeurIPS 2024â€œå¤§å‹æ‹å–ä¸­çš„è‡ªåŠ¨æŠ•æ ‡â€ç«èµ›æä¾›äº†åŠ¨åŠ›ï¼Œä¸ºæ¥è‡ªä¸–ç•Œå„åœ°çš„1500å¤šæ”¯é˜Ÿä¼æä¾›äº†ç«èµ›ç¯å¢ƒã€‚è¯¥ç«èµ›è§£å†³äº†åœ¨ä¸ç¡®å®šå’Œç«äº‰ç¯å¢ƒä¸­è¿›è¡Œé«˜é¢‘æŠ•æ ‡å†³ç­–åˆ¶å®šçš„å…³é”®é—®é¢˜ï¼Œå¹¶æŒç»­äº†4ä¸ªæœˆã€‚AuctionNetæä¾›çš„å¹¿å‘Šæ‹å–ç¯å¢ƒã€æ•°æ®é›†å’ŒåŸºçº¿æŠ•æ ‡å†³ç­–åˆ¶å®šç®—æ³•ä¸ºå‚èµ›è€…æä¾›äº†å‡†ç¡®å’Œå…¬å¹³çš„æ€§èƒ½è¯„ä¼°ï¼Œå¹¶æ¿€å‘äº†ä»–ä»¬çš„åˆ›é€ åŠ›ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯å‘å±•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AuctionNetä¸ä»…ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç ”ç©¶å¤§å‹æ‹å–ä¸­è‡ªåŠ¨æŠ•æ ‡ç®—æ³•çš„æœºä¼šï¼Œè¿˜å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…åœ¨åšå¼ˆè®ºã€å¼ºåŒ–å­¦ä¹ ã€ç”Ÿæˆæ¨¡å‹ã€è¿è¥ä¼˜åŒ–ç­‰é¢†åŸŸè§£å†³å¹¿æ³›çš„å†³ç­–åˆ¶å®šç ”ç©¶é—®é¢˜ã€‚æ­¤å¤–ï¼ŒAuctionNetè¿˜å¯ä»¥ç”¨äºç ”ç©¶å…¶ä»–å¤§å‹æ¸¸æˆä¸­çš„å†³ç­–åˆ¶å®šé—®é¢˜ï¼Œä¾‹å¦‚åœ¨çº¿å¹¿å‘Šã€æ¨èç³»ç»Ÿã€èµ„æºåˆ†é…ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>6</th>
      <td>From Code to Play: Benchmarking Program Search for Games Using Large Language Models</td>
      <td>Large language models (LLMs) have shown impressive capabilities in generating<br>program code, opening exciting opportunities for applying program synthesis to<br>games. In this work, we explore the potential of LLMs to directly synthesize<br>usable code for a wide range of gaming applications, focusing on two<br>programming languages, Python and Java. We use an evolutionary hill-climbing<br>algorithm, where the mutations and seeds of the initial programs are controlled<br>by LLMs. For Python, the framework covers various game-related tasks, including<br>five miniature versions of Atari games, ten levels of Baba is You, an<br>environment inspired by Asteroids, and a maze generation task. For Java, the<br>framework contains 12 games from the TAG tabletop games framework. Across 29<br>tasks, we evaluated 12 language models for Python and 8 for Java. Our findings<br>suggest that the performance of LLMs depends more on the task than on model<br>size. While larger models generate more executable programs, these do not<br>always result in higher-quality solutions but are much more expensive. No model<br>has a clear advantage, although on any specific task, one model may be better.<br>Trying many models on a problem and using the best results across them is more<br>reliable than using just one.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ä»ä»£ç åˆ°æ¸¸æˆï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¸¸æˆç¨‹åºæœç´¢çš„åŸºå‡†æµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆç¨‹åºä»£ç æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œå°†ç¨‹åºåˆæˆåº”ç”¨äºæ¸¸æˆé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç¨‹åºåˆæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰æˆ–JSONè½¬æ¢å™¨ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨èŒƒå›´å’Œçµæ´»æ€§ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsç›´æ¥åˆæˆå¯ç”¨äºå„ç§æ¸¸æˆåº”ç”¨çš„ä»£ç çš„æ½œåŠ›ï¼Œå¹¶è¯„ä¼°å…¶åœ¨æ¸¸æˆé¢†åŸŸçš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMsçš„ç¨‹åºæœç´¢æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆé¢†åŸŸåˆæˆç¨‹åºä»£ç çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è¿›åŒ–çˆ¬å±±ç®—æ³•ï¼Œå…¶ä¸­åˆå§‹ç¨‹åºçš„çªå˜å’Œç§å­ç”±LLMsæ§åˆ¶ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨Pythonå’ŒJavaä¸¤ç§ç¼–ç¨‹è¯­è¨€ï¼Œåœ¨29ä¸ªä¸åŒçš„æ¸¸æˆä»»åŠ¡ä¸Šè¯„ä¼°äº†12ä¸ªPythonè¯­è¨€æ¨¡å‹å’Œ8ä¸ªJavaè¯­è¨€æ¨¡å‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ç ”ç©¶å‘ç°ï¼ŒLLMsçš„æ€§èƒ½æ›´å¤šåœ°å–å†³äºä»»åŠ¡æœ¬èº«ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å¤§å°ã€‚è™½ç„¶æ›´å¤§çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´å¤šå¯æ‰§è¡Œçš„ç¨‹åºï¼Œä½†è¿™äº›ç¨‹åºå¹¶ä¸æ€»æ˜¯äº§ç”Ÿæ›´é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”æˆæœ¬æ›´é«˜ã€‚æ²¡æœ‰æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼ŒæŸäº›æ¨¡å‹å¯èƒ½è¡¨ç°æ›´å¥½ã€‚å°è¯•å¤šç§æ¨¡å‹å¹¶ä½¿ç”¨æœ€ä½³ç»“æœå¯ä»¥æé«˜å¯é æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆé¢†åŸŸåˆæˆç¨‹åºä»£ç çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨ä¸”å¯æ‰©å±•çš„å¹³å°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¸¸æˆé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ç”Ÿæˆçš„ä»£ç è´¨é‡ä¸é«˜ã€å¯æ‰§è¡Œæ€§å·®ç­‰ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„æœç´¢ç­–ç•¥å’Œæ›´å¥½çš„æç¤ºå·¥ç¨‹ï¼Œä»¥æé«˜LLMsçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¤šç§æ¨¡å‹å¯ä»¥é™ä½æˆæœ¬å¹¶æé«˜ç»“æœçš„å¯é æ€§ã€‚</td>
    </tr>
    <tr>
      <th>7</th>
      <td>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</td>
      <td>Large Language Models (LLMs) and Vision Language Models (VLMs) possess<br>extensive knowledge and exhibit promising reasoning abilities; however, they<br>still struggle to perform well in complex, dynamic environments. Real-world<br>tasks require handling intricate interactions, advanced spatial reasoning,<br>long-term planning, and continuous exploration of new strategies-areas in which<br>we lack effective methodologies for comprehensively evaluating these<br>capabilities. To address this gap, we introduce BALROG, a novel benchmark<br>designed to assess the agentic capabilities of LLMs and VLMs through a diverse<br>set of challenging games. Our benchmark incorporates a range of existing<br>reinforcement learning environments with varying levels of difficulty,<br>including tasks that are solvable by non-expert humans in seconds to extremely<br>challenging ones that may take years to master (e.g., the NetHack Learning<br>Environment). We devise fine-grained metrics to measure performance and conduct<br>an extensive evaluation of several popular open-source and closed-source LLMs<br>and VLMs. Our findings indicate that while current models achieve partial<br>success in the easier games, they struggle significantly with more challenging<br>tasks. Notably, we observe severe deficiencies in vision-based decision-making,<br>as models perform worse when visual representations of the environments are<br>provided. We release BALROG as an open and user-friendly benchmark to<br>facilitate future research and development in the agentic community.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | BALROGï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆä¸­çš„æ¨ç†èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚ç°å®ä¸–ç•Œä»»åŠ¡éœ€è¦å¤„ç†å¤æ‚çš„äº¤äº’ã€é«˜çº§çš„ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’å’ŒæŒç»­æ¢ç´¢æ–°ç­–ç•¥ï¼Œè€Œè¿™äº›é¢†åŸŸç¼ºä¹æœ‰æ•ˆçš„æ–¹æ³•æ¥å…¨é¢è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BALROGï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°LLMså’ŒVLMsä»£ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œé€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¸¸æˆè¿›è¡Œæµ‹è¯•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†BALROGåŸºå‡†ï¼ŒåŒ…å«å…­ä¸ªå…·æœ‰ä¸åŒéš¾åº¦çº§åˆ«çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œä»ç®€å•åˆ°æå…¶å¤æ‚ï¼Œä¾‹å¦‚NetHackå­¦ä¹ ç¯å¢ƒã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ç»†ç²’åº¦çš„æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ï¼Œå¹¶å¯¹å¤šä¸ªæµè¡Œçš„å¼€æºå’Œé—­æºLLMså’ŒVLMsè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿›è¡Œäº†å®šæ€§åˆ†æï¼Œç ”ç©¶äº†ä»£ç†åœ¨ç©ºé—´æ¨ç†ã€ç³»ç»Ÿæ¢ç´¢å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è¾ƒç®€å•çš„æ¸¸æˆä¸­å–å¾—äº†éƒ¨åˆ†æˆåŠŸï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡å‹åœ¨åŸºäºè§†è§‰çš„å†³ç­–æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œå½“æä¾›ç¯å¢ƒçš„è§†è§‰è¡¨ç¤ºæ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°æ›´å·®ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„BALROGåŸºå‡†ä¸ºè¯„ä¼°LLMså’ŒVLMsçš„ä»£ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†å½“å‰æ¨¡å‹åœ¨è§†è§‰å†³ç­–å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>8</th>
      <td>TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs</td>
      <td>The rapid advancement of large language models (LLMs) has accelerated their<br>application in reasoning, with strategic reasoning drawing increasing<br>attention. To evaluate LLMs' strategic reasoning capabilities, game theory,<br>with its concise structure, has become a preferred approach. However, current<br>research focuses on a limited selection of games, resulting in low coverage.<br>Classic game scenarios risk data leakage, and existing benchmarks often lack<br>extensibility, making them inadequate for evaluating state-of-the-art models.<br>To address these challenges, we propose TMGBench, a benchmark with<br>comprehensive game type coverage, novel scenarios, and flexible organization.<br>Specifically, we incorporate all 144 game types summarized by the<br>Robinson-Goforth topology of 2x2 games, constructed as classic games. We also<br>employ synthetic data generation to create diverse, higher-quality scenarios<br>through topic guidance and human inspection, referred to as story-based games.<br>Lastly, we provide a sustainable framework for increasingly powerful LLMs by<br>treating these games as atomic units and organizing them into more complex<br>forms via sequential, parallel, and nested structures. Our comprehensive<br>evaluation of mainstream LLMs covers tests on rational reasoning, robustness,<br>Theory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in<br>accuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,<br>OpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and<br>70.0% on sequential, parallel, and nested games, highlighting TMGBench's<br>challenges.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | TMGBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿæ¸¸æˆåŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶ä¸­æˆ˜ç•¥æ¨ç†èƒ½åŠ›å°¤å…¶å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¾€å¾€å±€é™äºå°‘æ•°ç»å…¸æ¸¸æˆï¼Œå¯¼è‡´æ¸¸æˆç±»å‹çš„è¦†ç›–ç‡ä½ï¼Œä¸”ç»å…¸æ¸¸æˆåœºæ™¯å­˜åœ¨æ•°æ®æ³„éœ²çš„é£é™©ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œéš¾ä»¥è¯„ä¼°æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TMGBenchï¼Œä¸€ä¸ªå…·æœ‰å…¨é¢æ¸¸æˆç±»å‹è¦†ç›–ç‡ã€æ–°é¢–åœºæ™¯å’Œçµæ´»ç»„ç»‡æ–¹å¼çš„åŸºå‡†æµ‹è¯•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢çš„æ¸¸æˆç±»å‹è¦†ç›–ç‡<br>TMGBenchåŒ…å«äº†ç”±Robinson-Goforthæ‹“æ‰‘ç»“æ„æ€»ç»“çš„144ç§2x2æ¸¸æˆç±»å‹ï¼Œæ¶µç›–äº†å„ç§ä¸åŒçš„æ¸¸æˆç»“æ„ï¼ŒåŒ…æ‹¬ç»å…¸æ¸¸æˆå¦‚å›šå¾’å›°å¢ƒç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–°é¢–çš„åœºæ™¯<br>ä¸ºäº†è§£å†³ç»å…¸æ¸¸æˆåœºæ™¯çš„æ•°æ®æ³„éœ²é—®é¢˜ï¼ŒTMGBenché‡‡ç”¨äº†åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºæ¯ç§ç»å…¸æ¸¸æˆåˆ›å»ºäº†äº”ä¸ªä¸åŒçš„åŸºäºæ•…äº‹çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯æ¶µç›–äº†å•†ä¸šã€æ³•å¾‹ã€äº¤é€šç­‰ç°å®ç”Ÿæ´»ä¸­çš„ä¸»é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šçµæ´»çš„æ¸¸æˆç»„ç»‡æ–¹å¼<br>TMGBenchå°†æ¸¸æˆè§†ä¸ºåŸå­å•ä½ï¼Œå¹¶é€šè¿‡é¡ºåºã€å¹¶è¡Œå’ŒåµŒå¥—ç»“æ„å°†å®ƒä»¬ç»„ç»‡æˆæ›´å¤æ‚çš„æ¸¸æˆå½¢å¼ï¼Œä»¥è¯„ä¼°LLMsåœ¨å¹¶è¡Œã€é¡ºåºå’Œå¤šå±‚çº§å†³ç­–æ–¹é¢çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹ä¸»æµLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ç†æ€§æ¨ç†ã€æ¨ç†é²æ£’æ€§ã€å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰èƒ½åŠ›å’Œå¤æ‚æ¸¸æˆå½¢å¼çš„æ¨ç†ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æˆ˜ç•¥æ¨ç†è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä»å­˜åœ¨ç¼ºé™·ï¼Œä¸”å¯¹ToMçš„æŒæ¡ç¨‹åº¦ä¹Ÿå„ä¸ç›¸åŒã€‚OpenAIçš„æœ€æ–°æ¨ç†æ¨¡å‹o1-miniåœ¨é¡ºåºã€å¹¶è¡Œå’ŒåµŒå¥—æ¸¸æˆä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º66.6%ã€60.0%å’Œ70.0%ï¼Œçªæ˜¾äº†TMGBenchçš„æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>TMGBenchä¸ºè¯„ä¼°LLMsçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬å…¨é¢çš„æ¸¸æˆç±»å‹è¦†ç›–ç‡ã€æ–°é¢–çš„åœºæ™¯å’Œçµæ´»çš„æ¸¸æˆç»„ç»‡æ–¹å¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†LLMsåœ¨æˆ˜ç•¥æ¨ç†æ–¹é¢çš„ç¼ºé™·ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ–¹å‘ï¼Œä¸ºLLMsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Game4Loc: A UAV Geo-Localization Benchmark from Game Data</td>
      <td>The vision-based geo-localization technology for UAV, serving as a secondary<br>source of GPS information in addition to the global navigation satellite<br>systems (GNSS), can still operate independently in the GPS-denied environment.<br>Recent deep learning based methods attribute this as the task of image matching<br>and retrieval. By retrieving drone-view images in geo-tagged satellite image<br>database, approximate localization information can be obtained. However, due to<br>high costs and privacy concerns, it is usually difficult to obtain large<br>quantities of drone-view images from a continuous area. Existing drone-view<br>datasets are mostly composed of small-scale aerial photography with a strong<br>assumption that there exists a perfect one-to-one aligned reference image for<br>any query, leaving a significant gap from the practical localization scenario.<br>In this work, we construct a large-range contiguous area UAV geo-localization<br>dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,<br>and targets using modern computer games. Based on this dataset, we introduce a<br>more practical UAV geo-localization task including partial matches of<br>cross-view paired data, and expand the image-level retrieval to the actual<br>localization in terms of distance (meters). For the construction of drone-view<br>and satellite-view pairs, we adopt a weight-based contrastive learning<br>approach, which allows for effective learning while avoiding additional<br>post-processing matching steps. Experiments demonstrate the effectiveness of<br>our data and training method for UAV geo-localization, as well as the<br>generalization capabilities to real-world scenarios.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Game4Locï¼šåŸºäºæ¸¸æˆæ•°æ®çš„æ— äººæœºåœ°ç†å®šä½åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ— äººæœºè§†è§‰åœ°ç†å®šä½æŠ€æœ¯ä½œä¸ºå…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰çš„è¾…åŠ©æ‰‹æ®µï¼Œåœ¨GPSä¿¡å·ä¸å¯ç”¨çš„ç¯å¢ƒä¸‹ä»èƒ½ç‹¬ç«‹å·¥ä½œã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å°†è¿™ä¸€ä»»åŠ¡è§†ä¸ºå›¾åƒåŒ¹é…å’Œæ£€ç´¢é—®é¢˜ã€‚é€šè¿‡åœ¨å¸¦æœ‰åœ°ç†æ ‡ç­¾çš„å«æ˜Ÿå›¾åƒæ•°æ®åº“ä¸­æ£€ç´¢æ— äººæœºè§†è§’å›¾åƒï¼Œå¯ä»¥è·å¾—è¿‘ä¼¼çš„å®šä½ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç”±äºæˆæœ¬é«˜æ˜‚å’Œéšç§é—®é¢˜ï¼Œé€šå¸¸éš¾ä»¥ä»è¿ç»­åŒºåŸŸè·å¾—å¤§é‡æ— äººæœºè§†è§’å›¾åƒã€‚ç°æœ‰çš„æ— äººæœºè§†è§’æ•°æ®é›†å¤§å¤šç”±å°è§„æ¨¡èˆªç©ºæ‘„å½±ç»„æˆï¼Œå¹¶å‡è®¾å¯¹äºä»»ä½•æŸ¥è¯¢éƒ½å­˜åœ¨ä¸€ä¸ªå®Œç¾çš„ä¸€å¯¹ä¸€åŒ¹é…çš„å‚è€ƒå›¾åƒï¼Œè¿™ä¸å®é™…å®šä½åœºæ™¯å­˜åœ¨æ˜¾è‘—å·®è·ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†ä¸€ä¸ªåä¸ºGTA-UAVçš„å¤§å‹è¿ç»­åŒºåŸŸæ— äººæœºåœ°ç†å®šä½æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å¤šä¸ªé£è¡Œé«˜åº¦ã€å§¿æ€ã€åœºæ™¯å’Œç›®æ ‡ï¼Œä½¿ç”¨ç°ä»£è®¡ç®—æœºæ¸¸æˆç”Ÿæˆã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†ä¸€ä¸ªæ›´å®é™…çš„æ— äººæœºåœ°ç†å®šä½ä»»åŠ¡ï¼ŒåŒ…æ‹¬è·¨è§†å›¾é…å¯¹æ•°æ®çš„éƒ¨åˆ†åŒ¹é…ï¼Œå¹¶å°†å›¾åƒçº§æ£€ç´¢æ‰©å±•åˆ°å®é™…å®šä½ï¼ˆä»¥ç±³ä¸ºå•ä½ï¼‰ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé‡‡ç”¨åŸºäºæƒé‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ„å»ºæ— äººæœºè§†è§’å’Œå«æ˜Ÿè§†è§’é…å¯¹ï¼Œå…è®¸æœ‰æ•ˆå­¦ä¹ ï¼ŒåŒæ—¶é¿å…é¢å¤–çš„åå¤„ç†åŒ¹é…æ­¥éª¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®å’Œæ–¹æ³•å¯¹æ— äººæœºåœ°ç†å®šä½æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”å…·æœ‰å°†æ¨¡å‹æ¨å¹¿åˆ°ç°å®ä¸–ç•Œåœºæ™¯çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GTA-UAVæ•°æ®é›†å’ŒåŸºäºæƒé‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä¸ºæ— äººæœºåœ°ç†å®šä½ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜æ— äººæœºåœ¨GPSä¿¡å·ä¸å¯ç”¨ç¯å¢ƒä¸‹çš„å®šä½ç²¾åº¦å’Œé²æ£’æ€§ã€‚</td>
    </tr>
    <tr>
      <th>10</th>
      <td>ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation</td>
      <td>This paper introduces ES-KT-24, a novel multimodal Knowledge Tracing (KT)<br>dataset for intelligent tutoring systems in educational game contexts. Although<br>KT is crucial in adaptive learning, existing datasets often lack game-based and<br>multimodal elements. ES-KT-24 addresses these limitations by incorporating<br>educational game-playing videos, synthetically generated question text, and<br>detailed game logs. The dataset covers Mathematics, English, Indonesian, and<br>Malaysian subjects, emphasizing diversity and including non-English content.<br>The synthetic text component, generated using a large language model,<br>encompasses 28 distinct knowledge concepts and 182 questions, featuring 15,032<br>users and 7,782,928 interactions. Our benchmark experiments demonstrate the<br>dataset's utility for KT research by comparing Deep learning-based KT models<br>with Language Model-based Knowledge Tracing (LKT) approaches. Notably, LKT<br>models showed slightly higher performance than traditional DKT models,<br>highlighting the potential of language model-based approaches in this field.<br>Furthermore, ES-KT-24 has the potential to significantly advance research in<br>multimodal KT models and learning analytics. By integrating game-playing videos<br>and detailed game logs, this dataset offers a unique approach to dissecting<br>student learning patterns through advanced data analysis and machine-learning<br>techniques. It has the potential to unearth new insights into the learning<br>process and inspire further exploration in the field.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ES-KT-24ï¼šåŸºäºæ•™è‚²æ¸¸æˆè§†é¢‘å’Œåˆæˆæ–‡æœ¬ç”Ÿæˆçš„å¤šæ¨¡æ€çŸ¥è¯†è¿½è¸ªåŸºå‡†æ•°æ®é›†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰æ˜¯è‡ªé€‚åº”å­¦ä¹ ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®å­¦ç”Ÿä¸å­¦ä¹ ææ–™çš„äº’åŠ¨æ¥å»ºæ¨¡ä»–ä»¬çš„çŸ¥è¯†çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„KTæ•°æ®é›†å¾€å¾€ç¼ºä¹åŸºäºæ¸¸æˆå’Œå¤šæ¨¡æ€å…ƒç´ ï¼Œé™åˆ¶äº†å…¶åœ¨æ¸¸æˆåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ES-KT-24ï¼Œä¸€ä¸ªåŒ…å«æ•™è‚²æ¸¸æˆè§†é¢‘ã€åˆæˆæ–‡æœ¬å’Œè¯¦ç»†æ¸¸æˆæ—¥å¿—çš„å¤šæ¨¡æ€KTæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€æ•°æ®é›†<br>ES-KT-24æ•°æ®é›†åŒ…å«äº†æ•™è‚²æ¸¸æˆè§†é¢‘ã€åˆæˆæ–‡æœ¬å’Œè¯¦ç»†æ¸¸æˆæ—¥å¿—ï¼Œæ¶µç›–äº†æ•°å­¦ã€è‹±è¯­ã€å°åº¦å°¼è¥¿äºšè¯­å’Œé©¬æ¥è¥¿äºšè¯­ç­‰ç§‘ç›®ï¼Œå¼ºè°ƒäº†å¤šæ ·æ€§å’Œéè‹±è¯­å†…å®¹ã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´å…¨é¢åœ°åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æ¨¡å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆæˆæ–‡æœ¬ç”Ÿæˆ<br>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæ–‡æœ¬ï¼Œæ¶µç›–äº†28ä¸ªä¸åŒçš„çŸ¥è¯†æ¦‚å¿µå’Œ182ä¸ªé—®é¢˜ï¼Œå…±æœ‰15,032åç”¨æˆ·å’Œ7,782,928æ¬¡äº’åŠ¨ã€‚è¿™ä¸ºKTç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ–‡æœ¬æ•°æ®ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ES-KT-24æ•°æ®é›†ä¸Šè¿›è¡Œçš„åŸºå‡†å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¯­è¨€æ¨¡å‹çš„KTï¼ˆLKTï¼‰æ–¹æ³•åœ¨æ€§èƒ½ä¸Šç•¥ä¼˜äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ KTï¼ˆDKTï¼‰æ¨¡å‹ã€‚è¿™çªå‡ºäº†è¯­è¨€æ¨¡å‹åœ¨KTé¢†åŸŸçš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>ES-KT-24æ•°æ®é›†ä¸ºKTç ”ç©¶æä¾›äº†æ–°çš„åŸºå‡†ï¼Œå¹¶å±•ç¤ºäº†å¤šæ¨¡æ€æ•°æ®åœ¨æ¸¸æˆåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„ä»·å€¼ã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¯¥æ•°æ®é›†æ¥æ¢ç´¢æ¸¸æˆåŒ–å­¦ä¹ ç°è±¡ã€åˆ†æå¤šæ¨¡æ€å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å¼€å‘æ›´å¤æ‚çš„æ¨¡å‹æ¥æ•æ‰æ•™è‚²ç¯å¢ƒä¸­çš„å¤šæ ·æ¨¡æ€ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜æ”¯æŒæ¸¸æˆéš¾åº¦åˆ†ç±»ã€ç‰¹å¾å½±å“åˆ†æã€ç”Ÿæˆæ¸¸æˆè®¾è®¡ã€å¤šæ¨¡æ€å­¦ä¹ åˆ†æã€å‚ä¸åº¦å’Œç»©æ•ˆç›¸å…³æ€§ã€è·¨æ–‡åŒ–å­¦ä¹ æ¨¡å¼ä»¥åŠæ—¶é—´å­¦ä¹ åŠ¨æ€ç­‰æ–¹é¢çš„ç ”ç©¶ã€‚</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games</td>
      <td>Recent advancements in large language models (LLMs) have expanded their<br>capabilities beyond traditional text-based tasks to multimodal domains,<br>integrating visual, auditory, and textual data. While multimodal LLMs have been<br>extensively explored for high-level planning in domains like robotics and<br>games, their potential as low-level controllers remains largely untapped. In<br>this paper, we introduce a novel benchmark aimed at testing the emergent<br>capabilities of multimodal LLMs as low-level policies in Atari games. Unlike<br>traditional reinforcement learning (RL) methods that require training for each<br>new environment and reward function specification, these LLMs utilize<br>pre-existing multimodal knowledge to directly engage with game environments.<br>Our study assesses the performances of multiple multimodal LLMs against<br>traditional RL agents, human players, and random agents, focusing on their<br>ability to understand and interact with complex visual scenes and formulate<br>strategic responses. Our results show that these multimodal LLMs are not yet<br>capable of being zero-shot low-level policies. Furthermore, we see that this<br>is, in part, due to their visual and spatial reasoning. Additional results and<br>videos are available on our project webpage:<br>https://dev1nw.github.io/atari-gpt/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Atari-GPTï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„çªç ´ï¼Œå®ƒä»¬çš„èƒ½åŠ›å·²ç»æ‰©å±•åˆ°äº†å¤šæ¨¡æ€é¢†åŸŸï¼Œæ•´åˆäº†è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®ã€‚å°½ç®¡å¤šæ¨¡æ€LLMsåœ¨æœºå™¨äººæŠ€æœ¯å’Œæ¸¸æˆç­‰é¢†åŸŸçš„å†³ç­–è§„åˆ’æ–¹é¢å¾—åˆ°äº†å¹¿æ³›æ¢ç´¢ï¼Œä½†å®ƒä»¬ä½œä¸ºä½çº§æ§åˆ¶å™¨çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æŒ–æ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæµ‹è¯•å¤šæ¨¡æ€LLMsåœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAtari-GPTåŸºå‡†<br>æœ¬æ–‡æå‡ºäº†Atari-GPTåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€LLMsåœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡æ¯”è¾ƒLLMsä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ã€äººç±»ç©å®¶å’Œéšæœºä»£ç†çš„æ€§èƒ½ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç†è§£å¤æ‚è§†è§‰åœºæ™¯å’Œåˆ¶å®šæˆ˜ç•¥ååº”æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°è§†è§‰å’Œç©ºé—´æ¨ç†èƒ½åŠ›<br>é™¤äº†æ¸¸æˆæ€§èƒ½è¯„ä¼°ï¼Œæœ¬æ–‡è¿˜é€šè¿‡ä¸€ç³»åˆ—æç¤ºæµ‹è¯•äº†LLMsçš„è§†è§‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œæˆ˜ç•¥ç›´è§‰èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯•æ—¨åœ¨æ­ç¤ºLLMsåœ¨ç†è§£æ¸¸æˆç¯å¢ƒæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ”¹è¿›æä¾›æ–¹å‘ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨Atariæ¸¸æˆä¸­çš„è¡¨ç°ä¸å¦‚äººç±»ç©å®¶æˆ–ä¸“é—¨çš„RLæ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶èƒ½å¤Ÿè¯†åˆ«å’Œä¸æ¸¸æˆå¸§ä¸­çš„å…³é”®å…ƒç´ è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½œä¸ºä½çº§æ§åˆ¶å™¨çš„æ€§èƒ½ä»ç„¶ä¸ä½³ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºç¼ºä¹é’ˆå¯¹æ­¤ä»»åŠ¡çš„è®­ç»ƒä»¥åŠç©ºé—´æ¨ç†çš„å›°éš¾ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼ŒLLMsçš„æ¨ç†æ—¶é—´å¯¹äºå®æ—¶å†³ç­–è‡³å…³é‡è¦ï¼Œè€Œç›®å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨é€Ÿåº¦æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºLLMsåœ¨ä½çº§æ§åˆ¶ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†å®è´µçš„è§è§£ã€‚å°½ç®¡LLMså°šæœªè¾¾åˆ°ä¸äººç±»æˆ–RLæ¨¡å‹ç›¸åŒ¹é…çš„æ°´å¹³ï¼Œä½†å®ƒä»¬åœ¨Atariæ¸¸æˆä¸­çš„è¡¨ç°ä»ç„¶å€¼å¾—æ³¨æ„ã€‚è¿™è¡¨æ˜LLMså…·æœ‰é€‚åº”æ€§å’Œæ½œåŠ›ï¼Œå¯ä»¥æ‰©å±•åˆ°å…¶åŸå§‹è®­ç»ƒèŒƒå›´ä¹‹å¤–ï¼Œä¸ºæœªæ¥åœ¨éœ€è¦è¯¦ç»†ç¯å¢ƒäº¤äº’å’Œå†³ç­–çš„åº”ç”¨ä¸­ä½œä¸ºæ›´é€šç”¨çš„ä½çº§æ§åˆ¶å™¨æä¾›äº†ä¸€ç¥ã€‚</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</td>
      <td>The advancement of Offline Reinforcement Learning (RL) and Offline<br>Multi-Agent Reinforcement Learning (MARL) critically depends on the<br>availability of high-quality, pre-collected offline datasets that represent<br>real-world complexities and practical applications. However, existing datasets<br>often fall short in their simplicity and lack of realism. To address this gap,<br>we propose Hokoff, a comprehensive set of pre-collected datasets that covers<br>both offline RL and offline MARL, accompanied by a robust framework, to<br>facilitate further research. This data is derived from Honor of Kings, a<br>recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate<br>nature, closely resembling real-life situations. Utilizing this framework, we<br>benchmark a variety of offline RL and offline MARL algorithms. We also<br>introduce a novel baseline algorithm tailored for the inherent hierarchical<br>action space of the game. We reveal the incompetency of current offline RL<br>approaches in handling task complexity, generalization and multi-task learning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Hokoffï¼šåŸºäºç‹è€…è£è€€çš„çœŸå®æ¸¸æˆæ•°æ®é›†åŠå…¶ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰å’Œç¦»çº¿å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆOffline MARLï¼‰çš„ç ”ç©¶è¿›å±•ä¾èµ–äºé«˜è´¨é‡ã€é¢„å…ˆæ”¶é›†çš„ç¦»çº¿æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åº”ä»£è¡¨ç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå®é™…åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†å¾€å¾€è¿‡äºç®€å•ï¼Œç¼ºä¹ç°å®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Hokoffï¼Œè¿™æ˜¯ä¸€å¥—å…¨é¢çš„é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†ç¦»çº¿RLå’Œç¦»çº¿MARLï¼Œå¹¶ä¼´éšç€ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚è¿™äº›æ•°æ®æ¥è‡ªç‹è€…è£è€€ï¼Œè¿™æ˜¯ä¸€æ¬¾ä»¥å…¶å¤æ‚æ€§è´¨è€Œé—»åçš„å¤šäººåœ¨çº¿æˆ˜æ–—ç«æŠ€åœºï¼ˆMOBAï¼‰æ¸¸æˆï¼Œä¸ç°å®ç”Ÿæ´»æƒ…å†µéå¸¸ç›¸ä¼¼ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç‹è€…è£è€€çš„å¤æ‚ç¯å¢ƒ<br>Hokoffçš„æ•°æ®é›†æ¥æºäºç‹è€…è£è€€ï¼Œè¿™æ˜¯ä¸€æ¬¾æ‹¥æœ‰è¶…è¿‡1äº¿æ—¥æ´»è·ƒç©å®¶çš„å…¨çƒæœ€å—æ¬¢è¿çš„MOBAæ¸¸æˆä¹‹ä¸€ã€‚è¯¥æ¸¸æˆçš„å¤æ‚æ€§è¿œè¿œè¶…è¿‡å…¶ä»–æ•°æ®é›†ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯çš„æ½œåŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æºã€æ˜“ç”¨çš„æ¡†æ¶<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€æºã€æ˜“ç”¨çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ç¦»çº¿RLï¼ˆé‡‡æ ·ã€è®­ç»ƒå’Œè¯„ä¼°ï¼‰çš„å…¨é¢æµç¨‹å’Œä¸€äº›æœ‰ç”¨çš„å·¥å…·ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸°å¯Œå¤šæ ·çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ä½¿ç”¨ä¸€ç³»åˆ—å…·æœ‰ä¸åŒè®¾è®¡å› ç´ çš„å‰è®­ç»ƒæ¨¡å‹ç”Ÿæˆï¼Œä¸ä»…é€‚ç”¨äºç¦»çº¿RLï¼Œä¹Ÿé€‚ç”¨äºç¦»çº¿MARLã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šçº§æ¨¡å‹<br>ä¸ºäº†ç¡®ä¿ä¸åŒç®—æ³•çš„æ€§èƒ½æ¯”è¾ƒçš„æœ‰æ•ˆæ€§å’Œå…¬æ­£æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å¤šçº§æ¨¡å‹ï¼ŒåŒ…å«å¤šä¸ªå…·æœ‰ä¸åŒæ°´å¹³çš„æ£€æŸ¥ç‚¹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºé‡‡æ ·å’Œè¯„ä¼°ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ ·åŒ–çš„æ•°æ®é›†<br>Hokoffæä¾›äº†å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¤šéš¾åº¦ã€å¤šä»»åŠ¡ã€æ³›åŒ–ã€å¼‚æ„é˜Ÿå‹å’Œå­ä»»åŠ¡ç­‰ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä¸ºç¦»çº¿RLå’Œç¦»çº¿MARLçš„ç ”ç©¶æä¾›æ›´çœŸå®çš„ç¯å¢ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Hokoffæ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šç§ç¦»çº¿RLå’Œç¦»çº¿MARLç®—æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ç‹è€…è£è€€å›ºæœ‰å±‚æ¬¡ç»“æ„åŠ¨ä½œç©ºé—´çš„æ–°çš„åŸºçº¿ç®—æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ç¦»çº¿RLæ–¹æ³•åœ¨å¤„ç†ä»»åŠ¡å¤æ‚æ€§ã€æ³›åŒ–å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Hokoffæ•°æ®é›†å’Œæ¡†æ¶ä¸ºç¦»çº¿RLå’Œç¦»çº¿MARLçš„ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚å…¶å¤šæ ·åŒ–çš„æ•°æ®é›†å’Œå¼ºå¤§çš„æ¡†æ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè¯„ä¼°ç¦»çº¿å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¸ºè§£å†³ç°å®ä¸–ç•Œé—®é¢˜æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>13</th>
      <td>A Benchmark Environment for Offline Reinforcement Learning in Racing Games</td>
      <td>Offline Reinforcement Learning (ORL) is a promising approach to reduce the<br>high sample complexity of traditional Reinforcement Learning (RL) by<br>eliminating the need for continuous environmental interactions. ORL exploits a<br>dataset of pre-collected transitions and thus expands the range of application<br>of RL to tasks in which the excessive environment queries increase training<br>time and decrease efficiency, such as in modern AAA games. This paper<br>introduces OfflineMania a novel environment for ORL research. It is inspired by<br>the iconic TrackMania series and developed using the Unity 3D game engine. The<br>environment simulates a single-agent racing game in which the objective is to<br>complete the track through optimal navigation. We provide a variety of datasets<br>to assess ORL performance. These datasets, created from policies of varying<br>ability and in different sizes, aim to offer a challenging testbed for<br>algorithm development and evaluation. We further establish a set of baselines<br>for a range of Online RL, ORL, and hybrid Offline to Online RL approaches using<br>our environment.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨èµ›è½¦æ¸¸æˆä¸­çš„æ–°åŸºå‡†ç¯å¢ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¸¸æˆè¡Œä¸šä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ä¼ ç»ŸRLçš„é«˜æ ·æœ¬å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå¦‚ç°ä»£AAAæ¸¸æˆã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆORLï¼‰é€šè¿‡åˆ©ç”¨é¢„å…ˆæ”¶é›†çš„è¿‡æ¸¡æ•°æ®é›†æ¥å‡å°‘å¯¹ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ï¼Œä»è€Œé™ä½äº†æ ·æœ¬å¤æ‚åº¦ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹æ¸¸æˆç¯å¢ƒï¼Œç‰¹åˆ«æ˜¯èµ›è½¦æ¸¸æˆï¼Œçš„ORLæ•°æ®é›†å’ŒåŸºå‡†ç¯å¢ƒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥OfflineManiaç¯å¢ƒ<br>OfflineManiaæ˜¯ä¸€ä¸ªåŸºäºUnity 3Dæ¸¸æˆå¼•æ“å¼€å‘çš„å•ä»£ç†èµ›è½¦æ¸¸æˆç¯å¢ƒï¼Œçµæ„Ÿæ¥æºäºTrackManiaç³»åˆ—æ¸¸æˆã€‚è¯¥ç¯å¢ƒæ¨¡æ‹Ÿäº†èµ›è½¦æ¸¸æˆï¼Œç›®æ ‡æ˜¯é€šè¿‡æœ€ä¼˜å¯¼èˆªå®Œæˆèµ›é“ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæä¾›å¤šæ ·åŒ–çš„æ•°æ®é›†<br>ä¸ºäº†è¯„ä¼°ORLçš„æ€§èƒ½ï¼Œè®ºæ–‡æä¾›äº†å¤šç§æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ç”±ä¸åŒèƒ½åŠ›æ°´å¹³çš„ç­–ç•¥ç”Ÿæˆï¼Œå¹¶å…·æœ‰ä¸åŒçš„è§„æ¨¡ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨ä¸ºç®—æ³•å¼€å‘å’Œè¯„ä¼°æä¾›ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå»ºç«‹åŸºçº¿<br>è®ºæ–‡ä½¿ç”¨OfflineManiaç¯å¢ƒï¼Œä¸ºä¸€ç³»åˆ—åœ¨çº¿RLã€ORLå’Œæ··åˆç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•å»ºç«‹äº†åŸºçº¿ã€‚è¿™äº›åŸºçº¿æœ‰åŠ©äºè¯„ä¼°ä¸åŒç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¿ƒè¿›ORLåœ¨æ¸¸æˆç¯å¢ƒä¸­çš„åº”ç”¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒIQLåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºTD3BCå’ŒCQLã€‚æ­¤å¤–ï¼ŒSDBGå’ŒJSRLåœ¨ç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜ç¦»çº¿è®­ç»ƒçš„æ”¿ç­–çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OfflineManiaç¯å¢ƒå’Œæä¾›çš„æ•°æ®é›†ä¸ºORLç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è®ºæ–‡ä¸­çš„åŸºçº¿ç»“æœä¸ºè¯„ä¼°ä¸åŒORLç®—æ³•çš„æ€§èƒ½æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†ç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•åœ¨æ¸¸æˆç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œä¸ºæ¸¸æˆå¼€å‘äººå‘˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</td>
      <td>We introduce a novel and extensible benchmark for large language models<br>(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.<br>The open-source game simulation code, available on GitHub, allows LLMs to<br>compete and generates detailed data files in JSON, CSV, TXT, and PNG formats<br>for leaderboard rankings and further analysis. We present the results of games<br>among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by<br>Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and<br>GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of<br>results from other LLMs. In total, we simulated 2,310 matches (5 sessions for<br>each pair among 7 LLMs and a random player) across three types of games, using<br>three distinct prompt types: list, illustration, and image. The results<br>revealed significant variations in LLM performance across different games and<br>prompt types, with analysis covering win and disqualification rates, missed<br>opportunity analysis, and invalid move analysis. The details of the leaderboard<br>and result matrix data are available as open-access data on GitHub. This study<br>enhances our understanding of LLMs' capabilities in playing games they were not<br>specifically trained for, helping to assess their rule comprehension and<br>strategic thinking. On the path to Artificial General Intelligence (AGI), this<br>study lays the groundwork for future exploration into their utility in complex<br>decision-making scenarios, illuminating their strategic thinking abilities and<br>offering directions for further inquiry into the limits of LLMs within<br>game-based frameworks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡åŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼šä¸€ä¸ªå¯æ‰©å±•çš„LLMåŸºå‡†å’Œæ’è¡Œæ¦œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å’Œæ€§èƒ½å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ç­‰ï¼Œå·²ç»æ— æ³•å…¨é¢è¯„ä¼°LLMsçš„å¤æ‚èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦æ–°çš„åŸºå‡†æ¥è¯„ä¼°LLMsåœ¨è§„åˆ™ç†è§£ã€æˆ˜ç•¥æ€è€ƒå’Œå†³ç­–åˆ¶å®šç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å¯æ‰©å±•çš„åŸºå‡†ï¼Œé€šè¿‡ç½‘æ ¼æ¸¸æˆï¼ˆå¦‚äº•å­—æ£‹ã€å››å­æ£‹å’Œäº”å­æ£‹ï¼‰æ¥è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚è¿™äº›æ¸¸æˆéœ€è¦æ¨¡å‹ç†è§£è§„åˆ™ã€åˆ¶å®šç­–ç•¥å¹¶åšå‡ºå†³ç­–ï¼Œä»è€Œå…¨é¢è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æ”¾æºä»£ç å’Œæ’è¡Œæ¦œ<br>æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå¼€æºçš„æ¸¸æˆæ¨¡æ‹Ÿä»£ç ï¼Œå…è®¸LLMsè¿›è¡Œç«èµ›ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„JSONã€CSVã€TXTå’ŒPNGæ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼Œç”¨äºæ’è¡Œæ¦œæ’åå’Œè¿›ä¸€æ­¥åˆ†æã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªæ’è¡Œæ¦œï¼Œå±•ç¤ºäº†ä¸åŒLLMsåœ¨ä¸åŒæ¸¸æˆå’Œæç¤ºç±»å‹ä¸‹çš„è¡¨ç°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹ä¸ƒä¸ªé¢†å…ˆçš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬Anthropicçš„Claude 3.5 Sonnetå’ŒClaude 3 Sonnetã€Googleçš„Gemini 1.5 Proå’ŒGemini 1.5 Flashã€OpenAIçš„GPT-4 Turboå’ŒGPT-4oä»¥åŠMetaçš„Llama3-70Bã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ä¸åŒæ¸¸æˆå’Œæç¤ºç±»å‹ä¸‹çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¾‹å¦‚ï¼ŒClaude 3.5 Sonnetåœ¨äº•å­—æ£‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨äº”å­æ£‹ä¸­è¡¨ç°è¾ƒå·®ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨å¤„ç†å¤æ‚å’ŒåŸºäºè§†è§‰çš„æç¤ºæ ¼å¼æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›åŸºå‡†ä¸ºè¯„ä¼°LLMsçš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç å’Œæ’è¡Œæ¦œï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†æå’Œæ¯”è¾ƒã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºç†è§£LLMsçš„èƒ½åŠ›å’Œå±€é™æ€§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Simple Stochastic Stopping Games: A Generator and Benchmark Library</td>
      <td>Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the<br>simplest version of Stochastic Games for which there is no known<br>polynomial-time algorithm. Condon showed that Stochastic Games are<br>polynomial-time reducible to SSGs, which in turn are polynomial-time reducible<br>to Stopping Games. SSGs are games where all decisions are binary and every move<br>has a random outcome with a known probability distribution. Stopping Games are<br>SSGs that are guaranteed to terminate. There are many algorithms for SSGs, most<br>of which are fast in practice, but they all lack theoretical guarantees for<br>polynomial-time convergence. The pursuit of a polynomial-time algorithm for<br>SSGs is an active area of research. This paper is intended to support such<br>research by making it easier to study the graphical structure of SSGs. Our<br>contributions are: (1) a generating algorithm for Stopping Games, (2) a proof<br>that the algorithm can generate any game, (3) a list of additional<br>polynomial-time reductions that can be made to Stopping Games, (4) an open<br>source generator for generating fully reduced instances of Stopping Games that<br>comes with instructions and is fully documented, (5) a benchmark set of such<br>instances, (6) and an analysis of how two main algorithm types perform on our<br>benchmark set.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç®€å•éšæœºåœæ­¢æ¸¸æˆçš„ç”Ÿæˆç®—æ³•ä¸åŸºå‡†åº“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç®€å•éšæœºæ¸¸æˆï¼ˆSSGsï¼‰æ˜¯éšæœºæ¸¸æˆä¸­å·²çŸ¥æ²¡æœ‰å¤šé¡¹å¼æ—¶é—´ç®—æ³•çš„æœ€ç®€å•ç‰ˆæœ¬ã€‚å°½ç®¡å­˜åœ¨è®¸å¤šåœ¨å®è·µä¸­å¿«é€Ÿçš„ç®—æ³•ï¼Œä½†å®ƒä»¬éƒ½ç¼ºä¹å¤šé¡¹å¼æ—¶é—´æ”¶æ•›çš„ç†è®ºä¿è¯ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æä¾›ä¸€ä¸ªç”Ÿæˆç®—æ³•å’ŒåŸºå‡†åº“æ¥æ”¯æŒå¯¹SSGsçš„ç ”ç©¶ï¼Œä½¿å…¶æ›´å®¹æ˜“ç ”ç©¶SSGsçš„å›¾å½¢ç»“æ„ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆç®—æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”Ÿæˆåœæ­¢æ¸¸æˆå®ä¾‹çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ç”Ÿæˆä»»ä½•åœæ­¢æ¸¸æˆï¼Œå¹¶å…·æœ‰å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé¡¹å¼æ—¶é—´ç®€åŒ–<br>æœ¬æ–‡è¿˜æä¾›äº†ä¸€ç³»åˆ—å¤šé¡¹å¼æ—¶é—´ç®€åŒ–ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–SSGsï¼Œä½¿å…¶æ›´å®¹æ˜“ç ”ç©¶ã€‚è¿™äº›ç®€åŒ–åŒ…æ‹¬ç§»é™¤å¯å¿«é€Ÿè§£å†³çš„å­å›¾ã€åˆå¹¶å…·æœ‰ç›¸åŒå€¼çš„èŠ‚ç‚¹ã€å°†å¼ºè¿é€šåˆ†é‡ç‹¬ç«‹è§£å†³ç­‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨ä¸¤ç§ç®—æ³•ï¼ˆHoffman-Karpå’Œæ’åˆ—æ”¹è¿›ç®—æ³•ï¼‰åœ¨ç”Ÿæˆçš„åŸºå‡†é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜æ’åˆ—æ”¹è¿›ç®—æ³•åœ¨è¿­ä»£æ¬¡æ•°å’Œæ—¶é—´ä¸Šå‡ä¼˜äºHoffman-Karpç®—æ³•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ç”Ÿæˆç®—æ³•å’ŒåŸºå‡†åº“ä¸ºSSGsçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£SSGsçš„å¤æ‚æ€§ï¼Œå¹¶å¯»æ‰¾æ›´æœ‰æ•ˆçš„ç®—æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¤šé¡¹å¼æ—¶é—´ç®€åŒ–æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œä»¥æé«˜ç®—æ³•çš„æ•ˆç‡ã€‚</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Application-level Benchmarking of Quantum Computers using Nonlocal Game Strategies</td>
      <td>In a nonlocal game, two noncommunicating players cooperate to convince a<br>referee that they possess a strategy that does not violate the rules of the<br>game. Quantum strategies allow players to optimally win some games by<br>performing joint measurements on a shared entangled state, but computing these<br>strategies can be challenging. We present a variational quantum algorithm to<br>compute quantum strategies for nonlocal games by encoding the rules of a<br>nonlocal game into a Hamiltonian. We show how this algorithm can generate a<br>short-depth optimal quantum strategy for a graph coloring game with a quantum<br>advantage. This quantum strategy is then evaluated on fourteen different<br>quantum hardware platforms to demonstrate its utility as a benchmark. Finally,<br>we discuss potential sources of errors that can explain the observed decreased<br>performance of the executed task and derive an expression for the number of<br>samples required to accurately estimate the win rate in the presence of noise.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨éå±€åŸŸæ¸¸æˆç­–ç•¥å¯¹é‡å­è®¡ç®—æœºè¿›è¡Œåº”ç”¨çº§åŸºå‡†æµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å½“å‰é‡å­ç¡¬ä»¶çš„å‘å±•å°šå¤„äºåˆçº§é˜¶æ®µï¼Œè¿è¡Œå…·æœ‰å¯è¯æ˜ä¼˜åŠ¿çš„ç®€å•é‡å­ç®—æ³•å®ä¾‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œå¼€å‘èƒ½å¤Ÿæµ‹è¯•å’ŒéªŒè¯é‡å­ç¡¬ä»¶ç‹¬ç‰¹ç‰¹æ€§çš„åŸºå‡†æµ‹è¯•å·¥å…·å’ŒæŠ€æœ¯è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ä½çº§åŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼ˆå¦‚éšæœºåŸºå‡†æµ‹è¯•ï¼‰æ— æ³•å¸®åŠ©ç¡®å®šç®—æ³•æµç¨‹ä¸­çš„é”™è¯¯æ¥æºï¼Œè€Œé«˜çº§åŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼ˆå¦‚é‡å­ä½“ç§¯ï¼‰åˆ™è¿‡äºå®½æ³›ï¼Œæ— æ³•æ•æ‰åˆ°ç‰¹å®šç¡¬ä»¶å¹³å°å®ç°é‡å­ä¼˜åŠ¿çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨å˜åˆ†é‡å­ç®—æ³•è®¡ç®—éå±€åŸŸæ¸¸æˆä¸­çš„é‡å­ç­–ç•¥ã€‚è¯¥æ–¹æ³•å°†éå±€åŸŸæ¸¸æˆçš„è§„åˆ™ç¼–ç ä¸ºå“ˆå¯†é¡¿é‡ï¼Œå¹¶é€šè¿‡åŒç›¸ä¼˜åŒ–æŠ€æœ¯æ‰¾åˆ°èµ„æºæ€å’Œæµ‹é‡æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š<br>1. **èµ„æºæ€ä¼˜åŒ–**ï¼šä½¿ç”¨å˜åˆ†é‡å­æœ¬å¾æ±‚è§£å™¨ï¼ˆå¦‚ADAPT-VQEï¼‰æ‰¾åˆ°å›ºå®šæµ‹é‡æ–¹æ¡ˆä¸‹çš„æœ€ä¼˜å…±äº«æ€ã€‚<br>2. **æµ‹é‡ä¼˜åŒ–**ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¼˜åŒ–æµ‹é‡å‚æ•°ï¼ŒåŒæ—¶å›ºå®šå…±äº«æ€ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å°†è¯¥æ–¹æ³•åº”ç”¨äºCHSHæ¸¸æˆã€N-åˆ†éƒ¨å¯¹ç§°æ¸¸æˆå’Œå›¾ç€è‰²æ¸¸æˆï¼Œå¹¶æˆåŠŸæ‰¾åˆ°äº†æœ€ä¼˜é‡å­ç­–ç•¥ã€‚å…¶ä¸­ï¼Œå›¾ç€è‰²æ¸¸æˆç­–ç•¥åœ¨14ä¸ªé¡¶ç‚¹çš„å›¾ä¸Šå®ç°äº†é‡å­ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨14ä¸ªä¸åŒçš„é‡å­ç¡¬ä»¶å¹³å°ä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆè¯„ä¼°é‡å­ç¡¬ä»¶çš„éå±€åŸŸèƒ½åŠ›ï¼Œå¹¶æ­ç¤ºæ½œåœ¨çš„é”™è¯¯æ¥æºã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„å˜åˆ†é‡å­ç®—æ³•ä¸ºè®¡ç®—éå±€åŸŸæ¸¸æˆä¸­çš„é‡å­ç­–ç•¥æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶ä¸ºé‡å­ç¡¬ä»¶åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éå±€åŸŸæ¸¸æˆï¼Œå¹¶æœ‰åŠ©äºå¼€å‘æ›´åˆé€‚çš„é‡å­åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œä»¥é€‚åº”é‡å­è®¾å¤‡çš„å‘å±•å’Œç¡¬ä»¶æ¶æ„çš„å¤æ‚æ€§ã€‚</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques</td>
      <td>The remarkable success of GPT models across various tasks, including toponymy<br>recognition motivates us to assess the performance of the GPT-3 model in the<br>geocoding address parsing task. To ensure that the evaluation more accurately<br>mirrors performance in real-world scenarios with diverse user input qualities<br>and resolve the pressing need for a 'gold standard' evaluation dataset for<br>geocoding systems, we introduce a benchmark dataset of low-quality address<br>descriptions synthesized based on human input patterns mining from actual input<br>logs of a geocoding system in production. This dataset has 21 different input<br>errors and variations; contains over 239,000 address records that are uniquely<br>selected from streets across all U.S. 50 states and D.C.; and consists of three<br>subsets to be used as training, validation, and testing sets. Building on this,<br>we train and gauge the performance of the GPT-3 model in extracting address<br>components, contrasting its performance with transformer-based and LSTM-based<br>models. The evaluation results indicate that Bidirectional LSTM-CRF model has<br>achieved the best performance over these transformer-based models and GPT-3<br>model. Transformer-based models demonstrate very comparable results compared to<br>the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in<br>performance, showcases potential in the address parsing task with few-shot<br>examples, exhibiting room for improvement with additional fine-tuning. We open<br>source the code and data of this presented benchmark so that researchers can<br>utilize it for future model development or extend it to evaluate similar tasks,<br>such as document geocoding.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ChatGPT åœ¨åœ°ç†ç¼–ç åœ°å€è§£æä»»åŠ¡ä¸­çš„æ½œåŠ›è¯„ä¼°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ°ç†ç¼–ç æ˜¯å°†åœ°å€æè¿°è½¬æ¢ä¸ºåœ°ç†åæ ‡çš„è¿‡ç¨‹ï¼Œå¹¿æ³›åº”ç”¨äºåŸå¸‚è§„åˆ’ã€å…¬å…±å«ç”Ÿç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œåœ°ç†ç¼–ç æ•°æ®çš„å‡†ç¡®æ€§å—åˆ°åœ°å€è§£æè´¨é‡çš„å½±å“ã€‚åœ°å€è§£ææ˜¯ä»ç”¨æˆ·è¾“å…¥çš„åœ°å€æè¿°ä¸­æå–åœ°å€ç»„ä»¶çš„è¿‡ç¨‹ï¼Œå…¶å‡†ç¡®æ€§å¯¹åœ°ç†ç¼–ç ç»“æœçš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åœ°å€è§£ææ–¹æ³•ä¸»è¦åŸºäºè§„åˆ™å’Œç»Ÿè®¡æ–¹æ³•ï¼Œä½†éš¾ä»¥å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„é”™è¯¯å’Œå˜åŒ–ã€‚æ­¤å¤–ï¼Œç¼ºä¹ä¸€ä¸ªâ€œé»„é‡‘æ ‡å‡†â€çš„è¯„ä¼°æ•°æ®é›†æ¥å…¨é¢è¯„ä¼°åœ°ç†ç¼–ç ç³»ç»Ÿçš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°åœ°ç†ç¼–ç åœ°å€è§£ææŠ€æœ¯çš„åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚è¯¥åŸºå‡†æ•°æ®é›†æ˜¯åŸºäºå®é™…åœ°ç†ç¼–ç ç³»ç»Ÿæ—¥å¿—ä¸­çš„äººç±»è¾“å…¥æ¨¡å¼åˆæˆçš„ä½è´¨é‡åœ°å€æè¿°ï¼ŒåŒ…å«21ç§ä¸åŒçš„è¾“å…¥é”™è¯¯å’Œå˜åŒ–ï¼Œå¹¶è¦†ç›–äº†ç¾å›½æ‰€æœ‰50ä¸ªå·å’Œå“¥ä¼¦æ¯”äºšç‰¹åŒºçš„è¡—é“ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚<br><br>æœ¬æ–‡è¯„ä¼°äº†äº”ç§ä¸åŒçš„åœ°å€è§£ææ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-3æ¨¡å‹ã€åŸºäºTransformerçš„æ¨¡å‹å’ŒåŸºäºLSTMçš„æ¨¡å‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŒå‘LSTM-CRFæ¨¡å‹åœ¨åœ°å€ç»„ä»¶æå–æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯Transformeræ¨¡å‹ã€‚å°½ç®¡GPT-3æ¨¡å‹çš„æ€§èƒ½ç•¥é€Šä¸€ç­¹ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å±•ç°å‡ºæ½œåŠ›ï¼Œè¡¨æ˜é€šè¿‡è¿›ä¸€æ­¥çš„å¾®è°ƒå¯ä»¥æå‡å…¶æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒå‘LSTM-CRFæ¨¡å‹åœ¨æ‰€æœ‰åœ°å€ç»„ä»¶çš„æå–æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…¶F1åˆ†æ•°å’Œè§£æåˆ†æ•°æœ€é«˜ã€‚Transformeræ¨¡å‹åœ¨å¤§å¤šæ•°åœ°å€ç»„ä»¶çš„æå–æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ç•¥é€ŠäºåŒå‘LSTM-CRFæ¨¡å‹ã€‚GPT-3æ¨¡å‹åœ¨åœ°å€ç»„ä»¶æå–æ–¹é¢è¡¨ç°ç›¸å¯¹è¾ƒå·®ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å±•ç°å‡ºæ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ä¸ºè¯„ä¼°åœ°ç†ç¼–ç åœ°å€è§£ææŠ€æœ¯æä¾›äº†ä¸€ä¸ªâ€œé»„é‡‘æ ‡å‡†â€ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGPT-3æ¨¡å‹åœ¨åœ°å€è§£æä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥çš„å¾®è°ƒæ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy</td>
      <td>How does one measure "ability to understand language"? If it is a person's<br>ability that is being measured, this is a question that almost never poses<br>itself in an unqualified manner: Whatever formal test is applied, it takes<br>place on the background of the person's language use in daily social practice,<br>and what is measured is a specialised variety of language understanding (e.g.,<br>of a second language; or of written, technical language). Computer programs do<br>not have this background. What does that mean for the applicability of formal<br>tests of language understanding? I argue that such tests need to be<br>complemented with tests of language use embedded in a practice, to arrive at a<br>more comprehensive evaluation of "artificial language understanding". To do<br>such tests systematically, I propose to use "Dialogue Games" -- constructed<br>activities that provide a situational embedding for language use. I describe a<br>taxonomy of Dialogue Game types, linked to a model of underlying capabilites<br>that are tested, and thereby giving an argument for the \emph{construct<br>validity} of the test. I close with showing how the internal structure of the<br>taxonomy suggests an ordering from more specialised to more general situational<br>language understanding, which potentially can provide some strategic guidance<br>for development in this field.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¯¹è¯æ¸¸æˆï¼šè¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆå·¥å…·<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰é¢†åŸŸé•¿æœŸä»¥æ¥ä¾èµ–äºåŸºäºæ–‡æœ¬çš„è¯„ä¼°æ–¹æ³•ï¼Œä¾‹å¦‚é—®ç­”æˆ–æ–‡æœ¬æ‘˜è¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è¯­è¨€åœ¨å®é™…æƒ…å¢ƒä¸­çš„ä½¿ç”¨ï¼Œä¾‹å¦‚å¯¹è¯ä¸­çš„æŒ‡ä»£ã€æ¾„æ¸…å’Œè¡ŒåŠ¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼šå¯¹è¯æ¸¸æˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯¹è¯æ¸¸æˆ<br>å¯¹è¯æ¸¸æˆæ˜¯ä¸€ç§æ„å»ºçš„æ´»åŠ¨ï¼Œå‚ä¸è€…é€šè¿‡è¯­è¨€äº¤æµæ¥è¾¾æˆç‰¹å®šç›®æ ‡ã€‚è¿™äº›æ¸¸æˆå¯ä»¥æ¨¡æ‹Ÿç°å®ç”Ÿæ´»ä¸­çš„å¯¹è¯åœºæ™¯ï¼Œä¾‹å¦‚ç»„è£…å®¶å…·æˆ–é¢„è®¢ç«è½¦ç¥¨ã€‚é€šè¿‡è®¾è®¡ä¸åŒçš„æ¸¸æˆç±»å‹ï¼Œå¯ä»¥é’ˆå¯¹è¯­è¨€ç†è§£çš„å„ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚æŒ‡ä»£ã€ä¿¡æ¯è¯·æ±‚ã€è¡ŒåŠ¨åè°ƒç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯¹è¯æ¸¸æˆåˆ†ç±»<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹è¯æ¸¸æˆåˆ†ç±»æ³•ï¼Œå°†æ¸¸æˆåˆ†ä¸ºä¸ƒç§ç±»å‹ï¼šæŒ‡ä»£æ¸¸æˆã€ä¿¡æ¯æ¸¸æˆã€æ„å»ºæ¸¸æˆã€å¯¼èˆªæ¸¸æˆã€è°ˆåˆ¤æ¸¸æˆå’Œæ•™å­¦æ¸¸æˆã€‚æ¯ç§ç±»å‹éƒ½ä¾§é‡äºè¯­è¨€ç†è§£çš„ç‰¹å®šæ–¹é¢ï¼Œå¹¶å¯¹åº”äºæ¨¡å‹ä¸­ä¸åŒçš„çŸ¥è¯†å’Œé”šå®šè¿‡ç¨‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯é‡ç‚¹ä»‹ç»äº†å¯¹è¯æ¸¸æˆä½œä¸ºä¸€ç§è¯„ä¼°å·¥å…·çš„ç†è®ºåŸºç¡€å’Œè®¾è®¡æ–¹æ³•ã€‚ç„¶è€Œï¼Œä½œè€…é€šè¿‡åˆ†æç°æœ‰çš„å¯¹è¯æ¸¸æˆæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>å¯¹è¯æ¸¸æˆä¸ºè¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚å®ƒå¯ä»¥ç”¨äºï¼š<br>* **è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°**ï¼šä¾‹å¦‚ï¼ŒæŒ‡ä»£æ¸¸æˆå¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹æŒ‡ä»£è¡¨è¾¾çš„ç†è§£èƒ½åŠ›ã€‚<br>* **è¯„ä¼°æ¨¡å‹çš„æ•´ä½“è¯­è¨€ç†è§£èƒ½åŠ›**ï¼šé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—ä¸åŒå¤æ‚åº¦çš„å¯¹è¯æ¸¸æˆï¼Œå¯ä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚<br>* **ä¿ƒè¿›æ¨¡å‹çš„å‘å±•**ï¼šå¯¹è¯æ¸¸æˆå¯ä»¥æ­ç¤ºæ¨¡å‹åœ¨è¯­è¨€ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶ä¸ºæ¨¡å‹æ”¹è¿›æä¾›æ–¹å‘ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>å¯¹è¯æ¸¸æˆæ˜¯ä¸€ç§è¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆå·¥å…·ï¼Œå®ƒå¯ä»¥æ¨¡æ‹Ÿç°å®ç”Ÿæ´»ä¸­çš„å¯¹è¯åœºæ™¯ï¼Œå¹¶é’ˆå¯¹è¯­è¨€ç†è§£çš„å„ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡æå‡ºçš„å¯¹è¯æ¸¸æˆåˆ†ç±»æ³•ä¸ºè®¾è®¡å¯¹è¯æ¸¸æˆæä¾›äº†æŒ‡å¯¼ï¼Œå¹¶ä¸ºè¯„ä¼°æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ã€‚</td>
    </tr>
    <tr>
      <th>19</th>
      <td>MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields</td>
      <td>Previous research has demonstrated the advantages of integrating data from<br>multiple sources over traditional unimodal data, leading to the emergence of<br>numerous novel multimodal applications. We propose a multimodal classification<br>benchmark MuG with eight datasets that allows researchers to evaluate and<br>improve their models. These datasets are collected from four various genres of<br>games that cover tabular, textual, and visual modalities. We conduct<br>multi-aspect data analysis to provide insights into the benchmark, including<br>label balance ratios, percentages of missing features, distributions of data<br>within each modality, and the correlations between labels and input modalities.<br>We further present experimental results obtained by several state-of-the-art<br>unimodal classifiers and multimodal classifiers, which demonstrate the<br>challenging and multimodal-dependent properties of the benchmark. MuG is<br>released at https://github.com/lujiaying/MUG-Bench with the data, tutorials,<br>and implemented baselines.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MuGï¼šåŸºäºæ¸¸æˆæ•°æ®çš„å¤šå…ƒåˆ†ç±»åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ å·²ç»æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚å¤šæ¨¡æ€å­¦ä¹ èƒ½å¤Ÿæ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º MuG çš„å¤šæ¨¡æ€åˆ†ç±»åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>MuG åŒ…å«äº†æ¥è‡ªå››ä¸ªä¸åŒæ¸¸æˆç±»å‹çš„å…«ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–äº†è¡¨æ ¼ã€æ–‡æœ¬å’Œè§†è§‰ä¸‰ç§æ¨¡æ€ã€‚è¿™äº›æ•°æ®é›†ç»è¿‡å¿…è¦çš„æ¸…æ´—ã€è½¬æ¢å’Œä¿®æ”¹ï¼Œä»¥ä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ MuG çš„ç‰¹æ€§ï¼Œæœ¬æ–‡è¿›è¡Œäº†å¤šæ–¹é¢çš„æ•°æ®åˆ†æï¼ŒåŒ…æ‹¬æ ‡ç­¾å¹³è¡¡æ¯”ä¾‹ã€ç¼ºå¤±ç‰¹å¾ç™¾åˆ†æ¯”ã€æ¯ç§æ¨¡æ€ä¸­æ•°æ®çš„åˆ†å¸ƒä»¥åŠæ ‡ç­¾ä¸è¾“å…¥æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨å¤šä¸ªæœ€å…ˆè¿›çš„å•æ¨¡æ€åˆ†ç±»å™¨å’Œå¤šæ¨¡æ€åˆ†ç±»å™¨åœ¨ MuG ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€åˆ†ç±»å™¨åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½ä¼˜äºå•æ¨¡æ€åˆ†ç±»å™¨ï¼Œè¿™è¡¨æ˜ MuG ä¸­çš„åˆ†ç±»ä»»åŠ¡ç¡®å®ä¾èµ–äºå¤šæ¨¡æ€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å¤šæ¨¡æ€åˆ†ç±»å™¨ MUGNETï¼Œå®ƒåœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰çš„å¤šæ¨¡æ€åˆ†ç±»å™¨ç›¸å½“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MuG åŸºå‡†æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹çš„é‡è¦å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ MUGNET æ¨¡å‹ä¹Ÿä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨ MuG åŸºå‡†æ•°æ®é›†è¿›è¡Œæ›´å¤šå…³äºå¤šæ¨¡æ€å­¦ä¹ çš„ç ”ç©¶ï¼Œå¹¶æ¢ç´¢æ–°çš„åº”ç”¨åœºæ™¯ã€‚</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Equilibria in Repeated Games under No-Regret with Dynamic Benchmarks</td>
      <td>In repeated games, strategies are often evaluated by their ability to<br>guarantee the performance of the single best action that is selected in<br>hindsight, a property referred to as \emph{Hannan consistency}, or<br>\emph{no-regret}. However, the effectiveness of the single best action as a<br>yardstick to evaluate strategies is limited, as any static action may perform<br>poorly in common dynamic settings. Our work therefore turns to a more ambitious<br>notion of \emph{dynamic benchmark consistency}, which guarantees the<br>performance of the best \emph{dynamic} sequence of actions, selected in<br>hindsight subject to a constraint on the allowable number of action changes.<br>Our main result establishes that for any joint empirical distribution of play<br>that may arise when all players deploy no-regret strategies, there exist<br>dynamic benchmark consistent strategies such that if all players deploy these<br>strategies the same empirical distribution emerges when the horizon is large<br>enough. This result demonstrates that although dynamic benchmark consistent<br>strategies have a different algorithmic structure and provide significantly<br>enhanced individual assurances, they lead to the same equilibrium set as<br>no-regret strategies. Moreover, the proof of our main result uncovers the<br>capacity of independent algorithms with strong individual guarantees to foster<br>a strong form of coordination.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢é‡å¤åšå¼ˆä¸­çš„å‡è¡¡ç­–ç•¥ï¼šåŠ¨æ€åŸºå‡†ä¸€è‡´æ€§<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨é‡å¤åšå¼ˆä¸­ï¼Œç­–ç•¥é€šå¸¸é€šè¿‡å…¶åœ¨äº‹åé€‰æ‹©çš„æœ€ä½³å•æ¬¡è¡ŒåŠ¨çš„è¡¨ç°æ¥è¯„ä¼°ï¼Œè¿™ç§æ€§è´¨è¢«ç§°ä¸ºHannanä¸€è‡´æ€§æˆ–æ— åæ‚”ã€‚ç„¶è€Œï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­ï¼Œä»»ä½•é™æ€è¡ŒåŠ¨çš„è¡¨ç°éƒ½å¯èƒ½ä¸ä½³ï¼Œå› æ­¤è¿™ç§è¯„ä¼°ç­–ç•¥çš„æ ‡å‡†å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ›´ä¸ºé›„å¿ƒå‹ƒå‹ƒçš„åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µç¡®ä¿äº†åœ¨äº‹åé€‰æ‹©çš„æœ€ä½³åŠ¨æ€è¡ŒåŠ¨åºåˆ—çš„è¡¨ç°ï¼ŒåŒæ—¶å—åˆ°å…è®¸çš„è¡ŒåŠ¨å˜åŒ–æ¬¡æ•°çš„çº¦æŸã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1<br>æœ¬æ–‡æå‡ºäº†åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§çš„æ¦‚å¿µï¼Œå®ƒè€ƒè™‘äº†åœ¨é‡å¤åšå¼ˆä¸­ï¼Œç©å®¶å¯èƒ½é‡‡å–çš„ä¸€ç³»åˆ—åŠ¨æ€è¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯å•ä¸€çš„é™æ€è¡ŒåŠ¨ã€‚è¿™ç§ç­–ç•¥ä¸ä»…è€ƒè™‘äº†å•æ¬¡è¡ŒåŠ¨çš„æ•ˆæœï¼Œè¿˜è€ƒè™‘äº†æ•´ä¸ªè¡ŒåŠ¨åºåˆ—çš„é€‚åº”æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2<br>æœ¬æ–‡çš„ä¸»è¦ç»“æœæ˜¯ï¼Œå¯¹äºæ‰€æœ‰ç©å®¶éƒ½é‡‡ç”¨æ— åæ‚”ç­–ç•¥æ—¶å¯èƒ½å‡ºç°çš„ä»»ä½•è”åˆç»éªŒåˆ†å¸ƒï¼Œéƒ½å­˜åœ¨åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§ç­–ç•¥ï¼Œä½¿å¾—å½“åšå¼ˆçš„å‘¨æœŸè¶³å¤Ÿé•¿æ—¶ï¼Œæ‰€æœ‰ç©å®¶é‡‡ç”¨è¿™äº›ç­–ç•¥ä¼šäº§ç”Ÿç›¸åŒçš„ç»éªŒåˆ†å¸ƒã€‚è¿™ä¸€ç»“æœè¯æ˜äº†è™½ç„¶åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§ç­–ç•¥å…·æœ‰ä¸åŒçš„ç®—æ³•ç»“æ„ï¼Œå¹¶ä¸ºä¸ªä½“æä¾›äº†æ˜¾è‘—å¢å¼ºçš„ä¿éšœï¼Œä½†å®ƒä»¬å¯¼è‡´çš„å‡è¡¡é›†åˆä¸æ— åæ‚”ç­–ç•¥ç›¸åŒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡ç†è®ºåˆ†æè¯æ˜äº†åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§ç­–ç•¥çš„å­˜åœ¨æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä½†æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœã€‚ä½œè€…é€šè¿‡æ•°å­¦è¯æ˜å±•ç¤ºäº†ç‹¬ç«‹ç®—æ³•å…·æœ‰å¼ºçƒˆä¸ªä½“ä¿éšœçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¿ƒè¿›ä¸€ç§å¼ºçƒˆçš„åè°ƒå½¢å¼ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§æ¦‚å¿µä¸ºé‡å¤åšå¼ˆä¸­çš„ç­–ç•¥è¯„ä¼°æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå¯¹äºç†è§£å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„åšå¼ˆè¡Œä¸ºå…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„è¯æ˜è¿‡ç¨‹æ­ç¤ºäº†å…·æœ‰å¼ºä¸ªä½“ä¿éšœçš„ç‹¬ç«‹ç®—æ³•åœ¨ä¿ƒè¿›åè°ƒæ–¹é¢çš„æ½œåŠ›ï¼Œè¿™å¯¹äºç®—æ³•è®¾è®¡å’Œåšå¼ˆç†è®ºçš„åº”ç”¨å…·æœ‰å¯å‘æ„ä¹‰ã€‚</td>
    </tr>
    <tr>
      <th>21</th>
      <td>A Game Benchmark for Real-Time Human-Swarm Control</td>
      <td>We present a game benchmark for testing human-swarm control algorithms and<br>interfaces in a real-time, high-cadence scenario. Our benchmark consists of a<br>swarm vs. swarm game in a virtual ROS environment in which the goal of the game<br>is to capture all agents from the opposing swarm; the game's high-cadence is a<br>result of the capture rules, which cause agent team sizes to fluctuate rapidly.<br>These rules require players to consider both the number of agents currently at<br>their disposal and the behavior of their opponent's swarm when they plan<br>actions. We demonstrate our game benchmark with a default human-swarm control<br>system that enables a player to interact with their swarm through a high-level<br>touchscreen interface. The touchscreen interface transforms player gestures<br>into swarm control commands via a low-level decentralized ergodic control<br>framework. We compare our default human-swarm control system to a<br>flocking-based control system, and discuss traits that are crucial for swarm<br>control algorithms and interfaces operating in real-time, high-cadence<br>scenarios like our game benchmark. Our game benchmark code is available on<br>Github; more information can be found at<br>https://sites.google.com/view/swarm-game-benchmark.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å®æ—¶äººæœºç¾¤æ§æ¸¸æˆåŸºå‡†ï¼šæŒ‘æˆ˜ä¸æœºé‡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æœºå™¨äººæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæœºå™¨äººé›†ç¾¤åœ¨å±é™©ç¯å¢ƒä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç®—æ³•å’Œç•Œé¢åœ¨é«˜åº¦åŠ¨æ€çš„ç¯å¢ƒä¸­å¾€å¾€éš¾ä»¥èƒœä»»ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„æµ‹è¯•åŸºå‡†æ¥è¯„ä¼°è¿™äº›ç®—æ³•å’Œç•Œé¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¸¸æˆçš„å®æ—¶äººæœºç¾¤æ§åŸºå‡†ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿé«˜åº¦åŠ¨æ€ç¯å¢ƒä¸‹çš„ç¾¤æ§æŒ‘æˆ˜ï¼Œå¹¶è¯„ä¼°ä¸åŒç®—æ³•å’Œç•Œé¢çš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€ç¾¤æ§æ¸¸æˆåŸºå‡†<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä¸ºâ€œç¾¤æ§å¯¹æŠ—â€çš„æ¸¸æˆï¼Œç©å®¶é€šè¿‡ç•Œé¢æ§åˆ¶è‡ªå·±çš„æœºå™¨äººé›†ç¾¤ï¼Œç›®æ ‡æ˜¯æ•è·å¯¹æ–¹é›†ç¾¤çš„æ‰€æœ‰æœºå™¨äººã€‚æ¸¸æˆè§„åˆ™å¯¼è‡´é›†ç¾¤è§„æ¨¡å¿«é€Ÿå˜åŒ–ï¼Œè¿«ä½¿ç©å®¶åœ¨åˆ¶å®šç­–ç•¥æ—¶è€ƒè™‘è‡ªèº«é›†ç¾¤è§„æ¨¡å’Œå¯¹æ–¹é›†ç¾¤è¡Œä¸ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé»˜è®¤äººæœºç¾¤æ§ç³»ç»Ÿ<br>æœ¬æ–‡å±•ç¤ºäº†ä¸€ä¸ªé»˜è®¤çš„äººæœºç¾¤æ§ç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…å«ä¸€ä¸ªè§¦æ‘¸å±ç•Œé¢å’Œä¸€ä¸ªä½å±‚çº§çš„å»ä¸­å¿ƒåŒ–éå†æ§åˆ¶æ¡†æ¶ã€‚è§¦æ‘¸å±ç•Œé¢å°†ç©å®¶æ‰‹åŠ¿è½¬æ¢ä¸ºé›†ç¾¤æ§åˆ¶å‘½ä»¤ï¼Œè€Œæ§åˆ¶æ¡†æ¶åˆ™è´Ÿè´£æ‰§è¡Œè¿™äº›å‘½ä»¤ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå»ä¸­å¿ƒåŒ–éå†æ§åˆ¶<br>æœ¬æ–‡æå‡ºçš„å»ä¸­å¿ƒåŒ–éå†æ§åˆ¶æ¡†æ¶èƒ½å¤Ÿå°†ç©å®¶ç›®æ ‡åˆ†å¸ƒè½¬æ¢ä¸ºä½å±‚çº§çš„é›†ç¾¤æ§åˆ¶å‘½ä»¤ï¼Œå¹¶å®ç°è§„æ¨¡ä¸å˜æ€§å’Œæ’åˆ—ä¸å˜æ€§ã€‚è¿™ä½¿å¾—ç©å®¶å¯ä»¥ä¸“æ³¨äºé›†ç¾¤çº§åˆ«çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯å•ä¸ªæœºå™¨äººçš„è½¨è¿¹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡å®éªŒæ¯”è¾ƒäº†é»˜è®¤äººæœºç¾¤æ§ç³»ç»Ÿä¸åŸºäºç¾¤èšçš„æ§åˆ¶ç³»ç»Ÿçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œé»˜è®¤ç³»ç»Ÿåœ¨å®æ—¶ã€é«˜é¢‘ç‡çš„åœºæ™¯ä¸‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”ç¯å¢ƒå˜åŒ–å¹¶åˆ¶å®šç­–ç•¥ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŸºå‡†ä¸ºè¯„ä¼°å®æ—¶äººæœºç¾¤æ§ç®—æ³•å’Œç•Œé¢æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å»ä¸­å¿ƒåŒ–éå†æ§åˆ¶æ¡†æ¶ä¹Ÿä¸ºå®ç°çµæ´»ã€å¯æ‰©å±•çš„ç¾¤æ§ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ã€‚<br><br>## ğŸŒŸ æœªæ¥å±•æœ›<br>æœªæ¥ç ”ç©¶å¯ä»¥å¼€å‘è™šæ‹Ÿå¯¹æ‰‹æ¥ä¸äººç±»ç©å®¶ç«äº‰ï¼Œå¹¶æ‰©å±•æ¸¸æˆåŸºå‡†ä»¥è¿›è¡Œäººç±»ä¸»ä½“æµ‹è¯•ï¼Œä»¥æ›´æ·±å…¥åœ°äº†è§£äººç±»ç©å®¶åœ¨å®æ—¶ã€é«˜é¢‘ç‡åœºæ™¯ä¸‹çš„ç­–ç•¥å’Œè¡Œä¸ºã€‚</td>
    </tr>
    <tr>
      <th>22</th>
      <td>WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments</td>
      <td>Recent advances in deep reinforcement learning (RL) have demonstrated complex<br>decision-making capabilities in simulation environments such as Arcade Learning<br>Environment, MuJoCo, and ViZDoom. However, they are hardly extensible to more<br>complicated problems, mainly due to the lack of complexity and variations in<br>the environments they are trained and tested on. Furthermore, they are not<br>extensible to an open-world environment to facilitate long-term exploration<br>research. To learn realistic task-solving capabilities, we need to develop an<br>environment with greater diversity and complexity. We developed WILD-SCAV, a<br>powerful and extensible environment based on a 3D open-world FPS (First-Person<br>Shooter) game to bridge the gap. It provides realistic 3D environments of<br>variable complexity, various tasks, and multiple modes of interaction, where<br>agents can learn to perceive 3D environments, navigate and plan, compete and<br>cooperate in a human-like manner. WILD-SCAV also supports different<br>complexities, such as configurable maps with different terrains, building<br>structures and distributions, and multi-agent settings with cooperative and<br>competitive tasks. The experimental results on configurable complexity,<br>multi-tasking, and multi-agent scenarios demonstrate the effectiveness of<br>WILD-SCAV in benchmarking various RL algorithms, as well as it is potential to<br>give rise to intelligent agents with generalized task-solving abilities. The<br>link to our open-sourced code can be found here<br>https://github.com/inspirai/wilderness-scavenger.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WILD-SCAVï¼šåŸºäºUnity3Dçš„FPSæ¸¸æˆAIåŸºå‡†æµ‹è¯•ç¯å¢ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚Arcade Learning Environmentã€MuJoCoå’ŒViZDoomï¼‰ä¸­å±•ç°å‡ºå¤æ‚çš„å†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›ç¯å¢ƒåœ¨å¤æ‚æ€§å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¹Ÿä¸é€‚ç”¨äºå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œæ— æ³•ä¿ƒè¿›é•¿æœŸæ¢ç´¢ç ”ç©¶ã€‚ä¸ºäº†å­¦ä¹ ç°å®ä¸–ç•Œçš„ä»»åŠ¡è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘ä¸€ä¸ªå…·æœ‰æ›´é«˜å¤šæ ·æ€§å’Œå¤æ‚æ€§çš„ç¯å¢ƒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWILD-SCAVæ˜¯ä¸€ä¸ªåŸºäº3Då¼€æ”¾ä¸–ç•ŒFPSæ¸¸æˆçš„å¼ºå¤§ä¸”å¯æ‰©å±•çš„ç¯å¢ƒï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚å®ƒæä¾›äº†å…·æœ‰å¯å˜å¤æ‚æ€§çš„çœŸå®3Dç¯å¢ƒã€å„ç§ä»»åŠ¡å’Œå¤šç§äº¤äº’æ¨¡å¼ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä»¥ç±»ä¼¼äººç±»çš„æ–¹å¼å­¦ä¹ æ„ŸçŸ¥3Dç¯å¢ƒã€å¯¼èˆªå’Œè§„åˆ’ã€ç«äº‰å’Œåˆä½œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šWILD-SCAVæ”¯æŒä¸åŒçš„å¤æ‚æ€§ï¼Œä¾‹å¦‚å…·æœ‰ä¸åŒåœ°å½¢ã€å»ºç­‘ç»“æ„å’Œåˆ†å¸ƒçš„å¯é…ç½®åœ°å›¾ï¼Œä»¥åŠå…·æœ‰åˆä½œå’Œç«äº‰ä»»åŠ¡çš„å¤šä»£ç†è®¾ç½®ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒWILD-SCAVåœ¨å¯é…ç½®å¤æ‚æ€§ã€å¤šä»»åŠ¡å’Œå¤šä»£ç†åœºæ™¯ä¸­æœ‰æ•ˆåœ°è¯„ä¼°äº†å„ç§RLç®—æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWILD-SCAVå…·æœ‰äº§ç”Ÿå…·æœ‰é€šç”¨ä»»åŠ¡è§£å†³èƒ½åŠ›çš„æ™ºèƒ½ä»£ç†çš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WILD-SCAVä¸ºç ”ç©¶å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä»£ç†å­¦ä¹ æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ï¼Œå¹¶æ”¯æŒå„ç§å®éªŒå’Œä»»åŠ¡è®¾è®¡ã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å¼ºå¤§çš„RLç®—æ³•ï¼Œå¹¶æ¨åŠ¨3Då¼€æ”¾ä¸–ç•ŒAIç®—æ³•çš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>23</th>
      <td>The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning</td>
      <td>As machine learning (ML) is more tightly woven into society, it is imperative<br>that we better characterize ML's strengths and limitations if we are to employ<br>it responsibly. Existing benchmark environments for ML, such as board and video<br>games, offer well-defined benchmarks for progress, but constituent tasks are<br>often complex, and it is frequently unclear how task characteristics contribute<br>to overall difficulty for the machine learner. Likewise, without a systematic<br>assessment of how task characteristics influence difficulty, it is challenging<br>to draw meaningful connections between performance in different benchmark<br>environments. We introduce a novel benchmark environment that offers an<br>enormous range of ML challenges and enables precise examination of how task<br>elements influence practical difficulty. The tool frames learning tasks as a<br>"board-clearing game," which we call the Game of Hidden Rules (GOHR). The<br>environment comprises an expressive rule language and a captive server<br>environment that can be installed locally. We propose a set of benchmark<br>rule-learning tasks and plan to support a performance leader-board for<br>researchers interested in attempting to learn our rules. GOHR complements<br>existing environments by allowing fine, controlled modifications to tasks,<br>enabling experimenters to better understand how each facet of a given learning<br>task contributes to its practical difficulty for an arbitrary ML algorithm.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢æœºå™¨å­¦ä¹ æ–°æŒ‘æˆ˜ï¼šéšè—è§„åˆ™æ¸¸æˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨ç¤¾ä¼šä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¥½åœ°äº†è§£å…¶ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä»¥ä¾¿è´Ÿè´£ä»»åœ°ä½¿ç”¨å®ƒã€‚ç°æœ‰çš„MLåŸºå‡†ç¯å¢ƒï¼Œå¦‚æ£‹ç›˜æ¸¸æˆå’Œè§†é¢‘æ¸¸æˆï¼Œæä¾›äº†æ˜ç¡®çš„åŸºå‡†æ¥è¡¡é‡è¿›æ­¥ï¼Œä½†è¿™äº›ä»»åŠ¡é€šå¸¸å¾ˆå¤æ‚ï¼Œè€Œä¸”ä¸æ¸…æ¥šä»»åŠ¡ç‰¹å¾å¦‚ä½•å½±å“æœºå™¨å­¦ä¹ è€…çš„æ•´ä½“éš¾åº¦ã€‚æ­¤å¤–ï¼Œæ²¡æœ‰å¯¹ä»»åŠ¡ç‰¹å¾å¦‚ä½•å½±å“éš¾åº¦çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå¾ˆéš¾åœ¨ä¸åŒåŸºå‡†ç¯å¢ƒä¸­ç»˜åˆ¶å‡ºæœ‰æ„ä¹‰çš„æ€§èƒ½è”ç³»ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†ç¯å¢ƒï¼Œç§°ä¸ºâ€œéšè—è§„åˆ™æ¸¸æˆâ€ï¼ˆGOHRï¼‰ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„MLæŒ‘æˆ˜ï¼Œå¹¶èƒ½å¤Ÿç²¾ç¡®åœ°æ£€æŸ¥ä»»åŠ¡å…ƒç´ å¦‚ä½•å½±å“å®é™…éš¾åº¦ã€‚GOHRå°†å­¦ä¹ ä»»åŠ¡æ¡†æ¶åŒ–ä¸ºä¸€ä¸ªâ€œæ¸…ç›˜æ¸¸æˆâ€ï¼Œç¯å¢ƒåŒ…æ‹¬ä¸€ä¸ªä¸°å¯Œçš„è§„åˆ™è¯­è¨€å’Œä¸€ä¸ªå¯æœ¬åœ°å®‰è£…çš„ä¸“ç”¨æœåŠ¡å™¨ç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—åŸºå‡†è§„åˆ™å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶è®¡åˆ’ä¸ºæœ‰å…´è¶£å­¦ä¹ æˆ‘ä»¬è§„åˆ™çš„ç ”ç©¶äººå‘˜æ”¯æŒä¸€ä¸ªæ€§èƒ½æ’è¡Œæ¦œã€‚GOHRé€šè¿‡å…è®¸å¯¹ä»»åŠ¡è¿›è¡Œç²¾ç»†ã€å¯æ§çš„ä¿®æ”¹ï¼Œè¡¥å……äº†ç°æœ‰çš„ç¯å¢ƒï¼Œä½¿å®éªŒè€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç»™å®šå­¦ä¹ ä»»åŠ¡çš„æ¯ä¸ªæ–¹é¢å¦‚ä½•ä¸ºä»»æ„MLç®—æ³•çš„å®é™…éš¾åº¦åšå‡ºè´¡çŒ®ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ ·æœ¬è§„åˆ™ï¼Œä¸€ä¸ªæ ·æœ¬MLç®—æ³•ï¼ˆMLAï¼‰ï¼Œæ€§èƒ½æŒ‡æ ‡å’Œæœ‰ç”¨çš„æ•°æ®å‘ˆç°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨DQNç®—æ³•å¯¹æ ·æœ¬è§„åˆ™è¿›è¡Œäº†å­¦ä¹ ï¼Œå¹¶åˆ†æäº†ç´¯ç§¯é”™è¯¯æ•°ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæ‰€æœ‰æ ·æœ¬è§„åˆ™ï¼Œç´¯ç§¯é”™è¯¯æœ€ç»ˆéƒ½ä¼šè¶‹äºå¹³ç¨³ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œç»ˆç«¯ç´¯ç§¯é”™è¯¯â€ï¼ˆTCEï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨Mann-Whitney-Wilcoxon U-Testå¯¹è§„åˆ™éš¾åº¦è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å‘ç°U-Testä¸ç´¯ç§¯é”™è¯¯æ›²çº¿çš„é«˜åº¦ä¸€è‡´ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GOHRä¸ºç ”ç©¶MLç®—æ³•çš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªæ–°é¢–è€Œæœ‰åŸåˆ™çš„æ–¹æ³•ã€‚ä½¿ç”¨ä¸°å¯Œçš„è§„åˆ™è¯­æ³•ï¼Œç ”ç©¶äººå‘˜å¯ä»¥å¯¹æ„Ÿå…´è¶£çš„è§„åˆ™è¿›è¡Œç²¾ç¡®çš„æ›´æ”¹ï¼Œä»¥ç ”ç©¶å®ƒä»¬å¦‚ä½•å½±å“ç®—æ³•æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç”¨äºæ¯”è¾ƒä¸åŒç®—æ³•åœ¨ç‰¹å®šè§„åˆ™é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è§‚å¯Ÿè§„åˆ™ç‰¹å¾ï¼ˆä½ç½®ã€å±æ€§ã€é™æ€/åŠ¨æ€ï¼‰å¯¹MLAæ€§èƒ½çš„å½±å“ã€‚GOHRè¿˜å¯ä»¥ç”±äººç±»è§„åˆ™å­¦ä¹ è€…è¿›è¡Œæ¸¸æˆï¼Œä»è€Œèƒ½å¤Ÿåœ¨å¹³ç­‰çš„åŸºç¡€ä¸Šæ¯”è¾ƒäººç±»å­¦ä¹ å’ŒMLã€‚</td>
    </tr>
    <tr>
      <th>24</th>
      <td>GOAL: Towards Benchmarking Few-Shot Sports Game Summarization</td>
      <td>Sports game summarization aims to generate sports news based on real-time<br>commentaries. The task has attracted wide research attention but is still<br>under-explored probably due to the lack of corresponding English datasets.<br>Therefore, in this paper, we release GOAL, the first English sports game<br>summarization dataset. Specifically, there are 103 commentary-news pairs in<br>GOAL, where the average lengths of commentaries and news are 2724.9 and 476.3<br>words, respectively. Moreover, to support the research in the semi-supervised<br>setting, GOAL additionally provides 2,160 unlabeled commentary documents. Based<br>on our GOAL, we build and evaluate several baselines, including extractive and<br>abstractive baselines. The experimental results show the challenges of this<br>task still remain. We hope our work could promote the research of sports game<br>summarization. The dataset has been released at<br>https://github.com/krystalan/goal.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GOALï¼šè¿ˆå‘å°‘æ ·æœ¬ä½“è‚²æ¸¸æˆæ‘˜è¦åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ä½“è‚²æ¸¸æˆæ‘˜è¦çš„ç›®æ ‡æ˜¯åŸºäºå®æ—¶è¯„è®ºç”Ÿæˆä½“è‚²æ–°é—»ã€‚å°½ç®¡è¯¥ä»»åŠ¡å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œä½†ç”±äºç¼ºä¹ç›¸åº”çš„è‹±æ–‡æ•°æ®é›†ï¼Œå› æ­¤ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†GOALï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè‹±æ–‡ä½“è‚²æ¸¸æˆæ‘˜è¦æ•°æ®é›†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†GOALæ•°æ®é›†ï¼ŒåŒ…å«103ä¸ªè¯„è®º-æ–°é—»å¯¹ï¼Œå…¶ä¸­è¯„è®ºå’Œæ–°é—»çš„å¹³å‡é•¿åº¦åˆ†åˆ«ä¸º2724.9å’Œ476.3ä¸ªå•è¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒåŠç›‘ç£å­¦ä¹ ï¼ŒGOALè¿˜æä¾›äº†2160ä¸ªæœªæ ‡è®°çš„è¯„è®ºæ–‡æ¡£ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨GOALæ•°æ®é›†çš„åŸºç¡€ä¸Šï¼Œæ„å»ºå’Œè¯„ä¼°äº†å‡ ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬æå–å¼å’ŒæŠ½è±¡å¼åŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæå–å¼åŸºçº¿çš„æ€§èƒ½æœ‰é™ï¼Œå› ä¸ºè¯„è®ºå’Œæ–°é—»ä¹‹é—´çš„æ–‡æœ¬é£æ ¼ä¸åŒã€‚PGNæ¨¡å‹ä¼˜äºæå–å¼æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”Ÿæˆä¸å—åŸå§‹å•è¯æˆ–çŸ­è¯­é™åˆ¶çš„ä½“è‚²æ–°é—»ã€‚ç„¶è€Œï¼ŒLEDæ¨¡å‹åœ¨æ‰€æœ‰åŸºçº¿ä¸­è¡¨ç°æœ€ä½³ï¼Œå› ä¸ºå®ƒå…·æœ‰æŠ½è±¡æ€§è´¨å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GOALæ•°æ®é›†å’ŒåŸºçº¿æ¨¡å‹ä¸ºä½“è‚²æ¸¸æˆæ‘˜è¦ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†åˆ©ç”¨å¤–éƒ¨èµ„æºæ¥å¢å¼ºæ¨¡å‹å¯¹ä½“è‚²æ–‡æœ¬çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ¢ç´¢åŠç›‘ç£å’Œå¤šè¯­è¨€è®¾ç½®ï¼Œåˆ©ç”¨å›¾ç»“æ„æ¥å»ºæ¨¡è¯„è®ºä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å…¶ä»–é•¿æ–‡æ¡£æ‘˜è¦èµ„æºæ¥æ„å»ºå¤šé¢†åŸŸæˆ–è·¨é¢†åŸŸæ¨¡å‹ã€‚</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Meterstick: Benchmarking Performance Variability in Cloud and Self-hosted Minecraft-like Games Extended Technical Report</td>
      <td>Due to increasing popularity and strict performance requirements, online<br>games have become a workload of interest for the performance engineering<br>community. One of the most popular types of online games is the Minecraft-like<br>Game (MLG), in which players can terraform the environment. The most popular<br>MLG, Minecraft, provides not only entertainment, but also educational support<br>and social interaction, to over 130 million people world-wide. MLGs currently<br>support their many players by replicating isolated instances that support each<br>only up to a few hundred players under favorable conditions. In practice, as we<br>show here, the real upper limit of supported players can be much lower. In this<br>work, we posit that performance variability is a key cause for the lack of<br>scalability in MLGs. We propose a novel operational model for MLGs and use it<br>to design the first benchmark that focuses on MLG performance variability,<br>defining specialized workloads, metrics, and processes. We conduct real-world<br>benchmarking of MLGs and find environment-based workloads and cloud deployment<br>to be significant sources of performance variability: peak-latency degrades<br>sharply to 20.7 times the arithmetic mean, and exceeds by a factor of 7.4 the<br>performance requirements. We derive actionable insights for game-developers,<br>game-operators, and other stakeholders to tame performance variability.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Meterstickï¼šæ­ç§˜Minecraft-likeæ¸¸æˆæ€§èƒ½æ³¢åŠ¨ä¹‹è°œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br><br>éšç€åœ¨çº¿æ¸¸æˆè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå¯¹æ€§èƒ½çš„è¦æ±‚ä¹Ÿè¶Šæ¥è¶Šä¸¥æ ¼ï¼Œæ¸¸æˆæˆä¸ºäº†æ€§èƒ½å·¥ç¨‹é¢†åŸŸå…³æ³¨çš„é‡ç‚¹ã€‚Minecraft-likeæ¸¸æˆï¼ˆMLGï¼‰ä½œä¸ºå…¶ä¸­æœ€å—æ¬¢è¿çš„ç±»å‹ä¹‹ä¸€ï¼Œå…è®¸ç©å®¶æ”¹å˜æ¸¸æˆç¯å¢ƒï¼Œæ‹¥æœ‰åºå¤§çš„ç©å®¶ç¾¤ä½“ã€‚ç„¶è€Œï¼ŒMLGçš„å¯æ‰©å±•æ€§å´ä¸€ç›´å—é™ï¼Œå³ä½¿æ˜¯åƒMinecraftè¿™æ ·çš„çƒ­é—¨æ¸¸æˆï¼Œåœ¨æœ€ä½³æ¡ä»¶ä¸‹ä¹Ÿåªèƒ½æ”¯æŒå‡ ç™¾åç©å®¶ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œæ€§èƒ½æ³¢åŠ¨æ˜¯å¯¼è‡´MLGå¯æ‰©å±•æ€§å—é™çš„å…³é”®å› ç´ ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåä¸ºMeterstickçš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºé‡åŒ–MLGçš„æ€§èƒ½æ³¢åŠ¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMLGæ“ä½œæ¨¡å‹<br><br>æœ¬æ–‡é¦–å…ˆæå‡ºäº†ä¸€ä¸ªMLGæ“ä½œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è€ƒè™‘äº†MLGç‰¹æœ‰çš„å·¥ä½œè´Ÿè½½ï¼Œä¾‹å¦‚ç©å®¶è¡Œä¸ºã€åœ°å½¢æ¨¡æ‹Ÿå’Œå®ä½“æ¨¡æ‹Ÿã€‚è¿™äº›å·¥ä½œè´Ÿè½½ä¸ä¼ ç»Ÿæ¸¸æˆä¸åŒï¼Œéœ€è¦ç‰¹æ®Šçš„å¤„ç†æ–¹å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡MeterstickåŸºå‡†æµ‹è¯•å·¥å…·<br><br>Meterstickæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLGæ€§èƒ½æ³¢åŠ¨çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒå®šä¹‰äº†ä¸“é—¨çš„å·¥ä½œè´Ÿè½½ã€æŒ‡æ ‡å’Œæµç¨‹ï¼Œå¹¶æ”¯æŒäº‘ç¯å¢ƒå’Œè‡ªæ‰˜ç®¡ç¯å¢ƒã€‚Meterstickä½¿ç”¨äº†ä¸€ä¸ªåä¸ºInstability Ratioï¼ˆISRï¼‰çš„æ–°æŒ‡æ ‡æ¥é‡åŒ–æ€§èƒ½æ³¢åŠ¨ï¼Œè¯¥æŒ‡æ ‡åŸºäºå‘¨æœŸæ€§æŠ–åŠ¨ï¼Œå¹¶è€ƒè™‘äº†æ—¶é—´æˆ³é¡ºåºã€å¼‚å¸¸å€¼å’Œé‡‡æ ·æŒç»­æ—¶é—´ç­‰å› ç´ ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br><br>æœ¬æ–‡ä½¿ç”¨Meterstickå¯¹Minecraftã€Forgeå’ŒPaperMCä¸‰ç§MLGè¿›è¡Œäº†å®éªŒï¼Œå¹¶å‘ç°ï¼š<br><br>* **æ€§èƒ½æ³¢åŠ¨ä¼šå¯¼è‡´æ¸¸æˆä¸å¯ç©**ï¼šæœ€å¤§å“åº”æ—¶é—´å¯ä»¥è¾¾åˆ°å¹³å‡å€¼çš„20.7å€ï¼Œè¶…è¿‡å¯ç©æ€§é˜ˆå€¼çš„7.4å€ã€‚<br>* **åŸºäºç¯å¢ƒçš„å·¥ä½œè´Ÿè½½ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½æ³¢åŠ¨**ï¼šç¯å¢ƒå·¥ä½œè´Ÿè½½ä¼šå¯¼è‡´ISRæ˜¾è‘—å¢åŠ ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ¸¸æˆè¿‡è½½ç”šè‡³å´©æºƒã€‚<br>* **å•†ä¸šäº‘ç¯å¢ƒä¸­çš„æ€§èƒ½æ³¢åŠ¨æ¯”è‡ªæ‰˜ç®¡ç¯å¢ƒæ›´å¤§**ï¼šAWSå’ŒAzureäº‘ç¯å¢ƒä¸­çš„æ€§èƒ½æ³¢åŠ¨æ¯”DAS-5è¶…çº§è®¡ç®—æœºæ›´å¤§ã€‚<br>* **å¤„ç†å®ä½“çŠ¶æ€çš„è®¡ç®—æˆæœ¬å¾ˆé«˜**ï¼šå®ä½“ç›¸å…³çš„è®¡ç®—å ç”¨äº†å¤§éƒ¨åˆ†çš„è®¡ç®—æ—¶é—´å’ŒçŠ¶æ€æ›´æ–°æ¶ˆæ¯ã€‚<br>* **æ¨èçš„ç¡¬ä»¶é…ç½®ä¸è¶³ä»¥é¿å…æ€§èƒ½æ³¢åŠ¨**ï¼šæ¨èçš„äº‘ç¯å¢ƒç¡¬ä»¶é…ç½®ä¼šå¯¼è‡´æ€§èƒ½æ³¢åŠ¨ï¼Œè€Œæ›´å¼ºå¤§çš„ç¡¬ä»¶å¯ä»¥å°†å…¶é™åˆ¶åœ¨å¯æ¥å—çš„æ°´å¹³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br><br>* **æ¸¸æˆå¼€å‘è€…åº”æŠ¥å‘Šæ€§èƒ½æ³¢åŠ¨**ï¼šä½¿ç”¨ISRç­‰æŒ‡æ ‡æ¥è¯„ä¼°åœ¨çº¿æ¸¸æˆæ€§èƒ½ï¼Œå¹¶æä¾›ä¸€è‡´çš„è‰¯å¥½æ€§èƒ½ã€‚<br>* **æ¸¸æˆå¼€å‘è€…åº”å°†åŸºäºç¯å¢ƒçš„å·¥ä½œè´Ÿè½½çº³å…¥MLGåŸºå‡†æµ‹è¯•**ï¼šç¯å¢ƒå·¥ä½œè´Ÿè½½å¯¹MLGæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå¿…é¡»çº³å…¥åŸºå‡†æµ‹è¯•ã€‚<br>* **ç©å®¶åº”æ ¹æ®MLGé€‰æ‹©äº‘ç¯å¢ƒï¼Œå¹¶è€ƒè™‘è‡ªæ‰˜ç®¡**ï¼šä¸åŒMLGåœ¨ä¸åŒäº‘ç¯å¢ƒä¸­çš„æ€§èƒ½è¡¨ç°ä¸åŒï¼Œè‡ªæ‰˜ç®¡ä¹Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚<br>* **MLGæœåŠ¡æä¾›å•†åº”æé«˜ç¡¬ä»¶é…ç½®æ¨è**ï¼šæ¨èçš„äº‘ç¯å¢ƒç¡¬ä»¶é…ç½®ä¸è¶³ä»¥æ»¡è¶³MLGçš„æ€§èƒ½éœ€æ±‚ï¼Œéœ€è¦æ›´æ–°æ¨èé…ç½®ã€‚<br>* **æ¸¸æˆå¼€å‘è€…åº”ä¼˜åŒ–MLGä»¥å‡å°‘ç¯å¢ƒå·¥ä½œè´Ÿè½½çš„å½±å“**ï¼šé€šè¿‡æ€§èƒ½å·¥ç¨‹æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—é™ä½ç¯å¢ƒå·¥ä½œè´Ÿè½½å¯¹æ€§èƒ½çš„å½±å“ã€‚<br><br>## ğŸ¯ æœªæ¥å±•æœ›<br><br>æœ¬æ–‡æå‡ºçš„MeterstickåŸºå‡†æµ‹è¯•å·¥å…·å’ŒISRæŒ‡æ ‡ä¸ºMLGæ€§èƒ½è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶ISRä¸ç©å®¶æ„ŸçŸ¥è´¨é‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå…¬å…±æ’è¡Œæ¦œï¼Œæ–¹ä¾¿MLGæœåŠ¡æä¾›å•†å‘å¸ƒåŸºå‡†æµ‹è¯•åˆ†æ•°ã€‚</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game</td>
      <td>Simulated DAG models may exhibit properties that, perhaps inadvertently,<br>render their structure identifiable and unexpectedly affect structure learning<br>algorithms. Here, we show that marginal variance tends to increase along the<br>causal order for generically sampled additive noise models. We introduce<br>varsortability as a measure of the agreement between the order of increasing<br>marginal variance and the causal order. For commonly sampled graphs and model<br>parameters, we show that the remarkable performance of some continuous<br>structure learning algorithms can be explained by high varsortability and<br>matched by a simple baseline method. Yet, this performance may not transfer to<br>real-world data where varsortability may be moderate or dependent on the choice<br>of measurement scales. On standardized data, the same algorithms fail to<br>identify the ground-truth DAG or its Markov equivalence class. While<br>standardization removes the pattern in marginal variance, we show that data<br>generating processes that incur high varsortability also leave a distinct<br>covariance pattern that may be exploited even after standardization. Our<br>findings challenge the significance of generic benchmarks with independently<br>drawn parameters. The code is available at<br>https://github.com/Scriddie/Varsortability.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å°å¿ƒæ¨¡æ‹Ÿçš„DAGï¼å› æœå‘ç°åŸºå‡†å¯èƒ½å®¹æ˜“è¢«æ“çºµ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å› æœç»“æ„å­¦ä¹ æ—¨åœ¨ä»æ•°æ®ä¸­æ¨æ–­å› æœæ¨¡å‹ã€‚åœ¨ç”Ÿç‰©å­¦ã€åŒ»å­¦ã€é‡‘èå’Œæœºå™¨å­¦ä¹ ç­‰é¢†åŸŸï¼Œå› æœæ¨¡å‹éƒ½å¤‡å—å…³æ³¨ã€‚å› æœæ¨¡å‹ä¸ä»…æè¿°äº†å˜é‡çš„è§‚æµ‹è”åˆåˆ†å¸ƒï¼Œè¿˜å½¢å¼åŒ–äº†å¹²é¢„å’Œåäº‹å®é¢„æµ‹ã€‚æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ˜¯è¡¨ç¤ºå› æœç»“æ„çš„å¸¸ç”¨æ–¹æ³•ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å˜é‡ï¼Œæœ‰å‘è¾¹è¡¨ç¤ºå› æœå…³ç³»ã€‚ç„¶è€Œï¼Œä»è§‚æµ‹æ•°æ®ä¸­æ¨æ–­å› æœç»“æ„æ˜¯å›°éš¾çš„ï¼Œé€šå¸¸åªèƒ½è¯†åˆ«DAGçš„é©¬å°”å¯å¤«ç­‰ä»·ç±»ï¼ˆMECï¼‰ï¼Œå¹¶ä¸”æ‰¾åˆ°é«˜åˆ†çš„DAGæ˜¯NP-hardé—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†çº¿æ€§åŠ æ€§å™ªå£°æ¨¡å‹ï¼ˆANMï¼‰çš„DAGå­¦ä¹ é—®é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡å‘ç°ï¼Œå¯¹äºé€šç”¨é‡‡æ ·çš„åŠ æ€§å™ªå£°æ¨¡å‹ï¼Œè¾¹é™…æ–¹å·®å€¾å‘äºæ²¿ç€å› æœé¡ºåºå¢åŠ ã€‚ä¸ºäº†è¡¡é‡è¾¹é™…æ–¹å·®å¢åŠ é¡ºåºä¸å› æœé¡ºåºä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†â€œæ–¹å·®æ’åºæ€§â€ä½œä¸ºè¡¡é‡æŒ‡æ ‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å‘ç°ï¼Œå¯¹äºå¸¸è§çš„é‡‡æ ·å›¾å’Œæ¨¡å‹å‚æ•°ï¼Œä¸€äº›è¿ç»­ç»“æ„å­¦ä¹ ç®—æ³•çš„å‡ºè‰²æ€§èƒ½å¯ä»¥é€šè¿‡é«˜æ–¹å·®æ’åºæ€§æ¥è§£é‡Šï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„åŸºçº¿æ–¹æ³•æ¥åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™ç§æ€§èƒ½å¯èƒ½ä¸ä¼šè½¬ç§»åˆ°ç°å®ä¸–ç•Œçš„æ•°æ®ä¸­ï¼Œå› ä¸ºæ–¹å·®æ’åºæ€§å¯èƒ½é€‚ä¸­æˆ–ä¾èµ–äºæµ‹é‡å°ºåº¦çš„é€‰æ‹©ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œsortnregressâ€çš„ç®€å•åŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æŒ‰è¾¹é™…æ–¹å·®æ’åºå˜é‡å¹¶ä½¿ç”¨ç¨€ç–å›å½’æŠ€æœ¯é€‰æ‹©çˆ¶èŠ‚ç‚¹æ¥å·¥ä½œã€‚sortnregressçš„æ€§èƒ½åæ˜ äº†ç»™å®šè®¾ç½®ä¸­æ–¹å·®æ’åºæ€§çš„ç¨‹åº¦ï¼Œå¹¶ä¸ºç»“æ„å­¦ä¹ ç®—æ³•æä¾›äº†ä¸€ä¸ªå‚è€ƒåŸºçº¿ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸€äº›ç®—æ³•åœ¨åŸå§‹æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ ‡å‡†åŒ–æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®ã€‚sortnregressåœ¨åŸå§‹æ•°æ®ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè€Œåœ¨æ ‡å‡†åŒ–æ•°æ®ä¸Šåˆ™è¡¨ç°å‡ºäº†åŸºçº¿æ€§èƒ½ã€‚è¿™è¡¨æ˜ï¼Œæ–¹å·®æ’åºæ€§åœ¨ç»“æ„å­¦ä¹ ä»»åŠ¡ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿ç»­ç»“æ„å­¦ä¹ ç®—æ³•å¯¹æ•°æ®ç¼©æ”¾éå¸¸æ•æ„Ÿï¼Œå¹¶ä¸”åœ¨ä¸çŸ¥é“çœŸå®æ•°æ®å°ºåº¦çš„æƒ…å†µä¸‹å¯èƒ½æ— æ³•å¾ˆå¥½åœ°å·¥ä½œã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨å’Œè¯„ä¼°è¿™äº›ç®—æ³•æ—¶ï¼Œéœ€è¦è°¨æ…è€ƒè™‘æ•°æ®å°ºåº¦çš„å½±å“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹å·®æ’åºæ€§æ¦‚å¿µå’ŒsortnregressåŸºçº¿æ–¹æ³•å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒç»“æ„å­¦ä¹ ç®—æ³•åœ¨ä¸åŒåŸºå‡†åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>27</th>
      <td>OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research</td>
      <td>Owning to the unremitting efforts by a few institutes, significant progress<br>has recently been made in designing superhuman AIs in No-limit Texas Hold'em<br>(NLTH), the primary testbed for large-scale imperfect-information game<br>research. However, it remains challenging for new researchers to study this<br>problem since there are no standard benchmarks for comparing with existing<br>methods, which seriously hinders further developments in this research area. In<br>this work, we present OpenHoldem, an integrated toolkit for large-scale<br>imperfect-information game research using NLTH. OpenHoldem makes three main<br>contributions to this research direction: 1) a standardized evaluation protocol<br>for thoroughly evaluating different NLTH AIs, 2) four publicly available strong<br>baselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs<br>for public NLTH AI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,<br>hoping it facilitates further studies on the unsolved theoretical and<br>computational issues in this area and cultivate crucial research problems like<br>opponent modeling and human-computer interactive learning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OpenHoldemï¼šå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶çš„åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œåœ¨æ— é™åˆ¶å¾·å·æ‰‘å…‹ï¼ˆNLTHï¼‰é¢†åŸŸï¼Œè®¾è®¡å‡ºè¶…è¶Šäººç±»çš„AIå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒNLTHå·²æˆä¸ºå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶çš„ä¸»è¦æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŸºå‡†ï¼Œæ–°ç ”ç©¶äººå‘˜éš¾ä»¥ç ”ç©¶æ­¤é—®é¢˜ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†è¯¥ç ”ç©¶é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ‡å‡†åŒ–è¯„ä¼°åè®®<br>OpenHoldem æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬å››ç§ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºå…¨é¢è¯„ä¼°ä¸åŒçš„ NLTH AIã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬ï¼š<br>- **å¯¹å±€è¯„ä¼°æŒ‡æ ‡**ï¼šé€šè¿‡é‡å¤å¯¹å±€ï¼Œè¯„ä¼° AI çš„å¹³å‡æ•ˆç”¨ï¼Œå¹¶ä½¿ç”¨æ–¹å·®å‡å°‘æŠ€æœ¯ï¼ˆå¦‚å¤åˆ¶æ‰‘å…‹å’Œ AIVATï¼‰æ¥å‡å°‘éšæœºæ€§å½±å“ã€‚<br>- **å¯åˆ©ç”¨æ€§è¯„ä¼°æŒ‡æ ‡**ï¼šé€šè¿‡è®¡ç®—æœ€ä½³å“åº”ç­–ç•¥ï¼Œè¯„ä¼° AI çš„å¯åˆ©ç”¨æ€§ï¼Œå¹¶ä½¿ç”¨å±€éƒ¨æœ€ä½³å“åº”ï¼ˆLBRï¼‰å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRL-BRï¼‰æ¥è¿‘ä¼¼è®¡ç®—ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¬å¼€å¯ç”¨çš„å¼ºåŸºçº¿ AI<br>OpenHoldem è®¾è®¡å¹¶å®ç°äº†å››ç§ä¸åŒç±»å‹çš„ NLTH AIï¼Œä½œä¸ºæœªæ¥ç ”ç©¶çš„è‰¯å¥½èµ·ç‚¹ï¼š<br>- **åŸºäºè§„åˆ™çš„ AI**ï¼šç”±é¢†åŸŸä¸“å®¶è®¾è®¡çš„è§„åˆ™é›†åˆï¼Œç”¨äºå¤„ç†å„ç§åœºæ™¯ã€‚<br>- **åŸºäº CFR çš„é™æ€ AI**ï¼šä½¿ç”¨ CFR ç®—æ³•è¿‘ä¼¼æ±‚è§£çº³ä»€å‡è¡¡ç­–ç•¥ï¼Œå¹¶é€šè¿‡ä¿¡æ¯æŠ½è±¡å’ŒåŠ¨ä½œæŠ½è±¡æ¥é™ä½æ¸¸æˆè§„æ¨¡ã€‚<br>- **ç±»ä¼¼ DeepStack çš„åœ¨çº¿ AI**ï¼šä½¿ç”¨æŒç»­é‡è§£å’Œæ·±åº¦ç¥ç»ç½‘ç»œæ¥å¤„ç†ç¦»æ ‘åŠ¨ä½œï¼Œå¹¶æé«˜å†³ç­–æ•ˆç‡ã€‚<br>- **åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ AI**ï¼šä½¿ç”¨ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ä»æ¸¸æˆçŠ¶æ€å­¦ä¹ åˆ°åŠ¨ä½œï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡ç‰¹å¾æˆ–è¿›è¡Œè¿­ä»£æ¨ç†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåœ¨çº¿æµ‹è¯•å¹³å°<br>OpenHoldem å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æµ‹è¯•å¹³å°ï¼Œå†…ç½®äº†å››ç§å¼ºåŸºçº¿ AIï¼Œå¹¶æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜æµ‹è¯•å’Œæ¯”è¾ƒä»–ä»¬çš„ AIã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>OpenHoldem çš„åŸºçº¿ AI åœ¨ä¸ç°æœ‰å…¬å¼€å¯ç”¨çš„ NLTH AI çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒåˆ†æäº†ä¸åŒè®¾è®¡é€‰æ‹©å¯¹ AI æ€§èƒ½çš„å½±å“ï¼Œå¹¶éªŒè¯äº† OpenHoldem çš„å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OpenHoldem ä¸ºå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶æä¾›äº†ä¸€ä¸ªå®è´µçš„å·¥å…·ï¼Œå…¶æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€å¼ºåŸºçº¿ AI å’Œåœ¨çº¿æµ‹è¯•å¹³å°å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚OpenHoldem çš„è®¾è®¡æ€è·¯å’Œå®ç°æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–æ¸¸æˆ AI ç ”ç©¶æä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Towards Game-Playing AI Benchmarks via Performance Reporting Standards</td>
      <td>While games have been used extensively as milestones to evaluate game-playing<br>AI, there exists no standardised framework for reporting the obtained<br>observations. As a result, it remains difficult to draw general conclusions<br>about the strengths and weaknesses of different game-playing AI algorithms. In<br>this paper, we propose reporting guidelines for AI game-playing performance<br>that, if followed, provide information suitable for unbiased comparisons<br>between different AI approaches. The vision we describe is to build benchmarks<br>and competitions based on such guidelines in order to be able to draw more<br>general conclusions about the behaviour of different AI algorithms, as well as<br>the types of challenges different games pose.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆAIæ€§èƒ½è¯„ä¼°ï¼šè¿ˆå‘æ ‡å‡†åŒ–æŠ¥å‘Šæ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ¸¸æˆä¸€ç›´æ˜¯è¯„ä¼°æ¸¸æˆAIçš„é‡è¦é‡Œç¨‹ç¢‘ï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–çš„æŠ¥å‘Šæ¡†æ¶ï¼Œå¯¼è‡´éš¾ä»¥å¯¹ä¸åŒçš„æ¸¸æˆAIç®—æ³•è¿›è¡Œå…¬æ­£æ¯”è¾ƒã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºä¸€å¥—æŠ¥å‘ŠæŒ‡å—ï¼Œä»¥ä¿ƒè¿›ä¸åŒAIæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒï¼Œå¹¶æ„å»ºåŸºäºè¿™äº›æŒ‡å—çš„åŸºå‡†æµ‹è¯•å’Œç«èµ›ï¼Œä»è€Œæ›´å…¨é¢åœ°äº†è§£ä¸åŒAIç®—æ³•çš„è¡Œä¸ºå’Œä¸åŒæ¸¸æˆå¸¦æ¥çš„æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ¸¸æˆAIæ€§èƒ½æŠ¥å‘ŠæŒ‡å—<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæŠ¥å‘ŠæŒ‡å—ï¼Œæ—¨åœ¨ä¸ºæ¸¸æˆAIæ€§èƒ½æŠ¥å‘Šæä¾›ä¸€ä¸ªç»“æ„åŒ–çš„æ¡†æ¶ã€‚è¯¥æŒ‡å—åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š<br>1. **é—®é¢˜æè¿°**ï¼šè¯¦ç»†æè¿°æµ‹è¯•é—®é¢˜çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç±»å‹ã€æ¸¸æˆå†…æ€§èƒ½æŒ‡æ ‡ã€æ¸¸æˆ-AIäº¤äº’æ–¹å¼å’Œçº¦æŸæ¡ä»¶ã€‚<br>2. **è§£å†³æ–¹æ¡ˆå¤æ‚æ€§æè¿°**ï¼šæè¿°AIè§£å†³æ–¹æ¡ˆçš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ã€æ“ä½œç³»ç»Ÿã€è½¯ä»¶ã€è®¡ç®—å¤æ‚æ€§ã€æ¨¡å‹å¤æ‚æ€§å’Œå®ç°å¤æ‚æ€§ã€‚<br>3. **æ€§èƒ½é‡åŒ–**ï¼šæè¿°æ€§èƒ½æµ‹é‡çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç»Ÿè®¡æ˜¾è‘—æ€§è¯„ä¼°ã€å¯¹æ‰‹ç±»å‹ã€æµ‹è¯•æ¡ˆä¾‹çš„å¤šæ ·æ€§å’Œæ€§èƒ½æŒ‡æ ‡çš„èšåˆæ–¹å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå¯è§£é‡Šçš„åŸºå‡†æµ‹è¯•<br>æœ¬æ–‡æå‡ºäº†æ„å»ºå¯è§£é‡ŠåŸºå‡†æµ‹è¯•çš„æ„¿æ™¯ï¼Œå¹¶æå‡ºäº†ä»¥ä¸‹è¦æ±‚ï¼š<br>- äº§ç”Ÿå¯æµ‹é‡çš„ç»“æœ<br>- å…è®¸å¾—å‡ºæœ‰æ„ä¹‰çš„ç»“è®º<br>- å¯è§£é‡Šæ€§<br>- äº§ç”Ÿå¯æ¨å¹¿çš„ç»“è®º<br>- å¯é‡å¤æ€§<br>- äº§ç”Ÿç¨³å¥çš„ç»“æœ<br>- å…è®¸æ¯”è¾ƒ<br>- å®ç”¨æ€§<br>- å…¬å¼€åè§<br>- è€ƒè™‘è§£å†³æ–¹æ¡ˆå¤æ‚æ€§<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å°šæœªè¿›è¡Œå®éªŒï¼Œä½†æå‡ºäº†ä¸€ä¸ªæŠ¥å‘ŠæŒ‡å—ï¼Œæ—¨åœ¨ä¿ƒè¿›æ¸¸æˆAIæ€§èƒ½è¯„ä¼°çš„æ ‡å‡†åŒ–å’Œå¯è§£é‡Šæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æŠ¥å‘ŠæŒ‡å—å’ŒåŸºå‡†æµ‹è¯•æ¡†æ¶å¯¹äºæ¸¸æˆAIç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚é€šè¿‡éµå¾ªè¿™äº›æŒ‡å—ï¼Œå¯ä»¥æ›´å…¨é¢åœ°äº†è§£ä¸åŒAIç®—æ³•çš„è¡Œä¸ºå’Œæ€§èƒ½ï¼Œå¹¶ä¿ƒè¿›æ¸¸æˆAIé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Interbank lending with benchmark rates: Pareto optima for a class of singular control games</td>
      <td>We analyze a class of stochastic differential games of singular control,<br>motivated by the study of a dynamic model of interbank lending with benchmark<br>rates. We describe Pareto optima for this game and show how they may be<br>achieved through the intervention of a regulator, whose policy is a solution to<br>a singular stochastic control problem. Pareto optima are characterized in terms<br>of the solutions to a new class of Skorokhod problems with piecewise-continuous<br>free boundary.<br>  Pareto optimal policies are shown to correspond to the enforcement of<br>endogenous bounds on interbank lending rates. Analytical comparison between<br>Pareto optima and Nash equilibria provides insight into the impact of<br>regulatory intervention on the stability of interbank rates.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºå‡†åˆ©ç‡ä¸‹çš„é“¶è¡Œé—´å€Ÿè´·ï¼šä¸€ç±»å¥‡å¼‚æ§åˆ¶åšå¼ˆçš„å¸•ç´¯æ‰˜æœ€ä¼˜è§£<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>é“¶è¡Œé—´å€Ÿè´·å¸‚åœºæ˜¯é‡‘èæœºæ„ä¹‹é—´æˆ˜ç•¥äº’åŠ¨çš„æœ‰è¶£ä¾‹å­ï¼Œå‚ä¸è€…ä¼šæ ¹æ®å…¶ä»–å‚ä¸è€…çš„è¡ŒåŠ¨åˆ†å¸ƒåšå‡ºååº”ã€‚é“¶è¡Œé—´åŸºå‡†åˆ©ç‡çš„è®¾å®šæœºåˆ¶ï¼Œå¦‚ä¼¦æ•¦é“¶è¡ŒåŒä¸šæ‹†å€Ÿåˆ©ç‡ï¼ˆLIBORï¼‰ï¼Œåœ¨é‡‘èå¸‚åœºä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ã€‚ç„¶è€Œï¼Œè¿™ç§æœºåˆ¶å®¹æ˜“å—åˆ°æ“çºµï¼Œå¯¼è‡´å¸‚åœºä¸ç¨³å®šã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶é“¶è¡Œé—´å€Ÿè´·å¸‚åœºä¸­ï¼Œç›‘ç®¡è€…å¦‚ä½•é€šè¿‡å¹²é¢„æ¥è¾¾åˆ°å¸•ç´¯æ‰˜æœ€ä¼˜è§£ï¼Œä»è€Œæé«˜å¸‚åœºç¨³å®šæ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«åŸºå‡†åˆ©ç‡çš„é“¶è¡Œé—´å€Ÿè´·åŠ¨æ€æ¨¡å‹ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªå¥‡å¼‚æ§åˆ¶åšå¼ˆé—®é¢˜ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡é€šè¿‡ç ”ç©¶ç›‘ç®¡è€…çš„è¾…åŠ©æ§åˆ¶é—®é¢˜ï¼Œå¾—åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜è§£çš„åˆ»ç”»ï¼Œå¹¶å°†å…¶ä¸Nashå‡è¡¡è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ­ç¤ºäº†ç›‘ç®¡å¹²é¢„å¯¹é“¶è¡Œé—´åˆ©ç‡ç¨³å®šæ€§çš„å½±å“ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæœ¬æ–‡å°†å¸•ç´¯æ‰˜æœ€ä¼˜è§£æè¿°ä¸ºä¸€ç³»åˆ—å…·æœ‰åˆ†æ®µè¿ç»­è‡ªç”±è¾¹ç•Œçš„Skorokhodé—®é¢˜ï¼Œå¹¶é€šè¿‡æ±‚è§£è¿™äº›é—®é¢˜å¾—åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥çš„æ˜¾å¼è¡¨è¾¾å¼ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨N=2çš„æƒ…å†µä¸‹å¾—åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥çš„æ˜¾å¼è§£ï¼Œå¹¶ä¸Nashå‡è¡¡è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œå¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆé™ä½é“¶è¡Œé—´åˆ©ç‡çš„æ³¢åŠ¨æ€§ï¼Œæé«˜å¸‚åœºç¨³å®šæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºé“¶è¡Œé—´å€Ÿè´·å¸‚åœºçš„ç›‘ç®¡æä¾›äº†ç†è®ºä¾æ®ï¼Œå¹¶ä¸ºè®¾è®¡æœ‰æ•ˆçš„ç›‘ç®¡æ”¿ç­–æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¥‡å¼‚æ§åˆ¶åšå¼ˆæ¨¡å‹å’ŒSkorokhodé—®é¢˜çš„æ±‚è§£æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºå…¶ä»–é‡‘èå¸‚åœºçš„å»ºæ¨¡å’Œåˆ†æã€‚</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Benchmarking End-to-End Behavioural Cloning on Video Games</td>
      <td>Behavioural cloning, where a computer is taught to perform a task based on<br>demonstrations, has been successfully applied to various video games and<br>robotics tasks, with and without reinforcement learning. This also includes<br>end-to-end approaches, where a computer plays a video game like humans do: by<br>looking at the image displayed on the screen, and sending keystrokes to the<br>game. As a general approach to playing video games, this has many inviting<br>properties: no need for specialized modifications to the game, no lengthy<br>training sessions and the ability to re-use the same tools across different<br>games. However, related work includes game-specific engineering to achieve the<br>results. We take a step towards a general approach and study the general<br>applicability of behavioural cloning on twelve video games, including six<br>modern video games (published after 2010), by using human demonstrations as<br>training data. Our results show that these agents cannot match humans in raw<br>performance but do learn basic dynamics and rules. We also demonstrate how the<br>quality of the data matters, and how recording data from humans is subject to a<br>state-action mismatch, due to human reflexes.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¡Œä¸ºå…‹éš†æ˜¯ä¸€ç§åŸºäºäººç±»æ¼”ç¤ºæ¥è®­ç»ƒè®¡ç®—æœºæ‰§è¡Œä»»åŠ¡çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚å®ƒå·²è¢«æˆåŠŸåº”ç”¨äºå„ç§è§†é¢‘æ¸¸æˆå’Œæœºå™¨äººä»»åŠ¡ï¼ŒåŒ…æ‹¬ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå…¶ä¸­è®¡ç®—æœºé€šè¿‡è§‚å¯Ÿå±å¹•ä¸Šçš„å›¾åƒå¹¶å‘é€æŒ‰é”®æ¥åƒäººç±»ä¸€æ ·ç©æ¸¸æˆã€‚è¿™ç§æ–¹æ³•å…·æœ‰è®¸å¤šå¸å¼•äººçš„ç‰¹æ€§ï¼Œä¾‹å¦‚æ— éœ€å¯¹æ¸¸æˆè¿›è¡Œç‰¹æ®Šä¿®æ”¹ã€æ— éœ€é•¿æ—¶é—´è®­ç»ƒä»¥åŠèƒ½å¤Ÿåœ¨ä¸åŒæ¸¸æˆä¹‹é—´é‡ç”¨ç›¸åŒå·¥å…·ã€‚ç„¶è€Œï¼Œç›¸å…³å·¥ä½œé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¸¸æˆè¿›è¡Œå·¥ç¨‹åŒ–æ‰èƒ½å–å¾—æˆæœã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶è¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„é€šç”¨æ€§ï¼Œå¹¶ä½¿ç”¨äººç±»æ¼”ç¤ºä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œåœ¨åäºŒæ¬¾è§†é¢‘æ¸¸æˆï¼ˆåŒ…æ‹¬å…­æ¬¾ç°ä»£è§†é¢‘æ¸¸æˆï¼‰ä¸Šè¿›è¡Œäº†ç ”ç©¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å­¦ä¹ äººç±»ç©å®¶çš„è¡Œä¸ºï¼Œå¹¶ä½¿ç”¨äººç±»æ¼”ç¤ºæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æ¨¡å‹é€šè¿‡è§‚å¯Ÿå±å¹•å›¾åƒæ¥é¢„æµ‹äººç±»ç©å®¶çš„åŠ¨ä½œï¼Œå¹¶ä½¿ç”¨è¿™äº›é¢„æµ‹æ¥æ§åˆ¶æ¸¸æˆã€‚æœ¬æ–‡è¿˜ç ”ç©¶äº†æ•°æ®è´¨é‡å’Œæ•°é‡å¯¹è¡Œä¸ºå…‹éš†æ€§èƒ½çš„å½±å“ï¼Œä»¥åŠäººç±»ååº”æ—¶é—´å»¶è¿Ÿå¯¹æ•°æ®è´¨é‡çš„å½±å“ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œè¡Œä¸ºå…‹éš†ä»£ç†å¯ä»¥å­¦ä¹ æ¸¸æˆçš„åŸºæœ¬åŠ¨æ€å’Œè§„åˆ™ï¼Œä½†é€šå¸¸åªèƒ½è¾¾åˆ°äººç±»ç©å®¶æ€§èƒ½çš„ä¸€å°éƒ¨åˆ†ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¦‚éšæœºä»£ç†ã€‚æœ¬æ–‡è¿˜å‘ç°ï¼Œå½“åªæœ‰å°‘é‡æ•°æ®å¯ç”¨æ—¶ï¼Œæ•°æ®æ•°é‡å¯¹ç»“æœçš„å½±å“è¾ƒå°ï¼Œè€Œè°ƒæ•´äººç±»ååº”æ—¶é—´å»¶è¿Ÿå¯ä»¥æé«˜æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨å…·æœ‰æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºçš„å»ºè®®åŒ…æ‹¬ä½¿ç”¨é«˜è´¨é‡æ•°æ®ã€è°ƒæ•´äººç±»ååº”æ—¶é—´å»¶è¿Ÿä»¥åŠæ¢ç´¢å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜è¡Œä¸ºå…‹éš†çš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Design, Benchmarking and Explainability Analysis of a Game-Theoretic Framework towards Energy Efficiency in Smart Infrastructure</td>
      <td>In this paper, we propose a gamification approach as a novel framework for<br>smart building infrastructure with the goal of motivating human occupants to<br>reconsider personal energy usage and to have positive effects on their<br>environment. Human interaction in the context of cyber-physical systems is a<br>core component and consideration in the implementation of any smart building<br>technology. Research has shown that the adoption of human-centric building<br>services and amenities leads to improvements in the operational efficiency of<br>these cyber-physical systems directed towards controlling building energy<br>usage. We introduce a strategy in form of a game-theoretic framework that<br>incorporates humans-in-the-loop modeling by creating an interface to allow<br>building managers to interact with occupants and potentially incentivize energy<br>efficient behavior. Prior works on game theoretic analysis typically rely on<br>the assumption that the utility function of each individual agent is known a<br>priori. Instead, we propose novel utility learning framework for benchmarking<br>that employs robust estimations of occupant actions towards energy efficiency.<br>To improve forecasting performance, we extend the utility learning scheme by<br>leveraging deep bi-directional recurrent neural networks. Using the proposed<br>methods on data gathered from occupant actions for resources such as room<br>lighting, we forecast patterns of energy resource usage to demonstrate the<br>prediction performance of the methods. The results of our study show that we<br>can achieve a highly accurate representation of the ground truth for occupant<br>energy resource usage. We also demonstrate the explainable nature on human<br>decision making towards energy usage inherent in the dataset using graphical<br>lasso and granger causality algorithms. Finally, we open source the<br>de-identified, high-dimensional data pertaining to the energy game-theoretic<br>framework.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆåŒ–æ™ºèƒ½å»ºç­‘ï¼šæ¿€åŠ±èŠ‚èƒ½è¡Œä¸ºçš„æ–°æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å…¨çƒèƒ½æºæ¶ˆè€—çš„æŒç»­å¢é•¿ï¼Œå»ºç­‘èƒ½è€—å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„å…³æ³¨ç‚¹ã€‚å°½ç®¡æ™ºèƒ½å»ºç­‘æŠ€æœ¯æ—¨åœ¨æé«˜èƒ½æºæ•ˆç‡ï¼Œä½†äººç±»è¡Œä¸ºçš„ä¸ç¡®å®šæ€§å¾€å¾€å¯¼è‡´èƒ½æºæ¶ˆè€—çš„ä¸å¯é¢„æµ‹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåšå¼ˆè®ºçš„æ¸¸æˆåŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æ¿€åŠ±å»ºç­‘ä½¿ç”¨è€…é‡æ–°è€ƒè™‘ä¸ªäººèƒ½æºä½¿ç”¨ï¼Œä»è€Œå¯¹ç¯å¢ƒäº§ç”Ÿç§¯æå½±å“ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåšå¼ˆè®ºæ¡†æ¶<br>æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåšå¼ˆè®ºæ¡†æ¶ï¼Œå°†å»ºç­‘ä½¿ç”¨è€…è§†ä¸ºéåˆä½œåšå¼ˆä¸­çš„æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ›å»ºä¸€ä¸ªç•Œé¢ï¼Œå…è®¸å»ºç­‘ç®¡ç†è€…ä¸ä½¿ç”¨è€…äº’åŠ¨ï¼Œå¹¶å¯èƒ½æ¿€åŠ±ä»–ä»¬é‡‡å–èŠ‚èƒ½è¡Œä¸ºã€‚ä¸ä»¥å¾€åšå¼ˆè®ºåˆ†æé€šå¸¸å‡è®¾æ¯ä¸ªæ™ºèƒ½ä½“çš„æ•ˆç”¨å‡½æ•°æ˜¯é¢„å…ˆå·²çŸ¥çš„ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•ˆç”¨å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåŸºå‡†æµ‹è¯•ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¯¹èŠ‚èƒ½è¡Œä¸ºçš„ occupant è¡ŒåŠ¨è¿›è¡Œé²æ£’ä¼°è®¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ·±åº¦åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ<br>ä¸ºäº†æé«˜é¢„æµ‹æ€§èƒ½ï¼Œæœ¬æ–‡æ‰©å±•äº†æ•ˆç”¨å­¦ä¹ æ–¹æ¡ˆï¼Œåˆ©ç”¨æ·±åº¦åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€‚ä½¿ç”¨ä» occupant è¡ŒåŠ¨ä¸­æ”¶é›†çš„æ•°æ®ï¼Œä¾‹å¦‚æˆ¿é—´ç…§æ˜ï¼Œé¢„æµ‹èƒ½æºèµ„æºä½¿ç”¨æ¨¡å¼ï¼Œä»¥å±•ç¤ºæ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå¯ä»¥å®ç° occupant èƒ½æºèµ„æºä½¿ç”¨çš„é«˜åº¦å‡†ç¡®è¡¨ç¤ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¯è§£é‡Šæ€§åˆ†æ<br>æœ¬æ–‡ä½¿ç”¨å›¾å½¢æ‹‰ç´¢å’Œæ ¼å…°æ°å› æœç®—æ³•ï¼Œå±•ç¤ºäº†æ•°æ®é›†ä¸­å›ºæœ‰çš„å¯¹äººç±»å†³ç­–åˆ¶å®šçš„å¯è§£é‡Šæ€§ã€‚è¿™äº›ç®—æ³•æ­ç¤ºäº†ä¸åŒç‰¹å¾ä¹‹é—´çš„æ½œåœ¨æ¡ä»¶ä¾èµ–å…³ç³»ï¼Œä»¥åŠç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ï¼Œä»è€ŒåŠ æ·±äº†å¯¹äººç±»èƒ½æºä½¿ç”¨å†³ç­–åˆ¶å®šçš„ç†è§£ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ•ˆç”¨å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ occupant çš„èƒ½æºä½¿ç”¨è¡Œä¸ºï¼Œå¹¶ä¸”æ·±åº¦åŒå‘ RNN æ¨¡å‹åœ¨é¢„æµ‹æ€§èƒ½æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¯è§£é‡Šæ€§åˆ†ææ­ç¤ºäº†ä¸åŒèƒ½æºæ•ˆç‡è¡Œä¸ºç±»åˆ«ä¸­ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ï¼Œä»è€ŒåŠ æ·±äº†å¯¹äººç±»èƒ½æºä½¿ç”¨å†³ç­–åˆ¶å®šçš„ç†è§£ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŒ–æ™ºèƒ½å»ºç­‘æ¡†æ¶ä¸ºæé«˜å»ºç­‘èƒ½æºæ•ˆç‡æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¿€åŠ± occupant é‡‡å–èŠ‚èƒ½è¡Œä¸ºï¼Œå¹¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯é¢„æµ‹ occupant çš„èƒ½æºä½¿ç”¨è¡Œä¸ºï¼Œä»è€Œä¸ºæ™ºèƒ½å»ºç­‘ç®¡ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¯è§£é‡Šæ€§åˆ†ææ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£äººç±»èƒ½æºä½¿ç”¨å†³ç­–åˆ¶å®šï¼Œå¹¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„æ¿€åŠ±æªæ–½æä¾›ä¾æ®ã€‚</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Quantum Poker A game for quantum computers suitable for benchmarking error mitigation techniques on NISQ devices</td>
      <td>Quantum computers are on the verge of becoming a commercially available<br>reality. They represent a paradigm shift in computing, with a steep learning<br>gradient. The creation of games is a way to ease the transition for beginners.<br>We present a game similar to the Poker variant Texas hold 'em with the<br>intention to serve as an engaging pedagogical tool to learn the basics rules of<br>quantum computing. The concepts of quantum states, quantum operations and<br>measurement can be learned in a playful manner. The difference to the classical<br>variant is that the community cards are replaced by a quantum register that is<br>"randomly" initialized, and the cards for each player are replaced by quantum<br>gates, randomly drawn from a set of available gates. Each player can create a<br>quantum circuit with their cards, with the aim to maximize the number of \( 1 \)'s<br>that are measured in the computational basis. The basic concepts of<br>superposition, entanglement and quantum gates are employed. We provide a<br>proof-of-concept implementation using Qiskit. A comparison of the results for<br>the created circuits using a simulator and IBM machines is conducted, showing<br>that error rates on contemporary quantum computers are still very high. For the<br>success of noisy intermediate scale quantum (NISQ) computers, improvements on<br>the error rates and error mitigation techniques are necessary, even for simple<br>circuits. We show that quantum error mitigation (QEM) techniques can be used to<br>improve expectation values of observables on real quantum devices.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é‡å­æ‰‘å…‹ï¼šåœ¨NISQè®¾å¤‡ä¸Šæµ‹è¯•é”™è¯¯ç¼“è§£æŠ€æœ¯çš„æ¸¸æˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>é‡å­è®¡ç®—æœºæ­£é€æ¸æˆä¸ºå•†ä¸šç°å®ï¼Œä½†å®ƒä»¬çš„å­¦ä¹ æ›²çº¿é™¡å³­ï¼Œéœ€è¦è·¨å­¦ç§‘çš„çŸ¥è¯†ã€‚ä¸ºäº†å¸®åŠ©åˆå­¦è€…æ›´å®¹æ˜“åœ°å­¦ä¹ é‡å­è®¡ç®—çš„åŸºæœ¬è§„åˆ™ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œé‡å­æ‰‘å…‹â€çš„æ¸¸æˆã€‚è¯¥æ¸¸æˆæ—¨åœ¨é€šè¿‡æ¸¸æˆåŒ–çš„æ–¹å¼æ•™æˆé‡å­çŠ¶æ€ã€é‡å­æ“ä½œå’Œæµ‹é‡çš„æ¦‚å¿µã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡å­æ‰‘å…‹æ¸¸æˆ<br>é‡å­æ‰‘å…‹æ¸¸æˆç±»ä¼¼äºå¾·å·æ‰‘å…‹ï¼Œä½†ç¤¾åŒºç‰Œè¢«é‡å­å¯„å­˜å™¨å–ä»£ï¼Œç©å®¶æ‰‹ä¸­çš„ç‰Œè¢«é‡å­é—¨å–ä»£ã€‚æ¯ä¸ªç©å®¶éƒ½å¯ä»¥ä½¿ç”¨ä»–ä»¬çš„ç‰Œåˆ›å»ºä¸€ä¸ªé‡å­ç”µè·¯ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–åœ¨è®¡ç®—åŸºä¸­æµ‹é‡åˆ°çš„â€œ1â€çš„æ•°é‡ã€‚æ¸¸æˆåˆ©ç”¨äº†å åŠ ã€çº ç¼ å’Œé‡å­é—¨çš„åŸºæœ¬æ¦‚å¿µã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé”™è¯¯ç¼“è§£æŠ€æœ¯<br>æœ¬æ–‡ä½¿ç”¨é›¶å™ªå£°å¤–æ¨æ–¹æ³•æ¥ç¼“è§£é‡å­è®¡ç®—æœºä¸Šçš„é”™è¯¯ã€‚é€šè¿‡æ•…æ„æ”¾å¤§å™ªå£°å¹¶ä½¿ç”¨å¤–æ¨æŠ€æœ¯ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ä¼°è®¡å¯è§‚æµ‹é‡åœ¨çœŸå®é‡å­è®¾å¤‡ä¸Šçš„æœŸæœ›å€¼ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ç®€å•çš„ç”µè·¯ä¸­ï¼Œå½“ä»£é‡å­è®¡ç®—æœºçš„é”™è¯¯ç‡ä»ç„¶å¾ˆé«˜ã€‚ä½¿ç”¨é”™è¯¯ç¼“è§£æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æœŸæœ›å€¼çš„ä¼°è®¡ç²¾åº¦ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>é‡å­æ‰‘å…‹æ¸¸æˆå¯ä»¥ä½œä¸ºå­¦ä¹ é‡å­è®¡ç®—åŸºæœ¬æ¦‚å¿µçš„æœ‰ç›Šå·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„é”™è¯¯ç¼“è§£æŠ€æœ¯å¯ä»¥åº”ç”¨äºå…¶ä»–é‡å­è®¡ç®—ä»»åŠ¡ï¼Œä»¥æé«˜ç»“æœçš„å‡†ç¡®æ€§ã€‚</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks</td>
      <td>While Nash equilibrium in extensive-form games is well understood, very<br>little is known about the properties of extensive-form correlated equilibrium<br>(EFCE), both from a behavioral and from a computational point of view. In this<br>setting, the strategic behavior of players is complemented by an external<br>device that privately recommends moves to agents as the game progresses;<br>players are free to deviate at any time, but will then not receive future<br>recommendations. Our contributions are threefold. First, we show that an EFCE<br>can be formulated as the solution to a bilinear saddle-point problem. To<br>showcase how this novel formulation can inspire new algorithms to compute<br>EFCEs, we propose a simple subgradient descent method which exploits this<br>formulation and structural properties of EFCEs. Our method has better<br>scalability than the prior approach based on linear programming. Second, we<br>propose two benchmark games, which we hope will serve as the basis for future<br>evaluation of EFCE solvers. These games were chosen so as to cover two natural<br>application domains for EFCE: conflict resolution via a mediator, and<br>bargaining and negotiation. Third, we document the qualitative behavior of EFCE<br>in our proposed games. We show that the social-welfare-maximizing equilibria in<br>these games are highly nontrivial and exhibit surprisingly subtle sequential<br>behavior that so far has not received attention in the literature.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±å…¥ç†è§£æ‰©å±•å¼æ¸¸æˆä¸­çš„ç›¸å…³å‡è¡¡ï¼šéç‚¹å…¬å¼ä¸åŸºå‡†æµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ‰©å±•å¼æ¸¸æˆï¼ˆEFGsï¼‰æ˜¯åšå¼ˆè®ºä¸­çš„ä¸€ç§é‡è¦ç±»å‹ï¼Œå®ƒæè¿°äº†ç©å®¶åœ¨ä¸€ç³»åˆ—å†³ç­–ç‚¹ä¸Šçš„äº¤äº’è¿‡ç¨‹ã€‚åœ¨æ‰©å±•å¼æ¸¸æˆä¸­ï¼Œçº³ä»€å‡è¡¡ï¼ˆNEï¼‰æ˜¯ä¸€ä¸ªè¢«å¹¿æ³›ç ”ç©¶çš„è§£æ¦‚å¿µï¼Œå®ƒå‡è®¾æ¯ä¸ªç©å®¶éƒ½æ˜¯è‡ªç§çš„ï¼Œå¹¶è¯•å›¾æœ€å¤§åŒ–è‡ªå·±çš„æ”¶ç›Šã€‚ç„¶è€Œï¼Œçº³ä»€å‡è¡¡å¯èƒ½æ— æ³•è¾¾åˆ°ç¤¾ä¼šæœ€ä¼˜ï¼Œå› ä¸ºç©å®¶çš„è‡ªç§è¡Œä¸ºå¯èƒ½å¯¼è‡´æ•´ä½“æ”¶ç›Šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†æ‰©å±•å¼ç›¸å…³å‡è¡¡ï¼ˆEFCEï¼‰çš„æ¦‚å¿µï¼Œå®ƒå…è®¸ä¸€ä¸ªå¤–éƒ¨è®¾å¤‡ï¼ˆä¸­ä»‹ï¼‰å‘ç©å®¶æä¾›æ¨èåŠ¨ä½œï¼Œä½†ç©å®¶å¯ä»¥è‡ªç”±é€‰æ‹©æ˜¯å¦éµå¾ªè¿™äº›æ¨èã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†EFCEé—®é¢˜è½¬åŒ–ä¸ºéç‚¹é—®é¢˜<br>æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®ä¹‹ä¸€æ˜¯å°†EFCEé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªåŒçº¿æ€§éç‚¹é—®é¢˜ï¼ˆBSPPï¼‰ã€‚è¿™ç§æ–°çš„å…¬å¼åŒ–æ–¹æ³•ä¸ºå¼€å‘æ–°çš„ç®—æ³•æ¥è®¡ç®—EFCEæä¾›äº†åŸºç¡€ã€‚ä¸ºäº†å±•ç¤ºè¿™ç§æ–°å…¬å¼çš„æ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„å­æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†EFCEçš„å…¬å¼å’Œç»“æ„ç‰¹æ€§ã€‚ä¸åŸºäºçº¿æ€§è§„åˆ’çš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºä¸¤ä¸ªåŸºå‡†æ¸¸æˆ<br>ä¸ºäº†è¯„ä¼°EFCEæ±‚è§£å™¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªåŸºå‡†æ¸¸æˆï¼šæˆ˜èˆ°æ¸¸æˆå’Œè­¦é•¿æ¸¸æˆã€‚è¿™äº›æ¸¸æˆè¢«è®¾è®¡ä¸ºå¯å‚æ•°åŒ–çš„ï¼Œä»¥ä¾¿å¯ä»¥æ‰©å±•åˆ°ä¸åŒçš„å¤§å°ã€‚æˆ˜èˆ°æ¸¸æˆæ¨¡æ‹Ÿäº†é€šè¿‡è°ƒè§£å‘˜è¿›è¡Œå†²çªè§£å†³çš„åœºæ™¯ï¼Œè€Œè­¦é•¿æ¸¸æˆæ¨¡æ‹Ÿäº†è°ˆåˆ¤å’Œåå•†çš„åœºæ™¯ã€‚é€šè¿‡åˆ†æè¿™äº›æ¸¸æˆä¸­çš„EFCEï¼Œæœ¬æ–‡å‘ç°å³ä½¿è°ƒè§£å‘˜æ— æ³•å¼ºåˆ¶æ‰§è¡Œè¡Œä¸ºï¼Œå®ƒä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ˜¾è‘—æé«˜ç¤¾ä¼šç¦åˆ©å¹¶é˜»æ­¢ç©å®¶åç¦»æ¨èï¼š<br>1. ä½¿ç”¨ç‰¹å®šåŠ¨ä½œåºåˆ—ä½œä¸ºâ€œå¯†ç â€æ¥éªŒè¯ç©å®¶æ˜¯å¦åç¦»ï¼šåç¦»ä¼šå¯¼è‡´ä¸å®Œæ•´æˆ–é”™è¯¯çš„å¯†ç ï¼Œä»è€Œè¡¨æ˜ç©å®¶å·²ç»åç¦»ã€‚<br>2. å¦‚æœæ£€æµ‹åˆ°åç¦»ï¼Œè¯±å¯¼å¯¹æ‰‹å¯¹åç¦»æ¨èçš„ç©å®¶é‡‡å–æƒ©ç½šæ€§è¡ŒåŠ¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡å®éªŒéªŒè¯äº†åŸºäºBSPPå…¬å¼çš„å­æ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨å¤§å‹æ¸¸æˆå®ä¾‹ä¸­ä¼˜äºåŸºäºçº¿æ€§è§„åˆ’çš„å…ˆå‰æ–¹æ³•ã€‚è¿™è¡¨æ˜BSPPå…¬å¼åœ¨è®¡ç®—EFCEæ–¹é¢å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„BSPPå…¬å¼å’Œå­æ¢¯åº¦ä¸‹é™æ–¹æ³•ä¸ºè®¡ç®—EFCEæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå¼€å‘æ›´é«˜æ•ˆçš„EFCEæ±‚è§£å™¨å¥ å®šäº†åŸºç¡€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºå‡†æ¸¸æˆä¸ºè¯„ä¼°EFCEæ±‚è§£å™¨æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚è¿™äº›å‘ç°å¯¹äºç†è§£å’Œåº”ç”¨EFCEåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¾‹å¦‚äº¤é€šæ§åˆ¶ã€è´Ÿè½½å‡è¡¡å’Œç¢³å‡æ’ç­‰é¢†åŸŸã€‚</td>
    </tr>
    <tr>
      <th>34</th>
      <td>High-Level Representation of Benchmark Families for Petri Games</td>
      <td>Petri games have been introduced as a multi-player game model representing<br>causal memory to address the synthesis of distributed systems. For Petri games<br>with one environment player and an arbitrary bounded number of system players,<br>deciding the existence of a safety strategy is EXPTIME-complete. This result<br>forms the basis of the tool ADAM that implements an algorithm for the synthesis<br>of distributed controllers from Petri games. To evaluate the tool, it has been<br>checked on a series of parameterized benchmarks from manufacturing and workflow<br>scenarios. In this paper, we introduce a new possibility to represent benchmark<br>families for the distributed synthesis problem modeled with Petri games. It<br>enables the user to specify an entire benchmark family as one parameterized<br>high-level net. We describe example benchmark families as a high-level version<br>of a Petri game and exhibit an instantiation yielding a concrete 1-bounded<br>Petri game. We identify improvements either regarding the size or the<br>functionality of the benchmark families by examining the high-level Petri<br>games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜çº§Petriæ¸¸æˆåŸºå‡†å®¶æ—è¡¨ç¤º<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>Petriæ¸¸æˆä½œä¸ºä¸€ç§å¤šç©å®¶æ¸¸æˆæ¨¡å‹ï¼Œè¢«å¼•å…¥ç”¨äºè¡¨ç¤ºå› æœè®°å¿†ï¼Œä»¥è§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»¼åˆé—®é¢˜ã€‚å¯¹äºå…·æœ‰ä¸€ä¸ªç¯å¢ƒç©å®¶å’Œä»»æ„æœ‰ç•Œæ•°é‡çš„ç³»ç»Ÿç©å®¶çš„Petriæ¸¸æˆï¼Œå†³å®šå®‰å…¨ç­–ç•¥çš„å­˜åœ¨æ€§æ˜¯EXPTIMEå®Œå…¨çš„ã€‚è¿™ä¸€ç»“æœæ„æˆäº†å®ç°åˆ†å¸ƒå¼æ§åˆ¶å™¨ç»¼åˆç®—æ³•çš„å·¥å…·ADAMçš„åŸºç¡€ã€‚ä¸ºäº†è¯„ä¼°è¯¥å·¥å…·ï¼Œå®ƒå·²åœ¨ä¸€ç³»åˆ—æ¥è‡ªåˆ¶é€ å’Œå·¥ä½œæµç¨‹åœºæ™¯çš„å‚æ•°åŒ–åŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å¯èƒ½æ€§ï¼Œå³ç”¨Petriæ¸¸æˆè¡¨ç¤ºåˆ†å¸ƒå¼ç»¼åˆé—®é¢˜çš„åŸºå‡†å®¶æ—ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿå°†æ•´ä¸ªåŸºå‡†å®¶æ—æŒ‡å®šä¸ºä¸€ä¸ªå‚æ•°åŒ–çš„é«˜çº§ç½‘ç»œã€‚æˆ‘ä»¬æè¿°äº†ä½œä¸ºPetriæ¸¸æˆé«˜çº§ç‰ˆæœ¬çš„ç¤ºä¾‹åŸºå‡†å®¶æ—ï¼Œå¹¶å±•ç¤ºäº†äº§ç”Ÿå…·ä½“1æœ‰ç•ŒPetriæ¸¸æˆçš„å®ä¾‹åŒ–ã€‚é€šè¿‡æ£€æŸ¥é«˜çº§Petriæ¸¸æˆï¼Œæˆ‘ä»¬ç¡®å®šäº†åŸºå‡†å®¶æ—åœ¨å¤§å°æˆ–åŠŸèƒ½æ–¹é¢çš„æ”¹è¿›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜çº§Petriæ¸¸æˆè¡¨ç¤º<br>æœ¬æ–‡æå‡ºäº†åŸºäºé«˜çº§Petriç½‘çš„æ–°å‚æ•°åŒ–é«˜çº§è¡¨ç¤ºï¼Œç”¨äºç®€æ´å’Œæ¸…æ™°åœ°å®šä¹‰åŸºå‡†å®¶æ—ã€‚é«˜çº§Petriç½‘å…è®¸ç”¨æˆ·ä½¿ç”¨å…·æœ‰é¢„å®šä¹‰å‚æ•°å¤§å°çš„åŸŸä¸­çš„å•ä¸ªä»¤ç‰Œã€å…·æœ‰æ¡ä»¶æ ‡ç­¾çš„è½¬æ¢å’Œå…·æœ‰è¡¨è¾¾å¼æ ‡ç­¾çš„å¼§æ¥æŒ‡å®šæ•´ä¸ªåŸºå‡†å®¶æ—ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ä¾‹åŒ–æŠ€æœ¯<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ä¾‹åŒ–æŠ€æœ¯ï¼Œå¯ä»¥å°†å‚æ•°åŒ–é«˜çº§Petriæ¸¸æˆè½¬æ¢ä¸ºæ ‡å‡†1æœ‰ç•ŒPetriæ¸¸æˆã€‚è¿™ä½¿å¾—ç°æœ‰çš„ç®—æ³•å’Œå·¥å…·å¯ä»¥è§£å†³è¿™äº›Petriæ¸¸æˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡ä¸¤ä¸ªç¤ºä¾‹åŸºå‡†å®¶æ—ï¼ˆå¹¶å‘æœºå™¨å’Œè‡ªé‡æ„æœºå™¨äººï¼‰å±•ç¤ºäº†é«˜çº§Petriæ¸¸æˆè¡¨ç¤ºçš„åº”ç”¨ã€‚ä¸åŸå§‹å®ç°ç›¸æ¯”ï¼Œè¿™äº›åŸºå‡†å®¶æ—åœ¨å¤§å°æˆ–åŠŸèƒ½æ–¹é¢å¾—åˆ°äº†æ”¹è¿›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„é«˜çº§Petriæ¸¸æˆè¡¨ç¤ºæ–¹æ³•ä¸ºåˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»¼åˆé—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å…·æœ‰ç®€æ´ã€æ¸…æ™°å’Œå¯æ‰©å±•çš„ç‰¹ç‚¹ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»¼åˆé—®é¢˜ã€‚</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks</td>
      <td>From the beginning if the history of AI, there has been interest in games as<br>a platform of research. As the field developed, human-level competence in<br>complex games became a target researchers worked to reach. Only relatively<br>recently has this target been finally met for traditional tabletop games such<br>as Backgammon, Chess and Go. Current research focus has shifted to electronic<br>games, which provide unique challenges. As is often the case with AI research,<br>these results are liable to be exaggerated or misrepresented by either authors<br>or third parties. The extent to which these games benchmark consist of fair<br>competition between human and AI is also a matter of debate. In this work, we<br>review the statements made by authors and third parties in the general media<br>and academic circle about these game benchmark results and discuss factors that<br>can impact the perception of fairness in the contest between humans and<br>machines</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¹³å°å…¬å¹³æ€§ï¼šäººå·¥æ™ºèƒ½ä¸äººç±»æ¸¸æˆåŸºå‡†çš„å…¬å¹³æ€§<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è‡ªäººå·¥æ™ºèƒ½è¯ç”Ÿä»¥æ¥ï¼Œæ¸¸æˆä¸€ç›´æ˜¯å…¶ç ”ç©¶å’Œæµ‹è¯•çš„å¹³å°ã€‚éšç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ï¼Œäººç±»åœ¨å¤æ‚æ¸¸æˆä¸­çš„æ°´å¹³æˆä¸ºäº†ç ”ç©¶äººå‘˜è¿½æ±‚çš„ç›®æ ‡ã€‚è¿‘å¹´æ¥ï¼Œè¿™ä¸€ç›®æ ‡åœ¨ä¼ ç»Ÿæ¡Œé¢æ¸¸æˆå¦‚å›½é™…è±¡æ£‹ã€å›´æ£‹ç­‰ä¸­å¾—ä»¥å®ç°ã€‚ç„¶è€Œï¼Œéšç€ç ”ç©¶çš„æ·±å…¥ï¼Œç”µå­æ¸¸æˆå› å…¶ç‹¬ç‰¹çš„æŒ‘æˆ˜æ€§è€Œæˆä¸ºæ–°çš„ç ”ç©¶ç„¦ç‚¹ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œå¹¶åˆ†æå½±å“å…¬å¹³æ€§çš„å› ç´ ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡é¦–å…ˆå›é¡¾äº†äººå·¥æ™ºèƒ½åœ¨ç»å…¸æ¡Œé¢æ¸¸æˆï¼ˆå¦‚å›½é™…è±¡æ£‹ã€å›´æ£‹ï¼‰å’Œç°ä»£ç”µå­æ¸¸æˆï¼ˆå¦‚æ˜Ÿé™…äº‰éœ¸ã€Dota 2ï¼‰ä¸­çš„åŸºå‡†æµ‹è¯•æˆæœï¼Œå¹¶åˆ†æäº†è¿™äº›æˆæœåœ¨åª’ä½“å’Œå­¦æœ¯ç•Œçš„è®¨è®ºã€‚æ¥ç€ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¬å¹³æ€§ç»´åº¦åˆ†ç±»æ³•ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆç«èµ›ä¸­çš„å…¬å¹³æ€§ã€‚æœ€åï¼Œæœ¬æ–‡è®ºè¯äº†åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œäººå·¥æ™ºèƒ½ä¸äººç±»ä¹‹é—´ä¸å­˜åœ¨å®Œå…¨å…¬å¹³çš„æ¯”è¾ƒæ–¹å¼ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡åˆ†æäººå·¥æ™ºèƒ½åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå‘ç°ä»¥ä¸‹é—®é¢˜ï¼š<br>1. äººå·¥æ™ºèƒ½åœ¨æ¸¸æˆä¸­çš„è¡¨ç°å¾€å¾€è¢«å¤¸å¤§æˆ–è¯¯è§£ã€‚<br>2. äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­çš„å…¬å¹³æ€§å­˜åœ¨äº‰è®®ã€‚<br>3. äººå·¥æ™ºèƒ½åœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿ï¼ˆå¦‚ååº”é€Ÿåº¦ã€ä¿¡æ¯è·å–ç­‰ï¼‰å¯èƒ½è¢«è¯¯è§£ä¸ºä¸å…¬å¹³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„å…¬å¹³æ€§ç»´åº¦åˆ†ç±»æ³•ä¸ºè¯„ä¼°äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆç«èµ›ä¸­çš„å…¬å¹³æ€§æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†äººå·¥æ™ºèƒ½åœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿å¯èƒ½è¢«è¯¯è§£ä¸ºä¸å…¬å¹³ï¼Œå¹¶å‘¼åç ”ç©¶äººå‘˜å…³æ³¨è¿™ä¸€é—®é¢˜ã€‚</td>
    </tr>
    <tr>
      <th>36</th>
      <td>Marathon Environments: Multi-Agent Continuous Control Benchmarks in a Modern Video Game Engine</td>
      <td>Recent advances in deep reinforcement learning in the paradigm of locomotion<br>using continuous control have raised the interest of game makers for the<br>potential of digital actors using active ragdoll. Currently, the available<br>options to develop these ideas are either researchers' limited codebase or<br>proprietary closed systems. We present Marathon Environments, a suite of open<br>source, continuous control benchmarks implemented on the Unity game engine,<br>using the Unity ML- Agents Toolkit. We demonstrate through these benchmarks<br>that continuous control research is transferable to a commercial game engine.<br>Furthermore, we exhibit the robustness of these environments by reproducing<br>advanced continuous control research, such as learning to walk, run and<br>backflip from motion capture data; learning to navigate complex terrains; and<br>by implementing a video game input control system. We show further robustness<br>by training with alternative algorithms found in OpenAI.Baselines. Finally, we<br>share strategies for significantly reducing the training time.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Marathon Environmentsï¼šå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åº”ç”¨äºç°ä»£æ¸¸æˆå¼•æ“çš„æŒç»­æ§åˆ¶åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æŒç»­æ§åˆ¶é¢†åŸŸçš„å‘å±•ï¼Œæ¸¸æˆå¼€å‘è€…å¯¹ä½¿ç”¨ä¸»åŠ¨ragdollçš„æ•°å­—è§’è‰²äº§ç”Ÿäº†å…´è¶£ã€‚ç„¶è€Œï¼Œç›®å‰å¼€å‘è¿™äº›æƒ³æ³•çš„é€‰é¡¹è¦ä¹ˆæ˜¯ç ”ç©¶äººå‘˜æœ‰é™çš„ä»£ç åº“ï¼Œè¦ä¹ˆæ˜¯ä¸“æœ‰çš„å°é—­ç³»ç»Ÿã€‚æœ¬æ–‡æå‡ºäº†Marathon Environmentsï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æŒç»­æ§åˆ¶åŸºå‡†å¥—ä»¶ï¼Œåœ¨Unityæ¸¸æˆå¼•æ“ä¸Šå®ç°ï¼Œä½¿ç”¨Unity ML-Agents Toolkitã€‚é€šè¿‡è¿™äº›åŸºå‡†ï¼Œæœ¬æ–‡å±•ç¤ºäº†æŒç»­æ§åˆ¶ç ”ç©¶å¯ä»¥è½¬ç§»åˆ°å•†ä¸šæ¸¸æˆå¼•æ“ï¼Œå¹¶å±•ç¤ºäº†è¿™äº›ç¯å¢ƒçš„é²æ£’æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMarathon Environmentsæ˜¯ä¸€ä¸ªå¼€æºçš„æŒç»­æ§åˆ¶åŸºå‡†å¥—ä»¶ï¼Œåœ¨Unityæ¸¸æˆå¼•æ“ä¸Šå®ç°ï¼Œä½¿ç”¨Unity ML-Agents Toolkitã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è½»æ¾åœ°å°†æŒç»­æ§åˆ¶ç ”ç©¶åº”ç”¨äºå•†ä¸šæ¸¸æˆå¼•æ“ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å±•ç¤ºäº†Marathon Environmentsçš„é²æ£’æ€§ï¼Œé€šè¿‡å¤åˆ¶é«˜çº§æŒç»­æ§åˆ¶ç ”ç©¶ï¼Œä¾‹å¦‚ä»è¿åŠ¨æ•æ‰æ•°æ®ä¸­å­¦ä¹ è¡Œèµ°ã€è·‘æ­¥å’Œåç©ºç¿»ï¼›å­¦ä¹ å¯¼èˆªå¤æ‚åœ°å½¢ï¼›ä»¥åŠå®ç°è§†é¢‘æ¸¸æˆè¾“å…¥æ§åˆ¶ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ä½¿ç”¨OpenAI.Baselinesä¸­çš„æ›¿ä»£ç®—æ³•è¿›è¡Œè®­ç»ƒçš„é²æ£’æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡å®éªŒéªŒè¯äº†Marathon Environmentsçš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMarathon Environmentså¯ä»¥æˆåŠŸå¤åˆ¶é«˜çº§æŒç»­æ§åˆ¶ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä¸åŒç¯å¢ƒä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†äº«äº†ä¼˜åŒ–è®­ç»ƒæ—¶é—´çš„ç­–ç•¥ï¼Œä¾‹å¦‚å¢åŠ å¹¶å‘ä»£ç†æ•°é‡ã€ä½¿ç”¨æ— å¤´æ¨¡å¼ã€ä¿®æ”¹Academyè„šæœ¬ä»¥æ”¯æŒç‰©ç†æ­¥éª¤ç­‰ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Marathon Environmentsä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†ä¸€ä¸ªå¼€æºçš„æŒç»­æ§åˆ¶åŸºå‡†å¥—ä»¶ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå•†ä¸šæ¸¸æˆå¼•æ“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†äº«äº†ä¼˜åŒ–è®­ç»ƒæ—¶é—´çš„ç­–ç•¥ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æé«˜è®­ç»ƒæ•ˆç‡ã€‚</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Benchmarking Cognitive Abilities of the Brain with Computer Games</td>
      <td>Most of the players have experienced the feeling of temporarily losing their<br>character in a given gameplay situation when they cannot control the character,<br>simply because they temporarily cannot see it. The main reasons for this<br>feeling may be due to the interplay of the following factors: (1) the visual<br>complexity of the game is unexpectedly increased compared with the previous<br>time period as more and more game objects and effects are rendered on the<br>display; (2) and/or the game is lagging; (3) and finally, it is also possible<br>that the players have no sufficient experience with controlling the character.<br>This paper focuses on the first reason. We have developed a benchmark program<br>which allows its user to experience the feeling of losing character. While the<br>user can control the character well the benchmark program will increase the<br>visual complexity of the display. Otherwise, if the user lost the character<br>then the program will decrease the complexity until the user will find the<br>character again, and so on. The complexity is measured based on the number of<br>changed pixels between two consecutive display images. Our measurements show<br>that the average of bit per second values of losing and finding pairs describes<br>the user well. The final goal of this research is to further develop our<br>benchmark to a standard psychological test.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆä¸­çš„è®¤çŸ¥èƒ½åŠ›è¯„ä¼°ï¼šBrainB Test Series 6<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶æœ‰æ—¶ä¼šå› è§†è§‰å¤æ‚æ€§çš„å¢åŠ è€Œæš‚æ—¶å¤±å»å¯¹è§’è‰²çš„æ§åˆ¶ã€‚è¿™ç§ç°è±¡å¯èƒ½ç”±å¤šç§å› ç´ å¼•èµ·ï¼ŒåŒ…æ‹¬æ¸¸æˆè§†è§‰å¤æ‚æ€§çš„çªç„¶å¢åŠ ã€æ¸¸æˆå»¶è¿Ÿä»¥åŠç©å®¶å¯¹è§’è‰²æ§åˆ¶ç»éªŒçš„ä¸è¶³ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨è§†è§‰å¤æ‚æ€§å¢åŠ è¿™ä¸€å› ç´ ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåä¸º BrainB Test Series 6 çš„åŸºå‡†ç¨‹åºï¼Œç”¨äºæ¨¡æ‹Ÿç©å®¶å¤±å»è§’è‰²çš„æ„Ÿè§‰ï¼Œå¹¶è¯„ä¼°å…¶è®¤çŸ¥èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šBrainB Test Series 6 åŸºå‡†ç¨‹åº<br>è¯¥ç¨‹åºé€šè¿‡æ§åˆ¶å±å¹•ä¸Šç§»åŠ¨çš„æ–¹å—çš„è§†è§‰å¤æ‚æ€§æ¥æ¨¡æ‹Ÿç©å®¶å¤±å»è§’è‰²çš„æ„Ÿè§‰ã€‚å½“ç©å®¶èƒ½å¤Ÿå¾ˆå¥½åœ°æ§åˆ¶è§’è‰²æ—¶ï¼Œç¨‹åºä¼šå¢åŠ å±å¹•çš„è§†è§‰å¤æ‚æ€§ï¼›å¦‚æœç©å®¶å¤±å»äº†è§’è‰²ï¼Œç¨‹åºä¼šé™ä½å¤æ‚æ€§ï¼Œç›´åˆ°ç©å®¶å†æ¬¡æ‰¾åˆ°è§’è‰²ã€‚ç¨‹åºçš„å¤æ‚æ€§åŸºäºè¿ç»­æ˜¾ç¤ºå›¾åƒä¹‹é—´å˜åŒ–çš„åƒç´ æ•°é‡æ¥è¡¡é‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¤çŸ¥èƒ½åŠ›è¯„ä¼°<br>é€šè¿‡æµ‹é‡ç©å®¶åœ¨å¤±å»å’Œæ‰¾åˆ°è§’è‰²æ—¶å±å¹•å¤æ‚æ€§çš„å¹³å‡å€¼ï¼Œå¯ä»¥è¯„ä¼°ç©å®¶çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤±å»å’Œæ‰¾åˆ°è§’è‰²æ—¶å±å¹•å¤æ‚æ€§çš„å¹³å‡å€¼å¯ä»¥å¾ˆå¥½åœ°æè¿°ç©å®¶ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ç ”ç©¶äººå‘˜å¯¹ BrainB Test Series 6 è¿›è¡Œäº†åˆæ­¥æµ‹è¯•ï¼Œå¹¶å‘ç°ç©å®¶åœ¨å±å¹•å¤æ‚æ€§è¾ƒé«˜æ—¶æ›´å®¹æ˜“å¤±å»è§’è‰²ï¼Œè€Œåœ¨å±å¹•å¤æ‚æ€§è¾ƒä½æ—¶æ›´å®¹æ˜“æ‰¾åˆ°è§’è‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å‘ç°ï¼Œé€šè¿‡è°ƒæ•´é¼ æ ‡è®¾ç½®å’Œé¢œè‰²æ–¹æ¡ˆï¼Œå¯ä»¥æé«˜æµ‹è¯•çš„å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ BrainB Test Series 6 åŸºå‡†ç¨‹åºä¸ºè¯„ä¼°ç©å®¶çš„è®¤çŸ¥èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥ç”¨äºç”µå­ç«æŠ€äººæ‰é€‰æ‹”ã€è®¤çŸ¥èƒ½åŠ›ç ”ç©¶ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†æµ‹è¯•ç¯å¢ƒæ ‡å‡†åŒ–å’Œæµ‹è¯•ç»“æœåˆ†æçš„é‡è¦æ€§ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>38</th>
      <td>Game-theoretic approach to risk-sensitive benchmarked asset management</td>
      <td>In this article we consider a game theoretic approach to the Risk-Sensitive<br>Benchmarked Asset Management problem (RSBAM) of Davis and Lleo \cite{DL}. In<br>particular, we consider a stochastic differential game between two players,<br>namely, the investor who has a power utility while the second player represents<br>the market which tries to minimize the expected payoff of the investor. The<br>market does this by modulating a stochastic benchmark that the investor needs<br>to outperform. We obtain an explicit expression for the optimal pair of<br>strategies as for both the players.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆç†è®ºè§†è§’ä¸‹çš„é£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨èµ„äº§ç®¡ç†é¢†åŸŸï¼ŒæŠ•èµ„è€…å¾€å¾€éœ€è¦åœ¨è¿½æ±‚æŠ•èµ„å›æŠ¥å’Œè§„é¿é£é™©ä¹‹é—´åšå‡ºæƒè¡¡ã€‚ä¼ ç»Ÿçš„é£é™©æ•æ„Ÿæ§åˆ¶æ–¹æ³•è™½ç„¶èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šå¹³è¡¡è¿™ä¸¤è€…ï¼Œä½†åœ¨é¢å¯¹éšæœºåŸºå‡†æ—¶ï¼ŒæŠ•èµ„è€…çš„æœ€ä¼˜ç­–ç•¥å¯èƒ½ä¼šå—åˆ°å½±å“ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥æ¸¸æˆç†è®ºï¼Œæ„å»ºä¸€ä¸ªé£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜çš„æ¸¸æˆæ¨¡å‹ï¼Œä»¥æ­ç¤ºæŠ•èµ„è€…åœ¨é¢å¯¹å¸‚åœºå¯¹æŠ—æ—¶çš„æœ€ä¼˜ç­–ç•¥ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºé£é™©æ•æ„Ÿé›¶å’Œéšæœºå¾®åˆ†åšå¼ˆæ¨¡å‹<br>æœ¬æ–‡å°†é£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé›¶å’Œéšæœºå¾®åˆ†åšå¼ˆæ¨¡å‹ï¼Œå…¶ä¸­æŠ•èµ„è€…å’Œå¸‚åœºåˆ†åˆ«ä½œä¸ºä¸¤ä¸ªç©å®¶ã€‚æŠ•èµ„è€…è¿½æ±‚æœ€å¤§åŒ–å…¶æŠ•èµ„ç»„åˆçš„é¢„æœŸå¢é•¿ç‡ï¼Œè€Œå¸‚åœºåˆ™è¯•å›¾é€šè¿‡è°ƒæ•´éšæœºåŸºå‡†æ¥æœ€å°åŒ–æŠ•èµ„è€…çš„é¢„æœŸå›æŠ¥ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ±‚è§£æœ€ä¼˜ç­–ç•¥<br>æœ¬æ–‡é€šè¿‡æ±‚è§£å“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼åå¾®åˆ†æ–¹ç¨‹ï¼Œå¾—åˆ°äº†æŠ•èµ„è€…å’Œå¸‚åœºåœ¨åšå¼ˆä¸­çš„æœ€ä¼˜ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥ä¸ä»…è€ƒè™‘äº†æŠ•èµ„è€…çš„é£é™©åŒæ¶ç¨‹åº¦ï¼Œè¿˜è€ƒè™‘äº†å¸‚åœºå¯¹æŠ—è¡Œä¸ºçš„å½±å“ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡æ•°å€¼æ¨¡æ‹ŸéªŒè¯äº†æ‰€æå‡ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜ä¸­ï¼ŒæŠ•èµ„è€…å’Œå¸‚åœºä¹‹é—´çš„åšå¼ˆç¡®å®ä¼šå½±å“æŠ•èµ„è€…çš„æœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ï¼ŒæŠ•èµ„è€…çš„é£é™©åŒæ¶ç¨‹åº¦å’Œå¸‚åœºå¯¹æŠ—è¡Œä¸ºçš„å¼ºåº¦éƒ½ä¼šå¯¹æœ€ä¼˜ç­–ç•¥äº§ç”Ÿå½±å“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¸¸æˆç†è®ºè§†è§’ä¸ºé£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„ç ”ç©¶æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…å¯ä»¥ç”¨äºåˆ†ææŠ•èµ„è€…å’Œå¸‚åœºä¹‹é—´çš„åšå¼ˆï¼Œè¿˜å¯ä»¥ç”¨äºè®¾è®¡æ›´æœ‰æ•ˆçš„èµ„äº§ç®¡ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹å’Œæ±‚è§£æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–é‡‘èé¢†åŸŸçš„åšå¼ˆé—®é¢˜æä¾›å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Benchmarks for Parity Games (extended version)</td>
      <td>We propose a benchmark suite for parity games that includes all benchmarks<br>that have been used in the literature, and make it available online. We give an<br>overview of the parity games, including a description of how they have been<br>generated. We also describe structural properties of parity games, and using<br>these properties we show that our benchmarks are representative. With this work<br>we provide a starting point for further experimentation with parity games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¥ å®šå…¬å¹³åšå¼ˆç®—æ³•è¯„ä¼°åŸºçŸ³ï¼šå…¨é¢è§£æå…¬å¹³åšå¼ˆåŸºå‡†é›†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å…¬å¹³åšå¼ˆåœ¨æ¨¡å‹æ£€éªŒç ”ç©¶ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå…¶æ±‚è§£é—®é¢˜è¢«è¯æ˜å±äº NP âˆ© co-NP å¤æ‚åº¦ç±»ï¼Œå¹¶ä¸”ç›®å‰å°šæ— å·²çŸ¥çš„å¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‚è¿‘å¹´æ¥ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†å¤§é‡çš„å…¬å¹³åšå¼ˆæ±‚è§£ç®—æ³•ï¼Œå¹¶ç ”ç©¶äº†å¤šé¡¹å¼æ—¶é—´åŒ–ç®€æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŸºå‡†é›†ï¼Œéš¾ä»¥å¯¹ä¸åŒå·¥å…·å’Œç®—æ³•è¿›è¡Œæ¯”è¾ƒè¯„ä¼°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å…¬å¹³åšå¼ˆåŸºå‡†é›†ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥åŸºå‡†é›†åŒ…å«ä»¥ä¸‹ç‰¹ç‚¹ï¼š<br><br>* **å¤šæ ·æ€§**ï¼šæ¶µç›–æ¥è‡ªä¸åŒéªŒè¯é—®é¢˜çš„æ¸¸æˆï¼ŒåŒ…æ‹¬æ¨¡å‹æ£€éªŒã€ç­‰ä»·æ€§æ£€æŸ¥ã€å†³ç­–è¿‡ç¨‹å’Œåˆæˆé—®é¢˜ã€‚<br>* **ä»£è¡¨æ€§**ï¼šåŒ…å«æ–‡çŒ®ä¸­å·²ä½¿ç”¨çš„æ‰€æœ‰åŸºå‡†æ¸¸æˆï¼Œå¹¶ä½¿ç”¨ç»“æ„å±æ€§åˆ†æå…¶ä»£è¡¨æ€§ã€‚<br>* **æ˜“ç”¨æ€§**ï¼šæ‰€æœ‰æ¸¸æˆå‡ä»¥ PGSolver æ ¼å¼åœ¨çº¿æä¾›ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åˆ†æäº†åŸºå‡†é›†ä¸­æ¸¸æˆçš„å¤šç§ç»“æ„å±æ€§ï¼ŒåŒ…æ‹¬é¡¶ç‚¹æ•°ã€è¾¹æ•°ã€ä¼˜å…ˆçº§æ•°ã€åº¦æ•°ã€å¼ºè¿é€šåˆ†é‡ã€æœç´¢ç­–ç•¥ç‰¹æ€§ã€è·ç¦»ã€å±€éƒ¨ç»“æ„å’Œå®½åº¦åº¦é‡ç­‰ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºå‡†é›†æ¶µç›–äº†å¹¿æ³›çš„å±æ€§å€¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°å…¬å¹³åšå¼ˆç®—æ³•çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>* **æ„å»ºæ ‡å‡†åŸºå‡†é›†**ï¼šæœ¬æ–‡æå‡ºçš„åŸºå‡†é›†ä¸ºå…¬å¹³åšå¼ˆç®—æ³•çš„è¯„ä¼°æä¾›äº†é‡è¦çš„å‚è€ƒï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚<br>* **åˆ†æç»“æ„å±æ€§**ï¼šæœ¬æ–‡å¯¹å…¬å¹³åšå¼ˆçš„ç»“æ„å±æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¸ºç®—æ³•è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯ã€‚<br>* **æ¢ç´¢æ–°å±æ€§**ï¼šæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å…¬å¹³åšå¼ˆçš„å…¶ä»–ç»“æ„å±æ€§ï¼Œå¹¶ç ”ç©¶å…¶å¯¹ç®—æ³•æ€§èƒ½çš„å½±å“ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>æœ¬æ–‡æå‡ºçš„å…¬å¹³åšå¼ˆåŸºå‡†é›†ä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨å…¬å¹³åšå¼ˆç®—æ³•çš„è¯„ä¼°å’Œæ”¹è¿›ã€‚æœªæ¥ï¼Œéšç€æ›´å¤šæ¸¸æˆå’Œå±æ€§çš„åŠ å…¥ï¼Œè¯¥åŸºå‡†é›†å°†è¿›ä¸€æ­¥å®Œå–„ï¼Œä¸ºå…¬å¹³åšå¼ˆç ”ç©¶æä¾›æ›´å¼ºå¤§çš„æ”¯æŒã€‚</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Fate stochastic management and policy benchmark in 421, a popular game</td>
      <td>Using game and probability theories, I study the French popular game 421, a<br>perfect information stochastic stage game. The problem is to find strategies<br>maximizing the probability of some expected utility. I only solve a player's<br>round against providence, a problem of fate stochastic management: beyond the<br>backward induction solution, bounded complexity motivates heuristic policies.<br>For a unique goal utility, a simple optimal policy, ratchet, is obtained. Its<br>result probabilities are compiled and used, for arbitrary utilities, as the<br>logic of goal identification policies. Various policies appear, close to human<br>behavior, and are exactly evaluated by solving the Kolmogorov equation.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | 421æ¸¸æˆä¸­å‘½è¿éšæœºç®¡ç†ä¸ç­–ç•¥åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æœ¬æ–‡ç ”ç©¶äº†æ³•å›½æµè¡Œçš„æ¸¸æˆ421ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å®Œç¾ä¿¡æ¯çš„éšæœºé˜¶æ®µæ¸¸æˆã€‚æ¸¸æˆçš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€å¤§åŒ–æŸäº›é¢„æœŸæ•ˆç”¨æ¦‚ç‡çš„ç­–ç•¥ã€‚ä½œè€…å°†421æ¸¸æˆç®€åŒ–ä¸ºç©å®¶å›åˆå¯¹æŠ—å‘½è¿çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå‘½è¿éšæœºç®¡ç†é—®é¢˜ã€‚ä½œè€…é€šè¿‡åˆ†ææ¸¸æˆè§„åˆ™å’Œæ¦‚ç‡ç†è®ºï¼Œæå‡ºäº†å‡ ç§ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨Kolmogorovæ–¹ç¨‹ç²¾ç¡®è¯„ä¼°äº†è¿™äº›ç­–ç•¥ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†421æ¸¸æˆç®€åŒ–ä¸ºç©å®¶å›åˆå¯¹æŠ—å‘½è¿çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå‘½è¿éšæœºç®¡ç†é—®é¢˜ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºäº†å‡ ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ä¼¯åŠªåˆ©ç­–ç•¥å’Œæ£˜è½®ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨Kolmogorovæ–¹ç¨‹ç²¾ç¡®è¯„ä¼°äº†è¿™äº›ç­–ç•¥ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºäº†ç›®æ ‡è¯†åˆ«ç¼–ç¨‹æ–¹æ³•ï¼Œé€šè¿‡è€ƒè™‘ç»“æœæ¦‚ç‡æ¥è¯†åˆ«ç›®æ ‡ï¼Œä»è€Œç®€åŒ–ç­–ç•¥çš„å¤æ‚æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ä½œè€…ä½¿ç”¨Mathematicaè½¯ä»¶å®ç°äº†421æ¸¸æˆæ¨¡å‹ï¼Œå¹¶è®¡ç®—äº†å„ç§ç­–ç•¥çš„ç»“æœæ¦‚ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ£˜è½®ç­–ç•¥åœ¨D < Fçš„æƒ…å†µä¸‹æ˜¯æœ€ä¼˜çš„ï¼Œè€Œä¼¯åŠªåˆ©ç­–ç•¥åœ¨D > Fçš„æƒ…å†µä¸‹æ˜¯æœ€ä¼˜çš„ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å‘ç°ï¼Œå¼•å…¥éšæœºæ€§å’Œç›®æ ‡è¯†åˆ«ç¼–ç¨‹å¯ä»¥æ˜¾è‘—æé«˜ç­–ç•¥çš„æ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºç†è§£éšæœºæ¸¸æˆä¸­çš„ç­–ç•¥é€‰æ‹©å’Œå‘½è¿ç®¡ç†å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ç­–ç•¥è¯„ä¼°æ–¹æ³•å’Œç›®æ ‡è¯†åˆ«ç¼–ç¨‹æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–éšæœºå†³ç­–é—®é¢˜ã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        