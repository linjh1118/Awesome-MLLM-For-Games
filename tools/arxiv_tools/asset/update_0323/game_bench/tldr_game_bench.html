
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments</td>
      <td>Large Language Model~(LLM) based agents have been increasingly popular in<br>solving complex and dynamic tasks, which requires proper evaluation systems to<br>assess their capabilities. Nevertheless, existing benchmarks usually either<br>focus on single-objective tasks or use overly broad assessing metrics, failing<br>to provide a comprehensive inspection of the actual capabilities of LLM-based<br>agents in complicated decision-making tasks. To address these issues, we<br>introduce DSGBench, a more rigorous evaluation platform for strategic<br>decision-making. Firstly, it incorporates six complex strategic games which<br>serve as ideal testbeds due to their long-term and multi-dimensional<br>decision-making demands and flexibility in customizing tasks of various<br>difficulty levels or multiple targets. Secondly, DSGBench employs a<br>fine-grained evaluation scoring system which examines the decision-making<br>capabilities by looking into the performance in five specific dimensions and<br>offering a comprehensive assessment in a well-designed way. Furthermore,<br>DSGBench also incorporates an automated decision-tracking mechanism which<br>enables in-depth analysis of agent behaviour patterns and the changes in their<br>strategies. We demonstrate the advances of DSGBench by applying it to multiple<br>popular LLM-based agents and our results suggest that DSGBench provides<br>valuable insights in choosing LLM-based agents as well as improving their<br>future development. DSGBench is available at<br>https://github.com/DeciBrain-Group/DSGBench.</td>
      <td>## 🌟 论文解读 | DSGBench：评估大型语言模型在复杂决策环境中的战略决策能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）在解决复杂和动态任务方面的能力日益增强，评估这些模型在复杂决策任务中的实际能力变得至关重要。然而，现有的评估系统通常只关注单一目标任务或使用过于宽泛的评估指标，无法全面评估LLM模型在复杂决策任务中的实际能力。<br><br>## 🚀 核心方法<br>💡 创新点1：DSGBench是一个更严格的评估平台，用于评估战略决策能力。它包含了六个复杂的战略游戏，这些游戏因其长期和多维度的决策需求以及定制各种难度级别或多个目标的任务的灵活性而成为理想的测试平台。<br><br>💡 创新点2：DSGBench采用了一种细粒度的评估评分系统，通过考察在五个特定维度中的表现来检查决策能力，并以一种精心设计的方式提供全面的评估。此外，DSGBench还包含一个自动化的决策跟踪机制，能够深入分析代理的行为模式和策略的变化。<br><br>## 📈 实验结果<br>DSGBench通过应用于多个流行的LLM模型，展示了其在选择LLM模型以及改进其未来发展方面的价值。实验结果表明，DSGBench能够提供有价值的见解，帮助研究人员更好地理解LLM模型在不同决策环境中的表现。<br><br>## 💬 可借鉴之处<br>DSGBench为评估LLM模型在复杂决策环境中的战略决策能力提供了一个全面的框架。其细粒度的评估指标和决策跟踪机制可以帮助研究人员深入了解LLM模型的行为模式和策略变化，从而更好地改进模型的设计和开发。此外，DSGBench的灵活性和可定制性使其能够适应不同的评估需求，为LLM模型的研究和应用提供了有力的支持。</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests</td>
      <td>We examine three evaluation paradigms: large question-answering benchmarks<br>(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and<br>cognitive tests (e.g., for working memory or theory of mind). First, we<br>investigate which of the former two-benchmarks or games-is most effective at<br>discriminating LLMs of varying quality. Then, inspired by human cognitive<br>assessments, we compile a suite of targeted tests that measure cognitive<br>abilities deemed essential for effective language use, and we investigate their<br>correlation with model performance in benchmarks and games. Our analyses reveal<br>that interactive games are superior to standard benchmarks in discriminating<br>models. Causal and logical reasoning correlate with both static and interactive<br>tests, while differences emerge regarding core executive functions and<br>social/emotional skills, which correlate more with games. We advocate the<br>development of new interactive benchmarks and targeted cognitive tasks inspired<br>by assessing human abilities but designed specifically for LLMs.</td>
      <td>## 🌟 论文解读 | 通过基准测试、游戏和认知测试来评估大型语言模型<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，如何有效地评估这些模型的能力变得至关重要。传统的评估方法主要依赖于大规模的问答基准测试，但这些测试往往存在数据污染和模型对提示格式敏感的问题。此外，这些测试忽略了实际的语言使用场景，即在社会和任务导向的多轮交互中使用语言。因此，本文旨在探索更有效的评估方法，以更好地理解LLMs的能力和局限性。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了三种评估范式：大规模问答基准测试（如MMLU和BBH）、交互式游戏（如信号游戏或禁忌游戏）和认知测试（如工作记忆或心智理论测试）。通过比较这些范式，研究团队旨在找出最有效的评估方法。<br><br>💡 创新点2：受人类认知评估的启发，研究团队设计了一系列针对性的测试，以衡量对有效语言使用至关重要的认知能力。这些测试旨在评估LLMs在因果推理、逻辑推理、工作记忆、情绪智力等方面的能力，并分析这些能力与模型在基准测试和游戏中的表现之间的相关性。<br><br>## 📈 实验结果<br>实验结果表明，交互式游戏在区分不同质量的LLMs方面优于传统的问答基准测试。因果推理和逻辑推理与静态和交互式测试都相关，而核心执行功能和社交/情感技能则更多地与游戏相关。特别是，工作记忆和情绪智力仅与游戏中的表现显著相关。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，交互式游戏是一种更有效的评估LLMs的方法，因为它能够更好地捕捉模型在实际语言使用场景中的能力。此外，针对性的认知测试可以帮助我们更好地理解LLMs的内在能力，并为未来的模型设计和评估提供指导。因此，本文的研究结果对于LLMs的评估和开发具有重要意义。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents</td>
      <td>Large Language Model (LLM) agents frameworks often employ modular<br>architectures, incorporating components such as planning, reasoning, action<br>execution, and reflection to tackle complex tasks. However, quantifying the<br>contribution of each module to overall system performance remains a significant<br>challenge, impeding optimization and interpretability. To address this, we<br>introduce CapaBench (Capability-level Assessment Benchmark), an evaluation<br>framework grounded in cooperative game theory's Shapley Value, which<br>systematically measures the marginal impact of individual modules and their<br>interactions within an agent's architecture. By replacing default modules with<br>test variants across all possible combinations, CapaBench provides a principle<br>method for attributing performance contributions. Key contributions include:<br>(1) We are the first to propose a Shapley Value-based methodology for<br>quantifying the contributions of capabilities in LLM agents; (2) Modules with<br>high Shapley Values consistently lead to predictable performance gains when<br>combined, enabling targeted optimization; and (3) We build a multi-round<br>dataset of over 1,500 entries spanning diverse domains and practical task<br>scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench<br>bridges the gap between component-level evaluation and holistic system<br>assessment, providing actionable insights for optimizing modular LLM agents and<br>advancing their deployment in complex, real-world scenarios.</td>
      <td>## 🌟 论文解读 | 谁是MVP？基于博弈论的LLM Agent模块化评估基准<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）在人工智能领域的快速发展，LLM Agent框架在处理复杂任务时通常采用模块化架构，包括规划、推理、行动执行和反思等组件。然而，如何量化每个模块对整体系统性能的贡献仍然是一个重大挑战，这阻碍了优化和可解释性。为了解决这个问题，本文提出了CapaBench（能力级评估基准），这是一个基于合作博弈论的Shapley Value的评估框架，系统地衡量了单个模块及其在Agent架构中的交互的边际影响。<br><br>## 🚀 核心方法<br>💡 创新点1：首次提出基于Shapley Value的方法来量化LLM Agent中能力的贡献。<br>💡 创新点2：具有高Shapley Value的模块在组合时始终导致可预测的性能提升，从而实现有针对性的优化。<br>💡 创新点3：构建了一个包含超过1,500个条目的多轮数据集，涵盖各种领域和实际任务场景，使Agent能力的全面评估成为可能。<br><br>## 📈 实验结果<br>本文对九个大型语言模型进行了系统评估，涵盖了五个主要任务，揭示了显著的性能差异和独特的模块贡献模式。结果表明，具有高Shapley Value的模块在组合时始终导致可预测的性能提升，这为开发人员提供了有针对性的优化指导。<br><br>## 💬 可借鉴之处<br>CapaBench框架为优化模块化LLM Agent和推进其在复杂、现实世界场景中的应用提供了有价值的见解。通过系统地评估和量化每个模块的贡献，开发人员可以更好地理解Agent的性能，并针对性地进行优化，从而提高LLM Agent在现实世界中的应用效果。</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research</td>
      <td>After the great achievement of solving two-player zero-sum games, more and<br>more AI researchers focus on solving multiplayer games. To facilitate the<br>development of designing efficient learning algorithms for solving multiplayer<br>games, we propose a multiplayer game platform for solving Urban Network<br>Security Games (\textbf{UNSG}) that model real-world scenarios. That is,<br>preventing criminal activity is a highly significant responsibility assigned to<br>police officers in cities, and police officers have to allocate their limited<br>security resources to interdict the escaping criminal when a crime takes place<br>in a city. This interaction between multiple police officers and the escaping<br>criminal can be modeled as a UNSG. The variants of UNSGs can model different<br>real-world settings, e.g., whether real-time information is available or not,<br>and whether police officers can communicate or not. The main challenges of<br>solving this game include the large size of the game and the co-existence of<br>cooperation and competition. While previous efforts have been made to tackle<br>UNSGs, they have been hampered by performance and scalability issues.<br>Therefore, we propose an open-source UNSG platform (\textbf{GraphChase}) for<br>designing efficient learning algorithms for solving UNSGs. Specifically,<br>GraphChase offers a unified and flexible game environment for modeling various<br>variants of UNSGs, supporting the development, testing, and benchmarking of<br>algorithms. We believe that GraphChase not only facilitates the development of<br>efficient algorithms for solving real-world problems but also paves the way for<br>significant advancements in algorithmic development for solving general<br>multiplayer games.</td>
      <td>## 🌟 论文解读 | GraphChase：解决城市网络安全游戏的AI平台<br><br>## 📌 背景痛点/本文动机<br>随着人工智能在解决两人零和博弈方面取得巨大成就，越来越多的研究人员开始关注解决多人游戏。城市网络安全游戏（UNSG）作为一种模拟现实世界场景的多玩家游戏，对于研究多人博弈算法具有重要意义。然而，UNSG的解决面临着游戏规模庞大、合作与竞争共存等挑战，现有的算法在性能和可扩展性方面存在不足。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：GraphChase平台<br>本文提出了一个名为GraphChase的开源UNSG平台，旨在为研究人员提供一个统一的、灵活的游戏环境，用于模拟各种UNSG变体，并支持算法的开发、测试和基准测试。GraphChase平台具有以下特点：<br>- **灵活的游戏环境**：用户可以自定义游戏参数，包括图结构、初始位置、时间范围等，以模拟不同的UNSG场景。<br>- **多种算法支持**：GraphChase平台支持多种深度学习算法，例如CFR-MIX、NSG-NFSP、NSGZero、Pretrained PSRO和Grasper，方便研究人员进行算法比较和评估。<br>- **模块化设计**：GraphChase平台采用模块化设计，包括游戏模块、代理模块和求解器模块，方便用户进行自定义和扩展。<br><br>💡 创新点2：基准测试<br>本文在GraphChase平台上进行了实验，评估了现有算法的性能和可扩展性。实验结果表明，虽然现有算法在简单场景下能够取得合理性能，但在现实世界场景下仍然存在性能和可扩展性问题。这表明，开发高效且可扩展的算法来解决现实世界的UNSG仍然是一个重要的研究方向。<br><br>## 📈 实验结果<br>实验结果表明，GraphChase平台能够有效地模拟UNSG场景，并且能够加速算法的训练过程。此外，实验还发现，现有算法在解决复杂场景和大规模游戏时存在性能和可扩展性问题。<br><br>## 💬 可借鉴之处<br>GraphChase平台为研究UNSG和多人博弈算法提供了一个有价值的工具。其灵活性和可扩展性使得研究人员可以轻松地模拟不同的场景和测试不同的算法。此外，GraphChase平台还可以作为其他多人博弈问题的测试平台，例如对抗性团队游戏和追逃游戏。<br><br>## 📚 总结<br>GraphChase平台为解决城市网络安全游戏提供了一个重要的工具，并为研究多人博弈算法提供了新的思路。随着人工智能技术的不断发展，GraphChase平台有望在解决现实世界问题方面发挥更大的作用。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games</td>
      <td>The deployment of large language models (LLMs) in diverse applications<br>requires a thorough understanding of their decision-making strategies and<br>behavioral patterns. As a supplement to a recent study on the behavioral Turing<br>test, this paper presents a comprehensive analysis of five leading LLM-based<br>chatbot families as they navigate a series of behavioral economics games. By<br>benchmarking these AI chatbots, we aim to uncover and document both common and<br>distinct behavioral patterns across a range of scenarios. The findings provide<br>valuable insights into the strategic preferences of each LLM, highlighting<br>potential implications for their deployment in critical decision-making roles.</td>
      <td>## 🌟 论文解读 | 探索大型语言模型在行为经济学游戏中的行为模式<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在各个领域的广泛应用，理解这些模型的决策策略和行为模式变得至关重要。这不仅有助于优化它们在特定应用中的性能，还能更好地评估它们的可靠性和可预测性，尤其是在涉及重大责任的情境中。然而，目前对于不同LLMs的行为模式的研究仍然有限，尤其是在行为经济学游戏中的表现。<br><br>## 🚀 核心方法<br>💡 创新点1：通过行为经济学游戏评估LLMs<br>本文通过一系列经典的行为经济学游戏，如独裁者游戏、最后通牒游戏、信任游戏、公共物品游戏、炸弹风险游戏和囚徒困境游戏，对五种领先的LLM-based聊天机器人进行了全面分析。这些游戏旨在测试LLMs在信任、公平、风险规避、利他主义和合作等方面的行为模式。<br><br>💡 创新点2：引入行为图灵测试和分布相似性测试<br>为了评估LLMs的行为模式，本文引入了行为图灵测试和分布相似性测试。行为图灵测试通过比较LLMs和人类的行为分布，评估LLMs是否能够模仿人类的行为。分布相似性测试则使用Wasserstein距离来衡量LLMs的行为分布与人类行为分布之间的相似度。<br><br>## 📈 实验结果<br>实验结果表明，所有测试的聊天机器人都能成功地捕捉到特定的人类行为模式，导致决策分布高度集中。尽管旗舰聊天机器人表现出显著的通过图灵测试的概率，但它们只能产生与人类相似的行为分布。与人类相比，聊天机器人在收益偏好上更强调公平性。聊天机器人在不同游戏中的收益偏好可能存在不一致性。不同的聊天机器人在游戏中表现出不同的行为模式，这些模式可以通过图灵测试结果、收益偏好和行为的连贯性进一步区分。<br><br>## 💬 可借鉴之处<br>本文的研究结果为理解LLMs的行为模式提供了有价值的见解，并为未来在AI行为科学领域的研究奠定了基础。研究结果表明，LLMs在模仿人类行为方面仍然存在局限性，需要进一步的研究来提高LLMs在行为经济学游戏中的表现。此外，LLMs在不同游戏中的行为不一致性也表明，需要开发更通用的偏好和目标，以便LLMs能够有效地适应各种情境。</td>
    </tr>
    <tr>
      <th>5</th>
      <td>AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games</td>
      <td>Decision-making in large-scale games is an essential research area in<br>artificial intelligence (AI) with significant real-world impact. However, the<br>limited access to realistic large-scale game environments has hindered research<br>progress in this area. In this paper, we present AuctionNet, a benchmark for<br>bid decision-making in large-scale ad auctions derived from a real-world online<br>advertising platform. AuctionNet is composed of three parts: an ad auction<br>environment, a pre-generated dataset based on the environment, and performance<br>evaluations of several baseline bid decision-making algorithms. More<br>specifically, the environment effectively replicates the integrity and<br>complexity of real-world ad auctions through the interaction of several<br>modules: the ad opportunity generation module employs deep generative networks<br>to bridge the gap between simulated and real-world data while mitigating the<br>risk of sensitive data exposure; the bidding module implements diverse<br>auto-bidding agents trained with different decision-making algorithms; and the<br>auction module is anchored in the classic Generalized Second Price (GSP)<br>auction but also allows for customization of auction mechanisms as needed. To<br>facilitate research and provide insights into the environment, we have also<br>pre-generated a substantial dataset based on the environment. The dataset<br>contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over<br>500 million auction records. Performance evaluations of baseline algorithms<br>such as linear programming, reinforcement learning, and generative models for<br>bid decision-making are also presented as a part of AuctionNet. We believe that<br>AuctionNet is applicable not only to research on bid decision-making in ad<br>auctions but also to the general area of decision-making in large-scale games.</td>
      <td>## 🌟 论文解读 | AuctionNet：大型游戏中决策制定的新基准<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，大型游戏中的决策制定是一个重要的研究方向，它对现实世界有着深远的影响。然而，由于缺乏对真实大型游戏环境的访问，这一领域的研究进展受到了限制。现有的模拟环境往往与真实环境存在较大差距，无法完全反映现实世界中的多智能体动态。<br><br>## 🚀 核心方法<br>💡 创新点1：AuctionNet环境<br>AuctionNet是一个基于真实在线广告平台的大型广告拍卖决策制定基准。它由三个部分组成：广告拍卖环境、基于环境的预生成数据集以及几个基线投标决策制定算法的性能评估。该环境通过几个模块的交互有效地复制了现实世界广告拍卖的完整性和复杂性：<br>- 广告机会生成模块：使用深度生成网络来弥合模拟数据和现实世界数据之间的差距，同时降低敏感数据泄露的风险。<br>- 投标模块：实现了多种自动投标代理，这些代理使用不同的决策制定算法进行训练。<br>- 拍卖模块：以经典的广义第二价格（GSP）拍卖为基础，但也允许根据需要定制拍卖机制。<br><br>💡 创新点2：预生成数据集<br>为了促进研究和提供对环境的洞察，AuctionNet还基于环境预生成了一个庞大的数据集。该数据集包含1000万个广告机会、48种不同的自动投标代理和超过5亿条拍卖记录。这些数据可以用于建模环境，并有效地训练自动投标代理。<br><br>## 📈 实验结果<br>AuctionNet已经为NeurIPS 2024“大型拍卖中的自动投标”竞赛提供了动力，为来自世界各地的1500多支队伍提供了竞赛环境。该竞赛解决了在不确定和竞争环境中进行高频投标决策制定的关键问题，并持续了4个月。AuctionNet提供的广告拍卖环境、数据集和基线投标决策制定算法为参赛者提供了准确和公平的性能评估，并激发了他们的创造力，推动了该领域的技术发展。<br><br>## 💬 可借鉴之处<br>AuctionNet不仅为研究人员提供了研究大型拍卖中自动投标算法的机会，还帮助研究人员和实践者在博弈论、强化学习、生成模型、运营优化等领域解决广泛的决策制定研究问题。此外，AuctionNet还可以用于研究其他大型游戏中的决策制定问题，例如在线广告、推荐系统、资源分配等。</td>
    </tr>
    <tr>
      <th>6</th>
      <td>From Code to Play: Benchmarking Program Search for Games Using Large Language Models</td>
      <td>Large language models (LLMs) have shown impressive capabilities in generating<br>program code, opening exciting opportunities for applying program synthesis to<br>games. In this work, we explore the potential of LLMs to directly synthesize<br>usable code for a wide range of gaming applications, focusing on two<br>programming languages, Python and Java. We use an evolutionary hill-climbing<br>algorithm, where the mutations and seeds of the initial programs are controlled<br>by LLMs. For Python, the framework covers various game-related tasks, including<br>five miniature versions of Atari games, ten levels of Baba is You, an<br>environment inspired by Asteroids, and a maze generation task. For Java, the<br>framework contains 12 games from the TAG tabletop games framework. Across 29<br>tasks, we evaluated 12 language models for Python and 8 for Java. Our findings<br>suggest that the performance of LLMs depends more on the task than on model<br>size. While larger models generate more executable programs, these do not<br>always result in higher-quality solutions but are much more expensive. No model<br>has a clear advantage, although on any specific task, one model may be better.<br>Trying many models on a problem and using the best results across them is more<br>reliable than using just one.</td>
      <td>## 🌟 论文解读 | 从代码到游戏：使用大型语言模型进行游戏程序搜索的基准测试<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在生成程序代码方面的能力日益增强，将程序合成应用于游戏领域展现出巨大的潜力。然而，传统的程序合成方法通常依赖于预定义的领域特定语言（DSL）或JSON转换器，限制了其适用范围和灵活性。本文旨在探索LLMs直接合成可用于各种游戏应用的代码的潜力，并评估其在游戏领域的性能。<br><br>## 🚀 核心方法<br>本文提出了一种基于LLMs的程序搜索框架，用于评估LLMs在游戏领域合成程序代码的能力。该框架使用进化爬山算法，其中初始程序的突变和种子由LLMs控制。研究人员使用Python和Java两种编程语言，在29个不同的游戏任务上评估了12个Python语言模型和8个Java语言模型。<br><br>## 📈 实验结果<br>研究发现，LLMs的性能更多地取决于任务本身，而不是模型的大小。虽然更大的模型可以生成更多可执行的程序，但这些程序并不总是产生更高质量的解决方案，并且成本更高。没有模型在所有任务中都表现出明显的优势，但在特定任务上，某些模型可能表现更好。尝试多种模型并使用最佳结果可以提高可靠性。<br><br>## 💬 可借鉴之处<br>本文提出的框架为评估LLMs在游戏领域合成程序代码的能力提供了一个易于使用且可扩展的平台。研究结果表明，LLMs在游戏领域具有巨大的潜力，但仍存在一些局限性，例如生成的代码质量不高、可执行性差等。未来研究可以探索更复杂的搜索策略和更好的提示工程，以提高LLMs的性能。此外，使用多种模型可以降低成本并提高结果的可靠性。</td>
    </tr>
    <tr>
      <th>7</th>
      <td>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</td>
      <td>Large Language Models (LLMs) and Vision Language Models (VLMs) possess<br>extensive knowledge and exhibit promising reasoning abilities; however, they<br>still struggle to perform well in complex, dynamic environments. Real-world<br>tasks require handling intricate interactions, advanced spatial reasoning,<br>long-term planning, and continuous exploration of new strategies-areas in which<br>we lack effective methodologies for comprehensively evaluating these<br>capabilities. To address this gap, we introduce BALROG, a novel benchmark<br>designed to assess the agentic capabilities of LLMs and VLMs through a diverse<br>set of challenging games. Our benchmark incorporates a range of existing<br>reinforcement learning environments with varying levels of difficulty,<br>including tasks that are solvable by non-expert humans in seconds to extremely<br>challenging ones that may take years to master (e.g., the NetHack Learning<br>Environment). We devise fine-grained metrics to measure performance and conduct<br>an extensive evaluation of several popular open-source and closed-source LLMs<br>and VLMs. Our findings indicate that while current models achieve partial<br>success in the easier games, they struggle significantly with more challenging<br>tasks. Notably, we observe severe deficiencies in vision-based decision-making,<br>as models perform worse when visual representations of the environments are<br>provided. We release BALROG as an open and user-friendly benchmark to<br>facilitate future research and development in the agentic community.</td>
      <td>## 🌟 论文解读 | BALROG：评估大型语言模型和视觉语言模型在游戏中的推理能力<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）和视觉语言模型（VLMs）在知识储备和推理能力方面表现出色，但在复杂、动态的环境中表现不佳。现实世界任务需要处理复杂的交互、高级的空间推理、长期规划和持续探索新策略，而这些领域缺乏有效的方法来全面评估这些能力。为了解决这个问题，本文提出了BALROG，一个用于评估LLMs和VLMs代理能力的基准，通过一系列具有挑战性的游戏进行测试。<br><br>## 🚀 核心方法<br>💡 创新点1：构建了BALROG基准，包含六个具有不同难度级别的强化学习环境，从简单到极其复杂，例如NetHack学习环境。<br>💡 创新点2：设计了细粒度的指标来衡量性能，并对多个流行的开源和闭源LLMs和VLMs进行了广泛的评估。<br>💡 创新点3：进行了定性分析，研究了代理在空间推理、系统探索和长期规划方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，当前模型在较简单的游戏中取得了部分成功，但在更具挑战性的任务中表现不佳。值得注意的是，模型在基于视觉的决策方面存在严重缺陷，当提供环境的视觉表示时，模型的表现更差。<br><br>## 💬 可借鉴之处<br>本文提出的BALROG基准为评估LLMs和VLMs的代理能力提供了一个重要的工具，有助于推动该领域的研究和发展。此外，本文还指出了当前模型在视觉决策和长期规划方面的局限性，为未来的研究提供了方向。</td>
    </tr>
    <tr>
      <th>8</th>
      <td>TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs</td>
      <td>The rapid advancement of large language models (LLMs) has accelerated their<br>application in reasoning, with strategic reasoning drawing increasing<br>attention. To evaluate LLMs' strategic reasoning capabilities, game theory,<br>with its concise structure, has become a preferred approach. However, current<br>research focuses on a limited selection of games, resulting in low coverage.<br>Classic game scenarios risk data leakage, and existing benchmarks often lack<br>extensibility, making them inadequate for evaluating state-of-the-art models.<br>To address these challenges, we propose TMGBench, a benchmark with<br>comprehensive game type coverage, novel scenarios, and flexible organization.<br>Specifically, we incorporate all 144 game types summarized by the<br>Robinson-Goforth topology of 2x2 games, constructed as classic games. We also<br>employ synthetic data generation to create diverse, higher-quality scenarios<br>through topic guidance and human inspection, referred to as story-based games.<br>Lastly, we provide a sustainable framework for increasingly powerful LLMs by<br>treating these games as atomic units and organizing them into more complex<br>forms via sequential, parallel, and nested structures. Our comprehensive<br>evaluation of mainstream LLMs covers tests on rational reasoning, robustness,<br>Theory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in<br>accuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,<br>OpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and<br>70.0% on sequential, parallel, and nested games, highlighting TMGBench's<br>challenges.</td>
      <td>## 🌟 论文解读 | TMGBench：评估大型语言模型战略推理能力的系统游戏基准<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，它们在推理任务中的应用日益广泛，其中战略推理能力尤其受到关注。然而，现有的研究往往局限于少数经典游戏，导致游戏类型的覆盖率低，且经典游戏场景存在数据泄露的风险。此外，现有的基准测试往往缺乏可扩展性，难以评估最先进的模型。为了解决这些问题，本文提出了TMGBench，一个具有全面游戏类型覆盖率、新颖场景和灵活组织方式的基准测试。<br><br>## 🚀 核心方法<br>💡 创新点1：全面的游戏类型覆盖率<br>TMGBench包含了由Robinson-Goforth拓扑结构总结的144种2x2游戏类型，涵盖了各种不同的游戏结构，包括经典游戏如囚徒困境等。<br><br>💡 创新点2：新颖的场景<br>为了解决经典游戏场景的数据泄露问题，TMGBench采用了合成数据生成技术，为每种经典游戏创建了五个不同的基于故事的场景，这些场景涵盖了商业、法律、交通等现实生活中的主题。<br><br>💡 创新点3：灵活的游戏组织方式<br>TMGBench将游戏视为原子单位，并通过顺序、并行和嵌套结构将它们组织成更复杂的游戏形式，以评估LLMs在并行、顺序和多层级决策方面的战略推理能力。<br><br>## 📈 实验结果<br>本文对主流LLMs进行了全面评估，包括理性推理、推理鲁棒性、心智理论（ToM）能力和复杂游戏形式的推理。结果表明，LLMs在战略推理过程的准确性和一致性方面仍存在缺陷，且对ToM的掌握程度也各不相同。OpenAI的最新推理模型o1-mini在顺序、并行和嵌套游戏上的准确率分别为66.6%、60.0%和70.0%，突显了TMGBench的挑战性。<br><br>## 💬 可借鉴之处<br>TMGBench为评估LLMs的战略推理能力提供了一个全面的基准测试，其创新点包括全面的游戏类型覆盖率、新颖的场景和灵活的游戏组织方式。此外，本文还揭示了LLMs在战略推理方面的缺陷，并提出了改进方向，为LLMs的研究和应用提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Game4Loc: A UAV Geo-Localization Benchmark from Game Data</td>
      <td>The vision-based geo-localization technology for UAV, serving as a secondary<br>source of GPS information in addition to the global navigation satellite<br>systems (GNSS), can still operate independently in the GPS-denied environment.<br>Recent deep learning based methods attribute this as the task of image matching<br>and retrieval. By retrieving drone-view images in geo-tagged satellite image<br>database, approximate localization information can be obtained. However, due to<br>high costs and privacy concerns, it is usually difficult to obtain large<br>quantities of drone-view images from a continuous area. Existing drone-view<br>datasets are mostly composed of small-scale aerial photography with a strong<br>assumption that there exists a perfect one-to-one aligned reference image for<br>any query, leaving a significant gap from the practical localization scenario.<br>In this work, we construct a large-range contiguous area UAV geo-localization<br>dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,<br>and targets using modern computer games. Based on this dataset, we introduce a<br>more practical UAV geo-localization task including partial matches of<br>cross-view paired data, and expand the image-level retrieval to the actual<br>localization in terms of distance (meters). For the construction of drone-view<br>and satellite-view pairs, we adopt a weight-based contrastive learning<br>approach, which allows for effective learning while avoiding additional<br>post-processing matching steps. Experiments demonstrate the effectiveness of<br>our data and training method for UAV geo-localization, as well as the<br>generalization capabilities to real-world scenarios.</td>
      <td>## 🌟 论文解读 | Game4Loc：基于游戏数据的无人机地理定位基准<br><br>## 📌 背景痛点/本文动机<br>无人机视觉地理定位技术作为全球导航卫星系统（GNSS）的辅助手段，在GPS信号不可用的环境下仍能独立工作。近年来，基于深度学习的方法将这一任务视为图像匹配和检索问题。通过在带有地理标签的卫星图像数据库中检索无人机视角图像，可以获得近似的定位信息。然而，由于成本高昂和隐私问题，通常难以从连续区域获得大量无人机视角图像。现有的无人机视角数据集大多由小规模航空摄影组成，并假设对于任何查询都存在一个完美的一对一匹配的参考图像，这与实际定位场景存在显著差距。<br><br>## 🚀 核心方法<br>💡 创新点1：构建了一个名为GTA-UAV的大型连续区域无人机地理定位数据集，该数据集具有多个飞行高度、姿态、场景和目标，使用现代计算机游戏生成。<br>💡 创新点2：引入了一个更实际的无人机地理定位任务，包括跨视图配对数据的部分匹配，并将图像级检索扩展到实际定位（以米为单位）。<br>💡 创新点3：采用基于权重的对比学习方法构建无人机视角和卫星视角配对，允许有效学习，同时避免额外的后处理匹配步骤。<br><br>## 📈 实验结果<br>实验结果表明，我们的数据和方法对无人机地理定位是有效的，并且具有将模型推广到现实世界场景的能力。<br><br>## 💬 可借鉴之处<br>本文提出的GTA-UAV数据集和基于权重的对比学习方法为无人机地理定位研究提供了新的思路和方法，有助于提高无人机在GPS信号不可用环境下的定位精度和鲁棒性。</td>
    </tr>
    <tr>
      <th>10</th>
      <td>ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation</td>
      <td>This paper introduces ES-KT-24, a novel multimodal Knowledge Tracing (KT)<br>dataset for intelligent tutoring systems in educational game contexts. Although<br>KT is crucial in adaptive learning, existing datasets often lack game-based and<br>multimodal elements. ES-KT-24 addresses these limitations by incorporating<br>educational game-playing videos, synthetically generated question text, and<br>detailed game logs. The dataset covers Mathematics, English, Indonesian, and<br>Malaysian subjects, emphasizing diversity and including non-English content.<br>The synthetic text component, generated using a large language model,<br>encompasses 28 distinct knowledge concepts and 182 questions, featuring 15,032<br>users and 7,782,928 interactions. Our benchmark experiments demonstrate the<br>dataset's utility for KT research by comparing Deep learning-based KT models<br>with Language Model-based Knowledge Tracing (LKT) approaches. Notably, LKT<br>models showed slightly higher performance than traditional DKT models,<br>highlighting the potential of language model-based approaches in this field.<br>Furthermore, ES-KT-24 has the potential to significantly advance research in<br>multimodal KT models and learning analytics. By integrating game-playing videos<br>and detailed game logs, this dataset offers a unique approach to dissecting<br>student learning patterns through advanced data analysis and machine-learning<br>techniques. It has the potential to unearth new insights into the learning<br>process and inspire further exploration in the field.</td>
      <td>## 🌟 论文解读 | ES-KT-24：基于教育游戏视频和合成文本生成的多模态知识追踪基准数据集<br><br>## 📌 背景痛点/本文动机<br>知识追踪（KT）是自适应学习中的关键任务，旨在根据学生与学习材料的互动来建模他们的知识状态。然而，现有的KT数据集往往缺乏基于游戏和多模态元素，限制了其在游戏化学习环境中的应用。本文提出了ES-KT-24，一个包含教育游戏视频、合成文本和详细游戏日志的多模态KT数据集，旨在解决这一痛点。<br><br>## 🚀 核心方法<br>💡 创新点1：多模态数据集<br>ES-KT-24数据集包含了教育游戏视频、合成文本和详细游戏日志，涵盖了数学、英语、印度尼西亚语和马来西亚语等科目，强调了多样性和非英语内容。这使得研究人员能够更全面地分析学生的学习模式。<br><br>💡 创新点2：合成文本生成<br>使用大型语言模型生成合成文本，涵盖了28个不同的知识概念和182个问题，共有15,032名用户和7,782,928次互动。这为KT研究提供了丰富的文本数据，有助于提高模型的性能。<br><br>## 📈 实验结果<br>在ES-KT-24数据集上进行的基准实验表明，基于语言模型的KT（LKT）方法在性能上略优于传统的深度学习KT（DKT）模型。这突出了语言模型在KT领域的潜力。<br><br>## 💬 可借鉴之处<br>ES-KT-24数据集为KT研究提供了新的基准，并展示了多模态数据在游戏化学习环境中的价值。研究人员可以利用该数据集来探索游戏化学习现象、分析多模态学习过程，并开发更复杂的模型来捕捉教育环境中的多样模态。此外，该数据集还支持游戏难度分类、特征影响分析、生成游戏设计、多模态学习分析、参与度和绩效相关性、跨文化学习模式以及时间学习动态等方面的研究。</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games</td>
      <td>Recent advancements in large language models (LLMs) have expanded their<br>capabilities beyond traditional text-based tasks to multimodal domains,<br>integrating visual, auditory, and textual data. While multimodal LLMs have been<br>extensively explored for high-level planning in domains like robotics and<br>games, their potential as low-level controllers remains largely untapped. In<br>this paper, we introduce a novel benchmark aimed at testing the emergent<br>capabilities of multimodal LLMs as low-level policies in Atari games. Unlike<br>traditional reinforcement learning (RL) methods that require training for each<br>new environment and reward function specification, these LLMs utilize<br>pre-existing multimodal knowledge to directly engage with game environments.<br>Our study assesses the performances of multiple multimodal LLMs against<br>traditional RL agents, human players, and random agents, focusing on their<br>ability to understand and interact with complex visual scenes and formulate<br>strategic responses. Our results show that these multimodal LLMs are not yet<br>capable of being zero-shot low-level policies. Furthermore, we see that this<br>is, in part, due to their visual and spatial reasoning. Additional results and<br>videos are available on our project webpage:<br>https://dev1nw.github.io/atari-gpt/.</td>
      <td>## 🌟 论文解读 | Atari-GPT：评估多模态大型语言模型在Atari游戏中的低级策略能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的突破，它们的能力已经扩展到了多模态领域，整合了视觉、听觉和文本数据。尽管多模态LLMs在机器人技术和游戏等领域的决策规划方面得到了广泛探索，但它们作为低级控制器的潜力尚未得到充分挖掘。本文旨在通过引入一个新的基准，测试多模态LLMs在Atari游戏中的低级策略能力，以填补这一研究空白。<br><br>## 🚀 核心方法<br>💡 创新点1：提出Atari-GPT基准<br>本文提出了Atari-GPT基准，旨在评估多模态LLMs在Atari游戏中的低级策略能力。该基准通过比较LLMs与传统强化学习（RL）代理、人类玩家和随机代理的性能，评估它们在理解复杂视觉场景和制定战略反应方面的能力。<br><br>💡 创新点2：评估视觉和空间推理能力<br>除了游戏性能评估，本文还通过一系列提示测试了LLMs的视觉理解、空间推理和战略直觉能力。这些测试旨在揭示LLMs在理解游戏环境方面的局限性，并为进一步改进提供方向。<br><br>## 📈 实验结果<br>实验结果表明，尽管LLMs在Atari游戏中的表现不如人类玩家或专门的RL模型，但它们仍然能够识别和与游戏帧中的关键元素进行交互。然而，它们作为低级控制器的性能仍然不佳，这可能是由于缺乏针对此任务的训练以及空间推理的困难。此外，实验还发现，LLMs的推理时间对于实时决策至关重要，而目前的多模态模型在速度方面仍存在不足。<br><br>## 💬 可借鉴之处<br>本文的研究结果为LLMs在低级控制任务中的应用提供了宝贵的见解。尽管LLMs尚未达到与人类或RL模型相匹配的水平，但它们在Atari游戏中的表现仍然值得注意。这表明LLMs具有适应性和潜力，可以扩展到其原始训练范围之外，为未来在需要详细环境交互和决策的应用中作为更通用的低级控制器提供了一瞥。</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</td>
      <td>The advancement of Offline Reinforcement Learning (RL) and Offline<br>Multi-Agent Reinforcement Learning (MARL) critically depends on the<br>availability of high-quality, pre-collected offline datasets that represent<br>real-world complexities and practical applications. However, existing datasets<br>often fall short in their simplicity and lack of realism. To address this gap,<br>we propose Hokoff, a comprehensive set of pre-collected datasets that covers<br>both offline RL and offline MARL, accompanied by a robust framework, to<br>facilitate further research. This data is derived from Honor of Kings, a<br>recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate<br>nature, closely resembling real-life situations. Utilizing this framework, we<br>benchmark a variety of offline RL and offline MARL algorithms. We also<br>introduce a novel baseline algorithm tailored for the inherent hierarchical<br>action space of the game. We reveal the incompetency of current offline RL<br>approaches in handling task complexity, generalization and multi-task learning.</td>
      <td>## 🌟 论文解读 | Hokoff：基于王者荣耀的真实游戏数据集及其离线强化学习基准<br><br>## 📌 背景痛点/本文动机<br>离线强化学习（Offline RL）和离线多智能体强化学习（Offline MARL）的研究进展依赖于高质量、预先收集的离线数据集，这些数据集应代表现实世界的复杂性和实际应用。然而，现有的数据集往往过于简单，缺乏现实性。为了解决这个问题，本文提出了Hokoff，这是一套全面的预先收集的数据集，涵盖了离线RL和离线MARL，并伴随着一个强大的框架，以促进进一步的研究。这些数据来自王者荣耀，这是一款以其复杂性质而闻名的多人在线战斗竞技场（MOBA）游戏，与现实生活情况非常相似。<br><br>## 🚀 核心方法<br>💡 创新点1：基于王者荣耀的复杂环境<br>Hokoff的数据集来源于王者荣耀，这是一款拥有超过1亿日活跃玩家的全球最受欢迎的MOBA游戏之一。该游戏的复杂性远远超过其他数据集，展示了模拟现实世界场景的潜力。<br><br>💡 创新点2：开源、易用的框架<br>本文提出了一个开源、易用的框架，该框架包括离线RL（采样、训练和评估）的全面流程和一些有用的工具。基于该框架，我们发布了一系列丰富多样的数据集，这些数据集使用一系列具有不同设计因素的前训练模型生成，不仅适用于离线RL，也适用于离线MARL。<br><br>💡 创新点3：多级模型<br>为了确保不同算法的性能比较的有效性和公正性，本文提出了多级模型，包含多个具有不同水平的检查点。这些模型可以用于采样和评估，从而更准确地评估算法的性能。<br><br>💡 创新点4：多样化的数据集<br>Hokoff提供了多样化的数据集，包括多难度、多任务、泛化、异构队友和子任务等。这些数据集旨在模拟现实世界的复杂性和多样性，为离线RL和离线MARL的研究提供更真实的环境。<br><br>## 📈 实验结果<br>本文在Hokoff数据集上评估了多种离线RL和离线MARL算法，并提出了一个针对王者荣耀固有层次结构动作空间的新的基线算法。结果表明，当前的离线RL方法在处理任务复杂性、泛化和多任务学习方面存在不足。<br><br>## 💬 可借鉴之处<br>Hokoff数据集和框架为离线RL和离线MARL的研究提供了宝贵的资源。其多样化的数据集和强大的框架可以帮助研究人员更好地理解和评估离线学习算法的性能，并为解决现实世界问题提供新的思路和方法。</td>
    </tr>
    <tr>
      <th>13</th>
      <td>A Benchmark Environment for Offline Reinforcement Learning in Racing Games</td>
      <td>Offline Reinforcement Learning (ORL) is a promising approach to reduce the<br>high sample complexity of traditional Reinforcement Learning (RL) by<br>eliminating the need for continuous environmental interactions. ORL exploits a<br>dataset of pre-collected transitions and thus expands the range of application<br>of RL to tasks in which the excessive environment queries increase training<br>time and decrease efficiency, such as in modern AAA games. This paper<br>introduces OfflineMania a novel environment for ORL research. It is inspired by<br>the iconic TrackMania series and developed using the Unity 3D game engine. The<br>environment simulates a single-agent racing game in which the objective is to<br>complete the track through optimal navigation. We provide a variety of datasets<br>to assess ORL performance. These datasets, created from policies of varying<br>ability and in different sizes, aim to offer a challenging testbed for<br>algorithm development and evaluation. We further establish a set of baselines<br>for a range of Online RL, ORL, and hybrid Offline to Online RL approaches using<br>our environment.</td>
      <td>## 🌟 论文解读 | 离线强化学习在赛车游戏中的新基准环境<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）在游戏行业中的应用日益广泛，但传统RL的高样本复杂度限制了其在复杂环境中的应用，如现代AAA游戏。离线强化学习（ORL）通过利用预先收集的过渡数据集来减少对环境交互的需求，从而降低了样本复杂度。然而，目前缺乏专门针对游戏环境，特别是赛车游戏，的ORL数据集和基准环境。<br><br>## 🚀 核心方法<br>💡 创新点1：引入OfflineMania环境<br>OfflineMania是一个基于Unity 3D游戏引擎开发的单代理赛车游戏环境，灵感来源于TrackMania系列游戏。该环境模拟了赛车游戏，目标是通过最优导航完成赛道。<br><br>💡 创新点2：提供多样化的数据集<br>为了评估ORL的性能，论文提供了多种数据集，这些数据集由不同能力水平的策略生成，并具有不同的规模。这些数据集旨在为算法开发和评估提供一个具有挑战性的测试平台。<br><br>💡 创新点3：建立基线<br>论文使用OfflineMania环境，为一系列在线RL、ORL和混合离线到在线RL方法建立了基线。这些基线有助于评估不同算法的性能，并促进ORL在游戏环境中的应用。<br><br>## 📈 实验结果<br>实验结果表明，IQL在大多数数据集上表现优于TD3BC和CQL。此外，SDBG和JSRL在离线到在线RL方法中表现出色，能够有效提高离线训练的政策的性能。<br><br>## 💬 可借鉴之处<br>OfflineMania环境和提供的数据集为ORL研究提供了一个有价值的测试平台。论文中的基线结果为评估不同ORL算法的性能提供了参考。此外，论文还探讨了离线到在线RL方法在游戏环境中的应用，为游戏开发人员提供了新的思路。</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</td>
      <td>We introduce a novel and extensible benchmark for large language models<br>(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.<br>The open-source game simulation code, available on GitHub, allows LLMs to<br>compete and generates detailed data files in JSON, CSV, TXT, and PNG formats<br>for leaderboard rankings and further analysis. We present the results of games<br>among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by<br>Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and<br>GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of<br>results from other LLMs. In total, we simulated 2,310 matches (5 sessions for<br>each pair among 7 LLMs and a random player) across three types of games, using<br>three distinct prompt types: list, illustration, and image. The results<br>revealed significant variations in LLM performance across different games and<br>prompt types, with analysis covering win and disqualification rates, missed<br>opportunity analysis, and invalid move analysis. The details of the leaderboard<br>and result matrix data are available as open-access data on GitHub. This study<br>enhances our understanding of LLMs' capabilities in playing games they were not<br>specifically trained for, helping to assess their rule comprehension and<br>strategic thinking. On the path to Artificial General Intelligence (AGI), this<br>study lays the groundwork for future exploration into their utility in complex<br>decision-making scenarios, illuminating their strategic thinking abilities and<br>offering directions for further inquiry into the limits of LLMs within<br>game-based frameworks.</td>
      <td>## 🌟 论文解读 | 通过基于网格的游戏竞赛评估大型语言模型：一个可扩展的LLM基准和排行榜<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在人工智能领域的快速发展，评估这些模型的能力和性能变得至关重要。传统的评估方法，如准确率、精确率等，已经无法全面评估LLMs的复杂能力。因此，需要新的基准来评估LLMs在规则理解、战略思考和决策制定等方面的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：基于网格的游戏竞赛<br>本文提出了一种新颖且可扩展的基准，通过网格游戏（如井字棋、四子棋和五子棋）来评估LLMs的能力。这些游戏需要模型理解规则、制定策略并做出决策，从而全面评估LLMs的能力。<br><br>💡 创新点2：开放源代码和排行榜<br>本文提供了一个开源的游戏模拟代码，允许LLMs进行竞赛，并生成详细的JSON、CSV、TXT和PNG格式的数据文件，用于排行榜排名和进一步分析。此外，本文还提供了一个排行榜，展示了不同LLMs在不同游戏和提示类型下的表现。<br><br>## 📈 实验结果<br>本文对七个领先的LLMs进行了评估，包括Anthropic的Claude 3.5 Sonnet和Claude 3 Sonnet、Google的Gemini 1.5 Pro和Gemini 1.5 Flash、OpenAI的GPT-4 Turbo和GPT-4o以及Meta的Llama3-70B。实验结果表明，LLMs在不同游戏和提示类型下的表现存在显著差异。例如，Claude 3.5 Sonnet在井字棋中表现出色，但在五子棋中表现较差。此外，LLMs在处理复杂和基于视觉的提示格式时也面临挑战。<br><br>## 💬 可借鉴之处<br>本文提出的基于网格的游戏竞赛基准为评估LLMs的能力提供了一个有价值的工具。此外，本文还提供了一个开放源代码和排行榜，方便研究人员进行进一步的分析和比较。本文的研究结果对于理解LLMs的能力和局限性具有重要意义，并为未来的研究和开发提供了方向。</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Simple Stochastic Stopping Games: A Generator and Benchmark Library</td>
      <td>Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the<br>simplest version of Stochastic Games for which there is no known<br>polynomial-time algorithm. Condon showed that Stochastic Games are<br>polynomial-time reducible to SSGs, which in turn are polynomial-time reducible<br>to Stopping Games. SSGs are games where all decisions are binary and every move<br>has a random outcome with a known probability distribution. Stopping Games are<br>SSGs that are guaranteed to terminate. There are many algorithms for SSGs, most<br>of which are fast in practice, but they all lack theoretical guarantees for<br>polynomial-time convergence. The pursuit of a polynomial-time algorithm for<br>SSGs is an active area of research. This paper is intended to support such<br>research by making it easier to study the graphical structure of SSGs. Our<br>contributions are: (1) a generating algorithm for Stopping Games, (2) a proof<br>that the algorithm can generate any game, (3) a list of additional<br>polynomial-time reductions that can be made to Stopping Games, (4) an open<br>source generator for generating fully reduced instances of Stopping Games that<br>comes with instructions and is fully documented, (5) a benchmark set of such<br>instances, (6) and an analysis of how two main algorithm types perform on our<br>benchmark set.</td>
      <td>## 🌟 论文解读 | 简单随机停止游戏的生成算法与基准库<br><br>## 📌 背景痛点/本文动机<br>简单随机游戏（SSGs）是随机游戏中已知没有多项式时间算法的最简单版本。尽管存在许多在实践中快速的算法，但它们都缺乏多项式时间收敛的理论保证。本文旨在通过提供一个生成算法和基准库来支持对SSGs的研究，使其更容易研究SSGs的图形结构。<br><br>## 🚀 核心方法<br>💡 创新点1：生成算法<br>本文提出了一个生成停止游戏实例的算法，该算法可以生成任何停止游戏，并具有多项式时间复杂度。<br><br>💡 创新点2：多项式时间简化<br>本文还提供了一系列多项式时间简化，可以进一步简化SSGs，使其更容易研究。这些简化包括移除可快速解决的子图、合并具有相同值的节点、将强连通分量独立解决等。<br><br>## 📈 实验结果<br>本文使用两种算法（Hoffman-Karp和排列改进算法）在生成的基准集上进行了实验，结果表明排列改进算法在迭代次数和时间上均优于Hoffman-Karp算法。<br><br>## 💬 可借鉴之处<br>本文提出的生成算法和基准库为SSGs的研究提供了重要的工具，可以帮助研究人员更好地理解SSGs的复杂性，并寻找更有效的算法。此外，本文提出的多项式时间简化方法也可以应用于其他组合优化问题，以提高算法的效率。</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Application-level Benchmarking of Quantum Computers using Nonlocal Game Strategies</td>
      <td>In a nonlocal game, two noncommunicating players cooperate to convince a<br>referee that they possess a strategy that does not violate the rules of the<br>game. Quantum strategies allow players to optimally win some games by<br>performing joint measurements on a shared entangled state, but computing these<br>strategies can be challenging. We present a variational quantum algorithm to<br>compute quantum strategies for nonlocal games by encoding the rules of a<br>nonlocal game into a Hamiltonian. We show how this algorithm can generate a<br>short-depth optimal quantum strategy for a graph coloring game with a quantum<br>advantage. This quantum strategy is then evaluated on fourteen different<br>quantum hardware platforms to demonstrate its utility as a benchmark. Finally,<br>we discuss potential sources of errors that can explain the observed decreased<br>performance of the executed task and derive an expression for the number of<br>samples required to accurately estimate the win rate in the presence of noise.</td>
      <td>## 🌟 论文解读 | 利用非局域游戏策略对量子计算机进行应用级基准测试<br><br>## 📌 背景痛点/本文动机<br>当前量子硬件的发展尚处于初级阶段，运行具有可证明优势的简单量子算法实例仍然具有挑战性。因此，开发能够测试和验证量子硬件独特特性的基准测试工具和技术至关重要。现有的低级基准测试指标（如随机基准测试）无法帮助确定算法流程中的错误来源，而高级基准测试指标（如量子体积）则过于宽泛，无法捕捉到特定硬件平台实现量子优势的能力。<br><br>## 🚀 核心方法<br>本文提出了一种新的方法，使用变分量子算法计算非局域游戏中的量子策略。该方法将非局域游戏的规则编码为哈密顿量，并通过双相优化技术找到资源态和测量方案。具体来说，该方法分为两个阶段：<br>1. **资源态优化**：使用变分量子本征求解器（如ADAPT-VQE）找到固定测量方案下的最优共享态。<br>2. **测量优化**：使用梯度下降算法优化测量参数，同时固定共享态。<br><br>## 📈 实验结果<br>本文将该方法应用于CHSH游戏、N-分部对称游戏和图着色游戏，并成功找到了最优量子策略。其中，图着色游戏策略在14个顶点的图上实现了量子优势，并展示了其在14个不同的量子硬件平台上的性能。实验结果表明，该方法可以有效评估量子硬件的非局域能力，并揭示潜在的错误来源。<br><br>## 💬 可借鉴之处<br>本文提出的变分量子算法为计算非局域游戏中的量子策略提供了一种有效的方法，并为量子硬件基准测试提供了一种新的思路。该方法可以应用于其他非局域游戏，并有助于开发更合适的量子基准测试工具，以适应量子设备的发展和硬件架构的复杂性。</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques</td>
      <td>The remarkable success of GPT models across various tasks, including toponymy<br>recognition motivates us to assess the performance of the GPT-3 model in the<br>geocoding address parsing task. To ensure that the evaluation more accurately<br>mirrors performance in real-world scenarios with diverse user input qualities<br>and resolve the pressing need for a 'gold standard' evaluation dataset for<br>geocoding systems, we introduce a benchmark dataset of low-quality address<br>descriptions synthesized based on human input patterns mining from actual input<br>logs of a geocoding system in production. This dataset has 21 different input<br>errors and variations; contains over 239,000 address records that are uniquely<br>selected from streets across all U.S. 50 states and D.C.; and consists of three<br>subsets to be used as training, validation, and testing sets. Building on this,<br>we train and gauge the performance of the GPT-3 model in extracting address<br>components, contrasting its performance with transformer-based and LSTM-based<br>models. The evaluation results indicate that Bidirectional LSTM-CRF model has<br>achieved the best performance over these transformer-based models and GPT-3<br>model. Transformer-based models demonstrate very comparable results compared to<br>the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in<br>performance, showcases potential in the address parsing task with few-shot<br>examples, exhibiting room for improvement with additional fine-tuning. We open<br>source the code and data of this presented benchmark so that researchers can<br>utilize it for future model development or extend it to evaluate similar tasks,<br>such as document geocoding.</td>
      <td>## 🌟 论文解读 | ChatGPT 在地理编码地址解析任务中的潜力评估<br><br>## 📌 背景痛点/本文动机<br>地理编码是将地址描述转换为地理坐标的过程，广泛应用于城市规划、公共卫生等领域。然而，地理编码数据的准确性受到地址解析质量的影响。地址解析是从用户输入的地址描述中提取地址组件的过程，其准确性对地理编码结果的准确性至关重要。现有的地址解析方法主要基于规则和统计方法，但难以处理用户输入中的错误和变化。此外，缺乏一个“黄金标准”的评估数据集来全面评估地理编码系统的性能。<br><br>## 🚀 核心方法<br>本文提出了一个用于评估地理编码地址解析技术的基准数据集和评估指标。该基准数据集是基于实际地理编码系统日志中的人类输入模式合成的低质量地址描述，包含21种不同的输入错误和变化，并覆盖了美国所有50个州和哥伦比亚特区的街道。该数据集分为训练集、验证集和测试集，用于评估不同模型的性能。<br><br>本文评估了五种不同的地址解析模型，包括GPT-3模型、基于Transformer的模型和基于LSTM的模型。评估结果表明，双向LSTM-CRF模型在地址组件提取方面表现最佳，其次是Transformer模型。尽管GPT-3模型的性能略逊一筹，但在少量样本的情况下展现出潜力，表明通过进一步的微调可以提升其性能。<br><br>## 📈 实验结果<br>实验结果表明，双向LSTM-CRF模型在所有地址组件的提取方面都优于其他模型，其F1分数和解析分数最高。Transformer模型在大多数地址组件的提取方面表现良好，但略逊于双向LSTM-CRF模型。GPT-3模型在地址组件提取方面表现相对较差，但在少量样本的情况下展现出潜力。<br><br>## 💬 可借鉴之处<br>本文提出的基准数据集和评估指标为评估地理编码地址解析技术提供了一个“黄金标准”，有助于研究人员更好地评估和比较不同模型的性能。此外，本文的研究结果表明，GPT-3模型在地址解析任务中具有潜力，但需要进一步的微调才能达到最佳性能。</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy</td>
      <td>How does one measure "ability to understand language"? If it is a person's<br>ability that is being measured, this is a question that almost never poses<br>itself in an unqualified manner: Whatever formal test is applied, it takes<br>place on the background of the person's language use in daily social practice,<br>and what is measured is a specialised variety of language understanding (e.g.,<br>of a second language; or of written, technical language). Computer programs do<br>not have this background. What does that mean for the applicability of formal<br>tests of language understanding? I argue that such tests need to be<br>complemented with tests of language use embedded in a practice, to arrive at a<br>more comprehensive evaluation of "artificial language understanding". To do<br>such tests systematically, I propose to use "Dialogue Games" -- constructed<br>activities that provide a situational embedding for language use. I describe a<br>taxonomy of Dialogue Game types, linked to a model of underlying capabilites<br>that are tested, and thereby giving an argument for the \emph{construct<br>validity} of the test. I close with showing how the internal structure of the<br>taxonomy suggests an ordering from more specialised to more general situational<br>language understanding, which potentially can provide some strategic guidance<br>for development in this field.</td>
      <td>## 🌟 论文解读 | 对话游戏：评估语言理解能力的有效工具<br><br>## 📌 背景痛点/本文动机<br>自然语言理解（NLU）领域长期以来依赖于基于文本的评估方法，例如问答或文本摘要。然而，这些方法往往忽略了语言在实际情境中的使用，例如对话中的指代、澄清和行动。本文旨在解决这一问题，提出了一种新的评估方法：对话游戏。<br><br>## 🚀 核心方法<br>💡 创新点1：对话游戏<br>对话游戏是一种构建的活动，参与者通过语言交流来达成特定目标。这些游戏可以模拟现实生活中的对话场景，例如组装家具或预订火车票。通过设计不同的游戏类型，可以针对语言理解的各个方面进行评估，例如指代、信息请求、行动协调等。<br><br>💡 创新点2：对话游戏分类<br>本文提出了一种对话游戏分类法，将游戏分为七种类型：指代游戏、信息游戏、构建游戏、导航游戏、谈判游戏和教学游戏。每种类型都侧重于语言理解的特定方面，并对应于模型中不同的知识和锚定过程。<br><br>## 📈 实验结果<br>本文没有提供具体的实验结果，而是重点介绍了对话游戏作为一种评估工具的理论基础和设计方法。然而，作者通过分析现有的对话游戏案例，展示了这种方法的有效性和潜力。<br><br>## 💬 可借鉴之处<br>对话游戏为评估语言理解能力提供了一种新的视角和方法。它可以用于：<br>* **评估模型在特定任务上的表现**：例如，指代游戏可以评估模型对指代表达的理解能力。<br>* **评估模型的整体语言理解能力**：通过设计一系列不同复杂度的对话游戏，可以评估模型在不同情境下的语言理解能力。<br>* **促进模型的发展**：对话游戏可以揭示模型在语言理解方面的不足，并为模型改进提供方向。<br><br>## 🌟 总结<br>对话游戏是一种评估语言理解能力的有效工具，它可以模拟现实生活中的对话场景，并针对语言理解的各个方面进行评估。本文提出的对话游戏分类法为设计对话游戏提供了指导，并为评估模型的语言理解能力提供了新的视角。</td>
    </tr>
    <tr>
      <th>19</th>
      <td>MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields</td>
      <td>Previous research has demonstrated the advantages of integrating data from<br>multiple sources over traditional unimodal data, leading to the emergence of<br>numerous novel multimodal applications. We propose a multimodal classification<br>benchmark MuG with eight datasets that allows researchers to evaluate and<br>improve their models. These datasets are collected from four various genres of<br>games that cover tabular, textual, and visual modalities. We conduct<br>multi-aspect data analysis to provide insights into the benchmark, including<br>label balance ratios, percentages of missing features, distributions of data<br>within each modality, and the correlations between labels and input modalities.<br>We further present experimental results obtained by several state-of-the-art<br>unimodal classifiers and multimodal classifiers, which demonstrate the<br>challenging and multimodal-dependent properties of the benchmark. MuG is<br>released at https://github.com/lujiaying/MUG-Bench with the data, tutorials,<br>and implemented baselines.</td>
      <td>## 🌟 论文解读 | MuG：基于游戏数据的多元分类基准<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，多模态学习已经成为一个重要的研究方向。多模态学习能够整合来自不同来源的数据，从而提高模型的性能和鲁棒性。然而，目前缺乏一个全面的基准数据集，用于评估和改进多模态分类模型。本文提出了一个名为 MuG 的多模态分类基准，旨在解决这一问题。<br><br>## 🚀 核心方法<br>MuG 包含了来自四个不同游戏类型的八个数据集，涵盖了表格、文本和视觉三种模态。这些数据集经过必要的清洗、转换和修改，以便于研究人员使用。为了更好地理解 MuG 的特性，本文进行了多方面的数据分析，包括标签平衡比例、缺失特征百分比、每种模态中数据的分布以及标签与输入模态之间的相关性。<br><br>## 📈 实验结果<br>本文使用多个最先进的单模态分类器和多模态分类器在 MuG 上进行了实验。结果表明，多模态分类器在大多数情况下都优于单模态分类器，这表明 MuG 中的分类任务确实依赖于多模态信息。此外，本文还提出了一种新的基于图注意力网络的多模态分类器 MUGNET，它在性能上与现有的多模态分类器相当。<br><br>## 💬 可借鉴之处<br>MuG 基准数据集为研究人员提供了一个评估和改进多模态分类模型的重要工具。此外，本文提出的 MUGNET 模型也为多模态学习提供了一种新的思路。未来，研究人员可以利用 MuG 基准数据集进行更多关于多模态学习的研究，并探索新的应用场景。</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Equilibria in Repeated Games under No-Regret with Dynamic Benchmarks</td>
      <td>In repeated games, strategies are often evaluated by their ability to<br>guarantee the performance of the single best action that is selected in<br>hindsight, a property referred to as \emph{Hannan consistency}, or<br>\emph{no-regret}. However, the effectiveness of the single best action as a<br>yardstick to evaluate strategies is limited, as any static action may perform<br>poorly in common dynamic settings. Our work therefore turns to a more ambitious<br>notion of \emph{dynamic benchmark consistency}, which guarantees the<br>performance of the best \emph{dynamic} sequence of actions, selected in<br>hindsight subject to a constraint on the allowable number of action changes.<br>Our main result establishes that for any joint empirical distribution of play<br>that may arise when all players deploy no-regret strategies, there exist<br>dynamic benchmark consistent strategies such that if all players deploy these<br>strategies the same empirical distribution emerges when the horizon is large<br>enough. This result demonstrates that although dynamic benchmark consistent<br>strategies have a different algorithmic structure and provide significantly<br>enhanced individual assurances, they lead to the same equilibrium set as<br>no-regret strategies. Moreover, the proof of our main result uncovers the<br>capacity of independent algorithms with strong individual guarantees to foster<br>a strong form of coordination.</td>
      <td>## 🌟 论文解读 | 探索重复博弈中的均衡策略：动态基准一致性<br><br>## 📌 背景痛点/本文动机<br>在重复博弈中，策略通常通过其在事后选择的最佳单次行动的表现来评估，这种性质被称为Hannan一致性或无后悔。然而，在动态环境中，任何静态行动的表现都可能不佳，因此这种评估策略的标准存在局限性。本文针对这一问题，提出了一个更为雄心勃勃的动态基准一致性概念，该概念确保了在事后选择的最佳动态行动序列的表现，同时受到允许的行动变化次数的约束。<br><br>## 🚀 核心方法<br>💡 创新点1<br>本文提出了动态基准一致性的概念，它考虑了在重复博弈中，玩家可能采取的一系列动态行动，而不是单一的静态行动。这种策略不仅考虑了单次行动的效果，还考虑了整个行动序列的适应性。<br><br>💡 创新点2<br>本文的主要结果是，对于所有玩家都采用无后悔策略时可能出现的任何联合经验分布，都存在动态基准一致性策略，使得当博弈的周期足够长时，所有玩家采用这些策略会产生相同的经验分布。这一结果证明了虽然动态基准一致性策略具有不同的算法结构，并为个体提供了显著增强的保障，但它们导致的均衡集合与无后悔策略相同。<br><br>## 📈 实验结果<br>本文通过理论分析证明了动态基准一致性策略的存在性和有效性，但没有提供具体的实验结果。作者通过数学证明展示了独立算法具有强烈个体保障的能力，能够促进一种强烈的协调形式。<br><br>## 💬 可借鉴之处<br>本文提出的动态基准一致性概念为重复博弈中的策略评估提供了一个新的视角，对于理解复杂动态环境中的博弈行为具有重要意义。此外，本文的证明过程揭示了具有强个体保障的独立算法在促进协调方面的潜力，这对于算法设计和博弈理论的应用具有启发意义。</td>
    </tr>
    <tr>
      <th>21</th>
      <td>A Game Benchmark for Real-Time Human-Swarm Control</td>
      <td>We present a game benchmark for testing human-swarm control algorithms and<br>interfaces in a real-time, high-cadence scenario. Our benchmark consists of a<br>swarm vs. swarm game in a virtual ROS environment in which the goal of the game<br>is to capture all agents from the opposing swarm; the game's high-cadence is a<br>result of the capture rules, which cause agent team sizes to fluctuate rapidly.<br>These rules require players to consider both the number of agents currently at<br>their disposal and the behavior of their opponent's swarm when they plan<br>actions. We demonstrate our game benchmark with a default human-swarm control<br>system that enables a player to interact with their swarm through a high-level<br>touchscreen interface. The touchscreen interface transforms player gestures<br>into swarm control commands via a low-level decentralized ergodic control<br>framework. We compare our default human-swarm control system to a<br>flocking-based control system, and discuss traits that are crucial for swarm<br>control algorithms and interfaces operating in real-time, high-cadence<br>scenarios like our game benchmark. Our game benchmark code is available on<br>Github; more information can be found at<br>https://sites.google.com/view/swarm-game-benchmark.</td>
      <td>## 🌟 论文解读 | 实时人机群控游戏基准：挑战与机遇<br><br>## 📌 背景痛点/本文动机<br>随着机器人技术的不断发展，机器人集群在危险环境中的应用越来越广泛。然而，现有的算法和界面在高度动态的环境中往往难以胜任，且缺乏有效的测试基准来评估这些算法和界面。为了解决这一问题，本文提出了一种基于游戏的实时人机群控基准，旨在模拟高度动态环境下的群控挑战，并评估不同算法和界面的性能。<br><br>## 🚀 核心方法<br>💡 创新点1：动态群控游戏基准<br>本文设计了一种名为“群控对抗”的游戏，玩家通过界面控制自己的机器人集群，目标是捕获对方集群的所有机器人。游戏规则导致集群规模快速变化，迫使玩家在制定策略时考虑自身集群规模和对方集群行为。<br><br>💡 创新点2：默认人机群控系统<br>本文展示了一个默认的人机群控系统，该系统包含一个触摸屏界面和一个低层级的去中心化遍历控制框架。触摸屏界面将玩家手势转换为集群控制命令，而控制框架则负责执行这些命令。<br><br>💡 创新点3：去中心化遍历控制<br>本文提出的去中心化遍历控制框架能够将玩家目标分布转换为低层级的集群控制命令，并实现规模不变性和排列不变性。这使得玩家可以专注于集群级别的行为，而不是单个机器人的轨迹。<br><br>## 📈 实验结果<br>本文通过实验比较了默认人机群控系统与基于群聚的控制系统的性能。结果表明，默认系统在实时、高频率的场景下具有更好的性能，能够更快地适应环境变化并制定策略。<br><br>## 💬 可借鉴之处<br>本文提出的游戏基准为评估实时人机群控算法和界面提供了一个有效的工具。此外，本文提出的去中心化遍历控制框架也为实现灵活、可扩展的群控策略提供了新的思路。<br><br>## 🌟 未来展望<br>未来研究可以开发虚拟对手来与人类玩家竞争，并扩展游戏基准以进行人类主体测试，以更深入地了解人类玩家在实时、高频率场景下的策略和行为。</td>
    </tr>
    <tr>
      <th>22</th>
      <td>WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments</td>
      <td>Recent advances in deep reinforcement learning (RL) have demonstrated complex<br>decision-making capabilities in simulation environments such as Arcade Learning<br>Environment, MuJoCo, and ViZDoom. However, they are hardly extensible to more<br>complicated problems, mainly due to the lack of complexity and variations in<br>the environments they are trained and tested on. Furthermore, they are not<br>extensible to an open-world environment to facilitate long-term exploration<br>research. To learn realistic task-solving capabilities, we need to develop an<br>environment with greater diversity and complexity. We developed WILD-SCAV, a<br>powerful and extensible environment based on a 3D open-world FPS (First-Person<br>Shooter) game to bridge the gap. It provides realistic 3D environments of<br>variable complexity, various tasks, and multiple modes of interaction, where<br>agents can learn to perceive 3D environments, navigate and plan, compete and<br>cooperate in a human-like manner. WILD-SCAV also supports different<br>complexities, such as configurable maps with different terrains, building<br>structures and distributions, and multi-agent settings with cooperative and<br>competitive tasks. The experimental results on configurable complexity,<br>multi-tasking, and multi-agent scenarios demonstrate the effectiveness of<br>WILD-SCAV in benchmarking various RL algorithms, as well as it is potential to<br>give rise to intelligent agents with generalized task-solving abilities. The<br>link to our open-sourced code can be found here<br>https://github.com/inspirai/wilderness-scavenger.</td>
      <td>## 🌟 论文解读 | WILD-SCAV：基于Unity3D的FPS游戏AI基准测试环境<br><br>## 📌 背景痛点/本文动机<br>近年来，深度强化学习（RL）在模拟环境（如Arcade Learning Environment、MuJoCo和ViZDoom）中展现出复杂的决策能力。然而，这些环境在复杂性和多样性方面存在局限性，难以扩展到更复杂的问题。此外，它们也不适用于开放世界环境，无法促进长期探索研究。为了学习现实世界的任务解决能力，我们需要开发一个具有更高多样性和复杂性的环境。<br><br>## 🚀 核心方法<br>💡 创新点1：WILD-SCAV是一个基于3D开放世界FPS游戏的强大且可扩展的环境，旨在填补这一空白。它提供了具有可变复杂性的真实3D环境、各种任务和多种交互模式，使代理能够以类似人类的方式学习感知3D环境、导航和规划、竞争和合作。<br><br>💡 创新点2：WILD-SCAV支持不同的复杂性，例如具有不同地形、建筑结构和分布的可配置地图，以及具有合作和竞争任务的多代理设置。<br><br>## 📈 实验结果<br>实验结果表明，WILD-SCAV在可配置复杂性、多任务和多代理场景中有效地评估了各种RL算法的性能。此外，WILD-SCAV具有产生具有通用任务解决能力的智能代理的潜力。<br><br>## 💬 可借鉴之处<br>WILD-SCAV为研究开放世界智能代理学习提供了一个强大的平台，并支持各种实验和任务设计。它可以帮助研究人员开发更强大的RL算法，并推动3D开放世界AI算法的发展。</td>
    </tr>
    <tr>
      <th>23</th>
      <td>The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning</td>
      <td>As machine learning (ML) is more tightly woven into society, it is imperative<br>that we better characterize ML's strengths and limitations if we are to employ<br>it responsibly. Existing benchmark environments for ML, such as board and video<br>games, offer well-defined benchmarks for progress, but constituent tasks are<br>often complex, and it is frequently unclear how task characteristics contribute<br>to overall difficulty for the machine learner. Likewise, without a systematic<br>assessment of how task characteristics influence difficulty, it is challenging<br>to draw meaningful connections between performance in different benchmark<br>environments. We introduce a novel benchmark environment that offers an<br>enormous range of ML challenges and enables precise examination of how task<br>elements influence practical difficulty. The tool frames learning tasks as a<br>"board-clearing game," which we call the Game of Hidden Rules (GOHR). The<br>environment comprises an expressive rule language and a captive server<br>environment that can be installed locally. We propose a set of benchmark<br>rule-learning tasks and plan to support a performance leader-board for<br>researchers interested in attempting to learn our rules. GOHR complements<br>existing environments by allowing fine, controlled modifications to tasks,<br>enabling experimenters to better understand how each facet of a given learning<br>task contributes to its practical difficulty for an arbitrary ML algorithm.</td>
      <td>## 🌟 论文解读 | 探索机器学习新挑战：隐藏规则游戏<br><br>## 📌 背景痛点/本文动机<br>随着机器学习（ML）在社会中的应用越来越广泛，我们需要更好地了解其优势和局限性，以便负责任地使用它。现有的ML基准环境，如棋盘游戏和视频游戏，提供了明确的基准来衡量进步，但这些任务通常很复杂，而且不清楚任务特征如何影响机器学习者的整体难度。此外，没有对任务特征如何影响难度的系统评估，很难在不同基准环境中绘制出有意义的性能联系。<br><br>## 🚀 核心方法<br>本文提出了一种新的基准环境，称为“隐藏规则游戏”（GOHR），它提供了一个广泛的ML挑战，并能够精确地检查任务元素如何影响实际难度。GOHR将学习任务框架化为一个“清盘游戏”，环境包括一个丰富的规则语言和一个可本地安装的专用服务器环境。我们提出了一系列基准规则学习任务，并计划为有兴趣学习我们规则的研究人员支持一个性能排行榜。GOHR通过允许对任务进行精细、可控的修改，补充了现有的环境，使实验者能够更好地理解给定学习任务的每个方面如何为任意ML算法的实际难度做出贡献。<br><br>## 📈 实验结果<br>我们提出了一系列样本规则，一个样本ML算法（MLA），性能指标和有用的数据呈现方法。我们使用DQN算法对样本规则进行了学习，并分析了累积错误数。我们发现，对于所有样本规则，累积错误最终都会趋于平稳，我们将其称为“终端累积错误”（TCE）。我们还使用Mann-Whitney-Wilcoxon U-Test对规则难度进行了比较，并发现U-Test与累积错误曲线的高度一致。<br><br>## 💬 可借鉴之处<br>GOHR为研究ML算法的性能提供了一个新颖而有原则的方法。使用丰富的规则语法，研究人员可以对感兴趣的规则进行精确的更改，以研究它们如何影响算法性能。这种方法可以用于比较不同算法在特定规则集上的性能，并观察规则特征（位置、属性、静态/动态）对MLA性能的影响。GOHR还可以由人类规则学习者进行游戏，从而能够在平等的基础上比较人类学习和ML。</td>
    </tr>
    <tr>
      <th>24</th>
      <td>GOAL: Towards Benchmarking Few-Shot Sports Game Summarization</td>
      <td>Sports game summarization aims to generate sports news based on real-time<br>commentaries. The task has attracted wide research attention but is still<br>under-explored probably due to the lack of corresponding English datasets.<br>Therefore, in this paper, we release GOAL, the first English sports game<br>summarization dataset. Specifically, there are 103 commentary-news pairs in<br>GOAL, where the average lengths of commentaries and news are 2724.9 and 476.3<br>words, respectively. Moreover, to support the research in the semi-supervised<br>setting, GOAL additionally provides 2,160 unlabeled commentary documents. Based<br>on our GOAL, we build and evaluate several baselines, including extractive and<br>abstractive baselines. The experimental results show the challenges of this<br>task still remain. We hope our work could promote the research of sports game<br>summarization. The dataset has been released at<br>https://github.com/krystalan/goal.</td>
      <td>## 🌟 论文解读 | GOAL：迈向少样本体育游戏摘要基准<br><br>## 📌 背景痛点/本文动机<br>体育游戏摘要的目标是基于实时评论生成体育新闻。尽管该任务已经引起了广泛的关注，但由于缺乏相应的英文数据集，因此仍然没有得到充分的探索。为了解决这个问题，本文提出了GOAL，这是第一个英文体育游戏摘要数据集。<br><br>## 🚀 核心方法<br>💡 创新点1：构建了GOAL数据集，包含103个评论-新闻对，其中评论和新闻的平均长度分别为2724.9和476.3个单词。此外，为了支持半监督学习，GOAL还提供了2160个未标记的评论文档。<br>💡 创新点2：在GOAL数据集的基础上，构建和评估了几个基线模型，包括提取式和抽象式基线。实验结果表明，该任务仍然具有挑战性。<br><br>## 📈 实验结果<br>实验结果表明，提取式基线的性能有限，因为评论和新闻之间的文本风格不同。PGN模型优于提取式方法，因为它可以生成不受原始单词或短语限制的体育新闻。然而，LED模型在所有基线中表现最佳，因为它具有抽象性质和稀疏注意力机制。<br><br>## 💬 可借鉴之处<br>本文提出的GOAL数据集和基线模型为体育游戏摘要研究提供了重要的参考。此外，本文还讨论了利用外部资源来增强模型对体育文本的理解和处理能力的必要性。未来的研究方向包括探索半监督和多语言设置，利用图结构来建模评论信息，并采用其他长文档摘要资源来构建多领域或跨领域模型。</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Meterstick: Benchmarking Performance Variability in Cloud and Self-hosted Minecraft-like Games Extended Technical Report</td>
      <td>Due to increasing popularity and strict performance requirements, online<br>games have become a workload of interest for the performance engineering<br>community. One of the most popular types of online games is the Minecraft-like<br>Game (MLG), in which players can terraform the environment. The most popular<br>MLG, Minecraft, provides not only entertainment, but also educational support<br>and social interaction, to over 130 million people world-wide. MLGs currently<br>support their many players by replicating isolated instances that support each<br>only up to a few hundred players under favorable conditions. In practice, as we<br>show here, the real upper limit of supported players can be much lower. In this<br>work, we posit that performance variability is a key cause for the lack of<br>scalability in MLGs. We propose a novel operational model for MLGs and use it<br>to design the first benchmark that focuses on MLG performance variability,<br>defining specialized workloads, metrics, and processes. We conduct real-world<br>benchmarking of MLGs and find environment-based workloads and cloud deployment<br>to be significant sources of performance variability: peak-latency degrades<br>sharply to 20.7 times the arithmetic mean, and exceeds by a factor of 7.4 the<br>performance requirements. We derive actionable insights for game-developers,<br>game-operators, and other stakeholders to tame performance variability.</td>
      <td>## 🌟 论文解读 | Meterstick：揭秘Minecraft-like游戏性能波动之谜<br><br>## 📌 背景痛点/本文动机<br><br>随着在线游戏越来越受欢迎，对性能的要求也越来越严格，游戏成为了性能工程领域关注的重点。Minecraft-like游戏（MLG）作为其中最受欢迎的类型之一，允许玩家改变游戏环境，拥有庞大的玩家群体。然而，MLG的可扩展性却一直受限，即使是像Minecraft这样的热门游戏，在最佳条件下也只能支持几百名玩家。本文认为，性能波动是导致MLG可扩展性受限的关键因素，并设计了一个名为Meterstick的基准测试工具，用于量化MLG的性能波动。<br><br>## 🚀 核心方法<br><br>💡 创新点1：提出MLG操作模型<br><br>本文首先提出了一个MLG操作模型，该模型考虑了MLG特有的工作负载，例如玩家行为、地形模拟和实体模拟。这些工作负载与传统游戏不同，需要特殊的处理方式。<br><br>💡 创新点2：设计Meterstick基准测试工具<br><br>Meterstick是一个用于评估MLG性能波动的基准测试工具。它定义了专门的工作负载、指标和流程，并支持云环境和自托管环境。Meterstick使用了一个名为Instability Ratio（ISR）的新指标来量化性能波动，该指标基于周期性抖动，并考虑了时间戳顺序、异常值和采样持续时间等因素。<br><br>## 📈 实验结果<br><br>本文使用Meterstick对Minecraft、Forge和PaperMC三种MLG进行了实验，并发现：<br><br>* **性能波动会导致游戏不可玩**：最大响应时间可以达到平均值的20.7倍，超过可玩性阈值的7.4倍。<br>* **基于环境的工作负载会导致显著的性能波动**：环境工作负载会导致ISR显著增加，并可能导致游戏过载甚至崩溃。<br>* **商业云环境中的性能波动比自托管环境更大**：AWS和Azure云环境中的性能波动比DAS-5超级计算机更大。<br>* **处理实体状态的计算成本很高**：实体相关的计算占用了大部分的计算时间和状态更新消息。<br>* **推荐的硬件配置不足以避免性能波动**：推荐的云环境硬件配置会导致性能波动，而更强大的硬件可以将其限制在可接受的水平。<br><br>## 💬 可借鉴之处<br><br>* **游戏开发者应报告性能波动**：使用ISR等指标来评估在线游戏性能，并提供一致的良好性能。<br>* **游戏开发者应将基于环境的工作负载纳入MLG基准测试**：环境工作负载对MLG性能有显著影响，必须纳入基准测试。<br>* **玩家应根据MLG选择云环境，并考虑自托管**：不同MLG在不同云环境中的性能表现不同，自托管也是一个不错的选择。<br>* **MLG服务提供商应提高硬件配置推荐**：推荐的云环境硬件配置不足以满足MLG的性能需求，需要更新推荐配置。<br>* **游戏开发者应优化MLG以减少环境工作负载的影响**：通过性能工程方法，可以显著降低环境工作负载对性能的影响。<br><br>## 🎯 未来展望<br><br>本文提出的Meterstick基准测试工具和ISR指标为MLG性能评估提供了新的思路。未来可以进一步研究ISR与玩家感知质量之间的关系，并创建一个公共排行榜，方便MLG服务提供商发布基准测试分数。</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game</td>
      <td>Simulated DAG models may exhibit properties that, perhaps inadvertently,<br>render their structure identifiable and unexpectedly affect structure learning<br>algorithms. Here, we show that marginal variance tends to increase along the<br>causal order for generically sampled additive noise models. We introduce<br>varsortability as a measure of the agreement between the order of increasing<br>marginal variance and the causal order. For commonly sampled graphs and model<br>parameters, we show that the remarkable performance of some continuous<br>structure learning algorithms can be explained by high varsortability and<br>matched by a simple baseline method. Yet, this performance may not transfer to<br>real-world data where varsortability may be moderate or dependent on the choice<br>of measurement scales. On standardized data, the same algorithms fail to<br>identify the ground-truth DAG or its Markov equivalence class. While<br>standardization removes the pattern in marginal variance, we show that data<br>generating processes that incur high varsortability also leave a distinct<br>covariance pattern that may be exploited even after standardization. Our<br>findings challenge the significance of generic benchmarks with independently<br>drawn parameters. The code is available at<br>https://github.com/Scriddie/Varsortability.</td>
      <td>## 🌟 论文解读 | 小心模拟的DAG！因果发现基准可能容易被操纵<br><br>## 📌 背景痛点/本文动机<br>因果结构学习旨在从数据中推断因果模型。在生物学、医学、金融和机器学习等领域，因果模型都备受关注。因果模型不仅描述了变量的观测联合分布，还形式化了干预和反事实预测。有向无环图（DAG）是表示因果结构的常用方法，其中节点代表变量，有向边表示因果关系。然而，从观测数据中推断因果结构是困难的，通常只能识别DAG的马尔可夫等价类（MEC），并且找到高分的DAG是NP-hard问题。本文研究了线性加性噪声模型（ANM）的DAG学习问题。<br><br>## 🚀 核心方法<br>💡 创新点1：本文发现，对于通用采样的加性噪声模型，边际方差倾向于沿着因果顺序增加。为了衡量边际方差增加顺序与因果顺序之间的相关性，本文引入了“方差排序性”作为衡量指标。<br><br>💡 创新点2：本文发现，对于常见的采样图和模型参数，一些连续结构学习算法的出色性能可以通过高方差排序性来解释，并且可以通过一个简单的基线方法来匹配。然而，这种性能可能不会转移到现实世界的数据中，因为方差排序性可能适中或依赖于测量尺度的选择。<br><br>💡 创新点3：本文提出了一种名为“sortnregress”的简单基线方法，该方法通过按边际方差排序变量并使用稀疏回归技术选择父节点来工作。sortnregress的性能反映了给定设置中方差排序性的程度，并为结构学习算法提供了一个参考基线。<br><br>## 📈 实验结果<br>本文在合成数据上进行了实验，结果表明，一些算法在原始数据上表现良好，但在标准化数据上表现较差。sortnregress在原始数据上取得了有竞争力的性能，而在标准化数据上则表现出了基线性能。这表明，方差排序性在结构学习任务中起着重要作用。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，连续结构学习算法对数据缩放非常敏感，并且在不知道真实数据尺度的情况下可能无法很好地工作。因此，在使用和评估这些算法时，需要谨慎考虑数据尺度的影响。此外，本文提出的方差排序性概念和sortnregress基线方法可以用于评估和比较不同结构学习算法在不同基准场景中的性能。</td>
    </tr>
    <tr>
      <th>27</th>
      <td>OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research</td>
      <td>Owning to the unremitting efforts by a few institutes, significant progress<br>has recently been made in designing superhuman AIs in No-limit Texas Hold'em<br>(NLTH), the primary testbed for large-scale imperfect-information game<br>research. However, it remains challenging for new researchers to study this<br>problem since there are no standard benchmarks for comparing with existing<br>methods, which seriously hinders further developments in this research area. In<br>this work, we present OpenHoldem, an integrated toolkit for large-scale<br>imperfect-information game research using NLTH. OpenHoldem makes three main<br>contributions to this research direction: 1) a standardized evaluation protocol<br>for thoroughly evaluating different NLTH AIs, 2) four publicly available strong<br>baselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs<br>for public NLTH AI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,<br>hoping it facilitates further studies on the unsolved theoretical and<br>computational issues in this area and cultivate crucial research problems like<br>opponent modeling and human-computer interactive learning.</td>
      <td>## 🌟 论文解读 | OpenHoldem：大规模不完全信息游戏研究的基准<br><br>## 📌 背景痛点/本文动机<br>近年来，在无限制德州扑克（NLTH）领域，设计出超越人类的AI取得了显著进展，NLTH已成为大规模不完全信息游戏研究的主要测试平台。然而，由于缺乏标准基准，新研究人员难以研究此问题，这严重阻碍了该研究领域的进一步发展。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：标准化评估协议<br>OpenHoldem 提出了一个标准化的评估协议，包括四种不同的评估指标，用于全面评估不同的 NLTH AI。这些指标包括：<br>- **对局评估指标**：通过重复对局，评估 AI 的平均效用，并使用方差减少技术（如复制扑克和 AIVAT）来减少随机性影响。<br>- **可利用性评估指标**：通过计算最佳响应策略，评估 AI 的可利用性，并使用局部最佳响应（LBR）和深度强化学习（DRL-BR）来近似计算。<br><br>💡 创新点2：公开可用的强基线 AI<br>OpenHoldem 设计并实现了四种不同类型的 NLTH AI，作为未来研究的良好起点：<br>- **基于规则的 AI**：由领域专家设计的规则集合，用于处理各种场景。<br>- **基于 CFR 的静态 AI**：使用 CFR 算法近似求解纳什均衡策略，并通过信息抽象和动作抽象来降低游戏规模。<br>- **类似 DeepStack 的在线 AI**：使用持续重解和深度神经网络来处理离树动作，并提高决策效率。<br>- **基于深度强化学习的 AI**：使用端到端深度强化学习框架，直接从游戏状态学习到动作，无需手动设计特征或进行迭代推理。<br><br>💡 创新点3：在线测试平台<br>OpenHoldem 开发了一个在线测试平台，内置了四种强基线 AI，并提供了易于使用的 API，方便研究人员测试和比较他们的 AI。<br><br>## 📈 实验结果<br>OpenHoldem 的基线 AI 在与现有公开可用的 NLTH AI 的比较中表现出色，证明了其有效性。此外，消融实验分析了不同设计选择对 AI 性能的影响，并验证了 OpenHoldem 的各个组件的有效性。<br><br>## 💬 可借鉴之处<br>OpenHoldem 为大规模不完全信息游戏研究提供了一个宝贵的工具，其标准化评估协议、强基线 AI 和在线测试平台将促进该领域的进一步发展。OpenHoldem 的设计思路和实现方法也为其他游戏 AI 研究提供了参考。</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Towards Game-Playing AI Benchmarks via Performance Reporting Standards</td>
      <td>While games have been used extensively as milestones to evaluate game-playing<br>AI, there exists no standardised framework for reporting the obtained<br>observations. As a result, it remains difficult to draw general conclusions<br>about the strengths and weaknesses of different game-playing AI algorithms. In<br>this paper, we propose reporting guidelines for AI game-playing performance<br>that, if followed, provide information suitable for unbiased comparisons<br>between different AI approaches. The vision we describe is to build benchmarks<br>and competitions based on such guidelines in order to be able to draw more<br>general conclusions about the behaviour of different AI algorithms, as well as<br>the types of challenges different games pose.</td>
      <td>## 🌟 论文解读 | 游戏AI性能评估：迈向标准化报告框架<br><br>## 📌 背景痛点/本文动机<br>游戏一直是评估游戏AI的重要里程碑，但缺乏标准化的报告框架，导致难以对不同的游戏AI算法进行公正比较。本文旨在提出一套报告指南，以促进不同AI方法之间的公平比较，并构建基于这些指南的基准测试和竞赛，从而更全面地了解不同AI算法的行为和不同游戏带来的挑战。<br><br>## 🚀 核心方法<br>💡 创新点1：提出游戏AI性能报告指南<br>本文提出了一个报告指南，旨在为游戏AI性能报告提供一个结构化的框架。该指南包括以下三个主要部分：<br>1. **问题描述**：详细描述测试问题的特征，包括任务类型、游戏内性能指标、游戏-AI交互方式和约束条件。<br>2. **解决方案复杂性描述**：描述AI解决方案的复杂性，包括硬件、操作系统、软件、计算复杂性、模型复杂性和实现复杂性。<br>3. **性能量化**：描述性能测量的方法，包括统计显著性评估、对手类型、测试案例的多样性和性能指标的聚合方式。<br><br>💡 创新点2：构建可解释的基准测试<br>本文提出了构建可解释基准测试的愿景，并提出了以下要求：<br>- 产生可测量的结果<br>- 允许得出有意义的结论<br>- 可解释性<br>- 产生可推广的结论<br>- 可重复性<br>- 产生稳健的结果<br>- 允许比较<br>- 实用性<br>- 公开偏见<br>- 考虑解决方案复杂性<br><br>## 📈 实验结果<br>本文尚未进行实验，但提出了一个报告指南，旨在促进游戏AI性能评估的标准化和可解释性。<br><br>## 💬 可借鉴之处<br>本文提出的报告指南和基准测试框架对于游戏AI研究人员和开发者具有重要的参考价值。通过遵循这些指南，可以更全面地了解不同AI算法的行为和性能，并促进游戏AI领域的进一步发展。</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Interbank lending with benchmark rates: Pareto optima for a class of singular control games</td>
      <td>We analyze a class of stochastic differential games of singular control,<br>motivated by the study of a dynamic model of interbank lending with benchmark<br>rates. We describe Pareto optima for this game and show how they may be<br>achieved through the intervention of a regulator, whose policy is a solution to<br>a singular stochastic control problem. Pareto optima are characterized in terms<br>of the solutions to a new class of Skorokhod problems with piecewise-continuous<br>free boundary.<br>  Pareto optimal policies are shown to correspond to the enforcement of<br>endogenous bounds on interbank lending rates. Analytical comparison between<br>Pareto optima and Nash equilibria provides insight into the impact of<br>regulatory intervention on the stability of interbank rates.</td>
      <td>## 🌟 论文解读 | 基准利率下的银行间借贷：一类奇异控制博弈的帕累托最优解<br><br>## 📌 背景痛点/本文动机<br>银行间借贷市场是金融机构之间战略互动的有趣例子，参与者会根据其他参与者的行动分布做出反应。银行间基准利率的设定机制，如伦敦银行同业拆借利率（LIBOR），在金融市场中扮演着核心角色。然而，这种机制容易受到操纵，导致市场不稳定。本文旨在研究银行间借贷市场中，监管者如何通过干预来达到帕累托最优解，从而提高市场稳定性。<br><br>## 🚀 核心方法<br>💡 创新点1：本文构建了一个包含基准利率的银行间借贷动态模型，并将其转化为一个奇异控制博弈问题。<br>💡 创新点2：本文通过研究监管者的辅助控制问题，得到了帕累托最优解的刻画，并将其与Nash均衡进行了比较，揭示了监管干预对银行间利率稳定性的影响。<br>💡 创新点3：本文将帕累托最优解描述为一系列具有分段连续自由边界的Skorokhod问题，并通过求解这些问题得到了帕累托最优策略的显式表达式。<br><br>## 📈 实验结果<br>本文在N=2的情况下得到了帕累托最优策略的显式解，并与Nash均衡进行了比较。结果表明，帕累托最优策略能够有效降低银行间利率的波动性，提高市场稳定性。<br><br>## 💬 可借鉴之处<br>本文的研究结果为银行间借贷市场的监管提供了理论依据，并为设计有效的监管政策提供了参考。此外，本文提出的奇异控制博弈模型和Skorokhod问题的求解方法，可以应用于其他金融市场的建模和分析。</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Benchmarking End-to-End Behavioural Cloning on Video Games</td>
      <td>Behavioural cloning, where a computer is taught to perform a task based on<br>demonstrations, has been successfully applied to various video games and<br>robotics tasks, with and without reinforcement learning. This also includes<br>end-to-end approaches, where a computer plays a video game like humans do: by<br>looking at the image displayed on the screen, and sending keystrokes to the<br>game. As a general approach to playing video games, this has many inviting<br>properties: no need for specialized modifications to the game, no lengthy<br>training sessions and the ability to re-use the same tools across different<br>games. However, related work includes game-specific engineering to achieve the<br>results. We take a step towards a general approach and study the general<br>applicability of behavioural cloning on twelve video games, including six<br>modern video games (published after 2010), by using human demonstrations as<br>training data. Our results show that these agents cannot match humans in raw<br>performance but do learn basic dynamics and rules. We also demonstrate how the<br>quality of the data matters, and how recording data from humans is subject to a<br>state-action mismatch, due to human reflexes.</td>
      <td>## 🌟 论文解读 | 行为克隆在视频游戏中的端到端基准测试<br><br>## 📌 背景痛点/本文动机<br>行为克隆是一种基于人类演示来训练计算机执行任务的机器学习方法。它已被成功应用于各种视频游戏和机器人任务，包括端到端方法，其中计算机通过观察屏幕上的图像并发送按键来像人类一样玩游戏。这种方法具有许多吸引人的特性，例如无需对游戏进行特殊修改、无需长时间训练以及能够在不同游戏之间重用相同工具。然而，相关工作通常需要针对特定游戏进行工程化才能取得成果。本文旨在研究行为克隆在视频游戏中的通用性，并使用人类演示作为训练数据，在十二款视频游戏（包括六款现代视频游戏）上进行了研究。<br><br>## 🚀 核心方法<br>本文使用深度神经网络来学习人类玩家的行为，并使用人类演示数据来训练模型。模型通过观察屏幕图像来预测人类玩家的动作，并使用这些预测来控制游戏。本文还研究了数据质量和数量对行为克隆性能的影响，以及人类反应时间延迟对数据质量的影响。<br><br>## 📈 实验结果<br>实验结果表明，行为克隆代理可以学习游戏的基本动态和规则，但通常只能达到人类玩家性能的一小部分，有时甚至不如随机代理。本文还发现，当只有少量数据可用时，数据数量对结果的影响较小，而调整人类反应时间延迟可以提高性能。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，行为克隆在视频游戏中的应用具有潜力，但仍面临一些挑战。本文提出的建议包括使用高质量数据、调整人类反应时间延迟以及探索其他机器学习方法，以提高行为克隆的性能。</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Design, Benchmarking and Explainability Analysis of a Game-Theoretic Framework towards Energy Efficiency in Smart Infrastructure</td>
      <td>In this paper, we propose a gamification approach as a novel framework for<br>smart building infrastructure with the goal of motivating human occupants to<br>reconsider personal energy usage and to have positive effects on their<br>environment. Human interaction in the context of cyber-physical systems is a<br>core component and consideration in the implementation of any smart building<br>technology. Research has shown that the adoption of human-centric building<br>services and amenities leads to improvements in the operational efficiency of<br>these cyber-physical systems directed towards controlling building energy<br>usage. We introduce a strategy in form of a game-theoretic framework that<br>incorporates humans-in-the-loop modeling by creating an interface to allow<br>building managers to interact with occupants and potentially incentivize energy<br>efficient behavior. Prior works on game theoretic analysis typically rely on<br>the assumption that the utility function of each individual agent is known a<br>priori. Instead, we propose novel utility learning framework for benchmarking<br>that employs robust estimations of occupant actions towards energy efficiency.<br>To improve forecasting performance, we extend the utility learning scheme by<br>leveraging deep bi-directional recurrent neural networks. Using the proposed<br>methods on data gathered from occupant actions for resources such as room<br>lighting, we forecast patterns of energy resource usage to demonstrate the<br>prediction performance of the methods. The results of our study show that we<br>can achieve a highly accurate representation of the ground truth for occupant<br>energy resource usage. We also demonstrate the explainable nature on human<br>decision making towards energy usage inherent in the dataset using graphical<br>lasso and granger causality algorithms. Finally, we open source the<br>de-identified, high-dimensional data pertaining to the energy game-theoretic<br>framework.</td>
      <td>## 🌟 论文解读 | 游戏化智能建筑：激励节能行为的新框架<br><br>## 📌 背景痛点/本文动机<br>随着全球能源消耗的持续增长，建筑能耗已成为一个重要的关注点。尽管智能建筑技术旨在提高能源效率，但人类行为的不确定性往往导致能源消耗的不可预测性。本文提出了一种基于博弈论的游戏化方法，旨在激励建筑使用者重新考虑个人能源使用，从而对环境产生积极影响。<br><br>## 🚀 核心方法<br>💡 创新点1：博弈论框架<br>本文引入了一个博弈论框架，将建筑使用者视为非合作博弈中的智能体。该框架通过创建一个界面，允许建筑管理者与使用者互动，并可能激励他们采取节能行为。与以往博弈论分析通常假设每个智能体的效用函数是预先已知的相比，本文提出了一种新颖的效用学习框架，用于基准测试，该框架采用对节能行为的 occupant 行动进行鲁棒估计。<br><br>💡 创新点2：深度双向循环神经网络<br>为了提高预测性能，本文扩展了效用学习方案，利用深度双向循环神经网络（RNN）。使用从 occupant 行动中收集的数据，例如房间照明，预测能源资源使用模式，以展示方法的预测性能。结果表明，可以实现 occupant 能源资源使用的高度准确表示。<br><br>💡 创新点3：可解释性分析<br>本文使用图形拉索和格兰杰因果算法，展示了数据集中固有的对人类决策制定的可解释性。这些算法揭示了不同特征之间的潜在条件依赖关系，以及特征之间的因果关系，从而加深了对人类能源使用决策制定的理解。<br><br>## 📈 实验结果<br>实验结果表明，本文提出的效用学习方法能够准确预测 occupant 的能源使用行为，并且深度双向 RNN 模型在预测性能方面优于其他模型。此外，可解释性分析揭示了不同能源效率行为类别中特征之间的相关性，以及特征之间的因果关系，从而加深了对人类能源使用决策制定的理解。<br><br>## 💬 可借鉴之处<br>本文提出的游戏化智能建筑框架为提高建筑能源效率提供了一种新颖的方法。该方法通过激励 occupant 采取节能行为，并利用深度学习技术预测 occupant 的能源使用行为，从而为智能建筑管理提供了新的思路。此外，本文提出的可解释性分析方法可以帮助我们更好地理解人类能源使用决策制定，并为设计更有效的激励措施提供依据。</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Quantum Poker A game for quantum computers suitable for benchmarking error mitigation techniques on NISQ devices</td>
      <td>Quantum computers are on the verge of becoming a commercially available<br>reality. They represent a paradigm shift in computing, with a steep learning<br>gradient. The creation of games is a way to ease the transition for beginners.<br>We present a game similar to the Poker variant Texas hold 'em with the<br>intention to serve as an engaging pedagogical tool to learn the basics rules of<br>quantum computing. The concepts of quantum states, quantum operations and<br>measurement can be learned in a playful manner. The difference to the classical<br>variant is that the community cards are replaced by a quantum register that is<br>"randomly" initialized, and the cards for each player are replaced by quantum<br>gates, randomly drawn from a set of available gates. Each player can create a<br>quantum circuit with their cards, with the aim to maximize the number of \( 1 \)'s<br>that are measured in the computational basis. The basic concepts of<br>superposition, entanglement and quantum gates are employed. We provide a<br>proof-of-concept implementation using Qiskit. A comparison of the results for<br>the created circuits using a simulator and IBM machines is conducted, showing<br>that error rates on contemporary quantum computers are still very high. For the<br>success of noisy intermediate scale quantum (NISQ) computers, improvements on<br>the error rates and error mitigation techniques are necessary, even for simple<br>circuits. We show that quantum error mitigation (QEM) techniques can be used to<br>improve expectation values of observables on real quantum devices.</td>
      <td>## 🌟 论文解读 | 量子扑克：在NISQ设备上测试错误缓解技术的游戏<br><br>## 📌 背景痛点/本文动机<br>量子计算机正逐渐成为商业现实，但它们的学习曲线陡峭，需要跨学科的知识。为了帮助初学者更容易地学习量子计算的基本规则，本文提出了一种名为“量子扑克”的游戏。该游戏旨在通过游戏化的方式教授量子状态、量子操作和测量的概念。<br><br>## 🚀 核心方法<br>💡 创新点1：量子扑克游戏<br>量子扑克游戏类似于德州扑克，但社区牌被量子寄存器取代，玩家手中的牌被量子门取代。每个玩家都可以使用他们的牌创建一个量子电路，目标是最大化在计算基中测量到的“1”的数量。游戏利用了叠加、纠缠和量子门的基本概念。<br><br>💡 创新点2：错误缓解技术<br>本文使用零噪声外推方法来缓解量子计算机上的错误。通过故意放大噪声并使用外推技术，可以更准确地估计可观测量在真实量子设备上的期望值。<br><br>## 📈 实验结果<br>实验结果表明，即使在简单的电路中，当代量子计算机的错误率仍然很高。使用错误缓解技术可以显著提高期望值的估计精度。<br><br>## 💬 可借鉴之处<br>量子扑克游戏可以作为学习量子计算基本概念的有益工具。此外，本文提出的错误缓解技术可以应用于其他量子计算任务，以提高结果的准确性。</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks</td>
      <td>While Nash equilibrium in extensive-form games is well understood, very<br>little is known about the properties of extensive-form correlated equilibrium<br>(EFCE), both from a behavioral and from a computational point of view. In this<br>setting, the strategic behavior of players is complemented by an external<br>device that privately recommends moves to agents as the game progresses;<br>players are free to deviate at any time, but will then not receive future<br>recommendations. Our contributions are threefold. First, we show that an EFCE<br>can be formulated as the solution to a bilinear saddle-point problem. To<br>showcase how this novel formulation can inspire new algorithms to compute<br>EFCEs, we propose a simple subgradient descent method which exploits this<br>formulation and structural properties of EFCEs. Our method has better<br>scalability than the prior approach based on linear programming. Second, we<br>propose two benchmark games, which we hope will serve as the basis for future<br>evaluation of EFCE solvers. These games were chosen so as to cover two natural<br>application domains for EFCE: conflict resolution via a mediator, and<br>bargaining and negotiation. Third, we document the qualitative behavior of EFCE<br>in our proposed games. We show that the social-welfare-maximizing equilibria in<br>these games are highly nontrivial and exhibit surprisingly subtle sequential<br>behavior that so far has not received attention in the literature.</td>
      <td>## 🌟 论文解读 | 深入理解扩展式游戏中的相关均衡：鞍点公式与基准测试<br><br>## 📌 背景痛点/本文动机<br>扩展式游戏（EFGs）是博弈论中的一种重要类型，它描述了玩家在一系列决策点上的交互过程。在扩展式游戏中，纳什均衡（NE）是一个被广泛研究的解概念，它假设每个玩家都是自私的，并试图最大化自己的收益。然而，纳什均衡可能无法达到社会最优，因为玩家的自私行为可能导致整体收益下降。为了解决这个问题，本文研究了扩展式相关均衡（EFCE）的概念，它允许一个外部设备（中介）向玩家提供推荐动作，但玩家可以自由选择是否遵循这些推荐。<br><br>## 🚀 核心方法<br>💡 创新点1：将EFCE问题转化为鞍点问题<br>本文的主要贡献之一是将EFCE问题转化为一个双线性鞍点问题（BSPP）。这种新的公式化方法为开发新的算法来计算EFCE提供了基础。为了展示这种新公式的潜力，本文提出了一种简单的子梯度下降方法，该方法利用了EFCE的公式和结构特性。与基于线性规划的先前方法相比，该方法具有更好的可扩展性。<br><br>💡 创新点2：提出两个基准游戏<br>为了评估EFCE求解器，本文提出了两个基准游戏：战舰游戏和警长游戏。这些游戏被设计为可参数化的，以便可以扩展到不同的大小。战舰游戏模拟了通过调解员进行冲突解决的场景，而警长游戏模拟了谈判和协商的场景。通过分析这些游戏中的EFCE，本文发现即使调解员无法强制执行行为，它也可以通过以下两种方式显著提高社会福利并阻止玩家偏离推荐：<br>1. 使用特定动作序列作为“密码”来验证玩家是否偏离：偏离会导致不完整或错误的密码，从而表明玩家已经偏离。<br>2. 如果检测到偏离，诱导对手对偏离推荐的玩家采取惩罚性行动。<br><br>## 📈 实验结果<br>本文通过实验验证了基于BSPP公式的子梯度下降方法在大型游戏实例中优于基于线性规划的先前方法。这表明BSPP公式在计算EFCE方面具有更好的可扩展性。<br><br>## 💬 可借鉴之处<br>本文提出的BSPP公式和子梯度下降方法为计算EFCE提供了一种新的思路，并为开发更高效的EFCE求解器奠定了基础。此外，本文提出的基准游戏为评估EFCE求解器提供了有价值的工具。这些发现对于理解和应用EFCE在现实世界场景中具有重要意义，例如交通控制、负载均衡和碳减排等领域。</td>
    </tr>
    <tr>
      <th>34</th>
      <td>High-Level Representation of Benchmark Families for Petri Games</td>
      <td>Petri games have been introduced as a multi-player game model representing<br>causal memory to address the synthesis of distributed systems. For Petri games<br>with one environment player and an arbitrary bounded number of system players,<br>deciding the existence of a safety strategy is EXPTIME-complete. This result<br>forms the basis of the tool ADAM that implements an algorithm for the synthesis<br>of distributed controllers from Petri games. To evaluate the tool, it has been<br>checked on a series of parameterized benchmarks from manufacturing and workflow<br>scenarios. In this paper, we introduce a new possibility to represent benchmark<br>families for the distributed synthesis problem modeled with Petri games. It<br>enables the user to specify an entire benchmark family as one parameterized<br>high-level net. We describe example benchmark families as a high-level version<br>of a Petri game and exhibit an instantiation yielding a concrete 1-bounded<br>Petri game. We identify improvements either regarding the size or the<br>functionality of the benchmark families by examining the high-level Petri<br>games.</td>
      <td>## 🌟 论文解读 | 高级Petri游戏基准家族表示<br><br>## 📌 背景痛点/本文动机<br>Petri游戏作为一种多玩家游戏模型，被引入用于表示因果记忆，以解决分布式系统的综合问题。对于具有一个环境玩家和任意有界数量的系统玩家的Petri游戏，决定安全策略的存在性是EXPTIME完全的。这一结果构成了实现分布式控制器综合算法的工具ADAM的基础。为了评估该工具，它已在一系列来自制造和工作流程场景的参数化基准上进行测试。在本文中，我们引入了一种新的可能性，即用Petri游戏表示分布式综合问题的基准家族。它使用户能够将整个基准家族指定为一个参数化的高级网络。我们描述了作为Petri游戏高级版本的示例基准家族，并展示了产生具体1有界Petri游戏的实例化。通过检查高级Petri游戏，我们确定了基准家族在大小或功能方面的改进。<br><br>## 🚀 核心方法<br>💡 创新点1：高级Petri游戏表示<br>本文提出了基于高级Petri网的新参数化高级表示，用于简洁和清晰地定义基准家族。高级Petri网允许用户使用具有预定义参数大小的域中的单个令牌、具有条件标签的转换和具有表达式标签的弧来指定整个基准家族。<br><br>💡 创新点2：实例化技术<br>本文提出了一种实例化技术，可以将参数化高级Petri游戏转换为标准1有界Petri游戏。这使得现有的算法和工具可以解决这些Petri游戏。<br><br>## 📈 实验结果<br>本文通过两个示例基准家族（并发机器和自重构机器人）展示了高级Petri游戏表示的应用。与原始实现相比，这些基准家族在大小或功能方面得到了改进。<br><br>## 💬 可借鉴之处<br>本文提出的高级Petri游戏表示方法为分布式系统的综合问题提供了一种新的解决方案。该方法具有简洁、清晰和可扩展的特点，可以帮助研究人员更好地理解和解决分布式系统的综合问题。</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks</td>
      <td>From the beginning if the history of AI, there has been interest in games as<br>a platform of research. As the field developed, human-level competence in<br>complex games became a target researchers worked to reach. Only relatively<br>recently has this target been finally met for traditional tabletop games such<br>as Backgammon, Chess and Go. Current research focus has shifted to electronic<br>games, which provide unique challenges. As is often the case with AI research,<br>these results are liable to be exaggerated or misrepresented by either authors<br>or third parties. The extent to which these games benchmark consist of fair<br>competition between human and AI is also a matter of debate. In this work, we<br>review the statements made by authors and third parties in the general media<br>and academic circle about these game benchmark results and discuss factors that<br>can impact the perception of fairness in the contest between humans and<br>machines</td>
      <td>## 🌟 论文解读 | 平台公平性：人工智能与人类游戏基准的公平性<br><br>## 📌 背景痛点/本文动机<br>自人工智能诞生以来，游戏一直是其研究和测试的平台。随着人工智能领域的发展，人类在复杂游戏中的水平成为了研究人员追求的目标。近年来，这一目标在传统桌面游戏如国际象棋、围棋等中得以实现。然而，随着研究的深入，电子游戏因其独特的挑战性而成为新的研究焦点。本文旨在探讨人工智能与人类在游戏基准测试中的公平性问题，并分析影响公平性的因素。<br><br>## 🚀 核心方法<br>本文首先回顾了人工智能在经典桌面游戏（如国际象棋、围棋）和现代电子游戏（如星际争霸、Dota 2）中的基准测试成果，并分析了这些成果在媒体和学术界的讨论。接着，本文提出了一个公平性维度分类法，用于评估人工智能与人类在游戏竞赛中的公平性。最后，本文论证了在游戏基准测试中，人工智能与人类之间不存在完全公平的比较方式。<br><br>## 📈 实验结果<br>本文通过分析人工智能在游戏基准测试中的表现，发现以下问题：<br>1. 人工智能在游戏中的表现往往被夸大或误解。<br>2. 人工智能与人类在游戏基准测试中的公平性存在争议。<br>3. 人工智能在游戏中的优势（如反应速度、信息获取等）可能被误解为不公平。<br><br>## 💬 可借鉴之处<br>本文提出的公平性维度分类法为评估人工智能与人类在游戏竞赛中的公平性提供了参考。此外，本文还强调了人工智能在游戏中的优势可能被误解为不公平，并呼吁研究人员关注这一问题。</td>
    </tr>
    <tr>
      <th>36</th>
      <td>Marathon Environments: Multi-Agent Continuous Control Benchmarks in a Modern Video Game Engine</td>
      <td>Recent advances in deep reinforcement learning in the paradigm of locomotion<br>using continuous control have raised the interest of game makers for the<br>potential of digital actors using active ragdoll. Currently, the available<br>options to develop these ideas are either researchers' limited codebase or<br>proprietary closed systems. We present Marathon Environments, a suite of open<br>source, continuous control benchmarks implemented on the Unity game engine,<br>using the Unity ML- Agents Toolkit. We demonstrate through these benchmarks<br>that continuous control research is transferable to a commercial game engine.<br>Furthermore, we exhibit the robustness of these environments by reproducing<br>advanced continuous control research, such as learning to walk, run and<br>backflip from motion capture data; learning to navigate complex terrains; and<br>by implementing a video game input control system. We show further robustness<br>by training with alternative algorithms found in OpenAI.Baselines. Finally, we<br>share strategies for significantly reducing the training time.</td>
      <td>## 🌟 论文解读 | Marathon Environments：将深度强化学习应用于现代游戏引擎的持续控制基准<br><br>## 📌 背景痛点/本文动机<br>随着深度强化学习在持续控制领域的发展，游戏开发者对使用主动ragdoll的数字角色产生了兴趣。然而，目前开发这些想法的选项要么是研究人员有限的代码库，要么是专有的封闭系统。本文提出了Marathon Environments，这是一个开源的持续控制基准套件，在Unity游戏引擎上实现，使用Unity ML-Agents Toolkit。通过这些基准，本文展示了持续控制研究可以转移到商业游戏引擎，并展示了这些环境的鲁棒性。<br><br>## 🚀 核心方法<br>💡 创新点1：Marathon Environments是一个开源的持续控制基准套件，在Unity游戏引擎上实现，使用Unity ML-Agents Toolkit。这使得研究人员和开发者可以轻松地将持续控制研究应用于商业游戏引擎。<br><br>💡 创新点2：本文展示了Marathon Environments的鲁棒性，通过复制高级持续控制研究，例如从运动捕捉数据中学习行走、跑步和后空翻；学习导航复杂地形；以及实现视频游戏输入控制系统。此外，本文还展示了使用OpenAI.Baselines中的替代算法进行训练的鲁棒性。<br><br>## 📈 实验结果<br>本文通过实验验证了Marathon Environments的鲁棒性和有效性。实验结果表明，Marathon Environments可以成功复制高级持续控制研究，并展示了其在不同环境下的适应能力。此外，本文还分享了优化训练时间的策略，例如增加并发代理数量、使用无头模式、修改Academy脚本以支持物理步骤等。<br><br>## 💬 可借鉴之处<br>本文提出的Marathon Environments为研究人员和开发者提供了一个开源的持续控制基准套件，可以轻松地将深度强化学习应用于商业游戏引擎。此外，本文还分享了优化训练时间的策略，可以帮助研究人员和开发者提高训练效率。</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Benchmarking Cognitive Abilities of the Brain with Computer Games</td>
      <td>Most of the players have experienced the feeling of temporarily losing their<br>character in a given gameplay situation when they cannot control the character,<br>simply because they temporarily cannot see it. The main reasons for this<br>feeling may be due to the interplay of the following factors: (1) the visual<br>complexity of the game is unexpectedly increased compared with the previous<br>time period as more and more game objects and effects are rendered on the<br>display; (2) and/or the game is lagging; (3) and finally, it is also possible<br>that the players have no sufficient experience with controlling the character.<br>This paper focuses on the first reason. We have developed a benchmark program<br>which allows its user to experience the feeling of losing character. While the<br>user can control the character well the benchmark program will increase the<br>visual complexity of the display. Otherwise, if the user lost the character<br>then the program will decrease the complexity until the user will find the<br>character again, and so on. The complexity is measured based on the number of<br>changed pixels between two consecutive display images. Our measurements show<br>that the average of bit per second values of losing and finding pairs describes<br>the user well. The final goal of this research is to further develop our<br>benchmark to a standard psychological test.</td>
      <td>## 🌟 论文解读 | 游戏中的认知能力评估：BrainB Test Series 6<br><br>## 📌 背景痛点/本文动机<br>在游戏过程中，玩家有时会因视觉复杂性的增加而暂时失去对角色的控制。这种现象可能由多种因素引起，包括游戏视觉复杂性的突然增加、游戏延迟以及玩家对角色控制经验的不足。本文重点关注视觉复杂性增加这一因素，并开发了一个名为 BrainB Test Series 6 的基准程序，用于模拟玩家失去角色的感觉，并评估其认知能力。<br><br>## 🚀 核心方法<br>💡 创新点1：BrainB Test Series 6 基准程序<br>该程序通过控制屏幕上移动的方块的视觉复杂性来模拟玩家失去角色的感觉。当玩家能够很好地控制角色时，程序会增加屏幕的视觉复杂性；如果玩家失去了角色，程序会降低复杂性，直到玩家再次找到角色。程序的复杂性基于连续显示图像之间变化的像素数量来衡量。<br><br>💡 创新点2：认知能力评估<br>通过测量玩家在失去和找到角色时屏幕复杂性的平均值，可以评估玩家的认知能力。研究表明，失去和找到角色时屏幕复杂性的平均值可以很好地描述玩家。<br><br>## 📈 实验结果<br>研究人员对 BrainB Test Series 6 进行了初步测试，并发现玩家在屏幕复杂性较高时更容易失去角色，而在屏幕复杂性较低时更容易找到角色。此外，研究人员还发现，通过调整鼠标设置和颜色方案，可以提高测试的准确性和可用性。<br><br>## 💬 可借鉴之处<br>本文提出的 BrainB Test Series 6 基准程序为评估玩家的认知能力提供了一种新的方法。该方法可以用于电子竞技人才选拔、认知能力研究等领域。此外，本文还强调了测试环境标准化和测试结果分析的重要性，为相关研究提供了参考。</td>
    </tr>
    <tr>
      <th>38</th>
      <td>Game-theoretic approach to risk-sensitive benchmarked asset management</td>
      <td>In this article we consider a game theoretic approach to the Risk-Sensitive<br>Benchmarked Asset Management problem (RSBAM) of Davis and Lleo \cite{DL}. In<br>particular, we consider a stochastic differential game between two players,<br>namely, the investor who has a power utility while the second player represents<br>the market which tries to minimize the expected payoff of the investor. The<br>market does this by modulating a stochastic benchmark that the investor needs<br>to outperform. We obtain an explicit expression for the optimal pair of<br>strategies as for both the players.</td>
      <td>## 🌟 论文解读 | 游戏理论视角下的风险敏感基准资产管理<br><br>## 📌 背景痛点/本文动机<br>在资产管理领域，投资者往往需要在追求投资回报和规避风险之间做出权衡。传统的风险敏感控制方法虽然能够在一定程度上平衡这两者，但在面对随机基准时，投资者的最优策略可能会受到影响。本文旨在通过引入游戏理论，构建一个风险敏感基准资产管理问题的游戏模型，以揭示投资者在面对市场对抗时的最优策略。<br><br>## 🚀 核心方法<br>💡 创新点1：构建风险敏感零和随机微分博弈模型<br>本文将风险敏感基准资产管理问题转化为一个零和随机微分博弈模型，其中投资者和市场分别作为两个玩家。投资者追求最大化其投资组合的预期增长率，而市场则试图通过调整随机基准来最小化投资者的预期回报。<br><br>💡 创新点2：求解最优策略<br>本文通过求解哈密顿-雅可比-贝尔曼偏微分方程，得到了投资者和市场在博弈中的最优策略。这些策略不仅考虑了投资者的风险厌恶程度，还考虑了市场对抗行为的影响。<br><br>## 📈 实验结果<br>本文通过数值模拟验证了所提出模型的有效性。结果表明，在风险敏感基准资产管理问题中，投资者和市场之间的博弈确实会影响投资者的最优策略。此外，本文还发现，投资者的风险厌恶程度和市场对抗行为的强度都会对最优策略产生影响。<br><br>## 💬 可借鉴之处<br>本文提出的游戏理论视角为风险敏感基准资产管理问题提供了一种新的研究思路。该方法不仅可以用于分析投资者和市场之间的博弈，还可以用于设计更有效的资产管理策略。此外，本文提出的模型和求解方法也可以为其他金融领域的博弈问题提供参考。</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Benchmarks for Parity Games (extended version)</td>
      <td>We propose a benchmark suite for parity games that includes all benchmarks<br>that have been used in the literature, and make it available online. We give an<br>overview of the parity games, including a description of how they have been<br>generated. We also describe structural properties of parity games, and using<br>these properties we show that our benchmarks are representative. With this work<br>we provide a starting point for further experimentation with parity games.</td>
      <td>## 🌟 论文解读 | 奠定公平博弈算法评估基石：全面解析公平博弈基准集<br><br>## 📌 背景痛点/本文动机<br>公平博弈在模型检验研究中扮演着重要角色，其求解问题被证明属于 NP ∩ co-NP 复杂度类，并且目前尚无已知的多项式时间算法。近年来，研究人员开发了大量的公平博弈求解算法，并研究了多项式时间化简技术。然而，由于缺乏标准基准集，难以对不同工具和算法进行比较评估。<br><br>## 🚀 核心方法<br>本文提出了一个全面的公平博弈基准集，旨在解决上述问题。该基准集包含以下特点：<br><br>* **多样性**：涵盖来自不同验证问题的游戏，包括模型检验、等价性检查、决策过程和合成问题。<br>* **代表性**：包含文献中已使用的所有基准游戏，并使用结构属性分析其代表性。<br>* **易用性**：所有游戏均以 PGSolver 格式在线提供，方便研究人员使用。<br><br>## 📈 实验结果<br>本文分析了基准集中游戏的多种结构属性，包括顶点数、边数、优先级数、度数、强连通分量、搜索策略特性、距离、局部结构和宽度度量等。结果表明，基准集涵盖了广泛的属性值，能够有效评估公平博弈算法的性能。<br><br>## 💬 可借鉴之处<br>* **构建标准基准集**：本文提出的基准集为公平博弈算法的评估提供了重要的参考，有助于推动该领域的研究进展。<br>* **分析结构属性**：本文对公平博弈的结构属性进行了深入分析，为算法设计和优化提供了新的思路。<br>* **探索新属性**：未来可以进一步探索公平博弈的其他结构属性，并研究其对算法性能的影响。<br><br>## 🌟 总结<br>本文提出的公平博弈基准集为该领域的研究提供了重要的工具，有助于推动公平博弈算法的评估和改进。未来，随着更多游戏和属性的加入，该基准集将进一步完善，为公平博弈研究提供更强大的支持。</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Fate stochastic management and policy benchmark in 421, a popular game</td>
      <td>Using game and probability theories, I study the French popular game 421, a<br>perfect information stochastic stage game. The problem is to find strategies<br>maximizing the probability of some expected utility. I only solve a player's<br>round against providence, a problem of fate stochastic management: beyond the<br>backward induction solution, bounded complexity motivates heuristic policies.<br>For a unique goal utility, a simple optimal policy, ratchet, is obtained. Its<br>result probabilities are compiled and used, for arbitrary utilities, as the<br>logic of goal identification policies. Various policies appear, close to human<br>behavior, and are exactly evaluated by solving the Kolmogorov equation.</td>
      <td>## 🌟 论文解读 | 421游戏中命运随机管理与策略基准<br><br>## 📌 背景痛点/本文动机<br>本文研究了法国流行的游戏421，这是一个具有完美信息的随机阶段游戏。游戏的目标是找到最大化某些预期效用概率的策略。作者将421游戏简化为玩家回合对抗命运的问题，这是一个命运随机管理问题。作者通过分析游戏规则和概率理论，提出了几种策略，并使用Kolmogorov方程精确评估了这些策略。<br><br>## 🚀 核心方法<br>💡 创新点1：将421游戏简化为玩家回合对抗命运的问题，这是一个命运随机管理问题。<br>💡 创新点2：提出了几种策略，包括伯努利策略和棘轮策略，并使用Kolmogorov方程精确评估了这些策略。<br>💡 创新点3：提出了目标识别编程方法，通过考虑结果概率来识别目标，从而简化策略的复杂性。<br><br>## 📈 实验结果<br>作者使用Mathematica软件实现了421游戏模型，并计算了各种策略的结果概率。实验结果表明，棘轮策略在D < F的情况下是最优的，而伯努利策略在D > F的情况下是最优的。此外，作者还发现，引入随机性和目标识别编程可以显著提高策略的效率。<br><br>## 💬 可借鉴之处<br>本文的研究结果对于理解随机游戏中的策略选择和命运管理具有重要意义。此外，本文提出的策略评估方法和目标识别编程方法也可以应用于其他随机决策问题。</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        