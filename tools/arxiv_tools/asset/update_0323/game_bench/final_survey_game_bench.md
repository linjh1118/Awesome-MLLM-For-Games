# Paper List of Terms(games+benchmark)
- [25/03] **DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments**  
[[Paper](http://arxiv.org/pdf/2503.06047v1)] [[Code/Page](https://github.com/DeciBrain-Group/DSGBench.)] [[TLDR/Notes](#dsgbench-a-diverse-strategic-game-benchmark-for-evaluating-llm-based-agents-in-complex-decision-making-environments)]

- [25/02] **Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests**  
[[Paper](http://arxiv.org/pdf/2502.14359v1)] [[Code/Page]()] [[TLDR/Notes](#triangulating-llm-progress-through-benchmarks,-games,-and-cognitive-tests)]

- [25/02] **Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents**  
[[Paper](http://arxiv.org/pdf/2502.00510v2)] [[Code/Page]()] [[TLDR/Notes](#who's-the-mvp?-a-game-theoretic-evaluation-benchmark-for-modular-attribution-in-llm-agents)]

- [25/01] **Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research**  
[[Paper](http://arxiv.org/pdf/2501.17559v1)] [[Code/Page]()] [[TLDR/Notes](#solving-urban-network-security-games-learning-platform,-benchmark,-and-challenge-for-ai-research)]

- [24/12] **How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games**  
[[Paper](http://arxiv.org/pdf/2412.12362v1)] [[Code/Page]()] [[TLDR/Notes](#how-different-ai-chatbots-behave?-benchmarking-large-language-models-in-behavioral-economics-games)]

- [24/12] **AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games**  
[[Paper](http://arxiv.org/pdf/2412.10798v2)] [[Code/Page]()] [[TLDR/Notes](#auctionnet-a-novel-benchmark-for-decision-making-in-large-scale-games)]

- [24/12] **From Code to Play: Benchmarking Program Search for Games Using Large Language Models**  
[[Paper](http://arxiv.org/pdf/2412.04057v1)] [[Code/Page]()] [[TLDR/Notes](#from-code-to-play-benchmarking-program-search-for-games-using-large-language-models)]

- [24/11] **BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games**  
[[Paper](http://arxiv.org/pdf/2411.13543v1)] [[Code/Page]()] [[TLDR/Notes](#balrog-benchmarking-agentic-llm-and-vlm-reasoning-on-games)]

- [24/10] **TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs**  
[[Paper](http://arxiv.org/pdf/2410.10479v1)] [[Code/Page]()] [[TLDR/Notes](#tmgbench-a-systematic-game-benchmark-for-evaluating-strategic-reasoning-abilities-of-llms)]

- [24/09] **Game4Loc: A UAV Geo-Localization Benchmark from Game Data**  
[[Paper](http://arxiv.org/pdf/2409.16925v2)] [[Code/Page]()] [[TLDR/Notes](#game4loc-a-uav-geo-localization-benchmark-from-game-data)]

- [24/09] **ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation**  
[[Paper](http://arxiv.org/pdf/2409.10244v1)] [[Code/Page]()] [[TLDR/Notes](#es-kt-24-a-multimodal-knowledge-tracing-benchmark-dataset-with-educational-game-playing-video-and-synthetic-text-generation)]

- [24/08] **Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games**  
[[Paper](http://arxiv.org/pdf/2408.15950v2)] [[Code/Page](https://dev1nw.github.io/atari-gpt/.)] [[TLDR/Notes](#atari-gpt-benchmarking-multimodal-large-language-models-as-low-level-policies-in-atari-games)]

- [24/08] **Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks**  
[[Paper](http://arxiv.org/pdf/2408.10556v2)] [[Code/Page]()] [[TLDR/Notes](#hokoff-real-game-dataset-from-honor-of-kings-and-its-offline-reinforcement-learning-benchmarks)]

- [24/07] **A Benchmark Environment for Offline Reinforcement Learning in Racing Games**  
[[Paper](http://arxiv.org/pdf/2407.09415v1)] [[Code/Page]()] [[TLDR/Notes](#a-benchmark-environment-for-offline-reinforcement-learning-in-racing-games)]

- [24/07] **Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard**  
[[Paper](http://arxiv.org/pdf/2407.07796v2)] [[Code/Page]()] [[TLDR/Notes](#evaluating-large-language-models-with-grid-based-game-competitions-an-extensible-llm-benchmark-and-leaderboard)]

- [24/02] **Simple Stochastic Stopping Games: A Generator and Benchmark Library**  
[[Paper](http://arxiv.org/pdf/2402.02571v1)] [[Code/Page]()] [[TLDR/Notes](#simple-stochastic-stopping-games-a-generator-and-benchmark-library)]

- [23/11] **Application-level Benchmarking of Quantum Computers using Nonlocal Game Strategies**  
[[Paper](http://arxiv.org/pdf/2311.01363v4)] [[Code/Page]()] [[TLDR/Notes](#application-level-benchmarking-of-quantum-computers-using-nonlocal-game-strategies)]

- [23/10] **Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques**  
[[Paper](http://arxiv.org/pdf/2310.14360v4)] [[Code/Page]()] [[TLDR/Notes](#is-chatgpt-a-game-changer-for-geocoding----a-benchmark-for-geocoding-address-parsing-techniques)]

- [23/04] **Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy**  
[[Paper](http://arxiv.org/pdf/2304.07007v1)] [[Code/Page]()] [[TLDR/Notes](#dialogue-games-for-benchmarking-language-understanding-motivation,-taxonomy,-strategy)]

- [23/02] **MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields**  
[[Paper](http://arxiv.org/pdf/2302.02978v2)] [[Code/Page](https://github.com/lujiaying/MUG-Bench)] [[TLDR/Notes](#mug-a-multimodal-classification-benchmark-on-game-data-with-tabular,-textual,-and-visual-fields)]

- [22/12] **Equilibria in Repeated Games under No-Regret with Dynamic Benchmarks**  
[[Paper](http://arxiv.org/pdf/2212.03152v3)] [[Code/Page]()] [[TLDR/Notes](#equilibria-in-repeated-games-under-no-regret-with-dynamic-benchmarks)]

- [22/10] **A Game Benchmark for Real-Time Human-Swarm Control**  
[[Paper](http://arxiv.org/pdf/2210.15852v1)] [[Code/Page](https://sites.google.com/view/swarm-game-benchmark.)] [[TLDR/Notes](#a-game-benchmark-for-real-time-human-swarm-control)]

- [22/10] **WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments**  
[[Paper](http://arxiv.org/pdf/2210.09026v1)] [[Code/Page](https://github.com/inspirai/wilderness-scavenger.)] [[TLDR/Notes](#wild-scav-benchmarking-fps-gaming-ai-on-unity3d-based-environments)]

- [22/07] **The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning**  
[[Paper](http://arxiv.org/pdf/2207.10218v1)] [[Code/Page]()] [[TLDR/Notes](#the-game-of-hidden-rules-a-new-kind-of-benchmark-challenge-for-machine-learning)]

- [22/07] **GOAL: Towards Benchmarking Few-Shot Sports Game Summarization**  
[[Paper](http://arxiv.org/pdf/2207.08635v1)] [[Code/Page](https://github.com/krystalan/goal.)] [[TLDR/Notes](#goal-towards-benchmarking-few-shot-sports-game-summarization)]

- [21/12] **Meterstick: Benchmarking Performance Variability in Cloud and Self-hosted Minecraft-like Games Extended Technical Report**  
[[Paper](http://arxiv.org/pdf/2112.06963v2)] [[Code/Page]()] [[TLDR/Notes](#meterstick-benchmarking-performance-variability-in-cloud-and-self-hosted-minecraft-like-games-extended-technical-report)]

- [21/02] **Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game**  
[[Paper](http://arxiv.org/pdf/2102.13647v3)] [[Code/Page](https://github.com/Scriddie/Varsortability.)] [[TLDR/Notes](#beware-of-the-simulated-dag!-causal-discovery-benchmarks-may-be-easy-to-game)]

- [20/12] **OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research**  
[[Paper](http://arxiv.org/pdf/2012.06168v4)] [[Code/Page]()] [[TLDR/Notes](#openholdem-a-benchmark-for-large-scale-imperfect-information-game-research)]

- [20/07] **Towards Game-Playing AI Benchmarks via Performance Reporting Standards**  
[[Paper](http://arxiv.org/pdf/2007.02742v1)] [[Code/Page]()] [[TLDR/Notes](#towards-game-playing-ai-benchmarks-via-performance-reporting-standards)]

- [20/05] **Interbank lending with benchmark rates: Pareto optima for a class of singular control games**  
[[Paper](http://arxiv.org/pdf/2005.05766v3)] [[Code/Page]()] [[TLDR/Notes](#interbank-lending-with-benchmark-rates-pareto-optima-for-a-class-of-singular-control-games)]

- [20/04] **Benchmarking End-to-End Behavioural Cloning on Video Games**  
[[Paper](http://arxiv.org/pdf/2004.00981v2)] [[Code/Page]()] [[TLDR/Notes](#benchmarking-end-to-end-behavioural-cloning-on-video-games)]

- [19/10] **Design, Benchmarking and Explainability Analysis of a Game-Theoretic Framework towards Energy Efficiency in Smart Infrastructure**  
[[Paper](http://arxiv.org/pdf/1910.07899v1)] [[Code/Page]()] [[TLDR/Notes](#design,-benchmarking-and-explainability-analysis-of-a-game-theoretic-framework-towards-energy-efficiency-in-smart-infrastructure)]

- [19/08] **Quantum Poker A game for quantum computers suitable for benchmarking error mitigation techniques on NISQ devices**  
[[Paper](http://arxiv.org/pdf/1908.00044v4)] [[Code/Page]()] [[TLDR/Notes](#quantum-poker-a-game-for-quantum-computers-suitable-for-benchmarking-error-mitigation-techniques-on-nisq-devices)]

- [19/05] **Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks**  
[[Paper](http://arxiv.org/pdf/1905.12564v2)] [[Code/Page]()] [[TLDR/Notes](#correlation-in-extensive-form-games-saddle-point-formulation-and-benchmarks)]

- [19/04] **High-Level Representation of Benchmark Families for Petri Games**  
[[Paper](http://arxiv.org/pdf/1904.05621v1)] [[Code/Page]()] [[TLDR/Notes](#high-level-representation-of-benchmark-families-for-petri-games)]

- [19/03] **Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks**  
[[Paper](http://arxiv.org/pdf/1903.07008v4)] [[Code/Page]()] [[TLDR/Notes](#leveling-the-playing-field----fairness-in-ai-versus-human-game-benchmarks)]

- [19/02] **Marathon Environments: Multi-Agent Continuous Control Benchmarks in a Modern Video Game Engine**  
[[Paper](http://arxiv.org/pdf/1902.09097v1)] [[Code/Page]()] [[TLDR/Notes](#marathon-environments-multi-agent-continuous-control-benchmarks-in-a-modern-video-game-engine)]

- [18/09] **Benchmarking Cognitive Abilities of the Brain with Computer Games**  
[[Paper](http://arxiv.org/pdf/1809.00172v1)] [[Code/Page]()] [[TLDR/Notes](#benchmarking-cognitive-abilities-of-the-brain-with-computer-games)]

- [15/03] **Game-theoretic approach to risk-sensitive benchmarked asset management**  
[[Paper](http://arxiv.org/pdf/1503.01802v1)] [[Code/Page]()] [[TLDR/Notes](#game-theoretic-approach-to-risk-sensitive-benchmarked-asset-management)]

- [14/07] **Benchmarks for Parity Games (extended version)**  
[[Paper](http://arxiv.org/pdf/1407.3121v2)] [[Code/Page]()] [[TLDR/Notes](#benchmarks-for-parity-games-(extended-version))]

- [25/00] **Fate stochastic management and policy benchmark in 421, a popular game**  
[[Paper](http://arxiv.org/pdf/math/0007129v1)] [[Code/Page]()] [[TLDR/Notes](#fate-stochastic-management-and-policy-benchmark-in-421,-a-popular-game)]



# TLDR/Notes
## DSGBench A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments
### Abstract
Large Language Model~(LLM) based agents have been increasingly popular in
solving complex and dynamic tasks, which requires proper evaluation systems to
assess their capabilities. Nevertheless, existing benchmarks usually either
focus on single-objective tasks or use overly broad assessing metrics, failing
to provide a comprehensive inspection of the actual capabilities of LLM-based
agents in complicated decision-making tasks. To address these issues, we
introduce DSGBench, a more rigorous evaluation platform for strategic
decision-making. Firstly, it incorporates six complex strategic games which
serve as ideal testbeds due to their long-term and multi-dimensional
decision-making demands and flexibility in customizing tasks of various
difficulty levels or multiple targets. Secondly, DSGBench employs a
fine-grained evaluation scoring system which examines the decision-making
capabilities by looking into the performance in five specific dimensions and
offering a comprehensive assessment in a well-designed way. Furthermore,
DSGBench also incorporates an automated decision-tracking mechanism which
enables in-depth analysis of agent behaviour patterns and the changes in their
strategies. We demonstrate the advances of DSGBench by applying it to multiple
popular LLM-based agents and our results suggest that DSGBench provides
valuable insights in choosing LLM-based agents as well as improving their
future development. DSGBench is available at
https://github.com/DeciBrain-Group/DSGBench.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DSGBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å†³ç­–ç¯å¢ƒä¸­çš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚å’ŒåŠ¨æ€ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„å®é™…èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ç³»ç»Ÿé€šå¸¸åªå…³æ³¨å•ä¸€ç›®æ ‡ä»»åŠ¡æˆ–ä½¿ç”¨è¿‡äºå®½æ³›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMæ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„å®é™…èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDSGBenchæ˜¯ä¸€ä¸ªæ›´ä¸¥æ ¼çš„è¯„ä¼°å¹³å°ï¼Œç”¨äºè¯„ä¼°æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚å®ƒåŒ…å«äº†å…­ä¸ªå¤æ‚çš„æˆ˜ç•¥æ¸¸æˆï¼Œè¿™äº›æ¸¸æˆå› å…¶é•¿æœŸå’Œå¤šç»´åº¦çš„å†³ç­–éœ€æ±‚ä»¥åŠå®šåˆ¶å„ç§éš¾åº¦çº§åˆ«æˆ–å¤šä¸ªç›®æ ‡çš„ä»»åŠ¡çš„çµæ´»æ€§è€Œæˆä¸ºç†æƒ³çš„æµ‹è¯•å¹³å°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDSGBenché‡‡ç”¨äº†ä¸€ç§ç»†ç²’åº¦çš„è¯„ä¼°è¯„åˆ†ç³»ç»Ÿï¼Œé€šè¿‡è€ƒå¯Ÿåœ¨äº”ä¸ªç‰¹å®šç»´åº¦ä¸­çš„è¡¨ç°æ¥æ£€æŸ¥å†³ç­–èƒ½åŠ›ï¼Œå¹¶ä»¥ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„æ–¹å¼æä¾›å…¨é¢çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼ŒDSGBenchè¿˜åŒ…å«ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å†³ç­–è·Ÿè¸ªæœºåˆ¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æä»£ç†çš„è¡Œä¸ºæ¨¡å¼å’Œç­–ç•¥çš„å˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
DSGBenché€šè¿‡åº”ç”¨äºå¤šä¸ªæµè¡Œçš„LLMæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨é€‰æ‹©LLMæ¨¡å‹ä»¥åŠæ”¹è¿›å…¶æœªæ¥å‘å±•æ–¹é¢çš„ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSGBenchèƒ½å¤Ÿæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMæ¨¡å‹åœ¨ä¸åŒå†³ç­–ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DSGBenchä¸ºè¯„ä¼°LLMæ¨¡å‹åœ¨å¤æ‚å†³ç­–ç¯å¢ƒä¸­çš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ã€‚å…¶ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡å’Œå†³ç­–è·Ÿè¸ªæœºåˆ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ·±å…¥äº†è§£LLMæ¨¡å‹çš„è¡Œä¸ºæ¨¡å¼å’Œç­–ç•¥å˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°æ”¹è¿›æ¨¡å‹çš„è®¾è®¡å’Œå¼€å‘ã€‚æ­¤å¤–ï¼ŒDSGBenchçš„çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¯„ä¼°éœ€æ±‚ï¼Œä¸ºLLMæ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚

## Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests
### Abstract
We examine three evaluation paradigms: large question-answering benchmarks
(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and
cognitive tests (e.g., for working memory or theory of mind). First, we
investigate which of the former two-benchmarks or games-is most effective at
discriminating LLMs of varying quality. Then, inspired by human cognitive
assessments, we compile a suite of targeted tests that measure cognitive
abilities deemed essential for effective language use, and we investigate their
correlation with model performance in benchmarks and games. Our analyses reveal
that interactive games are superior to standard benchmarks in discriminating
models. Causal and logical reasoning correlate with both static and interactive
tests, while differences emerge regarding core executive functions and
social/emotional skills, which correlate more with games. We advocate the
development of new interactive benchmarks and targeted cognitive tasks inspired
by assessing human abilities but designed specifically for LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡åŸºå‡†æµ‹è¯•ã€æ¸¸æˆå’Œè®¤çŸ¥æµ‹è¯•æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡çš„é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œä½†è¿™äº›æµ‹è¯•å¾€å¾€å­˜åœ¨æ•°æ®æ±¡æŸ“å’Œæ¨¡å‹å¯¹æç¤ºæ ¼å¼æ•æ„Ÿçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æµ‹è¯•å¿½ç•¥äº†å®é™…çš„è¯­è¨€ä½¿ç”¨åœºæ™¯ï¼Œå³åœ¨ç¤¾ä¼šå’Œä»»åŠ¡å¯¼å‘çš„å¤šè½®äº¤äº’ä¸­ä½¿ç”¨è¯­è¨€ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢æ›´æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°ç†è§£LLMsçš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸‰ç§è¯„ä¼°èŒƒå¼ï¼šå¤§è§„æ¨¡é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMLUå’ŒBBHï¼‰ã€äº¤äº’å¼æ¸¸æˆï¼ˆå¦‚ä¿¡å·æ¸¸æˆæˆ–ç¦å¿Œæ¸¸æˆï¼‰å’Œè®¤çŸ¥æµ‹è¯•ï¼ˆå¦‚å·¥ä½œè®°å¿†æˆ–å¿ƒæ™ºç†è®ºæµ‹è¯•ï¼‰ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›èŒƒå¼ï¼Œç ”ç©¶å›¢é˜Ÿæ—¨åœ¨æ‰¾å‡ºæœ€æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå—äººç±»è®¤çŸ¥è¯„ä¼°çš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹æ€§çš„æµ‹è¯•ï¼Œä»¥è¡¡é‡å¯¹æœ‰æ•ˆè¯­è¨€ä½¿ç”¨è‡³å…³é‡è¦çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMsåœ¨å› æœæ¨ç†ã€é€»è¾‘æ¨ç†ã€å·¥ä½œè®°å¿†ã€æƒ…ç»ªæ™ºåŠ›ç­‰æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶åˆ†æè¿™äº›èƒ½åŠ›ä¸æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•å’Œæ¸¸æˆä¸­çš„è¡¨ç°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼æ¸¸æˆåœ¨åŒºåˆ†ä¸åŒè´¨é‡çš„LLMsæ–¹é¢ä¼˜äºä¼ ç»Ÿçš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚å› æœæ¨ç†å’Œé€»è¾‘æ¨ç†ä¸é™æ€å’Œäº¤äº’å¼æµ‹è¯•éƒ½ç›¸å…³ï¼Œè€Œæ ¸å¿ƒæ‰§è¡ŒåŠŸèƒ½å’Œç¤¾äº¤/æƒ…æ„ŸæŠ€èƒ½åˆ™æ›´å¤šåœ°ä¸æ¸¸æˆç›¸å…³ã€‚ç‰¹åˆ«æ˜¯ï¼Œå·¥ä½œè®°å¿†å’Œæƒ…ç»ªæ™ºåŠ›ä»…ä¸æ¸¸æˆä¸­çš„è¡¨ç°æ˜¾è‘—ç›¸å…³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼æ¸¸æˆæ˜¯ä¸€ç§æ›´æœ‰æ•ˆçš„è¯„ä¼°LLMsçš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¨¡å‹åœ¨å®é™…è¯­è¨€ä½¿ç”¨åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ€§çš„è®¤çŸ¥æµ‹è¯•å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMsçš„å†…åœ¨èƒ½åŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æä¾›æŒ‡å¯¼ã€‚å› æ­¤ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºLLMsçš„è¯„ä¼°å’Œå¼€å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents
### Abstract
Large Language Model (LLM) agents frameworks often employ modular
architectures, incorporating components such as planning, reasoning, action
execution, and reflection to tackle complex tasks. However, quantifying the
contribution of each module to overall system performance remains a significant
challenge, impeding optimization and interpretability. To address this, we
introduce CapaBench (Capability-level Assessment Benchmark), an evaluation
framework grounded in cooperative game theory's Shapley Value, which
systematically measures the marginal impact of individual modules and their
interactions within an agent's architecture. By replacing default modules with
test variants across all possible combinations, CapaBench provides a principle
method for attributing performance contributions. Key contributions include:
(1) We are the first to propose a Shapley Value-based methodology for
quantifying the contributions of capabilities in LLM agents; (2) Modules with
high Shapley Values consistently lead to predictable performance gains when
combined, enabling targeted optimization; and (3) We build a multi-round
dataset of over 1,500 entries spanning diverse domains and practical task
scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench
bridges the gap between component-level evaluation and holistic system
assessment, providing actionable insights for optimizing modular LLM agents and
advancing their deployment in complex, real-world scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è°æ˜¯MVPï¼ŸåŸºäºåšå¼ˆè®ºçš„LLM Agentæ¨¡å—åŒ–è¯„ä¼°åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼ŒLLM Agentæ¡†æ¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é€šå¸¸é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼ŒåŒ…æ‹¬è§„åˆ’ã€æ¨ç†ã€è¡ŒåŠ¨æ‰§è¡Œå’Œåæ€ç­‰ç»„ä»¶ã€‚ç„¶è€Œï¼Œå¦‚ä½•é‡åŒ–æ¯ä¸ªæ¨¡å—å¯¹æ•´ä½“ç³»ç»Ÿæ€§èƒ½çš„è´¡çŒ®ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™é˜»ç¢äº†ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CapaBenchï¼ˆèƒ½åŠ›çº§è¯„ä¼°åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåˆä½œåšå¼ˆè®ºçš„Shapley Valueçš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¡¡é‡äº†å•ä¸ªæ¨¡å—åŠå…¶åœ¨Agentæ¶æ„ä¸­çš„äº¤äº’çš„è¾¹é™…å½±å“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¦–æ¬¡æå‡ºåŸºäºShapley Valueçš„æ–¹æ³•æ¥é‡åŒ–LLM Agentä¸­èƒ½åŠ›çš„è´¡çŒ®ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…·æœ‰é«˜Shapley Valueçš„æ¨¡å—åœ¨ç»„åˆæ—¶å§‹ç»ˆå¯¼è‡´å¯é¢„æµ‹çš„æ€§èƒ½æå‡ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡1,500ä¸ªæ¡ç›®çš„å¤šè½®æ•°æ®é›†ï¼Œæ¶µç›–å„ç§é¢†åŸŸå’Œå®é™…ä»»åŠ¡åœºæ™¯ï¼Œä½¿Agentèƒ½åŠ›çš„å…¨é¢è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹ä¹ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæ¶µç›–äº†äº”ä¸ªä¸»è¦ä»»åŠ¡ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚å’Œç‹¬ç‰¹çš„æ¨¡å—è´¡çŒ®æ¨¡å¼ã€‚ç»“æœè¡¨æ˜ï¼Œå…·æœ‰é«˜Shapley Valueçš„æ¨¡å—åœ¨ç»„åˆæ—¶å§‹ç»ˆå¯¼è‡´å¯é¢„æµ‹çš„æ€§èƒ½æå‡ï¼Œè¿™ä¸ºå¼€å‘äººå‘˜æä¾›äº†æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–æŒ‡å¯¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
CapaBenchæ¡†æ¶ä¸ºä¼˜åŒ–æ¨¡å—åŒ–LLM Agentå’Œæ¨è¿›å…¶åœ¨å¤æ‚ã€ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°å’Œé‡åŒ–æ¯ä¸ªæ¨¡å—çš„è´¡çŒ®ï¼Œå¼€å‘äººå‘˜å¯ä»¥æ›´å¥½åœ°ç†è§£Agentçš„æ€§èƒ½ï¼Œå¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜LLM Agentåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ•ˆæœã€‚

## Solving Urban Network Security Games Learning Platform, Benchmark, and Challenge for AI Research
### Abstract
After the great achievement of solving two-player zero-sum games, more and
more AI researchers focus on solving multiplayer games. To facilitate the
development of designing efficient learning algorithms for solving multiplayer
games, we propose a multiplayer game platform for solving Urban Network
Security Games (\textbf{UNSG}) that model real-world scenarios. That is,
preventing criminal activity is a highly significant responsibility assigned to
police officers in cities, and police officers have to allocate their limited
security resources to interdict the escaping criminal when a crime takes place
in a city. This interaction between multiple police officers and the escaping
criminal can be modeled as a UNSG. The variants of UNSGs can model different
real-world settings, e.g., whether real-time information is available or not,
and whether police officers can communicate or not. The main challenges of
solving this game include the large size of the game and the co-existence of
cooperation and competition. While previous efforts have been made to tackle
UNSGs, they have been hampered by performance and scalability issues.
Therefore, we propose an open-source UNSG platform (\textbf{GraphChase}) for
designing efficient learning algorithms for solving UNSGs. Specifically,
GraphChase offers a unified and flexible game environment for modeling various
variants of UNSGs, supporting the development, testing, and benchmarking of
algorithms. We believe that GraphChase not only facilitates the development of
efficient algorithms for solving real-world problems but also paves the way for
significant advancements in algorithmic development for solving general
multiplayer games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GraphChaseï¼šè§£å†³åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆçš„AIå¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½åœ¨è§£å†³ä¸¤äººé›¶å’Œåšå¼ˆæ–¹é¢å–å¾—å·¨å¤§æˆå°±ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜å¼€å§‹å…³æ³¨è§£å†³å¤šäººæ¸¸æˆã€‚åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆï¼ˆUNSGï¼‰ä½œä¸ºä¸€ç§æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯çš„å¤šç©å®¶æ¸¸æˆï¼Œå¯¹äºç ”ç©¶å¤šäººåšå¼ˆç®—æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼ŒUNSGçš„è§£å†³é¢ä¸´ç€æ¸¸æˆè§„æ¨¡åºå¤§ã€åˆä½œä¸ç«äº‰å…±å­˜ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰çš„ç®—æ³•åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGraphChaseå¹³å°
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGraphChaseçš„å¼€æºUNSGå¹³å°ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ã€çµæ´»çš„æ¸¸æˆç¯å¢ƒï¼Œç”¨äºæ¨¡æ‹Ÿå„ç§UNSGå˜ä½“ï¼Œå¹¶æ”¯æŒç®—æ³•çš„å¼€å‘ã€æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ã€‚GraphChaseå¹³å°å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- **çµæ´»çš„æ¸¸æˆç¯å¢ƒ**ï¼šç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰æ¸¸æˆå‚æ•°ï¼ŒåŒ…æ‹¬å›¾ç»“æ„ã€åˆå§‹ä½ç½®ã€æ—¶é—´èŒƒå›´ç­‰ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒçš„UNSGåœºæ™¯ã€‚
- **å¤šç§ç®—æ³•æ”¯æŒ**ï¼šGraphChaseå¹³å°æ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚CFR-MIXã€NSG-NFSPã€NSGZeroã€Pretrained PSROå’ŒGrasperï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œç®—æ³•æ¯”è¾ƒå’Œè¯„ä¼°ã€‚
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šGraphChaseå¹³å°é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…æ‹¬æ¸¸æˆæ¨¡å—ã€ä»£ç†æ¨¡å—å’Œæ±‚è§£å™¨æ¨¡å—ï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œè‡ªå®šä¹‰å’Œæ‰©å±•ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºå‡†æµ‹è¯•
æœ¬æ–‡åœ¨GraphChaseå¹³å°ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯„ä¼°äº†ç°æœ‰ç®—æ³•çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ç°æœ‰ç®—æ³•åœ¨ç®€å•åœºæ™¯ä¸‹èƒ½å¤Ÿå–å¾—åˆç†æ€§èƒ½ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸‹ä»ç„¶å­˜åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚è¿™è¡¨æ˜ï¼Œå¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„ç®—æ³•æ¥è§£å†³ç°å®ä¸–ç•Œçš„UNSGä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphChaseå¹³å°èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹ŸUNSGåœºæ™¯ï¼Œå¹¶ä¸”èƒ½å¤ŸåŠ é€Ÿç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œç°æœ‰ç®—æ³•åœ¨è§£å†³å¤æ‚åœºæ™¯å’Œå¤§è§„æ¨¡æ¸¸æˆæ—¶å­˜åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GraphChaseå¹³å°ä¸ºç ”ç©¶UNSGå’Œå¤šäººåšå¼ˆç®—æ³•æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾åœ°æ¨¡æ‹Ÿä¸åŒçš„åœºæ™¯å’Œæµ‹è¯•ä¸åŒçš„ç®—æ³•ã€‚æ­¤å¤–ï¼ŒGraphChaseå¹³å°è¿˜å¯ä»¥ä½œä¸ºå…¶ä»–å¤šäººåšå¼ˆé—®é¢˜çš„æµ‹è¯•å¹³å°ï¼Œä¾‹å¦‚å¯¹æŠ—æ€§å›¢é˜Ÿæ¸¸æˆå’Œè¿½é€ƒæ¸¸æˆã€‚

### ğŸ“š æ€»ç»“
GraphChaseå¹³å°ä¸ºè§£å†³åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆæä¾›äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼Œå¹¶ä¸ºç ”ç©¶å¤šäººåšå¼ˆç®—æ³•æä¾›äº†æ–°çš„æ€è·¯ã€‚éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒGraphChaseå¹³å°æœ‰æœ›åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

## How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games
### Abstract
The deployment of large language models (LLMs) in diverse applications
requires a thorough understanding of their decision-making strategies and
behavioral patterns. As a supplement to a recent study on the behavioral Turing
test, this paper presents a comprehensive analysis of five leading LLM-based
chatbot families as they navigate a series of behavioral economics games. By
benchmarking these AI chatbots, we aim to uncover and document both common and
distinct behavioral patterns across a range of scenarios. The findings provide
valuable insights into the strategic preferences of each LLM, highlighting
potential implications for their deployment in critical decision-making roles.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡Œä¸ºæ¨¡å¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç†è§£è¿™äº›æ¨¡å‹çš„å†³ç­–ç­–ç•¥å’Œè¡Œä¸ºæ¨¡å¼å˜å¾—è‡³å…³é‡è¦ã€‚è¿™ä¸ä»…æœ‰åŠ©äºä¼˜åŒ–å®ƒä»¬åœ¨ç‰¹å®šåº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œè¿˜èƒ½æ›´å¥½åœ°è¯„ä¼°å®ƒä»¬çš„å¯é æ€§å’Œå¯é¢„æµ‹æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠé‡å¤§è´£ä»»çš„æƒ…å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºä¸åŒLLMsçš„è¡Œä¸ºæ¨¡å¼çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€šè¿‡è¡Œä¸ºç»æµå­¦æ¸¸æˆè¯„ä¼°LLMs
æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—ç»å…¸çš„è¡Œä¸ºç»æµå­¦æ¸¸æˆï¼Œå¦‚ç‹¬è£è€…æ¸¸æˆã€æœ€åé€šç‰’æ¸¸æˆã€ä¿¡ä»»æ¸¸æˆã€å…¬å…±ç‰©å“æ¸¸æˆã€ç‚¸å¼¹é£é™©æ¸¸æˆå’Œå›šå¾’å›°å¢ƒæ¸¸æˆï¼Œå¯¹äº”ç§é¢†å…ˆçš„LLM-basedèŠå¤©æœºå™¨äººè¿›è¡Œäº†å…¨é¢åˆ†æã€‚è¿™äº›æ¸¸æˆæ—¨åœ¨æµ‹è¯•LLMsåœ¨ä¿¡ä»»ã€å…¬å¹³ã€é£é™©è§„é¿ã€åˆ©ä»–ä¸»ä¹‰å’Œåˆä½œç­‰æ–¹é¢çš„è¡Œä¸ºæ¨¡å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥è¡Œä¸ºå›¾çµæµ‹è¯•å’Œåˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•
ä¸ºäº†è¯„ä¼°LLMsçš„è¡Œä¸ºæ¨¡å¼ï¼Œæœ¬æ–‡å¼•å…¥äº†è¡Œä¸ºå›¾çµæµ‹è¯•å’Œåˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•ã€‚è¡Œä¸ºå›¾çµæµ‹è¯•é€šè¿‡æ¯”è¾ƒLLMså’Œäººç±»çš„è¡Œä¸ºåˆ†å¸ƒï¼Œè¯„ä¼°LLMsæ˜¯å¦èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¡Œä¸ºã€‚åˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•åˆ™ä½¿ç”¨Wassersteinè·ç¦»æ¥è¡¡é‡LLMsçš„è¡Œä¸ºåˆ†å¸ƒä¸äººç±»è¡Œä¸ºåˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æµ‹è¯•çš„èŠå¤©æœºå™¨äººéƒ½èƒ½æˆåŠŸåœ°æ•æ‰åˆ°ç‰¹å®šçš„äººç±»è¡Œä¸ºæ¨¡å¼ï¼Œå¯¼è‡´å†³ç­–åˆ†å¸ƒé«˜åº¦é›†ä¸­ã€‚å°½ç®¡æ——èˆ°èŠå¤©æœºå™¨äººè¡¨ç°å‡ºæ˜¾è‘—çš„é€šè¿‡å›¾çµæµ‹è¯•çš„æ¦‚ç‡ï¼Œä½†å®ƒä»¬åªèƒ½äº§ç”Ÿä¸äººç±»ç›¸ä¼¼çš„è¡Œä¸ºåˆ†å¸ƒã€‚ä¸äººç±»ç›¸æ¯”ï¼ŒèŠå¤©æœºå™¨äººåœ¨æ”¶ç›Šåå¥½ä¸Šæ›´å¼ºè°ƒå…¬å¹³æ€§ã€‚èŠå¤©æœºå™¨äººåœ¨ä¸åŒæ¸¸æˆä¸­çš„æ”¶ç›Šåå¥½å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚ä¸åŒçš„èŠå¤©æœºå™¨äººåœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ä»¥é€šè¿‡å›¾çµæµ‹è¯•ç»“æœã€æ”¶ç›Šåå¥½å’Œè¡Œä¸ºçš„è¿è´¯æ€§è¿›ä¸€æ­¥åŒºåˆ†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºç†è§£LLMsçš„è¡Œä¸ºæ¨¡å¼æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥åœ¨AIè¡Œä¸ºç§‘å­¦é¢†åŸŸçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¨¡ä»¿äººç±»è¡Œä¸ºæ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥æé«˜LLMsåœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨ä¸åŒæ¸¸æˆä¸­çš„è¡Œä¸ºä¸ä¸€è‡´æ€§ä¹Ÿè¡¨æ˜ï¼Œéœ€è¦å¼€å‘æ›´é€šç”¨çš„åå¥½å’Œç›®æ ‡ï¼Œä»¥ä¾¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”å„ç§æƒ…å¢ƒã€‚

## AuctionNet A Novel Benchmark for Decision-Making in Large-Scale Games
### Abstract
Decision-making in large-scale games is an essential research area in
artificial intelligence (AI) with significant real-world impact. However, the
limited access to realistic large-scale game environments has hindered research
progress in this area. In this paper, we present AuctionNet, a benchmark for
bid decision-making in large-scale ad auctions derived from a real-world online
advertising platform. AuctionNet is composed of three parts: an ad auction
environment, a pre-generated dataset based on the environment, and performance
evaluations of several baseline bid decision-making algorithms. More
specifically, the environment effectively replicates the integrity and
complexity of real-world ad auctions through the interaction of several
modules: the ad opportunity generation module employs deep generative networks
to bridge the gap between simulated and real-world data while mitigating the
risk of sensitive data exposure; the bidding module implements diverse
auto-bidding agents trained with different decision-making algorithms; and the
auction module is anchored in the classic Generalized Second Price (GSP)
auction but also allows for customization of auction mechanisms as needed. To
facilitate research and provide insights into the environment, we have also
pre-generated a substantial dataset based on the environment. The dataset
contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over
500 million auction records. Performance evaluations of baseline algorithms
such as linear programming, reinforcement learning, and generative models for
bid decision-making are also presented as a part of AuctionNet. We believe that
AuctionNet is applicable not only to research on bid decision-making in ad
auctions but also to the general area of decision-making in large-scale games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AuctionNetï¼šå¤§å‹æ¸¸æˆä¸­å†³ç­–åˆ¶å®šçš„æ–°åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹æ¸¸æˆä¸­çš„å†³ç­–åˆ¶å®šæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œå®ƒå¯¹ç°å®ä¸–ç•Œæœ‰ç€æ·±è¿œçš„å½±å“ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯¹çœŸå®å¤§å‹æ¸¸æˆç¯å¢ƒçš„è®¿é—®ï¼Œè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿›å±•å—åˆ°äº†é™åˆ¶ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿç¯å¢ƒå¾€å¾€ä¸çœŸå®ç¯å¢ƒå­˜åœ¨è¾ƒå¤§å·®è·ï¼Œæ— æ³•å®Œå…¨åæ˜ ç°å®ä¸–ç•Œä¸­çš„å¤šæ™ºèƒ½ä½“åŠ¨æ€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuctionNetç¯å¢ƒ
AuctionNetæ˜¯ä¸€ä¸ªåŸºäºçœŸå®åœ¨çº¿å¹¿å‘Šå¹³å°çš„å¤§å‹å¹¿å‘Šæ‹å–å†³ç­–åˆ¶å®šåŸºå‡†ã€‚å®ƒç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šå¹¿å‘Šæ‹å–ç¯å¢ƒã€åŸºäºç¯å¢ƒçš„é¢„ç”Ÿæˆæ•°æ®é›†ä»¥åŠå‡ ä¸ªåŸºçº¿æŠ•æ ‡å†³ç­–åˆ¶å®šç®—æ³•çš„æ€§èƒ½è¯„ä¼°ã€‚è¯¥ç¯å¢ƒé€šè¿‡å‡ ä¸ªæ¨¡å—çš„äº¤äº’æœ‰æ•ˆåœ°å¤åˆ¶äº†ç°å®ä¸–ç•Œå¹¿å‘Šæ‹å–çš„å®Œæ•´æ€§å’Œå¤æ‚æ€§ï¼š
- å¹¿å‘Šæœºä¼šç”Ÿæˆæ¨¡å—ï¼šä½¿ç”¨æ·±åº¦ç”Ÿæˆç½‘ç»œæ¥å¼¥åˆæ¨¡æ‹Ÿæ•°æ®å’Œç°å®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„å·®è·ï¼ŒåŒæ—¶é™ä½æ•æ„Ÿæ•°æ®æ³„éœ²çš„é£é™©ã€‚
- æŠ•æ ‡æ¨¡å—ï¼šå®ç°äº†å¤šç§è‡ªåŠ¨æŠ•æ ‡ä»£ç†ï¼Œè¿™äº›ä»£ç†ä½¿ç”¨ä¸åŒçš„å†³ç­–åˆ¶å®šç®—æ³•è¿›è¡Œè®­ç»ƒã€‚
- æ‹å–æ¨¡å—ï¼šä»¥ç»å…¸çš„å¹¿ä¹‰ç¬¬äºŒä»·æ ¼ï¼ˆGSPï¼‰æ‹å–ä¸ºåŸºç¡€ï¼Œä½†ä¹Ÿå…è®¸æ ¹æ®éœ€è¦å®šåˆ¶æ‹å–æœºåˆ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢„ç”Ÿæˆæ•°æ®é›†
ä¸ºäº†ä¿ƒè¿›ç ”ç©¶å’Œæä¾›å¯¹ç¯å¢ƒçš„æ´å¯Ÿï¼ŒAuctionNetè¿˜åŸºäºç¯å¢ƒé¢„ç”Ÿæˆäº†ä¸€ä¸ªåºå¤§çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«1000ä¸‡ä¸ªå¹¿å‘Šæœºä¼šã€48ç§ä¸åŒçš„è‡ªåŠ¨æŠ•æ ‡ä»£ç†å’Œè¶…è¿‡5äº¿æ¡æ‹å–è®°å½•ã€‚è¿™äº›æ•°æ®å¯ä»¥ç”¨äºå»ºæ¨¡ç¯å¢ƒï¼Œå¹¶æœ‰æ•ˆåœ°è®­ç»ƒè‡ªåŠ¨æŠ•æ ‡ä»£ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AuctionNetå·²ç»ä¸ºNeurIPS 2024â€œå¤§å‹æ‹å–ä¸­çš„è‡ªåŠ¨æŠ•æ ‡â€ç«èµ›æä¾›äº†åŠ¨åŠ›ï¼Œä¸ºæ¥è‡ªä¸–ç•Œå„åœ°çš„1500å¤šæ”¯é˜Ÿä¼æä¾›äº†ç«èµ›ç¯å¢ƒã€‚è¯¥ç«èµ›è§£å†³äº†åœ¨ä¸ç¡®å®šå’Œç«äº‰ç¯å¢ƒä¸­è¿›è¡Œé«˜é¢‘æŠ•æ ‡å†³ç­–åˆ¶å®šçš„å…³é”®é—®é¢˜ï¼Œå¹¶æŒç»­äº†4ä¸ªæœˆã€‚AuctionNetæä¾›çš„å¹¿å‘Šæ‹å–ç¯å¢ƒã€æ•°æ®é›†å’ŒåŸºçº¿æŠ•æ ‡å†³ç­–åˆ¶å®šç®—æ³•ä¸ºå‚èµ›è€…æä¾›äº†å‡†ç¡®å’Œå…¬å¹³çš„æ€§èƒ½è¯„ä¼°ï¼Œå¹¶æ¿€å‘äº†ä»–ä»¬çš„åˆ›é€ åŠ›ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯å‘å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AuctionNetä¸ä»…ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç ”ç©¶å¤§å‹æ‹å–ä¸­è‡ªåŠ¨æŠ•æ ‡ç®—æ³•çš„æœºä¼šï¼Œè¿˜å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…åœ¨åšå¼ˆè®ºã€å¼ºåŒ–å­¦ä¹ ã€ç”Ÿæˆæ¨¡å‹ã€è¿è¥ä¼˜åŒ–ç­‰é¢†åŸŸè§£å†³å¹¿æ³›çš„å†³ç­–åˆ¶å®šç ”ç©¶é—®é¢˜ã€‚æ­¤å¤–ï¼ŒAuctionNetè¿˜å¯ä»¥ç”¨äºç ”ç©¶å…¶ä»–å¤§å‹æ¸¸æˆä¸­çš„å†³ç­–åˆ¶å®šé—®é¢˜ï¼Œä¾‹å¦‚åœ¨çº¿å¹¿å‘Šã€æ¨èç³»ç»Ÿã€èµ„æºåˆ†é…ç­‰ã€‚

## From Code to Play Benchmarking Program Search for Games Using Large Language Models
### Abstract
Large language models (LLMs) have shown impressive capabilities in generating
program code, opening exciting opportunities for applying program synthesis to
games. In this work, we explore the potential of LLMs to directly synthesize
usable code for a wide range of gaming applications, focusing on two
programming languages, Python and Java. We use an evolutionary hill-climbing
algorithm, where the mutations and seeds of the initial programs are controlled
by LLMs. For Python, the framework covers various game-related tasks, including
five miniature versions of Atari games, ten levels of Baba is You, an
environment inspired by Asteroids, and a maze generation task. For Java, the
framework contains 12 games from the TAG tabletop games framework. Across 29
tasks, we evaluated 12 language models for Python and 8 for Java. Our findings
suggest that the performance of LLMs depends more on the task than on model
size. While larger models generate more executable programs, these do not
always result in higher-quality solutions but are much more expensive. No model
has a clear advantage, although on any specific task, one model may be better.
Trying many models on a problem and using the best results across them is more
reliable than using just one.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»ä»£ç åˆ°æ¸¸æˆï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¸¸æˆç¨‹åºæœç´¢çš„åŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆç¨‹åºä»£ç æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œå°†ç¨‹åºåˆæˆåº”ç”¨äºæ¸¸æˆé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç¨‹åºåˆæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰æˆ–JSONè½¬æ¢å™¨ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨èŒƒå›´å’Œçµæ´»æ€§ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsç›´æ¥åˆæˆå¯ç”¨äºå„ç§æ¸¸æˆåº”ç”¨çš„ä»£ç çš„æ½œåŠ›ï¼Œå¹¶è¯„ä¼°å…¶åœ¨æ¸¸æˆé¢†åŸŸçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMsçš„ç¨‹åºæœç´¢æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆé¢†åŸŸåˆæˆç¨‹åºä»£ç çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è¿›åŒ–çˆ¬å±±ç®—æ³•ï¼Œå…¶ä¸­åˆå§‹ç¨‹åºçš„çªå˜å’Œç§å­ç”±LLMsæ§åˆ¶ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨Pythonå’ŒJavaä¸¤ç§ç¼–ç¨‹è¯­è¨€ï¼Œåœ¨29ä¸ªä¸åŒçš„æ¸¸æˆä»»åŠ¡ä¸Šè¯„ä¼°äº†12ä¸ªPythonè¯­è¨€æ¨¡å‹å’Œ8ä¸ªJavaè¯­è¨€æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç ”ç©¶å‘ç°ï¼ŒLLMsçš„æ€§èƒ½æ›´å¤šåœ°å–å†³äºä»»åŠ¡æœ¬èº«ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å¤§å°ã€‚è™½ç„¶æ›´å¤§çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´å¤šå¯æ‰§è¡Œçš„ç¨‹åºï¼Œä½†è¿™äº›ç¨‹åºå¹¶ä¸æ€»æ˜¯äº§ç”Ÿæ›´é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”æˆæœ¬æ›´é«˜ã€‚æ²¡æœ‰æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼ŒæŸäº›æ¨¡å‹å¯èƒ½è¡¨ç°æ›´å¥½ã€‚å°è¯•å¤šç§æ¨¡å‹å¹¶ä½¿ç”¨æœ€ä½³ç»“æœå¯ä»¥æé«˜å¯é æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆé¢†åŸŸåˆæˆç¨‹åºä»£ç çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨ä¸”å¯æ‰©å±•çš„å¹³å°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¸¸æˆé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ç”Ÿæˆçš„ä»£ç è´¨é‡ä¸é«˜ã€å¯æ‰§è¡Œæ€§å·®ç­‰ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„æœç´¢ç­–ç•¥å’Œæ›´å¥½çš„æç¤ºå·¥ç¨‹ï¼Œä»¥æé«˜LLMsçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¤šç§æ¨¡å‹å¯ä»¥é™ä½æˆæœ¬å¹¶æé«˜ç»“æœçš„å¯é æ€§ã€‚

## BALROG Benchmarking Agentic LLM and VLM Reasoning On Games
### Abstract
Large Language Models (LLMs) and Vision Language Models (VLMs) possess
extensive knowledge and exhibit promising reasoning abilities; however, they
still struggle to perform well in complex, dynamic environments. Real-world
tasks require handling intricate interactions, advanced spatial reasoning,
long-term planning, and continuous exploration of new strategies-areas in which
we lack effective methodologies for comprehensively evaluating these
capabilities. To address this gap, we introduce BALROG, a novel benchmark
designed to assess the agentic capabilities of LLMs and VLMs through a diverse
set of challenging games. Our benchmark incorporates a range of existing
reinforcement learning environments with varying levels of difficulty,
including tasks that are solvable by non-expert humans in seconds to extremely
challenging ones that may take years to master (e.g., the NetHack Learning
Environment). We devise fine-grained metrics to measure performance and conduct
an extensive evaluation of several popular open-source and closed-source LLMs
and VLMs. Our findings indicate that while current models achieve partial
success in the easier games, they struggle significantly with more challenging
tasks. Notably, we observe severe deficiencies in vision-based decision-making,
as models perform worse when visual representations of the environments are
provided. We release BALROG as an open and user-friendly benchmark to
facilitate future research and development in the agentic community.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BALROGï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆä¸­çš„æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚ç°å®ä¸–ç•Œä»»åŠ¡éœ€è¦å¤„ç†å¤æ‚çš„äº¤äº’ã€é«˜çº§çš„ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’å’ŒæŒç»­æ¢ç´¢æ–°ç­–ç•¥ï¼Œè€Œè¿™äº›é¢†åŸŸç¼ºä¹æœ‰æ•ˆçš„æ–¹æ³•æ¥å…¨é¢è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BALROGï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°LLMså’ŒVLMsä»£ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œé€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¸¸æˆè¿›è¡Œæµ‹è¯•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†BALROGåŸºå‡†ï¼ŒåŒ…å«å…­ä¸ªå…·æœ‰ä¸åŒéš¾åº¦çº§åˆ«çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œä»ç®€å•åˆ°æå…¶å¤æ‚ï¼Œä¾‹å¦‚NetHackå­¦ä¹ ç¯å¢ƒã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ç»†ç²’åº¦çš„æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ï¼Œå¹¶å¯¹å¤šä¸ªæµè¡Œçš„å¼€æºå’Œé—­æºLLMså’ŒVLMsè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿›è¡Œäº†å®šæ€§åˆ†æï¼Œç ”ç©¶äº†ä»£ç†åœ¨ç©ºé—´æ¨ç†ã€ç³»ç»Ÿæ¢ç´¢å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è¾ƒç®€å•çš„æ¸¸æˆä¸­å–å¾—äº†éƒ¨åˆ†æˆåŠŸï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡å‹åœ¨åŸºäºè§†è§‰çš„å†³ç­–æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œå½“æä¾›ç¯å¢ƒçš„è§†è§‰è¡¨ç¤ºæ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°æ›´å·®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„BALROGåŸºå‡†ä¸ºè¯„ä¼°LLMså’ŒVLMsçš„ä»£ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†å½“å‰æ¨¡å‹åœ¨è§†è§‰å†³ç­–å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## TMGBench A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs
### Abstract
The rapid advancement of large language models (LLMs) has accelerated their
application in reasoning, with strategic reasoning drawing increasing
attention. To evaluate LLMs' strategic reasoning capabilities, game theory,
with its concise structure, has become a preferred approach. However, current
research focuses on a limited selection of games, resulting in low coverage.
Classic game scenarios risk data leakage, and existing benchmarks often lack
extensibility, making them inadequate for evaluating state-of-the-art models.
To address these challenges, we propose TMGBench, a benchmark with
comprehensive game type coverage, novel scenarios, and flexible organization.
Specifically, we incorporate all 144 game types summarized by the
Robinson-Goforth topology of 2x2 games, constructed as classic games. We also
employ synthetic data generation to create diverse, higher-quality scenarios
through topic guidance and human inspection, referred to as story-based games.
Lastly, we provide a sustainable framework for increasingly powerful LLMs by
treating these games as atomic units and organizing them into more complex
forms via sequential, parallel, and nested structures. Our comprehensive
evaluation of mainstream LLMs covers tests on rational reasoning, robustness,
Theory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in
accuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,
OpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and
70.0% on sequential, parallel, and nested games, highlighting TMGBench's
challenges.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TMGBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿæ¸¸æˆåŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶ä¸­æˆ˜ç•¥æ¨ç†èƒ½åŠ›å°¤å…¶å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¾€å¾€å±€é™äºå°‘æ•°ç»å…¸æ¸¸æˆï¼Œå¯¼è‡´æ¸¸æˆç±»å‹çš„è¦†ç›–ç‡ä½ï¼Œä¸”ç»å…¸æ¸¸æˆåœºæ™¯å­˜åœ¨æ•°æ®æ³„éœ²çš„é£é™©ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œéš¾ä»¥è¯„ä¼°æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TMGBenchï¼Œä¸€ä¸ªå…·æœ‰å…¨é¢æ¸¸æˆç±»å‹è¦†ç›–ç‡ã€æ–°é¢–åœºæ™¯å’Œçµæ´»ç»„ç»‡æ–¹å¼çš„åŸºå‡†æµ‹è¯•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢çš„æ¸¸æˆç±»å‹è¦†ç›–ç‡
TMGBenchåŒ…å«äº†ç”±Robinson-Goforthæ‹“æ‰‘ç»“æ„æ€»ç»“çš„144ç§2x2æ¸¸æˆç±»å‹ï¼Œæ¶µç›–äº†å„ç§ä¸åŒçš„æ¸¸æˆç»“æ„ï¼ŒåŒ…æ‹¬ç»å…¸æ¸¸æˆå¦‚å›šå¾’å›°å¢ƒç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–°é¢–çš„åœºæ™¯
ä¸ºäº†è§£å†³ç»å…¸æ¸¸æˆåœºæ™¯çš„æ•°æ®æ³„éœ²é—®é¢˜ï¼ŒTMGBenché‡‡ç”¨äº†åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºæ¯ç§ç»å…¸æ¸¸æˆåˆ›å»ºäº†äº”ä¸ªä¸åŒçš„åŸºäºæ•…äº‹çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯æ¶µç›–äº†å•†ä¸šã€æ³•å¾‹ã€äº¤é€šç­‰ç°å®ç”Ÿæ´»ä¸­çš„ä¸»é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šçµæ´»çš„æ¸¸æˆç»„ç»‡æ–¹å¼
TMGBenchå°†æ¸¸æˆè§†ä¸ºåŸå­å•ä½ï¼Œå¹¶é€šè¿‡é¡ºåºã€å¹¶è¡Œå’ŒåµŒå¥—ç»“æ„å°†å®ƒä»¬ç»„ç»‡æˆæ›´å¤æ‚çš„æ¸¸æˆå½¢å¼ï¼Œä»¥è¯„ä¼°LLMsåœ¨å¹¶è¡Œã€é¡ºåºå’Œå¤šå±‚çº§å†³ç­–æ–¹é¢çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹ä¸»æµLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ç†æ€§æ¨ç†ã€æ¨ç†é²æ£’æ€§ã€å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰èƒ½åŠ›å’Œå¤æ‚æ¸¸æˆå½¢å¼çš„æ¨ç†ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æˆ˜ç•¥æ¨ç†è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä»å­˜åœ¨ç¼ºé™·ï¼Œä¸”å¯¹ToMçš„æŒæ¡ç¨‹åº¦ä¹Ÿå„ä¸ç›¸åŒã€‚OpenAIçš„æœ€æ–°æ¨ç†æ¨¡å‹o1-miniåœ¨é¡ºåºã€å¹¶è¡Œå’ŒåµŒå¥—æ¸¸æˆä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º66.6%ã€60.0%å’Œ70.0%ï¼Œçªæ˜¾äº†TMGBenchçš„æŒ‘æˆ˜æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TMGBenchä¸ºè¯„ä¼°LLMsçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬å…¨é¢çš„æ¸¸æˆç±»å‹è¦†ç›–ç‡ã€æ–°é¢–çš„åœºæ™¯å’Œçµæ´»çš„æ¸¸æˆç»„ç»‡æ–¹å¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†LLMsåœ¨æˆ˜ç•¥æ¨ç†æ–¹é¢çš„ç¼ºé™·ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ–¹å‘ï¼Œä¸ºLLMsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## Game4Loc A UAV Geo-Localization Benchmark from Game Data
### Abstract
The vision-based geo-localization technology for UAV, serving as a secondary
source of GPS information in addition to the global navigation satellite
systems (GNSS), can still operate independently in the GPS-denied environment.
Recent deep learning based methods attribute this as the task of image matching
and retrieval. By retrieving drone-view images in geo-tagged satellite image
database, approximate localization information can be obtained. However, due to
high costs and privacy concerns, it is usually difficult to obtain large
quantities of drone-view images from a continuous area. Existing drone-view
datasets are mostly composed of small-scale aerial photography with a strong
assumption that there exists a perfect one-to-one aligned reference image for
any query, leaving a significant gap from the practical localization scenario.
In this work, we construct a large-range contiguous area UAV geo-localization
dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,
and targets using modern computer games. Based on this dataset, we introduce a
more practical UAV geo-localization task including partial matches of
cross-view paired data, and expand the image-level retrieval to the actual
localization in terms of distance (meters). For the construction of drone-view
and satellite-view pairs, we adopt a weight-based contrastive learning
approach, which allows for effective learning while avoiding additional
post-processing matching steps. Experiments demonstrate the effectiveness of
our data and training method for UAV geo-localization, as well as the
generalization capabilities to real-world scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Game4Locï¼šåŸºäºæ¸¸æˆæ•°æ®çš„æ— äººæœºåœ°ç†å®šä½åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ— äººæœºè§†è§‰åœ°ç†å®šä½æŠ€æœ¯ä½œä¸ºå…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰çš„è¾…åŠ©æ‰‹æ®µï¼Œåœ¨GPSä¿¡å·ä¸å¯ç”¨çš„ç¯å¢ƒä¸‹ä»èƒ½ç‹¬ç«‹å·¥ä½œã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å°†è¿™ä¸€ä»»åŠ¡è§†ä¸ºå›¾åƒåŒ¹é…å’Œæ£€ç´¢é—®é¢˜ã€‚é€šè¿‡åœ¨å¸¦æœ‰åœ°ç†æ ‡ç­¾çš„å«æ˜Ÿå›¾åƒæ•°æ®åº“ä¸­æ£€ç´¢æ— äººæœºè§†è§’å›¾åƒï¼Œå¯ä»¥è·å¾—è¿‘ä¼¼çš„å®šä½ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç”±äºæˆæœ¬é«˜æ˜‚å’Œéšç§é—®é¢˜ï¼Œé€šå¸¸éš¾ä»¥ä»è¿ç»­åŒºåŸŸè·å¾—å¤§é‡æ— äººæœºè§†è§’å›¾åƒã€‚ç°æœ‰çš„æ— äººæœºè§†è§’æ•°æ®é›†å¤§å¤šç”±å°è§„æ¨¡èˆªç©ºæ‘„å½±ç»„æˆï¼Œå¹¶å‡è®¾å¯¹äºä»»ä½•æŸ¥è¯¢éƒ½å­˜åœ¨ä¸€ä¸ªå®Œç¾çš„ä¸€å¯¹ä¸€åŒ¹é…çš„å‚è€ƒå›¾åƒï¼Œè¿™ä¸å®é™…å®šä½åœºæ™¯å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†ä¸€ä¸ªåä¸ºGTA-UAVçš„å¤§å‹è¿ç»­åŒºåŸŸæ— äººæœºåœ°ç†å®šä½æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å¤šä¸ªé£è¡Œé«˜åº¦ã€å§¿æ€ã€åœºæ™¯å’Œç›®æ ‡ï¼Œä½¿ç”¨ç°ä»£è®¡ç®—æœºæ¸¸æˆç”Ÿæˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†ä¸€ä¸ªæ›´å®é™…çš„æ— äººæœºåœ°ç†å®šä½ä»»åŠ¡ï¼ŒåŒ…æ‹¬è·¨è§†å›¾é…å¯¹æ•°æ®çš„éƒ¨åˆ†åŒ¹é…ï¼Œå¹¶å°†å›¾åƒçº§æ£€ç´¢æ‰©å±•åˆ°å®é™…å®šä½ï¼ˆä»¥ç±³ä¸ºå•ä½ï¼‰ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé‡‡ç”¨åŸºäºæƒé‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ„å»ºæ— äººæœºè§†è§’å’Œå«æ˜Ÿè§†è§’é…å¯¹ï¼Œå…è®¸æœ‰æ•ˆå­¦ä¹ ï¼ŒåŒæ—¶é¿å…é¢å¤–çš„åå¤„ç†åŒ¹é…æ­¥éª¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®å’Œæ–¹æ³•å¯¹æ— äººæœºåœ°ç†å®šä½æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”å…·æœ‰å°†æ¨¡å‹æ¨å¹¿åˆ°ç°å®ä¸–ç•Œåœºæ™¯çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GTA-UAVæ•°æ®é›†å’ŒåŸºäºæƒé‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä¸ºæ— äººæœºåœ°ç†å®šä½ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜æ— äººæœºåœ¨GPSä¿¡å·ä¸å¯ç”¨ç¯å¢ƒä¸‹çš„å®šä½ç²¾åº¦å’Œé²æ£’æ€§ã€‚

## ES-KT-24 A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation
### Abstract
This paper introduces ES-KT-24, a novel multimodal Knowledge Tracing (KT)
dataset for intelligent tutoring systems in educational game contexts. Although
KT is crucial in adaptive learning, existing datasets often lack game-based and
multimodal elements. ES-KT-24 addresses these limitations by incorporating
educational game-playing videos, synthetically generated question text, and
detailed game logs. The dataset covers Mathematics, English, Indonesian, and
Malaysian subjects, emphasizing diversity and including non-English content.
The synthetic text component, generated using a large language model,
encompasses 28 distinct knowledge concepts and 182 questions, featuring 15,032
users and 7,782,928 interactions. Our benchmark experiments demonstrate the
dataset's utility for KT research by comparing Deep learning-based KT models
with Language Model-based Knowledge Tracing (LKT) approaches. Notably, LKT
models showed slightly higher performance than traditional DKT models,
highlighting the potential of language model-based approaches in this field.
Furthermore, ES-KT-24 has the potential to significantly advance research in
multimodal KT models and learning analytics. By integrating game-playing videos
and detailed game logs, this dataset offers a unique approach to dissecting
student learning patterns through advanced data analysis and machine-learning
techniques. It has the potential to unearth new insights into the learning
process and inspire further exploration in the field.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ES-KT-24ï¼šåŸºäºæ•™è‚²æ¸¸æˆè§†é¢‘å’Œåˆæˆæ–‡æœ¬ç”Ÿæˆçš„å¤šæ¨¡æ€çŸ¥è¯†è¿½è¸ªåŸºå‡†æ•°æ®é›†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰æ˜¯è‡ªé€‚åº”å­¦ä¹ ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®å­¦ç”Ÿä¸å­¦ä¹ ææ–™çš„äº’åŠ¨æ¥å»ºæ¨¡ä»–ä»¬çš„çŸ¥è¯†çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„KTæ•°æ®é›†å¾€å¾€ç¼ºä¹åŸºäºæ¸¸æˆå’Œå¤šæ¨¡æ€å…ƒç´ ï¼Œé™åˆ¶äº†å…¶åœ¨æ¸¸æˆåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ES-KT-24ï¼Œä¸€ä¸ªåŒ…å«æ•™è‚²æ¸¸æˆè§†é¢‘ã€åˆæˆæ–‡æœ¬å’Œè¯¦ç»†æ¸¸æˆæ—¥å¿—çš„å¤šæ¨¡æ€KTæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€ç—›ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€æ•°æ®é›†
ES-KT-24æ•°æ®é›†åŒ…å«äº†æ•™è‚²æ¸¸æˆè§†é¢‘ã€åˆæˆæ–‡æœ¬å’Œè¯¦ç»†æ¸¸æˆæ—¥å¿—ï¼Œæ¶µç›–äº†æ•°å­¦ã€è‹±è¯­ã€å°åº¦å°¼è¥¿äºšè¯­å’Œé©¬æ¥è¥¿äºšè¯­ç­‰ç§‘ç›®ï¼Œå¼ºè°ƒäº†å¤šæ ·æ€§å’Œéè‹±è¯­å†…å®¹ã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´å…¨é¢åœ°åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æ¨¡å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆæˆæ–‡æœ¬ç”Ÿæˆ
ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæ–‡æœ¬ï¼Œæ¶µç›–äº†28ä¸ªä¸åŒçš„çŸ¥è¯†æ¦‚å¿µå’Œ182ä¸ªé—®é¢˜ï¼Œå…±æœ‰15,032åç”¨æˆ·å’Œ7,782,928æ¬¡äº’åŠ¨ã€‚è¿™ä¸ºKTç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ–‡æœ¬æ•°æ®ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ES-KT-24æ•°æ®é›†ä¸Šè¿›è¡Œçš„åŸºå‡†å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¯­è¨€æ¨¡å‹çš„KTï¼ˆLKTï¼‰æ–¹æ³•åœ¨æ€§èƒ½ä¸Šç•¥ä¼˜äºä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ KTï¼ˆDKTï¼‰æ¨¡å‹ã€‚è¿™çªå‡ºäº†è¯­è¨€æ¨¡å‹åœ¨KTé¢†åŸŸçš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ES-KT-24æ•°æ®é›†ä¸ºKTç ”ç©¶æä¾›äº†æ–°çš„åŸºå‡†ï¼Œå¹¶å±•ç¤ºäº†å¤šæ¨¡æ€æ•°æ®åœ¨æ¸¸æˆåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„ä»·å€¼ã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¯¥æ•°æ®é›†æ¥æ¢ç´¢æ¸¸æˆåŒ–å­¦ä¹ ç°è±¡ã€åˆ†æå¤šæ¨¡æ€å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å¼€å‘æ›´å¤æ‚çš„æ¨¡å‹æ¥æ•æ‰æ•™è‚²ç¯å¢ƒä¸­çš„å¤šæ ·æ¨¡æ€ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜æ”¯æŒæ¸¸æˆéš¾åº¦åˆ†ç±»ã€ç‰¹å¾å½±å“åˆ†æã€ç”Ÿæˆæ¸¸æˆè®¾è®¡ã€å¤šæ¨¡æ€å­¦ä¹ åˆ†æã€å‚ä¸åº¦å’Œç»©æ•ˆç›¸å…³æ€§ã€è·¨æ–‡åŒ–å­¦ä¹ æ¨¡å¼ä»¥åŠæ—¶é—´å­¦ä¹ åŠ¨æ€ç­‰æ–¹é¢çš„ç ”ç©¶ã€‚

## Atari-GPT Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games
### Abstract
Recent advancements in large language models (LLMs) have expanded their
capabilities beyond traditional text-based tasks to multimodal domains,
integrating visual, auditory, and textual data. While multimodal LLMs have been
extensively explored for high-level planning in domains like robotics and
games, their potential as low-level controllers remains largely untapped. In
this paper, we introduce a novel benchmark aimed at testing the emergent
capabilities of multimodal LLMs as low-level policies in Atari games. Unlike
traditional reinforcement learning (RL) methods that require training for each
new environment and reward function specification, these LLMs utilize
pre-existing multimodal knowledge to directly engage with game environments.
Our study assesses the performances of multiple multimodal LLMs against
traditional RL agents, human players, and random agents, focusing on their
ability to understand and interact with complex visual scenes and formulate
strategic responses. Our results show that these multimodal LLMs are not yet
capable of being zero-shot low-level policies. Furthermore, we see that this
is, in part, due to their visual and spatial reasoning. Additional results and
videos are available on our project webpage:
https://dev1nw.github.io/atari-gpt/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Atari-GPTï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„çªç ´ï¼Œå®ƒä»¬çš„èƒ½åŠ›å·²ç»æ‰©å±•åˆ°äº†å¤šæ¨¡æ€é¢†åŸŸï¼Œæ•´åˆäº†è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®ã€‚å°½ç®¡å¤šæ¨¡æ€LLMsåœ¨æœºå™¨äººæŠ€æœ¯å’Œæ¸¸æˆç­‰é¢†åŸŸçš„å†³ç­–è§„åˆ’æ–¹é¢å¾—åˆ°äº†å¹¿æ³›æ¢ç´¢ï¼Œä½†å®ƒä»¬ä½œä¸ºä½çº§æ§åˆ¶å™¨çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æŒ–æ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæµ‹è¯•å¤šæ¨¡æ€LLMsåœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAtari-GPTåŸºå‡†
æœ¬æ–‡æå‡ºäº†Atari-GPTåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€LLMsåœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡æ¯”è¾ƒLLMsä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ã€äººç±»ç©å®¶å’Œéšæœºä»£ç†çš„æ€§èƒ½ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç†è§£å¤æ‚è§†è§‰åœºæ™¯å’Œåˆ¶å®šæˆ˜ç•¥ååº”æ–¹é¢çš„èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°è§†è§‰å’Œç©ºé—´æ¨ç†èƒ½åŠ›
é™¤äº†æ¸¸æˆæ€§èƒ½è¯„ä¼°ï¼Œæœ¬æ–‡è¿˜é€šè¿‡ä¸€ç³»åˆ—æç¤ºæµ‹è¯•äº†LLMsçš„è§†è§‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œæˆ˜ç•¥ç›´è§‰èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯•æ—¨åœ¨æ­ç¤ºLLMsåœ¨ç†è§£æ¸¸æˆç¯å¢ƒæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ”¹è¿›æä¾›æ–¹å‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨Atariæ¸¸æˆä¸­çš„è¡¨ç°ä¸å¦‚äººç±»ç©å®¶æˆ–ä¸“é—¨çš„RLæ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶èƒ½å¤Ÿè¯†åˆ«å’Œä¸æ¸¸æˆå¸§ä¸­çš„å…³é”®å…ƒç´ è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½œä¸ºä½çº§æ§åˆ¶å™¨çš„æ€§èƒ½ä»ç„¶ä¸ä½³ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºç¼ºä¹é’ˆå¯¹æ­¤ä»»åŠ¡çš„è®­ç»ƒä»¥åŠç©ºé—´æ¨ç†çš„å›°éš¾ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼ŒLLMsçš„æ¨ç†æ—¶é—´å¯¹äºå®æ—¶å†³ç­–è‡³å…³é‡è¦ï¼Œè€Œç›®å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨é€Ÿåº¦æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºLLMsåœ¨ä½çº§æ§åˆ¶ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†å®è´µçš„è§è§£ã€‚å°½ç®¡LLMså°šæœªè¾¾åˆ°ä¸äººç±»æˆ–RLæ¨¡å‹ç›¸åŒ¹é…çš„æ°´å¹³ï¼Œä½†å®ƒä»¬åœ¨Atariæ¸¸æˆä¸­çš„è¡¨ç°ä»ç„¶å€¼å¾—æ³¨æ„ã€‚è¿™è¡¨æ˜LLMså…·æœ‰é€‚åº”æ€§å’Œæ½œåŠ›ï¼Œå¯ä»¥æ‰©å±•åˆ°å…¶åŸå§‹è®­ç»ƒèŒƒå›´ä¹‹å¤–ï¼Œä¸ºæœªæ¥åœ¨éœ€è¦è¯¦ç»†ç¯å¢ƒäº¤äº’å’Œå†³ç­–çš„åº”ç”¨ä¸­ä½œä¸ºæ›´é€šç”¨çš„ä½çº§æ§åˆ¶å™¨æä¾›äº†ä¸€ç¥ã€‚

## Hokoff Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks
### Abstract
The advancement of Offline Reinforcement Learning (RL) and Offline
Multi-Agent Reinforcement Learning (MARL) critically depends on the
availability of high-quality, pre-collected offline datasets that represent
real-world complexities and practical applications. However, existing datasets
often fall short in their simplicity and lack of realism. To address this gap,
we propose Hokoff, a comprehensive set of pre-collected datasets that covers
both offline RL and offline MARL, accompanied by a robust framework, to
facilitate further research. This data is derived from Honor of Kings, a
recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate
nature, closely resembling real-life situations. Utilizing this framework, we
benchmark a variety of offline RL and offline MARL algorithms. We also
introduce a novel baseline algorithm tailored for the inherent hierarchical
action space of the game. We reveal the incompetency of current offline RL
approaches in handling task complexity, generalization and multi-task learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Hokoffï¼šåŸºäºç‹è€…è£è€€çš„çœŸå®æ¸¸æˆæ•°æ®é›†åŠå…¶ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰å’Œç¦»çº¿å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆOffline MARLï¼‰çš„ç ”ç©¶è¿›å±•ä¾èµ–äºé«˜è´¨é‡ã€é¢„å…ˆæ”¶é›†çš„ç¦»çº¿æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åº”ä»£è¡¨ç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå®é™…åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†å¾€å¾€è¿‡äºç®€å•ï¼Œç¼ºä¹ç°å®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Hokoffï¼Œè¿™æ˜¯ä¸€å¥—å…¨é¢çš„é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†ç¦»çº¿RLå’Œç¦»çº¿MARLï¼Œå¹¶ä¼´éšç€ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚è¿™äº›æ•°æ®æ¥è‡ªç‹è€…è£è€€ï¼Œè¿™æ˜¯ä¸€æ¬¾ä»¥å…¶å¤æ‚æ€§è´¨è€Œé—»åçš„å¤šäººåœ¨çº¿æˆ˜æ–—ç«æŠ€åœºï¼ˆMOBAï¼‰æ¸¸æˆï¼Œä¸ç°å®ç”Ÿæ´»æƒ…å†µéå¸¸ç›¸ä¼¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç‹è€…è£è€€çš„å¤æ‚ç¯å¢ƒ
Hokoffçš„æ•°æ®é›†æ¥æºäºç‹è€…è£è€€ï¼Œè¿™æ˜¯ä¸€æ¬¾æ‹¥æœ‰è¶…è¿‡1äº¿æ—¥æ´»è·ƒç©å®¶çš„å…¨çƒæœ€å—æ¬¢è¿çš„MOBAæ¸¸æˆä¹‹ä¸€ã€‚è¯¥æ¸¸æˆçš„å¤æ‚æ€§è¿œè¿œè¶…è¿‡å…¶ä»–æ•°æ®é›†ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯çš„æ½œåŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æºã€æ˜“ç”¨çš„æ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€æºã€æ˜“ç”¨çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ç¦»çº¿RLï¼ˆé‡‡æ ·ã€è®­ç»ƒå’Œè¯„ä¼°ï¼‰çš„å…¨é¢æµç¨‹å’Œä¸€äº›æœ‰ç”¨çš„å·¥å…·ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸°å¯Œå¤šæ ·çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ä½¿ç”¨ä¸€ç³»åˆ—å…·æœ‰ä¸åŒè®¾è®¡å› ç´ çš„å‰è®­ç»ƒæ¨¡å‹ç”Ÿæˆï¼Œä¸ä»…é€‚ç”¨äºç¦»çº¿RLï¼Œä¹Ÿé€‚ç”¨äºç¦»çº¿MARLã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šçº§æ¨¡å‹
ä¸ºäº†ç¡®ä¿ä¸åŒç®—æ³•çš„æ€§èƒ½æ¯”è¾ƒçš„æœ‰æ•ˆæ€§å’Œå…¬æ­£æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å¤šçº§æ¨¡å‹ï¼ŒåŒ…å«å¤šä¸ªå…·æœ‰ä¸åŒæ°´å¹³çš„æ£€æŸ¥ç‚¹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºé‡‡æ ·å’Œè¯„ä¼°ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ ·åŒ–çš„æ•°æ®é›†
Hokoffæä¾›äº†å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¤šéš¾åº¦ã€å¤šä»»åŠ¡ã€æ³›åŒ–ã€å¼‚æ„é˜Ÿå‹å’Œå­ä»»åŠ¡ç­‰ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä¸ºç¦»çº¿RLå’Œç¦»çº¿MARLçš„ç ”ç©¶æä¾›æ›´çœŸå®çš„ç¯å¢ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Hokoffæ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šç§ç¦»çº¿RLå’Œç¦»çº¿MARLç®—æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ç‹è€…è£è€€å›ºæœ‰å±‚æ¬¡ç»“æ„åŠ¨ä½œç©ºé—´çš„æ–°çš„åŸºçº¿ç®—æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ç¦»çº¿RLæ–¹æ³•åœ¨å¤„ç†ä»»åŠ¡å¤æ‚æ€§ã€æ³›åŒ–å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Hokoffæ•°æ®é›†å’Œæ¡†æ¶ä¸ºç¦»çº¿RLå’Œç¦»çº¿MARLçš„ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚å…¶å¤šæ ·åŒ–çš„æ•°æ®é›†å’Œå¼ºå¤§çš„æ¡†æ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè¯„ä¼°ç¦»çº¿å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¸ºè§£å†³ç°å®ä¸–ç•Œé—®é¢˜æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## A Benchmark Environment for Offline Reinforcement Learning in Racing Games
### Abstract
Offline Reinforcement Learning (ORL) is a promising approach to reduce the
high sample complexity of traditional Reinforcement Learning (RL) by
eliminating the need for continuous environmental interactions. ORL exploits a
dataset of pre-collected transitions and thus expands the range of application
of RL to tasks in which the excessive environment queries increase training
time and decrease efficiency, such as in modern AAA games. This paper
introduces OfflineMania a novel environment for ORL research. It is inspired by
the iconic TrackMania series and developed using the Unity 3D game engine. The
environment simulates a single-agent racing game in which the objective is to
complete the track through optimal navigation. We provide a variety of datasets
to assess ORL performance. These datasets, created from policies of varying
ability and in different sizes, aim to offer a challenging testbed for
algorithm development and evaluation. We further establish a set of baselines
for a range of Online RL, ORL, and hybrid Offline to Online RL approaches using
our environment.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨èµ›è½¦æ¸¸æˆä¸­çš„æ–°åŸºå‡†ç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¸¸æˆè¡Œä¸šä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ä¼ ç»ŸRLçš„é«˜æ ·æœ¬å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå¦‚ç°ä»£AAAæ¸¸æˆã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆORLï¼‰é€šè¿‡åˆ©ç”¨é¢„å…ˆæ”¶é›†çš„è¿‡æ¸¡æ•°æ®é›†æ¥å‡å°‘å¯¹ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ï¼Œä»è€Œé™ä½äº†æ ·æœ¬å¤æ‚åº¦ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹æ¸¸æˆç¯å¢ƒï¼Œç‰¹åˆ«æ˜¯èµ›è½¦æ¸¸æˆï¼Œçš„ORLæ•°æ®é›†å’ŒåŸºå‡†ç¯å¢ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥OfflineManiaç¯å¢ƒ
OfflineManiaæ˜¯ä¸€ä¸ªåŸºäºUnity 3Dæ¸¸æˆå¼•æ“å¼€å‘çš„å•ä»£ç†èµ›è½¦æ¸¸æˆç¯å¢ƒï¼Œçµæ„Ÿæ¥æºäºTrackManiaç³»åˆ—æ¸¸æˆã€‚è¯¥ç¯å¢ƒæ¨¡æ‹Ÿäº†èµ›è½¦æ¸¸æˆï¼Œç›®æ ‡æ˜¯é€šè¿‡æœ€ä¼˜å¯¼èˆªå®Œæˆèµ›é“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæä¾›å¤šæ ·åŒ–çš„æ•°æ®é›†
ä¸ºäº†è¯„ä¼°ORLçš„æ€§èƒ½ï¼Œè®ºæ–‡æä¾›äº†å¤šç§æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ç”±ä¸åŒèƒ½åŠ›æ°´å¹³çš„ç­–ç•¥ç”Ÿæˆï¼Œå¹¶å…·æœ‰ä¸åŒçš„è§„æ¨¡ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨ä¸ºç®—æ³•å¼€å‘å’Œè¯„ä¼°æä¾›ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå»ºç«‹åŸºçº¿
è®ºæ–‡ä½¿ç”¨OfflineManiaç¯å¢ƒï¼Œä¸ºä¸€ç³»åˆ—åœ¨çº¿RLã€ORLå’Œæ··åˆç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•å»ºç«‹äº†åŸºçº¿ã€‚è¿™äº›åŸºçº¿æœ‰åŠ©äºè¯„ä¼°ä¸åŒç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¿ƒè¿›ORLåœ¨æ¸¸æˆç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒIQLåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºTD3BCå’ŒCQLã€‚æ­¤å¤–ï¼ŒSDBGå’ŒJSRLåœ¨ç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜ç¦»çº¿è®­ç»ƒçš„æ”¿ç­–çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
OfflineManiaç¯å¢ƒå’Œæä¾›çš„æ•°æ®é›†ä¸ºORLç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è®ºæ–‡ä¸­çš„åŸºçº¿ç»“æœä¸ºè¯„ä¼°ä¸åŒORLç®—æ³•çš„æ€§èƒ½æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†ç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•åœ¨æ¸¸æˆç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œä¸ºæ¸¸æˆå¼€å‘äººå‘˜æä¾›äº†æ–°çš„æ€è·¯ã€‚

## Evaluating Large Language Models with Grid-Based Game Competitions An Extensible LLM Benchmark and Leaderboard
### Abstract
We introduce a novel and extensible benchmark for large language models
(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.
The open-source game simulation code, available on GitHub, allows LLMs to
compete and generates detailed data files in JSON, CSV, TXT, and PNG formats
for leaderboard rankings and further analysis. We present the results of games
among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by
Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and
GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of
results from other LLMs. In total, we simulated 2,310 matches (5 sessions for
each pair among 7 LLMs and a random player) across three types of games, using
three distinct prompt types: list, illustration, and image. The results
revealed significant variations in LLM performance across different games and
prompt types, with analysis covering win and disqualification rates, missed
opportunity analysis, and invalid move analysis. The details of the leaderboard
and result matrix data are available as open-access data on GitHub. This study
enhances our understanding of LLMs' capabilities in playing games they were not
specifically trained for, helping to assess their rule comprehension and
strategic thinking. On the path to Artificial General Intelligence (AGI), this
study lays the groundwork for future exploration into their utility in complex
decision-making scenarios, illuminating their strategic thinking abilities and
offering directions for further inquiry into the limits of LLMs within
game-based frameworks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡åŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼šä¸€ä¸ªå¯æ‰©å±•çš„LLMåŸºå‡†å’Œæ’è¡Œæ¦œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å’Œæ€§èƒ½å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ç­‰ï¼Œå·²ç»æ— æ³•å…¨é¢è¯„ä¼°LLMsçš„å¤æ‚èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦æ–°çš„åŸºå‡†æ¥è¯„ä¼°LLMsåœ¨è§„åˆ™ç†è§£ã€æˆ˜ç•¥æ€è€ƒå’Œå†³ç­–åˆ¶å®šç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å¯æ‰©å±•çš„åŸºå‡†ï¼Œé€šè¿‡ç½‘æ ¼æ¸¸æˆï¼ˆå¦‚äº•å­—æ£‹ã€å››å­æ£‹å’Œäº”å­æ£‹ï¼‰æ¥è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚è¿™äº›æ¸¸æˆéœ€è¦æ¨¡å‹ç†è§£è§„åˆ™ã€åˆ¶å®šç­–ç•¥å¹¶åšå‡ºå†³ç­–ï¼Œä»è€Œå…¨é¢è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æ”¾æºä»£ç å’Œæ’è¡Œæ¦œ
æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå¼€æºçš„æ¸¸æˆæ¨¡æ‹Ÿä»£ç ï¼Œå…è®¸LLMsè¿›è¡Œç«èµ›ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„JSONã€CSVã€TXTå’ŒPNGæ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼Œç”¨äºæ’è¡Œæ¦œæ’åå’Œè¿›ä¸€æ­¥åˆ†æã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªæ’è¡Œæ¦œï¼Œå±•ç¤ºäº†ä¸åŒLLMsåœ¨ä¸åŒæ¸¸æˆå’Œæç¤ºç±»å‹ä¸‹çš„è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹ä¸ƒä¸ªé¢†å…ˆçš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬Anthropicçš„Claude 3.5 Sonnetå’ŒClaude 3 Sonnetã€Googleçš„Gemini 1.5 Proå’ŒGemini 1.5 Flashã€OpenAIçš„GPT-4 Turboå’ŒGPT-4oä»¥åŠMetaçš„Llama3-70Bã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ä¸åŒæ¸¸æˆå’Œæç¤ºç±»å‹ä¸‹çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¾‹å¦‚ï¼ŒClaude 3.5 Sonnetåœ¨äº•å­—æ£‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨äº”å­æ£‹ä¸­è¡¨ç°è¾ƒå·®ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨å¤„ç†å¤æ‚å’ŒåŸºäºè§†è§‰çš„æç¤ºæ ¼å¼æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›åŸºå‡†ä¸ºè¯„ä¼°LLMsçš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç å’Œæ’è¡Œæ¦œï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†æå’Œæ¯”è¾ƒã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºç†è§£LLMsçš„èƒ½åŠ›å’Œå±€é™æ€§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æ–¹å‘ã€‚

## Simple Stochastic Stopping Games A Generator and Benchmark Library
### Abstract
Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the
simplest version of Stochastic Games for which there is no known
polynomial-time algorithm. Condon showed that Stochastic Games are
polynomial-time reducible to SSGs, which in turn are polynomial-time reducible
to Stopping Games. SSGs are games where all decisions are binary and every move
has a random outcome with a known probability distribution. Stopping Games are
SSGs that are guaranteed to terminate. There are many algorithms for SSGs, most
of which are fast in practice, but they all lack theoretical guarantees for
polynomial-time convergence. The pursuit of a polynomial-time algorithm for
SSGs is an active area of research. This paper is intended to support such
research by making it easier to study the graphical structure of SSGs. Our
contributions are: (1) a generating algorithm for Stopping Games, (2) a proof
that the algorithm can generate any game, (3) a list of additional
polynomial-time reductions that can be made to Stopping Games, (4) an open
source generator for generating fully reduced instances of Stopping Games that
comes with instructions and is fully documented, (5) a benchmark set of such
instances, (6) and an analysis of how two main algorithm types perform on our
benchmark set.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç®€å•éšæœºåœæ­¢æ¸¸æˆçš„ç”Ÿæˆç®—æ³•ä¸åŸºå‡†åº“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç®€å•éšæœºæ¸¸æˆï¼ˆSSGsï¼‰æ˜¯éšæœºæ¸¸æˆä¸­å·²çŸ¥æ²¡æœ‰å¤šé¡¹å¼æ—¶é—´ç®—æ³•çš„æœ€ç®€å•ç‰ˆæœ¬ã€‚å°½ç®¡å­˜åœ¨è®¸å¤šåœ¨å®è·µä¸­å¿«é€Ÿçš„ç®—æ³•ï¼Œä½†å®ƒä»¬éƒ½ç¼ºä¹å¤šé¡¹å¼æ—¶é—´æ”¶æ•›çš„ç†è®ºä¿è¯ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æä¾›ä¸€ä¸ªç”Ÿæˆç®—æ³•å’ŒåŸºå‡†åº“æ¥æ”¯æŒå¯¹SSGsçš„ç ”ç©¶ï¼Œä½¿å…¶æ›´å®¹æ˜“ç ”ç©¶SSGsçš„å›¾å½¢ç»“æ„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆç®—æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”Ÿæˆåœæ­¢æ¸¸æˆå®ä¾‹çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ç”Ÿæˆä»»ä½•åœæ­¢æ¸¸æˆï¼Œå¹¶å…·æœ‰å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé¡¹å¼æ—¶é—´ç®€åŒ–
æœ¬æ–‡è¿˜æä¾›äº†ä¸€ç³»åˆ—å¤šé¡¹å¼æ—¶é—´ç®€åŒ–ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–SSGsï¼Œä½¿å…¶æ›´å®¹æ˜“ç ”ç©¶ã€‚è¿™äº›ç®€åŒ–åŒ…æ‹¬ç§»é™¤å¯å¿«é€Ÿè§£å†³çš„å­å›¾ã€åˆå¹¶å…·æœ‰ç›¸åŒå€¼çš„èŠ‚ç‚¹ã€å°†å¼ºè¿é€šåˆ†é‡ç‹¬ç«‹è§£å†³ç­‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨ä¸¤ç§ç®—æ³•ï¼ˆHoffman-Karpå’Œæ’åˆ—æ”¹è¿›ç®—æ³•ï¼‰åœ¨ç”Ÿæˆçš„åŸºå‡†é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜æ’åˆ—æ”¹è¿›ç®—æ³•åœ¨è¿­ä»£æ¬¡æ•°å’Œæ—¶é—´ä¸Šå‡ä¼˜äºHoffman-Karpç®—æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç”Ÿæˆç®—æ³•å’ŒåŸºå‡†åº“ä¸ºSSGsçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£SSGsçš„å¤æ‚æ€§ï¼Œå¹¶å¯»æ‰¾æ›´æœ‰æ•ˆçš„ç®—æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¤šé¡¹å¼æ—¶é—´ç®€åŒ–æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œä»¥æé«˜ç®—æ³•çš„æ•ˆç‡ã€‚

## Application-level Benchmarking of Quantum Computers using Nonlocal Game Strategies
### Abstract
In a nonlocal game, two noncommunicating players cooperate to convince a
referee that they possess a strategy that does not violate the rules of the
game. Quantum strategies allow players to optimally win some games by
performing joint measurements on a shared entangled state, but computing these
strategies can be challenging. We present a variational quantum algorithm to
compute quantum strategies for nonlocal games by encoding the rules of a
nonlocal game into a Hamiltonian. We show how this algorithm can generate a
short-depth optimal quantum strategy for a graph coloring game with a quantum
advantage. This quantum strategy is then evaluated on fourteen different
quantum hardware platforms to demonstrate its utility as a benchmark. Finally,
we discuss potential sources of errors that can explain the observed decreased
performance of the executed task and derive an expression for the number of
samples required to accurately estimate the win rate in the presence of noise.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨éå±€åŸŸæ¸¸æˆç­–ç•¥å¯¹é‡å­è®¡ç®—æœºè¿›è¡Œåº”ç”¨çº§åŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å½“å‰é‡å­ç¡¬ä»¶çš„å‘å±•å°šå¤„äºåˆçº§é˜¶æ®µï¼Œè¿è¡Œå…·æœ‰å¯è¯æ˜ä¼˜åŠ¿çš„ç®€å•é‡å­ç®—æ³•å®ä¾‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œå¼€å‘èƒ½å¤Ÿæµ‹è¯•å’ŒéªŒè¯é‡å­ç¡¬ä»¶ç‹¬ç‰¹ç‰¹æ€§çš„åŸºå‡†æµ‹è¯•å·¥å…·å’ŒæŠ€æœ¯è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ä½çº§åŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼ˆå¦‚éšæœºåŸºå‡†æµ‹è¯•ï¼‰æ— æ³•å¸®åŠ©ç¡®å®šç®—æ³•æµç¨‹ä¸­çš„é”™è¯¯æ¥æºï¼Œè€Œé«˜çº§åŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼ˆå¦‚é‡å­ä½“ç§¯ï¼‰åˆ™è¿‡äºå®½æ³›ï¼Œæ— æ³•æ•æ‰åˆ°ç‰¹å®šç¡¬ä»¶å¹³å°å®ç°é‡å­ä¼˜åŠ¿çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨å˜åˆ†é‡å­ç®—æ³•è®¡ç®—éå±€åŸŸæ¸¸æˆä¸­çš„é‡å­ç­–ç•¥ã€‚è¯¥æ–¹æ³•å°†éå±€åŸŸæ¸¸æˆçš„è§„åˆ™ç¼–ç ä¸ºå“ˆå¯†é¡¿é‡ï¼Œå¹¶é€šè¿‡åŒç›¸ä¼˜åŒ–æŠ€æœ¯æ‰¾åˆ°èµ„æºæ€å’Œæµ‹é‡æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
1. **èµ„æºæ€ä¼˜åŒ–**ï¼šä½¿ç”¨å˜åˆ†é‡å­æœ¬å¾æ±‚è§£å™¨ï¼ˆå¦‚ADAPT-VQEï¼‰æ‰¾åˆ°å›ºå®šæµ‹é‡æ–¹æ¡ˆä¸‹çš„æœ€ä¼˜å…±äº«æ€ã€‚
2. **æµ‹é‡ä¼˜åŒ–**ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¼˜åŒ–æµ‹é‡å‚æ•°ï¼ŒåŒæ—¶å›ºå®šå…±äº«æ€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å°†è¯¥æ–¹æ³•åº”ç”¨äºCHSHæ¸¸æˆã€N-åˆ†éƒ¨å¯¹ç§°æ¸¸æˆå’Œå›¾ç€è‰²æ¸¸æˆï¼Œå¹¶æˆåŠŸæ‰¾åˆ°äº†æœ€ä¼˜é‡å­ç­–ç•¥ã€‚å…¶ä¸­ï¼Œå›¾ç€è‰²æ¸¸æˆç­–ç•¥åœ¨14ä¸ªé¡¶ç‚¹çš„å›¾ä¸Šå®ç°äº†é‡å­ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨14ä¸ªä¸åŒçš„é‡å­ç¡¬ä»¶å¹³å°ä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆè¯„ä¼°é‡å­ç¡¬ä»¶çš„éå±€åŸŸèƒ½åŠ›ï¼Œå¹¶æ­ç¤ºæ½œåœ¨çš„é”™è¯¯æ¥æºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å˜åˆ†é‡å­ç®—æ³•ä¸ºè®¡ç®—éå±€åŸŸæ¸¸æˆä¸­çš„é‡å­ç­–ç•¥æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶ä¸ºé‡å­ç¡¬ä»¶åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éå±€åŸŸæ¸¸æˆï¼Œå¹¶æœ‰åŠ©äºå¼€å‘æ›´åˆé€‚çš„é‡å­åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œä»¥é€‚åº”é‡å­è®¾å¤‡çš„å‘å±•å’Œç¡¬ä»¶æ¶æ„çš„å¤æ‚æ€§ã€‚

## Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques
### Abstract
The remarkable success of GPT models across various tasks, including toponymy
recognition motivates us to assess the performance of the GPT-3 model in the
geocoding address parsing task. To ensure that the evaluation more accurately
mirrors performance in real-world scenarios with diverse user input qualities
and resolve the pressing need for a 'gold standard' evaluation dataset for
geocoding systems, we introduce a benchmark dataset of low-quality address
descriptions synthesized based on human input patterns mining from actual input
logs of a geocoding system in production. This dataset has 21 different input
errors and variations; contains over 239,000 address records that are uniquely
selected from streets across all U.S. 50 states and D.C.; and consists of three
subsets to be used as training, validation, and testing sets. Building on this,
we train and gauge the performance of the GPT-3 model in extracting address
components, contrasting its performance with transformer-based and LSTM-based
models. The evaluation results indicate that Bidirectional LSTM-CRF model has
achieved the best performance over these transformer-based models and GPT-3
model. Transformer-based models demonstrate very comparable results compared to
the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in
performance, showcases potential in the address parsing task with few-shot
examples, exhibiting room for improvement with additional fine-tuning. We open
source the code and data of this presented benchmark so that researchers can
utilize it for future model development or extend it to evaluate similar tasks,
such as document geocoding.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ChatGPT åœ¨åœ°ç†ç¼–ç åœ°å€è§£æä»»åŠ¡ä¸­çš„æ½œåŠ›è¯„ä¼°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ°ç†ç¼–ç æ˜¯å°†åœ°å€æè¿°è½¬æ¢ä¸ºåœ°ç†åæ ‡çš„è¿‡ç¨‹ï¼Œå¹¿æ³›åº”ç”¨äºåŸå¸‚è§„åˆ’ã€å…¬å…±å«ç”Ÿç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œåœ°ç†ç¼–ç æ•°æ®çš„å‡†ç¡®æ€§å—åˆ°åœ°å€è§£æè´¨é‡çš„å½±å“ã€‚åœ°å€è§£ææ˜¯ä»ç”¨æˆ·è¾“å…¥çš„åœ°å€æè¿°ä¸­æå–åœ°å€ç»„ä»¶çš„è¿‡ç¨‹ï¼Œå…¶å‡†ç¡®æ€§å¯¹åœ°ç†ç¼–ç ç»“æœçš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åœ°å€è§£ææ–¹æ³•ä¸»è¦åŸºäºè§„åˆ™å’Œç»Ÿè®¡æ–¹æ³•ï¼Œä½†éš¾ä»¥å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„é”™è¯¯å’Œå˜åŒ–ã€‚æ­¤å¤–ï¼Œç¼ºä¹ä¸€ä¸ªâ€œé»„é‡‘æ ‡å‡†â€çš„è¯„ä¼°æ•°æ®é›†æ¥å…¨é¢è¯„ä¼°åœ°ç†ç¼–ç ç³»ç»Ÿçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°åœ°ç†ç¼–ç åœ°å€è§£ææŠ€æœ¯çš„åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚è¯¥åŸºå‡†æ•°æ®é›†æ˜¯åŸºäºå®é™…åœ°ç†ç¼–ç ç³»ç»Ÿæ—¥å¿—ä¸­çš„äººç±»è¾“å…¥æ¨¡å¼åˆæˆçš„ä½è´¨é‡åœ°å€æè¿°ï¼ŒåŒ…å«21ç§ä¸åŒçš„è¾“å…¥é”™è¯¯å’Œå˜åŒ–ï¼Œå¹¶è¦†ç›–äº†ç¾å›½æ‰€æœ‰50ä¸ªå·å’Œå“¥ä¼¦æ¯”äºšç‰¹åŒºçš„è¡—é“ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚

æœ¬æ–‡è¯„ä¼°äº†äº”ç§ä¸åŒçš„åœ°å€è§£ææ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-3æ¨¡å‹ã€åŸºäºTransformerçš„æ¨¡å‹å’ŒåŸºäºLSTMçš„æ¨¡å‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŒå‘LSTM-CRFæ¨¡å‹åœ¨åœ°å€ç»„ä»¶æå–æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯Transformeræ¨¡å‹ã€‚å°½ç®¡GPT-3æ¨¡å‹çš„æ€§èƒ½ç•¥é€Šä¸€ç­¹ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å±•ç°å‡ºæ½œåŠ›ï¼Œè¡¨æ˜é€šè¿‡è¿›ä¸€æ­¥çš„å¾®è°ƒå¯ä»¥æå‡å…¶æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒå‘LSTM-CRFæ¨¡å‹åœ¨æ‰€æœ‰åœ°å€ç»„ä»¶çš„æå–æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…¶F1åˆ†æ•°å’Œè§£æåˆ†æ•°æœ€é«˜ã€‚Transformeræ¨¡å‹åœ¨å¤§å¤šæ•°åœ°å€ç»„ä»¶çš„æå–æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ç•¥é€ŠäºåŒå‘LSTM-CRFæ¨¡å‹ã€‚GPT-3æ¨¡å‹åœ¨åœ°å€ç»„ä»¶æå–æ–¹é¢è¡¨ç°ç›¸å¯¹è¾ƒå·®ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å±•ç°å‡ºæ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ä¸ºè¯„ä¼°åœ°ç†ç¼–ç åœ°å€è§£ææŠ€æœ¯æä¾›äº†ä¸€ä¸ªâ€œé»„é‡‘æ ‡å‡†â€ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGPT-3æ¨¡å‹åœ¨åœ°å€è§£æä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥çš„å¾®è°ƒæ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## Dialogue Games for Benchmarking Language Understanding Motivation, Taxonomy, Strategy
### Abstract
How does one measure "ability to understand language"? If it is a person's
ability that is being measured, this is a question that almost never poses
itself in an unqualified manner: Whatever formal test is applied, it takes
place on the background of the person's language use in daily social practice,
and what is measured is a specialised variety of language understanding (e.g.,
of a second language; or of written, technical language). Computer programs do
not have this background. What does that mean for the applicability of formal
tests of language understanding? I argue that such tests need to be
complemented with tests of language use embedded in a practice, to arrive at a
more comprehensive evaluation of "artificial language understanding". To do
such tests systematically, I propose to use "Dialogue Games" -- constructed
activities that provide a situational embedding for language use. I describe a
taxonomy of Dialogue Game types, linked to a model of underlying capabilites
that are tested, and thereby giving an argument for the \emph{construct
validity} of the test. I close with showing how the internal structure of the
taxonomy suggests an ordering from more specialised to more general situational
language understanding, which potentially can provide some strategic guidance
for development in this field.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¯¹è¯æ¸¸æˆï¼šè¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆå·¥å…·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰é¢†åŸŸé•¿æœŸä»¥æ¥ä¾èµ–äºåŸºäºæ–‡æœ¬çš„è¯„ä¼°æ–¹æ³•ï¼Œä¾‹å¦‚é—®ç­”æˆ–æ–‡æœ¬æ‘˜è¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è¯­è¨€åœ¨å®é™…æƒ…å¢ƒä¸­çš„ä½¿ç”¨ï¼Œä¾‹å¦‚å¯¹è¯ä¸­çš„æŒ‡ä»£ã€æ¾„æ¸…å’Œè¡ŒåŠ¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼šå¯¹è¯æ¸¸æˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯¹è¯æ¸¸æˆ
å¯¹è¯æ¸¸æˆæ˜¯ä¸€ç§æ„å»ºçš„æ´»åŠ¨ï¼Œå‚ä¸è€…é€šè¿‡è¯­è¨€äº¤æµæ¥è¾¾æˆç‰¹å®šç›®æ ‡ã€‚è¿™äº›æ¸¸æˆå¯ä»¥æ¨¡æ‹Ÿç°å®ç”Ÿæ´»ä¸­çš„å¯¹è¯åœºæ™¯ï¼Œä¾‹å¦‚ç»„è£…å®¶å…·æˆ–é¢„è®¢ç«è½¦ç¥¨ã€‚é€šè¿‡è®¾è®¡ä¸åŒçš„æ¸¸æˆç±»å‹ï¼Œå¯ä»¥é’ˆå¯¹è¯­è¨€ç†è§£çš„å„ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚æŒ‡ä»£ã€ä¿¡æ¯è¯·æ±‚ã€è¡ŒåŠ¨åè°ƒç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯¹è¯æ¸¸æˆåˆ†ç±»
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹è¯æ¸¸æˆåˆ†ç±»æ³•ï¼Œå°†æ¸¸æˆåˆ†ä¸ºä¸ƒç§ç±»å‹ï¼šæŒ‡ä»£æ¸¸æˆã€ä¿¡æ¯æ¸¸æˆã€æ„å»ºæ¸¸æˆã€å¯¼èˆªæ¸¸æˆã€è°ˆåˆ¤æ¸¸æˆå’Œæ•™å­¦æ¸¸æˆã€‚æ¯ç§ç±»å‹éƒ½ä¾§é‡äºè¯­è¨€ç†è§£çš„ç‰¹å®šæ–¹é¢ï¼Œå¹¶å¯¹åº”äºæ¨¡å‹ä¸­ä¸åŒçš„çŸ¥è¯†å’Œé”šå®šè¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯é‡ç‚¹ä»‹ç»äº†å¯¹è¯æ¸¸æˆä½œä¸ºä¸€ç§è¯„ä¼°å·¥å…·çš„ç†è®ºåŸºç¡€å’Œè®¾è®¡æ–¹æ³•ã€‚ç„¶è€Œï¼Œä½œè€…é€šè¿‡åˆ†æç°æœ‰çš„å¯¹è¯æ¸¸æˆæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
å¯¹è¯æ¸¸æˆä¸ºè¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚å®ƒå¯ä»¥ç”¨äºï¼š
* **è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°**ï¼šä¾‹å¦‚ï¼ŒæŒ‡ä»£æ¸¸æˆå¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹æŒ‡ä»£è¡¨è¾¾çš„ç†è§£èƒ½åŠ›ã€‚
* **è¯„ä¼°æ¨¡å‹çš„æ•´ä½“è¯­è¨€ç†è§£èƒ½åŠ›**ï¼šé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—ä¸åŒå¤æ‚åº¦çš„å¯¹è¯æ¸¸æˆï¼Œå¯ä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚
* **ä¿ƒè¿›æ¨¡å‹çš„å‘å±•**ï¼šå¯¹è¯æ¸¸æˆå¯ä»¥æ­ç¤ºæ¨¡å‹åœ¨è¯­è¨€ç†è§£æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶ä¸ºæ¨¡å‹æ”¹è¿›æä¾›æ–¹å‘ã€‚

### ğŸŒŸ æ€»ç»“
å¯¹è¯æ¸¸æˆæ˜¯ä¸€ç§è¯„ä¼°è¯­è¨€ç†è§£èƒ½åŠ›çš„æœ‰æ•ˆå·¥å…·ï¼Œå®ƒå¯ä»¥æ¨¡æ‹Ÿç°å®ç”Ÿæ´»ä¸­çš„å¯¹è¯åœºæ™¯ï¼Œå¹¶é’ˆå¯¹è¯­è¨€ç†è§£çš„å„ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡æå‡ºçš„å¯¹è¯æ¸¸æˆåˆ†ç±»æ³•ä¸ºè®¾è®¡å¯¹è¯æ¸¸æˆæä¾›äº†æŒ‡å¯¼ï¼Œå¹¶ä¸ºè¯„ä¼°æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ã€‚

## MuG A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields
### Abstract
Previous research has demonstrated the advantages of integrating data from
multiple sources over traditional unimodal data, leading to the emergence of
numerous novel multimodal applications. We propose a multimodal classification
benchmark MuG with eight datasets that allows researchers to evaluate and
improve their models. These datasets are collected from four various genres of
games that cover tabular, textual, and visual modalities. We conduct
multi-aspect data analysis to provide insights into the benchmark, including
label balance ratios, percentages of missing features, distributions of data
within each modality, and the correlations between labels and input modalities.
We further present experimental results obtained by several state-of-the-art
unimodal classifiers and multimodal classifiers, which demonstrate the
challenging and multimodal-dependent properties of the benchmark. MuG is
released at https://github.com/lujiaying/MUG-Bench with the data, tutorials,
and implemented baselines.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MuGï¼šåŸºäºæ¸¸æˆæ•°æ®çš„å¤šå…ƒåˆ†ç±»åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ å·²ç»æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚å¤šæ¨¡æ€å­¦ä¹ èƒ½å¤Ÿæ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º MuG çš„å¤šæ¨¡æ€åˆ†ç±»åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
MuG åŒ…å«äº†æ¥è‡ªå››ä¸ªä¸åŒæ¸¸æˆç±»å‹çš„å…«ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–äº†è¡¨æ ¼ã€æ–‡æœ¬å’Œè§†è§‰ä¸‰ç§æ¨¡æ€ã€‚è¿™äº›æ•°æ®é›†ç»è¿‡å¿…è¦çš„æ¸…æ´—ã€è½¬æ¢å’Œä¿®æ”¹ï¼Œä»¥ä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ MuG çš„ç‰¹æ€§ï¼Œæœ¬æ–‡è¿›è¡Œäº†å¤šæ–¹é¢çš„æ•°æ®åˆ†æï¼ŒåŒ…æ‹¬æ ‡ç­¾å¹³è¡¡æ¯”ä¾‹ã€ç¼ºå¤±ç‰¹å¾ç™¾åˆ†æ¯”ã€æ¯ç§æ¨¡æ€ä¸­æ•°æ®çš„åˆ†å¸ƒä»¥åŠæ ‡ç­¾ä¸è¾“å…¥æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨å¤šä¸ªæœ€å…ˆè¿›çš„å•æ¨¡æ€åˆ†ç±»å™¨å’Œå¤šæ¨¡æ€åˆ†ç±»å™¨åœ¨ MuG ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€åˆ†ç±»å™¨åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½ä¼˜äºå•æ¨¡æ€åˆ†ç±»å™¨ï¼Œè¿™è¡¨æ˜ MuG ä¸­çš„åˆ†ç±»ä»»åŠ¡ç¡®å®ä¾èµ–äºå¤šæ¨¡æ€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å¤šæ¨¡æ€åˆ†ç±»å™¨ MUGNETï¼Œå®ƒåœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰çš„å¤šæ¨¡æ€åˆ†ç±»å™¨ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MuG åŸºå‡†æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹çš„é‡è¦å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ MUGNET æ¨¡å‹ä¹Ÿä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨ MuG åŸºå‡†æ•°æ®é›†è¿›è¡Œæ›´å¤šå…³äºå¤šæ¨¡æ€å­¦ä¹ çš„ç ”ç©¶ï¼Œå¹¶æ¢ç´¢æ–°çš„åº”ç”¨åœºæ™¯ã€‚

## Equilibria in Repeated Games under No-Regret with Dynamic Benchmarks
### Abstract
In repeated games, strategies are often evaluated by their ability to
guarantee the performance of the single best action that is selected in
hindsight, a property referred to as \emph{Hannan consistency}, or
\emph{no-regret}. However, the effectiveness of the single best action as a
yardstick to evaluate strategies is limited, as any static action may perform
poorly in common dynamic settings. Our work therefore turns to a more ambitious
notion of \emph{dynamic benchmark consistency}, which guarantees the
performance of the best \emph{dynamic} sequence of actions, selected in
hindsight subject to a constraint on the allowable number of action changes.
Our main result establishes that for any joint empirical distribution of play
that may arise when all players deploy no-regret strategies, there exist
dynamic benchmark consistent strategies such that if all players deploy these
strategies the same empirical distribution emerges when the horizon is large
enough. This result demonstrates that although dynamic benchmark consistent
strategies have a different algorithmic structure and provide significantly
enhanced individual assurances, they lead to the same equilibrium set as
no-regret strategies. Moreover, the proof of our main result uncovers the
capacity of independent algorithms with strong individual guarantees to foster
a strong form of coordination.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢é‡å¤åšå¼ˆä¸­çš„å‡è¡¡ç­–ç•¥ï¼šåŠ¨æ€åŸºå‡†ä¸€è‡´æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é‡å¤åšå¼ˆä¸­ï¼Œç­–ç•¥é€šå¸¸é€šè¿‡å…¶åœ¨äº‹åé€‰æ‹©çš„æœ€ä½³å•æ¬¡è¡ŒåŠ¨çš„è¡¨ç°æ¥è¯„ä¼°ï¼Œè¿™ç§æ€§è´¨è¢«ç§°ä¸ºHannanä¸€è‡´æ€§æˆ–æ— åæ‚”ã€‚ç„¶è€Œï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­ï¼Œä»»ä½•é™æ€è¡ŒåŠ¨çš„è¡¨ç°éƒ½å¯èƒ½ä¸ä½³ï¼Œå› æ­¤è¿™ç§è¯„ä¼°ç­–ç•¥çš„æ ‡å‡†å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ›´ä¸ºé›„å¿ƒå‹ƒå‹ƒçš„åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µç¡®ä¿äº†åœ¨äº‹åé€‰æ‹©çš„æœ€ä½³åŠ¨æ€è¡ŒåŠ¨åºåˆ—çš„è¡¨ç°ï¼ŒåŒæ—¶å—åˆ°å…è®¸çš„è¡ŒåŠ¨å˜åŒ–æ¬¡æ•°çš„çº¦æŸã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡æå‡ºäº†åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§çš„æ¦‚å¿µï¼Œå®ƒè€ƒè™‘äº†åœ¨é‡å¤åšå¼ˆä¸­ï¼Œç©å®¶å¯èƒ½é‡‡å–çš„ä¸€ç³»åˆ—åŠ¨æ€è¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯å•ä¸€çš„é™æ€è¡ŒåŠ¨ã€‚è¿™ç§ç­–ç•¥ä¸ä»…è€ƒè™‘äº†å•æ¬¡è¡ŒåŠ¨çš„æ•ˆæœï¼Œè¿˜è€ƒè™‘äº†æ•´ä¸ªè¡ŒåŠ¨åºåˆ—çš„é€‚åº”æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æœ¬æ–‡çš„ä¸»è¦ç»“æœæ˜¯ï¼Œå¯¹äºæ‰€æœ‰ç©å®¶éƒ½é‡‡ç”¨æ— åæ‚”ç­–ç•¥æ—¶å¯èƒ½å‡ºç°çš„ä»»ä½•è”åˆç»éªŒåˆ†å¸ƒï¼Œéƒ½å­˜åœ¨åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§ç­–ç•¥ï¼Œä½¿å¾—å½“åšå¼ˆçš„å‘¨æœŸè¶³å¤Ÿé•¿æ—¶ï¼Œæ‰€æœ‰ç©å®¶é‡‡ç”¨è¿™äº›ç­–ç•¥ä¼šäº§ç”Ÿç›¸åŒçš„ç»éªŒåˆ†å¸ƒã€‚è¿™ä¸€ç»“æœè¯æ˜äº†è™½ç„¶åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§ç­–ç•¥å…·æœ‰ä¸åŒçš„ç®—æ³•ç»“æ„ï¼Œå¹¶ä¸ºä¸ªä½“æä¾›äº†æ˜¾è‘—å¢å¼ºçš„ä¿éšœï¼Œä½†å®ƒä»¬å¯¼è‡´çš„å‡è¡¡é›†åˆä¸æ— åæ‚”ç­–ç•¥ç›¸åŒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡ç†è®ºåˆ†æè¯æ˜äº†åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§ç­–ç•¥çš„å­˜åœ¨æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä½†æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœã€‚ä½œè€…é€šè¿‡æ•°å­¦è¯æ˜å±•ç¤ºäº†ç‹¬ç«‹ç®—æ³•å…·æœ‰å¼ºçƒˆä¸ªä½“ä¿éšœçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¿ƒè¿›ä¸€ç§å¼ºçƒˆçš„åè°ƒå½¢å¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŠ¨æ€åŸºå‡†ä¸€è‡´æ€§æ¦‚å¿µä¸ºé‡å¤åšå¼ˆä¸­çš„ç­–ç•¥è¯„ä¼°æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå¯¹äºç†è§£å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„åšå¼ˆè¡Œä¸ºå…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„è¯æ˜è¿‡ç¨‹æ­ç¤ºäº†å…·æœ‰å¼ºä¸ªä½“ä¿éšœçš„ç‹¬ç«‹ç®—æ³•åœ¨ä¿ƒè¿›åè°ƒæ–¹é¢çš„æ½œåŠ›ï¼Œè¿™å¯¹äºç®—æ³•è®¾è®¡å’Œåšå¼ˆç†è®ºçš„åº”ç”¨å…·æœ‰å¯å‘æ„ä¹‰ã€‚

## A Game Benchmark for Real-Time Human-Swarm Control
### Abstract
We present a game benchmark for testing human-swarm control algorithms and
interfaces in a real-time, high-cadence scenario. Our benchmark consists of a
swarm vs. swarm game in a virtual ROS environment in which the goal of the game
is to capture all agents from the opposing swarm; the game's high-cadence is a
result of the capture rules, which cause agent team sizes to fluctuate rapidly.
These rules require players to consider both the number of agents currently at
their disposal and the behavior of their opponent's swarm when they plan
actions. We demonstrate our game benchmark with a default human-swarm control
system that enables a player to interact with their swarm through a high-level
touchscreen interface. The touchscreen interface transforms player gestures
into swarm control commands via a low-level decentralized ergodic control
framework. We compare our default human-swarm control system to a
flocking-based control system, and discuss traits that are crucial for swarm
control algorithms and interfaces operating in real-time, high-cadence
scenarios like our game benchmark. Our game benchmark code is available on
Github; more information can be found at
https://sites.google.com/view/swarm-game-benchmark.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å®æ—¶äººæœºç¾¤æ§æ¸¸æˆåŸºå‡†ï¼šæŒ‘æˆ˜ä¸æœºé‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æœºå™¨äººæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæœºå™¨äººé›†ç¾¤åœ¨å±é™©ç¯å¢ƒä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç®—æ³•å’Œç•Œé¢åœ¨é«˜åº¦åŠ¨æ€çš„ç¯å¢ƒä¸­å¾€å¾€éš¾ä»¥èƒœä»»ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„æµ‹è¯•åŸºå‡†æ¥è¯„ä¼°è¿™äº›ç®—æ³•å’Œç•Œé¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¸¸æˆçš„å®æ—¶äººæœºç¾¤æ§åŸºå‡†ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿé«˜åº¦åŠ¨æ€ç¯å¢ƒä¸‹çš„ç¾¤æ§æŒ‘æˆ˜ï¼Œå¹¶è¯„ä¼°ä¸åŒç®—æ³•å’Œç•Œé¢çš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€ç¾¤æ§æ¸¸æˆåŸºå‡†
æœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä¸ºâ€œç¾¤æ§å¯¹æŠ—â€çš„æ¸¸æˆï¼Œç©å®¶é€šè¿‡ç•Œé¢æ§åˆ¶è‡ªå·±çš„æœºå™¨äººé›†ç¾¤ï¼Œç›®æ ‡æ˜¯æ•è·å¯¹æ–¹é›†ç¾¤çš„æ‰€æœ‰æœºå™¨äººã€‚æ¸¸æˆè§„åˆ™å¯¼è‡´é›†ç¾¤è§„æ¨¡å¿«é€Ÿå˜åŒ–ï¼Œè¿«ä½¿ç©å®¶åœ¨åˆ¶å®šç­–ç•¥æ—¶è€ƒè™‘è‡ªèº«é›†ç¾¤è§„æ¨¡å’Œå¯¹æ–¹é›†ç¾¤è¡Œä¸ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé»˜è®¤äººæœºç¾¤æ§ç³»ç»Ÿ
æœ¬æ–‡å±•ç¤ºäº†ä¸€ä¸ªé»˜è®¤çš„äººæœºç¾¤æ§ç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…å«ä¸€ä¸ªè§¦æ‘¸å±ç•Œé¢å’Œä¸€ä¸ªä½å±‚çº§çš„å»ä¸­å¿ƒåŒ–éå†æ§åˆ¶æ¡†æ¶ã€‚è§¦æ‘¸å±ç•Œé¢å°†ç©å®¶æ‰‹åŠ¿è½¬æ¢ä¸ºé›†ç¾¤æ§åˆ¶å‘½ä»¤ï¼Œè€Œæ§åˆ¶æ¡†æ¶åˆ™è´Ÿè´£æ‰§è¡Œè¿™äº›å‘½ä»¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå»ä¸­å¿ƒåŒ–éå†æ§åˆ¶
æœ¬æ–‡æå‡ºçš„å»ä¸­å¿ƒåŒ–éå†æ§åˆ¶æ¡†æ¶èƒ½å¤Ÿå°†ç©å®¶ç›®æ ‡åˆ†å¸ƒè½¬æ¢ä¸ºä½å±‚çº§çš„é›†ç¾¤æ§åˆ¶å‘½ä»¤ï¼Œå¹¶å®ç°è§„æ¨¡ä¸å˜æ€§å’Œæ’åˆ—ä¸å˜æ€§ã€‚è¿™ä½¿å¾—ç©å®¶å¯ä»¥ä¸“æ³¨äºé›†ç¾¤çº§åˆ«çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯å•ä¸ªæœºå™¨äººçš„è½¨è¿¹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å®éªŒæ¯”è¾ƒäº†é»˜è®¤äººæœºç¾¤æ§ç³»ç»Ÿä¸åŸºäºç¾¤èšçš„æ§åˆ¶ç³»ç»Ÿçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œé»˜è®¤ç³»ç»Ÿåœ¨å®æ—¶ã€é«˜é¢‘ç‡çš„åœºæ™¯ä¸‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”ç¯å¢ƒå˜åŒ–å¹¶åˆ¶å®šç­–ç•¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŸºå‡†ä¸ºè¯„ä¼°å®æ—¶äººæœºç¾¤æ§ç®—æ³•å’Œç•Œé¢æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å»ä¸­å¿ƒåŒ–éå†æ§åˆ¶æ¡†æ¶ä¹Ÿä¸ºå®ç°çµæ´»ã€å¯æ‰©å±•çš„ç¾¤æ§ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ã€‚

### ğŸŒŸ æœªæ¥å±•æœ›
æœªæ¥ç ”ç©¶å¯ä»¥å¼€å‘è™šæ‹Ÿå¯¹æ‰‹æ¥ä¸äººç±»ç©å®¶ç«äº‰ï¼Œå¹¶æ‰©å±•æ¸¸æˆåŸºå‡†ä»¥è¿›è¡Œäººç±»ä¸»ä½“æµ‹è¯•ï¼Œä»¥æ›´æ·±å…¥åœ°äº†è§£äººç±»ç©å®¶åœ¨å®æ—¶ã€é«˜é¢‘ç‡åœºæ™¯ä¸‹çš„ç­–ç•¥å’Œè¡Œä¸ºã€‚

## WILD-SCAV Benchmarking FPS Gaming AI on Unity3D-based Environments
### Abstract
Recent advances in deep reinforcement learning (RL) have demonstrated complex
decision-making capabilities in simulation environments such as Arcade Learning
Environment, MuJoCo, and ViZDoom. However, they are hardly extensible to more
complicated problems, mainly due to the lack of complexity and variations in
the environments they are trained and tested on. Furthermore, they are not
extensible to an open-world environment to facilitate long-term exploration
research. To learn realistic task-solving capabilities, we need to develop an
environment with greater diversity and complexity. We developed WILD-SCAV, a
powerful and extensible environment based on a 3D open-world FPS (First-Person
Shooter) game to bridge the gap. It provides realistic 3D environments of
variable complexity, various tasks, and multiple modes of interaction, where
agents can learn to perceive 3D environments, navigate and plan, compete and
cooperate in a human-like manner. WILD-SCAV also supports different
complexities, such as configurable maps with different terrains, building
structures and distributions, and multi-agent settings with cooperative and
competitive tasks. The experimental results on configurable complexity,
multi-tasking, and multi-agent scenarios demonstrate the effectiveness of
WILD-SCAV in benchmarking various RL algorithms, as well as it is potential to
give rise to intelligent agents with generalized task-solving abilities. The
link to our open-sourced code can be found here
https://github.com/inspirai/wilderness-scavenger.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WILD-SCAVï¼šåŸºäºUnity3Dçš„FPSæ¸¸æˆAIåŸºå‡†æµ‹è¯•ç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚Arcade Learning Environmentã€MuJoCoå’ŒViZDoomï¼‰ä¸­å±•ç°å‡ºå¤æ‚çš„å†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›ç¯å¢ƒåœ¨å¤æ‚æ€§å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¹Ÿä¸é€‚ç”¨äºå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œæ— æ³•ä¿ƒè¿›é•¿æœŸæ¢ç´¢ç ”ç©¶ã€‚ä¸ºäº†å­¦ä¹ ç°å®ä¸–ç•Œçš„ä»»åŠ¡è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘ä¸€ä¸ªå…·æœ‰æ›´é«˜å¤šæ ·æ€§å’Œå¤æ‚æ€§çš„ç¯å¢ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWILD-SCAVæ˜¯ä¸€ä¸ªåŸºäº3Då¼€æ”¾ä¸–ç•ŒFPSæ¸¸æˆçš„å¼ºå¤§ä¸”å¯æ‰©å±•çš„ç¯å¢ƒï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚å®ƒæä¾›äº†å…·æœ‰å¯å˜å¤æ‚æ€§çš„çœŸå®3Dç¯å¢ƒã€å„ç§ä»»åŠ¡å’Œå¤šç§äº¤äº’æ¨¡å¼ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä»¥ç±»ä¼¼äººç±»çš„æ–¹å¼å­¦ä¹ æ„ŸçŸ¥3Dç¯å¢ƒã€å¯¼èˆªå’Œè§„åˆ’ã€ç«äº‰å’Œåˆä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šWILD-SCAVæ”¯æŒä¸åŒçš„å¤æ‚æ€§ï¼Œä¾‹å¦‚å…·æœ‰ä¸åŒåœ°å½¢ã€å»ºç­‘ç»“æ„å’Œåˆ†å¸ƒçš„å¯é…ç½®åœ°å›¾ï¼Œä»¥åŠå…·æœ‰åˆä½œå’Œç«äº‰ä»»åŠ¡çš„å¤šä»£ç†è®¾ç½®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWILD-SCAVåœ¨å¯é…ç½®å¤æ‚æ€§ã€å¤šä»»åŠ¡å’Œå¤šä»£ç†åœºæ™¯ä¸­æœ‰æ•ˆåœ°è¯„ä¼°äº†å„ç§RLç®—æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWILD-SCAVå…·æœ‰äº§ç”Ÿå…·æœ‰é€šç”¨ä»»åŠ¡è§£å†³èƒ½åŠ›çš„æ™ºèƒ½ä»£ç†çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WILD-SCAVä¸ºç ”ç©¶å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä»£ç†å­¦ä¹ æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ï¼Œå¹¶æ”¯æŒå„ç§å®éªŒå’Œä»»åŠ¡è®¾è®¡ã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å¼ºå¤§çš„RLç®—æ³•ï¼Œå¹¶æ¨åŠ¨3Då¼€æ”¾ä¸–ç•ŒAIç®—æ³•çš„å‘å±•ã€‚

## The Game of Hidden Rules A New Kind of Benchmark Challenge for Machine Learning
### Abstract
As machine learning (ML) is more tightly woven into society, it is imperative
that we better characterize ML's strengths and limitations if we are to employ
it responsibly. Existing benchmark environments for ML, such as board and video
games, offer well-defined benchmarks for progress, but constituent tasks are
often complex, and it is frequently unclear how task characteristics contribute
to overall difficulty for the machine learner. Likewise, without a systematic
assessment of how task characteristics influence difficulty, it is challenging
to draw meaningful connections between performance in different benchmark
environments. We introduce a novel benchmark environment that offers an
enormous range of ML challenges and enables precise examination of how task
elements influence practical difficulty. The tool frames learning tasks as a
"board-clearing game," which we call the Game of Hidden Rules (GOHR). The
environment comprises an expressive rule language and a captive server
environment that can be installed locally. We propose a set of benchmark
rule-learning tasks and plan to support a performance leader-board for
researchers interested in attempting to learn our rules. GOHR complements
existing environments by allowing fine, controlled modifications to tasks,
enabling experimenters to better understand how each facet of a given learning
task contributes to its practical difficulty for an arbitrary ML algorithm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢æœºå™¨å­¦ä¹ æ–°æŒ‘æˆ˜ï¼šéšè—è§„åˆ™æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨ç¤¾ä¼šä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¥½åœ°äº†è§£å…¶ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä»¥ä¾¿è´Ÿè´£ä»»åœ°ä½¿ç”¨å®ƒã€‚ç°æœ‰çš„MLåŸºå‡†ç¯å¢ƒï¼Œå¦‚æ£‹ç›˜æ¸¸æˆå’Œè§†é¢‘æ¸¸æˆï¼Œæä¾›äº†æ˜ç¡®çš„åŸºå‡†æ¥è¡¡é‡è¿›æ­¥ï¼Œä½†è¿™äº›ä»»åŠ¡é€šå¸¸å¾ˆå¤æ‚ï¼Œè€Œä¸”ä¸æ¸…æ¥šä»»åŠ¡ç‰¹å¾å¦‚ä½•å½±å“æœºå™¨å­¦ä¹ è€…çš„æ•´ä½“éš¾åº¦ã€‚æ­¤å¤–ï¼Œæ²¡æœ‰å¯¹ä»»åŠ¡ç‰¹å¾å¦‚ä½•å½±å“éš¾åº¦çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå¾ˆéš¾åœ¨ä¸åŒåŸºå‡†ç¯å¢ƒä¸­ç»˜åˆ¶å‡ºæœ‰æ„ä¹‰çš„æ€§èƒ½è”ç³»ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†ç¯å¢ƒï¼Œç§°ä¸ºâ€œéšè—è§„åˆ™æ¸¸æˆâ€ï¼ˆGOHRï¼‰ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„MLæŒ‘æˆ˜ï¼Œå¹¶èƒ½å¤Ÿç²¾ç¡®åœ°æ£€æŸ¥ä»»åŠ¡å…ƒç´ å¦‚ä½•å½±å“å®é™…éš¾åº¦ã€‚GOHRå°†å­¦ä¹ ä»»åŠ¡æ¡†æ¶åŒ–ä¸ºä¸€ä¸ªâ€œæ¸…ç›˜æ¸¸æˆâ€ï¼Œç¯å¢ƒåŒ…æ‹¬ä¸€ä¸ªä¸°å¯Œçš„è§„åˆ™è¯­è¨€å’Œä¸€ä¸ªå¯æœ¬åœ°å®‰è£…çš„ä¸“ç”¨æœåŠ¡å™¨ç¯å¢ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—åŸºå‡†è§„åˆ™å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶è®¡åˆ’ä¸ºæœ‰å…´è¶£å­¦ä¹ æˆ‘ä»¬è§„åˆ™çš„ç ”ç©¶äººå‘˜æ”¯æŒä¸€ä¸ªæ€§èƒ½æ’è¡Œæ¦œã€‚GOHRé€šè¿‡å…è®¸å¯¹ä»»åŠ¡è¿›è¡Œç²¾ç»†ã€å¯æ§çš„ä¿®æ”¹ï¼Œè¡¥å……äº†ç°æœ‰çš„ç¯å¢ƒï¼Œä½¿å®éªŒè€…èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç»™å®šå­¦ä¹ ä»»åŠ¡çš„æ¯ä¸ªæ–¹é¢å¦‚ä½•ä¸ºä»»æ„MLç®—æ³•çš„å®é™…éš¾åº¦åšå‡ºè´¡çŒ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—æ ·æœ¬è§„åˆ™ï¼Œä¸€ä¸ªæ ·æœ¬MLç®—æ³•ï¼ˆMLAï¼‰ï¼Œæ€§èƒ½æŒ‡æ ‡å’Œæœ‰ç”¨çš„æ•°æ®å‘ˆç°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨DQNç®—æ³•å¯¹æ ·æœ¬è§„åˆ™è¿›è¡Œäº†å­¦ä¹ ï¼Œå¹¶åˆ†æäº†ç´¯ç§¯é”™è¯¯æ•°ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæ‰€æœ‰æ ·æœ¬è§„åˆ™ï¼Œç´¯ç§¯é”™è¯¯æœ€ç»ˆéƒ½ä¼šè¶‹äºå¹³ç¨³ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œç»ˆç«¯ç´¯ç§¯é”™è¯¯â€ï¼ˆTCEï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨Mann-Whitney-Wilcoxon U-Testå¯¹è§„åˆ™éš¾åº¦è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å‘ç°U-Testä¸ç´¯ç§¯é”™è¯¯æ›²çº¿çš„é«˜åº¦ä¸€è‡´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GOHRä¸ºç ”ç©¶MLç®—æ³•çš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªæ–°é¢–è€Œæœ‰åŸåˆ™çš„æ–¹æ³•ã€‚ä½¿ç”¨ä¸°å¯Œçš„è§„åˆ™è¯­æ³•ï¼Œç ”ç©¶äººå‘˜å¯ä»¥å¯¹æ„Ÿå…´è¶£çš„è§„åˆ™è¿›è¡Œç²¾ç¡®çš„æ›´æ”¹ï¼Œä»¥ç ”ç©¶å®ƒä»¬å¦‚ä½•å½±å“ç®—æ³•æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç”¨äºæ¯”è¾ƒä¸åŒç®—æ³•åœ¨ç‰¹å®šè§„åˆ™é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è§‚å¯Ÿè§„åˆ™ç‰¹å¾ï¼ˆä½ç½®ã€å±æ€§ã€é™æ€/åŠ¨æ€ï¼‰å¯¹MLAæ€§èƒ½çš„å½±å“ã€‚GOHRè¿˜å¯ä»¥ç”±äººç±»è§„åˆ™å­¦ä¹ è€…è¿›è¡Œæ¸¸æˆï¼Œä»è€Œèƒ½å¤Ÿåœ¨å¹³ç­‰çš„åŸºç¡€ä¸Šæ¯”è¾ƒäººç±»å­¦ä¹ å’ŒMLã€‚

## GOAL Towards Benchmarking Few-Shot Sports Game Summarization
### Abstract
Sports game summarization aims to generate sports news based on real-time
commentaries. The task has attracted wide research attention but is still
under-explored probably due to the lack of corresponding English datasets.
Therefore, in this paper, we release GOAL, the first English sports game
summarization dataset. Specifically, there are 103 commentary-news pairs in
GOAL, where the average lengths of commentaries and news are 2724.9 and 476.3
words, respectively. Moreover, to support the research in the semi-supervised
setting, GOAL additionally provides 2,160 unlabeled commentary documents. Based
on our GOAL, we build and evaluate several baselines, including extractive and
abstractive baselines. The experimental results show the challenges of this
task still remain. We hope our work could promote the research of sports game
summarization. The dataset has been released at
https://github.com/krystalan/goal.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GOALï¼šè¿ˆå‘å°‘æ ·æœ¬ä½“è‚²æ¸¸æˆæ‘˜è¦åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä½“è‚²æ¸¸æˆæ‘˜è¦çš„ç›®æ ‡æ˜¯åŸºäºå®æ—¶è¯„è®ºç”Ÿæˆä½“è‚²æ–°é—»ã€‚å°½ç®¡è¯¥ä»»åŠ¡å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œä½†ç”±äºç¼ºä¹ç›¸åº”çš„è‹±æ–‡æ•°æ®é›†ï¼Œå› æ­¤ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†GOALï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè‹±æ–‡ä½“è‚²æ¸¸æˆæ‘˜è¦æ•°æ®é›†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†GOALæ•°æ®é›†ï¼ŒåŒ…å«103ä¸ªè¯„è®º-æ–°é—»å¯¹ï¼Œå…¶ä¸­è¯„è®ºå’Œæ–°é—»çš„å¹³å‡é•¿åº¦åˆ†åˆ«ä¸º2724.9å’Œ476.3ä¸ªå•è¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒåŠç›‘ç£å­¦ä¹ ï¼ŒGOALè¿˜æä¾›äº†2160ä¸ªæœªæ ‡è®°çš„è¯„è®ºæ–‡æ¡£ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨GOALæ•°æ®é›†çš„åŸºç¡€ä¸Šï¼Œæ„å»ºå’Œè¯„ä¼°äº†å‡ ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬æå–å¼å’ŒæŠ½è±¡å¼åŸºçº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæå–å¼åŸºçº¿çš„æ€§èƒ½æœ‰é™ï¼Œå› ä¸ºè¯„è®ºå’Œæ–°é—»ä¹‹é—´çš„æ–‡æœ¬é£æ ¼ä¸åŒã€‚PGNæ¨¡å‹ä¼˜äºæå–å¼æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”Ÿæˆä¸å—åŸå§‹å•è¯æˆ–çŸ­è¯­é™åˆ¶çš„ä½“è‚²æ–°é—»ã€‚ç„¶è€Œï¼ŒLEDæ¨¡å‹åœ¨æ‰€æœ‰åŸºçº¿ä¸­è¡¨ç°æœ€ä½³ï¼Œå› ä¸ºå®ƒå…·æœ‰æŠ½è±¡æ€§è´¨å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GOALæ•°æ®é›†å’ŒåŸºçº¿æ¨¡å‹ä¸ºä½“è‚²æ¸¸æˆæ‘˜è¦ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†åˆ©ç”¨å¤–éƒ¨èµ„æºæ¥å¢å¼ºæ¨¡å‹å¯¹ä½“è‚²æ–‡æœ¬çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ¢ç´¢åŠç›‘ç£å’Œå¤šè¯­è¨€è®¾ç½®ï¼Œåˆ©ç”¨å›¾ç»“æ„æ¥å»ºæ¨¡è¯„è®ºä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å…¶ä»–é•¿æ–‡æ¡£æ‘˜è¦èµ„æºæ¥æ„å»ºå¤šé¢†åŸŸæˆ–è·¨é¢†åŸŸæ¨¡å‹ã€‚

## Meterstick Benchmarking Performance Variability in Cloud and Self-hosted Minecraft-like Games Extended Technical Report
### Abstract
Due to increasing popularity and strict performance requirements, online
games have become a workload of interest for the performance engineering
community. One of the most popular types of online games is the Minecraft-like
Game (MLG), in which players can terraform the environment. The most popular
MLG, Minecraft, provides not only entertainment, but also educational support
and social interaction, to over 130 million people world-wide. MLGs currently
support their many players by replicating isolated instances that support each
only up to a few hundred players under favorable conditions. In practice, as we
show here, the real upper limit of supported players can be much lower. In this
work, we posit that performance variability is a key cause for the lack of
scalability in MLGs. We propose a novel operational model for MLGs and use it
to design the first benchmark that focuses on MLG performance variability,
defining specialized workloads, metrics, and processes. We conduct real-world
benchmarking of MLGs and find environment-based workloads and cloud deployment
to be significant sources of performance variability: peak-latency degrades
sharply to 20.7 times the arithmetic mean, and exceeds by a factor of 7.4 the
performance requirements. We derive actionable insights for game-developers,
game-operators, and other stakeholders to tame performance variability.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Meterstickï¼šæ­ç§˜Minecraft-likeæ¸¸æˆæ€§èƒ½æ³¢åŠ¨ä¹‹è°œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº

éšç€åœ¨çº¿æ¸¸æˆè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå¯¹æ€§èƒ½çš„è¦æ±‚ä¹Ÿè¶Šæ¥è¶Šä¸¥æ ¼ï¼Œæ¸¸æˆæˆä¸ºäº†æ€§èƒ½å·¥ç¨‹é¢†åŸŸå…³æ³¨çš„é‡ç‚¹ã€‚Minecraft-likeæ¸¸æˆï¼ˆMLGï¼‰ä½œä¸ºå…¶ä¸­æœ€å—æ¬¢è¿çš„ç±»å‹ä¹‹ä¸€ï¼Œå…è®¸ç©å®¶æ”¹å˜æ¸¸æˆç¯å¢ƒï¼Œæ‹¥æœ‰åºå¤§çš„ç©å®¶ç¾¤ä½“ã€‚ç„¶è€Œï¼ŒMLGçš„å¯æ‰©å±•æ€§å´ä¸€ç›´å—é™ï¼Œå³ä½¿æ˜¯åƒMinecraftè¿™æ ·çš„çƒ­é—¨æ¸¸æˆï¼Œåœ¨æœ€ä½³æ¡ä»¶ä¸‹ä¹Ÿåªèƒ½æ”¯æŒå‡ ç™¾åç©å®¶ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œæ€§èƒ½æ³¢åŠ¨æ˜¯å¯¼è‡´MLGå¯æ‰©å±•æ€§å—é™çš„å…³é”®å› ç´ ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåä¸ºMeterstickçš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç”¨äºé‡åŒ–MLGçš„æ€§èƒ½æ³¢åŠ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºMLGæ“ä½œæ¨¡å‹

æœ¬æ–‡é¦–å…ˆæå‡ºäº†ä¸€ä¸ªMLGæ“ä½œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è€ƒè™‘äº†MLGç‰¹æœ‰çš„å·¥ä½œè´Ÿè½½ï¼Œä¾‹å¦‚ç©å®¶è¡Œä¸ºã€åœ°å½¢æ¨¡æ‹Ÿå’Œå®ä½“æ¨¡æ‹Ÿã€‚è¿™äº›å·¥ä½œè´Ÿè½½ä¸ä¼ ç»Ÿæ¸¸æˆä¸åŒï¼Œéœ€è¦ç‰¹æ®Šçš„å¤„ç†æ–¹å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡MeterstickåŸºå‡†æµ‹è¯•å·¥å…·

Meterstickæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLGæ€§èƒ½æ³¢åŠ¨çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒå®šä¹‰äº†ä¸“é—¨çš„å·¥ä½œè´Ÿè½½ã€æŒ‡æ ‡å’Œæµç¨‹ï¼Œå¹¶æ”¯æŒäº‘ç¯å¢ƒå’Œè‡ªæ‰˜ç®¡ç¯å¢ƒã€‚Meterstickä½¿ç”¨äº†ä¸€ä¸ªåä¸ºInstability Ratioï¼ˆISRï¼‰çš„æ–°æŒ‡æ ‡æ¥é‡åŒ–æ€§èƒ½æ³¢åŠ¨ï¼Œè¯¥æŒ‡æ ‡åŸºäºå‘¨æœŸæ€§æŠ–åŠ¨ï¼Œå¹¶è€ƒè™‘äº†æ—¶é—´æˆ³é¡ºåºã€å¼‚å¸¸å€¼å’Œé‡‡æ ·æŒç»­æ—¶é—´ç­‰å› ç´ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ

æœ¬æ–‡ä½¿ç”¨Meterstickå¯¹Minecraftã€Forgeå’ŒPaperMCä¸‰ç§MLGè¿›è¡Œäº†å®éªŒï¼Œå¹¶å‘ç°ï¼š

* **æ€§èƒ½æ³¢åŠ¨ä¼šå¯¼è‡´æ¸¸æˆä¸å¯ç©**ï¼šæœ€å¤§å“åº”æ—¶é—´å¯ä»¥è¾¾åˆ°å¹³å‡å€¼çš„20.7å€ï¼Œè¶…è¿‡å¯ç©æ€§é˜ˆå€¼çš„7.4å€ã€‚
* **åŸºäºç¯å¢ƒçš„å·¥ä½œè´Ÿè½½ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½æ³¢åŠ¨**ï¼šç¯å¢ƒå·¥ä½œè´Ÿè½½ä¼šå¯¼è‡´ISRæ˜¾è‘—å¢åŠ ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ¸¸æˆè¿‡è½½ç”šè‡³å´©æºƒã€‚
* **å•†ä¸šäº‘ç¯å¢ƒä¸­çš„æ€§èƒ½æ³¢åŠ¨æ¯”è‡ªæ‰˜ç®¡ç¯å¢ƒæ›´å¤§**ï¼šAWSå’ŒAzureäº‘ç¯å¢ƒä¸­çš„æ€§èƒ½æ³¢åŠ¨æ¯”DAS-5è¶…çº§è®¡ç®—æœºæ›´å¤§ã€‚
* **å¤„ç†å®ä½“çŠ¶æ€çš„è®¡ç®—æˆæœ¬å¾ˆé«˜**ï¼šå®ä½“ç›¸å…³çš„è®¡ç®—å ç”¨äº†å¤§éƒ¨åˆ†çš„è®¡ç®—æ—¶é—´å’ŒçŠ¶æ€æ›´æ–°æ¶ˆæ¯ã€‚
* **æ¨èçš„ç¡¬ä»¶é…ç½®ä¸è¶³ä»¥é¿å…æ€§èƒ½æ³¢åŠ¨**ï¼šæ¨èçš„äº‘ç¯å¢ƒç¡¬ä»¶é…ç½®ä¼šå¯¼è‡´æ€§èƒ½æ³¢åŠ¨ï¼Œè€Œæ›´å¼ºå¤§çš„ç¡¬ä»¶å¯ä»¥å°†å…¶é™åˆ¶åœ¨å¯æ¥å—çš„æ°´å¹³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„

* **æ¸¸æˆå¼€å‘è€…åº”æŠ¥å‘Šæ€§èƒ½æ³¢åŠ¨**ï¼šä½¿ç”¨ISRç­‰æŒ‡æ ‡æ¥è¯„ä¼°åœ¨çº¿æ¸¸æˆæ€§èƒ½ï¼Œå¹¶æä¾›ä¸€è‡´çš„è‰¯å¥½æ€§èƒ½ã€‚
* **æ¸¸æˆå¼€å‘è€…åº”å°†åŸºäºç¯å¢ƒçš„å·¥ä½œè´Ÿè½½çº³å…¥MLGåŸºå‡†æµ‹è¯•**ï¼šç¯å¢ƒå·¥ä½œè´Ÿè½½å¯¹MLGæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå¿…é¡»çº³å…¥åŸºå‡†æµ‹è¯•ã€‚
* **ç©å®¶åº”æ ¹æ®MLGé€‰æ‹©äº‘ç¯å¢ƒï¼Œå¹¶è€ƒè™‘è‡ªæ‰˜ç®¡**ï¼šä¸åŒMLGåœ¨ä¸åŒäº‘ç¯å¢ƒä¸­çš„æ€§èƒ½è¡¨ç°ä¸åŒï¼Œè‡ªæ‰˜ç®¡ä¹Ÿæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚
* **MLGæœåŠ¡æä¾›å•†åº”æé«˜ç¡¬ä»¶é…ç½®æ¨è**ï¼šæ¨èçš„äº‘ç¯å¢ƒç¡¬ä»¶é…ç½®ä¸è¶³ä»¥æ»¡è¶³MLGçš„æ€§èƒ½éœ€æ±‚ï¼Œéœ€è¦æ›´æ–°æ¨èé…ç½®ã€‚
* **æ¸¸æˆå¼€å‘è€…åº”ä¼˜åŒ–MLGä»¥å‡å°‘ç¯å¢ƒå·¥ä½œè´Ÿè½½çš„å½±å“**ï¼šé€šè¿‡æ€§èƒ½å·¥ç¨‹æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—é™ä½ç¯å¢ƒå·¥ä½œè´Ÿè½½å¯¹æ€§èƒ½çš„å½±å“ã€‚

### ğŸ¯ æœªæ¥å±•æœ›

æœ¬æ–‡æå‡ºçš„MeterstickåŸºå‡†æµ‹è¯•å·¥å…·å’ŒISRæŒ‡æ ‡ä¸ºMLGæ€§èƒ½è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶ISRä¸ç©å®¶æ„ŸçŸ¥è´¨é‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå…¬å…±æ’è¡Œæ¦œï¼Œæ–¹ä¾¿MLGæœåŠ¡æä¾›å•†å‘å¸ƒåŸºå‡†æµ‹è¯•åˆ†æ•°ã€‚

## Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game
### Abstract
Simulated DAG models may exhibit properties that, perhaps inadvertently,
render their structure identifiable and unexpectedly affect structure learning
algorithms. Here, we show that marginal variance tends to increase along the
causal order for generically sampled additive noise models. We introduce
varsortability as a measure of the agreement between the order of increasing
marginal variance and the causal order. For commonly sampled graphs and model
parameters, we show that the remarkable performance of some continuous
structure learning algorithms can be explained by high varsortability and
matched by a simple baseline method. Yet, this performance may not transfer to
real-world data where varsortability may be moderate or dependent on the choice
of measurement scales. On standardized data, the same algorithms fail to
identify the ground-truth DAG or its Markov equivalence class. While
standardization removes the pattern in marginal variance, we show that data
generating processes that incur high varsortability also leave a distinct
covariance pattern that may be exploited even after standardization. Our
findings challenge the significance of generic benchmarks with independently
drawn parameters. The code is available at
https://github.com/Scriddie/Varsortability.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å°å¿ƒæ¨¡æ‹Ÿçš„DAGï¼å› æœå‘ç°åŸºå‡†å¯èƒ½å®¹æ˜“è¢«æ“çºµ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å› æœç»“æ„å­¦ä¹ æ—¨åœ¨ä»æ•°æ®ä¸­æ¨æ–­å› æœæ¨¡å‹ã€‚åœ¨ç”Ÿç‰©å­¦ã€åŒ»å­¦ã€é‡‘èå’Œæœºå™¨å­¦ä¹ ç­‰é¢†åŸŸï¼Œå› æœæ¨¡å‹éƒ½å¤‡å—å…³æ³¨ã€‚å› æœæ¨¡å‹ä¸ä»…æè¿°äº†å˜é‡çš„è§‚æµ‹è”åˆåˆ†å¸ƒï¼Œè¿˜å½¢å¼åŒ–äº†å¹²é¢„å’Œåäº‹å®é¢„æµ‹ã€‚æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ˜¯è¡¨ç¤ºå› æœç»“æ„çš„å¸¸ç”¨æ–¹æ³•ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å˜é‡ï¼Œæœ‰å‘è¾¹è¡¨ç¤ºå› æœå…³ç³»ã€‚ç„¶è€Œï¼Œä»è§‚æµ‹æ•°æ®ä¸­æ¨æ–­å› æœç»“æ„æ˜¯å›°éš¾çš„ï¼Œé€šå¸¸åªèƒ½è¯†åˆ«DAGçš„é©¬å°”å¯å¤«ç­‰ä»·ç±»ï¼ˆMECï¼‰ï¼Œå¹¶ä¸”æ‰¾åˆ°é«˜åˆ†çš„DAGæ˜¯NP-hardé—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†çº¿æ€§åŠ æ€§å™ªå£°æ¨¡å‹ï¼ˆANMï¼‰çš„DAGå­¦ä¹ é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡å‘ç°ï¼Œå¯¹äºé€šç”¨é‡‡æ ·çš„åŠ æ€§å™ªå£°æ¨¡å‹ï¼Œè¾¹é™…æ–¹å·®å€¾å‘äºæ²¿ç€å› æœé¡ºåºå¢åŠ ã€‚ä¸ºäº†è¡¡é‡è¾¹é™…æ–¹å·®å¢åŠ é¡ºåºä¸å› æœé¡ºåºä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†â€œæ–¹å·®æ’åºæ€§â€ä½œä¸ºè¡¡é‡æŒ‡æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å‘ç°ï¼Œå¯¹äºå¸¸è§çš„é‡‡æ ·å›¾å’Œæ¨¡å‹å‚æ•°ï¼Œä¸€äº›è¿ç»­ç»“æ„å­¦ä¹ ç®—æ³•çš„å‡ºè‰²æ€§èƒ½å¯ä»¥é€šè¿‡é«˜æ–¹å·®æ’åºæ€§æ¥è§£é‡Šï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„åŸºçº¿æ–¹æ³•æ¥åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™ç§æ€§èƒ½å¯èƒ½ä¸ä¼šè½¬ç§»åˆ°ç°å®ä¸–ç•Œçš„æ•°æ®ä¸­ï¼Œå› ä¸ºæ–¹å·®æ’åºæ€§å¯èƒ½é€‚ä¸­æˆ–ä¾èµ–äºæµ‹é‡å°ºåº¦çš„é€‰æ‹©ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œsortnregressâ€çš„ç®€å•åŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æŒ‰è¾¹é™…æ–¹å·®æ’åºå˜é‡å¹¶ä½¿ç”¨ç¨€ç–å›å½’æŠ€æœ¯é€‰æ‹©çˆ¶èŠ‚ç‚¹æ¥å·¥ä½œã€‚sortnregressçš„æ€§èƒ½åæ˜ äº†ç»™å®šè®¾ç½®ä¸­æ–¹å·®æ’åºæ€§çš„ç¨‹åº¦ï¼Œå¹¶ä¸ºç»“æ„å­¦ä¹ ç®—æ³•æä¾›äº†ä¸€ä¸ªå‚è€ƒåŸºçº¿ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸€äº›ç®—æ³•åœ¨åŸå§‹æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ ‡å‡†åŒ–æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®ã€‚sortnregressåœ¨åŸå§‹æ•°æ®ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè€Œåœ¨æ ‡å‡†åŒ–æ•°æ®ä¸Šåˆ™è¡¨ç°å‡ºäº†åŸºçº¿æ€§èƒ½ã€‚è¿™è¡¨æ˜ï¼Œæ–¹å·®æ’åºæ€§åœ¨ç»“æ„å­¦ä¹ ä»»åŠ¡ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿ç»­ç»“æ„å­¦ä¹ ç®—æ³•å¯¹æ•°æ®ç¼©æ”¾éå¸¸æ•æ„Ÿï¼Œå¹¶ä¸”åœ¨ä¸çŸ¥é“çœŸå®æ•°æ®å°ºåº¦çš„æƒ…å†µä¸‹å¯èƒ½æ— æ³•å¾ˆå¥½åœ°å·¥ä½œã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨å’Œè¯„ä¼°è¿™äº›ç®—æ³•æ—¶ï¼Œéœ€è¦è°¨æ…è€ƒè™‘æ•°æ®å°ºåº¦çš„å½±å“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹å·®æ’åºæ€§æ¦‚å¿µå’ŒsortnregressåŸºçº¿æ–¹æ³•å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒç»“æ„å­¦ä¹ ç®—æ³•åœ¨ä¸åŒåŸºå‡†åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚

## OpenHoldem A Benchmark for Large-Scale Imperfect-Information Game Research
### Abstract
Owning to the unremitting efforts by a few institutes, significant progress
has recently been made in designing superhuman AIs in No-limit Texas Hold'em
(NLTH), the primary testbed for large-scale imperfect-information game
research. However, it remains challenging for new researchers to study this
problem since there are no standard benchmarks for comparing with existing
methods, which seriously hinders further developments in this research area. In
this work, we present OpenHoldem, an integrated toolkit for large-scale
imperfect-information game research using NLTH. OpenHoldem makes three main
contributions to this research direction: 1) a standardized evaluation protocol
for thoroughly evaluating different NLTH AIs, 2) four publicly available strong
baselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs
for public NLTH AI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,
hoping it facilitates further studies on the unsolved theoretical and
computational issues in this area and cultivate crucial research problems like
opponent modeling and human-computer interactive learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | OpenHoldemï¼šå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶çš„åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œåœ¨æ— é™åˆ¶å¾·å·æ‰‘å…‹ï¼ˆNLTHï¼‰é¢†åŸŸï¼Œè®¾è®¡å‡ºè¶…è¶Šäººç±»çš„AIå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒNLTHå·²æˆä¸ºå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶çš„ä¸»è¦æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŸºå‡†ï¼Œæ–°ç ”ç©¶äººå‘˜éš¾ä»¥ç ”ç©¶æ­¤é—®é¢˜ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†è¯¥ç ”ç©¶é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ‡å‡†åŒ–è¯„ä¼°åè®®
OpenHoldem æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬å››ç§ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºå…¨é¢è¯„ä¼°ä¸åŒçš„ NLTH AIã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬ï¼š
- **å¯¹å±€è¯„ä¼°æŒ‡æ ‡**ï¼šé€šè¿‡é‡å¤å¯¹å±€ï¼Œè¯„ä¼° AI çš„å¹³å‡æ•ˆç”¨ï¼Œå¹¶ä½¿ç”¨æ–¹å·®å‡å°‘æŠ€æœ¯ï¼ˆå¦‚å¤åˆ¶æ‰‘å…‹å’Œ AIVATï¼‰æ¥å‡å°‘éšæœºæ€§å½±å“ã€‚
- **å¯åˆ©ç”¨æ€§è¯„ä¼°æŒ‡æ ‡**ï¼šé€šè¿‡è®¡ç®—æœ€ä½³å“åº”ç­–ç•¥ï¼Œè¯„ä¼° AI çš„å¯åˆ©ç”¨æ€§ï¼Œå¹¶ä½¿ç”¨å±€éƒ¨æœ€ä½³å“åº”ï¼ˆLBRï¼‰å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRL-BRï¼‰æ¥è¿‘ä¼¼è®¡ç®—ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¬å¼€å¯ç”¨çš„å¼ºåŸºçº¿ AI
OpenHoldem è®¾è®¡å¹¶å®ç°äº†å››ç§ä¸åŒç±»å‹çš„ NLTH AIï¼Œä½œä¸ºæœªæ¥ç ”ç©¶çš„è‰¯å¥½èµ·ç‚¹ï¼š
- **åŸºäºè§„åˆ™çš„ AI**ï¼šç”±é¢†åŸŸä¸“å®¶è®¾è®¡çš„è§„åˆ™é›†åˆï¼Œç”¨äºå¤„ç†å„ç§åœºæ™¯ã€‚
- **åŸºäº CFR çš„é™æ€ AI**ï¼šä½¿ç”¨ CFR ç®—æ³•è¿‘ä¼¼æ±‚è§£çº³ä»€å‡è¡¡ç­–ç•¥ï¼Œå¹¶é€šè¿‡ä¿¡æ¯æŠ½è±¡å’ŒåŠ¨ä½œæŠ½è±¡æ¥é™ä½æ¸¸æˆè§„æ¨¡ã€‚
- **ç±»ä¼¼ DeepStack çš„åœ¨çº¿ AI**ï¼šä½¿ç”¨æŒç»­é‡è§£å’Œæ·±åº¦ç¥ç»ç½‘ç»œæ¥å¤„ç†ç¦»æ ‘åŠ¨ä½œï¼Œå¹¶æé«˜å†³ç­–æ•ˆç‡ã€‚
- **åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ AI**ï¼šä½¿ç”¨ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ä»æ¸¸æˆçŠ¶æ€å­¦ä¹ åˆ°åŠ¨ä½œï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡ç‰¹å¾æˆ–è¿›è¡Œè¿­ä»£æ¨ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåœ¨çº¿æµ‹è¯•å¹³å°
OpenHoldem å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æµ‹è¯•å¹³å°ï¼Œå†…ç½®äº†å››ç§å¼ºåŸºçº¿ AIï¼Œå¹¶æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜æµ‹è¯•å’Œæ¯”è¾ƒä»–ä»¬çš„ AIã€‚

### ğŸ“ˆ å®éªŒç»“æœ
OpenHoldem çš„åŸºçº¿ AI åœ¨ä¸ç°æœ‰å…¬å¼€å¯ç”¨çš„ NLTH AI çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒåˆ†æäº†ä¸åŒè®¾è®¡é€‰æ‹©å¯¹ AI æ€§èƒ½çš„å½±å“ï¼Œå¹¶éªŒè¯äº† OpenHoldem çš„å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
OpenHoldem ä¸ºå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶æä¾›äº†ä¸€ä¸ªå®è´µçš„å·¥å…·ï¼Œå…¶æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€å¼ºåŸºçº¿ AI å’Œåœ¨çº¿æµ‹è¯•å¹³å°å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚OpenHoldem çš„è®¾è®¡æ€è·¯å’Œå®ç°æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–æ¸¸æˆ AI ç ”ç©¶æä¾›äº†å‚è€ƒã€‚

## Towards Game-Playing AI Benchmarks via Performance Reporting Standards
### Abstract
While games have been used extensively as milestones to evaluate game-playing
AI, there exists no standardised framework for reporting the obtained
observations. As a result, it remains difficult to draw general conclusions
about the strengths and weaknesses of different game-playing AI algorithms. In
this paper, we propose reporting guidelines for AI game-playing performance
that, if followed, provide information suitable for unbiased comparisons
between different AI approaches. The vision we describe is to build benchmarks
and competitions based on such guidelines in order to be able to draw more
general conclusions about the behaviour of different AI algorithms, as well as
the types of challenges different games pose.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆAIæ€§èƒ½è¯„ä¼°ï¼šè¿ˆå‘æ ‡å‡†åŒ–æŠ¥å‘Šæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ¸¸æˆä¸€ç›´æ˜¯è¯„ä¼°æ¸¸æˆAIçš„é‡è¦é‡Œç¨‹ç¢‘ï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–çš„æŠ¥å‘Šæ¡†æ¶ï¼Œå¯¼è‡´éš¾ä»¥å¯¹ä¸åŒçš„æ¸¸æˆAIç®—æ³•è¿›è¡Œå…¬æ­£æ¯”è¾ƒã€‚æœ¬æ–‡æ—¨åœ¨æå‡ºä¸€å¥—æŠ¥å‘ŠæŒ‡å—ï¼Œä»¥ä¿ƒè¿›ä¸åŒAIæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒï¼Œå¹¶æ„å»ºåŸºäºè¿™äº›æŒ‡å—çš„åŸºå‡†æµ‹è¯•å’Œç«èµ›ï¼Œä»è€Œæ›´å…¨é¢åœ°äº†è§£ä¸åŒAIç®—æ³•çš„è¡Œä¸ºå’Œä¸åŒæ¸¸æˆå¸¦æ¥çš„æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºæ¸¸æˆAIæ€§èƒ½æŠ¥å‘ŠæŒ‡å—
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæŠ¥å‘ŠæŒ‡å—ï¼Œæ—¨åœ¨ä¸ºæ¸¸æˆAIæ€§èƒ½æŠ¥å‘Šæä¾›ä¸€ä¸ªç»“æ„åŒ–çš„æ¡†æ¶ã€‚è¯¥æŒ‡å—åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š
1. **é—®é¢˜æè¿°**ï¼šè¯¦ç»†æè¿°æµ‹è¯•é—®é¢˜çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç±»å‹ã€æ¸¸æˆå†…æ€§èƒ½æŒ‡æ ‡ã€æ¸¸æˆ-AIäº¤äº’æ–¹å¼å’Œçº¦æŸæ¡ä»¶ã€‚
2. **è§£å†³æ–¹æ¡ˆå¤æ‚æ€§æè¿°**ï¼šæè¿°AIè§£å†³æ–¹æ¡ˆçš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ã€æ“ä½œç³»ç»Ÿã€è½¯ä»¶ã€è®¡ç®—å¤æ‚æ€§ã€æ¨¡å‹å¤æ‚æ€§å’Œå®ç°å¤æ‚æ€§ã€‚
3. **æ€§èƒ½é‡åŒ–**ï¼šæè¿°æ€§èƒ½æµ‹é‡çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç»Ÿè®¡æ˜¾è‘—æ€§è¯„ä¼°ã€å¯¹æ‰‹ç±»å‹ã€æµ‹è¯•æ¡ˆä¾‹çš„å¤šæ ·æ€§å’Œæ€§èƒ½æŒ‡æ ‡çš„èšåˆæ–¹å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå¯è§£é‡Šçš„åŸºå‡†æµ‹è¯•
æœ¬æ–‡æå‡ºäº†æ„å»ºå¯è§£é‡ŠåŸºå‡†æµ‹è¯•çš„æ„¿æ™¯ï¼Œå¹¶æå‡ºäº†ä»¥ä¸‹è¦æ±‚ï¼š
- äº§ç”Ÿå¯æµ‹é‡çš„ç»“æœ
- å…è®¸å¾—å‡ºæœ‰æ„ä¹‰çš„ç»“è®º
- å¯è§£é‡Šæ€§
- äº§ç”Ÿå¯æ¨å¹¿çš„ç»“è®º
- å¯é‡å¤æ€§
- äº§ç”Ÿç¨³å¥çš„ç»“æœ
- å…è®¸æ¯”è¾ƒ
- å®ç”¨æ€§
- å…¬å¼€åè§
- è€ƒè™‘è§£å†³æ–¹æ¡ˆå¤æ‚æ€§

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å°šæœªè¿›è¡Œå®éªŒï¼Œä½†æå‡ºäº†ä¸€ä¸ªæŠ¥å‘ŠæŒ‡å—ï¼Œæ—¨åœ¨ä¿ƒè¿›æ¸¸æˆAIæ€§èƒ½è¯„ä¼°çš„æ ‡å‡†åŒ–å’Œå¯è§£é‡Šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æŠ¥å‘ŠæŒ‡å—å’ŒåŸºå‡†æµ‹è¯•æ¡†æ¶å¯¹äºæ¸¸æˆAIç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚é€šè¿‡éµå¾ªè¿™äº›æŒ‡å—ï¼Œå¯ä»¥æ›´å…¨é¢åœ°äº†è§£ä¸åŒAIç®—æ³•çš„è¡Œä¸ºå’Œæ€§èƒ½ï¼Œå¹¶ä¿ƒè¿›æ¸¸æˆAIé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## Interbank lending with benchmark rates Pareto optima for a class of singular control games
### Abstract
We analyze a class of stochastic differential games of singular control,
motivated by the study of a dynamic model of interbank lending with benchmark
rates. We describe Pareto optima for this game and show how they may be
achieved through the intervention of a regulator, whose policy is a solution to
a singular stochastic control problem. Pareto optima are characterized in terms
of the solutions to a new class of Skorokhod problems with piecewise-continuous
free boundary.
  Pareto optimal policies are shown to correspond to the enforcement of
endogenous bounds on interbank lending rates. Analytical comparison between
Pareto optima and Nash equilibria provides insight into the impact of
regulatory intervention on the stability of interbank rates.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºå‡†åˆ©ç‡ä¸‹çš„é“¶è¡Œé—´å€Ÿè´·ï¼šä¸€ç±»å¥‡å¼‚æ§åˆ¶åšå¼ˆçš„å¸•ç´¯æ‰˜æœ€ä¼˜è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é“¶è¡Œé—´å€Ÿè´·å¸‚åœºæ˜¯é‡‘èæœºæ„ä¹‹é—´æˆ˜ç•¥äº’åŠ¨çš„æœ‰è¶£ä¾‹å­ï¼Œå‚ä¸è€…ä¼šæ ¹æ®å…¶ä»–å‚ä¸è€…çš„è¡ŒåŠ¨åˆ†å¸ƒåšå‡ºååº”ã€‚é“¶è¡Œé—´åŸºå‡†åˆ©ç‡çš„è®¾å®šæœºåˆ¶ï¼Œå¦‚ä¼¦æ•¦é“¶è¡ŒåŒä¸šæ‹†å€Ÿåˆ©ç‡ï¼ˆLIBORï¼‰ï¼Œåœ¨é‡‘èå¸‚åœºä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ã€‚ç„¶è€Œï¼Œè¿™ç§æœºåˆ¶å®¹æ˜“å—åˆ°æ“çºµï¼Œå¯¼è‡´å¸‚åœºä¸ç¨³å®šã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶é“¶è¡Œé—´å€Ÿè´·å¸‚åœºä¸­ï¼Œç›‘ç®¡è€…å¦‚ä½•é€šè¿‡å¹²é¢„æ¥è¾¾åˆ°å¸•ç´¯æ‰˜æœ€ä¼˜è§£ï¼Œä»è€Œæé«˜å¸‚åœºç¨³å®šæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«åŸºå‡†åˆ©ç‡çš„é“¶è¡Œé—´å€Ÿè´·åŠ¨æ€æ¨¡å‹ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªå¥‡å¼‚æ§åˆ¶åšå¼ˆé—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡é€šè¿‡ç ”ç©¶ç›‘ç®¡è€…çš„è¾…åŠ©æ§åˆ¶é—®é¢˜ï¼Œå¾—åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜è§£çš„åˆ»ç”»ï¼Œå¹¶å°†å…¶ä¸Nashå‡è¡¡è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ­ç¤ºäº†ç›‘ç®¡å¹²é¢„å¯¹é“¶è¡Œé—´åˆ©ç‡ç¨³å®šæ€§çš„å½±å“ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæœ¬æ–‡å°†å¸•ç´¯æ‰˜æœ€ä¼˜è§£æè¿°ä¸ºä¸€ç³»åˆ—å…·æœ‰åˆ†æ®µè¿ç»­è‡ªç”±è¾¹ç•Œçš„Skorokhodé—®é¢˜ï¼Œå¹¶é€šè¿‡æ±‚è§£è¿™äº›é—®é¢˜å¾—åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥çš„æ˜¾å¼è¡¨è¾¾å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨N=2çš„æƒ…å†µä¸‹å¾—åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥çš„æ˜¾å¼è§£ï¼Œå¹¶ä¸Nashå‡è¡¡è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œå¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆé™ä½é“¶è¡Œé—´åˆ©ç‡çš„æ³¢åŠ¨æ€§ï¼Œæé«˜å¸‚åœºç¨³å®šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºé“¶è¡Œé—´å€Ÿè´·å¸‚åœºçš„ç›‘ç®¡æä¾›äº†ç†è®ºä¾æ®ï¼Œå¹¶ä¸ºè®¾è®¡æœ‰æ•ˆçš„ç›‘ç®¡æ”¿ç­–æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¥‡å¼‚æ§åˆ¶åšå¼ˆæ¨¡å‹å’ŒSkorokhodé—®é¢˜çš„æ±‚è§£æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºå…¶ä»–é‡‘èå¸‚åœºçš„å»ºæ¨¡å’Œåˆ†æã€‚

## Benchmarking End-to-End Behavioural Cloning on Video Games
### Abstract
Behavioural cloning, where a computer is taught to perform a task based on
demonstrations, has been successfully applied to various video games and
robotics tasks, with and without reinforcement learning. This also includes
end-to-end approaches, where a computer plays a video game like humans do: by
looking at the image displayed on the screen, and sending keystrokes to the
game. As a general approach to playing video games, this has many inviting
properties: no need for specialized modifications to the game, no lengthy
training sessions and the ability to re-use the same tools across different
games. However, related work includes game-specific engineering to achieve the
results. We take a step towards a general approach and study the general
applicability of behavioural cloning on twelve video games, including six
modern video games (published after 2010), by using human demonstrations as
training data. Our results show that these agents cannot match humans in raw
performance but do learn basic dynamics and rules. We also demonstrate how the
quality of the data matters, and how recording data from humans is subject to a
state-action mismatch, due to human reflexes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¡Œä¸ºå…‹éš†æ˜¯ä¸€ç§åŸºäºäººç±»æ¼”ç¤ºæ¥è®­ç»ƒè®¡ç®—æœºæ‰§è¡Œä»»åŠ¡çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚å®ƒå·²è¢«æˆåŠŸåº”ç”¨äºå„ç§è§†é¢‘æ¸¸æˆå’Œæœºå™¨äººä»»åŠ¡ï¼ŒåŒ…æ‹¬ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå…¶ä¸­è®¡ç®—æœºé€šè¿‡è§‚å¯Ÿå±å¹•ä¸Šçš„å›¾åƒå¹¶å‘é€æŒ‰é”®æ¥åƒäººç±»ä¸€æ ·ç©æ¸¸æˆã€‚è¿™ç§æ–¹æ³•å…·æœ‰è®¸å¤šå¸å¼•äººçš„ç‰¹æ€§ï¼Œä¾‹å¦‚æ— éœ€å¯¹æ¸¸æˆè¿›è¡Œç‰¹æ®Šä¿®æ”¹ã€æ— éœ€é•¿æ—¶é—´è®­ç»ƒä»¥åŠèƒ½å¤Ÿåœ¨ä¸åŒæ¸¸æˆä¹‹é—´é‡ç”¨ç›¸åŒå·¥å…·ã€‚ç„¶è€Œï¼Œç›¸å…³å·¥ä½œé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¸¸æˆè¿›è¡Œå·¥ç¨‹åŒ–æ‰èƒ½å–å¾—æˆæœã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶è¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„é€šç”¨æ€§ï¼Œå¹¶ä½¿ç”¨äººç±»æ¼”ç¤ºä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œåœ¨åäºŒæ¬¾è§†é¢‘æ¸¸æˆï¼ˆåŒ…æ‹¬å…­æ¬¾ç°ä»£è§†é¢‘æ¸¸æˆï¼‰ä¸Šè¿›è¡Œäº†ç ”ç©¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å­¦ä¹ äººç±»ç©å®¶çš„è¡Œä¸ºï¼Œå¹¶ä½¿ç”¨äººç±»æ¼”ç¤ºæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æ¨¡å‹é€šè¿‡è§‚å¯Ÿå±å¹•å›¾åƒæ¥é¢„æµ‹äººç±»ç©å®¶çš„åŠ¨ä½œï¼Œå¹¶ä½¿ç”¨è¿™äº›é¢„æµ‹æ¥æ§åˆ¶æ¸¸æˆã€‚æœ¬æ–‡è¿˜ç ”ç©¶äº†æ•°æ®è´¨é‡å’Œæ•°é‡å¯¹è¡Œä¸ºå…‹éš†æ€§èƒ½çš„å½±å“ï¼Œä»¥åŠäººç±»ååº”æ—¶é—´å»¶è¿Ÿå¯¹æ•°æ®è´¨é‡çš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œè¡Œä¸ºå…‹éš†ä»£ç†å¯ä»¥å­¦ä¹ æ¸¸æˆçš„åŸºæœ¬åŠ¨æ€å’Œè§„åˆ™ï¼Œä½†é€šå¸¸åªèƒ½è¾¾åˆ°äººç±»ç©å®¶æ€§èƒ½çš„ä¸€å°éƒ¨åˆ†ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¦‚éšæœºä»£ç†ã€‚æœ¬æ–‡è¿˜å‘ç°ï¼Œå½“åªæœ‰å°‘é‡æ•°æ®å¯ç”¨æ—¶ï¼Œæ•°æ®æ•°é‡å¯¹ç»“æœçš„å½±å“è¾ƒå°ï¼Œè€Œè°ƒæ•´äººç±»ååº”æ—¶é—´å»¶è¿Ÿå¯ä»¥æé«˜æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨å…·æœ‰æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºçš„å»ºè®®åŒ…æ‹¬ä½¿ç”¨é«˜è´¨é‡æ•°æ®ã€è°ƒæ•´äººç±»ååº”æ—¶é—´å»¶è¿Ÿä»¥åŠæ¢ç´¢å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜è¡Œä¸ºå…‹éš†çš„æ€§èƒ½ã€‚

## Design, Benchmarking and Explainability Analysis of a Game-Theoretic Framework towards Energy Efficiency in Smart Infrastructure
### Abstract
In this paper, we propose a gamification approach as a novel framework for
smart building infrastructure with the goal of motivating human occupants to
reconsider personal energy usage and to have positive effects on their
environment. Human interaction in the context of cyber-physical systems is a
core component and consideration in the implementation of any smart building
technology. Research has shown that the adoption of human-centric building
services and amenities leads to improvements in the operational efficiency of
these cyber-physical systems directed towards controlling building energy
usage. We introduce a strategy in form of a game-theoretic framework that
incorporates humans-in-the-loop modeling by creating an interface to allow
building managers to interact with occupants and potentially incentivize energy
efficient behavior. Prior works on game theoretic analysis typically rely on
the assumption that the utility function of each individual agent is known a
priori. Instead, we propose novel utility learning framework for benchmarking
that employs robust estimations of occupant actions towards energy efficiency.
To improve forecasting performance, we extend the utility learning scheme by
leveraging deep bi-directional recurrent neural networks. Using the proposed
methods on data gathered from occupant actions for resources such as room
lighting, we forecast patterns of energy resource usage to demonstrate the
prediction performance of the methods. The results of our study show that we
can achieve a highly accurate representation of the ground truth for occupant
energy resource usage. We also demonstrate the explainable nature on human
decision making towards energy usage inherent in the dataset using graphical
lasso and granger causality algorithms. Finally, we open source the
de-identified, high-dimensional data pertaining to the energy game-theoretic
framework.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆåŒ–æ™ºèƒ½å»ºç­‘ï¼šæ¿€åŠ±èŠ‚èƒ½è¡Œä¸ºçš„æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å…¨çƒèƒ½æºæ¶ˆè€—çš„æŒç»­å¢é•¿ï¼Œå»ºç­‘èƒ½è€—å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„å…³æ³¨ç‚¹ã€‚å°½ç®¡æ™ºèƒ½å»ºç­‘æŠ€æœ¯æ—¨åœ¨æé«˜èƒ½æºæ•ˆç‡ï¼Œä½†äººç±»è¡Œä¸ºçš„ä¸ç¡®å®šæ€§å¾€å¾€å¯¼è‡´èƒ½æºæ¶ˆè€—çš„ä¸å¯é¢„æµ‹æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåšå¼ˆè®ºçš„æ¸¸æˆåŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æ¿€åŠ±å»ºç­‘ä½¿ç”¨è€…é‡æ–°è€ƒè™‘ä¸ªäººèƒ½æºä½¿ç”¨ï¼Œä»è€Œå¯¹ç¯å¢ƒäº§ç”Ÿç§¯æå½±å“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåšå¼ˆè®ºæ¡†æ¶
æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåšå¼ˆè®ºæ¡†æ¶ï¼Œå°†å»ºç­‘ä½¿ç”¨è€…è§†ä¸ºéåˆä½œåšå¼ˆä¸­çš„æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ›å»ºä¸€ä¸ªç•Œé¢ï¼Œå…è®¸å»ºç­‘ç®¡ç†è€…ä¸ä½¿ç”¨è€…äº’åŠ¨ï¼Œå¹¶å¯èƒ½æ¿€åŠ±ä»–ä»¬é‡‡å–èŠ‚èƒ½è¡Œä¸ºã€‚ä¸ä»¥å¾€åšå¼ˆè®ºåˆ†æé€šå¸¸å‡è®¾æ¯ä¸ªæ™ºèƒ½ä½“çš„æ•ˆç”¨å‡½æ•°æ˜¯é¢„å…ˆå·²çŸ¥çš„ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•ˆç”¨å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåŸºå‡†æµ‹è¯•ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¯¹èŠ‚èƒ½è¡Œä¸ºçš„ occupant è¡ŒåŠ¨è¿›è¡Œé²æ£’ä¼°è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ·±åº¦åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ
ä¸ºäº†æé«˜é¢„æµ‹æ€§èƒ½ï¼Œæœ¬æ–‡æ‰©å±•äº†æ•ˆç”¨å­¦ä¹ æ–¹æ¡ˆï¼Œåˆ©ç”¨æ·±åº¦åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€‚ä½¿ç”¨ä» occupant è¡ŒåŠ¨ä¸­æ”¶é›†çš„æ•°æ®ï¼Œä¾‹å¦‚æˆ¿é—´ç…§æ˜ï¼Œé¢„æµ‹èƒ½æºèµ„æºä½¿ç”¨æ¨¡å¼ï¼Œä»¥å±•ç¤ºæ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå¯ä»¥å®ç° occupant èƒ½æºèµ„æºä½¿ç”¨çš„é«˜åº¦å‡†ç¡®è¡¨ç¤ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¯è§£é‡Šæ€§åˆ†æ
æœ¬æ–‡ä½¿ç”¨å›¾å½¢æ‹‰ç´¢å’Œæ ¼å…°æ°å› æœç®—æ³•ï¼Œå±•ç¤ºäº†æ•°æ®é›†ä¸­å›ºæœ‰çš„å¯¹äººç±»å†³ç­–åˆ¶å®šçš„å¯è§£é‡Šæ€§ã€‚è¿™äº›ç®—æ³•æ­ç¤ºäº†ä¸åŒç‰¹å¾ä¹‹é—´çš„æ½œåœ¨æ¡ä»¶ä¾èµ–å…³ç³»ï¼Œä»¥åŠç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ï¼Œä»è€ŒåŠ æ·±äº†å¯¹äººç±»èƒ½æºä½¿ç”¨å†³ç­–åˆ¶å®šçš„ç†è§£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ•ˆç”¨å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ occupant çš„èƒ½æºä½¿ç”¨è¡Œä¸ºï¼Œå¹¶ä¸”æ·±åº¦åŒå‘ RNN æ¨¡å‹åœ¨é¢„æµ‹æ€§èƒ½æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¯è§£é‡Šæ€§åˆ†ææ­ç¤ºäº†ä¸åŒèƒ½æºæ•ˆç‡è¡Œä¸ºç±»åˆ«ä¸­ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠç‰¹å¾ä¹‹é—´çš„å› æœå…³ç³»ï¼Œä»è€ŒåŠ æ·±äº†å¯¹äººç±»èƒ½æºä½¿ç”¨å†³ç­–åˆ¶å®šçš„ç†è§£ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŒ–æ™ºèƒ½å»ºç­‘æ¡†æ¶ä¸ºæé«˜å»ºç­‘èƒ½æºæ•ˆç‡æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¿€åŠ± occupant é‡‡å–èŠ‚èƒ½è¡Œä¸ºï¼Œå¹¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯é¢„æµ‹ occupant çš„èƒ½æºä½¿ç”¨è¡Œä¸ºï¼Œä»è€Œä¸ºæ™ºèƒ½å»ºç­‘ç®¡ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¯è§£é‡Šæ€§åˆ†ææ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£äººç±»èƒ½æºä½¿ç”¨å†³ç­–åˆ¶å®šï¼Œå¹¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„æ¿€åŠ±æªæ–½æä¾›ä¾æ®ã€‚

## Quantum Poker A game for quantum computers suitable for benchmarking error mitigation techniques on NISQ devices
### Abstract
Quantum computers are on the verge of becoming a commercially available
reality. They represent a paradigm shift in computing, with a steep learning
gradient. The creation of games is a way to ease the transition for beginners.
We present a game similar to the Poker variant Texas hold 'em with the
intention to serve as an engaging pedagogical tool to learn the basics rules of
quantum computing. The concepts of quantum states, quantum operations and
measurement can be learned in a playful manner. The difference to the classical
variant is that the community cards are replaced by a quantum register that is
"randomly" initialized, and the cards for each player are replaced by quantum
gates, randomly drawn from a set of available gates. Each player can create a
quantum circuit with their cards, with the aim to maximize the number of $1$'s
that are measured in the computational basis. The basic concepts of
superposition, entanglement and quantum gates are employed. We provide a
proof-of-concept implementation using Qiskit. A comparison of the results for
the created circuits using a simulator and IBM machines is conducted, showing
that error rates on contemporary quantum computers are still very high. For the
success of noisy intermediate scale quantum (NISQ) computers, improvements on
the error rates and error mitigation techniques are necessary, even for simple
circuits. We show that quantum error mitigation (QEM) techniques can be used to
improve expectation values of observables on real quantum devices.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é‡å­æ‰‘å…‹ï¼šåœ¨NISQè®¾å¤‡ä¸Šæµ‹è¯•é”™è¯¯ç¼“è§£æŠ€æœ¯çš„æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é‡å­è®¡ç®—æœºæ­£é€æ¸æˆä¸ºå•†ä¸šç°å®ï¼Œä½†å®ƒä»¬çš„å­¦ä¹ æ›²çº¿é™¡å³­ï¼Œéœ€è¦è·¨å­¦ç§‘çš„çŸ¥è¯†ã€‚ä¸ºäº†å¸®åŠ©åˆå­¦è€…æ›´å®¹æ˜“åœ°å­¦ä¹ é‡å­è®¡ç®—çš„åŸºæœ¬è§„åˆ™ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œé‡å­æ‰‘å…‹â€çš„æ¸¸æˆã€‚è¯¥æ¸¸æˆæ—¨åœ¨é€šè¿‡æ¸¸æˆåŒ–çš„æ–¹å¼æ•™æˆé‡å­çŠ¶æ€ã€é‡å­æ“ä½œå’Œæµ‹é‡çš„æ¦‚å¿µã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé‡å­æ‰‘å…‹æ¸¸æˆ
é‡å­æ‰‘å…‹æ¸¸æˆç±»ä¼¼äºå¾·å·æ‰‘å…‹ï¼Œä½†ç¤¾åŒºç‰Œè¢«é‡å­å¯„å­˜å™¨å–ä»£ï¼Œç©å®¶æ‰‹ä¸­çš„ç‰Œè¢«é‡å­é—¨å–ä»£ã€‚æ¯ä¸ªç©å®¶éƒ½å¯ä»¥ä½¿ç”¨ä»–ä»¬çš„ç‰Œåˆ›å»ºä¸€ä¸ªé‡å­ç”µè·¯ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–åœ¨è®¡ç®—åŸºä¸­æµ‹é‡åˆ°çš„â€œ1â€çš„æ•°é‡ã€‚æ¸¸æˆåˆ©ç”¨äº†å åŠ ã€çº ç¼ å’Œé‡å­é—¨çš„åŸºæœ¬æ¦‚å¿µã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé”™è¯¯ç¼“è§£æŠ€æœ¯
æœ¬æ–‡ä½¿ç”¨é›¶å™ªå£°å¤–æ¨æ–¹æ³•æ¥ç¼“è§£é‡å­è®¡ç®—æœºä¸Šçš„é”™è¯¯ã€‚é€šè¿‡æ•…æ„æ”¾å¤§å™ªå£°å¹¶ä½¿ç”¨å¤–æ¨æŠ€æœ¯ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ä¼°è®¡å¯è§‚æµ‹é‡åœ¨çœŸå®é‡å­è®¾å¤‡ä¸Šçš„æœŸæœ›å€¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ç®€å•çš„ç”µè·¯ä¸­ï¼Œå½“ä»£é‡å­è®¡ç®—æœºçš„é”™è¯¯ç‡ä»ç„¶å¾ˆé«˜ã€‚ä½¿ç”¨é”™è¯¯ç¼“è§£æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æœŸæœ›å€¼çš„ä¼°è®¡ç²¾åº¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
é‡å­æ‰‘å…‹æ¸¸æˆå¯ä»¥ä½œä¸ºå­¦ä¹ é‡å­è®¡ç®—åŸºæœ¬æ¦‚å¿µçš„æœ‰ç›Šå·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„é”™è¯¯ç¼“è§£æŠ€æœ¯å¯ä»¥åº”ç”¨äºå…¶ä»–é‡å­è®¡ç®—ä»»åŠ¡ï¼Œä»¥æé«˜ç»“æœçš„å‡†ç¡®æ€§ã€‚

## Correlation in Extensive-Form Games Saddle-Point Formulation and Benchmarks
### Abstract
While Nash equilibrium in extensive-form games is well understood, very
little is known about the properties of extensive-form correlated equilibrium
(EFCE), both from a behavioral and from a computational point of view. In this
setting, the strategic behavior of players is complemented by an external
device that privately recommends moves to agents as the game progresses;
players are free to deviate at any time, but will then not receive future
recommendations. Our contributions are threefold. First, we show that an EFCE
can be formulated as the solution to a bilinear saddle-point problem. To
showcase how this novel formulation can inspire new algorithms to compute
EFCEs, we propose a simple subgradient descent method which exploits this
formulation and structural properties of EFCEs. Our method has better
scalability than the prior approach based on linear programming. Second, we
propose two benchmark games, which we hope will serve as the basis for future
evaluation of EFCE solvers. These games were chosen so as to cover two natural
application domains for EFCE: conflict resolution via a mediator, and
bargaining and negotiation. Third, we document the qualitative behavior of EFCE
in our proposed games. We show that the social-welfare-maximizing equilibria in
these games are highly nontrivial and exhibit surprisingly subtle sequential
behavior that so far has not received attention in the literature.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±å…¥ç†è§£æ‰©å±•å¼æ¸¸æˆä¸­çš„ç›¸å…³å‡è¡¡ï¼šéç‚¹å…¬å¼ä¸åŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ‰©å±•å¼æ¸¸æˆï¼ˆEFGsï¼‰æ˜¯åšå¼ˆè®ºä¸­çš„ä¸€ç§é‡è¦ç±»å‹ï¼Œå®ƒæè¿°äº†ç©å®¶åœ¨ä¸€ç³»åˆ—å†³ç­–ç‚¹ä¸Šçš„äº¤äº’è¿‡ç¨‹ã€‚åœ¨æ‰©å±•å¼æ¸¸æˆä¸­ï¼Œçº³ä»€å‡è¡¡ï¼ˆNEï¼‰æ˜¯ä¸€ä¸ªè¢«å¹¿æ³›ç ”ç©¶çš„è§£æ¦‚å¿µï¼Œå®ƒå‡è®¾æ¯ä¸ªç©å®¶éƒ½æ˜¯è‡ªç§çš„ï¼Œå¹¶è¯•å›¾æœ€å¤§åŒ–è‡ªå·±çš„æ”¶ç›Šã€‚ç„¶è€Œï¼Œçº³ä»€å‡è¡¡å¯èƒ½æ— æ³•è¾¾åˆ°ç¤¾ä¼šæœ€ä¼˜ï¼Œå› ä¸ºç©å®¶çš„è‡ªç§è¡Œä¸ºå¯èƒ½å¯¼è‡´æ•´ä½“æ”¶ç›Šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†æ‰©å±•å¼ç›¸å…³å‡è¡¡ï¼ˆEFCEï¼‰çš„æ¦‚å¿µï¼Œå®ƒå…è®¸ä¸€ä¸ªå¤–éƒ¨è®¾å¤‡ï¼ˆä¸­ä»‹ï¼‰å‘ç©å®¶æä¾›æ¨èåŠ¨ä½œï¼Œä½†ç©å®¶å¯ä»¥è‡ªç”±é€‰æ‹©æ˜¯å¦éµå¾ªè¿™äº›æ¨èã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†EFCEé—®é¢˜è½¬åŒ–ä¸ºéç‚¹é—®é¢˜
æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®ä¹‹ä¸€æ˜¯å°†EFCEé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªåŒçº¿æ€§éç‚¹é—®é¢˜ï¼ˆBSPPï¼‰ã€‚è¿™ç§æ–°çš„å…¬å¼åŒ–æ–¹æ³•ä¸ºå¼€å‘æ–°çš„ç®—æ³•æ¥è®¡ç®—EFCEæä¾›äº†åŸºç¡€ã€‚ä¸ºäº†å±•ç¤ºè¿™ç§æ–°å…¬å¼çš„æ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„å­æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†EFCEçš„å…¬å¼å’Œç»“æ„ç‰¹æ€§ã€‚ä¸åŸºäºçº¿æ€§è§„åˆ’çš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºä¸¤ä¸ªåŸºå‡†æ¸¸æˆ
ä¸ºäº†è¯„ä¼°EFCEæ±‚è§£å™¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªåŸºå‡†æ¸¸æˆï¼šæˆ˜èˆ°æ¸¸æˆå’Œè­¦é•¿æ¸¸æˆã€‚è¿™äº›æ¸¸æˆè¢«è®¾è®¡ä¸ºå¯å‚æ•°åŒ–çš„ï¼Œä»¥ä¾¿å¯ä»¥æ‰©å±•åˆ°ä¸åŒçš„å¤§å°ã€‚æˆ˜èˆ°æ¸¸æˆæ¨¡æ‹Ÿäº†é€šè¿‡è°ƒè§£å‘˜è¿›è¡Œå†²çªè§£å†³çš„åœºæ™¯ï¼Œè€Œè­¦é•¿æ¸¸æˆæ¨¡æ‹Ÿäº†è°ˆåˆ¤å’Œåå•†çš„åœºæ™¯ã€‚é€šè¿‡åˆ†æè¿™äº›æ¸¸æˆä¸­çš„EFCEï¼Œæœ¬æ–‡å‘ç°å³ä½¿è°ƒè§£å‘˜æ— æ³•å¼ºåˆ¶æ‰§è¡Œè¡Œä¸ºï¼Œå®ƒä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ˜¾è‘—æé«˜ç¤¾ä¼šç¦åˆ©å¹¶é˜»æ­¢ç©å®¶åç¦»æ¨èï¼š
1. ä½¿ç”¨ç‰¹å®šåŠ¨ä½œåºåˆ—ä½œä¸ºâ€œå¯†ç â€æ¥éªŒè¯ç©å®¶æ˜¯å¦åç¦»ï¼šåç¦»ä¼šå¯¼è‡´ä¸å®Œæ•´æˆ–é”™è¯¯çš„å¯†ç ï¼Œä»è€Œè¡¨æ˜ç©å®¶å·²ç»åç¦»ã€‚
2. å¦‚æœæ£€æµ‹åˆ°åç¦»ï¼Œè¯±å¯¼å¯¹æ‰‹å¯¹åç¦»æ¨èçš„ç©å®¶é‡‡å–æƒ©ç½šæ€§è¡ŒåŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å®éªŒéªŒè¯äº†åŸºäºBSPPå…¬å¼çš„å­æ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨å¤§å‹æ¸¸æˆå®ä¾‹ä¸­ä¼˜äºåŸºäºçº¿æ€§è§„åˆ’çš„å…ˆå‰æ–¹æ³•ã€‚è¿™è¡¨æ˜BSPPå…¬å¼åœ¨è®¡ç®—EFCEæ–¹é¢å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„BSPPå…¬å¼å’Œå­æ¢¯åº¦ä¸‹é™æ–¹æ³•ä¸ºè®¡ç®—EFCEæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå¼€å‘æ›´é«˜æ•ˆçš„EFCEæ±‚è§£å™¨å¥ å®šäº†åŸºç¡€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºå‡†æ¸¸æˆä¸ºè¯„ä¼°EFCEæ±‚è§£å™¨æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚è¿™äº›å‘ç°å¯¹äºç†è§£å’Œåº”ç”¨EFCEåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¾‹å¦‚äº¤é€šæ§åˆ¶ã€è´Ÿè½½å‡è¡¡å’Œç¢³å‡æ’ç­‰é¢†åŸŸã€‚

## High-Level Representation of Benchmark Families for Petri Games
### Abstract
Petri games have been introduced as a multi-player game model representing
causal memory to address the synthesis of distributed systems. For Petri games
with one environment player and an arbitrary bounded number of system players,
deciding the existence of a safety strategy is EXPTIME-complete. This result
forms the basis of the tool ADAM that implements an algorithm for the synthesis
of distributed controllers from Petri games. To evaluate the tool, it has been
checked on a series of parameterized benchmarks from manufacturing and workflow
scenarios. In this paper, we introduce a new possibility to represent benchmark
families for the distributed synthesis problem modeled with Petri games. It
enables the user to specify an entire benchmark family as one parameterized
high-level net. We describe example benchmark families as a high-level version
of a Petri game and exhibit an instantiation yielding a concrete 1-bounded
Petri game. We identify improvements either regarding the size or the
functionality of the benchmark families by examining the high-level Petri
games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é«˜çº§Petriæ¸¸æˆåŸºå‡†å®¶æ—è¡¨ç¤º

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Petriæ¸¸æˆä½œä¸ºä¸€ç§å¤šç©å®¶æ¸¸æˆæ¨¡å‹ï¼Œè¢«å¼•å…¥ç”¨äºè¡¨ç¤ºå› æœè®°å¿†ï¼Œä»¥è§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»¼åˆé—®é¢˜ã€‚å¯¹äºå…·æœ‰ä¸€ä¸ªç¯å¢ƒç©å®¶å’Œä»»æ„æœ‰ç•Œæ•°é‡çš„ç³»ç»Ÿç©å®¶çš„Petriæ¸¸æˆï¼Œå†³å®šå®‰å…¨ç­–ç•¥çš„å­˜åœ¨æ€§æ˜¯EXPTIMEå®Œå…¨çš„ã€‚è¿™ä¸€ç»“æœæ„æˆäº†å®ç°åˆ†å¸ƒå¼æ§åˆ¶å™¨ç»¼åˆç®—æ³•çš„å·¥å…·ADAMçš„åŸºç¡€ã€‚ä¸ºäº†è¯„ä¼°è¯¥å·¥å…·ï¼Œå®ƒå·²åœ¨ä¸€ç³»åˆ—æ¥è‡ªåˆ¶é€ å’Œå·¥ä½œæµç¨‹åœºæ™¯çš„å‚æ•°åŒ–åŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å¯èƒ½æ€§ï¼Œå³ç”¨Petriæ¸¸æˆè¡¨ç¤ºåˆ†å¸ƒå¼ç»¼åˆé—®é¢˜çš„åŸºå‡†å®¶æ—ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿå°†æ•´ä¸ªåŸºå‡†å®¶æ—æŒ‡å®šä¸ºä¸€ä¸ªå‚æ•°åŒ–çš„é«˜çº§ç½‘ç»œã€‚æˆ‘ä»¬æè¿°äº†ä½œä¸ºPetriæ¸¸æˆé«˜çº§ç‰ˆæœ¬çš„ç¤ºä¾‹åŸºå‡†å®¶æ—ï¼Œå¹¶å±•ç¤ºäº†äº§ç”Ÿå…·ä½“1æœ‰ç•ŒPetriæ¸¸æˆçš„å®ä¾‹åŒ–ã€‚é€šè¿‡æ£€æŸ¥é«˜çº§Petriæ¸¸æˆï¼Œæˆ‘ä»¬ç¡®å®šäº†åŸºå‡†å®¶æ—åœ¨å¤§å°æˆ–åŠŸèƒ½æ–¹é¢çš„æ”¹è¿›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜çº§Petriæ¸¸æˆè¡¨ç¤º
æœ¬æ–‡æå‡ºäº†åŸºäºé«˜çº§Petriç½‘çš„æ–°å‚æ•°åŒ–é«˜çº§è¡¨ç¤ºï¼Œç”¨äºç®€æ´å’Œæ¸…æ™°åœ°å®šä¹‰åŸºå‡†å®¶æ—ã€‚é«˜çº§Petriç½‘å…è®¸ç”¨æˆ·ä½¿ç”¨å…·æœ‰é¢„å®šä¹‰å‚æ•°å¤§å°çš„åŸŸä¸­çš„å•ä¸ªä»¤ç‰Œã€å…·æœ‰æ¡ä»¶æ ‡ç­¾çš„è½¬æ¢å’Œå…·æœ‰è¡¨è¾¾å¼æ ‡ç­¾çš„å¼§æ¥æŒ‡å®šæ•´ä¸ªåŸºå‡†å®¶æ—ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ä¾‹åŒ–æŠ€æœ¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ä¾‹åŒ–æŠ€æœ¯ï¼Œå¯ä»¥å°†å‚æ•°åŒ–é«˜çº§Petriæ¸¸æˆè½¬æ¢ä¸ºæ ‡å‡†1æœ‰ç•ŒPetriæ¸¸æˆã€‚è¿™ä½¿å¾—ç°æœ‰çš„ç®—æ³•å’Œå·¥å…·å¯ä»¥è§£å†³è¿™äº›Petriæ¸¸æˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡ä¸¤ä¸ªç¤ºä¾‹åŸºå‡†å®¶æ—ï¼ˆå¹¶å‘æœºå™¨å’Œè‡ªé‡æ„æœºå™¨äººï¼‰å±•ç¤ºäº†é«˜çº§Petriæ¸¸æˆè¡¨ç¤ºçš„åº”ç”¨ã€‚ä¸åŸå§‹å®ç°ç›¸æ¯”ï¼Œè¿™äº›åŸºå‡†å®¶æ—åœ¨å¤§å°æˆ–åŠŸèƒ½æ–¹é¢å¾—åˆ°äº†æ”¹è¿›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„é«˜çº§Petriæ¸¸æˆè¡¨ç¤ºæ–¹æ³•ä¸ºåˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»¼åˆé—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å…·æœ‰ç®€æ´ã€æ¸…æ™°å’Œå¯æ‰©å±•çš„ç‰¹ç‚¹ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»¼åˆé—®é¢˜ã€‚

## Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks
### Abstract
From the beginning if the history of AI, there has been interest in games as
a platform of research. As the field developed, human-level competence in
complex games became a target researchers worked to reach. Only relatively
recently has this target been finally met for traditional tabletop games such
as Backgammon, Chess and Go. Current research focus has shifted to electronic
games, which provide unique challenges. As is often the case with AI research,
these results are liable to be exaggerated or misrepresented by either authors
or third parties. The extent to which these games benchmark consist of fair
competition between human and AI is also a matter of debate. In this work, we
review the statements made by authors and third parties in the general media
and academic circle about these game benchmark results and discuss factors that
can impact the perception of fairness in the contest between humans and
machines
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¹³å°å…¬å¹³æ€§ï¼šäººå·¥æ™ºèƒ½ä¸äººç±»æ¸¸æˆåŸºå‡†çš„å…¬å¹³æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªäººå·¥æ™ºèƒ½è¯ç”Ÿä»¥æ¥ï¼Œæ¸¸æˆä¸€ç›´æ˜¯å…¶ç ”ç©¶å’Œæµ‹è¯•çš„å¹³å°ã€‚éšç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ï¼Œäººç±»åœ¨å¤æ‚æ¸¸æˆä¸­çš„æ°´å¹³æˆä¸ºäº†ç ”ç©¶äººå‘˜è¿½æ±‚çš„ç›®æ ‡ã€‚è¿‘å¹´æ¥ï¼Œè¿™ä¸€ç›®æ ‡åœ¨ä¼ ç»Ÿæ¡Œé¢æ¸¸æˆå¦‚å›½é™…è±¡æ£‹ã€å›´æ£‹ç­‰ä¸­å¾—ä»¥å®ç°ã€‚ç„¶è€Œï¼Œéšç€ç ”ç©¶çš„æ·±å…¥ï¼Œç”µå­æ¸¸æˆå› å…¶ç‹¬ç‰¹çš„æŒ‘æˆ˜æ€§è€Œæˆä¸ºæ–°çš„ç ”ç©¶ç„¦ç‚¹ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œå¹¶åˆ†æå½±å“å…¬å¹³æ€§çš„å› ç´ ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡é¦–å…ˆå›é¡¾äº†äººå·¥æ™ºèƒ½åœ¨ç»å…¸æ¡Œé¢æ¸¸æˆï¼ˆå¦‚å›½é™…è±¡æ£‹ã€å›´æ£‹ï¼‰å’Œç°ä»£ç”µå­æ¸¸æˆï¼ˆå¦‚æ˜Ÿé™…äº‰éœ¸ã€Dota 2ï¼‰ä¸­çš„åŸºå‡†æµ‹è¯•æˆæœï¼Œå¹¶åˆ†æäº†è¿™äº›æˆæœåœ¨åª’ä½“å’Œå­¦æœ¯ç•Œçš„è®¨è®ºã€‚æ¥ç€ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¬å¹³æ€§ç»´åº¦åˆ†ç±»æ³•ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆç«èµ›ä¸­çš„å…¬å¹³æ€§ã€‚æœ€åï¼Œæœ¬æ–‡è®ºè¯äº†åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œäººå·¥æ™ºèƒ½ä¸äººç±»ä¹‹é—´ä¸å­˜åœ¨å®Œå…¨å…¬å¹³çš„æ¯”è¾ƒæ–¹å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡åˆ†æäººå·¥æ™ºèƒ½åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå‘ç°ä»¥ä¸‹é—®é¢˜ï¼š
1. äººå·¥æ™ºèƒ½åœ¨æ¸¸æˆä¸­çš„è¡¨ç°å¾€å¾€è¢«å¤¸å¤§æˆ–è¯¯è§£ã€‚
2. äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­çš„å…¬å¹³æ€§å­˜åœ¨äº‰è®®ã€‚
3. äººå·¥æ™ºèƒ½åœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿ï¼ˆå¦‚ååº”é€Ÿåº¦ã€ä¿¡æ¯è·å–ç­‰ï¼‰å¯èƒ½è¢«è¯¯è§£ä¸ºä¸å…¬å¹³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å…¬å¹³æ€§ç»´åº¦åˆ†ç±»æ³•ä¸ºè¯„ä¼°äººå·¥æ™ºèƒ½ä¸äººç±»åœ¨æ¸¸æˆç«èµ›ä¸­çš„å…¬å¹³æ€§æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†äººå·¥æ™ºèƒ½åœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿å¯èƒ½è¢«è¯¯è§£ä¸ºä¸å…¬å¹³ï¼Œå¹¶å‘¼åç ”ç©¶äººå‘˜å…³æ³¨è¿™ä¸€é—®é¢˜ã€‚

## Marathon Environments Multi-Agent Continuous Control Benchmarks in a Modern Video Game Engine
### Abstract
Recent advances in deep reinforcement learning in the paradigm of locomotion
using continuous control have raised the interest of game makers for the
potential of digital actors using active ragdoll. Currently, the available
options to develop these ideas are either researchers' limited codebase or
proprietary closed systems. We present Marathon Environments, a suite of open
source, continuous control benchmarks implemented on the Unity game engine,
using the Unity ML- Agents Toolkit. We demonstrate through these benchmarks
that continuous control research is transferable to a commercial game engine.
Furthermore, we exhibit the robustness of these environments by reproducing
advanced continuous control research, such as learning to walk, run and
backflip from motion capture data; learning to navigate complex terrains; and
by implementing a video game input control system. We show further robustness
by training with alternative algorithms found in OpenAI.Baselines. Finally, we
share strategies for significantly reducing the training time.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Marathon Environmentsï¼šå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åº”ç”¨äºç°ä»£æ¸¸æˆå¼•æ“çš„æŒç»­æ§åˆ¶åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æŒç»­æ§åˆ¶é¢†åŸŸçš„å‘å±•ï¼Œæ¸¸æˆå¼€å‘è€…å¯¹ä½¿ç”¨ä¸»åŠ¨ragdollçš„æ•°å­—è§’è‰²äº§ç”Ÿäº†å…´è¶£ã€‚ç„¶è€Œï¼Œç›®å‰å¼€å‘è¿™äº›æƒ³æ³•çš„é€‰é¡¹è¦ä¹ˆæ˜¯ç ”ç©¶äººå‘˜æœ‰é™çš„ä»£ç åº“ï¼Œè¦ä¹ˆæ˜¯ä¸“æœ‰çš„å°é—­ç³»ç»Ÿã€‚æœ¬æ–‡æå‡ºäº†Marathon Environmentsï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æŒç»­æ§åˆ¶åŸºå‡†å¥—ä»¶ï¼Œåœ¨Unityæ¸¸æˆå¼•æ“ä¸Šå®ç°ï¼Œä½¿ç”¨Unity ML-Agents Toolkitã€‚é€šè¿‡è¿™äº›åŸºå‡†ï¼Œæœ¬æ–‡å±•ç¤ºäº†æŒç»­æ§åˆ¶ç ”ç©¶å¯ä»¥è½¬ç§»åˆ°å•†ä¸šæ¸¸æˆå¼•æ“ï¼Œå¹¶å±•ç¤ºäº†è¿™äº›ç¯å¢ƒçš„é²æ£’æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMarathon Environmentsæ˜¯ä¸€ä¸ªå¼€æºçš„æŒç»­æ§åˆ¶åŸºå‡†å¥—ä»¶ï¼Œåœ¨Unityæ¸¸æˆå¼•æ“ä¸Šå®ç°ï¼Œä½¿ç”¨Unity ML-Agents Toolkitã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è½»æ¾åœ°å°†æŒç»­æ§åˆ¶ç ”ç©¶åº”ç”¨äºå•†ä¸šæ¸¸æˆå¼•æ“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å±•ç¤ºäº†Marathon Environmentsçš„é²æ£’æ€§ï¼Œé€šè¿‡å¤åˆ¶é«˜çº§æŒç»­æ§åˆ¶ç ”ç©¶ï¼Œä¾‹å¦‚ä»è¿åŠ¨æ•æ‰æ•°æ®ä¸­å­¦ä¹ è¡Œèµ°ã€è·‘æ­¥å’Œåç©ºç¿»ï¼›å­¦ä¹ å¯¼èˆªå¤æ‚åœ°å½¢ï¼›ä»¥åŠå®ç°è§†é¢‘æ¸¸æˆè¾“å…¥æ§åˆ¶ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ä½¿ç”¨OpenAI.Baselinesä¸­çš„æ›¿ä»£ç®—æ³•è¿›è¡Œè®­ç»ƒçš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å®éªŒéªŒè¯äº†Marathon Environmentsçš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMarathon Environmentså¯ä»¥æˆåŠŸå¤åˆ¶é«˜çº§æŒç»­æ§åˆ¶ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä¸åŒç¯å¢ƒä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†äº«äº†ä¼˜åŒ–è®­ç»ƒæ—¶é—´çš„ç­–ç•¥ï¼Œä¾‹å¦‚å¢åŠ å¹¶å‘ä»£ç†æ•°é‡ã€ä½¿ç”¨æ— å¤´æ¨¡å¼ã€ä¿®æ”¹Academyè„šæœ¬ä»¥æ”¯æŒç‰©ç†æ­¥éª¤ç­‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Marathon Environmentsä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†ä¸€ä¸ªå¼€æºçš„æŒç»­æ§åˆ¶åŸºå‡†å¥—ä»¶ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå•†ä¸šæ¸¸æˆå¼•æ“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†äº«äº†ä¼˜åŒ–è®­ç»ƒæ—¶é—´çš„ç­–ç•¥ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æé«˜è®­ç»ƒæ•ˆç‡ã€‚

## Benchmarking Cognitive Abilities of the Brain with Computer Games
### Abstract
Most of the players have experienced the feeling of temporarily losing their
character in a given gameplay situation when they cannot control the character,
simply because they temporarily cannot see it. The main reasons for this
feeling may be due to the interplay of the following factors: (1) the visual
complexity of the game is unexpectedly increased compared with the previous
time period as more and more game objects and effects are rendered on the
display; (2) and/or the game is lagging; (3) and finally, it is also possible
that the players have no sufficient experience with controlling the character.
This paper focuses on the first reason. We have developed a benchmark program
which allows its user to experience the feeling of losing character. While the
user can control the character well the benchmark program will increase the
visual complexity of the display. Otherwise, if the user lost the character
then the program will decrease the complexity until the user will find the
character again, and so on. The complexity is measured based on the number of
changed pixels between two consecutive display images. Our measurements show
that the average of bit per second values of losing and finding pairs describes
the user well. The final goal of this research is to further develop our
benchmark to a standard psychological test.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆä¸­çš„è®¤çŸ¥èƒ½åŠ›è¯„ä¼°ï¼šBrainB Test Series 6

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶æœ‰æ—¶ä¼šå› è§†è§‰å¤æ‚æ€§çš„å¢åŠ è€Œæš‚æ—¶å¤±å»å¯¹è§’è‰²çš„æ§åˆ¶ã€‚è¿™ç§ç°è±¡å¯èƒ½ç”±å¤šç§å› ç´ å¼•èµ·ï¼ŒåŒ…æ‹¬æ¸¸æˆè§†è§‰å¤æ‚æ€§çš„çªç„¶å¢åŠ ã€æ¸¸æˆå»¶è¿Ÿä»¥åŠç©å®¶å¯¹è§’è‰²æ§åˆ¶ç»éªŒçš„ä¸è¶³ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨è§†è§‰å¤æ‚æ€§å¢åŠ è¿™ä¸€å› ç´ ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåä¸º BrainB Test Series 6 çš„åŸºå‡†ç¨‹åºï¼Œç”¨äºæ¨¡æ‹Ÿç©å®¶å¤±å»è§’è‰²çš„æ„Ÿè§‰ï¼Œå¹¶è¯„ä¼°å…¶è®¤çŸ¥èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šBrainB Test Series 6 åŸºå‡†ç¨‹åº
è¯¥ç¨‹åºé€šè¿‡æ§åˆ¶å±å¹•ä¸Šç§»åŠ¨çš„æ–¹å—çš„è§†è§‰å¤æ‚æ€§æ¥æ¨¡æ‹Ÿç©å®¶å¤±å»è§’è‰²çš„æ„Ÿè§‰ã€‚å½“ç©å®¶èƒ½å¤Ÿå¾ˆå¥½åœ°æ§åˆ¶è§’è‰²æ—¶ï¼Œç¨‹åºä¼šå¢åŠ å±å¹•çš„è§†è§‰å¤æ‚æ€§ï¼›å¦‚æœç©å®¶å¤±å»äº†è§’è‰²ï¼Œç¨‹åºä¼šé™ä½å¤æ‚æ€§ï¼Œç›´åˆ°ç©å®¶å†æ¬¡æ‰¾åˆ°è§’è‰²ã€‚ç¨‹åºçš„å¤æ‚æ€§åŸºäºè¿ç»­æ˜¾ç¤ºå›¾åƒä¹‹é—´å˜åŒ–çš„åƒç´ æ•°é‡æ¥è¡¡é‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¤çŸ¥èƒ½åŠ›è¯„ä¼°
é€šè¿‡æµ‹é‡ç©å®¶åœ¨å¤±å»å’Œæ‰¾åˆ°è§’è‰²æ—¶å±å¹•å¤æ‚æ€§çš„å¹³å‡å€¼ï¼Œå¯ä»¥è¯„ä¼°ç©å®¶çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤±å»å’Œæ‰¾åˆ°è§’è‰²æ—¶å±å¹•å¤æ‚æ€§çš„å¹³å‡å€¼å¯ä»¥å¾ˆå¥½åœ°æè¿°ç©å®¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç ”ç©¶äººå‘˜å¯¹ BrainB Test Series 6 è¿›è¡Œäº†åˆæ­¥æµ‹è¯•ï¼Œå¹¶å‘ç°ç©å®¶åœ¨å±å¹•å¤æ‚æ€§è¾ƒé«˜æ—¶æ›´å®¹æ˜“å¤±å»è§’è‰²ï¼Œè€Œåœ¨å±å¹•å¤æ‚æ€§è¾ƒä½æ—¶æ›´å®¹æ˜“æ‰¾åˆ°è§’è‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å‘ç°ï¼Œé€šè¿‡è°ƒæ•´é¼ æ ‡è®¾ç½®å’Œé¢œè‰²æ–¹æ¡ˆï¼Œå¯ä»¥æé«˜æµ‹è¯•çš„å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ BrainB Test Series 6 åŸºå‡†ç¨‹åºä¸ºè¯„ä¼°ç©å®¶çš„è®¤çŸ¥èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥ç”¨äºç”µå­ç«æŠ€äººæ‰é€‰æ‹”ã€è®¤çŸ¥èƒ½åŠ›ç ”ç©¶ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†æµ‹è¯•ç¯å¢ƒæ ‡å‡†åŒ–å’Œæµ‹è¯•ç»“æœåˆ†æçš„é‡è¦æ€§ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å‚è€ƒã€‚

## Game-theoretic approach to risk-sensitive benchmarked asset management
### Abstract
In this article we consider a game theoretic approach to the Risk-Sensitive
Benchmarked Asset Management problem (RSBAM) of Davis and Lleo \cite{DL}. In
particular, we consider a stochastic differential game between two players,
namely, the investor who has a power utility while the second player represents
the market which tries to minimize the expected payoff of the investor. The
market does this by modulating a stochastic benchmark that the investor needs
to outperform. We obtain an explicit expression for the optimal pair of
strategies as for both the players.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆç†è®ºè§†è§’ä¸‹çš„é£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨èµ„äº§ç®¡ç†é¢†åŸŸï¼ŒæŠ•èµ„è€…å¾€å¾€éœ€è¦åœ¨è¿½æ±‚æŠ•èµ„å›æŠ¥å’Œè§„é¿é£é™©ä¹‹é—´åšå‡ºæƒè¡¡ã€‚ä¼ ç»Ÿçš„é£é™©æ•æ„Ÿæ§åˆ¶æ–¹æ³•è™½ç„¶èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šå¹³è¡¡è¿™ä¸¤è€…ï¼Œä½†åœ¨é¢å¯¹éšæœºåŸºå‡†æ—¶ï¼ŒæŠ•èµ„è€…çš„æœ€ä¼˜ç­–ç•¥å¯èƒ½ä¼šå—åˆ°å½±å“ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥æ¸¸æˆç†è®ºï¼Œæ„å»ºä¸€ä¸ªé£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜çš„æ¸¸æˆæ¨¡å‹ï¼Œä»¥æ­ç¤ºæŠ•èµ„è€…åœ¨é¢å¯¹å¸‚åœºå¯¹æŠ—æ—¶çš„æœ€ä¼˜ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºé£é™©æ•æ„Ÿé›¶å’Œéšæœºå¾®åˆ†åšå¼ˆæ¨¡å‹
æœ¬æ–‡å°†é£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé›¶å’Œéšæœºå¾®åˆ†åšå¼ˆæ¨¡å‹ï¼Œå…¶ä¸­æŠ•èµ„è€…å’Œå¸‚åœºåˆ†åˆ«ä½œä¸ºä¸¤ä¸ªç©å®¶ã€‚æŠ•èµ„è€…è¿½æ±‚æœ€å¤§åŒ–å…¶æŠ•èµ„ç»„åˆçš„é¢„æœŸå¢é•¿ç‡ï¼Œè€Œå¸‚åœºåˆ™è¯•å›¾é€šè¿‡è°ƒæ•´éšæœºåŸºå‡†æ¥æœ€å°åŒ–æŠ•èµ„è€…çš„é¢„æœŸå›æŠ¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ±‚è§£æœ€ä¼˜ç­–ç•¥
æœ¬æ–‡é€šè¿‡æ±‚è§£å“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼åå¾®åˆ†æ–¹ç¨‹ï¼Œå¾—åˆ°äº†æŠ•èµ„è€…å’Œå¸‚åœºåœ¨åšå¼ˆä¸­çš„æœ€ä¼˜ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥ä¸ä»…è€ƒè™‘äº†æŠ•èµ„è€…çš„é£é™©åŒæ¶ç¨‹åº¦ï¼Œè¿˜è€ƒè™‘äº†å¸‚åœºå¯¹æŠ—è¡Œä¸ºçš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡æ•°å€¼æ¨¡æ‹ŸéªŒè¯äº†æ‰€æå‡ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜ä¸­ï¼ŒæŠ•èµ„è€…å’Œå¸‚åœºä¹‹é—´çš„åšå¼ˆç¡®å®ä¼šå½±å“æŠ•èµ„è€…çš„æœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ï¼ŒæŠ•èµ„è€…çš„é£é™©åŒæ¶ç¨‹åº¦å’Œå¸‚åœºå¯¹æŠ—è¡Œä¸ºçš„å¼ºåº¦éƒ½ä¼šå¯¹æœ€ä¼˜ç­–ç•¥äº§ç”Ÿå½±å“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¸¸æˆç†è®ºè§†è§’ä¸ºé£é™©æ•æ„ŸåŸºå‡†èµ„äº§ç®¡ç†é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„ç ”ç©¶æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…å¯ä»¥ç”¨äºåˆ†ææŠ•èµ„è€…å’Œå¸‚åœºä¹‹é—´çš„åšå¼ˆï¼Œè¿˜å¯ä»¥ç”¨äºè®¾è®¡æ›´æœ‰æ•ˆçš„èµ„äº§ç®¡ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹å’Œæ±‚è§£æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–é‡‘èé¢†åŸŸçš„åšå¼ˆé—®é¢˜æä¾›å‚è€ƒã€‚

## Benchmarks for Parity Games (extended version)
### Abstract
We propose a benchmark suite for parity games that includes all benchmarks
that have been used in the literature, and make it available online. We give an
overview of the parity games, including a description of how they have been
generated. We also describe structural properties of parity games, and using
these properties we show that our benchmarks are representative. With this work
we provide a starting point for further experimentation with parity games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¥ å®šå…¬å¹³åšå¼ˆç®—æ³•è¯„ä¼°åŸºçŸ³ï¼šå…¨é¢è§£æå…¬å¹³åšå¼ˆåŸºå‡†é›†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å…¬å¹³åšå¼ˆåœ¨æ¨¡å‹æ£€éªŒç ”ç©¶ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå…¶æ±‚è§£é—®é¢˜è¢«è¯æ˜å±äº NP âˆ© co-NP å¤æ‚åº¦ç±»ï¼Œå¹¶ä¸”ç›®å‰å°šæ— å·²çŸ¥çš„å¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‚è¿‘å¹´æ¥ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†å¤§é‡çš„å…¬å¹³åšå¼ˆæ±‚è§£ç®—æ³•ï¼Œå¹¶ç ”ç©¶äº†å¤šé¡¹å¼æ—¶é—´åŒ–ç®€æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŸºå‡†é›†ï¼Œéš¾ä»¥å¯¹ä¸åŒå·¥å…·å’Œç®—æ³•è¿›è¡Œæ¯”è¾ƒè¯„ä¼°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å…¬å¹³åšå¼ˆåŸºå‡†é›†ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥åŸºå‡†é›†åŒ…å«ä»¥ä¸‹ç‰¹ç‚¹ï¼š

* **å¤šæ ·æ€§**ï¼šæ¶µç›–æ¥è‡ªä¸åŒéªŒè¯é—®é¢˜çš„æ¸¸æˆï¼ŒåŒ…æ‹¬æ¨¡å‹æ£€éªŒã€ç­‰ä»·æ€§æ£€æŸ¥ã€å†³ç­–è¿‡ç¨‹å’Œåˆæˆé—®é¢˜ã€‚
* **ä»£è¡¨æ€§**ï¼šåŒ…å«æ–‡çŒ®ä¸­å·²ä½¿ç”¨çš„æ‰€æœ‰åŸºå‡†æ¸¸æˆï¼Œå¹¶ä½¿ç”¨ç»“æ„å±æ€§åˆ†æå…¶ä»£è¡¨æ€§ã€‚
* **æ˜“ç”¨æ€§**ï¼šæ‰€æœ‰æ¸¸æˆå‡ä»¥ PGSolver æ ¼å¼åœ¨çº¿æä¾›ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åˆ†æäº†åŸºå‡†é›†ä¸­æ¸¸æˆçš„å¤šç§ç»“æ„å±æ€§ï¼ŒåŒ…æ‹¬é¡¶ç‚¹æ•°ã€è¾¹æ•°ã€ä¼˜å…ˆçº§æ•°ã€åº¦æ•°ã€å¼ºè¿é€šåˆ†é‡ã€æœç´¢ç­–ç•¥ç‰¹æ€§ã€è·ç¦»ã€å±€éƒ¨ç»“æ„å’Œå®½åº¦åº¦é‡ç­‰ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºå‡†é›†æ¶µç›–äº†å¹¿æ³›çš„å±æ€§å€¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°å…¬å¹³åšå¼ˆç®—æ³•çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **æ„å»ºæ ‡å‡†åŸºå‡†é›†**ï¼šæœ¬æ–‡æå‡ºçš„åŸºå‡†é›†ä¸ºå…¬å¹³åšå¼ˆç®—æ³•çš„è¯„ä¼°æä¾›äº†é‡è¦çš„å‚è€ƒï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚
* **åˆ†æç»“æ„å±æ€§**ï¼šæœ¬æ–‡å¯¹å…¬å¹³åšå¼ˆçš„ç»“æ„å±æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¸ºç®—æ³•è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯ã€‚
* **æ¢ç´¢æ–°å±æ€§**ï¼šæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å…¬å¹³åšå¼ˆçš„å…¶ä»–ç»“æ„å±æ€§ï¼Œå¹¶ç ”ç©¶å…¶å¯¹ç®—æ³•æ€§èƒ½çš„å½±å“ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºçš„å…¬å¹³åšå¼ˆåŸºå‡†é›†ä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨å…¬å¹³åšå¼ˆç®—æ³•çš„è¯„ä¼°å’Œæ”¹è¿›ã€‚æœªæ¥ï¼Œéšç€æ›´å¤šæ¸¸æˆå’Œå±æ€§çš„åŠ å…¥ï¼Œè¯¥åŸºå‡†é›†å°†è¿›ä¸€æ­¥å®Œå–„ï¼Œä¸ºå…¬å¹³åšå¼ˆç ”ç©¶æä¾›æ›´å¼ºå¤§çš„æ”¯æŒã€‚

## Fate stochastic management and policy benchmark in 421, a popular game
### Abstract
Using game and probability theories, I study the French popular game 421, a
perfect information stochastic stage game. The problem is to find strategies
maximizing the probability of some expected utility. I only solve a player's
round against providence, a problem of fate stochastic management: beyond the
backward induction solution, bounded complexity motivates heuristic policies.
For a unique goal utility, a simple optimal policy, ratchet, is obtained. Its
result probabilities are compiled and used, for arbitrary utilities, as the
logic of goal identification policies. Various policies appear, close to human
behavior, and are exactly evaluated by solving the Kolmogorov equation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | 421æ¸¸æˆä¸­å‘½è¿éšæœºç®¡ç†ä¸ç­–ç•¥åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ¬æ–‡ç ”ç©¶äº†æ³•å›½æµè¡Œçš„æ¸¸æˆ421ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å®Œç¾ä¿¡æ¯çš„éšæœºé˜¶æ®µæ¸¸æˆã€‚æ¸¸æˆçš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€å¤§åŒ–æŸäº›é¢„æœŸæ•ˆç”¨æ¦‚ç‡çš„ç­–ç•¥ã€‚ä½œè€…å°†421æ¸¸æˆç®€åŒ–ä¸ºç©å®¶å›åˆå¯¹æŠ—å‘½è¿çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå‘½è¿éšæœºç®¡ç†é—®é¢˜ã€‚ä½œè€…é€šè¿‡åˆ†ææ¸¸æˆè§„åˆ™å’Œæ¦‚ç‡ç†è®ºï¼Œæå‡ºäº†å‡ ç§ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨Kolmogorovæ–¹ç¨‹ç²¾ç¡®è¯„ä¼°äº†è¿™äº›ç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†421æ¸¸æˆç®€åŒ–ä¸ºç©å®¶å›åˆå¯¹æŠ—å‘½è¿çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå‘½è¿éšæœºç®¡ç†é—®é¢˜ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºäº†å‡ ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ä¼¯åŠªåˆ©ç­–ç•¥å’Œæ£˜è½®ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨Kolmogorovæ–¹ç¨‹ç²¾ç¡®è¯„ä¼°äº†è¿™äº›ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºäº†ç›®æ ‡è¯†åˆ«ç¼–ç¨‹æ–¹æ³•ï¼Œé€šè¿‡è€ƒè™‘ç»“æœæ¦‚ç‡æ¥è¯†åˆ«ç›®æ ‡ï¼Œä»è€Œç®€åŒ–ç­–ç•¥çš„å¤æ‚æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½œè€…ä½¿ç”¨Mathematicaè½¯ä»¶å®ç°äº†421æ¸¸æˆæ¨¡å‹ï¼Œå¹¶è®¡ç®—äº†å„ç§ç­–ç•¥çš„ç»“æœæ¦‚ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ£˜è½®ç­–ç•¥åœ¨D < Fçš„æƒ…å†µä¸‹æ˜¯æœ€ä¼˜çš„ï¼Œè€Œä¼¯åŠªåˆ©ç­–ç•¥åœ¨D > Fçš„æƒ…å†µä¸‹æ˜¯æœ€ä¼˜çš„ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å‘ç°ï¼Œå¼•å…¥éšæœºæ€§å’Œç›®æ ‡è¯†åˆ«ç¼–ç¨‹å¯ä»¥æ˜¾è‘—æé«˜ç­–ç•¥çš„æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºç†è§£éšæœºæ¸¸æˆä¸­çš„ç­–ç•¥é€‰æ‹©å’Œå‘½è¿ç®¡ç†å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ç­–ç•¥è¯„ä¼°æ–¹æ³•å’Œç›®æ ‡è¯†åˆ«ç¼–ç¨‹æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–éšæœºå†³ç­–é—®é¢˜ã€‚

