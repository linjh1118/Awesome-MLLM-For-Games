{"title":"DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments","authors":"Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, Liquan Xiao","summary":"Large Language Model~(LLM) based agents have been increasingly popular in\nsolving complex and dynamic tasks, which requires proper evaluation systems to\nassess their capabilities. Nevertheless, existing benchmarks usually either\nfocus on single-objective tasks or use overly broad assessing metrics, failing\nto provide a comprehensive inspection of the actual capabilities of LLM-based\nagents in complicated decision-making tasks. To address these issues, we\nintroduce DSGBench, a more rigorous evaluation platform for strategic\ndecision-making. Firstly, it incorporates six complex strategic games which\nserve as ideal testbeds due to their long-term and multi-dimensional\ndecision-making demands and flexibility in customizing tasks of various\ndifficulty levels or multiple targets. Secondly, DSGBench employs a\nfine-grained evaluation scoring system which examines the decision-making\ncapabilities by looking into the performance in five specific dimensions and\noffering a comprehensive assessment in a well-designed way. Furthermore,\nDSGBench also incorporates an automated decision-tracking mechanism which\nenables in-depth analysis of agent behaviour patterns and the changes in their\nstrategies. We demonstrate the advances of DSGBench by applying it to multiple\npopular LLM-based agents and our results suggest that DSGBench provides\nvaluable insights in choosing LLM-based agents as well as improving their\nfuture development. DSGBench is available at\nhttps:\/\/github.com\/DeciBrain-Group\/DSGBench.","url":"http:\/\/arxiv.org\/abs\/2503.06047v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.06047v1","published":1741407443000,"comment":"43 pages, 5 figures, conference","pdf_text":"DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based\nAgents in Complex Decision-Making Environments\nWenjie Tang1,∗, Yuan Zhou2,∗, Erqiang Xu2 , Keyan Cheng2 , Minne Li2,† , Liquan Xiao1\n1National University of Defense Technology, Changsha, China\n2Intelligent Game and Decision Lab (IGDL), Beijing, China\n∗equal contribution, †corresponding author\nAbstract\nLarge Language Model (LLM) based agents have\nbeen increasingly popular in solving complex and\ndynamic tasks, which requires proper evaluation\nsystems to assess their capabilities. Nevertheless,\nexisting benchmarks usually either focus on single-\nobjective tasks or use overly broad assessing met-\nrics, failing to provide a comprehensive inspec-\ntion of the actual capabilities of LLM-based agents\nin complicated decision-making tasks. To address\nthese issues, we introduce DSGBench, a more rig-\norous evaluation platform for strategic decision-\nmaking. Firstly, it incorporates six complex strate-\ngic games which serve as ideal testbeds due to their\nlong-term and multi-dimensional decision-making\ndemands and flexibility in customizing tasks of var-\nious difficulty levels or multiple targets. Secondly,\nDSGBench employs a fine-grained evaluation scor-\ning system which examines the decision-making\ncapabilities by looking into the performance in\nfive specific dimensions and offering a compre-\nhensive assessment in a well-designed way. Fur-\nthermore, DSGBench also incorporates an auto-\nmated decision-tracking mechanism which enables\nin-depth analysis of agent behaviour patterns and\nthe changes in their strategies.\nWe demonstrate\nthe advances of DSGBench by applying it to multi-\nple popular LLM-based agents and our results sug-\ngest that DSGBench provides valuable insights in\nchoosing LLM-based agents as well as improving\ntheir future development. DSGBench is available at\nhttps:\/\/github.com\/DeciBrain-Group\/DSGBench1.\n1\nIntroduction\nRecent progress on large language models (LLMs) has\nshown substantial improvements in reasoning, planning, and\nproblem-solving, especially in well-defined closed-world en-\nvironments such as web navigation, household management,\nand assistant programming [Xi et al., 2023; Zhou et al., 2023;\nQian et al., 2024]. These advancements highlight the poten-\ntial of LLMs to be employed in more complicated settings\nlike open-world situations and strategic gaming[Wang et al.,\n2023; Ma et al., 2023b]. Nevertheless, applying LLMs in\nsuch environments demands not only a deep understanding of\nthe motivations and potential deceptive tactics of enemies, but\nalso advanced reasoning to anticipate risks and vulnerabilities\n1Please contact twj@nudt.edu.cn if any problem occurs.\nin our strategies. We believe that unlocking this potential re-\nquires further developments in model architectures as well as\nestablishing a rigorous evaluation framework to assess their\ncapabilities[Chang et al., 2024] systematically.\nDespite significant advances in LLM benchmarking, ex-\nisting assessment frameworks suffer from three key limita-\ntions when applied to complicated decision-making settings.\nFirstly, these benchmarks are usually built on environments\nfor static or single-objective tasks [Xie et al., 2024; Zhong\net al., 2023], failing to incorporate real-world dynamic sit-\nuations or constantly changing goals.\nSecondly, common\nevaluation metrics focus extensively on result-oriented per-\nformance like task completion rate and win rate [Liu et al.,\n2023; Xi et al., 2024; Wu et al., 2023; Xu et al., 2023]. How-\never, most models have near-zero success rates in challenging\nenvironments[Xie et al., 2024], and therefore, overly broad\nassessing scores are difficult to evaluate strengths and weak-\nnesses among LLM-based agents properly.\nFurthermore,\nsome existing works concentrate on single-dimensional capa-\nbilities[Bailis et al., 2024a; Mukobi et al., 2023], which lacks\nsystematic evaluation of core cognitive capabilities which\nare crucial for strategic gaming tasks. Such evaluation re-\nquires to analyze an LLM-based agent from aspects including\nlong-term strategic planning, short-term real-time decision-\nmaking, and social reasoning of external adversaries.\nTo address these challenges, we introduce DSGBench, a\ndiverse strategic game benchmark designed to assess LLM-\nbased agents across multiple dimensions of cognitive and in-\nteractive capabilities. DSGBench is built around three key\ncomponents: a set of diverse and complex strategy games,\na comprehensive evaluation scoring system, and a detailed\ndecision-tracking mechanism. The overall framework of DS-\nGBench is shown in Figure 1. The platform includes six clas-\nsic strategic games which are StarCraft II [Ma et al., 2023a],\nCivilization [Qi et al., 2024], Street Fighter III [Wikipedia,\n2024e], Diplomacy [Mukobi et al., 2023], Werewolf [Bailis\net al., 2024a], and Stratego [Wikipedia, 2024d]. They each\nselected to test specific skill sets, including strategic plan-\nning, real-time decision-making, social reasoning, team col-\nlaboration, and adaptive learning. These games replicate the\ndynamics of the real world through multi-agent interactions,\nlong-context decision-making, and the achievement of vari-\nous sub-goals, providing a varied scene setting that allows for\na comprehensive assessment of agents’ adaptability and cog-\nnitive integration.\nA key feature of DSGBench is its novel evaluation frame-\nwork that incorporates fine-grained metrics to provide a com-\nprehensive view of agent performance across multiple dimen-\nsions. These designed metrics capture the nuances of agent\nbehaviour, particularly in decision-making under uncertainty,\narXiv:2503.06047v1  [cs.AI]  8 Mar 2025\nFigure 1: The overall framework of DSGBench. The framework consists of (1) a multi-game environment supporting both asynchronous\nand synchronous interactions; (2) fine-grained capability metrics for strategic planning, real-time decision-making, and team collaboration;\nand (3) decision trajectory tracking tools that collaboratively analyze agents’ decision-making processes. Through observation-to-prompt and\nresponse-to-action loops, DSGBench enables systematic evaluation of LLM-based agents in dynamic, multi-agent scenarios.\nlong-term strategic planning, and adaptive strategy adjust-\nment. Through this multi-dimensional analysis, DSGBench\nprovides deeper insights into agent capabilities beyond con-\nventional success measures. Additionally, our benchmark in-\ntegrates detailed decision-tracking functionality, offering re-\nsearchers clear visibility into agents’ underlying decision-\nmaking processes. This transparency is essential for improv-\ning model interpretability and advancing agent design.\n2\nRelated Work\n2.1\nLLM-based agents\nLLM-based agents present human-like cognitive abilities to\nsolve decision-making problems.\nIn early applications of\nthe LLMs, they achieved significant success in handling rel-\natively simple tasks such as role-playing and code genera-\ntion[Nijkamp et al., 2022; Park et al., 2023] due to their\nstrong capabilities in instruction following, language com-\nprehension, and generation[Wang et al., 2024b]. As the re-\nsearch progressed, researchers began to focus on how to in-\nteract LLMs with external tools or environments[Schick et\nal., 2023; Tang et al., 2024], where the key technique is\nto provide relevant instruction and environment information\nthrough context, which enables LLM-based agents to gener-\nate executable actions in response to complex tasks. In ad-\ndition, researchers have systematically explored the capabili-\nties of LLM-based agents in perception, memory, decision-\nmaking, and tool use, with application areas ranging from\nweb navigation, software development, and game decision-\nmaking[Wang et al., 2024b; Zhou et al., 2023; Qian et al.,\n2024; Ma et al., 2023b]. Among these advances, the emer-\ngent reasoning capabilities of LLM-based agents are particu-\nlarly critical and are an integral part of the decision-making\nprocess[Hao et al., 2024]. Furthermore, it is found that LLMs\nhave demonstrated advanced cognitive capabilities such as\nhuman-like chain-of-thought reasoning, world modeling, and\ntheory of mind[Wei et al., 2022; Hao et al., 2023; Shapira\net al., 2023].\nThese capabilities influence aspects of how\nLLM-based agents respond to changes in adversary strategies,\nhow they make rational decisions in uncertain environments,\nand how they maintain long-term and short-term consistency\nacross successive decision-making processes.\n2.2\nExisting Benchmarks for LLM-based agents\nWith the enhancement of the comprehensive capabilities of\nLLMs, there is a growing trend to evaluate their performance\nin more challenging open-world or complex gaming scenar-\nios. Although static datasets provided effective evaluation in\nearly studies, they failed to reflect the true performance of\nLLMs in dynamic and complex environments [Huang et al.,\n2024; Bai et al., 2023]. To address this limitation, subse-\nquent research has begun to evaluate LLM-based agents in\nexecutable environments, focusing on revealing their capa-\nbilities in an open-ended generation, multi-round interaction,\nand agent-based role-playing[Wang et al., 2024b]. Current\nresearchers have focused on areas such as software develop-\nment, embodied agents, society simulations, gaming, and pol-\nicy making[Hong et al., 2023; Mandi et al., 2024; Kaiya et\nal., 2023; Mukobi et al., 2023], which have provided LLM\nassessments with more authentic and diverse contexts.\nIn particular, strategy games have been widely recog-\nnized as ideal experimental platforms for evaluating decision-\nmaking capabilities of LLM-based agents due to their com-\nplex reasoning requirements and dynamic interaction prop-\nerties [Liu et al., 2023]. They require agents to engage in\nmultistep reasoning, predict the actions of other agents, and\nBenchmark\nComplex\nGame Theory\nFine-Grained\nCapability Metrics\nDecision\nTrajectory Analysis\nReal-Time &\nTurn-Based\nDiverse\n-Scenarios\nAgentBench[Liu et al., 2023]\n✘\n✘\n✘\n✔\n✔\nSmartPlay[Wu et al., 2023]\n✘\n✘\n✘\n✘\n✘\nGameBench[Costarelli et al., 2024]\n✔\n✘\n✘\n✘\n✘\nGTBench[Duan et al., 2024]\n✔\n✘\n✘\n✘\n✘\nMAgIC[Xu et al., 2023]\n✔\n✘\n✘\n✘\n✘\nAgentBoard[Ma et al., 2024]\n✘\n✘\n✔\n✘\n✔\nAgentGym[Xi et al., 2024]\n✘\n✘\n✘\n✔\n✔\nDSGBench(Ours)\n✔\n✔\n✔\n✔\n✔\nTable 1: Compare various LLM-based agent benchmarks across key dimensions. While most benchmarks, such as GTBench and SmartPlay,\nfocus on specific tasks or dimensions, DSGBench stands out by offering broader support across complex game theory, fine-grained metrics,\ndecision trajectory analysis, and diverse scenarios. This makes it a more suitable option for evaluating LLM-based agents in complex envi-\nronments.\nbalance competing goals under uncertainty, thus simulating\nreal-world challenges. However, as shown in table 1, bench-\nmarks such as GTBench[Duan et al., 2024], SmartPlay[Wu\net al., 2023], and MAgIC[Xu et al., 2023] focus on uni-\ndimensional capabilities, neglecting the integrated cognitive\nskills required to cope with interdependence and uncertainty\nin dynamic environments. In addition, benchmarks such as\nAgentBench[Liu et al., 2023] and GameBench[Costarelli et\nal., 2024] use eventual success as the core metric, which lim-\nits their insight into complex decision-making processes.\n3\nDSGBench - Preliminary\n3.1\nGame Interaction Formulation\nIn DSGBench evaluation, we model agent-environment in-\nteractions as a Partially Observable Markov Decision Pro-\ncess (POMDP), formalized as the quintuple ⟨W, S, A, O, T⟩.\nHere, W denotes the victory condition—the strategic objec-\ntives agents must achieve, such as destroying the opponent’s\nbase in StarCraft II or achieving victory through multiple\npathways (combat conquest, cultural dominance, or scientific\nadvancement) in Civilization. S represents the state space,\nencompassing all observable environmental states within the\ncurrent game. A defines the action space of all legal actions\navailable to an agent per time step, including movement, com-\nbat, dialogue, and negotiation. O comprises the observation\nspace, containing environmental feedback ft that reflects im-\nmediate state changes and responses to agent actions. T rep-\nresents the state transition function S×A →S, mapping how\nthe current state st and agent action at determine the subse-\nquent state st+1.\nSingle-level Inference. Games such as Street Fighter III,\nDiplomacy, Werewolf, and Stratego Games are particularly\nsuited for single-level reasoning due to their centralized state\nand action spaces.\nThe reasoning process is formalized as\npπ(τ) = p(s0)\nT −1\nY\nt=0\np(at|st, ft) · T(st+1|st, at, ft)\n(1)\nwhere pπ(τ) represents the policy trajectory, p(s0) is the\ninitial state distribution, p(at|st, ft) denotes the probabil-\nity of taking action at given state st and feedback ft, and\nT(st+1|st, at, ft) is the state transition function. In single-\nlevel reasoning games, agents can effectively accomplish\ncomplex tasks with simplified state and action reasoning.\nTwo-level Inference. Complex strategy games like Star-\nCraft II and Civilization require agents to handle large obser-\nvation spaces and multi-dimensional tasks through two levels\nof reasoning: high-level strategic planning (for example, re-\nsource management and army deployment) and low-level tac-\ntical decisions (for example, executing micro-operations and\nmanaging local combat).\npπ(τ) = p(s0)\nT −1\nY\nt=0\np(ahigh\nt\n| st, chigh)·\np(alow\nt\n| st, ahigh\nt\n, clow) · T(st+1 | st, alow\nt , ft)\n(2)\nHere, ahigh\nt\nrepresents high-level strategic decisions based\non global policy chigh; alow\nt\ndenotes immediate tactical ac-\ntions guided by local feedback clow, and T(st+1|st, alow\nt\n, ft)\ndefines the state transition function reflecting environmental\nchanges in response to current actions.\n3.2\nCapability Score Computation\nTo compute the scores of the LLMs across different capabil-\nity dimensions, we first establish the mapping between each\ncapability dimension and multiple associated games, where\neach game is linked to a set of fine-grained metrics. The score\nfor each capability dimension is calculated by weighting the\nperformance metrics of the associated games on multiple fine-\ngrained metrics. Its definition is as follows:\nT =\nm\nX\ni=1\nWi · βi ·\n\n\nn\nX\nj=1\nwj ·\n1\nkj\nPkj\nk=1 Ryjk −minj Ryj\nmaxj Ryj −minj Ryj\n\n\n(3)\nwhere the aggregated capability score T integrates capability\ndimensions through weight coefficients Wi ∈[0, 1], where\nPm\ni=1 Wi = 1. To account for the varying emphasis of ca-\npabilities in different scenarios, each dimension incorporates\nan adjustment factor βi ∈(0, 1]. Performance in individual\nscenarios is weighted by wj ∈[0, 1], with Pn\nj=1 wj = 1.\nFor statistical robustness, we conduct kj evaluation runs per\nscenario, where Ryjk represents the performance metric from\nthe k-th run. These metrics are normalized using the pre-\ndefined theoretical minimum value minj Ryj and maximum\nvalue maxj Ryj to ensure fair comparison across different\ngaming environments.\n4\nDSGBench - Overview\nDSGBench is a comprehensive benchmark designed to evalu-\nate the strategic decision-making capabilities of LLM-based\nagents through a diverse set of strategy games.\nIt con-\nsists of three key components: complex game environments,\nCapability\nGames\nMetrics\nScenarios\nStrategic Planning\nStarcraft II[Ma et al., 2023b]\nRPM, EER, SUR, TCR\nMacro (Async\/Sync)\nCivilization[Qi et al., 2024]\nEGR, CER, TRP, LUR, MGR\nMap (World)\nDiplomacy[Mukobi et al., 2023]\nCCC, WS\nNegotiation and Alliances\nStratego[Wikipedia, 2024d]\nCPR, TPCV\nRandom Placement\nReal-Time Decision-Making Starcraft II[Ma et al., 2023b]\nAPM, EPM\nRush (Async\/Sync)\nStreet Fighter III[Wikipedia, 2024e]\nAHR, SMHR, HCR\nFast-Paced(Async)\nSocial Reasoning\nDiplomacy[Mukobi et al., 2023]\nBIR\nNegotiation and Alliances\nWerewolf[Bailis et al., 2024a]\nIRP\nSocial Deduction\nTeam Collaboration\nDiplomacy[Mukobi et al., 2023]\nASR, AD\nNegotiation and Alliances\nWerewolf[Bailis et al., 2024a]\nKSR, VSS\nSocial Deduction\nAdaptive Learning\nStarcraft II[Ma et al., 2023b]\nWR, GA\nRandom (Async\/Sync)\nCivilization[Qi et al., 2024]\nWR, GA\nMap (Small-Scale)\nStreet Fighter III[Wikipedia, 2024e]\nWR, GA\nSync\nStratego[Wikipedia, 2024d]\nWR, GA\nFixed Placement\nTable 2: Evaluation metrics and scenarios for assessing LLM-based agents across five key dimensions: strategic planning, real-time decision-\nmaking, social reasoning, team collaboration, and adaptive learning in diverse strategic games.\nfine-grained evaluation metrics, and decision tracking mech-\nanisms. Using a unified Gym interface, DSGBench provides\na standardised interaction model and supports customisable\ngame scenarios, facilitating the integration of new games and\nthe extension of existing ones. Additionally, the framework\nincorporates an automated scoring process that permits the\ncustomisation of scoring tasks as required. For a detailed de-\nscription of the architectural design, please refer to the Ap-\npendix A.\nIn this section, a detailed overview of DSGBench is pro-\nvided, following the sequence outlined in the table 2. Firstly,\nthe core cognitive decision-making abilities of LLM-based\nagents are explored, as outlined in the table. Secondly, the\nevaluation framework is introduced, including fine-grained\nmetrics and decision-tracking mechanisms. To effectively as-\nsess these abilities, a three-part evaluation framework is pro-\nposed, which includes a set of complex strategic games, fine-\ngrained metrics, and decision-tracking mechanisms. Finally,\nthe diverse game scenarios and evaluation tasks designed are\nshowcased, and the code framework for the automated evalu-\nation platform is discussed.\n4.1\nCognitive Decision-Making Capabilities\nIn addressing complex, multifaceted problems, human agents\nrequire the coordinated application of cognitive and adaptive\ncapabilities. According to Dual Systems Cognitive Theory\n[Kahneman, 2011], rational planning and analytical thinking\nrely on ”System 2”, while rapid decision-making is accom-\nplished through the intuitive responses of ”System 1”. The\ncomplementarity of the two allows individuals to make quick\ndecisions and maintain certain goals in dynamic and uncer-\ntain environments. However, in complex situations involv-\ning multiple intelligences, the ability to make decisions col-\nlectively, and individual capabilities alone are often insuffi-\ncient. Distributed Cognition Theory emphasizes that the abil-\nity to reason socially and work in a team stems from the in-\nteraction of the individual with the environment, tools, and\nother subjects[Hutchins, 1995]. Furthermore, Dynamic Deci-\nsion Theory states that, in dynamic environments, individuals\nand groups can adapt their decision-making strategies in re-\nsponse to feedback, thereby exhibiting adaptive behaviours in\nresponse to changing external conditions[Edwards, 1962].\nDrawing inspiration from this human cognitive decision-\nmaking framework, DSGBench constructs a five-dimensional\nassessment system covering the core dimensions of intelligent\ndecision-making: strategic planning (deep analysis by System\n2), real-time decision-making (fast response by System 1), so-\ncial reasoning (distributed interaction mechanism), team col-\nlaboration (multi-agents coordination) and adaptive learning\n(dynamic strategy optimization). The following outlines each\nof these dimensions in detail:\n• Strategic Planning refers to the ability to formulate and\nimplement long-term strategies that are consistent with\noverall goals. This ability includes optimizing resources,\nanticipating future scenarios, and adapting to changing\nenvironments.\n• Real-Time Decision-Making refers to the ability to\nmake effective decisions under time pressure.\nIt in-\nvolves managing competing objectives, processing dy-\nnamic information, and reacting quickly to unpredictable\nchanges.\n• Social Reasoning refers to the ability to understand and\nnavigate interactions in a team or competitive environ-\nment. This ability requires understanding the intentions\nof other agents, predicting their behaviour, and adjusting\nstrategies accordingly.\n• Team Collaboration refers to the ability of agents to\nwork together effectively in a multi-agent environment\nto achieve a common goal. This includes coordinating\nactions, communicating intentions, and solving collec-\ntive problems.\n• Adaptive Learning refers to the ability of an agent to\ncontinuously improve its capabilities by learning from\npast experiences and feedback.\nThis ability includes\nCapability\nSC\nCiv\nSF\nDip\nWer\nStr\nStrategic Planning\nReal-Time Decision-Making\nSocial Reasoning\nTeam Coordination\nAdaptive Learning\nTable 3: Capability requirements across different games. SC: Star-\nCraft II, Civ: Civilization, SF: Street Fighter III, Dip: Diplomacy,\nWer: Werewolf, Str: Stratergo.\nidentifying patterns, refining strategies, and adapting be-\nhaviours to incorporate new information into subsequent\niterations, and the decision-making process is continu-\nously optimized for continued success.\n4.2\nDiverse Strategic Games\nAs demonstrated in Table 3, a set of strategic games was se-\nlected to ensure that each capability is adequately evaluated,\nwith factors such as game mechanics, difficulty, and other key\naspects being considered. These games present long-term,\nmultidimensional decision-making challenges and also assess\nfive key dimensions of cognitive decision-making capability\nthrough multifaceted evaluation perspectives. The following\nsections explain the specific challenges posed by the games\nchosen to evaluate these dimensions.\n• StarCraft II[Wikipedia, 2024c] is a complex real-time\nstrategy (RTS) game where players build bases, manage\nresources, raise armies, and destroy enemy bases. LLM-\nbased agents must make efficient decisions, optimize re-\nsource management, engage in strategic planning, and\nadapt to their opponents’ tactics in real-time within a\nrapidly changing and high-pressure environment.\n• Civilization[Wikipedia, 2024a] is a turn-based strat-\negy game where players lead a civilization from ancient\ntimes to the future. The game involves city-building,\nresource management, technological development, cul-\ntural growth, and diplomacy, with the goal of creating a\nstrong, prosperous civilization. LLM-based agents must\nmake long-term decisions, wisely allocate resources,\nplan future development, and engage in complex diplo-\nmatic negotiations.\n• Street Fighter III[Wikipedia, 2024e] is a fast-paced\nfighting game where players control characters with\nunique skills and combos to battle each other. LLM-\nbased agents must make quick decisions, execute pre-\ncise combos, and anticipate and counter their opponents’\nmoves in a high-pressure environment.\n• Diplomacy[Wikipedia, 2024b] is a multiplayer strategy\nboard game where players expand their territory through\nnegotiations, alliances, and betrayals. Each player con-\ntrols a country, and the goal is to gain an advantage\nthrough strategic positioning and diplomatic agreements.\nLLM-based agents must build alliances, manage com-\nplex diplomatic relationships, and predict opponents’ ac-\ntions.\n• Werewolf[Wikipedia, 2024f] is a social reasoning-\nbased multiplayer game where players are secretly as-\nsigned roles, with some being werewolves and others\nvillagers. The werewolves aim to destroy the villagers,\nwhile the villagers must identify the werewolves. LLM-\nbased agents need to make decisions with limited infor-\nmation, assess the credibility of others, and adjust their\nstrategies based on changing social dynamics.\n• Stratego[Wikipedia, 2024d] is a strategic board game\nwhere players move pieces on a board to capture the op-\nponent’s flag. The game emphasizes planning, bluffing,\nand reasoning about the opponent’s strategy. LLM-based\nagents must make decisions with incomplete informa-\ntion, predict their opponents’ actions, and conceal their\nown plans.\nFor a detailed description of the game mechanics, aciton\nspace and other information, please refer to the Appendix B.\n4.3\nFine-Grained Capability Metrics\nIn order to address the limitations of traditional capability\nassessment methods in dynamic and complex environments,\na fine-grained capability metric is introduced. The multiple\nmetrics is motivated by two empirical findings: (1) outcome-\nbased metrics are difficult to capture detailed differences be-\ntween capability dimensions, and (2) gamification assess-\nment requires the establishment of interpretable measurement\nbenchmarks. The metrics in DSGBench are defined based\non expert insights into the core mechanics of each game, en-\nsuring alignment with the key competencies being assessed.\nSpecifically, each metric is chosen to reflect a critical aspect\nthat influences strategic decision-making performance. As il-\nlustrated in Table 2, the proposed methodology first estab-\nlishes a mapping relationship between capability dimensions\nand games at the macro level, and then associates each game\nwith a set of fine-grained metrics at the micro level. More\ndetails of each metric please refer to Appendix B.\nAn example.\nWe take evaluating the Strategic Planning ca-\npability in the game StarCraft II as an example. Efficient re-\nsource management, which involves collecting and allocat-\ning minerals and gases, allows players to sustain their forces,\nwhile supply utilization governs their capacity to deploy units\neffectively.\nBased on these mechanics, fine-grained met-\nrics such as resource management efficiency and supply uti-\nlization are key for assessing a player’s strategic planning.\nSpecifically, Resource Collection Performance (RPM) mea-\nsures the efficiency of resource gathering by calculating the\ntotal amount of minerals and gases collected during the game,\nindicating how well a player manages resources to support\ntheir strategy. The formula for RPM is as follows:\nRPMi =\nT\nX\nt=1\n(collected mineralsi(t) + collected vespenei(t))\n(4)\nIn addition, the Supply Utilization Rate (SUR) evaluates\nunit production efficiency through the ratio of used supply\ncapacity to maximum supply capacity:\nSURi =\nPT\nt=1 supply usedi(t)\nPT\nt=1 supply capi(t)\n(5)\nWhen SUR is higher, it means that players are performing\nmore efficiently in resource management and unit production.\n4.4\nDecision Trajectory Tracking\nAs a complement to the quantitative evaluation, this paper\nintroduces a decision-tracking and behavioural analysis sys-\ntem that combines performance metrics with contextual anal-\nysis to deepen the understanding of LLM decision-making.\nThe system captures key decision points throughout the game,\nlinking them to real-time game states and mission objec-\ntives, thereby revealing underlying strategic reasoning pat-\nterns. Specifically, the analytical framework consists of three\ncore components: (1) action types that categorize specific de-\ncisions, such as resource allocation and unit production; (2)\ndecision contexts that capture the game state and objectives at\neach decision point; and (3) outcomes that assess the impact\nof decisions on game progression.\nThis approach enables the rapid identification of key deci-\nsion points and anomalies, providing insight into the decision\nlogic of LLMs and a basis for optimisation. Action type clas-\nsification helps to analyse the impact of decisions on game\nprogression, while decision contexts clarify the scope of a\nGame\nScene Variables\nScene Count\nDynamic Space\nMulti-goal\nPrompt Structure\nIterations\nStarCraft II[Ma et al., 2023b]\nMode, Opponent strategy, Difficulty level\n6\nHierarchical\n450\nCivilization[Qi et al., 2024]\nMap\n3\nHierarchical\n141\nStreet Fighter III[Wikipedia, 2024e]\nMode, Role\n2\n×\nFlat\n24\nDiplomacy[Mukobi et al., 2023]\nMap, Role\n1\n×\n×\nFlat\n60\nWerewolf[Bailis et al., 2024b]\nRole\n2\n×\n×\nFlat\n32\nStratego[Wikipedia, 2024d]\nMode, Board placement\n2\n×\nFlat\n1270\nTable 4: Overview of Selected Games in DSGBench and Their Core Characteristics. This table provides a summary of the key attributes of\nthe games featured in DSGBench, highlighting their scene variables, scene count, dynamic action space, goal multiplicity, prompt structures,\nand iterations. The analysis specifically focuses on characteristics such as interaction paradigms (Mode), roles, and game-specific dynamics.\ndecision’s influence. Outcome analysis evaluates the actual\nimpact of each decision on the game and the final outcome.\nFor a detailed description of the decision trajectory tracking\nmethodology, refer to Section 5.4.\n4.5\nCustomizable Evaluation Scenarios\nDespite the fact that the various strategic games employed in\nDSGBench encompass a broad range of decision-making ca-\npabilities, the fixed game settings may not adequately assess\nan agent’s multi-dimensional capabilities performance in dy-\nnamic environments. Consequently, our benchmark offers the\nflexibility to create customized evaluation scenarios, enabling\nmore targeted assessments based on specific needs and sup-\nporting future scenario expansions.\nAs demonstrated in Table 4, our benchmark facilitates pre-\ncise control over a range of scenario variables, including op-\nponent behaviour patterns, interaction paradigms (e.g., syn-\nchronous vs. asynchronous), prompt engineering approaches,\nand reasoning strategies. These variables can be customised\nto align with the distinct characteristics inherent in each game\ntype. Furthermore, a salient feature of our benchmark is the\nextensive interaction trajectories, with the average number of\niterations ranging from 24 in Street Fighter III to 1270 in\nStratego across different games. This presents a substantial\nchallenge for LLM-based agents, as these extended decision\nsequences require advanced capabilities in contextual learn-\ning, long-term strategic planning, and decision consistency\nacross varied game environments.\n4.6\nImplementation of DSGBench\nThe evaluation framework is an automated and simplified\nplatform designed to evaluate LLM-based agents uniformly.\nThe main component of the framework is the GameM-\nanager, which coordinates the initialization and execution\nphases. It is responsible for configuring the environment and\nthe agent, ensuring a smooth game flow and accurately track-\ning the decision trajectory. The process starts with the Data-\nCollector, which is responsible for collecting the basic con-\nfiguration of the game and the agent, laying the foundation\nfor accurate evaluation. This data is then fed into the modules\nGameEnv and HistoryTracker. While the former manages\nthe action and observation space and enables seamless inter-\naction between the agent and its environment, the latter cap-\ntures the detailed game history and allows for in-depth analy-\nsis of the decision-making process and strategic choices. This\nsetup provides a comprehensive view of the performance of\nLLM-based agents and helps to gain a deeper understanding\nof their behaviour and how the strategy of the agent evolves\nthroughout the game. For a detailed description of the archi-\ntectural design, please refer to Appendix A.\n5\nExperimental Results\nThis section presents the results of the evaluation of the six\nrepresentative LLMs on DSGBench. The following subsec-\ntions describe the evaluation setup, the main experimental re-\nsults, and both quantitative and qualitative results that high-\nlight the strengths and limitations of the models.\n5.1\nEvaluation Setup\nWe evaluated six representative LLMs on DSGBench, in-\ncluding closed-source models (GPT-4o[OpenAI, 2024], GPT-\n3.5-Turbo[OpenAI, 2023], Gemini 1.5 Flash[Reid et al.,\n2024]) and open-source models ( DeepSeek-V2.5[DeepSeek-\nAI,\n2024],\nLlama-3.1-8B-Instruct\nand\nLlama-3.1-70B-\nInstruct[Dubey et al., 2024]).\nScenario Design: The scenarios were designed to include\ncontrollable variables such as opponent strategies, difficulty\nlevels, and interaction modes, with settings that differ be-\ntween games. As illustrated in Table 5, for StarCraft II, the\nkey controllable variables include opponent strategy, execu-\ntion mode (synchronous\/asynchronous), and opponent diffi-\nculty, which is set to medium with integrated AI. In this study,\nthe map is selected as a classic race map. Each scenario will\nundergo a series of experiments to ensure robustness and to\naccount for the variability in agent performance across differ-\nent runs. The configuration settings of other game scenarios\ncan be found in the appendix B.\nExperimental Setup: All evaluations were conducted us-\ning standardized prompts without model fine-tuning to ensure\ntask consistency. For prompt engineering, we adopted ex-\nisting prompt templates for established game environments\n(StarCraft II[Ma et al., 2023b], Civilization[Qi et al., 2024],\nDiplomacy[Mukobi et al., 2023], Werewolf[Bailis et al.,\n2024b], and Street Fighter III[Wikipedia, 2024e]), while de-\nveloping custom prompt structures and reasoning frameworks\nfor Stratego[Wikipedia, 2024d]. The temperature parameter\nwas set to 0.2 across all LLMs to balance response deter-\nminism and creative reasoning optimally. For each game sce-\nnario, we evaluated models against either the built-in AI of the\ngame or GPT4o-mini as opponents, conducting 10 matches\nper scenario to ensure reliability. All game environments are\nuniformly encapsulated as text-based interfaces that imple-\nment standardized “observation-to-promp” and “ response-to-\naction” loops for consistent agent interaction. Additionally,\nCapabilities\nMetrics\nScene selection\nOpponent strategy\nOperation mode\nStrategic Planning\nRPM, EER, SUR, TCR\nMacro\nAsync\/Sync\nReal-time Decision-Making\nAPM, EPM\nRush\nAsync\/Sync\nAdaptive Learning\nWR, GA\nRandom\nAsync\/Sync\nTable 5: Outlines the scene setup for evaluating LLM-based agents\nin StarCraft II, including key capabilities, associated metrics, and\nscene configurations.\nModel\nStrategic Planning\nReal-Time Decision-Making\nSocial Reasoning\nTeam Collaboration\nAdaptive Learning\nOverall\nClosed-Sourced Models\nGemini 1.5 Flash[Reid et al., 2024]\n72.88 ± 2.12\n48.45 ± 1.42\n60.17 ± 1.82\n22.46 ± 3.35\n64.23 ± 1.39\n56.16 ± 1.70\nGPT-3.5 Turbo[OpenAI, 2023]\n32.94 ± 0.22\n52.32 ± 2.54\n74.25 ± 9.52\n26.18 ± 7.97\n47.68 ± 1.04\n47.01 ± 3.02\nGPT-4o[OpenAI, 2024]\n54.59 ± 6.69\n40.47 ± 1.76\n83.27 ± 2.20\n34.31 ± 1.86\n52.79 ± 1.86\n54.10 ± 1.14\nOpen-Sourced Models\nDeepSeek-V2.5[DeepSeek-AI, 2024]\n51.92 ± 5.27\n46.97 ± 1.57\n68.23 ± 2.62\n26.85 ± 3.02\n68.50 ± 1.82\n53.75 ± 1.72\nLlama-3.1-70B-Instruct[Dubey et al., 2024]\n51.47 ± 2.48\n66.35 ± 1.54\n40.78 ± 4.81\n26.33 ± 3.63\n34.35 ± 1.72\n45.11 ± 1.30\nLlama-3.1-8B-Instruct[Dubey et al., 2024]\n0.00 ± 0.00\n36.99 ± 1.12\n0.00 ± 0.00\n0.00 ± 0.00\n17.72 ± 0.19\n10.94 ± 0.24\nTable 6: Performance Evaluation Scores of LLMs. The table presents the evaluation scores for five dimensions: Strategic Planning, Real-Time\nDecision-Making, Social Reasoning, Team Collaboration, and Adaptive Learning. Additionally, the Overall Score is the weighted sum of the\nevaluation scores across these five capabilities, with both mean and standard deviation for each model. For each game scenario, the scores for\neach model are the averages of ten match runs, with both the mean and variance calculated.\nall games employed a text-based action space, where the va-\nlidity of actions was evaluated through the grounding accu-\nracy rate.\n5.2\nQuantitative Results\nThe experimental evaluation of DSGBench highlights distinct\nstrengths and limitations of LLM-based agents in different\nstrategic environments. As shown in Table 6, the quantitative\nresults summarise the models’ capabilities in strategic plan-\nning, real-time decision-making, social reasoning, team col-\nlaboration, and adaptive learning. In addition, the specific for-\nmula for calculating the capability scores is presented in the\nprevious section 3.2. These findings are further elaborated\nbelow, incorporating both quantitative metrics and qualitative\nobservations, culminating in a detailed analysis of decision-\nmaking trajectories.\nFine-grained capability metrics reveal distinct patterns\nin model performance across different cognitive dimen-\nsions. The comprehensive evaluation framework, encompass-\ning strategic planning, real-time decision-making, social rea-\nsoning, team collaboration, and adaptive learning, demon-\nstrates that models exhibit specialised strengths rather than\nuniform capabilities. This specialised performance is partic-\nularly evident in the case of Gemini 1.5 Flash [Reid et al.,\n2024], which achieves exceptional results in strategic plan-\nning (72.88) and adaptive learning (64.23), while showing\nlimitations in real-time decision-making (48.45). In contrast,\nGPT-4o [OpenAI, 2024] exhibits a more balanced set of capa-\nbilities across various metrics. It demonstrates a particularly\nstrong performance in social reasoning (83.27) and team col-\nlaboration (34.31), along with consistent scores above 40 in\nother dimensions.\nThe performance analysis reveals a substantial capa-\nbility gap between closed-source and open-source mod-\nels. Closed-source models have been shown to demonstrate\nsuperior performance, with Gemini 1.5 Flash [Reid et al.,\n2024] and GPT-4o [OpenAI, 2024] achieving overall scores\nof 56.16 and 54.10, respectively. In contrast, open-source al-\nternatives such as DeepSeek-V2.5[DeepSeek-AI, 2024] and\nLlama-3.1-70B-Instruct [Dubey et al., 2024] achieve signifi-\ncantly lower overall scores of 53.75 and 45.11, respectively.\nThe most pronounced disparity is observed in strategic plan-\nning tasks, where Gemini 1.5 Flash[Reid et al., 2024] (72.88)\nsignificantly outperforms Llama-3.1-70B-Instruct[Dubey et\nal., 2024] (51.47). However, in specific scenarios such as\nStarCraft’s real-time decision-making, open-source models\ncan achieve competitive performance, as demonstrated by\nLlama-3.1-70B-Instruct’s[Dubey et al., 2024] high score in\nthat dimension.\nGame-specific analysis further illuminates the relation-\nship between model architecture and task performance.\nIn strategic games such as Civilization, closed-source mod-\nels demonstrate clear advantages, with Gemini 1.5 Flash[Reid\net al., 2024] achieving a high score of 72.88 in strategic\nplanning.\nConversely, GPT-3.5 Turbo[OpenAI, 2023] ex-\nhibits diminished efficacy in this domain, attaining a score\nof 32.94.\nIn contrast, in real-time gaming environments,\nwhere the speed of decision-making is paramount, models\nsuch as Llama-3.1-70B-Instruct demonstrate superior perfor-\nmance with a score of 66.35 in real-time decision-making.\nThe observed variance in performance across different game\ntypes suggests that current model architectures may be opti-\nmised for specific cognitive tasks, potentially at the expense\nof others.\n5.3\nQualitative Analysis\nThe qualitative observations provide a complementary per-\nspective on the models’ capabilities, particularly in dealing\nwith complex, dynamic scenarios. In structured turn-based\nenvironments such as Civilization and Stratego, closed-source\nmodels excel at aligning intermediate decisions with long-\nterm goals. Take GPT-4o[OpenAI, 2024] as an example. It\nexcels at resource prioritisation and foreign policy formula-\ntion, allowing it to outperform open-source models. How-\never, even closed-source models occasionally fail to adapt to\nunforeseen in-game developments, such as unexpected oppo-\nnent strategies.\nIn contrast, real-time environments present a contrasting\nscenario. In StarCraft II, the dynamic nature of the battle-\nfield exposes temporal reasoning limitations across all mod-\nels. Despite its overall proficiency, GPT-3.5-Turbo[OpenAI,\n2023] struggles to balance resource gathering with tactical\ndecision-making under time constraints. Open-source mod-\nels face even greater challenges, frequently failing to respond\neffectively to rapid environmental changes. This behaviour\nunderscores fundamental gaps in the temporal reasoning ca-\npabilities of contemporary LLM architectures.\nSocial reasoning tasks, such as Diplomacy and Were-\nwolf, reveal additional complexities. GPT-4o[OpenAI, 2024]\ndemonstrates reasonable success in forming alliances and\nadapting strategies based on evolving game dynamics. For in-\nstance, it effectively identifies cooperative opportunities while\nmitigating adversarial risks. However, these successes are in-\nconsistent, with models sometimes reverting to overly sim-\nplistic negotiation tactics. Conversely, open-source models\nfrequently generate responses that are either redundant or ir-\nrelevant, thus exposing their limitations in handling complex\nmulti-agent interactions.\nStage\nTrajectory\nStep\nEarly\nDecision:\n0: <TRAIN PROBE>→1: <BUILD PYLON>→2: <BUILD NEXUS>→3: <TRAIN PROBE>→4: <BUILD ASSIMILATOR>\nOur Strategy: Expand the number of workers, build additional resource collection structures, and ensure base defense. Suggestions: Focus on building\nadditional Probes to boost resource gathering, construct more Pylons to increase supply cap, and consider expanding to new resource locations.\n429\nDecision:\n0: <TRAIN STALKER>→1: <BUILD GATEWAY>→2: <TRAIN PROBE>→3: <BUILD ASSIMILATOR>\n→4: <RESEARCH PROTOSSGROUNDARMORSLEVEL1>\nOur Strategy: Build a stable economy and gradually advance technology and the army.\nSuggestions: Continue to build up the economy while gradually increasing army supply and advancing technology to prepare for mid-game engagements.\n4629\nDecision:\n0: <BUILD GATEWAY>→1: <BUILD SHIELDBATTERY>→2: <TRAIN SENTRY>→3: <RESEARCH PROTOSSGROUNDAR-\nMORSLEVEL2>→4: <SCOUTING PROBE>\nOur Strategy: Build a solid economic foundation, maintain a basic military presence, and invest\nin early upgrades to enhance force capabilities. Suggestions: Consider diversifying our army composition by introducing more advanced units\nlike Sentries or Adepts to complement the existing Zealots and Stalkers. Focus on scouting to gather intelligence on the enemy’s strategy.\n7004\nMid\nDecision:\n0: <TRAIN SENTRY>→1: <BUILD ROBOTICSFACILITY>→2: <TRAIN IMMORTAL>→3: <BUILD ASSIMILATOR>→\n4: <RESEARCH PROTOSSGROUNDARMORSLEVEL2>\nOur Strategy: Improve the effectiveness of the army around balanced economy, unit composition\nand early upgrades. Suggestions: Consider diversifying your unit composition to include more advanced units like Sentries or Immortals to counter potential enemy\nthreats. Additionally, continue upgrading your technology to stay ahead in battles.\n8454\nDecision:\n0: <TRAIN IMMORTAL>→1: <TRAIN OBSERVER>→2: <TRAIN VOIDRAY>→3: <BUILD GATEWAY>→4: <BUILD ASSIMILATOR>\nOur Strategy: Focus on a mix of ground and air units, supported by upgrades. We are also investing in infrastructure for advanced unit production.\nSuggestions: Consider diversifying our army composition by training units from the Robotics Facility and Stargates to counter a wider range of enemy units.\n9554\nDecision:\n0: <TRAIN HIGHTEMPLAR>→1: <BUILD ROBOTICSBAY>→2: <RESEARCH EXTENDEDTHERMALLANCE>→3: <BUILD ASSIMILATOR>\n→4: <TRAIN OBSERVER>\nOur Strategy: Focus on upgrading units and technology while maintaining a balanced economy and military composition. Suggestions:\nConsider expanding unit diversity by incorporating more advanced units like Colossi or High Templars to enhance our army’s capabilities. Additionally, focus on\ntech upgrades to stay ahead in battles.\n10529\nMid to Late\nDecision:\n0: <BUILD GATEWAY>→1: <BUILD PHOTONCANNON>→2: <TRAIN STALKER>→3: <RESEARCH PROTOSSAIRARMORSLEVEL2>\n→4: <EXPAND TO NEW RESOURCE LOCATION>\nOur Strategy: Around the mix of gateway units and technical units such as Stalkers, Immorals, and Voidray.\nSuggestions: Enhance our unit composition by adding more production structures like Gateways to increase unit production capacity. Consider expanding tech options\nto counter potential enemy strategies effectively.\n14129\nDecision:\n0: <TRAIN STALKER>→1: <BUILD GATEWAY>→2: <BUILD SHIELDBATTERY>→3: <RESEARCH PROTOSSGROUNDARMORSLEVEL2>\n→4: <SCOUTING OBSERVER>\nOur Strategy: Around the unit diversity and balance method of technological progress. The emphasis on stalkers suggests a flexible\nmilitary composition capable of handling a variety of threats. Suggestions: Consider diversifying our unit composition further to adapt to potential enemy strategies.\nFocus on maintaining map control and scouting to stay ahead of the opponent.\n17554\nDecision:\n0: <TRAIN STALKER>→1: <BUILD GATEWAY>→2: <CHRONOBOOST CYBERNETICSCORE>→3: <RESEARCH PROTOSSAIRWEAPONS-\nLEVEL2>→4: <TRAIN IMMORTAL>\nOur Strategy: Focus on balanced unit composition and technology upgrades to improve the effectiveness of our forces.\nSuggestions: Consider diversifying our army composition further to counter the enemy’s Stalkers effectively. Focus on unit production and upgrades to strengthen our army.\n20929\nTable 7: Presents a detailed decision trajectory analysis of an LLM’s gameplay in StarCraft II across various stages: Early, Mid, and Mid to\nLate. The table outlines specific decisions made by the model, including the training of units, building structures, and conducting research.\nEach entry details the decision-making process, the associated strategy, and suggestions for optimizing performance. By capturing these\ntrajectories, the table illustrates how the LLM navigates complex strategic choices, adapts to the game environment, and develops its military\nand economic strategies over time, providing insights into its strategic reasoning capabilities.\n(a) Strategic Planning - EER\n(b) Real-Time Decision-Making - EPM\n(c) Adaptive Learning - GA\nFigure 2: Performance indicators for evaluating LLM capabilities in StarCraft II: (a) Strategic Planning - EER (Efficiency of Resource\nUtilization), (b) Real-Time Decision-Making - EPM (Effective Actions Per Minute), and (c) Adaptive Learning - GA (Grounding Accuracy).\nEach graph displays the performance trends of different game sessions (Game0, Game1, Game2, Game3) over time steps.\n5.4\nResults of Decision Trajectory Tracking\nAnalysis\nThis section examines the LLM’s decision-making process\nwithin the context of the StarCraft II environment, utilis-\ning three key metrics: Strategic Planning(EER), Real-Time\nDecision-Making (EPM), and Adaptive Learning (GA). As\nillustrated by Figure 2, the decision trajectory over time is\ncharacterised by shifts in the LLM’s behaviour. For instance,\nat step 15k in Game 1, a significant decrease in EER coin-\ncides with a shift in strategy from resource gathering to mili-\ntary production, as shown in Table 7. Furthermore, EPM in-\ncreases during combat phases, particularly between the ”Mid”\nand ”Late” stages, aligning with higher decision-making com-\nplexity, such as advanced unit production and strategic up-\ngrades. The LLM also demonstrates adaptability by switch-\ning to specialised units such as Void Rays and Immortals in\nresponse to changing game conditions, as reflected in the data\nat step 10k in Table 7.\nThe insights derived from Figure 2 and Table 7 collec-\ntively provide a more precise understanding of the evolution\nof LLM decision-making. The figure provides a visual repre-\nsentation of decision trends, while the table provides specific\ndata points that explain the rationale behind key actions. For\nexample, the drop in EER at level 15k reflects a strategic pivot\ntowards military production, and the increase in EPM at level\n25k corresponds to decisions to expand infrastructure and up-\ngrade units.\n6\nConclusion\nWe introduce DSGBench, a comprehensive benchmark de-\nsigned to evaluate the strategic decision-making capabilities\nof LLM-based agents in diverse and dynamic gaming environ-\nments. For the first time, we assess LLM-based agents based\non key cognitive decision-making dimensions from human\ncognition and propose an integrated evaluation approach. Un-\nder standardized settings, we systematically evaluate the per-\nformance of six representative LLM-based agents in complex\nstrategic environments. Through fine-grained evaluation met-\nrics and decision trajectory analysis, we reveal the strengths\nand weaknesses of agents in various scenarios. Experimen-\ntal results show significant differences across multiple ability\ndimensions. Additionally, we have established a unified eval-\nuation framework that supports the integration of new games\nand the customization and expansion of new game scenarios.\nWe hope that DSGBench will see widespread application, as\ngaming itself is an evolving process. Agents can continuously\nlearn and evolve through interaction with opponents, making\ngame-based evaluation methods virtually limitless in poten-\ntial.\nWhile existing agents face significant challenges in DSG-\nBench, this analytical framework provides a concrete analysis\nfor the improvement of LLM-based agents’ integrated cogni-\ntive decision-making capabilities. Two important directions\nfor future research include the development of a unified tra-\njectory dataset for strategy games and the creation of an agent\nreasoning framework for multi-strategy games. The trajec-\ntory dataset would serve as a rich resource for training agents\nacross a range of strategic environments. Together, these will\nenhance the generalization capabilities of LLM-based agents\nand further the development of more complex AGI-oriented\nsystems.\nReferences\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai\nTang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan\nZeng, Lei Hou, et al. Longbench: A bilingual, multitask\nbenchmark for long context understanding. arXiv preprint\narXiv:2308.14508, 2023.\nSuma Bailis, Jane Friedhoff, and Feiyang Chen. Werewolf\narena: A case study in llm evaluation via social deduction.\narXiv preprint arXiv:2407.13943, 2024.\nSuma Bailis, Jane Friedhoff, and Feiyang Chen. Werewolf\narena: A case study in llm evaluation via social deduction.\narXiv preprint arXiv:2407.13943, 2024.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi\nYang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang\nWang, Yidong Wang, et al. A survey on evaluation of large\nlanguage models. ACM Transactions on Intelligent Systems\nand Technology, 15(3):1–45, 2024.\nAnthony Costarelli, Mat Allen, Roman Hauksson, Grace So-\ndunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, and\nArjun Yadav. Gamebench: Evaluating strategic reasoning\nabilities of llm agents. arXiv preprint arXiv:2406.06613,\n2024.\nDeepSeek-AI. Deepseek-v2: A strong, economical, and effi-\ncient mixture-of-experts language model, 2024.\nJinhao Duan, Renming Zhang, James Diffenderfer, Bhavya\nKailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal,\nTianlong Chen, and Kaidi Xu. Gtbench: Uncovering the\nstrategic reasoning limitations of llms via game-theoretic\nevaluations. arXiv preprint arXiv:2402.12348, 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024.\nWard Edwards. Dynamic decision theory and probabilistic in-\nformation processings. Human factors, 4(2):59–74, 1962.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with\nlanguage model is planning with world model.\narXiv\npreprint arXiv:2305.14992, 2023.\nShibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan\nShao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya\nSamavedhi, Qiyue Gao, et al. Llm reasoners: New eval-\nuation, library, and analysis of step-by-step reasoning with\nlarge language models. arXiv preprint arXiv:2404.05221,\n2024.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng,\nJinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta pro-\ngramming for multi-agent collaborative framework. arXiv\npreprint arXiv:2308.00352, 2023.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jing-\nhan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,\nYikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-\ndiscipline chinese evaluation suite for foundation models.\nAdvances in Neural Information Processing Systems, 36,\n2024.\nEdwin Hutchins. Cognition in the Wild. MIT press, 1995.\nDaniel Kahneman. Thinking, fast and slow. macmillan, 2011.\nZhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel\nCortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang,\nand Andrew Ahn.\nLyfe agents: Generative agents for\nlow-cost real-time social interactions.\narXiv preprint\narXiv:2310.02172, 2023.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei,\nHanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan\nYang, et al. Agentbench: Evaluating llms as agents. arXiv\npreprint arXiv:2308.03688, 2023.\nWeiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin,\nHaifeng Zhang, and Jun Wang.\nLarge language models\nplay starcraft ii: Benchmarks and a chain of summariza-\ntion approach. arXiv preprint arXiv:2312.11865, 2023.\nWeiyu Ma, Qirui Mi, Yongcheng Zeng, Xue Yan, Yuqiao Wu,\nRunji Lin, Haifeng Zhang, and Jun Wang. Large language\nmodels play starcraft ii: Benchmarks and a chain of sum-\nmarization approach.\narXiv preprint arXiv:2312.11865,\n2023.\nChang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu\nYang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and\nJunxian He. Agentboard: An analytical evaluation board\nof multi-turn llm agents. arXiv preprint arXiv:2401.13178,\n2024.\nZhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic\nmulti-robot collaboration with large language models. In\n2024 IEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 286–299. IEEE, 2024.\nGabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis\nHammond, Alan Chan, and Jesse Clifton. Welfare diplo-\nmacy: Benchmarking language model cooperation. arXiv\npreprint arXiv:2310.08901, 2023.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang,\nYingbo Zhou,\nSilvio Savarese,\nand Caiming\nXiong.\nCodegen:\nAn open large language model for\ncode with multi-turn program synthesis.\narXiv preprint\narXiv:2203.13474, 2022.\nOpenAI.\nGpt-3.5 turbo.\nhttps:\/\/platform.openai.com\/docs\/\nmodels\/gpt-3-5, 2023.\nOpenAI. Hello gpt-4o, 2024.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bernstein.\nGenerative agents: Interactive simulacra of human behav-\nior. In Proceedings of the 36th annual acm symposium on\nuser interface software and technology, pages 1–22, 2023.\nSiyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang,\nBangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan\nZhang, Zhaowei Zhang, et al.\nCivrealm:\nA learning\nand reasoning odyssey in civilization for decision-making\nagents. arXiv preprint arXiv:2401.10568, 2024.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang,\nJiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin\nCong, et al. Chatdev: Communicative agents for software\ndevelopment. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume\n1: Long Papers), pages 15174–15186, 2024.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry\nLepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu\nSoricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-\ntwieser, et al.\nGemini 1.5: Unlocking multimodal un-\nderstanding across millions of tokens of context.\narXiv\npreprint arXiv:2403.05530, 2024.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Lan-\nguage models can teach themselves to use tools. Advances\nin Neural Information Processing Systems, 36:68539–\n68551, 2023.\nXiao Shao, Weifu Jiang, Fei Zuo, and Mengqing Liu.\nSwarmbrain: Embodied agent for real-time strategy game\nstarcraft ii via large language models.\narXiv preprint\narXiv:2401.17749, 2024.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui\nZhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered\nShwartz.\nClever hans or neural theory of mind? stress\ntesting social reasoning in large language models. arXiv\npreprint arXiv:2305.14763, 2023.\nHao Tang, Darren Key, and Kevin Ellis.\nWorldcoder, a\nmodel-based llm agent: Building world models by writing\ncode and interacting with the environment. arXiv preprint\narXiv:2402.12275, 2024.\nOriol Vinyals,\nTimo Ewalds,\nSergey Bartunov,\nPetko\nGeorgiev, Alexander Sasha Vezhnevets, Michelle Yeo,\nAlireza Makhzani, Heinrich K¨uttler, John Agapiou, Julian\nSchrittwieser, et al. Starcraft ii: A new challenge for re-\ninforcement learning.\narXiv preprint arXiv:1708.04782,\n2017.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,\nMicha¨el Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev,\net al.\nGrandmaster level in starcraft ii\nusing\nmulti-agent\nreinforcement\nlearning.\nnature,\n575(7782):350–354, 2019.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023.\nJiawei Wang, Kai Wang, Runze Wu, Bihan Xu, Lingeng\nJiang, Shiwei Zhao, Renyu Zhu, Haoyu Liu, Zhipeng Hu,\nZhong Fan, LILE, Tangjie Lv, and Changjie Fan. Digital\nplayer: Evaluating large language models based human-\nlike agent in games, 2024.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,\nJingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\nYankai Lin, et al.\nA survey on large language model\nbased autonomous agents. Frontiers of Computer Science,\n18(6):186345, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,\nFei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in neural information processing systems,\n35:24824–24837, 2022.\nWikipedia. Civilization, 2024. Accessed: 2024-09-23.\nWikipedia. Diplomacy, 2024. Accessed: 2024-09-23.\nWikipedia. Starcraft ii, 2024. Accessed: 2024-09-23.\nWikipedia. Stratego, 2024. Accessed: 2024-09-23.\nWikipedia. Street fighter iii, 2024. Accessed: 2024-09-23.\nWikipedia. The werewolves of millers hollow, 2024. Ac-\ncessed: 2024-09-23.\nYue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smart-\nplay: A benchmark for llms as intelligent agents. arXiv\npreprint arXiv:2310.01557, 2023.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding,\nBoyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu\nZhou, et al. The rise and potential of large language model\nbased agents: A survey. arXiv preprint arXiv:2309.07864,\n2023.\nZhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong,\nHonglin Guo, Junzhe Wang, Dingwen Yang, Chenyang\nLiao, Xin Guo, Wei He, et al. Agentgym: Evolving large\nlanguage model-based agents across diverse environments.\narXiv preprint arXiv:2406.04151, 2024.\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou,\nYuandong Tian, Yanghua Xiao, and Yu Su.\nTravelplan-\nner: A benchmark for real-world planning with language\nagents. arXiv preprint arXiv:2402.01622, 2024.\nLin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen\nDong, Kurt Keutzer, See-Kiong Ng, and Jiashi Feng.\nMagic: Investigation of large language model powered\nmulti-agent in cognition, adaptability, rationality and col-\nlaboration. In ICLR 2024 Workshop on Large Language\nModel (LLM) Agents, 2023.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan.\nAgieval:\nA human-centric bench-\nmark for evaluating foundation models.\narXiv preprint\narXiv:2304.06364, 2023.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert\nLo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel\nFried, Uri Alon, et al. Webarena: A realistic web envi-\nronment for building autonomous agents. arXiv preprint\narXiv:2307.13854, 2023.\nA\nOverall Architecture\nThe proposed evaluation framework introduces an automated architecture for systematically evaluating LLM-based agents in\nvarious game environments. A unified gym interface is implemented for all games, which facilitates subsequent game exten-\nsions. In addition, the framework provides easy scenario customization to tailor the evaluation requirements based on a single\nprofile.\nThe key component of the framework is the GameManager, which coordinates the evaluation process’s initialization and\nexecution phases. The architecture consists of three main modules: DataCollector, GameEnv, and HistoryTracker. The Data-\nCollector module handles the game and agent configurations through parameter parsing and establishes the base parameters for\nthe evaluation. The GameEnv module defines the action and observation space and facilitates the interaction between agents\nand the evaluation through a standardized interface. The GameEnv module defines the action and observation space while fa-\ncilitating the interaction between the agents and the environment through a standardized interface. The HistoryTracker module\ncomprehensively records the game progress, agent model, and match-specific information.\nDuring initialization, the GameManager coordinates component configuration through calls to GameEnv(args) and\nAgent(args), followed by prompt processing through Prompt(args) and LLM(args). The execution phase manages state transi-\ntions via env.step(action), facilitates agent decisions via agent.action(obs), and records state information via HistoryTracker The\nframework supports a wide range of game genres, including real-time strategy games (StarCraft II), turn-based strategy games\n(Civilization), fighting games (Street Fighter III), and social deduction games (Werewolf, Diplomacy). This versatility allows\nfor comprehensive evaluation in different decision-making environments.\nFigure 3: Code Architecture of DSGBench Framework.\nB\nStarCraft II\nStarCraft II is a real-time strategy (RTS) game developed by Blizzard Entertainment. It is the successor to StarCraft: Broodwar,\none of the most successful RTS games. The objective of StarCraft II is to establish bases, manage resources, build armies,\nand destroy the enemy’s base. The most common game setup is 1v1, where each player chooses one of three races: Terran,\nProtoss, or Zerg. Each race has unique units, buildings, and mechanics, which result in different strategic approaches. Players\nstart with a small base and a few units, and they gather resources to build additional units and structures. Additionally, players\nscout their opponent and develop new tactics. Numerous resources are available for learning StarCraft II, including Battle.net,\nTeamLiquid, and Wikia.\nAs a highly complex RTS game, StarCraft II provides an ideal environment for evaluating LLM-based agents. The game\ndemands quick decision-making, efficient resource management, strategic planning, and the ability to adapt to the opponent’s\ntactics in real-time. LLM-based agents must make effective decisions in high-pressure, rapidly changing environments while\nalso adjusting strategies over the long term.\nStarCraft II has become a critical platform for reinforcement learning (RL) research[Vinyals et al., 2017]. DeepMind’s\nAlphaStarVinyals et al. [2019], for example, used deep reinforcement learning to defeat several professional players, demon-\nstrating the potential of RL in dynamic, complex environments, particularly in real-time decision-making and long-term strategic\nplanning. However, AlphaStar’s limitations in decision-making transparency have opened up further avenues for research.\nWith the rise of LLMs such as the GPT series[OpenAI, 2023, 2024], researchers have started exploring the application of\nLLMs to StarCraft II. However, the rapid decision-making required in RTS games presents significant challenges for directly\napplying LLMs. To address this, the research team[Ma et al., 2023b] developed the TextStarCraft II environment and introduced\nthe Chain of Summarization (CoS) method, which successfully defeated the LV5 built-in AI. Furthermore, the SwarmBrain\nsystemShao et al. [2024], which combines the Overmind intelligent matrix and Swarm ReflexNet, has further enhanced the\nperformance of LLMs in strategic decision-making.\nB.1\nGame Detailed Description\nA detailed description of the game settings, unit configurations, opening strategies, and goals is provided in Table 8.\nB.2\nAction Space\nIn StarCraft II, five types of actions are provided for the LLM agent, as defined in the TextStarCraft II [Ma et al., 2023b].\n• Train Units: Train 18 types of player’s units including PROBE, ZEALOT, etc.\n• Build Structure: Build 15 types of player’s buildings including PYLON, NEXUS, etc.\n• Research Technique: Updates on the player’s technological advancements, Specifically, includes 26 sub-actions.\n• Scouting: Use the appropriate type of user unit to scout the enemy situation, including 4 sub-actions.\n• Battle: Mainly combat strategies such as MULTI-ATTACK, MULTI-RETREAT, and CHRONOBOOST NEXUS.\nB.3\nMetrics\nFurther details on the metrics and their corresponding formulas for Starcraft II can be found in Table 9.\nB.4\nExample Trajectory\n• Early game\nGame time:00:00\nDecisions:\n0: <TRAIN PROBE> →1: <TRAIN PROBE> →2: <TRAIN PROBE> →3: <TRAIN PROBE> →4:\n<SCOUTING PROBE>\nOur Strategy: The strategy is currently undefined, as the game has just begun. The initial focus will be on\nexpanding the economy.\nEnemy’s Strategy: Unknown, as the game has just started.\nSuggestions:\n1).\nOur Strategy: Focus on rapid expansion and early scouting to determine the enemy’s race and strategy. A fast\nexpansion strategy is generally favorable for Protoss in the early game.\n2).\nUnits and Buildings: Continue producing Probes to expand our mining capabilities. Build additional Pylons\nas needed to increase our supply cap. Consider sending out a scouting Probe soon.\n3).\nEconomy: Maximize mineral collection by ensuring all Probes are assigned to mining. Focus on expanding to\na second base as soon as possible.\n4).\nTechnology: No technology research is needed at this stage. Focus on economic expansion first.\n• Early-mid game\nGame time:5:10\nDecisions:\n0:\n<TRAIN PROBE> →1:\n<BUILD PYLON> →2:\n<RESEARCH PROTOSSGROUNDWEAPON-\nSLEVEL1> →3: <SCOUTING PROBE> →4: <EMPTY ACTION>\nOur Strategy: A standard early expansion strategy with a focus on Zealots. The Robotics Facility suggests a\npotential transition towards more advanced units later in the game.\nEnemy’s Strategy: The presence of a single Probe suggests the enemy is also in the early stages of the game. More\ninformation is needed to determine their strategy.\nSuggestions:\n1).\nUnits and Buildings: We have 32 Probes, 9 Zealots, two Nexuses, five Pylons, three Assimilators, two Gate-\nways, one Warp Gate, one Cybernetics Core, and one Robotics Facility. We are currently building one Nexus,\ntwo Pylons, one Assimilator, and one Gateway. We are also training two more Zealots.\n2).\nEconomy: Our economy is developing. Mineral income is low (240), but gas income is decent (894). We need\nto focus on increasing mineral income by expanding our worker count and mining efficiency.\n3).\nTechnology:We have researched Warp Gates, which is a significant early-game advantage. The Robotics Fa-\ncility opens up the possibility of producing more advanced units later.\n• Mid-to-late game\nGame time:13:18\nGame Manual\nSetting\nMap specification\nStandard 1v1 map with mining, gas, expansion points, obstacle\nterrain, and other elements (e.g. map: Jagannatha LE).\nNumber of players\n2 players per match against each other.\nResource type\nTwo main resources - minerals and gases, used for unit production\nand technological upgrading.\nUnit Configuration\nand Policy\nBasic Unit Configuration\n12 farmers (SCV\/Probe\/Drone) for resource collection.\n1 main base (Command Center\/Nexus\/Hatchery).\n1 Supply Depot (Pylon\/Overlord) to control the population cap.\nEthnic Divisions\nTerran: Focuses on mechanical units and air power, with strong\ndefensive and multi-functional building capabilities.\nProtoss: Has shields and powerful individual units, but is\nslower to produce.\nZerg: Unit production is fast, relying on massive ground forces\nand good ecological control.\nUnit Production\nand Technology Tree\nTerran: Can produce ground units (e.g., Marine, Marauder) and\nair force units (e.g., Viking, Banshee).\nProtoss: Can produce high-attack units (e.g., Zealot, Stalker)\nand powerful air units (e.g., Carrier, Phoenix).\nZerg: Can produce a large number of cheap units (e.g., Zergling,\nHydralisk) and high-tech units (e.g., Mutalisk, Ultralisk).\nFixed Opening Strategy\nInitial Base Strategy\nRapid Expansion: Quickly establish a second base to enhance\neconomic output and increase resource collection speed.\nQuick Attack: Quickly produce early combat units, directly\nattack enemy bases, forcing opponents to defend.\nDefensive Strategy: Strengthen fortifications (e.g., Terran’s Bunker,\nProtoss Photon Cannon) to delay enemy attacks and save strength\nfor later development.\nArmy Layout and Defense\nDefensive Arrangement: Arrange defensive units near the base\nto ensure the safety of the mining area and the main base.\nExamples: Terran’s Bunker, Protoss Shield Battery, Zerg’s Spine Crawler.\nOffensive Placement: Deploy units to harass and control key\nlocations on the map, such as enemy resource points.\nGoals\nEconomic Development\nThrough the collection of minerals and gases, rapidly develop\nthe economy and technology to establish a powerful army.\nMilitary Victory\nDestroy all enemy main bases or render them incapable of reproducing units.\nMap Control\nCapture key positions on the map (e.g., resource points, highlands)\nand use tactical advantage to overwhelm the opponent’s economy.\nTable 8: Game manual detailing the settings, unit configuration and policy, fixed opening strategies, and goals for gameplay in StarCraft II.\nMetric\nDescription and Formula\nRPM (Resource Per Minute)\nQuantifies the efficiency of resource gathering. RPM is calculated as:\nRPM = collected minerals + collected vespene\ngame time\nEER (Efficiency of Resource\nUtilization)\nEvaluates the utilization efficiency of collected resources. EER is calculated as:\nEER =\n\u0012\nspent minerals + spent vespene\ncollected minerals + collected vespene\n\u0013\n× 100%\nSUR (Supply Usage Rate)\nMeasures the efficiency of supply usage in a game. SUR is calculated as:\nSUR =\n\u0012supply used\nsupply cap\n\u0013\n× 100%\nTCR\n(Technology\nComple-\ntion Rate)\nEvaluates the ratio of available technologies to the total number of technologies.\nTCR is calculated as:\nTCR =\n\u0012Available Technologies\nTotal Technologies\n\u0013\n× 100%\nAPM (Actions Per Minute)\nQuantifies the speed and frequency of LLM-based agents’ outputs in the game\nenvironment. APM is calculated as:\nAPM =\nmodel outputs\ngame time in minutes\nEPM (Effective Actions Per\nMinute)\nMeasures the efficiency and impact of LLM-based agents’ actions. EPM is\ncalculated as:\nEPM =\neffective actions\ngame time in minutes\nWR (Win Rate)\nPercentage of matches won against different levels of opponents. WR is calcu-\nlated as:\nWR =\n\u0012 matches won\ntotal matches\n\u0013\n× 100%\nGA (Grounding Accuracy)\nEvaluates the precision and success rate of the grounding actions. GA is calcu-\nlated as:\nGA =\n\u0012 successful groundings\ntotal grounding attempts\n\u0013\n× 100%\nTable 9: Metrics and their corresponding formulas for evaluating the performance of LLM-based agents in Starcraft II.\nDecisions:\n0: <TRAIN PROBE> →1: <TRAIN PROBE> →2: <TRAIN PROBE> →3: <BUILD PYLON> →4:\n<RESEARCH PROTOSSGROUNDWEAPONSLEVEL1>\nOur Strategy: Our strategy appears to be a balanced approach, combining strong economic expansion with a\ndiverse army composition. The focus on Warp Gates suggests an aggressive push, but the low mineral count is\nhindering this.\nEnemy’s Strategy: The enemy appears to be mirroring our strategy to some extent, with Stalkers and Observers\npresent. Their smaller army size suggests a more defensive or less aggressive approach than ours, or possibly a\ndifferent strategic focus.\nSuggestions:\n1).\nUnits and Buildings: We have a balanced army composition with Zealots, Stalkers, and Immortals, suggesting\na versatile approach. The high number of Warp Gates indicates a focus on rapid unit production. The presence\nof a Robotics Facility and Robotics Bay suggests a commitment to heavier units. However, the low mineral\ncount is concerning.\n2).\nEconomy: Our gas income is high (3231-3251), but our mineral income is low (455-485), which is a signif-\nicant problem. This imbalance needs immediate attention. The large number of workers suggests potential\ninefficiencies in mineral gathering.\n3).\nTechnology: Warpgate research is complete, which is excellent for fast unit production. Protoss Air Weapons\nLevel 1 is nearly complete. We have access to advanced units like Immortals, indicating a good technological\nprogression.\nB.5\nPrompt Example\nSystem Prompt\nYou are an AI trained in analyzing and summarizing StarCraft II games. You understand the nuances and strategies of the\nprotoss race.\nBased on the summaries of multiple rounds in a game, we want you to analyze the game progression in a structured way.\nYour analysis should include the following aspects:\n1 Game Overview: Provide a brief overview of the current situation based on all the rounds.\n2 Current Game Stage: Determine the stage of the game based on the information of all rounds. Is it the early game,\nmid-game, or late game?\n3 Our Situation: Describe our current status in terms of:\n3.1 Units and Buildings: Analyze the state of our units and buildings.\n3.2 Economy: Evaluate our economic condition, including resource collection and usage.\n3.3 Technology: Describe the status of our technological research and what technologies we have unlocked so far.\nAnalyze our technology tree, indicating the available and potential upgrades or units.\n4 Our Strategy: Infer our potential strategy based on our current situation and the information of all rounds.\n5 Enemy’s Strategy: Infer the enemy’s potential strategy, based on the available information.\n6 Key Information: Highlight the most important aspects from all rounds that have significantly influenced the game.\nFor Protoss, keep an eye on Nexus’s energy to Chrono Boost important structures.\nBased on the game situation and strategies used by both sides, provide specific suggestions for the following areas:\n1 Our Strategy: Propose adjustments to our current strategy to counter the enemy’s moves and capitalize on our strengths.\n2 Units and Buildings: Offer ways to enhance our unit composition and improve our building layout, suited to the current\nstage of the game.\n3 Economy: Recommend better practices for resource gathering and usage, in line with our strategic needs.\n4 Technology: Suggest focused research paths to gain technological advantages, considering our current research status\nand technology tree.\nLastly, consider the current situation and the suggestions provided, make 5 actionable and specific decisions from the action\ndictionary’TRAIN UNIT’: 0: ’TRAIN PROBE’, 1: ’TRAIN ZEALOT’, 2: ’TRAIN ADEPT’, 3: ’TRAIN STALKER’, 4:\n’TRAIN SENTRY’, 5: ’TRAIN HIGHTEMPLAR’, 6: ’TRAIN DARKTEMPLAR’, 7: ’TRAIN VOIDRAY’, 8: ’TRAIN\nCARRIER’, 9: ’TRAIN TEMPEST’, 10: ’TRAIN ORACLE’, 11: ’TRAIN PHOENIX’, 12: ’TRAIN MOTHERSHIP’,\n13: ’TRAIN OBSERVER’, 14: ’TRAIN IMMORTAL’, 15: ’TRAIN WARPPRISM’, 16: ’TRAIN COLOSSUS’, 17:\n’TRAIN DISRUPTOR’, 18: ’MORPH ARCHON’, ’BUILD STRUCTURE’: 19: ’BUILD PYLON’, 20: ’BUILD ASSIM-\nILATOR’, 21: ’BUILD NEXUS’, 22: ’BUILD GATEWAY’, 23: ’BUILD CYBERNETICSCORE’, 24: ’BUILD FORGE’,\n25: ’BUILD TWILIGHTCOUNCIL’, 26: ’BUILD ROBOTICSFACILITY’, 27: ’BUILD STARGATE’, 28: ’BUILD TEM-\nPLARARCHIVE’, 29: ’BUILD DARKSHRINE’, 30: ’BUILD ROBOTICSBAY’, 31: ’BUILD FLEETBEACON’, 32:\n’BUILD PHOTONCANNON’, 33: ’BUILD SHIELDBATTERY’, ’RESEARCH TECHNIQUE’: 34: ’RESEARCH WARP-\nGATERESEARCH’, 35: ’RESEARCH PROTOSSAIRWEAPONSLEVEL1’, 36: ’RESEARCH PROTOSSAIRWEAPON-\nSLEVEL2’, 37: ’RESEARCH PROTOSSAIRWEAPONSLEVEL3’, 38: ’RESEARCH PROTOSSAIRARMORSLEVEL1’,\n39: ’RESEARCH PROTOSSAIRARMORSLEVEL2’, 40: ’RESEARCH PROTOSSAIRARMORSLEVEL3’, 41: ’RE-\nSEARCH ADEPTPIERCINGATTACK’, 42:\n’RESEARCH BLINKTECH’, 43:\n’RESEARCH CHARGE’, 44:\n’RE-\nSEARCH PROTOSSGROUNDWEAPONSLEVEL1’, 45: ’RESEARCH PROTOSSGROUNDWEAPONSLEVEL2’, 46:\n’RESEARCH PROTOSSGROUNDWEAPONSLEVEL3’, 47:\n’RESEARCH PROTOSSGROUNDARMORSLEVEL1’,\n48: ’RESEARCH PROTOSSGROUNDARMORSLEVEL2’, 49: ’RESEARCH PROTOSSGROUNDARMORSLEVEL3’,\n50: ’RESEARCH PROTOSSSHIELDSLEVEL1’, 51: ’RESEARCH PROTOSSSHIELDSLEVEL2’, 52: ’RESEARCH\nPROTOSSSHIELDSLEVEL3’, 53:\n’RESEARCH EXTENDEDTHERMALLANCE’, 54:\n’RESEARCH GRAVITIC-\nDRIVE’, 55: ’RESEARCH OBSERVERGRAVITICBOOSTER’, 56: ’RESEARCH PSISTORMTECH’, 57: ’RESEARCH\nVOIDRAYSPEEDUPGRADE’, 58: ’RESEARCH PHOENIXRANGEUPGRADE’, 59: ’RESEARCH TEMPESTGROUN-\nDATTACKUPGRADE’, ’OTHER ACTION’: 60: ’SCOUTING PROBE’, 61: ’SCOUTING OBSERVER’, 62: ’SCOUT-\nING ZEALOT’, 63: ’SCOUTING PHOENIX’, 64: ’MULTI-ATTACK’, 65: ’MULTI-RETREAT’, 66: ’CHRONO-\nBOOST NEXUS’, 67: ’CHRONOBOOST CYBERNETICSCORE’, 68: ’CHRONOBOOST TWILIGHTCOUNCIL’, 69:\n’CHRONOBOOST STARGATE’, 70: ’CHRONOBOOST FORGE’, 71: ’EMPTY ACTION’. This dictionary comprises\nfour categories of actions: unit production, building construction, technology research, and other actions. Remember to\nalign these decisions with the current stage of the game, and avoid proposing actions that are not currently feasible.\nUser Prompt\nGame Observation\nchunk{0}:\nAt 10:07 game time, our current StarCraft II situation is as follows:\nResources:\n- Game time: 10:07\n- Worker supply: 15\n- Mineral: 5390\n- Gas: 328\n- Supply left: 8\n- Supply cap: 31\n- Supply used: 23\n- Army supply: 6\nBuildings:\n- Nexus count: 1\n- Pylon count: 2\n- Gas buildings count: 2\n- Warp gate count: 4\n- Cybernetics core count: 1\n- Twilight council count: 1\n- Robotics facility count: 1\nUnits:\n- Probe count: 15\n- Zealot count: 2\n- Stalker count: 1\nPlanning:\nPlanning structure:\n- Planning robotics facility count: 1\nResearch:\nCybernetics core:\n- Warpgate research status: 1\nchunk{1}: ...\nUser Prompt\nLLM Suggestions & Decisions\nSuggestions:\n1 Our Strategy:\n– Expand Army: Given our high mineral reserves, focus on expanding our army to prepare for potential engage-\nments.\n– Diversify Units: Introduce more advanced units like Adepts and Immortals once Blink research is complete.\n2 Units and Buildings:\n– Build More Warp Gates: Increase the number of Warp Gates to facilitate faster unit production.\n– Expand Supply: Build additional Pylons to increase supply cap and support more units.\n3 Economy:\n– Optimize Gas Collection: Ensure that our gas collection is efficient to support advanced unit production and\nresearch.\n– Expand Bases: Consider expanding to additional bases to further boost our economy.\n4 Technology:\n– Complete Blink Research: Prioritize completing Blink research to enhance our Stalkers’ mobility.\n– Research Upgrades: Consider researching Protoss Ground Weapons and Armor upgrades to improve our unit\neffectiveness.\nDecisions:\n1. TRAIN STALKER\n2. BUILD PYLON\n3. BUILD WARPGATE\n4. RESEARCH PROTOSSGROUNDWEAPONSLEVEL1\n5. CHRONOBOOST CYBERNETICSCORE\nUser Prompt\nLLM Analysis\n1. Game Overview:\nAt 10:07 game time, our current situation is characterized by a strong economy with substantial mineral reserves but\nrelatively low gas resources. We have a modest army supply and are in the process of researching key technologies.\n2. Current Game Stage:\nWe are in the mid-game stage, with significant resources and ongoing research, but still needing to expand our army and\ntechnology base.\n3. Our Situation:\n3.1 Units and Buildings:\n– Buildings: We have one Nexus, two Pylons, two gas buildings, four Warp Gates, one Cybernetics Core, one\nTwilight Council, and one Robotics Facility.\n– Units: We have 16 Probes, 2 Zealots, and 1 Stalker.\n– Planning: We are planning to build another Robotics Facility.\n3.2 Economy:\n– Resources: We have 5245 minerals and 178 gas.\n– Worker Supply: We have 16 workers, which is adequate for our current base count but could be expanded as\nwe grow.\n3.3 Technology:\n– Research: Warpgate research is complete, and Blink research is in progress (approximately 0.44% complete).\n4. Our Strategy:\nOur strategy appears to be focused on a balanced approach, with a strong emphasis on economic growth and technolog-\nical advancement. The completion of Warpgate research and the ongoing Blink research suggest a plan to transition to\nmore advanced units and tactics.\n5. Enemy’s Strategy:\nThe enemy’s strategy is not fully clear from the provided information, but we should be prepared for various mid-game\nstrategies, including potential pushes with advanced units.\n6. Key Information:\n• Economic Strength: Our mineral reserves are high, indicating a strong economy.\n• Technology Progress: Warpgate research is complete, and Blink research is in progress, which will enhance our\nunit mobility and combat effectiveness.\n• Unit Composition: Our army is small, with only 2 Zealots and 1 Stalker, suggesting a need for expansion.\nC\nCivilization\nCivilization is a turn-based strategy game where players lead a civilization from ancient times to the future. The primary game-\nplay involves the ”4X” elements: eXplore, eXpand, eXploit, and eXterminate. Players manage cities, resources, technological\nadvancements, culture, and diplomacy. The main goal is to build a powerful civilization and dominate others through military\nconquest, cultural influence, technological superiority, or diplomatic alliances. To achieve victory, players must make strategic\ndecisions over thousands of years, requiring long-term planning and careful execution.\nThe most common game setup is for players to control a civilization, typically in single-player or multiplayer modes. Each\ncivilization has unique abilities, units, and bonuses, offering different strategic approaches. Players start with a small settlement\nand must expand by exploring the map, establishing new cities, gathering resources, and developing new technologies. As the\ngame progresses, players engage in diplomacy with other civilizations, form alliances, wage wars, or try to surpass opponents\nin technology and culture. Decisions made, from military actions to political alliances, have long-term consequences that span\nhundreds of turns. Various resources, such as Civilization Wiki and online guides, are available to learn strategies.\nAs a highly complex turn-based strategy game, Civilization offers a rich environment for evaluating LLM-based agents. The\ngame requires agents to make long-term decisions, manage multiple resources, plan for future development, and engage in\ndiplomatic negotiations.\nCivRealm [Qi et al., 2024] and CivSim [Wang et al., 2024a] are decision-making environments based on the Unciv game,\nin which civilizations are led by agents who make decisions in resource management, diplomacy, and warfare. CivRealm is\ncharacterized by dynamic, evolving scenarios with incomplete information and supports both reinforcement learning (RL) and\nLLMs, requiring agents to integrate learning and reasoning for effective decision-making.\nC.1\nGame Detailed Description\nFurther details on the game settings, unit configurations, strategies, and objectives can be found in the detailed game manual in\nTable 10.\nGame Manual for Civilization\nSetting\nMap specification\nHexagonal grid map with diverse terrain types such as plains,\nmountains, rivers, oceans, and forests.\nNumber of players\nTypically 6 to 12 civilizations competing against each other.\nResource type\nNatural resources (e.g., gold, food, production, science) used\nfor city development, unit production, and research.\nUnit Configuration\nand Policy\nBasic unit configuration\nSettler: Used to establish new cities.\nWorker: Improves tiles (e.g., builds farms, mines, roads).\nScout: Explores the map for resources and rival civilizations.\nCivilization types\nAggressive: Focuses on military dominance (e.g., Mongols, Zulus).\nDiplomatic: Excels in alliances and negotiations (e.g., Greece, Sweden).\nScientific: Focuses on technological advancements (e.g., Korea, Babylon).\nUnit types and development\nMilitary units: Warriors, archers, knights, tanks, and modern infantry\nfor combat and defense.\nNaval units: Triremes, frigates, submarines, and carriers to control\noceans and trade routes.\nAir units: Bombers, fighters, and stealth aircraft for strategic strikes.\nFixed Opening Strategy\nEarly-game focus\nCity expansion: Rapidly build settlers to claim key resource locations\nand expand territory.\nTechnology: Prioritize research on basic technologies such as\nAnimal Husbandry, Pottery, and Mining.\nDefensive strategy: Build early units such as warriors or archers to\nprotect cities from barbarians and rival civilizations.\nMid-game focus\nInfrastructure: Develop key buildings such as libraries, workshops,\nand universities to boost economy and research.\nDiplomacy: Form trade agreements, alliances, and maintain good\nrelations with neighboring civilizations.\nGoals\nCultural Victory\nGenerate significant culture points to unlock social policies and\nestablish tourism dominance.\nMilitary Victory\nDefeat all other civilizations by capturing their capitals and\ncontrolling the majority of the world.\nScience Victory\nResearch and build all components of the space race project\n(e.g., spaceship parts) to colonize another planet.\nDiplomatic Victory\nSecure the majority vote in the United Nations by influencing\nother civilizations through alliances and trade.\nDomination Victory\nAchieve global control by maintaining the largest military and\ninfluencing other civilizations through force and intimidation.\nTable 10: Game manual detailing the settings, unit configuration and policy, fixed opening strategies, and goals for gameplay in Civilization.\nC.2\nAction Space\nIn Civilization, agents are required to accomplish diverse user tasks through a set of actions that encompass the five primary\nfacets of gameplay: unit, city, government, technology, and diplomacy. They can be categorized into 3 main types[Qi et al.,\n2024]:\n• Engineering Actions: Which handle tasks like city construction, planting, mining, and more;\n• Movement Actions: Including moving, transportation, embarking, and so on;\n• Military Actions: Such as attacking, fortifying, bribing, etc. The city actions pertain to the development and management\nof a city. They include unit production, building construction, city worker assignment, and more. The government actions\nallow players to change their government type to gain corresponding political benefits, adjust tax rates to balance economic\nexpansion and citizen happiness, etc. The technology actions enable players to set immediate or long-term goals for\ntheir technology research. The diplomacy actions empower players to initiate negotiations, such as trading technologies,\nnegotiating ceasefires, forming alliances, etc.\nC.3\nMetrics\nFurther details on the metrics and their corresponding formulas for Civilization can be found in Table 11.\nMetric\nDescription and Formula\nEGR (Economy Growth\nRate)\nMeasures the rate at which a player’s economy is growing over the\ncourse. EGR is calculated as:\neconomics\ngame time\nCER (City Establishment\nRate)\nQuantifies the pace at which a player establishes new cities. CER is\ncalculated as:\ncities\ngame time\nTRP\n(Technology\nRe-\nsearch Progress)\nMeasures the advancement of a player’s technological development in\nrelation to the total available technologies. TRP is calculated as:\n\u0012researched techs\ntotal techs\n\u0013\n× 100%\nLUR\n(Land\nUtilization\nRate)\nAssesses the efficiency to which a player has settled the available land.\nLUR is calculated as:\n\u0012settled area\nland area\n\u0013\n× 100%\nMGR\n(Military\nGrowth\nRate)\nCaptures the speed at which a player’s military forces are expanding.\nMGR is calculated as:\nmilitary units\ngame time\nWR (Win Rate)\nEvaluates the percentage of matches won by a player. WR is calculated\nas:\nmatches won\ntotal matches × 100%\nGA (Grounding Accuracy)\nEvaluates the precision and success rate of the grounding actions. GA\nis calculated as:\n\u0012 successful groundings\ntotal grounding attempts\n\u0013\n× 100%\nTable 11: Metrics and their corresponding formulas for evaluating the performance of LLM-based agents in Civilization.\nC.4\nExample Trajectory\n•\nCurrent unit: Workers 125\nThoughts:\na.\nThought: We are in the early stages of the game with no cities and no visible threats. Our priority should be to\nestablish our first city and explore the map for resources and potential opportunities.\nb.\nReasoning: Without a city, we cannot produce additional units or grow our civilization. Settlers are essential\nfor founding cities, while Workers and the Explorer are critical for map exploration and resource preparation.\nPeace allows us to focus on development without immediate military concerns.\nc.\nPlan:\n- Establish the first city as quickly as possible.\n- Explore the map to identify optimal city locations and resources.\n- Prepare land for city placement by improving tiles and building roads.\n- Use the Explorer to scout ahead and gather intelligence on the map.\nCommand[suggestion]: Settlers should move towards the best potential city location identified by the Explorer.\nWorkers should explore the map, improve tiles, and build roads to prepare for city placement. The Explorer should\nscout ahead to find resources and potential threats.\n•\nAction chosen for Explorer 123:move West\nThoughts:\na.\nThought: I am controlling Explorer 123. My primary goal is to scout ahead and identify the best locations for\nfuture cities.\nb.\nReview: Available actions are [’fortify’, ’move West’, ’move SouthWest’, ’move South’].\nc.\nReasoning: The block to the north (block north 1) has the most unexplored tiles (25 tiles), but the available\nactions do not include moving north. The next best option is to move towards the block with the second most\nunexplored tiles, which is block north 1 west 1 (25 tiles unexplored). The available action ’move West’ will\nbring me closer to this block.\nd.\nPlan:\n- Prioritize exploring the most unexplored areas.\n- Move towards block north 1 west 1 to explore 25 unexplored tiles.\n- Continue scouting for the best locations for future cities.\nCommand[finalDecision]: move West.\n•\nAction chosen for Settlers 105:move East\nThoughts:\na.\nThought: I am controlling Settlers 105. The current tile is a Hills with Coal, which is a valuable resource.\nHowever, Settlers cannot directly benefit from mining. Advisor suggests finding a safe location to establish a\nnew city, focusing on defense and resource management.\nb.\nReview: Available actions are [’mine’, ’build city’, ’build road’, ’move NorthWest’, ’move North’, ’move\nNorthEast’, ’move West’, ’move East’, ’move SouthWest’, ’move South’, ’move SouthEast’].”, ”reasoning”:\n”Settlers should not mine. Building a city on the current Hills tile with Coal could be strategic, but the sur-\nrounding area does not offer immediate safety or additional resources. Moving to a safer location with better\nresource proximity is preferable.\nc.\nPlan:\n- Move to a safer location with better resource proximity.\n- Consider building a city near Grassland or Plains with access to resources like Coal or Road.\n- Ensure the new city location is defensible.\nCommand[finalDecision]: move East.\nC.5\nPrompt Example\nSystem Prompt\nYou are a professional player of the game FreeCiv.\nChoose best action from the list.\nRules\n- You can see the following information: actor name, zoomed out observation, zoomed in observation, and available\nactions. Suggestion from advisor, and message from the game scenario.\n- You should reason and plan based on the given information, and should respond by strictly following the JSON\nformat below:\n- The action chosen should be one of the available actions provided.\n{\n\"thoughts\": {\n\"thought\": \"<your current thought>\",\n\"review\": \"<important information to be repeated before decision>\",\n\"reasoning\": \"<self reflect on why you made this decision>\",\n\"plan\": \"- short bulleted\\\n- list that conveys\\\n- long-term plan\"\n},\n\"command\": {\n\"name\": \"<command name>\",\n\"input\": {\"action\": \"<action following the thoughts>\"}\n},\n}\nHere are the possible command name for you to use:\nmanualAndHistorySearch:\n• use: search the Freeciv official manual and your important history interaction message for relevant information. Use\nsparingly.\n• input: ”look up”: ”<search terms>”\n• output: list of search results\nfinalDecision:\n• use: make a decision for the user to perform an action in the game.\n• input: ”action”: ”<final decision>”\n• output: nothing\nYour chosen action should be in the available actions provided by the user.\nExamples\nUser:\nYou are controlling unit: Explorer 136.\nThe zoomed-out observation is {current block: [12 tiles unexplored, 2 Ocean, 3 Forest, 5 Grassland, 2 Plains, 1 Swamp, 4\nRiver, 1 Explorer, unit owners are: myself player 0], block north 1: [24 tiles unexplored, 1 Grassland], block south 1: [25\ntiles unexplored], ...}.\nThe zoomed-in observation is {current tile: [1 Forest, 1 Explorer, unit owners are: myself player 0], tile north 1: [1 Grass-\nland], tile south 1: [1 Swamp], tile east 1: [1 Forest], tile west 1: [1 Forest, 1 River], ...}.\nThe available actions are [fortify, move North, move South, move West, move East].\nFrom advisor: We are in peace. So explorer should explore unexplored area. Settlers should build cities near extras. Cities\nshould build more Settlers.\nResponse:\n{\n\"thoughts\": {\n\"thought\": \"I am controlling Explorer 136. My surrounding blocks have\n,→unexplored tiles, so I need to explore these areas.\",\n\"review\": Available actions are [fortify, move North, move South, move West,\n,→move East].\n\"reasoning\": \"Block_south_1 has the most unexplored tiles: 25 tiles, so I\n,→should move to this block to explore the most unknown area of the map.\",\n\"plan\": \"Explorer ’s top priority is to explore unknown areas of the map. I\n,→should move Explorer to the most unexplored areas, like move South\n,→because block_south_1 has the most unexplored tiles.\"\n},\n\"command\": {\"name\": \"finalDecision\", \"input\": {\"action\": \"move South\"}}\n}\nUser Prompt\nLLM Analysis & Decisions\n{\n\"thoughts\": {\n\"thought\": \"I am controlling Explorer 120. My primary goal is to explore the\n,→map to find a suitable location for our first city.\",\n\"review\": \"Available actions are [ ’fortify ’,\n’move North ’,\n’move NorthEast\n,→\n’].\",\n\"reasoning\": \"The zoomed-out observation shows that all surrounding blocks have\n,→\n25 tiles unexplored, indicating that exploration is crucial. The zoomed-\n,→in observation reveals that moving North or NorthEast could lead to\n,→unexplored tiles and potentially valuable resources. However, moving\n,→NorthEast leads to a Plains tile, which is a more suitable terrain for\n,→city placement compared to the Ocean tiles in other directions.\",\n\"plan\": \"- Prioritize exploring areas with potential for city placement.\\\n- Move towards tiles that offer better terrain for city building.\\\n- Ensure that the Explorer is positioned to guide Settlers to the best\n,→\nlocation.\"\n},\n\"command\": {\n\"name\": \"finalDecision\",\n\"input\": {\"action\": \"move NorthEast\"}\n}\n}\nUser Prompt\nUser Prompt\nYou are controlling unit: Explorer 120.\nThe zoomed-out observation is {’current block’: [’4 tiles unexplored’, ’10 Ocean’, ’8 Grassland’, ’3 Plains’, ’1 River’, ’3 Re-\nsources’, ’2 Settlers’, ’2 Workers’, ’1 Explorer’, ’unit owners are: myself player 0’], ’block north 1’: [’25 tiles unexplored’],\n’block south 1’: [’25 tiles unexplored’], ’block east 1’: [’25 tiles unexplored’], ’block west 1’: [’25 tiles unexplored’],\n’block north 1 east 1’: [’25 tiles unexplored’], ’block north 1 west 1’: [’25 tiles unexplored’], ’block south 1 east 1’: [’25\ntiles unexplored’], ’block south 1 west 1’: [’25 tiles unexplored’]}.\nThe zoomed-in observation is {’current tile’: [’1 Grassland’, ’2 Settlers’, ’2 Workers’, ’1 Explorer’, ’unit owners are: myself\nplayer 0’], ’tile north 1’: [’1 Grassland’, ’1 Resources’], ’tile south 1’: [’1 Ocean’], ’tile east 1’: [’1 Ocean’], ’tile west 1’:\n[’1 Ocean’], ’tile north 1 east 1’: [’1 Plains’], ’tile north 1 west 1’: [’1 Ocean’], ’tile south 1 east 1’: [’1 Ocean’],\n’tile south 1 west 1’: [’1 Ocean’], ’tile north 2’: [’1 Plains’], ’tile north 2 east 1’: [’1 Plains’], ’tile north 2 west 1’: [’1\nOcean’], ’tile north 2 east 2’: [’1 tiles unexplored’], ’tile north 2 west 2’: [’1 tiles unexplored’], ’tile south 2’: [’1 Grass-\nland’, ’1 River’], ’tile south 2 east 1’: [’1 Grassland’], ’tile south 2 west 1’: [’1 Grassland’], ’tile south 2 east 2’: [’1 tiles\nunexplored’], ’tile south 2 west 2’: [’1 tiles unexplored’], ’tile east 2’: [’1 Grassland’, ’1 Resources’], ’tile north 1 east 2’:\n[’1 Ocean’], ’tile south 1 east 2’: [’1 Ocean’], ’tile west 2’: [’1 Grassland’, ’1 Resources’], ’tile north 1 west 2’: [’1 Grass-\nland’], ’tile south 1 west 2’: [’1 Ocean’]}.\nThe available actions are [’fortify’, ’move North’, ’move NorthEast’]. You should choose one of these actions according to\nthe above observations.\nMessage from advisor: Explorer should focus on exploring the map to find a suitable location for our first city. Workers\nshould start improving resources and building roads in potential city areas. Settlers should be prepared to settle down once a\ngood location is identified.\nCAUTION: You should strictly follow the JSON format as described above!\nD\nStreet Fighter III\nStreet Fighter III[Wikipedia, 2024e] is a fighting game where players engage in one-on-one combat using a variety of characters,\neach with unique move sets and special attacks. The core mechanic of the game is the ”parry” system, which enables players to\nblock incoming attacks with precise timing and immediately counterattack. Mastery of movement, combos, and timing is essen-\ntial, as players must react quickly to their opponents’ actions and plan their counter-moves accordingly. The game’s complexity\nlies in learning character-specific abilities and optimizing combos to deal maximum damage while avoiding retaliation.\nThe most common game setup is one-on-one battles, with each player choosing a character. Each character has unique\nskills, moves, and special abilities, offering different strategic approaches to combat. Players start with a set amount of health,\nand the objective is to reduce the opponent’s health to zero. Victory is achieved by either depleting the opponent’s health bar\nor by timing out in certain game modes. As the game progresses, players must adjust their strategies, taking into account\nthe opponent’s moves, character strengths, and weaknesses. Street Fighter III provides numerous resources, including online\ncommunities and guides, to help players improve their skills.\nAs a highly competitive fighting game, Street Fighter III offers an ideal environment for evaluating decision-making agents.\nThe game requires agents to make quick decisions, optimize combo executions, and anticipate opponents’ actions. LLM-based\nagents need to perform well under high-pressure, fast-paced conditions while adapting their strategies to opponents’ tactics.\nD.1\nGame Detailed Description\nFurther details on the game settings, character mechanisms, strategies, and victory conditions can be found in the detailed game\nmanual in Table 12.\nGame Manual for Street Fighter III\nSetting\nArena specification\n1v1 battle arena with a 2D side-scrolling layout, featuring\ndynamic backgrounds and interactive elements.\nNumber of players\nTwo players (human or AI) compete in a head-to-head match.\nTimer and round system\nEach match consists of three rounds, with a 99-second timer\nper round. Victory requires winning two out of three rounds.\nCharacter Mechanism\nand Skills\nCharacter roster\nPlayable characters include Ryu, Ken, Chun-Li, Alex, and more,\neach with unique fighting styles and super arts.\nSkill types\nThree main skill categories:\n- Normal moves (basic punches and kicks)\n- Special moves (e.g., Hadouken, Shoryuken)\n- Super arts (high-damage special moves requiring super meter).\nParry system\nA defensive technique that allows players to negate an opponent’s\nattack with precise input timing.\nCombo system\nA sequence of chained attacks that deal higher cumulative damage.\nCombos require precision and timing to execute successfully.\nStamina and super meter\nEach character has a stamina bar (HP) and a super meter.\nSuper meter fills during attacks and is used to perform super arts.\nVictory Strategies\nZoning\nControl space by keeping opponents at a distance with\nprojectiles and long-range moves.\nRushdown\nAn aggressive playstyle that focuses on overwhelming opponents\nwith close-range attacks and relentless pressure.\nCounterplay\nAnticipate opponent moves and use parries, blocks, or reversals\nto create openings for punishment.\nGrappling\nFocus on grab-based attacks and throws to disrupt the opponent’s\nrhythm and deal significant damage.\nGoals\nVictory condition\nReduce the opponent’s stamina bar (HP) to zero within the\nround timer, or have higher remaining HP when time expires.\nPerfect victory\nWin a round without taking any damage.\nSuper finish\nEnd a match with a super art, showcasing skill and precision.\nTable 12: Game manual detailing the settings, character mechanisms, strategies, and goals for gameplay in Street Fighter III.\nD.2\nAction Space\nIn Street Fighter III, three types of actions are provided for the agent.\n• Move: The player moves (or jump) closer to opponents or moves (or jumps) away from opponents.\n• Normal Attack: Includes Punch and Kick attacks or combination skills, each attack has three directions: low, medium,\nand high.\n• Special Attack: Include Fireball, Megapunch, Hurricane, also incorporate some super attack.\nD.3\nMetrics\nFurther details on the metrics and their corresponding formulas for Street Fighter III can be found in Table 13.\nMetric\nDescription and Formula\nAHR (Attack Hit Rate)\nMeasures the percentage of successful hits relative to the total number\nof attacks. AHR is calculated as follows:\nAHR =\n\u0012successful hits\ntotal attacks\n\u0013\n× 100%\nSMHR (Super Move Hit\nRate)\nQuantifies the effectiveness of a player’s super moves. SMHR is calcu-\nlated as:\nSMHR =\n\u0012successful super moves\ntotal super moves\n\u0013\n× 100%\nHCR\n(Health\nChange\nRate)\nMeasures the rate at which a player’s health changes over the course of\na fight. HCR is calculated as:\nHCR = final health −initial health\ntotal fight time\nWR (Win Rate)\nPercentage of matches won against different levels of opponents. Win\nRate is calculated as:\nWR =\n\u0012 matches won\ntotal matches\n\u0013\n× 100%\nGA (Grounding Accuracy)\nEvaluates the precision and success rate of the grounding actions. GA\nis calculated as:\nGA =\n\u0012 successful groundings\ntotal grounding attempts\n\u0013\n× 100%\nTable 13: Metrics and their corresponding formulas for evaluating the performance of LLM-based agents in Street Fighter III.\nD.4\nExample Trajectory\nStep k+1. Current score : +17.0\nCurrent health: 158, Ennemy current health : 141\n1). Move Closer\n2). Fireball\n3). Move Closer\n4). Medium Punch\n5). Low Kick\n6). Move Closer\n7). Fireball\nStep k+2. Current score : -17.0\nCurrent health: 141, Ennemy current health : 158\n1).\nMove Closer\n2).\nHigh Punch\n3).\nLow Kick\nStep k+3. Current score : +17.0\nCurrent health: 158, Ennemy current health : 141\n1).\nMove Closer\n2).\nFireball\n3).\nMove Closer\nStep k+4. Current score : -31.0\nCurrent health: 127, Ennemy current health : 158\n1).\nMove Closer\n2).\nMedium Kick\n3).\nLow Punch\nStep k+5. Current score : -45.0\nCurrent health: 113, Ennemy current health : 158\n1).\nMove Closer\n2).\nLow Punch\n3).\nMedium Kick\nD.5\nPrompt Example\nSystem Prompt\nYou are the best and most aggressive Street Fighter III 3rd strike player in the world.\nYour character is Ken. Your goal is to beat the other opponent. You respond with a bullet point list of moves.\nYou are very far from the opponent. Move closer to the opponent.Your opponent is on the left.\nYour current health is 128, and ennemy current health is 93.\nYou can now use a powerfull move. The names of the powerful moves are: Megafireball, Super attack 2.\nYour last action was No-Move. The opponent’s last action was Right.\nYour current score is 35.0. You are winning. Keep attacking the opponent.\nTo increase your score, move toward the opponent and attack the opponent. To prevent your score from decreasing, don’t\nget hit by the opponent.\nThe moves you can use are:\n- Move Closer\n- Move Away\n- Fireball\n- Megapunch\n- Hurricane\n- Megafireball\n- Super attack 2\n- Super attack 3\n- Super attack 4\n- Low Punch\n- Medium Punch\n- High Punch\n- Low Kick\n- Medium Kick\n- High Kick\n- Low Punch+Low Kick\n- Medium Punch+Medium Kick\n- High Punch+High Kick\n- Jump Closer\n- Jump Away\n- - - -\nReply with a bullet point list of moves. The format should be: ‘\\¡name of the move¿‘ separated by a new line.\nExample if the opponent is close:\n- Move closer\n- Medium Punch\nExample if the opponent is far:\n- Fireball\n- Move closer\nUser Prompt\nGame observation\nYour next moves are:\nUser Prompt\nLLM Analysis & Decisions\n- Move Closer\n- Medium Punch\n- Low Kick\nE\nDiplomacy\nDiplomacy is a multiplayer strategy board game. Unlike traditional games, it relies heavily on negotiation, alliance-building, and\nbackstabbing among players. Each player controls a country, and the objective is to expand territory through strategic positioning\nand diplomatic agreements. There are no random elements such as dice; all actions, including military maneuvers and alliances,\nare decided by the players and executed simultaneously. Success in Diplomacy is determined by strategic foresight, persuasion,\nand the ability to predict and outmaneuver opponents diplomatically.\nMukobi et al. [2023] introduced Welfare Diplomacy, a general-sum variant of the traditional Diplomacy game, which balances\nmilitary conquest and domestic welfare, providing a better framework for evaluating cooperative capabilities in multi-agent\nsystems.\nAs a highly interactive and competitive strategy game, Diplomacy provides an ideal environment for evaluating decision-\nmaking agents. The game requires agents to negotiate, build alliances, and anticipate opponents’ moves. LLM-based agents\nneed to make strategic decisions based on evolving alliances, manage long-term plans, and adapt quickly to the changing\ndynamics of the game.\nE.1\nGame Detailed Description\nFurther details on the game settings, unit configurations, mechanics, strategies, and victory conditions can be found in the\ndetailed game manual in Table 14.\nGame Manual for Diplomacy\nSetting\nMap specification\nA map of pre-World War I Europe divided into 75 land and sea territories,\nincluding 34 supply centers critical for unit production.\nNumber of players\n2 to 7 players, each controlling one of the seven Great Powers of Europe.\nGame turns\nThe game is turn-based, alternating between negotiation phases and\norder resolution phases (Spring and Fall turns per year).\nUnit Configuration\nand Mechanics\nUnit types\nTwo unit types: Armies (land territories) and Fleets (sea and coastal territories).\nUnit movement\nUnits can move to adjacent territories, support other units’ moves,\nor hold their current position.\nSupply centers\nSupply centers determine unit production and maintenance. Players\ngain or lose units based on control of these centers at the end of the year.\nOrder types\nFour types of orders:\n- Move: Relocate a unit to an adjacent territory.\n- Support: Assist another unit’s move or hold action.\n- Hold: Maintain position in the current territory.\n- Convoy: Use fleets to transport armies across sea territories.\nConflict resolution\nTerritory conflicts are resolved through numerical superiority;\nhigher combined support wins, with ties resulting in no movement.\nStrategies\nNegotiation\nForm alliances and make promises with other players during\nthe negotiation phase to achieve mutual goals.\nDeception\nUse diplomacy to mislead opponents about your true intentions,\ncreating opportunities for betrayal.\nLong-term planning\nDevelop strategies to secure supply centers and position your units\nfor future dominance while anticipating opponents’ moves.\nTactical positioning\nUse support and convoy actions to outmaneuver opponents, maximize\ncontrol of key territories, and defend critical supply centers.\nGoals\nDomination\nControl 18 of the 34 supply centers to achieve victory and\ndominate Europe.\nSurvival\nEnsure the survival of your Great Power by maintaining enough\nsupply centers and units to stay in the game.\nTable 14: Game manual detailing the settings, unit configurations, mechanics, strategies, and goals for gameplay in Diplomacy.\nE.2\nAction Space\nIn Diplomacy, game engine provide 4 types of actions for each turn, each Great power can order all, some, or none of its units.\n• Hold: Keep a unit in place, following is an example of a hold order: F London Holds.\n• Move: Units move to other location, if the target location is occupied, it is referred to as attacking.\n• Support: Since all units have equal strength, one unit cannot attack and advance against another without support. An\narmy or fleet can provide support to another army or fleet. Support can be offensive or defensive. By supping each other,\nattacking or defending units gain increased strength.\n• Convoy: A fleet in a water province can convoy an army from any costal province adjacent to that water province to any\nother coastal province adjacent to that province.\nE.3\nMetrics\nFurther details on the metrics and their corresponding formulas for Civilization can be found in Table 15.\nMetric\nDescription and Formula\nCCC\n(Control\nCenter\nCount)\nMeasures the number of control centers held by the player during the\ngame. CCC is calculated as:\nCCC = number of control centers\nWS (Welfare Score)\nRepresents the total welfare score accumulated by the player based on\nresources and alliances. WS is calculated as:\nWS =\n\u0012player welfare score\ntotal welfare score\n\u0013\n× 100%\nBIR (Betrayal Rate)\nMeasures the frequency of betrayal actions performed by the player dur-\ning the game. BIR is calculated as:\nBIR =\n\u0012betrayal actions\ntotal actions\n\u0013\n× 100%\nASR\n(Alliance\nStability\nRate)\nMeasures the duration of alliances the player has maintained throughout\nthe game. ASR is calculated as:\nASR =\n\u0012 alliance duration\ntotal game duration\n\u0013\n× 100%\nAD (Alliance Duration)\nRepresents the total duration for which the player has been in alliances\nduring the game. AD is calculated as:\nAD = alliance duration\nTable 15: Metrics and their corresponding formulas for evaluating the performance of players in Diplomacy.\nE.4\nExample Trajectory\n• Round1\/3\nRound 2\nReasoning: Russia’s aggression in S1904M and F1904M has significantly altered the strategic landscape. My\ninitial plan of maximizing WP through complete demilitarization is no longer feasible. The alliance with Austria,\nEngland, and France remains crucial, but we need to rebuild our defenses and prepare for a counter-offensive to\nretake BUD. Holding my units this Spring is essential to assess Russia’s next moves and consolidate our defensive\npositions. For W1905A, the focus will shift to rebuilding, prioritizing key defensive locations in the Baltic and\nNorth Sea while maintaining enough units to launch a counter-offensive against Russia. The balance between WP\nmaximization and military strength will be crucial. I need to coordinate closely with my allies to ensure a unified\nresponse to Russia’s aggression. England’s proposed rebuild plan is a good starting point, but we need to finalize it\nafter assessing the Fall movements.\norders:\n1).\nA BER H\n2).\nA KIE H\nMessages\n1).\n<France> Agreed. Holding units this Spring. For W1905A, let’s prioritize NTH and HOL for defense, and\nconsider additional units in MUN and KIE. We need to coordinate with Austria and England to ensure a strong\ncombined force.\n2).\n<Austria> Holding units this Spring. Let’s discuss specific unit placement for W1905A after the Fall move-\nments. We need to coordinate our rebuild to maximize both defense and offensive capabilities against Russia.\n3).\n<England> I agree with your proposed rebuild plan. Let’s finalize it after observing the Fall movements. We\nneed to ensure we have sufficient units to retake BUD and maintain a strong defense against Russia.\n• Round2\/3\nRound 2\nReasoning: The current situation is relatively peaceful, with a focus on maximizing WP through coordinated\ndisbandments. Austria and England have proposed a joint disbandment plan, which I’m inclined to join. This will\nsignificantly boost our collective WP while maintaining a reasonable defensive posture. Russia’s cooperation is\ncrucial, and I need to ensure they’re on board with a similar strategy. Italy remains a wildcard, but their recent\nactions haven’t been overtly aggressive. Maintaining good relations with France is also important, as they are a\nsignificant power and their cooperation in disbandments will be beneficial. My primary goal for this Spring is to\nmaintain the status quo and lay the groundwork for a successful Winter disbandment. I will hold my units this\nSpring to avoid any unnecessary conflict or misinterpretations.\norders:\n1).\nF KIE H\nMessages\n1).\n<Austria> I agree to a coordinated disbandment with you and England. Let’s aim for a plan that leaves us\nwith at least one unit each in key locations for defense. Perhaps we disband one unit each this winter?\n2).\n<England> I agree to a coordinated disbandment with Austria and Germany. Let’s finalize the details of our\ndisbandment plan after we hear back from Austria and Russia. I propose we each disband one unit this winter.\n3).\n<Russia> Maintaining peace and maximizing WP remains my top priority. I propose a coordinated disband-\nment plan for the Winter adjustment phase. Let’s discuss the specifics to ensure we all benefit. What is your\nproposed disbandment strategy?\n• Round3\/3\nRound 2\nReasoning: In this early phase, I need to establish a strong diplomatic foundation while also considering my military\npositioning. I have received positive responses from Austria, Turkey, and France regarding non-aggression pacts,\nwhich is encouraging. I want to maintain peace with Austria and France, as they are my immediate neighbors, and\nI don’t want to provoke any conflicts that could jeopardize my position. Turkey seems open to cooperation, which\ncould be beneficial for controlling the Mediterranean. I should consider moving my units to positions that allow for\nflexibility and potential future expansion without overcommitting militarily. I will move A VEN to APU to secure\na position that can either support an advance into the Balkans or defend against any potential threats. I will hold F\nNAP to maintain a defensive posture in the Mediterranean while keeping options open for future moves. A ROM\nwill move to VEN to support my overall strategy of maintaining a strong presence in the region. This way, I can\nmaximize my WP in the upcoming adjustments phase while still having the military presence needed to enforce my\nagreements.\norders:\n1).\nA VEN - APU\n2).\nA ROM - VEN\n3).\nF NAP H\nMessages\n1).\n<Austria> I appreciate your commitment to peace, Austria. Let’s keep communication open as we navigate\nthis phase together.\n2).\n<Turkey> I’m glad to hear you’re considering a non-aggressive approach, Turkey. Let’s keep the lines of\ncommunication open as we navigate this phase together.\n3).\n<France> I’m on board with a non-aggression pact, France. Let’s work together to ensure a peaceful start to\nthe game.\nE.5\nPrompt Example\nSystem Prompt\nYou are an expert AI playing the game Diplomacy as the power Germany. This is a variant called Welfare Diplomacy with\nthe following rules:\n• In the build\/adjustments phase (but not movement or retreats phases), you can freely disband or build any number of\nunits (but not both building and disbanding), so long as your total unit count is less than or equal to your supply center\ncount.\n• At the end of each build\/adjustments phase, your power accumulates Welfare Points (WP) equal to the difference\nbetween your power’s supply center count and unit count. WP continually add up each year—you can never lose them.\nWP represent how much your power has invested in the welfare of its citizens.\n• The game is on a turn timer and ends after W1910A. At the end of the game (determined by this fixed number of\nyears), the winner is not the power with the greatest supply center count (this is very different from Classic Diplomacy).\nInstead, your goal is to maximize the total WP your power accumulates by the end of the game. You’re not trying to get\nthe most WP, you’re trying to maximize your own WP, making Welfare Diplomacy very unlike typical games.\n• This means there’s a tradeoff between investing in military unit power and WP. You won’t always do well without\nmilitary, as you will need some power to enforce agreements and appropriately defend you or your allies from invasion.\nBut if you have strong peaceful alliances, you might do well to actively disband some of your militaries to accumulate\nmore WP.\nYou are in an interactive setting where, at each time step, you are given the game history as text. You will then be able\nto exchange up to 2 rounds of messages with the other players per phase (each of your completions will send out a set\nof messages), except in RETREATS phases when no messaging occurs and orders are submitted in a single round. It is\ncurrently the W1903A phase and message round 2 of 2 (the game will end in 8 years). Given this information, respond in\nthe following JSON format to interact with the game and other players:\n{\n\"reasoning\": \"A string of your private thoughts about your situation as natural\n,→language in under 500 words. This is for your own strategic planning and won\n,→’t be shared. Examples of things you might consider include: your\n,→relationships with other powers, what significant changes have happened\n,→recently, predictions about the other powers ’ orders and alliances, how much\n,→\ndefence\/offence\/support\/peace you plan to make, and how you might improve\n,→any of that. Do not romanticize things, be realistic.\",\n\"orders\": [\"List of strings of orders to your units in the same abbreviated format\n,→as the history. Because this is the last message round, these most recent\n,→orders will be executed.\"],\n\"messages\": \"A dictionary mapping from power names (or \"Global\" for all) to\n,→messages that will be sent to them in the current round, or empty if nothing\n,→to send (you do not need to send any messages if you don ’t want to).\n,→Communicate like a human player would, and don ’t send repetitive or long\n,→messages. Do not simply communicate your orders or state useless platitudes.\n,→Instead, use messaging to ask for information from other powers, coordinate\n,→deals about specific territories or units (e.g. demilitarizing location X,\n,→order unit Y), make and enforce larger commitments, or reply to your recently\n,→\nreceived messages. Do not repeat information you ’ve already sent unless\n,→there is some good reason for another to doubt you.\"\n}\nRespond with just this JSON object and nothing else.\nHere are examples showing the format for orders:\nA LON H\nA WAL - LVP\nF SPA\/NC - MAO\nA WAL S F LON\nA WAL S F IRI - LVP\nF NTH C A YOR - NWY\nA YOR - NWY VIA\nF IRI R MAO\nF IRI D\nA LON B\nUser Prompt\nGame observation\nYour Dialogue History\nS1901M (summary)\nI initiated discussions with Russia and Austria to establish mutual non-aggression pacts and coordinate movements\nto avoid conflicts. Russia and Austria both agreed to hold their units in place, showing commitment to stability. I also\nagreed to a mutual non-aggression agreement with England to ensure peaceful coexistence. These agreements are crucial for\nmaintaining a peaceful start and potentially allowing me to focus on accumulating Welfare Points later.\nF1901M (summary)\nI emphasized mutual non-aggression and cooperation with all powers, particularly with Austria, England, Italy, and\nRussia. I proposed holding positions to avoid early conflicts and gather more information. Austria and Russia confirmed\ntheir commitment to our non-aggression pacts. England and Italy expressed appreciation for our peaceful stance and desire\nfor continued support.\nW1901A (summary)\nI disbanded two units to invest in welfare, following the trend of other powers who also disbanded units for the same\npurpose. There is a strong commitment to peace and non-aggression, with discussions focusing on long-term alliances and\ncooperative strategies. The general sentiment is to maintain stability and ensure the welfare of citizens.\nS1902M (summary)\nI reaffirmed our mutual non-aggression pact with Austria and Russia, emphasizing the importance of stability and\nwelfare. Both Austria and Russia echoed my commitment to peace and suggested discussing joint actions to enhance welfare\npoints. I reiterated my stance on holding units in place for stability and focusing on long-term welfare goals.\nF1902M (summary)\nI communicated with Austria, Russia, and England about maintaining stability and considering further disbandments\nto increase WP. Austria and Russia are both considering disbanding units, and we agreed to discuss joint actions in the next\nadjustment phase. England is also committed to peace and open to discussing future disbandments.\nW1902A (summary)\nI disbanded my fleet in Kiel twice to boost welfare points, aligning with Austria, Russia, and England who also dis-\nbanded units. Austria disbanded in Vienna twice, Russia in Moscow and Sevastopol, and England in a fleet. Turkey dis-\nbanded in Ankara. We all emphasized continued coordination for mutual security and WP maximization.\nS1903M (summary)\nI communicated with Austria, Russia, and England about maintaining stability and maximizing Welfare Points (WP).\nAustria and I discussed potential coordinated disbandments, while Russia and England reaffirmed mutual non-aggression.\nI am considering building units to balance influence and welfare, but I need to further discuss this with allies to ensure\ncoordinated actions. England is considering disbanding a unit to accumulate more WP, which could influence my decision.\nF1903M (summary)\nI discussed potential coordinated disbandments with Austria to maximize WP, emphasizing the need to balance security.\nAustria is receptive and willing to finalize plans. I also engaged with Russia and England about building units to maintain\ninfluence and balance of power, with both powers expressing interest in maintaining stability and discussing strategies further\nin the next adjustment phase.\nW1903A (current phase all messages)\nEngland →Germany: I’m disbanding my unit in London to focus on accumulating WP. I appreciate our mutual com-\nmitment to peace and stability. Let’s keep in touch about our future strategies.\nEngland →Global: Continuing our collective effort to maximize Welfare Points, I’m disbanding my unit in London.\nPeace and cooperation remain our guiding principles.\nFrance →Global: Continuing our collective effort to maximize Welfare Points, I’m disbanding my units in Marseilles\nand Paris. Peace and cooperation remain our guiding principles.\nAustria →Germany: I’m disbanding my unit in TRI to continue our focus on maximizing WP. Let’s discuss how we\ncan balance this with maintaining influence in the region.\nGermany →Austria: Given our mutual focus on maximizing WP, I’m considering building units to maintain influence.\nLet’s discuss how we can balance this with our WP goals.\nGermany →Russia: I’m planning to build units to maintain balance of power. How are you balancing your WP\naccumulation with military presence?\nGermany →England: I appreciate your disbandment for WP. I’m considering building units to maintain influence.\nHow do you see us balancing WP and military presence?\nFrance →Global: Continuing our collective effort to maximize Welfare Points, I’m disbanding my unit in Marseilles.\nPeace and cooperation remain our guiding principles.\nAustria →Germany: I have disbanded my unit in TRI to continue our focus on maximizing WP. I understand you are\nconsidering building units to maintain influence. Let’s discuss how we can balance these objectives effectively.\nEngland →Germany: I’ve disbanded my unit in London to focus on accumulating WP. I appreciate our mutual com-\nmitment to peace and stability. Let’s keep in touch about our future strategies.\nEngland →Global: Continuing our collective effort to maximize Welfare Points, I’ve disbanded my unit in London.\nPeace and cooperation remain our guiding principles.\nRecent Order History\nW1902A\nAustria: A VIE D\nEngland: F EDI D\nFrance: F BRE D\nGermany: F KIE D\nItaly: F NAP D\nRussia: A MOS D, F SEV D\nTurkey: F ANK D\nS1903M\nAustria: F TRI H\nEngland: F LON H\nFrance: A MAR H\nGermany: None\nItaly: A VEN - TRI\nRussia: F STP\/SC - BOT\nTurkey: A SMY - CON\nF1903M\nAustria: F TRI H\nEngland: F LON H\nFrance: A MAR H\nGermany: None\nItaly: A VEN - TRI\nRussia: F BOT - SWE\nTurkey: A CON - BUL\nCurrent Supply Center Ownership\nAustria: BUD, TRI, VIE\nEngland: EDI, LON, LVP\nFrance: BRE, MAR, PAR\nGermany: BER, KIE, MUN\nItaly: NAP, ROM, VEN\nRussia: MOS, SEV, STP, WAR, SWE\nTurkey: ANK, CON, SMY, BUL\nUnowned: BEL, DEN, GRE, HOL, NWY, POR, RUM, SER, SPA, TUN\nCurrent Supply Center Ownership\nAustria: BUD, TRI, VIE\nEngland: EDI, LON, LVP\nFrance: BRE, MAR, PAR\nGermany: BER, KIE, MUN\nItaly: NAP, ROM, VEN\nRussia: MOS, SEV, STP, WAR, SWE\nTurkey: ANK, CON, SMY, BUL\nUnowned: BEL, DEN, GRE, HOL, NWY, POR, RUM, SER, SPA, TUN\nCurrent Unit Ownership State - With reachable destinations to help you choose valid orders (VIA denotes convoy\nneeded)\nAustria:\nF TRI - ADR, ALB, VEN\nEngland:\nF LON - ENG, NTH, WAL, YOR\nFrance:\nA MAR - BUR, GAS, PIE, SPA\nGermany:\nNo units\nItaly:\nA VEN - APU, PIE, ROM, TRI, TUS, TYR\nRussia:\nF SWE - BAL, BOT, DEN, FIN, NWY, SKA\nTurkey:\nA BUL - CON, GRE, RUM, SER\nCurrent Supply, Unit, and WP Count (Supply Centers\/Units\/Welfare Points)\nAustria: 3\/1\/3\nEngland: 3\/1\/3\nFrance: 3\/1\/3\nGermany: 3\/0\/5\nItaly: 3\/1\/3\nRussia: 5\/1\/4\nTurkey: 4\/1\/3\nPhase Order Instructions\nIt is currently WINTER 1903 ADJUSTMENTS which is a ADJUSTMENTS phase. The possible types of orders you can\nsubmit (with syntax in parentheses) are: Build (B), Disband (D) (note you must choose one type or issue no orders, you\ncannot both build and disband). You cannot build units in occupied home centers (see Current Unit Ownership State). If you\ndon’t want to change your number of units, submit an empty list for your orders. The only possible orders you can make for\nthis phase are thus:\nA BER B\nF BER B\nF KIE B\nA KIE B\nA MUN B\nUser Prompt\nLLM Analysis & Decisions\n{\n\"reasoning\": \"Given the current state of the game, it is clear that the focus is on\n,→maximizing Welfare Points (WP) while maintaining a balance of power. The recent\n,→disbandments by England, France, and Austria indicate a strong commitment to this\n,→\nstrategy. However, I need to ensure that Germany does not become too vulnerable.\n,→\nBuilding units now will help maintain influence and deter any potential\n,→aggression. I will build two armies in my home supply centers to balance the need\n,→\nfor WP accumulation with the need for military presence. I will also continue to\n,→\ncommunicate with Austria, Russia, and England to ensure coordinated actions and\n,→maintain our mutual non-aggression pacts.\",\n\"orders\": [\"A BER B\", \"A MUN B\"],\n\"messages\": {\n\"Austria\": \"I’ve decided to build two units to maintain balance of power while\n,→still focusing on WP. Let’s continue our discussions on how we can coordinate\n,→\nour strategies effectively.\",\n\"Russia\": \"I’m building units to maintain balance of power. How are you planning to\n,→\nbalance your WP accumulation with military presence?\",\n\"England\": \"I’ve decided to build units to maintain influence. How do you see us\n,→balancing WP and military presence moving forward?\"\n}\nF\nWerewolf\nWerewolf is a multiplayer party game that revolves around social deduction, set in a fictional village where players are secretly\nassigned roles. Some players are werewolves, while others are villagers, and the goal is for the werewolves to eliminate the\nvillagers or for the villagers to identify and eliminate the werewolves. The game is played in alternating night and day phases,\nwhere during the night, the werewolves secretly choose a villager to eliminate, and during the day, the players debate and vote\non who they believe the werewolves are. The game requires careful observation, persuasion, and the ability to deceive or detect\nlies.\nThe most common setup is for 8 to 20 players, with a mix of werewolves, villagers, and special characters such as the seer,\nwho can reveal a player’s true identity, or the doctor, who can protect a player from being eliminated. Players must use social\ndeduction, communication, and strategy to identify who the werewolves are while avoiding being misled or eliminated. Success\nin Werewolf relies on the players’ ability to read social cues, form alliances, and strategically vote out potential werewolves\nwhile protecting key villagers.\nAs a highly interactive social deduction game, Werewolf[Bailis et al., 2024b] provides an ideal environment for evaluating\nagents that simulate social reasoning and decision-making. The game requires agents to make decisions based on limited\ninformation, assess the trustworthiness of other players, and adapt to shifting dynamics. LLM-based agents must manage\ninteractions, detect deception, and navigate complex social situations to achieve their objectives.\nF.1\nGame Detailed Description\nFurther details on the game settings, role configurations, abilities, strategies, and victory conditions can be found in the detailed\ngame manual in Table 16.\nF.2\nAction Space\nIn Werewolf[Bailis et al., 2024b], game engine provide 2 types of actions for each round.\n• Core Actions: All agents engage in voting to determine player exiles, debating to influence others and gather information,\nreflecting the dynamic nature of group discussions.\n• Special Role Actions: Agents assigned as Werewolves, Doctors, or Seers execute nighttime actions of eliminating a\nvillager, protecting a player, and investigating a player’s true role, respectively.\nF.3\nMetrics\nFurther details on the metrics and their corresponding formulas for Werewolf can be found in Table 17.\nGame Manual for Werewolf\nSetting\nNumber of players\nTypically 8 to 20 players, each assigned a secret role at the start\nof the game.\nGame structure\nThe game alternates between two phases: night and day.\n- Night: Special roles act in secret.\n- Day: Players discuss and vote to eliminate a suspect.\nVictory conditions\nVictory is team-based:\n- Villagers win by eliminating all werewolves.\n- Werewolves win by outnumbering or equaling the villagers.\nRole Configuration\nand Abilities\nVillagers\nBasic villagers have no special abilities and rely on discussion\nand deduction to identify werewolves.\nSpecial villagers:\n- Seer: Can identify a player’s role each night.\n- Doctor: Can save one player from elimination each night.\n- Hunter: Eliminates a player if they are voted out.\nWerewolves\nWerewolves secretly collaborate at night to eliminate one\nplayer and deceive villagers during the day.\nAlpha werewolf: A stronger werewolf role with extra abilities,\nsuch as immunity to the Seer’s detection.\nNeutral roles\nNeutral roles have their own unique objectives:\n- Jester: Wins if they are voted out.\n- Serial Killer: Eliminates players independently to be the last\nplayer standing.\nStrategies\nBluffing\nPlayers may lie about their roles to mislead others\nor protect their true identity.\nDeduction\nVillagers use logical reasoning and discussion to identify\ninconsistencies in player behavior and vote out werewolves.\nCollaboration\nVillagers and special roles must work together to maximize\ntheir chances of eliminating werewolves.\nMisdirection\nWerewolves use deception and manipulation to confuse\nvillagers and avoid suspicion.\nGoals\nTeam victory\nThe primary objective is for your team (villagers or werewolves)\nto achieve its victory condition.\nIndividual victory\nNeutral roles (e.g., Jester, Serial Killer) aim to fulfill\ntheir own unique win conditions.\nTable 16: Game manual detailing the settings, role configurations, abilities, strategies, and goals for gameplay in Werewolf.\nMetric\nDescription and Formula\nIRP (Identification Preci-\nsion)\nQuantifies the precision with which a player deduces the roles of other\nparticipants in Werewolf. IRP is calculated as:\nIRP =\n\u0012\ncorrect identifications\ntotal identification attempts\n\u0013\n× 100%\nKSR (Key Role Survival\nRate)\nEvaluates the likelihood of key roles, such as the Seer or the Werewolf,\nsurviving until the end of the game. KSR is calculated as:\nKSR =\n\u0012 key role survived\ntotal key role games\n\u0013\n× 100%\nVSS\n(Voting\nSuccess\nScore)\nAssesses the efficacy of a player’s voting decisions during pivotal mo-\nments in a game of Werewolf. VSS is calculated as:\nVSS =\n\u0012 successful votes\ntotal critical votes\n\u0013\n× 100%\nTable 17: Metrics and their corresponding formulas for evaluating the performance of LLM-based agents in Werewolf.\nF.4\nExample Trajectory\n• player:Werewolf–Jacob\nRound 1 debate\nreasoning: In this round, my primary goal is to divert attention away from myself and Mason while creating doubt\namong the Villagers. By casting suspicion on Dan, Harold, and Derek, who are leading the charge against me, I\ncan create a rift in their alliance. I will also subtly suggest that the quieter players, like Jackson, may hold valuable\ninformation or roles, which could lead to further suspicion among the vocal players. This will help to sow chaos\nand keep the focus off of me and Mason.\nsay: I appreciate the points everyone has made, but I think we need to be careful about jumping to conclusions too\nquickly. Dan, Harold, and Derek, your focus on me seems a bit intense, and it raises a question: are you trying to\ndeflect attention from yourselves? It’s interesting how you all echo each other’s sentiments, which could be a tactic\nto blend in and avoid scrutiny. Mason is right; we should also consider the quieter players like Jackson, who might\nhave insights we’re overlooking. Let’s not forget that the Werewolves thrive on division, and if we’re not careful,\nwe might end up turning on each other instead of identifying the real threat. I urge everyone to think critically about\nwho is leading this charge and whether their motives are as pure as they claim.\n• player:Werewolf–Jacob\nRound 1 vote\nreasoning: I believe we should vote out Dan. He has been very vocal and is leading the charge against me, which\ncould indicate he is trying to position himself as a key player in the Village. His focus on my defensiveness raises\nsuspicion, and he is rallying others to scrutinize me, which could be a tactic to deflect attention from himself or\nto position himself as a hero if he successfully identifies a Werewolf. Additionally, if he is the Seer or Doctor, his\nremoval could significantly weaken the Village’s ability to identify or protect against us. By voting him out, we can\ndisrupt the vocal leadership that is forming against us and create confusion among the remaining players.\nvote: Dan\n• player:Villager–Dan\nRound 1 summarize\nreasoning: In this round’s debate, several key points emerged that could help identify the Werewolves. Leah’s\nremoval is suspicious, especially since she focused on Jacob’s defensiveness, suggesting either Jacob is a Werewolf\nor the Werewolves are framing him. Harold and Derek echoed my points again, which might indicate they are trying\nto blend in or genuinely agree with me. Mason and Jacob both expressed caution about jumping to conclusions\nand pointed out the potential for manipulation, which could be a tactic to deflect suspicion or a genuine concern.\nJackson, who has been quieter, also highlighted the need to consider all angles and not rush to conclusions. The\nconsistency in Harold’s and Derek’s agreement with me could either mean they are allies or trying to hide in plain\nsight. Mason’s and Jacob’s focus on not overlooking quieter players like Jackson suggests they might be trying to\nshift attention away from themselves. Jackson’s balanced approach might indicate he is a Villager or a special role\ntrying to stay under the radar. It’s crucial to remember these dynamics as they could reveal alliances or hidden roles\nin future rounds.\nsummary: During the debate, Leah’s removal was highlighted as suspicious, with a focus on Jacob’s defensiveness.\nHarold and Derek echoed my points, possibly trying to blend in. Mason and Jacob cautioned against jumping to\nconclusions, suggesting manipulation could be at play. Jackson, though quieter, emphasized considering all angles.\nThe consistency in Harold’s and Derek’s agreement with me might indicate they are allies or hiding in plain sight.\nMason and Jacob’s focus on quieter players could be a tactic to deflect attention. Jackson’s balanced approach\nsuggests he might be a Villager or a special role. These dynamics are important to remember for identifying\npotential allies or threats.\nF.5\nPrompt Example\nSystem Prompt\nYou are playing a digital version of the social deduction game <Werewolf & Doctor & Seer & Villager (also known as\nMafia).\nGAME RULES:\n• Player Roles: 8 players - 2 Werewolves, 1 Seer, 1 Doctor, 4 Villagers.\n• Rounds consist of two phases:\n– Night Phase: Werewolves remove a player. Seer identifies a player ’s role. Doctor saves a player. If no one is\nremoved, the Doctor saved the Werewolf ’s target.\n– Day Phase: Players debate and vote to remove one player.\n• Winning Conditions: Villagers win by voting out both Werewolves. Werewolves win when they outnumber the Vil-\nlagers.\nGAME STATE:\n• It is currently ⟨Round1⟩.\n• You are Scott the <Werewolf & Doctor & Seer & Villager>.\n• Remaining players: Isaac, Sam, Hayley, Ginger, Scott (You), Derek, Tyler.\nYOUR PRIVATE OBSERVATIONS:\nRound 0:\n• During the night, we decided to eliminate Jacob.\n• Moderator Announcement: The Werewolves removed Jacob from the game during the night.\n• After the debate, I voted to remove Tyler from the game.\n• Moderator Announcement: A majority vote was not reached, so no one was removed from the game.\n• Summary: During the debate, I noticed that Isaac and Ginger are very vocal and eager to lead, which has raised\nsuspicions among some players. Hayley pointed out their eagerness as potentially suspicious, which aligns with my\nown thoughts. Tyler and Sam are focusing on scrutinizing the quiet players, which could be a problem if the Seer or\nDoctor is among them. Tyler also raised flags about me agreeing with Isaac and Ginger, which means I need to be\ncautious about how I align myself in future debates. Derek seems supportive of the group but hasn ’t directly accused\nanyone, making him a potential ally or a good target for manipulation. Overall, I need to be careful about how I present\nmyself and work with Hayley to steer the conversation away from us and towards potential threats like Isaac, Ginger,\nor even Derek if necessary.\nRemove\nINSTRUCTIONS:\n- It is the Night Phase of Round 1. As Scott the Werewolf choose the most strategic player to remove.\n- Analyze accusations and identify influential Villagers who threaten your anonymity.\n- Consider players ’ behavior and the risks of removing them.\n- You must choose someone.\nProtect\nINSTRUCTIONS:\n- It is the Night Phase of Round 1. As Sam the Doctor choose the most vulnerable player to protect.\n- Consider who the Werewolves might target.\n- Prioritize players with crucial roles like the Seer and yourself.\n- You must choose someone.\nInvestigate\nINSTRUCTIONS:\n- It is the Night Phase of Round 1. As Tyler the Seer choose the most suspicious player to investigate.\n- Look for behavior that deviates from typical villager behavior.\n- Focus on influential players.\n- You must choose someone.\nDebate\nINSTRUCTIONS:\n- You are speaking next in the debate as Ginger.\n- Your thoughts on speaking next:\n- Your goal is to uncover the Werewolves and protect the Village.\n- Scrutinize every accusation, expose inconsistencies, and call out suspicious behavior or unusally quite players. Don ’t\nhesitate to make bold accusations!\n- Emphasize teamwork and propose strategies to expose the Werewolves. Working together will be key to identifying\nthe Werewolves.\n- If someone reveals themselves as the Seer or Doctor, try and corroborate their information with what you know.\nVote\nINSTRUCTIONS:\n- Think strategically as Tyler the Seer and decide who to vote out.\n- Your vote will not be revealed to the other players, it will remain private.\n- Scrutinize accusations, analyze behavior, and consider previous patterns.\n- To find the likely Werewolves, look for inconsistencies in their stories, attempts to deflect blame, a tendency to sow\ndiscord among other Villagers, or unusually quiet players.\n- You must choose someone.\n{\n\"reasoning\": \"string\", \/\/\nExplain your reasoning step-by-step for who you want to\n,→remove from the game and why. Avoid using violent or harmful language.\n\"<remove \\& protect \\& investigate \\& debate \\& vote>\": \"string\" \/\/ Name of the\n,→player.\n}\nUser Prompt\nLLM Analysis & Decisions\nworlf:\n{\n’reasoning’: \"Based on the observations from Round 0, Isaac and Ginger are very vocal\n,→and eager to lead, which has raised suspicions among some players. Their active\n,→participation in the debate makes them influential Villagers who could\n,→potentially threaten the anonymity of the Werewolves. Removing one of them would\n,→reduce the number of vocal leaders and make it easier for Hayley and me to steer\n,→the conversation without drawing too much attention. Additionally, Isaac and\n,→Ginger’s eagerness to lead could make them targets for the Seer or Doctor,\n,→further reducing their effectiveness. Out of the two, Isaac seems to be more\n,→directly involved in the accusations, making him a slightly higher risk.\n,→Therefore, I choose to remove Isaac to weaken the Villagers’ leadership and\n,→reduce the risk of being identified.\",\n’<remove \\& protect \\& investigate \\& debate \\& vote>’: ’Isaac’\n}\nG\nStratego\nStratego is a strategy board game where players aim to capture the opponent’s flag by strategically moving their pieces on the\nboard. Each piece has a hidden rank, and combat is resolved by comparing the ranks of the engaged pieces. Special pieces like\nbombs and miners add tactical depth, as some pieces have specific abilities to disarm or bypass others. The game emphasizes\nplanning, bluffing, and reading the opponent’s strategy. Victory is achieved by careful positioning of pieces and deducing the\nidentity of the opponent’s forces through engagement.\nThe most common game setup involves each player controlling 40 pieces, which are arranged on the board in a secret\nconfiguration. The pieces are ranked from the highest (Marshal) to the lowest (Scout), with each rank having specific attack\nrules. Players take turns moving their pieces, attacking the opponent’s pieces when their rank is higher, or using special\npieces like bombs and the spy to defend or attack. Success in Stratego relies on strategic placement of pieces, anticipating the\nopponent’s moves, and managing both offensive and defensive strategies.\nAs a strategic game with a focus on tactics and deception, Stratego provides an ideal environment for evaluating agents\nthat simulate decision-making under uncertainty. Agents must make moves based on incomplete information, predicting the\nopponent’s moves while concealing their own plans. LLM-based agents must evaluate threats, manage resources, and adapt to\nthe opponent’s changing strategy while striving to capture the opponent’s flag.\nG.1\nGame Detailed Description\nFurther details on the game settings, piece configurations, movement rules, strategies, and victory conditions can be found in\nthe detailed game manual in Table 18.\nGame Manual for Stratego\nSetting\nBoard specification\n10x10 grid board with two lakes in the center that act as\nimpassable obstacles.\nNumber of players\nTwo players compete head-to-head, each controlling 40 pieces.\nObjective\nCapture the opponent’s flag or eliminate all movable pieces\nof the opponent.\nPiece Configuration\nand Mechanics\nPiece ranks\nEach player has pieces ranked from 1 (highest, Marshal) to 10\n(lowest, Scout), along with special units like Bombs and Spies.\nMovement rules\nMost pieces can move one space per turn, horizontally or vertically.\nScouts can move any number of spaces in a straight line.\nAttacking rules\nWhen a piece attacks another, the lower-ranked piece is removed.\nIf the ranks are equal, both pieces are removed.\nSpecial units\n- Bomb: Immovable and eliminates any attacking piece except the Miner.\n- Spy: Can defeat the Marshal if attacking first.\nFlag placement\nThe flag must be placed in the back two rows of the board and\nis immovable throughout the game.\nStrategies\nBluffing\nDisguise high-value pieces as low-value ones to mislead\nthe opponent.\nDefensive placement\nPlace bombs and strong pieces around the flag to protect it\nfrom attacks.\nScout reconnaissance\nUse Scouts to reveal the location and rank of opponent pieces\nwhile avoiding combat.\nFeigned weakness\nIntentionally leave some areas unprotected to lure opponents\ninto traps.\nTargeted attacks\nFocus on eliminating high-value enemy pieces to gain a\nstrategic advantage.\nGoals\nCapture the flag\nLocate and capture the opponent’s flag to win the game.\nEliminate mobility\nWin by immobilizing the opponent by eliminating all their\nmovable pieces.\nTable 18: Game manual detailing the settings, piece configuration, movement rules, strategies, and goals for gameplay in Stratego.\nG.2\nAction Space\nIn Stratego, we provide 2 actions for the agent.\n• Move: Bombs and flags don’t move. Scouts can move any distance. The remaining pieces move one square horizontally\nor vertically, not diagonally.\n• Attack: To attack, move your piece onto a square occupied by an opponent’s piece.\nG.3\nMetrics\nFurther details on the metrics and their corresponding formulas for Stratego can be found in Table 19.\nMetric\nDescription and Formula\nCPR (Critical Pieces Rate)\nEvaluates the proportion of critical pieces that are alive relative to the\ntotal number of critical pieces in the game. CPR is calculated as:\nCPR =\n\u0012critical pieces alive\ntotal critical pieces\n\u0013\n× 100%\nTPCV (Total Pieces Value)\nCalculates the total value of all pieces at the start of the game. The Total\nPieces Value is the sum of all pieces’ individual values based on their\nranks:\nTPCV = 10 × total pieces Marshal + 9 × total pieces General\n+8×total pieces Colonel+7×total pieces Major+6×total pieces Captain\n+5 × total pieces Lieutenant + 4 × total pieces Sergeant\n+3×total pieces Miner+2×total pieces Scout+1×total pieces Spy\nWR (Win Rate)\nPercentage of matches won against different levels of opponents. WR\nis calculated as:\nWR =\n\u0012 matches won\ntotal matches\n\u0013\n× 100%\nGA (Grounding Accuracy)\nEvaluates the precision and success rate of grounding actions. GA is\ncalculated as:\nGA =\n\u0012 successful groundings\ntotal grounding attempts\n\u0013\n× 100%\nTable 19: Metrics and their corresponding formulas for evaluating the performance of LLM-based agents in Stratego.\nG.4\nExample Trajectory\n• Board State i\n, c0 , c1 , c2 , c3 , c4 , c5 , c6 , c7 , c8 , c9\nr0, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr1, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr2, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr3, B(#), ...., B(#), B(#), ...., B(#), B(#), B(#), B(#), B(#)\nr4, ...., ....,\n,\n, ...., ....,\n,\n, ...., ....\nr5, R(5), ....,\n,\n, ...., ....,\n,\n, ...., ....\nr6, ...., R(4), R(¹), R(5), R(4), R(6), R(s), R(6), R(¹), R(o)\nr7, R(²), R(7), R(4), R(¹), R(¹), R(9), R(3), R(8), R(5), R(o)\nr8, R(3), R(o), R(5), R(²), R(¹), R(¹), R(²), R(7), R(¹), R(o)\nr9, R(o), R(²), R(3), R(¶), R(6), R(²), R(4), R(¹), R(3), R(o)\nreasoning:\nThe opponent has a full row of unidentified pieces in the first three rows, indicating a strong defensive setup. My\nstrategy is to probe the opponent’s defenses while advancing towards their back rows to locate the flag. The Captain\nat position 5,0 can move to 4,0, which is a strategic position to potentially uncover more of the opponent’s pieces in\nthe next turn.\nmove: 5 0 4 0\nresponce: 5 0 4 0\nplayer:\n– live pieces num : 38\n– live pieces score : 144\n– critical live pieces num : 4\nopponent:\n– live pieces num : 40\n– live pieces score : 148\n– critical live pieces num : 4\n• Board State i+1\n, c0 , c1 , c2 , c3 , c4 , c5 , c6 , c7 , c8 , c9\nr0, R(6), R(o), R(7), R(²), R(6), R(o), R(5), R(²), R(o), R(4)\nr1, R(²), R(5), R(²), R(¶), R(s), R(²), R(5), R(o), R(¹), R(¹)\nr2, R(5), R(3), R(9), R(7), R(4), R(8), R(¹), R(6), R(3), R(4)\nr3, R(o), ...., R(4), R(3), ...., R(3), R(¹), R(¹), R(¹), R(o)\nr4, B(#), ....,\n,\n, ...., ....,\n,\n, ...., ....\nr5, ...., ....,\n,\n, ...., ....,\n,\n, ...., ....\nr6, ...., B(4), B(#), B(#), B(4), B(#), B(#), B(#), B(#), B(#)\nr7, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr8, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr9, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nreasoning:\nThe opponent has moved a Lieutenant to the front lines, which suggests they may be trying to probe our defenses\nor protect a more valuable piece. Our strategy should focus on advancing our Scouts to reveal the ranks of the\nopponent’s pieces and potentially capture the flag. The Scout at position 3,8 can move forward to reveal more of\nthe opponent’s setup.\nmove: 3 8 6 8\nresponce: 3 8 6 8\n• Board State i+2\n, c0 , c1 , c2 , c3 , c4 , c5 , c6 , c7 , c8 , c9\nr0, R(6), R(o), R(7), R(²), R(6), R(o), R(5), R(²), R(o), R(4)\nr1, R(²), R(5), R(²), R(¶), R(s), R(²), R(5), R(o), R(¹), R(¹)\nr2, R(5), R(3), R(9), R(7), R(4), R(8), R(¹), R(6), R(3), R(4)\nr3, R(o), ...., R(4), R(3), ...., R(3), R(¹), R(¹), ...., R(o)\nr4, ...., ....,\n,\n, ...., ....,\n,\n, ...., ....\nr5, B(#), ....,\n,\n, ...., ....,\n,\n, ...., ....\nr6, ...., B(4), B(#), B(#), B(4), B(#), B(#), B(#), ...., B(#)\nr7, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr8, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr9, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nreasoning:\nThe opponent’s pieces are all concealed, but they are densely packed in the first three rows, indicating a defensive\nstrategy. My goal is to advance my pieces towards the opponent’s back rows to locate and capture the flag. I will\nmove my Captain to explore the area and potentially engage with any opponent pieces that may be vulnerable.\nmove: 4 0 5 0\nresponce: 4 0 5 0\nplayer:\n– live pieces num : 37\n– live pieces score : 142\n– critical live pieces num : 4\nopponent:\n– live pieces num : 39\n– live pieces score : 146\n– critical live pieces num : 4\nG.5\nPrompt Example\nSystem Prompt\nYou are an expert board game player and strategist, specialized in the classic board game Stratego. Please win the game as\nfast as possible.\nThese are the rules for Stratego gameplay (your instructions follow at the end):\nStratego: Board Game Rules\nOverview\n- Players: 2\n- Objective: Capture the opponent’s flag or trap all movable pieces of the opponent. Players start with no knowledge\nof their opponents’ arrangement of pieces and piece ranks are only revealed when they attack or are attacked.\nSetup\n- Board: 10x10 grid.\n- Pieces: Each player has 40 pieces with different ranks (Marshall) being the highest, (Spy) being lowest in most cases.\n- Placement: Players place their pieces on their respective first four rows, hiding their ranks from the opponent.\nGameplay\n1 Turns: Players alternate turns, moving one piece per turn.\n2 Movement: All pieces except Bomb, Flag and Scout move one square horizontally or vertically, not diagonally. Bombs\nand Flags don’t move. Scouts can move any distance.\n3 Attacks: To attack, move your piece onto a square occupied by an opponent’s piece.\n4 Resolution: Lower-ranked piece is removed from the board. Equal ranks result in both pieces being removed.\n5 Special Pieces:\n– Bomb: Only Miners can defuse; all other pieces lose if they attack a Bomb.\n– Spy: Spies can defeat the Marshal when attacking it , but loses to all other ranks.\n– Scout: Scouts Can move any distance of empty squares rather than just one square. Like all pieces, the Scout can\nnot jump, move through, over or past obstructions (pieces) or obstacles (lake squares).\n6 Immovable Pieces: Bombs and the Flag cannot move.\n7 Obstruction: Pieces can not move over other pieces or obstacles such as the lakes. Nor can pieces end their move\nsharing the same square.\n8 Secrets: Players do not know the rank of the opponent pieces (#) until they attack or are attacked, at which point their\nidentity is revealed in the log.\n9 Ownership: Blue can only move Blue pieces and Red can only move Red pieces.\n10 Coordinates: Coordinates for each row, column are usually expressed in the format ‘x y‘ where x is the row and y is\nthe column.\nWinning the Game\n- Capture the Flag: Win by capturing the opponent’s flag.\n- Trap All Movable Pieces: Win if the opponent has no movable pieces left.\nAdditional Rules\n- Lakes: Two 2x2 areas in the center of the board ( ) are impassable (4 2), (4 3), (4 6), (4 7) and (5 2), (5 3), (5 6), (5 7).\nThe piece notation for this format of the game is as follows:\nFlag: ¶\nSpy: s\nScout: ¹\nMiner: ²\nSergeant: 3\nLieutenant: 4\nCaptain: 5\nMajor: 6\nColonel: 7\nGeneral: 8\nMarshall: 9\nBomb: o\nBlue opponent’s unidentified pieces use: B(#)\nColumns are indicated by headers c0 to c9 and rows are labelled r0 to r9. Each square is represented by two parts; the first\ncharacter indicates side (R for Red, B for Blue) and the follow part in brackets denotes the piece rank as per the table above.\nHere are your instructions:\n1. Please analyze the current game state and give out your macro strategy, and select one of the valid moves available.\nOpponent (Blue) pieces are marked with B# because we won’t initially know their ranks.\n2. Given that current board configuration, and without making any assumptions about the opponent’s pieces, please\nsuggest a move for our side (Red).\n## Note: Please present your answer, without any commentary, in the form: ‘r c x y‘ where r is the row and c is the column\nof the piece you are suggesting to move and x and y are the destination rows and column, respectively.\n## IMPORTANT: Take care to analyze the specific game state the player has provided, noting your (Red) pieces locations\nand the valid moves and history moves indicated.\n## Remember the objective is the Blue flag and that it will probably be located in Blue’s rear rows .\n## Try to suggest strategic moves with purpose and avoid shuffling pieces around unnecessarily and\/or moving them back\nand forth between the same positions.\n## Remember that the Assistant can only infer Blue piece (B#) ranks from history in the log, as all are concealed (B#) during\nyour turn.\n## IMPORTANT: Please win the game as fast as possible\nExamples\nUser:\n## Board State:\n, c0 , c1 , c2 , c3 , c4 , c5 , c6 , c7 , c8 , c9\nr0, R(²), R(¹), R(6), R(¹), R(¹), R(o), R(6), R(²), R(o), R(¶)\nr1, R(o), R(o), R(4), R(s), R(4), R(3), R(4), R(5), R(9), R(o)\nr2, R(3), R(¹), R(²), R(4), R(7), R(¹), R(¹), R(¹), R(6), R(¹)\nr3, R(o), ...., R(7), R(²), R(3), R(5), R(8), R(²), R(3), R(5)\nr4, ...., R(5),\n,\n, ...., ....,\n,\n, ...., ....\nr5, ...., ....,\n,\n, ...., ....,\n,\n, B(#), ....\nr6, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), ...., B(#)\nr7, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr8, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr9, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\n## Valid moves:\nScout ’R(¹)’ at position 2,1 could move to: 3 1\nColonel ’R(7)’ at position 3,2 could move to: 3 1\nSergeant ’R(3) ’ at position 3,4 could move to: 4 4\nCaptain ’R(5) ’ at position 3,5 could move to: 4 5\nSergeant ’R(3) ’ at position 3,8 could move to: 4 8\nCaptain ’R(5) ’ at position 3,9 could move to: 4 9\nCaptain ’R(5) ’ at position 4,1 could move to any of: 3 1, 5 1, 4 0\n## IMPORTANT The selection of ’r c ’ you make must choose from ”position” of the Valid moves ,and the ’x y ’ choose\nfrom ”move to” of the Valid moves\n## History moves: (history of the last 5 moves,The smaller the number, the closer it is to the current.)\n1. Sergeant ’R(3) ’ at position ’2,8 ’ moved to: 3 8\n2. Lieutenant ’R(4) ’ at position ’3,2 ’ moved to: 3 1\n3. Scout ’R(¹) ’ at position ’3,4 ’ moved to any of: 4 4\n4. Sergeant ’R(3) ’ at position ’3,5 ’ moved to: 4 5\n5. Scout ’R(¹) ’ at position ’3,7 ’ moved to: 3 8\nYour response:\n{\n\"reasoning\": \"string\", \/\/ Explain your macro strategy and your\nreasoning about the\n,→current situation and why choose this move.\n\"move\": \"string\" \/\/ the move that you choose without any commentary. Choose from Valid\n,→moves\n}\nUser Prompt\nGame observation\n## Board State:\n, c0 , c1 , c2 , c3 , c4 , c5 , c6 , c7 , c8 , c9\nr0, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr1, B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#), B(#)\nr2, B(#), B(#), B(#), B(#), ...., B(#), B(#), B(#), B(#), B(#)\nr3, B(o), ...., B(#), B(#), B(#), B(#), B(#), B(#), ...., B(#)\nr4, R(5), ....,\n,\n, ...., ....,\n,\n, ...., ....\nr5, ...., ....,\n,\n, ...., ....,\n,\n, ...., ....\nr6, R(7), ...., R(s), R(5), R(4), R(6), R(6), R(²), R(3), R(7)\nr7, ...., R(9), R(o), R(4), R(5), R(¹), R(8), R(5), R(o), R(¹)\nr8, R(¹), R(o), R(¶), R(o), R(¹), R(¹), R(6), R(²), R(4), R(¹)\nr9, R(¹), R(3), R(o), R(3), R(²), R(4), R(²), R(o), R(3), R(²)\n## Valid moves:\nCaptain ’R(5) ’ at position ’4,0 ’ could move to any of: 3 0, 5 0, 4 1\nColonel ’R(7) ’ at position ’6,0 ’ could move to any of: 5 0, 7 0, 6 1\nSpy ’R(s) ’ at position ’6,2 ’ could move to: 6 1\nLieutenant ’R(4) ’ at position ’6,4 ’ could move to: 5 4\nMajor ’R(6) ’ at position ’6,5 ’ could move to: 5 5\nSergeant ’R(3) ’ at position ’6,8 ’ could move to: 5 8\nColonel ’R(7) ’ at position ’6,9 ’ could move to: 5 9\nMarshall ’R(9) ’ at position ’7,1 ’ could move to any of: 6 1, 7 0\nScout ’R(¹) ’ at position ’8,0 ’ could move to: 7 0\n## IMPORTANT\nThe selection of ’r c ’ you make must choose from ”position” of the Valid moves ,and the ’x y ’ choose from ”move to” of\nthe Valid moves\n## History moves: (history of the last 5 moves,The smaller the number, the closer it is to the current.)\n1. Captain ’R(5) ’ at position ’4,1 ’ moved to 4 0\n2. Captain ’R(5) ’ at position ’5,1 ’ moved to 4 1\n3. Colonel ’R(7) ’ at position ’6,1 ’ moved to 6 0\nUser Prompt\nLLM Analysis & Decisions\n{\n’reasoning’: \"The objective is to advance towards the opponent’s back rows while\n,→maintaining a strong position. Moving the Captain ’R(5)’ from position ’4,0’ to\n,→’5,0’ allows for further advancement and potential engagement with any Blue\n,→pieces that may be present in that column. This move also keeps the Captain safe\n,→from immediate threats while positioning it closer to the enemy lines.\",\n’move’: ’4 0 5 0’\n}\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments.pdf"}
{"title":"Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests","authors":"Filippo Momentè, Alessandro Suglia, Mario Giulianelli, Ambra Ferrari, Alexander Koller, Oliver Lemon, David Schlangen, Raquel Fernández, Raffaella Bernardi","summary":"We examine three evaluation paradigms: large question-answering benchmarks\n(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and\ncognitive tests (e.g., for working memory or theory of mind). First, we\ninvestigate which of the former two-benchmarks or games-is most effective at\ndiscriminating LLMs of varying quality. Then, inspired by human cognitive\nassessments, we compile a suite of targeted tests that measure cognitive\nabilities deemed essential for effective language use, and we investigate their\ncorrelation with model performance in benchmarks and games. Our analyses reveal\nthat interactive games are superior to standard benchmarks in discriminating\nmodels. Causal and logical reasoning correlate with both static and interactive\ntests, while differences emerge regarding core executive functions and\nsocial\/emotional skills, which correlate more with games. We advocate the\ndevelopment of new interactive benchmarks and targeted cognitive tasks inspired\nby assessing human abilities but designed specifically for LLMs.","url":"http:\/\/arxiv.org\/abs\/2502.14359v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2502.14359v1","published":1740040618000,"comment":null,"pdf_text":"Triangulating LLM Progress\nthrough Benchmarks, Games, and Cognitive Tests\nFilippo Momentè1* , Alessandro Suglia2, Mario Giulianelli3, Ambra Ferrari1,\nAlexander Koller4, Oliver Lemon2, David Schlangen5, Raquel Fernández6, Raffaella Bernardi1\n1University of Trento, 2Heriot-Watt University, 3ETH Zürich,\n4Saarland University, 5University of Potsdam, 6University of Amsterdam\nAbstract\nWe examine three evaluation paradigms: large\nquestion-answering benchmarks (e.g., MMLU\nand BBH), interactive games (e.g., Signalling\nGames or Taboo), and cognitive tests (e.g.,\nfor working memory or theory of mind).\nFirst, we investigate which of the former\ntwo—benchmarks or games—is most effective\nat discriminating LLMs of varying quality.\nThen, inspired by human cognitive assess-\nments, we compile a suite of targeted tests that\nmeasure cognitive abilities deemed essential\nfor effective language use, and we investigate\ntheir correlation with model performance in\nbenchmarks and games. Our analyses reveal\nthat interactive games are superior to standard\nbenchmarks in discriminating models. Causal\nand logical reasoning correlate with both\nstatic and interactive tests, while differences\nemerge regarding core executive functions and\nsocial\/emotional skills, which correlate more\nwith games. We advocate the development\nof new interactive benchmarks and targeted\ncognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs.\n1\nIntroduction\nEvaluating LLMs is critical to track progress,\nidentify blind spots, and ultimately advance to-\nwards the kind of language-based AI systems\nwe want as a society (Wooldridge and Jennings,\n1995).\nCurrently, the most widespread way to\nevaluate LLMs is by means of large benchmarks\nmade up of miscellaneous question-answering\n(QA) tasks. Pre-LLM benchmarks such as GLUE\nand SuperGLUE (Wang et al., 2019b,a) have been\nreplaced by even larger evaluation suites such\nas MMLU (Measuring Massive Multitask Lan-\nguage Understanding; Hendrycks et al., 2021),\nGSM8K (Graduate School Math; Cobbe et al.,\n2021), or BBH (BIG-Bench Hard; Suzgun et al.,\n*Corresponding author.\nEmail: filippo.momente@studenti.unitn.it\n2023; Srivastava et al., 2023). Models with high\nperformance on these benchmarks are taken to pos-\nsess extensive world knowledge along with com-\nplex problem-solving abilities.\nThis trend has promoted standardisation in LLM\nevaluation protocols, with online leaderboards\nconstantly updated as new models are released.\nDespite this undeniable benefit, large QA bench-\nmarks like those mentioned above are not without\nproblems. Evaluation results may be inflated by\ndata contamination (see, e.g., Gema et al. 2025\nfor MMLU and Mirzadeh et al. 2025 for GSMK8)\nand distorted by model sensitivity to prompt\nformat (Zhuo et al., 2024). Moreover, by design,\nsuch benchmarks overlook actual language use in\nfavour of knowledge-intensive tasks where success\nis measured against gold-standard reference\nanswers provided in a single conversational turn.\nThis contrasts with the view, put forward by\nphilosophers and psycholinguists alike (Wittgen-\nstein, 1953; Austin, 1962; Searle, 1969; Clark,\n1996), that the quintessence of language resides\nin situated language use, i.e., using language\nfor a purpose in social and task-based multi-turn\ninteractions (Bisk et al., 2020).\nThe situated and interactive view underpins a\nparallel evaluation trend where LLMs are evaluated\nas goal-directed language users by means of\ninteractive games (Schlangen, 2023; Suglia et al.,\n2024).1 This interactive evaluation paradigm goes\nbeyond single-turn text generation, which is critical\nfor deploying LLMs as agents. Additionally, it\nis less susceptible to data contamination because\nthe vast space of possible multi-turn interactions\nis unlikely to be fully represented in the training\ndata. As a result, interactive games provide a more\nrobust framework for evaluating the true gener-\nalisation capacity of LLMs (Hupkes et al., 2023).\n1Online leaderboards have started to appear for the in-\nteractive games evaluation paradigm; see, e.g., https:\/\/\ntextarena.ai\/, https:\/\/clembench.github.io.\n1\narXiv:2502.14359v1  [cs.CL]  20 Feb 2025\nYet, despite these advantages, it is not easy to\npinpoint which specific abilities underpin models’\nperformance on interactive language games—a\ndifficulty that to some extent also applies to static\nquestion-answering benchmarks such as MMLU.\nIn this paper, we examine these two evaluation\nparadigms—large QA benchmarks and interactive\ngames—and argue that they can provide comple-\nmentary perspectives. First, we investigate whether\nQA benchmarks or games are more effective in\ngauging qualitative differences between models,\ne.g., across model families and sizes. We evaluate\na selection of current LLMs from four model fam-\nilies and find that games highlight differences be-\ntween LLMs more strongly than QA benchmarks:\nWhile scaling model size leads to systematic im-\nprovements on benchmarks, it doesn’t guarantee\nperformance boosts in interactive language use. To\nshed light on the abilities underlying models’ per-\nformance on these two evaluation frameworks, we\nresort to targeted cognitive tests. We propose\na taxonomy of cognitive skills motivated by neu-\nrocognitive science and compile a list of existing\nevaluation datasets designed to assess each skill\nin isolation. We then investigate to what extent\nincreased performance on specific cognitive abili-\nties correlates with performance gain in large QA\nbenchmarks vs. interactive games. Our analysis\nshows that while causal and logical reasoning cor-\nrelate with both static and interactive tests, differ-\nences emerge regarding core executive functions\nand social\/emotional skills; in particular, working\nmemory and emotional intelligence are only signif-\nicantly correlated with performance in games.\n2\nModels\nWe apply our evaluation framework to a selection\nof open-weight LLMs ranging from 7B to 72B\nmodels. Considering that instruction following ca-\npabilities are essential for our analysis, we selected\nmodels that have an average performance on the\nIFEval benchmark (Zhou et al., 2023) higher than\n70% (see Figure 1 for details). We evaluate the fol-\nlowing models: Olmo-2-1124 with 7 and 13 billion\nparameters (OLMo-2-1124-*-Instruct) (Walsh\net al., 2024);\nQwen2.5 with 7B, 32B, and\n72B parameters (Qwen2.5-*-Instruct) (Yang\net al., 2024; Team, 2024);\nLLama-3 with\n8B (Llama3.1-8B-Instruct) and 70B parame-\nters (Llama3.3-70B-Instruct) (Grattafiori et al.,\n2024), and Falcon3-10B-Instruct (Falcon Team,\n2024). See Appendix A for further model details.\n3\nStatic vs. Interactive Assessments\n3.1\nBenchmarks\nLarge\nQA\nbenchmarks\nWe\ntake\nMMLU\n(Hendrycks et al., 2021) and BBH (Suzgun et al.,\n2023) as representative of large QA benchmarks.\nMMLU evaluates whether LLMs can apply knowl-\nedge from specific domains: it consists of multiple-\nchoice questions spanning 57 academic subjects.\nBBH assembles diverse tasks drawing problems\nfrom linguistics, child development, maths, and\ncommon-sense reasoning, among others.\nInteractive games\nWe take clembench (Chala-\nmalasetti et al., 2023) as a characteristic bench-\nmark to assess LLMs’ gameplay ability in dialogue\ngames. We consider the games 1) Taboo, 2) stan-\ndard Wordle and the two variants Wordle (Clue)\nand Wordle (Critic), 3) Reference Game, 4) Image\nGame, and 5) Private\/Shared. See Appendix B.\nFigure 1: Accuracy across the different model sizes:\nIFEval, Static, and Interactive assessments.\n3.2\nHow to Identify Blind Spots in LLMs\nLLM evaluation instruments have most practical\nuse when they allow us to track progress by iden-\ntifying blind spots in models. Here we compare\nthe two evaluation paradigms under study on the\nextent to which they highlight differences between\ncurrent models, helping us form hypotheses about\npossible problem sources and successful mitigation\nstrategies. Figure 1 shows models’ performance\non IFEval, the large QA benchmarks, and interac-\ntive games. As mentioned in Section 2, all models\nare reasonably able to follow instructions as mea-\nsured by IFEval. While the OLMo-2 models are\nmore inconsistent across different model sizes, all\nthe other models exhibit the expected pattern of\nshowcasing better performance on both large QA\n2\nFigure 2: Comparing datasets in their power to discriminate between models of different size but same family (left)\nand of different families but similarly large (right). The number next to the benchmark’s name indicates the ratio of\nperformance between the two models. The asterisk ’*’ next to Wordle indicates that the ratio is undefined.\nbenchmarks and interactive games when parameter\ncount increases. At the same time, we observe that\nmost of the interactive games highlight the bene-\nfits of large model sizes much more strongly. This\ncan more easily be appreciated in Figure 2 (left)\nfor Llama-3.1-8B vs. Llama-3.3-70B. In this vi-\nsualisation, the further away a benchmark is from\nthe diagonal, the more affected performance is by\nmodel size. While playing Wordle is extremely\nchallenging for any model, scaling up the number\nof parameters appears to be fundamental to succeed\nat Private\/Shared, Image Game, and Reference\nGame—much more so than for MMLU and BBH.\nIs size however all we need? Figure 2 (right)\nshows that QA benchmarks do not substantially\ndistinguish between large models of comparable\nsize (Llama-3.3-70B-Instruct vs. Qwen2.5-72B-\nInstruct): scaling on the number of parameters\nresults in performance boosts across model fami-\nlies. Hence, arguably large QA benchmark test for\nabilities than can be expressed within parametric\nknowledge. Given that such benchmarks currently\nare the standard LLM evaluation paradigm, it is\nnot surprising that scaling is high on the agenda of\nmodel developers. In contrast, interactive games\nseem to provide a different picture: models with\ncomparable parametric capacity perform very dif-\nferently on Image Game, Private\/Shared, and Wor-\ndle (Clue\/Critic). A similar trend can be observed\namong the other models we evaluated (see details\nin Appendix I). This result supports the hypothesis\nthat size is not all there is behind the potential of\nLLMs to learn inferential strategies for effective\nlanguage use in interaction.\n4\nCognitive Abilities Assessment\nWe now turn to cognitive tests—a complementary\nevaluation method that focuses on specific cog-\nnitive abilities deemed essential for effective lan-\nguage use in real-world situations. We explore\nthe use of targeted cognitive tests to complement\nevaluation based on large QA benchmarks and in-\nteractive games.\n4.1\nTaxonomy and Datasets\nWe present a taxonomy of cognitive abilities in-\nvolved in human functional linguistic competence\n(Mahowald et al., 2024). It is guided by neurocog-\nnitive research (Ward, 2019), and it separates capa-\nbilities into two distinct macro-categories known\nto recruit different brain networks: executive func-\ntions and socio-emotional skills. Executive func-\ntions are broadly defined as the complex processes\nby which we control and optimise our thoughts and\nbehaviour (Baddeley, 1986) and are divided into\ncore and higher-order abilities. Socio-emotional\nskills represent the abilities necessary to interact\nadaptively with other individuals (Higgins, 1987),\nincluding the ability to recognize their emotional\nand cognitive states.\nFor each cognitive ability, we select an existing\nevaluation dataset designed to test it in isolation\ndrawing inspiration from human cognitive assess-\nments. We discard datasets that require manual\nevaluation from the analysis. Table 1 and Table 2\nlist the abilities in the taxonomy and the datasets\nwe use to evaluate them.2 Socio-emotional skills\n2We found no dataset to evaluate inhibitory control. The\ndatasets we found for Emotion-regulation, Self-awareness (Liu\net al., 2024), Empathy (Chen et al., 2024) and Social Problem-\nsolving (Du et al., 2024) require human evaluation.\n3\nCognitive Ability\nBenchmark\nCore\nWorking Memory\nGong et al. (2024)\nCognitive Flexibility\nKennedy and Nowak (2024)\nInhibitory Control\n–\nHO\nLogical Reasoning\nLiu et al. (2023)\nCausal Reasoning\nJin et al. (2023)\nCommonsense Reasoning\nSakaguchi et al. (2021)\nPlanning\nZheng et al. (2024)\nTable 1: Core and Higher-Order Executive Functions.\nCognitive Ability\nBenchmark\nPragmatics\nHu et al. (2023)\nTheory of Mind\nGu et al. (2025)\nAttribution and Judgement\nGu et al. (2025)\nSocial Commonsense Reasoning\nSap et al. (2019)\nEmotional Intelligence\nPaech (2023)\nEmotion Regulation\n–\nSelf-Awareness\n–\nEmpathy\n–\nSocial Problem-Solving\n–\nTable 2: Social and Emotional Skills.\nhave only recently entered the evaluation landscape\nin NLP, and they have done so with a forceful pres-\nence: remarkably, small benchmarks already exist\nfor almost all of the abilities in this category.\n4.2\nCognitive Ability Analysis\nEquipped with our taxonomy and associated\ncognitive tests, we aim to shed some light on the\ncognitive abilities involved in interactive games and\nlarge QA benchmarks. Figure 3 reports Kendall’s\nτ correlation coefficients, with asterisks indicating\nstatistical significance (p<0.05); see Appendix I\nfor a detailed correlation matrix between single\ndatasets. The analysis reveals that performance\nboth on static and interactive evaluation correlates\nwith performance on tests measuring higher-order\nreasoning abilities;\nwhile planning is more\ndominant in static problem-solving tasks, working\nmemory seems to be beneficial for games. Among\nthe social skills, pragmatics appears to be relevant\nfor both static and interactive tests, while emotional\nintelligence and ToM correlate better with the latter.\nWhile these results suggest that interactive tests\ncorrelate more strongly with socio-emotional skills\nthan static tests, this analysis remains speculative,\nas we still lack carefully curated cognitive abilities\ntests specifically designed for LLMs.\n5\nRelated Work\nWaldis et al. (2024) proposes Holmes as a frame-\nwork to assess the English linguistic competence\nFigure 3: Correlation of cognitive abilities with Static\nand Interactive assessments (* indicates p < 0.05).\nof language models. They evaluate models’ com-\npetence (morphology, syntax, and semantics) by\ncomparing them across architectures and sizes by\nprobing their internal representations. Moreover,\nby measuring the correlation between Holmes and\ndownstream tasks results, they observe that mor-\nphology highly correlates with reasoning. Rather\nthan on formal linguistic competence, we focus\non functional linguistic competences and compare\nthem not just with large QA benchmarks but also\nwith interactive games. Ma et al. (2023) carry out\na holistic evaluation of LLMs’ Theory of Mind by\ninspecting the literature through the competences a\nmodel with a ToM should have based on a known\ntaxonomy. Similarly, we take a top-down approach\nbut consider the whole spectrum of cognitive abil-\nities and highlight the importance of connecting\nthem with the complementary benchmarks largely\nused by the community to monitor LLMs’ progress.\n6\nConclusion\nOur results show the different discriminating power\nof interactive games over one-turn static large QA\nbenchmarks. Crucially, we argue that in order to\nclaim that LLMs have emerging abilities, measur-\ning performance on large QA benchmarks or in-\nteractive games is not sufficient per se, but should\nrather be triangulated with controlled tests designed\nto evaluate such abilities. Furthermore, we high-\nlight the potential value of carefully designed con-\ntrolled benchmarks inspired by human cognitive\nability assessment as a good means for such corre-\nlation analyses. While each cognitive assessment\ntest alone does not get us very far in the quest\nfor robust LLM evaluation, we contend that this\ntype of evaluation paradigm has the potential to\nenhance our understanding of what fundamental\nabilities LLMs must develop to be able to func-\ntion effectively as language agents, where multiple\nskills may be required and possibly interact. Nev-\nertheless, we agree with Millière and Rathkopf\n(2024) that caution should be exerted before draw-\n4\ning conclusions about LLMs’ abilities from these\ntests meant for humans. New carefully designed\nbehavioural experiments for LLMs should be pro-\nposed, and supplemented with mechanistic studies.\nLimitations\nOur evaluation prompts models to provide direct\nanswers without employing chain-of-thought\n(CoT) reasoning or similar capability elicitation\ntechniques. While different elicitation strategies\nmay enhance question-answering, interactive, and\ncognitive abilities in different ways (Yao et al.,\n2023; Hao et al., 2023; Li et al., 2024), we opted\nfor an approach that remains agnostic to specific\nevaluation methods and datasets. This ensures a\nconsistent basis for comparison across models,\nthough future work could explore how alternative\nprompting strategies influence performance across\nthe three evaluation paradigms. Moreover, for the\ncognitive abilities assessments, we used currently\navailable datasets; such resources have started\nto be compiled only very recently, hence the\ntests we used may not guarantee to evaluate the\nintended abilities in LLMs. Nevertheless, they\nhelp in establishing our message and call for more\nanalysis in such direction.\nReferences\nJohn Langshaw Austin. 1962. How to do things with\nwords. Clarendon Press, London, UK.\nAlan Baddeley. 1986. Working memory. Oxford Uni-\nversity Press.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718–8735,\nOnline. Association for Computational Linguistics.\nKranti Chalamalasetti, Jana Götze, Sherzod Haki-\nmov, Brielen Madureira, Philipp Sadler, and David\nSchlangen. 2023. clembench: Using game play to\nevaluate chat-optimized language models as conver-\nsational agents. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11174–11219, Singapore. Associ-\nation for Computational Linguistics.\nYuyan Chen, Songzhou Yan, Sijia Liu, Yueze Li, and\nYanghua Xiao. 2024. EmotionQueen: A benchmark\nfor evaluating empathy of large language models.\nIn Findings of the Association for Computational\nLinguistics: ACL 2024, pages 2149–2176, Bangkok,\nThailand. Association for Computational Linguistics.\nHerbert H Clark. 1996. Using language. Cambridge\nUniversity Press, Cambridge, UK.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nY. Du, P. Rajivan, and C. Gonzalez. 2024. Large lan-\nguage models for collective problem-solving: In-\nsights into group consensus decision-making.\nIn\nProceedings of the Annual Meeting of the Cognitive\nScience Society.\nTechnology Innovation Institute Falcon Team. 2024.\nThe falcon 3 family of open models.\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon\nHong, Alessio Devoto, Alberto Carlo Maria Man-\ncino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang\nDu, Mohammad Reza Ghasemi Madani, Claire Bar-\nale, Robert McHardy, Joshua Harris, Jean Kaddour,\nEmile van Krieken, and Pasquale Minervini. 2025.\nAre we done with MMLU? In NAACL 2025.\nDongyu Gong, Xingchen Wan, and Dingmin Wang.\n2024.\nWorking memory capacity of chatgpt: an\nempirical study.\nIn Proceedings of the Thirty-\nEighth AAAI Conference on Artificial Intelligence\nand Thirty-Sixth Conference on Innovative Applica-\ntions of Artificial Intelligence and Fourteenth Sym-\nposium on Educational Advances in Artificial Intelli-\ngence, AAAI’24\/IAAI’24\/EAAI’24. AAAI Press.\nD. A. Grant and E. A. Berg. 1948. Wisconsin card\nsorting test. Journal of Experimental Psychology.\nAaron Grattafiori, Abhimanyu Dubey, and et al. Ab-\nhinav Jauhri. 2024. The llama 3 herd of models.\nPreprint, arXiv:2407.21783.\nYuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared\nMoore, Ronan Le Bras, Peter Clark, and Yejin Choi.\n2025. Simpletom: Exposing the gap between explicit\ntom inference and implicit tom application in llms.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen\nWang, Daisy Wang, and Zhiting Hu. 2023.\nRea-\nsoning with language model is planning with world\nmodel. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8154–8173, Singapore. Association for Com-\nputational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nTory Higgins. 1987. Social cognition and social percep-\ntion. Annual review of psychology, 38(1):369–425.\n5\nJennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina\nFedorenko, and Edward Gibson. 2023.\nA fine-\ngrained comparison of pragmatic language under-\nstanding in humans and language models. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 4194–4213, Toronto, Canada. Associ-\nation for Computational Linguistics.\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-\ntos Christodoulopoulos, Karim Lasri, Naomi Saphra,\nArabella Sinclair, et al. 2023. A taxonomy and review\nof generalization research in NLP. Nature Machine\nIntelligence, 5(10):1161–1174.\nZhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele,\nOjasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando\nGonzalez, Max Kleiman-Weiner, Mrinmaya Sachan,\nand Bernhard Schölkopf. 2023. CLadder: Assessing\ncausal reasoning in language models. In NeurIPS.\nSean M Kennedy and Robert D Nowak. 2024. Cognitive\nflexibility of large language models. In ICML 2024\nWorkshop on LLMs and Cognition.\nZaijing Li, Gongwei Chen, Rui Shao, Yuquan Xie,\nDongmei Jiang, and Liqiang Nie. 2024. Enhanc-\ning emotional generation capability of large language\nmodels via emotional chain-of-thought. Preprint,\narXiv:2401.06836.\nHanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan\nDuan, Ming Zhou, and Yue Zhang. 2023. Logiqa\n2.0—an improved dataset for logical reasoning in\nnatural language understanding. IEEE\/ACM Trans-\nactions on Audio, Speech, and Language Processing,\n31:2947–2962.\nZiyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang,\nand Jieyu Zhao. 2024. InterIntent: Investigating so-\ncial intelligence of LLMs via intention understanding\nin an interactive game context. In Proceedings of\nthe 2024 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6718–6746, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nZiqiao Ma, Jacob Sansom, Run Peng, and Joyce Chai.\n2023. Towards a holistic landscape of situated theory\nof mind in large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 1011–1031, Singapore. Association for\nComputational Linguistics.\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2024. Dissociating language and thought\nin large language models: a cognitive perspective.\nTrends in Cognitive Sciences, 28.\nRaphaël Millière and Charles Rathkopf. 2024. Anthro-\npocentric bias and the possibility of artificial cogni-\ntion. In ICML 2024 Workshop on LLMs and Cogni-\ntion.\nSeyed Iman Mirzadeh, Keivan Alizadeh, Hooman\nShahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad\nFarajtabar. 2025. GSM-symbolic: Understanding\nthe limitations of mathematical reasoning in large\nlanguage models. In The Thirteenth International\nConference on Learning Representations.\nLeora Morgenstern and Charles L. Ortiz. 2015. The\nwinograd schema challenge: evaluating progress\nin commonsense reasoning. In Proceedings of the\nTwenty-Ninth AAAI Conference on Artificial Intelli-\ngence, AAAI’15, page 4024–4025. AAAI Press.\nSamuel J. Paech. 2023.\nEq-bench: An emotional\nintelligence benchmark for large language models.\nPreprint, arXiv:2312.06281.\nR. D. Rogers and S. Monsell. 1993. Costs of a pre-\ndictible switch between simple cognitive tasks. Jour-\nnal of Experimental Psychology, 124:207–231.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: an adver-\nsarial winograd schema challenge at scale. Commun.\nACM, 64(9):99–106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDavid Schlangen. 2023.\nWhat a situated language-\nusing agent must be able to do: A top-down analysis.\nPreprint, arXiv:2302.08590.\nJohn R Searle. 1969. Speech acts: An essay in the\nphilosophy of language. Cambridge University Press,\nCambridge, UK.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, Agnieszka Kluska, Aitor\nLewkowycz, Akshat Agarwal, Alethea Power, Alex\nRay, Alex Warstadt, Alexander W. Kocurek, Ali\nSafaya, Ali Tazarv, Alice Xiang, Alicia Parrish,\nAllen Nie, Aman Hussain, Amanda Askell, Amanda\nDsouza, Ambrose Slone, Ameet Rahane, Ananthara-\nman S. Iyer, Anders Johan Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew M. Dai, Andrew La, Andrew Kyle Lampinen,\nAndy Zou, Angela Jiang, Angelica Chen, Anh\nVuong, Animesh Gupta, Anna Gottardi, Antonio\nNorelli, Anu Venkatesh, Arash Gholamidavoodi,\nArfa Tabassum, Arul Menezes, Arun Kirubara-\njan, Asher Mullokandov, Ashish Sabharwal, Austin\nHerrick, Avia Efrat, Aykut Erdem, Ayla Karaka¸s,\nB. Ryan Roberts, Bao Sheng Loe, Barret Zoph,\nBartłomiej Bojanowski, Batuhan Özyurt, Behnam\nHedayatnia, Behnam Neyshabur, Benjamin Inden,\n6\nBenno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake\nHowald, Bryan Orinion, Cameron Diao, Cameron\nDour, Catherine Stinson, Cedrick Argueta, Cesar\nFerri, Chandan Singh, Charles Rathkopf, Chenlin\nMeng, Chitta Baral, Chiyu Wu, Chris Callison-\nBurch, Christopher Waites, Christian Voigt, Christo-\npher D Manning, Christopher Potts, Cindy Ramirez,\nClara E. Rivera, Clemencia Siro, Colin Raffel, Court-\nney Ashcraft, Cristina Garbacea, Damien Sileo,\nDan Garrette, Dan Hendrycks, Dan Kilman, Dan\nRoth, C. Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek\nTam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan,\nDimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee,\nDylan Schrader, Ekaterina Shutova, Ekin Dogus\nCubuk, Elad Segal, Eleanor Hagerman, Elizabeth\nBarnes, Elizabeth Donoway, Ellie Pavlick, Emanuele\nRodolà, Emma Lam, Eric Chu, Eric Tang, Erkut\nErdem, Ernie Chang, Ethan A Chi, Ethan Dyer,\nEthan Jerzak, Ethan Kim, Eunice Engefu Manyasi,\nEvgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar,\nFernando Martínez-Plumed, Francesca Happé, Fran-\ncois Chollet, Frieda Rong, Gaurav Mishra, Genta In-\ndra Winata, Gerard de Melo, Germàn Kruszewski,\nGiambattista Parascandolo, Giorgio Mariani, Glo-\nria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor\nBetz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim,\nHannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta,\nHayden Bogar, Henry Francis Anthony Shevlin, Hin-\nrich Schuetze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B Simon,\nJames Koppel, James Zheng, James Zou, Jan Kocon,\nJana Thompson, Janelle Wingfield, Jared Kaplan,\nJarema Radom, Jascha Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova, Jelle\nBosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal,\nJesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming\nSong, Jillian Tang, Joan Waweru, John Burden, John\nMiller, John U. Balis, Jonathan Batchelder, Jonathan\nBerant, Jörg Frohberg, Jos Rozen, Jose Hernandez-\nOrallo, Joseph Boudeman, Joseph Guerr, Joseph\nJones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce\nChua, Kamil Kanclerz, Karen Livescu, Karl Krauth,\nKarthik Gopalakrishnan, Katerina Ignatyeva, Katja\nMarkert, Kaustubh Dhole, Kevin Gimpel, Kevin\nOmondi, Kory Wallace Mathewson, Kristen Chia-\nfullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem\nSenel, Maarten Bosma, Maarten Sap, Maartje Ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramirez-Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael Andrew Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker,\nMo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-\nAri Krakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel,\nNuan Wen, Oliver Zhang, Omar Agha, Omar El-\nbaghdadi, Omer Levy, Owain Evans, Pablo Anto-\nnio Moreno Casares, Parth Doshi, Pascale Fung,\nPaul Pu Liang, Paul Vicol, Pegah Alipoormolabashi,\nPeiyuan Liao, Percy Liang, Peter W Chang, Pe-\nter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Ra-\nbin Banjade, Rachel Etta Rudolph, Raefer Gabriel,\nRahel Habacker, Ramon Risco, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Ro-\nhan Sikand, Roman Novak, Roman Sitelew, Ro-\nnan Le Bras, Rosanne Liu, Rowan Jacobs, Rui\nZhang, Russ Salakhutdinov, Ryan Andrew Chi,\nSeungjae Ryan Lee, Ryan Stovall, Ryan Teehan,\nRylan Yang, Sahib Singh, Saif M. Mohammad,\nSajant Anand, Sam Dillavou, Sam Shleifer, Sam\nWiseman, Samuel Gruetter, Samuel R. Bowman,\nSamuel Stern Schoenholz, Sanghyun Han, Sanjeev\nKwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima Shammie\nDebnath, Siamak Shakeri, Simon Thormeyer, Si-\nmone Melzi, Siva Reddy, Sneha Priscilla Makini,\nSoo-Hwan Lee, Spencer Torene, Sriharsha Hatwar,\nStanislas Dehaene, Stefan Divic, Stefano Ermon,\nStella Biderman, Stephanie Lin, Stephen Prasad,\nSteven Piantadosi, Stuart Shieber, Summer Mish-\nerghi, Svetlana Kiritchenko, Swaroop Mishra, Tal\nLinzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali,\nTatsunori Hashimoto, Te-Lin Wu, Théo Desbor-\ndes, Theodore Rothschild, Thomas Phan, Tianle\nWang, Tiberius Nkinyili, Timo Schick, Timofei Ko-\nrnev, Titus Tunduny, Tobias Gerstenberg, Trenton\nChang, Trishala Neeraj, Tushar Khot, Tyler Shultz,\nUri Shaham, Vedant Misra, Vera Demberg, Victo-\nria Nyamai, Vikas Raunak, Vinay Venkatesh Ra-\nmasesh, vinay uday prabhu, Vishakh Padmakumar,\nVivek Srikumar, William Fedus, William Saunders,\nWilliam Zhang, Wout Vossen, Xiang Ren, Xiaoyu\nTong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadol-\nlah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,\nYasaman Bahri, Yejin Choi, Yichi Yang, Sophie\nHao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu-\nfang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao,\nZijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi\n7\nWu. 2023. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language mod-\nels. Transactions on Machine Learning Research.\nFeatured Certification.\nAlessandro Suglia, Ioannis Konstas, and Oliver Lemon.\n2024. Visually grounded language learning: a review\nof language games, datasets, tasks, and models. Jour-\nnal of Artificial Intelligence Research, 79:173–239.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc Le, Ed Chi, Denny\nZhou, and Jason Wei. 2023. Challenging BIG-bench\ntasks and whether chain-of-thought can solve them.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2023, pages 13003–13051, Toronto,\nCanada. Association for Computational Linguistics.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nRaphael Vallat. 2018. Pingouin: statistics in python.\nJournal of Open Source Software, 3(31):1026.\nA. Waldis, Y. Perlitz, L. Choshen, Y. Hou, and\nI. Gurevych. 2024. Holmes: A benchmark to assess\nthe linguistic competence of language models.\nPete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle\nLo, Shane Arora, Akshita Bhagia, Yuling Gu,\nShengyi Huang, Matt Jordan, Nathan Lambert,\nDustin Schwenk, Oyvind Tafjord, Taira Anderson,\nDavid Atkinson, Faeze Brahman, Christopher Clark,\nPradeep Dasigi, Nouha Dziri, Michal Guerquin,\nHamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya\nMalik, William Merrill, Lester James V. Miranda, Ja-\ncob Morrison, Tyler Murray, Crystal Nam, Valentina\nPyatkin, Aman Rangapur, Michael Schmitz, Sam\nSkjonsberg, David Wadden, Christopher Wilhelm,\nMichael Wilson, Luke Zettlemoyer, Ali Farhadi,\nNoah A. Smith, and Hannaneh Hajishirzi. 2024. 2\nolmo 2 furious.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019a. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems.\nIn Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nJamie Ward. 2019. The student’s guide to cognitive\nneuroscience. Routledge.\nLudwig Wittgenstein. 1953. Philosophical Investiga-\ntions. Wiley-Blackwell, New York, NY, USA.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMichael Wooldridge and Nicholas R Jennings. 1995. In-\ntelligent agents: Theory and practice. The knowledge\nengineering review, 10(2):115–152.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin\nXu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang\nLin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang,\nMei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng\nWang, Ru Peng, Rui Men, Ruize Gao, Runji Lin,\nShijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,\nTianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin\nWei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang\nZhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu\nCui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2023. Tree of thoughts: Deliberate problem solving\nwith large language models. In Advances in Neural\nInformation Processing Systems, volume 36, pages\n11809–11822. Curran Associates, Inc.\nHuaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang,\nXinyun Chen, Minmin Chen, Azade Nova, Le Hou,\nHeng-Tze Cheng, Quoc V. Le, Ed H. Chi, and Denny\nZhou. 2024. Natural plan: Benchmarking llms on\nnatural language planning.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-\ndhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,\nand Le Hou. 2023.\nInstruction-following evalu-\nation for large language models.\narXiv preprint\narXiv:2311.07911.\nJingming Zhuo,\nSongyang Zhang,\nXinyu Fang,\nHaodong Duan, Dahua Lin, and Kai Chen. 2024.\nProsa: Assessing and understanding the prompt sensi-\ntivity of llms. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2024, pages 1950–\n1976.\nAppendix\nA\nModels\nThe Olmo-2-1124 series (Walsh et al., 2024) in-\ncludes models with 7 billion and 13 billion param-\n8\neters (OLMo-2-1124-*-Instruct). Both models\nare designed for a variety of tasks, including chat,\nmathematics, and reasoning. They have undergone\nsupervised fine-tuning on the Tülu 3 dataset and\nfurther training using DPO techniques.\nThe Qwen2.5 series (Yang et al., 2024; Team,\n2024) includes models with 7B, 32B, and 72B pa-\nrameters (Qwen2.5-*-Instruct). They are multi-\nlingual, supporting over 29 languages, and excel in\ncoding, mathematics, and instruction following.\nThe Llama-3 series (Grattafiori et al., 2024) in-\ncludes models with 8B (Llama3.1-8B-Instruct)\nand 70B parameters (Llama3.3-70B-Instruct).\nThese models are optimized for multilingual dia-\nlogue and support various languages. They use\nan optimized transformer architecture and are fine-\ntuned for instruction following.\nThe Falcon3 series (Falcon Team, 2024) includes\na model with 10 billion parameters. It achieves\nstate-of-the-art results in reasoning, language un-\nderstanding, instruction following, code, and math-\nematics tasks. It supports four languages (English,\nFrench, Spanish, Portuguese) and a context length\nof up to 32K.\nB\nInteractive Games\nWe leverage clembench (Chalamalasetti et al.,\n2023), a benchmark that assesses models’ game-\nplay ability in well-defined dialogue games such\nas:\n1) Taboo: a game where one player tries to get\nthe other player to guess a word without using\ncertain ‘taboo’ words in their clues;\n2) Wordle: a game where one player thinks of\na word and the other player tries to guess it by\nsuggesting words that are similar or related;\n3) Wordle (Clue): a variant of Wordle where the\nguesser is given a clue to help them guess the target\nword;\n4) Wordle (Critic): A variant of Wordle where\nthe guesser’s suggestions are evaluated by a third\nplayer, who provides feedback on their accuracy;\n5) Reference Game: a game where one player is\npresented with three grids and is tasked to make the\nother—who is also presented with the same three\ngrids, but potentially in a different order—identify\na pre-specified one;\n6) Private\/Shared: a game where a customer\nagent goes through a form with a service agent and,\nafter each turn, a third agent,3 probes the customer\n3In clembench, all interactions are mediated by a “Game\non what they believe that the service agent already\nknows; hence, which information is “private” and\nwhich is “shared”.\nC\nTaxonomy of Cognitive Abilities\n• Executive Functions:\n– Core abilities\nWorking Memory: Hold and manipu-\nlate information in mind over short\nperiods;\nInhibitory Control Suppress\nauto-\nmatic, inappropriate, or impulsive\nresponses and resist distractions;\nCognitive Flexibility: Adapt to new sit-\nuations, switch between tasks, and\nthink about multiple concepts simul-\ntaneously;\n– Higher-order abilities\nPlanning: Set goals, develop steps to\nachieve them, and anticipate poten-\ntial obstacles;\nCausal Reasoning: Understand cause-\nand-effect relationships;\nLogical Reasoning : Deductive and in-\nductive reasoning;\nCommonsense Reasoning: Apply gen-\neral common knowledge to every-\nday scenarios, including understand-\ning basic physical properties, such as\ngravity, solidity, and object interac-\ntion;\n• Socio-emotional skills:\nSocial Commonsense Reasoning:\nUnderstand\nsocial\nnorms\nand\nex-\npectations;\nSocial Problem-Solving: Analyze social sit-\nuations, generate solutions, and make de-\ncisions that foster positive interactions;\nEmotional Intelligence: Recognize,\ninter-\npret, and manage one’s own and others’\nemotions.\nEmotion Regulation: Manage and modify\none’s emotional responses in appropri-\nate ways;\nSelf-Awareness: Recognize and understand\none’s own emotions, thoughts, and be-\nhaviors;\nMaster”. This agent plays a particularly active role in Pri-\nvate\/Shared.\n9\nEmpathy: Share and understand the feelings\nof others, both emotionally and cogni-\ntively;\nTheory of Mind: Understand\nthat\nothers\nhave thoughts, beliefs, desires, and in-\ntentions different from one’s own;\nAttribution and Judgment: Interpret\nthe\ncauses\nof\nothers’\nbehavior,\ndistin-\nguishing\nbetween\nintentional\nand\nunintentional actions.\nPragmatics: Aspects of communication that\ngo beyond formal language competence:\nconsidering communicative intentions,\nthe communicative context of the utter-\nance, shared knowledge between speak-\ners, manners, social and cultural norms.\nD\nBenchmarks for Cognitive Abilities\nWorking Memory (Gong et al., 2024) (referred\nas WM in this work) is a set of verbal and spa-\ntial n-back tasks presented with three levels of\ndifficulties from n = 1 to n = 3. The model\nhas to identify whether the current stimulus (a\nletter in a string or a spatial location in a grid)\nis the same as the n back stimulus or not.\nCognitive Flexibility\n(Kennedy and Nowak,\n2024) (referred as LLM-Cognitive-Flexibility\nin this work) aims to test to what degree LLMs\ncan rapidly switch tasks within a single con-\ntext window. To this end, it employs two\nneuropsychological tests, the Wisconsin Card\nSorting Test (WCST) (Grant and Berg, 1948)\nand the Letter-Number Test (LNT) (Rogers\nand Monsell, 1993) commonly used to mea-\nsure cognitive flexibility in humans.\nLogical Reasoning LogiQA 2.0 (Liu et al., 2023)\nThis dataset evaluates logical reasoning us-\ning the same data both in NLI and Machine\nReading Comprehension format (text, ques-\ntion, multiple-choice) for each of the follow-\ning (deductive) reasoning types: categorical,\nsufficient condition, necessary condition, dis-\njunctive, conjunctive reasoning.\nCausal Reasoning CLADDER (Jin et al., 2023) fo-\ncus es on formal causal reasoning (causal in-\nference), as opposed to commonsense causal\nreasoning. The dataset is constructed from\nformal logic-based templates that are then ver-\nbalised into natural language as binary ques-\ntions.\nCommonsense Reasoning WinoGrande\n(Sak-\naguchi et al., 2021) A large-scale dataset of\n44k commonsense reasoning problems con-\nsisting of pairs of nearly identical questions\nwith two answer choices (as in the original\nWinograd Schema Challenge (Morgenstern\nand Ortiz, 2015) but without its bias).\nPlanning NATURAL PLAN (Zheng et al., 2024)\nis a realistic planning benchmark consisting\nof three tasks expressed in natural language:\nTrip Planning, Meeting Planning and Calen-\ndar Scheduling. Models are given a situation\nand a problem to solve (e.g. find a trip plan\nthat satisfies some constraints given the situa-\ntion described.) Each task contains problems\nof different levels of complexity based on the\nnumber of cities, people or days involved. The\nproblems are often based on numerical rea-\nsoning too. We evaluate models on the Trip\nPlanning and the Calendar Scheduling tasks.\nEmotional Intelligence EQ-Bench (Paech, 2023)\nthe model is given an emotionally charged\nshort dialogue (generated by GPT-4) and has\nto score the four possible emotions felt by a\ngiven character. Scores are compared against\na reference score.\nPragmatics (Hu et al., 2023) (referred as LM-\nPragmatics in this work) is a benchmark evalu-\nating LLMs’ understanding of seven pragmat-\nics phenomena: deceit, indirect speech, irony,\nmaxims, metaphor, humour, and coherence.\nScenarios are grounded into social situations,\nrequiring LLMs to interpret utterances. The\ntask is designed as a multi-choice question-\nnaire with 2-5 questions based on the subtask.\nSocial Commonsense SOCIAL IQA (Sap et al.,\n2019) a dataset for evaluating social common-\nsense reasoning and emotional intelligence.\nEach sample includes a short scenario and\nthree multiple-choice questions across six cat-\negories: intentions, reactions, descriptions,\nmotivations, needs, and consequences. Trans-\nfer learning on this dataset has shown strong\nperformance on other commonsense reason-\ning benchmarks.\nAttribution and Judgment\/Theory of Mind\nSimpleToM(Gu et al., 2025) contains concise,\ndiverse stories each with questions that ask\nmodels to predict behavior (\"Will Mary\n10\npay for the chips or report the mold?\"),\njudgment (\"Mary paid for the chips. Was\nthat reasonable?\") or mental states (\"Is Mary\nlikely to be aware that ’The can of Pringles\nhas moldy chips in it.’? Yes or No?\") The first\ntwo subtasks have been taken as a reference\nfor the Attribution and Judgment cognitive\nability, while the last as a reference for Theory\nof Mind.\nE\nBenchmark Implementations\nFor the majority of the static benchmarks evaluated\nin this work we relied on the popular framework\nfor the evaluation of LLMs lm-eval4 (ver. 0.4.7),\nwhich already made available many of the selected\nbenchmarks, and enabled a common interface for\nthe implementation of most of the remaining ones.\nThe benchmarks which were already present\nwithin the framework are: SOCIAL IQA (Sap et al.,\n2019), WinoGrande (Sakaguchi et al., 2021), EQ-\nBench (Paech, 2023), LogiQA 2.0 (Liu et al., 2023),\nMMLU (Hendrycks et al., 2021). The benchmarks\nwhich have been implemented in the framework\nover the course of the study are: CLADDER (Jin\net al., 2023), LM-Pragmatics (Hu et al., 2023), Sim-\npleToM (Gu et al., 2025), NATURAL PLAN (Zheng\net al., 2024).\nAs for BBH and IFEval, they were also available\nin the framework and potentially usable, however\nwe decided to rely on the scores made available by\nHuggingface in their Open Leaderboard 2 5 as they\nalso rely on the lm-eval framework for evaluation.\nThis has been possible for all of the models ex-\ncept one (OLMo-2-1124-13B-Instruct), for which\nthe scores were not available in the leaderboard at\nthe time of this study. In this case, we have used\nthe code made available by Huggingface and fol-\nlowed their instruction for reproducing the results\n(in particular, given the instruction-tuned nature of\nthe model, the settings apply_chat_template and\nfewshot_as_multiturn were applied.\nAs for the interactive games, we have used\nthe implementation provided by version 1.5 of\nthe clembench (Chalamalasetti et al., 2023). The\nremaining benchmarks (WM, LLM-Cognitive-\nFlexibility) have been implemented outside of the\nframework, as lm-eval did not provide support for\nthe multi-turn nature of the tasks.\n4https:\/\/github.com\/EleutherAI\/\nlm-evaluation-harness\n5https:\/\/huggingface.co\/spaces\/\nopen-llm-leaderboard\/blog\nE.1\nZero-shot and Few-shot Tasks\nThe majority of the tasks have been evaluated in a\nzero-shot setting with the exception of MMLU (5-\nshot), BBH (3-shot) (following common practices\nin model evaluation, e.g. in the Open Leaderboard\n2 fo BBH) and NATURAL PLAN (5-shot). In the\ncase of NATURAL PLAN, our models performed re-\nally poorly when evaluated in a zero-shot fashion—\nresulting in scores close to 0. Given that the task\nrelies on the models producing answers in a strict\nformat for parsing, we opted for using the 5-shot\nversion provided by the benchmark’s authors.\nE.2\nMetrics\nE.2.1\nEvaluation\nIn the evaluation of models, we followed the origi-\nnal works’ implementations as well as associated\nmetrics. However, it may be the case that for a cer-\ntain benchmark more metrics were defined, or that\nthe original work did not aggregate results across\nsubtasks. For this reason, we report here the met-\nrics we used for evaluating models.\nIn the case of Clembench games, we computed\nperformance by computing the ratio between the\nquality score (a number from 0 to 100) and the\npercentage of played games (a number between 0\nand 1) divided by 100.\nIn the case of IFEval, following what was done\nin the Open Leaderboard 2, we averaged the re-\nsults obtained on prompt-level and instruction-level\nstrict accuracy.\nAs for EQ-Bench, we computed the task-specific\nscore as it was implemented in the lm-eval.\nRegarding WM, we only considered the subtask\nVerbal N-3, and we computed the accuracy for the\nresults obtained across the 50 trials defined in the\noriginal work.\nIn the case of LLM-Cognitive-Flexibility, we ran\neach subtask 8 times with 25 trials each, and com-\nputed the average of the accuracy obtained in each\nrun. In this case, the accuracy was computed only\non the trials for which response parsing was suc-\ncessful. We then averaged the accuracy obtained\non both subtasks to compute the final score.\nIn the case of CLADDER, we followed the origi-\nnal work which treated the task as generative and\nprobed for the presence of the substrings \"yes\"\/\"no\"\nat the beginning of the model’s answer.\nIn NATURAL PLAN, the original work defined\na rule-based procedure to parse specific data from\nthe generated plan (e.g., dates). We reuse their\n11\nparsing procedure and verify whether the expected\nelements are all present in the parsed plan.\nFor the remaining tasks (LogiQA 2.0, Wino-\nGrande, LM-Pragmatics, SOCIAL IQA, MMLU,\nBBH, SimpleToM), we treated them as a multiple-\nchoice question answering task that is evaluated\nbased on the likelihood of the correct answer for\nthe task.\nIn the case of BBH, the Open Leaderboard\n2’s evaluation code excludes three of the original\ntasks from the overall score’s computing: dyck\nlanguages, navigate and word sorting. The perfor-\nmances on these subtasks are therefore also ignored\nin the performance reported in this study.\nIn the case multiple subtasks were present\n(LM-Pragmatics, MMLU, BBH, NATURAL PLAN,\nLLM-Cognitive-Flexibility), we computed the\nmicro-average over the results achieved on each\nsubtask. In the specific case of SimpleToM, since\nthe subtasks were associated with two different\nCognitive Abilities, we’ve aggregated the score of\nthe subtasks behaviour and judgment into a single\nscore (under Attribution and Judgment), and con-\nsidered the mental state subtask separately (under\nTheory of Mind).\nE.2.2\nCorrelation\nFor measuring the pair-wise correlation between\nbenchmarks, we’ve computed the Kendall rank cor-\nrelation coefficient (or Kendall’s Tau) (Tau-b ver-\nsion). It measures rank correlation according to\nthis formula:\nτb =\nP −Q\np\n(P + Q + Tx)(P + Q + Ty)\nwhere:\nP = number of concordant pairs,\nQ = number of discordant pairs,\nTx = tie correction for variable X,\nTy = tie correction for variable Y.\nThis method was preferable over the others given\nits robustness in case of few data points, as it was\nin our case. We have also experimented with the\nPearson correlation coefficient and observed that\nin the majority of the cases, the correlation pat-\nterns were similar, however with larger positive as\nwell as negative correlations compared to Kendall.\nWe’ve relied on the implementation provided by\nthe pingouin Python package (Vallat, 2018).\nE.3\nGeneration Settings\nThe tasks which required the models to gener-\nate text are: EQ-Bench, WM, MMLU, IFEval,\nthe clembench games, LLM-Cognitive-Flexibility,\nNATURAL PLAN, CLADDER.\nWith the excep-\ntion of Working Memory and LLM-Cognitive-\nFlexibility, all tasks have been evaluated by ap-\nplying a temperature of 0. Following the original\nimplementation, we have applied a temperature of\n1 to WM and 0.7 to LLM-Cognitive-Flexibility. In\nthese cases, however, the increased randomness\ncaused by the higher temperature was mitigated by\naveraging the results obtained over multiple trials.\nAs for the other generation settings, we also\nhave followed what was prescribed in the original\nworks regarding the tokens for the termination of\nthe generation, the maximum or minimum number\nof tokens. In the case of NATURAL PLAN, the\noriginal work did not provide specific information\nregarding the settings they have adopted for the\nevaluation. Given the highly challenging nature of\nthe task, we have set the minimum and maximum\nnumber of tokens to 90 and 350 respectively. This\nwas derived based on the minimum and maximum\nnumber of tokens in the gold plans.\nF\nLimitations in the Evaluations\nIn certain cases, results have not been computed on\nall the subtasks available for that benchmark. In ad-\ndition to BBH, we also made special arrangements\nfor Working Memory and NATURAL PLAN. In the\ncase of NATURAL PLAN, we have not considered\nresults coming from the meeting subtask, while for\nWM we have only considered those coming from\nthe Verbal (Base) N-3 subtask. In the first case, the\nhigh amount of resources required for evaluating\nthe task, especially for the larger models (60% of\nthe prompts contained above 14k space-separated\nwords (up to 38.3k) vs 100% below 15.1k for the\ntravel subtask and below 7.09k for the calendar\nsubtask prevented us from doing so. As for the\nsecond, we’ve only considered the base version of\nthe verbal subtask and excluded its variations as\nthey would not provide meaningful information for\nthis study.\nG\nModel Implementations\nAll the models used in this study have been made\navailable by Huggingface, and have been accessed\nthrough their transformers (Wolf et al., 2020) li-\nbrary. For text generation, we have been applying\n12\nthe default chat template specified within the same\nlibrary.\nH\nComputational Resources\nAs a reference, the time required for running\nthrough all the benchmarks for the Llama-3.1-8B-\nInstruct model on 1 A100 GPU with batch size set\nto ’auto’ in the lm-eval (i.e. it automatically fits\ninto the memory the maximum batch size possible\nfor each task). For the Clembench games, LLM-\nCognitive-Flexibility and WM, the batch size is 1.\nThe time also includes time required for procedures\nperformed by the lm-eval prior to the actual evalua-\ntion (only for those datasets evaluated through the\nframework) and loading the model into the memory\n(all tasks).\nLLM-Cognitive-Flexibility: ~1:50 min\nLogiQA 2.0: ~5 min\nCLADDER: ~19:30 min\nWinoGrande: ~1 min\nNATURAL PLAN: ~4:50 hours\nWM ~2:40 min\nEQ-Bench: ~3 min\nLM-Pragmatics: ~6:30 min\nSOCIAL IQA: ~1:30 min\nSimpleToM: ~2:40 min\nMMLU: ~14 min\nTaboo: ~3:30 min\nReference Game: ~3:00 min\nImage Game: ~2.40 min\nWordle: ~7:50 min\nWordle (Critic): ~2:50 min\nWordle (Clue): ~2:15 min\nPrivate\/Shared: ~17:30 min\nI\nSupplementary Plots\nFigure 4 and Figure 5 show the supplementary plots\nfor the results in Section 3 (comparing models of\ndifferent size but same family and models of similar\nsize respectively). Moreover, we report the supple-\nmentary plots for the results in Section 4. Figure 6\npresents a direct comparison of models based on\nour selected cognitive tests. Figure 7 reports an\nextended version of Figure 3. Finally, Figure 8\nreports by means of example two scatter plots rep-\nresenting respectively situations of high and low\ncorrelation between two benchmarks (a game and\na cognitive ability).\n13\nFigure 4: Comparing datasets in their power to discriminate models across size.\n14\nFigure 5: Comparing datasets in their power to discriminate models with similar size across families.\n15\nFigure 6: Cognitive Abilities Spectrum of LLMs\nFigure 7: Correlation of cognitive abilities with Large\nQA benchmarks and Interactive Games. The correlation\nmatrix does not include results on Wordle and Image\nGame as model performances’ were too low.\nFigure 8: High (left) and low (right) correlation\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests.pdf"}
{"title":"Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents","authors":"Yingxuan Yang, Bo Huang, Siyuan Qi, Chao Feng, Haoyi Hu, Yuxuan Zhu, Jinbo Hu, Haoran Zhao, Ziyi He, Xiao Liu, Zongyu Wang, Lin Qiu, Xuezhi Cao, Xunliang Cai, Yong Yu, Weinan Zhang","summary":"Large Language Model (LLM) agents frameworks often employ modular\narchitectures, incorporating components such as planning, reasoning, action\nexecution, and reflection to tackle complex tasks. However, quantifying the\ncontribution of each module to overall system performance remains a significant\nchallenge, impeding optimization and interpretability. To address this, we\nintroduce CapaBench (Capability-level Assessment Benchmark), an evaluation\nframework grounded in cooperative game theory's Shapley Value, which\nsystematically measures the marginal impact of individual modules and their\ninteractions within an agent's architecture. By replacing default modules with\ntest variants across all possible combinations, CapaBench provides a principle\nmethod for attributing performance contributions. Key contributions include:\n(1) We are the first to propose a Shapley Value-based methodology for\nquantifying the contributions of capabilities in LLM agents; (2) Modules with\nhigh Shapley Values consistently lead to predictable performance gains when\ncombined, enabling targeted optimization; and (3) We build a multi-round\ndataset of over 1,500 entries spanning diverse domains and practical task\nscenarios, enabling comprehensive evaluation of agent capabilities. CapaBench\nbridges the gap between component-level evaluation and holistic system\nassessment, providing actionable insights for optimizing modular LLM agents and\nadvancing their deployment in complex, real-world scenarios.","url":"http:\/\/arxiv.org\/abs\/2502.00510v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2502.00510v2","published":1738433254000,"comment":null,"pdf_text":"WHO’S THE MVP?\nA GAME-THEORETIC EVALUATION BENCHMARK FOR\nMODULAR ATTRIBUTION IN LLM AGENTS\nYingxuan Yang1, Bo Huang1, Siyuan Qi1, Chao Feng1, Haoyi Hu1, Yuxuan Zhu2, Jinbo Hu1, Haoran Zhao1,\nZiyi He3, Xiao Liu4, Zongyu Wang4, Lin Qiu4, Xuezhi Cao4, Xunliang Cai4, Yong Yu1, Weinan Zhang1\n1Shanghai Jiao Tong University 2University of Chicago 3University of Toronto 4Meituan\n{zoeyyx, wnzhang}@sjtu.edu.cn\nABSTRACT\nLarge Language Model (LLM) agents frameworks often employ modular architectures, incorporating\ncomponents such as planning, reasoning, action execution, and reflection to tackle complex tasks.\nHowever, quantifying the contribution of each module to overall system performance remains a\nsignificant challenge, impeding optimization and interpretability. To address this, we introduce\nCapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in\ncooperative game theory’s Shapley Value, which systematically measures the marginal impact of\nindividual modules and their interactions within an agent’s architecture. By replacing default modules\nwith test variants across all possible combinations, CapaBench provides a principle method for\nattributing performance contributions. Key contributions include: (1) We are the first to propose a\nShapley Value-based methodology for quantifying the contributions of capabilities in LLM agents;\n(2) Modules with high Shapley Values consistently lead to predictable performance gains when\ncombined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500\nentries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation\nof agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic\nsystem assessment, providing actionable insights for optimizing modular LLM agents and advancing\ntheir deployment in complex, real-world scenarios.\nKeywords LLM Agent · Evaluation Benchmark · Shapley Value · Capability\nIntroduction\nThe rapid advancements in Large Language Models (LLMs) have ushered in a transformative era for artificial intelligence\nagents. These models demonstrate unprecedented capabilities in understanding, generating, and integrating natural\nlanguage across diverse domains [Brown et al., 2020, OpenAI et al., 2024]. However, LLMs still face notable challenges\nas foundational models for supporting AI agents in real-world applications. These include accurately interpreting subtle\ncontextual shifts, effectively integrating with external tools, and ensuring both the accuracy and reliability of outputs.\nTo overcome these challenges, researchers have increasingly adopted modular architectures, decomposing agents into\ndistinct components responsible for planning, reasoning, and action execution. Such modular frameworks not only\nenhance the overall performance but also improve the interpretability and maintainability of the systems. Frameworks\nsuch as ReAct [Yao et al., 2022] and AutoGPT [Tang et al., 2023] exemplify how structured workflows, achieved by\nbreaking down tasks into manageable modules, can lead to more efficient task processing. These modular architectures\nlay the groundwork for systematic evaluations of LLM agents’ internal designs and effectiveness in various applications.\nDespite the impressive capabilities of LLM agents, accurately evaluating their performance remains an open challenge.\nTraditional evaluation methods have predominantly focused on task-specific benchmarks and domain-specific datasets.\nFor instance, AgentBench [Liu et al., 2023] assesses agents’ abilities through specialized tasks, while ToolBench\n[Guo et al., 2024a] evaluates the effectiveness of LLM agents in leveraging external tools across diverse application\narXiv:2502.00510v2  [cs.AI]  16 Feb 2025\nCapaBench\nTECHNICAL REPORT\nFigure 1: Conceptual Mapping between Coalition Game Theory and LLM Agent Evaluation. The left panel illustrates the mapping\nfrom coalition game theory to LLM agents, the right lists all possible module combinations (24 = 16) with their performance values.\nscenarios. Additionally, MMAU [Yin et al., 2024] investigates the capabilities of LLM Agents across a wide range\nof tasks. However, these benchmarks often rely on reductive assumptions, equating task success (e.g., solving a\nmath problem) with broader cognitive abilities (e.g., reasoning). This simplification neglects the complex interactions\nbetween an agent’s internal components, leading to an incomplete understanding of their true potential. The current task-\noriented evaluation framework faces several key challenges. First, LLM agents simultaneously require the integration\nof multiple capabilities to solve complex tasks. For example, solving a mathematical problem may necessitate\nreading comprehension, tool usage, and structured output generation. Second, existing methods fail to account for the\ninteractions between architectural components and their collective contributions to overall system behavior. Additionally,\ntask-specific success rates provide limited insight into the relative contributions of individual modules, making it difficult\nto identify key areas for optimization. Consequently, there is a pressing need for evaluation frameworks that can dissect\nand quantify the contributions of each module within modular LLM agents.\nTo address these challenges, we propose a novel evaluation framework, CapaBench, which integrates the assessment of\nmodular architectures with the evaluation of agent capabilities. CapaBench systematically quantifies the contributions\nof individual modules (e.g., planning, reasoning, action execution, reflection) within LLM architectures using the\nShapley Value [Hart, 1989], a cooperative game theory metric that fairly attributes performance based on all possible\npermutations of module contributions. This approach captures direct contributions and interaction effects at the\nsame time, offering a rigorous and interpretable evaluation of system dynamics. Our method provides several key\nadvantages: (1) evaluating the contributions of each module by capturing nuanced dynamics; (2) using a mathematically\nsound attribution method to enhance interpretability of agent performance; and (3) enabling predictions about system\nperformance based on specific module combinations, supporting targeted optimizations. To the best of our knowledge,\nCapaBench is the first framework to systematically quantify and attribute module contributions in LLM-based agents\nusing the Shapley Value approach.\nFurthermore, to ensure that our evaluation reflects realistic, multi-faceted application scenarios, we build a large-scale\ndataset of over 1,500 multi-round tasks spanning a diverse range of categories (e.g., shopping, navigation planning,\nticket ordering, operation system, robot control, math, and theorem proving). These tasks integrate various capabilities\nsuch as planning, tool usage, and reflection, thereby requiring holistic agent performance rather than isolated skill\nassessments. Our dataset will be open-sourced in the future to support further research and development, and we are\nactively adding more scenarios to broaden its coverage and applicability.\nOverall, CapaBench makes the following contributions:\n• Novel Evaluation Framework: We propose a rigorous methodology based on the Shapley Value to systemati-\ncally quantify the contributions of capabilities within LLM agents—which is the first work to adopt such an\napproach for evaluating LLM agents.\n• Predictive Module Combinations: Through comprehensive experiments, we show that modules attaining\nhigher Shapley Values consistently enhance task success when combined. These findings guide developers in\npinpointing and integrating high-value modules for performance gains.\n• Large-Scale Dataset: We build a multi-round dataset with over 1,500 entries spanning diverse domains\nsuch as daily activities, computation, and role control. The dataset is designed to challenge multiple agent\ncapabilities simultaneously, serving as a robust testbed for evaluating LLM agents. Our dataset will be released\nin the future to facilitate further research and development.\n2\nCapaBench\nTECHNICAL REPORT\nRelated Work\n2.1\nLLM Agent\nRecent advances in large language models (LLMs) have catalyzed the development of increasingly sophisticated AI\nagents. LLM agents typically employ modular architectures that decompose tasks into planning, reasoning, and action\nexecution. Early work, such as ReAct [Yao et al., 2022], highlighted the efficacy of explicit reasoning and action\nparadigms. Recent efforts, such as AutoGPT [Tang et al., 2023] pioneered autonomous task execution through iterative\nplanning and reflection. HuggingGPT [Shen et al., 2023] demonstrated advanced tool integration by orchestrating\nmultiple specialized models, while MetaGPT [Hong et al., 2024], introduced hierarchical planning strategies that enable\ndynamic task decomposition and recursive self-improvement. In addition, TRAD [Zhou et al., 2024] further advances\nthe paradigm by introducing thought-level retrieval and aligned decision-making to improve modular efficiency and\nreduce noise. These developments signify a shift from simple instruction-following to complex decision-making.\nBuilding on these works which highlight modular designs, our study systematically evaluates the marginal impact of\nindividual modules using the Shapley Value, uncovering the most suitable combinations of LLM modules for achieving\noptimal performance in different environments.\n2.2\nAgent Benchmark\nThe evaluation of LLM agents has evolved considerably, with early approaches primarily emphasizing task-specific\nperformance metrics. AgentBench [Liu et al., 2023] laid the groundwork by evaluating agents across diverse scenarios,\nsuch as web browsing and knowledge graph, highlighting the importance of assessing performance in diverse contexts.\nHowever, these evaluations often focused on task outcomes while overlooking the foundational skills driving these\nresults, making it difficult to analyze the root causes of failures. To address this limitation, MMAU [Yin et al., 2024]\nintroduced a novel benchmark that provides an evaluation of agent capabilities. But by combining capabilities with\npredefined tasks, MMAU risks equating task success with true capability strength, relying on limited problems that may\nnot generalize or capture complex real-world interactions.\nRecent benchmark developments have become increasingly sophisticated. OmniACT [Zhang et al., 2024] introduced\na comprehensive framework for evaluating agents in desktop environments, while AgentQuest [Yang et al., 2024a]\ndeveloped methods for assessing continuous learning and adaptation. These frameworks represent a shift toward\nunderstanding not just what agents can do, but how they handle complex, dynamic scenarios.\nBuilding on this trend, specialized benchmarks have emerged to target domain-specific skills. For example, Char-\nacterEval [Chen et al., 2024] assesses agents’ ability to maintain consistent personas, while WorkBench [Liu et al.,\n2024] focuses on workplace scenarios. ToolBench [Guo et al., 2024a] evaluates tool manipulation proficiency, and\nMobile-Bench [Wang et al., 2024] tests performance across mobile platforms. These frameworks reflect the growing\nrecognition that agent evaluation must encompass both general capabilities and domain-specific competencies.\nIn contrast, CapaBench extends beyond task-level evaluations by leveraging the Shapley Value to quantitatively capture\nboth individual module contributions and interaction effects, enabling a more nuanced analysis of how each component\ninfluences overall agent performance.\nBenchmark Design\nWe introduce the agent framework shown in Figure 2 as the foundation of our benchmark. This framework is specifically\ndesigned to assess LLM agents’ abilities in various environments and task scenarios. It follows established agent\nprocesses and features a modular design, which supports both single-turn and multi-turn interactions. This ensures that\nour evaluations are comprehensive and adaptable.\n3.1\nAgent Capability\nBuilding upon established agent architectures [Yao et al., 2022, Tang et al., 2023, Hong et al., 2024], our framework\nintegrates four fundamental capabilities essential for LLM agents: Planning, Reasoning, Action, and Reflection, as\nillustrated in Figure 2. These capabilities represent the core functionalities widely recognized in current agent systems,\nenabling agents to handle immediate completions and perform complex tasks.\nPlanning module initiates the agent workflow by decomposing complex instructions into structured subtasks, following\nprinciples established in hierarchical planning systems [Brown et al., 2020]. This decomposition enables effective task\nprioritization and resource allocation, particularly crucial for multi-step operations requiring strategic foresight.\n3\nCapaBench\nTECHNICAL REPORT\nFigure 2: Agent Workflow in CapaBench.\nReasoning module extends the ReAct framework [Yao et al., 2022] by incorporating both instruction context and\nenvironmental observations. Through chain-of-thought mechanisms [Wei et al., 2022], this module performs logical\ninference and causal analysis to determine appropriate action sequences. Integration with the planning module enables\ndynamic adjustment of reasoning strategies based on evolving task requirements.\nAction module implements the execution interface, translating cognitive processes into concrete operations. This\napproach builds on established action space formalization [Guo et al., 2024a], ensuring consistent mapping between\ninternal state representations and external behaviors. The module maintains state awareness through continuous\nenvironment monitoring, enabling responsive behavior adaptation.\nReflection module completes the architecture by implementing systematic performance analysis, drawing from recent\nadvances in self-improving systems [Yin et al., 2024]. Operating primarily in multi-turn scenarios, this module enables\niterative refinement of agent behavior through structured outcome analysis and strategy adjustment.\n3.2\nEvaluation Methodology\nTo evaluate the contribution of individual capability modules within LLM agent architectures, we leverage Shapley Value\n[Hart, 1989] analysis, a principled framework grounded in cooperative game theory. This methodology quantifies the\nmarginal impact of each module on system performance by systematically evaluating all possible module configurations.\nBy capturing both independent contributions and interaction effects among modules, this approach provides a robust\nmechanism for evaluating the modular design of LLM systems, while naturally handling the nonlinear dynamics\ninherent in such architectures.\nShapley Value Framework\nShapley Value provides a theoretical foundation for fairly allocating the overall perfor-\nmance of a system to its individual components. For a set of N modules, Shapley Value ϕi(v) for module i is defined\nas:\nϕi(v) =\nX\nS⊆N\\{i}\n|S|!(|N| −|S| −1)!\n|N|!\n[v(S ∪{i}) −v(S)],\n(1)\nwhere S denotes any subset of N that excludes module i, and v(S) represents the performance(task success rate) of the\nagent when only the modules in S are active. The term v(S ∪{i}) −v(S) quantifies the marginal impact of adding\nmodule i to the subset S, while the weight |S|!(|N|−|S|−1)!\n|N|!\nensures fair averaging across all possible subsets.\nEvaluation Flow\nCapaBench systematically evaluates the contributions of four key modules in the agent architecture:\nPlanning (P), Reasoning (R), Action (A), and Reflection (F). As shown in Figure 1, the evaluation involves testing\nall possible combinations of these modules (24 = 16 combinations) by replacing default implementations with test\nvariants provided by the target LLM model. The default \"whiteboard\" modules, implemented using Llama3-8b-instruct,\nserve as a fixed baseline to isolate the performance impact of each test module. Llama3-8b-instruct was chosen as\nthe default model implementation because it is open-source, lightweight, and easy to deploy, making it practical for\nextensive testing. While it possesses basic task completion capabilities, its moderate success rates provide an ideal\nbaseline to observe and quantify the impact of replacing modules with more advanced test models.\nFor each combination, CapaBench computes performance values to quantify the contribution of individual modules and\ntheir interactions. Diverse task benchmarks (B), including multi-step scenarios designed to simulate practical agent\napplications, are used to evaluate the system, providing insights into the optimal module configurations for various\nenvironments.\n4\nCapaBench\nTECHNICAL REPORT\nAlgorithm 1 CapaBench Evaluation Framework\n1: Input: Default model, Test model, Benchmarks B\n2: Output: Shapley Value ϕi(v) for each test module i\n3: Fix all modules to their default implementations: {Pd, Rd, Ad, Fd}\n4: for all subset S ⊆{Pt, Rt, At, Ft} do\n5:\nReplace default modules in S with test modules\n6:\nEvaluate task success rate v(S) using benchmarks B\n7: end for\n8: for all test module i ∈{Pt, Rt, At, Ft} do\n9:\nCompute Shapley Value ϕi(v)\n10: end for\n11: return ϕi(v) for all test modules i\nCapturing Synergistic Effects and Nonlinear Dynamics\nShapley Value provides a robust framework to quantify both\nthe independent contributions and synergistic interactions among modules in a modular architecture. By systematically\nevaluating all possible subsets S ⊆N, it inherently captures the nonlinear dynamics and interdependencies between\nmodules. For instance, Planning provides structured outputs for Reasoning, while Reasoning refines these outputs\nto guide Action execution. Tasks often require at least two modules to collaborate, such as Reasoning and Action\nworking together to decompose and solve complex tasks. These collaborative effects are reflected in the marginal\ncontributions v(S ∪{i}) −v(S), where v(S) represents the system’s performance (e.g., task success rate) with subset\nS. Shapley Value is particularly well-suited for nonlinear dynamics, as it fairly distributes contributions even when\nmodule interactions exhibit synergy or competition. Unlike linear or additive methods, it ensures unbiased attribution\nof both individual and collaborative contributions, making it ideal for evaluating modular LLM agents with complex\ninterdependencies.\n3.3\nDataset Construction\nOnline Shopping\nOnline Shopping tasks are based on the simulated online shopping platform WebShop [Yao et al.,\n2023]. The dataset includes 110 tasks, of which we modified 48 tasks to enhance the diversity and complexity of the\ninstructions. For example, the original instruction “find me scrubs & body treatments made with tea tree and other\nnatural ingredients” is rewritten as “Given my upcoming spa weekend, I’m on the lookout for scrubs & body treatments.\nCan you recommend ones specifically made with tea tree and other natural ingredients as I have sensitive skin?” These\nmodified prompts reflect more natural and contextually rich user queries, challenging the agent to demonstrate reasoning,\npersonalization, and relevance in its recommendations. The reward model and product definitions align with WebShop,\nproviding a consistent evaluation framework for agents’ performance in online shopping scenarios.\nNavigation Planning\nThe Navigation Planning task evaluates agents’ ability to collaboratively generate travel\nitineraries with a user while adapting to evolving constraints and preferences, inspired by [Lin et al., 2024]. This\ndataset’s 250 tasks are designed to reflect a wide range of planning challenges. In our setup, the user provides an initial\nset of three travel requirements sampled from a pool of potential preferences, such as budget limits, preferred activities,\nor group constraints.\nTo simulate real-world planning scenarios where user preferences may evolve, the evaluation process introduces\ndynamic updates. In each iteration, there is a 50% chance that a new preference is sampled from the predefined pool.\nThis new preference will be added to the current instruction set, leading to updated instructions. If no new preference is\nintroduced (also with 50% probability), the agent’s current proposal is evaluated directly.\nThe evaluation consists of two components: the first part is based on the precision derived from the experimental results,\nand the second part evaluates the rationality of the planned route, based on how well the proposal aligns with user\npreferences, considering factors such as budget adherence, inclusion of specified activities, and efficient travel distances.\nThis feedback measures the agent’s ability to prioritize user needs and adaptively produce actionable travel plans.\nTicket Ordering\nThe Ticket Ordering task, inspired by [Lin et al., 2024], evaluates an agent’s ability to determine the\noptimal flight combination based on user-specified constraints. This dataset comprises 150 tasks designed to simulate\neveryday ticket ordering scenarios. In our setup, two users provide their daily calendars along with requirements such\nas the flight price.\nTo mirror real-world ticket ordering, users can choose from a wide array of flights—each differing in price, duration,\narrival time, and more—which makes it challenging for agents to offer sound advice.\n5\nCapaBench\nTECHNICAL REPORT\nTable 1: Capability Coverage Across Dataset Categories. Each row corresponds to a core capability in our modular framework\n(planning, reasoning, action, reflection), and each column represents a task in our dataset.\nDaily Activities\nComputation\nRole Control\nShopping Navigation Ticket Math ATP OS\nRobot\nPlanning\nTask Steps\n✓\n✓\n✓\nResource Constraints\n✓\n✓\n✓\n✓\nReasoning\nLogical Validation\n✓\n✓\n✓\nKnowledge Inference\n✓\n✓\n✓\n✓\nAction\nEnvironmental Actions\n✓\n✓\n✓\nInteractive Actions\n✓\n✓\n✓\n✓\nReflection Failure Analysis\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nThe evaluation involves three components: the flight price, the significance of calendar conflicts with flight schedules,\nand the difference in arrival times between the two users. Lower prices, fewer calendar conflicts, and smaller differences\nin arrival times indicate a more favorable flight combination as determined by the agents.\nMath Solver\nThe Math Solver task evaluates the ability of agents to solve diverse mathematical problems by\nintegrating the usage of tools into the problem solving process. This task spans two categories: Algebra and Geometry.\nThe problems in these categories were generated based on Math [Hendrycks et al., 2021] with the assistance of GPT-4,\nresulting in a newly created set of problems.\nTo address the challenges posed by Math’s lack of detailed classification of points of knowledge and difficulty, we\norganized tasks into five distinct points of knowledge and 10 levels of difficulty. Each combination of knowledge\npoint and difficulty level contains 5 unique problems, resulting in a total of 250 problems. These were systematically\ndistributed to ensure balanced coverage across all knowledge points and difficulty levels.\nTo support agents in solving these problems, we introduced two tools:\n• A pseudo ‘search engine’ containing 200 curated knowledge points for Algebra and Geometry. This search\nengine allows agents to retrieve the top three most relevant knowledge points by using a BERT model based\non query similarity.\n• A calculator provided to LLM agents for performing numerical computations.\nThese tools enable agents to simulate human-like problem solving by integrating both retrieval-based and computational\ncapabilities.\nAutomatic Theorem Proving\nThe automatic theorem-proving(ATP) aspect of the task evaluates the ability of agents\nto construct formal proofs for logical problems. The MINIF2f [Zheng et al., 2021] dataset stands out in ATP, featuring a\nseries of complex Olympiad-level mathematical problems. However, a subset of this data set is oriented to Lean 3, and\ncurrently Lean 3 has been upgraded to Lean 4 and is no longer in use. In addition, Coq is also a popular formal proof\nlanguage, but MINIF2F is not involved. More importantly, an important feature of formal proof is that humans can\ninteract with the compiler’s information to complete the proof. However, the previous benchmark only tested whether\nthe response given by LLM could complete the proof in one step.\nTo address theorem-proving challenges, agents use formal verification tools specific to Coq, Lean4 and Isabelle3 [The\nCoq Development Team, The Lean Prover Team, The Isabelle Team]. These tools require agents to work within formal\nsyntax constraints, iteratively constructing proofs step by step. The problem solving process involves dynamically\nadjusting their strategies based on the current proof state, simulating human-like reasoning in formal logic. By engaging\nwith these tools and frameworks, agents are required to navigate the complexities of theorem proving, demonstrating\nthe ability to reason rigorously and adaptively in formal systems.\nOperation System\nThe Operation System dataset evaluates an agent’s ability to interact with a simulated OS terminal\nby executing commands for both Ubuntu and git tasks. For Ubuntu tasks, we utilized the AgentBench-OS framework\n[Liu et al., 2023] and expanded the dataset with GPT-4, covering key areas such as file system manipulation, system\nsetting and process running. During evaluation, agents propose bash commands to be executed in Ubuntu terminal and\nget the feedback from the terminal to complete the given task. The reflection module is designed as when last command\n6\nCapaBench\nTECHNICAL REPORT\nTable 2: Number of Data Entries per Dataset\nCategory\nShopping\nNavigation\nTicket\nMath Solver\nAutomatic Theorem Proving\nRobot\nOS\nSubcategory\nBlack\nWhite\nNone\nNone\nAlgebra\nGeometry\nCoq\nLean4\nIsabelle\nNone\nNone\nCount\n48\n62\n250\n150\n250\n250\n111\n111\n111\n100\n102\nfailed (use (echo $?) to get the execute success result of command), prompting agents to reflect on the error to improve\nfuture interactions.\nFor git tasks, we adopted data from Learn Git Branching [The learnGitBranching Team], which provides a sandbox\nenvironment that dynamically updates the git tree based on input terminal commands. The task form is given target git\ntree information and init git tree information, agents are required to propose git command to transform init git tree into\ntarget git tree state. The reflection module is designed as if no changes occur in the git tree after two interaction steps,\nagents are prompted to reflect on their previous commands to enhance their reasoning processes.\nRobot Cooperation\nThe Robot Cooperation task is based on scenarios from RoCo [Mandi et al., 2023], designed to\nevaluate LLM agents in diverse real-world-inspired robotic environments. We adopted and reformed five core tasks\nfrom the original benchmark: Sweep Floor, Move Rope, Arrange Cabinet, Make Sandwich, and Sort Cubes. Each task\nwas expanded with specific instances to ensure diversity and precision in evaluation.\nTo further challenge and assess the agents’ capabilities, we enhanced these tasks by incorporating additional constraints.\nFor instance, the Sweep Floor task was refined by requiring the agent to sweep cubes in a specific sequence (e.g., first\nred, then blue, and finally green), thereby assessing the agent’s ability to plan with order sensitivity. Similarly, the\nArrange Cabinet task now requires the agent to first remove a cup or mug and place it on a designated coaster before\nhandling other items, emphasizing the importance of sequential logic.\nBuilding on these enhanced tasks, we adopted the Central Plan mode from RoCo, wherein an oracle LLM-planner is\nprovided with complete environmental observations, comprehensive information on all robots’ capabilities, and uniform\nplan feedback. This setup prompts the LLM to devise actions for all robots simultaneously. To further enhance this\napproach, we modified it to allow the agent to plan multiple action steps within a single interaction. Unlike the original\nsingle-step-single-action approach, this modification reduces the number of required interactions, enabling the agent to\ncreate more comprehensive and integrated action plans.\nEvaluation\n4.1\nExperimental Implementation\nIn our experiments, we establish Llama3-8B-Instruct as the default implementation for all four core modules: planning,\nreasoning, action, and reflection. For each evaluation, we systematically replace the default implementation of one\nmodule with its test variant(driven by the test model), while keeping other modules in their default state. This systematic\nreplacement generates 24 = 16 distinct configurations for the four-module architecture. For each configuration S,\nwe measure the task success rate v(S) across a range of benchmark scenarios to ensure robust and representative\nperformance data.\nWe evaluate nine large language models, which are categorized into three groups:\n• Closed API Models: This includes four widely used commercial API-based models: Anthropic\/Claude-3.5-\nSonnet, OpenAI\/GPT-4-turbo-0409, OpenAI\/GPT-4o-mini, GLM-4-air, and Doubao-pro-4k.\n• Mid-parameter Open-Source Models (32B-100B): To assess mid-scale architectures, we evaluate three\nmodels: Llama3.1-70B-Instruct and Mixtral-8x7B-Instruct-v0.1 (46.7B).\n• Low-parameter Open-Source Models (≤32B): For lightweight implementations, we include Qwen2.5-32B-\nInstruct and Mistral-8B-Instruct-v0.2.\nThe selected models span a broad parameter range, including both open-source and closed-source architectures, enabling\na comprehensive comparison of their performance and adaptability within our benchmark framework. All experiments\nare conducted on NVIDIA A100-80GB GPUs, with vLLM employed for efficient inference of open-source models.\n7\nCapaBench\nTECHNICAL REPORT\nTable 3: Experimental Results Across Datasets. Metrics for baseline models are highlighted in blue. The evaluation covers nine\nmodels across five primary tasks, showcasing notable performance variations and unique module contributions. Results marked with\n‘*‘ below each dataset indicate the best-performing model combinations computed based on Shapley Value.\nDataset\nMetric\nLlama3\n8B\nClaude\n3.5\ngpt-4o\nmini\nglm-4\nair\nqwen2.5\n32B\nMistral\n8X7B\nMistral\n7B\ngpt-4\nturbo\ndoubao\npro-4k\nLlama3\n70B\nOnline\nShopping\nAcc: 43.31*\nPt\n–\n-0.004\n0.071\n0.106\n-0.030\n-0.048\n0.024\n0.026\n0.071\n-0.028\nRt\n–\n0.019\n-0.025\n0.077\n0.004\n0.036\n0.016\n-0.074\n0.011\n0.005\nAt\n–\n0.056\n0.068\n-0.059\n0.156\n0.080\n0.004\n0.014\n-0.045\n0.117\nFt\n–\n-0.009\n-0.003\n-0.011\n-0.021\n-0.015\n-0.022\n0.024\n-0.040\n-0.030\nAcc (%)\n26.27\n32.43\n37.43\n37.50\n37.18\n31.67\n28.48\n25.31\n25.95\n32.61\n∆Acc (%)\n–\n+6.16\n+11.16\n+11.23\n+10.91\n+5.40\n+2.21\n-0.96\n-0.32\n+6.34\nNavigation\nPlanning\nAcc: 74.42*\nPt\n–\n0.000\n0.006\n0.001\n-0.002\n0.021\n0.023\n0.008\n0.001\n-0.009\nRt\n–\n0.030\n0.027\n-0.008\n0.012\n-0.035\n0.055\n0.014\n-0.003\n-0.019\nAt\n–\n0.106\n0.081\n0.005\n0.099\n0.048\n0.042\n0.099\n-0.051\n0.046\nFt\n–\n-0.006\n0.002\n-0.021\n0.018\n-0.029\n0.007\n0.004\n-0.033\n-0.011\nAcc (%)\n58.70\n71.90\n70.29\n61.91\n68.26\n64.45\n71.48\n71.23\n50.90\n59.32\n∆Acc (%)\n–\n+13.20\n+11.59\n+3.21\n+9.56\n+5.75\n+12.78\n+12.53\n-7.8\n+0.62\nTicket\nOrdering\nAcc: 67.18*\nPt\n–\n0.003\n0.032\n-0.195\n0.119\n0.183\n-0.111\n-0.043\n0.151\n0.004\nRt\n–\n0.186\n0.243\n0.172\n0.181\n0.054\n-0.070\n0.301\n-0.001\n0.089\nAt\n–\n0.217\n0.049\n-0.020\n-0.000\n-0.083\n-0.020\n0.028\n0.006\n-0.275\nFt\n–\n0.024\n0.005\n-0.006\n0.043\n-0.011\n0.002\n0.058\n-0.027\n-0.001\nAcc (%)\n19.94\n62.85\n51.82\n15.01\n54.25\n34.24\n0.00\n54.37\n32.88\n1.59\n∆Acc (%)\n–\n+42.91\n+31.88\n-4.93\n+34.31\n+14.30\n-19.94\n+34.43\n+12.94\n-18.35\nMath\nAcc:83.80*\nPt\n–\n0.038\n0.067\n0.056\n0.065\n0.005\n-0.060\n0.048\n0.115\n0.028\nRt\n–\n0.131\n0.021\n0.044\n0.107\n0.003\n-0.000\n0.065\n0.059\n0.031\nAt\n–\n0.442\n0.343\n0.348\n0.483\n0.164\n-0.044\n0.492\n0.182\n0.327\nFt\n–\n0.042\n0.043\n0.005\n0.031\n-0.014\n-0.003\n0.022\n-0.002\n0.006\nAcc (%)\n18.00\n83.40\n65.40\n63.20\n86.60\n33.80\n7.20\n80.60\n53.40\n57.20\n∆Acc (%)\n–\n65.40\n47.40\n45.20\n68.60\n15.80\n-10.80\n62.60\n35.40\n39.20\nATP\nAcc: 86.79*\nPt\n–\n0.012\n0.018\n0.002\n0.018\n0.025\n0.008\n0.012\n0.016\n0.019\nRt\n–\n0.057\n-0.016\n0.005\n0.030\n0.018\n0.010\n0.027\n0.019\n-0.056\nAt\n–\n0.660\n0.345\n0.161\n0.511\n0.039\n-0.009\n0.541\n0.084\n0.125\nFt\n–\n0.069\n0.015\n0.021\n0.037\n-0.011\n-0.000\n0.023\n0.004\n0.011\nAcc (%)\n5.45\n85.29\n41.74\n24.32\n65.17\n12.61\n6.31\n65.77\n17.72\n15.32\n∆Acc (%)\n–\n79.84\n36.29\n18.87\n59.72\n7.16\n0.86\n60.32\n12.27\n9.874\nRobot\nCooperation\nRwd: 92.63*\nPt\n–\n0.114\n0.075\n-0.024\n0.090\n-0.005\n-0.014\n0.107\n0.021\n0.043\nRt\n–\n0.388\n0.189\n0.116\n0.268\n0.033\n-0.000\n0.329\n-0.004\n0.152\nAt\n–\n0.319\n0.196\n0.008\n0.277\n0.052\n-0.021\n0.316\n0.204\n0.175\nFt\n–\n0.017\n-0.003\n-0.012\n0.003\n0.004\n-0.001\n0.001\n-0.012\n-0.008\nReward (%)\n8.85\n92.63\n54.43\n17.60\n72.59\n17.27\n5.17\n84.18\n29.75\n45.06\n∆Reward (%)\n–\n+83.78\n+45.58\n+8.75\n+63.74\n+8.42\n-3.68\n+75.33\n+20.90\n+36.21\nOperating\nSystem\nAcc: 60.78*\nPt\n–\n0.078\n0.042\n0.047\n0.060\n0.032\n0.004\n0.050\n0.065\n0.077\nRt\n–\n0.458\n0.305\n0.305\n0.311\n0.194\n0.047\n0.395\n0.215\n0.313\nAt\n–\n0.071\n0.065\n0.041\n0.053\n0.009\n0.019\n0.070\n0.060\n0.040\nFt\n–\n-0.008\n0.020\n0.004\n0.037\n0.001\n0.019\n0.005\n-0.006\n0.012\nAcc (%)\n0.98\n60.78\n44.12\n40.71\n47.06\n24.51\n9.80\n52.94\n34.31\n45.10\n∆Acc (%)\n–\n+59.80\n+43.14\n+39.73\n+46.08\n+23.53\n+8.82\n+51.96\n+33.33\n+44.12\n4.2\nMain Results\nWe conducted a systematic evaluation of nine different models across five primary tasks, revealing significant perfor-\nmance disparities and distinct module contribution patterns. The following sections provide a detailed analysis of key\nfindings in each task domain, supplemented by comprehensive insights derived from the experimental results presented\nin Table 3. Results for the sub-datasets under the MATH solver and ATP can be found in the appendix.\nOnline Shopping Performance\nIn the e-commerce evaluation, model performance exhibited clear hierarchical differ-\nentiation. High-performance models, specifically GLM-4-air (37.50%) and GPT-4o-mini (37.43%), significantly\noutperformed the baseline model (Llama3-8B: 26.27%). This improvement is primarily attributed to effective module\nsynergy and optimized action execution. GLM-4-air demonstrated superior performance in the Planning (P: 0.1058)\nand Reasoning (R: 0.0770) modules, underscoring the importance of advanced cognitive abilities in managing complex\nshopping tasks. Additionally, Qwen2.5’s notable performance in the Action module (A: 0.1557) highlights the critical\nrole of precise action selection in enhancing task success rates. The reflection capabilities of GPT-4o-turbo (F:\n0.0244) further emphasize the significance of dynamic strategy adjustments in interactive scenarios.\nMath Solver Performance\nThe mathematical problem-solving evaluation encompassed both algebra and geometry\nsub-tasks, revealing distinct performance characteristics. In algebra, Qwen2.5 achieved an impressive accuracy\nof 86.8%, marking a 65.2 percentage point improvement over the baseline. This performance is largely due to\n8\nCapaBench\nTECHNICAL REPORT\n(P,R,A,F)\n(R,A,F)\n(P,R,A)\n(R,A)\n(P,A,F)\n(P,A)\n(A,F)\n(A)\n(P,R)\n(P,R,F)\n(R,F)\n(R)\n(F)\n(\n)\n(P,F)\n(P)\n0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\n84.4%\n80.0%\n78.0%\n69.6%\n64.8%\n63.2%\n62.0%\n58.8%\n42.0%\n41.6%\n38.8%\n38.0%\n21.6%\n21.6%\n21.2%\n18.4%\nFigure 3: Shapley value results of all combinations in Math (Algebra) for Claude-3.5-Sonnet under different model configurations.\nThe pattern of the bars indicates the number of modules (ranging from 0 to 4) that Claude is involved in.\nits robust Planning (P: 0.059) and Action (A: 0.436) modules, which facilitate effective strategy formulation and\nexecution. Similarly, Claude-3.5 excelled in the Reasoning module (R: 0.177), highlighting its capacity for complex\nmathematical derivations. In geometry, Qwen2.5 maintained a leading accuracy of 86.4%, supported by balanced\ncontributions across Planning (P: 0.071), Reasoning (R: 0.067), and Action (A: 0.530) modules. This balance indicates\nthe necessity of multi-dimensional capabilities in solving geometric problems. The consistently high Shapley values for\nthe Action module across models further emphasize the importance of precise step execution in this domain.\nAutomatic Theorem Proving Performance\nThe Automatic Theorem Proving task evaluates models’ abilities to\nreason and execute formal proofs in Coq, Lean4, and Isabelle. Table 3 highlights that Claude-3.5 achieves the best\nperformance across all three systems, with significant ∆Accuracy improvements (+90.0%, +82.0%, and +67.6%),\ndriven by its strong Action (At) contributions. qwen2.5 also performs well, particularly in Isabelle, with competitive\nReasoning (Rt) and Action (At) scores. The results emphasize the importance of precise execution (Action) and logical\ninference (Reasoning) for success in theorem proving, while Reflection (Ft) plays a limited role. This outcome may\nstem from the highly structured nature of theorem proving, which rewards models capable of following strict formal\nrules and applying precise, sequential reasoning without extensive trial-and-error.\nOperation System Performance\nThe Operation System task highlights the critical role of reasoning ability, as much\nof the necessary task information is acquired through ongoing interactions rather than being fully available initially.\nThis explains the higher Shapley Values for Reasoning (R: up to 0.4578) compared to Planning (P: up to 0.0777).\nAdditionally, the benchmark places relatively low demands on action ability due to the close alignment of benchmark\ncommands with real-world formats, reducing the complexity of action execution. Reflection (F) contributes minimally,\nas the task lacks strong feedback signals for iterative improvement. Claude-3.5 achieved the best performance\n(60.78% accuracy), emphasizing the importance of reasoning in dynamic OS environments.\nRobot Cooperation Performance\nRobot cooperation tasks best demonstrated the models’ comprehensive capabilities.\nClaude-3.5 led with a reward score of 92.63% and achieved the highest Reasoning module Shapley value (R:\n0.3879) across all tasks. This result highlights the central role of reasoning abilities in multi-agent collaboration.\nAdditionally, all modules exhibited relatively high contributions (P: 0.1140, A: 0.3186, F: 0.0172), confirming that\ncomplex cooperative scenarios necessitate balanced development across all functional areas. The Shapley Values, based\non marginal contribution averages, consistently align with model performance across tasks, demonstrating their stability\nand reliability. High-performing models, such as Claude-3.5 and Qwen2.5, exhibit strong Shapley Values in key\nmodules (e.g., Action for theorem proving, Reasoning for math solving), which correspond to their high task success\nrates. Conversely, weaker models like Mistral-7B show uniformly low or negative Shapley Values, reflecting their\npoor performance. Moreover, the Shapley Values adapt to task-specific demands, emphasizing Planning and Reasoning\nin Shopping and Math tasks, while prioritizing Action in theorem proving. This consistency validates Shapley Value as\na robust framework for assessing modular contributions in diverse tasks.\nModule Impact via Replacement\nThe experimental results in Figure 3 confirm that module replacement accurately\nreflects its impact on system performance, as demonstrated by Claude-3.5-Sonnet on Algebra. High-contribution\nmodule configurations, identified through Shapley Value calculations, achieve significantly better performance. For\ninstance, the configuration (P,R,A) achieves a success rate of 78.0%, far surpassing the baseline configuration with\nLlama3-8b-Instruct at 21.6%. Incremental module replacements align with theoretical predictions: only substituting the\ndefault Planning module with the test Planning module improves performance to 18.4% (P), while further integrating\na strong Action module (A) boosts it to 63.2% (P,A). Synergistic effects are particularly evident in configurations\nlike (P,R,A), which leverage robust Planning and Action capabilities to achieve peak success rates. In contrast,\nconfigurations with low-contribution modules result in diminished performance, as seen in (P,F), which achieves\nonly 0.212. These results highlight the predictive power of Shapley Values in quantifying module contributions and\nconfirm the alignment of task outcomes with theoretical expectations, reinforcing the validity of the framework.\n9\nCapaBench\nTECHNICAL REPORT\nPredictive Module Combinations\nThe experimental results in Table 2 demonstrate that modules with higher Shapley\nValues consistently lead to improved task performance when combined. For instance, in the \"Online Shopping\" dataset,\nthe optimal combination achieves an accuracy of 43.31%, which is significantly higher compared to the other models,\nindicating the advantage of leveraging high-contribution modules. Similarly, in ATP, the best combination computed\nbased on Shapley Values results in an 86.79% accuracy, showcasing a marked improvement over alternatives. These\nresults demonstrate that identifying and integrating key modules with high Shapley Values enables CapaBench to\nsystematically maximize performance across tasks, validating Shapley Values as a reliable guide for module selection\nand optimization.\n4.3\nAblation Study\nIn this section, we examine how changing the default model in our evaluation framework affects the Shapley\nValue results and the relative ranking of various LLMs.\nSpecifically, we replace our original default model\n(Llama3-8B-instruct) with the model (gpt-3.5-turbo-0613) and re-run the evaluation on the same set of\nseven test LLMs over the Robot Cooperation Task. Our aim is to examine (i) whether our evaluation framework is\nrobust against different baseline capabilities, and (ii) to what extent the relative ranking of the test models is affected by\nthis change.\n(a) Planning\n(b) Reasoning\n(c) Action\n(d) Reflection\nFigure 4: Comparation of Shapley Value under different default models.\nFigure 4 illustrates the Shapley Value results for the four modules under 2 default models. Although the absolute\nShapley Values vary due to the differences in baseline model capabilities, our primary focus is on the consistency of test\nmodel rankings.\nTo quantify this consistency, we define the preference pair consistency rate as\nPairwise Consistency Rate = {Consistent Preference Pairs}\n{All Model Pairs}\n, which measures the proportion of test model pairs that maintain the same relative ranking across both experiments. A\nhigher rate indicates that changes to the default model have minimal impact on the relative ranking of test models.\nThe Results show that Reasoning achieves the highest consistency rate (91.67%), followed by Action (86.11%), Planning\n(72.22%), and Reflection (58.33%). The high overall consistency (85.18%) confirms that our evaluation framework is\nrobust against changing the default model for most modules. Notably, Reasoning and Action, which contribute most to\ntask success according to Shapley Values, also exhibit the highest ranking consistency. By contrast, Reflection shows\nthe lowest consistency (58.33%), suggesting that its assessment may be more sensitive to the default model choice or\nthat the reflection module requires further refinement. Overall, while absolute Shapley Values naturally shift under a\nstronger or weaker default model, the relative ordering of test models—and thus the key insights into each model’s\nstrengths and weaknesses—remains largely stable.\n4.4\nAnalysis\nBased on Table 3, we further enrich our analysis with the following insights:\nCross-Task Model Performance Comparison\nA high-level comparison of model performance across diverse tasks\nreveals distinct strengths and weaknesses. Notably, Claude-3.5 outperforms other models in most categories,\nshowing particular prowess in formal verification (e.g., Coq, Lean 4, Isabelle) and robot cooperation tasks. This\nadvantage suggests that Claude-3.5 has a robust underlying chain-of-thought reasoning mechanism and effective\nmulti-agent collaboration strategies—capabilities essential for tasks that demand precise logical proof structures and\nsynchronized actions. On the other hand, open-source models like Qwen-2.5 and Mistral-8X7B exhibit moderate\ngains in more straightforward domains, such as shopping or basic Algebra, but underperform in cognitive-heavy tasks.\nTheir lag in automatic theorem proving and robot cooperation implies that while these models may be adept at handling\n10\nCapaBench\nTECHNICAL REPORT\nFigure 5: Radar plot comparing model performance across tasks with key contributions.\nroutine queries and procedural problem-solving, they lack the deeper reasoning, advanced planning, or specialized\nmodules needed for high-stakes coordination and rigorous proof validation. Strengthening these areas—possibly\nthrough fine-tuning on specialized corpora or integrating more advanced tool usage—could help bridge the gap between\nopen-source and proprietary models in complex, multi-stage tasks.\nModule Contribution Patterns\nOur findings highlight that module contributions vary according to task demands,\nreflecting the distinct cognitive processes involved. Specifically:\n• Tasks with High Cognitive Complexity (e.g., Online Shopping, Robot Cooperation, and OS): Reasoning\nand Planning play pivotal roles. Online shopping requires balancing constraints (e.g., budget and preferences)\nand sequencing decisions effectively. In robot cooperation, Reasoning enables dynamic information updates\nand efficient task distribution among agents. Operation system tasks, involving troubleshooting and resource\nmanagement, rely heavily on real-time problem-solving and feedback interpretation. Across these tasks, robust\nReasoning ensures logical inference and decision-making under uncertainty.\n• Tasks Requiring Precision (e.g., Math Solvers and ATP): Action is the dominant module. In math solvers,\nparticularly geometry, precise procedural execution, such as applying theorems or constructing diagrams,\noutweighs strategic planning. Similarly, in formal verification tasks (e.g., Coq or Lean), strict adherence to\nsyntactic and semantic correctness is critical. Both scenarios demand meticulous step-by-step actions to ensure\nreliability and prevent errors.\nBy identifying module-specific dependencies, developers can target optimizations, such as enhancing Reasoning for\ndynamic decision-making or refining Action for procedural accuracy, to maximize performance across diverse domains.\nLow Reflection Contribution\nWe conclude the seemingly low contribution of the Reflection module to overall task\nperformance through two main considerations. First, whether or not the reflection directly translates into a higher\nsuccess rate does not necessarily reflect the true quality or efficacy of the reflection itself. In other words, task success\nalone may not be the best measure of how well the model is “thinking about” its own mistakes. Second, when the model\nreflects on its own errors without extra information or guidance from a more capable model, it may fail to pinpoint the\nactual causes behind its mistakes. As a result, the lack of deeper insights into error sources means reflection often does\nnot generate meaningful improvements in task outcomes. Consequently, while the Reflection module is present, its\npractical impact on success rates remains limited.\nComparative Study\nThis experiment investigates whether Shapley Values can accurately capture model-specific\nabilities in core competencies, including planning, reasoning, and action. To this end, we conducted a capability\nevaluation experiment on a subset of 238 questions from successful trajectories in the Algebra dataset, focusing on\ncorrectly completed tasks. Using successful trajectories ensures reliable annotations for Planning, Reasoning, and\nAction modules by providing clear labels. From these trajectories, we extracted full interaction data and split it into\nsingle-step QA samples based on the three core modules. This process generated 2180 single-step samples. The\nreflection module was excluded due to its minimal impact on overall success rates and the insufficient number of\nsuccessful trajectories required to build a reliable dataset for this dimension. For each single-step sample, we asked\nthe tested models to provide responses, which were then evaluated by GPT-o1-mini as an independent evaluator. The\nevaluation focused on two aspects for the Planning and Reasoning modules: semantic rationality, assessing whether the\nresponse is clear and comprehensible, and task completion degree, measuring whether the agent effectively completed\n11\nCapaBench\nTECHNICAL REPORT\nFigure 6: Planning, Reasoning, and Action Evaluation on Algebra. Each color represents an ability. The left Y-axis shows the\nShapley value with solid lines and the right Y-axis shows the GPT scores with dashed lines.\nthe task. For the Action module, the evaluation centered on logical comprehension ability, which reflects the model’s\nunderstanding of task logic and its ability to execute correct actions based on Planning and Reasoning.\nFigure 6 shows the Shapley Values and the scores given by GPT-o1-mini for each model, with Pearson correlation\ncoefficients of 0.81, 0.77, 0.67 for the Planning, Reasoning, and Action modules, respectively. These high correlations\nvalidate the effectiveness of Shapley Values in quantifying each module’s specific contribution to task success.\nFurthermore, our method addresses critical limitations of ground truth-dependent evaluation approaches. Traditional\nmethods rely on predefined ground truth, which is vulnerable to changes in task prompts or adjustments to the ground\ntruth itself, leading to potential penalization of reasonable outputs due to reduced similarity. Additionally, traditional\nevaluations often ignore the diversity of valid responses and fail to capture interactions between modules, such as the\ninterplay between planning and reasoning in guiding actions. In contrast, the Shapley-based framework holistically\nevaluates each module’s marginal contributions and their interactions, offering a robust and flexible approach for\nmodular analysis.\nConclusion and Future Works\nThis paper introduced CapaBench, a game-theoretic framework that employs the Shapley Value to rigorously evaluate\nthe contributions of individual modules in LLM agents. By calculating effects among planning, reasoning, action,\nand reflection components, CapaBench enables more precise attribution, guiding targeted optimization and offering\npredictive insights into performance across diverse tasks. Moreover, our approach can potentially extend to LLM-based\nMulti-Agent Systems [Guo et al., 2024b, Yang et al., 2024b, Sun et al., 2024], where each module operates as a\nspecialized sub-agent, paving the way for future explorations in agent coordination, communication, and emergent\nbehaviors. Moving forward, we aim to expand the variety of tasks in CapaBench to improve the robustness and\ntransferability of our evaluation. Additionally, we plan to explore refined, domain-specific evaluation protocols that\nreduce computational overhead without compromising module-level insights. Ultimately, by incorporating these\nenhancements and investigating multi-agent paradigms, we hope to advance both the theoretical underpinnings and\npractical applications of modular LLM-based AI systems.\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, et al. Language models are few-shot learners. Advances in Neural\nInformation Processing Systems, 33:1877–1901, 2020.\nJosh Achi OpenAI, Steven Adler, Sandhini Agarwal, et al. Gpt-4 technical report, 2024. URL https:\/\/arxiv.\norg\/abs\/2303.08774.\nShinnung Yao et al. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nTao Tang, Zhihui Li, Jiangjie Chen, Mingyu Lin, and Wei Zhang. Autogpt: An autonomous gpt-4 experiment. arXiv\npreprint arXiv:2308.08155, 2023.\nXiao Liu, Hao Zhou, Zhiheng Zhang, Dian Peng, et al. Agentbench: Evaluating llms as agents. arXiv preprint\narXiv:2308.03688, 2023.\nZhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu.\nStabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models, 2024a.\nGuoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong,\nAonan Zhang, Dian Ang Yap, Yizhe zhang, Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh,\n12\nCapaBench\nTECHNICAL REPORT\nTobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, and\nZirui Wang. Mmau: A holistic benchmark of agent capabilities across diverse domains, 2024. URL https:\n\/\/arxiv.org\/abs\/2407.18961.\nSergiu Hart. Shapley Value, pages 210–216. Palgrave Macmillan UK, London, 1989. ISBN 978-1-349-20181-5.\ndoi:10.1007\/978-1-349-20181-5_25. URL https:\/\/doi.org\/10.1007\/978-1-349-20181-5_25.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks\nwith chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\nSirui Hong, Xiawu Wang, Mingyu Yang, Jiale Guo, Di Chen, and Bingchen Li. Metagpt: Meta programming for\nmulti-agent collaborative framework. arXiv preprint arXiv:2401.03066, 2024.\nRuiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, and\nWeinan Zhang. Trad: Enhancing llm agents with step-wise thought retrieval and aligned decision, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2403.06221.\nWei Zhang, Junnan Wu, Tianhao Wang, Zhihao Hu, et al. Omniact: A dataset and benchmark for enabling multi-modal\ntask completion in large language models. arXiv preprint arXiv:2402.00858, 2024.\nYifei Yang, Haoqiang Wu, Chen Zhao, Mingzhe Liu, et al. Agentquest: A multi-phase task planning and execution\nbenchmark for autonomous agents. arXiv preprint arXiv:2402.01786, 2024a.\nXiaoyan Chen, Wei Li, Yicheng Zhang, and Shaoliang Wang. Charactereval: A comprehensive benchmark for llm\nrole-playing consistency. arXiv preprint arXiv:2402.04126, 2024.\nMingyu Liu, Jiaxin Chen, Wei Zhang, and Yue Wang. Workbench: Evaluating language models in real-world\nprofessional scenarios. arXiv preprint arXiv:2402.05937, 2024.\nJinyang Wang, Zhiyu Li, Xuanhe Chen, and Ming Zhang. Mobile-bench: Can llms serve as universal mobile app\nagents? arXiv preprint arXiv:2401.12726, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\nChain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction\nwith grounded language agents, 2023. URL https:\/\/arxiv.org\/abs\/2207.01206.\nJessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. Decision-oriented dialogue for human-ai collaboration.\nTransactions of the Association for Computational Linguistics, 2024.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level\nmathematics. arXiv preprint arXiv:2109.00110, 2021.\nThe Coq Development Team. The coq proof assistant. https:\/\/coq.inria.fr\/.\nThe Lean Prover Team. The lean theorem prover. https:\/\/leanprover.github.io\/.\nThe Isabelle Team. The isabelle theorem prover. https:\/\/isabelle.in.tum.de\/.\nThe learnGitBranching Team. Learn git branching. https:\/\/learngitbranching.js.org\/\/.\nZhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models,\n2023. URL https:\/\/arxiv.org\/abs\/2307.04738.\nTaicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, and Xiangliang Zhang.\nLarge language model based multi-agents: A survey of progress and challenges. In International Joint Conference\non Artificial Intelligence, 2024b. URL https:\/\/api.semanticscholar.org\/CorpusID:267412980.\nYingxuan Yang, Qiuying Peng, Jun Wang, and Weinan Zhang. Llm-based multi-agent systems: Techniques and business\nperspectives. 2024b. URL https:\/\/api.semanticscholar.org\/CorpusID:274165614.\nChuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based multi-agent reinforcement learning: Current and\nfuture directions. ArXiv, abs\/2405.11106, 2024. URL https:\/\/api.semanticscholar.org\/CorpusID:\n269921354.\nGeorge Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and\nSwarat Chaudhuri. Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition.\narXiv preprint arXiv:2407.11214, 2024.\n13\nCapaBench\nTECHNICAL REPORT\nAPPENDIX\nOnline Shopping.\nA.1\nDataset Deatils\nThe Online Shopping dataset is designed to evaluate agents’ planning, reasoning, and action capabilities in completing\ne-commerce tasks. The dataset consists of 110 tasks, divided into two parts: white-box tasks (62), which are from\nthe Webshop dataset, and black-box tasks (48), which are expanded using GPT-4 to enhance instruction diversity and\ncomplexity.\nDataset expansion was constructed by modifying instructions from the original dataset. GPT-4 was used to rephrase\ninstructions for greater linguistic diversity, adding context or background such as “Next week is Halloween, and I need\nthemed decorations.” Additionally, parameters were enriched with attributes like size, color, or material to increase task\ncomplexity. For challenging cases, explicit prompts were created to guide planning, for example, “First search for\ndesks with wood finishes, then filter by size and price.”\nA typical instruction in Online Shopping might be: “I’m looking for a small portable folding desk that is already fully\nassembled; it should have a khaki wood finish, and price lower than 140 dollars, and length bigger than 40 inches.”\nAgents are evaluated based on their ability to follow optimal trajectories, such as:\n• Ideal Trajectory 1: Search for all attributes directly (\"desk, wood, folding, khaki, 40 inches, $140\") and\nproceed to the target item.\n• Ideal Trajectory 2: Broad search (\"desk, wood, folding\"), filter by price, and then refine attributes (color,\nsize).\nA.2\nExperiment Deatils\nTable 4 summarizes the experimental results for the Online Shopping task, including Shapley values for the four\nmodules (Planning (Pt), Reasoning (Rt), Action (At), and Reflection (Ft)), as well as task success rates (Accuracy\n(%)) and their improvement (∆Accuracy (%)) relative to the baseline (Llama3-8B-instruct).\nTable 4: Experimental Results on Online Shopping(110 pieces)\nLLM\nPt\nRt\nAt\nFt\nAcc (%)\n∆Acc (%)\nLlama3-8B-instruct (Default)\n-\n-\n-\n-\n26.27\n-\nclaude_3.5_sonnet\n-0.0038\n0.0187\n0.0555\n-0.0088\n32.43%\n+6.16\ngpt-4o-mini\n0.0711\n-0.0251\n0.0684\n-0.0028\n37.43\n+11.16\nglm-4-air\n0.1058\n0.077\n-0.0591\n-0.0114\n37.50\n+11.23\ngpt-4-turbo-0409\n0.0255\n-0.0737\n0.0142\n0.0244\n25.31\n-0.96\nqwen2.5-32b-ins\n-0.0299\n0.0041\n0.1557\n-0.0209\n37.18\n+10.91\nMistral-7B-Instruct\n0.0243\n0.0155\n0.0043\n-0.0221\n28.48\n+2.21\nLlama-3-70B-Instruct\n-0.0279\n0.0045\n0.1167\n-0.03\n32.61\n+6.34\ndoubao-pro-4k\n0.0712\n0.0107\n-0.045\n-0.0402\n25.95\n-0.32\nMistral-8X7B-instruct\n-0.0476\n0.0364\n0.0797\n-0.0147\n31.67\n+5.40\nbest\n\/\n\/\n\/\n\/\n43.31\n+17.04\nThe baseline model (Llama3-8B-instruct) achieves a task success rate of 26.27%. The best-performing models,\nglm-4-air and gpt-4o-mini, achieve accuracies of 37.50% and 37.43%, corresponding to improvements of\n+11.23% and +11.16%, respectively. These results highlight their strong overall performance relative to the baseline.\nThe experimental results on the Online Shopping dataset reveal several notable characteristics of the evaluated\nmodels and their performance on this task. Notably, the dataset places a strong emphasis on Planning and Action\ncapabilities, as evidenced by the high Shapley values for these modules among the top-performing models (glm-4-air,\n14\nCapaBench\nTECHNICAL REPORT\nqwen2.5-32b-ins, and Llama-3-70B-Instruct). The task’s structured nature, requiring precise attribute\nfiltering and logical decision-making, heavily rewards models with strong planning abilities (e.g., high Pt values) and\neffective action execution (At).\nAdditionally, the relatively low contributions from the Reflection (Ft) module suggest that this task does not involve\nsignificant trial-and-error or iterative refinement, which limits the importance of reflective reasoning. The dataset\ntherefore primarily evaluates an agent’s ability to efficiently process structured instructions, identify relevant attributes,\nand execute a coherent sequence of actions to achieve success. These findings highlight the suitability of this dataset\nfor benchmarking models’ structured decision-making and planning abilities in e-commerce-like environments, while\npointing to areas where iterative reasoning may play a lesser role.\nA.3\nPrompt Example\nA.3.1\nPlanning Module\nprompt_system_planning = \"\"\"\nWelcome to the Online Shopping Challenge! Four LLM agents are working together to do\nweb-shopping tasks step by step (planning -> reasoning -> acting -> reflecting).\nThey are responsible for planning, reasoning, acting, and reflecting respectively.\nYou are the first llm agent, who is a helpful web-shopping guidance assistant in\ncharge of planning.\nYour role is to assist players by generating strategic plans based on the game’s\ninstructions.\nHere is how the game is structured:\n- Each round, you will be given an instruction that describes the objective need to\nachieve.\n- Based on the instruction, you are to generate a clear and brief strategic plan.\n- Your plan will be used to guide other agents through the shopping site efficiently.\n- If there is no response click[Buy Now] within 15 actions, the game fails.\nYour Responsibilities:\n- Analyze the original problem and break it into clear, actionable steps.\n- Ensure the steps are logically ordered and comprehensive for achieving the goal.\n- Use concise language, focusing only on the key actions needed to complete the task\nsuccessfully.\nOUTPUT FORMAT:\nKeep your response concise and structure:\nStrategic Plan: (A list of sequential steps to achieve the objective)\nStep 1: ...\nStep 2: ...\nStep 3: ...\n(Add more steps as necessary, but keep it streamlined and goal-oriented)\nEnclose the plan with three backticks ‘‘‘.\nFor example:\n\"\"\"\nA.3.2\nReasoning Module Prompt\nprompt_system_reasoning = \"\"\"\nWelcome to the Online Shopping Challenge!\nFour llm agents are working together to do web-shopping tasks step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the second LLM agent, who is a helpful web-shopping guidance assistant in\ncharge of reasoning.\nYour reasoning thought will guide the acting agent in making informed decisions. You\nshould generate a thought that will be used as part of the PROMPT for acting agents\n.\n15\nCapaBench\nTECHNICAL REPORT\nIn each round, following information will be given to you:\n1. CURRENT OBSERVATION AND AVAILABLE ACTIONS\n2. PLANNING STRATEGY\n3. HISTORICAL ACTIONS\n4. REFLECTION INFORMATION(if any)\nHere is what you need to focus on:\n- Every round, you will receive updated information about the shopping scenario,\nincluding the current observation, available actions, planning strategy, and past\nactions.\n- Based on the current state, develop a clear thought process to guide the acting\nagent’s next move.\n- Ensure your response is directly actionable and aligns with the goal of achieving\nsuccess in the game within 15 actions.\n- If the game is nearing the interaction limit, prioritize quick decisions over\nperfect matches to ensure a [Buy Now] action happens promptly.\n- When you determine that a sufficient match is found (even if not perfect), guide the\nacting agent to click [Buy Now] immediately.\nOUTPUT FORMAT:\nBased on the provided observation and available actions, generate a clear and brief\nthought in one sentence that outlines your analysis and considerations for the next\nmove.\nNote: Please surround the reasoning content you generated with three backticks. That\nis:\n\"\"\"\nA.3.3\nAction Module Prompt\nprompt_system_action = \"\"\"\nWelcome to the Online Shopping Challenge!\nFour llm agents are working together to do web-shopping tasks step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the third LLM agent, who is a helpful web-shopping guidance assistant in\ncharge of acting.\nAs an acting agent, your role is to integrate various elements such as the instruction\n, the current state, historical actions, strategic planning, and current reasoning\nto recommend the best possible action for the next step.\nIn each round, the following information will be given to you:\n1. ORIGINAL PROBLEM\n2. PLANNING STRATEGY\n3. HISTORICAL ACTIONS\n4. CURRENT REASONING\nYour Role:\n- Each round, you will receive updated information, including the current observation,\navailable actions, strategic plan, reasoning, and past actions.\n- Based on this information, decide and respond with the best possible action to move\ncloser to completing the objective.\n- Actions you can perform:\nSearch if a search bar is available.\nClick one of the provided clickable buttons.\n- Follow the reasoning closely, but only deviate if you are confident that your choice\nis better.\nImportant Rules:\n- You must click [Buy Now] as soon as you are confident that a suitable match has been\nfound to avoid exceeding the 15-round limit.\n- If no valid action is available, perform no action and wait for the next round.\n- Ensure the clicked value exactly matches the available options, including case\nsensitivity and punctuation.\n16\nCapaBench\nTECHNICAL REPORT\n- Attention: Although you need to click to buy as early as possible to get rewards,\nremember that you must click on a product before clicking to buy;\nif you click to buy without clicking on the product, you will\nreceive 0 rewards.\nOUTPUT FORMAT:\nUse the following formats for your action:\n- searching: search [keywords]\n- clicking: click [value]\n- For example: click [b06xdg8xfx]\n- Keywords in search is up to you, but value in click must be a value in the list of\navailable actions.\n- The value must exactly match the original text, including case sensitivity (\nuppercase\/lowercase) and all symbols\/punctuation.\nNote: Please surround the action content you generated with three backticks. That is:\n\"\"\"\nA.3.4\nReflection Module Prompt\nprompt_system_reflection = \"\"\"\nWelcome to the Online Shopping Challenge!\nFour llm agents are working together to do web-shopping tasks step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the fourth llm agent in charge of reflecting. Your role is to reflect on\nwhether there was an error in the previous reasoning and action sequence.\nRemember, your clear and brief reflection will be used as part of the PROMPT for the\nlater agents to guide them to make wise decisions and succeed in the game.\nIn each round, the following information will be given to you:\n1. ORIGINAL PROBLEM\n2. HISTORICAL REASONINGS\n3. HISTORICAL ACTIONS\nHere is your role:\nAs an LLM Agent, your role is to reflect on the recent outcomes and consider the\nfollowing points:\n1. Identify why the current result is unsatisfactory. Explore factors such as\ninadequate search queries, irrelevant clicks, or repeated useless actions.\n2. Evaluate the effectiveness of past actions and thoughts. Were there missed signals\nor incorrect assumptions?\n3. Propose improvements for the next steps. Suggest specific actions or adjustments in\nsearch strategies, clicking behaviors, or decision-making processes.\n4. Consider the overall goal of achieving successful purchases within the game’s\nconstraints. How can future actions better align with this objective?\nUse these as a guide, and generate a plan for the next reasoning and action steps.\nOutline actionable insights and strategies to improve outcomes in the upcoming\nrounds.\nOUTPUT FORMAT:\n- You should carefully examine reasoning history and action history to find out where\nthings may have gone wrong, summarize where they went wrong.\n- Your reflection output should provide clear and concise suggestions for the next few\nreasoning and action agents, facilitating informed decision-making and guiding the\nLLM agent towards achieving better performance in subsequent interactions.\n- Ideally, it should contain:\n- Flaw: One sentence that summarizes key factors causing the unsatisfactory\nresult.\n- Improvement: One sentence that includes specifically how to adjust improve\nreasoning and action steps to achieve better outcomes in the future.\nNote: Please enclose the flaw and improvement with three backticks:\n\"\"\"\n17\nCapaBench\nTECHNICAL REPORT\nNavigation Planning.\nB.1\nDataset Details\nFigure 7: Dynamic Navigation Planning Task Framework. The task evaluates the agent’s ability to collaboratively generate and adapt\ntravel itineraries based on evolving user constraints and preferences.\nThe Navigation Planning task evaluates agents’ ability to collaboratively generate travel itineraries with a user while\nadapting to evolving constraints and preferences, The dataset includes 250 tasks,designed to benchmark performance in\nnavigation planning\/\nIn navigation tasks, agents are required to collaboratively generate and adapt travel itineraries based on evolving user\nconstraints and preferences.Inspired by[Lin et al., 2024], we utilized the framework to employ the evaluation.It evaluates\nthe rationality of the planned route, based on how well the proposal aligns with user preferences.\nWe enhanced the automated data generation method from [Lin et al., 2024] to construct our new dataset.The dataset\nprovides a list of locations and situations, and by randomly generating the conditions of tourist destinations for each\ninstance, it facilitates the next step of decision-making, thereby enabling significant scalability.\nB.2\nExperiment Details\nTable 5: Experimental Results on Navigation Planning(250 tasks)\nLLM\nPt\nRt\nAt\nFt\nAcc (%)\n∆Acc (%)\nLlama3-8B-instruct(Default)\n-\n-\n-\n-\n58.70\n-\nclaude-3.5-sonnet\n0.0002\n0.0297\n0.1058\n-0.0056\n71.90\n+13.20\ngpt-4-turbo-0409\n0.0083\n0.0136\n0.0994\n0.004\n71.23\n+12.53\nqwen2.5-32b-Instruct\n-0.0022\n0.0124\n0.0985\n0.0182\n68.26\n+9.56\ngpt-4o-mini\n0.0056\n0.0273\n0.0810\n0.0017\n70.29\n+11.59\nLlama-3.1-70B-Instruct\n-0.009\n-0.019\n0.0455\n-0.0108\n59.32\n+11.79\ndoubao-pro-4k\n0.0008\n-0.0029\n-0.0508\n-0.0328\n50.90\n-7.8\nglm-4-air\n0.0011\n-0.0080\n0.0050\n-0.0214\n61.91\n+3.21\nMistral-8X7B-instruct\n0.021\n-0.035\n0.048\n-0.029\n64.45\n+5.75\nMistral-7B-Instruct\n0.0230\n0.0552\n0.0423\n0.0065\n71.48\n+12.78\nbest\n\/\n\/\n\/\n\/\n74.42\n+15.72\nB.3\nPrompt Example\nB.3.1\nPlanning Module\nprompt_system_planning = \"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\n. Your objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user’s preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will be forced to make your final proposal, which will be\nscored. You can also make proposals before the word count is up.\n18\nCapaBench\nTECHNICAL REPORT\nYou need to make a plan for the task.Based on the instructions provided, outline a\nstrategic travel plan that includes\n- Setting the Ultimate Goal and Identifying Key Factors for Achievement\n- Suggested actions for the traveler to take, such as specific search queries or\nattractions\/activities to focus on.\n- Split the requirement into 3 combinations and find combinations that can be achieved\nin one place.\n- Comprehensive Consideration and Selection of One Approach Among Multiple Outcomes\nYour output will as follows.You should answer in one paragraph.Here is your format:\n[think][planing]HERE IS YOUR PLAN.\nHERE IS AN EXAMPLE\nUser: [message] I’d like to see some live music, eat only takeout from Korean, kosher,\nJapanese or seafood restaurants. vegann options are a plus and I’d like to stop by\nMad Seoul. My budget is $30.I hope the minimal distance.\nYou: [think][planing]To create a travel plan that aligns with the user’s preferences\nwhich has a high score, the key objectives are to select destinations known for\nlive music, diverse takeout options (Korean, kosher, Japanese, seafood, and vegan).\nThe budget constraint of $30 will also guide the choices.I should search for these\nrequests,to see if there are places that meets the requirements.If I get the\nlocation correctly,I will give a proposal.If there is many choice,I will consider\nall proposals and give one proposal that is best.\n\"\"\"\nB.3.2\nReasoning Module Prompt\nprompt_system_reasoning = \"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\n. Your objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user’s preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will be forced to make your final proposal, which will be\nscored. You can also make proposals before the word count is up.\n[reasoning]Based on the current state of your travel plan and your information gained\nfrom previous action develop your thought process that leads to a specific\nrecommended action or to propose.If you have got many results, please take them\ninto consider.\nYou can only search for at most 3 times in one time.\nYour output will as follows. Here is your format:\n[think][reasoning]HRER IS YOUR ANALYSE.\nAfter you give a propose you need to prepare for sending a message.Sothat next action,\nyou can send a message.If a reflection is given,you need to reasoning again and\nmessage is not allowed.\n[think][reasoning]I will send a message to ask how does user think of it\nYou need to answer in one line. More than one line is not allowed.\nHere is an example:\nUser: [message] I’d like to see some live music, eat only takeout from Korean, kosher,\nJapanese or seafood restaurants. Vegan options are a plus and I’d like to stop by\nMad Seoul. My budget is $30.I hope the minimal distance.\nYou: [think][planing]To create a travel plan that aligns with the user’s preferences\nwhich has a high score, the key objectives are to select destinations known for\nlive music, diverse takeout options (Korean, kosher, Japanese, seafood, and vegan).\nThe budget constraint of $30 will also guide the choices.I should search for these\n19\nCapaBench\nTECHNICAL REPORT\nrequests,to see if there are places that meets the requirements.If I get the\nlocation correctly,I will give a proposal.If there is many choice,I will consider\nall proposals and give one proposal that is best.\n\/\/HERE IS YOUR OUTPUT\nYou: [think][reasoning]I will search for cities or neighborhoods that are known for\ntheir vibrant music scenes and diverse food options, and then narrow it down to\nthose that fit within the budget. I will consider multiple options and propose the\nbest itinerary based on the gathered information.\n\"\"\"\nB.3.3\nAction Module Prompt\nprompt_system_action = \"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\n. Your objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user’s preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will be forced to make your final proposal, which will be\nscored. You can also make proposals before the word count is up.\n[action]You can use the ‘Search‘ tool,or you can give a proposal or you can send a\nmessage.\nYou can’t not propose directly when there is no other action before.If you are told\nyou have searched too many times please propose at once.\n- propose\n[propose]Your need to give me a propose.Give me a proposal no more than 3 places.You\nneed to give me 3 places.\nYour output will as follows.Your propose can only based the information your searched.\nIf there is places that only satisfies some requests,it is acceptable.\nYou: [propose] [Mad Seoul, Lincoln Park, Caribbean Corner]\nOnly when you can’t find enough places,you can submit 1 or 2 places.Or you will be\npunished.\n[propose][A,B,C]\n- message\nAfter you have done a proposal,you can ask user if it is acceptable.You need to format\nlike this:\n[message]YOUR Message.\n- tool\nwith the following API:\nfield: can be name, category, price, info, or any other field of an site\ncategory: can be [restaurant, cafe, museum, bar, landmark, park, shop]\nSearch:\nParameters\n- fields: list of field names to return\n- filters: list of filters to intersect with AND. Can only filter one of the\nfields above.\n- text_query: freeform text query to search in event descriptions. Will be intersected\nwith filters with AND.\n- sort_by: list of fields or callable function to sort results by.\n- limit: number of results to return\nYou will get a reply begin with \"---searching---\".Your output will as follows.\n[tool]Search\nHere is an example:\nYou: [tool]Search(fields=[name, category, price], filters=[category == restaurant],\ntext_query=Korean kosher Japanese seafood live music vegan,sort_by=[price])\n\"\"\"\n20\nCapaBench\nTECHNICAL REPORT\nB.3.4\nReflection Module Prompt\nrompt_system_reflection=\"\"\"\nYou are a travel agent. Book a set of three destinations that make the user most happy\n. Your objective is to maximize the \"Final Score\" at the end of the chat, which\nscores how well the final itinerary you proposed matches the user’s preferences.\nYou and the user are limited to a fixed number of words in the chat. When the word\ncount is below 0 then you will be forced to make your final proposal, which will be\nscored. You can also make proposals before the word count is up.\nPlease reflect on the\noutcomes and consider the following points:\n1. Identify why the current result is unsatisfactory.\n2. Evaluate the effectiveness of past actions and thoughts. Propose improvements for\nthe next steps.\nYour reflection output should provide clear insights and actionable suggestions,\nfacilitating informed decision-making and guiding the LLM agent towards achieving\nbetter performance in subsequent interactions.\nIdeally, it should contain flaw and improvements\nYour response should use the following format:\n[reflection]Reflection\nHere is an example:\nYour:[reflection] The flaw in the approach was not considering the user’s budget\nconstraint of $80 while proposing places like The Cakery, which exceeds this limit.\nAdditionally, the proposal did not fully align with the user’s updated preference\nfor exclusively takeout options. The improvement would be to search for more budget\n-friendly takeout options that also allow reservations and offer panoramic views,\nensuring all selections strictly adhere to the user’s specified budget and\npreferences.\n\"\"\"\nTicket Ordering.\nC.1\nDataset Details\nThe Ticket Ordering task evaluates the ability of agents to collaboratively provide the best flight combinations for two\nusers. The dataset consists of 150 tasks, which are designed to benchmark the performance of different agents in ticket\nordering.\nInspired by the framework presented by [Lin et al., 2024], we build our evaluation framework based on their structure.\nSpecifically, we use the provided code to generate the dataset, which includes two users’ calendars. The tasks are\ncreated by combining the users’ calendar data, and agents are then asked to provide flight recommendations based on\nthis information.\nC.2\nExperiment Details\nTable 6 summarizes the experimental results for the Ticket Ordering task. The baseline model achieves an ac-\ncuracy of 19.94%. Claude-3.5-Sonnet achieves the highest accuracy of 62.85%, improving by +42.91%.\ngpt-4-turbo-0409 follows with an accuracy of 54.37%, improving by +34.43%. The accuracy range, from\n0.0% (Mistral-7B-Instruct) to 62.85%, highlights the dataset’s ability to differentiate models based on their\nperformance.\nThe dataset emphasizes Reasoning and Action capabilities, as seen in the high Rt and At Shapley values for top models\nlike Claude-3.5-Sonnet, gpt-4-turbo-0409, and qwen2.5-32b-Instruct. Models with stronger\nReasoning and Action abilities show significant accuracy improvements, whereas those with lower values for these\nmodules, such as Mistral-7B-Instruct, experience considerable performance deficits.\n21\nCapaBench\nTECHNICAL REPORT\nTable 6: Experimental Results on Ticket Ordering(150 tasks)\nLLM\nPt\nRt\nAt\nFt\nAcc(%)\n∆Acc(%)\nLlama3-8B-instruct (Default)\n-\n-\n-\n-\n19.94\n-\nClaude-3.5-Sonnet\n0.0026\n0.1855\n0.2165\n0.0244\n62.85\n+42.91\ngpt-4-turbo-0409\n-0.0426\n0.3011\n0.0275\n0.0583\n54.37\n+34.43\nqwen2.5-32b-Instruct\n0.1190\n0.1812\n-0.0002\n0.0431\n54.25\n+34.31\ngpt-4o-mini\n0.0315\n0.2434\n0.0491\n0.0047\n51.82\n+31.88\nLlama-3.1-70B-Instruct\n0.0035\n0.0891\n-0.2751\n-0.0010\n1.59\n-18.35\ndoubao-pro-4k\n0.1512\n-0.0008\n0.0058\n-0.0268\n32.88\n+12.94\nglm-4-air\n-0.1951\n0.1718\n-0.0199\n-0.0061\n15.01\n-4.93\nMistral-8X7B-instruct\n0.1830\n0.0535\n-0.0825\n-0.0111\n34.24\n+14.30\nMistral-7B-Instruct\n-0.1113\n-0.0702\n-0.0197\n0.0018\n0.0\n-19.94\nbest\n\/\n\/\n\/\n\/\n67.18\n47.24\nC.3\nPrompt Example\nC.3.1\nPlanning Module\nprompt_system_planning = ’’’\nWelcome to dialop-mediation challenge!\nFour LLM agents are working together to do mediation tasks step by step (planning ->\nreasoning -> action -> reflection). They are responsible for planning, reasoning,\nacting, and reflecting respectively.\nYou are the first LLM agent in charge of planning. Your role is to assist players by\ngenerating strategic plans based on the game’s instructions.\nRemember, your strategic plan will be used as part of the PROMPT for the later agents\nto guide them to make wise decisions.\nHere is how the task is structured:\n- task: You are a travel agent helping two users, User 0 and User 1, plan a trip\ntogether. They are both traveling from different cities and arriving the same city.\n- requirements:\n1. Your job is to help mediate by considering the information given by each user\nindividually and proposing a set of flights that suit for both of them.\n2. You should propose a set of flights for each user following the rules mentioned\nbelow.\nRules:\n- You must choose the flight that is not conflict with the user’s important calendar.\nThe less the importance of the calendar, the better the flight. Of course, the\nflight that is not conflict with the user’s calendar is the best.\n- On the basis of the first rule, you should choose the flight with the lowest price.\nThe lower the price, the better the flight.\n- On the basis of the first and second rules, you should choose the flight that makes\nthe arrival time difference between two users as short as possible. The shorter the\narrival time difference, the better the flight.\n- The three rules above are in order of priority. That is, the first rule is the most\nimportant, the second rule is the second important, and the third rule is the least\nimportant.\nYou should output your strategy plan in a clear and brief sentence guiding the last\nthree agents through their decision-making process, including:\n- let them know the task they are responsible for.\n- let them know the rules of the task mentioned above.\n- let them know the priority of the rules.\nEnclose the plan with three backticks ‘‘‘, like this:\n‘‘‘\nHERE IS YOUR PLANNING CONTENT\n‘‘‘\n22\nCapaBench\nTECHNICAL REPORT\n’’’\nC.3.2\nReasoning Module Prompt\nprompt_system_reasoning = ’’’\nWelcome to dialop-mediation challenge!\nFour LLM agents are working together to do mediation tasks step by step (planning ->\nreasoning -> action -> reflection). They are responsible for planning, reasoning,\nacting, and reflecting respectively.\nYou are the second llm agent, who is a helpful mediation assistant in charge of\nreasoning. Your role is to provide the top five best flight combinations to help\nthe action agent make the best decision.\nRemember, your thought will be used as part of the PROMPT for action agents.\nHere is what you need to consider about:\n- You will receive the strategic plan from the planning agent, the past actions from\nthe action agent, the userdata, and the reflection information(if any).\n- Your reasoning should be based on the planning strategy given from the planning\nagent, the userdata in the CURRENT OBSERVATION section and the reflection\ninformation(if any) from the last reflection agent to help the action agent make\nthe best decision\n- You should consider the priority of the rules mentioned in the planning content and\nanalyze the user data to help the action agent make the best decision.\nIf there is no reflection information, it means that the last action agent made a good\ndecision, but it may not be the best. Therefore, you must make the latest action\nin the LAST ACTION section be your first choice.\nAdditionally, you need to analyze all possible flight combinations based on the user\ndata and the rules mentioned in the planning content and provide what you consider\nto be the other four best flight options.\nIf there is reflection information, then you should analyze the situation and provide\nthe top five best flight combinations for two users based on the rules mentioned in\nthe planning content and the suggestions from the reflection agent.\nRemember, you should not output your reasoning analysis, just the flight combinations.\nAnd you should output the top five best flight combinations in the following\nformat:\n‘‘‘\nFlight Combination 1:\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\nFlight Combination 2:\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\nFlight Combination 3:\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\nFlight Combination 4:\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\nFlight Combination 5:\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\n‘‘‘\n’’’\nC.3.3\nAction Module Prompt\n23\nCapaBench\nTECHNICAL REPORT\nprompt_system_action = ’’’\nWelcome to dialop-mediation challenge!\nFour LLM agents are working together to do mediation tasks step by step (planning ->\nreasoning -> action -> reflection). They are responsible for planning, reasoning,\nacting, and reflecting respectively.\nYou are the third llm agent, who is a helpful mediation assistant in charge of acting.\nIn this task, your job is to select the best flight combination for two users based on\nthe planning strategy from the planning agent, the reasoning content from the\nreasoning agent and the userdata.\nHere is what you need to notice:\n- You should select the best flight combination for two users based on the planning\nstrategy from the planning agent and the reasoning content from the reasoning agent\n.\n- - In the planning strategy, the planning agent has given you the rules to follow.\n- - In the reasoning content, the reasoning agent has given you the top five best\nflight combinations for two users.\nNormally, you should choose the best flight combination from the top five best flight\ncombinations given by the reasoning agent. But if you find that the flight\ncombination given by the reasoning agent is not in the corresponding User\nInformation\nor you find that the five flight combinations given by the reasoning agent are not the\nbest, you should analyze the situation by yourself and make the best decision.\nIf you choose the flight combination from the top five best flight combinations given\nby the reasoning agent, you should output the flight combination in the following\nformat:\n‘‘‘\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\n‘‘‘\nIf you choose the flight combination by yourself, you should output the flight\ncombination and the reason why you choose it in the following format:\n‘‘‘\nFlight for User 0: 19 | Alaska | 184 | 06\/02 03:25 PM - 11:25 PM\nFlight for User 1: 22 | American | 50 | 06\/02 06:25 PM - 09:25 PM\nReason: HERE IS THE REASON\n‘‘‘\n’’’\nC.3.4\nReflection Module Prompt\nprompt_system_reflection = ’’’\nWelcome to dialop-mediation challenge!\nFour LLM agents are working together to do mediation tasks step by step (planning ->\nreasoning -> action -> reflection). They are responsible for planning, reasoning,\nacting, and reflecting respectively.\nYou are the fourth llm agent in charge of reflecting.\nYou will receive the user data, the historical reasoning from the reasoning agent, and\nthe historical actions from the action agent.\nAnd here is your role:\n- You should carefully examine reasoning history to find out where things may have\ngone wrong\n- You should carefully examine action history to find out where things may have gone\nwrong, such as:\n- - the flight chosen by the action agent is not in the corresponding User Information\n- - the flight chosen by the action agent is too expensive or conflict with user’s\nimportant calendar\n24\nCapaBench\nTECHNICAL REPORT\n- You should remind the next reasoning and action agents to follow the rules mentioned\nin the planning section.\nIf you find the flight chosen by the action agent is not in the corresponding User\nInformation. Then you must report this in your output.\nIdeally, your output should also contain:\n- Flaw: clear and concise sentences that summarizes key factors causing the\nunsatisfactory result.\n- Improvement: One sentence that includes specifically how to adjust improve reasoning\nand action steps to achieve better outcomes in the future.\nNote: Please enclose the flaw and improvement with three backticks, like this:\n‘‘‘\nFlaw: HERE IS THE FLAW\nImprovement: HERE IS THE IMPROVEMENT\n‘‘‘\n’’’\nMath Solver.\nTable 7: PRAF Experiment Results on Mathematics Tasks with ∆Accuracy\nAlgebra\nGeometry\nLLM\nPt\nRt\nAt\nFt\nAcc(%)\n∆Acc(%)\nPt\nRt\nAt\nFt\nAcc(%)\n∆Acc(%)\nllama3-8B-instruct\n\/\n\/\n\/\n\/\n21.6\n\/\n\/\n\/\n\/\n\/\n14.4\n\/\nClaude-3.5-Sonnet\n0.021\n0.177\n0.398\n0.031\n84.4\n62.8\n0.055\n0.085\n0.486\n0.054\n82.4\n68.0\ngpt-4-turbo\n0.058\n0.082\n0.456\n0.020\n83.2\n61.6\n0.038\n0.047\n0.527\n0.025\n78.0\n63.6\nqwen2.5-32B\n0.059\n0.146\n0.436\n0.011\n86.8\n65.2\n0.071\n0.067\n0.530\n0.051\n86.4\n72.0\ngpt-4o-mini\n0.070\n0.020\n0.313\n0.053\n67.2\n45.6\n0.065\n0.024\n0.368\n0.035\n63.6\n49.2\ndoubao-pro-4k\n0.124\n0.086\n0.178\n0.004\n60.8\n39.2\n0.105\n0.032\n0.186\n-0.007\n46.0\n31.6\nGLM-4-air\n0.053\n0.069\n0.346\n0.004\n68.8\n47.2\n0.059\n0.019\n0.349\n0.006\n57.6\n43.2\nllama3-70B\n0.040\n0.051\n0.321\n0.007\n63.6\n42.0\n0.015\n0.011\n0.333\n0.005\n50.8\n36.4\nMistral-8X7B\n0.006\n-0.010\n0.190\n-0.010\n39.2\n17.6\n0.004\n0.016\n0.138\n-0.018\n28.4\n14.0\nMistral-7B\n-0.065\n-0.015\n-0.053\n-0.003\n8.0\n-13.6\n-0.055\n0.014\n-0.035\n-0.004\n6.4\n-8.0\nD.1\nDataset Deatils\nThe Math Solver dataset evaluates agents’ planning, reasoning, and action capabilities in solving diverse mathematical\nproblems, with a particular focus on tool usage during the problem-solving process. This dataset is divided into two\ncategories: Algebra and Geometry, comprising a total of 500 tasks (250 Algebra tasks and 250 Geometry tasks).\nDataset Construction. The dataset is derived from the MATH dataset [Hendrycks et al., 2021] and enhanced with\nGPT-4 to improve diversity and relevance. The MATH dataset’s original structure includes a large number of highly\nsimilar questions without detailed knowledge point categorization, making evaluation costly and inefficient. To address\nthis, we synthesized new data by:\n(1) Summarizing Knowledge Points: All problems in the MATH dataset were analyzed using GPT-4 to extract a\ncomprehensive list of key concepts.\n(2) Condensing Categories: GPT-4 distilled the extracted concepts into 10 key knowledge points for Algebra and\nGeometry, respectively.\n(3) Mapping Labels: Each problem in the original dataset was mapped to one of the 10 knowledge points and\nassigned a difficulty level (1–5).\n(4) Synthesizing New Problems: For each unique combination of knowledge point and difficulty level, GPT-4\ngenerated five new problems, ensuring coverage across all categories.\nOverall, both algebra and geometry each include ten knowledge points. Each knowledge point is divided into five\nlevels, and for each combination, there are five problems. Therefore, the total amount of data is 2 × 10 × 5 × 5 = 500.\nKnowledge points and corresponding examples can be seen in Table.10.\n25\nCapaBench\nTECHNICAL REPORT\nTable 8: Classification and Examples of Knowledge Points in Algebra\/Geometry Data Sets\nAlgebra\nGeometry\nKnowledge Point\nExample\nKnowledge Point\nExample\nComplex Numbers\nEvaluate the sum\ni100 + i101 + i102 + · · · + i204.\nCircles and Their\nProperties\nA chord of length 8 cm is 6 cm\naway from the center of a circle.\nWhat is the radius of the circle?\nAlgebra in\nCoordinate Geometry\nA circle has a center at (h, −1) and\npasses through the points (0, 2) and\n(4, 0). Find the radius of the circle.\nFundamentals of\nTrigonometry\nThe angle β in a right triangle\nsatisfies cos(β) = 12\/13. Find\nsin(2β).\nExponents and\nLogarithms\nIf 3x = 4, 4y = 5, 5z = 6, and\n6w = 7, find the value of\nx · y · z · w.\nGeometric\nConstructions and\nCoordinate Geometry\nA rhombus ABCD is situated in\nthe coordinate plane with vertices\nA(1, 2), B(4, 6), and C(7, 2).\nDetermine the side length of the\nrhombus.\nFunction\nComposition and\nInverses\nLet f(x) = 2x + 3. You are given\nthat g(f(x)) = 3x −4 for all x.\nDetermine the value of g(7).\nGeometric\nInequalities and\nOptimization\nProblems\nGiven a trapezoid with bases 10\nunits and 6 units, and one\nnon-parallel side 4 units, find the\nmaximum area of the trapezoid.\nInequalities and\nAbsolute Values\nFind the product of integer\nsolutions for z(-5 < z < 5) such that\n|z2 −9| equals a prime number.\nPolygons and Their\nProperties\nCalculate the area of a regular\ndodecagon (12-sided polygon) with\na circumradius (radius of the\ncircumscribed circle) of 8 cm.\nPolynomials and\nPolynomial\nOperations\nGive the factorization result of\nexpression (x +\n√\n3)3 + (y −\n√\n3)3\nProperties of Right\nTriangles\nIn triangle DEF, DE = EF = 13\nand DF = 10. Let G be the foot of\nthe altitude from D to EF.\nCompute the area of triangle DGF.\nQuadratic Equations\nand Functions\nDetermine the sum of all integer\nvalues of b for which the quadratic\nequation x2 + bx + b = 0 has\ninteger solutions.\nQuadrilateral\nFeatures and\nClassifications\nQuadrilateral ABCD has AB = 4,\nBC = 5, CD = 6, DA = 7, and\ndiagonal AC = 8. Find the area of\nABCD.\nRational Functions\nand Expressions\nDetermine the domain of the\nfunction\ng(x) =\n3x −7\nx2 −4x + 3.\nExpress your domain in interval\nnotation.\nSimilar Triangles and\nProportions\nIn triangle DEF, point G divides\nside DF in the ratio 2 : 3. If the\narea of triangle DEG is 12, find the\narea of triangle EFG.\nSequences and Series\nConsider the sequence defined\nrecursively by a1 = 1000 and\nan+1 = an −n for n ≥1.\nDetermine the smallest positive\ninteger n for which an < 0.\nThree-Dimensional\nGeometry\nA sphere with radius 7 is inscribed\nin a right circular cone. The cone’s\nheight is 24. Find the radius of the\ncone at its base.\nSystems of Linear\nEquations\nAssume x and y satisfy the system\nof equations: 3x2 + 4y = 16 and\n2x −y = 3. Compute sum of\npossible value of 4x + 3y.\nTransformative\nGeometry and\nSymmetry\nA circle with radius 5 cm is rotated\nabout its center by 180 degrees.\nWhat is the total area covered by\nthe circle during the rotation?\n26\nCapaBench\nTECHNICAL REPORT\nD.2\nExperiment Details\nTable 7 summarizes the experimental results for the Math task, including Shapley values for the four modules (Planning\n(Pt), Reasoning (Rt), Action (At), and Reflection (Ft)), as well as task success rates (Accuracy (%)) and their\nimprovement (∆Accuracy (%)) relative to the baseline (Llama3-8B-instruct).\nThe baseline model (Llama3-8B-instruct) achieves task success rates of 21.6% (algebra) and 14.4% (geometry).\nThe best-performing model, qwen2.5-32B, achieves accuracies of 86.8% and 86.4%, with significant improvements\nof +65.2% and +72.0%, respectively. This highlights its strong overall performance, driven by its balanced capabilities\nin reasoning, acting, and reflection.\nClaude-3.5-Sonnet demonstrates excellent reasoning but falls short in acting, leading to slightly lower success\nrates compared to qwen2.5-32B. Notably, doubao-pro-4k excels in planning but lacks strength in other compo-\nnents, limiting its overall accuracy. Open-source models lag significantly behind closed-source models, underscoring\nthe current gap in performance.\nThe evaluation also reveals the importance of tool usage during acting phases, where agents successfully leverage\ncalculators and search engines to solve complex tasks. Reflection phases are crucial for iterative problem-solving,\nenabling corrections and better outcomes in challenging mathematical scenarios.\nNote that in the last line, best refers to combining the optimal models of the four modules to conduct the p-r-a-f\nexperiment again. On the Algebra dataset, this approach increases the task success rate from the optimal model\nqwen2.5-32B’s 86.8% to 88.4%. This indicates that our evaluation method is meaningful, as combining the best\nmodels in each capability can outperform using a single best model. Unfortunately, there is no performance improvement\non the Geometry dataset, which may be related to poor collaboration between the models.\nD.3\nPrompt Example\nD.3.1\nPlanning Module Prompt\nprompt_system_planning = \"\"\"\nWelcome to the Math Problem Challenge!\nFour llm agents are working together to solve math problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the first llm agent, and your role is to assist other agents by generating\nstrategic plans based on the math problem.\nHere is how the plan is structured:\n- You will be given an instruction that describes the details of the current math\nproblem.\n- Based on the instruction, you are to generate a strategic plan that helps following\nagents solve this math problem efficiently.\n- Your generated plan should consider current known conditions, possible mathematical\nderivations, related calculation formulas, etc, and align with the ultimate goal of\ngetting the final answer within 10 rounds.\n- At each step, the acting agent can use a calculator to perform calculations or a\nsearch engine to search for information and other operations, etc.\n- Remember, your strategic insights are crucial for guiding following agents to make\ninformed decisions and achieve success in the math problem.\nNote: Please surround the planning content you generated with three backticks. That is\n:\n‘‘‘\nHERE IS YOUR PLANNING\n‘‘‘\n\"\"\"\nD.3.2\nReasoning Module Prompt\nprompt_system_reasoning = \"\"\"\nWelcome to the Math Problem Challenge!\nFour llm agents are working together to solve math problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\n27\nCapaBench\nTECHNICAL REPORT\nFigure 8: Example in Geometry.\nYou are the second llm agent, who is a helpful math problem-solving guidance assistant\nin charge of reasoning.\nAs an LLM Agent, your role is to use the given data to guide the player’s next\noperation effectively, analyze the updated solving progress, past operation, and\nknown condition of the problem to decide on a critical next operation.\nIn each round, following information will be given to you:\n1. ORIGINAL PROBLEM\n2. PLANNING STRATEGY\n3. HISTORICAL ACTIONS\n4. REFLECTION INFORMATION(if any)\nBased on these inforation, you should response with a reasoning to guide the acting\nagent’s next proving operation.\n28\nCapaBench\nTECHNICAL REPORT\nThe thought you give will guide the acting agent to use a calculator to do\ncalculations, or to use a search engine to search for information or do some other\noperations.\nNote: Please surround the reasoning content you generated with three backticks. That\nis:\n‘‘‘\nHERE IS YOUR reasoning\n‘‘‘\n\"\"\"\nD.3.3\nActing Module Prompt\nprompt_system_action = \"\"\"\nWelcome to the Math Problem Challenge!\nFour llm agents are working together to solve math problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the third llm agent, who is a helpful math problem-solving guidance assistant\nin charge of acting.\nIn each round, the following information will be given to you:\n1. ORIGINAL PROBLEM\n2. PLANNING STRATEGY\n3. CURRENT THOUGHT\n4. HISTORICAL ACTIONS\nBased current reasoning, you should give a response.\nYou have two tools:\n- One is a calculator, you can use this tool by responsing with an algebraic\nexpression. and I’ll give you the result;\n- The other is search engine, you can use this tool by responsing with some key words,\nand I’ll give you the most relavant three search results;\nIn each round, you need to determine whether the current problem has been solved based\non the current status.\n- If you think the problem has been solved, output should be following format(notice\nthat the answer should be just the precise value, no additional information is\nneeded such as unit.):\n‘‘‘\nAnswer: HERE IS THE ANSWER\n‘‘‘\n(Attention: You should confirm you answer as soon as possible. And the ANSWER must be\nin LATEX format.)\n- Otherwise, you should response with an action, and you can use at most one tool in\neach turn.\nYou must respond in one of three ways:\n1. If you think you need to use calculator, output should be following format:\n‘‘‘\nTool: Calculator\nAlgebraic expression: HERE IS THE ALGEBRAIC EXPRESSION\n‘‘‘\n(Attention: The ALGEBRAIC EXPRESSION must be standardized in LATEX format.\nThe calculator can also calculate trigonometric functions, note that the unit is\nradians, and you can use ‘pi‘ such as \\sin(\\pi\/6) = 0.5, but not \\sin(30))\n2. If you think you need to use search engine, output should be following format:\n‘‘‘\nTool: Search engine\nKey words: HERE IS THE KEY WORDS\n‘‘‘\n3. If you think you need to do some other operation, output should be following format\n:\n‘‘‘\nTool: None\n29\nCapaBench\nTECHNICAL REPORT\nAction: HERE IS THE ACTION\n‘‘‘\nAttention: Please enclose your response with three backticks.\nBesides, the environment can only give you result of using calculator or search engine\n, namely, any other operation should be done on your own.\n\"\"\"\nD.3.4\nReflection Module Prompt\nprompt_system_reflection = \"\"\"\nWelcome to the Math Problem Challenge!\nFour llm agents are working together to solve math problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the fourth llm agent, who is a helpful math problem-solving guidance assistant\nin charge of reflecting.\nIn each round, the following information will be given to you:\n1. ORIGINAL PROBLEM\n2. HISTORICAL THOUGHTS\n3. HISTORICAL ACTIONS\nAs an LLM Agent, your role is to reflect why the acting agent confirms a wrong answer.\nYou should carefully examine previous reasoning and action history to find out where\nthings may have gone wrong, summarize where they went wrong, and propose possible\nimprovements.\nUse these as a guide, and generate a reflection for the next reasoning and action\nsteps. Outline actionable insights and strategies to improve outcomes in the\nupcoming rounds.\nYour reflection output should provide clear insights and actionable suggestions,\nfacilitating informed decision-making and guiding the LLM agent towards achieving\nbetter performance in subsequent interactions.\nNote: A possible reason for the error is that the standard answer should retain\nfractions, radicals, pi, etc. If the question does not clearly indicate that it is\nexpressed in decimal, these should be retained.\nAnd another possible reason is that the answer given by the acting agent repeats the\nrequired variables, such as requiring the length of AB. Answering AB = 10 will be\njudged as an error, but only answering 10 can pass the test correctly. In other\nwords, just answer the value of the requested content.\nIdeally, it should contain:\n- Flaw: One sentence that summarizes key factors causing the unsatisfactory result.\n- Improvement: One sentence that includes specifically how to adjust improve reasoning\nand action steps to achieve better outcomes in the future.\nNote: Please enclose the flaw and improvement with three backticks:\n‘‘‘\nFlaw: HERE IS THE FLAW\nImprovement: HERE IS THE IMPROVEMENT\n‘‘‘\n\"\"\"\nAutomatic Theorem Proving.\nE.1\nDataset Details\nThe Automatic Theorem Proving dataset evaluates agents’ capabilities in solving formal proof problems, focusing on\ngenerating code for logical proofs. The dataset includes three categories: Coq, Lean 4, and Isabelle, with a total of 333\ntasks (111 tasks per category).\nDataset Construction. The dataset originates from 111 Coq problems curated from course material, covering the\nfollowing topics:\n30\nCapaBench\nTECHNICAL REPORT\n(1) Algebraic Calculations, e.g., derivation of linear systems.\n(2) Properties of Functions, e.g., translation and monotonicity of functions.\n(3) Properties of Recursive Structures, e.g., operations on tree structures.\n(4) Logical Problems, e.g., relationships between AND, OR, and NOT.\n(5) Properties of Natural Numbers, e.g., proving 6 is not a prime number.\nThese proof problems serve as introductory exercises in college formal proof courses, focusing on basic syntax and\nsimple logical relationships. They are challenging for students, making them a suitable benchmark for evaluating the\nperformance of large language models (LLMs).\nTo comprehensively assess LLMs’ formal proof capabilities, these problems were further translated into Lean 4 and\nIsabelle versions. Coq, Lean 4, and Isabelle are widely used formal proof languages, and using multiple languages\nallows for a more rigorous comparison of model capabilities. And Figure.11 shows different language versions of the\nsame question.\nFigure 9: An Example Problem in Three Languages.\nE.2\nExperiment Details\nTable 9 summarizes the experimental results for the Automatic Theorem Proving task, presenting Shapley values for the\nfour modules (Planning (Pt), Reasoning (Rt), Action (At), and Reflection (Ft)), task success rates (Accuracy (%)),\nand improvement (∆Accuracy (%)) over the baseline model (Llama3-8B-instruct).\nTable 9: Experiment Results on Automatic Theorem Proving Tasks with ∆Accuracy\nCoq\nLean 4\nIsabelle\nLLM\nPt\nRt\nAt\nFt\nAcc(%)\n∆Acc(%)\nPt\nRt\nAt\nFt\nAcc(%)\n∆Acc(%)\nPt\nRt\nAt\nFt\nAcc(%)\n∆Acc(%)\nllama3-8B\n\/\n\/\n\/\n\/\n6.4\n\/\n\/\n\/\n\/\n\/\n2.7\n\/\n\/\n\/\n\/\n\/\n7.2\n\/\nClaude-3.5\n0.010\n0.067\n0.795\n0.027\n96.4\n90.0\n0.002\n0.059\n0.662\n0.098\n84.7\n82.0\n0.025\n0.046\n0.523\n0.082\n74.8\n67.6\ngpt-4-turbo\n0.032\n0.038\n0.706\n0.024\n86.5\n80.1\n-0.015\n-0.006\n0.375\n0.033\n41.4\n38.7\n0.020\n0.048\n0.542\n0.012\n69.4\n62.2\nqwen2.5-32B\n0.014\n0.029\n0.615\n0.026\n74.8\n68.4\n-0.007\n0.020\n0.486\n0.050\n57.7\n55.0\n0.048\n0.041\n0.434\n0.036\n63.1\n55.9\ngpt-4o-mini\n0.038\n-0.016\n0.391\n0.018\n49.5\n43.1\n-0.013\n-0.020\n0.396\n0.007\n39.6\n36.9\n0.030\n-0.012\n0.249\n0.021\n36.0\n28.8\ndoubao-pro-4k\n0.007\n0.039\n0.204\n0.001\n31.5\n25.1\n-0.017\n0.029\n0.095\n0.028\n16.2\n13.5\n0.035\n0.007\n-0.064\n0.004\n5.4\n-1.8\nGLM-4-air\n0.015\n0.016\n0.115\n0.033\n24.3\n17.9\n-0.004\n0.005\n0.193\n0.013\n23.4\n20.7\n-0.006\n-0.006\n0.176\n0.017\n25.2\n18.0\nllama3-70B\n0.018\n-0.137\n0.190\n0.009\n14.4\n8.0\n-0.005\n-0.000\n0.030\n0.020\n7.2\n4.5\n0.043\n-0.032\n0.155\n0.005\n24.3\n17.1\nMistral-8X7B\n0.014\n0.056\n0.122\n0.014\n27.0\n20.6\n0.003\n-0.017\n0.068\n-0.018\n6.3\n3.6\n0.058\n0.014\n-0.071\n-0.028\n4.5\n-2.7\nMistral-7B\n0.018\n0.013\n0.028\n-0.015\n10.8\n4.4\n0.020\n0.011\n0.012\n0.012\n8.1\n5.4\n-0.014\n0.006\n-0.068\n0.003\n0.0\n-7.2\nbest\n\/\n\/\n\/\n\/\n94.6\n+88.2\n\/\n\/\n\/\n\/\n87.4\n+84.7\n\/\n\/\n\/\n\/\n78.4\n+71.2\nThe baseline model achieves task success rates of 6.4% (Coq), 2.7% (Lean 4), and 7.2% (Isabelle). The best-performing\nmodel, Claude-3.5, achieves 96.4%, 84.7%, and 67.6% on these datasets, with significant improvements of +90.0%,\n+82.0%, and +67.6%, respectively. This demonstrates Claude-3.5’s strong overall performance, driven by balanced\nreasoning, acting, and reflection abilities.\ngpt-4-turbo ranks second on Coq and Isabelle, mainly due to slightly weaker reasoning and acting capabilities on\nCoq and inferior reflection ability on Isabelle. For Isabelle, error messages trigger reflection frequently, making strong\nreflection critical. Claude-3.5’s superior reflection ability ensures the highest success rate in this scenario.\n31\nCapaBench\nTECHNICAL REPORT\nOn Lean 4, gpt-4-turbo underperforms significantly. Prior research [Tsoukalas et al., 2024] suggests this is due to\nits tendency to generate Lean 3 syntax, leading to failures. For Coq and Isabelle, the datasets share content since they\nwere translated from the Coq dataset, resulting in comparable evaluation outcomes.\nNote that in the last line, best refers to combining the optimal models of the four modules to conduct the p-r-a-f\nexperiment again. On the Lean 4 and Isabelle dataset, this approach increases the task success rate from the optimal\nmodel Claude-3.5’s 84.7% and 74.8% to 87.4% and 78.4%. This indicates that our evaluation method is meaningful,\nas combining the best models in each capability can outperform using a single best model. Unfortunately, there is\nno performance improvement on the Coq dataset, which may be related to poor collaboration between the models.\nOn the other hand, Claude-3.5 already has a high accuracy on this dataset, making it difficult to achieve further\nimprovements.\nFigure 10 illustrates this process. Initially, the acting agent provided code that failed to compile. The reflection agent\nidentified the issue and proposed improvements. In the next interaction, the reasoning and acting agents used these\nreflections to correct the code, ultimately completing the proof successfully.\nFigure 10: Example in Logical Proofs.\nE.3\nPrompt Example(Coq)\nE.3.1\nPlanning Module Prompt\nprompt_system_planning = \"\"\"\nWelcome to the Coq Problem Challenge!\nFour llm agents are working together to solve coq problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the first llm agent, and your role is to assist players by generating proving\nplans based on the coq problem.\n32\nCapaBench\nTECHNICAL REPORT\nHere is how the plan is structured:\n- You will be given an instruction that describes the details of the current coq\nproblem, including libraries required for the problem, definitions of related\nconcepts, possible lemmas and problems to be proved(with name of the theorem).\n- In the problem, there may be theorems that skip the proof process(use Admitted) and\ncan be used directly when proving the main theorem for this problem.\n- Based on the instruction, you are to generate a strategic proving plan that helps\nthe player solve this coq problem efficiently.\n- Your generated plan should consider problem description and known conditions in\ndetail.\n- Remember, your strategic insights are crucial for guiding players to make informed\ndecisions and achieve success in the coq problem.\nNote: Please surround the planning content you generated with three backticks. That is\n:\n‘‘‘\nHERE IS YOUR PLANNING\n‘‘‘\n\"\"\"\nE.3.2\nReasoning Module Prompt\nprompt_system_reasoning = \"\"\"\nWelcome to the Coq Problem Challenge!\nFour llm agents are working together to solve coq problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the second llm agent, who is a helpful coq problem proving guidance assistant\nin charge of reasoning.\nAs an LLM Agent, your role is to use the given information to guide the acting agent’s\nnext proving operation effectively, in each round, following information will be\ngiven to you:\n1. Problem description\n2. Planning strategy\n3. Historical action(i.e., historical proving process)\n4. Current observation(i.e., goals and messages which can be seen in coq IDE)\n5. Reflection information(if any)\nBased on these inforation, you should response with a reasoning to guide the acting\nagent’s next proving operation.\nNote: Please surround the reasoning content you generated with three backticks. That\nis:\n‘‘‘\nHERE IS YOUR reasoning\n‘‘‘\n\"\"\"\nE.3.3\nActing Module Prompt\nprompt_system_action = \"\"\"\nWelcome to the Coq Problem Challenge!\nFour llm agents are working together to solve coq problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the third llm agent, who is a helpful coq problem proving guidance assistant\nin charge of acting.\nIn each round, the following information will be given to you:\n1. Original coq problem\n2. Proving planning strategy\n3. Current reasoning\n4. Historical proving action\n33\nCapaBench\nTECHNICAL REPORT\n5. Current observation(including current goals and messages which can be seen in coq\nIDE).\nIn each round, you need to generate an action based on the current status, note that\nthe action is just coq proof code.\nNote: in each round, you may add proof statements, or you may revoke previous proofs\nand start new proofs.\nIn other words, you can complete the proof step by step based on feedback from the\nenvironment.\nFor convenience, no matter what the operation is, please give the total coq proof\ncontent after the current action.\nAttention: In the question, you’ll see the mark like:\n(**********)\n(** Fill in your proof here*)\n(**********)\nOnly give the coq code that needs to be filled in the mark. Please don’t give anything\nthat doesn’t need to be placed in this mark, such as the description of the\noriginal theorem.\nMake sure that the content in the backticks is entirely coq syntax code, do not attach\nadditional information.\nPlease enclose your response coq proof code with three backticks:\n‘‘‘\n(HERE IS COQ CODE NEED TO FILL IN THE MARK)\n‘‘‘\n\"\"\"\nE.3.4\nReflection Module Prompt\nprompt_system_reflection = \"\"\"\nWelcome to the Coq Problem Challenge!\nFour llm agents are working together to prove coq problems step by step(planning ->\nreasoning -> acting -> reflecting). They are responsible for planning, reasoning,\nacting and reflecting respectively.\nYou are the fourth llm agent, who is a helpful coq problem proving guidance assistant\nin charge of reflecting.\nAs an LLM Agent, your role is to reflect on the recent outcomes and consider the\nfollowing points:\n1. Identify why the current result is unsatisfactory. Explore factors such as wrong\nproving process, incorrect use of conditions and so on.\n2. Evaluate the effectiveness of past actions and thoughts. Were there missed signals\nor incorrect assumptions?\n3. Propose improvements for the next steps. Suggest specific actions or adjustments in\nproving process.\n4. Consider the overall goal of proving the problem successfully. How can future\nactions better align with this objective?\n5. Is ’Admitted’ used in the certification process? If so, you need to avoid using it\nin the proof of the target theorem and complete the proof rigorously.\nUse these as a guide, and generate a reflection for the next reasoning and action\nsteps. Outline actionable insights and strategies to improve outcomes in the\nupcoming rounds.\nYour reflection output should provide clear insights and actionable suggestions,\nfacilitating informed decision-making and guiding the LLM agent towards achieving\nbetter performance in subsequent interactions.\nIdeally, it should contain:\n- Flaw: One sentence that summarizes key factors causing the unsatisfactory result.\n- Improvement: One sentence that includes specifically how to adjust improve reasoning\nand action steps to achieve better outcomes in the future.\nNote: Please enclose the flaw and improvement with three backticks:\n‘‘‘\nFlaw: HERE IS THE FLAW\nImprovement: HERE IS THE IMPROVEMENT\n34\nCapaBench\nTECHNICAL REPORT\n‘‘‘\n\"\"\"\nOperation System.\nF.1\nDataset Deatils\nThe Operation System dataset evaluates an agent’s ability to interact with a simulated OS terminal by executing\ncommands to address OS-related tasks, comprising 71 Ubuntu terminal tasks and 31 Git tasks.\nIn Ubuntu tasks, agents are required to propose bash commands to execute in Ubuntu Terminal and get feedback from\nthe terminal to complete the task. We utilized the AgentBench-OS framework Liu et al. [2023] to employ the evaluation.\nWe enhanced the automated data generation method from AgentBench-OS to construct our new dataset, primarily\ngenerating operation-type data. The original method leverages LLMs to generate tasks and employs unit tests to ensure\ntheir accuracy. While creating the dataset, we used specific prompts to guide the generation of desired data types. The\ndataset comprises 71 AgentBench-OS tasks, categorized into 40 file system manipulation, 20 system setting, and 11\nprocess running tasks.\nTable 10: Categories and Examples of Operating System Datasets\nCategory\nCategory Description\nRelated Commands\nExample Task Description\nFile System\nManipulation\nEvaluate the knowledge of basic file\nsystem manipulation operation such\nas creating, deleting, copying,\nmoving, compressing and listing\nfiles and directories.\nmkdir, touch, zip, tar,\nls, rm\nList all files larger than 1MB inside\nthe ’\/var\/log’ directory and write the\nlist to a file named ’large_files.txt’\nin the home directory.\nSystem Setting\nEvaluate the knowledge of system\nsetting such as disk partition, OS\nversion, user management.\ndf, useradd,\ngroupadd, uname,\nchmod, whoami,\nchown\nA user needs permission to read a\nfile in ’\/var\/private\/info.txt’. Grant\nread access to all users.\nProcess Running\nEvaluate the knowledge of\nprocesses management\nrenice, gcc, g++,\npython\nChange the priority of the process\nwith PID stored in \/tmp\/pidfile to a\nnice value of 10.\nFor the git tasks, we selected data from learngitbranchingThe learnGitBranching Team. The learngitbranching website\nitself is a tutorial git beginner. It provides terminal and sandbox environment that simulates git using a tree structure.\nGit tree dynamically updates along with each git command from the terminal. Given initial and target states for both\nlocal and remote git trees, agents must interact with the git tree via the terminal to transform it from its initial state\nto the target state. The dataset assesses proficiency in fundamental git commands and their combination to execute\nadvanced git functionalities.\nFigure 11: Illustration of OS-git task\n35\nCapaBench\nTECHNICAL REPORT\nF.2\nExperiment Deatils\nTable 11 summarizes the experimental results for the Operation System task, including Shapley values for the four\nmodules (Planning (Pt), Reasoning (Rt), Action (At), and Reflection (Ft)), as well as task success rates (Accuracy\n(%)) and their improvement (∆Accuracy (%)) relative to the baseline (Llama3-8B-instruct).\nTable 11: Experimental Results on Operating System(102 pieces)\nLLM\nPt\nRt\nAt\nFt\nReward (%)\n∆Reward (%)\nLlama3-8B-instruct(Default)\n-\n-\n-\n-\n0.98\n-\nclaude-3.5-sonnet\n0.0777\n0.4578\n0.0705\n-0.0079\n60.78\n+59.80\ngpt-4o-mini\n0.0420\n0.3050\n0.0645\n0.0199\n44.12\n+43.14\nglm-4-airx\n0.0465\n0.3051\n0.0414\n0.0044\n40.71\n+39.73\ngpt-4-turbo-0409\n0.0501\n0.3949\n0.0700\n0.0045\n52.94\n+51.96\nqwen2.5-32b-ins\n0.0596\n0.3113\n0.0531\n0.0368\n47.06\n+46.08\nMistral-7B-Instruct\n0.0042\n0.0465\n0.0188\n0.0188\n9.80\n+8.82\nLlama-3-70B-Instruct\n0.0769\n0.3126\n0.0397\n0.0119\n45.1\n+44.12\ndoubao-pro-4k\n0.0645\n0.2149\n0.0597\n-0.0057\n34.31\n+33.33\nMistral-8X7B-instruct\n0.0318\n0.1938\n0.0089\n0.0008\n24.51\n+23.53\nbest\n\/\n\/\n\/\n\/\n60.78\n+59.80\nReasoning is more crucial than planning in terminal-based OS scenarios. In these environments, essential\ninformation is obtained through ongoing interactions rather than being available upfront. For Ubuntu terminal tasks,\ndetails about the system, such as file system layout and settings, are mostly acquired interactively. Similarly, in git tasks,\nwhile the git tree state is visible, the sandbox setting requires further interaction to clarify the exact command forms\nsupported. Our prompting method starts with planning based on limited initial information, which reduces its impact\ndue to insufficient data for comprehensive task execution. Thus, reasoning becomes vital, enabling models to adapt to\nnew information and make informed decisions. This is evident in performance metrics, where reasoning scores surpass\nplanning scores, highlighting the importance of effective reasoning for success in these tasks.\nOS demands less in action compared to other senarios, due to its lower sim2real gap. Action module’s main\nfunction is to translate reasoning outputs into actions that fit the environment’s input specifications. Most real terminal\ncommands are also available for our benchmark. This alignment means the action formats are not unique to the\nevaluation but are prevalent in existing data. As a result, models require less adaptation or transformation to meet the\nbenchmark’s requirements, reducing the complexity of action processing and the demand on action ability compared to\nbenchmarks needing adaptation to novel task formats.\nThe minimal contributions from the Reflection (Ft) module suggest that this task lacks strong feedback signals through\nreflection. Thus proving reasoning is the primary focus evaluation module in Operation System Tasks.\nF.3\nPrompt Example\nF.3.1\nPlanning Module\nUbuntu Terminal Tasks\nYou are an Operating System assistant who can interact with Ubuntu Terminal to\ncomplete Operating System tasks. You can interact with the Ubuntu Operating system\nby terminal commands.\n[Task description]\nThe OS task you need to solve is:\nFind all ’.txt’ files in the ’docs’ directory and change their permissions to read-\nonly for all users..\nBased on the task description, outline a concise and clear strategic plan that divides\nthe task into subtasks. Your plan should be detailed and actionable, thus guiding\nyourself to complete the task efficiently.\nAfter your thinking, you should output your plan like\n‘‘‘plan\n36\nCapaBench\nTECHNICAL REPORT\nPut you plan here\n‘‘‘\nYour thinking and your plan are:\nGit Tasks\nYou are a git agent to complete a git task. As you know, if we consider every commit\nin git as a child node of the parent commit, the git tree is a tree structure. Thus\nyour git task is to change the init git tree to the target git tree. You can\ninteract with the git tree through a terminal by git commands.\n[Task Description]\nThe git task you need to solve is to change the init git tree to the target git tree.\nThe init git tree is:\n{’branches’: {’main’: {’target’: ’C1’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/main\n’}, ’o\/main’: {’target’: ’C1’, ’id’: ’o\/main’, ’remoteTrackingBranchID’: None}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None}, ’side2’: {\n’target’: ’C4’, ’id’: ’side2’, ’remoteTrackingBranchID’: None}, ’side3’: {’target’:\n’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents\n’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’\n: {’parents’: [’C1’], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’\nparents’: [’C3’], ’id’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’\nparents’: [’C5’], ’id’: ’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}}, ’tags’: {},\n’HEAD’: {’target’: ’side3’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’\ntarget’: ’C8’, ’id’: ’main’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’\nparents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’\n}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}}, ’tags’: {}, ’HEAD’: {’target’: ’main’, ’\nid’: ’HEAD’}}}.\nThe target git tree is:\n{’branches’: {’main’: {’target’: ’C11’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/\nmain’, ’localBranchesThatTrackThis’: None}, ’o\/main’: {’target’: ’C11’, ’id’: ’o\/\nmain’, ’remoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: [’main’]}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None, ’\nlocalBranchesThatTrackThis’: None}, ’side2’: {’target’: ’C4’, ’id’: ’side2’, ’\nremoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: None}, ’side3’: {’\ntarget’: ’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None, ’\nlocalBranchesThatTrackThis’: None}}, ’commits’: {’C0’: {’parents’: [], ’id’: ’C0’,\n’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’: {’parents’: [’C1’\n], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’parents’: [’C3’], ’\nid’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’parents’: [’C5’], ’id’:\n’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’\n}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C10’: {’parents’: [’C4’, ’C9’], ’\nid’: ’C10’}, ’C11’: {’parents’: [’C10’, ’C7’], ’id’: ’C11’}}, ’HEAD’: {’target’: ’\nmain’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’target’: ’C11’, ’id’: ’\nmain’, ’remoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: None}}, ’\ncommits’: {’C0’: {’parents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’:\n[’C0’], ’id’: ’C1’}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}, ’C5’: {’parents’: [’C1\n’], ’id’: ’C5’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C2’: {’parents’: [’C1’], ’\nid’: ’C2’}, ’C6’: {’parents’: [’C5’], ’id’: ’C6’}, ’C4’: {’parents’: [’C3’], ’id’:\n’C4’}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C7’: {’parents’: [’C6’], ’id’:\n’C7’}, ’C10’: {’parents’: [’C4’, ’C9’], ’id’: ’C10’}, ’C11’: {’parents’: [’C10’, ’\nC7’], ’id’: ’C11’}}, ’HEAD’: {’target’: ’main’, ’id’: ’HEAD’}}}.\nBased on the task description, first to compare the init git tree with the target git\ntree, then to outline a concise and clear strategic plan that divides the task into\nsubtasks.\nAfter your thinking, you should output your plan like\n‘‘‘plan\nPut you plan here\n‘‘‘\nYour thinking and your plan are:\n37\nCapaBench\nTECHNICAL REPORT\nF.3.2\nReasoning Module Prompt\nUbuntu Terminal Tasks\nYou are an Operating System assistant who can interact with Ubuntu Terminal to\ncomplete Operating System Tasks. You can interact with the Ubuntu Operating system\nby terminal commands.\n[Task Description]\nThe OS task you need to solve is:\\nFind all ’.txt’ files in the ’docs’ directory and\nchange their permissions to read-only for all users..\n[Terminal Working Directory]\nThe working directory of the Ubuntu Terminal is:\n\/.\n[Proposed Plan]\nAn abstract plan on how to complete the task is:\n1. Navigate to the ’docs’ directory\n2. Find all ’.txt’ files in the directory and its subdirectories\n3. Change the permissions of the found files to read-only for all users\n4. Verify the changes.\n[History Interaction Information]\nYour past history interaction information is:\n[].\nThe abstract plan on how to complete the task is a guide to help you analyze the task\nand complete it efficiently. Based on the action history and the output of the\nUbuntu System Terminal, think about which subtask of the plan you are processing\ncurrently.\nIf you think the task is completed, you can just output ’The task is completed’ in\nyour reasoning output.\nOtherwise, based on the current stage, think how to use terminal commands to interact\nwith the Ubuntu terminal to solve the task efficiently. You need to propose\nspecific commands and corresponding command parameters of those commands.\nAfter your reasoning about the task, you should summarize your reasoning (your summary\nmust contain all key information) and output the summary result like\n‘‘‘reasoning\nPut your reasoning summary here\n‘‘‘\nYour thinking and your reasoning are:\nGit Tasks\nYou are a git agent to complete a git task. As you know, if we consider every commit\nin git as a child node of the parent commit, the git tree is in a tree structure.\nYou can interact with the git tree through a terminal by git commands.\n[Task Description]\nThe whole git task you need to solve is to change the init git tree to the target git\ntree, while after your past interaction, the git tree is currently in the state of\ncurrent git tree.\nThe init git tree is:\n{’branches’: {’main’: {’target’: ’C1’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/main\n’}, ’o\/main’: {’target’: ’C1’, ’id’: ’o\/main’, ’remoteTrackingBranchID’: None}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None}, ’side2’: {\n’target’: ’C4’, ’id’: ’side2’, ’remoteTrackingBranchID’: None}, ’side3’: {’target’:\n’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents\n’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’\n: {’parents’: [’C1’], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’\nparents’: [’C3’], ’id’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’\nparents’: [’C5’], ’id’: ’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}}, ’tags’: {},\n’HEAD’: {’target’: ’side3’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’\n38\nCapaBench\nTECHNICAL REPORT\ntarget’: ’C8’, ’id’: ’main’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’\nparents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’\n}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}}, ’tags’: {}, ’HEAD’: {’target’: ’main’, ’\nid’: ’HEAD’}}}.\nThe target git tree is:\n{’branches’: {’main’: {’target’: ’C11’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/\nmain’, ’localBranchesThatTrackThis’: None}, ’o\/main’: {’target’: ’C11’, ’id’: ’o\/\nmain’, ’remoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: [’main’]}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None, ’\nlocalBranchesThatTrackThis’: None}, ’side2’: {’target’: ’C4’, ’id’: ’side2’, ’\nremoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: None}, ’side3’: {’\ntarget’: ’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None, ’\nlocalBranchesThatTrackThis’: None}}, ’commits’: {’C0’: {’parents’: [], ’id’: ’C0’,\n’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’: {’parents’: [’C1’\n], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’parents’: [’C3’], ’\nid’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’parents’: [’C5’], ’id’:\n’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’\n}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C10’: {’parents’: [’C4’, ’C9’], ’\nid’: ’C10’}, ’C11’: {’parents’: [’C10’, ’C7’], ’id’: ’C11’}}, ’HEAD’: {’target’: ’\nmain’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’target’: ’C11’, ’id’: ’\nmain’, ’remoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: None}}, ’\ncommits’: {’C0’: {’parents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’:\n[’C0’], ’id’: ’C1’}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}, ’C5’: {’parents’: [’C1\n’], ’id’: ’C5’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C2’: {’parents’: [’C1’], ’\nid’: ’C2’}, ’C6’: {’parents’: [’C5’], ’id’: ’C6’}, ’C4’: {’parents’: [’C3’], ’id’:\n’C4’}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C7’: {’parents’: [’C6’], ’id’:\n’C7’}, ’C10’: {’parents’: [’C4’, ’C9’], ’id’: ’C10’}, ’C11’: {’parents’: [’C10’, ’\nC7’], ’id’: ’C11’}}, ’HEAD’: {’target’: ’main’, ’id’: ’HEAD’}}}.\nThe current git tree is:\n{’branches’: {’main’: {’target’: ’C1’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/main\n’}, ’o\/main’: {’target’: ’C1’, ’id’: ’o\/main’, ’remoteTrackingBranchID’: None}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None}, ’side2’: {\n’target’: ’C4’, ’id’: ’side2’, ’remoteTrackingBranchID’: None}, ’side3’: {’target’:\n’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents\n’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’\n: {’parents’: [’C1’], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’\nparents’: [’C3’], ’id’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’\nparents’: [’C5’], ’id’: ’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}}, ’tags’: {},\n’HEAD’: {’target’: ’side3’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’\ntarget’: ’C8’, ’id’: ’main’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’\nparents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’\n}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}}, ’tags’: {}, ’HEAD’: {’target’: ’main’, ’\nid’: ’HEAD’}}}.\n[Proposed Plan]\nAn abstract plan on how to complete the git task is:\n1. Fetch updates from origin to get C8\n2. Checkout side1 (C2)\n3. Merge o\/main (C8) into side1 to create C9\n4. Checkout side2 (C4)\n5. Merge the branch containing C9 to create C10\n6. Checkout side3 (C7)\n7. Merge the branch containing C10 to create C11\n8. Checkout main\n9. Reset main to C11\n10. Push main to origin to update remote\n11. Fetch from origin to update o\/main.\n[History Interaction Information]\nYour past history interaction information with the git tree is:\n[].\n39\nCapaBench\nTECHNICAL REPORT\nThe proposed plan on how to complete the task is a guide to help you analyze the task\nand complete it efficiently. Based on the history interaction information and the\ncurrent git tree state, think about which subtask of the plan you are processing\ncurrently.\nBased on the current stage, think how to use git commands to change the current git\ntree to the target git tree efficiently. You need to propose specific git commands\nto complete the subtask.\nAfter your reasoning about the task, you should summarize your reasoning(your summary\nmust contain all key information) and output the summary result like\n‘‘‘reasoning\nPut your reasoning summary here\n‘‘‘\nYour thinking and your reasoning are:\nF.3.3\nAction Module Prompt\nUbuntu Terminal Tasks\nYou are an Operating System assistant who can interact with Ubuntu Terminal to\ncomplete Operating System Tasks. You can interact with the Ubuntu Operating system\nby terminal commands. To solve the task, you must understand the [Reasoning Thought\n] and faithfully follow the guidance of [Reasoning Thought].\n[Reasoning Thought]\nYour analysis about the current state and how to solve the task is:\n1. To find the ’docs’ directory, locate all ’.txt’ files within it, change their\npermissions to read-only for all users, and verify the changes, we can use the\nfollowing command:\\n\\n\nfind \/ -type d -name docs -exec sh -c ’find {} -type f -\nname \\\"*.txt\\\" -exec chmod 444 {} \\\\; -exec ls -l {} \\\\;’ \\\\;\\n\\n\nThis command\ndoes the following:\\n\n- Searches for the ’docs’ directory starting from the root\n(\/)\\n\n- Finds all ’.txt’ files within the ’docs’ directory and its subdirectories\n\\n\n- Changes the permissions of each found file to 444 (read-only for all users)\\\nn\n- Lists the files with their new permissions\n2. If the command executes successfully, we’ll see a list of ’.txt’ files with their\nnew permissions. Each file should have permissions like \\\"-r--r--r--\\\".\n3. If we don’t see any output or get an error, it might mean that either the ’docs’\ndirectory doesn’t exist, there are no ’.txt’ files, or we don’t have the necessary\npermissions to access or modify the files.\nLet’s execute this command and analyze the output to determine if further actions are\nneeded..\n[Action Options and Action Output Instruction]\n1. Bash Action\nIf the reasoning tells that you should use some terminal commands to execute some bash\ncode, take bash action. Then you need to faithfully based on [Reasoning Thought],\ncompose the terminal command and corresponding command parameters to propose\nspecific command to solve the task. You should print like this:\nAct: bash\n‘‘‘bash\n# put your bash code here\n‘‘‘\n2. Finish Action\n40\nCapaBench\nTECHNICAL REPORT\nIf the [Reasoning Thought] think the task is completed, for example, it tells ’The\ntask is completed’. That means you have finished the task, no more action is needed\n, just take finish action, thus you should print like this:\nAct: finish\nYou need to faithfully based on the [Reasoning Thought], according to the [Action\nOptions and Action Output Instruction] to choose either Bash Action or Finish\nAction. Then you need to propose the action failthfully based on the [Reasoning\nThought] and make sure the action satisfies the action output instruction.\nNow, your action is:\"\nGit Tasks\nYou are a git agent to complete a git task. As you know, if we consider every commit\nin git as a child node of the parent commit, the git tree is in a tree structure.\nYou can interact with the git tree through a terminal by git commands.\nTo solve the task, you must understand the [Reasoning Thought] and faithfully follow\nthe guidance of [Reasoning Thought] to propose specific git commands to proceed the\ntask.\n[Reasoning Thought]\nThe current state of git tree is: {’branches’: {’main’: {’target’: ’C1’, ’id’: ’main’,\n’remoteTrackingBranchID’: ’o\/main’}, ’o\/main’: {’target’: ’C1’, ’id’: ’o\/main’, ’\nremoteTrackingBranchID’: None}, ’side1’: {’target’: ’C2’, ’id’: ’side1’, ’\nremoteTrackingBranchID’: None}, ’side2’: {’target’: ’C4’, ’id’: ’side2’, ’\nremoteTrackingBranchID’: None}, ’side3’: {’target’: ’C7’, ’id’: ’side3’, ’\nremoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents’: [], ’id’: ’C0’, ’\nrootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’: {’parents’: [’C1’\n], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’parents’: [’C3’], ’\nid’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’parents’: [’C5’], ’id’:\n’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}}, ’tags’: {}, ’HEAD’: {’target’: ’\nside3’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’target’: ’C8’, ’id’: ’\nmain’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents’: [], ’id’: ’\nC0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C8’: {’parents’:\n[’C1’], ’id’: ’C8’}}, ’tags’: {}, ’HEAD’: {’target’: ’main’, ’id’: ’HEAD’}}}.\nYour analysis about the current state and how to solve the task is:\nGiven the current git tree state and the target git tree, we need to follow the\nproposed plan to achieve the desired result. We’re at the beginning of the plan, so\nwe’ll start with the first subtask:\n1. Fetch updates from origin to get C8:\nWe need to fetch the latest changes from the remote repository to bring C8 into our\nlocal repository.\n2. Checkout side1 (C2):\nWe’ll move to the side1 branch, which is currently at C2.\n3. Merge o\/main (C8) into side1 to create C9:\nAfter fetching, we’ll merge the remote main branch (o\/main) into side1 to create\nthe new commit C9.\nThese steps will set the foundation for the subsequent merges and branch manipulations\n. The specific git commands to accomplish these subtasks are:\n1. git fetch origin\n2. git checkout side1\n3. git merge o\/main\nThese commands will update our local repository with the remote changes, switch to the\nside1 branch, and create the new merge commit C9, which combines the changes from\nside1 (C2) and o\/main (C8)..\n41\nCapaBench\nTECHNICAL REPORT\n[Action Options and Action Output Instruction]\nMost common git commands are available.\nA special case is there’s no need to use git add command for there’s no concept of\nadding \/ staging files.\nYou should analyze which git command and the parameters of the git command from [\nReasoning Thought].\nOnce you determine the git command, you should output the git command in the following\nformat:\n‘‘‘command\n# put your git command here\n‘‘‘\nAn example is like this:\n‘‘‘command\ngit commit\n‘‘‘\nYou need to faithfully based on the [Reasoning Thought], according to the [Action\nOptions and Action Output Instruction] to generate the next git command.\nNow, your action is:\nF.3.4\nReflection Module Prompt\nUbuntu Terminal Tasks\nYou are an Operating System assistant who can interact with Ubuntu Terminal to\ncomplete Operating System Tasks. You can interact with the Ubuntu Operating system\nby terminal commands.\n[Task Description]\nThe OS task you need to solve is Find all ’.txt’ files in the ’docs’ directory and\nchange their permissions to read-only for all users..\n[Proposed Plan]\nAn abstract plan on how to complete the task is 1. Navigate to the ’docs’ directory\n2. Find all ’.txt’ files in the directory and its subdirectories\n3. Change the permissions of the found files to read-only for all users\n4. Verify the changes.\n[History Interaction Information]\nYour past history interaction information is [\n{’role’: ’agent’, ’command’: ’find \/ -type d -name docs -exec sh -c \\\\’find {} -type f\n-name \\\"*.txt\\\" -exec chmod 444 {} \\\\\\\\; -exec ls -l {} \\\\\\\\;\\\\’ \\\\\\\\;’},\n{’role’: ’terminal’, ’output’: \\\"find: ’\/proc\/1\/map_files’: Operation not permitted\\\\r\n\\\\nfind: ’\/proc\/20\/map_files’: Operation not permitted\\\\r\\\\nfind: ’\/proc\/21\/\nmap_files’: Operation not permitted\\\\r\\\\nfind: ’\/proc\/102\/map_files’: Operation not\npermitted\\\\r\\\\ntotal 0\\\\r\\\\n-rw-r--r-- 1 root root 0 Jan 10 09:13 doc1.txt\\\\r\\\\n-\nrw-r--r-- 1 root root 0 Jan 10 09:13 doc2.txt\\\\r\\\\n-rw-r--r-- 1 root root 0 Jan 10\n09:13 doc3.log\\\\r\\\\ntotal 0\\\\r\\\\n-rw-r--r-- 1 root root 0 Jan 10 09:13 doc1.txt\\\\r\n\\\\n-rw-r--r-- 1 root root 0 Jan 10 09:13 doc2.txt\\\\r\\\\n-rw-r--r-- 1 root root 0 Jan\n10 09:13 doc3.log\\\"}].\nHowever, in your last interaction in the env, your proposed command failed. Usually,\nthe failure may be due to:\n1. Your command failed to be executed in the Ubuntu terminal.\n2. Your command can be executed, but it takes too long to be completed and get the\nterminal response.\nNo matter which case, you need to reflect on the recent interaction history and\nconsider the following points:\n42\nCapaBench\nTECHNICAL REPORT\n1. Identify why the current result is unsatisfactory.\n2. Propose improvements for the next steps.\n3. Consider the overall goal of completing the OS task. How can future actions better\nalign with this objective?\nAfter your thinking, you should output your reflection like:\n‘‘‘reflection\nPut your reflection here\n‘‘‘\nYour thinking and reflection are:\nGit Tasks\nYou are a git agent to complete a git task. As you know, if we consider every commit\nin git as a child node of the parent commit, the git tree is in a tree structure.\nYou can interact with the git tree through a terminal by git commands.\n[Task Description]\nThe whole git task you need to solve is to change the init git tree to the target git\ntree, while after your past interaction, the git tree is currently in the state of\ncurrent git tree.\nThe init git tree is:\n{’branches’: {’main’: {’target’: ’C1’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/main\n’}, ’o\/main’: {’target’: ’C1’, ’id’: ’o\/main’, ’remoteTrackingBranchID’: None}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None}, ’side2’: {\n’target’: ’C4’, ’id’: ’side2’, ’remoteTrackingBranchID’: None}, ’side3’: {’target’:\n’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents\n’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’\n: {’parents’: [’C1’], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’\nparents’: [’C3’], ’id’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’\nparents’: [’C5’], ’id’: ’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}}, ’tags’: {},\n’HEAD’: {’target’: ’side3’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’\ntarget’: ’C8’, ’id’: ’main’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’\nparents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’\n}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}}, ’tags’: {}, ’HEAD’: {’target’: ’main’, ’\nid’: ’HEAD’}}}.\nThe target git tree is:\n{’branches’: {’main’: {’target’: ’C11’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/\nmain’, ’localBranchesThatTrackThis’: None}, ’o\/main’: {’target’: ’C11’, ’id’: ’o\/\nmain’, ’remoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: [’main’]}, ’\nside1’: {’target’: ’C2’, ’id’: ’side1’, ’remoteTrackingBranchID’: None, ’\nlocalBranchesThatTrackThis’: None}, ’side2’: {’target’: ’C4’, ’id’: ’side2’, ’\nremoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: None}, ’side3’: {’\ntarget’: ’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None, ’\nlocalBranchesThatTrackThis’: None}}, ’commits’: {’C0’: {’parents’: [], ’id’: ’C0’,\n’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C2’: {’parents’: [’C1’\n], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C4’: {’parents’: [’C3’], ’\nid’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’: {’parents’: [’C5’], ’id’:\n’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’\n}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C10’: {’parents’: [’C4’, ’C9’], ’\nid’: ’C10’}, ’C11’: {’parents’: [’C10’, ’C7’], ’id’: ’C11’}}, ’HEAD’: {’target’: ’\nmain’, ’id’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’target’: ’C11’, ’id’: ’\nmain’, ’remoteTrackingBranchID’: None, ’localBranchesThatTrackThis’: None}}, ’\ncommits’: {’C0’: {’parents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’:\n[’C0’], ’id’: ’C1’}, ’C8’: {’parents’: [’C1’], ’id’: ’C8’}, ’C5’: {’parents’: [’C1\n’], ’id’: ’C5’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’C2’: {’parents’: [’C1’], ’\nid’: ’C2’}, ’C6’: {’parents’: [’C5’], ’id’: ’C6’}, ’C4’: {’parents’: [’C3’], ’id’:\n’C4’}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C7’: {’parents’: [’C6’], ’id’:\n’C7’}, ’C10’: {’parents’: [’C4’, ’C9’], ’id’: ’C10’}, ’C11’: {’parents’: [’C10’, ’\nC7’], ’id’: ’C11’}}, ’HEAD’: {’target’: ’main’, ’id’: ’HEAD’}}}.\nThe current git tree is:\n43\nCapaBench\nTECHNICAL REPORT\n{’branches’: {’main’: {’target’: ’C1’, ’id’: ’main’, ’remoteTrackingBranchID’: ’o\/main\n’}, ’o\/main’: {’target’: ’C8’, ’id’: ’o\/main’, ’remoteTrackingBranchID’: None}, ’\nside1’: {’target’: ’C9’, ’id’: ’side1’, ’remoteTrackingBranchID’: None}, ’side2’: {\n’target’: ’C10’, ’id’: ’side2’, ’remoteTrackingBranchID’: None}, ’side3’: {’target’\n: ’C7’, ’id’: ’side3’, ’remoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’\nparents’: [], ’id’: ’C0’, ’rootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’\n}, ’C2’: {’parents’: [’C1’], ’id’: ’C2’}, ’C3’: {’parents’: [’C1’], ’id’: ’C3’}, ’\nC4’: {’parents’: [’C3’], ’id’: ’C4’}, ’C5’: {’parents’: [’C1’], ’id’: ’C5’}, ’C6’:\n{’parents’: [’C5’], ’id’: ’C6’}, ’C7’: {’parents’: [’C6’], ’id’: ’C7’}, ’C8’: {’\nparents’: [’C1’], ’id’: ’C8’}, ’C9’: {’parents’: [’C2’, ’C8’], ’id’: ’C9’}, ’C10’:\n{’parents’: [’C4’, ’C9’], ’id’: ’C10’}}, ’tags’: {}, ’HEAD’: {’target’: ’side2’, ’\nid’: ’HEAD’}, ’originTree’: {’branches’: {’main’: {’target’: ’C8’, ’id’: ’main’, ’\nremoteTrackingBranchID’: None}}, ’commits’: {’C0’: {’parents’: [], ’id’: ’C0’, ’\nrootCommit’: True}, ’C1’: {’parents’: [’C0’], ’id’: ’C1’}, ’C8’: {’parents’: [’C1’\n], ’id’: ’C8’}}, ’tags’: {}, ’HEAD’: {’target’: ’main’, ’id’: ’HEAD’}}}.\n[Hisroty Interaction Information]\nThe history interaction information is: [’git fetch origin’, ’git checkout side1’, ’\ngit merge o\/main’, ’git checkout side2’, ’git merge side1’, ’git checkout side3’, ’\ngit merge side2’].\nHowever, in your last two interactions in the env, your proposed git command doesn’t\nchange the state of the git tree. This means that your past two interactions does\nnot contribute to the efficient completion of the git task. You need to reflect on\nthe past two interactions and consider the following possible reasons:\n1. You proposed wrong git command that failed to execute in the env.\n2. Your proposed git command is too complex. This env is just a simple git sandbox,\nyou don’t need to use complex git commands.\n3. You are obsessed with using some command like ’git log’ to get more information,\nbut it’s not necessary in this env because the current state of the git tree has\nalready provided all necessary information.\n4. Other reasons.\nNo matter which case, you need to reflect on the recent interaction history and\nconsider the following points:\n1. Identify why the current result is unsatisfactory.\n2. Evaluate the effectiveness of past actions and thoughts. Were there missed signals\nor incorrect assumptions?\n3. Propose improvements for the next steps.\n4. Consider the overall goal of completing the git task. How can future actions better\nalign with this objective?\nAfter your thinking, you should output your reflection like:\n‘‘‘reflection\nPut your reflection here\nRobot Cooperation.\nG.1\nDataset Details\nThe Robot Cooperation dataset evaluates agents’ planning, reasoning, action, and reflection capabilities in multi-robot\ncollaboration tasks. The dataset includes 100 tasks, designed to benchmark performance in robot planning scenarios.\nFramework and Dataset Construction. The dataset is built upon the RoCoBench environment framework [Mandi\net al., 2023], which provides an environment simulator and reward mechanisms for multi-robot collaboration tasks. We\nextended the original task set by introducing sequential constraints and leveraging random seed variations to generate\ndiverse task instances.\n• Task Extension: Sequential constraints were added to existing tasks, making them more complex. Examples\ninclude:\n– Sweep Floor Task: Added order constraints. In the Sweep RGB task, robots must first sweep the Red\nCube into the dustpan and dump it into the bin, followed by the Green Cube, and finally the Blue Cube.\n44\nCapaBench\nTECHNICAL REPORT\n– Arrange Cabinet Task: Introduced sequential object retrieval. In the CabinetCup task, robots must first\nplace the Cup on the Cup Coaster, followed by placing the Mug on the Mug Coaster.\n– Sandwich Task: Expanded with additional recipes requiring more planning steps.\n• Task Instances: Random seed variations in the RoCoBench environment were used to create different initial\nstates, generating 100 unique task instances. Each instance was manually verified to ensure it has a correct\nsolution, ensuring robustness and reliability for model evaluation.\nReward Mechanism Improvements. To better evaluate model capabilities, we proposed new reward methods tailored\nto the characteristics of the extended tasks:\n• Tasks were divided into smaller sub-tasks with rewards granted for completing each sub-task in sequence.\n• For example, in the Sweep RGB task, rewards are distributed as 1\n3 for successfully completing each step (e.g.,\nsweeping the Red Cube, Green Cube, and Blue Cube in order). This approach incentivizes correct sequencing\nand provides granular feedback on agent performance.\n• These new reward methods ensure even smaller models can effectively receive feedback, improving evaluation\nsensitivity.\nModel Differentiation Enhancements. To further enhance the differentiation capability of the models, we adopt a\nmethod where multiple actions are proposed within a single interaction. This approach, combined with a constraint\non the number of timesteps, improves the differentiation among models. By allowing the agent to plan and propose\nmultiple actions at once, we can better assess the agent’s planning and reasoning abilities. The constraint on timesteps\nensures that the agent must efficiently utilize its planning capabilities within a restricted timeframe, thereby providing a\nclearer distinction between the performance of different models.\nG.2\nExperiment Details\nTable 12: Experimental Results on Robot Cooperation(100 tasks)\nLLM\nPt\nRt\nAt\nFt\nReward (%)\n∆Reward (%)\nLlama3-8B-instruct(Default)\n-\n-\n-\n-\n8.85\n-\nclaude-3.5-sonnet\n0.1140\n0.3879\n0.3186\n0.0172\n92.63\n+83.78\ngpt-4o-mini\n0.0748\n0.1888\n0.1957\n-0.0034\n54.43\n+45.58\nglm-4-airx\n-0.0235\n0.1157\n0.0078\n-0.0124\n17.60\n+8.75\ngpt-4-turbo-0409\n0.1069\n0.3292\n0.3162\n0.0011\n84.18\n+75.33\nqwen2.5-32b-ins\n0.0895\n0.2683\n0.2768\n0.0029\n72.59\n+63.74\nMistral-7B-Instruct\n-0.0142\n-0.0001\n-0.0211\n-0.0014\n5.17\n-3.68\nLlama-3-70B-Instruct\n0.0426\n0.1524\n0.1750\n-0.0078\n45.06\n+36.21\ndoubao-pro-4k\n0.0208\n-0.004\n0.2043\n-0.0122\n29.75\n+20.90\nMistral-8X7B-instruct\n-0.0049\n0.0329\n0.0521\n0.0040\n17.27\n+8.42\nbest\n\/\n\/\n\/\n\/\n92.63\n+83.78\nTable 12 summarizes the experimental results for the Robot Cooperation task, including Shapley values for the four\nmodules (Planning (Pt), Reasoning (Rt), Action (At), and Reflection (Ft)), as well as Rewards (Reward (%)) and\ntheir improvement (∆Reward (%)) relative to the baseline (Llama3-8B-instruct).\nThe baseline model achieves a reward of 8.85%. Claude-3.5-sonnet and gpt-4-turbo-0409 achieve the\nhighest rewards, 92.63% and 84.18%, improving by +83.78% and +75.33%, respectively. The reward range, from\n5.17% (Mistral-7B-Instruct) to 92.63%, highlights the dataset’s strong ability to differentiate models.\nThe dataset emphasizes Reasoning and Action capabilities, as reflected by high Rt and At Shapley values for\ntop-performing models (claude-3.5-sonnet, gpt-4-turbo-0409, and qwen2.5-32b-instruct). Its\nstructured design rewards precise reasoning and efficient execution.\nG.3\nPrompt Example\nG.3.1\nPlanning Module\n45\nCapaBench\nTECHNICAL REPORT\nAlice is a robot holding a dustpan, Bob is a robot holding a broom, together they must\nfirstly sweep up the blue cube and dump it into the trash bin, secondly sweep up\nthe green cube and dump into the trash bin, thirdly sweep up the red cube and dump\ninto the trash bin, the order can not be disrupted.\nTo sweep up a cube, Alice must place the dustpan to one side, while Bob must sweep the\ncube from the other side into the dustpan.\nAt each round, given ’Scene description’ and ’Environment feedback’, use it to reason\nabout the task, and improve any previous plans. Each robot does **exactly** one\naction per round.\n[Action Options]\n1) MOVE <target>, <target> can only be a cube.\n2) SWEEP <target>, this moves the groom so it pushes the <target> into dustpan, only\nBob can SWEEP, Alice must WAIT in front of the same <target> cube when Bob SWEEP.\n3) WAIT, stays at the current spot.\n4) DUMP, only when there are one or more cubes in the dustpan, Alice can DUMP it into\ntrash_bin.\nOnly SWEEP a cube after both robots MOVEed to the cube.\n[Scene description]\nred_cube is at (0.8, 0.4, 0.2), on the table;\ngreen_cube is at (0.6, 0.6, 0.2), on the table;\nblue_cube is at (1.1, 0.4, 0.2), on the table;\nAlice’s gripper is at (0.3, -0.1, 0.5), holding dustpan, in front of red_cube with\ndistance: 0.62, in front of green_cube with distance: 0.59, in front of blue_cube\nwith distance: 0.89\nBob’s gripper is at (1.2, 0.8, 0.8), holding broom, in front of red_cube with distance\n: 0.59, in front of green_cube with distance: 0.63, in front of blue_cube with\ndistance: 0.47\nYou are an advanced intelligent system responsible for guiding multiple robots to\ncollaborate on tasks in a desktop operating environment. Based on the instructions\nprovided, outline a concise and clear strategic plan that divides the task into\nsubtasks. Your plan should be detailed and actionalble, thus guiding the robots\nthrough the decision-making process and helping them to complete the entire task\nefficiently.\nAfter your thinking, you should output your plan like\n‘‘‘plan\nPut your plan here\n‘‘‘\nYour thinking and your plan are:\nG.3.2\nReasoning Module Prompt\nAlice is a robot holding a dustpan, Bob is a robot holding a broom, together they must\nfirstly sweep up the blue cube and dump it into the trash bin, secondly sweep up\nthe green cube and dump into the trash bin, thirdly sweep up the red cube and dump\ninto the trash bin, the order can not be disrupted.\nTo sweep up a cube, Alice must place the dustpan to one side, while Bob must sweep the\ncube from the other side into the dustpan.\nAt each round, given ’Scene description’ and ’Environment feedback’, use it to reason\nabout the task, and improve any previous plans. Each robot does **exactly** one\naction per round.\n[Action Options]\n1) MOVE <target>, <target> can only be a cube.\n2) SWEEP <target>, this moves the groom so it pushes the <target> into dustpan, only\nBob can SWEEP, Alice must WAIT in front of the same <target> cube when Bob SWEEP.\n3) WAIT, stays at the current spot.\n46\nCapaBench\nTECHNICAL REPORT\n4) DUMP, only when there are one or more cubes in the dustpan, Alice can DUMP it into\ntrash_bin.\nOnly SWEEP a cube after both robots MOVEed to the cube.\n[Scene description]\nred_cube is at (0.8, 0.4, 0.2), on the table;\ngreen_cube is at (0.6, 0.6, 0.2), on the table;\nblue_cube is at (1.1, 0.4, 0.2), on the table;\nAlice’s gripper is at (0.3, -0.1, 0.5), holding dustpan, in front of red_cube with\ndistance: 0.62, in front of green_cube with distance: 0.59, in front of blue_cube\nwith distance: 0.89\nBob’s gripper is at (1.2, 0.8, 0.8), holding broom, in front of red_cube with distance\n: 0.59, in front of green_cube with distance: 0.63, in front of blue_cube with\ndistance: 0.47\nAn abstract plan on how to complete the task is Round 1:\nAlice: MOVE to blue_cube\nBob: WAIT\nRound 2:\nAlice: WAIT\nBob: MOVE to blue_cube\nAlice: SWEEP blue_cube\nBob: WAIT\nRound 3:\nAlice: WAIT\nBob: SWEEP blue_cube\nAlice: DUMP\nBob: WAIT\nRound 4:\nAlice: MOVE to green_cube\nBob: WAIT\nRound 5:\nAlice: WAIT\nBob: MOVE to green_cube\nAlice: SWEEP green_cube\nBob: WAIT\nRound 6:\nAlice: WAIT\nBob: SWEEP green_cube\nAlice: DUMP\nBob: WAIT\nRound 7:\nAlice: MOVE to red_cube\nBob: WAIT\nRound 8:\nAlice: WAIT\nBob: MOVE to red_cube\nAlice: SWEEP red_cube\nBob: WAIT\nRound 9:\nAlice: WAIT\nBob: SWEEP red_cube\nAlice: DUMP\nThe abstract plan on how to complete the task is a guide to help you analyze the task\nand complete it efficiently. Based on the action history and the observation of the\nenv, think about which subtask the robots are processing currently. To complete\n47\nCapaBench\nTECHNICAL REPORT\nthe task efficiently, you are encouraged to reason what actions to do in the\ncurrent step and several future steps. You are supposed to imagine env state after\neach step to help you make decision on next step, thus forms a step-by-step\nreasoning form. (Specifically, how futher you need to plan depends on your\nconfidence, but you are encounraged to reason futher). Remember in each step, each\nrobot can only take one action!!!\nAfter your reasoning about the task, you should summarize your reasoning and output\nthe summary result like\n‘‘‘reasoning\nPut your reasoning summary here\n‘‘‘\nYour thinking and your reasoning are:\nG.3.3\nAction Module Prompt\nAlice is a robot holding a dustpan, Bob is a robot holding a broom, together they must\nfirstly sweep up the blue cube and dump it into the trash bin, secondly sweep up\nthe green cube and dump into the trash bin, thirdly sweep up the red cube and dump\ninto the trash bin, the order can not be disrupted.\nTo sweep up a cube, Alice must place the dustpan to one side, while Bob must sweep the\ncube from the other side into the dustpan.\nAt each round, given ’Scene description’ and ’Environment feedback’, use it to reason\nabout the task, and improve any previous plans. Each robot does **exactly** one\naction per round.\n[Action Options]\n1) MOVE <target>, <target> can only be a cube.\n2) SWEEP <target>, this moves the groom so it pushes the <target> into dustpan, only\nBob can SWEEP, Alice must WAIT in front of the same <target> cube when Bob SWEEP.\n3) WAIT, stays at the current spot.\n4) DUMP, only when there are one or more cubes in the dustpan, Alice can DUMP it into\ntrash_bin.\nOnly SWEEP a cube after both robots MOVEed to the cube.\n[Action Output Instruction]\nMust first output ’EXECUTE\\n’, then give exactly one action per robot, put each on a\nnew line.\nExample#1: ’EXECUTE\\nNAME: Alice ACTION: MOVE red_cube\\nNAME: Bob ACTION: MOVE\nred_cube\\n’\nExample#2: ’EXECUTE\\nNAME: Alice ACTION: WAIT\\nNAME: Bob ACTION: SWEEP red_cube\\n’\nExample#3: ’EXECUTE\\nNAME: Alice ACTION: DUMP\\nNAME: Bob ACTION: MOVE green_cube\\n’\nIf actions for multiple steps are to generate, repeat the above process and follow the\nformat strictly.\nExample#4: ’EXECUTE\\nNAME: Alice ACTION: MOVE blue_cube\\nNAME: Bob ACTION: WAIT\\\nnEXECUTE\\nNAME: Alice ACTION: WAIT\\nNAME: Bob ACTION: SWEEP blue_cube\\n’\n[Scene description]\nred_cube is at (0.8, 0.4, 0.2), on the table;\ngreen_cube is at (0.6, 0.6, 0.2), on the table;\nblue_cube is at (1.1, 0.4, 0.2), on the table;\nAlice’s gripper is at (0.3, -0.1, 0.5), holding dustpan, in front of red_cube with\ndistance: 0.62, in front of green_cube with distance: 0.59, in front of blue_cube\nwith distance: 0.89\nBob’s gripper is at (1.2, 0.8, 0.8), holding broom, in front of red_cube with distance\n: 0.59, in front of green_cube with distance: 0.63, in front of blue_cube with\ndistance: 0.47\nYour current reasoning is\nreasoning\nAlice and Bob are currently processing the first subtask, which is to sweep up the\nblue cube and dump it into the trash bin. In the current state, Alice is in front\n48\nCapaBench\nTECHNICAL REPORT\nof the blue cube, and Bob is in front of the red cube. Alice should MOVE to the\nblue cube, and Bob should WAIT.\nYou need to propose a specific plan of one or more steps of actions for each robot\nfaithfully based on reasoning thought. Write and output the plan strictly in the\nformat of [Action Output Instruction].\nRemeber that in each step, each robot can only perform one action, if the reasoning\nthought is about several steps, you should create a new step action plan following\nthe action output intrstruction for each step.\nYour action is:\nG.3.4\nReflection Module Prompt\nAlice is a robot holding a dustpan, Bob is a robot holding a broom, together they must\nsweep up all the cubes on the table.\nTo sweep up a cube, Alice must MOVE to the cube on the one side, while Bob must MOVE\nto the cube from the other side. After that, Bob Sweep the cube into the dustpan.\nAfter all, Alice should DUMP everything in the dustpan.\nAt each round, given ’Scene description’ and ’Environment feedback’, use it to reason\nabout the task, and improve any previous plans. Each robot does **exactly** one\naction per round.\n[Action Options]\n1) MOVE <target>, <target> can only be a cube.\n2) SWEEP <target>, this moves the groom so it pushes the <target> into dustpan, only\nBob can SWEEP, Alice must WAIT in front of the same <target> cube when Bob SWEEP.\nRemember MOVE is the necessary step before SWEEP.\n3) WAIT, stays at the current spot.\n4) DUMP, only when there are one or more cubes in the dustpan, Alice can DUMP it into\ntrash_bin.\nOnly SWEEP a cube after both robots MOVEed to the cube.\n[Action Output Instruction]\nMust first output ’EXECUTE\n’, then give exactly one action per robot, put each on a new line.\nExample#1: ’EXECUTE\nNAME: Alice ACTION: MOVE red_cube\nNAME: Bob ACTION: MOVE red_cube\n’\nExample#2: ’EXECUTE\nNAME: Alice ACTION: WAIT\nNAME: Bob ACTION: SWEEP red_cube\n’\nExample#3: ’EXECUTE\nNAME: Alice ACTION: DUMP\nNAME: Bob ACTION: MOVE green_cube\n’\nIf actions for multiple steps are to generate, repeat the above process and follow the\nformat strictly.\nExample#4: ’EXECUTE\nNAME: Alice ACTION: MOVE blue_cube\nNAME: Bob ACTION: WAIT\nEXECUTE\nNAME: Alice ACTION: WAIT\nNAME: Bob ACTION: SWEEP blue_cube\n’\n[History]\n== Round#0 ==\n[Executed Action]\nAlice: MOVE green_cube\nBob: MOVE blue_cube\n49\nCapaBench\nTECHNICAL REPORT\n== Round#1 ==\n[Executed Action]\nAlice: WAIT\nBob: MOVE red_cube\n== Round#2 ==\n[Executed Action]\nAlice: MOVE red_cube\nBob: MOVE red_cube\n== Round#3 ==\n[Executed Action]\nAlice: WAIT\nBob: MOVE red_cube\n== Round#4 ==\n[Executed Action]\nBob: MOVE red_cube\nAlice: MOVE red_cube\n== Round#5 ==\n[Executed Action]\nAlice: WAIT\nBob: SWEEP red_cub\n== Round#6 ==\n[Executed Action]\nAlice: MOVE green_cube\nBob: MOVE green_cube\n== Round#7 ==\n[Executed Action]\nAlice: WAIT\nBob: WAIT\n== Round#8 ==\n[Executed Action]\nAlice: SWEEP green_cube\nBob: WAIT\n== Round#9 ==\n[Executed Action]\nBob: MOVE blue_cube\nAlice: MOVE blue_cube\n== Round#10 ==\n[Executed Action]\nAlice: WAIT\nBob: WAIT\n== Current Round ==\n[Scene description]\nred_cube is at (0.3, 0.4, 0.4), inside dustpan;\ngreen_cube is at (0.8, 0.5, 0.2), on the table;\nblue_cube is at (0.3, 0.5, 0.2), on the table;\nAlice’s gripper is at (0.4, 0.3, 0.4), holding dustpan, in front of green_cube with\ndistance: 0.53, in front of blue_cube with distance: 0.31\nBob’s gripper is at (0.3, 0.8, 0.6), holding broom, in front of green_cube with\ndistance: 0.58, in front of blue_cube with distance: 0.33\nBased on the current state, in order to complete task, someone proposed EXECUTE\nNAME: Alice ACTION: SWEEP blue_cube\nNAME: Bob ACTION: WAIT as action for the next step.\nHowever, this action failed to employ in the env. Usually, the failure may be due to\none of the following reasons.\n50\nCapaBench\nTECHNICAL REPORT\n1.Response does not contain some keyword. The keywords in this env includes [’NAME:’,\n’ACTION:’].\n2.Response missing plan for some robot. The robot in this env includes dict_values([’\nAlice’, ’Bob’]).\n3.Reponse must contain exactly one ACTION for each robot, and must contain all\nkeywords. The keywords in this env includes [’NAME:’, ’ACTION:’].\n4.Bad action for some robot, this robot at current state can only MOVE or WAIT. The\nrobot in this env includes dict_values([’Alice’, ’Bob’]).\n5.Planned PATH must have exact same number of steps of all agents.\nYou should think about which reason is most possible for the failure of the past\naction, you should clearly output the reason to help yourself to genetate better\nreasoning and action in future.\nAfter your thinking, you should output your reflection like\n‘‘‘reflection\nPut your reflection here\n‘‘‘\nYour thinking and reflection are:\n51\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents.pdf"}
{"title":"Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research","authors":"Shuxin Zhuang, Shuxin Li, Tianji Yang, Muheng Li, Xianjie Shi, Bo An, Youzhi Zhang","summary":"After the great achievement of solving two-player zero-sum games, more and\nmore AI researchers focus on solving multiplayer games. To facilitate the\ndevelopment of designing efficient learning algorithms for solving multiplayer\ngames, we propose a multiplayer game platform for solving Urban Network\nSecurity Games (\\textbf{UNSG}) that model real-world scenarios. That is,\npreventing criminal activity is a highly significant responsibility assigned to\npolice officers in cities, and police officers have to allocate their limited\nsecurity resources to interdict the escaping criminal when a crime takes place\nin a city. This interaction between multiple police officers and the escaping\ncriminal can be modeled as a UNSG. The variants of UNSGs can model different\nreal-world settings, e.g., whether real-time information is available or not,\nand whether police officers can communicate or not. The main challenges of\nsolving this game include the large size of the game and the co-existence of\ncooperation and competition. While previous efforts have been made to tackle\nUNSGs, they have been hampered by performance and scalability issues.\nTherefore, we propose an open-source UNSG platform (\\textbf{GraphChase}) for\ndesigning efficient learning algorithms for solving UNSGs. Specifically,\nGraphChase offers a unified and flexible game environment for modeling various\nvariants of UNSGs, supporting the development, testing, and benchmarking of\nalgorithms. We believe that GraphChase not only facilitates the development of\nefficient algorithms for solving real-world problems but also paves the way for\nsignificant advancements in algorithmic development for solving general\nmultiplayer games.","url":"http:\/\/arxiv.org\/abs\/2501.17559v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2501.17559v1","published":1738147617000,"comment":null,"pdf_text":"SOLVING\nURBAN\nNETWORK\nSECURITY\nGAMES:\nLEARNING PLATFORM, BENCHMARK,\nAND CHAL-\nLENGE FOR AI RESEARCH\nShuxin Zhuang\nCity University of Hong Kong\nCAIR, Hong Kong Institute of Science & Innovation\nshuxin.zhuang@my.cityu.edu.hk\nShuxin Li\nNanyang Technological University\nshuxin.li@ntu.edu.sg\nTianji Yang\nGeorgia Institute of Technology\ntyang425@gatech.edu\nMuheng Li\nUniversity of Toronto\nmuheng.li@mail.utoronto.ca\nXianjie Shi\nThe University of Hong Kong\nxianjieshi@connect.hku.hk\nBo An\nNanyang Technological University\nboan@ntu.edu.sg\nYouzhi Zhang∗\nCAIR, Hong Kong Institute of Science & Innovation\nyouzhi.zhang@cair.cas.org.hk\nABSTRACT\nAfter the great achievement of solving two-player zero-sum games, more and\nmore AI researchers focus on solving multiplayer games. To facilitate the develop-\nment of designing efficient learning algorithms for solving multiplayer games, we\npropose a multiplayer game platform for solving Urban Network Security Games\n(UNSG) that model real-world scenarios. That is, preventing criminal activity is\na highly significant responsibility assigned to police officers in cities, and police\nofficers have to allocate their limited security resources to interdict the escaping\ncriminal when a crime takes place in a city. This interaction between multiple po-\nlice officers and the escaping criminal can be modeled as a UNSG. The variants of\nUNSGs can model different real-world settings, e.g., whether real-time informa-\ntion is available or not, and whether police officers can communicate or not. The\nmain challenges of solving this game include the large size of the game and the co-\nexistence of cooperation and competition. While previous efforts have been made\nto tackle UNSGs, they have been hampered by performance and scalability issues.\nTherefore, we propose an open-source UNSG platform (GraphChase) for design-\ning efficient learning algorithms for solving UNSGs. Specifically, GraphChase\noffers a unified and flexible game environment for modeling various variants of\nUNSGs, supporting the development, testing, and benchmarking of algorithms.\nWe believe that GraphChase not only facilitates the development of efficient al-\ngorithms for solving real-world problems but also paves the way for significant\nadvancements in algorithmic development for solving general multiplayer games.\n1\nINTRODUCTION\nIn the field of AI research, a lot of focus has been placed on computing a Nash equilibrium (Nash,\n1951; Shoham & Leyton-Brown, 2008) in two-player zero-sum extensive-form games, where both\n∗Corresponding author: Youzhi Zhang (youzhi.zhang@cair.cas.org.hk).\n1\narXiv:2501.17559v1  [cs.AI]  29 Jan 2025\nFigure 1: The blueprint of our GraphChase platform.\nplayers receive opposing payoffs (Zinkevich et al., 2008; Moravˇc´ık et al., 2017; Brown & Sand-\nholm, 2018). In this scenario, a Nash equilibrium can be computed in polynomial time based on\nthe size of the extensive-form game (Shoham & Leyton-Brown, 2008). Recent significant achieve-\nments, such as achieving superhuman performance in the heads-up no-limit Texas hold’em poker\ngame (Moravˇc´ık et al., 2017; Brown & Sandholm, 2018), demonstrate that researchers have a good\nunderstanding of the problem of computing a Nash equilibrium in two-player zero-sum extensive-\nform games, both in theory and in practice. However, the problem of computing a Nash equilibrium\nin multiplayer games is not as well understood, as it is generally a challenging task (Chen & Deng,\n2005; Zhang et al., 2023b). Therefore, more and more AI researchers focus on solving multiplayer\ngames (Brown & Sandholm, 2019; FAIR et al., 2022; Carminati et al., 2022; Zhang et al., 2023a;\nMcAleer et al., 2023; Zhang et al., 2024)\nTo facilitate the development of designing efficient learning algorithms for solving multiplayer\ngames, we propose a multiplayer game platform for solving Urban Network Security Games\n(UNSGs) that model the following real-world situations. In urban areas, ensuring public safety\nand security is crucial for law enforcement agencies. One significant challenge they face is the high\nnumber of innocent bystanders who are injured or killed during police pursuits (Rivara & Mack,\n2004). It’s essential to develop effective strategies that allow multiple officers to apprehend fleeing\ncriminals while minimizing risks to civilians and property damage. This paper focuses on respond-\ning to major incidents such as terrorist attacks or bank robberies, where police officers need to\nswiftly intercept the attackers during their escape. This requires efficient strategies for apprehending\nfleeing criminals, which can be analyzed and developed using structured approaches like UNSGs.\nHowever, solving UNSGs is NP-hard (Jain et al., 2011; Zhang et al., 2017; 2019). More specifi-\ncally, the strategy space of players in UNSGs cannot be enumerated due to the memory constraint of\ncomputers (Jain et al., 2011; Zhang et al., 2019). Moreover, if players do not have real-time informa-\ntion, they have to make decisions with imperfect information. In addition, if police officers cannot\ncommunicate during the game play, they have to make decisions independently. Finally, UNSGs\nincorporate cooperation between police officers and competition between the criminal and team of\npolice officers. To address the above challenges, previous efforts have been made to tackle UNSGs.\nThat is, they extended the Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008) to\nCFR-MIX algorithm (Li et al., 2021), incorporating deep learning enhancements from Deep CFR\n(Brown et al., 2019). Additionally, they utilized the Neural Fictitious Self-Play (NFSP) approach\n(Heinrich & Silver, 2016), further developed into NSG-NFSP (Xue et al., 2021) and NSGZero (Xue\net al., 2022), which are tailored to solving UNSGs under the NFSP framework. Moreover, they ex-\ntended the learning framework, Policy-Space Response Oracles (PSRO) (Lanctot et al., 2017), to an\nadvanced variant Pretrained PSRO (Li et al., 2023a) to speed up. Finally, they developed Grasper (Li\net al., 2024) based on Pretrained PSRO, an innovative algorithm that can generalize across different\ngame settings. All of them are based on the state-of-the-art game-theoretical algorithm frameworks.\n2\nHowever, these efforts are still hampered by performance and scalability issues, as shown in our\nexperiments.\nTo foster the development of scalable algorithms capable of addressing city-scale UNSGs, we pro-\npose the creation of an open-source platform, GraphChase, specifically tailored for UNSG. The\narchitecture of GraphChase is depicted in Figure 1, designed to provide researchers with a com-\nprehensive UNSG platform and facilitate the development and evaluation of scalable strategy for\npursuers. Specifically, we made the following contributions: i) Development of a unified and flex-\nible UNSG environment: We developed a versatile platform designed to support various configu-\nrations of UNSGs. Specifically, this environment allows for modifying game parameters, enabling\nresearchers to simulate different real-world UNSG scenarios under various conditions. The inherent\nflexibility of GraphChase supports a wide range of experimental setups, from small-scale laboratory\nexperiments to city-wide simulations. All these make GraphChase a suitable tool for theoretical re-\nsearch and practical application testing. ii) Implementation of learning algorithms: GraphChase\nis designed to facilitate the execution of a wide range of algorithms. Based on the standardized\nplatform, we successfully implement several advanced deep learning-based algorithms, enabling\nthe consistent comparison of different strategic approaches. By efficiently integrating algorithms\nwithin the platform, it reduces the time overhead of the simulation resulting in faster convergence\nfrom the perspective of wall-clock time. And iii) Benchmark results: We conduct experiments on\nUNSGs with synthetic and real-world graphs to evaluate the performance of the different algorithms\nimplemented on the GraphChase platform. The results from these experiments are recorded and\ncompiled into comprehensive benchmarks. Our results show that, although previous algorithms can\nachieve reasonable performance, they still suffer performance and scalability issues in real-world\nsettings. These results suggest that substantial efforts are still required to develop effective and ef-\nficient algorithms for solving real-world UNSGs. We believe that GraphChase not only facilitates\nthe development of efficient algorithms for solving real-world problems but also paves the way for\nsignificant advancements in algorithmic development for solving general multiplayer games.\n2\nURBAN NETWORK SECURITY GAMES\nMotivated by the security games on urban roads (Jain et al., 2011; Zhang et al., 2017; 2019), we\nproposed our GraphChase platform for solving UNSGs that model the interactions between multiple\npursuers (police officers) and an evader (criminal). The variants of UNSGs can model different real-\nworld settings, e.g., whether real-time information is available, whether pursuers can communicate.\nNow, we introduce the definition of these games.\n2.1\nGAME DEFINITION\nTake, for instance, the scenario where pursuers are tasked with capturing an evader escaping on\nurban roads. We introduce the concept of UNSGs. First, urban roads and pathways naturally lend\nthemselves to being modeled as graphs, where intersections and streets form nodes and edges, re-\nspectively. The graph can be represented by G = (V, E), where V is a set of vertices, and E is a\nset of edges. In UNSGs, graphs can be directed or undirected, corresponding to one-way streets and\ntwo-way streets, and weighted or unweighted, where the weight can be used to reflect different travel\ncosts or terrains. This graphical representation allows for a structured and systematic approach to\nsimulating the complex dynamics of urban pursuits. Specifically, in graph G, we use a subset of the\nvertex set, Eexit ⊂E, to represent the set of exit nodes from which the criminal can escape. For\neach vertex v ∈V , we use N(v) to represent the set of neighbours of v.\nIn UNSGs, the pursuer and the evader are represented as agents moving across a network.\nIt\nis important to note that the evader and the pursuer can be a single agent or consist of multi-\nple agents. For example, several pursuers would collaborate to chase a single evader or chase a\nteam of evaders. Formally, the set of players N = (p, e), where p = (p1, p2, ..., pn), n ≥1\nrepresents pursuers and e = (e1, e2, ..., en), n ≥1 represents the evader. Since the pursuers can\nblock all exit nodes for a certain period, we can predefine the length of the lockdown. Formally,\nlet T represent the number of steps in which the game terminates and lp\n0 = (lp1\n0 , lp2\n0 , ..., lpn\n0 ),\nle\n0 = (le1\n0 , le2\n0 , ..., len\n0 ) represent the initial locations of the evader and the pursuer, respectively.\nAt each step, each player in the game would move from vertex v to one of its neighborhood\nvertices w ∈N(v). Specifically, at game step t < T, the locations of the evader and the pur-\nsuer, respectively, are lp\nt = (lp1\nt , lp2\nt , ..., lpn\nt ), le\nt = (le1\nt , le2\nt , ..., len\nt ). Then the available action\n3\nset of the pursuer is a Cartesian product of the sets of neighboring vertices of each evader, i.e.,\nAp(h) = {(lp1, lp2, ..., lpn)|li ∈N(li\nt), ∀i ∈{p1, p2, ..., pn}}. Similarly, the available action set of\nthe evader is Ae(h) = {(le1, le2, ..., len)|li ∈N(li\nt), ∀i ∈{e1, e2, ..., en}}. All players act simulta-\nneously at game step t, i.e., the pursuer and the evader select actions from their action sets. Then all\nplayers move from lp\nt and le\nt to lp\nt+1 and le\nt+1, respectively. We can also convert the simultaneous-\nmove game into a turn-based game by ignoring the selected action of the first-act player when the\nsecond player acts. This process repeats until a termination condition is met. The evader is con-\nsidered caught if the evader and any of the pursuers occupy the same point at any time within the\nmaximum time horizon. The termination conditions of the game include: (i) the pursuer catches\nthe evader (i.e., all criminals); (ii) the evader (i.e., all criminals) escapes from exit nodes; and (iii)\nthe game reaches the predefined game step T, i.e., t = T. In cases (i) and (iii) the pursuer wins.\nOtherwise, if the evader successfully escapes to the outside world, the evader wins. Based on these\nresults, each player gets their corresponding rewards.\n2.2\nINFORMATION AND STRATEGY\nIn different real-world cases, the pursuer and evader may access various information, i.e., the loca-\ntion information of each player. With the aid of tracking devices, such as the StarChase GPS-based\nsystem (Gaither et al., 2017), police officers can get the real-time location of the criminal. In another\ncase, the police officers may not know the ability of the criminal. To avoid the worst case, the police\nofficers usually assume that the criminal can get the real-time location of the police officers. There-\nfore, there are four cases: i) the evader can get the real-time location information of the pursuer\nwhile the pursuer cannot get the real-time location information of the evader; ii) the pursuer can get\nthe real-time location information of the evader while the evader cannot get the real-time location\ninformation of the pursuer; iii) both the evader and the pursuer can get the real-time location infor-\nmation of the opponent; and iv) both the evader and the pursuer cannot get the real-time location\ninformation.\nMoreover, if pursuers cannot communicate during the game play, they have to make decisions in-\ndependently. However, if pursuers can communicate during the game play, they can correlate their\nactions. Using this case as an example, based on the available real-time location information, the\nbehavior strategy σe or σp is a function that maps every decision point to a probability distribution\nover the available action set. Then, a strategy profile σ is a tuple of one strategy for each player, i.e.,\nσ = (σp, σe). The pursuer’s payoff function is up(σp, σe) ∈R with up(σp, σe) = −ue(σp, σe) for\nthe evader. We adopt the Nash equilibrium (NE) (Nash, 1950) as the solution concept for this case\nsince the NE strategy profile is a steady state in which no player can increase its utility by unilater-\nally deviating. In our GraphChase platform, we consider the NE strategy of the pursuer would be\nthe optimal strategy and take the worst-case utility of the pursuer as the measure for the pursuer’s\nstrategy, i.e., maxσp∈Σp minσe∈Σe up(σp, σe).\n2.3\nCHALLENGES\nIn UNSGs, pursuers are tasked with capturing an evader escaping on urban roads. The network-\nbased environment could lead to the strategy space of players in UNSGs cannot be enumerated due\nto the memory constraint of computers (Jain et al., 2011; Zhang et al., 2019). That is, if a player’s\nstrategy is a path, then we cannot enumerate all paths due to memory constraints in large-scale\nUNSGs. In fact, even with the relatively simple setting where the time dynamics are ignored, and\nthe pursuers can correlate their actions, the problem of solving UNSGs is still very hard (Jain et al.,\n2011). We could expect that solving UNSGs will be harder in more complicated settings.\nMoreover, some UNSGs operate under conditions of imperfect information when real-time infor-\nmation is not available. In some cases, players possess asymmetric knowledge about the state of the\nenvironment. In some UNSGs, the escaping evader location and potential strategies might not be\nfully known to the pursuers in some scenarios, and conversely, the evader may have limited informa-\ntion about the evader locations. The partial observability also poses unique challenges for addressing\nthe UNSGs. In some cases, the maximum number of time steps may not be predicted accurately.\nTherefore, it necessitates the development of robust algorithms capable of making decisions based\non imperfect data and under uncertainty, requiring sophisticated decision-making processes akin to\nthose used in real-world scenarios.\n4\nEnvironment\nGame Module\nGraph, initial point, time horizon,…\nPursuer\nRunner\nEvader\nRunner\nAgent Module\n…\nPursuer\nPolicy\nEvader\nPolicy\n…\nPSRO\nBased\nMethod\nSolver.solve()\nPSRO.solve()\nAlgorithms\nCFR-MIX\nNSG-NFSP\nNSGZero\nPretrained \nPSRO\nGrasper\n……\nfor iter in range(iterations):\n         add_new_evader()\nadd_new_pursuer()\nupdate_meta_game()\ncompute_meta_strategy()\n……\n…….\nfor episode in range(episodes):\n          obs, info = env.reset()\nwhile not done:\n                    evader.get_action()\n                    pursuer.get_action()\n                    env.step()\n          pursuer.train()\n          evader.train()\n……\nSolver Module\nYes\nNo\nUpdate\nNew Policy\nFigure 2: The core structure and workflow of GraphChase.\nFurthermore, pursuers cannot communicate during the game play in some UNSGs, and then they\nhave to make decisions independently. This case is similar to general multiplayer games, where NE\nis commonly used as a solution concept. However, computing an NE is hard generally (Chen &\nDeng, 2005; Zhang et al., 2023b).\nIn addition, the UNSG, inherently a zero-sum game, involves direct competition between the pur-\nsuers and the escaping evader, where one’s gain is precisely the other’s loss, reflecting the purely\nadversarial nature of their interactions. Concurrently, profound cooperation within the team of pur-\nsuers is also essential. pursuers must work together seamlessly to effectively capture the escaping\nevader. The pursuers share the same utility function, aiming collectively to minimize the escape pos-\nsibilities of the evader. This blend of competitive and cooperative elements introduces significant\ncomplexities in solving UNSGs. The dual nature of interactions demands algorithms that can han-\ndle both aspects simultaneously—optimizing competitive moves against the escaping evader while\ncoordinating strategies among multiple pursuers.\nThese elements—combined competitive and cooperative dynamics, along with the challenge of op-\nerating under imperfect information or independent moves — make the UNSG an exemplary bench-\nmark for assessing the effectiveness of algorithms in complex and unpredictable environments. By\nproviding a platform that mimics the diverse scales and complexities of UNSGs, GraphChase offers\na valuable tool for advancing the development of scalable algorithms.\n3\nPLATFORM: GRAPHCHASE\nAs shown in Figure 1, GraphChase provides template scripts for quick-start, and, once completed by\nthe user, it carries out training and testing procedures for comparison. Results, such as the worst-case\nreward, are generated and available for review.\n3.1\nCORE COMPONENTS\nOur GraphChase platform features a flexible game environment specifically designed to facilitate\ncomprehensive simulations of UNSGs. The parameters that users can control to generate the graph\nstructure are detailed in Appendix C. There is a brief introduction about how to use GraphChase in\nAppendix F. At the core of this environment is a versatile system architecture, as depicted in Figure\n2, which clearly outlines the primary components and their interactions within the platform. The\nmodular architecture comprising the Game Environment, Agent, and Solver components enhances\nplatform versatility, facilitating both adaptation to diverse research requirements and integration\nof various algorithmic approaches. This modular design architecture enables researchers to easily\ncustomization and scale their own problems.\n5\nGame Module. To enhance the flexibility of our platform, GraphChase is designed to support\nextensive customization of game parameters, enabling users to simulate different UNSG scenar-\nios tailored to their specific demands. This customization capability includes several key features.\nFirst, users have the option to design or import their graphs for simulation. This could range from\nsimple, manually-generated grid diagrams to more complex real-world urban layouts, such as the\nSingapore road map. Any graph format can be transformed into an adjacency list as the input to\nthe game generation function. This feature allows researchers to explore UNSG in simulations that\nare directly relevant to their specific areas of study or practical application needs. Second, users\ncan specify key strategic points within the graph, such as initial positions of the pursuer and the\nevader, and exit nodes. This level of customization not only adds complexity and variability to the\nsimulations but also allows for testing strategies under different initial conditions and escape routes,\nmaking each game unique even when played on the same graph. Third, the platform supports cus-\ntomization of the time horizon for each game, accommodating both quick resolutions and longer\nstrategic engagements. Fourth, since GraphChase is based on the Gymnasium library, the amount\nand type of information accessible to each player can be easily adjusted by users via the API of gym-\nnasium.Env.step(). This feature allows the evader and the pursuer to have limited visibility of each\nother’s locations and moves, creating more realistic scenarios that closely replicate the information\nasymmetry often found in real-world situations. In conclusion, by allowing users to freely define the\nstructure of the graph, GraphChase enables a broad spectrum of simulation possibilities. The flexi-\nbility of GraphChase allows users to meticulously design games that meet their specific research or\noperational requirements. Furthermore, through integration with the Gymnasium library, users can\nsignificantly reduce the time to learn and utilize GraphChase, while also leveraging various Gym-\nnasium wrappers to conveniently run environments in parallel and visualize the performance of the\ntrained models.\nAgent Module. The Agent Module consists of two parts: the agent policy and the agent runner.\nThe policy refers to the algorithms adopted by the agent, such as PPO, MAPPO, and NSGZero.\nThe agent runner is responsible for simulation in the environment against opponents and uses the\nobtained data to update the agent policy. Specifically, an agent runner must have a get action(data)\nmethod, where data is a tuple providing the input required for the agent policy to generate actions.\nThe actions made by the policy are returned as the output of the get action() method. Additionally, if\nthe agent needs to improve its policy (not necessary in some cases, such as random strategies), it must\nhave a train() method. Users can freely define this method according to the requirements of their\ndesigned algorithms. In summary, with this agent module structure, users can customize pursuers\nand evaders adopting various algorithms and can easily integrate with the Game module introduced\nearlier and the solver module discussed later in the paper. This flexible structure provides a rich\ntesting ground for developing both defensive and offensive strategies within the game environment,\nallowing users to test the performance of different algorithms efficiently.\nSolver Module. The Solver module of our GraphChase platform encompasses a variety of al-\ngorithms designed to address UNSGs, aiming to facilitate users in comparing the performance of\nvarious algorithms. Given that the current state-of-the-art algorithms, such as Pretrained PSRO\nand Grasper, are based on the PSRO framework, we have integrated the PSRO learning framework\nwithin our platform to solve UNSGs. Users merely need to define the training methods for both\nthe pursuer runner and the evader runner and provide the environment with parameters to initialize\nthe PSRO algorithm. By designing the code structure in this manner, users can freely modify the\nalgorithms used by the pursuer, such as PPO or MAPPO, and seamlessly integrate them within the\nPSRO framework, thereby maximizing code reusability. Additionally, if users design a new learning\nframework and want to compare its performance to PSRO, they only need to define the environment\nand agents as introduced before, then a training task can be easily started.\n3.2\nBENCHMARK ALGORITHMS\nBased on our GraphChase platform, we have implemented several deep-learning algorithms that\nsolve UNSGs. Here, we provide a brief overview of these algorithms and outline their operational\nprocess within our platform, as illustrated in Figure 2.\nTo address the inherent challenges of imperfect information in UNSGs, we integrate several sophis-\nticated algorithms into GraphChase. It includes: 1) CFR-MIX algorithm (Li et al., 2021), incorpo-\nrating deep learning enhancements (Brown et al., 2019) based on counterfactual regret minimization\n6\n(CFR) (Zinkevich et al., 2008); 2) NSG-NFSP (Xue et al., 2021) based on the neural fictitious self-\nplay approach (Heinrich & Silver, 2016); 3) NSGZero (Xue et al., 2022) based on neural Monte\nCarlo tree search; 4) Variants of the PSRO framework (Lanctot et al., 2017): Pretrained PSRO (Li\net al., 2023a) and Grasper (Li et al., 2024). Figure 2 illustrates the operational process of these\nalgorithms within our GraphChase platform.\nEach algorithm is implemented to integrate with the game’s underlying mechanics through well-\ndefined interfaces, ensuring they can operate effectively within the platform’s architecture. Firstly,\nby inputting the initial positions of agents and the time horizon of the game, we set up the game\nenvironment. Simultaneously, we initialize the pursuer runner according to the solver algorithms, as\nshown in the yellow frame. Then, depending on whether the chosen algorithm requires the PSRO\nlearning framework, different solving processes are employed. If the PSRO framework is not re-\nquired, the solver.solve() method is executed. In this method, the evader and pursuer runner interact\ncontinuously with the environment to generate data, which is then used to update the policy network,\nproducing new strategies. If the PSRO framework is used, the PSRO.solve() method is executed.\nDuring each iteration, the opponent’s strategy is alternately fixed, and a best response to the oppo-\nnent is generated. The meta game is then updated based on the simulation results, and the current\npolicy oracle’s meta strategy is computed. Subsequently, the runner’s policies are updated, and the\nprocess proceeds to the next training cycle.\nEvaluation. In our platform, the primary objective is to compute the optimal defense strategy for\nthe pursuer, akin to strategizing the most effective tactics for police officers in realistic scenarios.\nUpon determining the pursuer’s strategy through any of the algorithms available on the platform,\nwe adopt the worst-case utility as our principal evaluation metric. As introduced before, to compute\nthe worst-case utility, we first identify the best response strategy of the evader against the pursuer’s\nstrategy being evaluated. Then, we compute the pursuer’s worst-case utility by simulating the game\nusing the pursuer’s strategy and the best response strategy of the evader. This evaluation method\nhelps ensure that the strategy is not only theoretically sound but also practically viable under the\nmost demanding conditions.\n4\nEXPERIMENTAL EVALUATION\nWe conduct experiments to evaluate GraphChase and show the issues of existing algorithms.\n4.1\nEXPERIMENTAL SETTING\nWe conduct the following three sets of experiments for the experimental evaluation. 1) To evaluate\nthe effectiveness of GraphChase, we compare the training procedure of algorithms implemented in\nGraphChase with the training procedure of algorithms implemented by the original authors1. 2) To\nevaluate the performance of existing algorithms, we calculate the reward (the probability of catching\nthe evader) of the pursuer in the worst-case setting. That is, the pursuer’s policy in a trained model\nwill be played against all available paths of the evader, and the worst-case reward will be the reward\nof this model. 3) To evaluate the scalability of existing algorithms, we run these algorithms to solve\nrealistic and large games.\nWe run the first two sets of experiments on the following two games shown in Appendix A. The first\ngame is easier to solve as the evader will be caught with a probability of 1 (ground truth), but the\nsecond game is harder to solve as the evader will only be caught with a probability of 0.5 (ground\ntruth). Both games are run on a 7 × 7 grid network with four exits, four pursuers, and one evader. In\nthe first set of experiments, we set T = 6. In the second set of experiments, we evaluate the pursuers’\ntrained model in the first set of experiments against all paths of the evader with the maximum length\nof each path as 6 and 12, respectively. Finally, we conducted a third set of experiments on a game\nset in a 100 × 100 grid network with a maximum time horizon of T = 200. In this scenario, four\npursuers attempt to capture a single evader who is trying to escape successfully through one of 12\nexit nodes.\n1Codes were shared by the original authors of these algorithms.\n7\n4.2\nBENCHMARK RESULTS\nThe Effectiveness of GraphChase. The results of the evaluation of GraphChase are shown in\nFigures 3 and 4 (Results for other algorithms are in Appendix B). We can see that the algorithms\nbased on our GraphChase perform similarly to the algorithms based on the original codes. In most\ncases, we can see that algorithms based on GraphChase converge faster than the algorithms based\non the original codes, which shows the effectiveness and efficiency of our GraphChase.2\nTo further verify that algorithms based on GraphChase can recover the performance of the algorithms\nbased on the original codes with significantly less time, we first show that our algorithms based on\nGraphChase can recover the performance of the algorithms based on the original codes in a variety\nof scenarios used in the UNSG domain (Xue et al., 2021; 2022; Li et al., 2023a; 2024) in Appendix\nD. These networks, including the 15 × 15 grid network, the real-world Singapore map, and the real-\nworld Manhattan map, are representatives because the 15 × 15 grid network represents the randomly\ngenerated network, and two real-world networks represent different topological structures in real-\nworld cities. Then, in Appendix E, we show that algorithms based on GraphChase run significantly\nfaster than algorithms based on the original codes in terms of simulation and data-saving time, and\nwe explain the reasons behind the faster convergence of GraphChase.\n0\n10000\n20000\n30000\n40000\n50000\n60000\nRun Time (seconds)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPursuer Reward\nOriginal code\nGraphChase\n(a) NSG-NFSP\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n22500\nRun Time (seconds)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPursuer Reward\nOriginal code\nGraphChase\n(b) Pretrained PSRO\n0\n5000\n10000\n15000\n20000\nRun Time (seconds)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPursuer Reward\nOriginal code\nGraphChase\n(c) Grasper\nFigure 3: The training procedure on the easy game with a caught probability of 1.\n0\n10000\n20000\n30000\n40000\n50000\n60000\nRun Time (seconds)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPursuer Reward\nOriginal code\nGraphChase\n(a) NSG-NFSP\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nRun Time (seconds)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPursuer Reward\nOriginal code\nGraphChase\n(b) Pretrained PSRO\n0\n10000\n20000\n30000\n40000\nRun Time (seconds)\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nPursuer Reward\nOriginal code\nGraphChase\n(c) Grasper\nFigure 4: The training procedure on the hard game with a caught probability of 0.5.\nThe Performance Issue of Existing Algorithms. The performance evaluation of existing algo-\nrithms for solving UNSGs is shown in Table 1. We can see that if an algorithm converges during\ntraining, it will perform well for solving the easy game (with a caught probability of 1) but may not\nperform well for solving the hard game (with a caught probability of 0.5). Increasing the maximum\nlength of the evader’s paths also will damage the performance.\nThe main reason is that, when the evader does not have real-time location information of pursuers,\ncomputing the evader’s best response against the strategy of pursuers is a very hard sparse-reward\nproblem, which involves finding an escape path from the initial location to an exit node. To simplify\nthis problem, almost all existing algorithms use the following best response approach of the evader:\nThe evader first chooses an exit node and then randomly takes a simple (acyclic) path that guarantees\nreaching the chosen exit node before exceeding the maximum time horizon. This approach reduces\nthe strategy space of the evader but cannot provide the true best response strategy for the evader.\nIn addition, due to the above-mentioned challenge, almost all existing algorithms assume that the\nmaximum length of the evader paths is short during training. Then, the strategy of pursuers may be\nexploited if the evader takes a longer path.\n2CFR-MIX and NSGZero solved games on 5 × 5 network with T = 4 because they run too slow.\n8\nMaximum length of paths for evaluation\nAlgorithm\nTraining\nT = 6\nT = 6\nT = 12\nT = 12\nPretrained PSRO\n2h\/1.5h\n0.01\n0.93\n0.01\n0.92\nGrasper\n7.7h\/2.5h\n0.12\n0.97\n0.05\n0.95\nNSG-NFSP\n5.5h\/3.3h\n0.03\n0.59\n0.03\n0.57\nNSGZero\n18h\/18h\n0.03\n0.06\n0.03\n0.05\nNSGZero (T = 3)\n11.3h\/7h\n0.01\n0.32\n0.0\n0.19\nNSGZero (5 × 5)\n1.8h\/1h\n0.42\n1\n0.39\n0.94\nCFR-MIX (5 × 5)\n6.9h\/6.3h\n0.03\n0.38\n0.01\n0.16\nGround Truth\nhard(0.5)\/easy(1)\n0.5\n1\n0.5\n1\nTable 1: The performance of existing algorithms in the worst-case setting. For the grid network, the\nmaximum length of the evader’s paths for evaluation is T = 4 or T = 8.\nThe Scalability Issue of Existing Algorithms. From the first set of experiments above, we can\nsee that the existing algorithms require several hours to converge for solving small games, as shown\nin Table 1. For solving this large game with a 100 × 100 grid network, we cannot see reasonable\nresults after training several days. For example, NSG-NFSP and Grasper get nothing after running\nfour days; NSGZero and Pretrained PSRO were trained for some iterations after running four days,\nbut their worst-case rewards are still almost 0.\nThese results show that existing algorithms still suffer performance and scalability issues in real-\nworld settings, which suggest that substantial efforts are still required to develop effective and effi-\ncient algorithms for solving real-world UNSGs.\n5\nRELATED WORKS\nGame theory has emerged as a valuable tool in addressing complex interactions and has been suc-\ncessfully applied to various security challenges (Jain et al., 2011; McCarthy et al., 2016; Sinha et al.,\n2018), including allocating limited resources to protect infrastructure (Jain et al., 2013) or design-\ning patrolling strategies in adversarial settings (Vorobeychik et al., 2014). Behind these results, one\nimportant model is Stackelberg Security Games (SSGs), which is used to solve a variety of security\nproblems (Sinha et al., 2018). In SSGs, the defender moves first and then the attacker best responds\nto the defender’s strategy. Then, the UNSG model is a special case of SSG, which is used in the\nzero-sum environment on networks.\nThe UNSG is similar to pursuit-evasion games (Parsons, 1976), where pursuers chase evaders. The\npursuit-evasion game involves strategic interactions between multiple pursuers and one or more\nevaders within a well-defined environment (Bilgin & Kadioglu-Urtis, 2015), presenting enduring\nchallenges and significant applications ranging from civilian safety (Oyler et al., 2016) to military\noperations (Vlahov et al., 2018). As a complex and widely-studied research problem, the pursuit-\nevasion game has been extensively applied across physics (Isaacs, 1965), mathematics (Pachter,\n1987; Kopparty & Ravishankar, 2005), and engineering (Eklund et al., 2011). The pursuit-evasion\ngames are often studied in the framework of differential games. Several canonical pursuit-evasion\ngames were first formulated as differential games and extensively studied by Rufus Isaacs in his\nmasterpiece “Differential Games” (Isaacs, 1965). Later, many studies focusing on pursuit-evasion\ngames emerged, and different algorithms were developed. For example, Ho et al. introduced the lin-\near–quadratic differential game (LQDG) formulation to address pursuit-evasion problems (Ho et al.,\n1965). In 1976, Parsons first used graphs to describe the pursuit-evasion games (Parsons, 1976).\nFrom the origins of the pursuit-evasion games until today, the game underwent several changes and\nnow constitutes a large family of problems. Researchers have also focused on pursuit-evasion games\nin a discrete setting in the past several decades. The discrete-time multiple-pursuer single-evader\ngame is solved (Bopardikar et al., 2008). Later, there are several works (Hor´ak & Boˇsansk`y, 2016;\nHor´ak et al., 2017; Hor´ak & Boˇsansk`y, 2017) focusing on one-sided partially observable pursuit-\nevasion games, in which the evader knows the pursuers’ locations while the pursuers do not know\nthe evader’s location. Similarly, the patrolling security game (PSG) (Basilico et al., 2009; Vorob-\neychik et al., 2014), where the defender defends against an unseen intruder, and the intruder needs\n9\nmultiple turns to perform the attack in the environment, is typically modeled as a stochastic game\nwith an infinite horizon. Later, PSGs were extended to cover cases where the defender receives an\nuncertain signal after being attacked and then goes to the point of being attacked to catch the attacker\n(Basilico et al., 2017a;c). More recently, a hierarchical framework has been presented for solving\ndiscrete stochastic pursuit-evasion games in large grid worlds (Guan et al., 2022). Our GraphChase\ncan be extended to cover these settings.\nExisting multiplayer benchmarks based on pursuit-evasion games, such as SIMPE (Talebi & Simaan,\n2018), Multi-Agent RL Benchmark (MARBLER) (Jain et al., 2011), and Avalon (Albrecht et al.,\n2022), have significantly advanced the field by offering diverse scenarios and testing environments.\nSIMPE, for instance, focuses on interactive simulation with varied strategies for multiple pursuers\nand a single evader, allowing for the exploration of cooperative and non-cooperative tactics (Talebi\n& Simaan, 2018). However, it outputs the coordinates of the pursuer and evader in the x-y plane,\nwith a continuous position space. And it does not take time information into account, overlook-\ning the temporal constraints inherent in UNSGs. Similarly, MARBLER integrates physical robot\ndynamics with Multi-Agent Reinforcement Learning (MARL), bridging simulation with real-world\nrobot behavior (Jain et al., 2011). Avalon further extends these concepts by providing procedurally\ngenerated worlds aimed at testing the generalization capabilities of RL algorithms (Albrecht et al.,\n2022). However, It is designed to simulate biological survival skills (from basic actions like eat-\ning to complex behaviors like hunting and navigation). Google Research Football (Kurach et al.,\n2020) and Starcraft (Samvelyan et al., 2019) are MARL environments on a plane. Despite these\nadvances, these platforms primarily concentrate on MARL from an algorithmic development per-\nspective, often neglecting the nuanced game-theoretical aspects that can emerge in pursuit-evasion\ncontexts. Openspiel (Lanctot et al., 2019) is an established extensive collection of environments\nand algorithms for research in games. However, it mainly focuses on recreational games and does\nnot include pursuit-evasion games. Therefore, it results in a gap where the strategic, competitive,\nand cooperative elements integral to real-world applications of UNSGs need to be fully explored or\noptimized. Our GraphChase platform bridges the gap by building a flexible UNSG environment.\n6\nDISCUSSION: TESTBED FOR MULTIPLAYER GAMES\nComputing an NE in multiplayer games is generally hard (Chen & Deng, 2005; Zhang et al., 2023b),\nand designing efficient algorithms for computing such an NE is still an open challenge. Our platform\ncould be a testbed for algorithms for solving multiplayer games. In particular, our platform provides\nreal-world scenarios for adversarial team games (von Stengel & Koller, 1997; Basilico et al., 2017b;\nCelli & Gatti, 2018; Farina et al., 2018; Zhang & An, 2020a;b; Zhang et al., 2021; Farina et al.,\n2021; Zhang et al., 2022c;a;b; Zhang & Sandholm, 2022; Carminati et al., 2022; Zhang et al., 2023a;\nMcAleer et al., 2023; Anagnostides et al., 2023; Li et al., 2023b), where a group of players competes\nagainst an adversary or another team. Various solution concepts apply depending on the situation.\nWhen team players compete independently against the adversary, the relevant solution concepts\ninclude 1) NE (Nash, 1951; Zhang et al., 2023b), where no player gains by deviating from this\nequilibrium, and 2) team-maxmin equilibrium (TME) (von Stengel & Koller, 1997; Basilico et al.,\n2017b; Celli & Gatti, 2018; Zhang & An, 2020a;b; Zhang et al., 2022c), which is a type of NE\nthat optimizes the team’s utility across all NEs. Based on our platform, if we set that pursuers\nindependently try to interdict the evader, we can also use our platform to compute an NE or TME in\nnormal-form or extensive-form games. For normal-form games where team players can coordinate\ntheir strategies, the applicable solution concept is the correlated team-maxmin equilibrium (CTME)\n(Basilico et al., 2017b). This is essentially equivalent to an NE in zero-sum two-player games, as\nthe team with coordinated strategies and a unified payoff function behaves like a single player. In\nextensive-form games, the team with coordinated strategies has two solution concepts: 1) team-\nmaxmin equilibrium with a communication device (TMECom) (Celli & Gatti, 2018), applicable\nwhen the team can continuously communicate and coordinate strategies, making the game akin to a\ntwo-player zero-sum game with perfect recall; and 2) team-maxmin equilibrium with a coordination\ndevice (TMECor) (Celli & Gatti, 2018; Zhang et al., 2021; 2024), used when the team can only\ncoordinate strategies before gameplay, rendering the game similar to a two-player zero-sum game\nwith imperfect recall. The algorithms in (Zhang et al., 2019; Li et al., 2021; Xue et al., 2021;\n2022; Li et al., 2023a; 2024) implemented on GraphChase compute a TMECom that is NE in team\nadversarial games. If we set that the team can only coordinate strategies before gameplay in the\nextensive-form games, we can also compute a TMECor on GraphChase.\n10\n7\nCONCLUSION\nWe present GraphChase, an open-source platform for UNSGs, offering researchers a flexible multi-\nplayer game environment to aid in developing scalable algorithms. Specifically, we first develop a\nunified and flexible UNSG environment and then implement several deep learning-based algorithms\nas benchmarks. Finally, we evaluate GraphChase and the results will provide insights into these\nalgorithms and further refine instruction for them. We hope our GraphChase platform can facilitate\nthe establishment of a standardized criterion for evaluating and improving algorithms for UNSGs,\nthereby contributing to the advancement of theoretical research and practical applications for solving\ngeneral multiplayer games.\nLimitation. Although we have implemented some state-of-the-art algorithms for solving UNSGs,\nthese algorithms still face significant challenges on performance and scalability. As the size and\ncomplexity of the UNSG increase, computing the best response strategy for each player becomes in-\ncreasingly time-consuming and computationally expensive. Existing algorithms struggle to scale up\nas they typically require multiple computations of the best response strategy, which can be resource-\nintensive. Our GraphChase platform has been designed to facilitate to address these challenges by\nproviding a large-scale game environment. However, despite its advanced capabilities, our plat-\nform still has some limitations that we aim to address in future works. First, the abstract nature of\ngraph-based models may not accurately capture all the dynamic and unpredictable elements of real-\nworld environments, such as variable traffic patterns and spontaneous human behaviors. Second,\nGraphChase may struggle to adapt to rapid changes in urban settings, such as emergencies or unex-\npected social events, which can alter game dynamics and require immediate strategic adjustments.\n11\nREFERENCES\nJoshua Albrecht, Abraham Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wr´oblewski,\nNicole Seo, Michael Rosenthal, Maksis Knutins, Zack Polizzi, James Simon, et al.\nAvalon:\nA benchmark for rl generalization using procedurally generated worlds. Advances in Neural\nInformation Processing Systems, 35:12813–12825, 2022.\nIoannis Anagnostides, Fivos Kalogiannis, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-\nGkaragkounis, and Stephen Mcaleer. Algorithms and complexity for computing Nash equilibria\nin adversarial team games. In EC, 2023.\nNicola Basilico, Nicola Gatti, Francesco Amigoni, et al.\nLeader-follower strategies for robotic\npatrolling in environments with arbitrary topologies. In Proceedings of the International Joint\nConference on Autonomous Agents and Multi Agent Systems (AAMAS), pp. 57–64, 2009.\nNicola Basilico, Andrea Celli, Giuseppe De Nittis, and Nicola Gatti. Coordinating multiple defen-\nsive resources in patrolling games with alarm systems. In Proceedings of the 16th Conference on\nAutonomous Agents and MultiAgent Systems, pp. 678–686, 2017a.\nNicola Basilico, Andrea Celli, Giuseppe De Nittis, and Nicola Gatti. Team-maxmin equilibrium:\nEfficiency bounds and algorithms. In AAAI, pp. 356–362, 2017b.\nNicola Basilico, Giuseppe De Nittis, and Nicola Gatti. Adversarial patrolling with spatially uncertain\nalarm signals. Artificial Intelligence, 246:220–257, 2017c.\nAhmet Tunc Bilgin and Esra Kadioglu-Urtis. An approach to multi-agent pursuit evasion games\nusing reinforcement learning. In 2015 International Conference on Advanced Robotics (ICAR),\npp. 164–169. IEEE, 2015.\nShaunak D Bopardikar, Francesco Bullo, and Joao P Hespanha. On discrete-time pursuit-evasion\ngames with sensing limitations. IEEE Transactions on Robotics, 24(6):1429–1439, 2008.\nNoam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats\ntop professionals. Science, 359(6374):418–424, 2018.\nNoam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):\n885–890, 2019.\nNoam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-\nmization. In International conference on machine learning, pp. 793–802. PMLR, 2019.\nLuca Carminati, Federico Cacciamani, Marco Ciccone, and Nicola Gatti. A marriage between ad-\nversarial team games and 2-player games: Enabling abstractions, no-regret learning, and subgame\nsolving. In ICML, pp. 2638–2657. PMLR, 2022.\nAndrea Celli and Nicola Gatti. Computational results for extensive-form adversarial team games.\nIn AAAI, pp. 965–972, 2018.\nXi Chen and Xiaotie Deng. 3-Nash is PPAD-complete. In Electronic Colloquium on Computational\nComplexity, volume 134, pp. 2–29, 2005.\nJ Mikael Eklund, Jonathan Sprinkle, and S Shankar Sastry. Switched and symmetric pursuit\/evasion\ngames using online model predictive control with application to autonomous aircraft.\nIEEE\nTransactions on Control Systems Technology, 20(3):604–620, 2011.\nMeta Fundamental AI Research Diplomacy Team FAIR, Anton Bakhtin, Noam Brown, Emily Di-\nnan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu,\net al. Human-level play in the game of diplomacy by combining language models with strategic\nreasoning. Science, 378(6624):1067–1074, 2022.\nGabriele Farina, Andrea Celli, Nicola Gatti, and Tuomas Sandholm.\nEx ante coordination and\ncollusion in zero-sum multi-player extensive-form games. In NeurIPS, pp. 9638–9648, 2018.\n12\nGabriele Farina, Andrea Celli, Nicola Gatti, and Tuomas Sandholm. Connecting optimal ex-ante\ncollusion in teams to extensive-form correlation: Faster algorithms and positive complexity re-\nsults. In ICML, pp. 3164–3173, 2021.\nMorgan Gaither, Mark Gabriele, Nancy Andersen, Sean Healy, and Vivian Hung. Pursuit technology\nimpact assessment, version 1.1. Final Report to the US Department of Justice, 2017.\nYue Guan, Mohammad Afshari, Qifan Zhang, and Panagiotis Tsiotras. Hierarchical decompositions\nof stochastic pursuit-evasion games. In 2022 IEEE 61st Conference on Decision and Control\n(CDC), pp. 5062–5067. IEEE, 2022.\nJohannes Heinrich and David Silver.\nDeep reinforcement learning from self-play in imperfect-\ninformation games. arXiv preprint arXiv:1603.01121, 2016.\nY Ho, Arthur Bryson, and Sheldon Baron. Differential games and optimal pursuit-evasion strategies.\nIEEE Transactions on Automatic Control, 10(4):385–389, 1965.\nKarel Hor´ak and Branislav Boˇsansk`y. A point-based approximate algorithm for one-sided partially\nobservable pursuit-evasion games. In 7th International Conference on Decision and Game Theory\nfor Security, pp. 435–454, 2016.\nKarel Hor´ak and Branislav Boˇsansk`y. Dynamic programming for one-sided partially observable\npursuit-evasion games. In International Conference on Agents and Artificial Intelligence, vol-\nume 2, pp. 503–510. SCITEPRESS, 2017.\nKarel Hor´ak, Branislav Boˇsansk`y, and Michal Pˇechouˇcek. Heuristic search value iteration for one-\nsided partially observable stochastic games. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 31, pp. 558–564, 2017.\nR. Isaacs. Differential Games: A Mathematical Theory with Applications to Warfare and Pursuit,\nControl and Optimization. Dover books on mathematics. Wiley, 1965. ISBN 9780471428602.\nURL https:\/\/books.google.com.sg\/books?id=gtlQAAAAMAAJ.\nManish Jain, Dmytro Korzhyk, Ondˇrej Vanˇek, Vincent Conitzer, Michal Pˇechouˇcek, and Milind\nTambe. A double oracle algorithm for zero-sum security games on graphs. In AAMAS, pp.\n327–334, 2011.\nManish Jain, Vincent Conitzer, and Milind Tambe. Security scheduling for real-world networks. In\nAAMAS, pp. 215–222, 2013. ISBN 978-1-4503-1993-5.\nSwastik Kopparty and Chinya V Ravishankar.\nA framework for pursuit evasion games in rn.\nInformation Processing Letters, 96(3):114–122, 2005.\nKarol Kurach, Anton Raichuk, Piotr Sta´nczyk, Michał Zajac, Olivier Bachem, Lasse Espeholt, Car-\nlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al.\nGoogle research\nfootball: A novel reinforcement learning environment. In Proceedings of the AAAI conference\non artificial intelligence, volume 34, pp. 4501–4510, 2020.\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien\nP´erolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent rein-\nforcement learning. In NeurIPS, pp. 4190–4203, 2017.\nMarc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay,\nJulien P´erolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al.\nOpenSpiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453,\n2019.\nPengdeng Li, Shuxin Li, Xinrun Wang, Jakub Cerny, Youzhi Zhang, Stephen McAleer, Hau Chan,\nand Bo An. Grasper: A generalist pursuer for pursuit-evasion problems. AAMAS, 2024.\nShuxin Li, Youzhi Zhang, Xinrun Wang, Wanqi Xue, and Bo An. CFR-MIX: Solving imperfect\ninformation extensive-form games with combinatorial action space. In IJCAI, pp. 3663–3669,\n2021.\n13\nShuxin Li, Xinrun Wang, Youzhi Zhang, Wanqi Xue, Jakub ˇCern`y, and Bo An. Solving large-\nscale pursuit-evasion games using pre-trained strategies. In AAAI, volume 37, pp. 11586–11594,\n2023a.\nShuxin Li, Youzhi Zhang, Xinrun Wang, Wanqi Xue, and Bo An.\nDecision making in team-\nadversary games with combinatorial action space.\nCAAI Artificial Intelligence Research, 2,\n2023b.\nStephen Marcus McAleer, Gabriele Farina, Gaoyue Zhou, Mingzhi Wang, Yaodong Yang, and Tuo-\nmas Sandholm. Team-PSRO for learning approximate TMECor in large team games via cooper-\native reinforcement learning. In NeurIPS, 2023.\nSara Marie McCarthy, Milind Tambe, Christopher Kiekintveld, Meredith L Gore, and Alex Killion.\nPreventing illegal logging: Simultaneous optimization of resource teams and tactics for security.\nIn AAAI, pp. 3880–3886, 2016.\nMatˇej Moravˇc´ık, Martin Schmid, Neil Burch, Viliam Lis´y, Dustin Morrill, Nolan Bard, Trevor\nDavis, Kevin Waugh, Michael Johanson, and Michael Bowling. DeepStack: Expert-level artificial\nintelligence in no-limit poker. Science, 356:508–513, 2017.\nJohn Nash. Non-cooperative games. Annals of Mathematics, pp. 286–295, 1951.\nJohn F Nash. Equilibrium points in n-person games. PNAS, 36(1):48–49, 1950.\nDave W Oyler, Pierre T Kabamba, and Anouck R Girard. Pursuit–evasion games in the presence of\nobstacles. Automatica, 65:1–11, 2016.\nM Pachter.\nSimple-motion pursuit-evasion in the half plane.\nComputers & Mathematics with\nApplications, 13(1-3):69–82, 1987.\nTD Parsons. Pursuit-evasion in a graph. Theory and Applications of Graphs, 1976.\nFrederick P Rivara and Christopher D Mack. Motor vehicle crash deaths related to police pursuits\nin the united states. Injury Prevention, 10(2):93–95, 2004.\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas\nNardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.\nThe starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\nYoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and\nLogical Foundations. Cambridge University Press, 2008.\nArunesh Sinha, Fei Fang, Bo An, Christopher Kiekintveld, and Milind Tambe. Stackelberg security\ngames: Looking beyond a decade of success. In IJCAI, pp. 5494–5501, 2018.\nShahriar Talebi and Marwan A Simaan. Simpe: A simulation platform for multi-player pursuit-\nevasion problems.\nIn 2018 IEEE 14th International Conference on Control and Automation\n(ICCA), pp. 344–349. IEEE, 2018.\nBogdan Vlahov, Eric Squires, Laura Strickland, and Charles Pippin. On developing a uav pursuit-\nevasion policy using reinforcement learning. In 2018 17th IEEE International Conference on\nMachine Learning and Applications (ICMLA), pp. 859–864. IEEE, 2018.\nBernhard von Stengel and Daphne Koller. Team-maxmin equilibria. Games and Economic Behavior,\n21(1-2):309–321, 1997.\nYevgeniy Vorobeychik, Bo An, Milind Tambe, and Satinder Singh. Computing solutions in infinite-\nhorizon discounted adversarial patrolling games. In Proceedings of the International Conference\non Automated Planning and Scheduling, volume 24, pp. 314–322, 2014.\nWanqi Xue, Youzhi Zhang, Shuxin Li, Xinrun Wang, Bo An, and Chai Kiat Yeo. Solving large-scale\nextensive-form network security games via neural fictitious self-play. In IJCAI, pp. 3713–3720,\n2021.\n14\nWanqi Xue, Bo An, and Chai Kiat Yeo. Nsgzero: Efficiently learning non-exploitable policy in\nlarge-scale network security games with neural monte carlo tree search. In AAAI, pp. 4646–\n4653, 2022.\nBrian Zhang, Luca Carminati, Federico Cacciamani, Gabriele Farina, Pierriccardo Olivieri, Nicola\nGatti, and Tuomas Sandholm. Subgame solving in adversarial team games. In NeurIPS, pp.\n26686–26697, 2022a.\nBrian Hu Zhang and Tuomas Sandholm. Team correlated equilibria in zero-sum extensive-form\ngames via tree decompositions. In AAAI, pp. 5252–5259, 2022.\nBrian Hu Zhang, Gabriele Farina, Andrea Celli, and Tuomas Sandholm. Optimal correlated equilib-\nria in general-sum extensive-form games: Fixed-parameter algorithms, hardness, and two-sided\ncolumn-generation. In EC, pp. 1119–1120, 2022b.\nBrian Hu Zhang, Gabriele Farina, and Tuomas Sandholm. Team belief DAG: Generalizing the\nsequence form to team games for fast computation of correlated team max-min equilibria via\nregret minimization. In ICML, pp. 40996–41018, 2023a.\nYouzhi Zhang and Bo An. Computing team-maxmin equilibria in zero-sum multiplayer extensive-\nform games. In AAAI, pp. 2318–2325, 2020a.\nYouzhi Zhang and Bo An. Converging to team-maxmin equilibria in zero-sum multiplayer games.\nIn ICML, pp. 11033–11043, 2020b.\nYouzhi Zhang, Bo An, Long Tran-Thanh, Zhen Wang, Jiarui Gan, and Nicholas R Jennings. Optimal\nescape interdiction on transportation networks. In IJCAI, pp. 3936–3944, 2017.\nYouzhi Zhang, Qingyu Guo, Bo An, Long Tran-Thanh, and Nicholas R Jennings.\nOptimal in-\nterdiction of urban criminals with the aid of real-time information. In AAAI, volume 33, pp.\n1262–1269, 2019.\nYouzhi Zhang, Bo An, and Jakub ˇCern`y. Computing ex ante coordinated team-maxmin equilibria\nin zero-sum multiplayer extensive-form games. In AAAI, volume 35, pp. 5813–5821, 2021.\nYouzhi Zhang, Bo An, and V.S. Subrahmanian. Correlation-based algorithm for team-maxmin equi-\nlibrium in multiplayer extensive-form games. In IJCAI, pp. 606–612, 2022c.\nYouzhi Zhang, Bo An, and Venkatramanan Subrahmanian. Computing optimal nash equilibria in\nmultiplayer games. volume 36, 2023b.\nYouzhi Zhang, Bo An, and Daniel Dajun Zeng. Dag-based column generation for adversarial team\ngames. 2024.\nMartin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization\nin games with incomplete information. In NeurIPS, pp. 1729–1736, 2008.\n15\nA\nUNSGS IN EXPERIMENTS\nUNSGs for the first two sets of experiments are shown in Figures 5 and 6.\n7x7\nfour police officers, one evader, four exit nodes , T = 6, caught probability = 0.5\n7x7\nfour police officers, one evader, four exit nodes , T = 6, caught probability = 1\nFigure 5: UNSGs of 7 × 7 with the caught probability of 0.5 (left) or 1 (right).\n5x5\nfour police officers, one evader, four exit nodes , T = 4, caught probability = 0.5\n5x5\nfour police officers, one evader, four exit nodes , T = 4, caught probability = 1\nFigure 6: UNSGs of 5 × 5 with the caught probability of 0.5 (left) or 1 (right).\nB\nADDITIONAL EXPERIMENTAL RESULTS\nAdditional experimental results are shown in Figures 7 and 8.\n0\n20000\n40000\n60000\n80000\n100000\nRun Time (seconds)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nPursuer Reward\nOriginal code\nGraphChase\n(a) CFR-MIX\n0\n2000\n4000\n6000\n8000\n10000\n12000\nRun Time (seconds)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPursuer Reward\nOriginal code\nGraphChase\n(b) NSGZero\nFigure 7: The training procedure on the easy game on the 5 × 5 network with a caught probability\nof 1.\n16\n0\n20000\n40000\n60000\n80000\n100000\n120000\nRun Time (seconds)\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nPursuer Reward\nOriginal code\nGraphChase\n(a) CFR-MIX\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nRun Time (seconds)\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nPursuer Reward\nOriginal code\nGraphChase\n(b) NSGZero\nFigure 8: The training procedure on the hard game on the 5 × 5 network with a caught probability\nof 0.5.\nGrid Graph\nCustom Graph\nunderlying\ngraph\nstructure\ncolumn\nrow\nadjacency\nside exist prob\nmatrix\ndiagnoal exist prob\nagent\nnumber\nand\nposition\nmax time horizon\npursuer num\nevader num\nexit num\npursuer initial position\nevader initial position\nexit position\nTable 2: The parameters that users can control.\nC\nUSER-CONTROLLABLE PARAMETERS\nThe user-controllable parameters are shown in Table 2. In our platform, users can configure a\nrange of parameters depending on the type of graph utilized: Grid Graph or Custom Graph.\nFor the Grid Graph, the underlying graph structure can be controlled through parameters such\nas column and row, which define the grid’s dimensions, as well as side exist prob and\ndiagonal exist prob, which determine the probabilities of edges existing between adjacent\nnodes and diagonal nodes, respectively. For the Custom Graph, the underlying structure is specified\nvia an adjacency matrix, allowing users to define a completely customized graph topology.\nIn both graph types, users can also control parameters related to the agent number and positions,\nincluding max time horizon, which defines the maximum simulation duration; pursuer num\nand evader num, specifying the number of pursuer and evader agents; and exit num, which\nsets the number of exits in the graph.\nAdditionally, initial positions for agents and exits can\nbe customized through pursuer initial position, evader initial position, and\nexit position, enabling users to tailor the simulation to specific scenarios.\nD\nEXPERIMENTS ON OTHER SETTINGS\nWe conducted experiments on a 15 × 15 grid graph to evaluate the performance of our platform in\ncomparison to existing environments. While CFR-MIX (Li et al., 2021), NSG-NFSP (Xue et al.,\n2021), and NSGZero (Xue et al., 2022) utilize the 15×15 grid graph, we found that specific settings,\nincluding the positions of pursuers, the evader, and exits, were not clearly given in their works. To\nensure a fair evaluation, we adopted uniform settings for training policies across both the original\ncode and GraphChase. There are four pursuers and ten exits for an evader. The max time horizon is\n15. The same settings allow for a direct comparison of the effectiveness of our platform against the\noriginal paper.\n17\nWe also extracted two real-world maps of Singapore with 372 nodes and Manhattan with 620 nodes\nand developed two large-scale UNSGs based on these maps. Experiments conducted on the Sin-\ngapore map have been previously tested in NSG-NFSP (Xue et al., 2021), NSGZero (Xue et al.,\n2022), Pretrained PSRO (Li et al., 2023a), and Grasper (Li et al., 2024). Manhattan map was tested\nin NSG-NFSP (Xue et al., 2021), and NSGZero (Xue et al., 2022). However, specific settings for\nthese two maps were not detailed in prior studies. For our simulations, we designated four pursuers\nand ten exits for the evader, with a time horizon set to 15 on the Singapore map. And there are six\npursuers and ten exits for the evader, with a time horizon set to 15 on the Manhattan map. To ensure\na fair comparison, we adopted the same settings for the original code and GraphChase3. The results\nare shown in the Table 3 and Table 4.\nNSG-NFSP\nNSGZero\nPretrained PSRO\nGrasper\n15 × 15\nOriginal paper\n0.83±0.028\n0.87±0.021\n0.994±0.003\n0.995±0.002\nGraphChase\n0.85±0.021\n0.91±0.016\n0.996±0.002\n0.996±0.001\nSingapore\nOriginal paper\n0.92±0.027\n0.96±0.015\n0.996±0.001\n0.998±0.01\nGraphChase\n0.94±0.022\n0.97±0.014\n0.997±0.001\n0.998±0.01\nTable 3: Experiments on 15 × 15 gird graph and real-world map from Singapore. Approximate\nworst-case defender rewards, averaged over 1000 test episodes. The ”±” indicates 95% confidence\nintervals over the 1000 plays.\nNSG-NFSP\nNSGZero\nGraphChase\n0.8689 ± 0.1377\n0.8865 ± 0.0859\nOriginal Code\n0.8556 ± 0.1151\n0.8738 ± 0.1377\nTable 4: Experiments on real-world map from Manhattan. Approximate worst-case defender re-\nwards, averaged over 1000 test episodes. The ”±” indicates 95% confidence intervals over the 1000\nplays.\nE\nFASTER WALL-CLOCK CONVERGENCE\nOur platform incorporates several technical enhancements that contribute to its faster performance.\nFirst, we have adopted the Gymnasium for game simulation, replacing the custom class implementa-\ntions found in the original papers. This change results in faster simulation processes and eliminates\nredundant data copying operations, leading to improved efficiency.\nAdditionally, we have implemented various code optimizations to enhance the platform’s perfor-\nmance. These include improved data type conversions, such as using numpy-to-tensor conversions\ninstead of list-to-tensor operations, which reduces processing time. We have also focused on enhanc-\ning memory management throughout the platform, resulting in more efficient resource utilization.\nFrom the perspective of wall-clock time, this indeed accelerates the convergence speed. However,\nit’s crucial to note that in terms of the number of training iterations required for convergence, there\nis no significant improvement. For instance, if the original code necessitates sampling 104 episodes\nto initiate convergence, our platform’s reproduced algorithms similarly require approximately the\nsame number of training iterations. This consistency in training iterations is attributable to the fact\nthat we have not altered the underlying algorithms themselves.\nUnlike the original implementation, our platform is designed with modular components, making\nit unsuitable to directly compare the performance of individual components against the original\ncode. However, to emphasize the efficiency of our platform in simulation processes, we conducted\nexperiments to evaluate the time required for a single episode of simulation and the subsequent\ndata-saving process for each algorithm. The performance comparison between GraphChase and the\n3Due to the extended training time required for the CFR-MIX algorithm, we did not conduct tests for CFR-\nMIX.\n18\noriginal implementation, highlighting the significant speed improvements achieved by our platform,\nis presented in Table 5.\nNSG-NFSP\nNSGZero\nPretrained PSRO\nGrasper\nGraphChase\n0.0089 ± 0.005\n0.378 ± 0.12\n0.0065 ± 0.002\n0.0097 ± 0.002\nOriginal Code\n0.0187 ± 0.005\n0.523 ± 0.15\n0.0153 ± 0.004\n0.0178 ± 0.002\nTable 5: Performance comparison between Original Code and GraphChase in terms of simulation\nand data-saving time (in seconds). Each value represents the mean execution time for a single\nepisode, with the corresponding standard deviation shown after the symbol ±.\nF\nUSAGE INSTRUCTIONS FOR GRAPHCHASE\nThe following steps outline the process for setting up and utilizing the GraphChase platform:\nF.1\nCLONING THE REPOSITORY\nTo begin, clone the GraphChase repository from GitHub and navigate to the project directory:\ngit clone https:\/\/github.com\/GraphChase\/GraphChasePlatform.git\ncd GraphChasePlatform\nF.2\nINSTALLING DEPENDENCIES\nInstall the necessary dependencies including pytorch, DGL and other required dependencies\nF.3\nRUNNING AN ALGORITHM\nTo run a specific algorithm, such as NSGZero, perform the following steps:\n1. Customize\nthe\nGraph:\nModify\nthe\ngraph\nfile\nlocated\nat\ngraph\/graph files\/custom graph.py to configure the graph structure, as\nwell as the positions of pursuer, evader, and exits.\n2. Adjust Algorithm Parameters: Open the configuration file in the configs directory,\nsuch as nsgzero configs.py, and set the desired parameters.\n3. Run the Algorithm: Execute the script to run the NSGZero algorithm:\npython scripts\/run_nsgzero_solver.py\nThe procedure for executing other algorithms follows a similar structure, requiring adjustments to\ntheir respective configuration files and script execution.\nG\nREPRODUCIBILITY\nThe structure of the network and values of all parameters follow the original papers of our imple-\nmented algorithms. To ensure the fairness of the comparative experiments, all our experiments were\nconducted on the server with 48-core 3.00GHz Intel(R) Xeon(R) Gold 6248R CPU and 8 NVIDIA\nA30 GPUs.\nWe release our platform on: https:\/\/github.com\/GraphChase\/GraphChasePlatform.git\n19\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research.pdf"}
{"title":"How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games","authors":"Yutong Xie, Yiyao Liu, Zhuang Ma, Lin Shi, Xiyuan Wang, Walter Yuan, Matthew O. Jackson, Qiaozhu Mei","summary":"The deployment of large language models (LLMs) in diverse applications\nrequires a thorough understanding of their decision-making strategies and\nbehavioral patterns. As a supplement to a recent study on the behavioral Turing\ntest, this paper presents a comprehensive analysis of five leading LLM-based\nchatbot families as they navigate a series of behavioral economics games. By\nbenchmarking these AI chatbots, we aim to uncover and document both common and\ndistinct behavioral patterns across a range of scenarios. The findings provide\nvaluable insights into the strategic preferences of each LLM, highlighting\npotential implications for their deployment in critical decision-making roles.","url":"http:\/\/arxiv.org\/abs\/2412.12362v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.12362v1","published":1734384345000,"comment":"Presented at The First Workshop on AI Behavioral Science (AIBS 2024)","pdf_text":"How Different AI Chatbots Behave? Benchmarking Large\nLanguage Models in Behavioral Economics Games\nYutong Xie\nyutxie@umich.edu\nUniversity of Michigan\nAnn Arbor, Michigan, USA\nYiyao Liu∗\nyiyaoliu@umich.edu\nUniversity of Michigan\nAnn Arbor, Michigan, USA\nZhuang Ma∗\ndavidmaz@umich.edu\nUniversity of Michigan\nAnn Arbor, Michigan, USA\nLin Shi∗\nlinshia@umich.edu\nUniversity of Michigan\nAnn Arbor, Michigan, USA\nXiyuan Wang∗\ndenniswx@umich.edu\nUniversity of Michigan\nAnn Arbor, Michigan, USA\nWalter Yuan\nwalter.yuan@moblab.com\nMobLab\nPasadena, California, USA\nMatthew O. Jackson\njacksonm@stanford.edu\nStanford University\nStanford, California, USA\nQiaozhu Mei\nqmei@umich.com\nUniversity of Michigan\nAnn Arbor, Michigan, USA\nAbstract\nThe deployment of large language models (LLMs) in diverse applica-\ntions requires a thorough understanding of their decision-making\nstrategies and behavioral patterns. As a supplement to a recent\nstudy on the behavioral Turing test [7], this paper presents a com-\nprehensive analysis of five leading LLM-based chatbot families as\nthey navigate a series of behavioral economics games. By bench-\nmarking these AI chatbots, we aim to uncover and document both\ncommon and distinct behavioral patterns across a range of sce-\nnarios. The findings provide valuable insights into the strategic\npreferences of each LLM, highlighting potential implications for\ntheir deployment in critical decision-making roles.\nKeywords\nAI, Chatbot, Behavioral Economics Games, Turing Test\n1\nIntroduction\nIn the rapidly advancing field of artificial intelligence, large lan-\nguage models (LLMs) are playing a transformative role in decision-\nmaking across diverse domains. These AI systems, capable of en-\ngaging in conversations, offering guidance, and tackling complex\ndecisions, are becoming increasingly indispensable in scenarios\nrequiring nuanced, human-like judgment [1–5, 8]. Understanding\nthe behavioral patterns and decision-making strategies of AI chat-\nbots is therefore critical. Such insights not only help optimize their\nperformance in specific applications but also enable better assess-\nment of their reliability and predictability, particularly in contexts\ninvolving significant responsibilities.\nOne recent study conducted by Mei et al. [7], has primarily\nfocused on the behavior of OpenAI ChatGPT variations through\na Turing test involving classic behavioral economics games. This\nstudy has revealed intricate details about ChatGPT’s behavioral\npatterns and preferences in scenarios designed to test trust, fairness,\nrisk aversion, altruism, cooperation, and other traits. However, it\nremains unclear whether these findings are unique to ChatGPT\n∗These authors contributed equally to this research.\nor if they extend to other LLMs like Meta Llama, Google Gemini,\nAnthropic Claude, and Mistral models. These models, while being\ninfluential in the AI sphere, have not been extensively studied in\nsimilar contexts. Moreover, although some research has explored\nparticular traits (e.g., trust dynamics) among different AI models\n[10], analyses covering other behavioral dimensions are still lacking,\nraising questions about the generalizability of these behavioral\ntraits across various scenarios and models.\nAs a supplementary work of Mei et al. [7], this paper conducts\na comprehensive analysis of five prominent LLM-based AI chat-\nbot families through a series of behavioral economics games. By\nsystematically evaluating their behaviors across these games, we\naim to provide a detailed profile of these AI systems. Our study\nnot only advances the understanding of AI behaviors but also high-\nlights the nuanced differences that distinguish these models in\ndecision-making contexts. Some main findings are:\n• All tested chatbots successfully capture specific human behav-\nior modes, leading to highly concentrated decision distributions\n(Fig. 1).\n• Although flagship chatbots demonstrate a notable probability of\npassing the Turing test (Fig. 2), AI chatbots can merely produce\na behavior distribution similar to humans (Fig. 3).\n• Compared to humans, AI chatbots place greater emphasis on\nmaximizing fairness in their payoff preferences (Fig. 4).\n• AI chatbots may exhibit inconsistencies in their payoff prefer-\nences across different games (Table 2).\n• Different AI chatbots exhibit distinct behavioral patterns in\ngames (Fig. 1), which can be further distinguished through\nTuring test results (Fig. 2), revealed payoff preferences (Fig.\n4), and behavioral inconsistencies (Table 2). These findings\nhighlight the effectiveness of our behavioral benchmark in\nprofiling and differentiating AI chatbots.\narXiv:2412.12362v1  [cs.AI]  16 Dec 2024\nXie et al.\nCooperate\n100.00%\nCooperate\n100.00%\nCooperate\n60.00%\nCooperate\n100.00%\nCooperate\n94.00%\nCooperate\n45.12%\nDefect\n0.00%\nDefect\n0.00%\nDefect\n40.00%\nDefect\n0.00%\nDefect\n6.00%\nDefect\n54.88%\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(a) Dictator\n(b) Ultimatum - Proposer\n(c) Ultimatum - Responder\n(d) Trust - Investor\n(e) Trust - Banker\n(f) Public Goods\n(g) Bomb\n(h) Prisoner’s Dilemma\nFigure 1: Distributions of AI chatbot behaviors in economics games.\n2\nMethods\n2.1\nLLM-Based AI Chatbots\nThis study focuses on five families of LLM-based AI chatbots, as\ndetailed in Table 1. In the main text, results are presented exclusively\nfor the flagship models. All model checkpoints were obtained as of\nJuly 31, 2024.\nAI chatbot family\nModel variants\/checkpoints\nOpenAI GPT\ngpt-4o-2024-05-13∗\ngpt-4o-mini-2024-07-18\ngpt-4-0125-preview\ngpt-4-0613\ngpt-3.5-turbo-0125\ngpt-3.5-turbo-0613\nMeta Llama\nllama-3.1-405B-instruct∗\nllama-3-70b-chat\nllama-3-8b-chat\nGoogle Gemini\ngemini-1.5-pro-latest∗\ngemini-1.0-pro-001\nAnthropic Claude\nclaude-3-5-sonnet-20240620∗\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\nclaude-3-haiku-20240307\nMistral\nmistral-large-2407∗\nmistral-large-2402\nTable 1: LLM-based AI chatbots investigated in this study.\nIn the main texts, we only report the results from flagship\nmodels as marked by “∗”.\n2.2\nCollecting AI Chatbot Behaviors in\nEconomics Games\nFollowing Mei et al. [7], we employ six classic behavioral economics\ngames to evaluate multiple dimensions of AI behavior, including\naltruism, fairness, trust, risk aversion, and cooperation. These games\ninclude Dictator, Ultimatum, Trust, Public Goods, Bomb Risk, and\nPrisoner’s Dilemma. Detailed descriptions of the games and the\nassociated prompts can be found in Mei et al. [7].\nFor each game and AI chatbot, we generate multiple responses\nusing the respective game prompts, collecting 50 independent valid\nresponses to establish the behavior distribution of each model.\nHuman behavior distributions are taken from Mei et al. [7] for\ncomparison.\n3\nResults\n3.1\nBehaviors of AI Chatbots\nFigure 1 (and Figure 8 in the Appendix) illustrates the distributions\nof AI choices across the six games. Overall, the distributions of\nAI chatbots are notably more concentrated compared to human\ndistributions, capturing only specific modes of human behavior.\nAdditionally, different AI chatbots exhibit distinctly varied behav-\nioral patterns, reflecting their unique orientations across multiple\nbehavioral dimensions.\nAltruism. In games including Dictator (Fig. 1a) and Ultimatum\n- Proposer that reveal the altruism of players, AI chatbots display\nto be more altruistic than humans by offering more to the partner.\nSurprisingly, a large fraction of Google Gemini 1.5 Pro instances\nchoose to offer most of the money ($90-$99) in Ultimatum - Proposer,\nshowing its particularly high tendency of altruism.\nFairness. Fairness is often emphasized by AI chatbots across\ngames. In the Dictator (Fig. 1a) and Ultimatum - Proposer Game (Fig.\n1b), most AI chatbots choose to offer $50 to the partner, meaning\na fair split. Correspondingly, Meta Llama 3.1 405B fairly requires\na minimum split of $50 as the Responder in Ultimatum (Fig. 1c).\nSimilarly in the Trust - Banker Game (Fig. 1e), OpenAI GPT 4o and\nAnthropic Claude 3.5 Sonnet tend to return the investment and half\nthe profit ($100 in total) to the investor.\nTrust. The Trust Game (Fig. 1d) particularly shows the trust\ndynamics. As the investor in the Trust investment game, AI chatbots\npossess different levels of trust towards the banker – Anthropic\nClaude 3.5 Sonnet and Google Gemini 1.5 Pro display a higher trust\nlevel, investing $53.20 and $51.20 on average; While other models\nmostly invest $50 to the banker.\nHow Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games\n39.7%\n45.6%\n43.5%\n34.9%\n45.2%\n45.3%\n20.8%\n20.6%\n20.4%\n16.2%\n20.9%\n20.8%\n39.5%\n33.8%\n36.1%\n48.9%\n33.9%\n33.9%\n39.7%\n64.2%\n41.0%\n11.1%\n65.7%\n60.1%\n20.6%\n33.1%\n23.7%\n6.4%\n34.3%\n31.2%\n39.7%\n2.7%\n35.3%\n82.5%\n8.7%\n41.2%\n20.0%\n9.3%\n37.4%\n51.9%\n32.8%\n17.8%\n14.6%\n9.0%\n17.4%\n24.0%\n16.3%\n41.0%\n65.4%\n81.7%\n45.2%\n24.1%\n50.9%\n(b) Dictator\n(c) Ultimatum - Proposer\n(d) Ultimatum - Responder\n42.5%\n31.2%\n22.6%\n12.8%\n31.3%\n31.1%\n15.0%\n11.3%\n8.8%\n7.0%\n11.4%\n11.6%\n42.5%\n57.5%\n68.6%\n80.2%\n57.3%\n57.3%\n41.3%\n29.7%\n74.7%\n32.5%\n23.4%\n64.9%\n17.3%\n12.1%\n24.3%\n12.9%\n10.5%\n21.8%\n41.4%\n58.2%\n1.0%\n54.6%\n66.1%\n13.3%\n42.9%\n76.1%\n51.9%\n45.4%\n76.0%\n71.6%\n14.2%\n23.9%\n17.5%\n14.6%\n24.0%\n22.6%\n42.9%\n30.6%\n40.0%\n5.8%\n(e) Trust - Investor\n(f) Trust – Banker *\n(g) Public Goods\n43.7%\n65.6%\n47.7%\n47.0%\n78.5%\n16.4%\n12.5%\n19.1%\n12.5%\n14.4%\n21.5%\n7.7%\n43.8%\n15.3%\n39.8%\n38.6%\n75.9%\n24.8%\n18.0%\n2.7%\n50.4%\n45.1%\n45.1%\n45.1%\n45.1%\n45.6%\n24.8%\n54.9%\n54.9%\n32.9%\n54.9%\n51.7%\n(h) Bomb Risk\n(i) Prisoner’s Dilemma\n(a) Average\n39.5%\n41.6%\n36.3%\n29.9%\n46.4%\n40.6%\n21.0%\n22.4%\n20.2%\n17.2%\n29.5%\n22.2%\n39.5%\n36.0%\n43.5%\n52.9%\n24.1%\n37.2%\nEstimated More Likely Human\nEstimated Equally Likely Human\/AI\nEstimated More Likely AI\nOpenAI GPT 4\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\nHuman\nOpenAI GPT 4\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\nHuman\nOpenAI GPT 4\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\nHuman\nFigure 2: The Turing test results.\nHuman\nOpenAI GPT 4o\nMeta Llama  3.1\n405B\nGoogle Gemini\n1.5 pro\nAnthropic\nClaude 3.5\nSonnet\nMistral large 2\nHuman\nOpenAI GPT 4o\nMeta Llama  3.1\n405B\nGoogle Gemini\n1.5 pro\nAnthropic\nClaude 3.5\nSonnet\nMistral large 2\n0.00\n24.02\n22.28\n22.24\n20.69\n20.46\n24.02\n0.00\n12.52\n19.62\n10.99\n9.76\n22.28\n12.52\n0.00\n23.25\n11.95\n4.45\n22.24\n19.62\n23.25\n0.00\n21.90\n20.55\n20.69\n10.99\n11.95\n21.90\n0.00\n8.17\n20.46\n9.76\n4.45\n20.55\n8.17\n0.00\n0\n5\n10\n15\n20\nValue of Wasserstein distance\nFigure 3: Pairwise behavior distribution dissimilarities estimated with Wasserstein distance.\nRisk aversion. The Bomb Risk Game (Fig. 1) tests the risk pref-\nerence of players. In this game, Meta Llama 3.1 405B and Mistral\nLarge 2 are risk-neural and choose to open 50 boxes most of the\ntime, which yields the maximized expected payoff. However, com-\npared to these two models, OpenAI GPT 4o and Google Gemini 1.5\nPro are more risk-averse, opening 14.06 and 35.46 boxes on aver-\nage. Interestingly, Anthropic Claude 3.5 Sonnet displays different\nrisk preferences across instances – 44% instances open less than 50\nboxes, while 46% instances open more than 50 boxes.\nCooperation. In the Prisoner’s Dilemma Game (Fig. 1h), which\nshows the cooperation tendency of models, Meta Llama 3.1 405B,\nAnthropic Claude 3.5 Sonnet, and Mistral Large 2 have the high-\nest proportion of choosing the Cooperation action (100%). Google\nGemini 1.5 Pro has the lowest rate of cooperating (60.00%), but the\nratio is still significantly larger than humans (45.12%). However, in\nthe Public Goods Game (Fig. 1f), most models tend to contribute\nhalf the money ($10) to the group instead of a larger contribution.\n3.2\nThe Behavioral Turing Test\nUsing the collected behavior distributions of AI chatbots and the\nexcerpted human behaviors, we conduct Turing tests following\nthe methodology outlined in Mei et al. [7]. Adopting the same\nprocedure as described in the paper, each round of the test involves\nindependently sampling one human action and one action from the\nAI behavior distribution. These samples are then compared based\non their probabilities within the human distribution.\nFigure 2 presents the results of the Turing tests. Overall, all tested\nAI chatbots demonstrate a remarkable ability to pass the Turing\ntest, with Meta Llama 3.1 405B achieving the highest winning rate\nagainst humans at 46.4%.\nHowever, in certain games, the chatbots exhibit significant chal-\nlenges in replicating human behavior. For instance, in the Trust\nGame - Investor role (Fig. 2e), AI chatbots tend to invest conserva-\ntively, whereas a substantial fraction of human players opt to invest\ntheir entire amount (Fig. 1d). Similarly, in the Prisoner’s Dilemma\nXie et al.\n(Fig. 2i), AI chatbots show a pronounced inclination toward coop-\neration, while over half of the human players choose to defect (Fig.\n1h).\nAI chatbots exhibit diverse capabilities in passing the Turing\ntest across different games. OpenAI GPT-4 shows a relatively high\nsuccess rate in the Ultimatum - Proposer, Trust - Banker, and Public\nGoods games but struggles significantly in the Bomb Risk Game.\nSimilarly, Meta Llama 3.1 405B performs well in many games but\nfalls short in the Trust - Banker scenario. Anthropic Claude 3.5\nSonnet stands out in the Trust - Banker Game but barely passes the\nTuring test in the Ultimatum - Responder role.\n3.3\nBehavior Distribution Similarity\nWhile the Turing test is a valuable method for evaluating an AI’s\nability to act like a single human player [9], it has inherent limita-\ntions in capturing the complete spectrum of the behavior distribu-\ntion. To overcome these limitations, we introduce a complementary\napproach: a distribution similarity test that assesses whether AI\nchatbots can accurately represent the behavior distribution of a\nhuman population.\nTable 3 (and Table 5 in the Appendix) presents the pairwise dis-\nsimilarities of behavior distributions, measured using the Wasser-\nstein distance. Smaller distances indicate greater similarity between\ntwo distributions, whether comparing chatbots or humans and chat-\nbots.\nAmong the AI chatbots, gpt-3.5-turbo-0613 demonstrates the\nhighest similarity to the human population (Fig. 5), likely due to\nits ability to produce relatively diverse choices (Fig. 8). However,\ndespite this similarity, a significant gap remains between the human\nbehavior distribution and AI-generated actions, with no chatbot\nachieving a distribution that closely mirrors human behaviors.\nWe also observe relatively small Wasserstein distances among\nMeta Llama 3.1, Anthropic Claude models, and Mistral Large models\n(Fig. 5), indicating that these chatbots exhibit similar behavioral\npatterns.\n3.4\nRevealing the payoff Preferences\nTo uncover the intrinsic objectives underlying the behaviors of AI\nchatbots, we perform analyses to identify and characterize their\npayoff preferences.\nObjective optimization efficiency. The objective function of AI\nchatbots is quantitatively estimated by assessing the degree to\nwhich their behaviors align with the optimization goals. We adopt\nthe family of utility functions from Mei et al. [7]:\n𝑈𝑏=\n\u0002\n𝑏× Own payoff 𝑟+ (1 −𝑏) × Partner payoff 𝑟\u00031\/𝑟,\n(1)\nwhere 𝑏∈[0, 1] represents the trade-off between a player’s own\npayoff and their partner’s payoff. Specifically, 𝑏= 1 corresponds\nto purely selfish preferences, 𝑏= 0 represents purely selfless pref-\nerences, and 𝑏= 0.5 reflects a preference for maximizing the com-\nbined payoff of both players. In this context, 𝑟is a specification\nparameter that is frequently set to 1 (indicating a linear specifica-\ntion) or 1\/2 (corresponding to a constant elasticity of substitution\nutility function, CES specification), as commonly adopted in the\nliterature [6].\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nOptimization squared error\n0.34\n0.28\n0.22\n0.15\n0.09\n0.05\n0.04\n0.06\n0.09\n0.13\n0.17\n0.20\n0.17\n0.13\n0.09\n0.06\n0.03\n0.04\n0.07\n0.11\n0.15\n0.20\n0.24\n0.21\n0.17\n0.13\n0.09\n0.06\n0.06\n0.08\n0.12\n0.16\n0.19\n0.23\n0.19\n0.15\n0.11\n0.08\n0.05\n0.05\n0.08\n0.13\n0.19\n0.26\n0.22\n0.19\n0.15\n0.11\n0.07\n0.04\n0.04\n0.07\n0.11\n0.16\n0.21\n0.19\n0.16\n0.12\n0.09\n0.05\n0.02\n0.03\n0.06\n0.10\n0.14\n0.18\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(a) Linear specification (𝑟= 1).\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nOptimization squared error\n0.34\n0.28\n0.21\n0.15\n0.10\n0.06\n0.05\n0.07\n0.09\n0.13\n0.17\n0.20\n0.14\n0.09\n0.05\n0.03\n0.03\n0.03\n0.06\n0.10\n0.15\n0.20\n0.24\n0.18\n0.12\n0.08\n0.06\n0.06\n0.05\n0.07\n0.11\n0.15\n0.19\n0.23\n0.18\n0.13\n0.09\n0.07\n0.06\n0.08\n0.11\n0.16\n0.21\n0.26\n0.22\n0.16\n0.11\n0.07\n0.05\n0.04\n0.04\n0.06\n0.10\n0.15\n0.21\n0.19\n0.13\n0.08\n0.04\n0.03\n0.02\n0.02\n0.05\n0.09\n0.13\n0.18\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(b) Non-linear (CES) specification (𝑟= 1\/2).\nFigure 4: The mean squared error (MSE) of the actual play dis-\ntribution relative to the best-response utility, when matched\nwith a partner playing the human distribution. The errors\nare calculated for each possible preference 𝑏in the objective\nfunction (Eq. 1), and the average across all games is plotted.\nParticularly, 𝑏= 1 corresponds to purely selfish preferences,\n𝑏= 0 represents purely selfless preferences, and 𝑏= 0.5 re-\nflects a preference for maximizing the combined payoff of\nboth players. 𝑟is a specification parameter set as 𝑟= 1 and\n𝑟= 1\/2.\nFigure 4 displays the optimization errors for various values of\n𝑏for human players and each AI chatbot, computed under the as-\nsumption that the AI chatbots are interacting with a random human\nplayer. A lower optimization error indicates greater optimization\nefficiency, suggesting that the model is more likely to be optimizing\nfor that particular objective.\nThe figure reveals that, in both utility specifications (𝑟= 1 and\n𝑟= 1\/2), AI chatbots place a stronger emphasis on fairness, as\nindicated by the lowest optimization error consistently occurring\nHow Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games\nat 𝑏= 0.5. In contrast, human players exhibit a slight preference for\nselfishness, with their lowest optimization error occurring at𝑏= 0.6.\nAdditionally, AI chatbots demonstrate significantly higher optimiza-\ntion efficiency than humans when maximizing the partner’s payoff\n(𝑏= 0), but they exhibit lower efficiencies when optimizing their\nown payoff (𝑏= 1).\nDifferent AI models exhibit varying levels of optimization ef-\nficiency. For example, when optimizing for the partner’s payoff\n(𝑏= 0) or the combined payoff ( b=0.5 ), Mistral Large 2 achieves\nthe highest optimization efficiency, followed by OpenAI GPT-4o.\nIn contrast, Google Gemini 1.5 Pro and Meta Llama 3.1 405B show\nrelatively lower optimization efficiency in these scenarios. When\noptimizing for their own payoff (𝑏= 1), all chatbots perform with\nhigher errors compared to humans, with Google Gemini 1.5 Pro\ndisplaying an even greater error than the other models.\nDetailed optimization error plots for each individual game are\nprovided in Figures 6–7 in the Appendix.\nLogistic multinomial model fitting. In addition to the optimiza-\ntion efficiency analysis, a logistic multinomial model is also fitted\nto predict the behavior of AI chatbots. Following Mei et al. [7] , we\nassume AI takes action 𝑘with a probability\nPr(𝑘) =\nexp(𝑈𝑏(𝑘))\nÍ\n𝑗≤𝐾exp(𝑈𝑏(𝑗)) ,\n(2)\nwhere 𝐾is the number of all possible action choices.\nTable 3 summarizes the estimated 𝑏values based on the assumed\nlogistic multinomial model. The table reveals that in nearly all\ngames (except for the Ultimatum Game), the majority of AI chatbots\nexhibit estimated 𝑏values significantly lower than those of random\nhuman players. This indicates that, on average, AI chatbots tend to\nbe more selfless compared to human players.\n3.5\nBehavior Inconsistency\nPlayer\nInconsistency\n𝑟= 1.0\n𝑟= 0.5\nHuman players\n0.114\n0.122\nOpenAI GPT 4o\n0.115\n0.107\nMeta Llama 3.1 405B\n0.125\n0.125\nGoogle Gemini 1.5 Pro\n0.118\n0.139\nAnthropic Claude 3.5 Sonnet\n0.143\n0.133\nMistral Large 2\n0.108\n0.100\nTable 2: Behavior inconsistency across games of AI chatbots.\nThe inconsistency is estimated by the mean absolute error\nof payoff curves (Fig. 6-7 in the Appendix).\nAlthough AI chatbots generally emphasize fairness and exhibit\nmore selfless tendencies, they can display inconsistent behavior\nacross different scenarios. For instance, a significant portion of\nGoogle Gemini 1.5 Pro instances choose to split the money fairly\nin the Dictator Game, yet in the Ultimatum - Proposer role, many\ninstances propose offering nearly all the money ($90-$99) to the\npartner, reflecting an altruistic trait.\nTable 2 (and Table 4 in the Appendix) provides the estimated\nbehavior inconsistencies of AI chatbots. These inconsistencies are\nmeasured using the mean absolute error (MAE) of payoff curves\nacross different games (see Figures 6–7 in the Appendix).\nAmong the flagship AI chatbots, Mistral Large 2 demonstrates\nthe highest consistency across all games, as its own payoff and\nthe partner’s payoff are well-balanced in most scenarios (Fig. 1).\nIn contrast, Google Gemini 1.5 Pro and Anthropic Claude 3.5 Son-\nnet exhibit higher levels of inconsistency, even surpassing that of\nhuman players – despite the fact that human behaviors reflect the\ndiversity of a heterogeneous population.\n4\nDiscussion\nModel checkpoints. As AI chatbots evolve, their behavioral ten-\ndencies shift over time. Figures 8(i,ii) illustrate these changes across\ncheckpoints for OpenAI GPT-4 and GPT-3 models. For GPT-4, ex-\ncept for the Bomb Risk Game, the latest checkpoint (gpt-4-0125-preview)\nproduces more concentrated behavior distributions compared to\nolder checkpoints. The updated version also demonstrates higher\nrationality in the Ultimatum - Responder Game but shows increased\nrisk aversion in the Bomb Risk Game. For GPT-3, while the distribu-\ntion modes largely remain consistent, the behavior distributions for\nthe Ultimatum - Responder, Trust - Banker, and Bomb Risk games\nhave become less diverse over successive updates.\nModel size. In addition to different checkpoints, variations in\nmodel size can also influence behavior. As shown in Figures 8(iii,iv),\nMeta Llama 3 8B behaves notably differently from the 70B version.\nIn games like Ultimatum - Proposer, Trust - Investor, Public Goods,\nand Prisoner’s Dilemma, the 8B model exhibits more conservative\ntendencies. For both Llama 3 and Anthropic Claude 3, smaller mod-\nels (e.g., Llama 3 8B, Claude 3 Sonnet, and Claude 3 Haiku) display\nhigher diversity in their behavior distributions compared to their\nlarger counterparts.\n5\nConclusion and Future Work\nThis study benchmarked LLM-based AI chatbots across a series of\nbehavioral economics games. The analyses revealed the following\ncommon and distinct behavioral patterns of the chatbots: (1) All\ntested chatbots successfully capture specific human behavior modes,\nleading to highly concentrated decision distributions; (2) Although\nflagship chatbots demonstrate a notable probability of passing the\nTuring test, AI chatbots can merely produce a behavior distribution\nsimilar to humans; (3) Compared to humans, AI chatbots place\ngreater emphasis on maximizing fairness in their payoff prefer-\nences; (4) AI chatbots may exhibit inconsistencies in their payoff\npreferences across different games; (5) Different AI chatbots exhibit\ndistinct behavioral patterns in games, which can be further distin-\nguished in our analyses. These findings highlight the effectiveness\nof our behavioral benchmark in profiling and differentiating AI\nchatbots.\nWe hope our research contributes to a deeper understanding of\nAI behaviors and serves as a foundation for future studies in AI\nbehavioral science. For example, the discrepancies between Turing\ntest results and distribution dissimilarities highlight the need for\nfurther alignment objectives that enable LLMs to better represent\nthe diversity of the human behaviors. Additionally, the observed\ninconsistencies in AI behaviors across games underscore the im-\nportance of developing generalizable preferences and objectives for\nAI systems that can adapt effectively across various scenarios.\nXie et al.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\n2023. Sparks of artificial general intelligence: Early experiments with gpt-4.\narXiv preprint arXiv:2303.12712 (2023).\n[2] Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong. 2023. The emergence\nof economic rationality of GPT. Proceedings of the National Academy of Sciences\n120, 51 (2023), e2316205120.\n[3] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. Gpts\nare gpts: An early look at the labor market impact potential of large language\nmodels. arXiv preprint arXiv:2303.10130 (2023).\n[4] Karan Girotra, Lennart Meincke, Christian Terwiesch, and Karl T Ulrich. 2023.\nIdeas are dimes a dozen: Large language models for idea generation in innovation.\nAvailable at SSRN 4526071 (2023).\n[5] Peter Lee, Sebastien Bubeck, and Joseph Petro. 2023. Benefits, limits, and risks\nof GPT-4 as an AI chatbot for medicine. New England Journal of Medicine 388,\n13 (2023), 1233–1239.\n[6] Daniel McFadden. 1963. Constant elasticity of substitution production functions.\nThe Review of Economic Studies 30, 2 (1963), 73–83.\n[7] Qiaozhu Mei, Yutong Xie, Walter Yuan, and Matthew O Jackson. 2024. A Turing\ntest of whether AI chatbots are behaviorally similar to humans. Proceedings of\nthe National Academy of Sciences 121, 9 (2024), e2313925121.\n[8] Scott Shackelford, Lawrence J Trautman, and W Gregory Voss. 2023. How\nWe Learned to Stop Worrying and Love AI: Analyzing the Rapid Evolution of\nGenerative Pre-Trained Transformer (GPT) and its Impacts on Law, Business,\nand Society. Business, and Society (July 20, 2023) (2023).\n[9] Alan M. Turing. 1950. Computing machinery and intelligence. MIND: A Quarterly\nReview of Psychology and Philosophy 54, 236 (1950), 433–460.\n[10] Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu,\nPhilip Torr, Bernard Ghanem, and Guohao Li. 2024. Can Large Language Model\nAgents Simulate Human Trust Behaviors? arXiv preprint arXiv:2402.04559 (2024).\nAppendix\nHow Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games\nHuman\ngemini-1.5-pro-latest\ngemini-1.0-pro-001\nmistral-large-2407\nclaude-3-haiku-20240307\nmistral-large-2402\nclaude-3-opus-20240229\nclaude-3-5-sonnet-20240620\nllama-3.1-405B-instruct\nllama-3-70b-chat\ngpt-3.5-turbo-0125\ngpt-4-0125-preview\ngpt-4o-2024-05-13\ngpt-4-0613\ngpt-4o-mini-2024-07-18\nclaude-3-sonnet-20240229\nllama-3-8b-chat\ngpt-3.5-turbo-0613\nHuman\ngemini-1.5-pro-latest\ngemini-1.0-pro-001\nmistral-large-2407\nclaude-3-haiku-20240307\nmistral-large-2402\nclaude-3-opus-20240229\nclaude-3-5-sonnet-20240620\nllama-3.1-405B-instruct\nllama-3-70b-chat\ngpt-3.5-turbo-0125\ngpt-4-0125-preview\ngpt-4o-2024-05-13\ngpt-4-0613\ngpt-4o-mini-2024-07-18\nclaude-3-sonnet-20240229\nllama-3-8b-chat\ngpt-3.5-turbo-0613\n0.00\n22.24\n26.66\n20.46\n21.22\n20.71\n23.41\n20.69\n22.28\n24.46\n19.02\n25.47\n24.02\n20.38\n23.84\n27.39\n21.86\n17.24\n22.24\n0.00\n30.88\n20.55\n23.75\n22.86\n18.59\n21.90\n23.25\n26.83\n22.88\n19.02\n19.62\n19.26\n23.17\n24.47\n26.82\n26.50\n26.66\n30.88\n0.00\n16.80\n16.38\n20.90\n22.58\n20.37\n14.25\n21.07\n20.12\n27.06\n25.34\n22.98\n27.32\n15.65\n17.86\n24.68\n20.46\n20.55\n16.80\n0.00\n5.29\n8.03\n8.23\n8.17\n4.45\n11.58\n10.66\n10.49\n9.76\n11.15\n14.74\n11.31\n11.37\n16.82\n21.22\n23.75\n16.38\n5.29\n0.00\n6.76\n8.15\n7.22\n8.02\n10.78\n9.90\n11.38\n11.30\n10.39\n14.66\n11.01\n13.65\n13.99\n20.71\n22.86\n20.90\n8.03\n6.76\n0.00\n10.91\n9.24\n10.46\n9.42\n10.68\n10.59\n10.92\n11.60\n12.82\n15.61\n12.52\n12.95\n23.41\n18.59\n22.58\n8.23\n8.15\n10.91\n0.00\n6.00\n11.92\n14.42\n12.17\n6.50\n10.02\n7.77\n13.79\n10.45\n18.26\n18.20\n20.69\n21.90\n20.37\n8.17\n7.22\n9.24\n6.00\n0.00\n11.95\n11.30\n10.57\n9.66\n10.99\n9.50\n12.08\n13.20\n15.83\n16.32\n22.28\n23.25\n14.25\n4.45\n8.02\n10.46\n11.92\n11.95\n0.00\n14.24\n13.37\n13.59\n12.52\n14.12\n17.49\n15.22\n7.14\n18.47\n24.46\n26.83\n21.07\n11.58\n10.78\n9.42\n14.42\n11.30\n14.24\n0.00\n10.76\n17.41\n17.17\n19.29\n16.63\n16.91\n12.81\n18.97\n19.02\n22.88\n20.12\n10.66\n9.90\n10.68\n12.17\n10.57\n13.37\n10.76\n0.00\n14.14\n13.48\n16.26\n10.91\n17.02\n12.20\n11.44\n25.47\n19.02\n27.06\n10.49\n11.38\n10.59\n6.50\n9.66\n13.59\n17.41\n14.14\n0.00\n4.64\n7.84\n8.57\n12.72\n20.73\n17.85\n24.02\n19.62\n25.34\n9.76\n11.30\n10.92\n10.02\n10.99\n12.52\n17.17\n13.48\n4.64\n0.00\n8.45\n5.80\n14.87\n19.14\n17.43\n20.38\n19.26\n22.98\n11.15\n10.39\n11.60\n7.77\n9.50\n14.12\n19.29\n16.26\n7.84\n8.45\n0.00\n13.72\n13.12\n21.26\n16.50\n23.84\n23.17\n27.32\n14.74\n14.66\n12.82\n13.79\n12.08\n17.49\n16.63\n10.91\n8.57\n5.80\n13.72\n0.00\n19.11\n16.86\n16.90\n27.39\n24.47\n15.65\n11.31\n11.01\n15.61\n10.45\n13.20\n15.22\n16.91\n17.02\n12.72\n14.87\n13.12\n19.11\n0.00\n21.25\n21.53\n21.86\n26.82\n17.86\n11.37\n13.65\n12.52\n18.26\n15.83\n7.14\n12.81\n12.20\n20.73\n19.14\n21.26\n16.86\n21.25\n0.00\n17.67\n17.24\n26.50\n24.68\n16.82\n13.99\n12.95\n18.20\n16.32\n18.47\n18.97\n11.44\n17.85\n17.43\n16.50\n16.90\n21.53\n17.67\n0.00\n0\n5\n10\n15\n20\n25\n30\nValue of Wasserstein distance\nFigure 5: Pairwise behavior distribution dissimilarities estimated with Wasserstein distance.\nXie et al.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nOptimization squared error\n0.34\n0.28\n0.22\n0.15\n0.09\n0.05\n0.04\n0.06\n0.09\n0.13\n0.17\n0.20\n0.17\n0.13\n0.09\n0.06\n0.03\n0.04\n0.07\n0.11\n0.15\n0.20\n0.24\n0.21\n0.17\n0.13\n0.09\n0.06\n0.06\n0.08\n0.12\n0.16\n0.19\n0.23\n0.19\n0.15\n0.11\n0.08\n0.05\n0.05\n0.08\n0.13\n0.19\n0.26\n0.22\n0.19\n0.15\n0.11\n0.07\n0.04\n0.04\n0.07\n0.11\n0.16\n0.21\n0.19\n0.16\n0.12\n0.09\n0.05\n0.02\n0.03\n0.06\n0.10\n0.14\n0.18\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(a) Average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nOptimization squared error\n0.60\n0.47\n0.34\n0.20\n0.07\n0.00\n0.01\n0.04\n0.06\n0.09\n0.11\n0.25\n0.20\n0.14\n0.08\n0.03\n0.00\n0.03\n0.08\n0.14\n0.20\n0.25\n0.25\n0.20\n0.14\n0.08\n0.03\n0.00\n0.03\n0.08\n0.14\n0.20\n0.25\n0.23\n0.18\n0.13\n0.07\n0.03\n0.00\n0.03\n0.10\n0.17\n0.23\n0.30\n0.28\n0.22\n0.16\n0.09\n0.03\n0.00\n0.03\n0.08\n0.13\n0.18\n0.23\n0.25\n0.20\n0.14\n0.08\n0.03\n0.00\n0.03\n0.08\n0.14\n0.20\n0.25\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(b) Dictator\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nOptimization squared error\n0.47\n0.43\n0.38\n0.32\n0.25\n0.18\n0.14\n0.14\n0.13\n0.14\n0.15\n0.36\n0.31\n0.25\n0.18\n0.12\n0.06\n0.04\n0.03\n0.03\n0.02\n0.02\n0.29\n0.24\n0.18\n0.12\n0.05\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.18\n0.17\n0.15\n0.13\n0.11\n0.11\n0.16\n0.27\n0.44\n0.67\n0.57\n0.51\n0.44\n0.36\n0.27\n0.16\n0.11\n0.09\n0.07\n0.06\n0.05\n0.31\n0.26\n0.20\n0.14\n0.07\n0.02\n0.01\n0.01\n0.01\n0.00\n0.00\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(c) Ultimatum - Proposer\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nOptimization squared error\n0.47\n0.43\n0.38\n0.32\n0.25\n0.18\n0.14\n0.14\n0.13\n0.14\n0.15\n0.36\n0.31\n0.25\n0.18\n0.12\n0.06\n0.04\n0.03\n0.03\n0.02\n0.02\n0.29\n0.24\n0.18\n0.12\n0.05\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.18\n0.17\n0.15\n0.13\n0.11\n0.11\n0.16\n0.27\n0.44\n0.67\n0.57\n0.51\n0.44\n0.36\n0.27\n0.16\n0.11\n0.09\n0.07\n0.06\n0.05\n0.31\n0.26\n0.20\n0.14\n0.07\n0.02\n0.01\n0.01\n0.01\n0.00\n0.00\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(d) Ultimatum - Responder\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nOptimization squared error\n0.59\n0.53\n0.46\n0.38\n0.29\n0.20\n0.18\n0.18\n0.18\n0.19\n0.19\n0.47\n0.42\n0.35\n0.28\n0.20\n0.11\n0.12\n0.14\n0.16\n0.18\n0.20\n0.47\n0.42\n0.35\n0.28\n0.20\n0.11\n0.12\n0.14\n0.16\n0.18\n0.20\n0.52\n0.47\n0.41\n0.33\n0.25\n0.17\n0.16\n0.17\n0.18\n0.19\n0.21\n0.45\n0.40\n0.33\n0.26\n0.18\n0.10\n0.11\n0.13\n0.15\n0.17\n0.19\n0.47\n0.42\n0.35\n0.28\n0.20\n0.11\n0.12\n0.14\n0.16\n0.18\n0.20\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(e) Trust - Investor\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\nOptimization squared error\n0.24\n0.19\n0.14\n0.08\n0.03\n0.00\n0.02\n0.06\n0.11\n0.17\n0.22\n0.08\n0.06\n0.05\n0.03\n0.01\n0.00\n0.03\n0.10\n0.19\n0.29\n0.40\n0.14\n0.11\n0.08\n0.05\n0.02\n0.00\n0.02\n0.06\n0.12\n0.18\n0.25\n0.13\n0.10\n0.07\n0.04\n0.01\n0.00\n0.02\n0.07\n0.14\n0.21\n0.29\n0.07\n0.06\n0.04\n0.02\n0.01\n0.00\n0.03\n0.11\n0.21\n0.32\n0.44\n0.13\n0.10\n0.07\n0.04\n0.01\n0.00\n0.02\n0.07\n0.13\n0.20\n0.27\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(f) Trust - Banker\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nOptimization squared error\n0.18\n0.14\n0.09\n0.05\n0.02\n0.00\n0.01\n0.04\n0.08\n0.12\n0.16\n0.12\n0.09\n0.06\n0.03\n0.01\n0.00\n0.01\n0.03\n0.06\n0.10\n0.13\n0.12\n0.09\n0.06\n0.03\n0.01\n0.00\n0.01\n0.03\n0.07\n0.10\n0.14\n0.26\n0.20\n0.13\n0.07\n0.02\n0.00\n0.01\n0.03\n0.05\n0.08\n0.10\n0.12\n0.09\n0.06\n0.03\n0.01\n0.00\n0.01\n0.03\n0.07\n0.10\n0.14\n0.10\n0.07\n0.05\n0.03\n0.01\n0.00\n0.01\n0.04\n0.08\n0.13\n0.17\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(g) Public - Goods\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nOptimization squared error\n0.28\n0.22\n0.16\n0.10\n0.05\n0.01\n0.00\n0.02\n0.07\n0.12\n0.18\n0.03\n0.02\n0.02\n0.01\n0.01\n0.00\n0.00\n0.05\n0.14\n0.25\n0.37\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.15\n0.26\n0.39\n0.20\n0.16\n0.11\n0.07\n0.03\n0.01\n0.00\n0.03\n0.09\n0.16\n0.23\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.15\n0.26\n0.39\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.15\n0.26\n0.39\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(h) Prisoner’s Dilemma\nFigure 6: Mean squared error of the actual distribution of play relative to the best-response payoff, when matched with a\npartner playing the human distribution. The average is across all games. The errors are reported for each possible b, which is\nthe weight on own vs partner payoff in the utility function (linear blend, with CES specification r = 1). b = 1 is the purely selfish\n(own) payoff, b = 0 is the purely selfless (partner) payoff, and b = 1\/2 is the overall welfare (average) payoff. The values of mean\nsquare errors are annotated in the plots.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nOptimization squared error\n0.34\n0.28\n0.21\n0.15\n0.10\n0.06\n0.05\n0.07\n0.09\n0.13\n0.17\n0.20\n0.14\n0.09\n0.05\n0.03\n0.03\n0.03\n0.06\n0.10\n0.15\n0.20\n0.24\n0.18\n0.12\n0.08\n0.06\n0.06\n0.05\n0.07\n0.11\n0.15\n0.19\n0.23\n0.18\n0.13\n0.09\n0.07\n0.06\n0.08\n0.11\n0.16\n0.21\n0.26\n0.22\n0.16\n0.11\n0.07\n0.05\n0.04\n0.04\n0.06\n0.10\n0.15\n0.21\n0.19\n0.13\n0.08\n0.04\n0.03\n0.02\n0.02\n0.05\n0.09\n0.13\n0.18\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(a) Average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nOptimization squared error\n0.60\n0.51\n0.40\n0.28\n0.16\n0.08\n0.03\n0.02\n0.03\n0.07\n0.11\n0.25\n0.15\n0.07\n0.02\n0.00\n0.00\n0.00\n0.02\n0.07\n0.15\n0.25\n0.25\n0.15\n0.07\n0.02\n0.00\n0.00\n0.00\n0.02\n0.07\n0.15\n0.25\n0.23\n0.14\n0.06\n0.02\n0.01\n0.01\n0.02\n0.05\n0.11\n0.20\n0.30\n0.28\n0.18\n0.09\n0.03\n0.01\n0.00\n0.00\n0.02\n0.06\n0.14\n0.23\n0.25\n0.15\n0.07\n0.02\n0.00\n0.00\n0.00\n0.02\n0.07\n0.15\n0.25\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(b) Dictator\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nOptimization squared error\n0.47\n0.39\n0.32\n0.25\n0.21\n0.18\n0.17\n0.17\n0.16\n0.15\n0.15\n0.36\n0.26\n0.17\n0.10\n0.07\n0.05\n0.04\n0.03\n0.03\n0.02\n0.02\n0.29\n0.19\n0.10\n0.04\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.18\n0.16\n0.15\n0.15\n0.18\n0.26\n0.37\n0.48\n0.58\n0.67\n0.57\n0.47\n0.37\n0.27\n0.19\n0.14\n0.11\n0.09\n0.08\n0.06\n0.05\n0.31\n0.21\n0.12\n0.06\n0.02\n0.01\n0.01\n0.01\n0.01\n0.00\n0.00\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(c) Ultimatum - Proposer\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\nOptimization squared error\n0.23\n0.22\n0.20\n0.19\n0.18\n0.16\n0.15\n0.13\n0.12\n0.11\n0.09\n0.06\n0.06\n0.05\n0.05\n0.04\n0.04\n0.03\n0.03\n0.02\n0.02\n0.01\n0.41\n0.38\n0.36\n0.33\n0.30\n0.27\n0.25\n0.22\n0.19\n0.16\n0.14\n0.06\n0.05\n0.05\n0.05\n0.04\n0.04\n0.04\n0.03\n0.03\n0.03\n0.02\n0.02\n0.02\n0.02\n0.01\n0.01\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\n0.05\n0.04\n0.04\n0.03\n0.02\n0.02\n0.01\n0.01\n0.01\n0.00\n0.00\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(d) Ultimatum - Responder\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nOptimization squared error\n0.59\n0.51\n0.43\n0.35\n0.29\n0.25\n0.22\n0.21\n0.20\n0.20\n0.19\n0.47\n0.38\n0.27\n0.17\n0.13\n0.11\n0.11\n0.13\n0.15\n0.17\n0.20\n0.47\n0.38\n0.27\n0.17\n0.13\n0.11\n0.11\n0.13\n0.15\n0.17\n0.20\n0.52\n0.45\n0.36\n0.28\n0.23\n0.20\n0.18\n0.18\n0.19\n0.20\n0.21\n0.45\n0.36\n0.25\n0.16\n0.11\n0.10\n0.10\n0.12\n0.14\n0.17\n0.19\n0.47\n0.38\n0.27\n0.17\n0.13\n0.11\n0.11\n0.13\n0.15\n0.17\n0.20\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(e) Trust - Investor\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.0\n0.1\n0.2\n0.3\n0.4\nOptimization squared error\n0.24\n0.16\n0.09\n0.04\n0.01\n0.01\n0.03\n0.07\n0.12\n0.17\n0.22\n0.08\n0.04\n0.01\n0.00\n0.00\n0.00\n0.03\n0.10\n0.20\n0.30\n0.40\n0.14\n0.07\n0.03\n0.00\n0.00\n0.00\n0.01\n0.05\n0.11\n0.18\n0.25\n0.13\n0.07\n0.02\n0.00\n0.00\n0.00\n0.02\n0.06\n0.13\n0.21\n0.28\n0.07\n0.03\n0.01\n0.00\n0.00\n0.00\n0.04\n0.12\n0.23\n0.34\n0.44\n0.13\n0.07\n0.02\n0.00\n0.00\n0.00\n0.01\n0.06\n0.12\n0.20\n0.27\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(f) Trust - Banker\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nOptimization squared error\n0.18\n0.13\n0.08\n0.04\n0.01\n0.00\n0.01\n0.04\n0.08\n0.12\n0.16\n0.12\n0.08\n0.04\n0.01\n0.00\n0.00\n0.00\n0.02\n0.05\n0.09\n0.13\n0.12\n0.08\n0.04\n0.01\n0.00\n0.00\n0.00\n0.02\n0.05\n0.09\n0.14\n0.26\n0.19\n0.12\n0.06\n0.02\n0.00\n0.01\n0.02\n0.05\n0.08\n0.10\n0.12\n0.08\n0.04\n0.01\n0.00\n0.00\n0.00\n0.02\n0.05\n0.09\n0.14\n0.10\n0.06\n0.03\n0.01\n0.00\n0.00\n0.01\n0.03\n0.07\n0.12\n0.17\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(g) Public - Goods\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nb\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nOptimization squared error\n0.28\n0.22\n0.16\n0.10\n0.05\n0.01\n0.00\n0.03\n0.07\n0.12\n0.18\n0.03\n0.02\n0.02\n0.01\n0.01\n0.00\n0.01\n0.06\n0.15\n0.26\n0.37\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.16\n0.28\n0.39\n0.20\n0.16\n0.12\n0.08\n0.04\n0.01\n0.00\n0.04\n0.10\n0.17\n0.23\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.16\n0.28\n0.39\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.16\n0.28\n0.39\nHuman\nOpenAI GPT 4o\nMeta Llama 3.1 405B\nGoogle Gemini 1.5 Pro\nAnthropic Claude 3.5 Sonnet\nMistral Large 2\n(h) Prisoner’s Dilemma\nFigure 7: Mean squared error of the actual distribution of play relative to the best-response payoff, when matched with a\npartner playing the human distribution. The average is across all games. The errors are reported for each possible b, which is\nthe weight on own vs partner payoff in the utility function (non-linear blend, with CES specification r = 1\/2). b = 1 is the purely\nselfish (own) payoff, and b = 0 is the purely selfless (partner) payoff. The values of mean square errors are annotated in the\nplots.\nHow Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games\nTable 3: Estimation of the weight 𝑏by multinomial logit discrete choice analysis.\nGame\nPlayer\nWith CES specification 𝑟= 1\nWith CES specification 𝑟= 1\/2\nEstimated 𝑏\nStandard error\nConfidence interval\nEstimated 𝑏\nStandard error\nConfidence interval\nDictator\nHuman\n0.517\n0.000\n(0.516, 0.517)\n0.658\n0.000\n(0.657, 0.659)\nOpenAI GPT 4o\n0.500\n0.002\n(0.495, 0.505)\n0.500\n0.002\n(0.495, 0.505)\nMeta Llama 3.1 405B\n0.500\n0.002\n(0.495, 0.505)\n0.500\n0.002\n(0.495, 0.505)\nGoogle Gemini 1.5 Pro\n0.498\n0.002\n(0.493, 0.503)\n0.498\n0.002\n(0.493, 0.503)\nAnthropic Claude 3.5 Sonnet\n0.501\n0.002\n(0.497, 0.506)\n0.501\n0.002\n(0.497, 0.506)\nMistral Large 2\n0.500\n0.002\n(0.495, 0.505)\n0.500\n0.002\n(0.495, 0.505)\nUltimatum -\nProposer\nHuman\n1.000\n0.005\n(0.989, 1.011)\n1.000\n0.005\n(0.990, 1.01)\nOpenAI GPT 4o\n1.000\n0.059\n(0.884, 1.116)\n1.000\n0.059\n(0.884, 1.116)\nMeta Llama 3.1 405B\n1.000\n0.059\n(0.884, 1.116)\n1.000\n0.059\n(0.884, 1.116)\nGoogle Gemini 1.5 Pro\n0.503\n0.006\n(0.492, 0.514)\n0.503\n0.006\n(0.492, 0.514)\nAnthropic Claude 3.5 Sonnet\n1.000\n0.059\n(0.884, 1.116)\n1.000\n0.059\n(0.884, 1.116)\nMistral Large 2\n1.000\n0.059\n(0.884, 1.116)\n1.000\n0.059\n(0.884, 1.116)\nUltimatum -\nResponder\nHuman\n1.000\n0.005\n(0.990, 1.010)\n1.000\n0.006\n(0.989, 1.011)\nOpenAI GPT 4o\n0.994\n0.054\n(0.887, 1.100)\n1.000\n0.060\n(0.883, 1.117)\nMeta Llama 3.1 405B\n1.000\n0.054\n(0.893, 1.107)\n1.000\n0.063\n(0.877, 1.123)\nGoogle Gemini 1.5 Pro\n0.647\n0.047\n(0.556, 0.739)\n0.635\n0.049\n(0.539, 0.731)\nAnthropic Claude 3.5 Sonnet\n1.000\n0.054\n(0.893, 1.107)\n1.000\n0.057\n(0.888, 1.112)\nMistral Large 2\n1.000\n0.054\n(0.893, 1.107)\n1.000\n0.057\n(0.889, 1.111)\nTrust -\nInvestor\nHuman\n0.535\n0.000\n(0.535, 0.535)\n0.570\n0.000\n(0.569, 0.570)\nOpenAI GPT 4o\n0.530\n0.002\n(0.525, 0.535)\n0.564\n0.002\n(0.559, 0.569)\nMeta Llama 3.1 405B\n0.530\n0.002\n(0.525, 0.535)\n0.564\n0.002\n(0.559, 0.569)\nGoogle Gemini 1.5 Pro\n0.528\n0.003\n(0.523, 0.533)\n0.563\n0.003\n(0.557, 0.568)\nAnthropic Claude 3.5 Sonnet\n0.529\n0.003\n(0.524, 0.534)\n0.563\n0.003\n(0.558, 0.568)\nMistral Large 2\n0.530\n0.002\n(0.525, 0.535)\n0.564\n0.002\n(0.559, 0.569)\nTrust -\nBanker\nHuman\n0.504\n0.000\n(0.504, 0.505)\n0.475\n0.001\n(0.473, 0.477)\nOpenAI GPT 4o\n0.495\n0.002\n(0.491, 0.498)\n0.381\n0.005\n(0.371, 0.391)\nMeta Llama 3.1 405B\n0.500\n0.002\n(0.497, 0.503)\n0.435\n0.005\n(0.425, 0.445)\nGoogle Gemini 1.5 Pro\n0.499\n0.002\n(0.496, 0.502)\n0.422\n0.005\n(0.412, 0.432)\nAnthropic Claude 3.5 Sonnet\n0.494\n0.002\n(0.490, 0.497)\n0.368\n0.005\n(0.358, 0.378)\nMistral Large 2\n0.499\n0.002\n(0.496, 0.502)\n0.427\n0.005\n(0.417, 0.437)\nPublic -\nGoods\nHuman\n0.526\n0.001\n(0.524, 0.528)\n0.518\n0.001\n(0.516, 0.521)\nOpenAI GPT 4o\n0.502\n0.023\n(0.456, 0.548)\n0.489\n0.025\n(0.439, 0.538)\nMeta Llama 3.1 405B\n0.500\n0.023\n(0.454, 0.546)\n0.486\n0.025\n(0.436, 0.536)\nGoogle Gemini 1.5 Pro\n0.606\n0.026\n(0.555, 0.657)\n0.621\n0.029\n(0.563, 0.678)\nAnthropic Claude 3.5 Sonnet\n0.468\n0.024\n(0.422, 0.514)\n0.448\n0.025\n(0.398, 0.498)\nMistral Large 2\n0.500\n0.023\n(0.454, 0.546)\n0.486\n0.025\n(0.436, 0.536)\nPrisoner’s -\nDilemma\nHuman\n0.572\n0.000\n(0.572, 0.572)\n0.563\n0.000\n(0.563, 0.563)\nOpenAI GPT 4o\n0.567\n0.001\n(0.566, 0.569)\n0.559\n0.001\n(0.557, 0.561)\nMeta Llama 3.1 405B\n0.566\n0.001\n(0.563, 0.569)\n0.557\n0.001\n(0.555, 0.560)\nGoogle Gemini 1.5 Pro\n0.571\n0.000\n(0.570, 0.572)\n0.562\n0.000\n(0.562, 0.563)\nAnthropic Claude 3.5 Sonnet\n0.566\n0.001\n(0.563, 0.569)\n0.557\n0.001\n(0.555, 0.560)\nMistral Large 2\n0.566\n0.001\n(0.563, 0.569)\n0.557\n0.001\n(0.555, 0.560)\n∗: To be comparable, the Trust-Banker calculations are done assuming that the original investment is $50.\n† : The Prisoner’s Dilemma reports the estimation results in the first round of the game.\nXie et al.\nAI Chatbot family\nPlayer\nInconsistency\n𝑟= 1.0\n𝑟= 0.5\nHuman players\n0.114\n0.122\nOpenAI GPT\ngpt4-4o-2024-05-31\n0.115\n0.107\ngpt4-4o-mini-2024-07-18\n0.148\n0.140\ngpt-4-0125-preview\n0.124\n0.115\ngpt-4-0613\n0.096\n0.090\ngpt-3.5-turbo-0125\n0.160\n0.154\ngpt-3.5-turbo-0613\n0.168\n0.185\nMeta Llama\nllama-3.1-405B-instruct\n0.125\n0.125\nllama-3-70b-chat\n0.205\n0.201\nllama-3-8b-chat\n0.183\n0.182\nGoogle Gemini\ngemini-1.5-pro-latest\n0.118\n0.139\ngemini-1.0-pro-001\n0.146\n0.152\nAnthropic Claude\nclaude-3-5-sonnet-20240620\n0.143\n0.133\nclaude-3-opus-20240229\n0.115\n0.104\nclaude-3-sonnet-20240229\n0.117\n0.111\nclaude-3-haiku-20240307\n0.123\n0.115\nMistral\nmistral-large-2407\n0.108\n0.100\nmistral-large-2402\n0.134\n0.128\nTable 4: Behavior inconsistency across games of AI chatbots. The inconsistency is estimated by the mean absolute error of\npayoff curves.\nHow Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games\nFigure 8: Distributions of AI chatbot behaviors in behavioral economics games.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games.pdf"}
{"title":"AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games","authors":"Kefan Su, Yusen Huo, Zhilin Zhang, Shuai Dou, Chuan Yu, Jian Xu, Zongqing Lu, Bo Zheng","summary":"Decision-making in large-scale games is an essential research area in\nartificial intelligence (AI) with significant real-world impact. However, the\nlimited access to realistic large-scale game environments has hindered research\nprogress in this area. In this paper, we present AuctionNet, a benchmark for\nbid decision-making in large-scale ad auctions derived from a real-world online\nadvertising platform. AuctionNet is composed of three parts: an ad auction\nenvironment, a pre-generated dataset based on the environment, and performance\nevaluations of several baseline bid decision-making algorithms. More\nspecifically, the environment effectively replicates the integrity and\ncomplexity of real-world ad auctions through the interaction of several\nmodules: the ad opportunity generation module employs deep generative networks\nto bridge the gap between simulated and real-world data while mitigating the\nrisk of sensitive data exposure; the bidding module implements diverse\nauto-bidding agents trained with different decision-making algorithms; and the\nauction module is anchored in the classic Generalized Second Price (GSP)\nauction but also allows for customization of auction mechanisms as needed. To\nfacilitate research and provide insights into the environment, we have also\npre-generated a substantial dataset based on the environment. The dataset\ncontains 10 million ad opportunities, 48 diverse auto-bidding agents, and over\n500 million auction records. Performance evaluations of baseline algorithms\nsuch as linear programming, reinforcement learning, and generative models for\nbid decision-making are also presented as a part of AuctionNet. We believe that\nAuctionNet is applicable not only to research on bid decision-making in ad\nauctions but also to the general area of decision-making in large-scale games.","url":"http:\/\/arxiv.org\/abs\/2412.10798v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.10798v2","published":1734175881000,"comment":null,"pdf_text":"AuctionNet: A Novel Benchmark for Decision-Making\nin Large-Scale Games\nKefan Su1,2∗, Yusen Huo2, Zhilin Zhang2, Shuai Dou2, Chuan Yu2, Jian Xu2†,\nZongqing Lu1†, Bo Zheng2\n1School of Computer Science, Peking University\n2Alibaba Group\n1{sukefan,zongqing.lu}@pku.edu.cn\n2 {huoyusen.huoyusen,zhangzhilin.pt,doushuai.ds,\nyuchuan.yc,xiyu.xj,bozheng}@alibaba-inc.com\nAbstract\nDecision-making in large-scale games is an essential research area in artificial\nintelligence (AI) with significant real-world impact. However, the limited access to\nrealistic large-scale game environments has hindered research progress in this area.\nIn this paper, we present AuctionNet, a benchmark for bid decision-making in large-\nscale ad auctions derived from a real-world online advertising platform. AuctionNet\nis composed of three parts: an ad auction environment, a pre-generated dataset\nbased on the environment, and performance evaluations of several baseline bid\ndecision-making algorithms. More specifically, the environment effectively repli-\ncates the integrity and complexity of real-world ad auctions through the interaction\nof several modules: the ad opportunity generation module employs deep generative\nnetworks to bridge the gap between simulated and real-world data while mitigating\nthe risk of sensitive data exposure; the bidding module implements diverse auto-\nbidding agents trained with different decision-making algorithms; and the auction\nmodule is anchored in the classic Generalized Second Price (GSP) auction but also\nallows for customization of auction mechanisms as needed. To facilitate research\nand provide insights into the environment, we have also pre-generated a substantial\ndataset based on the environment. The dataset contains 10 million ad opportunities,\n48 diverse auto-bidding agents, and over 500 million auction records. Performance\nevaluations of baseline algorithms such as linear programming, reinforcement\nlearning, and generative models for bid decision-making are also presented as a\npart of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in\nLarge-Scale Auctions competition, providing competition environments for over\n1,500 teams. We believe that AuctionNet is applicable not only to research on bid\ndecision-making in ad auctions but also to the general area of decision-making in\nlarge-scale games. Code3: https:\/\/github.com\/alimama-tech\/AuctionNet.\n1\nIntroduction\nDecision-making in large-scale games is a fundamental area of research in artificial intelligence.\nAgents in a large-scale game need to make strategic decisions to fulfill their objectives under certain\nconstraints in a competitive environment. The research advances in this area have a profound impact\non a broad range of real-world applications [13, 34, 35, 37]. Online advertising, with a market size of\n∗This work is done during internship at Alibaba Group.\n†Corresponding author.\n3Alibaba Group retains full ownership rights to this benchmark.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\narXiv:2412.10798v2  [cs.AI]  28 Dec 2024\nadvertiser i\nadvertiser i\nusers\nBidding (II)\nAuction (III)\nImpressions (IV)\nObjective (I)\nPerformance (V)\narriving ad \nopportunities\nauto-bidding\nAgents\nbi,j\n(j)\n(i)\n1st\n2nd\n3rd\nad 1\nad 2\ne.g., maximize #conversions subject to \nbudget constraint\ne.g., #impressions, #clicks, \n#conversions, cost, and ROI\neach ad \nopportunity \ncan have \nmulti-slot \nimpressions\nFigure 1: Overview of typical large-scale online advertising platform. Numbers 1 through 5 illustrate\nhow an auto-bidding agent helps advertiser i optimize performance. For each advertiser’s unique\nobjective (I), auto-bidding agent make bid decision-making (II) for continuously arriving ad oppor-\ntunities, and compete against each other in the ad auction (III). Then, each agent may win some\nimpressions (IV), which may be exposed to users and potentially result in conversions. Finally, the\nagents’ performance (V) will be reported to advertisers.\nmore than $600 billion in 2023, is perhaps one of the most representative applications that calls for so-\nphisticated decision-making solutions in large-scale games. More specifically, as shown in Figure 1, a\nsignificant part of online advertising is based on real-time bidding (RTB), a process in which advertis-\ning inventory is bought and sold in real-time ad auctions. The auto-bidding agents strategically bid for\nimpressions on behalf of the advertisers across a large number of continuously arriving ad opportuni-\nties to maximize performance, subject to certain constraints such as return-on-investment (ROI) [28].\nBid decision-making in large-scale ad auctions is a concrete example of decision-making in\nlarge-scale games. However, researchers usually only have limited access to realistic large-scale\nad auction environments, hindering the research proccess in this area. Although a few existing\nworks provide certain environments, there remains a considerable gap between these environments\nand the real-world environments. For instance, AuctionGym [18] overlooks changes in advertiser\nbudgets across multiple auction rounds, while AdCraft [11] models competing bidders by sampling\nfrom a parameterized distribution, an approach that falls short of fully capturing the essence of the\nmulti-agent dynamics inherent to this problem.\nIn this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions\nderived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad\nauction environment, a pre-generated dataset based on the environment, and performance evaluations\nof a couple of baseline bid decision-making algorithms.\nMore specifically, the environment\neffectively replicates the integrity and complexity of real-world ad auctions with the interaction of\nseveral modules: the ad opportunity generation module employs deep generative networks to bridge\nthe gap between simulated and real-world data while mitigating the risk of sensitive data exposure;\nthe bidding module implements diverse auto-bidding agents trained with different decision-making\nalgorithms; and the auction module is anchored in the classic and popular Generalized Second Price\n(GSP) [9, 23, 7] auction but also allows customization of auction mechanisms as needed. To facilitate\nresearch and provide insights into the game environment, we also pre-generated a substantial dataset\nbased on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding\nagents, and over 500 million auction records. Performance evaluations of baseline algorithms such\nas linear programming, reinforcement learning, and generative models for bid decision-making are\nalso presented as a part of AuctionNet.\nWe believe that AuctionNet is applicable not only to research on bid decision-making algorithms in\nad auctions but also to the general area of decision-making in large-scale games. It can also benefit\n2\nresearchers in a broader range of areas such as reinforcement learning, generative models, operational\nresearch, and mechanism design.\n2\nThe Decision-Making Problem Concerned\nIn this paper, we are concerned with the auto-bidding problem in ad auctions. We use a Partially\nObservable Stochastic Game (POSG)[14] to formulate the problem. A POSG M can be represented\nas a tuple M = {S, A, P, r, γ, Z, O, I, T}, where I = {1, 2, · · · , n} is the set of all the agents, T\nis the horizon, i.e., the number of time steps in one episode, S is the state space and A is the action\nspace, P(·|s, a) : S × A →∆(S) is the transition probability, γ is the discount factor, Z is the\nobservation space, O(s, i) : S × I →Z is the mapping from state to observation for each agent i,\nr = r1 × r2 × · · · × rn is the joint reward function of all the agents, and ri(s, a) : S × A →R is the\nindividual reward function for each agent i, where a = (a1, a2, · · · , an) ∈A = A1 ×A2 ×· · ·×An\nis the joint action of all the agents.\nSpecifically, the interaction in one time step is as follows: The state s = (ω, u, q, v) consists of\nbudgets ω, ad opportunity features u, advertiser features q such as industry category, corresponding\nvalue matrix v = {vij}, where vij is the value of ad opportunity j for agent i. Agent i’s observation\noi = (ωi, ui, qi, vi) ∈Z contains only part of the information in state s, i.e., agent i may not\nknow the budgets of other agents. A convention in the auto-bidding area [3] proves that the\noptimal bid is proportional to the ad opportunity value. Following this convention, the action of\nagent i is a coefficient αi, and the bids of agent i for all the ad opportunities of this time step are\nbi = (bi1, bi2, · · · , bim) = (αivi1, αivi2, · · · , αivim), where m is the number of ad opportunities\nwithin this time step. Given the bids of all the agents, determined by the auction mechanism, agent\ni will receive the auction result xi = (xi1, xi2, · · · , xim), where xij = 1 if and only if agent i wins\nopportunity j. Agents will only receive rewards and incur costs from the winning impressions, i.e.,\nreward ri(s, a) = Pm\nj=1 xijvij and budget for the next time step ω′\ni = ωi −Pm\nj=1 xijcij, where\ncij is the cost of impression j for agent i.\nTaking a typical auto-bidding scenario as an example, given the definition above, the optimization\nobjective from the perspective of agent i is as follows:\nmaximize\n{αt\ni}\nT\nX\nt=1\n\nxt\ni, vt\ni\n\u000b\ns. t.\nT\nX\nt=1\n\nxt\ni, ct\ni\n\u000b\n≤ωi,\n(1)\nwhere xt\ni = (xt\ni1, xt\ni2, · · · , xt\nim), vt\ni = (vt\ni1, vt\ni2, · · · , vt\nim), ct\ni = (ct\ni1, ct\ni2, · · · , ct\nim), ωi is the budget\nof agent i, and ⟨·⟩denotes the inner product. As for the implementation, we know from our problem\nformulation that ri(st, at) = ⟨xt\ni, vt\ni⟩, so the objective in the optimization formulation is the same\nas PT\nt=1 ri(st, at). For more complex scenarios, we can add the CPA constraint to ensure effective\nutilization of the budget. More details on these CPA-constrained problems are included in Appendix\nE. The decision-making formulation above can be easily extended to various real-world scenarios.\n3\nAd Auction Environment\nTo comprehensively demonstrate large-scale games from real-world online advertising platforms,\nwe have developed an ad auction environment. To standardize the auto-bidding process, we divide\nad opportunities within a period into T decision time steps. Given the objective, the auto-bidding\nagent sequentially bids at each step, using the results from step t and prior historical information\nto refine its strategy for step t + 1. This design philosophy enables agents to continuously optimize\ntheir bidding strategies in order to adapt to the changing environment. Within each step, all ad\nopportunities are executed independently and in parallel. At the end of the period, the environment\nprovides the final performance for the agent.\nThe environment effectively replicates the integrity and complexity of real-world ad auctions through\nthe interaction of several modules: the ad opportunity generation module, the bidding module, and\nthe auction module. To better simulate large-scale auctions in reality, a substantial number of ad\nopportunities are fed into the environment and configured with dozens of bidding agents. These ad\nopportunities are generated using deep generative networks to reduce the gap between the simulation\n3\nQ\nReal-world ad opportunities\nAd Opportunity Generation\nAd Opportunity Value Prediction\nDenoising Unet\nConv\nConv\nConv\nConv\nEncoder\nLatent vector\nDiffusion process\nGenerated  ad opportunities\nDecoder\nSampled noise\nLatent vector\nN(0,1)\nAd opportunities\nK\nV\nQ\nK\nV\nAdvertisers\nGenerated  \nad opportunity values\nTemporal info\nCross\nattention \nCross\nattention \nFigure 2: Overview of the pipeline of the ad opportunity generation network. The generation process\nconsists of two stages. In the first stage, ad opportunity features are generated through a latent\ndiffusion model. In the second stage, the value prediction for the generated ad opportunity features is\nperformed, incorporating both the time feature and the advertiser feature. Moreover, the volume of\nad opportunities fluctuates over time, mirroring that of real-world online advertising platforms.\nenvironment and reality while avoiding the risks of sensitive data exposure. The agents are equipped\nwith diverse and sophisticated auto-bidding algorithms.\n3.1\nThe Ad Opportunity Generation Module\nThe target of the ad opportunity generation module is to generate diverse ad opportunities similar\nto real online advertising data with deep generative networks, as shown in Figure 2. We aimed to\nadopt the diffusion model to generate ad opportunity but encountered difficulties with the denoising\noperation, which can yield unreasonable outputs. Therefore, we followed the approach of the Latent\nDiffusion Model (LDM) [25] to generate ad opportunity. LDM adds noise and performs denoising in\nthe latent space using a diffusion model, and then generates data from the latent space with an encoder\nand decoder. Specifically, LDM maps the ad opportunity feature u to a latent vector y with the\nencoder and reconstructs this feature with the decoder during training. For generation, LDM samples\na random latent vector from a normal distribution and then generates an ad opportunity feature based\non this vector. Let U ⊂Rd be the space of ad opportunity feature data (u1, u2, · · · , uK), where d\nis the dimension of the original data and K is the number of ad opportunities. Let Y ⊂Rd′ be the\nlatent space (d′ < d). The encoder and decoder are represented as gϕ and hψ, respectively, where ϕ\nand ψ are the parameters. The function of the encoder gϕ is to obtain a latent representation of the\noriginal data as gϕ(uk) = (µk, σk), where yk ∼N(µk, σ2\nk) and yk ∈Y is the latent representation.\nIn practice, the reparameterization trick [20] is applied to ensure that this operation is differentiable\nduring backpropagation.\nGiven the latent representation yk, the decoder is responsible for reconstructing the original data\nfrom yk, i.e., hψ(yk) = ˜uk ∈U. In addition to the reconstruction, the latent distribution N(µk, σ2\nk)\nis expected to approximate the standard Gaussian distribution N(0, 1). Therefore, we have the\nfollowing loss function for the encoder and decoder:\nLrecons = 1\nK\nK\nX\nk=1\n∥uk −hψ(yk)∥2\n2 ,\nLreg = 1\nK\nK\nX\nk=1\nDKL\n\u0000N(µk, σ2\nk)\n\r\rN(0, 1)\n\u0001\n,\nwhere Lrecons is the reconstruction loss and Lreg is the regularization loss for the latent distribution.\nDifferent from the original idea of VAE [20], where the latent variable y ∈Y is sampled from\nN(0, 1) in the generation process, LDM uses a diffusion model in the latent space to generate the\nlatent variable. In general, the idea behind the diffusion model is to add Gaussian noise to the original\ndata to obtain variables that follow N(0, 1) and to denoise from N(0, 1) for generation. Given a\n4\nlatent variable y, we denote its noisy version after p iterations as yp. The diffusion model includes a\nnetwork to predict noise ϵθ(yp, p), and the loss function can be represented as\nLLDM = 1\nK\nK\nX\nk=1\n∥ϵk −ϵθ(yk,pk, pk)∥2\n2 ,\nwhere ϵk ∼N(0, 1), yk is the latent embedding of uk, and pk is uniformly sampled from the set\n{1, 2, · · · , pmax}. The network ϵθ(yp, p) is the only learnable component in the diffusion model,\nwhich enables the process of adding noise and denoising through basic operations.\nAs for the generation process, a latent variable ¯y is sampled from N(0, 1), and ˜y is obtained through\npmax denoising steps from ¯y using the noise prediction network ϵθ. Finally, the decoder generates an\nad opportunity feature based on ˜y as ˜u = hψ(˜y).\nGiven an ad opportunity feature uk, we also need to determine the value of this ad opportunity\ncombined with the category information of the corresponding advertiser qk and the time information\nutime\nk\n, where qk is the advertiser information in the real-world data associated with uk. We use\nMulti-head Attention (MHA) [31] as the network architecture for information integration. Let vξ\nrepresent the value prediction module, and vξ(uk, qk, utime\nk\n) denote the predicted value of the ad\nopportunity feature uk for a specific advertiser at a specific time step. The loss of the value prediction\nmodel is shown below:\nLpred = 1\nK\nK\nX\nk=1\n\r\rvk −vξ(uk, qk, utime\nk\n)\n\r\r2\n2 ,\nwhere vk is the true value of the ad opportunity in the record associated with uk.\n3.2\nThe Bidding Module\nThe bidding module replicates the dynamic competition between advertisers, each of whom has\ndistinct advertising objectives and utilizes a separate auto-bidding agent, while remaining unaware\nof their competitors’ strategies. Researchers can control a subset of the agents in the environment,\nwhile other agents remain uncontrollable, thereby better reflecting the complex and dynamic game in\nreal-world online advertising.\nSeveral algorithms in the auto-bidding area have been implemented as baselines, including the PID\nController [36], Online LP [15], IQL [21], Behavior Cloning [30], and Decision Transformer [8].\nThis facilitates researchers who are interested in quickly starting up and evaluating these baselines in\na unified environment.\n3.3\nThe Auction Module\nThe task of the auction module is to determine the winner and the winning price given all the bids\nfrom agents for ad opportunities. The costs for agents will vary depending on the different auction\nrules. The most commonly discussed auction rule is the Generalized Second-Price (GSP) Auction,\nwhich stipulates that the winner pays a cost slightly higher than the second-highest bid rather than\nthe highest bid. The auction module internally supports several popular auction rules, including\nGSP, for the convenience of researchers. Additionally, researchers can design specific auction rules\ntailored to their purposes using the interface of the auction module.\nAdditionally, the property of multiple slots has been implemented in the environment. Multiple slots\narise from applications in the industry, meaning that a single ad opportunity may have multiple ad\nslots for display. A slot with a higher exposure rate is more valuable to advertisers. Suppose the\nnumber of slots is l, then the auction module will allocate l slots to the top l bidders, and these\nbidders will receive different values according to the varying exposure rates of the slots. In summary,\nthe multiple slots feature increases the complexity of the optimal bidding strategy, as the exposure\nrate serves as a discount factor for both cost and value.\n3.4\nAPI\nThe code of the environment is implemented in Python. The environment API is similar to OpenAI\nGym[5], so the construction and interactions of the environment may be familiar to related researchers.\nWe included an example code as follows:\n5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\noriginal\ngenerated\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\noriginal\ngenerated\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\noriginal\ngenerated\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\noriginal\ngenerated\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\noriginal\ngenerated\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\noriginal\ngenerated\nFigure 3: The 3D PCA results of 100K generated data and 100K real-world data.\n1\nfrom AuctionNet import Controller\n2\n# Load player agent\n3\nbidding_controller =Controller(player_agent=player_agent)\n4\n# Init other competing agents\n5\nagents = bidding_controller.agents\n6\n# Init auction module\n7\nenvs = bidding_controller.biddingEnv\n8\n# Generate ad opportunities\n9\nad_opportunities = bidding_controller.adOpportunityGenerator.generate()\n10\n# Init the budget and reward of each agent\n11\nrewards = np.zeros(shape=(len(agents)))\n12\ncosts =\nnp.zeros(shape=(num_agents))\n13\nfor episode in range(num_episode):\n14\nfor tick_index in range(num_tick):\n15\n# load ad opportunities\n16\ntick_ad_opportunities = ad_opportunities[episode][tick_index]\n17\n# Collect bids from each agent\n18\nbids = []\n19\nfor agent in agents:\n20\nbids.append(agent.bidding())\n21\n# Simulate bidding process\n22\nauction_res = envs.simulate_ad_bidding(tick_ad_opportunities, bids)\n23\n# Aggregate bidding results\n24\nrewards+=auction_res[\"reward\"]\n25\ncosts+=auction_res[\"cost\"]\n4\nPre-Generated Dataset Based on the Environment\nIn this section, we first verify whether the ad opportunity generation module can generate ad\nopportunity features similar to those in real-world data. Next, we briefly introduce and analyze the\ndataset generated from the AuctionNet environment.\n4.1\nVerification of the Ad Opportunity Generation Module\nIn order to better demonstrate that the generated data can reflect the properties of real-world data,\nthe effectiveness of the ad opportunity generation module itself was verified. The ad opportunity\n6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ndensity\nlevel 1\nlevel 2\nlevel 3\nlevel 4\nlevel 5\nlevel 6\nlevel 7\nlevel 8\ngroups\nTaobao VIP Level\norigin\ngenerate\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\ndensity\n0~650\n650~1100\n1100~1700\n1700~1900\n1900~2600\n2600~3500\n3500~5000\n5000~8000\n>8000\nPreferred Phone Price\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\ndensity\n1 star\n2 stars\n3 stars\n4 stars\n5 stars\n1 diamond\n2 diamonds\n3 diamonds\n4 diamonds\n5 diamonds\n1 crown\n2 crowns\nBuyer Level\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\ndensity\nMale\nFemale\nGender\nFigure 4: The distribution of identity information including the Taobao VIP level, the preferred phone\nprice, the buyer level, and the gender in 100K generated data and 100K real-world data.\n0\n100\n200\n300\n400\n500\n600\nvalues\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nDensity\nNumber of collected items\ngenerated\noriginal\n0\n200\n400\n600\n800\n1000\n1200\nvalues\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nNumber of visited items\ngenerated\noriginal\n0\n20\n40\n60\n80\n100\n120\nvalues\n0.0\n0.1\n0.2\n0.3\n0.4\nNumber of collected sellers\ngenerated\noriginal\n0\n50000\n100000\n150000\n200000\n250000\n300000\nvalues\n0.000000\n0.000025\n0.000050\n0.000075\n0.000100\n0.000125\n0.000150\n0.000175\n0.000200\nConsumption amounts\ngenerated\noriginal\nFigure 5: The distribution of consumption behavior information including the number of collected\nitems, the number of visited items, the number of collected sellers, and the consumption amounts in\n100K generated data and 100K real-world data.\ngeneration module comprises two components: a feature generation model and a value prediction\nmodel. Experiments were conducted to verify the effectiveness of these models.\nWe randomly sample 100K real-world online advertising data points to compare with 100K generated\ndata points. The details of the generated data can be found in Appendix D. First, we perform PCA\n[19] to visualize the similarity between the real-world and generated data. The 3D PCA results are\nillustrated in Figure 3. For better presentation, we use six different views in the 3D space. We observe\nthat the generated data overlap with the original data in the 3D space. Moreover, the generated data\npoints form four main separate clusters in the 3D space, similar to the real-world data points. These\nvisualization results demonstrate that the generated data generally resemble the real-world data.\nTo further compare these two datasets, we study the value distributions of identity information and\nconsumption behavior information in both datasets. The empirical results are included in Figure 4\nand Figure 5. The feature vector contains over 20 fields, as described in Appendix D, so we only\nselect a subset of these fields for our experiments. Regarding identity information, the generated\nvalue distributions are similar to the real-world value distributions overall, although biases exist for\ncertain terms, such as ’level 7’ for the Taobao VIP Level. Distributions with more categories are\nmore challenging to match, while the gender distributions are nearly identical in both datasets. For\nconsumption behavior information, we observe that the distributions in the selected fields share a\nstrong resemblance and exhibit long-tail characteristics. A long-tail distribution indicates that most\nusers do not engage in frequent consumption, and users with a high volume of consumption behavior\nare rare. This phenomenon aligns with our experience in online advertising.\nWe investigate whether the generated data can capture the connections between different fields. Based\non the observation that users with higher VIP levels typically exhibit a higher volume of consumption\nbehavior, we examine the connection between the Taobao VIP level and consumption behavior. We\nselect four consumption behavior fields. The mean values of these fields across different VIP levels\nare shown in Figure 6. We find that the overall monotonically increasing trend is captured by the\ngenerated data, although biases exist in the specific values. Moreover, the drop in values from ’level\n7’ to ’level 8’ is also captured by the generated data in three out of the four fields, except for the\nconsumption amount. The rarity of ’level 8’ data points may be the reason why the generative model\nis unable to distinguish different trends for different fields.\nIn real-world online advertising, the metrics for bidding strategy evaluation are Click-Through Rate\n(CTR) and Conversion Rate (CVR). Bidding strategies make decisions based on the predicted CTR\n(pCTR) and predicted CVR (pCVR), which are the estimated values of CTR and CVR, respectively.\nFor simplicity, in this environment, we assume that the estimations are accurate and define the value\nas value = pCTR · pCVR. Our value prediction model learns to predict pCTR and pCVR and\nsubsequently calculates the value. We predict the pCTR, pCVR, and value for 100K real-world data\npoints and compare these predictions with the real-world ground truth.\nWe hope that the value prediction model can capture the value variation over changes in category and\ntime. The means of predicted pCTR, pCVR, and values across different categories and time steps,\n7\nlevel 1 level 2 level 3 level 4 level 5 level 6 level 7 level 8\nVIP Level\n0\n5\n10\n15\n20\n25\n30\n35\nmean values\nNumber of cart items\norigin\ngenerate\nlevel 1 level 2 level 3 level 4 level 5 level 6 level 7 level 8\nVIP Level\n0\n20\n40\n60\n80\n100\n120\nNumber of collected items\norigin\ngenerate\nlevel 1 level 2 level 3 level 4 level 5 level 6 level 7 level 8\nVIP Level\n0\n10000\n20000\n30000\n40000\n50000\n60000\nConsumption amounts\norigin\ngenerate\nlevel 1 level 2 level 3 level 4 level 5 level 6 level 7 level 8\nVIP Level\n0\n5\n10\n15\n20\n25\nNumber of visited categories\norigin\ngenerate\nFigure 6: The mean values of consumption behavior information including the number of cart items,\nthe number of collected items, the consumption amounts, and the number of visited categories in\ndifferent VIP levels in 100K generated data and 100K real-world data.\n0\n10\n20\n30\n40\n50\n60\ncategory\n0.00\n0.02\n0.04\n0.06\nmean values\npCTR\noriginal\npredicted\n0\n10\n20\n30\n40\n50\n60\ncategory\n0.00\n0.02\n0.04\n0.06\n0.08\npCVR\n0\n10\n20\n30\n40\n50\n60\ncategory\n0.000\n0.002\n0.004\n0.006\n0.008\nValues\n0\n20\n40\n60\n80\ntime\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\nmean values\noriginal\npredicted\n0\n20\n40\n60\n80\ntime\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0\n20\n40\n60\n80\ntime\n0.0005\n0.0010\n0.0015\n0.0020\n0.0025\nFigure 7: The means of the predicted pCTR, pCVR, and value in different categories and time steps\ncompared with the ground truth. The shaded areas are related to the standard deviation.\ncompared with the ground truth, are illustrated in Figure 7. The empirical results show that, in general,\nthe variation trends in predictions over changes in category and time are similar to the ground truth.\nTo present the results more intuitively, we provide additional quantitative results. We compare\nthe mean squared error (MSE) between the generated and original distributions with the standard\ndeviation of the original distribution. The quantitative results are shown in Table 1. It can be observed\nthat the MSEs are all smaller than the original standard deviations (original_stds), indicating that our\nprediction model can capture the patterns of value variation and is accurate.\n4.2\nPre-Generated Dataset\nTable 1: The comparison of the MSE between\nthe generated and original distribution with\nthe standard deviation of the original distribu-\ntion.\noriginal_std\nMSE\npCVR_category\n0.0685\n0.0341\npCTR_category\n0.0517\n0.0280\nvalue_category\n0.00573\n0.00496\npCVR_time\n0.0637\n0.0313\npCTR_time\n0.0590\n0.0259\nvalue_time\n0.00625\n0.00176\nThe dataset is derived from game data generated\nwithin the environment, where numerous auto-\nbidding agents compete against each other.\nWe\nhave pre-generated large-scale game data to assist\nresearchers in gaining deeper insights into the auc-\ntion ecosystem. This data can be used to model the\nenvironment and to train the auto-bidding agents ef-\nfectively.\nThe dataset contains 10 million ad opportunities, in-\ncluding 21 advertising episodes. Each episode con-\ntains more than 500,000 ad opportunities, divided\ninto 48 steps. Each opportunity includes the top 48 agents4 with the highest bids. The dataset\ncomprises over 500 million records, totaling 80 GB in size. Each record includes information such\nas the predicted value, bid, auction, and impression results, among other details. The specific data\nformat and data samples of the dataset are included in Appendix C.\nWe have conducted an analysis of the AuctionNet Dataset to provide some insights. We first investigate\nthe variation of impression values over time within a single day. We selected five categories from the\nAuctionNet Dataset and denote them as Category 1, Category 2, and so on. As shown in Figure 8,\nthe impression values of different categories exhibit distinct patterns of variation. Given the budget\nconstraint, agents should consider the variation in impression values over time to bid for appropriate\nimpressions at the optimal times. Furthermore, we examine the relations between the values of\ndifferent categories. The relations between Category 1 and other categories are illustrated in Figure 9.\n4Real-world data show that 48 agents can ensure competitive pressure for auto-bidding agent training.\n8\n5\n0\n5\n10\n15\n20\n25\n30\ntime\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 1\n5\n0\n5\n10\n15\n20\n25\n30\ntime\n0.2\n0.1\n0.0\n0.1\n0.2\ncategory 2\n5\n0\n5\n10\n15\n20\n25\n30\ntime\n0.4\n0.2\n0.0\n0.2\n0.4\ncategory 3\n5\n0\n5\n10\n15\n20\n25\n30\ntime\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 4\n5\n0\n5\n10\n15\n20\n25\n30\ntime\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\ncategory 5\nCategory Time Density\nFigure 8: The joint value distribution between different categories and time in the dataset.\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 1\n0.2\n0.1\n0.0\n0.1\n0.2\ncategory 2\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 1\n0.4\n0.2\n0.0\n0.2\n0.4\ncategory 3\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 1\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 4\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\ncategory 1\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\ncategory 5\nCategory Density\nFigure 9: The joint value distribution between Category 1 and other categories in the dataset.\nThe impression values of Category 1 and Category 3 are positively correlated, indicating that the\ncorresponding advertisers are competitors for similar ad opportunities. Therefore, considering the\npreferences of other agents may be beneficial for developing better bidding strategies. The full\ndatasheet of the dataset is included in Appendix B.\n5\nPerformance Evaluations of Baseline Algorithms\nIn this section, we evaluate the performance of baseline algorithms, such as linear programming,\nreinforcement learning, and generative models. It is important to note that we used the original\nalgorithms from the papers and did not perform any special optimization on the methods specifically\nfor the auto-bidding tasks. We provide a brief introduction to these baselines. The idea of the PID\nController is straightforward: it uses three parameters, λP , λI, and λD, for Proportional Control,\nIntegral Control, and Derivative Control, respectively. In this baseline, the PID Controller is employed\nto control the cost or bids of agents. Online LP utilizes linear programming for the auto-bidding\nproblem. At each time step, Online LP solves a knapsack problem using a greedy algorithm. IQL\nis an offline RL algorithm. The core idea behind IQL is to evaluate the offline Q-function only\non actions that appeared in the offline data, thereby avoiding overestimation in out-of-distribution\ndata.\nBehavior Cloning (BC) is a supervised learning algorithm that uses expert trajectories.\nThe agent’s policy is learned by predicting the expert’s actions in the state of given trajectories.\nDecision Transformer (DT) leverages the capabilities of the Transformer model[31] for sequential\ndecision-making. DT treats the trajectories in a MDP as sequences and predicts actions based on\nprevious transitions. More generative models such as AIGB [12] will also be integrated into baseline\nalgorithms in the future. To better illustrate the performances, we add a heuristic method, Abid, to\nthe experiments. Abid means the agent will give a fixed bid rate for all impressions. Its performance\ncan be seen as a reference in comparison. More details of the evaluation can be found in Appendix A.\nThe empirical results are included in Figure 10. For better illustration, we normalize the performances\nof all baselines by the mean episode reward of the heuristic baseline Abid. Therefore, the mean\nrelative performance of Abid is $1.0$ in the basic task. Online LP achieves the best performance,\npossibly because it is relatively robust and does not require special adaptation for auto-bidding tasks\nto achieve good results. Although methods like IQL and BC perform not as well as Online LP, we\nobserve that proposing optimized solution [12, 22]can significantly optimize the performance, proving\nthat such methods have great potential for optimization. In addition, the drop in rewards observed for\nall baselines during the target CPA task is due to the CPA penalty for exceeding constraints in (4).\n6\nApplications\nAuctionNet has powered the the NeurIPS 2024 competition \"Auto-bidding in Large-Scale\nAuctions\" [1].\nThe competition addressed the critical issue of making high-frequency bid\ndecision-making in uncertain and competitive environments and attracted more than 1,500 teams\n9\nPID\nIQL\nOnlineLP\nDecision-Transformer\nBC\nAbid\nalgorithm\n0\n2\n4\n6\n8\n10\nrelative performance\nBasic Task\nPID\nIQL\nOnlineLP\nDecision-Transformer\nBC\nAbid\nalgorithm\n2\n0\n2\n4\n6\n8\n10\nrelative performance\nTarget CPA Task\nFigure 10: The empirical results of baseline algorithms on the basic task and Target CPA task.\nfrom around the world to participate, lasting for 4 months. The ad auction environment, dataset, and\nbaseline bid decision-making algorithms used in the competition are derived from this benchmark.\nThe ad auction environment provided nearly ten thousand evaluations for the competition, offering\nparticipants accurate and fair performance assessments. The dataset and baseline algorithms allowed\nparticipants to quickly start the task and stimulated their creativity, leading to more diverse and\ninnovative solutions, thus driving technological development in this area.\n7\nRelated Work\nSimulation environments have been widely applied in decision-making research and have successfully\npromoted the development of related studies [6, 24, 32, 27, 29]. However, simulation environments\nfor real-world online advertising platforms are relatively scarce in the bid decision-making field.\nAuctionGym [18] models the bidding problem as a contextual bandit problem [2], where the\nadvertiser decides the bidding value given the information of the ad opportunity as context. The\ncontextual bandit has only one time step per episode, meaning that AuctionGym does not consider\nbudget constraints in auto-bidding. Moreover, AuctionGym describes the auto-bidding problem\nfrom a single-agent perspective and ignores the influence of other agents.\nAdCraft [11] is a\nsimulation environment for the bidding problem in Search Engine Marketing (SEM). Although\nAdCraft explicitly models the influences of other agents, these agents’ policies are sampled from\nparameterized distributions, which cannot fully reflect the multi-agent nature of this problem. Despite\nthe points discussed above, these existing simulation environments lack data-driven methods for\nmodeling real-world online advertising platforms.\n8\nConclusion and Limitations\nWe present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived\nfrom a real-world online advertising platform. AuctionNet consists of three components: an ad\nauction environment augmented with verified deep generative networks, a pre-generated dataset\nbased on this environment, and performance evaluations of several baseline bid decision-making\nalgorithms. The AuctionNet not only provides researchers with the opportunity to study auto-bidding\nalgorithms in large-scale auctions, but also helps researchers and practitioners in game theory,\nreinforcement learning, generative models, operations optimization, and other fields to solve a wide\nrange of decision-making research problems. Regarding limitations, while the generated data in\nthe AuctionNet environment and the real-world data are similar in general, there are biases in some\ndetails, and the performance of the generative model can be improved.\n9\nAcknowledgments\nThis work was supported in parts by NSFC under grants 62450001 and 62476008 and Alibaba Group\nthrough Alibaba Innovative Research Program. The authors would like to thank the anonymous\nreviewers for their valuable comments and advice.\n10\nReferences\n[1] NeurIPS 2024 Competition: Auto-Bidding in Large-Scale Auctions. https:\/\/tianchi.\naliyun.com\/specials\/promotion\/neurips2024_alimama#\/?lang=en_us. 2024.\n[2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear\npayoffs. In International conference on machine learning, pages 127–135. PMLR, 2013.\n[3] Santiago R Balseiro, Omar Besbes, and Gabriel Y Weintraub. Repeated auctions with budgets\nin ad exchanges: Approximations and design. Management Science, 61(4):864–884, 2015.\n[4] S. Bennett. Development of the pid controller. IEEE Control Systems Magazine, 13(6):58–62,\n1993.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n[7] Ioannis Caragiannis, Christos Kaklamanis, Panagiotis Kanellopoulos, and Maria Kyropoulou.\nOn the efficiency of equilibria in generalized second price auctions. In Proceedings of the 12th\nACM conference on Electronic commerce, pages 81–90, 2011.\n[8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084–15097,\n2021.\n[9] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the\ngeneralized second-price auction: Selling billions of dollars worth of keywords. American\neconomic review, 97(1):242–259, 2007.\n[10] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna\nWallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the\nACM, 64(12):86–92, 2021.\n[11] Maziar Gomrokchi, Owen Levin, Jeffrey Roach, and Jonah White. Adcraft: An advanced\nreinforcement learning benchmark environment for search engine marketing optimization. arXiv\npreprint arXiv:2306.11971, 2023.\n[12] Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Bo Zheng, and Yan\nZhang. Aigb: Generative auto-bidding via conditional diffusion modeling. In Proceedings of the\n30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5038–5049,\n2024.\n[13] Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in\nfinance. Mathematical Finance, 33(3):437–503, 2023.\n[14] Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially\nobservable stochastic games. In AAAI, volume 4, pages 709–715, 2004.\n[15] Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao, Shan Chen, Rongquan\nBai, Mingzhou Xie, Miao Xu, et al. Dynamic knapsack optimization towards efficient multi-\nchannel sequential advertising. In International Conference on Machine Learning, pages\n4060–4070. PMLR, 2020.\n[16] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and Xiaoqiang Zhu. A\nunified solution to constrained bidding in online display advertising. In Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2993–3001, 2021.\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840–6851, 2020.\n11\n[18] Olivier Jeunen, Sean Murphy, and Ben Allison. Off-policy learning-to-bid with auctiongym. In\nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\npages 4219–4228, 2023.\n[19] Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent devel-\nopments. Philosophical transactions of the royal society A: Mathematical, Physical and\nEngineering Sciences, 374(2065):20150202, 2016.\n[20] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n[21] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\nq-learning. In Deep RL Workshop NeurIPS 2021, 2021.\n[22] Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, and\nFan Wu. Trajectory-wise iterative reinforcement learning framework for auto-bidding. In\nProceedings of the ACM on Web Conference 2024, pages 4193–4203, 2024.\n[23] Brendan Lucier, Renato Paes Leme, and Éva Tardos. On revenue in the generalized second\nprice auction. In Proceedings of the 21st international conference on World Wide Web, pages\n361–370, 2012.\n[24] C Berner OpenAI, Greg Brockman, Brooke Chan, Vicki Cheung, P Debiak, Christy Dennison,\nDavid Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep\nreinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF\nconference on computer vision and pattern recognition, pages 10684–10695, 2022.\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks\nfor biomedical image segmentation. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,\n2015, Proceedings, Part III 18, pages 234–241. Springer, 2015.\n[27] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nan-\ntas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon\nWhiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\n[28] Yumin Su, Min Xiang, Yifei Chen, Yanbiao Li, Tian Qin, Hongyi Zhang, Yasong Li, and\nXiaobing Liu. Spending programmed bidding: Privacy-friendly bid optimization with roi\nconstraint in online advertising. In Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pages 5731–5740, 2024.\n[29] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In IEEE\/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.\n[30] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence.\nInternational Joint Conferences on Artificial Intelligence Organization, 2018.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[32] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-\nyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n[33] Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun\nGai. Budget constrained bidding by model-free reinforcement learning in display advertising.\nIn Proceedings of the 27th ACM International Conference on Information and Knowledge\nManagement, pages 1443–1451, 2018.\n12\n[34] Yuxin Wu, Tianyang Zhao, Haoyuan Yan, Min Liu, and Nian Liu. Hierarchical hybrid multi-\nagent deep reinforcement learning for peer-to-peer energy trading among multiple heterogeneous\nmicrogrids. IEEE Transactions on Smart Grid, 2023.\n[35] Wei Zhang, Yanjun Han, Zhengyuan Zhou, Aaron Flores, and Tsachy Weissman. Leveraging\nthe hints: Adaptive bidding in repeated first-price auctions. Advances in Neural Information\nProcessing Systems, 35:21329–21341, 2022.\n[36] Weinan Zhang, Yifei Rong, Jun Wang, Tianchi Zhu, and Xiaofan Wang. Feedback control of\nreal-time display advertising. In Proceedings of the Ninth ACM International Conference on\nWeb Search and Data Mining, pages 407–416, 2016.\n[37] Yiheng Zhu, Yang Zhan, Xuankun Huang, Yuwei Chen, Jiangwen Wei, Wei Feng, Yinzhi Zhou,\nHaoyuan Hu, Jieping Ye, et al. Ofcourse: A multi-agent reinforcement learning environment\nfor order fulfillment. Advances in Neural Information Processing Systems, 36, 2024.\n13\nA\nEvaluation Details\nThere are 48 agents of 7 types in our experiments and each type corresponds to one algorithm. We\ntest 7 rounds where we permute the order of agents in each round. Therefore, agents will represent\ndifferent advertisers with different budgets in different rounds. We choose the best agent as the\nrepresentative of an algorithm if there are multiple agents of this algorithm. We use the average\nperformances of the 7 rounds as the final performance of all the algorithms. We provide the model\nfile of these agents and the evaluation code for reproduction.\nB\nDatasheet for AuctionNet\nWe present a datasheet[10] for the AuctionNet Dataset.\nB.1\nMotivation\nFor what purpose was the dataset created?\nIn general, learning from interactions with the real-world online advertising platforms is difficult\nand expensive, so offline RL algorithms are more popular in auto-bidding. Therefore, we build the\nAuction Dataset to facilitate offline training of users. Moreover, the Auction Dataset will also be\nprovided to the participants of the competition we will hold in the future.\nWho created the dataset?\nThe dataset was created by the authors of this paper. The dataset was not created on the behalf of any\nentity.\nWho funded the creation of the dataset?\nAlibaba Group funds the creation of the AuctionNet Dataset.\nB.2\nComposition\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)?\nThe AuctionNet Dataset contains trajectories of diverse agents competing with each other. Please\nrefer to Appendix C and Section 4.2 for more details.\nIs there a label or target associated with each instance?\nThe AuctionNet Dataset contains offline trajectories where the actions or bids of agents can be seen\nas labels for the time step.\nIs any information missing from individual instances?\nNot to our knowledge.\nAre there recommended data splits (e.g., training, development\/validation, testing)?\nNo.\nAre there any errors, sources of noise, or redundancies in the dataset?\nThe AuctionNet Dataset contains trajectories of diverse agents, some of these agents may not perform\nwell. However, the tasks in the environment are still difficult for some algorithms and we think\nkeeping agents diverse in the AuctionNet Dataset is beneficial.\nDo\/did we do any data cleaning on the dataset?\nWe did not. All data is presented exactly as collected.\nB.3\nCollection Process\nHow was the data associated with each instance acquired?\nThe AuctionNet Dataset is collected from the interactions of baseline agents in the environment.\n14\nWho was involved in the data collection process and how were they compensated?\nThe data collection process is done by the authors and not involve with any crowdsource.\nOver what timeframe was the data collected?\nThe AuctionNet Dataset was collected between March 2024 and May 2024.\nB.4\nUses\nHas the dataset been used for any tasks already?\nNo.\nIs there a repository that links to any or all papers or systems that use the dataset?\nNo.\nIs there anything about the composition of the dataset or the way it was collected and prepro-\ncessed\/cleaned\/labeled that might impact future uses?\nWe do not believe so since the AuctionNet Dataset consists of data generated by the interactions of\nbaseline agents.\nB.5\nDistribution\nWill the dataset be distributed to third parties?\nYes, but the AuctionNet Dataset and environment are involved with a large competition we will\nhold in NeurIPS 2024, so we will not distribute them until the end of the competition considering\ncompetition fairness. However, we will open-source the AuctionNet Dataset as soon as possible.\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset\nhave a digital object identifier (DOI)?\nThe AuctionNet Dataset will be distributed by a Github link after the end of the competition we will\nhold. The AuctionNet Dataset doesn’t have a digital object identifier now.\nAll data is under the MIT license.\nHave any third parties imposed IP-based or other restrictions on the data associated with the\ninstances?\nNo.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances?\nNo.\nB.6\nMaintenance\nWho will be supporting\/hosting\/maintaining the dataset?\nThe authors of this paper will provide needed maintenance to the datasets.\nHow can the owner\/curator\/manager of the dataset be contacted (e.g., email address)?\nPlease email us at huoyusen.huoyusen@alibaba-inc.com.\nIs there an erratum?\nThere is not and we believe generated features, predicted values, and trajectories in our datasets do\nnot involve an erratum.\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\nYes, but as we won’t add extra data points, the update will be minimal.\n15\nC\nData Format of AuctionNet Dataset\nThe specific data format of the AuctionNet Dataset is as follows:\n(c1). deliveryPeriodIndex: The index of the current delivery period.\n(c2). advertiserIndex: The unique identifier of the advertiser.\n(c3). advertiserCategoryIndex: The index of the advertiser’s category.\n(c4). budget: The advertiser’s budget for a period.\n(c5). CPAConstraint: The CPA constraint of the advertiser.\n(c6). timeStepIndex: The index of the current decision time step.\n(c7). remainingBudget: The advertiser’s remaining budget before the current step.\n(c8). pvIndex: The index of the ad opportunity.\n(c9). pValue: The conversion probability when the ad is exposed to the user.\n(c10). pValueSigma: The variance of predicted probability.\n(c11). bid: The agent’s bid of the ad opportunity.\n(c12). xi: The winning status of the agent of the ad opportunity.\n(c13). adSlot: The won ad slot.\n(c14). cost: The cost needs to be paid if the ad is exposed to the user.\n(c15). isExposed: The indicator signifying whether the ad in the slot was displayed to the user.\n(c16). conversionAction: The indicator signifying whether the conversion action has occurred.\n(c17). leastWinningCost: The minimum cost to win the ad opportunity.\n(c18). isEnd: The completion status of the advertising period.\nTable 2 presents an ad opportunity involving the top five advertisers. The top three advertisers,\nnumbered 31, 22, and 15, won the ad opportunity with the highest bids and were allocated to ad slots\n1, 2, and 3, respectively. During this impression, slots 1 and 2 were exposed to the user, while slot 3\nremained unexposed. Consequently, ads in slots 1 and 2 need to pay 0.2702 and 0.2154, respectively.\nAdditionally, the user engaged in a conversion action with the ad in slot 2.\nTable 2: Bidding, auction, and impression processes for each advertiser during the same opportunity.\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\nc11\nc12\nc13\nc14\nc15\nc16\nc17\nc18\n1\n31\n2\n6500.00\n27.00\n5\n5962.49\n101000\n0.0103542\n0.0021549\n0.2845\n1\n1\n0.2702\n1\n0\n0.1832\n0\n1\n22\n6\n7000.00\n38.00\n5\n5988.25\n101000\n0.0070297\n0.0005213\n0.2702\n1\n2\n0.2154\n1\n1\n0.1832\n0\n1\n15\n7\n7000.00\n42.00\n5\n6132.52\n101000\n0.0051392\n0.0004312\n0.2154\n1\n3\n0.1832\n0\n0\n0.1832\n0\n1\n39\n3\n6000.00\n30.00\n5\n5443.27\n101000\n0.0062134\n0.0007254\n0.1832\n0\n0\n0\n0\n0\n0.1832\n0\n1\n43\n9\n7500.00\n25.00\n5\n6421.81\n101000\n0.0045392\n0.0006215\n0.1099\n0\n0\n0\n0\n0\n0.1832\n0\nTable 3 presents a data sample illustrating an advertiser’s bidding process across time steps within a\ndelivery period. The advertiser has a budget of 7500, a CPA constraint of 40, and belongs to industry\ncategory 6. Throughout different time steps, the advertiser engages in bidding for every available\nimpression and obtains the corresponding results. During this period, the advertiser’s remaining\nbudget decreases correspondingly. Additionally, the advertiser adjusts their bidding strategy based on\nprior performance, although this adjustment will not be directly evident in the data.\nD\nThe Structure of Generated Data\nStructure of the feature vector. The feature vector consists of several types of information including\none-hot vectors, integers and float numbers. The specific data format of the feature vector is as\nfollows:\n(c1). idAgeLevel: Represents the age level of the user. The meanings of values: 0 for unknown,\n1∼8 for ages over 12, 18, 22, 25, 30, 35, 40 and 50 respectively. Data format: one-hot vector,\ndimension [0, 9).\n16\nTable 3: An advertiser’s bidding process across time steps.\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nc10\nc11\nc12\nc13\nc14\nc15\nc16\nc17\nc18\n3\n48\n6\n7500.00\n40.00\n1\n7500.00\n1\n0.0032157\n0.0003567\n0.1345\n0\n0\n0\n0\n0\n0.1628\n0\n3\n48\n6\n7500.00\n40.00\n1\n7500.00\n2\n0.0146256\n0.0021352\n0.5852\n0\n0\n0\n0\n0\n0.6421\n0\n3\n48\n6\n7500.00\n40.00\n1\n7500.00\n3\n0.0054324\n0.0007631\n0.1924\n1\n1\n0.1673\n1\n1\n0.1454\n0\n3\n48\n6\n7500.00\n40.00\n1\n7500.00\n4\n0.0073145\n0.0006529\n0.2786\n0\n0\n0\n0\n0\n0.2862\n0\n. . .\n3\n48\n6\n7500.00\n40.00\n2\n7341.25\n20901\n0.0076453\n0.0006579\n0.2856\n0\n0\n0\n0\n0\n0.3245\n0\n3\n48\n6\n7500.00\n40.00\n2\n7341.25\n20902\n0.0139234\n0.0012358\n0.5629\n1\n2\n0\n0\n0\n0.6782\n0\n3\n48\n6\n7500.00\n40.00\n2\n7341.25\n20903\n0.0077212\n0.0006579\n0.3045\n0\n0\n0\n0\n0\n0.3122\n0\n3\n48\n6\n7500.00\n40.00\n2\n7341.25\n20904\n0.0021341\n0.0001873\n0.0926\n0\n0\n0\n0\n0\n0.1151\n0\n. . .\n3\n48\n6\n7500.00\n40.00\n43\n0.00\n895201\n0.0065274\n0.0005689\n0.0000\n0\n0\n0\n0\n0\n0.1243\n1\n3\n48\n6\n7500.00\n40.00\n43\n0.00\n895202\n0.0032125\n0.0002986\n0.0000\n0\n0\n0\n0\n0\n0.2986\n1\n3\n48\n6\n7500.00\n40.00\n43\n0.00\n895203\n0.0112986\n0.0013253\n0.0000\n0\n0\n0\n0\n0\n0.0932\n1\n3\n48\n6\n7500.00\n40.00\n43\n0.00\n895204\n0.0051678\n0.0006782\n0.0000\n0\n0\n0\n0\n0\n0.1687\n1\n(c2). idGender: Represents the gender of the user. The meanings of values: 0,1 and 2 for unknown,\nFemale and Male respectively. Data format: one-hot vector, dimension [9, 12).\n(c3). isForeign: Represents whether the user is foreign. The meanings of values: 0,1 and 2 for\nunknown, No and Yes respectively. Data format: one-hot vector, dimension [12, 15).\n(c4). cityLevel: Represents the level of the city where the user is living. The meanings of values:\n0 for unknown, 1∼6 for different city development levels in descending order. Data format:\none-hot vector, dimension [15, 22).\n(c5). isCap: Represents whether the city the user living in is the capital. The meanings of values: 0\nfor No and 1 for Yes. Data format: one-hot vector, dimension [22, 24).\n(c6). buyerStarName: Represents the rating of the user as a buyer. The meanings of values: 0 for\nunknown, 1∼5 for 1∼5 stars respectively, 6∼10 for 1∼5 diamonds respectively, 11∼15 for\n1∼5 crowns respectively, 16∼20 for 1∼5 golden crowns respectively, 21 for credit score ≤3,\nand 22 for credit score = 0. In general, the order of these values’ ratings is 22 < 21 < 1 <\n2 < · · · < 20. Data format: one-hot vector, dimension [24, 47).\n(c7). tmLevel: Represents the VIP level of the user in Tmall. The meanings of values: 0 for unknown\nor no VIP level, 1∼5 for VIP levels 1∼5 respectively. Data format: one-hot vector, dimension\n[47, 53).\n(c8). vipLevelName: Represents the VIP level of the user in Taobao. The meanings of values: 0 for\nunknown or no VIP level, 1∼8 for VIP levels 1∼8 respectively. Data format: one-hot vector,\ndimension [53, 62).\n(c9). phonePriceLevelPrefer: Represents the preferred phone price interval of the user. The meanings\nof values: 0 for unknown, 1 for 0∼650 CNY, 2 for 1100∼1700 CNY, 3 for 1700∼1900 CNY, 4\nfor 1900∼2600 CNY, 5 for 2600∼3500 CNY, 6 for 3500∼5000 CNY, 7 for 5000∼8000 CNY,\n8 for 650∼1100 CNY, 9 for higher than 8000 CNY. Data format: one-hot vector, dimension\n[62, 72).\n(c10). zipCode: Represents the zip code of the address where the user is living. The meanings of\nvalues: The zip code contains 6 digits and each digit is a number from 0∼9. We encode\neach digit with an one-hot vector and concatenate them together. Data format: one-hot vector,\ndimension [72, 132).\n(c11). idBirthyear: Represents the birthyear of the user. Data format: integer, dimension [132, 133).\n(c12). nationId: Represents the nation of the user. The meanings of values: 1 for China. (The real-\nworld training data contains almost no data points from other countries. So does our generated\ndata.) Data format: integer, dimension [133, 134).\n(c13). payOrdAmt: Represents the order amounts of the user in the last one month, one year, three\nmonths and six months. Data format: float numbers, dimension [134, 138).\n(c14). payOrdCnt: Represents the user’s number of orders in the last one month, one year, three\nmonths and six months. Data format: integers, dimension [138, 142).\n(c15). payOrdDays: Represents the number of days when the user placed orders in the last one month,\none year, three months and six months. Data format: integers, dimension [142, 146).\n(c16). payOrdItmCnt: Represents the number of item types the user bought in the last one month, one\nyear, three months and six months. Data format: integers, dimension [146, 150).\n17\n(c17). payOrdItmQty: Represents the number of items the user bought in the last one month, one year,\nthree months and six months. Data format: integers, dimension [150, 154).\n(c18). pvAndIpv: Represents the PV and IPV value of the user bought in the last month. Data format:\nfloat numbers, dimension [154, 156).\n(c19). vstSlrCnt: Represents the number of sellers the user visited in the last month. Data format:\nintegers, dimension [156, 157).\n(c20). vstCateCnt: Represents the number of categories the user visited in the last month. Data format:\nintegers, dimension [157, 158).\n(c21). vstItmCnt: Represents the number of items the user visited in the last month. Data format:\nintegers, dimension [158, 159).\n(c22). vstItmCnt: Represents the number of items the user visited in the last month. Data format:\nintegers, dimension [158, 159).\n(c23). vstDays: Represents the number of days the user visited items in the last month. Data format:\nintegers, dimension [159, 160).\n(c24). stayTimeLen: Represents the number of seconds the user spent on visiting items in the last\nmonth. Data format: integers, dimension [160, 161).\n(c25). cartItmCnt: Represents the number of items the user added to the cart in the last one week, two\nweeks, one month, three months, and six months. Data format: integers, dimension [161, 166).\n(c26). cltSlrCnt:\nRepresents the number of sellers the user collected in the last one week,\ntwo weeks, one month, six months, and one year.\nData format: integers, dimension\n{166, 168, 170, 172, 174}.\n(c27). cltItmCnt:\nRepresents the number of items the user collected in the last one week,\ntwo weeks, one month, six months, and one year.\nData format: integers, dimension\n{167, 169, 171, 173, 175}.\nStructure of the value vector. The value vector has 60 dimensions corresponding to 59 categories\ninvolved in our environment and one conserved category for the undefined or unknown category. The\ncorresponding relations between the dimension indexes and categories are listed in Table 4.\nE\nTasks\nThough AuctionNet provides a general framework for auto-bidding problem studies, we choose two\ntypical scenarios in auto-bidding as tasks in AuctionNet for easier understanding.\nE.1\nBasic Task\nOur basic task is based on the scenario Budget Constrained Bidding (BCB) [33], where agents\nmaximize their obtained values within the constraint on the budget. The optimization formulation of\nBCB from agent i’s perspective is as follows:\nmaximize\n{αt\ni}\nT\nX\nt=1\n\nxt\ni, vt\ni\n\u000b\ns. t.\nT\nX\nt=1\n\nxt\ni, ct\ni\n\u000b\n≤ωi,\n(2)\nwhere xt\ni = (xt\ni1, xt\ni2, · · · , xt\nim) is the auction result of all ad opportunities for agent i in time step t,\nvt\ni = (vt\ni1, vt\ni2, · · · , vt\nim) is the value for agent i, ct = (ct\ni1, ct\ni2, · · · , ct\nim) is the cost in time step t,\nbi is the budget for agent i, and ⟨·⟩is the inner product.\nAs for the implementation, we know from our problem formulation that ri(st, at) = ⟨xt\ni, vt\ni⟩, so\nthe objective in the optimization formulation is the same as the objective PT\nt=1 ri(st, at) in the RL\nformulation. The budget constraint is guaranteed by ignoring the bids exceeding agents’ budgets in\nthe environment. Therefore, BCB corresponds to the default setting of AuctionNet.\n18\nTable 4: Corresponding relations for categories.\nID\nCategory\nID\nCategory\n1\nSnacks\n31\nTravel Services\n2\nPersonal Care\n32\nTmall Home & Living\n3\nElectric Vehicles\n33\nMaternity & Childcare\n4\nTmall Underwear\n34\nMovies, Shows & Sports\n5\nSmart Toys & Games\n35\nEducation & Teaching\n6\nTea\n36\nTaobao Bags & Accessories\n7\nHousehold Cleaning\n37\nTaobao Underwear\n8\nChilled Food\n38\nAudio & Video Electronics\n9\nTmall Women’s Clothing\n39\nGaming\n10\nEnterprise Services\n40\nPets\n11\nDairy Products\n41\nVehicles\n12\nFragrances and Aromatherapy\n42\nMajor Appliances\n13\nLife Services\n43\nTmall Footwear\n14\nHousehold Appliances\n44\nFood Coupons\n15\nTaobao Men’s Clothing\n45\nAuto Accessories\n16\nTmall Home Decor\n46\nMobile Phones\n17\nTaobao Home & Living\n47\nTaobao Footwear\n18\nTaobao Home Decor\n48\nGrains & Instant Food\n19\nInstant Drinks\n49\nTmall Bags & Accessories\n20\nAlcohol\n50\nMobile & Digital Accessories\n21\nTaobao Women’s Clothing\n51\nTaobao Watches & Glasses\n22\nAuto Aftermarket\n52\nJewelry & Accessories\n23\nFruits and Vegetables\n53\nSports\n24\nFlowers and Gardening\n54\nToys & Fun\n25\nOffice & School Supplies\n55\nEntertainment Recharge\n26\nComputers\n56\nBeverages\n27\nTmall Watches & Glasses\n57\nOutdoor\n28\nComputer Accessories\n58\nMotorcycles\n29\nAquatic Products, Meat, Poultry & Eggs\n59\nTmall Men’s Clothing\n30\nCosmetics\n0\nOther\nE.2\nTarget CPA Task\nWe propose Target CPA Task based on the real-world scenario Target CPA (Cost Per Action)5 with\nsome simplifications for understanding. The CPA of agent i is defined as cpai =\nPT\nt=1⟨xt\ni,ct\ni⟩\nPT\nt=1⟨xt\ni,vt\ni⟩, which\ncan be seen as the cost taken by agent i for unit value. A low CPA means the budgets are consumed\nto obtain values effectively. Based on the basic task, Target CPA Task adds one more constraint on\nCPA that cpai should be lower than the desired CPA di. The formulation is as follows:\nmaximize\n{αt\ni}\nT\nX\nt=1\n\nxt\ni, vt\ni\n\u000b\ns. t.\nT\nX\nt=1\n\nxt\ni, ct\ni\n\u000b\n≤ωi\ncpai ≤di.\n(3)\nGiven that CPA can only be calculated at the end of one episode, the environment will only provide a\nsparse reward in Target CPA Task, which is different from the basic task. Unlike the budget constraint\nwhich cannot be violated in the environment, we allow agents to violate the CPA constraint, but we\nwill penalize those agents for violations on their obtained values based on their CPA cpai. The sparse\nreward formulation in Target CPA Task is as follows:\nrCSB\ni\n= p(cpai; di)\nT\nX\nt=1\n\nxt\ni, vt\ni\n\u000b\n,\n(4)\n5https:\/\/support.google.com\/google-ads\/answer\/6268632\n19\nwhere p(cpai; di) = min\n\u001a\u0010\ndi\ncpai\n\u0011β\n, 1\n\u001b\nis the penalty function for exceeding the CPA constraint.\nThe formulation of p(cpai; di) implies that the penalty is incurred only when cpai > di. The\nparameter β > 0 is typically set to 3. Therefore, Target CPA Task can be implemented with\nmodifications to the reward function in the basic task.\nF\nBaseline Algorithms\nWe have implemented multiple baseline algorithms in AuctionNet to facilitate a quick start-up and\ncomprehensive understanding of users. The baseline algorithms include PID Controller[36], Online\nLP[15], IQL[21], Behavior Cloning[30], and Decision Transformer[8].\nPID Controller. PID Controller is a traditional algorithm in the control field with a long history[4].\nIt is simple but effective in many scenarios. Recently, PID Controller has also been adopted in online\nadvertising[36]. The idea of PID Controller is straightforward: PID Controller takes three parameters\nλP , λI, and λD for Proportional Control, Integral Control, and Derivative Control, respectively. We\nuse the PID Controller to control the cost or bids of agents in this baseline.\nOnline LP. The optimization formulation (2) is a typical Linear Programming (LP) problem. More-\nover, the variable xt\nij ∈{0, 1} is binary, so the problem in each time step can be converted to\na dynamic knapsack problem. Online LP solves this dynamic knapsack problem using a greedy\nalgorithm.\nIQL. Implicit Q-learning (IQL) is an offline RL algorithm. The idea of IQL is evaluating offline\nQ-function only on the actions that appeared in the offline data, to avoid the overestimation in the\nout-of-distribution data. In practice, IQL utilizes expectile regression to realize the offline Q-learning\non in-distribution data.\nBehavior Cloning. Behavior Cloning (BC) is a supervised learning algorithm given expert trajectories.\nThe agent’s policy learns by predicting the expert’s action in the state of given trajectories. BC is a\nbaseline for verifying the effectiveness of RL algorithms.\nDecision Transformer. Decision Transformer (DT) utilizes the ability of Transformer[31] for\nsequential decision-making. DT views the trajectories in MDP as a sequence and predicts actions\ngiven previous transitions.\nG\nImplementation and Modules\nThe environment of AuctionNet consists of three main modules: the ad opportunity generation\nmodule, the auction module, and the bidding module. The general process of one time step in\nAuctionNet can be concluded as follows:\n1) The ad opportunity generation module generates features u = (u1, u2, · · · , um) and values\nv = {vij} of m ad opportunities for n agents, where the number of ad opportunities m is sampled\nfrom an intern distribution within AuctionNet. This intern distribution is obtained from real-world\nonline advertising statistics\n2) Agents bid for all the ad opportunities considering the predicted values provided by the environ-\nment and the historical auction logs.\n3) The auction module determines the winner of each auction, rewards, and costs by the auction\nmechanism.\n4) Agents receive rewards, costs, and new auction logs. The budgets of all the agents are updated\naccording to auction results. In the next time step, all the processes above will be repeated.\nGiven this general process, we will introduce the three main modules in order. The ad opportunity\ngeneration module will generate features u of ad impressions related to the real online data. The\nad auction module supports an auction similar to real-world online advertising and realizes several\npopular auction mechanisms for different research purposes. The bidding module supports explicitly\nmodeling a multi-agent environment with several implemented baselines.\n20\nG.1\nAd Opportunity Generation Module\nThe target of the ad opportunity generation module is to generate diverse ad opportunity features\nsimilar to real online advertising data. The core of this module is the generative model. The objective\nof the generative model in AuctionNet is to generate data resembling real advertising delivery data.\nUseful information in the real advertising delivery data can be divided into four parts: features of\nad opportunities (users’ information), features of advertisers, time when the ad opportunity arises,\nand the values of the ad opportunities. In our model, we simplify the feature of advertisers to be the\nadvertisers’ industry categories. We focus on the generation of ad opportunity features and take the\ncategories and time as conditions. The generative model consists of two components: the generative\nmodel for ad opportunity features and the prediction model for the values.\nFeature Generation. The ad opportunity feature contains two parts of information: the basic identity\ninformation of users and the consumption records such as the consumption amount. The identity\ninformation is discrete and the consumption records are continuous in general, which are processed\nwith different measures. Diffusion [17] model is the most popular generative model recently which\nobtains SOTA performances in image generation with a simplified training process. We would like\nto adopt the diffusion model to generate the ad opportunity feature but struggle with the denoising\noperation which can result in unreasonable outputs such as a negative consumption amount. So we\nfollow the idea of the Latent Diffusion Model (LDM)[25] to generate ad opportunity features. LDM\nadds noises and denoises in the latent space with a diffusion model and generates data from the latent\nspace with an encoder and decoder. More details can be found in Appendix H.1.\nValue Prediction. The value prediction model needs to handle three types of information: ad\nopportunity features, the industry category information of advertisers, and time information. We\nsimplify the category and time information as discrete values. Therefore, we aim to integrate the\ncategory and time information into the ad opportunity features for better value prediction. Besides,\nwe hope this integration can partly reflect the variation pattern of the impression values related to\nadvertisers’ features and time. Multi-head attention (MHA), as a popular network architecture and\nthe critical part of Transformer [31], can capture the relations among a sequence, thus we hope to\nutilize MHA for better integration. We combine cross-attention and self-attention to integrate the\nthree types of information. We also follow the idea of position embedding in the Transformer to\nprocess the time information. More details are included in Appendix H.2.\nFor the consideration of interaction efficiency in AuctionNet, the environment utilizes a dataset\nconsisting of generated features and corresponding predicted values. More details of the dataset will\nbe discussed in Section 4.1. Though the ad opportunity generation module is trained with real online\nadvertising data, an important question is whether the generated data can reflect the properties of\nreal data. Therefore, we have done several related experiments and the empirical results will also be\ndiscussed in Section4.1.\nG.2\nAuction Module\nThe task of the auction module is to determine the winner and the winning price given all bids of\nagents for the ad ad opportunities. The costs of agents will change given different auction rules.\nThe most commonly discussed auction rule is the Generalized Second-Price (GSP) Auction which\nmeans the winner should pay a cost slightly higher than the second-highest bid instead of the highest\nbid. The auction module internally supports several popular auction rules including GSP for the\nconvenience of researchers. Besides, researchers can also design a specific auction rule related to\ntheir purposes with the interface of the auction module.\nAdditionally, the property of multiple slots has been implemented in our simulation platform. Multiple\nslots emerge from the application in the industry, which means one ad opportunity has multiple\nad slots for ad displays. The ad slots are ranked by their exposure rates. A higher exposure rate\nslot is more valuable for advertisers. Suppose the number of slots is l, then the auction module\nwill attribute l slots to the top l bidders and these bidders will receive different values according\nto different exposure rates of slots. In the environment, l is set to 3. Let slotij represent the slot\nof ad opportunity j wined by agent i and eij ∈[0, 1] represent the exposure rate of slotij, then the\noptimization formulation of BCB with multiple slots is as follows:\n21\nmaximize\n{αt\ni}\nT\nX\nt=1\nm\nX\nj=1\net\nijxt\nijvt\nij\ns. t.\nT\nX\nt=1\nm\nX\nj=1\net\nijxt\nijct\nij ≤ωi,\n(5)\nIn summary, the multiple slots property increases the complexity of the optimal bidding strategy,\nsince the exposure rate is a discount factor for both the cost and values. For instance, a strategy using\na lower budget to bid for slots with relatively lower ranks may be better than the strategy that always\nchases the highest value slot. We believe supporting multiple slots in AuctionNet will be beneficial to\nreducing the gap between related research and the real-world online advertising platforms.\nG.3\nBidding Module\nThe bidding module is responsible for processing the multi-agent interactions between advertisers.\nThis module implements the budget constraint and models the auto-bidding problem with sequence\ndecision-making. Therefore, AuctionNet supports the mainstream paradigms including Budget\nConstrained Bidding (BCB) [33] and Multiple Constraints Bidding (MCB) [16] in the auto-bidding\nfield. This will help researchers validate and gain insights from existing algorithms.\nIn the bidding module, we explicitly model the multi-agent setting. Researchers can implement\nmulti-agent algorithms to achieve competition or cooperation among different agents. The varying\nbidding strategies of other agents can better reflect the complex and dynamic auction environment in\nreal-world online advertising platforms. Besides, researchers can only control a part of the agents in\nAuctionNet while others is uncontrollable. This scenario is closer to the real advertising platform.\nThe multi-agent setting of AuctionNet can adapt to different research objectives.\nThere are several different metrics for different business goals of advertisers in online advertising\nplatforms such as Return-on-Investment (ROI) and Return-On-Ad-Spend (ROAS). AuctionNet\nhas several built-in metrics covering the popular metrics used by the major advertising platform.\nResearchers can adopt these metrics conveniently to evaluate the performances of their auto-bidding\nstrategies. Besides, researchers can define customized metrics according to their research objectives.\nAdditionally, several popular algorithms in the auto-bidding field have been implemented as baselines\nin AuctionNet, including PID Controller[36], Online LP[15], IQL[21], Behavior Cloning[30], and\nDecision Transformer[8]. This can facilitate the interested researchers to quickly start up and evaluate\nthese baselines in a unified environment.\nH\nDetails of Deep Generative Networks\nH.1\nAd Opportunity Generation\nThe ad opportunity feature contains two parts of information: one is the basic identity information\nof users including gender, age, address and so on; another is the consumption records such as the\nconsumption amount and the number of orders in the last three months. The identity information\nconsists of discrete fields and each field has several candidates. The consumption records are\ncontinuous in general. Therefore, we process these two types of information with different measures.\nDiffusion [17] model is the most popular generative model recently which obtains SOTA performances\nin image generation with a simplified training process. The principle of the diffusion model is adding\nGaussian noises to original data in training and denoising from Gaussian noises in the generation\nprocess. We would like to adopt the diffusion model to generate the ad opportunity feature but\nstruggle with the denoising operation which can result in unreasonable outputs such as a negative\nconsumption amount. So we follow the idea of the Latent Diffusion Model (LDM)[25]. LDM has a\nlatent space to encode the original data. LDM combines the idea of diffusion model with VAE[20].\nLDM adds noises and denoises in the latent space with a diffusion model and generates data from the\nlatent space with an encoder and decoder.\nSpecifically, let U ⊂Rd be the space of ad opportunity feature data (u1, u2, · · · , uK) where d is the\ndimension of original data and K is the volume of the ad opportunity. Let Y ⊂Rd′ be the latent\n22\nspace (d > d′). The encoder and decoder are represented as gϕ and hψ respectively, where ϕ and ψ\nare the parameters. The function of the encoder gϕ is obtaining a latent representation of original\ndata as follows:\ngϕ(uk) = (µk, σk),\nyk ∼N(µk, σ2\nk),\nwhere yk ∈Y is the latent representation. In practice, the reparameterize trick [20] is applied to make\nsure this operation is differentiable in the backpropagation. Given the latent representation yk, the\ndecoder is responsible for reconstructing the original data from yk, i.e., hψ(yk) = ˜uk ∈U. Besides\nthe reconstruction, the latent distribution N(µk, σ2\nk) is expected to be close to the standard Gaussian\ndistribution N(0, 1). Therefore, we have the following loss function for the encoder and decoder:\nLrecons = 1\nK\nK\nX\nk=1\n∥uk −hψ(yk)∥2\n2 ,\nLreg = 1\nK\nK\nX\nk=1\nDKL\n\u0000N(µk, σ2\nk)∥N(0, 1)\n\u0001\n,\nwhere Lrecons is the reconstruction loss and Lreg is the regularization loss for the latent distribution.\nDifferent from the original idea of VAE, where the latent variable y ∈Y is sampled from N(0, 1) in\nthe generation process, LDM uses a diffusion model in the latent space to generate the latent variable.\nIn general, the idea of the diffusion model is adding Gaussian noises to the original data to obtain\nvariables in N(0, 1) and denoising from N(0, 1) for generation. Given a latent variable y, we denote\nthe noisy version of y after p iterations as yp. The diffusion model has a network to predict noise\nϵθ(yp, p) and the loss function can be represented as\nLLDM = 1\nK\nK\nX\nk=1\n∥ϵ −ϵθ(yk,pk, pk)∥2\n2,\nwhere ϵ ∼N(0, 1), yk is the latent embedding of uk and pk is uniformly sampled from the set\n{1, 2, · · · , pmax}. ϵθ(yp, p) is the only learnable network in the diffusion model, with which the\nprocess of adding noises and denoising can be completed by the basic operations.\nAs for the generation process, a latent variable ¯y is sampled from N(0, 1) and ˜y is obtained by tmax\ndenoising steps from ˜y given the noise prediction network ϵθ. Finally, the decoder generates an ad\nopportunity feature based on ˜y as ˜x = hψ(˜y).\nH.2\nValue Prediction\nThe value prediction model needs to handle three types of information: ad opportunity features,\ncategory information and time information. The category information corresponds to the industry\ncategories of advertisers and the time information corresponds to the time when the ad opportunity\narrived. In our model, the category and time information are simplified as discrete values. Therefore,\nwe aim to integrate the category and time information into the ad opportunity features for better value\nprediction. Besides, we hope this integration can partly reflect the variation pattern of the impression\nvalues related to advertisers’ features and time.\nMulti-head attention (MHA) [31] is a popular network architecture and the critical part of Transformer\n[31]. MHA can capture the relations among a sequence, thus we hope to utilize MHA for better\nintegration. The formulation of the attention network is straightforward as Attention(Q, K, V ) =\nsoftmax( QKT\n√\nd )·V . Multi-head attention can be viewed as applying the attention network in different\nrepresentation subspaces as follows:\nMultiHead(Q, K, V ) = Concat(head1, head2, · · · , headh)W O,\n(6)\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni ).\n(7)\nW Q\ni , W K\ni , W V\ni\nare the parameters for the projection networks of head i and W O is the output\nnetwork parameters of the MHA model.\nWe combine cross-attention and self-attention to integrate the three types of information. Suppose uk,\nutime\nk\nand qk are the ad opportunity feature, time information and category information in a single\n23\nrecord respectively, then we will process the information as follows:\nQ(1) = τ (1)\nQ (utime\nk\n),\nK(1) = τ (1)\nK (uk),\nV (1) = τ (1)\nV (uk),\nz(1)\nk\n= MultiHead(Q(1), K(1), V (1)),\nQ(2) = τ (2)\nQ (z(1)\nk ),\nK(2) = τ (2)\nK (z(1)\nk ),\nV (2) = τ (2)\nV (z(1)\nk ),\nz(2)\nk\n= MultiHead(Q(2), K(2), V (2)),\nQ(3) = τ (3)\nQ (qk),\nK(3) = τ (3)\nK (z(2)\nk ),\nV (3) = τ (3)\nV (z(2)\nk ),\nz(3)\nk\n= MultiHead(Q(3), K(3), V (3)),\nQ(4) = τ (4)\nQ (z(3)\nk ),\nK(4) = τ (4)\nK (z(3)\nk ),\nV (4) = τ (4)\nV (z(3)\nk ),\nzk = MultiHead(Q(4), K(4), V (4)),\nwhere τ (1), τ (2), τ (3), τ (4) are the projection function for the multi-head attention network.\nThe variation of ad opportunity values has some temporal patterns in the real world. Therefore, we\nfollow the position encoding idea in Transformer [31] and Diffusion Model [17] to process the time\ninformation. Let PE : N →Rd represent the position encoding function, then\nPE2s(t) = sin\n\u0012\nt\n10000\n2s\nd\n\u0013\n,\nPE2s+1(t) = cos\n\u0012\nt\n10000\n2s\nd\n\u0013\n,\nwhere t is the discrete time and s corresponds to the dimension in the embedding PE(t). Let\nek = PE(utime\nk\n), then the value prediction is conducted by ˆvk = Uξ(zk, ek), where Uξ(zk, ek) is the\nprediction network with a similar architecture to the U-Net [26] used by the Diffusion Model [17].\nThe loss of the value prediction model is shown below:\nLpred = 1\nN\nN\nX\nk=1\n∥vk −ˆvk∥2\n2,\nwhere vk is the true value of the ad opportunity in the record of uk.\n24\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s\ncontributions and scope? [Yes] See Section 1.\n(b) Did you describe the limitations of your work? [Yes] See Section 8.\n(c) Did you discuss any potential negative societal impacts of your work? [N\/A] We\nbelieve our benchmark does not involve any potential negative societal impacts.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N\/A]\n(b) Did you include complete proofs of all theoretical results? [N\/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See Appendix\nB.5.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Appendix A.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] See Section ??.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.\n4. If you are using existing assets (e.g., code, data, models) or curating\/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [N\/A]\n(b) Did you mention the license of the assets? [Yes] See Appendix B.5.\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nSee Appendix B.5.\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing\/curating? [Yes] See Appendix B.5.\n(e) Did you discuss whether the data you are using\/curating contains personally identifiable\ninformation or offensive content? [Yes] See Appendix B.3.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N\/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N\/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N\/A]\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games.pdf"}
{"title":"From Code to Play: Benchmarking Program Search for Games Using Large Language Models","authors":"Manuel Eberhardinger, James Goodman, Alexander Dockhorn, Diego Perez-Liebana, Raluca D. Gaina, Duygu Çakmak, Setareh Maghsudi, Simon Lucas","summary":"Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one.","url":"http:\/\/arxiv.org\/abs\/2412.04057v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.04057v1","published":1733395858000,"comment":"Submitted to Transactions on Games Special Issue on Large Language\n  Models and Games","pdf_text":"1\nFrom Code to Play: Benchmarking Program Search\nfor Games Using Large Language Models\nManuel Eberhardinger, James Goodman, Alexander Dockhorn, Diego Perez-Liebana, Raluca D. Gaina, Duygu\nC¸ akmak, Setareh Maghsudi, Simon Lucas\nAbstract—Large language models (LLMs) have shown impres-\nsive capabilities in generating program code, opening exciting\nopportunities for applying program synthesis to games. In this\nwork, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing\non two programming languages, Python and Java. We use an\nevolutionary hill-climbing algorithm, where the mutations and\nseeds of the initial programs are controlled by LLMs. For Python,\nthe framework covers various game-related tasks, including five\nminiature versions of Atari games, ten levels of Baba is You,\nan environment inspired by Asteroids, and a maze generation\ntask. For Java, the framework contains 12 games from the TAG\ntabletop games framework. Across 29 tasks, we evaluated 12\nlanguage models for Python and 8 for Java. Our findings suggest\nthat the performance of LLMs depends more on the task than\non model size. While larger models generate more executable\nprograms, these do not always result in higher-quality solutions\nbut are much more expensive. No model has a clear advantage,\nalthough on any specific task, one model may be better. Trying\nmany models on a problem and using the best results across them\nis more reliable than using just one.\nIndex Terms—Game AI, Large Language Models, Program\nSynthesis\nI. INTRODUCTION\nBefore the emergence of large language models (LLMs)\nfor code [1], program synthesis in imperative or object-\noriented languages like Python or Java was considered highly\nchallenging due to the combinatorial explosion of the search\nspace [2]. Therefore, most solvable tasks were restricted to\nsimple problem domains such as string manipulation or list\nsorting, typically implemented within a predefined domain-\nspecific language (DSL) [3]. Similarly, program synthesis\nfor games was limited to simple problems with well-defined\nsearch spaces, achievable only by incorporating high-level\ngame-specific concepts into the DSL [4], [5], [6], [7].\nThe use of program synthesis with high-level programming\nlanguages in game research has hardly been explored. Most\ndiscussions merely outlined its potential applications [8], or\nManuel Eberhardinger is with the Institute of Applied AI, Stuttgart Media\nUniversity, Nobelstr. 10, 70569 Stuttgart, Germany (Corresponding Author;\nemail: eberhardinger@hdm-stuttgart.de)\nJames Goodman, Diego Perez-Liebana, Raluca D. Gaina and Simon Lucas\nare with the School of Electronic Engineering and Computer Science, Queen\nMary University of London, E1 4NS London, U.K.\nAlexander Dockhorn is with the Institute for Information Processing, Leibniz\nUniversity Hannover, Appelstr. 9A, 30167 Hannover, Germany\nDuygu C¸ akmak is with Creative Assembly, RH12 1JW Horsham, U.K.\nSetareh Maghsudi is with the Chair of Learning Technical Systems, Ruhr-\nUniversity Bochum, Universit¨atsstr. 150, 44801 Bochum, Germany\nFig. 1: The general framework for program search begins by\ngenerating an initial task prompt (1), which is processed by one\nof the integrated LLMs to produce a function (2). This function\nis then evaluated within a subprocess (3), which executes the\nprogram in the given task (4) and then the results are reported\nback to the main process (5). The main process updates the\nprompt based on the evaluation outcomes and either returns it\nto the LLM (repeat from 1) for further refinement or concludes\nif the evaluation criteria are reached.\nfocused on the missing aspects of automated game develop-\nment systems to move from game description languages to\nprogramming languages [9].\nRecently, methods for LLM-based program search have\nbeen introduced for the automatic design of playable games\nbased on program code [10], [11], [12] and to generate game\ncontent through JSON representations [13]. LLMs have also\nbeen adapted to synthesize programmatic policies in Python,\nwhich are then converted into a DSL suitable for the target\nenvironment [14], as well as to construct world models in\nPython that approximate reward and state transition functions\nfor simple games, enabling action plan generation [15].\nIn this work, we explore the potential of LLM-based program\nsearch for a wider range of games without depending on a\npredefined JSON converter [13] or on predefined specifications\narXiv:2412.04057v1  [cs.AI]  5 Dec 2024\n2\nsuch as a DSL (e.g., Ludii [10], the video game description\nlanguage [11], or Karel [14]). Our aim is to enable LLMs to\nsynthesize program code that can be used directly, without\nrequiring additional transformations or prior specifications. We\nevaluate this approach across different domains using two\nprogramming languages: Python and Java. In Python, we focus\non synthesizing programmatic agent policies and functions for\nprocedural content generation (PCG). In Java, the method is\nintegrated into TAG, a tabletop games framework, in which\nLLMs design heuristics for board games [16].\nOur goal is not to propose a new method for program\nsynthesis, but to introduce an easy-to-use and extensible\nframework to evaluate the current performance of LLMs for\nthe synthesis of game-related program code. To achieve this,\nwe have integrated five different LLM architectures for Python\nand four for Java, and evaluated 12 and 8 models, respectively.\nFor the synthesis of Python code, the framework consists of\nfive miniature versions of Atari games where the input is\nrepresented symbolically [17], ten levels of the game Baba\nis you, in which various game mechanics are tested [18], a\nvehicle driving environment based on the game Asteroids, and\nprocedural content generation in the form of mazes. For Java,\nthe framework consists of 12 tabletop games of the TAG\nframework [16]. In total, we evaluate the LLMs on 29 different\ntasks. An overview of our proposed framework, as well as\ngames and LLMs used, is shown in Figure 1.\nOur contributions are:\n• We perform an empirical study to evaluate the current\nstate-of-the-art of LLM-based program search for games.\n• We introduce an easy-to-use and extensible framework\nwith 29 tasks that evaluate various aspects of game\nmechanics.\n• We open-source our code upon publication. Currently, only\nthe example prompts for the experiments are available in\nthe repository1.\nII. RELATED WORK\nThere are a considerable number of studies that use program\nsynthesis approaches for games. Butler et al. used SMT-solvers\nto search for programs within a Lisp-based DSL, enabling\nthe generation of diverse boss fights in Megaman [4] and\npuzzles for the game Nonograms [5]. In [6], a method was\nintroduced for learning combinations of logical programs to\nsolve simple grid-based games like Nim. Cropper et al. [19],\n[20], [21] developed a comprehensive benchmark of 50 games\nto recover game rules from gameplay traces using inductive\nlogic programming (ILP). Furthermore, Evans et al. [22]\napplied a differentiable form of ILP to learn interpretable rules\nfor Sokoban. Recently, a method for learning programmatic\npolicies for zero-shot coordination problems in cooperative\ntasks was introduced and demonstrated in the game Overcooked\n[23]. In contrast to learning programmatic policies, there is\nalso work focusing on using program synthesis to explain the\ndecision-making process of game-playing agents [24].\n1https:\/\/github.com\/ManuelEberhardinger\/Benchmarking-Language-Model-\nBased-Program-Search-for-Games\nMari˜no et al. [7] utilized program search to develop strategies\nfor the game MicroRTS, comparing the resulting programmatic\npolicies with those created by human developers. Their findings\ndemonstrated that the synthesized programs performed compa-\nrable to those written by humans. Subsequent research built\non this foundation by introducing improved search techniques,\nincluding bilevel search [25], by guiding the program search\n[26] or by searching in semantic spaces [27]. Recently, an\napproach for combining LLMs with local search algorithms was\nproposed for MicroRTS [28], where the authors showed that\nproviding initial programs with LLMs found better solutions\nfaster and improved the performance of the final programs.\nIn [29], Genetic Programming (GP) is used to search for\nevaluation functions within a predefined DSL for the board\ngame 7 Wonders. This approach resembles our experiments\nwith the TAG framework, where heuristic functions are syn-\nthesized; however, we use Java without relying on predefined\nconcepts. GP has also been applied to generate game agents for\nvarious scenarios, including a fighting game [30], a platformer\ngame [31], a puzzle game [32], and to create explanations for\na maze runner agent [33]. Additionally, Wilson et al. [34] used\nCartesian GP to develop programs for Atari games, processing\npixel observations through predefined mathematical, statistical,\nor list functions.\nA recent approach from cognitive science, known as\nlanguage-informed thinking [35], combines large language\nmodels (LLMs) with Bayesian inference. This method enables\nLLMs to pose questions in natural language, which are then\ntranslated into a language of thought [36] represented as a\nprobabilistic programming language. Grand et al. extended\nthis approach to the board game Battleship, demonstrating that\nthe questions generated by LLMs aligned closely with the\nperformance of human players [37].\nVerma et al. [38], [39] employed neurosymbolic methods\nto synthesize programmatic policies for a car racing game,\ndemonstrating that these programs were more robust than\nneural network policies while achieving comparable rewards.\nWhile this approach shares similarities with the vehicle driving\nexperiments in our work, it is more constrained, as the search\nspace is limited to the provided DSL and our vehicle driving\nproblem requires a planning algorithm to be solvable.\nIn [40], a reactive programming language with a novel\nprogram synthesis algorithm is introduced to discover causal\nstructures represented as state machines in combination with\nprogram code. They evaluate the proposed method for 2D grid\ngames similar to Super Mario.\nVoyager [41] is a lifelong learning agent for the Minecraft\nenvironment that uses an LLM to synthesize code in the\nMineflayer API2, which is then executable to obtain the actions.\nIn addition, a second LLM is used as a high-level planner to\ncreate a task curriculum for the agent. Moreover, Ma et al. [42]\nproposed an evolutionary approach using LLMs to synthesize\nreward functions for complex control tasks, achieving superior\nperformance compared to human-engineered reward functions.\nThe key distinction between our work and the discussed\nliterature is the use of high-level programming languages\n2https:\/\/github.com\/PrismarineJS\/mineflayer\/tree\/master\n3\n(Python and Java), making our approach applicable to a broader\nrange of tasks without relying on predefined building blocks or\na programming library. The work that is most similar to ours\nand is also used for game environments is [15], where Python\ncode is synthesized to approximate a world model. However,\nthis work is limited to a single, different type of task.\nIII. FRAMEWORK\nThe general framework we proposed is based on an evolu-\ntionary hill-climbing algorithm where the mutations and the\nseed of the initial program are performed by an LLM [14], [43].\nThus, our framework belongs to the group of neurosymbolic\nprogramming methods [44], as we use an LLM to generate\nprograms that are checked for correctness and functionality\nby symbolic verifiers, in our case the Python interpreter and\nJava compiler. The overview of the framework is illustrated in\nFigure 1, which shows the high-level interaction between the\ndifferent modules and processes. Synthesized program code by\nLLMs is always executed within a safe subprocess environment,\nensuring that the main process can terminate it after a certain\ntime limit to prevent infinite execution of the program.\nA detailed description of the complete algorithm is provided\nin Algorithm 1. Our framework consists of two iterative\nprocesses that control the length of the search, defined by\nthe input parameter iterations, as well as the number\nof attempts to generate, repair or improve the program in\neach iteration defined by the input parameter maxAttempts.\nEach iteration starts by generating a task prompt to obtain an\ninitial Python or Java function, and inject and run the code\nin a subprocess. Afterwards, the inner iterative process starts,\nwhere the task prompt is updated and the program is repaired\nor improved. If the function is executed successfully, we update\nthe prompt with the achieved evaluation metric and all relevant\nenvironment-specific details, such as the action trace of the\nexecuted function. If an error occurs, e.g. a syntax problem,\na runtime error due to improper array indexing or similar\nproblems, the error description is included in the prompt for\nrefinement of the program. These steps are repeated iteratively\nuntil the evaluation criteria defined by the fitness function are\nsatisfied or the specified number of maxAttempts is reached.\nThe outer loop is able to stop the current program search and\nrestart from the initial prompt or the last successful program\nfound. In order to achieve this we introduce the variable\nblankRestart, which is an input parameter to Algorithm 1.\nThis is necessary when the LLM tries to fix compilation errors\nin the case of Java or goes astray, such as when generating\nDQN code for the Atari environments, which we experienced\nin the preliminary experiments. This also depends on the\nproblem domain, therefore blankRestart is adapted to\nthe corresponding domain.\nWe explain the domain-specific adaptations of the framework\nin the respective chapters in section V. While the overall\nframework is similar for all tasks, domain-specific adaptations\nare necessary, such as the description of the environment or\nthe game logic, as well as the objective of the game.\nAlgorithm 1 The algorithm for our proposed framework, which\nconsists of two iterative processes that control the length of\nthe search and the number of attempts to generate, repair or\nimprove the program in each iteration.\nInput: task, iterations, maxAttempts, blankRestart\nOutput: program\n1: procedure PROGRAMSEARCH\n2:\nInit Vars: lastProgram, lastResult, lastFitness ←0\n3:\nfor i ←0 to iterations do\n4:\nif blankRestart or lastFitness == 0 then\n5:\nprompt ←GETTASKPROMPT(task)\n6:\nelse\n7:\nprompt ←UPDATETASKPROMPT(task, lastRe-\nsult, lastFitness, lastProgram)\n8:\nend if\n9:\nprogram ←QUERYLLM(prompt)\n10:\nresult ←INJECTANDRUNCODE(task, program)\n11:\nfitness ←EVALUATEFITNESS(task, result)\n12:\nj ←1\n13:\nwhile not CHECKCRITERION(fitness) or\n14:\nj < maxAttempts do\n15:\nprompt ←UPDATETASKPROMPT(task, result,\nfitness, program)\n16:\nprogram ←QUERYLLM(prompt)\n17:\nresult ←INJECTANDRUNCODE(task, program)\n18:\nfitness ←EVALUATEFITNESS(task, result)\n19:\nj ←j + 1\n20:\nif fitness > lastFitness then\n21:\nlastProgram ←program\n22:\nlastResult ←result\n23:\nlastFitness ←fitness\n24:\nend if\n25:\nend while\n26:\nend for\n27: return program\n28: end procedure\nIV. LARGE LANGUAGE MODELS\nFor our benchmark, we integrated five LLM providers for\nPython and four for Java in our framework, namely the small\nand the large version for each model type. From OpenAI,\nwe utilize models from the GPT-4o family3, based on GPT-\n4 [45]. We also incorporate the latest models from Mistral4,\nClaude 3.55 from Anthropic, based on Claude 3 [46], and\nthe latest Gemini models6 [47], provided by Google, for both\nprogramming languages.\nModels from the Llama 3.1 family [48] are included only\nfor Python tasks, as they are not supported by the LangChain4j\nlibrary,7 which we integrated into the TAG framework. For the\nnew ChatGPT models in the o1 generation, we use o1-mini,\n3https:\/\/platform.openai.com\/docs\/models\n4https:\/\/docs.mistral.ai\/getting-started\/models\/models overview\/\n5https:\/\/docs.anthropic.com\/en\/docs\/about-claude\/models\n6https:\/\/ai.google.dev\/gemini-api\/docs\/models\/gemini\n7https:\/\/docs.langchain4j.dev\n4\nLLM\nPython\nJava\nLlama 3.1 8B\n2024-04-18\n-\nLlama 3.1 70B\n2024-04-18\n-\nLlama 3.1 405B\n2024-04-18\n-\nClaude 3.5 Haiku\n2024-10-22\n2024-03-07\nClaude 3.5 Sonnet\n2024-10-22\n2024-06-20\nGPT 4o mini\n2024-07-18\n2024-07-18\nGPT 4o\n2024-08-06\n2024-08-06\no1-mini\n2024-09-12\n-\nMistral Small\n2024-09\n2024-09\nMistral Large\n2024-07\n2024-07\nGemini Pro\n2024-09\n2024-09\nGemini Flash\n2024-09\n2024-09\nTABLE I: The used LLMs with the specific version or date of\nthe release.\nwhich offers performance comparable to o1-preview for coding\ntasks.8 However, this model is also limited to Python.\nDetails on the model versions and their release dates are\nsummarized in Table I.\nV. GAME APPLICATIONS\nIn the following section, we describe the experiments that\nwere conducted for each of our target domains.\nA. Programmatic Policies: Minatar\nMinatar [17] is a collection of five games that are miniature\nversions of Atari games. In Minatar, the games are represented\nas a symbolic state space on a 10×10×n grid, where n\nrepresents the number of channels, and each channel represents\nan object such as walls, enemies or the agent. Minatar is an ideal\ntest bed for experiments, as the games are more efficient to learn\nwithout changing the game mechanics of the original game.\nPreviously, Minatar was used in [24] to explain the behaviour\nof agents through program synthesis, but it was only possible\nto explain short sub-trajectories since enumerative search-based\nmethods were used to search through a predefined domain-\nspecific language that resembles Lisp. In our experiments, we\nuse all available Minatar environments, which are shown in\nFigure 2. The game descriptions are outlined below:\n• Seaquest: In this game, the agent controls a submarine and\nis able to shoot bullets. The objective is to save as many\ndivers as possible, while also shooting enemy submarines\nor sharks. Each time an enemy is struck, the reward is\nincreased by one. When the submarine saves the divers,\nthe agent also receives a reward.\n• Freeway: The agent controls a chicken that needs to\ncross a road during rush hour, while avoiding the traffic.\nFor each chicken that crosses the road safely, the agent\nreceives one point.\n• Asterix: The objective of the game is to collect gold while\navoiding enemies. The player gets one reward for each\ncollected gold and the game is over when the player is\nhit by an enemy.\n8https:\/\/openai.com\/index\/openai-o1-mini-advancing-cost-efficient-\nreasoning\/\nFig. 2: The five miniature versions of the Atari games (left\nto right, top to bottom: Seaquest, Freeway, Asterix, Space\nInvaders and Breakout), which are used for the synthesis of\nthe programmatic strategies. Each colour represents a different\ntype of object, e.g. the paddle in dark blue, the ball in green\nand the track of the ball in pink for the game Breakout.\n• Space Invaders: The agent controls a cannon and shoots\naliens while dodging bullets launched from the alien\nspaceship. Additionally, the player must prevent the aliens\nfrom reaching the bottom of the screen. For each destroyed\nalien, one reward is received.\n• Breakout: The goal is to destroy all the bricks with the\nball by controlling the paddle to bounce the ball off before\nit goes out off the screen. With each destroyed brick the\nagent receives a reward of one.\nThe LLMs were prompted to generate a Python function\nwhich can be used as a policy to play the game. The prompt\ncontains information about the game rules, the objective of\nthe agent and also the possible actions of the environment\nand available game objects. The description of the game, was\ntaken from Young and Tian [17]. The prompts for the games\nare available in the code repository9. The LLM receives only\nthe initial state, which is preprocessed from the state input of\nthe environment, a one-hot encoded 3D array, into a 2D array\nwith text descriptions for each grid cell representing the cell’s\nobject. Figure 3 shows an example of the converted state for\nBreakout. All other games convert the state in a similar way\nso that the LLM can process the state input semantically.\nEach of the games tests the LLM for different game concepts.\nSpace Invaders, Breakout and Freeway restrict the agent’s\nmovement by only allowing horizontal or vertical movement.\nSpace Invader and Seaquest allow the player to fight the enemy,\nwhile in Asterix and Freeway the player can only avoid the\nenemies. In Asterix, the player must also collect items in order\nto receive a reward. Seaquest is the most difficult game, as the\nplayer has to collect six divers and then reach the surface so\nthat the divers can leave the submarine, but at the same time\nthe player has to shoot down enemies. Breakout, on the other\nhand, is one of the easier games as there are no opponents and\n9https:\/\/github.com\/ManuelEberhardinger\/Benchmarking-Language-Model-\nBased-Program-Search-for-Games\n5\nFig. 3: The text description of the state for the Breakout game\nwhich is included in the prompt.\nTABLE II: Average reward of all executable programs for the\nMinatar experiments with the successful programs for each\ngame. In each case, 10 iterations of the search were performed,\nusing 3 queries in each iteration to create or improve the policy.\nFor each program, 50 evaluation episodes were performed.\nModel\nSeaquest\nFreeway\nBreakout\nAsterix\nSpaceInvader\nLlama 3.1 8B\n0.0 (5)\n0.0 (7)\n0.27 (7)\n0.49 (4)\n1.5 (6)\nLlama 3.1 70B\n0.07 (10)\n1.88 (10)\n1.75 (10)\n1.72 (9)\n3.32 (10)\nLlama 3.1 405B\n0.12 (10)\n1.15 (10)\n1.92 (10)\n2.51 (9)\n5.79 (10)\nClaude 3.5 Haiku\n0.18 (10)\n1.96 (10)\n2.99 (10)\n2.81 (10)\n6.6 (10)\nClaude 3.5 Sonnet\n0.72 (10)\n0.56 (10)\n4.16 (10)\n2.92 (10)\n11.59 (10)\nGPT 4o mini\n0.06 (10)\n1.18 (9)\n0.82 (10)\n1.49 (10)\n4.62 (10)\nGPT 4o\n0.17 (10)\n4.23 (10)\n2.73 (10)\n3.88 (10)\n6.26 (10)\no1-Mini\n0.62 (10)\n3.94 (10)\n2.65 (10)\n3.12 (10)\n9.48 (10)\nMistral Small\n0.0 (0)\n1.16 (5)\n0.22 (4)\n1.02 (3)\n4.31 (8)\nMistral Large\n0.0 (10)\n5.25 (10)\n3.72 (10)\n2.08 (8)\n5.21 (10)\nGemini Flash\n0.0 (10)\n1.19 (7)\n1.21 (9)\n1.62 (7)\n3.77 (10)\nGemini Pro\n0.08 (10)\n1.72 (10)\n2.87 (10)\n1.81 (6)\n4.44 (10)\nthe player only has to anticipate where the ball will land in\norder to bounce it off with the paddle.\nFor all Minatar experiments, we use 10 iterations with\nmax three attempts in each iteration, which results in max\n30 prompts for each model. For each program, 50 evaluation\nepisodes were performed. Table II shows the average reward\nof all executable programs for each LLM with the number of\nsuccessful iterations in the brackets – an executable program\nthat returns a positive reward. In general, the larger models\nperform better than their smaller counterparts in terms of\naverage reward, with Claude Sonnet being the best performing\nmodel. Only at Freeway the Claude Haiku and Llama 70B\nmodels beat their larger counterparts in the family. o1-mini\nalso outperforms GPT 4o in Space Invader and Seaquest. This\nis, however, not represented by the maximum reward shown\nin Table III. Only in Breakout, Claude Sonnet is the top-\nperforming model regarding the average and maximum reward,\nbut is beaten in three games in terms of the maximum reward\nby Claude Haiku. This pattern is also visible in the other\nLLM families where the small model outperforms their larger\nversion.\nIt can also be seen from both tables that it is more difficult\nto find good programs for more complicated games such as\nSeaquest. Only o1-mini, which is praised by OpenAI for its\nsophisticated reasoning capabilities, performs well in this game,\nbut fails to beat the other LLMs on the simpler games. Looking\nat the best programs for each LLM, o1-mini manages to\ncorrectly locate enemies and shoot them if they are in a line,\nwhile the other programs only check if enemies are nearby\nwithout checking if they are in a line. o1-mini is also the only\nmodel that uses the Manhattan distance to move to nearby\nTABLE III: Max Reward of the best program for the Minatar\nexperiments with the same experiment setup as Table II.\nModel\nSeaquest\nFreeway\nBreakout\nAsterix\nSpaceInvader\nLlama 3.1 8B\n0.0\n0.0\n1.1\n1.2\n5.9\nLlama 3.1 70B\n0.6\n8.7\n9.3\n4.4\n6.3\nLlama 3.1 405B\n0.6\n4.8\n9.0\n6.4\n20.2\nClaude 3.5 Haiku\n0.8\n7.5\n12.2\n11.0\n24.5\nClaude 3.5 Sonnet\n2.3\n5.3\n20.2\n6.9\n22.8\nGPT 4o mini\n0.7\n5.6\n3.0\n3.8\n17.8\nGPT 4o\n1.1\n9.5\n7.3\n8.7\n22.4\no1-mini\n11.3\n8.8\n4.8\n10.4\n22.6\nMistral Small\n0.0\n5.8\n0.7\n3.9\n21.9\nMistral Large\n0.0\n9.9\n7.7\n5.9\n11.9\nGemini Flash\n0.0\n7.4\n5.6\n3.0\n13.3\nGemini Pro\n0.8\n6.4\n6.2\n4.8\n15.4\nenemies, while all other programs only try to shoot enemies\nor rescue divers.\nFor Freeway, all of the best performing programs show\nsimilar behavior for each LLM, while the poor performing\nmodels struggle to correctly implement a one step lookahead\n(OSLA) of the cars. The best performing program of Mistral\nLarge is the only program that uses the modulo operation to\ncorrectly predict when the car will be teleported to the other\nside when it drives out of the screen, which was mentioned in\nthe prompt.\nFor Breakout, the best programs of Mistral Small and Llama\n8B only receive points by chance, as they return a random\naction or do not take the position of the ball into account.\nAll other programs are able to locate the ball and also the\ndirection in which the ball is moving and move the paddle in\nthat direction. Claude Sonnet is the only model that correctly\nuses OSLA to predict the next column of the ball. GPT 4o\nalso uses OSLA, but confuses the row with the column.\nIn Asterix, the best performing model Claude Haiku priori-\ntizes the gold, but also checks whether it is moving towards\nan enemy as it approaches the gold. The mediocre performing\nmodels often prioritize the gold without checking if there are\nenemies nearby. o1-mini, instead, uses action values that are\nupdated depending on nearby gold coins and enemies, but is\nstill not able to beat Claude Haiku.\nFor Space Invader, the good performing programs with a\nreward over 20, correctly locate enemy bullets, aliens and the\ncannon. The programs below 20, do confuse sometimes the\ncolumn with the row or only take into account the first alien\nfound in the state. The smallest LLM, Llama 8B, produces\na program that fires when there are no friendly bullets, i.e.\nbullets shot by the program itself, in the state, and was still\nable to get a reward of 5.9 even though the program makes\nno sense at all.\nOverall, it can be said that in the Minatar games larger\nmodels in most cases show a more sophisticated behavior in\nthe programs, but as can be seen with Claude Haiku, this is\nnot always the case. Currently, only a very simple prompting\nstrategy is used, which already gives good results in most\ngames. Using more complicated prompting strategies, such as\nChain of Thought [49] or adding a crossover operator could\nlead to improvements in the programs found.\n6\nTABLE IV: The minimum distance of the best program for the Asteroids ship driving experiments for different rotation speeds\nω in degrees per second. In each case, 10 iterations of the search were performed, using 3 queries in each iteration to create or\nimprove the policy. For each program, the same set of 5 evaluation tasks were used, each with different target positions and\ninitial states of the ship.\nModel\nω = 10\nω = 20\nω = 30\nω = 40\nω = 50\nω = 60\nω = 70\nω = 80\nω = 90\nω = 100\nDavg\nLlama 3.1 405B\n145.84\n130.82\n109.03\n105.64\n80.97\n87.46\n57.69\n64.41\n82.57\n57.11\n92.15\nLlama 3.1 8B\n186.14\n186.14\n186.14\n177.24\n116.08\n186.14\n186.14\n131.11\n106.02\n179.77\n164.09\nLlama 3.1 70B\n111.84\n132.64\n147.98\n111.67\n80.97\n64.53\n79.61\n106.37\n74.42\n104.63\n101.47\nClaude 3.5 Haiku\n174.29\n142.54\n95.95\n86.20\n102.06\n76.60\n103.84\n76.42\n93.45\n67.51\n101.89\nClaude 3.5 Sonnet\n153.04\n122.59\n102.66\n88.09\n83.57\n85.54\n78.21\n85.75\n66.23\n78.59\n94.43\nGPT 4o mini\n154.57\n136.83\n118.76\n128.51\n102.21\n110.48\n111.68\n111.30\n104.90\n97.49\n117.67\nGPT 4o\n137.02\n142.58\n111.51\n111.69\n104.16\n105.40\n96.30\n85.88\n57.98\n94.43\n104.70\no1-mini\n130.86\n167.24\n114.41\n111.67\n105.38\n96.21\n68.51\n85.79\n93.41\n94.43\n106.79\nMistral Small\n186.14\n186.14\n186.14\n180.56\n120.38\n117.30\n108.41\n110.75\n104.90\n186.14\n148.69\nMistral Large\n186.85\n121.88\n96.40\n83.88\n101.07\n68.66\n97.34\n58.75\n92.38\n94.76\n100.20\nGemini Flash\n156.79\n145.74\n149.34\n178.73\n111.47\n110.73\n115.08\n117.94\n186.14\n186.14\n145.81\nGemini Pro\n180.41\n122.72\n146.10\n75.36\n106.88\n105.51\n112.58\n154.27\n102.75\n114.61\n122.12\nB. Vehicle Driving\nThe task is to pilot an Asteroids-style spaceship from its\nstart state to the target, where it should rest until the end of\nthe episode. Each episode is 101 steps. At each step, there\nare 4 discrete actions: NO OP, THRUST, ROTATE LEFT,\nROTATE RIGHT. We experimented with vehicle physics in\norder to make an interesting challenge. Drag is set to be low,\nwhich leads to a high risk of overshooting the target unless\ncountermeasures are taken. At each step, the agent is given an\nobservation of the ship state and the position of the target.\nThe prompt includes some helper classes and functions,\nincluding a Vector2d class and the Asteroids ship, as well\nas a Vehicle superclass. In addition, we add strong hints to\nmake the problem solvable for LLMs, which are summarized\nas follows10:\n• Best solved using search algorithms: try One Step\nLookahead, Monte Carlo Tree Search or Rolling Horizon\nEvolution.\n• Try using a heuristic function that values facing towards\nthe target as well as being close to the target.\n• Try using Macro-actions - e.g. simply repeating each\naction a number of times.\nTable IV shows the results of the driving experiments for\ndifferent rotation speeds ω to adjust the difficulty level of\nsteering the asteroid ship. The numbers are the minimum\ndistance achieved by the best program for five evaluation\nepisodes. Davg is the average of all distances for each LLM.\nThe experiments were conducted with 10 iterations of search\nand three attempts to generate or improve the program. We\nomitted the number of successful iterations because no LLM\nmanaged to stop the asteroid ship at the target position in all\nfive evaluation episodes. A program was considered successful\nonly if it could consistently stop the vehicle within a specified\ntolerance t across all evaluation episodes. In our experiments,\nthe synthesized programs succeeded in stopping the vehicle\nin only one or two episodes within a tolerance of t = 10,\nand thus no program qualified as successful. The overall\nbest model is Llama 3.1 405B followed by Claude Sonnet.\nRegarding Davg larger models generally outperformed their\n10The complete prompt is given in the code repository.\nsmaller counterparts, with o1-mini being slightly worse than\nGPT-4o. For the different rotation speeds, Llama 3.1 405B and\n70B generated the best program in three out of 10 tasks, for\nthe other tasks the best programs were synthesized by the large\nLLMs, except for ω = 30, where Claude Haiku found the best\nprogram. As no LLM successfully solved the problem with a\nsimple prompting strategy in this experiment, we consider it a\ncompelling challenge for future research.\nC. Baba is You\nBaba is you is a complex puzzle game in which the player\nmanipulates a 2D grid environment to reach a given goal. The\nenvironment consists of word blocks and corresponding entities\nthat can be pushed around. By placing word blocks next to\neach other, rules can be formed. These rules are active as long\nas the given word block sequence remains intact. This way,\nplayers can change how objects behave, which objects they\ncontrol, or which conditions must be satisfied to win.\nFor our experiments, we used a Python version11 of the Keke\nis You AI framework [18]. For this domain, we prompted the\nLLMs to provide a policy, giving a short description of the\ngame and the initial state of the level (the complete prompt is\ngiven in the repository). Similar to the Minatar experiments,\nthe state is converted into a text description. The function to\nbe written should use the current state as input and return a\nmovement direction or the command for waiting a turn. Each\nepisode ends after 100 actions or once the win condition is\nfulfilled. A reward is awarded based on the maximum number\nof actions (100) minus the number of steps taken. Thus, the\nreturn can be maximized by finishing the level as fast as\npossible. Each level can be solved in less than 20 actions.\nIn our tests, we queried the agent to solve 10 simple demo\nlevels (see Figure 4). Each of the levels focuses on one\nor multiple key mechanics of the framework such as rule\ninterpretation (levels 1-10), rule creation (levels 2, 3, 5) or\ndestruction (levels 6, 8, 9, 10), and object manipulation (level\n7). Table V shows the results of our comparison of the LLM\nmodels’ capabilities. Each entry shows the reward of the best\nprogram after 10 iterations, in which each iteration used 3\n11https:\/\/github.com\/ADockhorn\/Keke-AI-PY\n7\n(a) Level 1\n(b) Level 2\n(c) Level 3\n(d) Level 4\n(e) Level 5\n(f) Level 6\n(g) Level 7\n(h) Level 8\n(i) Level 9\n(j) Level 10\nFig. 4: Demo levels used for the evaluation of LLM capabilities in the Baba is You domain.\nTABLE V: Highest reward per language model and level (with number of successful runs per level).\nModel\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nLevel 5\nLevel 6\nLevel 7\nLevel 8\nLevel 9\nLevel 10\n#Levels\nsolved\nLlama 3.1 8B\n95 (7)\n0.0 (0)\n0.0 (0)\n95 (5)\n0.0 (0)\n0.0 (0)\n58 (2)\n95 (5)\n0.0 (0)\n0.0 (0)\n4\nLlama 3.1 70B\n95 (7)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n94 (3)\n95 (3)\n0.0 (0)\n0.0 (0)\n3\nLlama 3.1 405B\n95 (9)\n0.0 (0)\n0.0 (0)\n95 (1)\n0.0 (0)\n0.0 (0)\n94 (7)\n95 (4)\n0.0 (0)\n90 (1)\n5\nClaude 3.5 Haiku\n95 (10)\n0.0 (0)\n0.0 (0)\n95 (5)\n0.0 (0)\n25 (1)\n94 (8)\n95 (10)\n0.0 (0)\n0.0 (0)\n5\nClaude 3.5 Sonnet\n95 (10)\n0.0 (0)\n0.0 (0)\n95 (9)\n0.0 (0)\n91 (2)\n94 (6)\n95 (9)\n90 (2)\n92 (1)\n7\nGPT 4o mini\n95 (7)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n90 (1)\n95 (6)\n0.0 (0)\n0.0 (0)\n3\nGPT 4o\n95 (10)\n0.0 (0)\n0.0 (0)\n95 (4)\n0.0 (0)\n91 (1)\n94 (5)\n95 (8)\n0.0 (0)\n0.0 (0)\n5\no1-mini\n95 (10)\n0.0 (0)\n0.0 (0)\n95 (7)\n0.0 (0)\n83 (1)\n94 (7)\n95 (9)\n0.0 (0)\n92 (1)\n6\nMistral Small\n95 (5)\n0.0 (0)\n0.0 (0)\n95 (3)\n0.0 (0)\n0.0 (0)\n33 (1)\n95 (5)\n0.0 (0)\n0.0 (0)\n4\nMistral Large\n95 (10)\n89 (1)\n0.0 (0)\n95 (1)\n0.0 (0)\n0.0 (0)\n94 (8)\n95 (9)\n0.0 (0)\n0.0 (0)\n5\nGemini Flash\n95 (10)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n83 (1)\n94 (5)\n95 (8)\n0.0 (0)\n90 (1)\n5\nGemini Pro\n95 (6)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n0.0 (0)\n83 (4)\n76 (5)\n95 (6)\n0.0 (0)\n0.0 (0)\n4\nqueries to generate or improve the policy. The number of\nsuccessful iterations is shown in brackets.\nMost agents were able to solve at least 5 out of 10 levels,\nwith Claude 3.5 Sonnet being the only model able to solve 7.\nFor Claude, GPT and Mistral, models of the same vendor with\nhigher number of parameters were able to solve more levels.\nFor the Llama 3 models, the 8B model solved one more level\nthan the 70B model but was outperformed by the 405B model.\nSimilarly, Gemini Flash performed slightly better than Gemini\nPro. Tested models were mostly successful in interpreting\nexisting rules. As can be seen, some levels are rarely solved\nby any model. Creating or destroying rules and thus modifying\nthe logic of our game world has proven difficult for all models.\nNearly all models have failed in solving levels 2 and 3, which\nrequire rule creation, and levels 9 and 10 which require a\nrule’s destruction to finish the puzzle. Slight differences in\nthe observed success rate could be due to the low number of\nrepetitions per level resulting in sampling errors: levels 2, 9,\nand 10, which are rarely solved at all, could be affected by\nthis. Chain of thought prompting [49] may potentially help in\novercoming these more complex planning tasks.\nD. Procedural Content Generation\nPCG is a widely studied area in game research [50], [51].\nIn this experiment, we explored whether LLMs can synthesize\nFig. 5: Three generated mazes by algorithms aiming to optimise\nthe longest shortest path objective. Left: (score 18) example\nfrom a simple LLM generated algorithm setting wall cells\nwith a fixed probability. Middle: (score 38) example a more\nsophisticated LLM algorithm involving recursion and a shortest\npath algorithm. Right: (score 54) example from an evolutionary\nalgorithm directly optimising for the objective function.\nPython functions capable of generating diverse game content.\nTo assess this in a simple scenario, we tasked the LLMs with\ncreating functions that produce random mazes adhering to\nspecific design objectives.\nThe prompt advised the LLMs to use the longest shortest\npath objective to guide the maze generation process. This\nobjective encourages intricate and interesting mazes. Most of\nthe generated code ignored the hint and instead coded overly\nsimple algorithms, placing corridors and walls in each cell\nwith a given probability while usually ensuring that the start\n8\nTABLE VI: Maze generation LLM results. In each case 10\niterations of search were run, with up to three attempts to\nimprove or generate the policy. Dmax is the maximum distance\nof the shortest path of the generated mazes returned by the best\nprogram and Davg is the average distance of all executable\nprograms. Each program generates five mazes for evaluating\nDmax and Davg. S.Iter. is the percentage of iterations that\nresulted in working code.\nModel\nDmax\nDavg\nS.Iter.\nLlama 3.1 8B\n11.40\n5.25\n90%\nLlama 3.1 70B\n9.80\n2.50\n100%\nLlama 3.1 405B\n14.60\n2.63\n90%\nClaude 3.5 Haiku\n29.40\n5.58\n100%\nClaude 3.5 Sonnet\n16.40\n4.90\n100%\nGPT 4o mini\n29.40\n11.82\n100%\nGPT 4o\n29.40\n11.07\n100%\no1-mini\n17.40\n3.11\n100%\nMistral Small\n8.40\n1.84\n80%\nMistral Large\n7.40\n0.83\n100%\nGemini Flash\n28.40\n8.51\n70%\nGemini Pro\n4.00\n-0.08\n100%\nand end points were not on wall cells. An example generated\nmaze is shown in the left of figure 5. Occasionally, a better\nalgorithm was produced that mixed randomness, recursion\nand graph search in ways we’ve not fully analysed. These\nalgorithms sometimes produced mazes with no path between\nstart and end, resulting in a score of -1. When they worked,\nthey often produced reasonable mazes such as the one shown in\nthe middle of Figure 5. The LLMs failed to find an algorithm\nas effective as an evolutionary algorithm applied to directly\nsolve the objective. A sample maze from such an algorithm is\nshown on the right of the figure.\nNote that here we are evaluating the effectiveness of the\nalgorithms in meeting the specified objective, which is to\nproduce mazes with the longest shortest path between start\nand end. Depending on the application, this could be a poor\nobjective to maximise, with the best mazes have a mid-ranking\nscore, such as the central maze in Figure 5.\nTable VI shows the results for the maze generation experi-\nment. In contrast to the previous experiments, all small models\nbeat their larger counterparts. The only large model that is on\npar with the smaller ones is GPT 4o, which loses slightly to\nGPT 4o mini regarding the average distance Davg. o1-mini\nalso falls behind both GPT 4o models. The worst model is\nGemini Pro, which fails to even connect the start and end points\nof the maze most of the time, resulting in a negative average\nreward, while always producing executable programs. Even\nthe smallest LLM, Llama 3.1 8B, which was one of the worst\nmodels in the previous experiments, is better than the larger\nLlama models in terms of Davg and only loses by a small\nmargin to the largest Llama model in terms of Dmax. It should\nbe further investigated why larger LLMs have difficulties in\ngenerating good mazes, but this is beyond the scope of this\npaper.\nE. Python Code Evaluation\nTable VII shows the summary statistics of the synthesized\nPython code for the previous experiments. The two smaller\nTABLE VII: The overall evaluation of the synthesized Python\ncode. Cost is the total cost of all Python experiments. S.Iter. is\nthe percentage of iterations that resulted in working code and\nreturned a positive reward. Exec. Programs is the percentage of\nall generated programs that the Python interpreter could run.\nModel\nCost ($)\nS.Iter\nExec. Programs\nLlama 3.1 8B\nFree\n11.92%\n57.95%\nLlama 3.1 70B\nFree\n20.00%\n85.64%\nLlama 3.1 405B\n16.11\n20.00%\n82.31%\nClaude 3.5 Haiku\n5.19\n26.92%\n89.49%\nClaude 3.5 Sonnet\n17.58\n30.77%\n91.54%\nGPT 4o mini\n0.63\n18.08%\n86.28%\nGPT 4o\n9.25\n26.15%\n94.74%\no1-mini\n25.73\n30.38%\n95.26%\nMistral Small\n0.73\n10.00%\n63.21%\nMistral Large\n7.36\n20.00%\n83.97%\nGemini Flash\n0.28\n15.00%\n80.64%\nGemini Pro\n7.80\n17.31%\n88.46%\nLlama models are provided for free by Google Cloud as they\nare currently in public preview. It is visible that the more\nexpensive models lead to more executable programs. For the\nsuccessful iterations, the difference between smaller and larger\nmodels is not that significant. Claude Haiku is only 3.85%\nbelow the best model, Claude Sonnet, but costs less than a\nthird of the overall cost of the experiments. Being a small,\ncost-efficient model, it is impressive that Claude Haiku beats\nalmost all of the larger models, falling only behind Claude\nSonnet and OpenAI’s o1-mini. This was also evident in the\nprevious experiments where Claude Haiku produced the best\nprograms for Asterix, Space Invader, for ω = 30 in the vehicle\ndriving experiment, for level 8 of Baba is you and was also on\npar with two other LLMs for the maze generation experiment.\nThis indicates that large, expensive models are not always\nnecessary and that, depending on the task, small models are\njust as good as expensive ones, especially when an evolutionary\nsearch strategy is used.\nF. Tabletop Games Framework (TAG)\nThe TAG framework is a bespoke Java research framework\nthat supports the implementation of multiplayer tabletop board\ngames. This introduces a number of new challenges:\n• The games are in general more complex than the simple\none-player games in previous sections.\n• Related to this, they are also inherently multiplayer. As\nsuch there is implicit opponent modeling required for good\nplay strategies. The environment is no longer a ‘simple’\nstationary MDP, but is actively adversarial.\n• The\nTAG\nframework\nhas\na\nnumber\nof\nlocal\nlibraries\nand\ncoding\nconventions;\nfor\nexample\ndecks of cards are implemented via\nDeck<> or\nPartialObservableDeck<> parameterised classes.\nThese are not likely to be present in the LLM training\ndata to any degree, and require the LLM to generalise to\nunseen software architecture details. This contrasts to the\nstraightforward Python with mostly standard libraries of\nthe games in earlier sections.\n9\nTABLE VIII: TAG results by game. Key as for Table IX, plus\nP is the number of players, Best Agent records the model\nthat won the round robin tournament. SM is the number of\nmodels that produced working code on at least one iteration and\nentered an agent in the round robin tournament. BB indicates\nif the best agent significantly Beats the Baseline agent (OSLA\nor MCTS); ≈means performance matches the baseline.\nGame\nP\nS.Iter.\nSM\nBest Agent\nBB\nCan’t Stop\n3\n66%\n8\nGPT 4o\nYes\nColt Express\n3\n28%\n7\nClaude Haiku\n≈\nConnect 4\n2\n34%\n7\nClaude Haiku\n≈\nDiamant\n4\n30%\n5\nClaude Sonnet\nNo\nDominion\n3\n35%\n6\nGPT 4o\nYes\nHearts\n4\n24%\n5\nClaude Sonnet\nYes\nLove Letter\n3\n29%\n8\nGPT 4o\n≈\nPoker\n4\n35%\n7\nGemini Pro\nYes\nSeven Wonders\n4\n16%\n5\nClaude Sonnet\nYes\nSushi Go!\n4\n16%\n6\nGemini Pro\nNo\nTic-Tac-Toe\n2\n26%\n6\nGemini Pro\nYes\nVirus\n2\n59%\n7\nMistral Small\nNo\n• The language used is now Java. Integration of all language\nmodels used langchain4j.12\nAlgorithm 1 was applied to 12 tabletop board games (see\nTable VIII for the full list) implemented in TAG. These\nare multi-player environments (2 to 4 players) with partial\nobservability and stochasticity and varying levels of complexity.\nGiven the additional level of complexity of these games,\nand very different (and often dynamic) action spaces for each\ngame, the language models were not tasked with writing a full\npolicy to play the game. Instead they were tasked with writing\na heuristic function to estimate the value of a game state for\na player. This should be close to 1.0 for a position that is a\ndefinite win, to 0.0 for a position that is a definite loss. This\nheuristic function was then used within a search algorithm;\neither one step lookahead (OSLA) or Monte Carlo Tree Search\n(MCTS).\nEach of these games has very different rules and implemen-\ntations in TAG. To achieve the target of a scalable system that\nrequired no hand-writing and tuning of LLM prompts for each\nnew game, two new TAG-specific elements were implemented\nto augment the process:\n1) Automatic extraction of the game-specific APIs. This uses\nJava Reflections to extract information on the methods\nand associated Javadoc on the game state object. The\nentry point for this is the Class name of the main\ngame state. All public information gathering methods\non this are extracted (defined as names matching on\neither get*(...) or is*(...). APIs for any class\ndependencies on these methods, as parameters or return\nvalues, are also extracted and this recurses until the core\njava libraries are reached (these are excluded).\n2) Automatic rulebook digestion. This takes as input the\nPDF of the game rulebook. An approach inspired by [52]\nis used. The rulebook is first broken down into chunks\nof 1000 or 2000 words. The LLM is then given each\nchunk in turn and asked to summarise in 200 words or\n12https:\/\/docs.langchain4j.dev\nTABLE IX: TAG results by model. S.Iter is the percentage of\niterations that resulted in working code. Cost is the total cost\nof all 120 iterations (10 per game) on the LLM and Points are\nfrom round robin tournaments between the best agents from\neach LLM for each game. For each game 5 points are given\nfor first place, 4 for 2nd and so on down to 1 for 5th place.\nZero points are awarded otherwise, including for LLMs that\nfailed to produce any working code for a game. The maximum\nnumber of points is therefore 60.\nModel\nS.Iter.\nCost ($)\nPoints\nClaude Sonnet\n38%\n11.81\n35\nClaude Haiku\n14%\n1.18\n15\nGemini Pro\n48%\n3.22\n31\nGemini Flash\n25%\n0.24\n14\nMistral Large\n33%\n3.06\n20\nMistral Small\n19%\n0.51\n18\nGPT 4o\n48%\n6.95\n31\nGPT 4o mini\n40%\n0.30\n16\nless the information about the game rules. This final set\nof synopses is then fed to the LLM with a prompt to,\n‘Summarise this information in 500 words or less.’. This\nprovides an blocks of text to include in the prompt used\nin the main loop of Algorithm 1 that explains the rules\nof the game.\nThese new tools enable a scalable and game-agnostic process\nto be run on all games. The input for each game is the game\nrulebook as a PDF file, and a Java Class name for the main\ngame state. Additionally, the methods on the main game state\nwere briefly reviewed for meaningful Javadoc comments, public\nvisibility and name convention to ensure that they were picked\nup by the automated API process. An example full prompt (for\nSushi Go!) is included in the code repository.\nThe multi-player nature of these environments also neces-\nsitates a change in the evaluation criterion in Algorithm 1.\nEvaluation used a tournament of 500 games. The base agent\nin the tournament was either a one step lookahead (OSLA)\nagent (for Tic-Tac-Toe and Virus) or a vanilla MCTS agent\n(all other games) with a budget of 10ms and a rollout of 10\nactions before the generated heuristic function is applied.\nA base opponent used a heuristic function of the game score\n(for all other games). All games in Table VIII have the concept\nof score except for Tic-Tac-Toe and Connect 4, which only\nreward a win (+1) or draw (+0.5). To avoid overfitting to a\nspecific opponent, later iterations within a run of Algorithm 1\nadd all previous (working) agents to the evaluation tournament.\nThe evaluation score of each generated heuristic is the win rate\nfrom the most recent tournament, so this is updated to include\na broader range of opponents later in the run.\nFor each game a final tournament is then run between the\nbest agents from each model, for a maximum of 8 participants\nif all models generated at least one heuristic that compiled and\nexecuted successfully. Model performance is then judged by\ngiving 5 points to the winner of each tournament, 4 points to\nthe second-place and down to 1 point for 5th place.\nTable IX summarises the results by language model. Large\n10\nmodels do consistently better than their smaller counterpart in\nterms of both the number of successful iterations (ones that\ngenerate java code that compiles and runs) and in the quality\nof the best heuristics produced. However the smaller models\nare much cheaper to run, and do sometimes generate winning\ncode. This suggests that given a fixed budget for LLM calls\nusing a smaller model for a larger number of iterations and\nruns could be the better choice, although this has not been\ntested here experimentally.\nThere is no major difference between the four model families.\nGPT and Gemini fare slightly better overall, especially in\nterms of generating more consistently valid code; but this is\nin aggregate and there is high variability across games. The\nlarger GPT model failed to produce a single instance of working\ncode for Virus for example (while the large Claude and Gemini\nmodels both had 9 or 10 functional iterations for the same\ntask and prompt). Similarly, on three games the large Gemini\nmodel only had a single iteration out of the 10 that generated\nworking code.\nThe small Anthropic model (Claude 3.5 Haiku) is poor\noverall, but still produces the single best performing heuristic in\n2 of the 12 games. This variability is actually very encouraging\nas gives traction to this semi-evolutionary approach. If each\nmodel gave the same response to the same prompt then\nwe could not make progress (all models use their default\ntemperature and TopP or TopK settings). There is no clear\npattern in terms of which model is better at which type of\ngame, and these results suggest that using a variety of models\nto generate even wider phenotypic variety can be beneficial.\nOverall results by game are shown in Table VIII. One\ncommon reason for failure of an iteration was code that\ncompiled but then failed to execute in all edge cases due\nto poor error checking for division by zero (throwing a runtime\nerror during the evaluation tournament was counted as a failure\nof the iteration). Otherwise the models were often quite creative\nin their invention of undocumented API methods causing\ncompilation to fail.\nThe complexity of API to an LLM is not always the\nsame as complexity of a game. Tic-Tac-Toe and Connect\n4 are simple games that are won by building a straight\nline of a player’s markers (3 and 4 respectively). Each is\nrepresented by a grid of cells. In TAG this is represented by\na parameterised GridBoard<T extends Token> class,\nwith methods that require navigating a deeper class hierar-\nchy and understanding parameterised types. There is a int\ngetPlayerAt(int x, int y) helper method on the\ngame state as well, but the LLMs often fail to write code\nthat compiles due to issues with GridBoards. The game with\nthe highest iteration success rate, Can’t Stop, is also the only\ngame for which the base game state has no dependencies\noutside the core java.lang and java.util libraries.\nSushi Go! had a particularly low success rate for generating\nvalid code. A specific problem here was that the card types\nin the game were represented by an enum that had values\nexpressed in lower case; Maki, Sashimi, Dumpling.\nDespite these values being clearly stated in the Java API section\nof the prompt generated code more commonly used MAKI,\nSASHIMI, DUMPLING. This is presumably because an upper\ncase convention is more standard across Java more generally\nand hence in the training data of the models.\nThese results only compare the agents from the language\nmodel results with each other. In a few cases the ‘best’ agent\nwas not actually very good at the game, and was beaten by\nan OSLA\/MCTS agent using the game score as very simple\nheuristic as in [53]. This is shown in Table VIII. Six of\nthe games (i.e. half) having a heuristic generated (across 80\niterations in total) that could reliably beat the baseline score\nheuristic, and three fail to generate one that is even as good\nas simply targeting the score.\nVI. CONCLUSION\nIn this work we studied and evaluated the current possibilities\nof using LLMs for program search in the area of games for\nvarious applications. Previous work was mostly limited to a sin-\ngle problem or game without being easily transferable to other\ndomains, as the DSL had to be adapted. We demonstrated that\nLLMs can overcome the problem of combinatorial explosion\nof search spaces constructed with predefined DSLs, and that\nLLMs are able to synthesize programmatic policies in Python\nfor the Minatar domain, which was not possible with a custom\nDSL and previous methods. Furthermore, we have shown that\nthis framework can be easily adapted to different applications\nby modifying the prompts, and that it often provides reasonable\nresults even without much customization. We have shown that\neven with the default temperature settings on these standard\nlanguage models there is a very wide range of output for the\nsame input prompt; in this respect at least the models can be\nquite ‘creative’. Running many independent iterations of the\nsame task can create a varied population of outputs. This is\nvery promising as it provides the variation required for the\nevolutionary hill-climbing approach used here.\nWe observed limitations in the quality of the generated\ncode. For example, in the simple 2D vehicle driving task, the\ngenerated code drove the car to the target but then failed to stop\nmost of the time. Much of the generated code fails to run, or\nin the case of Java, to compile. These limitations become more\nevident as the complexity of the task increases. The need to\nuse framework-specific Java libraries in TAG leads to less than\n1 in 5 attempts generating valid code. We believe limitations\nsuch as this could be overcome with more sophisticated search\nand better prompt engineering, but the results so far give an\nidea of the limitations of what can be achieved with relatively\nlittle effort. The addition of tools to the LLM interfaces and a\nmore agentic workflow is a promising area for this future work.\nFor example instead of asking the LLM to generate the code\nin one pass, it could be asked to construct useful component\nfunctions or sub-modules with documented interfaces. In a\nlater pass the model could then be asked to combine these\nsub-modules (based on feedback of performance of previous\ncombinations).\nOne general recommendations that can be made is to use\na variety of models as there can be huge variability between\nthem on a given task. Given a robust evaluation method, as in\nthe 29 tasks here, the smaller models can be much more cost\neffective at up to a tenth of the cost of their larger brethren and\njust as able to produce top quality results if given the time.\n11\nVII. ACKNOWLEDGEMENTS\nThis work stems from a working group in the Dagstuhl Sem-\ninar 24261 (Computational Creativity for Game Development,\n2024), and it was supported by the EPSRC Centre for Doctoral\nTraining in Intelligent Games & Games Intelligence (IGGI)\n(EP\/S022325\/1).\nREFERENCES\n[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan,\nothers, and W. Zaremba, “Evaluating large language models trained on\ncode,” 2021, arXiv preprint.\n[2] S. Gulwani, O. Polozov, and R. Singh, “Program synthesis,” Foundations\nand Trends® in Programming Languages, vol. 4, no. 1-2, pp. 1–119,\n2017.\n[3] O. Polozov and S. Gulwani, “Flashmeta: A framework for inductive\nprogram synthesis,” in Proceedings of the 2015 ACM SIGPLAN Interna-\ntional Conference on Object-Oriented Programming, Systems, Languages,\nand Applications, October 2015, pp. 107–126.\n[4] E. Butler, K. Siu, and A. Zook, “Program synthesis as a generative\nmethod,” in Proceedings of the 12th International Conference on the\nFoundations of Digital Games, August 2017, pp. 1–10.\n[5] E. Butler, E. Torlak, and Z. Popovi´c, “Synthesizing interpretable strategies\nfor solving puzzle games,” in Proceedings of the 12th International\nConference on the Foundations of Digital Games, August 2017, pp.\n1–10.\n[6] T. Silver, K. R. Allen, A. K. Lew, L. P. Kaelbling, and J. Tenenbaum,\n“Few-shot bayesian imitation learning with logical program policies,” in\nProceedings of the AAAI Conference on Artificial Intelligence (Vol.\nNo.\n06: 34, April 2020, pp. 10 251–10 258.\n[7] J. R. H. Mari˜no, R. O. Moraes, T. C. Oliveira, C. Toledo, and\nL. H. S. Lelis, “Programmatic strategies for real-time strategy games,”\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 35,\nno. 1, pp. 381–389, May 2021.\n[8] M. Kreminski and M. Mateas, “Opportunities for approachable game\ndevelopment via program synthesis,” in AIIDE Workshops, 2021.\n[9] M. Cook, “Software engineering for automated game design,” 2020 IEEE\nConference on Games (CoG), pp. 487–494, 2020.\n[10] G. Todd, A. G. Padula, M. Stephenson, E. Piette, D. J. N. J.\nSoemers, and J. Togelius, “GAVEL: Generating games via evolution\nand language models,” in The Thirty-eighth Annual Conference on\nNeural Information Processing Systems, 2024. [Online]. Available:\nhttps:\/\/openreview.net\/forum?id=oBvaZJ1C71\n[11] C. Hu, Y. Zhao, and J. Liu, “Generating games via llms: An investigation\nwith video game description language,” 2024, arXiv preprint.\n[12] A. Anjum, Y. Li, N. Law, M. Charity, and J. Togelius, “The ink splotch\neffect: A case study on chatgpt as a co-creative game designer,” in\nProceedings of the 19th International Conference on the Foundations of\nDigital Games, May 2024, pp. 1–15.\n[13] S. Hu, Z. Huang, C. Hu, and J. Liu, “3d building generation in minecraft\nvia large language models,” 2024, arXiv preprint.\n[14] M. Liu, C. H. Yu, W. H. Lee, C. W. Hung, Y. C. Chen, and S. H. Sun,\n“Synthesizing programmatic reinforcement learning policies with large\nlanguage model guided search,” 2024, arXiv preprint.\n[15] H. Tang, D. Key, and K. Ellis, “Worldcoder, a model-based llm\nagent: Building world models by writing code and interacting with\nthe environment,” 2024, arXiv preprint.\n[16] R. D. Gaina, M. Balla, A. Dockhorn, R. Montoliu, and D. Perez-Liebana,\n“Design and implementation of tag: a tabletop games framework,” 2020,\narXiv preprint.\n[17] K. Young and T. Tian, “Minatar: An atari-inspired testbed for thorough\nand reproducible reinforcement learning experiments,” 2019, arXiv\npreprint.\n[18] M. Charity and J. K. A. I. Togelius, “Competition: Solving puzzle levels\nin a dynamically changing mechanic space,” in 2022 IEEE Conference\nOn Games (CoG), 2022, pp. 570–575.\n[19] A. Cropper, R. Evans, and M. Law, “Inductive general game playing,”\nMachine Learning, vol. 109, pp. 1393–1434, 2020.\n[20] C. Hocquette, A. Niskanen, R. Morel, M. J¨arvisalo, and A. Cropper,\n“Learning big logical rules by joining small rules,” 2024, arXiv preprint.\n[21] C. Hocquette, A. Niskanen, M. J¨arvisalo, and A. Cropper, “Learning mdl\nlogic programs from noisy data,” in Proceedings of the AAAI Conference\non Artificial Intelligence (Vol. No. 9: 38, March 2024, pp. 10 553–10 561.\n[22] R. Evans, M. Boˇsnjak, L. Buesing, K. Ellis, D. Pfau, P. Kohli, and\nM. Sergot, “Making sense of raw input,” Artificial Intelligence, vol. 299,\np. 103521, 2021.\n[23] Y. Gu, Q. Liu, Z. Li, and K. Zhang, “Knowpc: Knowledge-driven\nprogrammatic reinforcement learning for zero-shot coordination,” 2024,\narXiv preprint.\n[24] M. Eberhardinger, J. Maucher, and S. Maghsudi, “Learning of generaliz-\nable and interpretable knowledge in grid-based reinforcement learning\nenvironments,” in Proceedings of the AAAI Conference on Artificial\nIntelligence and Interactive Digital Entertainment (Vol.\nNo. 1: 19,\nOctober 2023, pp. 203–214.\n[25] D. S. Aleixo and L. H. Lelis, “Show me the way! bilevel search\nfor synthesizing programmatic strategies,” in Proceedings of the AAAI\nConference on Artificial Intelligence (Vol.\nNo. 4: 37, June 2023, pp.\n4991–4998.\n[26] R. O. Moraes, D. S. Aleixo, L. N. Ferreira, and L. H. Lelis, “Choosing\nwell your opponents: how to guide the synthesis of programmatic\nstrategies,” in Proceedings of the Thirty-Second International Joint\nConference on Artificial Intelligence, August 2023, pp. 4847–4854.\n[27] R. O. Moraes and L. H. Lelis, “Searching for programmatic policies in\nsemantic spaces,” 2024, arXiv preprint.\n[28] Q. A. Sadmine, H. Baier, and L. Lelis, “Language models speed\nup local search for finding programmatic policies,” Transactions\non Machine Learning Research, 2024. [Online]. Available: https:\n\/\/openreview.net\/forum?id=tBkj2I1mJY\n[29] D. Robilliard and C. Fonlupt, “Towards human-competitive game playing\nfor complex board games with genetic programming,” in Artificial\nEvolution: 12th International Conference, Evolution Artificielle, EA 2015,\nLyon, France, October 26-28, 2015. Revised Selected Papers 12: Springer\nInternational Publishing, 2016, pp. 123–135.\n[30] G. Martinez-Arellano, R. Cant, and D. Woods, “Creating ai characters\nfor fighting games using genetic programming,” IEEE transactions on\ncomputational intelligence and Ai in games, vol. 9, no. 4, pp. 423–434,\n2016.\n[31] S. E. Gaudl, “A genetic programming framework for 2d platform ai,”\n2018, arXiv preprint.\n[32] C. Olson, L. Wagner, and A. Dockhorn, “Evolutionary optimization of\nbaba is you agents,” in 2023 IEEE Congress on Evolutionary Computation\n(CEC), 2023, pp. 1–8, (to be published).\n[33] M. Eberhardinger, F. Rupp, J. Maucher, and S. Maghsudi, “Unveiling\nthe decision-making process in reinforcement learning with genetic\nprogramming,” in Advances in Swarm Intelligence.\nSingapore: Springer\nNature Singapore, 2024, pp. 349–365.\n[34] D. G. Wilson, S. Cussat-Blanc, H. Luga, and J. F. Miller, “Evolving\nsimple programs for playing atari games,” in Proceedings of the genetic\nand evolutionary computation conference, July 2018, pp. 229–236.\n[35] L. Wong, G. Grand, A. K. Lew, N. D. Goodman, V. K. Mansinghka,\nJ. Andreas, and J. B. Tenenbaum, “From word models to world models:\nTranslating from natural language to the probabilistic language of thought,”\n2023, arXiv preprint.\n[36] J. A. Fodor, The language of thought.\nCambridge, MA: Harvard\nuniversity press, 1975, vol. 5.\n[37] G. Grand, V. Pepe, J. Andreas, and J. Tenenbaum, “Loose lips sink\nships: Asking questions in battleship with language-informed program\nsampling,” in Proceedings of the Annual Meeting of the Cognitive Science\nSociety (Vol. 46), December 2023.\n[38] A. Verma, V. Murali, R. Singh, P. Kohli, and S. Chaudhuri, “Programmat-\nically interpretable reinforcement learning,” in International Conference\non Machine Learning.\nPMLR, July 2018, pp. 5045–5054.\n[39] A. Verma, H. Le, Y. Yue, and S. Chaudhuri, “Imitation-projected\nprogrammatic reinforcement learning,” Advances in Neural Information\nProcessing Systems, vol. 32, 2019.\n[40] R. Das, J. B. Tenenbaum, A. Solar-Lezama, and Z. Tavares, “Combining\nfunctional and automata synthesis to discover causal reactive programs,”\nin Proceedings of the ACM on Programming Languages, 7(POPL), 2023,\npp. 1628–1658.\n[41] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and\nA. Anandkumar, “Voyager: An open-ended embodied agent with large\nlanguage models,” Transactions on Machine Learning Research, 2024.\n[Online]. Available: https:\/\/openreview.net\/forum?id=ehfRiF0R3a\n[42] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman,\nY. Zhu, L. Fan, and A. Anandkumar, “Eureka: Human-level reward\ndesign via coding large language models,” in The Twelfth International\nConference on Learning Representations, 2024. [Online]. Available:\nhttps:\/\/openreview.net\/forum?id=IEduRUO55F\n[43] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar,\nE. Dupont, others, and A. Fawzi, “Mathematical discoveries from program\n12\nsearch with large language models,” Nature, vol. 625, no. 7995, pp. 468–\n475, 2024.\n[44] S. Chaudhuri, K. Ellis, O. Polozov, R. Singh, A. Solar-Lezama, Y. Yue\net al., “Neurosymbolic programming,” Foundations and Trends® in\nProgramming Languages, vol. 7, no. 3, pp. 158–243, 2021.\n[45] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nothers, and B. McGrew, “Gpt-4 technical report,” 2023, arXiv preprint.\n[46] Anthropic, “The claude 3 model family: Opus, sonnet, haiku.” [Online].\nAvailable: https:\/\/api.semanticscholar.org\/CorpusID:268232499\n[47] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer,\nD. Vincent, Z. Pan, S. Wang et al., “Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context,” 2024, arXiv preprint.\n[48] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nothers, and R. Ganapathy, “The llama 3 herd of models,” 2024, arXiv\npreprint.\n[49] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H.\nChi, Q. V. Le, and D. Zhou, “Chain of thought prompting elicits reasoning\nin large language models,” in Advances in Neural Information Processing\nSystems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.\n[Online]. Available: https:\/\/openreview.net\/forum?id= VjQlMeSB J\n[50] N. Shaker, J. Togelius, and M. J. Nelson, Procedural content generation\nin games, 2016.\n[51] A. Summerville, S. Snodgrass, M. Guzdial, C. Holmg˚ard, A. K. Hoover,\nA. Isaksen, others, and J. Togelius, “Procedural content generation via\nmachine learning (pcgml),” IEEE Transactions on Games, vol. 10, no. 3,\npp. 257–270, 2018.\n[52] Y. Wu, S. Y. Min, S. Prabhumoye, Y. Bisk, R. Salakhutdinov,\nA. Azaria, T. Mitchell, and Y. Li, “SPRING: Studying papers\nand reasoning to play games,” in Thirty-seventh Conference on\nNeural Information Processing Systems, 2023. [Online]. Available:\nhttps:\/\/openreview.net\/forum?id=jU9qiRMDtR\n[53] J. Goodman, D. Perez-Liebana, and S. Lucas, “Following the Leader in\nMultiplayer Tabletop Games,” in Proceedings of the 18th International\nConference on the Foundations of Digital Games, 2023, pp. 1–11.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/From Code to Play: Benchmarking Program Search for Games Using Large Language Models.pdf"}
{"title":"BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games","authors":"Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rocktäschel","summary":"Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities; however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas models perform worse when visual representations of the environments are\nprovided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community.","url":"http:\/\/arxiv.org\/abs\/2411.13543v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2411.13543v1","published":1732128872000,"comment":"Preprint, under review","pdf_text":"Preprint, under review.\nBALROG: BENCHMARKING AGENTIC LLM\nAND\nVLM REASONING ON GAMES\nDavide Paglieri1∗, Bartłomiej Cupiał2∗, Samuel Coward3, Ulyana Piterbarg4,\nMaciej Wolczyk2, Akbir Khan1,5, Eduardo Pignatelli1, Łukasz Kuci´nski2,Lerrel Pinto4\nRob Fergus4, Jakob Nicolaus Foerster3, Jack Parker-Holder1, Tim Rockt¨aschel1\n1AI Centre, University College London, 2IDEAS NCBR, 3University of Oxford,\n4New York University, 5Anthropic\nABSTRACT\nLarge Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they still\nstruggle to perform well in complex, dynamic environments. Real-world tasks re-\nquire handling intricate interactions, advanced spatial reasoning, long-term plan-\nning, and continuous exploration of new strategies—areas in which we lack effec-\ntive methodologies for comprehensively evaluating these capabilities. To address\nthis gap, we introduce BALROG, a novel benchmark designed to assess the agen-\ntic capabilities of LLMs and VLMs through a diverse set of challenging games.\nOur benchmark incorporates a range of existing reinforcement learning environ-\nments with varying levels of difficulty, including tasks that are solvable by non-\nexpert humans in seconds to extremely challenging ones that may take years to\nmaster (e.g., the NetHack Learning Environment). We devise fine-grained met-\nrics to measure performance and conduct an extensive evaluation of several pop-\nular open-source and closed-source LLMs and VLMs. Our findings indicate that\nwhile current models achieve partial success in the easier games, they struggle\nsignificantly with more challenging tasks. Notably, we observe severe deficien-\ncies in vision-based decision-making, as models perform worse when visual rep-\nresentations of the environments are provided. We release BALROG as an open\nand user-friendly benchmark to facilitate future research and development in the\nagentic community.\n1\nINTRODUCTION\nRecent successes of Large Language Models (LLMs) have renewed interest in building general-\npurpose agents capable of autonomously achieving complex goals Yang et al. (2023). LLMs possess\nvast knowledge across domains (Brown, 2020; Hendrycks et al., 2020), can reason in specific scenar-\nios (Wei et al., 2022a; Shinn et al., 2023; Rein et al., 2023), and can reliably follow human instruc-\ntions in simple settings (Ouyang et al., 2022). These abilities suggest that LLMs have the potential\nto become efficient agents, capable of autonomously performing a wide range of human tasks that\nrequire sequential decision making. In the present day, however, state-of-the-art models continue to\nexhibit persistent failure modes on many of the skills that are crucial for autonomous real-world in-\nteraction. For example, LLMs fail to act robustly in dynamic environments, and they cannot reliably\nlearn from mistakes, reason about space and time, or plan over long time horizons (Xing et al., 2024;\nYamada et al., 2023; Kambhampati et al., 2024). Improving our understanding of LLM capabilities\nthrough rigorous, safe evaluations is key for assessing the risks and limitations of deploying agentic\nLLMs in the real world.\nCurrent agentic benchmarks evaluate LLM performance in settings that involve no more than a\nfew dozen rounds of interaction between a model and an environment, e.g., solving simple office\ntasks (Wang et al., 2024), navigating the Internet (Zhou et al., 2023), and resolving GitHub is-\nsues (Jimenez et al., 2023). New agentic prompting frameworks and improvements to short-horizon\n∗Equal technical contribution, first author was the project lead. Correspondence to d.paglieri@cs.ucl.ac.uk.\nCode and Leaderboard at balrogai.com\n1\narXiv:2411.13543v1  [cs.AI]  20 Nov 2024\nPreprint, under review.\nFigure 1: An overview of the BALROG Benchmark for evaluating LLMs on long-context in-\nteractive tasks. Submissions of new inference-time methods for improving the capabilities of an\nexisting model via an “agentic strategy” need only modify the agent.py file. Similarly, bench-\nmarking a new model zero-shot can be done by adjusting a configuration file in client.py. The\nagent class includes a prompt builder to manage observation history, and a client that abstracts the\ncomplexities of various APIs and model-serving frameworks. The env wrapper.py file standard-\nizes interaction across settings, and the evaluator executes agents and collects performance metrics.\nreasoning via LLMs like OpenAI o1 have led to dramatic and fast-paced gains in state-of-the-art per-\nformance on these benchmarks (OpenAI, 2024b; Wang et al., 2023; Fernando et al., 2023; Hu et al.,\n2024). However, many realistic tasks require orders of magnitude more interactions (Pignatiello\net al., 2020; Wansink and Sobal, 2007).\nIn this paper, we argue that the next frontier for language and vision-language model capabilities\nlies in long-horizon reasoning and decision-making. To that end, we propose BALROG: Bench-\nmarking Agentic LLM\/VLM Reasoning On Games. BALROG is a benchmark and framework that\naggregates a diverse set of complex reinforcement learning game environments into a unified testbed\nfor research on long-context LLMs. Games have historically served as highly effective metrics for\nevaluating progress in deep reinforcement learning research (Bellemare et al., 2013; Silver et al.,\n2018; Schrittwieser et al., 2020; Vinyals et al., 2019). By aggregating many different game envi-\nronments into a single evaluation, we look to spur progress on developing truly generalist agents\nthat can meaningfully address embodied, real world tasks. Specifically BALROG enables seam-\nless running of LLM and VLM agents on BabyAI, Crafter, TextWorld, Baba Is AI, MiniHack,\nand NetHack (Chevalier-Boisvert et al., 2019; Hafner, 2021; Cˆot´e et al., 2019; Cloos et al., 2024;\nSamvelyan et al., 2021; K¨uttler et al., 2020). These environments have lightweight simulators,\nensuring that the benchmark is affordable for the research community. Furthermore, while all of\nthese games are long-horizon, they span a broad range of difficulty levels, from tasks where we see\nfair zero-shot performance by state-of-the-art long-context models (BabyAI) to those where even\nspecialized neural models trained on billions of in-domain datapoints make very limited progress\n(NetHack) (Piterbarg et al., 2024; Klissarov et al., 2023; Wołczyk et al., 2024). BALROG is dif-\nficult to solve through simple memorization – all of the environments used in the benchmark are\nprocedurally generated, and encountering the same instance of an environment twice is unlikely.\nUsing the six proposed environments, we evaluate the capabilities of various popular LLMs and\nVLMs. We employ a fine-grained metric that captures how close each model is to completing a task,\nwhich gives us a thorough understanding of the resulting trajectories. In our qualitative analysis, we\nstudy the agents’ capabilities for spatial reasoning, systematic exploration, long-term planning, and\ndiscovering environment dynamics. We find that the current top LLMs show promise on the simplest\n2\nPreprint, under review.\ntasks but completely fail to make meaningful progress on the more difficult tasks, such as MiniHack\nand NetHack. Some of the models exhibit knowledge about the game from pre-training but fail to\nuse it in practice. For example, in NetHack, GPT-4o often dies from the consumption of rotten food,\neven though, when prompted, it correctly identifies it as very dangerous. Furthermore, we study the\nimpact of the input representation. Although the majority of the environments were created with\nvision in mind, we find that multimodal LLMs perform much worse when also presented with an\nimage of the environment rather than a textual-only description of the observation. This suggests\nthat reliable vision-based decision-making is currently far outside our reach.\nOur results show that BALROG is a very difficult benchmark that still allows us to observe fine-\ngrained progress in crucial areas such as long-term planning, spatial reasoning and navigation. We\nshare the codebase and open the benchmark for external submissions. We summarize our contribu-\ntions as follows:\n• BALROG, a suite of six reinforcement learning environments for testing the agentic capa-\nbilities of long-context LLMs. We provide a fine-grained metric for model evaluation, and\nwe develop a novel data-informed progression system for NetHack.\n• Baseline evaluations of state-of-the-art LLMs on BALROG using zero-shot prompting, in\nboth Language-Vision and Language-only modalities. We show that while models exhibit\ndecent performance on easier games, all are very far from solving the hardest game in the\nbenchmark, NetHack. We observe that the performance drops further when images of the\nenvironment are presented, suggesting severe problems with VLM decision-making.\n• We perform a qualitative analysis of the results across capabilities such as spatial reasoning,\nsystematic exploration, and long-term planning. We identify an intriguing knowing-doing\ngap where the models cannot employ the knowledge they possess.\n• An open-source toolkit for benchmarking long-context models on BALROG. This toolkit\nenables researchers and practitioners to quickly evaluate model performance. While the\nbaseline evaluations performed in this paper are zero-shot, the BALROG toolkit supports\ninference-time prompting strategies like chain-of-thought (Wei et al., 2022b), few-shot\nlearning, and more.\n2\nBALROG\nBALROG is a benchmark and framework that aims to improve our understanding of whether exist-\ning long-context LLMs are agentic, i.e., whether they can be used to automate complex activities\nthat require sequential decision-making. It supports model evaluation on challenging reinforcement\nlearning environments that test skills such as long-term planning, spatial reasoning, and the ability\nto deduce the mechanics of the environment.\nBy design, the BALROG framework explicitly decouples inference-time prompting strategies from\nunderlying models. The goal of this design choice is two-fold: (1) to facilitate rapid prototyping of\ninference-time methods for improving model performance on long-context decision-making beyond\nzero-shot prompting and (2) to ensure that model evaluations are consistent and rigorous.\nIn the remainder of this section, we introduce the game environments evaluated in the benchmark\nand we discuss our protocols for model submission to the BALROG Benchmark Leaderboard1.\n2.1\nENVIRONMENTS\nBALROG evaluates long-context models as agents on the games described below.\nBabyAI. (Chevalier-Boisvert et al., 2019; Carta et al., 2023) A simple, two-dimensional grid-world\nin which the agent has to solve tasks of varying complexity described in natural language (e.g.,\n“go to the blue ball, then pick up the grey key”). Agents are tested across five different types of\nnavigation tasks, see Appendix A.\nCrafter. (Hafner, 2021) A Minecraft-inspired grid environment where the player has to explore,\ngather resources and craft items to ensure their survival. Agents are evaluated based on the number\nof achieved milestones, such as discovering new resources and crafting tools, see Appendix B.\n1This Leaderboard will open to the public at the time of publication.\n3\nPreprint, under review.\nTable 1: The tested skills, time horizons, and complexities of interactive decision-making tasks\nevaluated in BALROG. Compared to existing benchmarks, BALROG provides infrastructure for\nevaluating model reasoning and decision-making on harder, longer time-horizon interactive settings.\nThe evaluated tasks span a range of difficulties.\nSkills\nBabyAI\nTextWorld\nCrafter\nBaba Is AI\nMiniHack\nNLE\nNavigation\n✔\n✔\n✔\n✔\n✔\n✔\nExploration\n✔\n✔\n✔\n✔\n✔\n✔\nResource Management\n✗\n✔\n✔\n✗\n✔\n✔\nComplex Credit Assignment\n✗\n✗\n✔\n✔\n✔\n✔\nDeducing Env. Dynamics\n✗\n✗\n✗\n✔\n✔\n✔\nLong-term Planning\n✗\n✗\n✗\n✔\n✔\n✔\nTurns to Complete\n101\n102\n103\n102\n102\n104–105\nTime to Master for Humans\nSeconds\nMinutes\nHours\nHours\nHours\nYears\nTextWorld. (Cˆot´e et al., 2019) An entirely text-based game with no visual component, where the\nagent has to explore mazes and interact with everyday objects through natural language (e.g., “cook\npotato with oven”). Unlike the other environments in BALROG, TextWorld is not a grid-world.\nModels are evaluated on three different tasks, see Appendix C.\nBaba Is AI. (Cloos et al., 2024) An environment based on the popular puzzle video game Baba\nIs You. The player manipulates the rules of the game world by pushing word blocks, altering how\nobjects interact. Agents are tested on 40 puzzles, see Appendix D.\nMiniHack. (Samvelyan et al., 2021) MiniHack is a multi-task framework built on top of the NetHack\nLearning Environment (K¨uttler et al., 2020). We select five different tasks, Maze, Corridor, Corridor-\nBattle, Boxoban, and Quest. Collectively, they assess a wide range of skills, including exploration,\nnavigation, long-term planning, and resource management, see Appendix E.\nNetHack Learning Environment (NLE) (K¨uttler et al., 2020) is based on the classic roguelike\ngame NetHack, known for its extreme difficulty and complexity. Success in NetHack demands both\nlong-term strategic planning, since a winning game can involve hundreds of thousands of steps, as\nwell as short-term tactics to fight hordes of monsters. Accurate credit assignment is also crucial\nto understanding which actions contributed to success or failure. It takes human players years to\nmaster NetHack without accessing external guides. Notably, we find that research shows that LLMs\ncan answer questions about the game mechanics and optimal strategies (see Appendix F.5), but they\nfail to apply this knowledge in practice. See Appendix F for more details.\nTable 1 provides an overview of the environments used in the benchmark, detailing the reasoning\nand agentic capabilities required to succeed in each. This diverse set of environments positions\nBALROG as a comprehensive benchmark for assessing the capabilities of LLM agents, making it a\nvaluable tool for evaluating their performance for years to come.\n2.2\nSUBMITTING TO THE BENCHMARK LEADERBOARD\nThe BALROG benchmark accepts two types of submissions.\nNew Models. Submissions may include any type of new model, such as large language models\n(LLMs), vision-language models (VLMs), large-action models (LAMs), or fine-tuned versions of\nexisting models. The key requirement is that these models must be capable of generating actions in\nnatural language. By default, these models will be evaluated zero-shot.\nAgentic Strategies. Submissions may propose novel inference-time prompting strategies for im-\nproving the reasoning, planning, or in-context learning capability of an existing model. These strate-\ngies should extend beyond simple zero-shot prompting for direct action prediction, demonstrating\nmore sophisticated techniques for inference-time decision-making.\n4\nPreprint, under review.\n3\nZERO-SHOT EVALUATION PROTOCOL\nIn this section, we provide a description of our protocols for evaluating state-of-the-art, long-context\nLLMs and VLMs on BALROG. These evaluations are intended to serve as baselines for the bench-\nmark. As a result, they probe zero-shot performance only.\n3.1\nEVALUATION SETTING\nWe aim to keep the evaluation setting simple. During each timestep of interaction, agents are\nprompted to output the next action as a natural language string, conditioned on their past interaction\nhistory in the environment. To perform successfully in BALROG, models must demonstrate robust\ninstruction-following capabilities, including reading and interpreting game rules, understanding the\naction space, and producing valid actions to complete tasks effectively.\nTo address cases where the LLMs\/VLMs output hallucinated or invalid actions, BALROG provides\nfeedback to the agent indicating the action’s invalidity, it then executes a default fallback action (such\nas a “do-nothing” action or a standard move like “north”), and logs the occurrence for trajectory\nstatistics. This ensures that the interaction remains continuous and robust while enabling users to\nanalyze the context and frequency of such errors in post-evaluation analysis.\nA diagrammatic visualization of BALROG is shown in Figure 1. We conceptualize the agent as a\ncombination of the underlying LLM\/VLM model and a particular prompting strategy. We provide a\nunified client wrapper that seamlessly integrates APIs for closed-source LLMs and VLMs such as\nOpenAI, Gemini, and Claude and allows users to effortlessly switch and evaluate models. For the\nevaluation of locally-served models, we include native support for the vLLM library (Kwon et al.,\n2023), which optimizes throughput by efficiently batching generation requests. We use multiple\nseeds for each environment to ensure the statistical significance of the results.\nMetrics To ensure a fair and interpretable evaluation, we introduce a standardized metric, scoring\nperformance on each task within a range of 0 to 100. For environments like MiniHack, BabyAI,\nand Baba Is AI, each episode is scored as either 0 or 100 based on task completion. For TextWorld,\nCrafter, and NetHack we use as the score a real number between 0 and 100, representing the pro-\nportion of achievements toward the maximum score. For NetHack, as the game scoring system does\nnot adequately reflect actual progression (Wołczyk et al., 2024), we propose a novel, data-informed\nprogression metric, described in Appendix F.2, to better capture agent performance.\nPerformance BALROG supports highly parallelized evaluations, leveraging the lightweight simu-\nlators of each of the environments in the suite. These evaluations allow multiple agents and environ-\nment instances to run concurrently with minimal computational overhead. Environment instances\nrun asynchronously from one another, accommodating varying observation lengths and ensuring\nthat agents with faster generation speeds (per action) are not affected by slower agent bottlenecks.\n3.2\nOBSERVATIONS\nIn the initial prompt, the agent is introduced to the game rules and provided with a list of avail-\nable actions, each accompanied by a brief description. To prevent model overspecialization, we\ndesign a general prompt that is not fine-tuned to any specific LLM. Subsequent prompts present the\nobservation-action history in a chat-based format. The game rules and observations are conveyed\nfrom the perspective of the “user”, while prior actions are attributed to the “assistant” or “model”\nrole, depending on the type of model used. This structure mirrors the standard format used for fine-\ntuning instruction-following LLMs. Detailed examples of game observations are included in the\nappendices.\nExcept for TextWorld, which lacks a visual component, we evaluate all environments using two\nobservation modalities:\nLanguage Only Format Observations are expressed as natural language descriptions of the envi-\nronment’s state (e.g., “a wall 5 steps ahead, a wall 2 steps to the left...”). For environments without\nnative textual representations, we either generate descriptions using open-source language wrappers\n(BabyAI (Carta et al., 2023), Crafter (Wu et al., 2023), NetHack, and MiniHack (Goodger et al.,\n2023)) or develop a custom wrapper ourselves (Baba is AI, see Appendix D)\n5\nPreprint, under review.\nFigure 2: Baselines for BALROG. We evaluate the zero-shot performance of seven state-of-the-art\nand long-context LLMs and VLMs on BALROG. During each timestep of interaction, models are\nprompted to output the next in-game action conditioned on past interaction history. Standard error\nis obtained by running multiple replicate seeds, as detailed in the Appendix.\nVision-Language Format For VLMs, the observation consists of an image representing the envi-\nronment’s current state, alongside its natural language description (mentioned above). In this format,\nthe image corresponds only to the current observation, although we support including multiple im-\nages in the observation history.\nFor the most complex environments, i.e., MiniHack and NetHack, we augment the language-based\nobservations with a two-dimensional map rendered using ASCII characters. For all experiments, we\nuse a history length of 16 observations to maintain consistency across tasks. However, participants\nsubmitting to this benchmark are allowed to modify the observation history length as needed for\ntheir respective models and experiments.\n3.3\nMODELS\nWe evaluate a range of popular closed-source and open-source models, including Gemini-1.5-Flash\nand Gemini-1.5-Pro (Reid et al., 2024), GPT-4o-mini (2024-07-18 release) and GPT-4o (2024-05-\n13 release) (Achiam et al., 2023; OpenAI, 2024a), Claude 3.5 Sonnet (Anthropic, 2024), as well as\nLlama 3.1 instruct (8B and 70B) (Dubey et al., 2024) and Llama 3.2 instruct (1B, 3B, 11B and 90B)\n(MetaAI, 2024). Additionally, we test o1-mini (2024-09-12 release) and o1-preview (2024-09-12\nrelease) (OpenAI, 2024b) exclusively on the NetHack environment due to budget constraints.\n4\nRESULTS\nIn Figure 2, we present the results of our experiments using the BALROG evaluation script for\nboth language-only and vision-language formats. Most leading models demonstrate fair average\nprogression on BabyAI, Crafter, and Baba Is AI, with GPT-4o performing best. Interestingly, the\nopen-source Llama 3.1 70B and Llama 3.2 90B models achieve the highest results on the Baba\nIs AI language-only format, narrowly surpassing GPT-4o and Claude 3.5 Sonnet. In TextWorld,\nGPT-4o and Claude 3.5 Sonnet lead, while Gemini models fail to complete any tasks, being flagged\nas ‘unsafe’ by the Google Gemini API, despite the prompts containing no actual safety concerns.\n6\nPreprint, under review.\nThe MiniHack suite proves very challenging for all models, especially the quest and boxoban tasks,\nwhich were never solved by any model. Finally, all models flat line with NetHack, with the best-\nperforming model, o1-preview, achieving a meager 1.5% average game progression.\nTable 2 summarizes the aggregated results across all environments in the language-only format.\nOverall, GPT-4o is the best-performing model, with an average progression of 31.62%, followed\nclosely by Claude 3.5 Sonnet and Llama 3.1 70B. Gemini-1.5-Pro lags behind the other large models,\npartly due to its 0% performance on TextWorld. However, results differ for the vision-language\nformat, as shown in Table 3. Here, we observe that both GPT-4o and Llama 3.2 exhibit a decline in\nperformance when image observations are included, likely due to confusion arising from the added\nvisual input. In contrast, Gemini-1.5-Pro and Claude 3.5 Sonnet especially, maintain consistent\nperformance across both formats. This suggests that current multimodal Transformer architectures\nare still better equipped at handling textual information than visual input, a topic we explore further\nin Section 6. Additionally, Llama 3.1 70B outperforms the larger and more recent Llama 3.2 90B\nin the language-only format, suggesting that the introduction of visual processing in the latter may\nhave negatively impacted its linguistic and reasoning capabilities. We show more detailed results\nfor each environment in their appendices.\nTable 2: Language-Only Performance\nModel\nAverage Progress (%)\ngpt-4o\n32.34 ± 1.49\nclaude-3.5-sonnet\n29.98 ± 1.98\nllama-3.1-70b-it\n27.88 ± 1.43\nllama-3.2-90B-it\n23.66 ± 1.09\ngemini-1.5-pro\n21.00 ± 1.18\ngpt-4o-mini\n17.36 ± 1.35\nllama-3.1-8b-it\n14.14 ± 1.51\nllama-3.2-11B-it\n13.54 ± 1.05\ngemini-1.5-flash\n9.73 ± 0.77\nllama-3.2-3B-it\n8.47 ± 1.12\nllama-3.2-1B-it\n6.32 ± 1.00\nTable 3: Vision-Language Performance\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n29.08 ± 2.21\ngemini-1.5-pro\n25.76 ± 1.36\ngpt-4o\n22.56 ± 1.44\ngpt-4o-mini\n15.36 ± 1.29\ngemini-1.5-flash\n14.94 ± 1.40\nllama-3.2-90B-it\n13.43 ± 1.16\nllama-3.2-11B-it\n6.91 ± 0.84\n4.1\nQUALITATIVE ANALYSIS\nWe conducted an analysis of the model trajectories across the environments to identify common\nbehaviors and challenges specific to each setting.\nSpatial Reasoning While language models demonstrate some proficiency in basic navigation, they\nexhibit significant limitations in more complex spatial reasoning tasks. In the BabyAI suite, we\nobserved significant shortcomings in the agents’ ability to place objects adjacent to other objects,\nwhich is required in some scenarios. In NetHack and MiniHack CorridorBattle, good spatial reason-\ning is crucial during combat, as players need to maneuver within confined corridors to avoid being\nsurrounded by monsters. However, the agents frequently ended up cornered.\nSystematic Exploration Our experiments revealed a significant weakness in the models’ ability\nto explore. In TextWorld’s Coin Collector, where agents must explore a house to locate a coin,\nagents often wander aimlessly, revisiting rooms they’ve already explored while missing important\nareas entirely. An efficient agent would behave in DFS-like manner, methodically searching each\nroom, keeping track of visited areas and prioritizing unexplored spaces. The more complex quests\nin MiniHack expose similar issues, with models failing to efficiently navigate maze-like structures.\nLong-term planning The agents exhibit substantial deficiencies in devising and executing long-\nterm plans. We observe near-zero performance on MiniHack, and NLE, which both require careful\nplanning. In particular, we do not observe a single successful trajectory in the Boxoban logical\npuzzles in MiniHack, which requires careful planning at every step in order to avoid irreversible\nfailures. LLMs, with the finite amount of compute available to them in a single forward pass, are\nnecessarily confined to solving some subset of reasoning problems. We observe that with the current\nmodels’ depth, number of flops, and reasoning solution templates embedded in the weights, these\nmodels cannot solve the reasoning tasks in BALROG. We see a notable improvement with OpenAI\n7\nPreprint, under review.\no1’s chain of thought capabilities on NetHack, performing close to three times better than its closest\ncompetitor in language-only mode Claude-3.5-Sonnet. However, its average progression of 1.57%\nis still far from satisfactory.\nDiscovering and Leveraging Environment Dynamics Some games require inferring non-trivial\ncausal structure through experimentation to come up with new strategies. For example, a player\nmight identify a potion of paralysis by drinking it, and then realize they can use this strate-\ngically by throwing such potions at enemies to incapacitate them. This kind of experimentation\nand strategic thinking is crucial for success in NetHack. However, current models struggle to for-\nmulate and execute such context-dependent strategies. In MiniHack Quests environments, models\nfail to devise and implement multi-step strategies, such as utilizing wand of cold or ring of\nlevitation to cross lava rivers. In Crafter, where agents can handle basic tasks such as collect-\ning wood, crafting items, drinking water, and even engaging in combat, they fail to learn long-term\nsurvival skills such as building shelters for protection against nocturnal threats.\nKnowing-Doing Gap We observe a pronounced “knowing-doing” gap, where models execute unde-\nsirable actions during gameplay despite knowledge of their negative consequences. For instance, in\nNetHack, models often exit the dungeon shortly after starting the game, resulting in an instant game\ntermination. When queried in a separate thread about the consequences of exiting the first level in\nNetHack, they correctly identify that it results in an instant death, making it is a highly undesirable\naction. Similarly, although the models correctly identify that eating rotten food in NetHack can re-\nsult in death, this remains a common cause of failure, underscoring a disconnect between knowledge\nand decision-making. Additionally, models tend to ignore even the hints directly present in the input\nprompt and die from overeating even when advised against it. To study this problem in more detail,\nwe prepared a questionnaire probing basic NetHack knowledge (see Appendix F.5).\n5\nRELATED WORK\nThe evaluation of large language models has historically relied on benchmarks that emphasize static,\nnon-interactive tasks. Benchmarks such as SuperGLUE (Wang et al., 2019), which tests general-\npurpose language understanding and MMLU (Hendrycks et al., 2020), which measures massive\nmultitask language understanding, have been instrumental in advancing LLM research. BigBench\n(Srivastava et al., 2022) further expands the scope by including a diverse set of linguistic and cog-\nnitive challenges. Mathematical reasoning datasets like GSM8K and MATH (Cobbe et al., 2021;\nHendrycks et al., 2021) assess models’ abilities to solve grade-school and competition-level math\nproblems, while Shi et al. (2022) explore multilingual chain-of-thought reasoning. In the domain\nof code understanding and generation, benchmarks such as HumanEval (Chen et al., 2021) and\nCodeXGLUE (Lu et al., 2021) evaluate models capabilities in programming tasks.\nThese benchmarks, however, are limited to single-turn or short-context scenarios, do not require se-\nquential decision-making or adaptation to changing environments and have been saturating rapidly\n(Kiela et al., 2021). Static benchmarks may not fully capture the progress we are seeking, since\nthe research community aims to push the frontier of agentic foundation models capable of acting\nin dynamic environments, using tools, planning ahead, and reasoning about their surroundings. Re-\nsearchers have recently investigated how LLMs use these skills to solve practical tasks, including\nusing computer interfaces to perform office-related chores (Wang et al., 2024; Qin et al., 2024), navi-\ngating web pages (Yao et al., 2022; Zhou et al., 2023), and solve GitHub issues (Jimenez et al., 2023).\nSeveral works studied the multi-agent capabilities of LLMs to see if they can co-operate (Gong et al.,\n2023; Piatti et al., 2024) or effectively play against other agents (Jin et al., 2024; Wu et al., 2024).\nIn this work, we study agentic skills in the context of video games, as they offer challenges well-\ntailored for human players and test skills that are useful for embodied agents. Previously, some\nrelated works employed games to benchmark LLMs (Liu et al., 2023b; Todd et al., 2024; Wu et al.,\n2023), highlighting their emphasis on problem-solving, spatial reasoning, and well-defined rules\nand objectives. Some of these benchmarks, however, are already reaching saturation, with environ-\nments like Crafter being the most challenging in their suite. In contrast, BALROG fills an important\ngap by providing a wide range of games at varying difficulties—including the NetHack Learning\nEnvironment (K¨uttler et al., 2020), which takes humans years to master, and where zero-shot LLMs\nstruggle greatly, as also seen in prior work (Jeurissen et al., 2024). These tasks represent a rich and\ngranular testbed for evaluating agentic foundation models, pushing decision-making evaluations of\n8\nPreprint, under review.\nLLMs\/VLMs to the very limit of their context lengths. Other environments such as MineDojo (Fan\net al., 2022) and MineRL (Guss et al., 2019) also present open-ended challenges for agentic capa-\nbilities, their steep computational requirements and reliance on multimodal inputs make them less\npractical for accessible, large-scale benchmarks.\nWhile BALROG currently focuses on evaluating single-agent foundational capabilities, future ex-\ntensions could explore multi-agent collaboration environments that provide unique opportunities to\ntest teamwork and coordination skills in LLMs. For example, Overcooked (Carroll et al., 2019;\nLiu et al., 2023a) simulates a cooperative cooking environment where agents must collaborate effi-\nciently under time constraints and task dependencies, testing planning and communication abilities.\nAnother compelling environment is Hanabi (Bard et al., 2020), a cooperative card game where play-\ners must rely on indirect communication and inferential reasoning to achieve a shared objective\nunder partial observability. These environments present rich opportunities to benchmark advanced\ncollaboration and multi-agent decision-making skills, which are essential for broader deployment of\nagentic LLMs.\n6\nOPEN RESEARCH PROBLEMS\nAside from its utility for model evaluations, BALROG also offers a test-bed for rapidly prototyping\nnew inference-time methods for improving the agentic capabilities of LLMs and VLMs. There are\nmany open research problems in this space. As of the writing of this paper, some of the most per-\nformant methods for improving model reasoning capabilities on short-form and\/or shorter-context\nproblems are infeasible to apply naively to BALROG due to the extremely long-context nature of\ntasks. Addressing these challenges could further enhance the development of stronger autonomous\nagents. We highlight several key areas for future work below.\nIn-Context Learning and Few-Shot Prompting\nBALROG enables evaluation of In-Context\nLearning (ICL) agents, which can use few-shot examples to adapt to out-of-distribution tasks. We\nprovide a small dataset of human demonstrations for each environment and an implementation of\nfew-shot conditioning in the BALROG codebase. The benchmark codebase also supports the study\nof In-Context Reinforcement Learning (Lee et al., 2024; Laskin et al., 2022; Lin et al., 2023), where\nagents learn to improve from mistakes during inference. On the large models benchmarked in Sec-\ntion 4, naive few-shot learning (i.e., prompting LLM and VLM agents with examples of full human\ngames in-context) is extremely computationally expensive to run on BALROG. For example, a sin-\ngle demonstration of NetHack game-play can require upwards of 700, 000 input tokens to represent\nin a prompt. Despite advancements in fast inference technologies like caching and falling API costs\nfor long-context prompting, we found these experiments to be infeasible to conduct at this time.\nSub-selecting only the relevant parts of demonstrations via retrieval-augmented few-shot prompting\nstrategies (Lewis et al., 2020) might offer a way to circumvent these challenges. We leave explo-\nration of such methods for future work.\nAdvanced Reasoning Strategies\nBeyond simply prompting LLMs and VLMs to directly predict\nthe next action of game-play, BALROG also supports the study of more advanced reasoning tech-\nniques like chain-of-thought (Wei et al., 2022b), self-refinement (Madaan et al., 2024), and basic\nplanning. These methods have been demonstrated to improve model performance on shorter-context\nproblems. We believe them to be an exciting direction for future work on long-context reasoning and\ndecision-making. For example, model performance on the tasks in BALROG might be improved by\nintegrating multi-agent collaboration (Chang, 2023; Khan et al., 2024; Yao et al., 2024) and tool\nusage (Shen et al., 2024; Ruan et al., 2023; Schick et al., 2024; Qin et al., 2023) in decision-making.\nAdditionally, incorporating memory mechanisms or reinforcement learning techniques could help\nbridge the “knowing-doing” gap, enabling models to apply their knowledge effectively in practical,\nlong-horizon tasks. Finally, experimenting with open-ended self-improvement loops (Wang et al.,\n2023; Hu et al., 2024) could lead to more adaptive and general agents (Team et al., 2023; Hughes\net al., 2024), offering a pathway toward truly autonomous systems.\nLimitations of Current Vision-Language Models\nDespite their potential, our benchmark shows\nsignificant variability in VLM performance. While some models, like Llama 3.2, struggle to inte-\ngrate visual information into coherent decision-making, others—most notably Sonnet 3.5—demon-\n9\nPreprint, under review.\nstrate stronger performance in VLM mode. This disparity highlights significant variability in VLM\ncapabilities, which may stem from differences in training objectives and datasets. For example,\nSonnet 3.5’s superior performance can be attributed in part to its training on tasks involving com-\nputer usage (Anthropic, 2024), which inherently require integrating visual and textual inputs for\naction-based reasoning.\nRecent studies have identified key limitations of VLMs that align with our findings, including bi-\nases toward natural image-text pairs, optimization for image description rather than action-oriented\nreasoning, and challenges with out-of-distribution inputs (Tan et al., 2024; Tong et al., 2024; Rah-\nmanzadehgervi et al., 2024; Zang et al., 2024; Guan et al., 2023). These limitations are further exem-\nplified in our benchmark, where grid-based image observations differ significantly from the natural\nimage-text pairs on which many VLMs are trained (Yu et al., 2023; Rahmanzadehgervi et al., 2024).\nMoreover, the computational cost of image processing constrained our evaluation to a single image\nper observation, with the remainder of the history provided in text. While this constraint may hinder\nperformance for some models, our results show that certain VLMs like Claude 3.5 Sonnet can still\nperform robustly under these conditions.\nTo address these challenges, our codebase already supports multi-image observation histories, and\nfuture iterations will incorporate video observations, which are likely better suited for the long-\nhorizon sequential decision-making tasks central to our benchmark. These enhancements aim to\nbetter evaluate and leverage the potential of VLMs in complex reasoning scenarios. We plan to\nintroduce support for video observations once prominent models with efficient video-processing\ncapabilities become available, ensuring that our benchmark remains aligned with the latest advance-\nments in VLM technology.\nComputational Limitations of Large Language Models\nMechanistic interpretability could pro-\nvide valuable insights for understanding the computational limitations of agentic LLMs. The compu-\ntational expressiveness of LLMs is fundamentally linked with the ability to solve complex reasoning\nproblems (Wei et al., 2022a). While current models perform well on simple tasks such as navigation\nand object manipulation, they struggle with more complex tasks that could require non-trivial and\ngeneral-purpose computation, for example, building a shelter or developing combat strategies. This\ncould be due to the models’ inability to retrieve relevant computational circuits (Olah et al., 2020),\nlimitations to inference-time budget (Snell et al., 2024), or representational expressivity. This raises\nimportant questions about the scope of effectively solvable tasks for LLMs and VLMs, which is de-\npendent on factors such as model depth, context size, and the distribution shift between pre-training\nand downstream tasks. Further research is needed to understand the underlying causes of these limi-\ntations and to develop strategies for overcoming them, such as adaptive simulation of computational\ncircuits during runtime.\n7\nCONCLUSION\nWe introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs\nand VLMs across a diverse set of challenging, long-horizon tasks. Through easily reproducible\nevaluation protocols, BALROG reveals critical shortcomings in current models, particularly in areas\nsuch as vision-based decision-making and long-term planning, identifying clear gaps between model\nperformance and human-level capabilities. These deficiencies, uncovered through our qualitative\nanalysis, reflect the challenges faced in real-world scenarios, underscoring the practical relevance\nof our benchmark for agentic applications. Our evaluation framework leverages fast, procedurally\ngenerated environments, ensuring rigorous and fair comparisons by preventing test-set leakage, a\ncommon issue in other benchmarks. We believe that BALROG will serve as a critical tool for\nsupporting and advancing research towards autonomous LLM agents.\nETHICS STATEMENT\nThis work provides a benchmark for the agentic capabilities of LLMs. We believe that experimen-\ntation in simulated environments, where the behavior of the agents is easy to interpret, is crucial for\nbuilding safe agentic systems. It is important to address questions on how to ensure that the agent’s\nbehavior is well aligned with human intentions.\n10\nPreprint, under review.\nREPRODUCIBILITY STATEMENT\nWe strive to make all experiments in this paper fully reproducible. We share the codebase for\nevaluation, which is available in the supplementary materials. We describe the full descriptions of\nthe evaluation schemes of the specific environments in Appendices A to F.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nAnthropic. Developing a computer use model, 2024. URL https:\/\/www.anthropic.com\/\nnews\/developing-computer-use. Accessed: 2024-11-17.\nAnthropic. Claude 3.5 sonnet: Enhanced intelligence and versatility, 2024. URL https:\/\/www.\nanthropic.com\/news\/claude-3-5-sonnet. Accessed: 2024-11-18.\nNolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio\nParisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A\nnew frontier for ai research. Artificial Intelligence, 280:103216, 2020.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253–279, 2013.\nTom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nMicah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca\nDragan. On the utility of learning about humans for human-ai coordination. Advances in neural\ninformation processing systems, 32, 2019.\nThomas Carta, Cl´ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves\nOudeyer. Grounding large language models in interactive environments with online reinforcement\nlearning. In International Conference on Machine Learning, pages 3676–3713. PMLR, 2023.\nEdward Y Chang. Prompting large language models with the socratic method. In 2023 IEEE 13th\nAnnual Computing and Communication Workshop and Conference (CCWC), pages 0351–0360.\nIEEE, 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. BabyAI: First steps towards grounded language learning\nwith a human in the loop. In International Conference on Learning Representations, 2019. URL\nhttps:\/\/openreview.net\/forum?id=rJeXCo0cYX.\nNathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, and\nChristopher J Cueva. Baba is ai: Break the rules to beat the benchmark. In ICML 2024 Workshop\non LLMs and Cognition, 2024.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nMarc-Alexandre Cˆot´e, Akos K´ad´ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning\nenvironment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Con-\njunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm,\nSweden, July 13, 2018, Revised Selected Papers 7, pages 41–75. Springer, 2019.\n11\nPreprint, under review.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:\n18343–18362, 2022.\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt¨aschel.\nPromptbreeder:\nSelf-referential self-improvement via prompt evolution.\narXiv preprint\narXiv:2309.16797, 2023.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-\nChun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction.\narXiv preprint arXiv:2309.09971, 2023.\nNikolaj Goodger, Peter Vamplew, Cameron Foale, and Richard Dazeley. A nethack learning envi-\nronment language wrapper for autonomous agents. Journal of Open Research Software, 11, 06\n2023. doi: 10.5334\/jors.444.\nTianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang\nChen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for en-\ntangled language hallucination and visual illusion in large vision-language models. arXiv preprint\narXiv:2310.14566, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780,\n2021.\nEric Hambro, Sharada Mohanty, Dmitrii Babaev, Minwoo Byeon, Dipam Chakraborty, Edward\nGrefenstette, Minqi Jiang, Jo Daejin, Anssi Kanervisto, Jongmin Kim, et al. Insights from the\nneurips 2021 nethack challenge. In NeurIPS 2021 Competitions and Demonstrations Track, pages\n41–52. PMLR, 2022a.\nEric Hambro, Roberta Raileanu, Danielle Rothermel, Vegard Mella, Tim Rockt¨aschel, Heinrich\nK¨uttler, and Naila Murray. Dungeons and data: A large-scale nethack dataset. Advances in\nNeural Information Processing Systems, 35:24864–24878, 2022b.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\nShengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint\narXiv:2408.08435, 2024.\nEdward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge\nShi, Tom Schaul, and Tim Rocktaschel. Open-endedness is essential for artificial superhuman\nintelligence. arXiv preprint arXiv:2406.04268, 2024.\nDominik Jeurissen, Diego Perez-Liebana, Jeremy Gow, Duygu Cakmak, and James Kwan. Playing\nnethack with llms: Potential & limitations as zero-shot agents. arXiv preprint arXiv:2403.00690,\n2024.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\nNarasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint\narXiv:2310.06770, 2023.\n12\nPreprint, under review.\nXuanfa Jin, Ziyan Wang, Yali Du, Meng Fang, Haifeng Zhang, and Jun Wang. Learning to discuss\nstrategically: A case study on one night ultimate werewolf. arXiv preprint arXiv:2405.19946,\n2024.\nSubbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant\nBhambri, Lucas Saldyt, and Anil Murthy. Llms can’t plan, but can help planning in llm-modulo\nframeworks. arXiv preprint arXiv:2402.01817, 2024.\nAkbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Ed-\nward Grefenstette, Samuel R Bowman, Tim Rockt¨aschel, and Ethan Perez. Debating with more\npersuasive llms leads to more truthful answers. arXiv preprint arXiv:2402.06782, 2024.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie\nVidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking bench-\nmarking in nlp. arXiv preprint arXiv:2104.14337, 2021.\nMartin Klissarov, Pierluca D’Oro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal\nVincent, Amy Zhang, and Mikael Henaff. Motif: Intrinsic motivation from artificial intelligence\nfeedback. arXiv preprint arXiv:2310.00166, 2023.\nHeinrich K¨uttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward\nGrefenstette, and Tim Rockt¨aschel.\nThe nethack learning environment.\nAdvances in Neural\nInformation Processing Systems, 33:7671–7684, 2020.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\nMichael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald,\nDJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning\nwith algorithm distillation. arXiv preprint arXiv:2210.14215, 2022.\nJonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma\nBrunskill. Supervised pretraining can learn in-context reinforcement learning. Advances in Neural\nInformation Processing Systems, 36, 2024.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\n9459–9474, 2020.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-\nmaking. Advances in Neural Information Processing Systems, 35:31199–31212, 2022.\nLicong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforce-\nment learning via supervised pretraining. arXiv preprint arXiv:2310.08566, 2023.\nJijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, and Yu Wang. Llm-powered hi-\nerarchical language agent for real-time human-ai coordination. arXiv preprint arXiv:2312.15224,\n2023a.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, et al.\nAgentbench: Evaluating llms as agents.\narXiv preprint\narXiv:2308.03688, 2023b.\nCong Lu, Shengran Hu, and Jeff Clune. Intelligent go-explore: Standing on the shoulders of giant\nfoundation models. arXiv preprint arXiv:2405.15143, 2024.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark\ndataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021.\n13\nPreprint, under review.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\nMetaAI.\nLlama\n3.2:\nRevolutionizing\nedge\nai\nand\nvision\nwith\nopen,\ncustomizable\nmodels,\n2024.\nURL\nhttps:\/\/ai.meta.com\/blog\/\nllama-3-2-connect-2024-vision-edge-mobile-devices\/.\nAccessed: 2024-\n09-28.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\nZoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.\nOpenAI. Hello gpt-4o, 2024a. URL https:\/\/openai.com\/index\/hello-gpt-4o\/. Ac-\ncessed: 2024-09-28.\nOpenAI.\nIntroducing openai o1 preview, September 2024b.\nURL https:\/\/openai.com\/\nindex\/introducing-openai-o1-preview\/. Accessed: 2024-09-27.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-\nlow instructions with human feedback. Advances in neural information processing systems, 35:\n27730–27744, 2022.\nGiorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Sch¨olkopf, Mrinmaya Sachan, and Rada\nMihalcea. Cooperate or collapse: Emergence of sustainability behaviors in a society of llm agents.\narXiv preprint arXiv:2404.16698, 2024.\nGrant A Pignatiello, Richard J Martin, and Ronald L Hickman Jr. Decision fatigue: A conceptual\nanalysis. Journal of health psychology, 25(1):123–135, 2020.\nUlyana Piterbarg, Lerrel Pinto, and Rob Fergus. diff history for neural language agents. In Forty-first\nInternational Conference on Machine Learning, 2024.\nYanzhao Qin, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng\nChen, Zenan Zhou, Wentao Zhang, et al. Sysbench: Can large language models follow system\nmessages? arXiv preprint arXiv:2408.10943, 2024.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world\napis. arXiv preprint arXiv:2307.16789, 2023.\nPooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision\nlanguage models are blind. arXiv preprint arXiv:2407.06581, 2024.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.\nA generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-\nbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gem-\nini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Di-\nrani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a bench-\nmark. arXiv preprint arXiv:2311.12022, 2023.\nJingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Hangyu Mao, Ziyue Li, Xingyu\nZeng, Rui Zhao, et al. Tptu: Task planning and tool usage of large language model-based ai\nagents. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.\nMikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro,\nFabio Petroni, Heinrich K¨uttler, Edward Grefenstette, and Tim Rockt¨aschel. Minihack the planet:\nA sandbox for open-ended reinforcement learning research. arXiv preprint arXiv:2109.13202,\n2021.\n14\nPreprint, under review.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can\nteach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,\ngo, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information\nProcessing Systems, 36, 2024.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multi-\nlingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366, 2(5):9, 2023.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement\nlearning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–\n1144, 2018.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally\ncan be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\narXiv:2206.04615, 2022.\nWeihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia,\nJiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A mul-\ntimodal agent for red dead redemption ii as a case study. In ICLR 2024 Workshop on Large\nLanguage Model (LLM) Agents, 2024.\nAdaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar\nBhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al.\nHuman-timescale adaptation in an open-ended task space.\narXiv preprint arXiv:2301.07608,\n2023.\nGraham Todd, Tim Merino, Sam Earle, and Julian Togelius. Missed connections: Lateral thinking\npuzzles for large language models. arXiv preprint arXiv:2404.11730, 2024.\nShengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide\nshut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pages 9568–9578, 2024.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Juny-\noung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350–354, 2019.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman.\nSuperglue: A stickier benchmark for general-purpose language\nunderstanding systems. Advances in neural information processing systems, 32, 2019.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023.\nZilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, Da Yin, Bill Yuchen Lin, and Jingbo Shang.\nOfficebench: Benchmarking language agents across multiple applications for office automation.\narXiv preprint arXiv:2407.19056, 2024.\n15\nPreprint, under review.\nBrian Wansink and Jeffery Sobal. Mindless eating: The 200 daily food decisions we overlook.\nEnvironment and Behavior, 39(1):106–123, 2007.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. arXiv preprint arXiv:2206.07682, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837, 2022b.\nMaciej Wołczyk, Bartłomiej Cupiał, Mateusz Ostaszewski, Michał Bortkiewicz, Michał Zajac, Raz-\nvan Pascanu, Łukasz Kuci´nski, and Piotr Miło´s. Fine-tuning reinforcement learning models is\nsecretly a forgetting mitigation problem. arXiv preprint arXiv:2402.02868, 2024.\nShuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. Enhance\nreasoning for large language models in the game werewolf. arXiv preprint arXiv:2402.02330,\n2024.\nYue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li.\nSmartplay: A benchmark for llms as\nintelligent agents. arXiv preprint arXiv:2310.01557, 2023.\nMingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, and Zhen Xiao. Understanding the\nweakness of large language model agents within a complex android environment. In Proceedings\nof the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 6061–\n6072, 2024.\nYutaro Yamada, Yihan Bao, Andrew K Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluating\nspatial understanding of large language models. arXiv preprint arXiv:2310.14540, 2023.\nHui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and\nadditional opinions. arXiv preprint arXiv:2306.02224, 2023.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.\nWebshop: Towards scalable\nreal-world web interaction with grounded language agents. Advances in Neural Information Pro-\ncessing Systems, 35:20744–20757, 2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2(3), 2023.\nYuhang Zang, Hanlin Goh, Josh Susskind, and Chen Huang. Overcoming the pitfalls of vision-\nlanguage model finetuning for ood generalization. arXiv preprint arXiv:2401.15914, 2024.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nTianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for build-\ning autonomous agents. arXiv preprint arXiv:2307.13854, 2023.\n16\nPreprint, under review.\nA\nBABY AI\nBabyAI (Chevalier-Boisvert et al., 2019) is a research platform designed to study grounded language\nlearning and instruction following in artificial agents. It consists of a suite of 2D grid world envi-\nronments with increasing levels of complexity. In these environments, an agent navigates through\nrooms and interacts with various objects like doors, keys, balls, and boxes of different colors. The\nagent receives natural language instructions, called “missions”, which describe tasks it needs to com-\nplete, such as picking up specific objects or navigating to certain locations. Many existing works on\ndecision-making have studied model performance on this environment (Reed et al., 2022; Li et al.,\n2022). We use it as a historically relevant environment that we expect to be relatively easy to solve.\nA.1\nBABYAI-TEXT\nWe evaluate the agents on 5 tasks introduced in BabyAI-Text (Carta et al., 2023), which provides a\ndescription of each observation instead of a symbolic representation. A textual description consists\nof a list of template descriptions with the following structure:\n• ”You see a <object> <location>” if the object is a key, a ball, a box or a wall.\n• ”You see a(n) open\/closed door <location>” , if the agent sees a door.\n• ”You carry a <object>”, if the agent carries an object.\nA.2\nBABYAI RESULTS\nWe provide BabyAI results for LLM and VLM mode in Tables 4 and 5. Errors are computed with 25\nseeds for each of the 5 tasks of BabyAI. GPT-4o leads, closely followed by Llama 3.1 70B. When\nvision is added to the observation, GPT4o all models performance decrease, except for Gemini-1.5-\nPro, whose performance remains stable.\nTable 4: LLM Performance on BabyAI\nModel\nAverage Progress (%)\ngpt-4o\n77.60 ± 3.73\nllama-3.1-70B-it\n73.20 ± 3.96\ngemini-1.5-pro\n58.40 ± 4.41\nllama-3.2-90B-it\n55.20 ± 4.45\nclaude-3.5-sonnet\n52.00 ± 7.07\ngpt-4o-mini\n50.40 ± 4.47\nllama-3.2-11B-it\n32.80 ± 4.20\nllama-3.1-8B-it\n30.00 ± 6.48\ngemini-1.5-flash\n25.60 ± 3.90\nllama-3.2-3B-it\n10.00 ± 4.24\nllama-3.2-1B-it\n6.00 ± 3.36\nTable 5: VLM Performance on BabyAI\nModel\nAverage Progress (%)\ngpt-4o\n62.00 ± 4.34\ngemini-1.5-pro\n58.40 ± 4.41\nclaude-3.5-sonnet\n50.00 ± 7.07\ngemini-1.5-flash\n43.20 ± 4.43\ngpt-4o-mini\n38.00 ± 4.34\nllama-3.2-90B-it\n28.20 ± 4.02\nllama-3.2-11B-it\n10.40 ± 2.73\nA.3\nOBSERVATIONS\nExample of instruction prompt and observation for BabyAI\n17\nPreprint, under review.\nYou are an agent playing a simple navigation game. Your goal is to open the yellow door. The following are\nthe possible actions you can take in the game, followed by a short description of each action:\nturn left: turn to the left,\nturn right: turn to the right,\ngo forward: take one step forward,\npick up: pick up the object below you,\ndrop: drop the object that you are holding,\ntoggle: manipulate the object in front of you.\nIn a moment I will present you an observation.\nTips:\n- Once the desired object you want to interact or pickup in front of you, you can use the ’toggle’ action to\ninteract with it.\n- It doesn’t make sense to repeat the same action over and over if the observation doesn’t change.\nPLAY!\nCurrent Observation:\na wall 5 steps forward\na wall 2 steps left\na red key 1 step right and 1 step forward\na blue key 1 step right\na yellow key 2 steps right and 3 steps forward\na green key 2 steps right and 1 step forward\na red box 2 steps right\na blue ball 3 steps right and 4 steps forward\na blue box 3 steps right and 1 step forward\na blue box 3 steps right\nImage observation provided.\nGo forward\nB\nCRAFTER\nCrafter (Hafner, 2021) is an open-source 2D survival game designed specifically for research on\nstrong generalization, deep exploration, and long-term reasoning in reinforcement learning. It is a\nMinecraft-inspired, procedurally generated environment that combines resource gathering, craft-\ning, and combat elements.\nAdditionally, the game includes a comprehensive set of tasks and\nachievements, enabling researchers to evaluate agent performance across multiple objectives and\ntime scales. To enable interaction with language models we use the same language wrapper as\nproposed in Wu et al. (2023).\nFigure 3: Crafter’s examples of unique procedurally generated maps.\n18\nPreprint, under review.\nB.1\nCRAFTER RESULTS\nWe provide Crafter results for LLM and VLM format in Tables 6 and 7, standard errors are computed\nusing 10 seeds. GPT4o leads in language-only mode, and Gemini-1.5-Pro leads in vision-language\nmode. Surprisingly, Llama 3.2 90B performance decreases very sharply when images are added,\ngetting worse average progress than its smaller 11B model.\nTable 6: LLM Performance on Crafter\nModel\nAverage Progress (%)\ngpt-4o\n33.10 ± 2.32\nclaude-3.5-sonnet\n32.73 ± 3.20\nllama-3.2-90B-it\n31.69 ± 1.36\nllama-3.1-70B-it\n31.31 ± 2.68\ngemini-1.5-pro\n30.21 ± 2.86\nllama-3.2-11B-it\n26.20 ± 3.30\nllama-3.1-8B-it\n25.45 ± 3.23\ngemini-1.5-flash\n20.00 ± 0.74\ngpt-4o-mini\n12.72 ± 1.13\nllama-3.2-3B-it\n17.27 ± 2.79\nllama-3.2-1B-it\n12.73 ± 1.91\nTable 7: VLM Performance on Crafter\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n37.27 ± 3.14\ngemini-1.5-pro\n33.50 ± 2.07\ngpt-4o\n26.81 ± 3.74\nllama-3.2-11B-it\n23.63 ± 1.48\ngemini-1.5-flash\n20.70 ± 4.43\ngpt-4o-mini\n19.91 ± 3.13\nllama-3.2-90B-it\n10.00 ± 1.13\n19\nPreprint, under review.\nB.2\nOBSERVATIONS\nYou are an agent playing Crafter. The following are the only valid actions you can take in the game, followed\nby a short description of each action:\nNoop: do nothing,\nMove West: move west on flat ground,\nMove East: move east on flat ground,\nMove North: move north on flat ground,\nMove South: move south on flat ground,\nDo: Multiuse action to collect material, drink from lake and hit creature in front,\nSleep: sleep when energy level is below maximum,\nPlace Stone: place a stone in front,\nPlace Table: place a table,\nPlace Furnace: place a furnace,\nPlace Plant: place a plant,\nMake Wood Pickaxe: craft a wood pickaxe with a nearby table and wood in inventory,\nMake Stone Pickaxe: craft a stone pickaxe with a nearby table, wood, and stone in inventory,\nMake Iron Pickaxe: craft an iron pickaxe with a nearby table and furnace, wood, coal, and iron in inventory,\nMake Wood Sword: craft a wood sword with a nearby table and wood in inventory,\nMake Stone Sword: craft a stone sword with a nearby table, wood, and stone in inventory,\nMake Iron Sword: craft an iron sword with a nearby table and furnace, wood, coal, and iron in inventory.\nThese are the game achievements you can get:\n1. Collect Wood\n2. Place Table\n3. Eat Cow\n4. Collect Sampling\n5. Collect Drink\n6. Make Wood Pickaxe\n7. Make Wood Sword\n8. Place Plant\n9. Defeat Zombie\n10. Collect Stone\n11. Place Stone\n12. Eat Plant\n13. Defeat Skeleton\n14. Make Stone Pickaxe\n15. Make Stone Sword\n16. Wake Up\n17. Place Furnace\n18. Collect Coal\n19. Collect Iron\n20. Make Iron Pickaxe\n21. Make Iron Sword\n22. Collect Diamond\nIn a moment I will present a history of actions and observations from the game.\nYour goal is to get\nas far as possible by completing all the achievements.\nPLAY!\nCurrent Observation:\nYour status:\n- health: 9\/9\n- food: 9\/9\n- drink: 9\/9\n- energy: 9\/9\nYou have nothing in your inventory.\nYou see:\n- grass 1 steps to your west\n- tree 3 steps to your north-west\n- cow 3 steps to your west\nYou face grass at your front.\nImage observation provided.\n20\nPreprint, under review.\nMove West\nC\nTEXTWORLD\nTextWorld (Cˆot´e et al., 2019) is a text-based game environment developed by Microsoft Research\nthat allows for the creation and customization of interactive fiction games. In our experiments, we\nutilize three specific games from the TextWorld domain: “Treasure Hunter”, “The Cooking Game”,\nand “Coin Collector”. Each task can be generated with different levels of difficulty by changing\nnumber of rooms, enabling obstacles and including distractor rooms. We use the generation rules\nintroduced in Lu et al. (2024).\nC.1\nTREASURE HUNTER\nIn Treasure Hunter, we create a challenging maze-like environment with 20 rooms. The game is\nset to the maximum difficulty level of 30, introducing locked doors and containers that must be\nmanipulated to locate the target object. To increase complexity, we remove the solution description\nand filter out tasks that can be solved optimally in 20 steps or fewer. This setup requires the agent to\nnavigate a complex space, interact with various objects, and devise strategies to overcome obstacles\nin its quest to find the treasure.\nC.2\nTHE COOKING GAME\nThe Cooking Game presents a culinary challenge set across 13 rooms. We maximize the complexity\nby including up to 5 ingredients and enabling all additional challenging options. The agent must\nnavigate through doors, process food items using tools like knives, and cook ingredients using var-\nious methods such as grilling, frying, and roasting. This game tests the agent’s ability to plan and\nexecute multi-step processes in a dynamic environment, simulating the complexities of real-world\ncooking tasks.\nC.3\nCOIN COLLECTOR\nCoin Collector features an expansive environment with 40 rooms, including potential distractor\nrooms to increase navigation difficulty. Similar to Treasure Hunter, we remove the solution descrip-\ntion to enhance the challenge. The optimal path from the agent’s starting point to the target is set\nto 20 steps, requiring efficient exploration and decision-making. This game tests the agent’s ability\nto navigate large spaces, avoid distractions, and efficiently reach its goal in a complex, maze-like\nstructure.\nC.4\nTEXTWORLD RESULTS\nIn Table 8, we provide results for TextWorld. Standard errors are computed using 20 seeds for each\nof the 3 tasks. GPT-4o once again leads, obtaining more than twice the average progression of its\nclosest competitor Llama 3.1 70B. The coin collector task was by far the most challenging, with\nGPT-4o managing to solve it only once out of 20 attempts. Gemini models’ APIs often failed to\nreturn completions on textworld, flagging the inputs as ”unsafe”, despite there being absolutely no\nreal safety concerns in the textworld gameplays. This made it impossible to complete a full round\nof evaluation on the Gemini models, thus we marked them as 0% progression.\n21\nPreprint, under review.\nFigure 4: TextWorld interface along with visualization.\nTable 8: LLM Performance on Textworld\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n42.06 ± 5.41\ngpt-4o\n39.31 ± 5.24\nllama-3.1-70B-it\n15.00 ± 4.61\ngpt-4o-mini\n12.25 ± 3.55\nllama-3.2-90B-it\n11.18 ± 2.98\nllama-3.2-11B-it\n6.67 ± 2.17\ngemini-1.5-flash\n0.00 ± 0.00\ngemini-1.5-pro\n0.00 ± 0.00\n22\nPreprint, under review.\nC.5\nOBSERVATIONS\nYou are an agent playing TextWorld, a text-based adventure game where you navigate through different\nrooms, interact with objects, and solve puzzles.\nYour goal is to first find the recipe, find and prepare food according to the recipe, and finally prepare\nand eat the meal.\nHere are the available commands:\nlook: describe the current room\ngoal: print the goal of this game\ninventory: print player’s inventory\ngo ⟨dir⟩: move the player north, east, south or west. You can only go to directions indicated with an exit or a\ndoor.\nexamine ...: examine something more closely\neat ...: eat edible food\nopen ...: open a door or a container. You need to open a closed door before you can go through it.\ndrop ...: drop an object onto the floor\ntake ...: take an object that is visible\nput ... on ...: place an object on a supporter\ntake ... from ...: take an object from a container or a supporter\ninsert ... into ...: place an object into a container\nlock ... with ...: lock a door or a container with a key\nunlock ... with ...: unlock a door or a container with a key\ncook ... with ...: cook cookable food with something providing heat\nslice ... with ...: slice cuttable food with something sharp\nchop ... with ...: chop cuttable food with something sharp\ndice ... with ...: dice cuttable food with something sharp\nprepare meal: combine ingredients from inventory into a meal.\nYou can only prepare meals in the Kitchen.\n- You can examine the cookbook to see the recipe when it is visible.\n- The BBQ is for grilling things, the stove is for frying things, the oven is for roasting things. Cooking\ningredients in the wrong way will lead to a failure of the game.\n- Once you have got processed ingredients and the appropriate cooking tool ready, cook all of them according\nto the recipe.\n- There are two conditions to correctly cook something (grill\/fry\/roast):\na) the ingredient you want to cook is in your inventory and\nb) there is a suitable cooking tool in the room, and then use ‘cook . . . with . . . ’ command.\n- When you need to chop\/slice\/dice ingredients, you need to take the knife and the ingredient in your inventory\nand then ‘slice\/chop\/dice ... with knife’\n- Make sure to first process the food (chop\/slice\/dice) before you try to cook them.\n- When you have all the ingredients (that got processed or cooked according to the menu), you can ‘prepare\nmeal’ in the kitchen and then ‘eat meal’ to win the game.\n- The ingredients should EXACTLY match the color in the recipe, but if the recipe doesn’t specify color, any\ncolor would be fine. When you ‘take ... with ...’, use the EXACT name you see.\n- You don’t need to examine the container\/supporter (e.g. toolbox) when it says something like ”there isn’t a\nthing on it”\/”has nothing on it”\nYou have 80 steps to complete the task. Restarting is forbidden.\nPLAY!\n-= Street =-\nYou find yourself in a street. An usual kind of place.\nThere is a closed sliding door leading north. There is an exit to the south.\n-= Street =-0\/1\nopen sliding door\nD\nBABA IS AI\nBaba Is AI is a benchmark environment based on the puzzle game ”Baba Is You”. In this gridworld\ngame, players interact with various objects and textual rule blocks to achieve specific goals. The\nunique aspect of Baba Is AI is that the rules of the game can be manipulated and rearranged by the\nplayer, creating a dynamic environment where agents must identify relevant objects and rules and\n23\nPreprint, under review.\nthen manipulate them to change or create new rules to succeed. This benchmark allows researchers\nto explore a broader notion of generalization compared to current benchmarks, as it requires agents\nto not only learn and follow the rules but also to combine previously seen rules in novel ways.\nAgents are tested on 40 different puzzle levels.\nFigure 5: One of the Baba Is AI puzzles, where the agent has to break the “wall is stop” rule, create\nnew rule “door is win” and go to green door to solve the task.\nD.1\nBABA IS AI LANGUAGE WRAPPER\nTo enable interaction with language models, we made a custom language wrapper for Baba Is AI.\nIt constructs language observation from active rules and creates a description by formatting object\npositions relative to the player. We don’t provide the solution for the agent and don’t specify grid\nboundaries in the text-only experiments.\nD.2\nBABA IS AI RESULTS\nWe provide the Baba Is AI results for LLM and VLM mode in Tables 9 and 10. Standard errors are\ncomputed using 5 seeds for each of the 40 Baba Is AI tasks. Surprisingly, the Llama models lead,\nwith Llama 3.2 90B surpassing GPT-4o by a 10% margin in language-only mode. Once again, when\nvision is added, model performance suffers, with only Gemini-1.5-Pro remaining stable.\nTable 9: LLM Performance on BabaIsAI\nModel\nAverage Progress (%)\nllama-3.2-90B-it\n43.90 ± 3.47\nllama-3.1-70B-it\n40.00 ± 3.42\nclaude-3.5-sonnet\n37.50 ± 4.42\ngpt-4o\n33.66 ± 3.30\ngemini-1.5-pro\n32.02 ± 3.26\nllama-3.1-8B-it\n18.33 ± 3.53\nllama-3.2-3B-it\n17.50 ± 3.47\ngpt-4o-mini\n15.60 ± 2.53\nllama-3.2-11B-it\n15.60 ± 2.50\ngemini-1.5-flash\n12.80 ± 2.33\nllama-3.2-1B-it\n10.83 ± 2.84\nTable 10: VLM Performance on BabaIsAI\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n34.45 ± 4.36\ngemini-1.5-pro\n31.40 ± 3.24\nllama-3.2-90B-it\n21.90 ± 2.89\ngpt-4o\n18.62 ± 2.72\ngpt-4o-mini\n16.41 ± 2.59\ngemini-1.5-flash\n8.30 ± 1.93\nllama-3.2-11B-it\n5.76 ± 1.63\n24\nPreprint, under review.\nD.3\nOBSERVATIONS\nBaba Is You is a puzzle game where you can manipulate the rules of each level. The following are the possible\nactions you can take in the game, followed by a short description of each action:\nidle: wait for one step,\nup: take one step up,\nright: take one step to the right,\ndown: take one step down,\nleft: take one step to the left.\nTips:\n- Examine the level carefully, noting all objects and text blocks present.\n- Identify the current rules, which are formed by text blocks in the format ”[Subject] IS [Property]” (e.g.\n”BABA IS YOU”).\n- Consider how you can change or create new rules by moving text blocks around.\n- Remember that you can only move objects or text that are not defined as ”STOP” or similar immovable\nproperties.\n- Your goal is usually to reach an object defined as ”WIN”, but this can be changed.\n- Think creatively about how changing rules can alter the properties and behaviors of objects in unexpected\nways.\n- If stuck, try breaking apart existing rules or forming completely new ones.\n- Sometimes the solution involves making yourself a different object or changing what counts as the win\ncondition.\nPLAY!\nCurrent Observation:\nActive rules:\nbaba is you\nObjects on the map:\nrule ‘is‘ 1 step to the left and 1 step up\nrule ‘win‘ 1 step up\nrule ‘key‘ 1 step to the left\nkey 1 step to the right and 2 steps down\nball 2 steps to the right and 3 steps down\nrule ‘baba‘ 2 step to the left and 4 steps down\nrule ‘is‘ 1 step to the left and 4 steps down\nrule ‘you‘ 4 steps down\nrule ‘ball‘ 2 steps to the right and 4 steps down\nImage observation provided.\nleft\nE\nMINIHACK\nMiniHack (Samvelyan et al., 2021) is a powerful sandbox framework built on top of the\nNLE (K¨uttler et al., 2020) that enables researchers to easily design rich and diverse environments\nfor RL. It provides a flexible platform for creating custom RL tasks ranging from simple grid-world\nnavigation to complex, procedurally generated worlds with intricate game mechanics. The frame-\nwork allows users to define environments using a human-readable description language or a simple\nPython interface, giving fine-grained control over environment elements such as terrain, objects,\nmonsters, and traps. MiniHack offers a diverse array of tasks, which can be broadly categorized\ninto three main groups: Navigation Tasks, Skill Acquisition Tasks, and Ported Tasks. To enable\ninteraction with language models, we use NetHack Language Wrapper described in the NetHack\nAppendix F.\nFrom the MiniHack Navigation Tasks, we picked Maze 9x9, Maze 15x15, Corridor and Corridor-\nBattle, which challenge the agent to reach the goal position by overcoming various difficulties on\ntheir way, such as fighting monsters in corridors and navigating through complex or procedurally\n25\nPreprint, under review.\ngenerated mazes. These tasks feature a relatively small action space, i.e., movement towards 8\ncompass directions, and based on the environment, search, kick, open, and eat actions.\nFigure 6: Examples of MiniHack Corridor task.\nFigure 7: Example of MiniHack CorridorBattle task.\nFrom the MiniHack Skill Acquisition Tasks, we picked Quest (with three different difficulty levels,\nEasy, Medium, and Hard), which challenges the agent to use objects found in the environment to\ncross a lava river (these objects can provide levitation or freezing abilities), fight monsters, navigate\nthrough rooms or mazes and towards the end of the quest use a wand of death to defeat a powerful\nmonster guarding the goal location.\nFigure 8: Example of MiniHack Quest Hard task.\nWe additionally test the agents on MiniHack Boxoban. This family of environments is an adaptation\nof the Boxoban puzzle game, which itself is inspired by the classic Sokoban. These environments\npresent a challenging puzzle-solving task within the MiniHack framework, leveraging the NetHack\ngame mechanics. The primary goal in MiniHack Boxoban is to push four boulders (MiniHack’s\nequivalent of boxes) onto four designated goal locations, which are represented by fountains. This\ntask requires strategic thinking and planning, as the agent must carefully maneuver the boulders\nthrough the environment without getting them stuck in corners or against walls.\nWe provide MiniHack results for LLM and VLM mode in Tables 11 and 12, standard errors were\ncomputed using 5 seeds for each task. Here, GPT-4o and a Gemini-1.5-Pro equal each other both\nin language-only and vision-language mode, with both models only managing to complete some of\nthe corridor and corridor battle tasks. None of the other models solved any task.\n26\nPreprint, under review.\nFigure 9: Example of MiniHack Boxoban Hard task.\nTable 11: LLM Performance on MiniHack\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n15.00 ± 5.65\ngpt-4o\n10.00 ± 4.74\ngpt-4o-mini\n10.00 ± 4.74\nllama-3.1-70B-it\n7.50 ± 4.16\ngemini-1.5-pro\n5.00 ± 3.45\nllama-3.1-8B-it\n5.00 ± 3.45\ngemini-1.5-flash\n5.00 ± 3.45\nllama-3.2-1B-it\n5.00 ± 3.45\nllama-3.2-11B-it\n2.50 ± 2.47\nllama-3.2-3B-it\n2.50 ± 2.47\nTable 12: VLM Performance on MiniHack\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n22.50 ± 6.60\ngpt-4o\n5.00 ± 3.44\ngemini-1.5-pro\n5.00 ± 3.44\nllama-3.2-90B-it\n2.50 ± 2.47\ngpt-4o-mini\n2.50 ± 2.47\ngemini-1.5-flash\n2.50 ± 2.47\nllama-3.2-11B-it\n2.50 ± 2.47\n27\nPreprint, under review.\nE.1\nOBSERVATIONS\nYou are an agent playing MiniHack. The following are the possible actions you can take in the game, followed\nby a short description of each action:\nnorth: move north,\neast: move east,\nsouth: move south,\nwest: move west,\nnortheast: move northeast,\nsoutheast: move southeast,\nsouthwest: move southwest,\nnorthwest: move northwest,\nfar north: move far north,\nfar east: move far east,\nfar south: move far south,\nfar west: move far west,\nfar northeast: move far northeast,\nfar southeast: move far southeast,\nfar southwest: move far southwest,\nfar northwest: move far northwest,\ndown: go down the stairs,\nwait: rest one move while doing nothing,\nmore: display more of the message,\napply: apply (use) a tool,\nclose: close an adjacent door,\neat: eat something,\nforce: force a lock,\nkick: kick an enemy or a locked door or chest,\nloot: loot a box on the floor,\nopen: open an adjacent door,\npickup: pick up things at the current location if there are any,\npray: pray to the gods for help,\nputon: put on an accessory,\nquaff: quaff (drink) something,\nsearch: search for hidden doors and passages,\nzap: zap a wand.\nIn a moment I will present a history of actions and observations from the game.\nYour goal is to explore the level, fight monsters, and navigate rooms and mazes to ultimately reach\nthe stairs down.\nPLAY!\n28\nPreprint, under review.\nCurrent Observation:\nstatistics:\nStrength: 18\/18\nDexterity: 15\nConstitution: 18\nIntelligence: 8\nWisdom: 10\nCharisma: 8\nDepth: 1\nGold: 0\nHP: 15\/15\nEnergy: 2\/2\nAC: 4\nXP: 1\/0\nTime: 1\nPosition: 27—9\nHunger: Not Hungry\nMonster Level: 0\nEncumbrance: Unencumbered\nDungeon Number: 0\nLevel Number: 1\nScore: 0\nAlignment: Lawful\nCondition: None\ninventory:\na: a +0 katana (weapon in hand)\nb: a +0 wakizashi (alternate weapon; not wielded)\nc: a +0 yumi\nd: 42 +0 ya (in quiver)\ne: an uncursed rustproof +0 splint mail (being worn)\nmessage:\nlanguage observation:\nlava near eastnortheast, east, and southeast\narea of lava near eastsoutheast\nhorizontal wall near southeast and south\nvertical wall near southwest and west\nhorizontal wall very near north, northeast, and northwest\ngoblin adjacent south\nhorn adjacent west\nnewt adjacent northwest\ncursor:\nYourself a samurai\nImage observation provided.\nsouth\nF\nNETHACK LEARNING ENVIRONMENT\nThe NetHack Learning Environment (NLE) (K¨uttler et al., 2020) is a scalable, procedurally gen-\nerated, stochastic, rich, and challenging environment designed to drive long-term research in RL\non problems such as exploration, planning, skill acquisition, and language-conditioned RL. Built\naround the classic and highly complex terminal roguelike game NetHack, NLE provides a complex\nand dynamic environment where agents must navigate through procedurally generated dungeons,\ninteract with hundreds of entity types, and learn to overcome various challenges.\nThe goal of the player is to descend through procedurally generated dungeon levels while killing\nmonsters, solving puzzles, and gathering better equipment in order to retrieve the Amulet of Yen-\ndor and finally ascend back to the surface to win the game. NetHack is notoriously challenging,\neven for human players. Mastering the game can take years even with online resources like the\nNetHack Wiki. Success in NetHack demands long-term strategic planning, as a winning game can\ninvolve hundreds of thousands of steps, as well as short-term tactics to fight hordes of monsters.\nAccurate credit assignment is also crucial to understanding which actions contributed to success\n29\nPreprint, under review.\nor failure. NetHack has already been used extensively as a testbed for RL agents (Wołczyk et al.,\n2024; Piterbarg et al., 2024; Hambro et al., 2022b); tabula-rasa RL agents particularly struggle due\nto sparse reward, complex credit assignment, extremely long-time-horizon, and high stochasticity\nof the game. The current state-of-the-art agent still remains a hand-coded symbolic policy (Hambro\net al., 2022a).\nF.1\nNETHACK LANGUAGE WRAPPER\nThe NetHack Language Wrapper (Goodger et al., 2023) is a tool designed to interface with\nthe NLE and MiniHack by translating non-language observations into text-based representations.\nThis wrapper, converts various NLE observations such as glyphs, blstats, tty_chars,\ninv_letters, inv_strs, and tty_cursor into readable text equivalents. For example, it\ntransforms the visual display of the game environment into a textual description, including details\nabout the surroundings, inventory, and player statistics. The wrapper also supports text-based ac-\ntions, allowing users to interact with the environment using commands like wait, apply, and\nnorth, which are then converted into the discrete actions required by the NLE. This functionality\nenables easier interaction with the NetHack environment, particularly for language models.\n(a) NetHack progression by dungeon level reached\n(b) NetHack progression by experience level\nFigure 10\nF.2\nNEW NETHACK PROGRESSION SYSTEM\nNetHack features an in-game scoring system that rewards players for actions such as killing mon-\nsters, identifying objects, eating food, collecting gold and items, and ultimately ascending in the\ngame. However, we argue that this scoring system does not effectively capture true game progres-\nsion, as players can win the game with scores ranging from a few hundred thousand to several\nmillion points. To address this limitation, we developed a novel, data-informed progression metric\n30\nPreprint, under review.\nusing a dataset of human-played NetHack games (Hambro et al., 2022b). Specifically, we recorded\nthe dungeon levels and experience levels achieved in each game, as well as whether the game re-\nsulted in an ascension. Utilizing these statistics, we constructed a data-centric progression system\nwhere each data point represents the probability of a human player winning the game after reach-\ning a specific dungeon level or experience level. The resulting progression curves are presented in\nFigure 10. For practical purposes, we define Dungeon Level 1 (Dlvl:1) and Experience Level 1 as\nrepresenting 0% progression, corresponding to the game’s starting point, and ascension as 100%\nprogression. The agent’s overall progress is thus determined by the highest progression achieved\nbetween the dungeon level and experience level attained.\nF.3\nNETHACK RESULTS\nWe provide NetHack results for LLM and VLM mode in Tables 13 and 14. Standard errors are\ncomputed using 5 seeds. o1-preview achieves the highest progression out of all the tested models.\nHowever, it is still very far from making any significant progression in the game. The best indi-\nvidual run was achieved by Gemini-1.5-Pro vision-language mode, reaching dungeon level 3 and\nexperience level 4.\nTable 13: Language-Only Performance on NLE\nModel\nAverage Progress (%)\no1-preview\n1.57 ± 0.40\nclaude-3.5-sonnet\n0.58 ± 0.52\ngpt-4o\n0.37 ± 0.37\no1-mini\n0.36 ± 0.24\nllama-3.1-70B-it\n0.35 ± 0.35\nllama-3.1-8B-it\n0 ± 0\ngemini-1.5-pro\n0.31 ± 0.31\ngpt-4o-mini\n0 ± 0\ngemini-1.5-flash\n0 ± 0\nllama-3.2-90B-it\n0 ± 0\nllama-3.2-11B-it\n0 ± 0\nllama-3.2-3B-it\n0 ± 0\nllama-3.2-1B-it\n0 ± 0\nTable 14:\nVision-Language Performance on\nNLE\nModel\nAverage Progress (%)\nclaude-3.5-sonnet\n1.16 ± 0.42\ngemini-1.5-pro\n0.48 ± 0.48\ngpt-4o\n0.37 ± 0.37\ngpt-4o-mini\n0 ± 0\ngemini-1.5-flash\n0 ± 0\nllama-3.2-11B-it\n0 ± 0\ngemini-1.5-flash\n0 ± 0\nllama-3.2-90B-it\n0 ± 0\nllama-3.2-11B-it\n0 ± 0\nF.4\nOBSERVATION\nDespite having a language wrapper that describes its observations (Goodger et al., 2023), NetHack\nis not meant to be played with language only, thus we provided the ASCII map in language mode\nand the RGB tiles map in vision-language mode. In the LLM context, we only keep information that\nis important to be kept in the long term, i.e., the game message and language observation. Agent\nstats and inventory are only needed in the current step, so we do not keep them in the context. This\nis done also to prevent the context length of NetHack to explode out of control.\n31\nPreprint, under review.\nYou are an agent playing NetHack.\nThe following are the possible actions you can take in the game, followed by a short description of each action:\nnorth: move north,\neast: move east,\nsouth: move south,\nwest: move west,\nnortheast: move northeast,\nsoutheast: move southeast,\nsouthwest: move southwest,\nnorthwest: move northwest,\nfar north: move far north,\nfar east: move far east,\nfar south: move far south,\nfar west: move far west,\nfar northeast: move far northeast,\nfar southeast: move far southeast,\nfar southwest: move far southwest,\nfar northwest: move far northwest,\nup: go up a staircase,\ndown: go down a staircase (tip: you can only go down if you are standing on the stairs),\nwait: rest one move while doing nothing,\nmore: display more of the message (tip: ONLY ever use when current message ends with –More–),\nannotate: leave a note about the level,\napply: apply (use) a tool,\ncall: name a monster or object, or add an annotation,\ncast: cast a spell,\nclose: close an adjacent door,\nopen: open an adjacent door,\ndip: dip an object into something,\ndrop: drop an item,\ndroptype: drop specific item types (specify in the next prompt),\neat: eat something (tip: replenish food when hungry),\nesc: exit menu or message,\nengrave: engrave writing on the floor (tip: Elbereth),\nenhance: advance or check weapons skills,\nfire: fire ammunition from quiver,\nfight: fight a monster (even if you only guess one is there),\nforce: force a lock,\n[...]\nread: read a scroll or spellbook,\nremove: remove an accessory,\nrub: rub a lamp or a stone,\nsearch: search for hidden doors and passages,\nswap: swap wielded and secondary weapons,\ntakeoff: take off one piece of armor,\ntakeoffall: take off all armor,\nteleport: teleport to another level (if you have the ability),\nthrow: throw something (e.g. a dagger or dart),\ntravel: travel to a specific location on the map (tip: in the next action, specify ⟨or ⟩for stairs, { for fountain,\nand\nfor altar),\ntwoweapon: toggle two-weapon combat,\nuntrap: untrap something,\nwear: wear a piece of armor,\nwield: wield a weapon,\nwipe: wipe off your face,\nzap: zap a wand,\nTips:\n- When the message asks for a completion, such as: ”What do you want to eat? [d or ?*]”, you should respond\nwith a single character corresponding to the item you want to eat\/use.\n- For example, ”What do you want to eat? [dgh or ?*]” -¿ Possible answers are ”d”, ”g”, or ”h” to eat the\nassociated food.\n- When the message asks for a direction, such as: ”In what direction?” you should respond with a direction.\n- When the message has –More– at the end, your next action should be ”more” to see the rest of the message.\n- Explore the environment to find the stairs down to the next level.\n- Always carefully read the last message to understand the current state of the game and decide your next\naction accordingly.\n- If you keep moving in the same direction, you will eventually hit a wall and stop moving. Your message\nmight be: ”It’s solid stone”, or ”It’s a wall”. Change your action to move in another direction to continue\nexploring the environment.\n- Read the language observation carefully and look at ascii map or image observation provided to decide the\nnext action to take and where to move next.\n- You can attack monsters by moving into them.\nIn a moment I will present a history of actions and observations from the game.\nYour goal is to get as far as possible in the game.\nPLAY!\n32\nPreprint, under review.\nCurrent Observation:\nstatistics:\nStrength: 14\/14\nDexterity: 9\nConstitution: 11\nIntelligence: 8\nWisdom: 15\nCharisma: 18\nDepth: 1\nGold: 0\nHP: 16\/16\nEnergy: 6\/6\nAC: 3\nXP: 1\/0\nTime: 1\nPosition: 54—14\nHunger: Not Hungry\nMonster Level: 0\nEncumbrance: Unencumbered\nDungeon Number: 0\nLevel Number: 1\nScore: 0\nAlignment: Lawful\nCondition: None\ninventory:\na: a +1 long sword (weapon in hand)\nb: a +1 lance (alternate weapon; not wielded)\nc: an uncursed +1 ring mail (being worn)\nd: an uncursed +0 helmet (being worn)\ne: an uncursed +0 small shield (being worn)\nf: an uncursed +0 pair of leather gloves (being worn)\ng: 11 uncursed apples\nh: 10 uncursed carrots\nmessage:\nSalutations Agent, welcome to NetHack! You are a lawful female human\nKnight.\nlanguage observation:\nvertical closed door far east\ndark area far east\nscroll labeled THARR near eastsoutheast\nhorizontal wall near southeast and south\ndark area near west\nvertical wall very near southwest\nvertical open door very near west\nhorizontal wall adjacent north, northeast, and northwest\ntame pony adjacent southeast\ncursor:\nYourself a knight\nImage observation provided.\nwest\nF.5\nLLMS POSSESS EXTENSIVE NETHACK KNOWLEDGE\nAgents need extensive knowledge to play NetHack effectively. Skilled players leverage their under-\nstanding of the game, acquired through hours of gameplay, while new players often rely on written\nresources online, such as the NetHack Wiki. We use a questionnaire to test if LLMs possess some\nof this knowledge and assess if agents can apply it. The questions are as follows:\n1. Are there any consequences of eating rotten food in NetHack? (Answer: It might lead to\nblindness, hallucination or even instant death.)\n2. Are there any consequences of eating too much food in NetHack? (Answer: The player\ncharacter might choke on the food and die.)\n33\nPreprint, under review.\n3. What happens when you ascend the stairs on level 1 of NetHack? (Answer: It leads to\ninstant game over.)\n4. Is it a good idea to use an invisibility wand on a monster in NetHack? (Answer: No, as it\nwill be harder to fight them.)\n5. Is it a good idea to pray to an angered god in NetHack? (Answer: No, the god will punish\nthe player.)\nThese questions test a fundamental understanding of the game mechanics, particularly focusing on\nbehaviors that new players may mistakenly attempt and should be avoided.\nWe summarize each LLM’s responses in the tables below. For each question, we assess whether the\nresponse is accurate, whether the conclusion is correct (i.e., the LLM recognizes the need to avoid\nsuch behavior), and whether the agent successively avoids these mistakes.\nLLM\nQ1\nQ2\nQ3\nQ4\nQ5\nGPT 4o\nCorrect\n✔\n✔\n∼\n✔\n✔\nConclusion\n✔\n✔\n✔\n✔\n✔\nBehaviour\n✗\n✔\n✗\nN\/A\n✔\nGPT 4o-mini\nCorrect\n∼\n✗\n✔\n✗\n✔\nConclusion\n✔\n✔\n✔\n✔\n✔\nBehaviour\n✗\n✔\n✔\nN\/A\nN\/A\nGemini 1.5-flash\nCorrect\n✗\n✗\n✗\n✗\n✔\nConclusion\n✔\n✗\n✗\n✗\n✔\nBehaviour\n✔\n✔\n✗\nN\/A\nN\/A\nGemini 1.5-pro\nCorrect\n✔\n∼\n✗\n✔\n✔\nConclusion\n✔\n✔\n✗\n✔\n✔\nBehaviour\n✔\n✔\n✗\nN\/A\nN\/A\nLlama 3.1 70B Instruct\nCorrect\n✔\n✗\n✔\n✗\n✔\nConclusion\n✔\n✗\n✗\n✔\n✔\nBehaviour\n✗\n✗\n✗\n✗\n✗\nLlama 3.2 11B Instruct\nCorrect\n✗\n✗\n✗\n✗\n✔\nConclusion\n✔\n✗\n✗\n✔\n✔\nBehaviour\n✗\n✗\n✗\nN\/A\nN\/A\nLlama 3.2 90B Instruct\nCorrect\n✔\n∼\n✔\n✗\n✔\nConclusion\n✔\n✔\n✔\n✔\n✔\nBehaviour\n✗\n✔\n✗\nN\/A\nN\/A\nTable 15: Comparison of each LLMs (ability to apply) knowledge in Nethack. We manually grade\nthe responses to each question based on the correctness of the response given (i.e. does the response\nmatch information from the Nethack wiki), the correctness of their conclusion (i.e. does the LLM\ncorrectly identify that such behaviour should be avoided), and whether an LLM agent’s behaviour\nduring evaluation is consistent with the ground truth (i.e. does the agent successfully avoid the\nbehaviours indicated in the questions). For answers that are partially correct, we award a ∼. We\nrecord behaviour as N\/A when the agent does not encounter scenarios where knowledge of the\ncorresponding question should be applied.\nWe observe that while generally the LLMs understand to avoid common mistakes, regardless of\nwhether their reasoning is completely correct, they still struggle to consistently exploit that knowl-\nedge. Agents will often consume rotten food and prematurely exit the game by ascending the steps\non the first level. This illustrates a gap between LLM agents ability to exploit knowledge in practice.\n34\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games.pdf"}
{"title":"TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs","authors":"Haochuan Wang, Xiachong Feng, Lei Li, Zhanyue Qin, Dianbo Sui, Lingpeng Kong","summary":"The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges.","url":"http:\/\/arxiv.org\/abs\/2410.10479v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.10479v1","published":1728911734000,"comment":null,"pdf_text":"TMGBENCH\nTMGBENCH:\nA SYSTEMATIC GAME BENCHMARK\nFOR EVALUATING STRATEGIC REASONING ABILITIES\nOF LLMS\nHaochuan Wang♠♡∗\nXiachong Feng♠†\nLei Li♠\nZhanyue Qin♡\nDianbo Sui♡\nLingpeng Kong♠†\n♠The University of Hong Kong ♡Harbin Institute of Technology\npinkexsu0v0@gmail.com, fengxc@hku.hk, lpk@cs.hku.hk\nABSTRACT\nThe rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing attention.\nTo evaluate the strategic reasoning capabilities of LLMs, game theory, with its\nconcise structure, has become the preferred approach for many researchers. How-\never, current research typically focuses on a limited selection of games, resulting\nin low coverage of game types. Additionally, classic game scenarios carry risks\nof data leakage, and the benchmarks used often lack extensibility, rendering them\ninadequate for evaluating state-of-the-art models. To address these challenges, we\npropose TMGBENCH1, a benchmark characterized by comprehensive game type\ncoverage, novel and diverse scenarios, and flexible game organization. Specifi-\ncally, we incorporate all 144 game types summarized by the Robinson-Goforth\ntopology of 2×2 games, which are constructed as classic games in our benchmark.\nFurthermore, we employ synthetic data generation techniques to create diverse,\nhigher-quality game scenarios through topic guidance and human inspection for\neach classic game, which we refer to as story-based games. Lastly, to provide a\nsustainable evaluation framework adaptable to increasingly powerful LLMs, we\ntreat the aforementioned games as atomic units and organize them into more com-\nplex forms through sequential, parallel, and nested structures. We conducted a\ncomprehensive evaluation of mainstream LLMs, covering tests on rational rea-\nsoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in com-\nplex game forms. The results revealed that LLMs still have flaws in the accuracy\nand consistency of strategic reasoning processes, and their levels of mastery over\nTheory-of-Mind also vary. Additionally, o1-mini, the latest reasoning model from\nOpenAI, was also evaluated across the sequential, parallel, and nested game struc-\ntures and reached accuracy rates of 66.6%, 60.0%, and 70.0%, respectively, high-\nlighting the challenges posed by TMGBENCH.\n1\nINTRODUCTION\nThe rapid advancement of large language models (LLMs) has reshaped the paradigm of artificial\nintelligence, achieving breakthroughs across various domains (Zhao et al., 2023; Huang & Chang,\n2022; Lewkowycz et al., 2022; Huang et al., 2022; Paranjape et al., 2023). These achievements\nare largely attributed to LLMs’ ability to assimilate vast amounts of knowledge during training,\nemerging with the capacity to organize information at a coarse level and link knowledge at a fine-\ngrained level through their internal representations (Min et al., 2023; Zhao et al., 2023). These core\ncapabilities have driven the success of LLMs in numerous reasoning tasks, including mathematical\nreasoning (Hendrycks et al., 2021; Zhang et al., 2023), commonsense reasoning (Sap et al., 2019;\nBisk et al., 2020), logical reasoning (Lei et al., 2023), and strategic reasoning (Lor`e & Heydari,\n∗Work done during an internship at the University of Hong Kong.\n†Corresponding Author.\n1The dataset and evaluation codes will be available at https:\/\/github.com\/PinkEx\/TMGBench.\n1\narXiv:2410.10479v1  [cs.AI]  14 Oct 2024\nTMGBENCH\n2023; Duan et al., 2024). Among these, strategic reasoning has attracted considerable attention due\nto its multi-agent nature and close association with social intelligence (Gandhi et al., 2023).\nStrategic reasoning refers to the cognitive process of anticipating, planning, and responding to\nothers’ actions to achieve specific objectives within competitive or cooperative contexts (Zhang\net al., 2024a).\nConsequently, game scenarios—naturally involving both cooperation and com-\npetition—have intuitively become a fertile ground for studying LLMs’ strategic reasoning abili-\nties (Brookins & DeBacker, 2023). In particular, researchers have engaged LLMs in game-playing,\nanalyzing their decision-making behaviors and evaluating their strategic intelligence in such scenar-\nios (Duan et al., 2024). The Prisoner’s Dilemma, as one of the most classic game theory scenarios,\nhas been extensively studied in this context (Herr et al., 2024). Additionally, other traditional games\nsuch as the Battle of the Sexes (Kreps, 1990), the Stag Hunt (Carlsson & Van Damme, 1993), and\nthe Dictator Game (Forsythe et al., 1994) have also drawn significant attention. These studies pro-\nvide initial insights into the strategic reasoning capabilities of LLMs (Horton, 2023; Brookins &\nDeBacker, 2023; Phelps & Russell, 2023; Akata et al., 2023; Li et al., 2023; Aher et al., 2022).\nHowever, current research has three major limitations, hindering a comprehensive, robust, and sus-\ntainable evaluation of LLMs’ strategic reasoning capabilities: (1) Limited coverage of game types:\nMost studies focus on a handful of classic games without considering the full diversity of game\nstructures. (2) Potential risk of game scenario leakage: Classic game scenarios are likely to be\npresent in the training corpus, raising concerns over data leakage. (3) Poor extensibility of game\nforms: Existing studies primarily focus on a narrow range of game forms, which may no longer\nsuffice to challenge high-performing LLMs such as o1-mini from OpenAI.\nTo address the above issues, we introduce TMGBENCH, a benchmark that encompasses a com-\nprehensive range of game types, features synthesized game scenarios, and supports scalable and\nreorganizable game forms. Specifically, to address the first issue, we include all 144 game types de-\nfined by the Robinson-Goforth topology of 2x2 games (Robinson & Goforth, 2005). This topology\nencompasses a variety of game structures based on different numerical payoff matrices, including\nbut not limited to classic games like the Prisoner’s Dilemma(§2.2). To address the second issue, we\nemploy synthetic data generation techniques to create five different story-based games for each clas-\nsic game. In essence, a story-based game is a contextual framing counterpart of its corresponding\nclassic game, sharing the same structure but differing in context (Lor`e & Heydari, 2023). To ensure\nhigh-quality data synthesis, we introduce two additional steps: topic control and human inspection.\nWe first define a set of topics commonly associated with cooperation and competition, such as busi-\nness and law, to guide the data generation process. Then, to ensure that the synthesized games meet\nthe required game structures and are easily understandable, we conduct rigorous human inspection\n(§2.3). To address the third issue, we propose three forms for expanding and organizing games:\nsequential, parallel, and nested. Using the above constructed games as atomic units, we reorganize\nthem into these complex forms to assess the strategic reasoning of LLMs. The sequential and par-\nallel forms evaluate the model’s capacity for sequential and parallel decision-making, respectively,\nwhile the nested form explores the LLMs’ multi-layered strategic reasoning abilities (§2.4).\nBased on TMGBENCH, we conduct comprehensive analyses and evaluations of current mainstream\nLLMs (§3), including assessments of rational reasoning, reasoning robustness, Theory-of-Mind\n(ToM) capabilities, and reasoning in complex game forms, leading to the following key findings:\n(1) Advanced LLMs like gpt-4o demonstrate strong strategic reasoning, with an accuracy rate over\n80%, but struggle to generalize across various contexts and scenarios. Models like claude-3-5-sonnet\nfurther reveal this inconsistency, with performance variability marked by coefficients nearing 0.5.\n(2) Although GPT models often perform well, their reasoning inconsistencies on certain task sub-\ntypes are marked by an asymmetric pattern, which is the main cause of the statistical biases.\n(3) Several top-tier LLMs demonstrate stable first-order ToM abilities, with some effectively uti-\nlizing second-order ToM for comparable tasks. In contrast, models such as Llama-3.1-70B appear\nrestricted to first-order reasoning.\n(4) Complex-form games that are derived from atomic units in TMGBENCH present considerable\nchallenges for LLMs, including those with strong reasoning abilities like o1-mini from OpenAI,\nwhich often struggle as the number of games increases.\n2\nTMGBENCH\nRobinson-Goforth \nTopology\nPd\nB1\nB2\nA1\n4\n3\n1\n3\nA2\n2\n1\n2\n4\nGame Structure\n(e.g. Prisoner’s \nDilemma, PD)\nSarah\ncoffee shop owner\nMark\ncoffee chain entrepreneur\nKeep \nprices\nLower \nprices\nMaintain \noperations\nExpand \nbusiness\nContextual\nFraming\nDirect Answer\nChain of Thought\nFirst-order ToM\nSecond-order ToM\nLLM’s answer\nstandard answer\nDIFF\nData Preparation\nEvaluation\nIt will be the best \nif Sarah keeps \nprices steady, I \ncan maximize \nprofits by \nmaintain \noperations then ...\nKeeps prices \nsteady would be \nbetter, and Mark \nmust be \nreluctant to \nexpand his \nbusiness ...\n```python\nanswer= …\n```\nPython-style \nrequired answer\nBusiness\nPolitics\nTransportation\nGPT-4o\nAssisted\nFigure 1: An concept map of TMGBENCH.\nThe data preparation of the benchmark in-\ncludes 3 ingredients: Robinson-Goforth topol-\nogy, game structure and contextual framing.\nThe evaluation of the benchmark embraces\nseveral prompting methods (including ToM\npromptings) to elicit strategic reasoning pro-\ncess of LLMs.\nSequential\nParallel\nNested\npre-game\ngame1\ngame2\ngame3\ngame1\ngame2\ngame3\nNE\ncore-game\nNE\nif:\npre-game\nthen:\ncore-game\nnew NE of  the \npre-game:\nCondition\nGame Pair\nFigure 2: We design several complex forms of\nstrategic reasoning tasks using TMGBENCH.\nwhich include:\n(1) sequential form, where\nLLMs are required to response multiple game\ntasks in a row, with history of previous tasks; (2)\nparallel form, where LLMs are required to re-\nsponse multiple game tasks simultaneously; (3)\nnested form, where LLMs are required to re-\nsponse a set of interlinked game tasks (in our\nsettings, we relate to them as pre-game and\ncore-game).\nGames in the complex forms\ncan be selected with different game structures\nand various contexts.\n2\nTMGBENCH\n2.1\nBENCHMARK OVERVIEW\nTMGBENCH is a benchmark designed to evaluate the strategic reasoning capabilities of LLMs in\ngame-theoretic scenarios, illustrated by Figure 1. It comprehensively covers 144 types of games\n(see §2.2), with each type containing multiple instances (in each instance, there are two players and\neach player can choose between two strategies, resulting in four possible situations), which can be\ncategorized into classic and story-based settings. Notably, the story-based instances are produced\nusing synthetic data generation techniques and are grounded in real-life themes, effectively mitigat-\ning the issue of data leakage (see §2.3). Furthermore, each game in TMGBENCH can be treated as\nan atomic unit, and multiple atomic games can be structured in a more complex task with parallel,\nsequential, or nested form (see §2.4). These complex scenarios effectively facilitate the evaluation\nof advanced LLMs’ abilities in parallel, sequential, and multi-layered decision-making. To precisely\nevaluate the reasoning abilities of LLMs, we use their performance in inferring the optimal strategy\ncombination, i.e., the Nash equilibrium, as the evaluation criterion. Additionally, the designed eval-\nuation metrics provide a fine-grained assessment of the robustness and self-consistency of LLMs’\nstrategic reasoning abilities (see §2.5).\n2.2\nGAME TOPOLOGY\nAlthough previous research has explored LLMs’ reasoning abilities within the context of game the-\nory, existing studies have primarily focused on a few well-known games, such as the Prisoner’s\nDilemma, Battle of the Sexes, and Stag Hunt (Brookins & DeBacker, 2023; Phelps & Russell, 2023;\nGuo, 2023). However, these studies cover a limited game types, resulting in incomplete evaluations.\nThereby, a broader variety of games is urgently needed to conduct a systematic assessment of LLMs.\nTo address this, we incorporate 144 game types (we later refer to a type as an equivalence class)\nbased on the Robinson-Goforth topology of 2×2 games (Robinson & Goforth, 2005). Classic games\nlike the Prisoner’s Dilemma belong to one of the equivalence classes within this topology. Specif-\n3\nTMGBENCH\nPractical Map\nStandard Map\nInconsistency Map\nFigure 3: Demonstration of the inconsistency\nheat map. Each grid is divided into 4 quarter-\ngrids, indicating the 4 situations. By subtract-\ning the standard map from the practical map\nelement-wise, we get the inconsistency map,\nwhere blue indicate positive difference and red\nindicate negative difference.\nThe deeper the\ncolour means the larger the difference between\nthe LLM’s response and the standard answer.\nv\nFigure 4: Axisymmetry in heat maps is illus-\ntrated by the left sub-figure, where the standard\nheat map exhibits perfect axisymmetry across\nthe counter-diagonal. In contrast, LLMs’ re-\nsponses tend to show quasi-axisymmetry, as\nshown by the right sub-figure. Certain pairs of\npositions fail to align precisely when reflected\nacross the axis and may exhibit discrepancies,\ndeviating from the ideal symmetric pattern.\nically, the topology of 2×2 games elegantly illustrates the relationships among strictly ordinal 2×2\ngames, each with a unique payoff structure, leading to different dominant strategies, Nash equilibria,\nand reasoning approaches (more details in Appendix B.1). We categorize all the 144 games with\nnumerical payoffs from the original topology into the classic setting tasks. Due to space constraints,\nwe provide an introduction to the Robinson-Goforth topology in Appendix B.2.\n2.3\nCONTEXTUAL FRAMING\nRelying on the Robinson-Goforth topology, we can systematically construct all types of classic\nsetting tasks. However, this alone is insufficient, as games often take place in diverse real-life\ncontexts, involving different topics, types of participants and their preferences. Such contextual\nframing of games introduces new challenges for LLMs (Lor`e & Heydari, 2023).\nTo further explore LLMs’ strategic reasoning capabilities in real-world scenarios, we use classic\ngames as seed data and employ synthetic data generation techniques, leveraging GPT-4o to construct\nstory-based setting tasks. Specifically, in story-based setting tasks, we replace the pure game infor-\nmation of classic setting tasks with real-life scenarios, covering topics such as business, law and\ntransportation. Additionally, the two players are substituted with characters representing broader\nsemantics (e.g., people, animals, organizations, and even nations), and the payoff values are trans-\nformed from pure numbers into specific states or rewards relevant to the characters. For each classic\nsetting task, we generate 5 corresponding story-based setting tasks.\nTo ensure high-quality data generation, we undertake the following steps: First, we use GPT-4o\nto synthesize the contextual data. Second, we design precise prompts to ensure the generated data\nadhere to the given game structures. Third, we select topics from real-life scenarios where strategic\ninteractions are common, guiding the data generation process. Finally, we conduct rigorous human\nreviews to ensure the data’s quality and diversity.\nMore details on the data generation process, prompts, human review procedures, and topic distribu-\ntion of the data can be found in Appendix C.\n2.4\nCOMPLEX FORMS\nThe 2×2 games in the topology represent a highly condensed game structure. However, in real\nlife, we often encounter more complex game forms, such as making continuous decisions, making\nmultiple decisions simultaneously, or considering the impacts of one decision on another.\nTo evaluate LLMs’ strategic reasoning abilities with more constraints, we treat the aforementioned\nindividual games as atomic games and expand them in three forms: sequential, parallel, and nested.\nThe organization of these forms is illustrated in Figure 2. Specifically, in the sequential form, we\nrandomly sample multiple games from the story-based games, requiring the LLM to make decisions\nsequentially. Only if the LLM provides correct answers for all games is it considered to have made\ncorrect decisions. In the parallel form, the LLM is given multiple randomly sampled games and\n4\nTMGBENCH\nmust make decisions simultaneously. Similarly, the LLM is deemed to have made correct decisions\nonly if it solves all games correctly. In the nested form, we randomly sample two games, desig-\nnated as the pre-game and the core-game, where the core-game holds greater importance.\nThe decisions made by the LLM in the pre-game affect the strategy space in the core-game.\nThus, the LLM is judged to have made correct decisions only if it demonstrates forward-looking\nreasoning by choosing a sub-optimal solution in the pre-game to achieve the optimal solution in\nthe core-game. We demonstrate a template to generate an nested form game in Appendix D.3.\nTheoretically, using these atomic games, we can expand the framework to generate infinitely many\nincreasingly complex game forms, thereby providing a continuous benchmark for evaluating the\nperformance of more advanced LLMs.\n2.5\nEVALUATION METRICS\nAs explained in Section 2.2, our benchmark are perfectly suitable to display in a 12x12 square table,\neach grid representing one of the 144 equivalence classes. In the evaluation process we conduct\nrepetitive tests in every data point of each equivalence class. Each test starts with the input of the\nsetting (classic\/story-based) and the question, and ends with LLM’s response containing a list of\nchoices corresponding to multiple choices or no choice (when the given list is empty).\nNotation. For notation, we assign Freqi,j,o as the frequency of the o-th choice happening to be in\nthe tests of the grid at i-th row, j-th column, where the 1, 2, 3 and 4-th choice correspond to the\nupper-left, upper-right, lower-left and lower-right quarter-grid respectively.\nInconsistency Heat Map. According to conclusions of the Robinson-Goforth topology (Robinson\n& Goforth, 2005), we convert the standard answer of each equivalence class into a heat map named\nthe standard heat map, with the coloured quarter-grid to be the choice in the standard answer. Simi-\nlarly, as for practical result provide by LLMs, we set the value of Freqi,j,o as the colour depth of each\nquarter grid, which builds up the practical heat map. Naturally, we subtract the standard heat map\nfrom the practical heat map in an element-wise manner to get the inconsistency heat map, which is\na standardised tool for our evaluation, shown in Figure 3.\nInconsistency Degree. In order to display the quantified performance of LLMs, we extract inconsis-\ntency degree from a map, which helps reveal the gap between LLMs’ response and standard answer,\nand it is defined as\nID =\n1\n144\n12\nX\ni=1\n12\nX\nj=1\n1\n4\n4\nX\no=1\n∆Freq2\ni,j,o\nwhere ∆Freqi,j,o indicates the the difference (between the LLM’s answer and the standard answer)\nof frequency of the o-th choice at i-th row, j-th column.\nBias Degree. Owing to the symmetric property of the topology framework of 2×2 matrix games, the\ndistribution of answers over the heat map has axial symmetry by the counter-diagonal (Figure 4).\nMotivated by this elegant property, we set up another metric to evaluate the bias degree of LLMs’\nanswers, which we expect robuster LLMs to display lower degrees of bias. The bias degree reflects\nthe stability and symmetry of LLMs’ strategy, and it is defined as\nBD =\n1\n144\n12\nX\ni=1\n12\nX\nj=1\n1\n4\n4\nX\no=1\n(Freqi,j,o −Freqj,i,refo)2\nwhere the meaning of refo is the index of choice o’s counterpart considering the reflection operation\nby the counter-diagonal, and we have the mapping relation: {1, 2, 3, 4} 7→{4, 2, 3, 1}. (e.g. ref1 =\n4 means that the reflection counterpart of choice 1 is choice 4, vice versa)\nPerfect Accuracy Rate. In addition to the metrics mentioned above, we also set up a more rigorous\nmetric named perfect accuracy rate, which ignores the partially correct answer and only considers\nperfectly correct answer in each test, and it is defined as\nPAR =\n1\n144\n12\nX\ni=1\n12\nX\nj=1\n1\nT\nT\nX\nt=1\nI{rspt,i,j = stdi,j}\n5\nTMGBENCH\nTable 1: Overall statistics of LLMs’ performance on classic setting tasks. The up arrow(↑) means\nthe larger value indicates better performance, while the down arrow(↓) means the smaller value\nindicates better performance. All values are expressed as percentages.\nFamily\nModel\nMetric \/ Prompting\nPAR(↑)\nID(↓)\nBD(↓)\nDA\nCoT\nDA\nCoT\nDA\nCoT\nGPT\ngpt-4o\n52.08\n80.38\n16.81\n3.78\n28.49\n7.79\ngpt-4o-mini\n14.93\n74.02\n27.15\n4.38\n48.59\n8.29\ngpt-3.5-turbo\n30.21\n34.38\n27.64\n17.87\n50.15\n30.19\nClaude\nclaude-3-5-sonnet\n59.38\n79.69\n14.79\n7.13\n27.76\n14.34\nclaude-3-haiku\n24.31\n40.28\n39.58\n25.17\n72.22\n44.10\nLlama\nLlama-3.1-70B\n13.02\n54.29\n36.15\n15.32\n40.71\n26.63\nLlama-3.1-8B\n18.75\n22.63\n38.49\n31.19\n81.32\n47.64\nQwen\nQwen2-72B\n43.06\n46.21\n26.30\n19.94\n35.59\n29.29\nwhich means that we count only if the response perfectly matches the standard answer, where T\nrepresents the number of times we invoke a LLM to response on a certain game task.\nMetrics with Subscript. As a matter of fact, within the topology, different equivalence classes have\ndifferent number of Nash equilibria (ranging from {0, 1, 2}), leading to a discrepancy in reasoning\ndifficulty, therefore we propose metrics with subscript that represents for different types of equiva-\nlence groups (we refer them to 0-task, 1-task, 2-task respectively), which we refer to as sub-metrics.\nTherefore we have IDn, BDn, PARn(n = 0, 1, 2) which means the inconsistency degree, the bias\ndegree, and the perfect accuracy rate across all equivalence classes that have n equilibra.\n3\nANALYSIS\n3.1\nOVERVIEW OF LLMS’ PERFORMANCE\nOverall, we select several SOTA models according to Open LLM Leaderboard (Fourrier\net al., 2024) and conduct extensive experiments on TMGBENCH.\nThese models include GPT\n(gpt-4o-2024-05-13, gpt-4o-mini-2024-07-18, gpt-3.5-turbo-0125), Claude (claude-3-5-\nsonnet-20240620, claude-3-haiku-20240307), Llama (Llama-3.1-8B, Llama-3.1-70B), and\nQwen (Qwen2-72B). We perform 4 independent tests on each data point, covering both the clas-\nsic setting and the story-based setting. Basically, we conduct 2,880 tests to generally evaluate a\ncertain model. During the evaluation, we set the temperature of the tested LLMs to 0 or near 0,\nensuring the lowest degree of uncertainty and enhancing the faithfulness of our evaluation. More\ndetails of the evaluation process are provided in Appendix C.1.\nGames in TMGBENCH are not easy for most LLMs. First we overall evaluate how well LLMs\ncan behave on the classic setting tasks of our benchmark, to assess their basic capability of strategic\nreasoning. We initially adopt two basic prompting methods: Direct Answer (DA) prompting and\nChain-of-Thought (CoT, (Wei et al., 2022)) prompting, which represent shallower, faster thinking\npatterns and deeper, slower thinking patterns, respectively.\nAs seen from Table 1, gpt-4o, gpt-4o-mini and claude-3-5-sonnet are more capable compared to\nothers, with a high overall accuracy rate (≈80%) and low inconsistency and low bias score (≈\n5%). Specifically, as shown in Figure 5, gpt-4o performs the best on 1-tasks, gpt-4o-mini beats\nothers on 2-tasks, and claude-3-5-sonnet are relately better at 0-tasks. Moreover, comparing the\nperformance of employing DA prompting and CoT prompting, we find that CoT prompting almost\nprovides comprehensive improvement but few exceptions like the PAR2 of Llama-3.1-70B.\nDespite the excellent performance of the top-tier models (gpt-4o and claude-3-5-sonnet), other mod-\nels often do not exhibit robust performance across all 3 different types of tasks. The inconsistency\n2AntiBD = 1 −\n√\nBD, AntiID = 1 −\n√\nID\n6\nTMGBENCH\nPAR1\nAntiBD1\nAntiID2\nPAR2\nAntiBD2\nAntiID0\nPAR0\nAntiBD0\nAntiID1\n0.25\n0.50\n0.75\nDA\nPAR1\nAntiBD1\nAntiID2\nPAR2\nAntiBD2\nAntiID0\nPAR0\nAntiBD0\nAntiID1\n0.25\n0.50\n0.75\nCoT\ngpt-4o\ngpt-4o-mini\ngpt-3.5-turbo\nclaude-3-5-sonnet\nclaude-3-haiku\nLlama-3.1-70B\nLlama-3.1-8B\nQwen-2-72B\nFigure 5: Radar charts of the 9 sub-metrics of 8 LLMs’ performance, comparing the DA prompting\n(left side) and the CoT prompting (right side). AntiID and AntiBD are derived from ID and BD\nwhile higher values indicate better performances (in order to consistent with PAR).2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nValue of PARn(\n)\ngpt-4o\ngpt-4o-mini\ngpt-3.5-turbo\nclaude-3-5-sonnet\nclaude-3-haiku\nLlama-3.1-70B\nLlama-3.1-8B\nQwen2-72B\nModels\nC-PAR0\nC-PAR1\nC-PAR2\nS-PAR0\nS-PAR1\nS-PAR2\n(a) PARn(↑)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nValue of IDn(\n)\nC-ID0\nC-ID1\nC-ID2\nS-ID0\nS-ID1\nS-ID2\n(b) IDn(↓)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nValue of BDn(\n)\nC-BD0\nC-BD1\nC-BD2\nS-BD0\nS-BD1\nS-BD2\n(c) BDn(↓)\nFigure 6: Comparison of LLMs’ performance under the classic setting (indicated by ‘C-’ label, in\nopaque colour) and the story-based setting (indicated by ‘S-’ label, in semi-opaque colour with error\nbar), where the length of the bars represent the value of metrics, and the error bars represent the\nstandard deviation over all 5 data points of the story-based setting tasks.\ndegree and bias degree in these models can be more than double or triple those of the top-performing\nmodels. This indicates that from a systematic point of view, even classic setting tasks from TMG-\nBENCH are challenging for most LLMs.\nLLMs’ performance is vulnerable across various narratives. At the theoretical level, we consider\nclassic setting tasks and story-based tasks to be fundamentally the same problems within the domain\nof game theory. However, this conclusion appears not transferable to LLMs at the practical level. For\nLLMs, the complexity and nuance of story-based tasks introduce unique challenges, where LLMs\nare required to be robust in understanding and reasoning concurrently.\nIn Figure 6, we compare the performance of LLMs using CoT prompting, which is robuster ac-\ncording to previous analysis. The figure reveals the vulnerable performance of LLMs on tasks in\nstory-based setting (corresponding to various narratives), marked by two primary characteristics:\n(1) The advanced models, specifically gpt-4o, gpt-4o-mini and claude-3-5-sonnet, exhibit signifi-\ncant performance degradation. Notably, gpt-4o demonstrates a broad under-performance across the\nboard, while gpt-4o-mini experiences the most pronounced decline in performance on 2-task sce-\nnarios, where its S-PAR2 metric falls to less than one-third of its C-PAR2 counterpart. Similarly,\nclaude-3-5-sonnet shows the largest performance drop in 0-task, with its S-PAR0 metric reduced to\nless than one-fourth of C-PAR0, and its S-ID0 metric exceeding four times that of C-ID0.\n7\nTMGBENCH\nTable 2: Performance of LLMs using different ToM compared to CoT. Text in red color indicates the\nperformance gets better and text in blue color indicates the performance gets worse (both compared\nto CoT). Bold text means the best performance across the three prompting methods. Grey areas\nmean an LLM is good at using some kind(s) of ToM. All values are expressed as percentages.\n0-Task\n1-Task\n2-Task\nModel\nPrompting\nPAR0(↑)\nID0(↓)\nBD0(↓)\nPAR1(↑)\nID1(↓)\nBD1(↓)\nPAR2(↑)\nID2(↓)\nBD2(↓)\nCoT\n34.72\n13.37\n14.41\n92.36\n1.58\n6.76\n54.17\n7.38\n7.38\nFoToM\n43.06\n9.46\n9.81\n95.14\n0.72\n4.14\n50.00\n8.94\n8.59\ngpt-4o\nSoToM\n31.94\n9.81\n10.68\n91.67\n1.45\n6.00\n52.78\n7.99\n8.16\nCoT\n25.00\n15.62\n23.94\n72.45\n5.08\n11.09\n70.83\n7.97\n7.69\nFoToM\n25.00\n19.53\n19.53\n99.54\n0.03\n5.08\n47.22\n10.59\n10.59\ngpt-4o-mini\nSoToM\n18.06\n26.56\n26.22\n98.84\n0.19\n5.38\n68.06\n5.38\n5.38\nCoT\n0.00\n19.44\n29.69\n41.67\n17.55\n30.95\n25.00\n18.23\n26.13\ngpt-3.5-turbo\nFoToM\n0.00\n21.44\n22.83\n54.40\n19.30\n42.52\n0.00\n37.85\n59.20\nCoT\n86.11\n4.25\n20.23\n88.89\n4.72\n11.68\n18.06\n24.48\n24.48\nFoToM\n68.06\n7.73\n16.06\n92.13\n2.56\n7.74\n47.22\n15.10\n15.10\nclaude-3-5-sonnet\nSoToM\n47.22\n21.35\n28.99\n90.05\n4.05\n14.38\n33.33\n14.93\n14.93\nCoT\n0.00\n40.28\n47.22\n49.07\n22.45\n44.91\n27.78\n26.39\n36.11\nclaude-3-haiku\nFoToM\n0.00\n33.33\n37.50\n47.22\n22.22\n48.61\n11.11\n43.06\n56.94\nCoT\n8.33\n22.47\n26.43\n65.59\n13.43\n27.16\n25.00\n19.53\n23.70\nFoToM\n2.78\n30.82\n35.59\n49.54\n18.68\n27.49\n69.44\n6.08\n22.74\nLlama-3.1-70B\nSoToM\n23.61\n21.27\n28.73\n60.42\n14.09\n23.70\n12.50\n24.05\n25.26\nCoT\n0.00\n27.34\n46.09\n25.77\n32.90\n47.17\n26.39\n24.74\n52.00\nLlama-3.1-8B\nFoToM\n0.00\n22.14\n59.20\n27.55\n31.97\n67.18\n15.28\n33.64\n65.49\nCoT\n20.83\n29.25\n32.20\n50.78\n19.35\n28.73\n44.44\n14.15\n29.77\nQwen2-72B\nFoToM\n0.00\n36.46\n35.07\n45.14\n26.92\n49.54\n11.11\n37.50\n49.13\n(2) The performance of certain localities exhibits significant fluctuations. A particularly notable\ndegradation occurs in the PAR scores for 0-task and 2-task scenarios handled by claude-3-5-sonnet,\nwhere the coefficients of variation cv (defined as cv = σ\nµ, with σ representing the standard devi-\nation and µ the mean) approach 0.5. These eminent values of cv suggest a lack of robustness in\nperformance across different narratives.\n3.2\nFINDINGS OF LLMS’ BEHAVIOURS\nLLMs demonstrate first\/second-order ToM abilities. In tasks across all equivalence classes,\n1-tasks have the lowest reasoning difficulty because at least one player has a dominant strategy,\nwhich means the player can make an unconditionally optimal decision regardless of the counter-\npart’s choice. In such cases, once a player (denoted as A) can make this unconditionally optimal\ndecision, their counterpart (B) can, using first-order Theory-of-Mind (ToM), easily determine the\nbest response for themselves (B).\nThis insight motivated us to apply FoToM prompting to LLMs, representing the First-order Theory-\nof-Mind thinking, to aid in solving these tasks. As seen in Table 2, top-tier models like gpt-4o show\nimprovement in both 0-tasks and 1-tasks when utilizing FoToM. Model claude-3-5-sonnet improves\non 1-tasks and 2-tasks, and gpt-4o-mini displays a significant surge in performance on 1-tasks and so\ndoes Llama-3.1-70B on 2-tasks. However, for models like Llama-3.1-8B and Qwen2-72B, FoToM\ndoes not seem to provide any prominent advantage and may even result in worse performance.\nNotably, no LLM achieves overall improvement across all task categories by merely using first-\norder ToM, and 0-tasks appear to be the most challenging for LLMs to solve.\nFurthermore, we wondered if LLMs display some ability to use first-order ToM could also be capable\nof second-order ToM. According to Liddle & Nettle (2006), higher-order ToMs are generally more\ndifficult to master than first-order ToM. Thus we selected only advanced models that demonstrated\nproficiency in first-order ToM to attempt solving specific tasks using Second-order Theory-of-Mind\n(SoToM) prompting. As seen in Table 2, models like gpt-4o, gpt-4o-mini and claude-3-5-sonnet\nshow consistent performance when applying second-order ToM to tasks they are already capable of\nsolving better with first-order ToM. However, the improvements from using SoToM generally do not\nexceed those achieved with first-order ToM. In addition, Llama-3.1-70B’s underperformance with\n8\nTMGBENCH\nCoT\nFoToM\nSoToM\ngpt-4o-mini\ngpt-4o\nFigure 7: Inconsistency heat map of GPT se-\nries models using different prompting methods.\nThe yellow boxes and green boxes represent\nthe 0-task areas in the topological framework.\nsequential\nparallel\nnested\nForm\n0\n4\n8\n12\n16\n20\nAccuracy Count\ngpt-4o, 3-length\no1-mini, 3-length\ngpt-4o, 5-length\no1-mini, 5-length\ngpt-4o, 10-length\no1-mini, 10-length\ngpt-4o, 2-folds nested\no1-mini, 2-folds nested\nFigure 8:\nTop LLMs’ performance on the\ngames in complex forms of three types. Ow-\ning to the expensive inference cost, we run 20\ntimes for each configuration.\nSoToM suggests that possessing first-order ToM capabilities does not necessarily imply proficiency\nwith second-order ToM. The prompts used for FoToM and SoToM are provided in Appendix C.2.\nCertain behavioural pattern contributes to poor performance. Based on the analysis from the\nprevious sections, it is encouraging to note that top-tier LLMs demonstrate high accuracy and low\ninconsistency when solving 1-task scenarios, regardless of the prompting used (CoT, FoToM, or\nSoToM). However, their performance declines significantly when addressing other types of tasks.\nFor the advanced GPT series models, it is particularly noteworthy that they perform the worst on 0-\ntasks out of all types. Apart from the low PAR and high ID on 0-tasks compared to 1-tasks, the bias\ndegree also doubles (for gpt-4o) or even several times higher (for gpt-4o-mini). Surprisingly, as il-\nlustrated in Figure 7, these models display a similar answering pattern that appears non-coincidental.\nWithin the topological framework, there are two square areas representing 0-tasks (enclosed in yel-\nlow boxes and green boxes), which should theoretically be symmetric across the counter-diagonal.\nThe standard heat map of these two areas is entirely blank, reflecting no existing equilibrium, so the\ntwo areas of the inconsistency heat maps just reflect the distribution of LLMs’ practical responses.\nUnder closer inspection, it becomes evident that the models exhibit a consistent pattern when ad-\ndressing 0-tasks. In yellow-box areas, their answers tend to emphasize the upper-right and lower-left\nquarter-grids, whereas in green-box areas, their answers tend to emphasize the upper-left and lower-\nright quarter-grids. This pattern appears to be the primary cause of the high bias degree. However,\nthe phenomenon is quite counter-intuitive: it introduces a strong asymmetry along the counter-\ndiagonal. In other words, simply swapping the id of two players and their actions, which does not\nalter the fundamental game structure, leads the LLMs to identify different Nash equilibria. Never-\ntheless, it is quite strange for them to provide such uniform “wrong answers” within each box, while\nthe answers across the two boxes are entirely asymmetric.\nTo testify that this is not due to the position bias in the prompts (refer to the FoToM prompting and\nSoToM prompting in Appendix C.2), we design the reFoToM prompting and the reSoToM prompting\n(refer to the reFoToM prompting and reSoToM prompting in Appendix C.2) which swap the order of\nthe players happens in the FoToM prompting and the SoToM prompting respectively. The results in\nAppendix D.1 imply that such asymmetric inconsistency pattern is not strong related to the orders\nin the prompt. We demonstrate two typical examples of this phenomenon in Appendix D.2.\nComplex forms bring more challenging tasks. To verify that TMGBENCH can be extended to\nharder tasks which may better align with complicated scenarios from the reality, we run the test\non the three complex forms we mention in Section 2.4, to assess the performance of two strongest\nLLMs (o1-mini and gpt-4o) in complex strategic reasoning.\nWe setup the test by dividing it into several types: (1) in sequential form and parallel form, we set\nthe variable of number of the games from the set {3, 5, 10}; (2) in nested form, we just use some\n2-folds nested games (due to the high verification cost when the number increases).\nAs seen from Figure 8, the top-tier model gpt-4o has a dramatically low accuracy rate in either\nsequential or parallel games, even the strongest reasoning model o1-mini still failed at times; when\n9\nTMGBENCH\nthe number of the games increase, their performances both drop, which is consistent with intuition.\nAs for the games of nested form, two models’ performances are relatively reasonable, while it is fair\nto infer that if we increase the number of layers of the games that in the nested structures, it will\npresent a great challenge for LLMs. The overall accuracy rates of o1-mini over the three forms are\n66.6%, 60.0% and 70.0% respectively, while gpt-4o performs worse, with accuracy rates reaching\nonly 50.0%, 35.0% and 70.0% respectively.\n4\nRELATED WORK\nStrategical Reasoning of LLMs. Large language models have made notable breakthroughs in rea-\nsoning tasks, such as mathematical, causal, and commonsense reasoning, enabling their increasing\nuse in complex tasks that support human decision-making (Imani et al., 2023; Kıcıman et al., 2023;\nZhao et al., 2024). This progress has sparked a growing interest in studying their strategic reasoning\ncapabilities (Zhang et al., 2024a). Game theory, with its highly abstract representation of real-world\nstrategic scenarios, has garnered significant attention from researchers (Duan et al., 2024; Huang\net al., 2024). The prisoner’s dilemma, as one of the most classical games, has been widely used to\nevaluate the strategic reasoning abilities of LLMs (Brookins & DeBacker, 2023; Guo, 2023; Akata\net al., 2023; Phelps & Russell, 2023; Xu et al., 2023). In addition, several well-known game theory\nscenarios, such as the Dictator Game (Horton, 2023; Fan et al., 2023; Brookins & DeBacker, 2023),\nthe Ultimatum Game (Aher et al., 2022), the Public Goods Game (Li et al., 2023) and the Battle\nof the Sexes (Akata et al., 2023), have been employed to evaluate LLMs’ capabilities. However,\ncurrent studies often focus on individual games, resulting in incomplete assessments and less ro-\nbust conclusions. To address this, we propose TMGBENCH, a benchmark for evaluating LLMs by\n2×2 games, where its atomic games can be further organized using sequential, parallel, and nested\nformats to provide an in-depth evaluation of the SOTA models gpt-4o and o1-mini.\nTheory-of-Mind of LLMs. Theory-of-Mind (ToM) refers to the ability to understand and infer\nhuman mental states (Premack & Woodruff, 1978). Due to the multi-player nature of game theory,\nplayers’ ability to reason about the “minds” of other participants is crucial. Existing research has\ninitiated discussions on whether machines possess ToM capabilities. For instance, Kosinski (2023)\nsuggested that ToM might emerge spontaneously in LLMs, as demonstrated through assessments\nusing false-belief tasks. However, (Ullman, 2023) argued that such successes are fragile, easily\ndisrupted by minor perturbations that would not affect an entity genuinely possessing ToM. Never-\ntheless, many researchers propose enhancing LLMs’ strategic reasoning abilities by incorporating\nToM. Guo et al. (2023) designed the Suspicion-Agent, which integrates a ToM-aware planning ap-\nproach that leverages higher-order ToM capabilities, considering not only what the opponent might\ndo (first-order ToM) but also what the opponent believes the Suspicion-Agent will do (second-order\nToM). Additionally, Yim et al. (2024) introduced a ToM planning method in the Guandan poker\ngame, Liu et al. (2024) proposed an intention-guided mechanism, Xu et al. (2023) developed Prob-\nabilistic Graphical Modeling, and Zhang et al. (2024b) introduced K-Level-Reasoning, all utilizing\nToM to enhance LLMs’ strategic reasoning. Given the broad application of ToM, this paper lever-\nages TMGBENCH to comprehensively evaluate LLMs’ ability to employ first-order and second-\norder ToM reasoning techniques for strategic reasoning.\n5\nCONCLUSION\nIn this work, we introduce TMGBENCH, a benchmark for systematically evaluating the strategic\nreasoning abilities of LLMs by 2x2 matrix games. Based on Robinson-Goforth topology, we de-\nvelop the classic setting tasks, and introduce various narratives based on story contexts generated by\nGPT-4o. By utilizing TMGBENCH, we can identify current flaws in LLMs’ performance on these\ntasks, such as low accuracy rates and unstable inconsistency and bias degrees, even though the task\ndifficulty is relatively moderate compared to many others. Additionally, when employing prompts\nto elicit their Theory-of-Mind thinkings on these tasks, some LLMs show improved performance,\nindicating that LLMs can, to some extent, master ToM and apply it in their reasoning processes.\nHowever, possessing first-order ToM abilities does not necessarily mean that LLMs will excel at\nmastering higher-order ToM. Furthermore, based on TMGBENCH, we introduce more forms of\ncomplex strategic reasoning tasks and pose a new challenge for LLMs.\n10\nTMGBENCH\nREFERENCES\nGati Aher, RosaI. Arriaga, and Adam Tauman Kalai. Using large language models to simulate multi-\nple humans and replicate human subject studies. In International Conference on Machine Learn-\ning, 2022. URL https:\/\/api.semanticscholar.org\/CorpusID:251719353.\nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.\nPlaying repeated games with large language models. ArXiv preprint, abs\/2305.16867, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2305.16867.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about\nphysical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artifi-\ncial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelli-\ngence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7432–7439. AAAI Press, 2020.\nURL https:\/\/aaai.org\/ojs\/index.php\/AAAI\/article\/view\/6239.\nPhilip Brookins and Jason Matthew DeBacker. Playing games with gpt: What can we learn about a\nlarge language model from canonical strategic games? Available at SSRN 4493398, 2023.\nHans Carlsson and Eric Van Damme. 12 equilibrium selection in stag hunt games. Frontiers of game\ntheory, pp. 237, 1993.\nJinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-\nEskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench: Uncovering the strategic reasoning\nlimitations of llms via game-theoretic evaluations. ArXiv preprint, abs\/2402.12348, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2402.12348.\nCaoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational\nplayers in game theory? a systematic analysis. ArXiv preprint, abs\/2312.05488, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2312.05488.\nRobert Forsythe, Joel L Horowitz, Nathan E Savin, and Martin Sefton. Fairness in simple bargaining\nexperiments. Games and Economic behavior, 6(3):347–369, 1994.\nCl´ementine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open\nllm leaderboard v2. https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/\nopen_llm_leaderboard, 2024.\nKanishk Gandhi, Dorsa Sadigh, and Noah Goodman. Strategic reasoning with language models. In\nNeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.\nFulin Guo. Gpt in game theory experiments. ArXiv preprint, abs\/2305.05516, 2023. URL https:\n\/\/arxiv.org\/abs\/2305.05516.\nJiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo. Suspicion-\nagent: Playing imperfect information games with theory of mind aware gpt-4. ArXiv preprint,\nabs\/2309.17277, 2023. URL https:\/\/arxiv.org\/abs\/2309.17277.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv\npreprint, abs\/2103.03874, 2021. URL https:\/\/arxiv.org\/abs\/2103.03874.\nNathan Herr, Fernando Acero, Roberta Raileanu, Mar´ıa P´erez-Ortiz, and Zhibin Li. Are large lan-\nguage models strategic decision makers? a study of performance and bias in two-player non-\nzero-sum games. ArXiv preprint, abs\/2407.04467, 2024. URL https:\/\/arxiv.org\/abs\/\n2407.04467.\nJohn J Horton. Large language models as simulated economic agents: What can we learn from\nhomo silicus? Technical report, National Bureau of Economic Research, 2023.\nJen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenx-\niang Jiao, Xing Wang, Zhaopeng Tu, and Michael R Lyu.\nHow far are we on the decision-\nmaking of llms? evaluating llms’ gaming ability in multi-agent environments. ArXiv preprint,\nabs\/2403.11807, 2024. URL https:\/\/arxiv.org\/abs\/2403.11807.\n11\nTMGBENCH\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.\nArXiv preprint, abs\/2212.10403, 2022. URL https:\/\/arxiv.org\/abs\/2212.10403.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied rea-\nsoning through planning with language models. ArXiv preprint, abs\/2207.05608, 2022. URL\nhttps:\/\/arxiv.org\/abs\/2207.05608.\nShima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large\nlanguage models. ArXiv preprint, abs\/2303.05398, 2023. URL https:\/\/arxiv.org\/abs\/\n2303.05398.\nEmre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language\nmodels: Opening a new frontier for causality.\nArXiv preprint, abs\/2305.00050, 2023.\nURL\nhttps:\/\/arxiv.org\/abs\/2305.00050.\nMichal Kosinski. Theory of mind might have spontaneously emerged in large language models.\nArXiv preprint, abs\/2302.02083, 2023. URL https:\/\/arxiv.org\/abs\/2302.02083.\nDavid M Kreps. Game theory and economic modelling. Oxford University Press, 1990.\nBin Lei, Chunhua Liao, Caiwen Ding, et al. Boosting logical reasoning in large language models\nthrough a new framework: The graph of thought. ArXiv preprint, abs\/2308.08614, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2308.08614.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems,\n35:3843–3857, 2022.\nJiatong Li, Rui Li, and Qi Liu. Beyond static datasets: A deep interaction approach to llm evaluation.\nArXiv preprint, abs\/2309.04369, 2023. URL https:\/\/arxiv.org\/abs\/2309.04369.\nBethany Liddle and Daniel Nettle. Higher-order theory of mind and social competence in school-age\nchildren. Journal of Cultural and Evolutionary Psychology, 4(3-4):231–244, 2006.\nZiyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, and Jieyu Zhao. Interintent: Investigating\nsocial intelligence of llms via intention understanding in an interactive game context.\nArXiv\npreprint, abs\/2406.12203, 2024. URL https:\/\/arxiv.org\/abs\/2406.12203.\nNunzio Lor`e and Babak Heydari. Strategic behavior of large language models: Game structure\nvs. contextual framing. ArXiv preprint, abs\/2309.05898, 2023. URL https:\/\/arxiv.org\/\nabs\/2309.05898.\nBonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via\nlarge pre-trained language models: A survey. ACM Computing Surveys, 56(2):1–40, 2023.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models.\nArXiv preprint, abs\/2303.09014, 2023. URL https:\/\/arxiv.org\/abs\/2303.09014.\nSteve Phelps and Yvan I. Russell.\nThe machine psychology of cooperation: Can gpt mod-\nels operationalise prompts for altruism, cooperation, competitiveness and selfishness in eco-\nnomic games?\nArXiv preprint, 2023.\nURL https:\/\/api.semanticscholar.org\/\nCorpusID:258685424.\nDavid Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526, 1978.\nDavid Robinson and David Goforth. The topology of the 2x2 games: a new periodic table, volume 3.\nPsychology Press, 2005.\n12\nTMGBENCH\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Com-\nmonsense reasoning about social interactions.\nIn Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP), pp. 4463–4473, Hong Kong, China,\n2019. Association for Computational Linguistics. doi: 10.18653\/v1\/D19-1454. URL https:\n\/\/aclanthology.org\/D19-1454.\nTomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. ArXiv\npreprint, abs\/2302.08399, 2023. URL https:\/\/arxiv.org\/abs\/2302.08399.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837, 2022.\nLin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See-Kiong Ng, and\nJiashi Feng. Magic: Investigation of large language model powered multi-agent in cognition,\nadaptability, rationality and collaboration. In ICLR 2024 Workshop on Large Language Model\n(LLM) Agents, 2023.\nYauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, and Yangqiu Song.\nEvaluating and enhancing llms agent based on theory of mind in guandan: A multi-player co-\noperative game under imperfect information.\nArXiv preprint, abs\/2408.02559, 2024.\nURL\nhttps:\/\/arxiv.org\/abs\/2408.02559.\nSarah J Zhang, Samuel Florin, Ariel N Lee, Eamon Niknafs, Andrei Marginean, Annie Wang,\nKeith Tyser, Zad Chin, Yann Hicke, Nikhil Singh, et al. Exploring the mit mathematics and\neecs curriculum using large language models.\nArXiv preprint, abs\/2306.08997, 2023.\nURL\nhttps:\/\/arxiv.org\/abs\/2306.08997.\nYadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu,\nTing Song, Man Lan, and Furu Wei. Llm as a mastermind: A survey of strategic reasoning with\nlarge language models. ArXiv preprint, abs\/2404.01230, 2024a. URL https:\/\/arxiv.org\/\nabs\/2404.01230.\nYadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, and Furu Wei. K-level\nreasoning with large language models. ArXiv preprint, abs\/2402.01521, 2024b. URL https:\n\/\/arxiv.org\/abs\/2402.01521.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. ArXiv\npreprint, abs\/2303.18223, 2023. URL https:\/\/arxiv.org\/abs\/2303.18223.\nZirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for\nlarge-scale task planning. Advances in Neural Information Processing Systems, 36, 2024.\n13\nTMGBENCH\nA\nBASIC THINGS ABOUT GAME THEORY\nIn this section, we discuss two fundamental concepts in game theory: dominant strategy and Nash\nequilibrium.\nA dominant strategy is one that always provides a player with a payoff at least as high as any other\nstrategy, regardless of the actions of other players. In other words, if a player has a dominant strategy,\nthey will consistently choose it, as it either maximizes their payoff or does not reduce it, irrespective\nof the strategies chosen by others.\nNash equilibrium refers to a set of strategies, one for each player, where no player can benefit\nby unilaterally changing their strategy. At a Nash equilibrium, each player’s strategy is the best\nresponse to the strategies of the other players. This means that if all players are following their Nash\nequilibrium strategies, no one has an incentive to deviate from their current strategy. It represents a\nstable state in the game where players’ strategies are mutually optimal.\nIn many games, the dominant strategy equilibrium and Nash equilibrium may coincide, but not\nalways. A dominant strategy equilibrium is a specific type of Nash equilibrium where each player\nhas a strategy that is optimal regardless of others’ strategies. However, in many cases, dominant\nstrategies may not exist, requiring Nash equilibria to be identified through analysis and computation.\nChicken\nCh\nBattle of the Sexes\nBoS\nHero(BoSsw)\nHr\nDelight-Pure\nDp\nDelight-Mixed\nDm\nPrisoner’s\nPd\nDilemma\nS\nStag Hunt\nAne\nAssurancene\nAsw\nAssurancesw\nHm\nHarmony-Mixed\nHp\nHarmony-Pure\nNc\nNo Conflict\n(a) Most Famous Games\nPd\nColumn \npayoffs\n4\n3\nRow \npayoffs\n1\n3\n2\n1\n2\n4\nLayer id: 1\nPrisoner’s \nDilemma\nRow id: 1\nColumn id: 1\n(2, 2)\n(1, 4)\n(4, 1)\n(3, 3)\n(b) Details in a Grid\nFigure 9: The topology of the normal-form game system, which is presented by a square consisting\nof 12×12 grids. Figure 9a displays the position of the most famous games in the topology. In each\ngrid, there are specific details of the game, which is shown in Figure 9b.\nB\n2×2 MATRIX GAME\nB.1\nDEFINITION\nA normal-form game, commonly referred to as a 2×2 matrix game when involving two players each\nwith two strategies, is a fundamental concept in game theory for representing strategic interactions.\nIn this form, the game is depicted as a matrix, clearly outlining the players’ strategies and corre-\nsponding payoffs. A typical 2×2 matrix game is structured as shown in Table 3.\nTable 3: The form of typical 2×2 matrix games.\nPlayer B: Strategy 1\nPlayer B: Strategy 2\nPlayer A: Strategy 1\n(a, w)\n(b, x)\nPlayer A: Strategy 2\n(c, y)\n(d, z)\nIn this matrix, each cell represents the payoffs for both player A and player B, based on their chosen\nstrategies. For instance, if player A selects strategy 1 and player B selects strategy 2, player A\nreceives a payoff of a, while player B receives a payoff of w.\n14\nTMGBENCH\nB.2\nTOPOLOGY\nGame theory research often concentrates on the Prisoner’s Dilemma and a few other symmetric\ngames, even though most potential games are asymmetric, and many ordinal games involve ties.\nThe findings on the topology of ordinal normal-form games (Robinson & Goforth, 2005) provide an\nelegant framework for systematically studying these games, encompassing all equivalence classes in\nan ordinal sense (where “ordinal” refers to the ranking of payoffs rather than their specific values).\nIn this topological framework, as depicted in Figure 9, well-known games such as the Prisoner’s\nDilemma, Stag Hunt, Battle of the Sexes, and Chicken are all symmetric and situated on the counter-\ndiagonal of a 12×12 grid. The remaining games are located in the other grids, each with a corre-\nsponding “sister game” that can be derived by reflecting across the counter-diagonal. A pair of sister\ngames are identical when the roles of the two players are reversed.\nWithin each grid, basic information about the games in the equivalence classes is provided, including\nthe family name and abbreviation, the payoff matrix, and the order graph, which illustrates the\nincentives for the row\/column player to unilaterally change their choice for a higher payoff.\nThese 144 equivalence classes include 18 games with no equilibrium, 18 games with exactly two\nequilibria, and 108 games with a single equilibrium. Their distribution within the topology is sym-\nmetric across the counter-diagonal.\n1 NE\n0 NE\n1 NE\n2 NEs\n1 NE\n1 NE\n1 NE\n1 NE\n1 NE\n2 NEs\n1 NE\n0 NE\n1 NE\n1 NE\n1 NE\n1 NE\nFigure 10: The distribution of games with 0, 1, or 2 Nash equilibria (a) is depicted according to the\ntopology. Grids in grey indicate games with only 1 Nash equilibrium, while white grids represent\ngames with no Nash equilibrium. Grids in other colours represent games with exactly 2 Nash equi-\nlibria. Text in blue\/red indicates that the column\/row player has a dominant strategy in the game,\nwhile white text signifies that both players have dominant strategies. In contrast, black text indicates\nthat neither player has a dominant strategy.\nB.3\nSOLUTION STRUCTURE\nAs previously mentioned, all games in the topological framework can be categorized into three\ndistinct groups based on the number of Nash equilibria. If we consider Nash equilibrium as the\nsolution to finding stable strategy combinations, Figure 10 illustrates the structure of these solutions.\nIn games with exactly one Nash equilibrium, at least one player (either the column player, row\nplayer, or both) has a dominant strategy, meaning they do not need to consider the other player’s\nchoice. These games are represented by grey or black grids.\nConversely, games with either 0 or 2 Nash equilibria share the characteristic that neither player has\nan unconditionally optimal choice, meaning no dominant strategies exist. However, in games with\nno Nash equilibrium (white grids), at least one player always has an incentive to unilaterally change\ntheir choice, regardless of the situation. In contrast, games with two Nash equilibria (orange, blue,\nor green grids) feature two stable strategy combinations.\nAdditionally, from a symmetry perspective, two sister games that are symmetric across the counter-\ndiagonal belong to the same category and have identical Nash equilibria.\n15\nTMGBENCH\nC\nMORE INFORMATION ABOUT OUR TMGBENCH\nC.1\nGENERATION PIPELINE\nIn our study, we design an efficient dataset generation pipeline that leverages GPT-4o as the core\nto produce the entire dataset, with rigorous human quality reviews incorporated. The pipeline is\norganized into three carefully designed stages:\nClassic Game Construction. Based on the topology of 2×2 games, we first introduce game de-\nscriptions for the payoff matrices of 144 game types, resulting in 144 classic games. An example of\na classic game is shown below, which mirrors the structure of the Prisoner’s Dilemma. These 144\nclassic games will serve as seed games, with their inherent game structures generalized into more\ndiverse, story-based games.\nExample of classic game: classic\/111\n[Scenario]\nPlayer A and Player B are playing a game. Either of them has two choices, namely A1,\nA2\/B1, B2. The payoff matrix of their different choice combinations is given below (larger\nnumber means higher payoff):\n| A \\ B | B1\n| B2\n|\n|-------|-------|-------|\n| A1\n| 1 \\ 4 | 3 \\ 3 |\n| A2\n| 2 \\ 2 | 4 \\ 1 |\nBoth Player A and Player B are targeting maximizing their own payoff.\n[\/Scenario]\nStory-based Game Generation. The aforementioned classic games offer a highly condensed math-\nematical representation of diverse game scenarios. However, in the real world, games often occur in\ncomplex social contexts involving various themes. To capture this complexity, we further designed\nstory-based games, incorporating richer entities and more intricate game scenarios.\nSpecifically, we used synthetic data generation techniques and crafted detailed prompts to set the\nconstruction constraints for generating high-quality story-based games. Additionally, to enhance\nthe realism of our game scenarios, we manually defined several thematic categories to guide the\ndata synthesis process (see §C.3).\nBoth the prompt constraints and thematic categories ensure\nthe generated content aligns with the intended structure and thematic elements. An example of a\ngenerated story-based game is shown below, which follows the same game structure as the Pris-\noner’s Dilemma and is presented within a new narrative context. As such, the story-based game\nstory-based\/111 0 serves as a counterpart to the classic game classic\/111. For each\nclassic game, we generate five corresponding story-based games. The data synthesis prompt is as\nfollows. The red text are the placeholders for the variables of the generation code, where ”domain”\nindicates the topic we random-choose for the task, and ”matrix str” indicates the payoff matrix de-\nrived from the game structure we enumerate.\n16\nTMGBENCH\nStory-based Game Generation Prompt\nPlease generate a game theory short story with the following requirements:\n- Specific topic: {domain}\n- There are two characters who may be in a situation of ”cooperation” or ”competition”;\n- Each character has 2 choices, and the combinations of their choices form 4 different sce-\nnarios;\n- In these 4 scenarios, the two characters face different benefits\/losses, which can be ab-\nstracted as different rewards they can obtain or different states they can achieve in each\nscenario;\n- They each have a preference relationship for these rewards\/states. We use numbers to\nrepresent the degree of preference, with 4 representing the most preferred and 1 the least\npreferred (i.e., preference degree 4>3>2>1);\n- The payoff matrices for both characters can be abstracted and represented in one matrix,\nwhere A and B represent two characters and their choices are A1, A2\/B1, B2. The respon-\ndent matrix is shown as below (the story you generate should have the same payoff structure\nas it):\n{matrix str}\nNow please design a story that includes:\n- Characters\n- Each character’s choices\n- Characters’ preferences for different scenarios\n- Story description\nResponse format:\n[Characters]\n{\n“A”: “...”, “B”: “...”\n}\n[\/Characters]\n[Choices]\n{\n“A1”: “...”, “A2”: “...”, “B1”: “...”, “B2”: “...”\n}\n[\/Choices]\n[Preferences]\nCharacters’ preferences for different scenarios (4 most preferred, 1 least preferred):\n{\n“A”: {\n4: “...”, 3: “...”, 2: “...”, 1: “...”\n},\n“B”: {\n4: “...”, 3: “...”, 2: “...”, 1: “...”\n}\n}\n[\/Preferences]\n[Payoff Matrix]\n...\n[\/Payoff Matrix]\n[Scenario]\n...(to detailedly describe the situation, including the information of characters, choices, pref-\nerences and payoffs on different choice combinations)\n[\/Scenario]\n17\nTMGBENCH\nExample of story-based task: story-based\/111 0.txt\n[Characters]\n{\n“A”: “Sarah, a small coffee shop owner”,\n“B”: “Mark, owner of a large coffee chain”\n}\n[\/Characters]\n[Choices]\n{\n“A1”: “Keep prices steady”,\n“A2”: “Lower prices”,\n“B1”: “Maintain current operations”,\n“B2”: “Expand aggressively in Sarah’s area”\n}\n[\/Choices]\n[Preferences]\nCharacters’ preferences for different scenarios (4 most preferred, 1 least preferred):\n{\n“A”: {\n4: “Lower prices while Mark expands (survive and gain market share)”,\n3: “Keep prices steady while Mark maintains operations (stable coexistence)”,\n2: “Lower prices while Mark maintains operations (unnecessary price war)”,\n1: “Keep prices steady while Mark expands (lose customers)”\n},\n“B”: {\n4: “Maintain operations while Sarah keeps prices steady (maximize profits)”,\n3: “Expand while Sarah keeps prices steady (gain market share)”,\n2: “Maintain operations while Sarah lowers prices (status quo)”,\n1: “Expand while Sarah lowers prices (costly competition)”\n}\n}\n[\/Preferences]\n[Payoff Matrix]\n| A \\ B | B1\n| B2\n|\n|-------|-------|-------|\n| A1\n| 1 \\ 4 | 3 \\ 3 |\n| A2\n| 2 \\ 2 | 4 \\ 1 |\n[\/Payoff Matrix]\n[Scenario]\nIn a bustling city, Sarah runs a cozy, independent coffee shop that has been a local favorite\nfor years.\nRecently, Mark’s large coffee chain has been eyeing expansion into Sarah’s neighborhood,\nthreatening her business. Sarah faces a critical decision: she can either keep her prices steady\n(A1) or lower them (A2) to attract more customers. Meanwhile, Mark must decide whether\nto maintain his current operations (B1) or aggressively expand into Sarah’s area (B2). If\nSarah keeps her prices steady and Mark maintains his current operations (A1, B1), Sarah\nstruggles to compete (1) while Mark enjoys maximum profits (4). If Sarah lowers her prices\nand Mark stays put (A2, B1), both experience moderate success (2, 2) as Sarah attracts some\nnew customers without directly competing with Mark. Should Mark decide to expand while\nSarah keeps prices steady (A1, B2), both would face challenges but could coexist (3, 3) as\nSarah retains loyal customers and Mark gains new ones. However, if Sarah lowers her prices\nas Mark expands (A2, B2), Sarah might survive and even gain market share (4), but Mark\nwould face costly competition (1).\nTheir decisions will shape the local coffee market and determine the fate of Sarah’s beloved\nshop.\n[\/Scenario]\n18\nTMGBENCH\nQuality Verification. To ensure coherence and internal consistency in the generated games, we\nimplement a multi-step generation strategy, incorporating meticulous human review. First, GPT-4o\ngenerates an initial draft of the story, which is then reviewed by a human for any inconsistencies or\nlogical flaws. If the draft fails this review, GPT-4o is prompted to identify the problematic sections\nand apply a self-correction mechanism.\nDuring the self-correction phase, GPT-4o analyzes the story for inconsistencies and revises the\nflawed sections. The revised version undergoes another round of human review. This iterative\nrefinement process continues until the story meets the required quality standards.\nIf, after several rounds of regeneration, the story still contains significant issues or fails to meet the\ncriteria, we may reject the output entirely. In such cases, the process is restarted from scratch with a\nnew draft to ensure a fresh approach and to avoid perpetuating prior errors.\nC.2\nREASONING PROMPT USED\nIn this section, we present the prompts used by various reasoning methods. Notably, when invoking\no1-mini to give response, we only use DA prompting, since the model are reported to perform\nreasoning internally and user should avoid‘ prompting like chain-of-thought.\nDA prompting\nSystem: You are a spectator, and you should answer question based on given senario.\nUser:\n{task description} {task question}\nOnly give a block of python-style code containing your answer without any process. e.g.\n“‘python\nanswer = [(“Ax”, “By”)] # list-type\n”’\nCoT prompting\nSystem: You are a spectator, and you should answer question based on given senario.\nUser:\n{task description} {task question}\nThink step by step, and finally give a block of python-style code containing your answer.\ne.g.\n“‘python\nanswer = [(“Ax”, “By”)] # list-type\n”’\nFoToM prompting\nSystem: You are a spectator, and you should answer question based on given senario.\nUser:\n{task description} {task question}\nFrom A’s perspective, try to figure out B’s action and make choice. Then from B’s perspec-\ntive try to figure out A’s action and make choice. Finally as a spectator, give a block of\npython-style code containing your answer. e.g.\n“‘python\nanswer = [(“Ax”, “By”)] # list-type\n”’\n19\nTMGBENCH\nSoToM prompting\nSystem: You are a spectator, and you should answer question based on given senario.\nUser:\n{task description} {task question}\nFrom A’s perspective, try to figure out B’s action, note that he may also reason based on\nyour information or reasoning. Then from B’s perspective try to figure out A’s action, note\nthat he may also reason based on your information or reasoning. Finally as a spectator, give\na block of python-style code containing your answer. e.g.\n“‘python\nanswer = [(“Ax”, “By”)] # list-type\n”’\nreFoToM prompting\nSystem: You are a spectator, and you should answer question based on given senario.\nUser:\n{task description} {task question}\nFrom B’s perspective, try to figure out A’s action and make choice. Then from A’s perspec-\ntive try to figure out B’s action and make choice. Finally as a spectator, give a block of\npython-style code containing your answer. e.g.\n“‘python\nanswer = [(“Ax”, “By”)] # list-type\n”’\nreSoToM prompting\nSystem: You are a spectator, and you should answer question based on given senario.\nUser:\n{task description} {task question}\nFrom B’s perspective, try to figure out A’s action, note that he may also reason based on\nyour information or reasoning. Then from A’s perspective try to figure out B’s action, note\nthat he may also reason based on your information or reasoning. Finally as a spectator, give\na block of python-style code containing your answer. e.g.\n“‘python\nanswer = [(“Ax”, “By”)] # list-type\n”’\nC.3\nBENCHMARK DIVERSITY\nOur dataset is characterized by the diverse contexts encapsulated within the story-based tasks, a\ndiversity that manifests across several dimensions.\nFirstly, we have identified 20 distinct topics derived from everyday life scenarios where coopera-\ntion and competition are likely to occur. These topics align with situations commonly depicted in\nvarious game families. The distribution of story-based games across these 20 topics is visualized in\nFigure 11a.\nThe topics encompass a broad spectrum of fields, including Business, Ecology, Sports, Technology,\nHealth Care, Politics, and more. Notably, Business constitutes the largest proportion of the dataset at\n11.1%, while the remaining topics are more evenly distributed, with percentages generally ranging\nfrom approximately 1.4% to 7.9%.\nGiven the nature of these long-text reasoning tasks, the scenarios within our story-based games\ntypically range from 200 to 450 words in length. As illustrated in Figure 11b, over 90% of scenario\nlengths fall within the 250 to 400-word interval. Additionally, we provide a scatter plot of scenario\nlengths by topic to further demonstrate the diversity of our generated dataset.\n20\nTMGBENCH\n11.1%\n7.9%\n7.4%\n6.7%\n6.1%\n6.0%\n5.4%\n5.3%\n5.0%\n4.9%\n4.7%\n4.7%\n4.5%\n4.0%\n3.9%\n3.6%\n3.2%\n2.6%\n1.5%\n1.4%\nBusiness\nEcology\nSports\nArt\nTechnology\nSociology\nHealth Care\nPolitics\nMilitary Strategy\nTransportation\nEducation\nPsychology\nEngineering\nSpace Exploration\nInternational Relations\nBiology\nLaw\nEmployment\nInterpersonal Interaction\nEconomics\n(a) The topic distribution of story-based games.\n200\n250\n300\n350\n400\n450\n0.00\n0.25\n0.50\n0.75\n1.00\nCumulative Ratio\n200\n250\n300\n350\n400\n450\nTask Length (`Scenario` Part)\nArt\nBiology\nBusiness\nEcology\nEconomics\nEducation\nEmployment\nEngineering\nHealth Care\nInternational Relations\nInterpersonal Interaction\nLaw\nMilitary Strategy\nPolitics\nPsychology\nSociology\nSpace Exploration\nSports\nTechnology\nTransportation\nCategory\n(b) Cumulative distribution of lengths by ratio and scatter plot of lengths by topic.\nFigure 11: Statistical distribution of story-based games over 20 topics.\n21\nTMGBENCH\nreFoToM\nreSoToM\ngpt-4o-mini\ngpt-4o\nFigure 12: Inconsistency heat map of GPT series models using reFoToM and reSoToM prompting.\nTable 4: The significance degree of top-tier GPT models performance. The larger value indicates the\nhigher significance of the peculiar answering pattern. Near-zero value means no particular pattern.\nAll values are expressed as percentages.\nModel\nCoT\nFoToM\nReFoToM\nSoToM\nReSoToM\ngpt-4o\n13.89\n9.38\n8.33\n4.51\n6.25\ngpt-4o-mini\n5.56\n26.74\n20.49\n32.64\n35.42\nD\nADDITIONAL RESULTS\nD.1\nASYMMETRIC INCONSISTENCY PATTERN\nWe show in Figure 12 that GPT series models still display similar pattern when using reFoToM and\nreSoToM prompting. Yellow-box areas and green-box areas display an asymmetric inconsistency\npattern.\nIn order to further quantify how significant does the results display such pattern, we also propose a\nmetric named significance degree which confined in [0, 1] and it is defined as\nSD = 1\n18\nX\ni,j\nI{#NE(i, j) = 0} · 1\n4(Freqi,j,1 + Freqi,j,4 −Freqi,j,2 −Freqi,j,3) ∗S(i, j)\nwhere #NE(i, j) represents the number of Nash equilibria of the tasks of grid (i, j), and S(i, j) is\ndecided by the area that the grid (i, j) belongs to, having value 1 given the grid in the green area and\nvalue -1 given the grid in the yellow area.\nWe present the statistical results of LLMs’ performance in Table 4, which show that the SD values\nfor using ReFoToM are similar to those for FoToM, and the values for ReSoToM are close to those\nfor SoToM.\nAdditionally, the results indicate that employing ToM helps gpt-4o reduce the asymmetric incon-\nsistency pattern, while it conversely makes gpt-4o-mini more ’stubborn’ in maintaining this pattern.\nFurthermore, higher-order ToM appears to have a stronger effect than first-order ToM.\n22\nTMGBENCH\nD.2\nCASES OF FAILURE\nIn this section, we demonstrate two examples of the real response from gpt-4o-mini, which derived\nfrom a pair of symmetric data points (the grid (row 3, column 4) and the grid (row 9, column 10))\nthat theoretically have the same answer and similar reasoning pattern. We use the red coloured texts\nto indicate the mistaken parts of the reasoning processes.\nrow 3 column 4\nTo determine the most likely choice combinations for Players A and B, we can analyze the\npayoff matrix step by step.\n1. **Identify the Payoffs**: The payoff matrix is as follows:\n| A \\ B | B1\n| B2\n|\n|-------|-------|-------|\n| A1\n| 3 \\ 1 | 1 \\ 4 |\n| A2\n| 2 \\ 3 | 4 \\ 2 |\n2. **Player A’s Choices**:\n- If Player B chooses B1:\n- A1 gives Player A a payoff of 3.\n- A2 gives Player A a payoff of 2.\n- If Player B chooses B2:\n- A1 gives Player A a payoff of 1.\n- A2 gives Player A a payoff of 4.\nFrom Player A’s perspective:\n- Against B1, A1 (3) is better than A2 (2).\n- Against B2, A2 (4) is better than A1 (1).\n3. **Player B’s Choices**:\n- If Player A chooses A1:\n- B1 gives Player B a payoff of 1.\n- B2 gives Player B a payoff of 4.\n- If Player A chooses A2:\n- B1 gives Player B a payoff of 3.\n- B2 gives Player B a payoff of 2.\nFrom Player B’s perspective:\n- Against A1, B2 (4) is better than B1 (1).\n- Against A2, B1 (3) is better than B2 (2).\n4. **Best Responses**:\n- If Player A chooses A1, Player B will choose B2 (since 4 >1).\n- If Player A chooses A2, Player B will choose B1 (since 3 >2).\n- If Player B chooses B1, Player A will choose A1 (since 3 >2).\n- If Player B chooses B2, Player A will choose A2 (since 4 >1).\n5. **Nash Equilibria**:\n- The combination (A1, B2) gives payoffs (3, 4).\n- The combination (A2, B1) gives payoffs (2, 3).\n6. **Conclusion**:\n- The most likely choice combinations are (A1, B2) and (A2, B1). Since both combina-\ntions are viable and lead to different outcomes, we conclude that both are possible.\nThus, the final answer in the required format is:\n“‘python\nanswer = [(“A1”, “B2”), (“A2”, “B1”)] # list-type\n”’\n23\nTMGBENCH\nrow 9 column 10\nTo determine the most likely choice combinations for Players A and B, we can analyze the\npayoff matrix step by step.\n1. **Identify the Payoffs**:\n- For Player A:\n- If A chooses A1:\n- Against B1: Payoff = 2\n- Against B2: Payoff = 4\n- If A chooses A2:\n- Against B1: Payoff = 3\n- Against B2: Payoff = 1\n- For Player B:\n- If B chooses B1:\n- Against A1: Payoff = 4\n- Against A2: Payoff = 2\n- If B chooses B2:\n- Against A1: Payoff = 1\n- Against A2: Payoff = 3\n2. **Best Responses**:\n- If Player B chooses B1:\n- Player A’s best response is A2 (3 >2).\n- If Player B chooses B2:\n- Player A’s best response is A1 (4 >1).\n- If Player A chooses A1:\n- Player B’s best response is B1 (4 >1).\n- If Player A chooses A2:\n- Player B’s best response is B2 (3 >2).\n3. **Nash Equilibria**:\n- A Nash Equilibrium occurs when both players are choosing their best responses to each\nother.\n- From the analysis:\n- (A1, B1) is a Nash Equilibrium because A’s best response to B1 is A2, but B’s best\nresponse to A1 is B1.\n- (A2, B2) is also a Nash Equilibrium because A’s best response to B2 is A1, but B’s\nbest response to A2 is B2.\n4. **Conclusion**:\n- The most likely choice combinations are (A1, B1) and (A2, B2) since both are Nash\nEquilibria.\nNow, we can represent this in the required Python-style code:\n“‘python\nanswer = [(“A1”, “B1”), (“A2”, “B2”)] # list-type\n”’\n24\nTMGBENCH\nD.3\nNESTED FORM GAME\nIn this section, we demonstrate the template we use for generating nested form games. The text in\nred are the placeholders for the variables of the generation codes.\ntemplate of a kind of nested form game\n<Pre-Game >\n{pre game}\n<Core-Game >\n{core game}\n[Question]\nPlayer A and B are facing the two games, the pre-game and the core-game.\nNote that their final goal is to maximize own payoff first in the core Game, then in the\npre-game.\nAdditionally, {restricted player} is attached with an restriction that if the situation of the\npre-game is {restricted situation}, then he can not choose action {restricted choice}.\nWhat is\/are the most possible choice combination(s) of the pre-game ultimately? (when all\nchoice combinations have equal possibility, the answer should contain nothing)\n[\/Question]\nAfter a nested form game is generated through our template, we still need to check if the Nash\nequilibria of the pre-game changes after the restriction from the core game. If the set of Nash\nequilibria does change, then we use this as a piece of data to evaluate LLMs, observing if they can\nobserve such a violation of original NEs’ structure.\nE\nLIMITATIONS AND FUTURE WORKS\nWhile TMGBENCH provides valuable insights into LLMs’ strategic reasoning abilities, it also high-\nlights several limitations that open avenues for future research. One key limitation lies in the bench-\nmark’s narrow scope, as it focuses on specific game scenarios and does not encompass the full\ndiversity of game theory. LLMs may still struggle with more complex, multi-agent, or dynamic\ngames that require higher-dimensional reasoning and multi-step planning. Additionally, the models\ndemonstrate inconsistencies when dealing with slightly varied inputs, indicating a lack of robustness\nthat can hinder their effectiveness in real-world applications.\nAnother critical issue is that LLMs perform relatively well with first-order ToM tasks, but they often\nfalter when engaging with second-order or higher-order ToM, limiting their ability to model nuanced\nopponent strategies. Moreover, the reliance on carefully crafted scenarios means that the models’\nperformance may not generalize well to other environments beyond the benchmark. Addressing\nthese challenges will require expanding the range of games to include dynamic settings, multi-agent\nscenarios, and games with incomplete information.\nFuture research should also explore ways to enhance models’ ToM capabilities, particularly at higher\nlevels. Improving robustness through techniques like adversarial training or causal inference could\nhelp achieve more stable performance across varied inputs. In addition, evaluating the transferability\nof LLMs’ strategic abilities across other domains—such as economics, sociology, and politics—will\nbe essential to assess their broader utility. Finally, as game complexity increases, developing more\nefficient model architectures and training methods will be crucial to balancing performance with\ncomputational efficiency, ensuring these models remain practical for large-scale applications.\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs.pdf"}
{"title":"Game4Loc: A UAV Geo-Localization Benchmark from Game Data","authors":"Yuxiang Ji, Boyong He, Zhuoyue Tan, Liaoni Wu","summary":"The vision-based geo-localization technology for UAV, serving as a secondary\nsource of GPS information in addition to the global navigation satellite\nsystems (GNSS), can still operate independently in the GPS-denied environment.\nRecent deep learning based methods attribute this as the task of image matching\nand retrieval. By retrieving drone-view images in geo-tagged satellite image\ndatabase, approximate localization information can be obtained. However, due to\nhigh costs and privacy concerns, it is usually difficult to obtain large\nquantities of drone-view images from a continuous area. Existing drone-view\ndatasets are mostly composed of small-scale aerial photography with a strong\nassumption that there exists a perfect one-to-one aligned reference image for\nany query, leaving a significant gap from the practical localization scenario.\nIn this work, we construct a large-range contiguous area UAV geo-localization\ndataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,\nand targets using modern computer games. Based on this dataset, we introduce a\nmore practical UAV geo-localization task including partial matches of\ncross-view paired data, and expand the image-level retrieval to the actual\nlocalization in terms of distance (meters). For the construction of drone-view\nand satellite-view pairs, we adopt a weight-based contrastive learning\napproach, which allows for effective learning while avoiding additional\npost-processing matching steps. Experiments demonstrate the effectiveness of\nour data and training method for UAV geo-localization, as well as the\ngeneralization capabilities to real-world scenarios.","url":"http:\/\/arxiv.org\/abs\/2409.16925v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2409.16925v2","published":1727271208000,"comment":"AAAI 2025, Project page: https:\/\/yux1angji.github.io\/game4loc\/","pdf_text":"Game4Loc: A UAV Geo-Localization Benchmark from Game Data\nYuxiang Ji1*, Boyong He1*, Zhuoyue Tan1, Liaoni Wu1,2†\n1Institute of Artifcial Intelligence, Xiamen University\n2School of Aerospace Engineering, Xiamen University\nyuxiangji@stu.xmu.edu.cn, wuliaoni@xmu.edu.cn\nProject Page: https:\/\/yux1angji.github.io\/game4loc\nAbstract\nThe vision-based geo-localization technology for UAV, serv-\ning as a secondary source of GPS information in addition\nto the global navigation satellite systems (GNSS), can still\noperate independently in the GPS-denied environment. Re-\ncent deep learning based methods attribute this as the task of\nimage matching and retrieval. By retrieving drone-view im-\nages in geo-tagged satellite image database, approximate lo-\ncalization information can be obtained. However, due to high\ncosts and privacy concerns, it is usually difficult to obtain\nlarge quantities of drone-view images from a continuous area.\nExisting drone-view datasets are mostly composed of small-\nscale aerial photography with a strong assumption that there\nexists a perfect one-to-one aligned reference image for any\nquery, leaving a significant gap from the practical localiza-\ntion scenario. In this work, we construct a large-range con-\ntiguous area UAV geo-localization dataset named GTA-UAV,\nfeaturing multiple flight altitudes, attitudes, scenes, and tar-\ngets using modern computer games. Based on this dataset,\nwe introduce a more practical UAV geo-localization task in-\ncluding partial matches of cross-view paired data, and ex-\npand the image-level retrieval to the actual localization in\nterms of distance (meters). For the construction of drone-view\nand satellite-view pairs, we adopt a weight-based contrastive\nlearning approach, which allows for effective learning while\navoiding additional post-processing matching steps. Experi-\nments demonstrate the effectiveness of our data and training\nmethod for UAV geo-localization, as well as the generaliza-\ntion capabilities to real-world scenarios.\nIntroduction\nVision-based UAV geo-localization, as an independent on-\nboard technology that can work independently of communi-\ncation systems, enables UAVs to autonomously obtain GPS\ninformation even when GNSS communication fails. This\nUAV visual localization task could be refered as a special\ncase of cross-view geo-localization (Deuser, Habel, and Os-\nwald 2023; Zheng, Wei, and Yang 2020; Hu et al. 2018)\nand visual place recognition (Arandjelovic et al. 2016). Re-\ncent research formulates this as a cross-view image re-\ntrieval problem (Lin et al. 2022; Dai et al. 2023). Given a\n*These authors contributed equally.\n†Corresponding Author.\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nDrone-view\nAligned Satellite-view\nStrong Assumption\nArbitrary Satellite-view\nReal-world Scenario\nPerfect One-to-One Matching\nPartial N-to-N Matching\n1. Retrieval\n2. Localization\nArea to be located\nReal-world Scenario\nFigure 1: Comparision between perfect matching pair and\npartial matching pair.\ndrone-view image, the goal is to retrieve a matching scene\nfrom a database of GPS-tagged satellite-view images to in-\nfer the current GPS information of the UAV. Compared to\ntraditional hand-crafted feature extraction algorithms, deep\nlearning based methods achieve higher accuracy and bet-\nter generalization performance (Tian, Chen, and Shah 2017;\nDusmanu et al. 2019). However, such superiority is built\nupon the training on a large amount of paired images from\ndrone-view and satellite-view.\nExisting cross-view datasets are mostly composed of im-\nage pairs from different platform views, e.g., ground cam-\neras and satellites (Workman, Souvenir, and Jacobs 2015;\nZhai et al. 2017; Liu and Li 2019). The datasets for UAV\nlocalization follow this paradigm and expand the view to\ndrones (Zheng, Wei, and Yang 2020; Xu et al. 2024; Zhu\net al. 2023a; Dai et al. 2023). Due to high costs and pri-\nvacy concerns, most of these data are obtained through\nGoogle Earth Engine simulation, and the remaining real-\nworld data are very limited in terms of scale, height, angle,\netc. More critically, these datasets simply assume that each\nquery drone-view image has a perfectly one-to-one ailgned\nmatching satellite-view image as a reference, which does not\narXiv:2409.16925v2  [cs.CV]  12 Dec 2024\napply to practical scenarios because it is impossible to ob-\ntain an arbitrary view of drone in advance and align it with a\nsatellite-view reference. Consequently, such perfect matches\nare very unlikely to exist in practical scenarios; instead, it is\nmore common to encounter partial matching pairs between\ndrone-view and satellite-view as shown in Fig. 1. This leads\nto models trained on such paradigm datasets struggling to\nhandle practical UAV visual localization tasks.\nIn fact, some works already noticed the above problems,\nand attempted to address it from both task desgin and data\nconstruction perspectives. VIGOR (Zhu, Yang, and Chen\n2021) introduces a beyond ont-to-one matching task for\nground-satellite matching. DenseUAV (Dai et al. 2023) and\nUAV-VisLoc (Xu et al. 2024) are two continuous range real-\nworld drone-satellite paired datasets. Both of them expand\nthe retrieval task to localization; however, the former data\nconstruction method still does not align with practical sce-\nnarios, and the latter lacks a definition of data pair con-\nstruction and task design. Additionally, these real-world data\nare limited in terms of scenes, camera angles, and flight\naltitudes\/attitudes, which restricts its generalization perfor-\nmance in diverse scenarios.\nIn light of the above problems, we propose aligning di-\nrectly with practical tasks at the data construction level by\nexpanding the original perfect matching to encompass par-\ntial matching as Fig. 1. Under our setting, the drone-satellite\npairs are consturcted following the real-world scenarios,\nwhere drone-view images are retrieved from a gallery of\nsatellite-view images containing partial matches. By con-\nsturcting such retrieval task, we can recreate the real-world\nUAV visual geo-localization scenarios from the task design\nand evaluate the localization performance based on the re-\ntrieval results. Based on this, to replicate various drone flight\nconditions, we utilize commercial video games to simulate\nand collect a contiguous large-range of drone-satellite image\npairs dataset GTA-UAV from multiple flight alittudes\/atti-\ntudes, and various flight scenarios. In total, 33,763 drone-\nview images are collected from the entire game map, en-\ncompassing various scenes such as urban, mountain, desert,\nforest, field, and coast.\nIn conjunction with this data consturction method, we in-\ntroduce a weighted contrastive learning approach weighted-\nInfoNCE, to utilize the intersection ratio of the partially\nmatched data areas as weight labels for contrastive learn-\ning between the paired data. Exeperiments demonstrate that\nthrough this training method, the network can reduce the\nembedding distance of partially matched samples from dif-\nferent views, making retrieval and localization available.\nOur contribution can be summarized as following:\n• We introduce a new benchmark and dataset for the prob-\nlem of UAV geo-localization. This dataset, for the first\ntime, expands the perfect matching UAV geo-localization\ntask to include partial matches, allowing for a more real-\nistic task.\n• We propose a weighted contrastive learning method\nweighted-InfoNCE to enable the model to learn this par-\ntial matching paradigm.\n• We validate the effectiveness of proposed dataset and\nmethod, and demonstrate their potential and generaliza-\ntion capabilities in real-world tasks using a small amount\nof available real data.\nRelated Work\nCross-view Geo-Localization Datasets\nDue to the comprehensive coverage of high-altitude refer-\nence data such as satellite and aerial imagery, most stud-\nies use GPS-tagged satellite imagery as the reference view\nfor cross-view geolocalization. Among them, many datasets\nfocus on the cross-view matching between ground-level\nand satellite-view (Lin, Belongie, and Hays 2013; Tian,\nChen, and Shah 2017; Liu and Li 2019; Zhai et al. 2017;\nZhu, Yang, and Chen 2021). Specifically, VIGOR (Zhu,\nYang, and Chen 2021) doubts the perfect one-to-one match-\ning data pairs and introduces the concept of beyond one-\nto-one retrieval in ground-satellite matching. University-\n1652 (Zheng, Wei, and Yang 2020) frist introduces the\ndrone-view into the cross-view datasets, where each drone-\nsatellite pair focuses on a target university building. Al-\nthough the drone’s perspective can serve as a retrieval tar-\nget, the task still not achieve geolocalization. In following\nworks, DenseUAV (Dai et al. 2023) and SUES-200 (Zhu\net al. 2023a) change discrete sampling into continuous sam-\npling and consider different altitudes. Constrained by flight\ncosts and the limitations of Google Earth simulation, the va-\nriety of shooting angles and altitudes reamins very limited.\nMost importantly, these datasets construction methods still\nadhere to the one-to-one perfect matching paradigm and do\nnot align with practical scenarios. UAV-VisLoc (Xu et al.\n2024) is a recently released real high-altitude drone dataset\nwhere each drone-view image is geotagged, while no clear\ntask desgin has been defined for this data yet.\nCross-view Geo-Localization Methods\nOne of the first deep learning based geolocalization works\nby Workman et al. (Workman, Souvenir, and Jacobs 2015)\ndemonstrates the superior accuracy and generalization of\nCNNs compared to traditional hand-crafted features. They\nsimply utilize a L2 Loss to minimize the feature distance\nbetween cross-views and perform retrieval based on feature\ndistances. Some works (Lin et al. 2015; Vo and Hays 2016;\nArandjelovic et al. 2016) adopt the idea of contrastive learn-\ning, reducing the distance between positive sample pairs.\nYang et al. and Zhu et al. (Yang, Lu, and Zhu 2021; Zhu,\nShah, and Chen 2022) explore the Transformer architecture\nin geolocalization to extract additional geometric proper-\nties. Specifically, Chen et al. (Chen et al. 2024) proposes re-\nsearch on the unaligned case, i.e., the non-centered or shift-\ning targets. However, their experiments are still conducted\non aligned datasets. Sample4Geo (Deuser, Habel, and Os-\nwald 2023) adopts the recent pre-training approach used in\nvision-language work CLIP (Radford et al. 2021), applying\nlarge batch size contrastive learning to cross-view data. They\nenhance the learning effect by constructing numerous hard\nnegatives based on InfoNCE (van den Oord, Li, and Vinyals\n2019).\nTable 1: Comparison between the proposed GTA-UAV dataset and existing datasets for UAV visual geo-localization.\nUniversity\nSUES-200\nDenseUAV\nUAV-VisLoc\nGTA-UAV (proposed)\nDrone images\n37,854\n24,210\n18,198\n6,742\n33,763\nDrone-view GPS locations\nAligned\nAligned\nAligned\n-\nArbitrary\nAltitude range\n∼50m\n150m ∼300m\n80m ∼100m\n400m ∼840m\n80m ∼650m\nContiguous area\n×\n×\n✓\n✓\n✓\nEvaluation in terms of meters\n×\n×\n×\n✓\n✓\nMultiple attitudes\n✓\n×\n×\n×\n✓\nMultiple scenes\n×\n×\n×\n✓\n✓\nMultiple scales satellite images\n×\n×\n×\n-\n✓\n…\nDrone-view\nPositive partial matching satellite-view\nSemi-positive partial matching satellite-view\n𝐼𝑂𝑈∈[0.39, 1.0]\n𝐼𝑂𝑈∈[0.14, 0.39]\nMeta Data\nGPS Info\nCamera Pose\nFOV Angle\n𝑝!\n𝑝\"\n𝑝#\n𝑝$\n𝑝%\nℎ, (𝜙, 𝜓, 𝜃)\n𝐹𝑂𝑉&'()*\nFigure 2: The paired data construction process of GTA-UAV, where Positive and Semi-positive satellite-view are paired with\nDrone-view by IOU.\nGTA-UAV Dataset\nProblem Statement\nGiven a filed of view (FOV) captured by the UAVs, our tar-\nget is to construct a GPS-tagged reference satellite-view im-\nages set from a contiguous area and localize the drone by\nfinding a matching field within it. Due to the varying flight\naltitudes and attitudes of UAVs, the FOV can cover multi-\nple scales of the ground area. To accommodate the vary-\ning scales of drone-view, we divide the reference satellite-\nview of the entire coverage area into multiple hierachical\ntiles, where the ground resolution between different levels\ndiffering by a factor of two. Unlike the aligned one-to-one\nretrieval strong assumption of existing datasets in Tab. 1,\nwe do not center-align the drone-satellite pairs. Instead, we\nuse a collect-then-match approach, pairing them by calcu-\nlating the overlapping of the ground area covered by the\ntwo views. In such arbitrarily sampling way, the relation-\nship between pairs changes from perfectly aligned matching\nto partial matching. Refer to the definition of positive sam-\nples in VIGOR (Zhu, Yang, and Chen 2021), we attribute\nsamples with a ground area intersection over union (IOU)\ngreater than 0.39 as a positive pair, and IOU greater than\n0.14 as a semi-positive pair. The positive pairs are consid-\nered as ground truth for retrieval for their highest match,\nwhile semi-positive pairs are complementary to the paritial\nmatching learning. Such paritial matching, in contrast to the\nstrong assumption of perfect matching, can be considered\na more challenging retrieval task. On the basis of coarse re-\ntrieval, since each of our view data points is GPS-tagged, we\ncan also evaluate the retrieval results at the distance level.\nThis provides a foundation for fine localization in further re-\nsearch. Comparing to the existing datasets for UAV visual\ngeo-localization as Tab. 1, our proposed GTA-UAV dataset\noffers higher flexibility and can cover a wider range of task\nscenrarios. We believe that our dataset complements existing\nUAV visual localization datasets and significantly bridge the\ngap between current research and practical applications.\nData Collection and Construction\nIn light of the existing works (Richter et al. 2016; Ros\net al. 2016; Kiefer, Ott, and Zell 2022) on synthetic data,\nwe utilize Grand Thef Auto V (GTAV) as a simulation\nplatform. We collect 33,763 drone-view images covering\ndistinctive areas in the whole game map, including urban,\nmountain, desert, forest, field, and coast. To cover various\nflight altitudes and attitudes of UAVs, we simulate multi-\nple flight heights ranging from 80m to 650m, and multi-\nple camera angle ranges for roll ϕ ∈[−10◦, 10◦], pitch\nθ ∈[−100◦, −80◦] and yaw ψ ∈[−180◦, 180◦]. The raw\ndrone-view images are captured in 1920 × 1440 with GPS\ntagged for meter-level evaluation. Based on the entire game\nmap’s area of 81.3km2, we utilize a staellite map with a\nground resolution of about 0.2m and divide it into a total\nof 8 hierarchical tiles. Each tile image has a pixel resolu-\ntion of 256 × 256, where the highest zoom level tiles hav-\ning a ground resolution of about 0.27m. We collect totaling\n14,640 tiles from zoom levels 4 to 7 as reference satellite-\nview set, to accommodate possible flight altitudes. For each\ndrone-view image, we record the GPS information, flight al-\nMutually Exclusive Sampling\nDrone-view\nPositive\/semi-positive pairs\nIOU\nShare weights\nFq1·Fr1\nFq1·Fr2\nFq1·Fr3\n…\nFq1·Frb\nFq2·Fr1\nFq2·Fr2\nFq2·Fr3\n…\nFq2·Frb\nFq3·Fr1\nFq3·Fr2\nFq3·Fr3\n…\nFq3·Frb\n…\n…\n…\n…\nFqb·Fr1\nFqb·Fr2\nFq1·Fr3\n…\nFqb·Frb\nQuery\nReference\nFq1\nFq2\nFq3\n…\nFqb\nFr1\nFr2\nFr3\n…\nFrb\nPositive weight by IOU\nSatellite-view\nTrain\nInference\nQuery \nImage\nReference\nSet\n(geo-tagged)\nQuery \nFeature\nOffline\nRetrieval\n𝒙𝒊, 𝒚𝒊𝑲\nViT\nViT\nViT\nViT\nFeature Gallery\nTop-K\nLocations\n…\nFigure 3: The overview of our training and inference pipeline. (left) We use ViT as feature encoder and weighted-InfoNCE\nfor training positive and semi-positive batched samples from mutually exclusive sampling. (right) Then the retrieval could be\nbased on discriminative features to achieve localization.\ntitude, flight attitude, and camera angle at the time of cap-\nture. By combining the FOV angle setting, we could approx-\nimate the ground area covered by the drone-view FOV. Then\nby enumerating the nearby satellite tiles from each level for\neach drone-veiw image, we set those with a ground coverage\nIOU greater than 0.39 as a positive drone-satellite pair, and\nthe IOU between 0.14 and 0.39 as a semi-positive drone-\nsatellite pair as shown in Fig. 2. The detailed construction\nprocess and dataset statistics are put in the supplementary.\nThe Evaluation Protocal\nBased on the existing works of geo-localization (Zhu, Yang,\nand Chen 2021; Dai et al. 2023; Zheng, Wei, and Yang\n2020), we utilize two retrieval-based metrics (Recall@K,\nAP) and one localization-related metric (SDM@K (Dai et al.\n2023)) for evaluation. In addition, we include distance er-\nror between the retrieval results and the query location as\nan evaluation method. Based on this, we introduce two ap-\nplication scenarios as the same in VIGOR (Zhu, Yang, and\nChen 2021): same area and cross area. The same area rep-\nresents the scenario where both the training and the testing\ndata pairs are sampled from the same area, reflecting ap-\nplications where the flight area data is available. The cross\narea represents the case that the training and testing data are\nseperated. Under this setting, we divide half of the game map\ninto training data and evaluate on the other half, and these\nareas differ on the scenes.\nGeo-localization via Cross-view Matching\nBaseline Framework\nLarge-scale UAV geo-localization necessitates a trade-off\nbetween accuracy and performance. Practical application\nscenarios demand that the pipeline avoids complex pre-\nprocessing and post-processing steps. We avoid introduc-\ning additional matching modules in the retrieval-based\nparadigm, allowing the reference statellite-view set to be\nprocessed offline and retrieval to be performed through sim-\nple distance similarity measures. Recent works typically use\na Siamese Network to encode cross-view images and train\na model for generating cross-view descriptors using Triplet\nloss or some variant of metric learning (Deuser, Habel, and\nOswald 2023; Vo and Hays 2016; Hu et al. 2018; Li et al.\n2023; Zhu, Shah, and Chen 2022). To simplify the entire\npipeline and align with the model structure of standard vi-\nsual tasks for simply comparing different data pre-training\neffects, we directly utilize a pair of weight-sharing original\nVision-Transformer (ViT) models (Dosovitskiy et al. 2021)\nwith default Multi-Layer Perceptron (MLP) head as the de-\nscriptor model, without introducing any additional fusion\nmodules. We follow the training approach using Symmet-\nric InfoNCE from Sample4Geo (Deuser, Habel, and Oswald\n2023) as the baseline, leveraging all available negatives in\nbatch learning.\nWeighted Positive Training\nDirectly utilizing the original Triplet loss or symmetric In-\nfoNCE loss allows the constructed paired data to be treated\nas positive samples and non-paired data as negative samples\nfor contrastive learning. This approach works well in one-to-\none perfect matching pairs. However, in our arbitrary partial\nmatching paired data, treating all degrees of partial match-\ning as equal-weight positive samples could introduce signif-\nicant bias, affecting the learning result and training stability.\nBased on our data consturction method, we utilize the IOU\nof ground area covered by cross-view pairs IOUqr+ as addi-\ntional supervision information for contrastive learning as:\nLweighted-InfoNCE(Fq, αq, FR) =\n−αq log\nexp(Fq · Fr+\/τ)\nPR\ni exp(Fq · Fri\/τ)\n−(1 −αq) 1\n|R|\nR\nX\ni\nlog\nexp(Fq · Fr+,−\ni\n\/τ)\nPR\nj exp(Fq · Frj\/τ)\n= αqLInfoNCE(Fq, FR) + (1 −αq)Luniform-InfoNCE(Fq, FR),\n(1)\nwhere Fq represents an encoded query image from one-view,\nFR represents the encoded reference images from another\nview in the same batch, and r+ represents positive\/semi-\npositive reference pair. The τ denotes a learnable parame-\nter (Radford et al. 2021). The weight coefficients αq are cal-\nculated by parametric Sigmoid as Eq. 2:\nαq = σ(k, IOUqr+) =\n1\n1 + exp(−k × IOUqr+),\n(2)\nwhere k is a hyper-parameter and higher value represents\ngreater curvature change. When k approaches infinity, the\nloss function degenerates into the standard InfoNCE. In a\nsingle batch of size N, there are N positive\/semi-positive\npaired samples with corresponding positive weights from\ntwo views, and the loss function as Eq. 1 would be calculated\ntwice symmetrically in two directions (drone to satellite,\nsatellite to drone). The dot-production is utilized as the sim-\nilarity measurement, where positive\/semi-positive samples\nare pushed towards higher values. Building on the original\nInfoNCE, we incorporate weights for positive\/semi-positive\nsample pairs into the loss function, introducing a degree of\nflexibility. This allows the model to adapt the similarity loss\nbased on the extend of partial matching.\nMutually Exclusive Sampling\nIn the training process based on symmetric InfoNCE in-\ntroduced in above sections, to establish the negative rela-\ntionship between sample pairs, we need to sample N pairs\nof mutually independent positive sample pairs within each\nbatch. Since there is no guaranteed one-to-one relationship\nbetween drone and satellite views in our arbitrary partial\nmatching data construction process, each view image could\nhave neighboring relationships with multiple cross-view im-\nages. In this situation, to adapt to the training pipeline, we\nutilize a mutually exclusive sampling method as Alg. 1. By\nconsidering each view image as a node in graph theory and\nthe matching relation as an undirected edge, for each batch,\nwe remove the sampled nodes and all their adajacent nodes.\nWe then continue sampling from the remaining graph set to\navoid having related cross-view data within the same batch.\nExperiments\nImplementation Details\nIn our exeperiments the ViT-Base (Dosovitskiy et al. 2021)\nwith patch-size 16 × 16 and 64M parameters is adopted\nas the image encoding architecture. Both drone-view im-\nages and satellite-view images are resized to 384 × 384\nbefore feeding into the network. The hyper-parameter k of\nweighted-InfoNCE is set to 5 as default, and the learnable\ntemperature parameter τ is initialized to 1. Following Sam-\nple4Geo (Deuser, Habel, and Oswald 2023), we employ\nAdam optimizer (Kingma and Ba 2017) with a initial learn-\ning rate of 0.0001 and a cosine learning rate scheduler to\ntrain each experiment for 20 epochs in batch size of 64. The\nflipping, rotation, and grid dropout are included as data aug-\nmentation for training. Both positive and semi-positive pairs\nare used for training by default if not specifically noted, and\nwe conduct experiments on this in the subsequent subsec-\ntions. The further details are put in the supplementary.\nAlgorithm 1: Mutually Exclusive Sampling process\nData: partial paired data\nE = {(q1, r1), (q2, r2), . . . , (qN, rN)}, batch\nsize b\nResult: exclusive batched data D = {{q, r}b, ...}\nInitialize D = ∅, Dbatch = ∅, Gstack = ∅, Gremain = E;\nfor i ←1 to N\/b do\nfor e ∈Gremain do\nqi, ri ←e;\nDbatch ←Dbatch ∪(qi, ri);\nfor qi, rj ←E[qi] do\nGremain ←Gremain \\ (qi, rj);\nGstack ←Gstack ∪(qi, rj);\nfor qj, ri ←E[ri] do\nGremain ←Gremain \\ (qj, ri);\nGstack ←Gstack ∪(qj, ri);\nif len(Dbatch) = b then\nD ←D ∪Dbatch;\nDbatch ←∅;\nGremain ←Gremain ∪Gstack;\nGstack ←∅;\nreturn D;\nEvaluation Metrics\nFor each drone-view query, the top-K images with the high-\nest cosine similarity in the feature embedding space from the\nsatellite-view database would be considered as the retrieval\nresults. Following the previous works (Deuser, Habel, and\nOswald 2023; Zheng, Wei, and Yang 2020; Zhu, Yang, and\nChen 2021), we first evaluate the retrieval task by Recall@K\n(R@K) and average precision (AP). We also include Spa-\ntial Distance Metric SDM@K (Dai et al. 2023) as the com-\nbined metric for retrieval and localization to further evaluate\nthe positioning performance, where the calculation method\nis provided in the supplementary. Considering the average\nnumber of references a query may match, we use SDM@3\nhere. More intuitively, we provide the distance between the\nlocation of the top-1 retrieval result and the location of the\ndrone-view query (Dis@1) as an evaluation metric.\nGTA-UAV Dataset Benchmark\nFor our GTA-UAV dataset, we compare the proposed\nmethod with previous SOTA training methods under both\ncross-area and same-area settings using positive + semi-\npositive and positive-only as training data respectively. As\nresults in Tab. 2, in the proposed paritial matching settings,\nour proposed weighted-InfoNCE achieves the best results\nacross all metrics. Specifically, comparing to the previous\nSOTA method (Deuser, Habel, and Oswald 2023) using In-\nfoNCE, our method improves the R@1 for 20.08%, and\nDis@1 for 234.36m in the cross-area setting trained on pos-\nitive + semi-positive data. The results trained on positive\n+ semi-positive data have less retrieval accuracy compar-\ning to the results only trained on positive data. This is be-\ncause that the retrieval evaluation considers only the pos-\nTable 2: Performance on GTA-UAV comparing to different training methods. MES means Mutual Exclusive Sampling.\nMethods\nCross-Area\nSame-Area\nR@1↑\nR@5↑\nAP↑\nSDM@3↑\nDis@1↓\nR@1↑\nR@5↑\nAP↑\nSDM@3↑\nDis@1↓\nPositive-only\nTriplet Loss (Ltriplet)\n43.41%\n66.70%\n53.56%\n61.26%\n756.95m\n68.22%\n87.99%\n76.73%\n79.17%\n438.38m\nInfoNCE Loss (LInfoNCE)\n49.57%\n72.84%\n59.68%\n65.53%\n612.22m\n72.99%\n90.64%\n80.76%\n80.40%\n363.67m\nInfoNCE Loss (LInfoNCE, w\/. MES)\n52.64%\n74.63%\n62.40%\n67.64%\n552.90m\n72.34%\n91.42%\n80.86%\n81.57%\n369.59m\nOurs (Lweighted-InfoNCE, w\/. MES)\n57.52%\n80.10%\n67.24%\n72.33%\n444.13m\n75.97%\n94.53%\n83.35%\n82.80%\n325.61m\nPositive + Semi-positive\nTriplet Loss (Ltriplet)\n24.78%\n46.99%\n35.13%\n58.79%\n879.06m\n46.55%\n85.07%\n62.95%\n83.63%\n252.88m\nInfoNCE Loss (LInfoNCE)\n35.83%\n63.79%\n48.08%\n68.15%\n576.41m\n52.67%\n90.75%\n67.74%\n85.35%\n204.08m\nInfoNCE Loss (LInfoNCE, w\/. MES)\n45.97%\n71.43%\n57.19%\n71.48%\n460.08m\n65.89%\n93.09%\n77.84%\n86.52%\n196.59m\nOurs (Lweighted-InfoNCE, w\/. MES)\n55.91%\n81.07%\n66.56%\n76.35%\n342.05m\n84.95%\n97.59%\n90.15%\n88.03%\n149.07m\nFigure 4: Meter-level localization accuracy of different methods on (left) cross-area and (right) same-area.\nTable 3: Performance on GTA-UAV comparing to different pre-training datasets.\nPre-train datasets\nCross-Area\nSame-Area\nR@1↑\nR@5↑\nAP↑\nSDM@3↑\nDis@1↓\nR@1↑\nR@5↑\nAP↑\nSDM@3↑\nDis@1↓\nImageNet (Deng et al. 2009)\n9.74%\n21.73%\n15.74%\n33.58%\n1841.30m\n10.65%\n23.90%\n17.15%\n36.82%\n1470.50m\nPerfect Matching\nUniversity-1652 (Zheng, Wei, and Yang 2020)\n32.16%\n54.19%\n41.79%\n54.07%\n991.64m\n30.90%\n51.88%\n40.08%\n51.62%\n1166.06m\nSUES-200 (Zhu et al. 2023a)\n35.29%\n56.85%\n44.85%\n55.32%\n920.62m\n32.24%\n52.63%\n41.38%\n52.58%\n1138.93m\nDenseUAV (Dai et al. 2023)\n12.89%\n23.03%\n17.85%\n32.33%\n1848.47m\n12.14%\n22.06%\n17.11%\n30.25%\n2115.03m\nPartial Matching\nGTA-UAV\n55.91%\n81.07%\n66.56%\n76.35%\n342.05m\n84.95%\n97.59%\n90.15%\n88.03%\n149.07m\nitive references as the correct result, which is precisely\nthe training target of the positive data. However, for the\nlocalization task, the results trained on both positive and\nsemi-positive data achieve better results in the SDM@3 and\nDis@1 metrics. This is because the semi-positive data en-\nable the model to learn a more comprehensive understand-\ning of partial matching relationships. The further analysis of\nproposed weighted-InfoNCE are put in the supplementary.\nIn the above sections, we discuss about the significance\nof the unaligned partial N-to-N matching paradigm for real-\nworld scenarios. Here we categorize the existing UAV geo-\nlocalization datasets as perfect matching data, and com-\npare the performance of models pre-trained on these perfect\nmatching datasets with their performance on our proposed\npartial matching GTA-UAV dataset. The results in Tab. 3\ndemonstrate a significant gap between these two tasks, and\nhighlight the substantial importance of our proposed GTA-\nUAV data for more practical partial matching tasks.\nGTA-UAV Transfer Capability\nTo further demonstrate the significance of the proposed\nGTA-UAV dataset for real-world application scenarios, we\nevaluate the transferability of its pre-trained model to real\ndata with limited number and scenarios. We select a recently\nreleased drone-view dataset, UAV-VisLoc (Xu et al. 2024),\nwhich lacks data pairing and task design, as real data. It in-\ncludes 6,742 high-altitude, downward-facing images from\nUAVs, covering several continuous area, and each image\nis GPS-tagged. These settings are included in the GTA-\nUAV dataset, making it a suitable target subset to evaluate\nthe transferability of our dataset. By using the same data\nconstruction method as GTA-UAV, we pair the hierarchical\nsatellite-view images from seven regions and apply identi-\ncal training and evaluation settings. The detailed experiment\nTable 4: Transfer performance on UAV-VisLoc with same-area setting comparing different pre-training datasets.\nExp. Setup\nPre-training datasets\nSame-Area\nR@1↑\nR@5↑\nAP↑\nSDM@3↑\nDis@1↓\nzero-shot\nImageNet (Deng et al. 2009)\n8.35%\n16.47%\n13.16%\n26.53%\n2615.08m\nzero-shot\nUniversity-1652 (Zheng, Wei, and Yang 2020)\n9.61%\n19.70%\n14.73%\n31.67%\n2285.08m\nzero-shot\nSUES-200 (Zhu et al. 2023a)\n16.71%\n27.84%\n22.93%\n34.07%\n1959.02m\nzero-shot\nDenseUAV (Dai et al. 2023)\n18.79%\n27.09%\n23.65%\n32.95%\n2051.58m\nzero-shot\nGTA-UAV\n24.94%\n42.59%\n33.15%\n41.40%\n1689.24m\nfine-tune\nImageNet (Deng et al. 2009)\n74.41%\n92.36%\n83.29%\n80.94%\n166.63m\nfine-tune\nUniversity-1652 (Zheng, Wei, and Yang 2020)\n73.91%\n93.10%\n82.05%\n82.01%\n170.23m\nfine-tune\nSUES-200 (Zhu et al. 2023a)\n74.44%\n92.61%\n81.95%\n82.10%\n150.22m\nfine-tune\nDenseUAV (Dai et al. 2023)\n77.09%\n92.61%\n83.82%\n82.05%\n139.34m\nfine-tune\nGTA-UAV\n80.20%\n96.53%\n87.83%\n85.46%\n122.87m\nTable 5: Performance on GTA-UAV of different models.\nModel\nR@1↑\nAP↑\nSDM@3↑\nDis@1↓\nCross-Area\nResNet-101\n13.74%\n23.06%\n48.06%\n1126.52m\nConvNeXt-Base\n55.36%\n66.14%\n74.91%\n386.35m\nSwinv2-B\n53.70%\n65.13%\n77.07%\n343.30m\nViT-Base\/16\n55.91%\n66.56%\n76.35%\n342.05m\nSame-Area\nResNet-101\n58.10%\n69.98%\n82.64%\n371.78m\nConvNeXt-Base\n83.94%\n89.54%\n87.98%\n160.49m\nSwinv2-B\n81.73%\n88.32%\n87.35%\n196.06m\nViT-Base\/16\n84.95%\n90.15%\n88.03%\n149.07m\nsetup and implementations are put in the supplementary. As\nshown in Tab. 4, comparing to ImageNet, University, SUES-\n200, and DenseUAV, the model pre-trained on GTA-UAV\nshows the best zero-shot performance on real UAV geo-\nlocalization dataset with cross-area setting. Specifically, the\nR@1 is 6.15% higher than the second-best result, and the AP\nis 9.5% higher. Similarly, after fine-tuning on UAV-VisLoc,\nthe model pre-trained on GTA-UAV still maintains the high-\nest performance, where the distance error of top-1 retrieval\nDis@1 is reduced by 16.47m.\nAblation Study\nArchitecture Evaluation\nIn existing cross-view geo-localization (Deuser, Habel, and\nOswald 2023; Hu et al. 2018; Toker et al. 2021; Zhu,\nShah, and Chen 2022) research, CNNs and Transformers are\nwidely explored for learning useful representations. Some\nstudies make adaptive modifications to achieve better learn-\ning capabilities (Zhu, Shah, and Chen 2022; Hu et al. 2018;\nZhu et al. 2023b). Unlike previous tasks, in the GTA-UAV\ncross-area task and its corresponding real-world scenarios,\nthe generalization to unseen data in unkown scenes needs\nto be emphasized. Based on studies of model generaliza-\ntion (Hoyer, Dai, and Van Gool 2023; Ji et al. 2024) and pre-\nvious SOTA geo-localization methods (Deuser, Habel, and\nOswald 2023; Zhu, Shah, and Chen 2022), we compare sev-\nTable 6: Performance on GTA-UAV comparing different\nhyper-parameters.\nExp. Setup\nR@1↑\nR@5↑\nAP↑\nSDM@3↑\nDis@1↓\nCross-Area\nk = 1\n52.96%\n79.62%\n64.82%\n74.94%\n386.56m\nk = 5\n55.91%\n81.07%\n66.56%\n76.35%\n342.05m\nk = 20\n51.50%\n77.17%\n62.55%\n74.16%\n411.12m\nk →∞\n45.97%\n71.43%\n57.19%\n71.48%\n460.08m\nSame-Area\nk = 1\n79.53%\n97.35%\n88.79%\n87.91%\n173.66m\nk = 5\n84.95%\n98.53%\n90.15%\n88.03%\n149.07m\nk = 20\n77.31%\n96.80%\n83.91%\n87.03%\n189.73m\nk →∞\n65.89%\n93.09%\n77.84%\n86.52%\n196.59m\neral standard architectures in Tab. 5. The results show that\nthe ViT has the best performance under the same order of\nmagnitude parameters. The practical commonly used archi-\ntecture ResNet exhibits poor generalization ability, which\nmay be attributed to its relatively weak representational and\ngeneralization capacities when dealing with significant vari-\nations in displacement, angles, and scenes. We also conduct\nexperiments on the scale of model parameters in the supple-\nmentary.\nHyper-parameter Evaluation\nWe evaluate different hyper-parameter value k of proposed\nweighted InfoNCE in Tab. 6. There is a trade-off between\ntreating partial matches as fully positive and maintaining\nflexibility (controlled by k), while all these results outper-\nform when k →∞(i.e., the standard InfoNCE). In addi-\ntion, considering that the form of weighted-InfoNCE can be\nregarded as a weight-based label smoothing variant of In-\nfoNCE, we also compare the results of InfoNCE with differ-\nent fixed smooth value ϵ in the supplementary.\nConclusion\nWe propose a new benchmark GTA-UAV for UAV geo-\nlocalization with partial matching pairs, which is a more\npractical setting. A weighted InfoNCE loss is introduced to\nleverage the supervision of matching extends. Extensive ex-\nperiments validate the effectiveness of our data and method\nfor UAV geo-localization and demonstrate the potential in\nreal-world scenarios. This work provides a paradigm aligned\nwith real-world tasks for future research.\nReferences\nArandjelovic, R.; Gronat, P.; Torii, A.; Pajdla, T.; and Sivic,\nJ. 2016. NetVLAD: CNN architecture for weakly supervised\nplace recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 5297–5307.\nChen, Q.; Wang, T.; Yang, Z.; Li, H.; Lu, R.; Sun, Y.; Zheng,\nB.; and Yan, C. 2024.\nSDPL: Shifting-Dense Partition\nLearning for UAV-View Geo-Localization. IEEE Transac-\ntions on Circuits and Systems for Video Technology, 34(11):\n11810–11824.\nDai, M.; Zheng, E.; Feng, Z.; Qi, L.; Zhuang, J.; and Yang,\nW. 2023. Vision-based UAV self-positioning in low-altitude\nurban environments. IEEE Transactions on Image Process-\ning.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDeuser, F.; Habel, K.; and Oswald, N. 2023. Sample4Geo:\nHard Negative Sampling For Cross-View Geo-Localisation.\nIn 2023 IEEE\/CVF International Conference on Computer\nVision (ICCV), 16801–16810. Paris, France: IEEE. ISBN\n9798350307184.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. arXiv:2010.11929.\nDusmanu, M.; Rocco, I.; Pajdla, T.; Pollefeys, M.; Sivic, J.;\nTorii, A.; and Sattler, T. 2019. D2-Net: A Trainable CNN\nfor Joint Description and Detection of Local Features. In\n2019 IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 8084–8093. Long Beach, CA,\nUSA: IEEE. ISBN 978-1-72813-293-8.\nHoyer, L.; Dai, D.; and Van Gool, L. 2023. Domain adaptive\nand generalizable network architectures and training strate-\ngies for semantic image segmentation.\nHu, S.; Feng, M.; Nguyen, R. M. H.; and Lee, G. H. 2018.\nCVM-Net: Cross-View Matching Network for Image-Based\nGround-to-Aerial Geo-Localization.\nIn 2018 IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\n7258–7267. Salt Lake City, UT, USA: IEEE. ISBN 978-1-\n5386-6420-9.\nJi, Y.; He, B.; Qu, C.; Tan, Z.; Qin, C.; and Wu, L. 2024.\nDiffusion Features to Bridge Domain Gap for Semantic Seg-\nmentation. arXiv:2406.00777.\nKiefer, B.; Ott, D.; and Zell, A. 2022. Leveraging synthetic\ndata in object detection on unmanned aerial vehicles.\nIn\n2022 26th international conference on pattern recognition\n(ICPR), 3564–3571. IEEE.\nKingma, D. P.; and Ba, J. 2017.\nAdam: A Method for\nStochastic Optimization. arXiv:1412.6980.\nLi, H.; Wang, J.; Wei, Z.; and Xu, W. 2023.\nJointly\nOptimized Global-Local Visual Localization of UAVs.\narXiv:2310.08082.\nLin, J.; Zheng, Z.; Zhong, Z.; Luo, Z.; Li, S.; Yang, Y.; and\nSebe, N. 2022. Joint Representation Learning and Keypoint\nDetection for Cross-View Geo-Localization. IEEE Transac-\ntions on Image Processing, 31: 3780–3792.\nLin, T.-Y.; Belongie, S.; and Hays, J. 2013. Cross-View Im-\nage Geolocalization.\nIn 2013 IEEE Conference on Com-\nputer Vision and Pattern Recognition, 891–898. Portland,\nOR, USA: IEEE. ISBN 978-0-7695-4989-7.\nLin, T.-Y.; Yin Cui; Belongie, S.; and Hays, J. 2015. Learn-\ning Deep Representations for Ground-to-Aerial Geolocal-\nization.\nIn 2015 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 5007–5015. Boston, MA,\nUSA: IEEE. ISBN 978-1-4673-6964-0.\nLiu, L.; and Li, H. 2019. Lending orientation to neural net-\nworks for cross-view geo-localization.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, 5624–5633.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision.\nRichter, S. R.; Vineet, V.; Roth, S.; and Koltun, V. 2016.\nPlaying for data: Ground truth from computer games. In\nComputer Vision–ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11-14, 2016, Pro-\nceedings, Part II 14, 102–118. Springer.\nRos, G.; Sellart, L.; Materzynska, J.; Vazquez, D.; and\nLopez, A. M. 2016. The SYNTHIA Dataset: A Large Col-\nlection of Synthetic Images for Semantic Segmentation of\nUrban Scenes. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 3234–3243. Las Ve-\ngas, NV, USA: IEEE. ISBN 978-1-4673-8851-1.\nTian, Y.; Chen, C.; and Shah, M. 2017.\nCross-View Im-\nage Matching for Geo-Localization in Urban Environments.\nIn 2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 1998–2006. Honolulu, HI: IEEE.\nISBN 978-1-5386-0457-1.\nToker, A.; Zhou, Q.; Maximov, M.; and Leal-Taix´e, L. 2021.\nComing down to earth: Satellite-to-street view synthesis for\ngeo-localization. In Proceedings of the IEEE\/CVF Confer-\nence on Computer Vision and Pattern Recognition, 6488–\n6497.\nvan den Oord, A.; Li, Y.; and Vinyals, O. 2019.\nRep-\nresentation Learning with Contrastive Predictive Coding.\narXiv:1807.03748.\nVo, N. N.; and Hays, J. 2016. Localizing and orienting street\nviews using overhead imagery. In Computer Vision–ECCV\n2016: 14th European Conference, Amsterdam, The Nether-\nlands, October 11–14, 2016, Proceedings, Part I 14, 494–\n509. Springer.\nWorkman, S.; Souvenir, R.; and Jacobs, N. 2015. Wide-area\nimage geolocalization with aerial reference imagery. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, 3961–3969.\nXu, W.; Yao, Y.; Cao, J.; Wei, Z.; Liu, C.; Wang, J.; and\nPeng, M. 2024.\nUAV-VisLoc: A Large-scale Dataset for\nUAV Visual Localization. arXiv:2405.11936.\nYang, H.; Lu, X.; and Zhu, Y. 2021.\nCross-View Geo-\nlocalization with Layer-to-Layer Transformer. In Advances\nin Neural Information Processing Systems, volume 34,\n29009–29020. Curran Associates, Inc.\nZhai, M.; Bessinger, Z.; Workman, S.; and Jacobs, N. 2017.\nPredicting ground-level scene layout from aerial imagery. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 867–875.\nZheng, Z.; Wei, Y.; and Yang, Y. 2020. University-1652:\nA multi-view multi-source benchmark for drone-based geo-\nlocalization. In Proceedings of the 28th ACM international\nconference on Multimedia, 1395–1403.\nZhu, R.; Yin, L.; Yang, M.; Wu, F.; Yang, Y.; and Hu, W.\n2023a. SUES-200: A multi-height multi-scene cross-view\nimage benchmark across drone and satellite. IEEE Transac-\ntions on Circuits and Systems for Video Technology, 33(9):\n4825–4839.\nZhu, S.; Shah, M.; and Chen, C. 2022. Transgeo: Trans-\nformer is all you need for cross-view image geo-localization.\nIn Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, 1162–1171.\nZhu, S.; Yang, T.; and Chen, C. 2021. VIGOR: Cross-View\nImage Geo-localization beyond One-to-one Retrieval.\nIn\n2021 IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 5316–5325. Nashville, TN, USA:\nIEEE. ISBN 978-1-66544-509-2.\nZhu, Y.; Yang, H.; Lu, Y.; and Huang, Q. 2023b. Simple, Ef-\nfective and General: A New Backbone for Cross-view Image\nGeo-localization. arXiv:2302.01572.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Game4Loc: A UAV Geo-Localization Benchmark from Game Data.pdf"}
{"title":"ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation","authors":"Dohee Kim, Unggi Lee, Sookbun Lee, Jiyeong Bae, Taekyung Ahn, Jaekwon Park, Gunho Lee, Hyeoncheol Kim","summary":"This paper introduces ES-KT-24, a novel multimodal Knowledge Tracing (KT)\ndataset for intelligent tutoring systems in educational game contexts. Although\nKT is crucial in adaptive learning, existing datasets often lack game-based and\nmultimodal elements. ES-KT-24 addresses these limitations by incorporating\neducational game-playing videos, synthetically generated question text, and\ndetailed game logs. The dataset covers Mathematics, English, Indonesian, and\nMalaysian subjects, emphasizing diversity and including non-English content.\nThe synthetic text component, generated using a large language model,\nencompasses 28 distinct knowledge concepts and 182 questions, featuring 15,032\nusers and 7,782,928 interactions. Our benchmark experiments demonstrate the\ndataset's utility for KT research by comparing Deep learning-based KT models\nwith Language Model-based Knowledge Tracing (LKT) approaches. Notably, LKT\nmodels showed slightly higher performance than traditional DKT models,\nhighlighting the potential of language model-based approaches in this field.\nFurthermore, ES-KT-24 has the potential to significantly advance research in\nmultimodal KT models and learning analytics. By integrating game-playing videos\nand detailed game logs, this dataset offers a unique approach to dissecting\nstudent learning patterns through advanced data analysis and machine-learning\ntechniques. It has the potential to unearth new insights into the learning\nprocess and inspire further exploration in the field.","url":"http:\/\/arxiv.org\/abs\/2409.10244v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2409.10244v1","published":1726490918000,"comment":"11 pages, 5 figures","pdf_text":"ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with\nEducational Game Playing Video and Synthetic Text Generation\nDohee Kim1∗, Unggi Lee1,2∗†, Sookbun Lee1∗, Jiyeong Bae1, Taekyung Ahn1\nJaekwon Park1, Gunho Lee1, Hyeoncheol Kim2†\nEnuma, Inc.1, Korea University2\n{dohee, unggi, blackdew, jiyoung, taekyung, jaekwon, gunho}@enuma.com,\nharrykim@korea.ac.kr\n∗is first authors and † is corresponding authors.\nAbstract\nThis paper introduces ES-KT-24, a novel multi-\nmodal Knowledge Tracing (KT) dataset for in-\ntelligent tutoring systems in educational game\ncontexts. Although KT is crucial in adaptive\nlearning, existing datasets often lack game-\nbased and multimodal elements. ES-KT-24\naddresses these limitations by incorporating ed-\nucational game-playing videos, synthetically\ngenerated question text, and detailed game logs.\nThe dataset covers Mathematics, English, In-\ndonesian, and Malaysian subjects, emphasizing\ndiversity and including non-English content.\nThe synthetic text component, generated using\na large language model, encompasses 28 dis-\ntinct knowledge concepts and 182 questions,\nfeaturing 15,032 users and 7,782,928 interac-\ntions. Our benchmark experiments demonstrate\nthe dataset’s utility for KT research by compar-\ning Deep learning-based KT models with Lan-\nguage Model-based Knowledge Tracing (LKT)\napproaches.\nNotably, LKT models showed\nslightly higher performance than traditional\nDKT models, highlighting the potential of lan-\nguage model-based approaches in this field.\nFurthermore, ES-KT-24 has the potential to sig-\nnificantly advance research in multimodal KT\nmodels and learning analytics. By integrating\ngame-playing videos and detailed game logs,\nthis dataset offers a unique approach to dissect-\ning student learning patterns through advanced\ndata analysis and machine-learning techniques.\nIt has the potential to unearth new insights into\nthe learning process and inspire further explo-\nration in the field.\n1\nIntroduction\nKnowledge Tracing (KT) is a fundamental task\nthat aims to model students’ knowledge states over\ntime based on their interactions with learning ma-\nterials (Piech et al., 2015). These interactions typ-\nically include viewing problems, attempting so-\nlutions, and selecting answers in online learning\nGame Playing Video\n \nThis educational game enhances cognitive development in \nchildren by allowing them to identify and complete patterns using \nvarious everyday objects in a festive, carnival-themed \nenvironment.\nGame Log\nReg Date\n2024-01-10 \n01:49:16\nContent Status\nfinish\nDuration Time\n56.36362s\nPlay Mode\nlesson\nLesson Code\nMAT_C01_LV0\n2_U004_L004\nKC Text\npatterns_math\nQuestion Text\nFigure 1: An example of ES-KT-24. ES-KT-24 consists\nof a multimodal dataset, including game-playing video,\nsynthetic question text, knowledge concept (KC) text,\nand game logs collected from educational game con-\ntexts.\nsystems. The goal of the KT model is to use these\nsequences to predict students’ future performance\non unseen items (Lee et al., 2024b). Over the years,\nvarious KT models have been developed, ranging\nfrom traditional approaches like Bayesian Knowl-\nedge Tracing (BKT) (Corbett and Anderson, 1994)\nto more recent deep learning-based methods such\nas Deep Knowledge Tracing (DKT) (Piech et al.,\n2015). These models have shown promising results\nin predicting student performance and understand-\ning learning patterns.\nHowever, most existing KT datasets primarily\nconsist of numerical sequences, with only a few\n1\narXiv:2409.10244v1  [cs.SI]  16 Sep 2024\nEducational Game\n‘\nREC\nOpenAI\nWhisper (Speech-to-Text)\nGPT-4o (Video-to-Text)\nPlay & Record\nSynthetic Text Data\nQuestion Texts\nPreprocessing\nLog Data\nEDA\nSequence Data for KT\nKC Texts\nKC Sequences\nQuestion Sequences\nFigure 2: The process began with the manual game-play of educational games, which were screen-recorded. These\nrecordings were then converted to text using OpenAI GPT-4o (OpenAI, 2024) for visual content and Whisper\n(Radford et al., 2023) for audio transcription. The resulting text was utilized to create Questions corresponding to\neach game. Student problem-solving histories and game logs were preprocessed and explored through Exploratory\nData Analysis (EDA), then transformed into sequence data suitable for KT tasks. Finally, this text and sequence\ndata were released as a paired dataset.\nrecent datasets incorporating textual information.\nThese datasets typically focus on text-based inter-\nactions where students submit answers (correct or\nincorrect) to given problems (Lee et al., 2024b,a;\nLiu et al., 2019). This approach, while applicable,\nis limited in its ability to capture the complex in-\nteractions and outcomes possible in modern digital\neducational materials, particularly in game-based\nlearning environments.\nGame-based content offers a richer set of interac-\ntions and outcomes beyond simple correct\/incorrect\nanswers, including metrics such as time spent on\ntasks or the number of attempts made (Hooshyar\net al., 2022). Game-based learning often involves\nmultiple modalities, including text, images, anima-\ntions, and audio. To advance research in this area\nand develop more comprehensive KT models, there\nis a need for datasets that capture these multimodal\naspects of game-based learning.\nHowever, there are still significant limitations in\nthe publicly available datasets (Liu et al., 2024b).\nOne notable gap is the need for more game-based\nlearning datasets. While educational games have\nbecome increasingly popular as learning tools\n(Noemí and Máximo, 2014; Moreno-Ger et al.,\n2008; Petri et al., 2024), KT datasets from gaming\nenvironments, mainly publicly available, remain\nscarce. This scarcity is even more pronounced in\ngame-based learning. To the best of our knowl-\nedge, there are currently no publicly accessible KT\ndatasets derived from educational gaming environ-\nments.\nAnother critical limitation is the absence of mul-\ntimodal data in most KT datasets. There has been\na growing trend in research towards multimodal\nmodels that combine language models with other\nmodalities such as vision and audio (Liu et al.,\n2024a; Deshmukh et al., 2023; Borsos et al., 2023;\nDriess et al., 2023). These multimodal approaches\nhave shown remarkable capabilities in understand-\ning and generating content across different modali-\nties.\nIt is becoming increasingly apparent in educa-\ntion that leveraging multiple modalities is crucial\nfor developing comprehensive models that can fully\ncapture the complexity of learning environments\n(Lee et al., 2024c,d). Educational contexts often\ninvolve rich, multimodal interactions, including\nvisual aids, audio explanations, and hands-on ac-\ntivities, which a sequence of the number cannot\n2\nDataset\n# Students\n# Questions\n# KCs\n# Interactions\nSubjects\nQuestion Texts\nVideos\nGame Logs\nASSISTments2009\n4,217\n26,688\n123\n346,860\nMath\nNo\nNo\nNo\nASSISTments2012\n46,674\n179,999\n265\n6,123,270\nMath\nNo\nNo\nNo\nASSISTments2015\n19,917\n100\n-\n708,631\nMath\nNo\nNo\nNo\nASSISTments2017\n1,709\n3,162\n102\n942,816\nMath\nNo\nNo\nNo\nStatistics2011\n333\n1,224\n-\n194,947\nMath\nNo\nNo\nNo\nJunyi2015\n247,606\n722\n41\n25,925,922\nMath\nNo\nNo\nNo\nKDD2005\n574\n210,710\n112\n809,694\nMath\nNo\nNo\nNo\nKDD2006\n1,146\n207,856\n493\n3,679,199\nMath\nNo\nNo\nNo\nNeurIPS2020\n4,918\n948\n57\n1,382,727\nMath\nNo\nNo\nNo\nPOJ\n22,916\n2,750\n-\n996,240\nPL\nNo\nNo\nNo\nEdNet\n1,677,583\n52,676\n962\n372,366,720\nLinguistics\nNo\nNo\nNo\nDBE-KT22\n1,361\n212\n98\n167,222\nComputer and Information Science\nYes\nNo\nNo\nXES3G5M\n18,066\n7,652\n865\n5,549,635\nMath\nYes\nNo\nNo\nES-KT-24 (Ours)\n15,032\n182 (game)\n28\n7,783,466\nEnglish, Math, Indonesian, Malay\nYes\nYes\nYes\nTable 1: Comparison of KT benchmark dataset, referencing the XES3G5M research (Liu et al., 2024b). In several\nkey aspects, existing KT datasets differ significantly from ES-KT-24. ES-KT-24 stands out for its multimodal\napproach, incorporating text, video, and game log data with educational game context. This dataset also broadens\nthe scope of subjects beyond the typical Math and English, including Indonesian and Malay languages, to promote\nequity in educational research. Note that the number of questions corresponds directly to the number of games.\nadequately represent (Blikstein, 2013; Ochoa et al.,\n2017; Blikstein and Worsley, 2016).\nThis paper introduces ES-KT-24 1, a novel KT\ndataset based on educational game data.\nThis\ndataset is uniquely rich in multimodal features, cap-\nturing various aspects of game-based learning envi-\nronments. As shown in Figure 1, we provide Game\nPlaying Videos, where one video was recorded for\neach available game by researchers, along with the\nprocessed Question Texts derived from these videos.\nAdditionally, ES-KT-24 also contains Game Log\nsuch as the game category (KC Text), log registra-\ntion time (Reg Date), gameplay status (Content Sta-\ntus), gameplay duration (Duration Time), gameplay\nmode (Play Mode), and the curriculum code (Les-\nson Code). By providing this resource, we aim to\nsupport KT research and broader studies in Learn-\ning Analytics (LA) (Lang et al., 2022). Researchers\ncan utilize this dataset to investigate game-related\nlearning phenomena, analyze multimodal learning\nprocesses, and develop more sophisticated models\nthat account for the diverse modalities present in\neducational settings. The ES-KT-24 dataset thus\nrepresents a significant step towards more compre-\nhensive and realistic modeling of student learning\nin digital environments.\nOur research contributions are below:\n• We provide a benchmark for KT using game\ndata, offering a new standard for evaluating\nKT models in game-based learning environ-\nments.\n• By providing gameplay videos and learning\n1The dataset will be made publicly available following\nfinal review.\ndata, we enable researchers in learning analyt-\nics to explore and analyze multimodal educa-\ntional interactions.\n2\nRelated Work\n2.1\nKnowledge Tracing Datasets\nKT primarily involves utilizing datasets that record\nstudents’ interactions over time (Piech et al., 2015).\nThese datasets generally contain sequential infor-\nmation about student responses to various items\n(e.g., KCs or questions).\nEach interaction in\nthe dataset is typically associated with critical at-\ntributes such as the student ID, the item ID, the\ntime the interaction occurred, and whether the stu-\ndent’s response was correct (Lee et al., 2024b) (See\nFigure 3). By analyzing these sequences, KT mod-\nels aim to infer a student’s underlying knowledge\nstate and predict their future performance on new,\nunseen items based on patterns in their past inter-\nactions (Pandey and Srivastava, 2020; Ghosh et al.,\n2020).\nSeveral educational datasets have been widely\nused for KT tasks. These include the ASSIST-\nments datasets (Heffernan and Heffernan, 2014),\nwhich provide instructional guidance by assessing\nstudents’ knowledge states; the Junyi Academy\ndataset (Academy, 2015), containing mathemat-\nics question-solving activities; the Peking Online\nJudge (POJ) dataset (Pandey and Srivastava, 2020),\noffering coding practice data; EdNet (Choi et al.,\n2020), comprising TOEIC test preparation activi-\nties; and the Statics2011 (Project, 2011) dataset\nfrom an online engineering course.\nAddition-\nally, data mining competitions have contributed\nhigh-quality datasets, such as the KDD Cup 2010\n3\n𝑸𝟎\n𝑸𝟏\n𝑸𝟐\n𝑸𝟑\nQuestion\nQuestion \nNumber\nKC \nNumber\nResponse\nQuestion Text\nKC Text\n𝑸𝟎\n0\n2\n1\nJohn has 8 marbles. His sister gives him 5 more marbles. \nHow many marbles does John have now?\nAddition\n𝑸𝟏\n1\n2\n1\nSarah has 6 stickers. Her friend gives her 4 more stickers. \nHow many stickers does Sarah have now?\nAddition\n𝑸𝟐\n2\n1\n0\nWhat is 6 multiplied by 4?\nMultiplication\n𝑸𝟑\n3\n3\n?\nIf you have 15 apples and give 7 to your friend, how many \napples do you have left?\nSubtraction\n✘\n？\nFigure 3: KT dataset format. Left shows the exercising process of a student, where the student has already done\nthree questions and will answer question number 4. Right shows the corresponding materials of questions that\ncontain their contents and KCs. Note that question and KC texts are used for LKT.\nEDM Challenge datasets (Organizers, 2010) and\nthe NeurIPS2020 Education Challenge dataset (Or-\nganizers, 2020) from Eedi. The KDD Cup 2010\ndatasets consist of Algebra2005 and 2006 datasets.\nWe refer to Algebra2005 as KDD2005 and Alge-\nbra2006 as KDD2006.\nWhile most of these datasets shown in Table\n1 primarily contain ID features of questions and\nknowledge components along with timestamps, re-\ncent datasets like DBE-KT22 (Abdelrahman et al.,\n2022) and XES3G5M (Liu et al., 2024b) stand out\nby including the actual text content of the ques-\ntions. XES3G5M, in particular, provides rich auxil-\niary information, including textual content of ques-\ntions, knowledge component relationships, ques-\ntion types, and answer analyses that may enhance\nthe modeling process of students’ learning out-\ncomes.\nHowever, despite the variety of datasets avail-\nable, there are two significant gaps in the field\nof KT datasets. First, KT datasets related to ed-\nucational games are scarce. While game-based\nlearning environments have become increasingly\npopular, publicly available KT datasets from these\ncontexts are rare. The GameDKT study (Hooshyar\net al., 2022) highlights this gap, presenting one of\nthe few approaches for KT in game-based learning\nenvironments, but even this dataset is not publicly\navailable for research. Second, there is a signifi-\ncant lack of KT datasets incorporating multimodal\ndata, including text, images, videos, and audio. The\nscarcity of multimodal data limits researchers’ abil-\nity to develop KT models that can leverage diverse\ntypes of information to understand and predict stu-\ndent learning processes more accurately.\n2.2\nKnowledge Tracing Models\nKT models have evolved significantly, aiming to ac-\ncurately predict students’ knowledge states and fu-\nture performance. DKT (Piech et al., 2015) signif-\nicantly advanced the field by leveraging recurrent\nneural networks to model students’ knowledge ac-\nquisition over time. Following DKT, various mod-\nels were developed to enhance performance and\naddress specific challenges. For instance, DKVMN\n(Zhang et al., 2017) utilized a key-value memory\nnetwork to model the relationships between ex-\nercises and knowledge concepts. SAKT (Pandey\nand Karypis, 2019) employed self-attention mech-\nanisms to capture complex dependencies in stu-\ndent interaction sequences. AKT (Ghosh et al.,\n2020) further improved upon this by incorporat-\ning a monotonic attention mechanism and context-\naware representations.\nRecently, a novel approach called LKT has\nemerged, leveraging the power of Pre-trained Lan-\nguage Models (PLMs) to enhance KT performance.\nLee et al. (Lee et al., 2024b) introduced LKT,\nwhich directly utilizes PLMs to process learning\ndata in a textual format. This method has demon-\nstrated superior performance to traditional KT mod-\nels by effectively capturing semantic information\nfrom questions and KCs.\nBuilding upon the success of LKT, efforts are\nbeing made to apply it to specific domains. For\ninstance, CodeLKT (Lee et al., 2024a) adapts\nthe LKT framework to programming education,\ndemonstrating significant improvements in predict-\ning student performance on coding tasks.\nHowever, research on KT for game data and\nmultimodal KT has been limited due to a lack of\ndatasets in these areas. In response, this study intro-\nduces ES-KT-24, a multimodal dataset specifically\ndesigned for KT in game contexts.\n3\nData Description\n3.1\nLicense\nES-KT-24 is released under the CC BY-NC 4.0\n(Attribution-NonCommercial 4.0 International) li-\ncense. This license restricts the dataset to non-\ncommercial use while allowing modifications and\nsharing under similar terms. Under this license, oth-\n4\n(a) Sequence Length (Log 10 Scale)\n(b) Correct Ratio\n(c) Learning Time (Log 10 Scale)\n(d) Sequence Length By Subject (Log 10 Scale)\n(e) Correct Ratio By Subject\n(g) Sequence Time\n(h) Sequence Time Over 10 minutes\n(f) Learning Time By Subject (Log 10 Scale)\nFigure 4: Data analysis results. Upper figure (a) to (f) are Violin Plots of Sequence Length, Correct Answer Ratio,\nand Learning Time. Lower figure (g) and (h) are Sequence Time of Users with Subject Comparisons.\ners can non-commercially remix, adapt, and build\nupon the work if they credit the source and indicate\nif changes were made.\n3.2\nData Collection\nOur data collection process (Figure 2 left) involved\nactual gameplay sessions of educational software\ncomprising game-based content developed by an\nEdtech company. In this context, a session rep-\nresents a single playthrough of the game. The\ndataset consists of logs collected between January\n2024 and April 2024. These sessions were screen-\nrecorded to capture the visual aspects of the game-\nplay. Alongside the video recordings, we collected\ngame information, including player actions, in-\ngame events, correct\/incorrect responses, and time\nduration data. This multimodal approach allowed\nus to capture both the interactive elements of the\ngame and their corresponding learning outcomes.\nThe dataset spans four main subjects: Indonesian\n(IND), Malaysian (MAY), Mathematics (MAT),\nand English (ENG). Each subject is further divided\ninto specific categories reflecting different educa-\ntional focuses. These specific categories were used\nas Knowledge Concepts (KC):\n• Indonesian (IND): practice, alphabet, phon-\nics, vocabulary, listening, speaking, writing,\ntest\n• English (ENG): alphabet, phonics, vocabu-\nlary, listening, reading, speaking, writing\n• Mathematics (MAT): numbers, operations,\nshapes, measurement, data, reasoning\n• Malaysian (MAY): practice, alphabet, phon-\nics, vocabulary, speaking, writing\nNote that the presence of ’practice’ and ’test’ cat-\negories varies across subjects, reflecting differences\nin educational approaches and assessment methods\nspecific to each subject’s curriculum structure and\nlearning objectives.\nThe dataset comprises 28 distinct KCs and 182\nunique content questions, allowing for rich analysis\nacross different learning dimensions. These content\nquestions are textual representations of the educa-\ntional games created through the process described\nin Section 3.3, in which each game’s content was\ntransformed into a problem or task format.\n3.3\nSynthethic Data Generation\nWe utilized GPT-4o (OpenAI, 2024), an advanced\nlanguage model, to enrich our dataset with textual\ninformation. The recorded game-play videos and\n5\ncollected game information were input into GPT-\n4o to generate synthetic concept texts and problem\ndescriptions. This process allowed us to create a\ntextual representation of the game’s educational\ncontent, translating visual and interactive elements\ninto descriptive text.\nFollowing the initial generation, we underwent\na meticulous review and editing process. This step,\nwhich involved manual curation, was pivotal in en-\nsuring the accuracy and relevance of the generated\ntext. We meticulously corrected typographical er-\nrors, refined the language for clarity, and adjusted\ncontent that did not accurately reflect the game’s ed-\nucational objectives or mechanics. This human-in-\nthe-loop approach was instrumental in maintaining\nthe dataset’s quality and educational value while\nleveraging the capabilities of advanced language\nmodels.\n3.4\nData Cleaning and Processing\nGiven our educational game, which primarily tar-\ngets children aged 4 to 6, specific processing con-\nsiderations were required for the dataset. In this\ngame, users interact through hands-on activities\nor voice recognition, guided by visual aids and\naudio explanations. A vital characteristic of the\ngame is that there are no explicit correct or incor-\nrect answers. In other words, to proceed to the\nnext stage of a game, players must input correct\nanswers. This gameplay design limits the data on\nincorrect attempts, as only correct responses are\nlogged. Consequently, the game logs do not cap-\nture correct or incorrect answer data, necessitating\na new approach to defining correctness for the KT\nmodel. We implemented a rule-based system to\ninfer correctness based on Content Status and Du-\nration Time in Figure 1 to address this.\nIncorrect\nContentStatus\n< Avg.\nDurationTime\nAbort\nFinish\nYes\nNo\nIncorrect\nCorrect\nFigure 5: Flowchart for Correctness Determination\nBased on Game-play Interaction Data\nFor all game sessions, the session was marked\nas Incorrect if the game was interrupted, which\nmeans Abort in Figure 5 (e.g., a player exited mid-\ngame). It is important to note that the logs are\nrecorded when a game is completed. Each game\nconsists of multiple stages, and the content status is\nmarked as Finish only when all existing stages are\ncompleted. If a player only completes part of the\nstages, the status is recorded as Abort. The number\nof stages in a game can vary from as few as two\nto more than ten. The number of stages in each\ngame corresponds to the number of dots displayed\nat the top of the video. For example, as seen in the\ncaptured images of three games in Figure 1’s Game\nPlaying Video, the stage counts are 3, 3, and 10,\nrespectively, from left to right.\nBased on these log characteristics, the average\nduration time for each game was calculated from\ninstances where all stages were completed. The cor-\nrectness determination for games marked as Finish,\nmeaning all stages were completed, is explained as\nfollows: if the recorded duration time for a game\nwas longer than the average duration time for the\ngiven content, the attempt was considered Incor-\nrect. On the other hand, if the duration time was\nequal to or shorter than the average, the attempt\nwas considered Correct. This method allowed us to\nhandle incomplete or skipped attempts across the\ndataset systematically.\nAdditionally, extreme outliers were removed\nfrom the dataset. Expressly, 397 instances exceed-\ning 30 minutes were excluded to ensure data in-\ntegrity. After this cleaning process, the final dataset\ncomprises 15,032 users and 7,783,466 problem-\nsolving events.\n3.5\nStatistics\nThe finalized dataset (see Figure 2 right and Table\n1) provides a comprehensive overview of player\ninteractions and learning outcomes. Below is a\nbreakdown of key statistics:\n• Total users: 15,032\n• Total problem-solving events: 7,783,466\n• Subjects:\nIndonesian (IND), Malaysian\n(MAY), Mathematics (MAT), English (ENG)\n• Categories (KC): 28 across all subjects\n• ContentID (Questions): 182 across all sub-\njects\nFigure 4 provides visualizations that help to un-\nderstand key aspects of user interactions, such as se-\nquence length, correct answer ratio, learning time,\n6\nand sequence time, across different subjects. In\nsome visualizations, a log scale facilitates a more\nstraightforward interpretation of the data.\nThe distribution of students’ interaction se-\nquence length shows that 74.2% of student se-\nquences contain 100 to 1000 interactions, highlight-\ning that most students have substantial interaction\nsequences. Differences in sequence length by sub-\nject reveal that only the Malaysian subject has an\naverage sequence length below 100, indicating that\nstudents in this subject tend to solve fewer prob-\nlems than students in other subjects.\nThe distribution of correct answer ratios across\nstudents’ responses shows an average correct an-\nswer ratio of 45.19%. This even distribution sug-\ngests that students’ responses are balanced between\ncorrect and incorrect answers, regardless of the sub-\njects, indicating a balanced learning process.\nThe focus on students’ learning time demon-\nstrates that most students accumulate approxi-\nmately 10 hours of pure problem-solving time. The\ndistribution of learning time by subject aligns with\nthe findings regarding the Malaysian subject, where\nstudents not only complete fewer problems but also\nspend less cumulative time on problem-solving\ncompared to other subjects.\nThe distribution of sequence times, which cap-\ntures the intervals between consecutive problem-\nsolving logs from the app, reveals that a significant\nportion of logs (over 7 million) occurs within 10\nminutes, reflecting frequent and continuous user in-\nteraction. The insights provided by sequence times\nexceeding 10 minutes are significant, as they re-\nveal longer intervals of app usage, with the most\ncommon gap being between 1 day and 1 week, sug-\ngesting periodic re-engagement patterns with the\napp.\n4\nThe ES-KT-24 Benchmark\n4.1\nExperiment Setting\nWe comprehensively evaluated various KT mod-\nels on the ES-KT-24 dataset for our benchmark.\nWe employed a standard 5-fold cross-validation\napproach to ensure robust performance estimation.\nAll experiments were run multiple times to account\nfor variability, with results being reported as mean\nvalues and standard deviations.\n4.2\nBaseline Models\nWe compared a wide range of KT models, cat-\negorized into two main types: traditional DKT\nmodels and LKT models (Lee et al., 2024b). The\nDKT models included DKT (Piech et al., 2015),\nDKT+ (Yeung and Yeung, 2018), DKVMN (Zhang\net al., 2017), ATKT (Guo et al., 2021), SAKT\n(Pandey and Karypis, 2019), and SimpleKT (Liu\net al., 2023). The LKT models included various\npre-trained language models: BERT (Devlin et al.,\n2019), RoBERTa (Zhuang et al., 2021), ELECTRA\n(Clark et al., 2020), ERNIE-2.0 (Sun et al., 2020),\nALBERT (Lan, 2019), DistilBERT (Sanh et al.,\n2019), and DeBERTa-v3 (He et al., 2023).\n4.3\nPerformance Analysis\nOur experimental results reveal significant insights\ninto the performance of different KT approaches\non the ES-KT-24 dataset. Area Under the Curve\n(AUC) and Accuracy (ACC) were used as the eval-\nuation metrics.\nAmong the DKT models, SimpleKT demon-\nstrated the best performance with an AUC of\n0.7366 and ACC of 0.6709, significantly outper-\nforming traditional approaches. ATKT also showed\nstrong results with an AUC of 0.7100 and ACC of\n0.6521. Interestingly, some models like AKT and\nGKT failed to converge on our dataset, resulting in\nzero scores.\nThe LKT models generally outperformed their\nDKT counterparts, highlighting the effectiveness\nof leveraging pre-trained language models for KT\ntasks. RoBERTa achieved the highest performance\namong all models with an AUC of 0.7348 and ACC\nof 0.6691, closely followed by ERNIE-2.0 and\nDeBERTa-v3. Even the worst-performing LKT\nmodel (ALBERT) outperformed most DKT mod-\nels, emphasizing the potential of language model-\nbased approaches in this domain.\nThe consistent and robust performance across\ndifferent LKT models is worth noting, with all\nachieving AUC scores above 0.72 and ACC scores\nabove 0.65. This consistency suggests that the lan-\nguage model-based approach is practical and reli-\nable across various architectures.\nThe superior performance of LKT models can\nbe attributed to their ability to capture semantic\ninformation from the textual content of questions\nand concepts, which is particularly beneficial in\nour game-based learning scenario. However, the\nstrong showing of SimpleKT among DKT models\nindicates that well-designed traditional approaches\ncan still be competitive.\nThese results underscore the immense potential\nof language model-based approaches in KT, es-\n7\nType\nModels\nAUC\nACC\nDKT\nDKT\n0.6824±0.0004\n0.6335±0.0005\nDKT\nDKT+\n0.6829±0.0007\n0.6340±0.0007\nDKT\nDKVMN\n0.6821±0.0005\n0.6345±0.0002\nDKT\nATKT\n0.7100±0.0141\n0.6521±0.0092\nDKT\nSAKT\n0.6848±0.0009\n0.6361±0.0006\nDKT\nSimpleKT\n0.7366±0.0006\n0.6709±0.0007\nLKT\nBERT\n0.7287±0.0010\n0.6645±0.0012\nLKT\nRoBERTa\n0.7348±0.0022\n0.6691±0.0016\nLKT\nELECTRA\n0.7303±0.0017\n0.6660±0.0018\nLKT\nERNIE-2.0\n0.7325±0.0014\n0.6673±0.0012\nLKT\nALBERT\n0.7231±0.0021\n0.6588±0.0014\nLKT\nDistilBERT\n0.7279±0.0018\n0.6639±0.0013\nLKT\nDeBERTa-v3\n0.7326±0.0026\n0.6670±0.0029\nTable 2: Performance of KT models on ES-KT-24. Sim-\npleKT shows the best performance, followed by LKT-\nRoBERTa.\npecially in contexts with rich textual information.\nThey also highlight the value of our ES-KT-24\ndataset in advancing research in game-based KT\nand multimodal learning analytics, paving the way\nfor exciting future developments in the field.\n5\nAdditional Data Usage for Educational\nResearch\nThe ES-KT-24 dataset, with its rich game-based\nlearning data and multimodal information, offers\nnumerous opportunities for educational researchers\nbeyond traditional knowledge tracing. Here are\nseveral potential research directions that leverage\nthe unique aspects of our dataset:\n• Game Difficulty Classification: Researchers\ncan develop models to classify game difficulty\nlevels by analyzing the correlation between\ngameplay videos and students’ correct answer\nratios. This could lead to more accurate diffi-\nculty scaling in educational games and adap-\ntive learning systems.\n• Feature Impact Analysis: By examining the\nvarious features within the games, researchers\ncan identify which elements have the most\nsignificant impact on students’ performance.\nThis analysis could inform the design of more\neffective educational games and learning ma-\nterials.\n• Generative Game Design: Researchers could\nuse gameplay videos to explore the develop-\nment of AI models capable of generating sim-\nilar educational games. This approach could\nlead to the rapid prototyping of new educa-\ntional games tailored to specific learning ob-\njectives.\n• Multimodal Learning Analytics: The com-\nbination of gameplay videos, audio, and per-\nformance data enables researchers to conduct\nin-depth multimodal learning analytics. This\ncould reveal insights into how different modes\nof interaction affect learning outcomes.\n• Engagement and Performance Correlation:\nResearchers can investigate the relationship\nbetween student engagement (as observed in\nthe gameplay videos) and their performance,\npotentially uncovering new strategies for in-\ncreasing student motivation and learning effi-\nciency.\n• Cross-cultural Learning Patterns: With\ndata spanning multiple languages (Indonesian,\nMalaysian, English) and subjects, researchers\ncan explore cross-cultural learning patterns\nand how they might inform localized educa-\ntional strategies.\n• Temporal\nLearning\nDynamics:\nThe\ndataset’s temporal information allows for\nstudying\nhow\nlearning\npatterns\nevolve,\npotentially leading to a more nuanced\nunderstanding of knowledge retention and\noptimal spacing for learning sessions.\n6\nConclusion\nThe ES-KT-24 dataset significantly advances KT\nresearch, offering a unique combination of game-\nbased learning data and multimodal information.\nOur comprehensive evaluation of various KT mod-\nels on this dataset reveals the potential of language\nmodel-based approaches in capturing the nuances\nof student learning in interactive, game-based en-\nvironments.\nThe performance of LKT models,\nparticularly RoBERTa, underscores the value of\nleveraging pre-trained language models for KT\ntasks. However, the strong showing of SimpleKT\namong traditional DKT models indicates that well-\ndesigned conventional approaches remain compet-\nitive. These findings highlight the importance of\ncontinued research into LKT and DKT methods.\nBeyond KT, ES-KT-24 opens up numerous avenues\nfor educational research, including game difficulty\nclassification, multimodal learning analytics, and\ncross-cultural learning pattern analysis.\n8\n7\nLimitation\nThe ES-KT-24 dataset, while innovative, has sev-\neral limitations. Firstly, using duration time as the\nsole indicator of answer correctness may oversim-\nplify student interactions. Future iterations should\nimplement more nuanced classification criteria dur-\ning data collection. Secondly, the gameplay videos\nare from researchers rather than actual students,\npotentially limiting the dataset’s ecological valid-\nity. Including videos of children playing would\nprovide more authentic representations of learning\nbehaviors.\nAdditionally, expanding the depth and breadth of\nlog data would enable more comprehensive learn-\ning analytics studies. Lastly, while this dataset lays\nthe groundwork for multimodal knowledge tracing,\nit does not offer solutions for multimodal KT mod-\nels. This gap highlights a critical area for future\nresearch to develop models effectively utilizing\nmultifaceted educational data.\n8\nEthical Consideration\nIn this study, we prioritized ethical considerations\nin several key areas.\nFirstly, all data was pre-\nprocessed to ensure student anonymity and prevent\npersonal identification. We also used gameplay\nvideos recorded by researchers rather than actual\nstudents to further protect privacy and avoid ethical\nconcerns.\nIn the paper preparation process, we utilized\nClaude and GPT for paraphrasing to enhance read-\nability, strictly limiting their use to improving lin-\nguistic quality rather than generating content.\nWe employed GPT-4o and Whisper for data gen-\neration. It is important to note that, by OpenAI’s\nguidelines, the data generated using these tools\nshould be used solely for research purposes and not\nfor enhancing other generative models.\nThese measures are a testament to our unwaver-\ning commitment to maintaining ethical standards in\ndata handling, privacy protection, and the responsi-\nble use of AI tools in research.\nReferences\nGhodai Abdelrahman, Sherif Abdelfattah, Qing Wang,\nand Yu Lin. 2022. Dbe-kt22: A knowledge tracing\ndataset based on online student evaluation. arXiv\npreprint arXiv:2208.12651.\nJunyi\nAcademy.\n2015.\nJunyi\nacademy\nmath\npracticing log (to jan. 2015) partial samples.\nhttps:\/\/pslcdatashop.web.cmu.edu\/Files?\ndatasetId=1275. Accessed: 2024-09-10.\nPaulo Blikstein. 2013. Multimodal learning analytics.\nIn Proceedings of the third international conference\non learning analytics and knowledge, pages 102–106.\nPaulo Blikstein and Marcelo Worsley. 2016. Multi-\nmodal learning analytics and education data mining:\nUsing computational technologies to measure com-\nplex learning tasks. Journal of Learning Analytics,\n3(2):220–238.\nZalán Borsos, Raphaël Marinier, Damien Vincent,\nEugene Kharitonov, Olivier Pietquin, Matt Shar-\nifi, Dominik Roblek, Olivier Teboul, David Grang-\nier, Marco Tagliasacchi, et al. 2023. Audiolm: a\nlanguage modeling approach to audio generation.\nIEEE\/ACM transactions on audio, speech, and lan-\nguage processing, 31:2523–2533.\nYoungduck Choi, Youngnam Lee, Dongmin Shin,\nJunghyun Cho, Seoyon Park, Seewoo Lee, Jineon\nBaek, Chan Bae, Byungsoo Kim, and Jaewe Heo.\n2020. Ednet: A large-scale hierarchical dataset in\neducation. In Artificial Intelligence in Education:\n21st International Conference, AIED 2020, Ifrane,\nMorocco, July 6–10, 2020, Proceedings, Part II 21,\npages 69–73. Springer.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAlbert T Corbett and John R Anderson. 1994. Knowl-\nedge tracing: Modeling the acquisition of procedural\nknowledge. User modeling and user-adapted inter-\naction, 4:253–278.\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and\nHuaming Wang. 2023. Pengi: An audio language\nmodel for audio tasks. Advances in Neural Informa-\ntion Processing Systems, 36:18090–18108.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nAritra Ghosh, Neil Heffernan, and Andrew S Lan. 2020.\nContext-aware attentive knowledge tracing. In Pro-\nceedings of the 26th ACM SIGKDD international\nconference on knowledge discovery & data mining,\npages 2330–2339.\n9\nXiaopeng Guo, Zhijie Huang, Jie Gao, Mingyu Shang,\nMaojing Shu, and Jun Sun. 2021. Enhancing knowl-\nedge tracing via adversarial training. In Proceedings\nof the 29th ACM International Conference on Multi-\nmedia, pages 367–375.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. In The Eleventh International Conference on\nLearning Representations.\nNeil T Heffernan and Cristina Lindquist Heffernan.\n2014. The assistments ecosystem: Building a plat-\nform that brings scientists and teachers together for\nminimally invasive research on human learning and\nteaching. International Journal of Artificial Intelli-\ngence in Education, 24:470–497.\nDanial Hooshyar, Yueh-Min Huang, and Yeongwook\nYang. 2022. Gamedkt: Deep knowledge tracing in\neducational games. Expert Systems with Applica-\ntions, 196:116670.\nZ Lan. 2019. Albert: A lite bert for self-supervised\nlearning of language representations. arXiv preprint\narXiv:1909.11942.\nCharles Lang, George Siemens, Alyssa Friend Wise,\nDragan Gaševi´c, and Agathe Merceron, editors. 2022.\nThe Handbook of Learning Analytics. Society for\nLearning Analytics Research (SoLAR), Edmonton,\nAB, Canada.\nUnggi Lee, Jiyeong Bae, Yeonji Jung, Minji Kang,\nGyuri Byun, Yeonseo Lee, Dohee Kim, Sookbun\nLee, Jaekwon Park, Taekyung Ahn, et al. 2024a.\nFrom prediction to application: Language model-\nbased code knowledge tracing with domain adaptive\npre-training and automatic feedback system with ped-\nagogical prompting for comprehensive programming\neducation. arXiv preprint arXiv:2409.00323.\nUnggi Lee, Jiyeong Bae, Dohee Kim, Sookbun Lee,\nJaekwon Park, Taekyung Ahn, Gunho Lee, Damji\nStratton, and Hyeoncheol Kim. 2024b. Language\nmodel can do knowledge tracing: Simple but effec-\ntive method to integrate language model and knowl-\nedge tracing task. arXiv preprint arXiv:2406.02893.\nUnggi Lee, Minji Jeon, Yunseo Lee, Gyuri Byun,\nYoorim Son, Jaeyoon Shin, Hongkyu Ko, and Hyeon-\ncheol Kim. 2024c. Llava-docent: Instruction tun-\ning with multimodal large language model to sup-\nport art appreciation education.\narXiv preprint\narXiv:2402.06264.\nUnggi Lee, Yeil Jeong, Junbo Koh, Gyuri Byun, Yunseo\nLee, Hyunwoong Lee, Seunmin Eun, Jewoong Moon,\nCheolil Lim, and Hyeoncheol Kim. 2024d. I see you:\nTeacher analytics with gpt-4 vision-powered observa-\ntional assessment. arXiv preprint arXiv:2405.18623.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2024a. Visual instruction tuning. Advances in\nneural information processing systems, 36.\nQi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui\nXiong, Yu Su, and Guoping Hu. 2019. Ekt: Exercise-\naware knowledge tracing for student performance\nprediction. IEEE Transactions on Knowledge and\nData Engineering, 33(1):100–115.\nZitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang,\nand Weiqi Luo. 2023.\nsimplekt: A simple but\ntough-to-beat baseline for knowledge tracing.\nIn\nThe Eleventh International Conference on Learning\nRepresentations.\nZitao Liu, Qiongqiong Liu, Teng Guo, Jiahao Chen,\nShuyan Huang, Xiangyu Zhao, Jiliang Tang, Weiqi\nLuo, and Jian Weng. 2024b. Xes3g5m: A knowledge\ntracing benchmark dataset with auxiliary information.\nAdvances in Neural Information Processing Systems,\n36.\nPablo Moreno-Ger, Daniel Burgos, Iván Martínez-Ortiz,\nJosé Luis Sierra, and Baltasar Fernández-Manjón.\n2008. Educational game design for online education.\nComputers in Human Behavior, 24(6):2530–2540.\nPeña-Miguel Noemí and Sedano Hoyuelos Máximo.\n2014. Educational games for learning. Universal\nJournal of Educational Research, 2(3):230–238.\nXavier Ochoa, AWDG Charles Lang, and George\nSiemens. 2017. Multimodal learning analytics. The\nhandbook of learning analytics, 1:129–141.\nOpenAI. 2024. Hello GPT-4o. https:\/\/openai.com\/\nindex\/hello-gpt-4o\/. Accessed on September 10,\n2024.\nKDD Cup 2010 Organizers. 2010. Kdd cup 2010: Edu-\ncational data mining challenge. Accessed: 2024-09-\n10.\nNeurIPS 2020 Education Challenge Organizers. 2020.\nNeurips 2020 education challenge dataset. Accessed:\n2024-09-10.\nShalini Pandey and George Karypis. 2019.\nA self-\nattentive model for knowledge tracing.\narXiv\npreprint arXiv:1907.06837.\nShalini Pandey and Jaideep Srivastava. 2020.\nRkt:\nrelation-aware self-attention for knowledge tracing.\nIn Proceedings of the 29th ACM international con-\nference on information & knowledge management,\npages 1205–1214.\nGiani Petri, Christiane Gresse von Wangenheim, and\nAdriano Ferreti Borgatto. 2024. Meega+, system-\natic model to evaluate educational games. In En-\ncyclopedia of computer graphics and games, pages\n1112–1119. Springer.\nChris Piech, Jonathan Bassen, Jonathan Huang, Surya\nGanguli, Mehran Sahami, Leonidas J Guibas, and\nJascha Sohl-Dickstein. 2015. Deep knowledge trac-\ning. Advances in neural information processing sys-\ntems, 28.\n10\nOLI Engineering Statics Project. 2011. Oli engineering\nstatics - fall 2011. Accessed: 2024-09-10.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine Mcleavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In Proceedings of the 40th International\nConference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pages\n28492–28518. PMLR.\nVictor Sanh, L Debut, J Chaumond, and T Wolf. 2019.\nDistilbert, a distilled version of bert: Smaller, faster,\ncheaper and lighter. arxiv 2019.\narXiv preprint\narXiv:1910.01108.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34(05):8968–8975.\nChun Kit Yeung and Dit Yan Yeung. 2018. Address-\ning two problems in deep knowledge tracing via\nprediction-consistent regularization. In Proceedings\nof the 5th ACM Conference on Learning @ Scale,\npages 5:1–5:10. ACM.\nJiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan\nYeung. 2017. Dynamic key-value memory networks\nfor knowledge tracing. In Proceedings of the 26th\ninternational conference on World Wide Web, pages\n765–774.\nLiu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. A\nrobustly optimized BERT pre-training approach with\npost-training. In Proceedings of the 20th Chinese\nNational Conference on Computational Linguistics,\npages 1218–1227, Huhhot, China. Chinese Informa-\ntion Processing Society of China.\n11\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation.pdf"}
{"title":"Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games","authors":"Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks","summary":"Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps:\/\/dev1nw.github.io\/atari-gpt\/.","url":"http:\/\/arxiv.org\/abs\/2408.15950v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.15950v2","published":1724864936000,"comment":"Currently under review","pdf_text":"Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level\nPolicies in Atari Games\nNicholas R. Waytowich1, Devin White2, MD Sunbeam2, Vinicius G. Goecks1\n1DEVCOM Army Research Laboratory\n2 Army Educational Outreach Program\nnicholas.r.waytowich.civ@army.mil\nAbstract\nRecent advancements in large language models (LLMs)\nhave expanded their capabilities beyond traditional text-based\ntasks to multimodal domains, integrating visual, auditory, and\ntextual data. While multimodal LLMs have been extensively\nexplored for high-level planning in domains like robotics\nand games, their potential as low-level controllers remains\nlargely untapped. In this paper, we introduce a novel bench-\nmark aimed at testing the emergent capabilities of multimodal\nLLMs as low-level policies in Atari games. Unlike traditional\nreinforcement learning (RL) methods that require training\nfor each new environment and reward function specification,\nthese LLMs utilize pre-existing multimodal knowledge to di-\nrectly engage with game environments. Our study assesses\nthe performances of multiple multimodal LLMs against tra-\nditional RL agents, human players, and random agents, fo-\ncusing on their ability to understand and interact with com-\nplex visual scenes and formulate strategic responses. Our re-\nsults show that these multimodal LLMs are not yet capable of\nbeing zero-shot low-level policies. Furthermore, we see that\nthis is, in part, due to their visual and spatial reasoning. Ad-\nditional results and videos are available on our project web-\npage: https:\/\/dev1nw.github.io\/atari-gpt\/.\nIntroduction\nAdvancements in natural language processing, dataset scal-\ning, and model scaling have led to large language models,\nspecifically ChatGPT (GPT-3.5) (OpenAI 2022), which rev-\nolutionized text-to-text models. Evolving from these models\nare more advanced multimodal models with the ability to\ntake multiple types of input like text, images, and even au-\ndio, like GPT-4o and Gemini (OpenAI et al. 2024; Reid et al.\n2024; OpenAI 2024b). In addition, with each new iteration\nof these large multimodal models, we see vast improvements\nin efficiency. For example, the development of GPT-4 Turbo\nto GPT-4o to GPT-4o mini highlights the case where sacri-\nficing slight general capabilities improves the inference cost\nand speed (OpenAI 2024a).\nWith each development of these multimodal models, they\nshow potential beyond their traditional conversational task.\nResearchers have investigated their capabilities in areas like\nrobotics and high-level planning in automated systems (Li\net al. 2023; Rana et al. 2023). However, much of the current\nUnder Review\nliterature focuses on utilizing multimodal models for high-\nlevel planning (Xu et al. 2024), leaving their use as low-level\ncontrollers unexplored, akin to what is typically learned by\nreinforcement learning agents in complex environments like\nvideo games.\nTo investigate whether multimodal LLMs can function ef-\nfectively as low-level controllers, we perform initial tests\non GPT-4V (OpenAI et al. 2024), GPT-4o (OpenAI 2024b),\nGemini Flash (DeepMind 2024), and Claude 3 Haiku (An-\nthropic 2024) in Atari. Along with the raw performance of\neach of these models, we investigate their visual understand-\ning, spatial reasoning, and strategy formulation across mul-\ntiple environments.\nIn this paper, we show that these multimodal models are\nnot yet capable of zero-shot game-play in Atari. We found\nthat this is, in part, due to their inability to understand the\nvisual and spatial components of a given game-play image.\nWe do this by introducing a novel benchmark for multimodal\nLLMs to explore their emergent capabilities as low-level\npolicies in Atari games as outlined in Figure 1.\nAtari-GPT\nWe present a set of experiments designed to benchmark the\neffectiveness of multimodal LLMs as low-level decision-\nmaking agents in the domain of Atari video games, which\nwe refer to as “Atari-GPT”. Our primary focus is assess-\ning the models’ game-playing capabilities and performance\nmeasured by several factors: the game score, visual under-\nstanding, spatial reasoning, and proficiency in devising effi-\ncient game strategies.\nFirst, we evaluate the multimodal LLMs’ performance\nin playing Atari as a low-level policy, judged by each\ngame’s score. This assessment measures the models’ success\nby comparing their performance to standard reinforcement\nlearning algorithms, random agents, and human players, an-\nalyzing how well the models can act as low-level policies by\nmaking decisions based on the current game state.\nSecond, we examine the multimodal LLMs’ visual under-\nstanding and spatial reasoning capabilities. We do this by\ntesting how well the models properly identify different key\nvisual elements within a given frame, understand how these\nelements are related to one another spatially, and the ability\nof the models to create a meaningful strategy based on their\nscene understanding. Additionally, we test if the models are\narXiv:2408.15950v2  [cs.AI]  2 Dec 2024\nFigure 1: Atari-GPT: System diagram: illustrates the integration of a multimodal large language model (LLM) as a low-level\nagent within the Atari gaming environment. It highlights the flow of inputs from the game to the LLM and back, demonstrat-\ning how the model processes game observations and generates corresponding actions. Additionally, the diagram includes the\nframework for human evaluation, which assesses the LLM’s capabilities in visual understanding, spatial reasoning, strategic\nintuition, and environment recognition through a structured Q&A process.\nable to properly identify the game environment when given\nno context other than the image. For testing visual under-\nstanding and spatial reasoning, we use the same set of Atari\nenvironments used to evaluate game-play performance with\nthe addition of another environment, Basic Math.\nThis experimental structure provides a more comprehen-\nsive analysis of the decision-making processes of LLMs by\nassessing their overall understanding of the game environ-\nment within Atari video games, and evaluating their perfor-\nmance as low-level policies. Through this methodology, we\naim to establish a new benchmark for evaluating LLMs in\nlow-level control tasks, exploring how these language mod-\nels compare to humans and learning algorithms.\nExperimental Setup\nGame-Play Experiment\nWe conducted experiments using GPT-4V Turbo, GPT-4o,\nGemini 1.5 Flash and Claude 3 Haiku. We chose these mod-\nels because GPT-4V is considered state-of-the-art perfor-\nmance among the largest frontier LLMs at the time of writ-\ning this paper. GPT-4o, Gemini 1.5 Flash, and Claude 3\nHaiku were selected for their quicker inference speed, an\nimportant feature for real-time decision-making as a low-\nlevel policy. In our tests, the average inference time along\nwith the API call for GPT-4o, Gemini 1.5 Flash, and Claude\n3 Haiku was within 2-3 seconds, while GPT-4 Turbo had an\ninference time of 5-7 seconds.\nWe evaluated the performance in seven Atari games from\nthe Arcade Learning Environment (ALE) (Bellemare et al.\n2013): Space Invaders-v4, Breakout-v4, Seaquest-v4, Pong-\nv4, ALE\/Alien-v5, Ms. PacMan-v4, and ALE\/Frogger-v5.\nIn these experiments, the current game state was presented\nto the LLM, which then generated an action to be executed\nwithin the Atari environment. These models were used as\nlow-level policies, similar to how a reinforcement learning\npolicy, such as Deep Q-Networks (DQN) (Mnih et al. 2013),\nwould act in the environment.\nWe create a system prompt such that the output from the\nmodel is given in a JSON format with two keys, a reasoning\nkey containing the reasoning for why the model took an ac-\ntion and an action key that contains the numerical action the\nmodel would like to take:\n1\n{\n\"reasoning\": \"The player character\nis currently located at the bottom of\nthe screen, near an exit. The\nclosest enemy is directly in front,\none tile up, and could be threatening\nif no action is taken. The best\ncourse of action is to fire upwards\nto eliminate the threat and ensure\nthe path remains clear.\",\n\"action\":\n10 }\n(a) Pong\n(b) Breakout\n(c) Basic Math\n(d) Alien\n(e) Ms. Pacman\n(f) Frogger\n(g) Seaquest\n(h) Space Invaders\nFigure 2: Images used in Understanding tasks\nThis is an example from the environment ALE\/Alien-v5\nfrom GPT-4o. This format was used to encourage chain-of-\nthought reasoning to improve the game-playing performance\nof the LLM (Wei et al. 2023). The system prompt was used\nto maintain consistency in the structure of the output and\ninstruct the model to be a game-play assistant. In addition,\neach of the system prompts was tuned by providing the LLM\nwith the official documentation description of each of the\nAtari environments, specifically giving the model the action\nnames and numerical values, as detailed in the Appendix.\nSince not every frame needs to be given an action and\ninferencing LLMs is computationally intensive, we extend\nthe normal frame skipping of 4 frames in ALE (Bellemare\net al. 2013) to be 8 frames. With this new frame skipping\nwe then conduct a rollout of 1,000 timesteps, where at each\nstep, the model is provided a context buffer of the two pre-\nvious frames and responses, together with the current frame.\nFor the rollout there may be a terminal condition met when\nthe environment is reset, which results in the reward being\ncarried to the next episode. This is done because Atari does\nnot have terminal conditions based on a number of timesteps\nand we wanted to maintain consistency across the results.\nAdditionally, each frame generated is initially of size\n210x160x3 but resized to 512x512x3 for all models. We also\nintroduced error-handling code for cases when the model re-\nsponds with an invalid action, automatically replying to the\nLLM to correct its error.\nVisual And Spatial Reasoning\nWe also investigated the LLMs’ capability to understand and\nreason based on a game frame image. We evaluated the fol-\nlowing models: GPT-4V Turbo, GPT-4o, Gemini 1.5 Flash,\nClaude 3 Haiku, Gemini Pro 1.5, Gemini Ultra 1.0, Gemini\nPro 1.0, Claude 3 Opus, and Claude 3 Sonnet. This allowed\nus to assess the state of each frontier LLM and compare\ntheir performance across different model types and sizes,\nand across eight environments, as shown in Figure 2. All\nmodels were tested using their respective web interfaces.\nWe created a set of prompts to investigate the models’\nvisual reasoning, spatial reasoning, strategic intuition, and\nability to identify the environment:\n• Visual Understanding: Identify all the key elements in\nthis image. Be specific. Use at most 100 words.\n• Spatial Reasoning: Where are the key elements located\nrelative to each other? Be specific with respect to their\nposition in the image. Use at most 100 words.\n• Strategy: The given image is a screenshot of a game.\nDescribe the ideal next move if you were playing this\ngame. Be specific. Use at most 100 words.\n• Identification: Knowing that the image came from an\nAtari game, identify its name. Be specific.\nTo quantitatively evaluate the performance of the model out-\nputs, we created a rubric outlining the basic answers to\nthe proposed questions, as seen in the Appendix (Table 2).\nGiven that there are several acceptable actions and strate-\ngies, we do not directly define a single correct action or plan\nfor each state. In cases where we investigate the acceptable\nstrategy, we rather evaluate it as either a direct action or strat-\negy\/plan that does not put the agent in harm. Harm includes\nlosing a life or losing points within a game.\nFor each environment, we resize the original frame from\n210x160x3 to 1000x1000x3 and query the LLM together\nwith the visual reasoning prompt. Once a response was re-\nceived, we sent the spatial reasoning prompt, followed by\nthe strategic and identification prompts, respectively. After\nreceiving all outputs, we compared the multimodal LLMs’\noutput with the rubric, resulting in a percent score for that\nenvironment. We repeated this for all environments and\ncomputed the average score over four different trials.\nResults\nGame-Playing Performance\nWe evaluate GPT-4V Turbo, GPT-4o, Gemini 1.5 Flash, and\nClaude 3 Haiku across seven Atari environments and com-\npare their scores to a random agent, trained reinforcement\nlearning agent, and human. For each model, we perform\nfour rollouts of 1,000 timesteps and average their cumulative\nreward. We then normalize this average cumulative reward\nagainst the human scores, resulting in a normalized cumula-\ntive reward that relates the LLM scores to the human scores.\nAs seen in Figure 3, GPT-4o performed the best on av-\nerage with a normalized performance of 23.2% and Gemini\n1.5 Flash performed the worst on average with a normalized\nperformance of 8.5%. GPT-4V Turbo presented the second-\nbest performance with a normalized score of 18.36%, and\nClaude 3 Haiku had a normalized performance of 12.36%.\nFigure 4 breaks down the normalized reward for each envi-\nronment, illustrating that the most challenging game for the\nLLM-based policy was Pong.\nFigure 3: Normalized Average Reward for GPT-4V Turbo,\nGPT-4o, and Gemini 1.5 Flash.\nTable 1 presents the raw game-play performance of\nthe four LLMs across the Atari environments. This ta-\nble also includes the performance of human players, pre-\nFigure 4: Average Human Normalized reward for each envi-\nronment.\ntrained Deep Q-Network (DQN) reinforcement learning\nmodels (Gogianu et al. 2022), and random agents. While\na pre-trained DQN model(Gogianu et al. 2022) trained for\n49,750,000 steps was used for all other environments, a cus-\ntom DQN model was trained from scratch for 1,000,000\ntimesteps for ALE\/Frogger-v5 due to the lack of a pre-\ntrained model. The LLMs did not match the performance\nof the human players or the RL agents. However, they out-\nperformed the random agents, demonstrating a meaningful\nlevel of understanding and ability to play the games. This is\nan important finding, as it indicates that the LLMs are not\nmerely generating random actions but are making decisions\nthat reflect a basic comprehension of the game mechanics.\nSample videos for all rollouts are available in the project\nwebpage1.\nVisual And Spatial Reasoning\nWe further explored the factors influencing game-play per-\nformance by testing the visual, spatial, strategic, and game\nenvironment identification abilities of these LLMs. For each\nenvironment, we evaluated GPT-4V, GPT-4o, Gemini 1.5\nFlash, and Claude 3 Haiku using four designed prompts,\nwhich provided insight into why the models may not have\nperformed as well as low-level policies.\nFigure 5 displays the percentage of correct outputs for\neach of the four tasks—visual, spatial, acceptable strategy,\nand identification—across two runs for each model. GPT-\n4o consistently excelled across all tasks, demonstrating high\naccuracy in visual understanding, strategy formulation, and\nenvironment identification. However, it exhibited a notice-\nable decline in spatial reasoning accuracy. This pattern was\nconsistent across all models, suggesting that spatial reason-\ning remains a significant challenge for multimodal large lan-\nguage models and possibly accounting for their relatively\npoor performance on the game-playing tasks. Comprehen-\nsive results for each environment and all models can be\nfound in the Appendix.\n1Atari-GPT project webpage: https:\/\/sites.google.com\/view\/\natari-gpt\/.\nTable 1: Cumulative Reward for 1000 steps without In-Context Learning, * - Custom DQN model trained for 1,000,000\ntimesteps\nEnvironments\nRandom\nAgent\nRL\nAgent\nHuman\nGPT-4V\nTurbo\nGPT-4o\nGemini\n1.5\nFlash\nClaude\n3 Haiku\nFrogger\n26\n30*\n325\n61.25\n66.25\n5.25\n46.5\nBreakout\n3\n23\n37\n5.75\n9.75\n0\n3.25\nPong\n-20\n-8\n2\n-25.25\n-22.5\n-26\n-26\nSpaceInvaders\n100\n725\n575\n258.75\n272.5\n233.75\n197.5\nSeaquest\n80\n620\n680\n105\n135\n15\n40\nAlien\n270\n1670\n2480\n465\n532.5\n80\n305\nMs. Pacman\n280\n3780\n4220\n517.5\n610\n497.5\n395\nFigure 5: Visual, spatial, strategic and identification results.\nPercent average for 2 runs.\nDiscussion\nThis study represents one of the first attempts at benchmark-\ning the emergent capability of multimodal LLMS to act as\nlow-level controllers in Atari game environments, a signifi-\ncant departure from their traditional applications in language\nand visual tasks. The results, while not meeting the perfor-\nmance levels of human players or dedicated reinforcement\nlearning (RL) models, showcase the potential and limita-\ntions of LLMs in this context.\nOur experiments demonstrate that while LLMs exhibit\nsome ability to identify and interact with key elements\nwithin game frames, their performance as low-level con-\ntrollers is subpar, likely due to a lack of training for this\ntask as well as difficulty in spatial reasoning. We observed\na significant performance gap between GPT-4o and Claude\n3 Haiku and Gemini 1.5 Flash. In most cases, we observed\nthat models performed better than random. Though we saw\nperformance worse than random for Pong on all models,\nlikely due to the speed and accuracy requirements to prop-\nerly play the game, and in multiple environments for Gem-\nini 1.5 Flash, likely due to the size of the model. We ob-\nserved neither large nor small models are capable of acting\nas zero-shot low-level controllers. While large models can\ncomprehend the visual content fairly well, they struggle to\nconvert this to spatial reasoning, which makes choosing a\ncorrect action more difficult. This error compounded over\n1,000 frames resulted in poor performance when compared\nto a human player.\nThroughout our testing, we found another key element to\nbe inference time. For these models to realistically be used\nfor game-play tasks they will not only need to be able to see\nan image, interpret, and provide a correct action, but they\nwill need to be quick enough for real-time decision-making.\nOur experiments show that these multimodal models still\nlack enough speed for acting as real-time low-level policies,\nas Gemini 1.5 Flash was the best in terms of inference time\nwith an average inference taking roughly 2 seconds.\nA challenge we encountered was the inconsistency of the\nmodel’s outputs, with GPT-4V Turbo occasionally failing\nto generate appropriate responses coupled with the above-\nmentioned inference time of 5-7 seconds to inference. In ad-\ndition, rate limits for OpenAI, Anthropic, and Google APIs\ncontributed heavily to much longer experimentation time,\nadding more overhead to the inherent inference time of these\nmodels. The imposed rate limits currently make it impossi-\nble to run real-time experiments, highlighting the need for\nbetter and faster local multimodal LLMs for fast-paced, low-\nlevel decision-making tasks.\nConclusions\nDespite these setbacks, the findings are invaluable for sev-\neral reasons. First, they contribute to our understanding of\nthe current emergent capabilities and boundaries of LLMs\nwhen applied to low-level control tasks. Second, they offer\na new benchmark for the AI research community to mea-\nsure the progress of LLMs in handling dynamic and visu-\nally complex environments. Adjustments such as tuning the\nmodels’ temperature settings demonstrated some mitigation\nof output inconsistency, suggesting pathways for refining\nLLM performance in these tasks.\nImportantly, the continuous updates to LLM architectures\nand training methods suggest that the capabilities of these\nmodels will evolve, potentially overcoming some of the cur-\nrent deficiencies noted in our study. As such, this research\nshould be viewed as a foundational step that sets the stage\nfor future investigations, encouraging ongoing refinement\nand adaptation of LLMs for applications requiring detailed\nenvironmental interactions and decision-making.\nWhile LLMs have not yet reached the level of proficiency\nrequired to match the best human or RL performances in\nAtari gameplay, their ability to engage in this task at all\nis notable. It demonstrates the adaptability and potential of\nLLMs to extend beyond their original training confines, of-\nfering a glimpse into future emergent applications where\nthese models could serve as more general low-level con-\ntrollers.\nRelated Work\nMultimodal Large Language Models\nProcessing multimodal inputs such as images and sequen-\ntial data has undergone constant evolution in the do-\nmain of deep learning. Before the transformer architec-\nture (Vaswani et al. 2023), Convolutional Neural Networks\n(CNNs) (LeCun et al. 1998; Krizhevsky, Sutskever, and Hin-\nton 2012) for visual processing and Recurrent Neural Net-\nworks (RNNs) (Mikolov et al. 2010) for handling sequen-\ntial data such as text or audio represented the state of the\nart (Mao et al. 2015). Data was processed through separate\ninput networks and their latest outputs were combined via\ndifferent fusion strategies (Mao et al. 2015). Despite achiev-\ning notable success, these approaches were limited in their\nscale and capacity to capture the intricate interactions be-\ntween different modalities, primarily due to the inherent lim-\nitations in sequential data processing and cross-modal syn-\nthesis (Chung et al. 2019).\nThe advent of transformers introduced a more effec-\ntive and scalable mechanism for processing sequential data\nthrough self-attention mechanisms (Vaswani et al. 2023).\nAmong the key developments was the creation of CLIP\n(Contrastive Language-Image Pre-training) (Radford et al.\n2021), which leveraged transformers to learn a common la-\ntent space for both visual and linguistic data, leading to a\nmodel that could correlate images in the context of natu-\nral language. This development led to some of the most in-\nfluential Multimodal Large Language Models available to-\nday such as GPT-4 Vision (OpenAI et al. 2024), Gemini\nPro 1.5 (Reid et al. 2024), Gemini Ultra and Pro 1.0 (Team\net al. 2024), Ferret (You et al. 2023), Vicuna (Chiang et al.\n2023), Claude 3 (Anthropic 2024), Multimodal Large Lan-\nguage and Vision Assistant (Liu et al. 2023) and LLaVa (Liu\net al. 2023). Since then, multimodal LLMs have been ap-\nplied to different domains such as designing reward func-\ntions (Ma et al. 2023) and controlling general game-playing\nagents (Abi Raad et al. 2024).\nMultimodal LLMs as Low-Level Policies for\nGames\nLow-level policies act as controllers, processing observa-\ntions from the environment and returning actions. The ac-\ncessibility and complexity of games make them ideal bench-\nmarks for evaluating the performance of such policies (Mnih\net al. 2013; Badia et al. 2020). Traditionally, video game-\nplaying policies have employed reinforcement learning al-\ngorithms (Mnih et al. 2013), behavior cloning (Hussein et al.\n2017), or a combination of both (Goecks et al. 2019). Given\nthe increased performance of multimodal LLMs, they have\nemerged as an alternative to these methods.\nThe rationale for employing multimodal LLMs as low-\nlevel policies in gaming is grounded in their distinctive ca-\npabilities and how they align with the demands of various\ngame environments. When playing social games against one\nanother, LLMs perform well when playing games that re-\nquire valuing their self-interest but sub-optimally when they\nneed to coordinate with other players (Akata et al. 2023).\nWhen fine-tuned on gameplay data, LLMs have been shown\nto learn an internal representation of game states that can be\nused to make predictions (Li et al. 2022). Given their natu-\nral language processing capabilities, LLMs can also directly\nlearn from human-written game manuals to accelerate learn-\ning and improve their performance (Wu et al. 2024).\nSeveral works have demonstrated the capabilities of\nLLMs when playing games. Gato (Reed et al. 2022) lever-\nages a transformer architecture (Vaswani et al. 2023) similar\nto LLMs to tokenize multimodal data from multiple tasks,\nincluding playing games and robotic control, to train a gen-\neralist policy. The same model with the same weights can\nthen play games, caption images, control robotic arms, chat,\nand others. CICERO ( FAIR) leveraged LLMs to combine\nstrategic reasoning and natural language to cooperate, ne-\ngotiate, and coordinate with other players to play the game\nDiplomacy at a human level. LLMs have also been em-\nployed to solve text-based games (Yao et al. 2020; Tsai et al.\n2023) and directly write code to convey more complex be-\nhaviors when solving open-ended tasks in Minecraft (Wang\net al. 2023).\nWhile the applications of LLMs in gaming have demon-\nstrated considerable success across a variety of con-\ntexts (Gallotta et al. 2024), a comprehensive exploration of\nthese multimodal capabilities remains unexplored. In this\nwork, we address this gap by specifically investigating their\nvisual, spatial reasoning, and strategic capabilities when\nplaying Atari games.\nAcknowledgements\nThis research was sponsored by the Army Research Lab-\noratory and was accomplished under Cooperative Agree-\nment Number W911NF-23-2-0072. The views and conclu-\nsions contained in this document are those of the authors and\nshould not be interpreted as representing the official policies,\neither expressed or implied, of the Army Research Labora-\ntory or the U.S. Government. The U.S. Government is au-\nthorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\nReferences\nAbi Raad, M.; Ahuja, A.; Barros, C.; Besse, F.; Bolt, A.;\nBolton, A.; Brownfield, B.; Buttimore, G.; Cant, M.; Chak-\nera, S.; et al. 2024. Scaling instructable agents across many\nsimulated worlds. arXiv e-prints, arXiv–2404.\nAkata, E.; Schulz, L.; Coda-Forno, J.; Oh, S. J.; Bethge, M.;\nand Schulz, E. 2023. Playing repeated games with large lan-\nguage models. arXiv preprint arXiv:2305.16867.\nAnthropic. 2024.\nIntroducing the next generation of\nClaude.\n2024 https:\/\/www.anthropic.com\/news\/claude-3-\nfamily. (Accessed: 16 April 2024).\nBadia, A. P.; Piot, B.; Kapturowski, S.; Sprechmann,\nP.; Vitvitskyi, A.; Guo, D.; and Blundell, C. 2020.\nAgent57: Outperforming the Atari Human Benchmark.\narXiv:2003.13350.\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. Journal of Artificial Intelli-\ngence Research, 47: 253–279.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; et al.\n2023.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https:\/\/vicuna.lmsys.org (ac-\ncessed 14 April 2023), 2(3): 6.\nChung, S.; Lim, J.; Noh, K. J.; Kim, G.; and Jeong, H. 2019.\nSensor data acquisition and multimodal sensor fusion for\nhuman activity recognition using deep learning.\nSensors,\n19(7): 1716.\nDeepMind, G. 2024.\nGemini Flash.\nhttps:\/\/deepmind.\ngoogle\/technologies\/gemini\/flash\/.\n(Accessed: 20 May\n2024).\n(FAIR)†, M. F. A. R. D. T.; Bakhtin, A.; Brown, N.; Dinan,\nE.; Farina, G.; Flaherty, C.; Fried, D.; Goff, A.; Gray, J.; Hu,\nH.; et al. 2022. Human-level play in the game of Diplo-\nmacy by combining language models with strategic reason-\ning. Science, 378(6624): 1067–1074.\nGallotta, R.; Todd, G.; Zammit, M.; Earle, S.; Liapis, A.;\nTogelius, J.; and Yannakakis, G. N. 2024. Large Language\nModels and Games: A Survey and Roadmap. arXiv preprint\narXiv:2402.18659.\nGoecks, V. G.; Gremillion, G. M.; Lawhern, V. J.; Valasek,\nJ.; and Waytowich, N. R. 2019. Integrating behavior cloning\nand reinforcement learning for improved performance in\ndense and sparse reward environments.\narXiv preprint\narXiv:1910.04281.\nGogianu, F.; Berariu, T.; Bus,oniu, L.; and Burceanu, E.\n2022. Atari Agents.\nHussein, A.; Gaber, M. M.; Elyan, E.; and Jayne, C. 2017.\nImitation learning: A survey of learning methods.\nACM\nComputing Surveys (CSUR), 50(2): 1–35.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25.\nLeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.\nGradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11): 2278–2324.\nLi, B.; Wu, P.; Abbeel, P.; and Malik, J. 2023. Interactive\nTask Planning with Language Models. arXiv:2310.10645.\nLi, K.; Hopkins, A. K.; Bau, D.; Vi´egas, F.; Pfister, H.; and\nWattenberg, M. 2022. Emergent world representations: Ex-\nploring a sequence model trained on a synthetic task. arXiv\npreprint arXiv:2210.13382.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual Instruc-\ntion Tuning. arXiv:2304.08485.\nMa, Y. J.; Liang, W.; Wang, G.; Huang, D.-A.; Bastani,\nO.; Jayaraman, D.; Zhu, Y.; Fan, L.; and Anandkumar, A.\n2023.\nEureka: Human-Level Reward Design via Coding\nLarge Language Models. arXiv:2310.12931.\nMao, J.; Xu, W.; Yang, Y.; Wang, J.; Huang, Z.; and Yuille,\nA. 2015. Deep Captioning with Multimodal Recurrent Neu-\nral Networks (m-RNN). arXiv:1412.6632.\nMikolov, T.; Karafi´at, M.; Burget, L.; Cernock`y, J.; and Khu-\ndanpur, S. 2010. Recurrent neural network based language\nmodel. In Interspeech, volume 2, 1045–1048. Makuhari.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013.\nPlaying\nAtari\nwith\nDeep\nReinforcement\nLearning.\narXiv:1312.5602.\nOpenAI. 2022. Introducing ChatGPT. 2022 https:\/\/openai.\ncom\/blog\/chatgpt. (Accessed: 27 March 2024).\nOpenAI. 2024a.\nGPT-4o mini: advancing cost-efficient\nintelligence.\n2024 https:\/\/openai.com\/index\/gpt-4o-mini-\nadvancing-cost-efficient-intelligence\/.\n(Accessed: 6 Nov\n2024).\nOpenAI. 2024b. Hello GPT-4o. 2024 https:\/\/openai.com\/\nindex\/hello-gpt-4o\/. (Accessed: 20 May 2024).\nOpenAI; Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.;\nAkkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.;\nAltman, S.; Anadkat, S.; et al. 2024. GPT-4 Technical Re-\nport. arXiv:2303.08774.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nRana, K.; Haviland, J.; Garg, S.; Abou-Chakra, J.; Reid, I.;\nand Suenderhauf, N. 2023. SayPlan: Grounding Large Lan-\nguage Models using 3D Scene Graphs for Scalable Robot\nTask Planning. arXiv:2307.06135.\nReed, S.; Zolna, K.; Parisotto, E.; Colmenarejo, S. G.;\nNovikov, A.; Barth-Maron, G.; Gimenez, M.; Sulsky, Y.;\nKay, J.; Springenberg, J. T.; et al. 2022. A generalist agent.\narXiv preprint arXiv:2205.06175.\nReid, M.; Savinov, N.; Teplyashin, D.; Lepikhin, D.; Lilli-\ncrap, T.; baptiste Alayrac, J.; Soricut, R.; Lazaridou, A.; Fi-\nrat, O.; Schrittwieser, J.; Antonoglou, I.; et al. 2024. Gemini\n1.5: Unlocking multimodal understanding across millions of\ntokens of context. arXiv:2403.05530.\nTeam, G.; Anil, R.; Borgeaud, S.; Alayrac, J.-B.; Yu, J.; Sori-\ncut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; Millican, K.;\nand Silver, D. 2024. Gemini: A Family of Highly Capable\nMultimodal Models. arXiv:2312.11805.\nTsai, C. F.; Zhou, X.; Liu, S. S.; Li, J.; Yu, M.; and Mei,\nH. 2023. Can large language models play text games well?\ncurrent state-of-the-art and open questions. arXiv preprint\narXiv:2304.02868.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2023. At-\ntention Is All You Need. arXiv:1706.03762.\nWang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.; Zhu,\nY.; Fan, L.; and Anandkumar, A. 2023. Voyager: An open-\nended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023.\nChain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. arXiv:2201.11903.\nWu, Y.; Fan, Y.; Liang, P. P.; Azaria, A.; Li, Y.; and Mitchell,\nT. M. 2024. Read and reap the rewards: Learning to play\natari with the help of instruction manuals. Advances in Neu-\nral Information Processing Systems, 36.\nXu, Z.; Wu, K.; Wen, J.; Li, J.; Liu, N.; Che, Z.; and Tang,\nJ. 2024. A Survey on Robotics with Foundation Models:\ntoward Embodied AI. arXiv:2402.02385.\nYao, S.; Rao, R.; Hausknecht, M.; and Narasimhan, K. 2020.\nKeep calm and explore: Language models for action genera-\ntion in text-based games. arXiv preprint arXiv:2010.02903.\nYou, H.; Zhang, H.; Gan, Z.; Du, X.; Zhang, B.; Wang,\nZ.; Cao, L.; Chang, S.-F.; and Yang, Y. 2023. Ferret: Re-\nfer and Ground Anything Anywhere at Any Granularity.\narXiv:2310.07704.\nAppendix\nGameplaying Performance Prompts\n• Alien. “You are a game playing assistant and will be provided an image. This will be of the game Alien, your goal is to\nprovide me with what you believe to be the best action I could take to beat the game. Think about all possible actions and why\neach action is or is not the best action to take. The potential actions I can take are ’0’ NOOP, ’1’ FIRE, ’2’ UP, ’3’ RIGHT,\n’4’ LEFT, and ’5’ DOWN, ’6’ UPRIGHT, ’7’ UPLEFT, ’8’ DOWNRIGHT, and ’9’ DOWNLEFT,’10’: UPFIRE, ’11’:\nRIGHTFIRE, ’12’: LEFTFIRE, ’13’: DOWNFIRE, ’14’: UPRIGHTFIRE, ’15’: UPLEFTFIRE, ’16’: DOWNRIGHTFIRE,\n’17’: DOWNLEFTFIRE. Provide output as a json structured as reasoning: reasoning for actions and why to choose an action,\naction: The environment action which would provide the best next state. The action key should only have the action I should\ntake for the current frame as a number.”\n• Breakout. “You are a game playing assistant and will be provided an image. This will be of the game Breakout, your goal\nis to provide me with what you believe to be the best action I could take to beat the game. Think about all possible actions\nand why each action is or is not the best action to take. The potential actions I can take are ‘0’: NOOP ‘1’: FIRE ‘2’: RIGHT\n‘3’: LEFT. Provide output as a json structured as reasoning: reasoning for actions and why to choose an action, action: The\nenvironment action which would provide the best next state. The action key should only have the action I should take for the\ncurrent frame as a number.”\n• Frogger. “You are a game playing assistant and will be provided an image. This will be of the game Frogger, your goal is\nto provide me with what you believe to be the best action I could take to beat the game. Think about all possible actions\nand why each action is or is not the best action to take. The potential actions I can take are ‘0’: NOOP ‘1’: UP ‘2’: RIGHT\n‘3’: LEFT ‘4’: DOWN. Provide output as a json structured as reasoning: reasoning for actions and why to choose an action,\naction: The environment action which would provide the best next state. The action key should only have the action I should\ntake for the current frame as a number.”\n• Ms. Pacman. “You are a game playing assistant and will be provided an image. This will be of the game Ms. Pacman, your\ngoal is to provide me with what you believe to be the best action I could take to beat the game. Think about all possible\nactions and why each action is or is not the best action to take. The potential actions I can take are ‘0’: NOOP ‘1’: UP ‘2’:\nRIGHT ‘3’: LEFT ‘4’: DOWN ‘5’: UPRIGHT ‘6’: UPLEFT ‘7’: DOWNRIGHT ‘8’: DOWNLEFT. Provide output as a json\nstructured as reasoning: reasoning for actions and why to choose an action, action: The environment action which would\nprovide the best next state. The action key should only have the action I should take for the current frame as a number.”\n• Pong. “You are a game playing assistant and will be provided an image. This will be of the game Pong, your goal is to\nprovide me with what you believe to be the best action I could take to beat the game. Think about all possible actions and\nwhy each action is or is not the best action to take. The potential actions I can take are ‘0’: NOOP ‘1’: FIRE ‘2’: RIGHT ‘3’:\nLEFT ‘4’: RIGHTFIRE ‘5’: LEFTFIRE. Provide output as a json structured as reasoning: reasoning for actions and why to\nchoose an action, action: The environment action which would provide the best next state. The action key should only have\nthe action I should take for the current frame as a number.”\n• Seaquest. “You are a game playing assistant and will be provided an image. This will be of the game Seaquest, your goal is to\nprovide me with what you believe to be the best action I could take to beat the game. Think about all possible actions and why\neach action is or is not the best action to take. The potential actions I can take are ’0’ NOOP, ’1’ FIRE, ’2’ UP, ’3’ RIGHT,\n’4’ LEFT, and ’5’ DOWN, ’6’ UPRIGHT, ’7’ UPLEFT, ’8’ DOWNRIGHT, and ’9’ DOWNLEFT,’10’: UPFIRE, ’11’:\nRIGHTFIRE, ’12’: LEFTFIRE, ’13’: DOWNFIRE, ’14’: UPRIGHTFIRE, ’15’: UPLEFTFIRE, ’16’: DOWNRIGHTFIRE,\n’17’: DOWNLEFTFIRE. Provide output as a json structured as reasoning: reasoning for actions and why to choose an action,\naction: The environment action which would provide the best next state. The action key should only have the action I should\ntake for the current frame as a number.”\n• Space Invaders. “You are a game playing assistant and will be provided an image. This will be of the game Space Invaders,\nyour goal is to provide me with what you believe to be the best action I could take to beat the game. Think about all possible\nactions and why each action is or is not the best action to take. The potential actions I can take are ‘0’ NOOP ‘1’ FIRE ‘2’\nRIGHT ‘3’ LEFT ‘4’ RIGHTFIRE ‘5’ LEFTFIRE. Provide output as a json structured as reasoning: reasoning for actions\nand why to choose an action, action: The environment action which would provide the best next state. The action key should\nonly have the action I should take for the current frame as a number.”\nGround Truth Answers for Visual and Spatial Reasoning.\nTable 2: Ground truth values used by human evaluators to score performance of LLMs when answering questions about game\nimages.\nEnvironment\nVisual\nSpatial\nAlien\nPlayer, 2 Aliens, orbs, some\npower up, score (60), lifes\n(2)\nPlayer is in the center, one alien is below the cen-\nter, other alien is center bottom, all orbs are on the\nleft, score is at the bottom middle left and life’s are\nbottom left\nBasic Math\n3 numbers (5, 2, 9), addition\nsign, 2 horizontal lines\n5 is at the middle top, 2 is below 5, the addition sign\nis to the left of the 2, one horizontal line is below\nthe 2, 9 is below that horizontal line and the other\nhorizontal line is below the 9\nBreakout\nScore (2), 5 lives, 1 (I am\nnot sure what this is), a 6\nlines of bricks with different\ncolors, a red ball and a pad-\ndle\nScore is top left, life’s is top middle, 1 is top right,\nlines of bricks are in the center near the top of the\ngameplay area, red ball is middle left and paddle is\nbottom right\nFrogger\nVehicles\n(9),\nlife’s\n(4),\nplayer, logs (7), leaves (10),\nscore (2)\n9 vehicles all at the bottom half of the screen, life’s\nis at the bottom left, the player is at the bottom right\nin-between vehicles, logs are on the top half of the\nscreen, leaves are at the top half of the screen, score\nis at the top center\nMs. Pacman\nMs. Pacman, red ghost, blue\nghost, pink ghost, orange\nghost, orbs, power ups (2),\nlife’s (2), score (40), cherry\nMs. Pacman is in the center, red ghost is top left,\nblue ghost is middle top, pink ghost is center, or-\nange ghost is middle right, orbs are throughout the\nenvironment, 2 power ups on top left and bottom\nleft, 2 lifes are bottom left, score is bottom center,\ncherry is bottom right\nPong\n2 paddles, a ball and 2\nscores (0,0)\nOrange paddle top left, green paddle, middle right,\nball top center, orange score top left, green score\ntop right\nSeaquest\nSubmarine, shot, fish (2),\ndivers (2), oxygen, Activi-\nsion, life’s (2), score\nSubmarine is center, shot is to the left of the subma-\nrine, one fish is directly below the submarine and\nthe other is directly below that fish, one diver is\nabove the submarine and the other is to the center\nleft of the screen, oxygen bar is bottom left and is\nalmost full, Activision logo is bottom center of the\nscreen, the life’s are at the top center of the screen\nand the score is above and to the right of the life’s\nSpace Invaders\nAliens (33), home base (3),\nplayer, shot, score, time(?)\nAliens are aligned in the throughout the middle of\nthe game-play area, home bases are bottom cen-\nter with one damaged, player is bottom left, shot\nis center left, score is top left, time is top right\nComprehensive Understanding Results\nFigure 6: Comprehensive Understanding Test results.\nIndividual Performance for Visual and Spatial Reasoning\n(a) Visual performance\n(b) Spatial performance\n(c) Strategic performance\n(d) Identification performance\nFigure 7: Percent Performance for Individual Environments\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games.pdf"}
{"title":"Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks","authors":"Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Lin Liu, Junfeng Yang, Lin Lai, Hongyang Qin, Minwen Deng, Juchao Zhuo, Deheng Ye, Qiang Fu, Wei Yang, Guang Yang, Lanxiao Huang, Xiangyang Ji","summary":"The advancement of Offline Reinforcement Learning (RL) and Offline\nMulti-Agent Reinforcement Learning (MARL) critically depends on the\navailability of high-quality, pre-collected offline datasets that represent\nreal-world complexities and practical applications. However, existing datasets\noften fall short in their simplicity and lack of realism. To address this gap,\nwe propose Hokoff, a comprehensive set of pre-collected datasets that covers\nboth offline RL and offline MARL, accompanied by a robust framework, to\nfacilitate further research. This data is derived from Honor of Kings, a\nrecognized Multiplayer Online Battle Arena (MOBA) game known for its intricate\nnature, closely resembling real-life situations. Utilizing this framework, we\nbenchmark a variety of offline RL and offline MARL algorithms. We also\nintroduce a novel baseline algorithm tailored for the inherent hierarchical\naction space of the game. We reveal the incompetency of current offline RL\napproaches in handling task complexity, generalization and multi-task learning.","url":"http:\/\/arxiv.org\/abs\/2408.10556v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.10556v2","published":1724132330000,"comment":null,"pdf_text":"Hokoff: Real Game Dataset from Honor of Kings and\nits Offline Reinforcement Learning Benchmarks\nYun Qu∗†, Boyuan Wang∗†, Jianzhun Shao∗†, Yuhang Jiang†, Chen Chen†, Zhenbin Ye♮, Lin\nLiu♮, Junfeng Yang♮, Lin Lai♮, Hongyang Qin§, Minwen Deng§, Juchao Zhuo§, Deheng Ye§,\nQiang Fu§, Wei Yang§, Guang Yang♮, Lanxiao Huang♮, Xiangyang Ji†\n†Tsinghua University, ♮Tencent Timi Studio, §Tencent AI Lab\n{qy22,wangby22,sjz18,jiangyh19}@mails.tsinghua.edu.cn,chenchen.peach@gmail.com,\n{zhenbinye,lincliu,fengjunyang,linlai,hongyangqin,danierdeng,jojozhuo,dericye,leonfu,\nwillyang,mikoyang,jackiehuang}@tencent.com,xyji@tsinghua.edu.cn\nAbstract\nThe advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent\nReinforcement Learning (MARL) critically depends on the availability of high-\nquality, pre-collected offline datasets that represent real-world complexities and\npractical applications. However, existing datasets often fall short in their simplicity\nand lack of realism. To address this gap, we propose Hokoff, a comprehensive set\nof pre-collected datasets that covers both offline RL and offline MARL, accompa-\nnied by a robust framework, to facilitate further research. This data is derived from\nHonor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game\nknown for its intricate nature, closely resembling real-life situations. Utilizing this\nframework, we benchmark a variety of offline RL and offline MARL algorithms.\nWe also introduce a novel baseline algorithm tailored for the inherent hierarchi-\ncal action space of the game. We reveal the incompetency of current offline RL\napproaches in handling task complexity, generalization and multi-task learning.\n1\nIntroduction\nOnline Reinforcement Learning (Online RL) relies on the interaction between the training policy\nand the environment for data collection and policy optimization [37, 18, 7]. However, this paradigm\nmakes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous\ndriving [23, 18], as deploying untested policies to the environment can be costly and dangerous [19].\nIn contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a\nfixed dataset without the need for further interaction with the environment [23, 18, 7, 19]. This\ncharacteristic alleviates the aforementioned issue, making offline RL potentially more suitable for\ncertain real-world scenarios compared to online RL [23].\nThe research on offline RL has attracted significant attention in recent years and has made substantial\nprogress in both theoretical analysis and practical performance. The core challenge of offline RL is\nthe value overestimation issue induced by distributional shift [9, 21]. Existing studies mitigate this\nproblem by constraining the learning policy to closely resemble the behavior policy induced by the\ndataset [43, 18, 7], or adopting conservative value iteration [19, 16]. The success of offline RL can be\nlargely attributed to the availability of widely-adopted open-access datasets, such as D4RL [6] and RL\nUnplugged [12]. These datasets offer standardized and diverse pre-collected data for the development\nof new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons\nbetween different algorithms. However, despite their benefits, tasks contained in these datasets (such\n* Authors contributed equally\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2408.10556v2  [cs.AI]  22 Nov 2024\nas Atari 2600 [3] or Mujoco [39]) are often overly simplistic or purely academic, failing to simulate\nthe complexity of real-world scenarios and lacking practical applications. Thus, there is a significant\ngap between offline RL research and its practical application in real-world settings. This disparity\nhinders the usefulness of offline RL in addressing real-world problems, and thus, it is indispensable\nto create datasets that reflect a realistic level of complexity and practicality in real-world applications.\nOffline Multi-Agent Reinforcement Learning (Offline MARL) [29, 47, 36] is gaining increasing\nattention due to its close relationship to real scenarios, such as games [32, 42], sensor networks [53]\nand autonomous vehicle control [46]. However, the lack of standardized datasets restricts the\ndevelopment 40 of offline MARL. Existing works only rely on self-made datasets, which hampers\nfairness and reproducibility. Moreover, the settings they focus on are typically limited to toy examples\n(e.g., Multi-agent Particle Environment [26]) or simplified versions of classic games (e.g., StarCraft\nMulti-Agent Challenge [32]), facing the same impractical issues encountered in offline RL. Thus,\nthere is an urgent need for open-access datasets to further the progress of offline MARL.\nThe connections between offline RL and offline MARL are closely linked due to similar challenges\npertaining to offline learning. While, offline MARL also introduces distinct algorithm development\nrequirements, as it involves unique characteristics like multiple agents and intra-team cooperation.\nThe current offline MARL algorithms [29, 47] are mainly adaptations of offline RL algorithms,\nnecessitating the availability of standardized offline datasets that cater to both single-agent and\nmulti-agent settings. Thus, to enhance versatility and practicality, it is crucial to propose datasets that\nencompass both single-agent settings and multi-agent settings.\nIn this paper, we present Hokoff, a suite of pre-collected datasets for both offline RL and offline\nMARL, along with a comprehensive framework for conducting corresponding research. Our paper\nmakes several novel contributions, which are shown below:\n•\nThe tasks we adopt are based on one of the world’s most popular Multiplayer Online Battle\nArena (MOBA) games, Honor of Kings (HoK), which has over 100 million daily active players[42],\nensuring the practicality of our datasets. The complexity of this environment dramatically surpasses\nthose of its counterparts, demonstrating the potential for simulating real-world scenarios.\n• We present an open-source, easy-to-use framework 1 under Apache License V2.0. This framework\nincludes comprehensive processes for offline RL (sampling, training, and evaluation), and some\nuseful tools. Based on the framework, We release a rich and diverse set of datasets 2 which are\ngenerated using a series of pre-trained models featuring distinct design factors. These datasets cater\nnot only to offline RL but also offline MARL.\n• Building on the framework, we reproduce various offline RL and offline MARL algorithms and\npropose a novel baseline algorithm tailored for the inherent hierarchical structured action space of\nHonor of Kings. We fully validate and compare these baselines on our datasets. The results indicate\nthat current offline RL and offline MARL approaches are unable to effectively address complex\ntasks with discrete action space. Additionally, these methods exhibit shortcomings in terms of their\ngeneralization capabilities and their ability to facilitate multi-task learning.\n2\nRelated Works\n2.1\nOffline RL and Offline MARL\nOffline RL [23, 27] gains significant attention in recent years, primarily due to the inherent difficulties\nof directly applying online RL algorithms to offline environments. The main hurdles encountered is\nthe issue of erroneous value overestimation, which arises from the distributional shift between the\ndataset and the learning policy [9]. Theoretical studies have demonstrated that the overestimation\nissue can be alleviated by pessimism, which results in satisfactory performance even with imperfect\ndata coverage [4, 5, 14, 20, 25, 31, 45, 52]. In practice, certain studies [1, 2, 44, 1, 2, 44] employ\nuncertainty-based methods to estimate Q-values pessimistically or to perform learning on pessimistic\ndynamic models by estimating the epistemic uncertainty of Q-values or dynamics. Some studies [18,\n9, 43, 16, 19, 7] adopt behavior regularization-based approaches by imposing constraints on the\n1https:\/\/github.com\/tencent-ailab\/hokoff\n2https:\/\/sites.google.com\/view\/hok-offline\n2\nlearned policy to align closely with the behavior policy, either explicitly or implicitly, which offers\nbetter computational efficiency and memory consumption compared to uncertainty-based methods.\nOffline MARL [29, 34], combining offline RL and MARL, emerged in recent years to address safety\nand training efficiency concerns in practical multi-agent scenarios. Most studies in this domain adopt\na multi-agent paradigm, such as independent learning [38] or centralized training with decentralized\nexecution (CTDE) [26, 35]. These investigations also incorporat offline methods, similar to those\nemployed in single-agent settings, to mitigate distributional shift. Moreover, innovative treatments\nare introduced for cooperation, such as zeroth-order optimization in OMAR [29] or decomposing\nthe joint-policy in MAICQ [47]. In addition, Jiang & Lu [13] specifically focuses on decentralized\nlearning using BCQ [9], while Tseng et al. [40] regards offline MARL as a sequence modeling\nproblem, utilizing supervised learning and knowledge distillation to tackle the challenges it presents.\n2.2\nOffline Datasets\nThe availability of large-scale pre-collected datasets has greatly facilitated the progress of deep\nsupervised learning [11]. Offline RL, which is regarded as a bridge between RL and supervised\nlearning, also requires learning policies from pre-collected datasets [6]. Therefore, high-quality\npre-collected offline datasets play a significant role in the development of offline RL. To meet this\ndemand, some datasets have been published and widely adopted. D4RL [6] is designed to address\nkey challenges often faced in practical applications where datasets may have limited and biased\ndistributions, incomplete observations, and suboptimal data. To tackle these issues, D4RL offers\na range of datasets that enjoy these characteristics. Similarly, RL Unplugged [12] introduces a\nbenchmark to evaluate and compare offline RL methods with various settings, such as partially or\nfully observable and continuous or discrete actions. These offline datasets play a significant role\nin offline RL research, and many previous works train and evaluate their methods based on these\ndatasets [16, 2, 15, 29, 47].\nHowever, both D4RL and RL Unplugged primarily focus on relatively simple tasks and lack high-\ndimensional, practical and multi-agent tasks that closely resemble real-world scenarios. StarCraft\nII Unplugged [28] introduces a benchmark for StarCraft II, a complex simulated environment with\nseveral practical properties. However, they only utilize a dataset derived from human replays, which\nlacks diversity in design for offline RL, and they did not evaluate existing offline RL methods. To\naddress this research gap, we propose Hokoff, a benchmark based on HoK, which aims to provide\ndiverse offline datasets for high-dimensional, practical tasks, and present a comprehensive evaluation\nof previous offline RL and offline MARL methods with a general, easy-to-use framework.\n3\nBackground\nHonor of Kings (HoK) is one of the most popular MOBA games worldwide, boasting over 100 million\ndaily active players [42]. The game involves two teams, each consisting of several players who have\nthe option to select from a wide range of heroes with diverse roles and abilities. In the game, heroes\nare expected to eliminate enemy units, such as heroes, creeps, and turrets, to gain gold and experience.\nThe primary objective is to destroy the enemies’ turrets and crystal while defending their own. To\nsucceed in MOBA games, players must learn how to choose the appropriate hero combination, master\ncomplex information processing and action control, plan for long-term decision-making, cooperate\nwith allies, and balance multiple interests. The complex rules and properties of HoK make it be\nmore in line with the complex decision-making behavior of human society. Thus, HoK has attracted\nnumerous researchers interest [49, 48, 42, 41, 10].\nThe underlying system dynamic of HoK can be characterized by a Partially Observable Markov\nDecision Process (POMDP [37]), denoted by M = (S, O, A, P, r, γ, d). Due to the fog of war and\nprivate features, each agent has access to only local observations o rather than the global state s.\nSpecifically, the agents are limited to perceiving information about game units within their field\nof view, as well as certain global features. Due to the intricate nature of control, the action space\nA is organized in a hierarchically structured manner, rather than being flattened, which avoids the\nrepresentation of millions of discretized actions. Randomness is added into the transition distribution\nP in the form of critical hit rate. The reward r is decomposed into multi-head form and each hero’s\nreward is a weighted sum of different reward items and is designed to be zero-sum. Details of\nobservation space, action space and reward are presented in Appendix D.\n3\n(a) HoK1v1\n(b) HoK3v3\nFigure 1: (a) The Game replay user interface (UI) in HoK1v1. (b) The UI in HoK3v3. Important\ninformation and units of the game are highlighted using orange boxes.\n4\nHokoff\nThis study is based on the HoK gaming environment, which encompasses both 1v1 and 3v3 maps.\nOur research proposes a comprehensive offline RL framework applicable to this gaming environment\nand utilizes it to generate diverse datasets. This section provides an introduction to the framework,\ngame modes, datasets, and evaluation protocol employed in this study.\n4.1\nFramework\nTo enhance the usability of our Hokoff, we propose a reliable and comprehensive Offline RL\nframework that consists of three modules: sampling, training, and evaluation. This framework\nstreamlines the process of sampling new datasets, developing and training baselines, and evaluating\ntheir performance. The sampling module provides a simple and unified program for sampling diverse\ndatasets using any pre-trained checkpoints. There are several reasons why our framework excels\nin sampling. Firstly, diverse datasets at different levels of expertise can be sampled by leveraging\nMulti-Level Models as described in Sec. 4.1.1. Secondly, our framework employs parallel sampling\ntechniques, ensuring efficient sampling of large and diverse datasets. Based on the training module,\nwe have implemented various offline RL and offline MARL algorithms as baselines. Additionally, we\nconsolidate crucial components and provide user-friendly APIs, facilitating researchers to effortlessly\ndevelop novel algorithms or innovative network architectures. The evaluation module enables\nthe assessment of trained models from different algorithms, ensuring fair comparisons. Fig. 2\ndemonstrates the architecture of our framework and Appendix E provides an example of the APIs.\n4.1.1\nMulti-Level Models\nTo ensure a valid and unbiased comparison of the performance of distinct algorithms, it is crucial\nto establish appropriate evaluation protocols [6, 12]. One such effective evaluation protocol is the\nnormalized score [6]. However, HoK is a zero-sum adjustable rewards MOBA game. The episode\nreturn in the game is heavily influenced by the opponents and game settings, and the objective is to\nwin, which renders the use of return as a performance metric biased. Therefore, normalized score may\nnot fully capture our requirements. Furthermore, similar to our situation, the evaluation protocol for\nSMAC [32], a competitive game, is based on win rate against a pre-programmed AI. Nonetheless, it\nis exceedingly challenging to create a built-in AI with human-like performance due to the complexity\nof MOBA games.\nInspired by prior works of HoK [42], we present Multi-Level Models for sampling and evaluating\nwhich contains multiple checkpoints with different level. Specifically, we have extracted several\ncheckpoints from pre-trained dual-clip PPO [49, 48] models with varying levels determined by the\noutcome of the battle separately for HoK1v1 and HoK3v3. We adopt the win rate against different\ncheckpoints as our evaluation protocols to assess the ability of models. Additionally, these models,\nwith varying levels, can be utilized on both sides to sample diverse battle data. The capabilities of\nthese models surpass those of rule-based AI and match the levels of different human players, thus\nmaking these evaluation protocols more suitable for comparing algorithmic performance with human-\nlevel performance and facilitating diverse and effortless sampling. The details of these Multi-Level\nModels are provided in the Appendix F.\n4\nFigure 2: The architecture of the framework. The sampling and evaluation modules should interact\nwith the environment. Multi-Level Models are the foundation baseline models of these two modules,\nserving as opponents in the evaluation module and being on both sides in the sampling module, as\ndescribed in Sec 4.1.1. The training module is responsible for training offline RL algorithms using\nfixed datasets and producing trained models for evaluation.\n4.2\nGame Modes\nWe have incorporated two game modes from HoK into our study, namely HoK1v1 [42] and HoK3v3.\nThe environment code of HoK3v3 is integrated into the open-source HoK1v1 code3, following\nApache License V2.0. These game modes differ in the number of agents involved and the underlying\nmap used. Detailed information on each game mode is presented below.\n4.2.1\nHonor of Kings Arena\nHonor of Kings Arena (HoK Arena or HoK1v1) is a 1v1 game mode where each player attempts to\nbeat the other and destroy its opponent’s crystal. Specifically, each player chooses a hero before the\ngame starts and controls it during the whole game. There are a total of 20 heroes available for players\nto select, each possessing distinct skills that exert diverse effects on the game environment. The\nobservation space is a continuous space consisting of 725 dimensions that contain partial observable\ninformation about the hero, opponent, and other game units. The action space is hierarchically\nstructured and discretized, covering all possible actions of the hero in a hierarchical triplet form:\n(1) which action button to take; (2) who to target; and (3) how to act. Furthermore, the reward is a\nweighted sum of five categories: farming, kill-death-assist (KDA) , damage, pushing, and win-lose.\nFor a full description of this game mode, please refer to the Appendix D.1.\n4.2.2\nHonor of Kings 3v3 Arena\nTo further cater to the demand for Offline MARL, we adopt Honor of Kings 3v3 Arena (HoK3v3) as\nour experimental platform. HoK3v3 is a MOBA game, where each team comprises three heroes who\ncollaborate to defeat their opponents. The basic rules and win conditions of HoK3v3 are similar to\nHoK1v1. However, the HoK3v3 map contains additional turrets on the middle road and features a\nnew area called the \"wilderness\", inhabited by diverse monsters. Besides, collaboration is essential in\nHoK3v3, where players must select different heroes and fulfill distinct roles to work together more\nefficiently. For instance, one hero might focus on slaying monsters in the wilderness to earn gold and\nexperience, while the other heroes engage in offensive tactics against the enemy heroes and game\nunits. The design philosophies for observation space, action space, and reward are comparable to\nthose used in HoK1v1. However, the level of complexity in HoK3v3 is significantly elevated. We\nprovide a detailed description of the game mode in the Appendix D.2 for reference.\n3https:\/\/github.com\/tencent-ailab\/hok_env\n5\n4.2.3\nSubtasks\nBoth HoK1v1 and HoK3v3 are full MOBA games, featuring multi-camp competitions, which\ninherently pose challenges and limitations. Consequently, training on these game modes demands\nextensive training time and computational resources. However, HoK game comprises various sub-\nobjectives, allowing us to decompose the overall game into manageable subtasks. These subtasks\ncan represent diverse scenarios and are suitable for evaluating various algorithms. In this study, we\npropose two specific noncompetitive subtasks as outlined below. It is worth noting that researchers\ncan readily expand upon our framework to develop additional subtasks.\nDestroy Turret: One of the key sub-objectives in HoK is to destroy the enemy’s turrets as quickly as\npossible, to gain access to the enemy crystal. To train this specific skill, we have devised a subtask\ncalled Destroy Turret, which is based on HoK1v1. In this subtask, the focus is solely on destroying\nthe enemy’s turret and crystal as quickly as possible, and the enemy hero is removed.\nGain Gold: Gold is a critical resource in HoK that can be used to purchase equipment, which\nenhances the abilities of the heroes. Inspired by resource collection tasks from previous studies [22],\nwe have designed a subtask called Gain Gold, which is based on HoK3v3, where the new objective is\nto collect golds in restricted time steps, and the enemy heroes are removed. As a multi-agent setting,\nit focuses on the cooperation or intra-team competition while avoiding inter-team competition.\n4.3\nDatasets\nTo enhance the practical implications of our datasets, we have incorporated design factors that align\nwith the real-world applications of both HoK and other relevant scenarios.\nMulti-Difficulty\nIntuitively, the level of difficulty in the environment significantly impacts the performance of al-\ngorithms. However, previous researches only utilized one set of datasets with a uniform level of\ndifficulty in the environment, which is not appropriate for HoK, where the difficulty of the task can\nbe substantially affected by the level of opponents. Therefore, to examine the effects of varying levels\nof difficulty in the environment, we propose several multi-difficulty datasets with different difficulty\nlevels. Specifically, we develop two sets of datasets: norm and hard, which are categorized based\non the opponent’s level. Within each level, we propose four datasets according to diverse win rates\nagainst the opponent: poor, medium, expert and mixed. To elaborate, the poor\/medium\/expert dataset\nis generated by recording the battle trajectories of a relative lower\/equal\/higher level model compared\nto the opponent, and the mixed dataset is an equal mixture of the three datasets mentioned above.\nMulti-Task\nAs a MOBA game, HoK features a diverse cast of heroes with distinct roles and skillsets. While the\noverall objective remains consistent throughout matches, the selection of heroes can significantly\nalter the nature of the task at hand. Consequently, HoK presents multi-task challenge which requires\na single model to handle multiple tasks [51, 24, 50]. However, none of the current works provide\nuniform datasets for multi-task offline RL. To address this research gap, we propose a series of\nmulti-task datasets based on the multi-task nature of HoK and evaluate the multi-task learning ability\nof current offline RL and offline MARL algorithms. Specifically, we define a hero pool with several\nheroes and randomly select heroes from it to sample data. Depending on whether the selected heroes\nare on the controlled side or the opponent side, we sample either the multi_hero or the multi_oppo\ndataset. In cases where both sides choose random heroes, we sample the multi_hero_oppo dataset.\nFurthermore, as mentioned in the previous section, different levels of opponents naturally form\nmultiple tasks with varying environmental difficulties. Thus, we propose several level-based multi-\ntask datasets by sampling data with randomly selected opponent levels. According to different\ndifficulty levels, we have proposed two datasets, named norm_multi_level and hard_multi_level.\nGeneralization\nThe unique gameplay mechanics of HoK, characterized by a diverse cast of heroes with distinct\nroles and skillsets, lend themselves well to multi-task and serve as an ideal testbed for evaluating\nthe generality of models across a range of tasks. Building on the previous work [42] and taking\ninto account the realities of human combat in HoK, we have identified three key challenges for\ngeneralization: hero generalization, opponent generalization, and level generalization.\n6\nWe have developed six experiments: \"norm_general\" and \"hard_general\" for level generalization,\n\"norm_hero_general\" and \"hard_hero_general\" for hero generalization, and \"norm_oppo_general\"\nand \"hard_oppo_general\" for opponent generalization, for HoK1v1 and HoK3v3, respectively.\nAmong them, the first two experiments, \"norm_general\" and \"hard_general,\" have their corresponding\ndatasets, and we train models on these datasets. The latter four experiments do not require extra\ndatasets because we directly use the existing models that have already been trained using other\ndatasets. For more details on the design of generalization, please refer to the Appendix C.\nHeterogeneous Teammate\nHeterogeneous teammate is a crucial research direction in MARL [33, 17]. In the practical scenarios\nof HoK, the capacity of each player is generally different, making it naturally suitable for investigating\nthe challenges associated with heterogeneous teammate. In order to mimic real-world scenarios\nand facilitate research on heterogeneous teammate challenges, we design two datasets in HoK3v3:\nnorm_stupid_partner and norm_expert_partner. These datasets were collected in a standard manner,\nwith the exception that one random hero in each team is controlled by a model with a relatively\nlow\/high level of expertise, while the remaining heroes are controlled by the regular model.\nSub-Task\nAs introduced in Sec 4.2.3, we designed several practical and meaningful sub-tasks to provide diverse\nscenarios based on HoK. Based on these sub-tasks, we proposed diverse datasets to support Offline\nRL research similar to the design of previous studies [6, 12].\nTable 1: Details of datasets in HoK1v1 game mode\nFactors\nDatasets\/Experiments\nCapacity\nHeroes\nOppo_heroes\nWin_rate\nLevels\nMulti-Difficulty\nnorm_poor\n1000\ndefault\ndefault\n12%\n1\nnorm_medium\n1000\ndefault\ndefault\n50%\n1\nnorm_expert\n1000\ndefault\ndefault\n88%\n1\nnorm_mixed\n1000\ndefault\ndefault\n50%\n1\nhard_poor\n1000\ndefault\ndefault\n6%\n5\nhard_medium\n1000\ndefault\ndefault\n50%\n5\nhard_expert\n1000\ndefault\ndefault\n84%\n5\nhard_mixed\n1000\ndefault\ndefault\n45%\n5\nGeneralization\nhard_general\n1000\ndefault\ndefault\n90%\n5\nnorm_general\n1000\ndefault\ndefault\n46%\n1\nnorm_hero_general\n-\nmulti_hero\ndefault\n-\n1\nhard_hero_general\n-\nmulti_hero\ndefault\n-\n5\nnorm_oppo_general\n-\ndefault\nmulti_hero\n-\n1\nhard_oppo_general\n-\ndefault\nmulti_hero\n-\n5\nMulti-Task\nnorm_multi_level\n1000\ndefault\ndefault\n50%\n1\nhard_multi_level\n1000\ndefault\ndefault\n50%\n5\nnorm_multi_hero\n1000\nmulti_hero\ndefault\n23%\n1\nnorm_multi_oppo\n1000\ndefault\nmulti_hero\n77%\n1\nnorm_multi_hero_oppo\n1000\nmulti_hero\nmulti_hero\n50%\n1\n4.3.1\nDatasets Details\nTable 1, Table 2 and Table 3presents the details of our proposed datasets. All the datasets are sampled\nusing checkpoints with different levels as introduced in Sec. 4.1.1. Typically, each dataset consists of\n1000 trajectories, except for the sub-task datasets, which contain 100 trajectories. The default heroes\nchosen for both camps are luban with Summoner Spells set to frenzy in HoK1v1 and {{zhaoyun},\n{diaochan}, {liyuanfang}} with Summoner Spells assigned as {{smite}, {purify}, {purify}} based\non their respective roles in HoK3v3. However, in specific scenarios such as Generalization or\nMulti-Task settings, we employ a random selection of heroes from a predefined set, multi_hero. For\nthe HoK1v1 mode, the set comprises five heroes, {luban, direnjie, houyi, makeboluo, gongsunli}.\nIn HoK3v3, the set consists six heroes, with two heroes assigned to each role, namely {{zhaoyun,\nzhongwuyan}, {diaochan, zhugeliang}, {liyuanfang, sunshangxiang}}. The win rate of the behavior\npolicy is recorded in the column labeled Win_rate for reference. The column labeled Levels denotes\nthe levels of opponents used for evaluation. More details of the datasets are presented in Appendix C.\n7\nTable 2: Details of datasets in HoK3v3 game mode\nFactors\nDatasets\/Experiments\nCapacity\nHeroes\nOppo_heroes\nWin_rate\nLevels\nMulti-Difficulty\nnorm_poor\n1000\ndefault\ndefault\n16%\n1\nnorm_medium\n1000\ndefault\ndefault\n50%\n1\nnorm_expert\n1000\ndefault\ndefault\n82%\n1\nnorm_mixed\n1000\ndefault\ndefault\n49%\n1\nhard_poor\n1000\ndefault\ndefault\n18%\n7\nhard_medium\n1000\ndefault\ndefault\n50%\n7\nhard_expert\n1000\ndefault\ndefault\n83%\n7\nhard_mixed\n1000\ndefault\ndefault\n51%\n7\nGeneralization\nhard_general\n1000\ndefault\ndefault\n94%\n8\nnorm_general\n1000\ndefault\ndefault\n57%\n5\nnorm_hero_general\n-\nmulti_hero\ndefault\n-\n1\nhard_hero_general\n-\nmulti_hero\ndefault\n-\n7\nnorm_oppo_general\n-\ndefault\nmulti_hero\n-\n1\nhard_oppo_general\n-\ndefault\nmulti_hero\n-\n7\nMulti-Task\nnorm_multi_level\n1000\ndefault\ndefault\n50%\n1\nhard_multi_level\n1000\ndefault\ndefault\n50%\n7\nnorm_multi_hero\n1000\nmulti_hero\ndefault\n74%\n1\nnorm_multi_oppo\n1000\ndefault\nmulti_hero\n26%\n1\nnorm_multi_hero_oppo\n1000\nmulti_hero\nmulti_hero\n50%\n1\nHeterogeneous\nnorm_stupid_partner\n1000\ndefault\ndefault\n50%\n1\nnorm_expert_partner\n1000\ndefault\ndefault\n50%\n1\nnorm_mixed_partner\n1000\ndefault\ndefault\n50%\n1\nTable 3: Details of datasets in Sub-Tasks\nSub-Task\nDatasets\/Experiments\nCapacity\nHeroes\nOppo_heroes\nAverage Score\nLevels\nDestroy Turret\ndestroy_turret_medium\n100\ndefault\nno\n0.55\nmedium\ndestroy_turret_expert\n100\ndefault\nno\n1.00\nexpert\ndestroy_turret_mixed\n100\ndefault\nno\n0.73\n-\nGain Gold\ngain_gold_medium\n100\ndefault\nno\n0.13\nmedium\ngain_gold_expert\n100\ndefault\nno\n1.04\nexpert\ngain_gold_mixed\n100\ndefault\nno\n0.58\n-\n5\nBenchmarking\nBased on our framework, we reproduce various Offline RL and Offline MARL algorithms. Besides,\nwe fully validate and compare these baselines on our datasets. The results are presented in the form of\ntest winning rate. Each algorithm is run for three random seeds, and we report the mean performance\nwith standard deviation. The performance of behaviour policies is presented in Appendix C. Details\nof the implementations and experimental results can be referenced in Appendix G.\n5.1\nBaselines\n5.1.1\nHoK1v1\nThe Offline RL baseline algorithms we implement are briefly introduced below: BC: Behavior\ncloning. TD3+BC [7]: One of the state-of-the-art single agent offline algorithm, simply adding the\nBC term to TD3 [8]. CQL [19]: Conservative Q-Learning conducts conservative value iteration by\nadding a regularizer to the critic loss. IQL [16]: Implicit Q-Learning leverages upper expectile value\nfunction to learn Q-function and extracts policy via advantage-weighted behavioral cloning.\nThe structured action space in HoK is similar to the joint action space in multi-agent settings, which\ninspires us to resort to the design in MARL methods. We propose a novel baseline algorithm, named\nQMIX+CQL. Specifically, we import QMIX algorithm from the MARL literature [30] to tackle the\nstructured action space by regarding each head of the action space as a single agent and incorporate\nCQL regularizer term into local Q-funtion in QMIX for offline learning.\n8\n5.1.2\nHoK3v3\nThe Offline MARL baseline algorithms are briefly introduced below: IND+BC: Behavior cloning\nwith independent learning paradigm. IND+CQL: Adopts an independent learning paradigm for\nmulti-agent settings, using conservative Q-learning [19]. COMM+CQL: Incorporate inter-agent\ncommunication based on IND+CQL. IND+ICQ [47]: Implicit Constraint Q-learning with inde-\npendent learning paradigm, which only uses insample data for value estimation to alleviate the\nextrapolation error. MAICQ [47]: Multi-agent version of implicit constraint Q-learning by decom-\nposed multi-agent joint-policy under implicit constraint with CTDE paradigm. OMAR [29]: Using\nzeroth-order optimization for better coordination among agents’ policies, based on independent CQL.\n5.2\nBenchmark Results\nWe have validated the offline RL and offline MARL baselines on our datasets and aggregated the\nresults in Table 4 and Table 5.\nTable 4: Averaged test winning rate or normalized score (Sub-Task) of baselines in HoK1v1 game\nmode.\nFactors\nDatasets\nBC\nCQL\nQMIX+CQL\nIQL\nTD3+BC\nMulti-Difficulty\nnorm_poor\n0.08±0.02\n0.06±0.01\n0.08±0.02\n0.07±0.01\n0.0±0.0\nnorm_medium\n0.33±0.01\n0.32±0.01\n0.31±0.03\n0.32±0.01\n0.01±0.01\nnorm_expert\n0.64±0.01\n0.58±0.03\n0.67±0.01\n0.62±0.02\n0.03±0.01\nnorm_mixed\n0.17±0.01\n0.23±0.04\n0.20±0.01\n0.25±0.01\n0.01±0.01\nhard_poor\n0.01±0.01\n0.01±0.01\n0.01±0.01\n0.01±0.00\n0.00±0.00\nhard_medium\n0.13±0.01\n0.11±0.01\n0.20±0.01\n0.12±0.02\n0.00±0.00\nhard_expert\n0.33±0.01\n0.30±0.01\n0.44±0.05\n0.34±0.04\n0.00±0.00\nhard_mixed\n0.05±0.3\n0.02±0.01\n0.08±0.01\n0.06±0.01\n0.01±0.01\nGeneralization\nnorm_general\n0.19±0.01\n0.20±0.04\n0.32±0.03\n0.18±0.01\n0.02±0.02\nhard_general\n0.04±0.01\n0.03±0.01\n0.08±0.02\n0.02±0.01\n0.00±0.00\nnorm_hero_general\n0.06±0.01\n0.06±0.01\n0.08±0.01\n0.07±0.01\n0.00±0.00\nhard_hero_general\n0.03±0.01\n0.03±0.01\n0.04±0.01\n0.06±0.01\n0.00±0.00\nnorm_oppo_general\n0.58±0.03\n0.52±0.04\n0.42±0.22\n0.51±0.07\n0.12±0.01\nhard_oppo_general\n0.15±0.02\n0.12±0.03\n0.23±0.04\n0.14±0.03\n0.01±0.01\nMulti-Task\nnorm_multi_level\n0.32±0.03\n0.25±0.03\n0.41±0.02\n0.30±0.02\n0.02±0.01\nhard_multi_level\n0.08±0.02\n0.06±0.01\n0.16±0.03\n0.08±0.02\n0.00±0.00\nnorm_multi_hero\n0.08±0.01\n0.07±0.02\n0.11±0.01\n0.06±0.01\n0.00±0.00\nnorm_multi_oppo\n0.59±0.02\n0.55±0.03\n0.65±0.02\n0.60±0.05\n0.10±0.02\nnorm_multi_hero_oppo\n0.26±0.01\n0.21±0.02\n0.32±0.03\n0.28±0.05\n0.03±0.01\nSub-Task\ndestroy_turret_medium\n0.61±0.06\n0.63±0.01\n0.61±0.03\n0.60±0.02\n0.67±0.03\ndestroy_turret_expert\n0.94±0.02\n0.94±0.02\n0.92±0.05\n0.95±0.01\n0.57±0.13\ndestroy_turret_mixed\n0.88±0.04\n0.87±0.03\n0.89±0.02\n0.89±0.04\n0.82±0.03\n• Baselines Comparison: As indicated in Table 4, QMIX+CQL exhibits superior performance in\ncomparison to other approaches, implying that the integration of MARL methods may be a suitable\nchoice for environments with a structured action space. Moreover, in HoK3v3, IND+ICQ exhibits\nthe highest performance across most datasets, except for the Heterogeneous datasets. Conversely,\nalgorithms based on TD3, namely TD3+BC and OMAR, yield poor results.\n•\nMulti-Difficulty: The baseline performance exhibits a significant decrease on the hard-level\ndatasets compared with norm-level datasets, highlighting the limitations of current offline methods in\naddressing challenging tasks with discrete action space.\n•\nGeneralization:\nThe disparities between training and evaluation in Generalization settings\nimpede the achievement of desirable performance, indicating the inadequacy of current methods’\ngeneralization ability.\n•\nMulti-Task:\nTraining models on Multi-Task datasets results in a substantial performance\nenhancement compared to generalization settings. However, none of these models have been able to\nexceed the performance achieved by the behavior policy, underscoring the need for further research\ninto the direct application of offline methods to multiple tasks.\n9\nTable 5: Averaged test winning rate or normalized score (Sub-Task) of baselines in HoK3v3 game\nmode.\nFactors\nDatasets\nIND+BC\nCOMM+CQL\nIND+CQL\nIND+ICQ\nMAICQ\nOMAR\nMulti-Difficulty\nnorm_poor\n0.1±0.01\n0.09±0.02\n0.03±0.01\n0.12±0.02\n0.12±0.04\n0.02±0.01\nnorm_medium\n0.48±0.01\n0.47±0.04\n0.4±0.03\n0.45±0.01\n0.38±0.16\n0.23±0.03\nnorm_expert\n0.52±0.03\n0.76±0.13\n0.84±0.06\n0.65±0.12\n0.61±0.09\n0.39±0.16\nnorm_mixed\n0.35±0.25\n0.48±0.12\n0.46±0.12\n0.44±0.19\n0.24±0.16\n0.17±0.2\nhard_poor\n0.16±0.03\n0.11±0.04\n0.12±0.03\n0.17±0.02\n0.12±0.03\n0.08±0.04\nhard_medium\n0.38±0.05\n0.35±0.03\n0.31±0.02\n0.4±0.08\n0.2±0.06\n0.23±0.07\nhard_expert\n0.65±0.01\n0.66±0.05\n0.67±0.04\n0.67±0.02\n0.52±0.1\n0.35±0.17\nhard_mixed\n0.32±0.14\n0.34±0.11\n0.3±0.1\n0.34±0.08\n0.23±0.08\n0.16±0.17\nGeneralization\nnorm_general\n0.34±0.05\n0.35±0.04\n0.29±0.09\n0.37±0.04\n0.29±0.06\n0.09±0.1\nhard_general\n0.28±0.03\n0.3±0.05\n0.28±0.04\n0.31±0.09\n0.14±0.06\n0.13±0.04\nnorm_hero_general\n0.17±0.03\n0.13±0.02\n0.14±0.04\n0.2±0.09\n0.2±0.06\n0.13±0.04\nhard_hero_general\n0.16±0.05\n0.19±0.05\n0.17±0.02\n0.17±0.02\n0.07±0.05\n0.08±0.03\nnorm_oppo_general\n0.21±0.01\n0.14±0.03\n0.14±0.04\n0.18±0.06\n0.13±0.07\n0.12±0.03\nhard_oppo_general\n0.09±0.06\n0.08±0.02\n0.09±0.02\n0.08±0.04\n0.04±0.02\n0.04±0.01\nMulti-Task\nnorm_multi_level\n0.43±0.09\n0.36±0.02\n0.34±0.04\n0.44±0.02\n0.38±0.11\n0.22±0.07\nhard_multi_level\n0.38±0.08\n0.33±0.08\n0.29±0.07\n0.37±0.05\n0.27±0.05\n0.2±0.01\nnorm_multi_hero\n0.57±0.07\n0.31±0.2\n0.3±0.07\n0.59±0.05\n0.51±0.17\n0.39±0.06\nnorm_multi_oppo\n0.09±0.04\n0.08±0.05\n0.07±0.03\n0.12±0.04\n0.07±0.03\n0.02±0.01\nnorm_multi_hero_oppo\n0.3±0.04\n0.23±0.1\n0.26±0.07\n0.31±0.07\n0.26±0.03\n0.07±0.02\nHeterogeneous\nnorm_stupid_partner\n0.11±0.15\n0.33±0.06\n0.24±0.17\n0.22±0.14\n0.16±0.09\n0.08±0.05\nnorm_expert_partner\n0.36±0.09\n0.52±0.1\n0.57±0.04\n0.55±0.22\n0.31±0.15\n0.07±0.06\nnorm_mixed_partner\n0.49±0.2\n0.59±0.19\n0.32±0.03\n0.42±0.38\n0.17±0.27\n0.15±0.04\nSub-Task\ngain_gold_medium\n0.13±0.01\n0.12±0.01\n0.12±0.01\n0.15±0.01\n0.13±0.03\n0.14±0.01\ngain_gold_expert\n1.01±0.03\n0.98±0.02\n1.00±0.01\n1.03±0.01\n0.98±0.08\n0.79±0.06\ngain_gold_mixed\n0.64±0.29\n0.41±0.21\n0.41±0.1\n0.46±0.16\n0.25±0.23\n0.13±0.04\n• Heterogeneous: As expected, the presence of a low-ability partner can disrupt cooperation and\nhinder offline learning on the stupid_partner datasets, whereas an expert partner has the opposite\neffect, highlighting the limitations of existing research on heterogeneous offline MARL.\n• Sub-Task: The offline baselines exhibit robust performance in the Sub-Task at a low training\ncost. Additionally, BC demonstrates a competitive capability as well.\n• Ablations of learning paradigms: We conduct ablation experiments to investigate the impact of\ncommunication and the CTDE paradigm. Specifically, from the comparison of COMM-CQL and\nIND-CQL, we can reveal that incorporating communication generally results in better performance\ndue to the promotion of cooperation. Surprisingly, we found that the independent paradigm (IND-\nICQ) outperformed the CTDE paradigm (MAICQ), which may be attributed to the challenges in the\nCTDE paradigm associated with credit assignment of agents with distinct rewards and roles.\n6\nConclusion\nIn this paper, taking into account the limitations of existing offline RL datasets about practical\napplications, we introduce Hokoff, based on Honor of Kings, a well-known MOBA game that\noffers a high level of complexity for simulating real-world scenarios. We present a comprehensive\nframework for conducting research in offline RL and release a diverse and extensive collection of\ndatasets, incorporating various levels of difficulty and a range of research factors. Moreover, the\nchosen tasks for dataset collection not only cater to Offline RL but also serve the purpose of offline\nMARL. We replicate multiple offline RL and offline MARL algorithms and thoroughly validate these\nbaselines on our datasets. The obtained results highlight the shortcomings of existing Offline RL\nmethods, underscoring the necessity for further research in areas such as challenging task settings,\ngeneralization capabilities, and multi-task learning. All components, including the framework,\ndatasets, and baseline implementations, discussed in this paper are fully open-source.\n10\nReferences\n[1] Agarwal, R., Schuurmans, D., and Norouzi, M. An optimistic perspective on offline rein-\nforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR,\n2020.\n[2] An, G., Moon, S., Kim, J.-H., and Song, H. O. Uncertainty-based offline reinforcement learning\nwith diversified q-ensemble. Advances in Neural Information Processing Systems, 34, 2021.\n[3] Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment:\nAn evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253–279, 2013.\n[4] Buckman, J., Gelada, C., and Bellemare, M. G. The importance of pessimism in fixed-dataset\npolicy optimization. arXiv preprint arXiv:2009.06799, 2020.\n[5] Cheng, C.-A., Xie, T., Jiang, N., and Agarwal, A. Adversarially trained actor critic for offline\nreinforcement learning. arXiv preprint arXiv:2202.02446, 2022.\n[6] Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: Datasets for deep data-driven\nreinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[7] Fujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. Advances\nin neural information processing systems, 34:20132–20145, 2021.\n[8] Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic\nmethods. In International conference on machine learning, pp. 1587–1596. PMLR, 2018.\n[9] Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without\nexploration. In International conference on machine learning, pp. 2052–2062. PMLR, 2019.\n[10] Gao, Y., Shi, B., Du, X., Wang, L., Chen, G., Lian, Z., Qiu, F., Han, G., Wang, W., Ye, D., et al.\nLearning diverse policies in moba games via macro-goals. Advances in Neural Information\nProcessing Systems, 34:16171–16182, 2021.\n[11] Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT press, 2016.\n[12] Gulcehre, C., Wang, Z., Novikov, A., Paine, T., Gómez, S., Zolna, K., Agarwal, R., Merel,\nJ. S., Mankowitz, D. J., Paduraru, C., et al. Rl unplugged: A suite of benchmarks for offline\nreinforcement learning. Advances in Neural Information Processing Systems, 33:7248–7259,\n2020.\n[13] Jiang, J. and Lu, Z. Offline decentralized multi-agent reinforcement learning. arXiv preprint\narXiv:2108.01832, 2021.\n[14] Jin, Y., Yang, Z., and Wang, Z. Is pessimism provably efficient for offline rl? In International\nConference on Machine Learning, pp. 5084–5096. PMLR, 2021.\n[15] Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. Morel: Model-based offline\nreinforcement learning. arXiv preprint arXiv:2005.05951, 2020.\n[16] Kostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning.\narXiv preprint arXiv:2110.06169, 2021.\n[17] Kuba, J. G., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y. Trust region policy\noptimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021.\n[18] Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. Stabilizing off-policy q-learning via\nbootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.\n[19] Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforce-\nment learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n[20] Kumar, A., Hong, J., Singh, A., and Levine, S. Should i run offline reinforcement learning or\nbehavioral cloning? In Deep RL Workshop NeurIPS 2021, 2021.\n11\n[21] Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J. Offline-to-online reinforcement learning via\nbalanced replay and pessimistic q-ensemble. In Conference on Robot Learning, pp. 1702–1712.\nPMLR, 2022.\n[22] Leibo, J. Z., Dueñez-Guzman, E. A., Vezhnevets, A., Agapiou, J. P., Sunehag, P., Koster,\nR., Matyas, J., Beattie, C., Mordatch, I., and Graepel, T. Scalable evaluation of multi-agent\nreinforcement learning with melting pot. In International Conference on Machine Learning\n(ICML), 2021.\n[23] Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review,\nand perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[24] Li, J., Vuong, Q., Liu, S., Liu, M., Ciosek, K., Ross, K., Christensen, H. I., and Su, H. Multi-task\nbatch reinforcement learning with metric learning, 2020.\n[25] Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. Provably good batch reinforcement\nlearning without great exploration. arXiv preprint arXiv:2007.08202, 2020.\n[26] Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. Multi-agent actor-critic\nfor mixed cooperative-competitive environments. In Advances in neural information processing\nsystems (NeurIPS), 2017.\n[27] Mao, Y., Zhang, H., Chen, C., Xu, Y., and Ji, X. Supported trust region optimization for offline\nreinforcement learning. In International Conference on Machine Learning, pp. 23829–23851.\nPMLR, 2023.\n[28] Mathieu, M., Ozair, S., Srinivasan, S., Gulcehre, C., Zhang, S., Jiang, R., Le Paine, T., Zolna,\nK., Powell, R., Schrittwieser, J., et al. Starcraft ii unplugged: Large scale offline reinforcement\nlearning. In Deep RL Workshop NeurIPS 2021, 2021.\n[29] Pan, L., Huang, L., Ma, T., and Xu, H. Plan better amid conservatism: Offline multi-agent rein-\nforcement learning with actor rectification. In International Conference on Machine Learning,\npp. 17221–17237. PMLR, 2022.\n[30] Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. Qmix:\nMonotonic value function factorisation for deep multi-agent reinforcement learning. arXiv\npreprint arXiv:1803.11485, 2018.\n[31] Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. Bridging offline reinforcement learning\nand imitation learning: A tale of pessimism. Advances in Neural Information Processing\nSystems, 34, 2021.\n[32] Samvelyan, M., Rashid, T., de Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung,\nC.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv\npreprint arXiv:1902.04043, 2019.\n[33] Seraj, E., Wang, Z., Paleja, R., Martin, D., Sklar, M., Patel, A., and Gombolay, M. Learning\nefficient diverse communication for cooperative heterogeneous teaming. In Proceedings of the\n21st international conference on autonomous agents and multiagent systems, pp. 1173–1182,\n2022.\n[34] Shao, J., Qu, Y., Chen, C., Zhang, H., and Ji, X. Counterfactual conservative q learning for\noffline multi-agent reinforcement learning. arXiv e-prints, pp. arXiv–2309, 2023.\n[35] Shao, J., Zhang, H., Qu, Y., Liu, C., He, S., Jiang, Y., and Ji, X. Complementary attention\nfor multi-agent reinforcement learning. In International Conference on Machine Learning, pp.\n30776–30793. PMLR, 2023.\n[36] Shao, J., Qu, Y., Chen, C., Zhang, H., and Ji, X. Counterfactual conservative q learning for\noffline multi-agent reinforcement learning. Advances in Neural Information Processing Systems,\n36, 2024.\n[37] Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.\n12\n[38] Tan, M. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Interna-\ntional Conference on Machine Learning (ICML), 1993.\n[39] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012\nIEEE\/RSJ international conference on intelligent robots and systems, pp. 5026–5033. IEEE,\n2012.\n[40] Tseng, W.-C., Wang, T.-H., Lin, Y.-C., and Isola, P. Offline multi-agent reinforcement learning\nwith knowledge distillation. In Advances in Neural Information Processing Systems, 2022.\n[41] Wei, H., Ye, D., Liu, Z., Wu, H., Yuan, B., Fu, Q., Yang, W., and Li, Z. Boosting offline\nreinforcement learning with residual generative modeling. arXiv preprint arXiv:2106.10411,\n2021.\n[42] Wei, H., Chen, J., Ji, X., Qin, H., Deng, M., Li, S., Wang, L., Zhang, W., Yu, Y., Linc, L., et al.\nHonor of kings arena: an environment for generalization in competitive reinforcement learning.\nAdvances in Neural Information Processing Systems, 35:11881–11892, 2022.\n[43] Wu, Y., Tucker, G., and Nachum, O. Behavior regularized offline reinforcement learning. arXiv\npreprint arXiv:1911.11361, 2019.\n[44] Wu, Y., Zhai, S., Srivastava, N., Susskind, J., Zhang, J., Salakhutdinov, R., and Goh, H. Uncer-\ntainty weighted actor-critic for offline reinforcement learning. arXiv preprint arXiv:2105.08140,\n2021.\n[45] Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. Bellman-consistent pessimism for\noffline reinforcement learning. Advances in neural information processing systems, 34, 2021.\n[46] Xu, Z., Lyu, Y., Pan, Q., Hu, J., Zhao, C., and Liu, S. Multi-vehicle flocking control with deep\ndeterministic policy gradient method. In International Conference on Control and Automation\n(ICCA), 2018.\n[47] Yang, Y., Ma, X., Li, C., Zheng, Z., Zhang, Q., Huang, G., Yang, J., and Zhao, Q. Believe what\nyou see: Implicit constraint approach for offline multi-agent reinforcement learning. Advances\nin Neural Information Processing Systems, 34:10299–10312, 2021.\n[48] Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu, H.,\net al. Towards playing full moba games with deep reinforcement learning. arXiv preprint\narXiv:2011.12692, 2020.\n[49] Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang, S., Wu, X., Guo, Q., et al.\nMastering complex control in moba games with deep reinforcement learning. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pp. 6672–6679, 2020.\n[50] Yoo, M., Cho, S., and Woo, H. Skills regularized task decomposition for multi-task offline\nreinforcement learning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh,\nA. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 37432–37444.\nCurran Associates, Inc., 2022. URL https:\/\/proceedings.neurips.cc\/paper_files\/\npaper\/2022\/file\/f376f5dff6f6ec6364aea7a46ab49574-Paper-Conference.pdf.\n[51] Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., and Finn, C. Conservative data\nsharing for multi-task offline reinforcement learning, 2021.\n[52] Zanette, A., Wainwright, M. J., and Brunskill, E. Provable benefits of actor-critic methods for\noffline reinforcement learning. Advances in neural information processing systems, 34, 2021.\n[53] Zhang, C. and Lesser, V. R. Coordinating multi-agent reinforcement learning with limited\ncommunication. In International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS), 2013.\n13\nA\nAuthor Statement\nThe authors of this work would like to state that we bear full responsibility for any potential violation\nof rights, including copyright infringement or unauthorized use of data. We affirm our commitment\nto conducting this research in accordance with ethical guidelines and legal requirements.\nWe further guarantee that we will ensure access to the data4 and the framework code5 used in this\nstudy, making them available to interested researchers for verification and replication purposes.\nAdditionally, we are committed to providing the necessary maintenance and support to ensure the\nlongevity and accessibility of the data. For datasets, we have plans to consistently offer more datasets\nin the future. These datasets will include larger sizes for larger models, higher levels for expert\nagents, and novel design factors for other research directions. Updating our datasets is an ongoing and\nlong-term effort, and we welcome contributions from the community. Regarding benchmarks, we will\nactively monitor the latest state-of-the-art (SOTA) algorithms in the offline RL domain and integrate\nthem into our benchmarks. Additionally, we will develop new algorithms within the benchmarks\nbased on existing datasets and baselines. This ensures that our benchmarks remain up-to-date and\nreflect the advancements in offline RL research.\nShould any concerns or inquiries arise regarding the contents of this work or the associated data, we\nencourage readers and fellow researchers to contact us directly. We are dedicated to addressing any\nissues promptly and transparently to uphold the integrity of our research.\nB\nLimitations and Future Works\nIn our future endeavors, we plan to integrate our framework with a large-scale deep reinforcement\nlearning platform namely KaiwuDRL, specifically designed to support Honor of Kings. By doing so,\nwe will gain access to greater computational resources, enabling us to delve deeper into our current\nresearch endeavors and expand our investigations.\nC\nAdditional Datasets Details\nC.1\nHoK1v1\nIn the Generalization category, \"norm_general\" and \"hard_general,\" have their corresponding datasets.\nFor example, to sample the \"norm_general\" dataset, we let the level-1 model fight with level-0, level-\n2, and level-4 models. However, during the test stage, we assess the generalization capabilities of\nthe trained model by letting it fight against the level-1 model. Details about how we sample the\ngeneralization datasets can be referred to Table. 6. The latter four experiments do not require extra\ndatasets. For example, in the \"norm_hero_general\" experiment, we directly use the model trained on\nthe \"norm_medium\" dataset and let the model control different heroes. This is possible because the\n\"norm_medium\" dataset only contains the fixed default hero \"luban.\" Therefore, we use the model\ntrained on this dataset to test its generalization ability at controlling different heroes.\nIn the Sub-Task: Destroy Turret category presented in Table 3, there are three datasets sampled, each\nconsisting of 100 trajectories. Notably, these datasets lack an opponent hero, making them simpler in\nnature. This design choice allows for broad applicability, diversity, and cost-effectiveness in research\nendeavors.\nThe primary objective in the Sub-Task: Destroy Turret scenarios is to efficiently dismantle the enemy’s\nturret and crystal, with the enemy hero removed. Consequently, we adopt the number of game frames\nelapsed from the start of the game until the crystal’s destruction as our evaluation protocol. Equation 1\noutlines the scoring methodology employed, following a similar approach as presented in [6]. The\nscore is normalized by two factors: random_frame_length, set to 2880, and expert_frame_length, set\nto 1812. A higher score is achieved by minimizing the time required to destroy the crystal.\nIn addition, we have generated violin charts to represent the distribution of episode returns in each\ndataset as shown in Fig. 3. We calculate episode returns using the formula R = PT\nt=0 γtrt, where\n4https:\/\/sites.google.com\/view\/hok-offline\n5https:\/\/github.com\/tencent-ailab\/hokoff\n14\ngamma is set to 1.0 to showcase the overall rewards obtained throughout an entire episode. For the\nSub-Task: Destroy Turret datasets of HoK1v1, we have normalized the scores based on Equation 1.\nThe violin charts demonstrate the diverse distribution of episode returns within our datasets.\nnormalized_sub_task_score =\nrandom_frame_length −frame_length\nrandom_frame_length −expert_frame_length\n(1)\n(a) norm_level\n(b) hard_level\n(c) level_general\n(d) multi_level\n(e) multi_hero_oppo\n(f) sub_task: destroy_turret\nFigure 3: Violin diagrams of all datasets in HoK1v1.\nC.2\nHoK3v3\nThe design of datasets and experiments pertaining to the concept of Generalization closely resembles\nthat of the HoK1v1.\n15\nWe introduce a sub-task called \"Gain Gold\" that builds upon the HoK3v3 game. In this modified\nversion, we remove opponents and redefine the primary objective to focus on collecting gold within\na limited number of time steps. This transforms the original competitive task into a resource\ncollection task. Specifically, we set a maximum episode length of 8000 frames, and the controlled\nheroes are required to efficiently gather gold by killing monsters or creeps. As demonstrated in\nTable 3, we generate three datasets based on this sub-task, each consisting of 100 trajectories.\nThe gain_gold_medium dataset is collected by a model with moderate performance, averaging\n5904 gold collected. While, the gain_gold_expert dataset is obtained from a model with expert\nperformance, averaging 12271 gold collected. Lastly, the gain_gold_mixed dataset combines the data\nfrom the previous two datasets equally. The scores are normalized based on Equation 2, where the\nrandom_gain_gold is 5000 and expert_gain_gold is 12000.\nWe have also generated violin charts in HoK3v3 to represent the distribution of episode returns in\neach dataset as shown in Fig. 4. The plot method used is similar to that in HoK1v1, with the exception\nof not using normalized scores in the Sub-Task: Gain Gold.\nnormalized_sub_task_score =\ngain_gold −random_gain_gold\nexpert_gain_gold −random_gain_gold\n(2)\nTable 6: Details of sampling generalization datasets.\nSampling and Training\nTesting\nEnvironments\nDatasets\nControlled side model\nOpponent side model\nOpponent side model\nHoK1v1\nnorm_general\nlevel-1\nlevel-0,2,4\nlevel-1\nhard_general\nlevel-5\nlevel-0,2,4\nlevel-5\nHoK3v3\nnorm_general\nlevel-5\nlevel-1,4,7\nlevel-5\nhard_general\nlevel-8\nlevel-1,4,7\nlevel-8\nD\nEnvironment Details\nD.1\nHonor of Kings Arena\nFor a more detailed account of the game settings, please refer to the original paper [42] and its\ndocumentation6 of Honor of Kings Arena. In this context, we will only summarize the critical\ninformation that is relevant to the RL research.\n• Observation Space\nWe have utilized the fundamental set of observations presented in the aforementioned paper [42].\nSpecifically, the observation space of Honor of Kings Arena consists of a normalized vec-\ntor with 725 dimensions, which includes five main components: hero_state_common_feature,\nhero_private_feature, creep_feature, turret_feature, and global_feature. The details of the ob-\nservation vector are demonstrated in Table 7. In the table, Main_camp and Enemy_camp refer to\nthe information of the controlled side and enemy side, respectively. Moreover, the information of\ninvisible units is set to the default value.\n• Action Space To tackle the complicated control, the Honor of Kings adopt a structured action space.\nSpecifically, illustrated in Fig. 5 the action space is 6 dimensions, consisting of a triplet form, i.e. the\naction button, the movement or skill offset and the target game unit, which covers all the possible\nactions of the hero hierarchically: 1) what action button to take, e.g. skill or move.; 2) who to target,\ne.g., a turret, an enemy hero, or a creep in the troop; 3) how to act, e.g., the discretized direction to\nmove and release skills [42]. Please refer to Table 8 for details of action space in HoK1V1.\n• Action Mask There are two action masks designed to reduce the complexity of the action space,\nnamely the legal_action_mask and the sub_action_mask. The former is constructed based on the\nrules of the game in order to exclude illegal actions, while the latter is determined by the selected\nbutton to eliminate actions that cannot be executed simultaneously with the chosen button, such as\n’skill offset’ and ’target unit’ are not needed for ’move’.\n6https:\/\/aiarena.tencent.com\/hok\/doc\/\n16\nTable 7: Details of observation vector in Honor of Kings Arena\nfeature name\ndimensions\ndescription\nMain_camp_hero_state_common_feature\n102\nhero’s status, including whether it’s\nalive, its ID and its health points\n(HP)\nMain_camp_hero_private_feature\n133\nhero’s specific kill information\nEnemy_camp_hero_state_common_feature\n102\nenemy\nhero’s\nstatus,\nincluding\nwhether it’s alive, its ID, its health\npoints (HP)\nEnemy_camp_hero_private_feature\n133\nenemy hero’s specific kill informa-\ntion\nPublic_feature\n14\nvisible information because of the\nturret\nMain_camp_soldier_feature\n18*4\nthe status of the creeps in a troop,\nincluding location and HP\nEnemy_camp_soldier_feature\n18*4\nthe status of the enemy’s creeps in a\ntroop, including location and HP\nMain_camp_organ_feature\n18*2\nthe status of turret and crystal\nEnemy_camp_organ_feature\n18*2\nthe status of enemy’s turret and crys-\ntal\nGlobal_feature\n25\nthe period of the match\nTable 8: Description of action space in HoK1v1\nAction Class\nNumbers\nDescription\nButton\n12\nwhat action button to take, e.g. skill or move.\nMove X\n16\nmove direction along X-axis.\nMove Y\n16\nmove direction along Y-axis.\nSkill X\n16\nskill offset along X-axis.\nSkill X\n16\nskill offset along Y-axis.\nTarget\n8\nwho to target, e.g., a turret or an enemy hero\n• Reward Design The basic hero reward is a weighted average of several reward items, which is\ndemonstrated in Equation 3. Subsequently, the hero’s reward is transformed into a zero_sum value\nby subtracting the enemy’s reward from it, as shown in Equation 4. Here, team_reward represents\nthe average reward of the heroes within the team. Details of the reward items are demonstrated in\nTable 9.\n.\nhero_reward =w1 ∗farming_related + w2 ∗KDA_related + w3 ∗damage_related\n+ w4 ∗pushing_related + w5 ∗win\/lose_related\n(3)\nhero_rewardzero_sum = hero_reward −team_rewardenemy\n(4)\nD.2\nHonor of Kings 3v3 Arena\nFor a more detailed account of the game settings, please refer to the documentation of Honor of\nKings 3v3 Arena (HoK3v3) 7. The environment code of HoK3v3 is integrated into the open-source\n1v1 code, both with official authorization from Honor of Kings 8.\n• Observation Space Specifically, the observation space of HoK3v3 consists of a normalized vector\nwith 4586 dimensions. The details of observation vector are presented in Table 10.\n• Action Space The form of action space in HoK3v3 is similar to that in HoK1v1 while the number\nof actions is larger. Description of action space in HoK3v3 is presented in Table 11.\n7https:\/\/doc.aiarena.tencent.com\/paper\/hok3v3\/latest\/hok3v3_env\/honor-of-kings\/\n8https:\/\/github.com\/tencent-ailab\/hok_env\n17\nTable 9: Description of reward items in HoK1v1\nItems\nType\nDescription\nhp_point\ndense\nthe rate of health point of hero\ntower_hp_point\ndense\nthe rate of health point of tower\nmoney\ndense\nthe increment of gold\nep_rate\ndense\nthe rate of mana point\ndeath\nsparse\nbeing killed\nkill\nsparse\nkilling an enemy hero\nexp\ndense\nthe increment of experience\nlast_hit\nsparse\nthe lst hit for soldier\nTable 10: Details of observation vector in HoK3v3.\nfeature name\ndimensions\ndescription\nFeatureImgLikeMg\n6*17*17\nimage-like feature, comprising six\nchannels, which include barriers,\ngrass, and other elements.\nVecFeatureHero\n6*251\nthe status of six heroes from the re-\nspective of controlled hero.\nMainHeroFeature\n44\nprivate information of controlled\nhero.\nVecSoldier\n20*25\nthe status of all creeps.\nVecOrgan\n6*29\nthe status of turrets and crystals in\nboth side.\nVecMonster\n20*28\nthe status of all monsters.\nVecCampsWholeInfo\n68\nthe status feature of the whole game.\n• Reward Design The basic hero reward is a weighted average of several reward items. Then the\nreward of each hero is processed to be zero_sum in minus the team reward of enemy which is the\naverage of the hero rewards of 3 enemy heroes. The details of reward items are demonstrated in\nTable 12\nE\nFramework APIs\nWe provide an example of the APIs in our framework, Listing 1. A comprehensive account of our\nframework can be found in our readily accessible open-access code repository.\nF\nEvalution Protocols: Multi-Level Models\nBased on the parallel training system named SAIL proposed by previous work [49], we have extracted\nand published several checkpoints from pre-trained dual-clip PPO [49, 48] models with varying levels\ndetermined by the outcome of the battle separately for HoK1v1 and HoK3v3.\nHere, we present tables displaying the win rate of each level model against the model listed di-\nrectly below it. The win rate is calculated with fixed hero selection, i.e. luban for HoK1v1 and\nTable 11: Description of action space in HoK3v3\nAction Class\nNumbers\nDescription\nButton\n13\nwhat action button to take, e.g. skill or move.\nMove\n25\nmove direction.\nSkill X\n42\nskill offset along X-axis.\nSkill X\n42\nskill offset along Y-axis.\nTarget\n39\nwho to target, e.g., a turret or an enemy hero\n18\nTable 12: Description of reward items in HoK3v3\nItems\nType\nDescription\nhp_rate_sqrt_sqrt\ndense\nthe fourth root of the rate of health point of hero\nmoney\ndense\nthe increment of gold\nexp\ndense\nthe increment of experience\ntower\ndense\nthe rate of health point of turrets\nkillCnt\nsparse\nkill an enemy\nassistCnt\nsparse\nassisting in the termination of an adversary\ndeadCnt\nsparse\nbeing killed\ntotal_hurt_to_hero\ndense\ndamage dealt to the enemies\natk_monster\ndense\nattack an monster\natk_crystal\ndense\nattack the crystal of enemy\nwin_crystal\nsparse\ndestroy the crystal of enemy\n{zhaoyun,diaochan,liyuanfang} for HoK3v3, which may not right for other hero selection. Table 13\npresents the win rate in the HoK1v1, where the win_rate column represents the win rate of model1\nagainst model2. Table 14 displays the win rate in HoK3v3. Additionally, we have included an API in\nour framework that allows researchers to conveniently test the win rate between any levels.\nTable 13: Win rate of multi-level models in HoK1v1\nmodel1\nmodel2\nwin_rate\n1v1_level_1\n1v1_level_0\n88%\n1v1_level_2\n1v1_level_1\n79%\n1v1_level_3\n1v1_level_2\n59%\n1v1_level_4\n1v1_level_3\n97%\n1v1_level_5\n1v1_level_4\n70%\n1v1_level_6\n1v1_level_5\n73%\n1v1_level_7\n1v1_level_6\n70%\nTable 14: Win rate of multi-level models in HoK3v3\nmodel1\nmodel2\nwin_rate\n3v3_level_1\n3v3_level_0\n97%\n3v3_level_2\n3v3_level_1\n83%\n3v3_level_3\n3v3_level_2\n50%\n3v3_level_4\n3v3_level_3\n65%\n3v3_level_5\n3v3_level_4\n63%\n3v3_level_6\n3v3_level_5\n59%\n3v3_level_7\n3v3_level_6\n80%\n3v3_level_8\n3v3_level_7\n82%\nG\nAdditional Experimental Details\nG.1\nAdditional Algorithm Details\n• Encoder: Due to the complexity of the observation space, it is necessary to utilize a well-designed\nencoder for effective feature extraction. Taking inspiration from the \"divide and conquer\" approach\nemployed in previous works [49, 48], in each algorithm, we implement a shared encoder network to\nprocess features, instead of directly feeding raw observations into the policy or critic network. For\nfurther details on the design of the encoder network, please refer to the mentioned papers [49, 48] as\nwell as our code.\n•\nBC: Behavior clone with maximum likelihood estimation loss. While, in multi-agent setting,\nHoK3v3, we adopt shared parameter and independent learning paradigm [38].\n19\n• CQL [19]: The implementation of Conservative Q-Learning is based on the original version [19]\ndesigned for discrete action spaces9.\n• QMIX+CQL: Due to the decoupling of control dependencies [49], the action space of HoK is\nstructured with multi-head, which is similar to the joint action space in multi-agent settings. Inspired\nby this, we propose QMIX-CQL by incorporating mixer in QMIX [30] with CQL and use global Q\nto calculate td error term and use local Q to calculate CQL-loss term.\n• TD3+BC [7]: Our implementation of TD3-BC is based on the open-source code10. In addition,\nthe policy network and critic network share an encoder, which is updated simultaneously by both\nlosses. Besides, We utilize Gumbel-Softmax reparameterization method to generate discrete actions\nfor TD3 [8].\n• IQL [16]: We implement IQL based on the open-source pytorch version11. The network design is\nsimilar to TD3-BC except for an additional value network.\n• IND+CQL and COMM+CQL: To accommodate a multi-agent setting, based on the implemen-\ntation of CQL, we adopt the independent learning paradigm and shared parameters, referred to as\nIND-CQL. Additionally, we introduce COMM-CQL which adds communication between agents by\nmeans of shared information that is constructed using max pooling.\n•\nIND+ICQ and MAICQ [47]: We implement IND+ICQ and MAICQ based on the original\npublished code12. IND+ICQ adopts independent learning paradigm, while MAICQ adopts CTDE\nparadigm by decomposing the joint-policy under the implicit constraint. The actor and critic networks\nupdate the shared encoder simultaneously as TD3-BC.\n•\nOMAR [29]: The open-access code13 of OMAR is not suitable for a discrete action space.\nConsequently, based on the core idea of it, we have undertaken the task of re-implementing OMAR\nto accommodate a discrete version.\nG.2\nHyperparameters\nWe have compiled the hyperparameters of HoK1v1 and HoK3v3 in Tables 15 and 16, respectively.\nThese tables encompass the parameters of the training process, algorithm and optimizer settings.\nRegarding the computing resources employed in HoK1v1, we utilize the Tesla T4 GPU and the\nAMD EPYC 7K62 48-Core Processor CPU. For the sampling process, 50 CPU cores are utilized,\nand each dataset required approximately 30 to 40 minutes for sampling. During the training process,\neach training experiment is conducted with one Tesla T4 GPU and two CPU cores, with an average\ntraining time of 9 hours per seed for 500000 training steps.\nRegarding the computing resources employed in HoK3v3, we utilize the Tesla T4 GPU and the\nAMD EPYC 7K62 48-Core Processor CPU. For the sampling process, 50 CPU cores are utilized,\nand each dataset required approximately 80-90 minutes for sampling. During the training process,\neach training experiment is conducted with one Tesla T4 GPU and four CPU cores, with an average\ntraining time of 20 hours per seed for 500000 training steps.\nConsequently, a total of 14 GPUs and 552 CPU cores are used to accommodate the overall computa-\ntion requirements.\nG.3\nAdditional Results Discussion\n• Why is the performance of baseline models in the HoK1v1 comparatively inferior to those in\nthe HoK3v3 setting?\nThe experimental results conducted in the HoK1v1 reveal that the performance of baseline models is\ncomparatively inferior to those in the HoK3v3 setting. This disparity can be attributed to the higher\nlevel of adversarial conditions present in the HoK1v1 environment. Furthermore, within the context\n9https:\/\/github.com\/aviralkumar2907\/CQL\/tree\/master\/atari\n10https:\/\/github.com\/sfujim\/TD3_BC\n11https:\/\/github.com\/gwthomas\/IQL-PyTorch\n12https:\/\/github.com\/YiqinYang\/ICQ\/tree\/5a4da859ef597005040f79128ee6163547cf178d\n13https:\/\/github.com\/ling-pan\/OMAR\n20\nTable 15: Hyperparameters for HoK1v1. The values of hyperparameters for algorithms are derived\nfrom their original implementation.\nHyperparameters\nValue\nBatch Size\n128\nγ\n0.99\nMax Steps (Exclude Sub-Task Datasets)\n500000\nMax Steps (Sub-Task Datasets)\n100000\nLSTM Time Steps\n16\nτ (Soft-Target-Update)\n0.005\nnum_threads\n2\nfinal_evaluation_episodes\n150\nCQL α\n10.0\nTD3+BC α\n2.5\nIQL τ\n0.7\nIQL β\n3.0\nOptimizer\nAdam\nbeta1\n0.9\nbeta2\n0.999\neps\n1.00E-08\nLearning Rate\n3.00E-04\nTable 16: Hyperparameters for HoK3v3. The values of hyperparameters for algorithms are derived\nfrom their original implementation.\nHyperparameters\nValue\nBatch Size\n512\nγ\n0.99\nHard Update Frequency\n2000\nMax Steps\n500000\nMax Steps (Sub-Task)\n100000\nIteration Steps\n1000\nBuffer Workers\n2\nnum_threads\n4\nfinal_evaluation_episodes\n150\nCQL α\n10.0\nICQ critic β\n1000\nICQ policy β\n0.1\nOMAR coe\n0.5\nOptimizer\nAdam\nbeta1\n0.9\nbeta2\n0.999\neps\n1.00E-08\nLearning Rate\n1.00E-04\n21\nof HoK3v3, if one teammate makes a sacrifice during a battle, the remaining two teammates are able\nto maintain their collaboration and continue to fight. This aspect ensures a greater level of robustness\nin the HoK3v3 when compared to the HoK1v1.\n• What are the reasons behind the underperformance of TD3+BC and OMAR?\nTD3+BC and OMAR demonstrated subpar performance in HoK1v1 and HoK3v3, respectively. The\nmain cause of their lackluster outcomes stems from the fact that TD3 [8], upon which TD3+BC and\nOMAR are built, is incompatible with discrete action spaces. To enhance OMAR’s performance, we\nreplaced TD3 with advantage-weighted BC. This modification resulted in performance improvements.\n• QMIX+CQL in HoK3v3.\nWe also implemented QMIX+CQL in the HoK3v3 game mode by adopting an independent learning\nparadigm. We thoroughly validated the performance of QMIX+CQL and compared it with IND+BC\nand IND+CQL, aggregating the results in Table 17. It is demonstrated that QMIX+CQL exhibits\nsuperior performance compared to IND+CQL, indicating that our novel method is also suitable for\nmulti-agent settings with a structured action space.\nTable 17: Validation of QMIX+CQL in HoK3v3 game mode.\nFactors\nDatasets\nIND+BC\nIND+CQL\nQMIX+CQL\nMulti-Level\nnorm_poor\n0.1±0.01\n0.03±0.01\n0.11±0.03\nnorm_medium\n0.48±0.01\n0.4±0.03\n0.52±0.04\nnorm_expert\n0.52±0.03\n0.84±0.06\n0.85±0.04\nnorm_mixed\n0.35±0.25\n0.46±0.12\n0.47±0.29\nhard_poor\n0.16±0.03\n0.12±0.03\n0.13±0.02\nhard_medium\n0.38±0.05\n0.31±0.02\n0.37±0.08\nhard_expert\n0.65±0.01\n0.67±0.04\n0.7±0.03\nhard_mixed\n0.32±0.14\n0.3±0.1\n0.43±0.04\nGeneralization\nnorm_general\n0.34±0.05\n0.29±0.09\n0.34±0.02\nhard_general\n0.28±0.03\n0.28±0.04\n0.32±0.01\nnorm_multi_hero_general\n0.17±0.03\n0.14±0.04\n0.18±0.06\nhard_multi_hero_general\n0.16±0.05\n0.17±0.02\n0.19±0.02\nnorm_multi_oppo_general\n0.21±0.01\n0.14±0.04\n0.14±0.03\nhard_multi_oppo_general\n0.09±0.06\n0.09±0.02\n0.1±0.02\nMulti-Task\nnorm_multi_level\n0.43±0.09\n0.34±0.04\n0.45±0.02\nhard_multi_level\n0.38±0.08\n0.29±0.07\n0.35±0.04\nnorm_multi_hero\n0.57±0.07\n0.3±0.07\n0.56±0.13\nnorm_multi_oppo\n0.09±0.04\n0.07±0.03\n0.09±0.02\nnorm_multi_hero_oppo\n0.3±0.04\n0.26±0.07\n0.3±0.03\nHeterogeneous\nnorm_stupid_partner\n0.11±0.15\n0.24±0.17\n0.55±0.05\nnorm_expert_partner\n0.36±0.09\n0.57±0.04\n0.72±0.07\nnorm_mixed_partner\n0.49±0.2\n0.32±0.03\n0.66±0.05\nSub_Task\ngain_gold_medium\n0.13±0.01\n0.12±0.01\n0.13±0.02\ngain_gold_expert\n1.01±0.03\n1±0.01\n1.02±0.01\ngain_gold_mixed\n0.64±0.29\n0.41±0.1\n0.58±0.17\nH\nAdditional Discussion\nH.1\nThe significance of our design factors in the context of offline reinforcement learning\nTask difficulty: Intuitively, the level of difficulty in the environment significantly impacts the\nperformance of algorithms. However, previous researches only utilized one set of datasets with a\nuniform level of difficulty in the environment. Providing datasets with diverse difficulty can not only\nmore comprehensively evaluate the ability of offline algorithms but also be more suited for real-world\ntasks like HoK, which are characterized by diverse levels of difficulty.\n22\nMulti-task: Combining offline reinforcement learning with multi-task learning enables efficient use\nof limited data. Sharing knowledge[1] and representations across tasks enhances data efficiency,\nleading to more general and robust feature learning. Besides, multi-task learning facilitates knowledge\ntransfer between tasks. Leveraging shared parameters and representations accelerates learning for the\ntarget task in offline reinforcement learning, benefiting from related tasks’ knowledge.\nGeneralization: Firstly, in offline RL, learning is based on a fixed dataset collected from previous\nexperiences. This dataset might not cover all possible scenarios, so the learned policy needs to\ngeneralize well to new, unseen situations to perform effectively. Secondly, real-world environments\nare often complex and diverse. A policy only limited to the dataset without generalizing would likely\nfail when facing even slightly different conditions. Generalization ensures the policy’s adaptability to\nvarious situations in the real-world scenarios.\nHeterogeneous Teammate: Heterogeneous teammates are a crucial research direction in Multi-Agent\nReinforcement Learning (MARL). In practical scenarios such as HoK or other multi-agent systems,\nplayers typically possess varying capacities. Consequently, the datasets collected from real-world\nscenarios consist of heterogeneous teammate data, necessitating the need for corresponding research\nin the offline MARL domain.\nH.2\nFrom the perspective of the Honor of Kings game, why offline reinforcement learning is\nnecessary and what potential limitations exist when compared to online reinforcement\nlearning?\nTraining agents for the Honor of Kings game using offline reinforcement learning (RL) offers several\nadvantages, including reduced training time, lower computation resource requirements, and better\nutilization of existing data resources. We compare the computational and time costs of online RL\nand offline RL in Table 18. It is evident that training an online agent from scratch to reach specific\nlevels (level_5 for HoK1v1 and level_7 for HoK3v3) requires thousands of CPU cores and dozens\nof hours. On the other hand, training offline agents to reach same levels only requires a few CPUs\nand a shorter training time using pre-collected datasets. Additionally, there is a wealth of previously\ncollected battle data that can be used for training offline RL agents. However, compared to online RL,\nit is important to note that offline RL in the HoK game heavily relies on large amounts of high-level\nbattle data to train expert-level agents, which may be a potential limitation.\nTable 18: The comparison of the computational and time costs between online RL and offline RL.\nOnline\/Offline\nCPU cores\nGPU cores\nPerformance\nTraining time (hours)\nHoK1v1 (online)\n4000\n2\n1v1_level_5\n60h\nHoK1v1 (offline)\n2\n1\n1v1_level_5\n9h\nHoK3v3 (online)\n1000\n1\n3v3_level_7\n97h\nHoK3v3 (offline)\n4\n1\n3v3_level_7\n20h\n23\n(a) norm_level\n(b) hard_level\n(c) level_general\n(d) multi_level\n(e) multi_hero_oppo\n(f) heterogeneous\n(g) sub_task: gain_gold\nFigure 4: Violin diagrams of all datasets in HoK3v3.\n24\nFigure 5: Action space in HoK1v1 [42]\n1 cd\n<root_path >\n2\n3 # sample\nexample\n4 sh offline_sample \/scripts\/ start_sample .sh <args >\n5\n6 # train\nexample\n7 python\noffline_train\/train.py --<args >\n8\n9 # evaluate\nexample\n10 python\noffline_eval \/evaluation.py --<args >\n11\nListing 1: APIs example\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks.pdf"}
{"title":"A Benchmark Environment for Offline Reinforcement Learning in Racing Games","authors":"Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov","summary":"Offline Reinforcement Learning (ORL) is a promising approach to reduce the\nhigh sample complexity of traditional Reinforcement Learning (RL) by\neliminating the need for continuous environmental interactions. ORL exploits a\ndataset of pre-collected transitions and thus expands the range of application\nof RL to tasks in which the excessive environment queries increase training\ntime and decrease efficiency, such as in modern AAA games. This paper\nintroduces OfflineMania a novel environment for ORL research. It is inspired by\nthe iconic TrackMania series and developed using the Unity 3D game engine. The\nenvironment simulates a single-agent racing game in which the objective is to\ncomplete the track through optimal navigation. We provide a variety of datasets\nto assess ORL performance. These datasets, created from policies of varying\nability and in different sizes, aim to offer a challenging testbed for\nalgorithm development and evaluation. We further establish a set of baselines\nfor a range of Online RL, ORL, and hybrid Offline to Online RL approaches using\nour environment.","url":"http:\/\/arxiv.org\/abs\/2407.09415v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.09415v1","published":1720802643000,"comment":"Accepted at IEEE Conference on Games","pdf_text":"A Benchmark Environment for Offline\nReinforcement Learning in Racing Games\nGirolamo Macaluso1, Alessandro Sestini2, and Andrew D. Bagdanov1\n1University of Florence, 2SEED - Electronic Arts (EA)\n{girolamo.macaluso, andrew.bagdanov}@unifi.it, asestini@ea.com\nAbstract—Offline Reinforcement Learning (ORL) is a promis-\ning approach to reduce the high sample complexity of traditional\nReinforcement Learning (RL) by eliminating the need for contin-\nuous environmental interactions. ORL exploits a dataset of pre-\ncollected transitions and thus expands the range of application of\nRL to tasks in which the excessive environment queries increase\ntraining time and decrease efficiency, such as in modern AAA\ngames. This paper introduces OfflineMania a novel environment\nfor ORL research. It is inspired by the iconic TrackMania series\nand developed using the Unity 3D game engine. The environment\nsimulates a single-agent racing game in which the objective is to\ncomplete the track through optimal navigation. We provide a\nvariety of datasets to assess ORL performance. These datasets,\ncreated from policies of varying ability and in different sizes,\naim to offer a challenging testbed for algorithm development and\nevaluation. We further establish a set of baselines for a range of\nOnline RL, ORL, and hybrid Offline to Online RL approaches\nusing our environment.\nI. INTRODUCTION\nReinforcement Learning (RL) has become increasingly pop-\nular in the gaming industry as it offers a promising way of\ncreating immersive gaming experiences. From training AI-\ncontrolled non-player characters [1, 2] to automated game\ntesting [3, 4].\nHowever the widespread use of RL is often limited by\nits sample complexity which makes training in complex en-\nvironments, such as modern AAA games, slow and ineffi-\ncient. Offline RL (ORL) has recently garnered interest as a\nframework aimed at improving the sample efficiency of RL\nagents [5]. With ORL, one can completely eliminate the need\nfor interaction with the environment and instead rely on a\npreviously collected dataset of experiences. Such datasets can\nbe made readily accessible to game developers; for instance,\nthey could use samples obtained from playtesting sessions or\ndata extracted from previously released games.\nIn this paper we introduce OfflineMania a novel game\nenvironment for Online RL and ORL, centered around a\nsingle-agent racing game inspired by the iconic TrackMa-\nnia [6] game series. Our environment, built using the Unity\n3D game engine [7], provides a track in which the agent\nmust complete the race through optimal navigation. Moreover,\nwe provide datasets of agent experiences tailored specifically\nfor benchmarking ORL techniques. These datasets are of\nvarying quality, ranging from those generated using random\nFig. 1. Visualization of OfflineMania, highlighting the track centerline used\nin the reward function, the episode starting area, the agent, and the positive\npart of the reward during a transition.\npolicies to those crafted by expert agents. We additionally offer\nsmaller and mixed versions of these datasets. These variants\nare designed to test algorithmic performance under complex\nscenarios that challenge the robustness of learning methods.\nOur work focuses on providing a gaming testbed environment\nand multiple datasets tailored for game AI research in ORL.\nTo the best of our knowledge such a combination is not\ncurrently available in the existing literature. We further provide\na study assessing the performance of different Online RL and\nORL algorithms using our new datasets. We also investigate\nthe performance of fine-tuning policies trained offline using\nOnline RL. This last framework is a more natural approach in\ngame development, as it allows developers to take advantage of\ndatasets to create a policy that then can be effectively improved\nwith fewer game interactions.\nThe key contributions of this work are:\n• we introduce OfflineMania, a new environment inspired\nby TrackMania developed using the Unity 3D engine;\n• we provide diverse datasets of varying sizes, collected\nusing policies of different expertise levels; and\n• we present results for a variety of baseline algorithms\nincluding both Online and ORL approaches, as well as\nhybrid methods that combine Offline training with Online\nfine-tuning.\nII. RELATED WORK\nOffline Reinforcement Learning (ORL).\nOffline Rein-\nforcement Learning is a promising way to address the sample\n979-8-3503-5067-8\/24\/$31.00 ©2024 IEEE\narXiv:2407.09415v1  [cs.AI]  12 Jul 2024\ncomplexity challenges faced by Online RL. The core objective\nof ORL is to create a robust policy from a fixed dataset of pre-\ncollected environment transitions, without requiring further\nonline interactions with the environment [8, 9, 10]. ORL has\nemerged as a promising approach to training game agents [11].\nModern AAA games are often computationally demanding,\nslow to simulate, and inherently unstable, all of which lead to\nincreased need for extensive interactions with the environment.\nORL has significantly benefited from the development of\nbenchmarks like D4RL [12], which provides a wide range of\ndatasets across various domains such as locomotion, robotic\nmanipulation, and vision-based autonomous driving. However,\nto the best of our knowledge, there is no ORL dataset available\nin the literature specifically tailored for studying these tech-\nniques in gaming environments, particularly in the context of\nracing games. This paper aims to bridge this gap.\nOffline to Online RL.\nThe Offline to Online approach\nfocuses on how to effectively improve policies trained with\nORL in a online setting which allows further environment\ninteractions. It is a promising approach for training game\nagents [13] since a game in development can change on\na daily basis, potentially rendering the datasets collected\nin an environment iteration insufficient for subsequent it-\nerations. Offline to Online RL can allow game developers\nto seamlessly transition to new environment with minimal\nonline interactions. However, this transition poses significant\nchallenges [14, 15], including dealing with distribution shifts\nbetween the offline data and the new online interactions. For\nthese reasons, with this paper we aim to provide a benchmark\nsuite for investigating the impact of Offline to Online training\nin gaming environments.\nIII. ENVIRONMENT AND DATASETS\nA. Environment\nWe developed OfflineMania, shown in Figure 1, using the\nUnity 3D game engine leveraging the ML-Agents package [7].\nIt features a Gymnasium-compatible interface [16], ensuring\nstraightforward integration into existing experimental setups.\nThe game is computationally efficient, with the game speed\neasily adjustable to speed up online training process. Addition-\nally, the environment supports rendering capabilities, offering\na birds-eye view of the car. This visualization facilitates\nqualitative assessment of agent behavior and simplifies the\nevaluation process.\nWe now describe the main elements of the environment: the\nstate and action-space, the reward function, and episode loop.\nState Space.\nThe state space is a vector in R33. It is\ncomposed by 15 raycasts covering a 180-degree field of view\nin front of the car, with each ray are associated two values:\none indicating the presence of an object within its path,\nand the other specifying the distance to the detected object.\nAdditionally, the components of the velocity of the car are\nincluded as part of the state representation.\nAction Space.\nThe agent action space consists of two\ncontinuous values. The first value controls the steering angle\nof the car which ranges from -1 (indicating a left turn) to 1\n(indicating a right turn). The second value controls the accel-\neration or braking of the car, with a value of 1 corresponding\nto full acceleration and a value of -1 representing braking or\nreversing when the car is stationary.\nReward Signal.\nOur reward function draws inspiration from\nprior work [2]. We denote with pt the position of the car\nprojected onto the track centerline at timestep t, and by pbest\nthe most advanced position achieved thus far in the episode.\nOur reward is then:\nrt = rprog\nt\n−\n(\nλ ∥vcar ∥\nif in contact with wall\n0\notherwise\n,\n(1)\nwhere rprog\nt\nquantifies the progress of position pt along the\ncenterline relative to pbest, as shown in Figure 1. We set rprog\nt\nto 0 if pt does not advance beyond pbest. vcar is the magnitude\nof the velocity at the moment of impact, and λ is a fixed\ncoefficient penalizing collisions. In our environment, λ = 50.\nEpisode.\nDuring each episode the vehicle starts with its\nposition dynamically chosen within a designated square area\nbefore the fist turn in the track. We also randomize the\norientation of the car, between -30 and 30 degrees from the\ncenterline, which ensures that it is always facing the correct\ndirection. Every episode has a fixed length of 2,000 steps. In\nthis many steps an expert agent can complete at most 5 laps.\nB. Datasets\nIn order to support comprehensive research in ORL, we\ngenerated a diverse series datasets. First, we train three dis-\ntinct policies using Proximal Policy Optimization (PPO) [17],\nstopping the training after 1,000, 5,000, and 12,500 network\nupdates, respectively. Each of the trained policy represents\nvarying degrees of ability in navigating the race track. The\npolicies obtain mean cumulative reward of -360, 327, and\n1183 respectively, over five episodes. The first policy struggles\nwith the initial corner. The second policy, while capable\nof occasionally completing the track, exhibits inconsistent\nperformance. In contrast, the third policy consistently achieved\nhigh performance by efficiently navigating the track, including\ncorner-cutting strategies, successfully completing 5 laps in\neach episode.\nUsing these three policies, we collected three distinct\ndatasets: basic, medium, and expert, each consisting of\n100,000 transitions. Additionally, we created mixed datasets,\none consisting of 200,000 transitions in total and another\nof only 5,000. We refer to these datasets with mix large\nand mix small, respectively. The mixed datasets consist of\n90% transitions sampled from the basic policy, 7% from the\nmedium policy, and only 3% from the expert policy. The\ndistribution of transitions in the mixed datasets was chosen\nto simulate a complex scenario in which ORL algorithms\nmust stitch together different behaviors in order to correctly\nlearn an optimal policy. The smaller version of the mixed\ndataset is useful to understand the behavior of ORL approaches\nwhen dealing with small datasets. Following this idea, we built\nanother datasets from only 5,000 transitions collected exclu-\nsively from the basic policy, called basic small. Those two\nvariants – mix small and basic small – offer a more demanding\ntestbed for evaluating the robustness and adaptability of ORL\nalgorithms under complex training conditions.\nIV. BENCHMARK STUDY\nOfflineMania aims to be a testbed for developing new\ntraining techniques. We provide results of a set of baselines for\nwidely recognized methods for Online RL, ORL, and Offline\nto Online RL. For all approaches we present the mean results\nover five different seeds. All experiments were conducted\nusing a system equipped with a Nvidia RTX 2070 and an\nAMD Ryzen 3600X processor.\nFor Online RL baselines, we opted for two state-of-the-\nart methods: Proximal Policy Optimization (PPO) [17] and\nSoft Actor Critic (SAC) [18]. PPO, known for its efficacy and\nrobustness to hyperparameter selection, represents a widely\nused policy-based approach. Similarly, we selected SAC, an\nactor-critic algorithm, for its efficiency and for its importance\nin the RL landscape. We trained our PPO agent over 12,500\nnetwork updates, for a total of 15 million environment interac-\ntions and approximately 10 hours of training time. In contrast,\ntraining with SAC spanned 3 million network updates, with\nan equivalent number of environment interactions, totaling\napproximately 20 hours of training time.\nFor ORL approaches, we chose Conservative Q-Learning\n(CQL) [10], Twin Delayed Deep Deterministic policy gradi-\nent with Behavioral cloning (TD3BC) [9], and Implicit Q-\nLearning (IQL) [8]. For all algorithms we present results after\n300,000 network updates, corresponding to about one hour of\ntraining time for all algorithms.\nFor Offline to Online approaches, we compare various\nmethods. These include: an approach combining TD3BC [9]\nfor offline training and TD3 [19] for online fine tuning;\nIQL [8], following the fine-tuning process outlined in the orig-\ninal paper; Jump Start Reinforcement Learning (JSRL) [20],\nwhich utilizes offline policies as guides for online training;\nPolicy EXpainsion (PEX) [21], a method combining offline\npolicies with online training to enhance exploration; and the\nwork of Macaluso et al. [22] that we will refer as SDBG,\ndesigned specifically for small offline datasets, utilizing a\nworld model based augmentation to improve offline training.\nFor each approach we present results after 300,000 offline\nnetwork updates and 1 million online fine-tuning updates, for\na total of about 4 hours of training across all algorithms. Since\nPEX, SDGB, and JSRL are agnostic to the algorithm used, we\ndecided to show results using IQL.\nA. Online RL Results\nWe use online RL for training a policy from scratch with\nenvironment interactions. The resulting mean reward achieved\nby PPO at the end of the training was 1183, indicative of\nconsistent high-quality behaviors on the track. The policy is\nproficient at navigating the track, and is also able to cut corners\neffectively. It can complete a lap in 385 steps, for a total of 5\nTABLE I\nAVERAGE REWARDS OF OFFLINE REINFORCEMENT LEARNING TRAINING\nAFTER 300,000 NETWORK UPDATES.\nMethods\nTD3BC\nCQL\nIQL\nExpert\n-3981±57\n-3325±1353\n1192±1\nMedium\n335±87\n-4227±571\n789±58\nBasic\n12±9\n39±81\n98±38\nMix Large\n219±96\n-4080±873\n828±38\nMix Small\n-1125±154\n-3972±858\n10±32\nBasic Small\n64±31\n-3488±626\n20±102\nlaps in a single episode. PPO succeeds at the cost of a large\nnumber of environment interactions (about 15 million).\nConversely, despite training for 20 hours and 3 million\nenvironment interactions, the SAC outcomes are considerably\nless promising. The SAC policy achieves a total mean re-\nward of only 215 and demonstrates suboptimal performance\ncharacterized by slower and less stable navigation of the\ntrack compared to the PPO-trained policy. This highlights the\nlimitations of SAC, despite its better sample efficiency, in\nproducing effective policies even after extensive training.\nThese results quantify the efficacy of Online RL approaches.\nHowever, they also highlight the challenges posed by the slow\ntraining speed and significant sample inefficiency.\nB. Offline RL Results\nIn Table I we present the results for ORL baselines on\nthe datasets described in Section III-B. For all algorithms the\ntraining time for 300,000 network updates was under one hour.\nRemarkably, IQL consistently outperforms TD3BC and CQL\nacross nearly all datasets. Of particular note is the ability of\nIQL to learn policies from the expert dataset that even surpass\nthe performance of the policy used to generate it. On all other\ndatasets, IQL produces policies that are capable of navigating\nthe track without major collisions.\nConversely, TD3BC and CQL unexpectedly fall short on\nall datasets, most notably on the expert dataset. This may be\nattributed to sensitivity to the hyperparameters that is a well\nknown problem in ORL [5].\nC. Offline to Online RL Results\nIn this section we report baseline performance for the\ncombination of ORL pre-training and Online RL fine-tuning.\nSuch approaches are useful when an offline policy fails to meet\ndeployment standards. In these cases, we would like to im-\nprove the offline-trained policy by allowing some interactions\nwith the environment, while still minimizing such interactions.\nWhile this process might appear straightforward, it is\ncomplex primarily due to the distributional shift problem.\nDistributional shift occurs during the first training iterations\nwhen moving to online training, as the agent navigates into\nunexplored state-action spaces. In this setting the values of\nthe Q-function trained during the offline phase may become\nhighly inaccurate. This inaccuracy can lead to incorrect policy\nevaluations and arbitrary policy updates in these unseen states,\nwhich undermines the policy learned through ORL [15].\nTABLE II\nAVERAGE REWARDS FOR OFFLINE-TO-ONLINE APPROACHES AFTER 300,000 OFFLINE NETWORK UPDATES AND 1 MILLION ONLINE UPDATES.\nMethods\nTD3BC+TD3\nIQL\nSDBG-IQL\nPEX-IQL\nJSRL-IQL\nExpert\n15±11\n1192±4\n1199±3\n-2425±2132\n1193±1\nMedium\n-497±773\n856±58\n852±12\n776±39\n856±1\nBasic\n11±7\n71±13\n130±35\n-173±504\n99±33\nMix Large\n-2538±1170\n932±50\n1152±12\n671±175\n1142±2\nMix Small\n-3886±923\n-614±882\n340±187\n-2147±1561\n-826±86\nBasic Small\n-4352±1425\n3±33\n105±3\n-2200±2042\n152±1\nIn Table II we present the results of the experiments in this\nsetup. The combination of TD3BC and TD3 fails to improve\nthe offline policy and achieves worse performances on all\ndatasets. This might be due to the distributional shift problem\nwhich is not explicitly addressed by this approach. On the\nother hand, IQL demonstrates good fine-tuning performance\nand improves its offline results in almost all datasets. Only\nfor the mix small dataset the score is substantially reduced.\nThis is probably due to the difficulty in learning with such a\nsmall dataset derived from different policies.\nAlthough PEX attempts to directly address the challenge of\ntraining an online policy after offline pre-training, it achieves\nthe worst performance – lower even than the standard fine-\ntuning process of IQL. On the other hand, SDBG and JSRL\nshow remarkable performance. Even though SDBG specifi-\ncally addresses training setups with small datasets – as seen\nin the mix small and basic small results from Table II – it is\nthe only algorithm that improves the offline performance in\nall tasks. JSRL achieves good performance across all tasks as\nwell, except again for the mix small dataset.\nV. DISCUSSION\nORL is a promising approach to fostering more widespread\nuse of RL in modern video games, however there is a lack\nof environments with datasets that can be used to test the\ncapabilities of ORL methods in challenging conditions. This\npaper introduces a novel game environment which serves as a\nbenchmark for ORL research. We provide datasets of varying\ndata quality and quantity which facilitates the simulation of\ncomplex training scenarios1. Additionally, we present results\nfrom state-of-the-art methods for Online RL, ORL, and hybrid\nOffline to Online RL approaches. This benchmark aims to\nfacilitate future investigation into the use of offline data in\ngaming environments. By leveraging offline datasets, we can\neffectively mitigate the challenges in applying RL techniques\nto modern games, thereby contributing the integration of RL\ninto modern game development workflows.\nREFERENCES\n[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, 2016.\n[2] F. Fuchs, Y. Song, E. Kaufmann, D. Scaramuzza, and P. D¨urr, “Super-\nhuman performance in gran turismo sport using deep reinforcement\nlearning,” IEEE Robotics and Automation Letters, 2021.\n1Env. and datasets are available at: https:\/\/github.com\/ganjiro\/OfflineMania\n[3] A. Sestini, L. Gissl´en, J. Bergdahl, K. Tollmar, and A. D. Bagdanov,\n“Automated gameplay testing and validation with curiosity-conditioned\nproximal trajectories,” IEEE Transactions on Games, 2022.\n[4] J. Bergdahl, C. Gordillo, K. Tollmar, and L. Gissl´en, “Augmenting\nautomated game testing with deep reinforcement learning,” in IEEE\nConference on Games (CoG), 2020.\n[5] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline reinforcement\nlearning: Tutorial, review, and perspectives on open problems,” arXiv\npreprint arXiv:2005.01643, 2020.\n[6] Ubisoft, “Trackmania,” 2020. [Online]. Available: https:\/\/www.ubisoft.\ncom\/en-us\/game\/trackmania\/trackmania\n[7] A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy,\nY. Gao, H. Henry, M. Mattar et al., “Unity: A general platform for\nintelligent agents,” arXiv preprint arXiv:1809.02627, 2018.\n[8] I. Kostrikov, A. Nair, and S. Levine, “Offline reinforcement learning\nwith implicit q-learning,” arXiv preprint arXiv:2110.06169, 2021.\n[9] S. Fujimoto and S. S. Gu, “A minimalist approach to offline reinforce-\nment learning,” Advances in neural information processing systems,\n2021.\n[10] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative q-learning\nfor offline reinforcement learning,” Advances in Neural Information\nProcessing Systems, 2020.\n[11] A. Kobanda, C. Valliappan, J. Romoff, and L. Denoyer, “Learning\ncomputational efficient bots with costly features,” in IEEE Conference\non Games (CoG), 2023.\n[12] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:\nDatasets for deep data-driven reinforcement learning,” arXiv preprint\narXiv:2004.07219, 2020.\n[13] A. Sestini, J. Bergdahl, K. Tollmar, A. D. Bagdanov, and L. Gissl´en,\n“Towards informed design and validation assistance in computer games\nusing imitation learning,” in IEEE Conference on Games (CoG), 2023.\n[14] A. Nair, A. Gupta, M. Dalal, and S. Levine, “Awac: Accelerating\nonline reinforcement learning with offline datasets,” arXiv preprint\narXiv:2006.09359, 2020.\n[15] S. Lee, Y. Seo, K. Lee, P. Abbeel, and J. Shin, “Offline-to-online\nreinforcement learning via balanced replay and pessimistic q-ensemble,”\nin Conference on Robot Learning.\nPMLR, 2022, pp. 1702–1712.\n[16] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola,\nT. Deleu, M. Goul˜ao, A. Kallinteris, A. KG et al., “Gymnasium,” Mar.\n2023. [Online]. Available: https:\/\/zenodo.org\/record\/8127025\n[17] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning.\nPMLR, 2018.\n[19] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in International conference on\nmachine learning.\nPMLR, 2018.\n[20] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice,\nC. Fu, C. Ma, J. Jiao et al., “Jump-start reinforcement learning,” in\nInternational Conference on Machine Learning.\nPMLR, 2023.\n[21] H. Zhang, W. Xu, and H. Yu, “Policy expansion for bridging offline-to-\nonline reinforcement learning,” arXiv preprint arXiv:2302.00935, 2023.\n[22] G. Macaluso, A. Sestini, and A. D. Bagdanov, “Small dataset, big gains:\nEnhancing reinforcement learning by offline pre-training with model-\nbased augmentation,” in Computer Sciences & Mathematics Forum.\nMDPI, 2024.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/A Benchmark Environment for Offline Reinforcement Learning in Racing Games.pdf"}
{"title":"Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard","authors":"Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper","summary":"We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.","url":"http:\/\/arxiv.org\/abs\/2407.07796v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.07796v2","published":1720628074000,"comment":null,"pdf_text":"EVALUATING LARGE LANGUAGE MODELS WITH GRID-BASED\nGAME COMPETITIONS: AN EXTENSIBLE LLM BENCHMARK\nAND LEADERBOARD\nOguzhan Topsakal, Colby J. Edell, Jackson B. Harper\nComputer Science Department\nFlorida Polytechnic University\nLakeland, Florida, 33805\notopsakal@floridapoly.edu\nABSTRACT\nWe introduce a novel and extensible benchmark for large language models (LLMs) through grid-based\ngames such as Tic-Tac-Toe, Connect Four, and Gomoku. The open-source game simulation code,\navailable on GitHub, allows LLMs to compete and generates detailed data files in JSON, CSV, TXT,\nand PNG formats for leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by Anthropic, Gemini 1.5\nPro and Gemini 1.5 Flash by Google, GPT-4 Turbo and GPT-4o by OpenAI, and Llama3-70B by Meta.\nWe also encourage submissions of results from other LLMs. In total, we simulated 2,310 matches (5\nsessions for each pair among 7 LLMs and a random player) across three types of games, using three\ndistinct prompt types: list, illustration, and image. The results revealed significant variations in LLM\nperformance across different games and prompt types, with analysis covering win and disqualification\nrates, missed opportunity analysis, and invalid move analysis. The details of the leaderboard and\nresult matrix data are available as open-access data on GitHub. This study enhances our understanding\nof LLMs’ capabilities in playing games they were not specifically trained for, helping to assess their\nrule comprehension and strategic thinking. On the path to Artificial General Intelligence (AGI),\nthis study lays the groundwork for future exploration into their utility in complex decision-making\nscenarios, illuminating their strategic thinking abilities and offering directions for further inquiry into\nthe limits of LLMs within game-based frameworks.\nKeywords large language model · LLM · benchmark · evaluate · performance · test · leaderboard · competition ·\nchampionship · challenge · tournament · AGI · AI · deep learning · NLP · Generative AI · analysis · game · grid-based ·\ntext-based · strategic · Tic-Tac-Toe · Connect Four · Gomoku · decision-making · prompt engineering · list · illustration ·\nimage · Anthropic · Claude · Gemini · GPT4 · GPT4-o · Gemini-Pro · Gemini-Flash · Meta · LlaMA\n1\nIntroduction\nRecent advancements in large language models (LLMs) have marked significant progress in the field of Artificial\nIntelligence (AI) [1]. These developments prompt questions about the potential for achieving Artificial General\nIntelligence (AGI) [2] and the timeline for such advancements. Predictions on the timeline for AGI vary [3], [4], with\nsome experts suggesting its inevitability [5]. A critical challenge in the journey towards AGI is developing benchmarks\nto assess AI’s evolving intelligence.\nIn this study, we introduce a novel and extensible benchmark for LLMs using grid-based games such as Tic-Tac-Toe,\nConnect Four, and Gomoku, utilizing three distinct types of prompts (list, illustration, image). This benchmark helps\nassess the capabilities of LLMs, including rule comprehension, strategic thinking, and the ability to process and\nunderstand complex text and image prompts. The benchmark provides open-source code for simulating these board\ngames among LLMs and generating data files that store details of the simulated games. This study also includes the\nanalysis of a total of 2,310 games played among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\narXiv:2407.07796v2  [cs.AI]  11 Jul 2024\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and GPT-4o by OpenAI, and Llama3-70B\nby Meta. The open-source game simulation code can be utilized to test other LLMs and prepare submission data for\nthe leaderboard. The benchmark and leaderboard are designed to be extensible, accommodating new games and data\nsubmissions. The authors encourage contributions and welcome new results from other LLMs.\n2\nBackground and Related Research\nThe deep learning revolution has profoundly transformed natural language processing (NLP) since the 2010s, with\nthe introduction of the Transformer architecture in 2017 [6] playing a pivotal role in this evolution. The Transformer\narchitecture enabled parallel word processing, significantly improving the efficiency and handling of long-range\ntext dependencies. This innovation led to the creation of models like BERT (Bidirectional Encoder Representations\nfrom Transformers) [7] and OpenAI’s GPT (Generative Pre-trained Transformer) series [8]. BERT advanced context\nunder-standing by analyzing word relationships within sentences, while the GPT series excelled in generative language\ncapabilities [1].\nThe scale of LLMs expanded exponentially, resulting in models with billions of parameters and exceptional performance\nacross various NLP tasks. Recent models include GPT-4 by OpenAI [9], Gemini by Google [10], Claude by Anthropic\n[11], Grok by xAI [12], and open-source options like LLaMA by Meta [13] and Mistral by Mistral [14]. These models\nhave pushed the boundaries of what is possible with LLMs, showcasing significant advancements in the field. LLMs are\nemployed in diverse tasks such as text summarization, language translation, content generation, and question-answering\n[15].\n2.1\nLarge Language Model Benchmarks\nLLMs produce outputs such that their responses can vary even with identical input [16]. Traditional metrics like\naccuracy, precision, F1 score, and mean squared error (MSE) are not suitable for evaluating LLM performance. Instead,\nspecialized datasets and benchmarks are needed to assess LLM capabilities comprehensively [17].\nBenchmarks such as GLUE [18], SuperGLUE [19], HELM [20], MMLU [21], BIG-bench [22], ARC [23], TruthfulQA\n[24], HellaSwag [25], and LiveBench [26] provide diverse tasks that test various aspects of LLMs. GLUE, introduced\nin 2018, includes tasks like sentiment analysis and question-answering to evaluate natural language understanding.\nSuperGLUE, launched in 2019, extends GLUE with more demanding tasks such as multi-sentence reasoning and\ncomplex reading comprehension. The Massive Multitask Language Understanding (MMLU) benchmark tests LLMs\nacross a wide array of subjects, including mathematics, history, computer science, and law, requiring extensive\nworld knowledge and problem-solving abilities [21]. BIG-bench offers 204 varied tasks in areas such as linguistics,\nmathematics, reasoning, biology, and software development, allowing researchers to evaluate LLMs comprehensively\nwhile managing operational costs [22]. HELM emphasizes transparency and performance in specific tasks, using a\nmulti-metric approach that includes fairness, bias, and toxicity assessments. It continually adapts to add new scenarios,\nmetrics, and models [20]. The AI2 Reasoning Challenge (ARC) from the Allen Institute for Artificial Intelligence\nassesses AI systems’ complex reasoning capabilities through multiple-choice questions. The ARC includes an Easy Set\nfor basic retrieval methods and a Challenge Set for advanced reasoning, pushing AI towards deeper knowledge-based\nunderstanding [23]. The TruthfulQA benchmark assesses the accuracy and truthfulness of LLM responses, specifically\ndesigned to measure how well models can generate accurate answers and avoid hallucinations [24]. The HellaSwag\nbenchmark tests common sense reasoning by presenting models with sentences and multiple possible endings, requiring\nthem to choose the most logical continuation [25]. LiveBench addresses the test set contamination issue in LLM\nevaluation by offering a benchmark immune to such contamination and biases from human or LLM judging [26]. It\nfeatures frequently updated questions from recent sources, automatic scoring against objective ground-truth values, and\ndiverse tasks spanning math, coding, reasoning, language, instruction following, and data analysis [26].\nThe recent survey papers have provided comprehensive frameworks and methodologies that are invaluable for developing\nrobust benchmarks. One such paper, \"A Survey on Evaluation of Large Language Models,\" presents a detailed review\nfocusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate [27]. It encompasses a broad\nspectrum of tasks, including general natural language processing, reasoning, medical applications, ethics, education, and\nmore. This survey emphasizes the critical role of evaluation methods and benchmarks in assessing LLM performance,\nsummarizing both successes and failures across different tasks, and highlighting future challenges in the field [27].\nSimilarly, \"Evaluating Large Language Models: A Comprehensive Survey\" categorizes LLM evaluation into knowledge\nand capability, alignment, and safety [28]. This survey underscores the importance of rigorous assessment and the\ndevelopment of comprehensive evaluation platforms to ensure the safe and beneficial development of LLMs. It aims\nto guide responsible LLM advancement, ensuring that their evolution maximizes societal benefits while minimizing\n2\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\npotential risks [28]. Both surveys stress the necessity of treating evaluation as an essential discipline to aid the\ndevelopment of more proficient and ethically sound LLMs [29].\n2.2\nUtilizing Games for Evaluating LLMs\nExisting benchmarks primarily focus on language understanding tasks such as sentiment analysis, question-answering,\nand comprehension. Although some tasks in BIG-bench involve game-like problem-solving skills, they do not assess\nLLMs’ performance in conventional games like chess or Go, which are valuable for evaluating strategic thinking and\ndecision-making abilities. Using games as a benchmarking tool provides a unique perspective on LLM capabilities,\nhighlighting their proficiency in understanding rules, formulating strategies, and making decisions. Strategic games\nlike chess and Go emphasize predicting opponents’ moves, while games involving linguistic interaction test language\nmastery and contextual understanding. The dynamic nature of games allows researchers to observe LLMs’ adaptability\nand learning in real-time.\nEngaging in gameplay offers a standardized framework for comparing various LLMs’ performances under the same\nconditions, evaluating their strategic and creative problem-solving abilities, and capacity for innovative solutions. The\ncontrolled environment of games is instrumental for safely testing LLMs, allowing researchers to observe behaviors\nand mitigate potential risks or ethical concerns. Games involving human–AI interaction reveal how LLMs collaborate\nwith or compete against humans, shedding light on human–AI relationship dynamics. Therefore, testing LLMs within\nthe gaming domain extends beyond evaluating their ability to play games; it offers a comprehensive examination of\nstrategic thinking, language processing, creativity, and adaptability, which is crucial for advancing AI research and\nensuring the responsible development and deployment of these technologies.\nText-based games present a distinctive and challenging domain for benchmarking LLMs. These interactive fiction games\nrequire models to understand natural language, interpret evolving game states, and generate appropriate commands\nwithin narrative-driven environments, demanding a profound grasp of language, context, and strategic application [9].\nStudies on models like FLAN-T5, Turing, and OPT in the text-based game \"Detective\" reveal that these LLMs fall\nshort of state-of-the-art or human performance levels, facing challenges in adapting to game dynamics, learning from\npast interactions, and goal-oriented processing [30].\nThe \"GameEval-Evaluating LLMs on Conversational Games\" paper introduces a framework for assessing LLMs\nthrough goal-driven conversational games, highlighting their abilities in complex discussions, decision-making, and\nproblem-solving [31]. The SmartPlay benchmark assesses LLMs across diverse games, emphasizing their evolution\nas intelligent agents [32]. The MindAgent infrastructure evaluates multi-agent collaboration, enhancing human–AI\ncoordination [33].\nStudies on LLM behavior in social interaction games like the iterated Prisoner’s Dilemma and the Battle of the Sexes\nshow challenges in adapting to strategies requiring mutual understanding and flexibility [34]. Research by Lorè and\nHeydari on \"Strategic Behavior of Large Language Models\" underscores the role of context in strategic decision-making\n[35]. Tsai et al. highlight limitations in LLMs like ChatGPT and GPT-4 in constructing world models and leveraging\nknowledge in text-based games, suggesting the potential for targeted benchmarks [36].\nThe study \"Can Large Language Models Serve as Rational Players in Game Theory?\" evaluates LLMs’ potential in\ngame theory, identifying gaps in mimicking human rationality [37]. Another study explores models like Claude 2,\nGPT-3.5, and GPT-4 in processing game strategy and spatial information through Tic-Tac-Toe, finding that prompt\ndesign significantly impacts performance [38].\nGTBENCH evaluates LLMs’ strategic reasoning in competitive game-theoretic tasks [39]. It features 10 tasks covering\ncomplete vs. incomplete information, dynamic vs. static, and probabilistic vs. deterministic scenarios. Results show\nLLMs struggle in complete, deterministic games but perform better in probabilistic ones. Commercial LLMs like\nGPT-4 outperform open-source models such as CodeLlama-34b-Instruct. Code-pretraining aids strategic reasoning, but\nadvanced methods like Chain-of-Thought (CoT) and Tree-of-Thought do not consistently help. Detailed error profiles\nare provided to understand LLM behaviors [39].\nGAMEBENCH evaluates strategic reasoning in LLMs across nine game environments, each highlighting key reasoning\nskills [40]. Using GPT-3 and GPT-4, along with Chain-of-Thought prompting and Reasoning Via Planning (RAP),\nthe study finds that while these frameworks improve performance, no model matches human capabilities, with GPT-4\nsometimes performing worse than random actions. The games are selected to avoid overlap with the models’ pretraining\ncorpuses.\nIn a previous study, the authors evaluated the strategic thinking capabilities of various LLMs, including Claude 2.1,\nGemini-Pro 1.0, GPT-3.5-Turbo, GPT-4, Llama2-70B, and Mistral Large, by having them play Tic-Tac-Toe through a\n3\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nmobile app [41]. This study builds upon that research with additional games, more in-depth analysis, and a user-friendly\nweb-based game simulation software to evaluate more recent LLMs.\nA recent survey paper explores the state of the art in applying LLMs to gaming, identifying the various roles LLMs\ncan play within games. It highlights underexplored areas and promising directions for future research, reconciling the\npotential and limitations of LLMs in the gaming domain. This survey aims to serve as a foundation for future research\nand innovation in this emerging field [42].\nAnother recent survey paper explores LLM-based game agents and their role in advancing toward AGI. It introduces\nthe conceptual architecture centered on perception, memory, thinking, role-playing, action, and learning. The paper\nreviews methodologies and adaptation agility of existing LLM-based game agents across six game genres: adventure,\ncommunication, competition, cooperation, simulation, and crafting & exploration. It also provides future research\ndirections in this field [43].\nThese studies collectively deepen our understanding of LLMs’ strengths and weaknesses in gaming and interactive\ncontexts, providing a foundation for future research to enhance their performance and cognitive skills. They highlight\nthe value of using games as benchmarks to expose the capabilities and limitations of current AI systems, paving the\nway for developing advanced models with sophisticated reasoning and strategic thinking.\n3\nMethodology\nWe have developed a benchmark to evaluate the capabilities of LLMs in rule comprehension and decision-making\nthrough grid-based games. This benchmark includes open-source web-based software for simulating games, accessible\non GitHub [44]. The web application is built using JavaScript, HTML, and CSS, with server-side AWS Lambda\nfunctions written in Python to leverage LLMs hosted on AWS Bedrock. The game simulation web app enables LLMs\nto compete against each other, recording the details of each move for further analysis in JSON, CSV, TXT, and PNG\nformats, as well as summarizing game results.\nCurrently, the benchmark includes Tic-Tac-Toe, Connect Four, and Gomoku, and is designed to be extensible to\naccommodate additional board games. A step-by-step guide for adding new game simulations is provided. As illustrated\nin Figure 1, the user interface of the game simulation web app allows users to select a game and the LLMs for the first\nand second players from a curated list. Users can also choose the type of predefined prompts (e.g., list, illustration,\nimage) and specify the number of consecutive games for the selected game, prompt, and player combination.\nFigure 1: Web-based app for game simulation shows the progress of a Connect Four game.\nThe game simulation initiates by sending the selected prompt to the web API of the chosen LLM for the first player,\nthen awaits its move. Upon receiving a response, the application updates the user interface to reflect the game’s progress,\nas demonstrated in Figure 1, subsequently queries the chosen LLM for the second player, and awaits its move. The\n4\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nprompts, which include the current state of the game, are continuously sent to each LLM’s web service until a player\nwins, the game ends in a draw, or a player is disqualified for making invalid moves. Each query and response are\nrecorded for every move. This methodology ensures seamless interaction between the application and the LLMs via\nweb API calls. The interactions are illustrated in Figure 2.\nFigure 2: The illustration of web-based app and web service interactions to play a game.\n3.1\nGames Available on the Benchmark and Possibility of Expansion to New Games\nWe have utilized three games in the benchmark; Tic-Tac-Toe, Connect Four and Gomoku. All of these games are\nclassical two-player games played on a grid; 3 by 3, 6 by 7, and 15 by 15, respectively [45] [46] [47]. These games can\nbe adapted to larger grids. The explanations of these games are given in Table 1 and the same explanations are used in\nthe prompts.\nTic-Tac-Toe, Connect Four, and Gomoku are all solved games meaning their outcome (win, lose, or draw) can be\ncorrectly predicted from any position, assuming that both players play perfectly. In Tic-Tac-Toe, optimal play from\nboth participants guarantees a draw. The first player can always win with optimal play in Connect Four. In the Gomoku\ngame, the first player is guaranteed to win with optimal play [48].\nWe designed this benchmark to be extensible, allowing for the addition of new games such as checkers and chess. The\ncode is modular, facilitating the easy integration of additional games. Additionally, we prepared a step-by-step guide on\nhow to add a new game to the benchmark, which can be found on the game simulation page under the ‘How to Add\nYour Own Game’ link. We encourage interested individuals to contribute to the development of the benchmark\n3.2\nLLMs Tested & New Result Submission to the Leaderboard\nNumerous LLMs are available for evaluation. To ensure a meaningful and comprehensive assessment, we carefully\nselected LLMs based on several criteria. Firstly, we chose LLMs that are not specifically trained for the games used in\nthe benchmark. Although the training data of proprietary LLMs is not publicly disclosed, we assume they are not trained\nexplicitly for any of the benchmark games. We prioritized well-known, high-performing LLMs developed by industry\nleaders such as OpenAI and Google, given their significant contributions to AI advancements. Additionally, we included\nLLMs from emerging startup companies that have gained attention in the AI community, such as Anthropic. To further\nenrich our evaluation, we aimed to test open-source models and included Meta’s Llama3-70B model in our evaluation.\nThis selection covers a broad spectrum of innovative approaches, technological capabilities, and accessibility options.\nThe landscape of LLMs changes rapidly, with new models frequently emerging with improved capabilities. Therefore,\nwe provide game simulation software in the benchmark that generates submission files and encourage new submissions\nto the leaderboard. Contributors can evaluate other LLMs by integrating their LLM web service URL or API keys,\n5\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\ngenerating new results, and submitting them to the leaderboard. We believe the leaderboard will allow people to see the\nprogress of LLMs in different games as the leaderboard continues to be updated.\nCurrently, the benchmark includes results and detailed files for the following LLMs: Claude 3.5 Sonnet and Claude 3\nSonnet from Anthropic, Gemini 1.5 Flash and Gemini 1.5 Pro from Google, GPT-4 Turbo and GPT-4o from OpenAI,\nand Llama3-70B from Meta. To access these models, we utilized the web APIs provided by Google and OpenAI for the\nGemini and GPT-4 models. For accessing models from Meta and Anthropic, we employed Amazon Bedrock services,\nleveraging serverless AWS Lambda functions and API Gateways, as depicted in Figure 2.\nTo evaluate the decision-making capabilities of LLMs compared to random play, we included an option to select\n‘random play‘ as the opponent. This option generates random responses for each move. By testing all the LLMs against\nrandom play, we aim to determine the extent to which LLMs outperform random decision-making in game scenarios.\n3.3\nDetails of the Prompts\nWe utilized three types of prompts: list, illustration, and image. Each prompt is divided into eight main components:\n1) an explanation of the game, 2) an explanation of the format for the game status, 3) the current game status, 4) a\ndefinition of the LLM’s role followed by a request for its next move, 5) an explanation of the response format, 6) an\nexplanation of invalid moves, 7) a warning if the previous move was invalid, including an explanation of why it was\ndeemed invalid, and 8) the current number of invalid moves made by the player, as well as the number of invalid moves\nuntil the player is dis-qualified. The current game status, the invalid move warning, and the invalid move counts are\ndynamically generated and updated as the game progresses. Table 2 presents the components of a ’list’ type prompt for\nthe Tic-Tac-Toe game. This standardized format ensures consistency in prompts throughout the game while allowing\nfor dynamic updates of the game state.\nTable 1: Explanation of the games used in the prompts.\nTic-Tac-Toe\nTic-Tac-Toe is a two-player game played on a 3 by 3 grid. The first player uses X symbols, and the\nsecond player uses O symbols. Players take turns placing their symbols in an empty cell on the\ngrid. The objective is to align three of your symbols either horizontally, vertically, or diagonally.\nThe player who first aligns three of their symbols wins the game. Strategic placement is crucial;\nbesides aiming to align their symbols, players must also block their opponent's potential alignments\nto avoid defeat.\nConnect Four\nConnect Four is a two-player game played on a 6 by 7 grid. The first player uses red (R) discs,\nand the second player uses yellow (Y) discs. Players take turns dropping their discs into a column\nfrom the top row where there is still at least one empty space. The dropped disc falls straight down,\noccupying the lowest available row within the column. The objective is to align four of your discs\neither horizontally, vertically, or diagonally. The player who first aligns four of their discs wins the\ngame. Strategic placement is crucial; besides aiming to align their discs, players must also block\ntheir opponent's potential alignments to avoid defeat.\nGomoku\nGomoku is a two-player game played on a 15 by 15 grid. The first player uses black (B) dots, and\nthe second player uses white (W) dots. Players take turns placing their dots on an empty intersection\nof the grid. The objective is to align five of your dots either horizontally, vertically, or diagonally.\nThe player who first aligns five of their dots wins the game. Strategic placement is crucial; besides\naiming to align their dots, players must also block their opponent's potential alignments to avoid\ndefeat.\nThe content of the three types of prompts is consistent, except for the representation of the current state of the game\n(previous moves). The ‘list’ prompt enumerates previous moves for each player in a “row, column” format. The\n‘illustration’ prompt depicts the current state of the grid using specific symbols for the first and second players (X and O\nfor Tic-Tac-Toe, R and Y for Connect Four, and B and W for Gomoku) and ’e’ for empty cells. The ‘image’ prompt\nvisualizes the current state by providing a snapshot of the game board. These differences are detailed in Table 3.\nLLMs use parameters like max tokens, temperature, top-p, and frequency penalty to fine-tune their outputs [49] [50].\nMax tokens control length, temperature adjusts creativity, top-p limits word choices to balance creativity and coherence,\nand frequency penalty reduces repetition. These settings customize LLM responses for applications such as customer\nsupport and content creation. We use default configurations for all parameters except the prompt, trusting the creators’\nfine-tuning for optimal performance.\nThe games continued until one player won, a draw occurred, or a disqualification was necessary. To gather statistical\ndata on the outcomes, each game between opponents was repeated five times. Disqualification occurred if a player\n6\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nTable 2: The parts of a prompt for the TicTacToe game. This table shows sample parts for the ‘list’ type of prompt. The\ndifferences in the ‘illustration’ and ‘image’ prompts are given in Table 3.\nPart\nPrompt Content\nThe explanation of the game.\nSame as the corresponding game explanation given in Table 1\nThe explanation of the format for\nthe status of the game. The same\nfor every game for the selected\nprompt type. The sample on the\nright is for the ‘list’ type of prompt.\nThe current state of the game is recorded in a specific format: each occupied\nlocation is delineated by a semicolon (';'), and for each occupied location, the row\nnumber is listed first, followed by the column number, separated by a comma\n(','). If no locations are occupied by a player, 'None' is noted. Both the row and\ncolumn numbers start from 1, with the top left corner of the grid indicated by\n1,1.\nThe current game status. Dynami-\ncally generated. The sample on the\nright shows the current state for the\n‘list’ type of prompt.\nThe current state of the game is as follows:\nThe locations occupied by the first player: 1,1; 1,2; 3,2.\nThe locations occupied by the second player: 2,2; 3,3.\nDefining the role of the LLM and\nthen asking its next move. The\nsame for every game.\nYou are an adept strategic player, aiming to win the game in the fewest moves\npossible. You are the first (second) player. What would be your next move?\nThe explanation of the response\nformat.\nSuggest your next move in the following JSON format: {'row': RowNumber,\n'column': ColumnNumber}. Do not include any additional commentary in\nyour response. Replace RowNumber and ColumnNumber with the appropriate\nnumbers for your move. Both RowNumber and ColumnNumber start at 1 (top\nleft corner is {'row': 1, 'column': 1}). The maximum value for RowNumber and\nColumnNumber is 3, as the grid is 3 by 3.\nThe explanation of the invalid\nmoves.\nPlease note that your move will be considered invalid if your response does not\nfollow the specified format, or if you provide a RowNumber or ColumnNumber\nthat is out of the allowed range, or already occupied by a previous move. Making\nmore than 3 invalid moves will result in disqualification.\nThe warning if the last move was\ninvalid, including a copy of the pre-\nvious move and an explanation of\nwhy the move was invalid. Dynam-\nically generated.\nYour previous response was '{\"row\": X, “column\": Y}'. This move was deemed\ninvalid for the following reason: 'Already Taken'. Please adjust accordingly.\nThe current number of invalid\nmoves, as well as the number of\ninvalid moves left until disqualifi-\ncation. Dynamically generated.\nYou currently have X invalid move(s). Y more invalid moves will result in\ndisqualification.\nmade more than a specified number of invalid moves: three for Tic-Tac-Toe, six for Connect Four, and fifteen for\nGomoku. A move was deemed invalid if the response did not follow the specified format, the provided RowNumber or\nColumnNumber was out of range, or a move was made to an already occupied space. When a player made an invalid\nmove, they were warned about the invalidity, provided with the reason (as shown in Table 2), and asked to make their\nmove again. Continuous invalid moves led to disqualification to ensure fairness and prevent indefinite delays. During\nthe game sessions conducted through the web application, data on gameplay was collected and stored in JSON, CSV,\nTXT, and PNG formats. Samples of these files are available on GitHub, along with zip files containing the complete\ndata from bulk runs performed for the results presented here. The JSON files include comprehensive details such as\ndate\/time, players, game result, duration, and all moves, covering both valid and invalid attempts. They also include the\ncurrent game status sent to the LLM and the responses received from the LLM for each move. Additionally, the JSON\nformat is used for the leaderboard submissions. A streamlined summary of the game is available in a CSV file. The\nTXT file provides an illustrated representation of the game moves, while the PNG files display snapshots of the board\nafter each move. All files generated during this study are publicly available on GitHub.\n3.4\nDetails of the Data Generated by the Game Simulation Web App\nThe data generated by the game simulation web app is downloaded as a zip file either after a ’run’ between two LLMs\nor after a ’bulk run’ between all LLMs listed as first and second players. The zip file generated after a ’bulk run’\nincludes all the files that would be produced for each LLM pair match, as well as single files with the ’_all’ suffix\nthat encompass the content of all corresponding files from each pair’s match. Each generated file’s name is prefixed\n7\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nTable 3: The differences between the list, illustration and image type of prompts for the TicTacToe game. The prompt\ncontent slightly changes for Connect Four and Gomoku games. Please refer to the GitHub page for sample prompts for\nConnect Four and Gomoku.\nType\nPart\nPrompt Content\nlist\nThe explanation of\nthe format for the\nstatus of the game.\nThe same for every\ngame.\nThe current state of the game is recorded in a specific format: each oc-\ncupied location is delineated by a semicolon (';'), and for each occupied\nlocation, the row number is listed first, followed by the column number,\nseparated by a comma (','). If no locations are occupied by a player,\n'None' is noted. Both the row and column numbers start from 1, with\nthe top left corner of the grid indicated by 1,1.\nThe current game\nstatus.\nDynami-\ncally generated.\nThe current state of the game is as follows:\nThe locations occupied by the first player: 1,1; 1,2; 3,2.\nThe locations occupied by the second player: 2,2; 3,3.\nillustration\nThe explanation of\nthe format for the\nstatus of the game.\nThe same for every\ngame.\nThe current state of the game is illustrated on a 3 by 3 grid. 'X' represents\npositions taken by the first player and 'O' represents positions taken by\nthe second player, while 'e' indicates an empty (available) position.\nThe current game\nstatus.\nDynami-\ncally generated.\nThe current state of the game is as follows:\neXe\neeO\neOe\nimage\nThe explanation of\nthe format for the\nstatus of the game.\nThe same for every\ngame.\nThe current state of the game is depicted in an image showing a 3 by\n3 grid, where 'X' represents positions taken by the first player and 'O'\nrepresents positions taken by the second player.\nThe current game\nstatus.\nDynami-\ncally generated.\nThe current state of the game is given in the attached image.\n[Image is sent in base64 format]\nusing the following format: ’Game-Type_PromptType_FirstPlayerLLM_SecondPlayerLLM_Result_DateTime’. For\nexample, ’tic-tac-toe_list_gemini-1.5-pro_gemini-1.5-flash_winner1st_240707-164940’. The data is stored in JSON,\nCSV, TXT, and PNG formats. The CSV file includes the same data as the corresponding JSON file. The TXT file\nincludes concise statistics of a game and an illustration of the game’s progress in text format. If the ’Save Progress\nImages in ZIP File’ box is checked on the game simulation page, PNG files showing snapshots of each move during the\ngame will be generated as well. The JSON file with the ’_submission’ suffix can be sent to the first author to add the\nresults of the matches to the leaderboard page. Table 4 lists the data included in the JSON file that provides the details\nof a game and the JSON file that can be used to submit the results. The sample files and zip files generated during the\n’bulk run’ of data collection for the results presented in this study are available on the GitHub page [44].\n3.5\nMetrics and Methods for Evaluation\nWe evaluated the performance of LLMs across three games (Tic-Tac-Toe, Connect Four, and Gomoku) using different\nprompt types (list, illustration, and image) to assess their ability to handle various formats of game state representation.\nPerformance comparisons were made against a random play strategy to establish a baseline, highlighting the strategic\nadvantages of the LLMs. The primary metrics for evaluation included win rates, draw rates, and disqualification\nrates, providing an overview of the LLMs’ performance as both the first and second players. Additionally, we tracked\nthe number of invalid moves per game and the average number of moves per game to assess rule adherence and\ngame engagement. To delve deeper into the LLMs’ strategic thinking, we analyzed missed opportunities to win\nor block an opponent’s win, counting instances where the LLMs failed to make critical moves. We presented the\nmissed opportunities per game by averaging the missed opportunities across all games that resulted in a win, draw, or\ndisqualification. We also normalized the number of missed opportunities by the number of valid moves to calculate the\npercentage of missed opportunities per valid move. The results were visualized through charts and tables to provide\na clear depiction of performance metrics and trends, as shown in the Results section. Additionally, we present the\noutcomes of each match between seven LLMs and a random play generator across different games (a total of 2,310\nmatches) in a results matrix table. We also maintain a leaderboard on the GitHub page that allows for filtering and\n8\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nTable 4: The data included in the JSON files.\nJSON File\nContent\nThe main JSON\nfile\nthat\nin-\ncludes\ndetailed\noutcomes\nof\nthe game.\nFor\nexample,\nthe\nname of the file\ncould be “tic-tac-\ntoe_list_gemini-\n1.5-\npro_gemini-1.5-\nflash_winner1st_240707-\n164940.json”\n- UUID: Unique identifier for the game instance.\n- DateTime: Timestamp indicating when the game was played.\n- GameType: Type of game (tic-tac-toe, connect four, or gomoku).\n- PromptType: Type of prompt used (list, illustration, or image).\n- PromptVersion: Version of the prompt (date that the prompt was last modified).\n- GameNumber: Sequential identifier for the game.\n- Player1: LLM model name for the first player.\n- Player2: LLM model name for the second player.\n- Result: Outcome of the game (winner1st, winner2nd, draw, disqualified1st, disqualified2nd).\n- GameDuration: Duration of the game in seconds.\n- TotalMoves: Total number of moves made during the game.\n- Player1Moves: Number of moves by the first player.\n- Player2Moves: Number of moves by the second player.\n- Player1InvalidAlreadyTaken: Number of moves where the first player attempted to place a\nmove in an already occupied location.\n- Player2InvalidAlreadyTaken: Number of moves where the second player attempted to place a\nmove in an already occupied location.\n- Player1InvalidFormat: Number of moves in invalid format by the first player.\n- Player2InvalidFormat: Number of moves in invalid format by the second player.\n- Player1OutOfBounds: Number of moves made outside the board boundaries by the first player.\n- Player2OutOfBounds: Number of moves made outside the boundaries by the second player.\n- FinalGameState: The final status of the board presented in the chosen prompt type.\n- Moves: Array of move objects detailing each move made during the game. Each move object\nincludes:\n- - MoveNumber: Sequence number of the move.\n- - Player: Indicates whether the move was made by Player 1 or Player 2.\n- - Row: Row coordinate of the move on the grid-based board.\n- - Column: Column coordinate of the move on the grid-based board.\n- - Outcome: Result of the move (e.g., \"Valid\", \"Already Taken\").\n- - CurrentStatus: The current status of the board sent to the LLM in the prompt format\n(list, illustration, or image). If the prompt type is image, it includes a base64-encoded string\nrepresenting the game board's state after the move.\n- - Response: The response provided by the player, specifying the move.\nSubmission (the\nfile type has the\nsuffix\n‘_submis-\nsion.json’)\n- ProviderEmail: Email of the provider submitting the results. This information can be entered\nin the 'Manage LLMs' settings on the game simulation page.\n- UUID: Unique identifier for the game instance.\n- DateTime: Timestamp indicating when the game was played.\n- GameType: Type of game (tic-tac-toe, connect four, or gomoku).\n- PromptType: Type of prompt used (list, illustration, or image).\n- PromptVersion: Version of the prompt (date that the prompt was last modified).\n- LLM1stPlayer: The LLM model name for the first player.\n- LLM2ndPlayer: The LLM model name for the second player.\n- WinRatio-1st: Win ratio of the first player.\n- WinRatio-2nd: Win ratio of the second player.\n- Wins-1st: Number of wins by the first player.\n- Wins-2nd: Number of wins by the second player.\n- Disqualifications-1st: Number of disqualifications for the first player.\n- Disqualifications-2nd: Number of disqualifications for the second player.\n- Draws: Number of games that ended in a draw.\n- InvalidMovesRatio-1st: Ratio of invalid moves made by the first player.\n- InvalidMovesRatio-2nd: Ratio of invalid moves made by the second player.\n- TotalMoves-1st: Total number of moves made by the first player.\n- TotalMoves-2nd: Total number of moves made by the second player.\nsorting results by different metrics. We encourage community contributions to suggest and implement new evaluation\nmetrics and methodologies, fostering a collaborative approach to advancing the understanding of LLM capabilities.\n9\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\n4\nResults\nIn this section, we present the outcomes of games played among LLMs. These results are based on data files generated\nby the open-source game simulation web software and shared on the GitHub page.\nFigure 3 displays the outcomes of Tic-Tac-Toe games using the list prompt type, where seven LLMs competed against\nothers and a random play opponent, engaging in five matches per opponent for a total of 280 games. The chart\nsummarizes the performance of the seven LLMs as well as random play in terms of win rates, draw rates, and dis-\nqualification rates as both the first and second players. Claude 3.5 Sonnet has the highest winning percentage as the first\nplayer (88.57%) but a lower winning percentage as the second player (17.14%). GPT-4o and Gemini 1.5 Pro show\nstrong performance as both the first and second players, while random play results in the highest disqualification rates\nFigure 3: Tic-Tac-Toe game outcomes using the ‘list’ prompt where each LLM faced six others and the ‘random play’\nas both player 1 and player 2, playing each opponent 5 times (280 games total).\nFigure 4 displays the performance metrics of seven LLMs and a random play strategy in terms of win rates, draw rates,\nand disqualification rates when playing Tic-Tac-Toe as the first and second player, based on the illustration prompt\nformat. The chart shows significant performance variations among the LLMs, with Claude 3.5 Sonnet exhibiting the\nhighest winning rates as the first player, indicating a strong strategic advantage. Llama3-70B and GPT-4 Turbo also\ndemonstrate strong performance. Disqualification rates are generally low but notable for some models, such as Gemini\n1.5 Flash, indicating occasional invalid moves. The random player serves as a baseline comparison, with lower winning\nrates and higher disqualification rates, highlighting the superior strategic capabilities of the LLMs.\nFigure 5 displays the performance metrics of various LLMs and a random play strategy in terms of win rates, draw\nrates, and disqualification rates when playing Tic-Tac-Toe as the first and second player, based on the image prompt\nformat. Key observations indicate that Claude 3.5 Sonnet has a high disqualification rate as both the first (46.67%) and\nsecond player (53.33%), indicating a struggle with rule compliance. Similarly, GPT-4 Turbo and Gemini 1.5 Pro also\nshow significant disqualification rates. GPT-4 Turbo and GPT-4o exhibit the highest winning rates. The random play\nbaseline has high disqualification rates, highlighting the strategic advantages of LLMs compared to random strategies.\nNo draws occurred in the Tic-Tac-Toe games using the image prompt. Llama3-70B does not accept images, so it was\nnot used when testing any of the games with the image prompt type.\nThe chart in Figure 6 displays the performance metrics of various LLMs and a random play strategy in terms of win\nrates, draw rates, and disqualification rates when playing Connect Four as the first and second player using the list\nprompt type. Claude 3.5 Sonnet and Gemini 1.5 Pro show outstanding performance with a high winning rate of 88.57%\nas the first player. Most LLMs demonstrated strong performance when considering their total win rates as both first\nand second players. The random player, serving as a baseline, has lower winning rates and some disqualifications,\nhighlighting the strategic advantages of the LLMs. No draws occurred in the Connect Four games using the list prompt.\nFigure 7 presents the performance metrics of various LLMs and a random play strategy in terms of win rates and\ndisqualification rates when playing Connect Four as the first and second player using the illustration prompt type.\nGPT-4 Turbo has the highest disqualification rates as both the first and second players. The random play, serving as\n10\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 4: Tic-Tac-Toe game outcomes using the ‘illustration’ prompt where each LLM faced six others and the ‘random\nplay’ as both player 1 and player 2, playing each opponent 5 times (280 games total).\nFigure 5: Tic-Tac-Toe game outcomes using the ‘image’ prompt where each LLM faced five others and the ‘random\nplay’ as both player 1 and player 2, playing each opponent 5 times (210 games total).\na baseline, has the second lowest win rate as the first player and the lowest win rate as the second player. No draws\noccurred in the Connect Four games using the illustration prompt.\nThe chart in Figure 8 illustrates the performance metrics of various LLMs and a random play strategy in terms of win\nrates and disqualification rates when playing Connect Four as the first and second player using the image prompt format.\nGPT-4 Turbo and Claude 3.5 Sonnet demonstrate strong winning performance as both the first and second players.\nClaude 3 Sonnet and Gemini 1.5 Flash have high disqualification rates overall. The random play baseline has the lowest\nwinning rates. No draws occurred during the matches between the opponents.\nFigure 9 displays the performance metrics of various LLMs and a random play strategy in terms of win rates and\ndisqualification rates when playing Gomoku as the first and second player using the list prompt. Claude 3.5 Sonnet\ndemonstrates exceptional performance with a 94.29% win rate as the first player and 25.71% win rate as the second\nplayer, with no disqualifications. Claude 3 Sonnet also performs well with an 85.71% win rate as the first player and\n25.71% win rate as the second player, maintaining a clean record. Gemini 1.5 Pro has a high win rate of 71.43% as the\nfirst player and 45.71% as the second player but exhibits an 11.43% disqualification rate as both the first and second\nplayer. GPT-4 Turbo stands out with a 74.29% win rate as the first player and 37.14% win rate as the second player,\nshowing minimal disqualifications. GPT-4o performs well with a 57.14% win rate as the first player and 37.14% win\n11\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 6: Connect Four game outcomes using the ‘list’ prompt where each LLM faced sic others and the ‘random play’\nas both player 1 and player 2, playing each opponent 5 times (280 games total).\nFigure 7: Connect Four game outcomes using the ‘illustration’ prompt where each LLM faced six others and the\n‘random play’ as both player 1 and player 2, playing each opponent 5 times (280 games total).\nrate as the second player, with some disqualifications as the first player. Llama3-70B exhibits a win rate of 65.71% as\nthe first player and 22.86% as the second player, with no disqualifications. The random player, serving as a baseline,\nhas no wins, draws, or disqualifications recorded.\nThe chart in Figure 10 displays the performance metrics of various LLMs and a random play strategy in terms of win\nrates and disqualification rates when playing Gomoku as the first and second player. Significant disqualification rates\nare evident among several LLMs, particularly when playing as the second player, indicating challenges in adhering\nto the game’s rules. Models like Gemini 1.5 Flash and Llama3-70B had high disqualification rates. Winning rates\nvary widely, with some models showing strong performance as the first player while struggling as the second player.\nThe notable disqualification rates suggest that strategic complexity and rule comprehension are significant factors\naffecting LLM performance. Overall, the chart highlights the variability in strategic abilities and reliability of different\nLLMs, emphasizing the need for further improvements in their rule adherence and strategic planning capabilities. The\nrandom play baseline, with no recorded wins, draws, or disqualifications, underscores the superior strategic thinking\nand performance of the LLMs despite their challenges.\nFigure 11 illustrates the performance metrics of various LLMs and a random play strategy in terms of win rates and\ndisqualification rates when playing Gomoku as the first and second player using the image prompt type. A notable\n12\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 8: Connect Four game outcomes using the ‘image’ prompt where each LLM faced five others and the ‘random\nplay’ as both player 1 and player 2, playing each opponent 5 times (210 games total).\nFigure 9: Gomoku game outcomes using the ‘list’ prompt where each LLM faced six others and the ‘random play’ as\nboth player 1 and play-er 2, playing each opponent 5 times (280 games total).\npattern is the high disqualification rates for some models, particularly when playing as the second player, suggesting\ndifficulties in rule adherence. There is a marked variation in win rates among the models, with some achieving higher\nsuccess as the first player. The random player baseline demonstrates no recorded wins, draws, or disqualifications.\nThe chart in Figure 12 illustrates the performance of LLMs and a random play strategy in terms of moves per game and\ninvalid moves per game when they participated as both first and second players in Tic-Tac-Toe across three prompt\ntypes: list, illustration, and image. The random play strategy serves as a baseline, indicating performance without\nstrategic thinking. Generally, the number of moves per game increases with the complexity of the prompt, with image\nprompts resulting in the highest number of moves across all models. For list prompts, LLM performance ranged from\n6.46 to 7.43 moves per game, while random play showed a higher number at 10.11. Illustration prompts saw a slight\nincrease in moves for most models, peaking with Gemini 1.5 Flash. Invalid moves were minimal for list prompts but\nincreased significantly for illustration prompts, particularly for Gemini 1.5 Flash, and were highest for image prompts,\nnotably for Claude 3 Sonnet and Gemini 1.5 Pro. Random play consistently exhibited higher invalid moves across all\nprompt types, underscoring its lack of strategic planning compared to the LLMs. Llama3-70B was not used for the\nimage prompt since it cannot accept images.\nThe chart in Figure 13 compares the performance of LLMs and a random play strategy in terms of moves per game\nand invalid moves per game, when they participated as both first and second players, for Connect Four across three\n13\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 10: Gomoku game outcomes using the ‘illustration’ prompt where each LLM faced six others and the ‘random\nplay’ as both player 1 and player 2, playing each opponent 5 times (280 games total).\nFigure 11: Gomoku game outcomes using the ‘image’ prompt where each LLM faced five others and the ‘random play’\nas both player 1 and player 2, playing each opponent 5 times (210 games total).\nprompt types: list, illustration, and image. Overall, the number of moves per game tends to increase with the complexity\nof the prompt, with the highest number of moves observed in the image prompt format. For list prompts, the LLMs\ndemonstrated consistent moves per game with minimal invalid moves. However, there was a significant increase in\ninvalid moves in the illustration prompts for GPT-4 Turbo and in the image prompts for Claude 3 Sonnet. Notably,\nrandom play showed relatively lower invalid moves, likely because invalid moves (already taken slots) can only occur\nwhen all rows of a column are filled in Connect Four. These results highlight the challenges faced by LLMs in handling\nmore complex and visually demanding prompt formats.\nFigure 14 illustrates the performance of LLMs and a random play strategy in terms of moves per game and invalid\nmoves per game for Gomoku, across list, illustration, and image prompt types. Generally, the number of moves per\ngame and the number of invalid moves per game increases for LLMs with the complexity of the prompt, with the highest\nmoves recorded in the image prompt format. Invalid moves are minimal in list prompts but increase significantly in\nillustration and image prompts, especially for models like Gemini 1.5 Flash, GPT-4 Turbo, and Llama3-70B. Random\nplay shows relatively fewer invalid moves, as it is less likely to place a move on an already occupied space before the\ngame has progressed significantly in the 15 by 15 grid of Gomoku. This chart highlights the challenges LLMs face in\nhandling more complex and visually demanding prompt formats.\n14\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 12: Moves per game and invalid moves (already taken) per game for Tic-Tac-Toe.\nFigure 13: Moves per game and invalid moves (already taken) per game for Connect Four.\nWe analyzed the strategic decision-making capabilities of LLMs by counting instances where they missed opportunities\nto win or block an opponent’s win with one move. For example, in Tic-Tac-Toe, if the first player had two of its symbols\nin a row along with an empty space and did not place its next move in that space to win the game, it was counted as a\nmissed opportunity to win. Similarly, if the second player did not place its next move in the empty space to block the\nfirst player from winning after the first player had two symbols in a row, it was recorded as a missed opportunity to\nblock. Our analysis covered 70 games per LLM for the list and illustration prompt types, and 60 games per LLM for\nthe image prompt type.\nFigure 15 presents the frequency of missed opportunities to win or avoid a loss per Tic-Tac-Toe game. LLMs generally\nperformed better by missing fewer opportunities in list prompts compared to illustration and image prompts. Claude 3.5\nSonnet showed the fewest missed opportunities to win in the list and illustration prompts, while GPT-4 Turbo showed\nthe fewest in the image prompt. The frequency of missed opportunities to block an opponent’s win was generally higher\nacross all prompt types, with Gemini 1.5 Flash facing notable challenges in the illustration prompts.\nIn general, missing fewer win and block opportunities per game indicates better performance for an LLM. However,\nif an LLM makes many invalid moves and gets disqualified without creating any opportunities to win, the number of\nmissed opportunities to win will be zero, falsely suggesting no missed opportunities. To avoid such confusion in the\ninterpretation of results and to further analyze performance, we normalized the missed opportunities by using the number\nof valid moves and calculated the percentage of opportunities missed per valid move. The chart in Figure 16 shows\nthe percentage of missed win and block opportunities per valid move for various LLMs in Tic-Tac-Toe across three\nprompt types: list, illustration, and image. The blue bars rep-resent the percentage of missed win opportunities, while\nthe orange bars represent the percentage of missed block opportunities. Generally, LLMs missed fewer opportunities in\n15\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 14: Moves per game and invalid moves (already taken) per game for Gomoku.\nFigure 15: Strategic move opportunities missed per Tic-Tac-Toe game.\nthe list prompts compared to illustration and image prompts. For instance, Claude 3.5 Sonnet had a 9% missed win\nopportunity rate and 20% missed block opportunity rate for list prompts, while it missed 21% and 28%, respectively,\nfor illustration prompts. Similarly, GPT-4 Turbo had a notable increase in missed block opportunities for illustration\nprompts. The trend is consistent across other models, indicating that LLMs face greater challenges in handling visually\ncomplex prompts, leading to higher rates of missed strategic opportunities. The chart highlights that while LLMs\ncan identify winning moves, they often struggle more with blocking opponents’ winning moves, especially as prompt\ncomplexity increases.\nFigure 17 shows the performance of various LLMs in terms of missed opportunities to either win or block the opponent\nfrom winning in Connect Four. In Connect Four, if a player had three of its discs in a row (horizontally, vertically, or\ndiagonally) along with an empty slot and did not place its next move in that slot to complete four in a row and win the\ngame, it was counted as a missed opportunity to win. Similarly, if the opponent did not place its next move in the empty\nslot to block the first player from winning once the first player had achieved three discs in a row, it was recorded as a\nmissed opportunity to block. According to the chart, LLMs tend to miss more opportunities to block than to win. For\nexample, in the list prompt format, Claude 3 Sonnet has a high rate of missed opportunities to block at 1.28 per game,\nwhile it misses 0.88 opportunities to win. Similarly, Gemini 1.5 Flash misses 0.95 opportunities to block and 0.22 to\nwin. This trend is consistent across other models and prompt types, indicating a common difficulty among LLMs in\nanticipating the opponent’s winning moves. In the image prompt format, Claude 3.5 Sonnet misses 0.68 opportunities\n16\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 16: Percentage of strategic move opportunities missed per Tic-Tac-Toe valid move.\nto block and 0.97 to win, indicating challenges in both defensive and offensive strategies. In the illustration prompt,\nGPT-4o and Claude 3 Sonnet miss the most opportunities compared to other LLMs.\nFigure 17: Strategic move opportunities missed per Connect Four game.\nGPT-4 Turbo appears to miss fewer opportunities than other LLMs in the illustration prompt format. However, a careful\nanalysis of the previous chart in Figure 7 reveals that GPT-4 Turbo had a disqualification rate of 82.66% as the first\nplayer and 68.57% as the second player out of a total of 70 games using the illustration prompt in Connect Four. These\nrates are significantly higher than those of other LLMs. Further review of Figure 13 shows that GPT-4 Turbo averaged\n13.49 invalid moves per game out of 23.43 moves, which is again significantly more than other LLMs. While GPT-4\nTurbo seems to have missed fewer opportunities, this may result from not having many valid moves. To gain better\ninsight into the LLMs’ strategic capabilities in utilizing opportunities, we focused on valid moves and analyzed the\npercentage of missed win and block opportunities per valid move. Figure 18 shows the results of this analysis. For list\nprompts, Claude 3 Sonnet missed the most block opportunities. In illustration prompts, Llama3-70B and GPT-4 Turbo\nperformed better. In image prompts, missed block opportunities were notably higher for Claude 3.5 Sonnet and Gemini\n1.5 Pro.\n17\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 18: Percentage of strategic move opportunities missed per Connect Four valid move.\nAccording to the chart in Figure 19, LLMs generally miss more opportunities to win than to block in the Gomoku\ngame. In Gomoku, if a player has four stones in a row (horizontally, vertically, or diagonally) and an empty space\nbut does not place its next move in that space to complete five in a row, it is counted as a missed opportunity to win.\nSimilarly, if the opponent does not place its next move in the empty space to block the first player from winning once\nthe first player has four stones in a row, it is recorded as a missed opportunity to block. In the list prompt format, both\nClaude 3 Sonnet and Gemini 1.5 Flash miss approximately 1.87 opportunities to win per game, while their missed\nopportunities to block are slightly lower. In the illustration prompt format, Gemini 1.5 Flash misses 1.7 opportunities to\nwin and 0.82 to block. This trend indicates that while LLMs can often identify opportunities to block, they struggle\nmore with recognizing and capitalizing on winning opportunities. In the image prompt format, GPT-4o exhibits the\nhighest number of missed opportunities, with 1.34 missed chances to win and 1.32 to block per game. This highlights\nsignificant challenges for LLMs in processing and acting upon image-based inputs. Overall, the chart emphasizes that\nwhile LLMs are reasonably adept at blocking the opponent’s winning moves, they face greater difficulty in identifying\nand seizing their own winning opportunities, especially in more complex and visually demanding formats.\nFigure 19: Strategic move opportunities missed per Gomoku game.\n18\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nIn Figure 20, we further analyzed the missed opportunities by normalizing them per valid move, as we did in Figure\n16 for Tic-Tac-Toe and Figure 18 for Connect Four. Figure 20 displays the percentage of missed win and block\nopportunities per valid move for various LLMs in Gomoku across three prompt types: list, illustration, and image.\nGenerally, the LLMs show a higher percentage of missed block opportunities compared to win opportunities across\nall prompt types. In list prompts, Gemini 1.5 Flash missed the most block opportunities. Gemini 1.5 Pro performed\nthe best with no missed opportunities for illustration prompts. In image prompts, GPT-4 Turbo performed the best in\nminimizing missed opportunities.\nFigure 20: Percentage of strategic move opportunities missed per Gomoku valid move.\nUtilizing the Game Simulation web app to generate data for new LLMs or the open-access data available on GitHub for\nthe LLMs assessed here, further analysis can be conducted. For instance, analyzing the creation of winning opportunities\ncan provide additional insights. This can be achieved by counting instances where the LLM creates potential winning\nopportunities (i.e., aligning two moves for Tic-Tac-Toe, three moves for Connect Four, and four moves for Gomoku).\nSuch analysis can reveal that a high number of created opportunities may indicate proactive strategic thinking by the\nLLMs. Contributions to the repository with suggestions and new evaluation metrics are encouraged to enhance the\nassessment of LLM capabilities in grid-based games.\nFigures 21, 22 and 23 summarize the outcomes of 2,310 games played between seven LLMs and a random play\ngenerator across different prompt types (list, illustration, image) for the games Tic-Tac-Toe, Connect Four, and Gomoku,\nrespectively. As shown in these results matrices, the LLMs demonstrated varying degrees of success across different\ngames and prompt types. Models like Claude 3.5 Sonnet, GPT-4o, and Gemini 1.5 Pro showed strong performance in\nsimpler formats but struggled with more complex prompts. Random play consistently had the highest number of invalid\nmoves, highlighting its lack of strategy. The data suggests that while some LLMs handle simpler game prompts well,\nmore complex formats reveal significant challenges in their decision-making processes. In the result matrices, the W\ncolumn shows the total number of games the LLM won as the first and second player, D indicates draws, and Q shows\nthe total number of games the LLM was disqualified as the first and second player. Each LLM played 5 games with\neach corresponding opponent for each game and prompt type combination, totaling 35 games per player for list and\nillustration prompts, and 30 games per player for the image prompt type. The lower game count for the image prompt\ntype is because Llama3-70B cannot accept images and therefore was not used with the image prompt type. The result\nmatrix is also hosted on the project’s GitHub page.\nFigure 24 displays a portion of the leaderboard page of the LLM Game Benchmark, summarizing the results of games\nplayed between various LLMs and a random play generator. Key metrics on the leaderboard include win ratios, number\nof wins, dis-qualifications, invalid moves, and total moves for both the first and second players. Users can filter games\nby type, prompt type, and LLM players, and sort results by clicking on any of the column headers. Functionality to\naggregate results by game type, prompt type, and LLM player is provided on the leaderboard page. The initial data\nincludes results from Claude 3.5 Sonnet, Claude 3 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4 Turbo, GPT-4o and\nRandom-Play. Each LLM played against each other LLM five times for each game and prompt type combination. New\ngame result submissions are welcome and can be generated using the game simulation web software. The leaderboard\n19\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 21: The results matrix showing the outcomes of matches between seven LLMs and a random play generator\nsoftware across different games for Tic-Tac-Toe.\nweb page can be accessed on the benchmark’s GitHub page 1 [44]. The data used to fill in the leaderboard page can be\ndownloaded in JSON format.\n5\nDiscussion\nThis study provides a comprehensive evaluation of seven LLMs, Claude 3.5 Sonnet, Claude 3 Sonnet, Gemini 1.5\nFlash, Gemini 1.5 Pro, GPT-4 Turbo, GPT-4o, and Llama3-70B, alongside a random play generator across three games\n(Tic-Tac-Toe, Connect Four, and Gomoku) and three prompt types (list, illustration, image). Each LLM played five\ngames per game and prompt type against each opponent, resulting in a total of 2,310 games for analysis.\nThe performance of the LLMs varied significantly across different games and prompt types. Simpler games like\nTic-Tac-Toe experienced fewer invalid moves and disqualifications compared to more complex games like Connect\nFour and Gomoku, highlighting the models’ varying capacities to handle increasing game complexity. LLMs performed\nbetter with list prompts for Tic-Tac-Toe and Connect Four, while more complex prompt formats, particularly illustration\nand image prompts, revealed challenges in strategic decision-making. These formats led to higher disqualification rates\nand missed strategic opportunities, indicating difficulties in interpreting visual data and maintaining consistency.\nThe random play strategy consistently recorded the highest number of losses and invalid moves, serving as a useful\nbaseline for gauging LLM performance. The stark contrast between random play and the LLMs underscores the models’\ncapacity for strategic decision-making, though there remains room for improvement in handling complex and visual\ndata.\nIn Tic-Tac-Toe, LLMs demonstrated strong performance with list prompts, exhibiting minimal invalid moves, indicating\na good understanding of the game’s basic rules. Performance declined with illustration prompts, as some LLMs\n1LLM Grid-Based Game Leaderboard: https:\/\/research-outcome.github.io\/LLM-Game-Benchmark\/leaderboard\/\n20\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 22: The results matrix showing the outcomes of matches between seven LLMs and a random play generator\nsoftware across different games for Connect Four.\nshowed increased invalid moves, suggesting challenges in interpreting visual prompts. The most significant decline was\nobserved with image prompts, where many LLMs displayed a high number of in-valid moves, highlighting difficulties\nin processing and responding to image-based prompts.\nIn Connect Four, LLMs generally performed well with list prompts, although some showed increased invalid moves\ncompared to Tic-Tac-Toe, reflecting the higher complexity of Connect Four. A significant increase in invalid moves and\ndisqualifications was observed with both illustration and image prompts, indicating substantial challenges in visual\ninterpretation and decision-making in a more complex game context.\nFor Gomoku, performance was mixed across all prompt types, with a notable increase in invalid moves and disqualifica-\ntions. This game, being more complex and less common, likely posed significant interpretative and decision-making\nchallenges for the LLMs.\nDifferent prompt types had a notable impact on the performance of the LLMs. List prompts were generally well-handled\nby all LLMs, suggesting that textual representation of game states is within their current capabilities. Illustration\nprompts posed moderate challenges, as reflected in the increased number of invalid moves, indicating that graphical\nrepresentations are harder for LLMs to interpret. Image prompts were the most challenging, with the highest number of\ninvalid moves, highlighting a significant area for improvement in LLMs’ ability to process and act on image-based\ninputs.\nInvalid moves analysis revealed no out-of-bounds errors in any games, unlike in our previous study. This improvement\nis likely due to updated prompts that clearly define the range of possible column and row values. Invalid format errors\nwere made only by GPT-4 Turbo and Gemini 1.5 Flash, mostly due to hallucinated tag names in the JSON format.\nGPT-4 Turbo made several invalid format errors in Tic-Tac-Toe and Gomoku games for list and illustration prompt types,\nwhile Gemini 1.5 Flash made several errors during Gomoku games for illustration and image prompts. A significant\npercentage of the invalid moves were due to moving to an already occupied space, as shown in Figures 12, 13, and 14.\n21\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nFigure 23: The results matrix showing the outcomes of matches between seven LLMs and a random play generator\nsoftware across different games for Gomoku.\nFigure 24: A snapshot from the leaderboard showing aggregated results.\nThe analysis of missed strategic opportunities highlights the variability in LLMs’ decision-making processes. Models\nlike Claude 3.5 Sonnet and GPT-4 Turbo showed fewer missed opportunities in list prompts, suggesting a better grasp\nof straightforward game mechanics. However, the higher frequency of missed opportunities in illustration and image\nprompts indicates that LLMs struggle with interpreting and acting upon visual data.\nWhile the study uses games as a benchmark, the findings have broader implications for LLM applications in fields such\nas robotics, autonomous systems, and interactive AI. Improving LLMs’ strategic thinking and decision-making abilities\ncan enhance their performance in various real-world tasks requiring similar cognitive skills.\nThe extensible nature of the benchmark, with its modular code and open invitation for community contributions,\nrepresents a significant step towards collaborative LLM research. Encouraging researchers to add new games and share\n22\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\ntheir results can lead to a more dynamic and comprehensive evaluation framework. Future work can include a broader\nrange of games and tasks to evaluate LLMs across different strategic environments.\n6\nLimitations and Future Directions\nThe study’s methodology primarily focuses on grid-based games, which, while useful, may not fully capture the breadth\nof real-world strategic interactions. Future benchmarks should incorporate a wider variety of game types, including\nthose with more complex rules and longer-term strategic planning, to provide a more comprehensive assessment of\nLLM capabilities. Designing new, custom, purpose-built games to test specific aspects of LLM capabilities, such as\nadapting to unusual rules, would enhance benchmarking effectiveness and prevent the possibility of LLMs becoming\nfamiliar with the game, even if they were not specifically trained for it.\nThe simplicity of the games used in this benchmark facilitates basic evaluation but may not challenge LLMs’ strategic\ncapabilities as much as more complex games like chess or Go might. The fact that current LLMs have not mastered\neven these simple games provides valuable insights into their capabilities and limitations. Expanding the evaluation to\nlarger grids, such as 4 × 4 or 5 × 5 for Tic-Tac-Toe or 19 × 19 for Gomoku, could present additional challenges and\nprovide a clearer indicator of LLM performance.\nRelying on predefined prompts to guide LLMs’ moves may not adequately capture their potential for independent\nstrategic thinking or their ability to respond to changing game states. Although we updated prompts dynamically to\nwarn LLMs of invalid moves, further techniques, such as providing all previous invalid moves, could be explored to\nreduce invalid move numbers and disqualifications.\nThis study tested LLMs using structured prompts. Future research should investigate how these prompts influence LLM\nperformance and how variations in prompt structure might affect their understanding of game states and subsequent\nmoves. Such insights could help optimize LLMs for more complex and varied applications.\nThe evaluation metrics used in this study revealed a wide range of LLM capabilities. While these metrics provide a\ngood indication of performance, they may not fully capture the strategic complexity of the models. Further analysis\nof the moves—drawn from the JSON and PNG files—could offer a more detailed assessment of game progress over\ntime. The new analysis can be conducted using the Game Simulation web app to generate data for new LLMs or the\nopen-access data on GitHub. For example, evaluating the creation of winning opportunities, such as aligning moves\nfor Tic-Tac-Toe, Connect Four, and Gomoku, can provide insights into proactive strategic thinking by the LLMs. The\nauthors encourage and welcome contributions to the repository in the form of suggestions and implementations of new\nmetrics and methods to evaluate the capabilities of LLMs.\nFocusing on a select group of LLMs might not capture the full diversity of strategic approaches across available models,\nhighlighting the importance of including a broader array in future research. The rapidly expanding landscape of LLMs,\nwith new models and improved versions emerging frequently, necessitates continuous updates to benchmarks. We\nwelcome submissions of other and new LLMs using the open-source game simulation software.\nFuture work could explore several promising directions to extend research and deepen our understanding of LLM\ncapabilities in strategic games and beyond. Multi-agent collaboration scenarios, where multiple LLMs work together\nagainst a common opponent or compete in teams, could assess their abilities in coordination, cooperation, and\ncompetitive strategy. Comparing newer versions of LLMs against those tested in this study could track progress and\nimprovements in AI strategic gaming capabilities over time.\nThis study suggests several avenues for future research and development. Firstly, improving LLMs’ abilities to interpret\nand act on visual data is crucial, as evidenced by high invalid move rates in illustration and image prompts. Enhancing\nvisual processing capabilities could significantly boost overall performance and utility. Secondly, further research is\nneeded to enhance LLMs’ decision-making processes in more complex environments.\n7\nConclusion\nThis study introduces a novel and extensible benchmark for LLMs through grid-based games such as Tic-Tac-Toe,\nConnect Four, and Gomoku. The open-source game simulation code, available on GitHub, enables LLMs to compete\nand generates data files in JSON, CSV, TXT, and PNG formats for leaderboard rankings and further analysis. We\npresent the results of games among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by Anthropic,\nGemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and GPT-4o by OpenAI, and Llama3-70B by Meta,\nand encourage submissions from other LLMs. By analyzing the performance of these models over 2,310 games,\nwe observed significant variations in their capabilities, particularly highlighting their struggles with complex and\n23\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\nvisually-based prompt formats. Comparisons with a random play generator underscore the LLMs’ superior yet still\ndeveloping capacity for strategic decision-making.\nThe study reveals that while LLMs perform relatively well in simpler formats, such as list prompts for Tic-Tac-Toe\nand Connect Four, their performance declines with more complex prompts, especially those involving illustrations and\nimages. This trend indicates the current limitations in LLMs’ ability to interpret and act on visual data and manage\nincreased game complexity. Additionally, the models showed a tendency to make invalid moves when faced with more\ncomplex prompts, underscoring the need for improved strategic decision-making processes.\nSeveral areas for further investigation could be explored, such as expanding the types and complexity of games used for\nevaluation, testing more sophisticated prompt engineering techniques, and delving deeper into the effects of different\nprompt structures.\nThe findings of this study have broader implications beyond gaming, suggesting that advancements in LLMs’ strategic\nthinking and decision-making abilities could enhance their application in fields such as robotics, autonomous systems,\nand interactive AI. Furthermore, the modular and open-source nature of the benchmarking framework encourages\ncommunity contributions, which can lead to a more dynamic and comprehensive evaluation of LLM capabilities.\nIn conclusion, while the current evaluation highlights both the strengths and limitations of LLMs, it also points to\nthe need for ongoing research to enhance their ability to process complex and visual data, improve decision-making\nprocesses, and develop more sophisticated benchmarking tools. This continuous development will ultimately broaden\nthe applicability and effectiveness of LLMs in various real-world tasks.\nAcknowledgments\nThis study was partially supported by Florida Polytechnic University with grant number GR-24SUMR-OT.\nReferences\n[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian. A\ncomprehensive overview of large language models. arXiv, 2023.\n[2] B. Goertzel and C. Pennachin, editors. Artificial General Intelligence, volume 2. Springer, New York, NY, USA,\n2007.\n[3] J. Huang. Nvidia ceo predicts agi in 5 years, 2024. Available online: https:\/\/www.barrons.com\/articles\/\nnvidia-ceo-jensen-huang-agi-breakthrough-a7029004 (accessed on 7 June 2024).\n[4] Y. LeCun. Meta ai chief skeptical about agi, quantum computing, 2023. Available online: https:\/\/www.cnbc.\ncom\/2023\/12\/03\/meta-ai-chief-yann-lecun-skeptical-about-agi-quantum-computing.html\n(accessed on 7 June 2024).\n[5] I. Sutskever. The exciting, perilous journey toward agi, 2024. Available online: https:\/\/www.ted.com\/talks\/\nilya_sutskever_the_exciting_perilous_journey_toward_agi (accessed on 7 June 2024).\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention\nis all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.\n[7] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv, 2018. arXiv:1810.04805.\n[8] A. Radford,\nK. Narasimhan,\nT. Salimans,\nand I. Sutskever.\nImproving language understand-\ning by generative pre-training,\n2024.\nAvailable online:\nhttps:\/\/paperswithcode.com\/paper\/\nimproving-language-understanding-by (accessed on 7 June 2024).\n[9] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg,\net al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv [Cs.CL], 2024. Available\nonline: http:\/\/arxiv.org\/abs\/2303.12712 (accessed on 7 June 2024).\n[10] G. Team, R. Anil, S. Borgeaud, Y. Wu, J. B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\net al.\nGemini: A family of highly capable multimodal models.\narXiv [Cs.CL], 2024.\nAvailable online:\nhttp:\/\/arxiv.org\/abs\/2312.11805 (accessed on 7 June 2024).\n[11] Anthropic. Model card and evaluations for claude models, 2024. Available online: https:\/\/www-files.\nanthropic.com\/production\/images\/Model-Card-Claude-2.pdf (accessed on 7 June 2024).\n24\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\n[12] Grok Large Language Model. Grok large language model, 2024. Available online: https:\/\/x.ai\/blog\/grok\n(accessed on 7 June 2024).\n[13] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro,\nF. Azhar, et al. Llama: Open and efficient foundation language models. arXiv [Cs.CL], 2024. Available online:\nhttp:\/\/arxiv.org\/abs\/2302.13971 (accessed on 7 June 2024).\n[14] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel,\nG. Lample, L. Saulnier, et al. Mistral 7b. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/\n2310.06825 (accessed on 7 June 2024).\n[15] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy. Challenges and applications of large\nlanguage models. arXiv, 2023. arXiv:2307.10169.\n[16] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao. Large language models:\nA survey. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/2402.06196 (accessed on 7 June\n2024).\n[17] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, et al. A survey on\nevaluation of large language models. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/2307.\n03109 (accessed on 7 June 2024).\n[18] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis\nplatform for natural language understanding. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/\n1804.07461 (accessed on 7 June 2024).\n[19] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Superglue: A\nstickier benchmark for general-purpose language understanding systems. arXiv [Cs.CL], 2024. Available online:\nhttp:\/\/arxiv.org\/abs\/1905.00537 (accessed on 7 June 2024).\n[20] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar,\net al. Holistic evaluation of language models. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/\n2211.09110 (accessed on 7 June 2024).\n[21] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask\nlanguage understanding. CoRR, abs\/2009.03300, 2020. Available online: https:\/\/arxiv.org\/abs\/2009.\n03300 (accessed on 7 June 2024).\n[22] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta,\nA. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language\nmodels. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/2206.04615 (accessed on 7 June\n2024).\n[23] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge. arXiv, 2018. arXiv:1803.05457.\n[24] S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv, 2021.\narXiv:2109.07958.\n[25] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence?\narXiv, 2019. arXiv:1905.07830.\n[26] C. White, S. Dooley, M. Roberts, A. Pal, B. Feuer, S. Jain, et al. Livebench: A challenging, contamination-free\nllm benchmark. arXiv, 2024. arXiv:2406.19314.\n[27] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, et al. A survey on evaluation of large language models.\nACM Transactions on Intelligent Systems and Technology, 15(3):1–45, 2024.\n[28] Z. Guo, R. Jin, C. Liu, Y. Huang, D. Shi, L. Yu, et al. Evaluating large language models: A comprehensive survey.\narXiv, 2023. arXiv:2310.19736.\n[29] Andrew Ng. We need better evals for llm applications, 2024. Available online: https:\/\/www.deeplearning.\nai\/the-batch\/we-need-better-evals-for-llm-applications\/ (accessed on 7 June 2024).\n[30] Q. Tan, A. Kazemi, and R. Mihalcea. Text-based games as a challenging benchmark for large language models,\n2024. Available online: https:\/\/openreview.net\/forum?id=2g4m5S_knF (accessed on 7 June 2024).\n[31] D. Qiao, C. Wu, Y. Liang, J. Li, and N. Duan. Gameeval: Evaluating llms on conversational games. arXiv [Cs.CL],\n2024. Available online: http:\/\/arxiv.org\/abs\/2308.10032 (accessed on 7 June 2024).\n[32] Y. Wu, X. Tang, T. M. Mitchell, and Y. Li. Smartplay: A benchmark for llms as intelligent agents. arXiv [Cs.CL],\n2024. Available online: http:\/\/arxiv.org\/abs\/2310.01557 (accessed on 7 June 2024).\n25\nEvaluating LLMs with Grid-Based Game Competitions - An Extensible Benchmark & Leaderboard\n[33] R. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei, et al.\nMindagent: Emergent gaming interaction. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/\n2309.09971 (accessed on 7 June 2024).\n[34] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz. Playing repeated games with large\nlanguage models. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/2305.16867 (accessed on\n7 June 2024).\n[35] N. Lorè and B. Heydari. Strategic behavior of large language models: Game structure vs. contextual framing. SSRN\nElectronic Journal, September 2023. Available online: https:\/\/ssrn.com\/abstract=4569717 (accessed on\n7 June 2024).\n[36] C. F. Tsai, X. Zhou, S. S. Liu, J. Li, M. Yu, and H. Mei. Can large language models play text games well? current\nstate-of-the-art and open questions. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/2304.\n02868 (accessed on 7 June 2024).\n[37] C. Fan, J. Chen, Y. Jin, and H. He. Can large language models serve as rational players in game theory? a\nsystematic analysis. arXiv [Cs.CL], 2024. Available online: http:\/\/arxiv.org\/abs\/2312.05488 (accessed\non 7 June 2024).\n[38] D. Liga and L. Pasetto. Testing spatial reasoning of large language models: The case of tic-tac-toe. In A. Bruno,\nA. Pipitone, R. Manzotti, A. Augello, P. L. Mazzeo, F. Vella, and A. Chella, editors, Proceedings of the 1st\nWorkshop on Artificial Intelligence for Perception and Artificial Consciousness (AIxPAC 2023) Co-Located with\nthe 22nd International Conference of the Italian Association for Artificial Intelligence (AIxIA 2023), pages 64–79,\nRoma, Italy, 2023.\n[39] J. Duan, R. Zhang, J. Diffenderfer, B. Kailkhura, L. Sun, E. Stengel-Eskin, et al. Gtbench: Uncovering the\nstrategic reasoning limitations of llms via game-theoretic evaluations. arXiv, 2024. arXiv:2402.12348.\n[40] A. Costarelli, M. Allen, R. Hauksson, G. Sodunke, S. Hariharan, C. Cheng, et al. Game-bench: Evaluating\nstrategic reasoning abilities of llm agents. arXiv, 2024. arXiv:2406.06613.\n[41] O. Topsakal and J. B. Harper. Benchmarking large language model (llm) performance for game playing via\ntic-tac-toe. Electronics, 13(8), 2024.\n[42] R. Gallotta, G. Todd, M. Zammit, S. Earle, A. Liapis, J. Togelius, and G. N. Yannakakis. Large language models\nand games: A survey and roadmap. arXiv, 2024. arXiv:2402.18659.\n[43] S. Hu, T. Huang, F. Ilhan, S. Tekin, G. Liu, R. Kompella, and L. Liu. A survey on large language model-based\ngame agents. arXiv, 2024. arXiv:2404.02039.\n[44] LLM Game Benchmark.\nLlm game benchmark, 2024.\nAvailable online:\nhttps:\/\/github.com\/\nresearch-outcome\/LLM-Game-Benchmark\/ (accessed on 19 June 2024).\n[45] Tic tac toe game, 2024. Available online: https:\/\/en.wikipedia.org\/wiki\/Tic-tac-toe (accessed on 7\nJune 2024).\n[46] Connect4, 2024. Available online: https:\/\/en.wikipedia.org\/wiki\/Connect_Four (accessed on 7 June\n2024).\n[47] Gomoku, 2024. Available online: https:\/\/en.wikipedia.org\/wiki\/Gomoku (accessed on 7 June 2024).\n[48] L. V. Allis, H. J. van den Herik, and M. P. Huntjens. Go-moku solved by new search techniques. Computational\nIntelligence, 12(1):7–72, 1996.\n[49] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao. Large language models:\nA survey. arXiv, 2024. arXiv:2402.06196.\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, et al. A survey of large language models. arXiv, 2023.\narXiv:2303.18223.\n26\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard.pdf"}
{"title":"Simple Stochastic Stopping Games: A Generator and Benchmark Library","authors":"Avi Rudich, Isaac Rudich, Rachel Rue","summary":"Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the\nsimplest version of Stochastic Games for which there is no known\npolynomial-time algorithm. Condon showed that Stochastic Games are\npolynomial-time reducible to SSGs, which in turn are polynomial-time reducible\nto Stopping Games. SSGs are games where all decisions are binary and every move\nhas a random outcome with a known probability distribution. Stopping Games are\nSSGs that are guaranteed to terminate. There are many algorithms for SSGs, most\nof which are fast in practice, but they all lack theoretical guarantees for\npolynomial-time convergence. The pursuit of a polynomial-time algorithm for\nSSGs is an active area of research. This paper is intended to support such\nresearch by making it easier to study the graphical structure of SSGs. Our\ncontributions are: (1) a generating algorithm for Stopping Games, (2) a proof\nthat the algorithm can generate any game, (3) a list of additional\npolynomial-time reductions that can be made to Stopping Games, (4) an open\nsource generator for generating fully reduced instances of Stopping Games that\ncomes with instructions and is fully documented, (5) a benchmark set of such\ninstances, (6) and an analysis of how two main algorithm types perform on our\nbenchmark set.","url":"http:\/\/arxiv.org\/abs\/2402.02571v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.02571v1","published":1707067899000,"comment":"18 pages, 1 figure, 4 tables","pdf_text":"Simple Stochastic Stopping Games:\nA Generator and Benchmark Library\nAvi Rudich∗2, Isaac Rudich∗1, Rachel Rue∗2\n∗All authors contributed equally.\n1 Polytechnique Montr´eal, Montreal, Canada\n2 Unaffiliated\nAbstract\nSimple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the\nsimplest version of Stochastic Games for which there is no known polynomial-time\nalgorithm [1]. Condon showed that Stochastic Games are polynomial-time reducible\nto SSGs, which in turn are polynomial-time reducible to Stopping Games. SSGs are\ngames where all decisions are binary and every move has a random outcome with\na known probability distribution. Stopping Games are SSGs that are guaranteed to\nterminate. There are many algorithms for SSGs, most of which are fast in practice,\nbut they all lack theoretical guarantees for polynomial-time convergence. The pursuit\nof a polynomial-time algorithm for SSGs is an active area of research. This paper is\nintended to support such research by making it easier to study the graphical structure\nof SSGs. Our contributions are: (1) a generating algorithm for Stopping Games, (2)\na proof that the algorithm can generate any game, (3) a list of additional polynomial-\ntime reductions that can be made to Stopping Games, (4) an open source generator\nfor generating fully reduced instances of Stopping Games that comes with instructions\nand is fully documented, (5) a benchmark set of such instances, (6) and an analysis of\nhow two main algorithm types perform on our benchmark set.\n1\nIntroduction\nStochastic Games were first introduced in 1953 to model situations where strategic decisions\nare made under conditions of uncertainty. A strategy is the set of decisions each respective\nplayer makes about how to move when the game is at a position they control, and the\noutcome of each move is governed by a known probability distribution. [2]. There exist\noptimal strategies that are both global (all moves are chosen in advance of the game) and\ndeterministic (exactly one move is chosen for each game position) [1].\nThere is no known polynomial-time algorithm for general stochastic games. In 1990, Con-\ndon defined Simple Stochastic Games (SSGs), gave a polynomial-time reduction of Stochas-\n1\narXiv:2402.02571v1  [cs.CC]  4 Feb 2024\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\ntic Games to SSGs, and proved that SSGs are in NP ∩co-NP [1]. In an SSG, there are\ntwo players, and all of the decisions made by the players are binary. Simple Stochastic\nGames have become a focus of research because they are the simplest version of stochastic\ngames for which there is no known polynomial-time algorithm. They are also among the\nfew combinatorial problems in NP ∩co-NP not known to be in P [1].\nSSGs are played on a directed graph composed of four types of nodes: max, min, average,\nand terminal. The two terminal nodes are labeled 1 and 0, and have out-degree 0. All other\nnodes have out-degree 2. At max nodes, the max player determines which arc to take; at\nmin nodes, the min player determines which arc to take; and at average nodes, each arc\nis taken with probability 1\n2. The goal of the max player is to maximize the probability\nthat a random walk on the graph from a given start node reaches terminal-1, and the goal\nof the min player is to minimize that probability. The value of a node given a pair of\nmax\/min strategies is the probability that a random walk starting from that node reaches\nterminal-1. A stable assignment of values is any assignment compatible with mutually\noptimal max\/min strategies.\nWhile it is an open question whether SSGs admit a polynomial-time algorithm, many\nalgorithms are fast in practice [3, 4, 5, 6]. These algorithms generally fall into two cate-\ngories: value improvement and strategy improvement. Both types of algorithms start from\nsome initial position and make improvements by updating either node values or player\nstrategies until the constraints that define an SSG are satisfied. Another approach is the\npermutation-improvement algorithm introduced in [7]. The algorithm starts from an initial\npermutation of stochastic (generalized average) nodes where a permutation represents an\nordering of values, it computes the optimal strategies for that permutation, and then it\nupdates the permutation until optimal strategies are found.\nDespite considerable work in refining existing approaches, there remains a large gap be-\ntween the best known lower and upper bounds.\nHalman [8] gave an upper bound of\neO(√n log n) for LP-type problems and showed that SSGs can be formulated as an LP-type\nproblem. A lower bound can be demonstrated by constructing an example that forces the\nHoffman-Karp strategy improvement algorithm, which solves an LP in each iteration, to\ngo through a linear number of iterations [6]. To our knowledge, no one has been able to\nproduce an example that requires more than a linear number of iterations.\nCondon made a further simplification of SSGs by proving that SSGs are polynomial-time\nreducible to Stopping Games, which terminate with probability 1 and have a unique stable\nvalue vector [1]. Additionally, it is possible to make several other simple reductions to\nremove all subgraphs that are solvable in polynomial time. Improved algorithms have been\nproposed that leverage the graphical structure of SSGs [7, 4, 5, 3, 9, 10, 11, 12]. Further\nexploration of the structure might produce a better understanding of why it is hard to prove\npolynomial-time convergence, or why it is challenging to produce hard instances. For that\npurpose, it is useful to have a benchmark set of SSGs that are as simple as possible without\n2\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nlosing complexity.\nPrevious research on the behavior of different types of algorithms for solving SSGs used\neither small or non-stopping instances of SSGs [5, 6]. In this work, we generate a set of\nlarge stopping games. The contributions of this paper are as follows: (1) We describe\nan algorithm for generating Stopping Game instances. (2) We prove that our algorithm is\ncapable of generating any Stopping Game. (3) We describe several other simple polynomial-\ntime reductions performed by the generator. (4) We provide an open source implementation\nof our generator algorithm that comes with instructions and is fully documented. (5) We\nprovide a benchmark set of fully reduced problems with games ranging from 32 nodes\nto 4096 nodes.\n(6) We study the behavior of Hoffman-Karp (a strategy improvement\nalgorithm) and a permutation improvement algorithm on the problems in our benchmark\nset.\n1.1\nDefinitions\nA Simple Stochastic Game (SSG) is a directed graph G with four types of nodes: max,\nmin, average, and terminal nodes. Each max, min, and average node v has exactly two\nout-arcs. There are two terminal nodes, terminal-1 and terminal-0, with no out-arcs. By\nconvention, the nodes of G are numbered 1, . . . , n, with the terminals 0 and 1 numbered\nn −1 and n, respectively. Each node i ∈G is assigned a value vi ∈[0, 1], and the values of\nthe terminal nodes are always set to 0 and 1 (vn−1 = 0, vn = 1).\nA stable assignment to G is an assignment of values to nodes such that:\nvi = max (vj, vk)\nfor each max node i with children j, k\nvi = min (vj, vk)\nfor each min node i with children j, k\nvi = (vj + vk)\n2\nfor each average node i with children j, k\nWe say that nodes whose values satisfy the equations above are satisfied.\nA strategy σ for the max player is a set of arcs consisting of one arc (i, j) from each max\nnode i. Equivalently, a strategy can be represented by the strategy subgraph Gσ produced\nfrom G by removing every max node out-arc not contained in σ. Similarly, a strategy τ\nfor the min player is a set of arcs consisting of one arc (i, j) from each min node i. The\nstrategy subgraph Gσ,τ is the subgraph of G induced by the max\/min strategy pair (σ, τ).\nGiven a pair of strategies (σ, τ), the solution value of each node is the probability that a\nrandom walk starting at that node will reach terminal-1 by following the unique out-arc\nfrom each max and min node in Gσ,τ, and by randomly following one of the two out-arcs\nfrom each average node with equal probability [1]. The solution to Gσ,τ can be found by\n3\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nsetting the value of all nodes with no path to a terminal to zero, and then solving the\nsystem of equations below for the remaining node values [1].\nvi = vj\nfor each max node i with children j, k, where j ∈σ\nvi = vj\nfor each min node i with children j, k, where j ∈τ\nvi = (vj + vk)\n2\nfor each average node i with children j, k\nThe goal of the max player is to maximize the probability that a random walk will end at\nterminal-1 from any given start node, and the goal of the min player is to minimize that\nprobability. A max strategy σ is optimal with respect to a min strategy τ if all max nodes\nare satisfied in G by the solution values for Gσ,τ, and similarly for the optimality of a min\nstrategy τ with respect to a max strategy σ. It was shown in [1] that there always exists at\nleast one mutually optimal pair of strategies (σ∗, τ ∗), and that the vector of solution values\nis the same for every pair of mutually optimal strategies. Thus, the algorithmic problem\nfor the max player is to find a strategy that maximizes the solution value of every node if\nthe min player plays optimally.\nThe associated decision problem for SSGs is: Given a simple stochastic game G and a node\nv, does there exist a max strategy σ such that for any min strategy τ, a token starting at\nnode v in Gσ,τ arrives at terminal-1 with probability at least 1\n2? The decision problem is\nknown to be in NP ∩co −NP [1].\nA Stopping Game is a simple stochastic game G with the property that for every pair\nof strategies (σ, τ) and every node v in G, there is a path from v to a terminal in Gσ,τ.\nEquivalently, a Stopping Game is an SSG where, for any pair of strategies, a random walk\nstarting from any node will reach a terminal with probability 1.\nAnne Condon proved in [1]:\n1. A Stopping Game has exactly one stable assignment, which is the solution to the\ngame played with optimal max\/min strategies.\n2. For any SSG G, it is possible to construct a Stopping Game G′ such that a solution to\nG can be recovered in polynomial time from the unique solution to G′. In addition,\nif the max player has a strategy in G′ such that the start node has a value ≥1\n2, then\nthe max player also has a winning strategy in G.\nIt follows that if Stopping Games are in P, then Simple Stochastic Games (and Stochastic\nGames) are in P. For a Stopping Game G, the algorithmic problem is to find the unique\nstable assignment of values to nodes in G.\n4\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\n2\nStopping Game Generator\nIn this section, we provide an algorithm that generates Stopping Games, with parameters\nfor the number of max, min and average nodes. We prove that it generates only Stopping\nGames, and that, given fixed parameters, every Stopping Game has a non-zero probability\nof being generated. In Section 3, we describe simple subgraphs that can be solved quickly\nand give a modified version of the generator that produces Stopping Games with no such\nsubgraphs.\n2.1\nBad Subgraphs\nA simple stochastic game is a Stopping Game if and only if it does not contain certain\nsubgraphs, which we call bad subgraphs. The generator outputs simple stochastic games\nthat are guaranteed not to contain any bad subgraphs.\nDefinition 1 Let G be a simple stochastic game. A subgraph S of G is a bad subgraph if:\n1. Every max node in S has at least one arc pointing into S.\n2. Every min node in S has at least one arc pointing into S.\n3. Every average node in S has both arcs pointing into S.\n4. S contains no terminal nodes.\n5. S is strongly connected (any node in S is reachable from any other node in S).\nLemma 1 Let G be a simple stochastic game. If S is a subgraph of G satisfying criteria\n1-4 in the definition of a bad subgraph, then S contains a bad subgraph.\nProof: Let G be a simple stochastic game, and let S be a subgraph of G satisfying criteria\n1-4 in the definition of a bad subgraph. Let (σ, τ) be a pair of max\/min strategies such\nthat the single arc from each max and min node in Sσ,τ points back into Sσ,τ. Sσ,τ also\nsatisfies criteria 1-4. Any path in Gσ,τ starting from a node in Sσ,τ must eventually reach a\ncycle inside Sσ,τ. A cycle is a strongly connected component (SCC). Let C = {C1, . . . , Ci}\nbe the set of maximal SCCs in Sσ,τ. Let C′ be a directed graph with one node for each\nSCC in C, and an arc from Ci to Cj if there is a path in Sσ,τ from Ci to Cj. C′ is an acyclic\ngraph with at least one node with no out-arcs. Let Ck be an SCC in Sσ,τ represented by\na node with no out-arcs in C′. No node in Ck can have any arc at all pointing out of\nCk, because any path starting with such an arc must eventually cycle inside of Sσ,τ. By\nassumption such a cycle cannot be an SCC outside of Ck. Suppose there is a path starting\nwith an arc from a node in Ck to a node outside of Ck, ending in a cycle contained in Ck.\nThe union of Ck and a path leading out of it and back would be an SCC, but this violates\nthe maximality of Ck. So there can be no arc out of Ck. It follows that Ck satisfies all five\ncriteria in the definition of a bad subgraph.\n□\n5\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nLemma 2 A simple stochastic game G is a stopping game if and only if it contains no\nbad subgraphs.\nProof: Let G be a simple stochastic game containing a bad subgraph S. Let σ be a max\nstrategy that chooses arcs pointing into S for all max nodes in S, and let τ be a min\nstrategy that chooses arcs pointing into S for all min arcs in S. Then in Gσ,τ, there is\nno path to a terminal from any node in S, so G is not a stopping game. Conversely, let\nG be a simple stochastic game with no bad subgraphs. By Lemma 1, G cannot contain\nany subgraph satisfying criteria 1-4 of the definition of bad subgraphs. Therefore for any\nsubset V of non-terminal nodes of G and any strategy pair (σ, τ), there must exist some\nnode with an arc out of V in Gσ,τ. It follows that it is possible to construct a path from\nany node v0 to a terminal in Gσ,τ, by starting with V = v0 and in each step i finding a\nnode vi ∈G \\ V such that there is an arc from a node in V to vi, then setting V = V ∪vi.\nThe number of nodes is finite, so eventually a terminal node must be reached.\n□\n2.2\nA Simple Stopping Game Generator\nWe introduce a simple Stopping Game generator that has a non-zero probability of pro-\nducing any stopping game with one minor condition. The generator only generates games\nwhere no max or min node points directly to a terminal.\nThese nodes can be solved\nindependently of the rest of the graph in constant time (this is proven in Section 3.1).\nThe simple Stopping Game generator works in two phases.\nIn phase 1, all nodes are\nnumbered and each non-terminal node is randomly assigned an arc to a higher-numbered\nnode. Lemma 3 shows that this does not prevent any stopping games from being generated.\nIn phase 2, all nodes receive second arcs.\nFirst, all average nodes receive second arcs\nuniformly at random. To assign second arcs to max and min nodes, we pick a max or min\nnode m uniformly at random and find the set Q of all nodes q such that adding arc (m, q)\nto the graph will not create a bad subgraph. Q is non-empty because there is an always an\naverage node pointing to a terminal, and that node is in Q. We then pick a node v from\nQ uniformly at random and add the arc (m, v) to the graph. We prove below that this is\nsufficient to guarantee that the final constructed graph contains no bad subgraphs.\nLemma 3 Let G be a Stopping Game with n nodes. Then there is a numbering of nodes\nin G with the terminals numbered n −1 and n, such that for every non-terminal node v, v\nhas an arc to a higher-numbered node.\nProof. Fix any strategy pair (σ, τ) in G, and let S = Gσ,τ be the strategy subgraph induced\nby (σ, τ). By definition of a Stopping Game, for each node v, there is a path in S from v to\na terminal. Let S′ be a shortest path tree on S with all paths ending in a terminal node.\nDefine a partial order P on nodes of S as follows: For any non-terminal node a in S′ at\ndistance k from a terminal, let b be the next node in the unique path from a to the terminal\n(so b is at distance k−1 from the terminal). Let a < b in the partial order P. Let T be any\n6\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\ntotal order on the nodes of S that preserves P, and number the nodes of G according to\nT. By construction, every non-terminal node in G has an arc to a higher-numbered node.\n□\nAlgorithm 1: Stopping Game Generator\n1 node labeling Number the nodes from 1 to n. Pick integers a, b, c ≥1, such that\nn = a + b + c + 2. Label nodes n −1 and n as terminal-0 and terminal-1,\nrespectively. Label node n −2 as an average node. Assign the remaining numbers to\na −1 average nodes, b min nodes, and c max nodes uniformly at random.\n2 foreach non-terminal node v do\n3\nPick a higher-numbered node w uniformly at random and add an arc from v to w.\n\/* (Assigns an out-arc to each non-terminal node that has none.)\n*\/\n4 while there are average nodes with exactly one out-arc do\n5\nPick an average node m with exactly one out-arc (m, p) uniformly at random.\n6\nPick any node q \/∈{m, p} uniformly at random and add arc (m, q).\n7 while there are max or min nodes with exactly one out-arc do\n8\nPick a node m with exactly one out-arc (m, p) uniformly at random.\n9\nLet Q be the set of nodes such that arc (m, q) can be added to the graph without\nadding a bad subgraph, and q \/∈{m, p} (Algorithm 2).\n10\nRemove the terminal nodes from Q.\n11\nPick a node q ∈Q uniformly at random, and add arc (m, q).\n\/* (Assigns a second out-arc to each max, and min node.)\n*\/\n12 return The constructed graph.\n2.2.1\nProof of Correctness\nProof that the generator can produce any stopping game: By Lemma 3, in every stopping\ngame the nodes can be numbered in such a way that every non-terminal node has at least\none arc to a higher numbered node. Let G be a stopping game, and let G′ be a subgraph of\nG where every non-terminal node has degree 1 and its single arc goes to a higher-numbered\nnode. Every arc in G′ can be generated in Line 3 of the Stopping Game Generator.\nAll remaining arcs can be generated in Lines 6-11. Second arcs from average nodes are\ngenerated first, followed by second arcs from max and min nodes, in any order. The only\narcs that cannot be added are those that would create a bad subgraph; bad subgraphs\npersist as additional arcs from max and min nodes are added to the graph, so if a bad\nsubgraph were created at any point, the final constructed graph would not be a stopping\ngame. Consequently, each arc in G can be generated in Lines 6-11.\nProof that the generator produces only stopping games: Let G0 be the graph constructed\n7\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nin lines 1-6. In G0 each max and min arc has out-degree one. Therefore, there is only one\npossible pair of max\/min strategies, and G0 is also the only possible strategy subgraph. G0\nmust be a stopping game, since in G0, every node has a path through successively higher-\nnumbered nodes to a terminal node. Let Gk be the graph after k second arcs from max\nand min nodes have been added, with k ≥0. Assume inductively that Gk is a stopping\ngame, and so it contains no bad subgraphs. Let m be the next max or min node to receive\na second arc. Since Algorithm 2 generates a set of arc choices that will not create a bad\nsubgraph, Gk+1 must also be a stopping game for any of these arc choices.\nIt remains to be shown that the algorithm for finding valid arcs (Algorithm 2) correctly\nidentifies the set of nodes Q such that ∀q ∈Q, adding the arc (m, q) to Gk does not create\na bad subgraph. Suppose adding arc (m, v) to Gk would create a bad subgraph S. By\ndefinition, S is a strongly connected component and all nodes in S must be ancestors of\nm. Algorithm 2 removes all ancestors of m from the list of candidates for a new arc, so the\ninitial list Q is guaranteed to contain no node v such that adding arc (m, v) would create\na bad subgraph. Ancestor nodes are then added back to the candidate list one at a time,\nwith a node q restored to the candidate list Q if and only if:\n1. q is an average node with an arc to a node in Q or a terminal node (Lines 13-17,\n22-26); or\n2. q is a max or min node such that all of its arcs point to a node in Q (Lines 26-29).\nLet S be any bad subgraph that could be created from a subset of m’s ancestors by adding\nan arc from m to a node in S. Initially, Q ∩S = ∅. Condition 1 guarantees that any\naverage node restored to the candidate list Q has at least one arc pointing out of S and so\ncould not be contained in S. Condition 2 guarantees that any max or min node restored\nto Q has no arc pointing into S, and so could not be contained in S. Thus, no arc added\nback into Q is in S, and it remains the case that Q ∩S = ∅.\nLet v be any node such that adding arc (m, v) would not create a bad subgraph. Suppose\nv \/∈Q, the set returned by Algorithm 2. Let v-reachable be the set of nodes reachable from\nv without going through nodes in Q. All arcs out of nodes in v-reachable must either point\nback into v-reachable or point into Q. Define the perimeter of v-reachable to be the set\nof nodes in v-reachable with at least one arc into Q. None of the perimeter nodes can be\naverage nodes, because an average node with an arc into Q would be added to Q in Lines\n22-26. Any max or min node in the perimeter must have at least one arc into v-reachable,\nor it would have been added to Q in Lines 26-29. Thus all average nodes in v-reachable have\nboth arcs pointing into v-reachable and all max and min nodes in v-reachable have at least\none arc pointing into v-reachable. By Lemma 1, v-reachable must contain a bad subgraph,\ncontradicting the assumption that adding arc (m, v) will not create a bad subgraph. Thus\nall valid arcs in the ancestor set of m must be added back to Q.\nIt follows that the final list Q of valid arcs is correct.\n□\n8\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nAlgorithm 2: Find Valid Arcs\nInput: a node m that has exactly one out-arc (m, n)\n1 Let Q be a list containing all of the nodes in the graph.\n2 Q = Q\\{m, terminal-0, terminal-1}.\n3 Let P = {m}\n4 while P ̸= ∅do\n5\nLet p be a random node ∈P.\n6\nSet P = P\\{p}.\n7\nforeach parent p′ of p do\n8\nif p′ ∈Q then\n9\nQ = Q\\{p′}.\n10\nAdd p′ to P.\n\/* (Removes all ancestors from the candidate list.)\n*\/\n11 Let T = ∅\n12 foreach node v \/∈Q do\n13\nLet (v, n) be v’s first out-arc, and (v, p) be v’s second out-arc if it exists.\n14\nif v is an average node then\n15\nif n ∈{Q, terminal-0, terminal-1} OR p ∈{Q, terminal-0, terminal-1} if (v, p)\nexists then\n16\nSet T = T ∪{v}.\n17\nSet Q = Q ∪{v}.\n\/* (Finds initial nodes for final processing phase.)\n*\/\n18 while T ̸= ∅do\n19\nLet p be a random node ∈T.\n20\nSet T = T\\{p}.\n21\nforeach parent p′ of p do\n22\nLet (p′, p) be p′’s first out-arc, and (p′, u) be p′’s second out-arc if it exists.\n23\nif p′ \/∈Q then\n24\nif p′ is an average node then\n25\nSet T = T ∪{p′}.\n26\nSet Q = Q ∪{p′}.\n27\nelse if u ∈Q then\n28\nSet T = T ∪{p′}.\n29\nSet Q = Q ∪{p′}.\n\/* (Add back in nodes that don’t cause a bad subgraph.)\n*\/\n30 return Q.\n9\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\n3\nSimple Reductions\nThe reduction of SSGs to Stopping Games is not the only useful reduction for generating\ninstances of theoretical interest. In this section we discuss subgraphs that can be solved in\npolynomial (linear) time, and further reductions. Any research on the complexity of SSGs\ncan safely assume that each instance is a Stopping Game from which all of the following\nsubgraphs have been removed, and to which all of the following reductions have been\napplied.\n3.1\nTrivially Removable Subgraphs\nLet G be a Stopping Game. We show that we may assume for the purposes of complexity\nanalysis that G contains none of the following subgraphs. The strategy will proceed by\ndefining a subgraph S of a Stopping Game G, and then showing that a solution to G can\nbe recovered in constant time from a solution to G \\ S.\nMax or min node with at least one arc to a terminal. Let v be a max node with\narcs (v, a), (v, b) and suppose that a is a terminal node. If a is terminal-0, then v\ncan be merged with b without changing the solution value of any other nodes. If a is\nterminal-1, then v can be merged with a. The reverse is true for min nodes.\nAny node with two identical arcs. Let (v, w), (v, w) be the identical arcs. v can be\nmerged with w without changing the solution value of any other nodes.\nAn average node v with a self-arc. Let v have arcs (v, v), (v, w). Then:\nvalue(v) = value(v) + value(w)\n2\n⇒value(v) = value(w)\nSo, v can be merged with w without changing the solution value of any other nodes.\nA max, min, or average node with in-degree zero. Let v be a node with in-degree\nzero. The value of v has no effect on the value of any other node. We can find a\nsolution to the original SSG by removing v, solving the remaining graph, and then\nsolving v.\nA terminal with in-degree zero. Suppose that one of the two terminal nodes has in-\ndegree zero. Then the value of all nodes in the graph is trivially equal to the value\nof the other terminal node.\nAfter removing the above subgraphs, either G contains at least two distinct average nodes,\none with an arc to terminal-1 and one with an arc to terminal-0, or there is only one such\naverage node and the entire graph has a value of 1\n2. In our algorithm, we eliminate the\nsecond possibility.\n10\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\n3.2\nCollapsing Clusters\nIn this section we provide a linear-time algorithm for solving all nodes with values strictly\nequal to 1 or 0. It is based on the observation that a max node will always choose the arc\nthat points to the higher-value node. There is no higher value than 1, and no lower value\nthan 0. The reverse is true for the min nodes.\nThe algorithm for finding 1-valued nodes, detailed in Algorithm 3, starts by defining a\ncluster set of nodes containing only terminal-0. Any average nodes that point to terminal-\n0 have a value less than 1 and are added to the cluster. Any min nodes or average nodes\nthat have an arc to the cluster have a value less than 1. Similarly any max nodes with\nboth arcs to the cluster have a value less than 1. Nodes are added to the cluster until no\nremaining nodes can be added to the cluster. The remaining nodes have a value of 1. The\nalgorithm can be trivially reversed to find 0-valued nodes.\nAlgorithm 3: Find 1-Valued Nodes\n1 Label the nodes from 1 to n, with terminal-0 labeled n −1.\n2 Let q be a list containing only terminal-0.\n3 Let bv be a binary string of 1s with length n.\n4 Set bv[n −1] to 0.\n5 while q is not empty do\n6\nLet u be a node popped from q.\n7\nforeach parent p of u do\n8\nLet i be the index of node p.\n9\nif bv[i] is a 1 then\n10\nif p is a min or average node then\n11\nSet bv[i] to 0.\n12\nAdd p to q.\n13\nLet a and b be the indexes of the nodes that p has arcs to.\n14\nif p is a max node AND bv[a] = 0 = bv[b] then\n15\nSet bv[i] to 0.\n16\nAdd p to q.\n17 return The indexes of the ones in bv.\nThe algorithm is linear because the number of arcs, and therefore parents, is also linear.\nEach node can be considered only once, and when a node is considered, its parents are\niterated over. Each node is a parent to only two nodes and will only appear twice as a\nparent in the algorithm. Thus, the total number of parents considered is no more than 2n.\n11\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\n3.3\nStrongly Connected Components\nA strongly connected component (SCC) in a graph is a set of nodes such that every node\nin the SCC is reachable from every other node in the SCC. Dividing a graph into SCCs can\nbe done in linear time[13]. Note that the terminal nodes will always be their own SCCs.\nOnce a graph is divided into SCCs, it must be the case that there is an SCC C with no\npath to any other SCC except for the terminal nodes. The values of the nodes in this\nSCC are independent of the values of the nodes in the other SCCs. Thus, C can be solved\nindependently from the rest of the nodes. Once it is solved, there must be a new SCC with\nno path to any other SCC except for the terminal nodes and nodes with constant solved\nvalues. The process can be repeated until all of the nodes are solved. In other words, all\nSCCs can be solved independently, and it is safe to assume that your SSG is a single SCC.\nHowever, it is not necessarily true that the complexity of solving a simple stochastic game\nwith multiple SCCs is the same as that of solving a simple stochastic game with only one\nSCC apart from the terminals. Any individual SCC may have arcs to any number of solved\nnodes with values ∈(0, 1). So by assuming that every SSG is a single SCC, you lose the\nassumption that the only two nodes with constant values are terminal-0 and terminal-1.\n3.4\nUseful Assumptions Summary\nTo summarize this section, if you are pursuing a constructive proof that Stochastic Games\nare in P, you may assume all of the following about the game you are trying to solve:\n1. It is both a Simple Stochastic Game, and a Stopping Game.\n2. There are no max or min nodes with arcs to the terminal.\n3. There are no nodes with identical arcs or self-arcs.\n4. There are no nodes with in-degree zero.\n5. There is at least one pair of average nodes where one has an arc to terminal-0, and\nthe other has an arc to terminal-1.\n6. There are no nodes with a value of 1 or 0.\n7. EITHER it is a single SCC with the only out-arcs going to nodes with constant\nvalues in [0, 1], OR there are only two nodes with constant values, terminal-0 and\nterminal-1.\n3.5\nGenerator Implementation\nA modified version of the generator algorithm, with all of the modifications from this\nsection (Section 3), is shown on page 13 as Algorithm 4. Note that it is possible for this\ngenerator to create instances with nodes that have in-degree 0, but it occurs infrequently.\nWhen a specific size is desired, such instances can be thrown out and the generator rerun.\n12\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nAlgorithm 4: Modified Stopping Game Generator\n1 node labeling Number the nodes from 1 to n. Pick integers a ≥2 and b, c ≥1, such\nthat n = a + b + c + 2. Label nodes n −1 and n as terminal-0 and terminal-1,\nrespectively. Label node n −2 as an average node, and add an arc from node n −2\nto terminal-0. Label node n −3 as an average node, and add an arc from node n −3\nto terminal-1. Randomly assign the remaining numbers to a −2 average nodes, b\nmin nodes, and c max nodes.\n2 foreach average node v with no out-arcs do\n3\nPick a higher-numbered node w uniformly at random and add an arc from v to w.\n\/* (Assigns an out-arc to each non-terminal average node that has\nnone.)\n*\/\n4 foreach max and min node v do\n5\nPick a higher-numbered non-terminal node w uniformly at random and add an arc\nfrom v to w.\n\/* (Assigns an out-arc to each max and min node.)\n*\/\n6 Let z be the number of nodes with in-degree zero.\n7 Pick a random number r between max(z −(b + c), 0) and min(a, z).\n8 if r ̸= 0 then\n9\nforeach integer from 1 to r do\n10\nSelect an average node m with exactly one out-arc (m, p) uniformly at random.\n11\nPick a node q \/∈{m, p} with in-degree zero uniformly at random, and add arc\n(m, q).\n12 while there are average nodes with exactly one out-arc do\n13\nPick an average node m with exactly one out-arc (m, p) uniformly at random.\n14\nPick any node q \/∈{m, p} uniformly at random and add arc (m, q).\n15 while there are max or min nodes with exactly one out-arc do\n16\nPick a node m with exactly one out-arc (m, p) uniformly at random.\n17\nLet Q be the set of nodes such that arc (m, q) can be added to the graph without\ncreating a bad subgraph, and q \/∈{m, p} (Algorithm 2).\n18\nif there are nodes with in-degree zero ∈Q then\n19\nRandomly pick a node q ∈Q that has in-degree zero, and add arc (m, q).\n20\nelse\n21\nRandomly pick a node q ∈Q, and add arc (m, q).\n\/* (Assigns a second out-arc to each max, and min node.)\n*\/\n22 Merge 1-valued and 0-valued nodes into their respective terminals (Algorithm 3).\n23 return The constructed graph.\n13\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\n4\nBenchmark Instances\nIn this section, we discuss the implementation of our generator algorithm and the bench-\nmark set of problems we generated. The code and benchmark set is publicly available,\nwith documentation explaining how to generate new instances1.\n4.1\nBenchmark Set\nWe generated instances with sizes in powers of two from 25 to 212.\nFor each size, we\ngenerated 800 instances, with 100 instances at each ratio of average nodes to max nodes\n∈{1 : 4, 2 : 4, ..., 7 : 4, 8 : 4}. In order to maintain the intended ratio, some instances are\nslightly larger or slightly smaller than the labeled size. For example, instances labeled with\nsize 4096 and ratio 1 : 4 have 1820 max nodes, 1820 min nodes, 455 average nodes, and 2\nterminal nodes, for a true total of 4097 nodes.\nEach instance in our benchmark set is fully reduced using all of the reductions listed in\nSection 3.4, including that each instance is a single SCC plus two terminal nodes. This was\naccomplished by simply generating instances until we found enough in each category that\nwere already fully reduced when they were generated. This means that each instance in our\nbenchmark set has the property of being both a single SCC and having only two nodes with\nconstant value. However, if you are interested in studying the behavior of instances with\nseveral constant-valued nodes, that can be easily accomplished using the same instances.\nSimply reassign each arc that points to a terminal to a node with a random fixed value\n∈[0, 1]. The instance will still be fully reduced, and will still be a single SCC.\n4.2\nExperimental Results\nWe solved each of the 6400 problems in our benchmark set 100 times using random seeds\nwith two different algorithms and recorded the number of iterations and time to solve for\neach run. The experiments were performed on a computer equipped with an 2023 M2\nPro and 32Gb RAM. The first algorithm tested was Hoffman-Karp, the standard value-\niteration algorithm. The second algorithm tested was an implementation of a permutation\nimprovement algorithm based on an algorithm from Gimbert and Horn [7]. We achieved\na substantial speed-up over a naive implementation of the permutation improvement algo-\nrithm by generalizing the concept of collapsing clusters to nodes of any value, then pre-\ncalculating the collapsing clusters that would be implied by any given candidate ordering\nof the average nodes.\nFigure 4.2 shows data for the two algorithms, grouped by ratio, for the size 4096 SSGs.\nThe graphs for the other sizes look nearly identical but with globally lower numbers of\niterations; they are available with the code for the generator. The data shows that the\n1https:\/\/github.com\/IsaacRudich\/SimpleStochasticGamesBenchmark\n14\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nFigure 1: Average Iterations to Solve for Size 4096 Games\npermutation improvement algorithm consistently outperforms Hoffman-Karp in terms of\nboth iterations and time. Hoffman-Karp performs better as the number of decision nodes\ngoes down, while the permutation improvement algorithm shows consistent performance\nfor all of the ratios tested.\nAdditional data showing the average run-time, in both time and iterations, for both al-\ngorithms and each category of problem is available in Appendix A Tables 1, 2, 3, and 4.\nIn terms of average number of iterations, the permutation improvement algorithm out-\nperformed Hoffman-Karp on every category of problem. In terms of average amount of\ntime, the permutation improvement algorithm outperformed Hoffman-Karp on almost ev-\nery category of problem. The 4 categories (out of 48) where Hoffman-Karp out-performed\nare shown in bold.\n5\nConclusion\nThis paper presented a fast algorithm for generating Simple Stochastic Stopping Games.\nIt also provided several polynomial reductions and assumptions for further simplifying and\nreducing Simple Stochastic Games. These assumptions are useful for anyone attempting a\nconstructive proof that the complexity of Stochastic Games is in P. They are also useful\nfor trying to understand why hard-to-solve instances are so challenging to generate. Our\ncode for generating instances is open source, as well as the the benchmark instances we\ngenerated.\n15\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nReferences\n[1] A. Condon, “The complexity of stochastic games,” Information and Computation,\nvol. 96, no. 2, pp. 203–224, 1992.\n[2] L. S. Shapley, “Stochastic games,” Proceedings of the national academy of sciences,\nvol. 39, no. 10, pp. 1095–1100, 1953.\n[3] A. Condon, “On algorithms for simple stochastic games.,” Advances in computational\ncomplexity theory, vol. 13, pp. 51–72, 1990.\n[4] D. Auger, X. B. de Montjoye, and Y. Strozecki, “A generic strategy iteration method\nfor simple stochastic games,” CoRR, vol. abs\/2102.04922, 2021.\n[5] J. Kˇret´ınsk´y, E. Ramneantu, A. Slivinskiy, and M. Weininger, “Comparison of\nalgorithms for simple stochastic games,” Information and Computation, vol. 289,\np. 104885, 2022. Special Issue on 11th Int. Symp. on Games, Automata, Logics and\nFormal Verification.\n[6] C. W. Klingler, “An empirical analysis of algorithms for simple stochastic games,”\nGraduate Theses, Dissertations, and Problem Reports, 2023.\n[7] H. Gimbert and F. Horn, “Simple stochastic games with few random vertices are easy\nto solve,” in Foundations of Software Science and Computational Structures (R. Ama-\ndio, ed.), (Berlin, Heidelberg), pp. 5–19, Springer Berlin Heidelberg, 2008.\n[8] N. Halman, “Simple stochastic games, parity games, mean payoff games and dis-\ncounted payoff games are all lp-type problems,” Algorithmica, vol. 49, pp. 37–50,\n2007.\n[9] D. Dai and R. Ge, “New results on simple stochastic games,” in International Sym-\nposium on Algorithms and Computation, pp. 1014–1023, Springer, 2009.\n[10] R. Ibsen-Jensen and P. B. Miltersen, “Solving simple stochastic games with few coin\ntoss positions,” in Algorithms–ESA 2012: 20th Annual European Symposium, Ljubl-\njana, Slovenia, September 10-12, 2012. Proceedings 20, pp. 636–647, Springer, 2012.\n[11] D. Auger, P. Coucheney, and Y. Strozecki, “Solving simple stochastic games with few\nrandom nodes faster using bland’s rule,” arXiv preprint arXiv:1901.05316, 2019.\n[12] R. Tripathi, E. Valkanova, and V. A. Kumar, “On strategy improvement algorithms\nfor simple stochastic games,” Journal of Discrete Algorithms, vol. 9, no. 3, pp. 263–\n278, 2011.\n[13] R. Tarjan, “Depth-first search and linear graph algorithms,” SIAM journal on com-\nputing, vol. 1, no. 2, pp. 146–160, 1972.\n16\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nA\nExperimental Data\nratio\nsize\n128\n256\n512\n1024\n2048\n4096\n1-4\n5.5\n7.2\n8.6\n9.9\n11.4\n12.9\n2-4\n5.6\n7.0\n8.2\n9.2\n10.1\n10.9\n3-4\n5.4\n6.5\n7.6\n8.5\n9.5\n10.1\n4-4\n5.3\n6.2\n7.0\n8.0\n8.8\n9.5\n5-4\n4.9\n5.9\n6.7\n7.4\n8.3\n8.9\n6-4\n4.8\n5.6\n6.4\n7.2\n7.8\n8.6\n7-4\n4.6\n5.3\n6.0\n6.8\n7.6\n8.3\n8-4\n4.3\n5.2\n5.9\n6.5\n7.3\n7.9\nTable 1: Average Hoffman-Karp Iterations\nratio\nsize\n128\n256\n512\n1024\n2048\n4096\n1-4\n2.2\n3.0\n3.9\n4.8\n5.9\n6.8\n2-4\n2.9\n3.8\n4.9\n5.8\n6.4\n7.0\n3-4\n3.3\n4.1\n5.0\n5.9\n6.4\n7.1\n4-4\n3.4\n4.2\n4.9\n5.6\n6.3\n6.9\n5-4\n3.4\n4.1\n4.9\n5.4\n6.1\n6.6\n6-4\n3.4\n4.0\n4.8\n5.4\n5.9\n6.5\n7-4\n3.3\n4.0\n4.6\n5.2\n5.8\n6.4\n8-4\n3.2\n3.9\n4.4\n5.1\n5.6\n6.1\nTable 2: Average Permutation Improvement Iterations\n17\nA. Rudich, I. Rudich, R. Rue\nStopping Game Generator\nratio\nsize\n128\n256\n512\n1024\n2048\n4096\n1-4\n5.0\n12.1\n29.1\n71.1\n194.2\n1068.4\n2-4\n5.0\n12.3\n28.6\n74.6\n198.3\n1497.1\n3-4\n5.0\n11.7\n27.3\n71.4\n203.2\n2769.9\n4-4\n4.8\n11.1\n25.6\n69.5\n199.6\n4096.5\n5-4\n4.4\n10.5\n24.7\n64.7\n195.0\n3934.0\n6-4\n4.2\n10.1\n23.7\n64.1\n187.2\n4308.5\n7-4\n4.1\n9.4\n22.2\n59.9\n185.2\n4378.9\n8-4\n3.8\n9.2\n21.6\n58.4\n181.2\n5330.0\nTable 3: Average Time in Milliseconds for Hoffman-Karp (bold indicates that HK outper-\nformed perm-impr)\nratio\nsize\n128\n256\n512\n1024\n2048\n4096\n1-4\n1.9\n4.9\n13.1\n34.4\n103.2\n591.5\n2-4\n2.6\n7.0\n18.3\n50.6\n135.8\n945.8\n3-4\n3.1\n8.1\n20.4\n56.1\n157.9\n1817.7\n4-4\n3.3\n8.6\n21.3\n59.1\n168.0\n3210.5\n5-4\n3.4\n8.7\n22.8\n58.7\n174.0\n3326.0\n6-4\n3.5\n8.9\n22.7\n62.6\n177.8\n3195.3\n7-4\n3.5\n9.1\n22.7\n62.8\n183.3\n3922.9\n8-4\n3.6\n9.2\n22.5\n62.8\n180.8\n3918.1\nTable 4: Average Time in Milliseconds for Permutation Improvement (bold indicates that\nHK outperformed perm-impr)\n18\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Simple Stochastic Stopping Games: A Generator and Benchmark Library.pdf"}
{"title":"Application-level Benchmarking of Quantum Computers using Nonlocal Game Strategies","authors":"Jim Furches, Sarah Chehade, Kathleen Hamilton, Nathan Wiebe, Carlos Ortiz Marrero","summary":"In a nonlocal game, two noncommunicating players cooperate to convince a\nreferee that they possess a strategy that does not violate the rules of the\ngame. Quantum strategies allow players to optimally win some games by\nperforming joint measurements on a shared entangled state, but computing these\nstrategies can be challenging. We present a variational quantum algorithm to\ncompute quantum strategies for nonlocal games by encoding the rules of a\nnonlocal game into a Hamiltonian. We show how this algorithm can generate a\nshort-depth optimal quantum strategy for a graph coloring game with a quantum\nadvantage. This quantum strategy is then evaluated on fourteen different\nquantum hardware platforms to demonstrate its utility as a benchmark. Finally,\nwe discuss potential sources of errors that can explain the observed decreased\nperformance of the executed task and derive an expression for the number of\nsamples required to accurately estimate the win rate in the presence of noise.","url":"http:\/\/arxiv.org\/abs\/2311.01363v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.01363v4","published":1698941838000,"comment":"Significantly expanded version of the original article, v3 fixed\n  typos","pdf_text":"Application-level Benchmarking of Quantum Computers\nusing Nonlocal Game Strategies\nJim Furches1,2, Sarah Chehade3, Kathleen Hamilton3, Nathan Wiebe4,5,6, and\nCarlos Ortiz Marrero2,7\n1College of Computing, Georgia Institute of Technology, Atlanta, GA 30332\n2Physical Detection Systems and Deployment Division, Pacific Northwest National Laboratory, Richland, WA 99354\n3Quantum Computational Science Group, Oak Ridge National Laboratory, Oak Ridge, TN 37830\n4Department of Computer Science, University of Toronto, ON M5S 1A1, Canada\n5High Performance Computing Group, Pacific Northwest National Laboratory, Richland, WA 99354\n6Canadian Institute for Advanced Research, Toronto, On M5G 1M1, Canada\n7Department of Electrical & Computer Engineering, North Carolina State University, Raleigh, NC 27607\nIn a nonlocal game, two noncommunicating players cooperate to convince a ref-\neree that they possess a strategy that does not violate the rules of the game. Quan-\ntum strategies allow players to optimally win some games by performing joint mea-\nsurements on a shared entangled state, but computing these strategies can be chal-\nlenging. We present a variational quantum algorithm to compute quantum strategies\nfor nonlocal games by encoding the rules of a nonlocal game into a Hamiltonian.\nWe show how this algorithm can generate a short-depth optimal quantum strategy\nfor a graph coloring game with a quantum advantage. This quantum strategy is\nthen evaluated on fourteen different quantum hardware platforms to demonstrate\nits utility as a benchmark. Finally, we discuss potential sources of errors that can\nexplain the observed decreased performance of the executed task and derive an ex-\npression for the number of samples required to accurately estimate the win rate in\nthe presence of noise.\n1\nIntroduction\nRunning simple instances of quantum algorithms with a provable advantage is difficult given\nthe current state of quantum hardware [18, 48]. For this reason, it is important to develop\nbenchmarking tools and techniques that can test and validate the unique aspects of quantum\nJim Furches: james.furches@pnnl.gov\nSarah Chehade: chehades@ornl.gov\nKathleen Hamilton: hamiltonke@ornl.gov\nNathan Wiebe: nawiebe@cs.toronto.edu\nCarlos Ortiz Marrero: carlos.ortizmarrero@pnnl.gov\n1\narXiv:2311.01363v4  [quant-ph]  10 Mar 2025\nhardware that are consistent with the predictions of quantum theory. In particular, recent work\non quantum benchmarking has highlighted the importance of developing benchmarking metrics\nthat can measure progress toward quantum utility of useful quantum algorithms [46].\nLow-level benchmark metrics such as randomized benchmarking [21, 34, 37] aim to measure the\naverage error rates of a gate set independent of the initial state or measurement scheme, but are\nlimited, for example, in that it cannot help specify sources of error in an algorithmic pipeline [28]\nand can overestimate gate fidelity in the presence of errors [11]. High-level benchmarks such\nas Quantum Volume [16] aim to measure the performance of the entire quantum computing\nstack, including all classical control systems, but this can be too broad a metric and does not\nnecessarily capture the performance of useful quantum algorithms. In addition, both of these\nbenchmark metrics are difficult to compute at scale and fail to capture the ability of a specific\nhardware platform at attaining some quantum advantage.\nRecent work on nonlocal games has begun to shed light into their utility for quantum hardware\nverification, quantum advantage, and self-testing [43, 32, 53, 27, 20].\nIn a nonlocal game,\ntwo noncommunicating players cooperate to convince a referee that they possess a strategy\nthat does not violate the rules of a game. When players are allowed to use entanglement as\na resource in the development of their joint strategy, they are able to perform computations\nthat no classical computer can replicate without communication and can win the game with\nhigher probability.\nNonlocal games have been historically important and provide a unique\nsetting to explore the relationship between classical physics, quantum theory, and other non-\nsignaling theories [2, 13, 15, 47]. An extensive body of research links these games to foundational\nproblems in quantum physics, conjectures in operator algebras, and computational complexity\ntheory [22, 52, 31].\nMoreover, advances in quantum information theory and combinatorics\nhave revealed broad classes of games with a provable quantum advantage when players are\nallowed to incorporate quantum resources into their strategies, such as graph coloring and\ngraph homomorphism games [8, 40], making them exciting experimental candidates for testing\nquantum hardware [19]. Moreover, nonlocal games are classically verifiable, i.e. given a strategy,\nyou can check in polynomial time if the answers satisfy the rules of the game.\nDespite many breakthroughs in our theoretical understanding of nonlocal games, constructing\noptimal strategies for general nonlocal games remains a challenge. In our work, we propose a\nnew methodology for constructing strategies using variational methods and outline the utility\nof the strategies found for benchmarking. We begin Section 2 with an introduction to nonlocal\ngames and some definitions. In Section 3, we propose the use of a dual-phase optimization\ntechnique to find the resource state and the measurement scheme of a quantum strategies for a\nnonlocal game. In Section 4, we demonstrate how our method is able to successfully find optimal\nstrategies for CHSH, an N-Partite Symmetric game, and the graph coloring game. For the graph\ncoloring game, we were able to find a short-depth perfect quantum strategy for a graph on 14\nvertices shown to be the smallest graph instance where there exists a strict separation between\nclassical and quantum strategies [39, 35]. We then proceed to test the performance of this novel\nshort-depth strategy on 14 superconducting quantum computing devices and highlight some\npotential sources of errors causing decreased performance on some of the devices we tested.\nIn Section 5, we outline how we can use quantum strategies to benchmark quantum devices,\ntheir desirable noise robustness properties, and win rate estimation procedure in the presence\nof device shot noise.\n2\nStrategy\nPlayers devise joint strategy \nfrom rules λ\n1\nSeparation\nThey separate, taking\nentangled state 𝜌\n2\nMeasurement\nMeasure 𝜌 independently based \non 𝑞 from referee\n3\n𝑎𝐴\n𝑎𝐵\nEvaluation\nReferee evaluates the \nanswers with λ \n4\nFigure 1: Flow of a nonlocal game. After formulating a strategy, Alice and Bob separate and cannot commu-\nnicate. For a quantum strategy, they each take a part of an entangled state ρ and upon receiving a question q\nfrom the referee, they perform a measurement on their respective states, giving an answer a ∼p(a|q). Finally,\nthe referee receives their answers and verifies them against the rules λ(a|q).\n2\nBackground\nA nonlocal game of N players G = (Q1, ..., QN, A1, ..., AN, λ) (illustrated in Fig. 1) consists\nof a set of possible questions Qj that player j receives from a referee and a set of answers Aj\nthat player j is allowed to send to the referee, which the referee then evaluates against a rule\nfunction λ : Q1 × ... × QN × A1 × ... × AN →{0, 1}. Each set of questions Qj and the set of\nanswers Aj has cardinality mj and kj, respectively; however, in our work, we assume that there\nare m questions and k answers for each player. The game proceeds in the following steps:\n1. The players are informed about the rules of the game λ, and can collaborate to create\na joint strategy, modeled as a conditional probability density between the questions and\nanswers, to maximize their chances of satisfying the rules of the game before it starts.\n2. Players are then separated or isolated to prevent them from communicating.\nThis is\nreferred to as non-signaling, or in other words, each player’s actions are independent of\neach other.\n3. The referee tests the strategy by asking questions to each player q = [q1, ..., qN] and\nreceiving their responses a = [a1, ..., aN], where qi ∈Qi and ai ∈Ai, respectively.\n4. The players win a round if λ(a|q) = 1, and lose if λ(a|q) = 0. Multiple rounds are played\nwith different questions to establish that players have a valid strategy.\nIt is common that all players share the same set of possible questions Q and answers A. In\nparticular, Synchronous games have rules that require that the answers of two (or more) players\nbe identical when asked the same questions λ(a1, . . . , ai, aj, . . . , aN|˜q) = 0, for all ai ̸= aj, where\n˜q is a vector of questions. In our work, we only consider computing strategies for synchronous\ngames, although the optimization procedure we propose in Section 3 applies for more general\nstrategies.\n3\nUsing the rules, we can define the value of the game as\nV (G) =\nX\nqa\nλ(a|q)p(q)p(a|q),\n(1)\nwhere the sum is taken over all possible values of q ∈QN and a ∈AN (we drop the vector\nnotation for convenience). The distribution p(q) of the questions asked is typically chosen to\nbe uniform, and the behavior p(a|q) is determined by the strategy that the players construct.\nNotice that this is the only term that players can control to maximize their win rate. A strategy\nis said to be perfect if λ(a|q) = 0 =⇒p(a|q) = 0 and, consequently, V (G) = 1.\nClassical strategies consist of a lookup table that indexes each player’s response to a particular\nquestion.\nIt suffices to consider deterministic strategies since stochastic strategies involving\nshared randomness between the players cannot outperform deterministic strategies due to the\nlinearity of the value of the game [15].\nSuppose that players share a quantum state |ψ⟩∈⊗iHi, and each player has a set of positive\noperator-valued measures (POVMs) with elements of the form {Ma|q}, which they perform\non their subspace. Using this setup, players can generate the following conditional probability\ndensity,\np(a|q) = Tr\nh\nρ\n\u0010\n⊗iMai|qi\n\u0011i\n,\n(2)\nwhere ρ = |ψ⟩⟨ψ|. These densities are known as Quantum Strategies.\nIn addition to the above definition of a quantum strategy, there are a variety of competing\ndefinitions for a quantum strategy depending on the choice of axioms that describe how joint\nmeasurements between two parties should be performed [36]. In our work, we will only con-\nsider strategies as defined above, but the study of quantum strategies is a very active area of\nresearch [30, 29, 44, 38]. Note that in [7], the authors prove that for synchronous games, a\nmaximally entangled state is sufficient for a quantum strategy to win the graph coloring game.\nA game exhibits a quantum advantage if there exists a quantum strategy that performs better\nthan the best possible classical strategy, in which case there is a Bell inequality I that is violated\nfor some quantum strategies. The inequality has historically served as an experimental test of\nlocal realism [14]. Such inequalities are constraints satisfied by classical (local hidden-variable)\nmodels, and are often linear inequalities derived from the local realism assumption.\nMore\nspecifically, a Bell inequality consists of a function I with respect to the probabilities {p(a|q)}\nsuch that\nI({p(a|q)}) ≤ξ,\n(3)\nfor some ξ ∈R.\nBell inequalities are a central object for self-testing of states [53].\nThe\nconstruction of such functions I and constants ξ are as follows: for a given Bell inequal-\nity I = P\nq,a wq,ap(a|q), where wq,a are weights, there is a corresponding Bell operator B =\nP\nq,a wq,a\nN\ni\nMai|qi, such that a violation is obtained as ξ = Tr(Bρ). If the maximal achievable\nviolation is obtained by using quantum resources, denote ξQ for this distinction and consider\nthe shifted Bell operator ξQ1 −B. If the shifted Bell operator admits a decomposition\nξQ1 −B =\nX\nγ\nP †\nγPγ,\n(4)\n4\nwhere each Pγ is a polynomial with respect to the measurement operators {Mai|qi}, then we\ncall the decomposition a sum of squares (SOS) for the Bell inequality. Such a decomposition is\nextremely hard to find [33].\n3\nMethod\nIn this section, we present a variational quantum algorithm for computing quantum strategies\nof nonlocal games. Let |ψ⟩be the shared entangled state between the players and Ma|q =\nN\ni Mai|qi be the joint POVM applied to that state for question q, returning a with probability\np(a|q) = ⟨Ma|q⟩. It was noted in [5] that fixing these measurement operators gives a Hamiltonian\nwhose ground state is the optimal shared state for this measurement setting. This fact has been\nused with reinforcement learning to optimize measurements while selecting the optimal shared\nstate through exact diagonalization [4].\n3.1\nDual-Phase Optimization\nOur approach is a dual-phase optimization (DPO) that alternates between 2 phases: preparing\nthe optimal state |ψ⟩for the fixed measurements {Ma|q}, and optimizing the measurements,\nwhile fixing the shared state.\nWe assume that the players parameterize their state |ψ⟩→\n|ψ(θ)⟩and POVMs Ma|q →Ma|q(ϕ). The particular choice of parameterization depends on\ncharacteristics of the game (e.g. number of qubits required depends on the number of answers).\nAlgorithm 1 Dual-Phase Optimization\nInitialize ϕ randomly\nwhile ∆⟨H⟩> ϵ do\nConstruct H(ϕ)\n|ψ(θ)⟩←V QE(H(ϕ))\nϕ ←GD(⟨H(ϕ)⟩)\nend while\nThe preparation of the Hamiltonian depends on the specific measurement scheme the players de-\ncide on, which depend on the game. Later, we outline a method for constructing a Hamiltonian\nfrom arbitrary game rules λ and measurements {Ma|q}.\nThe optimal shared state is prepared in the first phase using any VQE procedure V QE(·).\nHere, we choose ADAPT-VQE [24] because it generates compact variational circuits for use on\nnear-term quantum hardware, but any other solver can also be used (see Appendix B). The\nreference state |ψ0⟩can be a product state, e.g. |0⟩, |+⟩. We choose a qubit operator pool\nconsisting of all possible Pauli strings P acting on the entire system. The operators added to\nthe state take the form eiθP , giving |ψ(θ)⟩=\n1Q\nj=N\neiθjPj |ψ0⟩, and they are capable of generating\nthe entanglement required to win nonlocal games, provided they act non-trivially on at least 2\nqubits.\nThe second phase uses a gradient descent-like optimizer GD(·) to update the measurement\n5\nparameters ϕ. This requires the calculation of gradients ∇ϕ ⟨ψ(θ)|H(ϕ)|ψ(θ)⟩on the quantum\ndevice, which can be done through parameter shift rules [42, 50]. In Appendix E, we outline the\ncost of computing this gradient for larger problem instances and some optimization challenges.\n3.2\nGame Hamiltonians\nAs mentioned above, DPO requires the construction of a Hamiltonian based on the measure-\nments of the players, which determines the quantum strategy. Player i may measure their qubits\nρi in an arbitrary basis depending on the question, leading to a form for the measurement op-\nerators\nMa|q =\nO\ni\nU†\niqiPaiUiqi\n(5)\n= U†\nq PaUq,\n(6)\nwhere Pai = |ai⟩⟨ai| is the projector onto answer ai, and Uiqi acts on ρi in response to question\nqi. Because ⟨Ma|q⟩= p(a|q), we can substitute this into (1) to construct the game operator\nβ =\nX\nqa\nλ(a|q)p(q)Ma|q\n(7)\n=\nX\nq\np(q)U†\nq\n X\na\nλ(a|q)Pa\n!\nUq\n(8)\nwith the property ⟨β⟩= V (G). A VQE finds the ground state of a Hamiltonian, so to maximize\nthe win rate, we use a value Hamiltonian H = −β in DPO.\nProposition 3.1. The value ⟨β⟩= 1 if and only if the players have a perfect quantum strategy,\notherwise ⟨β⟩< 1.\nProof 3.1.\nWe show this by first computing the value for a perfect strategy and then for an\nimperfect strategy. Let I = {(q, a) | ∀q, a λ(a|q) = 0} be the set of question-answer pairs for\nwhich the strategy violates the rules, and let P = Ic be its complement, the set of correctly\nanswered question-answer pairs.\nFor a perfect quantum strategy, I = ∅and P = QN × AN, therefore we get\n⟨β⟩=\nX\nq,a∈P∪I\np(q)λ(a|q) ⟨Ma|q⟩\n=\nX\nqa∈P\np(q, a) + (0)\nX\nqa∈I\np(q, a).\nSince I = ∅for all q, a pairs for which λ = 1 are contained in P, it follows that the joint\nprobability density in the left term must sum to 1. Hence, we obtain ⟨β⟩= 1.\nA very similar line of reasoning holds for an imperfect strategy, where I ̸= ∅. Reusing the above\nexpression,\n⟨β⟩=\nX\nqa∈P\np(q, a) + (0)\nX\nqa∈I\np(q, a)\n=\nX\nqa∈P\np(q, a) < 1,\n6\nsince for p(q, a), q, a ∈P no longer contains the full probability density as I contains some\npossible pairs. We conclude that ⟨β⟩≤1, with ⟨β⟩= 1 iff a strategy is perfect.\n□\nTo parameterize this Hamiltonian for DPO, a general single-qubit unitary U1 may be decom-\nposed into 3 parameters, leading to a parameterization of the full measurement gate Uq =\nN\ni,ji U1(ϕiqiji), where i indexes the player, qi indexes the question, and ji indexes the partic-\nular qubit of player i. In measurement layers acting on multiple qubits, we expand each entry\nof ϕiqiji to be the concatenated vector of parameters for all gates applied to that qubit, i.e.\nUq = N\ni UNi(ϕiqi), where UNi is an Ni-qubit unitary.\n4\nExperiments\nTo evaluate the performance of DPO, we apply it to two nonlocal games with known quantum\nbounds: CHSH and the N-partite symmetric (NPS) game [4]. Then, we use DPO to explicitly\ncompute an optimal quantum strategy for the coloring game of a 14 vertex graph called G14 [39].\nThis strategy is then evaluated on quantum hardware, demonstrating that it can be used to\nbenchmark the nonlocal capabilities of quantum devices and find sources of errors.\n4.1\nCHSH\nThe Clauser-Horne-Shimony-Holt (CHSH) scenario [14] is the simplest nonlocal game that ad-\nmits a quantum advantage. CHSH features 2 players, Alice and Bob, who each receive a question\nqi ∈Q = {0, 1}, answering ai ∈A = {0, 1}. The inequality operator can be expressed in the\nfamiliar form I = A0B0 + A1B0 + A0B1 −A1B1 following the rules\nλ(a|q) =\n(\naa ⊕ab,\nif qa = qb = 1\nδaa,ab,\notherwise\n(9)\nand making the substitution λ = 0 →λ = −1 in (8). Here Aq denotes Alice’s measurement\noperator and likewise Bq for Bob. All classical strategies are bounded by ⟨I⟩≤2, whereas\nquantum strategies can violate this up to ⟨I⟩= 2\n√\n2. i.e., from Equation (3), the violation\noccurs with ξQ = 2\n√\n2.\nIt suffices to share a Bell state and then perform the appropriate\nsingle-qubit measurements.\nWe applied DPO to the CHSH game using Ry(ϕ) = e−i ϕ\n2 Y gates as the Uq operators.\nIn\nthe first iteration, ADAPT chose |ψ(θ)⟩= ei π\n4 Y X |00⟩, which correctly generates a Bell state\n|Φ−⟩. In the second phase of that iteration, the measurement parameters were optimized to\nϕ ≈[0, −π\/2, π\/4, −π\/4] by constraining ϕa0 = 0, giving the optimal inequality value 2\n√\n2.\n7\n0\n10\n20\n30\n40\n50\n60\nIteration\n0.2\n0.1\n0.0\n0.1\nInequality Value\nClassical\nQuantum\n0\n10\n20\nNo. Trials\n19\n1\n1\n2\n3\n2\n1\n21\nFigure 2: Trials of DPO on NPS for N = 6. (Left) Trajectory of all 50 trials. Negative inequality values\nare not reachable with classical states. (Right) Distribution of the final inequality values. Despite the non-\nconvexity of the problem, many trials still reach the optimal value.\n4.2\nN-partite Symmetric\nThe NPS scenario [54] involves the correlations between players N, each receiving a binary\nquestion qi ∈{0, 1} and returning a dichotomic answer ai = ±1. The inequality is expressed in\nterms of one- and symmetric two-body correlators,\nSq =\nX\ni\n⟨Miq⟩, Sqq′ =\nX\ni̸=j\n⟨MiqMjq′⟩,\n(10)\nwhere measurement Miq = U†\niqZiUiq. The classical bound on the correlations is\nI = −2S0 + 1\n2S00 −S01 + 1\n2S11 + 2N ≥0,\n(11)\nwith negative values only achievable with quantum strategies [4].\nWe tested 50 DPO trials for the N = 6 case (Fig. 2). Our algorithm encounters some local\nminima, particularly at the classical bound of I = 0, but still succeeds in 19\/50 attempts\nreaching I = −0.258 as found in [4] as well.\nAdditionally, 29\/50 of the trials violated the\nclassical bound.\n4.3\nChromatic Number Game\nThe objective of the chromatic number game [39] is to color a graph G = (V, E) in such a way\nthat adjacent vertices are never given the same color. If this can be done using c colors, we call\n8\nthis a c-coloring of the graph. It has been shown recently that winning strategies for this game\ngenerate the set of all possible correlations for synchronous nonlocal games [26]. This differs\nfrom the other nonlocal games we mentioned, as the sets of questions and answers are much\nlarger, and each player requires more qubits to encode their answer. The referee asks a question\nq = [va, vb] ∈V ∪E, and the players respond with colors a = [ca, cb], ci ∈[c]. The rules are\ngiven by\nλ(a|q) =\n(\nδca,cb\nif va = vb\n(1 −δca,cb)\nif va ̸= vb\n.\n(12)\nThe graph G14 in Fig. 3 admits a quantum advantage since there exists a perfect quantum\nstrategy with 4 colors, while the smallest possible coloring strategy classically requires 5. The\nnotion of finding the smallest possible coloring strategy classically is equivalent to finding the\nchromatic number of this graph [45]. G14 was discovered in [39] and was conjectured to be\nthe smallest possible graph with a quantum advantage for this game. This was subsequently\nproven to be the case [35]. The authors construct a strategy using an orthogonal representation\nof G14 in R4, but it is not clear how to obtain an explicit set of ans¨atze from the representation\nto construct a short-depth circuit for the strategy. We use the DPO algorithm to generate a\nperfect (up to numerical precision error) quantum strategy that achieves χq(G14) = 4.\nFigure 3: Picture of the G13 graph. To get the graph G14 just add an apex vertex, vertex connected to all\nthe other vertices, α to this graph. Image courtesy of [39].\nIn [39], quantum c-colorings start with a c-dimensional orthogonal representation of the graph,\nwhere adjacent vertices are assigned orthogonal vectors.\nAn orthogonal representation of a\ngraph is an assignment complex unit vectors of a fixed dimension to vertices of the graph,\nwhere adjacent vertices receive orthogonal vectors. These vectors are then used to define the\nmeasurement operators for the quantum strategy to color the graph. Given that this quantum\nstrategy is built from projections, it is unclear if the unitaries produced by DPO are related to\nthese projections. Further investigation, outside the scope of this work, is required to answer\nthis question, potentially utilizing a linear combination of unitaries to decompose the projective\nmeasurements into unitaries than can then be executed on hardware [12].\nFor convenience, we denote a vertex question [v, v] as v, and an edge question [va, vb] ∈E as e.\n9\nLet the answers also be given by c = [c1, c2]. Evaluating the expression in (8) gives\nβ =\n1\n|Q|\n\"X\nv\nU†\nvPccUv +\nX\ne\nU†\ne(I −Pcc)Uv\n#\n(13)\n=\n1\n|Q|\n\"X\nv\nU†\nvPccUv −\nX\ne\nU†\nePccUv + |E|\n#\n,\n(14)\nwhere Pcc = P\nc |cc⟩⟨cc| is the projector onto the space of equal colors (i.e. p(c1 = c2|q) =\n⟨U†\nq PccUq⟩), and |Q| = |V | + |E|. Intuitively, the first term maximizes p(c1 = c2|v), and the\nsecond term maximizes p(c1 ̸= c2|e) = 1 −p(c1 = c2|e). Note that to ensure that all possible\nquestions are asked to each player, E contains both edges (v1, v2) and their reverse (v2, v1).\nTo further simplify the search for strategies, we restrict the players to 2 qubits each, since 2\nqubits suffice to represent c ∈{0, ..., χq −1} using a binary encoding. We also impose a known\nconstraint on an optimal strategy [39]: Bob’s measurement operators are complex conjugate\nto Alice’s, halving the number of measurement parameters ϕ required. We use a measurement\nlayer per player of general single-qubit U gates, a CNOT from qubit 0 to qubit 1, and Ry gates\napplied to each qubit, resulting in 8 parameters per question or 112 in total (in our code, this\nis the U3Ry layer).\nWe classically simulated 500 trials of DPO, achieving a minimum energy of E = −1.0000. We\nremove the gates added by ADAPT with |θi| < 10−4.\nThe corresponding circuit was then\nconverted into a Qiskit circuit (Fig. 4), and the evaluation using the classical AerSimulator\nconfirmed a 100% win rate (Fig. 6a). The 112 parameters ϕ can be found in our code repository\n(see Appendix A).\nFigure 4: Generated circuit for G14. The initial state |+⟩⊗N is prepared, then ADAPT added the operators\nY0Z2 and Y0Z1Y2Y3, giving the shared state |ψ(θ)⟩. The remaining gate layers along with the 112 mea-\nsurement parameters ϕ constitute the measurement strategy. We only adaptively added circuits on the state\npreparation and fixed the measurement scheme in this case.\nIt is worth noting that in a nonlocal game the referee cannot cross-check answers from previ-\nous questions (otherwise the graph would not be 4-quantum colorable), and the players change\ntheir coloring for each vertex probabilistically in subsequent runs, using the entanglement to\ncoordinate their answers as required. For example, when asked q = [A, A] multiple times, the\nresponses are nearly uniform among [χq] but always match. Furthermore, we found that mea-\nsurement layers consisting of only single-qubit gates were insufficient and generated imperfect\nstrategies at E = −0.9921. In these cases, we frequently observed that the errors consisted of a\ncyclic path through some graph edges.\nThe operators chosen by ADAPT are nonlocal as expected, acting on 2 and 4 qubits. The\n10\nshared state discovered,\n|ψ(θ)⟩= e−i π\n4 Y ZY Y ei π\n4 Y IZI |+⟩⊗4\n(15)\n= 1\n2H⊗4 X\nc∈[χq]\n|cc⟩,\n(16)\nis the maximally entangled state followed by local Hadamard gates. This matches the existing\nstrategy described in [39], which leverages the maximally entangled state, up to local unitary\noperations.\nThe circuit preparing the shared state |ψ⟩needs 8 CNOT gates to be transpiled using a ladder-\nlike formulation with CNOT gates applied between nearest-neighbor qubits. This can be reduced\nto 2 CNOT gates (Fig. 5) by noting that the state 1\n2\nP\nc |cc⟩can be generated with transversal\nBell pairs shared between the players on qubits a0, b0 and a1, b1.\nApplying the transversal\nHadamards H⊗4 in (16) flips the direction of the CNOT gates using a simple circuit identity.\nWe refer to this version of the shared state circuit as the “Bell pair” strategy, which uses the\nsame measurement layer and parameters as the original strategy.\n𝐻\n𝐻\n𝐻\n𝐻\n𝑒𝑖𝜃0𝑌𝐼𝑍𝐼\n𝑒𝑖𝜃1𝑌𝑍𝑌𝑌\n𝑎0\n𝑎1\n𝑏0\n𝑏1\n𝜋\/4\n−𝜋\/4\n𝐻\n𝐻\nFigure 5: Simplification of the G14 state preparation. (Left) The original strategy using the gates found with\nthe adaptive procedure. (Right) The “Bell pair” strategy, an equivalent circuit producing two independent\nBell state pairs between the players. This requires just 2 CNOT gates compared to the 8 required for the\noriginal strategy.\n4.4\nExperiments on IBM Devices\nThis strategy was submitted to 11 IBM quantum devices with 4 or more qubits (Fig.\n6).\nA decrease in performance was observed on IBM quantum devices compared to the classical\nsimulation due to noise, particularly affecting the success rate of vertex questions (Fig. 6b, 7).\nThere are several possible sources of noise:\n1. Vertex questions are more sensitive to bit flips, as any 1-bit error will result in the answer\nviolating the rules Xj |cc⟩→|cacb⟩, while the same is not true for edge questions, since\nbit flips may not necessarily make the answers agree Xj |cacb⟩̸→|cc⟩(see Section 6). This\nasymmetry comes from the rules of the game.\n2. As the resource state depends on entanglement, error in entangling 2-qubit gates disrupts\nthe strategy.\n3. Circuit transpilation to hardware with fixed qubit connectivity further incurs two-qubit\ngate overhead. If SWAP gates are required these are commonly decomposed by 3 CNOT\ngates.\n11\nA\nB\nC\nL\nM\nN\nP\nQ\nR\nW\nX\nY\nZ\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\n(a) Qiskit AerSimulator\nA\nB\nC\nL\nM\nN\nP\nQ\nR\nW\nX\nY\nZ\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\n(b) IBMQ ibm hanoi\nFigure 6: Win rate of the original G14 strategy by question. a) Classical statevector simulation with 10000\nshots per question. b) Performance on a quantum device with 1024 shots per question.\n0.4\n0.6\n0.8\n1.0\nWin Rate\nMumbai\nHanoi\nCairo\nAuckland\nGuadalupe\nNairobi\nJakarta\nQuito\nManila\nLima\nBelem\n0.72\n0.68\n0.64\n0.65\n0.90\n0.89\n0.88\n0.88\n0.63\n0.65\n0.88\n0.88\n0.51\n0.83\n0.63\n0.65\n0.83\n0.68\n0.88\n0.88\n0.94\n0.89\nDevice Qubits\n5\n7\n16\n27\n(a) All devices\n10\n4\n10\n3\n10\n2\n10\n1\nCX Error perr\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWin Rate\nReported\n7.39e-03\nFitted\n1.21e-02\nVertex\nEdge\nHanoi Data\nPredicted CX Error\n(b) Noise Simulation\nFigure 7: a) Average performance of the strategy on quantum devices grouped by question category, either a\nvertex q = [v, v] or an edge q = [v1, v2] (vertex winrate on the left, edge winrate on the right). The number\nof device qubits is reported to distinguish processor types; the circuit was executed on 4 qubits. b) Classical\nsimulation of the circuit with random Pauli noise applied to CNOT gates with probability perr. The circuit\nwas transpiled to the basis gates and coupling map of ibm hanoi, and the observed data are fitted to the\ncurves by maximizing the probability of observing the data assuming a normal distribution. The estimated\nerror for CNOT gates is higher than reported, since our simulation did not account for measurement readout\nor decoherence errors.\n12\nThis sensitivity suggests that measuring the win rate of the strategy for G14 is a good benchmark\nto evaluate the ability of a quantum device at accurately controlling for bit flip errors, while\nsimultaneously performing nonlocal operations. In particular, the vertex question win rate is\nvery sensitive to noise, measuring the fidelity of the device gates, whereas the edge question\nwin rate can confirm if a device is using quantum resources. The optimal classical strategy of 4\ncolors consists of a 4-coloring of G13 and assigning the most infrequently used color to the apex\nvertex α. Therefore, all vertex questions would be correctly answered and one edge would be\nincorrectly answered, resulting in an edge win rate of 36\/37 ≈97.3%, or an overall win rate of\n86\/88 ≈97.7%. Any win rate higher than this requires quantum resources. In our experiments,\nno device exceeded this threshold (Fig. 7). However, introducing an error-correcting version\nof our quantum strategy could improve the robustness of this test, which we leave to future\ninvestigations.\n5\nNonlocal Games as Quantum Hardware Benchmarks\nNonlocal games with perfect strategies can serve as hardware benchmarks by assessing and ana-\nlyzing the empirical win rates when executed on near-term hardware. Under certain assumptions\nabout the structure of quantum noise, Nonlocal games can exhibit quantum advantage in shal-\nlow circuits, even with noisy qubits [6]. The ‘noisy entanglement’ generated in shallow circuits\nenables correlations that classical circuits fundamentally struggle to reproduce. This is seen in\n[6]: their classical circuits of constant depth cannot simulate the long-range correlations.\nIn this section, we demonstrate the effectiveness of this benchmark by analyzing hardware noise\nand its strong correlation to strategy performance. We proceed backwards,from the unentangled\nreadout measurements, to the independent Bell state measurements, to the initial entangled\nresource state preparation. By investigating the effects of hardware noise on the empirical win\nrates we seek to establish: a) which questions are most affected by noise, b) which components\nof the circuit are most affected by noise, and finally, if classical correlations, or quantum noise,\ncould be misinterpreted as a winning quantum strategy.\nIn addition to the classical bounds provided in Section 4.4, we also consider the worst outcome\non hardware: a nearly uniform distribution over all bitstrings. This would skew the win rates\nin the G14 game as follows: for any vertex question would only be 1\/4 = 0.25 and the average\nwin rate of any edge question would be 12\/16 = 0.75. Thus random guessing would return an\noverall win rate of 59%. In Figs. 10, 11, 12 and 13 we include these values as a reference.\nQuantum hardware is affected by many sources of noise. The noise profile is time dependent\nand there are many strategies developed to optimize the scheduling and execution of quantum\ncircuits.\nUsing superconducting qubit platforms from IBM and Rigetti, we investigate the\nrobustness of the original G14 strategy, and the Bell pair strategy on superconducting qubit\nplatforms with respect to hardware noise fluctuations over several days, and also to changes in\nthe circuit made during the transpiration step.\n13\n5.1\nTheoretical Noise Robustness\nThere is an asymmetric sensitivity to noise between the vertex and edge questions due to the\ngame rules (see Section 4.4). Furthermore, there is variance in the edge questions performance.\nWe hypothesize this arises from the particular strategy and distribution of answers found via the\nADAPT algorithm. We further investigate the effects of bit-flip errors on the game strategies.\nMultiple bitstrings satisfy the constraints for edge questions λ(c, c|vA, vB) = 0 for all colors c.\nPlayers using the four qubit strategy can win an edge question by outputting a bit string that\nis either Hamming distance H(a, b) = 1 from matching (e.g. 0001) or distance 2 (e.g. 1100).\nWhile both options satisfy the game rules, choosing bitstrings with a greater Hamming distance\nreduces the likelihood of losing due to device noise, as higher-weight errors occur less frequently.\nTo quantify the noise robustness of the strategy resulting from this, we introduce the expected\nHamming distance (EHD),\nEHD(vA, vB) = EcA,cB∼p(cA,cB|vA,vB) [H(cA, cB)] ,\n(17)\nwhere H(a, b) denotes the Hamming distance between the binary representations of answers a\nand b. In general, the EHD is not efficiently computable on a classical computer since it requires\nsampling the strategy. However, because the G14 strategy is sufficiently small, we calculate the\nEHD for each circuit via classical simulation (Fig. 8).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\nFigure 8: (Left) Noise robustness of the G14 strategy. The adjacency matrix of the G14 graph is shown with\nthe color scale denoting the EHD. Vertex questions form the main diagonal. (Right) Average performance on\nibm sherbrooke over 7 runs plotted for comparison (same as in Fig. 9).\nTo show that the EHD effectively predicts question performance, we also plot results collected\non ibm sherbrooke1, a superconducting qubit platform available from IBM with 127 qubits.\nWe executed the strategy described in Section 4.3 multiple times over the course of a week.\nSupplemental experimental details are available in the Appendix F. The heatmaps exhibit a\nhigh degree of correlation (r = 0.8812, p < 0.001), suggesting the strategy produced greatly\ninfluences noise robustness. The standard deviation is also presented alongside the win rate (Fig.\n9), further highlighting the sensitivity of different questions to variations in device calibration.\nThere are some outliers, particularly (0, 2) and (2, 0) that perform worse than expected, and\nthe EHD cannot account for variation in the vertex question performance. We leave these to\nfuture investigations.\n1Eagle r3 processor\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\nFigure 9: (Left) Win rate of the Bell pair strategy averaged over 7 separate experiments on ibm sherbrooke.\nAll runs used the same hardware qubits and layout, forming a linear chain. Because the experiments were\nspaced out, the calibration parameters differed between each trial. (Right) Standard deviation of each question\nfor those experiments. This shows how sensitive each circuit in the strategy is to fluctuations in the device\nparameters.\nWe executed circuits for the original G14 strategy and the Bell pair strategy on Rigetti’s\nAnkaa-2, and Ankaa-3 devices. Both have square lattice qubit connectivity and to take ad-\nvantage of this, we prioritized running experiments on qubit subsets with cyclic connectivity.\nThe circuits first constructed in Qiskit are exported to Open Quantum Assembly Language\n(QASM) [17], then imported and compiled into Quil using the qiskit-rigetti plugin [49].\nDuring compilation into native operations that are executable on the Rigetti platforms, circuit\noptimization is possible.\n5, 12, 13, 6\n17, 16, 9, 10\n30, 29, 22, 23\n24, 25, 18, 17\n25, 24, 17, 18\n12, 5, 6, 13\n16, 17, 10, 9\n29, 30, 23, 22\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWin Rate\nVertex Questions\nPARTIAL\nNAIVE\nNOT SPECIFIED\nrandom guessing\n12, 5, 6, 13\n5, 12, 13, 6\n30, 29, 22, 23\n24, 25, 18, 17\n16, 17, 10, 9\n17, 16, 9, 10\n29, 30, 23, 22\n25, 24, 17, 18\nEdge Questions\nPARTIAL\nNAIVE\nNOT SPECIFIED\nrandom guessing\nFigure 10: (Left) All vertex question win rates of the original G14 strategy grouped by hardware qubits used\non Ankaa-2 from Rigetti. (Right) Edge question win rate of the original G14 strategy grouped by hardware\nqubits used on Ankaa-3 from Rigetti.\n15\nThe compiled circuit can be further optimized through rewiring directives that determine how\nprogram qubits are mapped onto hardware qubits.\nThe NAIVE rewiring uses the program\nqubit register index as the hardware qubit index. This rewiring may require the use of addi-\ntional operations to mitigate non-neighboring interactions. The PARTIAL rewiring attempts\nto optimize the mapping between program and physical qubits to optimize the fidelity of the\ncompiled circuit. We specified the rewiring strategy through the use of pre-compilation hooks.\nIf no hooks were specified by the user, the rewiring strategy was not verified and we denote the\nstrategy as (NOT SPECIFIED).\n13, 12, 5, 6\n17, 16, 10, 9\n25, 24, 18, 17\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWin Rate\nVertex Questions\nPARTIAL\nNAIVE\nNOT SPECIFIED\nrandom guessing\n13, 12, 5, 6\n17, 16, 10, 9\n25, 24, 18, 17\nEdge Questions\nPARTIAL\nNAIVE\nNOT SPECIFIED\nrandom guessing\nFigure 11: (Left) Vertex question win rate using the Bell pair strategy averaged over multiple experiments on\nRigetti’s Ankaa-2. The runs used different hardware qubits and wiring strategies. (Right) Edge question win\nrate using the Bell pair strategy averaged over multiple experiments on Rigetti’s Ankaa-3.\n5.2\nNoise Robustness of Game Components\nIn this section we analyze how hardware noise affects different nonlocal game circuit compo-\nnents, supported by results collected on superconducting qubit platforms. This extends the\nsimulated noise results shown in Fig. 7b where the error rate of two-qubit gates was inferred\nfrom the hardware results reported in Section 4.4. We supplement these results with additional\nexperiments designed to characterize key components of the strategy: readout measurement\nerror, independent entangled measurements, and imperfect resource state preparation (shown\nin Fig. 14). Throughout this section we analyze and characterize each element individually.\nWe determine the effective win rate that would be observed by the players if one of these\nelements failed or was replaced by randomness and use this to demonstrate the effectiveness\nof nonlocal games as a hardware benchmark. The readout measurement error can be charac-\nterized by a 2n × 2n dimensional matrix constructed row-wise from individual computational\nbasis state preparation and measurement: preparing the register in |0⟩⊗n, applying X-gates,\nand projecting the final state onto the computational basis. This can be used to estimate bit\nflip error probabilities (independent or correlated) [25], and also can be leveraged for readout\n16\n5, 2, 1, 4\n2, 5, 4, 1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWin Rate\nVertex Questions\nNOT SPECIFIED\nNAIVE\nrandom guessing\n5, 2, 1, 4\n2, 5, 4, 1\nEdge Questions\nNAIVE\nNOT SPECIFIED\nrandom guessing\nFigure 12: (Left) All vertex question win rates of the original G14 strategy grouped by hardware qubits used\non Ankaa-3 from Rigetti. (Right) Edge question win rate of the original G14 strategy grouped by hardware\nqubits used on Ankaa-3 from Rigetti.\n5, 2, 4, 1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWin Rate\nVertex Questions\nNOT SPECIFIED\nNAIVE\nrandom guessing\n5, 2, 4, 1\nEdge Questions\nNOT SPECIFIED\nNAIVE\nrandom guessing\nFigure 13: (Left) Vertex question win rate using the Bell pair strategy averaged over multiple experiments on\nRigetti’s Ankaa-3. The runs used different hardware qubits and wiring strategies. (Right) Edge question win\nrate using the Bell pair strategy averaged over multiple experiments on Rigetti’s Ankaa-3.\nerror mitigation 2. We collected data to characterize the readout error on ibm sherbrooke and\nRigetti’s Ankaa-2, and Ankaa-3 platforms. In Fig. 15 we plot the Ankaa-2 and Ankaa-3 results\nto emphasize the connection to the EHD metric (see Section 5.1). Though the circuits executed\non the hardware are very shallow, SPAM error can have a significant impact.\nConnecting the SPAM error back to the EHD if a nonlocal circuit was correctly executed, and\nthe only errors occurred during the readout stage, in Fig. 15 we see that vertex questions are\nmore likely to return incorrect answers, while for edge questions, correct answers can still be\n2The results we report do not include readout error mitigation, we reserve this for future work.\n17\n|Ψ⟩\n𝒰!\n𝒰\"\nM\nM\nM\nM\nFigure 14: The components of the nonlocal game circuit that we characterize: resource state (purple),\nindependent Bell basis measurements (orange), and readout error (grey).\nreturned with high probability. For vertex questions, the all-zero bitstring is relatively unaffected\nby errors during the readout measurement step, in contrast to the remaining three bitstrings.\nFor edge questions, bit-flip errors that occur during the readout step can still return valid edge\nquestion bitstrings. The edge question in which the probability of erroneously returning a non-\nvalid bitstring are bitstrings with high Hamming weights. Thus, if a state is correctly prepared\nand the error only occurs during the readout stage, it affects vertex questions and low-weight\nedge answers. The full quantum strategy is composed of multiple circuits needed to evaluate\nthe players’ performance on all questions posed by the referee. The construction assumes that\nthe two players are separated in space to prohibit classical communication, and implementing\nthe strategy requires nonlocal operations. Prior to the final qubit readout, the two players\nimplement entangled unitaries (UA ⊗UB) that are assumed to be independent. We assess the\nability of each player to apply these entangled measurements with high fidelity independently,\nand simultaneously without corrupting each others operations. This is tested on four qubits\n[46, 47, 48, 49] connected in a linear chain (see Appendix F). A specific Bell state is prepared by\napplying UA ⊗1 ⊗1 or 1 ⊗1 ⊗UB– only two qubits prepare a Bell state while the other two\nqubits remain in the |0⟩state. Then, a Bell basis measurement is applied to the prepared Bell\nstate and the remaining two qubits are measured in the computational basis. This is compared\nto the preparation of two independent Bell states both measured by Bell state measurement.\nTo amplify the gate noise we construct and execute these circuits with basic unitary folding by\ninserting pairs of CNOT gates.\nThe general success probabilities are plotted in Fig. 16. For the single Bell state preparations,\nwe extract the marginal distributions of each subset and plot the mean probability of observing\ncounts of each Bell state. The mean is evaluated using 14 executions of these experiments on\nibm sherbrooke. The distinct separation between the success probabilities of isolated Bell state\npreparation either on [qa, qb] or [qc, qd] could be caused by individual two qubit gate error rates\n– indicative that a coupler between particular pairs of qubits could be more stable compared\nto neighboring qubits. Another cause could be the choice of hardware qubits combined with\ncircuit optimization options (see Appendix F).\nOn Ankaa-2 we prepared the state |Ψ+⟩⊗|0⟩⊗|0⟩and observe that over 75% of the observed\nbitstrings correspond to the correct Bell state. The highest number of counts are returned in\nthe all-zero bitstring, indicated that the state was prepared correctly and measured correctly\nwhile the idle qubits remained idle. Preparing the state |0⟩⊗|0⟩⊗|Ψ+⟩we observe that between\n71-72% of the observed bitstrings correspond to the correct Bell state. However, preparing and\nmeasuring the state |Ψ+⟩⊗|Ψ+⟩showed a sharp decline in counts observed in the expected\n18\nedge\nvertex\nObserved\n0000\n0101\n1010\n1111\ntarget\n0.2124\n0.7876\n0.6279\n0.3721\n0.4165\n0.5835\n0.3428\n0.6572\nVertex Question Bitstrings\nedge\nvertex\nObserved\n0001\n0010\n0011\n0100\n0110\n0111\n1000\n1001\n1011\n1100\n1101\n1110\ntarget\n0.8530\n0.1470\n0.8359\n0.1641\n0.9683\n0.0317\n0.9141\n0.0859\n0.9717\n0.0283\n0.8916\n0.1084\n0.8481\n0.1519\n0.7822\n0.2178\n0.8657\n0.1343\n0.9824\n0.0176\n0.6978\n0.3022\n0.9170\n0.0830\nEdge Question Bitstrings\n0.3\n0.4\n0.5\n0.6\n0.7\n0.2\n0.4\n0.6\n0.8\n(a) Rigetti Ankaa-2\nedge\nvertex\nObserved\n0000\n0101\n1010\n1111\ntarget\n0.0376\n0.9624\n0.1206\n0.8794\n0.1406\n0.8594\n0.1890\n0.8110\nVertex Question Bitstrings\nedge\nvertex\nObserved\n0001\n0010\n0011\n0100\n0110\n0111\n1000\n1001\n1011\n1100\n1101\n1110\ntarget\n0.9473\n0.0527\n0.8838\n0.1162\n0.9956\n0.0044\n0.9502\n0.0498\n0.9893\n0.0107\n0.8682\n0.1318\n0.9204\n0.0796\n0.9932\n0.0068\n0.9517\n0.0483\n0.9932\n0.0068\n0.9302\n0.0698\n0.9468\n0.0532\nEdge Question Bitstrings\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n(b) Rigetti Ankaa-3\nedge\nvertex\nObserved\n0000\n0101\n1010\n1111\ntarget\n0.0369\n0.9631\n0.0493\n0.9507\n0.0686\n0.9314\n0.0732\n0.9268\nVertex Question Bitstrings\nedge\nvertex\nObserved\n0001\n0010\n0011\n0100\n0110\n0111\n1000\n1001\n1011\n1100\n1101\n1110\ntarget\n0.9851\n0.0149\n0.9729\n0.0271\n0.9988\n0.0012\n0.9734\n0.0266\n0.9985\n0.0015\n0.9705\n0.0295\n0.9673\n0.0327\n0.9990\n0.0010\n0.9829\n0.0171\n0.9985\n0.0015\n0.9683\n0.0317\n0.9697\n0.0303\nEdge Question Bitstrings\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n(c) ibm sherbrooke\nFigure 15: Example of spurious bitstring counts caused by SPAM errors.\n19\n0\n2\n4\nstretch\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSuccess probability\n|\n+\n|0\n|0\n0\n2\n4\nstretch\n|0\n|0\n|\n+\n0\n2\n4\nstretch\n|\n+\n|\n+\n(a) |Ψ+⟩(ibm sherbrooke).\n0\n2\n4\nstretch\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSuccess probability\n|\n+\n|0\n|0\n0\n2\n4\nstretch\n|0\n|0\n|\n+\n0\n2\n4\nstretch\n|\n+\n|\n+\n(b) |Φ+⟩(ibm sherbrooke).\nFigure 16: Median probability of successfully preparing and measuring independent copies of Bell states\n(|Ψ+⟩) or (|Φ+⟩) on ibm sherbrooke.\nStretch\n|Φ+⟩A\n|Φ+⟩B\n|Ψ+⟩A\n|Ψ+⟩B\n0\n0.94 (1)\n0.96(2)\n0.91(2)\n0.96(3)\n2\n0.92 (1)\n0.94(2)\n0.89(2)\n0.94(3)\n4\n0.89 (2)\n0.90(2)\n0.86(2)\n0.92(4)\nTable 1: Mean and standard error of measuring counts in the target Bell state.\nbitstrings. Connecting this characterization back to the nonlocal game as a benchmark: the\ngame construction assumes the players are separated in space and classical communication is\nnot possible. However the implementation on near-term hardware will likely use physical qubits\nthat are connected via tunable couplings.\nIf correlated noise is significant when executing\nsimultaneous multi-qubit gates on non-overlapping qubit subsets, this can affect the win rate\nof the players. For the Bell state example we observe that this affects the ability to implement\nand measure two identical states. We believe that correlated noise may impede the performance\nagain of vertex questions. Finally, we consider the impact of hardware noise on the resource state\n|Ψ⟩shared by Alice and Bob. With mirrored unitary circuits [41], we measure the probability of\napplying URU†\nR and successfully returning to the initial all zero register. Testing the four qubit\nunitary on ibm sherbrooke, Ankaa-2, and Ankaa-3 multiple times we find that the success rate\nfluctuates depending on: hardware, qubit subset, and the choice of resource state.\nOn ibm sherbrooke the success rate of the mirrored four qubit unitary was 19.43%.\nOn\nAnkaa-2, the mirrored four qubit unitary of the original G14 strategy, this approach had a suc-\ncess rate < 10%. Specifically on September 29, 2024 the mirrored unitaries successfully returned\nto the initial state |0⟩⊗4: on qubit subset (9,10,17,16) 8.06%; (2,3,10,9) 6.49 %; (9,10,16,17) 5.66\n%; (2,9,16,23) 8.06%; and (2,3,9,10) 7.32 %. The circuit on Ankaa-2 were compiled with PAR-\nTIAL re-wiring. For Ankaa-3, the mirrored four qubit unitary success rate was much higher.\nOn September 30, 2024 the mirrored unitaries successfully returned to the initial state |0⟩⊗4:\n(0,1,4,3) 55.32%; (0,1,3,4) 33.08%. The circuits on Ankaa-3 were compiled with NAIVE re-\nwiring.\nThe mirrored circuits are much deeper than the resource state preparation alone, and contain\nmore multi-qubit operations. Since noisy hardware can better prepare shallower, sparser re-\nsource state constructions, the mirror fidelity provides a pessimistic estimate of the fidelity of\nthe resource state preparation. However we find it informative to compare the mirror fidelity of\nthe arbitrary four qubit unitary to the mirror fidelity of the shared states used in the Bell state\nstrategy, which we measured 14 times during one week using ibm sherbrooke. For this set of\n20\n|\n+\n|\n+\n|\n|\n|\n+\n|\n+\n|\n|\n0.95\n0.022\n0.02\n0.013\n0.049\n0.92\n0.014\n0.019\n0.027 0.0083\n0.94\n0.027\n0.0093 0.026\n0.057\n0.91\nStretch = 0\n|\n+\n|\n+\n|\n|\n0.91\n0.035\n0.034\n0.016\n0.061\n0.89\n0.018\n0.031\n0.033\n0.016\n0.92\n0.03\n0.018\n0.031\n0.058\n0.89\nStretch = 2\n|\n+\n|\n+\n|\n|\n0.89\n0.047\n0.035\n0.029\n0.073\n0.86\n0.03\n0.032\n0.037\n0.024\n0.9\n0.039\n0.024\n0.037\n0.071\n0.87\nStretch = 4\n00\n01\n10\n11\n|\n+\n|\n+\n|\n|\n0.99\n0.0088 0.0051 7.2e-05\n0.98\n0.011 0.0064 7.2e-05\n0.98\n0.011 0.0056 5.7e-05\n0.99\n0.0084 0.0055 4.3e-05\n00\n01\n10\n11\n0.99\n0.0085 0.0053 7.2e-05\n0.98\n0.013 0.0052 2.9e-05\n0.98\n0.009 0.0068 0.0001\n0.99\n0.0087 0.0057 2.9e-05\n00\n01\n10\n11\n0.98\n0.0085 0.0068 0.00013\n0.99\n0.008 0.0055 4.3e-05\n0.99\n0.0084 0.0055 0.0001\n0.98\n0.011 0.0066 7.2e-05\n(a) (Top) Bell states prepared on qubit subset A. (Bottom) Qubit subset B remains idle.\n00\n01\n10\n11\n|\n+\n|\n+\n|\n|\n0.98\n0.0072 0.012 0.00032\n0.98\n0.0083 0.013 0.00032\n0.98\n0.0071 0.012 0.00033\n0.98\n0.01\n0.012 0.00023\nStretch = 0\n00\n01\n10\n11\n0.98\n0.0085 0.011 0.00042\n0.98\n0.0074 0.012 0.0003\n0.98\n0.01\n0.014 0.00033\n0.98\n0.0078 0.012 0.0003\nStretch = 2\n00\n01\n10\n11\n0.98\n0.01\n0.012 0.0003\n0.98\n0.011\n0.014 0.0003\n0.98\n0.0086 0.014 0.00023\n0.98\n0.011\n0.013 0.00036\nStretch = 4\n|\n+\n|\n+\n|\n|\n|\n+\n|\n+\n|\n|\n0.96\n0.013\n0.014 0.0096\n0.018\n0.96\n0.0047 0.014\n0.031 0.0061\n0.95\n0.015\n0.0062\n0.03\n0.02\n0.94\n|\n+\n|\n+\n|\n|\n0.94\n0.02\n0.024\n0.012\n0.024\n0.95\n0.0073 0.019\n0.041 0.0083\n0.93\n0.024\n0.013\n0.037\n0.027\n0.92\n|\n+\n|\n+\n|\n|\n0.92\n0.027\n0.035\n0.018\n0.029\n0.93\n0.013\n0.026\n0.049\n0.015\n0.91\n0.026\n0.018\n0.044\n0.035\n0.9\n(b) (Top) Qubit subset A remains idle. (Bottom) Bell states prepared on qubit subset B.\nFigure 17: Mean probability of successfully preparing independent copies of Bell states combined with Bell\nstate measurements. Mean probability of successfully observing idle qubits in the |00⟩state.\n21\nshared states the mean success probability was 91.56 ± 0.91%.\nOverall what we can infer from these individual characterizations is how hardware can generate\nnonlocal correlations (in the resource state preparation), how independent qubits can be con-\ntrolled (via the players entangled operations) and finally the robustness of the players answers\nto readout errors. The development of a full predictive model is beyond the scope of this work,\nbut from the initial characterizations of the game components it is clear that improving indi-\nvidual components can significantly impact the overall win rate which is of importance in the\nG14 game, where the separation between the classical and quantum strategies is small.\n6\nStatistical fluctuations and Sample Complexity of Estimating the Win Rate\nOn near-term quantum devices, the win rate of each circuit (question) is estimated by statistical\nsampling, using independently drawn samples to estimate the probability that the players return\nthe correct answers. Finite sample effects lead to statistical fluctuations. In this section, we\nderive an upper bound on the number of individual samples (shots) to draw from a prepared\nstate to sufficiently assess whether a circuit has correctly answered the referee’s question.\nIn the interactive nonlocal game setup, the scenario is repeated with random questions until\nthe referee is satisfied with the outcome. We consider how to obtain a low error estimate of the\nwin rate with high probability using a finite number of repetitions.\nLet n be the number of rounds performed, where each round consists of the referee asking the\nplayers all m possible questions once and checking their answers using the rule function λ(a|q).\nIn the context of quantum hardware, this can be viewed as the execution of m quantum circuits\nwith n shots per circuit.\nBecause the outcome of each question is binary, i.e., λ(a|q) ∈{0, 1}, we model the outcome of\nquestion qj as a Bernoulli random variable λj with an unknown success probability pj. The\nrandom variable describing the game value of a single round is V = 1\nm\nPm\nj λj. We denote the\nempirical estimate of the win rate with n rounds as ¯V = 1\nn\nPn\ni Vi where V1, . . . , Vn ∼V are i.i.d.\nsamples. Under these mild assumptions, we derive an expression for the number of samples\nneeded to accurately estimate the win rate within error ϵ.\nTheorem 6.1. Let ¯V = 1\nn\nPn\ni Vi be the empirical estimate of the game win rate after n rounds,\nwhere each round Vi is independent and identically distributed (i.i.d.). Then, for any ϵ > 0,\nP(| ¯V −E[ ¯V ]| ≥ϵ) ≤2 exp\n \n−nϵ2\/2\n¯σ2\/m + ϵ\/3\n!\n,\n(18)\nwhere m = |Q| is the number of questions and ¯σ2 = 1\nm\nPm\nj pj(1 −pj), where pj is the win rate\nof question qj.\nProof. We make use of the Bernstein inequality [3, 56], which is restated here for convenience.\nLet Sn = Pn\ni Xi be the sum of zero-mean random variables X1, . . . , Xn and |Xi| ≤c almost\nsurely. Then, for any ϵ > 0,\n22\nP(|Sn| ≥ϵ) ≤2 exp\n \n−ϵ2\/2\nPn\ni Var[Si] + cϵ\/3\n!\n.\n(19)\nTo use the inequality, we construct the sum Sn = Pn\ni Vi −E[Vi], subtracting the expectation\nvalues to meet the zero-mean condition, yielding\nP(|n ¯V −nE[ ¯V ]| ≥ϵ) ≤2 exp\n \n−ϵ2\/2\nPn\ni Var[Vi] + cϵ\/3\n!\n.\n(20)\nThe magnitude of each term is bounded |Vi −E[Vi]| ≤1 = c. Furthermore, because each round\nVi ∼V is i.i.d., Pn\ni Var[Vi] = n¯σ2\/m, where ¯σ is defined above and we have used the fact that\nthe variance of a Bernoulli random variable is p(1 −p). Substituting nϵ in place of ϵ gives\n(18).\nCorollary 6.2 (Sample complexity). With probability 1 −δ, we obtain an ϵ-close estimate of\n¯V using at least\nn ≥2 log(2\/δ)\n \n¯σ2\nmϵ2 + 1\n3ϵ\n!\n(21)\nrounds.\nProof. This results from setting (18) less than or equal to δ and solving for n.\nCorollary 6.3. [Confidence interval] With n rounds and with probability 1−δ, the error of our\nestimate is\n| ¯V −E[ ¯V ]| ≤2 log(2\/δ)\n3n\n+ ¯σ\ns\n2 log(2\/δ)\nmn\n.\n(22)\nProof. This can be obtained by solving (21) for ϵ and taking the positive solution, then applying\nthe identity √x + y ≤√x + √y.\nFrom (21), we see two possibilities to achieve asymptotic O(log(1\/δ)\/ϵ) sampling: with a large\nnumber of questions m and when ¯σ ≃ϵ. The first case is not practical because increasing the\nnumber of questions counterproductively increases the total number of circuit samples mn.\nThe second case is also hard to achieve (at present) because it requires a near-perfect strategy\non high-fidelity quantum hardware. This results from ¯σ being directly linked to the win rate\nof the questions, which depends on both the strategy and quantum hardware. Assuming all\nquestions have equal win rate p for simplicity, this requires (again taking the positive solution)\np ≈1\n2(1 +\n√\n1 −4ϵ2), which is approximately p ≈1 −2ϵ + O(ϵ2). We expect that O(log(1\/δ)\/ϵ)\nsampling may become feasible for perfect strategies with improved gate fidelity, quantum error\ncorrection, or amplitude amplification.\nTable 2 contains all the win rates of our executed\nexperiments with confidence intervals derived from Corollary 6.3.\n23\nYear\nProvider\nDevice\nStrategy\nShots\nWin rate (%)\n2023\nIBM\nGuadalupe\nOriginal\n1024\n78.1(6)\nAuckland\nOriginal\n1024\n83.9(6)\nJakarta\nOriginal\n1024\n84.2(6)\nManila\nOriginal\n1024\n84.2(6)\nCairo\nOriginal\n1024\n84.3(6)\nNairobi\nOriginal\n1024\n84.5(6)\nQuito\nOriginal\n1024\n84.6(6)\nMumbai\nOriginal\n1024\n85.6(6)\nLima\nOriginal\n1024\n85.8(6)\nBelem\nOriginal\n1024\n87.3(6)\nHanoi\nOriginal\n1024\n92.5(5)\n2024\nIBM\nSherbrooke\nBell Pair\n4096\n94.3(2)\nRigetti\nAnkaa-2\nBell Pair\n2048\n83.8(4)\nAnkaa-3\nBell Pair\n2048\n91.8(3)\nTable 2: Overall win rate for each device with 95% confidence intervals.\n7\nConclusion\nGuadalupe\nAuckland\nJakarta\nManila\nCairo\nNairobi\nQuito\nMumbai\nLima\nBelem\nHanoi\nSherbrooke\nAnkaa\n2\nAnkaa\n3\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nWin rate\nClassical\nFigure 18: Overall win rate for all devices tested. The symbol † means that trial used the Bell pair strategy\ninstead of the original strategy. Uncertainty on the win rates are 95% confidence intervals. Colors represent\nthe two device providers we tested: IBM in purple and Rigetti in teal. The horizontal line represent classical\nwinrate threshold for this game; any strategy generated above this line requires quantum resources. All results\nare above the win rate achievable by random guessing.\nWe present a variational algorithm to compute novel quantum strategies for nonlocal games\nby encoding the rules of a nonlocal game into a Hamiltonian and employing a two-step opti-\n24\nmization procedure. Our key insight is to optimize separately the state preparation circuit and\nthe measurement scheme while leveraging robust circuit initialization and general techniques,\nsuch as ADAPT, during optimization. The proposed algorithm successfully reproduces known\nquantum strategies and has also discovered new short-depth, perfect quantum strategies for\na graph on 14 vertices using four qubits. This demonstrates that variational techniques can\nbe effectively used on classical computers to identify short-depth, optimal strategies for small\nexamples of nonlocal games where analytic methods fail. Moreover, these techniques extend to\na quantum setting, where sample-based gradient estimation is employed. However, the presence\nof barren plateaus is a known challenge with the training objective function, suggesting that\n“warm starts” or other techniques to mitigate vanishing gradients may be necessary for scaling\nthese methods to larger nonlocal games.\nWe further illustrate how the execution of a nonlocal game strategy can serve as an application-\nlevel benchmark for quantum devices. By evaluating the win rates of both vertex and edge\nquestions in these games, the win rate of vertex questions reflects a device’s ability to perform\nnonlocal operations and maintain gate fidelity, while the win rate of edge questions can help\nconfirm the utilization of entanglement across a device. Although none of the devices we tested\nsurpassed the quantum advantage threshold, primarily due to noise in circuit execution, we\nbelieve our results can be improved by optimizing the transpilation of the individual circuit\nbefore execution and control of the device calibration schedules. It is also worth noting that\nalthough our experiments do not provide a full proof of quantum advantage, given that the\nparticles are not spatially separated enough to guarantee that classical communication does\nnot happen during the experiment, it does provide validation that the quantum hardware in\nquestion outputs results consistent with the hypotheses of quantum theory. Recent work has\nbegun to outline ways of guaranteeing a “loop-hole free” full verification of quantum advantage\nby compiling a multi-prover nonlocal game strategy into a single prover strategy [23, 43, 32]\nand we leave it to future work to investigate the feasibility and implications of these schemes for\nthe games we studied. In a recent survey [1], the authors outlined five desirable properties for\na good quantum benchmark and in our work we argued how the win rate from nonlocal game\nstrategies fit all five points:\n• Relevant: The win rate measures the ability to prepare, control, and manipulate entan-\ngled states.\n• Reproducible: Strategy and questions are fixed.\n• Fair: Device independent and the executed circuits are shallow.\n• Verifiable: Straightforward to calculate the win rate via sampling.\n• Usable: Circuits can be made accessible via QASM files and can easily be ported to other\nquantum devices.\nWe believe that the continued study and extensions of nonlocal games, in particular graph-\nbased games, can enable the design of more appropriate quantum benchmarks as quantum\ndevices scale and hardware architectures become more complex. Ultimately, our research not\nonly advances the understanding of variational quantum strategies but also lays the foundation\nfor leveraging quantum machine learning techniques to explore other nonlocal games strategies\nbeyond the reach of classical methods.\n25\nAcknowledgments\nThanks to David Roberson for providing valuable feedback. NW, JF, and COM were funded\nby grants from the US Department of Energy, Office of Science, National Quantum Information\nScience Research Centers, Co-Design Center for Quantum Advantage under contract number\nDE-SC0012704. JF and COM were partially supported by the Laboratory Directed Research\nand Development Program and Mathematics for Artificial Reasoning for Scientific Discovery\ninvestment at the Pacific Northwest National Laboratory, a multiprogram national laboratory\noperated by Battelle for the U.S. Department of Energy under Contract DEAC05- 76RLO1830.\nS. C. is supported in part by the DOE Advanced Scientific Computing Research (ASCR) Acceler-\nated Research in Quantum Computing (ARQC) Program under field work proposal 3ERKJ354.\nK. H. was supported by the DOE Advanced Scientific Computing Research (ASCR) Pathfinder\nTestbed Program under FWP ERKJ418.\nThis research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE\nOffice of Science User Facility supported under Contract DE-AC05-00OR22725.\nThis manuscript has been authored in part by UT-Battelle, LLC, under Contract No. DE-\nAC0500OR22725 with the U.S. Department of Energy. The United States Government retains\nand the publisher, by accepting the article for publication, acknowledges that the United States\nGovernment retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or re-\nproduce the published form of this manuscript, or allow others to do so, for the United States\nGovernment purposes. The Department of Energy will provide public access to these results of\nfederally sponsored research in accordance with the DOE Public Access Plan.\nReferences\n[1] Acuaviva, A., Aguirre, D., Pe˜na, R., and Sanz, M. Benchmarking quantum comput-\ners: Towards a standard performance evaluation approach. arXiv preprint arXiv:2407.10941\n(2024).\n[2] Bell, J. S. On the Einstein Podolsky Rosen paradox. Physics Physique Fizika 1, 3 (1964),\n195.\n[3] Bernstein, S. On a modification of Chebyshev’s inequality and of the error formula of\nLaplace. Ann. Sci. Inst. Sav. Ukraine, Sect. Math 1, 4 (1924), 38–49.\n[4] Bharti, K., Haug, T., Vedral, V., and Kwek, L.-C. How to teach AI to play Bell\nnon-local games: Reinforcement learning, 2019.\n[5] Bharti, K., Haug, T., Vedral, V., and Kwek, L.-C. Machine learning meets quan-\ntum foundations: A brief survey. AVS Quantum Science 2, 3 (jul 2020).\n[6] Bravyi, S., Gosset, D., K¨onig, R., and Tomamichel, M. Quantum advantage with\nnoisy shallow circuits. Nature Physics 16, 10 (2020), 1040–1045.\n[7] Cameron, P. J., Montanaro, A., Newman, M. W., Severini, S., and Winter, A.\nOn the quantum chromatic number of a graph. arXiv preprint quant-ph\/0608016 (2006).\n[8] Cameron, P. J., Montanaro, A., Newman, M. W., Severini, S., and Winter, A.\nOn the quantum chromatic number of a graph. The Electronic Journal of Combinatorics\n14, 1 (2007), R81.\n26\n[9] Catli, A. B., Simon, S., and Wiebe, N. Exponentially better bounds for quantum\noptimization via dynamical simulation. arXiv preprint arXiv:2502.04285 (2025).\n[10] Cerezo, M., Arrasmith, A., Babbush, R., Benjamin, S. C., Endo, S., Fujii, K.,\nMcClean, J. R., Mitarai, K., Yuan, X., Cincio, L., et al. Variational quantum\nalgorithms. Nature Reviews Physics 3, 9 (2021), 625–644.\n[11] Chen, Y.-H., and Baldwin, C. H. Randomized benchmarking with leakage errors, 2025.\n[12] Childs, A. M., and Wiebe, N. Hamiltonian simulation using linear combinations of\nunitary operations. arXiv preprint arXiv:1202.5822 (2012).\n[13] Clauser, J. F., Horne, M. A., Shimony, A., and Holt, R. A. Proposed experiment\nto test local hidden-variable theories. Physical review letters 23, 15 (1969), 880.\n[14] Clauser, J. F., Horne, M. A., Shimony, A., and Holt, R. A. Proposed experiment\nto test local hidden-variable theories. Physical Review Letters 23, 15 (oct 1969), 880–884.\n[15] Cleve, R., Hoyer, P., Toner, B., and Watrous, J. Consequences and limits of\nnonlocal strategies.\nIn Proceedings. 19th IEEE Annual Conference on Computational\nComplexity, 2004. (2004), IEEE, pp. 236–249.\n[16] Cross, A. W., Bishop, L. S., Sheldon, S., Nation, P. D., and Gambetta, J. M.\nValidating quantum computers using randomized model circuits. Physical Review A 100,\n3 (2019), 032328.\n[17] Cross, A. W., Bishop, L. S., Smolin, J. A., and Gambetta, J. M. Open quantum\nassembly language. arXiv preprint arXiv:1707.03429 (2017).\n[18] Dalzell, A. M., McArdle, S., Berta, M., Bienias, P., Chen, C.-F., Gily´en,\nA., Hann, C. T., Kastoryano, M. J., Khabiboulline, E. T., Kubica, A., et al.\nQuantum algorithms: A survey of applications and end-to-end complexities. arXiv preprint\narXiv:2310.03011 (2023).\n[19] Daniel, A. K., Zhu, Y., Alderete, C. H., Buchemmavari, V., Green, A. M.,\nNguyen, N. H., Thurtell, T. G., Zhao, A., Linke, N. M., and Miyake, A. Quan-\ntum computational advantage attested by nonlocal games with the cyclic cluster state.\nPhysical Review Research 4, 3 (2022), 033068.\n[20] Drmota, P., Main, D., Ainley, E., Agrawal, A., Araneda, G., Nadlinger, D.,\nNichol, B., Srinivas, R., Cabello, A., and Lucas, D. Experimental quantum ad-\nvantage in the odd-cycle game. Physical Review Letters 134, 7 (2025), 070201.\n[21] Emerson, J., Alicki, R., and ˙Zyczkowski, K. Scalable noise estimation with random\nunitary operators. Journal of Optics B: Quantum and Semiclassical Optics 7, 10 (2005),\nS347.\n[22] Fritz, T.\nTsirelson’s problem and Kirchberg’s conjecture.\nReviews in Mathematical\nPhysics 24, 05 (2012), 1250012.\n[23] Grilo, A. B.\nA simple protocol for verifiable delegation of quantum computation in\none round. In 46th International Colloquium on Automata, Languages, and Programming\n(ICALP 2019) (2019), Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.\n[24] Grimsley, H. R., Economou, S. E., Barnes, E., and Mayhall, N. J. An adaptive\nvariational algorithm for exact molecular simulations on a quantum computer.\nNature\ncommunications 10, 1 (2019), 3007.\n27\n[25] Hamilton, K. E., Kharazi, T., Morris, T., McCaskey, A. J., Bennink, R. S.,\nand Pooser, R. C. Scalable quantum processor noise characterization. In 2020 IEEE\nInternational Conference on Quantum Computing and Engineering (QCE) (2020), IEEE,\npp. 430–440.\n[26] Harris, S. J.\nUniversality of graph homomorphism games and the quantum coloring\nproblem. In Annales Henri Poincar´e (2024), Springer, pp. 1–36.\n[27] Hart, O., Stephen, D. T., Williamson, D. J., Foss-Feig, M., and Nandkishore,\nR. Playing nonlocal games across a topological phase transition on a quantum computer.\narXiv preprint arXiv:2403.04829 (2024).\n[28] Helsen, J., Roth, I., Onorati, E., Werner, A. H., and Eisert, J. General frame-\nwork for randomized benchmarking. PRX Quantum 3, 2 (2022), 020357.\n[29] Helton, J. W., Meyer, K. P., Paulsen, V. I., and Satriano, M. Algebras, syn-\nchronous games, and chromatic numbers of graphs. New York J. Math 25 (2019), 328–361.\n[30] Helton, J. W., Mousavi, H., Nezhadi, S. S., Paulsen, V. I., and Russell, T. B.\nSynchronous values of games. arXiv preprint arXiv:2109.14741 (2021).\n[31] Ji, Z., Natarajan, A., Vidick, T., Wright, J., and Yuen, H.\nMIP*=RE.\nCommunications of the ACM 64, 11 (2021), 131–138.\n[32] Kalai, Y., Lombardi, A., Vaikuntanathan, V., and Yang, L. Quantum advantage\nfrom any non-local game. In Proceedings of the 55th Annual ACM Symposium on Theory\nof Computing (2023), pp. 1617–1628.\n[33] Kempe, J., Kobayashi, H., Matsumoto, K., Toner, B., and Vidick, T. Entangled\ngames are hard to approximate. SIAM Journal on Computing 40, 3 (2011), 848–877.\n[34] Knill, E., Leibfried, D., Reichle, R., Britton, J., Blakestad, R. B., Jost,\nJ. D., Langer, C., Ozeri, R., Seidelin, S., and Wineland, D. J.\nRandomized\nbenchmarking of quantum gates.\nPhysical Review A—Atomic, Molecular, and Optical\nPhysics 77, 1 (2008), 012307.\n[35] Lalonde, O.\nOn the quantum chromatic numbers of small graphs.\narXiv preprint\narXiv:2311.08194 (2023).\n[36] Lupini, M., Manˇcinska, L., Paulsen, V. I., Roberson, D. E., Scarpa, G., Sev-\nerini, S., Todorov, I. G., and Winter, A. Perfect strategies for non-local games.\nMathematical Physics, Analysis and Geometry 23, 1 (2020), 7.\n[37] Magesan, E., Gambetta, J. M., Johnson, B. R., Ryan, C. A., Chow, J. M.,\nMerkel, S. T., Da Silva, M. P., Keefe, G. A., Rothwell, M. B., Ohki, T. A.,\net al. Efficient measurement of quantum gate error by interleaved randomized bench-\nmarking. Physical review letters 109, 8 (2012), 080505.\n[38] Manˇcinska, L., Paulsen, V. I., Todorov, I. G., and Winter, A.\nProducts of\nsynchronous games. arXiv preprint arXiv:2109.12039 (2021).\n[39] Manˇcinska, L., and Roberson, D. E. Oddities of quantum colorings. Baltic Journal\non Modern Computing 4, 4 (2016), 846–859.\n[40] Manˇcinska, L., and Roberson, D. E.\nQuantum homomorphisms.\nJournal of\nCombinatorial Theory, Series B 118 (2016), 228–267.\n[41] Mayer, K., Hall, A., Gatterman, T., Halit, S. K., Lee, K., Bohnet, J., Gresh,\nD., Hankin, A., Gilmore, K., Gerber, J., et al. Theory of mirror benchmarking\nand demonstration on a quantum computer. arXiv preprint arXiv:2108.10431 (2021).\n28\n[42] Mitarai, K., Negoro, M., Kitagawa, M., and Fujii, K. Quantum circuit learning.\nPhysical Review A 98, 3 (sep 2018).\n[43] Natarajan, A., and Zhang, T.\nBounding the quantum value of compiled nonlo-\ncal games: from chsh to bqp verification.\nIn 2023 IEEE 64th Annual Symposium on\nFoundations of Computer Science (FOCS) (2023), IEEE, pp. 1342–1348.\n[44] Ortiz, C. M., and Paulsen, V. I. Quantum graph homomorphisms via operator systems.\nLinear Algebra and its Applications 497 (2016), 23–43.\n[45] Paulsen, V. I., and Todorov, I. G. Quantum chromatic numbers via operator systems.\nThe Quarterly Journal of Mathematics 66, 2 (2015), 677–692.\n[46] Proctor, T., Young, K., Baczewski, A. D., and Blume-Kohout, R. Benchmarking\nquantum computers. Nature Reviews Physics (2025), 1–14.\n[47] Reichardt, B. W., Unger, F., and Vazirani, U. A classical leash for a quantum\nsystem:\nCommand of quantum systems via rigidity of CHSH games.\narXiv preprint\narXiv:1209.0448 (2012).\n[48] Rieffel, E. G., Asanjan, A. A., Alam, M. S., Anand, N., Neira, D. E. B., Block,\nS., Brady, L. T., Cotton, S., Izquierdo, Z. G., Grabbe, S., et al. Assessing and\nadvancing the potential of quantum computing: A nasa case study. Future Generation\nComputer Systems (2024).\n[49] Rigetti. Qiskit-Rigetti plugin. https:\/\/github.com\/rigetti\/qiskit-rigetti, 2024.\n[50] Schuld, M., Bergholm, V., Gogolin, C., Izaac, J., and Killoran, N. Evaluating\nanalytic gradients on quantum hardware. Physical Review A 99, 3 (mar 2019).\n[51] Sherbert, K., Furches, J., Shirali, K., Economou, S. E., and Marrero, C. O.\nAdaptive quantum generative training using an unbounded loss function. In 2024 IEEE\nInternational Conference on Quantum Computing and Engineering (QCE) (2024), vol. 1,\nIEEE, pp. 1731–1738.\n[52] Slofstra, W. Tsirelson’s problem and an embedding theorem for groups arising from\nnon-local games. Journal of the American Mathematical Society 33, 1 (2020), 1–56.\n[53] ˇSupi´c, I., and Bowles, J. Self-testing of quantum systems: a review. Quantum 4 (2020),\n337.\n[54] Tura, J., Augusiak, R., Sainz, A. B., V´ertesi, T., Lewenstein, M., and Ac´ın, A.\nDetecting nonlocality in many-body quantum states. Science 344, 6189 (2014), 1256–1258.\n[55] Warren, A., Zhu, L., Mayhall, N. J., Barnes, E., and Economou, S. E.\nAdaptive variational algorithms for quantum gibbs state preparation.\narXiv preprint\narXiv:2203.12757 (2022).\n[56] Zhang,\nH.,\nand Chen,\nS.\nConcentration Inequalities for Statistical Inference.\nCommunications in Mathematical Research 37, 1 (2021), 1–85.\n29\nAppendix\nA\nData Availability\nThe code used to generate the data and figures in this article can be found at\nhttps:\/\/github.com\/jfurches\/nonlocalgames.\nThe authors will make available the data\ncollected for noise characterization by reasonable request.\nB\nADAPT-VQE\nThe Adaptive Derivative-Assembled Pseudo-Trotter ansatz Variational Quantum Eigensolver\n(ADAPT-VQE) is a hybrid quantum-classical algorithm designed to dynamically construct an\nefficient and compact ansatz for molecular simulations on quantum hardware [24]. It enhances\nthe traditional Variational Quantum Eigensolver (VQE) by adaptively building a problem-\nspecific ansatz for the quantum state. Unlike traditional approaches such as Unitary Coupled\nCluster (UCC), which rely on pre-defined and often redundant wavefunction ans¨atze, ADAPT-\nVQE grows the ansatz iteratively by selecting operators that maximize energy recovery at each\nstep. This adaptive approach minimizes the number of parameters and quantum gates required,\nmaking it well-suited for noisy intermediate-scale quantum (NISQ) devices.\nADAPT-VQE operates by measuring the gradient of the Hamiltonian’s expectation value with\nrespect to each operator in a predefined operator pool. The operator with the largest gradient\nis added to the ansatz, and its parameter is optimized alongside previously added parameters\nusing a classical variational optimizer. This process is repeated until the norm of the gradient\nvector falls below a threshold, ensuring convergence to the desired accuracy.\nMore concretely: assume we have variational parameters θ(k) = (θ1, . . . , θk) and the operator\npool A = {A(1), A(2), . . . A(N)}, the ansatz in iteration k + 1 of the algorithm may be written as\n|ψk+1(θ(k+1))⟩= e−iθk+1Ak+1|ψk(θ(k))⟩.\nNotice that the ansatz at iteration k is grown by appending operator Ak+1 with coefficient θk+1;\nthe operator is chosen by measuring the energy gradients\n\f\f\f∂⟨H⟩\/∂θk+1|θk+1=0\n\f\f\f for each operator\nin the pool and selecting the one with the largest gradient. For this step, it can be shown that\n\f\f\f∂⟨H⟩\/∂θk+1|θk+1=0\n\f\f\f =\n\f\f\f⟨ψk(θ(k))| [Ak+1, H] |ψk(θ(k))⟩\n\f\f\f ,\nwhere the right hand side can be efficiently measured on a quantum processor as the size of\na problem scales. The pool operator gradient-measurement step is followed by a convergence\ncheck: if the pool operator gradient norm is smaller than a threshold ε, the calculation is\nterminated; if not, the iteration procedure continues. The ansatz-growing step is followed by a\nVQE optimization of all variational parameters.\nBy tailoring the ansatzs to the problem at hand, ADAPT-VQE achieves high accuracy with\nsignificantly reduced circuit depth compared to fixed ansatz methods. This variational technique\nhas been studied extensively [10] and it has been extended to tackle problems in Quantum\nGenerative training [55, 51]\n30\nC\nMeasurement Parameters\nAlice’s measurement parameters of the G14 strategy are contained within\ndata\/g14 constrained u3ry\/g14 state.json with the key phi.\nConstructing this into a\nNumPy array should return a tensor of shape (1, 14, 2, 4), corresponding to (players, questions,\nqubits, parameters). This tensor can be transformed to produce the conjugated measure-\nment angles for Bob, as seen in U3RyLayer in measurement.py.\nD\nHyperparameters\nProblem\nHyperparameter\nValue\nCHSH\nADAPT Grad Max ϵθ\n10−3\nBFGS Grad Max ϵϕ\n10−5\nDPO Tolerance ∆E\n10−3\nNPS\nSame as CHSH\nG14\nADAPT Grad Max ϵθ\n10−6\nBFGS Grad Max ϵϕ\n10−5\nDPO Tolerance ∆E\n10−6\nTable 3: Hyperparameters for DPO experiments\nWe give the algorithm hyperparameters for our experiments. The parameter ϵθ refers to the\nconvergence criteria of ADAPT used to prepare the shared state |ψ(θ)⟩. ADAPT finishes when\nthe maximum pool gradient element reaches the threshold, maxAi |⟨[H, Ai]⟩| < ϵθ. Similarly, the\nparameter ϵϕ controls the convergence of the second phase of DPO, as the BFGS optimizer halts\nwhen maxi |∇ϕ ⟨H⟩| < ϵϕ. Finally, ∆E controls the termination of the overall DPO procedure,\nending when ⟨H(k−1)⟩−⟨H(k)⟩< ∆E at iteration k.\nE\nGradient Sample Complexity\nIn this section, we analyze the efficiency of the gradient simulation to understand its sample\ncomplexity. This addresses the practical and theoretical challenges faced when implementing\nour algorithm. The gradient complexity we consider is in terms of the number of exponentials\nrequired to achieve any ϵ precision.\nTheorem E.1. Let Ej be a random variable describing the error in the gradient estimate for\nthe j −th experiment with variance E[E2\nj ] = ϵ2\n0. Then the sample complexity of estimating the\ngradient with ϵ2 variance is given by Nexp ∈O\n\u0010\nN2\nϵ2\n\u0011\n, where N is the dimensionality of the\nparameter space.\nProof. Let ϵ0 =\nϵ\n√\nN . By the additivity of the variance, it follows that E\n\"\nN\nP\nj=1\nE2\nj\n#\n= NE[E2\nj ] =\nNϵ2\n0. The Euclidean norm of the gradient is approximated using the variances of the measure-\n31\nment outcomes. Hence\n∥∇∥2 ≈E\n\n\nN\nX\nj=1\nE2\nj\n\n= N ϵ2\nN = ϵ2.\n(23)\nSince each experiment requires O\n\u0010\n1\nϵ2\n0\n\u0011\n= O\n\u0010\nN\nϵ2\n\u0011\noperator exponentials and this must be re-\npeated N times, the total number of operator exponentials Nexp is\nNexp ∈O\n \nN2\nϵ2\n!\n(24)\nas desired.\nTheorem E.2. Assume that the variational state |ψ(θ)⟩requires N parameters to specify and\nthat we wish to minimize F(θ) := ⟨ψ(θ)|H|ψ(θ)⟩over θ. Assume that F is Lipshitz continuous\nwith constant C and that ∇F is Lipshitz continuous with constant L. We then have that the\nnumber of exponentials required to perform gradient descent optimization with final error in the\nobjective function at most ϵtot using learning rate η and Nepochs epochs is in\nO\n \nN2NepochC2((1 + ηL)Nepoch −1)2\nϵ2\ntotL2\n!\nNepoch,tot ∈O\n\n\nN2Nepoch\nγ2 min\nθ∈Γ ∥∇⟨ψ(θ)|H(ϕ)|ψ(θ)⟩∥\n\n.\n(25)\nProof. The gradient descent rule with learning rate η reads\nθ →θ −η∇ϕ⟨ψ(θ)|H(ϕ)|ψ(θ)⟩.\n(26)\nUsing our assumption that the gradient is Lipshitz-continuous with constant L, then\n∥∇⟨ψ(θ)|H|ψ(θ)⟩−∇⟨ψ(θ + δ)|H|ψ(θ + δ)⟩∥≤Lδ.\n(27)\nIf we define ˜G(θ) to be an approximate gradient evaluated at the parameters θ, then\n∥∇⟨ψ(θ)|H|ψ(θ)⟩−˜G(θ + δ)∥≤∥∇⟨ψ(θ)|H|ψ(θ)⟩−∇⟨ψ(θ + δ)|H|ψ(θ + δ)⟩∥\n+ ∥∇⟨ψ(θ + δ)|H|ψ(θ + δ) −˜G(θ + δ)∥\n≤L∥δ∥+ ϵ.\n(28)\nThus, we can recursively define the error in the parameter vector after k epochs to be δk and\nthus from the triangle inequality and the gradient update rule we have\n∥δk∥≤η(L∥δk−1∥+ ϵ) + ∥δk−1∥\n(29)\nWe can then solve this recursion relation to find that\n∥δk∥≤ηϵ + (1 + ηL)ηϵ + (1 + ηL)2ηϵ + · · ·\n≤ϵ((1 + ηL)k −1)\nL\n.\n(30)\nUsing the assumption that the objective function is Lipshitz-continuous with constant C,\n∥⟨ψ(θ + δ)|H|ψ(θ + δ)⟩−⟨ψ(θ)| H |ψ(θ)⟩≤C∥δ∥.\n(31)\n32\nThen it suffices to choose the error per gradient evaluation such that\nCϵ((1 + ηL)Nepoch −1)\nL\n≤ϵtot.\n(32)\nIsolating ϵ yields\nϵ ≤\nϵtotL\nC((1 + ηL)Nepoch −1).\n(33)\nThis means that from Theorem E.1, the total number of exponentials per epoch that are needed\nis\nNexp ∈O\n \nN2C2((1 + ηL)Nepoch −1)2\nϵ2\ntotL2\n!\n.\n(34)\nUsing the fact that there are Nepoch repetitions of the above\nNexp,tot ∈O\n \nN2NepochC2((1 + ηL)Nepoch −1)2\nϵ2\ntotL2\n!\n.\n(35)\nThis shows that the sample complexity of such problems can, in general, be substantial. In\nparticular, if a small learning rate is required for the evolution, the number of operations\nneeded for optimization can be exponential.\nThe learning rate η should be chosen (in the\nstrongly convex case) to be proportional to the smallest eigenvalue of the Hessian matrix,\nimplying that the number of samples scales exponentially with the condition number. This can\nbe prohibitive in cases where some optimization directions are vastly steeper than others, such\nas in the vicinity of a saddle point. The number of epochs required for optimization is similarly\ndifficult to bound. However, in the case where the optimization function is strongly convex,\nthe number of epochs varies logarithmically with the error in the final objective function. In\ngeneral, however, such optimization problems are not necessarily strongly convex. For these\nreasons, we leave the parameters of the gradient descent arbitrary.\nAs a final note, this suggests that variationally optimizing the parameters for a nonlocal game\nis not necessarily expected to be efficient, in general. To make this optimization tractable at\nscale, we need to minimize the number of epochs as much as possible. This can be achieved\nby starting with a well-informed initital guess for the protocol before attempting to optimize\nthe result. If such conditions are met, the above analysis suggests that a manageable number\nof operations will be needed to achieve a constant distance from the locally optimized strategy.\nTo tackle the general problem, we suggest exploring alternative optimization approaches such\nas solving the variational problem using dynamical simulation-based methods [9].\nF\nExperimental Details\nThe experiments on ibm sherbrooke were conducted 7 different times between Sep. 27 - Oct.\n1, 2024 with 4096 shots per circuit. The layout was chosen on the first run to be qubits 46-49\n(a linear chain) using the dense method of the Qiskit transpiler with no optimization (level\n0). For subsequent runs, the same layout was repeated. Each batch contained: the SPAM\n33\nname\nvalue\necr45 44\n0.010494\necr45 46\n0.007731\necr47 46\n0.004980\necr47 48\n0.006505\necr49 48\n0.005589\necr49 50\n0.010321\necr50 51\n0.007020\nTable 4: Calibration data for individual ECR gates between hardware qubits 46-49 on ibm sherbrooke.\ncharacterization circuits, the independent unitary noise characterization circuits, mirror fidelity\ncircuits and the Bell pair game circuits. In Table 2 and Fig. 18, the best run on ibm sherbrooke\nis reported. Calibration data for the backend was queried and saved at the time the circuit batch\nentered the queue and in Table 4 we report the two qubit gate error (ECR gates).\nThe reported data from Rigetti’s Ankaa-2 was collected on September 29 2024, and September\n30 2024. Each circuit was sampled with 2048 shots and the hardware qubits used are reported\nin Figs. 10 and 11.\nThe reported data from Rigetti’s Ankaa-3 was collected on September 30 2024 . Each circuit\nwas sampled with 2048 shots and the hardware qubits used are reported in Figs. 12 and 13.\n34\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Application-level Benchmarking of Quantum Computers using Nonlocal Game Strategies.pdf"}
{"title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques","authors":"Zhengcong Yin, Diya Li, Daniel W. Goldberg","summary":"The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.","url":"http:\/\/arxiv.org\/abs\/2310.14360v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.14360v4","published":1697994236000,"comment":null,"pdf_text":"Is ChatGPT a game changer for geocoding - a benchmark for\ngeocoding address parsing techniques*\nZhengcong Yin\nTexas A&M University\nCollege Station, TX\nyinzhengcong@tamu.edu\nDiya Li\nTexas A&M University\nCollege Station, TX\ndiya.li@tamu.edu\nDaniel W. Goldberg\nTexas A&M University\nCollege Station, TX\ndaniel.goldberg@tamu.edu\nABSTRACT\nThe remarkable success of GPT models across various tasks, includ-\ning toponymy recognition motivates us to assess the performance\nof the GPT-3 model in the geocoding address parsing task. To ensure\nthat the evaluation more accurately mirrors performance in real-\nworld scenarios with diverse user input qualities and resolve the\npressing need for a ‘gold standard’ evaluation dataset for geocoding\nsystems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining\nfrom actual input logs of a geocoding system in production. This\ndataset has 21 different input errors and variations; contains over\n239,000 address records that are uniquely selected from streets\nacross all U.S. 50 states and D.C.; and consists of three subsets\nto be used as training, validation, and testing sets. Building on\nthis, we train and gauge the performance of the GPT-3 model in\nextracting address components, contrasting its performance with\ntransformer-based and LSTM-based models. The evaluation re-\nsults indicate that Bidirectional LSTM-CRF model has achieved the\nbest performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable\nresults compared to the Bidirectional LSTM-CRF model. The GPT-3\nmodel, though trailing in performance, showcases potential in the\naddress parsing task with few-shot examples, exhibiting room for\nimprovement with additional fine-tuning. We open source the code\nand data of this presented benchmark1 so that researchers can uti-\nlize it for future model development or extend it to evaluate similar\ntasks, such as document geocoding.\nCCS CONCEPTS\n• Information systems →Location-based service; • Comput-\ning methodologies →Natural language processing;\nKEYWORDS\nGeocoding, Address parsing, Benchmark, NER, LLM, GPT\n1\nINTRODUCTION\nGeocoding is the process of converting address descriptions into\ngeographic coordinates [12] and has been widely used as a data-\nprocessing step in various domains to conduct spatial analysis, from\nenabling efficient urban planning to advancing public health [21, 28,\n29, 40, 42]. However, the validity of conclusions of studies that em-\nployed geocoding as part of their workflow can be largely impacted\nby the quality of geocoded data. [18, 31, 39]. Although every step\n1https:\/\/github.com\/zhengcongyin\/Geocoding-Address-Parsing-Benchmark\n *Preprint accepted by 2nd ACM SIGSPATIAL International Workshop on Searching\nand Mining Large Collections of Geospatial Data.\nin a geocoding process can accumulate errors in the final output\n[8, 12], address parsing, which is to extract address components\n(shown in the Figure 1) from the address description input by users,\nplays a profound role in determining the quality of geocoded data.\nThis is because outputs from the address parsing process are used to\nassemble query strings to retrieve matching candidates for further\ncalculation and ranking to derive final geocoded outputs [8, 9, 38].\nFor example, if an address parser mistakenly recognizes the ‘116\nS’ as the house number, the geocoding engine can not extract the\ncorrect geocode output from the reference dataset by employing\n‘116 S’ as a house number search criterion. Moreover, geocoding\ninput is proven to be error-prone, containing syntactic or semantic\nerrors [3, 17]; such quality of user input demands an address parser\nto handle them appropriately to ensure the quality of geocoded\noutput.\nRecently, significant breakthroughs have been witnessed in Large\nLanguage Models (LLMs) and their applications in Natural Lan-\nguage Processing (NLP). Models like GPT-3 [1] are making waves\nby setting new benchmarks in various tasks [35] and by shifting\nthe model training workflow [7]. Researchers in the geospatial do-\nmain have also evaluated the capability of Generative Pre-trained\nTransformer (GPT) models in handling toponymy recognition and\nlocation description recognition tasks and have shown promising\nresults [26]. This raises the question that can GPT-based models\nmake a difference in the task of address parsing?\nYet, there has been minimal effort in evaluating the performance\nof GPT-based models for address parsing or geocoding, and the\nfavorable evaluation outcomes presented in [26] can not provide\ntoo many insights into the performance of GPT-based models on\ngeocoding address parsing, as their evaluation framework [15]\nis not designed for the address geocoding task, and input for to-\nponymy resolution tasks, which contain place names in short text\nmessages, is different from the input for geocoding: a postal address\ndescription. In fact, in the realm of geocoding, a \"gold-standard\"\nbenchmark dataset that can fully evaluate geocoding systems is\nhighly demanded [18]. Compared to the magnitude of human input\nerrors, input datasets in existing geocoding evaluation frameworks\nonly contain relatively simple misspellings [3, 17]. We argue that\nevaluation data used in existing work did not fully reflect the qual-\nity of geocoding input in real scenarios, making the performance\nof a geocoding system and address parser remain uncertain when\nfacing (erroneous) input in reality.\nTherefore, in this work, we present a benchmark that is specific\nto evaluate geocoding address parsing techniques using synthe-\nsized low-quality input by mining human input patterns from real\nZ. Yin et al.\ngeocoding system logs, and we evaluate address parsing perfor-\nmance using GPT-3 model and compare to other transformer-based\nand recurrent neural network-based address parsing methods.\nFigure 1: USPS standard address components of a postal ad-\ndress description\nThe contributions of this work can be summarized as follows.\n• A benchmark dataset that contains diverse address descrip-\ntions (e.g., highway and grid style) covering all U.S. states\nand 21 input errors and variations is generated by mining\nreal geocoding system logs. A data processing pipeline is de-\nveloped to analyze input errors and variations occurring in\ndifferent address components from real user input, and then\nthe harvested inject errors and variations using these identi-\nfied patterns to synthesize low-quality geocoding input. To\nthe best of our knowledge, this is the first publicly released\nannotated low-quality geocoding input dataset for U.S. ad-\ndresses with such magnitude of coverage and error\/variation.\n• Address parsers built upon five different models (i.e., GPT-3\nmodel, transformer-based model, and LSTM-based model)\nare evaluated by synthesized low-quality address input with\ndifferent errors to reflect their performance when facing\nvarious input qualities in real scenarios. These evaluation\nresults can provide insights into the potential capabilities\nof each model, especially the GPT-3 model, for further fine-\ntuning or enhancement.\n• The proposed benchmark, encompassing benchmark datasets\nand address parsing methods, is available as open source and\ncan be accessed at Github2. Researchers could use the bench-\nmark dataset for other geospatial text processing tasks or use\nthe evaluation results as baselines for future development\nand experimental comparisons. This proposed framework\ncan be extended to synthesize language- or county-specific\nlow-quality input to evaluate address parsing or geocoding\nsystems in different countries.\nThe remainder of this paper is organized as follows. Section 2\nsummarizes recent work on address parsing techniques in geocod-\ning and Name Entity Recognition in other domains. Section 3 de-\nscribes the design details of the proposed benchmark, including the\napproach to synthesize the low-quality geocoding input, evaluated\naddress parsing techniques, and evaluation metrics. In Section 4, we\nillustrate the evaluation outcomes and discuss the results. We con-\nclude this paper with potential avenues for future work in Section\n5.\n2\nRELATED WORK\nGeocoding address parsing is a domain-specific Named Entity\nRecognition (NER) task that has received extensive research atten-\ntion. In previous research endeavors, the primary competitor has\n2https:\/\/github.com\/zhengcongyin\/Geocoding-Address-Parsing-Benchmark\ncentered on algorithms designed to improve address parsing capa-\nbilities. In the initial stage, these parsing algorithms were predomi-\nnantly built on rule-based and statistical methodologies. Rule-based\napproaches usually leverage the format of local address schema\nand its hierarchy to determine the sequence of labels for a given ad-\ndress input [8]. Typically, a tire or tree-based data structure is used\nto mimic the hierarchy of address systems, string matching (i.e.,\nforward\/backward string method), beam search, heuristic search\nstrategies, and fine-state machines are used to explore the possible\nlabel sequences for addresses. Given that rule-based methods heav-\nily rely on address system rules and lexicons to recognize certain\naddress components (e.g., road types), the variation of user input in\nterms of quality and descriptions could easily result in the \"Out Of\nVocabulary\" issue. Later, statistical-based address parsing represents\na learning and tagging process, as an annotated corpus is required\nfor training, and sequence tagging algorithms make decisions for\neach label. Two popular models, Hidden Markov Models (HMMs)\nand conditional Random Fields (CRFs) have been used to build ad-\ndress parser [2, 33] and achieved SOTA at that time. To augment\nthe coverage of the state transition matrix for variations, [2] has\nenhanced the training data to contain intentionally manipulated\naddresses. Later, the hybrid-based address parser that combines the\nrule-based and statistical-based approaches [23] has shown better\nparsing performance. In recent years, research has shifted towards\nusing neural networks and LLMs as the foundational framework\nfor building address parsers[13, 14, 27, 30, 36], given their proven\nsuccess in NER tasks across various domains [20]. Another avenue\nof research related to address parsing involves reducing the need\nfor annotated data [4] or predicting noisy tokens in geocoding\nqueries[34].\nGiven that address descriptions and formats differ among coun-\ntries [36], the aforementioned studies are using input address de-\nscriptions from the address system specifics to their study areas,\nincluding U.S. [9], China [22], Japan [24], and India [27]. However,\nthe lack of a standardized evaluation dataset for each individual\naddress system complicates the direct comparison of the experi-\nmental results of different studies targeting the same country. Our\nwork extends the existing works by presenting a unified evaluation\nframework, including a benchmark dataset, evaluation procedures,\nand evaluation metrics created specifically to assess geocoding\naddress parsing. The benchmark dataset, which accounts for the\nheterogeneity in address formats and encompasses a wide range\nof input errors\/variations, is publicly released to facilitate future\nresearch investigations.\n3\nBENCHMARK DESIGNS\nThis section describes how benchmark datasets are generated, the\nselection of evaluated models, and the metrics to access address\nparsing performance.\n3.1\nBenchmark Dataset\nFigure 2 depicts the workflow to generate the benchmark dataset,\nnamely, the low-quality geocoding input dataset. This workflow\ncontains three major steps: (1) extracting ground-truth dataset; (2)\nbuilding address component error injector that can generate common\nBenchmark for geocoding address parsing\ngeocoding input error and variations; (3) synthesizing low-quality\ngeocoding input.\nFigure 2: Benchmark dataset processing workflow\n3.1.1\nGround-truth data. The ground-truth data is generated by\nextracting from reference datasets, as reference datasets are the sin-\ngle source of truth for geocoding systems to perform the retrieval\nprocessing to derive final outputs. We extracted address description\nfrom the Navteq 2016 address point reference datasets 3 used by\nTexas A&M geocoding platform4, because this dataset has been\nutilized by other studies [37, 38], and every address description\nin this reference dataset has already been segmented and aligned\nwith a USPS standard address component label shown in Figure\n1. To ensure the diversity of address descriptions across the U.S.,\nwe first get the unique combination of address components except\nfor house number (i.e., street name, predirectional, postdirectional,\ncity name, and postal code) from each U.S. state and the District of\nColumbia, meaning that we obtained one address description from\nevery street from all U.S. 50 states and D.C to formalize a unique\naddress description dataset. Then, we further split this unique ad-\ndress dataset into three smaller datasets designated for training,\nvalidation, and testing procedures in this benchmark. The testing\ndataset is generated by extracting one address description from\nevery pair of state and postal codes in the U.S., resulting in a dataset\nof 30,622 addresses. To obtain the training and validation datasets,\nwe first exclude the testing dataset from the unique address collec-\ntion; we then randomly select up to 9 addresses from every unique\ncombination of city, state, and postal code from the unique address\ncollection and put the first two addresses and the last address into\nthe training and validation dataset, respectively, when applicable.\nIn the end, the training and validation datasets have 148,173 and\n60,522 addresses, respectively, and all address descriptions in the\ntraining, validation, and test datasets are mutually exclusive.\n3.1.2\nAddress component error injector. To synthesize low-quality\ngeocoding input, we build an address component error injector to\nrandomly generate errors and variations based on human input\npatterns. To capture such patterns for geocoding input, we firstly ex-\ntract three-month geocoding transactions from Texas A&M geocod-\ning platform5 and only keep these inputs, which cannot lead to full\nmatching scores (i.e., the reference data can only partially match\nwith the input.) In total, we obtained roughly 30 million input\nqueries. Next, we iterate each input and compare it to its corre-\nsponding reference data to detect input errors and variations. Since\nuser input and matched reference data have already been segmented\nbased on address components to seek a match by the geocoding\n3https:\/\/www.here.com\/en\/navteq\n4https:\/\/geoservices.tamu.edu\/\n5https:\/\/geoservices.tamu.edu\/\nplatform, input errors can be found by aligning user input address\ndescriptions with their corresponding description in address refer-\nence datasets. For example, if the city name is missing in the input\ncompared to the corresponding reference data, the error of omission\nis detected. While iterating the historical user input data, we collect\nsets of mismatched input samples per each address component and\nthen further distill to get cases of commonly used abbreviations\nand common substations for each address component. Totally, we\nidentify 21 errors and variations on different address components\nlisted in Table 1.\nLastly, we create the logic to generate these identified errors\nand variations by aligning and comparing between the segmented\nuser input and the segmented reference data. Addition or omission\nerrors can be generated by reversing the process of how these er-\nrors\/variations are detected. For instance, a directional addition\nerror can be identified by comparing the user input and the refer-\nence data; such an error can be synthesized by adding a directional\nto an address record in the reference data. For typographic errors,\nwe employ the same mechanism of Freely Extensible Biomedical\nRecord Linkage (FEBRL) [3] to randomly swap, delete, insert, or\nreplace a character. We quantify the degree of typographic error by\nedit distance and set the probabilities of typographic errors with\nedit distance 1 or 2 to be the same. As for error\/variation of abbre-\nviation and substitution, we leverage the collected common cases\nto reproduce the error\/variation, for example, replacing Los Angeles\nby LA for a city name input.\n3.1.3\nLow-quality geocoding input for benchmarks. The last step\nis synthesizing low-quality geocoding input used as the bench-\nmark dataset to access address parsing techniques. We apply the\naddress component error injector to the training, validation, and\ntest ground-truth datasets obtained in Section 3.1.1. Specifically,\nwe set the probability of injecting errors\/variations to an address\nrecord in these three split datasets to 0.5, and the ratio of injecting\ntwo or one error\/variation is 7:3. Every address component has the\nsame chance to be manipulated to contain an error\/variation. It’s\nworth noting that we only inject errors that are applicable to an ad-\ndress record. For example, if an address is an ordinal number street\nsuch as \"5th Avenue\", it is applicable to have the error of ordinal\nnumber suffix omission to become \"5 Avenue\". We intentionally\nreduce the postal code digits mismatched error to prevent it from\ndominating synthesized errors. To this end, training, validation,\nand test datasets all contain address records with zero, one, or two\nerrors\/variations, as summarized in Table 2. The distribution of\neach error\/variation for each dataset is summarized in Figure 3.\nTo label these three datasets for training and evaluation, we em-\nploy the IOB (Inside–outside–beginning) tagging scheme to assign\nthe corresponding label to each chuck segmented by white space.\nFor example, the city name Los Angeles would receive two labels:\nB-CITY and I-CITY.\n3.2\nBaseline models\nThe following section provides an overview of the baseline models\nutilized to build address parsers in our experiments, each represent-\ning significant strides in the field of NLP.\nZ. Yin et al.\nTable 1: Geocoding input errors and variations\nAddress component\nError\/Variation\nExample\nHouse number\nOmission\n1600 Main St →Main St\nPre-\/Post-directional\nOmission\nEast Main St →Main St\nPre\/Post-Direction swap\nE Main St NW →NW Main St E\nStreet base name\nTypo (edit distance 1)\nMain St →Man St\nTypo (edit distance 2)\nMain St →Mian St\nNumber suffix omission\n5th Ave →5 Ave\nSpanish prefix omission\nLa Brea Ave →Brea Ave\nSpace omission\nMemory Hill →Memoryhill\nSpace addition\nReachcliff →Reach Cliff\nPartial abbreviation\nWarm Mountain →Warm Mtn\nRoad type\nOmission\nMain St →Main\nValid road type substitution\nMain St →Main Ave\nInvalid road type substitution\nMain St →Main St St\nCity\nOmission\nHouston, TX 77845 →TX 77845\nTypo (edit distance 1)\nAustin →Austiun\nTypo (edit distance 2)\nLuverne →Luvre\nDirection addition\nHouston →South Houston\nDirection omission\nNorth Little Rock →Little Rock\nFirst character abbreviation\nLos Angeles →LA\nSpace addition\nRedlands →Red Lands\nState\nOmission\nHouston, TX 77001 →Houston, 77001\nPostal code\nOmission\nHouston, TX 77001 →Houston, TX\nAny digits mismatched\n77845 →77843\nTable 2: Frequency of address records with different quality\nin the benchmark dataset\nSubset\nTotal\nNo error\/variation\nOne error\/variation\nTwo error\/variation\nTraining\n148,173\n74,086\n51,898\n22,189\nValidation\n60,522\n30,230\n21,247\n9,045\nTest\n30,622\n15,286\n10,736\n4,600\nBidirectional LSTM-CRF [16]: The Bidirectional LSTM-CRF\nmodel combines the strengths of both Bidirectional Long Short-\nTerm Memory (Bi-LSTM) and Conditional Random Fields (CRF)\nfor sequence labeling tasks. Bi-LSTM, a type of Recurrent Neural\nNetwork (RNN), is capable of capturing the context from both\ndirections of a sequence and hence is widely used for NLP tasks.\nCRF, on the other hand, is a statistical modeling method often\nused for structured prediction. In the context of NLP, CRFs are\nused to predict the most likely labels for a sequence of words. The\nBidirectional LSTM-CRF model leverages the Bi-LSTM to extract\ncomplex features from input sequences, and then use the CRF to\npredict the optimal labeling sequence, considering both the input\nsequence and the correlation of labels, resulting in state-of-the-art\nperformance on various sequence labeling tasks.\nBERT [5]: Bidirectional Encoder Representations from Trans-\nformers (BERT), developed by Google, revolutionized the NLP\nlandscape by introducing a novel pre-training objective known\nas Masked Language Model (MLM). This objective allows BERT to\nunderstand the context of a word by considering both its preceding\nand following words, a significant departure from previous models\nthat only captured unidirectional contexts. Pre-trained on a substan-\ntial corpus of unlabelled text, including the entirety of Wikipedia\nand the Book Corpus [6, 41], BERT has shown remarkable perfor-\nmance across a variety of NLP tasks. We choose to use the standard\nbert-base-uncased model which contains 110M parameters.\nroBERTa [25]: roBERTa, a variant of BERT introduced by Face-\nbook, further refines the pre-training process. It eliminates the\nnext-sentence pretraining objective, modifies several key hyper-\nparameters, and leverages larger mini-batches and learning rates.\nAdditionally, roBERTa is trained on an augmented version of the\nBookCorpus dataset [6, 41], leading to improved performance over\nBERT in several benchmark tasks. We choose to use the standard\nroberta-base model which contains 125M parameters.\nDistilBERT [32]: As a distilled variant of BERT, DistilBERT\nrepresents an effort to optimize the balance between model perfor-\nmance and resource efficiency. DistilBERT is 60% smaller in size, six\ntimes faster, yet retains 95% of BERT’s performance. This is achieved\nthrough a process known as distillation, where a smaller model\n(the student) is trained to mimic the behavior of a larger model\n(the teacher). We choose to use the standard distilbert-base-uncased\nmodel which contains 67M parameters.\nGPT-3 [1]: GPT-3, also known as ChatGPT, the successor to\nGPT-2 and also developed by OpenAI, is an autoregressive language\nmodel with a staggering 175 billion machine learning parameters.\nBenchmark for geocoding address parsing\nFigure 3: The distribution of synthesized geocoding input\nerrors\/variations in training (a), validation (b), and test (c)\ndatasets\nGPT-3’s size and complexity enable it to excel in tasks involving the\ngeneration of long, coherent text passages. In addition to this, GPT-\n3 exhibits remarkable proficiency in translating between languages,\nanswering questions, summarizing text, and more, making it one\nof the most versatile language models to date.\n3.3\nEvaluation metrics\nGiven the task of geocoding address parsing is to segment the input\naddress description and assign a corresponding address component\nlabel to each segmentation based on the USPS address standard, we\nquantify the parsing performance by the standard NER evaluation\nmetrics, namely, the precision and recall, and the F1 score (i.e.,\nthe harmonic mean of precision and recall) of every annotated\nlabel. Such a measurement indicates a parsing model’s capability to\nrecognize all address components correctly. Since the output from\ngeocoding address parsing is to build a query string to retrieve\nand rank matched candidates, it’s possible that not all address\ncomponents would be used to build queries, and some address\ncomponents are more important than others, depending on how\nthe matching component of a geocoding system is built. To this end,\nwe further calculate a score (denoted as parsing score) based on the\nweight of each address component used by Texas A&M geocoding\nplatform6 using Equation 1 as follows.\nParsing Score =\n∑︁\n𝑊𝑎𝑑𝑑𝑟𝑒𝑠𝑠× 𝐹1𝑎𝑑𝑑𝑟𝑒𝑠𝑠\n(1)\nwhere, 𝑊𝑎𝑑𝑑𝑟𝑒𝑠𝑠and 𝐹1𝑎𝑑𝑑𝑟𝑒𝑠𝑠represents the weight and F1 score\nof every address component, respectively. The weight of each ad-\ndress component (shown in Table 3) is obtained from Texas A&M\ngeocoding platform7 given its performance in [10, 11].\nTable 3: Weight of each address component\nAddress Component\nWeight\nHouse number\n20\nPredirectional\n7\nStreet base name\n45\nRoad type\n10\nPostdirectional\n4\nCity\n17\nState\n1\nZip code\n45\n4\nEXPERIMENT RESULTS AND DISCUSSION\n4.1\nModel Implementation\nSince the GPT-3 model generates output via user prompt, we con-\nducted the NER task via the promptify library 8. This library sends\na structured input to LLMs, which is equivalent to asking a properly\nstructured question that would help these GPT-3 understand the\nquestion better. The API version we used is gpt-3.5-turbo 9. We\nsupplied three examples to the GPT-3 model to help it understand\nthe expectations for the output, as we found the output under the\nzero-shot scenario is suboptimal. These three examples listed be-\nlow are randomly selected from the training dataset, containing\npre-directional, post-directional, and no directional.\n(1) 467 W BROOKWOOD CIR OZARK AL 36360\n(2) 27195 DORY RD W SALVO NC 27972\n(3) 118 LUKE HICKS RD HAZEL GREEN AL 35750\nThe three transformer-based models, along with the Bidirectional\nLSTM-CRF model, were implemented utilizing the Pytorch frame-\nwork. These transformer-based models were built using the hugging\nface library 10. We trained these models using an Adam optimizer\n6https:\/\/geoservices.tamu.edu\/\n7https:\/\/geoservices.tamu.edu\/\n8https:\/\/github.com\/promptslab\/Promptify\n9https:\/\/platform.openai.com\/docs\/models\/gpt-3-5\n10https:\/\/huggingface.co\/docs\/transformers\/index\nZ. Yin et al.\n[19], a popular choice for training deep learning models due to its\nefficiency and low memory requirements. The initial learning rate\nwas set to 0.00002, with the linear learning rate schedule type. The\nAdam optimizer was configured with beta1 and beta2 parameters\nset to 0.9 and 0.999, respectively. The dropout is set to 0.5, as we\nobserved that the default dropout can easily lead to over-fitting\nin the initial stage of this experiment. The batch size is set to 30.\nThe Bidirectional LSTM-CRF model was implemented on an open-\nsourced work11. Specifically, we employed GloVe.6B.100d 12 for\nword embedding to fed into neural network, the stochastic gradient\ndescent optimizer with a learning rate of 0.1, the hidden size of an\nLSTM cell of 200, and a batch size of 10. We added an IOB label\nconstraint for transition parameters to enforce valid transitions.\n4.2\nExperiment Settings\nThis experiment aims to compare the different baseline models’\nperformance on the task of geocoding address parsing. To have a fair\ncomparison, we utilized the same datasets, run-time environment,\nand training\/evaluation procedures to ensure any differences in\nperformance could be attributed to the models’ architecture and\ncapabilities rather than external factors. Among these five baseline\nmodels, the four (i.e., the Bidirectional LSTM-CRF model and three\ntransformers-based models) require a training process, whereas the\nGPT-3 model does not need to train, as we directly leveraged the gpt-\n3.5 turbo API to conduct NER inference for address parsing. Thus,\nwe first trained and evaluated these five baselines using training\nand validation datasets to get their trained models; we then applied\nthese trained models and the GPT-3 model to the test dataset to\ncompare their performance. We set the training epoch to be 25, as\nthe preliminary experiment indicated the evaluation loss was less\nthan 0.001. Each training model was evaluated on the validation\ndataset at the end of each epoch, and their evaluation loss was\nrecorded. This allowed us to monitor the models’ learning progress\nand adjust the training parameters if necessary. All training and\nevaluation processes were conducted on Google Colaboratory with\nthe Tesla V100 GPU.\n4.3\nResults and discussion\nFigure 4 presents the trajectories of training and validation loss for\nthe baseline models throughout the entirety of the experimental\nprocesses. The roBERTa model’s validation performance is initially\nhigh but experiences a rapid decrease as training progresses. In\ncontrast, the other models exhibit a steady validation loss through-\nout the entire process. Most models reach convergence around the\n20-epoch mark. Notably, the DistilBERT model stands out for its\nfaster convergence rate than the other models. Having trained these\nfour models, we then tested them alongside the GPT-3 model using\nthe same test dataset detailed in Section 3.1. The evaluation results\nof the five evaluated baseline models are presented in Table 4, illus-\ntrating their effectiveness in recognizing and extracting individual\naddress elements and the overall performance.\nAcross all address components, the Bidirectional LSTM-CRF\nmodel consistently demonstrates superior or comparable perfor-\nmance to the other models. For instance, in identifying the house\n11https:\/\/github.com\/allanj\/pytorch_neural_crf\n12https:\/\/nlp.stanford.edu\/projects\/glove\nnumber, this model achieved the highest F1 score of 0.99977, marginally\nsurpassing the performance of roBERTa (0.99976) and BERT (0.99963).\nIts superiority is also evident in parsing the state and postal code\ncomponents, where it yielded an F1 score of 0.99993 and a per-\nfect score of 1.00000, respectively. The BERT model exhibits robust\nperformance across all tasks, with its performance closely trailing\nthat of the Bidirectional LSTM-CRF model. It performed particu-\nlarly well in identifying the house number and postal code, with F1\nscores of 0.99963 and 1.00000, respectively. Notably, the roBERTa\nmodel, while generally performing well, exhibited a slight drop in\nperformance when parsing the postdirectional component, with an\nF1 score of 0.94003. The Bidirectional LSTM-CRF model also has\nthe highest Parsing Score, indicating that it not only performs well\nin parsing each address component but also excels in parsing the\ncomponents that carry the most weight in the geocoding process.\nThis is significantly lower than the scores achieved by the other\nmodels for this task. On the other hand, the DistilBERT model’s\nperformance was consistently high across all tasks, with its low-\nest F1 score being 0.96771 for the postdirectional component. Its\nperformance was particularly strong in parsing the house number\nand postal code, achieving F1 scores of 0.99970 and 1.00000, re-\nspectively. The GPT-3 model, however, displayed a relatively lower\nperformance compared to the other models. While it performed\nreasonably well in parsing the house number, state, and postal code\nwith F1 scores of 0.98810, 0.97505, and 0.97851, respectively, it strug-\ngled significantly with the postdirectional component, achieving\nan F1 score of 0.42917, which is markedly lower than the scores of\nthe other models.\nThe Bidirectional LSTM-CRF model consistently outperforms\nor matches the other models across all address components. This\nsuperior performance could be attributed to the inherent strengths\nof this model. The Bidirectional LSTM-CRF model combines the\nadvantages of both bidirectional LSTM and conditional random\nfields, which allows the model to capture context from both past\nand future input while CRF can make the most of the sentence-\nlevel tag information, making it a powerful model for sequence\nlabeling tasks such as NER. The BERT model and its variant, while\nperforming robustly across all tasks, fall slightly behind the Bidi-\nrectional LSTM-CRF model in terms of performance. This could\nbe due to the fact that while BERT is a powerful model, it is pre-\ntrained on a masked language model and next-sentence prediction\ntasks, which may not be perfectly aligned with the NER tasks in\naddress parsing. On the other hand, the pre-trained models have\nbeen trained on large amounts of data, their performance could\npotentially be improved with hyperparameter tuning to optimize\nthem for the specific task of address parsing. This could involve\nadjusting parameters like learning rate and batch size, adding or\nremoving layers, or changing the number of hidden units, among\nother things. However, its strong performance in identifying house\nnumbers and postal codes suggests that it is still a valuable tool for\nthese tasks. As a generative model, GPT-3 demonstrates a lower\nperformance compared to others. One of the potential reasons for\nthat is the GPT-3 generates output based on the context provided\nby the prompt. Therefore, the way the prompt is framed can signif-\nicantly affect the model’s performance. The other reason could be\nthe selected few-shot learning examples used by the GPT-3 model\nwere completely error-free. It would be interesting to compare the\nBenchmark for geocoding address parsing\nFigure 4: The training and validation loss of baseline models\nimpact of different few-shot learning examples on the GPT-3 model\nperformance. It’s worth noting that the hyperparameters of evalu-\nated models come from common settings used by other studies, as\nthe main scope of this paper is to provide a solid foundation to fa-\ncilitate future model evaluations. Fine-tuning hyperparameters for\neach model to find out their best performance can be one direction\nof future work.\n5\nCONCLUSION AND FUTURE WORK\nIn this work, we introduce a benchmark consisting of benchmark\ndatasets and evaluation metrics to assess the performance of the\nGPT-3 model in geocoding address parsing and compare with three\ntransformer-based models and one LSTM-based model. We create\na benchmark dataset capturing 21 input errors\/variations observed\nin real user input logs, and this dataset also contains the unique ad-\ndress formatting across the U.S. (i.e., 50 states and D.C). This helps\nto address the demand for a ’gold standard’ evaluation dataset in\ngeocoding and further guarantees that evaluation results closely re-\nflect their performance in real-world scenarios. Our findings reveal\nthat the Bidirectional LSTM-CRF model slightly outperforms the\ntransformer-based models. Though the GPT-3 model’s performance\nlags behind the other evaluated models, it shows encouraging re-\nsults in address parsing using few-shot examples, suggesting room\nfor improvement with additional fine-tuning. We aim this work to\nserve as a solid baseline for future development and experimental\ncomparisons in similar geographic information retrieval-related\ntasks.\nFuture work includes (1) enhancing the evaluation benchmark\ndataset by capturing more input errors\/variations, (2) fine-tuning\nthe models to improve their performance to attempt to achieve\nSOTA performance in the given input dataset and comparing to\ntraditional models (e.g., CRF and HMM models), and (3) extending\nthis benchmark to be applicable to evaluate address parsing or\ngeocoding systems in other countries, given the heterogeneity of\nlanguage and address systems in different countries.\nREFERENCES\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877–1901.\n[2] Peter Christen, Daniel Belacic, et al. 2005. Automated probabilistic address stan-\ndardisation and verification. In Australasian Data Mining Conference. Citeseer.\n[3] Peter Christen, Tim Churches, et al. 2002. Febrl-Freely extensible biomedical\nrecord linkage. (2002).\n[4] Helen Craig, Dragomir Yankov, Renzhong Wang, Pavel Berkhin, and Wei Wu.\n2019. Scaling Address Parsing Sequence Models through Active Learning. In\nProceedings of the 27th ACM SIGSPATIAL International Conference on Advances in\nGeographic Information Systems. 424–427.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[6] Wikimedia Foundation. [n. d.]. Wikimedia Downloads. https:\/\/dumps.wikimedia.\norg\n[7] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng\nZhang. 2023.\nOpenagi: When llm meets domain experts.\narXiv preprint\narXiv:2304.04370 (2023).\n[8] Daniel Goldberg. 2013. Geocoding Techniques and Technologies for Location-\nBased Services. In Advanced Location-Based Technologies and Services. CRC Press:\nBoca Raton, FL, 75–106.\n[9] Daniel W Goldberg. 2008. A geocoding best practices guide. (2008).\n[10] Daniel W. Goldberg. 2011. Improving Geocoding Match Rates with Spatially-\nVarying Block Metrics. Trans. GIS 15 (2011), 829–850.\n[11] Daniel W Goldberg and Myles G Cockburn. 2010. Improving geocode accuracy\nwith candidate selection criteria. Transactions in GIS 14, s1 (2010), 149–176.\n[12] Daniel W Goldberg, John P Wilson, and Craig A Knoblock. 2007. From text\nto geographic coordinates: the current state of geocoding. URISA journal 19, 1\n(2007), 33–46.\n[13] Yassine Guermazi, Sana Sellami, and Omar Boucelma. 2022. A roberta based\napproach for address validation. In European Conference on Advances in Databases\nand Information Systems. Springer, 157–166.\n[14] Berkay Güler, Betül Aygün, Aydın Gerek, and Alaeddin Selçuk Gürel. 2023. Deep\nActive Learning for Address Parsing Tasks with BERT. In 2023 31st Signal Pro-\ncessing and Communications Applications Conference (SIU). IEEE, 1–4.\n[15] Yingjie Hu, Krzysztof Janowicz, and Sathya Prasad. 2014. Improving wikipedia-\nbased place name disambiguation in short texts using structured data from\ndbpedia. In Proceedings of the 8th workshop on geographic information retrieval.\n1–8.\n[16] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for\nsequence tagging. arXiv preprint arXiv:1508.01991 (2015).\n[17] Matthew John Hutchinson. 2010. Developing an agent-based framework for intel-\nligent geocoding. Ph.D. Dissertation. Curtin University.\n[18] Geoffrey M Jacquez. 2012. A research agenda: does geocoding positional error\nmatter in health GIS studies? Spatial and spatio-temporal epidemiology 3, 1 (2012),\n7–16.\nZ. Yin et al.\nTable 4: Evaluation results of baseline models\nAddress component\nBiLSTM-CRF\nBERT\nroBERTa\nDistilBERT\nGPT-3\nHouse number\n0.99977\n0.99963\n0.99976\n0.99970\n0.98810\nPredirectional\n0.99719\n0.99378\n0.98996\n0.99579\n0.70077\nStreet base name\n0.99241\n0.98963\n0.98022\n0.99061\n0.83853\nRoad type\n0.99705\n0.99345\n0.98543\n0.99427\n0.88328\nPostdirectional\n0.96739\n0.96680\n0.94003\n0.96771\n0.42917\nCity\n0.99399\n0.99293\n0.98539\n0.99341\n0.90404\nState\n0.99993\n0.99986\n0.99894\n0.99991\n0.97505\nPostal code\n1.00000\n1.00000\n0.99987\n1.00000\n0.97851\nOverall F1\n0.99677\n0.99545\n0.99084\n0.99590\n0.90875\nParsing Score\n148.37200\n148.16378\n147.39396\n148.24340\n133.32740\n[19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[20] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,\nand Chris Dyer. 2016. Neural architectures for named entity recognition. arXiv\npreprint arXiv:1603.01360 (2016).\n[21] Diya Li, Harshita Chaudhary, and Zhe Zhang. 2020. Modeling spatiotemporal\npattern of depressive symptoms caused by COVID-19 using social media data\nmining. International Journal of Environmental Research and Public Health 17, 14\n(2020), 4988.\n[22] Hao Li, Wei Lu, Pengjun Xie, and Linlin Li. 2019. Neural Chinese address pars-\ning. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers). 3421–3431.\n[23] Lin Li, Wei Wang, Biao He, and Yu Zhang. 2018. A hybrid method for Chinese\naddress segmentation. International Journal of Geographical Information Science\n32, 1 (2018), 30–48.\n[24] Cheng-Lin Liu, Masashi Koga, and Hiromichi Fujisawa. 2002. Lexicon-driven\nsegmentation and recognition of handwritten character strings for Japanese\naddress reading. IEEE Transactions on Pattern Analysis and Machine Intelligence\n24, 11 (2002), 1425–1437.\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[26] Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, and Stefano Er-\nmon. 2022. Towards a foundation model for geospatial artificial intelligence\n(vision paper). In Proceedings of the 30th International Conference on Advances in\nGeographic Information Systems. 1–4.\n[27] Shreyas Mangalgi, Lakshya Kumar, and Ravindra Babu Tallamraju. 2020. Deep\ncontextual embeddings for address classification in e-commerce. arXiv preprint\narXiv:2007.03020 (2020).\n[28] Yolanda J. McDonald, Daniel W. Goldberg, Isabel C. Scarinci, Philip E. Castle,\nJack Cuzick, Michael Robertson, and Cosette M. Wheeler. 2017. Health Service\nAccessibility and Risk in Cervical Cancer Prevention: Comparing Rural Versus\nNonrural Residence in New Mexico. The Journal of Rural Health 4 (2017), 382–392.\n[29] Yolanda J McDonald, Daniel W Goldberg, Isabel C Scarinci, Philip E Castle,\nJack Cuzick, Michael Robertson, and Cosette M Wheeler. 2017. Health service\naccessibility and risk in cervical cancer prevention: comparing rural versus\nnonrural residence in New Mexico. The Journal of Rural Health 33, 4 (2017),\n382–392.\n[30] Shekoofeh Mokhtari, Ahmad Mahmoody, Dragomir Yankov, and Ning Xie. 2019.\nTagging Address Queries in Maps Search. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 33. 9547–9551.\n[31] Daniela Nuvolone, Roberto della Maggiore, Sara Maio, Roberto Fresco, Sandra\nBaldacci, Laura Carrozzi, Francesco Pistelli, and Giovanni Viegi. 2011. Geographi-\ncal information system and environmental epidemiology: a cross-sectional spatial\nanalysis of the effects of traffic-related air pollution on population respiratory\nhealth. Environmental Health 10, 1 (2011), 12.\n[32] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 (2019).\n[33] Wenqiao Sun. 2017. Chinese named entity recognition using modified conditional\nrandom field on postal address. In 2017 10th International Congress on Image and\nSignal Processing, BioMedical Engineering and Informatics (CISP-BMEI). IEEE, 1–6.\n[34] Tin Vu, Solluna Liu, Renzhong Wang, and Kumarswamy Valegerepura. 2020.\nNoise Prediction for Geocoding Queries using Word Geospatial Embedding\nand Bidirectional LSTM. In Proceedings of the 28th International Conference on\nAdvances in Geographic Information Systems. 127–130.\n[35] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682\n(2022).\n[36] Marouane Yassine, David Beauchemin, François Laviolette, and Luc Lamontagne.\n2021. Leveraging Subword Embeddings for Multinational Address Parsing. In 2020\n6th IEEE Congress on Information Science and Technology (CiSt). IEEE, 353–360.\n[37] Zhengcong Yin, Daniel W Goldberg, Tracy A Hammond, Chong Zhang, Andong\nMa, and Xiao Li. 2020. A probabilistic framework for improving reverse geocoding\noutput. Transactions in GIS 24, 3 (2020), 656–680.\n[38] Zhengcong Yin, Andong Ma, and Daniel W Goldberg. 2019. A deep learning\napproach for rooftop geocoding. Transactions in GIS 23, 3 (2019), 495–514.\n[39] Paul A Zandbergen and Joseph W Green. 2007. Error and bias in determining\nexposure potential of children at school locations using proximity-based GIS\ntechniques. Environmental Health Perspectives 115, 9 (2007), 1363.\n[40] Zhe Zhang, Zhangyang Wang, Angela Li, Xinyue Ye, E Lynn Usery, and Diya\nLi. 2021. An Al-based Spatial Knowledge Graph for Enhancing Spatial Data and\nKnowledge Search and Discovery. In Proceedings of the 1st ACM SIGSPATIAL\nInternational Workshop on Searching and Mining Large Collections of Geospatial\nData. 13–17.\n[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,\nAntonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards\nStory-Like Visual Explanations by Watching Movies and Reading Books. In The\nIEEE International Conference on Computer Vision (ICCV).\n[42] Kate Zinszer, Christian Jauvin, Aman Verma, Lucie Bedard, Robert Allard, Kevin\nSchwartzman, Luc de Montigny, Katia Charland, and David L Buckeridge. 2010.\nResidential address errors in public health surveillance data: A description and\nanalysis of the impact on geocoding. Spatial and Spatio-temporal Epidemiology 1,\n2-3 (2010), 163–168.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques.pdf"}
{"title":"Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy","authors":"David Schlangen","summary":"How does one measure \"ability to understand language\"? If it is a person's\nability that is being measured, this is a question that almost never poses\nitself in an unqualified manner: Whatever formal test is applied, it takes\nplace on the background of the person's language use in daily social practice,\nand what is measured is a specialised variety of language understanding (e.g.,\nof a second language; or of written, technical language). Computer programs do\nnot have this background. What does that mean for the applicability of formal\ntests of language understanding? I argue that such tests need to be\ncomplemented with tests of language use embedded in a practice, to arrive at a\nmore comprehensive evaluation of \"artificial language understanding\". To do\nsuch tests systematically, I propose to use \"Dialogue Games\" -- constructed\nactivities that provide a situational embedding for language use. I describe a\ntaxonomy of Dialogue Game types, linked to a model of underlying capabilites\nthat are tested, and thereby giving an argument for the \\emph{construct\nvalidity} of the test. I close with showing how the internal structure of the\ntaxonomy suggests an ordering from more specialised to more general situational\nlanguage understanding, which potentially can provide some strategic guidance\nfor development in this field.","url":"http:\/\/arxiv.org\/abs\/2304.07007v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2304.07007v1","published":1681463496000,"comment":null,"pdf_text":"Dialogue Games for Benchmarking Language\nUnderstanding: Motivation, Taxonomy, Strategy\nDavid Schlangen\nComputational Linguistics \/ Department of Linguistics\nUniversity of Potsdam, Germany\ndavid.schlangen@uni-potsdam.de\nAbstract\nHow does one measure “ability to understand\nlanguage”? If it is a person’s ability that is be-\ning measured, this is a question that almost\nnever poses itself in an unqualiﬁed manner:\nWhatever formal test is applied, it takes place\non the background of the person’s language\nuse in daily social practice, and what is mea-\nsured is a specialised variety of language un-\nderstanding (e.g., of a second language; or of\nwritten, technical language).\nComputer pro-\ngrams do not have this background.\nWhat\ndoes that mean for the applicability of for-\nmal tests of language understanding?\nI ar-\ngue that such tests need to be complemented\nwith tests of language use embedded in a prac-\ntice, to arrive at a more comprehensive eval-\nuation of “artiﬁcial language understanding”.\nTo do such tests systematically, I propose to\nuse “Dialogue Games”—constructed activities\nthat provide a situational embedding for lan-\nguage use. I describe a taxonomy of Dialogue\nGame types, linked to a model of underlying\ncapabilites that are tested, and thereby giving\nan argument for the construct validity of the\ntest.\nI close with showing how the internal\nstructure of the taxonomy suggests an order-\ning from more specialised to more general sit-\nuational language understanding, which poten-\ntially can provide some strategic guidance for\ndevelopment in this ﬁeld.\n1\nIntroduction\nSteff is sitting at a desk, intently focussed on the\npiece of paper in front of them. The task is to read\nshort paragraphs of text and then to answer ques-\ntions about them, and how well Steff does at this\nwill determine their “language proﬁciency score”,\nand thus contribute to whether they will get ad-\nmission to the University of their choice or not –\nfor it is a requirement to understand the language\nthat is being tested. Steff gets up and heads toward\nthe door – “the other one” shushes the proctor.\nOutside, Steff sees their friend, looking at them,\nand greets them with “next time”; the reply comes\nimmediately: “drinks?”\nThe subﬁeld of “Natural Language Understand-\ning” (NLU) within the ﬁeld of Natural Language\nProcessing (NLP) uses tests of the ﬁrst kind—\nwritten responses to written material—to measure\nthe degree to which a technical artefact can be said\nto possess the ability of understanding natural lan-\nguage. More recently, NLP has expanded towards\ntackling more situated and less abstracted cases of\nlanguage use—as in the second part of the story,\nif not quite as social—, under the headings “lan-\nguage and vision (navigation)” or “embodied AI”\n(Duan et al., 2022; Gu et al., 2022; Sundar and\nHeck, 2022),1 with evaluation practices not yet\nfully established.\nThis paper aims to systematise already ongoing\nefforts in this direction and to support future ones,\nby ﬁrst asking how these kinds of language un-\nderstanding settings—formal, and situated—relate.\nComing to the conclusion that Situated Language\nUnderstanding (SLU) requires different testing ap-\nproaches, and that NLU evaluation has proceeded\nsomewhat haphazardly, I will describe the design\nchoices for creating situated language use activities,\nrelating them to a particular, but abstract, model of\nsituated language understanding; thereby address-\ning for this new ﬁeld the concern of Schlangen\n(2021) that progress cannot be measured with-\nout clarity about underlying theoretical commit-\nments. More speciﬁcally, I want to show a way\nhow Dialogue Games can be integrated into a sound\nmethodology for computational research on mean-\ning, by providing explicit information about rela-\ntions between research objects (see Figure 1).\nIt might be useful to mention at the outset what\nthis paper is not aiming to do, which is to make rec-\n1The ﬁeld has moved back to this, one should say, as of\ncourse situated language used to be much more in the center,\nas for example in the very early SHRDLU system (Winograd,\n1972).\narXiv:2304.07007v1  [cs.CL]  14 Apr 2023\nSet CL of capabilites of \ncompetent language user\nTask T \n• task \ndescription\nDataset D \n• collection \ninstructions\nModel M \n• architecture\nCognitive \nCapability C\nCognitive \nCapability C’\nCognitive \nCapability C’’\noptimized for\nexempliﬁes\ninvolves\nTask T’ \n• (task \ndescription)\nEnvironment E \nModel M’ \n• architecture\nDataset D’ \n• collection \ninstructions\noptimized  \nfor\n…\nTask T’’ \n• (task \ndescription)\nSetting S\nGame G\nPlayer P\nPlayer P’\nexempliﬁes\ninvolves\nFigure 1: The structure of relations between the research objects model, dataset, task, environment, setting, game,\nand cognitive capability. Adapted from (Schlangen, 2019b).\nommendations for how SLU should be modelled,\nin the technical sense. While I see value in being\nable to understand the components of a task and\nhow they interact (which suggests modularity in\ndesign), nothing precludes attempting benchmarks\nof the types described here with monolithic mod-\nels, and even, insofar as the requisite information\ncan be represented in the right way, with “general\npurpose” models such as Large Language Models.\n2\nBackground On Measurement\nLanguage understanding, as a psychological pro-\ncess, is observable only in its reﬂections in be-\nhaviour.2 But not any behaviour counts, and not\nany behaviour is measurable—and measurement\nis our goal here. Experimental psychology has de-\nveloped many ways to deal with the problem of\nmeasurement of unobservables in a principled man-\nner. A central notion here is that of validity of a\nmeasurement instrument: Does the instrument in-\ndeed measure the unobservable construct that it is\nset up to measure?\nThis is not the place to give a full introduction\ninto that ﬁeld,3 so I will concentrate on those as-\npects of validity that I see as attainable through\nthe methodology described below. A ﬁrst claim\nfor validity of an instrument is via an appeal to its\nface validity: That it intuitively appears to capture\nthe construct. Being able to count in a text the\noccurrences of the letter “o”, for example, would\nlack such face validity for the construct “language\n2This is independent of whether you think that it is a pro-\ncess resulting in a speciﬁc psychological state, or a behavioural\ndisposition (Ryle, 1949).\n3See (Frank et al., 2023; Flake and Fried, 2020; Sireci and\nSukin, 2013) for some recent overviews.\nunderstanding”, while being able to answer ques-\ntions about it may be argued to have it. (Although\nour intuitions leave us quickly here: What if some\nquestions are answered well, but others bizarrely\nbadly? More on this below.) A second element is\necological validity, an argument for how closely\nthe measure resembles the use of the construct in\nthe domains in which it ordinarily shows. Measures\nof situated language understanding (as will be de-\nveloped here) can arguably make a claim for high\necological validity—this kind of language under-\nstanding plays a large role in people’s lives—but\nmore abstract or formalised understanding tasks\ndo occur, in situations as described above. Lastly,\nquantiﬁable support comes from convergent valid-\nity, as different measures that are purportly address-\ning the same construct can be expected to correlate,\nand if they do so, mutually support their validity.\nWhat is important to note is that behind all these\naspects of validity there is a argumentative connec-\ntion to the construct and its structure, lending a\nkind of network character to the notion: “the mea-\nsure is valid if there is evidence that it ﬁts into the\nnomological network – the network of predicted\nrelationships with other constructs and their mea-\nsures” (Frank et al., 2023). We will see below that\nthis is something that is missing in the evaluation\npractices in NLU, and it is something that I will try\nto develop here for SLU. (In Figure 1, this is the\nbox on the left.)\nSummarising this brief review, to avoid “Ques-\ntionable Measurement Practices”, Flake and Fried\n(2020) propose a number of questions to which\nexperiment designers must be able to give a good\nanswer (Flake and Fried, 2020, p. 459):4\n1. What is your construct?\n2. Why and how did you select your measure?\n3. What measure did you use to operationalize\nthe construct?\n4. How did you quantify your measure?\nThe questions shall serve as a guide for the discus-\nsion below.\n3\nCurrent Practices in Measuring NLU\nThe practice of benchmarking in NLP \/ AI is cu-\nriously disconnected from that of measurement in\nexperimental psychology, even if it sets itself what\nlooks like rather closely related goals (for exam-\nple, to provide a “General Language Understanding\nEvaluation”, as indicated in the name of the GLUE\ncorpus, (Wang et al., 2019b)).\nEvaluation in NLU centers on the language task,\na functional mapping between input and output,\nwhere at least one of these involves language.5 For\na given NLU evaluation corpus, this mapping is typ-\nically characterised verbally; e.g., “the text labelled\n‘answer’ is a correct answer to the question in the\ntext labelled ‘question’, given the context in the\ntext labelled ‘passage’ ”, as this description could\ngo for the example in Figure 2. It is this verbal (or\nintensional) description that enters into an intuitive\nappeal to face validity—surely, answering ques-\ntions must require understanding them. However,\nit can be remarked that the notion of understanding\nin NLU evaluation typically remains an intuitive\none and no further attempt is made at specifying\nthe construct.\nIn any case, the actual measurement instrument\nis one step further removed, as the task needs to\nbe operationalised via instances collected into a\ndataset; this then serves as the extensional def-\ninition of the task. As observed in (Schlangen,\n2021), to not lose connection to the validity argu-\nment (which goes via the intensional description)\nrequires care in setting up the dataset, which some-\ntimes is missing. (For example if the collected in-\nstances do not span the domain in the way claimed\nby the intensional description.) For specialised ma-\n4These are the ﬁrst four of the six questions they give; the\nlatter ones concern pre-registration, the use of which in NLP\nwould be a topic for another paper and is glossed over here.\n5The discussion in this section follows Schlangen (2021),\nwhich however did not yet use the language of measurement\nfrom experimental psychology, however; this connection is\nhelpfully made in (Raji et al., 2021).\nPassage: Barq’s – Barq’s is an American soft drink. Its\nbrand ofroot beer is notable for having caffeine. Barq’s,\ncreated by Edward Barq and bottled since the turn ofthe\n20th century, is owned by the Barq family but bottled\nby the Coca-Cola Company. It was known as Barq’s\nFamous Olde Tyme Root Beer until 2012.\nQuestion: is barq’s root beer a pepsi product\nAnswer: No\nFigure 2: An Example of a GLUE-type task (from the\nBoolQ subset, (Clark et al., 2019), as cited in (Wang\net al., 2019a))\nchine learning models, a further challenge is posed\nby the fact that they are typically trained on a (set\naside) portion of the dataset. Machine learning\nmethods are very good at identifying predictors\nthat optimise performance, regardless of whether\nthese predictors are related to the construct that is to\nbe measured (Lapuschkin et al., 2019). (Note again\nthat for humans, tests of language understanding\nhappen on the tacit and unquestioned background\nof already existing general language competence,\nacquired through material distinct from the testing\nmaterial.)\nWith respect to these concerns, practices in NLU\nevaluation have not much improved. With existing\ntests saturating when probing newer models, the\nresponse has become to go bigger, and efforts such\nas BigBench (Srivastava et al., 2022) and HELM\n(Liang et al., 2022) invested in bringing in very\nmany different evaluation sets. While this may be\nseen as potentially improving convergent validity\n(if a model achieves high performance on so many\ntests, it must be doing something right), there is\nstill little concern about what exactly the underly-\ning construct is. This we can then take with us to\nthe next section: NLU evaluation centers on lan-\nguage tasks, and relies on the face validity of the\ntask, without making much effort to connect to any\nfurther speciﬁed construct.\nIt is also worth noting the criticism of this ap-\nproach put forward by Raji et al. (2021), which\nis that the aim of measuring understanding per-\nformance in the abstract (without tasks that have\nextrinsic value beyond their role in the test) through\ndatasets is misguided, conﬂating as it does a lan-\nguage ability with a recall test on the necessarily\nopen-ended world knowledge that enters into many\nof these tasks. We will see below how the method-\nology developed here can answer this reservation,\nthrough controlling the world knowledge required\nto perform. First, we shall look more closely at\nhow NLU and SLU differ.\n4\nSLU is Different From NLU\nTo give us more examples of situated language use,\nhere is another short story:\nYou are assembling ﬂat-packed furniture,\nwith the help of your friendly household\nrobot. You send the robot to “fetch the\nbox cutter from the drawer in the other\nroom.”(1) “Which one, it’s not in the one\nwith the tools”(2), you hear it shout from\nthe other room. Later, the both of you\nlook over the instructions – why are the\npictograms always so obscure? – and\ndiscuss how to proceed. Having reached\nstep 24, you look at a screw and won-\nder whether it is of type 35784, of which\nthere were supposed to be 12 in package\nA, but the robot just says, “no, the other\none”(3). “Alright, so can you pass me\nthe torx?”(4), you say. “Sure, here you\ngo. That’s a torx then?”(5)\nThis—obviously constructed, but nevertheless\nhopefully coherent—story showcases several fea-\ntures of situated language use unlikely to be found\nin monological text corpora: Example (1) contains\nreferring expressions that express an exophoric ref-\nerence—a reference to singular objects outside of\nthe discourse itself, but to its immediate situational\ncontext—, and it realises a request speech act, for\nwhich one sign of understanding is compliance\nthrough (non-verbal) action. (2) realises a clariﬁca-\ntion request, which is another way understanding\ncan be signalled, albeit a partial understanding only.\nThis hints at the processual nature of understanding\nin interaction, different from the single-shot fram-\ning as in the example in Figure 2. (3) highlights\nhow the syntax of situated language can be differ-\nent from the edited written language found in NLU\ncorpora; it is a “non-sentential” or “fragmental” ut-\nterance of a kind which is frequent in dialogue and\nnot at all syntactically or semantically malformed\n(Schlangen and Lascarides, 2002; Fernández and\nGinzburg, 2002). It also shows that the acts that are\nto be understood need not be linguistic ones; here,\nthe fragment itself is a reaction to a presumed men-\ntal state. (4) again is a request for action, this time\nin the guise of a question as an indirect speech act.\n(5) ﬁnally shows that an outcome of understanding\nin situated interaction can be that understanding\nitself can be adapted—here, we would expect an\nagent that has real understanding to be able to later\nuse the term that at that point was new to them.\nWhat this short example has shown, when con-\ntrasted with the example in Figure 2, is that SLU\ndiffers both on the side of the “input” (the act that is\nto understood) as well as on the side of the “output”,\nwhere the action space is much larger—in fact, in-\nﬁnite (but compositional). That is, SLU poses lan-\nguage tasks that do not occur in the text corpora\nused in NLU research. Even more importantly, the\nindividual acts of understanding (from one turn to\nthe next) are embedded in the general goal-directed\nstructured of the interaction as a whole; something\nthat cannot be captured in the i.i.d. (independent\nand identically distributed) nature of a static dataset.\nThis argues for ﬁnding a measurement instrument\nthat provides not only richer context information\nin a static way (as could be recorded in a richer\ndataset), but also an active embedding of language\nuse in varying, goal-directed interactions.\n5\nSLU Requires Different Benchmarking\nMethods: Dialogue Games\nThe scenario described above makes for a good use\ncase—having such a robot would be useful!—but a\nbad measurement instrument. One reason for that\nis that it simply is far out of reach of current tech-\nnology (not just in the language abilities, but also in\nthe physical abilities that it suggests). Any attempts\nat approximating such abilities with current tech-\nnologies would require making design choices that\nare more driven by the speciﬁc goal rather than by\ntesting language abilities. This also suggests a sec-\nond problem, which is that this scenario does not\nisolate the language abilities well enough to serve\nas a good test. We hence need more controlled\nsituations in which the situated language use can\nbe modelled, while preserving the goal-orientation\nexhibited by this scenario, as the structure it pro-\nvides is, as argued above, a crucial element that is\nnot captured by dataset-based methods.6\nThis discussion motivates the use of what I will\ncall Dialogue Games as benchmarking instrument,\n6This can be seen as a restriction compared to the general\nphenomenon of Situated Language Use: Not every language\nuse situation must necessarily be understood as goal-directed.\nHowever, if interaction episodes generally are seen as having\na beginning and an ending (Clark, 1996) and the notion of\nactivity types (Levinson, 1979) is accepted, a broad goal of\ngetting from beginning to ending can be assumed to be active\nin general.\nwhere:7,8\nA Dialogue Game is a constructed ac-\ntivity with a clear beginning an end, in\nwhich players attempt to reach a prede-\ntermined goal state primarily by means\nof producing and understanding linguis-\ntic material.\nIt is the goal-orientation and the constructed nature\nof the activity, as we will see, that makes it possible\nto target particular aspects of Situated Language\nUnderstanding, without conﬂating understanding\nwith recall performance on general world knowl-\nedge.\nBefore moving on to how such games can be\nconstructed in such a way that they make clear\nconnections to (assumptions about) underlying ca-\npabilities, and how they can be organised into a\nstrategic plan for making progress, we need to reg-\nister a cautionary note from the (long) history of\nthis type of approach. In 1972, Minsky and Papert\nintroduced the notion of “micro-world”, as a way to\nexplore “intelligence” problems in context: “we see\nsolving a problem often as getting to know one’s\nway around a ‘micro-world’ in which the problem\nexists” (Minsky and Papert, 1972). The most fa-\nmous of these micro-worlds is the “blocks-world”\nof the system SHRDLU (Winograd, 1972)—which\nwould count as a Dialogue Game according to the\ndeﬁnition above. SHRDLU seemed to demonstrate\nwhat I have called here Situated Language Under-\nstanding quite well, but criticism of the approach\nsoon arose, of which the following quote is repre-\nsentative (see also Dreyfus (1981); Marr (1982)):\nSHRDLU performs so glibly only because his do-\nmain has been stripped of anything that could\never require genuine wit or understanding. [...]\nNeglecting the tangled intricacies of everyday life\nwhile pursuing a theory of common sense is not\nlike ignoring friction while pursuing the laws of\nmotion; it’s like throwing the baby out with the\nbathwater. A round frictionless wheel is a good\napproximation of a real wheel because the devia-\ntions are comparatively small and theoretically lo-\ncalized; the blocks-world “approximates” a play-\nroom more as a paper plane approximates a duck.\n(Haugeland, 1985, p. 190)\n7Named of course with a nod to Wittgenstein’s language\ngames: “I shall call the whole, consisting of language and the\nactivities into which it is woven, a ‘language game”’ (Wittgen-\nstein, 1984 [1953], §7); with another inspiration coming from\nLevinson’s “activity types” (Levinson, 1979).\n8Note that this deﬁnition is general enough to cover “book\na train ticket” or even “interactively instruct agent to sum-\nmarise a text” under the name “game” as well.\nincremental \nprocessing \nincremental \nlearning\n  multimodal \ngrounding\nlanguage \nmodel\nsituation \nmodel\ndiscourse \nmodel\nactual \nreported \nsocial\nworld \nmodel\nagent model\nself\nothers\nrepresentational \nneeds\nanchoring processes\n  conversational \ngrounding\nFigure 3: Representational Domains (bottom) and An-\nchoring Processes (top) Structuring the Situated Agent\nThis gives us a warning to bring with us to the\nfurther discussion, which is to take care that any\nabstractions made in simulation shall not abstract\naway the real challenges. We will come to a delin-\neation of the design space in which we can search\nfor Games that meet this challenge in a moment,\nbut ﬁrst I will sketch a model of the capabilties\nunderlying SLU, to which we can then connect the\nGame taxonomy.\n6\nThe Construct: A Model of\nCapabilities Involved in SLU\nThe methodology described above (illustrated in\nFigure 1) rests on the benchmarking instruments\nbeing explicitly grounded in (assumed) capabilities\nthat are being tested. Elsewhere, I have developed\na model that distinguishes between various kinds\nof capabilities involved in SLU (Schlangen, 2023);\nthis will serve us here as the “nomological network\nof relationships between constructs” from Section 2\nabove.\nThis model, illustrated in Figure 3, assumes that\nthe agent represents what I call “knowledge do-\nmains”, and maintains “anchoring processes” that\noperate on them. The knowledge domains are as\nfollows: the language model (here meant to collate\nonly linguistic knowledge about the form\/meaning\nmapping; updated rarely), the world model (con-\ncepts, concept hierarchies, script knowledge, etc.;\nalso updated rarely), the situation model (details\nof the current conversational situation and\/or the\nreported situation; updated continuously), the dis-\ncourse model (what has been said so far, and how it\nrelates; discourse referents; updated continuously),\nand ﬁnally the agent model (of the beliefs, desires,\nintentions of agents, and recursively what it rep-\nresents of the participating agents; also updated\ncontinuously).\nAs anchoring processes (which “bind the agent\nto the here, now, and us”), there is incremental pro-\nEnvironment\nobject presence \nobject \/ scene familiarity \nimmediate \/ mediated \nreal \/ virtual \ndynamics \nmanipulability\nGame\nSetting\nspoken \/ written \nmutual observability \ntask feedback channel \ntask common ground \nenvironment common gr. \nrole division \nverbal action space \ngoal knowledge \ngoal type \nscoring \nanticipated strategy\nFigure 4: The main components of the proposed taxonomy\ncessing (updating situation and agent model, based\non minimal units of observation), conversational\ngrounding (the process of negotiating shared un-\nderstanding, for example through asking for clariﬁ-\ncation, if necessary), incremental learning (which\nranges from the establishment of spontaneous lo-\ncal conventions, e.g. on how to refer to objects,\nestablished during the conversational grounding, to\nlearning facts from observation and from testimony,\nand, crucially, from discussion and disagreement),\nand multimodal grounding (resolving references to\nobjects in the shared surrounding, as well as de-\nriving meaning from non-verbal actions such as\ngestures).9\nWhat this gives us is a ﬁner-grained picture\nof the construct: understanding means applying,\nbuilding up and maintaining these representations,\nvia these processes.10 Not all acts of understand-\ning rely on all aspects equally, and this makes it\npossible to develop a strategy for working towards\nmodelling the overall capability, as we will see.\nWith this in hand, we can now come to the de-\nsign aspects of Dialogue Games, and how they\nmay put certain of these capabilities (knowledge\ndomains and anchoring processes) more or less\ninto focus. You can read the following section also\nas advice on best practices in experiment design,\ndrawn from extensive experience in setting up what\nis here systematised as Dialogue Game (Fernán-\ndez et al., 2006; Kennington et al., 2013; Zarrieß\net al., 2016; Ilinykh et al., 2019; Attari et al., 2019;\nSchlangen, 2019a).\n7\nA Taxonomy of Game Types\nThe most salient aspect of a Dialogue Game might\nbe the task that it poses to the players; that is, the\ngoal state and how to get there. For what could\n9For a more detailed description and a justiﬁcation of this\nway of analysing the SLU agent, including references to prior\nwork making use of releated concepts, see the original paper\n(Schlangen, 2023).\n10Let me stress again that I am not making any claims about\nwhether such representations should be built into models of\nsituated language understanders or not; the (falsiﬁable) claim\nis just that something like them will be found in such models.\nbe called the “furniture assembly game” from Sec-\ntion 4, this would be have the furniture fully as-\nsembled (goal state) and provide required assis-\ntance (game “play”); for a more realistic Dialogue\nGame (of the type “reference game”, see below),\nthat could be bring cards into same order (goal\nstate) and ask each other which cards there are,\nand jointly decide on order (game play). But hid-\nden behind descriptions like these there is a large\nnumber of additional design decisions that need\nto be made before the game can be played. These\ndecisions have many degrees of freedom, but all\ncome with subtle inﬂuences on the shape of the\ninteraction, and on the phenomena that one can\nexpect to see in protocols of the game play.\nI distinguish here between three major areas in\nwhich decisions must be made when setting up\na concrete Dialogue Game: Enviroment, Setting,\nGame Proper; with many sub-aspects within. A\ncomprehensive overview is given in Appendix A.\nThe discussion here presents these design features\nand directly links them to the model of the under-\nlying construct(s) from the previous section.\n7.1\nEnvironment\nThis section groups together all design decisions\nthat inﬂuence what the relevant entities and ac-\ntions in the game are, and how they are pre-\nsented to the players.\nA high-level decision here is whether the game\nrequires talking about objects that are currently\npresent (in some form), or not. (An example of\na task that is not about currently present objects\nwould be booking a train ticket, which does re-\nquire talking about entities such as train stations,\nwithout them needing to be present. Such a con-\nversation is still situated in the sense that the in-\nterlocutors share time, but it is at the boundaries\nof what I consider here.) This decision inﬂuences\nhow the situation model is constructed (e.g., from\nvisual evidence or not) and how the world model is\nchallenged (because for non-present objects, agree-\nment on referents must come from prior common\nground). A different design dimension concerns\nprior knowledge of these objects, whether they\nare (expected to be) familiar to the players or not.\nWhether something is familiar or not depends on\nthe world model, and whether it is assumed to be\nmutually familiar on the agent model; succesfully\nreferring to unfamliar objects means more effort in\nconversational grounding.\nAnother decision is whether access of the play-\ners to the objects is immediate or mediated, and\nif mediated, if the objects are real or computer\nsimulated. (So, a video call would be mediated but\nreal; operating with representations on a computer\nscreen would be mediated and virtual.) Virtual en-\nvironments make further abstractions possible, for\nexample by discretising changes (the world “jumps”\nfrom one state to the next), or reducing the action\nspace (what can be done to and with objects). The\ndifference here is less one in what capabilities are\nchallenged than in the control that is given over the\nsituation; for example, a static environment will\nforce fewer updates to the situation model as one\nwith discrete updates, which in turn may require\nslower changes than one that is fully dynamic.\n7.2\nSetting\nThis dimension collects decisions about how the\nplayers can interact with each other—which of\ncourse determines to a large extent what kind of\ndata can be expected.\nA ﬁrst high-level decision here is whether the\nverbal interaction is done via speech, or through\ntyped messages. Written language, even in the dy-\nnamic form that it can take in chat interactions, is\na restricted channel compared to spoken language\n(where prosody and other para-linguistic informa-\ntion provides a channel for multimodal grounding);\nthe interaction also slows down and is, at least in\ntypical setups, more discretised (as messages need\nto be sent before they are seen; this inﬂuences the\ndegree to which incremental processing is chal-\nlenged). Finally, turn-taking, which is an essen-\ntial process in the organisation of free interaction\n(de Ruiter et al., 2006), works differently in chat\ncommunication than in spoken interaction. On the\nother hand, practical advantages in choosing typed\nmessages are also clear, in that written language is\ntypically easier to store, and (for artiﬁcial agents)\nto process, and to generate.\nAnother set of decisions concern what the play-\ners see of each other, and what they see of their\nactions in the environment. Multimodal ground-\ning of the signal in actions of the interlocutor is\nan important aspect of meaning making (Holler\nand Levinson, 2019); disabling it through hiding\nthe interloctor forces more meaning into the verbal\nchannel (which can be desired, but reduces ecologi-\ncal validity). Similarly, other aspects of interaction\nmanagement get harder when there is no visual\ncontact between interloctors (Brennan, 2000), but\nat same time become more visible in the linguistic\nmaterial.\nI have also grouped under this heading ques-\ntions of how common ground between the partici-\npants (other than what they can see of each other)\ncan form. If the players knowingly play repeated\nrounds of the game (from some initial state to a\nrespective goal state), they can build up personal\ncommon ground (Clark, 1996), a form of incremen-\ntal learning inﬂuencing their agent model.11 When\nplayers (knowingly) share the same environment\n(be that a simulated and mediated one or a real one;\nwhere even looking at the same image would count\nas sharing the environment), there is an automatic\nassumption that large parts of the respective situ-\nation models are shared (and represented as thus\nin the agent model); if this is not the case, or not\nknowingly so, linguistic labor must be performed\nto reach such common ground (if the tasks requires\nit).\n7.3\nGame\nThe decisions grouped here concern the game\nin the narrow sense: how initial state and goal\nstate are deﬁned, but also what the players know\nabout this, and how the games deﬁnes roles and\nsuggests strategies. In the terminology of Suits\n(1978), a game must subordinate under a prelu-\nsory goal, which can be stated independently of the\ngame (e.g., in football (soccer), “make the ball be in\nthe opponent team’s goal”); it is further deﬁned by\nconstitutive rules, which make reaching that goal\nmore difﬁcult than necessary (e.g., by disallowing\nto just grab the ball and carry it to the goal); it must\nalso trigger in the players a lusory attitude, which\nis the acceptance of the complications posed by the\nconstitutive rules. Doing the latter successfully can\nincrease the quality of the collected data, as players\nwith higher engagement can be expected to show a\nwider range of behaviours (von Ahn and Dabbish,\n2004). Inspiration can be taken here from the lit-\n11These days in Artiﬁcial Intelligence more typically called\n“theory of mind”, see e.g. (Bara et al., 2021).\nerature on the design of games (e.g., (Adams and\nDormans, 2012)); ultimately, however, the purpose\nof a Dialogue Game in the sense developed here\nis to provide data and a testing environment in a\nprincipled way, and not primarily enjoyment.\nTo classify games, one high-level aspect con-\ncerns the goal type, where we can distinguish ref-\nerence games (a time honoured instrument in Psy-\ncholinguistics, going back at least to (Krauss and\nWeinheimer, 1964); see (Ji et al., 2022) for a recent\noverview), which focus on reference and hence\nmultimodal grounding and alignment of situation\nmodels; information games, which center on the\nrequesting and giving of information, which de-\npending on the domain can lead to demands on the\nworld model and\/or the situation model; construc-\ntion games, which go beyond reference and infor-\nmation in that they require the execution of actions,\nand hence require coordination of the anchoring\nprocesses to a higher degree; navigation games,\nwhich center spatial language and spatially com-\nplex situation models; negotiation games, which\nfocus on explicit coordination of agent models; and\nﬁnally teaching games, which make explicit the\nincremental learning and how it updates the world\nmodel. This is not a complete categorisation, and\neach concrete game will contain elements of more\nthan one of these types; but this does represent\ngood coverage of types of games typically used.\n(We are currently preparing a comprehensive sur-\nvey of the ﬁeld, which will provide a plethora of\nreferences for representatives of all types.)\nSome more ﬁnal subdimensions. In the design of\nthe game, the player can be assigned distinct roles\nwith different responsibilities, such as for example\nan assigned questioner paired with an assigned an-\nswerer, or an instruction giver with an instruction\nfollower. The stricter these roles are, the lower the\ncoordination effort required, deemphasising func-\ntions such as conversational grounding and the\nkeeping of detailed agent models. Goal-directed\ngames naturally come with a notion of success, but\nbeyond that, scoring functions can be introduced\n(for example, “faster is better”; or a reconstruction\nloss for construction-instruction tasks). Making the\nscore known to players introduces incentives that\ncan change the dynamics of the interaction (e.g.,\nproritizing speed over accuracy, or vice versa).\nFinally, the design of the game can also make\na desired strategic behaviour more salient.\nA\ncooperative player would be one who does their\nbest in understanding intents behind requests (e.g.,\nthrough replying correctly to indirect speech acts,\nor to providing partial information when a question\ncannot be answered fully), whereas a collabora-\ntive player is one who takes their goal to be shared\nwith the other player, and who hence has an inter-\nest in being proactive as well—likely to be more\nchallenging to the anchoring processes and to the\nalignment of the agent models.12\n8\nDialogue Games as Evaluation\nInstrument\nLet us assume that we now have designed a Di-\nalogue Game, starting from ideas about which\naspects of the construct we particularly want to\nchallenge and making careful decisions on all the\ndesign features mentioned above. Of the ques-\ntions listed above in Section 2, we have an answer\nto numbers 1 to 3 (Q1: What is the construct?—\nA: The model in Section 6); Q2,3: Why and\nhow did we select measure? What measure to\noperationalise?—A: By making a decision to fo-\ncus on some aspects, and selecting according to\nSection 7). This leaves one crucial element, Q4:\nDeciding on how to quantify the measure. This for\nus translates into how to use the dialogue game to\nquantify the abilities of an artiﬁcial model, which\nis what this section will look into.\nDialogue games can be used as a means for data\ngeneration, simply by collecting game play from\npeople playing the game. Using a dialogue game\npromises to offer some control over the data that is\nto be expected, insofar as the connection between\nproperties of the game to underlying capabilities (as\ndiscussed in the previous section) is also reﬂected\nin properties of the language use. For example, a\nreference game will make the use of referring ex-\npressions likely; a game where mutual understand-\ning is particularly forwarded will make linguistic\ndevices for conversational grounding prominent\n(Schlangen, 2019a). This can be interesting for\nthe study of these linguistic phenomena (see, e.g.,\n12We note here that a demand for cooperation can lead to\nincreased coordination effort, which can result in the players\nnegotiating in the game to follow a merely cooperative strategy\n(with one instruction giver and one instruction follower), if\nthis seems more efﬁcient to them. This is something that\nwe have experienced with the MeetUp game (Ilinykh et al.,\n2019), where two players moving in separate copies of the\nsame virtual environment must manage to meet up in the same\nroom (without seeing each other), and which we had designed\nto trigger collaborative interactions; it however turned out to\nbe a frequent strategy for one player to just stop moving and\nonly answer questions by the other.\n(Fernández et al., 2006; Schlangen and Fernández,\n2007)). Moreover, the control over environment\nand setting makes it possible to record rich contex-\ntual information alongside with the language use\n(Kousidis et al., 2012).\nThis does not mean, however, that the use ends\nwith the recording of richer corpora, to then be used\nin the same way as the NLU corpora mentioned\nabove. In fact, as we have touched on above, using\nstatic corpora for research on SLU is problematic,\nas here the recorded actions can count even less\nas reference than they do in other generative tasks,\nsuch as machine translation.13\nIn this section, I will highlight some uses that\ngo beyond the train\/val\/test dataset paradigm typ-\nical of NLP, all revolving around the ability of\nDialogue Games—at least those with simulated\nenvironments—to serve as execution environments.\nRollout\nOne such use, which in a way stands be-\ntween the use of static corpora and the evaluation of\nagents in interactive game play, is nicely exempli-\nﬁed by the benchmarks built on the TEACh dataset\n(Padmakumar et al., 2022). The game, according to\nour taxonomy, uses a simulated, dynamic environ-\nment with familiar objects (household objects), is a\nNavigation Game with elements of an Information\nGame (as a Commander, for whom the environ-\nment is fully observable, instructs via a written\nchannel a Follower, for whom the environment is\nonly partially observable, to perform a task in the\nenvironment, with the Follower getting an opportu-\nnity to ask for clariﬁcation). Two tasks are deﬁned\nthat one may call rollout tasks (our terminology), in\nthe sense that they require the prediction of actions,\nbased on partial or complete history. Crucially,\nsince the environment simulator is provided, the\npredicted actions can be executed, and the evalu-\nation target is whether the required state changes\nhave been affected. This abstracts away at least to\na certain extent from what is recorded in the cor-\npus, as only the state changes (and not all actions)\nserve as reference. This type of evaluation is only\npossible in a Dialogue Game setup and not with a\nstatic corpus alone.\nAgent\/Agent Play\nIf artiﬁcial agents for all roles\nin the game are provided, another mode of evalua-\ntion comes available, that of fully simulated play.\n13Where metrics that compare model predictions against\na reference have long been criticised, see inter alia (Turian\net al., 2003); see (Liu et al., 2016) for an early extension of\nthis critique to the evaluation of “dialogue systems”.\nGames as deﬁned above will come with a some\nsort of score that measures success in reaching the\npre-deﬁned goal state, and this can then serve as\nthe evaluation target. If a human\/human reference\ncorpus is available, the produced language itself\ncan then furthermore be evaluated along formal pa-\nrameters (e.g., average turn length and distribution,\nvocabulary size, etc.).\nNote however that the agent\/agent mode leads\nto a even further deviation from the “test is like\ntraining” approach described above for NLU, as,\nunlike in non-communicative game like arcade-\ntype games—one of the early successes of new-\ngeneration reinforcement learning, e.g. (Mnih et al.,\n2013)—this setup can here not be used for learn-\ning the agents: In the language case, a competent\nplayer needs to already exist from which competent\nlanguage use can be learned.14\nWe note that Dialogue Games (in our sense, as\ncombination of environment, setting, and game)\nprovide an interesting perspective for the evalua-\ntion of (supposedly) general-purpose “foundation\nmodels” (Bommasani et al., 2021), which have\nbeen claimed to be able to function as general sim-\nulators of agents (Andreas, 2022).\nHuman\/Agent Play\nThe most informative eval-\nuation mode of interactive system remains evalua-\ntion in actual online interaction with human play-\ners.15 Beyond the measures deﬁned by the game\n(measuring task parameters) and other such mea-\nsures, which in the dialogue systems community\nusually are called objective measures such as dia-\nlogue length, etc. (see (Walker et al., 1998) for a\nseminal reference), this mode also makes it possi-\nble to evaluate the interactive experience, through\nphenomenological or subjective measures elicited\nin questionnaires (Kocaballi et al., 2019).16\nOne possible objection against this evaluation\nmode is quickly dismissed: While there may be\n14A problem that has led to the research area of “language\nemergence” in what could be called Dialogue Games between\ndeep reinforcement agents, where however the agents are\nallowed to coordinate on their own language system, folding\nthe language evolution and language learning problem into\none. (See e.g., (Lazaridou et al., 2017).) We concentrate here\non the setting of evaluating agents that have acquired (to the\nextent needed for the Game) an existing natural language.\n15Compare to what is called “human evaluation” in many\nﬁelds of NLP (see (Howcroft et al., 2020) for recent critical\noverview of human evaluation practices in Natural Language\nGeneration).\n16Although not many fully validated scales exist; a popular\none is Goodspeed questionnaire from the neighbouring ﬁeld\nof Human\/Robot interaction (Bartneck et al., 2009).\nSetting\n• spoken > typed\n• embodiment y > n\n• repeated y > n\n• view shared ~ part \n~ diﬀ\nGame\n• role equality > div.\n• action space unrestr. > \nrestr.\n• symmetry > asymmetry\n• negot. ~ instr. foll. > inf. \n> ref.\n• collab. > coop. > control\nEnvironment\n• present y ~ n\n• familiar y ~ n\n• real > simulated\n• high ﬁdelity ~ low\n• dynamic > static \nFigure 5: A partial order on the space of Dialogue Games. ∼denotes “similar complexity”, > denotes “leading to\nhigher complexity”.\na superﬁcial similarity to Turing’s imitation game\n(Turing, 1950)—what is now known as the “Turing\nTest”—in that quality of a conversation is an evalu-\nation criterion, it is important to note that deception\nabout whether a player is human or machine need\nnot be, and in most cases is not, part of the evalua-\ntion; and this is what is commonly criticised (see\ne.g., (Levesque, 2014)). To keep clear of the decep-\ntion incentive, however, it is important that such\nsubjective measures do not on their own become\noptimization targets and are always combined with\nobjective, task-oriented measures (Edlund et al.,\n2008).\nRepresentation Probing\nAnother way to gain\ninformation about a game-playing model is via\nrepresentation probing. To cite just one example,\nMadureira and Schlangen (2022) use existing dia-\nlogue models as what could be called “overhearers”\nof dialogues from a corpus, and probed whether\nthey, at a given state of the overheard dialogue, rep-\nresented information about the assumed common\nground status of propositions. Techniques such as\nthese can help further validate claims about the link\nbetween capabilities and games (if the assumption\nis that the game challenges a certain capability, we\nwould expect to ﬁnd information that it is based on\nto be represented during the model processing).\nLet us take stock. Our long journey has taken us\nfrom an argument that Situated Language Under-\nstanding needs to be evaluated in the context of,\nwell, situated interactions, through the claim that\nthe underlying construct is multi-faceted, to a re-\ncipy for constructing measurement instruments—\nDialogue Games. For a given Dialogue Game, the\nrecipy provides at least the beginnings of a valid-\nity argument, through the links between taxonomy\nand construct. We also now know that there is a\nvariety of ways of making use of the instrument\nand deriving from it a quantiﬁed measure, which\nin turn facilitaties (with the usual caveats) a com-\nparison between models. (“Bigger is better.”) The\nintroduction promised more, however, namely the\nderivation from this of a general strategy that might\nget us, eventually, from simple games to scenarios\nlike the robo-helper described above. This is what\nthe next section will discuss.\n9\nStrategy: From Simple to Complex\nDialogue Games\nIn this paper, I have tried to use insights from as-\nsessment in experimental psychology to suggest\nimprovements in practices in assessment in Arti-\nﬁcial Intelligence. There is a point, however, at\nwhich the similarities end. AI, as a constructive\ndiscipline, aims to build the artefacts it studies; ex-\nperimental psychology aims to understand existing\n(biological) systems. I speciﬁcally made the point\nin the introduction that situated interactions of the\nkinds discussed here come easy to most people; as\nassessments of understanding abilities for people,\nDialogue Games would not have much purchase.17\nOn the other hand, if we look at the current state of\nthe ﬁeld, it is clear that we are still at the relatively\nlow end of game complexity. To pick two exam-\nples, “visual dialogue” (Das et al., 2017) represents\nperhaps one of the simplest game types: player A\nsees an image and a caption, player B only the cap-\ntion; player B asks 10 questions about the image—\nwithout any further goal—which player A must\nanswer). The TEACh benchmark that was men-\ntioned above (Padmakumar et al., 2022) still relies\non a relatively simple game (the tasks are things\nlike “fetch a potato” or “put all plates on the table”),\nand still, the performance of even the best models\nis quite modest. The historical development that\nhas lead to these games is easy to reconstruct and\n17Perhaps more so when players are restricted to what for\nthem is not their ﬁrst language; but we leave this unexplored\nhere.\nleads to them from non-interactive tasks (image\ncaptioning, natural language navigation), extended\nto the “adjacent possible”.\nThe taxonomy described here offers a view to-\nwards what is not only adjacent and possible, but\nalso “uphill”. To make a given game more com-\nplex, a simple parameter is to increase the variabil-\nity of the entities that it involves. Then, restrictions\ncan be removed successively, e.g. by going from\na static to a dynamic environment, from typed to\nspoken interaction, and so on. (Figure 5 suggests\na partial complexity order by ranking possible fea-\nture values.) Following the discussion above, many\nof these changes will also shift or extend the way\nthe game challenges understanding, and a model\ncapable of this change thus shows itself to reach\na higher level. In this way, the taxonomy also\nsuggests a way to construct a meta-benchmark on\nwhich (hypothetical) general models of SLU can\nbe measured. (Among two models that perform\ncomparable on one game, that one will rank higher\nthat performs better on a more complex task.)\n10\nRelated Work\nWhat I call Dialogue Games here has been used\nfor a long time as instrument in driving forward\nresearch on situated language modelling. This is\nnot the contribution of this paper—there is a rich,\nand ever more strongly growing, literature making\nuse of such games (see (Duan et al., 2022; Gu et al.,\n2022; Sundar and Heck, 2022); and our forthcom-\ning general survey). What there is less work on is\non this instrument itself. Bisk et al. (2020) make\ngeneral points about types of information the avail-\nability of which during learning might improve the\n“understanding” of AI models. Fried et al. (2022)\ncover similar ground to this paper, but more gen-\nerally focus on the kinds of contexts needed for\ncertain pragmatic phenomena.\nThere are now several simulation environments\nthat support setting up Dialogue Games in simu-\nlated, continuous environments with high ﬁdelity\n(Gu et al., 2022). In my research group, we have\nfocussed on the development of a ﬂexible environ-\nment that makes the implementation of Dialogue\nGames and the collection of game play for example\nthrough crowd sourcing easier (Götze et al., 2022;\nSchlangen et al., 2018).\nFor a related argument for how SLU differs from\na monological perspective, taking a wider Cogni-\ntive Science perspective, see (Dingemanse et al.,\n2023).\n11\nConclusions\nAfter brieﬂy reviewing how experimental psychol-\nogy thinks about the validity of assessments, I re-\nviewed practices of evaluation in NLU, in the light\nof these considerations of validty. This served as\nthe foil on which to develop a guide for evaluat-\ning what I call Situated Language Understanding\n(SLU). I argued that SLU is different from NLU\n(Section 4) and hence requires different evaluation\ninstruments: Dialogue Games (Section 5). As an\nelement in the argument for the validity of this in-\nstrument, I reviewed a model of the capabilities\ninvolved in SLU, that is, of the internal structure\nof the construct that is to be measured (Section 6).\nThis was then followed by the description of a\ndetailed taxonomy of Dialogue Game types (Sec-\ntion 7), which can be read as a guide for construct-\ning a particular game in such a way that it can serve\nas an assessment instrument focussing on particular\naspects of the construct. Different ways of using a\ngiven game for evaluation were then discussed in\nSection 8, before Section 9 brought together these\ninsights into a discussion of how this suggests a\nordering of assessments from simpler to more com-\nplex, thereby suggesting a possible development\nstrategy for models in this space.\nEthics Statement\nLet us address the ethical elephant in the room.\nShould we even attempt to build systems that can\ndo this kind of situated language understanding?\nShould research be conducted on increasing the\ncomplexity of the tasks in which they can be used,\nas indicated in Section 9? It should be clear that\nthere are potential enormously beneﬁcial use cases,\nfor example where such systems are used to restore\nthe physical reach of humans that have lost abilities\n(or never had them). But understanding of the kind\ndiscussed here shows in action, making systematic\nfailures or biases of such systems potentially more\ndirectly harmful than for language-only systems:\nWhat holds for language models holds even more\nfor models with arms. The hope is that the method-\nology described here of starting with simpler set-\ntings and thoroughly evaluating performance on\nthem before moving on might provide one way in\nwhich this research can be made safer—although\nof course further work is needed on working out\nwhether this argument holds water.\nReferences\nErnest Adams and Joris Dormans. 2012.\nGame Me-\nchanics:\nAdvanced Game Design.\nNew Riders\nGames.\nJacob Andreas. 2022. Language models as agent mod-\nels.\nIn Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 5769–5779,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nNazia Attari, Martin Heckmann, and David Schlangen.\n2019. From explainability to explanation: Using a\ndialogue setting to elicit annotations with justiﬁca-\ntions. In Proceedings of SIGdial 2019, Short Papers,\nStockholm, Sweden.\nCristian-Paul Bara, Sky CH-Wang, and Joyce Chai.\n2021. MindCraft: Theory of mind modeling for sit-\nuated dialogue in collaborative tasks. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 1112–1125,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nChristoph Bartneck, Dana Kuli´c, Elizabeth Croft, and\nSusana Zoghbi. 2009.\nMeasurement instruments\nfor the anthropomorphism, animacy, likeability, per-\nceived intelligence, and perceived safety of robots.\nInternational Journal of Social Robotics, 1(1):71–\n81.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience grounds language. EMNLP 2020 - 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, Proceedings of the Conference,\npages 8718–8735.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, S. Buch, Dallas Card,\nRodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen, Kathleen A. Creel, Jared Davis, Dora Dem-\nszky, Chris Donahue, Moussa Doumbouya, Esin\nDurmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,\nLauren E. Gillespie, Karan Goel, Noah D. Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas F. Icard,\nSaahil Jain, Dan Jurafsky, Pratyusha Kalluri, Sid-\ndharth Karamcheti, Geoff Keeling, Fereshte Khani,\nO. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay\nKrishna, Rohith Kuditipudi, Ananya Kumar, Faisal\nLadhak, Mina Lee, Tony Lee, Jure Leskovec, Is-\nabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu\nMa, Ali Malik, Christopher D. Manning, Suvir P.\nMirchandani,\nEric Mitchell,\nZanele Munyikwa,\nSuraj Nair, Avanika Narayan, Deepak Narayanan,\nBenjamin Newman, Allen Nie, Juan Carlos Niebles,\nHamed Nilforoshan, J. F. Nyarko, Giray Ogut,\nLaurel Orr, Isabel Papadimitriou, Joon Sung Park,\nChris Piech, Eva Portelance, Christopher Potts,\nAditi Raghunathan, Robert Reich, Hongyu Ren,\nFrieda Rong, Yusuf H. Roohani, Camilo Ruiz,\nJack Ryan, Christopher R’e, Dorsa Sadigh, Sh-\niori Sagawa, Keshav Santhanam, Andy Shih, Kr-\nishna Parasuram Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W. Thomas, Florian Tramèr, Rose E.\nWang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\nWu, Sang Michael Xie, Michihiro Yasunaga, Jiax-\nuan You, Matei A. Zaharia, Michael Zhang, Tianyi\nZhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng,\nKaitlyn Zhou, and Percy Liang. 2021. On the op-\nportunities and risks of foundation models. ArXiv.\nSusan E. Brennan. 2000. Processes that shape conver-\nsation and their implications for computational lin-\nguistics. In Proceedings of the 38th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2000), Hong Kong, China.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes\/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nHerbert H. Clark. 1996. Using Language. Cambridge\nUniversity Press, Cambridge.\nAbhishek Das, Satwik Kottur, José M. F. Moura, Ste-\nfan Lee, and Dhruv Batra. 2017. Learning cooper-\native visual dialog agents with deep reinforcement\nlearning. In 2017 IEEE International Conference on\nComputer Vision (ICCV), pages 2970–2979.\nJ.P. de Ruiter, H. Mitterer, and N.J. Enﬁeld. 2006.\nProjecting the end of a speaker’s turn: a cognitive\ncornerstone of conversation. Language, 82(3):504–\n524.\nMark Dingemanse, Andreas Liesenfeld, Marlou Rasen-\nberg, Saul Albert, Felix K. Ameka, Abeba Birhane,\nDimitris Bolis, Justine Cassell, Rebecca Clift, Elena\nCuffari, Hanne De Jaegher, Catarina Dutilh No-\nvaes, N. J. Enﬁeld, Riccardo Fusaroli, Eleni Gre-\ngoromichelaki, Edwin Hutchins, Ivana Konvalinka,\nDamian Milton, Joanna R ˛aczaszek-Leonardi, Va-\nsudevi Reddy, Federico Rossano, David Schlangen,\nJohanna Seibt, Elizabeth Stokoe, Lucy Suchman,\nCordula Vesper, Thalia Wheatley, and Martina\nWiltschko. 2023.\nBeyond single-mindedness: A\nﬁgure-ground reversal for the cognitive sciences.\nCognitive Science, 47(1):e13230.\nHubert L. Dreyfus. 1981. From micro-worlds to knowl-\nedge: AI at an impasse. In John Haugeland, editor,\nMind Design. MIT Press.\nJiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu,\nand Cheston Tan. 2022. A Survey of Embodied AI:\nFrom Simulators to Research Tasks. IEEE Transac-\ntions on Emerging Topics in Computational Intelli-\ngence, 6(2):230–244.\nJens Edlund, Joakim Gustafson, Mattias Heldner, and\nAnna Hjalmarsson. 2008. Towards human-like spo-\nken dialogue systems.\nSpeech Communication,\n50:630–645.\nRaquel Fernández and Jonathan Ginzburg. 2002. Non-\nsentential utterances in dialogue: A corpus-based\nstudy.\nIn Proceedings of the Third SIGdial Work-\nshop on Discourse and Dialogue, pages 15–26,\nPhiladelphia, USA. ACL Special Interest Group on\nDialog.\nRaquel Fernández, Tatjana Lucht, Kepa Rodríguez, and\nDavid Schlangen. 2006. Interaction in task-oriented\nhuman–human dialogue: The effects of different\nturn-taking policies.\nIn Proceedings of the First\nInternational IEEE\/ACL Workshop on Spoken Lan-\nguage Technology, Palm Beach, Aruba.\nJessica Kay Flake and Eiko I. Fried. 2020.\nMea-\nsurement Schmeasurement : Questionable Measure-\nment Practices and How to Avoid Them. Advances\nin Methods and Practices in Psychological Science,\n3(4):456–465.\nMichael C. Frank, Mika Braginsky, Julie Cachia,\nNicholas Coles, Tom Hardwicke, Robert Hawkins,\nMaya Mathur, and Rondeline Williams. 2023. Ex-\nperimentology: An open science approach to experi-\nmental psychology methods. Website.\nDaniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Pa-\ntel, and Aida Nematzadeh. 2022.\nPragmatics in\ngrounded language learning: Phenomena, tasks, and\nmodeling approaches. CoRR, abs\/2211.08371.\nJana Götze, Maike Paetzel-Prüsmann, Wencke Lier-\nmann, Tim Diekmann, and David Schlangen. 2022.\nThe slurk interaction server framework: Better data\nfor better dialog models. In Proceedings of the Lan-\nguage Resources and Evaluation Conference, pages\n4069–4078, Marseille, France. European Language\nResources Association.\nJing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and\nXin Wang. 2022. Vision-and-language navigation:\nA survey of tasks, methods, and future directions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 7606–7623, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJohn Haugeland. 1985.\nArtiﬁcial Intelligence: The\nVery Idea. MIT Press, Cambridge, Mass.\nJudith Holler and Stephen C. Levinson. 2019. Multi-\nmodal Language Processing in Human Communica-\ntion. Trends in Cognitive Sciences, pages 1–14.\nDavid M. Howcroft, Anya Belz, Miruna-Adriana\nClinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad\nMahamood, Simon Mille, Emiel van Miltenburg,\nSashank Santhanam,\nand Verena Rieser. 2020.\nTwenty years of confusion in human evaluation:\nNLG needs evaluation sheets and standardised def-\ninitions. In Proceedings of the 13th International\nConference on Natural Language Generation, pages\n169–182, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nNikolai Ilinykh, Sina Zarrieß, and David Schlangen.\n2019. Meetup! a corpus of joint activity dialogues\nin a visual environment. In Proceedings of the 23rd\nWorkshop on the Semantics and Pragmatics of Dia-\nlogue (SemDial 2019 \/ LondonLogue), London, UK.\nAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr,\nWai Keen Vong, Robert Hawkins, and Yoav Artzi.\n2022.\nAbstract visual reasoning with tangram\nshapes. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 582–601, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nCasey Kennington,\nSpyros Kousidis,\nand David\nSchlangen. 2013. Interpreting situated dialogue ut-\nterances: an update model that uses speech, gaze,\nand gesture information.\nIn Proceedings of the\nSIGDIAL 2013 Conference, pages 173–182, Metz,\nFrance. Association for Computational Linguistics.\nAhmet Baki Kocaballi, Liliana Laranjo, and Enrico\nCoiera. 2019. Understanding and Measuring User\nExperience in Conversational Interfaces.\nInteract-\ning with Computers, 31(2):192–207.\nSpyros Kousidis, Thies Pfeiffer, Zoﬁa Malisz, Pe-\ntra Wagner, and David Schlangen. 2012. Evaluat-\ning a minimally invasive laboratory architecture for\nrecording multimodal conversational data. In Pro-\nceedings of the Interdisciplinary Workshop on Feed-\nback Behaviors in Dialog at Interspeech 2012, pages\n39–42, Stevenson, WA, USA.\nRobert M. Krauss and Sidney Weinheimer. 1964.\nChanges in reference phrases as a function of fre-\nquency of usage in social interaction: A preliminary\nstudy. Psychonomic Science, 1:266–278.\nSebastian Lapuschkin, Stephan Wäldchen, Alexander\nBinder, Grégoire Montavon, Wojciech Samek, and\nKlaus Robert Müller. 2019.\nUnmasking Clever\nHans predictors and assessing what machines really\nlearn. Nature Communications, 10(1):1–8.\nAngeliki Lazaridou, Alexander Peysakhovich, and\nMarco Baroni. 2017. Multi-agent cooperation and\nthe emergence of (natural) language. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nHector J. Levesque. 2014. On our best behaviour. Arti-\nﬁcial Intelligence, 212(1):27–35.\nStephen C. Levinson. 1979.\nActivity types and lan-\nguage. Linguistics, 17:365–399.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel J. Orr, Lucia\nZheng, Mert Yüksekgönül, Mirac Suzgun, Nathan\nKim, Neel Guha, Niladri S. Chatterji, Omar Khat-\ntab, Peter Henderson, Qian Huang, Ryan Chi,\nSang Michael Xie, Shibani Santurkar, Surya Gan-\nguli, Tatsunori Hashimoto, Thomas Icard, Tianyi\nZhang, Vishrav Chaudhary, William Wang, Xuechen\nLi, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\n2022.\nHolistic evaluation of language models.\nCoRR, abs\/2211.09110.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An\nempirical study of unsupervised evaluation metrics\nfor dialogue response generation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2122–2132, Austin,\nTexas. Association for Computational Linguistics.\nBrielen Madureira and David Schlangen. 2022. Can\nvisual dialogue models do scorekeeping? exploring\nhow dialogue representations incrementally encode\nshared knowledge. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 651–\n664, Dublin, Ireland. Association for Computational\nLinguistics.\nDavid Marr. 1982. Vision: A Computational Investi-\ngation into the Human Representation and Process-\ning of Visual Information. W.H. Freeman, San Fran-\ncisco, USA.\nMarvin Minsky and Seymour Papert. 1972. Progress\nReport on Artiﬁcial intelligence. Technical report,\nMIT Artiﬁcial Intelligence Laboratory, Cambridge,\nMass., USA.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAlex Graves, Ioannis Antonoglou, Daan Wierstra,\nand Martin A. Riedmiller. 2013. Playing atari with\ndeep reinforcement learning. CoRR, abs\/1312.5602.\nAishwarya Padmakumar, Jesse Thomason, Ayush Shri-\nvastava, Patrick Lange, Anjali Narayan-Chen, Span-\ndana Gella, Robinson Piramuthu, Gokhan Tur, and\nDilek Hakkani-Tur. 2022.\nTEACh: Task-Driven\nEmbodied Agents That Chat. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, pages\n2017–2025.\nDeborah Raji, Emily Denton, Emily M. Bender, Alex\nHanna, and Amandalynne Paullada. 2021. Ai and\nthe everything in the whole wide world benchmark.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks, vol-\nume 1. Curran.\nGilbert Ryle. 1949. The Concept of Mind. Hutchinson\n& Co.\nDavid Schlangen. 2019a. Grounded agreement games:\nEmphasizing conversational grounding in visual dia-\nlogue settings. CoRR, abs\/1908.11279.\nDavid Schlangen. 2019b. Language tasks and language\ngames: On methodology in current natural language\nprocessing research. CoRR, abs\/1908.10747.\nDavid Schlangen. 2021. Targeting the benchmark: On\nmethodology in current natural language processing\nresearch. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 2: Short Papers),\npages 670–674, Online. Association for Computa-\ntional Linguistics.\nDavid Schlangen. 2023. What A Situated Language-\nUsing Agent Must be Able to Do: A Top-Down\nAnalysis. CoRR, abs\/2302.08590.\nDavid Schlangen, Tim Diekmann, Nikolai Ilinykh, and\nSina Zarrieß. 2018.\nslurk – A Lightweight Inter-\naction Server For Dialogue Experiments and Data\nCollection. In Short Paper Proceedings of the 22nd\nWorkshop on the Semantics and Pragmatics of Dia-\nlogue (AixDial \/ semdial 2018).\nDavid Schlangen and Raquel Fernández. 2007. Speak-\ning through a noisy channel - experiments on\ninducing clariﬁcation behaviour in human-human\ndialogue.\nIn Proceedings of Interspeech 2007,\nAntwerp, Belgium.\nDavid Schlangen and Alex Lascarides. 2002.\nRe-\nsolving fragments using discourse information. In\nProceedings of the 6th International Workshop on\nFormal Semantics and Pragmatics of Dialogue\n(EDILOG 2002), pages 161–168, Edinburgh.\nStephen G. Sireci and Tia Sukin. 2013. Test Validity.\nIn K. F. Geisinger, editor, APA Handbook of Testing\nand Assessment in Psychology: Vol. 1. Test Theory\nand Testing and Assessment in Industrial and Orga-\nnizational Psychology, chapter 4. The American Psy-\nchological Association.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Rahane, Anantharaman S.\nIyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmüller, Andrew M. Dai, Andrew La, An-\ndrew K. Lampinen, Andy Zou, Angela Jiang, Angel-\nica Chen, Anh Vuong, Animesh Gupta, Anna Got-\ntardi, Antonio Norelli, Anu Venkatesh, Arash Gho-\nlamidavoodi, Arfa Tabassum, Arul Menezes, Arun\nKirubarajan, Asher Mullokandov, Ashish Sabhar-\nwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla\nKarakas, and et al. 2022.\nBeyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models. CoRR, abs\/2206.04615.\nBernard Suits. 1978. The Grasshopper: Games, Life,\nand Utopia.\nThe University of Toronto Press,\nToronto, Canada.\nAnirudh Sundar and Larry Heck. 2022.\nMultimodal\nconversational AI: A survey of datasets and ap-\nproaches. In Proceedings of the 4th Workshop on\nNLP for Conversational AI, pages 131–147, Dublin,\nIreland. Association for Computational Linguistics.\nJoseph P. Turian, Luke Shen, and I. Dan Melamed.\n2003. Evaluation of machine translation and its eval-\nuation. In Proceedings of Machine Translation Sum-\nmit IX: Papers, New Orleans, USA.\nAlan Turing. 1950. Computing machinery and intelli-\ngence. Mind, 59:433–460.\nLuis von Ahn and Laura Dabbish. 2004. Labeling im-\nages with a computer game. In Proceedings of the\nSIGCHI Conference on Human Factors in Comput-\ning Systems, CHI ’04, pages 319–326, New York,\nNY, USA. ACM.\nMarilyn A. Walker, Diane J. Litman, Candace A.\nKamm, and Alicia Abella. 1998. Evaluating spoken\ndialogue agents with PARADISE: Two case studies.\nComputer Speech and Language, 12(3).\nAlex Wang,\nYada Pruksachatkun,\nNikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a.\nSuper-\nGLUE: A Stickier Benchmark for General-Purpose\nLanguage Understanding Systems. In NeurIPS, July,\npages 1–30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In ICLR\n2019, pages 1–20.\nTerry Winograd. 1972.\nUnderstanding natural lan-\nguage. Cognitive Psychology, 3(1):1–191.\nLudwig Wittgenstein. 1984 [1953]. Tractatus Logicus\nPhilosophicus und Philosophische Untersuchungen,\nvolume 1 of Werkausgabe. Suhrkamp, Frankfurt am\nMain.\nSina Zarrieß, Julian Hough, Casey Kennington, Rames\nManuvinakurike, David DeVault, Raquel Fernández,\nand David Schlangen. 2016. Pentoref: A corpus of\nspoken references in task-oriented dialogues. In Pro-\nceedings of LREC 2016, Portoroz, Slovenia.\nA\nExample Appendix\nTable 1 shows the full taxonomy, with explanations\nof all attributes and values.\nCategory \/ Feature\nPossible Values\nDescription\nEnvironment\nCharacterises the relevant objects and conﬁgura-\ntions, and how they are presented to players\npresence\nready-to-hand \/ absent\nare players talking about objects that are immedi-\nately perceivable to them or not?\nobject familiarity\ninstance familiar \/ type fa-\nmiliar \/ unfamiliar\nprior to game, do they know object instance (Barack\nObama), type (fridge), or likely not at all (pento\npieces)\nscene familiarity\ninstance familiar \/ type fa-\nmiliar \/ unfamiliar\nsame for constellations of objects\naccess\nimmediate \/ mediated\nare objects physically present or via interface?\nreality\nreal \/ virtual\nare objects real or computer represented?\nﬁdelity\nhigh \/ low\nrealism of representation\ndynamics\ncontinuous \/ discrete \/\nstatic\nhow changes of the environment proceed\nenv action space\nobject\nmanipulation\n\/\nviewpoint man. \/ none\nwhat can be done to environment\nSetting\nCharacterises how the players can interact with each\nother\nverbal action channel\nspoken \/ written\nw\/ variations on turn taking, e.g. \"free turn taking\",\n\"push to talk\", \"turn-based\", \"round-robin\"\nmutual observability\nreal \/ avatar \/ none\nwhether other player is visible \/ embodied\ntask action channel\nin-environment\n\/\nsym-\nbolic feedback \/ none\nhow actions of other player are perceived (symb\nfeedback would be e.g.\njust info whether they\npicked the right object, w\/o player seeing the pick-\ning action)\ntask common ground\nsingle game \/ repeated\ngames\nwhether players (knowingly) play repeated rounds\nenv. common ground\nfull \/ partial \/ none\nwhether players are in same environment or not\nGame\nCharacterises the goal of the interaction and the\nconstraints on how to reach it\nrole equality\nequal\n\/\nspecialised\n\/\nsequentially-equal\ndo players have the same action space or not (e.g.,\ninstruction giver \/ follower); \"sequentially-equal\"\nmeaning they swap roles\nverbal action space\nunrestricted \/ restricted\nwhether they can talk freely or are limited to range\nof utterances (e.g., just \"yes\" or \"no\"); by player\ngoal information\nsymmetric \/ asym. \/ com-\nplementary \/ none\nwhether one player has solution, or both, or both\nhave (different) parts; none means neither player\nknows more than general goal spec (e.g., like in\nchess)\ngoal type\n(games can contain sev-\neral goal types)\nreference\nidentify object(s)\ninformation\nrequest \/ provide information\nconstruction\nconﬁgure objects\nnavigation\ngo somewhere \/ direct somewhere\nnegotiation\nagree on something\nteaching\nteach \/ learn something\nscoring\nbinary \/ graded \/ none\nmeasure of immediate task success (Interaction as\na whole might additionally be evaluated otherwise\nas well)\nscore impact\nyes \/ no\nwhether players are motivated to achieve good score\nanticipated strategy\ncooperative \/ collabora-\ntive \/ anti-collaborative\nby player; whether player is expected to facilitate\nother player’s goals, or has own goals which coin-\ncide (or not) w\/ other player and for which other\nplayer is needed\nTable 1: The proposed ﬁne-grained classiﬁcation scheme\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy.pdf"}
{"title":"MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields","authors":"Jiaying Lu, Yongchen Qian, Shifan Zhao, Yuanzhe Xi, Carl Yang","summary":"Previous research has demonstrated the advantages of integrating data from\nmultiple sources over traditional unimodal data, leading to the emergence of\nnumerous novel multimodal applications. We propose a multimodal classification\nbenchmark MuG with eight datasets that allows researchers to evaluate and\nimprove their models. These datasets are collected from four various genres of\ngames that cover tabular, textual, and visual modalities. We conduct\nmulti-aspect data analysis to provide insights into the benchmark, including\nlabel balance ratios, percentages of missing features, distributions of data\nwithin each modality, and the correlations between labels and input modalities.\nWe further present experimental results obtained by several state-of-the-art\nunimodal classifiers and multimodal classifiers, which demonstrate the\nchallenging and multimodal-dependent properties of the benchmark. MuG is\nreleased at https:\/\/github.com\/lujiaying\/MUG-Bench with the data, tutorials,\nand implemented baselines.","url":"http:\/\/arxiv.org\/abs\/2302.02978v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.02978v2","published":1675706946000,"comment":null,"pdf_text":"MUG: A Multimodal Classification Benchmark on Game Data with\nTabular, Textual, and Visual Fields\nJiaying Lu∗and Yongchen Qian∗and Shifan Zhao and Yuanzhe Xi and Carl Yang\nEmory University\n{jiaying.lu, yongchen.qian, shifan.zhao, yuanzhe.xi, j.carlyang}@emory.edu\nAbstract\nPrevious research has demonstrated the advan-\ntages of integrating data from multiple sources\nover traditional unimodal data, leading to the\nemergence of numerous novel multimodal ap-\nplications. We propose a multimodal classi-\nfication benchmark MUG with eight datasets\nthat allows researchers to evaluate and im-\nprove their models. These datasets are col-\nlected from four various genres of games that\ncover tabular, textual, and visual modalities.\nWe conduct multi-aspect data analysis to pro-\nvide insights into the benchmark, including la-\nbel balance ratios, percentages of missing fea-\ntures, distributions of data within each modal-\nity, and the correlations between labels and\ninput modalities. We further present experi-\nmental results obtained by several state-of-the-\nart unimodal classifiers and multimodal classi-\nfiers, which demonstrate the challenging and\nmultimodal-dependent properties of the bench-\nmark. MUG is released at https:\/\/github.\ncom\/lujiaying\/MUG-Bench with the data, tu-\ntorials, and implemented baselines.\n1\nIntroduction\nThe world surrounding us is multimodal. Real-\nworld data is often stored in well-structured\ndatabases that contain tabular fields, with textual\nand visual fields co-occurring. Numerous auto-\nmated classification systems have been deployed\non these multimodal data to provide efficient and\nscalable services. For instance, medical decision\nsupport systems (Soenksen et al., 2022) utilize pa-\ntients’ electronic health record data that contains\ntabular inputs (e.g., ages, genders, races), textual\ninputs (e.g., notes, prescriptions, written reports),\nand visual inputs (e.g., x-rays, magnetic resonance\nimaging, ct-scans) to help precise disease predic-\ntion. Similarly, e-commerce product classification\nsystems (Erickson et al., 2022) categorize prod-\nucts based on their categorical\/numerical quanti-\n*These authors contributed equally to this work\nname: Clawfury Adept\ntext: Give all other friendly \ncharacters +1 attack this turn\nattack: 2\nhealth: 3\ncost: 2\nrarity: Common\ntype: Minion\ncardClass =? (ground truth: Druid)\nname: Pidgey\nability_1: Keen Eye\nability_2: Tangled Feet\ngeneration: 1\nheight_m: 0.3\nweight_kg: 1.8\ncatch_rate: 70\ngrowth_rate: Medium Slow\ntype_2 =?  (ground truth: Flying)\nFigure 1: Illustration of data examples from MUG. Note\nthat inputs cover tabular, textual and visual modalities,\nand the task is multiclass classification.\nties, textual descriptions, and teasing pictures, thus\nenhancing user search experiences and recommen-\ndation outcomes. Therefore, accurate classification\nmodels for table-text-image input are desired.\nDeep neural networks have shown significant\nprogress in multimodal learning tasks, such as\nCLIP (Radford et al., 2021) for image-text re-\ntrieval and Fuse-Transformer (Shi et al., 2021)\nfor tabular-with-text classification. This progress\nhas been made with large-scale datasets provided\nto train the data-eager models. So far, there ex-\nist many datasets (Ovalle et al., 2017; Wu et al.,\n2021; Shi et al., 2021; Qu et al., 2022; Lin et al.,\n2020; Lee et al., 2019; Kautzky et al., 2020; Srini-\nvasan et al., 2021) that cover one or two modali-\nties. However, the progress in tabular-text-image\nmultimodal learning lags due to the lack of avail-\nable resources. In this paper, we provide a mul-\ntimodal benchmark, namely MUG, that contains\neight datasets for researchers to examine their al-\narXiv:2302.02978v2  [cs.LG]  17 Oct 2023\ngorithms’ multimodal perception ability. MUG\ncontains data samples with tabular, textual, and vi-\nsual fields that are collected from various genres of\ngames. We have made necessary cleaning, transfor-\nmations, and modifications to the original data to\nmake MUG easy to use. We further conduct com-\nprehensive data analysis to demonstrate the diverse\nand multimodal-dependent properties of MUG.\nMUG can enable future studies of many mul-\ntimodal tasks, and we focus on the multimodal\nclassification task in this paper. For the primary\nclassification evaluation, we incorporate two state-\nof-the-art (SOTA) unimodal classifiers for each of\nthe three input modalities, resulting in a total of\nsix, along with two SOTA multimodal classifiers.\nWe also propose a novel baseline model MUGNET\nbased on the graph attention network (Veliˇckovi´c\net al., 2018).\nIn addition to capturing the in-\nteractions among the three input modalities, our\nMUGNET takes the sample-wise similarity into\naccount, yielding a compatible performance to ex-\nisting multimodal classifiers. We further conduct\nefficiency evaluations to reflect the practical re-\nquirements of many machine learning systems.\n2\nRelated Works\n2.1\nMultimodal Classification Datasets with\nTabular, Textual, and Visual Fields\nMachine learning models in real-world applications\nneed to deal with multimodal data that contains\nboth tabular, textual, and visual fields. Due to pri-\nvacy or license issues, there exist very few datasets\nthat cover these three modalities. To the best of our\nknowledge, PetFinder1 is one of the few publicly\navailable datasets. HAIM-MIMIC-MM (Soenksen\net al., 2022) is a multimodal healthcare dataset\ncontaining tabular, textual, image, and time-series\nfields. However, only credentialed users can access\nHAIM-MIMIC-MM. On the other hand, there exist\nmany datasets that cover two modalities (out of\ntable, text, and image modalities). The most com-\nmon datasets are the ones with both textual and\nvisual features (MM-IMDB (Ovalle et al., 2017),\nV-SNLI (Vu et al., 2018), MultiOFF (Suryawan-\nshi et al., 2020), WIT (Srinivasan et al., 2021),\nMELINDA (Wu et al., 2021), etc.). Meanwhile,\n(Shi et al., 2021; Xu et al., 2019; Qu et al., 2022)\nprovide benchmark datasets for table and text\nmodalities, For the combination of table and image\n1PetFinder: https:\/\/www.kaggle.com\/competitions\/\npetfinder-adoption-prediction\/overview\nmodalities, there are a bunch of datasets from the\nmedical domain (Lin et al., 2020; Lee et al., 2019;\nKautzky et al., 2020; Gupta et al., 2020). Other\nthan the mentioned table, text, and image modali-\nties, multimodal learning has also been conducted\nin time-series, speech, and video modalities (Zhang\net al., 2022, 2020; Li et al., 2020).\n2.2\nMultimodal Classifiers for Tabular,\nTextual, and Visual Fields\nFusion is the core technology for multimodal classi-\nfication problems, which integrates data from each\ninput modality and utilizes fused representations\nfor downstream tasks (classification, regression,\nretrieval, etc.). Based on the stage of fusion (Iv\net al., 2021), existing methods can be divided into\nearly, late, or hybrid fusion. Early fusion mod-\nels (Sun et al., 2019; Shi et al., 2021; Zhu et al.,\n2021) usually fuse raw data or extracted features\nbefore they are fed into the learnable classifier,\nwhile late fusion models (Erickson et al., 2020;\nSoenksen et al., 2022; Lu et al., 2022) employ sep-\narate learnable encoders for all input modalities\nand then fuse these learned representations into\nthe learnable classifier. Hybrid fusion models are\nmore flexible, allowing for modality fusion to oc-\ncur at different stages simultaneously (Qingyun\net al., 2021; Li et al., 2021). Although existing\nworks have demonstrated remarkable capability\nin modeling feature interactions, they ignore sig-\nnals of sample proximity, such as the tendency for\nwithin a group to exhibit similar behavior or share\ncommon interests. In response, we propose our ap-\nproach, MUGNET, which dynamically constructs\ngraphs based on sample similarity and effectively\ncombines graphical representation learning with\nmultimodal fusion. Our approach draws inspira-\ntion from pioneering graph neural networks (Guo\net al., 2021; Wang et al., 2021; Georgantas and\nRichiardi, 2022), which have achieved success in\nvarious classification tasks.\n3\nMUG: the benchmark\nWe create and release MUG with eight datasets\nfor multimodal classification with tabular, text, and\nimage fields to the community for future studies.\nRaw data and examples of how to appropriately\nload the data are provided in https:\/\/github.\ncom\/lujiaying\/MUG-Bench. MUG is under the\n\"CC BY-NC-SA 4.0\" license2, and is designated to\nuse for research purposes.\n3.1\nData Sources\nTo collect multiple and large-scale datasets that\nsupport multimodal automated machine learning,\nwe collected data from four games: Pokémon,\nHearthstone, League of Legends, and Counter-\nStrike: Global Offensive. We deliberately chose\nthese video games as they have distinct video game\ngenres (e.g., role-playing, card, multiplayer online\nbattle arena, and shooting). All of these datasets\nwere gathered from publicly accessible web con-\ntent by October 2022, and there are no licensing\nissues associated with them. They do not contain\nany user-specific private information. In particular,\n• Pokémon is a video game centered around fic-\ntional creatures called \"Pocket Monsters\" that\ntrainers capture and train to battle each other.\nPokémon is owned by Nintendo Co., Ltd., Crea-\ntures Inc., and Game Freak Inc.\nPokémon\ndata is collected from https:\/\/bulbapedia.\nbulbagarden.net\/wiki under the “CC BY-NC-\nSA 2.5” license.\n• HearthStone is an online collectible card game\ndeveloped by Blizzard Entertainment, Inc., featur-\ning strategic gameplay where players build decks\nand compete against each other using a variety of\nspells, minions, and abilities. Hearthstone data\nis collected from https:\/\/hearthstonejson.\ncom\/ under the “CC0” license.\n• League of Legends (LoL) is a multiplayer online\nbattle arena (MOBA) video game developed by\nRiot Games, Inc. where teams of players compete\nin fast-paced matches, utilizing unique champi-\nons with distinct abilities to achieve victory. LoL\ndata is collected from https:\/\/lolskinshop.\ncom\/product-category\/lol-skins\/.\n• Counter-Strike: Global Offensive (CS:GO) is\na multiplayer first-person shooter video game\ndeveloped by Valve Corporation and Hidden\nPath Entertainment, Inc., where players join\nteams to compete in objective-based matches\ninvolving tactical gameplay and precise shoot-\ning. CS:GO data is collected from https:\/\/www.\ncsgodatabase.com\/.\n2CC BY-NC-SA 4.0: https:\/\/creativecommons.org\/\nlicenses\/by-nc-sa\/4.0\/\n3.2\nCreation Process\nTo create MUG, we first identify the categorical\ncolumns that can serve as the prediction targets.\nThe reasons for choosing these targets are elabo-\nrated in Appendix B.1. We obtain a total of eight\ndatasets from the four games, including pkm_t1\nand pkm_t2 from Pokémon; hs_ac, hs_as, hs_mr,\nand hs_ss; lol_sc from LoL; csg_sq from CS:GO.\nThen, we conduct necessary data cleaning and\nverification to ensure the quality of MUG. To allevi-\nate the class imbalance issue in some datasets (e.g.,\none class may contain less than 10 samples), we\nre-group sparse classes into one new class that con-\ntains enough samples for training and evaluation.\nFor missing values of target categorical columns,\nwe manually assign a special None_Type as one\nnew class of the dataset. For missing values of\ninput columns, we keep them blank to allow classi-\nfication models to decide the best imputation strate-\ngies. Moreover, we also anonymize columns that\ncause data leakage (e.g., the id column in hs_as is\ntransformed to anonymous_id column).\nAfter the abovementioned preprocessing, we\nsplit the dataset into training, validation, and test-\ning sets with an 80\/5\/15 ratio. Each dataset com-\npromises between 1K and 10K samples, associ-\nated with tabular, textual, and visual features. An\noverview of these datasets is shown in Table 1.\nThis broad range of sample sizes and diverse data\ntypes ensures the representation of a wide variety\nof instances, allowing for robust model training and\nevaluation across different data modalities.\n3.3\nBenchmark Analysis\nThe MUG benchmark is curated to meet the fol-\nlowing list of criteria:\n(i) Publicly available data and baseline models can\nfacilitate reproducible experiments and accelerate\nthe development of advanced models.\n(ii) Diversity should be preserved in the bench-\nmark. We do not want the benchmark to have a\nbias toward certain data or class distribution. The\nbenchmark with a high variety of datasets aids the\nresearch community in examining the robustness\nof models.\n(iii) Multimodal-dependent classification is ex-\npected for each dataset. Datasets that are too easy\nto be classified by a single modality are not suit-\nable, since they would hide the gap between the\nmultimodal perceptron abilities of models.\nWe conduct a rich set of analyses to verify\nDataset\nGame\nPred. Target\n#Row\n#Class\n#Feat (tab\/txt\/img)\npkm_t1\nPokémon\nPrimary Type\n719\/45\/133\n18\n23 (17\/5\/1)\npkm_t2\nPokémon\nSecondary Type\n719\/45\/133\n19\n23 (17\/5\/1)\nhs_ac\nHearthStone\nAll card’s Category\n8569\/536\/1605\n14\n18 (12\/5\/1)\nhs_as\nHearthStone\nAll card’s Set\n8566\/533\/1607\n38\n18 (12\/5\/1)\nhs_mr\nHearthStone\nMinion card’s Race\n5421\/338\/1017\n16\n13 (7\/5\/1)\nhs_ss\nHearthStone\nSpell card’s School\n2175\/170\/508\n8\n11 (5\/5\/1)\nlol_sc\nLoL\nSkin Category\n1000\/64\/188\n7\n11 (3\/7\/1)\ncsg_sq\nCS:GO\nSkin Quality\n766\/49\/141\n6\n7 (5\/1\/1)\nTable 1: The statistics of the eight datasets in MUG.\npkm_t1\npkm_t2\nhs_ac\nhs_as\nhs_mr\nhs_ss\nlol_sc\ncsg_sq\n0.6\n0.8\n1\n0.94\n0.68\n0.76\n0.93\n0.64\n0.58\n0.73\n0.81\nShannon Equitability\n(a) Class balance ratios.\npkm\nhs_a\nhs_m\nhs_s\nlol\ncsg\n70\n80\n90\n100\nPercentage(%)\nValid\nMissing\n(b) Percentages of missing features.\n101\n102\n103\n100\n101\n102\n103\nMean\nStandard Deviation\npkm\nhs\nlol\ncsg\n(c) Means and SD of numerical features.\npkm_t1\npkm_t2\nhs_ac\nhs_as\nhs_mr\nhs_ss\nlol_sc\ncsg_sq\n2\n8\n32\n#Categories\n(d) Counts of categorical features.\n4\n16\n64\n0\n20\n40\nWord count\nPercentage(%)\npkm\nhs\nlol\ncsg\n(e) Distributions of word counts.\nR\n0\n50\n100\n150\n200\nG\n0\n50\n100\n150\n200\nB\n0\n50\n100\n150\n200\npkm\nhs\nlol\ncsg\n(f) Distributions of mean RGB pixels.\nFigure 2: Multi-aspect of data analysis for MUG (duplicated datasets are merged into one group).\nthat MUG indeed satisfied the diversity require-\nment. Figure 2 shows the properties of datasets\nin multi-aspect. For the classification task proper-\nties(Figure 2a), we adopt the Shannon equitability\nindex (Shannon, 1948) (definition in Appendix B.2)\nto measure the class balance ratio.\nThe index\nranges from 0 to 1, and the larger the Shannon\nequitability index, the more balanced the dataset is.\nFor the feature properties, we include percentages\nof missing features (Figure 2b), means and stan-\ndard deviations of numerical features (Figure 2c),\ncategory counts of categorical features (Figure 2d),\ndistributions of word counts per sample (Figure 2e),\nand distributions of image mean RGB pixel values\n(Figure 2f). In these figures, we merged dupli-\ncated results from some datasets into one group to\nmake the presentation clean and neat (i.e., pkm_t1,\npkm_t2 are grouped into pkm; hs_ac, hs_as, hs_mr,\nhs_ss are grouped into hs). As shown in the fig-\nures, the eight datasets reflect real-world problems\nthat are diverse and challenging. We further study\nthe correlation between category labels and input\nmodalities in MUG. Referring to the t-SNE pro-\njection of multimodal embeddings in Figure 7, it\nis evident that MUG exhibits a strong multimodal\ndependency. In this case, the use of unimodal in-\nformation alone is inadequate to differentiate be-\ntween samples belonging to different classes. For\na more comprehensive analysis, we encourage in-\nterested readers to refer to the details provided in\nAppendix B.3\n4\nBaseline Models\nWe employ several state-of-the-art unimodal classi-\nfiers and multimodal classifiers in the experiments.\nWe also proposed our own graph neural network-\nbased multimodal classifier as one baseline model\nto be compared.\n4.1\nExisting State-Of-The-Art Classifiers\nIn this paper, we adopt the following SOTA uni-\nmodal classifiers in the experiments:\nTabular modality classifiers:\n• GBM (Ke et al., 2017) is a light gradient boosting\nframework based on decision trees. Due to its\nability to capture nonlinear relationships, handle\ncomplex tabular data, provide feature importance\ninsights, and robustness to outliers and missing\nvalues, GBM has achieved state-of-the-art results\nin various tabular data tasks,\n• tabMLP (Erickson et al., 2020) is a multilayer\nperceptron (MLP) model that is specifically de-\nsigned to work with tabular data. tabMLP con-\ntains multiple separate embedding layers to han-\ndle categorical and numerical input features.\nTextual modality classifiers:\n• RoBERTa (Liu et al., 2019) is a robustly\noptimized transformer-based masked language\nmodel (masked LM). RoBERTa builds upon the\nsuccess of BERT by refining and optimizing its\ntraining methodology, and achieves superior per-\nformance on a wide range of NLP tasks.\n• Electra (Clark et al., 2020) is another variant of\nthe transformer-based model, which differs from\ntraditional masked LMs like BERT or RoBERTa.\nWhile masked LMs randomly mask tokens and\npredict these masked tokens, Electra is trained as\na discriminator to identify whether each token is\nreplaced by a generator.\nVisual modality classifiers:\n• ViT (Dosovitskiy et al., 2020) extends the trans-\nformer model to image data, by dividing the input\nimage into a grid of patches and processing each\npatch as a token. Empirical results show that ViT\noutperforms previous SOTA convolutional neural\nnetworks in image classification tasks.\n• SWIN (Liu et al., 2021) is another vision trans-\nformer that benefits from hierarchical architec-\nture and the shifted windowing scheme. The pro-\nposed techniques address several key challenges\nwhen adapting transformers in image modality,\nsuch as large variations in the scale of visual\nentities and the high resolution of pixels.\nIn practice, we adopt the following multimodal\nclassifiers in the experiments:\n• AutoGluon (Erickson et al., 2022) is an\nensemble-learning model for multimodal clas-\nsification and regression tasks. The concept of\nAutoGluon is stack ensembling, where the final\nprediction is obtained by combining intermediate\npredictions from multiple base models. To han-\ndle multimodal classification, SOTA unimodal\nclassifiers (e.g., tree models, MLPs, CNNs, trans-\nformers) are adopted as base models.\n• AutoMM (Shi et al., 2021) is a late-fusion model\nwhere separate neural operations are conducted\non each data type and extracted high-level repre-\nsentations are aggregated near the output layer.\nSpecifically, MLPs are used for tabular modal-\nity, and transformers are used for text and image\nmodalities. After that, dense vector embeddings\nfrom the last layer of each network are pooled\ninto one vector, and the final prediction is ob-\ntained via an additional two-layer MLP.\n4.2\nMUGNET\nMUGNET is our own multimodal classifier which\nis further proposed as a competitor to existing mod-\nels. We propose three key components to make\nMUGNET a powerful graph neural network for\nthe multimodal classification task. They are adap-\ntive multiplex graph construction module, GAT en-\ncoder module, and attention-based fusion module,\nas shown in Figure 3. Firstly, adaptive multiplex\ngraphs are constructed to reflect sample-wise simi-\nlarity within each modality. Then, separate GAT en-\ncoders (Veliˇckovi´c et al., 2018) are employed to ob-\ntain dense embeddings of samples, by propagating\ninformation between neighbors. Finally, tabular,\ntext and image embeddings are combined by inter-\nmodality attention to obtaining the fused embed-\nding for multimodal classification. GNNs (Yang\net al., 2020; Guo et al., 2021) show great capability\nto leverage the graph structure, propagate informa-\ntion, integrate features, and capture higher-order\nrelationships. This leads to accurate and robust\nclassification performance across various domains.\nsamples\nTabular\nText\nImage\nsamples\nsamples\nGATtab\nGATtxt\nGATimg\nTab. embs\nFused embs.\nMUGNET\nAdaptive Multiplex\nGraph Construction\nsamples\nInter-modality \nAttention\nTxt. embs\nImg. embs\nFigure 3: Model architecture of MUGNET.\nIn this work, we propose to regard the whole sam-\nples as a correlation network (Wang et al., 2021;\nGeorgantas and Richiardi, 2022) that represents\nsample-to-sample similarities, while existing mul-\ntimodal classifiers rarely consider this before.\nAdaptive multiplex graph construction module.\nFollowing the notation defined in §5.1, the adaptive\nmultiplex graph construction module first utilizes\npre-processing pipelines (e.g., monotonically in-\ncreasing integer mapping for categorical inputs, no\nalteration for numerical inputs) or pre-trained fea-\nture extractors (e.g., CLIP (Radford et al., 2021)\nfor text and image inputs) to obtain dense mul-\ntimodal features F = f(XL) ∈RN×(dt+ds+di),\nwhere F = {Ft, Fs, Fi} denotes feature matri-\nces for tabular, text, and image modalities. The\nadaptive multiplex graph construction module then\nderives multiplex sample-wise similarity graph\nG = {Gt, Gs, Gi} = {(At, Ft),\n(As, Fs), (Ai, Fi)}, where each modality-specific\nadjacency matrix Am ∈RN×N, ∀m ∈{t, s, i} is\ncalculated based on the multimodal features\nAm\ni,j = sim(Fm\ni , Fm\nj ).\n(1)\nIt is worth noting that the sample-wise similarity\nfunction sim is adaptive, and is chosen from co-\nsine similarity, radial basis function (RBF) ker-\nnel, or k-nearest neighbor. For these modality-\nspecific graphs, we use separate hyperparameters\n(e.g., threshold for score-based functions, or the\nvalue of k for k-nearest neighbor) to control their\nsparsity properties. The similarity function and its\nassociated hyperparameters are determined through\nhyperparameter tuning (Liaw et al., 2018) on the\nheld-out validation set, so that the multiplex graph\nconstruction is adaptive to any downstream task.\nGAT encoder module.\nWe use the power-\nful multi-head graph attention neural network\n(GAT) (Veliˇckovi´c et al., 2018) as the encoder to\nobtain structure-aware representations of samples.\nSeparate GATs are employed for each view of the\nmultiple graph, so that Hm = GAT(Am, Fm; θ),\nwhere Hm ∈RN×dm\nh , and θ represents the learn-\nable parameters of the GAT encoder. We want to\nstate there is no information leakage in MUGNET,\nbecause we follow the inductive learning setting\nof GNNs (Hamilton et al., 2017) where the GAT\nencoder is trained on the multiplex graph G de-\nrived from labeled training samples XL, and new\nunseen multiplex graph is derived from all sam-\nples XL ∪XU at the inference stage.\nFurther-\nmore, we adopt a graph sampling technique (Graph-\nSAINT (Zeng et al., 2019)) during the GAT train-\ning process, to improve the efficiency and general-\nization. The graph sampling technique essentially\nsamples a subgraph by random walks for each train-\ning step, thus the “neighbor explosion” issue is al-\nleviated with a constrained number of neighbors\nper node and the variance of GAT is reduced with\nfewer outliers or noise in the sampled graph.\nAttention-based fusion module. After we obtain\nthe structure-aware embeddings of samples from\nthe tabular, text, and image modalities Ht, Hs, Hi,\nthe attention-based fusion module is responsible\nfor fusing them into one single embedding via\nthe attention-based fusion module. The attention\nweight αm\nj ∈R for j-th sample of modality m is\ncomputed as:\nαm\nj =\nexp(em\nj )\nP\nm′∈{t,s,j} exp(em′\nj ),\n(2)\nem\nj = wa2 · tanh(W m\na1hm\nj ),\n(3)\nwhere em\nj ∈R denotes the unnormalized attention\nweight, wa2 ∈Rdm\na ×1, Wa1 ∈Rdm\nh ×dm\na denote\nlearnable parameters, and hm\nj ∈Rdm\nh denotes the\nj-th row of Hm (i.e., embedding of j-th sample of\nmodality m). The fused embedding of j-th sample\nis then calculated by:\nhj = αt\njht\nj + αs\njhs\nj + αi\njhi\ni.\n(4)\nThe fused embedding hj incorporates cross-\nmodalities interactions and provides a complete\ncontext for the downstream tasks.\nAn addi-\ntional two-layer MLP is trained to predict the\ncategory of j-th sample ˆyj = softmax(Wcls2 ·\nLeakyReLU(Wcls1hj)). We adopt cross-entropy\nbetween prediction ˆy and target y as MUGNET’s\nloss function.\n5\nExperiments\n5.1\nProblem Definition\nGiven a finite set of categories Y and labeled train-\ning pairs (xi, yi) ∈XL × Y, multimodal classifica-\ntion aims at finding a classifier ˆf : XL →Y such\nthat ˆyj = ˆf(xj) is a good approxmiation of the\nunknown label yj for unseen sample xj ∈XU. It\nis worth noting that the each multimodal sample\nx ∈XL ∪XU consists of tabular fields t, textual\nfields s, and image fields i (i.e., x = {t, s, i}).\n5.2\nExperimental Setup\nWe use the official training, validation, and testing\nsplits provided by MUG to conduct experiments.\nWe choose the log-loss and accuracy to evaluate\nmodel performance, since these metrics are rea-\nsonable and commonly used in previous studies.\nFor comparable and reproducible results, all mod-\nels are trained and tested using the same hardware.\nSpecifically, the machine is equipped with 16 Intel\nXeon Gold 6254 CPUs (18 cores per CPU) and one\n24GB TITAN RTX GPU. We add an 8-hour time\nlimitation for the training process to reflect real-\nworld resource constraints. The implementation\nand hyperparameter details of evaluated models are\nput in Appendix C.\n5.3\nPerformance Comparisons\nTable 2a and 2b show the performance of all eval-\nuated models on MUG. As can be seen, multi-\nmodal classifiers (except AutoMM) consistently\noutperform unimodal classifiers in both log-loss\nand accuracy.\nIt demonstrates that the classifi-\ncation tasks in MUG are multimodal-dependent\nwhere each modality only conveys partial informa-\ntion about the required outputs. Among the three\nmultimodal classifiers we used, AutoGluon and\nMUGNET are the top-2 models with well-matched\nperformances. In Table 2a and 2b, AutoGluon\nachieves the best performance eight times, while\nMUGNET also achieves the best performance eight\ntimes. More specifically, AutoGluon is superior in\nlog-loss whereas MUGNET has better accuracy\nscores. AutoMM performs the worst among multi-\nmodal classifiers, and it sometimes underperforms\nunimodal classifiers. Considering that AutoMM\ntrains powerful deep neural networks on a small\nscale of datasets and we have observed the gap\nbetween the training loss and validation loss, it is\nhighly possible that AutoMM is overfitting. While\nAutoGluon and MUGNET also adopt deep neu-\nral networks as base models, they are more robust\nsince AutoGluon proposes a repeated bagging strat-\negy and MUGNET utilizes graph sampling tech-\nniques to avoid overfitting. Among unimodal clas-\nsifiers, tabular models seem to outperform textual\nand visual models in most cases (six out of eight\ndatasets). There is a slight performance gain com-\nparing textual models to visual models because\ntextual models are better on five datasets.\nTo better understand the overall performance of\nmodels across multiple datasets, we propose using\ncritical difference (CD) diagrams (Demšar, 2006).\nIn a CD diagram, the average rank of each model\nand which ranks are statistically significantly dif-\nferent from each other are shown. Figure 4a and\n4b show the CD diagrams using the Friedman test\nwith Nemenyi post-hoc test at p < 0.05. In sum-\nmary, we observe that AutoGluon and MUGNET\nrespectively achieve the best rank among all tested\nmodels with respect to log-loss and accuracy, al-\nthough never by a statistically significant margin.\nMoreover, tabular models obtain higher ranks than\nother unimodal classifiers. The similar observa-\ntions from Table 2 and Figure 4 support that effec-\ntively aggregating information across modalities is\ncritical for the multimodal classification task.\nMethod\npkm_t1\npkm_t2\nhs_ac\nhs_as\nhs_mr\nhs_ss\nlol_sc\ncsg_sq\nUnimodal Classifiers\nGBM\n1.838\n2.038\n0.911\n2.352\n0.913\n0.603\n0.198\n1.107\ntabMLP\n1.442\n1.909\n1.172\n2.155\n1.247\n0.672\n0.533\n0.718\nRoBERTa\n1.834\n2.191\n1.999\n2.393\n1.920\n1.254\n0.847\n0.734\nElectra\n2.907\n2.179\n2.118\n3.155\n2.085\n1.263\n0.611\n0.757\nViT\n3.680\n2.543\n1.527\n2.786\n1.032\n2.056\n2.049\n0.835\nSwin\n2.657\n2.229\n2.018\n2.795\n2.089\n1.397\n1.470\n0.750\nMultimodal Classifiers\nAutoGluon\n0.973\n1.507\n0.654\n1.793\n0.403\n0.350\n0.159\n0.631\nAutoMM\n1.736\n2.029\n1.987\n2.193\n1.836\n1.320\n0.792\n0.674\nMUGNET\n1.000\n1.499\n0.922\n1.499\n0.321\n0.442\n0.248\n0.654\n(a) Results in ‘log-loss’ (the less the better).\nMethod\npkm_t1\npkm_t2\nhs_ac\nhs_as\nhs_mr\nhs_ss\nlol_sc\ncsg_sq\nUnimodal Classifiers\nGBM\n0.489\n0.489\n0.726\n0.421\n0.737\n0.795\n0.963\n0.610\ntabMLP\n0.662\n0.481\n0.627\n0.377\n0.617\n0.776\n0.851\n0.681\nRoberta\n0.662\n0.466\n0.475\n0.366\n0.535\n0.683\n0.883\n0.688\nElectra\n0.120\n0.466\n0.475\n0.168\n0.535\n0.683\n0.878\n0.702\nViT\n0.308\n0.406\n0.568\n0.236\n0.787\n0.593\n0.436\n0.674\nSwin\n0.346\n0.451\n0.470\n0.248\n0.536\n0.657\n0.431\n0.702\nMultimodal Classifiers\nAutoGluon\n0.744\n0.617\n0.787\n0.495\n0.879\n0.882\n0.963\n0.766\nAutoMM\n0.639\n0.511\n0.475\n0.415\n0.549\n0.671\n0.888\n0.738\nMUGNET\n0.774\n0.669\n0.724\n0.572\n0.908\n0.880\n0.968\n0.745\n(b) Result in ‘accuracy’ (the more the better).\nTable 2: Overall experimental results with explicit modality performance. The bold text represents the best\nperformance and the underlined text represents the runner-up performance.\n2\n4\n6\n8\nAutoGluon\nMUGNET\ntabMLP\nGBM\nAutoMM\nRoberta\nElectra\nViT\nSwin\nCD=4.247\n(a) Log-loss.\n2\n4\n6\n8\nMUGNET\nAutoGluon\nGBM\nAutoMM\ntabMLP\nRoberta\nElectra\nViT\nSwin\nCD=4.247\n(b) Accuracy.\nFigure 4: The critical difference diagrams show the mean ranks of each model for the test data of the eight datasets.\nThe lower rank (further to the right) represents the better performance of a model. Groups of models that are not\nsignificantly different (p < 0.05) are connected by thick lines.\n5.4\nEfficiency Evaluations\nAlthough accuracy (or other metrics such as log-\nloss in our case) is the central measurement of a\nmachine learning model, efficiency is also a prac-\ntical requirement in many applications. Trade-off\noften exists between how accurate the model is\nGBM\ntabMLP\nRoBERTa\nElectra\nViT\nSwin\nAutoGluon\nAutoMM\nMUGNET\n101\n102\n103\n104\nTraining Duration (seconds)\nFigure 5: Training duration on all datasets.\nand how long it takes to train and infer the model.\nTherefore, we record the training durations and test\ndurations of models to examine their efficiency. In\nFigure 5, we show the aggregated training dura-\ntion of evaluated models via a box plot. As can\nbe seen, tabular models require an order of magni-\ntude less training duration than the other models,\nwhile AutoGluon stands out as requiring signif-\nicantly longer training duration. Among tabular\nmodels, tabMLP is 4x faster than GBM in terms\nof the median training duration. Except for tabular\nmodels and AutoGluon, other models are approxi-\nmately lightweight to train. It is worth noting that\nAutoGluon hits the 8-hour training duration con-\nstraint on every dataset, thus the variance of its\ntraining durations across datasets is very small.\nIn Figure 6, we show the trade-offs between\nmean inference time and mean accuracy of models.\nSince the accuracy is not commensurable across\ndatasets, we first normalize all accuracies through\na dataset-wise min-max normalization. After the\nnormalization, the best model in each dataset is\nscaled to 1 while the worst model is scaled to 0.\nFinally, we take the average on the normalized ac-\ncuracies and the test durations to draw the scatter\nplot. When both accuracy and efficiency are objec-\ntives models try to improve, there does not exist a\nmodel that achieves the best in both objectives si-\nmultaneously. As an illustration, MUGNET has the\nhighest test accuracy, but tabMLP has the fastest\ninference speed. Therefore, we adopt the Pareto-\noptimal3 concept to identify which models achieve\n“optimal” trade-offs. Pareto-optimal is widely used\nin the decision-making process for multi-objective\noptimization scenarios. By definition, a solution\nis Pareto-optimal if any of the objectives cannot\n3Pareto-optimal Definition: https:\/\/w.wiki\/6sLB\nbe improved without degrading at least one of the\nother objectives. Following this concept, we ob-\nserve that tabMLP, GBM, and MUGNET are the\nmodels with the best trade-offs between accuracy\nand efficiency, as these models reside in the Pareto\nfrontier in Figure 6. Meanwhile, other models are\nsuboptimal with regard to this trade-off, since we\ncan always find a solution that has higher accu-\nracy and better efficiency simultaneously than these\nmodels.\n10−2\n10−1\n100\n101\n102\n103\n0\n0.2\n0.4\n0.6\n0.8\n1\nGBM\ntabMLP\nRoBERTa\nElectra\nViT\nSwin\nAutoGluon\nAutoMM\nMUGNET\nTest Duration (seconds)\nTest Accuracy\nFigure 6: Mean testing duration and mean normalized\naccuracy tradeoffs on all datasets.\n6\nConclusion\nThis paper presents a benchmark dataset MUGNET\nalong with multiple baselines as a starting point for\nthe machine learning community to improve upon.\nMUGNET is a multimodal classification bench-\nmark on game data that covers tabular, textual, and\nvisual modalities. All eight datasets and nine evalu-\nated baselines are open-source and easily-extended\nto motivate rapid iteration and reproducible ex-\nperiments for researchers. A comprehensive set\nof analyses is included to provide insight into the\ncharacteristics of the benchmark. The experimen-\ntal results reported in the paper are obtained from\nmodels trained with constrained resources, which\nis often required by real-life applications. However,\nwe also welcome future works that utilize enor-\nmous resources. Finally, we hope this work can\nfacilitate research on multimodal learning, and we\nencourage any extensions to MUGNET to support\nnew tasks or applications such as open-domain re-\ntrieval, AI-generated content, multimodal QA, etc.\nLimitations\nWhile our study emphasizes the importance of effi-\nciency in real-world machine learning applications,\nwe acknowledge certain limitations in our approach.\nSpecifically, we deliberately focused on training\nand evaluating relatively \"small\" models within the\ncontext of the current era of large vision and lan-\nguage models (LVLMs) (Li et al., 2023; Liu et al.,\n2023; Lu et al., 2023). As a result, the performance\nof LVLMs on our proposed MUG benchmark re-\nmains unexplored. Early exploration (Hegselmann\net al., 2023) about applying large language models\non tabular classification shows that LLMs can be\ncompetitive with strong tree-based models. Based\non the explorations and conducted experiments, we\nspeculate LLVMs can not beat efficient ensemble\nor GNN baselines using the same training time\nconstraint. However, to provide a comprehensive\nunderstanding of multimodal classification, further\nresearch is expected. It would be also intriguing to\ninvestigate the performance of LLVMs when pro-\nvided with unlimited training (fine-tuning) time.\nAcknowledgement\nThis research is partly supported by the National\nInstitute Of Diabetes And Digestive And Kidney\nDiseases of the National Institutes of Health under\nAward Number K25DK135913 and the Division\nof Mathematical Sciences of the National Science\nFoundation under Award Number 2208412. Any\nopinions, findings, and conclusions or recommen-\ndations expressed herein are those of the authors\nand do not necessarily represent the views, either\nexpressed or implied, of the National Science Foun-\ndation, National Institutes of Health, or the U.S.\ngovernment.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn ICCV.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020.\nELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nJanez Demšar. 2006. Statistical comparisons of classi-\nfiers over multiple data sets. The Journal of Machine\nlearning research.\nXiangjue Dong, Jiaying Lu, Jianling Wang, and James\nCaverlee. 2023. Closed-book question generation via\ncontrastive learning. In EACL.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers for\nimage recognition at scale. In ICLR.\nNick Erickson, Jonas Mueller, Alexander Shirkov,\nHang Zhang, Pedro Larroy, Mu Li, and Alexander\nSmola. 2020. Autogluon-tabular: Robust and ac-\ncurate automl for structured data. arXiv preprint\narXiv:2003.06505.\nNick Erickson, Xingjian Shi, James Sharpnack, and\nAlexander Smola. 2022. Multimodal automl for im-\nage, text and tabular data. In KDD.\nCosta Georgantas and Jonas Richiardi. 2022. Multi-\nview omics translation with multiplex graph neural\nnetworks. In WWW.\nXiawei Guo, Yuhan Quan, Huan Zhao, Quanming Yao,\nYong Li, and Weiwei Tu. 2021. Tabgnn: Multiplex\ngraph neural network for tabular data prediction. In\ndlp-kdd.\nYubraj Gupta, Ji-In Kim, Byeong Chae Kim, and Goo-\nRak Kwon. 2020. Classification and graphical anal-\nysis of alzheimer’s disease and its prodromal stage\nusing multimodal features from structural, diffusion,\nand functional neuroimaging data and the apoe geno-\ntype. Frontiers in aging neuroscience.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017.\nInductive representation learning on large graphs.\nNeurIPS.\nHuan He, Shifan Zhao, Yuanzhe Xi, and Joyce C Ho.\n2023. Meddiff: Generating electronic health records\nusing accelerated denoising diffusion model. arXiv\npreprint arXiv:2302.04355.\nStefan Hegselmann, Alejandro Buendia, Hunter Lang,\nMonica Agrawal, Xiaoyi Jiang, and David Sontag.\n2023. Tabllm: Few-shot classification of tabular data\nwith large language models. In AISTATS. PMLR.\nMD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shi-\nratuddin, and Hamid Laga. 2019. A comprehensive\nsurvey of deep learning for image captioning. ACM\nComputing Surveys.\nMahesh G Huddar, Sanjeev S Sannakki, and Vijay S\nRajpurohit. 2018. An ensemble approach to utterance\nlevel multimodal sentiment analysis. In CTEMS.\nWilliam C Sleeman Iv, Rishabh Kapoor, and Preetam\nGhosh. 2021. Multimodal classification: Current\nlandscape, taxonomy and future directions. CSUR.\nA Kautzky, T Vanicek, C Philippe, GS Kranz,\nW Wadsak, M Mitterhauser, A Hartmann, A Hahn,\nM Hacker, D Rujescu, et al. 2020. Machine learning\nclassification of adhd and hc by multimodal seroton-\nergic data. Translational psychiatry.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2017. Lightgbm: A highly efficient gradient boosting\ndecision tree. NeurIPS.\nMackenzie Leake, Hijung Valentina Shin, Joy O Kim,\nand Maneesh Agrawala. 2020. Generating audio-\nvisual slideshows from text articles using word con-\ncreteness. In CHI.\nJeungchan Lee, Ishtiaq Mawla, Jieun Kim, Marco L\nLoggia, Ana Ortiz, Changjin Jung, Suk-Tak Chan,\nJessica Gerber, Vincent J Schmithorst, Robert R Ed-\nwards, et al. 2019. Machine learning-based predic-\ntion of clinical pain using multimodal neuroimaging\nand autonomic metrics. Pain.\nJiao Li, Xing Xu, Wei Yu, Fumin Shen, Zuo Cao, Kai\nZuo, and Heng Tao Shen. 2021. Hybrid fusion with\nintra-and cross-modality attention for image-recipe\nretrieval. In SIGIR.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In ICML.\nMingzhe Li, Xiuying Chen, Shen Gao, Zhangming\nChan, Dongyan Zhao, and Rui Yan. 2020. Vmsmo:\nLearning to generate multimodal summary for video-\nbased news articles. In EMNLP.\nPaul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun\nCheng, Jason Wu, Leslie Yufan Chen, Peter Wu,\nMichelle A Lee, Yuke Zhu, et al. 2021. Multibench:\nMultiscale benchmarks for multimodal representa-\ntion learning. In NeurIPS.\nRichard Liaw, Eric Liang, Robert Nishihara, Philipp\nMoritz,\nJoseph E Gonzalez,\nand Ion Stoica.\n2018.\nTune: A research platform for distributed\nmodel selection and training.\narXiv preprint\narXiv:1807.05118.\nWeiming Lin, Qinquan Gao, Jiangnan Yuan, Zhiying\nChen, Chenwei Feng, Weisheng Chen, Min Du, and\nTong Tong. 2020. Predicting alzheimer’s disease con-\nversion from mild cognitive impairment using an ex-\ntreme learning machine-based grading method with\nmultimodal data. Frontiers in aging neuroscience.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. 2021.\nSwin transformer: Hierarchical vision transformer\nusing shifted windows. In ICCV.\nJiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo,\nYawen Zhang, Baochen Sun, Carl Yang, and Jie\nYang. 2023. Evaluation and mitigation of agnosia in\nmultimodal large language models. arXiv preprint\narXiv:2309.04041.\nJiaying Lu, Xin Ye, Yi Ren, and Yezhou Yang. 2022.\nGood, better, best: Textual distractors generation\nfor multiple-choice visual question answering via\nreinforcement learning. In CVPR Workshop on O-\nDRUM.\nLiqiang Nie, Mengzhao Jia, Xuemeng Song, Ganglu\nWu, Harry Cheng, and Jian Gu. 2021. Multimodal\nactivation: Awakening dialog robots without wake\nwords. In SIGIR.\nJohn Edison Arevalo Ovalle, Thamar Solorio, Manuel\nMontes-y-Gómez, and Fabio A. González. 2017.\nGated multimodal units for information fusion. In\nICLR.\nAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann,\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\nDipanjan Das. 2020. ToTTo: A controlled table-to-\ntext generation dataset. In EMNLP.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011.\nScikit-learn: Machine learning in\nPython. JMLR, 12:2825–2830.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,\nShaohan Huang, Shuming Ma, and Furu Wei.\n2023.\nKosmos-2: Grounding multimodal large\nlanguage models to the world.\narXiv preprint\narXiv:2306.14824.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben\nMildenhall. 2022. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988.\nFang Qingyun, Han Dapeng, and Wang Zhaokui. 2021.\nCross-modality fusion transformer for multispectral\nobject detection. arXiv preprint arXiv:2111.00273.\nYubin Qu, Fang Li, Long Li, Xianzhen Dou, and Hong-\nmei Wang. 2022. Can we predict student perfor-\nmance based on tabular and textual data?\nIEEE\nAccess.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In ICML.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gener-\nation. In ICML.\nShailaja Keyur Sampat, Maitreya Patel, Subhasish Das,\nYezhou Yang, and Chitta Baral. 2022. Reasoning\nabout actions over visual and linguistic modalities: A\nsurvey. arXiv preprint arXiv:2207.07568.\nClaude Elwood Shannon. 1948. A mathematical the-\nory of communication. The Bell system technical\njournal.\nXingjian Shi, Jonas Mueller, Nick Erickson, Mu Li, and\nAlex Smola. 2021. Multimodal automl on structured\ntables with text fields. In 8th ICML Workshop on\nAutoML.\nLuis R Soenksen, Yu Ma, Cynthia Zeng, Leonard Bous-\nsioux, Kimberly Villalobos Carballo, Liangyuan Na,\nHolly M Wiberg, Michael L Li, Ignacio Fuentes, and\nDimitris Bertsimas. 2022. Integrated multimodal\nartificial intelligence framework for healthcare appli-\ncations. NPJ digital medicine.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. In SIGIR.\nBaohua Sun, Lin Yang, Wenhan Zhang, Michael Lin,\nPatrick Dong, Charles Young, and Jason Dong. 2019.\nSupertml: Two-dimensional word embedding for the\nprecognition on structured tabular data. In CVPR\nWorkshops.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,\nXiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. 2023.\nGenerative pretraining in multimodality.\nShardul Suryawanshi, Bharathi Raja Chakravarthi, Mi-\nhael Arcan, and Paul Buitelaar. 2020. Multimodal\nmeme dataset (multioff) for identifying offensive con-\ntent in image and text. In Proceedings of the second\nworkshop on trolling, aggression and cyberbullying.\nGolsa Tahmasebzadeh, Endri Kacupaj, Eric Müller-\nBudack, Sherzod Hakimov, Jens Lehmann, and\nRalph Ewerth. 2021. Geowine: Geolocation based\nwiki, image, news and event retrieval. In SIGIR.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2020. Mul-\ntimodalqa: complex question answering over text,\ntables and images. In ICLR.\nJialin Tian, Kai Wang, Xing Xu, Zuo Cao, Fumin Shen,\nand Heng Tao Shen. 2022. Multimodal disentangle-\nment variational autoencoders for zero-shot cross-\nmodal retrieval. In SIGIR.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. JMLR.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In ICLR.\nHoa Trong Vu, Claudio Greco, Aliia Erofeeva, Somayeh\nJafaritazehjan, Guido Linders, Marc Tanti, Alberto\nTestoni, Raffaella Bernardi, and Albert Gatt. 2018.\nGrounded textual entailment. In COLING.\nMinjie Yu Wang. 2019. Deep graph library: Towards\nefficient and scalable deep learning on graphs. In\nICLR workshop on representation learning on graphs\nand manifolds.\nTongxin Wang, Wei Shao, Zhi Huang, Haixu Tang,\nJie Zhang, Zhengming Ding, and Kun Huang. 2021.\nMogonet integrates multi-omics data using graph con-\nvolutional networks allowing patient classification\nand biomarker identification. Nature Communica-\ntions.\nTe-Lin Wu, Shikhar Singh, Sayan Paul, Gully Burns,\nand Nanyun Peng. 2021. Melinda: A multimodal\ndataset for biomedical experiment method classifica-\ntion. In AAAI.\nXueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Text-\nto-table: A new way of information extraction. In\nACL.\nCheng Xu, Xiaofeng Hou, Jiacheng Liu, Chao Li, Tian-\nhao Huang, Xiaozhi Zhu, Mo Niu, Lingyu Sun, Peng\nTang, Tongqiao Xu, et al. 2022. Mmbench: Bench-\nmarking end-to-end multi-modal dnns and under-\nstanding their hardware-software implications. arXiv\ne-prints arXiv:2212.01241.\nKeyang Xu, Mike Lam, Jingzhi Pang, Xin Gao, Char-\nlotte Band, Piyush Mathur, Frank Papay, Ashish K\nKhanna, Jacek B Cywinski, Kamal Maheshwari, et al.\n2019. Multimodal machine learning for automated\nicd coding. In Machine learning for healthcare con-\nference.\nCarl Yang, Jieyu Zhang, Haonan Wang, Sha Li, Myung-\nwan Kim, Matt Walker, Yiou Xiao, and Jiawei Han.\n2020.\nRelation learning on social networks with\nmulti-modal graph edge variational autoencoders. In\nWSDM.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\nbria, and Louis-Philippe Morency. 2017. Tensor fu-\nsion network for multimodal sentiment analysis. In\nEMNLP.\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Ra-\njgopal Kannan, and Viktor Prasanna. 2019. Graph-\nsaint:\nGraph sampling based inductive learning\nmethod. In ICLR.\nZhixiong Zeng, Shuai Wang, Nan Xu, and Wenji Mao.\n2021. Pan: Prototype-based adaptive network for\nrobust cross-modal retrieval. In SIGIR.\nDong Zhang, Xincheng Ju, Junhui Li, Shoushan Li,\nQiaoming Zhu, and Guodong Zhou. 2020. Multi-\nmodal multi-label emotion detection with modality\nand label dependence. In EMNLP.\nYi Zhang, Mingyuan Chen, Jundong Shen, and\nChongjun Wang. 2022. Tailor versatile multi-modal\nlearning for multi-label emotion recognition.\nIn\nAAAI.\nYitan Zhu, Thomas Brettin, Fangfang Xia, Alexander\nPartin, Maulik Shukla, Hyunseung Yoo, Yvonne A\nEvrard, James H Doroshow, and Rick L Stevens.\n2021. Converting tabular data into images for deep\nlearning with convolutional neural networks. Scien-\ntific reports.\nAppendix\nA\nBroad Impact of Multimodal Datasets\nfor Tasks beyond Classification\nThe proposed multimodal classification benchmark,\nMUG, is a valuable resource to inspire future stud-\nies for tasks including but not limited to classifica-\ntion. First, it is natural to utilize MUG to examine\nthe model’s abilities to understand the complex\nworld surrounding us (Liang et al., 2021; Xu et al.,\n2022). The multimodal perception ability is crucial\nfor open-domain retrieval (Tahmasebzadeh et al.,\n2021; Tian et al., 2022; Zeng et al., 2021), multi-\nmodal classification (Zadeh et al., 2017; Huddar\net al., 2018), multimodal question answering (An-\ntol et al., 2015; Talmor et al., 2020; Lu et al., 2022),\ninteractive robots (Nie et al., 2021; Sampat et al.,\n2022), precision medicine (Soenksen et al., 2022;\nLin et al., 2020; Gupta et al., 2020), etc. These\napplications all involve multi-modal input where\neach modality conveys partial information and a\ncomplete understanding can be achieved by tak-\ning all modalities into account. Second, MUG\nthat contains aligned tabular, textual, and visual\ndata is also beneficial to multimodal generation\napplications (Dong et al., 2023; He et al., 2023;\nSun et al., 2023; Peng et al., 2023). For instance,\nmany text-to-image generation models (Ramesh\net al., 2021; Poole et al., 2022) employ CLIP (Rad-\nford et al., 2021) as their feature encoder, and\nCLIP is an alignment-based fusion model trained\non semantically equivalent text and image pairs.\nThere also exist many studies exploring unimodal-\nto-unimodal generation tasks, such as image-to-\ntext captioning (Hossain et al., 2019), table-to-text\ngeneration (Parikh et al., 2020), text-to-table gen-\neration (Wu et al., 2022), etc. Following this idea,\nthe aligned triples can be utilized to pre-train multi-\nmodal encoders. Furthermore, multimodal datasets\nthat cover more than two modalities can inspire\nmore novel generation applications, such as audio-\nvisual slideshows generation from text (Leake et al.,\n2020) or textual-visual summarization from video-\nbased news articles (Li et al., 2020).\nB\nDetails of MUG\nB.1\nPrediction Targets\nDataset\nSource\nPrediction Target\npkm_t1\nPokémon\nPokémon’s primary Type\npkm_t2\nPokémon\nPokémon’s secondary Type\nhs_ac\nHearthStone\nAll card’s Category\nhs_as\nHearthStone\nAll card’s Set\nhs_mr\nHearthStone\nMinion card’s Race\nhs_ss\nHearthStone\nSpell card’s School\nlol_sc\nLoL\nSkin Category\ncsg_sq\nCS:GO\nSkin Quality\nTable 3: The prediction targets of datasets in MUG.\nWe identify appropriate categorical columns that\ncan serve as the prediction targets from these four\ngames, with a handy reference presented in Table 3.\nMore specifically, we provide detailed elaborations\nabout the prediction targets and their corresponding\ninput multimodal features. Some MUG datasets\nmay share same input multimodal features.\n• pkm_t1 and pkm_t2: Pokémon can be catego-\nrized into various elemental types, such as Fire,\nIce, Normal (non-elemental), and more. Each\nPokémon can have up to one primary type (for\npkm_t1) and one secondary type (for pkm_t2).\n– 17 tabular features:\ngeneration, status,\nheight_m,\nweight_kg,\nabilities_num,\ntotal_points,\nhp,\nattack,\ndefense,\nsp_attack, sp_defense, speed, catch_rate,\nbase_friendship,\nbase_experience,\ngrowth_rate, percentage_male.\n– 5 text features: name, species, ability_1,\nability_2, ability_hidden.\n– 1 image feature: image.\n• hs_ac and hs_as: Each HearthStone card belongs\nto one certain category (for hs_ac) such as min-\nion, spell, weapon, etc. Moreover, each card is\npart of a set (for hs_as) where new card sets are\nreleased periodically to introduce new content\nand strategies to the game.\n– 12 tabular features: health, attack, cost,\ntype, rarity, collectible, spellSchool, race,\ndurability, overload, spellDamage, set (for\nhs_ac) \/ cardClass (for hs_as).\n– 5 text features: name, id, artist, text, me-\nchanics.\n– 1 image feature: image.\n• hs_mr: Each HeathStone minion card is essen-\ntially a creature, thus it can be divided into differ-\nent races (for hs_mr).\n– 7 tabular features: health, attack, cost, rarity,\ncollectible, cardClass, set.\n– 5 text features: name, id, artist, text, me-\nchanics.\n– 1 image feature: image.\n• hs_ss: Each HeathStone spell card, similarly to\nthe minion race, each spell card belongs to a spe-\ncific school (for hs_ss) such as Shadow, Nature,\netc.\n– 5 tabular features: cost, rarity, collectible,\ncardClass, set, attack.\n– 5 text features: name, id, artist, text, me-\nchanics.\n– 1 image feature: image.\n• lol_sc: A champion skin in LoL is a cosmetic\nalteration to the appearance of the champion. De-\npending on the rarity and price, a champion’s\nskin belongs to a specific category (for lol_sc).\nIt is worth noting that the champion skins are\nstylish decorations that have nothing to do with\nrace, gender, or other unethical variables.\n– 3 tabular features: id, price, soldInGame.\n– 7 text features: skinName, concept, model,\nparticles, animations, sounds, releaseDate.\n– 1 image feature: image.\n• csg_sq: Similar to champion skin in LoL, CS:GO\nskins alter the appearance of weapons. The pre-\ndiction target is the skin quality according to its\nrarity (for csg_sq).\n– 5 tabular features: id, availability, skinCate-\ngory, minPrice, maxPrice.\n– 1 text features: skinName.\n– 1 image feature: image.\nB.2\nDefinition of Shannon Equitability Index.\nThe Shannon equitability index (Shannon, 1948),\nalso known as the Shannon evenness index or Shan-\nnon’s diversity index, is a measure used in ecology\nto assess the evenness or equitability of species\nabundance in a given community or ecosystem. It is\nderived from the Shannon entropy, which quantifies\nthe diversity or richness of species in a community.\nFor the classification task properties (Figure 2a),\nwe adopt it to measure the class balance ratio,\nEH =\nH\nlog(k) = −Pk\ni=1\nci\nn log ci\nn\nlog(k)\n,\n(5)\nwhere H denotes the entropy of each class’s counts,\nk denotes the dataset containing k classes. EH\nranges from 0 to 1, and the large EH, the more\nbalanced the dataset is.\nB.3\nAnalysis on Multimodal-dependent.\nTo study the correlation between category labels\nand input modalities in MUG, we plot 2D t-\nSNE (Van der Maaten and Hinton, 2008) projec-\ntions of various embeddings for the hs_mr dataset.\nIn the first row of Figure 7, the four subfigures\npresent projections obtained from the raw features\nof tabular, textual, visual, and fused modalities,\nseparately. Essentially, we conduct unsupervised\ndimension reduction (e.g., SVD) on the raw fea-\ntures and then use t-SNE to obtain 2D projections.\nFor tabular features, numerical columns are kept as\nthey are, and categorical columns are transformed\ninto numbers between 0 and n_class −1. For tex-\ntual features, we first transformed them into token\ncount vectors, and then use TruncatedSVD (Pe-\ndregosa et al., 2011) to reduce the number of di-\nmensions to a reasonable amount (e.g., 50) before\nfeeding into t-SNE. For visual features, we conduct\nPCA (Pedregosa et al., 2011) on each color channel\nto reduce the number of dimensions (e.g., 30) as\nwell.\nFor a neat presentation, we select a subgroup of\ncategories in the hs_mr dataset and assign different\ncolors to samples belonging to different categories.\nFor the fused raw features, we simply concatenate\nthe three single modality features without any fur-\nther modifications. The 2D projection of fused fea-\ntures is obtained following the same procedure, as\nin unimodal features. As can be seen in these four\nsubfigures, samples from different categories are\nclustered together no matter what modality is used\nas the input. The second row of Figure 7 shows the\n50\n0\n50\n40\n30\n20\n10\n0\n10\n20\n30\nTab raw feats\n20\n0\n20\n15\n10\n5\n0\n5\n10\n15\n20\nTxt raw feats\n50\n25\n0\n25\n50\n30\n20\n10\n0\n10\n20\nImg raw feats\n50\n0\n50\n30\n20\n10\n0\n10\n20\n30\nFused raw feats\n20\n0\n20\n15\n10\n5\n0\n5\n10\n15\nTab trained embs\n20\n0\n20\n15\n10\n5\n0\n5\n10\n15\nTxt trained embs\n50\n25\n0\n25\n50\n60\n40\n20\n0\n20\n40\n60\nImg trained embs\n20\n0\n20\n20\n10\n0\n10\n20\nFused trained embs\nBEAST\nDRAGON\nDEMON\nMURLOC\nELEMENTAL\nMECHANICAL\nFigure 7: 2D t-SNE projections of raw features (the 1st row) and trained embeddings (the 2nd row), based on\nunimodal (1st to 3rd columns) or multimodal (the 4th column) inputs.\n2D t-SNE projections obtained from embeddings\nof models trained on tabular, textual, visual, and\nfused modalities, where these embeddings are ob-\ntained from the output of the penultimate layers.\nThe models we used are tabMLP, RoBERTa, Vit,\nand MUGNET (evaluated baselines as in Sec 4).\nCompared to the t-SNE projections using raw fea-\ntures, all projections using trained embeddings pro-\nvide better insights into categorical structures of the\ndata. Among all subfigures, the one using fused em-\nbeddings is significantly better than the others, in\nwhich the separation between different categories is\nalmost perfect with only a small number of points\nmis-clustered. In summary, MUG is multimodal-\ndependent that requires multimodal information to\ndistinguish samples from different classes.\nC\nImplementation and Hyperparameters\nof Baselines\nWe\nimplement\nthe\nevaluated\nmodels\nusing\nopen-source codebases (Ke et al., 2017; Paszke\net al., 2019; Wang, 2019; Erickson et al., 2020;\nShi et al., 2021; Erickson et al., 2022), and\nmodels’ hyperparameters without specification are\nset as default. For GBM, we set the maximum\nnumber of leaves in one tree as 128, the minimal\nnumber of data inside one bin as 3, and the feature\nfraction ratio in one tree as 0.9. For tabMLP, we\nfollow (Erickson et al., 2020) to adaptively set the\nembedding dimension of each categorical feature\nas min(100, 1.6 ∗num_cates0.56), all hidden\nlayer sizes as 128, and the number of layers as\n4. For RoBERTa, we use the “RoBERTa-base”\nvariant.\nFor Electra, we use the “Electra-base-\ndiscriminator” variant.\nFor ViT, we use the\n“vit_base_patch32_224” variant.\nFor SWIN,\nwe use the “swin_base_patch4_window7_224”\nvariant. For AutoGluon, we use its “multimodal-\nbest_quality” preset.\nFor AutoMM, we use\nits default preset. For our own MUGNET, we\noptimize it using AdamW with a learning rate\nset as 0.001 and a cosine annealing learning rate\nschedule. Regarding the graph sampling strategy,\nwe set the number of root nodes to generate\nrandom walks as 80% of the original number\nof nodes, and the length of each random walk\nas 2. MUGNET chooses other hyperparameters\nvia HPO (Liaw et al., 2018). The search space\nincludes the sample-wise similarity function sim ∈\n{cosine sim, RBF kernel, k-nearest neighbor}\nused in Equation (1). More specifically, (i) when\nsim := cosine sim or sim := RBF kernel, HPO of\nMUGNET also search along their associated graph\nsparsity hyperparameter spy ∈{0.5, 0.75, 0.95}.\n(ii) when sim := k-nearest neighbor, MUGNET\nsearch along k ∈{5, 10, 32}.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields.pdf"}
{"title":"Equilibria in Repeated Games under No-Regret with Dynamic Benchmarks","authors":"Ludovico Crippa, Yonatan Gur, Bar Light","summary":"In repeated games, strategies are often evaluated by their ability to\nguarantee the performance of the single best action that is selected in\nhindsight, a property referred to as \\emph{Hannan consistency}, or\n\\emph{no-regret}. However, the effectiveness of the single best action as a\nyardstick to evaluate strategies is limited, as any static action may perform\npoorly in common dynamic settings. Our work therefore turns to a more ambitious\nnotion of \\emph{dynamic benchmark consistency}, which guarantees the\nperformance of the best \\emph{dynamic} sequence of actions, selected in\nhindsight subject to a constraint on the allowable number of action changes.\nOur main result establishes that for any joint empirical distribution of play\nthat may arise when all players deploy no-regret strategies, there exist\ndynamic benchmark consistent strategies such that if all players deploy these\nstrategies the same empirical distribution emerges when the horizon is large\nenough. This result demonstrates that although dynamic benchmark consistent\nstrategies have a different algorithmic structure and provide significantly\nenhanced individual assurances, they lead to the same equilibrium set as\nno-regret strategies. Moreover, the proof of our main result uncovers the\ncapacity of independent algorithms with strong individual guarantees to foster\na strong form of coordination.","url":"http:\/\/arxiv.org\/abs\/2212.03152v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2212.03152v3","published":1670347561000,"comment":"The primary result of has been significantly generalized and\n  incorporated into a new paper, arXiv:2501.11897. Since the original paper\n  contained only this single result, it is too small to stand on its own as a\n  replacement. To avoid redundancy, I have withdrawn the earlier version","pdf_text":"","pdf_path":""}
{"title":"A Game Benchmark for Real-Time Human-Swarm Control","authors":"Joel Meyer, Allison Pinosky, Thomas Trzpit, Ed Colgate, Todd D. Murphey","summary":"We present a game benchmark for testing human-swarm control algorithms and\ninterfaces in a real-time, high-cadence scenario. Our benchmark consists of a\nswarm vs. swarm game in a virtual ROS environment in which the goal of the game\nis to capture all agents from the opposing swarm; the game's high-cadence is a\nresult of the capture rules, which cause agent team sizes to fluctuate rapidly.\nThese rules require players to consider both the number of agents currently at\ntheir disposal and the behavior of their opponent's swarm when they plan\nactions. We demonstrate our game benchmark with a default human-swarm control\nsystem that enables a player to interact with their swarm through a high-level\ntouchscreen interface. The touchscreen interface transforms player gestures\ninto swarm control commands via a low-level decentralized ergodic control\nframework. We compare our default human-swarm control system to a\nflocking-based control system, and discuss traits that are crucial for swarm\ncontrol algorithms and interfaces operating in real-time, high-cadence\nscenarios like our game benchmark. Our game benchmark code is available on\nGithub; more information can be found at\nhttps:\/\/sites.google.com\/view\/swarm-game-benchmark.","url":"http:\/\/arxiv.org\/abs\/2210.15852v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2210.15852v1","published":1666925234000,"comment":"8 pages, IEEE Conference on Automation Science and Engineering\n  (CASE), 2022","pdf_text":"A Game Benchmark for Real-Time Human-Swarm Control\nJoel Meyer, Allison Pinosky, Thomas Trzpit, Ed Colgate, Todd D. Murphey\nAbstract— We present a game benchmark for testing human-\nswarm control algorithms and interfaces in a real-time, high-\ncadence scenario. Our benchmark consists of a swarm vs.\nswarm game in a virtual ROS environment in which the goal of\nthe game is to “capture” all agents from the opposing swarm;\nthe game’s high-cadence is a result of the capture rules, which\ncause agent team sizes to ﬂuctuate rapidly. These rules require\nplayers to consider both the number of agents currently at\ntheir disposal and the behavior of their opponent’s swarm when\nthey plan actions. We demonstrate our game benchmark with\na default human-swarm control system that enables a player\nto interact with their swarm through a high-level touchscreen\ninterface. The touchscreen interface transforms player gestures\ninto swarm control commands via a low-level decentralized\nergodic control framework. We compare our default human-\nswarm control system to a ﬂocking-based control system, and\ndiscuss traits that are crucial for swarm control algorithms\nand interfaces operating in real-time, high-cadence scenarios\nlike our game benchmark. Our game benchmark code is\navailable on Github; more information can be found at https:\n\/\/sites.google.com\/view\/swarm-game-benchmark.\nI. INTRODUCTION\nAn appealing aspect of robot swarms is their potential\nto be deployed into areas that are dangerous for humans\nto enter. Many dangers—like ﬁre and unstable structures—\ncause physical harm and evolve rapidly. Operators control-\nling robot swarms deployed into environments with these\ndangers cannot assume the environment will remain constant;\nthey need to be able to rapidly adapt to changing conditions,\nnew information, and swarm agent dropout.\nMany dangers are challenging to simulate because each\nenvironment is unique, however, simulations can still play\nan important role in testing human-swarm control algorithms\nand interfaces for these environments. Thus, the vision of\nsending human-swarm teams into dangerous, rapidly evolv-\ning scenarios faces two challenges: 1) many existing algo-\nrithms and interfaces are ill suited to the challenges of highly-\ndynamic environments and 2) there are no benchmarks\n(real-world or simulation-based) for testing such proposed\nalgorithms and interfaces. To tackle these challenges, we\npresent a game benchmark inspired by the challenges of real-\ntime human-swarm control in high-cadence environments.\nEven in benign scenarios, controlling swarms is challeng-\ning. As swarm size grows, the cognitive load on the operator\nincreases, and it becomes difﬁcult for an operator to task\nindividual agents [1]. Prior work has sought to reduce the\noperator’s cognitive load by autonomously planning agent\ntrajectories via ﬂocking algorithms [2], potential ﬁelds [3],\nﬁxed formations [4], linear temporal logic [5], collective\nAuthors are with the Center for Robotics and Biosystems at Northwestern\nUniversity. Corresponding Author Email: joelmeyer@u.northwestern.edu\nFig. 1.\nSwarm vs. Swarm Game: Two players use interfaces to control\nopposing swarms in a shared, continuous space. The objective is to capture\nall agents on the opposing swarm by maneuvering one’s own swarm to\nsurround opposing agents. In this ﬁgure, as the blue swarm moves about, the\nred swarm forms a circular structure that surrounds the center of the game\nenvironment. Blue agents that pass through this surrounded area are captured\nby the red swarm according to the game rules. The game rules, which lend\nthemselves to the emergence of vaguely Go-like structures during game play,\nare further explained in Section II-C). The blue agents that are captured\nbecome part of the red swarm, and can now be controlled by the red player.\nmotion [6] [7], and Voronoi partitions [8]. Unfortunately,\nmany of these algorithms are too rigid for highly dynamic\nand dangerous scenarios and limit the variety of commands\nan operator can use to achieve a task.\nDensity-based swarm control algorithms, on the other\nhand, enable operators to specify ﬂexible behavior to their\nswarm in dynamic and potentially dangerous scenarios. For\nexample, authors in [9] developed a system that enabled\nhuman operators to control their swarm through density\nspeciﬁcations via a touchscreen interface. Authors in [10]\ncreated an end-to-end swarm control system that leveraged\ntheir swarm’s heterogeneous capability, used autonomously\ndetected information to keep the swarm safe, and enabled\noperators to specify multimodal commands to their swarm\nvia touchscreen that were adaptable in real-time. Further-\nmore, the touchscreen interfaces used in these works to\nsend commands enabled both persistent swarm behavior\nand multiple variations of operator commands to achieve\ntasks through density speciﬁcations. Other work in swarm\ninterfaces (e.g., touchscreen interfaces [11], brain-machine\ninterfaces [12], swarm programming languages [13], and\nhaptic control devices [14]), were more rigid in the types of\nbehavior they could specify. Some of these interfaces were\nalso not persistent and required the operator to constantly\ninput commands to their system. In this work, we extend the\nsystem presented in [10] to demonstrate our game benchmark\ndue to the system’s promising real-time performance.\nPrior work involving swarm testbeds and real-world swarm\ndemonstrations has enabled researchers to test formation\ncontrol, motion planning, and collision avoidance [15], [16],\n[17], [18], [19]. However, none of these testbeds or demon-\nstrations were particularly dynamic nor did they assess high-\ncadence swarming. Work in game-theoretic scenarios involv-\narXiv:2210.15852v1  [cs.RO]  28 Oct 2022\nFig. 2.\nOverview of the Game Benchmark: Two players use separate\ninterfaces (pictured here are the touchscreen interfaces for our human-swarm\ncontrol system described in Section III; researchers can choose to develop\ntheir own interfaces) to send commands via ROSbridge to their separate\nteams of agents. A ROS node tracks the state of the game (the positions\nof all agents) and determines whether an agent has been captured based\non the rules of the game. Both players have their own displays (in RViz)\nshowing the current state of the game. All code for the game benchmark will\nbe made open source and can be executed on any laptop running Ubuntu\n18.04 LTS with an Intel I5 or equivalent processor, and 4 GB of RAM.\nThe code will contain a touchscreen interface for sending commands and\nan implementation of decentralized ergodic control for executing commands\nto enable researchers to demo the game benchmark “out of the box”.\ning pursuit-evasion and racing with multi-agent systems [20],\n[21] [22], [23], [24] has created scenarios and algorithms that\nhave approached what is needed to assess real-time swarm\ncontrol in dynamic, high-cadence environments. The authors\nof [25] conducted a ﬁeld test with unmanned aerial vehicles\ninvolving teams of 10 vs. 10 agents in which they examined\nswarm combat tactics in a mock aerial dog ﬁght. However,\nall of these works were limited to team sizes under 10 agents,\nand some did not involve human operators.\nGiven the lack of systems and testing scenarios for real-\ntime high-cadence human-swarm control, we have created a\ngame benchmark (shown in Figure 1) that consists of a ROS-\nbased virtual environment containing a swarm vs. swarm\ngame. The swarm vs. swarm game targets scenarios that\nrequire evolving strategies—that is, rapid re-speciﬁcation of\nstrategies—in a rapidly changing environment (with respect\nto agent positions). We demonstrate our game benchmark\nwith a human-swarm control system that uses a touchscreen\ninterface to specify gesture-based objectives as distributions\nfor robot swarms using real-time, decentralized ergodic con-\ntrol.\nSection II describes our dynamic swarm vs. swarm game\nbenchmark. Section III describes the default system for real-\ntime human-swarm control we use to demonstrate our game.\nSection IV describes an example tactic that can be deployed\nin the game with our default system, highlights considera-\ntions operators may need to make when planning tactics in\nthe game, compares our default system for demonstrating the\ngame to a ﬂocking-based method, and discusses the short-\ncomings of other swarm control systems for planning tactics\nin high-cadence scenarios. In Section V, we discuss traits\nof real-time human-swarm control algorithms and interfaces\nthat may be desirable for high-cadence scenarios like our\ngame benchmark and conclude with future work.\nII. THE GAME BENCHMARK\nIn this section, we describe our benchmark, which is a\ndynamic game in which players control their swarms to\ncapture all of the agents on the opposing team. Both teams\nstart with the same number of agents, which is speciﬁed by\nthe players when the game is initialized. The game is played\nuntil all the available agents are captured by one team (i.e.,\nif the game starts with 20 agents total, 10 on each team, the\ngame does not end until one team has all 20 agents and the\nother team has 0).\nPlayers must strategically maneuver their swarm to sur-\nround agents on the opposing swarm while preventing their\nown agents from being captured—resulting in the emergence\nof vaguely Go-like structures (see Figure 1). We note,\nhowever, that our game is played in continuous time and\nspace—there is no notion of teams “taking turns”. Also, all\nof the agents in the game can be at any location in the\nenvironment at any time. Figure 2 shows the architecture\nfor our game benchmark. The game benchmark architecture\nincludes the agents, the control algorithm and interface the\nplayers choose to deploy, the virtual ROS environment, and\nthe game engine dictating the capture rules.\nA. The Agents\nOur virtual agents are modeled as second order, 2-D\npoint masses. Agents are spawned in one of four corners\nof the virtual ROS environment. Each agent is given a\nrandom initial altitude, distinct from all other agents in\nthe environment, at which it remains for the entire game\nto avoid collisions. Agents subscribe to ROS topics that\nplayers publish commands to. In our demonstrated examples,\neach agent receives commands via touchscreen interface\n(discussed further in Section III-A), runs its own real-time,\ndecentralized ergodic control algorithm (discussed further in\nSection III-B), and receives information from members of\nthe team it is currently on. Each agent runs their controls at\n10 Hz.\nB. The Virtual ROS Environment\nOur virtual ROS environment is rendered in a [0,1]2\ngrid in RViz. The game visualization shows areas of the\nenvironment in which a player’s swarm can capture agents\non the opposing team. These areas are denoted by black\nshading on the background of the environment. A ROS game\nengine node enforcing the rules of the game (discussed in\nSection II-C) sends information to another ROS node that\ncontrols the game visualization. This information determines\neach agent’s color (representing the team they are on) at each\ntime step. The game visualization also renders a decaying\ntrajectory history of each agent so players can see where\nmembers of the opposing swarm have recently been. The\ngame visualization and game engine nodes both run at 30\nHz.\nFig. 3.\nSwarm-based Game Geometry: Each agent contributes to the time-averaged trajectories of the team it is on through its individual trajectory\nhistory. At each instance in time, the area surrounding an agent’s current position is weighted as a capture area for its team (as seen in the pop-out on\nthe left). Several agents with overlapping capture areas are more likely than an individual agent to capture an agent on the opposing team. The combined\ncapture areas of the two red agents (visualized in the pop-out on the right) create a “net” and successfully capture the blue agent. Thus, commanding\none’s swarm to surround an opposing agent, or to trap an opposing agent along the sides of the environment, are more likely to successfully capture the\nopposing agent than driving one’s swarm to be directly on top of the opposing agent. The dark shaded areas of the environment shown in the middle ﬁgure\nrepresent areas the red swarm can capture blue agents in. Darker shaded regions are more likely to lead to captures than lighter shaded regions.\nC. Rules for Capturing Agents\nThe time-averaged trajectories of each swarm dictates\nareas of the environment in which opposing agents can be\ncaptured. The pop-out in Figure 3 shows how agents are\ncaptured. In order to create vaguely Go-like structures and\nmotivate the concept of surrounding an opposing swarm,\nthe time-averaged trajectories of the swarm over the whole\nenvironment (a 50x50 grid) are averaged over smaller\nneighborhoods represented by 5x5 sub-grids centered at\neach grid cell in the environment. Only the outer rows and\ncolumns of these sub-grids are used to calculate the “value”\nof each neighborhood.\nThus, regions of the environment that are completely\nsurrounded by a swarm, or that are surrounded by a swarm\nand one or more of the four boundaries of the environment,\nhave higher potential to capture opposing agents than areas\nof the environment a swarm is directly positioned over. At\neach time step in the game, if an opposing agent enters\ninto a neighborhood containing a capture value greater than\n75% of the maximum value over all neighborhoods, it will\nbe captured. When an agent is captured, it immediately\nbecomes controlled by the opposing player. The captured\nagent’s command is updated to the last command published\nby the player controlling the new team it is now on. The\ncaptured agent also begins contributing to the areas its new\nteam can capture opposing agents in through its trajectory\nhistory. If the new team the agent is on is running a\ndecentralized control algorithm, the captured agent will also\nbegin communicating with its teammates via ROS topics.\nPlayers have a clear visual representation of where their\nswarm can capture opposing agents via the game visualiza-\ntion in RViz. Areas where their swarms are likely to capture\nopposing agents are shaded in black, while areas with a low\nor no chance of capture are shaded in white. The visual\nrepresentation helps the players build intuition about the\ngame rules. The density of one’s own swarm has no bearing\non its safety; a player must ensure the safety of their swarm\nthrough commands driving their swarm away from areas the\nopposing swarm is surrounding.\nIII. OUR DEFAULT SYSTEM FOR REAL-TIME\nHUMAN-SWARM CONTROL\nThis section describes our default human-swarm control\nsystem, which consists of a touchscreen interface and de-\ncentralized ergodic control framework we have developed for\ndemonstrating our game benchmark. Our system is scale and\npermutation-invariant, which enables players to command\ntheir swarm in real-time and respond to changes in the\nenvironment by re-specifying behavior for their swarm.\nA. Touchscreen Interface\nFigure 4 shows our system’s touchscreen interface for\nsending player commands to the swarm. We created our\ntouchscreen interface using Kivy—an open source Python\nframework for developing graphical applications with touch\ncapabilities. Our touchscreen interface works on any PC\nor tablet running Ubuntu 18.04 LTS or Windows 10 with\nTkinter, Python3, and OpenCV. Players can control their\nswarm by specifying distributions with hand-gestures on a\ntouchscreen tablet or with a mouse on PC; these distributions\nspecify what their team of agents should do by interpreting\nthe path of the gesture as a spatial distribution.\nPlayers draw their distributions on a 2-D, top-down render-\ning of the virtual ROS environment. Players can select one\nof two input distribution types for areas of the environment:\n“Attract” (denoted in blue) which agents will converge to and\nFig. 4.\nTouchscreen Interface: The virtual ROS environment (left) next\nto our touchscreen interface with a target distribution drawn (right). Players\ncan draw areas of high interest (which will attract the agents) in blue and\nareas of low interest (which will repel the agents) in green. When a player\nis satisﬁed with the target distribution they have drawn, they can send\nthe target distribution to their team. Players can clear the map to draw a\nnew distribution. Players can also modify an existing target distribution by\ndrawing new targets on top of old targets. The touchscreen’s drawing area\ndirectly maps to the virtual ROS environment’s area. The darker shaded\nregions in the virtual ROS environment ﬁgure on the left correspond to\nareas in which the red swarm can capture members of the blue swarm\n(darker shaded regions are more likely to lead to captures than lighter shaded\nregions).\nspend more time in, and “Repel” (denoted in green) which\nagents will avoid. New target distributions can be continually\noverlaid on top of previous ones, which enables quick\nupdates to previous distributions. Players’ target distributions\nare smoothed out with a Gaussian ﬁlter. They are then scaled\nin value according to the resolution and size of the virtual\nROS environment before being sent via ROS websocket to\nthe swarm agents.\nB. Decentralized Ergodic Control\nPlayer target distribution speciﬁcations provided via the\ntouchscreen are transformed into low-level swarm commands\nthrough decentralized ergodic control. Decentralized ergodic\ncontrol (described in detail in previous works [26], [27],\n[28]) provides a natural framework for specifying density-\nbased swarm objectives, and enables a player to quickly and\nﬂexibly specify behavior for their swarm in response to the\nopposing swarm’s behavior.\nTo deﬁne decentralized ergodic control, we start by in-\ntroducing the dynamics of our system. Consider a set of N\nagents with state x(t) =\n\u0002\nx1(t)⊤, x2(t)⊤, . . . , xN(t)⊤\u0003⊤:\nR+ →RnN. From work in [26], we deﬁne the dynamics of\nthe collective multi-agent system as\n˙x = f(x, u) = g(x) + h(x)u\n=\n\n\ng1(x1)\ng2(x2)\n...\ngN(xN)\n\n+\n\n\nh1(x1)\n. . .\n0\n...\n...\n0\nhN(xN)\n\nu,\n(1)\nwhere g(x) is the free, unactuated dynamics of the multi-\nagent system and h(x) is the system’s dynamic control\nresponse. We seek to ﬁnd a set of controls u for the multi-\nagent system that minimizes the system’s ergodic metric E\nwith respect to some player target speciﬁcation φ(s) where\ns ∈R2 is a point in the game environment.\nThe ergodic metric E provides a way to calculate the\n“difference” between player target speciﬁcations φ(s) and\npast agent trajectories [28] (which are represented as ck val-\nues using Fourier decompositions)—similar to the Kullback-\nLeibler divergence for comparing two distributions.\nWe can use both Fourier decompositions to calculate the\nergodic metric E (introduced in [28]):\nE(x(t)) = q\nX\nk∈Nν\nΛk(ck −φk)2\n(2)\nwhere q ∈R+ is a scalar weight, Λk = (1 + ||k||2)\nν+1\n2\nis a weight on the frequency coefﬁcients, ck is the Fourier\ndecomposition of the agents’ trajectories in a player’s swarm\nover a ﬁxed time horizon, and φk is the Fourier decomposi-\ntion of the player’s target speciﬁcation φ(s) [28].\nWe can determine the ergodic metric’s sensitivity to differ-\nent control inputs u by differentiating the ergodic metric with\nrespect to a control application duration λ at some optimal\napplication time τ. This results in the costate equation\n˙ρ = −E(x(t))∂Fk\n∂x −∂Φ\n∂x −∂f\n∂xρ(t)\n(3)\nwhere Φ is the exponential control barrier function that keeps\nthe agents in the operating environment, Fk is the cosine\nbasis function, and f(x, u) is the system dynamics.\nWe can then write an unconstrained optimization problem\nJ =\nZ ti+T\nti\n∂E\n∂λ\n\f\f\f\f\nτ\n+ 1\n2||u⋆(t) −udef(t)||2\nR dt\n(4)\nwhere R is a positive deﬁnite matrix that weights the control,\nu⋆is the optimal control and udef is some default control. The\ndefault control udef could be that the agent moves forward\nat its current velocity. In this paper, udef is zero. The control\nthat minimizes this objective J is:\nu⋆(t) = −R−1 ∂f\n∂u\nT\nρ(t) + udef(t)\nwhich is calculated and applied at every time step to the\nplayer’s team of agents for the player’s current target speci-\nﬁcation input.\nC. Scale and Permutation-Invariance\nOur touchscreen interface and decentralized ergodic con-\ntrol algorithm enable our system to be scale and permutation-\ninvariant with respect to player target distributions. Figure 5\nshows swarms containing 6, 12, and 24 agents responding to\nthree different player target distributions speciﬁed through\nthe touchscreen interface. The combination of our touch-\nscreen interface and decentralized ergodic control algorithm\nproduces scale-invariant swarm behavior, since all three\nswarm sizes converge to the three different target distribu-\ntions. Our system also produces permutation-invariant swarm\nbehavior, since the swarm will converge to the player’s target\ndistribution from different permutations of agent positions\n(i.e., the player’s swarm contains 10 agents that are in\ndifferent positions when the player speciﬁes the target. If\nAgent 1, Agent 2, etc. swapped positions, the swarm would\nstill converge to the player’s target).\nOur system’s scale and permutation-invariance enables the\nplayer to plan swarm-level behavior instead of individual\nFig. 5.\nSpeciﬁcation is Independent of Swarm Size with our Default\nHuman-Swarm Control System: In this example, the black and white\nshaded areas of the environment represent the time-averaged trajectories of\nthe swarm (not capture areas based on the game rules), with black areas\ncorresponding to areas the player’s swarm’s trajectories have spent more\ntime in relative to white areas. The player’s speciﬁcations are scale-invariant,\nsince all three swarm sizes converge to the player target speciﬁcations. The\nplayer target speciﬁcations are also permutation-invariant, since the swarm\nwill converge to the player’s target speciﬁcation from different permutations\nof the same set of agent positions.\nagent trajectories. For high-cadence scenarios like our game\nbenchmark, as the number of agents the player controls in-\ncreases and the time horizon the player has to plan decreases,\nthe player cannot think strategically in terms of individual\nagents and their trajectories. Our system enables players to\nmake strategic decisions based on the general positions and\ndensities of their own swarm and their opponent’s swarm.\nFurthermore, the decentralized nature of our ergodic con-\ntrol algorithm enables players to maintain their strategy (the\nmost recent target distribution they sent to their swarm)\nregardless of how many agents are currently under their\ncontrol. Even if the number of agents under their control\nis ﬂuctuating, their swarm will still converge to their last\ntarget. These traits are advantageous for players planning\ntactics in high-cadence scenarios like our game benchmark.\nWe elaborate on these traits in Section IV.\nIV. SWARM TACTICS\nIn this section, we describe an example ensemble tactic\na player can deploy with our default human-swarm control\nsystem to beat an opponent in the game benchmark. We then\ndiscuss tactical considerations players may need to make\nduring the game. We also discuss the shortcomings of other\nmethods for human-swarm control and the challenges players\nmay face when using these other methods to plan tactics in\nhigh-cadence scenarios like our game benchmark.\nA. Example Game Tactics\nFigure 6 shows an example sequence of target speciﬁ-\ncations a player can use with our default human-swarm\ncontrol system to capture agents on the opposing team. The\nFig. 6.\nEnsemble Game Tactics: In the game state on the far left, it is\ndifﬁcult to discern any particular structure. Once the red player speciﬁes\na unimodal target distribution, the red agents converge to the center of the\nenvironment and capture opposing blue agents in the process. The red player\nthen speciﬁes their red agents to track the remaining blue agent along the\nleft side of the environment to capture it and win the game. The dark shaded\nareas in the game state ﬁgures correspond to areas in which the red team\ncan capture blue agents. The darker shaded regions are more likely to lead\nto captures than the lighter shaded regions.\nnature of our game enables a player to quickly capture all\nthe opposing agents (and win the game) with a well-timed\nsequence of commands. In the initial game state shown in\nFigure 6 on the far left, it is difﬁcult to discern any particular\nstructure in the agent positions. Once the red player speciﬁes\na unimodal distribution in the center of the environment with\nour interface, the red agents converge to a visible structure\nvia decentralized ergodic control and capture opposing blue\nagents in the process. The red player then speciﬁes their\nswarm to track the remaining blue agent as it circles along\nthe left side of the environment. The red swarm then captures\nthe blue agent and wins the game.\nThis is one of many possible winning speciﬁcation se-\nquences players can make with our human-swarm control\nsystem (see multimedia attachment). Players can reason\nabout how they can use the distribution of their swarm in\nthe environment to create the most opportunities to capture\nopposing agents. Players can also reason about the current\nposition of the opposing swarm and attempt to predict where\nthe opposing swarm will be in subsequent time steps.\nB. Player Tactic Considerations\nSince the rules of the game allow any agent to be captured\nregardless of how many of its team members are surrounding\nit, players might be wary of sending commands to their\nswarm that cause all of their agents to be in positions\nclose to one another. While agents on the same team that\nare close together create areas of the environment that are\nmore likely to capture agents on the opposing team (due to\noverlapping capture areas from the game rules), the opponent\nmay maneuver their swarm in a way that enables them to\nsimultaneously capture all the agents on the player’s team\nand win the game. Thus, there are consequences to every\nmaneuver a player makes; a player may maneuver to capture\na small number of agents on the opposing team, only to ﬁnd\nthat their agents have now been steered into an area where\nthey can be captured.\n 0\n25\n 50\n 75\n100\n125\n150\n175\n200\nTime[s]\n0\n5\n10\n15\n20\nNumber of Agents on Each Team\nNumber of Agents on Each Team Over Time\n# of Red Agents\n# of Blue Agents\nFig. 7.\nSwarm Size Changes Rapidly During Game Play: This ﬁgure\nshows the number of agents on each team over the course of an example\ngame between two players. The game starts at approximately the 15s mark\nand ends right before the 200s mark. The number of agents on each team\nchanges rapidly over the course of the game. Near the end of the game, the\nfrequency of agent captures stabilizes as the blue player accumulates more\ntotal agents than the red player. The blue player eventually captures all the\nred agents and wins the game.\nThe dynamic, high-cadence nature of the game also re-\nquires the player to consider the amount of time they take to\nsend commands to their swarm. Intricate commands (such\nas drawing detailed target speciﬁcations with our default\nhuman-swarm control system) may require the player to\nsolely focus on creating their command (i.e., focusing on\ntheir touchscreen interface while they draw) instead of\nlooking at the rapidly changing swarm positions in the\nenvironment. Figure 7 shows how the number of agents on\neach team ﬂuctuates over the course of an example game.\nAs seen in the ﬁgure, team sizes change rapidly over short\nperiods of time. A brief advantage in number of agents\nunder a player’s control can quickly vanish in a matter of\nseconds. This rapid cadence may also affect player strategy\nin that players with larger swarms may be able to study\nthe environment for emerging patterns in an opponent’s\nplay and take longer to specify new commands for their\nswarm since they can afford to lose agents without losing\nthe game. Players down to the last member of their swarm\nmay instead have to repeatedly specify commands that cause\ntheir agent to quickly move around the environment to create\nopportunities to capture opposing agents and rebuild the size\nof their swarm.\nWe note here that it is possible to recover from a dramatic\nratio in numbers of agents between teams. One agent,\ncontrolled strategically, can be sufﬁcient to build up team\nsize again. Players may ﬁnd that they need to play many\ngames before they become adept at reasoning about their\nswarm’s behavior as a whole. A naive player may ﬁnd that\neven an opponent swarm speciﬁed by a uniform coverage\ncommand across the domain—with no changes over time—\nis a challenging adversary.\nVarious strategies for trapping and capturing agents on the\nopposing team may emerge for players from repeated game\nplay. Player tactics may mirror some of those presented in\nother work in pursuit-evasion, such as [23], or in swarm\ntactics work such as [25]. In some instances, players may\nﬁnd it useful to make the environment more chaotic by\nhaving their swarm spread out in all directions, which\ncould disorient the opponent. For instance, players using our\ndefault human-swarm control system could crash their swarm\ninto one side of the environment through “wavefront” attacks\nvia speciﬁcally-drawn attraction and repulsion regions, which\nmay quickly capture many opposing agents at high risk to\nthe player losing their own agents.\nC. Challenges of Applying Traditional Swarm Control Meth-\nods to Game Tactics\nLeader-follower methods such as those presented in [29],\n[30] may not be able to perform the tactics described above\nbecause pre-designated leaders (or leaders elected at each\ntime step) could be captured. Each team would have to re-\nappoint a leader from their available agents, which may\nnot be possible with certain leader-election algorithms or\nmethods that involve multiple leaders [31] at high-cadence.\nLikewise, inﬂuencing the swarm by having the player take\ncontrol of an individual agent or sub-team (such as in\n[32]) may also be infeasible since the agent the player is\ncontrolling could be captured. It may also be difﬁcult for the\nplayer to select an individual agent or sub-team in a high-\ncadence environment due to cognitive load.\nFormation-based control methods, such as those presented\nin [4], [33], [34], may limit how a player can specify different\nlocations for sub-groups of their swarm to converge to for\nstrategic purposes. It is not clear how the same multimodal\nmaneuvers performed by drawing density speciﬁcations with\nour default system can be achieved with swarm forma-\ntion control methods that may require ﬁxed formations to\nbe determined beforehand. Some of these formation-based\nmethods that are non-decentralized may also be affected by\ncommunications failure or agents changing teams.\nBoth leader-follower and formation-based control meth-\nods, however, do have advantages over our default system\nin the different types of automatic swarm-response behavior\nthey can enable. For instance, leader-follower methods could\nenable a player to create automatic multi-modal behaviors\nfor elected leaders in their swarm to perform in response to\ndifferent opposing swarm conﬁgurations during game play.\nFormation-based methods could enable a player to create a\nlibrary of swarm formations they found useful for strategic\npurposes that they could then deploy at any instant during\nthe game. Such automatic behaviors would not be possible\nwith our default system (in its current form).\nFigure 8 compares our default method for human-swarm\ncontrol to a ﬂocking-based control algorithm (adapted from\nwork in [34]). In the ﬁgure, the player speciﬁes a bimodal\ntarget for their swarm to converge to. Our system enables\nplayers to specify both targets at once, while ﬂocking re-\nquires both targets to be speciﬁed in sequence, one at a\ntime (denoted by the circles in the ﬁgure). The desired\n“compactness” of the player’s swarm at each target would\nalso have to be speciﬁed through an attractor weight param-\neter (green color saturation). On the other hand, our system\nenables the player to draw a larger region for more diffuse\nFig. 8.\nOur Default Method for Human-Swarm Control vs. Flocking: This ﬁgure compares how our system for human-swarm control and a ﬂocking-\nbased method converge to a bimodal target (with a more diffuse target in the bottom left corner and a more compact target in the upper right). The top\nrow shows the results for our default system for human-swarm control. The player speciﬁes the bimodal target all at once, drawing a larger circle on their\ninterface for the more diffuse target and a smaller circle for the more compact target. The bottom row shows the results for the ﬂocking-based method. The\nplayer has to specify each of the targets in a sequence instead of all at once. The player also has to specify both the ﬂocking attractor positions (denoted\nby circles) and the attraction weights (denoted by green color saturation) for each of these targets to achieve the desired level of compactness.\ncoverage and a smaller region for more compact coverage\nin a single speciﬁcation. Although our default system can\nperform some tactical maneuvers (such as the one above)\nin fewer speciﬁcations than the ﬂocking-based method, the\nﬂocking-based method is still scale and permutation-invariant\n(like our default system) and may prove superior in other\nscenarios. For instance, specifying a single attractor position\nand weight to capture opposing agents in a particular area\nof the game environment may take a player less time than\ndrawing an attractor region with our touchscreen interface.\nThus, while the system we have used to demonstrate\nthe game benchmark contains many advantages over more\ntraditional methods for swarm control, we only make those\narguments in speciﬁc cases of speciﬁc algorithms; other\nresearchers implementing their own systems for real-time\nhuman-swarm control in this game benchmark will lead\nto better comparisons and faster development of capable\nalgorithms. The game benchmark is a useful opportunity\nfor the swarm and human-robot interaction communities to\ndetermine what traits are necessary for these systems to have\nto succeed in high-cadence scenarios. We conclude with what\nsome of these traits are in the next section.\nV. DISCUSSION\nWe have designed a game benchmark for assessing human-\nswarm control algorithms and interfaces in a real-time, high-\ncadence scenario. We demonstrated our game benchmark\nscenario using a default human-swarm control system that\nwas scale and permutation-invariant. We provided discussion\non example tactics players can employ, tactical considera-\ntions players may need to make while playing our game\nbenchmark, and compared our default system to other meth-\nods for human-swarm control.\nBased on our demonstrations, scale and permutation-\ninvariance are important characteristics for algorithms and\ninterfaces operating in high-cadence scenarios like our game\nbenchmark. It is difﬁcult to deploy tactics at high-cadence\nwithout algorithms and interfaces that scale to teams of\ndifferent sizes and enable players to specify locations for\ntheir swarm to converge to without having to keep track of\nunique agent locations or which agents have been captured\nor newly acquired. Instead of reasoning about individual\nagents, players must be able to reason about their and their\nopponent’s swarms as a whole.\nA speciﬁc advantage of our default system for human-\nswarm control is that the touchscreen interface does not\nrequire player swarm speciﬁcations to be pre-determined.\nDifferent players can draw different distributions on the\ntouchscreen that represent the same “tactic” at a higher level\n(i.e., trapping opposing agents by drawing some type of\nshape). Players who are interested in using techniques from\nmachine learning to learn possible tactics (that humans could\ndeploy) for swarm systems operating in high-cadence sce-\nnarios may want to develop control algorithms and interfaces\n(with or without touch-based modalities) that afford machine\nlearning agents the same speciﬁcation ﬂexibility as our\nsystem during the learning process. Scale and permutation-\ninvariance may also be necessary traits for these algorithms\nand interfaces to make tactic learning feasible (especially if\ntechniques from reinforcement learning are used).\nFuture work could develop a virtual adversary to compete\nagainst human players in this benchmark scenario. We en-\nvision this adversary as an opponent benchmark that other\nresearchers could use to test their real-time human-swarm\ncontrol systems against. Then, the performance of real-\ntime human-swarm control systems could be compared via\nnumber of wins and duration of game-play. Finally, the game\nbenchmark could be extended to conduct human subject\ntesting to empirically determine what types of strategies\nhuman players employ. Biometric data could be collected\nfrom players as they play the game to determine how much\ncognitive load they are experiencing and if certain changes\nto the human-swarm control algorithm or interface produce\nmore or less cognitive load.\nACKNOWLEDGMENTS\nThis material is based on work supported by the United\nStates National Science Foundation grant CNS 1837515\nand by the Defense Advanced Research Projects Agency\n(DARPA) OFFSET SPRINT grant HR00112020035. The\nviews, opinions, and\/or ﬁndings expressed are those of the\nauthors, and should not be interpreted as representing the\nviews or policies of either agency. Author Joel Meyer was\nsupported by a National Defense Science and Engineering\nGraduate Fellowship.\nREFERENCES\n[1] G. Durantin, J.-F. Gagnon, S. Tremblay, and F. Dehais, “Using\nnear infrared spectroscopy and heart rate variability to detect mental\noverload,” Behavioural Brain Research, vol. 259, pp. 16–23, Feb.\n2014.\n[2] R. Olfati-Saber, “Flocking for multi-agent dynamic systems: algo-\nrithms and theory,” IEEE Transactions on Automatic Control, vol. 51,\nno. 3, pp. 401–420, Mar. 2006.\n[3] A. Howard, M. J. Matari´c, and G. S. Sukhatme, “Mobile Sensor\nNetwork Deployment using Potential Fields: A Distributed, Scalable\nSolution to the Area Coverage Problem,” in Distributed Autonomous\nRobotic Systems 5, 2002, pp. 299–308.\n[4] N. Michael and V. Kumar, “Planning and Control of Ensembles of\nRobots with Non-holonomic Constraints,” The International Journal\nof Robotics Research, vol. 28, no. 8, pp. 962–975, Aug. 2009.\n[5] J. Chen, R. Sun, and H. Kress-Gazit, “Distributed Control of Robotic\nSwarms from Reactive High-Level Speciﬁcations,” in IEEE 17th\nInternational Conference on Automation Science and Engineering\n(CASE), Aug. 2021.\n[6] K. Szwaykowska, L. M.-y.-T. Romero, and I. B. Schwartz, “Collective\nMotions of Heterogeneous Swarms,” IEEE Transactions on Automa-\ntion Science and Engineering, vol. 12, no. 3, pp. 810–818, July 2015.\n[7] H. Zhao, H. Liu, Y.-W. Leung, and X. Chu, “Self-Adaptive Collective\nMotion of Swarm Robots,” IEEE Transactions on Automation Science\nand Engineering, vol. 15, no. 4, pp. 1533–1545, Oct. 2018.\n[8] C. Sampedro, H. Bavle, J. L. Sanchez-Lopez, R. A. S. Fern´andez,\nA. Rodr´ıguez-Ramos, M. Molina, and P. Campoy, “A ﬂexible and\ndynamic mission planning architecture for UAV swarm coordination,”\nin 2016 International Conference on Unmanned Aircraft Systems\n(ICUAS), June 2016.\n[9] Y. Diaz-Mercado, S. G. Lee, and M. Egerstedt, “Distributed dynamic\ndensity coverage for human-swarm interactions,” in 2015 American\nControl Conference (ACC), July 2015.\n[10] A. Prabhakar, I. Abraham, A. Taylor, M. Schlaﬂy, K. Popovic, G. Di-\nniz, B. Teich, B. Simidchieva, S. Clark, and T. Murphey, “Ergodic\nSpeciﬁcations for Flexible Swarm Control: From User Commands to\nPersistent Adaptation,” Robotics: Science and Systems, June 2020.\n[11] E. Tsykunov, R. Agishev, R. Ibrahimov, L. Labazanova, A. Tleugazy,\nand D. Tsetserukou, “SwarmTouch: Guiding a Swarm of Micro-\nQuadrotors With Impedance Control Using a Wearable Tactile Inter-\nface,” IEEE Transactions on Haptics, vol. 12, no. 3, pp. 363–374, July\n2019.\n[12] G. K. Karavas, D. T. Larsson, and P. Artemiadis, “A hybrid BMI\nfor control of robotic swarms: Preliminary results,” in IEEE\/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\nSept. 2017.\n[13] C. Pinciroli and G. Beltrame, “Buzz: An extensible programming lan-\nguage for heterogeneous swarm robotics,” in IEEE\/RSJ International\nConference on Intelligent Robots and Systems (IROS), Oct. 2016.\n[14] D. Lee, A. Franchi, P. R. Giordano, H. I. Son, and H. H. B¨ulthoff,\n“Haptic teleoperation of multiple unmanned aerial vehicles over the\ninternet,” in IEEE International Conference on Robotics and Automa-\ntion, May 2011.\n[15] J. A. Preiss, W. Honig, G. S. Sukhatme, and N. Ayanian, “Crazyswarm:\nA large nano-quadcopter swarm,” in IEEE International Conference\non Robotics and Automation (ICRA), May 2017.\n[16] N. Michael, D. Mellinger, Q. Lindsey, and V. Kumar, “The GRASP\nMultiple Micro-UAV Testbed,” IEEE Robotics Automation Magazine,\nvol. 17, no. 3, pp. 56–65, Sept. 2010.\n[17] D. Pickem, P. Glotfelter, L. Wang, M. Mote, A. Ames, E. Feron, and\nM. Egerstedt, “The Robotarium: A remotely accessible swarm robotics\nresearch testbed,” in IEEE International Conference on Robotics and\nAutomation (ICRA), May 2017.\n[18] B. Kate, J. Waterman, K. Dantu, and M. Welsh, “Simbeeotic: A\nsimulator and testbed for micro-aerial vehicle swarm experiments,” in\nACM\/IEEE 11th International Conference on Information Processing\nin Sensor Networks (IPSN), Apr. 2012.\n[19] T. H. Chung, M. R. Clement, M. A. Day, K. D. Jones, D. Davis,\nand M. Jones, “Live-ﬂy, large-scale ﬁeld experimentation for large\nnumbers of ﬁxed-wing UAVs,” in IEEE International Conference on\nRobotics and Automation (ICRA), May 2016.\n[20] M. Wang, Z. Wang, J. Talbot, J. C. Gerdes, and M. Schwager, “Game-\nTheoretic Planning for Self-Driving Cars in Multivehicle Competitive\nScenarios,” IEEE Transactions on Robotics, vol. 37, no. 4, Aug. 2021.\n[21] K. Shah and M. Schwager, “Multi-agent Cooperative Pursuit-Evasion\nStrategies Under Uncertainty,” in Distributed Autonomous Robotic\nSystems, 2019.\n[22] ——, “GRAPE: Geometric Risk-Aware Pursuit-Evasion,” Robotics\nand Autonomous Systems, vol. 121, Nov. 2019.\n[23] A. Pierson, Z. Wang, and M. Schwager, “Intercepting Rogue Robots:\nAn Algorithm for Capturing Multiple Evaders With Multiple Pur-\nsuers,” IEEE Robotics and Automation Letters, vol. 2, no. 2, pp. 530–\n537, Apr. 2017.\n[24] A. Pierson and M. Schwager, “Controlling Noncooperative Herds with\nRobotic Herders,” IEEE Transactions on Robotics, vol. 34, no. 2, pp.\n517–525, Apr. 2018.\n[25] M. Day, L. Strickland, E. Squires, K. DeMarco, and C. Pippin,\n“Responding to unmanned aerial swarm saturation attacks with au-\ntonomous counter-swarms,” in Ground\/Air Multisensor Interoperabil-\nity, Integration, and Networking for Persistent ISR IX, May 2018.\n[26] I. Abraham and T. D. Murphey, “Decentralized Ergodic Control:\nDistribution-Driven Sensing and Exploration for Multiagent Systems,”\nIEEE Robotics and Automation Letters, vol. 3, no. 4, Oct. 2018.\n[27] A. Mavrommati, E. Tzorakoleftherakis, I. Abraham, and T. D.\nMurphey, “Real-Time Area Coverage and Target Localization us-\ning Receding-Horizon Ergodic Exploration,” IEEE Transactions on\nRobotics, vol. 34, no. 1, pp. 62–80, Aug. 2017.\n[28] G. Mathew and I. Mezi´c, “Metrics for ergodicity and design of ergodic\ndynamics for multi-agent systems,” Physica D: Nonlinear Phenomena,\nvol. 240, no. 4-5, pp. 432–442, Feb. 2011.\n[29] M. A. Goodrich, S. Kerman, and S.-Y. Jung, “On Leadership and Inﬂu-\nence in Human-Swarm Interaction,” Association for the Advancement\nof Artiﬁcial Intelligence (AAAI), p. 6, 2012.\n[30] P. Walker, S. Amirpour Amraii, N. Chakraborty, M. Lewis, and\nK. Sycara, “Human control of robot swarms with dynamic leaders,” in\nIEEE\/RSJ International Conference on Intelligent Robots and Systems,\nSept. 2014.\n[31] P. Walker, S. Amirpour Amraii, M. Lewis, N. Chakraborty, and\nK. Sycara, “Control of swarms with multiple leader agents,” in IEEE\nInternational Conference on Systems, Man, and Cybernetics (SMC),\nOct. 2014.\n[32] G. Swamy, S. Reddy, S. Levine, and A. D. Dragan, “Scaled Auton-\nomy: Enabling Human Operators to Control Robot Fleets,” in IEEE\nInternational Conference on Robotics and Automation (ICRA), May\n2020.\n[33] S. Hauri, J. Alonso-Mora, A. Breitenmoser, R. Siegwart, and P. Beard-\nsley, “Multi-Robot Formation Control via a Real-Time Drawing Inter-\nface,” in Field and Service Robotics, 2014, vol. 92, pp. 175–189.\n[34] S. Haubert, S. Leven, M. Varga, F. Ruini, A. Cangelosi, J.-C. Zuf-\nferey, and D. Floreano, “Reynolds Flocking in Reality with Fixed-\nWing Robots: Communication Range vs. Maximum Turning Rate,”\nin IEEE\/RSJ International Conference on Robots and Systems, Sept.\n2011, pp. 5015–5020.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/A Game Benchmark for Real-Time Human-Swarm Control.pdf"}
{"title":"WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments","authors":"Xi Chen, Tianyu Shi, Qingpeng Zhao, Yuchen Sun, Yunfei Gao, Xiangjun Wang","summary":"Recent advances in deep reinforcement learning (RL) have demonstrated complex\ndecision-making capabilities in simulation environments such as Arcade Learning\nEnvironment, MuJoCo, and ViZDoom. However, they are hardly extensible to more\ncomplicated problems, mainly due to the lack of complexity and variations in\nthe environments they are trained and tested on. Furthermore, they are not\nextensible to an open-world environment to facilitate long-term exploration\nresearch. To learn realistic task-solving capabilities, we need to develop an\nenvironment with greater diversity and complexity. We developed WILD-SCAV, a\npowerful and extensible environment based on a 3D open-world FPS (First-Person\nShooter) game to bridge the gap. It provides realistic 3D environments of\nvariable complexity, various tasks, and multiple modes of interaction, where\nagents can learn to perceive 3D environments, navigate and plan, compete and\ncooperate in a human-like manner. WILD-SCAV also supports different\ncomplexities, such as configurable maps with different terrains, building\nstructures and distributions, and multi-agent settings with cooperative and\ncompetitive tasks. The experimental results on configurable complexity,\nmulti-tasking, and multi-agent scenarios demonstrate the effectiveness of\nWILD-SCAV in benchmarking various RL algorithms, as well as it is potential to\ngive rise to intelligent agents with generalized task-solving abilities. The\nlink to our open-sourced code can be found here\nhttps:\/\/github.com\/inspirai\/wilderness-scavenger.","url":"http:\/\/arxiv.org\/abs\/2210.09026v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2210.09026v1","published":1665754781000,"comment":null,"pdf_text":"WILD-SCAV: Benchmarking FPS Gaming AI on\nUnity3D-based Environments\nXi Chen\nInspir.AI\nchenxi@inspirai.com\nTianyu Shi\nUniversity of Toronto\nty.shi@mail.utoronto.ca\nQingpeng Zhao\nNanjing University\nmf20150077@smail.nju.edu.cn\nYuchen Sun\nInspir.AI\nsunyuchen@inspirai.com\nYunfei Gao\nInspir.AI\nclaude@inspirai.com\nXiangjun Wang\nInspir.AI\nxj@inspirai.com\nAbstract\nRecent advances in deep reinforcement learning (RL) have demonstrated complex\ndecision-making capabilities in simulation environments such as Arcade Learn-\ning Environment [1], MuJoCo [2] and ViZDoom [3]. However, they are hardly\nextensible to more complicated problems, mainly due to the lack of complexity\nand variations in the environments they are trained and tested on. Furthermore,\nthey are not extensible to an open world environment to facilitate long-term explo-\nration research. To learn realistic task-solving capabilities, we need to develop an\nenvironment with greater diversity and complexity. We developed WILD-SCAV, a\npowerful and extensible environment based on a 3D open-world FPS (First-Person\nShooter) game to bridge the gap. It provides realistic 3D environments of variable\ncomplexity, various tasks, and multiple modes of interaction, where agents can\nlearn to perceive 3D environments, navigate and plan, compete and cooperate in\na human-like manner. WILD-SCAV also supports different complexities, such\nas conﬁgurable maps with different terrains, building structures and distributions,\nand multi-agent settings with cooperative and competitive tasks. The experimen-\ntal results on conﬁgurable complexity, multi-tasking, and multi-agent scenarios\ndemonstrate the effectiveness of WILD-SCAV in benchmarking various RL algo-\nrithms, as well as it is potential to give rise to intelligent agents with generalized\ntask-solving abilities. The link to our open-sourced code can be found here1.\n1\nIntroduction\nA challenging benchmark is essential to fully assess the capabilities of deep reinforcement learning\n(RL) algorithms. Many simulators have been used to challenge and evaluate RL algorithms, bringing\nsigniﬁcant contributions and development to the RL community. For example, the Arcade Learning\nEnvironment (ALE) [1] presents a valuable test-bed with a collection of Atari 2600 games to evaluate\nRL algorithms. Deep Q Network (DQN) successfully learns to play the Atari 2600 games given screen\npixels as inputs and achieves outstanding performance on ALE [4]. Other examples, e.g., Dueling\nDQN [5], Prioritized DQN [6], Quantile Regression DQN [7], also largely beneﬁted from ALE as\n1https:\/\/github.com\/inspirai\/wilderness-scavenger\nPreprint. Under review.\narXiv:2210.09026v1  [cs.LG]  14 Oct 2022\nthey were developed. To support more general task scenarios, Gym [8], built by OpenAI, exposes\nstandard interfaces for testing different RL algorithms over a variety of environments, allowing\nresearchers to easily compare the performance of alternative approaches.\nRecently, RL methods have demonstrated superior ability to humans on multiple Atari tasks. Deep-\nMind built Agent57 [9], which is the ﬁrst deep reinforcement learning model that surpasses human\nbaseline performance on all Atari games. The success of Agent57 comes from its meta-controller\ndesign to balance efﬁcient exploration and exploitation. As we are building more and more intelligent\ntask-solving agents, it appears that memory utilization [10], curiosity seeking [11], and exploration-\nexploitation balancing [9] have become some of the key points to motivate these developments.\nTo explore the potential of RL algorithms and push forward the performance limits, the community\nkeeps seeking more challenging environments that generally involve real-world tasks and correspond-\ning interactions. For example, navigation and resource collection tasks [12] are very common in the\nreal world and are integrated into several environments, e.g., Coin-Run [13], Deepmind Lab [14].\nFurthermore, by introducing more comprehensive forms of observational inputs and more ﬂexible\naction types, agents can be evaluated on not only navigation and resource collection but also more\ncomplex tasks like the move-and-shoot task in ViZDoom [3]. While adding action options may lead\nto an exponential increase in the complexity of the policy, several advanced RL algorithms have been\nproposed to solve these challenges. For instance, hierarchical Reinforcement Learning (HRL) [15]\nhas been proposed to solve the huge action space and exploration challenges, in which the high-level\nlayer learns policy through options while the low-level layer executes basic options, such as motion,\nattack, tool, and resources.\nFollowing the success of ViZDoom, training intelligent agents with general task-solving capabilities\nin open-world environments has attracted increasing attention. However, the lack of adequate training\nand testing benchmark environments remains an obstacle to research in this ﬁeld. ViZDoom is a useful\nand fast 3D environment but still oversimpliﬁed in both the visual style and the world composition\ncompared to the simulation appearance in modern 3D games, which is relatively detached from the\nmore practical side of the community’s needs. In this context, the goal of WILD-SCAV is to advance\nresearch in the ﬁeld of open-world intelligent agent learning. As a stepping stone to the ultimate\ngoal of learning highly intelligent agents’ “living” in the virtual world, we decided to ﬁrst focus on\nopen-world FPS games. With the popularity of battle royale games, such an environment can serve as\na ﬂexible and open playground for both algorithmic research in RL and solution optimization in the\ngame industry.\nTo the best of our knowledge, WILD-SCAV is the ﬁrst FPS-based environment that allows agents to\nexplore in such a 3D world environment. Our main contributions are listed as follows:\n• WILD-SCAV provides a customizable 3D environment, where the ground landscape, the\nstructure of houses, and the placement of various types of objects can be generated with\nPCG (Procedural Content Generation) technique [16], which is very suitable for NPC\nresearch [12].\n• WILD-SCAV focuses on learning intelligent agents in the open-world environment. It\nprovides more spaces for exploration, more ﬂexibility in task design and open challenges\nfor generalization with diverse learning scenarios.\n• WILD-SCAV supports agent training in multi-task and multi-agent scenarios, similar to the\ncommon setup in recently popular battle royale games (e.g., PUBG [17]), which includes\ntasks like random target navigation, competitive\/cooperative resources gathering, and free\nﬁght etc.\n2\nBackground: 3D open-world FPS game environment\nAI-powered next-generation gaming experiences for open-world games have attracted increasing\nattention after the success of AI in StarCraft II [18] and DOTA [19] as the next big challenge.\nHowever, the lack of satisfactory testing environments remains an obstacle to research in this area. In\nthis context, the goal of this competition is to promote research on intelligent agent learning for 3D\nopen-world FPS games. In 3D open-world environments, an agent perceives its environment in a\nhuman-like manner, using visual scenes as input. Intelligent agents are expected to integrate visual\nperception and contextual game features, process incomplete information, deal with the dynamic\n2\nvariation of environments and multiplayer enemies, and then perform long-term planning. In addition,\nto maximize scores on the tasks set, agents must generalize their learned skills to unknown test\nenvironments. For the competition, we provide an FPS game environment similar to popular Battle\nRoyale games (e.g., PUBG), where multiple players compete against each other for limited supply\nresources. We will evaluate each trained agent on the randomly generated battlegrounds.\n3\nWILD-SCAV Environment\n3.1\nObservation Space\nThe gameplay interface provides multiple sources of information about the agent (e.g., location,\norientation) and its surrounding environment. The observation mainly consists of two parts, visual\nperception inputs and game variables.\nVisual perception. We implemented an efﬁcient way to compute a low-resolution depth map (e.g.\n10 × 10) from the agent’s camera using only the location, orientation values, and mesh data of the\nstatic scenes, to enable efﬁcient feature learning in the scenario of large 3D open worlds. In this\nway, we trade-off between granularity of visual information and computation efﬁciency to allow\nresearchers to get most out of this new environment when using it for agent training. However, to\nmake our environment more usable for more generalized tasks, we also added support for calculating\nhigh-resolution depth maps, LIDAR detection, and the ﬁrst person camera images, as shown in\nFigure 1 and Figure 2. A short video visualization of the camera images can be found here.2\nGame variables. We also provide access to multiple game variable information. The variables\ninclude current location and orientation of the agent, state of motion (e.g. on ground or in the air),\nhealth, state of combat (e.g. ﬁring, being hit), and task-related metrics (e.g. target location, number\nof collected resources). These game variables can be essential to task-solving in more complex tasks.\nFigure 1: left: depth map visualization example.\nright: schematic diagram of LIDAR perception.\nThe arrows means the depth map is a 360-degree panorama.\nFigure 2: Images of the agent’s ﬁrst person camera from different viewing angles.\nThe above observations provide rich and efﬁcient information for the agent to learn optimal policies.\nUsers have the freedom to construct customized observational features for different training tasks.\n2https:\/\/youtu.be\/VrXPF8HoIbU\n3\n3.2\nAction Space\nWILD-SCAV provides comprehensive and structured action spaces. Users can easily explore the\naction spaces to facilitate skill learning of the agent. To be more speciﬁc, it support actions for\nnavigation task, such as walking direction (WALK_DIR); walking speed (WALK_SPEED). It also supports\ncamera angle direction change for wider range of observation, such as horizontal camera angle\n(yaw) between two frames (TURN_LR_DELTA) and vertical camera angle (pitch) between two frames\n(LOOK_UD_DELTA).\nUnlike previous shooting game environments [3], our environment has more complex terrain, where\nagents have huge space of movement trajectories when moving around. An agent can jump up stairs\n(JUMP) and pick up randomly spawned resources (PICKUP). These characteristics of WILD-SCAV\nmake it a favorable testbed for cooperative and competitive resource gathering tasks.\nIn addition, in contrast to multi-agent cooperative and competitive environments like Multi-Agent\nParticle environment [20], we also introduce the combat game system to improve the complexity of\nthe environment. There are two combat actions: (a) whether to ﬁre the weapon and cost one bullet at\nthe current time step (SHOOT) and (b) whether to reﬁll the weapon’s clip using spare ammo (RELOAD).\nThe detailed tasks illustration will be analyzed in 3.3 and a summarized action space description for\neach task is shown in Table 3.2.\nAction Class\nNavigation\nSupply Gathering\nSupply Battle\nType\nRange\nWALK_DIR\n\u0013\n\u0013\n\u0013\nﬂoat\n[0., 360.]\nWALK_SPEED\n\u0013\n\u0013\n\u0013\ninteger\n[0, 10]\nTURN_LR_DELTA\n\u0013\n\u0013\n\u0013\nﬂoat\n[−∞, ∞]\nLOOK_UD_DELTA\n\u0013\n\u0013\n\u0013\nﬂoat\n[−∞, ∞]\nJUMP\n\u0013\n\u0013\n\u0013\nboolean\nTrue \/ False\nPICKUP\n\u0017\n\u0013\n\u0013\nboolean\nTrue \/ False\nSHOOT\n\u0017\n\u0017\n\u0013\nboolean\nTrue \/ False\nRELOAD\n\u0017\n\u0017\n\u0013\nboolean\nTrue \/ False\nTable 1: Action space description for the typical tasks\n3.3\nTypical Tasks\nIn this section, we will discuss some typical tasks supported by the WILD-SCAV environment. The\nbasic tasks are navigation, supply gathering, and battle. By combinations of different typical tasks,\ndifferent experiment scenarios can be created to validate the performance of different RL algorithms.\n3.3.1\nNavigation\nThe challenge for the agent is to navigate as quickly as possible from a starting location to a\ndestination (both randomly selected) in a randomly generated open world. The world consists\nof various structures, such as buildings, trees, rocks, and lakes. Ideally, an efﬁcient and general\nnavigation strategy must be able to skillfully use these elements and ﬂexibly adapt to new worlds that\nhave not been seen yet.\n• State: The observation is the target position, current position and the depth image.\n• Action: Walking direction, walking speed, camera angle (yaw, pitch), and jump.\n• Reward: The agent is rewarded when it reaches the target point.\n• Evaluation criteria: Time consumed to reach the target, i.e., the episode length.\n3.3.2\nSupply Gathering\nThe challenge for the agent is to collect as many supplies as possible by opening the blue supply\nboxes in a randomly generated open world with unknown supply distribution. Supply boxes may\nappear at any accessible location in the open world, for example, on outdoor grounds or on a certain\n4\nFigure 3: Navigation task: The agent is trying to navigate to the target location\nﬂoor of a house, hiding behind a tree or a stone. To make it even more challenging, we have designed\na special mechanism to determine the number of supplies stored in each supply box. In general,\na supply box inside a building contains a signiﬁcantly higher number of supplies than an outdoor\nbox, while the number of supply boxes inside buildings could be relatively small. These agents are\nencouraged to explore both outdoor areas and indoor spaces to maximize their collection of supplies.\nAlso, the actual supply quantity in each box has some randomness but is constrained within given\nranges.\nFigure 4: Supply gathering task 1: two groups of agents searching supplies in the housing area\nFigure 5: Supply gathering task 2: the agent team trying to collect supplies as fast as possible\n• State: The observation are the locations of all nearby supplies, the speciﬁc number of\nsupplies at each location, the agent’s current location and the depth image.\n• Action: Walking direction, walking speed, camera angle (yaw, pitch), and jump.\n5\n• Reward: To add variability to the task, we can set different termination criteria. We design\ntwo separate sub tasks. In the ﬁrst task, the agent will get a high reward if it can collect\nsupply as many as possible. In the second task, the agent is rewarded for collecting the\ntarget supply as fast as possible.\n• Evaluation criteria: In the ﬁrst sub-task, we use the number of collected supplies to\nevaluate. In the second sub-task, we use episode length as evaluation criteria.\n3.3.3\nSupply Battle\nBased on the supply gathering, multiple agents are dropped into a large, randomly generated world\n(terrain, buildings, plants, supplies) and compete for supplies. These agents can ﬁght with weapons\nand respawn when killed. During combat, agents are encouraged to take cover to ensure their safety.\nWhen an agent dies, some of his supplies drop and can be collected by other agents. Since the total\nnumber of supplies in the game is limited, agents should learn to optimize their search strategy to\ncollect supplies faster. With the introduction of the combat system, agents can also use their combat\nadvantage to snatch supplies from other agents to speed up the accumulation of supplies.\nThe observed state and actions of the agent are very similar to the material collection task of the\nprevious task or are added based on it. In addition to its own attributes in the gathering task,\nobservation can also include the state of other agents in the ﬁeld of view, but if the agent is not in the\nﬁeld of view, this state cannot be obtained, which leads to the non-stationarity and partial observation\nof the environment, making it more complex and difﬁcult to learn. In addition to basic actions such\nas movement, we further extend the action types by adding the behavior of shooting attacks and\nchanging bullets. It is expected to learn a strategy that can obtain their supplies by killing other agents\nand also avoid being killed by other agents.\n4\nExperiments\nIn this section, we present the results and analysis of the experiments based on the combination of the\nelements of the typical task from the above section 3.3. Note that we could extend more combinations\nof different typical tasks, but for this paper, we decided to focus on several representative experiments\nand demonstrate how our environment can be conﬁgured to assess the performance of different RL\nmethods across different learning scenarios.\nFor a fair comparison, we adopt the same resource conﬁguration and ﬁxed hyper-parameters for all\ntasks and experiments to test the adaptability and scalability of the compared algorithms. We use the\nsame machine with 96 Intel(R) 8163 (2.50GHz) CPU cores to run all algorithms. For simplicity and\nbetter reproducibility, we construct all training pipelines using the standard trainer APIs from the\ncommonly used RLlib [21] among the research community.\nTo keep it simple and clear, for the benchmark experiments in this paper, we only select three\ncommonly used RL algorithms, including IMPALA [22] Proximal Policy Optimization (PPO) [23]\nand Asynchronous Advantage Actor-Critic (A3C) [24]. For each episode rollout during training,\nwhile it is possible to let the agent explore the environment for a long time, we can deﬁnitely speed\nup the learning by setting appropriate episode lengths T for different tasks. In our experiments, we\nempirically choose the episode length for each task. Speciﬁcally, we ﬁx the maximum episode length\nto T = 400 for the single-agent navigation and the multi-agent target capture task, and we ﬁx the\nepisode length to T = 600 for the multi-agent supply gathering task.\n4.1\nSingle-Agent Navigation\nFor an open-world 3D environment, the navigation task is the basis for completing other more\ncomplex tasks. For navigation tasks, even if the environment is very complex, the agent can quickly\nlearn a relatively good strategy to reach any target point thanks to the easy randomization of the start\nlocation. The agent is given a reward of +1 if it reaches the target, and 0 otherwise. However, such a\nreward distribution can be very sparse, especially for large map sizes.\nFigure 6 shows the task training curve of the evaluated algorithms for different map sizes and\nscene complexity. Overall, for tasks of different complexity and different sizes, all algorithms can\n6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAP 103\nPPO\nA3C\nIMPALA\nMAP 101\nPPO\nA3C\nIMPALA\nMAP 008\nPPO\nA3C\nIMPALA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAP 104\nPPO\nA3C\nIMPALA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\nMAP 102\nPPO\nA3C\nIMPALA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\nMAP 014\nPPO\nA3C\nIMPALA\nFigure 6: Average episode rewards during training on different sizes of maps. Small (100 × 100):\n103, 104; Medium (200 × 200): 101, 102; Large (500 × 500): 008, 014.\nquickly ﬁnd some decent strategies to reach the target point, and they will always slowly explore new\nstrategies to reach the target in faster ways.\nSeen from the results in Figure 6 and Table 2, with large size and higher scene complexity of the\nmap, the average episode reward becomes smaller, resulting in more “slow-learning” steps. To help\nimprove the learning efﬁciency, one promising approach is to design better Intrinsic curiosity-driven\nreward to encourage more efﬁcient exploration [25].\nIn addition, both A3C and PPO can learn a good strategy, while the performance of IMPALA is\nrelatively poor. PPO performs best in maps with a small size and shows the highest improvement\nspeed at the early stage, while A3C performs better under larger sizes of the map and may need longer\ntime steps to reach a high-performance level. IMPALA in this task indeed shows steady growth of\nperformance but has a much slower speed and higher ﬂuctuation.\nA3C\nPPO\nIMPALA\nMap\nEpsLen\nSuccRate\nEpsLen\nSuccRate\nEpsLen\nSuccRate\n101\n219.4 (132.8)\n0.73\n228.4 (140.1)\n0.69\n330.9 (124.4 )\n0.27\n102\n185.2 (111.5)\n0.90\n170.6 (115.0)\n0.90\n330.5 (119.8)\n0.29\n103\n158.2 (85.4)\n0.96\n87.5 (73.6)\n0.97\n136.8 (130.8)\n0.82\n104\n118.8 (77.9)\n0.96\n63.9 (45.4)\n0.99\n109.5 (122.5)\n0.86\n008\n255.9 (126.9)\n0.71\n283.1 (118.6)\n0.66\n354.1 (105.0)\n0.19\n014\n239.6 (124.3)\n0.74\n284.0 (121.7)\n0.62\n353.4 (97.8)\n0.23\nTable 2: Evaluation results on the single agent navigation task. We compare average episode lengths\nand success rates achieved with the learned policies on different maps. For the episode length, we\nreport both the mean value and the standard deviation (in the brackets).\n4.2\nCooperative Supply Gathering.\nThe second task is a multiplayer cooperative task. In this task, multiple agents are taking action as\na team to collect as many supplies as possible within an episode. In this task, the agent is trying to\nlearn a policy that jointly considers their own states (e.g., current agent location), the supply state\n(e.g., location of a nearby supply), and the teammate state (e.g., location of other agents) to collect the\n7\nsupplies in a more efﬁcient way than the single-agent scenario. We run all the experiments on map\n101, where randomly generated supplies are distributed denser near the center of the map and sparser\nfar away from the center. For each supply collected, the agent is given a small positive reward.\nFigure 7 shows the change in the average number of supplies collected in an episode during training.\nWe can see that PPO learns to handle the task quite fast, with only 2e6 time steps when reaching an\nacceptable level of performance. IMPALA has the lowest improvement speed at the start of training\nbut shows a relatively steady and moderate speed of performance growth. However, A3C performs\nnot very well and seems to require more time steps of experience to basically handle the task. The\nresults may have shown the low exploration efﬁciency of A3C in case of sparse reward distribution\nand that PPO can be a favorable choice in such an open-world environment.\nIn such a multi-agent case, how to coordinate the actions of all agents in order to collect the most\nsupplies in a limited time is still a challenging problem. Although here we only test the performance\nof three commonly used algorithms, some recently proposed MARL algorithms [26, 27] can be\nsimply applied with little modiﬁcation of the environment. From the results in Table 3, we can see\nthat the best method, PPO, can only reach a level of around 100 average supplies while there are\nmore than 200 supplies on the entire map. This may be due to the exploration difﬁculty that agents\nare more likely to collect supplies in a small sub-region where supplies are relatively close to the\nagents’ current locations other rather than explore large open areas. But some “treasure” places with\nmore densely distributed supplies can be far away from the start locations. As the number of agents\nincreases, supplies in a sub-region can be quickly depleted by a small group of agents, leaving other\nagents a sparser reward distribution, which suggests that the environmental setups should be more\nsubtle on the multi-agent task when the number of agents increase, to avoid insufﬁcient local optimal.\nA3C\nPPO\nIMPALA\nMap\nSupplyNum\nSupplyNum\nSupplyNum\n101\n49.49 (18.67)\n98.92 (52.62)\n91.68 (45.90)\nTable 3: Evaluation results on the cooperation supply gathering task. We report both the mean and\nstandard deviation (in the brackets) of the total collected supply numbers in 3 minutes with 4 agents.\n4.3\nCooperative Target Capture\nThe last task is also a multiplayer cooperative task, where multiple agents are trying to capture the\ntarget supply as fast as possible cooperatively. The supply is hidden at some point in the open world.\nOnce an agent captures it, the episode is considered a success and ﬁnished, and a +1 reward is given\nto this agent.\nIn Figure 8, we show the learning results of different methods where PPO can achieve the highest\nreward. When collecting the state and supply position of each agent, the strategy will assign the\nnearest agent to quickly catch this supply. For the same map conﬁguration, such a multi-agent\nenvironment can solve the task faster than a single-agent environment compared to the results in\nTable 4.1. Similar to previous experiments, we report the mean and standard deviation in the bracket.\nNote that the standard deviation is a bit high, which is because we randomly generate the agent in\nthe map without ﬁxed agents’ start and target locations. To conclude, we ﬁnd that PPO once again\nachieves the best performance with the smallest episode length.\nA3C\nPPO\nIMPALA\nMap\nEpsLen\nEpsLen\nEpsLen\n101\n143.18 (122.71)\n94.69 (98.55)\n251.58 (166.72)\nTable 4: Evaluation results on the multi-agent target capture task. We report both the mean value and\nstandard deviation of the episode length to reach the target location.\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTotal Time Steps\n1e7\n20\n40\n60\n80\n100\nNumber of Supply\nCooperative Supply Gathering (MAP 101)\nPPO\nA3C\nIMPALA\nFigure 7: Average episode rewards during train-\ning on the supply gathering task. Five agents are\nrandomly generated in the map 101. Each agent\nreceives +1 reward for each supply collected\nwithin the episode time limit. The agent team\ntries to cooperatively collect supplies as many as\npossible.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTotal Time Steps\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess Rate\nCooperative Target Capture (MAP 101)\nPPO\nA3C\nIMPALA\nFigure 8: Average episode reward during training\non the cooperative target capture. The agent team\nis encouraged to capture the target supply as fast\nas possible\n5\nRelated work\nIn general, modern 3D FPS games are inherently incomplete information games that are extremely\nhard to learn winning strategies in multiple-player scenarios and are known to have no optimal\npolicy. Despite the difﬁculties, there have been attempts over the past decade to apply reinforcement\nlearning in FPS games. To our best knowledge, the most inﬂuential work is the Augmented DRQN\nmodel [28], where the method leverages both visual input and the game feature information (e.g.,\npresence of enemies or items) and modularizes the model architecture to incorporate independent\nnetworks to handle different phases of the game. Their approach successfully learned a competitive\nFPS agent by minimizing a Q-learning objective and showed better performance than average human\nplayers. Following this success, more work on learning FPS game agents has been proposed, such\nas Arnold [29] which beneﬁts from the Action-Navigation architecture, Divide and Conquer deep\nreinforcement learning [30] which further reﬁned the idea of separating the control strategies of\nmap exploration from enemy combat. Although these methods have shown promising results,\ntheir training and evaluation context is largely limited to old-fashioned video games with relatively\nsmall world sizes, such as VizDoom (originally 1993) and Quake 3 (originally 1999). Recently,\nPearce and Zhu [31] tried to learn an FPS agent to play CSGO, a phenomenal modern 3D FPS\ngame with high-resolution visual rendering. This new game environment not only introduces more\ncomputational burden (mostly due to extracting visual features) but also makes it more difﬁcult\nfor the agent to explore and adapt to the game world efﬁciently. The new approach addressed the\nchallenge primarily by using behavioral cloning, and the learned agents showed reasonably good\nperformance compared to normal human players in the Deathmatch mode. Note that there are also\nother 3D simulators, such as Mujoco [2], DeepMind Lab [14], etc. They are not extensible to more\ncomplicated real-world problems. On the other hand, our simulator is more suitable for open-world\nexploration. Recently, MINEDOJO has been developed with thousands of diverse open-ended\ntasks [32]. With MINEDOJO’s data, one can leverage large pre-trained video language models to\nlearn reward functions and then guide agent learning in various tasks. Building upon these works, we\nseek to further expand the frontiers of intelligent agent learning in modern large-scale open-world\ngames.\n6\nFuture work\nAs discussed in Section 3.3, we can create great combinations of different tasks (i.e., navigation,\nsupply gathering, battle) to support various experiments. Besides these representative experiments, as\ndiscussed in Section 4, we also tried other experiments. However, we ﬁnd that traditional methods,\n9\nas discussed in Section 4, will fail in more challenging experiments. For example, we randomly\ngenerated ten agents on map 103, as shown in Figure 9. These agents are controlled by the random\npolicy while we train another agent, which is controlled by A3C. However, we ﬁnd that the trained\nagent is hard to learn an appropriate behavior because it cannot go to the target point successfully.\nWe suggest that there are a few potential research ideas for future improvement. Firstly, when the map\nis becoming larger, the agent will become hard to ﬁnd an enemy, which will be a barrier to efﬁcient\nexploration. Curiosity-driven reinforcement learning method can be utilized to encourage exploration\nin sparse reward setting [33, 34]. Secondly, we ﬁnd that when the agents’ number increase, the\nlearned agent is hard to process in such a large observation space, and the learning becomes very\ndifﬁcult. In the future, we can investigate how to better capture the mutual interplay of different\nagents through communication, such as graph neural network [35]. Lastly, as WILD-SCAV can\nsupport Procedural Level Generation (PCG), it is also worth studying PCG-based reinforcement\nlearning to improve the generalizability of our agent [36].\n7\nConclusion\nIn this paper, to bridge the gap with realistic NPC gaming problems, we present WILD-SCAV, the\nﬁrst realistic 3D FPS-based environment, with the support for conﬁgurable complexities, multi-\ntasks, and multi-agents. Built upon PCG world generation techniques, WILD-SCAV enables the\nconﬁgurations of object types, areas, locations, orientations, etc. We have created several demo\ntasks, i.e., navigation, supply gathering, and supply battle, to evaluate the performance of different\nRL methods and enable researchers to develop more powerful algorithms through conﬁgurable\nenvironments. To facilitate further research development, we open-sourced our code for the simulator\nand training agents with A3C, PPO, and IMPALA. We also host the open-world FPS game AI\ncompetition to attract global researchers to innovate on the algorithms. Winners will be selected\nbased on the evaluation of randomly generated environments. We believe WILD-SCAV could further\npush forward the development of AI algorithms in the 3D Open World, bringing it a step closer to\nintelligent and generalized task-solving agents in realistic NPC research.\nReferences\n[1] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\n[2] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE\/RSJ international conference on intelligent robots and systems, pages\n5026–5033. IEEE, 2012.\n[3] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski.\nVizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE\nconference on computational intelligence and games (CIG), pages 1–8. IEEE, 2016.\n[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[5] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.\nDueling network architectures for deep reinforcement learning. In International conference on\nmachine learning, pages 1995–2003. PMLR, 2016.\n[6] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\narXiv preprint arXiv:1511.05952, 2015.\n[7] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforce-\nment learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 32, 2018.\n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n10\n[9] Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvit-\nskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human\nbenchmark. In International Conference on Machine Learning, pages 507–517. PMLR, 2020.\n[10] Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent\nexperience replay in distributed reinforcement learning. In International conference on learning\nrepresentations, 2018.\n[11] Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andew Bolt, et al. Never\ngive up: Learning directed exploration strategies. arXiv preprint arXiv:2002.06038, 2020.\n[12] Eloi Alonso, Maxim Peter, David Goumard, and Joshua Romoff. Deep reinforcement learning\nfor navigation in aaa video games. arXiv preprint arXiv:2011.04764, 2020.\n[13] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying\ngeneralization in reinforcement learning. In International Conference on Machine Learning,\npages 1282–1289. PMLR, 2019.\n[14] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich\nKüttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv\npreprint arXiv:1612.03801, 2016.\n[15] Shihong Song, Jiayi Weng, Hang Su, Dong Yan, Haosheng Zou, and Jun Zhu. Playing fps\ngames with environment-aware hierarchical reinforcement learning. In IJCAI, pages 3475–3482,\n2019.\n[16] Julian Togelius, Alex J Champandard, Pier Luca Lanzi, Michael Mateas, Ana Paiva, Mike\nPreuss, and Kenneth O Stanley. Procedural content generation: Goals, challenges and actionable\nsteps. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.\n[17] Wikipedia.\nPUBG: Battlegrounds — Wikipedia,\nthe free encyclopedia.\nhttp:\n\/\/en.wikipedia.org\/w\/index.php?title=PUBG%3A%20Battlegrounds&oldid=\n1090843995, 2022. [Online; accessed 09-June-2022].\n[18] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, et al.\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782,\n2017.\n[19] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu,\nMarcello Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial\nimages. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 3974–3983, 2018.\n[20] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.\nMulti-agent actor-critic for mixed cooperative-competitive environments. Advances in neural\ninformation processing systems, 30, 2017.\n[21] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph\nGonzalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement\nlearning. In International Conference on Machine Learning, pages 3053–3062. PMLR, 2018.\n[22] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl\nwith importance weighted actor-learner architectures. In International Conference on Machine\nLearning, pages 1407–1416. PMLR, 2018.\n[23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[24] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Re-\ninforcement learning through asynchronous advantage actor-critic on a gpu. arXiv preprint\narXiv:1611.06256, 2016.\n11\n[25] Joshua Hare.\nDealing with sparse rewards in reinforcement learning.\narXiv preprint\narXiv:1910.09281, 2019.\n[26] Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms\nfor decentralized multi-agent reinforcement learning. In International Conference on Machine\nLearning, pages 15007–15049. PMLR, 2022.\n[27] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao\nTang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv\npreprint arXiv:2002.03939, 2020.\n[28] Guillaume Lample and Devendra Singh Chaplot. Playing fps games with deep reinforcement\nlearning. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.\n[29] Devendra Singh Chaplot and Guillaume Lample. Arnold: An autonomous agent to play fps\ngames. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.\n[30] Georgios Papoudakis, Kyriakos C Chatzidimitriou, and Pericles A Mitkas. Deep reinforcement\nlearning for doom using unsupervised auxiliary tasks. arXiv preprint arXiv:1807.01960, 2018.\n[31] Tim Pearce and Jun Zhu. Counter-strike deathmatch with large-scale behavioural cloning. arXiv\npreprint arXiv:2104.04258, 2021.\n[32] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.\n[33] Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven rein-\nforcement learning. Theory in Biosciences, 131(3):139–148, 2012.\n[34] Lulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen, Changjie Fan,\nYang Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-\ndriven exploration. Advances in Neural Information Processing Systems, 34:3757–3769, 2021.\n[35] Tianyu Shi, Jiawei Wang, Yuankai Wu, Luis Miranda-Moreno, and Lijun Sun. Efﬁcient\nconnected and automated driving system with multi-agent graph reinforcement learning. arXiv\npreprint arXiv:2007.02794, 2020.\n[36] Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and\nSebastian Risi. Illuminating generalization in deep reinforcement learning through procedural\nlevel generation. arXiv preprint arXiv:1806.10729, 2018.\n[37] Jie Hu, Wang gen Wan, and Xiaoqing Yu. A pathﬁnding algorithm in real-time strategy game\nbased on unity3d. In 2012 International Conference on Audio, Language and Image Processing,\npages 1159–1162. IEEE, 2012.\nA\nAppendix\nA.1\nGenerating benchmark maps with PCG\nTo demonstrate the potential in using our benchmark environment to learn robust agent policies with\nreasonably good capacity of generalization, we created the training maps with different sizes, house\nnumbers, house densities and storey numbers. For each size of the map, we ﬁrst select the number\nof houses, the house density and the range of the number of ﬂoors of a house. Then we randomly\nsample from all the legal locations where to generate the houses based on the selected density. After\nthis, speciﬁc building structure of the house and the storey numbers are randomly selected from the\npredetermined asset pool. In this way, we can easily generate thousands of different maps. But in this\npaper we focus on a small scale evaluation where we only generate 6 maps of 3 sizes to compare the\nperformance of different algorithms on various tasks. Figure 9 shows the top view of the generated\nmaps and the detailed generation conﬁgurations of them are given in Table A.1.\n12\nFigure 9: Different maps visualization. We can see different terrain shapes and different housing\ndensity. For map 008 and map 014, we select two big maps. For map 101,102,103,104, we randomly\ngenerated a big map then cut out a small area of the big map.\nTable 5: Generation conﬁgurations of the benchmark maps\nMap ID\nSize\nHouse numbers\nHouse density\nStorey numbers\n008\n500 × 500\n12\nlow\n2-3\n014\n500 × 500\n15\nhigh\n2-3\n101\n200 × 200\n4\nlow\n1\n102\n200 × 200\n8\nhigh\n2-3\n103\n100 × 100\n2\nlow\n1\n104\n100 × 100\n4\nhigh\n2-3\n13\nA.2\nComputation cost and simulation speed\nWe also evaluate the simulation speed of different number of workers and agents, this result is shown\nin table A.2. All the results are tested on a single AMD 5800H CPU core.\nTable 6: Throughputs (total FPS) of the environment when running on a single CPU machine with\ndifferent numbers of concurrent process and agents. The maximum throughput for each number of\nagents is bolded.\nthroughput\n# process\n1\n2\n4\n6\n8\n10\n# agents\n1\n499.61\n996.90\n1974.84\n2766.21\n3506.31\n4187.03\n5\n500.18\n996.94\n1609.81\n1463.60\n1864.10\n2023.76\n10\n500.54\n997.62\n1095.73\n960.10\n1426.10\n1246.24\nA.3\nReward Conﬁguration\nWe designed the reward functions for the three evaluation tasks simply by translating the agent state\ninto a binary reward of whether reaching the goal. The detailed operations to compute the reward are\ngiven below in a form of Python code snippets.\n• Single-Agent Navigation\nstate = self.update_state ()\ncurr_loc = self.get_location (state)\ntarg_loc = self. target_location\nif self.get_distance(curr_loc , targ_loc) <= 1:\nreward = 1\nelse:\nreward = 0\n• Cooperative task 1: supply gathering\nstate = self.get_states(agent_id)\nif state.num_supply > self. collected_supply [agent_id]:\nrewards[agent_id] = 1\nself. collected_supply [agent_id] = state.num_supply\nelse:\nrewards[agent_id] = 0\n• Cooperative task 2: target capture\nstate = self.get_states(agent_id)\ncurr_loc = self.get_location (state)\ntarg_loc = self. target_location\nif self.get_distance(curr_loc , targ_loc) <= 1:\nrewards[agent_id] = 1\nelse:\nrewards[agent_id] = 0\nA.4\nEnvironment interface\nWILD-SCAV can support popular Gym interface [8]. The typical interface is shown below. To\nfacilitate the design of new algorithm and benchmark comparison, our gaming interface can support\ndifferent maps, different locations, different reward design for different tasks.\n% Define\ngame\nbackend\ngame = Game(\nmap_dir=\"\/path\/to\/map_data\",\nengine_dir=\"\/path\/to\/engine_backend \",\n)\n14\ngame.init ()\n% Create\nenvironment\nenv = gym.make(\nid=\"WildScav -Navigation -v0\",\nenv_config={\n\"engine\": game\n\"map_id\": 1,\n\"timeout\": 30 ,\n\"start_location\": [0, 1, 0],\n\" target_location \": [5, 0, 3],\nobs=[depth_image , position],\nreward= RewardManager .add(’Navigation ’)\n}\n)\nA.5\nModel description\nWe also provide the training details of the RL algorithms on WILD-SCAV using standard RLlib\nlibrary [21]. We run all experiments on a Linux server with 96 CPU cores. We ﬁx the number of\nexperience sampler workers to 80 (1 CPU per worker) and allocate the rest of the CPUs to the master\nlearner. We use Adam as the default optimizer for all models trained in the experiments. The detailed\ntraining conﬁgurations for experiments on PPO, A3C, IMPALA can be found in Table 7, Table A.5,\nTable A.5 respectively.\nTable 7: Training conﬁguration of PPO experiments\nParameter Name\nValue\nlearning rate\n0.00001\ntrain batch size\n32000\nSGD mini-batch size\n256\nrollout fragment length\n400\nentropy coefﬁcient\n0\nvalue loss coefﬁcient\n1.0\nKL-divergence coefﬁcient\n0.2\nParameter Name\nValue\nlearning rate\n0.0001\nrollout fragment length\n400\nentropy coefﬁcient\n0.01\nvalue loss coefﬁcient\n0.5\nTable 8: Training conﬁguration of A3C\nParameter Name\nValue\nlearning rate\n0.0005\ntrain batch size\n32000\nrollout fragment length\n400\nentropy coefﬁcient\n0.01\nvalue loss coefﬁcient\n0.5\nTable 9: Training conﬁguration of IMPALA\nA.6\nNetwork architecture description\nWe use the following network architecture conﬁguration to encode the information.\n# FullyConnectedNetwork\n# Number of hidden\nlayers to be used.\n\"fcnet_hiddens\": [256 , 256],\n# Activation\nfunction\ndescriptor.\n# Supported\nvalues are: \"tanh\", \"relu\", \"swish\" (or \"silu \"),\n# \"linear\" (or None).\n\" fcnet_activation \": \"tanh\",\n# VisionNetwork\n15\n# If None , automatically\ndetermines\nthe CNN filter\nshapes\nbased on\nthe input\nsize\n\"conv_filters\": None ,\n# Activation\nfunction\ndescriptor.\n# Supported\nvalues are: \"tanh\", \"relu\", \"swish\" (or \"silu \"),\n# \"linear\" (or None).\n\" conv_activation \": \"relu\",\nA.7\nRunning time\nA3C\nPPO\nIMPALA\nMap\nRunning time (s)\nRunning time (s)\nRunning time(s)\n101\n10456.62\n29414.05\n4470.43\n102\n10760.02\n28100.36\n4255.84\n103\n16922.06\n31898.47\n6455.39\n104\n1962.49\n9855.69\n7014.64\n008\n6076.44\n28854.93\n4211.52\n014\n6352.47\n28305.63\n4278.53\nTable 10: Evaluation results on the navigation task. We report the total running time in second.\nA3C\nPPO\nIMPALA\nMap\nRunning time (s)\nRunning time (s)\nRunning time (s)\n101\n1925.99\n12250.53\n2312.53\nTable 11: Evaluation results on the cooperative supply gathering. We report the total running time in\nsecond.\nA3C\nPPO\nIMPALA\nMap\nRunning time (s)\nRunning time (s)\nRunning time (s)\n101\n1300.56\n16885.55\n1748.60\nTable 12: Evaluation results on the cooperative target capture. We report the total running time in\nsecond.\nB\nWinning Policy Analysis\nWe held an online competition in the past few months to test the RL algorithm for solving this\nproblem. Based on our previous results in the competition 3, we analyze the winning policy in the\nleader board below. Compared to the rule-based method, such as A star algorithm [37]. We can ﬁnd\nthat the traditional reinforcement learning agent cannot efﬁciently handle this task. Therefore, we\nanalyze the winning strategies from the top submissions.\nB.1\nNavigation\nThe goal of this task is to make the agent reach the target as soon as possible. The challenge is that in\nthis track, the spawn point and destination location basically do not involve indoors, but the agent\nstill has the possibility of entering the room by mistake. Some destinations are set on hillsides, and\nthe agent needs to know how to climb hills.\nAfter testing the map, we found that the extreme value of the boundary coordinates does not exceed\n±300, and the tallest building does not exceed four ﬂoors. Therefore, we established a 600*600*4\n3https:\/\/sites.google.com\/view\/inspirai-wildscav-cog2022\/home\n16\nobstacle marker matrix for sampling and recording terrain information. There are several tricks to\nsolve these tasks:\n• During the movement of the agent, the current position will be stored as a checkpoint every\ntime given a certain number of time steps. If the agent fails to move a valid distance for a\nlong time, it will try to return to the previous saved checkpoint.\n• When we use visual depth image to detect an obstacle, the slope of the obstacle is also\ncalculated. We assume there is no obstacle if the slope is low.\n• When the agent is walking, if there is obstacle but there is no obstacle in the image, we will\ntry to jump.\nB.2\nSupply gathering\nIn the second task, the agent needs to collect as many supplies as possible on the map in a ﬁxed time.\nThere are a large number of indoor scenes on the map, which requires the agent to have the ability to\nexplore complex indoor scenes. The buildings on the map have different ﬂoors, and the spawn point\nmay also be located on the upper ﬂoor. There are several tricks to solve this task:\n• The supply gathering problem can be decomposed into multiple navigation sub-problems. In\neach step, we maintain a list of information, such as supply points, and location information,\nto make the decision.\n• The agent will use a decision tree policy to determine when and where to collect supply.\n• In the high-ﬂoor area, the agent will record the exploration route and return to the original\nroute ﬁrst when it is necessary to go downstairs.\nB.3\nSupply battle\nIn the third task, compared with supply gathering, multiple agents are competing on the same map,\nand it is necessary to consider killing other agents to gain more resources. There are a large number\nof indoor scenes on the map, which requires the agent to have the ability to explore complex indoor\nscenes. The buildings on the map have different ﬂoors, and the spawn point may also be located on\nthe upper ﬂoor. Agents need to have certain attacking capabilities, such as shooting other agents.\n• If the agent detects local information, it will automatically aim at the nearest enemy and ﬁre.\n• We will consider reloading when the current clip is lower than a threshold.\nC\nFurther information about our gaming enginee\nOur environment consists of two major components, the game engine (backend) and gameplay\nAPI (frontend). We have described the gameplay interface and training algorithm conﬁgurations in\nprevious sections. In this section, we provide more information on how we build the infrastructure\nand optimize the simulation throughput.\nCommunication. we use GRPC 4 protocol to design the format and content of observation, action,\nenvironmental control command and event messages. The protocol will convert data from both\nends into messages with a uniform format so that they can be transmitted between processes across\nmultiple languages. To ensure synchronous control of the game simulation, we built a simple message\nqueue server on the frontend side to receive the observation message from the engine, wait for the\nalgorithm level to consume, and then send back the new action to the engine.\nSimulation optimization. We consider the following strategies to optimize the gaming performance:\n• The game scene is too large, and there are too many resources, resulting in excessive\nrendering pressure and causing stuttering. As a result, we reduce the rendering batch of the\ngame from more than 3000 to an average of around 200.\n4https:\/\/grpc.io\/\n17\n• On the premise of meeting the requirements of the algorithm, the communication request of\nGRPC is reduced. Therefore, the running speed of the engine is improved, and the training\nrate is accelerated.\n• We optimize the management of the protocol data to reduce running consumption caused by\ngame memory management.\n18\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments.pdf"}
{"title":"The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning","authors":"Eric Pulick, Shubham Bharti, Yiding Chen, Vladimir Menkov, Yonatan Mintz, Paul Kantor, Vicki M. Bier","summary":"As machine learning (ML) is more tightly woven into society, it is imperative\nthat we better characterize ML's strengths and limitations if we are to employ\nit responsibly. Existing benchmark environments for ML, such as board and video\ngames, offer well-defined benchmarks for progress, but constituent tasks are\noften complex, and it is frequently unclear how task characteristics contribute\nto overall difficulty for the machine learner. Likewise, without a systematic\nassessment of how task characteristics influence difficulty, it is challenging\nto draw meaningful connections between performance in different benchmark\nenvironments. We introduce a novel benchmark environment that offers an\nenormous range of ML challenges and enables precise examination of how task\nelements influence practical difficulty. The tool frames learning tasks as a\n\"board-clearing game,\" which we call the Game of Hidden Rules (GOHR). The\nenvironment comprises an expressive rule language and a captive server\nenvironment that can be installed locally. We propose a set of benchmark\nrule-learning tasks and plan to support a performance leader-board for\nresearchers interested in attempting to learn our rules. GOHR complements\nexisting environments by allowing fine, controlled modifications to tasks,\nenabling experimenters to better understand how each facet of a given learning\ntask contributes to its practical difficulty for an arbitrary ML algorithm.","url":"http:\/\/arxiv.org\/abs\/2207.10218v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2207.10218v1","published":1658355934000,"comment":"9 pages, 5 figures. Additional documentation information available at\n  http:\/\/sapir.psych.wisc.edu:7150\/w2020\/captive.html","pdf_text":"The Game of Hidden Rules: A New Kind of\nBenchmark Challenge for Machine Learning\nEric Pulick1∗, Shubham Bharti1, Yiding Chen1, Vladimir Menkov2\nYonatan Mintz1, Paul Kantor1, Vicki M. Bier1\n1University of Wisconsin - Madison, 2Rutgers University\n{pulick, sbharti, ychen695, ymintz, pkantor, vmbier}@wisc.edu, vmenkov@gmail.com\nAbstract\nAs machine learning (ML) is more tightly woven into society, it is imperative\nthat we better characterize ML’s strengths and limitations if we are to employ it\nresponsibly. Existing benchmark environments for ML, such as board and video\ngames, offer well-deﬁned benchmarks for progress, but constituent tasks are often\ncomplex, and it is frequently unclear how task characteristics contribute to overall\ndifﬁculty for the machine learner. Likewise, without a systematic assessment of\nhow task characteristics inﬂuence difﬁculty, it is challenging to draw meaning-\nful connections between performance in different benchmark environments. We\nintroduce a novel benchmark environment that offers an enormous range of ML\nchallenges and enables precise examination of how task elements inﬂuence practi-\ncal difﬁculty. The tool frames learning tasks as a “board-clearing game,” which we\ncall the Game of Hidden Rules (GOHR). The environment comprises an expressive\nrule language and a captive server environment that can be installed locally. We\npropose a set of benchmark rule-learning tasks and plan to support a performance\nleader-board for researchers interested in attempting to learn our rules. GOHR\ncomplements existing environments by allowing ﬁne, controlled modiﬁcations\nto tasks, enabling experimenters to better understand how each facet of a given\nlearning task contributes to its practical difﬁculty for an arbitrary ML algorithm.\n1\nIntroduction\nLearning computational representations of rules has been one of the main objectives of the ﬁeld of\nmachine learning (ML) since its inception. In contrast to pattern recognition and classiﬁcation (the\nother main domains of ML), rule learning is concerned with identifying a policy or computational\nrepresentation of the hidden process by which data has been generated. These sorts of learning tasks\nhave been common in applications of ML to real world settings such as biological research [10],\nimitation learning [7], and most recently game play [13, 21]. Since this process involves sequential\nexperimentation with the system, much of the recent work exploring rule learning has focused on\nusing reinforcement learning (RL) for learning rules as optimal policies of Markov decision processes.\nRules, however, can be quite complex in many applications, and may not map to a single optimal\npolicy, but instead to a whole set of optimal policies. For instance, in the case of protein folding, there\nmay be multiple correct ways in which a potential set of atoms can be folded into a feasible molecule.\nWhile computational and theoretical results exist to characterize which properties of the models or\nalgorithms may cause these differences, these results are highly task-dependent, making it difﬁcult to\ncompare relative task difﬁculty. Thus, an important question is whether some characteristics make\nparticular rules easier or harder to learn by a speciﬁc algorithm (or in general). To date, this has been\na difﬁcult question to answer, since many rules of interest in the real world are multifaceted and not\nwell characterized. For instance, while there are effective RL algorithms that can play go, chess, and\nPreprint. Under review.\narXiv:2207.10218v1  [cs.LG]  20 Jul 2022\nbackgammon, it is not clear which speciﬁc features of go and chess make them more difﬁcult to learn\nin practice than backgammon (e.g., is it the number of pieces? size of the board? lack of random\nelements?). In order to interrogate these questions, new ways of generating rules and data must be\ndevised that allow for researchers to examine these characteristics in a controlled environment.\nIn this paper, we propose a new data environment called the Game Of Hidden Rules (GOHR), which\naims to help researchers in this endeavor. The main component of the environment is a game played\nin a 6 × 6 board with game pieces of different shapes and colors. The task of the learner is to\nclear the board in each round by moving the game pieces to “buckets” at the corners of the board\naccording to a hidden rule, known to the researcher but not to the learner. Our environment allows\nresearchers to express a hidden rule using a rich syntax that can map to many current tasks of interest\nin both the classiﬁcation and RL settings. A key advantage of our environment is that researchers can\ncontrol each aspect of the hidden rules and test them at a granular level, allowing for experiments\nthat determine exactly which characteristics make some learning tasks harder than others and which\nalgorithms are better at learning speciﬁc types of rules.\nThe rest of the paper proceeds as follows. We ﬁrst describe how our environment relates to other\ndata-generating environments from the literature in Section 2. Then, in Section 3, we go into the\ndetails of GOHR and the rule syntax, explaining how the environment can be used to interrogate the\neffects of rule structure. We introduce our ML competition and refer readers to benchmark rules,\ninstructions, and documentation available at our public site in Section 4. In Section 5, we present\nsample rules and analysis for an example algorithm. Finally, we conclude with some discussion on\nthe implications of our results and on other questions that can be answered by the GOHR environment\nin Section 6.\n2\nLiterature Review\nLearning rules remains a core challenge in many aspects of ML, particularly in RL settings where the\nagent must learn iteratively from its environment. Games have historically served as rich benchmark\nenvironments for RL, with novel challenges in each game environment spurring progress for the ﬁeld\nas a whole. RL has tackled increasingly complex classical board games, such as backgammon, chess,\nshogi, and go [23, 5, 19–21], eventually surpassing the performance of human experts in each. Of late,\nvideo-game environments have also become drivers of progress in RL. Beginning with human-level\nperformance in Atari 2600 games [13, 1], machine players have become competitive with humans\nin a variety of environments, including real-time-strategy and multiplayer games such as Quake III,\nStarCraft II, and Dota 2 [8, 16, 24, 25].\nInstrumental in such RL progress has been the growing space of benchmark game environments\nand supporting tools, such as the Arcade Learning Environment [2], General Video Game AI [17],\nOpenAI Gym [4], Gym Retro [15], Obstacle Tower [9], Procgen [6], and NetHack environments\n[11, 18]. Taken together, these represent an impressive range of new tasks for RL agents, bridging\nmany gaps and challenges in achieving aspects of artiﬁcial intelligence.\nWe note a clear trend in these benchmark tools away from static games and toward conﬁgurability (e.g.\nprocedurally-generated and customizable environments), something we believe is critical to better\nunderstanding task difﬁculty. Many environments demand that agents solve complex tasks inspired\nby those a human might encounter in the real world. Such tasks offer well-deﬁned benchmarks, but\noften do not support precise, controlled modiﬁcations without affecting other aspects of the learner’s\nenvironment. We believe that GOHR, which allows the experimenter to change the learning task\nwithout any change to the game’s rules, playing environment, or board-clearing objective, can serve\nas an important complement to existing benchmarks. GOHR’s mechanics allow for isolated study of\nchanges in learning tasks and their effects on practical difﬁculty, an important prerequisite for the\nsystematic study of ML algorithms and their suitability for different tasks.\n3\nGame of Hidden Rules\nIn this section we describe the GOHR’s structure, syntax, and the expressivity of the rule language.\nIn each episode of our game, the player is presented with a game board containing a conﬁgurable set\nof game pieces, each drawn from a standard set of shapes and colors. The player’s objective is to\nremove game pieces from play by dragging and dropping them into buckets located at each corner of\n2\nthe game board. A “hidden rule,” unknown to the player, determines which pieces may be placed into\nwhich buckets at a given point in the game play. For instance, a rule might assign game pieces to\nspeciﬁc buckets based on their shape or color. If the player makes a move permitted by the rule, the\ncorresponding game piece is removed from play; otherwise, it remains in its original location. The\nepisode concludes once the player has fully satisﬁed the rule. Typically, this occurs when all game\npieces have been cleared from play, but some rules may be fully satisﬁed even with some pieces\nremaining on the board. The scoring for GOHR rewards players for completing episodes in as few\nmoves as possible, incentivizing players to quickly discern the hidden rule.\nThe GOHR is played on a board with 36 identical cells, arranged in a 6 × 6 grid. Each cell on the\nboard is indexed with a label, 1-36, and has x and y coordinates 1-6. Each bucket is indexed with a\nlabel, 0-3, with x, y coordinates in the set {(0, 0), (0, 7), (7, 0), (7, 7)}. The game board, including\nall numeric labels, can be seen in Figure 1.\nIn each GOHR experiment, the game engine generates game pieces from user-deﬁned sets of shapes\nand colors. Depending on the experimental goals, these sets can be of arbitrary size. If no speciﬁcation\nis provided in the experimental setup, the game engine defaults to a set of 4 shapes (circles, triangles,\nsquares, stars) and 4 colors (red, blue, black, yellow). The experiment designer may add shapes or\ncolors to the experiment in the associated color and shape conﬁguration ﬁles. A sample board, built\nusing a set of 4 shapes and 4 colors, is shown in Figure 2.\nFigure 1: Game board diagram, with numeric\nposition labels included.\nFigure 2: Game board, conﬁgured with 4 col-\nors and 4 shapes, as it would be seen by a\nhuman player.\nBoards in the GOHR can be deﬁned in advance of play (to permit precise experimenter control), or\ncan be generated randomly by the game engine according to a set of input parameters. This ﬂexibility\nallows the experimenter to evaluate the impact of deterministic or randomized “curricula” (e.g., to\ndetermine whether seeing a particular set of boards in a speciﬁed order is conducive to learning).\nWhen generating boards randomly, the experimenter must specify the minimum and maximum\nnumbers of game pieces, colors, and shapes to appear on each new board; the game engine randomly\nselects values in the range [minimum, maximum] for each quantity and generates boards accordingly.\nAdditional documentation can be found at our public site, as noted in Section 4.\n3.1\nRule deﬁnition\nA rule playable within GOHR must be deﬁned in a standalone ﬁle. Each rule is constructed from one\nor more rule lines, each of which is built from one or more atoms. For instance, a two-line rule with\nﬁve atoms might look like:\n(atom 1) (atom 2) (atom 3)\n(atom 4) (atom 5)\nOnly one rule line is active at a time; this active line determines the current rule state (how game\npieces may be placed into buckets for the player’s next move). In the example above, the rule state\nis formed by the contents of either atoms 1, 2, and 3 or atoms 4 and 5, depending on which line is\nactive. Later sections discuss the mechanisms by which the active rule line can change during play.\n3\nEach atom maps a set of game pieces to a set of permitted buckets and is deﬁned as follows:\n(count, shapes, colors, positions, buckets)\nThe “count” parameter deﬁnes the number of successful moves that the atom permits (also referred to\nas its “metering”). Any game pieces matching the “shapes,” “colors,” and “positions” speciﬁed in\nthe atom are accepted in its corresponding set of “buckets.” Multiple values can be listed for each\nnon-count ﬁeld, and are grouped in square brackets. For example, the rule that “stars and triangles go\nin bucket 0, squares go in bucket 1, and circles can go in bucket 2 and bucket 3” would be constructed\nwith the following three atoms on a single rule line:\n(*, [star, triangle], *, *, 0) (*, square, *, *, 1) (*, circle, *, *, [2,3])\nThe ∗character denotes a wildcard. In the count ﬁeld, ∗denotes that the atom is not metered; in the\nshape, color, or position ﬁelds, it denotes that every value for that feature is included in the allowable\nset for that atom. In the above example, the atoms apply to any star, triangle, square, or circle on\nthe board, regardless of their color or position. Since the atoms are not metered, any shape may be\nplaced in its corresponding bucket at any point in the game.\n3.2\nRule expressivity\nThe above syntax permits an enormous range of rules within the GOHR. Stationary rules, such as\nthe previous example, are those where the permitted bucket set for each game piece never changes\nduring play. In that example, any star or triangle is allowed only in bucket 0, any square allowed only\nin bucket 1, etc., for as long as the game is played. Thus, stationary rules permit the experimenter\nto study the practical difﬁculty associated with learning feature-based patterns–i.e., those related\nto game-piece color, shape, position, or a combination thereof. Though stationary rules have no\ndependence on play history, these rules can still be made arbitrarily difﬁcult, such as designating a\nspeciﬁc bucket for every possible combination of a game piece’s features. Importantly, since the set\nof colors and shapes used is left to the experimenter, such rules can become arbitrarily complex.\nNon-stationary rules are those in which the permitted bucket set for game pieces changes during\nplay. Non-stationary rules can involve following a deﬁned sequence, such as as clearing a triangle, a\nsquare, a circle, and then a star in that order (repeating the sequence as needed). It is also possible to\ncreate non-stationary rules with an implicit priority between objects, such as clearing all available red\npieces before all available blue pieces, or clearing pieces on the board from top to bottom. Methods\nfor implementing non-stationary rules are described below.\nAs mentioned, each atom contains a “count” ﬁeld dictating the number of corresponding correct\nmoves that atom permits. For example, the following rule requires the player to alternate between\nplacing game pieces in the top (0,1) and bottom (2,3) buckets:\n(1, *, *, *, [0,1])\n(1, *, *, *, [2,3])\nThe ﬁrst rule line is always active at the beginning of a new episode, in this case allowing any game\npiece on the board to be placed into either bucket 0 or bucket 1. Once a player makes a move\nsatisfying one (or more) of the atoms on the active rule line, the counts associated with the satisﬁed\natoms are decremented by one. An atom with a count of 0 is considered exhausted and no longer\naccepts moves. At each move, the game engine evaluates the active rule line and the current board; if\nthere are no valid moves available to the player, the engine makes the next rule line active and resets\nall counts in the new line. Thus, when all atoms on the active rule line are exhausted, there are no\nvalid moves available and the engine moves control to the next rule line. If there are no lines below\nthe current rule line, the ﬁrst rule line is made active again. In this example, when the player puts a\ngame piece in buckets 0 or 1, the ﬁrst atom becomes exhausted and the second line becomes active.\nAfter placing a piece into either bucket 2 or 3, the second atom is exhausted and the ﬁrst line is reset\nand becomes active again, repeating this cycle until the board is cleared.\nGOHR also permits the experimenter to attach a count to each rule line. When the line is active,\nthis count decrements each time any atom on the line is satisﬁed. For instance, a rule that alternates\nbetween uniquely assigning shapes and colors to buckets would look like:\n4\n1 (*, star, *, *, 0) (*, square, *, *, 1) (*, circle, *, *, 2) (*, triangle, *, *, 3)\n1 (*, *, red, *, 0) (*, *, blue, *, 1) (*, *, black, *, 2) (*, *, yellow, *, 3)\nIf the rule-line count is exhausted, the game engine makes the next line active, even if there are\nnon-exhausted atoms on that line. For this example, the active rule line will alternate after each\nsuccessful move, regardless of which atom in the active line is satisﬁed. If no count is provided for a\ngiven rule line, the game engine assumes that the rule line is not metered and can be used until all\natoms on that line are exhausted or no valid move exists for the game pieces currently on the board.\nThe GOHR rule syntax allows the experimenter to write expressions in an atom’s bucket ﬁeld that\ndeﬁne sequences based on previous successful moves. The game engine stores values for the bucket\nthat most recently accepted any game piece (p) as well as the bucket that most recently accepted an\nitem of each color (pc) and shape (ps). A simple rule expressible using these values is “objects must\nbe placed in buckets in a clockwise pattern, beginning with any bucket”:\n(1, *, *, *, [0,1,2,3])\n(*, *, *, *, p+1)\nThe expressions used in the buckets ﬁeld are evaluated modulo 4 to ensure that the resultant expression\ngives values in the range 0-3. The game engine also supports the terms “nearby” and “remotest” as\nbucket deﬁnitions, which allow the experimenter to require that game pieces be put into either the\nclosest or furthest bucket, respectively, evaluated by Euclidean distance.\nThe arrangement of atoms allows the experimenter to encode priority between component tasks\nwithin a rule. For instance, the rule that “all red pieces go into bucket 1, then all blue pieces go into\nbucket 2” would be expressed as follows:\n(*, *, red, *, 1)\n(*, *, blue, *, 2)\nSince both the ﬁrst rule line and associated atom are not metered, they can never become exhausted,\neven if there are no more red game pieces left on the board. However, as noted above, the engine\nevaluates if there are any valid moves available to the player given the current rule line; if there are\nno valid moves available, the engine shifts control to the next line. In this example, once the player\nhas cleared all red game pieces from the board, the engine will make the second rule line active.\n4\nDocumentation and Competition\nBeyond using GOHR for the study of task characteristics and their impact on difﬁculty, we hope\nthat interested researchers will share performance results on benchmark rules with us so that we\ncan disseminate community-wide benchmark performance. To this effect, we plan to host a public\nleader-board through June 2023 which will present results for benchmark rules, updated on a weekly\nbasis. The public site, http:\/\/sapir.psych.wisc.edu:7150\/w2020\/captive.html, houses\ndocumentation for downloading and setting up the captive game server (CGS), conﬁguring and\nrunning experiments, a list of benchmark rules beyond what is presented in Section 5, as well as the\npublic leader-board.\n5\nSample Analysis\nIn this section we propose a series of sample rules, a sample ML algorithm (MLA), performance\nmetrics, and useful data presentation methods.\n5.1\nSample rules\nWe present results for the following sample rules, each played using the default set of 4 shapes\nand colors. Boards were randomly generated with 9 game pieces per board and the minimum and\nmaximum parameters for color and shape set to 4.\n1. Color Match: Each of the 4 shapes is uniquely assigned to a bucket\n(*, star, *, *, 0) (*, triangle, *, *, 1) (*, square, *, *, 2) (*, circle, *, *, 3)\n5\n2. Clockwise: The ﬁrst game piece can go in any bucket, but each subsequent piece must go\nin the next clockwise bucket\n(1, *, *, *, [0,1,2,3])\n(*, *, *, *, (p + 1))\n3. B23 then B01: Game pieces must be alternatively placed in the bottom and top buckets\n(1, *, *, *, [2,3])\n(1, *, *, *, [0,1])\n4. B3 then B1: Game pieces must be alternatively placed in buckets 3 and 1\n(1, *, *, *, 3)\n(1, *, *, *, 1)\n5.2\nSample MLA\nWe model the GOHR as an episodic Markov decision process (MDP) characterized by the tuple,\nM = (S, A, R, P, µ, H). A is the action space, where each action puts the object in row r and\ncolumn c in bucket b: A = {(r, c, b) : r ∈[6], c ∈[6], b ∈{(0, 0), (0, 7), (7, 0), (7, 7)}}. Before\nintroducing the state space, we ﬁrst formally describe the board. In a board B, the cell at row r, column\nc is characterized by color and shape: B[r, c].color ∈{red, black, blue, green, ∅}, B[r, c].shape ∈\n{star, square, triangle, circle, ∅}, where we deﬁne the shape and color of an empty cell to be ∅.\nS is the state space, where each state St = (B0, A0, B1, A1, · · · , Bt−1, At−1, Bt) is a sequence\nincluding all the historical boards Bi’s and actions Ai’s up to time step t. We deﬁne B−1 to be\nan empty board before the board generation process. P is the deterministic transition probability\nmatrix speciﬁed by the hidden rule. Importantly, when action At is not accepted, resulting in no\nchange to the board, we still insert the board Bt+1 and action At to St+1. In that case, Bt+1 = Bt.\nR is the deterministic reward function. If an action is accepted or the board is already cleared, the\nreward is set to be 0; otherwise, the reward is −1. Speciﬁcally: R(St, At) = 0 if Bt+1 ̸= Bt\nor Bt = B−1; R(St, At) = −1, o.w. µ is the initial state distribution. Since the initial state\nS0 = (B0), which only includes the initial board, µ is determined by the board generating process\nin Section 3. H is the time horizon that characterizes the number time steps in each episode. The\nMDP evolves according to Section 3. We deﬁne the value function as the sum of future rewards by\ntaking action according to policy π, i.e. V π(St) = E\nhPH−1\nk=t R(Sk, Ak)\ni\n, where the expectation\nis over the potential randomness of the policy. Similarly, we deﬁne the state-action value function\nby the sum of future rewards by taking action At at state St and following policy π after that, i.e.\nQπ(St, At) = R(St, At) + V π(St+1). Because the step information has already been encoded in\nthe state, we do not need to use step h as the subscript for the value function and state-action value\nfunction. The goal of the learner is to ﬁnd a policy π to maximize the expected cumulative reward\nES0∼µ [V π(S0)].\nEven though the MDP M has a ﬁnite state space S, the size of S scales exponentially in H, which\nmakes it impractical to learn a proper policy directly. Instead, we handcraft a feature mapping φ(·, ·)\nto extract only the essential information for some particular hidden rules. The feature function maps\neach state-action pair (St, A) to a 3720-dimensional Boolean vector φ(St, A). Recall that each action\nA chooses a cell (r, c) and puts the object in this cell in bucket b. φ(St, A) encodes the following\ninformation to Boolean variables: the shape and color information of the cell chosen by A, the target\nbucket, and the shape, color, and bucket information of the last accepted action. Products of some of\nthe Boolean variables are also included in φ(St, A). Interested readers are referred to Appendix A.1.1\nfor additional details on the feature representation.\nWe use a variant of DQN [12], a model-free algorithm, as our learning algorithm. DQN models the\nstate-action value function Q as a linear function in the feature vector φ: Q(s, a; θ) = θ⊤φ(s, a) and\ntries to estimate the optimal Q∗using stochastic gradient updates on moving loss L:\nL(θ) = E\n\u0002\n(y −Q(s, a; θ))2\u0003\n,\nwhere y = r + maxa′ Q(s′, a′; θtarget) is the target Q estimate obtained for the sample (s, a, r, s′).\nWe distinguish the running parameter iterate θ and the target parameter θtarget. Unlike the DQN in\n[12], θtarget is updated less frequently to stabilize the algorithm.\nAdditionally, the algorithm maintains a buffer memory of size 1000 for past experiences and samples\na batch of 128 tuples to calculate gradients at each time step. We run the DQN with an ϵ-greedy\n6\nbehavior policy. The ϵ starts at 0.9 and terminates at 0.001. ϵ decays exponentially with step-size 200.\nMore precisely, ϵ = 0.001 + (0.9 −0.001) exp(−number of moves\n200\n). For each rule we run 100 trials,\nwith each trial including 200 episodes of length H = 100. The full learning algorithm is described in\nAppendix A.1.2 and relevant hardware information is described in Appendix A.1.3.\n5.3\nMetrics and sample results\nFor any given rule and MLA, difﬁculty can be evaluated by examining the cumulative error count\nmade by the MLA during each independent learning run. Given a sufﬁcient number of episodes (200\nin this analysis), we found that the cumulated error eventually ﬂattened for all learning runs of our\nsample DQN; we refer to the cumulated error after 200 episodes as the Terminal Cumulated Error\n(TCE) for that learning run. For other rules or algorithms, longer learning runs might be necessary.\nAs an example, Figure 3 shows the cumulated error, tallied by episode, of our sample DQN for 100\nseparate learning runs of the Color Match rule. Median learning curves, also plotted by episode,\nacross all four sample rules can be found in Figure 4. We found median cumulated error plots to be a\nuseful tool for visualizing differences in difﬁculty among a set of rules, with higher curves indicating\ngreater difﬁculty. Box-and-whisker plots for the distributions of TCE for each sample rule are shown\nin Figure 5, with higher distributions again indicating greater difﬁculty.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpisode\n101\n102\n103\n104\nCumulated Errors\n0.000\n0.005\nDensity\n101\n102\n103\n104\nFigure 3: Cumulated error curves for 100 MLA learning runs of the Color Match rule. The y-axis is\nlogarithmic to capture the highly skewed nature of the distribution. The marginal plot to the right\ndisplays a kernel density estimate for the Terminal Cumulated Error for this rule and MLA.\nAlthough the heavily-skewed TCE distribution seen for Color Match was not seen for all sample\nrules, it highlights the need for non-parametric tests when comparing rules. To perform pairwise\ncomparison of rule difﬁculty, for instance between arbitrary rules A and B, we recommend using\nthe Mann-Whitney-Wilcoxon U-Test [14] with TCE as the point-metric. The test compares every\nTCE value associated with rule A against every TCE value associated with rule B; the associated\ntest statistic sums the number of times that values from A exceed values from B, with ties counting\nfor 0.5. We found that one-sided U-Tests ranked rules identically (p < 0.002) to the difﬁculty order\ngiven by the height of the median cumulated error curves in Figure 4. The sample rules were chosen\nto illustrate a range of rules possible within the rule game, not with a particular variation in task\ncharacteristics in mind; we expect interested researchers to design targeted sets of rules investigating\ncertain characteristics of learning tasks and to perform similar comparisons to assess their practical\ndifﬁculty.\nIn addition to comparing the difﬁculty of two different rules for a given algorithm, the U-Test also\nsupports comparison between different MLAs. The U-test test statistics can be used to form an “ease\nratio,” representing the ratio of learning runs that, for example, rule A was found easier than rule B\nby a given MLA. We propose aggregating ease ratios for many MLAs across the set of benchmark\nrules to create a tournament graph as described in [22]. The ranking method corrects for the fact\nthat algorithms R and S may have solved a different subset of rules, leading to the possibility that R\nencountered a larger fraction of “easy” rules (where rule difﬁculty can be assessed by the TCE of all\n7\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpisode\n0\n50\n100\n150\n200\n250\n300\n350\nCumulated Errors\nColor Match\nClockwise\nB3 then B1\nB23 then B01\nFigure 4: Median cumulated errors at each episode across 100 MLA learning runs for each sample\nrule. The shaded regions denote the 95% conﬁdence intervals for the medians, calculated using\n50,000 bootstraps.\n1000\n2000\n3000\nColor Match\nClockwise\nB3 then B1\nB23 then B01\nRule\n0\n100\n200\n300\n400\n500\nCumulative Errors\nFigure 5: Box-and-whisker plots of TCE for each learning run for all sample rules. The chart has\na broken y-axis with a scale change to capture the most extreme outliers. The colored area is the\ninterquartile range (IQR), with whiskers displaying the most extreme data point within 1.5 IQRs of\nthe upper and lower quartiles.\nalgorithms that attempt a particular rule). Additional information regarding the methods to be used in\nranking competition submissions can be found at the public site.\n6\nDiscussion\nThe GOHR tool provides a capability for studying the performance of MLAs in a novel and principled\nway. Using the expressive rule syntax, researchers can make precise changes to rules of interest to\nstudy how they affect algorithm performance. As suggested in Section 5, this approach can be used\nto compare the performance of different algorithms on any speciﬁc set of rules and to see the effect\nof rule characteristics (position, properties, static\/dynamic) on MLA performance. This is a useful\ncomplement to present competitions among ML algorithms on existing board games and video games\nthat can only look at complex tasks without the same granularity. We note here that the GOHR can\n8\nalso be played by human rule learners, enabling comparison of human learning and ML on an equal\nfooting. For additional discussion on this please refer to [3].\nEach choice of ML parametrization, learning model, and optimization is expected to produce different\nresults. With the GOHR, we believe it will be possible to interrogate how these choices interact with\nspeciﬁc rule characteristics to result in differences in practical performance. With this goal in mind,\nwe are sharing the complete suite of tools with all interested researchers. We hope that researchers\nusing the GOHR tool for ML research will share their ﬁndings to help this inquiry.\nGiven our current analysis and discussion, we believe there are at least two interesting broad research\nquestions that can be answered with the GOHR environment. First, can a given ML algorithm learn\nmany substantially different rules, thus approaching a general learning. Second, when results for\nmultiple algorithms are available, is it possible to cluster rules into difﬁculty classes based on practical\nalgorithm performance.\nAcknowledgments and Disclosure of Funding\nSupport for this research was provided by the University of Wisconsin-Madison Ofﬁce of the Vice\nChancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research\nFoundation, and by the National Science Foundation under Grant No. 2041428. Any opinions,\nﬁndings, and conclusions or recommendations expressed in this material are those of the author(s)\nand do not necessarily reﬂect the views of the sponsors. We also thank Mr. Kevin Mui for building\nthe ﬁrst instance of the GOHR, to support research on human learning.\nReferences\n[1] Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,\nZhaohan Daniel Guo, and Charles Blundell.\nAgent57: Outperforming the Atari Human\nBenchmark.\nIn Proceedings of the 37th International Conference on Machine Learning,\npages 507–517. PMLR, November 2020. URL https:\/\/proceedings.mlr.press\/v119\/\nbadia20a.html. ISSN: 2640-3498.\n[2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment:\nAn Evaluation Platform for General Agents. Journal of Artiﬁcial Intelligence Research, 47:\n253–279, June 2013. ISSN 1076-9757. doi: 10.1613\/jair.3912. URL https:\/\/www.jair.\norg\/index.php\/jair\/article\/view\/10819.\n[3] Vicki Bier, Paul B. Kantor, Gary Lupyan, and Xiaojin Zhu. Can We Distinguish Machine\nLearning from Human Learning? Technical report, 2019. URL http:\/\/arxiv.org\/abs\/\n1910.03466. _eprint: 1910.03466.\n[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. OpenAI Gym, June 2016. URL http:\/\/arxiv.org\/abs\/1606.\n01540. Number: arXiv:1606.01540 arXiv:1606.01540 [cs].\n[5] Murray Campbell, A.Joseph Hoane, and Feng-hsiung Hsu. Deep Blue. Artiﬁcial Intelligence,\n134(1-2):57–83, January 2002. ISSN 00043702. doi: 10.1016\/S0004-3702(01)00129-1. URL\nhttps:\/\/linkinghub.elsevier.com\/retrieve\/pii\/S0004370201001291.\n[6] Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Generation\nto Benchmark Reinforcement Learning. In Proceedings of the 37th International Conference on\nMachine Learning, pages 2048–2056. PMLR, November 2020. URL https:\/\/proceedings.\nmlr.press\/v119\/cobbe20a.html. ISSN: 2640-3498.\n[7] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation Learning:\nA Survey of Learning Methods. ACM Computing Surveys, 50(2):1–35, March 2018. ISSN\n0360-0300, 1557-7341. doi: 10.1145\/3054912. URL https:\/\/dl.acm.org\/doi\/10.1145\/\n3054912.\n[8] Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia\nCastañeda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas\n9\nSonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray\nKavukcuoglu, and Thore Graepel. Human-level performance in 3D multiplayer games with\npopulation-based reinforcement learning. Science, 364(6443):859–865, May 2019. ISSN\n0036-8075, 1095-9203. doi: 10.1126\/science.aau6249. URL https:\/\/www.science.org\/\ndoi\/10.1126\/science.aau6249.\n[9] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter\nHenry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle Tower: A Generalization\nChallenge in Vision, Control, and Planning. Technical Report arXiv:1902.01378, arXiv, July\n2019. URL http:\/\/arxiv.org\/abs\/1902.01378. arXiv:1902.01378 [cs] type: article.\n[10] Firas Khatib, Seth Cooper, Michael D. Tyka, Kefan Xu, Ilya Makedon, Zoran Popovi´c, David\nBaker, and Foldit Players. Algorithm discovery by protein folding game players. Proceedings\nof the National Academy of Sciences, 108(47):18949–18953, November 2011. ISSN 0027-8424,\n1091-6490. doi: 10.1073\/pnas.1115898108. URL https:\/\/pnas.org\/doi\/full\/10.1073\/\npnas.1115898108.\n[11] Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici,\nEdward Grefenstette, and Tim Rocktäschel.\nThe NetHack Learning Environment.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 7671–7684. Curran As-\nsociates, Inc., 2020.\nURL https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/\n569ff987c643b4bedf504efda8f786c2-Paper.pdf.\n[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013. URL\nhttps:\/\/arxiv.org\/abs\/1312.5602.\n[13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan\nWierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529–533, February 2015. ISSN 0028-0836, 1476-4687. doi:\n10.1038\/nature14236. URL http:\/\/www.nature.com\/articles\/nature14236.\n[14] Nadim Nachar.\nThe Mann-Whitney U: A Test for Assessing Whether Two Independent\nSamples Come from the Same Distribution. Tutorials in Quantitative Methods for Psychology,\n4(1):13–20, March 2008. ISSN 1913-4126. doi: 10.20982\/tqmp.04.1.p013. URL http:\n\/\/www.tqmp.org\/RegularArticles\/vol04-1\/p013.\n[15] Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. Gotta Learn\nFast: A New Benchmark for Generalization in RL. Technical Report arXiv:1804.03720, arXiv,\nApril 2018. URL http:\/\/arxiv.org\/abs\/1804.03720. arXiv:1804.03720 [cs, stat] type:\narticle.\n[16] OpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław\nD˛ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal\nJózefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d O.\nPinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor,\nIlya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with Large Scale Deep\nReinforcement Learning. Technical Report arXiv:1912.06680, arXiv, December 2019. URL\nhttp:\/\/arxiv.org\/abs\/1912.06680. arXiv:1912.06680 [cs, stat] type: article.\n[17] Diego Perez-Liebana, Spyridon Samothrakis, Julian Togelius, Tom Schaul, Simon M. Lucas,\nAdrien Couetoux, Jerry Lee, Chong-U Lim, and Tommy Thompson. The 2014 General Video\nGame Playing Competition. IEEE Transactions on Computational Intelligence and AI in Games,\n8(3):229–243, September 2016. ISSN 1943-068X, 1943-0698. doi: 10.1109\/TCIAIG.2015.\n2402393. URL http:\/\/ieeexplore.ieee.org\/document\/7038214\/.\n[18] Mikayel Samvelyan,\nRobert Kirk,\nVitaly Kurin,\nJack Parker-Holder,\nMinqi Jiang,\nEric Hambro, Fabio Petroni, Heinrich Kuttler, Edward Grefenstette, and Tim Rock-\ntäschel.\nMiniHack the Planet:\nA Sandbox for Open-Ended Reinforcement Learning\n10\nResearch.\nIn J. Vanschoren and S. Yeung, editors, Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.\nURL https:\/\/datasets-benchmarks-proceedings.neurips.cc\/paper\/2021\/file\/\nfa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper-round1.pdf.\n[19] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den\nDriessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,\nSander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timo-\nthy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis.\nMastering the game of Go with deep neural networks and tree search. Nature, 529(7587):\n484–489, January 2016. ISSN 0028-0836, 1476-4687. doi: 10.1038\/nature16961. URL\nhttp:\/\/www.nature.com\/articles\/nature16961.\n[20] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy\nLillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis\nHassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354–\n359, October 2017. ISSN 0028-0836, 1476-4687. doi: 10.1038\/nature24270. URL http:\n\/\/www.nature.com\/articles\/nature24270.\n[21] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters\nchess, shogi, and Go through self-play. Science, 362(6419):1140–1144, December 2018. ISSN\n0036-8075, 1095-9203. doi: 10.1126\/science.aar6404. URL https:\/\/www.science.org\/\ndoi\/10.1126\/science.aar6404.\n[22] Alexander Strang, Karen C. Abbott, and Peter J. Thomas. The Network HHD: Quantifying\nCyclic Competition in Trait-Performance Models of Tournaments. SIAM Review, 64(2):360–\n391, May 2022. ISSN 0036-1445, 1095-7200. doi: 10.1137\/20M1321012. URL https:\n\/\/epubs.siam.org\/doi\/10.1137\/20M1321012.\n[23] Gerald Tesauro. TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-\nLevel Play. Neural Computation, 6(2):215–219, March 1994. ISSN 0899-7667, 1530-888X.\ndoi: 10.1162\/neco.1994.6.2.215. URL https:\/\/direct.mit.edu\/neco\/article\/6\/2\/\n215-219\/5771.\n[24] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John\nQuan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt,\nDavid Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David\nLawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. StarCraft II: A New Challenge\nfor Reinforcement Learning. Technical Report arXiv:1708.04782, arXiv, August 2017. URL\nhttp:\/\/arxiv.org\/abs\/1708.04782. arXiv:1708.04782 [cs] type: article.\n[25] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik,\nJunyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh,\nDan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P.\nAgapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dal-\nibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang,\nTobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney,\nOliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps,\nand David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning.\nNature, 575(7782):350–354, November 2019. ISSN 0028-0836, 1476-4687. doi: 10.1038\/\ns41586-019-1724-z. URL http:\/\/www.nature.com\/articles\/s41586-019-1724-z.\n11\nA\nAppendix\nA.1\nDetails of the reinforcement learning algorithm\nA.1.1\nFeature representation\nThe feature vector φ(S, A) is constructed with the following four different types of components:\n1. The 12 current-step unary features φu(S, A), where u can be a color, a shape, or a bucket. If u is\na color:\nφu(St, A) = 1{Bt[A.r, A.c].color = u}.\nIf u is a shape:\nφu(St, A) = 1{Bt[A.r, A.c].shape = u}.\nIf u is a bucket:\nφu(St, A) = 1{A.b = u}.\n2. The 48 current-step binary features, a set of binary features that depend on the current board\nconﬁguration φu,v : S × A →{0, 1} where (u, v) can be a (color, shape), (color, bucket) or\n(shape, bucket) tuple. These features φu,v(St, A) are deﬁned as follows:\nu, v ∈\nφu,v(St, A)\ncolor × shape\n1{Bt[A.r, A.c].color = u} ∧1{Bt[A.r, A.c].shape = v}\ncolor × bucket\n1{Bt[A.r, A.c].color = u} ∧1{A.b = v}\nshape × bucket\n1{Bt[A.r, A.c].shape = u} ∧1{A.b = v}\n3. The 60 last-successful-step and current-step binary features, a set of binary features constructed\nfrom the board and action information at the last successful step and the current step. For a time\nstep t > 0, let\nℓ(t) = max{τ ∈{−1, 0, · · · , t −1} s.t. Bt ̸= Bτ}\nbe the latest successful time step. Since B−1 is an empty board, B−1 ̸= Bt, ∀t ≥0 until the\ngame is over, so ℓ(t) is well deﬁned.\nThe features are deﬁned as follows: φ(u),v : S × A →{0, 1}, where (u) is extracted from the\nboard and action information at the last successful time step, and v is extracted from the board and\naction information at the current time step. Note that u can be the empty set ∅when ℓ(t) = −1.\n(u), v ∈\nφ(u),v(St, A)\n(color∅) × color\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = u} ∧1{Bt[A.r, A.c].color = v}\n(shape∅) × shape\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u} ∧1{Bt[A.r, A.c].shape = v}\n(bucket∅) × bucket\n1{Aℓ(t).b = u} ∧1{A.b = v}\nwhere x∅= x ∪{∅}, ∀x ∈{color, shape, bucket}\n4. The 3600 last-successful-step and current-step 4-ary features, a set of 4-ary features constructed\nfrom the board and action information at the last successful step and the current step.\nφ(u,v),y,z : S × A →{0, 1}\nwhere (u, v) are extracted from the board and action information at the last successful time step,\nand y, z are extracted from the current time step. These features, φ(u,v),y,z(St, A), are deﬁned as\nfollows:\n12\n(u, v), y, z ∈\nφ(u,v),y,z(St, A)\n(shape∅, color∅)\n×shape, color\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u}\n∧1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = v}\n∧1{Bt[A.r, A.c].shape = y} ∧1{Bt[A.r, A.c].color = z}\n(shape∅, color∅)\n×shape, bucket\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u}\n∧1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = v}\n∧1{Bt[A.r, A.c].shape = y} ∧1{A.b = z}\n(shape∅, color∅)\n×color, bucket\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u}\n∧1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = v}\n∧1{Bt[A.r, A.c].color = v} ∧1{A.b = z}\n(shape∅, bucket∅)\n×shape, color\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u} ∧1{Aℓ(t).b = v}\n∧1{Bt[A.r, A.c].shape = y} ∧1{Bt[A.r, A.c].color = z}\n(shape∅, bucket∅)\n×shape, bucket\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u} ∧1{Aℓ(t).b = v}\n∧1{Bt[A.r, A.c].shape = y} ∧1{A.b = z}\n(shape∅, bucket∅)\n×color, bucket\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].shape = u} ∧1{Aℓ(t).b = v}\n∧1{Bt[A.r, A.c].color = y} ∧1{A.b = z}\n(color∅, bucket∅)\n×shape, color\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = u} ∧1{Aℓ(t).b = v}∧\n1{Bt[A.r, A.c].shape = y} ∧1{Bt[A.r, A.c].color = z}\n(color∅, bucket∅)\n×shape, bucket\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = u} ∧1{Aℓ(t).b = v}\n∧1{Bt[A.r, A.c].shape = y} ∧1{A.b = z}\n(color∅, bucket∅)\n×color, bucket\n1{Bℓ(t)[Aℓ(t).r, Aℓ(t).c].color = u} ∧1{Aℓ(t).b = v}\n∧1{Bt[A.r, A.c].color = y} ∧1{A.b = z}\nFor any (S, A)-tuple, the ﬁnal feature vector φ(S, A) is created by stacking up all of the above\nfeatures, which gives a feature function φ : S × A →{0, 1}3720.\nA.1.2\nLearning algorithm\nAlgorithm 1 Deep Q-learning(DQN) with Experience Replay\nInput : target iteration T, episode count M, buffer size N, batch size B.\nInitialize experience replay buffer memory D with capacity N.\nInitialize the iterate parameters θtarget\n1\n←0, θ1,1 ←0.\nfor target iteration τ = 1, · · · , T do\nfor episode e = 1, · · · , M do\nDraw s1 ∼µ.\nfor time step t = 1, · · · , H do\nDraw at ∼ϵ · Unif(A) + (1 −ϵ) · 1A{a = maxa′ Q(st, a′; θe,t)}.\nTake action at and observe reward rt and observation xt+1.\nSet st+1 ←(st, at, xt+1) and enque (st, at, rt, st+1) to D.\nUniformly sample a batch {\n\u0000sj, aj, rj, s′\nj\n\u0001\n}|B\nj=1 from D and\nset ∀j ∈[B], yj = rj + 1{j is terminal} · γ maxa′ Q(s′\nj, a′; θtarget\nτ\n)\nUpdate θe,t+1 using Q-objective 1\nB\nPB\nj=1 (yj −Q(sj, aj; θ))2 \f\f\nθ=θe,t.\nend for\nUpdate the parameter for next episode θe+1,1 ←θe,H+1.\nend for\nUpdate the target parameter θtarget\nτ+1 ←θM,H+1.\nend for\n13\nA.1.3\nTraining hardware\nFor all experiments, we used a local server (at the University of Wisconsin - Madison) running\nUbuntu 20.04 LTS, with 2 Intel Xeon Silver 4214R CPUs (2.40 GHz) and 196GB of RAM. No GPUs\nwere used in our experiments.\nA.2\nCode Licensing\nThe source code for the GOHR, provided at our public site, is licensed under the Apache License\nversion 2.0.\n14\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning.pdf"}
{"title":"GOAL: Towards Benchmarking Few-Shot Sports Game Summarization","authors":"Jiaan Wang, Tingyi Zhang, Haoxiang Shi","summary":"Sports game summarization aims to generate sports news based on real-time\ncommentaries. The task has attracted wide research attention but is still\nunder-explored probably due to the lack of corresponding English datasets.\nTherefore, in this paper, we release GOAL, the first English sports game\nsummarization dataset. Specifically, there are 103 commentary-news pairs in\nGOAL, where the average lengths of commentaries and news are 2724.9 and 476.3\nwords, respectively. Moreover, to support the research in the semi-supervised\nsetting, GOAL additionally provides 2,160 unlabeled commentary documents. Based\non our GOAL, we build and evaluate several baselines, including extractive and\nabstractive baselines. The experimental results show the challenges of this\ntask still remain. We hope our work could promote the research of sports game\nsummarization. The dataset has been released at\nhttps:\/\/github.com\/krystalan\/goal.","url":"http:\/\/arxiv.org\/abs\/2207.08635v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2207.08635v1","published":1658154558000,"comment":"work in progress","pdf_text":"GOAL: Towards Benchmarking Few-Shot\nSports Game Summarization\nJiaan Wang1∗\nTingyi Zhang1\nHaoxiang Shi2\n1Soochow University, Suzhou, China\n2Waseda University, Tokyo, Japan\n{jawang1,tyzhang1}@stu.suda.edu.cn\nhollis.shi@toki.waseda.jp\nAbstract\nSports game summarization aims to generate\nsports news based on real-time commentaries.\nThe task has attracted wide research attention\nbut is still under-explored probably due to the\nlack of corresponding English datasets. There-\nfore, in this paper, we release GOAL, the ﬁrst\nEnglish sports game summarization dataset.\nSpeciﬁcally, there are 103 commentary-news\npairs in GOAL, where the average lengths of\ncommentaries and news are 2724.9 and 476.3\nwords, respectively. Moreover, to support the\nresearch in the semi-supervised setting, GOAL\nadditionally provides 2,160 unlabeled com-\nmentary documents. Based on our GOAL, we\nbuild and evaluate several baselines, including\nextractive and abstractive baselines. The ex-\nperimental results show the challenges of this\ntask still remain. We hope our work could pro-\nmote the research of sports game summariza-\ntion. The dataset has been released at https:\n\/\/github.com\/krystalan\/goal.\n1\nIntroduction\nGiven the live commentary documents, the goal\nof sports game summarization is to generate the\ncorresponding sports news (Zhang et al., 2016).\nAs shown in Figure 1, the commentary document\nrecords the commentaries of a whole game while\nthe sports news brieﬂy introduces the core events\nin the game. Both the lengthy commentaries and\nthe different text styles between commentaries and\nnews make the task challenging (Huang et al., 2020;\nWang et al., 2021, 2022a).\nZhang et al. (2016) propose sports game summa-\nrization task and construct the ﬁrst dataset which\ncontains 150 samples. Later, another dataset with\n900 samples is presented by Wan et al. (2016). To\nconstruct the large-scale sports game summariza-\ntion data, Huang et al. (2020) propose SportsSum\nwith 5,428 samples. Huang et al. (2020) ﬁrst adopt\n∗Corresponding author.\nSports News\n...\nJean-Christophe Bahebeck burst into the left side of the Inter box and \nsaw his shot blocked by Samir Handanovic, before Valerio Verre smashed \nthe rebound just over.\n...\nAnd eventually the result was put beyond all doubt 17 minutes from time \n– Mauro Icardi squaring across the face of goal despite appearing to be \noffside and teeing Eder up for an easy finish.\nCommentary Document\n...\n23' | Corner, Inter Milan. Conceded by Albano Bizzarri.\n23' | Goal! Inter Milan 1, Pescara 0. Danilo D'Ambrosio (Inter Milan) \nright footed shot from the right side of the six yard box to the bottom \nright corner. Assisted by Marcelo Brozovic with a cross following a \ncorner.\n26' | Ledian Memushaj (Pescara) wins a free kick on the left wing.\n26' | Foul by Gary Medel (Inter Milan).\n...\n90' + 3' | Second Half ends, Inter Milan 3, Pescara 0.\n1306 words in total\n368 words in total\nFigure 1: An example of sports game summarization.\ndeep learning technologies for this task. Further,\nWang et al. (2021) ﬁnd the quality of SportsSum is\nlimited due to the original rule-based data cleaning\nprocess. Thus, they manually clean the SportsSum\ndataset and obtain SportsSum2.0 dataset with 5,402\nsamples. Wang et al. (2022a) point out the halluci-\nnation phenomenon in sports game summarization.\nIn other words, the sports news might contain addi-\ntional knowledge which does not appear in the cor-\nresponding commentary documents. To alleviate\nhallucination, they propose K-SportsSum dataset\nwhich includes 7,428 samples and a knowledge\ncorpus recording the information of thousands of\nsports teams and players.\nThough great contributions have been made, all\nthe above sports game summarization datasets are\nChinese since the data is easy to collect. As a result,\nall relevant studies (Zhang et al., 2016; Yao et al.,\n2017; Wan et al., 2016; Zhu et al., 2016; Liu et al.,\n2016; Lv et al., 2020; Huang et al., 2020; Wang\net al., 2021) focus on Chinese, which might be difﬁ-\ncult for global researchers to understand and study\nthis task. To this end, in this paper, we present\nGOAL, the ﬁrst English sports game summariza-\narXiv:2207.08635v1  [cs.CL]  18 Jul 2022\ntion dataset which contains 103 samples, each of\nwhich includes a commentary document as well\nas the corresponding sports news. Speciﬁcally, we\ncollect data from an English sports website1 that\nprovides both commentaries and news. We study\nthe dataset in few-shot scenario due to: (1) When\nfaced with real applications, few-shot scenarios are\nmore common. Compared with the summarization\ndatasets in the news domain which usually con-\ntain tens of thousands of samples, the largest Chi-\nnese sports game summarization dataset is still in a\nlimited scale. (2) The English sports news is non-\ntrivial to collect. This is because manually writ-\ning sports news is labor-intensive for professional\neditors (Huang et al., 2020; Wang et al., 2022a).\nWe ﬁnd only less than ten percent of sports games\non the English website are provided with sports\nnews. To further support the semi-supervised re-\nsearch on GOAL, we additionally provide 2,160\nunlabeled commentary documents. The unlabeled\ncommentary documents together with labeled sam-\nples could train sports game summarization models\nin a self-training manner (Lee et al., 2013).\nBased on our GOAL, we build and evaluate sev-\neral baselines of different paradigms, i.e., extractive\nand abstractive baselines. The experimental results\nshow that the sports game summarization task is\nstill challenging to deal with.\n2\nGOAL\n2.1\nData Collection\nGoal.com is a sports website with plenty of foot-\nball games information. We crawl both the com-\nmentary documents and sports news from the\nwebsite. Following Zhang and Eickhoff (2021),\nthe football games are collected from four major\nsoccer tournaments including the UEFA Cham-\npions League, UEFA Europa League, Premier\nLeague and Series A between 2016 and 2020.\nAs a result, we collect 2,263 football games in\ntotal.\nAmong them, all games have commen-\ntary documents but only 103 games have sports\nnews. Next, the 103 commentary-news pairs form\nGOAL dataset with the splitting of 63\/20\/20 (train-\ning\/validation\/testing). The remaining 2,160 (un-\nlabeled) commentary documents are regarded as\na supplement to our GOAL, supporting the semi-\nsupervised setting in this dataset. Figure 1 gives an\nexample from GOAL.\n1https:\/\/www.goal.com\/\nDataset\n# Num.\nCommentary\nNews\nAvg.\n95th pctl\nAvg.\n95th pctl\nChinese\nZhang et al. (2016)\n150\n-\n-\n-\n-\nWan et al. (2016)\n900\n-\n-\n-\n-\nSportsSum (Huang et al., 2020)\n5428\n1825.6\n3133\n428.0\n924\nSportsSum2.0 (Wang et al., 2021)\n5402\n1828.6\n-\n406.8\n-\nK-SportsSum (Wang et al., 2022a)\n7854\n1200.3\n1915\n351.3\n845\nEnglish\nGOAL (supervised)\n103\n2724.9\n3699\n476.3\n614\nGOAL (semi-supervised)\n2160\n2643.7\n3610\n-\n-\nTable 1: Statistics of GOAL and previous datasets. “#\nNum.” indicates the number of samples in the datasets.\n“Avg.” and “95th pctl” denote the average and 95th per-\ncentile number of words, respectively.\n2.2\nStatistics\nAs shown in Table 1, Goal is more challenging\nthan previous Chinese datasets since: (1) the num-\nber of samples in GOAL is signiﬁcantly less than\nthose of Chinese datasets; (2) the length of input\ncommentaries in GOAL is much longer than the\ncounterparts in Chinese datasets.\nIn addition, compared with Chinese commen-\ntaries, English commentaries do not provide real-\ntime scores. Speciﬁcally, each commentary in Chi-\nnese datasets is formed as (t, c, s), where t is the\ntimeline information, c indicates the commentary\nsentence and s denote current scores at time t. The\ncommentaries in GOAL do not provide s element.\nWithout this explicit information, models need to\nimplicitly infer the real-time status of games.\n2.3\nBenchmark Settings\nBased on our GOAL and previous Chinese datasets,\nwe introduce supervised, semi-supervised, multi-\nlingual and cross-lingual benchmark settings. The\nﬁrst three settings all evaluate models on the testing\nset of GOAL, but with different training samples:\n(i) Supervised Setting establishes models on the\n63 labeled training samples of GOAL; (ii) Semi-\nSupervised Setting leverages 63 labeled and 2160\nunlabeled samples to train the models; (iii) Multi-\nLingual Setting trains the models only on other\nChinese datasets.\nMoreover, inspired by Feng et al. (2022); Wang\net al. (2022b); Chen et al. (2022b), we also pro-\nvide a (iv) Cross-Lingual Setting on GOAL, that\nlets the models generate Chinese sports news based\non the English commentary documents. To this\nend, we employ four native Chinese as volunteers\nto translate the original English sports news (in\nGOAL dataset) to Chinese. Each volunteer majors\nin Computer Science, and is proﬁcient in English.\nThe translated results are checked by a data ex-\npert. Eventually, the cross-lingual setting uses the\n63 cross-lingual samples to train the models, and\nverify them on the cross-lingual testing set.\n2.4\nTask Overview\nFor a given live commentary document C\n=\n{(t1, c1), (t2, c2), ..., (tn, cn)}, where ti is the\ntimeline information, ci is the commentary sen-\ntence and n indicates the total number of commen-\ntaries, sports game summarization aims to generate\nits sports news R = {r1, r2, ..., rm}. ri denotes\nthe i-th news sentence. m is the total number of\nnews sentences.\n3\nExperiments\n3.1\nBaselines and Metrics\nWe only focus on the supervised setting, and re-\nserve other benchmark settings for future work.\nThe adopted baselines are listed as follows:\n• Longest: the longest k commentary sentences\nare selected as the summaries (i.e., sports news).\nIt is a common baseline in summarization.\n• TextRank (Mihalcea and Tarau, 2004) is a pop-\nular unsupervised algorithm for extractive sum-\nmarization. It represents each sentence from a\ndocument as a node in an undirected graph, and\nthen extracts sentences with high importance as\nthe corresponding summary.\n• PacSum (Zheng and Lapata, 2019) enhances the\nTextRank algorithm with directed graph struc-\ntures, and more sophisticated sentence similarity\ntechnology.\n• PGN (See et al., 2017) is a LSTM-based abstrac-\ntive summarization model with copy mechanism\nand coverage loss.\n• LED (Beltagy et al., 2020) is a transformer-based\ngenerative model with a modiﬁed self-attention\noperation that scales linearly with the sequence\nlength.\nAmong them, Longest, TextRank and PacSum\nare extractive summarization models which directly\nselect commentary sentences to form sports news.\nPGN and LED are abstractive models that gener-\nate sports news conditioned on the given commen-\ntaries. It is worth noting that previous sports game\nsummarization work typically adopts the pipeline\nmethods (Huang et al., 2020; Wang et al., 2021,\n2022a), where a selector is used to select important\ncommentary sentences, and then a rewriter con-\nveys each commentary sentence to a news sentence.\nMethod\nValidation\nTesting\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nExtractive Methods\nLongest\n31.2\n4.6\n20.0\n30.3\n4.2\n19.5\nTextRank\n30.3\n3.7\n20.2\n27.6\n2.9\n18.8\nPacSum\n31.8\n4.9\n20.6\n31.0\n5.3\n19.6\nAbstractive Methods\nPGN\n34.2\n6.3\n22.7\n32.8\n5.7\n21.4\nLED\n37.8\n9.5\n25.2\n34.7\n7.8\n24.3\nTable 2: Experimental results on GOAL.\nHowever, due to the extremely limited scale of our\nGOAL, we do not have enough data to train the se-\nlector. Therefore, all baselines in our experiments\nare end-to-end methods.\nTo evaluate baseline models, we use ROUGE-\n1\/2\/L scores (Lin, 2004) as the automatic met-\nrics. The employed evaluation script is based on\npy-rouge toolkit2.\n3.2\nImplementation Details\nFor Longest, TextRank3 and PacSum4 baselines,\nwe extract three commentary sentences to form\nthe sports news. β, λ1 and λ2 used in PacSum\nare set to 0.1, 0.9 and 0.1, respectively. The sen-\ntence representation in PacSum is calculated based\non TF-IDF value. For LED baseline, we utilize\nled-base-163845 with the default settings. We set\nthe learning rate to 3e-5 and batch size to 4. We\ntrain the LED baselines with 10 epochs and 20\nwarmup steps. During training, we truncate the in-\nput and output sequences to 4096 and 1024 tokens,\nrespectively. In the test process, the beam size is\n4, minimum decoded length is 200 and maximum\nlength is 1024.\n3.3\nMain Results\nTable 2 shows experimental results on GOAL. The\nperformance of extractive baselines is limited due\nto the different text styles between commentaries\nand news (commentaries are more colloquial than\nnew). PGN outperforms the extractive methods\nsince it can generate sports news not limited to\noriginal words or phrases. However, such a LSTM-\nbased method cannot process long documents ef-\nﬁciently. LED, as an abstractive method, achieves\n2https:\/\/github.com\/Diego999\/py-rouge\n3https:\/\/github.com\/summanlp\/textrank\n4https:\/\/github.com\/mswellhao\/PacSum\n5https:\/\/huggingface.co\/allenai\/\nled-base-16384\nNews Sentence: Juve almost levelled on the stroke of \nhalf-time, but Maxi Olivera crucially [MASK] \nHiguain's goal-bound effort to preserve the hosts' lead.\nPredict (top5): stopped, assisted, ended, saved, broke\nLabel: blocked\nNews Sentence: Fiorentina finally made the most of \ntheir superiority eight minutes before the break – \nKalinic [MASK] Buffon from a tight angle after being \nfed into the box by Filippo Bernardeschi.\nPredict (top5): beat, caught, charged, run, hit\nLabel: beating\nFigure 2: Predicting key verbs during generating sports\nnews based on LED baseline. During predicting, the\nmodel is conditioned on the corresponding commen-\ntaries. The gray text indicates where the model do not\ncompute self-attention mechanism.\nthe best performance among all baselines due to its\nabstractive nature and sparse attention.\n3.4\nDiscussion\nMoreover, we also manually check the generated\nresults of LED baseline, and ﬁnd that the gener-\nated sports news contains many repeated phrases\nand sentences, and fail to capture some important\nevents in the games. We conjecture this is because\nthe few-shot training samples make the baseline\ndifﬁcult to learn the task effectively.\nTo further verify whether the model is familiar\nwith sports texts, we let the model predict the key\nverbs during generating the sports news, which\nis similar to Chen et al. (2022a). As shown in\nFigure 2, when faced with a simple and common\nevent (i.e., beating), the trained LED model could\npredict the right verb (i.e., beat). However, for a\ncomplex and uncommon event (i.e., blocking), the\nmodel cannot make correct predictions.\nTherefore, it is an urgent need to utilize the ex-\nternal resources to enhance the model’s abilities\nto know sports texts and deal with sports game\nsummarization. For example, considering the semi-\nsupervised setting and multi-lingual settings, where\nthe models could make use of unlabeled commen-\ntaries and Chinese samples, respectively. Inspired\nby Wang et al. (2022c), the external resources and\nvanilla few-shot English training samples could be\nused to jointly train sports game summarization\nmodels in the multi-task, knowledge-distillation\nor pre-training framework. In addition, follow-\ning Feng et al. (2021a), another promising way\nis to adopt other long document summarization\nresources to build multi-domain or cross-domain\nmodels with sports game summarization.\n4\nConclusion\nIn this paper, we present GOAL, the ﬁrst English\nsports game summarization dataset, which contains\n103 commentary-news samples. Several extractive\nand abstractive baselines are built and evaluated on\nGOAL to benchmark the dataset in different settings.\nWe further analyze the model outputs and show the\nchallenges still remain in sports game summariza-\ntion. In the future, we would like to (1) explore\nthe semi-supervised and multi-lingual settings on\nGOAL; (2) leverage graph structure to model the\ncommentary information, and generate sports news\nin a graph-to-text manner (Feng et al., 2021b), or\neven consider the temporal information (i.e., ti) in\nthe graph structure (Zhang et al., 2022).\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\nArXiv, abs\/2004.05150.\nWeijie Chen, Yongzhu Chang, Rongsheng Zhang, Ji-\nashu Pu, Guandan Chen, Le Zhang, Yadong Xi, Yi-\njiang Chen, and Chang Su. 2022a. Probing simile\nknowledge from pre-trained language models.\nIn\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5875–5887, Dublin, Ireland.\nAssociation for Computational Linguistics.\nYulong Chen, Ming Zhong, Xuefeng Bai, Naihao Deng,\nJing Li, Xianchao Zhu, and Yue Zhang. 2022b. The\ncross-lingual conversation summarization challenge.\nArXiv, abs\/2205.00379.\nXiachong Feng, Xiaocheng Feng, and Bing Qin. 2021a.\nA survey on dialogue summarization: Recent ad-\nvances and new frontiers. ArXiv, abs\/2107.03175.\nXiachong Feng, Xiaocheng Feng, and Bing Qin. 2022.\nMSAMSum: Towards benchmarking multi-lingual\ndialogue summarization.\nIn Proceedings of the\nSecond DialDoc Workshop on Document-grounded\nDialogue and Conversational Question Answering,\npages 1–12, Dublin, Ireland. Association for Com-\nputational Linguistics.\nXiachong Feng, Xiaocheng Feng, Bing Qin, and Xin-\nwei Geng. 2021b. Dialogue discourse-aware graph\nmodel and data augmentation for meeting sum-\nmarization.\nIn Proceedings of the Thirtieth In-\nternational Joint Conference on Artiﬁcial Intelli-\ngence, IJCAI-21, pages 3808–3814. International\nJoint Conferences on Artiﬁcial Intelligence Organi-\nzation. Main Track.\nKuan-Hao Huang, Chen Li, and Kai-Wei Chang. 2020.\nGenerating sports news from live commentary: A\nChinese dataset for sports game summarization. In\nProceedings of the 1st Conference of the Asia-Paciﬁc\nChapter of the Association for Computational Lin-\nguistics and the 10th International Joint Conference\non Natural Language Processing, pages 609–615,\nSuzhou, China. Association for Computational Lin-\nguistics.\nDong-Hyun Lee et al. 2013. Pseudo-label: The simple\nand efﬁcient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges\nin representation learning, ICML, volume 3, page\n896.\nChin-Yew Lin. 2004.\nROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nMaofu Liu, Qiaosong Qi, Huijun Hu, and Han Ren.\n2016.\nSports news generation from live webcast\nscripts based on rules and templates. In Natural Lan-\nguage Understanding and Intelligent Applications,\npages 876–884. Springer.\nXue-Qiang Lv, Xin-Dong You, Wen-Chao Wang, and\nJian-She Zhou. 2020. Generate football news from\nlive webcast scripts based on character-cnn with ﬁve\nstrokes. Journal of Computers, 31(1):232–241.\nRada Mihalcea and Paul Tarau. 2004.\nTextRank:\nBringing order into text. In Proceedings of the 2004\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 404–411, Barcelona, Spain.\nAssociation for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nXiaojun Wan, Jianmin Zhang, Jin-ge Yao, and Tian-\nming Wang. 2016.\nOverview of the nlpcc-iccpol\n2016 shared task: sports news generation from live\nwebcast scripts. In Natural Language Understand-\ning and Intelligent Applications, pages 870–875.\nSpringer.\nJiaan Wang, Zhixu Li, Qiang Yang, Jianfeng Qu,\nZhigang Chen, Qingsheng Liu, and Guoping Hu.\n2021.\nSportsSum2.0:\nGenerating High-Quality\nSports News from Live Text Commentary, page\n3463–3467. Association for Computing Machinery,\nNew York, NY, USA.\nJiaan Wang, Zhixu Li, Tingyi Zhang, Duo Zheng, Jian-\nfeng Qu, An Liu, Lei Zhao, and Zhigang Chen.\n2022a. Knowledge enhanced sports game summa-\nrization. In Proceedings of the Fifteenth ACM Inter-\nnational Conference on Web Search and Data Min-\ning, WSDM ’22, page 1045–1053, New York, NY,\nUSA. Association for Computing Machinery.\nJiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng,\nZhixu Li, Jianfeng Qu, and Jie Zhou. 2022b. Clid-\nsum: A benchmark dataset for cross-lingual dia-\nlogue summarization. ArXiv, abs\/2202.05599.\nJiaan Wang, Fandong Meng, Duo Zheng, Yunlong\nLiang, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2022c.\nA survey on cross-lingual summarization.\nArXiv,\nabs\/2203.12515.\nJin-ge Yao, Jianmin Zhang, Xiaojun Wan, and Jianguo\nXiao. 2017. Content selection for real-time sports\nnews construction from commentary texts. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 31–40, Santi-\nago de Compostela, Spain. Association for Compu-\ntational Linguistics.\nJianmin Zhang, Jin-ge Yao, and Xiaojun Wan. 2016.\nTowards constructing sports news from live text\ncommentary.\nIn Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1:\nLong Papers), pages 1361–\n1371, Berlin, Germany. Association for Computa-\ntional Linguistics.\nRuochen Zhang and Carsten Eickhoff. 2021.\nSOC-\nCER: An information-sparse discourse state track-\ning collection in the sports commentary domain. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4325–4333, Online. Association for Compu-\ntational Linguistics.\nTingyi Zhang, Zhixu Li, Jiaan Wang, Jianfeng Qu, Lin\nYuan, An Liu, Lei Zhao, and Zhigang Chen. 2022.\nAligning internal regularity and external inﬂuence\nof multi-granularity for temporal knowledge graph\nembedding. In Database Systems for Advanced Ap-\nplications.\nHao Zheng and Mirella Lapata. 2019. Sentence cen-\ntrality revisited for unsupervised summarization. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n6236–6247, Florence, Italy. Association for Compu-\ntational Linguistics.\nLiya Zhu, Wenchao Wang, Yujing Chen, Xueqiang Lv,\nand Jianshe Zhou. 2016. Research on summary sen-\ntences extraction oriented to live sports text. In Nat-\nural Language Understanding and Intelligent Appli-\ncations, pages 798–807. Springer.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GOAL: Towards Benchmarking Few-Shot Sports Game Summarization.pdf"}
{"title":"Meterstick: Benchmarking Performance Variability in Cloud and Self-hosted Minecraft-like Games Extended Technical Report","authors":"Jerrit Eickhoff, Jesse Donkervliet, Alexandru Iosup","summary":"Due to increasing popularity and strict performance requirements, online\ngames have become a workload of interest for the performance engineering\ncommunity. One of the most popular types of online games is the Minecraft-like\nGame (MLG), in which players can terraform the environment. The most popular\nMLG, Minecraft, provides not only entertainment, but also educational support\nand social interaction, to over 130 million people world-wide. MLGs currently\nsupport their many players by replicating isolated instances that support each\nonly up to a few hundred players under favorable conditions. In practice, as we\nshow here, the real upper limit of supported players can be much lower. In this\nwork, we posit that performance variability is a key cause for the lack of\nscalability in MLGs. We propose a novel operational model for MLGs and use it\nto design the first benchmark that focuses on MLG performance variability,\ndefining specialized workloads, metrics, and processes. We conduct real-world\nbenchmarking of MLGs and find environment-based workloads and cloud deployment\nto be significant sources of performance variability: peak-latency degrades\nsharply to 20.7 times the arithmetic mean, and exceeds by a factor of 7.4 the\nperformance requirements. We derive actionable insights for game-developers,\ngame-operators, and other stakeholders to tame performance variability.","url":"http:\/\/arxiv.org\/abs\/2112.06963v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2112.06963v2","published":1639422206000,"comment":null,"pdf_text":"Meterstick: Benchmarking Performance Variability in Cloud and\nSelf-hosted Minecraft-like Games Technical Report\nJerrit Eickhoff\nJ.D.Eickhoff@student.tudelft.nl\nDelft University of Technology\nDelft, Netherlands\nJesse Donkervliet\nJ.J.R.Donkervliet@vu.nl\nVrije Universiteit Amsterdam\nAmsterdam, Netherlands\nAlexandru Iosup\nA.Iosup@vu.nl\nVrije Universiteit Amsterdam\nAmsterdam, Netherlands\nABSTRACT\nDue to increasing popularity and strict performance requirements,\nonline games have become a workload of interest for the perfor-\nmance engineering community. One of the most popular types of\nonline games is the Minecraft-like Game (MLG), in which players\ncan terraform the environment. The most popular MLG, Minecraft,\nprovides not only entertainment, but also educational support and\nsocial interaction, to over 130 million people world-wide. MLGs\ncurrently support their many players by replicating isolated in-\nstances that support each only up to a few hundred players under\nfavorable conditions. In practice, as we show here, the real upper\nlimit of supported players can be much lower. In this work, we\nposit that performance variability is a key cause for the lack of\nscalability in MLGs. We propose a novel operational model for\nMLGs and use it to design the first benchmark that focuses on MLG\nperformance variability, defining specialized workloads, metrics,\nand processes. We conduct real-world benchmarking of MLGs and\nfind environment-based workloads and cloud deployment to be sig-\nnificant sources of performance variability: peak-latency degrades\nsharply to 20.7 times the arithmetic mean, and exceeds by a factor\nof 7.4 the performance requirements. We derive actionable insights\nfor game-developers, game-operators, and other stakeholders to\ntame performance variability.\nKEYWORDS\nMeterstick, Benchmarking, Workloads, Performance Variability,\nOnline Games\n1\nINTRODUCTION\nThe gaming industry is the world’s largest entertainment indus-\ntry [65]—world-wide, games engage over 3 billion players and\nyield over $170 billion in revenue [62]. In this work, we focus\non Minecraft-like Games (MLGs), an emergent and highly popular\ntype of game where users can change almost every part of the en-\nvironment. The canonical example of an MLG is Minecraft, which\nis already the best-selling game of all time [55]. All MLGs, includ-\ning Minecraft, present an important challenge to the performance\nengineering community: although their user-bases can exceed 100\nmillion active users per month, their scalability is limited to only\n200-300 players even under very favorable conditions [76]. (MLGs\nsupport high concurrency by creating separate replicas of their\nvirtual worlds, essentially sharding state and not allowing cross-\ninstance interaction.) What limits MLG scalability? In this work,\nwe posit performance variability is a key limit to MLG scalability,\nand design and use an MLG benchmark focusing on this concept.\nMLGs represent an important and unique class of online mul-\ntiplayer games. Most importantly, MLGs allow players to create,\n0\n50\n100\n150\n200 300 600 900\nGame response time [ms]\nControl\nResource\nFarms\nNoticeable\nDelay\nUnplayable\nGame\nLower is better\nFigure 1: Minecraft response time in the AWS cloud.\nmodify, and remove in-game objects (e.g., player apparel) and geo-\ngraphical features (e.g., terrain) [40]. Moreover, some game objects\nand features are self-acting, that is, they act even when no player\ninput is applied to them. Players can use them to create dynamic\nelements, by “programming” the environment with combinations\nof self-acting parts.\nPerformance variability prevents MLG service providers from giv-\ning strict Quality of Service (QoS) guarantees, and simultaneously\nincentivizes overprovisioning of resources and limiting the num-\nber of players that can interact together. For example, Minecraft\nRealms, a Minecraft cloud-based service offered by Microsoft, limits\nthe number of players per game-instance to at most 10 (ten)! In\ncontrast, Hypixel, at 216,762 online players [58] the most populated\nMinecraft server, achieves high player-count by stitching together\nthousands of (independent) MLG instances using specialized tools,\nbut players in different instances cannot interact.\nIn this work, we show for the first time empirical evidence that\ncurrent MLGs experience significant performance variability. Fig-\nure 1 depicts an exemplary result—even with a single connected\nplayer, the response time varies from good (below 60 ms) to un-\nplayable (above 118 ms). We discuss this and similar real-world\nexperiments in §5.2.\nFurthermore, ours is the first study to systematically analyze the\neffects of performance variability on the operation of MLGs. By design-\ning and using for this purpose a novel benchmark, we provide an\nimportant complement to an emerging body of knowledge. Game\nresearchers and engineers are already aware of the impact of sev-\neral types of performance variability. Performance variability in\nnetworks causes players to stop playing sooner [33], and there are\nwidespread techniques in industry to prevent variability in frame\nrates [45, 74]. However, the effect of performance variability on the\ninteractive simulation of virtual worlds, and in particular on MLGs,\nis much less understood.\nPrior work in understanding the performance of MLGs [48, 76]\nand on improving their scalability [37, 39, 44] forms a valuable\ncontribution to the field, but does not currently consider explicitly\nperformance variability. Addressing this important gap, we make a\nfour-fold contribution:\narXiv:2112.06963v2  [cs.PF]  20 Feb 2023\n \nProtocol\nServer\nPlayers\nEntities\n\n\n\nGame Loop (Tick)\n\n\n\n\n\n\n\n\nEntities\nTerrain Simulation\nPlayer Behaviour\n...\nTerrain\nInput \nQueue\nOutput\nQueue\n50ms\nState\nUpdates\nFrames\nPlayer\nActions\nClient\nNetworking\nRendering\nInputs\nPlayers\nHome network\nCloud\nState\nData\nCenter\n1\n4\n3\n5\n2\nFigure 2: Overview of an MLG.\nC1 We propose an operational model of MLGs. Ours is the first\nto consider MLG-specific workloads (§2). Because MLGs allow\nplayers to program the virtual environment and terraform the\nterrain, they support new types of workload not available in\nmost traditional online games.\nC2 We design Meterstick, a benchmark that quantifies performance\nvariability in MLGs (§3). To this end, we propose a novel perfor-\nmance variability metric, and define a benchmarking approach\nto produce it experimentally. Our benchmark supports common\ndeployment-environments for MLGs offered as a service, in par-\nticular, both cloud-based and self-hosted. Our benchmark is the\nfirst to quantify performance variability in MLGs.\nC3 We conduct real-world experiments using Meterstick (§5) and,\nafter analyzing the results, propose actionable insights (§6). We\nevaluate the performance variability of three popular MLGs,\nrunning on two popular commercial cloud providers and one\nlocal compute-cluster.\nC4 Following open-science and reproducibility principles, we pub-\nlish Findable, Accessible, Interoperable, and Reusable (FAIR [79])\ndata, available on Zenodo [43], and Free-access Open-Source\nSoftware (FOSS) artifacts, available on GitHub [42].\n2\nOPERATIONAL MODEL OF\nMINECRAFT-LIKE GAMES\nFor contribution C1, first, we summarize a state-of-the-art oper-\national model and a reference architecture for MLGs (§2.1). Sec-\nond, we define MLG-specific environment-based workloads that are\ncaused by terrain and entity simulation; §2.2 defines the resulting\nMLG workload model. Third, we model the operational elements\nof these workloads (§2.3).\n2.1\nReference Architecture for MLGs\nWe leverage in this work a common reference architecture for\nMLGs [76]. As Figure 2 depicts, MLGs use a client-server architec-\nture and are commonly deployed in cloud environments. Players\nrun a client on their own device, which connects to a server instance\nrunning in the cloud. Some MLG developers publicly distribute their\nserver software, allowing players to self-host game instances. Pop-\nular cloud providers such as Amazon AWS and Microsoft Azure\nprovide tutorials for running these servers on their platform. Mi-\ncrosoft, the company that currently owns Minecraft, markets it\nas a cloud-based service through Minecraft Realms, which offers\nplayers a fully-managed Minecraft instance for a monthly fee [16].\nAdditionally, many smaller companies offer MLGs as a service; an\nextensive list appears in §5.1.\nThe client ( 1 ) has two main tasks. First, it translates player input\ninto in-game actions, which it speculatively applies to the local\nstate and also sends to the server for validation. The client-server\ncommunication uses an implementation-specific protocol ( 4 ) that\ncan be shared between different MLGs. Second, the client visualizes\nthe game state, at a fixed rate.\nThe server ( 2 ) is responsible for performing all in-game (virtual-\nworld) simulations, maintaining the global state, and disseminating\nstate-updates to clients. Different from simulators in science or engi-\nneering, video game simulations tolerate (temporary) inconsistency,\nand must support modifying the environment via user input. The\ngame loop ( 3 ) performs simulations, by applying state-updates to\nthe global state in discrete steps (ticks), at a fixed frequency. In MLGs,\nthis frequency is typically set to 20 Hz, or 50 ms per tick. If a tick\ntakes under 50 ms, the MLG waits for the next scheduled tick start.\nHowever, if a tick exceeds 50 ms, the tick frequency drops below\n20 Hz and the server enters an overloaded state. While in this state,\nthe game fails to meet its QoS requirements and can cause players\nto experience game stuttering, visual inconsistency, and increased\ninput latency. Prior work has shown direct causality between in-\ncreased input latency and reduced player experience [34, 38, 46].\nMLGs generate workloads, both data- and compute-intensive,\nthat do not exist in other types of games. In contrast to traditional\ngames, MLGs allow modifications to the terrain. This requires\nthe game server to simulate terrain changes and manage terrain-\nstate alongside the player- and entity-state found in traditional\ngames ( 5 ). Unlike other types of state, terrain state can be both\ndata-intensive and compute-intensive, without direct player input.\n2.2\nWorkloads in MLGs\nThis section presents our workload model for MLGs, which focuses\non players, terrain, and entities. We discuss each of these compo-\nnents, in turn, focusing on unique and challenging aspects. We\ndistinguish novel aspects in our research.\nFigure 3 presents a visual overview of our model. Beyond the\nstate-of-the-art, our workload model captures environment-based\nworkloads, which are caused by simulating the modifiable environ-\nment itself, and scale independently from the number of active\nplayers. We argue environment-based workloads are an important\npart of representative benchmarking workloads for MLGs. How-\never, existing benchmarks do not include this type of workload.\n2\nPlayer Behaviour\nTerrain Simulation\nEntities\nMovement\n\n\nCollision\nInteractions\n\n\n\n\nWith Players\nWith Entities\nTerrain Modification\nTerrain Generation\nPhysics Simulation\n\n\n\n\nGravity\nFluids\nLighting\nPlant Growth\nSimulated Constructs\nMovement\n\n\nCollision\nSpawning\nDecision Making\n\n\n\n\nPathfinding\nTerrain Modification\nEnvironment-Based Workload\nInefficient in MLGs\nUnique to MLGs\nExample of Workload\nCauses\/Creates\nFigure 3: Workload components in MLGs.\nAddressing this gap, we propose an MLG workload model which de-\nscribes a wide range of environment-based workloads. In Figure 3,\nTerrain Simulation and Entities are examples of environment-based\nworkloads.\n2.2.1\nWorkload from Players (known). Players cause workload for\nMLGs, and games in general, through their actions. MLGs support\nplayer-actions found in traditional games, e.g., player movement\nand interactions, and also MLG-specific actions, e.g., to modify ter-\nrain. For player movement, the game computes collisions to prevent\nplayers from walking through obstacles such as walls, and dissemi-\nnates location-changes to other players. Players can also interact\nwith other players and entities (i.e., objects), for example by collect-\ning resources and exchanging them with other players.\nAn important difference between MLGs and traditional games is\nsupport for player-actions that modify the terrain. In MLGs, players\ncan terraform—create, modify, and destroy the terrain, as well as\nthe buildings standing on the terrain. This can generate resource-\nintensive workloads in two ways. First, players can change a large\npart of the terrain in a short amount of time, for example through\nthe use of explosives. This is both compute- and data-intensive,\nbecause the game needs to compute the new terrain, and commu-\nnicate state updates to keep a consistent view across all players.\nSecond, players can construct dynamic elements such as simulated\nconstructs, which increase the complexity of the terrain simulation\nand are discussed in §2.2.2. The impact of player workloads has\nbeen previously examined in both the context of traditional video\ngames architectures and MLGs specifically [61, 76].\n2.2.2\nWorkload from Terrain Simulation (novel). In contrast to tra-\nditional games, a significant part of the MLG workload can come\nfrom generating and simulating the terrain. MLGs typically present\nplayers with an endless open world. This world is split into areas,\nwhich are lazily generated when players come near them. Once the\nterrain is generated, the game simulates it and allows players to\nmodify it.\nWe identify four important components of terrain simulation:\nphysics, lighting, plant growth, and simulated constructs. Although\nphysics and lighting simulations are present in traditional games,\nthe modifiable nature of the terrain makes it significantly more\nchallenging to manage such features in MLGs. Unlike static envi-\nronments, where physics simulation only needs to happen for the\nrelatively few entities that can move through the world, MLGs need\nto perform physics simulations on the many blocks that compose\nthe terrain itself. For example, a bridge can collapse when a player\nremoves its support pillars, or the terrain underneath them. Once\nthe bridge has collapsed, the bridge no longer casts shadow, so\nthe simulator needs to recompute lighting (frequently) at runtime;\nstatic environments do not have this dynamic workload.\nPlant growth is an example of a dynamic element unique to MLGs.\nPlants and trees change over time, reshaping the nearby terrain,\nthus generating new workload.\nThrough terrain modification, players can create simulated con-\nstructs. In a simulated construct, players place together dynamic\nelements (e.g., plants, automatic croppers) to achieve a certain goal.\nFor example, many players build irrigation systems that grow and\nharvest vegetables automatically, with high yield. Such systems can\nleverage tens to hundreds of dynamic elements, whose interaction\ngenerates compute-intensive workload for terrain simulation.\nIn MLGs, as we show in §5, even a single player can overload the\ngame simulator. This is in part because, in MLGs, a single player\ncan trigger complex simulations, for example, by building simu-\nlated constructs of arbitrary size. By contrast, in traditional games,\nonly the number of concurrent players is strongly correlated with\nworkload intensity.\n2.2.3\nWorkload from Entities (novel). An entity is an object that\nexists in the virtual world but is not a player or terrain. Examples\ninclude Non-Playable Characters (NPCs), mobiles (i.e., mobs), and\nitems (e.g., a sword). Entities can typically move or be moved by\nplayers and collide with each other. Here we describe two important\naspects of entity simulation which are challenging for MLGs.\nFirst, games typically instantiate entities at spawn points, e.g.,\nspawn an NPC at a spawn point in a dark cave when a player\nis about to enter. In contrast to static environments, where game\ndevelopers typically place these spawn points manually, MLGs need\nto compute spawn points dynamically as terrain modification may\nobstruct existing spawn points.\nSecond, NPCs use path-finding algorithms to move around the\nmap. Static worlds pre-compute overlay graphs with viable NPC\nlocations, improving computational efficiency. In contrast, MLGs\nhave changing terrain, so they must compute path-finding graphs\ndynamically, leading to additional compute-intensive workload.\n2.3\nOperational Model of MLGs\nWe detail in this section the game loop used by MLGs. We define the\noperational model as the set of operations, and of events triggering\nand linking them, of individual components in the implementation\nof the game loop. Novel, in this work, we analyze the performance\nimplications of the unique aspects of MLG workloads (see §2.2)\nwhen executed with the MLG operational model.\nFigure 4 depicts a constructed, generalized, and simplified opera-\ntional model of the MLG game loop. To run the game loop, the game\nserver orchestrates primarily three main components, the Network-\ning Queues (component 1 in Figure 4), the Game Loop ( 2 ), and\nthe Game State ( 3 ), which we discuss in turn.\n3\nOut\nNetworking\nTerrain Simulation\nEntities\nReceive\nPlayer \nInput\nSend Player \nActions\nSend \nfor \nRendering\nReceive State \nUpdates\nTerrain\nClient\nServer\nGame Loop\nPlayer Handler\nState \nUpdates\nState \nUpdates\nGame State\nIn\nNetworking Queues\nGet\nPlayer\nActions\nState \nUpdates\nRead\/Write\nRead\/Write\nRead\/Write\nProcess \nActions\nProcess \nActions\nProcess \nActions\nBlocking message\nNon-blocking message\nReturn value\nLOOP: while server is running\nLOOP: while simulation rules applicable\nTimeline\n1\n2\n3\n4\n5\n6\nB\nA\nC\nFigure 4: Operational model of an MLG. Note different elements of the Game Loop can be run concurrently.\nThe Networking Queues ( 1 ) buffer between the game clients\nand the server. When a client sends a player-action to the server,\nit is buffered in the incoming network queue until the next tick.\nWhen the server needs to send a state-update to one or several\nclients, it forwards the message to the networking queues, to be\nfurther buffered in the outgoing queue or sent directly to the client.\nThe Game Loop ( 2 ) simulates the virtual world and is the core\nof the game server. In an MLG, the game loop consists of three ele-\nments: players, the terrain, and entities. These elements correspond\nto the workloads specified in §2.2. Figure 4 shows each of these\nelements, and how they differ from the others. For each element,\nits simulation typically requires reading the current game state ( 3 ),\nand may result in terrain state changes that need to be persisted (i.e.,\nwritten). In this section, we focus on the terrain state because it is\nidiosyncratic to MLGs. Below, we discuss each simulation element\nin turn.\nThe Player Handler ( 4 ) is driven by player actions, which the\nGame Loop retrieves from the Networking Queues once per tick.\nFor example, players can move or build something in the virtual\nworld. Because the terrain can obstruct the player from performing\nthese actions, the Player Handler must read the terrain state in\nthe vicinity of the player. If the action is successful, player actions\nthat affect the terrain (e.g., building) need to be written back to the\nglobal Game State.\nTerrain Simulation ( 5 ) is largely independent from player input,\nand is instead driven by terrain state updates. When a terrain state\nupdate occurs, the Terrain Simulation applies its simulation rules\nto the new state. For example, a terrain simulation rule such as if\nterrain is not physically supported, it falls down can be triggered\nwhen a player removes the keystone from a bridge. These rules\ntrigger in a loop, where each iteration informs the adjacent terrain\nthat it is no longer supported. The resulting state changes are\nwritten back to the global Game State.\nEntities ( 6 ) are primarily driven by the Game State, including the\nstate of the terrain, players, and entities themselves. Entities such\nas NPCs need the terrain state primarily for pathfinding. In some\ncases, entities may themselves modify the terrain. For example, an\nNPC may place or remove terrain, or items such as explosives may\nremove large parts of terrain all at once.\nAlthough, from a performance perspective, it is desirable to run\nthe game loop elements concurrently, there are two challenges\nwith this approach. First, while these elements are in principle inde-\npendent, they have implicit dependencies through the game state\nwhich they access. Individual elements can only run concurrently\nas long as they do not access the same state. Second, terrain simu-\nlation rules can cause a sequence of state changes which cannot be\nparallelized, as is the case in the bridge example above.\nUsing our operational model for MLGs, we formulate two impli-\ncations for MLG performance variability. First, because environment-\nbased workloads do not rely on the presence of players, large\nenvironment-based workloads can cause ticks to exceed their max-\nimum duration, even with few or no players connected. Second,\nbecause player simulation and environment-based workloads must\nbe completed sequentially when they access the same state, even\nsmall environment-based workloads can affect tick duration given\nthey are spatially clustered.\n3\nMETERSTICK BENCHMARK DESIGN\nTo address contribution C2, we design Meterstick, a benchmark for\nevaluating performance variability in MLGs. The main novelty of\nMeterstick relates to its workloads (§3.3) and performance metrics\n(§3.5 and §4).\n4\nPlayer Emulation\n\n\n\n\n\n\n\nController\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLG\n\n\n\nMLG\nPlayer\nEmulation\nControl\nClient\nI\/O\nQueue\nGame\nLoop\nSystem\nMetrics\nCollector\n\n\n\n\n\nUser\nDeployment\nData\nVisualization\nData Retrieval\nControl\nData\nGame Messages\nRemote\nMachine\nLocal\nMachine\nControl\nServer\nControl\nClient\nMetric Externalizer\nApplication \nand \nSystem \nMetrics\nResults\n3\n2\n5\n44\n44\n47\n9\n10\n48\nConfiguration\n\n\n\n1\nWorkload\nIPs\n...\nKeys\n6\nExisting Component\nFigure 5: Architecture of Meterstick. Component 6 is sys-\ntem under test. Component 5 adapts tool [76].\n3.1\nSystem Requirements\nHere we describe our eight requirements for Meterstick. We define\nthe first three requirements specifically for our use case. The last\nfive relate to benchmarking computer systems in general, and are\nbased on existing guidelines [50, 77].\nR1 Captures performance variability of MLGs: Meterstick must\nbe capable of capturing relevant performance metrics at a\ngranularity sufficient for analysis of variability. The specific\nmeasure of variability must be applicable and meaningful in\nthe context of MLGs.\nR2 Validity of workloads: The workloads used in benchmarking\nof the MLG should be representative of real-world use and\naddress the workload types listed in §2.2.\nR3 Relevant metrics and experiments: The benchmark should\nsupport relevant experiments to isolate different sources of\nvariability, and collect meaningful metrics to allow suitable\nanalysis of these sources.\nR4 Fairness: The benchmark should provide a fair assessment\nfor compatible systems. In particular, bias towards any one\nsystem should be limited.\nR5 Ease of Use: The benchmark should be easy to configure and\nuse with any compatible system.\nR6 Clarity: The benchmark should present results to the user in\na way that is suitable for system performance variability.\nR7 Portability: The benchmark should support common deploy-\nment environments and be easy to port to others.\nR8 Scalability: Benchmark workloads should be scalable to ac-\ncommodate benchmarking on increasingly powerful hard-\nware or with more performant systems.\n3.2\nDesign Overview\nHere we present the design of Meterstick, our system for bench-\nmarking of performance variability in MLGs. Figure 5 presents\nMeterstick’s high-level design. We discuss the benchmark work-\nloads (addresses R2) and metrics (partially addresses R3) in more\ndetail in §3.3 and §3.5 respectively.\nIn our design, the user mainly interacts with Meterstick through\nits Configuration component ( 1 ). The Configuration allows the\nuser to capture performance variability by specifying the duration\nand number of iterations of experiments (partially addresses R1).\nThe Configuration further allows users to configure benchmark\nparameters, such as the systems under test and workload, and\ndeployment parameters, such as machine IP addresses (partially\naddresses R5).\nAfter specifying the configuration, the user launches Meterstick.\nThis triggers the Deployment component ( 2 ), which deploys com-\nponents and software dependencies to remote machines specified\nin the configuration. This only requires the user needs to specify a\nset of IP addresses of ssh-accessible machines. This makes Meter-\nstick portable (R7), and allows users to evaluate MLG performance\nvariability under cloud or self-hosted deployments.\nWhen deployment is complete, the Deployment component\nhands control to the Control Server ( 3 ). Meterstick follows a Con-\ntroller\/Worker pattern, with the Control Server as the controller,\nand the Control Clients as the workers ( 4 ). The Control Server\ncontains the operation logic, and is responsible for synchronizes the\noperation of all workers by exchanging messages with each Control\nClient running on each worker. The Control Server and Clients\nexchange the messages enumerated in Table 1. Depending on the\nconfiguration, the Control Client runs either player emulation or\nthe MLG.\nMeterstick uses one or more workers for player emulation ( 5 ).\nThese workers emulate players by connecting the MLG and auto-\nmatically sending player actions based on programmed behavior.\nMeterstick implements this by using the player emulation from\nYardstick [76], an existing MLG benchmark which we compare to\nMeterstick in detail in §7.\nOne worker runs the MLG ( 6 ), which is the system under test.\nMeterstick captures the MLG’s performance variability metrics\nusing the player emulator ( 5 ), the metric externalizer ( 7 ), and the\nsystem metrics collector ( 8 ). §3.5 details the operation of these\ncomponents and the metrics they collect, including our novel metric\nto capture performance variability.\nWhen the benchmark experiments are done, the Control Server\nactivates the Data Retrieval and Data Visualization components ( 9\nand 10 ), This component moves the collected data from the worker\nnodes to the user’s local machine, where it pre-processes the data\nthrough aggregation and reformatting. The Data Visualization com-\nponent ( 10 ) takes as input the processed data and automatically\noutputs producing basic plots for MLG performance and perfor-\nmance variability. Users can view these plots, and, if desired, pro-\nvide their own advanced plotting scripts for in-depth analysis (con-\ncludes R5, R6).\n5\nTable 1: List of controller messages. Dest specifies what\nnodes the message is for where Y is player emulation, M is\nthe server node and C is the controller.\nMessage\nEffect\nDest\nset_server:server\nSpecifies name of server\nY\/M\nset_jmx:jmx url\nSpecifies JMX URL\nM\niter:iteration\nSpecifies what iteration to start at\nY\/M\ninitialize\nStarts the selected server\nM\nlog_start\nStarts metric logging tools\nM\nlog_stop\nStops metric logging tools\nM\nstop_server\nStops running server\nM\nconnect\nStarts player emulation\nY\nconvert\nConverts metric bin files to CSV\nY\nok\nAcknowledges the previous message\nC\nkeep_alive\nNo-op, keeps TCP connection open\nM\/Y\nerr:error\nPrevious message has caused error\nC\nexit\nStops the controller client\nM\/Y\n3.3\nBenchmark Workloads (address R2, R4, R8)\nThis section presents Meterstick’s workloads. Meterstick uses the\nworkload model presented in §2, which divides workloads in three\nmain components: players, terrain simulation, and entities. By us-\ning this model, Meterstick’s workloads are applicable to MLGs\nin general, thus avoiding favoring specific systems (partially ad-\ndresses R4). In practice, the user specifies in the Configuration ( 1 )\nonly the player and terrain simulation parts of the workload, as\nentities are a result of terrain simulation (spawning, see §2.2).\nAs future systems may become sufficiently performant to mit-\nigate the impact of Meterstick’s workloads, Meterstick supports\nworkload scaling (R8). To increase Meterstick’s workload complex-\nity, the user can specify an increased number of players to scale\nthe player workload, and use Meterstick’s scale parameter to select\nhigher-complexity versions of the pre-configured workloads.\nWhile Meterstick supports arbitrary valid Minecraft worlds as\nworkloads, the remainder of this section describes the workloads\nwe design for use in our experiments to highlight performance\nvariability based off our workload model and observable community\nuse (R2).\nThe conceptual challenge of designing the benchmark workloads\nstems from the vast design space. MLGs give players fine-grained\ncontrol over the virtual environment, resulting in an endless number\nof possible world permutations and a large variety in types of\nsimulated constructs.\nAdditionally, finding evidence to support our selection proved\nto be challenging, as no peer-reviewed analysis of such artifacts\ncurrently exists and game operators do not want to share such\ninformation; searching for trustworthy communities and identi-\nfying suitable artifacts poses additional challenges. We detail our\nworkload design and the evidence supporting it throughout this\nsection.\n3.3.1\nThe Environment-Based Workloads. The environment work-\nload is determined by the MLG’s terrain generation and the terrain\nmodifications made by players. To obtain representative worlds and\nworkloads, we reconstruct highly popular creations (templates of\nTable 2: Minecraft worlds used as workload starting points\nby the Meterstick benchmark.\nName\nProperties\nSize [MB]\nControl\nFreshly generated world\n5.4\nTNT\nEntity actions, terrain updates\n6.3\nFarm\nResource Farm constructs\n26.0\nLag\nComplex simulated construct, stress test\n4.7\nTable 3: Simulated constructs in Farm world and their au-\nthor. Popularity measured in millions of views.\nName\nAmount\nAuthor\nPopularity\n[106 views]\nEntity Farm\n12\ngnembon [47]\n1.7\nStone Farm\n4\nShulkercraft [69]\n1.3\nKelp Farm\n4\nMumbo Jumbo [59]\n2.5\nItem Sorter\n1\nMysticat [60]\n0.8\nuseful simulated constructs, see §2.2) available on common sharing\nplatforms in the MLG community. Because the MLG community\nthrives on sharing player-created content with other players, this\napproach captures essential features of how the community uses\nthese systems.\nTo cover the range of valid workloads (R2), we include two\nworlds that result in a best-case workload and worst-case workload\nrespectively. During all environment-based workloads, Meterstick\nconnects to the game a single player that performs no actions. This\nis necessary to correctly capture the response time metric discussed\nin §3.5. The remainder of this section describes the worlds and their\nresulting workloads. We list the worlds used in Table 2.\nThe Control world results in a best-case workload while still\nbeing realistic. The Control world is an unmodified world gener-\nated by Minecraft version 1.16.4 using the seed -392114485. The\nmeasured results of this workload are used as a workload-level\nbaseline to compare the other workloads.\nThe TNT world contains a 16-by-16-by-14 cuboid filled with\nTNT blocks which are set to explode around 20 seconds after a\nplayer connects. In the systems tested, TNT operates by spawning\nan entity, which can be interacted with by other entities, including\nother TNT entities. Thus, when a large section of TNT is activated,\nthe MLG must perform a large number of both entity-collision and\nphysics calculations. Intentionally creating large-scale TNT chain\nreactions is a popular activity, which can be observed in a plethora\nof community-made content. For example, a video from 2018 that\nshows a chain reaction of thousands of TNT blocks has 21 million\nviews [41].\nThe Farm world contains multiple resource farms, which are\nsimulated constructs built by players to automatically generate in-\ngame resources. The specific designs of the simulated constructs in\nthis workload were sourced from popular community creators and\neach have 1.6 million views on average. Table 3 gives an overview\nof all resource farms present in the Farm world, their respective\nauthors and popularity. The Farm world has many different resource\nfarm simulated constructs, which we list in Table 3. The Entity and\n6\nTable 4: List of configurable parameters. V, F and P refer to\nVanilla, Forge and PaperMC respectively.\nParameter\nSpecifies\nTypical Value\nIPs\nNodes used\nnone\nSSL Keys\nAuthentication\nnone\nServers\nMinecraft-like games\nV, F, P\nWorld\nWorld workloads\nControl\nFile Locations\nOutput directories\n\/ys\/, \/mc\/\nResume\nContinue experiment\nFalse\nPorts\nNetwork config\n25555\/25565\nJMX URLs\nMetric collection\nVarious\nJMX Ports\nMetric collection\n25585-25635\nRAM\nRAM (JVM argument)\n4GB\nAffinity\nCPU Affinity\n0xFFFFFFFF\nNumber of Bots\nPlayer count\n25\nBehavior\nPlayer actions\nBounded random\nDuration\nIteration length\n60 seconds\nIterations\nIteration amount\n1\nScale\nWorkload intensity\n1\nStone farms are activated at a fixed interval of around 4 seconds,\nwhereas the Kelp and Item sorter use event-based activation. These\nfarms rely on entities in their functioning, through spawning driven\nentities and manipulating their pathfinding, or through the creation\nof passive entities to represent item resources. A core feature of\nMLGs is collection of resources from the game environment. The\nability for players to construct simulated constructs that automate\nthis process is both an intended and common behavior. The Entity\nfarm relies on spawning driven entities and manipulating their\nmovement. The Stone and Kelp farm continuously destroy blocks,\nwhich create passive entities to represent items. These item entities\nare then transported through terrain simulation rules (e.g. liquid\nphysics simulation).\nThe Lag world results in a worst-case workload. This world\ncontains a simulated construct known in the MLG community as\na Lag Machine. Lag Machines are a specific subset of simulated\nconstructs that are designed to cause high computational load for\nthe MLG, either for the purpose of stress testing it, or to cause it\nto crash as part of a denial of service attack. The design of the Lag\nMachine used in this workload is publicly available and provided by\na community-creator with 52 thousand subscribers [73]. It is chosen\nas it operates based on terrain simulation rules. Specifically, it uses\nmany logic-gate constructs in a small area to cause a high volume\nof simulation rule activations. Importantly, the simulation rules\nthis Lag Machine uses are generally non-malicious, and are used\nin many resource farm constructs, as well as forming the basis for\nsimulated constructs such as an operational digital Computer [54].\n3.4\nConfiguration Parameters\nMeterstick is highly configurable in order to be extensible to new\nMinecraft-like games, environments, workloads, and experiments (R5).\nA list of configuration parameters are given in Table 4.\nThe experiment can be configured by specifying the duration,\nnumber of iterations, what servers to run, how many players to\nTable 5: Metrics collected by Meterstick. The metric type is\nDerived, Application level, or System level.\nType\nMetric\nDescription\nD\nInstability Ratio\nTick instability (see §4)\nA\nResponse time\nRound trip latency for clients\nA\nTick duration\nDuration of each tick\nA\nTick distribution\nTick time by workload\nS\nCPU\nCPU utilization\nS\nMemory\nMemory usage\nS\nThreads\nThread total\nS\nDisk I\/O\nBytes read\/written\nS\nNetwork I\/O\nBytes sent\/received\nconnect and the behavior of those players. Since the IPs of the\nnodes that the deployment tool connects to can be anywhere that\nis network accessible, nodes can be chosen that are geographically\ndistant or within the same datacenter in order to measure (or avoid)\nthe performance impact caused by public network infrastructure.\n3.4.1\nThe Player-Based Workload. Meterstick uses a player-based\nworkload facilitated by the player emulation component ( 5 ). In\nthis workload, Meterstick is configured to connect 25 players which\nmove randomly in a 32-by-32 area. The existing Yardstick bench-\nmark [76] focuses solely on the impact of player workload. So, we\ninclude this player workload to represent a high-density area in\nMLGs and allow Meterstick to compare the impact of environment-\nbased workloads with a traditional player-based workload. We\nselect a player count of 25 based on the Minecraft Wiki’s dedicated\nserver recommendation [25], as well as the recommendations from\nvarious commercial cloud providers (see §5.1).\n3.5\nMetrics (address R1, R3, R4)\nThis section describes the application-level and system-level met-\nrics metrics collected by Meterstick, selected to fulfill R3. In §4 we\ndescribe our novel Instability Ratio (ISR) which quantifies perfor-\nmance variability (concludes R1). Our ISR metric and all application\nand system metrics are general to MLGs to avoid bias for specific\nimplementations (concludes R4). Table 5 gives an overview of the\ncollected metrics.\n3.5.1\nApplication-level Metrics. Meterstick collects three applica-\ntion level metrics: response time, tick duration, and tick distribution.\nResponse time is how system latency becomes visible to the user.\nLower values are better, and we use existing latency thresholds for\nthe game becoming noticeable and unplayable at 60ms and 116ms\nrespectively [38, 46].\nThe response time is measured as the time between a player tak-\ning an action and the results of that action becoming visible. During\nthis time, the action is sent over network to the MLG server, added\nto an input queue, simulated during the next tick, has its resulting\nstate changes added to the output queue, and then sent back to\nthe client over the network. In our workload model (Figure 4), this\nis visible as the time difference between the client sending player\nactions ( A ) and receiving state updates ( C ). Meterstick captures\nthis metric by having a player send chat messages to all players\n7\n(including itself), and measuring how long it takes for the player to\nreceive its own message.\nWhile tick duration and tick distribution cannot be directly ob-\nserved by players, MLGs typically expose these metrics through\ninterfaces commonly used by debugging tools. Meterstick’s Met-\nric Externalizer ( 7 ) uses these interfaces to gain access to these\nmetrics without requiring access to the game’s source code. As\na consequence, Meterstick is easily configured to work with new\nMLGs.\nThe tick duration is the amount of time it takes the MLG to\ncomplete a single iteration of the game loop, and tick distribution is\nthe percent of tick time the MLG spent simulating each workload\ncomponent, such as simulating entities. Both metrics are directly\nrelated to game response time and are important indicators of game\nperformance. More detail about the relationship between these\nmetrics is available in §2.1.\n3.5.2\nSystem-level Metrics. Meterstick captures system-level met-\nrics to allow users to perform a more in-depth performance analysis.\nMeterstick collects system-level metrics using the System Metrics\nCollector ( 8 ), which queries the operating system twice per second.\nMeterstick collects CPU utilization, memory usage, the number\nof operating-system threads associated with the MLG, disk I\/O, and\nnetwork I\/O. These metrics allow users to analyze causes of high\ntick duration, and check for potential performance bottlenecks.\n4\nINSTABILITY RATIO METRIC\nIn this section we present our novel Instability Ratio metric. We\npresent a definition, analyze its properties, and compare it to exist-\ning metrics.\n4.1\nInstability Ratio Definition\nIn the context of online gaming, players prefer stable performance\nto unstable, but on average faster, performance [33, 63, 66]. Stabil-\nity facilitates predictability, allowing players to acclimate to game\nupdate delays, up to a point. Thus, it is beneficial to quantitatively\nanalyze the stability of game performance by analyzing the variabil-\nity of game cycles, or ticks (see §2.1). However, existing measures\nof variability are insufficient, because they do not capture the order\nof ticks, outlier values, the duration of the trace, or a combination\nof these elements.\nWe describe here our novel Instability Ratio (ISR), a normalized\nmetric based on cycle-to-cycle jitter [35, 53]. In the context of MLGs,\nwe measure cycle-to-cycle jitter as the difference in delay between\nconsecutive ticks (see §2.1). This delay starts to vary when the game\nbecomes overloaded. We compute the ISR as the normalized sum of\nMLG cycle-to-cycle jitter. The cycle-to-cycle jitter considers only\nthe difference between two consecutive ticks; reports for this metric\ninclude the maximum or moving average value. Novel in this work,\nour ISR metric sums the differences, and normalizes the result.\nThe full metric equation is shown in Equation 1, where 𝑁𝑒is\nthe expected number of ticks, 𝑡𝑖is the duration of the 𝑖th game tick,\n𝑏is the delay between ticks when the MLG runs at its intended\nfrequency, max(𝑏,𝑡𝑖) is the period of tick 𝑖, and 𝑁𝑎is the actual\nnumber of ticks.\nWhen a tick lasts longer than the 𝑏value, the proceeding tick\nis delayed. Thus, if the game meets its performance requirements\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n25\n50\n75\n100\nλ\nISR\ns\n2\n10\n20\n(a) Behavior of ISR for varying\noutlier periods (𝜆).\n0\n250\n500\n750\n1000\n0\n10\n20\n30\n40\n50\ntime [s]\ntick duration [ms]\nHigh ISR\nLow ISR\n(b) Example traces resulting in\ndifferent ISR values.\nFigure 6: Numerical analysis of Instability Ratio. Higher val-\nues indicate higher performance variability. 𝑠indicates the\noutlier scaling factor. 𝜆indicates the period between outliers\nin number of ticks.\n𝑁𝑎= 𝑁𝑒, but if it becomes overloaded 𝑁𝑎≤𝑁𝑒(i.e., ∃𝑖: 𝑡𝑖>\n𝑏=⇒𝑁𝑎≤𝑁𝑒).\nISR =\nÍ𝑁𝑎\n𝑖=1 |max(𝑏,𝑡𝑖) −max(𝑏,𝑡𝑖−1)|\n𝑁𝑒× 2𝑏\n(1)\nUsing this metric as a measure of variability, the range of possible\nvalues is 0 to 1. A ISR of 0 indicates no variability: the tick period\nis constant for all ticks in the trace. A ISR of 1 indicates maximum\nvariability, and is reached when the sum of differences between\nconsecutive ticks is equal to twice the duration of the trace (i.e.,\n𝑁𝑒×2𝑏). This value is reached when tick periods alternate between\ntheir intended value and extremely large values.\n4.2\nAnalysis of ISR Behavior\nWe analyze the behavior of ISR by modeling a trace where every\n𝜆ticks, one tick has a duration of 𝑠𝑏, while the others all have\nduration 𝑏. This means 1 in 𝜆ticks exceeds the performance re-\nquirement by a factor 𝑠. This allows expressing ISR as 𝐼𝑆𝑅=\n𝑠−1\n𝑠+𝜆−1.\nA plot of this function and an example trace based on this model\nare available in Figure 6.\nFigure 6a shows how ISR responds to outlier scaling and fre-\nquency. The horizontal axis shows 𝜆, which is the number of ticks\nbetween outliers, and the vertical axis shows the value of ISR. The\ncurves show the value of ISR for three values of 𝑠. The three values\nof s (2, 10, 20) indicate that all outliers exceed the latency require-\nment by a factor 2, 10, or 20, respectively.\nThe plot shows that ISR increases when outliers become larger (in-\ncreasing s), and when outliers are more common (lower 𝛿). For exam-\nple, a tick exceeding 𝑏by a factor 10 (𝑠= 10) every 25 ticks (𝜆= 25)\nresults in an ISR value of 0.26.\nFigure 6b shows two example traces: High ISR and Low ISR. Both\ntraces contain 1000 ticks. The horizontal axis shows time, and the\nvertical axis shows tick duration. Most ticks have a duration below\n50 ms (b), but each trace has five outliers with a scaling factor of 20,\nresulting in a 1 second spike. For the Low ISR trace, all outliers are\nthe start of the trace, whereas in the High ISR trace the outliers are\nevenly distributed over time. While the statistical distributions of\nthe traces are identical, the ISR for the Low ISR trace is 0.009, the\nISR for the High ISR trace is 0.15, an order of magnitude higher.\n8\nTable 6: Comparison of ISR with existing variability metrics.\nMetric\nOrder\nIrregular\nNormalized\nDependent\nSampling\nStandard deviation\n✗\n✗\n✗\nAllan variance [67]\n✓\n✗\n✗\nJitter [68]\n✓\n✓\n✗\nISR\n✓\n✓\n✓\n4.3\nComparing ISR to Alternative Metrics\nTable 6 compares ISR to existing measures of variability. Standard\ndeviation captures spread from an average value. It is not order\ndependent, and thus is not a measure of stability, but dispersion.\nAllan variance is used in the field of electrical engineering as a\ntime domain measure of frequency stability, typically applied to\nclocks or oscillators [67]. Allan variance is order dependent, but\nrelies on a constant sampling frequency and a continuous sampling\ndomain. Neither property is applicable to the duration of tick values.\nJitter is defined in the domain of networking as the smoothed\nabsolute difference between consecutive packets [68]. While most\nsimilar to ISR, which is based on cycle-to-cycle jitter, it is not nor-\nmalized, but rather reported as an average and defined for any\npacket, rather than an entire sampling duration.\n5\nREAL-WORLD EXPERIMENTS\nTo address contribution C3, we present here the setup and results\nfrom our real-world experiments. We use Meterstick to evaluate\nthe performance variability of three MLGs: Minecraft, Forge, and\nPaperMC. Our experiments use the workloads described in §3.3,\nand are conducted on two commercial cloud environments, Ama-\nzon AWS and Microsoft Azure, as well as on DAS-5, a supercom-\nputer for scientific workloads [31].\nMF1 Performance variability can make MLGs unplayable (§5.2).\nWe find that the maximum response time can be up to 20.7\ntimes higher than the arithmetic mean, and exceed by more\nthan a factor of 7.4 the threshold for playable games.\nMF2 Environment-based workloads cause significant performance\nvariability (§5.3). We find that environment-based workloads\nintroduce significant performance variability, increasing ISR\nby 0.04 up to 0.92. This variability can overload popular\nMLGs by 58 times the normal tick duration and even crash\nthe game.\nMF3 MLGs exhibit increased variability in commercial cloud en-\nvironments compared to self-hosted environments (§5.4).\nWe show that both clouds, AWS and Azure, introduce ad-\nditional performance variability between iterations of the\nsame workload compared to the local environment, DAS-5.\nThe choice of cloud causes a 1.39 up to 15.44 times increase\nin ISR IQR and a 1.09 up to 5.61 times increase tick time IQR.\nThe minimum observed ISR for both clouds exceeding the\nmaximum observed ISR on DAS-5.\nMF4 Processing the state of entities is computationally expen-\nsive (§5.5). Entity-related computations account for a major-\nity of the non-idle tick distribution in all experiments.\nMF5 The common hardware resource recommendations are in-\nsufficient to avoid performance variability (§5.6). The recom-\nmended node size exhibits high performance variability and\nhigh mean tick duration. Larger node sizes result in lower\nvalues of both, such that a node with 8 vCPUs reduces per-\nformance variability and mean tick duration to acceptable\nlevels.\n5.1\nExperimental Setup\nIn this section we describe our experimental setup. In our experi-\nments, we evaluate three MLGs, i.e., the systems under test, in three\ndifferent environments.\n5.1.1\nSystem under test. We use in our experiments three MLGs\nthat use the Minecraft protocol: the original Minecraft as developed\nby Mojang [1], Forge, and PaperMC. We select these services because\nof their popularity and utility.\nWe choose the official Minecraft server [1], as it is the default ser-\nvice that most users and hosts operating Minecraft servers employ.\nThis includes users of the server hosting service Realms, which is\nadvertised inside the Minecraft client [16]. Notably, the Minecraft\nservice does not allow for gameplay modifications (“mods\") of any\nkind.\nForge is the most popular MLG for operating modified (i.e., mod-\nded) services [15]. Of the top-50 most downloaded Minecraft mods,\n45 work exclusively with Forge. Of the 5 mods that are not exclusive\nto Forge, only one is incompatible with it [17]. PaperMC is marketed\nas a high-performance alternative to Minecraft [19]. While the Pa-\nperMC project does not quantify its performance improvement\nover Minecraft, it does provide documentation of its optimizations,\nwhich include extensive changes to threading models and virtual\nenvironment processing.\n5.1.2\nDeployment Environment. We evaluate the MLGs in two com-\nmercial cloud environments, Amazon AWS and Microsoft Azure,\nand DAS-5, a supercomputer for academic and educational use [31].\nWe choose AWS and Azure because they are the two cloud en-\nvironments with the biggest market share, with 32% and 20% re-\nspectively [64]. We use DAS-5 to evaluate how commercial cloud\nenvironments affect the performance variability of MLGs, compared\nto self-hosting these games on dedicated hardware.\nOur experiments on cloud environments use T3.Large and Stan-\ndard_D2_v3 nodes respectively. Both node types are equipped with\n2 vCPUs and 8 GB memory. We choose these nodes based on the de-\nfault hardware configurations recommended by Minecraft service\nproviders as well as guidelines published by AWS and Azure [29, 57].\nOn DAS-5, we use a regular node, which is equipped with a dual\n8-core 2.4 GHz processor and 64 GB memory, and limit the number\nof CPU cores available to the MLG by setting its CPU affinity to\ntwo cores, unless indicated otherwise. Because the MLGs used in\nour experiments run on the Java Virtual Machine (JVM), we limit\nmemory available to the MLG in both cloud an DAS-5 nodes by\nsetting the JVM’s maximum heap size to 4 GB.\nTable 7 lists a sampling of hardware recommendations from\ncommercial cloud companies that offer Minecraft-like game hosting.\nIf no plan was marked “recommended,\" data is taken from plans\nthat are comparable to recommended plan on other services. From\n9\nTable 7: Hardware recommendations from companies that\noffer cloud hosting of Minecraft-like games. Field marked\nNP are information not provided to consumers, fields\nmarked V are variable.\nService\nRAM [GB]\nvCPU[#]\nCPU Speed[GHz]\nHostinger [12]\n3\n3\nNP\nServer.pro [24]\n4\n2\n2.4\nSkynode [27]\n4\n2\n3.6\nScalaCube [22]\n3\n2\n3.4\nNodecraft [18]\n4\nNP\n3.8\nApex Hosting [4]\n4\nNP\n3.9\nGGServers [9]\n4\nNP\n3.2\nBisectHosting [6]\n4\nNP\n3.4\nShockbyte [26]\n4\nNP\n4.0\nCubedHost [7]\n2.5\nNP\n4.5\nServerMiner [23]\n3\nNP\n4.0\nAkliz [2]\n4\nNP\n3.4\nRamShard [21]\n2\nNP\n4.0\nMCProHosting [13]\n2\nNP\nNP\nGTXGaming [10]\n3\nNP\n3.8\nStickyPiston [28]\n2.5\nNP\nNP\nHostHavoc [11]\n4\nNP\n4\nFerox Hosting [8]\n4\nNP\nNP\nAquatis [5]\n4\nNP\n4.2\nPebbleHost [20]\n3\nNP\n3.7\nMelonCube [14]\n4\nNP\n3.4\nAzure [57]\n4\n2\nV\nAWS [29]\n1\n1\nV\nthese recommendations we find that 2 vCPU and 4 GB RAM is the\nmost common configuration. On AWS and Azure it is possible to\nselect specific configurations of hardware, however, to ensure that\nthe Benchmark metric tools have sufficient memory during the\nexperimental duration, we use nodes with at least 8 GB RAM and\nlimit the heap memory of the Minecraft-like game to 4 GB using\nthe -Xmx JVM argument in all experiments.\n5.2\nMF1: Performance variability can make\nMLGs unplayable\nDue to significant performance variability, the median and mean\ngame response times give an optimistic view of game performance,\nand is worse than the performance observed by players. Figure 7\ndepicts the result, and shows that the 95th percentile of game re-\nsponse time can be up to 4.1 times higher than the arithmetic mean,\nand exceed by more than a factor of 12.8 the threshold that makes\nthe game unplayable. In real-time games, a temporary spike in delay\ncan significantly affect the user’s experience, similar to a temporary\nfreeze in a phone call or video stream.\nFigure 7 shows the response time (horizontal axis) for two MLGs\n(Minecraft in green, and Forge in blue) under three different work-\nloads (vertical axis). The workloads and response time metric are\ndescribed in §3.3 and §3.5.1 respectively. PaperMC is omitted as\n0\n50\n100\n150\n200\n500\n1000\n1500\nMinecraft\nForge\nMean\nPlayer action response time [ms]\nControl\nFarm\nTNT\n2718 ms\n2303 ms\nNoticeable\nDelay\nUnplayable\nGame\nLower is better\nFigure 7: Game response time in AWS environment when\nrunning separate environment-based workloads. Whiskers\nindicate 5th and 95th percentile respectively. The black dia-\nmonds indicate arithmetic mean.\nit uses a dedicated asynchronous thread for chat messages, sepa-\nrately from the game tick. The whiskers extend to the 5th and 9th\npercentiles, respectively,\nand the black diamond indicates the arithmetic mean. The No-\nticeable Delay line (at 60 ms, in orange) and Unplayable Game\nline (at 118 ms, in red) indicate high game-latency which respec-\ntively marks the values where latency becomes noticeable to players\nand makes the game so unresponsive it becomes unplayable [38, 46].\nUnder the Control workload (top two boxes), the 95th percentile\nis below the noticeable threshold for both Minecraft and Forge.\nHowever, the maximum value for Forge (514 ms) is 20.7 times larger\nthan the mean, and the maximum value for Minecraft (679 ms) is\nexceeds by 7.4 times the Unplayable threshold at 118 ms. These\noutliers occur directly after a player connects to the game. This\nmeans that, even with good average performance, the game can still\nbe unplayable if players frequently connect, which is a common\noccurrence in online multiplayer games.\nCompared to the Control workload, the Farm and TNT work-\nloads show significantly more performance variability, pointing to\na further degradation of player experience. In all cases, the mean\nand median values give an overly optimistic view of the game’s per-\nformance. For the Farm workload, the mean and median values\nfor Forge (third bar from the top) indicate the response time is\nnoticeable, but not unplayable. However, the plot shows a 95th\npercentile of 225.8 ms, which is 1.9 times as high as the Unplayable\nthreshold. For Minecraft (fourth bar from the top), the mean and\nmedian values indicate that the response time is not noticeable\nfor players. However, the plot shows that performance variabil-\nity causes the response time to exceed the Noticeable threshold\nmore than 25% of the time (box’s right edge exceeds the Noticeable\nthreshold), and exceeds the Unplayable threshold more than 5%\nof the time. The TNT workload causes the highest performance\nvariability for both Forge and Minecraft (bottom two boxes, 547 ms\ninterquartile range for Forge and 503 ms for Minecraft). In both\ncases, the median response time is below the noticeable threshold,\nwhile the 95th percentiles are 12.7 times the unplayable threshold,\nand the maximum observed values (indicated with black arrows)\nare at least 19 times larger than the unplayable threshold.\n10\n0.00\n0.05\n0.10\n0.15\nPlayers\nLag\nTNT\nFarm\nControl\nPlayers\nLag\nTNT\nFarm\nControl\nPlayers\nLag\nTNT\nFarm\nControl\n0.85 0.90 0.95 1.00\nMLG:\nForge\nPaperMC\nMinecraft\nInstability Ratio\nSelf-Host,\n DAS5 16-core\nSelf-Host,\n DAS5 2-core\nCloud,\n AWS 2-core\ncrashed\nFigure 8: ISR for each MLG on the AWS and DAS-5 environ-\nments. The Lag workload crashed all MLGs on AWS; see text\nfor explanation. §3.4.1 defines the “Players” workload.\n0\n20\n40\n60\n0\n650\n1300\n1950\n2600\n0\n100\n200\n300\n400\n0\n20\n40\n60\n0\n100\n200\n300\n400\n0\n100\n200\n300\n400\nPaperMC\nForge\nMinecraft\nOverloaded Threshold\n480ms\nControl\nFarm\nTNT\nPlayers, n=25\ntime [s]\ntime [s]\ntick time [ms]\ntick time [ms]\nWorse\nWorse\nFigure 9: Tick time over time for each MLG in the AWS en-\nvironments running the Control, Farm, TNT and Players\nworkloads. The Lag workload on AWS is omitted as each\nMLG crashes. §3.4.1 defines the “Players” workload.\nFrom results in this section, we conclude that the mean and\nmedian values give an overly optimistic view of MLG performance,\nand that performance variability in MLGs results in noticeable and\nunplayable game latency, impacting players.\n5.3\nMF2: Environment-based workloads cause\nsignificant performance variability\nEnvironment-based workloads cause significantly increased perfor-\nmance variability on each game and in each environment tested,\nand can overload or crash the game. Figure 8 shows the perfor-\nmance variability of each MLG when running environment-based\nworkloads on AWS and DAS-5. Compared to the control work-\nload, each MLG on each environment exhibits higher performance\nvariability when operating environment-based workloads.\nFigure 8 shows performance variability, quantified using ISR\n(see Equation 1). The three top-level rows show three environment\nconfigurations, each containing five workloads. The color and shape\nof the marker indicate one of three MLGs.\nEnvironment-based workloads (i.e., Farm, TNT, Lag) cause sig-\nnificantly higher performance variability than the player workload\nand control workload for all games in all environments, with the\nexception of PaperMC on AWS (red circles in the top row). This\nprovides evidence that environment-based workloads cause signifi-\ncant performance variability. Further analysis into the behavior of\nPaperMC reveals that it contains performance optimizations specif-\nically for handling TNT explosions, improving its performance on\nthe TNT workload, and Redstone, a simulated block type which is\nused in the Farm workload (analysis of PaperMC given in ). This\nprovides evidence that the performance variability caused by these\nenvironment-based workloads are known to the MLG community\nand can (at least partially) be addressed through engineering.\nOf all workloads, the Lag workload causes the most performance\nvariability. Further analysis reveals that this happens because this\nworkload consists mainly of parts which are only simulated every\nother tick, causing the game to alternate between extremely short\nand extremely long ticks. This maximizes the value of ISR, which\nis based on the difference in duration between consecutive ticks.\nThere are no results for running the Lag workload on AWS because\nall three MLGs crash when a player joins and the environment\nsimulation begins. The corresponding increase in tick duration\ncauses the player’s connection to time-out, forcing each MLG to\nstop.\nFigure 9 shows the game’s tick duration over time for each game\nwhen running on AWS. The dashed black line indicates the over-\nloaded threshold at 50 ms, and the green line allows calibrating the\nvertical axis across the four sub-plots.\nThe stability observed when running the Control workload in\nFigure 8 is visible in the top-left sub-plot in Figure 9 as three rela-\ntively stable curves with few spikes. In contrast, the high perfor-\nmance variability observed for the Farm and TNT workloads is\nvisible in the top-right and bottom-left sub-plots as jittery curves.\nThe Farm workload depicted in the top-right shows curves which\nchange value at high frequency, resulting in high ISR. PaperMC’s\ntick durations are frequently below the 50 ms threshold, resulting\nin lower ISR. The TNT workload depicted in the bottom-left shows\ncurves which change value at a much lower frequency, but reach\nsignificantly higher values, exceeding 2500 ms for both Minecraft\nand Forge. Similar to the Farm workload, PaperMC’s tick durations\nare often below 50 ms, resulting in lower ISR.\n5.4\nMF3: MLGs exhibit increased variability in\ncommercial cloud environments\nIn our experiments, all MLGs show increased performance vari-\nability in terms of both variability (i.e. ISR) and tick times, when\nrun on the AWS and Azure cloud environments, compared to the\nself-hosted DAS-5. Figure 10 shows ISR and tick time distribution\nacross 50 iterations of the Player workload (see §3.4.1) of all three\n11\n0\n50\n100\n150\n0\n0.05\n0.1\n0.15\nDAS5\nAzure\nAWS\nMean\nTick time [ms]\nInstability Ratio\nPaperMC\nPaperMC\nForge\nForge\nMinecraft\nMinecraft\nFigure 10: Distribution of tick times and ISR from 50 itera-\ntions of the Players workload. Whiskers extend to ± 1.5 ×\nIQR, bounded by the minimum and maximum values.\ngames (on the vertical axis) in DAS-5 (green), Azure (blue), and\nAWS (red).\nThe results show that all three games are the most stable, with\nthe lowest median ISR (line inside boxes) and the lowest ISR overall,\nwhen run on DAS-5. The maximum ISR observed on the DAS-5\nis 0.021 (Forge), which is smaller than 0.029, the minimum ISR\nobserved in AWS and Azure (PaperMC). Distribution of tick time\nfollows a similar trend, with each game exhibiting lowest mean and\nmedian tick time on the DAS-5, as well as the smallest interquartile\nrange (IQR).\nFrom this result, we highlight two surprising observations. First,\nno game performs best in all environments. On DAS-5, PaperMC\nperforms best, slightly outperforming Minecraft with a median\nISR of 0.007 and 0.010 respectively. Although PaperMC also has\nthe lowest median ISR on Azure, it simultaneously has the highest\nIQR of both ISR, 0.028 compared to Forge’s 0.009 and Minecraft’s\n0.011, and tick time, 40.75 to Forge’s 23.25 and Minecraft’s 26.71.\nMoreover, on AWS, PaperMC is the worst performing game, with a\nmedian ISR of 0.094 and a median tick time of 48.98. Second, neither\ncloud performs best for all games. While AWS performs better for\nMinecraft and Forge, Azure performs best for PaperMC.\nIncreased performance variability in commercial cloud environ-\nments is a well-documented phenomenon [32, 56, 71, 75], with a\nwide variety of sources identified for the cause of increased variabil-\nity, including hardware manufacturing differences, shared tenancy\nof hardware and networks, specific software configurations, and\nresource allocation and scheduling systems. With so many variables\noperating in the context of commercial cloud hosting, it is infeasible\nto identify a single source responsible for the variability of these\ngames, especially since commercial cloud hosting companies do\nnot make internal data on resource allocation and shared tenancy\npublicly available. However, we can conclude that this variability\nobservably impacts the performance of MLGs, and can be compared\nbetween MLGs and commercial cloud services.\n0\n20\n40\n60\n80\n100\nPaperMC\nForge\nMinecraft\nPaperMC\nForge\nMinecraft\nPaperMC\nForge\nMinecraft\nOperation:\nBlock Add\/Remove\nBlock Update\nEntities\nWait After\nWait Before\nOther\nTick time [%]\nTNT\nFarm\nControl\nFigure 11: Distribution of tick duration attributed to specific\nMLG features on AWS environment. “Other” is the unspeci-\nfied category.\n5.5\nMF4: Processing entity-state is\ncomputationally expensive\nEntity workload components account for a large majority of com-\nputation time and state update messages.\nFigure 11 shows that entity related workload components con-\ntribute to a majority of non-waiting tick computation time. After\nentities, the next most time intensive component is environment\nrule processing, and then block creation or destruction.\nThe horizontal axis is percentage of tick computation time. The\nvertical axis on the left is MLG and Workload on the right. The color\nof each bar indicates workload operation, and the width of each\nbar corresponds to their share of tick computation time throughout\nthe experimental duration on the AWS environment.\nEntities account for a majority of non-waiting tick time during\nevery workload on each server. Most notably, PaperMC has a much\nsmaller proportion of entity calculation time under each workload\ncompared to Minecraft and Forge. During the TNT workload, in\nwhich Minecraft and Forge show large percentages of entity tick\ncomputation time compared to both the Control and Farm work-\nload, PaperMC has only a small increase. This reduction in Entity\ncomputation may explain how PaperMC manages its comparatively\nhigh performance during the TNT workload as seen in MF1.\nTable 8 shows that entity-related state updates account for a\nmajority of messages sent to the client from the server in all config-\nurations except PaperMC running the Farm workload. Conversely,\nentity-related state updates account for only a small percent of\nnetwork bytes sent.\nThus, we find that efficient computation and dissemination of\nentity state is a crucial performance challenge to MLGs. Unlike\n12\nTable 8: Percentage of network messages sent during exper-\niments on AWS that are related to entities. Computation\nrefers to percentage of messages sent and communication\nrefers to percentage of bytes sent.\nServer\nWorkload\nComputation\nCommunication\nControl\n97.5\n3.8\nMinecraft\nFarm\n91.7\n17.4\nTNT\n97.0\n9.8\nControl\n97.2\n3.2\nForge\nFarm\n86.7\n9.7\nTNT\n97.1\n10.3\nControl\n89.1\n1.3\nPaperMC\nFarm\n47.5\n1.2\nTNT\n94.8\n3.5\n0\n50\n100\n150\n0.00\n0.05\n0.10\n0.15\nNode Size:\n2XL\nXL\nL\nTick time [ms]\nInstability Ratio\nPaperMC\nForge\nMinecraft\nOverloaded Threshold\nFigure 12: Tick time distribution and ISR during TNT work-\nload on various AWS node sizes. Whiskers extend to ± 1.5 ×\nIQR, bounded by the minimum and maximum values. Black\ndiamond indicates arithmetic mean.\nenvironment processing, which exhibits spacial locality; entities\nrequire computation and take actions regardless of proximity to\nplayers, terrain or other entities.\n5.6\nMF5: Using recommended hardware results\nin significant performance variability\nRecommended hardware configurations in cloud environments\nresult in unacceptable levels of performance variability, which de-\ngrades player experience. By using more powerful cloud hardware,\nperformance variability can be limited to acceptable levels. Fig-\nure 12 shows this result, showing both the mean tick duration and\nISR for varying VM sizes in AWS. We use the notation 2XL, XL, L to\ndenote AWS VM sizes t3.large, t3.xlarge, and t3.2xlarge respectively.\nCompanies that specialize in cloud hosting of MLGs commonly\nlist recommended hardware configurations, with the most frequent\nrecommendation being 2 vCPUs and 4 GB memory. An overview of\nthese recommendations is available in Table 7. These recommended\nvalues are significantly lower than those listed on the community-\ndriven Minecraft wiki, which recommends a dedicated full CPU\n(e.g., Intel i5 or i7, or AMD Ryzen 5 or 7) and 6 GB memory [25].\nThis indicates that players experienced performance problems with\nthe recommended hardware configuration.\nFigure 12 shows that using the recommended hardware config-\nuration as listed by cloud-hosting companies, which corresponds\nto the L node type, results in poor performance and significant\nperformance variability. On this node size, each MLG becomes sig-\nnificantly overloaded by environment-based workloads and exhibits\nhigh performance variability.\nThe larger node types XL and 2XL have 4 and 8 vCPUs respec-\ntively [3]. While XL provides better performance and less perfor-\nmance variability than L, it remains insufficient to keep the mean\ntick time below 50 ms. The 2XL node type is required to provide suffi-\nciently low mean tick duration. However, this node type still shows\nsignificant performance variability for Minecraft (green cross) and\nForge (blue square), which means these games can still become\noverloaded temporarily, as shown in §5.3.\nInterestingly, we observe that the benefit of more powerful hard-\nware varies per MLG. Specifically, while PaperMC’s (red circle)\nperformance instability (i.e., ISR) increases significantly when de-\ncreasing hardware resources, from 0.025 to 0.08 in the top-right\nsub-plot, it is the only game whose mean and 75th percentile tick du-\nration stays well below the 50 ms threshold. Further analysis shows\nthat, while PaperMC becomes overloaded and its tick duration\nexceeds 50 ms, the number of extreme outliers is low, preventing\nthis performance problem from becoming visible in the mean tick\nduration.\n6\nACTIONABLE INSIGHTS AND\nLIMITATIONS\nThe main findings in §5 lead to actionable insights:\nI1 Game developers and hardware producers should report perfor-\nmance variability when evaluating the performance of online\ngames, using measures of variance such as Instability Ratio (see\n§3.5) and the distribution of game response time and frames\nper second (FPS). Games must provide consistently good per-\nformance to their users. Our experiments show that MLGs can\nbe overloaded and become unplayable, even when mean and\nmedian performance values indicate good performance (MF1,\nFigure 7).\nI2 Game developers and hardware producers should include envi-\nronment based workloads in their benchmarks for MLGs. It is not\nsufficient to evaluate the performance of MLGs using only large\nnumbers of players (i.e., player-based workloads). Environment-\nbased workloads cause significant performance variability in\nMLGs and make them unplayable (MF2, Figure 8), and must\ntherefore be included in MLG benchmarks.\nI3 Players should choose their cloud environment depending on\ntheir MLG, and should consider self-hosting their game. Our\nresults indicate that choice of best cloud provider depends on the\nMLG. Minecraft and Forge obtain lower performance variability\non AWS, while PaperMC obtains lower performance variability\non Azure (MF3, Figure 10). Moreover, self-hosting remains a\nvaluable alternative, resulting in significantly lower performance\nvariability overall.\nI4 MLG service providers should increase their hardware recom-\nmendations. Prior work has shown that when asked to estimate\nin advance the hardware requirements of a given program, users\neither pick a provided default configuration, or overestimate\nto an extreme value to avoid performance issues [30, 52, 78].\nWe find that recommended hardware configurations for hosting\n13\nMLGs on cloud environments are insufficient (MF5) and con-\nclude that users who employ the first strategy will experience\ndecreased quality of service which may cause them to switch to\na competing commercial cloud provider.\nTo prevent this, commercial cloud providers should update hard-\nware recommendations in line with our findings in Figure 12: a\n2-core size is insufficient, and a node with 8 cores is necessary for\nsmooth operation. The 4-core size provides a balance of cost and\nperformance. Beyond these recommendations, commercial cloud\nproviders should use our benchmark to determine adequate hard-\nware allocations capable of fulfilling the service requirements\nof MLGs under realistic workloads, and further adapt resource\nscheduling to be aware of the performance patterns of MLGs.\nSimilarly, users who seek to avoid adverse performance vari-\nability when operating MLG cloud environments should choose\nnode sizes comparable to the 8 core t3.2xlarge node, or use our\nbenchmark to compare both various cloud providers and the\nspecific MLG implementations.\nI5 Game developers should engineer MLGs to reduce impact of\nenvironment-based workloads. Engineering for this goal can\nreduce the impact (i.e., performance variability) of environment-\nbased workloads by 60% on the same hardware, as shown by\nPaperMC operating the Farm workload on AWS (Figure 8). The\ngoal of the PaperMC project is to implement a high performance\nMLG, including efficient processing of environment-based work-\nloads. We provide an analysis of PaperMC in Appendix A. De-\nvelopers creating MLGs should consider and mitigate the impact\nof environment-based workloads, and use our benchmark to\nmeasure, analyze and subsequently reduce the performance vari-\nability caused by such workloads.\nHere we discuss limitations of our work related to the Instability\nRatio metric and the workloads. ISR cannot be used as a singular\nperformance metric, but rather is designed as an additional axis by\nwhich to quantitatively appraise the performance of a game server,\nby capturing behavior that other metrics cannot. Because ISR is a\nmeasure of variability over an entire sampling duration, it does not\ncapture extremely poor but stable performance, or the occurrence\nof singular relatively small outliers. Thus, other measures, such as\npercentiles, are necessary to observe the magnitude of tick durations\nand detect lone outliers. It is not currently understood how ISR\ndirectly relates to player-perceived quality of experience and quality\nof experience, and should be explored in future work, for instance\nthrough player studies.\nThe workloads included in Meterstick are intended to cover a\nwide range of realistic environment-based workloads, from com-\nmon to extreme cases. However, there is no publicly available anal-\nysis of environment-based workload prevalence. Thus, we select\nenvironment-based workloads using proxy metrics such as total\nviews and downloads in online MLG communities, which may not\nbe representative of all MLG players. Additionally, our experiments\nmeasuring the impact of environment-based workloads utilized a\npurposefully minimal player-based workload component. Finally,\nbecause our experiments focused on environment-based workloads,\nour player-based workloads (“Players\") uses random avatar move-\nment. Although real player behavior is likely more complex, no\nplayer-behavior models exists for MLGs.\n7\nRELATED WORK\nWe summarize in this section a developing overview of related work.\nOverall, this study is the first to evaluate performance variability\nin MLGs. This is challenging because there is neither a generally\naccepted set of relevant workloads for MLGs, nor a standardized\nmetric to quantify performance variability in computer systems.\nClosest to our work, Yardstick is an MLG benchmark used to\nshow the limited scalability of MLGs [76]. The authors use Yardstick\nto evaluate the scalability and network characteristics of several\nMLG services. However, Yardstick does not quantify performance\nvariability, resulting in optimistic results. Moreover, the authors do\nnot evaluate MLG performance under environment-based work-\nloads or in the cloud.\nThe MineRL competition [48] provides a dataset of Minecraft\nplayer recordings. This dataset provides demonstrations to train ar-\ntificial intelligence systems to complete a challenging in-game task.\nIn contrast, the workloads used in this work focus on commonly\nobserved patterns in the MLG community.\nThere exist several systems that aim to improve the scalability of\nMLGs. Manycraft [37] increases the maximum number of players in\na Minecraft instance by using Kiwano. Kiwano [36] allows horizon-\ntal scaling of virtual environments through Voronoi partitioning,\nbut requires a static environment, which disables the MLG’s mod-\nifiable world and is incompatible with environment-based work-\nloads. Similar in many ways to Manycraft, Koekepan [44] uses\nzone-partitioning and scales horizontally. Dyconits [39] are a dis-\ntributed architecture that scales MLGs vertically, through the use of\ndynamically managed consistency-units. None of these approaches\nconsiders explicitly performance variability.\nIosup et al. [49] find that commercial cloud environments exhibit\nsignificant yearly and daily performance variability patterns. The\nauthors show that performance variability varies per cloud operator,\nand use simulation experiments to show that this can affect the\nperformance of applications, including a social online game. In\ncontrast, our benchmark uses real-world experiments to evaluate\nthe effect of performance variability on MLGs, which are real-time\nonline games.\n8\nCONCLUSION\nOnline gaming is a popular and lucrative part of the entertainment\nindustry, but raises important performance challenges. In this work,\nwe posit performance variability is an important cause for the lack\nof scalability in MLGs.\nWe make a four-fold contribution to better understand the be-\nhavior of MLGs. First, we propose a novel workload model for these\nsystems, which identifies important sources of performance vari-\nability not considered elsewhere. Second, we design and implement\nMeterstick, the first benchmark to evaluate performance variabil-\nity in MLGs. Meterstick uses realistic workload types; novel, it\nconsiders environment-based workloads, and can evaluate MLGs\nrunning both in self-hosted and cloud environments such as Ama-\nzon AWS and Microsoft Azure. Third, we use Meterstick to perform\nreal-world experiments and analyze the results. We find that per-\nformance variability negatively affects players in MLGs, that both\nenvironment-based workloads and cloud environments can cause\nsignificant performance variability. This leads us to formulate four\n14\nactionable insights. Fourth, we release FAIR and FOSS artifacts that\nenable reproducibility for this work.\nIn future work, we aim to conduct user studies to directly link\nour Instability Ratio (ISR) values to player-perceived quality of ex-\nperience. To encourage community adoption, we aim to create a\npublic score-board where operators of MLG-as-a-service can pub-\nlish benchmark scores.\nREFERENCES\n[1] 2020. Minecraft Server Download. https:\/\/www.minecraft.net\/en-us\/download\/\nserver [accessed Oct. 2021].\n[2] 2021. Akliz. https:\/\/www.akliz.net\/pricing [accessed Dec. 2021].\n[3] 2021. Amazon EC2 T3 Instances – Amazon Web Services (AWS). https:\/\/aws.\namazon.com\/ec2\/instance-types\/t3 [accessed Oct. 2021].\n[4] 2021. Apex Minecraft Hosting. https:\/\/apexminecrafthosting.com\/ [accessed\nDec. 2021].\n[5] 2021. Aquatis. https:\/\/aquatis.host\/minecraft-hosting\/ [accessed Dec. 2021].\n[6] 2021. Bisect Hosting. https:\/\/www.bisecthosting.com\/ [accessed Dec. 2021].\n[7] 2021. CubedHost. https:\/\/cubedhost.com\/ [accessed Dec. 2021].\n[8] 2021. Ferox Hosting. https:\/\/feroxhosting.nl\/ [accessed Dec. 2021].\n[9] 2021. Ggservers. https:\/\/ggservers.com\/ [accessed Dec. 2021].\n[10] 2021. GTXGaming. https:\/\/www.gtxgaming.co.uk\/ [accessed Dec. 2021].\n[11] 2021. Host Havoc. https:\/\/hosthavoc.com\/minecraft [accessed Dec. 2021].\n[12] 2021. Hostinger. https:\/\/www.hostinger.com\/minecraft-server-hosting [accessed\nDec. 2021].\n[13] 2021. MCProHosting. https:\/\/mcprohosting.com\/ [accessed Dec. 2021].\n[14] 2021. Meloncube. https:\/\/www.meloncube.net\/ [accessed Dec. 2021].\n[15] 2021. Minecraft Forge downloads. https:\/\/files.minecraftforge.net\/net\/ [accessed\nOct. 2021].\n[16] 2021. Minecraft Realms for Java. https:\/\/www.minecraft.net\/en-us\/realms-for-\njava [accessed Oct. 2021].\n[17] 2021. Mods - Minecraft - CurseForge. https:\/\/www.curseforge.com\/minecraft\/mc-\nmods [accessed Oct. 2021].\n[18] 2021. Nodecraft. https:\/\/nodecraft.com [accessed Dec. 2021].\n[19] 2021. PaperMC – The High Performance Fork.\nhttps:\/\/papermc.io [accessed\nOct. 2021].\n[20] 2021. Pebble Host. https:\/\/pebblehost.com\/ [accessed Dec. 2021].\n[21] 2021. Ramshard. https:\/\/ramshard.com\/hosting\/minecraft [accessed Dec. 2021].\n[22] 2021. Scalacube. https:\/\/scalacube.com [accessed Dec. 2021].\n[23] 2021. Server Miner. https:\/\/serverminer.com\/buy-minecraft-server [accessed\nDec. 2021].\n[24] 2021. Server.pro. https:\/\/server.pro\/ [accessed Dec. 2021].\n[25] 2021. Server\/Requirements\/Dedicated.\nhttps:\/\/minecraft.fandom.com\/wiki\/\nServer\/Requirements\/Dedicated [accessed Oct. 2021].\n[26] 2021. Shockbyte. https:\/\/shockbyte.com\/ [accessed Dec. 2021].\n[27] 2021. Skynode. https:\/\/www.skynode.pro\/ [accessed Dec. 2021].\n[28] 2021. Sticky Piston. https:\/\/stickypiston.co\/ [accessed Dec. 2021].\n[29] Amazon. 2021. Run your own Minecraft Server. https:\/\/web.archive.org\/web\/\n20201126074839\/https:\/\/aws.amazon.com\/getting-started\/hands-on\/run-your-\nown-minecraft-server\/ [accessed Dec. 2021].\n[30] Cynthia Bailey Lee. 2009. On the user-scheduler relationship in high-performance\ncomputing. UC San Diego.\n[31] Henri E. Bal, Dick H. J. Epema, Cees de Laat, Rob van Nieuwpoort, John W.\nRomein, Frank J. Seinstra, Cees Snoek, and Harry A. G. Wijshoff. 2016. A Medium-\nScale Distributed System for Computer Science Research: Infrastructure for the\nLong Term. Computer 49, 5 (2016), 54–63.\n[32] Zhen Cao, Vasily Tarasov, Hari Prasath Raman, Dean Hildebrand, and Erez Zadok.\n2017. On the Performance Variation in Modern Storage Stacks. In FAST. 329–344.\n[33] Kuan-Ta Chen, Polly Huang, and Chin-Laung Lei. 2006. How sensitive are online\ngamers to network quality? Commun. ACM 49, 11 (2006), 34–38.\n[34] Mark Claypool and Kajal T. Claypool. 2006. Latency and player actions in online\ngames. Commun. ACM 49, 11 (2006), 40–45.\n[35] N. Da Dalt and A. Sheikholeslami. 2018. Understanding Jitter and Phase Noise.\nCambridge University Press. 15–37 pages.\n[36] Raluca Diaconu and Joaquín Keller. 2013. Kiwano: A scalable distributed infras-\ntructure for virtual worlds. In HPCS. 664–667.\n[37] Raluca Diaconu, Joaquín Keller, and Mathieu Valero. 2013. Manycraft: Scaling\nMinecraft to Millions. In NetGames. 1:1–1:6.\n[38] Matthias Dick, Oliver Wellnitz, and Lars C. Wolf. 2005. Analysis of factors\naffecting players’ performance and perception in multiplayer games. In NetGames.\n1–7.\n[39] Jesse Donkervliet, Jim Cuijpers, and Alexandru Iosup. 2021. Dyconits: Scaling\nMinecraft-like Services through Dynamically Managed Inconsistency. In ICDCS.\n[40] Jesse Donkervliet, Animesh Trivedi, and Alexandru Iosup. 2020. Towards Sup-\nporting Millions of Users in Modifiable Virtual Environments by Redesigning\nMinecraft-Like Games as Serverless Systems. In HotCloud.\n[41] DudeItsRocky. 2018. \"Huge Minecraft Tnt World Explosion With Aftermath\"\n(Minecraft TNT Explosion, Minecraft Explosion). https:\/\/www.youtube.com\/\nwatch?v=9ZbenrBO_wI [accessed Oct. 2021].\n[42] Jerrit Eickhoff. 2021. Meterstick. https:\/\/github.com\/JerritEic\/Meterstick\n[43] Jerrit Eickhoff, Jesse Donkervliet, and Alexandru Iosup. 2021. Meterstick Bench-\nmark: Source, Documentation and Data. https:\/\/doi.org\/10.5281\/zenodo.5567720\n[44] Herman Arnold Engelbrecht and Gregor Schiele. 2013. Koekepan: Minecraft as a\nResearch Platform. In NetGames. 16:1–16:3.\n[45] Epic Games. 2021.\nDynamic Resolution | Unreal Engine Documenta-\ntion.\nhttps:\/\/docs.unrealengine.com\/4.27\/en-US\/RenderingAndGraphics\/\nDynamicResolution\/ [accessed Dec. 2021].\n[46] Valentin Forch, Thomas Franke, Nadine Rauh, and Josef F. Krems. 2017. Are 100\nms Fast Enough? Characterizing Latency Perception Thresholds in Mouse-Based\nInteraction. In EPCE, Vol. 10276. 45–56.\n[47] gnembon. 2017. Simple Hostile Mob Farms, Minecraft 1.12 - 1.16+ (Fun Farms Ep.\n15). https:\/\/www.youtube.com\/watch?v=guvMmdeZqiI [accessed Oct. 2021].\n[48] William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden\nCodel, Manuela Veloso, and Ruslan Salakhutdinov. 2019. MineRL: A Large-Scale\nDataset of Minecraft Demonstrations. CoRR abs\/1907.13440 (2019).\n[49] Alexandru Iosup, Nezih Yigitbasi, and Dick H. J. Epema. 2011. On the Performance\nVariability of Production Cloud Services. In CCGrid. 104–113.\n[50] Raj Jain. 1991. The art of computer systems performance analysis - techniques for\nexperimental design, measurement, simulation, and modeling. Wiley.\n[51] Jrudolph. 2020. perf-map-agent. https:\/\/github.com\/jvm-profiling-tools\/perf-\nmap-agent Accessed: 2021-02-10.\n[52] Cynthia Bailey Lee, Yael Schwartzman, Jennifer Hardy, and Allan Snavely. 2004.\nAre User Runtime Estimates Inherently Inaccurate?. In JSSPP. 253–263.\n[53] David Lee. 2002. Analysis of Jitter in Phase-Locked Loops. IEEE Transactions on\nCircuits and Systems II: Analog and Digital Signal Processing 49 (2002).\n[54] legomasta99. 2018. [Minecraft Computer Engineering] - Quad-Core Redstone\nComputer v5.0 [12k sub special!].\nhttps:\/\/www.youtube.com\/watch?v=\nSbO0tqH8f5I [accessed Oct. 2021].\n[55] Asher Madan. 2019. Minecraft (likely) becomes the best-selling game of all time\non its 10th birthday.\nhttps:\/\/www.windowscentral.com\/minecraft-becomes-\nbest-selling-game-all-time-its-10th-birthday [accessed Sep. 2021].\n[56] Aleksander Maricq, Dmitry Duplyakin, Ivo Jimenez, Carlos Maltzahn, Ryan\nStutsman, Robert Ricci, and Ana Klimovic. 2018. Taming Performance Variability.\nIn OSDI. 409–425.\n[57] Microsoft. 2021.\nBasic Game Server Hosting on Azure.\nhttps:\n\/\/docs.microsoft.com\/en-us\/gaming\/azure\/reference-architectures\/multiplayer-\nbasic-game-server-hosting [accessed Dec. 2021].\n[58] Minetrack. 2021. Minetrack. https:\/\/minetrack.me\/ [accessed Oct. 2021].\n[59] Mumbo Jumbo. 2020. Hermitcraft 7: Episode 52 - QUAD INDUSTRIAL FARM.\nhttps:\/\/www.youtube.com\/watch?v=y87rPUCkjrQ [accessed Oct. 2021].\n[60] Mysticat. 2020. Minecraft Item Sorting System: Easy & Expandable Tutorial 1.16.\nhttps:\/\/www.youtube.com\/watch?v=zQhG9hn7lME [accessed Oct. 2021].\n[61] Vlad Nae, Alexandru Iosup, and Radu Prodan. 2011. Dynamic Resource Provi-\nsioning in Massively Multiplayer Online Games. TPDS 22, 3 (2011), 380–395.\n[62] Newzoo. 2021. Newzoo Global Games Market Report 2021 | Free Version |\nNewzoo.\nhttps:\/\/newzoo.com\/insights\/trend-reports\/newzoo-global-games-\nmarket-report-2021-free-version [accessed Oct. 2021].\n[63] Aline Normoyle, Gina Guerrero, and Sophie Jörg. 2014. Player perception of de-\nlays and jitter in character responsiveness. In Proceedings of the ACM Symposium\non Applied Perception. 117–124.\n[64] Felix Richter. 2021.\nInfographic: Amazon Leads $150-Billion Cloud Mar-\nket. https:\/\/www.statista.com\/chart\/18819\/worldwide-market-share-of-leading-\ncloud-infrastructure-service-providers [accessed Oct. 2021].\n[65] Felix Richter. 2021. Infographic: Gaming: The Most Lucrative Entertainment\nIndustry By Far.\nhttps:\/\/www.statista.com\/chart\/22392\/global-revenue-of-\nselected-entertainment-industry-sectors [accessed Sep. 2021].\n[66] Michal Ries, Philipp Svoboda, and Markus Rupp. 2008. Empirical study of subjec-\ntive quality for massive multiplayer games. In 2008 15th International Conference\non Systems, Signals and Image Processing. IEEE, 181–184.\n[67] William J Riley. 2008. Handbook of frequency stability analysis. (2008).\n[68] Henning Schulzrinne, Steven Casner, R Frederick, and Van Jacobson. 2003.\nRFC3550: RTP: A transport protocol for real-time applications.\n[69] Shulkercraft. 2020. Minecraft Fully Automatic Cobblestone Farm - 120,000 Cobble\nPer Hour - 1.16\/1.15.\nhttps:\/\/www.youtube.com\/watch?v=azGc3cWkTAM\n[accessed Oct. 2021].\n[70] Spigot. 2020. SpigotMC. https:\/\/www.spigotmc.org\/ Accessed: 2021-02-10.\n[71] Javid Taheri, Albert Y. Zomaya, and Andreas Kassler. 2017. vmBBProfiler: a\nblack-box profiling approach to quantify sensitivity of virtual machines to shared\ncloud resources. Computing 99, 12 (2017), 1149–1177.\n[72] PaperMC Team. 2020. PaperMC\/Paper\/Spigot-Server-Patches. https:\/\/github.\ncom\/PaperMC\/Paper\/tree\/master\/Spigot-Server-Patches Accessed: 2021-02-10.\n[73] timeam. 2020. The Most Efficient Minecraft Lag Machine. https:\/\/www.youtube.\ncom\/watch?v=QI0zdI4mDcA [accessed Oct. 2021].\n15\n[74] Unity. 2021. Unity - Manual: Dynamic resolution.\nhttps:\/\/docs.unity3d.com\/\nManual\/DynamicResolution.html [accessed Feb. 2023].\n[75] Alexandru Uta, Alexandru Custura, Dmitry Duplyakin, Ivo Jimenez, Jan S. Reller-\nmeyer, Carlos Maltzahn, Robert Ricci, and Alexandru Iosup. 2020. Is Big Data\nPerformance Reproducible in Modern Cloud Networks?. In NSDI. 513–527.\n[76] Jerom van der Sar, Jesse Donkervliet, and Alexandru Iosup. 2019. Yardstick: A\nBenchmark for Minecraft-like Services. In ICPE. ACM, 243–253.\n[77] Reinhold Weicker. 2002. Benchmarking. In Performance Evaluation of Complex\nSystems: Techniques and Tools, Performance 2002, Tutorial Lectures, Vol. 2459.\n179–207.\n[78] Ahuva Mu’alem Weil and Dror G. Feitelson. 2001. Utilization, Predictability,\nWorkloads, and User Runtime Estimates in Scheduling the IBM SP2 with Back-\nfilling. TPDS 12, 6 (2001), 529–543.\n[79] Wilkinson et al. 2016. The FAIR Guiding Principles for scientific data management\nand stewardship. Nature SciData 3 (2016).\nA\nTHE PAPERMC PROJECT\nThroughout all experiments it is seen that PaperMC performs better\nthan either Minecraft or Forge. This merits an explanation of the\ndifferences between PaperMC and the other servers, such to drive\nfuture research into how to minimize the impact of environment-\nbased workloads through performance engineering.\nMuch like Forge, the PaperMC project [19] is an open source\nset of community ’patches’ applied on top of another server type\n(Spigot [70]) which in turn were modifications made to the Mojang-\nprovided Minecraft server. Unlike Forge, PaperMC is not made\nin order to facilitate mods but instead to deal with a plethora of\ncommunity grievances about server experience. As such, PaperMC\nat its core is a set of code patches and a repackaging of the server.\nThe set of patches is outlined in their Github page [72], with the\nmost important being asynchronous threads and reworked thread\npriorities, limited per-thread cache duplication, a built-from scratch\nscheduler, custom player defined logic algorithms, and importantly,\nan entirely new entity logic handler (as is found in MF4, entities\nare computationally expensive).\nFor how these changes improve performance, we compare the\nCPU flamegraph of the Vanilla and PaperMC server during the\nTNT workload, collected using perf-map-agent [51]. The flame-\ngraph of Forge is omitted as it is (in this case) identical to Vanilla’s.\nFlamegraphs are read as follows: the y-axis shows stack depth, the\nwidth of each box shows the total (not necessarily consecutive) time\na given thread was on CPU. The ordering of boxes on the x-axis is\narbitrary, in this case they are in alphabetical order.\nFigures 13 and 14 show that PaperMC hoists a number of envi-\nronment operations out of the main thread into dedicated threads\nhandled by a separate asynchronous scheduler thread. Thus the\namount of time spent on worker locks is decreased, minimizing total\nresource contention. It can be extrapolated that these changes con-\ntribute strongly to PaperMC remaining less overloaded compared\nto Vanilla and Forge during the TNT workload.\n16\nFigure 13: Minecraft flamegraph during TNT workload.\n17\nFigure 14: PaperMC flamegraph during TNT workload.\n18\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Meterstick: Benchmarking Performance Variability in Cloud and Self-hosted Minecraft-like Games Extended Technical Report.pdf"}
{"title":"Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game","authors":"Alexander G. Reisach, Christof Seiler, Sebastian Weichwald","summary":"Simulated DAG models may exhibit properties that, perhaps inadvertently,\nrender their structure identifiable and unexpectedly affect structure learning\nalgorithms. Here, we show that marginal variance tends to increase along the\ncausal order for generically sampled additive noise models. We introduce\nvarsortability as a measure of the agreement between the order of increasing\nmarginal variance and the causal order. For commonly sampled graphs and model\nparameters, we show that the remarkable performance of some continuous\nstructure learning algorithms can be explained by high varsortability and\nmatched by a simple baseline method. Yet, this performance may not transfer to\nreal-world data where varsortability may be moderate or dependent on the choice\nof measurement scales. On standardized data, the same algorithms fail to\nidentify the ground-truth DAG or its Markov equivalence class. While\nstandardization removes the pattern in marginal variance, we show that data\ngenerating processes that incur high varsortability also leave a distinct\ncovariance pattern that may be exploited even after standardization. Our\nfindings challenge the significance of generic benchmarks with independently\ndrawn parameters. The code is available at\nhttps:\/\/github.com\/Scriddie\/Varsortability.","url":"http:\/\/arxiv.org\/abs\/2102.13647v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2102.13647v3","published":1614365547000,"comment":"accepted at the 35th Conference on Neural Information Processing\n  Systems (NeurIPS), 2021; cf. https:\/\/openreview.net\/forum?id=wEOlVzVhMW_","pdf_text":"Beware of the Simulated DAG!\nCausal Discovery Benchmarks May Be Easy To Game\nAlexander G. Reisach1,2\nChristof Seiler2,3\nSebastian Weichwald1\n1Department of Mathematical Sciences, University of Copenhagen, Denmark\n2Department of Data Science and Knowledge Engineering, Maastricht University, The Netherlands\n3Mathematics Centre Maastricht, Maastricht University, The Netherlands\nAbstract\nSimulated DAG models may exhibit properties that, perhaps inadvertently, render\ntheir structure identiﬁable and unexpectedly affect structure learning algorithms.\nHere, we show that marginal variance tends to increase along the causal order\nfor generically sampled additive noise models. We introduce varsortability as a\nmeasure of the agreement between the order of increasing marginal variance and\nthe causal order. For commonly sampled graphs and model parameters, we show\nthat the remarkable performance of some continuous structure learning algorithms\ncan be explained by high varsortability and matched by a simple baseline method.\nYet, this performance may not transfer to real-world data where varsortability may\nbe moderate or dependent on the choice of measurement scales. On standardized\ndata, the same algorithms fail to identify the ground-truth DAG or its Markov\nequivalence class. While standardization removes the pattern in marginal variance,\nwe show that data generating processes that incur high varsortability also leave a\ndistinct covariance pattern that may be exploited even after standardization. Our\nﬁndings challenge the signiﬁcance of generic benchmarks with independently\ndrawn parameters. The code is available at https:\/\/github.com\/Scriddie\/\nVarsortability.\n1\nIntroduction\nCausal structure learning\naims to infer a causal model from data. Academic disciplines anywhere\nfrom biology, medicine, ﬁnance, to machine learning are interested in causal models [Rothman et al.,\n2008, Imbens and Rubin, 2015, Sanford and Moosa, 2012, Schölkopf, 2019]. Causal models not only\ndescribe the observational joint distribution of variables but also formalize predictions under inter-\nventions and counterfactuals [Spirtes et al., 2000, Pearl, 2009, Peters et al., 2017]. Directed acyclic\ngraphs (DAGs) are common to represent causal structure: nodes represent variables and directed\nedges point from cause to effect representing the causal relationships. This graphical representation\nrests on assumptions which have been critically questioned, for example by Dawid [2010]. Inferring\ncausal structure from observational data is difﬁcult: Often we can only identify the DAG up to its\nMarkov equivalence class (MEC) and ﬁnding high-scoring DAGs is NP-hard [Chickering, 1996,\nChickering et al., 2004]. Here, we focus on learning the DAG of linear additive noise models (ANM).\nData scale and marginal variance\nmay carry information about the data generating process. This\ninformation can dominate benchmarking results, such as, for example, the outcome of the NeurIPS\nCausality 4 Climate competition [Runge et al., 2020]. Here, the magnitude of regression coefﬁcients\nwas informative about the existence of causal links such that ordinary regression-based methods on\nraw data outperformed causal discovery algorithms [Weichwald et al., 2020]. Multiple prior works\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2102.13647v3  [stat.ML]  10 Nov 2021\nstate the importance of data scale for structure learning either implicitly or explicitly. Structure\nidentiﬁcation by ICA-LiNGAM [Shimizu et al., 2006], for example, is susceptible to rescaling of the\nvariables. This motivated the development of DirectLiNGAM [Shimizu et al., 2011], a scale-invariant\ncausal discovery algorithm for linear non-Gaussian models. The causal structure of ANMs is proven\nto be identiﬁable given the noise scale (cf. Section 2.2). Yet, such identiﬁability results require\nknowledge about the ground-truth data scale.\nSimulated DAGs\nmay be identiﬁable from marginal variances under generic parameter distribu-\ntions. An instructive example is the causal graph A →B with structural equations A = NA and\nB = wA + NB with w ̸= 0 and independent zero-centered noise variables NA, NB. The mean\nsquared error (MSE) of a model X →Y is given by MSE (X →Y ) = Var(X) + Var(Y |X). It\nholds that MSE (A →B) < MSE (B →A) ⇐⇒Var(A) < Var(B) ⇐⇒(1−w2) Var(NA) <\nVar(NB) (see Appendix A). Deciding the directionality of the edge between A and B based on the\nMSE amounts to inferring an edge from the lower-variance variable to the higher-variance variable.\nFor error variances Var(NA) ≤Var(NB) and any non-zero edge weight w, the MSE-based inference\nis correct. This resembles known scale-based identiﬁability results based on equal or monotonically\nincreasing error variances [Peters and Bühlmann, 2014, Park, 2020]. However, if the observations\nof A were multiplied by a sufﬁciently large constant, the MSE-based inference would wrongfully\nconclude that A ←B. This is problematic since simply choosing our units of measurement differ-\nently may change the scale and variance of A. Arguably, this is often the case for observations from\nreal-world systems: There is no canonical choice as to whether we should pick meters or yards for\ndistances, gram or kilogram for weights, or yuan or dollar as currency. A researcher cannot rely on\nobtaining the same results for different measurement scales or after re-scaling the data when applying\nany method that leverages the data scale (examples include Peters and Bühlmann [2014], Park [2020],\nor Zheng et al. [2018], who employ the least squares loss studied by Loh and Bühlmann [2014]).\nContinuous causal structure learning algorithms\noptimize model ﬁt under a differentiable\nacyclicity constraint [Zheng et al., 2018]. This allows for the use of continuous optimization\nand avoids the explicit combinatorial traversal of possible causal structures. This idea has found\nnumerous applications and extensions [Lachapelle et al., 2019, Lee et al., 2019, Ng et al., 2020,\nYu et al., 2019, Brouillard et al., 2020, Pamﬁl et al., 2020, Wei et al., 2020, Zheng et al., 2020,\nBhattacharya et al., 2021]; Vowels et al. [2021] provide a review. NOTEARS [Zheng et al., 2018]\nuses the MSE with reference to Loh and Bühlmann [2014], while GOLEM [Ng et al., 2020] assesses\nmodel ﬁt by the penalized likelihood assuming a jointly Gaussian model. On simulated data and\nacross noise distributions, both methods recover graphs that are remarkably close to the ground-truth\ncausal graph in structural intervention distance (SID) and structural hamming distance (SHD). We\nagree with the original authors that these empirical ﬁndings, especially under model misspeciﬁcation\nand given the non-convex loss landscape, may seem surprising at ﬁrst. Here, we investigate the\nperformance under data standardization and explain how the causal order is (partially) identiﬁable\nfrom the raw data scale alone in common generically simulated benchmarking data.\nContribution.\nWe show that causal structure drives the marginal variances of nodes in an ANM and\ncan lead to (partial) identiﬁability. The pattern in marginal variances is dominant in ANM benchmark\nsimulations with edge coefﬁcients drawn identically and independently. We introduce varsortability\nas a measure of the information the data scale carries about the causal structure. We argue that high\nvarsortability affects the optimization procedures of continuous structure learning algorithms. Our\nexperiments demonstrate that varsortability dominates the optimization and helps achieve state-of-\nthe-art performance provided the ground-truth data scale. Data standardization or an unknown data\nscale remove this information and the same algorithms fail to recover the ground-truth DAG. Even\nmethods using a score-equivalent likelihood criterion (GOLEM) recover neither ground-truth DAG\nnor its MEC on standardized data. To illustrate that recent benchmark results depend heavily on\nhigh varsortability, we provide a simple baseline method that exploits increasing marginal variances\nto achieve state-of-the-art results on these benchmarks. We thereby provide an explanation for the\nunexpected performance of recent continuous structure learning algorithms in identifying the true\nDAG. Neither algorithm dominates on raw or standardized observations of the analyzed real-world\ndata. We show how, even if data is standardized and even in non-linear ANMs, a causal discovery\nbenchmark may be gamed due to covariance patterns. Consequently, recent benchmark results may\nnot transfer to (real-world) settings where the correct data scale is unknown or where edge weights\nare not drawn independent and identically distributed (iid). We conclude that structure learning\n2\nbenchmarks on ANMs with generically sampled parameters may be distorted due to unexpected and\nperhaps unintended regularity patterns in the data.\n2\nBackground\n2.1\nModel Class\nWe consider acyclic linear additive noise models. Single observations are denoted by x(i) ∈Rd\nwhere x(i)\nj\ndenotes the jth dimension of the ith iid observation of random vector X = [X1, ..., Xd]⊤.\nAll observations are stacked as X = [x(1), ..., x(n)]⊤∈Rn×d and xj ∈Rn refers to the jth column\nof X. Analogously, n(i) denotes the corresponding ith iid observation of the random noise variable\nN = [N1, ..., Nd]⊤with independent zero-centred components. The linear effect of variable Xk\non Xj is denoted by wk→j = wkj. The causal structure corresponding to the adjacency matrix\nW = [wkj]k,j=1,...,d with columns wj = [wk→j]k=1,...,d ∈Rd can be represented by a directed\nacyclic graph G = (VG, EG) with vertices VG = {1, ..., d} and edges EG = {(k, j) : wk→j ̸= 0}.\nEdges can be represented by an adjacency matrix E such that the (k, j)th entry of El is non-zero if\nand only if a directed path of length l from k to j exists in G. For a given graph, the parents of j are\ndenoted by PA (j). The structural causal model is X = W ⊤X + N.\n2.2\nIdentiﬁability of Additive Noise Models\nIdentiﬁability of the causal structure or its MEC requires causal assumptions. Under causal faithful-\nness and Markov assumptions, the causal graph can be recovered up to its MEC [Chickering, 1995,\nSpirtes et al., 2000]. Faithfulness, however, is untestable [Zhang and Spirtes, 2008]. Shimizu et al.\n[2006] show that under the assumptions of no unobserved confounders, faithfulness, linearity, and\nnon-Gaussian additive noise, the causal graph can be recovered from data. Hoyer et al. [2009] show\nthat this holds for any noise distribution under the assumption of strict non-linearity. This ﬁnding\nis generalized to post-nonlinear functions by Zhang and Hyvarinen [2009]. Peters and Bühlmann\n[2014] prove that the causal structure of a linear causal model with Gaussian noise is identiﬁable if\nthe error variances are equal or known. Any unknown re-scaling of the data breaks this condition. For\nthe case of linear structural causal models, Loh and Bühlmann [2014] provide a framework for DAG\nestimation based on a noise variance-weighted least squares score function. For ANMs, they give\nconditions under which the general Gaussian case can be identiﬁed via approximating it by the equal\nnoise-variance case given knowledge of the (approximate) noise scale. Finally, subsuming further\nprior results on (linear) ANMs [Hoyer et al., 2009, Ghoshal and Honorio, 2017, 2018, Chen et al.,\n2019], Park [2020] shows that the causal structure is identiﬁable under regularity conditions on the\nconditional variances along the causal order. In particular, identiﬁability holds if the error variances\nof nodes are weakly monotonically increasing along the causal order.\n2.3\nStructure Learning Algorithms\nCombinatorial structure learning algorithms (such as PC, FGES, DirectLiNGAM)\nseparately\nsolve the combinatorial problem of searching over structures and ﬁnding the optimal parameters\nfor each structure. To remain computationally feasible, the search space of potential structures is\noften restricted or traversed according to a heuristic. One can, for example, carefully choose which\nconditional independence statements to evaluate in constraint-based algorithms, or employ greedy\n(equivalence class) search in score-based algorithms. In our experiments, we consider PC [Spirtes\nand Glymour, 1991], FGES [Meek, 1997, Chickering, 2002b], DirectLiNGAM [Shimizu et al., 2011],\nand a greedy DAG search (GDS) algorithm MSE-GDS that greedily includes those edges that reduce\nthe MSE the most. For details see Appendix D.\nContinuous structure learning algorithms (such as NOTEARS and GOLEM)\nemploy continu-\nous optimization to simultaneously optimize over structures and parameters. As a ﬁrst step towards\nexpressing causal structure learning as a continuous optimization problem, Aragam and Zhou [2015]\npropose l1-regularization instead of the conventional l0-penalty for model selection. Zheng et al.\n[2018] propose a differentiable acyclicity constraint, allowing for end-to-end optimization of score\nfunctions over graph adjacency matrices. We examine and compare the continuous structure learning\nalgorithms NOTEARS [Zheng et al., 2018] and GOLEM [Ng et al., 2020]. For details see Appendix D.\n3\nNOTEARS [Zheng et al., 2018] minimizes the MSE between observations and model predictions\nsubject to a hard acyclicity constraint. The MSE with respect to W on observations X is deﬁned as\nMSEX (W) = 1\nn∥X −XW∥2\n2 where ∥· ∥2 = ∥· ∥F denotes the Frobenius norm.\nGOLEM [Ng et al., 2020] performs maximum likelihood estimation (MLE) under the assumption of\na Gaussian distribution with equal (EV) or non-equal (NV) noise variances. There are soft acyclicity\nand sparsity constraints. The unnormalized negative likelihood-parts of the objective function are\nLEV (W, X) = log(MSEX (W)) and LNV (W, X) = Pd\nj=1 log\n\u0000 1\nn∥xj −Xwj∥2\n2\n\u0001\n, respectively,\nomitting a −log(| det(I −W)|) term that vanishes when W represents a DAG [Ng et al., 2020].\nTo ease notation, we sometimes drop the explicit reference to X when referring to MSE, LEV , LNV .\n3\nVarsortability\nThe data generating process may leave information about the causal order in the data scale. We\nintroduce varsortability as a measure of such information. When varsortability is maximal, the causal\norder is identiﬁable. Varsortability is high in common simulation schemes used for benchmarking\ncausal structure learning algorithms. We describe how continuous structure learning algorithms are\naffected by marginal variances and how they may leverage high varsortability. This elucidates the\nresults of continuous methods reported by Zheng et al. [2018], Ng et al. [2020], and others on raw\ndata and predicts impaired performance on standardized data as conﬁrmed in Section 4. We introduce\nsortnregress as simple baseline method that sorts variables by marginal variance followed by parent\nselection. The performance of sortnregress reﬂects the degree of varsortability in a given setting and\nestablishes a reference baseline to benchmark structure learning algorithms against.\n3.1\nDeﬁnition of Varsortability\nWe propose varsortability as a measure of agreement between the order of increasing marginal\nvariance and the causal order. For any causal model over variables {X1, ..., Xd} with (non-degenerate)\nDAG adjacency matrix E we deﬁne varsortability as the fraction of directed paths that start from a\nnode with strictly lower variance than the node they end in, that is,\nv :=\nPd−1\nk=1\nP\ni→j∈Ek increasing(Var(Xi),Var(Xj))\nPd−1\nk=1\nP\ni→j∈Ek 1\n∈[0, 1] where increasing(a, b) =\n\n\n\n1\na < b\n1\/2\na = b\n0\na > b\nFor example, we calculate the varsortability as v = 1+1+1\n1+1+1+1 = 3\n4 given the causal graph below.\nVar(A) = 2\nA\nB\nVar(B) = 1\nVar(C) = 3\nC\nVarsortability equals one if the marginal variance of each\nnode is strictly greater than that of its causal ancestors.\nVarsortability equals zero if the marginal variance of each\nnode is strictly greater than that of its descendants. Var-\nsortability does not depend on choosing one of the possibly\nmultiple causal orders and captures the overall agreement\nbetween the partial order induced by the marginal vari-\nances and all pathwise descendant relations implied by the\ncausal structure. In the two-node introductory example\n(cf. Section 1), varsortability v = 1 is equivalent to\nVar(A) < Var(B) ⇐⇒Var(NA) < w2 Var(NA) + Var(NB)\nwhere A and B are nodes in the causal graph A\nw→B with noise variances Var(NA) and Var(NB).\nWe can also understand varsortability as a property of the distribution of graphs and parameters that\nwe sample from for benchmarks on synthetic data. The distribution of weights and noise variances\ndetermines whether the causal order of any two connected nodes in the graph agrees with the order of\nincreasing marginal variance and in turn determines the varsortability of the simulated causal models.\nWe observe that even for modest probabilities of any two neighboring nodes being correctly ordered\nby their marginal variances, the variance of connected nodes tends to increase quickly along the causal\norder for many ANM instantiations (cf. Appendix G.3). For a heuristic explanation, recall that we\nobtain the marginal variance of a node by adding the variance contribution of all its ancestors to the\n4\nnode’s own noise variance; to obtain the variance contribution of an ancestor, we take the product of\nthe edge weights along each directed path from ancestor to node, sum these path coefﬁcient products,\nsquare, and multiply with the ancestor’s noise variance. While the sum of path coefﬁcient products\nmay vanish or be small such that the variance contribution of an ancestor cancels out or is damped\nacross the different connecting paths, we ﬁnd it is unlikely if edge weights are drawn independently\n(cf. Meek [1995] for why exact cancellation and faithfulness violations are unlikely). Furthermore,\nthe further apart a connected pair of nodes, the more variance may be accumulated in the descendant\nnode along all incoming paths in addition to one ancestor’s (possibly damped) variance contribution\nfurther fueling the tendency for descendant nodes to have higher variance than their ancestors. In\npractice we indeed ﬁnd that an ordering of nodes by increasing marginal variance closely aligns with\nthe causal order for commonly simulated linear and non-linear ANMs (cf. Appendix G).\n3.2\nVarsortability and Identiﬁability\nIf varsortability v = 1, the causal structure is identiﬁable. It can be recovered by ordering the nodes\nby increasing marginal variance and regressing each node onto its predecessors using conventional\nsparse regression approaches. The causal structure learning problem is commonly decoupled into\ncausal order estimation and parent selection [Shimizu et al., 2006, Shojaie and Michailidis, 2010,\nBühlmann et al., 2014, Chen et al., 2019, Park, 2020]. This decoupling is further warranted, since\nwe only need the causal ordering to consistently estimate interventional distributions [Bühlmann\net al., 2014, Section 2.6]. At v = 1, an ordering by marginal variance is a valid causal ordering.\nGiven a causal ordering, one can construct a fully connected DAG and use parent selection to prune\nedges and reconstruct the graph in the sample limit under mild assumptions. Bühlmann et al. [2014,\nSection 2.5] discuss parent selection and Shojaie and Michailidis [2010] establish the consistency of\nan adaptive lasso approach for edge selection given a valid causal order. The identiﬁability conditions\nby Park [2020] are closely related to varsortability, though not equivalent as we prove in Appendix C.\nIdentiﬁability of the causal order is immediate if varsortability v = 1, though, this shares severe\ndrawbacks with other identiﬁability conditions that rely on data scale by Peters and Bühlmann [2014],\nLoh and Bühlmann [2014], and Park [2020]. First, it is difﬁcult to verify or assess the plausibility of\nassumptions about the correctness or suitability of the data scale for any given dataset. Second, any\nunknown rescaling may break previously met identiﬁability conditions. Third, even if variables are\non the same measurement scale the units may not correctly capture the ground-truth causal scale. For\nexample, a dartist’s distance from the dartboard may affect the precision of their throw measured by\nthe distance between hit and target. Here, the effect variable’s marginal variance may be smaller than\nthat of the cause (even) if both distances are measured in centimetres. Nonetheless, it may be possible\nto exploit varsortability if one can establish that certain assumptions on the data scale be met.\n3.3\nVarsortability in Benchmarking Scenarios\nFor real-world data we cannot readily assess nor presume varsortability as we do not know the\nparameters and data scale of the data generating process. When benchmarking causal structure\nlearning algorithms, however, we can evaluate varsortability for the simulated DAGs and parameter\nsettings. We may acquire an intuition about the probability of varsortable cause-effect pairs in our\nsimulation settings by considering two neighboring nodes A\nw→B in the sampled graph without\ncommon ancestors and no other directed path from A to B. Under these assumptions, Var(B) =\nw2 Var(A) + Var(P\nC∈PA(B)\\{A} w2\nC→BC) + Var(NB) such that |w| > 1 implies that the variable\npair is varsortable. To simulate an ANM, we need to sample a DAG, decide on the noise distributions,\nand then sample edge weights and noise variances. Across simulation instances in a given benchmark\nset-up, the edge weights wk→j and noise variances s2\nk are iid instantiations of independent random\nvariables W and S2. The distributions of W and S2 induce a distribution of the marginal variance\nVY of node Y in the resulting ANM. The probability for the variable pair A →B to be varsortable\nin a simulated ANM is then bounded from below by P[(1 −W 2\nA→B)VA < S2\nNB] (cf. Appendix B).\nIf A is a root node, VA = S2\nNA. In the experimental settings used by, for example, Zheng et al.\n[2018, 2020], Lachapelle et al. [2019], Ng et al. [2020], edge weights are independently drawn\nfrom a uniform distribution and noise standard deviations or variances are either ﬁxed or also drawn\nindependently from a uniform distribution. For our parameters W\niid∼Unif((−2, −0.5) ∪(0.5, 2))\nand S\niid∼Unif((0.5, 2)), which resemble common choices in the literature, any pair is varsortable\nwith probability at least 2\/3 due to P[|W| > 1] = 2\/3, and with probability p > 0.93 provided A is a\n5\nroot node. Empirically, we ﬁnd that varsortability averages above 0.94 in our simulated graphs and\nabove 0.71 in commonly considered non-linear ANMs (cf. Appendix G). This result indicates that in\nbenchmark simulations the marginal variance of any two nodes in a graph tends to increase along the\ncausal order and that we may game these benchmarks and perform well by exploiting this pattern.\nIf A and B have a common ancestor or mediator C, the effect of C on B may either compound or\npartially cancel out the effect of A on B. In practice, the effect commonly increases the variance of\nthe effect node B, which may be attributed to the independent sampling of path coefﬁcients which\nalso renders faithfulness violations improbable [Meek, 1995]. We ﬁnd varsortability to increase\nwith graph density and the lower bound presented above to be loose. Motivated by the strong\nimpact of different levels of varsortability on some structure learning algorithms as reported in\nSection 4 and Appendix H.2, we advocate an empirical evaluation and reporting of varsortability\n(cf. Appendix G.4 for the implementation) when simulating ANMs. We emphasize that even for\nvarsortability < 1, where the order of increasing variance does not perfectly agree with the causal\norder, experimental results may still be largely driven by the overall agreement between increasing\nmarginal variance and causal order. The extent to which varsortability may distort experimental\ncomparisons of structure learning algorithms on linear ANMs is demonstrated in Section 4.\n3.4\nMarginal Variance yields Asymmetric Gradients for Causal and Anti-Causal Edges\nWe explain how varsortability may dominate the performance of continuous structure learning al-\ngorithms. We do not expect combinatorial structure learning algorithms that use a score-equivalent\n(see e.g. Yang and Chang [2002], Chickering [2002a]) criterion or scale-independent (conditional)\nindependence tests to be dependent on the data scale. This includes PC, as local constraint-based\nalgorithm, FGES as locally greedy score-based search using a score-equivalent criterion, and Di-\nrectLiNGAM, a procedure minimizing residual dependence. By contrast, combinatorial algorithms\nwith a criterion that is not score-equivalent (such as the MSE) depend on the data scale. Due to\nthe optimization procedure, continuous structure learning algorithms may depend on the data scale\nirrespective of whether the employed score is score-equivalent (as, for example, GOLEM for Gaussian\nmodels) or not (as, for example, GOLEM under likelihood misspeciﬁcation or NOTEARS).\nWe ﬁrst establish how varsortability affects the gradients of MSE-based score functions (which\nare akin to assuming equal noise variances in the Gaussian setting) and when initialising with the\nempty graph 0d×d (as is done in NOTEARS and GOLEM). Full statements of objective functions\nand respective gradients are found in Appendix E. Since ∇MSE (W) ∝X⊤(X −XW) we have\nthat ∇MSE (0d×d) ∝X⊤X and ∇LEV (0d×d) ∝1\/∥X∥2\n2X⊤X. The initial gradient step of both\nNOTEARS and GOLEM-EV is symmetric. We have ∇MSE(W) ∝[X⊤(x1 −Xw1), ..., X⊤(xd −\nXwd)] where the jth column X⊤(xj −Xwj) reﬂects the vector of empirical covariances of the\njth residual vector xj −Xwj with each xi. Provided a small identical step size is used across all\nentries of W in the ﬁrst step (as, for example, in GOLEM-EV), we empirically ﬁnd the residual\nvariance after the ﬁrst gradient step to be larger in those components that have higher marginal\nvariance (see Appendix E.3 for a heuristic argument). We observe that during the next optimization\nsteps ∇MSE(W) tends to be larger magnitude for edges pointing in the direction of nodes with\nhigh-variance residuals (which tends to be those with high marginal variance) than for those pointing\nin the direction of nodes with low-variance residuals (which tends to be those with low marginal\nvariance). Intuitively, when cycles are penalized, the insertion of edges pointing to nodes with high\nresiduals is favored as a larger reduction in MSE may be achieved than by including the opposing\nedge. Given high varsortability, this corresponds to favoring edges in the causal direction. This way,\nthe global information about the causal order in case of high varsortability is effectively exploited.\nOnce we allow for unequal noise variances as in GOLEM-NV, the marginal variances lead the\ngradients differently. Letting MSEj (wj) = 1\nn∥xj −Xwj∥2\n2, we have\n∇LNV (W) ∝\n\u0014X⊤(xj −Xwj)\nMSEj (wj)\n\u0015\nj=1,...,d\nsuch that the logarithmic derivative breaks the symmetry of the ﬁrst step for the non-equal vari-\nance formulation of GOLEM and we have ∇LNV (0d×d) ∝X⊤X diag(∥x1∥-2\n2 . . . , ∥xd∥-2\n2 ). While\n∇W MSE(W) ∝[X⊤(xj −Xwj)]j=1,...,d tends to favor edges in causal direction (see above),\nthe column-wise inverse MSE scaling of X⊤(xj −Xwj) by MSEj (wj) (the residual variance in\n6\nthe jth component) leads to larger-magnitude gradient steps for edges pointing in the direction of\nlow-variance nodes rather than high-variance nodes. Given high varsortability, this corresponds\npredominantly to the anti-causal direction.\nWe conjecture that the ﬁrst gradient steps have a dominant role in determining the causal structure,\neven though afterwards the optimization is governed by a non-trivial interplay of optimizer, model ﬁt,\nconstraints, and penalties. For this reason we focus on the ﬁrst optimization steps to explain a) why\ncontinuous structure learning algorithms that assume equal noise variance work remarkably well in\nthe presence of high varsortability and b) why performance changes once data is standardized and\nthe marginal variances no longer hold information about the causal order. Because of the acyclicity\nconstraint, it may be enough for a weight wi→j to be greater in magnitude than its counterpart wj→i\nearly on in the optimization for the smaller edge to be pruned from there on. For a discussion of the\ninterplay between sparsity penalties and data scale see Appendix J, which indicates that the nodes\nneed to be on a comparable data scale for l1-penalization to be well calibrated. Ng et al. [2020]\nprovide further discussion on sparsity and acyclicity constraints in continuous DAG learning.\n3.5\nsortnregress: A Diagnostic Tool to Reveal Varsortability\nWe propose an algorithm sortnregress performing the following two steps:\norder search Sort nodes by increasing marginal variance.\nparent search Regress each node on all of its predecessors in that order, using a sparse regression\ntechnique to prune edges [Shojaie and Michailidis, 2010]. We employ Lasso regression [Tibshirani,\n1996] using the Bayesian Information Criterion [Schwarz, 1978] for model selection.\nAs a baseline, sortnregress is easy to implement (cf. Appendix H.1) and highlights and evaluates to\nwhich extent the data scale is informative of the causal structure in different benchmark scenarios.\nAn extension for non-linear additive noise models is obtained by using an appropriate non-linear\nregression technique in the parent search step, possibly paired with cross-validated recursive feature\nelimination. It facilitates a clear and contextualized assessment of different structure learning algo-\nrithms in different benchmark scenarios. The relationship between varsortability and the performance\nof sortnregress in a linear setting is shown in Appendix H.2. Varying degrees of varsortability and\nperformance of sortnregress add an important dimension which current benchmarks do not consider.\n4\nSimulations\nWe compare the performance of the algorithms introduced in Section 2.3 on raw and standardized\nsynthetic data. In our comparison, we distinguish between settings with different noise distributions,\ngraph types, and graph sizes. Our experimental set-up follows those in Zheng et al. [2018], Ng\net al. [2020] and we contribute results obtained repeating their experiments in Appendix K. We\ncomplement our and previous DAG-recovery results by additionally evaluating how well the DAG\noutput by continous structure learning algorithms identiﬁes the MEC of the ground-truth DAG.\n4.1\nData Generation\nWe sample Erdös-Rényi (ER) [Erd˝os and Rényi, 1960] and Scale-Free (SF) [Barabási and Albert,\n1999] graphs and the parameters for ANMs according to the simulation details in Table 1. For a graph\nspeciﬁed as ER-k or SF-k with d nodes, we simulate dk edges. For every combination of parameters,\nwe create a raw data instance and a standardized version that is de-meaned and re-scaled to unit\nvariance. On standardized data, we have varsortability v = 1\n2 and the marginal variances hold no\ninformation about the causal ordering of the nodes. In all our experimental settings, varsortability\naverages above 0.94 on the raw data scale (cf. Appendix G.1).\nTable 1: Parameters for synthetic data generation.\nRepetitions\n10\nEdge weights\niid Unif((−2, −.5) ∪(.5, 2))\nGraphs\nER-2, SF-2, SF-4\nNoise distributions\nExponential, Gaussian, Gumbel\nNodes\nd ∈{10, 30, 50}\nNoise standard deviations\n1 (Gaussian-EV); iid Unif(.5, 2) (others)\nSamples\nn = 1000\n7\nRecovery of ground-truth DAG\n0\n500\n1000\n1500\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n0\n100\n200\n300\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\nRecovery of ground-truth DAG’s Markov equivalence class\n0\n20\n40\n60\n80\nMEC Structural Intervention Distance (upper bound)\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n0\n10\n20\n30\nMEC Structural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\nFigure 1: SID (left, lower is better) and SHD (right, lower is better) between recovered and ground-\ntruth graphs (top) or Markov equivalence classes (bottom) for ER-2 graphs with 50 (top) or 10\n(bottom) nodes and Gaussian-NV noise. The performance of sortnregress, which only exploits\nvarsortability, matches that of the continuous methods NOTEARS and GOLEM.\n4.2\nEvaluation\nWe evaluate performance using structural intervention distance (SID) [Peters and Bühlmann, 2015]\nand structural Hamming distance (SHD) between the recovered graph and ground truth. Additionally,\nwe contribute a performance assessments of continuous structure learning algorithms in terms of\nthe SID and SHD between the ground-truth MEC and the recovered MEC (PC, FGES) or the MEC\nidentiﬁed by the recovered graph (NOTEARS, GOLEM). SID assesses in how far the recovered\nstructure enables to correctly predict the effect of interventions. SHD measures the distance between\nthe true and recovered graph by counting how many edge insertions, deletions, and reversals are\nrequired to turn the former into the latter. Since interventional distributions can consistently be\nestimated given only the causal ordering [Bühlmann et al., 2014], SID is less susceptible to arbitrary\nchoices of edge pruning procedures and thresholds than SHD. Intuitively, SID prioritizes the causal\norder, while SHD prioritizes the correctness of individual edges. We follow common practices for\nedge thresholding and scoring (see Appendix K).\n4.3\nPerformance on Raw Versus Standardized Data\nWe group our algorithms into combinatorial and continuous algorithms. We propose a novel baseline\nalgorithm termed sortnregress which serves as a reference marking the performance achievable by\ndirectly exploiting marginal variances. We indicate its performance on standardized data as random-\nregress, since it amounts to choosing a random order and regressing each node on its predecessors.\nWe use boxplots aggregating the performance achieved in the 10 repetitions on raw and standardized\ndata by each algorithm and create separate plots per noise type to account for identiﬁability and MLE\nspeciﬁcation differences. We show the results obtained for ER-2 graphs and Gaussian noise with\nnon-equal variances in Figure 1. These results are representative of the results obtained for different\ngraphs and noise distributions (cf. Appendix K). For the simulated settings, varsortability is high\n(> 0.94) on the raw data scale (cf. Appendix G.1).\nWe observe that some algorithms are highly scale-sensitive and perform vastly different on raw and\nstandardized data. The algorithms NOTEARS, MSE-GDS, GOLEM-EV are most affected – their\nperformance is excellent on raw data but far worse on standardized data. Note that all of these rely on\n8\na loss function that revolves around the MSE. The performance of GOLEM-NV is also scale-sensitive\nbut improves upon standardization. The direction of the effect of standardization is in line with the\npredictions by our gradient analysis in Section 3.4. Note that we initialize all algorithms with the\nempty graph since we are primarily interested in comparing the impact of standardization given\nequal starting conditions. On standardized data, an initialization of GOLEM-NV with the results\nof GOLEM-EV, as recommended by Ng et al. [2020], does not improve performance and may fail\nto converge. sortnregress achieves competitive performance on raw, and baseline performance on\nstandardized data. It thus qualiﬁes as diagnostic tool to highlight how much of a given causal structure\nlearning task can be resolved by exploiting data scale and sorting nodes by their marginal variance.\nIn summary, the evidence corroborates our claim that the remarkable performance on raw data and\nthe overall behavior upon standardization of the continuous structure learning algorithms may be\ndriven primarily by high varsortability. On a real-world causal protein signaling dataset [Sachs et al.,\n2005] we measure a mean varsortability of 0.57 (which is close to chance level at 0.5) with a standard\ndeviation of 0.01 across our bootstrapped samples and do not observe the consistent performance\npattern described for synthetic data with high varsortability (cf. Appendix I).\n5\nGaming Further Benchmarks\n5.1\nOrienting Causal Chains on Standardized Data\nIn order to design a causal discovery benchmark that does not favor methods that explicitly exploit\nmarginal variances we may standardize the data or employ coefﬁcient re-scaling schemes. Mooij et al.\n[2020], for example, propose a scale-harmonization by dividing each column wj = [wk→j]j=1,...,d ∈\nRd of the drawn adjacency matrices by\np\n∥wj∥2 + 1 such that each variable would have comparable\nscale if all its direct parents were independently standard-normally distributed. However, this\ndoes not avoid the problem of potentially inadvertent patterns in simulated ANMs. Even after\nstandardization or scale-harmonization, DAGs with previously high varsortability generate data with\ndistinct covariance patterns that may be exploited.\nIn Appendix F we present an instructive example of a decision rule that can infer the orientation\nof a causal chain from raw, standardized, and scale-harmonized data with accuracy strictly greater\nthan 50%. For a causal chain X1 →X2 →... →Xd where edge weights and noise terms are\ndrawn iid we can decide between the two Markov-equivalent graphs X1 →X2 →... →Xd and\nX1 ←X2 ←... ←Xd whith above-chance accuracy. The empirical results for varying chain-lengths\nand various edge-weight distribution are deferred to the appendix where we discuss the 3-variable\nchain in detail and illustrate that the phenomenon extends from ﬁnite-sample to the population setting.\nThe intuition is as follows. Consider data generated by X1 →X2 →... →Xd and the aim is to\ninfer from standardized data whether X1 →... →Xd or X1 ←... ←Xd. For data with high\nvarsortability and comparable noise variance on the raw data scale it holds that the further downstream\na node Xi is in the causal chain, the stronger the variance of its parent Var(Xi−1) contributes to\nits marginal variance Var(Xi) = Var(Xi−1) + Var(Ni) relative to its noise variance Var(Ni), and\nthe stronger is it correlated with its parent. Thus, the sequence of regression coefﬁcients, which\nin the standardized case amounts to (Corr(Xi, Xi+1))i=1,...,d−1, tends to increase in magnitude\nalong the causal order and decrease in the anti-causal direction. The proposed decision rule predicts\nthe causal direction as the one in which the absolute values of the regression coefﬁcients tend to\nincrease. This chain orientation rule achieves above-chance performance on raw, standardized, and\nscale-harmonized data (cf. Appendix F).\n5.2\nSorting by Variance in Non-Linear Settings\nVarsortability may also be exploited in non-linear settings. Table 2 shows the results of sorting by\nmarginal variance and ﬁlling in all edges from lower-variance nodes to higher-variance nodes in\na non-linear setting. This variance sorting strategy is more naive than sortnregress and places no\nassumption on the functional form. The results are substantially better than random sorting and\nmay therefore be a more informative baseline than commonly used random graphs. We do not show\nperformance in terms of SHD, as our variance sorting baseline always yields a fully connected graph.\nAlthough the data generating process is not identical, we note that the improvement of our crude\nvariance sorting over random sorting compares favorably to some of the improvements gained by\n9\nmore involved methods over random graphs as shown in Lachapelle et al. [2019, Table 1]. Our results\nindicate that exploiting varsortability may also deliver competitive results in non-linear settings.\nTable 2: SID of naive baselines on non-linear data. Results on 1000 observations of additive Gaussian\nprocess ANMs with noise variance 1 simulated as by Zheng et al. [2020] (10 repetitions each; average\nvarsortability v per graph type shown in parentheses).\nAlgorithm\nGraph\nER-1\nER-4\nSF-1\nSF-4\n(average v)\n(0.87)\n(0.95)\n(0.95)\n(0.98)\nvariance sorting\n7.7 ± 5.72\n25.2 ± 12.36\n1.9 ± 2.28\n7.6 ± 3.37\nrandom sorting\n27.9 ± 11.44\n63.1 ± 8.10\n22.3 ± 13.14\n59.5 ± 7.32\nWe ﬁnd similarly high levels of varsortability for many non-linear functional relationships and graph\nparameters (cf. Appendix G.2). This begs the question how much other successful methods exploit\nvarsortability, how they compare to non-linear nonparametric methods that leverage assumptions\non the residual variances [Gao et al., 2020], and how they perform under data standardization. We\nencourage such an exploration in future work and suggest that varsortability and sortnregress or\nvariance sorting should always be included in future benchmarks.\n6\nDiscussion and Conclusion\nWe ﬁnd that continuous structure learning methods are highly susceptible to data rescaling and some\ndo not perform well without access to the true data scale. Therefore, scale-variant causal structure\nlearning methods should be applied and benchmarked with caution, especially if the variables do not\nshare a measurement scale or when the true scale of the data is unattainable. It is important to declare\nwhether data is standardized prior to being fed to various structure learning algorithms.\nFollowing the ﬁrst release of the present paper, Kaiser and Sipos [2021] also independently reported\nthe drop in performance of NOTEARS upon standardizing the data and presented a low-dimensional\nexemplary case. Beyond a reporting of impaired NOTEARS performance, we also analyze score-\nequivalent methods, provide exhaustive simulation experiments, and explain the phenomenon.\nOur aim is to raise awareness of the severity with which scaling properties in data from simulated\nDAGs and causal additive models may distort algorithm performance. Increasing marginal variances\ncan render scenarios identiﬁable, which may commonly not be expected to be so—for example the\nGaussian case with non-equal variances. We therefore argue that varsortability should be taken into\naccount for future benchmarking. Yet, with any synthetic benchmark there remains a risk that the\nresults are not indicative of algorithm performance on real data. Our results indicate that current\nstructure learning algorithms may perform within the range of naive baselines on real-world datasets.\nThe theoretical results of our paper are limited to the setting of linear ANMs. Additionally, our\nconjecture regarding the importance of the ﬁrst gradient steps, and with it a rigorous causal explanation\nfor the learning behavior of different continuous algorithms and corresponding score functions remain\nopen and require further research to be settled. Our empirical ﬁndings indicate that causal discovery\nbenchmarks can be similarly gamed on standardized data and in non-linear settings, but further\nresearch is needed to conﬁrm this. We focus on a speciﬁc subset of algrorithms, the impact of patterns\nin benchmarking data on a wider class of algorithms and score functions remains to be explored.\nVarsortability arises in many ANMs and the marginal variances increase drastically along the causal\norder, at least in common simulation settings. This begs the question what degree of varsortability\ncan be observed or assumed in real-world data. If the marginal variances carry information about the\ncausal order, our results suggest that it can and should be leveraged for structure learning. Otherwise,\nour contribution motivates future research into representative benchmarks and may put the practical\napplicability of the additive noise assumption into question.\nAcknowledgements\nWe thank Jonas M. Kübler, Jonas Peters, and Sorawit Saengkyongam for helpful discussions and\ncomments. SW was supported by the Carlsberg Foundation.\n10\nReferences\nBryon Aragam and Qing Zhou. Concave penalized estimation of sparse Gaussian Bayesian networks. Journal of\nMachine Learning Research, 16(69):2273–2328, 2015.\nAlbert-László Barabási and Réka Albert. Emergence of scaling in random networks. Science, 286(5439):\n509–512, 1999.\nRohit Bhattacharya, Tushar Nagarajan, Daniel Malinsky, and Ilya Shpitser. Differentiable causal discovery under\nunmeasured confounding. In International Conference on Artiﬁcial Intelligence and Statistics, 2021.\nPhilippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin.\nDifferentiable causal discovery from interventional data. In Advances in Neural Information Processing\nSystems, 2020.\nPeter Bühlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional order search\nand penalized regression. The Annals of Statistics, 42(6):2526–2556, 2014.\nWenyu Chen, Mathias Drton, and Y Samuel Wang. On causal discovery with an equal-variance assumption.\nBiometrika, 106(4):973–980, 09 2019.\nDavid M. Chickering. A Transformational Characterization of Equivalent Bayesian Network Structures. In\nProceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 1995.\nDavid M. Chickering. Learning Bayesian Networks is NP-complete. In Learning From Data, pages 121–130.\nSpringer, 1996.\nDavid M. Chickering. Learning equivalence classes of Bayesian-network structures. Journal of Machine\nLearning Research, 2:445–498, 2002a.\nDavid M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning\nResearch, 3(11):507–554, 2002b.\nDavid M. Chickering, David Heckerman, and Chris Meek. Large-sample learning of Bayesian networks is\nNP-hard. Journal of Machine Learning Research, 5:1287–1330, 2004.\nA. Philip Dawid. Beware of the DAG! In Proceedings of Workshop on Causality: Objectives and Assessment at\nNIPS 2008, volume 6 of Proceedings of Machine Learning Research, pages 59–86. PMLR, 2010.\nPaul Erd˝os and Alfréd Rényi. On the evolution of random graphs. Publication of the Mathematical Institute of\nthe Hungarian Academy of Sciences, 5(1):17–60, 1960.\nMing Gao, Yi Ding, and Bryon Aragam. A polynomial-time algorithm for learning nonparametric causal graphs.\nIn Advances in Neural Information Processing Systems, 2020.\nAsish Ghoshal and Jean Honorio. Learning Identiﬁable Gaussian Bayesian Networks in Polynomial Time and\nSample Complexity. In Advances in Neural Information Processing Systems, 2017.\nAsish Ghoshal and Jean Honorio. Learning linear structural equation models in polynomial time and sample\ncomplexity. In International Conference on Artiﬁcial Intelligence and Statistics, 2018.\nPatrik Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal\ndiscovery with additive noise models. In Advances in Neural Information Processing Systems, 2009.\nGuido W. Imbens and Donald B. Rubin. Causal inference in statistics, social, and biomedical sciences.\nCambridge University Press, 2015.\nMarcus Kaiser and Maksim Sipos. Unsuitability of NOTEARS for Causal Graph Discovery. arXiv preprint\narXiv:2104.05441, 2021.\nSébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural\nDAG learning. arXiv preprint arXiv:1906.02226, 2019.\nHao-Chih Lee, Matteo Danieletto, Riccardo Miotto, Sarah T. Cherng, and Joel T. Dudley. Scaling structural\nlearning with NO-BEARS to infer causal transcriptome networks. arXiv preprint arXiv:1911.00081, 2019.\nPo-Ling Loh and Peter Bühlmann. High-dimensional learning of linear causal networks via inverse covariance\nestimation. Journal of Machine Learning Research, 15:3065–3105, 2014.\n11\nChristopher Meek. Strong Completeness And Faithfulness In Bayesian Networks. In Proceedings of Eleventh\nConference on Uncertainty in Artiﬁcial Intelligence (UAI), 1995.\nChristopher Meek. Graphical Models: Selecting causal and statistical models. PhD thesis, Carnegie Mellon\nUniversity, 1997.\nJoris M. Mooij, Sara Magliacane, and Tom Claassen. Joint Causal Inference from Multiple Contexts. Journal of\nMachine Learning Research, 21(99):1–108, 2020.\nIgnavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and DAG constraints for learning\nlinear DAGs. In Advances in Neural Information Processing Systems, 2020.\nRoxana Pamﬁl, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Georgatzis, Paul\nBeaumont, and Bryon Aragam. DYNOTEARS: Structure Learning from Time-Series Data. In International\nConference on Artiﬁcial Intelligence and Statistics, 2020.\nGunwoong Park. Identiﬁability of additive noise models using conditional variances. Journal of Machine\nLearning Research, 21(75):1–34, 2020.\nJudea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96–146, 2009.\nJonas Peters and Peter Bühlmann. Identiﬁability of Gaussian structural equation models with equal error\nvariances. Biometrika, 101(1):219–228, 2014.\nJonas Peters and Peter Bühlmann. Structural intervention distance for evaluating causal graphs. Neural\nComputation, 27(3):771–799, 2015.\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of Causal Inference. MIT Press, 2017.\nJoseph D. Ramsey, Kun Zhang, Madelyn Glymour, Ruben S. Romero, Biwei Huang, Imme Ebert-Uphoff, Savini\nSamarasinghe, Elizabeth A. Barnes, and Clark Glymour. TETRAD—A toolbox for causal discovery. In 8th\nInternational Workshop on Climate Informatics (CI 2018), 2018.\nKenneth J. Rothman, Sander Greenland, and Timothy L. Lash. Modern Epidemiology. Lippincott Williams &\nWilkins, 2008.\nJakob Runge, Xavier-Andoni Tibau, Matthias Bruhns, Jordi Muñoz-Marí, and Gustau Camps-Valls. The causality\nfor climate competition. In NeurIPS 2019 Competition and Demonstration Track, pages 110–120. PMLR,\n2020.\nKaren Sachs, Omar Perez, Dana Pe’er, Douglas A. Lauffenburger, and Garry P. Nolan. Causal protein-signaling\nnetworks derived from multiparameter single-cell data. Science, 308(5721):523–529, 2005.\nAndrew D. Sanford and Imad A. Moosa. A Bayesian network structure for operational risk modelling in\nstructured ﬁnance operations. Journal of the Operational Research Society, 63(4):431–444, 2012.\nBernhard Schölkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.\nGideon Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978.\nShohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-Gaussian acyclic model\nfor causal discovery. Journal of Machine Learning Research, 7:2003–2030, 2006.\nShohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio,\nPatrik O. Hoyer, and Kenneth Bollen. DirectLiNGAM: A direct method for learning a linear non-Gaussian\nstructural equation model. Journal of Machine Learning Research, 12:1225–1248, 2011.\nAli Shojaie and George Michailidis. Penalized likelihood methods for estimation of sparse high-dimensional\ndirected acyclic graphs. Biometrika, 97(3):519–538, 07 2010.\nPeter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. Social Science\nComputer Review, 9(1):62–72, 1991.\nPeter Spirtes, Clark N. Glymour, Richard Scheines, and David Heckerman. Causation, Prediction, and Search.\nMIT press, 2000.\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:\nSeries B (Methodological), 58(1):267–288, 1996.\n12\nMatthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. D’ya like DAGs? A Survey on Structure\nLearning and Causal Discovery. arXiv preprint arXiv:2103.02582, 2021.\nDennis Wei, Tian Gao, and Yue Yu. DAGs with No Fears: A Closer Look at Continuous Optimization for\nLearning Bayesian Networks. arXiv preprint arXiv:2010.09133, 2020.\nSebastian Weichwald, Martin E. Jakobsen, Phillip B. Mogensen, Lasse Petersen, Nikolaj Thams, and Gherardo\nVarando. Causal structure learning from time series: Large regression coefﬁcients may predict causal links\nbetter in practice than small p-values. In NeurIPS 2019 Competition and Demonstration Track, pages 27–36.\nPMLR, 2020.\nShulin Yang and Kuo-Chu Chang. Comparison of score metrics for Bayesian network learning. IEEE Transac-\ntions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 32(3):419–428, 2002.\nYue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks. arXiv\npreprint arXiv:1904.10098, 2019.\nJiji Zhang and Peter Spirtes. Detection of unfaithfulness and robust causal inference. Minds and Machines, 18\n(2):239–271, 2008.\nKun Zhang and Aapo Hyvarinen. On the identiﬁability of the post-nonlinear causal model. In Proceedings of\nthe Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2009.\nXun Zheng, Bryon Aragam, Pradeep K. Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Continuous\noptimization for structure learning. In Advances in Neural Information Processing Systems, 2018.\nXun Zheng, Chen Dan, Bryon Aragam, Pradeep K. Ravikumar, and Eric P. Xing. Learning sparse nonparametric\nDAGs. In International Conference on Artiﬁcial Intelligence and Statistics, 2020.\n13\nAppendix\nTable of Contents\nA Varsortability in the Two-Node Case\n15\nB\nDerivation of Lower Bound on Pairwise Varsortability\n15\nC Varsortability and Identiﬁability by Conditional Noise Variances\n16\nC.1\nPark, 2020, Theorem 4 conditions satisﬁed without varsortability . . . . . . . . .\n16\nC.2\nVarsortability without Park, 2020, Theorem 4 conditions satisﬁed\n. . . . . . . .\n16\nD Algorithms\n17\nE\nThe Subtle Interplay Between Marginal Variance and Gradient Directions\n18\nE.1\nExample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nE.2\nStepwise Gradient Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nE.3\nIncreasing Marginal and Residual Variances\n. . . . . . . . . . . . . . . . . . .\n21\nE.4\nGradient Asymmetry\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nE.5\nNOTEARS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nF\nStandardization Is Not Enough and Regression Coefﬁcients Tend to Increase Along\nthe Causal Order\n22\nF.1\nInﬁnite Sample\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nF.2\nFinite Sample\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nG Empirical Evaluation of Varsortability\n25\nG.1\nVarsortability in Linear Additive Noise Models . . . . . . . . . . . . . . . . . .\n25\nG.2\nVarsortability in Non-Linear Additive Noise Models . . . . . . . . . . . . . . .\n26\nG.3\nCausal Order and Marginal Variance\n. . . . . . . . . . . . . . . . . . . . . . .\n26\nG.4\nVarsortability Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nH sortnregress: A Diagnostic Tool to Reveal Varsortability\n28\nH.1\nImplementation of Sortnregress . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nH.2\nVarsortabiltiy and Score Attainable by Variance Ordering\n. . . . . . . . . . . .\n28\nI\nEvaluation on Real-World Data\n29\nJ\nModel Selection in Continuous Optimization\n29\nK Detailed Results\n30\nK.1\nMEC Recovery\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nK.2\nResults Across Thresholding Regimes . . . . . . . . . . . . . . . . . . . . . . .\n31\nK.3\nResults Across Noise Distributions and Graph Types . . . . . . . . . . . . . . .\n31\nK.4\nResults Across Noise Distributions, Graph Types, and Graph Sizes . . . . . . . .\n33\n14\nA\nVarsortability in the Two-Node Case\nConsider the following ground truth and two competing linear acyclic models:\nGround-truth model:\nA := NA\nB := wA + NB\nModel M1) “A →B”:\nbA = 0\nbB = bwA\nModel M2) “A ←B”:\nbA = bvB\nbB = 0\nwhere w ̸= 0, NA and NB are independent zero-centred noise terms that follow some distributions\nwith non-vanishing corresponding variance VA and VB. The model parameters bw = Cov(A,B)\nVar(A)\n=\nw and bv =\nCov(A,B)\nVar(B)\n=\nwVA\nVar(B) are the corresponding ordinary least-squares linear regression\ncoefﬁcients.\nWe evaluate in which cases the true model M1 obtains a smaller MSE than the wrong model M2, to\ndecide if and under which conditions a MSE-based orientation rule recovers the ground-truth edge\ndirection:\nMSE (M1) < MSE (M2)\n⇐⇒\nVar(A) + Var(B −bwA) < Var(A −bvB) + Var(B)\n⇐⇒\nVar(A) + Var ((wNA + NB) −wNA) < Var\n\u0012\nNA −\nwVA\nVar(B)(wNA + NB)\n\u0013\n+ Var(B)\n⇐⇒\nVA + VB < VAVB\nVar(B) + w2VA + VB\n⇐⇒\n0 < VA\n\u0012\nVB\nVar(B) −1\n\u0013\n+ w2VA\n⇐⇒\n0 < −w2VA\nVar(B) + w2\n⇐⇒\nVA < Var(B)\n⇐⇒\n(1 −w2)VA < VB\nFor error variances VA ≤VB and any non-zero edge weight w, the MSE-based inference is correct.\nThis resembles known scale-based identiﬁability results based on equal or monotonically increasing\nerror variances [Peters and Bühlmann, 2014, Park, 2020].\nB\nDerivation of Lower Bound on Pairwise Varsortability\nLet A and B be any two nodes in the sampled graph with edge A\nw→B, noise terms NA, NB, and\nwithout common ancestors and no other directed path from A to B. When sampling edge coefﬁcients\nand noise variances randomly for the simulation of ANMs, distributions are incurred over the variances\nof A and B across those simulated ANMs. Let edge weights be sampled as [Wx→y]x,y=1,...,d ∼PW ,\nand noise variances be sampled as [S2\nNy]y=1,...,d ∼PS2. Across simulations, the marginal variances\nof A and B are transformations of S and W and themselves random variables denoted as VA and\nVB. The marginal variance VY of any node Y depends on its noise variance and the additional\nvariance incurred by predecessor nodes given as P\nX∈PA(Y ) W 2\nX→Y VX. We can therefore bound the\nprobability for the variable pair (A, B) to be varsortable from below via\nP[VA < VB] = P\n\nVA <\n\nW 2\nABVA +\nX\nX∈PA(B)\\{A}\nW 2\nXBVX + S2\nNB\n\n\n\n\n≥P[VA < W 2\nABVA + S2\nNB]\n15\nwhere equality holds if A is the only parent of B contributing to B’s marginal variance.\nIn common benchmarks,\nedge weights are drawn independently according to PW\n∼\n⊗k,j=1,...,d Unif((−2, −.5)∪(.5, 2)) and noise standard deviations are drawn iid SNj ∼Unif(.5, 2).\nC\nVarsortability and Identiﬁability by Conditional Noise Variances\nWhile closely related, varsortability is not equivalent to the identiﬁability conditions laid out in\nTheorem 4, Park [2020], (henceforth referred to as “Theorem 4”). We prove this by providing two\nexamples. In Appendix C.1 part A) of the conditions in Theorem 4 is satisﬁed, while varsortability\ndoes not hold. In Appendix C.2 varsortability holds but neither part A) nor part B) of Theorem 4 are\nsatisﬁed.\nC.1\nPark, 2020, Theorem 4 conditions satisﬁed without varsortability\nConsider the following ground-truth model with unique causal order A, B, C:\nA := NA\nB := βA→BA + NB = 1A + NB\nC := βB→CB + NC =\nr\n2\n3B + NC\nwhere NA, NB, NC are jointly independent zero-centred noise terms with respective variances\nσ2\nA = 4, σ2\nB = 2, σ2\nC = 1. The marginal variances are Var(A) = 4 < Var(C) = 5 < Var(B) = 6.\nOur example resembles the examples in Section 3.1 of Park [2020]. We can verify the three conditions\nfor part A) of Theorem 4:\n(A1)\nσ2\nA < σ2\nB + β2\nA→Bσ2\nA,\n(A2)\nσ2\nB < σ2\nC + β2\nB→Cσ2\nB,\n(A3)\nσ2\nA < σ2\nC + β2\nB→Cσ2\nB + β2\nA→Bβ2\nB→Cσ2\nA\nInserting the values from above, we obtain\n(A1)\n4 < 2 + 1 · 4,\n(A2)\n2 < 1 + 2\n3 · 2,\n(A3)\n4 < 1 + 2\n3 · 2 + 1 · 2\n3 · 4.\nOur result veriﬁes that identiﬁability is given as per Theorem 4 in Park [2020], while the order of\nincreasing marginal variances is not in complete agreement with the causal order and varsortability is\nnot equal to 1.\nC.2\nVarsortability without Park, 2020, Theorem 4 conditions satisﬁed\nConsider the following ground-truth model with unique causal order A, B, C:\nA := NA\nB := βA→BA + NB = A + NB\nC := βA→CA + βB→CB + NC =\n1\n√\n2A + 1\n√\n2B + NC\nwhere NA, NB, NC are jointly independent zero-centred noise terms with respective variances σ2\nA =\n4, σ2\nB = 3, σ2\nC = 1. The marginal variances are Var(A) = 4 < Var(B) = 7 < Var(C) = 10.5.\nWe now verify, that for both case A) and B) in Theorem 4 of Park [2020] at least one of the inequality\nconstraints is violated.\nOne of the three conditions in A) is\nσ2\nB < σ2\nC + β2\nB→Cσ2\nB,\n16\nwhile for the above model we have\n3 ≮1 + 1\n2 · 3.\nOne of the three conditions in B) is\nσ2\nC\nσ2\nB\n> (1 −β2\nB→C),\nwhile for the above model we have\n1\n3 ≯(1 −1\n2).\nFor both criteria A) and B) in Theorem 4 at least one of the inequalities is not satisﬁed. We thus verify\nthat even if identiﬁability is not given as per the sufﬁcient conditions in Theorem 4, Park [2020],\nvarsortability may still render the causal order identiﬁable.\nD\nAlgorithms\nDirectLiNGAM\nis a method for learning linear non-Gaussian acyclic models [Shimizu et al., 2011].\nIt recovers the causal order by iteratively selecting the node whose residuals are least dependent on\nany predecessor node. In a strictly non-Gaussian setting, DirectLiNGAM is guaranteed to converge\nto the optimal solution asymptotically within a small ﬁxed number of steps and returns a DAG. We\nuse the implementation provided by the authors1. We deliberately keep the default of a least-angle\nregression penalized by the Bayesian Informaion Criterion. We ﬁnd that this penalty strikes a good\nbalance between SID and SHD performance. Cross-validated least-angle regression performs better\nin terms of SID but poorer in terms of SHD.\nPC\n[Spirtes and Glymour, 1991] is provably consistent in estimating the Markov equivalence\nclass of the true data-generating graph if the causal Markov and faithfulness assumptions hold.\nThe algorithm returns a completed partially directed acyclic graph (CPDAG). For computational\nreasons, we refrain from computing the lower and upper bounds of the SID for comparing CPDAGS\nwith the ground-truth DAG as proposed by Peters and Bühlmann [2015]. Instead, we adopt the\napproach by Zheng et al. [2018] and resolve bidirectional edges favorably to obtain a DAG. We use\nthe implementation in the Tetrad2 package Ramsey et al. [2018].\nFGES\nis an optimized version of the fast greedy equivalence search algorithm developed by Meek\n[1997], Chickering [2002b]. Under causal Markov and faithfulness assumptions, it is provably\nconsistent for estimating the Markov equivalence class of the true data-generating graph. The\nalgorithm returns a CPDAG, which we resolve favorably to obtain a DAG. We use the implementation\nin the Tetrad3 package [Ramsey et al., 2018].\nMSE-GDS\nis a greedy DAG search procedure with a MSE score criterion. We implement MSE-\nGDS following other GDS procedures, for example, as described by Peters and Bühlmann [2014,\nSection 4], but use the MSE as score criterion instead of a likelihood- or BIC-based score criterion.\nFor simplicity and computational ease, we consider a smaller search space and greedily forward-\nsearch over new edge insertions only instead of greedily searching over all neighbouring DAGs\nobtainable by edge insertions, removals, and deletions. For the linear setting, linear regression is\nused to determine the edge weights and the corresponding MSE-score for a given graph. For the\nnon-linear setting, support vector regression can be used instead. The algorithm returns a DAG.\nNOTEARS\nis a score-based method that ﬁnds both structure and parameters simultaneously by\ncontinuous optimization [Zheng et al., 2018]. The optimization formulation is based on the mean\nsquared error and includes a sparsity penalty parameter λ and a differentiable acyclicity constraint:\nargmin\nW ∈Rd×d\nMSEX (W) + λ∥W∥1\ns.t.\ntr(exp(W ⊙W)) −d = 0.\n1https:\/\/github.com\/cdt15\/lingam\n2https:\/\/github.com\/cmu-phil\/tetrad\n3https:\/\/github.com\/cmu-phil\/tetrad\n17\nThe algorithm returns a DAG. We use the implementation provided by the authors4. Throughout\nall our experiments we use NOTEARS over NOTEARS-L1 (setting λ = 0), following the ﬁndings of\nZheng et al. [2018, Tables 1 and 2, Figure 3], which suggest regularization only for samples smaller\nthan the n = 1000 we use throughout.\nGOLEM\ncombines a soft version of the differentiable acyclicity constraint from Zheng et al. [2018]\nwith a MLE objective [Ng et al., 2020]. The authors propose a multivariate Gaussian MLE for equal\n(EV) or unequal (NV) noise variances and optimize\nargmin\nW ∈Rd×d\neL(W, X) −log(| det(I −W)|) + λ1∥W∥1 + λ2(tr(exp(W ⊙W)) −d)\nwhere eL is either\neLEV (W, X) = d\n2 (LEV (W, X) + log(n)) = d\n2 log(n MSEX (W)), or\neLNV (W, X) = 1\n2 (LNV (W, X) + d log(n)) = 1\n2\nd\nX\nj=1\nlog(n MSEj (wj)).\nWe use the implementation and hyperparameters provided by the authors5. We train for 104 episodes\nas we found that half of that sufﬁces to ensure convergence. Notably, we do not perform pretraining\nfor our version of GOLEM-NV.\nsortnregress\nis implemented as shown in Appendix H.1. We ﬁnd that a least-angle regression\npenalized by the Bayesian Information Criterion strikes a good balance between SID and SHD\nperformance.\nE\nThe Subtle Interplay Between Marginal Variance and Gradient Directions\nWe describe observations about the gradients involved in the optimization procedures of NOTEARS\nand GOLEM-EV\/-NV. We present an instructive example in Appendix E.1 and provide some intuition\nabout how the adjacency matrix changes throughout the optimization. For convenience and reference\nwe provide gradients of the individual terms involved in the respective objective functions (cf.\nAppendix E.2). In Appendix E.3 we argue why the nodes’ residual variances for the ﬁrst gradient steps\nin an unconstrained optimization of MSE- or log-MSE-EV-based (GOLEM-EV) objective functions\nwith acyclicity penalties tend to follow the same ordering as the nodes’ marginal variances. We\nanalyze gradient symmetry and asymmetry in GOLEM-EV\/-NV’s gradient descent optimization under\nvarsortability in Appendix E.4. While the intuition for small step size gradient-based unconstrained\noptimization partially carries over to the NOTEARS optimization procedure, here the interplay\nbetween varsortability and gradient directions is intricate due to a constrained optimization that is\nsolved via the augmented Lagrangian method and dual descent with line-search instead of gradient\ndescent as used in GOLEM [Zheng et al., 2018] (cf. Appendix E.5).\nThe heuristic arguments presented here are preliminary and aim to provide intuition. The optimization\nbehaviour also heavily depends on the implementation of the optimization routine. For example,\nthe original implementation of NOTEARS ﬁxes the diagonal of W at zero and leverages curvature\ninformation (L-BFGS-B), while GOLEM updates all entries of W and employs learning rate optimiz-\ners. Future research is required to determine how precisely continuous structure learning algorithms\nachieve state-of-the-art results on highly varsortable data and, given our observations, we expect\nexplanations to be speciﬁc to individual algorithms and their distinct implementations.\nE.1\nExample\nThe following example considers the population limit and illustrates a few intuitions about gradient\nbased optimization and varsortability. Consider data is generated according to\n\u0012\nX\nY\n\u0013\n=\n\u0012\n0\nβ\n0\n0\n\u0013⊤\u0012\nX\nY\n\u0013\n+\n\u0012\nNX\nNY\n\u0013\n4https:\/\/github.com\/xunzheng\/notears\n5https:\/\/github.com\/ignavier\/golem\n18\nwhere NX and NY are independently normally distributed with standard deviations σNX and σNY .\nHere, varsortability v = 1 and 1 = Var X < Var Y = 2.\nInitializing the weight matrix at the zero matrix, the gradient of the population MSE is\n−2\n\u0012\nVar(X)\nCov(X, Y )\nCov(Y, X)\nVar(Y )\n\u0013\n= −2\n\u0012 σ2\nNX\nβσ2\nNX\nβσ2\nNX\nβ2σ2\nNX + σ2\nNY\n\u0013\n(see also Appendix E.2). The models for X and Y after a ﬁrst gradient descent step of step size η are\nb\nX = 2η(σ2\nNXX + βσ2\nNXY )\nbY = 2η(βσ2\nNXX + (β2σ2\nNX + σ2\nNY )Y )\nIf the diagonal of the weight matrix is clamped to 0 throughout the optimization, the terms corre-\nsponding to self-loops (2ησ2\nNXX in b\nX and (β2σ2\nNX + σ2\nNY )Y in bY ) are dropped above. This is the\ncase in the original implementation of NOTEARS, where the unconstrained subproblem is optimized\nvia L-BFGS-B with identity bounds on the diagonal entries of W.\nBelow we visualize Var(X −b\nX) (residual variance in X), Var(Y −bY ) (residual variance in Y ),\nand the MSE Var(X −b\nX) + Var(Y −bY ), for varying step sizes η of the ﬁrst gradient step where\nwe exemplary choose β = σNX = σNY = 1.\n0.0\n0.1\n0.2\n0.3\n0.4\nstep size\n0\n1\n2\n3\n4\nresidual variance after 1st gradient step \nresidual variance in X\nresidual variance in Y\noverall residual variance (MSE)\n0.0\n0.2\n0.4\n0.6\n0.8\nstep size\n0\n1\n2\n3\n4\nresidual variance after 1st gradient step \n(self-loop edge weights fixed at 0)\nresidual variance in X\nresidual variance in Y\noverall residual variance (MSE)\nSince the residual variances change continuously for increasing step sizes, the residual variances\nfollow the order of the marginal variances for small step sizes (cf. also Appendix E.3). Since in\nGOLEM we solve an unconstrained optimization problem by gradient descent (with small step\nsize and learning rate), the order of residual variances tends to remain unchanged during the ﬁrst\noptimization steps. The order of the residual variances may swap relative to the order of marginal\nvariances, though, if line-search is employed to determine the step size that minimizes the MSE-\nobjective. This is the case in NOTEARS, where the MSE is minimized by a dual descent routine\nwith increasing weight on the acyclicity penalty term. Here, the ﬁrst symmetric update of the weight\nmatrix occurs with a large step size that minimizes the MSE (minimum of the green curves in above\nplots). The ordering of the resulting residual variances is less obvious. In the above example, if the\ndiagonal terms of the weight matrix are updated as well (left), the residual variance order after the ﬁrst\ngradient step is opposite to the marginal variance order. If the diagonal entries are clamped at 0 (as is\nthe case in NOTEARS and corresponding to the setting shown on the right), the ﬁrst gradient step in\nthe above example leads to a scenario where the residual variance order follows the marginal variance\norder and where the resulting edge weight for the direction X ←Y overshoots the optimum, that is,\nthe blue curve’s minimum is attained for a smaller step size than the green curve’s minimum. The\nintuition is as follows: If we minimize the MSE the step size calibrates a trade-off between residual\nvariances in the different nodes; the high marginal variance nodes dominate the MSE such that the\nstep size that minimizes the MSE may result in ill-chosen edge weights for the edges incoming\ninto low-variance nodes. In the next optimization step, the gradient of the MSE loss for the edge\nX →Y pushes towards increasing that edge weight, while it pushes for decreasing the edge weight\n19\nX ←Y (besides a gradient contribution from the acyclicity constraint). As a result, the edge weights\nfor X →Y and X ←Y are equal after the ﬁrst step of NOTEARS, but better calibrated for the\ndirection from low- to high-variance nodes, which here corresponds to the correct edge X →Y . In\nthe subsequent optimization step, decreasing the edge weight X ←Y is favored both by the MSE\ngradient and the acyclicity penalty, while for the correct edge X →Y the MSE gradient pushes to\nfurther increasing the edge. Intuitively, if one needs to cut one of the two edges to avoid cycles, it is\n“cheaper” in MSE to cut the wrong edge X ←Y from a high- to low-variance node.\nE.2\nStepwise Gradient Derivation\nMSE\nFor X ∈Rn×d, the gradient of MSEX (W) = 1\nn∥X −XW∥2\n2 is\n∇W MSEX (W) = 1\nn∇W\n\u0000Tr[X⊤X] −Tr[W ⊤X⊤X] −Tr[X⊤XW] + Tr[W ⊤X⊤XW]\n\u0001\n= 1\nn\n\u0000−X⊤X −X⊤X + X⊤XW + X⊤XW\n\u0001\n= −2\nn\n\u0000X⊤X −X⊤XW\n\u0001\n= −2\nnX⊤(X −XW)\n∝X⊤(X −XW)\nIf W is polynomial in X⊤X, ∇W MSEX (W) is symmetric. ∇W MSEX (0d×d) = −2\nnX⊤X.\nGOLEM-EV\nThe gradient of the unnormalized negative likelihood-part of the GOLEM-EV objec-\ntive denoted as eLEV (W, X) is\n∇W eLEV (W, X) = d\n2∇W log(n MSEX (W))\n= d\n2\n1\nMSEX (W)∇W MSEX (W)\n∝\n1\nMSEX (W)X⊤(X −XW)\nIf W is polynomial in X⊤X, ∇W eLEV (W, X) is symmetric. ∇W eLEV (0d×d, X) = −\nd\n∥X∥2\n2 X⊤X.\nGOLEM-NV\nThe gradient of the unnormalized negative likelihood-part of the GOLEM-NV objec-\ntive denoted as eLNV (W, X) is\n∇W eLNV (W, X) = 1\n2\nd\nX\nj=1\n∇W log(n MSEj (wj))\n=\n\u0014\n−\n1\nn MSEj (wj)X⊤(xj −Xwj)\n\u0015\nj=1,...,d\n∝\n\u0014X⊤(xj −Xwj)\nMSEj (wj)\n\u0015\nj=1,...,d\nFor the zero matrix, we have ∇W eLNV (0d×d, X) = −X⊤X diag\n\u0000∥x1∥−2\n2 , ..., ∥xd∥−2\n2\n\u0001\n.\nWe focus on the gradients of MSE, LEV , and LNV since l1 penalty, acyclicity penalty h, LogDet\nterm, and exact scaling of eLEV and eLNV play a subordinate role at the zero initialization, where the\nLogDet gradient has zero off-diagonals and ∇W h vanishes:\nThe LogDet in GOLEM-EV and GOLEM-NV\nLogDet(W) = log(det(I −W)) has gradient\n∇W LogDet(W) = −(I −W)−⊤\nand vanishes when W is the adjacency matrix of a DAG [Ng et al., 2020]. If W is symmetric,\n∇W LogDet(W) is symmetric. For the zero matrix, we have ∇W LogDet(0d×d) = −I.\n20\nAcyclicity Penalty\/Constraint\nThe function h(W) = tr(exp(W ⊙W)) −d has gradient\n∇W h(W) = exp(W ⊙W)⊤⊙2W. The h(W)=0-level set characterizes adjacency matrices\nof DAGs [Zheng et al., 2018]. If W is symmetric, ∇W h(W) is symmetric. For the zero matrix, we\nhave h(0d×d) = 0 and ∇W h(0d×d) = 0d×d.\nE.3\nIncreasing Marginal and Residual Variances\nWe observe a strong positive correlation between the ordering by marginal variance and the ordering\nby residual variance after the ﬁrst gradient step when minimizing a MSE- or likelihood-based objective\nfunction via gradient descent with small step size (as in GOLEM-EV\/-NV). For small step sizes and\nlearning rates, marginal variance order and residual variance order are perfectly aligned for the ﬁrst\nfew optimization steps. Here we argue for a MSE-based loss function why the residual variance\nfollows the order of increasing marginal variance after the ﬁrst optimisation step with sufﬁciently\nsmall step size. Future work may investigate subsequent optimisation steps and the non-MSE terms\nof the objective functions.\nConsider the data matrix X ∈Rn×d. Without loss of generality, we assume the columns are zero-\ncentred and ordered such that the sequence of diagonal entries diag(X⊤X) is weakly monotonically\nincreasing. The diagonal entries diag(X⊤X) correspond to (n-times) the marginal variances at\nstep 0. After the ﬁrst gradient step with step size α in direction −∇W MSEX (0d×d) = 2\nnX⊤X (cf.\nAppendix E.2) the vector of (n-times) the residual variances is\nR = diag\n\u0000[X −aXX⊤X]⊤[X −aXX⊤X]\n\u0001\n= diag (D) −2a diag\n\u0000D2\u0001\n+ a2 diag\n\u0000D3\u0001\nwhere D = X⊤X and a =\n2\nnα. For each coordinate i the residual variance Ri is a continuous\nfunction in a (and α). For a = 0 and every i ∈[1, ..., d −1] we have Ri+1 −Ri = Di+1 −Di ≥0\nwith strict inequality if the variable pair i, i + 1 is varsortable. Due to continuity, for any pair of\nvariables with unequal marginal variances, there exists a sufﬁciently small step size to ensure that the\nresulting residual variances follow the same order as the marginal variances.\nE.4\nGradient Asymmetry\nWe combine what we laid out in Appendix E.2.\nThe GOLEM-EV optimization problem is\nargmin\nW\neLEV (W, X) −LogDet(W) + λ1∥W∥1 + λ2h(W)\nwith the following gradient of the objective function\n−\nd\nn MSEX (W)X⊤(X −XW) + (I −W)−⊤+ λ1W ⊘|W| + λ2 exp(W ⊙W)⊤⊙2W\nwhich at zero reduces to\n−\nd\n∥X∥2\n2\nX⊤X + I.\nThe GOLEM-NV optimization problem is\nargmin\nW\neLNV (W, X) −LogDet(W) + λ1∥W∥1 + λ2h(W)\nwith the following gradient of the objective function\n\u0014\n−\n1\nn MSEj (wj)X⊤(xj −Xwj)\n\u0015\nj=1,...,d\n+(I −W)−⊤+λ1W ⊘|W|+λ2 exp(W ⊙W)⊤⊙2W\nwhich at zero reduces to\n−X⊤X diag(∥x1∥−2\n2 , ..., ∥xd∥−2\n2 ) + I.\nThe gradient in GOLEM-EV is symmetric at 0d×d at the ﬁrst gradient descent step, but not in general\nfor later steps. The gradient in GOLEM-NV is in general not symmetric and at 0d×d (at the ﬁrst\n21\ngradient descent step) the gradients for edges incoming into a node are inversely scaled by its marginal\nvariance; consequently, for weights wi→j and wj→i of opposing edges the ﬁrst gradient step is larger\nmagnitude for the direction with lower-variance end-note and wi→j is preferred over wj→i if the\nvariance of Xi is higher than that of Xj. Under high-varsortability, the ﬁrst GOLEM-NV gradient\nstep thus tends to favor edges in anti-causal direction over those in causal direction.\nE.5\nNOTEARS\nThe NOTEARS optimization problem is argminW\n1\n2 MSEX (W) s.t. h(W) = 0 which is solved via\nthe augmented Lagrangian method and dual descent [Zheng et al., 2018] (we omit the penalty term\nfor the NOTEARS-l1 variant). In the original implementation, the algorithm is initialized at 0d×d and\nthe diagonal of W is not updated but ﬁxed to zero (this amounts to dual projected descent, where the\nadjacency matrix is projected onto the matrices with zero diagonal at each step avoiding self-loops\nper ﬁat).\nThe augmented Lagrangian\n1\n2 MSEX (W) + ρ\n2h(W)2 + αh(W)\nhas gradient\n−1\nnX⊤(X −XW) + (ρh + α)\n\u0000exp(W ⊙W)⊤⊙2W\n\u0001\nwhich at zero reduces to\n−1\nnX⊤X\nThe step size of the ﬁrst gradient step in direction ∝X⊤X is optimized by line-search to minmize\nthe overall MSE. As seen in the example in Appendix E.1, the residual variances may or may not\nfollow the order of the marginal variances after this ﬁrst step due to the step size being larger than the\nsmall step size that would ensure agreement between the orders (cf. Appendix E.3). Nonetheless, the\nstep size optimized by line-search aims to optimize the overall MSE which tends to favor a better ﬁt\nfor edges incoming into nodes with high-marginal variance. As a result, the ﬁrst gradient step results\nin edge weights that are better calibrated for edges incoming into high-marginal variance nodes than\ninto low-marginal variance nodes. In subsequent steps of the dual ascent procedure with increasing\nacyclicity penalty, the reduction of overall MSE stands at odds with satisfying the DAG constraints;\nit is then more costly in terms of MSE to change the weights for edges into high-marginal nodes\nthan into low-marginal nodes such that predominantly the edges into low-variance nodes tend to be\nremoved to eventually satisfy the acyclicity constraint. Under high varsortability, this amounts to a\npreference for causal edges.\nF\nStandardization Is Not Enough and Regression Coefﬁcients Tend to\nIncrease Along the Causal Order\nCode to reproduce the calculations and results in this section is available at https:\/\/github.com\/\nScriddie\/Varsortability.\nF.1\nInﬁnite Sample\nHere, we ﬁrst discuss the three-variable case to complement the intuition provided in the main\ntext. Consider the following ground-truth linear additive acyclic models, where the second model\ncorresponds to a standardization of the ﬁrst, and the third model corresponds to a re-scaled version of\nthe ﬁrst following Mooij et al. [2020]:\nRaw ground-truth model\nStandardized model\nScale-harmonized model\nA := NA\nAs := A\/√\nVar(A)\nAm := NA\nB := βA→BA + NB\nBs := B\/√\nVar(B)\nBm :=\nβA→B\np\nβ2\nA→B + 1\nAm + NB\nC := βB→CB + NC\nCs := C\/√\nVar(C)\nCm :=\nβB→C\np\nβ2\nB→C + 1\nBm + NC\n22\nwhere, following common benchmark sampling schemes, NA, NB, and NC are independent zero-\ncentred noise terms that follow some distributions with non-vanishing standard deviations σA, σB,\nand σC sampled independently from Unif(.5, 2) and where βA→B and βB→C are independently\ndrawn from Unif((−2, −.5) ∪(.5, 2)). For any two nodes X and Y , βX→Y denotes an underlying\nmodel parameter, while bβX→Y denotes the ordinary least-squares linear regression coefﬁcient when\nregressing Y onto X which is given as bβX→Y = Cov(X,Y )\nVar(X) .\nGiven observations from a variable triplet (X, Y, Z), the causal chain orientation task is to infer\nwhether the data generating causal chain is X →Y →Z, that is, (X, Y, Z) = (A, B, C) or\nZ ←Y ←X, that is, (Z, X, Y ) = (A, B, C). While both graphs are Markov equivalent, we can\nidentify the correct orientation of the causal chain, for all three considered scaling regimes, with\naccuracy strictly greater than 50% by applying the following procedure:\nChain orientation rule:\n• If |bβX→Y | < |bβY →Z| and |bβZ→Y | > |bβY →X|, conclude (X, Y, Z) = (A, B, C).\nWe conclude that X →Y →Z, if the regression coefﬁcients are increasing in magnitude\nwhen regressing pairwise from “left to right”.\n• If |bβX→Y | > |bβY →Z| and |bβZ→Y | < |bβY →X|, conclude (X, Y, Z) = (C, B, A).\nWe conclude that X ←Y ←Z, if the regression coefﬁcients are increasing in magnitude\nwhen regressing pairwise from “right to left”.\n• Otherwise, ﬂip a coin to decide the orientation of the underlying causal chain.\nFor each data scale regime, we can obtain the population regression coefﬁcients and express those in\nterms of the sampled model coefﬁcients βA→B, βB→C, σA, σB, σC:\n• Raw ground-truth model\n– “left to right”: bβA→B = βA→B\nand\nbβB→C = βB→C\n– “right to left”: bβC→B =\nβB→C(β2\nA→Bσ2\nA+σ2\nB)\nβ2\nA→Bβ2\nB→Cσ2\nA+β2\nB→Cσ2\nB+σ2\nC\nand\nbβB→A =\nβA→Bσ2\nA\nβ2\nA→Bσ2\nA+σ2\nB\n• Standardized model\n– “left to right”:\nbβAs→Bs =\nβA→Bσ2\nA\n√\nβ2\nA→Bσ2\nA+σ2\nB\n√\nσ2\nA\nand\nbβBs→Cs =\nβB→C√\nβ2\nA→Bσ2\nA+σ2\nB\n√\nβ2\nA→Bβ2\nB→Cσ2\nA+β2\nB→Cσ2\nB+σ2\nC\n– “right to left”:\nbβCs→Bs =\nβB→C√\nβ2\nA→Bσ2\nA+σ2\nB\n√\nβ2\nA→Bβ2\nB→Cσ2\nA+β2\nB→Cσ2\nB+σ2\nC\nand\nbβBs→As =\nβA→Bσ2\nA\n√\nβ2\nA→Bσ2\nA+σ2\nB\n√\nσ2\nA\n• Scale-harmonized model\n– Regression coefﬁcients “from left to right”:\nbβAm→Bm =\nβA→B\n√\nβ2\nA→B+1\nand\nbβBm→Cm =\nβB→C\n√\nβ2\nB→C+1\n– Regression coefﬁcients “from right to left”:\nbβCm→Bm =\nβB→C(β2\nB→C+1)\n1.5(β2\nA→Bσ2\nA+σ2\nB(β2\nA→B+1))\nβ2\nA→Bβ2\nB→Cσ2\nA(β2\nB→C+1)+β2\nB→Cσ2\nB(β2\nA→B+1)(β2\nB→C+1)+σ2\nC(β2\nA→B+1)(β2\nB→C+1)\n2\nand\nbβBm→Am =\nβA→Bσ2\nA\n√\nβ2\nA→B+1\nβ2\nA→Bσ2\nA+σ2\nB(β2\nA→B+1)\nWe obtain the following probabilities by Monte Carlo approximation, resampling the 5 model\nparameters 100, 000 times:\n23\nTable 3: Chain orientation results in the population limit.\nWeight distribution\nChain orientation rule cases\nUnif((−2, .5) ∪(.5, 2))\nP\nh\n|bβA→B| < |bβB→C| and |bβC→B| > |bβB→A|\ni\n29.376%\nP\nh\n|bβA→B| > |bβB→C| and |bβC→B| < |bβB→A|\ni\n5.486%\nP [“orientation rule correct on raw data”]\n61.945%\nP\nh\n|bβAs→Bs| < |bβBs→Cs| and |bβCs→Bs| > |bβBs→As|\ni\n73.181%\nP\nh\n|bβAs→Bs| > |bβBs→Cs| and |bβCs→Bs| < |bβBs→As|\ni\n26.819%\nP [“orientation rule correct on standardized data”]\n73.181%\nP\nh\n|bβAm→Bm| < |bβBm→Cm| and |bβCm→Bm| > |bβBm→Am|\ni\n31.631%\nP\nh\n|bβAm→Bm| > |bβBm→Cm| and |bβCm→Bm| < |bβBm→Am|\ni\n17.318%\nP [“orientation rule correct on scale-harmonized data”]\n57.1565%\nUnif((−.9, −.5) ∪(.5, .9))\nP\nh\n|bβA→B| < |bβB→C| and |bβC→B| > |bβB→A|\ni\n31.033%\nP\nh\n|bβA→B| > |bβB→C| and |bβC→B| < |bβB→A|\ni\n18.124%\nP [“orientation rule correct on raw data”]\n56.454%\nP\nh\n|bβAs→Bs| < |bβBs→Cs| and |bβCs→Bs| > |bβBs→As|\ni\n62.231%\nP\nh\n|bβAs→Bs| > |bβBs→Cs| and |bβCs→Bs| < |bβBs→As|\ni\n37.769%\nP [“orientation rule correct on standardized data”]\n62.231%\nP\nh\n|bβAm→Bm| < |bβBm→Cm| and |bβCm→Bm| > |bβBm→Am|\ni\n30.025%\nP\nh\n|bβAm→Bm| > |bβBm→Cm| and |bβCm→Bm| < |bβBm→Am|\ni\n20.607%\nP [“orientation rule correct on scale-harmonized data”]\n54.709%\nUnif((−.9, −.1) ∪(.1, .9))\nP\nh\n|bβA→B| < |bβB→C| and |bβC→B| > |bβB→A|\ni\n32.480%\nP\nh\n|bβA→B| > |bβB→C| and |bβC→B| < |bβB→A|\ni\n24.012%\nP [“orientation rule correct on raw data”]\n54.234%\nP\nh\n|bβAs→Bs| < |bβBs→Cs| and |bβCs→Bs| > |bβBs→As|\ni\n55.790%\nP\nh\n|bβAs→Bs| > |bβBs→Cs| and |bβCs→Bs| < |bβBs→As|\ni\n44.210%\nP [“orientation rule correct on standardized data”]\n55.790%\nP\nh\n|bβAm→Bm| < |bβBm→Cm| and |bβCm→Bm| > |bβBm→Am|\ni\n31.867%\nP\nh\n|bβAm→Bm| > |bβBm→Cm| and |bβCm→Bm| < |bβBm→Am|\ni\n25.136%\nP [“orientation rule correct on scale-harmonized data”]\n53.3655%\nWe draw edge weights independently from the uniform distribution indicated in the ﬁrst column\nof Table 3 and noise standard-deviations σA, σB, σC are drawn independently from Unif(.5, 2)\nin all cases. A 99% conﬁdence interval for the orientation accuracy under random guessing is\n(49.593%, 50.407%). The orientation rule achieves above chance accuracy in all regimes.\nF.2\nFinite Sample\nGiven observations from (X1, ..., Xd) generated by a linear ANM with either X1 →X2 →... →Xd\nor Xd →Xd−1 →... →X1, we can decide the directionality by identifying the direction in which\nthe absolute values of the regression coefﬁcients tend to increase. More precisely, we compare the\nsequences of absolute regression coefﬁcients\n“left-to-right regression coefﬁcients” |bβX1→X2|, ..., |bβXd−1→Xd|\nto\n“right-to-left regression coefﬁcients” |bβXd→Xd−1|, ..., |bβX2→X1|.\n24\nWe infer X1 →... →Xd if the former is in better agreement with an ascending sorting than the latter\nand infer Xd →... →X1 otherwise.\nIn the main text, we discussed the case for standardized data where the regression coefﬁcients for any\ntwo nodes Xi and Xj are given as | Corr(Xi, Xj)|. We expect the sequence of absolute regression\ncoefﬁcients to increase along the causal order because the correlation between consecutive nodes\ntends to be higher further downstream as parent nodes contribute more to a nodes marginal variance\nrelative to its noise term.\nOn the raw data scale, the sequences of regression coefﬁcients are\n“left-to-right”\n√\nVar(X2)\n√\nVar(X1)| Corr(X1, X2)|, ...,\n√\nVar(Xd)\n√\nVar(Xd−1)| Corr(Xd−1, Xd)|\nand\n“right-to-left”\n√\nVar(Xd−1)\n√\nVar(Xd) | Corr(Xd−1, Xd)|, ...,\n√\nVar(X1)\n√\nVar(X2)| Corr(X1, X2)|.\nOn both raw and standardized data, we ﬁnd that the direction in which absolute regression coefﬁcients\ntend to increase most corresponds to the causal direction in more than 50% of cases. To quantify\n“increasingness” of sequences of absolute regression coefﬁcients we count the number of correctly\nordered pairs of regression coefﬁcients, that is, how often a regression coefﬁcient is smaller in\nmagnitude than regression coefﬁcients later in the sequence and substract the number of discordant\npairs. The decision rule then predicts the direction in which the sequence of regression coefﬁcients is\nmore increasing according to this criterion.\nWe apply this orientation rule to simulated data (sample size 1000) for varying chain lengths and edge\ndistributions, and when applied to raw observational data, standardized observational data, and data\nwhen the parameters were scale-harmonized as per Mooij et al. [2020]. The table below establishes,\nthat for iid distributed parameters of the underlying data generating process, the orientation of a\ncausal chain can be identiﬁed with probability strictly greater than 50%.\nTable 4: Empirical Chain Orientation Results\naccuracy by variance-sorting\naccuracy by coefﬁcient-sorting\nd\nedge range\nraw\nstandardized\nharmonized\nraw\nstandardized\nharmonized\n3\n±(0.5, 2.0)\n97.50%\n50.05%\n84.70%\n62.58%\n73.03%\n57.30%\n±(0.5, 0.9)\n80.38%\n50.05%\n69.62%\n57.15%\n62.38%\n55.65%\n±(0.1, 0.9)\n65.65%\n50.30%\n60.08%\n54.17%\n55.88%\n53.45%\n5\n±(0.5, 2.0)\n98.67%\n50.15%\n82.17%\n78.60%\n86.58%\n64.20%\n±(0.5, 0.9)\n77.65%\n49.27%\n66.30%\n61.83%\n68.65%\n57.50%\n±(0.1, 0.9)\n63.08%\n50.38%\n57.65%\n58.17%\n57.33%\n56.35%\n10\n±(0.5, 2.0)\n99.38%\n50.02%\n79.30%\n93.72%\n96.97%\n69.08%\n±(0.5, 0.9)\n73.75%\n50.25%\n62.00%\n64.97%\n70.70%\n58.50%\n±(0.1, 0.9)\n62.55%\n51.23%\n58.25%\n55.85%\n56.05%\n54.40%\nA 99% conﬁdence interval for the orientation accuracy under random guessing is (47.975%, 52.025%)\n(1000 repetitions for each of the four noise types). Thus, variance-sorting on the standardized data\nis the only setting in which no above-chance orientation accuracy is achieved. This is expected, as\nvariance sorting amounts to a random sorting once nodes are standardized.\nG\nEmpirical Evaluation of Varsortability\nWe empirically estimate expected varsortability for our experimental set-up and a non-linear version\nof our experimental set-up by calculating the fraction of directed paths that are correctly sorted by\nmarginal variance in the randomly sampled ANMs.\nG.1\nVarsortability in Linear Additive Noise Models\nConsistent with our theoretical results, varsortability is close to 1 across all graph and noise types in\nour experimental set-up, cf. Table 5. Varsortability is higher in denser than in sparser graphs.\n25\nTable 5: Empirical varsortability in our experimental linear ANM set-up. Average varsortability is\nhigh in all settings. Our parameter choices are common in the literature. We sample 1000 observations\nof ten 50-node graphs for each combination of graph and noise type.\nvarsortability\nmin\nmean\nmax\ngraph\nnoise\nER-1\nGauss-EV\n0.94\n0.97\n0.99\nexponential\n0.94\n0.97\n0.99\ngumbel\n0.94\n0.97\n1.00\nER-2\nGauss-EV\n0.97\n0.99\n1.00\nexponential\n0.97\n0.99\n1.00\ngumbel\n0.98\n0.99\n0.99\nER-4\nGauss-EV\n0.98\n0.99\n0.99\nexponential\n0.98\n0.99\n0.99\ngumbel\n0.98\n0.99\n0.99\nSF-4\nGauss-EV\n0.98\n1.00\n1.00\nexponential\n0.98\n1.00\n1.00\ngumbel\n0.98\n1.00\n1.00\nG.2\nVarsortability in Non-Linear Additive Noise Models\nTable 6 shows varsortabilities for a non-linear version of our experimental set-up as used by Zheng\net al. [2020]. While the ﬂuctuations in Table 6 are greater than in Table 5, all settings exhibit\nhigh varsortability on average. Our ﬁndings indicate that varsortability is a concern for linear and\nnon-linear ANMs.\nTable 6: Empirical varsortability in non-linear ANM. Average varsortability is high in all settings.\nOur parameter choices are common in the literature. We sample 1000 observations of ten 20-node\ngraphs for each combination of graph and ANM-type.\nvarsortability\nmin\nmean\nmax\ngraph\nANM-type\nER-1\nAdditive GP\n0.81\n0.91\n1.00\nGP\n0.72\n0.86\n0.96\nMLP\n0.55\n0.79\n0.96\nMulti Index Model\n0.62\n0.82\n1.00\nER-2\nAdditive GP\n0.79\n0.91\n0.98\nGP\n0.82\n0.89\n0.97\nMLP\n0.46\n0.71\n0.87\nMulti Index Model\n0.65\n0.79\n0.89\nER-4\nAdditive GP\n0.90\n0.95\n0.98\nGP\n0.74\n0.88\n0.93\nMLP\n0.59\n0.72\n0.85\nMulti Index Model\n0.57\n0.73\n0.85\nSF-4\nAdditive GP\n0.95\n0.97\n0.99\nGP\n0.88\n0.94\n0.97\nMLP\n0.75\n0.83\n0.93\nMulti Index Model\n0.77\n0.84\n0.97\nG.3\nCausal Order and Marginal Variance\nWe observe strong empirical evidence in Figure 2 that marginal variance tends to increase quickly\nalong the causal order, even if the settings are not guaranteed to yield high expected varsortability\nbetween a pair of root cause and effect (for example, if all edges are chosen in a small-magnitude\nrange). This indicates that high levels of varsortability can scarcely be avoided on larger graphs.\n26\n0\n1\n2\n3\n4\nPosition in causal order\n5\n10\n15\n20\n25\n30\nAverage marginal variance\nEdge weight range\n(0.1, 0.3)\n(0.3, 0.5)\n(0.5, 0.7)\n(0.7, 0.9)\nFigure 2: Average marginal variance along the causal order for 1000 observations of 1000 simulated\n30-node ER-2 graphs with Gaussian noise standard deviations sampled uniformly in (0.5, 2) for each\nedge weight range. Edge weights are drawn independently and uniformly from the union of negative\nand positive of the indicated edge range, that is, for example, the edge weights for the red curve are\ndrawn from Unif((−.9, −.7) ∪(.7, .9)).\nG.4\nVarsortability Algorithm\nThe implementation is also available at https:\/\/github.com\/Scriddie\/Varsortability.\nimport numpy as np\ndef\nv a r s o r t a b i l i t y (X, W,\nt o l =1e −9):\n\"\"\"\nTakes n x d data and a d x d adjaceny\nmatrix ,\nwhere\nthe\ni , j −th\ne n t r y\ncorresponds\nto\nthe\nedge\nweight\nf o r\ni −>j ,\nand\nr e t u r n s\na value\ni n d i c a t i n g\nhow well\nthe\nvariance\norder\nr e f l e c t s\nthe\ncausal\norder .\n\"\"\"\nE = W != 0\nEk = E . copy ( )\nvar = np . var (X,\na x i s =0 ,\nkeepdims=True )\nn_paths = 0\nn _ c o r r e c t l y _ o r d e r e d _ p a t h s = 0\nfor _ in\nrange (E . shape [ 0 ] −1 ) :\nn_paths += Ek . sum ( )\nn _ c o r r e c t l y _ o r d e r e d _ p a t h s += ( Ek * var\n\/\nvar . T > 1 +\nt o l ) . sum ( )\nn _ c o r r e c t l y _ o r d e r e d _ p a t h s += 1\/2*(\n( Ek * var\n\/\nvar . T <= 1 +\nt o l ) *\n( Ek * var\n\/\nvar . T >\n1 −t o l ) ) . sum ( )\nEk = Ek . dot (E)\nreturn\nn _ c o r r e c t l y _ o r d e r e d _ p a t h s\n\/\nn_paths\ni f\n__name__ == \" __main__ \" :\nW = np . a r r a y ( [ [ 0 ,\n1 ,\n0] ,\n[0 ,\n0 ,\n2] ,\n[0 ,\n0 ,\n0 ] ] )\nX = np . random . randn (1000 ,\n3 ) . dot ( np . l i n a l g . inv ( np . eye ( 3 ) −W) )\nprint ( \" V a r s o r t a b i l i t y : \" ,\nv a r s o r t a b i l i t y (X, W) )\nX_std = (X −np . mean (X,\na x i s = 0 ) ) \/ np . s t d (X,\na x i s =0)\nprint ( \" V a r s o r t a b i l i t y\ns t a n d a r d i z e d : \" ,\nv a r s o r t a b i l i t y ( X_std , W) )\n27\nH\nsortnregress: A Diagnostic Tool to Reveal Varsortability\nIn Section 3.5 we introduce sortnregress as a simple baseline method. In the following subsections,\nwe provide Python code that implements sortnregress thereby establishing its ease and illustrate how\nits DAG recovery performance reﬂects varying degrees of varsortability.\nH.1\nImplementation of Sortnregress\nThe implementation is also available at https:\/\/github.com\/Scriddie\/Varsortability.\nimport numpy as np\nfrom\ns k l e a r n . linear_model\nimport\nLinearRegression ,\nLassoLarsIC\ndef\ns o r t n r e g r e s s (X) :\n\"\"\"\nTake n x d data ,\norder\nnodes by\nmarginal\nvariance\nand\nr e g r e s s e s\neach node\nonto\nthose\nwith\nlower\nvariance ,\nusing\nedge\nc o e f f i c i e n t s\nas\ns t r u c t u r e\ne s t i m a t e s .\n\"\"\"\nLR = LinearRegression ( )\nLL = LassoLarsIC ( c r i t e r i o n = ’ bic ’ )\nd = X. shape [ 1 ]\nW = np . zeros ( ( d ,\nd ) )\ni n c r e a s i n g = np . a r g s o r t ( np . var (X,\na x i s =0))\nfor k in\nrange (1 , d ) :\nc o v a r i a t e s = i n c r e a s i n g [ : k ]\nt a r g e t\n= i n c r e a s i n g [ k ]\nLR. f i t (X[ : ,\nc o v a r i a t e s ] , X[ : ,\nt a r g e t ] . r a v e l ( ) )\nweight = np . abs (LR. coef_ )\nLL . f i t (X[ : ,\nc o v a r i a t e s ] * weight , X[ : ,\nt a r g e t ] . r a v e l ( ) )\nW[ c o v a r i a t e s ,\nt a r g e t ] = LL . coef_ * weight\nreturn W\nH.2\nVarsortabiltiy and Score Attainable by Variance Ordering\nIn Figure 3 we observe that sortnregress improves linearly with varsortability. For a varsortability\nof 0.93 as in our experimental settings (cf. Section 3.3), it recovers the structure near-perfectly.\nrandomregress uses a random ordering but is otherwise identical to sortnregress. The different ranges\nof varsortability can be classiﬁed as follows (n=30):\n• < 0.33: sortnregress performs signiﬁcantly worse than randomregress (p<1e-4)\n• 0.33–0.66: no signiﬁcant difference between sortnregress and randomregress (p=0.40)\n• > 0.66: sortnregress performs signiﬁcantly better than randomregress (p<1e-4)\n28\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVarsortability\n0\n10\n20\n30\n40\n50\nStructural Intervention Distance\nAlgorithm\nFGES\nDirectLiNGAM\nrandomregress\nsortnregress\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVarsortability\n0\n5\n10\n15\nStructural Hamming Distance\nAlgorithm\nFGES\nDirectLiNGAM\nrandomregress\nsortnregress\nFigure 3: Relationship between varsortability and score attainable through ordering by variance.\nResults shown for 10 simulated 10-node ER-1 graphs in each of 10 equally spaced varsortability\nbins. Note that for standard simulation settings most models have high varsortability. We use edge\nweights in (−0.5, −0.1) ∪(0.1, 0.5), Gumbel noise with standard deviations in (0.5, 2), and still\nneed to discard many models with high varsortability to obtain 10 instances per varsortability bin.\nI\nEvaluation on Real-World Data\nWe analyze a dataset on protein signaling networks obtained by Sachs et al. [2005]. We evaluate\nour algorithms on ten bootstrap samples of the observational part of the dataset consisting of 853\nobservations, 11 nodes, and 17 edges. Our results show that there is no dominating algorithm. On\naverage, most algorithms achieve performances similar to those of randomregress or the empty graph.\nNote that the results in terms of SHD are susceptible to thresholding choices and the empty graph\nbaseline outperforms a majority of the algorithms. Our results are in line with previous reports\n[Lachapelle et al., 2019, Ng et al., 2020]. We observe scale-sensitivity of the continuous learning\nalgorithms and sortnregress. However, in contrast to our simulation study in Section 4, the effect\nis small and inconsistent. The results do not show the patterns observed under high varsortability,\nwhich is consistent with the measured mean varsortability of 0.57 with a standard deviation of 0.01\nacross our bootstrapped samples.\n0\n10\n20\n30\n40\n50\n60\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\nempty graph\n0\n5\n10\n15\n20\n25\n30\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\nempty graph\nFigure 4: SID (left) and SHD (right) performance of combinatorial and continuous methods on\nreal-world data.\nJ\nModel Selection in Continuous Optimization\nWe illustrate the optimization landscape for the Gaussian MLE under Gaussian noise. This cor-\nresponds to the loss of GOLEM-NV as stated in Appendix D with a sparsity penalty of zero. We\ncompare vanilla MLE to MLE with Lasso regularization for raw and standardized data. In Figure 5\nwe show the loss landscape in terms of SID and SHD difference to the true structure and highlight\nglobal optima. In the case of tied scores between the true structure and an alternative structure we\nselect the true structure. For MLE with Lasso regularization using a penalty of 0.1, the optimal loss\nis achieved by the true structure more frequently under standardization (red dots accumulate in the\n29\nbottom left corner). Our result indicates that the Lasso sparsity penalty is inﬂuenced by the data scale\nand is better calibrated on standardized data. It is not unexpected that penalization is scale dependent,\na problem that is, for example, discussed in applications of Ridge regression.\n0\n1\n2\n3\n4\n5\n6\nStructural Intervention Distance\n0\n1\n2\n3\nStructural Hamming Distance\nMLE\nRaw\nOptimum\n0\n1\n2\n3\n4\n5\n6\nStructural Intervention Distance\n0\n1\n2\n3\nStructural Hamming Distance\nStandardized\nOptimum\n0\n1\n2\n3\n4\n5\n6\nStructural Intervention Distance\n0\n1\n2\n3\nStructural Hamming Distance\nMLE+l1\nOptimum\n0\n1\n2\n3\n4\n5\n6\nStructural Intervention Distance\n0\n1\n2\n3\nStructural Hamming Distance\nOptimum\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: Standardized loss landscape for all 25 candidate graphs relative to each of the 25 possible\n3-node ground-truth structures (a total of 25 × 25 candidate-true graph pairs). The loss is scaled to\n[0, 1], see colorbar.\nK\nDetailed Results\nWe provide a comprehensive overview over our empirical DAG\/MEC recovery results for different\nevaluation metrics, graph types, and graph sizes.\nK.1\nMEC Recovery\nAn analysis of MEC recovery allows us to distinguish whether any drops in performance are within\nthe expectations of identiﬁability. We evaluate the discovery of the MEC of the ground-truth DAG\nin a Gaussian setting with non-equal noise variances where only the ground-truth MEC but not the\nground-truth DAG are identiﬁable. Since evaluating the SID between Markov equivalence classes is\ncomputationally expensive and prohibitively so for large graphs, we restrict ourselves to the setting\nhere. When comparing MEC, we choose the upper limit of SID differences in Figure 1 in the main\ntext. In Figure 6 we show that the relative performances are similar for the lower SID limit.\n0\n20\n40\n60\n80\nMEC Structural Intervention Distance (lower bound)\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\nFigure 6: Lower bound of SID in MEC recovery for 10 node ER-2 graphs with non-equal Gaussian\nnoise.\n30\nWe conclude that the drop in performance extends from the recovery of the DAG to the recovery of\nthe MEC and therefore goes beyond the difﬁculty of identifying the correct DAG within a MEC.\nK.2\nResults Across Thresholding Regimes\nTo ensure the effects we observe constitute a general phenomenon, we evaluate algorithm performance\nfor different thresholding regimes. This is especially critical on standardized data. By re-scaling\nthe data, standardization may impact the correct edge weights between nodes, potentially pushing\nthem outside the thresholding range. Following Zheng et al. [2018], Ng et al. [2020], we perform\nthresholding for the continuous structure learning algorithms and prune edges with an edge weight in\nthe recovered adjacency matrix of less than 0.3. If the returned graph is not acyclic, we iteratively\nremove the edge with the smallest magnitude weight until all cycles are broken. We ﬁnd that the\nqualitative performance differences between raw and standardized data are robust to a wide range of\nthreshold choices.\nFigure 7a and Figure 7b show SID performance for different thresholds. Even though the thresholds\nare orders of magnitude apart, a comparison reveals that the relative performances are nearly identical.\nWe observe that SHD performance is also robust across different thresholding regimes. Figure 7c\nshows performance using favorable thresholding. In this regime, the threshold leading to the most\nfavorable SHD performance is applied to each instance individually. Figure 7d shows performance\nfor a ﬁxed threshold of 0.3. A comparison reveals nearly identical relative performances in both\ncases.\nOverall, we observe that the effect of varsortability is present even for the most favorable threshold in\ncase of SHD, and for a wide range of thresholds in case of SID, where computation of a favorable\nthreshold is computationally infeasible.\n0\n500\n1000\n1500\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(a) SID, threshold=0.001\n0\n500\n1000\n1500\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(b) SID, threshold=0.3\n0\n100\n200\n300\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(c) SHD, favorable thresholding\n0\n100\n200\n300\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(d) SHD, threshold=0.3\nFigure 7: Results for different thresholding regimes. Gaussian-NV noise, ER-2 graph, 50 nodes.\nK.3\nResults Across Noise Distributions and Graph Types\nFigure 8 and Figure 9 show algorithm comparisons in terms of SID and SHD, respectively. The\ndifferences in performance on raw versus standardized data are qualitatively similar regardless of\nthe noise distribution. We showcase results for different graph types in the non-Gaussian setting.\nDirectLiNGAM performs well only in the non-Gaussian cases, as is expected based on its underlying\nidentiﬁability assumptions.\n31\n0\n500\n1000\n1500\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(a) SID, Gaussian-NV noise, ER-2 graph, 50 nodes\n0\n500\n1000\n1500\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(b) SID, Gaussian-EV noise, ER-2 graph, 50 nodes\n0\n500\n1000\n1500\n2000\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(c) SID, Exponential noise, ER-4 graph, 50 nodes\n0\n500\n1000\n1500\n2000\nStructural Intervention Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(d) SID, Gumbel noise, SF-4 graph, 50 nodes\nFigure 8: SID results across noise types and for different graph types with 50 nodes\n0\n100\n200\n300\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(a) SHD, Gaussian-NV noise, ER-2, 50 nodes\n0\n100\n200\n300\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(b) SHD, Gaussian-EV noise, ER-2, 50 nodes\n0\n200\n400\n600\n800\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(c) SHD, Exponential Noise, ER-4, 50 nodes\n0\n200\n400\n600\nStructural Hamming Distance\nDirectLiNGAM\nFGES\nPC\nMSE-GDS\nNOTEARS\nGOLEM-EV\nGOLEM-NV\nsortnregress\nAlgorithm\nraw\nstandardized\ncombinatorial\ncontinuous\nrandomregress\n(d) SHD, Gumbel Noise, SF-4, 50 nodes\nFigure 9: SHD results across noise types and for different graph types with 50 nodes\n32\nK.4\nResults Across Noise Distributions, Graph Types, and Graph Sizes\nThe following experimental results largely follow earlier settings and results by Zheng et al. [2018],\nNg et al. [2020].\n20\n40\n0\n100\n200\n300\n400\nExponential\n20\n40\n0\n100\n200\n300\n400\nGaussian-EV\n20\n40\n0\n100\n200\n300\n400\nER-1\nGumbel\n20\n40\n0\n500\n1000\n20\n40\n0\n250\n500\n750\n1000\n1250\n20\n40\n0\n500\n1000\nER-2\n20\n40\n0\n500\n1000\n1500\n2000\n20\n40\n0\n500\n1000\n1500\n2000\n20\n40\n0\n500\n1000\n1500\n2000\nER-4\n20\n40\n0\n200\n400\n600\n800\n20\n40\n0\n200\n400\n600\n800\n1000\n20\n40\n0\n200\n400\n600\n800\nSF-4\nNodes\nStructural Intervention Distance\nFGES\nGOLEM-EV\nGOLEM-NV\nDirectLiNGAM\nNOTEARS\nPC\nsortnregress\nMSE-GDS\n(a) Raw data\n20\n40\n0\n100\n200\n300\n400\nExponential\n20\n40\n0\n100\n200\n300\nGaussian-EV\n20\n40\n0\n100\n200\n300\n400\nER-1\nGumbel\n20\n40\n0\n500\n1000\n1500\n20\n40\n0\n500\n1000\n1500\n20\n40\n0\n500\n1000\n1500\nER-2\n20\n40\n0\n500\n1000\n1500\n2000\n20\n40\n0\n500\n1000\n1500\n2000\n20\n40\n0\n500\n1000\n1500\n2000\nER-4\n20\n40\n0\n500\n1000\n1500\n2000\n20\n40\n0\n500\n1000\n1500\n2000\n20\n40\n0\n500\n1000\n1500\n2000\nSF-4\nNodes\nStructural Intervention Distance\nFGES\nGOLEM-EV\nGOLEM-NV\nDirectLiNGAM\nNOTEARS\nPC\nsortnregress\nMSE-GDS\n(b) Standardized data\nFigure 10: SID results across noise types, graph types, and graph sizes.\n10\n20\n30\n40\n50\n0\n20\n40\n60\nExponential\n10\n20\n30\n40\n50\n0\n20\n40\n60\n80\n100\nGaussian-EV\n10\n20\n30\n40\n50\n0\n20\n40\n60\nER-1\nGumbel\n10\n20\n30\n40\n50\n0\n50\n100\n10\n20\n30\n40\n50\n0\n50\n100\n150\n200\n250\n10\n20\n30\n40\n50\n0\n50\n100\nER-2\n10\n20\n30\n40\n50\n0\n100\n200\n300\n400\n10\n20\n30\n40\n50\n0\n100\n200\n300\n400\n10\n20\n30\n40\n50\n0\n100\n200\n300\n400\nER-4\n10\n20\n30\n40\n50\n0\n100\n200\n300\n10\n20\n30\n40\n50\n0\n100\n200\n300\n10\n20\n30\n40\n50\n0\n100\n200\n300\nSF-4\nNodes\nStructural Hamming Distance\nFGES\nGOLEM-EV\nGOLEM-NV\nDirectLiNGAM\nNOTEARS\nPC\nsortnregress\nMSE-GDS\n(a) Raw data\n10\n20\n30\n40\n50\n0\n25\n50\n75\n100\nExponential\n10\n20\n30\n40\n50\n0\n25\n50\n75\n100\nGaussian-EV\n10\n20\n30\n40\n50\n0\n25\n50\n75\n100\nER-1\nGumbel\n10\n20\n30\n40\n50\n0\n100\n200\n10\n20\n30\n40\n50\n100\n200\n300\n10\n20\n30\n40\n50\n0\n100\n200\nER-2\n10\n20\n30\n40\n50\n0\n200\n400\n600\n10\n20\n30\n40\n50\n0\n200\n400\n600\n10\n20\n30\n40\n50\n0\n200\n400\n600\nER-4\n10\n20\n30\n40\n50\n0\n200\n400\n600\n10\n20\n30\n40\n50\n0\n200\n400\n600\n10\n20\n30\n40\n50\n0\n200\n400\n600\nSF-4\nNodes\nStructural Hamming Distance\nFGES\nGOLEM-EV\nGOLEM-NV\nDirectLiNGAM\nNOTEARS\nPC\nsortnregress\nMSE-GDS\n(b) Standardized data\nFigure 11: SHD results across noise types, graph types, and graph sizes.\n33\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game.pdf"}
{"title":"OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research","authors":"Kai Li, Hang Xu, Enmin Zhao, Zhe Wu, Junliang Xing","summary":"Owning to the unremitting efforts by a few institutes, significant progress\nhas recently been made in designing superhuman AIs in No-limit Texas Hold'em\n(NLTH), the primary testbed for large-scale imperfect-information game\nresearch. However, it remains challenging for new researchers to study this\nproblem since there are no standard benchmarks for comparing with existing\nmethods, which seriously hinders further developments in this research area. In\nthis work, we present OpenHoldem, an integrated toolkit for large-scale\nimperfect-information game research using NLTH. OpenHoldem makes three main\ncontributions to this research direction: 1) a standardized evaluation protocol\nfor thoroughly evaluating different NLTH AIs, 2) four publicly available strong\nbaselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs\nfor public NLTH AI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,\nhoping it facilitates further studies on the unsolved theoretical and\ncomputational issues in this area and cultivate crucial research problems like\nopponent modeling and human-computer interactive learning.","url":"http:\/\/arxiv.org\/abs\/2012.06168v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2012.06168v4","published":1607671448000,"comment":null,"pdf_text":"JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n1\nOpenHoldem: A Benchmark for Large-Scale\nImperfect-Information Game Research\nKai Li, Member, IEEE, Hang Xu, Enmin Zhao, Zhe Wu, and Junliang Xing, Senior Member, IEEE\nAbstract—Owning to the unremitting efforts by a few institutes,\nsigniﬁcant progress has recently been made in designing superhu-\nman AIs in No-limit Texas Hold’em (NLTH), the primary testbed\nfor large-scale imperfect-information game research. However, it\nremains challenging for new researchers to study this problem\nsince there are no standard benchmarks for comparing with\nexisting methods, which seriously hinders further developments\nin this research area. In this work, we present OpenHoldem,\nan integrated toolkit for large-scale imperfect-information game\nresearch using NLTH. OpenHoldem makes three main contri-\nbutions to this research direction: 1) a standardized evaluation\nprotocol for thoroughly evaluating different NLTH AIs, 2) four\npublicly available strong baselines for NLTH AI, and 3) an\nonline testing platform with easy-to-use APIs for public NLTH\nAI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,\nhoping it facilitates further studies on the unsolved theoretical\nand computational issues in this area and cultivate crucial\nresearch problems like opponent modeling and human-computer\ninteractive learning.\nIndex Terms—Artiﬁcial Intelligence, Imperfect-Information\nGame, Nash Equilibrium, No-limit Texas Hold’em, Benchmark.\nI. INTRODUCTION\nFrom its inception, artiﬁcial intelligence (AI) research has\nbeen focusing on building agents that can play games like\nhumans. Both Turing [1] and Shannon [2] developed programs\nfor playing chess to validate initial ideas in AI. For more\nthan half a century, games have continued to be AI testbeds\nfor novel ideas, and the resulting achievements have marked\nimportant milestones in the history of AI [3]–[17]. Notable\nexamples include the checkers-playing bot Chinook winning\na world championship against top humans [3], Deep Blue\nbeating Kasparov in chess [4], and AlphaGo defeating Lee\nSedol [6] in the complex ancient Chinese game Go. Although\nsubstantial progress has been made in solving these large-scale\nperfect-information games that all players know the exact state\nof the game at every decision point, it remains challenging\nto solve large-scale imperfect-information games that require\nreasoning under the uncertainty about the opponents’ hidden\nKai Li, Hang Xu, and Enmin Zhao contributed equally to this work.\nJunliang Xing is the corresponding author.\nKai Li, Hang Xu, Enmin Zhao, Zhe Wu, and Junliang Xing are with the\nInstitute of Automation, Chinese Academy of Sciences, and School of Artiﬁ-\ncial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n(e-mail: kai.li@ia.ac.cn; xuhang2020@ia.ac.cn; zhaoenmin2018@ia.ac.cn;\nwuzhe2019@ia.ac.cn; jlxing@nlpr.ia.ac.cn).\nThis work was supported in part by the Natural Science Foundation\nof China under Grant No. 62076238 and 61902402, in part by the Na-\ntional Key Research and Development Program of China under Grant No.\n2020AAA0103401, in part by the CCF-Tencent Open Fund, and in part by\nthe Strategic Priority Research Program of Chinese Academy of Sciences\nunder Grant No. XDA27000000.\ninformation. The hidden information is omnipresent in real-\nworld strategic interactions, such as business, negotiation, and\nﬁnance, making the research of imperfect-information games\nparticularly important both theoretically and practically.\nPoker has a long history as a challenging problem for\ndeveloping algorithms that deal with hidden information [18],\n[19]. The poker game involves all players being dealt with\nsome private cards visible only to themselves, with players\ntaking structured turns making bets, calling opponents’ bets,\nor folding. As one of the most popular global card games,\npoker has played an essential role in developing general-\npurpose techniques for imperfect-information games. In par-\nticular, No-limit Texas Hold’em (NLTH), the world’s most\npopular form of poker, has been the primary testbed for\nimperfect-information game research for decades because of\nits large-scale decision space and strategic complexity. For\nexample, Heads-up No-limit Texas Hold’em (HUNL), the\nsmallest variant of NLTH, has 10161 decision points [20]\nwhich makes it almost impossible to solve directly.\nThere have been many efforts to design poker AIs for\nNLTH over the past few years [21], [22]. Most of these\nsystems exploit some equilibrium-ﬁnding algorithms, e.g.,\ncounterfactual regret minimization (CFR) [23], with various\nabstraction strategies to merge similar game states to reduce\nthe size of the game tree. Recently, a series of breakthroughs\nhave been made in the NLTH AI research community. Deep-\nStack [16], which combines the continual re-solving and the\ndepth-limited sparse look-ahead algorithms, defeated 10 out\nof 11 professional poker players by a statistically signiﬁcant\nmargin. Libratus [17] defeated a team of four top HUNL-\nspecialist professionals by using a nested safe subgame solving\nalgorithm with an extensible blueprint strategy. Pluribus [24]\ndefeated elite human professional players in six-player NLTH\nby extending the techniques behind Libratus.\nAlthough many important milestones have been achieved\nin NLTH AI research in recent years, the problem is far\nfrom being solved, and there remain many theoretical and\ncomputational issues to be addressed. For example, the game-\ntheoretic solution for multiplayer NLTH, the best way to game\ntree abstraction, more efﬁcient equilibrium-ﬁnding algorithms\nthat converge faster and consume fewer resources, etc. To\nsolve these challenges, further studies are urgently needed.\nHowever, one main obstacle to further research in NLTH AI\nis the lack of standard benchmarks in this area. First, there are\nno standard evaluation protocols in this community; different\npapers use different evaluation metrics, making comparisons\nof different methods difﬁcult. Second, there is no publicly\navailable baseline AI which can serve as a starting point for\narXiv:2012.06168v4  [cs.LG]  14 Dec 2021\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n2\nfuture improvements. Third, there are no public easy-to-use\nplatforms for researchers to test the performance of their AIs\nat any time.\nConsidering the important role of standard benchmarks in\nAI development, we present OpenHoldem, a benchmark for\nNLTH AI research developed to boost the studies on large-\nscale imperfect-information games. OpenHoldem provides an\nintegrated toolkit for evaluating NLTH AIs with three main\ncomponents: the evaluation protocols, the baseline AIs, and\na testing platform. For each component, we have made the\nfollowing contributions to the community:\n• For the evaluation part, we propose to use four differ-\nent evaluation metrics to test different algorithms from\ndifferent aspects comprehensively.\n• For the baseline part, we design and implement four\ndifferent types of NLTH AIs: rule-based AI, CFR based\nstatic AI, DeepStack-like online AI, and deep reinforce-\nment learning based AI. These diverse AIs can serve as\nstrong baselines for further development in this ﬁeld.\n• For the platform part, we develop an online testing\nplatform with multiple NLTH AIs built-in. Researchers\ncan link their AIs to this platform through easy-to-use\nAPIs to play against each other for mutual improvement.\nOur proposed OpenHoldem provides a standardized bench-\nmark for the NLTH AI research. The adopted approach,\nnamely to propose an evaluation protocol via several metrics,\nthe provision of baselines tested to have strong performances,\nand the establishment of an online testing platform, is per-\nfectly rigorous and will allow algorithm improvements and\ncomparisons with the state-of-the-arts, which impossible to do\ntoday without spending much time re-implementing other peo-\nple’s methods. OpenHoldem can potentially have a signiﬁcant\nimpact on the poker AI research, and more generally in the\nAI community dealing with decision-making problems under\nuncertainty. We hope that OpenHoldem makes the NLTH AI\nresearch easier and more accessible, and further facilitates\nthe research of the key problems in large-scale imperfect-\ninformation games, such as large-scale equilibrium-ﬁnding,\nopponent modeling, human-computer interactive learning, and\nonline exploiting sub-optimal opponents.\nII. RELATED WORK\nStandard benchmarks have played an indispensable role\nin promoting the research in many AI tasks like speech\nrecognition, computer vision, and natural language process-\ning. For example, in the task of speech to text, the NIST\nSwitchboard benchmark [25] helps reduce the word error rate\nfrom 19.3% in 2000 to 5.5% in 2017; In the task of image\nclassiﬁcation, the creation of the ImageNet [26] benchmark\nhas helped in the development of highly efﬁcient models\nwhich reduce the image classiﬁcation error rate from 26.2%\ndown to 1.8%; In the task of machine translation, the WMT\nbenchmark helps the machine translation system achieves\nhuman-level performance on the Chinese to English translation\ntask [27]. These benchmarks that have greatly inﬂuenced\nthe research communities have some common characteristics:\nclear evaluation metrics, rich baseline models, and convenient\nonline testing platforms. Motivated by this, we propose the\nOpenHoldem benchmark that meets the above requirements to\nfacilitate the future development of general-purpose techniques\nfor large-scale imperfect-information games.\nThere are already some benchmarks on game AI. Examples\ninclude the Atari environments in OpenAI Gym [28], ViZ-\nDoom [29], and MineRL [30], but most of these benchmarks\nare oriented towards the research of reinforcement learning\nalgorithms. Recently, some benchmarks for game theory re-\nsearch have been proposed. For example, Google DeepMind\nreleases the OpenSpiel [31] benchmark, which contains a\ncollection of environments and algorithms for research in n-\nplayer zero-sum and general-sum games. Although OpenSpiel\nimplements many different kinds of games and state-of-the-\nart algorithms, it currently does not provide high-performance\nNLTH AIs. RLCard [32] developed by the Texas A&M\nUniversity includes many large-scale complex card games,\nsuch as Dou dizhu, Mahjong, UNO, Sheng Ji, and NLTH.\nHowever, most of the implemented baseline AIs are relatively\nweak. In contrast, the proposed OpenHoldem contains very\nstrong baseline AIs, which can serve as a better starting point\nfor future improvements.\nTexas Hold’em, the primary testbed for imperfect informa-\ntion game research, has been studied in the computer poker\ncommunity for years [19]. The earliest Texas Hold’em AIs\nare rule-based systems that consist of a collection of if-then\nrules written by human experts. For example, the early agents\n(e.g., Loki [33]) produced by the University of Alberta are\nmostly based on carefully designed rules. While the rule-\nbased approach provides a simple framework for implementing\nTexas Hold’em AIs, the resulting handcrafted strategies are\neasily exploitable by observant opponents. Since 2006, the\nAnnual Computer Poker Competition (ACPC) [34] has greatly\nfacilitated poker AI development, and many game-theoretic\nTexas Hold’em AIs are proposed [21], [22]. These systems\nﬁrst use various abstraction strategies [35], [36] to merge\nsimilar game states to reduce the game size, then exploit some\nequilibrium-ﬁnding algorithms (e.g., CFR [23] and its various\nvariants [37]–[40]) to ﬁnd the approximate Nash equilibrium\nstrategies which are robust to different opponents.\nRecently, the research on these game-theoretic approaches\nhas made signiﬁcant breakthroughs. Examples include Deep-\nStack [16] proposed by the University of Alberta that defeats\nprofessional poker players by a large margin, Libratus [17]\nfrom the Carnegie Mellon University that decisively defeats\nfour top HUNL-specialist professionals, and Pluribus [24]\nas a direct descendant of Libratus that defeats elite human\nprofessional players in six-player NLTH. Nevertheless, almost\nall of these Texas Hold’em AIs are not publicly available,\nmaking it very challenging for new researchers to study this\nproblem further. Our OpenHoldem is the ﬁrst open benchmark\nwith publicly available strong baseline AIs for large-scale\nimperfect-information game research.\nIII. PRELIMINARIES\nHere we present some background knowledge needed for\nthe rest of the paper. We ﬁrst provide some notations to\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n3\nformulate imperfect-information games. Next, we discuss the\nCFR algorithm which is the most commonly used equilibrium-\nﬁnding algorithm for imperfect-information games. Finally, we\nintroduce the game rule of no-limit Texas Hold’em.\nA. Imperfect-Information Games\nImperfect-information games are usually described by a\ntree-based formalism called extensive-form games [41]. In an\nimperfect-information extensive-form game G there is a ﬁnite\nset N = {1,. . ., N} of players, and there is also a special\nplayer c called chance; H refers to a ﬁnite set of histories,\neach member h ∈H denotes a possible history (or state),\nwhich consists of actions taken by players including chance;\ng ⊑h denotes the fact that g is equal to or a preﬁx of h;\nZ ⊆H denotes the terminal states and any member z ∈Z is\nnot a preﬁx of any other states; A(h) = {a : ha ∈H} is the\nset of available actions in the non-terminal state h ∈H \\ Z;\nA player function P : H \\ Z →N ∪{c} assigns a member\nof N ∪{c} to each non-terminal state in H \\ Z, i.e., P(h) is\nthe player who takes an action in state h.\nFor a state set {h ∈H : P(h) = i}, Ii denotes an infor-\nmation partition of player i; A set Ii ∈Ii is an information\nset of player i and I(h) represents the information set which\ncontains the state h. If g and h belong to the same information\nset Ii, then the player i cannot distinguish between them,\nso we can deﬁne A(Ii) = A(h) and P(Ii) = P(h) for\narbitrary h ∈Ii. We deﬁne |I| = maxi∈N |Ii| and |A| =\nmaxi∈N maxIi∈Ii |A(Ii)|. For each player i ∈N, a utility\nfunction ui(z) deﬁne the payoff received by player i upon\nreaching a terminal state z. ∆i is the range of payoffs reach-\nable by player i, i.e., ∆i = maxz∈Z ui(z) −minz∈Z ui(z)\nand ∆= maxi∈N ∆i.\nA strategy proﬁle σ = {σi|σi ∈Σi, i ∈N} is a\nspeciﬁcation of strategies for all players, where Σi is the\nset of all possible strategies for player i, and σ−i refers to\nthe strategies of all players other than player i. For each\nplayer i ∈N, its strategy σi assigns a distribution over\nA(Ii) to each information set Ii of player i. The strategy\nof the chance player σc is usually a ﬁxed probability dis-\ntribution. σi(a|h) denotes the probability of action a taken\nby player i ∈N at state h. In imperfect information games,\n∀h1, h2 ∈Ii, we have σi(Ii) = σi(h1) = σi(h2). The\nstate reach probability of h is denoted by πσ(h) if all\nplayers take actions according to the strategy proﬁle σ. The\nstate reach probability can be composed into each player’s\ncontribution, i.e., πσ(h) = Q\ni∈N∪{c} πσ\ni (h) = πσ\ni (h)πσ\n−i(h),\nwhere πσ\ni (h) = Q\nh′a⊑h,P(h′)=i σi(a|h′) is player i′s con-\ntribution and πσ\n−i(h) = Q\nh′a⊑h,P(h′)̸=i σP(h′)(a|h′) is all\nplayers’ contribution except player i. The information set\nreach probability of Ii is deﬁned as πσ(Ii) = P\nh∈Ii πσ(h).\nThe interval state reach probability from state h′ to h is\ndeﬁned as πσ(h′, h) = πσ(h)\/πσ(h′) if h′ ⊑h. πσ\ni (Ii),\nπσ\n−i(Ii), πσ\ni (h′, h), and πσ\n−i(h′, h) are deﬁned similarly.\nFor each player i ∈N, the expected utility uσ\ni\n=\nP\nz∈Z πσ(z)ui(z) under a strategy proﬁle σ is the expected\npayoff of player i obtained at all possible terminal states.\nThe best response to the strategy proﬁle σ−i is any strategy\nσ∗\ni of player i that achieves optimal payoff against σ−i, i.e.,\nσ∗\ni = arg maxσ′\ni∈Σi u(σ′\ni,σ−i)\ni\n. For the two-player zero-sum\ngames, i.e., N = {1, 2} and ∀z ∈Z, u1(z) + u2(z) = 0,\nthe Nash equilibrium is the most commonly used solution\nconcept which is a strategy proﬁle σ∗= (σ∗\n1, σ∗\n2) such that\neach player’s strategy is the best response to the other. An\nϵ-Nash equilibrium is an approximate Nash equilibrium,\nwhose strategy proﬁle σ satisﬁes: ∀i ∈N, uσ\ni + ϵ ≥\nmaxσ′\ni∈Σi u(σ′\ni,σ−i)\ni\n. The exploitability of a strategy σi is\ndeﬁned as ϵi(σi) = uσ∗\ni −u\n(σi,σ∗\n−i)\ni\n. A strategy is unexploitable\nif ϵi(σi) = 0.\nB. Counterfactual Regret Minimization\nCounterfactual Regret Minimization (CFR) [23] is an iter-\native algorithm for computing approximate Nash equilibrium\nin imperfect-information games and is widely used in NLTH\nAI. CFR frequently uses counterfactual value, which is the\nexpected payoff of an information set given that player i\ntries to reach it. Formally, for player i at an information\nset I ∈Ii given a strategy proﬁle σ, the counterfactual\nvalue of I is vσ\ni (I) = P\nh∈I(πσ\n−i(h) P\nz∈Z(πσ(h, z)ui(z)).\nThe counterfactual value of an action a in I is vσ\ni (a|I) =\nP\nh∈I(πσ\n−i(h) P\nz∈Z(πσ(ha, z)ui(z)).\nCFR typically starts with a random strategy σ1. On each\niteration T, CFR ﬁrst recursively traverses the game tree using\nthe strategy σT to calculate the instantaneous regret rT\ni (a|I)\nof not choosing action a in an information set I for player i,\ni.e., rT\ni (a|T) = vσT\ni\n(a|I) −vσT\ni\n(I). Then CFR accumulates\nthe instantaneous regret to obtain the cumulative regret\nRT\ni (a|I) = PT\nt=1 rt\ni(a|I) and uses regret-matching [42] to\ncalculate the new strategy for the next iteration:\nσT +1\ni\n(a|I) =\n\n\n\nRT,+\ni\n(a|I)\nP\na′∈A(I) RT,+\ni\n(a′|I),\nP\na′ RT,+\ni\n(a′|I) > 0\n1\n|A(I)|,\notherwise\n(1)\nwhere RT,+\ni\n(a|I) = max(RT\ni (a|I), 0).\nIn two-player zero-sum imperfect-information games, if\nboth players play according to CFR on each iteration then their\naverage strategies ¯σT converge to an ϵ-Nash equilibrium in\nO(|I|2|A|∆2\/ϵ2) iterations [23]. ¯σT is calculated as:\nST\ni (a|I)=\nT\nX\nt=1\n\u0010\nπσt\ni\n(I)σt\ni(a|I)\n\u0011\n, ¯σT\ni (a|I)=\nST\ni (a|I)\nP\na′∈A(I) ST\ni (a′|T ) .\n(2)\nThus, CFR is a ready-to-use equilibrium ﬁnding algorithm in\ntwo-player zero-sum games.\nC. No-limit Texas Hold’em\nNo-limit Texas hold’em (NLTH) has been the most widely\nplayed type of poker for more than a decade. The heads-\nup (i.e., two-player) variant prevents opponent collusion and\nallows a clear winner to be determined, so heads-up no-limit\nTexas hold’em (HUNL) becomes the primary testbed in the\ncomputer poker and game theory communities. HUNL is a\nrepeated game in which the two players play a match of\nindividual games, usually called hands. On each hand, one\nplayer will win some number of chips from the other player,\nand the goal is to win as many chips as possible throughout the\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n4\nmatch. In this paper, we follow the standard form of HUNL\npoker agreed upon by the research community [34], where\neach player starts each hand with a stack of $20,000 chips.\nResetting the stacks after each hand allows for each hand to\nbe an independent sample of the same game and is called\n“Doyle’s Game”, named for the professional poker player\nDoyle Brunson who publicized this variant.\nHUNL consists of four rounds of betting. On each round of\nbetting, each player can choose to either fold, call, or raise. If\na player folds, the game will end with no player revealing their\nprivate cards, and the opponent will take the pot. If a player\ncalls, he or she places several chips in the pot by matching the\namount of chips entered by the opponent. If a player raises by\nx, he or she adds x more chips to the pot than the opponent.\nA raise of all remaining chips is called an all in bet. A betting\nround ends if each player has taken actions and has entered\nthe same amount of chips in the pot as every other player still\nin the hand. At the beginning of a round, when there are no\nopponent chips yet to match, the raise action is called bet, and\nthe call action is called check. If either player chooses to raise\nﬁrst in a round, they must raise a minimum of $100 chips. If a\nplayer raises after another player has raised, that raise must be\ngreater than or equal to the last raise. The maximum amount\nfor a bet or raise is the remainder of that player’s stack, which\nis $20,000 at the beginning of a hand.\nIn HUNL, at the beginning of each hand, the ﬁrst player,\ni.e., P1, enters a big blind (usually $100) into the pot; the\nsecond player, i.e., P2, enters a small blind which is generally\nhalf the size of the big blind; and both players are then dealt\nwith two hole (private) cards from a standard 52-card deck.\nThere is then the ﬁrst round of betting (called the pre-ﬂop),\nwhere the second player P2 acts ﬁrst. The players alternate\nin choosing to fold, call or raise. After the pre-ﬂop, three\ncommunity (public) cards are dealt face up for all players to\nobserve, and the ﬁrst player P1 now starts a similar round of\nbetting (called the ﬂop) to the ﬁrst round. After the ﬂop round\nends, another community card is dealt face up, and the third\nround of betting (called the turn) commences where P1 acts\nﬁrst. Finally, a ﬁfth community card is dealt face up, and a\nfourth betting round (called the river) occurs, again with P1\nacting ﬁrst. If none of the players folds at the end of the fourth\nround, the game enters a show-down process: the private cards\nare revealed, the player with the best ﬁve-card poker hand (see\nFigure 1 for the hand strength), constructed from the player’s\ntwo private cards and the ﬁve community cards, wins the pot.\nIn the case of a tie, the pot is split equally among the players.\nFor a better understanding of these rounds, Figure 2 provides\na visualized example of the four rounds in one HUNL game.\nA match consists of a large number of poker hands, in which\nthe players alternate their positions as the ﬁrst and the second\nplayer. The rules of Six-player NLTH and HUNL are roughly\nthe same. For the detailed rules of Six-player NLTH, please\nrefer to the supplementary materials of [24].\nSince NLTH can be played for different stakes, such as\na big blind being worth $0.01 or $1000, it is inappropriate\nto measure the performance by chips, so players commonly\nmeasure their performance over a match as their average\nnumber of big blinds won per hand. The computer poker\nName\nExample\nDescription\nRoyal Flush\nStraight flush from Ten to Ace\nStraight Flush\nStraight of the same suit\nFour-of-a-Kind\nFour cards of the same value\nFull House\nCombination of three of a kind and a pair\nFlush\nFive cards of the same suit\nStraight\nSequence of 5 cards in increasing value\nThree-of-a-Kind\nThree cards with the same value\nTwo Pair\nTwo times two cards with the same value\nOne Pair\nTwo cards with the same value\nHigh Card\nFive cards do not make any of the above hands\nK\nA\nQ\nJ\n10\nQ\nK\nJ\n10\n9\nA\nA\nA\nA\n10\n4\n2\n7\n9\nQ\nA\nA\nA\nK\nK\nK\nA\nQ\nJ\n10\nA\nA\nK\n7\nQ\nA\nA\nA\nQ\nK\nA\nA\nK\nQ\nQ\nK\nA\nQ\n4\nJ\nStrong\nWeak\nFig. 1. The hand strength of Texas hold’em poker.\nPlayer1\nPre-flop\n(Private Cards)\nTurn\nRiver\nBetting\nFlop\nBetting\nBetting Betting\nFlush\nTwo Pair\nFlush wins\nShow-down\nPlayer2\nFig. 2. A visualized example of the four rounds in one HUNL game.\ncommunity has standardized on the unit milli-big-blinds per\nhand, or mbb\/h, where one milli-big-blind is one thousandth\nof one big blind. For example, a player that always folds will\nlose 750 mbb\/h (by losing 1000 mbb as the big blind and 500\nas the small blind).\nIV. OPENHOLDEM\nAs shown in Figure 3, the proposed OpenHoldem bench-\nmark for large-scale imperfect information game research\nconsists of three parts: the evaluation protocols, the baseline\nAIs, and an online testing platform. Next, we will expatiate\nthese three parts respectively.\nA. Evaluation Protocols\nEvaluating the performance of different NLTH agents is\nchallenging due to the inherent variance present in the game.\nA better agent may lose in a short period simply because it\nwas dealt with weaker cards. Moreover, different papers use\ndifferent evaluation metrics, making comparisons of different\nmethods difﬁcult. In OpenHoldem, we propose using the\nfollowing evaluation metrics to test different algorithms from\ndifferent aspects thoroughly.\n1) Head-to-Head Based Evaluation Metrics: One of the\nmain goals of agent evaluation is to estimate the expected\nutility uσ\ni of some player i ∈N given a strategy proﬁle σ. If\nthe game is small, one can compute this expectation exactly by\nenumerating all terminal states, i.e., uσ\ni = P\nz∈Z πσ(z)ui(z).\nIn the large-scale NLTH, however, this approach is unpractical.\nThe most commonly used approach to approximately estimate\nuσ\ni is sampling. Speciﬁcally, the NLTH agents repeatedly play\nagainst each other, drawing independent samples z1, . . . , zT\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n5\nBaseline NLTH AIs\nOpenHoldem\nTesting Platform\nEvaluation Protocols\nDuplicate Poker\nAIVAT\nHead-to-Head Based\nLBR\nDRL-BR\nExploitability Based\nCFR Based\nRule Based\nDeepStack\nRL Based\nSDK\nGUI\nSocket\nTCP\/IP\nFig. 3. OpenHoldem provides an integrated toolkit for large-scale imperfect-\ninformation game research using NLTH with three main components: the\nevaluation protocols, the baseline NLTH AIs, and an online testing platform.\nwith the probability πσ(z). The estimator ˆuσ\ni is simply the\naverage utility,\nˆuσ\ni = 1\nT\nT\nX\nt=1\nui(zt).\n(3)\nThis estimator is unbiased, i.e., E[ˆuσ\ni ] = uσ\ni , so the mean-\nsquared-error (MSE) of ˆuσ\ni is its variance,\nMSE(ˆuσ\ni ) = Var[ˆuσ\ni ] = 1\nT Var[ui(z)].\n(4)\nThis sampling based approach is effective when the domain\nhas little stochasticity, i.e., Var[ui(z)] is small, but this is not\nthe case in NLTH. To alleviate the effects of randomness and\nensure statistically signiﬁcant results, we propose to use the\nfollowing two variance reduction techniques in head-to-head\nbased evaluation.\nDuplicate Poker is a simple variance reduction technique\nthat attempts to mitigate the effects of luck and is widely used\nin the Annual Computer Poker Competitions (ACPC) [34]. For\nexample, in HUNL, let us say agent A plays one seat and agent\nB plays the other seat. First, we let A and B play M hands of\npoker, then we switch their seats and play another M hands\nof poker with the same set of cards for each seat. By doing\nso, if agent A is dealt two aces in the ﬁrst hand, then agent\nB will be dealt two aces in the M + 1-th hand, so the effects\nof luck are signiﬁcantly alleviated. The process of duplicate\npoker for multiplayer NLTH is similar.\nAIVAT is a more principled variance reduction technique\nfor evaluating performance of agents in imperfect-information\ngames [43]. The core idea of AIVAT is to derive a real-valued\nfunction ˜ui that is used in place of the true utility function ui.\nOn one hand, the expectation of ˜ui(z) matches that of ui(z)\nfor any choice of strategy proﬁle σ, so ˜uσ\ni = 1\nT\nPT\nt=1 ˜ui(zt)\nis also an unbiased estimator of the expected utility uσ\ni . On\nthe other hand, the variance of ˜ui(z) is designed to be smaller\nthan that of ui(z), so MSE(˜uσ\ni ) < MSE(ˆuσ\ni ), i.e., ˜uσ\ni is a\nbetter estimator than ˆuσ\ni . More speciﬁcally, AIVAT adds a\ncarefully designed control variate term for both chance actions\nand actions of players with known strategies, resulting in a\nprovably unbiased low-variance evaluation tool for imperfect-\ninformation games. It is worth noting that duplicate poker and\nAIVAT can be combined to further reduce the variance.\n2) Exploitability Based Evaluation Metrics: Most works on\ncomputer poker are to approximate a Nash equilibrium, i.e.,\nproduce a low-exploitability strategy. However, head-to-head\nevaluation has been shown to be a poor equilibrium approxi-\nmation quality estimator in imperfect-information games [16].\nFor example, in the toy game of Rock-Paper-Scissors, consider\nthe exact Nash equilibrium strategy (i.e., playing each option\nwith equal probability) playing against a dummy strategy that\nalways plays “rock”. The head-to-head based evaluation results\nare a tie in this example, but the two strategies are vastly\ndifferent in terms of exploitability. Therefore, the exploitability\nis also a crucial evaluation metric in imperfect-information\ngames. The exploitability of one strategy can be measured\nby calculating its best-response strategy, but the large size\nof NLTH’s game tree makes an explicit best-response com-\nputation intractable. We propose to use the following two\ntechniques to calculate the exploitability approximately.\nLocal Best Response (LBR) is a simple and computation-\nally inexpensive method to ﬁnd a lower-bound on a strategy’s\nexploitability [44]. The most important concept in this algo-\nrithm is the agent’s range, i.e., the probability distribution on\neach of the possible private cards the agent holds. Suppose\nwe want to ﬁnd the LBR of the agent A with known strategy\nσa. At the beginning of each poker hand, it is equally likely\nthat A holds any pair of private cards. The probabilities of\nactions performed by A depend on the private cards it holds.\nKnowing the strategy of A, we can use Bayes’ theorem to\ninfer the probabilities that A holds each of the private cards.\nBased on the range of A, LBR greedily approximates the\nbest response actions, i.e., the actions which maximize the\nexpected utility under the assumption that the game will be\nchecked\/called until the end. Thus, LBR best-responds locally\nto the opponent’s actions by looking only at one action ahead,\nproviding a lower bound on the opponent’s exploitability.\nLBR also relies on playing standard poker hands, so the\nvariance reduction techniques (e.g., AIVAT) can be exploited\nto reduce the number of hands required to produce statistically\nsigniﬁcant results.\nDeep Reinforcement Learning Based Best Response\n(DRL-BR). Because the game tree of NLTH is too large, the\nLBR algorithm does not explicitly compute a best-response\nstrategy but uses its local approximation to play against the\nevaluated agent A directly. In DRL-BR, we try to explicitly\napproximate the best response strategy by training an DRL\nagent B against A. More speciﬁcally, by treating A as part\nof the environment, then from the perspective of B, the\nenvironment can be modeled as a Markov Decision Process\n(MDP). B can leverage some suitable DRL algorithms (e.g.,\nDQN [5], PPO [45], etc.) to learn to maximize its payoff\nfrom its experience of interacting with the environment, i.e.,\nplaying against A. This approach turns the problem of ﬁnding\nthe best response strategy into a single agent RL problem. An\napproximate solution of the MDP by RL yields an approximate\nbest response to the evaluated agent A. After obtaining the ap-\nproximate best response B, the head-to-head evaluation result\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n6\n(e.g., AIVAT) can be used to approximate the exploitability of\nA by having them repeatedly play against each other.\nB. Baseline AIs\nDespite signiﬁcant progress in designing NLTH AIs in\nrecent years, almost all of these AIs are not publicly available.\nThis situation makes it very challenging for new researchers to\nfurther study this problem since designing and implementing\na decent NLTH AI is often very complicated and tedious. To\nﬁll this gap, in OpenHoldem, we design and implement four\ndifferent types of NLTH AIs, which are strong enough to serve\nas a good starting point for future research in this area.\n1) Rule Based AI: The rule-based method is probably the\nmost straightforward way to implement NLTH AI. A rule-\nbased NLTH AI consists of a collection of rules designed by\ndomain experts. In OpenHoldem, we develop AR, a strong\nrule-based NLTH AI designed by some skilled Texas Hold’em\nplayers in our research group. Our rule-based AI AR handles\nabout 106 different scenarios that are likely to occur in the\nreal play of NLTH and contains tens of thousands of lines of\ncode. As a suggestion, when researchers implement their own\nNLTH AIs, it is useful to compare them to our rule-based AI\nAR as a sanity check.\nBesides the strong rule-based AI AR, we also designed\nsome other rule-based AIs with different styles and strengths\n(Table I). These agents can be used as learning materials\nfor beginners, and more importantly, they can also help re-\nsearchers to carry out research on opponent modeling. These\nrule-based AIs calculate the expected winning probability at\neach stage, and then make decisions based on these probabil-\nities and different predeﬁned rules.\n2) CFR Based Static AI: While the rule-based approach\nprovides a simple framework for implementing NLTH AIs,\nthe resulting strategies are exploitable. Therefore, most recent\nstudies in NLTH AIs are focused on approximating the the-\noretically unexploitable Nash equilibrium strategies. Among\nthem, the most successful approach is the CFR algorithm [23]\nand its various variants [38], [39], [46]. CFR type algorithms\niteratively minimizes the regrets of both players so that\nthe time-averaged strategy gradually approximates the Nash\nequilibrium. In OpenHoldem, we design and implement AC,\na strong CFR based NLTH AI, which aims to serve as a\nstarting point for the large-scale equilibrium-ﬁnding research.\nOverall, AC ﬁrst uses the abstraction algorithm to create a\nsmaller abstract game, then approximates the Nash equilibrium\nstrategy in this abstract game, and ﬁnally executes the resulting\nstrategy in the original game.\nThe abstraction algorithm aims to take a large-scale imper-\nfect information game as input and output a smaller but strate-\ngically similar game that is solvable by current equilibrium-\nﬁnding algorithms. It usually consists of two parts, information\nabstraction and action abstraction. In AC, we use the potential-\naware information abstraction algorithm [36], which uses the\nk-means algorithm with the earth mover’s distance metric to\ncluster cards with similar potential. Action abstraction further\nreduces the size of the game tree by restricting the available\nactions, which is especially important in games with large\nAlgorithm 1 The CFR+ algorithm which is used to train AC.\nInput: The abstract game G, the randomly initialized strategy\nproﬁle σ1, the zero initialized cumulative regret R0 and\ncumulative strategy S0.\nParameter: The number of iterations T.\nOutput: The approximate Nash equilibrium ¯σT = {¯σT\n1 , ¯σT\n2 }.\n1: for t = 1 →T do\n2:\nfor i = 1 →2 do\n3:\nvσt\ni (h) = P\nh⊑z,z∈Z πσt\n−i(h)πσt(h, z)ui(z)\n4:\nvσt\ni (a|h) = vσt\ni (ha)\n5:\nvσt\ni (Ii) = P\nh∈Ii vσt\ni (h)\n6:\nvσt\ni (a|Ii) = P\nh∈Ii vσt\ni (ha)\n7:\nrσt\ni (a|Ii) = vσt\ni (a|Ii) −vσt\ni (Ii)\n8:\nRt\ni(a|Ii) = max(0, Rt−1\ni\n(a|Ii) + rσt\ni (a|Ii))\n9:\nσt+1\ni\n(a|Ii) = Rt\ni(a|Ii)\/P\na∈A(Ii) Rt\ni(a|Ii)\n10:\nSt\ni(a|Ii) = St−1\ni\n(a|Ii) + πσt\ni (Ii)σt\ni(a|Ii)\n11:\nend for\n12: end for\n13: ¯σiT (a|Ii) = ST\ni (a|Ii)\/P\na∈A(Ii) ST\ni (a|Ii)\naction spaces, such as NLTH. In AC, we restrict the actions\nto Fold, Call\/Check, Bet Half Pot, Bet Pot, and All-In.\nAfter obtaining the manageable abstract game G, we use\nthe iterative CFR+ [38] algorithm to approximating the Nash\nequilibrium in G. As shown in Algorithm 1, given the current\nstrategy proﬁle σt, we ﬁrst calculate the cumulative regret of\neach action after t iterations in Line 8. Then, the new strategy\nin the t + 1-th iteration is updated in Line 9 by the regret-\nmatching algorithm. Finally, by normalizing the cumulative\nstrategy ST in Line 13, the average strategy ¯σT will approach\na Nash equilibrium when T is large enough. During the actual\nplay phase, AC ﬁrst ﬁnds the abstract state that corresponds\nto the current real state of the game. Then, the approximate\nNash equilibrium ¯σT of the abstract game is queried for\nthe probability distribution over different actions. Finally, an\naction is sampled from this distribution and played in the\nactual game, if applicable.\n3) DeepStack-Like Online AI: In essence, the AC agent is\na static table calculated ofﬂine that contains the probability\ndistributions over possible actions in all situations. During\nactual play, if the opponent chooses an action that is not\nin the action abstraction of AC, i.e., an off-tree action, AC\nround this off-tree action to a nearby in-abstraction action.\nA more principled approach to calculate the off-tree action’s\nresponse is by solving a subgame that immediately follows\nthat off-tree action. DeepStack [16] is a representative online\nalgorithm based on this idea. In particular, DeepStack allows\ncomputation to be focused on speciﬁc situations raised when\nmaking decisions using a sound local strategy computation\nalgorithm called continual re-solving. To make continual re-\nsolving computationally tractable, DeepStack replaces sub-\ntrees beyond a certain depth with a learned value function\nbased on deep neural network.\nThe authors of DeepStack [16] does not release the training\ncode or model for NLTH. They only release a pedagogical\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n7\nTABLE I\nOPENHOLDEM PROVIDES MANY RULE-BASED AIS WITH DIFFERENT STYLES AND STRENGTHS.\nNLTH AI Name\nExploitability\nDescription\nCallAgent\nVery High\nAlways Call\/Check.\nManiacAgent\nVery High\nAlways raise by half or one pot randomly.\nRandomAgent\nHigh\nRandomly select legal actions.\nTimidAgent\nHigh\nCalls when holding the nut; else folds to any bet.\nCandidAgent\nHigh\nBets 1\/4 to one pot depending on hand strength, checks\/calls with marginal hands, folds weak hands.\nFickleAgent\nHigh\nRandomly change the strategy every N hands.\nLooseAggressiveAgent\nHigh\nBets\/raises aggressively with a wide range of hands.\nLoosePassiveAgent\nHigh\nCalls with most hands, folds weak hands, rarely raises.\nTightPassiveAgent\nHigh\nCalls with good hands, folds most hands, rarely raises.\nTightAggressiveAgent\nModerate\nSimilar to CandidAgent, with reﬁned hand ranges and blufﬁng.\nAR\nLow\nA relatively strong rule AI designed by using the knowledge of some skilled Texas Hold’em players.\ncode for Leduc Hold’em1 which cannot be transferred directly\nto NLTH because the game tree of NLTH is much larger\nthan that of Leduc Hold’em, and the pedagogical code does\nnot contain the necessary acceleration techniques for NLTH.\nBased on this situation, we reimplement DeepStack for NLTH\nfollowing the original paper’s key ideas and obtain an online\nAI called AD, which aims to serve as a starting point for\nthe research of subgame solving in large-scale imperfect-\ninformation games. Speciﬁcally, we spend several weeks using\n120 GPUs to generate millions of training samples for the\nriver, turn, and ﬂop value networks. Each training sample\nis generated by running 1000 CFR+ iterations based on a\nrandom reach probability. Since generating these training data\nrequires huge computing resources, we will provide download\nlinks for these training data later. Everyone can freely use\nthese data for research. It is worth noting that Noam Brown,\nthe creator of Libratus, recently co-authored a paper [47], in\nwhich they also reimplemented DeepStack. AD has achieved\nsimilar results to theirs, which validates the correctness of our\nreimplementation.\n4) Deep Reinforcement Learning Based AI:\nThe three\nagents, i.e., the rule-based AI AR, the CFR based static\nAI AC, and the DeepStack-like online AI AD, described\nin the previous sections are all based on improvements of\nexisting techniques. These AIs often rely on different kinds\nof NLTH domain knowledge, such as expert rules in AR\nand handcrafted abstraction algorithms in AC. Besides, there\nare also computational issues, i.e., in the inference stage of\nAD, the CFR iteration process consumes much computation.\nSpeciﬁcally, to ensure AD’s high-quality prediction, this itera-\ntion process often needs to be carried out for more than 1,000\ntimes in practice.\nBased on the above considerations, in OpenHoldem, we\nfurther propose a high-performance and lightweight NLTH AI,\ni.e., ARL, obtained with an end-to-end deep reinforcement\nlearning framework. ARL adopts a pseudo-Siamese archi-\ntecture to directly learn from the input state information to\nthe output actions by competing the learned model with its\ndifferent historical versions. The main technical contributions\nof ARL include a novel state representation of card and betting\ninformation, a novel reinforcement learning loss function, and\na new self-play procedure to generate the ﬁnal model. We\n1https:\/\/github.com\/lifrordi\/DeepStack-Leduc\nConvNets\nConvNets\ncard information\nrepresentation\naction information\nrepresentation\nState Representation\nFCN\nFCN\nAction Probability\nValue Prediction\nPseudo Siamese Architecture\nTraining Losses\nValue\nLoss\nPolicy\nLoss\nK-Best Self-Play procedure for model evaluation and generation  \nTrinal-Clip \nPPO\nFig. 4. End-to-end learning architecture of our deep RL based AI ARL.\nﬁnish the training of ARL in three days using only one\nsingle computing server of 8 GPUs and 64 CPU cores. During\ninference, ARL takes only 3.8×10−3 second for each decision\nin a single-core CPU of 2.00GHz. ARL is the ﬁrst AI that\nobtains competitive performance in NLTH solely through RL.\na) The Overall Architecture: ARL aims to remove the\nexpensive computation of CFR iteration in both the training\nand testing stages of a NLTH AI while eliminating the need\nof domain knowledge. It thus pursues an end-to-end learning\nframework to perform efﬁcient and effective decision-making\nin imperfect-information games. Here end-to-end means that\nthe framework directly accepts the game board information\nand outputs the actions without encoding handcrafted features\nas inputs or performing iterative reasoning in the decision\nprocess. ARL adopts the RL framework to achieve this goal,\nand the only force to drive the model to learn is the reward.\nIn NLTH, the game board information includes the current\nand historical card information and the player action informa-\ntion. The agent chooses from a set of betting actions to play the\ngame and try to win more rewards. To capture the complex\nrelationship among the game board information, the desired\nbetting actions, and the game rewards, we design a pseudo-\nSiamese architecture equipped with the RL schema to learn\nthe underlying relationships from end to end. We illustrate the\nend-to-end learning architecture of ARL in Figure 4.\nAs shown in Figure 4, the input of the architecture is the\ngame state representations of action and card information,\nwhich are respectively sent to the top and bottom streams\nof the Siamese architecture. Since the action and card rep-\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n…\n0\n0\n0\n0\n0\n0\n0\n0\n…\n0\n0\n0\n0\n0\n0\n0\n0\n…\n1\n0\n0\n0\n1\n0\n1\n1\n…\n1\n1\n1\n0\n0\n0\n1\n0\n…\n0\n0\n0\n0\n0\n0\n0\n0\n…\n0\n0\n0\n0\n0\n0\n0\n0\n…\n1\n0\n0\n0\n1\n0\n1\n1\n…\n1\n1\n1\n1\n0\n0\n0\n0\n…\n1\n0\n0\n0\n0\n0\n0\n0\n…\n0\n0\n0\n0\n0\n0\n0\n0\n…\n1\n0\n0\n0\n1\n0\n1\n1\n…\n1\n1\n1\n1\nCard Feature Representation\nAction Feature Representation\ntwo hole cards\nthree flop cards\nall hole and \npublic cards\none turn card\nRound 1 Bet 1\nRound 1 Bet 2\nRound 4 Bet 6\n0\n0\n0\n0\n…\n1\n0\n0\n0\n0\n0\n0\n0\n…\n0\n0\n0\n0\n0\n0\n0\n0\n…\n1\n0\n0\n0\n1\n0\n1\n1\n…\n1\n1\n1\n1\np1\np2\nsum\nlegal\nfold check call bet 0.5 0.75 1\n…\npot allin\nOriginal card information\nAction information encoding\nExample:\nPlayer 1 in the \nsmall blind plays \nan action `bet \npot' after getting \na hand `AsAc’.\nFig. 5. A state representation example when Player 1 in the small blind plays\n‘bet pot’ after getting an hand ‘AsAc’.\nresentations provide different kinds of information to the\nlearning architecture, we ﬁrst isolate the parameter-sharing\nof the Siamese architecture to enable the two ConvNets to\nlearn adaptive feature representations, which are then fused\nthrough fully connected layers to produce the desired actions.\nThis design is the reason why we call it pseudo-Siamese\narchitecture. To train ARL, we present a novel Trinal-Clip loss\nfunction to update the model parameters using RL algorithms.\nWe obtain the ﬁnal model through a new self-play procedure\nthat plays the current model with a pool of its K best historical\nversions to sample diverse training data from the huge game\nstate space. We believe these new techniques and underlying\nprinciples are helpful to develop general learning algorithms\nfor more imperfect-information games.\nb) Effective Game State Representation: The existence of\nprivate information and ﬂexibility of bet size cause the NLTH\nAI learning extremely challenging. To obtain an effective and\nsuitable feature representation for end-to-end learning from\nthe game state directly to the desired action, we design a new\nmulti-dimensional feature representation to encode both the\ncurrent and historical card and bet information.\nIn NLTH, the card and action information exhibit different\ncharacteristics. We thus represent them as two separated three-\ndimension tensors and let the network learn to fuse them\n(Figure 4). We design the card tensor in six channels to\nrepresent the agent’s two private cards, three ﬂop cards, one\nturn card, one river card, all public cards, and all private and\npublic cards. Each channel is a 4 × 13 sparse binary matrix,\nwith 1 in each position denoting the corresponding card. For\nthe action tensor, since there are usually at most six sequential\nactions in each of the four rounds, we design it in 24 channels.\nEach channel is a 4×nb sparse binary matrix, where nb is the\nnumber of betting options, and the four dimensions correspond\nto the ﬁrst player’s action, the second player’s action, the sum\nof two player’s action, and the legal actions. To understand this\nrepresentation, Figure 5 illustrates one example that a player\nin the small blind plays an action ‘bet pot’ after getting a hand\n‘AsAc’.\nThis representation has several advantages: 1) there is no\nabstraction of the card information thus reserves all the game\ninformation; 2) the action representation is general and can\ndenote different number of betting options (though nb = 9\nproduce satisfactory results in the experiment); 3) all the\nhistorical information is encoded to aid reasoning with hidden\ninformation; and 4) the multi-dimensional tensor representa-\ntion is very suitable for modern deep neural architectures like\nResNet [48] to learn effective feature hierarchies, as veriﬁed\nin the AlphaGo AI training.\nc) Effective Learning with Trinal-Clip PPO: With the\nmulti-dimensional feature representation, a natural choice is\nto use the current state-of-the-art reinforcement learning algo-\nrithms such as PPO [45] to train the deep architecture. PPO\nis an actor-critic framework which trains a value function\nVθ(st) and a policy πθ(at|st). PPO deﬁnes a ratio function\nrt(θ) =\nπθ(at|st)\nπθ′(at|st) as the ratio between the current policy πθ\nand the old policy πθ′, and a policy loss function Lp as:\nLp(θ) = Et\nh\nmin\n\u0010\nrt(θ) ˆAt, clip (rt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011i\n,\n(5)\nwhere ˆ\nAt is the advantage function, clip(rt(θ), 1 −ϵ, 1 + ϵ)\nensures rt lie in the interval (1−ϵ, 1+ϵ), and ϵ is a clip ratio\nhyper-parameter with typical value 0.2. PPO’s value loss Lv\nis deﬁned as:\nLv(θ) = Et\nh\n(Rγ\nt −Vθ(st))2i\n,\n(6)\nin which Rγ\nt represents the traditional γ-return [49].\nHowever, the above PPO loss function is difﬁcult to con-\nverge for NLTH AI training. We ﬁnd two main reasons for this\nproblem: 1) when πθ(at|st) ≫πθ′(at|st) and the advantage\nfunction ˆ\nAt<0, the policy loss Lp(θ) will introduce a large\nvariance; 2) due to the strong randomness of NLTH, the value\nloss Lv(θ) is often too large. To speed up and stabilize the\ntraining process, we design a Trinal-Clip PPO loss function. It\nintroduces one more clipping hyper-parameter δ1 for the policy\nloss when ˆ\nAt<0, and two more clipping hyper-parameters δ2\nand δ3 for the value loss. The policy loss function Ltcp for\nTrinal-Clip PPO is deﬁned as:\nLtcp(θ)=Et\nh\nclip (rt(θ), clip (rt(θ), 1−ϵ, 1+ϵ), δ1) ˆAt\ni\n,\n(7)\nwhere δ1 > 1+ϵ, and ϵ is the original clip in PPO. The clipped\nvalue loss function Ltcv for Trinal-Clip PPO is deﬁned as:\nLtcv(θ) = Et\nh\n(clip (Rγ\nt , −δ2, δ3) −Vθ(st))2i\n,\n(8)\nwhere δ2 and δ3 do not require manual tuning but represent\nthe total number of chips the player and the opponent has\nplaced, respectively. −δ2 represent the state value when the\nplayer folds, similarly, δ3 is the state value when the opponent\nfolds. This value-clip loss signiﬁcantly reduces the variance\nduring the training process. Our proposed Trinal-Clip PPO\nloss function improves the learning effectiveness of the actor-\ncritic framework, and we believe it is applicable for a wide\nrange of RL applications with imperfect information.\nd) Efﬁcient Self-Play Procedure:\nWith the proposed\nTrinal-Clip PPO loss function, the most direct way is using the\nself-play algorithm [50] to train the NLTH agent. However,\ndue to the private information in NLTH, simple self-play\nlearning designed for perfect information games [6], [8] often\ncauses the agent trapped in a local minimum and defeated\nby agents with counter-strategies. AlphaStar [11] designs a\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n9\npopulation-based training (PBT) procedure to maintain multi-\nple self-play agents and obtains excellent results in the real-\ntime strategy game StarCraft II. However, the PBT procedure\nneeds a tremendous computational resource to ensure good\nperformance.\nTo obtain a high-performance NLTH AI with both low\ncomputation cost and strong decision-making ability, we pro-\npose a new type of self-play algorithm which trains only one\nagent but learns strong and diverse policies. The proposed\nalgorithm maintains a pool of competing agents from the\nhistorical versions of the main agent. Then, by competing\namong different agents, the algorithm selects the K best\nsurvivors from their ELO [11] scores and generates training\ndata simultaneously. The main agent learns from the data\nand thus can compete with different opponents, maintaining a\nstrong decision-making ability of high-ﬂexible policies. Since\nthe proposed algorithm performs self-play among the main\nagent and its K best historical versions, we refer to it as K-\nBest Self-Play. Our proposed K-Best Self-Play inherits PBT’s\nmerit of diverse policy styles while maintains computational\nefﬁciency of single-thread agent training, striking a good\nbalance between efﬁciency and effectiveness.\nC. Online Testing Platform\nIn order to make the comparisons between different NLTH\nAIs easier, we develop an online testing platform with the\nabove four strong baseline AIs, i.e., AR, AC, AD and ARL\nbuilt-in. Researchers can compare the performances between\ntheir own AIs and the built-in baselines through easy-to-use\nAPIs. Figure 6 shows an example Python code of connecting\nto the platform for testing NLTH AIs. The NLTH AI designers\nonly need to implement one function, i.e., act, without caring\nabout the internal structure of the platform. The input of act\nis the current game state, which is obtained from the platform\nthrough TCP sockets. The output of act is the action to take in\nthe current game state according to the designer’s algorithm.\nThe output action is also sent to the platform through TCP\nsockets. Figure 7 shows the system architecture of our testing\nplatform. The server is responsible for playing the poker\nhands according to the rules of NLTH. It also dynamically\nschedules requests and allocates resources when necessary.\nOur platform not only supports testing between different AIs,\nbut also between humans and AIs.\nWe are more than happy to accept high-performance AIs\nsubmitted by everyone to continuously enrich the baseline\nAIs of OpenHoldem, with the ultimate goal of providing an\nNLTH AI Zoo for the research community. Currently, there\nare dozens of NLTH AI researchers and developers are using\nthis platform. It has accumulated about 20 million high-quality\npoker data and the data increases by about 100,000 per day.\nWe believe that these large-scale data will also facilitate the\nresearch of data-driven imperfect-information game solving,\nimitation learning and opponent modeling algorithms.\nV. EXPERIMENTS\nIn this section, we ﬁrst compare the performance of our\nbaseline NLTH AIs with other publicly available NLTH AIs\nimport json\nimport socket\n...\n# The IP address and port of the platform\nserver_ip = ’127.0.0.1’\nserver_port = 1080\n# Create socket and connect to the platform\nclient = socket.socket(socket.AF_INET, socket.\nSOCK_STREAM)\nclient.connect(server_ip, server_port)\nwhile True:\n# Get state in json format from the platform\nstate = recvJson(client)\n...\n# Use your awesome AI to get the action\naction = act(state)\n...\n# send your action to the platform\nsendJson(client, action)\n# Close the socket\nclient.close()\nFig. 6.\nAn example Python code of connecting to the platform for testing\nNLTH AIs.\nTCP\/IP\nState\nHuman\nState\nAction\nState\nAction\nAction\nHuman\nAI\nDatabase\nServer\nFig. 7. The schematic diagram of our testing platform’s system architecture.\nusing the proposed evaluation protocols and online testing\nplatform. Then, we conduct a set of ablation studies to analyze\nthe effects of various design choices in the baseline NLTH AIs.\nA. Comparison to the State-of-the-Arts\nTo the best of our knowledge, Slumbot [21], the champion\nof the 2018 Annual Computer Poker Competition (ACPC), is\nthe only publicly available NLTH AI that provides compar-\nisons through an online website2. Slumbot is a strong CFR-\nbased agent whose entire policy is precomputed and used as\na lookup table. Similar to our AC, Slumbot ﬁrst uses some\nabstraction algorithm to create a smaller abstract NLTH game.\nThen it approximates the Nash equilibrium in the abstract\n2https:\/\/www.slumbot.com\/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n10\nTABLE II\nTHE HEAD-TO-HEAD PERFORMANCES (MBB\/H) OF THE RULE BASED AI\nAR, THE CFR BASED AI AC, THE DEEPSTACK-LIKE AI AD, AND THE\nREINFORCEMENT LEARNING BASED AI ARL WHEN PLAYING AGAINST\nSLUMBOT, RESPECTIVELY.\nBaseline NLTH AIs\nAR\nAC\nAD\nARL\nPerformance (mbb\/h)\n57\n-20\n103\n111\n0\n10\n20\n30\n40\n50\n60\n70\nEpoch\n0.025\n0.03\n0.035\n0.04\n0.045\n0.05\n0.055\nLoss\ntrain_loss_x:1\nvalid_loss_x:1\ntrain_loss_x:2\nvalid_loss_x:2\ntrain_loss_x:3\nvalid_loss_x:3\nFig. 8.\nThe training and validation loss curves of the ﬂop network when\nusing x ∈{1, 2, 3} million training samples, respectively.\ngame using the CFR-type algorithm and ﬁnally executes the\nresulting strategy in the original game.\nThe original intention of Slumbot’s website is to facilitate\nhuman players to compete with it, and there are no open source\ntools available to test the performance of AI against Slumbot.\nDue to the poor stability of Slumbot’s website, the way of\nplaying with a simulated browser will lose the connection after\na certain number of matches, so we develop a software which\nuse an alternative method of sending data packets directly.\nBased on this software3, we compare each of our baseline\nNLTH AIs with Slumbot for 100,000 hands, and the head-to-\nhead based evaluation results (AIVAT) are shown in Table II.\nWe can see that both the DeepStack-like AI AD and the\nreinforcement learning based AI ARL outperform Slumbot by\na large margin. Although the performance of the CFR based\nAI AC is not as good as that of Slumbot, its performance is\nalso commendable because Slumbot exploits a far more ﬁne-\ngrained abstraction algorithm. An interesting result is that the\nrule-based AI AR outperforms Slumbot. This result is not\nsurprising, as it has been reported that the abstraction-based\nprograms from the Annual Computer Poker Competition are\nexploitable [44]. These experimental results illustrate that our\nbaseline NLTH AIs are adequate to serving as a good starting\npoint for NLTH AI research.\nThe DeepStack-like AI AD and the RL based AI ARL\nobtain the best performance among the four baselines. They\nare also the most complicated baselines in terms of design\nand implementation. Next, We conduct some ablation studies\nto understand the effects of their various design choices.\nB. Ablation Study on AD\n3We will open source this tool in OpenHoldem.\nTABLE III\nABLATION ANALYSES OF EACH COMPONENT OF ARL.\nName\nTraining time (Hours)\nELO\nVector\n3.8\n78\nPokerCNN\n5.4\n359\nW\/O History Information\n6.3\n896\nOriginal PPO Loss\n8.4\n1257\nDual-Clip PPO Loss\n8.4\n1308\nNaive Self-Play\n8.4\n1033\nBest-Win Self-Play\n8.4\n1024\nDelta-Uniform Self-Play\n8.6\n931\nPBT Self-Play\n8.9\n892\nARL\n8.4\n1597\n1) The Effects of Training Data Size: The training of the\nriver, turn, and ﬂop value networks of AD requires a lot of\ntraining data. We use AD\nx to denote the DeepStack-like NLTH\nAIs whose ﬂop networks are obtained by training with x\nmillion samples. Figure 8 shows the loss curves of the ﬂop\nnetwork during training when x ∈{1, 2, 3}. It is clear that\nthe ﬂop network suffers from severe over-ﬁtting when the\ntraining data size is small, and increasing the training data size\nalleviates this phenomenon. The head-to-head based evaluation\nresults (AIVAT) in Figure 9 also show that DeepStack-type AI\nis data-hungry and more training data results in a stronger AI.\n-222\n -13\n  93\nPerformance (mbb\/h)\nFig. 9. The head-to-head performances of AD\n1 , AD\n2 and AD\n3 when playing\nagainst Slumbot, respectively.\n2) The Effects of CFR Iterations During Continual Re-\nsolving: We use AD:y\n3\nto denote the DeepStack-like NLTH\nAIs, which use y CFR iterations during the continual re-\nsolving procedure. We ﬁnd that AD:500\n3\nloses 224 mbb to\nSlumbot per hand, while AD:1000\n3\nwins Slumbot 93 mbb per\nhand. These experimental results demonstrate that the number\nof CFR iterations during continual re-solving is critical to the\nperformance of DeepStack-type AI.\nC. Ablation Study on ARL\nTo analyze the effectiveness of each component of the\nRL based AI ARL, we have conducted extensive ablation\nstudies, as shown in Table III. The results of each row are\nobtained by replacing one component of ARL, and the rest\nremains unchanged. All models use the same number of\ntraining samples, and we use ELO scores to compare their\nperformance.\n1) The Effects of Different State Representations: For state\nrepresentation comparison, we consider three alternative meth-\nods: 1) Vectorized state representation like DeepCFR [51]\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n11\nA\nK\nQ\nJ\nT\n9\n8\n7\n6\n5\n4\n3\n2\nA\nK\nQ\nJ\nT\n9\n8\n7\n6\n5\n4\n3\n2\nA\nK\nQ\nJ\nT\n9\n8\n7\n6\n5\n4\n3\n2\nA\nK\nQ\nJ\nT\n9\n8\n7\n6\n5\n4\n3\n2\nA\nK\nQ\nJ\nT\n9\n8\n7\n6\n5\n4\n3\n2\nA\nK\nQ\nJ\nT\n9\n8\n7\n6\n5\n4\n3\n2\n𝐴𝑅𝐿\n𝐴𝐷\nProfessional Human\nFig. 10. Probabilities for not folding as the ﬁrst action for each possible hand. The bottom-left half shows the policy when the suits of two private cards do\nnot match, and the top-right half shows the policy when the suits of two private cards match. Left to right represent the policies of Professional Human, AD,\nand ARL, respectively.\n(Vector). It uses vectors to represent the card information (52-\ndimensional vectors) and the action information (each betting\nposition represented by a binary value specifying whether a\nbet has occurred and a ﬂoat value specifying the bet size); 2)\nPokerCNN-based state representation [52] (PokerCNN) uses\n3D tensors to represent card and action information together\nand use a single ConvNet to learn features; 3) State represen-\ntation without history information (W\/O History Information)\nis similar to ARL except that it does not contain history action\ninformation.\nAs shown in Table III, state representation has a signiﬁcant\nimpact on the ﬁnal performance. PokerCNN performs better\nthan the vectorized state representation Vector, demonstrating\nthat it is more effective to represent state information using\nstructured tensors. ARL outperforms PokerCNN since it uses\na pseudo-Siamese architecture to handle card and action\ninformation separately. ARL is also better than W\/O History\nInformation since historical action information is critical to\ndecision-making in NLTH. ARL obtains the best performance\nthanks to its effective multi-dimensional state representation,\nwhich encodes historical information and is suitable for Con-\nvNets to learn effective feature hierarchies.\n2) The Effects of Different Loss Functions: For the loss\nfunction, we evaluate ARL’s Trinal-Clip PPO loss against two\nkinds of PPO losses: 1) the Original PPO loss [45] (Original\nPPO); 2) the Dual-Clip PPO loss [14] (Dual-Clip PPO). As\nshown in Table III, compared with the Original PPO, Dual-\nClip PPO has a slight performance boost, and Trinal-Clip\nPPO (ARL) obtains the best performance. This performance\nimprovement is mainly because ARL’s policy-clip and value-\nclip loss effectively limit its output to a reasonable range, thus\nensuring the stability of the policy update. In addition, we ﬁnd\nthe model with a small overall loss generally performs better\nafter adding the value-clip loss, which is very convenient for\nmodel selection during training.\n3) The Effects of Different Self-Play Methods: For self-\nplay methods, we compare ARL’s K-Best Self-Play with 1)\nNaive Self-Play [50], which plays with the agent itself; 2) Best-\nWin Self-Play [6], which plays with the best agent in history;\n3) Delta-Uniform Self-Play [53], which plays with the agent\nin the last δ timestamps; and 4) PBT Self-Play [11], which\ntrains multiple agents and play with each other. Interestingly,\ncompared with the more sophisticated Delta-Uniform Self-\nPlay and PBT Self-Play, Naive Self-Play and Best-Win Self-\nPlay achieve better performance, possible because more com-\nplex self-play strategies are more data-hungry. However, the\nperformance of Naive and Best-Win Self-Play are still behind\nK-Best Self-Play, since simplistic self-play methods can not\novercome the notorious cyclical strategy problem in imperfect-\ninformation games. Our K-Best Self-Play method obtains the\nbest performance under the same amount of training data,\nstriking a good balance between efﬁciency and effectiveness.\n4) Exploitability Analysis: We evaluate the exploitability of\nARL with LBR. However, we ﬁnd that LBR fails to exploit\nARL, i.e., LBR loses to ARL by over 335.82 mbb\/h in\n40,000 hands. While this result does not prove that ARL is\nﬂawless, it does demonstrate that ARL seeks to compute and\nplay a low-exploitability strategy. ARL’s low exploitability is\nmainly attributed to its effective state representation, which\nencodes historical information to alleviate the partial observ-\nable problem and its efﬁcient self-play strategy to address the\ngame-theoretic challenges (i.e., cyclical strategy behavior) in\nimperfect-information games.\n5) Visualization of the Learned Policy: To analyze ARL’s\nlearned policy, we compare the action frequencies where the\nagent is the ﬁrst player to act and has no prior state inﬂuencing\nit [47] with those from human professional4 and AD. Figure 10\nshows the policies on how to play the ﬁrst two cards from the\nprofessional human and the two agents. The polices of AD\nand ARL are very similar to those of the human professional,\nwhich further explains their good performance.\nVI. CONCLUSION\nIn this work, we present OpenHoldem, a benchmark for\nlarge-scale imperfect-information game research using NLTH.\nOpenHoldem provides an integrated toolkit with three main\ncomponents: the comprehensive evaluation protocols, the\nstrong baseline NLTH AIs, and an easy-to-use online testing\nplatform. We plan to add more NLTH AIs to OpenHoldem in\nthe future, with the ultimate goal of providing an NLTH AI\nZoo for the research community. We hope OpenHoldem will\nfacilitate further studies on the unsolved theoretical and com-\nputational issues in large-scale imperfect-information games.\n4Obtained from https:\/\/www.wsop.com\/how-to-play-poker\/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n12\nREFERENCES\n[1] A. Turing, “Faster than thought,” Pitman, New York, vol. 4, no. 1, pp.\n286–310, 1953.\n[2] C. E. Shannon, “XXII. programming a computer for playing chess,” The\nLondon, Edinburgh, and Dublin Philosophical Magazine and Journal of\nScience, vol. 41, no. 314, pp. 256–275, 1950.\n[3] J. Schaeffer, “One jump ahead: Challenging human supremacy in\ncheckers,” ICGA Journal, vol. 20, no. 2, pp. 93–93, 1997.\n[4] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu, “Deep blue,” Artiﬁcial\nIntelligence, vol. 134, no. 1, pp. 57–83, 2002.\n[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[6] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of Go with deep neural networks\nand tree search,” Nature, vol. 529, no. 7587, pp. 484–489, 2016.\n[7] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering\nthe game of Go without human knowledge,” Nature, vol. 550, no. 7676,\npp. 354–359, 2017.\n[8] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “A general\nreinforcement learning algorithm that masters chess, shogi, and Go\nthrough self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.\n[9] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,\nS. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel et al.,\n“Mastering atari, Go, chess and shogi by planning with a learned model,”\nNature, vol. 588, no. 7839, pp. 604–609, 2020.\n[10] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.\nCastaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruder-\nman et al., “Human-level performance in 3D multiplayer games with\npopulation-based reinforcement learning,” Science, vol. 364, no. 6443,\npp. 859–865, 2019.\n[11] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\nJ. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al.,\n“Grandmaster level in StarCraft II using multi-agent reinforcement\nlearning,” Nature, vol. 575, no. 7782, pp. 350–354, 2019.\n[12] C. Berner, G. Brockman, B. Chan, V. Cheung et al., “Dota 2 with large\nscale deep reinforcement learning,” arXiv preprint arXiv:1912.06680,\n2019.\n[13] J. Li, S. Koyamada, Q. Ye, G. Liu, C. Wang, R. Yang, L. Zhao,\nT. Qin, T.-Y. Liu, and H.-W. Hon, “Suphx: Mastering mahjong with\ndeep reinforcement learning,” arXiv preprint arXiv:2003.13590, 2020.\n[14] D. Ye, Z. Liu, M. Sun, B. Shi, P. Zhao, H. Wu, H. Yu, S. Yang, X. Wu,\nQ. Guo et al., “Mastering complex control in moba games with deep\nreinforcement learning,” in AAAI Conference on Artiﬁcial Intelligence,\nvol. 34, no. 04, 2020, pp. 6672–6679.\n[15] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan, B. Liu, J. Chen, Z. Liu,\nF. Qiu, H. Yu et al., “Towards playing full moba games with deep\nreinforcement learning,” in Advances in Neural Information Processing\nSystems, 2020.\n[16] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis`y, D. Morrill, N. Bard,\nT. Davis, K. Waugh, M. Johanson, and M. Bowling, “DeepStack: Expert-\nlevel artiﬁcial intelligence in heads-up no-limit poker,” Science, vol. 356,\nno. 6337, pp. 508–513, 2017.\n[17] N. Brown and T. Sandholm, “Superhuman AI for heads-up no-limit\npoker: Libratus beats top professionals,” Science, vol. 359, no. 6374,\npp. 418–424, 2018.\n[18] J. Nash, “Non-cooperative games,” Annals of Mathematics, vol. 54,\nno. 2, pp. 286–295, 1951.\n[19] J. Rubin and I. Watson, “Computer poker: A review,” Artiﬁcial Intelli-\ngence, vol. 175, no. 5-6, pp. 958–987, 2011.\n[20] M. Johanson, “Measuring the size of large no-limit poker games,” arXiv\npreprint arXiv:1302.7008, 2013.\n[21] E. G. Jackson, “Slumbot NL: Solving large games with counterfactual\nregret minimization using sampling and distributed processing,” in AAAI\nConference on Artiﬁcial Intelligence Workshops, 2013, pp. 35–38.\n[22] N. Brown, S. Ganzfried, and T. Sandholm, “Hierarchical abstraction,\ndistributed equilibrium computation, and post-processing, with appli-\ncation to a champion no-limit texas hold’em agent,” in International\nConference on Autonomous Agents and Multiagent Systems, 2015, pp.\n7–15.\n[23] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione, “Regret\nminimization in games with incomplete information,” in Advances in\nNeural Information Processing Systems, 2008, pp. 1729–1736.\n[24] N. Brown and T. Sandholm, “Superhuman AI for multiplayer poker,”\nScience, vol. 365, no. 6456, pp. 885–890, 2019.\n[25] J. J. Godfrey, E. C. Holliman, and J. McDaniel, “Switchboard: Telephone\nspeech corpus for research and development,” in International Confer-\nence on Acoustics, Speech and Signal Processing, 1992, pp. 517–520.\n[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA large-scale hierarchical image database,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2009, pp. 248–255.\n[27] H. Hassan, A. Aue, C. Chen, V. Chowdhary, J. Clark, C. Federmann,\nX. Huang, M. Junczys-Dowmunt, W. Lewis, M. Li et al., “Achieving\nhuman parity on automatic Chinese to English news translation,” arXiv\npreprint arXiv:1803.05567, 2018.\n[28] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-\nman, J. Tang, and W. Zaremba, “OpenAI Gym,” arXiv preprint\narXiv:1606.01540, 2016.\n[29] M. Wydmuch, M. Kempka, and W. Ja´skowski, “ViZDoom competitions:\nPlaying doom from pixels,” IEEE Transactions on Games, vol. 11, no. 3,\npp. 248–29, 2019.\n[30] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and\nR. Salakhutdinov, “MineRL: a large-scale dataset of minecraft demon-\nstrations,” in International Joint Conference on Artiﬁcial Intelligence,\n2019, pp. 2442–2448.\n[31] M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay,\nJ. P´erolat, S. Srinivasan, F. Timbers, K. Tuyls, S. Omidshaﬁei et al.,\n“OpenSpiel: A framework for reinforcement learning in games,” arXiv\npreprint arXiv:1908.09453, 2019.\n[32] D. Zha, K.-H. Lai, Y. Cao, S. Huang, R. Wei, J. Guo, and X. Hu,\n“RLCard: A toolkit for reinforcement learning in card games,” in\nInternational Joint Conference on Artiﬁcial Intelligence, 2020, pp. 2442–\n2448.\n[33] D. Billings, D. Papp, J. Schaeffer, and D. Szafron, “Opponent modeling\nin poker,” in AAAI Conference on Artiﬁcial Intelligence, 2015, pp. 493–\n499.\n[34] N. Bard, J. Hawkin, J. Rubin, and M. Zinkevich, “The annual computer\npoker competition,” AI Magazine, vol. 34, no. 2, pp. 112–112, 2013.\n[35] M. Johanson, N. Burch, R. Valenzano, and M. Bowling, “Evaluating\nstate-space abstractions in extensive-form games,” in International Con-\nference on Autonomous Agents and Multiagent Systems, 2013, pp. 271–\n278.\n[36] S. Ganzfried and T. Sandholm, “Potential-aware imperfect-recall abstrac-\ntion with earth mover’s distance in imperfect-information games,” in\nAAAI Conference on Artiﬁcial Intelligence, 2014, pp. 682–690.\n[37] M. Lanctot, K. Waugh, M. Zinkevich, and M. Bowling, “Monte Carlo\nsampling for regret minimization in extensive games,” in Advances in\nNeural Information Processing Systems, 2009, pp. 1078–1086.\n[38] O. Tammelin, “Solving large imperfect information games using cfr+,”\narXiv preprint arXiv:1407.5042, 2014.\n[39] E. G. Jackson, “Compact cfr,” in AAAI Conference on Artiﬁcial Intelli-\ngence Workshops, 2016.\n[40] M. Schmid, N. Burch, M. Lanctot, M. Moravcik, R. Kadlec, and\nM. Bowling, “Variance reduction in Monte Carlo counterfactual regret\nminimization for extensive form games using baselines,” in AAAI\nConference on Artiﬁcial Intelligence, 2019, pp. 2157–2164.\n[41] M. J. Osborne and A. Rubinstein, A course in game theory. MIT press,\n1994.\n[42] D. Blackwell, “An analog of the minimax theorem for vector payoffs.”\nPaciﬁc Journal of Mathematics, vol. 6, no. 1, pp. 1–8, 1956.\n[43] N. Burch, M. Schmid, M. Moravcik, D. Morill, and M. Bowling,\n“AIVAT: A new variance reduction technique for agent evaluation\nin imperfect information games,” in AAAI Conference on Artiﬁcial\nIntelligence, 2018, pp. 949–956.\n[44] V. Lisy and M. Bowling, “Eqilibrium approximation quality of current\nno-limit poker bots,” in AAAI Conference on Artiﬁcial Intelligence\nWorkshops, 2017, pp. 361–366.\n[45] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[46] N. Brown and T. Sandholm, “Solving imperfect-information games\nvia discounted regret minimization,” in AAAI Conference on Artiﬁcial\nIntelligence, vol. 33, no. 01, 2019, pp. 1829–1836.\n[47] R. Zarick, B. Pellegrino, N. Brown, and C. Banister, “Unlocking\nthe potential of deep counterfactual value networks,” arXiv preprint\narXiv:2007.10442, 2020.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2021\n13\n[48] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 770–778.\n[49] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[50] A. L. Samuel, “Some studies in machine learning using the game of\ncheckers,” IBM Journal of Research and Development, vol. 3, no. 3, pp.\n210–229, 1959.\n[51] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Deep counterfactual\nregret minimization,” in International Conference on Machine Learning,\n2019, pp. 793–802.\n[52] N. Yakovenko, L. Cao, C. Raffel, and J. Fan, “Poker-CNN: A pattern\nlearning strategy for making draws and bets in poker games using\nconvolutional networks,” in AAAI Conference on Artiﬁcial Intelligence,\n2016, pp. 360–367.\n[53] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch, “Emergent\ncomplexity via multi agent competition,” in International Conference on\nLearning Representations, 2018, pp. 1–12.\nKai Li is currently an associate professor at Institute\nof Automation, Chinese Academy of Sciences. He\nreceived his Ph.D. degree in pattern recognition\nand intelligent system from Institute of Automation,\nChinese Academy of Sciences in 2018. His main re-\nsearch interest are large-scale imperfect-information\ngames and deep multi-agent reinforcement learning.\nHang Xu is currently a Ph.D. candidate in pattern\nrecognition and intelligent systems from Institute\nof Automation, Chinese Academy of Sciences. He\nreceived his bachelor’s degree in engineering from\nWuhan University in 2020. His research interests\ninclude computer game and reinforcement learning.\nEnmin Zhao is currently a Ph.D. candidate in pat-\ntern recognition and intelligent systems from Insti-\ntute of Automation, Chinese Academy of Sciences.\nHe received his bachelor’s degree in engineering\nfrom Tsinghua University in 2018. His research\ninterests include computer poker and deep reinforce-\nment learning.\nZhe Wu is currently a master candidate in pattern\nrecognition and intelligent systems from Institute\nof Automation, Chinese Academy of Sciences. He\nreceived his bachelor’s degree in engineering from\nShandong University in 2019. His research interests\ninclude opponent modeling and meta learning.\nJunliang Xing received his dual B.S. degrees in\ncomputer science and mathematics from Xi’an Jiao-\ntong University, Shaanxi, China, in 2007, and the\nPh.D. degree in computer science from Tsinghua\nUniversity, Beijing, China, in 2012. He is currently a\nProfessor with the Institute of Automation, Chinese\nAcademy of Sciences, Beijing, China. His research\ninterests mainly focus on computer vision problems\nrelated to human faces and computer gaming prob-\nlems in imperfect information decision.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research.pdf"}
{"title":"Towards Game-Playing AI Benchmarks via Performance Reporting Standards","authors":"Vanessa Volz, Boris Naujoks","summary":"While games have been used extensively as milestones to evaluate game-playing\nAI, there exists no standardised framework for reporting the obtained\nobservations. As a result, it remains difficult to draw general conclusions\nabout the strengths and weaknesses of different game-playing AI algorithms. In\nthis paper, we propose reporting guidelines for AI game-playing performance\nthat, if followed, provide information suitable for unbiased comparisons\nbetween different AI approaches. The vision we describe is to build benchmarks\nand competitions based on such guidelines in order to be able to draw more\ngeneral conclusions about the behaviour of different AI algorithms, as well as\nthe types of challenges different games pose.","url":"http:\/\/arxiv.org\/abs\/2007.02742v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2007.02742v1","published":1594042020000,"comment":"IEEE Conference on Games 2020","pdf_text":"Towards Game-Playing AI Benchmarks via\nPerformance Reporting Standards\nVanessa Volz\nmodl.ai, Denmark\nBoris Naujoks\nTH Cologne University, Germany\nAbstract—While games have been used extensively as mile-\nstones to evaluate game-playing AI, there exists no standardised\nframework for reporting the obtained observations. As a result, it\nremains difﬁcult to draw general conclusions about the strengths\nand weaknesses of different game-playing AI algorithms. In\nthis paper, we propose reporting guidelines for AI game-playing\nperformance that, if followed, provide information suitable for\nunbiased comparisons between different AI approaches. The\nvision we describe is to build benchmarks and competitions based\non such guidelines in order to be able to draw more general\nconclusions about the behaviour of different AI algorithms, as\nwell as the types of challenges different games pose.\nI. INTRODUCTION\nArtiﬁcial intelligence (AI) game-playing agents in com-\nmercial games are often used as in-game opponents for its\nplayers. As a result, such AIs need to fulﬁll several criteria.\nA common criterion is how well the agent can play the game\n– this is signiﬁcant for determining the challenge the player\nwill encounter. For many types of games it is crucial that the\nAI agent embodies a particular role believably. For example, a\npathﬁnding algorithm may result in paths that a human would\nnot normally follow. Efﬁciency is another important criterion,\nas processing time is often at a premium in commercial games.\nWhile these criteria are reﬂected in the evaluation of game-\nplaying AI in some research-focused competitions, they are\nusually conducted on simpliﬁed games resulting in limited\ncommercial interest. In contrast, several milestones of AI\nperformance against human players were achieved in popular\ncomplex games and widely covered in media. However, it\nis often difﬁcult to draw general conclusions from these\nmilestone performances. Adaptation into the games industry\nis also difﬁcult due to dramatic computational costs.\nWhat is lacking is a clear understanding of the strengths and\nweaknesses of AI algorithms and good approaches to predict-\ning their approximate behaviour on a given complex game (or\nother AI challenge). However, this information is crucial for\ngame developers who can only invest limited resources into\nexploration and research. It is also a cornerstone of research in\nacademia, as better comprehension of an algorithm can inspire\nimprovements and further research.\nWe argue that the adoption of general scientiﬁc standards\nfor the evaluation and reporting on the performance of AI\nalgorithms can help in gathering these missing insights. To this\nend, in this paper, we make the following main contributions:\n• A clear description of our vision of how benchmarks can\nbe used to elicit empirically sound results that improve\nour ability to draw general conclusions about their be-\nhaviour and performance. This includes a discussion of\npotential issues with our approach and a list of require-\nments such benchmarks would need to fulﬁll.\n• A set of guidelines intended as a starting point for a\ndiscussion of what information is required in order to\nallow the most efﬁcient information gain for each set of\nempirical experiments conducted. In addition, we hope\nthat reports adhering to such guidelines would lend them-\nselves to unbiased and meaningful comparisons between\ntwo different game-playing AI agents. We propose to\ninclude (1) a detailed characterisation of the problem in\norder to identify behavioural patterns, (2) a description\nof the complexity of the AI solution in order to allow\nnuanced comparisons and (3) a speciﬁcation of how\nresults are collected and aggregated in order to record\nthe setup and thus ensure their reproducibility as well as\ninterpretability.\nAs stated above, there are often several different objectives\nthat game-playing agents are developed to fulﬁl, including sub-\njective aspects of their gameplay behaviour. Believability is an\nimportant example of such objectives. While these subjective\naspects are certainly very interesting and important, they are,\nby deﬁnition, difﬁcult to measure automatically. They thus do\nnot lend themselves well to automatic evaluation, especially if\nintended as larger scale empirical studies. As facilitating and\nencouraging such research is the main objective of this paper,\nwe will focus on evaluating in-game performance as measured\nby in-game score or winrates here.\nIn the following section, we ﬁrst give some background\non the evaluation of game-playing AI, including an overview\nof existing approaches, as well as more details on our mo-\ntivation to propose benchmarking in this context. Next, we\ndescribe in more detail our vision of how benchmarks can\nbe developed that are able to characterise general strengths\nand weaknesses displayed by tested AI algorithms, including\na set of requirements for suitable benchmarks. As a ﬁrst step\ntowards fulﬁlling these requirements, we develop reporting\nguidelines for the performance of game-playing AI agents\nin section IV. We conclude the paper in section V with a\nsummary and suggestions for future work needed to implement\nthe vision described in this paper.\n978-1-7281-4533-4\/20\/$31.00 ©2020 IEEE\narXiv:2007.02742v1  [cs.AI]  6 Jul 2020\nII. BACKGROUND AND RELATED WORK\nGames are often cited as a promising testbed for AI, as\nproblems are fully observable, ﬂexible and reasonably complex\nto act as simulations for real-world processes [20]. As games\nare designed to allow interaction between (often several)\nhumans and a digital system, games are also well suited for\nresearching these aspects of AI. As a result, there exist a\nplethora of environments for evaluating game-playing AI.\nBelow, we give deﬁnitions for several of these existing\napproaches of evaluating game-playing AI in order to allow\na precise communication of our ideas. After that, we discuss\nrelated work for all of these approaches in more detail.\nDeﬁnition: Benchmarks: For the remainder of this paper,\nwe explicitly deﬁne benchmarks to be sets of test cases for\nthe given test subject (game-playing AI agents, in this case).\nBenchmarks thus evaluate different aspects of a given algo-\nrithm by posing a selection of different artiﬁcial challenges.\nIdeally, these tests cover all types of challenges expected to\nbe encountered in the intended application.\nDeﬁnition: Milestone challenges: Milestone challenges\nare test case for algorithms (game-playing AI agents), but they\nare only made up of one speciﬁc problem setting that needs to\nbe solved in a speciﬁc environment under speciﬁed constraints.\nMilestones thus constitute problems that push the boundaries\nof state-of-the-art research, but are narrowly deﬁned.\nDeﬁnition: Competitions: While competition frameworks\nusually implement a collection of challenges and form a great\nbasis for the implementation of a benchmark, the format of\na competition does not lend itself well to drawing general\nconclusions. This is because by necessity, they need to deﬁne\nwinning criteria, thus implicitly prioritising speciﬁc types of\napproaches and risking overﬁtting. This is a different intention\nfrom benchmarks, which seek to obtain an unbiased assess-\nment of algorithms in different contexts without requiring a\nﬁnal ranking, following the no free lunch principle.\nA. Human vs. AI Milestones\nMilestone-driven evaluations of AI have gained a large\ninterest in the media in recent years. They are often conducted\nas events where an AI agent plays against human players\nof increasing skill level in a popular game. The winner is\ndetermined as speciﬁed in the game.\nArguably one of the ﬁrst milestone challenges was chess\nstarting in the 1960’s, gaining much publicity with the 1997\nwin of IBM’s Deep Blue over the human chess champion\nGarry Kasparov [8]. Increasingly complex games have been\ntackled since then, culminating recently in big news stories\nabout AI beating human players in e.g. DotA 2 [2]. Real-time\nstrategy (RTS) games, for example, have been proposed as an\nAI research challenge as early as 2003 [4], arguing that RTS\ngames pose several AI research problems that were fundamen-\ntal at that time. Since then, these games have been an area of\nactive research [13]. Of course, Google DeepMind’s AlphaStar\nreached Starcraft II Grandmaster level in 2019, playing in the\ngame’s European league against human players after winning\nseveral show matches against professional players1.\nThese wins of AI against skilled \/ professional human\nplayers are undoubtedly very impressive achievements and\nsuch challenges are continuing to motivate AI research. How-\never, these events only generate very limited interpretable\nresults which impedes the ability to draw reliable conclusions\nregarding the strengths and weaknesses of different AI ap-\nproaches. Successful approaches in these challenges often con-\nsist of multiple interacting components, and require expensive\ncomputational resources as well as large datasets. Extensive\ntests are thus impractical and comparisons are impossible, as\nusually only a single solution exists. There further has been\ndiscussion on whether these matches are truly fair comparisons\nof in-game skill between human and AI players. [5] argue\nthat at the time of writing, it is impossible to achieve perfect\nfairness across all dimensions. One example for the lack\nof fairness the authors state are the signiﬁcant differences\nbetween the interfaces available to human and AI players.\nB. AI Research Competitions\nBesides matches against human players, many of the eval-\nuation environments for game-playing agents published in\nresearch are used in context of competitions. The challenges\nand games range from pathﬁnding [17], over Mario [12] to\nStarCraft. Performance is usually evaluated against other AI\nsystems. This can take the form of direct competition in\na multiplayer game, such as the Bot Bowl competition for\nBloodbowl, where competing AI systems play games against\none another [11]. Competitions can also be indirect, e.g in\nsingle-player games that have easily comparable metrics, such\nas the Angry Birds competition which ranks AI systems based\non how many points they score when playing the game [15].\nIn order to get more insight into the types of problems posed\nas competitions, we reviewed all competitions held at the IEEE\nConference on Games 20192. Most of the competitions use\nthe in-game score as an evaluation measure for performance.\nHowever, some evaluate other aspects of AI players, such\nas generality (General Video Game Playing Competitions -\nGVGAI [14], ALE [1]) or believability (2K Bot Prize in\nUnreal Tournament [9]). Here, we focus on in-game score and\nwill therefore not be discussing these competitions further.\nFurther, in all surveyed competitions, the AI agent acted as\na player in the game, as a human player would. This means\nno competitions targeted explicitly the development of AIs\nfor non-player characters (NPCs), such as e.g. quest givers in\na role-playing game. Still, overall, we ﬁnd there are several\ndifferent competitions that result in different challenges to AI\nagents. Unfortunately, each competition is evaluated separately\nand uses their own framework and execution environment.\nThis makes comparisons of the same set of algorithms across\ndifferent competitions impracticable or even infeasible. A\ngiven algorithm implementation with the same hyperparameter\n1https:\/\/deepmind.com\/blog\/article\/alphastar-mastering-real-time-strategy-\\\ngame-starcraft-ii\n2http:\/\/ieee-cog.org\/2019\/competitions conference\/\nsettings and evaluation resources is usually only evaluated in\na single competition or by a single group of authors.\nAt the same time, most competitions have a relatively\nnarrow focus - often not broad enough to reach generalisable\nconclusions that could easily be applied to problems in the\ngame industry. Still, due to the nature of competitions, even if\nthe focus of a competition is perfectly aligned with a problem\nof interest, it remains difﬁcult to generate transferable insights.\nThis is because competitions usually only exist in speciﬁc\nsettings and are not scalable. For example, a competition might\nrequire agents to make decision within a certain time-window.\nResults obtained from this setting do not necessarily contain\ninformation about the performance of agents when the allowed\ntime for a decision is modiﬁed.\nSummarising, we ﬁnd that competitions are great drivers\nof research and a plethora of different challenges are tackled\nat the same time. However, it is also clear that there is\na large amount of untapped potential for game-playing AI\ncompetitions that do not focus on in-game score. Further, while\nthese competitions provide a number of very helpful evaluation\nenvironments, at the time of writing, their results can not easily\nbe transferred to more general research insights.\nC. Interpretation of Results\nSome related work discusses the interpretation of AI eval-\nuation results. This is mostly on a more theoretical basis, as\nusable empirical results are scarce. For example, in [16], the\nauthors make a case for using games to measure general intel-\nligence. Various environments implement this idea, including\nthe General Video Game Competition (GVGAI, [14]) and the\nArcade Learning Environment (ALE, [1]). However, in [6],\nthe author argues that state-of-the-art AI benchmarks is not\ncurrently capable of measuring general intelligence, as it is\nusually skill exhibited at speciﬁc tasks that is measured instead.\nOther research has used data analysis to identify behavioural\npatterns in competition results [10]. However, an open issue\nremaining in these approaches is the incomplete understanding\nof the characteristics of the different problems the AI algo-\nrithms are tested on. The guidelines for reporting proposed in\nsection IV are intended to help alleviating this issue.\nIII. TOWARDS INTERPRETABLE BENCHMARKS\nThis section explains our vision of using benchmarks to\ncollect interpretable empirical insights into the behaviour of\ngame-playing AI algorithms. We ﬁrst specify how we envision\nbenchmarks to be used in this context and then develop a set\nof requirements needed to fulﬁl this vision.\nA. Motivation and Usecase\nBenchmarking of course can have various different pur-\nposes. However, one that is particularly relevant in the context\nof game-playing AI is identifying which AI approach should\nbe used for a given problem. While it is often possible to\nmake educated guesses about which approaches are promis-\ning, due to computational costs, it is often not feasible to\ntest variations of algorithms thoroughly. This is especially\nproblematic in complex games such as StarCraft II, where\nthe only known well-performing AI (AlphaStar by Deepmind)\ncombines various approaches and requires extensive amounts\nof computational resources to evaluate [18].\nIssues are especially apparent in the fact that, despite the\nwidespread success of Monte-Carlo Tree Search (MCTS)\nover the last decade, with many examples of MCTS systems\ndeveloped for competitions such as GVGAI3, it has not seen a\nsimilar magnitude of impact on the games industry. One pos-\nsible explanation for this is the cost of knowledge acquisition,\nand commercial games studios are often already operating on\ntight budgets in terms of both money and time. Even with clear\nexplanations of how to implement a new algorithm, its beneﬁts\nmust be clear in order to justify the expenditure involved in\nadopting it. Our aim is to achieve industry-accepted standards\nby enabling meaningful comparisons between different ap-\nproaches in relevant test environments.\nWe thus propose to develop benchmarks that primarily focus\non being able to gain a better understanding of a given AI\nalgorithm in the aforementioned usecase. Here, benchmarks\ncan be useful in the following ways:\n• Understanding strengths \/ weaknesses of AI algorithms:\nGaining insight into which types of problems an AI\nhas been proven successful at, is immensely helpful in\nidentifying suitable candidate solutions for a given game.\n• Measuring AI improvement: Changing even small as-\npects of a game-playing AI often leads to signiﬁcant\nbehavioural changes due to the complexity of both AI\nand game environment [3]. Gathering information on be-\nhaviour changes to small AI adjustments in a benchmark-\ning setting can help to identify promising modiﬁcations\nto test on the full scale.\nB. Identifying AI Abilities with Benchmarks\nGames pose complex problems to a player, independent of\nwhether they are human or AI. The performance of a player\non these problems thus gives indirect insight into their various\nabilities (such as e.g. the ability to react quickly, or to predict\nan opponent’s behaviour). There is however no clear mapping\nbetween different abilities and a given game, as usually a\ncombination of abilities is required. Thus, in order to gain\nuseful information, we propose the following workﬂow:\n1) Identify research questions, i.e. which types of abilities\nare of interest in which context.\n2) Identify problems, i.e. tasks in games, where these\nabilities are required in different degrees. Ideally, it is\npossible to scale this requirement in the same game. This\nis relatively straightforward for abilities such as working\nmemory, but might not be possible for others. Enough\ntasks need to be selected so that the abilities in question\ncan be isolated, i.e. they are the only common ability\nrequired between the different problems.\n3) Describe the problems in detail, focusing on which\nabilities and cognitive skills are required, and what\n3http:\/\/gvgai.net\/gvg rankings.php\nmakes them difﬁcult to solve for human and AI players.\nWe discuss problem descriptions and characterisation\nmore in section IV-A.\n4) Identify baseline performances in order to be able to in-\nterpret the results. Ideally, this includes the performance\nof some human players, as well as some basic AI players\n(random agent, as well as popular approaches such as\nMonte-Carlo Tree Search and Reinforcement Learning).\nC. Benchmark Requirements\nWith our suggestion for the usage of benchmarks in mind,\nwe have developed several criteria that a game-playing AI\nbenchmark should fulﬁll. A benchmark should ...\n• produce measurable results. The target value should be\nmeaningful and clearly deﬁned.\n• allow meaningful conclusions. The conclusions that can\nbe drawn should be a meaningful resource of information\nfor a research question \/ industrial games application.\n• be interpretable. The results from the benchmark should\nallow conclusions on the behaviour of the tested AI under\ndifferent conditions. This means a basic understanding\nof the included problems is necessary. It further needs\nto be established what the measured performance means\nin context of performance baselines, e.g. by observing\nrandom agents and average human players.\n• produce generalisable conclusions. Results from bench-\nmarks should ideally be transferable to similar games,\nalgorithms, environments and hyperparameters. This can\nbe achieved by facilitating the testing of different setups,\nsuch as different hyperparameter conﬁgurations and prob-\nlems at different scales. Unfortunately, due to the com-\nplexity of the setup, this criterion is hard to guarantee. At\nleast, known limits of generalisation should be disclosed.\n• be reproducible. The same results should be reproducible\nduring each run of the benchmark. If the results contain\nnoise, it should be ensured that the benchmark is run\noften enough to produce robust statistics.\n• produce robust results. Slight modiﬁcations to the game\n(e.g. modifying the colour of a game object) should not\nhave a signiﬁcant effect of the obtained results. This can\nbe ensured by including multiple such modiﬁcations of\nthe same setup in the benchmark and reporting averages.\n• allow for comparisons. Two AI algorithms should be\ncomparable based on their independently obtained bench-\nmarking results without requiring further experiments.\n• be practical. Running the benchmark and comparing to\nexisting results cannot be prohibitively expensive consid-\nering commonly available computational resources.\n• disclose bias. Problems should be chosen with great care\nas to not favour speciﬁc algorithms or types of problems.\nAny consciously chosen bias should be disclosed.\n• account for solution complexity. The complexity of each\nAI should be clearly communicated as it is not straight-\nforward to compare algorithms with vastly differing com-\nputational costs.\nBased on the above requirements, we identify three main\ntypes of information that needs to be reported in order to\nensure their fulﬁlment:\n• Description of problems included in the benchmarks and\nthe abilities required to solve them (cf. previous section).\n• Description of the AI solution and its complexity.\n• Details of how performance is quantiﬁed and aggregated,\nin particular including statistical information.\nWe develop our reporting guidelines in section IV based on\nthis conclusion.\nD. Limitations and a Word of Caution\nWhile benchmarks can undoubtedly help to identify the\nstrengths and weaknesses of algorithms and thus inspire ideas\nfor new algorithms or for improvements of existing ones, this\ntype of results-driven research can also be fairly limiting for\nresearchers. They should thus be used with caution.\nFor example, while we in this paper argue for a generalised\nreporting standard, we also want to stress that this standard\nneeds to be adaptable enough to ﬁt new or newly identiﬁed\nneeds. While we aim for general applicability, it is clear that\nour proposal can and should be adapted to ﬁt speciﬁc aims\nof different applications. We plan to give an explicit demon-\nstration of such an adaptation in future work and modify the\nguidelines based on more detailed discussions. Furthermore,\nnot all objectives for AI agents (such as believability) are\neasily quantiﬁable. Conventional performance-related bench-\nmarks are thus not suitable as evaluation tools in these cases.\nEven if great care is taken to represent a wide variety of\nchallenges in a benchmark, ensuring appropriate coverage is\ndifﬁcult. Especially in the domain of games, even character-\nising a given test challenge is very problematic (see section\nIV-A). They should therefore be continuously evaluated for\nimplicitly introduced biases and extended as much as possible.\nFor the same reason, performance on a benchmark should\nnever be used as the sole decision criterion for the success\nof an AI method or the acceptance of a publication. Only\nallowing submissions that improve some measurement, how-\never well thought out, will inevitably lead to overﬁtting to this\nmeasure. This would most likely lead to an inﬂux of papers\nabout small modiﬁcations to existing algorithms that lead to\nequally small improvements in the measure speciﬁed. This\nwould severely hinder progress in this ﬁeld of research and\nmust thus be avoided.\nIt is further important to acknowledge that for practical\nreasons benchmarking problems usually do not represent the\nfull scale of their counterparts in the real-world. While this\nissue is unavoidable due to limits of computational resources,\nit is important to consider the additional challenges that arise\nthrough scaling up the size and complexity of a given problem.\nConsider for example the Kaggle Connect-X challenge4, where\nagents are tasked to played versions of Connect 4 with a\nvariable number of items that need to be connected (X). For\nsmall boards and a small number X, it is fairly straightforward\n4https:\/\/www.kaggle.com\/c\/connectx\nFig. 1. Proposed structure for reporting guidelines\nto achieve game-theoretic perfect play with algorithms that\nexhaustively search the game tree. However, these approaches\neventually run into issues with available computational re-\nsources when scaling up the problem. Technically, the type of\nproblem hasn’t changed, but the types of challenges resulting\nfrom it have. This is why deﬁning the complexity of both the\nproblem and AI solution (see section III-C) is important and\nthe scalability of algorithms should be investigated.\nObviously, this raises the question: Why benchmark with\nsimpliﬁed artiﬁcial functions at all? We argue that understand-\ning algorithms and challenges in a detailed manner, even for\nsmaller problems, can help divide and conquer larger-scale\nones. Complex solutions rarely have end-to-end solutions and\nit is thus helpful to be able to understand the available building\nblocks and make educated guesses of how they could work\ntogether (see e.g. the many components working together in\nAlphaStar [18]. However, this also means that benchmarks do\nnot offer all requirements for thoroughly evaluating a given\ngame-playing agent. To advance the ﬁeld, we should therefore\nalso continue working on other approaches to evaluate AI\nperformance, such as competitions and milestone challenges\n(see section II).\nIV. REPORTING GUIDELINES\nBased on the requirements we identiﬁed in the previous\nsection, we concluded that in order to be able to draw general\nconclusions on the behaviour of AI agents, we need good\nreporting about (A) the problem, (B) the solution, and (C) the\nperformance measurement procedure. Ideally, this reporting is\nstandardised to some extent. A structure will help prevent the\nomission of facts that might not appear immediately relevant.\nStructure also facilitates a comparison between different al-\ngorithms, even if they were produced and tested in different\ncontexts. The structure we use here is based on [19] and\nvisualised in ﬁgure 1.\nWe propose such reporting guidelines below. They are inten-\ntionally kept relatively abstract to allow general applicability.\nHowever, this generality needs to be thoroughly tested in fu-\nture work by applying these guidelines to several experiments.\nA. Shaping the Problem\nSeveral useful characteristics have been proposed to de-\nscribe a problem, for example in [20]. These include for\nexample the number of players and the role the AI agent\nis assuming in the context of the game (e.g. an opponent,\na teammate, a boss). However, these characteristics are not\nsufﬁcient to identify the types of skills and abilities that would\naffect the performance of a player on a a given problem. We\nthus propose to describe the following additional aspects:\n• Task: What type of task the AI agent needs to perform\n(e.g. pathﬁnding), what types of obstacles exist that make\nthis more difﬁcult (e.g. dead ends) and abilities that would\naffect its performance (e.g. size of working memory).\n• In-Game Performance Measure: How the degree of task\nfulﬁllment is reported (e.g. cost of the taken path)\n• Game-AI Interface: How the AI agent interacts with\nthe game and what information is made available (e.g.\nthe game expects a cardinal direction from the agent)\n• Constraints: Which constraints are set for the agent (e.g.\nmaximum response time)\nAll of these aspects should be described in as much details\nas possible. Below, we propose a format for doing so in a\nstructured manner.\n1) Characterising the Task: Tasks in games often offer\nseveral different cognitive and physical challenges. Common\nexamples of challenges posed to a player include decision\nmaking, resource management, planning, prioritisation, logical\ndeduction, attentiveness, predicting an opponent’s actions and\nquick reactions [20].\nHowever, games usually do not challenge any speciﬁc\nability separately and often introduce additional modiﬁers to\nvary the given problem. Let us look at chess as an example.\nEach turn, the player has to decide between a ﬁnite set of\navailable legal moves. As all information is available, algo-\nrithms that search the gametree with minmax approaches seem\nsuitable at ﬁrst glance. However, due to the sheer size of the\ngametree caused by the relatively large branching factor, these\napproaches are not computationally feasible. If the branching\nfactor was severely reduced, the type of the tasks would not\nchange, but a big aspect of its challenge for an AI player\nwould be removed.\nAnother example of added obstacles is hidden information.\nIf some of the chess pieces were obscured by a fog of\nwar as is common in real-time strategy games, making a\ndecision about the next move suddenly requires going through\nseveral possible gamestates and evaluating the options under\nuncertainty. Different games introduce different modiﬁers by\ne.g. introducing stochasticity or an adversary.\nAt the same time, abilities can be identiﬁed that are required\nto fulﬁll the challenges or to meet the modiﬁcations introduced\nby the added obstacles. In our chess example from above, the\nability to process and store a larger amount of information (i.e.\nworking memory) could help solve the problem introduced\nby a larger branching factor. Other examples of such abilities\ninclude reaction speed, timing, motor precision, perception and\nthe ability to detect patterns in observed behaviour. These\nabilities are the information we are mainly interested in as\na result from a benchmark.\nUnfortunately, abilities do not map clearly to any given\nchallenge or obstacle. For example, a player could react to\nhidden information by memorising previous states and pre-\ndicting future game states based on these. An alternative way\nwould be to consider uncertainty in their plans and develop a\nmore robust strategy. A third option would be to just rely on\nquick reactions if new information is revealed unexpectedly.\nMost likely, it is best to improve all abilities. It is thus very\nimportant that the tasks are chosen and described with care.\nFor the time being, we can not provide a formal and\nexhaustive taxonomy that would facilitate the characterisation\nof a task. We suggest to approach this topic in collaboration\nwith cognitive scientists. However, at this point in time, there\nis no consensus even in this ﬁeld of research about how to\nclassify different types of tasks and associated brain activity\nyet. While not in scope for this paper, in the future, we would\nlike to be able to develop an exhaustive list of different types\nof tasks in games by conducting a large-scale survey.\n2) In-game Performance Measure: It is of course important\nhow the observed performance of the players is evaluated on\nthe task as speciﬁed above, even if we focus exclusively on\nin-game measures. In particular, it is important to describe the\nmanner in which players receive feedback on their measured\nin-game performance:\n• Timing: Does the player receive feedback continuously\nor only at the end of a round?\n• Granularity: Does the performance measure reﬂect the\ndegree of fulﬁlment of the task, or is it win \/ lose?\n• Dependencies: Is the feedback absolute or relative, i.e.\ndepending on other competing players?\nThe above aspects shape the problem faced by a player fur-\nther. For example, late and scarce feedback might necessitate\nmore exploration. Further, if performance is measured relative\nto other players, it is often helpful to react to other player’s\nstrength and weaknesses. The exact way how performance\nis measured and the manner in which the player receives\nfeedback on it should thus be part of the problem description.\n3) Game-AI Interaction: The way the interaction between\nthe game and an AI algorithm is set up also shapes a large\npart of the overall problem. Through various competition\nframeworks, a defacto standard interface has been established\nin terms of the ﬂow of information. The AI is usually expected\nto work within the game-loop, receiving information about\nthe gamestate and additional resources if available at each\ntick. It is then expected to return an action which the game\nengine executes, resulting in the next gamestate. The agent\nperformance is then measured and fed back to the player in\na speciﬁc manner as discussed above. Assuming this standard\ninterface, the following aspects require description:\na) Game-State Representation: The representation of the\ngame-state that is made available to the game-playing agent.\nFor example, for a pathﬁnding task, sensible representations\ncould be a graph or a grid. For playing arcade-style games,\nrepresentations could be a list of sprites and their locations\n(like in GVGAI) or the game-output in pixels (in e.g. ALE).\nThe type of representation plays a major role on what type of\nalgorithms are suitable to the problem. Pixel representations\nrequire at least some form of image processing. Jump Point\nsearch, for example, only runs on grids, while A* runs on\nanything that can be represented as a graph, including grids.\nb) Actions: There are several popular formats of actions,\nwhich are usually deﬁned a priori. For example, in several\ncases, the interface expects an integer indicating which action\nshould be chosen from a set of available actions (as in\nGVGAI). Another popular format is a location, along with\nan action and target unit (as e.g. in StarCraft II). It is further\nrelevant to specify the number of available actions to choose\nfrom at each game tick (branching factor), as well as whether\nit is possible to choose invalid actions.\nc) Additional Resources: Several setups also include\nadditional information that is made available to the agent.\n• Forward model that simulates actions in the game. The\navailability of a forward model dictates whether statistical\nforward planning approaches are practical.\n• Heuristics for gamestate evaluation. Heuristics can en-\ncode domain knowledge and can be immensely helpful\nto add performance feedback.\n• Experimental setup. This allows extensive training on the\nspeciﬁc setup, thus requiring less generalisation during\ntesting. In some competition, the speciﬁc game used for\nevaluation is kept secret in order to encourage generality.\n• Replays from human players. Replays can be used to\nbias the choice of suitable actions by modelling human\nbehaviour. This can be especially helpful in environment\nwith large action spaces.\nd) Player Interaction: Another important aspect in this\ncontext is the manner of interaction with other players, espe-\ncially in cooperative games. While some frameworks provide\nan additional communication interface, others do not allow\ncommunication at all (sometimes based on the game’s rules).\n4) Constraints: In some setups, constraints are speciﬁed\nfor eligible solutions, which naturally further shape the chal-\nlenge tackled. Common examples include the required reaction\nspeed (40ms in GVGAI) or restrictions in terms of program-\nming languages. We describe ways to describe the complexity\nof a solution in the following section. All of these aspects can\nalso be used to deﬁne a constraint for valid solutions.\nB. Describing Solution Complexity\nAs different complexities have a meaningful inﬂuence on\nthe performance of game-playing AIs, reporting these com-\nplexities is important to allow for comparisons. Here we list\nseveral such complexities and explain these in a bit more\ndetail. Of course, this list does not claim to be exhaustive.\n1) Hardware, Operating System and Software: Core hard-\nware components of a computer system (such as CPUs, GBUs\nand working memory) inﬂuence the execution of experiments\nsigniﬁcantly, especially regarding execution time. They should\nthus be part of any report on experiments. In addition, for the\nsake of reproducibility, we suggest to note and make available\nall software related to the experiments, including compilers,\ninterpreters and libraries, as well as the operating system.\n2) Computational Complexity: Computational complexity\ndescribes the computational resources consumed during the\nexecution of a given algorithm. Relevant numbers here include\nCPU and GPU workload, as well as consumed working\nmemory and running time (both system and wall clock time).\nIt is further important to indicate whether the resources were\nspent in a training phase or during online execution.\nIn addition, more domain-speciﬁc measures of computa-\ntional complexity are relevant as well. For example how\noften a forward model was called or the response time of\na given algorithm per round. In general, it is assumed that\nmore expenditure of computational resources results in better\nalgorithm performance.\n3) Model Complexity:\nModel complexity describes the\ncomplexity of the algorithm after training, which relates both\nto its explainability, as well as to its speciﬁcity. It is often\nmeasured as a function of the number of hyperparameters and\nthe architecture required to specify the the model. The more\ncomplex a model, the more prone it is to overﬁtting.\n4) Implementation Complexity: The complexity of imple-\nmenting a given algorithm is mainly relevant for its practical-\nity. In addition, the less complicated an algorithm is, the easier\nit is to add modiﬁcations for further improvement.\n5) Human Knowledge Engineering: Various parts of AI\nalgorithms can be supported by inserting engineered domain\nknowledge and simultaneously reducing its generality. This\nreduces the complexity in other aspects of the algorithm, but\nrequires formal domain knowledge instead. It should thus\nbe carefully recorded what type of domain knowledge is\nintegrated into the algorithm, speciﬁcally detailing inputs such\nas game state evaluation heuristics and constants chosen by\ndomain experts. In addition, any domain-speciﬁc strategies and\ntactics the algorithm is based on should be disclosed.\nC. Quantifying Performance\nThe in-game performance is of course deﬁned based on the\ngame (see section IV-A2). However, the performance measure\nfor an AI should depend on its intended purpose. For an AI de-\nsigned as a team member in a cooperative game, for example, a\nsuitable measure might be how closely its behaviour resembles\nhuman gameplay. If the intention is to produce agents that\nperform well in-game, naturally, measures such as the winrate\nor the average in-game score would be suitable. Of course, the\nmore different measures are introduced, the more information\nis obtained on the AI performance.\nBecause\nmost\ngames\nand\nAI\nalgorithms\nare\nnon-\ndeterministic, general standards for reporting the performance\nof empirical studies also apply here. One of the most important\ncriteria in this regard is to communicate an assessment of the\nstatistical signiﬁcance of the obtained results. Further discus-\nsions on general best practices in the context of benchmarking\ncan be found in other literature, such as [7].\nHowever, some fairly unique aspects of AI performance\nexist that need to be reported accordingly. For example, it\nis important to specify which adversaries the AI is tested\nagainst. We identiﬁed the following categories:\n• game: Single-player game where the game itself poses\nthe challenge.\n• NPC(s): Competitive multi-player game. Opponents are\nbaseline AIs provided by environment (not competitors).\n• game with NPC(s): Cooperative multi-player game. The\ngame itself poses challenge, but is tackled together with\nbaseline AIs provided by environment (not competitors).\n• competitor AI(s): Competitive multi-player game. Oppo-\nnents are other competing AI agents (past and present)\nas well as baseline AIs.\n• humans: Human players.\nFor all setups that use match-ups against other submitted\nAIs, the resulting scores are by design relative to the other\nsubmissions. This is a common setup in competitions, as a\nranking between all entries is desired. Further, in multi-player\ncompetitions, this tournament-style setup is used regularly, as\nin-game performance is usually not transitive. Well-known\nexamples of this include sports tournaments and leagues.\nHowever, this style of competitions makes it difﬁcult to add al-\ngorithms to the evaluation and the results are not interpretable\nout of context. They are thus not suited for evaluating a given\nalgorithm independently, as is desired in benchmarks.\nAn important concern for evaluation is the robustness of\nthe results. This is inﬂuenced both by the number of trials as\nwell as the variety of test cases. It is difﬁcult to make general\nstatements about a suitable number of trials as this depends\non the variance of obtained results. There are different sources\nof varied test cases:\n• inherent: Variety inherent in game and NPC behaviour.\nBasically fulﬁlls the function of rematches and is intended\nto avoid drawing conclusions from statistical outliers.\n• competitor(s): AIs are tested against competitors creating\nvariety in how the opponent reacts to the agent’s actions.\n• instance: AIs are tested on different instances of the same\ngame, e.g. different game maps or levels creating variety\nin the environment and the available actions per tick.\n• games: AIs are tested on different games of the same\ngenre. The environment, available actions and the effects\nof actions vary.\nMany usecases of benchmarks, and especially competitions,\nrequire further aggregation of the different performance mea-\nsures obtained this way. There are different popular approaches\nto this, depending on the setup:\n• Aggregated absolute performance: Mean or median of\nan absolute performance measure (e.g. single player\ngames with in-game score as performance measure, or\nthe winrate in multiplayer games against a immutable set\nof adversaries).\n• Aggregated performance rank: In order to ensure equal\nweighting of each test case, some frameworks use the\nper-test-case-rank of each AI as a basis for their aggre-\ngated performance measure. GVGAI, for example, uses\na Formula 1 scoring system per game and aggregates the\nallotted points for the ﬁnal score. Another 6option is to\nperform Wilcoxon Rank-Sum tests on the obtained ranks.\n• Aggregated relative performance: In multi-player games\nwhere the adversaries are not kept constant, the resulting\nperformance measures per game are necessarily relative\nto the other AI agents. Popular ways of aggregating the\nresults is by computing the win rates, by distributing\npoints per match-up (as in many sports leagues) or by\nusing iterative measures such as player rating (MMR in\nStarCraft II, ELO in chess).\nAs an AI player is designed to optimise the given perfor-\nmance measure, its speciﬁcation of course deﬁnes an integral\npart of the problem it needs to solve. While current research,\nespecially in competitions, focuses mostly on relative evalu-\nations, we recommend to use absolute and usecase-speciﬁc\nmeasures for benchmarks so that algorithms are comparable\nindependently. The adversaries and variety of testcases the\nmeasures are aggregated on should depend on the chosen\nusecase as well. This naturally requires the identiﬁcation of a\nhypothesis prior to conducting a study. Further, it is important\nto consciously choose the different sets of testcases that the\nobtained performance measures are aggregated on, in order to\navoid implicit weighting biases as well as interpretation errors.\nV. CONCLUSION AND FUTURE WORK\nIn this paper, we detail how we envision benchmarks to\nbe employed to understand the behaviour of game-playing\nartiﬁcial intelligence algorithms better. In particular, we are\ninterested in characterising the abilities of algorithms and gen-\neralising these results across different games. A requirement\ncrucial to our vision is the ability to report on different aspects\nof empirical results in detail, speciﬁcally the type of problems\ntested, the complexity of the AI algorithm in question, as well\nas how performance is quantiﬁed.\nAs a ﬁrst step towards fulﬁlling these requirements, in this\npaper, we propose guidelines for reporting on empirical results\nof game-playing AI algorithms. In order to fully understand\ntheir usefulness, we plan to apply them to multiple games in\nthe future. Based on resulting insights, we expect to be able to\nidentify the usefulness and clarity of the different suggested\nreporting criteria. Following that, we plan to demonstrate the\neffectiveness of the obtained reports by conducting a detailed\ncomparison between different AI algorithms, and identifying\ntheir respective strengths and weaknesses.\nWe also plan to identify more concepts from related dis-\nciplines of research, such as evolutionary optimisation, that\ncould be helpful in the context of benchmarking game-playing\nAI. One promising example is the concept of anytime perfor-\nmance, i.e. measuring performance continuously for varying\nbudgets. Another example is landscape analysis, where numer-\nous low-level characteristics are computed to express speciﬁc\naspects of a given problem. Data analysis approaches are then\nused to computationally identify patterns. In addition to a data-\ndriven approach, other ﬁelds to consider are cognitive and\nneuro-science, especially when it comes to modelling mental\nchallenges encountered in games.\nACKNOWLEDGEMENTS\nBoris\nNaujoks\nacknowledges\nthe\nEuropean\nCommis-\nsions H2020 programme, H2020-MSCA-ITN-2016 UTOPIAE\n(grant agreement No. 722734) as well as the DAAD (German\nAcademic Exchange Service), Project-ID: 57515062 Multi-\nobjective Optimization for Artiﬁcial Intelligence Systems in\nIndustry.\nVanessa Volz acknowledges the helpful discussions on the\ntopic of benchmarking during Dagstuhl Seminar 19511 with\nYngvi Bjrnsson, Michael Buro, Mike Cook, Raluca D. Gaina,\nGnter Rudolph, Christoph Salge, Nathan Sturtevant, Tommy\nThompson and Georgios N. Yannakakis.\nREFERENCES\n[1] M. G. Bellemare et al. The arcade learning environment: An evaluation\nplatform for general agents. Journal of Artiﬁcial Intelligence Research,\n47:253–279, 2013.\n[2] C. Berner et al. Dota 2 with large scale deep reinforcement learning,\n2019. arXiv, https:\/\/arxiv.org\/abs\/1912.06680.\n[3] I. Bravi et al.\nRinascimento: Optimising statistical forward planning\nagents for playing splendor.\nIn IEEE Conference on Games (CoG).\nIEEE Press, Piscataway, NJ, 2019.\n[4] M. Buro. Real-time strategy games: A new ai research challenge. In\nInternational Joint Conference on Artiﬁcial Intelligence, pages 1534–\n1535, 2003.\n[5] R. Canaan et al. Leveling the playing ﬁeld: Fairness in ai versus human\ngame benchmarks. In Foundations of Digital Games (FDG). ACM Press,\nNew York, 8 2019.\n[6] Franois Chollet. On the measure of intelligence, 2019. arXiv, https:\n\/\/arxiv.org\/abs\/1911.01547.\n[7] T. Eftimov, P. Koroˇsec, and B. Korouˇsi´c Seljak. A novel approach to sta-\ntistical comparison of meta-heuristic stochastic optimization algorithms\nusing deep statistics. Information Sciences, 417:186–215, 2017.\n[8] N. Ensmenger. Is chess the drosophila of artiﬁcial intelligence? A social\nhistory of an algorithm. Social Studies of Science, 42(1):5–30, 2012.\n[9] P. Hingston. A turing test for computer game bots. IEEE Transactions\non Computational Intelligence and AI in Games, 1(3):169–186, 2009.\n[10] H. Horn et al.\nMcts\/ea hybrid gvgai players and game difﬁculty\nestimation.\nIn IEEE Conference on Computational Intelligence and\nGames (CIG). IEEE Press, Piscataway, NJ, 2016.\n[11] N. Justesen et al.\nBlood bowl: A new board game challenge and\ncompetition for ai. In IEEE Conference on Games (CoG), pages 1–\n8. IEEE, IEEE Press, Piscataway, NJ, 2019.\n[12] S. Karakovskiy and J. Togelius.\nThe Mario AI Benchmark and\nCompetitions. IEEE Transactions on Computational Intelligence and\nAI in Games, 4(1):55–67, 2012.\n[13] S. Ontan et al.\nA survey of real-time strategy game ai research\nand competition in starcraft.\nIEEE Transactions on Computational\nIntelligence and AI in Games, 5(4):293–311, Dec 2013.\n[14] D. Perez-Liebana et al. General video game ai: Competition, challenges,\nand opportunities.\nIn AAAI Conference on Artiﬁcial Intelligence,\nAAAI16, page 43354337. AAAI Press, 2016.\n[15] J. Renz. Aibirds: The angry birds artiﬁcial intelligence competition. In\nAAAI Conference on Artiﬁcial Intelligence. AAAI press, 2015.\n[16] T. Schaul, J. Togelius, and J. Schmidhuber.\nMeasuring Intelligence\nthrough Games, 2011. arXiv, https:\/\/arxiv.org\/abs\/1109.1314.\n[17] N. R. Sturtevant.\nThe grid-based path planning competition.\nAI\nMagazine, 35:66–69, 2014.\n[18] O. Vinyals et al.\nGrandmaster level in starcraft ii using multi-agent\nreinforcement learning. Nature, 575:350–354, 2019.\n[19] V. Volz et al. Game-playing agent evaluation. In Dagstuhl Reports: Sem-\ninar 19511, page 110. Schloss Dagstuhl Leibniz-Zentrum fr Informatik,\n220.\n[20] G. N. Yannakakis and J. Togelius. Artiﬁcial Intelligence and Games.\nSpringer, Cham, Switzerland, 2018. http:\/\/gameaibook.org.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Towards Game-Playing AI Benchmarks via Performance Reporting Standards.pdf"}
{"title":"Interbank lending with benchmark rates: Pareto optima for a class of singular control games","authors":"Rama Cont, Xin Guo, Renyuan Xu","summary":"We analyze a class of stochastic differential games of singular control,\nmotivated by the study of a dynamic model of interbank lending with benchmark\nrates. We describe Pareto optima for this game and show how they may be\nachieved through the intervention of a regulator, whose policy is a solution to\na singular stochastic control problem. Pareto optima are characterized in terms\nof the solutions to a new class of Skorokhod problems with piecewise-continuous\nfree boundary.\n  Pareto optimal policies are shown to correspond to the enforcement of\nendogenous bounds on interbank lending rates. Analytical comparison between\nPareto optima and Nash equilibria provides insight into the impact of\nregulatory intervention on the stability of interbank rates.","url":"http:\/\/arxiv.org\/abs\/2005.05766v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2005.05766v3","published":1589187848000,"comment":"31 pages; 1 figure","pdf_text":"Interbank lending with benchmark rates:\nPareto optima for a class of singular control games\nIn memory of Mark H Davis, mentor and friend\nRama Cont ∗\nXin Guo †\nRenyuan Xu ∗\nFirst version: October 30, 2020. This version: June 12, 2021.\nAbstract\nWe analyze a class of stochastic diﬀerential games of singular control, motivated by the study of\na dynamic model of interbank lending with benchmark rates. We describe Pareto optima for this\ngame and show how they may be achieved through the intervention of a regulator, whose policy is\na solution to a singular stochastic control problem. Pareto optima are characterized in terms of the\nsolutions to a new class of Skorokhod problems with piecewise-continuous free boundary.\nPareto optimal policies are shown to correspond to the enforcement of endogenous bounds on\ninterbank lending rates. Analytical comparison between Pareto optima and Nash equilibria provides\ninsight into the impact of regulatory intervention on the stability of interbank rates.\nKeywords: LIBOR rate, interbank markets, stochastic diﬀerential game, singular stochastic control,\nPareto optimum, Nash equilibrium, Skorokhod problem.\nContents\n1\nIntroduction\n2\n1.1\nA model of interbank lending with benchmark rates . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nA class of stochastic diﬀerential games of singular control . . . . . . . . . . . . . . . . .\n4\n2\nMathematical formulation of the game\n5\n3\nRegulator’s problem\n6\n4\nPareto-optimal policies\n14\n4.1\nOptimal policy for the regulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.2\nPareto-optimal policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.3\nPareto-optimal policies for interbank lending . . . . . . . . . . . . . . . . . . . . . . . .\n22\n5\nExplicit solution for two players\n23\n5.1\nPareto-optimum for N = 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5.2\nBeneﬁts of regulation: Pareto optimum vs Nash equilibrium\n. . . . . . . . . . . . . . .\n26\n∗Mathematical Institute, University of Oxford. Rama.Cont@maths.ox.ac.uk, xur@maths.ox.ac.uk\n†Dept of Industrial Engineering and Operations Research, University of California, Berkeley. xinguo@berkeley.edu\n1\narXiv:2005.05766v3  [math.OC]  12 Jun 2021\nA Veriﬁcation theorem\n30\n1\nIntroduction\nThe market for interbank lending oﬀers an interesting example of strategic interaction among ﬁnancial\ninstitutions in which players react to the distribution of the actions of other players. One of the widely\ncommented features of the interbank market is the ﬁxing mechanism for interbank benchmark interest\nrates, the most well-known example of which is the London Interbank Oﬀer Rate (LIBOR) which plays\na central role in ﬁnancial markets. Historically these benchmarks have not been negotiated rates but a\n‘trimmed’ average of quotes collected daily from major banks. Every day, participating banks contribute\na quote representing their oﬀered rate; a calculation agent then ‘trims’ the tails of the distribution by\nremoving the highest and lowest quotes and computes the value of the benchmark rate as a weighted\naverage of the remaining non-discarded quotes (Avellaneda & Cont, 2010). The resulting benchmark\nrate –the LIBOR rate– then serves as a reference for the valuation of interbank loans and debt contracts,\nas well as many other ﬁnancial contracts indexed on the benchmark rate. A deviation (spread) of a\nbank’s rate from the benchmark may lead to a perception of credit risk and loss of market share -if the\nspread is positive- or an opportunity cost if the spread is negative, thus incentivizing banks to align\ntheir oﬀered rates with the benchmark.\nThis mechanism leads to strategic interactions among market participants in a dynamic setting,\nwhere interactions are mediated through an average action, or more generally through the distribution\nof actions of other participants and has been criticized for its vulnerability to manipulations (Avellaneda\n& Cont, 2010), which have been extensively documented (H. M. Treasury, 2012; Duﬃe & Stein, 2015).\nOne of the lessons from the manipulation of LIBOR and other benchmarks is that insuﬃcient attention\nhad been paid to incentives, strategic interactions, mechanism design and the role of the regulator in\nsuch markets.\n1.1\nA model of interbank lending with benchmark rates\nWe shall now describe a stylized model of interbank rates which represents interactions among banks\nin terms of a stochastic dynamic game.\nConsider ﬁrst an exogenous process rt representing a rate set by the central bank, with respect\nto which banks will position their lending rates. rt is typically modeled as a mean-reverting diﬀusion\nprocess driven by a multidimensional Brownian motion BBB representing risk factors driving random\nmacroeconomic shocks. Each bank i quotes a rate ri\nt at a ‘spread’ Xi\nt with respect to the reference rate\nrt: ri\nt = rt + Xi\nt. The spread of each bank i is aﬀected by the macroeconomic shocks but the bank may\ncontrol its rate ri\nt through positive or negative adjustments to its spread Xi\nt, which we may represent\nby a pair (ξi,+, ξi,−) of non-decreasing processes representing increases (resp. decreases) in the spread:\ndXi\nt = σσσi · dBBBt + dξi,+\nt\n−dξi,−\nt\n,\n(1.1)\nwhere σσσi is a volatility matrix representing the sensitivity of the spread Xi to macroeconomic factors.\nThe benchmark (‘LIBOR’) rate Lt is then deﬁned as a weighted average of these oﬀered rates:\nLt = rt + Xt,\nXt =\nN\nX\ni=1\naiXi\nt\nand\nai ≥0,\nN\nX\ni=1\nai = 1.\nNote that the ‘drift’ term in the dynamics (1.1) originates from the control. One may also consider an\nadditional drift term µidt in the uncontrolled dynamics, a positive drift corresponding to a bank whose\n2\ncreditworthiness is gradually deteriorating, leading to a steady increase of its spread. (See more general\nset-up in Section 2.)\nWe now turn to the incentives and costs faced by the banks. Each bank i receives interest income\nfrom its lending activity, at rate ri\nt. The interest income of the bank over a short period [t, t+dt] is ri\ntQi\nt dt\nwhere Qi\nt > 0 is the volume of lending activity (loan volume). Given that the bank can borrow at the\ninterbank Lt = rt + Xt, this represents an opportunity cost of (Xt −Xi\nt)Qi\nt dt. In a competitive lending\nmarket, the loan volume Qi\nt of bank i will be a decreasing function qi(.) of its spread ri\nt −Lt = Xi\nt −Xt\nrelative to the benchmark rate: Qi\nt = qi(Xi\nt −Xt). Assuming an inter-temporal discount rate of ρ > 0,\nthis leads to a running cost term\nZ ∞\n0\ne−ρt(Xt −Xi\nt) qi(Xi\nt −Xt) dt.\nFor example, an aﬃne dependence qi(x) = Qi\n0 −κix, where κi > 0 represents the sensitivity of loan\nvolume to the interest rate, leads to a linear-quadratic cost\nR ∞\n0 e−ρt[Qi\n0(Xt −Xi\nt) + κi(Xt −Xi\nt)2]dt.\nThese considerations only pertain to the relative costs of bank simultaneously engaging in borrowing\nand lending. Other constraints prevent the banks from deviating from the reference rate beyond a certain\nlevel; these are often ‘soft’, rather than hard (i.e., inequality), constraints and may be modeled by a\npenalty on |ri\nt|, or equivalently a running cost fi(Xi\nt) where fi is centered at some reference value and\nincreases fast enough (e.g., quadratically) at inﬁnity. As an example we shall use fi(x) = νi(x −s0)2\nwith νi > 0.\nThe benchmark ﬁxing mechanism described above may be incorporated in the model through a cost\nterm associated with the control (ξi,+, ξi,−). Recall that the LIBOR is computed as a trimmed average\nof quotes, discarding the highest and lowest ‘outliers’. This means an oﬀered rate Xi will not be taken\ninto account if it lies too far from the mean. In absence of collusion between banks, this mechanism\ndiscourages them from making large daily adjustments to their oﬀered rates, as a large upward or\ndownward adjustment may result in their quotes being disregarded in the benchmark calculation. This\nmay be modeled through a cost term which penalizes the size of the adjustment e.g., K+\ni dξi,+\nt\n+K−\ni dξi,−\nt\n,\nwith K+\ni , K−\ni > 0, where 1\/K−\ni (resp. 1\/K+\ni ) represents a typical distance (Xi −X)+ (resp. (X −Xi)+)\nbeyond which quotes are discarded. For instance one can take K+\ni = K−\ni = 1\/γ where γ represents a\nmeasure of dispersion (interquartile range or multiple of standard deviation) of the quote distribution.\nThe case of an asymmetric penalty K+\ni > K−\ni (resp. K+\ni < K−\ni ) is useful to model the case of a bank i\nsystematically quoting above (or below) the benchmark. This leads to an objective function\nJi(xxx;ξξξ) = E\n\" Z ∞\n0\ne−ρt\n \n\u0000Xt −Xi\nt\n\u0001\nqi\n\u0000Xi\nt −Xt\n\u0001\ndt + νi\n\u0000Xi\nt −s0\n\u00012 dt + K+\ni dξi,+\nt\n+ K−\ni dξi,−\nt\n! \f\f\f\f\fXXX0−= xxx\n#\n(1.2)\nfor bank i, where the control variable is a pair of non-decreasing processes (ξi,+, ξi,−) representing the\nrate adjustments of bank i and the expectation is taken with respect to the law of the controlled process\n(1.1). The controls ξi,+, ξi,−are in general allowed to be right-continuous with left limits (càdlàg) with\npossible jumps as well as continuous adjustments to the rates. Such controls are called singular controls\n(Beneš et al., 1980; Karatzas, 1983) and have been used for analyzing optimal investment policy and\noption pricing and hedging problems with transaction costs (Davis & Norman, 1990; Davis et al., 1993;\nKallsen & Muhle-Karbe, 2017; Zariphopoulou, 1992).\nIn the case where ai = 1\nN , qi = qj, νi = νj and K±\ni = K±\nj for i ̸= j, the payoﬀstructure is symmetric\nunder permutation of indices and this can be formulated as mean ﬁeld game (Lasry & Lions, 2007;\nHuang et al., 2006), which was studied under Nash equilibrium in (Guo & Xu, 2019). However we shall\nnot need this assumption and will treat below the case of a more general, not necessarily symmetric,\ncost function hi(XXXt). This is more natural for the interbank lending problem.\n3\n1.2\nA class of stochastic diﬀerential games of singular control\nMotivated by the example above, we study a class of N-player stochastic diﬀerential games, where each\nplayer i = 1, · · · , N controls a diﬀusive process Xi\nt through ξξξi := (ξi,+, ξi,−) additive control terms\ndXi\nt = µidt + σσσi · dBBBt + dξi,+\nt\n−dξi,−\nt\n,\nXi\n0−= xi,\n(1.3)\nand seeks to minimize the sum of a discounted running cost and a proportional cost of intervention\nJi(xxx;ξξξ) = E\n\" Z ∞\n0\ne−ρt \u0000hi(XXXt)dt + K+\ni dξi,+\nt\n+ K−\ni dξi,−\nt\n\u0001\n\f\f\f\f\fXXX0−= xxx\n#\n.\nThe ﬁrst two terms in (1.3) correspond to the ‘baseline’ (uncontrolled) diﬀusion dynamics, and the\nlast two term correspond to the control ξξξi = (ξi,+, ξi,−), modeled as a pair of non-decreasing càdlàg\nprocesses. Here we focus on Pareto-optimal outcomes.\nContribution.\nThe present work is a study of Pareto-optimal policies for the class of stochastic\nsingular control games considered above, motivated by the interbank lending problem. We relate the\nPareto optima of this game to the solution of a regulator’s problem, characterized as a high-dimensional\nsingular stochastic control problem which we study in detail.\nThe regularity analysis of the value\nfunction, following the approach of Soner & Shreve (1989), for the regulator’s problem enables us to\ncharacterize the optimal controls for this problem and subsequently the Pareto-optimal policies for the\nN-player game.\nWe obtain a description of Pareto-optimal policies in terms of a multidimensional Skorokhod problem\nfor a ‘regulated diﬀusion’ in a bounded region whose boundary is piece-wise smooth with possible\ncorners. The state process follows a diﬀusion process in the interior, and the control intervenes only at\nthe boundary to reﬂect it back into the interior.\nFinally, we derive explicit descriptions of Pareto-optimal policies when N = 2. This complements\nthe existing literature on Nash equilibrium for stochastic two player games (De Angelis & Ferrari,\n2018; Dianetti & Ferrari, 2020; Hernandez-Hernandez et al., 2015; Kwon & Zhang, 2015). Analytical\ncomparison between the Pareto-optimal and the Nash equilibrium solutions demonstrates the role of\nregulator in the interbank lending market.\nOur analysis for the general case (N ≥2) provides insights for regulatory intervention on the\ninterbank market. In particular, it allows us to quantify the impact of a regulator on the stability of\nthe benchmark rate.\nRelation with previous literature.\nStylized mean-ﬁeld models of interbank borrowing and lending\nhave been considered by Carmona et al. (2015) and Sun (2018), who focus on Nash equilibria in the\ncase of a large number of (indistinguishable) players. Here we consider the case of a ﬁnite number of\nplayers, allowing them to be non-identical which is more realistic in terms of the interbank problem at\nhand, and our focus is on Pareto optima and the role of a regulator.\nA related strand of literature consists of studies for central bank interventions on interest rates and\nexchange rates using an impulse control approach (Bensoussan et al., 2012; Cadenillas & Zapatero,\n2000; Jeanblanc-Picqué, 1993). In these approaches, interventions are associated with a ﬁxed cost. The\nsingular control framework adopted here seems more natural for modeling situations such as interbank\nmarkets where the cost of intervention is proportional to the action rather than ﬁxed. Singular controls\nallow for discontinuities and include impulse controls as special cases.\nNash equilibria for stochastic games of singular control have been studied by Chiarolla et al. (2013);\nDe Angelis & Ferrari (2018); Dianetti & Ferrari (2020); Hernandez-Hernandez et al. (2015); on the\n4\nother hand, there are few studies of Pareto-optimal strategies for such games. Aïd et al. (2017) consider\na two-player game in an impulse control framework between a representative energy consumer and a\nrepresentative electricity producer, and derive an asymptotic Pareto-optimal policy. Fischer & Livieri\n(2016) solve explicitly a mean-variance portfolio optimization problem with N stocks. Ferrari et al.\n(2017) and Wang & Ewald (2010) consider the problem of public good contribution and analyze the\nPareto-optimal policy for the N-player stochastic game under the framework of regular control and\nsingular control, respectively.\nThe analysis of Pareto optima in stochastic games is often through studying an auxiliary N-\ndimensional stochastic control problem. This approach can be traced back to the economic literature\non mechanism design and social welfare optimization in Bator (1957) and Coleman (1979). The math-\nematical challenge lies in the associated high-dimensional Hamilton–Jacobi–Bellman (HJB) equations\nand characterizing the optimal control policy from the regulator.\nOutline.\nThe remainder of the paper is organized as follows. Section 2 presents the mathematical\nformulation of the N-player stochastic diﬀerential game, and describes its relation with the auxiliary\ncontrol problem. Section 3 provides detailed analysis of the auxiliary control problem and the con-\nstruction of the optimal strategies. Section 4 characterizes the Pareto optima in terms of a sequence\nof Skorokhod problems. Implications of our analysis for the interbank lending problem are discussed\nin Section 4.3. Section 5 provides explicit solutions in the case N = 2, and compares it with the Nash\nequilibrium.\n2\nMathematical formulation of the game\nIn this section, we describe the mathematical framework of the N-player game.\nControlled dynamics.\nLet (Xi\nt)t≥0 ∈R denote the state of player i at time t, 1 ≤i ≤N. With\nabsence of controls, XXXt := (X1\nt , . . . , XN\nt ) ∈RN follows\nXXXt = XXX0 + µµµt + σσσBBBt,\nXXX0 = (x1, . . . , xN),\n(2.1)\nwhere BBB := (B1, . . . , BD) ∈RD is a D-dimensional Brownian motion on a ﬁltered probability space\n(Ω, F, {Ft}t≥0, P), and µµµ := (µ1, . . . , µN) ∈RN and σσσ := (σij)1≤i≤N,1≤j≤D ∈RN×D are constants with\nσσσσσσT ⪰λI for some λ > 0.\nWhen player i chooses a control ξξξi := (ξi,+, ξi,−) from an admissible control set Ui\nN, then Xi\nt evolves\nas\ndXi\nt = µidt + σσσi · dBBBt + dξi,+\nt\n−dξi,−\nt\n,\nXi\n0−= xi.\n(2.2)\nHere ξξξi = (ξi,+, ξi,−) is a pair of non-decreasing càdlàg processes and σσσi is the ith row of the volatility\nmatrix σσσ. We will denote by Pxxx the law of the process (2.2) and Exxx the expectation with respect to this\nlaw.\nAdmissible controls.\nThe set Ui\nN of admissible controls for player i is deﬁned as\nUi\nN =\n\b\n(ξi,+\nt\n, ξi,−\nt\n)t≥0 | ξi,+\nt\nand ξi,−\nt\nare Ft-progressively measurable, càdlàg non-decreasing,\nwith E\n\u0014Z ∞\n0\ne−ρtdξi,+\nt\n\u0015\n< ∞, E\n\u0014Z ∞\n0\ne−ρtdξi,−\nt\n\u0015\n< ∞, ξi,+\n0−= 0, ξi,−\n0−= 0\n\u001b\n.\n(2.3)\n5\nObjective functions.\nEach player i chooses a control (ξi,+, ξi,−) in Ui\nN to minimize\nJi(xxx;ξξξ) = Exxx\nZ ∞\n0\ne−ρt \u0002\nhi(XXXt)dt + K+\ni dξi,+\nt\n+ K−\ni dξi,−\nt\n\u0003\n.\n(N-player)\nHere ρ > 0 is a constant discount factor, K+\ni , K−\ni > 0 are the cost of controls, and hi(xxx) : RN →R+ is\nthe running cost function.\nWe have focused on characterizing Pareto optima of the game (N-player) subject to the dynamics\n(2.2).\nDeﬁnition 1 (Pareto optimality). ξξξ∗∈UN := ΠN\ni=1Ui\nN is a Pareto-optimal policy for the game\n(N-player) if and only if there does not exist ξξξ ∈UN such that, for all xxx ∈RN,\n∀i ∈{1, . . . , N},\nJi (xxx;ξξξ) ≤Ji (xxx;ξξξ∗) ;\nand\n∃j ∈{1, . . . , N},\nJj (xxx;ξξξ) < Jj (xxx;ξξξ∗) .\nPareto optima correspond to eﬃcient outcomes of a game, which may or may not come from de-\ncentralized optimization by N players. The intervention of a regulator may be necessary to enforce a\nPareto-optimal policy.\n3\nRegulator’s problem\nTo study Pareto optima for game (N-player), we introduce a ‘welfare function’ deﬁned as an aggregate\ncost:\nJ(xxx;ξξξ)\n=\nN\nX\ni=1\nLi Ji(xxx,ξξξ)\n(3.1)\n=\nExxx\nZ ∞\n0\ne−ρt\n\"\nH(XXXt)dt +\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n+\nN\nX\ni=1\nLiK−\ni dξi,−\nt\n#\n,\nwhere the dynamics of XXXt is given by (2.2), and\nH(xxx) :=\nN\nX\ni=1\nLihi(xxx), with Li > 0 and\nN\nX\ni=1\nLi = 1.\n(3.2)\nWe will show that Pareto optima of (N-player) correspond to solutions of the following auxiliary\nstochastic control problem\nv(xxx) = min\nξξξ∈UN J(xxx;ξξξ),\n(Regulator)\nwhich may be interpreted as the problem facing a market regulator seeking to optimize the aggregate\ncost (3.1).\nTo ensure the well-deﬁnedness of the game, the following assumptions will be made throughout,\nunless otherwise speciﬁed.\nAssumptions.\nThere exist C > c > 0 such that\nA1. ∀xxx ∈RN, 0 ≤H(xxx) ≤C(1 + ∥xxx∥2).\nA2. ∀xxx,xxx′ ∈RN, |H(xxx) −H(xxx′)| ≤C(1 + ∥xxx∥+ ∥xxx′∥)∥xxx −xxx′∥.\nA3. H(xxx) ∈C2(RN), H is convex, with 0 < c ≤∂2\nzzzH(xxx) ≤C for all unit direction zzz ∈RN.\n6\nFor example, for the payoﬀdescribed in the interbank lending problem in Section 1.1,\nH(xxx) =\nN\nX\ni=1\nLi\n\nκi\n \nxi −\nX\nj̸=i\najxj\n!2\n+ νi(xi)2\n\n\nwith\nκi, νi > 0.\n(3.3)\nThen H satisﬁes A1-A3 for any choice of weight Li > 0.\nWe shall ﬁrst analyze the regularity of the value function v, which is necessary for subsequently\nestablishing the existence and uniqueness of the optimal control. As we shall see, the optimal control\nfor (Regulator) yields a Pareto-optimal policy for game (N-player).\nThe regularity analysis of the value function involves several steps. The ﬁrst step is to show that\nthe value function for (Regulator) is a viscosity solution to the following HJB equation\nmax{ρu −Lu −H(xxx), β(∇u) −1} = 0,\n(3.4)\nwith the operator L = 1\n2\nPN\ni,j=1 σσσi · σσσj ∂2\nxixj + PN\ni=1 µi ∂xi, and\nβ(qqq) = max\n1≤i≤N\n\"\u0012\nqi\nLiK−\ni\n\u0013+\n∨\n\u0012\nqi\nLiK+\ni\n\u0013−#\n,\n(3.5)\nwhere qqq := (q1, · · · , qN), (a)+ = max{0, a} and (a)−= max{0, −a} for any a ∈R. The second step is\nto show that the value function for (Regulator) is W2,∞\nloc .\nLet us start with the following property of the value function v for (Regulator). Throughout the\npaper, K will be used in the proof for generic positive constants which may represent diﬀerent values\nfor diﬀerent estimates.\nProposition 2. Under Assumptions A1-A2, there exists K > 0 such that\n(i) 0 ≤v(xxx) ≤K(1 + ∥xxx∥2), ∀xxx ∈RN;\n(ii) |v(xxx) −v(xxx′)| ≤K(1 + ∥xxx∥+ ∥xxx′∥)∥xxx −xxx′∥, ∀xxx,xxx′ ∈RN.\nProof. First, v(xxx) ≥0 is clear by the non-negativity of H(xxx). Moreover, by the property that σσσσσσT ⪰λI\nwith λ > 0, it follows from a known estimate and martingale argument (Menaldi & Robin, 1983, (2.15))\nthat the solution { ˜XXXt}t≥0 := {xxx + µµµt + σσσBBBt}t≥0 with ξξξ = 000 satisﬁes\nExxx\nZ ∞\n0\ne−ρt∥˜XXXt∥2dt ≤K(1 + ∥xxx∥2),\n∀xxx ∈RN,\nfor some constant K > 0. By Assumption A1, there exists a constant K > 0 such that\nv(xxx) ≤J(xxx,000) ≤K(1 + ∥xxx∥2),\n∀xxx ∈RN.\nThus (i) of Proposition 2 is established.\nFor each ﬁxed xxx ∈RN, let\nUxxx = {ξξξ ∈U : J(xxx,ξξξ) ≤J(xxx;000)}.\n(3.6)\nBy Assumption A1,\nExxx\nZ ∞\n0\ne−ρt∥XXXt∥2dt ≤K(1 + ∥xxx∥2),\n∀xxx ∈RN,ξξξ ∈Uxxx.\n(3.7)\n7\nFor ξξξ ∈Uxxx, it is easy to verify\nExxx\nZ ∞\n0\ne−ρt∥ξξξt∥2dt ≤K(1 + ∥xxx∥2),\n(3.8)\nand\n|v(xxx) −v(xxx′)| ≤sup {|J(xxx;ξξξ) −J(xxx′;ξξξ)| : ξξξ ∈Uxxx ∪Uxxx′} , ∀xxx,xxx′ ∈RN.\nMeanwhile,\n|J(xxx;ξξξ) −J(xxx′;ξξξ)| ≤E\nZ ∞\n0\ne−ρt|H(XXXxxx\nt ) −H(XXXxxx′\nt )|dt.\nStatement (ii) for v follows by Assumption A2, along with the facts that XXXxxx\nt −XXXxxx′\nt = xxx −xxx′ and that\nfor any ξξξ ∈Uxxx ∪Uxxx′,\nExxx\nZ ∞\n0\ne−ρt∥XXXxxx\nt ∥dt\n≤\nK(1 + ∥xxx∥+ ∥xxx′∥),\n(3.9)\nExxx′\nZ ∞\n0\ne−ρt∥XXXxxx′\nt ∥dt\n≤\nK(1 + ∥xxx∥+ ∥xxx′∥).\nIn fact, if ξξξ ∈Uxxx, (3.9) follows immediately from (3.8) by the Hölder inequality. Meanwhile, if ξξξ ∈Uxxx′,\n(3.9) holds because\n∥XXXxxx\nt ∥≤∥XXXxxx′\nt ∥+ ∥xxx −xxx′∥≤∥XXXxxx′\nt ∥+ ∥xxx∥+ ∥xxx′∥.\nNext, we establish the viscosity property of the value function in the following sense.\nDeﬁnition 3 (Continuous viscosity solution). The value function v for problem (Regulator) is a\ncontinuous viscosity solution to (3.4) on RN if\n• ∀xxx0 ∈RN, ∀φ ∈C2(RN) such that xxx0 is a local minimum of (v −φ)(xxx) with v(xxx0) = φ(xxx0),\nmax{ρφ −Lφ −H(xxx), β(∇φ) −1} ≥0.\n• ∀xxx0 ∈RN, ∀φ ∈C2(RN) such that xxx0 is a local maximum of (v −φ)(xxx) with v(xxx0) = φ(xxx0),\nmax{ρφ −Lφ −H(xxx), β(∇φ) −1} ≤0.\nTheorem 4 (Viscosity solution). Under Assumptions A1 - A3, the value function v to the control\nproblem (Regulator) is convex and a continuous viscosity solution of the HJB equation (3.4).\nProof. The convexity of v follows from the joint convexity of J(xxx;ξξξ) in the following sense:\nJ(θxxx1 + (1 −θ)xxx2; θξξξ1 + (1 −θ)ξξξ2) ≤θJ(xxx1;ξξξ1) + (1 −θ)J(xxx2;ξξξ2),\n(3.10)\nholds for any xxx1,xxx2 ∈RN and any ξξξ1,ξξξ2 ∈UN. To see this, XXXxxx\nt depends linearly on (xxx,ξξξ), and both\nthe set UN and the function H are convex.\nUnder Assumption A1 - A3, the existence of the optimal\ncontrol to problem (Regulator) follows from Theorem 4.5 and Corollary 4.11 in (Menaldi & Taksar,\n1989). The convexity of v is veriﬁed as below, which follows the standard argument (Guo & Pham,\n2005; Williams et al., 1994). Take ξξξ∗\n1 = arg minξξξ∈UN J(xxx1; ξξξ) and ξξξ∗\n2 = arg minξξξ∈UN J(xxx2 ; ξξξ), then by\ndeﬁnition,\nθJ(xxx1;ξξξ∗\n1) + (1 −θ)J(xxx2;ξξξ∗\n2) = θv(xxx1) + (1 −θ)v(xxx2).\n(3.11)\n8\nNote that θξξξ∗\n1 + (1 −θ)ξξξ∗\n2 ∈UN by the convexity of UN, therefore\nv(θxxx1 + (1 −θ)xxx2) = min\nξξξ∈UN J(θxxx1 + (1 −θ)xxx2;ξξξ) ≤J(θxxx1 + (1 −θ)xxx2; θξξξ∗\n1 + (1 −θ)ξξξ∗\n2).\n(3.12)\nCombining (3.10), (3.11), and (3.12),\nv(θxxx1 + (1 −θ)xxx2) ≤θv(xxx1) + (1 −θ)v(xxx2).\nWe now show that v is both a viscosity super-solution and a viscosity sub-solution to the HJB equa-\ntion (3.4).\nSub-solution. Consider the following controls: ξi,−\nt\n= 0 and\nξi,+\nt\n=\n\u001a 0,\nt = 0−,\nηi,+,\nt ≥0,\nwhere 0 ≤ηi,+ ≤ϵ. Deﬁne the exit time\nτϵ := inf{t ≥0,XXXt \/∈¯Bϵ(xxx0)}.\nNote that XXX has at most one jump at t = 0 and is continuous on [0, τϵ).\nThe dynamic programming\nprinciple states that for any xxx ∈RN,\nv(xxx) = inf\nξξξ∈UN Exxx\n\"Z θ\n0\n \ne−ρtH(XXXt)dt +\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n+\nN\nX\ni=1\nLiK−\ni dξi,−\nt\n!\n+ e−ρθv(XXXθ)\n#\n,\n(3.13)\nfor any θ ∈F possibly depending on ξξξ in the inﬁmum of (3.13). Therefore,\nφ(xxx0) = v(xxx0) ≤Exxx0\nZ τϵ∧h\n0\ne−ρt\n\"\nH(XXXt)dt +\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n#\n+ Exxx0\n\u0002\ne−ρ(τϵ∧h)φ(XXXτϵ∧h)\n\u0003\n.\n(3.14)\nApplying Itô’s formula to the process e−ρtφ(XXXt) between 0 and τϵ∧h, and taking expectation, we obtain\nExxx0\n\u0002\ne−ρ(τϵ∧h)φ(XXXτϵ∧h)\n\u0003\n= φ(xxx0)\n+\nExxx0\n\u0014Z τϵ∧h\n0\ne−ρt(−ρφ + Lφ)(XXXt)dt\n\u0015\n+\nExxx0\n\"\nX\n0≤t≤τϵ∧h\n[φ(XXXt) −φ(XXXt−)]\n#\n.\n(3.15)\nCombining (3.14) and (3.15), we have\nExxx0\n\u0014Z τϵ∧h\n0\ne−ρt(ρφ −Lφ −H)(XXXt)dt\n\u0015\n−\nExxx0\n\"Z τϵ∧h\n0\ne−ρt(\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n)\n#\n−\nExxx0\n\"\nX\n0≤t≤τϵ∧h\nφ(XXXt) −φ(XXXt−)\n#\n≤0.\n(3.16)\n• Taking ﬁrst ηi,+ = 0 for all i = 1, 2, · · · , N, i.e., ξi,+ = ξi,−= 0, we see that XXX is continuous and\nthat only the ﬁrst term in the LHS of (3.16) is nonzero. Dividing the above inequality (3.16) by\nh and letting h →0, then by the dominated convergence theorem,\nρφ(xxx0) −Lφ(xxx0) −H(xxx0) ≤0.\n9\n• Now, by taking ηi,+ > 0 and ηj,+ = 0 for j ̸= i in (3.16), and noting that ξi,+ and XXX jump only\nat t = 0 with size ηi,+, we get\nExxx0\n\u0014Z τϵ∧h\n0\ne−ρt(ρφ −Lφ −H)(XXXt)dt\n\u0015\n−LiK+\ni ηi,+ −φ(xxx0 + ηi,+eeei) + φ(xxx0) ≤0.\nTaking h →0, then dividing by ηi,+ and letting η →0, we have\n−LiK+\ni ≤∂xiφ(xxx).\n• Meanwhile, taking an admissible control such that ξi,+ = 0 and\nξi,−\nt\n=\n\u001a 0,\nt = 0−,\nηi,−,\nt ≥0,\nwhere 0 ≤ηi,−≤ϵ. By a similar argument, we have\n∀i = 1, 2, · · · , N,\n∂xiφ(xxx) ≤LiK−\ni .\nThis proves the sub-solution viscosity property\nmax{ρφ −Lφ −H(x), β(∇φ) −1} ≤0.\nSuper-solution. This part is proved by contradiction. Suppose otherwise. Then there exist xxx0 ∈RN,\nϵ > 0, φ(xxx) ∈C2(RN) with φ(xxx0) = v(xxx0), v ≥φ in ¯Bϵ(xxx0) and ν > 0 such that for all xxx ∈¯Bϵ(xxx0),\nρφ(xxx0) −Lφ(xxx0) −H(xxx0) ≤−ν,\n(3.17)\nand for all i = 1, 2, · · · , N,\n−LiK+\ni + ν ≤∂xiφ ≤LiK−\ni −ν.\n(3.18)\nGiven any admissible control ξξξ, consider the exit time τϵ = inf{t ≥0,XXXt \/∈¯Bϵ(xxx0)}. Applying Itô’s\nformula (Meyer, 1976, Theorem 21) to e−ρtφ(xxx) and any semi-martingale {XXXt}t≥0 under admissible\ncontrol (ξi,+, ξi,−)N\ni=1 leads to\nExxx0\n\u0002\ne−ρτϵφ(XXXτϵ−)\n\u0003\n=\nφ(xxx0) + Exxx0\n\u0014Z τϵ\n0\ne−ρt(−ρφ + Lφ)(XXXt)dt\n\u0015\n+\nExxx0\n\"Z τϵ\n0\ne−ρt\nN\nX\ni=1\n∂xiφ(XXXt)[(dξi,+\nt\n)c −(dξi,−\nt\n)c]\n#\n+\nExxx0\n\" X\n0≤t<τϵ\ne−ρt[φ(XXXt) −φ(XXXt−)]\n#\n.\nNote that for all 0 ≤t < τϵ, XXXt ∈¯Bϵ(xxx0). Then, by (3.17), and noting that ∆Xi\nt = ∆ξi,+\nt\n−∆ξi,−\nt\n, we\nhave for all 0 ≤t < τϵ,\nφ(XXXt) −φ(XXXt−) =\nN\nX\ni=1\n∆Xi\nt\nZ 1\n0\n∂xiφ(XXXt + z∆XXXt)dz ≤\nN\nX\ni=1\n\u0002\n(LiK−\ni −ν)∆ξi,+\nt\n+ (LiK+\ni −ν)∆ξi,−\nt\n\u0003\n.\n10\nSimilarly,\nφ(XXXt) −φ(XXXt−) ≥\nN\nX\ni=1\n\u0002\n−(LiK−\ni −ν)∆ξi,−\nt\n−(LiK+\ni −ν)∆ξi,+\nt\n\u0003\n.\n(3.19)\nIn light of relations (3.17)-(3.19),\nExxx0\n\u0002\ne−ρτϵφ(XXXτϵ−)\n\u0003\n≥φ(xxx0)\n+\nExxx0\n\u0014Z τϵ\n0\ne−ρt(−H + ν)(XXXt)dt\n\u0015\n+\nExxx0\n\"Z τϵ−\n0\ne−ρt\nN\nX\ni=1\n−(LiK+\ni −ν)dξi,+\nt\n−(LiK−\ni −ν)dξi,−\nt\n#\n= φ(xxx0)\n−\nExxx0\nZ τϵ\n0\ne−ρt\n\"\nH(XXXt)dt +\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n+\nN\nX\ni=1\nLiK−\ni dξi,−\nt\n#\n+\nN\nX\ni=1\n\u0000Exxx0\n\u0002\ne−ρτϵLiK+\ni ∆ξi,+\nτϵ\n\u0003\n+ Exxx0\n\u0002\ne−ρτϵLiK−\ni ∆ξi,−\nτϵ\n\u0003\u0001\n+\nν\n\u001a\nExxx0\n\u0014Z τϵ\n0\ne−ρtdt\n\u0015\n+ Exxx0\n\u0014Z τϵ−\n0\ne−ρt(dξi,+\nt\n+ dξi,−\nt\n)\n\u0015\u001b\n.\n(3.20)\nNote that XXXτϵ−∈Bϵ(xxx0), XXXτϵ is either on the boundary ∂Bϵ(xxx0) or out of Bϵ(xxx0). However, there is\nsome random variable δ valued in [0, 1] such that\nxxxδ = XXXτϵ−+ δ∆XXXτϵ = XXXτϵ−+ δ(∆ξξξ+\nτϵ −∆ξξξ−\nτϵ) ∈∂¯Bϵ(xxx0).\nThen similar to (3.19), we have\nφ(xxxδ) −φ(XXXτϵ−) ≥δ\nN\nX\ni=1\n\u0002\n−(LiK−\ni −ν)∆ξi,−\nτϵ −(LiK+\ni −ν)∆ξi,+\nτϵ\n\u0003\n.\n(3.21)\nNote that XXXτϵ = xxxδ + (1 −δ)(∆ξξξ+\nτϵ −∆ξξξ−\nτϵ), thus\nv(xxxδ) ≤(1 −δ)\nN\nX\ni=1\n\u0000LiK+\ni ∆ξi,+\nτϵ + LiK−\ni ∆ξi,−\nτϵ\n\u0001\n+ v(XXXτϵ).\n(3.22)\nRecalling that v(xxxδ) ≥φ(xxxδ), inequalities (3.21)-(3.22) imply\n(1−δ)\nN\nX\ni=1\n\u0000LiK+\ni ∆ξi,+\nτϵ + LiK−\ni ∆ξi,−\nτϵ\n\u0001\n+v(XXXτϵ) ≥φ(XXXτϵ−)+δ\nN\nX\ni=1\n\u0002\n−(LiK−\ni −ν)∆ξi,−\nτϵ −(LiK+\ni −ν)∆ξi,+\nτϵ\n\u0003\n.\nTherefore,\nN\nX\ni=1\n\u0010\n(LiK+\ni −δν)∆ξi,+\nτϵ + (LiK−\ni −δν)∆ξi,−\nτϵ\n\u0011\n+ v(XXXτϵ) ≥φ(XXXτϵ−).\n11\nPlugging the last inequality into (3.20), along with φ(xxx0) = v(xxx0), yields\nExxx0\ne−ρτϵ\n\" N\nX\ni=1\n\u0000(LiK+\ni −δν)∆ξi,+\nτϵ + (LiK−\ni −δν)∆ξi,−\nτϵ\n\u0001\n+ v(XXXτϵ)\n#\n≥v(xxx0) −Exxx0\nZ τϵ\n0\ne−ρt\n\"\nH(XXXt)dt +\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n+\nN\nX\ni=1\nLiK−\ni dξi,−\nt\n#\n+\nN\nX\ni=1\n\u0000Exxx0\n\u0002\ne−ρτϵLiK+\ni ∆ξi,+\nτϵ\n\u0003\n+ Exxx0\n\u0002\ne−ρτϵLiK−\ni ∆ξi,−\nτϵ\n\u0003\u0001\n+\nν\n\u001a\nExxx0\n\u0014Z τϵ\n0\ne−ρtdt\n\u0015\n+ Exxx0\n\u0014Z τϵ−\n0\ne−ρt(dξi,+\nt\n+ dξi,−\nt\n)\n\u0015\u001b\n.\nHence\nExxx0e−ρτϵv(XXXτϵ) + Exxx0\nZ τϵ\n0\ne−ρt\n\"\nH(XXXt)dt +\nN\nX\ni=1\nLiK+\ni dξi,+\nt\n+\nN\nX\ni=1\nLiK−\ni dξi,−\nt\n#\n≥\nv(xxx0) + ν\n\u001a\nExxx0\n\u0014Z τϵ\n0\ne−ρtdt\n\u0015\n+ Exxx0\n\u0014Z τϵ−\n0\ne−ρt(dξi,+\nt\n+ dξi,−\nt\n)\n\u0015\n+ δExxx0\n\u0002\ne−ρτϵ∆ξi,+\nτϵ + e−ρτϵ∆ξi,−\nτϵ\n\u0003\u001b\n.\nWe now claim that there exists a constant g0 > 0 such that for all admissible control ξξξ,\nExxx0\n\u0014Z τϵ\n0\ne−ρtdt\n\u0015\n+ Exxx0\n\u0014Z τϵ−\n0\ne−ρt(dξi,+\nt\n+ dξi,−\nt\n)\n\u0015\n+ δExxx0\n\u0002\ne−ρτϵ∆ξi,+\nτϵ + e−ρτϵ∆ξi,−\nτϵ\n\u0003\n≥g0.\n(3.23)\nIndeed, one can always ﬁnd some constant G0 such that the C2 function\nψ(xxx) = G0((xxx −xxx0)2 −ϵ2)\nsatisﬁes\n(\nmini{ρψ −Lψ + 1, 1 −|∂xiψ|} ≥0,\non Bϵ(xxx0),\nψ = 0 on ∂Bϵ(xxx0).\nApplying Itô’s formula to e−ρtψ(xxx) and any semi-martingale {XXXt}t≥0 under admissible control\n(ξi,+, ξi,−)N\ni=1 leads to\nExxx0\n\u0002\ne−ρτϵψ(XXXτϵ−)\n\u0003\n≤ψ(xxx0) + Exxx0\n\u0014Z τϵ\n0\ne−ρtdt\n\u0015\n+\nN\nX\ni=1\nExxx0\n\u0014Z τϵ−\n0\ne−ρt(dξi,+\nt\n+ dξi,−\nt\n)\n\u0015\n.\n(3.24)\nSince ∂xiψ(xxx0) ≥−1 for all i = 1, 2, · · · , N,\nψ(XXXτϵ−) −ψ(xxxδ) ≥−∇ψ(XXXτϵ−−xxxδ) ≥−δ\nN\nX\ni=1\n∆ξi,−\nτϵ ,\nwhich, combined with (3.24), yields\nExxx0\n\u0014Z τϵ\n0\ne−ρtdt\n\u0015\n+\nN\nX\ni=1\nExxx0\n\u0014Z τϵ−\n0\n(dξi,+\nt\n+ dξi,−\nt\n)\n\u0015\n+ Exxx0\n\"\ne−ρτϵδ\nN\nX\ni=1\n∆ξi,−\nτϵ\n#\n≥\nExxx0\n\u0002\ne−ρτϵψ(xxxδ)\n\u0003\n−ψ(xxx0) = G0ϵ2.\nHence (3.23) holds with g0 = G0ϵ2.\n12\nWe can further show that the value function is a W2,∞\nloc (RN) solution to the HJB equation (3.4).\nTheorem 5 (Regularity). Under Assumption A3, the value function v deﬁned by (Regulator) belongs\nto W2,∞\nloc (RN) and is a solution to the HJB equation (3.4). In addition,\n0 ≤∂2\nzzzv(xxx) ≤C, a.e. for xxx ∈RN,\n(3.25)\nwith C > 0 deﬁned in Assumption A3. Furthermore the continuation region\nCN := {xxx | β(∇v(xxx)) < 1}\n(3.26)\nis bounded and non-empty. In addition, we have v ∈C4,α(CN).\nNote that W2,∞\nloc (RN) ⊂C1(RN) by the Sobolev embedding (see Corollary 9.15 in Chapter 9 from\nBrezis (2010)).\nRemark 6 (Uniqueness). Our primary goal is to identify and characterize Pareto optimal policies.\nTo this end, it suﬃces to show that the value function of (Regulator) is in W2,∞\nloc (RN) and a convex\nsolution to the HJB equation (3.4). Uniqueness of the HJB solution, although not essential, can be\nestablished by a veriﬁcation argument as discussed in Appendix A: the regularity, the convexity, and\nthe bounded second-order derivative of the value function allow to apply an Itô-Tanaka-Meyer Formula.\nProof. To prove (3.25), let ∆i(η) := (0, · · · , 0, η, 0, · · · , 0) be the N-dimensional row vector with the i-th\nentry being η for i = 1, 2, · · · , N. For any function F : RN →R, deﬁne the second diﬀerence of F in\nthe xi direction by\nδ2\ni (F,xxx, η) := F (xxx + ∆i(η)) + F (xxx −∆i(η)) −2F(xxx).\n(3.27)\nIt is easy to check\nδ2\ni (v,xxx, η) ≤sup{δ2\ni (J (·;ξξξ) ,xxx, η) : ξξξ ∈Uxxx}.\n(3.28)\nSince H ∈C2(RN), for xxx ∈RN,\nδ2\ni (H,xxx, η) = (η)2\nZ 1\n0\nZ λ\n−λ\n∂2\nxiH(x1, . . . , xi + µη, . . . , xN)dµdλ.\n(3.29)\nBy Assumption A3,\nδ2\ni (H,xxx, η) ≤C η2\nZ 1\n0\nZ λ\n−λ\ndµdλ = η2C.\n(3.30)\nHence\n0 ≤δ2\ni (v,xxx, η) ≤Cη2, xxx ∈RN, |η| ≤1.\n(3.31)\nThe lower bound of (3.31) follows from the convexity of v by Theorem 4.\nTo prove v ∈W2,∞\nloc , let G ⊆RN be any open ball and let ψ ∈C∞\n0 (RN) be any test function such\nthat supp(ψ) ⊂G. According to (3.31), we have\n|η−2δ2\ni (v,xxx, η)| ≤C for xxx ∈G and |η| ≤1.\n13\nTherefore by Theorem 1.1.2 in (Evans, 1990), there is a sequence ηk →0+ as k →∞such that, denoting\nby gk(xxx) := η−2\nk δ2\ni (v,xxx, ηk), we have gk(xxx) →Q weakly in Lp(G) for some p with 1 < p < ∞. It is then\neasy to see that\nZ\nRN ψ(xxx)Q(xxx)dxxx =\nZ\nRN ∂2\nxiψv(xxx)dxxx,\n∀ψ ∈C∞\n0 (G),\n(3.32)\nwhere Q = ∂2\nxiv. The existence and local boundedness of second order derivatives is now immediate:\nfor k = 1, 2, . . . , N, let eeek denote the unit vector in the direction of the positive xk axis; for any ﬁxed\ni ̸= j with 1 ≤i, j ≤N, let yyy be a new coordinate whose axis points to the eeei+eeej\n√\n2\ndirection, then\n∂2\nxixjv = ∂2\nyyyv −1\n2(∂2\nxiv + ∂2\nxjv).\nSince |∂xiv(xxx)| ≤Li max{K+\ni , K−\ni } (i = 1, 2, · · · , N) on RN but H grows at least quadratically by\nAssumption A3, CN must be bounded.\nFinally, let G be any open ball such that G ∈CN. By Theorem 6.13 in (Gilbarg & Trudinger, 2015),\nthe Dirichlet problem in G,\n\u001a ρ˜v −L˜v = H(xxx),\n∀x ∈G,\n˜v = v,\n∀x ∈∂G,\n(3.33)\nhas a solution ˜v ∈C0( ¯G)∩C2,α(G). In particular, ˜v−v ∈W2,∞(G), therefore by (3.33), ˜v−v ∈W1,2\n0 (G).\nBy Theorem 8.9 in (Gilbarg & Trudinger, 2015), v = ˜v in G, thus v ∈C2,α(G). By Theorem 6.17 in\n(Gilbarg & Trudinger, 2015), v ∈C4,α(G) thus v ∈C4,α(CN) for all α ∈(0, 1).\nRemark 7. The proof of Theorem 5 is inspired by the approach in (Soner & Shreve, 1989, Theorem 4.5)\nand (Williams et al., 1994, Theorem 3.1). In (Soner & Shreve, 1989), the following HJB equation (3.34)\n(See Eqn. (3.1) in (Soner & Shreve, 1989)) has been studied for an N-dimensional control problem\nmax\n\n\nρu −Lu −H(xxx),\nv\nu\nu\nt\nN\nX\ni=1\n(∂xiu)2 −1\n\n\n= 0.\n(3.34)\nComparing the gradient constraints in (3.34) with (3.4), it is clear that the operator β in (3.4) is less\nregular than ∥∇u∥2 in (3.34) as ∥∇u(·)∥2 has smoother and gradual changes in the state space RN. In\ncontrast, β in (3.4) involves a maximum operator as a result of game interactions.\nThe HJB equation (3.4) has appeared in Menaldi & Taksar (1989) for analyzing the convergence\nof ﬁnite variation controls. To our best knowledge, our characterization of the optimal control and\nregularity results are novel.\n4\nPareto-optimal policies\nThe regularity analysis of the value function for problem (Regulator) enables us to establish the\nexistence and the uniqueness of its optimal control, for any given weight (L1, · · · , LN) such that Li > 0\nand PN\ni=1 Li = 1 (Section 4.1). The optimal control in (Regulator) is then shown to lead to a Pareto-\noptimal policy for game (N-player) (Theorem 12) for each choice of weights (L1, · · · , LN).\n4.1\nOptimal policy for the regulator\nTo ensure the uniqueness of the Pareto-optimal policy, we impose the following assumption on the value\nfunction v.\n14\nA4. The diagonal dominates the row\/column in the Hessian matrix ∇2v. That is,\n∂2\nxiv(xxx)>\nX\nj̸=i\n\f\f∂2\nxixjv(xxx)\n\f\f , ∀i, = 1, 2, · · · , N and xxx ∈CN.\n(4.1)\nNote that a similar assumption has been used in (Gomes et al., 2010, Assumption 3) to analyze Nash\nequilibrium strategies. This assumption guarantees that the reﬂection direction of the Skorokhod prob-\nlem is not parallel to the boundary, and that the controlled dynamics are continuous when xxx ∈CN.\nAssumption A4 can be relaxed using techniques of Kruk (2000) to deal with possible jumps at the\nreﬂection boundary.\nGiven this additional assumption and the regularity of the value function, we are now ready to\ncharacterize the Pareto-optimal policy to game (N-player).\nWe shall show that when xxx ∈CN, the optimal policy may be constructed by solving a sequence\nof Skorokhod problems with piecewise C1 boundaries, then passing to the limit of this sequence of ϵ-\noptimal policies. We shall also show that the reﬂection ﬁeld of the Skorokhod problem can be extended\nto the entire state space under appropriate conditions, completing the construction of the Pareto-optimal\npolicy when xxx is outside CN.\nOptimal policy for xxx ∈CN.\nFirst, recall the deﬁnition of the Skorokhod problem in (Ramanan,\n2006).\nDeﬁnition 8 (Skorokhod problem). Let G be an open domain in RN with S = ∂G. Let Γ(aaa, b) = {xxx ∈\nRN : |xxx −aaa| = b}. To each point xxx ∈S, we will associate a set rrr(xxx) ⊂Γ(000, 1) called the directions of\nreﬂection. We say that a continuous process\nξξξt =\nZ t\n0\nNNN sdηs,\n(4.2)\nwith ηt = W\n[0,t] ξξξ the total variation up to time t, is a solution to a Skorokhod problem with data\n(xxx + µµµt + σσσBBBt, G,rrr,xxx) if\n(a) |NNN t| = 1, ηt is continuous and nondecreasing;\n(b) the process XXXt = xxx + µµµt + σσσBBBt +\nR t\n0 NNN sdηs satisﬁes XXXt ∈G, 0 ≤t < ∞, a.s;\n(c) for every 0 ≤t < ∞,\nηt =\nZ t\n0\n1(X\nX\nXs∈∂G,N\nN\nNs∈rrr(X\nX\nXs))dηs.\nNow let us introduce some notations for the Skorokhod problem associated with the continuation\nregion CN deﬁned in (3.26). By deﬁnition,\nCN = {xxx | β(∇v(xxx)) < 1} = ∩2N\nj=1Gj,\n(4.3)\nwhere for i = 1, 2, · · · , N,\nGi = {xxx | ∂xiv(xxx) < LiK−\ni },\nGi+N = {xxx | ∂xiv(xxx) > −LiK+\ni }.\n(4.4)\nDenote S = ∂CN as the boundary of CN, denote I(xxx) = {j | xxx \/∈Gj, j = 1, 2, · · · , 2N} as the boundary\nthat xxx lies on, and deﬁne the vector ﬁeld γj on each face Gj as\nγi = −eeei,\nγi+N = eeei,\n(4.5)\n15\nwhere eeei = (0, · · · , 0, 1, 0, · · · , 0) with the ith component being 1. Then the directions of the reﬂection\nis deﬁned as\nrrr(xxx) =\n\n\n\nX\nj∈I(xxx)\ncjγj(xxx) : ci ≥0 and\n\r\r\r\r\r\r\nX\nj∈I(xxx)\ncjγj(xxx)\n\r\r\r\r\r\r\n= 1\n\n\n.\n(4.6)\nTheorem 9 (ϵ-policy). Assume Assumptions A1-A4 and xxx ∈CN. For any ϵ > 0, there exist Cϵ ⊆CN\nnon-empty and rrrϵ such that the unique solution to the Skorokhod problem with data (xxx+µµµt+σσσBBBt, Cϵ,rrrϵ,xxx)\nis an ϵ-optimal (admissible) policy of the control problem (Regulator) with\nξξξϵ\nt =\nZ t\n0\nNNN ϵ\ns · dηϵ\ns,\n(4.7)\nand NNN ϵ\ns∈rrrϵ(XXXϵ\ns) on Sϵ, where XXXϵ\nt = xxx + µµµt + σσσBBBt + ξξξϵ\nt. That is,\n(1 −C0ϵ)J(xxx,ξξξϵ) ≤v(xxx),\nfor some constant C0 that is independent of ϵ. Here Cϵ ⊆C has piecewise smooth boundaries.\nProof. The proof consists of two steps. We ﬁrst construct an approximation Cϵ of CN with piecewise\nsmooth boundaries. Clearly, if ∂CN itself is piecewise smooth, the Cϵ = CN. We then show that the\nsolution to the Skorokhod problem with piecewise smooth boundary provides an ϵ-policy to the control\nproblem (Regulator).\nStep 1: Skorokhod problem with piecewise smooth boundary.\nLet φδ(xxx) ∈C∞(RN, R+) be\nsuch that φδ(xxx) = 0 for |xxx| ≥δ and\nZ\nRN φδ(xxx)dxxx = 1.\n(4.8)\nSince v ∈W2,∞\nloc (RN), consider a regularization of v(xxx) via φϵ such that\nvδ(xxx) := φδ ∗v(xxx).\n(4.9)\nSimilarly deﬁne Hδ(xxx) := φδ ∗H(xxx). The boundedness of v, ∇v, D2v on BR(0), with CN ⊂BR−1(000),\nimplies that Hδ and vδ are bounded uniformly on CN for δ < 1, and\nvδ →v,\n∇vδ →∇v,\nHδ →H\nuniformly in CN.\nDenote Kmax = maxi=1,2,··· ,N{LiK+\ni , LiK−\ni }, Kmin = mini=1,2,··· ,N{LiK+\ni , LiK−\ni } and recall C in (3.25)\nsuch that 0 ≤∂2\nzzzv(xxx) ≤C for any second order directional derivative ∂2\nzzz. Then, for any ϵk ∈(0, 1\n4),\nthere exists δk := δk(ϵk) ∈\n\u00000, ϵkKmin\nC\n\u0001\nsuch that for all δ ∈[0, δk], ∥∇vδ −∇v∥1 < Kminϵk. Take a\nnon-negative and non-increasing sequence {ϵk}k such that limk→∞ϵk = 0. Denote wδk(xxx) = β(∇vδk(xxx))\nand Cϵk := {xxx | wδk(xxx) < 1 −2ϵk} = ∩2N\nj=1Gϵk\nj , where i = 1, 2, · · · , N,\nGϵk\ni\n=\n{xxx | ∂xivδk(xxx) < (1 −2ϵk)LiK−\ni },\nGϵk\ni+N\n=\n{xxx | ∂xivδk(xxx) > (−1 + 2ϵk)LiK+\ni }.\n(4.10)\nSince ∥∇vδk −∇v∥1 < Kminϵk in CN and by the deﬁnition in (4.10), we have Cϵk ⊂CN.\nFirst, let us show Cϵk is non-empty when ϵk ∈(0, 1\n4). We claim that v attains its minimum in CN.\nTo see this, let θ ∈\n\u00000, Kmin\n2\n\u0001\nbe given, and choose xxxθ ∈RN such that v(xxxθ) ≤v(xxx) + θ,\n∀xxx ∈RN.\n16\nDeﬁne ψθ(xxx) := v(xxx) + θ∥xxxθ −xxx∥2 for all xxx ∈RN, and note that ψθ attains its minimum over RN at\nsome point yyyθ. In particular,\n0 = ∇ψθ(yyyθ) = ∇v(yyyθ) + 2θ(yyyθ −xxxθ).\n(4.11)\nBut also\nv(yyyθ) + θ∥yyyθ −xxxθ∥2 = ψθ(yyyθ) ≤ψθ(xxxθ) = v(xxxθ) ≤v(yyyθ) + θ.\nIt follows that ∥xxxθ −yyyθ∥≤1. Returning to (4.11), we have ∥∇v(yyyθ)∥2 ≤4θ2 < K2\nmin. Hence |∂xiv(yyyθ)| <\nKmin (i = 1, 2, · · · , N) and yyyθ ∈CN for all θ ∈(0, Kmin\n2 ). Since CN is bounded, there exists a sequence\nθk ∈(0, Kmin\n2 ) with limk→∞θk = 0 such that limk→∞yyyθk = yyy0 for some yyy0 ∈CN. From (4.11) we have\n∇v(yyy0) = 0 and hence yyy0 ∈CN. In addition, the convexity of v implies that v attains its minimum at\nyyy0. We now show that B(yyy0, Kmin\n4C ) ⊆Cϵk for all ϵk ∈(0, 1\n4). For any xxx ∈B(yyy0,\n1\nKmax) and i = 1, 2, · · · , N,\n|∂xivδk(xxx)| ≤|∂xiv(xxx)| + Kminϵ ≤K∥xxx −yyy0∥+ Kminϵ ≤1\n2Kmin.\nThe ﬁrst inequality holds by the deﬁnition of δk and the second inequality holds since ∥∇2v∥≤C. By\ndeﬁnition of Cϵk in (4.10), we have B(yyy0, Kmin\n4C ) ⊆Cϵk and hence Cϵk ̸= ∅holds for all ϵk ∈(0, 1\n4).\nAlso notice that ∂Gϵk\nj ∩Cϵk ∈C2 because vδk is smooth. Now, take any ϵ = ϵl from the sequence {ϵk}k\nand take δ ∈[0, δl], and denote Sϵ = ∂Cϵ as the boundary of Cϵ and Iϵ(xxx) =\n\b\nj | xxx \/∈Gϵ\nj, j = 1, 2, · · · , 2N\n\t\n.\nDeﬁne the vector ﬁeld γj on each face Gϵ\nj as (4.5) and the directions of reﬂection by\nrrrϵ(xxx) =\n\n\n\nX\nj∈Iϵ(xxx)\ncjγj(xxx) : ci ≥0 and\n\r\r\r\r\r\r\nX\nj∈Iϵ(xxx)\ncjγj(xxx)\n\r\r\r\r\r\r\n= 1\n\n\n.\n(4.12)\nWhen ϵ = 0, denote I(xxx) := I0(xxx) and rrr(xxx) := rrr0(xxx) for the index set and reﬂection cone of region CN,\nrespectively. Then deﬁne the normal direction on face Gϵ\nj as nϵ\nj (j = 1, 2, · · · , 2N) with\nnϵ\ni = −∇(∂xivδ)\n∥∇(∂xivδ)∥2\n,\nnϵ\ni+N =\n∇(∂xivδ)\n∥∇(∂xivδ)∥2\n,\ni = 1, 2, · · · , N.\nNote that the normal direction nϵ\nj (j = 1, 2, · · · , 2N) is well-deﬁned by the construction of (4.10).\nNext we show that nϵ\ni · γi =\n∂2\nxivδ\n∥∇(∂xivδ)∥2 > 0 and nϵ\ni+N · γi+N =\n∂2\nxivδ\n∥∇(∂xivδ)∥2 > 0 for i = 1, 2, · · · , N.\nTo do so, we shall show that Bδ(xxx) ∈CN for xxx ∈Sϵ. Note that (−1 + 2ϵ)LiK+\ni\n≤∂xiv(xxx) ≤\n(1 −2ϵ)LiK−\ni for xxx ∈¯Cϵ. For any yyy ∈Bδ(xxx), |∂xiv(xxx) −∂xiv(yyy)| ≤C∥xxx −yyy∥≤Cδ ≤ϵKmin. Therefore,\n(−1+ϵ)LiK+\ni ≤(−1+2ϵ)LiK+\ni −ϵKmin ≤∂xiv(yyy) ≤(1−2ϵ)LiK−\ni +ϵKmin ≤(1−ϵ)LiK−\ni . Thus, yyy ∈CN\nfor all yyy ∈Bδ(xxx) and xxx ∈Sϵ. Moreover, under Assumption A4, ∂2\nxivδ(xxx) =\nR\nyyy∈Bδ(xxx) ∂2\nxiv(yyy)φδ(xxx−yyy)dyyy >\n0 for all xxx ∈Sϵ.\nFurthermore, at each point xxx ∈Sϵ, there exists γ ∈rrrϵ(xxx) pointing into Cϵ. This is because there\nis no xxx ∈∂Cϵ such that {i, i + N} ∈Iϵ(xxx) for any i = 1, 2, · · · , N, and this implies |Iϵ(xxx)| ≤N for\nall xxx ∈∂Cϵ. Now Assumption A4 implies the following condition (3.8) in (Dupuis & Ishii, 1993): the\nexistence of scalars bj ≥0 j ∈Iϵ(xxx), such that\nbj ⟨γj(xxx), nj(xxx)⟩>\nX\nk∈Iϵ(xxx)\\{i}\nbk |⟨γk(xxx), nk(xxx)⟩| .\nHere we can simply take bj = 1 for all j ∈Iϵ(xxx).\nTherefore, by Theorem 4.8 and Corollary 5.2\nof (Dupuis & Ishii, 1993), there exists a unique strong solution to the Skorokhod problem with data\n({xxx + µµµt + σσσBt}t≥0, Cϵ,rrrϵ,xxx).\n17\nStep 2. ϵ-optimal policy.\nNow we shall show that the solution to the Skorokhod problem with data\n(xxx + µµµt + σσσBBBt, Cϵ,rrrϵ,xxx) is an ϵ-optimal policy of the control problem (Regulator) with\nξξξϵ\nt =\nZ t\n0\nNNN ϵ\ns · dηϵ\ns,\n(4.13)\nand NNN ϵ\ns ∈rrrϵ(XXXϵ\ns) on Sϵ, with XXXϵ\nt = xxx + µµµt + σσσBBBt + ξξξϵ\nt. By Theorem 4.8 of (Dupuis & Ishii, 1993), XXXϵ is\na continuous process. Since v ∈C4,α(CN), applying Itô formula to the semi-martingale XXXϵ yields\nv(xxx)\n=\nExxx\nZ ∞\n0\ne−ρt [H(XXXϵ\nt)dt + ∇v(XXXϵ\nt) · NNN ϵ\ntdηϵ\nt]\n≥\nExxx\nZ ∞\n0\ne−ρt \u0002\nH(XXXϵ\nt)dt + (1 −3ϵ)\n\u0002\n(NNN ϵ\nt)+ · K+\nL\nK+\nL\nK+\nL + (NNN ϵ\nt)−· K+\nL\nK+\nL\nK+\nL\n\u0003\ndηϵ\nt\n\u0003\n≥\n(1 −3ϵ)Exxx\nZ ∞\n0\ne−ρt \u0002\nH(XXXϵ\nt)dt +\n\u0002\n(NNN ϵ\nt)+ · K+\nL\nK+\nL\nK+\nL + (NNN ϵ\nt)−· K+\nL\nK+\nL\nK+\nL\n\u0003\ndηϵ\nt\n\u0003\n=\n(1 −3ϵ)J(xxx;ξξξϵ)\n(4.14)\nwhere NNN ϵ(xxx) ∈rrrϵ(xxx) on Sϵ,\nK+\nL\nK+\nL\nK+\nL := (L1K+\n1 , · · · , LNK+\nN), K−\nL\nK−\nL\nK−\nL := (L1K−\n1 , · · · , LNK−\nN), and Kmax = max\n1≤i≤N{LiK+\ni , LiK−\ni }. (4.15)\nThe ﬁrst inequality of (4.14) holds since ∥∇vδ −∇v∥L1 < Kminϵ for δ ∈[0, δ(ϵ)] and (4.10). The second\ninequality of (4.14) holds since H(xxx) ≥0.\nNow we are ready to establish the main theorem when xxx ∈CN.\nTheorem 10 (Existence and uniqueness of optimal control). Take xxx ∈CN and assume A1- A4. Then\nthere exists a unique optimal control ξξξ∗to problem (Regulator), which is a solution to the Skorokhod\nproblem (8) with data (xxx + µµµt + σσσBBBt, CN,rrr,xxx) such that XXX∗\nt ∈CN under control ξξξ∗.\nProof. Step 1: Optimality.\nThe existence of the optimal control to problem (Regulator) follows\nfrom Theorem 4.5 and Corollary 4.11 in (Menaldi & Taksar, 1989). According to Corollary 4.11 of\nMenaldi & Taksar (1989), if (NNN ϵk, ηϵk) is a sequence of ϵk−optimal policies for xxx and limk→∞ϵk →0,\nthen one can extract a subsequence ϵk′ such that\nξξξ\nϵk′\nt (ω) =\nZ t\n0\nNNN ϵk′\ns (ω)dηϵk′\ns (ω)\nk→∞\n→ξξξ∗\nt(ω)\ndt × dP −a.e.\n(4.16)\nwhere ξξξ∗, deﬁned in (4.16), is optimal, i.e., ξξξ∗∈arg minξξξ∈UN J(xxx;ξξξ). By the analysis in Theorem 9,\nthere exits a sequence of ϵk−optimal policy and ϵk →0 when k →∞. Therefore, the optimal control\nexists, which is the limit of ξξξ\nϵk′\nt (ω) deﬁned in (4.16).\nStep 2: Skorokhod condition. We next show that the limiting control ξξξ∗in (4.16) is a solution to\nthe Skorokhod problem (8) with data (xxx + µµµt + σσσBBBt, CN,rrr,xxx) such that XXX∗\nt ∈CN. Let us ﬁrst check\nProperty (b) of the Skorokhod problem (Deﬁnition 8). Denote\nA =\n\b\nω | XXX\nϵk′\nt (ω) ∈Cϵk′ for all 0 ≤t < ∞and all k′ ≥0\n\t\n,\nthen by deﬁnition (4.13), P(A) = 1. Also deﬁne\nB =\n\b\nω | XXX\nϵk′\nt\n→XXXt a.e. Leb on [0, ∞)\n\t\n,\n18\nthen by (4.16), P(B) = 1. For all ω ∈A ∩B, since CN is closed,\nXXXt(ω) ∈CN Leb a.e. on [0, ∞).\nNow we check property (c) in Deﬁnition 8, i.e., the optimal policy acts only on ∂CN, and its reﬂection\ndirection is in rrr(xxx).\nTake the smooth function φϵ in (4.8) and the smooth version of value function vϵ in (4.9). Let\nHϵ(xxx) = φϵ ∗H(xxx). From the HJB Equation (3.4),\nρv −Lv ≤H,\nβ(∇v) ≤1 in RN,\n(4.17)\nand\nρvϵ −Lvϵ ≤Hϵ,\nβ(∇vϵ) ≤1 in RN.\n(4.18)\nTo see this, take xxx ∈RN. By (4.17),\nρvϵ(xxx) −Lvϵ(xxx) =\nZ\nB(0,ϵ)\nφϵ(yyy)[ρ(xxx −yyy) −Lv(xxx −yyy)]dyyy ≤\nZ\nB(0,ϵ)\nφϵ(yyy)H(xxx −yyy)dyyy = Hϵ(xxx),\nwhere B(0, ϵ) = {xxx ∈RN : ∥xxx∥≤ϵ}. For any i = 1, 2, · · · , N and xxx ∈RN, by (4.17) we have\n∂xivϵ(xxx)\n=\n∂xi\n\u0012Z\nB(0,ϵ)\nφϵ(yyy)v(xxx −yyy)dyyy\n\u0013\n=\nZ\nB(0,ϵ)\nφϵ(yyy)∂xiv(xxx −yyy)dyyy\n≤\nZ\nB(0,ϵ)\nφϵ(yyy)LiK−\ni dyyy = LiK−\ni .\nSimilarly −LiK+\ni ≤∂xivϵ(xxx) holds for all i = 1, 2, · · · , N and xxx ∈RN. Hence β(∇vϵ) ≤1 in RN.\nLetting T > 0 and applying the Itô formula (Meyer, 1976, Theorem 21) to e−ρtvϵ(xxx) and the semi-\nmartingale {XXXt}t≥0 under any admissible control (ξi,+, ξi,−)N\ni=1 yields\nExxx\n\u0002\ne−ρtvϵ(XXXT)\n\u0003\n=\nvϵ(xxx) + Exxx\nZ T\n0\ne−ρt (Lvϵ −ρvϵ) (XXXt)dt\n+\nExxx\nZ T\n0\ne−ρt∇vϵ(XXXt) · dξξξt\n+\nExxx\nZ T\n0\nX\n0≤t<T\ne−ρt(vϵ(XXXt) −vϵ(XXXt−) −∇vϵ · (XXXt)(ξξξt −ξξξt−)),\nwith the last term coming from the jumps of XXXt. By (4.18),\nExxx\n\u0002\ne−ρTvϵ(XXXT)\n\u0003\n+ Exxx\nZ T\n0\ne−ρtHϵ(XXXt)dt −Exxx\nZ T\n0\ne−ρt∇vϵ(XXXt) · dξξξt\n+Exxx\nZ T\n0\nX\n0≤t<T\ne−ρt(−vϵ(XXXt) + vϵ(XXXt−) + ∇vϵ(XXXt) · (ξξξt −ξξξt−)) ≥vϵ(xxx).\n(4.19)\nMoreover, Hϵ, vϵ are bounded uniformly on CN for ϵ < 1 because v, ∇v, D2v are bounded on B(0, R),\nwith CN ⊂B(0, R −1), thus\nvϵ →v,\n∇vϵ →∇v,\nHϵ →H\nuniformly in CN.\n19\nMeanwhile, for ∀xxx ∈CN,\nv(xxx) = Exxx\nZ ∞\n0\ne−ρt \u0002\nH(XXX∗\nt)dt +\n\u0002\n(NNN ∗\nt)+ · K+\nL\nK+\nL\nK+\nL + (NNN ∗\nt)−· K−\nL\nK−\nL\nK−\nL\n\u0003\ndη∗\nt\n\u0003\n,\n(4.20)\nwhere XXX∗\nt = xxx +µµµt +σσσBBBt +ξξξ∗\nt with ξξξ∗\nt :=\nR t\n0 NNN ∗\nsdη∗\ns the optimal control, and K+\nL\nK+\nL\nK+\nL and K−\nL\nK−\nL\nK−\nL are deﬁned in\n(4.15). In particular,\nExxx\nZ ∞\n0\ne−ρtdη∗\nt < ∞,\n(4.21)\nwhich leads to\nExxx\nZ T\n0\ne−ρt \u0002\n(NNN ∗\nt)+ · K+\nL\nK+\nL\nK+\nL + (NNN ∗\nt)−· K−\nL\nK−\nL\nK−\nL\n\u0003\ndη∗\nt < ∞.\nBy the bounded convergence theorem and (4.19),\nExxx\n\u0002\ne−ρTv(XXX∗\nT)\n\u0003\n+ Exxx\nZ T\n0\ne−ρtH(XXX∗\nt)dt −Exxx\nZ T\n0\ne−ρt∇v(XXX∗\nt) · NNN ∗\ntdη∗\nt\n+Exxx\nZ T\n0\nX\n0≤t<T\ne−ρt \u0000−v(XXX∗\nt) + vϵ(XXX∗\nt−) + ∇v(XXX∗\nt) · NNN ∗\nt(η∗\nt −η∗\nt−)\n\u0001\n≥v(xxx).\n(4.22)\nThe last term on the left-hand side is nonpositive because of convexity of v, hence\nExxx\n\u0002\ne−ρTv(XXX∗\nT)\n\u0003\n+ Exxx\nZ T\n0\ne−ρtH(XXX∗\nt)dt −Exxx\nZ T\n0\ne−ρt∇v(XXX∗\nt) · NNN ∗\ntdη∗\nt ≥v(xxx).\nLetting T →∞, by the boundedness of XXX∗\nt, β(∇v) ≤1, |NNN ∗\nt| = 1, and (4.21),\nExxx\nZ ∞\n0\ne−ρtH(XXX∗\nt)dt −Exxx\nZ ∞\n0\ne−ρt∇v(XXX∗\nt) · NNN ∗\ntdη∗\nt ≥v(xxx).\nAlong with (4.20), we have\n0 ≥Exxx\nZ ∞\n0\ne−ρt \u0000\u0002\n∇v(XXX∗\nt) + K+\nL\nK+\nL\nK+\nL\n\u0003\n· (NNN ∗\nt)+dη∗\nt +\n\u0002\n−∇v(XXX∗\nt) + K−\nL\nK−\nL\nK−\nL\n\u0003\n· (NNN ∗\nt)−dη∗\nt\n\u0001\n.\nGiven β(∇v) ≤1, we have −K+\ni ≤vxi(xxx) ≤K−\ni , ∀x ∈RN and i = 1, 2, · · · , N. Hence\n0 ≥Exxx\nZ ∞\n0\ne−ρt \u0000\u0002\n∇v(XXX∗\nt) + K+\nL\nK+\nL\nK+\nL\n\u0003\n· (NNN ∗\nt)+dη∗\nt +\n\u0002\n−∇v(XXX∗\nt) + K−\nL\nK−\nL\nK−\nL\n\u0003\n· (NNN ∗\nt)−dη∗\nt\n\u0001\n≥0.\nThis implies dη∗\nt = 0 when β(∇v(XXX∗\nt)) < 1 a.e. in t. Also, when dη∗\nt ̸= 0, NNN ∗\nt(xxx) ∈rrr(xxx) for xxx ∈S a.e.\nfor t ∈[0, ∞), where the reﬂection cone rrr(xxx) is deﬁned in (4.6).\nBy Assumption A4, for any xxx ∈∂CN and γ(xxx) ∈rrr(xxx), γ(xxx) is not parallel to ∂CN at xxx. Hence,\nproperty (a) holds, i.e., the optimal control is continuous.\nStep 3: Uniqueness. It remains to show the uniqueness of the optimal control. This is done by\na contradiction argument. Suppose that there are two optimal controls {ξξξ∗}t≥0 and {ξξξ∗∗}t≥0 such that\nξξξ∗̸= ξξξ∗∗almost surely. Let {XXX∗\nt}t≥0 and {XXX∗∗\nt }t≥0 be the corresponding trajectories. Let ξξξt = ξξξ∗\nt +ξξξ∗∗\nt\n2\nand XXXt = X\nX\nX∗\nt +X\nX\nX∗∗\nt\n2\n. Then by Assumption A3,\nv(xxx) −J(xxx;ξξξt)\n=\n(J(xxx;ξξξ∗) + J(xxx;ξξξ∗∗))\n2\n−J(x;ξξξ)\n≥\nExxx\nZ ∞\n0\ne−ρt\n\u0014H(XXX∗\nt) + H(XXX∗∗\nt )\n2\n−H\n\u0012XXX∗\nt + XXX∗∗\nt\n2\n\u0013\u0015\ndt > 0.\nTherefore v(xxx) > J(xxx;ξξξ), which contradicts the optimality of {ξξξ∗\nt}t≥0 and {ξξξ∗∗\nt }t≥0. Hence the uniqueness\nof the optimal control.\n20\nOptimal policy for xxx \/∈CN.\nWhen xxx \/∈CN, the optimal policy is to jump immediately to some point\nˆxxx ∈CN and then follows the optimal policy in CN. We will need the following assumption so that the\nreﬂection ﬁeld of the Skorokhod problem is extendable to the RN plane (Dupuis & Ishii, 1991). Note\nthat when N = 2, A5 follows directly from Assumptions A1-A3.\nA5. There is a map π : RN →CN satisfying π(xxx) = xxx for all xxx ∈CN and π(xxx) −xxx ∈rrr(π(xxx)).\nThis assumption was also adopted in (Dupuis & Ishii, 1991, Assumption 3.1).\nTheorem 11. Given A1-A3, and A5. For any xxx \/∈CN, there exists an optimal policy π such that\nπ(xxx) ∈∂CN at time 0 and\nv(xxx) = v(π(xxx)) + l(xxx −π(xxx)),\nwith l(yyy) = P\ni li(yi), where\nli(yi) =\n\u001a LiK−\ni yi,\nif yi ≥0,\n−LiK+\ni yi,\nif yi < 0.\n(4.23)\nProof. Notice that l(yyy) is convex and\nli(yi) =\nmax\n−LiK+\ni ≤k≤LiK−\ni\n{kyi} = max{−LiK+\ni yi, LiK−\ni yi} for yi ∈R.\nHere we deﬁne two linear approximations which correspond to the lower and the upper bounds of the\nvalue function v(xxx), respectively.\nFor xxx ̸∈CN, deﬁne\nu1(xxx)\n=\nv(π(xxx)) + ∇v(π(xxx)) · (xxx −π(xxx)),\nu2(xxx)\n=\nv(π(xxx)) + l(xxx −π(xxx)).\n(4.24)\nThen u2(xxx) ≥v(xxx) by the sub-optimality of the policy, and u1(xxx) ≤v(xxx) by convexity. Thus,\nu1(xxx) ≤v(xxx) ≤u2(xxx).\n(4.25)\nWe now show u1(xxx) = u2(xxx). By Assumption A5, u1 and u2 in (4.24) can be rewritten as\nu1(xxx)\n=\nv(π(xxx)) + ∇v(π(xxx)) · d(π(xxx))∥xxx −π(xxx)∥,\nu2(xxx)\n=\nv(π(xxx)) + PPP(π(xxx)) · d(π(xxx))∥xxx −π(xxx)∥,\nwhere d(π(xxx)) ∈rrr(π(xxx)) and PPP(xxx) = (P1, · · · , PN)(xxx), with\nPi(xxx) = LiK+\ni 1(∂xiv(xxx) < 0) + LiK−\ni 1(∂xiv(xxx) > 0).\nTherefore u1(xxx) = u2(xxx).\n4.2\nPareto-optimal policies\nPareto-optimal policies for (N-player) may be constructed from the optimal control for problem\n(Regulator) as described below.\nTheorem 12. The optimal control for the regulator’s problem (Regulator) yields a Pareto-optimal\npolicy for the game (N-player).\n21\nProof. To see this, take the payoﬀfunction Ji in (N-player), v(xxx) the value function in (Regulator),\nand the optimal control ξξξ∗:= (ξξξ1∗, . . . ,ξξξN∗), if exists, to problem (Regulator), then for any ξξξ :=\n(ξξξ1, . . . ,ξξξN) ∈UN and Li, with Li > 0, PN\ni=1 Li = 1,\nN\nX\ni=1\nLiJi(xxx;ξξξ) ≥v(xxx),\n(4.26)\nwhere value v(xxx) is reached when player i takes the control ξξξi∗\nt (i = 1, 2, . . . , N).\nIf there is another ξξξ′ := (ξξξ1′, . . . ,ξξξN′) ∈UN and k ∈{1, . . . , N} such that\nJk(xxx;ξξξ1′, . . . ,ξξξN′) < Jk(xxx;ξξξ1∗, . . . ,ξξξN∗),\nthen given Li > 0 for all i, there must exists j ∈{1, . . . , N} such that\nJj(xxx;ξξξ1′, . . . ,ξξξN′) > Jj(xxx;ξξξ1∗, . . . ,ξξξN∗).\nHence the control ξξξ∗is a Pareto-optimal policy by deﬁnition.\nCombining Theorems 10, 11 and 12 yields the following result which summarizes the structure of the\nset of Pareto optima:\nTheorem 13 (Pareto-optimal policies). Under Assumptions A1-A5, for any set of weights LLL =\n(L1, · · · , LN) with Li > 0 and PN\ni=1 Li = 1, the unique solution ξL\nξL\nξL ∈UN to the regulator’s problem\n(Regulator) yields a Pareto-optimal policy for the game (N-player).\nThe analytical structure of the continuation region (4.3) and the Pareto-optimal policy suggest the\nfollowing description: XXXt evolves according to the uncontrolled diﬀusion process inside the interior of\nCN and when it hits boundary at a point belonging to ∂Gi or ∂Gi+N, then bank i will adjust its rate to\npush it back instantaneously inside CN. In particular the optimal policies lead to continuous controls\nξi.\n4.3\nPareto-optimal policies for interbank lending\nLet us now translate these results in the setting of the interbank lending model described in Section 1.1.\nTheorem 13 implies that Pareto optima for the interbank lending market may be described in terms\nof the policy of a regulator facing the optimization problem (Regulator) with an aggregate payoﬀ\nfunction (3.2) representing a weighted average of payoﬀs of individual banks.\nUnder a Pareto-optimal policy, the interbank rates may be described as a ‘regulated diﬀusion’ in\na bounded region CN deﬁned by (4.3). The boundedness of CN implies that the payoﬀstructure (1.2)\nleads to endogenous bounds on the interbank rates: the regulator only intervenes when the rates reach\nthese bounds, represented by the boundary of the continuation region CN.\nThe Pareto-optimal policy leads XXXt to remain conﬁned in the bounded region CN, which implies in\nparticular that the spread Xi remains bounded. In the context of the LIBOR mechanism, this can be\nseen as the impact of ‘trimmed’ averaging, which is the origin of the terms K+\ni , K−\ni , as explained in\nSection 1.1: as banks internalize the risk of being ‘outliers’ in the benchmark ﬁxing, they conﬁne their\nrates to a bounded region.\nThe process XXXt diﬀuses in the interior of CN, following the random shocks banks are subjected to,\nand is pushed into the interior when it reaches the boundary. More precisely, the boundary ∂CN is\ncomposed of 2N ‘faces’ corresponding to the saturation of the constraints in (4.4). Edges correspond to\nintersections of two or more faces. When XXXt reaches a point xxx ∈∂CN, action is taken by all banks i such\nthat xxx \/∈Gi ∪Gi+N: if xxx \/∈Gi then Xi is reduced i.e. dξi,−> 0 and if xxx \/∈Gi+N then Xi is increased\n22\ni.e. dξi,+ > 0. When XXXt reaches the interior of such a face, only bank i adjusts its rate in order to push\nback XXXt to the interior. Similarly, if XXXt reaches an edge, two or more banks need to simultaneously\nadjust their rates. The rate at which such simultaneous adjustments occur is given by the intersection\nlocal time (Rosen, 1987) of (X1, ..., XN) on the boundary. Therefore Pareto-optimal policy rarely leads\nto more than one bank’s rate to be adjusted; a simultaneous rate adjustment by several banks is most\nlikely not associated with a Pareto-optimal policy and is thus a signature of a non-optimal behavior by\nbanks.\nWe also note that, our admissible controls allow for discontinuous adjustments of rates, and Pareto-\noptimal policies correspond to instantaneously pushing the process to the interior.\nAs discussed in\nTheorem 11, Pareto-optimal policies may involve an initial push at t = 0 to bring the initial condition\ninto CN, which we may interpret as the entry of a new bank into the interbank market.\nThe set of all such Pareto optima is parameterized by the set of allocations L = (L1, ..., LN) with\nLi > 0 and PN\ni=1 Li = 1. These allocations lead to diﬀerent outcomes across banks. A natural choice\nis to take Li proportional to the loan volume of bank i; (3.2) then represents an aggregate wealth\nmaximization problem and this policy leads to the same pro-rata cost across banks. As is clear from\n(3.5), choosing a higher weight Li leads to a tighter control on the rates of bank i.\n5\nExplicit solution for two players\nWe now study in more detail the structure of the optimal strategies for the case of N = 2. Our analytical\nresults illustrate the diﬀerence between Nash equilibria and Pareto optima and demonstrate the impact\nof regulatory intervention in this game.\n5.1\nPareto-optimum for N = 2\nFor the special case of N = 2, we can derive explicitly its Pareto-optimal solution. For ease of exposition,\nwe shall assume the following conditions in the case of N = 2.\nB1. a1 = a2 and L1 = L2. In other words, the regulator allocates equal weights to the banks.\nB2. h1(x1, x2) = h2(x1, x2) = h(x1 −x2), h ∈C3(R) is symmetric, and there exist 0 < c < C such that\nc < h′′ < C, and h′′ is non-decreasing and bounded away from 0.\nB3. µ1 = µ2 = 0, K+\n1 = K−\n1 =: K1 > 0 and K+\n2 = K−\n2 =: K2 > 0.\nNote that Assumption B2 is more general than Assumptions A1-A3. As a result, we will see in\nProposition 16 that the non-action region may not necessarily be bounded and the Pareto-optimal policy\nfor the game may not be unique with ﬁxed weights L1 = L2.\nUnder Assumption B3, the rates X1\nt and X2\nt are assumed to be\nXi\nt = σσσi · dBBBt + dξi,+\nt\n−dξi,−\nt\n, with xi\n0−= xi, i = 1, 2.\n(5.1)\nThe value function v(x1, x2) of (Regulator) becomes\nv(x1, x2)\n=\ninf\n(ξξξ1,ξξξ2)∈U2 J(x1, x2,ξξξ1,ξξξ2) =\ninf\n(ξξξ1,ξξξ2)∈U2\n1\n2\n\u0002\nJ1(x1, x2,ξξξ1,ξξξ2) + J2(x1, x2,ξξξ1,ξξξ2)\n\u0003\n(5.2)\n=\ninf\n(ξξξ1,ξξξ2)∈U2 E(x1,x2)\n\u0014Z ∞\n0\ne−ρt\n\u0012\nh\n\u0000X1\nt −X2\nt\n\u0001\ndt + K1\n2 dξ1,+\nt\n+ K1\n2 dξ1,−\nt\n+ K2\n2 dξ2,+\nt\n+ K2\n2 dξ2,−\nt\n\u0013\u0015\n,\nsubject to (5.1).\n23\nLemma 14. Assume K2 < K1 and B1-B3. Then for any (ξξξ1∗,ξξξ2∗) ∈arg inf(ξξξ1,ξξξ2)∈U2 J(x1, x2,ξξξ1,ξξξ2),\n(ξ1,+∗\nt\n, ξ1,−∗\nt\n) = (0, 0) for any t ≥0 a.s..\nProof. The statement is proved by contradiction. Assume there exists an optimal policy (ξξξ1∗,ξξξ2∗) ∈\narg inf(ξξξ1,ξξξ2)∈U2 J(x1, x2, (ξξξ1,ξξξ2)) and t0 ≥0 such that\nξ1∗,+\nt0\n> 0.\nSince ξ1,+∗is a non-decreasing process, we have ξ1,+∗\nt\n> 0 for all t ≥t0. Now construct the following\nadmissible policy (ξξξ\n1,ξξξ\n2) such that, ∀t ≥0,\n\n\n\n\n\nξ\n2,−\nt\n= ξ1∗,+\nt\n+ ξ2∗,−\nt\n,\nξ\n1,+\nt\n= 0,\nξ\n1,−\nt\n= ξ1∗,−\nt\n, ξ\n2,+\nt\n= ξ2∗,+\nt\n.\n(5.3)\nThen\nJ(x1, x2,ξξξ1∗,ξξξ2∗) −J(x1, x2,ξξξ\n1,ξξξ\n2) = E(x1,x2)\n\u0014Z ∞\n0\ne−ρtK1 −K2\n2\ndξ1∗,+\nt\n\u0015\n> 0,\nwhich contradicts the optimality of the control process (ξξξ1∗,ξξξ2∗).\nWe now show that solving the control problem (5.1)-(5.2) is equivalent to the following control\nproblem (5.4)-(5.5) when K1 > K2,\nu(y)\n=\ninf\nηηη∈U1\nbJ(y,ηηη) = inf\nηηη∈U1 Ey\n\u0014Z ∞\n0\ne−ρt\n\u0012\nh (Yt) dt + K2\n2 dη+\nt + K2\n2 dη−\nt\n\u0013\u0015\n,\n(5.4)\nwhere\ndYt\n=\n(σσσ1 −σσσ2) · dBBBt −dη+\nt + dη−\nt , with Y0−= y.\n(5.5)\nLemma 15 (Equivalence). Assume B1-B3 and K1 > K2, then\n(i) v(x1, x2) = u(x1 −x2);\n(ii) If (ξξξ1∗,ξξξ2∗) ∈arg inf(ξξξ1,ξξξ2)∈U2 J(x1, x2, (ξξξ1,ξξξ2)), then (ξ1∗,+\nt\n, ξ1∗,−\nt\n) = (0, 0) ∀t a.s., and\nξξξ2∗∈arg infηηη∈U1 bJ(x1 −x2,ηηη);\n(iii) If ηηη∗∈arg infηηη∈U1 bJ(x1 −x2,ηηη), then ((000,000),ηηη) ∈arg inf(ξξξ1,ξξξ2)∈U2 J(x1, x2, (ξξξ1,ξξξ2)).\nProof. By Lemma 14, (ξ1∗,+\nt\n, ξ1∗,−\nt\n) = (0, 0) for any t ≥0 a.s.. Therefore, we can consider a smaller class\nof admissible control set where (ξ1,+\nt\n, ξ1,−\nt\n) = (0, 0) ∀t ≥0 and ξξξ2 ∈U1. Note that with (ξ1,+\nt\n, ξ1,−\nt\n) =\n(0, 0), we have\nX1\nt −X2\nt = (σσσ1 −σσσ2) · BBBt −ξ2,+\nt\n+ ξ2,−\nt\n+ (x1 −x2),\n(5.6)\nand\nJ(x1, x2,ξξξ1,ξξξ2) = E(x1,x2)\n\u0014Z ∞\n0\ne−ρt\n\u0012\nh\n\u0000X1\nt −X2\nt\n\u0001\ndt + K2\n2 dξ2,+ + K2\n2 dξ2,−\n\u0013\u0015\n.\n(5.7)\nClearly problem (5.6)-(5.7) is equivalent to the one-dimensional control problem (5.4)-(5.5) with y =\nx1 −x2. Hence the claim.\n24\nProposition 16 (Pareto-optimal solution when N = 2). Assume B1-B3.\n(i) If K1 = K2 = K, then the following control yields one Pareto-optimal policy to game (5.1)-(5.2):\nξξξ1∗\nt = (ξ1∗,+\nt\n, ξ1∗,−\nt\n) =\n\u0012\n0, max\n\u001a\n0, max\n0≤u≤t\n\b\n(x1 −x2) + (σσσ1 −σσσ2) · BBBu + ξ2∗,−\nu\n−c1\n\t\u001b\u0013\n,\nξξξ2∗\nt = (ξ2∗,+\nt\n, ξ2∗,−\nt\n) =\n\u0012\n0, max\n\u001a\n0, max\n0≤u≤t\n\b\n−(x1 −x2) + (σσσ2 −σσσ1) · BBBu + ξ1∗,−\nu\n−c1\n\t\u001b\u0013\n,\n(5.8)\nwhere c1 is the unique solution to\neσ\n√2ρ tanh\n\u0012√2ρ\neσ x\n\u0013\n= p′\n1(x) −K\n2\np′′\n1(x)\n,\n(5.9)\nand\np1(x) = E\n\u0014Z ∞\n0\ne−ρth (x + eσBt) dt\n\u0015\n,\n(5.10)\nwith eσ =\nqP2\ni=1\nPD\nj=1 σ2\nij. The associated Pareto-optimal value is\nv(x1, x2) =\n\n\n\n\n\n\n\n−\neσ2p′′\n1 (c1) cosh\n\u0010\n(x1−x2)\n√2ρ\neσ\n\u0011\n2ρ cosh\n\u0010\nc1\n√2ρ\neσ\n\u0011\n+ p1(x1 −x2),\n0 ≤\nx1 −x2 ≤c1,\nv(x2 + c1, x2) + K\n2 (x1 −x2 −c1),\nx1 −x2 ≥c1,\nv(−x1, −x2),\nx1 −x2 < 0.\n(5.11)\n(ii) If K1 > K2 then the following control yields a Pareto-optimal policy to game (5.1)-(5.2),\nξξξ1∗\nt\n=\n(0, 0),\nand ξξξ2∗\nt = (ξ2∗,+\nt\n, ξ2∗,−\nt\n) with\n(5.12)\nξ2∗,−\nt\n=\nmax\n\u001a\n0, max\n0≤u≤t\n\b\n−(x1 −x2) + (σσσ2 −σσσ1) · BBBu + ξ2∗,+\nu\n−ec1\n\t\u001b\n,\n(5.13)\nξ2∗,+\nt\n=\nmax\n\u001a\n0, max\n0≤u≤t\n\b\n(x1 −x2) + (σσσ1 −σσσ2) · BBBu + ξ2∗,−\nu\n−ec1\n\t\u001b\n,\n(5.14)\nwhere ec1 is the unique solution to\neσ\n√2ρ tanh\n\u0012√2ρ\neσ x\n\u0013\n= p′\n1(x) −K2\n2\np′′\n1(x)\n,\n(5.15)\nand the associated Pareto-optimal value is\nv(x1, x2) =\n\n\n\n\n\n\n\n−\neσ2p′′\n1 (ec1) cosh\n\u0010\n(x1−x2)\n√2ρ\neσ\n\u0011\n2ρ cosh\n\u0010\nec1\n√2ρ\neσ\n\u0011\n+ p1(x1 −x2),\n0 ≤\nx1 −x2 ≤ec1,\nv(x2 + ec1, x2) + K2\n2 ((x1 −x2) −ec1),\nx1 −x2 ≥ec1,\nv(−x1, −x2),\nx1 −x2 < 0.\n(5.16)\nRemark 17. Note that under B1-B3, the Pareto-optimal policy is no longer unique with ﬁxed L1 =\nL2 = 1\n2. For instance, when K1 = K2 = K, the following control yields another Pareto-optimal policy\nwith the same value function deﬁned in (5.11):\nξξξ1∗\nt = (ξ1∗,+\nt\n, ξ1∗,−\nt\n) = (0, 0)\nand ξξξ2∗\nt = (ξ2∗,+\nt\n, ξ2∗,−\nt\n), with\nξ2∗,−\nt\n= max\n\u001a\n0, max\n0≤u≤t\n\b\n−(x1 −x2) + (σσσ2 −σσσ1) · BBBu + ξ2∗,+\nu\n−c1\n\t\u001b\n,\nξ2∗,+\nt\n= max\n\u001a\n0, max\n0≤u≤t\n\b\n(x1 −x2) + (σσσ1 −σσσ2) · BBBu + ξ2∗,−\nu\n−c1\n\t\u001b\n.\n(5.17)\n25\nRemark 18. Under the Pareto-optimal policy, the controlled dynamics X1∗\nt\nand X2∗\nt\nare such that\nP(∥X1∗\nt −X2∗\nt ∥≤c1, ∀t ≥0) = 1. This suggests that there should be a mechanism, such as ‘trimming’,\nto maintain the dispersion of rates within a certain range. In addition, this solution form indicates that\nit is socially optimal for the more eﬃcient bank (i.e., the one with the lower cost of adjustment) to take\nthe lead in lending rate adjustment. The other banks then become ‘free riders’.\nProof. First let us prove the case when K1 > K2.\nBy Lemma 15, it is suﬃcient to focus on the\nsingle-agent problem (5.4)-(5.5) with y = x1 −x2. Following the standard analysis (Beneš et al., 1980;\nKaratzas, 1983), the HJB equation for the one-dimensional control problem follows (5.4)-(5.5) is\nmax\n\u001a\nρu(x) −h(x) −eσ2\n2 u′′(x), u′(x) −K2\n2 , −u′(x) −K2\n2\n\u001b\n= 0.\n(5.18)\nThere is a C2 solution (Beneš et al., 1980; Karatzas, 1983) given by\nu(x) =\n\n\n\n\n\n\n\n−\neσ2p′′\n1 (ec1) cosh\n\u0010\nx\n√2ρ\neσ\n\u0011\n2ρ cosh\n\u0010\nec1\n√2ρ\neσ\n\u0011\n+ p1(x),\n0 ≤\nx ≤ec1,\nu(ec1) + K2\n2 (x −ec1),\nx ≥ec1,\nu(−x),\nx < 0,\n(5.19)\nwhere ec1 is the unique positive solution to (5.15) and p1(x) is deﬁned as in (5.10). The corresponding\ncontrol of the regulator is a bang-bang type such that (5.13)-(5.14) hold. Furthermore, it is easy to see\nthat v(x1, x2) := u(x1 −x2), with u(x) deﬁned in (5.19), is indeed the value function of problem (5.2).\nNext when K1 = K2, ξ1,+ and ξ2,−controls Yt in the same direction with the same cost. The same\nholds for ξ2,+ or ξ1,−, hence the Pareto-optimal policy (5.8) and (5.17).\n5.2\nBeneﬁts of regulation: Pareto optimum vs Nash equilibrium\nWe now use the above analytical results to compare the Pareto-optimal strategies with Nash equilibrium\nstrategies, whose deﬁnition we recall:\nDeﬁnition 19 (Nash equilibrium). ηηη =\n\u0000η1, . . . , ηN\u0001\n∈UN is a Nash equilibrium strategy of the\nstochastic game (N-Player), if for any i = 1, . . . , N, XXX0−= xxx, and any (ηηη−i, ξi) ∈UN, the following\ninequality holds,\nJi (xxx;ηηη) ≤Ji \u0000xxx;\n\u0000ηηη−i, ξi\u0001\u0001\n.\nvi(xxx) := Ji (xxx;ηηη) is called the Nash equilibrium value for player i associated with ηηη.\nProposition 20 (Pareto optimum vs Nash equilibrium solutions for N = 2 players). Assume B1-B3\nand K1 = K2 = K.\n(i) The following controls give a Nash equilibrium policy to game (5.1)-(5.2):\n(η1,+\nt\n, η1,−\nt\n) =\n\u0012\n0, max\n\u001a\n0, max\n0≤u≤t\n\b\n(x1 −x2) + (σσσ1 −σσσ2) · BBBu + η2,−\nu\n−c2\n\t\u001b\u0013\n,\n(η2,+\nt\n, η2,−\nt\n) =\n\u0012\n0, max\n\u001a\n0, max\n0≤u≤t\n\b\n−(x1 −x2) + (σσσ2 −σσσ1) · BBBu + η1,−\nu\n−c2\n\t\u001b\u0013\n,\n(5.20)\nwhere c2 > 0 is the unique positive solution to\neσ\n√2ρ tanh\n\u0012√2ρ\neσ x\n\u0013\n= p′\n1(x) −K\np′′\n1(x)\n,\n(5.21)\n26\nwith p1 deﬁned in (5.10). The value functions v1 and v2 corresponding to the Nash equilibrium\n(ηηη1,ηηη2) deﬁned in (5.20) are\nv1(x1, x2) =\n\n\n\n\n\n\n\nv1(x2 −c2, x2),\nx1 ≤x2 −c2,\n−\neσ2p′′\n1 (c2) cosh\n\u0010 √2ρ\neσ (x1−x2)\n\u0011\n2ρ cosh\n\u0010\nc2\n√2ρ\neσ\n\u0011\n+ p1(x1 −x2),\nx2 −c2 ≤x1 ≤x2 + c2,\nK(x1 −x2 −c2) + v1(x2 + c2, x2),\nx1 ≥x2 + c2,\n(5.22)\nand\nv2(x1, x2) =\n\n\n\n\n\n\n\nv2(x1, x1 −c2),\nx2 ≤x1 −c2,\n−\neσ2p′′\n1 (c2) cosh\n\u0010 √2ρ\neσ (x2−x1)\n\u0011\n2ρ cosh\n\u0010\nc2\n√2ρ\neσ\n\u0011\n+ p1(x2 −x1),\nx1 −c2 ≤x2 ≤x1 + c2,\nK(x2 −x1 −c2) + v2(x1, x1 + c2),\nx2 ≥x1 + c2;\n(5.23)\n(ii) c2 > c1, where c1 is the unique positive solution to (5.9) and c2 is the unique positive solution to\n(5.21).\nThat is, a Pareto-optimal policy yields a tighter threshold for spreads, hence reduces volatility of\ninterbank rates compared to the Nash equilibrium (see Figure 1).\nFigure 1: Comparison: Nash and Pareto (K1 = K2).\nProof. Similar to the derivation in (Guo & Xu, 2019), we have the following quasi-variational inequalities\nfor the Nash equilibrium of game (5.1) with J1 and J2 and K1 = K2 = K,\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax\nn\nρvi(x1, x2) −h(x1 −x2) −eσ2\n2\n\u0010\n∂2\nx1vi(x1, x2) + ∂2\nx2vi(x1, x2)\n\u0011\n,\n∂xivi(x1, x2) −K, −∂xivi(x1, x2) −K\no\n= 0,\non\n{(x1, x2) : −K < ∂xjvj(x1, x2) < K} ,\n∂xivi(x1, x2) = 0,\non\n{(x1, x2) : ∂xjvj(x1, x2) = K or ∂xjvj(x1, x2) = −K} ,\nfor i ̸= j and 1 ≤i, j ≤2. Moreover, one can show that (5.22)-(5.23) are the solution to (5.24).\nApplying a veriﬁcation theorem (Guo & Xu, 2019, Theorem 3), some further calculations can verify\nthat (5.22)-(5.23) are the game values associated with the Nash equilibrium policy (5.20).\n27\nNow we provide the proof for Claim (ii). Deﬁne g(x) =\neσ\n√2ρ tanh\n\u0010 √2ρ\neσ x\n\u0011\n, g1(x) =\np′\n1(x)−K\n2\np′′\n1 (x)\nand\ng2(x) =\np′\n1(x)−K\np′′\n1 (x) , where p1 is deﬁned in (5.10).\nThen g(0) = 0, g′(x) > 0\nfor any x ∈R+, and\nlimx→∞g(x) =\neσ\n√2ρ. Thanks to Assumption (B2),\n0 < c\nρ ≤p′′\n1(x) = E\nZ ∞\n0\ne−ρth′′(x + eσBt)dt ≤C\nρ .\nThe function p′\n1(x) is negative at x = 0 and increases monotonically to ∞on R+. Hence there exists an\nunique positive zero c0. Moreover, for any x > c0, g′\n1(x) = 1−p′′′\n1 (x)\np′′\n1 (x) g1(x) ≥1. This is because p′′′\n1 (x) ≤0\nfor x ≥0. We conclude that there exists a unique point c0 < c1 < ∞such that g(c1) = g1(c1).\nNow by similar analysis, c2 is the unique solution to g(x) = g2(x) such that 0 < c2 < ∞. Notice\nthat, g1(x) −g2(x) =\nK\n2p′′\n1 (x) > 0 because p′′\n2(x) > 0. Hence c2 > c1.\nReferences\nAïd, R., Basei, M., & Pham, H. (2017). The coordination of centralised and distributed generation.\narXiv preprint arXiv:1705.01302.\nAvellaneda, M., & Cont, R. (2010). Transparency in over-the-counter interest rate derivatives markets\n(Report). Finance Concepts.\nBator, F. M. (1957). The simple analytics of welfare maximization. The American Economic Review,\n47(1), 22–59.\nBeneš, V. E., Shepp, L. A., & Witsenhausen, H. S. (1980). Some solvable stochastic control problems.\nStochastics: An International Journal of Probability and Stochastic Processes, 4(1), 39–83.\nBensoussan, A., Long, H., Perera, S., & Sethi, S. (2012). Impulse control with random reaction periods:\na central bank intervention problem. Operations Research Letters, 40(6), 425–430.\nBrezis, H. (2010). Functional Analysis, Sobolev Spaces and Partial Diﬀerential Equations. Springer.\nCadenillas, A., & Zapatero, F. (2000). Classical and impulse stochastic control of the exchange rate\nusing interest rates and reserves. Mathematical Finance, 10(2), 141–156.\nCarlen, E., & Protter, P. (1992). On semimartingale decompositions of convex functions of semimartin-\ngales. Illinois journal of mathematics, 36(3), 420–427.\nCarmona, R., Fouque, J.-P., & Sun, L.-H. (2015). Mean ﬁeld games and systemic risk. Communications\nin Mathematical Sciences, 13(4), 911–933.\nChiarolla, M. B., Ferrari, G., & Riedel, F. (2013). Generalized Kuhn–Tucker conditions for N-ﬁrm\nstochastic irreversible investment under limited resources. SIAM Journal on Control and Optimiza-\ntion, 51(5), 3863–3885.\nColeman, J. L. (1979). Eﬃciency, utility, and wealth maximization. Hofstra L. Rev., 8, 509.\nDavis, M. H., & Norman, A. R. (1990). Portfolio selection with transaction costs. Mathematics of\nOperations Research, 15(4), 676–713.\nDavis, M. H., Panas, V. G., & Zariphopoulou, T. (1993). European option pricing with transaction\ncosts. SIAM Journal on Control and Optimization, 31(2), 470–493.\n28\nDe Angelis, T., & Ferrari, G. (2018). Stochastic nonzero-sum games: a new connection between singular\ncontrol and optimal stopping. Advances in Applied Probability, 50(2), 347–372.\nDianetti, J., & Ferrari, G. (2020). Nonzero-sum submodular monotone-follower games: existence and\napproximation of Nash equilibria. SIAM Journal on Control and Optimization, 58(3), 1257–1288.\nDuﬃe, D., & Stein, J. C. (2015). Reforming LIBOR and other ﬁnancial market benchmarks. Journal\nof Economic Perspectives, 29(2), 191–212.\nDupuis, P., & Ishii, H. (1991). On Lipschitz continuity of the solution mapping to the Skorokhod\nproblem, with applications. Stochastics, 35(1), 31–62.\nDupuis, P., & Ishii, H. (1993). SDEs with Oblique Reﬂection on Nonsmooth Domains. Annals of\nProbability, 21(1), 554 – 580. doi: 10.1214\/aop\/1176989415\nEvans, L. C. (1990). Weak convergence methods for nonlinear partial diﬀerential equations. American\nMathematical Society.\nFerrari, G., Riedel, F., & Steg, J.-H. (2017). Continuous-time public good contribution under uncer-\ntainty: a stochastic control approach. Applied Mathematics & Optimization, 75(3), 429–470.\nFischer, M., & Livieri, G. (2016). Continuous time mean-variance portfolio optimization through the\nmean ﬁeld approach. ESAIM: Probability and Statistics, 20, 30–44.\nGilbarg, D., & Trudinger, N. S. (2015). Elliptic Partial Diﬀerential Equations of Second Order. Springer.\nGomes, D. A., Mohr, J., & Souza, R. R. (2010). Discrete time, ﬁnite state space mean ﬁeld games.\nJournal de Mathématiques Pures et Appliquées, 93(3), 308–328.\nGuo, X., & Pham, H. (2005). Optimal partially reversible investment with entry decision and general\nproduction function. Stochastic Processes and their Applications, 115(5), 705–736.\nGuo, X., & Xu, R. (2019). Stochastic games for fuel follower problem: N versus MFG. SIAM Journal\non Control and Optimization, 57(1), 659–692.\nH. M. Treasury.\n(2012).\nThe Wheatley Review of LIBOR: Final Report (Tech. Rep.).\nAuthor.\nRetrieved\nfrom\nhttps:\/\/assets.publishing.service.gov.uk\/government\/uploads\/system\/\nuploads\/attachment_data\/file\/191762\/wheatley_review_libor_finalreport_280912.pdf\nHernandez-Hernandez, D., Simon, R. S., & Zervos, M. (2015). A zero-sum game between a singular\nstochastic controller and a discretionary stopper. Annals of Applied Probability, 25(1), 46–80.\nHuang, M., Malhamé, R. P., & Caines, P. E. (2006). Large population stochastic dynamic games:\nclosed-loop McKean-Vlasov systems and the Nash certainty equivalence principle. Communications\nin Information & Systems, 6(3), 221–252.\nJeanblanc-Picqué, M. (1993). Impulse control method and exchange rate. Mathematical Finance, 3(2),\n161–177.\nKallsen, J., & Muhle-Karbe, J. (2017). The general structure of optimal investment and consumption\nwith small transaction costs. Mathematical Finance, 27(3), 659–703.\nKaratzas, I. (1983). A class of singular stochastic control problems. Advances in Applied Probability,\n15(2), 225 – 254.\n29\nKruk, L. (2000). Optimal policies for N-dimensional singular stochastic control problems part I: the\nSkorokhod problem. SIAM Journal on Control and Optimization, 38(5), 1603–1622.\nKwon, H., & Zhang, H. (2015). Game of singular stochastic control and strategic exit. Mathematics of\nOperations Research, 40(4), 869–887.\nLasry, J.-M., & Lions, P.-L. (2007). Mean ﬁeld games. Japanese Journal of Mathematics, 2(1), 229–260.\nMenaldi, J.-L., & Robin, M. (1983). On some cheap control problems for diﬀusion processes. Transac-\ntions of the American Mathematical Society, 278(2), 771–802.\nMenaldi, J.-L., & Taksar, M. I. (1989). Optimal correction problem of a multidimensional stochastic\nsystem. Automatica, 25(2), 223–232.\nMeyer, P. A. (1976). Martingales locales changement de variables, formules exponentielles. In Séminaire\nde Probabilités X Université de Strasbourg (pp. 291–331). Springer.\nRamanan, K. (2006). Reﬂected diﬀusions deﬁned via the extended Skorokhod map. Electronic journal\nof probability, 11, 934–992.\nRosen, J.\n(1987).\nJoint continuity of the intersection local times of Markov processes.\nAnnals of\nProbability, 15, 659–675.\nSoner, H. M., & Shreve, S. E. (1989). Regularity of the value function for a two-dimensional singular\nstochastic control problem. SIAM Journal on Control and Optimization, 27(4), 876–907.\nSun, L.-H. (2018). Systemic risk and interbank lending. Journal of Optimization Theory and Applica-\ntions, 179(2), 400–424.\nWang, W.-K., & Ewald, C.-O. (2010). Dynamic voluntary provision of public goods with uncertainty:\na stochastic diﬀerential game model. Decisions in Economics and Finance, 33(2), 97–116.\nWidder, D. V. (1941). The Laplace Transform. Princeton university press.\nWilliams, S., Chow, P., & Menaldi, J. (1994). Regularity of the free boundary in singular stochastic\ncontrol. Journal of Diﬀerential Equations, 111(1), 175 - 201.\nZariphopoulou, T. (1992). Investment-consumption models with transaction fees and Markov-chain\nparameters. SIAM Journal on Control and Optimization, 30(3), 613–636.\nA\nVeriﬁcation theorem\nTheorem 21. Let u ∈W 2,∞\nloc (RN) be a convex solution to the HJB equation (3.4) and 0 ≤∂2\nνννu(xxx) ≤C\n(in the weak sense). Under Assumptions A1-A3, u is equal to the value function v of (Regulator):\nv(xxx) = min\nξξξ∈U J(xxx;ξξξ) = u(xxx).\nIn addition, if there exists ξξξ∗∈U such that\n• XXX∗\nt = xxx + σσσBBBt + ξξξ∗\nt ∈CN for every t ≥0, P-a.s.;\n• ξξξ∗\nt = ξξξ∗\n0 +\nR t\n0 NNN sdη∗\ns with ξξξ∗\n0 = π(xxx) −xxx and η∗\nt =\nR t\n0 1{X\nX\nX∗s∈∂CN,N\nN\nNs∈r(X\nX\nX∗s)}dη∗\ns for every t ≥0, P-a.s.;\n30\n• ξξξ∗is continuous if ξξξ∗\n0 = 0;\nwhere CN = {xxx | β(∇u(xxx)) < 1}, and γ and rrr are deﬁned in (4.5)-(4.6) such that Assumption A5 holds,\nthen ξξξ∗is an optimal control.\nProof. By the Sobolev embedding (Brezis, 2010, Ch. 9, Cor. 9.15), u ∈C1(RN) since u ∈W2,∞\nloc (RN).\nIn addition, u is convex and 0 ≤∂2\nνννu(xxx) ≤C, then apply the Itô-Tanaka-Meyer formula (Carlen &\nProtter, 1992) to the function e−ρtu(XXXt) of the semi-martingale XXXt = xxx + µµµt + σσσBBBt + ξξξt,\ne−αTu(XXXt) −u(xxx)\n=\nZ T\n0\ne−αt∇u(XXXt)dBBBt +\nZ T\n0\ne−αt(Lu(XXXt) −αu(XXXt))dt\n+\nZ T\n0\ne−αT(\nN\nX\ni=1\n∂xiu(XXXt)dξi,+\nt\n−\nN\nX\ni=1\n∂xiu(XXXt)dξi,−\nt\n)\n+\nX\n0≤t≤T\ne−αT\n \n∆u(XXXt) −\nN\nX\ni=1\n∂xiu(XXXt)∆Xi\nt\n!\n,\n(A.1)\nwith the notation ∆φt := φt −φt−. Since u is a convex solution to the HJB equation (3.4), we have\nP-a.s. for all 0 ≤t ≤T,\nρu(XXXt) −Lu(XXXt) −H((XXXt)) ≤0,\n(A.2)\n∂xiu(XXXt)dξi,−\nt\n≤LiK−\ni dξi,−\nt\n, −LiK+\ni dξi,+\nt\n≤∂xiu(XXXt)dξi,+\nt\n,\n(A.3)\n∆u(XXXt) −\nN\nX\ni=1\n∂xiu(XXXt)∆Xi\nt ≥0.\n(A.4)\nTaking expectation on both sides of (A.1), we have for any admissible policy ξξξ,\ne−αTE[u(XXXt)] + E\nZ T\n0\ne−ρt\u0010\nH(XXXt)dt + K+\ni dξi,+\nt\n+ K−\ni dξi,−\nt\n\u0011\n≥u(xxx).\n(A.5)\nSince 0 ≤∂2\nνννu(xxx) ≤C, there exists constant K = K(C) > 0 such that |u(xxx)| ≤K(1 + ∥xxx∥2). Hence\nE[u(XXXt)] ≤9K (1 + ∥xxx∥2 + ∥σσσ∥2∥BBBT∥2 + ∥ξξξT∥2).\nNow we show that E[∥ξξξT∥2] = o(eρt). If this does not hold, then standard arguments (e.g. (Widder,\n1941, P 39)) can show that there exists i ∈{1, 2, · · · , N} such that E[\nR ∞\n0 e−ρt(dξi,+\nt\n+dξi,−\nt\n)] = ∞, which\nviolates the condition in the deﬁnition of admissible control set UN. Hence by letting T →∞we have\nE\nZ ∞\n0\ne−ρt\u0010\nH(XXXt)dt + K+\ni dξi,+\nt\n+ K−\ni dξi,−\nt\n\u0011\n≥u(xxx).\n(A.6)\nUnder Assumption A1-A3, Theorem 5 holds and hence u(xxx) = v(xxx) for all xxx ∈RN.\nTo achieve the equality in (A.6), it suﬃces to achieve the equalities in conditions (A.2)-(A.4), which\nrequires the following properties from the optimal control process ξξξ∗:\n• XXX∗\nt = xxx + σσσBBBt + ξξξ∗\nt ∈CN hence ρu(XXX∗\nt) −Lu(XXX∗\nt) −H(XXX∗\nt) = 0 for every t ≥0, P-a.s.;\n• The only possible jump is at time 0 when xxx \/∈CN. Under Assumption A5 and the convexity of u,\nwe can show that u(xxx) = u(π(xxx)) + l(xxx −π(xxx)) with l(yyy) = P\ni li(yi), where\nli(yi) =\n\u001a LiK−\ni yi,\nif yi ≥0,\n−LiK+\ni yi,\nif yi < 0.\n(A.7)\nThe proof is the same as the one for Theorem 11. And hence the equality in (A.4) holds.\n• By the deﬁnition of ξξξ∗, dξξξ∗\nt ̸= 0 only when XXX∗\nt−\/∈CN. Hence the equality in (A.3) holds.\n31\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Interbank lending with benchmark rates: Pareto optima for a class of singular control games.pdf"}
{"title":"Benchmarking End-to-End Behavioural Cloning on Video Games","authors":"Anssi Kanervisto, Joonas Pussinen, Ville Hautamäki","summary":"Behavioural cloning, where a computer is taught to perform a task based on\ndemonstrations, has been successfully applied to various video games and\nrobotics tasks, with and without reinforcement learning. This also includes\nend-to-end approaches, where a computer plays a video game like humans do: by\nlooking at the image displayed on the screen, and sending keystrokes to the\ngame. As a general approach to playing video games, this has many inviting\nproperties: no need for specialized modifications to the game, no lengthy\ntraining sessions and the ability to re-use the same tools across different\ngames. However, related work includes game-specific engineering to achieve the\nresults. We take a step towards a general approach and study the general\napplicability of behavioural cloning on twelve video games, including six\nmodern video games (published after 2010), by using human demonstrations as\ntraining data. Our results show that these agents cannot match humans in raw\nperformance but do learn basic dynamics and rules. We also demonstrate how the\nquality of the data matters, and how recording data from humans is subject to a\nstate-action mismatch, due to human reflexes.","url":"http:\/\/arxiv.org\/abs\/2004.00981v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2004.00981v2","published":1585834311000,"comment":"To appear in IEEE Conference on Games 2020. Experiment code available\n  at https:\/\/github.com\/joonaspu\/video-game-behavioural-cloning and\n  https:\/\/github.com\/joonaspu\/ViControl","pdf_text":"Benchmarking End-to-End Behavioural Cloning on\nVideo Games\nAnssi Kanervisto*\nSchool of Computing\nUniversity of Eastern Finland\nJoensuu, Finland\nanssk@uef.ﬁ\nJoonas Pussinen*\nSchool of Computing\nUniversity of Eastern Finland\nJoensuu, Finland\njoopu@student.uef.ﬁ\nVille Hautam¨aki\nSchool of Computing\nUniversity of Eastern Finland\nJoensuu, Finland\nvilleh@uef.ﬁ\nAbstract—Behavioural cloning, where a computer is taught to\nperform a task based on demonstrations, has been successfully\napplied to various video games and robotics tasks, with and\nwithout reinforcement learning. This also includes end-to-end\napproaches, where a computer plays a video game like humans\ndo: by looking at the image displayed on the screen, and sending\nkeystrokes to the game. As a general approach to playing video\ngames, this has many inviting properties: no need for specialized\nmodiﬁcations to the game, no lengthy training sessions and the\nability to re-use the same tools across different games. However,\nrelated work includes game-speciﬁc engineering to achieve the\nresults. We take a step towards a general approach and study\nthe general applicability of behavioural cloning on twelve video\ngames, including six modern video games (published after 2010),\nby using human demonstrations as training data. Our results\nshow that these agents cannot match humans in raw performance\nbut do learn basic dynamics and rules. We also demonstrate how\nthe quality of the data matters, and how recording data from\nhumans is subject to a state-action mismatch, due to human\nreﬂexes.\nIndex Terms—video game, behavioral cloning, imitation learn-\ning, reinforcement learning, learning environment, neural net-\nworks\nI. INTRODUCTION\nReinforcement learning (RL) [1] has been successfully\napplied to create super-human players in multiple video games,\nincluding classic Atari 2600 games [2], as well as more mod-\nern shooters [3], MOBAs [4], [5] and real-time strategy games\n[6]. Even more so, all before-mentioned accomplishments\nuse “end-to-end” systems, where input features are not pre-\nprocessed by crafting speciﬁc features, and instead rely on raw\ninformation like image pixels. However, RL is not without\nits limitations: they require an environment where to play\nthe game. Whether this is achieved by modifying an existing\ngame (like Starcraft II [7]) or by using their engines to create\nenvironments from ground-up (like Unity ML-Agents [8]), it\n*Equal contribution, alphabetical ordering. This research was partially\nfunded by the Academy of Finland (grant #313970). We gratefully acknowl-\nedge the support of NVIDIA Corporation with the donation of the Titan Xp\nGPU used for this research. We thank the reviewers for extensive comments\nused to improve the ﬁnal version of this paper.\n©2020 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting\/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nstill requires considerable engineering. Even worse, after the\nenvironment is created, training the agents may take thousands\nof years of in-game time [4], [6].\nAn alternative approach is imitation learning, in which\nagents learn to replicate demonstrators’ actions. Behavioural\ncloning (BC) [9] is the simplest form of this: given an\nobservation and an associated action from a demonstrator,\npredict this action based on observation (i.e a classiﬁcation\ntask). This has been used to kick-start RL agents [6], [10], but\nalso applied alone in e.g. autonomous driving [9], [11], [12],\nand Vinyals et al. [6] show that Starcraft II can be played at\nproﬁcient human-level with behavioural cloning alone. This\nbegs the question: How well can behavioural cloning play\nvideo games, in general? Can we reach the level of a human\nplayer? How much data do we need? Do we need data from\nmultiple players?\nIf we can create performant, end-to-end agents with BC\nand human gameplay alone, it would skip many hurdles\nexperienced with RL: we do not need to create an environment\nfor agents to play in, nor do we need to spend large amounts\nof compute resources for training. We only need the video\ngame, a tool to record the gameplay, and players for the game.\nIf BC can manage with just an hour or two of gameplay\ndemonstration, a single person could record the demonstration\ndata. If the recording tool captures the same output and input\na human player would have (i.e. image of the screen and\nkeyboard\/mouse, end-to-end), this would require no game-\nspeciﬁc coding and could be applied to any game. Even if\nBC does not reach human-level performance, it could still be\nused as a starting point for other learning methods, or as a\nsupport for diversifying the agent’s behaviour [6].\nVideo games have been in active use as benchmarks in\nresearch using BC [10], [13]–[15], and as milestones to beat\nin AI research [2], [5], [6]. The other way around, “BC for\nvideo games”, has seen works like human-like bots in ﬁrst-\nperson shooter (FPS) games using hand-crafted features and\nimitation learning [16]–[18], end-to-end FPS bots with RL and\nBC [19]. Our setting and motivation resemble the motivation\nof [20], where authors employ end-to-end imitation learning\nto play two Nintendo 64 games successfully. However, these\nstudies have been limited to only a few games a time, making it\nhard to tell how well BC performs in general at playing video\narXiv:2004.00981v2  [cs.AI]  18 May 2020\nFig. 1.\nGames tested with behavioural cloning. Images represent what the BC agent would see. From left to right: Ms. Pac-Man, Video Pinball, Q*bert,\nMontezuma’s Revenge, Space Invaders, Deathmatch (Doom), HGS (Doom), Downwell, Crypt of The NecroDancer, Super Hexagon, Boson X, Binding of\nIsaac: Rebirth and BeamNG.drive.\ngames. Apart from [15], related work does not study how data\nshould be chosen for behavioural cloning. In addition, Zhang\net al. [14] bring up an important point on how human delay\ncan adversarially affect the quality of the dataset but did not\ninclude experimental results on this.\nIn this work, we aim to answer these three questions and\nto assess the general applicability of end-to-end behavioural\ncloning for video game playing. We use data from human\ndemonstrators to train a deep network to predict their actions,\ngiven the same observations human players saw (the screen\nimage). We run empirical experiments to study how well\nBC agents play Atari 2600 games, Doom (1993) and various\nmodern video games. Along with the raw performance, we\nstudy the effect of quality and quantity of the training data, and\nthe effect of delay of human reﬂexes on the data quality. Along\nthe results, we present ViControl (“Visual Control”), a multi-\nplatform tool to record and play an arbitrary game, which we\nuse to do behavioural cloning on the modern games. ViControl\nis available at https:\/\/github.com\/joonaspu\/ViControl.\nII. END-TO-END BEHAVIOURAL CLONING FOR VIDEO\nGAMES\nA. Behavioural cloning\nWe wish to train computer to play a game, based on given\ndemonstrations of humans playing it. We model the environ-\nment as a truncated version of Markov Decision Processes\n(MDPs) [1], where playing the game consists of observations\ns ∈S and associated actions a ∈A. We do not include a\nnotion of time, reward signal nor terminal\/initial states. The\ntask of behavioural cloning is simple: given a dataset of human\ngameplay D containing tuples (s, a), learn the conditional\ndistribution p(a|s), i.e. probability of human players picking\naction a in state s. After learning this distribution, we can\nuse it to play the game by sampling an action a ∼p(a|s) for\na given state s (agent). An immediate limitation here is the\nlack of temporal modelling, or “memory”, which could limit\nthe agent’s abilities. It has been shown that including past\ninformation with behavioural cloning can be detrimental to\nperformance [21], but on the other hand there exists work that\nsuccessfully do BC with recurrent neural networks [22]. We\nopt not to use recurrent networks for the model and training\nsimplicity, and as most of the games used in this work do not\nrequire memory to master.\nWe take an end-to-end approach, where states are pixels of\nan RGB image s ∈RH×W ×3, H, W ∈N, and actions a are\na vector of one or more discrete variables ai ∈0, 1, . . . di,\nwhere i ∈N represents the number of discrete variables, and\ndi tells the number of options per discrete variable. In Atari\nenvironments [23], action contains one discrete variable with\n18 options, including all possible choices human player could\nmake (multi-class classiﬁcation task). With a PC game using a\nkeyboard with, say, four buttons available, the actions consist\nof four discrete variables, all with two options: down or up\n(multi-label classiﬁcation task).\nTo model the conditional distribution p(a|s), we use deep\nneural networks. They support the different actions we could\nhave and are known to excel in image classiﬁcation tasks [24].\nWe treat action discrete variables i independent from each\nother, and the network is trained to minimize cross-entropy\nbetween predictions and labels in the dataset.\nB. Challenges of general end-to-end control of video games\nCompared to Atari 2600 games and Doom (1993), modern\nvideo games (published after 2010) can take advantage of more\ncomputing power and tend to be more complex when it comes\nto visual aesthetics and dynamics. We also do not assume to\nhave control over game program’s ﬂow, so the game will run\nat a ﬁxed rate, as humans would experience it. All-together,\nthese raise some speciﬁc challenges for generalized end-to-end\ncontrol, where we wish to avoid per-game engineering.\na) High resolution: Modern games commonly run at\n“high deﬁnition” resolutions, with most common monitor\nresolution for players being 1920×1080. However, RL and BC\nagents resize images to small resolutions due to computational\nefﬁciency, usually capped around 200 to 300 pixels per axis\n[3], [25], and commonly lower [2], [10]. If we take a modern\ngame with a resolution of at least 1280×720, and downscale it\nto these resolutions, we lose a great deal of detail: any smaller\nuser-interface (UI) elements, like text, may get blurred out, and\nalready-small objects on the screen may disappear completely.\nOn top of this, different interpolation methods used for resizing\nimages have been reported to affect the training results [20],\n[26]. We leave approaches for solving this to future work, and\nsimply resize the images.\nb) Complex action space: The natural action space of\na computer game, a keyboard and a mouse, contains over a\nhundred keys to press in total, as well as the movement of the\nmouse. Such large action spaces have shown to be an issue to\nRL agents [27], [28], and many of these buttons do nothing\nin games (when was the last time you have used Insert\nin a video game?). Even when we modify the action space to\nonly include buttons that are used by the game, we can end up\nwith a large, parametrized action space with its own difﬁculties\n[29], like in Starcraft II [7]. We pre-deﬁne a minimal set of\nactions required to play games in this work.\nc) Asynchronous execution: As the game environment\nruns asynchronously from the agent’s decisions, the agent must\nexecute an action quickly after observing an image, otherwise\nits decisions will lag behind. This “control delay” is known\nto reduce performance of RL methods [30], [31]. In addition,\nif we gather BC data from human players, the recorded ac-\ntions are subject to delays from human-reﬂexes. If something\nsurprising happens on the screen, average humans react to\nthis with a split-second delay. This action was supposed to\nbe associated with the surprising event, but instead it will be\nrecorded few frames later, associated with possibly a wrong\nobservation. Other way around, human players could plan\ntheir action before the observation and execute it pre-maturely.\nThese both lead to state-action mismatch [14], effect of which\nwe study in the experiments.\nd) Confounding information:\nBehavioural cloning is\nprone to causal confusion [21] or “superstition” [32], where\nproviding more information may be detrimental to BC\nagent’s performance. With more information (e.g. history,\npast frames\/actions), the model has a larger chance to ﬁnd\nmisleading correlations between observations and actions. For\nexample, ﬁring a plasma-weapon in Doom. This creates a\nblue, long-lasting muzzle-ﬂash on the weapon. Since many\nframes with ATTACK pressed down include this blue ﬂash,\nthe model learns to focus on this ﬂash to predict if we should\nﬁre the weapon. However, the ﬂash is not the cause of ﬁring\nthe weapon, it is the effect. Similarly, games have numerous\nUI elements with various information, which could lead to\nsimilar confusion. In this work we do not provide historical\ninformation to the agent, limiting its capabilities in exchange\nfor less chance of destructive causal confusion.\nIII. RESEARCH QUESTIONS AND EXPERIMENTAL SETUP\nAlong with the main evaluation of BC in different games,\nwe study two important aspects of the training setup, brought\nup by related work: “how does the quantity and quality of\nthe data affect the results?” [15], and “how the state-action\nmismatch from human reﬂexes affects the results?” [14]. The\nformer sheds light on if we should gather data from only few\nexperts, or should we use data from many different players.\nA similar comparison of different sized datasets was done in\n[15]. The latter was brought up by authors of [14], but without\nexperimental results.\n0\n20000\nScore\nDensity\nMs. Pac-Man\nAtari-HEAD\nAtari GC\n0\n20000 40000\nScore\nVideo Pinball\n0\n20000\nScore\nQ*bert\n0\n20000\n40000\nScore\nMontezuma's\nRevenge\n0\n2000\n4000\nScore\nSpace\nInvaders\nFig. 2.\nComparison of the score distribution in the Atari Grand Challenge\n(Atari-GC) dataset and in the Atari-HEAD dataset. Atari-HEAD does not have\ndata for Video Pinball or Q*bert\nTo study the state-action mismatch, we run experiments with\nmodiﬁed versions of the Atari and ViZDoom datasets, where\nan action delay d is added between the state and action. In\nthe modiﬁed datasets, the state si at frame i is matched with\nan action ai+d. Both positive and negative values are used for\nthe action delay d.\nWe will use Atari [23] and Doom [33] environments to\nanswer these questions, as they can be used synchronously\nand therefore allow fast evaluation of trained models. We will\nthen include six modern video games to assess how well BC\nworks under the challenges presented in Section II-B. Images\nof all of the games are shown in Figure 1. Code used to run\nthese experiments is available at https:\/\/github.com\/joonaspu\/\nvideo-game-behavioural-cloning.\nA. Evaluation\nFor the Atari and Doom experiments, each training run\nis evaluated by taking models from the three last epochs of\ntraining, evaluating their performance and averaging over. The\ntraining is then repeated three times with random seeds, and\nthe result shown is an average over these three runs. We do\nthis to capture the variance between different training steps,\nillustrated in Figure 3. The same is done when evaluating with\nthe modern games, except we only evaluate the ﬁnal model\ninstead of the last three epochs.\nThe evaluation results are reported as percentage of human\nscore [2], where 0% is a baseline score set by an agent that\npicks a random action on each frame, and 100% is the mean\nscore of the human players in the dataset used for training. In\nAtari experiments, we use the mean score of the episodes with\na score above the 95th percentile in the Atari Grand Challenge\ndataset [15] for average human score.\nB. Behavioural cloning model\nThe neural network model is based on the convolutional\nneural network used in the original deep Q-learning work\n[2] and in related BC experiments [14], consisting of three\nconvolutional layers, followed by a single fully connected\nlayer of 512 units and a layer that maps these to probabilities\nfor each action. All layers use ReLU (rectiﬁed linear unit)\nactivations. While small by modern standards, this architecture\nis the de facto architecture used in RL experiments [10], [34].\nResidual networks [35] have also been used for improved\nperformance [26], [36], but are slower to run. We opt for the\n1\n2\n3\n4\n5\n6\n7\n8\n9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLoss\nTraining loss\n1\n2\n3\n4\n5\n6\n7\n8\n9 10\nEpoch\n0\n200\n400\n600\n800\n1000\n1200\nScore\nEvaluation score\nFig. 3.\nAn example of the training loss (left) and evaluation score curves\n(right). Black lines are scores of individual evaluation episodes. Plots are\nfrom training on Space Invaders on the Atari-HEAD dataset and are similar\nacross different games.\nfaster, simpler network to keep up with the fast pace of actions\nrequired for experiments with asynchronous games, described\nin Section III-E. All code is implemented in PyTorch.\nIn all of the experiments, the network is trained using\nthe Adam optimizer [37] to minimize cross-entropy, with a\nlearning rate of 0.001 and L2-normalization weight 10−5. In\nall experiments, we train until training loss does not improve.\nWe did not ﬁnd shorter or longer training to be helpful, and\nagent’s performance did not increase signiﬁcantly after ﬁrst\n10−50% of the training regimen (see Figure 3). Interestingly,\ntraining loss continues to reduce while evaluation performance\ndoes not change. This is expected to a degree, as the training\nloss (per-sample prediction error) does not reﬂect the agent’s\nperformance [38]. During evaluation, we sample the ﬁnal\nactions according to the probabilities provided by the network.\nWe found this to work better than deterministically selecting\nthe action with the highest probability.\nC. Atari games\nFor the Atari experiments, we used two existing datasets:\nthe Atari Grand Challenge dataset (Atari GC) [15] and the\nAtari-HEAD dataset [14]. The Atari Grand Challenge dataset\nincludes ﬁve games, which were all used for our experiments.\nThe Atari-HEAD dataset includes a set of 20 Atari games, out\nof which we used the three games that are also in the Atari\nGrand Challenge dataset. Atari-HEAD includes episodes with\nhigher score. A comparison of the distribution of ﬁnal scores\nin these two datasets can be seen in Figure 2.\nTo study effect of the amount and quality of the data on\nbehavioural cloning, we include experiments similar to ones in\n[15], where we repeat behavioural cloning only using episodes\nwith scores above the 95th percentile and 50th percentile (“top\n5%” and “top 50%”). It should be noted that we use only BC,\nwhile [15] used deep Q-learning from demonstrations [10].\nThe amount of data for all these setups are shown in Table I.\nIn both datasets, the input frames are 160×210 RGB images\nthat are resized to 84 × 84 when training the models. To\neliminate ﬂickering of certain in-game elements, each frame\nis merged with its preceding frame by setting each pixel to\nhave the lighter value from these two frames (maximum). The\nframe rate in both datasets is 60 frames per second. Models\nTABLE I\nSTATISTICS FOR THE ATARI GRAND CHALLENGE AND ATARI-HEAD\nDATASETS\nEnvironment and dataset\nEpisodes\nTotal samples\nMs. Pac-Man\nAtari Grand Challenge, All data\n667\n2829068\nAtari Grand Challenge, Top 50%\n335\n2066077\nAtari Grand Challenge, Top 5%\n34\n362056\nAtari-HEAD\n20\n353428\nVideo Pinball\nAtari Grand Challenge, All data\n380\n2352787\nAtari Grand Challenge, Top 50%\n190\n1688256\nAtari Grand Challenge, Top 5%\n19\n224150\nQ*bert\nAtari Grand Challenge, All data\n1136\n3329088\nAtari Grand Challenge, Top 50%\n576\n2419198\nAtari Grand Challenge, Top 5%\n57\n614193\nMontezuma’s Revenge\nAtari Grand Challenge, All data\n1196\n4623879\nAtari Grand Challenge, Top 50%\n931\n3991548\nAtari Grand Challenge, Top 5%\n92\n646985\nAtari-HEAD\n20\n335276\nSpace Invaders\nAtari Grand Challenge, All data\n905\n4005345\nAtari Grand Challenge, Top 50%\n483\n2765214\nAtari Grand Challenge, Top 5%\n46\n422372\nAtari-HEAD\n20\n332483\nwere trained for 10 epochs, except for the full Atari GC dataset\nand its top 50% subset, which were trained for 5 epochs.\nThe models are evaluated with the OpenAI Gym Atari\nenvironments with 100 episodes, with default environments\n(“v4” versions). Evaluation runs until the game ends or the\n40000th frame is reached.\nD. Doom\nFor the Doom experiments, we use two scenarios provided\nby the ViZDoom [33]: Health-Gathering-Supreme (HGS) and\nDeathmatch. In both scenarios the input observation is an RGB\nimage of size 80 × 60, and the network predicts which of\nthe allowed buttons are pressed down. Human gameplay is\nrecorded every other ViZDoom tick (17.5 frames per second),\nand the trained model takes actions at the same rate. We\ncollect data from three players, and train models for 30 epochs.\nEvaluation is done the same way as with the Atari experiments,\nexcept with 200 games per epoch.\nIn the HGS scenario, the player constantly takes damage,\nand must navigate around a small maze to collect med kits\nto survive longer. The longer the player survives, the higher\nthe score. Allowed buttons are TURN_LEFT, TURN_RIGHT\nand MOVE_FORWARD. The game ends when the player dies\nor a timeout (one minute) is reached. We record 20 full games\nper person, totaling around one hour of gameplay and 62615\nsamples.\nIn the Deathmatch scenario, the player is pitted against\na room of randomly spawning enemies, with a generous\nnumber of pickups and weapons on the sides of the levels.\nAllowed buttons are ATTACK, SPEED (hold down for faster\nmovement), TURN_LEFT, TURN_RIGHT, MOVE_FORWARD\nand MOVE_BACKWARD. The game ends when the player dies,\nor a timeout of two minutes is reached. We record 10 games\nTop 5%\nTop 50%\nAll\nAtari-HEAD\nMs. Pac-Man\nTop 5%\nTop 50%\nAll\nVideo Pinball\nTop 5%\nTop 50%\nAll\nQ*bert\nTop 5%\nTop 50%\nAll\nAtari-HEAD\nMontezuma's Revenge\n10\n5\n0\n5\n10\n15\n20\n25\n30\n35\n% of human score\nTop 5%\nTop 50%\nAll\nAtari-HEAD\nSpace Invaders\nFig. 4. Human-normalized scores of behavioural cloning on the three different\nsubsets of Atari Grand Challenge dataset and for the Atari-HEAD dataset.\nper person, with total of 46243 samples, corresponding to 45\nminutes of gameplay.\nE. Modern video games\nAs for the experiments with modern video games (released\nafter 2010), we selected games that are familiar to players\nwho provide the data, and which do not require a mouse\nto play. The selected games are described in Appendix A,\nwith example images in Figure 1. We use a speciﬁcally built\ntool, ViControl, to capture the screen image, the corresponding\nkeyboard\/mouse input, and to later emulate these buttons to\nallow the agent to play the game. During recording, ViControl\nbehaves like any game recording\/streaming software, except\nit also tracks keypresses. We collect data from two players,\nwith 30 minutes of gameplay from both, totaling ≈72000\nframes of demonstration per game. Models were trained for\n30 epochs. The only pre-processing we apply is resizing the\nimage. Evaluation is done by letting the trained model play\nthe game until the game ends, the episode lasts too long, or\nwhen some other game-speciﬁc criteria is met. The ﬁnal score\nis an average over ten such games.\n0\n5\n10\n15\n20\n25\n30\n% of human score\nHealth Gathering\n(ViZDoom)\nDeathmatch\n(ViZDoom)\nDownwell\nCrypt of the\nNecroDancer\nSuper\nHexagon\nBoson X\nBinding of Isaac:\nRebirth\nBeamNG.drive\nFig. 5. Human-normalized scores of behavioural cloning on the two ViZDoom\nscenarios and the six modern video games.\nIV. RESULTS AND DISCUSSION\nA. General behavioural cloning performance\nFigure 4 shows the results for the model trained with\nboth Atari datasets. Ms. Pac-Man results show a fairly poor\nperformance of under 5% of human score. Video Pinball fails\nto achieve the baseline score set by a random agent. Q*bert,\nMontezuma’s Revenge and Space Invaders, however, reach a\nscore of over 20% of human score.\nThe results in Figure 5 show the performance of the two\nViZDoom scenarios as well as the modern video games. Out of\nthese, ViZDoom health gathering is the only one to achieve\na human normalized score of more than 30%, while others\nremain under 15%. Out of the modern video games, Binding\nof Isaac: Rebirth and BeamNG.drive are the only games that\nget a score signiﬁcantly above the baseline set by a random\nagent.\nDespite the low scores in most tested games, watching\nthe agents’ gameplay shows that the models still learn some\nof the basic dynamics of the games. See video available\nat https:\/\/youtu.be\/2SMLpnUEIPw. In Super Hexagon, the\nagent moves in the correct direction, but often overshoots or\nundershoots the correct position. In Binding of Isaac: Rebirth,\nthe agent moves through doors and shoots towards enemies\nand in BeamNG.drive, the agent accelerates and steers in the\ncorrect direction, but still hits the walls and damages the car\noften. In Boson X, agent learns to jump at the right moments,\nbut often jumps too short to reach the other platforms. In Crypt\nof the NecroDancer, the agent learns to hit nearby enemies and\nmove in the tunnels, but often throws away their weapon or\nkills themselves with a bomb.\nComparing our results with earlier BC experiments done\nby Hester et al. [10] and Zhang et al. [14] (Table II) we\nreached higher scores in all tested Atari games except for Ms.\nPac-Man, by adjusting for human action-delay and only using\nhigher quality data. The results in Kurin et al. [15] are not\ndirectly comparable, since they did not use a pure BC method.\nB. Data quality versus quantity\nLooking more closely at the Atari results in Figure 4 we can\nsee that Q*bert and Space Invaders beneﬁt signiﬁcantly from\nhaving smaller but higher quality training datasets. Q*bert\nTABLE II\nRESULTS WITH BEHAVIOURAL CLONING. OUR SCORE IS THE HIGHEST AVERAGE SCORE OVER DIFFERENT DATASET SIZES AND ACTION-DELAYS USED.\nVARIANCES ARE NOT INCLUDED AS THEY DIFFER FROM WORK TO WORK (WE REPORT VARIANCE OVER MULTIPLE TRAINING RUNS, ZHANG ET AL. 2019\nREPORTS VARIANCE OVER MULTIPLE EPISODES).\nGame\nRandom agent\nHuman Average\nBehavioural cloning (our)\nHester et al. 2018\nZhang et al. 2019\nMs. Pac-Man\n173.3\n12902.5\n811.7 (GC, All, +2 delay)\n692.4\n1167.5\nVideo Pinball\n22622.4\n34880.1\n21715.0 (GC, top 5%)\n10655.5\nN\/A\nQ*bert\n162.9\n23464.0\n9691.6 (GC, top 5%, +5 delay)\n5133.8\nN\/A\nMontezuma’s Revenge\n0.2\n4740.2\n1812.1 (GC, top 5%, +5 delay)\n576.3\n970.2\nSpace Invaders\n158.8\n1775.9\n564.9 (HEAD, +2 delay)\nN\/A\n247.1\nHealth Gathering (ViZDoom)\n3.1\n20.9\n9.4 (+2 delay)\nDeathmatch (ViZDoom)\n2.5\n93.1\n13.1 (+2 delay)\nDownwell\n92\n1054.8\n81.2\nCrypt of the NecroDancer\n0\n440.4\n4.0\nSuper Hexagon\n3.3\n112.5\n4.6\nBoson X\n0\n170.7\n2.4\nBinding of Isaac: Rebirth\n287.6\n2045.8\n463.4\nBeamNG.drive\n27.8\n3525\n477.1\n0\n1\n2\n3\n4\n% of human score\nMs. Pac-Man\n(Atari GC)\n200\n150\n100\n50\n0\nVideo Pinball\n(Atari GC)\n0\n10\n20\n30\n40\nQ*bert\n(Atari GC)\n0\n10\n20\n30\n40\nMontezuma's Revenge\n(Atari GC)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nSpace Invaders\n(Atari GC)\n-100\n-10\n-5\n-2\n0\n2\n5\n10\n100\n0\n1\n2\n3\n% of human score\nMs. Pac-Man\n(Atari-HEAD)\n-100\n-10\n-5\n-2\n0\n2\n5\n10\n100\n0\n10\n20\n30\nMontezuma's Revenge\n(Atari-HEAD)\n-100\n-10\n-5\n-2\n0\n2\n5\n10\n100\n0\n10\n20\nSpace Invaders\n(Atari-HEAD)\n-100\n-10\n-5\n-2\n0\n2\n5\n10\n100\n0\n10\n20\n30\n40\nHealth Gathering\n(ViZDoom)\n-100\n-10\n-5\n-2\n0\n2\n5\n10\n100\n0\n5\n10\nDeathmatch\n(ViZDoom)\nFig. 6.\nResults with action delay of Atari Grand Challenge (top 5%), Atari-HEAD and ViZDoom datasets. X-axis represents the action-delay used while\ntraining the model, with positive meaning the action lags behind. E.g. delay of ﬁve means we move all actions ﬁve steps back in time, and associate with\ncorresponding observation.\nscore increases from just barely above the random agent’s\nperformance to over 20% of human score when using the top\n5% of episodes. Space Invaders gets a similar increase when\nmoving from the Atari Grand Challenge dataset to the Atari-\nHEAD dataset. Differences in Ms. Pac-Man are not signiﬁcant,\ngiven the small change and relatively large variance.\nTo further study the effect that the quantity of data has on\nthe results, we ran experiments with datasets that only con-\ntained the top 1, 2 and 3 episodes of the Atari Grand Challenge\ndataset. In many games the results were still comparable to\nresults shown here, considering the very small amount of data.\nFor example, Ms. Pac-Man got a score of 515 with just the\nbest two episodes (28330 samples) of the dataset. Training\nwith the entire dataset (2829068 samples) resulted in a score\nof 774. The score with the top two episodes of Space Invaders\n(20112 samples) was 193, while a model trained with the full\ndataset (4005345 samples) got a slightly lower score of 190\npoints. Q*bert score, however, dropped sharply when smaller\ndatasets than the top 5% were used. These results suggest that\neven a very small amount of high-quality data can result in a\ncomparatively well performing agent.\nFor Doom experiments, we trained models with each\nplayer’s data, as well as with all players’ data combined. On\nHGS (Health Gathering Supreme), an agent trained with the\ndata collected from one of the players achieved a slightly\nhigher score than the agent trained with all players’ combined\ndata. With the Deathmatch scenario, however, the agent trained\nwith the combined data reached a higher score than any of the\nagents trained with individual players’ data. We believe this\nis because of the complexity of the two scenarios: deathmatch\nhas a wide variety of different enemies and available weapons\nand items, so having more data is beneﬁcial. HGS is a more\nstraightforward scenario. Interestingly, despite all three players\nhad highest score possible in HGS in all of the recorded data,\nthe performance of trained agents varied between 4 and 11\naverage score, depending on which player’s data agent was\ntrained on. We believe this is because of the differences in\nhow different participants played the game.\nC. Action delay\nThe ﬁrst row of Figure 6 shows the action delay results\nfor the Atari Grand Challenge dataset. Q*bert, Montezuma’s\nRevenge and Space Invaders see a signiﬁcant increase in\nevaluation scores with positive action delay values, with the\nlargest increase seen when using a delay of ﬁve frames. Action\ndelay does not have a large effect with Ms. Pac-Man, apart\nfrom a large drop in ﬁnal score caused by delay values of\n−100 and 100. Video Pinball achieves the best performance\nwith zero action delay, although the score is still well below the\n0% mark set by the random agent. Results for Atari-HEAD\ndataset show smaller yet consistent improvements with two\nframe delay in Montezuma’s Revenge and Space Invaders.\nSame applies to both ViZDoom scenarios, where delay of two\nimproved the performance slightly over zero delay.\nWith the Atari games’ frame rate of 60, the delay of ﬁve\nframes (Atari-GC) corresponds to about 83 milliseconds of\ndelay, and two frames (Atari-HEAD) is about 33 milliseconds.\nOur ViZDoom datasets are collected at 17.5 frames per second,\nand delay of two frames corresponds to 114 milliseconds.\nThe differences between two Atari datasets reﬂect how Atari-\nHEAD was collected in a synchronous manner (game waited\nfor human to execute an action), with delay from observation\nto action being lower albeit not zero.\nV. CONCLUSION\nWe benchmarked end-to-end behavioural cloning in various\nvideo games and studied the effect of quality of expert data\nand the delay from human reaction time. Our results show that\nbehavioural cloning agents can learn basic mechanics\/rules of\nthe games (e.g. coherent movement) with a small amount of\ndata (one hour of human gameplay), but generally only achieve\nfraction of the performance of human players, and sometimes\neven worse than a random agent. We demonstrate how the\nquantity of the data matters less when only a limited amount of\ndata is available, and how adjusting for the human delay from\nobservations to actions (reﬂexes) improves the performance.\nBased on these results, we recommend using high-quality\ndata, rather than just large quantities of any data, for be-\nhavioural cloning. If data is gathered from human demon-\nstrators, we also recommend offsetting the recorded action\nby assigning them to observations 100ms earlier. This is to\ncounteract the state-action mismatch introduced by the delay\nfrom observations to actions.\nAs a future work, we would like to solve issues that still\nremain, e.g. using “super-resolution” networks to handle high-\ndeﬁnition images instead of resizing them, using recurrent\nnetworks and trying to avoid causal confusion [21]. A simple\nquestion remaining is also how far we can get with BC with a\nlarge amount of data, like in Starcraft II [6]. Going beyond BC,\nmethods like generative adversarial imitation learning (GAIL)\n[39] and batch reinforcement learning [40] require simulations\nor reward signals but show improvements over behavioural\ncloning. All things considered, including successful applica-\ntions of reinforcement learning and the recent improvements in\nimitation learning, we remain hopeful for human-level agents\nin video games, despite the less-than-ideal results presented\nhere.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, p. 529, 2015.\n[3] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.\nCastaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman,\net al., “Human-level performance in ﬁrst-person multiplayer games\nwith population-based deep reinforcement learning,” arXiv:1807.01281,\n2018.\n[4] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison,\nD. Farhi, Q. Fischer, S. Hashme, C. Hesse, et al., “Dota 2 with large\nscale deep reinforcement learning,” arXiv:1912.06680, 2019.\n[5] D. Ye, Z. Liu, M. Sun, B. Shi, P. Zhao, H. Wu, H. Yu, S. Yang, X. Wu,\nQ. Guo, et al., “Mastering complex control in moba games with deep\nreinforcement learning,” in AAAI, 2020.\n[6] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\nJ. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al., “Grand-\nmaster level in starcraft ii using multi-agent reinforcement learning,”\nNature, pp. 1–5, 2019.\n[7] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhn-\nevets, M. Yeo, A. Makhzani, H. K¨uttler, J. Agapiou, J. Schrittwieser,\net al., “Starcraft ii: A new challenge for reinforcement learning,”\narXiv:1708.04782, 2017.\n[8] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar,\nand D. Lange, “Unity: A general platform for intelligent agents,”\narXiv:1809.02627, 2018.\n[9] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural\nnetwork,” in NIPS, 1989.\n[10] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,\nD. Horgan, J. Quan, A. Sendonaris, I. Osband, et al., “Deep q-learning\nfrom demonstrations,” in AAAI, 2018.\n[11] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al., “End to\nend learning for self-driving cars,” arXiv:1604.07316, 2016.\n[12] F. Codevilla, M. Miiller, A. L´opez, V. Koltun, and A. Dosovitskiy, “End-\nto-end driving via conditional imitation learning,” in ICRA, 2018.\n[13] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso,\nand R. Salakhutdinov, “MineRL: A large-scale dataset of Minecraft\ndemonstrations,” IJCAI, 2019.\n[14] R. Zhang, Z. Liu, L. Guan, L. Zhang, M. M. Hayhoe, and D. H. Bal-\nlard, “Atari-head: Atari human eye-tracking and demonstration dataset,”\narXiv:1903.06754, 2019.\n[15] V. Kurin, S. Nowozin, K. Hofmann, L. Beyer, and B. Leibe, “The atari\ngrand challenge dataset,” arXiv:1705.10998, 2017.\n[16] C. Pelling and H. Gardner, “Two human-like imitation-learning bots with\nprobabilistic behaviors,” in COG, IEEE, 2019.\n[17] S. Zanetti and A. E. Rhalibi, “Machine learning techniques for fps in\nq3,” in ACM SIGCHI, 2004.\n[18] B. Gorman and M. Humphrys, “Imitative learning of combat behaviours\nin ﬁrst-person computer games,” CGAMES, 2007.\n[19] J. Harmer, L. Gissl´en, J. del Val, H. Holst, J. Bergdahl, T. Olsson,\nK. Sj¨o¨o, and M. Nordin, “Imitation learning with concurrent actions\nin 3d games,” in CIG, 2018.\n[20] Z. Chen and D. Yi, “The game imitation: Deep supervised convolutional\nnetworks for quick video game ai,” arXiv:1702.05663, 2017.\n[21] P. de Haan, D. Jayaraman, and S. Levine, “Causal confusion in imitation\nlearning,” in NeurIPS, 2019.\n[22] C. Scheller, Y. Schraner, and M. Vogel, “Sample efﬁcient reinforce-\nment learning through learning from demonstrations in minecraft,”\narXiv:2003.06066, 2020.\n[23] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The arcade\nlearning environment: An evaluation platform for general agents,” Jour-\nnal of Artiﬁcial Intelligence Research, vol. 47, pp. 253–279, 2013.\n[24] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[25] M. Wydmuch, M. Kempka, and W. Ja´skowski, “Vizdoom competitions:\nPlaying doom from pixels,” IEEE Transactions on Games, vol. 11, no. 3,\npp. 248–259, 2018.\n[26] B. Bukaty and D. Kanne, “Using human gameplay to augment rein-\nforcement learning models for crypt of the necrodancer.” https:\/\/github.\ncom\/bbukaty\/CoNBot, 2017.\n[27] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap,\nJ. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep rein-\nforcement learning in large discrete action spaces,” arXiv:1512.07679,\n2015.\n[28] T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor,\n“Learn what not to learn: Action elimination with deep reinforcement\nlearning,” in NIPS, 2018.\n[29] O. Delalleau, M. Peter, E. Alonso, and A. Logut, “Discrete and con-\ntinuous action representation for practical rl in video games,” in AAAI\nWorkshop on Reinforcement Learning in Games, 2019.\n[30] V. Firoiu, T. Ju, and J. Tenenbaum, “At human speed: Deep reinforce-\nment learning with action delay,” arXiv:1810.07286, 2018.\n[31] E. Schuitema, L. Bus¸oniu, R. Babuˇska, and P. Jonker, “Control delay\nin reinforcement learning for real-time dynamic systems: a memoryless\napproach,” in IROS, 2010.\n[32] P. Bontrager, A. Khalifa, D. Anderson, M. Stephenson, C. Salge, and\nJ. Togelius, “” superstition” in the network: Deep reinforcement learning\nplays deceptive games,” in AAAI Artiﬁcial Intelligence and Interactive\nDigital Entertainment, 2019.\n[33] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Ja´skowski,\n“Vizdoom: A doom-based ai research platform for visual reinforcement\nlearning,” in CIG, 2016.\n[34] A. Hill, A. Rafﬁn, M. Ernestus, A. Gleave, A. Kanervisto, R. Traore,\nP. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,\nJ. Schulman, S. Sidor, and Y. Wu, “Stable baselines.” https:\/\/github.com\/\nhill-a\/stable-baselines, 2018.\n[35] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, 2016.\n[36] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward,\nY. Doron, V. Firoiu, T. Harley, I. Dunning, et al., “Impala: Scalable dis-\ntributed deep-rl with importance weighted actor-learner architectures,”\narXiv:1802.01561, 2018.\n[37] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv:1412.6980, 2014.\n[38] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning\nand structured prediction to no-regret online learning,” in AISTATS,\n2011.\n[39] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in NIPS,\n2016.\n[40] S. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau, “Benchmarking\nbatch deep reinforcement learning algorithms,” arXiv:1910.01708, 2019.\nAPPENDIX A\nGAME DESCRIPTIONS\na) Downwell:\nA roguelike, vertically scrolling plat-\nformer published by Devolver Digital in 2015, with simple\ndynamics and graphics. Human players were instructed not to\nuse shops, as buying items affects the ﬁnal score.\n• Resolution: 760 × 568 (95 × 70)\n• Allowed buttons: jump\/shoot, left and right.\n• Game start: Start of the game (when player selects\n“restart”).\n• Game end: Player death or 5 minute timeout.\n• Score: Number of gems upon end of the game.\nb) Crypt of The NecroDancer (CoTN): A roguelike,\nrhythm-based dungeon exploration game published by Brace\nYourself Games in 2015. Normally, players and NPCs move\nonly at the beats of the music, but we remove this mechanic\nby using an easier character (“Bard”), to focus on the dungeon\nexploration aspect. Human players were instructed not to use\nshops, as buying items affects the ﬁnal score.\n• Resolution: 1280 × 720 (160 × 90).\n• Allowed buttons: left, right, up and down.\n• Game start: Start of the “all ﬂoors run”.\n• Game end: Death, reaching Zone 2 or 10 minute timeout.\n• Score: Number of coins in the end.\nc) Super Hexagon: A 2D “twitch” video game, where\nplayer has to simply avoid incoming obstacles, published by\nTerry Cavanagh in 2012.\n• Resolution: 1280 × 720 (160 × 90).\n• Allowed buttons: left and right.\n• Game start: Start of the ﬁrst level (“Hexagon”, normal\nmode).\n• Game end: Death.\n• Score: Time survived in seconds.\nd) Boson X: A 3D twitch game by Ian MacLarty (2014),\nwhere player has to jump over holes and obstacles in speeding-\nup platform.\n• Resolution: 1280 × 720 (160 × 90).\n• Allowed buttons: left and right.\n• Game start: Start of the ﬁrst level (“Geon”).\n• Game end: Death.\n• Score: In-game score.\ne) Binding of Isaac: Rebirth (BoI): A roguelike, top-\ndown shooter published by Nicalis Inc. in 2014 (a remake\nof “Binding of Isaac”), where player progresses in rooms\nby killing all the enemies and collecting items to power\nthemselves up. In-game score ticks down as time progresses,\nbut we use it to include activity of the player.\n• Resolution: 1280 × 720 (160 × 90).\n• Allowed buttons: left, right, up, down, shoot\nleft, shoot right, shoot up, shoot down and\nplace bomb.\n• Game start: Start of the game with “Isaac” character with\ndefault settings.\n• Game end: Death, beating the second boss or 10 minute\ntimeout.\n• Score: In-game score.\nf) BeamNG.drive: A driving game with accurate models\nof car mechanics, published by BeamNG in 2015.\n• Resolution: 1280 × 768 (165 × 96).\n• Allowed buttons: accelerate, brake, left, right.\n• Game start: The “Handling Circuit” spawn point on the\n“Automation Test Track” map with the “Gavril D-Series\nD15 V8 4WD (A)” vehicle.\n• Game end: Two full laps completed, or agent does not\nmove for 10 seconds (e.g. stuck, car immobilized).\n• Score: Meters driven until a collision or the end of the\nsecond lap (as reported by the in-game “Trip Computer”).\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Benchmarking End-to-End Behavioural Cloning on Video Games.pdf"}
{"title":"Design, Benchmarking and Explainability Analysis of a Game-Theoretic Framework towards Energy Efficiency in Smart Infrastructure","authors":"Ioannis C. Konstantakopoulos, Hari Prasanna Das, Andrew R. Barkan, Shiying He, Tanya Veeravalli, Huihan Liu, Aummul Baneen Manasawala, Yu-Wen Lin, Costas J. Spanos","summary":"In this paper, we propose a gamification approach as a novel framework for\nsmart building infrastructure with the goal of motivating human occupants to\nreconsider personal energy usage and to have positive effects on their\nenvironment. Human interaction in the context of cyber-physical systems is a\ncore component and consideration in the implementation of any smart building\ntechnology. Research has shown that the adoption of human-centric building\nservices and amenities leads to improvements in the operational efficiency of\nthese cyber-physical systems directed towards controlling building energy\nusage. We introduce a strategy in form of a game-theoretic framework that\nincorporates humans-in-the-loop modeling by creating an interface to allow\nbuilding managers to interact with occupants and potentially incentivize energy\nefficient behavior. Prior works on game theoretic analysis typically rely on\nthe assumption that the utility function of each individual agent is known a\npriori. Instead, we propose novel utility learning framework for benchmarking\nthat employs robust estimations of occupant actions towards energy efficiency.\nTo improve forecasting performance, we extend the utility learning scheme by\nleveraging deep bi-directional recurrent neural networks. Using the proposed\nmethods on data gathered from occupant actions for resources such as room\nlighting, we forecast patterns of energy resource usage to demonstrate the\nprediction performance of the methods. The results of our study show that we\ncan achieve a highly accurate representation of the ground truth for occupant\nenergy resource usage. We also demonstrate the explainable nature on human\ndecision making towards energy usage inherent in the dataset using graphical\nlasso and granger causality algorithms. Finally, we open source the\nde-identified, high-dimensional data pertaining to the energy game-theoretic\nframework.","url":"http:\/\/arxiv.org\/abs\/1910.07899v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1910.07899v1","published":1571211386000,"comment":"arXiv admin note: substantial text overlap with arXiv:1809.05142,\n  arXiv:1810.10533","pdf_text":"Design, Benchmarking and Explainability Analysis\nof a Game-Theoretic Framework towards Energy\nEfﬁciency in Smart Infrastructure\nIoannis C. Konstantakopoulos†∗, Hari Prasanna Das†, Andrew R. Barkan‡, Shiying He†,\nTanya Veeravalli†, Huihan Liu†, Aummul Baneen Manasawala§, Yu-Wen Lin†, Costas J. Spanos†\n†Department of Electrical Engineering and Computer Sciences, University of California, Berkeley\n‡Department of Mechanical Engineering, University of California, Berkeley\n§Department of Industrial Engineering and Operations Research, University of California, Berkeley\nAbstract\nIn this work, we propose a gamiﬁcation approach as a novel framework for smart\nbuilding infrastructure with the goal of motivating human occupants to reconsider\npersonal energy usage and to have positive effects on their environment. Human\ninteraction in the context of cyber-physical systems is a core component and\nconsideration in the implementation of any smart building technology. Research\nhas shown that the adoption of human-centric building services and amenities\nleads to improvements in the operational efﬁciency of these cyber-physical systems\ndirected towards controlling building energy usage. We introduce a strategy in form\nof a game-theoretic framework that incorporates humans-in-the-loop modeling by\ncreating an interface to allow building managers to interact with occupants and\npotentially incentivize energy efﬁcient behavior. Prior works on game theoretic\nanalysis typically rely on the assumption that the utility function of each individual\nagent is known a priori. Instead, we propose novel utility learning framework for\nbenchmarking that employs robust estimations of occupant actions towards energy\nefﬁciency. To improve forecasting performance, we extend the utility learning\nscheme by leveraging deep bi-directional recurrent neural networks. Using the\nproposed methods on data gathered from occupant actions for resources such as\nroom lighting, we forecast patterns of energy resource usage to demonstrate the\nprediction performance of the methods. The results of our study show that we can\nachieve a highly accurate representation of the ground truth for occupant energy\nresource usage. We also demonstrate the explainable nature on human decision\nmaking towards energy usage inherent in the dataset using graphical lasso and\ngranger causality algorithms. For demonstrations of our infrastructure and for\ndownloading de-identiﬁed, high-dimensional data sets, please visit our website 2.\n1\nIntroduction\nNearly half of all energy consumed in the U.S. can be attributed to usage by residential and commer-\ncial buildings (McQuade, 2009). In efforts to improve energy efﬁciency in buildings, researchers and\nindustry leaders have attempted to implement novel control and automation approaches alongside\ntechniques like incentive design and adaptive price adjustment to more effectively regulate energy\nusage. All the solutions pertaining to making a building occupant friendly and energy efﬁcient fall\nunder the umbrella of smart building techniques. To summarize the space, an ideal smart building\ninfrastructure accommodates a variety of occupant preferences including thermal comfort (Karmann\n∗Corresponding Author. Email: ioanniskon@berkeley.edu\n2Open sourced high-dimensional data and demonstrations: https:\/\/smartntu.eecs.berkeley.edu\narXiv:1910.07899v1  [cs.LG]  16 Oct 2019\nOccupant1\nOccupant2\nOccupantn\nCost1 + mech.\nCost2 + mech.\nCostn + mech.\n…\nNon-Cooperative Game\nIncentive Design \n(Control)\nUtility Learning \n(Estimation)\nEnergy Cyber-Physical System\nEnergy Game-Theoretic Framework\nEnergy \nUsage\nPatterns\nDemand \nResponse\nPrice\nGrid \nLevel\nUtility \nCompany\n(a) Gamiﬁcation abstraction for Human-Centric\nCyber-Physical Systems\nSmart Building\nData Management\nIncentive Design\nHuman-Centric CPS\nSmart Grid\nDemand Response\n(b) High level view of the proposed design framework\nFigure 1: Gamiﬁcation abstraction and high level view of proposed framework\net al., 2018; Liu et al., 2018, 2019), satisfaction\/well-being (Frontczak et al., 2012), lighting com-\nfort (Zhu et al., 2017b), acoustical quality (Yang et al., 2017), occupancy based control (Zou et al.,\n2019b), indoor air quality (Sundell et al., 2011), indoor environmental monitoring (Jin et al., 2018c),\nactivity\/gesture based control (Zou et al., 2019c,a), privacy (Jia et al., 2017) and productivity (Horr\net al., 2016), while simultaneously optimizing for energy efﬁciency and agile connectivity to the grid.\nA common avenue for the regulation of energy usage in buildings is through their on-site building and\nfacility managers. Building managers are obligated to maintain an energy efﬁcient schedule according\nto a standard operating procedure. Even with these considerations, there is still a tremendous need for\nscalable and robust frameworks that can efﬁciently coordinate and control building resource usage in\nthe presence of confounding dynamics such as human behavior.\nRecently, utility companies have invested in demand response programs that can address improper\nload forecasting while also helping building managers encourage energy efﬁciency among building\noccupants (Shariatzadeh et al., 2015; Bianchini et al., 2016). Commonly, the implementation of these\nprograms is enacted on a contract basis between utility providers and the consumers under arranged\nconditions of demand\/usage. The building managers will then be bound by contract to operate\naccording to the agreed-upon schedule. However, the conditions of these contracts are static and do\nnot consider dynamic changes in occupant behavior or preferences, which can result in discrepancies\nin demand\/usage expectations. To facilitate the adoption of more dynamic protocols for demand\nresponse, our setup features a gamiﬁcation interface (seen in the building level in Figure 1a) that\nallows building managers to interact with a building’s occupants. By leveraging our gamiﬁcation\ninterface, retailers and utility companies at the provider level can utilize a wealth of dynamic and\ntemporal data on building energy usage, extending even to occupant usage predictions, in order to\ncustomize demand response program approaches to observed or predicted conditions Jin et al. (2017,\n2018b). Above all, our gamiﬁcation interface is designed to support engagement and integration on\nmultiple levels in a human-centric cyber-physical system, which is formally deﬁned as:\nSystems or mechanisms that combine computer-based technologies with physical processes to\nintegrate direct human coordination through various interaction modalities.\nThe availability of a game-theoretic framework will allow researchers to demonstrate gaming data\nin a discrete choice setting, run simulations that include data on dynamic occupant preferences, test\ncorrelations between actions and external parameters (e.g. provided weather metrics), and leverage\ntemporal data in demand response program scenarios.\nThe cooperation of human agents with building automation in smart infrastructure setting helps in\nimproving the system robustness and sustainability by taking advantage of both the control potential\nof computational methods and the real life information provided by humans-in-the-loop elements. The\ninherent adaptability and simultaneous automation in such a system makes it possible to accommodate\na wide variety of dynamic situations that might arise in the maintenance of building infrastructure,\nlike the automatic shifting of demand during peak energy\/usage hours. Put into more broad terms,\nthe goal of many building infrastructure systems is to enact system-level efﬁciency improvements by\nusing a high-level planner (e.g. facility manager) to coordinate autonomously acting agents in the\nsystem (e.g. selﬁsh human decision-makers). It is this type of functionality that makes smart building\ntechnology so essential to the development of an ideal smart city.\n2\nOur approach to efﬁcient building energy management focuses on improving energy usage behavior\namong occupants in residential dorm buildings, by utilizing cutting-edge Internet of Things (IoT)\nsensors and cyber-physical system sensing\/actuation platform integrated with the aforementioned\ngamiﬁcation interface (Figure 1a). The interface is designed to support occupant engagement and\nintegration while learning occupant preferences over shared resources. It also provides an window to\nunderstand how human preferences change as a function of critical factors such as manual control of\ndevices, time of day, and provided incentives. Our gamiﬁcation framework can be used in the design\nof incentive mechanisms in the form of fair compensation that help to realign agent preferences with\nthose of the planner, which are often representative of system-level performance criteria.\nWe present a social game aimed at incentivizing occupants to modify their behavior so that the overall\nenergy consumption in their room is reduced. In certain living situations, occupants in residential\nbuildings are not responsible for paying for the energy resources they consume. For this reason, there\nis often an imbalance between the motivations of the facility manager and those of the occupants. The\ncompetitive aspects of the social game framework motivate occupants to address their inefﬁciencies\nwhile encouraging responsible energy usage on an individual basis. At the core of our approach is a\ndecision model, which treats building occupants as non-cooperative agents who play according to a\nsequential discrete choice game. Discrete choice models have been used extensively to investigate\nrepresentations for scenarios like variation in modes of transportation (Qin et al., 2017), demand for\norganic foods (Hillier et al., 2015), and even school social interactions (Karmargianni et al., 2014).\nOur framework is centered around learning agent preferences over room resources, such as lighting, as\nwell as external parameters like weather conditions, high-level grid control, and provided incentives.\nSpeciﬁcally, agents are strategic entities that make decisions based on their own preferences without\nconsideration of other decision makers. The game-theoretic framework allows for qualitative insights\nto be made about the outcome of this selﬁsh behavior (better than a simple predictive model) and,\nmore importantly, can be leveraged in designing effective incentive mechanisms to motivate agents.\nThe broader motivation of this paper is to introduce a general framework that utilizes game theoretic\nconcepts to learn models of players decision-making in residential buildings, which is made possible\nby the implementation of our energy game-theoretic framework. We present a variety of benchmark\nutility learning models and a novel pipeline for the efﬁcient training of these models. In order to\nboost predictive power, we propose end-to-end Deep Learning models focused on the utilization\nof deep recurrent neural networks for analyzing gaming data. To handle sequential information\ndependencies in our data, we implement a deep learning based architecture using Long Short Term\nMemory cells (LSTM). With the advent of explainable Artiﬁcial Intelligence (AI), there has been\na massive move towards making statistical models explainable. So, we perform feature correlation\nstudy using graphical lasso algorithm and causality study using grangers causality to conﬁrm that the\ndata obtained from the social game holds accurate information on human decision making towards\nenergy usage in competetive settings. Finally, we open source our data and provide a web portal\nfor demonstrating our infrastructure and for downloading de-identiﬁed, high-dimensional data3.\nHigh-dimensional data can serve either as a benchmark for alternative discrete choice model learning\nschemes or as a helpful source for analyzing occupant energy usage in residential buildings.\nThe body of the paper is organized as follows. Previous works are surveyed in Section 2 with\nan emphasis on human-centric models with integration in smart grid infrastructures. Section 3\ndescribes the social game experiment on the Nanyang Technological University campus and the\nhuman decision-making model. Section 4 introduces utility estimation along with several proposed\nmachine learning & deep learning algorithms for sequential decision games. It also describes the\nmethodology for explainability analysis. Results are presented in Section 5. We make concluding\nremarks in Section 6.\n2\nRelated Work\nSmart grid technology focuses on enabling efﬁcient grid integration and comprehensive analysis\nto support advances in renewable energy sources, power systems management, minimization of\ninefﬁciencies in usage, and maximization of user savings. However, challenges in power grid\napplications, such as the lack of predictability, and the stochastic environment in which the power\ngrid operates complicate the synthesis of an all-encompassing solution. To address these problems,\n3Demo web portal and open sourced data: https:\/\/smartntu.eecs.berkeley.edu\n3\nindustry leaders and researchers in the ﬁelds of power grid design and control have put forth\nconsiderable research and development efforts in smart grid design, demand-side management, and\npower system reliability.\nWith the help of digital hardware and information technology, smart grid design relies more and\nmore on the development of decision-capable intelligence in the context of grid automation. Novel\nmethods (Salehi et al., 2012; Das & Pradhan, 2016) for smart grid design incorporate real-time\nanalysis and stochastic optimization to provide power grid observability, controllability, security,\nand an overall reduction of operational costs. Speciﬁcally, the integration of data analytics and\ninnovative software platforms have led to effective trends in demand-side management (Ramchurn\net al., 2011) and demand response (Maharjan et al., 2013). These studies explored and drew upon\nmethods from behavioral decision research to analyze the decision patterns of users and consumers.\nThe simulations and empirical results from these studies reinforce the signiﬁcance of forecasting\nenergy demands and the potential advantages of managing these demands by leveraging models of\nintelligent decision-makers. In this context, we can see that the process of modeling and predicting\nthe actions of decision-makers in the control of large networks is a signiﬁcant development towards\nimproving the operational efﬁciency of smart grids.\nGame theory can serve as an extremely useful tool for the real-time forecasting of decision-makers\nin an interactive setting. Classical models from game theory allow for qualitative insights about\nthe outcome of scenarios involving the selﬁsh behavior of competitive agents and can be leveraged\nin the design of incentives for inﬂuencing the goals of these agents. Contemporary research in the\nenergy and power systems domain leverage game theoretic models in a multitude of applications. As\npreviously mentioned, these types of approaches have been implemented in the modeling of various\naspects of smart grid control. Speciﬁcally, we can observe instances of game theory applications in\nthe context of smart grid demand response programs using methods such as supply-balancing (Yu &\nHong, 2016), hierarchical Stackelberg game settings (Yu & Hong, 2017), and Vickrey-Clarke-Groves\n(VCG) auction mechanisms (Samadi et al., 2012). The use of game theoretic models creates new\navenues for modeling dynamic economic interactions between utility providers and consumers inside\na distributed electricity market (Zhang et al., 2015). Another example study is the investigation of\ncrowdfunding as an incentive design methodology for the construction of electric vehicle charging\npiles (Zhu et al., 2017a). Game theory has also been directed towards the optimal design of curtailment\nschemes that control the fair allocation of curtailment among distributed generators (Andoni et al.,\n2017). Expanding on previous work, researchers have studied game theory applications in the context\nof incentive-based demand response programs for customer energy reduction as well (Vuelvas et al.,\n2018). In these types of applications, customer interaction with the incentive provider is modeled\nusing game theory while their engagement is represented probabilistically.\nIn majority of the previously discussed game theoretic applications, results are generated purely by\nsimulation without the use of real data. Furthermore, previous applications fail to propose any novel\ntechniques for learning the underlying utility functions that dynamically predict strategic actions. Due\nto these limitations, we cannot reasonably expect to learn (or estimate) user functions in a gaming\nsetting nor generalize results to broader scenarios. In real-life applications, the player utilities are not\nknown a priori; therefore, the developed methods should have some way to account for data-driven\nlearning techniques. In our past work, we have explored utility learning and incentive design as a\ncoupled problem both in theory (Konstantakopoulos et al., 2016b, 2018b; Jia et al., 2018b) and in\npractice (Ratliff et al., 2014; Konstantakopoulos et al., 2016a, 2017) under a Nash equilibrium model.\nOur utility learning approaches are presented in a platform-based design ﬂow for smart buildings (Jia\net al., 2018a; Jin et al., 2018a). The broader purpose of this paper is to present a general learning\nframework that leverages game theoretic concepts for learning models of occupant decision making\nin a competitive setting and under a discrete set of actions.\nContemporary building energy management techniques employ a variety of algorithms in order to\nimprove performance and sustainability. Many of these approaches leverage ideas from topics such\nas optimization theory and machine learning. Our goal was to improve building energy efﬁciency\nby introducing a gamiﬁcation system that engages users in the process of energy management and\nintegrates seamlessly through the use of a human-centric cyber-physical framework. There exists a\nconsiderable amount of previous work demonstrating the success of control and automation in the\nimprovement of building energy efﬁciency Nagy et al. (2015); Ascione et al. (2017). Some other\nnotable techniques implement concepts such as incentive design and adaptive pricing Mathieu et al.\n(2012); He et al. (2018). Modern control theory has been a critical source of inspiration for several\n4\napproaches that employ ideas like model predictive and distributed control and have demonstrated\nencouraging results in applications like HVAC. Unfortunately, these control approaches lack the\nability to consider the individual preferences of occupants, which highlights a signiﬁcant advantage\nof human-centric scenarios over contemporary methods. While machine learning approaches are\ncapable of generating optimal control designs, they fail to adjust to occupant preferences and the\nassociated implications of these preferences to the control of building systems. The heterogeneity\nof user preferences in regard to building utilities is considerable and necessitates a system that can\nadequately account for differences from occupant to occupant.\nClearly, the presence of occupants greatly complicates the determination of an efﬁcient building\nmanagement system. With this in mind, focus has shifted toward modeling occupant behavior within\nthe system in an effort to incorporate their preferences. To accomplish this task, the building and\nits occupants are represented as a multi-agent system targeting occupant comfort Nagy et al. (2015).\nFirst, occupants and managers are allowed to express their building preferences, and these preferences\nare used to generate an initial control policy. An iteration on this control policy is created by using a\nrule engine that attempts to ﬁnd compromises between preferences. Some drawbacks of this control\ndesign are immediately apparent. There should be some form of communication to the manager\nabout occupant preferences. In addition, there is no incentive for submission of true user preferences\nand no system for considering occupant feedback. Other related topics in the same vein focus on grid\nintegration Samad et al. (2016), while still others consider approaches for policy recommendations\nand dynamic pricing systems Mathieu et al. (2012).\nAs alluded to previously, the key to our approach is the implementation of a social game among\nusers in a non-cooperative setting. Similar methods that employ social games have been applied to\ntransportation systems with the goal of improving ﬂow (Merugu et al., 2009; Pluntke & Prabhakar,\n2013). Entrepreneurial ventures have also sought to implement solutions of their own to the problem\nof managing building energy efﬁciency 4 5 6. Finally, it has been shown that societal network games\nare useful in a smart city context for improving energy efﬁciency and human awareness (De Luca &\nCastri, 2014). The critical motivation behind the social game context is to create friendly competition\nbetween occupants. In turn, this competition will encourage occupants to individually consider their\nown energy usage and, hopefully, seek to improve it. This same gamiﬁcation technique has also\nbeen used as a way to educate the public about energy usage (Salvador et al., 2012; Orland et al.,\n2014). Additionally, it has been cleverly implemented in a system that presents feedback about\noverall energy consumption to occupants (Simon et al., 2012). Another notable case of a gamiﬁcation\nmethodology was used to engage individuals in Demand Response (DR) schemes (Li et al., 2014).\nIn this application, each of the users were represented as an utility maximizer within the model of a\nnash equilibrium where occupants gain incentive for reduction in consumption during DR events.\nIn contrast to approaches that target user devices with known usage patterns (Li et al., 2014), our\napproach focuses on personal room utilities, such as lighting, without initial usage information, which\nsimulates scenarios of complete ignorance to occupant behaviors. For our method, we utilize past user\nobservations to learn the utility functions of individual occupants by way of several novel algorithms.\nUsing this approach, we can generate excellent prediction of expected occupant actions. Our unique\nsocial game methodology simultaneously learns occupant preferences while also opening avenues\nfor feedback. This feedback is generated through individual surveys that provide opportunities\nto inﬂuence occupant behavior with adaptive incentive. With this technique, we are capable of\naccommodating occupant behavior in the automation of building energy usage by learning occupant\npreferences and applying a variety of novel algorithms. Furthermore, the learned preferences can be\nadjusted through incentive mechanisms to enact improved energy usage.\nA series of experimental trials were conducted to generate real-world data, which was then used as the\nmain source of data for our approach. This differentiates our work from a large portion of other works\nin the same ﬁeld that use simulations in lieu of experimental methods. In many cases, participants\nexhibit a tendency to revert to previously inefﬁcient behavior after the conclusion of a game. Our\napproach combats this effect by implementing incentive design that can adapt to the behavior and\npreferences of occupants progressively, which ensures that continuous participant engagement.\n4https:\/\/comfyapp.com\n5https:\/\/coolchoices.com\/how-it-works\/improve\n6https:\/\/www.keewi-inc.com\/index.php\n5\nWe use conventional machine learning algorithms as well as deep learning based algorithms to\nbenchmark the forecast of energy resource usage by occupants (or their utility function). We use the\ngraphical lasso algorithm as a powerful tool to understand the latent conditional dependence between\nvariables (Hastie et al., 2015). This in turn provides insights into how different features interplay\namong each other. Historically, Graphical Lasso has been used in various ﬁelds of science, ranging\nfrom study of how individual elements of the cell interact with each other (Zuo et al., 2015) and to\nthe broad area of computer vision for scene labelling Souly & Shah (2016). A modiﬁed version of the\noriginal algorithm, named time-varying graphical lasso, has been used on ﬁnancial and automotive\ndata (Hallac et al., 2017). However, the novelties of graphical lasso has not been well utilized in\nthe area of energy cyber-physical systems. We use Granger causality(Granger, 1980) to explain\nthe causal relationship between the features in energy usage behavior of agents in social game. It\nhas been widely used in the energy domain in applications such as deducing the causal relationship\nbetween economic growth and energy consumption (Chiou-Wei et al., 2008).\nWith this social game framework, a building manager will be capable of considering the individual\npreferences of the occupants within the scope of the building’s energy consumption. This social game\nsystem could potentially offer an unprecedented amount of control for managers without sacriﬁcing\noccupant comfort and independence.\n3\nSmart Building Social Game: Implementation & Human Decision-Making\nIn this section, we introduce our proposed social game concept as a gamiﬁcation application imple-\nmented at Nanyang Technological University (NTU) residential housing apartments, along with the\nsoftware architecture design for the deployed Internet of Things (IoT) sensors. In addition to the\nimplementation of this gamiﬁcation application, we abstract the agent decision-making processes in a\ngame theoretic framework and introduce the discrete choice theory that we draw upon for forecasting\nagent actions with high accuracy.\n3.1\nDescription of the Social Game Experiment\nOur experimental environment is comprised of residential housing single room apartments on the\nNanyang Technological University (NTU) campus. The residential housing single room apartments\non the NTU campus were divided into four blocks, each of which had two ﬂoors. In this space,\nthere were a total of seventy-two occupants who were eligible to participate in the social game.\nParticipation in our social game platform was voluntary. We ran the experiment in both the Fall 2017\nand Spring 2018 semesters. In the Fall 2017 version, we included ceiling light, desk light, and ceiling\nfan resources in the graphical user interface for the social game, while in the Spring 2018 version we\nincluded all of the potential resources that were available.\nWe designed a social game web portal so that all single room dorm occupants could freely view their\ndaily room’s energy resource usage with a convenient interface. In each dorm room, we installed two\nInternet of Things (IoT) sensors 7, one close to the desk light and another near the ceiling fan. With\nthe deployment of IoT sensors, dorm occupants can monitor in real-time their room’s lighting system\n(desk and ceiling light usage) and HVAC (ceiling fan and A\/C usage) with a refresh interval of up to\none second.\nDorm occupants were rewarded with points based on how energy efﬁcient their daily usage is in\ncomparison to their past usage before the social game was deployed. The past usage data that serves\nas our baseline is gathered by monitoring occupant energy usage for approximately one month before\nthe introduction of the game for each semester. Using this prior data, we calculated a weekday and\nweekend baseline for each of the occupant’s resources. We accumulate data separately for weekdays\nand weekends so as to maintain fairness for occupants who have alternative schedules of occupancy\n(e.g. those who tend to stay at their dorm room over the weekends versus weekdays). We employ\na lottery mechanism consisting of several gift cards awarded on a bi-weekly basis to incentivize\noccupants, i.e. occupants with more points are more likely to win the lottery. Earned points for each\nresource is given by the following equation:\n7IoT sensor tag: http:\/\/www.ti.com\/ww\/en\/wireless_connectivity\/sensortag\/index.html\n6\nˆpd\ni (bi, ui) = si\nbi −ud\ni\nbi\n(1)\nwhere ˆpd\ni is the points earned at day d for room’s resource i which corresponds to ceiling light, desk\nlight, ceiling fan, and A\/C. Also, bi is the baseline calculated for each resource i, ud\ni is the usage\nof the resource at day d, and si is a points booster for inﬂating the points as a process of framing\n(Tversky & Kahneman, 1981). This process of framing can greatly impact a user’s participation, and\nit is routinely used in rewards programs for credit cards among many other point-based programs\nused in industry. In addition, we rewarded dorm occupants for the percentage of savings (equation 1)\nbecause it is important to motivate all of the participants to optimize their usage independent of the\ntotal amount of energy consumed in their normal schedule. However, over-consumption resulted in\nnegative points.\nIn Figure 2a, we present how our graphical user interface is capable of reporting to occupants the\nreal-time status (on\/off) of their devices, their accumulated daily usage, time left for achieving daily\nbaseline, and the percentage of allowed baseline being used, by hovering above the utilization bars.\nIn order to boost participation, we introduced a randomly appearing coin close to the utilization bars\nwith the purpose of incentivizing occupants to log in to the web portal and view their usage. The coin\nwas designed to have a psychological impact on the occupants, i.e. to motivate occupants towards\nviewing their resource usage and understanding their impact to energy consumption by getting exact\nusage feedback in real-time. Based on this game principle, we gave occupants points when they\nclicked on the coin, which could increase both their perceived and actual chances of winning rewards.\n3.2\nInternet of Things (IoT) System Architecture\nWe enabled the design and implementation of a large-scale networked social game through the\nutilization of cutting-edge Internet of Things (IoT) sensors. In total, we deployed one hundred and\nforty-four sensors in single dorm rooms. These were part of a hardware and software infrastructure\nthat achieved near real-time monitoring of various metrics of resource usage in each room, e.g.\nlighting and A\/C. Moreover, our system was capable of saving occupant actions in the web portal.\nWeather data was gathered from an externally-installed local weather monitoring station at per-second\nresolution. The actual design and dataﬂow is depicted in Figure 2b.\nDaily counter\nCoin boosting\nparticipation\nBaseline\nfeedback\nDevice Status\n(a) Graphical user interface (GUI)\n4G wifi Network\nSingapore\nIoT sensors\nRoom 1\nIoT sensors\nRoom 2\nIoT sensors\nRoom n-1\nIoT sensors\nRoom n\nper second feed\nvia Bluetooth\nper second feed\nvia wifi\nNear Real Time Feed\n(b) Social game dataﬂow architecture design\nFigure 2: Graphical user interface (GUI) and dataﬂow design for social game\nUtilizing the data gathered from each dorm room, we leveraged several indoor metrics like indoor\nilluminance, humidity, temperature, and vibrations for the ceiling fan sensor. Having performed\nvarious tests during Summer 2017 within the actual unoccupied dorm rooms, we derived simple\nthresholds indicating if a resource is in use or not. For instance, the standard deviation of acceleration\ngathered from the ceiling fan mounted sensor is an easy way to determine whether the ceiling fan\nis in the on state. Additionally, by combining humidity and temperature values, we were able to\nreliably identify whether A\/C is in use with limited false positives. Our calibrated detection thresholds\nwere robust over daylight patterns, external humidity\/temperature patterns, and measurement noise\nintroduced by IoT sensors.\n7\nWhile we receive data from various dorm room sensors, our back-end processes update the status\nof the devices in near real-time in each occupant’s account and update points based on their usage\nand point formula (equation 1). This functionality allows occupants to receive feedback, view their\npoints balance, check rankings, and more. In order to allow participants to assess and visualize their\nenergy efﬁcient behavior, each user’s background in the web portal changed based on their ranking\nand energy efﬁciency. We used background pictures of rain forest settings for encouraging more\nenergy efﬁcient behavior and images of desert scenes to for occupants with limited energy savings.\n3.3\nAgent Decision-Making Model\nDiscrete choice theory is greatly celebrated in the literature as a means of data-driven analysis of\nhuman decision-making. Under a discrete choice model, the possible outcome of an agent can be\npredicted from a given choice set using a variety of available features describing either external\nparameters or characteristics of the agent. We use a discrete choice model as a core abstraction for\ndescribing occupant actions related to their dorm room resources.\nConsider an agent i and the decision-making choice set which is mutually exclusive and exhaustive.\nThe decision-making choice set is indexed by the set I = {J 1, . . . , J S}. Decision maker i chooses\nbetween S alternative choices and would earn a representative utility fi for i ∈I. Each decision\namong decision-making choice set leads agents to get the highest possible utility, fi > fj for all\ni, j ∈I. In our setting, an agent has an utility which depends on a number of features xz for\nz = 1, . . . , N. However, there are several unobserved features of the representative utility which\nshould be treated as random variables. Hence, we deﬁne a random utility decision-making model for\neach agent given by\nˆfi(x) = gi(βi, x) + ϵi\n(2)\nwhere ϵi is the unobserved random component of the agent’s utility, gi(βi, x) is a nonlinear general-\nization of agent i’s utility function, and where\nx = (x1, . . . , xi−1, xi+1, . . . , xN) ∈RN\n(3)\nis the collective n features explaining an agent’s decision process. The choice of nonlinear mapping\ngi and x abstracts the agent’s decision; it could represent, e.g., how much of a particular resource\nthey choose to use and when an agent optimizes its usage over a speciﬁc resource. Discrete choice\nmodels in their classical representation Hensher & Johnson (2018) are given by a linear mapping\ngi(βi, x) = βT\ni x in which ϵi is part of independently and identically distributed random variables\nmodeled using a Gumbel distribution.\n3.4\nGame Formulation\nTo model the outcome of the strategic interactions of agents in the deployed social game, we use a\nsequential non-cooperative discrete game concept. To introduce our generalized decision-making\nmodel for each agent (equation 2), a sequential non-cooperative discrete game is given by,\nDeﬁnition 1 Each agent i has a set Fi = f 1\ni , . . . , f N\ni\nof N random utilities. Each random utility j\nhas a convex decision-making choice set Ij = {J 1\nj , . . . , J S\nj }. Given a collection of n features (3)\ncomprising the decision process and the temporal parameter T, agent i faces the following optimiza-\ntion problem for their aggregated random utilities:\nmax{\nN\nX\ni=1\nf T\ni (x)|fi ∈Fi}.\n(4)\nLike a sequential equilibrium concept, we simulate the game deﬁned by the estimated random\nutility functions per resource to demonstrate the actual decision-making process of each individual\ndorm occupant. Agents in the game independently co-optimize their aggregated random utilities\n(equation 4) given a collection of n features (equation 3) at each time instance T. A general\nincentive design mechanism (equation 1) (seen in the building level of the gamiﬁcation framework\n8\nin Figure 1a) motivates their potential actions across various given decision-making choice sets. The\nabove deﬁnition extends the deﬁnition of a discrete choice model (Hensher & Johnson, 2018) to\nsequential games in which agents co-optimize several discrete, usually mutually exclusive, choices.\n3.5\nSocial Game Data Set\nAs a ﬁnal step, we aggregate occupant data in per-minute resolution. We have several per-minute\nfeatures like time stamp, resource status, accumulated resource usage in minutes per day, resource\nbaseline, gathered points (both from game and surveys), occupant ranking over time, and number\nof occupant visits to the web portal. In addition to these features, we add several external weather\nmetrics like humidity, temperature, and solar radiation.\nAfter gathering a high-dimensional data set with all of the available features, we propose a pooling and\npicking scheme to enlarge the feature space and then apply a Minimum Redundancy and Maximum\nRelevance (mRMR) (Peng et al., 2005) feature selection procedure to identify useful features for our\npredictive algorithms. We pool additional features from a subset of the already derived features by\nleveraging domain knowledge. Speciﬁcally, we consider two different feature types: dummy features\n(using one-hot encoding) and resource features. Dummy features represent intangible variables\nrelating to weekly or seasonal events such as weekends in the former case and holidays in the latter.\nResource features include deterministic data sets gathered by our instrumentation such as daily\npercentage of resource usage. We open source the dataset after proper benchmarking.\n4\nBenchmarking and Explainability Analysis\nIn this section, we explore the utility learning problem using Deep Learning methods that serve\nto improve forecasting accuracy. From a broader perspective, our goal is to demonstrate how the\nproposed learning scheme ﬁts into the overall gamiﬁcation abstraction in Figure 1. We will show that\nour utility learning methods lead to accurate energy usage forecasts, which in turn can be integrated\nin demand response programs (seen in the provider\/retailer level in Figure 1a). This goal motivates\nwhy we are interested in learning more than a simple predictive model for agents, but rather an\nutility-based forecasting framework that accounts for individual preferences, dynamic changes in\nagent behavior, and heterogeneous actions.\n4.1\nBenchmarking using Conventional Machine Learning Framework\nIn section 3.3, we introduced an extension to discrete choice models for sequential decision-making\nover a set of different choices. More concretely, we examine the utility learning problem using a\nnovel pipeline including a variety of statistical learning methods and models that improve estimation\nand prediction accuracy for our proposed sequential discrete choice model. Furthermore, well-trained\nclassiﬁcation models serve as an excellent benchmark for our proposed Deep Learning models.\n4.1.1\nRandom Utility Estimation Pipeline\nWe start by describing the basic components of our proposed random utility estimation pipeline using\nand data gathered from the game played between agents and pooled features. Let us now introduce\nthe benchmarking pipeline formulation as it serves as the basis for the random utility estimation.\nAfter gathering streaming data in our MySQL data-base (as described in Section 3), we pool several\ncandidate features and expand our feature space. Next, a large set of proposed high-dimensional\ncandidate features is constructed. Using this feature set, we adopt a greedy feature selection algorithm\ncalled Minimum Redundancy Maximum Relevance (mRMR) (Peng et al., 2005). The mRMR greedy\nheuristic algorithm utilizes mutual information as the metric of goodness for a candidate feature set.\nGiven the large number of pooled candidate features, mRMR feature selection is a useful method\nfor ﬁnding a subset of features that are relevant for the prediction of occupant resource usage. The\nmRMR feature selection algorithm is applied to batched data from the game period either in the Fall\nor Spring version of the Social Game. From the total number of available features, we decided to\nkeep nearly half of them.\nAfter getting a number of important features as a result of the mRMR greedy algorithm, we apply\na simple data pre-processing step with mean subtraction across each individual feature. Mean\n9\nsubtraction centers the cloud of data around the origin along every dimension. On top of mean\nsubtraction, we normalize the data dimensions by dividing each dimension by its standard deviation\nin order to achieve nearly identical scale in the data dimensions. However, the training phase of the\nrandom utility estimation pipeline has one potentially signiﬁcant challenge, which is the fact that data\nin almost every resource is heavily imbalanced (e.g. the number of resources with off samples is on\nthe order of 10-20 times more than those with on samples). This is expected considering occupants\ndaily patterns of resource usage in buildings, but it poses a risk of having potentially poorly trained\nrandom utility estimation models.\nFor optimizing around highly imbalanced data sets, we adapt the Synthetic Minority Over-Sampling\n(SMOTE) Chawla et al. (2002) technique for providing balanced data sets for each resource and\nfor boosting prediction (e.g. classiﬁcation) accuracy. SMOTE over-samples a data set used in a\nclassiﬁcation problem by considering k nearest neighbors of the minority class given one current data\npoint of this class. The SMOTE algorithm can be initialized by leveraging a pre-processing phase\nwith Support Vector Machines as a grouping.\nAfter the SMOTE step, we train several classiﬁers as a ﬁnal step for the random utility estimation\npipeline. We propose a base model of logistic regression. In an effort to improve this discrete choice\nmodel, we include penalized logistic regression (regLR) with l1 norm protocol (Lasso) for the model\ntraining optimization procedure, among other classical classiﬁcation machine learning algorithms. We\nperform a randomized grid search for optimizing classiﬁers using the Area Under the Curve (AUC)\nmetric (Majnik & Bosnic, 2013), aiming to co-optimize TPR (sensitivity) and FPR (1-speciﬁcity).\nWe use the Area Under the Receiver Operating Characteristic (ROC) Curve as our performance\nmetric. ROC curves describe the predictive behavior of a binary classiﬁer by plotting the probability\nof true positive rate (i.e. correct classiﬁcation of samples as positive) over false positive rate (i.e. the\nprobability of falsely classifying samples as positive). For training the proposed machine learning\nalgorithms, we used k-fold cross validation combined with the AUC metric to randomly split the\ndata into training and validation sets in order to quantify the performance of each proposed machine\nlearning model in the training phase. Each machine learning algorithm used in our benchmark\npipeline and their respective hyper-parameters are described in more depth in Konstantakopoulos\net al. (2018a).\n4.2\nLeveraging Deep Learning for Sequential Decision-Making\nIn this section, we formulate a novel deep learning framework for random utility estimation that\nallows us to drastically reduce our forecasting error by increasing model capacity and by structuring\nintelligent deep sequential classiﬁers. The architecture for deep networks is adaptive to proposed\nsequential non-cooperative discrete game models and achieves a tremendous increase in forecasting\naccuracy. Hence, deep networks achieve an end-to-end training for modeling agents random utility\n(equation 2) with exceptional accuracy. Due to ease of access to big data and the rapid development\nof adaptive artiﬁcial intelligence techniques, energy optimization and the implementation of smart\ncities has become a popular research trend in the energy domain. Researchers have deployed Deep\nLearning and Reinforcement Learning techniques in the ﬁeld of energy prediction (Mocanu et al.,\n2016; Fan et al., 2017) and intelligent building construction (Manic et al., 2016).\nIn our framework of random utility learning in a non-cooperative game setting, deep networks work\nas powerful models that can generalize our core model by increasing capacity and by working towards\nan intelligent machine learning model for predicting agent behavior.\n4.2.1\nDeep Neural Networks for Decision-Making\nDeep neural network techniques have drawn ever-increasing research interests ever since Deep\nLearning in the context of rapid learning algorithms was proposed in 2006 Hinton et al. (2006). Our\napproach using neural networks has the inherent capacity to overcome deﬁciencies of the classical\nmethods that are dependent on the limited series of features located in the training data set (e.g. such\nas the features resulting from mRMR in our setting). A deep neural net can be seen as a typical\nnetwork in which the input ﬂows from the input layer to the output layer through a number of hidden\nlayers. An illustration of the deep neural network for our random utility learning is depicted in\nFigure 3a.\n10\nOur proposed deep neural network model for random utility learning includes exponential linear\nunits (ELUs) (Goodfellow et al., 2016) at each hidden layer. The usage of exponential linear units\n(Goodfellow et al., 2016) normally adds additional hyper-parameter in the search space as a trade-off\nfor signiﬁcant increase in ﬁtting accuracy due to enormous decrements of \"dead\" units, a classical\nproblem of rectiﬁed linear unit (ReLU) implementations (Clevert et al., 2015). The output layer is\nmodeled using sigmoid units for classifying agents discrete choices. The proposed model is optimized\nby minimizing the cross-entropy cost function using stochastic gradient descent combined with a\nnesterov optimization scheme. The initialization of the weights utilizes He normalization (He et al.,\n2015), which gives increased performance and better training results. Unlike a random initialization,\nHe initialization avoids local minimum points and makes convergence signiﬁcantly faster. Batch\nNormalization (Ioffe & Szegedy, 2015) has also been adopted in our deep neural network framework\nto improve the training efﬁciency and to address the vanishing\/exploding gradient problems in the\ntraining of deep neural networks. By using batch normalization, we avoid drastic changes in the\ndistribution of each layers inputs during training while the deep network parameters of the previous\nlayers keep changing. Knowing that adding more capacity in our deep neural network model will\npotentially lead to over-ﬁtting, we apply dropout technique (Srivastava et al., 2014) as a regularization\nstep. The dropout technique involves the following procedure in the training phase (both in forward\nand backward graph learning traversal steps): each neuron, excluding the output neurons, has a\nprobability to be totally ignored. The probability to ignore a neuron is another hyper-parameter of the\nalgorithm and normally gets values between 50% - 70%.\n(a) Architecture of proposed\ndeep neural network\nLSTM \nForward Cell\nLSTM \nForward Cell\nLSTM \nForward Cell\nLSTM \nBackward Cell\nLSTM \nBackward Cell\nLSTM \nBackward Cell\n.\n.\n.\n.\n.\n.\nHidden  Layers\nOutput\nAction\nt = 1\nOutput\nt = 2\nt = N\nShifted \nSequence\nShifted \nSequence\nShifted \nSequence\nt = 1\nt = 2\nt = N\nBatch \nsize\n(b) Architecture of deep bi- direc-\ntional recurrent neural networks\nConventional Variational Auto-encoder\nRecurrent Based Variational Auto-encoder\n(c) Conventional and recurrent based auto-\nencoder\nFigure 3: Proposed deep neural networks, deep bi-directional recurrent neural network, and auto-\nencoders\n4.2.2\nDeep Bi-directional Recurrent Neural Networks for Sequential Decision-Making\nOne of the basic drawbacks of both benchmark random utility learning models as well as the\nproposed deep neural networks is that they have strong assumptions for the data generation process.\nAn important challenge for efﬁcient learning of sequential decision-making models is the actual\nmodeling of the dependence of future actions of an agent with the present and previous actions. In\nparticular, an agent naturally tries to co-optimize around a set of discrete choices and gains the higher\nutility (equation 4). Both benchmark models and deep neural networks adopt the assumption of\nindependent and identically distributed data points. One way to model the underlying time series\ndependencies is through efﬁcient feature engineering and by potentially using a novel feature selection\nalgorithm. In Section 3.5, we use domain knowledge along with a pooling & picking method to\ncreate a feature set that can accurately predict agents’ behavior. However, this step helps sparingly in\nthe presence of time series dependencies and cannot generalize.\nLeveraging the latest Deep Learning models, like recurrent neural networks, we address the issue of\ntime dependence by looking at temporal dependencies within the data. Recurrent neural networks\nhave the capability to allow information to persist, even over long periods, by simply inserting loops\nthat point to them. Lately, recurrent neural networks have been implemented with huge success\nin energy and automotive sectors. Speciﬁcally, recurrent neural networks can be applied to energy\nrelated ﬁelds such as wind energy conversion systems (Zhao et al., 2016; Shi et al., 2018) and solar\nradiation prediction (Gensler et al., 2016; Al-Sbou & Alawasa, 2017). In the automotive sector,\nrecurrent neural networks are used for anticipating driver activity (Jain et al., 2016) in addition to\nautonomous driving testing (Tian et al., 2018).\n11\nAs we see in the architecture of a deep bi-directional recurrent neural network in Figure 3b, informa-\ntion passes from one time step of the network to the next. The information of the network passes\nto successor nodes. In the case of a bi-directional recurrent neural network, information can also\nﬂow in the opposite direction to the predecessor. In a simple implementation, however, recurrent\nneural networks tend to either vanish or become incapable of modeling long-term dependencies. In\nour proposed novel sequential utility learning model, we enable an end-to-end training using Long\nShort Term Memory cells (LSTM). Mainly, LSTM includes several gates that decide how long and\nshort-term relations should be modeled. The overall output of the LSTM cell is a combination of\nsub-gates describing the dependencies (Goodfellow et al., 2016).\nOur deep bi-directional architecture is described in Figure 3b. Given an agent’s actions, we deﬁne\na time step N (sliding window of actions), which is a hyper-parameter and models time series\ndependencies in agent actions. Each training instance in the network is a tensor with the following\ndimensions: mini-batch size (using a multiple of time step N), target sequence, and all given features.\nOur deep network is designed to leverage sequential data and build several layers and time steps of\nhidden memory cells (LSTMs). Moreover, we propagate the unrolled deep network both forward and\nbackward (bi-directional recurrent neural network) for modeling the exact series time-lagged features\nfor agents actions. For the proposed deep bi-directional recurrent neural network, we use three hidden\nlayers. To perform classiﬁcation for the agent actions, we add a fully connected layer. The fully\nconnected layer is in turn connected to the output of the ending time-step propagating network, which\nis ﬁnally followed by a soft-max layer, which is what actually performs the classiﬁcation task. Similar\nto the deep neural network model, we optimize by minimizing the cross-entropy cost function using\nstochastic gradient descent combined with a Nesterov optimization scheme. Additionally, we employ\nan exponentially decaying learning rate as our learning rate schedule. Again, the initialization of the\nweights utilizes He normalization (He et al., 2015), which gives increased performance and better\ntraining results. To avoid over-ﬁtting with the enormous capacity of the proposed deep network, we\napply dropout as the core regularization step.\n4.2.3\nDeep Learning for Generative Sequential Decision Models\nOn top of the existing data set resulting from our experiment, we can create even larger data sets\nbased on the existing ones. The idea of bootstrapping (Hastie et al., 2009) is widely applied both in\nstatistics and machine learning in many applications to help with the creation of new data sets that\nmimic the original data. However, bootstrapping is not a scalable solution. As data sets become larger\nand larger, the computational complexity restricts the capabilities of the system. In our approach, we\npropose the use of deep variational auto-encoders (Kingma & Welling, 2014) as an approach to create\na nonlinear manifold (encoder) that can be used as a generative model. Variational auto-encoders\nformalize the necessary generative model in the framework of probabilistic graphical models by\nmaximizing a lower bound on the log-likelihood of the given high-dimensional data. Furthermore,\nvariational auto-encoders can ﬁt large high-dimensional data sets (like our social game dataset) and\ntrain a deep model to generate data resembling the original data set. In a sense, generative models\nautomate the natural features of a data set and then provide a scalable way to reproduce known data.\nThis capability can be employed either in the utility learning framework for boosting estimation or\nas a general way to create simulations mimicking occupant behavior\/preferences in software like\nEnergyPlus 8.\nUsing such a Deep Learning model, we can generate data by simply sampling a latent vector from\nthe latent space of the auto-encoder and decoding via the decoder component. In Figure 3c, we\nprovide the overall architecture behind training a variational auto-encoder. We use two hidden layers\nin encoder and decoder while tying parameters between them. Also, the latent space is modeled\nusing a standard gaussian distribution. By using this architecture of deep auto-encoder, however,\nwe limit the generative model in applications in which the data process has a natural time-series\ndependence. Hence, we proposed the implementation of a recurrent variational auto-encoder (Gregor\net al., 2015). In its architecture shown in Figure 3c, the proposed recurrent variational auto-encoder\nallows time-series modeling for progressive reﬁnement and spatial attention in the shifted tensor\ninputs. Using progressive reﬁnement, the deep network simply breaks up the joint distribution over\nand over again in several steps resulting in a chain of latent variables. This gives the capability\nto sequentially output the time-series data rather than compute them in a single shot. Moreover, a\n8https:\/\/energyplus.net\n12\nrecurrent variational auto-encoder can potentially improve the generative process over the spatial\ndomain. By adding time series in the model as tensors with shifted data points, we can reduce the\nburden of complexity by implementing improvements over small regions of the tensor input at a time\ninstance (spatial attention).\nWith these mechanisms, we achieve reduction of the complexity burden that an auto-encoder has to\novercome. As a result, using a recurrent based variational auto-encoder allows for more generative\ncapabilities that can handle larger, more complex distributions such as those in the given social game\ntime series data. Our models were tested in several sets of data from individual occupants and were\ncapable of randomly generating new data with signiﬁcant similarities to the training data. This result\nprovides a powerful tool for generating new data on top of the existing data and provides more\nﬂexibility in the application of the data in several real scenario mechanisms like demand response.\n4.3\nExplainability Analysis using Graphical Lasso\nWith the growth of artiﬁcial intelligence, there has been a pressing need for models and datasets to\nhave explainable nature. For our proposed social game data, we ensure it has explainability, i.e. the\ndata correctly encodes the human decision making towards energy usage in competetive environments.\nWe divide the players into three classes in a supervised way taking the ranks of the users as the label.\nLet the classes be represented by CHigh, CMedium and CLow, where the superscripts signify the\nenergy efﬁciency behavior of each class. The behavior of a player in a particular class represents the\ncharacteristic behavior of players, e.g. CHigh represents the behavior of players showcasing high\nenergy efﬁciency. We then study the feature correlations in different classes using graphical lasso\nalgorithm. We formulate a framework that allows us to understand users decision making model.\nSpeciﬁcally, we adopt graphical lasso algorithm Friedman et al. (2008); Hastie et al. (2015) to study\nthe way in which features in a energy game-theoretic framework interplay among each other.\nLet the features representing the social game data be denoted by the collection Y = (Y1, Y2, · · · , YH).\nFrom a graphical perspective, Y can be associated with the vertex set V = {1, 2, · · · , H} of some\nunderlying graph. The structure of the graph is utilized to derive inferences about the relationship\nbetween the features. We use the graphical lasso algorithm Hastie et al. (2015) to realize the\nunderlying graph structure, under the assumption that the distribution of the random variables is\nGaussian.\nConsider the random variable Yh at h ∈V . We use the Neighbourhood-Based Likelihood for\ngraphical representation of multivariate Gaussian random variables. Let the edge set of the graph be\ngiven by E ⊂V × V . The neighbourhood set of Yh is deﬁned by\nN(h) = {k ∈V |(k, h) ∈E}\n(5)\nand the collection of all other random variables be represented by:\nYV \\{h} = {Yk, k ∈(V −{h})}\n(6)\nFor undirected graphical models, node for Yh is conditionally independent of nodes not directly\nconnected to it given YN(h), i.e.\n(Yh|YV \\{h}) ∼(Yh|YN(h))\n(7)\nThe problem of constructing the inherent graph out of observations is nothing but ﬁnding the edge\nset for every node. This problem becomes predicting the value of Yh given YN(h), or equivalently,\npredicting the value of Yh given Y\\{h}by the conditional independence property. The conditional\ndistribution of Yh given Y\\{h} is also Gaussian, so the best predictor for Yh can be written as:\nYh = Y T\nV \\h.βh + WV \\h\n(8)\nwhere WV \\h is zero-mean gaussian prediction error. The βh terms dictate the edge set for node h in\nthe graph. We use l1-regularized likelihood methods for getting a sparse βh. Let the total number\nof data samples available be N. The optimization problem is formulated as: corresponding to each\nvertex h = 1, 2, · · · , H, solve the following lasso problem:\nˆ\nβh ∈argmin\nβh∈RH−1\n\u001a 1\n2N\nN\nX\nj=1\n(yjh −yT\nj,V \\hβh)2 + λ∥βh∥1\n\u001b\n(9)\nThe implementation of Graphical Lasso algorithm is summarized in Appendix.\n13\n5\nExperimental Results\nWe now present the results of the proposed utility learning method applied to high-dimensional data\ncollected from the social game experiment in the Fall 2017 and Spring 2018 semesters. As previously\ndescribed, our data set consists of the per-minute high-dimensional data of occupant energy usage\nacross several resources in their rooms. We evaluate the performance of utility learning under two\ncharacteristic scenarios. The ﬁrst scenario involves having full information from the installed IoT\nsensors for performing \"step-ahead\" predictions. In this scenario, IoT sensors continuously feed\ninformation from the previous actions of the occupants. For the second scenario, referred to as\n\"sensor-free\", we stop taking the IoT sensor readings into account in each room. In the second\ninstance, the aggregated past features of the occupants are missing. For this case, we have a model in\nwhich we only use features that we can acquire from external weather conditions (e.g. from a locally\ninstalled weather station), information about occupant engagement with the web portal, and seasonal\ndummy variables. All of these features are much easier to acquire without needing to keep the highly\naccurate but expensive IoT devices. The broader purpose of our proposed gamiﬁcation approach\nis the development of exceptional forecasting models representing dynamic occupant behavior. As\nillustrated in Figure 1a, the building level energy usage prediction is fed to upper level components\nof the smart grid at the provider or microgrid level. In a real application scenario, the proposed\nbuilding level modeling opens new avenues for demand response programs, which can incorporate\nreal-time predictions of building occupant energy patterns. The proposed game theoretic models\nand iterative incentive design mechanisms are powerful in the sense that they can simultaneously be\nused to predict but also to incentivize desirable human behavior. Adaptive incentive design motivates\nbuilding occupant energy efﬁciency through gamiﬁcation platforms while accurately predicting their\nenergy usage in order to feed it back to the higher provider levels of our framework.\nFrom our experiment, we present estimation results for the data set in both Fall and Spring versions\nof the experiment for two characteristic occupants. Both occupants have ample data for all of the\nrelevant resources being considered. We used the ﬁrst four game periods for the training of our\nmodels:\n• Fall: Sep. 12th, 2017 - Nov. 19th, 2017 (n = 100, 800); Nov. 20th, 2017 - Dec. 03rd, 2017\n(n = 20, 160).\n• Spring: Feb. 19th, 2018 - Apr. 22nd, 2018 (n = 90, 720); Apr. 23rd, 2018 - May 06th, 2018\n(n = 20, 160).\nBefore we trained our benchmark classiﬁers, we applied the mRMR algorithm to the total data set\n(data from all occupants) in the training period. This accounts for almost 4 million distinct data\npoints in the Fall semester data set and 2.5 million distinct data points in the Spring semester data\nset. Applying mRMR results in several top features in both the Fall and Spring semester data sets.\nInterestingly, mRMR included several external features in the top relevant feature candidates. In\nparticular, the presence of external humidity as an important feature for the ceiling fan is a good\ndemonstration of the mRMR algorithm’s capability to extract salient features. Moreover, features\nlike survey points illustrate that some occupants co-optimized their resource usage while also trying\nto view their point balance, usage, and ranking in the game (by visiting the web portal).\n5.1\nForecasting via Conventional Machine Learning & Deep Learning Frameworks\nWe have dual objectives in our leveraging of conventional machine learning and Deep Learning\nframeworks. Our ﬁrst objective is to achieve highly accurate forecasts of building occupant resource\nusage. Providers and retailers at the higher levels of Figure 1a can integrate energy usage forecasts\nin demand response programs. Our second objective is to improve building energy efﬁciency by\ncreating an adaptive model that learns how user preferences change over time and thus generate the\nappropriate incentives to ensure active participation. Furthermore, the learned preferences can be\nadjusted through incentive mechanisms Ratliff et al. (2014) to enact improved energy efﬁciency (seen\nin the building level of Figure 1a).\nFor learning optimal random utility models in the benchmark setting, we use the top twenty-ﬁve\nresulting features from the mRMR algorithm along with a pre-processing step of SMOTE with\nSVM initialization. Using SMOTE, we boost the accuracy of benchmark models due to the fact that\nour data set was heavily imbalanced. We achieve decent accuracy using well-trained conventional\n14\n(a) AUC Scores using Fall semester data\n(b) AUC Scores using Spring semester data\nFigure 4: Forecasting accuracy (\"step-ahead\" \/ \"sensor-free\" predictions) for ceiling fan usage\n(on\/off).\nmachine learning models. Area Under the Receiver Operating Characteristic Curve is our forecasting\nperformance metric. All of the classiﬁers achieve decent AUC scores in both the Fall and Spring\nsemester results, as shown in Table 1. In the “sensor-free”results, we have a signiﬁcant drop in\nthe achieved accuracy, but this is expected given that the IoT feed is decoupled. However, even in\n\"sensor-free\" examples we are able to predict occupant behavior using less representative features\nand having excluded the IoT sensors.\nFor the deep neural networks, we used training data resulting from the applied SMOTE step. We\nused two hidden layers of the feed-forward neural network, with 50% dropout and stochastic gradient\ndescent method leveraging nesterov’s Momentum to accelerate convergence. 20% of the training\ndata was used as a validation data set for hyper-parameter tuning.\nTo further exploit the continuity of the sequential decision-making model, we experiment on the\nbi-directional deep recurrent neural network. We used a time sliding window—time step of two hours\n(120 distinct data points). We processed the data without being pre-processed from the SMOTE\nalgorithm as we wanted to retain the underlying sequence of actions of the occupants (temporal\ndependencies of the data). We used three hidden layers with 60% dropout rate, and we applied an\nexponentially decaying learning rate (simulated annealing). In the training of bi-directional recurrent\nneural networks, we applied the principle of early stopping using a validation data set over the AUC\nmetric. For our deep bi-directional networks, thirty-ﬁve epochs were optimal to be trained. As in\ndeep neural networks, 20% of the training data was used as validation set for hyper-parameter tuning.\nTable 1: AUC scores for “step-ahead”\/“sensor-free”predictions.\nFall Semester\nSpring Semester\n“step-ahead”\/ “sensor-free”\nCeiling Fan\nCeiling Light\nDesk Light\nCeiling Fan\nA\/C\nCeiling Light\nDesk Light\nLogistic regression\n0.83 \/ 0.65\n0.78 \/ 0.61\n0.78 \/ 0.68\n0.71 \/ 0.55\n0.76 \/ 0.73\n0.75 \/ 0.55\n0.76 \/ 0.50\nPenalized l1 Logistic regression\n0.80 \/ 0.65\n0.77 \/ 0.56\n0.78 \/ 0.64\n0.71 \/ 0.55\n0.76 \/ 0.70\n0.75 \/ 0.55\n0.76 \/ 0.50\nBagged Logistic regression\n0.84 \/ 0.66\n0.80 \/ 0.59\n0.79 \/ 0.68\n0.73 \/ 0.54\n0.73 \/ 0.73\n0.76 \/ 0.54\n0.79 \/ 0.51\nLDA\n0.81 \/ 0.65\n0.78 \/ 0.58\n0.74 \/ 0.68\n0.70 \/ 0.55\n0.73 \/ 0.73\n0.75 \/ 0.55\n0.70 \/ 0.51\nK-NN\n0.76 \/ 0.53\n0.77 \/ 0.56\n0.74 \/ 0.55\n0.70 \/ 0.50\n0.76 \/ 0.57\n0.68 \/ 0.54\n0.73 \/ 0.57\nSupport Vector Machine\n0.82 \/ 0.65\n0.78 \/ 0.60\n0.76 \/ 0.68\n0.70 \/ 0.55\n0.75 \/ 0.73\n0.75 \/ 0.55\n0.70 \/ 0.50\nRandom Forest\n0.91 \/ 0.60\n0.78 \/ 0.59\n0.98 \/ 0.63\n0.83 \/ 0.58\n0.83 \/ 0.65\n0.99 \/ 0.54\n0.96 \/ 0.50\nDeep Neural Network\n0.80 \/ 0.55\n0.76 \/ 0.60\n0.78 \/ 0.64\n0.74 \/ 0.56\n0.78 \/ 0.68\n0.77 \/ 0.54\n0.84 \/ 0.50\nDeep Bi-directional RNN\n0.97 \/ 0.71\n0.85 \/ 0.66\n0.99 \/ 0.76\n0.91 \/ 0.66\n0.89 \/ 0.80\n0.99 \/ 0.64\n0.99 \/ 0.62\nTo evaluate the effectiveness of our proposed deep learning framework, we present the AUC scores\nof a representative example for comparison. From the results, it is clear that deep RNN outperforms\nthe majority of alternative algorithms. One important remark is that deep RNN exceeds even the\nrandom forest algorithm, which is considered a top-performing, robust classiﬁcation model. Deep\nNN also achieved better performance in some examples over the Random Forest classiﬁer, but\nthis is not a general case. Figure 4 introduces bar charts representing AUC scores for ceiling fan\nusage (on\/off). Prediction results are divided into AUC scores for the two scenarios discussed\n15\nTable 2: Feature comparison between proposed generative models (auto-encoders) using DTW score.\nTime Series Feature\nConventional Auto-encoder\nRNN-based Auto-encoder\np-values\nCeiling Fan Status (On \/ Off)\n1.5e+04\n1.2e+04\n0.11\nCeiling Light Status (On \/ Off)\n1.6e+04\n2.2e+03\n1.0\nDesk Light Status (On \/ Off)\n6.7e+03\n0.0e+00\n1.0\nDorm Room Temperature\n1.3e+05\n1.2e+05\n0.0\nDorm Room Humidity\n4.8e+05\n3.7e+05\n0.0\nExternal Temperature\n1.0e+05\n1.8e+05\n0.0\nExternal Humidity\n2.9e+05\n4.3e+05\n0.0\npreviously, “step-ahead”and “sensor-free”. Upon examination of these results, it is clear that deep\nRNN outperforms all other deep learning and machine learning models. Figure 4 demonstrates that\ndeep bi-directional RNN based models achieve accuracy almost equal to one. For more results, please\nrefer to Konstantakopoulos et al. (2018a).\n5.2\nGenerative Modeling via Sequential Deep Auto-Encoders\nIn Table 2, we present the results of two trained generative models using the full data set of a\nrandomly selected occupant in the Fall semester. We trained both a conventional auto-encoder and a\nrecurrent based auto-encoder. The resulting deep generative models can be used as a way to create\nsimulations for mimicking building occupant behavior and preferences. This is an extra tool for\nquantifying variations in building occupant behavior as dynamic parts of a microgrid. Generative\nmodels are capable of adapting to variations of the external weather conditions, which in turn creates\nan interesting view of building occupant energy usage patterns aligned with external weather patterns.\nIn Table 2, we present results for several selected features from the interior of a dorm room and\nfrom external weather data. For evaluating artiﬁcially generated time-series from the proposed\nauto-encoders, we utilize dynamic time warping (DTW), which measures the similarity between\ntwo temporal sequences, the ground truth and the artiﬁcial data (Rakthanmanon et al., 2012). In\nbold, we see that the recurrent based auto-encoder achieves a smaller DTW score in most of the\nfeatures, leading to a generative model that isn’t mimicking exactly or is deviating signiﬁcantly from\nthe original data set.\nIn efforts to evaluate the statistical signiﬁcance of the calculated DTW scores from the recurrent\nbased auto-encoder, we used a permutation hypothesis test. In this approach, we permute original\nand generated time-series data, and we compute their DTW score looking for events that are more\n\"extreme\" than the one that is presented in Table 2. Interestingly, we have inside and outside weather\nbased features (temperature and humidity) that have zero p-values, showing that the DTW scores\nusing a recurrent based auto-encoder are statistically signiﬁcant. For indoor device status features,\nhowever, p-values are large, indicating DTW score has high variability under the permutation test.\n5.3\nExplainability Analysis\n5.3.1\nFeature correlation learning using graphical lasso\nWe learn the feature correlations using graphical lasso algorithm in classes CHigh, CMedium and\nCLow to obtain the knowledge about factors governing human decision making towards various\n(high\/medium\/low) energy efﬁcient behaviors. We consider a representative player (selected as the\nplayer holding the median rank in the class) for each of the three classes to run graphical lasso\nand study the correlation between the features for that class. We group the features into different\ncategories so as to study their inﬂuence on energy efﬁciency behaviors. Speciﬁcally, we consider\nTemporal features like time of the day, academic schedules and weekday\/weekends, External features\nas outdoor temperature, humidity, rain rate etc. and Game Engagement features like frequency of\nvisits to game web portal.\nThe feature correlations for a low energy efﬁcient player is given in Fig 5. The player tries to use\neach resource independently which can be observed in Figure 5(a) with no correlation between the\ncorresponding resource usage identiﬁers. There is a positive correlation between morning and desk\nlight usage indicating heedless behavior towards energy savings. The absolute energy savings\n16\nPositive\nCorrelation\n+\n-\nNegative\nCorrelation\n0\nNo Correlation (correlation value not \ncomparable with + or – correlation values)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nMorning\nAfternoon\nEvening\nBreak\nMidterm\nFinal\nWeekday\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n+\n-\n-\n-\n-\n-\n-\n-\n+ + +\n-\n-\n+ +\n+\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTemporal Dependencies\n(a)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nSolar\nRadiation\nRain Rate\nTemperature\nHumidity\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n+\n-\n-\n-\n-\n-\n+\n-\n-\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nExternal Factor Dependencies\n(b)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nGame Portal\nVisit Frequency\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n+\n+\n+\n-\n-\n-\n-\n+\n+\n-\n+\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nGame Engagement Dependencies\n(c)\nFigure 5: Feature correlations for a Low Energy Efﬁcient Player (∈CLow)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nMorning\nAfternoon\nEvening\nBreak\nMidterm\nFinal\nWeekday\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n- +\n-\n-\n-\n+\n- + - +\n-\n-\n-\n+ -\n+ -\n-\n-\n-\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTemporal Dependencies\n(a)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nSolar\nRadiation\nRain Rate\nTemperature\nHumidity\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n-\n+\n-\n+\n-\n+\n-\n-\n-\n+\n-\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nExternal Factor Dependencies\n(b)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nGame Portal\nVisit Frequency\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n-\n+\n-\n+\n-\n+\n-\n-\n-\n+\n-\n+\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nGame Engagement Dependencies\n(c)\nFigure 6: Feature correlations for a Medium Energy Efﬁcient Player (∈CMedium)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nMorning\nAfternoon\nEvening\nBreak\nMidterm\nFinal\nWeekday\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n- +\n-\n+\n-\n+ -\n-\n-\n-\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTemporal Dependencies\n(a)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nSolar\nRadiation\nRain Rate\nTemperature\nHumidity\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n-\n+\n-\n+\n-\n-\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nExternal Factor Dependencies\n(b)\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\nGame Portal\nVisit Frequency\nCeiling Light\nStatus\nDesk Light\nStatus\nFan Status\nTotal Points\nRank\n-\n+\n-\n+\n-\n-\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nGame Engagement Dependencies\n(c)\nFigure 7: Feature correlations for a High Energy Efﬁcient Player (∈CHigh)\nincrease during the breaks and ﬁnals, given by positive correlation with total points, but it is not\nsigniﬁcant as compared to other players during the same period, thus increasing the rank. External\nparameters play a signiﬁcant role in energy usage behavior of this class. The operation of the ceiling\nfan is driven by external humidity as given in Figure 5(b). Figure 5(c) indicates that their frequency\nof visits to the game web portal is motivated by sub-optimal performance in the game.\nFeature correlations for a medium energy efﬁcient player is given in Fig 6. The player showcases\npredictable behaviors with correlations between desk light, ceiling light and ceiling fan usage\n(Figure 6(a)). The player co-optimizes the usage by alternating the use of ceiling and desk light.\nDifferent occasions like break, midterm and ﬁnal are marked by energy saving patterns. Unlike a low\nenergy efﬁcient player, the player in this class tries to save energy in a conscious manner shown by\nreduced fan usage during the morning and reduced light usage during the afternoon. The fan usage is\ninﬂuenced by the external humidity, shown by Fig 6(b). The game engagement patterns for a player\nin this class (Fig 6(c)) is similar to that of the low energy efﬁcient class.\n17\nTest whether X causes Y\nFan ⇒Ceiling Light\nHumidity ⇒Fan\nDesk Light ⇒Fan\nCeiling Light ⇒Desk Light\nMorning ⇒Desk Light\nAfternoon ⇒Fan\nEvening ⇒Ceiling Light\nPlayer type\np-value\nF-statistic\np-value\nF-statistic\np-value\nF-statistic\np-value\nF-statistic\np-value\nF-statistic\np-value\nF-statistic\np-value\nF-statistic\nLow Energy Efﬁcient\n0.54\n0.37\n0.004\n8.12\n0.06\n3.55\n0.81\n0.06\n0.4\n0.71\n0.01\n6.1\n0\n25.3\nMedium Energy Efﬁcient\n0\n21.2\n0.008\n7.06\n0\n113.6\n0\n25.8\n0.23\n1.41\n0.46\n0.55\n0.0007\n11.5\nHigh Energy Efﬁcient\n0\n21.9\n0.12\n2.36\n0.99\n0.003\n0.93\n0.007\n0.63\n0.22\n0.04\n4.2\n0.52\n0.41\nTable 3: Causality test results among various potential causal relationships. In bold are the p-values\n(shaded in blue) in cases that Granger causality is established through F-statistic test between features.\np-values lower than 0.05 indicate strong causal relationship in 5% signiﬁcance level\nFig 7 shows the feature correlations for a high energy efﬁcient player. This player also exhibits\npredictable behavior. Opportunistically, this player saves energy during breaks and midterms as\nshown by negative correlation between the corresponding ﬂags and rank in Figure 7(a). Notice\nthat there exists a negative correlation between midterm ﬂag and total points, indicating decrease in\nabsolute amount of points. However, the points are still higher than the points by other players which\nmarks improvement in the rank. This behavior is completely opposite to what is exhibited by a player\nin low energy efﬁcient class. The player is neither affected by the time of the day, nor by the external\nfactors (Figure 7(b)) showing a dedicated effort to save energy. The game engagement behavior,\ngiven in Figure 7(c) is inconclusive, possibly due to dominance by other energy saving factors.\n5.3.2\nCausal Relationship between features\nTo enhance the explainable nature of our model, we studied the causal relationship between features\nusing Granger causality test. Granger causality is a statistical test used to determine causal relationship\nbetween two signals. If signal A granger-causes signal B, then past values of A can be used to predict\nB for future timesteps beyond what is available for B. The results for causal relationship study is\ngiven in Table 3. Under null hypothesis H0, X does not Granger-cause Y . So, a p-value lower than\n0.05 (5% signiﬁcance level) indicates a strong causal relationship between the tested features and\nimplies rejecting the null hypothesis H0.\nThe p-values (shaded in blue) for which Granger causality is established are highlighted in the table.\nInterestingly, for medium and high energy efﬁcient building occupants, ceiling fan usage causes\nceiling light usage. This in fact conﬁrms the predictive behavior for them as mentioned earlier. In\nboth low and medium energy efﬁcient building occupants, external humidity causes ceiling fan usage.\nThis is an indicator that their energy usage is affected by external weather conditions. However, for\nhigh energy efﬁcient building occupants external humidity doesn’t cause ceiling fan usage. This\nshows that they are highly engaged with the proposed gamiﬁcation interface and try to minimize\ntheir energy usage. Another interesting result is that the evening label causes ceiling light usage for\nboth low and medium energy efﬁcient building occupants. But this is not the case for high energy\nefﬁcient building occupants, for whom ceiling light usage is better optimized as a result of their\nstrong engagement with the ongoing social game, leading to exhibition of better energy efﬁciency.\n5.4\nEnergy Savings Through Gamiﬁcation\n(a) A\/C daily average usage vs. baselines\n(b) Ceiling fan daily average usage vs. baselines\nFigure 8: Spring semester daily average minutes usage compared to weekday and weekend average\nbaselines. Vertical black dashed lines indicate a weekend period.\n18\nTable 4: Hypothesis testing for Fall & Spring Game (before vs. after) from minutes per day usage.\nFall Semester\nWeekday\nWeekend\nDevice\nBefore (Mean)\nAfter (Mean)\np-value\n∆%\nBefore (Mean)\nAfter (Mean)\np-value\n∆%\nCeiling Light\n417.5\n393.9\n0.02\n5.6\n412.3\n257.5\n0\n37.6\nDesk Light\n402.2\n157.5\n0\n60.8\n517.6\n123.3\n0\n76.2\nCeiling Fan\n663.5\n537.6\n0\n19.0\n847.1\n407.0\n0\n51.9\nSpring Semester\nWeekday\nWeekend\nDevice\nBefore (Mean)\nAfter (Mean)\np-value\n∆%\nBefore (Mean)\nAfter (Mean)\np-value\n∆%\nCeiling Light\n452.0\n314.2\n0\n30.5\n426.0\n195.6\n0\n54.1\nDesk Light\n430.1\n104.6\n0\n75.7\n509.4\n81.5\n0\n84\nCeiling Fan\n777.4\n541.6\n0\n30.3\n847.1\n331.8\n0\n60.8\nAir Con\n469.8\n225.8\n0\n51.9\n412.3\n81.8\n0\n80.2\nIn this section, we present the resulting energy savings in both Fall and Spring semester versions of\nthe social game. Our gamiﬁcation framework introduces occupants to a friendly non-cooperative\ngame and motivates reductions in their energy usage. Through the deployed IoT sensors and custom\nweb portal, each individual building occupant received live feedback about their room’s usage and\ntheir energy efﬁciency throughout the day. In Figure 8, we present the average daily usage in minutes\ncompared to average weekday and weekend baselines. The vertical black dashed lines indicate a\nweekend period, which has a different corresponding average baseline target for the occupants. In\nterms of energy usage and savings, A\/C and ceiling fan resources demonstrate an impressive reduction\nin usage, especially during weekends.\nFor quantifying the results, we employ hypothesis testing (A\/B testing) using dorm occupant usage\ndata before and after the beginning of the experiment. In Table 4, we see the hypothesis testing\nvalues for the different devices in both iterations of the experiment (Fall and Spring). In this table,\nthe \"before\" column denotes the data points gathered from before the game was ofﬁcially started,\nwhile \"after\" is during the game period. Data points in the tables are bucketed in both weekday and\nweekend data and represent the average usage of all of the occupants. Usage is deﬁned in minutes\nper day. In all of the devices, we have a signiﬁcant drop in usage between the two periods. Drop in\nusage is given in the column named ∆%, and indicates reduction in the average usage of all of the\nparticipating occupants. The p-values resulting from the 2-sample t-tests show that the change in\nusage patterns is signiﬁcant. Furthermore, we can see a much larger drop in usage is achieved over\nthe weekends. These results are signiﬁcant in that they demonstrate the capacity of our methods to\noptimally incentivize occupants in residential buildings to make their energy usage more efﬁcient.\n6\nConclusion\nThis study presents a general framework for utility learning in sequential decision-making models.\nWe leveraged several Deep Learning architectures and proposed a novel sequential Deep Learning\nclassiﬁer model. We also introduced a framework that serves as a basis for creating generative models,\nwhich are ideal for modeling and simulating human-building interaction toward improving energy\nefﬁciency. To demonstrate the utility learning methods, we applied them to data collected from a\nsmart building social game where the goal was to have occupants optimize their room’s resources. We\nwere able to estimate several agent proﬁles and signiﬁcantly reduce the forecasting error compared\nto all benchmark models. The deep sequential utility learning framework outperformed all other\nmodels being considered, and it improved prediction accuracy to an extraordinary degree in speciﬁc\nexamples. We also performed explainability analysis using graphical lasso and granger causality to\nconﬁrm the data encoded knowledge on human decision making towards energy usage in competetive\nenvironments.\nThis last result shows that a Deep Learning architecture that handles a sequential data process has\nthe effect of improving the overall accuracy. In this application we apply these methods speciﬁcally\nto smart building social game data; however, it can generalize to other scenarios with the task of\ninverse modeling of competitive agents, and it provides a useful tool for many smart infrastructure\napplications where learning decision-making behavior is crucial. Under our gamiﬁcation application,\noccupants were highly motivated to drastically reduce their energy impact. This result is even\nmore signiﬁcant considering the fact that no effort was directed towards optimizing the incentive\ndesign for encouraging energy efﬁcient behavior. Hence, research in optimal incentive design\nmechanisms should be pursued in the context of this work. Recent research on segmentation analysis\nusing graphical lasso (Das et al., 2019) can further improve the intelligent incentive design process.\n19\nFurthermore, special attention should be given to the management of pricing and how it affects the\ndynamics between the smart building and utility provider for applications like demand response\nprograms. In general, we have demonstrated that our proposed framework can be used successfully\nfor the purpose of accurately forecasting energy usage. However, deep learning models require a\ncontinuous feed of data and are not particularly robust to missing data points. This poses a challenge\nto many real-world applications, especially in such cases where there might be missing data due to\nIoT sensors losing connection. Hence, we identify this as a limitation that should be addressed for\neffective implementation of deep learning models in energy game-theoretic framework. Despite these\nconstraints, we have shown that our implementation of a gamiﬁcation approach to human-building\ninteraction in smart infrastructure offers tremendous opportunities for improving energy efﬁciency\nand smart grid management.\n7\nAcknowledgement\nThe authors would like to thank Chris Hsu, the applications programmer at CREST laboratory,\nwho developed and deployed the web portal application as well as the social game data pipeline\narchitecture. Also, we want to thank Energy Research Institute (ERI@N) at Nanyang Technological\nUniversity. Geraldine Thoung, Patricia Alvina and Nilesh Y. Jadhav at ERI@N kindly supported and\nhelped during the social game experiment. This work was supported by the Republic of Singapore’s\nNational Research Foundation through a grant to the Berkeley Education Alliance for Research\nin Singapore for the Singapore–Berkeley Building Efﬁciency and Sustainability in the Tropics\n(SinBerBEST) Program. The work of I. C. Konstantakopoulos was supported by a scholarship of the\nAlexander S. Onassis Public Beneﬁt Foundation.\nReferences\nYazeed A Al-Sbou and Khaled M Alawasa. Nonlinear autoregressive recurrent neural network model\nfor solar radiation prediction. International Journal of Applied Engineering Research, 12(14):\n4518–4527, 2017.\nMerlinda Andoni, Valentin Robu, Wolf-Gerrit Früh, and David Flynn. Game-theoretic modeling\nof curtailment rules and network investments with distributed generation. Applied energy, 201:\n174–187, 2017.\nFabrizio Ascione, Nicola Bianco, Claudio De Stasio, Gerardo Maria Mauro, and Giuseppe Peter\nVanoli. A new comprehensive approach for cost-optimal building design integrated with the\nmulti-objective model predictive control of hvac systems. Sustainable Cities and Society, 31:\n136–150, 2017.\nGianni Bianchini, Marco Casini, Antonio Vicino, and Donato Zarrilli. Demand-response in building\nheating systems: A model predictive control approach. Applied Energy, 168:159–170, 2016.\nNitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic\nminority over-sampling technique. Journal of artiﬁcial intelligence research, 16:321–357, 2002.\nSong Zan Chiou-Wei, Ching-Fu Chen, and Zhen Zhu. Economic growth and energy consumption\nrevisited evidence from linear and nonlinear granger causality. Energy Economics, 30(6):3063–\n3076, 2008.\nDjork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\nlearning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\nHari Prasanna Das and Ashok Kumar Pradhan. Development of a micro-phasor measurement unit for\ndistribution system applications. In Power Systems Conference (NPSC), 2016 National, pp. 1–5.\nIEEE, 2016.\nHari Prasanna Das, Ioannis C. Konstantakopoulos, Aummul Baneen Manasawala, Tanya Veeravalli,\nHuihan Liu, and Costas J. Spanos. A novel graphical lasso based approach towards segmentation\nanalysis in energy game-theoretic frameworks, 2019.\n20\nVanessa De Luca and Roberta Castri. The social power game: A smart application for sharing\nenergy-saving behaviours in the city. FSEA 2014, 27, 2014.\nCheng Fan, Fu Xiao, and Yang Zhao. A short-term building cooling load prediction method using\ndeep learning algorithms. Applied energy, 195:222–233, 2017.\nJerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with\nthe graphical lasso. Biostatistics, 9(3):432–441, 2008.\nMonika Frontczak, Stefano Schiavon, John Goins, Edward Arens, Hui Zhang, and Pawel War-\ngocki. Quantitative relationships between occupant satisfaction and satisfaction aspects of indoor\nenvironmental quality and building design. Indoor air, 22(2):119–131, 2012.\nAndré Gensler, Janosch Henze, Bernhard Sick, and Nils Raabe. Deep learning for solar power\nforecasting—an approach using autoencoder and lstm neural networks. In Systems, Man, and\nCybernetics (SMC), 2016 IEEE International Conference on, pp. 002858–002865. IEEE, 2016.\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\nMIT press Cambridge, 2016.\nC.W.J. Granger. Testing for causality: A personal viewpoint. Journal of Economic Dynamics and\nControl, 2:329 – 352, 1980. ISSN 0165-1889. doi: https:\/\/doi.org\/10.1016\/0165-1889(80)90069-X.\nURL http:\/\/www.sciencedirect.com\/science\/article\/pii\/016518898090069X.\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A\nrecurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.\nDavid Hallac, Youngsuk Park, Stephen P. Boyd, and Jure Leskovec. Network inference via the\ntime-varying graphical lasso. CoRR, abs\/1703.01958, 2017. URL http:\/\/arxiv.org\/abs\/\n1703.01958.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning, Data\nMining, Interference, and Prediction. Springer, 2009.\nTrevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity: The\nLasso and Generalizations. Chapman & Hall\/CRC, 2015. ISBN 1498712169, 9781498712163.\nJianping He, Chengcheng Zhao, Lin Cai, Peng Cheng, and Ling Shi. Practical closed-loop dynamic\npricing in smart grid for supply and demand balancing. Automatica, 89:92–102, 2018.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international\nconference on computer vision, pp. 1026–1034, 2015.\nDavid A. Hensher and Lester W. Johnson. Applied discrete-choice modelling. Routledge, 2018.\nAmy Hillier, Tony Smith, and Carolyn C. Cannuscio. A discrete choice approach to modeling food\nstore access. Environment and Planning B: Urban Analytics and City Science, 42(2):263–278,\n2015.\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural computation, 18(7):1527–1554, 2006.\nYousef Al Horr, Mohammed Arif, Amit Kaushik, Ahmed Mazroei, and Martha Katafygiotou. Occu-\npant productivity and ofﬁce indoor environment quality: A review of the literature. Building and\nEnvironment, 105:369–389, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nAshesh Jain, Avi Singh, Hema S Koppula, Shane Soh, and Ashutosh Saxena. Recurrent neural net-\nworks for driver activity anticipation via sensory-fusion architecture. In Robotics and Automation\n(ICRA), 2016 IEEE International Conference on, pp. 3118–3125. IEEE, 2016.\n21\nRuoxi Jia, Roy Dong, S Shankar Sastry, and Costas J Sapnos. Privacy-enhanced architecture\nfor occupancy-based hvac control. In Cyber-Physical Systems (ICCPS), 2017 ACM\/IEEE 8th\nInternational Conference on, pp. 177–186. IEEE, 2017.\nRuoxi Jia, Baihong Jin, Ming Jin, Yuxun Zhou, Ioannis C Konstantakopoulos, Han Zou, Joyce\nKim, Dan Li, Weixi Gu, Reza Arghandeh, et al. Design automation for smart building systems.\nProceedings of the IEEE, 106(9):1680–1699, 2018a.\nRuoxi Jia, Ioannis C Konstantakopoulos, Bo Li, and Costas Spanos. Poisoning attacks on data-driven\nutility learning in games. In 2018 Annual American Control Conference (ACC), pp. 5774–5780.\nIEEE, 2018b.\nM. Jin, R. Jia, H.P. Das, W. Feng, and C. Spanos. Biscuit: Building intelligent system customer\ninvestment tools. In Proc. 10th International Conference on Applied Energy (ICAE), 2018a.\nMing Jin, Wei Feng, Ping Liu, Chris Marnay, and Costas Spanos. Mod-dr: Microgrid optimal\ndispatch with demand response. Applied Energy, 187:758–776, 2017.\nMing Jin, Wei Feng, Chris Marnay, and Costas Spanos. Microgrid to enable optimal distributed\nenergy retail and end-user demand response. Applied Energy, 210:1321–1335, 2018b.\nMing Jin, Shichao Liu, Stefano Schiavon, and Costas Spanos. Automated mobile sensing: Towards\nhigh-granularity agile indoor environmental quality monitoring. Building and Environment, 127:\n268–276, 2018c.\nCaroline Karmann, Stefano Schiavon, and Edward Arens. Percentage of commercial buildings\nshowing at least 80% occupant satisﬁed with their thermal comfort. Proceedings of the Windsor\nConference, 2018.\nMaria Karmargianni, Moshe Ben-Akiva, and Amalia Polydoropoulou. Incorporating social interaction\ninto hybrid choice models. Transportation, 41(6):1263–1285, 2014.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:10, 2014.\nIoannis C Konstantakopoulos, Lillian J Ratliff, Ming Jin, Costas Spanos, and S Shankar Sastry. Smart\nbuilding energy efﬁciency via social game: A robust utility learning framework for closing–the–\nloop. In 2016 1st International Workshop on Science of Smart City Operations and Platforms\nEngineering (SCOPE) in partnership with Global City Teams Challenge (GCTC)(SCOPE-GCTC),\npp. 1–6. IEEE, 2016a.\nIoannis C Konstantakopoulos, Lillian J Ratliff, Ming Jin, Costas J Spanos, and S Shankar Sastry.\nInverse modeling of non-cooperative agents via mixture of utilities. In Decision and Control\n(CDC), 2016 IEEE 55th Conference on, pp. 6327–6334. IEEE, 2016b.\nIoannis C Konstantakopoulos, Lillian J Ratliff, Ming Jin, and Costas J Spanos. Leveraging correlations\nin utility learning. In American Control Conference (ACC), 2017, pp. 5249–5256. IEEE, 2017.\nIoannis C Konstantakopoulos, Andrew R Barkan, Shiying He, Tanya Veeravalli, Huihan Liu, and\nCostas Spanos. A deep learning and gamiﬁcation approach to energy conservation at nanyang\ntechnological university. arXiv preprint arXiv:1809.05142, 2018a.\nIoannis C Konstantakopoulos, Lillian J Ratliff, Ming Jin, S Shankar Sastry, and Costas J Spanos. A\nrobust utility learning framework via inverse optimization. IEEE Transactions on Control Systems\nTechnology, 26(3):954–970, 2018b.\nS. Li, K. Deng, and M. Zhou. Social incentive policies to engage commercial building occupants in\ndemand response. In IEEE Inter. Conf. Automation Science and Engineering, pp. 407–412, Aug\n2014. doi: 10.1109\/CoASE.2014.6899357.\nS. Liu, M. Jin, H. Das, C. Spanos, and S. Schiavon. Personal thermal comfort models based on\nphysiological parameters measured by wearable sensors. Proceedings of the Windsor Conference,\npp. 431–441, 2018.\n22\nShichao Liu, Stefano Schiavon, Hari Prasanna Das, Ming Jin, and Costas J. Spanos. Personal thermal\ncomfort models with wearable sensors. Building and Environment, 162:106281, 2019. ISSN 0360-\n1323. doi: https:\/\/doi.org\/10.1016\/j.buildenv.2019.106281. URL http:\/\/www.sciencedirect.\ncom\/science\/article\/pii\/S0360132319304913.\nSabita Maharjan, Quanyan Zhu, Yan Zhang, Stein Gjessing, and Tamer Basar. Dependable demand\nresponse management in the smart grid: A stackelberg game approach. IEEE Transactions on\nSmart Grid, 4(1):120–132, 2013.\nMatjaz Majnik and Zoran Bosnic. Roc analysis of classiﬁers in machine learning: A survey. Intelligent\nData Analysis, 17:531–558, 2013.\nMilos Manic, Kasun Amarasinghe, Juan J Rodriguez-Andina, and Craig Rieger. Intelligent buildings\nof the future: Cyberaware, deep learning powered, and human interacting. IEEE Industrial\nElectronics Magazine, 10(4):32–49, 2016.\nJ.L. Mathieu, M.E. Dyson, D.S. Callaway, and A. Rosenfeld. Using residential electric loads for fast\ndemand response: The potential resource and revenues, the costs, and policy recommendations. In\nACEEE Summer Study on Energy Efﬁciency in Buildings, 2012.\nJM McQuade. A system approach to high performance buildings. United Technologies Corporation,\nTech. Rep, 2009.\nDeepak Merugu, Balaji S Prabhakar, and NS Rama. An incentive mechanism for decongesting the\nroads: A pilot program in bangalore. In Proceedings of ACM NetEcon Workshop, 2009.\nElena Mocanu, Phuong H Nguyen, Madeleine Gibescu, and Wil L Kling. Deep learning for estimating\nbuilding energy consumption. Sustainable Energy, Grids and Networks, 6:91–99, 2016.\nZoltan Nagy, Fah Yik Yong, Mario Frei, and Arno Schlueter. Occupant centered lighting control for\ncomfort and energy efﬁcient building operation. Energy and Buildings, 94:100–108, 2015.\nBrian Orland, Nilam Ram, Dean Lang, Kevin Houser, Nate Kling, and Michael Coccia. Saving\nenergy in an ofﬁce environment: A serious game intervention. Energy and Buildings, 74:43–52,\n2014.\nHanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria\nof max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis\nand machine intelligence, 27(8):1226–1238, 2005.\nChristopher Pluntke and Balaji Prabhakar. Insinc: A platform for managing peak demand in public\ntransit. JOURNEYS, Land Transport Authority Academy of Singapore, pp. 31–39, 2013.\nHuanmei Qin, Jianqiang Gao, Hongzhi Guan, and Hongbo Chi. Estimating heterogeneity of car\ntravelers on mode shifting behavior based on discrete choice models. Transportation Planning and\nTechnology, 40(8):914–927, 2017.\nThanawin Rakthanmanon, Bilson Campana, Abdullah Mueen, Gustavo Batista, Brandon Westover,\nQiang Zhu, Jesin Zakaria, and Eamonn Keogh. Searching and mining trillions of time series\nsubsequences under dynamic time warping. In Proceedings of the 18th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, pp. 262–270, 2012.\nSarvapali D Ramchurn, Perukrishnen Vytelingum, Alex Rogers, and Nick Jennings. Agent-based\ncontrol for decentralised demand side management in the smart grid. In The 10th International\nConference on Autonomous Agents and Multiagent Systems-Volume 1, pp. 5–12. International\nFoundation for Autonomous Agents and Multiagent Systems, 2011.\nLillian J Ratliff, Ming Jin, Ioannis C Konstantakopoulos, Costas Spanos, and S Shankar Sastry. Social\ngame for building energy efﬁciency: Incentive design. In Communication, Control, and Computing\n(Allerton), 2014 52nd Annual Allerton Conference on, pp. 1011–1018. IEEE, 2014.\nVahid Salehi, Ahmed Mohamed, Ali Mazloomzadeh, and Osama A Mohammed. Laboratory-based\nsmart power system, part i: Design and system development. IEEE Transactions on Smart Grid, 3\n(3):1394–1404, 2012.\n23\nRicardo Salvador, Teresa Romão, and Pedro Centieiro. A gesture interface game for energy consump-\ntion awareness. In Proc. 9th Int. Conf. Advances in Computer Entertainment, pp. 352–367, 2012.\ndoi: 10.1007\/978-3-642-34292-9_25.\nT. Samad, E. Koch, and P. Stluka. Automated demand response for smart buildings and microgrids:\nThe state of the practice and research challenges. Proc. of the IEEE, 104(4):726–744, April 2016.\ndoi: 10.1109\/JPROC.2016.2520639.\nPedram Samadi, Hamed Mohsenian-Rad, Robert Schober, and Vincent WS Wong. Advanced demand\nside management for the future smart grid using mechanism design. IEEE Transactions on Smart\nGrid, 3(3):1170–1180, 2012.\nFarshid Shariatzadeh, Paras Mandal, and Anurag K. Srivastava. Demand response for sustainable\nenergy systems: A review, application and implementation strategy. Renewable and Sustainable\nEnergy Systems, 45:343–350, 2015.\nZhichao Shi, Hao Liang, and Venkata Dinavahi. Direct interval forecast of uncertain wind power\nbased on recurrent neural networks. IEEE Transactions on Sustainable Energy, 9(3):1177–1187,\n2018.\nJonathan Simon, Marco Jahn, and Amro Al-Akkad. Saving energy at work: The design of a pervasive\ngame for ofﬁce spaces. In Proc. 11th Int. Conf. Mobile and Ubiquitous Multimedia, 2012. doi:\n10.1145\/2406367.2406379.\nN. Souly and M. Shah. Scene labeling using sparse precision matrix. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 3650–3658, June 2016. doi: 10.1109\/\nCVPR.2016.397.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\nJan Sundell, Hal Levin, William W Nazaroff, William S Cain, William J Fisk, David T Grimsrud,\nF Gyntelberg, Y Li, AK Persily, AC Pickering, et al. Ventilation rates and health: multidisciplinary\nreview of the scientiﬁc literature. Indoor air, 21(3):191–204, 2011.\nYuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: Automated testing of deep-neural-\nnetwork-driven autonomous cars. In Proceedings of the 40th International Conference on Software\nEngineering, pp. 303–314. ACM, 2018.\nAmos Tversky and Daniel Kahneman. The framing of decisions and the psychology of choice.\nScience, 211(4481):453–458, 1981.\nJosé Vuelvas, Fredy Ruiz, and Giambattista Gruosso. Limiting gaming opportunities on incentive-\nbased demand response programs. Applied Energy, 225:668–681, 2018.\nWonyoung Yang, Hyeun Jun Moon, and Myung-Jun Kim. Combined effects of short-term noise\nexposure and hygrothermal conditions on indoor environmental perceptions. Indoor and Built\nEnvironment, 27(8):1119–1133, 2017.\nMengmeng Yu and Seung Ho Hong. Supply–demand balancing for power management in smart grid:\nA stackelberg game approach. Applied energy, 164:702–710, 2016.\nMengmeng Yu and Seung Ho Hong. Incentive-based demand response considering hierarchical\nelectricity market: A stackelberg game approach. Applied Energy, 203:267–279, 2017.\nNi Zhang, Yu Yan, and Wencong Su. A game-theoretic economic operation of residential distribution\nsystem with high participation of distributed electricity prosumers. Applied energy, 154:471–479,\n2015.\nYongning Zhao, Lin Ye, Zhi Li, Xuri Song, Yansheng Lang, and Jian Su. A novel bidirectional\nmechanism based on time series model for wind power forecasting. Applied energy, 177:793–803,\n2016.\n24\nLijing Zhu, Qi Zhang, Huihui Lu, Hailong Li, Yan Li, Benjamin McLellan, and Xunzhang Pan. Study\non crowdfunding’s promoting effect on the expansion of electric vehicle charging piles based on\ngame theory analysis. Applied energy, 196:238–248, 2017a.\nYingying Zhu, Minqi Yang, Ying Yao, Xiao Xiong, Xiaoran Li, and Ning Ma. Effects of illuminance\nand correlated color temperature on daytime cognitive performance, subjective mood, and alertness\nin healthy adults. Environment and Behavior, pp. 1–32, 2017b.\nH. Zou, Y. Zhou, J. Yang, H. Liu, H.P. Das, and C.J. Spanos. Consensus adversarial domain adaptation.\nIn AAAI Conference on Artiﬁcial Intelligence, 2019a.\nHan Zou, Hari Prasanna Das, Jianfei Yang, Yuxun Zhou, and Costas Spanos. Machine learning\nempowered occupancy sensing for smart buildings. 2019b.\nHan Zou, Jianfei Yang, Hari Prasanna Das, Huihan Liu, Yuxun Zhou, and Costas J Spanos. Wiﬁ\nand vision multimodal learning for accurate and robust device-free human activity recognition. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp.\n0–0, 2019c.\nY. Zuo, G. Yu, and H. W. Ressom. Integrating prior biological knowledge and graphical lasso for\nnetwork inference. In 2015 IEEE International Conference on Bioinformatics and Biomedicine\n(BIBM), pp. 1543–1547, Nov 2015. doi: 10.1109\/BIBM.2015.7359905.\n25\nAppendix\nA. Description of Graphical Lasso algorithm\nAlgorithm 1: GRAPHICAL LASSO ALGORITHM FOR GAUSSIAN GRAPHICAL MODELS\n1. For vertices h = 1, 2, · · · , H:\na. Calculate initial loss ∥Yh −Y T\nV \\hβh∥\n2\n2\nb. Untill Convergence:\ni. Calculate partial residual r(h) = Yh - Y T\nV \\hβh\nii. For all j ∈V \\h, Get βh,new\nj\n= Sλ\n\u0000 1\nN ⟨r(h), Yj⟩\n\u0001\niii. Compute new loss = ∥Yh −Y T\nV \\hβh,new∥\n2\n2\niv. Update βh = βh,new\nc. Get the neighbourhood set N(h) = supp(βh) for h\n2. Combine the neighbourhood estimates to form a graph estimate G = (V, E) of the random\nvariables.\nSλ(θ) is soft thresholding operator as sign(θ)(|θ| −λ)+.\nFor optimal design of penalty factor λ in Graphical Lasso run for a vertex s, we take 10 values\nin logarithmic scale between λmax and λmin as given below and conduct a line search to ﬁnd the\npenalty factor which brings the minimum loss.\nλmax = 1\nN max\nj∈V \\h|⟨Yj, Yh⟩|,\nλmin = λmax\n100\n(10)\nImplementing a coordinate descent approach Hastie et al. (2015), the time complexity of the proposed\nalgorithm is O(HN) for a complete run through all H features. We also do 5-fold cross validation\nto ensure accurate value of the coefﬁcients βh. Use of partial residuals for each node signiﬁcantly\nreduces the time complexity of the algorithm.\n26\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Design, Benchmarking and Explainability Analysis of a Game-Theoretic Framework towards Energy Efficiency in Smart Infrastructure.pdf"}
{"title":"Quantum Poker A game for quantum computers suitable for benchmarking error mitigation techniques on NISQ devices","authors":"Franz G. Fuchs, Vemund Falch, Christian Johnsen","summary":"Quantum computers are on the verge of becoming a commercially available\nreality. They represent a paradigm shift in computing, with a steep learning\ngradient. The creation of games is a way to ease the transition for beginners.\nWe present a game similar to the Poker variant Texas hold 'em with the\nintention to serve as an engaging pedagogical tool to learn the basics rules of\nquantum computing. The concepts of quantum states, quantum operations and\nmeasurement can be learned in a playful manner. The difference to the classical\nvariant is that the community cards are replaced by a quantum register that is\n\"randomly\" initialized, and the cards for each player are replaced by quantum\ngates, randomly drawn from a set of available gates. Each player can create a\nquantum circuit with their cards, with the aim to maximize the number of $1$'s\nthat are measured in the computational basis. The basic concepts of\nsuperposition, entanglement and quantum gates are employed. We provide a\nproof-of-concept implementation using Qiskit. A comparison of the results for\nthe created circuits using a simulator and IBM machines is conducted, showing\nthat error rates on contemporary quantum computers are still very high. For the\nsuccess of noisy intermediate scale quantum (NISQ) computers, improvements on\nthe error rates and error mitigation techniques are necessary, even for simple\ncircuits. We show that quantum error mitigation (QEM) techniques can be used to\nimprove expectation values of observables on real quantum devices.","url":"http:\/\/arxiv.org\/abs\/1908.00044v4","pdf_url":"http:\/\/arxiv.org\/pdf\/1908.00044v4","published":1564598488000,"comment":"accepted for publication at The European Physical Journal Plus","pdf_text":"Quantum Poker\nA game for quantum computers suitable for benchmarking error\nmitigation techniques on NISQ devices\nFranz G. Fuchs†, Vemund Falch‡, and Christian Johnsen‡\n†SINTEF AS, Department of Mathematics and Cybernetics, Oslo, Norway\n‡NTNU, Department of Physics, Trondheim, Norway\nMarch 24, 2020\nAbstract\nQuantum computers are on the verge of becoming a commercially available reality. They represent a paradigm\nshift in computing, with a steep learning gradient.\nThe creation of games is a way to ease the transition for\nbeginners.\nWe present a game similar to the Poker variant Texas hold ’em with the intention to serve as an\nengaging pedagogical tool to learn the basics rules of quantum computing.\nThe concepts of quantum states,\nquantum operations and measurement can be learned in a playful manner. The diﬀerence to the classical variant\nis that the community cards are replaced by a quantum register that is ”randomly” initialized, and the cards\nfor each player are replaced by quantum gates, randomly drawn from a set of available gates. Each player can\ncreate a quantum circuit with their cards, with the aim to maximize the number of 1’s that are measured in the\ncomputational basis. The basic concepts of superposition, entanglement and quantum gates are employed. We\nprovide a proof-of-concept implementation using Qiskit[1]. A comparison of the results for the created circuits\nusing a simulator and IBM machines is conducted, showing that error rates on contemporary quantum computers\nare still very high. For the success of noisy intermediate scale quantum (NISQ) computers, improvements on the\nerror rates and error mitigation techniques are necessary, even for simple circuits. We show that quantum error\nmitigation (QEM) techniques can be used to improve expectation values of observables on real quantum devices.\n1\nIntroduction\nQuantum computing is an emerging technology exploiting quantum mechanical phenomena – namely superposition,\nentanglement, and tunneling – in order to perform computation. Quantum computers have huge potential to transform\nsociety in a similar way that classical computers have, because they open up the possibility to tackle certain types\nof problems that are beyond the reach of classical computers. The ﬁrst commercially available quantum computers\nare expected within the next ﬁve years, and it is expected that quantum computers will outperform their classical\ncounterparts in some tasks within the same time period. In order to utilize the potential power of quantum computers,\none has to formulate a given problem in a form that is suitable for a quantum computer (encoding step) and develop\nspecialized algorithms.\nThese type of algorithms are fundamentally diﬀerent from classical algorithms.\nGetting\naccustomed to quantum algorithms has a considerable learning curve and requires a multidisciplinary approach.\nTypically, knowledge from physics, mathematics, computers science and a ﬁrm understanding from an application\narea such as quantum chemistry, optimization, or machine learning is required. In addition, it is advantageous to have\nknowledge of the underlying physics, particularly in the NISQ era.\nThe New York Times estimated in October 20181 that the global number of high-level researchers in quantum\ncomputing may be less than a thousand. The design of games that make use of the underlying rules of quantum\ncomputers is a way to attract more interest and ease the transition from classical algorithms to quantum algorithms\nfor beginners. In order to play the basic version of this game, knowledge of quantum physics or advanced mathematics\nis not required. Just as with any other game, one has to learn a set of rules, which are described in Section 3. Since\none of the best ways to learn is through play, this game can help to attract more people – from elementary school\nto post-graduate level at the university – to the ﬁeld of quantum computing and we hope that the game awakens\ncuriosity for present challenges in the ﬁeld, such as the design of quantum algorithms and error mitigation schemes.\nNear-term applications of early quantum devices, such as electronic structure problems and optimization, rely\non accurate estimates of expectation values to become of practical relevance. However, inherent noise in quantum\ndevices leads to wrong estimates of the expectation values of observables. Therefore, getting rid of (most of) the\nnoise inherent in quantum computing is a critical step toward making it useful for practical applications. Quantum\nerror correction (QEC) can only be achieved by increasing quantum resources (ancillary qubits). The ﬁrst scheme\nwas proposed by Shor (1995)[16] and many other schemes were proposed since then, e.g., the class called stabilizer\n1https:\/\/www.nytimes.com\/2018\/10\/21\/technology\/quantum-computing-jobs-immigration-visas.html\n1\narXiv:1908.00044v4  [quant-ph]  23 Mar 2020\ncodes, see Gottesman (1997)[4]. However, the number of ancillary qubits needed to achieve QEC depends intrinsically\non the error rates and is out of reach for NISQ devices. Quantum error mitigation (QEM), on the other hand can be\nachieved with additional classical resources only and is therefore applicable to NISQ devices.\nThe main contributions of this article are as follows.\n• We introduce a novel quantum game based on classic Poker. The game is useful to introduce basic quantum\ncomputing concepts to beginners.\n• We implement error mitigation schemes based on the extrapolation technique. A comparison of the results on\nsimulators and real quantum devices is provided. We show that errors can successfully be mitigated when one\nis interested in expectation values of an observable.\nThis article is organized as follows. After describing related work in Section 2 we present the description of our\ngame in Section 3. Using a representative circuit from an example game we report results of ideal and noisy circuit\nsampling in Section 4. Finally, we describe methods for error mitigation in Section 5 before concluding in Section 8.\n2\nRelated Work\nThere exist a number of games based on quantum physics and they can be categorized into the following two types.\nThe ﬁrst type attempt to illustrate quantum mechanical eﬀects, and one might therefore call them quantum mechanics\ngames. The second type illustrate quantum computing via qubits and quantum circuit building. It is the latter type of\ngame that has been developed in connection with this paper. For a recent review of the subject of quantum games we\nrefer to, e.g., [7]. Existing quantum computing games include ”Battleships with partial NOT gates”, solving puzzles\nby creating simple programs, and ”quantum chess”. Some are available as Jupyter Notebooks in Qiskit’s Github\nrepository of tutorials2.\nMitigating the eﬀect of noise on the execution of circuits is critical for the success of NISQ devices. The ideal\naction of a gate is given by a unitary operator U transforming a state |φ⟩into U |φ⟩. The are two basic types of\nnoise. Coherent noise means that a small perturbation eU of U is executed, where eU is still unitary and preserves the\npurity of the state |φ⟩. An example is a slight over-rotation. Incoherent noise does not preserve the purity of the\nstate. This type of noise comes from the (unwanted) interaction with the environment. In this case the evolution must\nbe described through density matrices and Kraus operators. An example of incoherent noise is amplitude damping\nmodeling relaxation from an excited state to the ground state. For a single qubit with decay probability p, the density\nmatrix ρ = |φ⟩⟨φ| is mapped to K0ρK†\n0 + K1ρK†\n1 with K0 =\n\u00121\n0\n0\n√1 −p\n\u0013\n, K1 =\n\u00120\n√p\n0\n0\n\u0013\n. Diﬀerent types of\ntechniques have been presented in the literature that can be used to mitigate the inﬂuence of noise on the ideal circuit.\nIn the following we discuss the most important ones.\nQuantum subspace expansion (QSE). The main idea is to use operators from a given set (such as the set of Pauli\noperators) to expand about the variational solution. A generalized eigenvalue problem using linear subspaces is solved.\nThis method leaves the circuit width (number of qubits) and depth (largest number of gates on any inputoutput path)\nunchanged. The method is introduced by Wecker et al. (2015)[19] where numerical evidence is provided. For the\nvariational quantum eigensolver numerical evidence for error mitigation is provided by McClean et al. (2017) in [10].\nAn extension of the approach is given in Colless et al. (2018)[2]. The complete energy spectrum of the H2 molecule is\ncalculated with near chemical accuracy. Premakumar and Joynt (2018)[13] generalize the concept of decoherence-free\nsubspace (DFS) to noise with correlations in space to avoid regions where decoherence measures are high. However,\nas the authors point out themselves, only dephasing noise is considered and it is unlikely that the method is useful for\ngeneral error models. DFSs do not exist for other types of noise, e.g., if noise ﬂips spins.\nProbabilistic error cancellation. The main idea is to represent the ideal circuit as a quasi-probabilistic mixture of\nnoisy ones. The circuit depth and width remain unchanged with this method. Temme et al. (2017)[18] present the\nmethod together with numerical evidence. Song et al. (2019)[17] demonstrate an error mitigation protocol based on\ngate set tomography and quasiprobability decomposition. One- and two-qubit circuits are tested on a superconducting\ndevice, and computation errors are successfully suppressed. Process tomography is not feasible for more than a few\nqubits since it scales exponentially with the number of qubits. In addition, process tomography is sensitive to noise\nin the pre- and post rotation gates plus the measurements (SPAM errors). Gate set tomography can take these errors\ninto account, but the scaling becomes even worse.\nExtrapolation techniques. The main idea is to amplify the noise deliberately in a controlled way. The information\nof the dependence of the expectation value on the noise level is used to extrapolate back to the zero noise level. The\ncircuit width remains unchanged, but the circuit depth is longer (or gate times are prolonged in case of phase control).\nTemme et al. (2017)[18] and Li and Benjamin (2017)[9] introduced the technique and provide numerical evidence.\nEndo et al. (2018)[3] extend the work of [18, 9] by accounting for the inevitable imperfections in the experimentalist’s\nmeasuring the eﬀect of errors in order to design eﬃcient QEM circuits. Kandala et al. (2019)[6] presents important\nconsiderations for hardware and algorithmic implementations of the zero-noise extrapolation technique, and demon-\nstrates tremendous improvements in the accuracy of variational eigensolvers implemented by a noisy superconducting\n2Qiskit tutorials - Quantum games https:\/\/github.com\/Qiskit\/qiskit-tutorials-community\/tree\/master\/games\n2\n(a) A screenshot from the game at the end of all betting rounds. Niels\n(green) is the dealer and Max (red) is revealing his cards. For the commu-\nnity cards the color cyan, blue, pink indicate a 0%, 50%, 100% probability\nto obtain a 1, respectively.\n”community cards”\n”Max’s cards”\nq0 : |0⟩\nX\nq1 : |0⟩\nX\nq2 : |0⟩\nX\nq3 : |0⟩\nH\nZ\nZ\nH\nq4 : |0⟩\nH\nZ\n(b) Circuit that Max creates by placing his X,\nZH and CX gates on the community cards. By doing\nso he changes the quantum state to obtain more 1’s\nwith a higher probability.\nX |0⟩= |1⟩\nH |0⟩= |+⟩\nZ |0⟩= |0⟩\nCX |00⟩= |00⟩\nCX |+0⟩= |00⟩+ |11⟩\nCX |++⟩= |++⟩\nX |1⟩= |0⟩\nH |1⟩= |−⟩\nZ |1⟩= |1⟩\nCX |01⟩= |01⟩\nCX |+1⟩= |01⟩+ |10⟩\nCX |+−⟩= |−−⟩\nX |+⟩= |+⟩\nH |+⟩= |0⟩\nZ |+⟩= |−⟩\nCX |10⟩= |11⟩\nCX |−0⟩= |00⟩−|11⟩\nCX |−+⟩= |−+⟩\nX |−⟩= |−⟩\nH |−⟩= |1⟩\nZ |−⟩= |+⟩\nCX |11⟩= |10⟩\nCX |−1⟩= |01⟩−|10⟩\nCX |−−⟩= |+−⟩\n(c) List of useful rules showing the eﬀect of gates on quantum states. The symbols X, H, Z, CX represent the available cards and |.⟩\nis the state of the quantum register they act on. Global phase and normalization have been ignored.\nFigure 1: Example of a quantum Poker game and the basic rules needed to play. The state of the quantum register\nbefore each player places their gates, is shown in the middle of (a). Max has one out of total four X, CX, and ZH gates\nand no H gate. (b) shows the underlying circuit generating the quantum register. Note that qubit q2 is entangled\nwith qubits q0 and q1, but the resulting state is shown to be |0⟩in (a), since the bit gets ﬂipped twice. In order to\nwin, Max needs to alter the state of the ”community cards” in such a way that the states of the qubits get ”closer to\n|1⟩. Looking at the list of operations shown in (c) Max applies his X-gate to change the state of q2 to be |1⟩. He then\napplies the ZH gate to qubit q3 to obtain the state |0⟩and ﬂips it to |1⟩by applying a CX with qubit q2, which is in\nstate |1⟩.\nquantum processor. Evidence on real quantum hardware is presented. In contrast to previous works, the increase\nof errors is not done by artiﬁcially introducing additional gates, but directly by pulse control. For IBM’s quantum\ncomputers, access at this level (pulse\/machine level) is only possible for customers of, which makes this technique\ninaccessible to us. Otten and Gray (2019)[12] introduce a technique that can be viewed as a multidimensional ex-\ntension. The approach corresponds to repeating the same quantum evolution many times with known variations on\nthe underlying systems’ error properties. They show that the eﬀective spontaneous emission, T1, and dephasing, T2,\ntimes can be increased using their method in both simulation and experiments on an actual quantum computer.\nIn this article we present results using the extrapolation method. We conduct experiments on simulators with error\nmodels and real quantum computers.\n3\nQuantum Poker Rules\nThe classical Texas hold’em round involves ﬁve community cards on the table shared by all the players, while each\nplayer holds 2 unique cards in their own hand. The small and big blind bets are placed at the start of each round,\nrelative to a rotating dealer position. The community cards are gradually revealed, the ﬁrst three - called ”the ﬂop” -\nare revealed simultaneously, and the last two called ”the turn” and ”the river” are revealed one at a time. The players\ntake turns betting at the start of each round. In order to stay in the game one has to match or increase the highest bet\ncurrently in play. Otherwise, one can fold and forfeit the chance to win the pot, i.e., the sum of money that players\nwager during a single hand or game. After all ﬁve cards have been revealed there is a ﬁnal round of betting before the\nplayers can choose to either show their hand in the hope of winning or folding it and forfeiting the pot. The goal is\nto combine up to ﬁve cards from both your own hand and the table to form the best Poker hand (hands are ordered\nby their probability and for equal probability by the ”highest” card) out of all the players. The player or players with\nthe best hand win(s) the pot.\nThe quantum Poker game considered here draws inspiration from Texas hold ’em Poker and shares its structure.\nThe betting rounds in the two games are identical. Community cards are replaced with qubits and the cards received\nby each player are replaced by quantum logic gates which can be applied to the qubits, i.e., community cards.\n• In quantum Poker each player acts on a personal ”copy” of the community cards, consisting of a quantum\nregister. Hence, the only interaction between the players is through betting. An example initial state is shown\n3\nin Figure 1. The qubits can be in an any state, e.g. the ground state |0⟩, the excited state |1⟩, a state of\nsuperposition |±⟩=\n1\n√\n2(|0⟩± |1⟩), or in an entangled state like the Bell states\n1\n√\n2(|00⟩± |11⟩).\n• The personal cards come from a set of available quantum logic gates which can be applied to the qubits. This\nset is known to all players, such that they can deduce the probability of another player having, e.g., a CX gate.\nThe set of quantum logic gates consist of operations acting on one or two qubits. In our implementation, the\nset consists of the Hadamard gate H, the phase ﬂip gate Z, the NOT gate X and the controlled NOT gate CX.\nThe action of each gate applied to the (|1⟩, |0⟩)- and (|+⟩, |−⟩)-basis is shown in Figure 1c.\n• The ﬁnal score of each player is the number of qubits measured to the state |1⟩in the computational basis at\nthe end of the round. The winner(s) of each pot is\/are the player(s) with the highest score.\nThe game is designed such that it can be played without knowledge of physics and advanced mathematics. The\nonly rules to learn are given in the list shown in Figure 1c and the following rules: The state |1⟩has a 100% probability\nto ”give a 1”, |0⟩a 0% probability to ”give a 1”, and the states |+⟩, |−⟩a 50% chance to ”give a 1”. In addition,\nstates marked as ”Pair” the probabilities of the two qubits cannot be described independently. As an example, the\nBell states\n1\n√\n2(|00⟩± |11⟩) ”give two 1s” with a probability of 50%. For the sake of correctness, the expression ”give a\n1\/give two 1s” refers to measurement in the computational basis. The idea of the game is that players will get familiar\nwith the rules underlying quantum computing.\nIn the quantum Poker game a state\n|φ⟩= V U |0⟩⊗n\n(1)\nis created. The matrix U creates the n ”community cards” and is equal for all players. The matrix V is created by\nthe players through placing their ”personal cards”. Given the state |φ⟩and an observable A, the expectation value of\nA in the state φ is given by\n⟨A⟩φ := ⟨φ| A |φ⟩=\nX\ni\nλi |⟨φ|ψi⟩|2 .\n(2)\nHere, A is a self-adjoint operator on the Hilbert space C⊗n, and {λi, |ψi⟩} is the set of eigenvalues and eigenvectors\nof A. To match the objective of our game, we need to deﬁne an observable A such that ⟨A⟩φ is equal to the expected\nnumber of ones in the computational basis. This can be done by choosing\nA =\n25\nX\ni=1\nb(i)Pi,\n(3)\nwhere b(i) is a function returning the number of ones of the binary representation of i, and Pi = |i⟩⟨i| is the\nmeasurement operator in the computational basis. A is a diagonal matrix with eigensystem {b(i), |i⟩}. The matrix A\ncan also be constructed via the number operator in the second quantization (a formalism used to describe and analyze\nquantum many-body systems), which is given by\nA =\nX\ni\nNi,\nwhere Ni = a†\niai.\n(4)\nThe creation and annihilation operators are given by\na†\ni = I⊗n−i−1 ⊗Q+ ⊗σ⊗i\nz ,\nai = I⊗n−i−1 ⊗Q−⊗σ⊗i\nz ,\n(5)\nand the raising and lowering operator is given by\nQ± = 1\n2 (σx ∓iσy) ,\n(6)\ni.e, Q+ =\n\u00120\n1\n0\n0\n\u0013\n, Q−=\n\u00120\n0\n1\n0\n\u0013\n. As an example, for two qubits A is a diagonal matrix with entries (0, 1, 1, 2), from\nupper left to lower right.\nUsing Equation 3, it is easy to calculate the expectation value as\n⟨Φ|A|Φ⟩= ⟨Φ|\n25\nX\ni=1\nb(i)Pi |Φ⟩=\n25\nX\ni=1\nb(i) ⟨Φ| Pi |Φ⟩=\n25\nX\ni=1\nb(i) ⟨Φ| P †\ni Pi |Φ⟩=\n25\nX\ni=1\nb(i)p(i),\n(7)\nsee Postulate 3 [11, page 84], where p(i) is the probability that result i occurs. In order to get the expectation value,\nwe can therefore measure the states |Φ⟩in the computational basis and multiply the resulting bit strings with b(i).\n4\nIdeal and noisy quantum circuit sampling\nIn the following we use Max’s circuit, shown in Figure 1b, as an example to investigate the eﬀects of noise in NISQ\ndevices.\n4\n(a) Distribution of the expectation value for Max’s circuit.\n(b) Same as (a), but with Pauli-twirling, see Section 5.1.\n(c) Convergence of the expectation values. Dashed lines\nrepresent the Pauli-twirled versions of the experiments.\nFigure 2: Result for Max’s circuit shown in Figure 1b\nusing simulators and IBM’s quantum devices. In total\n1024 repetitions with 8192 shots each were used. The\nerror models are more optimistic than the real quan-\ntum devices. Quantum noise on a real device leads to\nconvergence of the expectation value to a much lower\nvalue than its simulated noise model, which is in turn\nlower than the theoretical value. A comparison of (a)\nand (b) shows that Pauli-twirling, see Section 5.1, has\nlittle to no eﬀect on the obtained expectation value,\neven when noise is present. It leads, however, to an\nincreased variance in the presence of noise.\n4.1\nIdeal simulator\nThe state that Max creates is given by |φMax⟩=\n1\n√\n2(|01101⟩+ |11111⟩). Each realization of the circuit on an ideal\n(simulated) quantum computer results in a classical bit string qn−1 . . . q1q0 after measurement. A state φ = P\ni αi |i⟩\ninduces a probability distribution Pφ(i) = |αi|2. For Max’s circuit this distribution is thus given by a 50% chance of\nbeing in either state |01101⟩and |11111⟩. The expectation value for Max’s circuit is thus ⟨A⟩|φMax⟩= 4. Figure 2 shows\nthe convergence of sequence averages on an ideal simulator with respect to the number of repetitions to the expectation\nvalue, as well as the according probability distribution of the sequence. Here, we have used Qiskit’s simulator with\n1024 repetitions, each consisting of 8192 shots.\n4.2\nSet of universal quantum gates and circuit mappings\nOn a real quantum device, such as IBM’s QX architectures, only certain types of operations\/gates are supported. This\nset contains the single qubit gates U1, U2, U3, and the CX gate. In order to execute a circuit, one needs to express\nthe circuit in these basis gates. The Solovay-Kitaev theorem guarantees that this is always possible up to a given\naccuracy.\nAn additional complication comes from the fact that only a subset of qubits are physically connected. On IBM’s\nQX devices CX gates can only be applied to qubits that are connected by a bus resonator. Figures 3a,b show the\nconnectivity graph of two of IBM’s quantum devices, where edges mark qubits that are physically connected. In order\nto execute a circuit with CX gates between two not connected qubits, additional gates, such as SWAP or BRIDGE\ngates, need be used to transform the circuit into an equivalent one that obeys the connectivity graph. Inserting one\nSWAP or BRIDGE gate increases the number of CX gates by three. On current NISQ devices the noise level of\ntwo-qubit gate (CX) times and error rates are one order of magnitude higher than for single qubit gates[14], see also\nFigures 3a,b. One therefore wishes to ﬁnd a mapping with the lowest number of CX gates. In general, the problem\nof ﬁnding an optimal mapping is NP-complete problem[20]. For recent heuristics we refer to [5, 20] and references\ntherein. For Max’s circuit it is easy to ﬁnd an optimal mapping for IBM’s QX2 device manually using only one extra\nSWAP gate. For the IBM’s ourense device we have used the built-in optimizer. The resulting circuits which are shown\nin Figures 3c,d have the same number of CX gates. However, the circuit adapted to the ourense device has a larger\ndepth due to lower connectivity of the qubits.\n5\n(a) Connectivity and error rates on IBM’s QX2.\n(Feb 6 2020)\n(b) Connectivity and error rates on IBM’s ourense.\n(Feb 6 2020)\nU3\nU3\nU2\nU3\n(c) Max’s circuit transpiled for IBM’s QX2.\n#CX gates = 9, depth = 11.\nU3\nU3\nU3\nU2\nU3\nU3\nU3\nU3\nU2\nU3\nU3\nU2\nU3\nU3\nU3\nU3\n(d) Max’s circuit transpiled for IBM’s ourense.\n#CX gates = 9, Depth = 16.\nFigure 3: A comparison of the two IBM devices used, shows that the qubits have better connectivity on the QX2\ndevice (6 vs 4 bus resonators), the average readout error is much smaller, but the CX error rate is worse, as compared\nto IBM’s ourense. In order to execute Max’s circuit, shown in Figure 1b, equivalent circuits, shown in (c) and (d).\nBoth use the same number of CX gates, but have diﬀerent depth. The variables of the U2, U3 gates are skipped for\nsimplicity. The average depth of the circuits can be reduced to a constant after combining neighboring single qubit\ngates.\n4.3\nThe eﬀect of noise on quantum computation\nNoise is inherent to quantum computers. Qiskit provides methods for automatic generation of approximate noise\nmodels matching a given hardware device. This enables us to simulate the eﬀects of realistic noise on our computation\nbefore we run our circuits on a real quantum computer. Figure 2 shows the results of the Max’s circuit on diﬀerent\nsimulators and real quantum devices, using the transpiled circuits shown in Figure 3. Due to the inﬂuence of noise,\nthe resulting expectation values converge to a value around 3.86\/3.76 for the simulated noise model and 3.70\/3.52 on\nthe IBM’s quantum computers for the QX2\/ourense device. The gate noise level of IBM’s ourense device is slightly\nbetter than that of IBM’s QX2, but the average readout error and connectivity (and hence on average the depth of a\ncircuit) is worse for the ourense device. In our case, we achieve better results with the QX2 device, since we can use a\nshorter depth circuit with less readout errors. Both values are far oﬀthe ideal value of 4, obtained through a noiseless\nsimulation. In the following we will show how to mitigate the eﬀect of errors on expectation values in order to get a\nbetter estimate of the ideal expectation value.\n5\nError mitigation.\nIn this section we will apply the zero-noise extrapolation method to our circuit. The basic assumption of the method\nis that the expectation value of an observable depends smoothly on a small noise parameter λ ≪1 and admits the\nfollowing power series,\n⟨A⟩|φ⟩(λ) = ⟨A⟩∗\n|φ⟩+\nn\nX\ni=1\naiλi + O(λi+1),\n(8)\nwhere ⟨A⟩∗\nφ is the zero noise value we are trying to recover. Richardson’s deferred approach to the limit[15] can then\nbe applied to get a better estimate of the zero noise value. The method requires to generate n estimates to the\nexpectation value, i.e., ⟨A⟩φ(riλ) for r1 < r2 < · · · < rn. A better estimate of ⟨A⟩∗\nφ is then constructed by combining\nthese values in such a way that the lowest order terms in the power series cancel. As an example we can get a second\norder approximation of the expectation value by combining the results for r1 = 1 and r2 = 2 through\n2⟨A⟩|φ⟩(λ) −⟨A⟩|φ⟩(2λ) = ⟨A⟩∗\n|φ⟩+ O(λ2).\n(9)\nClearly, using r1 = 1 generates the expectation value with the least noise. Ampliﬁcation of noise with the factors\nri > 1 can either be achieved directly through pulse control or through modifying the circuit by adding certain extra\n6\nPauli-twirling\nNoise ampl.\nσa\nσc\nσe\nσb\nσd\nσf\n(a) Pauli-twirling and noise ampliﬁcation.\nσa\n1\n1\n1\n1\nσx\nσx\nσx\nσx\nσy\nσy\nσy\nσy\nσz\nσz\nσz\nσz\nσb\n1\nσx\nσy\nσz\n1\nσx\nσy\nσz\n1\nσx\nσy\nσz\n1\nσx\nσy\nσz\nσc\n1\n1\nσz\nσz\nσx\nσx\nσy\nσy\nσy\nσy\nσx\nσx\nσz\nσz\n1\n1\nσd\n1\nσx\nσy\nσz\nσx\n1\nσz\nσy\nσx\n1\nσz\nσy\n1\nσx\nσy\nσz\n(b) Valid combinations for Pauli-twirling of the CX gate.\nFigure 4: Pauli-twirling and noise ampliﬁcation.\ngates. For IBM’s QX devices pulse control on devices with more than one qubit is only accessible for their customers,\nwhich leaves us with the second possibility.\n5.1\nPauli-twirling\nBefore we apply the noise ampliﬁcation, we convert the non-stochastic errors of CX gates into stochastic errors, see\ne.g. [9, section VII] for a detailed description. One way to achieve this is to apply Pauli-twirling. Given a ﬁnite group\nG of quantum operations and a quantum channel Λ the average\n1\n|G|\nX\nU∈G\nU †ΛU,\n(10)\nis called a twirl of the channel Λ. In our case gates σa, σb, σc, σd are inserted before and after each CX gate Λ, where σi\nis chosen from the twirling set consisting of the Pauli gates {1, σx, σy, σz}. After randomly (with uniform probability)\nchoosing σa, σb the gates σc, σd are then chosen to satisfy\nσc ⊗σd = eiθΛ(σa ⊗σb)Λ†.\n(11)\nThis ensures that the overall eﬀect results only in a phase change, which does not change the measurement outcome.\nThe circuit constructed with Pauli-twirling applied to all CX gates is therefore equivalent to the original circuit.\nFigure 4 shows a schematic depiction of Pauli-twirling as well as all valid combinations for the CX gate. In practice\nthis method is applicable, if the assumption holds that the qualities of single-qubit gates are an order of magnitude\nsmaller than two-qubit gates. Twirling should then only have a negligible eﬀect on the ﬁdelity of the expectation\nvalue on NISQ devices. Figure 2 indicates that noise manifests itself in an increase of the variance of the distribution.\nThere is no eﬀect for the ideal simulator.\n5.2\nNoise ampliﬁcation\nIn order to amplify the strength of the noise, we will apply random Pauli gates with a probability proportional to the\nerror rate of the CX gate between a given pair of qubits. More precisely this is means applying gates σe, σf randomly\nchosen form the set of Pauli gates {1, σx, σy, σz} after the twirled CX gates with probability (r −1)ϵi,j, see a depiction\nin Figure 4a. Note that there are only 15 possible choices for σe ⊗σf, since 1 ⊗1 must be excluded because it does\nnot increase the error. Here, ϵi,j is the two-qubit gate error rate between qubits qi and qj. On average this increases\nthe error rate to the desired value ϵnew = ϵi,j + (r −1)ϵi,j = rϵi,j.\nFigure 5 shows the result for both the simulated error models and the real quantum devices. The assumption that\nthe expectation value of an observable depends smoothly on r seems to hold for the simulator with the IBM QX2 noise\nmodel and the IBM QX2 device, but less so for the IBM ourense device, see Figure 5d. This is likely because some\nof the underlying assumptions of the method are violated for the ourense device, e.g., the existence of non-Markovian\nnoise, spatially or temporally correlated noise, etc. The result shown in Figure 5d seems to justify the assumption of\nthe exponential variant of the extrapolation method presented in [3].\nAdditional insight is provided by looking at the distribution for r ∈{1, 4, 32}. Since we increase the noise of CX\ngates artiﬁcially by adding Pauli gates, this means that other outcome strings are becoming more likely. Figure 5a-c\nshows that expectation values of 1, 2, 3 become increasingly likely. The result is a multi-peaked distribution. The\nnoisy results show the same basic behaviour as the ideal circuit. In general, the noise models seem to lead to better\nestimates of the expectation values than the real quantum devices, limiting their usefulness somewhat.\n7\n(a) Result without noise ampliﬁcation r = 1.\n(b) Result for r = 4.\n(c) Result for r = 32.\n(d) Expectation value obtained for diﬀerent noise ampliﬁcation\nfactors r.\n(e) Depth of the circuits for varying r. The average depth is\nroughly constant when combining single qubit gates.\nFigure 5: Eﬀect of artiﬁcially amplifying the noise on Max’s circuit. Each sample point uses N = 1024 randomly\ngenerated circuits with 8192 shots per circuit. The r-dependence of the simulated noise model is much smoother than\nthat of IBM’s QX2 device. Amplifying the noise with random Pauli gates leads to a multi-peaked distribution, because\nthe expectation value is a sum of diﬀerent outcomes.\n5.3\nError mitigation of measurement noise\nMeasurement noise is another major source of error. Here, we use the model that assumes spatially uncorrelated errors\nof a bit ﬂip. We compute the probability that the state |i⟩is observed if the state |j⟩is prepared, i.e. the conditional\nprobability Pi,j := P(|i⟩| |j⟩). The matrix\nP =\n\n\nP1,1\n. . .\nP1,2n\n...\n...\n...\nP2n,1\n. . .\nP2n,2n\n\n,\n(12)\nis a (right) stochastic matrix, as P\nj Pi,j = 1. In the absence of noise Pi,j = δi,j, but measurement (and other) noise\nleads to non-zero oﬀ-diagonal entries. The resulting probabilities for IBM’s QX2 are shown in Figure 6a.\nLet us assume that, for a quantum computer, we are given P and a probability distribution Dnoisy induced by\nmeasurement of a quantum state |Ψ⟩. Using the equation\nDnoisy = PDideal,\n(13)\nwe can retrieve the ideal distribution Dideal of |Ψ⟩. As an example, we are preparing the Bell state |Ψ⟩= 1\/\n√\n2(|00⟩+\n|11⟩), but the resulting distribution is Dnoisy = (13, 2, 2, 13)\/30. In addition we have that Pi,j is 0.8 if i = j and 0.2\/3\notherwise. By solving (13) we can then retrieve the noiseless distribution Dideal = (1\/2, 0, 0, 1\/2).\nIn order for the method to work, measurement errors must be at least one order of magnitude larger than state\npreparation and the execution of the X gate. This condition is satisﬁed for IBM’s QX2 and ourense device, compare\nFigure 3. In addition it must be mentioned that this requires an exponential amount (in the number of qubits) of\nstates to be prepared and measured to build the matrix P. In this work we use the implementation provided by\nQiskit[1]. Figure 6b and the column for E1 in Table 7c show a clear improvement by applying the measurement ﬁlter\nfor Max’s circuit.\n5.4\nOverall results\nIn all of our experiments we generate N circuits randomly with Pauli-twirling and a noise ampliﬁcation factor r. Each\nof these circuits is called a ”repetition” and uses 8192 shots. The number of this random circuits (repetitions) have to\nbe large enough to cover the whole sample space. Max’s circuits has 9 CX operations, which is why we used N = 1024\n8\n(a) Conditional probabilities P(|i⟩| |j⟩) for IBM’s QX2 device.\n(b) Eﬀect of measurement ﬁlter.\nFigure 6: The results from the prepared vs measured state can be used to construct a ﬁlter to mitigate measurement\nerrors. The ﬁlter applied to Max’s circuit improves the probabilities.\nrepetitions. We can see in Figures 7a,b that this number is suﬃcient for convergence. The results for other circuits\nare similar.\nFigure 7 shows the convergence of the circuits and the eﬀect of error mitigation techniques on the expectation value.\nWith Er = E(r) we denote the expectation value achieved with ampliﬁcation factor r, and by R(.) the Richardson\nextrapolation. Without error mitigation the expectation value for the X2 device is closer to the theoretical value than\nthe ourense device. The execution on real devices leads to a worse result as the simulators. Compare also Figure 2b\nApplying Richardson extrapolation clearly improves the resulting expectation value in all cases. With increasing\nnumber of terms the achieved estimate of the expectation value seems to converge. For the X2 device, already 2 to 3\nterms are suﬃcient to achieve a very good approximation. For the ourense device, the results are not as good. This\nis most likely due to longer circuit depth and higher measurement errors.\nApplying the measurement error ﬁlter alone helps to improve the expectation value as well, particularly when using\na quantum simulator. However, on the real devices, the results are not as good as for the Richardson extrapolation.\nCombining Richardson extrapolation and measurement error ﬁlter seems to only work for the ourense model and\ndevice.\n6\nRelationship to benchmarking\nWe would like to remark that circuits from the proposed game could be used to benchmark quantum devices. In\nrandomized benchmarking, see e.g., [8], random sequences are generated from the Cliﬀord group, including a computed\nreversal element such that the overall unitary is the identity up to a global phase factor. Quantum Poker has a random\npart, the ”community cards”, and a part that depends on the strategy of the individual player, see Equation (1). In\norder to use quantum poker in this setting, one would need to make the following changes:\n• Negate the original deﬁnition of winning to be maximizing the number of 0’s that are measured.\n• Provide a player with enough cards, such that the overall unitary of an identity modulo global phase can be\ncreated, i.e., such that U = eiθV .\nWith this changes the game can be used to benchmark subspaces of 5 qubits. A generalization to n qubits is also\nstraight forward.\n7\nAvailability of Data and Code\nThe open source python\/jupyter notebook implementation of the game is available at https:\/\/github.com\/sintefmath\/\nquantumpoker and the complete code for reproducing the results obtained in this article is available at https:\n\/\/github.com\/OpenQuantumComputing.\n8\nConclusion\nWe have presented a game intended to serve as a pedagogical tool for learning the basic rules of quantum computers.\nThe aim was to make it a fun experience in order to get more people acquainted with the rules of quantum computing\nand raise interest in algorithms and error mitigation. It could therefore help to avoid a shortage in experts when\nquantum computers become commercialized. To make the threshold for acquiring the game lower, it could be made\n9\n(a) Convergence of Expectation values for IBM QX2.\n(b) Convergence of Expectation values for IBM ourense.\nE1 R(E1, E2) R(E1, E2, E4) R(E1, E2, E4, E8) R(E1, E2, E4, E8, E16) R(E1, E2, E4, E8, E16, E32)\nsim X2\n3.85\n3.97\n3.97\n3.97\n3.97\n3.96\nsim ourense\n3.77\n3.82\n3.83\n3.83\n3.83\n3.83\nX2\n3.68\n3.90\n3.96\n3.98\n3.99\n4.00\nourense\n3.45\n3.53\n3.54\n3.54\n3.53\n3.53\nsim X2 (mf)\n3.95\n4.06\n4.07\n4.06\n4.06\n4.06\nsim ourense (mf) 3.87\n3.93\n3.94\n3.94\n3.94\n3.94\nX2 (mf)\n3.80\n4.04\n4.10\n4.12\n4.13\n4.14\nourense (mf)\n3.62\n3.71\n3.72\n3.72\n3.71\n3.71\n(c) Expectation values achieved by simulators and real quantum devices. ”mf” denotes results using the measurement ﬁlter.\nFigure 7: Results achieved using the simulator and IBM’s quantum devices. The symbols E1, E2, · · · E32 denote the\nexpectation value for the noise ampliﬁcation factor r equal to 1, 2, · · · , 32 respectively. R(.) denotes the Richardson\nextrapolation. All results were obtained by using 1024 repetitions with 8192 shots each.\navailable on smartphones as well. In order to make the game more diﬃcult one could generate a completely random\ninitial state. The quantum Poker game can be easily simulated on classical computers because it requires only 5\nqubits. However, on contemporary quantum computers the use of multiple error-prone CX gates and measurement\noperations, gives a large error in the output state of the circuits. We have presented and discussed several error\nmitigation techniques that produce better estimates of expectation values. The overall results rely on Pauli-twirling\nto convert non-stochastic errors of CX gates into stochastic errors.\nReferences\n[1] Gadi Aleksandrowicz, Thomas Alexander, Panagiotis Barkoutsos, Luciano Bello, Yael Ben-Haim, David Bucher,\nFrancisco Jose Cabrera-Hern´andez, Jorge Carballo-Franquis, Adrian Chen, Chun-Fu Chen, Jerry M. Chow, Anto-\nnio D. C´orcoles-Gonzales, Abigail J. Cross, Andrew Cross, Juan Cruz-Benito, Chris Culver, Salvador De La Puente\nGonz´alez, Enrique De La Torre, Delton Ding, Eugene Dumitrescu, Ivan Duran, Pieter Eendebak, Mark Everitt,\nIsmael Faro Sertage, Albert Frisch, Andreas Fuhrer, Jay Gambetta, Borja Godoy Gago, Juan Gomez-Mosquera,\nDonny Greenberg, Ikko Hamamura, Vojtech Havlicek, Joe Hellmers,  Lukasz Herok, Hiroshi Horii, Shaohan Hu,\nTakashi Imamichi, Toshinari Itoko, Ali Javadi-Abhari, Naoki Kanazawa, Anton Karazeev, Kevin Krsulich, Peng\nLiu, Yang Luh, Yunho Maeng, Manoel Marques, Francisco Jose Mart´ın-Fern´andez, Douglas T. McClure, David\nMcKay, Srujan Meesala, Antonio Mezzacapo, Nikolaj Moll, Diego Moreda Rodr´ıguez, Giacomo Nannicini, Paul\nNation, Pauline Ollitrault, Lee James O’Riordan, Hanhee Paik, Jes´us P´erez, Anna Phan, Marco Pistoia, Viktor\nPrutyanov, Max Reuter, Julia Rice, Abd´on Rodr´ıguez Davila, Raymond Harry Putra Rudy, Mingi Ryu, Ninad\nSathaye, Chris Schnabel, Eddie Schoute, Kanav Setia, Yunong Shi, Adenilton Silva, Yukio Siraichi, Seyon Sivara-\njah, John A. Smolin, Mathias Soeken, Hitomi Takahashi, Ivano Tavernelli, Charles Taylor, Pete Taylour, Kenso\nTrabing, Matthew Treinish, Wes Turner, Desiree Vogt-Lee, Christophe Vuillot, Jonathan A. Wildstrom, Jessica\nWilson, Erick Winston, Christopher Wood, Stephen Wood, Stefan W¨orner, Ismail Yunus Akhalwaya, and Christa\nZoufal. Qiskit: An open-source framework for quantum computing, 2019.\n[2] J. I. Colless, V. V. Ramasesh, D. Dahlen, M. S. Blok, M. E. Kimchi-Schwartz, J. R. McClean, J. Carter, W.\nA. de Jong, and I. Siddiqi. Computation of molecular spectra on a quantum processor with an error-resilient\nalgorithm. Physical Review X, 8(1), February 2018. doi: 10.1103\/physrevx.8.011021. URL https:\/\/doi.org\/\n10.1103\/physrevx.8.011021.\n[3] Suguru Endo, Simon C. Benjamin, and Ying Li. Practical quantum error mitigation for near-future applica-\n10\ntions. Physical Review X, 8(3), July 2018. doi: 10.1103\/physrevx.8.031027. URL https:\/\/doi.org\/10.1103\/\nphysrevx.8.031027.\n[4] Daniel Gottesman. Stabilizer codes and quantum error correction. arXiv preprint quant-ph\/9705052, 1997.\n[5] Toshinari Itoko, Rudy Raymond, Takashi Imamichi, and Atsushi Matsuo.\nOptimization of quantum circuit\nmapping using gate transformation and commutation. Integration, 70:43–50, January 2020. doi: 10.1016\/j.vlsi.\n2019.10.004. URL https:\/\/doi.org\/10.1016\/j.vlsi.2019.10.004.\n[6] Abhinav Kandala, Kristan Temme, Antonio D. C´orcoles, Antonio Mezzacapo, Jerry M. Chow, and Jay M.\nGambetta. Error mitigation extends the computational reach of a noisy quantum processor. Nature, 567(7749):\n491–495, March 2019. doi: 10.1038\/s41586-019-1040-7. URL https:\/\/doi.org\/10.1038\/s41586-019-1040-7.\n[7] Faisal Shah Khan, Neal Solmeyer, Radhakrishnan Balu, and Travis S Humble. Quantum games: a review of the\nhistory, current state, and interpretation. Quantum Information Processing, 17(11):309, 2018.\n[8] Emanuel Knill, Dietrich Leibfried, Rolf Reichle, Joe Britton, R Brad Blakestad, John D Jost, Chris Langer, Roee\nOzeri, Signe Seidelin, and David J Wineland. Randomized benchmarking of quantum gates. Physical Review A,\n77(1):012307, 2008.\n[9] Ying Li and Simon C. Benjamin. Eﬃcient variational quantum simulator incorporating active error minimiza-\ntion. Physical Review X, 7(2), June 2017. doi: 10.1103\/physrevx.7.021050. URL https:\/\/doi.org\/10.1103\/\nphysrevx.7.021050.\n[10] Jarrod R. McClean, Mollie E. Kimchi-Schwartz, Jonathan Carter, and Wibe A. de Jong. Hybrid quantum-classical\nhierarchy for mitigation of decoherence and determination of excited states. Physical Review A, 95(4), April 2017.\ndoi: 10.1103\/physreva.95.042308. URL https:\/\/doi.org\/10.1103\/physreva.95.042308.\n[11] M.A. Nielsen and I.L. Chuang. Quantum Computation and Quantum Information. Cambridge Series on Infor-\nmation and the Natural Sciences. Cambridge University Press, 2000.\n[12] Matthew Otten and Stephen K. Gray. Recovering noise-free quantum observables. Physical Review A, 99(1),\nJanuary 2019. doi: 10.1103\/physreva.99.012338. URL https:\/\/doi.org\/10.1103\/physreva.99.012338.\n[13] Vickram N. Premakumar and Robert Joynt. Error mitigation in quantum computers subject to spatially correlated\nnoise, 2018.\n[14] 5 qubit backend: IBM Q team.\n”IBM Q 5 yorktown backend speciﬁcation v1.0.0”.\nRetrieved from https:\n\/\/ibm.biz\/qiskit-yorktown, (2018).\n[15] L. F. Richardson and J. A. Gaunt. The deferred approach to the limit. part i. single lattice. part II. interpenetrating\nlattices. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,\n226(636-646):299–361, January 1927. doi: 10.1098\/rsta.1927.0008. URL https:\/\/doi.org\/10.1098\/rsta.1927.\n0008.\n[16] Peter W. Shor. Scheme for reducing decoherence in quantum computer memory. Physical Review A, 52(4):R2493–\nR2496, October 1995. doi: 10.1103\/physreva.52.r2493. URL https:\/\/doi.org\/10.1103\/physreva.52.r2493.\n[17] Chao Song, Jing Cui, H. Wang, J. Hao, H. Feng, and Ying Li.\nQuantum computation with universal error\nmitigation on a superconducting quantum processor. Science Advances, 5(9):eaaw5686, September 2019. doi:\n10.1126\/sciadv.aaw5686. URL https:\/\/doi.org\/10.1126\/sciadv.aaw5686.\n[18] Kristan Temme, Sergey Bravyi, and Jay M. Gambetta.\nError mitigation for short-depth quantum circuits.\nPhysical Review Letters, 119(18), November 2017. doi: 10.1103\/physrevlett.119.180509. URL https:\/\/doi.org\/\n10.1103\/physrevlett.119.180509.\n[19] Dave Wecker, Matthew B. Hastings, and Matthias Troyer. Progress towards practical quantum variational algo-\nrithms. Physical Review A, 92(4), October 2015. doi: 10.1103\/physreva.92.042303. URL https:\/\/doi.org\/10.\n1103\/physreva.92.042303.\n[20] Robert Wille, Lukas Burgholzer, and Alwin Zulehner. Mapping quantum circuits to IBM QX architectures using\nthe minimal number of SWAP and h operations. In Proceedings of the 56th Annual Design Automation Conference\n2019 on - DAC 19. ACM Press, 2019. doi: 10.1145\/3316781.3317859. URL https:\/\/doi.org\/10.1145\/3316781.\n3317859.\n11\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Quantum Poker A game for quantum computers suitable for benchmarking error mitigation techniques on NISQ devices.pdf"}
{"title":"Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks","authors":"Gabriele Farina, Chun Kai Ling, Fei Fang, Tuomas Sandholm","summary":"While Nash equilibrium in extensive-form games is well understood, very\nlittle is known about the properties of extensive-form correlated equilibrium\n(EFCE), both from a behavioral and from a computational point of view. In this\nsetting, the strategic behavior of players is complemented by an external\ndevice that privately recommends moves to agents as the game progresses;\nplayers are free to deviate at any time, but will then not receive future\nrecommendations. Our contributions are threefold. First, we show that an EFCE\ncan be formulated as the solution to a bilinear saddle-point problem. To\nshowcase how this novel formulation can inspire new algorithms to compute\nEFCEs, we propose a simple subgradient descent method which exploits this\nformulation and structural properties of EFCEs. Our method has better\nscalability than the prior approach based on linear programming. Second, we\npropose two benchmark games, which we hope will serve as the basis for future\nevaluation of EFCE solvers. These games were chosen so as to cover two natural\napplication domains for EFCE: conflict resolution via a mediator, and\nbargaining and negotiation. Third, we document the qualitative behavior of EFCE\nin our proposed games. We show that the social-welfare-maximizing equilibria in\nthese games are highly nontrivial and exhibit surprisingly subtle sequential\nbehavior that so far has not received attention in the literature.","url":"http:\/\/arxiv.org\/abs\/1905.12564v2","pdf_url":"http:\/\/arxiv.org\/pdf\/1905.12564v2","published":1559146531000,"comment":"Full version of NeurIPS 2019 paper","pdf_text":"CORRELATION IN EXTENSIVE-FORM GAMES:\nSADDLE-POINT FORMULATION AND BENCHMARKS∗\nARXIV PREPRINT\nGabriele Farina\nComputer Science Department\nCarnegie Mellon University\ngfarina@cs.cmu.edu\nChun Kai Ling\nComputer Science Department\nCarnegie Mellon University\nchunkail@cs.cmu.edu\nFei Fang\nInstitute for Software Research\nCarnegie Mellon University\nfeif@cs.cmu.edu\nTuomas Sandholm\nComputer Science Department, CMU\nStrategic Machine, Inc.\nStrategy Robot, Inc.\nOptimized Markets, Inc.\nsandholm@cs.cmu.edu\nOctober 27, 2019\nAbstract\nWhile Nash equilibrium in extensive-form games is well understood, very little is known about\nthe properties of extensive-form correlated equilibrium (EFCE), both from a behavioral and from\na computational point of view. In this setting, the strategic behavior of players is complemented by\nan external device that privately recommends moves to agents as the game progresses; players are\nfree to deviate at any time, but will then not receive future recommendations. Our contributions\nare threefold. First, we show that an EFCE can be formulated as the solution to a bilinear saddle-\npoint problem. To showcase how this novel formulation can inspire new algorithms to compute\nEFCEs, we propose a simple subgradient descent method which exploits this formulation and\nstructural properties of EFCEs. Our method has better scalability than the prior approach based on\nlinear programming. Second, we propose two benchmark games, which we hope will serve as the\nbasis for future evaluation of EFCE solvers. These games were chosen so as to cover two natural\napplication domains for EFCE: conﬂict resolution via a mediator, and bargaining and negotiation.\nThird, we document the qualitative behavior of EFCE in our proposed games. We show that the\nsocial-welfare-maximizing equilibria in these games are highly nontrivial and exhibit surprisingly\nsubtle sequential behavior that so far has not received attention in the literature.\n1. Introduction\nNash equilibrium (NE) (Nash, 1950), the most seminal concept in non-cooperative game theory, captures a\nmulti-agent setting where each agent is selﬁshly motivated to maximize their own payoff. The assumption\nunderpinning NE is that the interaction is completely decentralized: the behavior of each agent is not\nregulated by any external orchestrator. Contrasted with the other—often utopian—extreme of a fully\nmanaged interaction, where an external dictator controls the behavior of each agent so that the whole system\nmoves to a desired state, the social welfare that can be achieved by NE is generally lower, sometimes\ndramatically so (Koutsoupias & Papadimitriou, 1999; Roughgarden & Tardos, 2002). Yet, in many\n∗This paper was accepted for publication at NeurIPS 2019.\narXiv:1905.12564v2  [cs.GT]  28 Oct 2019\nARXIV PREPRINT - OCTOBER 27, 2019\nrealistic interactions, some intermediate form of centralized control can be achieved. In particular, in his\nlandmark paper, Aumann (1974) proposed the concept of correlated equilibrium (CE), where a mediator\n(the correlation device) can recommend behavior, but not enforce it. In a CE, the correlation device is\nconstructed so that the agents—which are still modeled as fully rational and selﬁsh just like in an NE—have\nno incentive to deviate from the private recommendation. Allowing correlation of actions while ensuring\nselﬁshness makes CE a good candidate solution concept in multi-agent and semi-competitive settings such\nas trafﬁc control, load balancing (Ashlagi et al., 2008), and carbon abatement (Ray & Gupta, 2009), and it\ncan lead to win-win outcomes.\nIn this paper, we study the natural extension of correlated equilibrium in extensive-form (i.e., sequential)\ngames, known as extensive-form correlated equilibrium (EFCE) (von Stengel & Forges, 2008). Like CE,\nEFCE assumes that the strategic interaction is complemented by an external mediator; however, in an\nEFCE the mediator only privately reveals the recommended next move to each acting player, instead of\nrevealing the whole plan of action throughout the game (i.e., recommended move at all decision points)\nfor each player at the beginning of the game. Furthermore, while each agent is free to defect from the\nrecommendation at any time, this comes at the cost of future recommendations.\nWhile the properties of correlation in normal-form games are well-studied, they do not automatically transfer\nto the richer world of sequential interactions. It is known in the study of NE that sequential interactions can\npose different challenges, especially in settings where the agents retain private information. Conceptually,\nthe players can strategically adjust to dynamic observations about the environment and their opponents as the\ngame progresses. Despite tremendous interest and progress in recent years for computing NE in sequential\ninteractions with private information, with signiﬁcant milestones achieved in poker games (Bowling et al.,\n2015; Brown & Sandholm, 2017; Moravˇcík et al., 2017; Brown & Sandholm, 2019b) and other large,\nreal-world domains, not much has been done to increase our understanding of (extensive-form) correlated\nequilibria in these settings.\nContributions\nOur primary objective with this paper is to spark more interest in the community towards\na deeper understanding of the behavioral and computational aspects of EFCE.\n• In Section 3 we show that an EFCE in a two-player general-sum game is the solution to a bilinear\nsaddle-point problem (BSPP). This conceptual reformulation complements the EFCE construction by von\nStengel & Forges (2008), and allows for the development of new and efﬁcient algorithms. As a proof\nof concept, by using our reformulation we devise a variant of projected subgradient descent which\noutperforms linear-programming(LP)-based algorithms proposed by von Stengel & Forges (2008) in\nlarge game instances.\n• In Section 5 we propose two benchmark games; each game is parametric, so that these games can scale\nin size as desired. The ﬁrst game is a general-sum variant of the classic war game Battleship. The second\ngame is a simpliﬁed version of the Sheriff of Nottingham board game. These games were chosen so as to\ncover two natural application domains for EFCE: conﬂict resolution via a mediator, and bargaining and\nnegotiation.\n• By analyzing EFCE in our proposed benchmark games, we show that even if the mediator cannot enforce\nbehavior, it can induce signiﬁcantly higher social welfare than NE and successfully deter players from\ndeviating in at least two (often connected) ways: (1) using certain sequences of actions as ‘passcodes’ to\nverify that a player has not deviated: defecting leads to incomplete or wrong passcodes which indicate\ndeviation, and (2) inducing opponents to play punitive actions against players that have deviated from\nthe recommendation, if such a deviation is detected. Crucially, both deterrents are unique to sequential\ninteractions and do not apply to non-sequential games. This corroborates the idea that the mediation of\nsequential interactions is a qualitatively different problem than that of non-sequential games and further\njustiﬁes the study of EFCE as an interesting direction for the community. To our knowledge, these are\nthe ﬁrst experimental results and observations on EFCE in the literature.\nThe source code for our game generators and subgradient method is published online2.\n2https:\/\/github.com\/Sandholm-Lab\/game-generators\nhttps:\/\/github.com\/Sandholm-Lab\/efce-subgradient\n2\nARXIV PREPRINT - OCTOBER 27, 2019\n2. Preliminaries\nExtensive-form games (EFGs) are sequential games that are played over a rooted game tree. Each node\nin the tree belongs to a player and corresponds to a decision point for that player. Outgoing edges from\na node v correspond to actions that can be taken by the player to which v belongs. Each terminal node\nin the game tree is associated with a tuple of payoffs that the players receive should the game end in that\nstate. To capture imperfect information, the set of vertices of each player is partitioned into information\nsets. The vertices in a same information set are indistinguishable to the player that owns those vertices.\nFor example, in a game of Poker, a player cannot distinguish between certain states that only differ in\nopponent’s private hand. As a result, the strategy of the player (specifying which action to take) is deﬁned\non the information sets instead of the vertices. For the purpose of this paper, we only consider perfect-recall\nEFGs. This property means that each player does not forget any of their previous action, nor any private or\npublic observation that the player has made. The perfect-recall property can be formalized by requiring that\nfor any two vertices in a same information set, the paths from those vertices to the root of the game tree\ncontain the exact same sequence of actions for the acting player at the information set.\nA pure normal-form strategy for Player i deﬁnes a choice of action for every information set that belongs\nto i. A player can play a mixed strategy, i.e., sample from a distribution over their pure normal-form\nstrategies. However, this representation contains redundancies: some information sets for Player i may\nbecome unreachable reachable after the player makes certain decisions higher up in the tree. Omitting these\nredundancies leads to the notion of reduced-normal-form strategies, which are known to be strategically\nequivalent to normal-form strategies (e.g., (Shoham & Leyton-Brown, 2009) for more details). Both the\nnormal-form and the reduced-normal-form representation are exponentially large in the size of the game\ntree.\nHere, we ﬁx some notations. Let Z be the set of terminal states (or equivalently, outcomes) in the game\nand ui(z) be the utility obtained by player i if the game terminates at z ∈Z. Let Πi be the set of pure\nreduced-normal-form strategies for Player i. We deﬁne Πi(I), Πi(I, a) and Πi(z) to be the set of reduced-\nnormal-form strategies that (a) can lead to information set I, (b) can lead to I and prescribes action a\nat information set I, and (c) can lead to the terminal state z, respectively. We denote by Σi the set of\ninformation set-action pairs (I, a) (also referred to as sequences), where I is an information set for Player i\nand a is an action at set I. For a given terminal state z let σi(z) be the last (I, a) pair belonging to Player i\nencountered in the path from the root of the tree to z.\nExtensive-Form Correlated Equilibrium\nExtensive-form correlated equilibrium (EFCE) is a solution\nconcept for extensive-form games introduced by von Stengel & Forges (2008).3 Like in the traditional\ncorrelated equilibrium (CE), introduced by Aumann (1974), a correlation device selects private signals\nfor the players before the game starts. These signals are sampled from a correlated distribution µ—a joint\nprobability distribution over Π1 × Π2—and represent recommended player strategies. However, while in a\nCE the recommended moves for the whole game tree are privately revealed to the players when the game\nstarts, in an EFCE the recommendations are revealed incrementally as the players progress in the game tree.\nIn particular, a recommended move is only revealed when the player reaches the decision point in the game\nfor which the recommendation is relevant. Moreover, if a player ever deviates from the recommended move,\nthey will stop receiving recommendations. To concretely implement an EFCE, one places recommendations\ninto ‘sealed envelopes’ which may only be opened at its respective information set. Sealed envelopes may\nimplemented using cryptographic techniques (see (Dodis et al., 2000) for one such example).\nIn an EFCE, the players know less about the set of recommendations that were sampled by the correlation\ndevice. The beneﬁts are twofold. First, the players can be more easily induced to play strategies that\nhurt them (but beneﬁt the overall social welfare), as long as “on average” the players are indifferent as\nto whether or not to follow the recommendations: the set of EFCEs is a superset of that of CEs. Second,\nsince the players observe less, the set of probability distributions for the correlation device for which no\n3Other CE-related solution concepts in sequential games include the agent-form correlated equilibrium (AFCE),\nwhere agents continue to receive recommendations even upon defection, and normal-form coarse CE (NFCCE). NFCCE\ndoes not allow for defections during the game, in fact, before the game starts, players must decide to commit to\nfollowing all recommendations upfront (before receiving them), or elect to receive none.\n3\nARXIV PREPRINT - OCTOBER 27, 2019\nplayer has an incentive to deviate can be described succinctly in certain classes of games: von Stengel &\nForges (2008, Theorem 1.1) show that in two-player, perfect-recall extensive-form games with no chance\nmoves, the set of EFCEs can be described by a system of linear equations and inequalities of polynomial\nsize in the game description. On the other hand, the same result cannot hold in more general settings: von\nStengel & Forges (2008, Section 3.7) also show that in games with more than two players and\/or chance\nmoves, deciding the existence of an EFCE with social welfare greater than a given value is NP-hard. It is\nimportant to note that this last result only implies that the characterization of the set of all EFCEs cannot be\nof polynomial size in general (unless P = NP). However, the problem of ﬁnding one EFCE can be solved in\npolynomial time: Huang (2011) and Huang & von Stengel (2008) show how to adapt the Ellipsoid Against\nHope algorithm (Papadimitriou & Roughgarden, 2008; Jiang & Leyton-Brown, 2015) to compute an EFCE\nin polynomial time in games with more than two players and\/or with chance moves. Unfortunately, that\nalgorithm is only theoretical, and known to not scale beyond extremely small instances (Leyton-Brown,\n2019).\n3. Extensive-Form Correlated Equilibria as Bilinear Saddle-Point Problems\nOur objective for this section is to cast the problem of ﬁnding an EFCE in a two-player game as a bilinear\nsaddle-point problem, that is a problem of the form minx∈X maxy∈Y x⊤Ay, where X and Y are compact\nconvex sets. In the case of EFCE, X and Y are convex polytopes that belong to a space whose dimension is\npolynomial in the game tree size. This reformulation is meaningful:\n• From a conceptual angle, it brings the problem of computing an EFCE closer to several other solution\nconcepts in game theory that are known to be expressible as BSPP. In particular, the BSPP formulation\nshows that an EFCE can be viewed as a NE in a two-player zero-sum game between a deviator, who is\ntrying to decide how to best defect from recommendations, and a mediator, who is trying to come up\nwith an incentive-compatible set of recommendations.\n• From a geometric point of view, the BSPP formulation better captures the combinatorial structure\nof the problem: X and Y have a well-deﬁned meaning in terms of the input game tree. This has\nalgorithmic implications: for example, because of the structure of Y (which will be detailed later), the\ninner maximization problem can be solved via a single bottom-up game-tree traversal.\n• From a computational standpoint, it opens the way to the plethora of optimization algorithms (both\ngeneral-purpose and those speciﬁc to game theory) that have been developed to solve BSPPs. Examples\ninclude Nesterov’s excessive gap technique (Nesterov, 2005), Nemirovski’s mirror prox algorithm (Ne-\nmirovski, 2004) and regret-methods based methods such as mirror descent, follow-the-regularized-leader\n(e.g., (Hazan, 2016)), and CFR and its variants (Zinkevich et al., 2007; Farina et al., 2019; Brown &\nSandholm, 2019a).\nFurthermore, it is easy to show that by dualizing the inner maximization problem in the BSPP formulation,\none recovers the linear program introduced by von Stengel & Forges (2008) (we show this in Appendix A).\nIn this sense, our formulation subsumes the existing one.\nTriggers and Deviations\nOne effective way to reason about extensive-form correlated equilibria is via\nthe notion of trigger agents, which was introduced (albeit used in a different context) in Gordon et al. (2008)\nand Dudik & Gordon (2009):\nDeﬁnition 1. Let ˆσ :=(ˆI, ˆa) ∈Σi be a sequence for Player i, and let ˆµ be a distribution over Πi(ˆI). A\n(ˆσ, ˆµ)-trigger agent for Player i is a player that follows all recommendations given by the mediator unless\nthey get recommended ˆa at ˆI; in that case, the player ‘gets triggered’, stops following the recommendations\nand instead plays based on a pure strategy sampled from ˆµ until the game ends.\nA correlated distribution µ is an EFCE if and only if any trigger agent for Player i can get utility at most\nequal to the utility that Player i earns by following the recommendations of the mediator at all decision\npoints. In order to express the utility of the trigger agent, it is necessary to compute the probability of the\ngame ending in each of the terminal states. As we show in Appendix B, this can be done concisely by\npartitioning the set of terminal nodes in the game tree into three different sets. In particular, let ZˆI,ˆa be the\n4\nARXIV PREPRINT - OCTOBER 27, 2019\nset of terminal nodes whose path from the root of the tree contains taking action ˆa at ˆI and let ZˆI be the set\nof terminal nodes whose path from the root passes through ˆI and are not in ZˆI,ˆa. We have\nLemma 1. Consider a (ˆσ, ˆµ)-trigger agent for Player 1, where ˆσ = (ˆI, ˆa). The value of the trigger agent,\ndeﬁned as the expected difference between the utility of the trigger agent and the utility of an agent that\nalways follows recommendations sampled from correlated distribution µ, is computed as\nv1,ˆσ(µ, ˆµ) :=\nX\nz∈Z ˆ\nI\nu1(z)ξ1(ˆσ; z)y1,ˆσ(z) −\nX\nz∈Z ˆ\nI,ˆa\nu1(z)ξ1(σ1(z); z),\nwhere ξ1(ˆσ; z) := P\nπ1∈Π1(ˆσ)\nP\nπ2∈Π2(z) µ(π1, π2) and y1,ˆσ(z) := P\nˆπ1∈Π1(z) ˆµ(ˆπ1).\n(A symmetric result holds for Player 2, with symbols ξ2(ˆσ; z) and y2,ˆσ(z).) It now seems natural to perform\na change of variables, and pick distributions for the random variables y1,ˆσ(·), y2,ˆσ(·), ξ1(·; ·) and ξ2(·; ·)\ninstead of µ and ˆµ. Since there are only a polynomial number (in the game tree size) of combinations\nof arguments for these new random variables, this approach allows one to remove the redundancy of\nrealization-equivalent normal-form plans and focus on a signiﬁcantly smaller search space. In fact, the\ndeﬁnition of ξ = (ξ1, ξ2) also appears in (von Stengel & Forges, 2008), referred to as (sequence-form)\ncorrelation plan. In the case of the y1,ˆσ and y2,ˆσ random variables, it is clear that the change of variables is\npossible via the sequence form (von Stengel, 2002); we let Yi,ˆσ be the sequence-form polytope of feasible\nvalues for the vector yi,ˆσ. Hence, the only hurdle is characterizing the space spanned by ξ1 and ξ2 as\nµ varies across the probability simplex. In two-player perfect-recall games with no chance moves, this\nis exactly one of the merits of the landmark work by von Stengel & Forges (2008). In particular, the\nauthors prove that in those games the space of feasible ξ can be captured by a polynomial number of linear\nconstraints. In more general cases the same does not hold (see second half of Section 2), but we prove the\nfollowing (Appendix C):\nLemma 2. In a two-player game, as µ varies over the probability simplex, the joint vector of ξ1(·; ·), ξ2(·; ·)\nvariables spans a convex polytope X in Rn, where n is at most quadratic in the game size.\nSaddle-Point Reformulation\nAccording to Lemma 1, for each Player i and (ˆσ, ˆµ)-trigger agent for them,\nthe value of the trigger agent is a biafﬁne expression in the vectors yi,ˆσ and ξi, and can be written as\nvi,ˆσ(ξi, yi,ˆσ) = ξ⊤\ni Ai,ˆσyi,ˆσ −b⊤\ni,ˆσξi for a suitable matrix Ai,ˆσ and vector bi,ˆσ, where the two terms in the\ndifference correspond to the expected utility for deviating at ˆσ according to the (sequence-form) strategy\nyi,ˆσ and the expected utility for not deviating at ˆσ. Given a correlation plan ξ = (ξ1, ξ2) ∈X, the maximum\nvalue of any deviation for any player can therefore be expressed as\nv∗(ξ) :=\nmax\n{i,ˆσ,yi,ˆσ} vi,ˆσ(ξi, yi,ˆσ) = max\ni∈{1,2} max\nˆσ∈Σi max\nyˆσ∈Yˆσ{ξ⊤\ni Ai,ˆσyi,ˆσ −b⊤\ni,ˆσξi}.\nWe can convert the maximization above into a continuous linear optimization problem by introducing the\nmultipliers λi,ˆσ ∈[0, 1] (one per each Player i ∈{1, 2} and trigger ˆσ ∈Σi), and write\nv∗(ξ) =\nmax\n{λi,ˆσ,zi,ˆσ}\nX\ni\nX\nˆσ\nξ⊤\ni Ai,ˆσzi,ˆσ −λi,ˆσb⊤\ni,ˆσξi,\nwhere the maximization is subject to the linear constraints [C1] P\ni∈{1,2}\nP\nˆσ∈Σi λi,ˆσ = 1 and [C2]\nzi,ˆσ ∈λi,ˆσYi,ˆσ for all i ∈{1, 2}, ˆσ ∈Σi. These linear constraints deﬁne a polytope Y.\nA correlation plan ξ is an EFCE if an only if vi,ˆσ(ξ, yi,ˆσ) ≤0 for every trigger agent, i.e., v∗(ξ) ≤0.\nTherefore, to ﬁnd an EFCE, we can solve the optimization problem minξ∈X v∗(ξ), which is a bilinear\nsaddle point problem over the convex domains X and Y, both of which are convex polytopes that belong to\nRn, where n is at most quadratic in the input game size (Lemma 2). If an EFCE exists, the optimal value\nshould be non-positive and the optimal solution is an EFCE (as it satisﬁes v∗(ξ) ≤0). In fact, since EFCE’s\nalways exist (as EFCEs are supersets of CEs (von Stengel & Forges, 2008)), and one can select triggers to\nbe terminal sequences for Player 1, the optimal value of the BSPP is always 0. The BSPP can be interpreted\nas the NE of a zero-sum game between the mediator, who decides on a suitable correlation plan ξ and a\ndeviator who selects the yi,ˆσ’s to maximize each vi,ˆσ(ξi, yi,ˆσ). The value of this game is always 0.\n5\nARXIV PREPRINT - OCTOBER 27, 2019\nFinally, we can enforce a minimum lower bound τ on the sum of players’ utility by introducing an additional\nvariable λsw ∈[0, 1] and maximizing the new convex objective\nv∗\nsw(ξ) :=\nmax\nλsw∈[0,1]\n(\n(1 −λsw) · v∗(ξ) + λsw\n\"\nτ −\nX\nz∈Z\nu1(z)ξ1(z; z) −\nX\nz∈Z\nu2(z)ξ2(z; z)\n#)\n.\n(1)\n4. Computing an EFCE using Subgradient Descent\n(von Stengel & Forges, 2008) show that a SW-maximizing EFCE of a two-player game without chance may\nbe expressed as the solution of an LP and solved using generic methods such as the simplex algorithm or\ninterior-point methods. However, this does not scale to large games as these methods require storing and\ninverting large matrices. Another way of computing SW-maximizing EFCEs was provided by (Dudik &\nGordon, 2009). However, their algorithm assumes that sampling from correlation plans is possible using the\nMonte Carlo Markov chain algorithm and does not factor in convergence of the Markov chain. Furthermore,\neven though their formulation generalizes beyond our setting of two-player games without chance, our\ngradient descent method admits more complex objectives. In particular, it allows the mediator to maximize\nover general concave objectives (in correlation plans) instead of only linear objectives with potentially\nsome regularization. Here, we showcase the beneﬁts of exploiting the combinatorial structure of the BSPP\nformulation of Section 3 by proposing a simple algorithm based on subgradient descent; in Section 6 we\nshow that this method scales better than commercial state-of-the-art LP solver in large games.\nFor brevity, we only provide a sketch of our algorithm, which computes a feasible EFCE; the extension\nto the slightly more complicated objective v∗\nsw(ξ) (Equation 1) is straightforward—see Appendix D for\nmore details. First, observe that the objective v∗(ξ) is convex since it is the maximum of linear functions\nof ξ. This suggests that we may perform subgradient descent on v∗, where the subgradients are given\nby ∂\/∂ξ v∗(ξ) = Ai∗,ˆσ∗y∗\ni∗,ˆσ∗−bi,ˆσ∗, where (i∗, ˆσ∗, y∗\ni∗,ˆσ∗) is a triplet which maximizes the objective\nfunction v∗(ξ). The computation of such a triplet can be done via a straightforward bottom-up traversal\nof the game tree. In order to maintain feasibility (that is, ξ ∈X), it is necessary to project onto X, which\nis challenging in practice because we are not aware of any distance-generating function that allows for\nefﬁcient projection onto this polytope. This is so even in games without chance (where ξ can be expressed\nby a polynomial number of constraints (von Stengel & Forges, 2008)). Furthermore, iterative methods such\nas Dykstra’s algorithm, add a dramatic overhead to the cost of each iterate.\nTo overcome this hurdle, we observe that in games with no chance moves, the set X of correlation plans—as\ncharacterized by von Stengel & Forges (2008) via the notion of consistency constraints—can be expressed\nas the intersection of three sets: (i) X1, the sets of vectors ξ that only satisfy consistency constraints for\nPlayer 1; (ii) X2, the sets of vectors ξ that only satisfy consistency constraints for Player 2; and (iii) Rn\n+, the\nnon-negative orthant. X1 and X2 are polytopes deﬁned by equality constraints only. Therefore, an exact\nprojection (in the Euclidean sense) onto X1 and X2 can be carried out efﬁciently by precomputing a suitable\nfactorization the constraint matrices that deﬁne X1 and X2. In particular, we are able to leverage the speciﬁc\ncombinatorial structure of the constraints that form X1 and X2 to design an efﬁcient and parallel sparse\nfactorization algorithm (see Appendix D for the full details). Furthermore, projection onto the non-negative\northant can be done conveniently, as it just amounts to computing a component-wise maximum between ξ\nand the zero vector. Since X = X1 ∩X2 ∩Rn\n+, and since projecting onto X1, X2 and Rn\n+ individually is\neasy, we can adopt the recent algorithm proposed by (Wang & Bertsekas, 2013) designed to handle exactly\nthis situation. In that algorithm, gradient steps are interlaced with projections onto X1, X2 and Rn\n+ in a\ncyclical manner. This is similar to projected gradient descent, but instead of projecting onto the intersection\nof X1, X2 and Rn\n+ (which we believe to be difﬁcult), we project onto just one of them in round-robin fashion.\nThis simple method was shown to converge by (Wang & Bertsekas, 2013). However, no convergence bound\nis currently known.\n5. Introducing the First Benchmarks for EFCE\nIn this section we introduce the ﬁrst two benchmark games for EFCE. These games are naturally parametric\nso that they can scale in size as desired and hence used to evaluate different EFCE solvers. In addition,\nwe show that the EFCE in these games are interesting behaviorally: the correlation plan in social-welfare-\n6\nARXIV PREPRINT - OCTOBER 27, 2019\nmaximizing EFCE is highly nontrivial and even seemingly counter-intuitive. We believe some of these\ninduced behaviors may prove practical in real-world scenarios and hope our analysis can spark an interest\nin EFCEs and other equilibria in sequential settings.\n5.1. Battleship: Conﬂict Resolution via a Mediator\nIn this section we introduce our ﬁrst proposed benchmark game to illustrate the power of correlation in\nextensive-form games. Our game is a general-sum variant of the classic game Battleship. Each player takes\nturns to secretly place a set of ships S (of varying sizes and value) on separate grids of size H × W. After\nplacement, players take turns ﬁring at their opponent—ships which have been hit at all the tiles they lie\non are considered destroyed. The game continues until either one player has lost all of their ships, or each\nplayer has completed r shots. At the end of the game, the payoff of each player is computed as the sum of\nthe values of the opponent’s ships that were destroyed, minus γ times the value of ships which they lost,\nwhere γ ≥1 is called the loss multiplier of the game. The social welfare (SW) of the game is the sum of\nutilities to all players.\nIn order to illustrate a few interesting feature of social-welfare-maximizing EFCE in this game, we will\nfocus on the instance of the game with a board of size 3 × 1, in which each player commands just 1 ship\nof value and length 1, there are 2 rounds of shooting per player, and the loss multiplier is γ = 2. In this\ngame, the social-welfare-maximizing Nash equilibrium is such that each player places their ship and shoots\nuniformly at random. This way, the probability that Player 1 and 2 will end the game by destroying the\nopponent’s ship is 5\/9 and 1\/3 respectively (Player 1 has an advantage since they act ﬁrst). The probability\nthat both players will end the game with their ships unharmed is a meagre 1\/9. Correspondingly, the\nmaximum SW reached by any NE of the game is −8\/9.\nIn the EFCE model, it is possible to induce the players to end the game with a peaceful outcome—that is, no\ndamage to either ship—with probability 5\/18, 2.5 times of the probability in NE, resulting in a much-higher\nSW of −13\/18. Before we continue with more details as to how the mediator (correlation device) is able to\nachieve this result in the case where γ = 2, we remark that the beneﬁt of EFCE is even higher when the\nloss multiplier γ increases: Figure 1 (left) shows, as a function of γ, the probability with which Player 1\nand 2 terminate the game by sinking their opponent’s ship, if they play according to the SW-maximizing\nEFCE. For all values of γ, the SW-maximizing NE remains the same while with a mediator, the probability\nof reaching a peaceful outcome increases as γ increases, and asymptotically gets closer to 1\/3 and the\ngap between the expected utility of the two players vanishes. This is remarkable, considering Player 1’s\nadvantage for acting ﬁrst.\n1\n2\n3\n4 5 6\n8 10\n20\n30 40\n60\n100\n0.2\n0.3\n0.4\n0.5\nPlayer 2\nPlayer 1\nNo sunken ship\nShip loss value (γ)\nProbability\nPl.1 in cell a, Pl.2 in cell a\nSh. b\nSh. a\nSh. c\nSh. b\nSh. a\nSh. c\nSh. a\nSh. c\nSh. c\nSh. a\nSh. c\nSh. b\nSh. a\nSh. b\nSh. a\nSh. b\nSh. a\nSh. c\nSh. c\nSh. a\nSh. b\nSh. b\nSh. a\n25\/54\n2\/27\n25\/54\n1\/2\n1\/2\n2\/5\n3\/5\n2\/5\n3\/5\n1\/2\n1\/2\n1\/2\n1\/2\n1\/2\n1\/2\n2\/5\n3\/5\n2\/5\n3\/5\n1\/2\n1\/2\n1\/2\n1\/2\nFigure 1: (Left) Probabilities of players sinking their opponent when the players play according to the SW-maximizing\nEFCE. For γ ≥2, the probability of the game ending with no sunken ship and the probability of Player 2 sinking Player\n1 coincide. (Right) Example of a playthrough of Battleship assuming both players are recommended to place their ship\nin the same position a. Edge labels represents the probability of an action being recommended. Squares and hexagons\ndenote actions taken by Players 1 and 2 respectively. Blue and red nodes represent cases where Players 1 and 2 sink\ntheir opponent, respectively. The Shoot action is abbreviated ‘Sh.’.\nWe now resume our analysis of the SW-maximizing EFCE in the instance where γ = 2. In a nutshell, the\ncorrelation plan is constructed in a way that players are recommended to deliberately miss, and deviations\n7\nARXIV PREPRINT - OCTOBER 27, 2019\nfrom this are punished by the mediator, who reveals to the opponent the ship location that was recommended\nto the deviating player. First, the mediator recommends the players a ship placement that is sampled\nuniformly at random and independently for each players. This results in 9 possible scenarios (one per\npossible ship placement) in the game, each occurring with probability 1\/9. Due to the symmetric nature of\nship placements, only two scenarios are relevant: whether the two players are recommended to place their\nship in the same spot, or in different spots. Figure 1 (right) shows the probability of each recommendation\nfrom the mediator in the former case, assuming that the players do not deviate. The latter case is symmetric\n(see Appendix E for details). Now, we explain the ﬁrst of the two methods in which the mediator compels\nnon-violent behavior. We focus on the ﬁrst shot made by Player 1 (i.e., the root in Figure 3). The mediator\nsuggests that Player 1 shoot at the Player 2’s ship with a low 2\/27 probability, and deliberately miss with\nhigh probability. One may wonder how it is possible for this behavior to be incentive-compatible (that is,\nwhat are the incentives that compel Player 1 into not defecting), since the player may choose to randomly\nﬁre in any of the 2 locations that were not recommended, and get almost 1\/2 chance of winning the game\nimmediately. The key is that if Player 1 does so and does not hit the opponent’s ship, then the mediator\ncan punish him by recommending that Player 2 shoot in the position where Player 1’s was recommended\nto place their ship. Since players value their ships more than destroying their opponents’, the player is\nincentivized to avoid such a situation by accepting the recommendation to (most probably) miss. We see\nthe ﬁrst example of deterrent used by the mediator: inducing the opponent to play punitive actions against\nplayers that have deviated from the recommendation, if ever that deviation can be detected from the player.\nA similar situation arises in the ﬁrst move of Player 2, where Player 2 is recommended to deliberately miss,\nhitting each of the 2 empty spots with probability 1\/2. A more detailed analysis is available in Appendix E.\n5.2. Sheriff: Bargaining and Negotiation\nOur second proposed benchmark is a simpliﬁed version of the Sheriff of Nottingham board game. The\ngame models the interaction of two players: the Smuggler—who is trying to smuggle illegal items in their\ncargo—and the Sheriff—who is trying to stop the Smuggler. At the beginning of the game, the Smuggler\nsecretly loads his cargo with n ∈{0, . . . , nmax} illegal items. At the end of the game, the Sheriff decides\nwhether to inspect the cargo. If the Sheriff chooses to inspect the cargo and ﬁnds illegal goods, the Smuggler\nmust pay a ﬁne worth p · n to the Sheriff. On the other hand, the Sheriff has to compensate the Smuggler\nwith a utility s if no illegal goods are found. Finally, if the Sheriff decides not to inspect the cargo, the\nSmuggler’s utility is v · n whereas the Sheriff’s utility is 0. The game is made interesting by two additional\nelements (which are also present in the board game): bribery and bargaining. After the Smuggler has loaded\nthe cargo and before the Sheriff chooses whether or not to inspect, they engage in r rounds of bargaining. At\neach round i = 1, . . . , r, the Smuggler tries to tempt the Sheriff into not inspecting the cargo by proposing\na bribe bi ∈{0, . . . bmax}, and the Sheriff responds whether or not they would accept the proposed bribe.\nOnly the proposal and response from round r will be executed and have an impact on the ﬁnal payoffs—that\nis, all but the r-th round of bargaining are non-consequential and their purpose is for the two players to\nsettle on a suitable bribe amount. If the Sheriff accepts bribe br, then the Smuggler gets p · n −br, while\nthe Sheriff gets br. See Appendix F for a formal description of the game.\nWe now point out some interesting behavior of EFCE in this game. We refer to the game instance where\nv = 5, p = 1, s = 1, nmax = 10, bmax = 2, r = 2 as the baseline instance.\nEffect of v, p and s. First, we show what happens in the baseline instance when the item value v, item\npenalty p, and Sheriff compensation (penalty) s are varied in isolation over a continuous range of values.\nThe results are shown in Figure 2. In terms of general trends, the effect of the parameter to the Smuggler is\nfairly consistent with intuition: the Smuggler beneﬁts from a higher item value as well as from higher sheriff\npenalties, and suffers when the penalty for smuggling is increased. However, the ﬁner details are much more\nnuanced. For one, the effect of changing the parameters not only is non-monotonic, but also discontinuous.\nThis behavior has never been documented and we ﬁnd it rather counterintuitive. More counterintuitive\nobservations can be found in Appendix F. Effect of nmax, bmax, and r. Here, we try to empirically\nunderstand the impact of n and b on the SW maximizing equilibrium. As before we set v = 5, p = 1, s = 1\nand vary n and r simultaneously while keeping bmax constant. The results are shown in Table 1. The most\nstriking observation is that increasing the capacity of the cargo nmax may decrease social welfare. For\n8\nARXIV PREPRINT - OCTOBER 27, 2019\n100\n101\n102\n10−1\n100\n101\nSmuggler\nSheriff\nIllegal item value (v)\nUtility\nSheriff game with varying\nillegal item value\n0\n1\n2\n3\n4\n5\n10−1\n100\n101\nSheriff\nSheriff\nSmuggler\nSmuggler\nIllegal item penalty (p)\nSheriff game with varying\nillegal item penalty\n101\n102\n100\n101\nSheriff\nSmuggler\nSheriff penalty (s)\nSheriff game with varying sheriff penalty\n(upon inspection of a cargo with no illegal items)\nFigure 2: Utility of players with varying v, p and s for the SW-maximizing EFCE. We veriﬁed that these plots are not\nthe result of equilibrium selection issues.\nexample, consider the case when bmax = 2, nmax = 2, r = 1 (shown in blue in Table 1, right) where the\npayoffs are (8.0, 2.0). This achieves the maximum attainable social welfare by smuggling nmax = 2 items\nand having the Sheriff accept a bribe of 2. When nmax is increased to 5 (red entry in the table), the payoffs\nto both players drop signiﬁcantly, and even more so when nmax increases further. While counter-intuitive,\nthis behavior is consistent in that the Smuggler may not beneﬁt from loading 3 items every time he was\nrecommended to load 2; the Sheriff reacts by inspecting more, leading to lower payoffs for both players.\nnmax\nr = 1\nr = 2\nr = 3\n1\n(3.00, 2.00)\n(3.00, 2.00)\n(3.00, 2.00)\n2\n(8.00, 2.00)\n(8.00, 2.00)\n(8.00, 2.00)\n5\n(2.28, 1.26)\n(8.00, 2.00)\n(8.00, 2.00)\n10\n(1.76, 0.93)\n(7.26, 1.82)\n(8.00, 2.00)\nTable 1: Payoffs for (Smuggler, Sheriff) in the\nSW-maximizing EFCE.\nThat behavior is avoided by increasing the number of rounds\nr: by increasing to r = 2 (entry shown in purple), the be-\nhavior disappears and we revert to achieving a social welfare\nof 10 just like in the instance with nmax = 2, r = 1. With\nsufﬁcient bargaining steps, the Smuggler, with the aid of\nthe mediator, is able to convince the Sheriff that they have\ncomplied with the recommendation by the mediator. This\nis because the mediator spends the ﬁrst r −1 bribes to give\na ‘passcode’ to the Smuggler so that the Sheriff can verify\ncompliance—if an ‘unexpected’ bribe is suggested, then the Smuggler must have deviated, and the Sheriff\nwill inspect the cargo as punishment. With more rounds, it is less likely that the Smuggler will guess the\ncorrect passcode. See also Appendix F for additional insights.\n6. Experimental Evaluation\nEven our proof-of-concept algorithm based on the BSSP formulation and subgradient descent, introduced in\nSection 3, is able to beat LP-based approaches using the commercial solver Gurobi (Gurobi Optimization,\n2018) in large games. This conﬁrms known results about the scalability of methods for computing NE,\nwhere in the recent years ﬁrst-order methods have afﬁrmed themselves as the only algorithms that are able\nto handle large games.\nWe experimented on Battleship over a range of parameters while ﬁxing γ = 2. All experiments were run on\na machine with 64 cores and 500GB of memory. For our method, we tuned step sizes based on multiples of\n10. In Table 2, we report execution times when all constraints (feasibility and deviation) are violated by\nno greater than 10−1, 10−2 and 10−3. Our method outperforms the LP-based approach for larger games.\nHowever, while we outperform the LP-based approach for accuracies up to 10−3, Gurobi spends most of its\ntime reordering variables and preprocessing and its solution converges faster for higher levels of precision;\nthis is expected of a gradient-based method like ours. On very large games with more than 100 million\nvariables, both our method and Gurobi fail—in Gurobi’s case, it was due to a lack of memory while in our\ncase, each iteration required nearly an hour which was prohibitive. The main bottleneck in our method was\nthe projection onto X1 and X2. We also experimented on the Sheriff game and obtained similar ﬁndings\n(Appendix H).\n9\nARXIV PREPRINT - OCTOBER 27, 2019\n(H, W)\nr\nShip\n#Actions\n#Relevant\nTime (LP)\nTime (ours)\nlength\nPl 1\nPl 2\nseq. pairs\n10−1\n10−2\n10−3\n10−1\n10−2\n10−3\n(2, 2)\n3\n1\n741\n917\n35241\n2s\n2s\n2s\n1s\n2s\n3s\n(3, 2)\n3\n1\n15k\n47k\n3.89M\n3m 6s\n3m 17s\n3m 24s\n8s\n34s\n52s\n(3, 2)\n4\n1\n145k\n306k\n26.4M\n42m 39s 42m 44s\n43m\n2m 48s 14m 1s 23m 24s\n(3, 2)\n4\n2\n970k 2.27M\n111M\n— out of memory† —\n— did not achieve ‡ —\nTable 2: #Relevant seq. pairs is the dimension of ξ under the compact representation of (von Stengel & Forges, 2008).\nFor LPs, we report the fastest of Barrier, Primal and Dual Simplex, and 3 different formulations (Appendix G). † Gurobi\nwent out of memory and was killed by the system after ∼3000 seconds ‡ Our method requires 1 hour per iteration and\ndid not achieve the required accuracy after 6 hours.\n7. Conclusions\nIn this paper, we proposed two parameterized benchmark games in which EFCE exhibits interesting\nbehaviors. We analyzed those behaviors both qualitatively and quantitatively, and isolated two ways through\nwhich a mediator is able to compel the agents to follow the recommendations. We also provided an\nalternative saddle-point formulation of EFCE and demonstrated its merit with a simple subgradient method\nwhich outperforms standard LP based methods.\nWe hope that our analysis will bring attention to some of the computational and practical uses of EFCE,\nand that our benchmark games will be useful for evaluating future algorithms for computing EFCE in large\ngames.\nAcknowledgments\nThis material is based on work supported by the National Science Foundation under grants IIS-1718457,\nIIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Gabriele Farina is\nsupported by a Facebook fellowship. Co-authors Ling and Fang are supported in part by a research grant\nfrom Lockheed Martin.\nReferences\nAshlagi, I., Monderer, D., and Tennenholtz, M. On the value of correlation. Journal of Artiﬁcial Intelligence\nResearch, 33:575–613, 2008.\nAumann, R. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics, 1:\n67–96, 1974.\nBowling, M., Burch, N., Johanson, M., and Tammelin, O. Heads-up limit hold’em poker is solved. Science,\n2015.\nBrown, N. and Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals.\nScience, Dec. 2017.\nBrown, N. and Sandholm, T. Solving imperfect-information games via discounted regret minimization. In\nAAAI, 2019a.\nBrown, N. and Sandholm, T. Superhuman AI for multiplayer poker. Science, 365(6456):885–890, 2019b.\nISSN 0036-8075. doi: 10.1126\/science.aay2400. URL https:\/\/science.sciencemag.org\/\ncontent\/365\/6456\/885.\nCrawford, V. P. and Sobel, J. Strategic information transmission. Econometrica: Journal of the Econometric\nSociety, pp. 1431–1451, 1982.\nDodis, Y., Halevi, S., and Rabin, T. A cryptographic solution to a game theoretic problem. In Annual\nInternational Cryptology Conference, pp. 112–130. Springer, 2000.\n10\nARXIV PREPRINT - OCTOBER 27, 2019\nDudik, M. and Gordon, G. J. A sampling-based approach to computing equilibria in succinct extensive-form\ngames. In UAI, pp. 151–160. AUAI Press, 2009.\nFarina, G., Kroer, C., and Sandholm, T. Online convex optimization for sequential decision processes and\nextensive-form games. In AAAI Conference on Artiﬁcial Intelligence, 2019.\nGordon, G. J., Greenwald, A., and Marks, C. No-regret learning in convex games. In Proceedings of the\n25th international conference on Machine learning, pp. 360–367. ACM, 2008.\nGurobi Optimization, L. Gurobi optimizer reference manual, 2018. URL http:\/\/www.gurobi.com.\nHazan, E. Introduction to online convex optimization. Foundations and Trends in Optimization, 2016.\nHuang, W. Equilibrium computation for extensive games. PhD thesis, London School of Economics and\nPolitical Science, January 2011.\nHuang, W. and von Stengel, B. Computing an extensive-form correlated equilibrium in polynomial time. In\nInternational Workshop On Internet And Network Economics (WINE), pp. 506–513. Springer, 2008.\nJiang, A. X. and Leyton-Brown, K. Polynomial-time computation of exact correlated equilibrium in\ncompact games. Games and Economic Behavior, 91:347–359, 2015.\nKoutsoupias, E. and Papadimitriou, C. Worst-case equilibria. In Symposium on Theoretical Aspects in\nComputer Science, 1999.\nLeyton-Brown, K. Personal communication, 2019.\nMoravˇcík, M., Schmid, M., Burch, N., Lisý, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M.,\nand Bowling, M. Deepstack: Expert-level artiﬁcial intelligence in heads-up no-limit poker. Science,\n2017.\nNash, J. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36:\n48–49, 1950.\nNemirovski, A. Prox-method with rate of convergence O(1\/t) for variational inequalities with Lipschitz\ncontinuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on\nOptimization, 2004.\nNesterov, Y. Excessive gap technique in nonsmooth convex minimization. SIAM Journal of Optimization,\n2005.\nPapadimitriou, C. H. and Roughgarden, T. Computing correlated equilibria in multi-player games. Journal\nof the ACM, 55(3):14, 2008.\nRay, I. and Gupta, S. S. Technical Report, 2009.\nRoughgarden, T. and Tardos, É. How bad is selﬁsh routing? Journal of the ACM (JACM), 49(2):236–259,\n2002.\nShoham, Y. and Leyton-Brown, K. Multiagent systems: Algorithmic, game-theoretic, and logical founda-\ntions. Cambridge University Press, 2009.\nvon Stengel, B. Efﬁcient computation of behavior strategies. Games and Economic Behavior, 1996.\nvon Stengel, B. Computing equilibria for two-person games. In Aumann, R. and Hart, S. (eds.), Handbook\nof game theory, volume 3. North Holland, Amsterdam, The Netherlands, 2002.\nvon Stengel, B. and Forges, F. Extensive-form correlated equilibrium: Deﬁnition and computational\ncomplexity. Mathematics of Operations Research, 33(4):1002–1022, 2008.\nWang, M. and Bertsekas, D. P. Incremental constraint projection-proximal methods for nonsmooth convex\noptimization. SIAM J. Optim.(to appear), 2013.\nZinkevich, M., Bowling, M., Johanson, M., and Piccione, C. Regret minimization in games with incomplete\ninformation. In NIPS, 2007.\n11\nARXIV PREPRINT - OCTOBER 27, 2019\nA. Recovering the Linear Program of (von Stengel & Forges, 2008)\nRecall the continuous version of the primal version of the inner maximization problem which was obtained\nby adding the multipliers λi,ˆσ ∈[0, 1].\nmax\nλ,zi,ˆσ\nX\ni∈{1,2}\nX\nˆσ∈Σi\nξ⊤\ni Ai,ˆσzi,ˆσ −λi,ˆσb⊤\ni,ˆσξi\nsuch that\nX\ni∈{1,2}\nX\nˆσ∈Σi\nλi,ˆσ = 1\nλi,ˆσ ≥0\nzi,ˆσ ∈λi,ˆσYi,ˆσ,\n∀i ∈{1, 2}, ˆσ ∈Σi\nwhere zi,ˆσ may be seen as the sequence form representation of a game rooted at a particular information set\nof player i, and scaled by the factor λi,ˆσ. By expanding the sequence form constraints which deﬁne Yi,ˆσ,\nwe get\nmax\nλ,z\nX\ni∈{1,2}\nX\nˆσ∈Σi\nξ⊤\ni Ai,ˆσzi,ˆσ −λi,ˆσb⊤\ni,ˆσξi\nsuch that\nX\ni∈{1,2}\nX\nˆσ∈Σi\nλi,ˆσ = 1\nλi,ˆσ ≥0\nzi,ˆσ ≥0\nFi,ˆσzi,ˆσ −λˆσfi,ˆσ = 0,\n∀i ∈{1, 2}, ˆσ ∈Σi\nwhere Fi,ˆσ and fi,ˆσ are sequence form constraint matrices rooted at the information set ˆI containing ˆσ, with\nthe only difference that instead of having the ‘empty sequence’ be equal to 1, we require that all actions\nbelonging to ˆI sum to λi,ˆσ. We are now in a position to take duals; the only non-zero elements on the right\nhand side of the constraints are from the sum-to-one constraints over λi,ˆσ. This give s the following dual\nmin\nu,νi(ˆσ,·)\nu\nsuch that\nF T\ni,ˆσνi(ˆσ, ·) ≥AT\ni,ˆσξi\n∀i ∈{1, 2}, ˆσ ∈Σi\nu −νi(ˆσ, ˆI) ≥−bT\ni,ˆσξi\n∀i ∈{1, 2}, ˆσ = (ˆI, ˆa) ∈Σi,\nwhere u and ν(ˆσ, ·) are free in sign. Combining this with the outer minimization over ξi gives us the linear\nprogram by (von Stengel & Forges, 2008), up to a change in variable names and conventions.\nB. Derivation of Probabilities over Terminal States\nIn order to express the utility of a trigger agent, it is necessary to compute the probability of the game\nending in each of the terminal states. Before that, we will review the notation introduced in earlier sections\nin more detail.\n• Z be the set of terminal states (or equivalently, outcomes) in the game, and z ∈Z is some terminal state.\n• ui(z) be the utility obtained by player i if the game terminates at some terminal state z ∈Z.\n• Πi be the set of pure reduced-normal-form strategies for Player i. We also require notation for subsets of\nΠi, namely,\n– Πi(I), is the set of reduced-normal-form strategies that can lead to information set I (which belongs\nto player i) assuming that the other player acts to do so as well. This is equivalent (assuming no\nzero-chance nodes or disconnected game trees) to saying that all reduced-normal-from strategies in\nΠi(I) have some action which belongs to information set I.\n– Πi(I, a) is the set of reduced normal form strategies which will lead to information set I and\nrecommend the action a in I. This is equivalent to the set of reduced normal form strategies which\ncontain a as part of their recommendation (this set is typically a subset of Πi(I, a)).\n12\nARXIV PREPRINT - OCTOBER 27, 2019\n– Πi(z) is the set of reduced-normal-form strategies which can lead to the terminal state z (assuming\nthe other player players to do so). This is equivalent to the set of reduced-normal-form strategies\nwhich contain the σ = (I, a) pair where σ = (I, a) is the unique last information set-action pair\nwhich has to be encountered by player i before the terminal state z.\n• Σi the set of information set-action pairs (I, a) (also known as sequences), where I is an information set\nfor Player i and a is an action at set I.\n• σi(z) is the last (I, a) pair belonging to Player i encountered before some terminal state z ∈Z.\nWe are interested in characterizing the random variable tˆσ : Π1 × Π2 × Π1(ˆI) →Z that maps a triple of\nreduced-normal-form strategies (π1, π2, ˆπ1) to the terminal state of the game that is reached when Player 1\nis a ˆσ-trigger agent and Player 2 follows all recommendations. That is, we want to ﬁnd the probabillity\nof terminating at each z ∈Z for a ˆσ-trigger agent, given the mediator’s joint distribution µ over reduced\nnormal form strategies and the trigger strategy ˆµ for the deviating player, which we will assume to be Player\n1 without loss of generality. For each trigger ˆσ, the terminal leaves may be partitioned into the following 3\nsets.\n• Zˆσ (or equivalently ZˆI,ˆa) is the set of terminal nodes that are descendants of the trigger ˆσ = (ˆI, ˆa).\nIn order for the game to end in one of these terminal nodes, it is necessary that the recommendation\ndevice recommended to Player 1 the trigger sequence ˆσ, and therefore the agent must have deviated.\nFurthermore, Player 2 must have been recommended the terminal sequence σ2(z) corresponding to\nthe terminal state, and ﬁnally ˆπ1 must be compatible with σ1(z). We can capture all these constraints\nconcisely by saying that the sampled (π1, π2, ˆπ1) must be such that π1 ∈Π1(ˆσ), π2 ∈Π2(z) and\nˆπ1 ∈Π1(z). Therefore the probability that a ˆσ trigger agent terminates at some z ∈Zˆσ is given by,\nPµ,ˆµ[tˆσ = z ∈Zˆσ] =\n\n\n\n\nX\nπ1∈Π1(ˆσ)\nπ2∈Π2(z)\nµ(π1, π2)\n\n\n\n\n\n\nX\nˆπ1∈Π1(z)\nˆµ1(ˆπ1)\n\n,\nwhere the ﬁrst term in the product is the probability that Player 2 plays to z and Player 1 gets triggered,\nand the second term is the probability that the deviation strategy from Player 1 upon getting triggered is\none that reaches z.\n• ZˆI is the set of terminal states that are descendant of any sequence in ˆI, except ˆσ. In order for the game\nto reach this terminal state, recommendations issued to Player 1 by the correlation device must have been\nsuch that Player 1 reached ˆI. There are two cases: either the correlation device recommended ˆσ at ˆI,\nor it did not. In the former case, Player 1 started deviating (using the sampled reduced-normal-form\nplan ˆπ1); hence, in this case it must be ˆπ1 ∈Π1(z). In the latter case, Player 1 does not deviate\nfrom the recommendation, and therefore it must be π1 ∈Π1(z). Either way, Player 2 must have\nbeen recommended the terminal sequence z corresponding to the terminal state z; that is, π2 ∈Π2(z).\nCollecting all these constraints, it must be\n(π1, π2, ˆπ1) ∈Π1(ˆσ) × Π2(z) × Π1(z) ∪Π1(z) × Π2(z) × Π1(ˆI).\nUsing the fact that the two cases as to whether or not Player 1 was recommended ˆσ or not at ˆI are disjoint,\nwe can write\nPµ,ˆµ[tˆσ = z ∈ZˆI] =\n\n\n\n\nX\nπ1∈Π1(ˆσ)\nπ2∈Π2(z)\nµ(π1, π2)\n\n\n\n\n\n\nX\nˆπ1∈Π1(z)\nˆµ1(ˆπ1)\n\n+\n\n\n\n\nX\nπ1∈Π1(z)\nπ2∈Π2(z)\nµ(π1, π2)\n\n\n\n.\nThe ﬁrst term in the summation may be understood as the probability that the agent was triggered and its\ndeviation was to play something other than ˆσ. The second term is that probability that the agent was not\ntriggered and the game simply terminates at z based on µ.\n• Finally, Z−ˆI is the set of terminal nodes that are neither in Zˆσ nor in ZˆI. If the game has ended\nin any terminal state that belongs to Z−ˆI, Player 1 has not deviated from the recommended strategy,\n13\nARXIV PREPRINT - OCTOBER 27, 2019\nsince they have never even reached the trigger information set, ˆI. Hence, in this case it must be\n(π1, π2) ∈Π1(z) × Π2(z). Hence,\nPµ,ˆµ[tˆσ = z ∈Z−ˆI] =\nX\nπ1∈Π1(z)\nπ2∈Π2(z)\nµ(π1, π2).\nWith the above, we can ﬁnally express the constraint that no deviation strategy ˆµ can lead to a higher utility\nfor Player 1 than simply following each recommendation. Indeed, for all ˆµ, the utility of the trigger agent is\nexpressed as\nX\nz∈Z\nu1(z)Pµ,ˆµ[tˆσ = z],\nwhere the correct expression for Pµ,ˆµ[tˆσ = z] must be selected depending on whether z ∈Zˆσ, z ∈ZˆI or\nz ∈Z−ˆI. On the other hand, the utility of an agent that follows all recommendations is\nX\nz∈Z\nu1(z)Pµ,ˆµ[π1 ∈Π1(z), π2 ∈Π2(z)] =\nX\nz∈Z\n\n\n\nu1(z)\nX\nπ1∈Π1(z)\nπ2∈Π2(z)\nµ(π1, π2)\n\n\n\n.\nTherefore, following all recommendations is a best response for the ˆσ-trigger agent if and only if µ is\nchosen so that\nX\nz∈Z\nu1(z)\n\n\n\nPµ,ˆµ[tˆσ = z] −\nX\nπ1∈Π1(z)\nπ2∈Π2(z)\nµ(π1, π2)\n\n\n\n≤0\n∀ˆµ ∈∆|Π1(ˆI)|.\n(2)\nThe crucial observation is that all the probabilities Pµ,ˆµ[t = z] deﬁned above can be expressed via the\nfollowing quantities:\ny1,ˆσ(z) :=\nX\nˆπ1∈Π1(z)\nˆµ1(ˆπ1)\n∀z ∈Z;\nξ1(σ1; z) :=\nX\nπ1∈Π1(σ1)\nπ2∈Π2(z)\nµ(π1, π2)\n∀σ1 ∈Σ1, z ∈Z.\nFor example, for all z ∈ZˆI we can write\nPµ,ˆµ[tˆσ = z] = ξ1(ˆσ; z)yi,ˆσ(z) + ξ1(σ1(z); z).\nWhen deviations relative to Player 2 are brought into the picture, the following two sets of symmetric\nquantities also become relevant:\ny2,ˆσ(z) :=\nX\nˆπ2∈Π2(z)\nˆµ1(ˆπ2)\n∀z ∈Z;\nξ2(σ2; z) :=\nX\nπ1∈Π1(z))\nπ2∈Π2(σ2)\nµ(π1, π2)\n∀σ2 ∈Σ2, z ∈Z.\nIt would now seem natural to perform a change of variables, and pick (correlated) distributions for the\nrandom variables y1,ˆσ(·), y2,ˆσ(·), ξ1(·; ·) and ξ2(·; ·) instead of µ, ˆµ1 and ˆµ2. Since there are only a\npolynomial number (in the game tree size) of combinations of arguments for these new random variables,\nthis approach would allow one to remove the redundancy of realization-equivalent normal-form plans and\nfocus on a polynomially-small search space. In the case of the random variables y1,ˆσ and y2,ˆσ, it is clear\nthat the change of variables is possible via the sequence form (von Stengel, 2002). Therefore, the only\ndifﬁculty is in characterizing the space spanned by ξ1 and ξ2 as µ varies across the probability simplex.\nIn two-player perfect-recall games with no chance moves, this is exactly the merit of the landmark work\nby von Stengel & Forges (2008). In particular, the authors prove that in those games the space of feasible\nξ1, ξ2 can be captured by a polynomial number of linear constraints.\n14\nARXIV PREPRINT - OCTOBER 27, 2019\nC. Proof of Lemma 2\nThe vectors of entries ξ1(·; ·), ξ2(·; ·) are obtained from µ via a linear mapping. Hence, the set of values\nthat can be assumed by ξ is the image of the probability simplex via a linear mapping. Since images of\npolytopes via linear functions are polytopes, the lemma holds.\nD. Details of Our Subgradient Method\nFirst, observe that the objective v∗(ξ) is convex since it is the maximum of linear functions of ξ. This\nsuggests that we may perform subgradient descent on v∗, where the subgradients are given by\n∂\/∂ξ v∗(ξ) = Ai∗,ˆσ∗y∗\ni∗,ˆσ∗−bi,ˆσ∗,\n(3)\nwhere (i∗, ˆσ∗, y∗\ni∗,ˆσ∗) is a triplet which maximizes the objective function v∗(ξ). The computation of such a\ntriplet is a straightforward bottom-up traversal of the game tree. In order to maintain feasibility (that is,\nξ ∈X), it is necessary to project onto X, which is challenging in practice, because we are not aware of any\ndistance-generating function which allows for efﬁcient projection onto this polytope. This is so even in\ngames without chance (where ξ can be expressed by a polynomial number of constraints (von Stengel &\nForges, 2008)). Furthermore, iterative methods such as Dykstra’s algorithm, add an dramatic overhead to\nthe cost of each iterate.\nTo overcome this hurdle, we observe that in games with no chance moves, the set X of correlation plans—as\ncharacterized by von Stengel & Forges (2008) via the notion of consistency constraints—can be expressed\nas the intersection of three sets: (i) X1, the sets of vectors ξ that only satisfy consistency constraints for\nPlayer 1; (ii) X2, the sets of vectors ξ that only satisfy consistency constraints for Player 2, respectively; and\n(iii) Rn\n+, the non-negative orthant. X1 and X2 are polytopes deﬁned by equality constraints only. Therefore,\nan exact projection (in the Euclidean sense) onto X1 and X2 can be carried out efﬁciently by precomputing\na suitable factorization the constraint matrices that deﬁne X1 and X2. In particular, we are able to leverage\nthe speciﬁc combinatorial structure of the constraints that form X1 and X2 to design an efﬁcient and parallel\nsparse factorization algorithm (see Appendix D for the full details). Furthermore, projection onto the\nnon-negative orthant can be done conveniently, as it just amounts to computing a component-wise maximum\nbetween ξ and the zero vector. Since X = X1 ∩X2 ∩Rn\n+, and since projecting onto X1, X2 and Rn\n+\nindividually is easy, we can adopt the recent algorithm proposed by (Wang & Bertsekas, 2013) designed to\nhandle exactly this situation. In that algorithm, gradient steps are interlaced with projections onto X1, X2\nand Rn\n+ in a cyclical manner. This is similar to projected gradient descent, but instead of projecting onto\nthe intersection of X1, X2 and Rn\n+ (which we believe to be difﬁcult), we project onto just one of them in\nround-robin fashion. This simple method was shown to converge by (Wang & Bertsekas, 2013), however,\nno convergence bound is currently known.\nD.1. Factorization of constraints over X\nvon Stengel & Forges (2008) showed that a ξ may be represented compactly as a 2-dimensional matrix,\nwith dimensions equal to the sequence form representation (von Stengel, 1996) of each player, where one\nis only interested in entries corresponding to relevant sequence pairs (von Stengel & Forges (2008) for\ndetails). Then, the aforementioned constraints (i) and (ii) deﬁning X1 and X2 are equivalent to the sequence\nform constraints for each row and column respectively. Constraint (iii) ensures that the entries of ξ are\nnon-negative and that the entry for the empty sequence pair is 1.\nObserve that projection (based on L2 distance) on X1 and X2 individually can be decomposed a series\nof disjoint projections (either on rows or columns) and thus computed in parallel. We now show that L2-\nprojection of each individual row\/column over the sequence form constraints (von Stengel & Forges, 2008)\nmay be done efﬁciently. Let F and f be matrices and vectors corresponding to the sequence form constraints\nFx −f = 0. Here, F is a (sparse) matrix of size #information sets × #sequences which contains entries\nin {−1, 0, 1} and f is a vector containing 1’s or 0’s. Each information set in F corresponds to the ‘ﬂow’\nconstraints for an information set, with a coefﬁcient of −1 for the unique parent sequence leading to that\n15\nARXIV PREPRINT - OCTOBER 27, 2019\ninformation set, and a coefﬁcient of −1 for all sequences immediately following that information set. 4\nGiven a vector w, the projection onto the afﬁne space given by Fx −f is given by the optimization problem\nmin\nx\n1\n2∥x −w∥2\n2\ns.t.\nFx −f = 0\nThe closed form solution may be found using Lagrange multipliers, and is given by\nx∗= F T (FF T )−1(f −Fw) + w,\nSince F is sparse, the main difﬁculty in computing x∗is overcome if we can efﬁciently compute (FF T )−1q\nfor any vector q.\nLemma 3. Let F be the sequence form constraint matrix. Computing (FF T )−1q may be done efﬁciently.\nProof. The key here is to exploit the structure of FF T . Observe that FF T is symmetric, positive-deﬁnite\nand has dimension equal to the number of information sets. Furthermore, FF T may be expressed in closed\nform:\n(FF T )ij =\n\n\n\n\n\n\n\n\n\n−1\ni is the direct parent\/child of j\n1,\ni is the sibling of j\n1 + # actions at i,\ni = j\n0,\notherwise\n,\nwhere i, j above are information sets, and i being the parent of j means that there is some action in i which\ncan lead to information set j (without any other information set from the same player in between), and\ni being the sibling of j means that the (unique) sequence leading to i and j are the same. Observe that\nFF T is almost, but not quite tree-structured. However, it is sparse and more importantly, has 0 ﬁll-in if we\norder variables in a bottom-up fashion in the player’s game tree. That is, we treat FF T as a graph with\ninformation sets as vertices, then repeatedly remove vertices (information sets) in a bottom-up fashion,\nwhile forming cliques with all neighbors of the removed vertex. Note that due to the structure of FF T ,\nwe will not introduce any new edges. In other words, performing Gaussian elimination on (FF T ) may be\ndone without introducing additional non-zero entries. If the number of maximum number of actions that an\ninformation set may have is upper bounded by a constant amax, then eliminating a single variable will only\nrequire time quadratic in amax. This means that computing (FF T )−1q may be done efﬁciently when xmax\nis small.\nRemark.\nLemma 3 and the fact that L2 projections onto sequence form constraints can be done\nefﬁciently may be of separate interest to researchers beyond the scope of EFCEs.\nIn practice, we precompute a sparse Cholesky factor of FF T . From the previous discussion, the Cholesky\nfactors are guaranteed to be sparse and easily stored. Withe the Cholesky decomposition of FF T , ﬁnding\n(FF T )−1q becomes straightforward. This precomputation is done once per trigger-sequence ˆσ, since the\nset of relevant sequence pairs for each trigger sequence (i.e., the location of non-zero entries in the matrix\nrepresenting ξ) differs. This precomputation step is trivially parallel. In our experiments, computing the\nCholesky factors was rarely the bottleneck (although we do include this timing when evaluating our method)\nD.2. Social Welfare Maximization\nObserve that Equation (1) may be rewritten in the form of (1 −λsw)v∗\nsw(ξ) −λswbT ξ for a suitable vector b.\nHence, the gradient for the modiﬁed objective is given by\n∂\/∂ξ v∗(ξ) =\n(\nAi∗,ˆσ∗y∗\ni∗,ˆσ∗−bi,ˆσ∗\nv∗(ξ) ≥κ(ξ)\n−b\notherwise\n,\n4In our implementation, f need not have this restriction, but it is included her to be more in line with the classic\nwork of (von Stengel, 1996).\n16\nARXIV PREPRINT - OCTOBER 27, 2019\nwhere κ(ξ) = τ −P\nz∈Z u1(z)ξ1(z; z) −P\nz∈Z u2(z)ξ2(z; z), the difference between τ and the social\nwelfare obtained from ξ.\nE. Battleship Game\nE.1. Extended Description of the Game\nA game of Battleship is parameterized by a tuple (H, W, S, r, γ), where\n• the integers H, W ≥1 deﬁne the height and width of the playing ﬁeld for each player;\n• S is an ordered list containing ship descriptions si for each player. Each description is a pair si =\n(ℓi, vi), where ℓi is the length of the i-th ship and vi is its value;\n• r ≥1 is the number of rounds in the game;\n• γ ≥1 is a loss multiplier that controls the relative value of a losing versus destroying ships.\nThe game proceeds in 2 phases: ship placement and shooting. During the ship placement phase, the players\n(starting with Player 1) take turns placing their ships on their playing ﬁeld. The players must place all\ntheir ships, in the same order in which they appear in S, on the playing ﬁeld. The ship placement phase\nends when all ships have been placed. We remark that the players’ playing ﬁelds are separate: in other\nwords, there are two playing ﬁelds of dimensions H × W, one per player. The ships may be placed either\nhorizontally or vertically on each player’s grid (playing ﬁeld); all ships must lie entirely within the playing\nﬁeld and may not overlap with other ships the player has already placed. Finally, the locations of a player’s\nships is private information for each player.\nIn the shooting phase, players take turns ﬁring at each other; Player 1 starts ﬁrst. This is done by selecting a\npair of integer coordinates (x, y) that identify a cell within the playing ﬁeld. After taking a shot, the player\nis told if the shot was a hit, that is, the selected cell (x, y) is occupied by a ship of the opponent, or if it is a\nmiss, that is, (x, y) does not contain an opponent’s ship. If all cells covered by a ship have been shot at, the\nship is destroyed and this fact is announced. Note that the identity of the ship which was hit or sunk is not\nrevealed; players only know that some ships was hit or sunk. The game ends when r shots have been made\nby each player, or if one player has lost all their ships, whichever comes ﬁrst. At the end of the game, each\nplayer’s payoff is computed as follows: for each opponent’s ship that the player has destroyed, the player\nreceives a payoff equal to the value v of that ships; for each ship that the player has lost to the opponent, the\nplayer incurs a negative payoff equal to γ · v, that is the value of the ship times the loss multiplier γ. Note\nthat when γ > 1 the game is general sum.\nSince γ ≥1, this asymmetric model describes situations where players are encouraged to destroy other\nships, but are ultimately more protective of their own assets. The loss multiplier γ governs this gap; a higher\nvalue of γ makes so that each player values their ships more than destroying others. Note that when γ = 1,\nwe obtain a zero-sum version of battleships (with varying scores for each ship).\nFor the remainder of the discussion, we deﬁne the social welfare (SW) of any outcome to be the sum of\npayoffs of each player. We will demonstrate that with the aid of a mediator (the correlation device), the\nsocial welfare of the optimal correlated equilibria are dramatically higher than the social welfare of even the\nbest Nash equilibrium. In other words, the mediator leads to signiﬁcantly less destructive outcomes, and\nleads to more frequent ties where the players sometimes agree to deliberately miss their opponents, while\nstill retaining incentive-compatibility and rationality in the standard game-theoretic sense.\nE.2. Analysis of Social-Welfare-Maximizing EFCE\nWe analyze one social-welfare-maximizing EFCE in the same small instance of Battleship as the previous\nsection. The mediator in this EFCE recommends the players a ship placement that is sampled uniformly\nat random and independently for each players. This results in 9 possible scenarios (one per possible\nship placement) in the game, each occurring with probability 1\/9. Due to the symmetric nature of ship\n17\nARXIV PREPRINT - OCTOBER 27, 2019\nplacements, only two scenarios are relevant: whether the two players are recommended to place their ship\nin the same spot, or in different spots. Figure 3 details the strategy of the the mediator in each of these two\nscenarios, assuming that the players do not deviate. Note that the game trees in Figure 3 are parametric on\nthe recommended ship placements a and b; all 9 possible ship placements can be recovered from Figure 3\nby setting a and b to appropriate values in {1, 2, 3}.\nPl.1 in cell a, Pl.2 in cell a\nSh. b\nSh. a\nSh. c\nSh. b\nSh. a\nSh. c\nSh. a\nSh. c\nSh. c\nSh. a\nSh. c\nSh. b\nSh. a\nSh. b\nSh. a\nSh. b\nSh. a\nSh. c\nSh. c\nSh. a\nSh. b\nSh. b\nSh. a\n25\/54\n2\/27\n25\/54\n1\/2\n1\/2\n2\/5\n3\/5\n2\/5\n3\/5\n1\/2\n1\/2\n1\/2\n1\/2\n1\/2\n1\/2\n2\/5\n3\/5\n2\/5\n3\/5\n1\/2\n1\/2\n1\/2\n1\/2\nPl.1 in cell b, Pl.2 in cell a\nSh. b\nSh. a\nSh. c\nSh. a\nSh. a\nSh. c\nSh. b\nSh. c\nSh. c\nSh. a\nSh. c\nSh. b\nSh. a\nSh. a\nSh. a\nSh. b\nSh. b\nSh. c\nSh. c\nSh. a\nSh. b\nSh. b\nSh. a\n25\/54\n2\/27\n25\/54\n1\/2\n1\/2\n2\/5\n3\/5\n2\/5\n3\/5\n1\/2\n1\/2\n1\/2\n1\/2\n1\/2\n1\/2\n2\/5\n3\/5\n2\/5\n3\/5\n1\/2\n1\/2\n1\/2\n1\/2\nFigure 3: Example of a playthrough of Battleship assuming both players were recommended to place their ship in a\n(left), or that Player 1 and 2 were recommended to place their ships in a and b respectively (right). For both pictures,\nthe numbers along each edge denote probabilities of each action being recommended; no edge is shown for actions\nrecommended with zero probability. Squares and hexagons denote actions taken by Players 1 and 2 respectively.\nSimilarly, blue and red nodes represent cases where Players 1 and 2 sink their opponent’s ship, respectively. Green leaf\nnodes are where the game results in no ship loss. The Shoot action is abbreviated to ‘Sh.’\nFor both game trees, note that the correlation device suggests that Player 1 shoot at the Player 2’s ship with\na low 2\/27 probability, and deliberately miss with high probability. As hinted in earlier sections, this type of\nrecommendation is key to understanding why the EFCE succeeds in promoting less destructive outcomes.\nOne may wonder why this behavior is incentive-compatible (that is, what are the incentives that compel\nPlayer 1 into not defecting), since the player may choose to randomly ﬁre in any of the 2 locations that were\nnot recommended, and get almost 1\/2 chance of winning the game immediately. The key is that if Player\n1 does so and does not hit the opponent’s ship, then the mediator can punish him by recommending that\nPlayer 2 shoot at the location of Player 1’s ship. Since players value their ships more than destroying their\nopponents, the player is incentivized to avoid such a situation by accepting the recommendation to (most\nprobably) miss.\nA similar situation arises in the ﬁrst move of Player 2. Here, Player 2 is recommended to deliberately miss,\nhitting each of the 2 empty spots with probability 1\/2. If he deviates and attempts to destroy Player 1’s ship,\nthen he risks the mediator revealing his location to his opponent if his shot misses; this risk is enough to\nkeep Player 2 ‘in line’. The second move of Player 1 (third shot of the full game) bears a similar ideas.\nHere, Player 1 is recommended to hit Player 2’s ship with probability 2\/5. Similar to his ﬁrst shot, Player 1\nmay deviate and ﬁre at the remaining location and enjoy 3\/5 chance of winning the game out right. Yet, this\nbehavior is discouraged, since in the 2\/5 chance that he misses the shot (i.e., the recommendation was in\nfact, the correct location of Player 2’s ship), then his location would be revealed by the mediator and he\nloses the next round. Again, this threat from the mediator encourages peaceful behavior, even though the\nrecommendation to Player 1 reveals a more accurate ‘posterior’ of Player 2’s ship location, as compared to\nthe uniform distribution of 1\/2. While making these recommendations, the mediator ensures that Player 2\nhas a uniform distribution of Player 1’s ship location, meaning that even though Player 2 has the ﬁnal move,\nhe may not do better than guessing at uniform at this stage.\nRemark.\nIt is important to note that Figure 3 does not convey the full information of the correlated\nplans. Crucially, it does not show the consequences suffered if a player deviates from his recommended\nstrategy—in this case, the deviating player stops receiving recommendations and risks having his ship’s\n18\nARXIV PREPRINT - OCTOBER 27, 2019\nlocation revealed to the opponent. These ‘counterfactual’ scenarios may be counter-intuitive but are key to\nunderstanding how SW-maximizing EFCEs achieve their purpose.\nF. Sheriff Game\nF.1. Extended Version of the Game\nThe Sheriff game is described by the the parameters v, p, s ∈R+, nmax, bmax, r ∈N. The parameters\nv, p, s ≥0 describe the value of each illegal item, the penalty that the Smuggler has to pay for each\ndiscovered illegal item, and the compensation that the Sheriff pays to the Smuggler in the case of a false\nalarm. At the beginning of the game, the Smuggler loads n ∈{0, . . . , nmax} items into his cargo. The\namount of goods loaded is unknown to the Sheriff. The game then proceeds for r ≥1 rounds of bargaining.\nEach round comprises two steps. First, the Smuggler offers a bribe bt ∈{0, . . . , bmax} to the Sheriff, where\nt ≤r is the round of bargaining. After that, the Sheriff responds with ‘Yes’ or ‘No’.\nAll actions are public knowledge, except for the selection of cargo contents, which only the Smuggler\nknows. In the ﬁnal step, we compute the payoffs to players. The outcome of the game is decided by the\nlast step of bargaining. In particular, the ﬁrst r −1 rounds of bargaining have no explicit bearing on the\noutcome of the game, except for purposes of coordination. The payoffs for each outcome are:\n1. Sheriff accepts the bribe. The Smuggler’s gets n · v −br, and the Sheriff’s gets the bribe offered br.\n2. Sheriff inspects and discovers illegal items. The Smuggler is ﬁned and gets a payoff of −n · p while\nthe Sheriff gets a payoff of n · p.\n3. Sheriff chooses to inspect and does not ﬁnd illegal items. The Smuggler receives a compensation of s,\nwhile the Sheriff gets −s.\nThe objective of the mediator is to maximize social welfare in the space EFCEs. Ideally, this will involve\nthe Smuggler bringing in goods and the Sheriff accepting bribes – any other outcome would simply be\nzero-sum, since it no goods will be successfully smuggled and money only changes hands between players.\nA qualitative description of the welfare maximizing equilibrium is not obvious, since the game contains\nelements of both lying and bargaining.\nRemark.\nThe communication in the bargaining steps is at a superﬁcial level similar to that in cheap\ntalk (Crawford & Sobel, 1982), where costless and non-binding signals are transmitted between players.\nHowever, in our setting, the signals are transmitted in the middle of the game as opposed to just at the\nbeginning. More importantly, the presence of the mediator during the phase of bargaining bestows more\nuses for the signals—in particular, the mediator may be able to take punitive measures against players who\ndeviate from recommendations, since future recommendations will be withheld from players who deviate.\nWe will illustrate the importance of this at the end of Appendix F.2.\nF.2. Effect of Additional Rounds of Bargaining (r)\nnmax\nr = 1\nr = 2\nr = 3\nr = 4\n1\n(4.00, 1.00)\n(4.00, 1.00)\n(4.00, 1.00)\n(4.00, 1.00)\n2\n(1.24, 0.19)\n(4.00, 1.00)\n(4.00, 1.00)\n(4.00, 1.00)\n5\n(0.89, 0.11)\n(1.11, 1.00)\n(4.00, 1.00)\n(4.00, 1.00)\n10\n(0.82, 0.00)\n(0.84, 1.00)\n(3.62, 1.00)\n(4.00, 1.00)\nTable 3: Payoffs for (Smuggler, Sheriff) when players play according to the SW-maximizing EFCE in the Sheriff game\nwith bmax = 2 (right).\n19\nARXIV PREPRINT - OCTOBER 27, 2019\nWe illustrate the effect of the non-consequential bribes with two small settings, where v = 5, p = 1, s =\n1, nmax = 3, bmax = 2, r ∈{1, 2}. Examples of SW-maximizing equilibria are shown in Figure 4 and\nFigure 5. 5\nStart of Game\nLoad 1\nLoad 3\nBribe $0\nBribe $1\nBribe $2\nInspect\nPass\nPass\nBribe $2\nPass\n62\/79\n17\/79\n29\/62\n8\/31\n17\/62\n1\n1\n1\n1\n1\nFigure 4: Example of a playthrough of the Sheriff game with r = 1. Edge labels correspond to action probabilities,\nedges with 0 probability are omitted. Squares and hexagons denote actions taken by Players 1 and 2 respectively, while\ngreen and red nodes denote the Sheriff choosing to pass or inspect.\nStart of Game\nLoad 1\nLoad 3\nBribe $0\nBribe $1\nBribe $2\nBribe $0\nBribe $1\nBribe $2\nFeedback 1\nFeedback 1\nFeedback 1\nFeedback 1\nFeedback 1\nFeedback 1\nBribe $2\nBribe $2\nBribe $2\nBribe $2\nBribe $2\nBribe $2\nPass\nPass\nPass\nPass\nPass\nPass\n1\/2\n1\/2\n3\/8\n3\/8\n1\/4\n3\/8\n3\/8\n1\/4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nFigure 5: Example of a playthrough of the Sheriff game with r = 2. Edge labels correspond to action probabilities,\nedges with 0 probability are omitted. Squares and hexagons denote actions taken by Players 1 and 2 respectively, while\ngreen and red nodes denote the Sheriff choosing to pass or inspect.\nThe SW maximizing EFCE yields payoffs of (3.89, 1.43) and (8.0, 2.0) for r = 1 and r = 2 respectively.\nWe will ﬁrst consider the case where r = 2 (Figure 5. Here, what occurs happens along the equilibrium\npath is straightforward. The Smuggler loads in 1 or 3 items with equal probability. Next, he offers a\n(non-consequential) bribe of either 0, 1, or 2. Then, he receives some feedback of 1, and proceeds to offer a\nbribe of $2, which the Sheriff gladly accepts. The payoffs to players is (13, 2) and (3, 2) depending if the\nSmuggler was recommended to load 1 or 3 items, leading to an average payoff of (8, 2).\nThe underlying mechanism is in fact fairly straightforward and mirrors the idea in the modiﬁed sig-\nnalling game of (von Stengel & Forges, 2008). Assume that a random number is chosen uniformly from\n{0, . . . , bmax}. This acts as a ‘passcode’ which the Sheriff expects from the Smuggler in the ﬁrst round.\n5As with the analysis of Battleship, note that this only shows interactions of players on the equilibrium path, that is,\nthe graph omits what would happen if some player deviated.\n20\nARXIV PREPRINT - OCTOBER 27, 2019\nThis passcode forms part of the correlated plan, and will eventually be revealed to the Smuggler assuming\nhe did not deviate when selecting the number of illegal items (recall that the sequential nature of the EFCE\nmeans that the recommended amount to bribe is not revealed until the Smuggler loads the cargo with the\nrecommended number of items.) In other words, the ﬁrst (non-consequential) bribe may be used as a signal\nwhich hints to the Sheriff if the Smuggler has deviated—if it is not equal to the passcode, the Smuggler must\nhave deviated somewhere. On the other hand, a deviating Smuggler may successfully guess the passcode\nwith probability no greater than 1\/(bmax + 1); if the number of signals bmax is sufﬁciently large, then it is\nnear impossible to guess the code. Using these tools, the mediator is able to engineer a ‘deviation detector’\nwhich checks if the Smuggler ever deviated. Note, however, that unlike the Signaling game, the Sheriff is\nnot able to glean exactly how what was recommended (in this case, the number of items in the cargo); he is\nonly able to deduce if the player deviated from the recommendation (in this case, this would be load either 1\nor 3 items).\nIssuing threats to the Smuggler becomes straightforward with this deviation detector. If the Sheriff knows\nthe Smuggler is lying, he employs a ‘grim trigger’ for the rest of the game—in this case, the Sheriff opts to\ninspect all of the player’s cargo, regardless of the bribe offered in the second round. The Smuggler could also\nbe pretending to bring in illegal goods, i.e., by loading 0 items and hoping that he would guess the incorrect\npasscode, resulting in the Sheriff making a false accusation. However, because the Smuggler’s payoff for\ndeceiving the Sheriff in this manner is just 1, he remains incentivized to stick to the recommendations,\nwhich guarantees him a payoff of either 3 of 13.\nWe now make the following hypotheses. First, the effect of additional bargaining rounds r is that the chance\nof randomly guessing the passcode is reduced. If there are r rounds, then there are (b + 1)r−1 different\npossible signals that the Smuggler could have sent to the Sheriff through the ﬁrst r −1 rounds. When r = 1,\nthis class of correlation plans fails since the bribe by the Smuggler serves both as the answer to the ‘secret\nquestion’ and as the actual bribe to be offered. This aliasing of roles is what leads to a lower payoff; the risk\nof sending an incorrect passcode is not sufﬁciently high to dissuade the Smuggler from deviating.\nG. 3 LP formulations for computing EFCEs\nRefer to the dualized problem in Appendix A. Observe that u is the value of the maximum deviation over all\nˆσ—when all incentive constraints are met, u should be non-positive. We propose 3 different formulations.\n• Min-Deviation: what was presented in Appendix A.\n• Feas-Deviation: instead of minimizing u in the objective, replace that by a hard constraint that u ≤0.\n• Maximum-SW: formulate the LP similar to Feas-Deviation, but with the SW-maximizing objective.\nH. Additional Experiments on Sheriff Game\nThe results for the Sheriff game were run using the parameters p = 1, v = 5, bmax = 3, r = 5, s = 4 while\nvarying the maximum number of items that can be smuggled nmax. The time required for the error to drop\nbelow a certain threshold is reported for both Gurobi and our subgradient method. The results are reported\nin Table 4. As before, we observe that our method can outperforms Gurobi if lower levels of accuracy are\nnmax\n#Actions\n#Relevant\nTime (LP)\nTime (ours)\nPl 1\nPl 2\nseq. pairs\n2\n1\n0.75\n0.5\n2\n1\n0.75\n0.5\n6\n131k\n37k\n6.5M\n723\n723\n723\n743\n42\n42\n44\n88\n8\n168k\n37k\n8.4M\n1187\n1223 1223 1223 63\n69\n102\n333\n10\n206k\n37k\n10M\n1662s 1774 1774 1829 83 240\n298\nN\/A\nTable 4: #Seq. pairs is the dimension of ξ under the compact representation of (von Stengel & Forges, 2008). For LPs,\nwe report the fastest of Barrier, Primal and Dual Simplex, and 3 different formulations (Appendix G). Our subgradient\nmethod did not manage to achieve an accuracy of 0.5 after 1 hour of running.\ndesired. However, it was observed that for higher levels of accuracy, Gurobi requires signiﬁcantly less\n21\nARXIV PREPRINT - OCTOBER 27, 2019\ntime, if our method converges at all. This is because Gurobi spends the majority of its time performing\npreprocessing steps.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks.pdf"}
{"title":"High-Level Representation of Benchmark Families for Petri Games","authors":"Manuel Gieseking, Ernst-Rüdiger Olderog","summary":"Petri games have been introduced as a multi-player game model representing\ncausal memory to address the synthesis of distributed systems. For Petri games\nwith one environment player and an arbitrary bounded number of system players,\ndeciding the existence of a safety strategy is EXPTIME-complete. This result\nforms the basis of the tool ADAM that implements an algorithm for the synthesis\nof distributed controllers from Petri games. To evaluate the tool, it has been\nchecked on a series of parameterized benchmarks from manufacturing and workflow\nscenarios. In this paper, we introduce a new possibility to represent benchmark\nfamilies for the distributed synthesis problem modeled with Petri games. It\nenables the user to specify an entire benchmark family as one parameterized\nhigh-level net. We describe example benchmark families as a high-level version\nof a Petri game and exhibit an instantiation yielding a concrete 1-bounded\nPetri game. We identify improvements either regarding the size or the\nfunctionality of the benchmark families by examining the high-level Petri\ngames.","url":"http:\/\/arxiv.org\/abs\/1904.05621v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1904.05621v1","published":1554980170000,"comment":"20 pages, 9 figures","pdf_text":"High-Level Representation of\nBenchmark Families for Petri Games ⋆\nManuel Gieseking and Ernst-R¨udiger Olderog\nUniversity of Oldenburg {gieseking, olderog}@informatik.uni-oldenburg.de\nAbstract. Petri games have been introduced as a multi-player game\nmodel representing causal memory to address the synthesis of distributed\nsystems. For Petri games with one environment player and an arbitrary\nbounded number of system players, deciding the existence of a safety\nstrategy is EXPTIME-complete. This result forms the basis of the tool\nAdam that implements an algorithm for the synthesis of distributed\ncontrollers from Petri games. To evaluate the tool, it has been checked on\na series of parameterized benchmarks from manufacturing and workﬂow\nscenarios.\nIn this paper, we introduce a new possibility to represent benchmark\nfamilies for the distributed synthesis problem modeled with Petri games.\nIt enables the user to specify an entire benchmark family as one param-\neterized high-level net. We describe example benchmark families as a\nhigh-level version of a Petri game and exhibit an instantiation yielding a\nconcrete 1-bounded Petri game. We identify improvements either regard-\ning the size or the functionality of the benchmark families by examining\nthe high-level Petri games.\n1\nIntroduction\nAutomatically creating a program from a formal speciﬁcation without any hu-\nman programming involved, is of great interest for the implementation of correct\nsystems. A synthesis algorithm either automatically derives an implementation\nsatisfying a given formal speciﬁcation or states the non-existence of such an\nimplementation [3]. For reactive systems, i.e., system which continuously inter-\nact with their environment, the synthesis problem is often described as a game\nbetween the environment and the system. In this game-theoretic approach the\nspeciﬁcation is given as a winning condition of the game and a correct imple-\nmentation is a strategy for the system players which satisﬁes the given winning\ncondition against all moves of the environment. The synthesis approach funda-\nmentally simpliﬁes the development of complex systems by deﬁning only the\npossible actions of the system and specifying the winning condition over these\nactions. This puts the development process on a more abstract level and avoids\nthe error-prone manual coding.\n⋆This work was supported by the German Research Foundation (DFG) through the\ngrant Petri Games (No. 392735815)\narXiv:1904.05621v1  [cs.GT]  11 Apr 2019\n2\nM. Gieseking and E.-R. Olderog\nFor the monolithic synthesis, where the system can be seen as one unit with\na central controller as strategy, there is a growing number of tools [19,4,1,2] solv-\ning nontrivial applications. However, for the synthesis of distributed systems, i.e.,\nsystems composed of multiple independent processes possibly distributed over\nwide distances, the tool support is restricted. This is mainly due to the high\ncomplexity of the solving algorithms or the undecidability results for the general\nproblem. In the two well-established models, the Pnueli\/Rosner model [23] and\nZielonka’s asynchronous automata [26], the complexity is in general nonelemen-\ntary [14,15,20] or even undecidable [23,13]. For the class of Zielonka automata\nwith acyclic communication architectures the control problem has been shown to\nbe decidable, with non-elementary complexity in general but EXPTIME for the\nspecial case of architectures of depth 1 [21]. For Petri games [11,12], which have\nbeen shown to adequately model problems from manufacturing and workﬂow\nscenarios, reasonable subclasses can be solved with aﬀordable costs and suitable\ntool support [9,7,8].\nIn this paper, we extend the work on Petri games and present a model for\nrepresenting benchmark families for the synthesis of distributed systems in a\nconcise way. Petri games model the distributed synthesis problem as a game\nbetween two teams: the environment players, representing external inﬂuences\n(the uncontrollable behavior), and the system players, representing the processes\n(the controllable behavior). In Petri games each player is modeled as a token of\nan underlying place\/transition Petri net. The places of the net are partitioned\nbetween the teams. All players remember their own causal past and communicate\nthis knowledge to every player participating in a joint transition. An example\ncan be seen in Fig. 1.\nBenchmark families depend on parameters which deﬁne a set of problems\nwith increasing complexity. The new representation is based on schemata of\nColoured Petri Nets [16,18] rather than place\/transition Petri nets, to resem-\nble the parameters and sets of problems. We use places with individual tokens\nranging over predeﬁned domains of parametric size, transitions labelled with con-\nditions that guard their ﬁrability, and arcs labelled with expressions stating the\nresult of the ﬁring. Conditions and expressions may have variables ranging over\nthe predeﬁned domains. This enables the user to specify the entire benchmark\nfamily as one parametric high-level net rather than introducing a set of instances\nof the family and descriptions how to generalize these Petri games. Generally,\nthe individual elements of a benchmark family (e.g., robots, work pieces, tools,\nhumans, etc.) can be modeled by parametric sets of individual tokens and are\nprocessed by the transitions according to the semantics. Figure 4 serves as an\nexample for a set of alarm systems and locations of a burglary.\nIn this paper, we introduce a new parameterized high-level representation\nof Petri games based on high-level Petri nets for a concise and clear deﬁnition\nof benchmark families. We apply the new deﬁnition to some of the existing\nbenchmark families and show the correspondence of the high-level version to an\nexample instantiation. During the application we identiﬁed improvements (either\nin size or functionality) of these benchmark families.\nHigh-Level Representation of Petri Games\n3\nThe remainder of the paper is structured as follows. Section 2 recaps the\nideas, results, and solving techniques of Petri games and informally motivates\nthe new high-level representation by an example. The formal deﬁnition of the\nhigh-level representation is given in Section 3. In Section 4 we illustrate the new\napproach by presenting two examples from the manufacturing domain and de-\npicting for each example both the high-level representation and an instantiation.\n2\nPetri Games for the Synthesis of Distributed Systems\nIn this section a brief overview of Petri games [11,12] is given. We illustrate\nthe model via an instantiation of the benchmark family of an distributed alarm\nsystem from [8] and motivate the new high-level representation for a concise\nand clear presentation of the family. Basic knowledge about Petri nets [22,24] is\nassumed. We ﬁx the notation of a Petri net N = (P, T, F, In), with places P,\ntransitions T, a ﬂow relation F ⊆(P ∪T) × (T ∪P), and an initial marking\nIn ⊆P.\n2.1\nPetri Games\nA Petri game G = (PS, PE, T, F, In, B) models the distributed synthesis prob-\nlem as a multi-player game where the tokens of an underlying Petri net N rep-\nresent the players of the game. The players act in two teams: the uncontrollable\nplayers (environment players) are the token residing on environment places PE\n(depicted as white circles) and controllable players (system players) are the to-\nken residing on the system places PS (depicted as gray circles). Those sets are\nthe disjunct union of the places of the underlying net, i.e., P = PE ˙∪PS. The\nuncontrollable players are used for modeling external inﬂuences on the system,\nwhereas the controllable ones represent its processes. Each player knows its own\ncausal past, i.e., the places and transitions which had been used to reach the\ncurrent place. This information is exchanged with all players participating at a\njoint transition. These intricate causal dependencies (and independencies) are\nnaturally represented by the unfolding of the Petri net [6,5]. An unfolding rep-\nresents the behavior of a Petri net by unrolling each loop of the execution and\nintroducing copies of p ∈P for each join of transitions in p. The system players\nhave to cooperate to win the game, i.e., to avoid reaching certain bad places\np ∈B (depicted as double circled places). To satisfy this safety objective, the\nplayers can solely use their locally available information.\nA strategy is a local controller for each system player which only decides on\nits current view and available information about the whole system. A strategy\ncan be obtained by removing certain branches of the unfolding. That is, tran-\nsitions and their complete future are removed which are considered as not be\ntaken from the system. We search for deterministic, i.e., in every situation no\ntwo transition are enabled, and deadlock-avoiding strategies, i.e., whenever the\nsystem can proceed in G there must also be a continuation in the corresponding\n4\nM. Gieseking and E.-R. Olderog\nEnv\niA\niB\nCA\nCB\ntA\nDA\nIA\nSA\nfaA\nfr A\ninfoB\npA\naa\nab\nAA\nAB\ntB\nDB\nIB\nSB\nfaB\nfr B\ninfoA\npB\nba\nbb\nBA\nBB\nBad\nGood\nBurglary\nComm.\nAlarm\nFig. 1: Two distant locations A and B are secured by the alarm systems repre-\nsented by the token initially residing in SA and SB. The alarm system in location\nX can state that there should be a burglary at location Y by putting a token\nat place XY (for X, Y ∈{A, B}). The goal is that no system produces a false\nalarm, or, in case of an intrusion, indicates the wrong intrusion point.\nsituation in the strategy. Furthermore, no behavior of the environment is allowed\nto be restricted.\nWe illustrate the model with an example of two system players and one\nenvironment player, modeling a distributed alarm system from [11], visualized\nin Fig. 1.\nExample 1 (Alarm System). We consider two alarm systems SA and SB secur-\ning one location each. Location A is depicted as the left and location B as the\nright part of Fig. 1. The alarm systems are represented by the tokens in the\ncorresponding system places. In case of a burglary at any of these locations, ex-\necuted by the environment token, each alarm system should indicate the correct\nintruding point despite their distribution. That is, for an intrusion in location\nY ∈{A, B} the token of each system X ∈{A, B} should eventually reach XY .\nEach alarm system has in addition to its correct behavior the possibility to trig-\nger a false alarm, i.e., setting oﬀan alarm without any burglary, or to give a\nfalse report, i.e., indicating the wrong location of the burglary. These incorrect\nbehavior can occur by taking transition faX or fr X for X ∈{A, B}, respectively.\nThus, each alarm system SX for X ∈{A, B} should wait until a burglary has\nhappened and then inform the other system Y (via transition infoY ), or wait\non getting informed by the other system (via transition infoX). Generally, the\nsystem should only take a decision (and the right one) when it is well enough\nHigh-Level Representation of Petri Games\n5\nEnv\niA\niB\nCA\nCB\ntA\nDA\nIA\nSA\nfaA\nfr A\ninfoB\npA2\npA1\npA3\npA4\nAB\nAA\ntB\nDB\nIB\nSB\nfaB\nfr B\ninfoA\npB 2\npB 1\npB3\npB4\nBB\nBA\nBurglary\nComm.\nAlarm\nFig. 2: Unfolding and winning strategy of the Petri game from Fig. 1. The places\nBad and Good and the corresponding transitions are omitted for readability\nreasons. The winning strategy for the system players is visualized by the solid\nelements.\ninformed. This results in the strategy depicted as part of the unfolding in Fig. 2\nby the solid elements.\n2.2\nSolving Petri Games\nThere are four major results on ﬁnding winning strategies for Petri games with\nsafety objectives. Firstly, deciding the question whether it exists a strategy for\nthe system players for Petri games with one environment player and an arbitrary\nbut bounded number of system players and a safety objective is EXPTIME-\ncomplete [12]. The strategy can be obtained in single-exponential time. Secondly,\ninterchanging the players, meaning the setting of n ∈N distributed environment\nplayers and one system player, yields the same complexity results [10]. Thirdly,\nfor unbounded underlying Petri nets the question is undecidable [12]. Finally,\nthe paper [7] introduces a bounded synthesis approach which limits the size of\nthe strategy. This constitutes a semi-decision procedure which is optimized in\nﬁnding small implementations.\nIn the following we brieﬂy recap the idea of the decision procedure for one\nenvironment player and n ∈N system players on a Petri game with a safety\nobjective and an underlying 1-bounded Petri net. The algorithm consists of\nfour major steps: Firstly, the input Petri game is reduced to a two-player game\nover a ﬁnite graph G with complete information. Secondly, the question of the\nexistence of a strategy in G is answered with standard symbolic game solving\nalgorithms and a strategy for G is constructed. Thirdly, the strategy of G is\n6\nM. Gieseking and E.-R. Olderog\nEnv\niA\niB\nCA\nCB\ntA\nDA\nIA\nSA\nfaA\nfr A\ninfoB\npA\naa\nab\nAA\nAB\ntB\nDB\nIB\nSB\nfaB\nfr B\ninfoA\npB\nba\nbb\nBA\nBB\nBad\nGood\nBurglary\nComm.\nAlarm\nPetri game\nEnv,\n(SA, 1, ⊤, {}),\n(SB, 1, ⊤, {})\nEnv,\n(SA, 1, !⊤, {tA, infoA}),\n(SB, 1, !⊤, {tB, infoB})\nCA,\n(SA, 1, !⊤, {tA, infoA}),\n(SB, 1, !⊤, {tB, infoB})\nIA,\n(DA, 1, ⊤, {}),\n(SB, 1, !⊤, {tB, infoB})\nIA,\n(DA, 1, !⊤, {infoB}),\n(SB, 1, !⊤, {tB, infoB})\nIA,\n(pA, 1, !⊤, {aa}),\n(pB, 1, !⊤, {ba})\nIA,\n(AA, 1, !⊤, {}),\n(pB, 1, !⊤, {ba})\nIA,\n(AA, 1, !⊤, {}),\n(BA, 1, !⊤, {})\nCB,\n(SA, 1, !⊤, {tA, infoA}),\n(SB, 1, !⊤, {tB, infoB})\nIB,\n(SA, 1, !⊤, {tA, infoA}),\n(DB, 1, ⊤, {})\nIB,\n(SA, 1, !⊤, {tA, infoA}),\n(DB, 1, !⊤, {infoA})\nIB,\n(pA, 1, !⊤, {ab}),\n(pB, 1, !⊤, {bb})\nIB,\n(AB, 1, !⊤, {}),\n(pB, 1, !⊤, {bb})\nIB,\n(AB, 1, !⊤, {}),\n(BB, 1, !⊤, {})\nEnv,\n(SA, 1, !⊤, {faA}),\n(SB, 1, !⊤, {tB, infoB})\nEnv,\n(pA, 1, !⊤, {aa, ab}),\n(SB, 1, !⊤, {tB, infoB})\nEnv,\n(pA, 1, !⊤, {aa}),\n(SB, 1, !⊤, {tB, infoB})\nEnv,\n(AA, 1, !⊤, {t⊥}),\n(SB, 1, !⊤, {tB, infoB})\nEnv,\n(AA, 1, !⊤, {}),\n(SB, 1, !⊤, {tB, infoB})\nCA,\n(AA, 1, !⊤, {}),\n(SB, 1, !⊤, {tB, infoB})\nCB,\n(AA, 1, !⊤, {}),\n(SB, 1, !⊤, {tB, infoB})\nIB,\n(AA, 1, !⊤, {}),\n(DB, 1, ⊤, {})\nEnv,\n(SA, 1, !⊤, {}),\n(SB, 1, !⊤, {})\nCA,\n(SA, 1, !⊤, {}),\n(SB, 1, !⊤, {})\nCB,\n(SA, 1, !⊤, {}),\n(SB, 1, !⊤, {})\nEnv,\n(SA, 1, !⊤, {infoA}),\n(SB, 1, !⊤, {tB})\nCB,\n(SA, 1, !⊤, {infoA}),\n(SB, 1, !⊤, {tB})\nIB,\n(SA, 1, !⊤, {infoA}),\n(DB, 1, ⊤, {})\nIB,\n(SA, 1, !⊤, {infoA}),\n(DB, 1, !⊤, {frB})\nbb\ninfoA\niB\ntB\niA\ntA\ninfoB\nab\naa\nba\nfaA\nfaA\naa\naa\niB\niA\ntB\niA\ntB\niB\ntB\nWinning strategy for the system players\n2-player game over finite graph\nEnv,\n(SA, 1, ⊤, {}),\n(SB, 1, ⊤, {})\nEnv,\n(SA, 1, !⊤, {tA, infoA}),\n(SB, 1, !⊤, {tB, infoB})\nCA,\n(SA, 1, !⊤, {tA, infoA}),\n(SB, 1, !⊤, {tB, infoB})\nIA,\n(DA, 1, ⊤, {}),\n(SB, 1, !⊤, {tB, infoB})\nIA,\n(DA, 1, !⊤, {infoB}),\n(SB, 1, !⊤, {tB, infoB})\nIA,\n(pA, 1, !⊤, {aa}),\n(pB, 1, !⊤, {ba})\nIA,\n(AA, 1, !⊤, {}),\n(pB, 1, !⊤, {ba})\nIA,\n(AA, 1, !⊤, {}),\n(BA, 1, !⊤, {})\nCB,\n(SA, 1, !⊤, {tA, infoA}),\n(SB, 1, !⊤, {tB, infoB})\nIB,\n(SA, 1, !⊤, {tA, infoA}),\n(DB, 1, ⊤, {})\nIB,\n(SA, 1, !⊤, {tA, infoA}),\n(DB, 1, !⊤, {infoA})\nIB,\n(pA, 1, !⊤, {ab}),\n(pB, 1, !⊤, {bb})\nIB,\n(AB, 1, !⊤, {}),\n(pB, 1, !⊤, {bb})\nIB,\n(AB, 1, !⊤, {}),\n(BB, 1, !⊤, {})\nbb\ninfoA\niB\ntB\niA\ntA\ninfoB\nab\naa\nba\nStrategy for Player 1\nEnv\niA\niB\nCA\nCB\ntA\nDA\nIA\nSA\ninfoB\npA3\npA4\naa\nab\nAA\nAB\ntB\nDB\nIB\nSB\ninfoA\npB3\npB4\nba\nbb\nBA\nBB\nBurglary\nComm.\nAlarm\nPetri game strategy for system players\nEnv\niA\niB\nCA\nCB\ntA\ntB\nIA\nIB\nSA\ntA\ninfoA\nDA\ninfoB\npA3\naa\nAA\npA4\nab\nAB\nSB\ntB\ninfoB\nDB\ninfoA\npB3\nbb\nBB\npB4\nba\nBA\nBurglary\nComm.\nAlarm\nDistributed controllers\nreduction\nsymbolic game solving\n“unfolding”\ndistribute\nFig. 3: An overview of the symbolic game solving algorithm for 1-bounded Petri\ngames with one environment and an arbitrary number of system players with a\nsafety objective implemented in Adam [9,8].\nused to extract a common strategy for the system players of G. Fourthly, this\nstrategy is distributed into one local controllers for each process. An overview of\nthe general approach is visualized in Fig. 3.\nThe algorithm starts with a Petri game in the upper left corner, which con-\nsists of one environment, a bounded number of system players, and places de-\nnoted as bad. This net is reduced to a two-player game over a ﬁnite graph with\ncomplete information, i.e., both players know in any point in time everything\nabout the opponent. Player 0 (depicted as the white rectangles) represents the\none environment player and Player 1 (depicted as the gray rectangles) represents\nall system players together. The key idea of the reduction is that the behavior\nof the environment player is delayed until no system player can move without\nany interaction with the environment (or never depend on the environment any-\nmore). This ensures that we can consider the players as completely informed\nHigh-Level Representation of Petri Games\n7\nabout all actions in the game. The system players will be informed of the envi-\nronment action by their next movement and they are also informed of the other\nsystem player’s behavior because deterministic strategies are build. A two-player\ngame over a ﬁnite graph with complete information can be solved with standard\ngame solving techniques. Furthermore, the existence of a strategy already yields\nthe existence of a memoryless strategy, i.e., a strategy which is only dependent\non the current state and not on the previous states of the run. In [9] this is done\nwith a symbolic game solving algorithm utilizing BDDs for the representation of\nthe state space. In [11] it is shown that a strategy for the system players of the\nPetri game exists if and only if a strategy for Player 1 exists in the two-player\ngame. Thus, we achieve a memoryless strategy for the system players such that\nthey can cooperatively play without encountering any bad behavior against all\npossible actions of an hostile environment. By traversing the winning strategy\nof the two-player game over the ﬁnite graph in breadth-ﬁrst order, a ﬁnite Petri\nnet can be constructed which is a winning strategy of the system players of\nthe Petri game. Finkbeiner and Olderog [11] showed that for a concurrency-\npreserving strategy, i.e., the number of ingoing arcs is equal to the number of\noutgoing arcs for each transition, this common strategy of the system players\ncan be distributed into local strategies. This yields one controller for each player.\n2.3\nMotivating the High-Level Representation\nAs for the comparison of standard P\/T Petri nets and high-level Petri nets, the\nbeauty of the high-level representation explicated in the following section consists\nof the conciseness and clarity of the illustration of the system’s behavior. This can\nbe seen in Fig. 4. In [8] the Petri game of the two alarm systems of Example 1\nis extended to a benchmark family of n alarm systems which all have to be\ninformed about the burglary and set oﬀthe alarm accordingly. This means that\nnew intrusion points and new alarm systems are introduced by adding copies of\nthe corresponding places and transitions. Using P\/T Petri nets only, the exact\ndescription of a benchmark family requires a high amount of precise descriptive\ntexts but nevertheless bears the risk of introducing misunderstandings. Also an\ninstantiation, e.g., the one in Fig. 1, cannot show the details appropriately. For\nexample, it does not imply whether in this benchmark family the burglarized\nalarm system informs the others synchronously or in any manner sequentially.\nBut visualizing the family for three systems is already quite unwieldy; especially\nfor the transitions leading into the bad place. The representation in Fig. 4,\nwhich syntax and semantics deﬁnitions are given in the following section, however\nallows for a concise, parametric deﬁnition of the benchmark family.\nWe introduce the concept by the example presented in Fig. 4 before providing\nthe technical details in the following section.\nExample 2 (Parameterized Alarm System). In the high-level representation in-\ndividual tokens can reside on places and the transitions can move them individ-\nually. Furthermore, the ﬁring of transitions can be restricted by guards. In the\nexample, n individual tokens, with n ∈N, representing n alarm systems initially\n8\nM. Gieseking and E.-R. Olderog\nN\nSys\nt\nD\ninfo\nP\na\nAlarm\ng\nGood\nC\ni\nI\nfa\nfr\n⊥1\n⊥2\nb ̸= x\nBad\nx\nx\nx\nx\nx\nx\nx\ny\ny\nx\nF(x)\nN\nz\n(z, v)\nx\nG(x)\n(a, b)\nx\n(a, b)\nx\npar\nn : N\nN = {1, . . . , n}\nvar\nx, y, z, v, a, b : N\nF : N →P (N) , x 7→N \\ {x}\nG : N →P (N × N) , x 7→{(z, x) | z ∈N}\nBurglary\nComm.\nAlarm\nFig. 4: Parameterized high-level Petri game for a benchmark family with n ∈N\ndistributed alarm systems. The low-level Petri game of Fig. 1 can be seen as an\ninstantiation with n = 2 of the benchmark family.\nreside in the place Sys. The burglar, represented by the black token, can intrude\nany of the n locations (represented by putting a token x ∈N into the place\nC) via transition i. Only the corresponding alarm system at that location can\ndetect the intrusion by transition t, because both ingoing arcs are labelled with\nthe same identiﬁer x. In any case, every alarm system can set oﬀa false alarm by\ntransition fa. After detecting an intrusion, an alarm system can synchronously\ninform all other systems by transition info or do not report the intrusion by fr.\nFinally, it can decide by transition a which alarm to set oﬀ. It is bad if one alarm\nsystem z decides to set oﬀan alarm for location v, by putting (z, v) into Alarm\nbut another location has been intruded (transition ⊥2 leading to place Bad) or\nif some alarm is set oﬀ, but no intrusion has ever been detected (transition ⊥1\nleading to place Bad). If all alarm systems have detected the intruded location\ncorrectly, the place Good can be reached.\nBy replacing the transition info according to Fig. 5, we can easily switch from\na synchronously informing of the other systems to an arbitrary sequential order\nof information dissemination. Note that the strategy of each alarm system can\ndecide which other system should be informed next. The last informed system\ncan take transition fr because no other system has to be informed anymore. This\nyields n · (n −1) transitions in the low-level version, in contrast to the n ones\nHigh-Level Representation of Petri Games\n9\nN\nSys\nD\ninfo\nP\nx\nF(x)\nN\n(a) Transition info of Fig. 4 for informing\nall other systems synchronously.\nN\nSys\nD\ninfo\nP\nx ̸= y\nx\ny\nx\ny\n(b) A possible replacement for info to in-\nform all other systems sequentially.\nFig. 5: The left ﬁgure shows the synchronously informing of the burglary of Fig. 4.\nThe right one introduces a possible replacement of the transition info, such that\nafter one system detected the intruding or got informed about the intrusion, it\ninforms an arbitrary other system. Thus, the systems can inform one another in\nany arbitrary order.\nin the synchronous case. Such diﬀerences are more complex to visualize in the\nlow-level presentation because the diﬀerence can only be recognized for n > 2.\n3\nParameterized High-level Petri Games\nIn high-level Petri nets values may appear as individual tokens in places [25].\nSuch a value is also referred to as a “color”, leading to the terminology of\nColoured Petri Nets [18]. In high-level Petri nets, the ingoing and outgoing arcs\nof transitions are labelled by expressions that specify which of the individual\ntokens are withdrawn from the preset and which ones are added to the places\nin the postset of the transition. Additionally, Boolean expressions labelling the\ntransitions serve as guards.\nIn this section, we use these concepts to introduce high-level Petri games.\nWe constrain ourselves to high-level Petri games that have sets (rather than\nmultisets) of individual tokens in their places. We consider parameterized high-\nlevel games where the size of the sets of individual tokens that may appear in\nthe places depends on parameters.\n3.1\nPreliminaries\nWe consider parameters, with typical letters k, m, n, ranging over the set N of\nnatural numbers and write par k, m, n : N to declare that k, m, n are parameters.\nThere may be a constraint added to the parameters like m ≤n. An instantiation\nassigns a ﬁxed natural number to each parameter. Parameters may appear in\nset expressions S, deﬁned inductively by the following syntax:\nS ::= {1, . . . , n} | {•} | S1 × · · · × Sn | P(S)\nHere {1, . . . , n} is a ﬁnite set of parametric size n, the symbol • denotes the\nblack token used in normal Petri nets, × denotes cartesian product, and P the\n10\nM. Gieseking and E.-R. Olderog\npower set. Set expressions are used as (parametric) types. An instantiation of\nthe parameters turns each set expression into a ﬁxed set. Constants, with typical\nletters K,M,N, are used as abbreviations for set expressions. We write K = S to\ndeclare that K abbreviates the set expression S.\nWe consider variables, with typical letters x, y, z, ranging over set expressions\nand write var x, y, z : S to declare that x, y, z are variables of type S. We write\nty(x) to denote the type of a variable x. We consider function symbols, with\ntypical letters F,G, and write F : S1 −→S2 to declare that F is a symbol standing\nfor a function from elements of S1 to elements of S2, for set expressions S1, S2.\nOut of parameters, constants, variables, and function symbols we construct\nBoolean expressions and expressions of set type. We shall not deﬁne the syntax of\nthese expressions in detail here, but give typical examples. Suppose par m, n : N\nand var x, y, x′, y′ : S1 and F : S1 −→S2. Then m < n, x ̸= y, and x = x′∧y = y′\nare Boolean expressions, the pair (x, y) is an expression of type S1 × S1 and the\nfunction application F(x) is an expression of type S2. To deﬁne the function\ndenoted by F we write a maplet x 7→e, where x is a variable of type S1 and e is\nan expression of type S2 containing x as a free variable. For a given instantiation,\nthe maplet describes how F assigns to a given element d of type S1 a value of\ntype S2 by evaluating e with d substituted for x in e.\n3.2\nHigh-level Petri Games\nIn high-level Petri games values may appear as individual tokens in addition to\nthe black tokens of normal Petri nets. Syntactically, a high-level Petri game is a\nstructure\nH = (PH\nS , PH\nE , TH, FH, InH, BH, ty, g, e, in),\nwhere the following components are as in 1-bounded Petri games:\n– PH\nS is a set of system places,\n– PH\nE is a set of environment places,\n– PH is the set of all places: PH = PH\nS ∪PH\nE ,\n– TH is a set of transitions,\n– FH ⊆(PH × TH) ∪(TH × PH) is the ﬂow relation,\n– InH ⊆PH is the set of initially marked places,\n– BH ⊆PH is the set of bad places.\nAdditionally, the following components represent the high-level structure:\n– ty is a mapping that assigns to each place p ∈PH a type ty(p) in the form\nof a set expression, describing the set of individual tokens that may reside\nin p during the game,\n– g is a mapping that assigns to each transition t ∈TH a Boolean expression\ng(t) serving as a guard describing when t can ﬁre,\n– e is a mapping that assigns to each ingoing arc (p, t) ∈FH and each outgoing\narc (t, q) ∈FH of a transition t ∈TH an expression e(p, t) and e(t, q) of set\ntype, respectively, describing which tokens are withdrawn by t from p and\nwhich tokens are placed by t on q when t is ﬁred,\nHigh-Level Representation of Petri Games\n11\n– in is a mapping that assigns to each initially marked place p ∈InH a non-\nempty subset of in(p) ⊆ty(p).\nGuards and expressions will typically contain variables. For a transition t ∈TH\nlet var(t) denote the set of free variables occurring in the guard g(t) or in one of\nthe expressions e(p, t) and e(t, q) for places p in t’s preset, deﬁned by pre(t) =\n{p ∈PH | (p, t) ∈FH}, or q in t’s postset, deﬁned by post(t) = {q ∈PH |\n(t, q) ∈FH}.\nGraphically, a high-level Petri game H looks like a normal Petri game, except\nthat guards g(t) appear inside a dashed box connected to the transition t by a\ndashed line, expressions e(p, t) and e(t, q) appear as labels of the arcs (p, t) and\n(t, q), respectively, and types ty(p) appear as labels of places p. To avoid clutter,\nguards equivalent to true are not shown. Also, if the type of a place p can be\neasily deduced from the context, the label ty(p) is not shown. The declarations\nof parameters, constants, variables, and function symbols are listed in a dashed\nbox near the graphics of the Petri game.\nThe semantics of a high-level Petri game H is given by its token game.\nTo deﬁne it, we assume an instantiation of the parameters so that each set\nexpression deﬁnes a ﬁxed set. A marking M of H assigns to each place p a set\nM(p) ⊆ty(p). Unlike in [18], we do not admit multisets as markings because\nwe aim at 1-bounded Petri games as low-level instantiations of high-level Petri\ngames. The initial marking M0 of H is the marking with M0(p) = in(p) for\np ∈InH and M0(p) = ∅otherwise.\nA valuation v of a transition t assigns to each variable x ∈var(t) a value\nv(x) ∈ty(x). By Val(t) we denote the set of all valuations of t. Each valuation\nv of t is lifted inductively from the variables in var(t) to the expressions around\nt. For the guard g(t) we denote by v(t) the Boolean value assigned by v to g(t).\nFor an ingoing arc (p, t) we denote by v(p, t) the value assigned by v to e(p, t),\nand analogously for an outgoing arc (t, p).\nA transition t is enabled at a marking M under a valuation v of t if v(t) = true\nand v(p, t) ⊆M(p) for each arc (p, t). Firing (the enabled) transition t at M\nunder v yields the marking M ′, where for each place p\nM ′(p) = (M(p) −v(p, t)) ∪v(t, p).\nThis is denoted by M [t, v⟩M ′. We assume here that ∪is a disjoint union, which\nis satisﬁed if the Petri game is contact-free, i.e., if for all t ∈TH and all reachable\nmarkings M\npre(t) ⊆P(M) ⇒post(t) ⊆(PH −P(M)) ∪pre(t),\nwhere P(M) = {p ∈PH | M(p) ̸= ∅}. The set of reachable markings of H is\nR(H) = {M | ∃n ≥0 ∃t1, . . . , tn ∈TH ∃v1 ∈Val(t1) . . . ∃vn ∈Val(tn) :\nM0 [t1, v1⟩M1 [t2, v2⟩. . . [tn, vn⟩Mn = M}.\n12\nM. Gieseking and E.-R. Olderog\n3.3\nInstantiations of High-level Petri Games\nFor ﬁxed parameter values, a given high-level Petri game\nH = (PH\nS , PH\nE , TH, FH, InH, BH, ty, g, e, in)\nwith PH = PH\nS ∪PH\nE can be transformed into a safe Petri game\nG = (PS, PE, T, F, In, B).\nLet D = S\np∈PH ty(p) be the set of all possible values that individual tokens in\nplaces p ∈PH can take, and let Val be the set of valuations assigning values\nd ∈D of the right type to each variable. The constituents of G are as follows:\n– system places: PS = {(p, d) ∈PH\nS × D | d ∈ty(p)},\n– environment places: PE = {(p, d) ∈PH\nE × D | d ∈ty(p)},\n– transitions: T = {(tH, v) | tH ∈TH ∧v ∈Val(tH) ∧v(tH) = true},\n– an arc from (p, d) to (tH, v) occurs in F if v(p, tH) = d holds in H,\n– an arc from (tH, v) to (q, d) occurs in F if v(tH, q) = d holds in H,\n– initial marking: In = {(p, d) ∈InH × D | d ∈in(p)},\n– bad places: B = {(p, d) ∈BH × D | d ∈ty(p)}.\nThe set of all places of G is thus given by\nP = PS ∪PE = {(p, d) ∈PH × D | d ∈ty(p)}.\nExample 3. Figure 1 shows the instantiation of the alarm system for n = 2\nlocations of the high-level Petri game in Fig. 4.\n3.4\nCorrespondence of High-level and Low-level Petri Games\nWe relate the ﬁring behaviour of the high-level Petri game H to that of the\nlow-level Petri game G deﬁned in Section 3.3. To this end, we deﬁne a mapping\nρ from markings M H in H to sets of places in G as follows:\nρ(M H) = {(p, d) ∈PH × D | d ∈M H(p)} ⊆P.\nNote that for the initial markings M0 of H and In of G we have ρ(M0) = In.\nThen we can state the following correspondence that is essentially due to [18].\nTheorem 1. For all markings M H\n1\nand M H\n2\nof H, all transitions tH ∈TH,\nand all valuations v ∈Val(tH) the following properties hold:\n– The transition tH is enabled at M H\n1\nunder v in H iﬀthe transition (tH, v)\nis enabled at ρ(M H\n1 ) in G.\n– The ﬁring of enabled transitions under v corresponds to each other:\nM H\n1\n[tH, v⟩M H\n2\niﬀ\nρ(M H\n1 ) [(tH, v)⟩ρ(M H\n2 ).\nHigh-Level Representation of Petri Games\n13\n4\nParametric Benchmark Families\nUsing the tool Adam [9,8], several benchmark families served to demonstrate the\napplicability of the algorithm for solving Petri games. With parameterized high-\nlevel Petri games these benchmark families can now be represented concisely\nby one single formal object. We exemplify this for the benchmarks Concurrent\nMachines (CM) and Self-Reconﬁguring Robots (SR). Due to the clarity of the\nhigh-level representation both families could be optimized (in the size of the game\nor the functionality, respectively) in comparison to the implemented versions\nof [9,8].\n4.1\nCM: Concurrent Machines\nThis benchmark family models n machines of which only n −1 are working\ncorrectly. The environment decides nondeterministically which one is defective.\nThe machines should process k orders and no machine is allowed to process in\ntotal more than one order. Each order can inform itself of the defective machine\nand decide, with or without this information, on which machine it would like\nto be processed. At the end, no order should decide for the defective or for an\nalready used machine. The high-level version of the benchmark family is depicted\nin Fig. 6.\nd\nOK\nERR\ng\nb\nG\nB\nM\np\nO\nSys\ntest\nF(m)\nm\nm\nO\nO\no\n(o, m)\n(o, m)\nm\n(o, m)\n(o, m)\n(o, m)\npar\nn, k : N, k < n\nM = {1, . . . , n}\nO = {1, . . . , k}\nvar\nm : M, o : O\nF : M →P (M)\nm 7→M \\ {m}\nFig. 6: Parameterized high-level Petri game for the benchmark family of con-\ncurrent machines. There are k ∈N orders which can be processed on n ∈N\nmachines. Each machine should only process one order. A hostile environment\ndecides on the functionality of the machines.\nThe n diﬀerent machines of the family are identiﬁed by the individual tokens\nin the set M = {1, . . . , n}. The hostile environment decides to destroy one of\n14\nM. Gieseking and E.-R. Olderog\nthem by putting it into place ERR and all other but this token into place OK\nvia transition d. The k orders which should be processed by the machines are\nidentiﬁed by the individual tokens in the set O = {1, . . . , k}, which initially\nreside in place Sys. The orders can decide to ﬁrst test which machine is defective\n(via transition test) and decide afterwards on which machine they want to be\nprocessed, or choose a machine without any knowledge about the functionality\nof the machines (both via transition p). A tuple (o, m) residing in M, for o ∈O\nand m ∈M, indicates that the order o should be processed by machine m.\nSince the place OK only contains one unique token for each intact machine,\ntransition g can only ﬁre at most |M| −1 times and takes one of those machine\nidentiﬁers of OK each time. Hence, a token (i, e) ∈O × M for orders i which\ndecide on the defective machine e or a machine e which already processed another\norder, is not moved to G but stays in M. Since we are searching for deadlock-\navoiding strategies, this token must eventually end up in the bad place B for\nevery strategy.\nSys\nM1\nM2\nM3\nG1\nB1\nB2\nB3\nG2\nG3\nSys′\nM ′\n1\nM ′\n2\nM ′\n3\nG′\n1\nB′\n1\nB′\n2\nB′\n3\nG′\n2\nG′\n3\ntest1\ntest2\ntest3\nERR1\nOK 1\nERR2\nOK 2\nERR3\nOK 3\nd3\nd2\nd1\nFig. 7: Instantiation of the Petri game of Fig. 6 for |M| = 3 and |O| = 2. The\nk = 2 orders of this instantiation of the concurrent machines benchmark family\nare initially residing in Sys and Sys′. The n = 3 machines are represented by the\nsix places: Mi for the ﬁrst order and M ′\ni for the second order (for i ∈{1, . . . , 3}).\nFigure 7 shows the instantiation of this benchmark family for three machines\nand two orders. The nondeterministic destruction of machines is visualized in the\nleft-most part, whereas the possibilities of the two orders is depicted in the middle\nand the right-most part of the ﬁgure, respectively. Each of the three machines\nm ∈{1, 2, 3} can be functioning, i.e., a token resides in OK i, or defective, i.e.,\na token resides in ERRi. The place M collecting which order is processed on\nHigh-Level Representation of Petri Games\n15\nwhich machine of Fig. 6, as well as the corresponding good and bad place, is\nsplit into |M × O| = 6 places each. The three undecorated copies of each of these\nplaces in the middle belong to the ﬁrst and the decorated in the most-right part\nof Fig. 7 to the second order. This game can be won by the system players by\nﬁrst testing which of the three machines is defective, hence knowing which two\nare functioning, and afterwards unequally deciding on one of the two functioning\nmachines.\n4.2\nSR: Self-Reconﬁguring Robots\nThis benchmark is inspired by [17] and deals with the following scenario. Each\npiece of material needs to be processed by n diﬀerent tools. There are n robots\nhaving all n tools to their disposal, of which only one tool is currently used.\nThe environment may (repeatedly) destroy a tool on a robot r. Then the robots\nreconﬁgure themselves so that r uses another tool and the other robots adapt\ntheir usage of tools accordingly. Destructions can occur repeatedly in diﬀerent\nphases p. We consider two safety objectives for this family of games:\n1. No wrong tool assignment, i.e., no robot uses a tool that is destroyed by the\nenvironment.\n2. Unique tool assignment, i.e., each tool is assigned only to a single robot.\nThe high-level representation of this benchmark family is depicted in Fig. 8.\nThe game proceeds in k = |P| phases, each one starting in the place Phases\nwith type P. Initially, the game starts with phase 1 and for each but the last\nphase k (ensured by the predicate p < k) the transition i1 puts the number of\nthe next phase into Phases and remembers the current phase in place S. For the\nlast phase the identiﬁer k is directly put into S via transitions i2 and no token\nresides in Phases anymore. Next the environment can destroy via the transition\ndes on one robot r one tool t in this phase p by putting the information triple\n(r, t, p) into the system place RTP and the environment token into place W\n(for working). Now the system in place work gets active by ﬁring transition tw,\nwhich withdraws the system token from place work and puts the set of all robot\nidentiﬁers equipped with the current phase p into the system place RP and the\nenvironment token into place C (for completed). Next the transition chg (for\nchange) is enabled. In its preset are the places RP and RT of which the latter\ncontains the current assignment of tools to robots. Here we assume w.l.o.g. that\ninitially RT stores the assignment I where robot i ∈R uses tool i ∈T. In general,\ntransition chg takes one robot identity (r, p) of the current phase p from place\nRP and a tool assignment (r, t) out of place RT and replaces it by the (possibly\nnew) assignment (r, t′). The idea is that t′ is the tool that robot r should use\nfrom now on. The transition chg stores this new assignment by putting the triple\n(r, t′, p) into place R′T ′P′. If t is destroyed by the environment transition des in\nany prior phase ep yielding (r, t, ep) in place RTP then a winning strategy for the\nsystem players should choose t ̸= t′. Otherwise the transition ⊥1 is enabled and\neventually has to ﬁre, i.e., ⊥1 puts the wrong tool assignment (r, t) into the bad\n16\nM. Gieseking and E.-R. Olderog\nwork\ntw\nRP\nchg\nI\nRT\ncheck\nrestart\nc\nT\nTools\nnxt\nW\ndes\nRTP\nR′T ′P ′\nS\ni1\ni2\np < k\n1\nPhases\nP\nEnv\nC\n⊥1\np ≤p′\n⊥2\nBad2\nBad1\np\np\np + 1\nk\nk\np\nF(p)\np\n(r, t, p)\np\n(r, p)\n(r, t)\n(r, t′)\n(r, t′, p)\n(r, t′)\n(r, t)\nt\nr\nR\nT\n(r, t, p)\n(r, t, p′)\n(r, p′)\n(r, t)\nr\npar\nn, k : N\nR = {1, . . . , n}\nT = {1, . . . , n}\nP = {1, . . . , k}\nI = {(i, i) ∈R × T | i ∈{1, . . . , n}}\nvar\nr : R, t, t′ : T, p, p′ : P\nF : P →P (R × P)\np 7→{(r, p) | r ∈R}\nFig. 8: Parameterized high-level Petri game for the benchmark family of self-\nreconﬁguring robots. The n robots have n tools to their disposal of which non-\ndeterministically k tools can be destroyed over time. In this smart factory each\npiece of material needs to be machined by every tool. Every robot can use one\nsingle tool at a time and can decide on a diﬀerent one after the factory recognizes\na defective tool on any of the robots.\nplace Bad1. Additionally, transition chg stores the new tool assignment (r, t′)\ninto place check. Here the unique tool assignment property, i.e., whether each\ntool is assigned only to a single robot, is checked. The place Tools contains one\nunique identiﬁer t ∈T for each tool. Every ﬁring of transition c withdraws one\nof these tools. A robot r can only reach the place restart via transition c if it\ncurrently uses a tool t, i.e., the current tool assignment (r, t) resides in check,\nwhich has not already been used by a robot already moved to restart. This means\nfor two robots r1 and r2 using the same tool t, i.e., (r1, t) and (r2, t) residing\nin check, that one of these duplicate assignment remains in check. Since every\nwinning strategy has to be deadlock-avoiding, transition ⊥2 eventually ﬁres and\nputs one of the robots with the duplicate tool assignment into the bad place\nBad2. When every robot uses a diﬀerent tool, eventually all robots gather in\nplace restart and the transition nxt can enable a new phase by putting a black\ntoken back into the environment place Env.\nHigh-Level Representation of Petri Games\n17\nP1\ni1\nS1\nR1T1P1\nR1T2P1\nR2T1P1\nR2T2P1\nW1\ntw1\nR′\n1T ′\n1P ′\n1\nR′\n1T ′\n2P ′\n1\nR′\n2T ′\n1P ′\n1\nR′\n2T ′\n2P ′\n1\nR1P1\nR2P1\nBad1\nR1T1\nR1T2\ncheck 11\n⊥0\nR2T1\nR2T2\ncheck21\n⊥2\nP2\ni2\nS2\nR1T1P2\nR1T2P2\nR2T1P2\nR2T2P2\nEnv\nW2\ntw2\nR′\n1T ′\n1P ′\n2\nR′\n1T ′\n2P ′\n2\nR′\n2T ′\n1P ′\n2\nR′\n2T ′\n2P ′\n2\nR1P2\nR2P2\n⊥\nBad2\nBad3\n⊥′\ncheck 12\n⊥1\nBad4\ncheck 22\n⊥4\nBad5\nTools1\nrestart1\nTools2\nrestart2\nnxt\nC\nwork\nFig. 9: Instantiation of the Petri game depicted in Fig. 8 for |R| = |T| = |P| = 2.\nThe k = 2 destruction phases are presented in the upper left and upper right\npart, respectively. The tool changing of the robots is depicted in the middle:\nin the upper left part for the ﬁrst robot in the ﬁrst phase, in the upper right\npart for the ﬁrst robot in the second phase, in the lower left part for the second\nrobot in the ﬁrst phase, and in the lower right for the second robot in the second\nphase. The bottom of the ﬁgure presents the starting of the second phase.\n18\nM. Gieseking and E.-R. Olderog\nFigure 9 shows an instantiation of this high-level representation for two robots\nequipped with two tools each and two destruction phases. The ﬁrst phase is\npresented in the left part of the ﬁgure, whereas the second part is depicted in\nthe right one. The destruction of the tools is done in the upper part of the ﬁgure.\nNote that we simpliﬁed the unfolding of the high-level place Bad1, for a clearer\npresentation. In the middle of the ﬁgure at the ﬁrst the changing of the tool for\nthe ﬁrst and below the changing for the second robot is depicted. The bottom\nof Fig. 9 shows the starting of the second phase.\nThis game cannot be won by the system players, because the environment\ncan either decide to destroy both tools of one robot, say of robot r = 1, or decide\nto destroy the same tool, say the tool t = 1, on each robot. In the ﬁrst case the\nrobot has no other possibility than to chose an already destroyed tool, say the\ntool t = 1, in the second phase. This enables transition ⊥1 in the high-level\nversion, which corresponds in the low-level version to the enabledness of either\n⊥or ⊥′, depending on the phase in which the tool t has been destroyed. In the\nsecond case, either one of the robots decides on an already destroyed tool (which\nleads us to the previous case), or both robots decide on taking tool t′ = 2. This\nmeans transition c of the high-level version can only ﬁre for one robot, because\nafterwards the place Tools only contains the tool 1. Thus, nxt cannot ﬁre and\ntherewith ⊥2 eventually has to put the other robot into Bad2. For the low-level\nversion both robots choosing tool 2 results in having a token in each of the places\ncheck 12 and check 22. Hence, only one of the transitions in the postset of Tools2\ncan ﬁre, resulting in the eventually ﬁring of ⊥1 or ⊥4, respectively.\n5\nConclusion\nWe have introduced a new representation of benchmark families for Petri games.\nSimilarly to the advantages of high-level Petri nets versus place\/transition Petri\nnets, the representation captivates by its concise and complete abilities of deﬁn-\ning the families. The possibility to keep the expression sets parametric allows\nfor a uniform representation of the entire family. We have presented an instanti-\nation technique to obtain a low-level version as standard 1-bounded Petri game\nfor each element of the benchmark family. Those Petri games can then be solved\nby the existing algorithms and tool.\nFurthermore, we have experienced that parameterized high-level representa-\ntions of Petri games help to understand the key ideas of benchmark families even\nbetter. This has enabled us to improve each of the presented benchmark families\ncompared to their original implementations regarding their size or functionality.\nCurrently, the known synthesis techniques only apply for the instantiations\nof the parameterized high-level Petri games. In future work we would like to\ndevelop algorithms to directly obtain parameterized distributed controllers from\nsuch parameterized high-level representations.\nAcknowledgement. We thank Wolfgang Reisig for suggesting to use high-level\nPetri nets to represent families of benchmarks during a Dagstuhl Workshop in\nJune 2017.\nHigh-Level Representation of Petri Games\n19\nReferences\n1. Bloem, R.P., Gamauf, H.J., Hoﬀerek, G., K¨onighofer, B., K¨onighofer, R.: Synthe-\nsizing robust systems with RATSY. In: Association, O.P. (ed.) SYNT 2012. vol. 84,\npp. 47–53. Electronic Proceedings in Theoretical Computer Science (2012)\n2. Bohy, A., Bruy`ere, V., Filiot, E., Jin, N., Raskin, J.F.: Acacia+, a tool for LTL\nsynthesis. In: Computer Aided Veriﬁcation (CAV). pp. 652–657 (2012)\n3. Church, A.: Applications of recursive arithmetic to the problem of circuit synthesis.\nIn: Summaries of the Summer Institute of Symbolic Logic. vol. 1, pp. 3–50. Cornell\nUniv., Ithaca, NY (1957)\n4. Ehlers, R.: Unbeast: Symbolic bounded synthesis. In: Tools and Algorithms for the\nConstruction and Analysis of Systems (TACAS). pp. 272–275 (2011)\n5. Engelfriet, J.: Branching processes of Petri nets. Acta Inf. 28(6), 575–591 (1991)\n6. Esparza, J., Heljanko, K.: Unfoldings – A Partial-Order Approach to Model Check-\ning. Springer (2008)\n7. Finkbeiner, B.: Bounded synthesis for Petri games. In: Meyer, R., Platzer, A.,\nWehrheim, H. (eds.) Correct System Design. LNCS, vol. 9360, pp. 223–237.\nSpringer (2015)\n8. Finkbeiner, B., Gieseking, M., Hecking-Harbusch, J., Olderog, E.R.: Symbolic vs.\nbounded synthesis for Petri games. In: Dana Fisman, S.J. (ed.) Proceedings Sixth\nWorkshop on Synthesis, SYNT 2017. pp. 19–39. EPTCS (2017)\n9. Finkbeiner, B., Gieseking, M., Olderog, E.R.: Adam: Causality-based synthesis\nof distributed systems. In: Kroening, D., Pasareanu, C.S. (eds.) Computer Aided\nVeriﬁcation (CAV). LNCS, vol. 9206, pp. 433–439. Springer (2015)\n10. Finkbeiner, B., G¨olz, P.: Synthesis in distributed environments. In: 37th IARCS\nAnnual Conference on Foundations of Software Technology and Theoretical Com-\nputer Science, FSTTCS 2017, December 11-15, 2017, Kanpur, India. pp. 28:1–28:14\n(2017)\n11. Finkbeiner, B., Olderog, E.R.: Petri games: Synthesis of distributed systems with\ncausal memory. In: Peron, A., Piazza, C. (eds.) Proc. Fifth Intern. Symp. on Games,\nAutomata, Logics and Formal Veriﬁcation (GandALF). EPTCS, vol. 161, pp. 217–\n230 (2014). https:\/\/doi.org\/10.4204\/EPTCS.161.19\n12. Finkbeiner, B., Olderog, E.R.: Petri games: Synthesis of distributed systems with\ncausal memory. Information and Computation 253, Part 2, 181–203 (2017).\nhttps:\/\/doi.org\/10.1016\/j.ic.2016.07.006\n13. Finkbeiner,\nB.,\nSchewe,\nS.:\nUniform\ndistributed\nsynthesis.\nIn:\nIEEE\nSymposium\non\nLogic\nin\nComputer\nScience.\npp.\n321–330\n(June\n2005).\nhttps:\/\/doi.org\/10.1109\/LICS.2005.53\n14. Gastin, P., Lerman, B., Zeitoun, M.: Distributed games with causal memory are\ndecidable for series-parallel systems. In: Lodaya, K., Mahajan, M. (eds.) Founda-\ntions of Software Technology and Theoretical Computer Science (FSTTCS), LNCS,\nvol. 3328, pp. 275–286. Springer (2005)\n15. Genest, B., Gimbert, H., Muscholl, A., Walukiewicz, I.: Asynchronous games over\ntree architectures. In: Fomin, F.V., Freivalds, R., Kwiatkowska, M., Peleg, D. (eds.)\nAutomata, Languages, and Programming (ICALP), LNCS, vol. 7966, pp. 275–286.\nSpringer (2013)\n16. Genrich, H.J., Lautenbach, K.: System modelling with high-level Petri nets. Theor.\nComput. Sci. 13, 109–136 (1981). https:\/\/doi.org\/10.1016\/0304-3975(81)90113-4\n17. G¨udemann, M., Ortmeier, F., Reif, W.: Formal modeling and veriﬁcation of sys-\ntems with self-x properties. In: Yang, L., Jin, H., Ma, J., Ungerer, T. (eds.) Auto-\nnomic and Trusted Computing. LNCS, vol. 4158, pp. 38–47. Springer (2006)\n20\nM. Gieseking and E.-R. Olderog\n18. Jensen, K.: Coloured Petri Nets: Basic Concepts, Analysis Methods and Practical\nUse, Volume 1. Springer (1992)\n19. Jobstmann, B., Galler, S., Weiglhofer, M., Bloem, R.: Anzu: A tool for property\nsynthesis. In: Computer Aided Veriﬁcation (CAV). LNCS, vol. 4590, pp. 258–262.\nSpringer (2007)\n20. Madhusudan, P., Thiagarajan, P.S., Yang, S.: The MSO theory of connectedly\ncommunicating processes. In: Foundations of Software Technology and Theoretical\nComputer Science (FSTTCS). pp. 201–212 (2005)\n21. Muscholl, A., Walukiewicz, I.: Distributed synthesis for acyclic architectures. In:\nProc. FSTTCS. LIPIcs, vol. 29, pp. 639–651. Schloss Dagstuhl - Leibniz-Zentrum\nf¨ur Informatik (2014). https:\/\/doi.org\/10.4230\/LIPIcs.FSTTCS.2014.639\n22. Nielsen, M., Plotkin, G.D., Winskel, G.: Petri nets, event structures and domains,\npart I. Theor. Comput. Sci. 13, 85–108 (1981)\n23. Pnueli, A., Rosner, R.: Distributed reactive systems are hard to synthesize. In:\nProc. FOCS’90. pp. 746–757 (1990)\n24. Reisig, W.: Petri Nets: An Introduction. Springer (1985)\n25. Reisig, W.: Understanding Petri Nets - Modeling Techniques, Analysis Methods,\nCase Studies. Springer (2013). https:\/\/doi.org\/10.1007\/978-3-642-33278-4\n26. Zielonka, W.: Notes on ﬁnite asynchronous automata. Theoret. Informatics and\nApplications (ITA) 21(2), 99–135 (1987)\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/High-Level Representation of Benchmark Families for Petri Games.pdf"}
{"title":"Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks","authors":"Rodrigo Canaan, Christoph Salge, Julian Togelius, Andy Nealen","summary":"From the beginning if the history of AI, there has been interest in games as\na platform of research. As the field developed, human-level competence in\ncomplex games became a target researchers worked to reach. Only relatively\nrecently has this target been finally met for traditional tabletop games such\nas Backgammon, Chess and Go. Current research focus has shifted to electronic\ngames, which provide unique challenges. As is often the case with AI research,\nthese results are liable to be exaggerated or misrepresented by either authors\nor third parties. The extent to which these games benchmark consist of fair\ncompetition between human and AI is also a matter of debate. In this work, we\nreview the statements made by authors and third parties in the general media\nand academic circle about these game benchmark results and discuss factors that\ncan impact the perception of fairness in the contest between humans and\nmachines","url":"http:\/\/arxiv.org\/abs\/1903.07008v4","pdf_url":"http:\/\/arxiv.org\/pdf\/1903.07008v4","published":1552783346000,"comment":"7 pages","pdf_text":"Leveling the Playing Field\nFairness in AI Versus Human Game Benchmarks\nRodrigo Canaan\nDepartment of Computer Science and Engineering,\nNew York University\nBrooklyn, NY\nrodrigo.canaan@nyu.edu\nChristoph Salge\nSchool of Computer Science,\nUniversity of Hertfordshire\nHatfield, UK\nChristophSalge@gmail.com\nJulian Togelius\nDepartment of Computer Science and Engineering,\nNew York University\nBrooklyn, NY\njulian@togelius.com\nAndy Nealen\nSchool of Cinematic Arts,\nUniversity of Southern California\nLos Angeles, CA\nanealen@cinema.usc.edu\nABSTRACT\nFrom the beginning of the history of AI, there has been interest in\ngames as a platform of research. As the field developed, human-\nlevel competence in complex games became a target researchers\nworked to reach. Only relatively recently has this target been finally\nmet for traditional tabletop games such as Backgammon, Chess\nand Go. This prompted a shift in research focus towards electronic\ngames, which provide unique new challenges. As is often the case\nwith AI research, these results are liable to be exaggerated or mis-\nrepresented by either authors or third parties. The extent to which\nthese game benchmarks constitute “fair” competition between hu-\nman and AI is also a matter of debate. In this paper, we review\nstatements made by reseachers and third parties in the general\nmedia and academic publications about these game benchmark\nresults. We analyze what a fair competition would look like and\nsuggest a taxonomy of dimensions to frame the debate of fairness\nin game contests between humans and machines. Eventually, we\nargue that there is no completely fair way to compare human and\nAI performance on a game.\nCCS CONCEPTS\n• Computing methodologies →Philosophical\/theoretical foun-\ndations of artificial intelligence; • Applied computing →Com-\nputer games.\nKEYWORDS\nGames, Game AI, AI Benchmarks, Fairness\nACM Reference Format:\nRodrigo Canaan, Christoph Salge, Julian Togelius, and Andy Nealen. 2019.\nLeveling the Playing Field: Fairness in AI Versus Human Game Benchmarks.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand\/or a fee. Request permissions from permissions@acm.org.\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\n© 2019 Copyright held by the owner\/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-7217-6\/19\/08...$15.00\nhttps:\/\/doi.org\/10.1145\/3337722.3337750\nIn The Fourteenth International Conference on the Foundations of Digital\nGames (FDG ’19), August 26–30, 2019, San Luis Obispo, CA, USA. ACM, New\nYork, NY, USA, 8 pages. https:\/\/doi.org\/10.1145\/3337722.3337750\n1\nINTRODUCTION\nGames have a long history of being used as Artificial Intelligence\ntestbeds and benchmarks. The formalized representation, especially\nof board games, and the often explicit and clear reward structure\nmakes them well suited for AI approaches. With recent advances\nin AI research there is also an emergent narrative that general\ngame playing, i.e. playing a range of different games with the same\nagent, is a necessary stepping stone towards Artificial General\nIntelligence (AGI) [39, 57]. Setting this question aside, we want to\nfocus on another related question: Are games a good way to test if\nan AI has human-level intelligence?\nThis question is often brought up, given that humans are widely\naccepted to be the only known example of general intelligence.\nSurpassing or at least achieving parity with human-level artificial\nintelligence seems to be a necessary step to reach AGI, and what\nbetter way to demonstrate this then to beat a human (or the best\nhuman) in a “fair” competition of intelligence, i.e. beat them in a\ngame? Consequently, the media, and to some extent the scientific\nliterature, often characterizes human-AI game competition as a way\nto determine if an AI has finally reached human-level intelligence.\nThe central argument presented in this paper objects to this\ncharacterization. We argue that there is no, and that there can never\nbe a “fair” comparison between an AI and a human that answers the\nquestion if an AI has human-level intelligence. Building up to this\nargument, we will first give a short survey of how human-AI game\ncompetitions are currently portrayed, both in the media and in\nscientific literature. We will then look at several existing examples\nof human-AI game comparisons and demonstrate why it is hard\nto establish a fair comparison for those specific examples. Based\non these examples we will then make a more general argument,\noutlining how the range of different AIs is just too wide to find\nany game that offers a fair comparison. As a corollary, we can say\nthat for something to have human-level AI in a meaningful way it\nwould have to be human, or very close to human, in both physical,\nmental and social embedding.\narXiv:1903.07008v4  [cs.AI]  29 Aug 2019\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\nCanaan et al.\n2\nPORTRAYAL OF AI GAME BENCHMARK\nACHIEVEMENTS\nTo set the stage for our discussion about benchmarks comparing\nAI and humans, we will look at important AI benchmarks in classic\nboard games such as Backgammon, Chess an Go, as well as modern\nelectronic games. We will inspect the claims that were made by the\noriginal authors of the systems that achieved success in these game\nbenchmarks, and how those results were subsequently discussed in\nthe general media and follow-up academic papers. As an analysis\ntool, we will use previously published guidelines on how to write\nabout AI research such as [3] and [40]. Our goal in this section is\nto illustrate how game AI benchmarks are perceived by society,\nand what the main concerns regarding the fairness of comparison\nbetween human and AI programs are.\nNote that the examples gathered in this sections are not meant\nas a statistically representative sample of all that has been written\nabout each of these achievements.\n2.1\nTD Gammon\nTD-Gammon [48] is a Backgammon-playing program developed\nby Gerald Tesauro at IBM using temporal-difference learning, a\nreinforcement learning technique where a neural network is trained\nthrough self-play to minimize the difference between its prediction\nof the game’s outcome and the actual outcome over successive\ngame states. Between 1991 and 1992, it played over a hundred\ngames against some of the best players in the world across three\ndifferent versions of the software. The last version (TGD 2.1) came\nvery close to parity with Bill Robertie, a former world champion,\nlosing a 40-game series by a difference of a single point.\nTesauro highlights that observing the algorithm play has led to\na change in how humans evaluate positions, especially in opening\ntheory for the game. In particular, with some opening rolls, the\nsystem preferred \"splitting\" its back checkers rather than the more\nrisky, but favored at the time, option of \"slotting\" its 5-point. Since\nthen, the splitting opening has been confirmed as the superior\nchoice by computer rollouts and is now the standard choice for the\n2-1, 4-1 and 5-1 initial rolls.\nWhen discussing applicability in other domains, Tesauro lists\nrobot motor control and financial trading as potential applications\nwhile cautioning that the lack of a forward model and the scarcity\nof data might limit the success in these real world environments.\nNot much discussion of TD-Gammon’s achievements was found in\ngeneral media dating from the time of its release, but Woolsey, an\nanalyst in Tesauro’s paper [48] states that TD-Gammon’s algorithm\nis “smart” and learns “pretty much the same way humans do”, as\nopposed to “dumb” chess programs that merely calculate faster\nthan humans. It is interesting from a historical perspective to see\nthat the distinction between real or human-like intelligence and\nmere calculations was already a concern even before Deep Blue’s\nsuccess in Chess, which we examine next, brought the issue to the\nforefront.\n2.2\nDeep Blue\nDeep Blue [7] is a program and purpose-built computer for playing\nchess, designed by a team at IBM led by Murray Campbell. It uses\na combination of specialized hardware and software, such as tree-\nsearch augmented by heuristics for pruning and state evaluation\ncrafted by human experts. It also uses a database of opening moves\nand endgame scenarios to select moves at the start and end of\na match with little computational effort. It achieved enormous\nvisibility in 1997 when it defeated the reigning champion Garry\nKasparov in a six-game match with tournament rules by a score of\n3 1\n2 −2 1\n2. Kasparov had previously beaten a former version of the\nalgorithm in 1996 by a score of 4 −2.\nIn their paper describing the system [7], the authors refrain\nfrom making speculative claims about the algorithm or its impact\non the future of AI. However, the same cannot be said about the\nmedia. One article from the Weekly Standard, with the ominous\ntitle “Be Afraid” [54], on the one hand claims that the system’s\n“brute force” approach in the first game is mere calculation and “not\nartificial intelligence”, but on the other hand attributes brilliance,\ncreativity and humanity to a win in the second game from a position\nthat allegedly did not benefit as much from brute-force calculation.\nThey also speculate on the real-world applications of Artificial\nIntelligence by imagining a scenario where machines might become\n“creatures sharing our planet who not only imitate and surpass us\nin logic, who have perhaps even achieved consciousness and free\nwill, but are utterly devoid of the kind of feelings and emotions\nthat, literally, humanize human beings”.\nOther commentators, such as in this New York Times article [49]\ncharacterize both Deep Blue and the human brain as information\nprocessing machines, with the main difference being that Kasparov\nand humans have feelings such as fear and regret. While Deep\nBlue has no such feelings, the authors speculate that in the future,\nmore sophisticated machines (which they name “Deeper Blue” and\n“Deepest Blue”) might be able to model its opponent and even have\nlife goals outside of chess, such becoming famous, and even achieve\nconsciousness. Ironically, they speculate that the machine might\nthen be vulnerable to psychological warfare, at which point humans\nwould again stand a chance in a game of chess.\nAnother article, also from the New York Times [50] highlights\nKasparov’s own comments not only on the psychological differ-\nences such as fear, but also physical differences such as the need\nto rest. Kasparov also argues that previous games by Deep Blue\nshould have been made available prior to the match. This final\nremark might be justified by the fact that Deep Blue’s “opening\npreparation was most extensive in those openings expected to arise\nin match play against Kasparov”, although ultimately “none of the\nKasparov-specific preparation arose in the 1997 match.” [7].\nThe biggest takeaway from Deep Blue’s success is that the game\nof chess had held, to some extent, the status of a “representative\nmeasure of both human and computer intelligence” [11]. When a\nconceptually simple algorithm was finally able to convincingly beat\none of the best players of its time, the implications of this feat were\nwidely discussed, including whether or not the victory was achieved\nunder fair circumstances, considering the differences among the\nsystems (human feelings, exhaustion), the amount of preparation\ntime and the nature of the underlying reasoning systems, that is,\nthe difference between supposedly sophisticated human reasoning\nversus brute-force calculations.\nLeveling the Playing Field\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\n2.3\nAlpha Go, Alpha Go Zero and Alpha Zero\nIn 2016, AlphaGo [43], an agent developed by group of Google\nDeepMind researchers led by David Silver, became the first program\nto beat a human champion of Go in a match against Lee Sedol, in\nwhich AlphaGo won by 4 −1. The game had been considered one\nof the next big challenges for game AI after chess, due to its large\nsearch space and the difficulty to craft a good state evaluation\nfunction that accurately predicts the winner of each position.\nThe system achieved success through a combination of Monte\nCarlo Tree Search with convolutional neural networks, which learned\nfrom professional human games and self-play. In 2017, they an-\nnounced a new version, AlphaGo Zero [45], which learned entirely\nfrom self-play, with no human examples, and which was able to\nbeat the previous AlphaGo version (AlphaGo Lee). Still in 2017,\nthey announced AlphaZero [44], which uses a similar architecture\n(but different input representations and training) to beat other top\nengines in Go, Chess and Shogi.\nThe authors claim that the later versions of the system, (i.e.,\nAlphaGo Zero and Alpha Zero) master the games without human\nhelp, or Tabula Rasa. These claims were scrutinized in a paper\nby Gary Marcus [23], who views the agent not as learning from\na “blank slate”, but as an example of hybrid system that benefits\nboth from learned behaviors and from prior human knowledge that\ncontributed to the system. In particular, he points out the inability\nof the system to generalize to variations of the game without further\ntraining. The system is also unable to learn the paradigm of tree\nsearch or the rules of the game, which humans are capable of.\nIn the general public sphere, AlphaGo and its successors also\nreceived wide media coverage. While Deep Blue raised questions\nabout the nature of intelligence due to its conceptually simple “brute\nforce” approach, discussions about AlphaGo and its successors are\nmore focused on:\n• the claim that it uses comparatively less human supervision\nand expertise and\n• the fact that it can learn from large amounts of data.\nThe first point leads into a perception of it being both more\ngeneral and less controllable, which raises questions of automation,\njob loss, general intelligence, creativity and the need for supervision\nand interpretability. The second leads to concerns about privacy\nand bias when systems using similar techniques are applied to more\nsensitive domains.\nAn article from Wired [55] acknowledges the concerns that the\nunderlying technology behind AlphaGo could be hard to control\nand lead to job losses, but ultimately delivers a vision of a relation-\nship between human and machine where they complement each\nother, pointing that there are still things machines cannot do, that\nplayers are now able to view the game in a different light thanks to\nAlphaGo and highlighting that both Lee Sedol and AlphaGo (and\nthus humans and machines) are able to generate “transcendent”\nmoments, such as the now famous move 37 (by AlphaGo) and move\n78 (by Sedol), both of which were evaluated by AlphaGo as having\na probability of being played by a human close to one in ten million.\nAn article by The Washington Post [34] also looks at move 37, and\nasks experts about its implications for creativity. One interviewee,\nPedro Domingos, sees the move as creative. Others, such as Jerry\nKaplan, attribute the move to clever programming, not creativity\nof the software.\nAnother article, titled “Why is Elon Musk afraid of AlphaGo-\nZero?” [56] takes a bleaker view, describing the advancements from\nAlphaGo to AlphaGo Zero as an example of the AI becoming “smart\nand self-aware” and creating “its own AI which was as smart as\nitself if not smarter”. The article goes on to wonder about the risks of\nsuch an AI being able to generalize from data in other domains, such\nas in the defense and military industry, and potentially becoming\n“unsafe”, again showing concern for what it would mean for a super-\nhuman, uncontrollable AI when applied to non-game applications,\nechoing the fear of the “technological singularity” hypothesized by\nVinge [52].\n2.4\nElectronic games\nElectronic games (or video games) offer additional challenges to\nAI researchers compared to traditional tabletop games. Due to\na combination of almost continuous time scale (limited by the\nsystem’s frame rate) and potentially huge game state space and\naction space, electronic games are typically even more intractable by\nbrute-force search than games such as Go or Chess. As an example,\nan estimate by Ontañon et al, [26] estimates the state space of\nStarcraft as 101685, its branching factor as 1050 and its depth as\n36000, whereas Go has corresponding values of roughly 10170, 300\nand 200. As such, a number of video game AI benchmarks have\nbeen proposed. The use of video games as AI benchmarks goes\nback a long way but interest in these benchmarks has spiked since\nAlphaGo’s results in 2016, because Go, which was considered among\nthe most challenging tabletop games, was finally beaten and new,\nharder challenges had to be explored.\nSome of these benchmarks encourage the development of gen-\neral techniques, that can be applied for a large number of domain\nproblems, such as different games. That is the case of frameworks\nsuch as the Arcade Learning Environment (ALE) [2], where agents\ncan be evaluated in one of hundreds of Atari 2600 games and the\nGeneral Video Game AI Competition [33], where agents are evalu-\nated in previously unseen arcade-like games.\nOther examples of benchmarks proposed for specific games are\nVizdoom [19] (first person shooter), the Mario AI Benchmark [18]\n(platform game) and even benchmarks not focused on winning a\ngame, but on tasks such as designing game levels in a platform\ngame [42], building adaptive settlements in Minecraft [38] or, in-\nspired by the Turing test, playing in a way that is indiscernible\nfrom humans [14].\nWhile all these benchmarks have garnered academic interest,\nnone has arguably received as much general media coverage and\nplayer attention as AI challenges using Starcraft (both the Brood-\nwar [24] and Starcraft II [5] versions) and Dota2 [16]. The fact that\nboth Starcraft and Dota 2 are popular eSports seems to have helped\ngarner a lot of attention from the community of players.\nReal-Time Strategy (RTS) games, of which Starcraft is an exam-\nple, have been proposed as an AI research environment at least\nsince 2003 [6] and games of the genre have seen organized AI\ncompetitions since 2006 [26]. Starcraft itself has had yearly AI com-\npetitions since 2009, and gained additional popularity as a research\nplatform after Google DeepMind and Blizzard, the game’s publisher,\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\nCanaan et al.\njointly released a reinforcement learning environment for the game\nin 2017 [53]. At the time of the first draft of this paper, the state of\nthe art had been steadily advancing, but humans still had a clear\nadvantage in the game, even if some observers were betting on an\nAI victory in the near future [37].\nIn the time between this paper’s first draft and final version,\nhowever, DeepMind unveiled AlphaStar [10], their Reinforcement\nLearning agent for Starcraft 2 that managed to beat professional hu-\nman players TLO and MaNa behind closed doors, before eventually\nlosing a live showmatch to MaNa. Interestingly, between the closed\ndoor matches that were won and the live event, the developers made\nsome changes to the agent, which previously could observe the\nwhole map at once (but not areas covered by “fog of war”) and later\nwere required to actively control which regions of the map were\nobserved by moving the camera similar to how a human would.\nThis can be seen as an effort to minimize one potential source of\nunfairness in the comparison with humans, by making the input\nand output spaces more similar to the ones experienced by human\nplayers.\nThe main criticism of the agent came from the fact that it was too\nefficient at controlling multiple units at once (also called “micro”),\nhowever Huang [15] argues that, because research in AlphaStar\nis meant to help develop techniques that help solve complex real-\nworld problems, it is a good thing that the agent is able to find a\nstrategy that leverages its advantages to such extent.\nAgents for playing Dota 2 have also been target of recent research\nand seen significant developments. In 2017, an agent by OpenAI\ndefeated Dendi, one of the best players in the world, in a limited\n1v1 version of the game in a showmatch during an official Valve\ntournament [27]. Then, a version capable of 5v5 team play defeated\na team of 5 semi-professional players in the 99th percentile of skill\nin another showmatch in 2018 [29, 30] before eventually losing\nto professional players in a showmatch during The International\n8 [28], the biggest Dota 2 event of 2018.\nAs happened with Starcraft, the state-of-the art moved between\nthis paper’s first draft and final print. In April 2019, a new version\nof the agent was finally able to defeat OG, the world champion\nteam, in a live showmatch [31]. The new version was also able to\nplay alongside (not just against) humans. This version of the bot\nwas made available for play with and against players around the\nworld for 4 days, after which the agent held a win rate of 99.4% in\ncompetitive mode, although one team of humans was able to win\n10 games in a row against the bots [32].\nA major point of debate has been the way the OpenAI agent visu-\nalizes and interacts with the game. OpenAI describes the interface\nin [29] and it includes high-level features which allow the agent to\n\"see\", at any point in time, information such as the remaining health\nand attack value of all units in its sight. A human would have to\nclick on each unit, one by one, to view this information. Agents can\nalso specify its actions at a high level by selecting ability, target,\noffset and even a time delay (from one to four frames). A human\nwould have to make a combination of key presses and imprecise\nmouse movements to achieve the same effect.\nAn article on Motherboard [25] has described the advantages\nprovided to the AI as “basically cheating”, summing up that, while\nhumans have previously been disqualified from tournaments due\nto the use of a mouse with illegal programmable macro actions,\n“Open AI Five plays like an entire team with programmable mice\nand telepathy”. The article also proposes that the agent should learn\ndirectly from visuals.\nIn a blog post [9], AI researcher Mike Cook, while ultimately\nhaving a positive view on the benchmark, also comments on the\ninterface advantages, drawing attention to some highlights of the\ngames where, even though the agents have a reaction speed of\n200ms (in theory comparable to humans), they executed key actions\nsuch as interrupting a spell or coordinating powerful abilities in a\nway that is seemingly impossible for humans. Cook also warned\nabout the potential of the AI to fall prey to situations it has never\nencountered in its self-play (such as the technique of “creep pulling”\nor unusual hero lineups) and that good performance in a few facets\nof the game (such as teamfighting) might give the illusion of greater\noverall competence in the game.\nA final critique against OpenAI’s agents came from the number\nof simplifications that had to be made to tackle a game as complex\nas Dota 2, such as playing with a reduced hero pool, the innability\nto fight Roshan (a powerful NPC that typically takes risky team-\nwide efforts to kill, but drops a valuable reward and is often the\nfocus of game-deciding fights between teams) and the choice to\nhave individual invulnerable couriers per player (as opposed to\na vulnerable courier shared by the entire team). These demands\ncan be seen in game forums such as [21, 36] and ultimately led to\nOpenAI’s decision to drop most restrictions (other than the limited\nhero pool) in preparation for the final matches at The International\n8 (which OpenAI lost) and [28] and against OG (which OpenAI\nwon) [31].\n3\nDIMENSIONS OF FAIRNESS\nFairness is a slippery concept to define. Much has been said on\nthe subject in contexts such as political philosophy [35], economic\ntheory [17] and even social behavior between primates [4]. A gen-\neral definition of fairness is outside the scope of this paper, but,\nin the realm of sports and gaming, a common stance seems to be\nthat a competition is unfair if one side is substantially favored by\ncircumstances that should not affect the game.\nA version of chess where players started with different pieces\nbased on how much weight they can lift or how much money they\nchoose to pay before the match would be considered unfair, as\nphysical strength and monetary wealth should not affect chess.\nHowever, no one would argue that weight lifting is unfair because\nit gives an advantage to the strongest lifters, and some online games\n(not without controversy) allow players to pay real money for game\ncontent that can be used against other players. These examples\nshow both that what exactly constitutes “fair game” is highly game-\nspecific, and that it is often a matter of debate even within a single\ngame community.\nPeter Stone et al. [46] refer to “preserving the essence of the\ngame” when discussing what the rules should be for a fair soccer\ncompetition between human and robot players. Preserving the\ncharacteristics that define the essence of the game not only allows,\nin their view, for a competition to be seen as fair, but also makes it\nso that, if the robots should win, most people would agree that the\nrobots beat human at soccer (and not some other game that simply\nresembles soccer on the surface).\nLeveling the Playing Field\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\nHowever, there are multiple ways in which a competition could\nbe seen as breaking the essence of the game and allowing factors\nthat should not affect the game to interfere with the results of the\nmatch. While Stone et al. list the rules and restrictions they believe\nshould apply to their competition (e.g. a robot should not be able to\nrun much faster than a human can, and should only communicate\nwith each other through sounds that would be perceptible to the\nhuman ear), our aim in this section is to provide a (non-exhaustive)\ntaxonomy of dimensions of fairness for human versus AI competi-\ntions in general each representing one category of factors that are\noften pointed to (in our observations) as the cause of unfairness in\nthe types of human versus AI competitions in games such as the\nones analyzed in section 2.\nInput fairness: do the two systems have the same input space,\ne.g. pixels from a screen? This is especially relevant in electronic\ngames, where it might be the case that a human can only see the lim-\nited information on the screen, and needs to make specific actions\nto gather more information, such as scrolling the viewing window\nor clicking units to see their attributes. In contrast, an algorithm\nmight have as input a structured list with the location and status of\ngame objects. Even in cases where the input is ostensibly the same\n(such as an algorithm playing from pixels), is the image capturing\napparatus equivalent to a human eye? As an example, the human\nretina has blind spots and lower peripheral resolution [47]. Does\nthe computer suffer from the same limitations?\nOutput fairness: do both systems have the same output space,\nthat is, the same ways of interacting with the world and with other\nplayers? Do they have the same actions to choose from? The same\nreaction speed? Does the computer have to actually press physically\nbuttons or move pieces on a board? Are the systems limited by the\nsame constraints of strength, speed and precision? In cases where\ncommunication with other players is possible, do they communicate\nusing the same channels and protocols?\nExperience fairness: have the human and the machine spent\nthe same amount of time playing the game? Have they played the\ngame under the same conditions? What about time spent playing\nclosely related games?\nKnowledge fairness: have the agents had access to the same\ndeclarative knowledge (compiled by others) about the game? For\nexample opening books, pre-trained neural networks, tables of state\nvalues?\nCompute fairness: do the agents have the same computational\npower? How should computational power be measured in this case?\nA natural metric to use might be power consumption in watts or\ntotal energy spent in joules, but it has the shortcoming that, in the\ncase of humans, it is not easy to determine how much power is\nbeing used to play the game as opposed to other essential cognitive\nor bodily functions. The issue gets even more complicated when\nconsidering the total energy spent before the match, for example\nwhile learning rules and strategies for the game. We could turn to\nother metrics such as the number of neurons or storage capacity,\nbut it is even harder to establish a meaningful comparison in these\ncases. Another alternative could be based on the number or depth\nof game states explored during tree search, which tends to heavily\nfavor computers.\nFinally, the monetary cost of the total computation and the infras-\ntructure required could also be considered, especially in the context\nof discussing the possibility that a machine might replace humans\nin a task after achieving “super-human performance”, which is im-\npractical if deploying and maintaining the artificial agent is several\ntimes more expensive than hiring a human to do the same task.\nPsychological fairness: are agents subject to the same range\nof variation in performance due to emotions and states such as fear,\njoy, mental fatigue, etc.? Currently, this would seem to only apply\nto humans.\nCommon-sense fairness: do the agents have the same knowl-\nedge about other things that may factor into the game? Have they\ngone to school? Do they know that dragons are typically dangerous\nand coins are typically desirable? Have they seen an American\ntraffic sign? Do they know that cracked walls are more likely to\nbreak when you bomb them? Do they understand instructions given\nin natural language by NPCs? With current technology, this is a\ndimension that tends to heavily favor humans.\nNote that when it comes to human-versus-human competitions,\nthe dimensions of input and output are usually explicitly addressed\nas a matter of fairness (such as the creation of divisions and leagues\nby weight or disability, and the banning of certain performance-\naltering substances), but variations in the remaining dimensions are\nusually treated as part of individual player skill. A human player\nmight be praised for having more knowledge of the game, calculat-\ning more moves ahead or handling stress better than their opponent,\nbut a machine with advantages in the same areas might be seen as\nunfair.\nEach of the dimensions is actually a continuum rather than a\nbinary, and, as will be discussed below, not all of them are equally\nrelevant to all games. Regardless, it should be clear at this point\nthat even determining what constitutes fairness at a game is a\nchallenging task. We will argue that achieving complete fairness in\nall dimensions would require building a system that is essentially\nequivalent to a real, flesh-and-blood human with a full history of\nhuman-like experiences.\n4\nA DISCUSSION OF FAIRNESS IN HUMAN-AI\nGAME BENCHMARKS\nAt a first glance, the issue of fair conditions in between a computer\nagent and a human seems more tractable in tabletop games, such\nas Backgammon, Chess and Go, than in electronic games. A major\ndifference between the two domains seems to be that what we\ncalled Input and Output fairness are less relevant in tabletop games.\nAs an example, AlphaGo required a human facilitator to input the\ncurrent board state into the system, and to apply the move selected\nby the algorithm to the physical board. Modifying the system with\na camera to read the board state and a robotic arm to move the\npieces might be interesting Computer Vision and Robotic problems\non their own, but it would be hard to argue that it would constitute\na better Go player or that the competition with humans would be\nfairer.\nDue to this significant difference, we divide discussion below\nbetween tabletop games and electronic games. Issues discussed for\ntabletop games in general also apply to electronic games, but the\nreverse is not necessarily true.\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\nCanaan et al.\nAnother category that could be considered is that of games\ninvolving direct interaction with the real world, such as the afore-\nmentioned Robocup [46], where robots play soccer against robots.\nThese raise even more questions about fairness in the sensorimotor\nsense, as the physical speed, strength, weight and dimensions of\nthe robots have to be constrained to a similar range than that of\nhumans. However, an in-depth discussion of this topic is outside\nthe scope of this paper and we will focus the remaining discussion\non tabletop and electronic games such as the ones described in\nsection 2\n4.1\nFairness in Tabletop games\nThe first key issue affecting the fairness between human and ar-\ntificial players in tabletop games are feelings such as fatigue, fear,\nanxiety, etc. In [50], Kasparov comments on the role these factors\ncan play in a match. Ke Jie [51], another prominent Go player\nwho has also lost to AlphaGo, stated that psychological factors are\npossibly “the weakest part of human beings”. Additionally, sports\ncommentators regularly build a narrative around the mental fac-\ntors going into an important match, especially one where a lot of\npride or money is involved. The magnitude of the psychological\neffect is unclear from this brief study, but, to whatever degree it\nmight change the outcomes, compensating for it is intractable with\ncurrent technology. There is no straightforward way to account for\nthese emotions in a computer simulation, and attempting to do so\n(e.g. by artificially injecting noise in the algorithm’s evaluation in\nsituations of high stress) would defeat the purpose of building the\nbest possible game-playing systems.\nA second issue that can be raised is the use of look-up tables for\nspecific points of a match, such as the opening and endgame, and\nthe availability of information about a specific opponent in a match.\nThese could be seen as a matter of Knowledge fairness. Look-up\ntables have been used in Deep Blue [7] and suggested as a potential\nimprovement for TD-Gammon’s identified weakness in endgame\nsituations. [48]. The use of similar resources in most competitive\nmatches between humans is banned, but when playing versus a\ncomputer, should a human have access to the same tables that are\navailable to the algorithm? Similarly, if an algorithm is capable\nof studying examples of human play in general (as is the case for\nthe original AlphaGo [43]) or even have some of its parameters or\ndesign decisions tuned to face a specific human player (as happened\nwith Deep Blue [7]), wouldn’t it be fair for a human to review a\nlarge number of games by an artificial agent, receive a detailed\nsummary of its preferred openings and strategies, perhaps even\ninspect the source code?\nIn the same vein, the use of a forward model to simulate future\ngame states, as is done during tree search in Deep Blue [7] and the\nrollouts of AlphaGo [43] could be considered an issue of Compute\nfairness . This could be compared to giving a human a set of extra\nboards and pieces with which to simulate potential lines of play\nduring a match, which is also not allowed in competitive play. An\nimportant observation is that while it is possible for a human to fully\nsimulate a game of Chess or Go in this way, it is harder to do the\nsame for games that involve randomness and hidden information,\nand simulation becomes even harder for an unassisted human if\nthe task involves continuous dimensions such as time and distance.\nWe would like to relate these issues to what is called System 1\nthinking and System 2 thinking in dual-process theory [12]. System\n1 thinking, often called intuition or heuristic thinking, has been\nrelated in the Reinforcement Learning context to the selection of\nactions without lookahead [1], such as using a look-up table for\nopenings or a neural network pre-trained on a database of game\nstates. System 2 thinking consists of conscious analytical reasoning,\nand has been related to Tree Search [1]. While both types of thinking\ncould be augmented for humans through the use of external tools\nsuch as notes on a piece of paper or extra boards for simulation of\nlines of play, an argument could be made following the Extended\nMind [8] that whether such resources are internal or external to\na system makes little difference when considering the system’s\ncognitive abilities. Taking this argument to the extreme, we could\nimagine a situation where a complete artificial game-playing system\nis viewed as mere augmentation of a human’s cognitive abilities,\nleading to the absurd scenario of a \"human versus AI\" match where\nnonetheless all moves are selected by the same algorithm, one\nplaying for itself, the other in the human’s stead.\nIn the opposite direction, we could attempt to reduce the com-\nputer’s advantage by restricting what kinds of techniques it is\nallowed to use, disallowing the ones that are viewed as inherently\nunfair. Ultimately, however, by a line of reasoning similar to the\nChinese Room thought experiment [41] , all of the aforementioned\nadvantages could be reduced to following instructions on a piece of\npaper or doing mere calculations and so no AI achievements could\never be considered as proof of true mastery in a game.\nA final issue, unrelated to the use of a forward model, is the ma-\nchine’s ability to generalize what it learned across different games\nor variations of the same game. According to Brooks [3], humans\nare prone to infer competence from performance. As humans, we\nmight expect a system that performs as the best Go player in the\nworld to be competent enough to play on a board of different di-\nmensions, or play with a different goal (such as the intent to lose) or\nbe at least a passable player in another similar game (such as chess).\nMarcus [23] points out that this is not the case with most existing\ntechniques, and addressing this issue is the motivation behind com-\npetition frameworks such as ALE [2], GVGAI [33] and GGP [13].\nWhile this doesn’t strictly affect the fairness of competitions based\non playing a single game, with a single ruleset, it is an important\npoint to consider against the narrative that sees the success of AI\nin a new task as evidence that AGI is just around the corner.\n4.2\nFairness in Electronic Games\nThe major difference between tabletop games and electronic games\nwhen it comes to perception of fairness seems to be rooted on\nthe representation of the observation and action space, as well as\nreaction time, as discussed in [9, 25]. These are related to what we\ncall Input and Output fairness.\nRegarding the observation space, a common paradigm suggestion\nto achieve greater fairness is playing the game from pixels, rather\nthan from higher level game features. This is the approach followed\nby Vizdoom [19] and ALE [2]. While the approach can be said to\nmore closely emulate the way humans perceive video games, the\ncomparison is not perfect.\nLeveling the Playing Field\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\nOn one hand, favoring the AI, questions such as “is the distance\nbetween these two objects smaller than the range of my spell?” are\nstill much easier to answer accurately for an agent playing from\npixels than for a human. On the other hand, when a human sees\npixels in the shape of a coin, a spider and fire, they can reasonably\ninfer that the first object has to be collected, the second attacked and\nthe third avoided, and such heuristic would work well for many\ngames (what we call the common-sense dimension of fairness).\nEmbedding this representation and real-world knowledge in a visual\nAI system is an unsolved problem, which provides humans with an\nadvantage that is not easy to surmount at the moment.\nWhile objections to high-level representations are valid, taken to\nthe extreme, these objections would imply that no meaningful ad-\nvancements could be made in video game-playing AI before the field\nof computer vision is essentially solved. This would be disappoint-\ning from a game AI perspective. After all, low-level recognition\nof pixel patterns is not what immediately comes to mind when\nwe picture a human expertly playing a game. Results obtained on\nless structured or more general representations can arguably be\ncharacterized as more impressive, but the challenges involved in\ndealing with lower level representations don’t necessarily capture\nwhat makes games such interesting AI problems in the first place.\nFor this reason, considerations about the input representation\nshouldn’t be a barrier for game AI research, especially in environ-\nments where humans currently have the upper hand. We believe\nnovel results using higher level representations are important, and\nfurther research that attempts to replicate these results while using\nless favorable or more general representations are also important\nand will likely naturally follow the initial results.\nSimilarly, the representation of the action space can take many\nforms, such as high-level representations like used in OpenAI’s\nmethodology [29] where an action is viewed as a tuple (e.g. [ability,\ntarget, offset]), a simulation of user interface commands such as\nscreen movements and unit highlighting in Starcraft [53] and the\ndirect simulation of aa virtual controller as in ALE [2]. The extreme\nposition would be to insist on a robotic arm manipulating a physical\ncontroller or keyboard, which would again distract researchers from\nother legitimate game AI problems that can be tackled with higher\nlevel representations.\nExcessively fast reaction speed is often cited as one of the factors\nthat make an agent play in a perceived artificial fashion [20]. A\npopular solution, used by OpenAI [29] is to directly enforce a spe-\ncific reaction time. Alternate solutions involve the \"Sticky Action\"\nand other methods discussed in [22] for the ALE environment. In-\nterestingly, the original motivation of Sticky Actions in that paper\nis not to emulate human play, but to provide enough randomness\nto the otherwise deterministic ALE environment. This forces the\nagent to learn \"closed loop policies\" that react to a perceived game\nstate, rather than potential \"open loop policies\" that merely memo-\nrize effective action sequences. Regardless, this also works to avoid\ninhuman reaction speeds.\n5\nCONCLUSION\nWe have briefly recapitulated some of the most important game\nAI benchmark results in the past three decades, for both tabletop\ngames (Backgammon, Chess and Go) and electronic games (spe-\ncially Starcraft and Dota 2) and looked at some of the claims made\nby the authors of these game-playing systems and third-party com-\nments made by general media and research communities. From\nthose, we conclude that there is a tendency to extrapolate from AI\nachievements in game Benchmarks to speculation about Artificial\nGeneral Intelligence (AGI) scenarios where AI will eventually beat\nhumans in all or most tasks. We have also seen examples of public\nconcerns about the fairness of these benchmarks.\nWe have proposed a taxonomy of dimensions on which to evalu-\nate fairness in a competition between two game-playing systems\n(such as a human and an AI agent). We provided examples of how\nthese apply to tabletop games and electronic games, noting that\nthere is greater focus on the Input and Output dimensions of fair-\nness for electronic games.\nUltimately, we argue that there are so many possible games, and\nso many possible architectures of game-playing agents, differing\nso widely in the dimensions of fairness, that it is impossible to\ninfer human-level intelligence from success in any single game, and\nthat a completely fair competition can only be achieved against an\nartificial system that is essentially equivalent to a flesh and blood\nhuman.\nThis conclusion should not serve to infer that, if complete fair-\nness is impossible, accomplishments in game-playing AI are mean-\ningless in general, or that benchmarks that feature many differences\nbetween humans and AI along our fairness dimensions have no\nvalue. We observe that, usually, when a significant benchmark is\nreached, research often follows in order to make the system less\nreliant on human expertise, more sample-efficient, less reliant on\nextremely fast reaction speeds and more generalizable to similar\nproblems, leading to systems with fewer restrictions and wider\napplications.\nIt is also important to highlight that research in AI for games\ndoes not happen in a vacuum, and is often used as a stepping stone\nto solve complex real-world problems. While the ability of an agent\nto make a large number of actions per minute or to access a vast\namount of information can seem unfair in a game context, this\ncould be extremely desirable when applied to real-world problems\nthat require a high frequency of actions and with access to more in-\nformation than could conceivably be processed by humans. Finally,\nwe acknowledge that discussions of fairness can have complex\npolitical implications, especially when it comes to systems with\nso-called “super-human” abilities being employed in the real world.\nWhile these considerations are out of the scope of this paper, we\nbelieve this is an important discussion to be had in the research\ncommunity.\n6\nACKNOWLEDGEMENTS\nRC gratefully acknowledges the financial support from Honda Re-\nsearch Institute Europe (HRI-EU). CS is funded by the EU Horizon\n2020 programme \/ Marie Sklodowska- Curie grant 705643.\nREFERENCES\n[1] Thomas Anthony, Zheng Tian, and David Barber. 2017. Thinking fast and slow\nwith deep learning and tree search. In Advances in Neural Information Processing\nSystems. 5360–5370.\n[2] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. The\narcade learning environment: An evaluation platform for general agents. Journal\nFDG ’19, August 26–30, 2019, San Luis Obispo, CA, USA\nCanaan et al.\nof Artificial Intelligence Research 47 (2013), 253–279.\n[3] Rodney Brooks. 2017. The Seven Deadly Sins of Predicting the Future of AI.\nRetrieved October 23, 2018 from https:\/\/rodneybrooks.com\/the-seven-deadly-\nsins-of-predicting-the-future-of-ai\/\n[4] Sarah F Brosnan and Frans BM De Waal. 2003. Monkeys reject unequal pay.\nNature 425, 6955 (2003), 297.\n[5] Dustin Browder. 2010. StarCraft II: Wings of Liberty.\n[6] Michael Buro. 2003. Real-time strategy games: A new AI research challenge. In\nIJCAI, Vol. 2003. 1534–1535.\n[7] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. 2002. Deep blue.\nArtificial intelligence 134, 1-2 (2002), 57–83.\n[8] Andy Clark and David Chalmers. 1998. The extended mind. analysis 58, 1 (1998),\n7–19.\n[9] Mike Cook. 2018. OpenAI Dota 2: Game Is Hard. Retrieved October 23, 2018\nfrom http:\/\/www.gamesbyangelina.org\/2018\/08\/openai-dota-2-game-is-hard\/\n[10] DeepMind. 2019. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II.\nRetrieved April 23, 2019 from https:\/\/deepmind.com\/blog\/alphastar-mastering-\nreal-time-strategy-game-starcraft-ii\/\n[11] Nathan Ensmenger. 2012. Is chess the drosophila of artificial intelligence? A\nsocial history of an algorithm. Social Studies of Science 42, 1 (2012), 5–30.\n[12] Jonathan St BT Evans. 1984. Heuristic and analytic processes in reasoning. British\nJournal of Psychology 75, 4 (1984), 451–468.\n[13] Michael Genesereth, Nathaniel Love, and Barney Pell. 2005. General game playing:\nOverview of the AAAI competition. AI magazine 26, 2 (2005), 62.\n[14] Philip Hingston. 2009. A turing test for computer game bots. IEEE Transactions\non Computational Intelligence and AI in Games 1, 3 (2009), 169–186.\n[15] Haomiao Huang. 2019.\nAlphaStar’s Strategies Might Be Bad for Star-\ncraft 2 But They’re Great for AI.\nRetrieved May 03, 2019\nfrom https:\/\/medium.com\/datadriveninvestor\/alphastars-strategies-might-be-\nbad-for-starcraft-2-but-they-re-great-for-ai-c0a879564da22\n[16] IceFrog. 2013. Dota 2.\n[17] Daniel Kahneman, Jack L Knetsch, and Richard H Thaler. 1986. Fairness and the\nassumptions of economics. Journal of business (1986), S285–S300.\n[18] Sergey Karakovskiy and Julian Togelius. 2012. The mario ai benchmark and\ncompetitions. IEEE Transactions on Computational Intelligence and AI in Games 4,\n1 (2012), 55–67.\n[19] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech\nJaśkowski. 2016. Vizdoom: A doom-based ai research platform for visual rein-\nforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE\nConference on. IEEE, 1–8.\n[20] Ahmed Khalifa, Aaron Isaksen, Julian Togelius, and Andy Nealen. 2016. Modify-\ning MCTS for Human-Like General Video Game Playing.. In IJCAI. 2514–2520.\n[21] Team Liquid. 2018.\nOpenAI’s Dota 2 bots vs. 5 top professionals in TI.\nRetrieved October 23, 2018 from https:\/\/www.liquiddota.com\/forum\/dota-2-\ngeneral\/534977-openais-dota-2-bots-vs-5-top-professionals-in-ti\n[22] Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew\nHausknecht, and Michael Bowling. 2017. Revisiting the arcade learning en-\nvironment: Evaluation protocols and open problems for general agents. arXiv\npreprint arXiv:1709.06009 (2017).\n[23] Gary Marcus. 2018. Innateness, AlphaZero, and Artificial Intelligence. arXiv\npreprint arXiv:1801.05667 (2018).\n[24] Chris Metzen and Rob Pardo. 1998. StarCraft: Brood War.\n[25] Motherboard. 2018. OpenAI Is Beating Humans at ’Dota 2’ Because It’s Basically\nCheating. Retrieved October 23, 2018 from https:\/\/motherboard.vice.com\/enus\/\narticle\/gy3nvq\/ai-beat-humans-at-dota-2\n[26] Santiago Ontanón, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David\nChurchill, and Mike Preuss. 2013. A survey of real-time strategy game AI research\nand competition in StarCraft. IEEE Transactions on Computational Intelligence\nand AI in games 5, 4 (2013), 293–311.\n[27] OpenAI. 2017. Dota2. Retrieved October 23, 2018 from https:\/\/blog.openai.com\/\ndota-2\/\n[28] OpenAI. 2018. The International 2018: Results. Retrieved October 23, 2018 from\nhttps:\/\/blog.openai.com\/the-international-2018-results\/\n[29] OpenAI. 2018.\nOpenAI Five.\nRetrieved October 23, 2018 from https:\n\/\/blog.openai.com\/openai-five\/\n[30] OpenAI. 2018. OpenAI Five Benchmark: Results. Retrieved October 23, 2018\nfrom https:\/\/blog.openai.com\/openai-five-benchmark-results\/\n[31] OpenAI. 2019. How to Train Your OpenAI Five. Retrieved May 03, 2019 from\nhttps:\/\/openai.com\/blog\/how-to-train-your-openai-five\/\n[32] OpenAI. 2019. OpenAI Five Arena.\nRetrieved May 03, 2019 from https:\/\/\narena.openai.com\/#\/\n[33] Diego Perez-Liebana, Spyridon Samothrakis, Julian Togelius, Simon M Lucas,\nand Tom Schaul. 2016. General video game ai: Competition, challenges and\nopportunities. In Thirtieth AAAI Conference on Artificial Intelligence. 4335–4337.\n[34] Washington Post. 2016. What AlphaGo’s sly move says about machine creativity.\nRetrieved October 23, 2018 from https:\/\/www.washingtonpost.com\/news\/\ninnovations\/wp\/2016\/03\/15\/what-alphagos-sly-move-says-about-machine-\ncreativity\/?utmterm=.543bc9ade906\n[35] John Rawls. 2001. Justice as fairness: A restatement. Harvard University Press.\n[36] Reddit. 2018. Team Human vs. OpenAI Five Match Discussions.\nRetrieved\nOctober 23, 2018 from https:\/\/www.reddit.com\/r\/DotA2\/comments\/94udao\/\nteamhumanvsopenaifivematchdiscussions\/\n[37] MIT Technology Review. 2017. Humans Are Still Better Than AI at StarCraftâĂŤ-\nfor Now. Retrieved October 23, 2018 from https:\/\/www.technologyreview.com\/\ns\/609242\/humans-are-still-better-than-ai-at-starcraftfor-now\/\n[38] Christoph Salge, Michael Cerny Green, Rodgrigo Canaan, and Julian Togelius.\n2018. Generative design in minecraft (GDMC): settlement generation competition.\nIn Proceedings of the 13th International Conference on the Foundations of Digital\nGames. ACM, 49.\n[39] Tom Schaul, Julian Togelius, and Jürgen Schmidhuber. 2011. Measuring intelli-\ngence through games. arXiv preprint arXiv:1109.1314 (2011).\n[40] Oscar Scwartz. 2018. ’The discourse is unhinged’: how the media gets AI alarm-\ningly wrong. Retrieved October 23, 2018 from https:\/\/www.theguardian.com\/\ntechnology\/2018\/jul\/25\/ai-artificial-intelligence-social-media-bots-wrong\n[41] John Searle. 1999. The Chinese Room. (1999).\n[42] Noor Shaker, Julian Togelius, Georgios N Yannakakis, Ben Weber, Tomoyuki\nShimizu, Tomonori Hashiyama, Nathan Sorenson, Philippe Pasquier, Peter\nMawhorter, Glen Takahashi, et al. 2011. The 2010 Mario AI championship:\nLevel generation track. IEEE Transactions on Computational Intelligence and AI\nin Games 3, 4 (2011), 332–347.\n[43] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural\nnetworks and tree search. nature 529, 7587 (2016), 484.\n[44] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\net al. 2017. Mastering chess and shogi by self-play with a general reinforcement\nlearning algorithm. arXiv preprint arXiv:1712.01815 (2017).\n[45] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\nHuang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,\net al. 2017. Mastering the game of Go without human knowledge. Nature 550,\n7676 (2017), 354.\n[46] Peter Stone, Michael Quinlan, and Todd Hester. 2010. The Essence of Soccer, Can\nRobots Play Too? In Soccer and Philosophy: Beautiful Thoughts on the Beautiful\nGame, Ted Richards (Ed.). Popular Culture and Philosophy, Vol. 51. Open Court\nPublishing Company, 75–88.\n[47] Hans Strasburger, Ingo Rentschler, and Martin Jüttner. 2011. Peripheral vision\nand pattern recognition: A review. Journal of vision 11, 5 (2011), 13–13.\n[48] Gerald Tesauro. 1995. Temporal difference learning and TD-Gammon. Commun.\nACM 38, 3 (1995), 58–68.\n[49] The New York Times. 1997. Deep, Deeper, Deepest Blue. Retrieved October 23,\n2018 from https:\/\/www.nytimes.com\/1997\/05\/18\/weekinreview\/deep-deeper-\ndeepest-blue.html\n[50] The New York Times. 1997. Swift and Slashing, Computer Topples Kasparov. Re-\ntrieved October 23, 2018 from https:\/\/www.nytimes.com\/1997\/05\/12\/nyregion\/\nswift-and-slashing-computer-topples-kasparov.html\n[51] The New York Times. 2017. Google’s A.I. Program Rattles Chinese Go Master as\nIt Wins Match.\nRetrieved October 23, 2018 from https:\/\/www.nytimes.com\/\n2017\/05\/25\/business\/google-alphago-defeats-go-ke-jie-again.html\n[52] Vernor Vinge. 1993. Technological singularity. In VISION-21 Symposium sponsored\nby NASA Lewis Research Center and the Ohio Aerospace Institute. 30–31.\n[53] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha\nVezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou,\nJulian Schrittwieser, et al. 2017. Starcraft ii: A new challenge for reinforcement\nlearning. arXiv preprint arXiv:1708.04782 (2017).\n[54] WeeklyStandard. 1997. Be Afraid.\nRetrieved October 23, 2018 from https:\n\/\/www.weeklystandard.com\/charles-krauthammer\/be-afraid-9802\n[55] Wired. 2016. IN TWO MOVES, ALPHAGO AND LEE SEDOL REDEFINED THE\nFUTURE. Retrieved October 23, 2018 from https:\/\/www.wired.com\/2016\/03\/\ntwo-moves-alphago-lee-sedol-redefined-future\/\n[56] WorldAIShow. 2018. Why is Elon Musk afraid of AlphaGo-Zero?\nRetrieved\nOctober 23, 2018 from https:\/\/singapore.worldaishow.com\/elon-musk-afraid-\nalphago-zero-ai\/\n[57] Georgios N. Yannakakis and Julian Togelius. 2018. Artificial Intelligence and\nGames. Springer. http:\/\/gameaibook.org.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks.pdf"}
{"title":"Marathon Environments: Multi-Agent Continuous Control Benchmarks in a Modern Video Game Engine","authors":"Joe Booth, Jackson Booth","summary":"Recent advances in deep reinforcement learning in the paradigm of locomotion\nusing continuous control have raised the interest of game makers for the\npotential of digital actors using active ragdoll. Currently, the available\noptions to develop these ideas are either researchers' limited codebase or\nproprietary closed systems. We present Marathon Environments, a suite of open\nsource, continuous control benchmarks implemented on the Unity game engine,\nusing the Unity ML- Agents Toolkit. We demonstrate through these benchmarks\nthat continuous control research is transferable to a commercial game engine.\nFurthermore, we exhibit the robustness of these environments by reproducing\nadvanced continuous control research, such as learning to walk, run and\nbackflip from motion capture data; learning to navigate complex terrains; and\nby implementing a video game input control system. We show further robustness\nby training with alternative algorithms found in OpenAI.Baselines. Finally, we\nshare strategies for significantly reducing the training time.","url":"http:\/\/arxiv.org\/abs\/1902.09097v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1902.09097v1","published":1551074195000,"comment":"AAAI-2019 Workshop on Games and Simulations for Artificial\n  Intelligence","pdf_text":" \n \nMarathon Environments: Multi-Agent Continuous Control Benchmarks  \nin a Modern Video Game Engine \n     Joe Booth                        Jackson Booth \n              Vidya Gamer, LLC                   Summit Sierra High School, Seattle \n          joe@joebooth.com                            jbooth.si@mysummitps.org \n \n \n \nAbstract \nRecent advances in deep reinforcement learning in the para-\ndigm of locomotion using continuous control have raised the \ninterest of game makers for the potential of digital actors us-\ning active ragdoll. Currently, the available options to develop \nthese ideas are either researchers’ limited codebase or propri-\netary closed systems. We present Marathon Environments, a \nsuite of open source, continuous control benchmarks imple-\nmented on the Unity game engine, using the Unity ML-\nAgents Toolkit. We demonstrate through these benchmarks \nthat continuous control research is transferable to a commer-\ncial game engine. Furthermore, we exhibit the robustness of \nthese environments by reproducing advanced continuous \ncontrol research, such as learning to walk, run and backflip \nfrom motion capture data; learning to navigate complex ter-\nrains; and by implementing a video game input control sys-\ntem. We show further robustness by training with alternative \nalgorithms found in OpenAI.Baselines. Finally, we share \nstrategies for significantly reducing the training time. \n1. Introduction   \nVideo games make substantial use of real time animation. \nBecause video games are user driven, gameplay program-\nmers must work closely with motion capture directors and \nanimators to deal with ‘edge cases’ between the player and \nnon-player characters and\/or the environment. This is an ex-\npensive, painful, and never-ending process because of the \ncomplexity of the problem space. Furthermore, animation is \nmotion captured, which requires specialist post-processing, \nwhich is tedious and time-consuming.  \nRecent advances in reinforcement learning have provided \nalgorithms that can learn character animation from scratch \n(Brockman et al., 2016; Todorov, Erez, & Tassa, 2012) and \nwhich adapt to a wide variety of environmental challenges \n(Mnih et al., 2015). Further research has shown that agents \ncan learn from motion capture, DeepMimic (Peng et al., \n                                                \nAAAI-2019 Workshop on Games and Simulations for Artificial Intelli-\ngence. \n2018) and from videos, SFV: Reinforcement Learning of \nPhysical Skills from Video (Peng et al., 2018). \nIf this research can be shown to be transferable into pro-\nfessional video game engines, then this development would \nhave the potential to reduce costs and increase democratiza-\ntion, while improving realism and the range of user interac-\ntion. However, commercial video game engines such as \nUnity3D1 and Unreal2 have to take into account many con-\nstraints, such as the need to work on many platforms (mobile \ndevices, web, pc\/mac, Xbox, PlayStation, and Nintendo), \nsupport networking, perform high-end rendering, and main-\ntain an accessible workflow. The physics engines in game \nengines are optimized for stability and performance at the \nexpense of realism (Erez, Tassa, & Todorov, 2015).  \nIn this paper, we present the core suite of environments \nand their integration into Unity ML-Agents Toolkit (Juliani \net al., 2018). Next, we outline our experiments to reproduce \ntraining from motion capture data, to implement a video \ngame controller, and to navigate complex environments. Fi-\nnally, we describe our integration and benchmarks with \nOpenAI.Baselines (Dhariwal, et al., 2017) and share our op-\ntimization strategies.  \n2. Related work \nThe MuJoCo physics engine (Todorov et al., 2012) is a pop-\nular choice for researchers for building continuous control \nbenchmarks and is used by OpenAI.Gym (Brockman et al., \n2016) and the DeepMind Control Suite (Tassa et al., 2018). \nWhile MuJoCo is a powerful physics engine, its rendering \nengine is limited, it is not open source, and, to the best of \nour knowledge, it has not been implemented in a commer-\ncial video game.  \n The MuJoCo Unity Plugin (Todorov, 2018) enables a \nMuJoCo physics simulation to be rendered using the Unity \nGame Engine. It has a number of limitations: the physics \n1www.unity3d.com \n2www.unrealengine.com \nsimulation needs to run as a separate PC program, it has lim-\nited integration with Unity’s core object system, and it is \nproprietary.   \n Unity ML-Agents Toolkit (Juliani et al., 2018) is an open \nsource framework for machine learning using the Unity plat-\nform. It includes some continuous control samples such as a \nversion of ant and humanoid. \n Analog RL Locomotion (Booth, 2018) demonstrated that \nsome continuous control environments could be imple-\nmented using a commercial game engine; however, it is not \nopen source.  \n DeepMotion3 is a proprietary physics platform aimed at \ngame developers wanting to use active ragdolls trained with \nreinforcement learning. It is the most advanced proposal in \nthis space and supports the leading game engines (Unity & \nUnreal). However, it is not open source. \n Roboschool4 is a set of open source continuous control \nbenchmarks for OpenAI.Gym (Brockman et al., 2016). Im-\nplemented using the open source Bullet physics engine, Ro-\nboschool removes the requirement for a Mujuco (Todorov \net al., 2012) license. However, it has limited rendering ca-\npability and is limited in terms of appeal for game develop-\ners. \n3. Marathon Environments \nMarathon Environments5 consists of four continuous control \nenvironments. Each environment parses an xml script con-\ntaining the agent model definition; this is converted into a \n                                                \n3 https:\/\/www.deepmotion.com \n4 https:\/\/github.com\/openai\/roboschool \n5 https:\/\/github.com\/Unity-Technologies\/marathon-envs \nUnity object. The xml scripts are slightly modified versions \nof the xml scripts from the DeepMind Control Suite (Tassa \net al., 2018) for Hopper, Walker, Humanoid, and \nOpenAI.Roboschool for Ant.  \n All environments are Multi-Agent environments by \nwhich we mean they contain multiple instances of a single \nagent. In each training step the environment collects an ob-\nservation for each agent instance. For clarity, there is no \ncompetition or communication between the agent instances. \nUnless specified, all environments contain 16 agents. \n3.1 Integration with Unity ML-Agents Toolkit \nMarathon Environments was released alongside Unity ML-\nAgents Toolkit v0.56. All environments have a single ML-\nAgent brain, with continuous observations and continuous \nactions. There are no visual observations. Each action cor-\nrelates to one motorized joint axis. Multi-axis joints are im-\nplemented using a nested joint strategy.  \nReward, termination, and observation functions are influ-\nenced by DeepMind Control Suite (Tassa et al., 2018) and \nOpenAI.Roboschool.  The one exception was for Humanoid \nas we implemented a phase function in the reward to en-\nhance training speed. \nEach environment is a separate Unity scene with a prefab \nfor each agent type and contains one or more pre-trained \nTensorflow models. A custom agent class, which inherits \nfrom MarathonAgent.cs, is used to define the behavior of \nthat agent. See Supplementary Materials A for further de-\ntails.\n6 \nhttps:\/\/blogs.unity3d.com\/2018\/09\/11\/ml-agents-toolkit-v0-5-new-\nresources-for-ai-researchers-available-now\/ \nFigure 1: The four Marathon Environments. \nTop left: Hopper Environment. Top right: Walker Environment. Bottom left: Humanoid Environment. Bottom right: Ant Environment. \nThe Hopper agent has 4 joints\/actions and 31 observations. \nThe reward function has a positive reward signal for the pel-\nvic velocity and pelvis uprightness. Similarly, there is a neg-\native reward signal for the effort of the current action state \nand a penalty if the height is below 1.1m. The termination \nfunction triggers if a non-foot labeled body part collides \nwith the terrain, if the height falls below <.3m, or if the head \ntilts more than 0.4 units. \n \nThe Humanoid agent has 21 joints\/actions and 88 observa-\ntions. The reward function has a positive reward signal for \nthe pelvic velocity and pelvic uprightness, a negative reward \nsignal for the effort of the current action state, and a penalty \nif the height is below 1.2m. It also adds additional reward \nbased on the phase cycle of its legs. The termination func-\ntion triggers if a non-foot labeled body part collides with the \nterrain. \nThe Walker agent has 6 joints\/actions and 41 observations. \nThe reward function has a positive reward signal for the pel-\nvic velocity and pelvis uprightness. There is a negative re-\nward signal for the effort of the current action state and a \npenalty if the height is below 1.1m. The termination func-\ntion triggers if a non-foot body part collides with the terrain. \n \nThe Ant agent has 8 joints\/actions and 53 observations. The \nreward function has a positive reward signal for the pelvic \nvelocity, a negative reward signal for the effort of the current \naction state, and a negative signal if the joints are at their \nlimit. The termination function triggers if the body of the ant \nrotates more than 0.2 units from upright. \n \n \n \n \n \n \n \nTable 1: Training speeds. Compares wall clock training time, and agent training steps per second across environment.\nTraining Performance \nWe compared training performance between each environ-\nment, training 16 concurrent agents (see Table 1). We found \nthat the steps per second decreased with the increased com-\nplexity of the agent. \nEnvironment \nTraining Steps \nTraining \nTime \nSteps Per \nSecond \nObserva-\ntions \nActions \nHopper \n4,800,000 \n36m 31s \n2,190 \n31 \n4 \nWalker \n4,800,000 \n37m 14s \n2,149 \n43 \n6 \nHumanoid \n16,000,000 \n3h 37m \n1,229 \n92 \n21 \nAnt \n4,800,000 \n35m 55s  \n2,359 \n54 \n8 \nFigure 2: Additional Experiments. Top left: Active Ragdoll Style Transfer ~ Learning Backflip from motion capture data. \nTop right: Active Ragdoll Assault Course. Bottom left: Active Ragdoll Controllers. \n4. Additional Experiments \nAs well as reproducing Continuous Control Benchmarks, a \nnumber of additional experiments have been produced that \nuse Marathon Environments as a base. The goal of these ad-\nditional experiments is for the resulting improvements from \nthese experiments to be included in future versions of Mar-\nathon Environments.  \n4.1 Active Ragdoll Style Transfer: Learning Locomo-\ntion from Motion Capture Data \nThe goal of these experiments was to confirm that research \ninto training Continuous Control Humanoids from motion \ncapture data will transfer to Unity and Marathon Environ-\nments, DeepMimic (Peng, Abbeel, et al., 2018). Source \ncode, animations, and downloadable demos of these experi-\nments can be found on GitHub7. \n \nMethod: Experiments were performed with both a standard \nUnity Humanoid, modified using Unity's built-in Ragdoll \ntool, and Marathon Man, a modified version of Marathon \nEnvironments' Humanoid which was adapted to work with \nMotion Capture data and has modified joints. We used the \nstandard Unity animation functions to cycle through and \nrecord slices of animation. The reward function is based on \n                                                \n7 https:\/\/github.com\/Sohojoe\/ActiveRagdollStyleTransfer \nthe distance from the reference animation at each step. To \nimprove training speed, 64 agents were trained in parallel, \nand a modified version of the Academy script was used so \nthat actions were only applied to the model every 1 in 5 \nphysics steps. The beta version of Unity 2018.3 was also \nused, as this release of Unity includes improvements to the \nPhysX engine.  \n \nResults: While we were able to train the Unity Humanoid, \nthe results were not as high quality as those of DeepMimic \n(Peng, et al., 2018). On closer inspection, we found that the \nauto-generated ragdoll had problems with self-collision. Be-\ncause the Marathon Environments Humanoid had success-\nfully trained to walk without motion capture data, we \nswitched to using Marathon Man. This change allowed us to \ntrain against a walking animation with relative ease; how-\never, we found we needed to modify the joints to train run-\nning. Additionally, we added independent sensors for the \nheel and toes. We found that with the modified joints and \nsensors we were able to train running and improve the per-\nformance of walking.  \n Backflip proved much more challenging. In one experi-\nment, the agent completed half the backflip but without \nenough height to avoid landing on its head. We addressed \nthis through tuning the termination function to include early \ntermination when the agent was in a state greater than a pre-\ndefined distance from the reference animation. In another \nexperiment, the agent aborted the backflip and landed on its \narms and legs and then returned to a standing position. This \nfail scenario was due to the agent learning to avoid early ter-\nmination and maximize the number of steps needed to re-\nceive a reward. Despite these challenges we did achieve \ntraining backflip when we updated to the Beta version of \nUnity 2018.3, which includes improvements to the underly-\ning PhsyX physics engine.  \n Our experiments of learning locomotion from motion \ncapture data also included interesting findings for rate of \nlearning. It took 50,000,000 training steps for the agent to \nfirst complete a backflip, and we continued training to \n128,000,000 training steps to improve stability. This com-\npares to the average of 60,000,000 training steps required \nfor DeepMimic (Peng, et al., 2018) and implies that we have \nroom to improve training performance. Our wall clock train-\ning speed was 25hrs, implying that we have a 4x training \nspeed improvement over DeepMimic (Peng, et al., 2018) \nwhich required 48hrs. \nFigure 3: Training curves for Learning Backflip from Motion \nCapture Data. Two independent runs. Each training step collects \nobservations for 64 agents for a total training length of \n128,000,000 steps.   \n4.2 Active Ragdoll Controllers: Implementing Player \nControllers \nThe motivation of these experiments was to explore how a \ntypical video game player controller implementation per-\nforms when driving a continuous control agent trained \nthrough reinforcement learning. The target audience for \nthese experiments is video game designers and developers. \nSource code, animations, and downloadable demos of these \nexperiments can be found on Github8. \n \nMethod: All experiments are 2D and used the Hopper or \nWalker agents. Experiments included both continuous input \n(a float value from -1 to 1) and discrete inputs (left, right, \nno-op, jump, jump+left, jump+right). \n For the continuous control experiments, we mapped the \nUnity game engine controller input strength to the agent's \ntarget velocity whereby the human player has control over \nthe agents target direction and velocity. For training, we ran-\ndomly choses the agent's target velocity between -1 and +1. \n                                                \n8 https:\/\/github.com\/Sohojoe\/ActiveRagdollControllers \nThe target velocity was reselected on a variable time basis \nbetween 40 to 240 time steps. We also trained using fixed \ntarget velocities of -1, 0 and +1. \n For the discrete control experiments we mapped control-\nler input to agent actions of left, right and no-action (station-\nary). For training, we randomly chose the target action with \na probability of 40% choose left, 40% choose right, 20% \nchoose stationary. We also ran experiments adding the ad-\nditional actions of jump, jump + left, jump + right. During \ntraining there was a probability of 25% choose jumping ac-\ntion. \n We include the subjective summary by one of our re-\nsearchers who has 30+ years of video game development \nexperience. We chose this methodology due to the early na-\nture of this work and the prototype feel of the experiments. \nThe discrete controller experiments felt satisfying and \norganic due to the relationship between the input and \nthe movement of the agent. For example, when moving \nat top speed to the right, on holding the left input, the \nagent response is contextual depending on its current \ncontent. If a jump is in progress, the agent will re-posi-\ntion its weight to stop and reverse on contact with the \nterrain. If the agent is preparing to jump right, it will \nabort the jump and perform a left jump. These behav-\niors are very time consuming to create using traditional \ngame development techniques as they require branch \npoints within animations and controllers. The one area \nof concern is that the agent would not sit still; it always \nhas some movement, almost like a hyperactive child \nunable to sit still in their seat. I would like to see further \ndevelopment in environmental challenges, such as var-\niable height terrain and jump challenges. \nFigure 3: Subjective Summary by Experienced Game Developer. \nResults: We expected the continuous control experiments \nto give a higher sense of control due to the continuous map-\nping of the controller to the target velocity of the agent. \nHowever, it was strikingly apparent that the discrete control-\nler agent was more responsive and gave the player more \ncontrol. We tried training the continuous controller agent us-\ning only three inputs (of -1, 0, 1) however, this did not im-\nprove the results. This suggests that the lack of control is due \nto the tuning of the Unity input controller (how physical in-\nputs are mapped to the logical input value). We chose to fo-\ncus efforts on the discrete controller agent, due to the faster \nprogress and reduced complexity. \n The discrete controller agent was more responsive and \nprovided a better sense of control (see Figure 3). The dis-\ncrete controller agent learned behavior mappings not ex-\nplicit in the design (see Figure 3), which give the discrete \ncontroller agent the higher sense of control we expected \nfrom the continuous controller agent. The discrete controller \nagent also demonstrated contextual behavior, such as adapt-\ning its weight to a change input during a jump.  \n4.3 Active Ragdoll Assault Course: Learning to navi-\ngate challenging environments. \nThe goal of these experiments was to validate that adapta-\nbility to environmental challenges such as those demon-\nstrated in Emergence of Locomotion Behaviours in Rich En-\nvironments (Heess et al., 2017), would transfer to Marathon \nEnvironments. Source code, animations, and downloadable \ndemos of these experiments can be found on GitHub9. \n \nMethod: All experiments were 2D and used the Hopper or \nWalker agents. Experiments included manually modifying \nthe terrain, manually adding box objects to the environment, \nand using an adversarial network to dynamically generate \nthe terrain based on the inverse of the agent’s reward signal. \nWe experimented with and without height observations. \n \nResults: We showed that our agents were able to train to \nnavigate complex terrains, and that successful strategies in \nthe reference paper transferred to our experiments. We \nfound that the agent was able to learn to navigate basic ter-\nrains, even without height observations. We found that the \nmost optimal training was with height observations, and the \nadversarial network to dynamically generate the terrain.  \n4.4 Marathon Environment Baselines  \nThe original intention of the following experiments was to \nsee if the OpenAI.Baseline algorithms could improve train-\ning speed for our Style Transfer experiments. We expected \nto find a significant performance improvement using algo-\nrithms optimized for GPU; however, we found the inverse, \nin that every algorithm that supported multi-agent learning \nperformed better on CPU training. In addition, we found al-\nternative ways to optimize ML-Agents Toolkit. Our meth-\nods and findings may be of interested to researchers looking \nto use ML-Agents Toolkit and OpenAI.Baselines (Dhari-\nwal, et al., 2017). Source code can be found on Github.10 \n \nMethod: First, we created a new repository that contains \nsource code for ML-Agents Toolkit, Marathon Environ-\nments, OpenAI.Baselines and Stable-Baselines. To make it \neasier for others to use this repository, we created pre-built \nMacOS and Windows environments for Hopper and Walker \nfor solo agent training, 16 agent training, and solo inference \nmode, all of which can be downloaded from the release page \nof Github. \nIn addition, we created Python scripts for invoking solo \nand multi-agent ML-Agents Toolkit environments from the \n                                                \n9 https:\/\/github.com\/Sohojoe\/ActiveRagdollAssaultCourse \n10 https:\/\/github.com\/Sohojoe\/MarathonEnvsBaselines \ncommand line, along with the required modifications to sup-\nport the execution of these scripts. This includes \nunity_vec_env.py, which handles mapping the ML-Agents \nToolkits' multi-agent implementation to the requirements \nexpected from OpenAI.Baselines (Dhariwal, et al., 2017). \nOpenAI.Baselines (Dhariwal, et al., 2017) uses two dif-\nferent strategies for concurrent agent training. The first strat-\negy is concurrently running multi-environments in which \nMPI is used to handle communication between multiple en-\nvironments with single agents. To invoke multi-environ-\nments, either wrap the command line, Python invoke, with \nmpiexec python ...., or use the --num_env=XX \ncommand line option. The second strategy is a multi-agent \nenvironment, wherein a single environment contains multi-\nple agents which train concurrently.  \nAll our experiments used Marathon Environment Hopper \nover 1m training steps. All multi-agent experiments were \nperformed using the same Unity ML-Agent environment. \nWe used the MuJoCo hyper-parameters; the only modifica-\ntion made was to divide the default nsteps by the number \nof concurrent agents, which in this experiment was 16. \n For collecting the ML-Agent.PPO data, we modified the \nMarathon Environments hyper-parameters to limit to 1m \ntraining steps and to match Baseline’s hyper-parameters as \nclosely as possible (see supplementary materials). \n Unless specified, all experiments were performed on a 6 \ncore i7 8700k @ 3.70 GHz, with a NVIDIA GTX 1080 \nGPU, using tensorflow-gpu v1.7.1, optimized for AVX2 in-\nstructions11. \n For comparisons between MPI multi-environment and \nmulti-agent environments, we used four concurrent MPI en-\nvironments and the 16-agent environment. We tested these \non both the PC architecture, as shown above, and a Mac-\nBook Pro 2016.  \nLimitations: We experienced the following limitations \nwhen using OpenAI.Baselines with Unity ML-Agents \nToolkit: \n1. \nSave\/Load is broken when using normalized training, \nmeaning it is not possible to load trained models. \n2. \nMPI is broken on Windows. We were able to train stand-\nard OpenAI.Gym environments with MPI, but found er-\nrors when training Unity ML-Agents Toolkit environ-\nments.  \n3. \nMulti-agent training is only supported by a subset of \nOpenAI.Baseline algorithms (PPO2 CPU & GPU, A2C \nCPU & GPU, ACKTR CPU only). \n4. \nA2C and ACKTR did not report score as the Base-\nlines.Monitor wrapper does not support vectorized envi-\nronments. \nPreliminary Results: \nMPI Multi-Environment vs Multi-Agent: On PC we \nobserved a 7.5 x performance improvement in wall clock \n11https:\/\/github.com\/fo40225\/tensorflow-windows-\nwheel\/tree\/master\/1.7.1\/py36\/GPU\/cuda92cudnn71avx2 \ntime (87% per agent), while on MacOS we observed a 6x \nspeed improvement (50% per agent). While we were limited \nin our capacity to test with MPI, our results indicate that \nMulti-Agent has the most potential for improving training \ntime, especially when considering the findings reported in \nthe performance section below. \nCPU vs GPU training speed: We found training with \nCPU significantly faster than training with GPU (ranging \nfrom 30% to 3x improvement as shown in Table 2). This \nwas unexpected; we had hoped that OpenAI.Baseline would \noutperform CPU training with a GPU. We don’t believe that \nthis is caused by the Unity game engine competing for GPU \nresources, as we re-ran experiments with rendering disabled, \nPhysX forced to CPU, and found this had little impact on \nthe training time. By measuring the wall clock time the py-\nthon algorithm spends between collecting samples and batch \ntraining, we found that the increase was during the batch \ntraining phase. The collecting samples phase took an in-\ncrease of 60% (700ms for GPU, 430ms for CPU), whereas \nthe batch training phase took an increase of 500% (2,030ms \nfor GPU, 330ms for CPU). More research is needed to con-\nfirm whether the root cause is due to an inefficiency in Base-\nlines implementation of vectorized environments, or a quirk \nrelated to our specific GPU, the version of Tensorflow we \nused, or an edge case specific to environment size and hy-\nperparameters.  \n \nAlgorithm \nCPU Train-\ning Speed \nGPU Train-\ning Speed \nGPU Delta \nOpenAI.Baselines.A2C \n4m 51s (291s) \n7m 38s (458s) \n158% of CPU \nOpenAI.Baselines.PPO2 \n7m 55s (475s) \n23m 43s (1423s) \n300% of CPU \nUnity ML-Agents.PPO \n6m 58s (418s) \n9m 23 (563s) \n135% of CPU \nTable 2: CPU training speed outperforms GPU. The algorithm \ncolumn states the framework and algorithm. The CPU and GPU \ntraining speed columns state the training time with and without a \nGPU. The GPU Delta column states the percentage of time the \nGPU took to train vs training with the CPU. \nML-Agents Toolkit vs OpenAI.Baselines: Our prelimi-\nnary findings demonstrated that OpanAI.Baselines algo-\nrithms have the potential to outperform Unity ML-Agents \nToolkit. Baselines.PPO2 scored 50% higher, while training \n14% slower. A2C & ACKTR trained 33% faster. \n \nAlgorithm \nCPU \nTraining \nSpeed \nScore \nSpeed \nvs ML-\nAgents \nScore \nvs ML-\nAgents \nUnity ML-Agents.PPO \n6m 58s (418s) \n455 \n- \n- \nOpenAI.Baselines.A2C \n4m 51s (291s) \nn\/a \n65%  \nn\/a \nOpenAI.Baselines.ACKTR \n4m 41s (281s) \nn\/a \n67% \nn\/a \nOpenAI.Baselines.PPO2 \n7m 55s (475s) \n700 \n114% \n154% \nTable 3: ML-Agents Toolkit vs OpenAI.Baselines training perfor-\nmance. The algorithm column states the framework and algo-\nrithm. The score column states the final scores. A2C and ACKTR \ndid not report scores due to limitations of their implementation. \nThe Speed vs ML-Agents column details the relative training time \nas a percentage of ML-Agents Toolkit’s training time. The Score \nvs ML-Agents column details the relative training score as a per-\ncentage of ML-Agents Toolkit’s score. \n4.5 Optimization Strategies \nThe following optimizations strategies were implemented in \nour Style Transfer experiments. Our goal was to explore \nstrategies to reduce training wall clock time. \nIncreasing the number of concurrent Agents: Stooke \n& Abbeel, 2018, demonstrated that the PPO algorithm has \nthe potential to run up to 512 concurrent environments with \nsmall variations in the results on discrete Atari environ-\nments. By default, Marathon Environments uses 16 concur-\nrent agents. We found that we were able to scale to 64 con-\ncurrent agents and improve wall clock training time by 45%. \nThe only modification made was to scale max_steps. \nUnity ML-Agents Toolkits’ total training steps is the prod-\nuct of the number of concurrent agents and the max_steps \nbrain setting in the yaml config file. \n Headless Mode: Unity ML-Agents Toolkit supports \nheadless mode, sometimes called Server Mode, by specify-\ning the --no-graphics command line option.  \n Modified Academy to Support Physics Only Steps: \nMarathon Agents increases the number of physics steps per \nsecond to 200-500, depending on environment. One reduces \nthe frequency of learning steps by setting the Decision-\nFrequency parameter in the ML-Agent agent’s settings; \nhowever, the action step is triggered with every physics step, \nimpacting performance. By modifying the Academy script \nand adding a PhysicsOnlySteps, we reduced the ac-\ntion frequency to that of the training frequency. The com-\nbined impact of this optimization and the --no-\ngraphics optimization improved wall clock training time \nby 50%. \n Optimized Tensorflow for CPU: As discussed in the \nOpenAI.Baselines sections, installing an optimized version \nof Tenesorflow and running the algorithms in CPU mode \nsignificantly improved training wall clock time.  \n5. Challenges \n5.1 PhysX.  \nWe faced three major challenges with the PhysX physics en-\ngine: \n Bugs and Compromises made by the PhysX engine, es-\npecially with regard to joints, was our primary challenge. \nFor example, the Configurable Joint object combines set-\ntings between the Y and Z axis, limiting the ability of the \njoint to map to all requirements. A second problem is the \ngeneral stability of joints and the need to experiment with \ndifferent joint strategies and settings. The general advice of \nthe community is to use Configurable Joints or nested joints \ninstead of Character Joints.  \n Poor Documentation of the PhysX physics engine was \nthe second challenge we faced. PhysX is developed by \nNVIDIA and implemented into Unity by Unity engineers. \nThere is rudimentary documentation in the Unity manual \nand SDK documentation. However, for more technical mat-\nters, one is left to search through support forums or explore \nthe native PhysX documentation. To make matters worse, \nbecause PhysX has been around for a number of years, many \nposts and documentation sources are out of date.  \n Our Lack of Experience with PhysX and physics en-\ngines was our third area of challenge. Working with Quater-\nnions is not natural, having to convert between left-handed \nand right-handed coordinate systems is confusing, and the \nmultiple coordinate systems within Unity cause extra com-\nplications (world, object, joint, local, global). \n5.2 Overfitting the Reward Function. \nA second area of challenge is the tendency of reinforcement \nlearning algorithms to overfit. We found that development \nwas slowed down because bugs would not always be appar-\nent as the agent learned to compensate and solve a buggy \nimplementation of the environment. This would not become \napparent until scaling to a more complex environment where \nthe agent would fail to learn leaving us unsure whether the \nproblem was with the newer environment or with something \nlower in the stack.  \nThese overfitting challenges were compounded by the \nPhysX problems. \n6. Conclusion \nIn this paper, we presented Marathon Environments, a suite \nof open source continuous control benchmarks implemented \nin a modern commercial video game engine. We demon-\nstrated the robustness of these environments and the ability \nto transfer cutting-edge reinforcement learning research into \na video game setting. We discussed various strategies to re-\nduce training time, and we documented how these environ-\nments can be used with OpenAI.Baselines (Dhariwal, et al., \n2017). Collectively, these experiments show that reinforce-\nment learning research is transferable to a video game en-\ngine, and that future investment towards digital actors using \nactive ragdoll is justified. \nAcknowledgements \nWe would like to thank Arthur Juliani, Jeffrey Shih, and \nLeon Chen for their contributions to and testing of the inte-\ngration into ML-Agents Toolkit, and Dr. Nicole Swedberg \nand Anna Booth for their contributions to, and feedback in \nwriting this paper. \nReferences \nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schul-\nman, J., Tang, J., & Zaremba, W. (2016). OpenAI Gym. \nArXiv:1606.01540 [Cs]. http:\/\/arxiv.org\/abs\/1606.01540 \nBooth, J. (2018). Analog RL Locomotion- Beyond Bots: Making \nMachine Learning Accessible and Useful. Game Developers \nConference, \n2018. \nhttps:\/\/www.gdcvault.com\/play\/1025519\/Beyond-Bots-Making-\nMachine-Learning \nDhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and \nNichol, Alex and Plappert, Matthias and Radford, Alec and \nSchulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, \nPeter. (2017). OpenAI Baselines: high-quality implementations of \nreinforcement \nlearning \nalgorithms. \nhttps:\/\/github.com\/openai\/baselines (Original work published \n2017) \nErez, T., Tassa, Y., & Todorov, E. (2015). Simulation tools for \nmodel-based robotics: Comparison of Bullet, Havok, MuJoCo, \nODE and PhysX. In 2015 IEEE International Conference on Ro-\nbotics \nand \nAutomation \n(ICRA) \n(pp. \n4397–4404). \nhttps:\/\/doi.org\/10.1109\/ICRA.2015.7139807 \nHeess, N., TB, D., Sriram, S., Lemmon, J., Merel, J., Wayne, G., \net el. (2017). Emergence of Locomotion Behaviours in Rich Envi-\nronments. \nArXiv:1707.02286 \n[Cs]. \nhttp:\/\/arxiv.org\/abs\/1707.02286 \nJuliani, A., Berges, V.-P., Vckay, E., Gao, Y., Henry, H., Mattar, \nM., & Lange, D. (2018). Unity: A General Platform for Intelligent \nAgents. \nArXiv:1809.02627 \n[Cs, \nStat]. \nhttp:\/\/arxiv.org\/abs\/1809.02627 \nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., \nBellemare, M. G., et el. (2015). Human-level control through deep \nreinforcement \nlearning. \nNature, \n518, \n529–533. \nhttps:\/\/doi.org\/10.1038\/nature14236 \nPeng, X. B., Abbeel, P., Levine, S., & van de Panne, M. (2018). \nDeepMimic: Example-Guided Deep Reinforcement Learning of \nPhysics-Based Character Skills. ACM Transactions on Graphics, \n37(4), 1–14. https:\/\/doi.org\/10.1145\/3197517.3201311 \nPeng, X. B., Kanazawa, A., Malik, J., Abbeel, P., & Levine, S. \n(2018). SFV: Reinforcement Learning of Physical Skills from Vid-\neos. ArXiv:1810.03599 [Cs]. http:\/\/arxiv.org\/abs\/1810.03599 \nStooke, A., & Abbeel, P. (2018). Accelerated Methods for Deep \nReinforcement \nLearning. \nArXiv:1803.02811 \n[Cs]. \nhttp:\/\/arxiv.org\/abs\/1803.02811 \nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. de L., \net \nel. \n(2018). \nDeepMind \nControl \nSuite. \nhttps:\/\/arxiv.org\/abs\/1801.00690 \nTodorov, E. (2018). MuJoCo Unity Plugin. November 6, 2018, \nfrom http:\/\/www.mujoco.org\/book\/unity.html \nTodorov, E., Erez, T., & Tassa, Y. (2012). MuJoCo: A physics en-\ngine for model-based control. 2012 IEEE\/RSJ International Con-\nference on Intelligent Robots and Systems, 5026–5033. \nhttps:\/\/doi.org\/10.1109\/IROS.2012.6386109 \nSupplementary Materials \nA. Detail on using the AgentReset, StepReward,  \nand TerminateFunction functions when modifying \nagents.  \n AgentReset() is used to initialize the agent. It should \nset the callbacks for StepRewardFunction, Termi-\nnateFunction, and ObservationsFunction. The \ndeveloper should add model elements to the BodyParts \nlist and call SetupBodyParts()to initialize the body \nparts. This enables callbacks to leverage helper functions; \nfor example, GetForwardBonus(pelvis) calculates a \nbonus based on the body part's distance from the forward \nvector. \n StepReward() returns a float with the reward for \nthe current action step. Helper functions include the follow-\ning: GetVelocity(pelvis) returns the velocity of the \nspecified body part; GetEffort() returns the sum of the \ncurrent actions (one can pass a list of body parts to ignore); \nand GetJointsAtLimitPenality() returns a pen-\nalty for actions which are at their limit. \n TerminateFunction() returns true if a termina-\ntion condition is met. Termination functions help improve \ntraining speed by reducing the agent’s exposure to unhelpful \nobservations. Helper termination functions include Termi-\nnateNever(), which never terminates (always returns \nfalse) and TerminateOnNonFootHitTerrain(), \nwhich returns true if a body part that is not a foot collides \nwith the terrain. Body parts are defined in the function On-\nTerrainCollision(). Some agents required the lower \nleg body parts to be labeled as foot as they protrude \nthrough the foot geometry and create false positive termina-\ntions. \n \nB. Training Hyperparameters for the Training \nPerformance in section 3.  \nDeepMindHumanoidBrain: \n  normalize: true \n  num_epoch: 3 \n  beta: 0.01 \n  time_horizon: 1000 \n  batch_size: 2048 \n  buffer_size: 20480 \n  gamma: 0.995 \n  max_steps: 2e6 \n  summary_freq: 1000 \n  num_layers: 2 \n  hidden_units: 512 \nDeepMindHopperBrain: \n  beta: 1.0e-2 \n  epsilon: 0.20 \n  gamma: 0.99 \n  lambd: 0.95 \n  learning_rate: 1.0e-3 \n  num_epoch: 3 \n  time_horizon: 128 \n  summary_freq: 1000 \n  use_recurrent: false \n  normalize: true \n  num_layers: 2 \n  hidden_units: 90 \n  batch_size: 2048 \n  buffer_size: 10240 \n  max_steps: 3e5 \n  use_curiosity: true \n  curiosity_strength: 0.01 \n  curiosity_enc_size: 256     \nDeepMindWalkerBrain: \n  beta: 1.0e-2 \n  epsilon: 0.20 \n  gamma: 0.99 \n  lambd: 0.95 \n  learning_rate: 1.0e-3 \n  num_epoch: 3 \n  time_horizon: 128 \n  summary_freq: 1000 \n  use_recurrent: false \n  normalize: true \n  num_layers: 3 \n  hidden_units: 41 \n  batch_size: 2048 \n  buffer_size: 10240 \n  max_steps: 3e5 \n  use_curiosity: true \n  curiosity_strength: 0.01 \n  curiosity_enc_size: 256     \nOpenAIAntBrain: \n  beta: 5.0e-3 \n  epsilon: 0.20 \n  gamma: 0.99 \n  lambd: 0.95 \n  learning_rate: 1.0e-3 \n  num_epoch: 3 \n  time_horizon: 128 \n  summary_freq: 1000 \n  use_recurrent: false \n  normalize: true \n  batch_size: 2048 \n  buffer_size: 10240 \n  num_layers: 3 \n  hidden_units: 53 \n  max_steps: 3e5 \n \n \nC. Training Hyperparameters for Marathon Envi-\nronment Baselines.  \nDeepMindHopperBrain: \n  learning_rate: 3.0e-3 \n  num_epoch: 10 \n  time_horizon: 128 \n  summary_freq: 1000 \n  use_recurrent: false \n normalize: true \n num_layers: 2 \n  hidden_units: 64 \n  batch_size: 2048 \n  buffer_size: 10240 \n  max_steps: 62500 \n \n \n \n  \n \n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Marathon Environments: Multi-Agent Continuous Control Benchmarks in a Modern Video Game Engine.pdf"}
{"title":"Benchmarking Cognitive Abilities of the Brain with Computer Games","authors":"Norbert Bátfai, Dávid Papp, Renátó Besenczi, Gergő Bogacsovics, Dávid Veres","summary":"Most of the players have experienced the feeling of temporarily losing their\ncharacter in a given gameplay situation when they cannot control the character,\nsimply because they temporarily cannot see it. The main reasons for this\nfeeling may be due to the interplay of the following factors: (1) the visual\ncomplexity of the game is unexpectedly increased compared with the previous\ntime period as more and more game objects and effects are rendered on the\ndisplay; (2) and\/or the game is lagging; (3) and finally, it is also possible\nthat the players have no sufficient experience with controlling the character.\nThis paper focuses on the first reason. We have developed a benchmark program\nwhich allows its user to experience the feeling of losing character. While the\nuser can control the character well the benchmark program will increase the\nvisual complexity of the display. Otherwise, if the user lost the character\nthen the program will decrease the complexity until the user will find the\ncharacter again, and so on. The complexity is measured based on the number of\nchanged pixels between two consecutive display images. Our measurements show\nthat the average of bit per second values of losing and finding pairs describes\nthe user well. The final goal of this research is to further develop our\nbenchmark to a standard psychological test.","url":"http:\/\/arxiv.org\/abs\/1809.00172v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1809.00172v1","published":1535807823000,"comment":"20 pages","pdf_text":"Benchmarking Cognitive Abilities of the Brain\nwith Computer Games\nNorbert Bátfai1,*, Dávid Papp2, Renátó Besenczi1, Gergő\nBogacsovics1, and Dávid Veres1\n1Department of Information Technology, University of Debrecen,\nHungary\n2Department of Psychology, University of Debrecen, Hungary\n*Corresponding author: Norbert Bátfai,\nbatfai.norbert@inf.unideb.hu\nSeptember 5, 2018\nAbstract\nMost of the players have experienced the feeling of temporarily losing\ntheir character in a given gameplay situation when they cannot control\nthe character, simply because they temporarily cannot see it. The main\nreasons for this feeling may be due to the interplay of the following factors:\n(1) the visual complexity of the game is unexpectedly increased compared\nwith the previous time period as more and more game objects and eﬀects\nare rendered on the display; (2) and\/or the game is lagging; (3) and\nﬁnally, it is also possible that the players have no suﬃcient experience\nwith controlling the character. This paper focuses on the ﬁrst reason. We\nhave developed a benchmark program which allows its user to experience\nthe feeling of losing character. While the user can control the character\nwell the benchmark program will increase the visual complexity of the\ndisplay. Otherwise, if the user lost the character then the program will\ndecrease the complexity until the user will ﬁnd the character again, and so\non. The complexity is measured based on the number of changed pixels\nbetween two consecutive display images. Our measurements show that\nthe average of bit per second values of losing and ﬁnding pairs describes\nthe user well. The ﬁnal goal of this research is to further develop our\nbenchmark to a standard psychological test.\nKeywords: esport, talent search, benchmark program, complexity, psychol-\nogy test.\n1\nIntroduction\nLosing the control of the character in a given gameplay situation is a very\ncommon feeling that is well known among gamers. In this situation, players\ncannot control their character, simply because they temporarily cannot see it\ndue to the visual complexity of the display is unexpectedly increased and\/or\n1\narXiv:1809.00172v1  [cs.HC]  1 Sep 2018\nFigure 1: A screenshot of BrainB Test Series 6 in action. Can you ﬁnd the box\nlabelled by the name Samu Entropy in this picture?\nthe game is lagging and, ﬁnally, it is also possible that the players have no\nsuﬃcient experience to control the character. In this paper, we introduce our\nbenchmark computer program called BrainB Test Series 6 that can abstract\nthis feeling. In this test, game objects are symbolized by boxes as it can be seen\nin Fig 1. All boxes move according to random walks. There is a distinguished\nbox labelled by the name Samu Entropy. It represents the character controlled\nby the player.\nThe benchmark test lasts for 10 minutes.\nDuring the test,\nthe user must continuously hold and drag the mouse button on the center of\nSamu Entropy. If the user succeeds in this task then the benchmark program\nwill increase the visual complexity of the display. It will draw more and more\noverlapping boxes which will move faster and faster. Otherwise, if the mouse\npointer cannot follow the center of Samu Entropy then the visual complexity\nwill be decreased. The test will delete more and more boxes and the remaining\nboxes move slower and slower until the user ﬁnds Samu Entropy again, i.e.,\nclicks on Samu Entropy.\nThe BrainB Series 1 to 4 were developed in the family setting of the ﬁrst\nauthor1. Then, in our university environment, we had already done a prelimi-\nnary study [BBP+18] on a previous (BrainB Series 5) version of our benchmark.\nSome of its measurements were streamed live on Twitch2. The main research\ngoal of this study is to show that players lose the character on a higher com-\nplexity level of the display and they ﬁnd it on a relatively lower complexity\nlevel.\nThe organization of the paper is the following. In the next section, we give\na brief overview of the psychological and informatics background and the phe-\nnomenon of losing the character is illustrated. The second section presents the\nalgorithm and the operation of our benchmark program including the presen-\ntation of the ﬁrst measurements followed by systematic measurements. Finally,\nwe conclude our paper and show some future plans.\n1For example, see https:\/\/www.twitch.tv\/videos\/139186614\n2For example, see https:\/\/www.twitch.tv\/videos\/206478952\n2\n1.1\nPsychological Background\nThe cognitive ability of attention is a signiﬁcant factor in everyday life, either\nit comes from work, hobby or the daily activities, as it aﬀects the performance\nof all the previously mentioned things. The alertness, or in other words, the\nlong upheld attention, in technical terms is called vigilance. The research of\nvigilance is an important topic in Psychology from 1970 to the present day. The\nﬁrst method used was the Mackworth Clock [Mac48], in which the participants\nhad to pay attention to a clock that had a second hand which sometimes sprang\ntwice, and then the participants had to signal as soon as possible. For measuring\nattention and concentration, there is another method, the Toulouse-Piéron test\n[TP13], in which participants have to follow a given scheme to separate right\nand wrong signs. To measure vigilance we must take into consideration the hit\nratio and the number of false alarms. In almost all of our activities there are\nalso interfering stimuli that aﬀects our performance as well. These other factors\nvary by quantity and quality, and some can be stimulating, while some detain\nus from the optimal performance. The Yerkes-Dodson law [YD08] says that for\nachieving the best performance there is an optimal arousal level, which level is\nhigher in simpler tasks, and lower in complex activities. It can be represented\nby a inverted U-shaped curve. We must not forget that as in some other things,\nin the attentional system there are also personal diﬀerences that should be taken\ninto consideration while researching the subject [CGR07].\nOther objects in the environment can aﬀect how we perceive the one object\nthat is interesting for us. In 1940, Witkin et al. did a research on perception\n[WLH+54], and from this work, they created a theory about two diﬀerent cog-\nnitive styles, which they called ﬁeld dependent, and ﬁeld independent. A ﬁeld\ndependent person perception is mainly aﬀected by the ﬁeld, the environment\nof the observed object. On the contrary, a ﬁeld independent person does not\naﬀected by the ﬁeld created by the observed object’s environment. This phe-\nnomenon was investigated by a task, in which the participants had to determine\nwhether a straight rod, in diﬀerent planes is vertical or not. Moreover, there\nis another typical method used in this topic, that is the Tilting room, Tilting\nchair test, in which the participant is sitting in a tiltable chair that he or she\nneeds to controll in order to get him\/herself into vertical position despite the\ntilting room. Later, Witkin and Goodenough reinvestigated the topic, and they\ncame to a conclusion that the two styles are two ends of the spectrum, however,\nsome people are ﬁxed with one of the cognitive styles, while others can adapt\nto the style they use depending on the situation [AWRG76].\nSpeaking of attention, it’s important to talk about the main processing sys-\ntem, i.e., the brain. The operation of the brain is frequently compared to the\nmechanism of a personal computer by many researchers. Carl Sagan based his\ntheory on the binary coding, so he used the information content in binary. When\nwe are watching something, the picture seen that our brain maps, is made of\nplenty of information. Sagan wanted to calculate the information processing\nspeed of the brain, to do so, he based his calculation on the example of looking\nat the moon, and from this example he drew the consequence, that the brain\ncan process about 5000 bit\/sec at its peak performance [Sag12]. In a modern\nproject, called Building 8, the main thought is to make the brain into a com-\nputer. Based on this project, the information processing speed of the brain is\nabout a terrabyte\/sec, which far exceeds the speed estimated by Sagan [Nie17].\n3\n1.1.1\nPracticing ﬁlling out tests\nFilling out tests and experiments are common tools in the science of psychology.\nCountless methods were created to date, but these methods are not just used,\nbecause researchers improve them, as well as, try to test them in a wider range.\nHowever, we need to consider certain factors in each experiment and test that\nhow they aﬀect the method’s usability and the ﬁnal results as well. Among\nthese factors, there is one, when the participant obtains knowledge about what\nis expected from her\/him, or which answer are considered the ’best’. This way\nthe participant will accomodate to this information, because he\/she, as everyone\nelse, wants to portray herself\/himself in the best manner possible and to be the\n’best’ in performance. In multiple choice questions, there are some tricks, that\nare well known in the common knowledge, which we all use, when we don’t know\nthe right answer for sure. A somewhat similar tool is the experience or routine\nwith ﬁlling out tests, which can help to choose the adequate strategy for solving\nthe situation, this is called test-wisdom. To achieve that, one must discover the\nlogic behind the method, or practise it many times. But the test-wisdom often\ncause inconvenience for test developers, because they have to keep in mind a\nbonus factor, which is totally diverge from the basic variables they meant to\nmanipulate, and vary in each individual [RSA06].\nThe eﬀect of being experienced in ﬁlling in tests was studied in a research,\nin which an aptitude test called GRE (Grand Record Examination) was used.\nPractise samples were sent to random participants 5 weeks before the exam-\nination. Those who got these samples also receieved advices for completing.\nIn conclusion, the group with prior knowledge and practise earned signiﬁcantly\nbetter results in the examination. Furthermore, there were also a notable growth\nin points, when the participants received an only 4 hours educational practise\nbefore the examination. It’s important to note that this diﬀerence and growth\nwas present only in the logical reasoning part of the exam, and not in the math-\nematical and verbal parts [PS84].\nThis data was reexamined later, because\nresearchers wanted to know, if there is any diﬀerence when the existing groups\nwould be split into subgroups by the diﬀerent attributes of the participants. As a\nconclusion, there was no signiﬁcant diﬀerence between the subgroups, but there\nwas a notable diﬀerence in the group in which the members’ primary language\nwas not English, they scored lesser points than the others [Pow86].\nRepeatedly performing the same experiment or test with the same partici-\npants could aﬀect the results. Previously, as we speciﬁed, repeatedly using the\nsame method could cause the lowering of its validity, and the results could be\ndistorted. Participants can learn and adapt to certain methods, even if its just\nmeans a small percentage of diﬀerence. The current test takes 10 minutes to\ncomplete, in this 10 minutes the participant’s full attention and concentration\nis needed. We should keep in mind, that the negative eﬀects of fatigue could\nbalance the positive eﬀects of practise, in a direct way of repeated examinations.\nSo this two factors should be considered in the evaulation, and while drawing\nconsequence.\nIt is therefore proposed to perform our benchmark test in a competitive way\ntrying to beat friends, family members, colleagues or ourselves.\n4\n1.2\nInformatics Background\nSince computer games have a relatively short history and their eﬀects on cog-\nnitive skills have just been started to be researched recently, there are plenty\nof questions to be answered. In [HPR+18], authors reported an increase in ex-\necutive functions in school students after playing computer games. Moisala et\nal. in [MSH+17] shows that enhancements in speed and performance accuracy\nof working memory tasks is related to daily gaming activity.\nIn [BAM+18],\nauthors present an analysis of the impact of action video games on cognitive\nskills.\nUsing computer games to measure cognitive abilities has a short history, but\na promising future. Most research try to measure the presence or severity of a\ncertain cognitive disease such as dementia or Alzheimer’s disease. In [ABR+13],\nauthors show how a long-term use of video games can reduce multitasking costs\nin older adults. Geyer et al. in [GIF+15] show that the change of the score of an\nonline game is in connection with the age-related changes in working memory.\nSeldom can we ﬁnd applications that has been developed for the measure-\nment of cognitive abilities. One such application is reported in [PHC15b] and\n[PHC15a], it is a framework that has been developed to measure cognitive abil-\nities and its change of elders with computer games. This framework is able to\nlog and analyze scores achieved in various online computer games.\nFrom the viewpoint of information theory and HCI (Human-Computer In-\nteraction), the Hick’s law [Seo05] could be an interesting aspect. This law states\nthat the response time of the brain increases with logarithm of the size of the\ninput. For our purposes, it can be an interesting question: how can we apply\nthe Hick’s law (or other information theory ﬁgure) in our benchmark software?\n1.3\nLosing The Character\nWe have experienced the feeling of losing the character during playing several\ngames like for example League of Legends3, Clash of Clans4, Clash Royale5,\nHeroes of the Storm6, Dota 27, World of Warcraft8 or Cabal9.\nNow we share our thoughts about the phenomenon of „losing the character”,\nand give some examples to illustrate it from the game called League of Legends.\nBasically, a match starts kind of slowly and quietly: the laners are farming, as\nwell as the junglers in their own territory. Of course smaller ﬁghts can occur\nin the early stages of the game, like a 1v1 in the solo lanes, or a 3v3 in the\nbottom lane as both of the junglers decides to gank, but these situations are\nrelatively easy to see through. As we head into the mid and late game, teams\nstart ﬁghts more often with more people, even with all of them. This is what\nwe call teamﬁghts. These are harder to handle, because a lot of things can\nappear on our screen at the same time: the champions who participate in the\nﬁght, optionally minions or jungle monsters, and the visual eﬀects of the spells,\nsummoner spells, and the active or passive abilities of the items. Besides them,\n3https:\/\/na.leagueoflegends.com\n4http:\/\/supercell.com\/en\/games\/clashofclans\/\n5http:\/\/supercell.com\/en\/games\/clashroyale\/\n6https:\/\/heroesofthestorm.com\n7https:\/\/www.dota2.com\n8https:\/\/worldofwarcraft.com\n9http:\/\/cabal.playthisgame.com\n5\nwe see a lot of things, we still have to make sure that we fulﬁll our ingame\nrole properly: position well, attack the proper target, or defend our teammates.\nWe have to handle a lot of information at a blink of an eye, so it is completely\nnatural, that sometimes we do not know where to look at or what to do. We can\nlose our own character, which can end with our death; we can lose the target\ncharacter, and it can survive; or we can lose the character that we wanted to\nprotect, thus an important member of the team can die. This can be a short\nexplanation of the phenomenon, which we can also call „losing the focus”.\nAn example ingame footage can be viewed at https:\/\/youtu.be\/wdy3KUm1454,\nstarting at 2:12.\nThese situations are one of the hardest parts of the game, and it is not\neasy to handle them well. The easiest way to prepare for them is to play a\nlot of games, and get experience in them. Also it can help a lot, if we think\nahead before a potential teamﬁght, e.g. which character will be our target, who\nshould we be afraid of, what summoner spells the enemy still has, etc. All of\nthese things can help to execute the ﬁghts more properly.\n2\nBrain Benchmarking Series 6\nBrainB is a Qt C++ desktop application that uses the OpenCV library. It is\ndeveloped as an open source project that is available on GitHub [B´17]. Its source\ncan be built easily on GNU\/Linux systems. But the latest (6.0.3) Windows\nbinary release can also be downloaded as a ZIP ﬁle from http:\/\/smartcity.\ninf.unideb.hu\/~norbi\/BrainBSeries6\/.\nIt is important to show the algorithm of BrainB as precise as possible because\nthe randomness plays a key role in its operation due to boxes doing random\nwalks.\nThe code snippet shown in Listing 1 is the heart of our benchmark\nprogram. It is a simpliﬁed version of the original source code that can be found\nin the GitHub repository at https:\/\/github.com\/nbatfai\/esport-talent-\nsearch\/blob\/master\/BrainBWin.cpp#L65. This code is executed at every 100\nmilliseconds that is ten times per second. First, as shown in Line 1, it computes\nthe distance between the mouse pointer and the center of the box of Samu\nEntropy and the result is stored in the variable called dist that holds the square\nof the Euclidean distance. If the distance is larger than 121 pixels (11 is the\nsquare root of 121) and if it reoccurs 12 consecutive times or more in a row (that\nmeans at least a time interval of 1.2 seconds) and it is also true that the player\nwas controlling the character well in the previous time slices (that is in Line 10\nthe state is equal to found) then we say that the user has lost the character\nSamu Entropy and the visual complexity of the display will be saved in Line 12.\nThe sequence of these losing values and the symmetrical ﬁnding values saved in\nLine 28 are shown in Fig 2. The complexity is computed in bits per second (bps)\nunits that is based on the number of changed pixels between two consecutive\nrectangular environments of the character with a given width and height.\nListing 1: The algorithm for administration of losing and ﬁnding the character.\n1 int\ndist = ( this ->mouse_x\n- x ) * ( this ->mouse_x\n- x )\n2\n+ ( this ->mouse_y\n- y ) * ( this ->mouse_y\n- y );\n3\n4 if ( dist > 121 )\n5\n{\n6\n++ nofLost;\n6\n7\nnofFound = 0;\n8\nif ( nofLost\n> 12 )\n9\n{\n10\nif ( state == found &&\nfirstLost )\n11\n{\n12\nfound2lost.push_back(brainBThread ->get_bps ());\n13\n}\n14\nfirstLost = true;\n15\nstate = lost;\n16\nnofLost = 0;\n17\nbrainBThread ->decComp ();\n18\n}\n19\n}\n20 else\n21\n{\n22\n++ nofFound;\n23\nnofLost = 0;\n24\nif ( nofFound\n> 12 )\n25\n{\n26\nif ( state == lost &&\nfirstLost )\n27\n{\n28\nlost2found.push_back(brainBThread ->get_bps ());\n29\n}\n30\nstate = found;\n31\nnofFound = 0;\n32\nbrainBThread ->incComp ();\n33\n}\n34\n}\nThe ﬁnal result printed by the benchmark after it ends in the form “U R\nabout 5.92902 Kilobytes” is the mean of upper bounds for the bps values of the\ndisplay measured when the variable state changes from found to lost (in Listing\n1 from Line 10 to 14) and vice versa, when the variable state changes from lost\nto found (in Listing 1 from Lines 26 to 30). The simple calculation of this ﬁnal\nresult is shown in Listing 2.\nListing 2: The calculation of the ﬁnal result of the benchmark that is produced\nin a text ﬁle that is saved in the folder where the benchmark was started.\n1\nint m1 = mean ( lost2found\n);\n2\nint m2 = mean ( found2lost\n);\n3\n4\ndouble\nres = ( ( ( ( double ) m1\n5\n+ ( double ) m2 ) \/2.0 ) \/8.0 ) \/1024.0;\n6\n7\ntextStream\n<< \"U␣R␣about␣\" << res\n<< \"␣Kilobytes\\n\";\n2.1\nFirst Measurements\nAs concluded in our former preliminary study [BBP+18], one of the further de-\nvelopments of Series 5 is changing to full screen from ﬁxed-size window. This\nmodiﬁcation aﬀects the basic operation of the benchmark, so the ﬁrst objective\nwas to verify that whether the feeling of losing the character still appears cor-\nrectly or not. On Windows systems there were no problems. Some experiments\nusing default settings on Windows 10 can be seen in Fig 3, Fig 4 and Fig 5.\nBut on GNU\/Linux systems test subjects reported that the feeling of losing\nthe character is not experienced. These observations will be detailed in a next\nsection.\n2.2\nLogging Data\nThe state of the BrainB benchmark can be saved at any time by pressing the S\nbutton but measured data is automatically saved after the test is ﬁnished. The\n7\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n50\n60\n20000\n40000\n60000\n80000\nIndex\nbps\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nFigure 2: The bps values associated to events of losing and ﬁnding. The ﬁrst\nelement of this sequence is the ﬁrst element of the lost2found (shown in Listing\n1 Line 10) sequence. The second element is the ﬁrst element of the found2lost,\nand so on.\nIt should be noticed that the losing (labelled by L) and ﬁnding\n(F) events are mixed, see, for example the 13th event on the x axis where\nthe complexity of ﬁnding is greather than the complexity of losing in this in-\ndividual measurement. This test was performed by the ﬁrst author (46 years\nold, on a Dell XPS 9333 ultrabook with Windows 10 using the touchpad, res-\nolution 1920x1080, scale 150%). The ﬁnal result was 5.92902 Kilobytes. All\nthe logged data can be found at http:\/\/smartcity.inf.unideb.hu\/~norbi\/\nBrainBSeries6\/measurements\/NB\/.\nFig 1 shows the last screenshot of this\nexperiment.\n8\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n50\n20000\n40000\n60000\n80000\nIndex\nbps\n(a) With using the touchpad.\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n50\n20000\n40000\n60000\n80000\nIndex\nbps\n(b) With using a standalone mouse.\n(c) The ﬁnal result was 5.45563 Kilo-\nbytes.\n(d) The ﬁnal result was 6.37927 Kilo-\nbytes.\nFigure 3:\nThese tests were also performed by the ﬁrst author on the\nsame environment as in Fig 2.\nAll the logged data and ﬁnal screenshots\ncan be found at http:\/\/smartcity.inf.unideb.hu\/~norbi\/BrainBSeries6\/\nmeasurements\/NB\/.\n9\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n20\n40\n60\n80\n20000\n40000\n60000\n80000\nIndex\nbps\n(a) 6.51813 Kilobytes (performed with\nthe touchpad).\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n50\n20000\n40000\n60000\n80000\nIndex\nbps\n(b) 4.31812 Kilobytes (performed with\na mouse).\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n20\n40\n60\n0\n20000\n40000\n60000\n80000\nIndex\nbps\n(c) 6.79218 Kilobytes (performed with\nthe touchpad).\nFigure 4:\nThis test was performed with a male child (10 years old, on\nthe same environment as in Fig 2).\nThe data and ﬁnal screenshots\ncan be found at http:\/\/smartcity.inf.unideb.hu\/~norbi\/BrainBSeries6\/\nmeasurements\/NaB\/. It should be noted that test subjects with touchpad can\nuse both hands, one for holding the button and the other for motion.\n10\n(a) This ﬁnal screenshot corresponds to\nFig 4a.\n(b) This ﬁnal screenshot corresponds\nto Fig 4b.\n(c) This ﬁnal screenshot corresponds to\nFig 4c.\nFigure 5: This test was performed with a male child (10 years old, on the same\nenvironment as in Fig 2). The data and ﬁnal screenshots can be found at http:\n\/\/smartcity.inf.unideb.hu\/~norbi\/BrainBSeries6\/measurements\/NaB\/.\nprogram saves a screenshot of the display to a PNG ﬁle. For example, such a\nscreenshot was shown in Fig 1. The data is saved to a text ﬁle that contains\nthe information shown in Listing 3, where the most important lines are the\nfollowing. The two time values tell the time when data is saved. The ﬁrst one\n(in Line 2) is expressed in 100 millisecond units and the second one (in Line\n48) is expressed in the form minutes:seconds. The noc value tells the number\nof characters (boxes). The nop value tells the number of pause events initiated\nby the test subject. The relation symbol in Line 47 indicates the fulﬁllment of\nour research hypothesis that the mean of the complexity of changing from lost\nto found is less than the mean of the changing found to lost.\nListing 3: The structure of the measured data. This log ﬁle is belong to the\nmeasurement shown in Fig 3d.\n1\nNEMESPOR\nBrainB\nTest\n6.0.3\n2\ntime\n: 6000\n3\nbps\n: 28170\n4\nnoc\n: 71\n5\nnop\n: 0\n6\nlost\n:\n7\n30530\n31840\n39910\n10960\n60270\n71280\n50340\n51580\n31670\n8\n49260\n53710\n41620\n86830\n72580\n56310\n70560\n68870\n45500\n9\n52480\n52660\n45640\n46870\n44660\n75860\n68150\n70110\n69610\n10\n47130\n61980\n75310\n90440\n75700\n62670\n54870\n69820\n75170\n11\n84350\n76990\n80480\n70840\n54920\n40720\n33800\n31590\n28860\n12\n24650\n27250\n53490\n58180\n56200\n57490\n53930\n39030\n83870\n13\n87180\n78270\n70990\n43600\n52360\n43910\n33820\n31120\n34830\n14\n32370\n32840\n37080\n32390\n15\nmean\n: 54181\n16\nvar\n: 18541.5\n17\nfound\n:\n18\n12880\n22240\n26690\n11190\n19880\n36170\n14930\n28100\n25860\n11\n19\n27580\n36040\n34590\n22250\n12060\n11760\n8880\n10660\n30840\n20\n48000\n33030\n43040\n26330\n45880\n50380\n34970\n45950\n36610\n21\n46660\n47980\n45330\n65290\n57080\n55340\n54700\n43930\n34850\n22\n55030\n43240\n69500\n50770\n58680\n54750\n65470\n59610\n79030\n23\n67190\n63890\n61550\n65590\n54100\n69460\n69210\n37390\n41850\n24\n53130\n31650\n45400\n46430\n50490\n44310\n35960\n53510\n25760\n25\n38950\n33250\n39360\n46650\n63050\n64890\n68590\n76430\n50570\n26\n57630\n57250\n28830\n42020\n45500\n67160\n63310\n69930\n80200\n27\n76980\n56300\n44320\n58340\n79850\n81590\n69740\n88200\n89160\n28\n62640\n55030\n60510\n39810\n51660\n51730\n47720\n62330\n66150\n29\n47100\n60470\n70810\n88930\n75110\n65290\n68830\n59430\n63710\n30\n22570\n36940\n29450\n43630\n53100\n55560\n64750\n39530\n59610\n31\n58250\n71950\n62800\n75250\n76720\n81910\n31730\n47010\n44890\n32\n58490\n61750\n66900\n69380\n81650\n79450\n72420\n33\nmean\n: 51442\n34\nvar\n: 18616.1\n35\nlost2found: 14930\n22250\n11760\n43040\n26330\n34970\n46660\n36\n43930\n50770\n61550\n54100\n37390\n31650\n44310\n25760\n50570\n37\n28830\n56300\n69740\n62640\n39810\n62330\n65290\n59430\n22570\n38\n39530\n31730\n72420\n39\nmean\n: 43235\n40\nvar\n: 16826.7\n41\nfound2lost: 31840\n10960\n60270\n51580\n31670\n49260\n53710\n42\n86830\n70560\n68870\n45500\n52660\n45640\n46870\n75860\n69610\n43\n61980\n75310\n90440\n54870\n69820\n75170\n84350\n80480\n53490\n44\n56200\n83870\n78270\n45\nmean\n: 61283\n46\nvar\n: 18824.2\n47\nmean(lost2found) < mean(found2lost )\n48\ntime\n: 10:0\n49\nU R about\n6.37927\nKilobytes\n2.3\nChoosing Colors\nWe put a lot of emphasis on what colors to choose for our benchmark. The\nreason for this is that even the standard test requires a constant focus of 10\nminutes, which can put a lot of pressure on one’s eyes. To ease this strain as\nmuch as we possibly could, we took lots of things into account. Firstly, we tried\nto maximize the contrast between the background and the ﬁgures. This means\nthat we picked some colors that could be easily distinguished and then we ran\nsome manual tests. The result was a signiﬁcant drop in the overall burden of\nthe eyes.\nAfter this, we thought about how we could make the benchmark available\nfor a wider range of people, namely for those who suﬀer from parachromatism\nor even disambiguation. This is rather important as it is said that roughly 8%\nof men and 0.5% of women10 suﬀer from one of these. In order for them to be\nable to comfortably run our benchmark, we tried to pick colors that are easily\ndistinguishable even for these people.\nAnother problem was that we did not target a speciﬁc age group. On the\ncontrary, we were especially curious about the results of adults, adolescents,\nteenagers and children. Therefore, we needed to pick a color scheme that was\nmodern, vivid, yet not too complex and not too abstract. This, too, required a\nlot of experimentation.\n2.4\nKnown Problems with Series 6\nDespite that our benchmark is developed on Linux it is surprising that test\nsubjects who performed it on Linux did not experience the feeling of losing\n10http:\/\/www.color-blindness.com\/2006\/04\/28\/colorblind-population\n12\nthe character. This problem causes the deteriorated results shown in Fig 6.\nIt is important to note that it has not been detected in earlier series of the\napplication. Moreover, before Series 6, there was no Windows binary edition of\nBrainB program. In Series 6, changing to full screen from windowed causes the\nproblem because Series 6 is sensitive to the diﬀerent mouse sensitivity settings on\nWindows and Linux systems (the measurements shown in Fig 6 were performed\nwith a Logitech mouse with acceleration: 5\/1 and threshold: 5 xset m11 setting).\nA short-term solution may be to standardize the test environment used by each\nmember of a given subset of test subjects. We apply this method to perform\nsystematic measurements with Series 6 in the next section.\nThe long-term\nsolution will be to ﬁne-tune the control of movements of boxes that is hardwired\ninto the Series 6 from Series 5 at this moment. Another possibility is to take\nthe liberty of ﬁne-tuning of the mouse for test subjects who thus would be able\nto choose their custom mouse settings in order to increase their eﬀectiveness.\nThis is also in well accordance with the competitive way of performing our test.\nFig 7 presents two measurements using custom mouse settings.\n2.5\nSystematic Measurements with Series 6\nThe BrainB Series 6 was measured in two groups:\nUDPROG and DEAC-\nHackers. The ﬁrst one is a Facebook community of over 550 actual or former\nstudents of the BSc course of “High Level Programming Languages” at the Uni-\nversity of Debrecen. The second one is an esport department of the University\nof Debrecen’s Athletic Club. Participation in the BrainB Series 6 survey was\nvoluntary in both groups.\nIn the UDPROG community 33 members send back their results including\nthe PNG screenshot and the produced text ﬁle within 2 days from the date of\nannouncement (20 August 2018). The arithmetic mean of the ﬁnal results of\nUDPROG participants is 4.95345. The mean of the number of boxes at the\nmoment when the benchmark ends is 57.1818. The averaged losing and ﬁnding\ncurve for all members is shown in Fig 8a. At the end of the curve the arithmetic\nmean values of complexity of the losing and ﬁnding events are irrelevant because\nthe size of the sequences of losing and ﬁnding events are diﬀerent for every\nparticipants. Fig 8b indicates these diﬀerent sizes.\nIn the DEAC-Hackers community 12 esport athletes have sent back their\nresults that can be seen in Fig 9. It is important to notice that despite low\nsample sizes of test subjects the averaged losing and ﬁnding curves shown in\nFig 8a and 9a have already separated the losing and ﬁnding events.\n3\nConclusion\nOur research hypothesis was that the mean of the complexity of changing lost\nto found is less than the mean of the changing found to lost. Fig 8a and 9a show\nthe fulﬁllment of this hypothesis. It seems very well in these ﬁgures that the\naveraged losing and ﬁnding curve has precisely separated the losing and ﬁnding\nevents.\nIntuitively, this result shows that we lose the character on a higher\ncomplexity level then we ﬁnd it on a relatively lower level again. This simple\nhypothesis has been proved by the results of this study.\n11https:\/\/www.x.org\/archive\/X11R7.7\/doc\/man\/man1\/xset.1.xhtml\n13\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n2\n4\n6\n8\n10\n12\n0\n10000\n20000\n30000\n40000\n50000\n60000\nIndex\nbps\n(a) The test subject was the same as\nin the experiment shown in Fig 3. The\nﬁnal result was 3.76904 Kilobytes.\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n5\n10\n15\n20\n25\n10000\n20000\n30000\n40000\n50000\n60000\nIndex\nbps\n(b) The test subject was the same as\nin the experiment shown in Fig 4. The\nﬁnal result was 3.75116 Kilobytes.\n(c) This ﬁnal screenshot corresponds to\nFig 6a.\n(d) This ﬁnal screenshot corresponds\nto Fig 6b.\nFigure 6: These tests were performed on a GNU\/Linux desktop (Ubuntu 16.04,\nSyncMaster S24B300 monitor with resolution 1920x1080).\nTest subjects re-\nported that the feeling of losing the character is not experienced.\n14\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n20000\n30000\n40000\n50000\n60000\n70000\n80000\nIndex\nbps\n(a) The ﬁnal result was 5.95587 Kilo-\nbytes. xinput settings were the follow-\ning “Device Accel Constant Decelera-\ntion (277):\n1.000000”, “Device Accel\nVelocity Scaling (279): 1.000000” and\n“Device Accel Proﬁle (276): -1”\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\n0\n10\n20\n30\n40\n20000\n40000\n60000\n80000\nIndex\nbps\n(b) The ﬁnal result was 5.71674 Kilo-\nbytes. xinput settings were the follow-\ning “Device Accel Constant Decelera-\ntion (277):\n2.000000”, “Device Accel\nVelocity Scaling (279): 15.000000” and\n“Device Accel Proﬁle (276): -1”\n(c) This ﬁnal screenshot corresponds to\nFig 7a.\n(d) This ﬁnal screenshot corresponds\nto Fig 7b.\nFigure 7: The test subject was the same as in the experiment shown in Fig 3.\nThe subject reported that the feeling of losing the character has already been\nexperienced.\n15\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n50\n60\n70\n20000\n40000\n60000\n80000\nIndex\nbps\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\n(a) This ﬁgure shows the averaged los-\ning and ﬁnding curve for all UDPROG\nparticipants where the losing (L) and\nﬁnding (F) events are also indicated.\nGGGGGGGGG\nGGGG\nGG\nGGGG\nGGGGG\nG\nGGGG\nGG\nGGGG\nGGGGGG\nGGG\nGGG\nG\nG\nGG\nG\nGG\nG\nG\nG\nGG\nG\nGGG\nGGG\nG\nGG\nGG\n0\n10\n20\n30\n40\n50\n60\n70\n0\n5\n10\n15\n20\n25\n30\nIndex\nsize\n(b) The sizes of samples of losing and\nﬁnding events.\nThe x-axis shows the\nsizes and the y-axis shows the number\nof test-subjects.\nFigure 8: Measurements in the community UDPROG. The arithmetic mean\nof the ﬁnal results of UDPROG participants is 4.95345.\nThe mean of the\nnumber of boxes at the moment when the benchmark ends is 57.1818.\nThe\nanonymized data can be found at http:\/\/smartcity.inf.unideb.hu\/~norbi\/\nBrainBSeries6\/measurements\/UDPROG\/.\n16\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\n50\n60\n70\n20000\n30000\n40000\n50000\n60000\n70000\n80000\nIndex\nbps\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\nF\nL\n(a) This ﬁgure shows the averaged los-\ning and ﬁnding curve for all DEAC-\nHackers participants where the losing\n(L) and ﬁnding (F) events are also in-\ndicated.\nGGGGGGG\nGGGG\nGGG\nGGGGG\nGGGGGG\nG\nGGGGG\nGGGGGGGGGGGGGG\nGGGGGGGG\nGGGGGGGGGGGGG\nGGGGGG\n0\n10\n20\n30\n40\n50\n60\n70\n2\n4\n6\n8\n10\n12\nIndex\nsize\n(b) The sizes of samples of losing and\nﬁnding events.\nThe x-axis shows the\nsizes and the y-axis shows the number\nof test-subjects.\nFigure 9: Measurements in the community DEAC-Hackers.\nThe arithmetic\nmean of the ﬁnal results of DEAC-Hackers participants is 3.71036. It is sur-\nprisingly lower than expected if compared to the value 4.95345 of the examined\nprogramming community. The mean of the number of boxes at the moment\nwhen the benchmark ends is 49. The anonymized data can be found at http:\/\/\nsmartcity.inf.unideb.hu\/~norbi\/BrainBSeries6\/measurements\/DEACH\/.\n17\nIn order to further strengthen the completion of our benchmark test in a\ncompetitive way in the following versions we are going to oﬀer to test subjects a\nlittle more liberty of ﬁne-tuning the settings. The ﬁne-tuning of mouse settings\nwas already mentioned earlier. A further possibility is to allow using custom\ncolors.\nThe next research objective will be to verify the satisfaction of Hick’s law.\nTo achieve this goal it is simple enough to compare the complexity of ﬁnding\nand losing events with the time diﬀerences of these. Unfortunately, the actual\nversion of the BrainB benchmark do not record these timestamps. The BrainB\nSeries 7 will contain this feature.\nOur long-term research goal is to further\ndevelop our benchmark to a standard psychological test that can be used for\ntalent search in esport.\n4\nAcknowledgement\nThanks to the students of the BSc course titled “High Level Programming\nLanguages” at the University of Debrecen, to the members of the NEMES-\nPOR mailing lists https:\/\/groups.google.com\/forum\/#!members\/nemespor,\nto the members of the UDPROG Facebook community https:\/\/www.facebook.\ncom\/groups\/udprog and to the members of the DEAC-Hackers esport depart-\nment http:\/\/deac.hu\/szakosztalyok\/esport for their interest and for per-\nforming the BrainB Test Series 6. Special thanks to Renátó Besenczi for pretest-\ning the Windows release of the BrainB program, to Roland Paszerbovics, Gergő\nHajzer and Péter Rozsos for their interest and support and to Nándor Benjámin\nBátfai for performing the BrainB Test Series 6 on several occasions.\nFinally, our thanks go to Dr. Péter Jeszenszky for reading the manuscript\nand suggesting improvements.\nAuthor contributions were the following: N. B.12 conceived the idea, devel-\noped the benchmark program, collected the data from the UDPROG community\nand analyzed the measurements. D. P.13 wrote the section “Psychological Back-\nground”. G. B.12 wrote the section “Choosing colors”. R. B.12 wrote the section\n“ Informatics Background” and collected the data from the DEAC-Hackers. D.\nV.12 wrote the section “Losing the Character”. All authors edited and reviewed\nthe ﬁnal version of the manuscript.\nReferences\n[ABR+13]\nJoaquin A Anguera, Jacqueline Boccanfuso, James L Rintoul, Omar\nAl-Hashimi, Farhoud Faraji, Jacqueline Janowich, Eric Kong, Yudy\nLarraburo, Christine Rolle, Eric Johnston, et al. Video game train-\ning enhances cognitive control in older adults. Nature, 501(7465):97,\n2013.\n[AWRG76] Herman A. Witkin and Donald R. Goodenough. Field dependence\nrevisited. pages i–85, 1976.\n12This work was supported by the construction EFOP-3.6.3-VEKOP-16-2017-00002. The\nproject was co-ﬁnanced by the Hungarian Government and the European Social Fund.\n13The work\/publication is supported by the GINOP-2.3.2-15-2016-00062 project.\n18\n[B´17]\nNorbert Bátfai.\nesport-talent-search.\nGitHub repository, 2017.\nhttps:\/\/github.com\/nbatfai\/esport-talent-search\n(visited:\n2018-09-30).\n[BAM+18] Benoit Bediou, Deanne M Adams, Richard E Mayer, Elizabeth Tip-\nton, C Shawn Green, and Daphne Bavelier. Meta-analysis of action\nvideo game impact on perceptual, attentional, and cognitive skills.\nPsychological bulletin, 144(1):77, 2018.\n[BBP+18]\nNorbert Bátfai, Gergő Bogacsovics, Roland Paszerbovics, Asztrik\nAntal,\nIstván\nCzevár,\nViktor\nKelemen,\nand\nRenátó\nBes-\nenczi.\nE-sportolók mérése (Measuring Esport Athletes).\nIn-\nformációs Társadalom,\n18(1):146–155,\n2018.\nOriginal docu-\nment in Hungarian http:\/\/real.mtak.hu\/79216\/1\/it_2018_1_\n10_batfai_et_al.pdf (visited: 2018-09-30).\n[CGR07]\nV. Csépe, M. Győri, and A. Ragó. Általános pszichológia 1. - Ész-\nlelés és ﬁgyelem. Osiris Kiadó, 2007.\n[GIF+15]\nJason Geyer, Philip Insel, Faraz Farzin, Daniel Sternberg, Joseph L.\nHardy, Michael Scanlon, Dan Mungas, Joel Kramer, R. Scott\nMackin, and Michael W. Weiner. Evidence for age-associated cog-\nnitive decline from internet game scores. Alzheimer’s & Dementia:\nDiagnosis, Assessment & Disease Monitoring, 1(2):260–267, 2015.\n[HPR+18]\nBruce D. Homer, Jan L. Plass, Charles Raﬀaele, Teresa M. Ober,\nand Alisha Ali.\nImproving high school students’ executive func-\ntions through digital game play. Computers & Education, 117:50–\n58, 2018.\n[Mac48]\nN. H. Mackworth. The breakdown of vigilance during prolonged vi-\nsual search. Quarterly Journal of Experimental Psychology, 1(1):6–\n21, 1948.\n[MSH+17] M. Moisala, V. Salmela, L. Hietajärvi, S. Carlson, V. Vuontela,\nK. Lonka, K. Hakkarainen, K. Salmela-Aro, and K. Alho. Gaming is\nrelated to enhanced working memory performance and task-related\ncortical activity. Brain Research, 1655:204–215, 2017.\n[Nie17]\nR. Nieva. Facebook’s moonshots: Making brains type and skin hear,\n2017.\nhttps:\/\/www.cnet.com\/news\/facebook-f8-building-8-\nmoonshot-projects-zuckerberg-regina-dugan\/ (visited: 2018-\n07-31).\n[PHC15a]\nBéla Pataki, Peter Hanák, and Gábor Csukly. Computer games for\nolder adults beyond entertainment and training: Possible tools for\nearly warnings-concept and proof of concept. In ICT4AgeingWell,\npages 285–294, 2015.\n[PHC15b]\nBéla Pataki, Péter Hanák, and Gábor Csukly. Surpassing enter-\ntainment with computer games: Online tools for early warnings of\nmild cognitive impairment. In Markus Helfert, Andreas Holzinger,\nMartina Zieﬂe, Ana Fred, John O’Donoghue, and Carsten Röcker,\n19\neditors, Information and Communication Technologies for Ageing\nWell and e-Health, pages 217–237. Springer International Publish-\ning, 2015.\n[Pow86]\nDonald E. Powers. Test preparation for the gre analytical ability\nmeasure: Diﬀerential eﬀects for subgroups of gre test takers. 1986.\n[PS84]\nD. E. Powers and S. S. Swinton. Eﬀects of self-study for coachable\ntest item types. Journal of Educational Psychology, 76(2):266–278,\n1984.\n[RSA06]\nNagybányai N. O. Rózsa S. and Oláh A.\nA pszichológiai mérés\nalapjai: Elmélet, módszer és gyakorlati alkalmazás. Bölcsész Kon-\nzorcium, 2006.\n[Sag12]\nCarl Sagan.\nDragons of Eden: Speculations on the Evolution of\nHuman Intelligence. Random House Publishing Group, 2012.\n[Seo05]\nSteven C. Seow. Information theoretic models of hci: A comparison\nof the hick-hyman law and ﬁtts’ law. Human–Computer Interaction,\n20(3):315–352, 2005.\n[TP13]\nE. Toulouse and H. Piéron.\nToulouse-Piéron-Revisado Prueba\nperceptiva y de atención.\nTEA, 8 edition, 2013.\nhttp:\/\/www.\nweb.teaediciones.com\/Ejemplos\/Extracto_libro_TP-R.pdf\n(visited: 2018-07-30).\n[WLH+54] H. A. Witkin, H. B. Lewis, M. Hertzman, K. Machover, P. Meiss-\nner, and S. Bretnall Wapner. Personality through perception: an\nexperimental and clinical study. Harper, Oxford, England, 1954.\n[YD08]\nRobert M. Yerkes and John D. Dodson. The relation of strength\nof stimulus to rapidity of habit-formation. Journal of Comparative\nNeurology and Psychology, 18:459–482, 1908.\n20\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Benchmarking Cognitive Abilities of the Brain with Computer Games.pdf"}
{"title":"Game-theoretic approach to risk-sensitive benchmarked asset management","authors":"Amogh Deshpande, Saul D. Jacka","summary":"In this article we consider a game theoretic approach to the Risk-Sensitive\nBenchmarked Asset Management problem (RSBAM) of Davis and Lleo \\cite{DL}. In\nparticular, we consider a stochastic differential game between two players,\nnamely, the investor who has a power utility while the second player represents\nthe market which tries to minimize the expected payoff of the investor. The\nmarket does this by modulating a stochastic benchmark that the investor needs\nto outperform. We obtain an explicit expression for the optimal pair of\nstrategies as for both the players.","url":"http:\/\/arxiv.org\/abs\/1503.01802v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1503.01802v1","published":1425593366000,"comment":"Forthcoming in Risk and Decision Analysis. arXiv admin note: text\n  overlap with arXiv:0905.4740 by other authors","pdf_text":"arXiv:1503.01802v1  [math.OC]  5 Mar 2015\nGame-theoretic approach to risk-sensitive benchmarked asset\nmanagement.\nAmogh Deshpande* and Saul Jacka**\n*Institute for Financial and Actuarial Mathematics, Department of Mathematical Sciences,\nUniversity of Liverpool and Department of Statistics, University of Warwick, UK. Email:\naddeshpa@gmail.com\n**Department of Statistics, University of Warwick ,UK. Email: S.D.Jacka@warwick.ac.uk\nAbstract\nIn this article we consider a game theoretic approach to the Risk-Sensitive Benchmarked Asset Manage-\nment problem (RSBAM) of Davis and Lleo [6]. In particular, we consider a stochastic diﬀerential game\nbetween two players, namely, the investor who has a power utility while the second player represents the\nmarket which tries to minimize the expected payoﬀof the investor. The market does this by modulating\na stochastic benchmark that the investor needs to outperform. We obtain an explicit expression for the\noptimal pair of strategies as for both the players.\nKey Words: Risk- Sensitive control , zero sum stochastic diﬀerential game.\n1\n1\nIntroduction\nIn this article we shall develop a game theoretic version of a continuous time optimization model\nwith risk-sensitive control approach more speciﬁcally termed as Risk-sensitive control portfolio optimization\n(RSCPO). The RSCPO balances an investor’s interest in maximizing the expected growth rate of wealth\nagainst his aversion to risk due to deviations of the realized rate from the expectation. The subjective notion\nof investor’s risk aversion is parameterized by a single variable, say θ. More formally, we write the ﬁnite\nhorizon risk-sensitive optimization criterion as : Maximize,\nJT,h := −1\nθ log E[e−θF (T,h)]\nwhere F(T, h) is the time-T value reward function corresponding to control h. In the optimal investment\nproblem we take F(T, h) = log V (T ) where V (t) is the time t-value of the portfolio corresponding to portfolio\nasset allocation h.\nAn asymptotic expansion around θ = 0 for the above criterion yields\nJT,h = E[F(T, h)] −θ\n2V ar(F(T, h)) + O(θ2)\nFrom this expression it is clear this criterion compromises between maximizing the portfolio return while\npenalizing the riskiness . The optimal expected utility function depends on θ and is a generalization of\nthe traditional stochastic control approach to utility optimization in the sense that now the degree of risk\naversion of the investor is explicitly parameterized through θ rather than importing it in the problem via\nan exogenous utility function. Values of θ > 0 correspond to a risk-averse investor, θ < 0 to a risk-seeking\ninvestor and θ = 0 to a risk-neutral investor who maximizes\nJT,h := E[F(T, h)]\nThere has been a substantial amount of research on the inﬁnite-time horizon ergodic problem:\nmax\n¯J∞where\n¯J∞= lim inf\nt→∞−1\nθt−1 log E[e−θF (t,h)]\nThough these type of problems are interesting in their own right, they are not readily applicable to practical\nasset management because of non-uniqueness of optimal controls.\n2\nIn the past decade, applications of risk-sensitive control to asset management have proliferated. Risk-\nsensitive control was ﬁrst applied to solve ﬁnancial problems by Lefebvre and Montulet [12] in a corporate\nﬁnance context. Fleming [8] was the ﬁrst to show that some investment optimization models could be re-\nformulated as risk-sensitive control problems. Bielecki and Pliska [2] considered a model with n securities\nand m economic factors with no transaction cost. They were the ﬁrst to apply continuous-time risk-sensitive\ncontrol as a practical tool that could be used to solve “real-world” portfolio selection problems. They con-\nsidered a long-term asset allocation problem and proposed the logarithm of the investor’s wealth as a reward\nfunction, so that the investor’s objective is to maximize the risk-sensitive (log) return of his\/her portfolio.\nThey derived the optimal control and solved the associated Hamilton-Jacobi-Bellman (HJB) PDE under the\nrestrictive assumption that the securities and economic factors have independent noise. In [3], Bielecki and\nPliska went on to study the economic properties of the risk-sensitive asset management criterion and then\nextended the asset management model into an intertemporal CAPM in [4]. Fleming and Sheu [7] analyzed an\ninvestment model similar to that of Bielecki and Pliska [2]. In their model, however, the factor process and\nthe security price process were assumed correlated. A major contribution was made by Kuroda and Nagai\n[11] who introduced an elegant solution method based on a change of measure argument which transforms the\nrisk sensitive control problem into a linear exponential of a quadratic regulator. They solved the associated\nHJB PDE over a ﬁnite time horizon and then studied the properties of the ergodic HJB PDE related to ¯J∞.\nRecently, Davis and Lleo [6] applied this change of measure technique to solve, for both the ﬁnite and an\ninﬁnite horizon, a risk-sensitive benchmark investment problem (RSBAM) in which an investor selects an\nasset allocation to outperform a given ﬁnancial benchmark. In the Kuroda and Nagai set-up θ represents the\nsensitivity of an investor to total risk, whereas in the RSBAM, θ represents the investors sensitivity to active\nrisk i.e. additional risk the investor is willing to take in order to outperform the benchmark. It is obvious\nthat for outperforming a stochastic benchmark, an investor will have to modify his or her optimal trading\nstrategy. Then the question of interest to us is: “What is the investor’s worst case strategy for an opposing\nstochastic benchmark”?. In particular, one can even take the jaundiced point of view that the benchmark\nwill be set retrospective to the worst case. For example, if a portfolio fund manager outperforms the set\nbenchmark, the principal may remark this out-performance either as best achieved or poorly achieved with\nrespect to the underlying worst-case scenario. So, in this article we consider a game-theoretic version of the\nproblem within the benchmark framework of Davis and Lleo [6]. In it, we consider a stochastic diﬀerential\ngame between two players, namely, the investor (who has a power utility) and a second player, representing\nthe market, who tries to minimize the expected payoﬀof the investor. We explicitly characterize the optimal\nallocation of assets and the optimal choice of benchmark index.\nIn this article, we consider the benchmark process ex-ante that evolves according to a controlled\n3\ndiﬀusion process. We contrast this approach to the one of Heath and Platen [10]. In their methodology,\nthey use the growth optimal portfolio itself as a benchmark which is closer to the concept of the numeraire\nportfolio. Although there has been a long history of applying risk-sensitive optimal control to problems in\nﬁnance, a game-theoretic version of such problems in ﬁnite horizon is missing from the literature. We intend\nto elaborate further on this now.\nIn the next section we brieﬂy describe the framework of the risk-sensitive zero sum stochastic diﬀer-\nential game corresponding to the desired game (P1)( refer 2.8a). In the third section we reformulate the\nobjective criterion under evaluation as a linear exponential of quadratic regulator problem (P2) (refer 3.11).\nIn the fourth section we provide a veriﬁcation lemma that will help us solve this game problem. In the ﬁfth\nsection we derive the optimal controls and obtain an explicit expression for the associated value of the game.\nThe article as usual concludes with remarks and pointers to future direction of work.\nBroadly speaking our aim is to derive the saddle-point equilibrium pair for the game (P1). To achieve\nthis, we ﬁrst obtain saddle point strategy for the game (P2). We then show that the saddle point equilibrium\nfor (P2) is also saddle point equilibrium for (P1).\n2\nRisk-sensitive zero sum stochastic diﬀerential game\nWe consider a market consisting of m + 1 ≥2 securities with n ≥1 factors. We assume that the set of\nsecurities includes one bond whose price is governed by the ODE\ndS0\nt = rtS0\nt dt,\nS0\n0 = s0\n(2.1)\nwhere rt is a deterministic function of t. The other security prices and factors are assumed to satisfy the\nfollowing SDE’s\ndSi\nt = Si\nt{(a + AXt)idt +\nn+m\nX\nk=1\nσi\nkdW k\nt },\nSi\n0 = si, i = 1, ..., m,\n(2.2)\nwhere the factor process Xt satisﬁes,\ndXt = {(b + BXt)dt + ΛdWt}, X0 = x ∈Rn\n(2.3)\nHere Wt = (Wt)k=1,...,n+m is an n+m dimensional standard Brownian motion deﬁned on a ﬁltered probability\nspace (Ω, F, P, Ft).\n4\nThe factor process can represent macro-economic indicators such as GDP, inﬂation and market index\ndata. The stock price dynamics are modulated by the factor process. Hence one can incorporate the eﬀect\nof macro-economic indicators into the investment optimization problem by using the stock price process\nmodulated by the factor process Xt.\nThe model parameters A, B, Λ are respectively m×n, n×n, n×(m+n) constant matrices and a ∈Rm,\nb ∈Rn. The constant matrix (σi\nk){i=1,2....,m;k=1,2,...,(n+m)} will be denoted by Σ in what follows.\nIn Kuroda and Nagai [11] it is assumed that the factor process and the stock price process do not\nhave independent noise i.e. ΣΛ\n′ ̸= 0. This assumption is in sharp contrast to Bielecki and Pliska [2] who\nconversely assume that ΣΛ\n′ = 0. We will assume that ΣΛ\n′ ̸= 0.\nLet Gt = σ(Su, Xu, Lγ\nu; u ≤t) be the sigma-ﬁeld generated by the underlying stock price process,\nfactor process and benchmark process Lγ to be deﬁned later up to time t. The investment strategy which\nrepresents the proportional allocation of total wealth in the ith security Si\nt is denoted by hi\nt for i = 1, ..., m.\nStrategy (h0\nt, ht)0≤t≤T is said to be an investment strategy up to time T . We set S\n′\nt := (S1\nt , S2\nt , ..., Sm\nt )\n′, h\n′\nt :=\n(h1\nt, ..., hm\nt )\n′. The space of controls H(T ) consists of Rm-valued controls for the investor as follows: H(T ) is\nthe set of {B[0, T ] ⊗Gt}{t≥0}-progressively measurable stochastic processes such that Pm\ni=1 hi\nt + h0\nt = 1 and\nwhere P(\nR T\n0 |hs|2ds < ∞) = 1 ∀T < ∞and E[e\nR T\n0 θ2h\n′\nsΣΣ\n′ hsds]\n1\n2 < ∞.\nFor given h ∈H(T ), the process Vt = V h\nt represents the investor’s wealth at time t, under the control\nh, and satisﬁes the following SDE dynamics,\ndV h\nt\nV h\nt\n=\n(rt + h\n′\nt((a + AXt) −rt1))dt + h\n′\ntΣdWt; V h\n0 = v\nwhich can be rewritten as,\ndV h\nt\nV h\nt\n=\n(rt + h\n′\ntdt)dt + h\n′\ntΣdWt; V h\n0 = v\n(2.4)\nwhere dt ≜a + AXt −rt1. From equation (2.4) it can be seen that if a + AXt = rt1 i.e. dt = 0, then the\nportfolio wealth process evolves with drift equal to the riskless interest rate rt. We make an assumption\nhere that the securities price volatility matrix Σ is a full rank matrix. If it is not full-rank then h\n′Σ = 0 for\nsome h ̸= 0. Hence the market contains redundant asset(s) and the portfolio value process V h\nt will grow at\na rate diﬀerent than the risk-less interest rate rt when h\n′d ̸= 0 resulting in an arbitrage. This is the case if\nthe portfolio contains two or more redundant assets for example a stock and an option on the same stock.\nHence we remove redundancy till the resultant matrix Σ is of full rank thereby ensuring that there exist no\nfurther possibility of arbitrage by trading in the resultant portfolio. In our benchmark model we express the\nobjective through a new optimization criterion corresponding to a reward function F which represents the\n5\nlog excess return of the asset portfolio over its benchmark and is given as\nF(t; h, γ) = log V h\nt\nLγ\nt\nF(0; h, γ) = log f\nWe now formally state the Risk-sensitive Benchmarked Asset management problem (RSBAM) that we solve.\nProblem : Risk-sensitive Benchmarked Asset Management (RSBAM)\nWe ﬁrst deﬁne the objective criterion J as,\nJ(f, x, h, γ; T )\n≜\n−2\nθ log E[exp [−θ\n2 F(T, h, γ)]]\n=\n−2\nθ log E[\n\u0012V h\nT\nLγ\nT\n\u0013−θ\/2\n]\n=\n−2\nθ log E[U(V h\nT\nLγ\nT\n)]\n(2.5)\nwhere the utility function U(·) is U : x →x−θ\n2 . The dynamics of the benchmark process is a diﬀusion process\nLγ modulated by a (Markovian) control γ given by\ndLγ\nt\nLγ\nt\n= (αt + βtXt)dt + γ\n′\ntdWt\n(2.6)\nwhere αt ∈R and β ∈R1×n. The space of controls Γ(T ) consists of the market control represented by γ that\nis Rn+m-valued. Γ(T ) consists of progressively measurable controls measurable w.r.t to {B[0, T ] ⊗Gt}t≥0\nand where P(\nR T\n0 |γs|2ds < ∞) = 1 ∀T < ∞and E[eθ2 R T\n0 γ\n′\nsγsds]\n1\n2 < ∞.\nBy a simple application of Ito’s formula we have:\ndF(t, h, γ) = d log(V h\nt\nLγ\nt\n)\n=\nF(t, h, γ){[rt + ht\n′(a + AXt −rt1) −(αt + βtXt) −1\n2h\n′\ntΣΣ\n′ht + 1\n2γ\n′\ntγt]dt\n+\n(h\n′\ntΣ −γ\n′\nt)dWt}\n(2.7)\nWe are now in a position to formally state the game-theoretic version of the game. For a given θ > 0,\nwe consider a stochastic diﬀerential game between two players, namely, the investor (who has a power utility)\nU and who modulates the payoﬀfor given γ ∈Γ(T ) via control h ∈H(T ). On the other hand the second\nplayer, say the market, behaves antagonistically to the investor by setting a benchmark for the investor to\noutperform by modulating the control γ for a given control h. This can be conceptualized as a risk-sensitive\nzero sum stochastic diﬀerential game between the investor on one side and the market on the other and is\n6\nformalized as follows\nProblem (P1) Obtain ˆh ∈H(T ) and ˆγ ∈Γ(T ) such that,\nJ(f, x, ˆh, ˆγ; T ) =\nsup\nh∈H(T )\ninf\nγ∈Γ(T )\n−2\nθ log E[(V h\nT\nLγ\nT\n)−θ\n2 ] =\ninf\nγ∈Γ(T )\nsup\nh∈H(T )\n−2\nθ log E[(V h\nT\nLγ\nT\n)−θ\n2 ]\n(2.8a)\nThis can be construed as a game-theoretic version of the RSBAM problem.\nRemark 2.1:\nThe problem set up (P1) is an extension of Kuroda and Nagai [11] and Davis and Lleo [6]. However the\nformer does not consider the benchmarked version i.e. the benchmark index is identically one in [11] while\nin Davis and Lleo [6] though have a benchmarked portfolio criterion, they solve the one player optimization\nproblem and not the two player saddle point problem.\nIn light of the mathematical preliminaries just discussed, we formally elaborate the plan to solve the\nzero sum stochastic diﬀerential game (P1).\nStep 1\nWe reformulate the original objective criterion as a power utility function to an exponential of an\nintegral function.\nStep 2 Deﬁne a new path functional I(f, x, h, γ, t; T ) (refer equation (3.9)) related to the exponential of the\nintegral function. Deﬁne ¯u(t, x) to be the upper-value function while u(t, x) be the lower-value function for\nthe game associated with I. Denote the game related to this objective functional as (P2).\nStep 3 Deduce the HJBI PDE corresponding to game (P2)( refer (3.11).\nStep 4 Formulate the conditions that a candidate value function should satisfy for the game with regards to\nobjective function I to have a value. This constitutes the veriﬁcation lemma.\nStep 5 Solve the HJBI PDE derived in step 3 while obtaining the expression for optimal controls. This\noptimal control pair will constitute a saddle point equilibrium for (P2).\nThe candidate value function\nsatisfying all the conditions of the veriﬁcation lemma is our desired value function for (P2).\nStep 6 Reverting back to the original problem (P1), show using facts derived in Step 4, that the game with\nobjective criterion J now has a value as well, and is in fact u(0, x).\nIn the next section we reformulate the objective criterion and formalize our game problem.\n3\nProblem Reformulation\nStep 1\nWe will ﬁrst transform the utility optimization problem (2.5) into optimizing the exponential-of-integral\nperformance criterion.\nCriterion under the expectation\n7\nOur ﬁrst aim is to write the objective criterion J only in terms of the factor process. Towards that end we\ndeﬁne the function g(x, h, γ, r; θ) as follows:\ng(x, h, γ, r; θ)\n=\n1\n2(θ\n2 + 1)h\n′ΣΣ\n′h −r −h\n′(a + Ax −r1) + (α + βx) −1\n2\nθ\n2(h\n′Σγ + γ\n′Σ\n′h)\n+\n1\n2(θ\n2 −1)γ\n′γ\n(3.1)\nFrom (2.7) and (3.1) we therefore have,\nd exp(−θ\n2 F(t; h, γ))\n=\nθ\n2\n\u0012\ng(Xt, ht, γt, rt; θ) −(h\n′\ntΣ −γ\n′\nt)ΣdWt\n\u0013\n−θ2\n8 (h\n′\ntΣ −γ\n′\nt)ΣΣ\n′(Σ\n′ht −γt)dt\n(3.2)\nThus we have,\nexp(−θ\n2 F(t; h, γ))\n=\nf −θ\/2 exp{θ\n2\nZ t\n0\ng(Xs, hs, γs, r; θ)ds\n−\nθ\n2\nZ t\n0\n(h\n′\nsΣ −γ\n′\ns)dWs −1\n2(θ\n2)\n2 Z t\n0\n(h\n′\nsΣ −γ\n′\ns)(h\n′\nsΣ −γ\n′\ns)\n′ds}\n(3.3)\nwhere V h\n0 = v, Lγ\n0 = l and f = V h\n0\nLγ\n0 = v\nl .\nChange of measure\nLet Ph,γ be the measure on (Ω, F) deﬁned by,\ndPh,γ\ndP |Ft\n=\n¯Xt,\n(3.4)\nwhere ¯Xt is given by\n¯Xt = E(θ\n2\nZ\n0\n(h\n′Σ −γ\n′)dW)t\n(3.5)\nand where E(·) denotes the Doleans-Dade or martingale exponential. From the assumption made on the\nspace of admissible controls H(T ) and Γ(T ) it is clear that the Kazamaki condition E[e\nR t\n0 θ\nh\n′\nsΣ−γ\n′\ns\n2\ndWs] < ∞\n∀t ∈[0, T ] is satisﬁed so that Ph,γ to be a probability measure. i.e.\nE[E(θ\n2\nZ\n0\n(h\n′Σ −γ\n′)dW)T ] = 1.\n(3.6)\n8\nWe note that,\nW h,γ\nt\n≜Wt + θ\n2\nZ t\n0\n(h\n′\nsΣ −γ\n′\ns)ds,\n(3.7)\nby Girsanov’s formula, is a standard Brownian motion under Ph,γ and the factor process Xt satisﬁes,\ndXt = (b + BXt −θ\n2(Σ\n′ht −γt))\n′dt + ΛdW h,γ\nt\n(3.8)\nStep 2\nThe HJB equation\nTaking expectation w.r.t to the physical measure P and multiplying both sides of equation (3.3) by\n−2\nθ\nfollowed by the change of measure argument of (3.4-3.5) one considers the new path functional I deﬁned as\nI(f, x, h, γ, t, T ) = log f −2\nθ log Eh,γ[exp {θ\n2\nZ T −t\n0\ng(Xs, hs, γs, rs+t; θ)ds}]\n(3.9)\nand then the upper-value function and lower-value function ¯u and u respectively for the game corresponding\nto the new path functional I are given by :\n¯u(t, x) =\nsup\nh∈H(T )\ninf\nγ∈Γ(T ) I(f, x, h, γ, t, T )\n(3.10a)\nu(t, x) =\ninf\nγ∈Γ(T )\nsup\nh∈H(T )\nI(f, x, h, γ, t, T )\n(3.10b)\nu(t, x) = ¯u(t, x) = u(t, x)\n(3.10c)\nIf a pair of controls satisfy (3.10c), then the game corresponding to the new path functional I has the\nvalue u and the pair of controls constitutes saddle point strategies for the game with regards to I. Let the\nexponentially transformed function ˜I be deﬁned as ˜I = exp(−θ\n2I) and ˜u(t, x) := exp(−θ\n2u(t, x)). We now\nconsider the problem of determining the saddle-point equilibrium for the game corresponding to the new\npath functional ˜I. We call this problem (P2) and it is formally stated as follows:\nProblem P2 Obtain ˆh ∈H(T ) and ˆγ ∈Γ(T ) such that,\n˜u(t, x)\n=\ninf\nh∈H(T ) sup\nγ∈Γ(T )\n˜I(f, x, h, γ, t, T )\n=\nsup\nγ∈Γ(T )\ninf\nh∈H(T )\n˜I(f, x, h, γ, t, T )\n=\nE\nˆh,ˆγ[exp{θ\n2\nZ T −t\n0\ng(Xs, ˆhs, ˆγs, rs+t; θ)ds}f −θ\/2]\n(3.11)\nWe now provide a veriﬁcation lemma for this game. Let us ﬁrst deﬁne the process Y h,γ(t) by\ndY h,γ(t) =\n\u0012\ndt\ndXt\n\u0013\n=\n\u0012\ndt\n(b + BXt −θ\n2(h\n′\ntΣ −γ\n′\nt))dt + ΛdW h,γ\nt\n\u0013\n9\nLet y ≜(t, x).\nThe control process h(t) = h(t, ω) and γ(t) = γ(t, ω) for ω ∈Ωcan be assumed to be\nMarkovian. Let O = (0, T )× Rn. Then the process Y h,γ(t) is a Markov process whose generator ˜\nAh,γ acting\non a function ˜u(t, x) ∈C2\n0([0, T ] × Rn) is given by,\n˜\nAh,γ˜u(t, x)\n=\n∂˜u(t, x)\n∂t\n+ (b + Bx −θ\n2Λ(Σ\n′h −γ))\n′D˜u(t, x) + 1\n2tr(ΛΛ∗D2˜u(t, x))\n(3.12)\nin which D˜u(t, x) ≜( ∂˜u(t,x)\n∂x1 , ..., ∂˜u(t,x)\n∂xn )\n′ and D2˜u(t, x) is the matrix deﬁned by D2˜u(t, x) ≜[ ∂2 ˜u(t,x)\n∂xixj ], i, j =\n1, 2, ..., n.\nStep 3\nBy an application of the Feynman-Kac formula, it can be deduced from (3.11) that the HJBI PDE\nfor ˜u(t, x) is given by\n\u0012\n˜\nA\nˆh,ˆγ + θ\n2g(x, ˆh, ˆγ, r; θ)\n\u0013\n˜u(t, x) = 0\n(3.13)\nReversing the exponential transformation , dividing by −(θ\/2)˜u(t, x), we can deduce from (3.13) that the\nHJBI PDE for u(t, x) is given for h ∈Rm and γ ∈R(m+n) by\nA\nˆh,ˆγu(t, x) = 0\n(3.14)\nwhere the operator Ah,γ is given by,\nAh,γu(t, x)\n=\n∂u(t, x)\n∂t\n+ (b + Bx −θ\n2Λ(Σ\n′h −γ))\n′Du(t, x) + 1\n2tr(ΛΛ\n′D2u(t, x))\n−\nθ\n4(Du(t, x))\n′ΛΛ\n′Du(t, x) −g(x, h, γ, r; θ)\n(3.15)\nIn the next section we provide a veriﬁcation lemma for the game based on the criterion function I.\n4\nVeriﬁcation lemma for the game PII\nStep 4\nWe now provide a veriﬁcation lemma related to the game (PII).\nProposition 4.1.\nSuppose ˜w ∈C1,2(O) ∩C( ¯O) (is the space of twice diﬀerentiable functions on O with\nrespect to x, once continuously diﬀerentiable on O with respect to t and which are continuous on ¯O ). Suppose\nthere exists a (Markov) control ˆh(y), ˆγ(y) such that\n10\n1. ( ˜\nAh,ˆγ(y) + θ\n2g(x, h, ˆγ(y), r; θ))[( ˜w(y))] ≥0 ∀h ∈Rm;\n2. ( ˜\nAˆh(y),γ + θ\n2g(x, ˆh(y), γ, r; θ))[( ˜w(y))] ≤0 ∀γ ∈Rm+n;\n3. ( ˜\nAˆh(y),ˆγ(y) + θ\n2g(x, ˆh(y), ˆγ(y), r; θ))[( ˜w(y))] = 0 ∀y ∈O;\n4. ( ˜w(T, XT )) = f −θ\/2.\nDeﬁne,\n˜Z(s)\n=\n˜Z(s)(h, γ) = θ\n2\n\u001a Z s\n0\ng(Xτ, hτ, γτ, rt+τ; θ)dτ\n\u001b\n(4.1)\n5. Eh,γ[\nR T −t\n0\nD ˜w\n′(t + s, Xs)Λe ˜ZsdW h,γ\ns\n] = 0 ∀h ∈Rm, ∀γ ∈Rm+n\nNow, deﬁne for each y ∈O and h ∈H(T ) and γ ∈Γ(T ),\n˜I(f, x, h, γ, t, T )\n=\nexp(−θ\n2I(f, x, h, γ, t, T ))\n=\nEh,γ[exp{θ\n2\nZ T −t\n0\ng(Xs, hs, γs, rs+t; θ)ds}f −θ\/2],\nThen (ˆh(y), ˆγ(y)) is an optimal (Markov) control i.e.,\n˜w(0, x) = ˜u(0, x) = ˜I(f, x, ˆh, γ, 0, T)\n=\ninf\nh∈H(T ){ sup\nγ∈Γ(T )\n[˜I(f, x, h, γ, 0, T)]}\n=\nsup\nγ∈Γ(T )\n{\ninf\nh∈H(T )[˜I(f, x, h, γ, 0, T)]}\n=\nsup\nγ∈Γ(T )\n˜I(f, x, ˆh, γ, 0, T)\n=\ninf\nh∈H(T )\n˜I(f, x, h, ˆγ, 0, T) = ˜I(f, x, ˆh, ˆγ, 0, T)\nProof\nApply Ito’s formula to ˜w(s, Xs)e ˜Zs to obtain\nd( ˜w(t + s, Xs)e\n˜Zs)\n=\n\u0014\ne\n˜Zs( ˜\nAh,γ + θ\n2g(Xs, hs, γs, rs+t; θ))\n\u0015\n[( ˜w(t + s, Xs))]ds + e\n˜Zs(D ˜w(t + s, Xs))dW h,γ\ns\n˜w(T, XT −t)e\n˜ZT −t\n=\n˜w(t, x) +\nZ T −t\n0\n(( ˜\nAh,γ + θ\n2g(Xs, hs, γs, rs+t; θ)) ˜w(t + s, Xs))e\n˜Zsds\n+\nZ T −t\n0\n(D ˜w\n′(t + s, Xs)Λ)e\n˜ZsdW h,γ\ns\n(4.2)\nFrom condition(4) of statement of the Proposition, we have ˜w(T, XT ) = f −θ\/2. Taking expectation with\nrespect to Ph,γ , setting t = 0 and using conditions (1) and (5) of the Proposition we get\n11\nEh,Γ[ ˜w(T, XT )e\n˜ZT ] ≥˜w(0, x)\nSince this inequality is true for all h ∈H(T ) we have\ninf\nh∈H(T ) Eh,Γ[f −θ\/2e\n˜ZT ] ≥˜w(0, x)\nHence we have,\nsup\nγ∈Γ(T )\ninf\nh∈H(T ) Eh,γ[f −θ\/2e\n˜ZT ] ≥\ninf\nh∈H(T ) Eh,Γ[f −θ\/2e\n˜ZT ] ≥˜w(0, x)\n(4.3)\nSimilarly, setting t = 0 we get, using condition (2) of the Proposition, we get the following lower bound,\nE\nˆh,γ[ ˜w(T, XT)e\n˜ZT ] ≤˜w(0, x)\nSince this inequality is true for all γ ∈Γ(T ) we have\nsup\nγ∈Γ(T )\nE\nˆh,γ[f −θ\/2e\n˜ZT ] ≤˜w(0, x)\nHence we have,\ninf\nh∈H(T ) sup\nγ∈Γ(T )\nEh,γ[f −θ\/2e\n˜ZT ] ≤\nsup\nγ∈Γ(T )\nE\nˆh,γ[f −θ\/2e\n˜ZT ] ≤˜w(0, x)\n(4.4)\nAlso , setting t = 0 and using condition (3) of the Proposition and using the deﬁnition of ˜u in (3.11) we get,\nE\nˆh,ˆγ[ ˜w(T, XT)e\n˜ZT ]\n=\n˜w(0, x)\n=\nE\nˆh,ˆγ[exp{θ\n2\nZ T\n0\ng(Xs, ˆhs, ˆγs, rs+t; θ)ds}f −θ\/2]\n(4.5)\nIt is automaticaly true that\nsup\nγ∈Γ(T )\ninf\nh∈H(T ) Eh,γ[f −θ\/2e\n˜ZT ] ≤\ninf\nh∈H(T ) sup\nγ∈Γ(T )\nEh,γ[f −θ\/2e\n˜ZT ].\n(4.6)\n12\nConversely, from (4.3), (4.4) and (4.5) we have,\ninf\nh∈H(T ) sup\nγ∈Γ(T )\nEh,γ[f −θ\/2e\n˜ZT ]\n≤\n˜w(0, x) ≤\nsup\nγ∈Γ(T )\ninf\nh∈H(T ) Eh,γ[f −θ\/2e\n˜ZT ]\n(4.7)\nHence from (4.6) and (4.7) we have,\nsup\nγ∈Γ(T )\ninf\nh∈H(T ) Eh,γ[f −θ\/2e\n˜ZT ]\n=\ninf\nh∈H(T ) sup\nγ∈Γ(T )\nEh,γ[f −θ\/2e\n˜ZT ]\n=\n˜w(0, x) = E\nˆh,ˆγ[f −θ\/2e\n˜ZT ]\n(4.8)\nCorollary 4.2\nAdmissible(optimal) strategies for the exponentially transformed problem given by (3.11)\nare also admissible(optimal) for the problem (3.10c). Formally,\nu(0, x)\n=\nsup\nh∈H(T )\n{ inf\nγ∈Γ(T )[I(f, x, h, γ, 0, T)]}\n=\ninf\nγ∈Γ(T ){ sup\nh∈H(T )\n[I(f, x, h, γ, 0, T)]}\n=\ninf\nγ∈Γ(T ) I(f, x, ˆh, γ, 0, T)\n=\nsup\nh∈H(T )\nI(f, x, h, ˆγ, 0, T) = I(f, x, ˆh, ˆγ, 0, T)\nProof\nThe value function u and ˜u are related through the strictly monotone continuous transformation\n˜u(t, x) = exp(−θ\n2u(t, x)). Thus admissible (optimal) strategies for the exponentially transformed problem\nare also admissible(optimal) for the problem (3.10c).\n5\nSolving the risk-sensitive zero sum stochastic diﬀerential game\nStep 5\nWe seek to ﬁnd the value function u for the game deﬁned in (3.12). We guess a solution assuming that\nit belongs to the class C1,2((0, T )×Rn) and show that the guess satisﬁes all the conditions of our veriﬁcation\nlemma given by Proposition 4.1. Conditions (1)-(4) of the veriﬁcation lemma can be written in a compact\nform as\nsup\nh∈H(T )\ninf\nγ∈Γ(T ) Ah,γu(t, x) = 0;\nu(T, x) = log f\n(5.1)\n13\nMotivated by the results in Kuroda and Nagai [11], we will look for a u given by u(t, x) = 1\n2x\n′Qtx + q\n′\ntx + kt\nwhere Q is an n × n symmetric matrix, q ∈Rn and k is a scalar. Substituting this form in (3.15) we get\nAh,γu(t, x)\n=\n1\n2x\n′ dQt\ndt x + dqt\ndt\n′\nx + dkt\ndt +\n\u0012\nb + Bx −θ\n2Λ(Σ\n′ht −γ(t))\n\u0013′\n(Qtx + qt)\n+\n1\n2(ΛΛ\n′QtQ\n′\ntΛ\n′Λ) −θ\n4(Qtx + kt)\n′ΛΛ\n′(Qtx + kt)\n−\n1\n2(θ\n2 + 1)h\n′\ntΣΣ\n′ht + rt −(αt + βx) + h\n′\nt(a + Ax −rt1) + 1\n2\nθ\n2(h\n′\ntΣγ + γ\n′Σ\n′ht)\n−\n1\n2(θ\n2 −1)γ\n′\ntγt\n(5.2)\nRemark 5.1\nSince the game considered is for the risk-averse investor θ > 0. Moreover based in the\nexpression for ˆγ in (5.5), θ ̸= 2. This leaves for two possibilities: θ ∈(0, 2) or θ ∈(2, ∞). For the optimal\nstrategies (ˆh, ˆγ) to be a saddle-point equilibrium for the game, we would desire that the equation with the\nquadratic term in h be negative deﬁnite while the quadratic term in γ be positive deﬁnite. In fact for the\nchoice θ > 0, the quadratic term in h desirably is negative deﬁnite while for θ < 2, the quadratic term in γ\nis positive deﬁnite . Hence for our case the valid range of θ is between 0 and 2 and excludes the other two\npossibilities for the range of θ.\nWe now solve the ﬁrst order condition for ˆγ to minimize Aˆh,γu(t, x) over all γ ∈Rn+m:\n(2 −θ) ˆγt −θ(Σ\n′ˆht −ˆγ\n′)Du(t, x) = 0\n(5.3)\nThe ﬁrst order condition for ˆh that maximizes Ah,ˆγ(y)˜u(t, x) over all h ∈Rm in terms of u(t, x) is,\nˆht =\n2\n(θ + 2)(ΣΣ\n′)−1[dt + θ\n2Σˆγt −θ\n2ΣΛ\n′Du(t, x)]\n(5.4)\nSubstituting back ˆh obtained in (5.4) into (5.3) we get\nˆγt\n=\nθ\n2 −θ [Σ\n′ˆht −Λ\n′Du(t, x)]\n(5.5)\nThe optimal control ˆht is a global maximum while ˆγt is a global minimum for t ≤[0, T ]. We substitute ˆh\nfrom (5.4) and ˆγ from (5.5) in (5.1) to obtain\nA\nˆh,ˆγu(t, x) = 0;\nu(T, x) = log f\n(5.6)\nWe then group all the resulting quadratic terms in x, linear terms in x and constants together to conclude\nthat the choice of u(t, x) = 1\n2x\n′Qtx + q\n′\ntx + kt is indeed the solution to the HJBI PDE (5.1) provided that\n14\nQ, q and k satisfy the following system of diﬀerential equations:\n• a matrix Ricatti equation related to the coeﬃcient of the quadratic term and used to determine the\nsymmetric non-negative matrix Qt, given as\ndQt\ndt\n=\nQtK0Qt + K\n′\n1Qt + QtK1 + 2\n2 −θ\n(2 −θ2)2 A\n′(ΣΣ−1)\n−1A = 0\n0 ≤t ≤T,\nQT\n=\n0\n(5.7)\nwhere\nK0 =\n−θ2\n2(2−θ)ΛΛ\n′ +\n2θ2\n(2−θ)(2−θ2)2 ΛΣ\n′(ΣΣ\n′)\n−1ΣΛ\n′\nK1 = B −\n2θ\n(2−θ2)2 A\n′(ΣΣ\n′)\n−1ΣΛ\n′\n•\nThe following linear ordinary diﬀerential equation satisﬁed by the n element column vector q(t)\ndqt\ndt\n+\n(K\n′\n1 + QtK0)qt + Q\n′\ntb + (a −r(t)1)\n′(ΣΣ\n′)\n−1[\n−2θ\n(2 −θ2)2 ΣΛ\n′Q(t) + (2 −θ)\n(2 −θ2)2 A]\n−\nβt\nqT = 0\n(5.8)\n•\nThe following linear ordinary diﬀerential equation satisﬁed by the constant kt\ndkt\ndt + 1\n2tr(ΛΛ\n′Qt) + rt −αt −\n2θ\n(2 −θ2)2 (a −r(t)1)\n′(ΣΣ\n′)\n−1ΣΛ\n′q(t)\n+\n2 −θ\n(2 −θ2)2 (a −r(t)1)−1(ΣΣ\n′)\n−1(a −r(t)1) +\nθ2\n(2 −θ)(2 −θ2)2 q\n′(t)ΛΣ\n′(ΣΣ\n′)\n−1ΣΛ\n′q(t)\n−\nθ2\n4(2 −θ)q\n′(t)ΛΛ\n′q(t)\nkT\n=\nlog f\n(5.9)\nCondition 4 of Proposition 4.1 in terms of u imposes the terminal condition in (5.9).\nIf K0 is positive deﬁnite then a unique solution to the Riccati equation (5.7), Qt , exists for all t ≤T .\nThis property of positive deﬁniteness follows from interpretation of the solution Qt as the covariance matrix\nof observations from a Kalman ﬁlter used to estimate the state of a dynamical system (see Theorem 4.4.1 in\nDavis [5]) for details. The uniqueness property of Qt follows from the standard existence-uniqueness theorem\nfor ﬁrst order diﬀerential equations (see Proposition 4.4.2 in Davis [5]).\nIt remains to be seen if ˜u = exp(−θ\n2u) for the choice of u satisﬁes condition (5) of Proposition 4.1.\nProposition 5.2 Eh,γ[\nR T −t\n0\ne ˜Zs(D˜u\n′(t + s, Xs)Λ)dW h,γ\ns\n] = 0.\nProof From the deﬁnition of ˜u in (3.11), for any optimal control belonging to Γ(T ), the strategy ˆh ≡0\n15\nis sub-optimal, and hence will provide an upper bound on ˜u. Further for the zero-benchmark case namely,\nˆγ ≡0, we would obtain now an upper bound on ˜u\n˜u(t, x)\n=\ninf\nh∈H(T ) Eh,ˆγ[exp{θ\n2\nZ T −t\n0\ng(Xs, hs, ˆγs, rs+t; θ)ds}f −θ\/2]\n≤\nE0,ˆγ[exp{θ\n2\nZ T −t\n0\ng(Xs, 0, ˆγs, rs+t; θ)ds}f −θ\/2]\n∴˜u(t, x)\n≤\nE0,0[exp{θ\n2\nZ T −t\n0\ng(Xs, 0, 0, rs+t; θ)ds}f −θ\/2]\n=\nexp(−θ\n2\nZ T −t\n0\nrs+tds)f −θ\/2\nNow Q and q are solutions to the system of o.d.e, and hence are integrals of bounded functions . Hence\nQ and q are continuous functions of time t ∈[0, T ] and hence bounded on [0, T ].\nThe matrix Λ is a\nknown constant. From standard existence-uniqueness result of stochastic diﬀerential equation (refer Oksendal\n([14])) we have X ∈L2(Ω, F, Ph,γ). Hence from the upper bound on ˜u , Remark 3.1 and the fact that\nDu(t, Xt) = QtXt + qt is in L2(Ω, F, Ph,γ), we have that Eh,γ([D˜u Λe ˜Z, D˜u Λe ˜Z]t) < ∞∀t ∈[0, T ]. Hence\nwe have Eh,γ[\nR T −t\n0\nD˜u\n′(t + s, Xs)Λe ˜ZsdW h,γ\ns\n] = 0.\n.\nIt is clear that our guess for ˜u = exp(−θ\n2u) satisﬁes conditions (1)-(5) of Proposition 4.1. Hence our choice\nof ˜u indeed is the value of the game (P2) and controls ˆh, ˆγ are the saddle point equilibrium of this game.\nLemma 5.2 For the choice of space of controls H(T ) and Γ(T ), we have\nE[E\n\u0012\n−θ\n2\nZ\n0\n[(QtXt + qt)Λ + (h\n′\ntΣ −γ\n′\nt)]dWt\n\u0013\nT\n] = 1\n(5.10)\nProof: From the Kazamaki condition, refer (Oksendal [14]), (5.10) holds if\nE[exp(\nR t\n0 θ( (QsXs+qs)Λ+(h\n′\nsΣ−γ\n′\ns)\n2\n)dWs)] < ∞∀t ∈[0, T ]. Hence by application of Cauchy-Schwartz inequal-\nity we have,\nE[exp(\nZ t\n0\nθ((QsXs + qs)Λ + (h\n′\nsΣ −γ\n′\ns)\n2\n)dWs)]\n≤\n(E[e\nR t\n0 θ(QsXs+qs)ΛdWs])1\/2(E[e\nR t\n0 θ(h\n′\nsΣ−γ\n′\ns)dWs])\n1\/2\nHowever for E[e\nR t\n0 θ(QsXs+qs)ΛdWs] < ∞to hold , it is enough to show that the Novikov condition given by\nE[e\nR T\n0 θ2(QsXs+qs)ΛΛ\n′ (QsXs+qs)ds] < ∞hold; refer (Oksendal [14]). Since X is Gaussian process and Qt and\nqt are deterministic, (QtXt + qt)Λ is Gaussian and hence by completion of squares argument detailed in\nTheorem 5.3 below we have E[e\nR T\n0 θ2(QsXs+qs)ΛΛ\n′ (QsXs+qs)ds] < ∞holds and hence E[e\nR t\n0 θ(QsXs+qs)ΛdWs] <\n∞∀t ∈[0, T ] is validated. (E[e\nR t\n0 θ(h\n′\nsΣ−γ\n′\ns)dWs])\n1\/2\n< ∞is validated from similar application of Cauchy-\nSchwartz inequality followed by the assumption made earlier in the deﬁnition of the space of controls H(T )\n16\nand Γ(T ). Thus the Kazamaki condition holds and the conclusion follows.\nTheorem 5.3 If there exist a solution Q to (5.7), then the strategies (ˆh, ˆγ) deﬁned by\nˆht =\n2\n(θ + 2)(ΣΣ\n′)−1[dt + θ\n2Σγt −θ\n2ΣΛ\n′(QtXt + qt)]\n(5.11)\nˆγt =\nθ\n2 −θ [Σ\n′ˆht −Λ\n′(QtXt + qt)]\n(5.12)\nwhere q is a solution of (5.8) are admissible i.e. h ∈H(T ) and γ ∈Γ(T ) and are optimal for the ﬁnite\nhorizon game problem (P1), namely,\nu(0, x)\n=\nsup\nh∈H(T )\ninf\nγ∈Γ(T ) J(f, x, h, γ, T ; θ)\n=\ninf\nγ∈Γ(T )\nsup\nh∈H(T )\nJ(f, x, h, γ, T ; θ)\n=\ninf\nγ∈Γ(T ) J(f, x, ˆh, γ, T ; θ)\n=\nsup\nh∈H(T )\nJ(f, x, h, ˆγ, T ; θ)\n=\nJ(f, x, ˆh, ˆγ, T ; θ)\n=\n1\n2x\n′Q0x + q\n′\n0x + k0\nProof\nThe controls derived in section 5, (ˆh, ˆγ) forms the saddle point equilibrium for the (P2) game . We\naim to show that these controls are in fact admissible and optimal for the problem (P1) as well.\nProof of admissibility From the expression for ˆh and ˆγ in (5.11) and (5.12) respectively we note that\n−θ\n2\n\u0012\n(QtXt + qt)Λ + (ˆh\n′\ntΣ −ˆγ\n′\nt)\n\u0013\ncan be written linearly in Xt as X\n′\ntv1\nt + v2\nt where, constants v1\nt and\nv2\nt are given by,\nv1\nt\n=\n−θ\n2Q\n′(t)Λ + θ(θ −1)\n(2 −θ2) A\n′(ΣΣ\n′)−1ΣΛ\n′ + θ(θ −1)\n2 −θ2 Q\n′(t)ΛΣ\n′(ΣΣ\n′)−1(a −r1)\n−\n2θ2(θ −1)\n(2 −θ)(2 −θ2)Q\n′(t)ΛΣ\n′(ΣΣ\n′)−1ΣΛ\n′q(t) −\nθ2\n(2 −θ)Q\n′(t)ΛΛ\n′q(t).\nv2\nt\n=\n−θ\n2q\n′(t)Λ + θ(θ −1)\n(2 −θ2)(a −r1)\n′(ΣΣ\n′)−1ΣΛ\n′q(t)\n−\nθ2(θ −1)\n(2 −θ)(2 −θ2)q\n′(t)ΛΣ\n′(ΣΣ\n′)−1ΣΛ\n′q(t) −\nθ2\n(2 −θ)q\n′(t)ΛΛ\n′q(t)\nSince X satisﬁes the SDE , dXt = (b + BXt)dt + ΛdWt, so\nE|Xt| ≤E|X(0)| + |b|T + |B|\nR t\n0 E|Xs|ds.\nBy Gronwall’s inequality, therefore E|Xt| ≤(E|X(0)| + |b|T ) exp(|B|t) and Cov(Xt) = Λ\n′Λt. Let φ(t) ≜\nv1\nt Xt+v2\nt . We now explicitly calculate E[eδ|φt|2] for some δ > 0 since from Remark 2 in Lemma 2, of section 12\n17\n(Gihman and Skorokhod [9]) would imply that the Novikov’s condition holds true. Let Rt = e−BtXt + e−bt.\nHence dRt = e−BtΛdWt. Therefore Rt is a Gaussian process and hence φt is Gaussian process with drift.\nAlso µt = E[|φt|] ≤sup0≤t≤T |v1\nt |(E|X0| + |b|T ) exp(|B|t) + sup0≤t≤T |v2\nt | and ˜Σt = Cov(φt) ≤v1\n′\ntΛ\n′Λv1\nt .\nThus mean µt and co-variance ˜Σt are bounded above by t. We use the following completion of squares\nargument: 1\n2z\n′Az + b\n′z + c = 1\n2(z + A−1b)\n′A(z + A−1b) + c −1\n2b\n′A−1b .\nE[eδ|φt|2]\n=\nZ\nRn\n1\n2πn\/2|˜Σt ˜Σ\n′\nt|1\/2 eδ|φ|2\nt e−1\n2 (φ−µt)\n′(˜Σt ˜Σ\n′\nt)−1(φ−µt)dx1dx2...dxn\n=\n1\n2πn\/2|˜Σt ˜Σ\n′\nt|1\/2\nZ\nRn e\n−φ\n′\n(−2δI+(˜\nΣt ˜\nΣ\n′\nt)−1)\n−1\nφ+2µ\n′\n(t)(˜\nΣt ˜\nΣ\n′\nt)−1φ−µ\n′\nt(˜\nΣt ˜\nΣ\n′\nt)−1µt\n2\ndx1....dxn\n=\n|(˜Σ\n′\nt ˜Σt)|−1\/2\n|(−2δI + (˜Σt ˜Σ\n′\nt)−1)−1|\n−1\/2 ×\ne\n−µ\n′\nt(˜\nΣt ˜\nΣ\n′\nt)−1µt+4µ\n′\nt(˜\nΣt ˜\nΣ\n′\nt)−1(−2δI+(˜\nΣt ˜\nΣ\n′\nt)−1)\n−1\n(˜\nΣt ˜\nΣ\n′\nt)−1µt\n2\nMatrix (˜Σt ˜Σt)\n−1 is symmetric positive deﬁnite with lowest eigenvalue say λmin. Then it is easy to show that\nfor δ < λmin\n2\n, matrix (−2δI + (˜Σt ˜Σ\n′\nt)−1)−1 is positive deﬁnite . Along with the derived fact that µt and ˜Σt\nis bounded above by t ≤T , hence there exists some constant C such that E[eδ|φt|2] ≤C. Hence the optimal\ncontrols ˆh, ˆγ belong to their respective admissible class viz. H(T ) and Γ(T ) respectively.\nProof of optimality\nDeﬁne,\nZs\n=\nZs(h, γ) = θ\n2\n\u001a Z s\n0\ng(Xτ, hτ, γτ, rt+τ; θ)dτ −(h\n′\nτΣ −γ\n′\nτ)dWτ\n−\nθ\n4(h\n′\nτΣ −γ\n′\nτ)\n′\n(h\n′\nτΣ −γ\n′\nτ)dτ\n\u001b\n(5.13)\nAlso deﬁne, χ(t, x) = −θ\n2(u(t, x) −log f) and Lu(t, x) = 1\n2tr(ΛΛ\n′D2u(t, x)) + (b + Bx)\n′Du(t, x)\nHence, we have\ndχ(t + s, Xs)\n=\n−θ\n2(∂u\n∂t + Lu)(t + s, Xs)ds −θ\n2Du(t + s, Xs)\n′ΛdWs\nHence,\nd exp{χ(t + s, Xs)}\nexp{χ(t + s, Xs)}\n=\n−θ\n2(∂u\n∂t (t, x) + Lu)(t + s, Xs) −θ\n2Du(t + s, Xs)\n′ΛdWs\n+\nθ2\n8 Du\n′ΛΛ\n′Du(t + s, Xs)ds\n18\nand so,\nd exp{χ(t + s, Xs)} exp{Z(s)}\nexp{χ(t + s, Xs)} exp{Z(s)}\n=\n−θ\n2(∂u\n∂t (t, x) + Lu)(t + s, Xs) −θ\n2Du(t + s, Xs)\n′\nΛdWs\n+\nθ2\n8 Du\n′ΛΛ\n′Du(t + s, Xs)ds + θ\n2g(Xs, hs, γs, rs + t; θ)ds\n−\nθ\n2(h\n′(s)Σ −γ\n′(s))dWs + θ2\n4 (h\n′(s)Σ −γ\n′(s))Λ\n′Du(t + s, Xs)ds\nHence from (3.15), we have,\nexp{χ(T, X(T −t)) + Z(T −t)} = exp(χ(t, x)) exp\n\u0014 Z T −t\n0\n−θ\n2(Ah,γu(t + s, Xs))ds\n−\nZ T −t\n0\nθ\n2[Du(t + s, Xs)\n′Λ + (h\n′\ntΣ −γ\n′\nt)]dWt\n−\nZ T −t\n0\nθ2\n8 [Du(t + s, Xs)\n′ + (h\n′\ntΣ −γ\n′\nt)][Du(t + s, Xs)\n′ + (h\n′\ntΣ −γ\n′\nt)]\n′ds\n\u0015\n(5.14)\nWe have shown that u satisﬁes conditions (1)-(5) of Proposition 4.1 Hence from condition(4) of Proposition\n4.1, we have χ(T, x) = 0. Now setting t = 0 and taking condition (1) of Proposition 4.1 into account for\nγ = ˆγ, and for any h ∈ˆH(T ) we see from (5.14) that\n(V h\nT\nLγ\nT\n)\n−θ\/2\n≥e−θ\n2 u(0,x) exp\n\u0014\n−\nZ T\n0\nθ\n2[Du(s, Xs)\n′Λ + (h\n′\nsΣ −Γ\n′\ns)]dWs\n−\nZ T\n0\nθ2\n8 [Du(s, Xs)\n′ + (h\n′\nsΣ −γ\n′\ns)][Du(s, Xs)\n′ + (h\n′\nsΣ −Γ\n′\ns)]\n′ds\n\u0015\nNow by taking expectations w.r.t to the physical probability measure P on both sides of above equation and\nusing Lemma 5.2, we obtain\nJ(f, x, h, γ, T ) ≤u(0, x)\nThis inequality is true for all h ∈H(T ) so we have,\nsup\nh∈H(T )\nJ(f, x, h, γ, T ) ≤u(0, x)\n19\nHence we have,\ninf\nγ∈Γ(T )\nsup\nh∈H(T )\nJ(f, x, h, γ, T ) ≤\nsup\nh∈H(T )\nJ(f, x, h, γ, T ) ≤u(0, x)\n(5.15)\nLikewise, setting t = 0 and taking condition (2) of Proposition 4.1 into account for h = ˆh, and for any\nγ ∈Γ(T ) we see that\nJ(f, x, ˆh, γ, T ) ≥u(0, x)\nThis inequality is true for all h ∈H(T ) so:\ninf\nγ∈Γ(T ) J(f, x, ˆh, γ, T ) ≥u(0, x)\nHence we have,\nsup\nh∈H(T )\ninf\nγ∈Γ(T ) J(f, x, h, γ, T ) ≥\ninf\nγ∈Γ(T ) J(f, x, ˆh, γ, T ) ≥u(0, x)\n(5.16)\nHence from (5.15) and (5.16) we have,\nsup\nh∈H(T )\ninf\nγ∈Γ(T ) J(f, x, h, γ, T ) ≥u(0, x) ≥\ninf\nγ∈Γ(T )\nsup\nh∈H(T )\nJ(f, x, h, γ, T )\n(5.17)\nMoreover, setting t = 0 and taking condition (3) of Proposition 4.1 into account for h = ˆh, γ = ˆγ (such that\nˆh ∈H(T ) and ˆγ ∈Γ(T )) we see that\nJ(f, x, ˆh, ˆγ, T ) = u(0, x)\n(5.18)\nIt is always true that\nsup\nh∈H(T )\n( inf\nγ∈Γ(T ) J(f, x, h, γ, T )) ≤\ninf\nγ∈Γ(T )( sup\nh∈H(T )\nJ(f, x, h, γ, T ))\n(5.19)\nHence combining (5.17) and (5.19) we deduce the ﬁnal conclusion that the game (P1) has a value and is\nu(0, x).\n20\n6\nConclusion\nIn this article we provide a two player zero sum stochastic diﬀerential game in the context of the risk-sensitive\nbenchmark asset management problem. We obtain an explicit expression for the optimal strategies for both\nthe players. Future work could be directed towards considering a game theoretic benchmark problem with\ninﬁnite horizon risk sensitive criterion.\nReferences\n[1] Bensoussan, A. (1992) Stochastic control of Partially Observable Systems, Cambridge University Press.\n[2] Bielecki, T. and Pliska, S. R. (1999) Risk–sensitive dynamic asset management, Applied Mathematics\nand Optimization, 39, 337–360.\n[3] Bielecki, T. and Pliska, S. R.(2003) Economic properties of the risk sensitive criterion for portfolio\nmanagement. Review of Accounting and Finance, 2(2), 3-17.\n[4] Bielecki, T. and Pliska, S. R. (2004) Risk sensitive intertemporal CAPM with applications to ﬁxed income\nmanagement. IEEE Transactions on Automatic Control, 49(3), 420-432.\n[5] Davis, M.H.A. (1977) Linear Estimation and Stochastic Control(Chapman and Hall: London).\n[6] Davis, M.H.A and Lleo, S.(2008) Risk–sensitive benchmarked asset management, Quantitative Finance,\n8(4), 415–426.\n[7] Fleming, W. and Sheu, S.J. (2002) Risk–sensitive Control and an Optimal Investment Model II, Annals\nof Applied Probability, 12(2),730–767.\n[8] Fleming, W. (1995) Optimal investment models and risk sensitive stochastic control. In Mathematical\nFinance, IMA Volumes in Mathematics and its Applications, 65, 75–88, Springer-Verlag, New York.\n[9] Gihman, I.I. and Skorokhod,A. (1972) Stochastic Diﬀerential Equations, volume New-York. Springer-\nVerlag.\n[10] Heath, D. and Platen, E. (2006) A Benchmark Approach to Quantitative Finance Springer Finance.\n[11] Kuroda, M and Nagai,H. (2002) Risk–sensitive portfolio optimization on inﬁnite time horizon, Stochas-\ntics and Stochastic Reports, 73, 309–331.\n21\n[12] Lefebvre,M. and Montulet,P. (1994) Risk-sensitive optimal investment policy. International Journal of\nSystems Science, 22, 183-192.\n[13] Mataramvura, S. and ∅ksendal,B. (2008) Risk minimizing portfolios and HJBI equations for stochastic\ndiﬀerential games, Stochastics An International Journal of Probability and Stochastic Processes, 80(4),\n317–337.\n[14] ∅ksendal,B. (2003) Stochastic Diﬀerential Equations and Applications, 6th eds., Springer, New York.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Game-theoretic approach to risk-sensitive benchmarked asset management.pdf"}
{"title":"Benchmarks for Parity Games (extended version)","authors":"Jeroen J. A. Keiren","summary":"We propose a benchmark suite for parity games that includes all benchmarks\nthat have been used in the literature, and make it available online. We give an\noverview of the parity games, including a description of how they have been\ngenerated. We also describe structural properties of parity games, and using\nthese properties we show that our benchmarks are representative. With this work\nwe provide a starting point for further experimentation with parity games.","url":"http:\/\/arxiv.org\/abs\/1407.3121v2","pdf_url":"http:\/\/arxiv.org\/pdf\/1407.3121v2","published":1405079125000,"comment":"The corresponding tool and benchmarks are available from\n  https:\/\/github.com\/jkeiren\/paritygame-generator. This is an extended version\n  of the paper that has been accepted for FSEN 2015","pdf_text":"arXiv:1407.3121v2  [cs.LO]  16 Jan 2015\nBenchmarks for Parity Games\n(extended version)\nJeroen J.A. Keiren1,2\nj.j.a.keiren@vu.nl\n1 Theoretical Computer Science, VU University Amsterdam, The Netherlands\n2 Faculty of Management, Science & Technology, Open Universiteit, Heerlen, The\nNetherlands\nAbstract We propose a benchmark suite for parity games that includes\nthe benchmarks that have been used in the literature, and make it avail-\nable online. We give an overview of the parity games, including a de-\nscription of how they have been generated. We also describe structural\nproperties of parity games, and using these properties we show that our\nbenchmarks are representative. With this work we provide a starting\npoint for further experimentation with parity games.\n1\nIntroduction\nParity games (see, e.g., [24,55,79]) play an important role in model checking\nresearch. The µ-calculus model checking problem is polynomial time reducible\nto the problem of deciding the winner in parity games [73]. Other problems that\nare expressible in parity games are equivalence checking of labelled transition\nsystems [73], as well as synthesis, satisﬁability and validity of temporal logics\n[66].\nBesides their practical interest for veriﬁcation, solving (deciding the winner\nof) parity games is known to be in the complexity class NP ∩co −NP, and\nmore speciﬁcally in UP ∩co −UP [42]. Parity game solving is one of the few\nproblems in this complexity class that is not known to be in P, yet there is\nhope that a polynomial time algorithm exists. In recent years this has led to the\ndevelopment of (1) a large number of algorithms for solving parity games, such\nas [44,67,68], all of which were recently shown to be exponential, and (2) the\nstudy of (polynomial time) reduction techniques for parity games [30,47,21,22].\nSo far, practical evaluation of parity game algorithms has been based on ad-\nhoc benchmarks, mainly consisting of random games or synthetic benchmarks.\nFriedmann and Lange observed in 2009 [30] that no standard benchmark set for\nparity games was available. They introduced a small benchmark set in the con-\ntext of their comprehensive comparison of parity game solving algorithms and\ntheir related heuristics [30]. The set of benchmarks was extended in [47,21,22]\nusing model checking and equivalence checking cases. To the best of our know-\nledge, the situation has not improved since then, and the benchmarks in these\npapers still are the most comprehensive benchmarks included in a single paper.\nThe number of games and the diversity of parity games in each set in isolation\nare however limited. The lack of standard benchmarks makes it hard to compare\nthe diﬀerent tools and algorithms presented in the literature.\nTo improve the current situation, in this paper we propose a set of parity\ngames for benchmarking purposes that (1) is diverse, (2) contains games that\noriginate from diﬀerent veriﬁcation problems, and (3) includes those games that\nhave been used to experimentally evaluate algorithms in the literature.\nIn general, parity game examples in the literature can be classiﬁed as follows\n(we indicate their origins):\n1. Encodings of problems such as model checking, equivalence checking and\ncomplementation of B¨uchi automata to parity games [53,54,76,47,21,22,30].\n2. Synthetic parity games for which a certain solving algorithm requires expo-\nnential time [53,43,58,26,31,29,35].\n3. Random games [6,49,68,69,30,31].\nOur benchmarks include games from each of these categories.\nAdditionally, inspired by the properties for explicit state spaces in [60] we\nintroduce a set of structural properties for parity games, and in the spirit of\n[61,62] we analyse our benchmarks. Among others, we introduce a novel notion\nof alternation depth for parity games.\nThe structure of the paper is as follows. We ﬁrst introduce parity games\nand their structural properties in Section 2. Next we describe the benchmarks\n(Section 3) and the way in which they have been generated (Section 4). Finally\nwe illustrate diversity of our benchmarks with respect to the structural properties\nin Section 5. This paper is based on the PhD thesis of the author [45, Chapter 5].\nThe current paper is an extended version of the work in [46]. We plan to update\nthis version when new benchmarks are added, and we invite the community to\ncontribute benchmarks.\n2\nParity Games and Their Structural Properties\nA parity game is a two-player game played on a ﬁnite, directed graph by two\nplayers, even and odd, denoted\n□\nand □, respectively. We use ⃝∈{\n□\n, □} to\ndenote an arbitrary player. Formally, a parity game is a structure (V\n□\n, V□, →, Ω),\nwhere V\n□\nand V□are disjoint sets of vertices. We say that ⃝owns v if v ∈V⃝,\nwe write V for V\n□\n∪V□; →⊆V ×V provides the total edge relation—hence each\nvertex has a successor—and Ω: V →N assigns a non-negative integer priority\nto every vertex. The parity game is played by placing a token on some initial\nvertex, and then the players take turns moving the token: if the token is on a\nvertex v ∈V⃝then ⃝plays the token to one of the successors of v. This way, an\ninﬁnite play through the game is constructed. If the largest priority that occurs\ninﬁnitely often on this play is even (resp. odd) then\n□\n(resp. □) wins the play.\nThe time required for parity game solving and reduction algorithms depends\non the structure of the game. Typically the algorithmic complexity of parity\ngame algorithms are expressed in terms of the size of the game graph, i.e. the\n2\nnumber of vertices and edges, and the number of priorities in the game. Although\nother structural properties may not aﬀect the asymptotic running times of the\nalgorithms, in general they do aﬀect the actual running time. We therefore de-\nscribe a number structural properties that could be used for the further study\nof parity games.\nSizes. As basic parity game properties, we consider the numbers of vertices |V |,\n\f\fV\n□\n\f\f and |V□|, and the number of edges |→|. We write Ω(V ) for the set of\npriorities {Ω(v) | v ∈V }, and denote the number of priorities in the game by\n|Ω(V )|. The number of vertices with priority k is represented by\n\f\fΩ−1(k)\n\f\f. The\ncomplexity of most parity game algorithms is expressed in these quantities. For\nparity games in which either |V□| = 0 or\n\f\fV\n□\n\f\f = 0, special polynomial time\nsolving algorithms are available, see [30].\nDegrees. Typical structural properties in the graph are the in- and out-degrees of\nvertices, i.e., the number of incoming and outgoing edges of vertices. Formally, for\nvertex v ∈V , indeg(v) = |{u ∈V | u →v}|, outdeg(v) = |{w ∈V | v →w}|, and\ndeg(v) = |{w ∈V | v →w ∨w →v}| are the in-degree, out-degree and degree\nof v. We consider the minimum, maximum and average of these values.\nThe degrees of vertices might have an eﬀect on, e.g., algorithms that use\nlifting strategies to propagate information between vertices. Examples of such\nalgorithms are small progress measures [43] and the strategy improvement al-\ngorithm [68].\nStrongly Connected Components. The strongly connected components (SCCs) of\na graph are the maximal strongly connected subgraphs. More formally, a strongly\nconnected component is a maximal set C ⊆V for which, for all u, v ∈C, u →∗v,\ni.e., each vertex in C can reach every other vertex in C.\nThe strongly connected components in a graph induce a quotient graph. Let\nsccs(G) denote the strongly connected components of the graph. The quotient\ngraph is the graph (sccs(G), →′) and for C1, C2 ∈sccs(G), there is an edge C1 →′\nC2 if and only if C1 ̸= C2 and there exist u ∈C1 and v ∈C2 such that u →v.\nObserve that the quotient graph is a directed acyclic graph.\nWe say that an SCC C is trivial if |C| = 1 and C ̸→C, i.e., it only contains\none vertex and no edges, and we say that C is terminal if C ̸→′, i.e., its outdegree\nin the quotient graph is 0. The SCC quotient height of a graph is the length of\nthe longest path in the quotient graph.\nParity game algorithms and heuristics can beneﬁt from a decomposition into\nstrongly connected components (SCCs). One prominent example of this is the\nglobal parity game solving algorithm presented by Friedmann and Lange [30],\nfor which it was shown that SCC decomposition generally works well in practice.\nProperties of Search Strategies. Given some initial vertex v0 ∈V , breadth-ﬁrst\nsearch (BFS) and depth-ﬁrst search (DFS) are search strategies that can be used\nto systematically explore all vertices in the graph. The fundamental diﬀerence\nbetween BFS and DFS is that the BFS maintains a queue of vertices that still\n3\nneed to be processed, whereas the DFS maintains a stack of vertices. We record\nthe queue and stack sizes during the search.\nBreadth-ﬁrst search induces a natural notion of levels, where a vertex is at\nlevel k if it has least distance k to v0. The BFS height of a graph is k if k is\nthe maximal non-empty level of the BFS. For each level the number of vertices\nat that level is recorded. During a BFS, three kinds of edges can be detected,\nviz. edges that go to a vertex that was not yet seen, edges that go to a vertex\nthat was seen, but has not yet been processed (i.e., vertices in the queue) and\nedges that go back to a vertex on a previous level. This last type of edges is also\nreferred to as a back-level edge. Formally it is an edge u →v where the level of\nu, say ku is larger than the level of v, say kv. The length of a back-level edge\nu →v is ku −kv.\nGraph algorithms are typically based on a search strategy like BFS or DFS,\ngiven some initial vertex v0 ∈V . The characteristics of these search strategies\nare therefore likely to aﬀect the performance of such graph algorithms.\nDistances. The diameter of a graph is the maximal length of a shortest path\nbetween any pair of vertices. The girth is the length of the shortest cycle in the\ngraph. Both measures require solving the all-sources-shortest path problem with\nunit edge-weights, which is quadratic in the size of the graph.\nFor undirected graphs the diameter can be computed more eﬃciently using\nthe techniques from Takes and Koster [74]. For directed graphs, however, no\nmore eﬃcient algorithm is known.\nThe diameter and the girth characterise global properties of graphs. Intuit-\nively, they describe how hard it is to get from one vertex in the graph to another,\nor back to itself. A girth of 1 denotes that the graph contains a self-loop. We\nexpect to see this value quite often when analysing parity games due to the\noccurrence of vertices that are trivially won by one of the two players.\nLocal Structure. P´elanek also studied some local graph properties. A diamond\nrooted at a vertex u is a quadruple (u, v, v′, w) such that v ̸= v′, u →v, u →v′,\nv →w, and v′ →w. For parity games, we characterise two more speciﬁc classes\nof diamonds. A diamond (u, v, v′, w) is deﬁned to be even if P(u) = P(v) =\nP(v′) =\n□\n, and odd if P(u) = P(v) = P(v′) = □. These structures might prove\nto be interesting in the sense that from vertex u, P(u) has at least two strategies\nto play to w in two steps. The question is open whether these kinds of structures\ncan be used to improve parity game solving.\nThe k-neighbourhood of v is the set of vertices that can be reached from v\nin at most k steps (not counting v). The k-clustering coeﬃcient of v is the ratio\nof the number of edges and the number of vertices in the k-neighbourhood of v.\nThe k-neighbourhood can be thought of as a generalisation of the out-degree,\nexcept that we exclude a vertex from its own neighbourhood.\nWidth-measures on Graphs. Width-measures of graphs are based on cops-and-\nrobbers games [56,63], where diﬀerent measures are obtained by varying the rules\nof the game. For various measures, specialised algorithms are known that can\n4\nsolve games polynomially if their width is bounded. Most of the measures have\nan alternative characterisation using graph decompositions.\nThe classical width notion for undirected graphs is treewidth [64,11]. Intuit-\nively, the treewidth of a graph expresses how tree-like the graph is—the treewidth\nof a tree is 1. This corresponds to the idea that some problems are easier to solve\nfor trees, or graphs that are almost trees, than for arbitrary graphs. For direc-\nted graphs, the treewidth is deﬁned as the treewidth of the graph obtained by\nforgetting the direction of the edges. The complexity for solving parity games is\nbounded in the treewidth [57]; this means that, for parity games with a small,\nconstant treewidth, parity game solving is polynomial.\nTreewidth has been lifted to directed graphs in a number of diﬀerent ways.\nFor instance, Directed treewidth [41] is bounded by the treewidth [1]. DAG-\nwidth [7] describes how much a graph is like a directed acyclic graph. DAG-\nwidth bounds the directed tree width of a graph from above, and is at most\nthe treewidth. The Kelly-width [40] is yet another generalitation of treewidth to\ndirected graphs. If the Kelly-width of a graph is bounded, then also a bound\non its directed treewidth can be given, however, classes of directed graphs with\nbounded directed treewidth and unbounded Kelly-width exist. Entanglement\n[9,10] is a graph measure that aims to express how much the cycles in a graph\nare intertwined. If an undirected graph has bounded treewidth or bounded DAG-\nwith, then it also has bounded entanglement. Finally, clique-width [19] measures\nhow close a graph is to a complete bipartite graph. For every directed graph\nwith bounded treewidth an exponential upper bound on its clique-width can be\ngiven. Unlike the other width measures that we discussed clique-width does not\nhave a characterisation in terms of cops-and-robbers games.\nIf a parity game is bounded to a constant in any of the measures introduced\nabove, it can be solved in polynomial time.\nAlternation Depth. Typically, the complexity of parity game algorithms is ex-\npressed in the number of vertices, the number of edges, and the number of pri-\norities in the game. If we look at other veriﬁcation problems, such as µ-calculus\nmodel checking, or solving Boolean equation systems, the complexity is typic-\nally expressed in terms of the alternation depth. Diﬀerent versions of alternation\ndepth (with varying precision) have been coined, see [14]. Intuitively, the altern-\nation depth of a formula captures the number of alternations between diﬀerent\nﬁxed point symbols.\nWe aim to develop a characterisation of the ‘important’ alternations between\npriorities in parity games, inspired by the notion of alternation depth. We base\nour deﬁnition on the notion of alternation depth for modal equation systems as\nit was deﬁned by Cleaveland et al. [18].\nAnalogous to this deﬁnition, the alternation depth that we deﬁne comes in\ntwo stages. First we deﬁne the nesting depth of a strongly connected component\nwithin a parity game, next we deﬁne the alternation depth of the parity game\nas the maximum of the nesting depths of its strongly connected components.\nDeﬁnition 1. Let G = (V\n□\n, V□, →, Ω) be a parity game, and let sccs(G) be\nthe set of strongly connected components of G. Let C ∈sccs(G) be a strongly\n5\nconnected component. The nesting depth of vi in C is given by\nnd(vi, C)\n∆= max{1,\nmax{nd(vj, C) | vj →∗\nC,Ω(vi) vi, vj ̸= vi and Ω(vi) ≡2 Ω(vj)},\nmax{nd(vj, C) + 1 | vj →∗\nC,Ω(vi) vi and Ω(vi) ̸≡2 Ω(vj)}\n}\nwhere vj →C,k vi if vj →vi is an edge in the SCC C with Ω(vj) ≤k and\nΩ(vi) ≤k. Intuitively, the nesting depth of a vertex v counts the number of\nalternations between even and odd priorities on paths of descending priorities\nin the SCC of v. Note that this is well-deﬁned since we forbid paths between\nidentical nodes.\nThe nesting depth of an SCC C ∈sccs(G) is deﬁned as the maximum nesting\ndepth of any vertices in C, i.e., nd(C)\n∆= max{nd(v, C) | v ∈C}. The alternation\ndepth of a parity game is deﬁned as the maximal nesting depth of its SCCs.\nDeﬁnition 2. Let G = (V\n□\n, V□, →, Ω) be a parity game, and let sccs(G) be the\nset of strongly connected components of G. Then the alternation depth of G is\ndeﬁned as ad(G)\n∆= max{nd(C) | C ∈sccs(G)}.\nThere are reasonable translations of the µ-calculus model checking problem into\nparity games, such that the alternation depth of the resulting parity game is at\nmost the ﬁxed point alternation depth of the µ-calculus formula as described by\nEmerson and Lei [25], see [45, Proposition 5.4]. Note that the alternation depth\nof a game can be smaller than the number of priorities in the game, and could\nprovide an interesting alternative to the number of priorities in computing the\ncomplexity of parity game algorithms.\n3\nBenchmarks\nFor benchmarking parity game algorithms, it makes sense to distinguish three\nclasses of parity games, (1) the games that are the result of encoding a problem\ninto parity games, (2) games that represent hard cases for certain algorithms,\nand (3) random games. All three classes of games occur in the literature, and\nour benchmark set contains games from each of these classes. In the rest of this\nsection we discuss our benchmarks. In the next section we brieﬂy discuss these\ngames with respect to the properties described in Section 2.\n3.1\nEncodings\nA broad range of veriﬁcation problems can be encoded as a parity game. The\nmost prominent examples of these are the µ-calculus model checking problem—\ndoes a model satisfy a given property?—, equivalence checking problems—are\ntwo models equivalent?—, decision procedures—is a formula valid or satisﬁable?—\nand synthesis—given a property, give a model that satisﬁes the property.\n6\nModel Checking. The model checking problems we consider are mainly selected\nfrom the literature. All of the systems are encodings that, given a model L of\na system, and a property ϕ, encode the model checking problem L |= ϕ, i.e.,\ndoes L satisfy property ϕ. Most sensible encodings of model checking problems\ntypically lead to a low number of priorities, corresponding to the low alternation\ndepths of these properties. We verify fairness, liveness and safety properties.\nThis set includes, but is not limited to, the model checking problems described\nin [54,76,30,21,22].\nWe take a number of communication protocols from the literature, see, e.g.,\n[4,15,48,38]: two variations of the Alternating Bit Protocol (ABP), the Con-\ncurrent Alternating Bit Protocol (CABP), the Positive Acknowledgement with\nRetransmission Protocol (PAR), the Bounded Retransmission Protocol (BRP),\nthe Onebit sliding window protocol, and the Sliding Window Protocol (SWP).\nAll protocols are parameterised with the number of messages that can be sent,\nand the sliding window protocol is parameterised by the window size. For these\nprotocols a number of properties of varying complexity was considered, ranging\nfrom alternation free properties, e.g. deadlock freedom, to fairness properties.\nA Cache Coherence Protocol (CCP) [77] and a wait-free handshake register\n(Hesselink) [39] are considered. For the cache coherence protocol we consider\na number of properties from [59] and for the register we consider properties\nfrom [39]. Additionally we consider a leader election protocol for which we verify\nwhether it eventually stabilises.\nTo obtain parity games with a high degree of alternation between vertices\nowned by diﬀerent players we also consider a number of two-player board games,\nviz. Clobber [2], Domineering [34], Hex, see e.g. [5,52], Othello, also known as\nreversi, see e.g. [65], and Snake. For these games we check for each of the players\nwhether the player has a winning strategy starting from the initial conﬁguration\nof the game. The games are parameterised by their board size.\nAdditionally, we consider a number of industrial model checking problems.\nThe ﬁrst is a system for lifting trucks (Lift) [37], of which we consider both a\ncorrect and an incorrect version. We verify the liveness and safety properties\ndescribed in [37]. For the IEEE 1394 Link Layer Protocol (1394) we verify the\nproperties from [51]. We translated the ACTL properties from [71] to the µ-\ncalculus.\nFinally, we check the Elevator described by Friedmann and Lange, in a ver-\nsion in which requests are treated on a ﬁrst-in-ﬁrst-out basis (FIFO), and on\na last-in-ﬁrst-out basis (LIFO). We then check whether, globally, if the lift is\nrequested on the top ﬂoor, then it is eventually served. This holds for the FIFO\nversion, but does not hold for the LIFO version of the model. The elevator model\nis parameterised by the strategy and the number of ﬂoors. Furthermore we con-\nsider the parity games generated using an encoding of an LTS with a µ-calculus\nformula, as well as the direct encoding presented in [30]. In a similar way we\nconsider the Hanoi towers from [30] as well as our own version of this problem.\nEquivalence Checking. Given two processes L1, L2, the problem whether L1 ≡\nL2, for relations ≡, denoting that L1 and L2 are equivalent under some pro-\n7\ncess equivalence, can be encoded as a parity game [50,78]. We consider strong\nbisimulation, weak bisimulation, branching bisimulation and branching simula-\ntion equivalence in our benchmarks, using the approach described in [17]. The\nnumber of diﬀerent priorities in these parity games is limited to 2, but they do\ninclude alternations between vertices owned by diﬀerent players.\nHere we again use the speciﬁcations of the communication protocols that we\nalso used for model checking, i.e., two ABP versions, CABP, PAR, Onebit and\nSWP. In addition we include a model of a buﬀer. We vary the capacity of the\nbuﬀer, the number of messages that can be transmitted, and the window size in\nthe sliding window protocol. We compare each pair of protocols using all four\nequivalences, resulting in both positive and negative cases. These cases are a\nsuperset of the ones described in [21,22].\nIn addition, we include a comparison of the implementation of the wait-free\nhandshake register with a possible speciﬁcation. The implementation is trace\nequivalent to the speciﬁcation, but it is not equivalent with respect to the equi-\nvalences that we consider here.\nDecision Procedures. Parity games can also be obtained from decision procedures\nfor temporal logics such as LTL, CTL, CTL∗, PDL and the µ-calculus. Friedmann\net al. presented a decision procedure that is based on a combination of inﬁnite\ntableaux in which the existence of a tableau is coded as a parity game [33]. For\na given formula, it is checked whether it is (1) valid, i.e., whether the formula\nholds in all models, or (2) satisﬁable, i.e., whether the formula is satisﬁable in\nsome model.\nOur benchmark set includes a number of scalable satisﬁability and validity\nproblems that are provided as examples for the MLSolver tool [32]. In particular,\nwe include the benchmarks used in [32]: encoding that a deterministic parity con-\ndition is expressible as a nondeterministic B¨uchi condition, and nesting Kleene\nstars in diﬀerent logics. Additionally we consider formulas that involve encodings\nof a binary counter in various logics.\nSynthesis. Another problem that involves solving parity games is the LTL syn-\nthesis problem. Traditional synthesis approaches convert a formula into a non-\ndeterministic B¨uchi automaton, which is, in turn, transformed into a determ-\ninistic parity automaton using Safra’s construction [66]. Emptiness of this de-\nterministic parity automaton can then be checked using parity games with three\npriorities. Synthesis tools have been implemented that employ parity games in-\nternally, most notably GOAL [75] and Gist [16]. All synthesis tools that we are\naware of, however, are research quality tools, of which we have not been able to\nobtain working versions on current computing platforms. As a result, our bench-\nmark set currently does not include parity games obtained from the synthesis\nproblem. We plan to extend our benchmarks with such games, and update this\npaper accordingly.\n8\n3.2\nHard Games\nThe interesting complexity of solving parity games, and its link to the model\nchecking problem, have led to the conception of a large number of parity game\nsolving algorithms. For most of these algorithms it has long been an open prob-\nlem whether they have exponential lower bounds.\nWe consider the games described by Jurdzi´nski that shows the exponential\nlower bound for small progress measures [43], the ladder games described by\nFriedmann [28] defeating strategy guessing heuristics, recursive ladder games\nthat give a lower bound for the recursive algorithms, and model checker lad-\nder games [27] for which the algorithm by Stevens and Stirling [72] behaves\nexponentially.\n3.3\nRandom Games\nThe ﬁnal class of games that is typically used in publications that empirically\nevaluate the performance of algorithms on parity games are random parity games\n[6,69,68,49,30]. We study three classes of random games. We expect that the\nstructural properties of random games are, typically, diﬀerent from parity games\nobtained in the previous classes. This class is, therefore, unlikely to give insights\nin the performance of parity game algorithms on practical problems.\n4\nImplementation\nAll games were generated on a 1TB main memory, 56-core Linux machine, where\neach core was running at 2.27GHz. Executions of tools generating and solving\nparity games, and tools collecting statistics about parity games, were limited to\nrunning times of 1 hour and their memory usage was limited to 32GB.\nTo systematically generate the benchmarks, we have implemented tooling\nthat allows the parallel execution of individual cases. Here a case is either\ngenerating or solving a game, or collecting a single measure. Each individual\ncase only uses a single core. The tools are implemented in an extensible way,\ni.e., additional parity games, additional encodings, as well as additional meas-\nures can be added straightforwardly. The tools are available for download from\nhttps:\/\/github.com\/jkeiren\/paritygame-generator.\n4.1\nGenerating Parity Games\nFor the generation of our benchmarks we rely on a number of external tools:\nversion 3.3 of PGSolver [31] for generating random games, and games that prove\nto be hard for certain algorithms; version 1.2 of MLSolver to generate the games\nfor satisﬁability and validity problems [32]; and revision 11703 of the mCRL2\ntoolset [20] for the model checking and equivalence checking problems. For all\ngames we have collected the information described in Section 2 to the extent in\nwhich this is feasible.\n9\n4.2\nCollecting Statistics\nWe developed the tool pginfo for collecting structural information from parity\ngames. The tool is available from https:\/\/github.com\/jkeiren\/pginfo and\naccepts parity games in the ﬁle format used by PGSolver. The tool reads a\nparity game, and writes statistics to a ﬁle in a structured way.\nThe implementation is built on top of the Boost Graph library [70], which\nprovides data structures and basic algorithms for manipulating graphs. Com-\nputing the exact value for the width-measures is problematic: it is known to be\nNP-complete [3]. Approximation algorithms are known that compute upper- and\nlower bound for these measures; especially for treewidth these have been thor-\noughly studied [12,13]. To determine feasibility of computing width-measures\nfor our benchmarks we have implemented three approximation algorithms. For\ncomputing upper and lower bounds on treewidth we implemented the greedy\ndegree algorithm [12] and the minor min-width algorithm [36], respectively. For\ncomputing an upper bound of the Kelly-width we implemented the elimination\nordering described in [40]. Even these approximation algorithms have proven to\nbe impractical due to their complexity. Computing (bounds) on the other width\nmeasures is equally complex.\n4.3\nAvailability of Parity Games\nAll parity games that are described in this paper are available for download\nfrom http:\/\/www.github.com\/jkeiren\/paritygame-generator in bzip2 com-\npressed PGSolver format [31]. The dataset is approximately 10GB in size, and\nincludes the structural information that was collected from these games.\n5\nAnalysis of Benchmarks\nWe have presented benchmarks originating from diﬀerent problems. Next we\nanalyse them with respect to the measures described in Section 2. This analysis\nillustrates that our benchmarks exhibit a wide variety of properties. Furthermore,\nthis gives us some insights in the characteristics of typical parity games. For\neach of the statistics, we only consider games for which that speciﬁc statistic\ncould be computed within an hour, and we only include those statistics that\ncan feasibly be computed for the majority of games, as a consequence the width\nmeasure are excluded from the analysis we present here. We used this selection\nto avoid timeouts for computing the measures that are expensive to compute,\nsuch as the diameter and the girth. All graphs in this section are labelled by\ntheir class. Note that the satisﬁability and validity problems are labelled by\n“mlsolver” and the games that are hard for some solving algorithms are labelled\nby “specialcases”. The full data presented in this chapter is also available from\nhttp:\/\/www.github.com\/jkeiren\/paritygame-generator.\nOur data set contains 1037 parity games that range from 2 vertices to 40\nmillion vertices, and on average they have about 95,000 vertices. The number of\n10\nedges ranges from 2 to 167 million, with an average of about 3.1 million. The\n59 parity games are games in which all vertices are owned by a single player,\nthe so-called solitaire games [8], the rest are parity games in which both players\nown non-empty sets of vertices. The parity games that we consider have diﬀering\ndegrees. There are instances in which the average degree is 1, the average degree\nis maximally 9999, but it is typically below 10. The ratio between the number of\nvertices and the number of edges is, therefore, relatively small in general. This\ncan also be observed from Figure 1a, which displays the correlation between the\ntwo. The games in which these numbers coincide are on the line x = y, the\nother games lie around this line due to the log scale that we use. Our parity\ngames generally contain a vertex with in-degree 0, which is the starting vertex.\nMost of the games contain vertices with a high in-degree—typically representing\nvertices that are trivially won by either of the players—, and vertices with a high\nout-degree.\n101\n102\n103\n104\n105\n106\n107\n101\n102\n103\n104\n105\n106\n107\n108\nVertices\nEdges\n(a)\n101\n102\n103\n104\n105\n100\n101\n102\n103\n104\nVertices\nGirth\nmodelchecking\nequivalence\nmlsolver\nspecialcases\nrandom\n(b)\nFigure 1: Relation between number of vertices and (a) number of edges, and (b)\ngirth.\nIn general, the SCC quotient height ranges up to 513 for the parity games\nthat we consider with an average of around 14. The number of non-trivial SCCs\ncan grow large, up to 1.4 million for our games.\nThe diameter and girth have been computed only for smaller parity games,\nand the data we present for them, therefore, considers a subset of the parity\ngames only. We expect that typical parity games contain self-loops, which leads\nto a small girth—the girth is 1 if the game contains a self-loop. This is conﬁrmed\nby the data in Figure 1b. Note that the girth is large for some of the hard cases\nthat we consider. A closer investigation shows that this is solely due to the model\nchecker ladder games [27].\nThe diameters of the parity games, i.e., the maximal length of any shortest\npath in the game, are nicely distributed over the sizes of the game. Figure 2a\n11\n101\n102\n103\n104\n105\n100\n101\n102\n103\n104\nVertices\nDiameter\n(a)\n101\n102\n103\n104\n100\n101\n102\n103\n104\nBFS Height\nDiameter\nmodelchecking\nequivalence\nmlsolver\nspecialcases\nrandom\n(b)\nFigure 2: Relation between (a) number of vertices and diameter, and (b) BFS\nheight and diameter.\nshows that for every size of game we have parity games of a large range of\ndiﬀerent diameters. For the hard cases the diameter is, generally, large due to\nvariations of ladder games. Generally, the diameter for satisﬁability and validity\nproblems is larger than the diameter for model checking problems. For random\ngames the diameter is typically small.\nFigure 2b indicates that the diameter and the number of BFS levels are cor-\nrelated, the number of BFS levels is therefore likely to be a good approximation\nof the diameter, also for larger instances. Note that this corresponds to a similar\nobservation made by P´elanek, who stated that typically the diameter is smaller\nthan 1.5 times the number of BFS levels for state spaces [60].\nOf the parity games that we consider, 882 contain diamonds. Of these, 494\ncontain even diamonds, and 656 contain odd diamonds, 382 contain both. This\nindicates that it is worth investigating techniques, such as conﬂuence reduction,\nthat use these diamonds to either simplify parity games or speed up solving. In\ngeneral, the number of diamonds is independent of the number of vertices in the\ngame.\nThe average 3-neighbourhoods range from 3 to 5000 across the sizes of the\ngames, as can be seen from Figure 3a. Note that the average size of the 3-\nneighbourhoods is typically high for random games, and limited to 100 for most\nother classes of games.\nWe have included parity games with alternation depths up to 50,000 as shown\nin Figure 3b. Observe that the games for model checking and equivalence check-\ning included in our benchmarks all have alternation depth at most 2. Model\nchecking problems could be formulated that have a higher alternation depth—up\nto arbitrary numbers—however, in practice properties have limited alternation\ndepth because they become too hard to understand otherwise. The satisﬁability\nand validity properties have alternation depths between 1 and 4. The alternation\n12\n101\n102\n103\n104\n105\n106\n107\n100\n101\n102\n103\nVertices\nAvg 3-neighbourhood\n(a)\n101\n102\n103\n104\n105\n106\n107\n100\n101\n102\n103\n104\nVertices\nAlternation depth\nmodelchecking\nequivalence\nmlsolver\nspecialcases\nrandom\n(b)\nFigure 3: Number of vertices in relation to (a) size of 3-neighbourhoods, and (b)\nalternation depth.\ndepths of the random games are between 10 and 15. All parity games with more\nthan 50 priorities represent special cases. Closer investigation shows that these\nspecial cases are the clique games and recursive ladder games.\nTo summarise, we have presented a large set of parity games. For a selection\nof the structural properties introduced in Section 2 we have shown that the\ngames cover a large range of values. Also observe that, for parity game speciﬁc\nproperties such as alternation depth, higher values are only available for smaller\ngames due to generation times. Unsurprisingly, the random games considered in\nthis paper are not structurally similar to parity games that represent encodings\nof veriﬁcation problems.\n6\nClosing Remarks\nNo standard benchmarks for parity game algorithms existed. As a consequence,\nit was virtually impossible to make a good comparison between algorithms and\napplications described in the literature. In this paper we have addressed this issue\nby presenting a comprehensive set of parity game benchmarks. Our benchmarks\ninclude the games that appear in the literature, and provides a ﬁrst step towards\nstandardising experimental evaluation of parity game algorithms. All games have\nbeen generated in an extensible way, and are available on-line.\nWe also presented a set of structural properties for parity games, and analysed\nour benchmarks with respect to these properties. Of particular interest is a\nnew notion of alternation depth for parity games, that is always at most the\nnumber of priorities in a parity game, and that is bounded also by the alternation\ndepth of µ-calculus formulae given a reasonable translation of the model checking\nproblem.\n13\nFuture work. Some of the structural properties, such as treewidth, cannot be\ncomputed for all games in the benchmark suite due to their complexity. An\ninteresting algorithmic question is, therefore, whether algorithms or heuristics\ncan be devised that can compute or approximate these measures for large graphs.\nAdditionally, we have presented a selection of structural properties in this\npaper. One can wonder whether there are other structural properties of parity\ngames that are relevant to the practical performance of parity game algorithms.\nThe question whether the theoretical complexity of existing parity game al-\ngorithms can be made tighter using structural properties, such as our notion of\nalternation depth.\nWe believe our work also paves the way for a full-scale comparison of par-\nity game algorithms and the eﬀect of heuristics in the spirit of [30], including\nthe comparison of alternative implementations of algorithms [20,23]. Here also\nthe impact of the structural properties on the performance of implementations\nshould be studied, since we have only scratched the surface of this aspect in this\npaper.\nFinally, we welcome the addition of problems and properties to our bench-\nmark suite to establish and maintain a corpus for experimentation with parity\ngame algorithms. In particular parity games with a large number of priorities\nand a high alternation depth stemming from encodings of, e.g., veriﬁcation and\nsynthesis problems form a welcome addition.\nAcknowledgements For generating the parity games described in the paper,\na large number of tools have been used. The author would like to thank the\ndevelopers of, in particular, Gist, GOAL, mCRL2, MLSolver and PGSolver.\nThanks also go to Wan Fokkink and Tim Willemse for helpful feedback on earlier\nversions of this paper, and remarks by anonymous reviewers that led to usability\nimprovements of the benchmarks and tools presented.\nReferences\n1. I. Adler. Directed tree-width examples. Journal of Combinatorial Theory, Series\nB, 97(5):718–725, 2007.\n2. M.H. Albert, J.P. Grossman, R.J. Nowakowski, and D. Wolfe. An introduction to\nclobber. Integers, 5(2), 2005.\n3. S. Arnborg, D.G. Corneil, and A. Proskurowski. Complexity of ﬁnding embeddings\nin a k-tree. SIAM Journal on Algebraic Discrete Methods, 8(2):277–284, 1987.\n4. K.A. Bartlett, R.A. Scantlebury, and P.T. Wilkinson. A note on reliable full-duplex\ntransmission over half-duplex links. Communications of the ACM, 12(5):260–261,\n1969.\n5. A. Beck, M.N. Bleicher, and D.W. Crowe.\nExcursions into Mathematics: The\nMillennium Edition. CRC Press, 2000.\n6. E. Beﬀara and S.G. Vorobyov. Adapting Gurvich-Karzanov-Khachiyan’s algorithm\nfor parity games. Technical report, Uppsala University, Sweden, Uppsala, 2001.\n7. D. Berwanger, A. Dawar, P.W. Hunter, S. Kreutzer, and J. Obdrˇz´alek. The DAG-\nwidth of directed graphs. Journal of Combinatorial Theory, Series B, 102(4):900–\n923, 2012.\n14\n8. D. Berwanger and E. Gr¨adel. Fixed-point logics and solitaire games. Theory of\nComputing Systems, 37(6):675–694, 2004.\n9. D. Berwanger and E. Gr¨adel. Entanglement — a measure for the complexity of dir-\nected graphs with applications to logic and games. In Logic for Programming, Arti-\nﬁcial Intelligence, and Reasoning, volume 3452 of LNCS, pages 209–223. Springer,\n2005.\n10. D. Berwanger, E. Gr¨adel, L. Kaiser, and R. Rabinovich. Entanglement and the\ncomplexity of directed graphs. Theoretical Computer Science, 463:2–25, 2012.\n11. H.L. Bodlaender. Treewidth: Algorithmic techniques and results. In Mathemat-\nical Foundations of Computer Science 1997, volume 1295 of LNCS, pages 19–36.\nSpringer, 1997.\n12. H.L. Bodlaender and A.M.C.A. Koster. Treewidth computations I. upper bounds.\nInformation and Computation, 208(3):259–275, 2010.\n13. H.L. Bodlaender and A.M.C.A. Koster. Treewidth computations II. lower bounds.\nInformation and Computation, 209(7):1103–1119, 2011.\n14. J.C. Bradﬁeld and C. Stirling. Modal logics and mu-calculi: an introduction. In\nHandbook of Process Algebra, chapter 4, pages 293–330. Elsevier, 2000.\n15. V. Cerf and R.E. Kahn. A protocol for packet network intercommunication. IEEE\nTransactions on Communications, 22(5):637–648, 1974.\n16. K. Chatterjee, T.A. Henzinger, B. Jobstmann, and A. Radhakrishna. GIST: A\nsolver for probabilistic games. In CAV 2012, volume 6174 of LNCS, pages 665–\n669. Springer, 2010.\n17. T. Chen, S.C.W. Ploeger, J.C. van de Pol, and T.A.C. Willemse.\nEquival-\nence checking for inﬁnite systems using parameterized Boolean equation systems.\nIn CONCUR’07: Concurrency Theory, volume 4703 of LNCS, pages 120–135.\nSpringer, 2007.\n18. R. Cleaveland, M. Klein, and B. Steﬀen. Faster model checking for the modal mu-\ncalculus. In Computer Aided Veriﬁcation, volume 663 of LNCS, pages 410–422.\nSpringer, 1993.\n19. B. Courcelle and S. Olariu. Upper bounds to the clique width of graphs. Discrete\nApplied Mathematics, 101(1-3):77–114, 2000.\n20. S. Cranen, J.F. Groote, J.J.A. Keiren, F.P.M. Stappers, E.P. de Vink, J.W.\nWesselink, and T.A.C. Willemse. An overview of the mCRL2 toolset and its recent\nadvances. In TACAS’13, volume 7795 of LNCS, pages 199–213. Springer, 2013.\n21. S. Cranen, J.J.A. Keiren, and T.A.C. Willemse. Stuttering mostly speeds up solv-\ning parity games. In NASA Formal Methods, volume 6617 of LNCS, pages 207–221.\nSpringer, 2011.\n22. S. Cranen, J.J.A. Keiren, and T.A.C. Willemse. A cure for stuttering parity games.\nIn Theoretical Aspects of Computing — ICTAC 2012, volume 7521 of LNCS, pages\n198–212. Springer, 2012.\n23. A. Di Stasio, A. Murano, V. Prignano, and L. Sorrentino. Solving parity games in\nScala. In FACS, 2014.\n24. E.A. Emerson and C.S. Jutla. Tree automata, mu-calculus and determinacy. In\nSFCS ’91: Proceedings of the 32nd annual symposium on Foundations of computer\nscience, pages 368–377. IEEE Computer Society, 1991.\n25. E.A. Emerson and C.L.L. Lei. Eﬃcient model checking in fragments of the propos-\nitional mu-calculus. In Proceedings of LICS 1986, pages 267–278. IEEE Computer\nSociety, 1986.\n26. O. Friedmann.\nA super-polynomial lower bound for the parity game strategy\nimprovement algorithm as we know it. 2009 24th Annual IEEE Symposium on\nLogic In Computer Science, 7:145–156, 2009.\n15\n27. O. Friedmann.\nThe Stevens-Stirling-algorithm for solving parity games locally\nrequires exponential time. International Journal of Foundations of Computer Sci-\nence, 21(03):277–287, 2010.\n28. O. Friedmann. An exponential lower bound for the latest deterministic strategy\niteration algorithms. Logical Methods in Computer Science, 7:1–42, 2011.\n29. O. Friedmann. Recursive algorithm for parity games requires exponential time.\nRAIRO - Theoretical Informatics and Applications, 45(4):449–457, 2011.\n30. O. Friedmann and M. Lange. Solving parity games in practice. In Automated Tech-\nnology for Veriﬁcation and Analysis 7th International Symposium, ATVA 2009,\nvolume 5799 of LNCS, pages 182–196. Springer, 2009.\n31. O. Friedmann and M. Lange. The PGSolver collection of parity game solvers. Tech-\nnical report, Institut f¨ur Informatik, Ludwig-Maximilians-Universit¨at M¨unchen,\nGermany, 2010.\n32. O. Friedmann and M. Lange. A solver for modal ﬁxpoint logics. In Electronic\nNotes in Theoretical Computer Science, volume 262, pages 99–111. Elsevier, 2010.\n33. O. Friedmann, M. Latte, and M. Lange. A decision procedure for CTL∗based on\ntableaux and automata. In Automated Reasoning, volume 6173 of LNCS, pages\n331–345. Springer, 2010.\n34. M. Gardner. Mathematical games: Cram, crosscram and quadraphage: New games\nhaving elusive winning strategies. Scientiﬁc American, 230:106–108, 1974.\n35. M.W. Gazda and T.A.C. Willemse. Zielonka’s recursive algorithm: dull, weak and\nsolitaire games and tighter bounds. In Proceedings GandALF 2013, volume 119 of\nEPTCS, pages 7–20. 2013.\n36. V. Gogate and R. Dechter.\nA complete anytime algorithm for treewidth.\nIn\nProceedings of the 20th conference on Uncertainty in artiﬁcial intelligence, UAI\n’04, pages 201–208. AUAI Press, 2004.\n37. J.F. Groote, J. Pang, and A.G.G. Wouters. Analysis of a distributed system for\nlifting trucks. The Journal of Logic and Algebraic Programming, 55(1-2):21–56,\n2003.\n38. J.F. Groote and J. van de Pol. A bounded retransmission protocol for large data\npackets. In Algebraic Methodology and Software Technology, volume 1101 of LNCS,\npages 536–550. Springer, 1996.\n39. W.H. Hesselink. Invariants for the construction of a handshake register. Informa-\ntion Processing Letters, 68:173–177, 1998.\n40. P.W. Hunter and S. Kreutzer. Digraph measures: Kelly decompositions, games,\nand orderings. Theoretical Computer Science, 399(3):206–219, 2008.\n41. T. Johnson, N. Robertson, P.D. Seymour, and R. Thomas. Directed tree-width.\nJournal of Combinatorial Theory, Series B, 82(1):138–154, 2001.\n42. M. Jurdzi´nski. Deciding the winner in parity games is in UP ∩co-UP. Information\nProcessing Letters, 68(3):119–124, 1998.\n43. M. Jurdzi´nski. Small progress measures for solving parity games. In STACS ’00:\nProceedings of the 17th Annual Symposium on Theoretical Aspects of Computer\nScience, volume 1770 of LNCS, pages 290–301. Springer, 2000.\n44. M. Jurdzi´nski, M. Paterson, and U. Zwick.\nA deterministic subexponential al-\ngorithm for solving parity games. Proceedings of the seventeenth annual ACM-\nSIAM symposium on Discrete algorithm - SODA ’06, pages 117–123, 2006.\n45. J.J.A. Keiren. Advanced Reduction Techniques for Model Checking. PhD thesis,\nEindhoven University of Technology, 2013.\n46. J.J.A. Keiren. Benchmarks for parity games. In Fundamentals of Software Engin-\neering (FSEN 2015), LNCS. Springer, 2015. Accepted for publication, to appear.\n16\n47. J.J.A. Keiren and T.A.C. Willemse. Bisimulation minimisations for Boolean equa-\ntion systems.\nIn Proceedings of Haifa Veriﬁcation Conference 2009 (HVC’09),\nvolume 6405 of LNCS, pages 102–116. Springer, Heidelberg, 2011.\n48. C.P.J. Koymans and J.C. Mulder. A modular approach to protocol veriﬁcation\nusing process algebra. In Applications of Process Algebra, number 17 in Cambridge\nTracts in Theoretical Computer Science, pages 261–306. 1990.\n49. M. Lange. Solving parity games by a reduction to SAT. In Proc. of the Workshop\non Games in Design and Veriﬁcation, GDV’05, 2005.\n50. K.G. Larsen. Eﬃcient local correctness checking. In Computer Aided Veriﬁcation,\nFourth International Workshop, CAV ’92, Proceedings, volume 663 of LNCS, pages\n30–43. Springer, 1993.\n51. S.P. Luttik. Description and formal speciﬁcation of the link layer of P1394. In\nWorkshop on Applied Formal Methods in System Design, pages 43–56, 1997.\n52. T. Maarup.\nHex - everything you always wanted to know about hex but were\nafraid to ask. Master’s thesis, 2005.\n53. A. Mader. Veriﬁcation of Modal Properties Using Boolean Equation Systems. PhD\nthesis, Technische Universit¨at M¨unchen, 1997.\n54. R. Mateescu.\nA generic on-the-ﬂy solver for alternation-free Boolean equation\nsystems. In TACAS’03, volume 2619 of LNCS, pages 81–96. Springer, 2003.\n55. R. McNaughton. Inﬁnite games played on ﬁnite graphs. Annals of Pure and Applied\nLogic, 65(2):149–184, 1993.\n56. R. Nowakowski and P. Winkler.\nVertex-to-vertex pursuit in a graph.\nDiscrete\nMathematics, 43(2-3):235–239, 1983.\n57. J. Obdrˇz´alek. Fast mu-calculus model checking when tree-width is bounded. In\nComputer Aided Veriﬁcation, volume 2725 of LNCS, pages 80–92. Springer, 2003.\n58. J. Obdrˇz´alek. Algorithmic Analysis of Parity Games. PhD thesis, Laboritory for\nFoundations of Computer Science, School of Informatics, University of Edinburgh,\n2006.\n59. J. Pang, W.J. Fokkink, R. Hofman, and R. Veldema.\nModel checking a cache\ncoherence protocol of a Java DSM implementation.\nThe Journal of Logic and\nAlgebraic Programming, 71(1):1–43, 2007.\n60. R. Pel´anek.\nTypical structural properties of state spaces.\nIn Model Checking\nSoftware, volume 2989 of LNCS, pages 5–22. Springer, 2004.\n61. R. Pel´anek.\nWeb portal for benchmarking explicit model checkers.\nTechnical\nReport FIMU-RS-2006-03, Faculty of Informatics Masaryk University Brno, 2006.\n62. R. Pel´anek. BEEM: benchmarks for explicit model checkers. In Model Checking\nSoftware, volume 4595 of LNCS, pages 263–267. Springer, 2007.\n63. A. Quilliot. Jeux et pointes ﬁxes sur les graphes. PhD thesis, Universit´e de Paris\nVI, 1978.\n64. N. Robertson and P.D. Seymour. Graph minors. II. algorithmic aspects of tree-\nwidth. Journal of Algorithms, 7(3):309–322, 1986.\n65. B. Rose. Othello: A Minute to Learn... A Lifetime to Master. 2005.\n66. S. Safra. On the complexity of omega-automata. In 29th Annual Symposium on\nFoundations of Computer Science, pages 319–327. IEEE, 1988.\n67. S. Schewe. Solving parity games in big steps. volume 4855 of LNCS, pages 449–460.\nSpringer, 2007.\n68. S. Schewe.\nAn optimal strategy improvement algorithm for solving parity and\npayoﬀgames. In Computer Science Logic, volume 5213 of LNCS, pages 369–384.\nSpringer, 2008.\n69. S. Schewe. Synthesis of Distributed Systems. Phd thesis, Universit¨at des Saar-\nlandes, 2008.\n17\n70. J.G. Siek, L.Q. Lee, and A. Lumsdaine. The Boost Graph Library: User Guide and\nReference Manual. Addison-Wesley, 2002.\n71. M. Sighireanu and R. Mateescu. Veriﬁcation of the link layer protocol of the IEEE-\n1394 serial bus (FireWire): An experiment with e-Lotos. STTT, 2(1):68–88, 1998.\n72. P. Stevens and C. Stirling. Practical model checking using games. In TACAS’98,\nvolume 1384 of LNCS, pages 85–101. Springer, 1998.\n73. C. Stirling. Bisimulation, modal logic and model checking games. Logic Journal\nof IGPL, 7(1):103124, January 1999.\n74. F.W. Takes and W.A. Kosters.\nDetermining the diameter of small world net-\nworks. In Proceedings of the 20th ACM international conference on Information\nand knowledge management - CIKM ’11, pages 1191–1196. ACM Press, 2011.\n75. Y.K. Tsay, Y.F. Chen, M.H. Tsai, W.C. Chan, and C.J. Luo. GOAL extended:\nTowards a research tool for omega automata and temporal logic. In TACAS’08,\nvolume 4963 of LNCS, pages 346–350. Springer, 2008.\n76. J.C. van de Pol and M. Weber. A multi-core solver for parity games. Electronic\nNotes in Theoretical Computer Science, 220(2):19–34, 2008.\n77. R. Veldema, R.F.H. Hofman, R.A.F. Bhoedjang, C.J.H. Jacobs, and H.E. Bal.\nSource-level global optimizations for ﬁne-grain distributed shared memory systems.\nACM SIGPLAN Notices, 36(7):83–92, June 2001.\n78. B. Vergauwen and J. Lewi.\nEﬃcient local correctness checking for single and\nalternating Boolean equation systems. In Automata, Languages and Programming,\nvolume 820 of LNCS, pages 304–315. Springer, 1994.\n79. W. Zielonka. Inﬁnite games on ﬁnitely coloured graphs with applications to auto-\nmata on inﬁnite trees. Theoretical Computer Science, 200(1-2):135–183, 1998.\n18\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Benchmarks for Parity Games (extended version).pdf"}
{"title":"Fate stochastic management and policy benchmark in 421, a popular game","authors":"Pierre Albarede","summary":"Using game and probability theories, I study the French popular game 421, a\nperfect information stochastic stage game. The problem is to find strategies\nmaximizing the probability of some expected utility. I only solve a player's\nround against providence, a problem of fate stochastic management: beyond the\nbackward induction solution, bounded complexity motivates heuristic policies.\nFor a unique goal utility, a simple optimal policy, ratchet, is obtained. Its\nresult probabilities are compiled and used, for arbitrary utilities, as the\nlogic of goal identification policies. Various policies appear, close to human\nbehavior, and are exactly evaluated by solving the Kolmogorov equation.","url":"http:\/\/arxiv.org\/abs\/math\/0007129v1","pdf_url":"http:\/\/arxiv.org\/pdf\/math\/0007129v1","published":964162258000,"comment":"35 page LaTeX article, with figures. See also\n  http:\/\/pierre.albarede.free.fr\/","pdf_text":"arXiv:math\/0007129v1  [math.OC]  21 Jul 2000\nF\nate\nsto\n \nhasti \nmanagemen\nt\nand\np\noli y\nb\nen \nhmark\nin\n421,\na\np\nopular\ngame\nPierre\nAlbarède\nb.\nA,\nrés.\nV\nalv\nert,\n12,\na\nv.\nde\nla\nF\nourane,\n13090\nAix-en-Pro\nv\nen e,\nF\nran e\npalbarede y\naho\no. om\nApril\n26,\n2024\nAbstra t\nUsing\ngame\nand\nprobabilit\ny\ntheories,\nI\nstudy\nthe\nF\nren \nh\np\nopular\ngame\n421,\na\np\nerfe t\ninformation\nsto\n \nhasti \nstage\ngame.\nThe\nprob-\nlem\nis\nto\n\u001cnd\nstrategies\nmaximizing\nthe\nprobabilit\ny\nof\nsome\nexp\ne ted\nutilit\ny\n.\nI\nonly\nsolv\ne\na\npla\ny\ner's\nround\nagainst\npro\nviden e,\na\nproblem\nof\nfate\nsto\n \nhasti \nmanagemen\nt:\nb\ney\nond\nthe\nba \nkw\nard\nindu tion\nsolu-\ntion,\nb\nounded\n omplexit\ny\nmotiv\nates\nheuristi \np\noli ies.\nF\nor\na\nunique\ngoal\nutilit\ny\n,\na\nsimple\noptimal\np\noli y\n,\nrat \nhet,\nis\nobtained.\nIts\nresult\nprobabilities\nare\n ompiled\nand\nused,\nfor\narbitrary\nutilities,\nas\nthe\nlogi \nof\ngoal\niden\nti\u001c ation\np\noli ies.\nV\narious\np\noli ies\napp\near,\n lose\nto\nh\nu-\nman\nb\neha\nvior,\nand\nare\nexa tly\nev\naluated\nb\ny\nsolving\nthe\nK\nolmogoro\nv\nequation.\nk.\nw.:\nsto\n \nhasti \nmanagemen\nt,\nK\nolmogoro\nv\nequation,\nb\nounded\n om-\nplexit\ny\n,\nh\numan\nb\neha\nvior.\nJEL\nC61,\nC63,\nC73.\nMSC:\n60J20,\n65K05,\n90B50,\n91A15,\n93E20.\n1\nCONTENTS\n2\nCon\nten\nts\n1\nAim\nand\nin\nterest\nof\nthe\nstudy\n3\n2\nBa \nkw\nard\nindu tion\noptimal\np\noli y\n4\n2.1\nAlea\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n4\n2.2\nF\nate\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n6\n2.3\nUtilit\ny\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n7\n2.4\nOptimal\nstrategies\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n8\n3\nF\nate\nas\na\nsto\n \nhasti \npro\n ess\n10\n3.1\nThe\nK\nolmogoro\nv\nequation\non\nexp\ne ted\nutilit\ny\n.\n.\n.\n.\n.\n.\n.\n.\n.\n10\n3.2\nThe\nF\nokk\ner-Plan \nk\nequation\non\npresen e\ndensit\ny\n.\n.\n.\n.\n.\n.\n.\n.\n11\n3.3\nComputing\nresult\nprobabilities\nb\ny\ndualit\ny\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n13\n3.4\nAnalogy\nwith\nlinear\ntransp\nort\ntheory\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n14\n4\nSimple\noptimal\np\noli ies\nfor\none-goal\nutilities\n14\n4.1\nThe\nrat \nhet\nand\nBernoulli\np\noli ies\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n14\n4.2\nOptimal\none-goal\nstrategy\nresult\nprobabilities\n.\n.\n.\n.\n.\n.\n.\n.\n.\n15\n5\nGoal\niden\nti\u001c ation\nprogramming\n17\n5.1\nMotiv\nation:\nb\nounded\n omplexit\ny\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n17\n5.2\nRedu ed\nhorizon\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n18\n5.3\nDynami \nprogramming\nand\ngoal\nrevision\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n19\n5.4\nP\noli y\nb\nen \nhmark\nand\nin\nterpretation\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n20\n6\nCon lusions\n22\nA\nThe\n(ten\ntativ\ne)\nrules\nof\n421\n23\nB\nA\nGalton-W\natson\npro\n ess\nin\nthe\n421\nround\n25\nC\nRealization\nwith\nmathemati a\n26\n1\nAIM\nAND\nINTEREST\nOF\nTHE\nSTUD\nY\n3\n1\nAim\nand\nin\nterest\nof\nthe\nstudy\nF\nollo\nwing\n[1℄,\nI\nlo\nok\nfor\nstrategies,\nmaximizing\nthe\nprobabilit\ny\nof\nwin,\nor\nsome\nexp\ne ted\nutilit\ny\n,\nin\nthe\ngame\n421\n om\nbining\n \nhan e\nand\nde ision\n(see\napp\nendix\nA).\nBy\nthe\nw\na\ny\n,\nor,\nindeed,\nb\ny\nser\nendipity,\nI\nen oun\nter\nthe\nproblem\nof\nfate\nsto\n hasti \nmanagement\n:\noptimizing\nto\nda\ny's\nde isions,\nwith\nresp\ne t\nto\na\nfu-\nture\nutilit\ny\n,\nand\nin\nspite\nof\ntomorro\nw's\no\ndds.\nSu \nh\nissues\nas\nwhat\nare\nthe\noptimal\np\noli ies,\nin\nwhat\n ir umstan es,\nand\nho\nw\nm\nu \nh\nthey\ndemand\non\nin\ntelle tual\nresour es,\n an\nb\ne\nresolv\ned\nmathemati ally\n,\nsuggesting\nthat\nman-\nagemen\nt\n ould\nb\ne\nan\nexa t\ns ien e\n(as\npart\nof\nop\nerations\nresear \nh).\nA\nlottery\nis\nnot\na\ngame,\nin\nthe\nsense\nof\ngame\ntheory\n,\nbut\na\nsto\n \nhasti \npro\n ess\n(a\nsequen e\nof\nrandom\nv\nariables).\nGame\ntheory\ntreats\n lassi ally\nm\nultiple\npla\ny\ner\nde ision\ngames,\nthe\nar \nhet\nyp\ne\nof\nwhi \nh\nis\n \nhess.\nA\ngame\nin\nwhi \nh\npla\ny\ners'\nfates\ndep\nend\non\nb\noth\n \nhan e\nand\ntheir\nde isions,\nlik\ne\n421\nand\nba \nkgammon\n[2℄,\nis\na\nsto\n hasti \ngame\n[3℄.\nChan e\nmak\nes\nde ision\nmore\n omplex.\nF\nor\nexample,\n onsider\na\nv\nariation\non\n \nhess:\nthe\npla\ny\ner\nprop\noses\na\nlist\nof n\nmo\nv\nes,\nthe\na tual\none\nb\neing\ndetermined\nb\ny\n asting\ndi e. n = 1\nyields\nthe\nstandard\npure\nde ision\ngame; n = n1\n,\nwhere n1\nis\nthe\nn\num\nb\ner\nof\np\nossible\nmo\nv\nes,\nyields\na\npure\n \nhan e\ngame; n ≈n1\/2\nyields\na\ngame\nof\n \nhan e\nand\nde ision,\nmore\n omplex\nthan\nthe\nformer\nand\nthe\nlatter.\nGame\ntheory\nprimarily\nfo\n uses\non\nthe\nexisten e\npro\nofs\nfor\noptimal\nstrate-\ngies.\nHo\nw\nev\ner,\n\u0010usable\nte \nhniques\nfor\nobtaining\npra ti al\nansw\ners\u0011\nalso\nmat-\nter\n[4,\n1.1℄.\nIndeed,\nlittle\n an\nb\ne\ndone\nfrom\nexisten e\nwithout\n onstru tion:\nthis\nis\nthe\nold\ndebate\naround\nZermelo's\naxiom\nof\n \nhoi e.\nHen e\nthe\nin\nterest\nof\nin\nv\nestigating,\nas\nin\nCh\nur \nh's\nthesis\n[5℄,\n al ulabilit\ny\n,\nthe\nexisten e\nof\nan\nalgorithmi \nsolution.\nBut\nev\nen\n al ulabilit\ny\nma\ny\nnot\nb\ne\nsu\u001e ien\nt\nfor\na tual\n omputation.\nF\nor\nexample,\n onsider\nagain\n \nhess,\na\n\u001cnite\nbut\nv\nery\nlarge\ngame:\nthe\nalgorithmi \nsolution\npro\nvided\nb\ny\nthe\nZermelo\ntheorem\n[6,\n \nh.\n6℄\nis\nof\nno\npra ti al\nuse\n(un\ntil\nthe\n\u001cnal\nmo\nv\nes),\nas\nnoti ed\nb\ny\n[7,\n11.4℄,\nas\nit\nex eeds\nthe\n apa it\ny\nof\nan\ny\n omputer.\nThe\nstudy\nof\n\u001cnite\ngames\ndo\nes\nnot\nstop\nwith\nZer-\nmelo\ntheorem,\nand\nthis\nis\nb\ne ause\nof\n omplexit\ny\nb\noundedness.\nAlgorithms\nshall\nb\ne\n ompared\nnot\nonly\nwith\nresp\ne t\nto\noptimalit\ny\n(degree\nof\n ompletion\nof\nthe\ntask)\nbut\nalso\n omplexit\ny\n,\nusing\na\nbit\nof\n \nomplexity\nthe\nory\n[5℄.\nAn\nalgorithm\nis\n \nhara terized\nb\ny\nits\noptimalit\ny\n,\nsize\nand\n omputing\ntime\non\na\ngiv\nen\n omputer,\nsp\ne ialized\nb\ny\nhigh-lev\nel\nfun tions\nand\ndata.\nThe\nalgo-\nrithm\nma\ny\nb\ne\ngo\no\nd\nor\nbad,\nshort\nor\nlong,\nfast\nor\nslo\nw.\nThe\nthree\nqualities\nand\nquan\ntities\nare\nnot\nindep\nenden\nt:\nthe\nex \nhange\nof\n omputing\ntime\nagainst\nsize\n2\nBA\nCKW\nARD\nINDUCTION\nOPTIMAL\nPOLICY\n4\nis\nthe\nprin iple\nof\ndata\n ompression,\nthe\nex \nhange\nof\n omputing\ntime\nagainst\noptimalit\ny\nis\nthe\nprin iple\nof\nheuristi s.\nWhen\nsetting\nt\nw\no\nquan\ntities,\nthe\nminim\num\nof\nthe\nthird\none,\nas\na\nfun tion\nof\nother\nimpli it\nparameters,\n an\nb\ne\nde\u001cned:\nthe\nminim\num\nsize\nand\nthe\nminim\num\n omputing\ntime\nare\nresp\ne -\ntiv\nely\nrelated\nwith\nthe\nK\nolmogoro\nv\n omplexit\ny\nand\nthe\nBennett\nlogi al\ndepth\n[5℄.\nF\nor\nstrategy-generating\nalgorithms,\nor\nde iding\nalgorithms,\nor\np\noli ies,\noptimalit\ny\nis\nthe\nexp\ne ted\nutilit\ny\n.\nF\nor\na\np\nerfe t\ninformation\nstage\ngame,\nit\nis\nin\nteresting,\nfor\np\nossible\nextensions,\nto\n \nhara terize\nthe\nasymptoti \nb\neha\nvior\nof\nthe\n omputing\ntime,\nwhen\nthe\ndepth\ntends\nto\nin\u001cnit\ny\n,\ni.\ne.\nto\nkno\nw\nwhether\nthe\nalgorithm\nis\nlinear\nor\np\nolynomial,\nrather\nthan\nexp\nonen\ntial,\nas\nfeared\nfrom\nthe\ntree\nstru ture\nof\nthe\ngame.\nThe\npresen\nt\nstudy\nis\nth\nus\nan\no\n  asion\nto\nrelate\nwith\nea \nh\nother,\non\na\nliv\ne\n ase,\nv\narious\nto\nols\nand\n on epts\natta \nhed\nto\ngames,\npro\n esses,\nprobabil-\nities,\n on\ntrol,\nprogramming,\nalgorithmi s\nand\n omplexit\ny\n,\nwith\nappli ations\nin\nmanagemen\nt\nand\ngame\npra ti e.\n2\nBa \nkw\nard\nindu tion\noptimal\np\noli y\nA\nsto\n \nhasti \ngame\nredu es\nto\na\npure\nde ision\ngame,\nb\ny\n onsidering\npro\nviden e\nas\na\nparti ular\npla\ny\ner\n[8,\n \nh.\n4℄,\nwhose\nmixed\nstrategy\n,\nkno\nwn\na\npriori,\nresults\nfrom\nusual\nstatisti al\np\nostulates\n(indep\nenden e,\nstationarit\ny)\nand\n annot\nb\ne\noptimized.\n1\n421,\nth\nus\nredu ed,\nand\nwith\nsome\npre autions\non\nthe\nrules\n(ap-\np\nendix\nA),\nis\na\np\nerfe t\ninformation\n\u001cnite\ngame\nand\nthe\nZermelo\ntheorem\napplies.\nI\nwill\nsolv\ne\nonly\na\nsub-game,\nthe\npla\ny\ner's\nround\nagainst\npro\nviden e\n(while\nother\npla\ny\ners\nstand\nstill),\na\nsto\n \nhasti \nmanagemen\nt\nproblem,\nfeaturing\na\nmar-\ntingale\nproblem\nand,\nfor\nthe\n\u001crst\npla\ny\ner,\na\nstopping\ntime\nproblem\n[9℄.\nThe\nanalogy\nwith\nBro\nwnian\nmotion\npro\nvides\nstatisti al\nme \nhani s\nto\nols.\n2.1\nAlea\nLet D ∈N\nb\ne\nthe\nn\num\nb\ner\nof\ndi e,\nnormally\n3,\nand F ∈N∗\nthe\nn\num\nb\ner\nof\nfa es\nof\nev\nery\ndi e,\nnormally\n6.\nDi e\nare\ndis ernible\n2\n,\nso\nthat\nthe\nprobabilit\ny\n1\nProbabilit\ny\ntheory\nb\negan\nas\nthe\nstudy\nof\nthe\npro\nviden\ntial\nstrategy\nin\n \nhan e\ngames,\nat\nthe\ntime\nof\nBa\ny\nes\nor\nthe\nBernoullis.\n2\nDis ernibilit\ny\nis\nnot\nan\ninno\n uous\nh\nyp\nothesis,\nas\nsho\nwn\nb\ny\nGibbs'\nparado\nx\n[10\n℄.\n2\nBA\nCKW\nARD\nINDUCTION\nOPTIMAL\nPOLICY\n5\nspa e\nis\nthe\nset\nof\nfa e\nsequen es,\nor\narrangemen\nts.\nThe\n lass\nof\narrange-\nmen\nts\n orresp\nonding\nto\nea \nh\nother\nb\ny\na\np\nerm\nutation\nis\na\n om\nbination,\ne.\ng.,\n\u0010nénette\u0011,\n221,\nis\nthe\nsubset\nof\narrangemen\nts {(1, 2, 2), (2, 1, 2), (2, 2, 1)} ,\nof\nredundan y\nthree.\nIn\n421,\nwhi \nh\ndi e\npro\ndu ed\nwhi \nh\nfa e\ndo\nes\nnot\nmatter,\nb\ne ause\nranking\ndep\nends\nonly\non\n om\nbination.\nI\ndes rib\ne\nthe\ndie\nsystem\nas\nin\nstatisti al\nme \nhani s:\nea \nh\ndie\nis\na\nparti-\n le,\nwith\nonly\none\nphase\nv\nariable,\nfa \ne.\nThe\nla\nws\nof\nme \nhani s\nare\nrepla ed\nb\ny\nusual\nstatisti al\nh\nyp\nothesis,\nabstra ting\n \nhan e\nfrom\nan\ny\nsp\ne i\u001c \nrandom\ngenerator.\nThe\nsystem\nis\ndes rib\ned,\nin\nLagrangian\nnotation,\nb\ny\na\nfa e\n om-\nbination,\nor,\nin\nEulerian\nnotation,\nb\ny\nthe\nsequen e df\nof\no\n  upation\nn\num\nb\ners\nof\nev\nery\nfa e f = 1 . . .F\n,\ne.\ng.\nthe\nLagrangian\nnotation\n421\ntranslates\nin\nto\nthe\nEulerian\nnotation (1, 1, 0, 1, 0, 0)\n(F = 6).\nThe\nin\nterest\nof\nEulerian\nnotation\nlies\nin\nthat\nthe\nset\nof\nEulerian\n om\nbi-\nnations\nd = (df)f=1...F.\nis\nthe\npartially\nordered\nnormed\nv\ne tor\nspa e ZF\n.\nThe\n anoni \nbasis eg =\n(δf,g)f\nis\naligned\nwith\n\u0010brelans\u0011,\n om\nbinations\nwith\nall\nfa es\nof\na\nkind.\nI\nde\u001cne\nthe\nball\nB(D ∈N) ≡{d ∈ZF, |d| ≤D}\nand\nsimilarly\n(repla ing ≤\nab\no\nv\ne\nb\ny =\nor <\n)\nthe\nsphere ∂B(D)\nand\nthe\nop\nen\nball ˘B(D).\nThe\nin\nterse tions\nwith\nthe\np\nositiv\ne\n one\nare\nrepresen\nted\nb\ny +\nexp\nonen\nts;\nthe\nset\nof\na tual\n om\nbinations\nis B+(D).\nThe\nnorm\nof\na\n om\nbination\nis\nthe\nsum\nof\nEulerian\n omp\nonen\nt\nabsolute\nv\nalues.\nThe\nnorm\nof\na\np\nositiv\ne\n om\nbination\nis\njust\nits\nn\num\nb\ner\nof\ndi e.\nThe\n anoni \norder ≤,\npartial\non ZF\n,\ndi\u001bers\nfrom\nthe\nhierar \nhi \norder ⪯\n(54),\ntotal\non ∂B+(D).\nDistin t\n asts\nare\nindep\nenden\nt\nand\nthe\nprobabilit\ny\nof\nan\ny\nfa e\nto\nb\ne\non\ntop\nis 1\/F\n(unloaded\ndi e).\nThe\narrangemen\nts\nof\none\n om\nbination\nare\nth\nus\nequiprobable,\nand\nthe\nprobabilit\ny\nof\na\n om\nbination\nis\njust\nthat\nof\nan\ny\nof\nits\narrangemen\nts,\ntimes\nthe\n om\nbination\nredundan y\n.\nF\nor\nexample,\nthe\nprob-\nabilit\ny\nof\nobtaining\nthe\n om\nbination\n21\nis 2\/F 2\n,\nwhile\nthe\nprobabilit\ny\nof\nobtaining\nthe\n om\nbination\n11\nis 1\/F 2\n.\nMore\ngenerally\n,\nthe\nprobabilit\ny\nof\nobtaining\nthe\n om\nbination d,\nafter\none\n ast,\nis\ngiv\nen\nb\ny\nthe\nm\nultinomial\nla\nw,\nwith\nusual\nnotations\ngeneralizing\np\no\nw\ner\nand\nfa torial\nto\nin\nteger\nv\ne tors:\np(d) = pd|d|!\nd! , p = 1\nF (1 . . . 1) ∈QF,\nX\nd∈∂B+(D)\np(d) = 1.\n(1)\n2\nBA\nCKW\nARD\nINDUCTION\nOPTIMAL\nPOLICY\n6\n2.2\nF\nate\nF\nor\nall j ∈N,\nlet\nthe\nstate dj\nb\ne\nthe\n om\nbination,\na  um\nulated\nafter j\n asts,\nand\nthe\nev\nen\nt dj+1\/2\nb\ne\nthe\n om\nbination,\nobtained\nfrom\nthe j + 1-th\n ast.\nF\nate\nis\nthe\nin\u001cnite\nstate\nand\nev\nen\nt\nalternate\nsequen e\nϕ ≡(d0, d1\/2, d1, d3\/2 . . . ).\n(2)\nThe\nin\nteger\nor\nhalf-in\nteger\nindex\nis\nused\nas\na\ndis rete\ntime,\nin\nteger\ntime\nfor\nstates,\nhalf-in\nteger\nfor\nev\nen\nts.\nThe\nset\nof\np\nossible\nfates\nis\ndes rib\ned\nb\ny\nthe\nfate\ntree,\nwhere\nbran \nhing\nrepresen\nts\n \nhan e\n(from\nin\nteger\ntime\nto\nhalf-in\nteger\ntime)\nor\nde ision\n( on\nv\nersely).\nThe\nrules\nof\n421\nimply:\nd0\n≡\n0,\n(3)\n∀j ∈N, dj+1\/2\n∈\nB+(Dj ≡D −|dj|),\n(4)\n0 ≤dj+1 −dj\n≤\ndj+1\/2,\n(5)\n∃(j ∈N, j ≤J), dj\n∈\nB+(D),\n(6)\nwhere J ∈N\nis\nthe\nmaxim\num\nround\nduration,\nnormally\n3. Dj\nis\nthe\nn\num\nb\ner\nof\nliv\ne\ndi e,\nwhi \nh\nha\nv\ne\nnot\nb\neen\na  um\nulated\nafter j\nev\nen\nts\nand\none\nstate.\nF\nrom\n(3,\n6), D0 = D, DJ = 0.\nF\nrom\n(4,\n5),\nDj+1\n≤\nDj,\n(7)\n(dj+1 −dj = dj+1\/2)\n⇔\ndj+1 ∈B+(D).\n(8)\nThe\ne\u001be tiv\ne\nround\nduration J1\nis\nthe\nminim\num\nof j\nin\n(6).\nThe\nnext\npla\ny\ners'\ne\u001be tiv\ne\nround\ndurations\nm\nust\nequal\nthe\n\u001crst\npla\ny\ner's.\nTherefore,\nfor\nall\npla\ny\ners,\n∀(j ∈N, j < J1), dj ∈˘B+(D)\n,\ndj+1\/2 ̸= 0,\n(9)\ndJ1 ∈B+(D)\n,\n(10)\n∀(j ∈N, j > J1), dj−1\/2 = 0\n,\ndj = dJ1.\n(11)\n(9,\n10)\nare\nused,\n\u001crstly\n,\nafter\nthe\n\u001crst\npla\ny\ner's\nend\nof\nround,\nto\ndetermine\nJ1\n,\nsubsequen\ntly\n,\nas\nadditional\nrules\nfor\nnext\npla\ny\ners.\nWhen j\nin reases,\nthe\nstate\nv\ne tor dj\nmo\nv\nes\nin\nthe\np\nositiv\ne\nball,\no\u001b\nthe\norigin,\nto\nw\nards\nits\nb\noundary\nwhere\nit\ngets\nstu \nk\nat dJ1\n,\nthe\nround\nr\nesult.\nF\nate\nis\nvirtually\n on\ntin\nued\nb\ny\nan\nin\u001cnite\nsequen e,\nasymptoti ally\nalternating\nthe\nresult\nand\nthe\nn\null\nev\nen\nt.\n2\nBA\nCKW\nARD\nINDUCTION\nOPTIMAL\nPOLICY\n7\n2.3\nUtilit\ny\nF\nollo\nwing\nv\non\nNeumann\nand\nMorgenstern\n[7,\n \nh.\n27℄,\na\npla\ny\ner's\nutilit\ny\nis\na\nn\num\nb\ner,\ngiv\nen\nb\ny\na\n ausal\nfun tion,\ni.\ne.\na\nfun tion\nof\nhistory\n(past\nfate),\n ompatible\nwith\nthe\npla\ny\ner's\npreferen es,\nand\nsu \nh\nthat\nthe\nutilit\ny\nb\nefore\na\nrandom\nev\nen\nt\nis\njust\nthe\nexp\ne ted\nutilit\ny\n,\ni.\ne.\nthe\nprobabilit\ny-w\neigh\nted\nutilit\ny\na\nv\nerage,\no\nv\ner\np\nossible\nout omes.\nTh\nus,\nexp\ne ted\nutilit\ny\nis\nan\nti- ausal,\ni.\ne.\npres ribing\nutilities\nat\nsome\nfuture\ntime\ndetermines\nits\nexp\ne tation\nat\nall\nprior\ntimes.\nOne\nnev\ner\nkno\nws\nwhen\na\ngame\na tually\nstops,\nas\na\nit\nis\noften\nem\nb\nedded\nin\na\nlarger\ngame.\nT\nennis\nis\na\nfamiliar\nexample:\na\ntennis\n\u0010game\u0011\nis\na\nsub-\ngame\nof\na\nset,\nitself\na\nsub-game\nof\na\nmat \nh,\ntournamen\nt,\nranking\nsystem . . .\nthis\n as ade\ndo\nes\nnot\nev\nen\nstop\nwith\na\npla\ny\ner's\nlife,\nb\ne ause\nof\n o\nop\neration\nb\net\nw\neen\nindividuals.\nBut,\nif\nw\ne\nw\nan\nt\nto\nobtain\nan\ny\nresult,\nw\ne\nm\nust\nstop\nsomewhere\nin\nthe\ngame\n as ade,\nand\njudge\nutilit\ny\nmore\nor\nless\nempiri ally\n.\n(Quite\nsimilarly\n,\nin\nme \nhani s\nor\nthermo\ndynami s,\nthe\nstudied\nsystem\nis\n oupled\nwith\nthe\nrest\nof\nthe\nw\norld,\nb\ny\nan\noften\ndeli ate\nb\noundary\nor\n ut-o\u001b\n ondition.)\nThe\nstudy\nof\n421\nshould\nstop\nat\nend\nof\ngame,\nb\ny\nsetting\npla\ny\ners'\nutilities,\nfor\nexample,\na\nbinary\nutilit\ny:\none\nfor\nwin,\nzero\nfor\nloss,\nor\nin orp\norating\ne onom\ny\n,\nà\nla\nBernoulli,\nthe\nlogarithm\nof\nearning\ndivided\nb\ny\nw\nealth\n[11\n℄.\nHo\nw\nev\ner,\nI\ntreat\nonly\nthe\nround.\nA\nt\nend\nof\nround,\nthe\nBernoulli\nform\nula\ndo\nes\nnot\nmak\ne\nsense\nand\nutilit\ny\nis\nnot\ngiv\nen\ndire tly\nb\ny\nthe\nrules\n(in\nparti ular,\nthe\ntransfer\nfun tion\nof\ntable\n5).\nBy\nexamining\nthe\nrules,\na\nfew\nprop\nerties\nof\nutilit\ny\nare\nobtained;\nfor\nexample,\nat\n onstan\nt\ntime,\nfor\na\nrational\npla\ny\ner,\nutilit\ny\nm\nust\nb\ne\n ompatible\nwith\nthe\nhierar \nhi \norder\n(54),\net .\nBut\nI\nwill\nnot\nfurther\n \nhara terize\nutilit\ny\n.\nOn\nthe\n on\ntrary\n,\nI\nwill\n onsider\nthe\nround\nindep\nenden\ntly\nof\nthe\nrest\nof\nthe\ngame,\nwith\narbitrary\nutilities,\nin\norder\nto\ntreat\nthe\nproblem\nof\nfate\nsto\n \nhasti \nmanagemen\nt\nin\na\nrather\ngeneral\nw\na\ny\n.\nF\nor\nall\nfate ϕ\n(2),\nutilit\ny\nis\njudged\nat\nsome\ntime Jϕ\n,\neither\nin\nteger\nor\nhalf-\nin\nteger\nin\ngeneral\n(in\nthe\nround, Jϕ ∈{1\/2, 3\/2, 5\/2} ),\nas\na\n ausal\nfun tion:\nu(d0, d1\/2 . . . dJϕ) ∈Q.\n(12)\nThe\nfun tion u\nhas\na\nv\nariable\nn\num\nb\ner\nof\nargumen\nts,\nformally\n,\nit\nis\nde\u001cned\non S\nj∈N∗B+(D)j\n.\nUtilit\ny\nis\njudged\nforev\ner:\nu(. . . dJϕ, .) ≡u(. . . dJϕ).\n(13)\n2\nBA\nCKW\nARD\nINDUCTION\nOPTIMAL\nPOLICY\n8\nThe\nrules\n(4,\n5,\n6,\n9,\n10)\nare\nsup\nerseded\nb\ny −∞\nutilities\nfor\nrule\nbreaking\nhistories\n(ex luding\n \nheating).\nIn\nparti ular,\nthe\nutilitarian\nv\nersion\nof\n(4)\nis\n∀(j ∈N, dj+1\/2 \/∈B+(Dj)), u(. . .dj, dj+1\/2) = −∞\n(14)\nand\nthe\nnext\npla\ny\ners'\nround\nduration\n onditions\n(9,\n10)\nb\ne ome\n∀(j ∈N, j < J1, dj ∈∂B+(D)), u(. . .dj) = −∞.\n(15)\n2.4\nOptimal\nstrategies\nThe\ngreatest\nutilit\ny\n,\ndra\nwn\nfrom\nan\ny\nev\nen\nt-terminated\nhistory\n,\nis\n∀j ∈N, u(. . .dj, dj+1\/2)\n=\nmax\ndj+1 u(. . .dj, dj+1\/2, dj+1),\n(16)\nu(.)\n=\nmax\nd1 u(., d1).\n(17)\nThe\nlatter\nequation,\nwhere d1\nis\na\ndumm\ny\nv\nariable,\nis\na\nmore\nformal\nexpres-\nsion\nof\nthe\nformer.\nThe\nnature\nof\nthe\ndumm\ny\nv\nariable\nis\nsho\nwn\nb\ny\nits\nindex\n(state\nfor\nin\nteger,\nev\nen\nt\nfor\nhalf-in\nteger).\nThe\nset\nof\nstates,\n orresp\nonding\nto\noptimal\nde isions,\nis\nSu(.) ≡argmax\nd1\nu(., d1).\n(18)\nA\npla\ny\ner's\nmixed\nstrategy\n onsists\nin\n \nho\nosing\nrandomly\nb\net\nw\neen\nman\ny\nde isions,\na  ording\nto\na\n ausal\nprobabilit\ny\nla\nw,\nd 7→P(., d) ≡P(d1 = d|.),\nX\nd1\nP(., d1) = 1.\n(19)\nP(X)\nmeans\nthe\nprobabilit\ny\nof\nthe\nev\nen\nt X\n.\nThe\noptimal\nmixed\nstrategies\nare\nsu \nh\nthat\nthe\nsupp\nort\nof\nthe\nprobabilit\ny\nla\nw\n(19)\nis\na\nsubset\nof Su(.)\n(among\nthem\nare\npure\noptimal\nstrategies).\nF\nrom\nthe\nv\non\nNeumann-Morgenstern\ntheorem,\nu(.) =\nX\nd1\/2\np(., d1\/2)u(., d1\/2).\n(20)\nwhere p\nis\na\n ausal\nprobabilit\ny\nla\nw,\nexpressing\nthe\npro\nviden\ntial\nstrategy\nand\nrules.\nBe ause\nof\nutilit\ny\n onditions,\nsu \nh\nas\n(14,\n15),\nthere\nare,\nin\n(20),\n2\nBA\nCKW\nARD\nINDUCTION\nOPTIMAL\nPOLICY\n9\npro\ndu ts p × u\nof\nthe\nundetermined\nform 0 × ∞,\nwhi \nh\nough\nt\nto\nb\ne\nrepla ed\nb\ny\nzero\n(or\nthe\nsummation\nough\nt\nto\nb\ne\nprop\nerly\nrestri ted).\nCom\nbining\n(17,\n20),\nor\n on\nv\nersely\n,\nu(.)\n=\nmax\nd1\nX\nd3\/2\np(., d3\/2)u(., d1, d3\/2),\n(21)\nu(.)\n=\nX\nd1\/2\np(., d1\/2) max\nd1 u(., d1\/2, d1).\n(22)\nThe\n omp\nosition\nof max −moy\nop\nerations\nnames\nthe\nalgorithm,\nwhi \nh\nis\nthe\n lassi al\nzero-sum\ngame max −min,\nwhere\nthe\nrational\nopp\nonen\nt\nhas\nb\neen\nrepla ed\nb\ny\nneutral\npro\nviden e.\n(21,\n22)\nare\n onsisten\nt\nwith\n(13):\nafter\nthe\njudgmen\nt,\nthey\nsimply\nrep\neat\nthe\nutilit\ny\nforev\ner,\nso\nthat\nthe max −moy\nop\nerations\n an\nb\ne\n \nhained\nad\nin\u001cnitum,\nno\nmatter\nthe\nend\nof\nround.\nTh\nus,\nthe\njudgmen\nt\n an\nb\ne\narbitrarily\np\nostp\noned,\nwithout\na\u001be ting\nstrategy\n.\nIf\njudgmen\nt\ntimes\nha\nv\ne\nan\nupp\ner\nb\nound\n(e.\ng.\nthe\nn\num\nb\ner\nof\nfates\nis\n\u001cnite),\nthen\nall\njudgmen\nts\n an\nb\ne\np\nostp\noned\nun\ntil\na\n( olle tiv\ne)\nlast\njudgmen\nt\na\ntime\nJ ∈N, J ≥max Jϕ\n,\ne.\ng.\nthe\nmaxim\num\nround\nduration.\nRelaxing\nthe\nrule\n(3),\nand\ntaking d0\nas\na\nparameter,\nthe\nproblem\nof\nfate\nmanagemen\nt,\ni.\ne.\n\u001cnding\noptimal\nstrategies,\nis\nself-similar\nunder\ntime-\nshifts,\nex ept\nfor\nthe\nparameter\n\u0010renormalization\u0011\n(as\nin\nstatisti al\nph\nysi s)\n(D0, J, d0, (. . . dj), (p, u)(.)) →(Dj, J −j, dj, (), (p, u)(. . .dj, .)).\n(23)\nLet χV\nb\ne\nthe\n \nhara teristi \nfun tion\nof V ⊂B(D).\nThe\nround\npro\nviden-\ntial\nstrategy\nis\ndetermined\nb\ny\n(1)\nand\np(., d0, d1\/2) ≡p(d1\/2)χ∂B+(D0)(d1\/2).\n(24)\nThe\nexp\ne ted\nutilit\ny\nis\n omputed\nwith\n(17,\n20),\nfrom\nthe\nlast\njudgmen\nt\nba \nk-\nw\nard\nin\ntime:\nu(. . .dJ−1, dJ−1\/2), u(. . . dJ−1) . . . u(d0, d1\/2, d1), u(d0, d1\/2), u(d0),\n(25)\ne.\ng.,\nfor J = 3,\nand\nusing\n(24),\nu(. . .d3\/2) = max\nd2\nX\nd5\/2∈∂B+(D2)\np(d5\/2)u(. . . d3\/2, d2, d5\/2),\nu(d0, d1\/2) = max\nd1\nX\nd3\/2∈∂B+(D1)\np(d3\/2)u(d0, d1\/2, d1, d3\/2).\nu(d0) =\nX\nd1\/2∈∂B+(D0)\np(d1\/2)u(d0, d1\/2).\n3\nF\nA\nTE\nAS\nA\nSTOCHASTIC\nPR\nOCESS\n10\n3\nF\nate\nas\na\nsto\n \nhasti \npro\n ess\nF\nor\na\ngiv\nen\nstrategy\n,\nwhat\nis\nthe\npresen e\ndensit\ny\n(of\nthe\ndie\nsystem\nin\na\nsubset\nof\nphase\nspa e)?\nWhat\nis\nthe\nexp\ne tation\nof\nan\narbitrary\nutilit\ny\n,\nfor\nwhi \nh\nthe\ngiv\nen\nstrategy\nis\nnot\nne essarily\noptimal?\n3.1\nThe\nK\nolmogoro\nv\nequation\non\nexp\ne ted\nutilit\ny\nF\nate\nis\na\nsto\n \nhasti \npro\n ess,\nnot\nonly\nb\ne ause\nit\n on\ntains\nrandom\nev\nen\nts\n(the\nprobabilit\ny\nla\nw p),\nbut\nalso\nrandom\nde isions,\na  ording\nto\nmixed\nstrategies\n(the\nprobabilit\ny\nla\nw P\n).\nF\nor\nan\ny\n ausal\npro\n ess\nlik\ne\n(2),\nthe\nsequen e\nof\nhistories\n(d0), (d0, d1\/2), (d0, d1\/2, d1) . . .\nis\na\ndis rete\nMark\no\nv\n \nhain,\nfor\nwhi \nh\n lassi al\nresults\nare\na\nv\nailable\n[12\n,\n \nh.\n6℄,\n[13\n,\n \nh.\n15℄,\noriginating\nmostly\nfrom\nBro\nwnian\nmotion\nstudies\n[10,\n \nh.\n15℄.\nThe\nfate\nsto\n \nhasti \nev\nolution\nequation,\nthe\nLangevin\nequation,\nis\njust\na\nrandom\nsum,\nob\neying\n(4,\n5):\ndj+1\n=\ndj + ˆdj+1,\nP(ˆdj+1 = d| . . .dj, dj+1\/2)\n=\nP(. . . dj, dj+1\/2, dj + d),\nP(dj+1\/2 = d| . . . dj)\n=\np(. . . dj, d).\nˆdj+1\nis\na\nrandom\nsour e\nterm,\n onditioned\nb\ny\nhistory\n,\na  ording\nto\nthe\nmixed\nstrategies P, p. dj\nundergo\nes\na\nstrategy-driv\nen\nBro\nwnian\nmotion\nas,\nfor\nex-\nample,\na\n \nharged\nBro\nwnian\nparti le\ndriv\nen\nb\ny\nele trophoresis.\nThe\nChapman-K\nolmogoro\nv\nequation\nyields\nthe\nprobabilit\ny\nof\ntransition,\nor\njump,\nfrom\none\nstate\nto\nthe\nother,\nin\none\ntime\nstep:\nσ(., d0 ↷d1) ≡P(d1 = d|., d0) =\nX\nd1\/2\np(., d0, d1\/2)P(., d0, d1\/2, d).\n(26)\nLet P\nb\ne\na\npla\ny\ner's\nmixed\nstrategy\n,\np\nossibly\nnot\noptimal.\nF\nrom\nthe\nv\non\nNeumann-Morgenstern\ntheorem,\nt\nwi e\napplied,\nu(., d0) =\nX\nd1\/2\np(., d0, d1\/2)\nX\nd1\nP(., d0, d1\/2, d1)u(., d0, d1\/2, d1).\n(27)\n3\nF\nA\nTE\nAS\nA\nSTOCHASTIC\nPR\nOCESS\n11\nRev\nersing\nthe\norder\nof\nsummation,\nusing\n(26)\nand\nassuming\nthat\nutility\ndo\nes\nnot\ndep\nend\non\nevents,\nbut\nonly\non\nstates,\nwhi \nh\nis\ntrue\nin\nthe\n421\nround,\nI\nobtain\nthe\nK\nolmogoro\nv\nequation\non\nthe\nexp\ne ted\nutilit\ny:\nu(., d0) =\nX\nd1\nσ(., d0 ↷d1)u(., d0, ∗, d1).\n(28)\n(By\nh\nyp\nothesis, u\ndo\nes\nnot\ndep\nend\non ∗.)\nAs\nopp\nosed\nto\nthe max −moy\nalgorithm,\n(28)\ndo\nes\nnot\npro\ndu e\nan\ny\nde-\n ision,\nbut,\ngiv\nen\nthe\nmixed\nstrategies P, p\n(e\u001be tiv\ne\nthrough σ\n),\ndetermines\nthe\nexp\ne tation\nof\nan\ny\nutilit\ny\n,\nfor\nwhi \nh P\nma\ny\nnot\nb\ne\noptimal.\nNev\nertheless,\nif P\nis\noptimal,\nfrom\n(17)\nand\n(19),\nthere\nis\nan\nequalit\ny\n,\nb\net\nw\neen\nop\nerators\non u(., d0, d1\/2, d1):\nmax\nd1\n=\nX\nd1\nP(., d0, d1\/2, d1).\n(29)\nT\naking\n(29)\nin\nto\n(27)\nreturns\n(22).\n3.2\nThe\nF\nokk\ner-Plan \nk\nequation\non\npresen e\ndensit\ny\nI\nde\u001cne\nthe\nstate\nfate ψ ≡(dj)j=0,1...\n(fate\nwith\nonly\nstates,\nnot\nev\nen\nts).\nF\nrom\n(26),\nP(ψ = (., d0, d1)) = σ(., d0 ↷d1)P(ψ = (., d0)),\n(30)\nso\nthat\nthe\nsequen e\nof\npast\nstates\n(d0), (d0, d1), (d0, d1, d2) . . .\nalso\nis\na\nMark\no\nv\n \nhain.\nSumming\n(30)\no\nv\ner\nall\nstate\nfates\n on\nv\nerging\nto\nthe\nsame\nstate d\nat\ntime\nj + 1\ngiv\nes\nthe\npresen e\ndensit\ny ρj+1(d):\nρ0(d)\n=\nδd,d0,\n(31)\n∀j ∈N, ρj+1(d)\n=\nX\nd0...dj\nP(ψ = (d0 . . . dj)).\n(32)\nIn\nthe\nround,\nfrom\n(11), ρj\nis\nstationary\n,\nas\nso\non\nas j ≥J\n.\n3\nF\nA\nTE\nAS\nA\nSTOCHASTIC\nPR\nOCESS\n12\nI\nassume\nthat\nutility\nis\na\nfun tion\nof\nstate\nand\ntime\nonly,\nless\ngeneral\nthan\n ausal\n(12):\nu(. . . dJϕ) = uJϕ(dJϕ).\n(33)\nThe\nend-of-round\nutilit\ny\nis\nindeed\nof\nthe\nkind\n(33),\nb\ne ause\nend-of-set\nranking\n(see\nthe\nrules)\nonly\ndep\nends\non\nround\nresults,\nnot\non\nin\ntermediary\nstates\nand,\nfor\nthe\n\u001crst\npla\ny\ner,\nthe\ne\u001be tiv\ne\nround\nduration.\nF\nor\nall\npla\ny\ner's\noptimal\n(or\nrational)\nmixed\nstrategy P\nderiv\ned\nfrom\na\nutilit\ny\nof\nthe\nkind\n(33),\nPj(dj, dj+1\/2, dj+1)\n≡\nP(., dj, dj+1\/2, dj+1),\n(34)\nσj(dj ↷dj+1)\n≡\nσ(., dj ↷dj+1)\n(35)\nT\naking\n(33,\n35)\nin\nto\n(28)\nallo\nws\nto\nextend\n(33)\nto\nall\ntime\n(for\nthe\nexp\ne ted\nutilit\ny),\nb\ny\nindu tion:\n∀j ∈N, uj(dj) ≡u(. . .dj).\n(36)\nThe\npro\n ess (j, dj, dj+1\/2)\nis\nMark\no\nvian.\nThe\n onsequen e\n(35)\nof\n(33),\ntak\nen\nin\nto\n(32),\nallo\nws\nto\nexpress ρj+1(dj+1)\nas\na\nfun tional\non ρj\n:\nρj+1(dj+1) =\nX\ndj\nρj(dj)σj(dj ↷dj+1),\n(37)\nthe\nF\nokk\ner-Plan \nk\nequation.\nAs\nopp\nosed\nto\n(37),\n(28)\ndo\nes\nnot\nneed\n(33).\nNev\nertheless,\nwith\n(33),\n(28),\nb\ne omes\nuj(dj) =\nX\ndj+1\nσj(dj ↷dj+1)uj+1(dj+1),\n(38)\nadjoin\nt\nto\n(37).\n(37,\n38)\nare\nthe\nev\nolution\nequations,\nadjoin\nt\nto\nea \nh\nother,\nlinear,\nunsta-\ntionary\n,\nof\npresen e\ndensit\ny\nand\nexp\ne ted\nutilit\ny\n.\nTheir\ninputs\nare\na\npla\ny\ner's\nmixed\nstrategy\nand\nutilit\ny\n.\n3\nF\nA\nTE\nAS\nA\nSTOCHASTIC\nPR\nOCESS\n13\n3.3\nComputing\nresult\nprobabilities\nb\ny\ndualit\ny\nLet F ≡F(B+(D), Q)\nb\ne\nthe\nspa e\nof\nn\numeri al\nfun tions\non B+(D),\nwith\nthe\ns alar\npro\ndu t\n∀(f, g) ∈F, ⟨f, g⟩≡\nX\nd∈B+(D)\nf(d)g(d).\n(39)\nσj\nis\nan\nop\nerator,\na\nlinear\nendomorphism\non F\n,\nfully\ndetermined\nb\ny\nthe\nMark\no\nvian\nmatrix σj(d0 ↷d1).\nIts\ntransp\nosed\nop\nerator\nis σt\nj\n,\nof\nmatrix\nσt\nj(d1 ↷d0) ≡σj(d0 ↷d1).\nIn\nop\nerator\nnotation,\n(37,\n38)\nb\ne ome\nρj+1 = σt\njρj, uj = σjuj+1.\nAs σt\nj\net σj\nare\nadjoin\nt\nto\nea \nh\nother,\nthe\nexp\ne ted\nutilit\ny\nfollo\nws\na\n on-\nserv\nation\nla\nw:\n⟨uj, ρj⟩= ⟨σjuj+1, ρj⟩\n=\n⟨uj+1, σt\njρj⟩= ⟨uj+1, ρj+1⟩,\n⟨uj, ρj⟩\n=\n⟨u0, ρ0⟩= u0(d0).\n(40)\nThe\nlast\nequalit\ny\nis\na\n onsequen e\nof\n(31).\nGiv\nen\nthe\npla\ny\ner's\nmixed\nstrategy\nP\n,\n(40)\nholds\nfor\nan\ny\nutilit\ny\n.\nThe\ndire t\n omputation\nof ⟨uj, ρj⟩\n onsists\nin\nsolving\nfor ρj(dj)\nthe\nF\nokk\ner-\nPlan \nk\nequation,\nwhi \nh\nm\nust\nb\ne\nrep\neated,\nto\n omplete\nthe\ns alar\npro\ndu t,\nat\nleast\nfor\nall dj\nwhere uj\ndo\nes\nnot\nv\nanish.\nMore\nshrewdly\n, ⟨uj, ρj⟩\n an\nb\ne\n omputed\nindire tly\n,\nas\nthe\nr.\nh.\ns.\nof\n(40):\nthe\nK\nolmogoro\nv\nequation\nis\nsolv\ned\nonly\non e\nfor\nthe\nexp\ne ted\nutilit\ny\nat\nthe\ntrunk\nof\nthe\nfate\ntree,\nor\nthe\ninitial\nexp\ne ted\nutilit\ny\n.\nThe\nindire t\n omputation\nis\nfaster\nthan\nthe\ndire t\n omputation,\nb\ny\na\nfa tor\nwhi \nh\nis\nthe\n ardinal\nof\nthe\nsupp\nort\nof uj\n.\nThe\nindi-\nre t\n omputation\nb\nene\u001cts\nfrom\nthe\nuni it\ny\nof\nthe\nfate\ntree,\nand\nthe\ndi\u001busiv\ne\ngro\nwth\nof\nthe\nsupp\nort\nof uj\n.\nMoreo\nv\ner,\nto\nobtain\nthe\nK\nolmogoro\nv\nalgorithm\nfrom\nthe max −moy\nal-\ngorithm,\none\nmerely\nhas\nto\nrepla e,\nin\n(22),\nthe\nop\nerator max\napp\nearing\nat\nthe\nl.\nh.\ns.\nof\n(29),\nb\ny\nthe\nop\nerator strat\napp\nearing\nat\nthe\nr.\nh.\ns.\nof\n(29).\n(These\nop\nerators\ndi\u001ber\nif P\nis\nnot\noptimal.)\nThe\nK\nolmogoro\nv\nequation\nis\nth\nus\nsolv\ned\nb\ny\na strat −moy\nalgorithm.\nHere\nare\nexamples\nof\nusing\nthe\nK\nolmogoro\nv\nequation\nand\n(40):\n4\nSIMPLE\nOPTIMAL\nPOLICIES\nF\nOR\nONE-GO\nAL\nUTILITIES\n14\n1.\nThe\nprobabilit\ny\nof\nthe\nresult\nto\nb\ne\nin V ⊂B+(D)\n(indep\nenden\ntly\nof\ntime)\nis\nthe\ninitial\nexp\ne tation\nof\nthe\nstationary\nutilit\ny uj = χV\n.\n2.\nThe\nprobabilit\ny\nof Dj0\nis\nthe\ninitial\nexp\ne tation\nof\nthe\nutilit\ny uj =\nδj,j0χ∂B+(D−Dj0)\n.\n3.4\nAnalogy\nwith\nlinear\ntransp\nort\ntheory\nThe\nround\nis\na\nlinear\ntransp\nort\nphenomenon,\nwith\nresp\ne t\nto\nthe\nfa e\nv\nariable.\nF\na e,\nexp\ne ted\nutilit\ny\n,\npresen e\ndensit\ny\n,\ntransition\nprobabilit\ny\n orresp\nond\nre-\nsp\ne tiv\nely\n,\nin\ntransp\nort\ntheory\n[14℄,\nto\nphase\n(p\nosition,\nv\nelo\n it\ny),\nimp\nortan e\n[15\n℄,\n\u001dux\nand\n ross\nse tion.\nHarris\n[16℄\nsho\nws\nthat\na\nmonokineti \nparti le\np\nopulation\ngro\nwn\nb\ny\nbran \nhing\n(e.\ng.\nneutrons\npro\ndu ed\nb\ny\nn\nu lear\n\u001cssion)\nfollo\nws\na\nGalton-W\natson\npro\n ess.\nSimilarly\n,\nin\napp\nendix\nB,\nI\ndis uss\nthe\nGalton-W\natson\n \nhara ter\nof\nthe\n\u001crst\npla\ny\ner's\nliv\ne\ndi e\np\nopulation Dj\n.\n4\nSimple\noptimal\np\noli ies\nfor\none-goal\nutilities\nT\naking\nfor\ngoal\na\nunique\n om\nbination d∗∈∂B+(D),\nthe\nutilit\ny\nis\na\nbinary\nKrone \nk\ner\nfun tion δd∗,.\n(mo\ndulo\nan\na\u001ene\ntransform),\nand\noptimal\nstrategies\nare\nsimply\n onstru ted.\n4.1\nThe\nrat \nhet\nand\nBernoulli\np\noli ies\nI\nexamine\nt\nw\no\n\u001crst\npla\ny\ner's\np\noli ies,\nwith\na\none-goal\nutilit\ny:\n1.\nThe\nBernoulli\np\noli y\n onsists\nin\na  um\nulating\nno\ndie,\nunless\nthe\ngoal\nhas\nb\neen\nattained\n(then,\nall\ndi e\nare\na  um\nulated);\nthe\n ast\nsequen e\nis\na\nstationary\nBernoulli\npro\n ess\n(a\nsequen e\nof\nindep\nenden\nt\ntrials\nter-\nminated\nb\ny\nsu  ess\nor\nfailure).\n2.\nThe\nrat \nhet\np\noli y\n onsists\nin\nputting\naside\nas\nman\ny\ndi e\nas\np\nossible,\n on\ntributing\nto\nthe\ngoal:\n∀(j ∈N, j + 1 < J), dj+1\n=\nd∗∧(dj + dj+1\/2),\n(41)\nPj(dj, dj+1\/2, dj+1)\n=\nδdj+1,d∗∧(dj+dj+1\/2).\n(42)\n4\nSIMPLE\nOPTIMAL\nPOLICIES\nF\nOR\nONE-GO\nAL\nUTILITIES\n15\n(∧\nis\nthe\nin\u001cx\nnotation\nof\nthe\nminim\num\nin\nthe\npartially\nordered\nspa e ZF\n,\ngeneralizing,\nin\nLagrangian\nnotation,\nthe\nensem\nble\nin\nterse tion ∩.)\nThe\nrat \nhet\nstrategy\nto\nw\nards d∗\nis\noptimal,\nwith\nresp\ne t\nto\nthe d∗\n-goal\nutilit\ny\n,\nif\nand\nonly\nif p\nde reases\nin B+(D).\nThis\nmeans\nthat\nas\nman\ny\ndi e\nas\np\nossible\nshould\nb\ne\na  um\nulated,\nin\norder\nto\nmaximize\nthe\nsu  ess\nprobabilit\ny\nat\nan\ny\nfuture\ntime.\nF\nor\nunloaded\ndi e,\nfrom\n(1),\n∀(d ∈B+(F), d + e1 ∈B+(F)), p(d + e1)\np(d)\n= 1\nF\n|d| + 1\nd1 + 1 ≤1,\n(43)\ni.\ne. p\nde reases\non B+(F).\nThe\nrat \nhet\nstrategy\nis\noptimal\nif\nand\nonly\nif\nD ≤F\n,\nstri tly\nif\nand\nonly\nif D < F\n.\nF\nor\nexample,\nwith D = 3 < F = 6, J > 1, d∗= 421, d1\/2 = 651,\nthe\nrat \nhet\nde ision\n(to\na  um\nulate\n1)\nis\noptimal,\nb\ne ause p(421) < p(42)\n(it\nwill\nb\ne\neasier\nto\nobtain\n42\nthan\n421).\nWith D = 3 > F = 2, d∗= 211, d1\/2 = 222,\nthe\nBernoulli\nde ision\n(to\nrepla\ny\nall\ndi e)\nis\noptimal,\nb\ne ause p(11) = 1\/F 2 =\n2\/8 < p(211) = 3\/F 3 = 3\/8 .\nWith D = F = 2, d∗= 21, d1\/2 = 11,\nb\noth\nBernoulli\nand\nrat \nhet\nde isions\nare\noptimal.\nA\nnext\npla\ny\ner's\nmaxim\num\nround\nduration\nis\nimp\nosed.\nIn\n ase\nof\na\nprema-\nture\nsu  ess,\nhe\nis\nin\na\ndilemma,\nha\nving\nto\nde ide\nb\net\nw\neen\nequally\nunpleasan\nt\nw\na\nys\nof\nbreaking\nthe\ngoal,\nobtained\nto\no\nearly\n.\nF\nor F > 2 ,\noptimal\nde isions\n onsist\nin\nrepla\nying\nan\ny\none\ndie;\nthe\nn\num\nb\ner\nof\npure\noptimal\nstrategies\nis\nth\nus\nthe\nn\num\nb\ner\nof\ndistin t\nfa es\nin\nthe\ngoal\n om\nbination,\nat\nthe\np\no\nw\ner J −1 .\nIf\nthe\ngoal\nis\na\nbrelan,\nthen\nno\ndilemma\nexists.\n4.2\nOptimal\none-goal\nstrategy\nresult\nprobabilities\nF\nor\nan\ny\nstrategy\n,\nI\n onsider\nthe\nprobabilit\ny\nto\nobtain\nan\ny\nresult,\ne.\ng.\n111\nafter\nthree\n asts.\nA\n  ording\nto\nse tion\n3.3,\nthis\nprobabilit\ny\nis\nthe\ninitial\nexp\ne ted\nutilit\ny\n,\ndetermined\nb\ny\nthe\nK\nolmogoro\nv\nequation\nand\nthe\n\u001cnal\n on-\ndition\nof\na\nKrone \nk\ner\nutilit\ny\non\nthe\nresult.\nThis\nprobabilit\ny\ndep\nends\non\nthe\npla\ny\ner i = 1, 2\n(\u001crst\nor\nnext),\nthe\n(renormalized)\nmaxim\num\nround\nduration\nJ1\n,\nthe\npla\ny\ner's\nmixed\nstrategy P\n,\nthe\ndela\ny j\n,\nand\nthe\nresult d:\npi(J1, P, j, d), 0 ≤j ≤J1 ≤J, d ∈B+(D).\n(44)\nThe\nset\nof\nresult\nprobabilities,\nfor\nall\np\nossible\npure\nstrategies\nand (D, F, J) =\n(3, 6, 3),\nis\n(m\nu \nh\nlarger\nthan\nthe\nfate\ntree,\nitself\nv\nery\nlarge\nand)\nto\no\nlarge\nto\nb\ne\nextensiv\nely\nlisted.\nTh\nus,\nI\nwill\nw\nork\non\na\nredu ed\nstrategy\nsubset,\nfor\n4\nSIMPLE\nOPTIMAL\nPOLICIES\nF\nOR\nONE-GO\nAL\nUTILITIES\n16\nwhi \nh\na\nreasonable\n \nhoi e\nis\nthe\nset\nof\noptimal\none-goal\nstrategies,\nfor\nall\np\nos-\nsible\ngoals.\nAs\nfar\nas\nthe\ngoal\ndetermines\nthe\noptimal\nstrategy\n,\nthe\nv\nariable\nP\nin\n(44)\nis\nsimply\nrepla ed\nb\ny\nthe\ngoal d∗\n:\npi(J1, d∗, j, d), 0 ≤j ≤J1 ≤J, (d, d∗) ∈B+(D)2\n(45)\nwhi \nh\nlo\noks\nlik\ne\nthe\nMark\no\nvian\nmatrix\nof\nse tion\n3.3,\nex ept\nthat d∗\nis\nnot\na tual,\nbut\n on\ntemplated.\nThere\nare\ndiagonal\n(d = d∗\n)\nand\nnon-diagonal\nresult\nprobabilities.\nF\nor\nthe\n\u001crst\npla\ny\ner,\nthe\noptimal\none-goal\nstrategy\nis\nunequiv\no\n ally\nde\u001cned\nb\ny\nthe\ngoal\n(D < F\n:\nthe\nrat \nhet)\nand\nthe\nfun tion p1\nis\nde\u001cned\nev\nerywhere.\nThis\nin\nnot\ntrue\nfor p2\n,\nb\ne ause\nof\ndilemmas.\nHo\nw\nev\ner,\nnext\npla\ny\ner\ndiagonal\nprobabilities\nare\nuna\u001be ted\nb\ny\ndilemmas,\nso\nthat p2\nis\nde\u001cned\non\nthe\ndiagonal,\nd = d∗\n;\nit\nis\nev\nen\nde\u001cned\nfor\nall (d∗, d),\nif\nand\nonly\nif d∗\nis\na\nbrelan,\nsin e\nbrelans\ndo\nnot\npro\ndu e\ndilemma,\nas\nnoti ed\nat\nend\nof\nse tion\n4.1.\nHere\nare\na\nfew\nprop\nerties\nof\nthe\nfun tions pi\n:\npi(0, d∗, 0, d)\n=\nδd∗,d,\n(46)\npi(J, 0, j, 0)\n=\nδj,0,\n(47)\npi(1, d∗, 1, d)\n=\np(d),\n(48)\npi(J, d∗, j, d)\n=\n0, j < J, d∗̸= d,\np1(J, d, j, d)\n=\npi(j, d, j, d), j < J,\np2(J, d∗, j, d)\n=\n0, j < J,\n(49)\nX\nd∈∂B+(|d∗|)\nJ\nX\nj=0\np1(J, d∗, j, d)\n=\n1.\nX\nd∈∂B+(|d∗|)\np2(j, Def, j, d)\n=\n1.\nLet\nthe\n um\nulativ\ne\ndiagonal\nprobabilit\ny\nb\ne\nsi(J, d) ≡\nJ\nX\nj=1\npi(J, d, j, d).\n(50)\nBe ause\nof\nthe\nnext\npla\ny\ners'\nround\nduration\n ondition\n∀J > 1, s1(J, d) > s2(J, d) = p2(J, d, J, d) > p1(J, d, J, d).\n5\nGO\nAL\nIDENTIFICA\nTION\nPR\nOGRAMMING\n17\nT\no\nredu e\nthe pi\n omputational\ndomain,\nI\nuse\nin\nv\narian e\nwith\nresp\ne t\nto\nfa e\np\nerm\nutations\n(for\nunloaded\ndi e).\nFirstly\n,\ndiagonal\nprobabilities\nde-\np\nend\non\nonly\none\n om\nbination.\nAs\nin\n(1),\nt\nw\no\n om\nbinations\nare\nequiv\nalen\nt,\nmo\ndulo\nthe\nfun tions d 7→pi(J, d, j, d),\nfor\nall (i, J, j),\nif\nand\nonly\nif\ntheir\no\n  upation\nn\num\nb\ners\n(Lagrangian\n omp\nonen\nts)\nform\nthe\nsame\n om\nbination,\ne.\ng. 441 ∼655.\nWith (D, F) = (3, 6),\nthe\nquotien\nt\nset\n on\ntains\nthree\n lasses:\nthat\nof\nbrelans\n(∋111),\nthat\nof\nsequen es\n(∋123)\n3\n,\nthat\nof\npairs\n(∋112).\nSe ondly\n,\nnon-diagonal\nprobabilities\ndep\nend\non\na\n ouple\nof\n om\nbi-\nnations.\nT\nw\no\n \nouples\nof\n om\nbinations\nare\nequiv\nalen\nt,\nmo\ndulo\nthe\nfun tions\n(d∗, d) 7→pi(J, d∗, j, d),\nfor\nall (i, J, j),\nif\nand\nonly\nif\ntheir\n ouples\nof\no\n  u-\npation\nn\num\nb\ners\nform\nthe\nsame\n om\nbination,\ne.\ng. (421, 442) ∼(321, 211).\nA\nfa e\np\nerm\nutation\ntransforms\na\nnext\npla\ny\ner's\noptimal\none-goal\nstrategy\nin\nto\nanother,\np\nossibly\ndi\u001beren\nt\nif\nthe\ngoal\nis\nnot\na\nbrelan.\nT\naking\nin\nto\na  oun\nt\n(46)\nand\nfa e\np\nerm\nutation\nin\nv\narian e,\nthe\nresult\nprob-\nabilities\n(45)\nare\n omputed,\nfor (D, F, J) = (3, 6, 3),\nb\ny\napplying strat −moy\non\noptimal d∗\n-goal\nstrategies\nand d-Krone \nk\ner\nutilities.\nAs\na\n onsequen e\nof\nself-similarit\ny\n(23),\nthe\nprobabilities\nafter\nthe\ninitial\ntime\n(J1 < J\n),\nare\nobtained\nas\nin\ntermediary\nresults\nin\nthe\n omputation\nof\na\npriori\nprobabilities\n(J1 = J\n).\nThe\nresults\nare\npresen\nted\nin\nthe\nprobabilit\ny\n \nharts\n6,\n7,\n8,\n9,\n10\n(app\nendix\nC),\nwhi \nh\ndo\nnot\n\u001cll\nmore\nthan\na\nfew\npages\nthanks\nto\nthe\nexten-\nsiv\ne\nuse\nof\nfa e\np\nerm\nutation\nin\nv\narian e\nand\nother\nprop\nerties\n(46 . . .\n).\nThere\nare\n31\n lasses\nof\nthree-die\n om\nbination\n ouples\n(in luding\nthe\nthree\ndiagonal\n lasses).\n5\nGoal\niden\nti\u001c ation\nprogramming\nI\nwill\nprop\nose\nheuristi \np\noli ies,\nbased\non\nthe\nglobal\nmaximization\nof\nexp\ne ted\nutilit\ny\n,\nwith\nresp\ne t\nto\nthe\nsubset\nof\noptimal\none-goal\nstrategies,\nfor\nwhi \nh\nresult\nprobabilities\nw\nere\nobtained\nin\nthe\nlast\nse tion.\n5.1\nMotiv\nation:\nb\nounded\n omplexit\ny\nThe max −moy\nba \nkw\nard\nindu tion\nalgorithm\nis\noptimal,\nshort,\nbut\nthe\nn\num\nb\ner\nof\nn\numeri al\nop\nerations\np\ner\ntime\nstep,\nalready\nlarge\nfor (D, F, J) =\n(3, 6, 3),\nis\nun\nb\nounded\nas\na\nfun tion\nof\nthe\nmaxim\num\nround\nduration J\n.\nInformation\ntheory\n[17,\n5\n℄\ntea \nhes\nthat\na\nmessage\nwill\nb\ne\ntransmitted\nfaster\n3\nI\ndo\nnot\nmean\nthat\nall\n om\nbination\nin\nthe\n lass\nof\nsequen es\nis\na\nsequen e.\n5\nGO\nAL\nIDENTIFICA\nTION\nPR\nOGRAMMING\n18\nb\ny\na\nsp\ne ialized\n o\nde. max −moy\nba \nkw\nard\nindu tion\nis\nslo\nw,\nfor\nthe\ngeneral\nreasons\nthat\nit\nis\nunsp\ne ialized\n(and\noptimal).\nT\no\nsp\need-up\np\noli y\n,\np\nossibly\nat\nthe\nexp\nense\nof\nbrevit\ny\nand\noptimalit\ny\n,\nsp\ne ialization\nis\nne essary\n.\nF\nor\nexample,\n onsider\nthe\ngame\nof\nNim\n[7,\n\n1.3℄:\nb\nesides max −moy\nba \nkw\nard\nindu tion,\na\nstratagem\nis\nfound,\nbased\non\n on-\ngruen e,\npro\ndu ing\noptimal\nstrategies,\nwith\na\nb\nounded\nn\num\nb\ner\nof\nop\nerations\np\ner\ntime\nstep.\nThe\nrat \nhet\n(D < F\n)\nw\nould\nb\ne\na\nstratagem\nof\n421,\nif\nonly\nthe\ngoal\nw\nere\nkno\nwn.\nI\nprop\nose\nto\niden\ntify\nthe\ngoal,\nrigorously\n,\nb\ny\n onsidering\nnot\nonly\nthe\nutilit\ny\n,\nbut\nalso\nthe\nresult\nprobabilities\n(45),\nobtained\nin\nse tion\n4.\nI\nwill\nobtain\ngoal\niden\nti\u001c ation\nheuristi \np\noli ies,\nthat\nma\ny\nb\ne\n onsidered\nas\nquasi-\nMark\no\nvian,\nfrom\nthe\nremark\nfollo\nwing\n(45).\nRoughly\n,\nthey\ntransfer\nthe\n om-\nplexit\ny\nof max −moy\nba \nkw\nard\nindu tion\nto\nthe\nresult\nprobabilities,\nwith\nthe\nadv\nan\ntage\nthat\nthe\nlatter\n an\nb\ne\n ompiled\non e\nfor\nall\n(and\nthe\nin on\nv\nenien e\nthat\nthey\nm\nust\nb\ne\nremem\nb\nered).\nF\nor\na\none-goal\nutilit\ny\n,\ngoal\niden\nti\u001c ation\nis\nsimple.\nF\nor\na\n onstan\nt\nutilit\ny\n,\nas\nw\nell:\nan\ny\ngoal\nis\noptimal.\nDi\u001e ulties\nare\nth\nus\nwith\nutilities\nsomewhere\nb\net\nw\neen\np\neak\ned\nand\n\u001dat,\n\u0010\nfuzzy\n\u0011,\ne.\ng.\nwith\np\neaks\nof\nab\nout\nthe\nsame\nheigh\nt,\npla\nying\nthe\nroles\nof\nattra tors,\nthat\none\nhas\nto\n \nho\nose\nb\net\nw\neen.\n4\n5.2\nRedu ed\nhorizon\nI\n onsider\na\ntime\nand\nstate\ndep\nenden\nt\nutilit\ny\n,\nas\nin\n(33,\n36),\nin\na\nround\nof\nmaxim\num\nduration J\n. d0\nis\nthe\nstate\nat\ntime j0 ≤J\n.\nI\nde\u001cne\nthe\n\u0010ev\naluation\nfun tion\u0011,\nu∗0\nj0(d0) ≡\nmax\nd∗∈∂B+(D0)\nJ−j0\nX\nj=0\npi(J −j0, d∗, j, d∗)uj0+j(d0 + d∗),\n(51)\nwhere j\nis\nthe\nrenormalized\ntime\nand D0 = D −|d0| .\nEv\naluation\nfun tions\nare\noften\nused\nin\nstage\ngame\n( \nhess,\nothello,\n \nhe \nk\ners . . .\n)\nprogramming,\nbut\nthey\nare\nusually\nde\u001cned\nempiri ally\n,\nunlik\ne\n(51),\nwhi \nh\nis\nprobabilisti .\nT\no\ntak\ne\nin\nto\na  oun\nt\nserendipit\ny\n\u0015\nthat\na\nresult\nother\nthan\nthe\ngoal\nma\ny\nb\ne\nnot\nso\nbad,\nafter\nall\n\u0015\n(51)\nis\nimpro\nv\ned:\nu∗1\nj0(d0) ≡\nmax\nd∗∈∂B+(D0)\nJ−j0\nX\nj=0\nX\nd∈∂B+(D0)\np1(J −j0, d∗, j, d)uj0+j(d0 + d),\n(52)\n4\nLik\ne\nBuridan's\ndonk\ney\n,\nstarving\nfrom\nhesitating\nb\net\nw\neen\nbushels\nof\noats\nand\nw\nater.\n5\nGO\nAL\nIDENTIFICA\nTION\nPR\nOGRAMMING\n19\nwhi \nh\n annot\nb\ne\nused\nfor\nnext\npla\ny\ners,\nb\ne ause\nof\ndilemmas.\nF\nor\nall d0 ∈\n∂B+(D),\n onsidering\n(47),\nthe\nev\naluation\nfun tions\n(51,\n52)\nsimply\nreturn\nthe\nutilit\ny\n.\nmax −moy\nba \nkw\nard\nindu tion\nis\nparti ularly\nslo\nw,\nb\ne ause\nit\nneeds\nto\n ompletely\nanalyze\nthe\nround\nev\nen\nb\nefore\nits\n\u001crst\nde ision.\nHen e\nthe\nidea\nthat\nshort-sigh\nted\np\noli ies\nma\ny\nb\ne\nfaster.\nA\nt\ntime j0 ∈N,\na\nhorizon h ∈N\nma\ny\nb\ne\n \nhosen,\nsu \nh\nthat j1 = j0 + h ≤J\n,\nand\nthe\nround\nis\nvirtually\nterminated\nat j1\n,\ntaking\nfor\nersatz\nutilit\ny\nthe\nev\naluation\nfun tion u∗s\nj1\ngiv\nen\nb\ny\n(51)\nor\n(52),\ndep\nending\non\nthe\nserendipit\ny\nbit s ∈{0, 1} .\nWith j1 = J −1 ,\n onsidering\n(48),\n(52)\nrepro\ndu es\nthe\ndeep\nest max −moy\niteration,\nso\nthat\nan\noptimal\nstrategy\nis\ngenerated.\nI\nwill\nfurther\nexamine h = 0, 1 .\nWith h = 0,\nthe\ngoal\nis\nfound\nb\ny\nmax-\nimizing u∗s\nj0\n,\nindep\nenden\ntly\nof\nthe\n\u001crst\nev\nen\nt.\nWith h = 1,\nas\nthere\nis\nno\nin\nterest\nin\nthinking\nb\nefore\n asting\nthe\ndi e,\nthe\nde ision d1\nis\nrather\ntak\nen\nafter\nthe\n\u001crst\nev\nen\nt d1\/2\n,\na  ording\nto\nmax\nd1 u∗s\nj0+1(d1).\n(53)\nIn\n ase\nof\nman\ny\noptimal\nde isions\nin\n(53),\nthe\n orresp\nonding\nstates,\nwritten\nas\nin reasing\nLagrangian\nlists,\ne.\ng.\n124,\nare\ndis riminated\na  ording\nto\nthe\nlexi ographi \norder\n(only\npure\nstrategies\nare\ngenerated).\nIn\n ase\nof\nman\ny\noptimal\ngoals\nin\n(51)\nor\n(52),\nw\ne\nneed\nnot\ndis riminate\nb\net\nw\neen\nthem,\nand\nthe\np\noli y\nrepro\ndu es\nthe\nh\numan\n \nhara ter\nof\ndupli ity.\nDilemma\nimplies\ndupli it\ny\n,\nbut\nthe\n on\nv\nerse\nis\nfalse.\n5.3\nDynami \nprogramming\nand\ngoal\nrevision\nThe\nstrategy\nma\ny\nb\ne\nrevised\nto\ntak\ne\nin\nto\na  oun\nt\nnew\nev\nen\nts,\nwhi \nh\nis\nan\ninstan e\nof\ndynami \nprogramming\n[18\n℄\nor\nb\nelief\nrevision\n[19\n℄,\nrealizing\na\nfeed-\nba \nk\nof\nfate\non\nstrategy\n.\nBy\nself-similarit\ny\nof\nthe\nround,\na\np\noli y\nma\ny\nb\ne\napplied\nat\nan\ny\ntime,\nwith\nsuitable\nparameter\nrenormalization.\nSelf-similar\nrevision\nbased\non\nthe max −moy\nba \nkw\nard\nindu tion\np\noli y\nw\nould\njust\n on-\n\u001crm\nthe\noptimal\nstrategy\n,\n omputed\na\npriori:\nit\nis\ntherefore\nuseless.\nOnly\nfallible\np\noli ies\nare\nw\north\nrevising.\nA\nheuristi \np\noli y\nof\nhorizon h ≥1\nfore asts,\nat\nan\ny\ngiv\nen\ntime,\nonly\nthe\nnext h\nde isions.\nTh\nus,\nit\nm\nust\nb\ne\nrun\nwith\nthe\np\nerio\nd\nat\nleast h.\nThe\nrevised\nserendipitous\ngoal\niden\nti\u001c ation\np\noli y\nof\nhorizon h\nis\noptimal\nin\nits\nlast h\nde isions.\nThe\ngoal\niden\nti\u001c ation\np\noli y\nwith h = 0\ndo\nes\nnot\nrequire\nrevision\n5\nGO\nAL\nIDENTIFICA\nTION\nPR\nOGRAMMING\n20\nT\nable\n1:\n123\none-goal\nutilit\ny\nu0r = 0.22811\nu0\/u0r\np\noli y\npla\ny\ner\nhorizon\nserendip.\n\u001crst\nnext\n0\n0\n1\n0.57858\n0\n1\n1\n1\n0\n1\n0.57858\n1\n1\n1\nmax −moy\n1\n0.57858\nand\nis\nv\nery\nsimple\n(short\nand\nfast).\nIt\nma\ny\nb\ne\nthe\nonly\nrational\np\noli y\n,\nsimple\nenough\nfor\nunaided\nh\numan\npla\ny\ners\nin\nnormal\ngame\n onditions.\n5.4\nP\noli y\nb\nen \nhmark\nand\nin\nterpretation\nF\nor (D, F, J) = (3, 6, 3),\nI\n onsider\na\nfew\nin reasingly\nfuzzy\nstationary\nutili-\nties:\n1. u = δ123\n,\na\none-goal\nutilit\ny\n,\n2. u = δ123 + δ224 + δ345\n,\na\nthree-goal\nutilit\ny\n,\n3. u = t,\nthe\ntransfer\nfun tion\nde\u001cned\nb\ny\ntable\n5\nin\napp\nendix,\n4.\nthe\nsum\nof\nfa es.\nThese\nutilities\nare\nunrealisti ,\nin\nthe\nsense\nthat\nthey\nma\ny\nnot\nb\ne\np\nossible\nwithin\na\nreal\n421\nset\n(see\nse tion\n2.3).\nI\n onsider\nthe\np\noli ies: max −moy\nba \nkw\nard\nindu tion,\nand\nthe\nfour\ngoal\niden\nti\u001c ation\np\noli ies (h, s) ∈{0, 1}2\n;\nthe h = 0\np\noli ies\nare\nwithout\nrevision.\nF\nrom\nthe\n\u001cnal\nutilit\ny\n,\non\nthe\nlea\nv\nes\nof\nthe\nfate\ntree,\nev\nery\np\noli y\nyields\na\npure\nstrategy\n,\nand\nits\ninitial\nexp\ne ted\nutilit\ny u0\n,\non\nthe\ntrunk,\nis\nobtained\nb\ny\nsolving\nthe\nK\nolmogoro\nv\nequation\nexa tly\n,\nwith\nthe strat −moy\nalgorithm.\nOptimalit\ny\nis\nde\u001cned\nas\nthe\nratio\nof\nthe\nexp\ne ted\nutilit\ny\n,\no\nv\ner\nthe\n\u001crst\npla\ny\ner\noptimal\nexp\ne ted\nutilit\ny u0r\n.\nThe\nn\numeri al\nresults\n(appro\nximated\nb\ny\nde i-\nmal\nn\num\nb\ners)\nare\n opied\nfrom\n[20℄\nin\nto\nthe\ntables\n1,\n2,\n3,\n4.\nT\nable\n1\n on\u001crms\nthat\nfor\na\none-goal\nutilit\ny\n,\nall\ngoal\niden\nti\u001c ation\np\noli-\n ies\nare\nb\ny\nde\u001cnition\noptimal.\nCompared\nto\nthe\n\u001crst\npla\ny\ner,\nnext\npla\ny\ners\n5\nGO\nAL\nIDENTIFICA\nTION\nPR\nOGRAMMING\n21\nT\nable\n2:\n123,\n224,\n345\nthree-goal\nutilit\ny\nu0r = 0.32805\nu0\/u0r\np\noli y\npla\ny\ner\nhorizon\nserendip.\n\u001crst\nnext\n0\n0\n0.73037\n0.43734\n0\n1\n0.73037\n1\n0\n0.97777\n0.47746\n1\n1\n0.98657\nmax −moy\n1\n0.49152\nT\nable\n3:\nutilit\ny\n=\ntransfer\nfun tion\nu0r = 3.7467\nu0\/u0r\np\noli y\npla\ny\ner\nhorizon\nserendip.\n\u001crst\nnext\n0\n0\n0.90834\n0.68812\n0\n1\n0.90834\n1\n0\n0.87962\n0.68991\n1\n1\n0.99634\nmax −moy\n1\n0.77663\nT\nable\n4:\nutilit\ny\n=\nsum\nof\nfa es\nu0r = 14\nu0\/u0r\np\noli y\npla\ny\ner\nhorizon\nserendip.\n\u001crst\nnext\n0\n0\n0.94194\n0.92599\n0\n1\n0.96418\n1\n0\n0.75\n0.85875\n1\n1\n0.99900\nmax −moy\n1\n0.97321\n6\nCONCLUSIONS\n22\nare\nhandi app\ned,\nbut\nless\nwith\na\nfuzzier\nutilit\ny\n.\nThe\nn\numeri al\nresults\nsho\nw\na\np\nositiv\ne\n on\ntribution\nof\nserendipit\ny\n,\nm\nu \nh\ngreater\nwith\nthe\ngreater\nhori-\nzon\nand\nrevision.\nThe\n on\ntribution\nof\nhorizon\nand\nrevision\nis\np\nositiv\ne\nwith\nserendipit\ny\n.\nWithout\nserendipit\ny\n,\nthe\n on\ntribution\nof\nhorizon\nand\nrevision\nis\np\nositiv\ne\nfor\np\neak\ned\nutilities,\nnegativ\ne\nfor\nfuzzy\nutilities\n(3,\n4).\nI\ntak\ne\nadv\nan\ntage\nof\nthis\ne\u001be t\nto\ngiv\ne\na\n(less\nfuzzy)\nde\u001cnition\nof\nfuzzi-\nness:\na\nutilit\ny\nis\nfuzzy\nif\nand\nonly\nif\nin\ntro\ndu ing\nhorizon\nand\nrevision\nwith-\nout\nserendipit\ny\n on\ntributes\nnegativ\nely\nto\nits\nexp\ne tation.\nTh\nus,\nI\nha\nv\ne\n on-\nstru ted\nfuzzy\nutilities,\nfor\nwhi \nh\nin\ntro\ndu ing\nhorizon\nand\nrevision\nde reases\nthe\nexp\ne ted\nutilit\ny\n,\nev\nen\nthough\nit\nis\nmore\n omplex.\nThe\nresp\nonse\nof\nex-\np\ne ted\nutilit\ny\nwith\nresp\ne t\nto\n omplexit\ny\nis\nnon-in reasing\n(this\ne\u001be t\n om-\npares,\nin\nele tri it\ny\n,\nwith\na\nnegativ\ne\nresistan e).\n6\nCon lusions\nThe\nmathemati s\nof\nfate\nin\n421\nlea\nv\ne\nas\nthe\nonly\nunsolv\ned\ndi\u001e ult\ny\n\u0010bifur-\n ations\u0011,\nthat\nmaximizing\nthe\nexp\ne ted\nutilit\ny\ndo\nes\nnot\nalw\na\nys\ndetermine\na\nunique\nde ision,\nas\nin\nnext\npla\ny\ners'\ndilemmas.\nHere\nis\na\nto\ny\nexample:\na\ngame\nwith\nthree\npla\ny\ners,\nP\n,\nA,\nB.\nIf\nP\nsa\nys\nwhite,\nthen\nA\ngiv\nes\none\neuro\nto\nB;\nif\nP\nsa\nys\nbla \nk,\nthen\nB\ngiv\nes\none\neuro\nto\nA.\nP\nearns\nnothing\nan\nyw\na\ny;\nA,\nB\ntak\ne\nno\nde ision.\nMaximizing\nP's\nexp\ne ted\nutilit\ny\ndo\nes\nnot\ndetermine\nits\nde ision.\nIn\ntro\ndu ing\na\nmixed\nstrategy\namoun\nts\nto\n onsider\nP\nas\na\nrandom\ngenerator,\nwith\nunkno\nwn\nprobabilities.\nA\n lassi \np\nostulate\nof\nstatisti al\ntheories\nis\nto\nmaximize\nthe\nen\ntrop\ny\nor\nmissing\ninformation\n[10\n,\n17\n℄,\nwhi \nh\nhere\nsets\nthe\nprobabilities\nof\neither\nout ome\nto 1\/2 .\nAre\nthe\np\nostulates\nof\nmixed\nstrategy\nand\nmaxim\num\nen\ntrop\ny\nso\neasily\na  eptable?\nW\ne\n annot\nex lude\nhidden\nde-\nterminism\nor\nbias\nin\nP\n.\nF\nor\nexample,\nP\nma\ny\nalw\na\nys\n \nho\nose\nthe\n\u001crst\nansw\ner\nin\nthe\nlexi ographi \norder\n(bla \nk),\nor\nP\nma\ny\nha\nv\ne\na\nse ret\nagreemen\nt\nwith\nA\nto\nshare\nhis\ngain.\nBounded\n omplexit\ny\n,\nsimilar\nto\nb\nounde\nd\nr\nationality\nin\n[21℄,\nmotiv\nates\nheuristi \np\noli ies,\nwhere\n \nhara ters\n lose\nto\na tual\nh\numan\nb\neha\nvior\nare\nfound,\nin\nagreemen\nt\nwith\n[22\n℄.\nThese\n \nhara ters\nare\nfate,\ndilemma,\ngoal\niden\nti\u001c a-\ntion\nand\nrevision,\nrestri ted\nhorizon,\nserendipit\ny\n,\ndupli it\ny\nand\npani .\nWhen\nthe\np\noli y\nb\nelongs\nto\nan\norganization,\nw\ne\nare\nin\nmanagemen\nt.\nWhen\nan\nin-\ndividual\nde ides\nfor\nhimself,\nw\ne\nare\nin\npsy \nhology\n.\nF\nor\nexample,\nthe\nsame\nmathemati al\ne\u001be t\nis\nb\nehind\n oun\nterpro\ndu tiv\ne\nmanagemen\nt\nor\npani .\nGoal\niden\nti\u001c ation\n onsumes\na\nb\nounded\nn\num\nb\ner\nof\nop\nerations\np\ner\ntime\nA\nTHE\n(TENT\nA\nTIVE)\nR\nULES\nOF\n421\n23\nstep,\nwhatev\ner\nthe\nround\nduration,\nb\ne ause\nit\ndo\nes\nnot\nresolv\ne\nall\nde isions\nin\nthe\nfate\ntree,\nbut\nonly\nthose\nwhi \nh\nare\n ompatible\nwith\nthe\npresen\nt\nstate,\nand\nb\nefore\nthe\nhorizon.\nGoal\niden\nti\u001c ation\nis\nnot\ngenerally\noptimal,\nas\nop-\np\nosed\nto\na\n ommon\nassertion\nin\nbusiness\n ourses.\nOnly max −moy\nba \nkw\nard\nindu tion,\nwhi \nh\nhas\nno\ngoal,\njust\nlik\ne\nrandom\npla\nying,\nis\ngenerally\noptimal.\nIn\nthe\nround,\nthe\nrat \nhet\nstratagem\nallo\nws\nthe\nimmediate\ntranslation\nfrom\ngoal\nto\nde ision.\nI\nused\nprobabilit\ny\ntheory\nas\nthe\nlogi \nof\ngoal\niden\nti\u001c ation,\nà\nla\nJa\nynes\n[11\n℄.\nComplexit\ny\nhides\nin\nthe\nresult\nprobabilities,\nto\nb\ne\n ompiled\nb\nefore\npla\nying,\nas\na\nkind\nof\ntraining.\nDep\nending\non\n omplexit\ny\nresour es\nand\nutilit\ny\n,\np\noli ies\nma\ny\nb\ne\nv\nariably\nappli able\nor\ngo\no\nd.\nStarting\nfrom\na\ngiv\nen\np\noli y\n,\none\nma\ny\nin rease\noptimalit\ny\n,\nb\ny\nmo\ndifying\nits\n \nhara ters\nor\nthe\nutilit\ny:\nthis\nis\nthe\ntask\nof\nh\numan\nresour es\nmanagemen\nt,\nwhen\nthe\np\noli y\nis\nthat\nof\nan\nindividual\ntaking\nde isions\nfor\na\n ompan\ny\n,\na\nmanager.\nThe\nshort-sigh\nted\nmanager\n(h = 0)\ngets\nhardly\nan\ny\nhelp\nfrom\nserendipit\ny\n.\nThe\nunserendipitous\nmanager\nshould\na\nv\noid\nfuzzy\nutilities\nand\nfa\nv\nor\npre ise\ngoal\nassignmen\nts.\nI\nobtain\nexamples\nof\n oun\nterpro-\ndu tiv\ne\nmanagemen\nt:\nwith\na\nfuzzy\nutilit\ny\nand\nno\nserendipit\ny\n,\ngoal\nrevision\ndramati ally\nredu es\nthe\noptimalit\ny\n.\nThe\nrole\nof\nserendipit\ny\nw\nas\np\noin\nted\nout,\non\npurely\nqualitativ\ne\nground,\nb\ny\nN.\nWiener,\nab\nout\ns ien\nti\u001c \nand\nte \nhni al\nin\nv\nen\ntion\n[23℄.\nThe\npresen\nt\nw\nork\nalso\np\nertains\nto\nWiener's\n yb\nerneti s.\nRationalit\ny\n an\nb\ne\nfurther\nredu ed.\nA\nt\nthe\nextreme,\nthe\nfo\nol\nmanager\n an\nb\ne\ntrusted\nonly\nfor\na\n\u001dat\nutilit\ny\n.\nThe\nstudy\nof\nirrational\nor\nillogi al\nbut\na tual\nb\neha\nvior\nis\nthe\ntask\nof\nsophistry\n[24℄.\nIt\nma\ny\nb\ne\nquite\nuseful\nin\ngame\npra ti e,\nto\npro\ndu e\nb\nest\nresp\nonses.\nI\nthank\nresear \nhers\nof\nthe\nGREQAM\nin\nMarseilles,\nfor\nfruitful\ndis ussions.\nA\nThe\n(ten\ntativ\ne)\nrules\nof\n421\nI\nde\u001cne\nthe\ngame,\nfrom\noral\ntradition\nand\n[25\n,\n26℄.\nThe\nhardw\nare\n onsists\nof\nthree\ndi e\nand\nelev\nen\ntok\nens,\ninitially\nin\na\np\not.\nThere\nare\nt\nw\no\nor\nmore\npla\ny\ners\nwho\n an\nalw\na\nys\nsee\nthe\np\nositions\nof\ndi e\nand\ntok\nens.\nIn\nthe\n\u001crst\npart\nof\nthe\ngame,\nthe\n \nharge,\npla\ny\ners\nget\ntok\nens\nfrom\nthe\np\not.\nIn\nthe\nse ond\npart\nof\nthe\ngame,\nthe\ndis \nharge,\npla\ny\ners\nget\ntok\nens\nfrom\nea \nh\nother.\nA\npla\ny\ner\nwins\nwhen\nhe\ngets\nno\ntok\nen\nduring\nthe\n \nharge\n(man\ny\npla\ny\ners\nma\ny\nth\nus\nwin),\nor\nwhen\nhe\n\u001crst\ngets\nrid\nof\nhis\ntok\nens\nduring\nthe\ndis \nharge.\nThe\n \nharge\nor\ndis \nharge\nis\na\nsequen e\nof\nsets.\nIn\nev\nery\nset,\nea \nh\npla\ny\ner\nat\nhis\nturn\npla\nys\na\nround\nagainst\nthe\ndi e,\nwhile\nthe\nothers\nw\nait.\nThe\na tiv\ne\nA\nTHE\n(TENT\nA\nTIVE)\nR\nULES\nOF\n421\n24\nT\nable\n5:\ntok\nen\ntransfer\nfun tion\nhighest\n om\nbination\ntok\nen\nn\num\nb\ner\n421\n10\n111\n7\nf11, fff, f ̸= 1\nf\nsequen e\n2\nother\n1\npla\ny\ner\n asts\nthe\ndi e\nup\nto\nthree\ntimes;\nafter\nev\nery\n ast,\nhe\n an\nput\naside\nan\ny\nn\num\nb\ner\nof\ndi e,\nth\nus\na  um\nulating\na\n om\nbination.\nNext\npla\ny\ners\nm\nust\n ast\ndi e\nas\nman\ny\ntimes\nas\nthe\n\u001crst\npla\ny\ner.\n5\nEnd-of-round\na  um\nulated\n om-\nbinations,\nobtained\nb\ny\nall\npla\ny\ners\nin\nthe\nset,\nare\nrank\ned\nin\nthe\nhierar \nhi \norder\n421 ≻111 ≻611 ≻666 ≻511 ≻555 ≻411 ≻444 ≻311\n≻333 ≻211 ≻222 ≻654 ≻543 ≻432 ≻321 ≻665 ≻. . . 221,\n(54)\nwhere ≻\nmeans\n`higher\nthan'.\nThe\n om\nbinations,\nimpli it\nin\n(54),\nare\nordered\nas\nthe\nn\num\nb\ners\nformed\nb\ny\ntheir\nfa es\nin\na\nde reasing\nsequen e:\ne.\ng. 655 ≻\n654.\nThe\ndominan\nt\n om\nbination\n421\nand\nthe\ndominated\n om\nbination\n221,\nkno\nwn\nas\n\u0010nénette\u0011,\ndi\u001ber\nonly\nb\ny\none\ndie. fff\nis\nthe f\n-brelan, f11\nis\nthe\nf\n-pair\n(f ̸= 1), 654, 543, 432, 321\nare\nthe\nsequen es.\nA\nt\nend\nof\nset,\nthe\nlast\n6\npla\ny\ner\nwho\nhas\ngot\nthe\nlo\nw\nest\n om\nbination\ngets\nthe\nn\num\nb\ner\nof\ntok\nens\ndetermined\nb\ny\ntable\n5,\ne.\ng.\nif\nthe\nhighest\n om\nbination\nis\n411,\nthen\nthe\nlast\npla\ny\ner\nwith\nthe\nlo\nw\nest\n om\nbination\n(whatev\ner\nit\nis)\ngets\n4\ntok\nens.\nDuring\nthe\n \nharge,\ntok\nens\nare\ntak\nen\nfrom\nthe\np\not,\nif\np\nossible.\nWhen\nthe\np\not\nis\nempt\ny\n,\nthe\ndis \nharge\nb\negins,\nand\ntok\nens\nare\nno\nw\ntak\nen\nfrom\nthe\npla\ny\ner\nwho\nhas\ngot\nthe\nhighest\n om\nbination.\n5\nThe\norder\nof\npla\ny\ners\nin\nthe\nset\nmatters,\nbut\nI\n ould\nnot\n\u001cnd\nde\u001cnite\nrules\nfor\nits\ndetermination.\n6\nThe\nadje tiv\ne\n`last'\nis\nm\ny\no\nwn\nsuggestion\nfor\nautomati \ntie-breaking.\nB\nA\nGAL\nTON-W\nA\nTSON\nPR\nOCESS\nIN\nTHE\n421\nR\nOUND\n25\nB\nA\nGalton-W\natson\npro\n ess\nin\nthe\n421\nround\nT\naking\nthe\ngenealogi \np\noin\nt\nof\nview,\nea \nh\ndie\nis\n onsidered\nas\nan\nindividual,\ndying\nafter\nb\neing\n ast,\neither\nwithout\na\n \nhild,\nin\n ase\nof\na  um\nulation,\nor\nwith\na\nsingle\n \nhild\n(itself\nindeed).\nThe\n \nhild\nn\num\nb\ner\nb\neing\nlo\nw\ner\nthan\none,\nthe\nn\num\nb\ner\nof\nliv\ne\ndi e Dj\n(se tion\n3.3)\nde reases\nin\ntime.\nMoreo\nv\ner,\nthe\np\nopulation\nb\ne omes\nextin t\nafter J\n asts\n(or\nso\noner).\nA\nGalton-W\natson\npro\n ess\n[16℄\nis\nobtained\nwhen\nthe\no\u001bspring\nof\nea \nh\nin-\ndividual\nis\nindep\nenden\nt\nof\nothers'.\nWith\nan\noptimal d∗\n-goal\nstrategy\n,\nthe\ndi e\ndying\nwithout\n \nhildren\nha\nv\ne\ntheir\nfa es\nin d∗\n,\nbut\nthe\n on\nv\nerse\nis\nnot\ntrue.\nF\nor\nexample,\nwith d∗= 221, d1 = 211, J > 1 ,\nthe\nt\nw\no\ndi e\n11\nha\nv\ne\n orrelated\no\u001bspring:\none\nhas\na\n \nhild\nif\nand\nonly\nif\nthe\nother\nhas\nnone.\nDi e\nha\nv\ne\nindep\nenden\nt\no\u001bspring\nif\nand\nonly\nif d∗\nis\na\nbrelan\nand\nthe\npla\ny\ner\nis\n\u001crst.\nI\napply\nthe\nGalton-W\natson\ntheory\n[12,\n6.2℄\nto\nobtain\nthe\nprobabilit\ny\nla\nw\nof Dj\n,\nfor\nan\noptimal d∗\n-goal\nstrategy\n,\nwhere d∗= DeF\nis\nthe F\n-brelan.\nDi e\nare\nindexed\nb\ny d = 1 . . .Dj\n.\nLet Zd ∈{0, 1}\nthe\nn\num\nb\ner\nof\n \nhildren\nof\nthe\ndie\nindexed\nb\ny d .\nDj =\nDj−1\nX\nd=1\nZd.\n(55)\nThe Zd\nare\nrandom\nv\nariables,\nwith\nthe\nsame\nla\nw qi ≡P(Zd = i),\nof\ngenerating\nfun tion\ng(z) ≡⟨zZd⟩= q0 + q1z, q0 = 1\nF , q1 = 1 −q0.\nThe Zd\nare\nalw\na\nys\nindep\nenden\nt\nif\nand\nonly\nif d∗\nis\na\nbrelan\nand\nthe\npla\ny\ner\nis\n\u001crst.\nWhen\nthis\nis\ntrue,\nfrom\n(55),\nthe\ngenerating\nfun tion\nof Dj\n,\n onditioned\nb\ny Dj−1\n,\nis\n⟨zDj|Dj−1 = d⟩= g(z)d.\nThe\ngenerating\nfun tion\nof Dj\nis\nth\nus\ndetermined\nb\ny\ng0(z) = zD, gJ(z) = 1,\n∀(j, 1 ≤j < J), gj(z) ≡⟨zDj⟩=\nD\nX\nd=0\n⟨zDj|Dj−1 = d⟩P(Dj−1 = d)\n=\nD\nX\nd=0\nP(Dj−1 = d)g(z)d = gj−1 ◦g(z).\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n26\nBy\nindu tion,\ngj = g0 ◦g◦j.\nThe\n omp\nosition\np\no\nw\ners\nof\nthe\na\u001ene\nfun tion g\nare\ng◦j(z) = 1 −qj\n1 + qj\n1z.\nTherefore\ngj(z)\n=\n(1 −qj\n1 + qj\n1z)D,\nP(Dj = d)\n=\n\u0012D\nd\n\u0013\n(1 −qj\n1)D−dqjd\n1 .\n(56)\nDj\nfollo\nws\na\nbinomial\nla\nw,\ndire tly\nobtained\nb\ny\n onsidering\nthat\na\ndie\ndies\nwhen\na  um\nulated,\nor\nsta\nys\naliv\ne,\nwith\nthe\nprobabilit\ny q1\np\ner\ntime\nstep,\nindep\nenden\ntly\nof\nothers:\na\nBernoulli\npro\n ess\nis\nobtained,\nwith\nthe\nla\nw\n(56).\nThe\nin\nterest\nof\n onsidering\na\nGalton-W\natson\npro\n ess\nis\nin\nthe\nanalogy\nwith\nbran \nhing\npro\n esses\n[16\n℄.\nC\nRealization\nwith\nmathemati a\nThe\npresen\nt\narti le\nis\nsupp\norted\nb\ny\n[20℄,\nan\nop\nen\nsour e\nsoft\nw\nare\nand\ndata\nbase\nin\nthe\nmathemati a\nlanguage\n[27\n℄,\nwhi \nh,\nlik\ne\nLISP,\nis\nin\nterpreted\nand\nallo\nws\nfun tional\nand\nre ursiv\ne\ntreatmen\nts\non\narbitrary\nexpressions,\nequiv\na-\nlen\nt\nto\ntrees.\nThe\nmathemati a\nfron\ntend\nallo\nws\nliterate\nprogramming\n[28\n℄\nin\nthe\nform\nof\nnoteb\no\noks,\ngathering\nliv\ne\n o\nde,\noutputs\nand\n ommen\nts,\nwithin\na\ntree\nstru ture,\nthat\n an\nb\ne\nunfolded\nat\nwill.\nCom\nbination\nmanipulation\ndi\u001bers\nsligh\ntly\nfrom\nlist\nmanipulation\n(sin e\norder\ndo\nes\nnot\nmatter\nin\n om\nbinations)\nor\nensem\nble\nmanipulation\n(sin e\nrep\ne-\ntitions\nare\nallo\nw\ned\nin\n om\nbinations).\nA\nto\nol\nb\no\nx\nis\ndev\nelop\ned.\nThe\nn\numeri al\nparameters (D, F, J)\nare\narbitrary\n,\nwhi \nh\nrealizes\na\ns alable\nmo\ndel,\nin\nv\nalu-\nable\nfor\ndev\nelopmen\nt.\nF\nate\ntrees\nare\n reated\nre ursiv\nely\n.\nAll\nfates\n on\nv\nerging\nto\nthe\nsame\nstate\nat\nthe\nsame\ntime\nare\nmerged\nb\ny\nindexing,\nso\nthat\nthe\nsize\ngro\nws\nonly\nlinearly\nwith\nthe\ndepth J\nand\nremains\neasily\nmanageable\nfor\n(D, F, J) = (3, 6, 3).\nIn\nex \nhange,\nthe\n omputing\ntime\nis\nin reased\nand\nthe\nhistory\nis\nlost,\nwhi \nh\nallo\nws\nto\ntreat\nonly\ntime\nand\nstate\ndep\nenden\nt\nutilities\n(as\nrequired\nin\nthe\n421\nset).\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n27\nStarting\nfrom\nthe\nlea\nv\nes\nof\nthe\nfate\ntree,\nwhere\nutilit\ny\nis\ngrafted,\nopti-\nmal\nstrategies\nand\nexp\ne ted\nutilities\nare\nbuild\nre ursiv\nely\n,\na  ording\nto\nthe\nmax −moy\nalgorithm.\nA\nutilit\ny-strategy\ntree\nis\n\u001cnally\nobtained,\nfrom\nwhi \nh\nthe\nstrategy\n an\nb\ne\nextra ted,\nthen\npip\ned\nin\nto\nthe strat −moy\nalgorithm,\na\nv\nariation\non max −moy\n,\nsolving\nthe\nK\nolmogoro\nv\nequation.\nmax −moy\npro\ndu es\nthe\nexp\ne ted\noptimal\none-goal\nstrategies,\nBernoulli\nor\nrat \nhet,\ndep\nending\non D < F\n,\nand\ndilemmas.\nThe\nresult\nprobabilities\nare\n omputed,\nsa\nv\ned,\nand\nman\ny\nprop\nerties\nare\n \nhe \nk\ned\nsystemati ally\n.\nSome\nresult\nprobabilities\nare\n \nhe \nk\ned\nb\ny\nMon\nte\nCarlo\nsim\nulations,\nwith\nsu  ess.\nThe\n \nharts\n6,\n7,\n8,\n9,\n10\nare\ngenerated\nautomati ally\n.\nThere\nis\nv\nery\nlittle\nro\nom\nfor\nerrors,\nand\nif\nthere\nare\nan\ny\n,\nthey\nare\ntra eable.\nThe\ngoal\niden\nti\u001c ation\nheuristi \np\noli ies\nare\nrealized.\nTheir\nwrong\nde i-\nsions\nare\np\noin\nted\nout.\nThey\nare\nexa tly\nev\naluated\nwith strat −moy\n,\nwhi \nh\nis\nv\nery\nslo\nw,\nsin e\nit\nrequires\nthe\n omputation\nof\nev\nery\nheuristi \nde ision\nin\nthe\nfate\ntree,\na  ording\nto\nan\nalgorithm\na tually\nlonger\nand\nslo\nw\ner,\nfor\none\nde ision,\nthan\nthe\nsimple\nmaximization\nin max −moy\n.\nObtaining\nthe\ntruth\nab\nout\nheuristi \np\noli ies\nis\na\nlength\ny\ntask.\nProbabilit\ny\n \nharts\npla\ny\ner's\nguide\np1,\np2\nmean\n\u001crst\nor\nnext\npla\ny\ners.\nIn\nev\nery\nb\no\nx\nof\na\ndiagonal\nprobabilit\ny\n \nhart\nstands\na\n olumn\nof\nthe\nprobabilities,\nordered\nfrom\ntop\nto\nb\nottom\nb\ny\ngro\nwing\ndela\ny\n,\nto\nobtain\nthe\ngoal\nwritten\nat\nhead\nof\nline.\nIn\nev\nery\nb\no\nx\nof\na\nnon-diagonal\nprobabilit\ny\n \nhart,\nstand\nt\nw\no\n olumns:\nat\nleft,\nfrom\ntop\nto\nb\nottom,\nthe\ngoal\nand\nthe\nresult;\nat\nrigh\nt,\nthe\nprobabil-\nities,\nordered\nfrom\ntop\nto\nb\nottom\nb\ny\ngro\nwing\ndela\ny\n,\nto\nobtain\nthe\nresult,\nwith\nthe\ngoal\nin\nmind\n(and\ntaking\noptimal\nde isions\nas\ndetermined\nb\ny\nthe\nrat \nhet).\nMoreo\nv\ner,\nfor\neasy\na  ess,\nthe\n ouples\n(goal,\nresult)\nare\nrepresen\nted\nin\na\nsquare\narra\ny\n,\nwhere\nheads\nof\nlines\nand\n olumns\nare\nthe\nresp\ne tiv\ne\nrep-\nresen\ntativ\nes\nof\ngoal\nand\nresult,\nmo\ndulo\nfa e\np\nerm\nutations\n(se tion\n4.2).\nThe\nthree-die\nrepresen\ntativ\ne\n3X3\narra\ny\nis\nspread\non\nto\nthe\nthree\n \nharts\n8,\n9,\n10,\none\nfor\nea \nh\ngoal\n lass.\nHere\nis\nan\nexample\nfor\nusing\nnon-diagonal\n \nharts.\nLet\nthe\ngoal\nb\ne 641\nand\nthe\nresult 652.\nThe\nrepresen\ntativ\nes\nof 641\nand 655\nare,\nseparately\nand\nresp\ne tiv\nely\n, 123\nand 112.\n(Represen\ntativ\nes\nare\n \nhosen\nso\nas\nto\nminimize\nthe\nsum\nof\ntheir\nfa es.)\nThe\nrepresen\ntativ\ne\nof\nthe\n \nouple (641, 655)\nis (123, 144).\n123\ntak\nes\nus\nto\n \nhart\n10\n(the\nthird\nline\nof\nthe\nrepresen\ntativ\ne\nsquare\narra\ny),\nwhen e 112\ntak\nes\nus\nto\nthe\nse ond\n olumn, (123, 144)\nto\nthe\nthird\nro\nw,\nwhere\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n28\n\u001cnally\nare\nthe\nprobabilities\nto\nobtain,\nwith\nthe\ngoal\n641,\nthe\nresult\n652,\nafter\none,\nt\nw\no\nor\nthree\n asts.\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n29\nT\nable\n6:\ndiagonal\nresult\nprobabilities\np1\np2\n1\n16\n=\n0.1667\n5\n\n36\n=\n0.1389\n16\n=\n0.1667\n16\n=\n0.1667\np1\np2\n11\n1\n\n36\n=\n0.02778\n85\n\n1296\n=\n0.06559\n1\n\n36\n=\n0.02778\n91\n\n1296\n=\n0.07022\n12\n1\n\n18\n=\n0.05556\n35\n\n324\n=\n0.108\n1\n\n18\n=\n0.05556\n19\n\n162\n=\n0.1173\np1\np2\n111\n1\n\n216\n=\n0.00463\n1115\n\n46656\n=\n0.0239\n466075\n\n10077696\n=\n0.04625\n1\n\n216\n=\n0.00463\n1151\n\n46656\n=\n0.02467\n513991\n\n10077696\n=\n0.051\n112\n1\n\n72\n=\n0.01389\n143\n\n2592\n=\n0.05517\n23681\n\n279936\n=\n0.08459\n1\n\n72\n=\n0.01389\n149\n\n2592\n=\n0.05748\n26903\n\n279936\n=\n0.0961\n123\n1\n\n36\n=\n0.02778\n227\n\n2592\n=\n0.08758\n21043\n\n186624\n=\n0.1128\n1\n\n36\n=\n0.02778\n239\n\n2592\n=\n0.09221\n24631\n\n186624\n=\n0.132\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n30\nT\nable\n7:\n\u001crst\npla\ny\ner's\nnon-diagonal\nresult\nprobabilities\n1\n1\n1\n2\n16\n=0.1667\n5\n\n36 =0.1389\n11\n12\n11\n11\n22\n1\n\n36\n=0.02778\n25\n\n1296 =0.01929\n11\n23\n1\n\n18 =0.05556\n25\n\n648 =0.03858\n11\n12\n1\n\n18 =0.05556\n55\n\n648 =0.08488\n12\n12\n33\n1\n\n36 =0.02778\n1\n\n81 =0.01235\n12\n11\n1\n\n36 =0.02778\n35\n\n648 =0.05401\n12\n34\n1\n\n18 =0.05556\n2\n\n81 =0.02469\n12\n13\n1\n\n18 =0.05556\n43\n\n648 =0.06636\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n31\nT\nable\n8:\n\u001crst\npla\ny\ner's\nnon-diagonal\nresult\nprobabilities\n(1)\n111\n112\n123\n111\n111\n222\n1\n\n216\n=0.00463\n125\n\n46656\n=0.002679\n15625\n\n10077696 =0.00155\n111\n223\n1\n\n72\n=0.01389\n125\n\n15552\n=0.008038\n15625\n\n3359232 =0.004651\n111\n122\n1\n\n72\n=0.01389\n275\n\n15552\n=0.01768\n56875\n\n3359232 =0.01693\n111\n112\n1\n\n72\n=0.01389\n605\n\n15552\n=0.0389\n207025\n\n3359232 =0.06163\n111\n234\n1\n\n36\n=0.02778\n125\n\n7776\n=0.01608\n15625\n\n1679616 =0.009303\n111\n123\n1\n\n36\n=0.02778\n275\n\n7776\n=0.03537\n56875\n\n1679616 =0.03386\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n32\nT\nable\n9:\n\u001crst\npla\ny\ner's\nnon-diagonal\nresult\nprobabilities\n(2)\n111\n112\n123\n112\n112\n333\n1\n\n216\n=0.00463\n1\n\n729\n=0.001372\n8\n\n19683 =0.0004064\n112\n222\n1\n\n216\n=0.00463\n215\n\n23328\n=0.009216\n20605\n\n2519424 =0.008178\n112\n111\n1\n\n216\n=0.00463\n205\n\n11664 =0.01758\n16105\n\n629856 =0.02557\n112\n334\n1\n\n72\n=0.01389\n1\n\n243 =0.004115\n8\n\n6561 =0.001219\n112\n233\n1\n\n72\n=0.01389\n31\n\n2592\n=0.01196\n839\n\n93312 =0.008991\n112\n133\n1\n\n72\n=0.01389\n5\n\n486 =0.01029\n38\n\n6561 =0.005792\n112\n223\n1\n\n72\n=0.01389\n77\n\n3888\n=0.0198\n7039\n\n419904 =0.01676\n112\n113\n1\n\n72\n=0.01389\n103\n\n3888\n=0.02649\n6499\n\n209952 =0.03095\n112\n122\n1\n\n72\n=0.01389\n91\n\n1944\n=0.04681\n28219\n\n419904 =0.0672\n112\n345\n1\n\n36\n=0.02778\n2\n\n243 =0.00823\n16\n\n6561 =0.002439\n112\n234\n1\n\n36\n=0.02778\n31\n\n1296\n=0.02392\n839\n\n46656 =0.01798\n112\n134\n1\n\n36\n=0.02778\n5\n\n243 =0.02058\n76\n\n6561 =0.01158\n112\n123\n1\n\n36\n=0.02778\n37\n\n648\n=0.0571\n10217\n\n139968 =0.073\nC\nREALIZA\nTION\nWITH\nMATHEMATICA\n33\nT\nable\n10:\n\u001crst\npla\ny\ner's\nnon-diagonal\nresult\nprobabilities\n(3)\n111\n112\n123\n123\n123\n444\n1\n\n216\n=0.00463\n1\n\n1728\n=0.0005787\n1\n\n13824 =0.00007234\n123\n111\n1\n\n216\n=0.00463\n83\n\n15552\n=0.005337\n3115\n\n1119744 =0.002782\n123\n445\n1\n\n72\n=0.01389\n1\n\n576 =0.001736\n1\n\n4608 =0.000217\n123\n112\n1\n\n72\n=0.01389\n179\n\n5184\n=0.03453\n15067\n\n373248 =0.04037\n123\n144\n1\n\n72\n=0.01389\n101\n\n15552\n=0.006494\n3277\n\n1119744 =0.002927\n123\n114\n1\n\n72\n=0.01389\n175\n\n15552\n=0.01125\n6311\n\n1119744 =0.005636\n123\n456\n1\n\n36\n=0.02778\n1\n\n288 =0.003472\n1\n\n2304 =0.000434\n123\n145\n1\n\n36\n=0.02778\n101\n\n7776\n=0.01299\n3277\n\n559872 =0.005853\n123\n124\n1\n\n36\n=0.02778\n319\n\n7776\n=0.04102\n24239\n\n559872 =0.04329\nREFERENCES\n34\nReferen es\n[1℄\nPierre\nAlbarède.\n421\n:\nun\njeu\nsto\n \nhastique.\nIn\nCol\nlo\nque\nSo\n .\nMath.\nApp.\nInd.\/Math.\nOpt.\n&\nDé ision\n(F\nr\nan \ne),\n2000.\n[2℄\nGerald\nT\nesauro.\nT\nd-gammon,\na\nself-tea \nhing\nba \nkgammon\nprogramm,\na \nhiev\nes\nmaster-lev\nel\npla\ny\n.\nNeur\nal\nComputation,\n6:215\u0015219,\n1994.\n[3℄\nL.\nS.\nShapley\n.\nSto\n \nhasti \ngames.\nPr\no\n .\nN.\nA.\nS.\n(USA),\n39:1095\u00151100,\n1953.\n[4℄\nRufus\nIsaa s.\nDi\u001ber\nential\ngames\nA\nmathemati \nal\nthe\nory\nwith\nappli \na-\ntions\nto\nwarfar\ne\nand\npursuit,\n \nontr\nol\nand\noptimization.\nWiley\n,\n1965.\n[5℄\nJean-P\naul\nDelaha\ny\ne.\nInformation\n \nomplexité\net\nhasar\nd.\nHERMES\nS i-\nen e,\nP\naris,\n1999.\n[6℄\nClaude\nBerge.\nThéorie\ndes\ngr\naphes\net\nses\nappli \nations.\nDuno\nd,\nP\naris,\n1967.\n[7℄\nPra\njit\nK.\nDutta.\nStr\nate\ngies\nand\ngames\nThe\nory\nand\nPr\na ti \ne.\nMIT\nPress,\n1999.\n[8℄\nBernard\nGuerrien.\nL\na\nthé\norie\ndes\njeux.\nE onomi a,\n1995.\n[9℄\nG.\nA.\nHun\nt.\nMartingales\net\npr\no\n \nessus\nde\nMarkov.\nDuno\nd,\n1966.\n[10℄\nF.\nReif.\nF\nundamentals\nof\nstatisti \nal\nand\nthermal\nphysi s.\nM \nGra\nw\nHill,\n1988.\n[11℄\nE.\nT.\nJa\nynes.\nPr\nob\nability\nthe\nory,\nthe\nlo\ngi \nof\ns ien \ne.\nh\nttp:\/\/ba\ny\nes.wustl.edu\/etj\/prob.h\ntml,\n1998.\n[12℄\nEman\nuel\nP\narzen.\nSto\n hasti \npr\no\n \ness.\nHolden\nda\ny\nseries\nin\nprobabilities\nand\nstatisti s.\nHolden\nda\ny\n,\nSan\nF\nran is o,\n1962.\n[13℄\nA\nthanasios\nP\nap\noulis.\nPr\nob\nability,\nr\nandom\nvariables\nand\nsto\n hasti \npr\no-\n \nesses.\nM \nGra\nw\nHill,\n1965.\n[14℄\nKenneth\nM.\nCase\nand\nP\naul\nZw\neifel.\nLine\nar\nT\nr\nansp\nort\nThe\nory.\nA\nddison-\nW\nesley\nPublishing\nCompan\ny\n,\n1967.\nREFERENCES\n35\n[15℄\nJe\u001bery\nLewins.\nImp\nortan \ne\nThe\nA\ndjoint\nF\nun tion.\nP\nergamon\nPress,\n1965.\n[16℄\nT.\nE.\nHarris.\nThe\nthe\nory\nof\nbr\nan hing\npr\no\n \nesses.\nSpringer,\n1963.\n[17℄\nLéon\nBrillouin.\nS ien \ne\nand\ninformation\nthe\nory.\nA\n ad.\nPress,\n1962.\n[18℄\nRi \nhard\nBellman\nand\nRob\nert\nKalaba.\nDynami \npr\no\ngr\namming\nand\nmo\nd-\nern\n \nontr\nol\nthe\nory.\nA\n ademi \nPress\nNew\nY\nork,\n1965.\n[19℄\nP\natri \nk\nF\nabiani.\nR\neprésentation\ndynamique\nde\nl'in \nertain\net\nstr\natégie\nde\np\ner\n \neption\np\nour\nun\nsystème\nautonome\nen\nenvir\nonnement\névolutif.\nPhD\nthesis,\nÉ ole\nNat.\nSup.\nAéro.\nEspa e,\nT\noulouse,\nF\nran e,\n1996.\n[20℄\nPierre\nAlbarède.\nAlgorithmes\nde\n on\ntrôle\nsto\n \nhastique\np\nour\nle\njeu\nde\n421.\nhttp:\/\/www.ge\no\n ities. \nom\/R\nese\nar\n hT\nriangle\/Cam\npus\/6253\n,\n2000.\n[21℄\nBernard\nW\nalliser.\nA\nsp\ne trum\nof\nequilibration\npro\n esses\nin\ngame\ntheory\n.\nJ.\nof\nEvolutionary\nE \nonomi s,\n1998.\n[22℄\nJa ob\nK.\nGo\neree\nand\nCharles\nA.\nHolt.\nSto\n \nhasti \ngame\ntheory:\nF\nor\npla\nying\ngames,\nnot\njust\nfor\ndoing\ntheory\n.\nPr\no\n .\nNat.\nA\n \nad.\nS i.\nUSA,\n96:10564\u001510567,\n1999.\n[23℄\nNorb\nert\nWiener.\nInvention,\nthe\n \nar\ne\nand\nfe\ne\nding\nof\nnew\nide\nas.\nMIT\nPress,\n1993.\n[24℄\nArth\nur\nS \nhop\nenhauer.\nL'art\nd'avoir\ntoujours\nr\naison,\nv\nolume\n191.\nMille\net\nune\nn\nuits,\n1998.\n[25℄\nLe\nv\nalet\nd' ÷ur.\nh\nttp:\/\/www.lev\nalet. om,\n1999.\n[26℄\nÉditions\nJeujura.\nRègles\nde\njeux\nde\nso\n iété.\n1999.\n[27℄\nStephen\nW\nolfram\net\nal.\nMathemati \na.\nIn\nt.\nThomson\nPub.,\n1997.\nh\nttp:\/\/www.wri. om.\n[28℄\nDonald\nE.\nKn\nuth.\nLiter\nate\nPr\no\ngr\namming.\nCen\nter\nfor\nthe\nStudy\nof\nLan-\nguage\nand\nInformation,\nStanford,\nCalifornia,\n1992.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Fate stochastic management and policy benchmark in 421, a popular game.pdf"}
