# Paper List of geo_cite

- [22/05] **ARLO: A Framework for Automated Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2205.10416v1)] [[Code/Page]()] [[TLDR/Notes](#arlo--a-framework-for-automated-reinforcement-learning)]

- [23/07] **Challenges and Applications of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2307.10169v1)] [[Code/Page]()] [[TLDR/Notes](#challenges-and-applications-of-large-language-models)]

- [23/09] **Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf**  
[[Paper](http://arxiv.org/pdf/2309.04658v2)] [[Code/Page]()] [[TLDR/Notes](#exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf)]

- [23/12] **Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach**  
[[Paper](http://arxiv.org/pdf/2312.11865v3)] [[Code/Page]()] [[TLDR/Notes](#large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach)]

- [24/02] **PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2402.01118v3)] [[Code/Page](https://github.com/git-disl/PokeLLMon.)] [[TLDR/Notes](#pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models)]

- [23/02] **Cooperative Open-ended Learning Framework for Zero-shot Coordination**  
[[Paper](http://arxiv.org/pdf/2302.04831v4)] [[Code/Page](https://sites.google.com/view/cole-2023.)] [[TLDR/Notes](#cooperative-open-ended-learning-framework-for-zero-shot-coordination)]

- [23/02] **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**  
[[Paper](http://arxiv.org/pdf/2302.01560v3)] [[Code/Page](https://github.com/CraftJarvis/MC-Planner.)] [[TLDR/Notes](#describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents)]

- [23/02] **Guiding Pretraining in Reinforcement Learning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.06692v2)] [[Code/Page](https://github.com/yuqingd/ellm.)] [[TLDR/Notes](#guiding-pretraining-in-reinforcement-learning-with-large-language-models)]

- [23/12] **Creative Agents: Empowering Agents with Imagination for Creative Tasks**  
[[Paper](http://arxiv.org/pdf/2312.02519v1)] [[Code/Page](https://github.com/PKU-RL/Creative-Agents).)] [[TLDR/Notes](#creative-agents--empowering-agents-with-imagination-for-creative-tasks)]

- [23/10] **Octopus: Embodied Vision-Language Programmer from Environmental Feedback**  
[[Paper](http://arxiv.org/pdf/2310.08588v2)] [[Code/Page]()] [[TLDR/Notes](#octopus--embodied-vision-language-programmer-from-environmental-feedback)]

- [23/10] **Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds**  
[[Paper](http://arxiv.org/pdf/2310.13255v2)] [[Code/Page]()] [[TLDR/Notes](#steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds)]

- [22/03] **ScienceWorld: Is your Agent Smarter than a 5th Grader?**  
[[Paper](http://arxiv.org/pdf/2203.07540v2)] [[Code/Page]()] [[TLDR/Notes](#scienceworld--is-your-agent-smarter-than-a-5th-grader-)]

- [20/10] **ALFWorld: Aligning Text and Embodied Environments for Interactive Learning**  
[[Paper](http://arxiv.org/pdf/2010.03768v2)] [[Code/Page]()] [[TLDR/Notes](#alfworld--aligning-text-and-embodied-environments-for-interactive-learning)]

- [23/10] **Welfare Diplomacy: Benchmarking Language Model Cooperation**  
[[Paper](http://arxiv.org/pdf/2310.08901v1)] [[Code/Page](https://github.com/mukobi/welfare-diplomacy.)] [[TLDR/Notes](#welfare-diplomacy--benchmarking-language-model-cooperation)]

- [23/10] **Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation**  
[[Paper](http://arxiv.org/pdf/2310.01320v3)] [[Code/Page]()] [[TLDR/Notes](#avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation)]

- [24/01] **PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2401.06781v1)] [[Code/Page]()] [[TLDR/Notes](#pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model)]

- [23/09] **Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4**  
[[Paper](http://arxiv.org/pdf/2309.17277v3)] [[Code/Page]()] [[TLDR/Notes](#suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4)]

- [23/06] **ChessGPT: Bridging Policy Learning and Language Modeling**  
[[Paper](http://arxiv.org/pdf/2306.09200v2)] [[Code/Page](https://github.com/waterhorse1/ChessGPT.)] [[TLDR/Notes](#chessgpt--bridging-policy-learning-and-language-modeling)]

- [24/01] **CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents**  
[[Paper](http://arxiv.org/pdf/2401.10568v2)] [[Code/Page](https://github.com/bigai-ai/civrealm.)] [[TLDR/Notes](#civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents)]

- [20/09] **A reinforcement learning approach to hybrid control design**  
[[Paper](http://arxiv.org/pdf/2009.00821v1)] [[Code/Page]()] [[TLDR/Notes](#a-reinforcement-learning-approach-to-hybrid-control-design)]

- [22/03] **FPGA-extended General Purpose Computer Architecture**  
[[Paper](http://arxiv.org/pdf/2203.10359v3)] [[Code/Page]()] [[TLDR/Notes](#fpga-extended-general-purpose-computer-architecture)]

- [23/04] **Generative Agents: Interactive Simulacra of Human Behavior**  
[[Paper](http://arxiv.org/pdf/2304.03442v2)] [[Code/Page]()] [[TLDR/Notes](#generative-agents--interactive-simulacra-of-human-behavior)]

- [22/01] **Multi-Stage Episodic Control for Strategic Exploration in Text Games**  
[[Paper](http://arxiv.org/pdf/2201.01251v3)] [[Code/Page]()] [[TLDR/Notes](#multi-stage-episodic-control-for-strategic-exploration-in-text-games)]

- [24/02] **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**  
[[Paper](http://arxiv.org/pdf/2402.17574v3)] [[Code/Page]()] [[TLDR/Notes](#agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization)]

- [25/02] **Towards Automation of Cognitive Modeling using Large Language Models**  
[[Paper](http://arxiv.org/pdf/2502.00879v1)] [[Code/Page]()] [[TLDR/Notes](#towards-automation-of-cognitive-modeling-using-large-language-models)]

- [25/01] **RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing**  
[[Paper](http://arxiv.org/pdf/2501.18160v2)] [[Code/Page]()] [[TLDR/Notes](#repoaudit--an-autonomous-llm-agent-for-repository-level-code-auditing)]

- [23/12] **Empowering Working Memory for Large Language Model Agents**  
[[Paper](http://arxiv.org/pdf/2312.17259v2)] [[Code/Page]()] [[TLDR/Notes](#empowering-working-memory-for-large-language-model-agents)]

- [23/05] **Voyager: An Open-Ended Embodied Agent with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.16291v2)] [[Code/Page](https://voyager.minedojo.org/.)] [[TLDR/Notes](#voyager--an-open-ended-embodied-agent-with-large-language-models)]

- [23/05] **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**  
[[Paper](http://arxiv.org/pdf/2305.17144v2)] [[Code/Page](https://github.com/OpenGVLab/GITM.)] [[TLDR/Notes](#ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory)]

- [24/03] **Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation**  
[[Paper](http://arxiv.org/pdf/2403.08282v2)] [[Code/Page]()] [[TLDR/Notes](#hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation)]

- [16/06] **Human-Agent Decision-making: Combining Theory and Practice**  
[[Paper](http://arxiv.org/pdf/1606.07514v1)] [[Code/Page]()] [[TLDR/Notes](#human-agent-decision-making--combining-theory-and-practice)]

- [23/08] **CALYPSO: LLMs as Dungeon Masters' Assistants**  
[[Paper](http://arxiv.org/pdf/2308.07540v1)] [[Code/Page]()] [[TLDR/Notes](#calypso--llms-as-dungeon-masters--assistants)]

- [23/10] **RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.00746v3)] [[Code/Page]()] [[TLDR/Notes](#rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models)]

- [23/05] **PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits**  
[[Paper](http://arxiv.org/pdf/2305.02547v5)] [[Code/Page]()] [[TLDR/Notes](#personallm--investigating-the-ability-of-large-language-models-to-express-personality-traits)]

- [23/08] **ChatHaruhi: Reviving Anime Character in Reality via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2308.09597v1)] [[Code/Page](https://github.com/LC1332/Chat-Haruhi-Suzumiya)] [[TLDR/Notes](#chatharuhi--reviving-anime-character-in-reality-via-large-language-model)]

- [23/04] **LaMP: When Large Language Models Meet Personalization**  
[[Paper](http://arxiv.org/pdf/2304.11406v4)] [[Code/Page]()] [[TLDR/Notes](#lamp--when-large-language-models-meet-personalization)]

- [23/03] **CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society**  
[[Paper](http://arxiv.org/pdf/2303.17760v2)] [[Code/Page](https://github.com/camel-ai/camel.)] [[TLDR/Notes](#camel--communicative-agents-for--mind--exploration-of-large-language-model-society)]

- [23/09] **An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents**  
[[Paper](http://arxiv.org/pdf/2309.05076v1)] [[Code/Page]()] [[TLDR/Notes](#an-appraisal-based-chain-of-emotion-architecture-for-affective-language-model-game-agents)]

- [15/08] **Word sense disambiguation: a survey**  
[[Paper](http://arxiv.org/pdf/1508.01346v1)] [[Code/Page]()] [[TLDR/Notes](#word-sense-disambiguation--a-survey)]

- [23/03] **Self-Refine: Iterative Refinement with Self-Feedback**  
[[Paper](http://arxiv.org/pdf/2303.17651v2)] [[Code/Page]()] [[TLDR/Notes](#self-refine--iterative-refinement-with-self-feedback)]

- [23/08] **ProAgent: Building Proactive Cooperative Agents with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2308.11339v3)] [[Code/Page](https://pku-proagent.github.io}.)] [[TLDR/Notes](#proagent--building-proactive-cooperative-agents-with-large-language-models)]

- [23/03] **Language Models can Solve Computer Tasks**  
[[Paper](http://arxiv.org/pdf/2303.17491v3)] [[Code/Page](https://github.com/posgnu/rci-agent.)] [[TLDR/Notes](#language-models-can-solve-computer-tasks)]

- [25/00] **Quantizing Constrained Systems: New Perspectives**  
[[Paper](http://arxiv.org/pdf/quant-ph/9810037v1)] [[Code/Page]()] [[TLDR/Notes](#quantizing-constrained-systems--new-perspectives)]

- [23/10] **LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03903v2)] [[Code/Page](https://github.com/eric-ai-lab/llm_coordination}.)] [[TLDR/Notes](#llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models)]

- [23/09] **AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback**  
[[Paper](http://arxiv.org/pdf/2309.17176v3)] [[Code/Page]()] [[TLDR/Notes](#adarefiner--refining-decisions-of-language-models-with-adaptive-feedback)]

- [23/11] **ADaPT: As-Needed Decomposition and Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2311.05772v2)] [[Code/Page]()] [[TLDR/Notes](#adapt--as-needed-decomposition-and-planning-with-language-models)]

- [23/05] **SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks**  
[[Paper](http://arxiv.org/pdf/2305.17390v2)] [[Code/Page]()] [[TLDR/Notes](#swiftsage--a-generative-agent-with-fast-and-slow-thinking-for-complex-interactive-tasks)]

- [22/01] **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2201.07207v2)] [[Code/Page](https://huangwl18.github.io/language-planner)] [[TLDR/Notes](#language-models-as-zero-shot-planners--extracting-actionable-knowledge-for-embodied-agents)]

- [14/03] **Software Agents Interaction Algorithms in Virtual Learning Environment**  
[[Paper](http://arxiv.org/pdf/1403.5734v2)] [[Code/Page]()] [[TLDR/Notes](#software-agents-interaction-algorithms-in-virtual-learning-environment)]

- [23/09] **MindAgent: Emergent Gaming Interaction**  
[[Paper](http://arxiv.org/pdf/2309.09971v2)] [[Code/Page]()] [[TLDR/Notes](#mindagent--emergent-gaming-interaction)]

- [23/03] **Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks**  
[[Paper](http://arxiv.org/pdf/2303.16563v2)] [[Code/Page](https://sites.google.com/view/plan4mc.)] [[TLDR/Notes](#skill-reinforcement-learning-and-planning-for-open-world-long-horizon-tasks)]

- [19/10] **On the Utility of Learning about Humans for Human-AI Coordination**  
[[Paper](http://arxiv.org/pdf/1910.05789v2)] [[Code/Page](https://github.com/HumanCompatibleAI/overcooked_ai.)] [[TLDR/Notes](#on-the-utility-of-learning-about-humans-for-human-ai-coordination)]

- [17/05] **Text-based Adventures of the Golovin AI Agent**  
[[Paper](http://arxiv.org/pdf/1705.05637v1)] [[Code/Page]()] [[TLDR/Notes](#text-based-adventures-of-the-golovin-ai-agent)]

- [19/09] **Interactive Fiction Games: A Colossal Adventure**  
[[Paper](http://arxiv.org/pdf/1909.05398v3)] [[Code/Page]()] [[TLDR/Notes](#interactive-fiction-games--a-colossal-adventure)]

- [23/12] **LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination**  
[[Paper](http://arxiv.org/pdf/2312.15224v2)] [[Code/Page]()] [[TLDR/Notes](#llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination)]

- [23/02] **Q-Cogni: An Integrated Causal Reinforcement Learning Framework**  
[[Paper](http://arxiv.org/pdf/2302.13240v1)] [[Code/Page]()] [[TLDR/Notes](#q-cogni--an-integrated-causal-reinforcement-learning-framework)]

- [18/06] **VirtualHome: Simulating Household Activities via Programs**  
[[Paper](http://arxiv.org/pdf/1806.07011v1)] [[Code/Page]()] [[TLDR/Notes](#virtualhome--simulating-household-activities-via-programs)]

- [22/10] **Grounding Language with Visual Affordances over Unstructured Data**  
[[Paper](http://arxiv.org/pdf/2210.01911v3)] [[Code/Page](http://hulc2.cs.uni-freiburg.de)] [[TLDR/Notes](#grounding-language-with-visual-affordances-over-unstructured-data)]

- [23/11] **War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars**  
[[Paper](http://arxiv.org/pdf/2311.17227v2)] [[Code/Page](https://github.com/agiresearch/WarAgent}.)] [[TLDR/Notes](#war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars)]

- [23/05] **Language Models Meet World Models: Embodied Experiences Enhance Language Models**  
[[Paper](http://arxiv.org/pdf/2305.10626v3)] [[Code/Page]()] [[TLDR/Notes](#language-models-meet-world-models--embodied-experiences-enhance-language-models)]

- [23/10] **LLaMA Rider: Spurring Large Language Models to Explore the Open World**  
[[Paper](http://arxiv.org/pdf/2310.08922v1)] [[Code/Page]()] [[TLDR/Notes](#llama-rider--spurring-large-language-models-to-explore-the-open-world)]

- [23/06] **Large Sequence Models for Sequential Decision-Making: A Survey**  
[[Paper](http://arxiv.org/pdf/2306.13945v1)] [[Code/Page](https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5)] [[TLDR/Notes](#large-sequence-models-for-sequential-decision-making--a-survey)]

- [23/02] **Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2302.02662v4)] [[Code/Page]()] [[TLDR/Notes](#grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning)]

- [17/07] **Proximal Policy Optimization Algorithms**  
[[Paper](http://arxiv.org/pdf/1707.06347v2)] [[Code/Page]()] [[TLDR/Notes](#proximal-policy-optimization-algorithms)]

- [24/02] **Enhance Reasoning for Large Language Models in the Game Werewolf**  
[[Paper](http://arxiv.org/pdf/2402.02330v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-reasoning-for-large-language-models-in-the-game-werewolf)]

- [23/03] **Reward Design with Language Models**  
[[Paper](http://arxiv.org/pdf/2303.00001v1)] [[Code/Page]()] [[TLDR/Notes](#reward-design-with-language-models)]

- [23/10] **Motif: Intrinsic Motivation from Artificial Intelligence Feedback**  
[[Paper](http://arxiv.org/pdf/2310.00166v1)] [[Code/Page]()] [[TLDR/Notes](#motif--intrinsic-motivation-from-artificial-intelligence-feedback)]

- [23/12] **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**  
[[Paper](http://arxiv.org/pdf/2312.09238v2)] [[Code/Page]()] [[TLDR/Notes](#auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft)]

- [24/03] **Automated Feature Selection for Inverse Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2403.15079v1)] [[Code/Page](https://sites.google.com/view/feature4irl.)] [[TLDR/Notes](#automated-feature-selection-for-inverse-reinforcement-learning)]

- [23/10] **Eureka: Human-Level Reward Design via Coding Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.12931v2)] [[Code/Page]()] [[TLDR/Notes](#eureka--human-level-reward-design-via-coding-large-language-models)]

- [23/10] **Lyfe Agents: Generative agents for low-cost real-time social interactions**  
[[Paper](http://arxiv.org/pdf/2310.02172v1)] [[Code/Page]()] [[TLDR/Notes](#lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions)]

- [21/09] **Benchmarking the Spectrum of Agent Capabilities**  
[[Paper](http://arxiv.org/pdf/2109.06780v2)] [[Code/Page]()] [[TLDR/Notes](#benchmarking-the-spectrum-of-agent-capabilities)]

- [20/10] **Keep CALM and Explore: Language Models for Action Generation in Text-based Games**  
[[Paper](http://arxiv.org/pdf/2010.02903v1)] [[Code/Page](https://github.com/princeton-nlp/calm-textgame.)] [[TLDR/Notes](#keep-calm-and-explore--language-models-for-action-generation-in-text-based-games)]

- [23/04] **Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions**  
[[Paper](http://arxiv.org/pdf/2304.02868v1)] [[Code/Page]()] [[TLDR/Notes](#can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions)]

- [23/10] **Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game**  
[[Paper](http://arxiv.org/pdf/2310.18940v3)] [[Code/Page]()] [[TLDR/Notes](#language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game)]

- [23/10] **AvalonBench: Evaluating LLMs Playing the Game of Avalon**  
[[Paper](http://arxiv.org/pdf/2310.05036v3)] [[Code/Page]()] [[TLDR/Notes](#avalonbench--evaluating-llms-playing-the-game-of-avalon)]

- [23/12] **Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game**  
[[Paper](http://arxiv.org/pdf/2312.17515v1)] [[Code/Page]()] [[TLDR/Notes](#cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game)]

- [24/02] **What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents**  
[[Paper](http://arxiv.org/pdf/2402.13184v5)] [[Code/Page](https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.)] [[TLDR/Notes](#what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents)]

- [23/10] **Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.20499v2)] [[Code/Page]()] [[TLDR/Notes](#leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models)]

- [23/08] **GameEval: Evaluating LLMs on Conversational Games**  
[[Paper](http://arxiv.org/pdf/2308.10032v1)] [[Code/Page](https://github.com/GameEval/GameEval.)] [[TLDR/Notes](#gameeval--evaluating-llms-on-conversational-games)]

- [22/10] **Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task**  
[[Paper](http://arxiv.org/pdf/2210.13382v5)] [[Code/Page]()] [[TLDR/Notes](#emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task)]

- [23/08] **Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis**  
[[Paper](http://arxiv.org/pdf/2308.12466v2)] [[Code/Page]()] [[TLDR/Notes](#are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis)]

- [23/10] **Humanoid Agents: Platform for Simulating Human-like Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.05418v1)] [[Code/Page](https://www.humanoidagents.com/)] [[TLDR/Notes](#humanoid-agents--platform-for-simulating-human-like-generative-agents)]

- [18/06] **TextWorld: A Learning Environment for Text-based Games**  
[[Paper](http://arxiv.org/pdf/1806.11532v2)] [[Code/Page]()] [[TLDR/Notes](#textworld--a-learning-environment-for-text-based-games)]

- [18/06] **Counting to Explore and Generalize in Text-based Games**  
[[Paper](http://arxiv.org/pdf/1806.11525v2)] [[Code/Page]()] [[TLDR/Notes](#counting-to-explore-and-generalize-in-text-based-games)]

- [21/07] **Pre-trained Language Models as Prior Knowledge for Playing Text-based Games**  
[[Paper](http://arxiv.org/pdf/2107.08408v2)] [[Code/Page]()] [[TLDR/Notes](#pre-trained-language-models-as-prior-knowledge-for-playing-text-based-games)]

- [23/11] **Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games**  
[[Paper](http://arxiv.org/pdf/2311.07687v1)] [[Code/Page]()] [[TLDR/Notes](#language-model-in-the-loop--data-optimal-approach-to-learn-to-recommend-actions-in-text-games)]

- [20/10] **Keep CALM and Explore: Language Models for Action Generation in Text-based Games**  
[[Paper](http://arxiv.org/pdf/2010.02903v1)] [[Code/Page](https://github.com/princeton-nlp/calm-textgame.)] [[TLDR/Notes](#keep-calm-and-explore--language-models-for-action-generation-in-text-based-games)]

- [20/01] **Graph Constrained Reinforcement Learning for Natural Language Action Spaces**  
[[Paper](http://arxiv.org/pdf/2001.08837v1)] [[Code/Page]()] [[TLDR/Notes](#graph-constrained-reinforcement-learning-for-natural-language-action-spaces)]

- [20/10] **Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2010.02386v1)] [[Code/Page](https://github.com/XiaoxiaoGuo/rcdqn.)] [[TLDR/Notes](#interactive-fiction-game-playing-as-multi-paragraph-reading-comprehension-with-reinforcement-learning)]

- [19/10] **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**  
[[Paper](http://arxiv.org/pdf/1910.13461v1)] [[Code/Page]()] [[TLDR/Notes](#bart--denoising-sequence-to-sequence-pre-training-for-natural-language-generation--translation--and-comprehension)]

- [23/12] **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**  
[[Paper](http://arxiv.org/pdf/2312.00746v2)] [[Code/Page]()] [[TLDR/Notes](#deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games)]

- [21/02] **Chess as a Testbed for Language Model State Tracking**  
[[Paper](http://arxiv.org/pdf/2102.13249v2)] [[Code/Page]()] [[TLDR/Notes](#chess-as-a-testbed-for-language-model-state-tracking)]

- [17/01] **DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker**  
[[Paper](http://arxiv.org/pdf/1701.01724v3)] [[Code/Page]()] [[TLDR/Notes](#deepstack--expert-level-artificial-intelligence-in-no-limit-poker)]

- [20/03] **Too many cooks: Bayesian inference for coordinating multi-agent collaboration**  
[[Paper](http://arxiv.org/pdf/2003.11778v2)] [[Code/Page]()] [[TLDR/Notes](#too-many-cooks--bayesian-inference-for-coordinating-multi-agent-collaboration)]

- [20/10] **Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration**  
[[Paper](http://arxiv.org/pdf/2010.09890v2)] [[Code/Page]()] [[TLDR/Notes](#watch-and-help--a-challenge-for-social-perception-and-human-ai-collaboration)]

- [21/03] **The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI**  
[[Paper](http://arxiv.org/pdf/2103.14025v1)] [[Code/Page]()] [[TLDR/Notes](#the-threedworld-transport-challenge--a-visually-guided-task-and-motion-planning-benchmark-for-physically-realistic-embodied-ai)]

- [17/12] **AI2-THOR: An Interactive 3D Environment for Visual AI**  
[[Paper](http://arxiv.org/pdf/1712.05474v4)] [[Code/Page](http://ai2thor.allenai.org.)] [[TLDR/Notes](#ai2-thor--an-interactive-3d-environment-for-visual-ai)]

- [19/10] **Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments**  
[[Paper](http://arxiv.org/pdf/1910.14442v3)] [[Code/Page](https://sites.google.com/view/interactivegibsonenv))] [[TLDR/Notes](#interactive-gibson-benchmark-(igibson-0-5)--a-benchmark-for-interactive-navigation-in-cluttered-environments)]

- [19/04] **Habitat: A Platform for Embodied AI Research**  
[[Paper](http://arxiv.org/pdf/1904.01201v2)] [[Code/Page]()] [[TLDR/Notes](#habitat--a-platform-for-embodied-ai-research)]

- [21/08] **BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments**  
[[Paper](http://arxiv.org/pdf/2108.03332v1)] [[Code/Page]()] [[TLDR/Notes](#behavior--benchmark-for-everyday-household-activities-in-virtual--interactive--and-ecological-environments)]

- [24/03] **BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation**  
[[Paper](http://arxiv.org/pdf/2403.09227v1)] [[Code/Page](https://behavior.stanford.edu.)] [[TLDR/Notes](#behavior-1k--a-human-centered--embodied-ai-benchmark-with-1-000-everyday-activities-and-realistic-simulation)]

- [17/05] **Automatic Goal Generation for Reinforcement Learning Agents**  
[[Paper](http://arxiv.org/pdf/1705.06366v5)] [[Code/Page]()] [[TLDR/Notes](#automatic-goal-generation-for-reinforcement-learning-agents)]

- [23/05] **ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings**  
[[Paper](http://arxiv.org/pdf/2305.11554v4)] [[Code/Page]()] [[TLDR/Notes](#toolkengpt--augmenting-frozen-language-models-with-massive-tools-via-tool-embeddings)]

- [23/10] **MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.06500v1)] [[Code/Page]()] [[TLDR/Notes](#metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents)]

- [14/03] **Face Recognition Methods & Applications**  
[[Paper](http://arxiv.org/pdf/1403.0485v1)] [[Code/Page]()] [[TLDR/Notes](#face-recognition-methods-&-applications)]

- [24/03] **Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents**  
[[Paper](http://arxiv.org/pdf/2403.00690v1)] [[Code/Page]()] [[TLDR/Notes](#playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents)]

- [22/03] **Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation**  
[[Paper](http://arxiv.org/pdf/2203.06386v2)] [[Code/Page]()] [[TLDR/Notes](#enabling-multimodal-generation-on-clip-via-vision-language-knowledge-distillation)]

- [24/01] **Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering**  
[[Paper](http://arxiv.org/pdf/2401.08500v1)] [[Code/Page](https://github.com/Codium-ai/AlphaCodium)] [[TLDR/Notes](#code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering)]

- [24/06] **World Models with Hints of Large Language Models for Goal Achieving**  
[[Paper](http://arxiv.org/pdf/2406.07381v1)] [[Code/Page]()] [[TLDR/Notes](#world-models-with-hints-of-large-language-models-for-goal-achieving)]

- [24/07] **Enhancing Agent Learning through World Dynamics Modeling**  
[[Paper](http://arxiv.org/pdf/2407.17695v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-agent-learning-through-world-dynamics-modeling)]

- [17/10] **Watch Your Step: Learning Node Embeddings via Graph Attention**  
[[Paper](http://arxiv.org/pdf/1710.09599v2)] [[Code/Page]()] [[TLDR/Notes](#watch-your-step--learning-node-embeddings-via-graph-attention)]

- [23/12] **From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2312.08662v1)] [[Code/Page]()] [[TLDR/Notes](#from-centralized-to-self-supervised--pursuing-realistic-multi-agent-reinforcement-learning)]

- [24/05] **Agent Planning with World Knowledge Model**  
[[Paper](http://arxiv.org/pdf/2405.14205v4)] [[Code/Page](https://github.com/zjunlp/WKM.)] [[TLDR/Notes](#agent-planning-with-world-knowledge-model)]

- [24/05] **THREAD: Thinking Deeper with Recursive Spawning**  
[[Paper](http://arxiv.org/pdf/2405.17402v1)] [[Code/Page]()] [[TLDR/Notes](#thread--thinking-deeper-with-recursive-spawning)]

- [24/03] **EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2403.12014v2)] [[Code/Page]()] [[TLDR/Notes](#envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents)]

- [24/07] **Odyssey: Empowering Minecraft Agents with Open-World Skills**  
[[Paper](http://arxiv.org/pdf/2407.15325v2)] [[Code/Page]()] [[TLDR/Notes](#odyssey--empowering-minecraft-agents-with-open-world-skills)]

- [24/03] **ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy**  
[[Paper](http://arxiv.org/pdf/2403.14589v3)] [[Code/Page]()] [[TLDR/Notes](#react-meets-actre--when-language-agents-enjoy-training-data-autonomy)]

- [24/02] **Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents**  
[[Paper](http://arxiv.org/pdf/2402.11651v2)] [[Code/Page]()] [[TLDR/Notes](#learning-from-failure--integrating-negative-examples-when-fine-tuning-large-language-models-as-agents)]

- [24/03] **Language Guided Exploration for RL Agents in Text Environments**  
[[Paper](http://arxiv.org/pdf/2403.03141v1)] [[Code/Page]()] [[TLDR/Notes](#language-guided-exploration-for-rl-agents-in-text-environments)]



# TLDR/Notes
## the-arcade-learning-environment--an-evaluation-platform-for-general-agents
### Abstract
In this article we introduce the Arcade Learning Environment (ALE): both a
challenge problem and a platform and methodology for evaluating the development
of general, domain-independent AI technology. ALE provides an interface to
hundreds of Atari 2600 game environments, each one different, interesting, and
designed to be a challenge for human players. ALE presents significant research
challenges for reinforcement learning, model learning, model-based planning,
imitation learning, transfer learning, and intrinsic motivation. Most
importantly, it provides a rigorous testbed for evaluating and comparing
approaches to these problems. We illustrate the promise of ALE by developing
and benchmarking domain-independent agents designed using well-established AI
techniques for both reinforcement learning and planning. In doing so, we also
propose an evaluation methodology made possible by ALE, reporting empirical
results on over 55 different games. All of the software, including the
benchmark agents, is publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šç”¨æ™ºèƒ½ä½“è¯„ä¼°å¹³å°ï¼šArcade Learning Environment

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é•¿æœŸä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªç›®æ ‡å°±æ˜¯å¼€å‘èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡å’Œé¢†åŸŸä¸­é€šç”¨çš„ç®—æ³•ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå®šåˆ¶ã€‚ç„¶è€Œï¼Œå¦‚ä½•è¯„ä¼°è¿™äº›é€šç”¨æ™ºèƒ½ä½“çš„æ€§èƒ½ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¾€å¾€åªåœ¨ä¸€å°éƒ¨åˆ†å‚æ•°åŒ–çš„åŸºå‡†é—®é¢˜ä¸Šè¿›è¡Œï¼Œå®¹æ˜“å¯¼è‡´æ–¹æ³•è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”å¿½ç•¥äº†å°†ç®—æ³•è½¬ç§»åˆ°æ–°é¢†åŸŸæ‰€éœ€çš„ä¸“å®¶åŠªåŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†Arcade Learning Environment (ALE)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æŒ‘æˆ˜é—®é¢˜ã€å¹³å°å’Œå®éªŒæ–¹æ³•ï¼Œç”¨äºè¯„ä¼°é€šç”¨æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚ALEæä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå¯ä»¥ä¸æ•°ç™¾ä¸ªAtari 2600æ¸¸æˆç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ¯ä¸ªæ¸¸æˆç¯å¢ƒéƒ½ä¸åŒã€æœ‰è¶£ï¼Œå¹¶ä¸”å¯¹äººç±»ç©å®¶æ¥è¯´éƒ½æ˜¯ä¸€ç§æŒ‘æˆ˜ã€‚ALEä¸ºå¼ºåŒ–å­¦ä¹ ã€æ¨¡å‹å­¦ä¹ ã€åŸºäºæ¨¡å‹çš„è§„åˆ’ã€æ¨¡ä»¿å­¦ä¹ ã€è¿ç§»å­¦ä¹ å’Œå†…åœ¨åŠ¨æœºç­‰é¢†åŸŸçš„ç ”ç©¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒè¿™äº›é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å¼€å‘å’ŒåŸºå‡†æµ‹è¯•ä½¿ç”¨å·²å»ºç«‹çš„AIæŠ€æœ¯è®¾è®¡çš„åŸŸæ— å…³æ™ºèƒ½ä½“ï¼Œå±•ç¤ºäº†ALEçš„æ½œåŠ›ã€‚è¿™äº›æ™ºèƒ½ä½“åˆ†åˆ«ç”¨äºå¼ºåŒ–å­¦ä¹ å’Œè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›å­¦ä¹ è¿›å±•å·²ç»åœ¨Atari 2600æ¸¸æˆä¸­å®ç°ï¼Œä½†ä»æœ‰å¤§é‡å·¥ä½œè¦åšã€‚ä¸åŒçš„æ–¹æ³•åœ¨ä¸åŒçš„æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰æ¸¸æˆä¸­éƒ½è¡¨ç°è‰¯å¥½ã€‚ä¸€äº›æ¸¸æˆç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¾‹å¦‚éœ€è¦é«˜çº§è§„åˆ’çš„Montezumaâ€™s Revengeæ¸¸æˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ALEä¸ºè¯„ä¼°å’Œæ¯”è¾ƒé€šç”¨æ™ºèƒ½ä½“çš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´é€šç”¨çš„ç®—æ³•ï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚æ­¤å¤–ï¼ŒALEè¿˜å¯ä»¥ç”¨äºç ”ç©¶å„ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œä¾‹å¦‚å¼ºåŒ–å­¦ä¹ ã€è§„åˆ’ã€æ¨¡ä»¿å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ã€‚

## arlo--a-framework-for-automated-reinforcement-learning
### Abstract
Automated Reinforcement Learning (AutoRL) is a relatively new area of
research that is gaining increasing attention. The objective of AutoRL consists
in easing the employment of Reinforcement Learning (RL) techniques for the
broader public by alleviating some of its main challenges, including data
collection, algorithm selection, and hyper-parameter tuning. In this work, we
propose a general and flexible framework, namely ARLO: Automated Reinforcement
Learning Optimizer, to construct automated pipelines for AutoRL. Based on this,
we propose a pipeline for offline and one for online RL, discussing the
components, interaction, and highlighting the difference between the two
settings. Furthermore, we provide a Python implementation of such pipelines,
released as an open-source library. Our implementation has been tested on an
illustrative LQG domain and on classic MuJoCo environments, showing the ability
to reach competitive performances requiring limited human intervention. We also
showcase the full pipeline on a realistic dam environment, automatically
performing the feature selection and the model generation tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ARLOï¼šè‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§£å†³å¤æ‚æ§åˆ¶é—®é¢˜æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ“ä½œå’Œé‡‘èã€‚ç„¶è€Œï¼ŒRL æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å—åˆ°æ•°æ®æ”¶é›†ã€ç®—æ³•é€‰æ‹©å’Œè¶…å‚æ•°è°ƒæ•´ç­‰æŒ‘æˆ˜çš„é™åˆ¶ã€‚è¿™ä½¿å¾— RL æŠ€æœ¯éš¾ä»¥è¢«éä¸“å®¶ç”¨æˆ·ä½¿ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº† ARLOï¼ˆè‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨ï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºè‡ªåŠ¨åŒ–ç®¡é“ä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚ARLO æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç®¡é“ï¼šç¦»çº¿ RL ç®¡é“å’Œåœ¨çº¿ RL ç®¡é“ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç¦»çº¿ RL ç®¡é“
ç¦»çº¿ RL ç®¡é“åŒ…æ‹¬æ•°æ®ç”Ÿæˆã€æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹ã€ç­–ç•¥ç”Ÿæˆå’Œç­–ç•¥è¯„ä¼°äº”ä¸ªé˜¶æ®µã€‚è¯¥ç®¡é“åˆ©ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†è¿›è¡Œç­–ç•¥å­¦ä¹ ï¼Œæ— éœ€ä¸ç¯å¢ƒç›´æ¥äº¤äº’ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨çº¿ RL ç®¡é“
åœ¨çº¿ RL ç®¡é“åŒ…æ‹¬ç‰¹å¾å·¥ç¨‹ã€ç­–ç•¥ç”Ÿæˆå’Œç­–ç•¥è¯„ä¼°ä¸‰ä¸ªé˜¶æ®µã€‚è¯¥ç®¡é“ç›´æ¥ä¸ç¯å¢ƒäº¤äº’ï¼Œå®æ—¶æ”¶é›†æ•°æ®å¹¶æ›´æ–°ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ LQG åŸŸå’Œ MuJoCo ç¯å¢ƒä¸Šæµ‹è¯•äº† ARLO æ¡†æ¶ã€‚ç»“æœè¡¨æ˜ï¼ŒARLO æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°é…ç½®ï¼Œå¹¶åœ¨æœ‰é™çš„äººå·¥å¹²é¢„ä¸‹è¾¾åˆ°ç«äº‰æ€§çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ARLO æ¡†æ¶ä¸ºè‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **æ¨¡å—åŒ–è®¾è®¡**ï¼š ARLO æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®å…·ä½“é—®é¢˜æ·»åŠ æˆ–ä¿®æ”¹ç®¡é“ä¸­çš„é˜¶æ®µã€‚
* **è‡ªåŠ¨åŒ–è¶…å‚æ•°è°ƒæ•´**ï¼š ARLO æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°é…ç½®ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
* **å¼€æºå®ç°**ï¼š ARLO æ¡†æ¶çš„å¼€æºå®ç°ä¸ºç”¨æˆ·æä¾›äº†æ–¹ä¾¿çš„å®éªŒå¹³å°ã€‚

### ğŸŒŸ æ€»ç»“
ARLO æ¡†æ¶ä¸ºè‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºé™ä½ RL æŠ€æœ¯çš„ä½¿ç”¨é—¨æ§›ï¼Œä½¿å…¶æ›´æ˜“äºè¢«éä¸“å®¶ç”¨æˆ·ä½¿ç”¨ã€‚

## challenges-and-applications-of-large-language-models
### Abstract
Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹ï¼šæŒ‘æˆ˜ä¸åº”ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ­çŸ­å‡ å¹´å†…ä»æ— åˆ°æœ‰ï¼Œè¿…é€Ÿæˆä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸»æµã€‚ç„¶è€Œï¼Œç”±äºè¯¥é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œè¯†åˆ«å‰©ä½™çš„æŒ‘æˆ˜å’Œå·²ç»å–å¾—æˆåŠŸçš„åº”ç”¨é¢†åŸŸå˜å¾—å›°éš¾ã€‚æœ¬æ–‡æ—¨åœ¨å»ºç«‹ä¸€ä¸ªç³»ç»Ÿçš„å¼€æ”¾é—®é¢˜å’Œåº”ç”¨æˆåŠŸæ¡ˆä¾‹çš„é›†åˆï¼Œä»¥ä¾¿æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜å¯ä»¥æ›´å¿«åœ°ç†è§£è¯¥é¢†åŸŸçš„å½“å‰çŠ¶æ€å¹¶æé«˜æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡å°†LLMsçš„æŒ‘æˆ˜åˆ†ä¸ºä¸‰ä¸ªä¸»è¦ç±»åˆ«ï¼šâ€œè®¾è®¡â€ã€â€œè¡Œä¸ºâ€å’Œâ€œç§‘å­¦â€ã€‚

#### è®¾è®¡æŒ‘æˆ˜
* **æ•°æ®é›†éš¾ä»¥ç†è§£**ï¼š é¢„è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡ä½¿å¾—ä»»ä½•ä¸ªäººéƒ½æ— æ³•å½»åº•é˜…è¯»æˆ–å¯¹åŒ…å«çš„æ–‡æ¡£è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚
* **åˆ†è¯å™¨ä¾èµ–æ€§**ï¼š åˆ†è¯å™¨å¼•å…¥äº†å¤šä¸ªæŒ‘æˆ˜ï¼Œä¾‹å¦‚è®¡ç®—å¼€é”€ã€è¯­è¨€ä¾èµ–æ€§ã€å¤„ç†æ–°è¯ã€å›ºå®šè¯æ±‡é‡ã€ä¿¡æ¯ä¸¢å¤±å’Œä½äººç±»å¯è§£é‡Šæ€§ã€‚
* **é«˜é¢„è®­ç»ƒæˆæœ¬**ï¼š è®­ç»ƒå•ä¸ªLLMå¯èƒ½éœ€è¦æ•°åä¸‡å°æ—¶çš„è®¡ç®—æ—¶é—´ï¼Œæˆæœ¬é«˜è¾¾æ•°ç™¾ä¸‡ç¾å…ƒï¼Œå¹¶æ¶ˆè€—ç›¸å½“äºå‡ ä¸ªå…¸å‹ç¾å›½å®¶åº­ä¸€å¹´çš„èƒ½æºé‡ã€‚
* **å¾®è°ƒå¼€é”€**ï¼š é¢„è®­ç»ƒLLMsåœ¨å¤§é‡å’Œå¤šæ ·åŒ–çš„æ–‡æœ¬æ•°æ®é›†ä¸Šï¼Œå¯èƒ½å¯¼è‡´ç»“æœæ¨¡å‹éš¾ä»¥æ˜ç¡®æ•è·ç‰¹å®šä»»åŠ¡æ•°æ®é›†çš„åˆ†å¸ƒç‰¹æ€§ã€‚
* **é«˜æ¨ç†å»¶è¿Ÿ**ï¼š LLMsçš„æ¨ç†å»¶è¿Ÿä»ç„¶å¾ˆé«˜ï¼Œå› ä¸ºä½å¹¶è¡Œæ€§å’Œå¤§å‹å†…å­˜å ç”¨ã€‚

#### è¡Œä¸ºæŒ‘æˆ˜
* **æç¤ºè„†å¼±æ€§**ï¼š æç¤ºçš„è¯­æ³•å’Œè¯­ä¹‰å¯ä»¥å¯¹æ¨¡å‹çš„è¾“å‡ºäº§ç”Ÿé‡å¤§å½±å“ã€‚
* **å¹»è§‰**ï¼š LLMsç»å¸¸äº§ç”ŸåŒ…å«ä¸å‡†ç¡®ä¿¡æ¯çš„å¹»è§‰ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½éš¾ä»¥æ£€æµ‹ï¼Œå› ä¸ºæ–‡æœ¬æµç•…è‡ªç„¶ã€‚
* **è¡Œä¸ºé”™ä½**ï¼š LLMsç»å¸¸ç”Ÿæˆä¸äººç±»ä»·å€¼è§‚æˆ–æ„å›¾ä¸ä¸€è‡´çš„è¾“å‡ºï¼Œè¿™å¯èƒ½å¯¼è‡´æ„å¤–æˆ–è´Ÿé¢çš„åæœã€‚

#### ç§‘å­¦æŒ‘æˆ˜
* **è¿‡æ—¶çš„çŸ¥è¯†**ï¼š é¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„äº‹å®æ€§ä¿¡æ¯å¯èƒ½åŒ…å«ä¸å‡†ç¡®æˆ–éšæ—¶é—´è¿‡æ—¶çš„ä¿¡æ¯ã€‚
* **åŸºäºé™æ€ã€äººå·¥ç¼–å†™çš„çœŸå®å€¼è¯„ä¼°**ï¼š é™æ€åŸºå‡†éšç€æ—¶é—´çš„æ¨ç§»å˜å¾—ä¸é‚£ä¹ˆæœ‰ç”¨ï¼Œå› ä¸ºæ¨¡å‹çš„æ€§èƒ½ä¸æ–­æé«˜ï¼Œè€Œæ›´æ–°å®ƒä»¬é€šå¸¸ä¾èµ–äºäººå·¥ç¼–å†™çš„çœŸå®å€¼ã€‚
* **éš¾ä»¥åŒºåˆ†ç”Ÿæˆæ–‡æœ¬å’Œäººå·¥ç¼–å†™æ–‡æœ¬**ï¼š éš¾ä»¥åŒºåˆ†LLMsç”Ÿæˆçš„æ–‡æœ¬å’Œäººå·¥ç¼–å†™çš„æ–‡æœ¬ã€‚
* **æ— æ³•é€šè¿‡è§„æ¨¡è§£å†³çš„ä»»åŠ¡**ï¼š ä¸€äº›ä»»åŠ¡ä¼¼ä¹æ— æ³•é€šè¿‡è¿›ä¸€æ­¥æ‰©å±•æ•°æ®/æ¨¡å‹è§„æ¨¡æ¥è§£å†³ã€‚
* **ç¼ºä¹å®éªŒè®¾è®¡**ï¼š è®¸å¤šè®ºæ–‡æ²¡æœ‰è¿›è¡Œå—æ§å®éªŒï¼ˆæ¶ˆèå®éªŒï¼‰ï¼Œè¿™å°¤å…¶æˆé—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¾ˆå¤§çš„è®¾è®¡ç©ºé—´ã€‚
* **ç¼ºä¹å¯é‡å¤æ€§**ï¼š LLMç ”ç©¶çš„å¯é‡å¤æ€§å­˜åœ¨ä¸¤ä¸ªç‹¬ç‰¹çš„é—®é¢˜ï¼šè®­ç»ƒè¿è¡Œçš„é‡å¤æ€§å’Œé—­æºAPIæœåŠ¡æ¨¡å‹çš„æ¨ç†é‡å¤æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯å¯¹LLMsçš„æŒ‘æˆ˜å’Œåº”ç”¨è¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºLLMsçš„æŒ‘æˆ˜å’Œåº”ç”¨æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¦‚è¿°ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å®è´µçš„è§è§£ã€‚æœ¬æ–‡è¿˜å¼ºè°ƒäº†LLMsçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚

## exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf
### Abstract
Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ²Ÿé€šæ¸¸æˆä¸­çš„åº”ç”¨ï¼šä»¥ç‹¼äººæ€ä¸ºä¾‹çš„å®è¯ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ²Ÿé€šæ¸¸æˆï¼Œå¦‚ç‹¼äººæ€ï¼Œæ˜¯ä¸€ç§é‡è¦çš„ç ”ç©¶å·¥å…·ï¼Œå¯ä»¥ç”¨æ¥æ¢ç´¢ç»æµå­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸä¸­çš„å„ç§é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIä»£ç†åœ¨ç©è¿™ç±»æ¸¸æˆæ—¶ï¼Œè¦ä¹ˆå¯¹è¯­è¨€çš„ä½¿ç”¨æœ‰ä¸¥æ ¼çš„é™åˆ¶ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè¿™ä½¿å¾—AIä»£ç†åœ¨è‡ªç„¶åœ°ç©è¿™ç±»æ¸¸æˆæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œç”¨äºç©æ²Ÿé€šæ¸¸æˆï¼Œå¹¶ä»¥ç‹¼äººæ€ä¸ºä¾‹è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå†å²ä¿¡æ¯æ”¶é›†
ä¸ºäº†è§£å†³LLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ä¸‰ä¸ªè§’åº¦ï¼ˆæ–°é²œåº¦ã€ä¿¡æ¯é‡å’Œå®Œæ•´æ€§ï¼‰æ”¶é›†å†å²ä¿¡æ¯çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ï¼š
- æ”¶é›†æœ€è¿‘çš„Kæ¡æ¶ˆæ¯ï¼›
- ä½¿ç”¨è§„åˆ™åŒ¹é…å’Œå¯å‘å¼æŒ‡æ ‡é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„Næ¡æ¶ˆæ¯ï¼›
- é€šè¿‡å›ç­”é—®é¢˜çš„æ–¹å¼ï¼Œä»æ•´ä¸ªå†å²ä¸­æå–æ›´å¤šä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»éªŒå­¦ä¹ 
ä¸ºäº†ä½¿LLMèƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§éå‚æ•°å­¦ä¹ æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ï¼š
- åœ¨æ¯è½®æ¸¸æˆç»“æŸåï¼Œæ”¶é›†æ‰€æœ‰ç©å®¶çš„å“åº”ã€åæ€å’Œå¾—åˆ†ï¼Œå½¢æˆç»éªŒæ± ï¼›
- åœ¨æ–°çš„ä¸€è½®æ¸¸æˆä¸­ï¼Œæ ¹æ®å½“å‰æƒ…å†µä»ç»éªŒæ± ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ç»éªŒï¼Œå¹¶ä»ä¸­æå–å»ºè®®ï¼Œä»¥æŒ‡å¯¼LLMçš„æ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç©ç‹¼äººæ€æ¸¸æˆï¼Œå¹¶ä¸”èƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€å¾®è°ƒLLMçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œå®éªŒä¸­è¿˜è§‚å¯Ÿåˆ°ä¸€äº›ç­–ç•¥æ€§è¡Œä¸ºï¼Œå¦‚ä¿¡ä»»ã€å¯¹æŠ—ã€ä¼ªè£…å’Œé¢†å¯¼ï¼Œè¿™äº›è¡Œä¸ºå¹¶éé¢„å…ˆç¼–ç¨‹ï¼Œè€Œæ˜¯è‡ªå‘åœ°ä»LLMä¸­æ¶Œç°å‡ºæ¥çš„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºä½¿ç”¨LLMç©æ²Ÿé€šæ¸¸æˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶LLMåœ¨æ²Ÿé€šæ¸¸æˆä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ç»éªŒå­¦ä¹ æœºåˆ¶ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿã€‚

## large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach
### Abstract
StarCraft II is a challenging benchmark for AI agents due to the necessity of
both precise micro level operations and strategic macro awareness. Previous
works, such as Alphastar and SCC, achieve impressive performance on tackling
StarCraft II , however, still exhibit deficiencies in long term strategic
planning and strategy interpretability. Emerging large language model (LLM)
agents, such as Voyage and MetaGPT, presents the immense potential in solving
intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs
on StarCraft II, a highly complex RTS game.To conveniently take full advantage
of LLMs` reasoning abilities, we first develop textual StratCraft II
environment, called TextStarCraft II, which LLM agent can interact. Secondly,
we propose a Chain of Summarization method, including single frame
summarization for processing raw observations and multi frame summarization for
analyzing game information, providing command recommendations, and generating
strategic decisions. Our experiment consists of two parts: first, an evaluation
by human experts, which includes assessing the LLMs`s mastery of StarCraft II
knowledge and the performance of LLM agents in the game; second, the in game
performance of LLM agents, encompassing aspects like win rate and the impact of
Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the
relevant knowledge and complex planning abilities needed to address StarCraft
II scenarios; 2. Human experts consider the performance of LLM agents to be
close to that of an average player who has played StarCraft II for eight years;
3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)
difficulty level. We have open sourced the code and released demo videos of LLM
agent playing StarCraft II.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­çš„è¡¨ç°ï¼šåŸºå‡†æµ‹è¯•ä¸æ‘˜è¦é“¾æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ˜Ÿé™…äº‰éœ¸IIï¼ˆStarCraft IIï¼‰æ˜¯ä¸€æ¬¾æå…·æŒ‘æˆ˜æ€§çš„å®æ—¶æˆ˜ç•¥æ¸¸æˆï¼Œè¦æ±‚ç©å®¶åœ¨å¾®è§‚æ“ä½œå’Œå®è§‚æˆ˜ç•¥è§„åˆ’ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å°½ç®¡ä¹‹å‰çš„AIç ”ç©¶ï¼Œå¦‚AlphaStarå’ŒSCCï¼Œåœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œä½†å®ƒä»¬åœ¨é•¿æœŸæˆ˜ç•¥è§„åˆ’å’Œç­–ç•¥å¯è§£é‡Šæ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œæœ¬æ–‡æ—¨åœ¨éªŒè¯LLMåœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTextStarCraft IIç¯å¢ƒ
ä¸ºäº†å……åˆ†åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªåä¸ºTextStarCraft IIçš„æ–‡æœ¬ç¯å¢ƒï¼ŒLLMä»£ç†å¯ä»¥ä¸ä¹‹äº¤äº’ã€‚è¯¥ç¯å¢ƒå°†æ˜Ÿé™…äº‰éœ¸IIçš„å¤æ‚æ¸¸æˆåŠ¨æ€è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå…è®¸LLMä»£ç†é€šè¿‡è¯­è¨€å‘½ä»¤æ‰§è¡Œå®è§‚æˆ˜ç•¥è¡ŒåŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‘˜è¦é“¾ï¼ˆCoSï¼‰æ–¹æ³•
æœ¬æ–‡æå‡ºäº†æ‘˜è¦é“¾ï¼ˆCoSï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å•å¸§æ‘˜è¦å’Œå¤šå¸§æ‘˜è¦ã€‚å•å¸§æ‘˜è¦ç”¨äºå¤„ç†åŸå§‹è§‚å¯Ÿæ•°æ®ï¼Œè€Œå¤šå¸§æ‘˜è¦ç”¨äºåˆ†ææ¸¸æˆä¿¡æ¯ï¼Œæä¾›å‘½ä»¤å»ºè®®å¹¶ç”Ÿæˆæˆ˜ç•¥å†³ç­–ã€‚CoSæ–¹æ³•é€šè¿‡ä¿¡æ¯å‹ç¼©ã€æ¨ç†åŠ é€Ÿå’Œå…¨å±€ç†è§£ï¼Œå¢å¼ºäº†LLMä»£ç†åœ¨å¤„ç†å¤æ‚ä¿¡æ¯å’Œåšå‡ºæˆ˜ç•¥å†³ç­–æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMå…·å¤‡è§£å†³æ˜Ÿé™…äº‰éœ¸IIåœºæ™¯æ‰€éœ€çš„ç›¸å…³çŸ¥è¯†å’Œå¤æ‚è§„åˆ’èƒ½åŠ›ã€‚äººç±»ä¸“å®¶è®¤ä¸ºï¼ŒLLMä»£ç†åœ¨æ¸¸æˆä¸­çš„è¡¨ç°æ¥è¿‘äºç©äº†å…«å¹´æ˜Ÿé™…äº‰éœ¸IIçš„å¹³å‡ç©å®¶ã€‚æ­¤å¤–ï¼ŒLLMä»£ç†èƒ½å¤Ÿåœ¨Harderï¼ˆLv5ï¼‰éš¾åº¦çº§åˆ«ä¸‹å‡»è´¥å†…ç½®AIã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„TextStarCraft IIç¯å¢ƒå’ŒCoSæ–¹æ³•ä¸ºè¯„ä¼°LLMåœ¨å®æ—¶æˆ˜ç•¥å†³ç­–å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥åœ¨æ˜Ÿé™…äº‰éœ¸IIå’Œå…¶ä»–å®æ—¶æˆ˜ç•¥æ¸¸æˆä¸­çš„AIç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models
### Abstract
We introduce PokeLLMon, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pokemon
battles. The design of PokeLLMon incorporates three key strategies: (i)
In-context reinforcement learning that instantly consumes text-based feedback
derived from battles to iteratively refine the policy; (ii) Knowledge-augmented
generation that retrieves external knowledge to counteract hallucination and
enables the agent to act timely and properly; (iii) Consistent action
generation to mitigate the panic switching phenomenon when the agent faces a
powerful opponent and wants to elude the battle. We show that online battles
against human demonstrates PokeLLMon's human-like battle strategies and
just-in-time decision making, achieving 49% of win rate in the Ladder
competitions and 56% of win rate in the invited battles. Our implementation and
playable battle logs are available at: https://github.com/git-disl/PokeLLMon.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PokeLLMonï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å®ç°äººç±»æ°´å¹³çš„å®å¯æ¢¦æˆ˜æ–—AI

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç”Ÿæˆå¼AIå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„æˆåŠŸï¼Œäººä»¬å¼€å§‹æ¢ç´¢LLMså¦‚ä½•è‡ªä¸»åœ°åœ¨ç‰©ç†ä¸–ç•Œä¸­è¡ŒåŠ¨ï¼Œå°†ç”Ÿæˆç©ºé—´ä»æ–‡æœ¬æ‰©å±•åˆ°è¡ŒåŠ¨ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å…³é”®èŒƒå¼ã€‚æ¸¸æˆæ˜¯å¼€å‘LLM-basedä»£ç†ä¸è™šæ‹Ÿç¯å¢ƒäº¤äº’çš„åˆé€‚æµ‹è¯•å¹³å°ã€‚æˆ˜æœ¯æˆ˜æ–—æ¸¸æˆï¼Œå¦‚å®å¯æ¢¦æˆ˜æ–—ï¼Œå› å…¶çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç¦»æ•£ã€å›åˆåˆ¶æ ¼å¼ã€æˆ˜ç•¥æ€§å’Œå¤æ‚æ€§ï¼Œæˆä¸ºè¯„ä¼°LLMsæ¸¸æˆèƒ½åŠ›çš„ç†æƒ³åŸºå‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰
ä¸ºäº†è§£å†³LLMsåœ¨å®å¯æ¢¦æˆ˜æ–—ä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ICRLç­–ç•¥ã€‚ICRLåˆ©ç”¨æˆ˜æ–—ä¸­å³æ—¶ç”Ÿæˆçš„æ–‡æœ¬åé¦ˆä½œä¸ºâ€œå¥–åŠ±â€ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¿­ä»£ä¼˜åŒ–åŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚é€šè¿‡åˆ†æå‰ä¸€è½®çš„è¡ŒåŠ¨å’Œç›¸åº”çš„æ–‡æœ¬åé¦ˆï¼Œä»£ç†èƒ½å¤Ÿä¸æ–­è°ƒæ•´å…¶ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹æˆ˜æ–—ä¸­çš„å˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçŸ¥è¯†å¢å¼ºç”Ÿæˆï¼ˆKAGï¼‰
ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘å¹»è§‰ï¼Œè®ºæ–‡å¼•å…¥äº†KAGç­–ç•¥ã€‚KAGé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼Œå¦‚ç±»å‹ä¼˜åŠ¿/åŠ£åŠ¿å…³ç³»å’ŒæŠ€èƒ½/èƒ½åŠ›æ•ˆæœï¼Œæ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ã€‚è¿™äº›çŸ¥è¯†æ¥æºäºå®å¯æ¢¦æ¸¸æˆä¸­çš„å®å¯æ¢¦å›¾é‰´ï¼ˆPokÃ©dexï¼‰ï¼Œå®ƒæä¾›äº†å…³äºå®å¯æ¢¦ç±»å‹ã€æŠ€èƒ½å’Œèƒ½åŠ›çš„è¯¦ç»†ä¿¡æ¯ã€‚é€šè¿‡å°†å¤–éƒ¨çŸ¥è¯†æ·»åŠ åˆ°çŠ¶æ€æè¿°ä¸­ï¼Œä»£ç†èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£æˆ˜æ–—æƒ…å†µï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆ
ä¸ºäº†è§£å†³ä»£ç†åœ¨é¢å¯¹å¼ºå¤§å¯¹æ‰‹æ—¶å‡ºç°çš„ææ…Œåˆ‡æ¢ç°è±¡ï¼Œè®ºæ–‡æå‡ºäº†ä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆç­–ç•¥ã€‚è¯¥ç­–ç•¥é€šè¿‡å¤šæ¬¡ç‹¬ç«‹ç”Ÿæˆè¡ŒåŠ¨å¹¶æŠ•ç¥¨é€‰å‡ºæœ€ä¸€è‡´çš„è¡ŒåŠ¨ï¼Œæ¥å‡å°‘è¡ŒåŠ¨çš„ä¸ä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºä»£ç†åœ¨é¢å¯¹å‹åŠ›æ—¶ä¿æŒå†·é™ï¼Œé¿å…è¿‡åº¦æ€è€ƒå’Œææ…Œï¼Œä»è€Œåšå‡ºæ›´ç¨³å®šçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨çº¿æˆ˜æ–—ç»“æœè¡¨æ˜ï¼ŒPokeLLMonåœ¨æ¢¯å­æ¯”èµ›ä¸­å–å¾—äº†49%çš„èƒœç‡ï¼Œåœ¨é‚€è¯·æ¯”èµ›ä¸­å–å¾—äº†56%çš„èƒœç‡ï¼Œå±•ç°å‡ºä¸äººç±»ç©å®¶ç›¸å½“çš„æ¯”èµ›èƒ½åŠ›å’Œç­–ç•¥ã€‚ç„¶è€Œï¼ŒPokeLLMonåœ¨é¢å¯¹äººç±»ç©å®¶çš„æ¶ˆè€—ç­–ç•¥å’Œæ¬ºéª—æŠ€å·§æ—¶ä¹Ÿå­˜åœ¨å¼±ç‚¹ï¼Œè¿™è¡¨æ˜æœªæ¥éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›å…¶é•¿æœŸè§„åˆ’å’Œå¯¹æ‰‹è¡Œä¸ºé¢„æµ‹èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PokeLLMonçš„è®¾è®¡å’Œå®ç°ä¸ºLLMsåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚ICRLã€KAGå’Œä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–æ¸¸æˆï¼Œå¸®åŠ©LLMsæ›´å¥½åœ°ç†è§£å’Œåº”å¯¹æ¸¸æˆä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒPokeLLMonçš„å®éªŒç»“æœä¹Ÿæ­ç¤ºäº†LLMsåœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## cooperative-open-ended-learning-framework-for-zero-shot-coordination
### Abstract
Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é›¶æ ·æœ¬åè°ƒçš„åä½œå¼€æ”¾å­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨åˆä½œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸­ï¼Œé›¶æ ·æœ¬åè°ƒï¼ˆZSCï¼‰æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå³å¦‚ä½•æœ‰æ•ˆåœ°ä¸å„ç§æœªè§è¿‡çš„ä¼™ä¼´è¿›è¡Œåè°ƒã€‚ä¼ ç»Ÿçš„è‡ªæˆ‘åšå¼ˆï¼ˆSPï¼‰æ–¹æ³•è™½ç„¶å¯ä»¥æ”¶æ•›åˆ°æ¸¸æˆçš„å‡è¡¡çŠ¶æ€ï¼Œä½†å¾€å¾€å½¢æˆç‰¹å®šçš„è¡Œä¸ºå’Œæƒ¯ä¾‹ï¼Œéš¾ä»¥é€‚åº”ä¸æœªè§è¿‡çš„ç­–ç•¥è¿›è¡Œåè°ƒã€‚ä¸ºäº†å…‹æœSPçš„å±€é™æ€§ï¼Œè®¸å¤šZSCæ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºç¾¤ä½“çš„è®­ç»ƒï¼ˆPBTï¼‰æ¥ä¿ƒè¿›ç­–ç•¥æˆ–è¡Œä¸ºçš„å¤šæ ·æ€§ï¼Œä»¥æé«˜ç­–ç•¥çš„é€‚åº”æ€§ã€‚ç„¶è€Œï¼Œå½“ä¼˜åŒ–å›ºå®šçš„äººå£çº§ç›®æ ‡æ—¶ï¼Œç¾¤ä½“å†…ç­–ç•¥çš„åè°ƒèƒ½åŠ›å¯èƒ½ä¸ä¼šå¾—åˆ°æé«˜ï¼Œå¯¼è‡´æ‰€è°“çš„â€œåˆä½œä¸å…¼å®¹æ€§â€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå›¾å½¢å½¢å¼æ¸¸æˆï¼ˆGFGsï¼‰å’Œåå¥½å›¾å½¢å½¢å¼æ¸¸æˆï¼ˆP-GFGsï¼‰
æœ¬æ–‡æå‡ºäº†å›¾å½¢å½¢å¼æ¸¸æˆï¼ˆGFGsï¼‰å’Œåå¥½å›¾å½¢å½¢å¼æ¸¸æˆï¼ˆP-GFGsï¼‰çš„æ¦‚å¿µï¼Œå°†åˆä½œä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºå›¾å½¢å½¢å¼ï¼Œä»¥ä¾¿æ›´æœ‰æ•ˆåœ°è¯„ä¼°å’Œè¯†åˆ«å­¦ä¹ è¿‡ç¨‹ä¸­çš„åˆä½œä¸å…¼å®¹æ€§ã€‚åœ¨GFGsä¸­ï¼Œç­–ç•¥è¢«è¡¨å¾ä¸ºèŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹æƒé‡è¡¨ç¤ºä¸¤ä¸ªç›¸å…³ç­–ç•¥çš„å¹³å‡åˆä½œæ”¶ç›Šã€‚é€šè¿‡åˆ©ç”¨GFGsçš„å­å›¾ï¼Œå³åå¥½å›¾å½¢å½¢å¼æ¸¸æˆï¼ˆP-GFGsï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥åˆ†ææ¯ä¸ªèŠ‚ç‚¹åœ¨å›¾ä¸­çš„æœ€å¤§åˆä½œæ”¶ç›Šï¼Œä»è€Œè¯„ä¼°åˆä½œä¸å…¼å®¹æ€§å¹¶è¯†åˆ«æ— æ³•åä½œçš„ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåä½œå¼€æ”¾å­¦ä¹ æ¡†æ¶ï¼ˆCOLEï¼‰
ä¸ºäº†è§£å†³åˆä½œä¸å…¼å®¹æ€§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åä½œå¼€æ”¾å­¦ä¹ æ¡†æ¶ï¼ˆCOLEï¼‰ï¼Œè¯¥æ¡†æ¶ä»å›¾å½¢ç†è®ºçš„è§’åº¦æ„å»ºäº†å¼€æ”¾çš„ç›®æ ‡ï¼Œä»¥è¯„ä¼°å’Œè¯†åˆ«æ¯ä¸ªç­–ç•¥çš„åˆä½œèƒ½åŠ›ã€‚COLEæ¡†æ¶é€šè¿‡è¿­ä»£ç”Ÿæˆæ–°çš„ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥è¿‘ä¼¼äºP-GFGsçš„ç»éªŒæ¸¸æˆåœºæ™¯çš„æœ€ä½³ååº”ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§å®ç”¨çš„ç®—æ³•COLESVï¼Œè¯¥ç®—æ³•ç»“åˆäº†åšå¼ˆè®ºå’Œå›¾è®ºçš„çŸ¥è¯†ï¼Œå¹¶è¯æ˜äº†è¯¥ç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å…‹æœåˆä½œä¸å…¼å®¹æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Overcookedæ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨åè°ƒä¸åŒçº§åˆ«çš„ä¼™ä¼´æ—¶ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†æGFGså’ŒP-GFGsï¼ŒCOLESVçš„å­¦ä¹ è¿‡ç¨‹æ­ç¤ºäº†è¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°å…‹æœåˆä½œä¸å…¼å®¹æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GFGså’ŒP-GFGsçš„æ¦‚å¿µä»¥åŠCOLEæ¡†æ¶ä¸ºè§£å†³åˆä½œä¸å…¼å®¹æ€§é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„COLESVç®—æ³•åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¾ˆå¥½çš„æ•ˆæœï¼Œå¯ä»¥ä¸ºå…¶ä»–åˆä½œAIä»»åŠ¡æä¾›å‚è€ƒã€‚

## describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents
### Abstract
We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
"$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼è§„åˆ’ï¼ŒåŠ©åŠ›å¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¤šä»»åŠ¡æ™ºèƒ½ä½“é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰æ‰§è¡Œè®¡åˆ’éœ€è¦ç²¾ç¡®çš„å¤šæ­¥æ¨ç†ï¼Œå› ä¸ºä»»åŠ¡å…·æœ‰é•¿æœŸæ€§ï¼›2ï¼‰ä¼ ç»Ÿçš„è§„åˆ’å™¨åœ¨æ’åºå¤æ‚çš„è®¡åˆ’ä¸­çš„å¹¶è¡Œå­ç›®æ ‡æ—¶ï¼Œæ²¡æœ‰è€ƒè™‘å½“å‰æ™ºèƒ½ä½“å®Œæˆç»™å®šå­ä»»åŠ¡çš„éš¾æ˜“ç¨‹åº¦ï¼Œå¯¼è‡´ç”Ÿæˆçš„è®¡åˆ’å¯èƒ½æ•ˆç‡ä½ä¸‹ç”šè‡³ä¸å¯è¡Œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œæè¿°ã€è§£é‡Šã€è§„åˆ’å’Œé€‰æ‹©â€ï¼ˆDEPSï¼‰çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæè¿°ã€è§£é‡Šå’Œè§„åˆ’
DEPS é€šè¿‡é›†æˆè®¡åˆ’æ‰§è¡Œè¿‡ç¨‹çš„æè¿°å’Œæä¾›è‡ªæˆ‘è§£é‡Šçš„åé¦ˆï¼Œæ›´å¥½åœ°çº æ­£åˆå§‹ LLM ç”Ÿæˆçš„è®¡åˆ’ä¸­çš„é”™è¯¯ã€‚å½“é‡åˆ°å¤±è´¥æ—¶ï¼Œæè¿°å™¨ä¼šæ€»ç»“å½“å‰æƒ…å†µå¹¶å‘é€ç»™ LLMï¼ŒLLM ä½œä¸ºè§£é‡Šå™¨å®šä½é”™è¯¯ï¼Œç„¶åæ ¹æ®æè¿°å™¨å’Œè§£é‡Šå™¨çš„ä¿¡æ¯æ›´æ–°è®¡åˆ’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç›®æ ‡é€‰æ‹©å™¨
DEPS åŒ…å«ä¸€ä¸ªå¯è®­ç»ƒçš„ç›®æ ‡é€‰æ‹©å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®å®Œæˆæ¯ä¸ªå¹¶è¡Œå€™é€‰å­ç›®æ ‡çš„ä¼°è®¡æ­¥éª¤å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œä»è€Œç»†åŒ–åˆå§‹è®¡åˆ’ã€‚é€‰æ‹©å™¨ä½¿ç”¨é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°æ¥å®Œæˆæ¯ä¸ªç›®æ ‡ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æœ€æ¥è¿‘çš„ç›®æ ‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒDEPS åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ Minecraftï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œèƒ½å¤Ÿç¨³å¥åœ°å®Œæˆ 70 å¤šä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”æ•´ä½“æ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼ŒDEPS åœ¨éå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ ALFWorld å’Œæ¡Œé¢æ“ä½œï¼‰ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DEPS çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ä¸ºå¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡é›†æˆæè¿°ã€è§£é‡Šå’Œè§„åˆ’ï¼Œä»¥åŠä½¿ç”¨ç›®æ ‡é€‰æ‹©å™¨ï¼ŒDEPS èƒ½å¤Ÿç”Ÿæˆæ›´å¯é å’Œé«˜æ•ˆçš„è®¡åˆ’ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚

## guiding-pretraining-in-reinforcement-learning-with-large-language-models
### Abstract
Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„é¢„è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ç¼ºä¹å¯†é›†ã€è‰¯å¥½å½¢çŠ¶çš„å¥–åŠ±å‡½æ•°æ—¶é€šå¸¸ä¼šé‡åˆ°å›°éš¾ã€‚å†…åœ¨åŠ¨æœºæ¢ç´¢æ–¹æ³•é€šè¿‡å¥–åŠ±ä»£ç†è®¿é—®æ–°é¢–çŠ¶æ€æˆ–è½¬æ¢æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œä½†åœ¨å¤§å¤šæ•°å‘ç°çš„æ–°é¢–æ€§å¯¹ä¸‹æ¸¸ä»»åŠ¡æ— å…³ç´§è¦çš„å¤§å‹ç¯å¢ƒä¸­ï¼Œè¿™äº›æ–¹æ³•æä¾›çš„ç›Šå¤„æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ¥è‡ªæ–‡æœ¬è¯­æ–™åº“çš„èƒŒæ™¯çŸ¥è¯†æ¥å¡‘é€ æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºELLMï¼ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¢ç´¢ï¼‰ï¼Œå®ƒå¥–åŠ±ä»£ç†å®ç°ç”±è¯­è¨€æ¨¡å‹æå‡ºçš„ä¸ä»£ç†å½“å‰çŠ¶æ€æè¿°ç›¸å…³çš„ç›®æ ‡ã€‚é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼ŒELLMå¼•å¯¼ä»£ç†æœç€äººç±»æœ‰æ„ä¹‰ä¸”å¯èƒ½æœ‰ç”¨çš„è¡Œä¸ºå‘å±•ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒŒæ™¯çŸ¥è¯†æ¥å¡‘é€ æ¢ç´¢ã€‚LLMæ˜¯æ¦‚ç‡æ–‡æœ¬æ¨¡å‹ï¼Œå…¶é¢„æµ‹ç¼–ç äº†ä¸°å¯Œçš„å…³äºäººç±»å¸¸è¯†çŸ¥è¯†å’Œæ–‡åŒ–ä¹ ä¿—çš„ä¿¡æ¯ã€‚ELLMé€šè¿‡æŸ¥è¯¢LLMæ¥è·å–å¯èƒ½çš„ç›®æ ‡ï¼Œå¹¶å¥–åŠ±ä»£ç†å®ç°è¿™äº›å»ºè®®ï¼Œä»è€Œå¼•å¯¼æ¢ç´¢æœç€å®Œæˆå¤šæ ·åŒ–ã€ä¸Šä¸‹æ–‡æ•æ„Ÿå’Œäººç±»æœ‰æ„ä¹‰çš„ç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨LLMç”Ÿæˆçš„ç›®æ ‡ä½œä¸ºå†…åœ¨å¥–åŠ±å‡½æ•°ã€‚ELLMé€šè¿‡æµ‹é‡LLMç”Ÿæˆçš„ç›®æ ‡ä¸ç¯å¢ƒä¸­ä»£ç†è½¬æ¢çš„æè¿°ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥è®¡ç®—å¥–åŠ±ã€‚å½“è½¬æ¢çš„æè¿°ä¸ç›®æ ‡æè¿°è¶³å¤Ÿæ¥è¿‘æ—¶ï¼Œä»£ç†å°†è·å¾—ä¸ç›¸ä¼¼åº¦æˆæ¯”ä¾‹çš„å¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Crafteræ¸¸æˆç¯å¢ƒå’ŒHousekeepæœºå™¨äººæ¨¡æ‹Ÿå™¨ä¸­è¯„ä¼°äº†ELLMã€‚ç»“æœè¡¨æ˜ï¼ŒELLMè®­ç»ƒçš„ä»£ç†åœ¨é¢„è®­ç»ƒæœŸé—´å¯¹å¸¸è¯†è¡Œä¸ºçš„è¦†ç›–èŒƒå›´æ›´å¥½ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½é€šå¸¸ä¸åŸºçº¿ç›¸å½“æˆ–æœ‰æ‰€æé«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç”¨äºå¼•å¯¼å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ç¼ºä¹å¤–éƒ¨å®šä¹‰çš„å¥–åŠ±çš„æƒ…å†µä¸‹å­¦ä¹ æœ‰ç”¨çš„è¡Œä¸ºã€‚é€šè¿‡åˆ©ç”¨LLMçš„èƒŒæ™¯çŸ¥è¯†ï¼ŒELLMå¯ä»¥å¼•å¯¼ä»£ç†æœç€äººç±»æœ‰æ„ä¹‰ä¸”å¯èƒ½æœ‰ç”¨çš„è¡Œä¸ºå‘å±•ï¼Œä»è€Œæé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†LLMæ€§èƒ½å¯¹æç¤ºé€‰æ‹©ã€çŠ¶æ€å’Œè½¬æ¢æè¿°çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›LLMæ€§èƒ½çš„æ½œåœ¨æ–¹æ³•ã€‚

## creative-agents--empowering-agents-with-imagination-for-creative-tasks
### Abstract
We study building embodied agents for open-ended creative tasks. While
existing methods build instruction-following agents that can perform diverse
open-ended tasks, none of them demonstrates creativity -- the ability to give
novel and diverse task solutions implicit in the language instructions. This
limitation comes from their inability to convert abstract language instructions
into concrete task goals in the environment and perform long-horizon planning
for such complicated goals. Given the observation that humans perform creative
tasks with the help of imagination, we propose a class of solutions for
creative agents, where the controller is enhanced with an imaginator that
generates detailed imaginations of task outcomes conditioned on language
instructions. We introduce several approaches to implementing the components of
creative agents. We implement the imaginator with either a large language model
for textual imagination or a diffusion model for visual imagination. The
controller can either be a behavior-cloning policy learned from data or a
pre-trained foundation model generating executable codes in the environment. We
benchmark creative tasks with the challenging open-world game Minecraft, where
the agents are asked to create diverse buildings given free-form language
instructions. In addition, we propose novel evaluation metrics for open-ended
creative tasks utilizing GPT-4V, which holds many advantages over existing
metrics. We perform a detailed experimental analysis of creative agents,
showing that creative agents are the first AI agents accomplishing diverse
building creation in the survival mode of Minecraft. Our benchmark and models
are open-source for future research on creative agents
(https://github.com/PKU-RL/Creative-Agents).
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ›æ„æ™ºèƒ½ä½“ï¼šèµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›ä»¥å®Œæˆåˆ›æ„ä»»åŠ¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„æ™ºèƒ½ä½“å¤§å¤šåªèƒ½æ‰§è¡Œé¢„å®šä¹‰çš„ä»»åŠ¡ï¼Œç¼ºä¹å¤„ç†å¼€æ”¾æ€§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftæ¸¸æˆä¸­ï¼Œç°æœ‰çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œç®€å•çš„æŒ‡ä»¤ï¼Œå¦‚â€œæ”¶é›†çŸ³å¤´â€æˆ–â€œå»ºé€ ä¸€ä¸ªé›ªäººâ€ï¼Œä½†æ— æ³•å®Œæˆæ›´å¤æ‚çš„åˆ›æ„ä»»åŠ¡ï¼Œå¦‚â€œå»ºé€ ä¸€ä¸ªç ‚å²©å®«æ®¿â€ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•å°†æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºå…·ä½“çš„ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œé•¿æœŸè§„åˆ’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œåˆ›æ„æ™ºèƒ½ä½“â€çš„æ¦‚å¿µï¼Œé€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›æ¥å¤„ç†å¼€æ”¾æ€§åˆ›æ„ä»»åŠ¡ã€‚åˆ›æ„æ™ºèƒ½ä½“ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šæƒ³è±¡å™¨å’Œæ§åˆ¶å™¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæƒ³è±¡å™¨
æƒ³è±¡å™¨è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»»åŠ¡ç»“æœçš„è¯¦ç»†æƒ³è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æƒ³è±¡å™¨çš„æ–¹æ³•ï¼š
- **æ–‡æœ¬æƒ³è±¡**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æŠ€æœ¯ç”Ÿæˆæ–‡æœ¬æƒ³è±¡ã€‚
- **è§†è§‰æƒ³è±¡**ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionï¼Œç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸ç¬¦çš„è§†è§‰æƒ³è±¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ§åˆ¶å™¨
æ§åˆ¶å™¨è´Ÿè´£å°†æƒ³è±¡è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æ§åˆ¶å™¨çš„æ–¹æ³•ï¼š
- **è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨**ï¼šä»ç¯å¢ƒä¸­å­¦ä¹ è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå°†å›¾åƒæƒ³è±¡è½¬æ¢ä¸ºå»ºç­‘è“å›¾ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚
- **åŸºäºGPT-4(V)çš„æ§åˆ¶å™¨**ï¼šåˆ©ç”¨GPT-4(V)çš„è§†è§‰è¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ¥å®Œæˆä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Minecraftæ¸¸æˆä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ›æ„æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æŒ‡ä»¤åˆ›å»ºå¤šæ ·åŒ–å’Œè§†è§‰ä¸Šå¸å¼•äººçš„å»ºç­‘ã€‚å…¶ä¸­ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œå¹¶ç»“åˆGPT-4(V)è¿›è¡Œæ§åˆ¶çš„æ™ºèƒ½ä½“è¡¨ç°æœ€ä½³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åˆ›æ„æ™ºèƒ½ä½“æ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ å’Œåˆ›æ„AIæ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•æé«˜è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨çš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•å¢å¼ºæ™ºèƒ½ä½“çš„åˆ›é€ åŠ›ã€‚

## octopus--embodied-vision-language-programmer-from-environmental-feedback
### Abstract
Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. When integrated into an embodied agent,
existing embodied VLM works either output detailed action sequences at the
manipulation level or only provide plans at an abstract level, leaving a gap
between high-level planning and real-world manipulation. To bridge this gap, we
introduce Octopus, an embodied vision-language programmer that uses executable
code generation as a medium to connect planning and manipulation. Octopus is
designed to 1) proficiently comprehend an agent's visual and textual task
objectives, 2) formulate intricate action sequences, and 3) generate executable
code. To facilitate Octopus model development, we introduce OctoVerse: a suite
of environments tailored for benchmarking vision-based code generators on a
wide spectrum of tasks, ranging from mundane daily chores in simulators to
sophisticated interactions in complex video games such as Grand Theft Auto
(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an
explorative agent that generates training data, i.e., action blueprints and
corresponding executable code. We also collect feedback that enables an
enhanced training scheme called Reinforcement Learning with Environmental
Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's
functionality and present compelling results, showing that the proposed RLEF
refines the agent's decision-making. By open-sourcing our simulation
environments, dataset, and model architecture, we aspire to ignite further
innovation and foster collaborative applications within the broader embodied AI
community.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Octopusï¼šåŸºäºç¯å¢ƒåé¦ˆçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°†å®ƒä»¬é›†æˆåˆ°å…·èº«æ™ºèƒ½ä½“ä¸­æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…·èº«VLMå·¥ä½œè¦ä¹ˆåœ¨æ“ä½œå±‚é¢è¾“å‡ºè¯¦ç»†çš„åŠ¨ä½œåºåˆ—ï¼Œè¦ä¹ˆä»…åœ¨æŠ½è±¡å±‚é¢æä¾›è®¡åˆ’ï¼Œå¯¼è‡´é«˜çº§è§„åˆ’å’Œç°å®ä¸–ç•Œæ“ä½œä¹‹é—´å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºOctopusçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹å™¨ï¼Œå®ƒä½¿ç”¨å¯æ‰§è¡Œä»£ç ç”Ÿæˆä½œä¸ºè¿æ¥è§„åˆ’å’Œæ“ä½œçš„åª’ä»‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOctopusèƒ½å¤Ÿç†Ÿç»ƒåœ°ç†è§£æ™ºèƒ½ä½“çš„è§†è§‰å’Œæ–‡æœ¬ä»»åŠ¡ç›®æ ‡ï¼Œåˆ¶å®šå¤æ‚çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸ºäº†ä¿ƒè¿›Octopusæ¨¡å‹çš„å‘å±•ï¼Œæœ¬æ–‡å¼•å…¥äº†OctoVerseï¼Œè¿™æ˜¯ä¸€å¥—ä¸ºåœ¨å„ç§ä»»åŠ¡ä¸Šè¯„ä¼°åŸºäºè§†è§‰çš„ä»£ç ç”Ÿæˆå™¨è€Œé‡èº«å®šåˆ¶çš„ç¯å¢ƒï¼ŒåŒ…æ‹¬ä»æ¨¡æ‹Ÿå™¨ä¸­çš„æ—¥å¸¸å®¶åŠ¡åˆ°å¤æ‚è§†é¢‘æ¸¸æˆï¼ˆå¦‚GTAå’ŒMinecraftï¼‰ä¸­çš„å¤æ‚äº¤äº’ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ºäº†è®­ç»ƒOctopusï¼Œæœ¬æ–‡åˆ©ç”¨GPT-4æ§åˆ¶ä¸€ä¸ªæ¢ç´¢æ€§æ™ºèƒ½ä½“ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå³åŠ¨ä½œè“å›¾å’Œç›¸åº”çš„å¯æ‰§è¡Œä»£ç ã€‚åŒæ—¶ï¼Œæ”¶é›†åé¦ˆï¼Œä»¥å®ç°ä¸€ç§ç§°ä¸ºç¯å¢ƒåé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLEFï¼‰çš„å¢å¼ºè®­ç»ƒæ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæœ¬æ–‡å±•ç¤ºäº†Octopusçš„åŠŸèƒ½ï¼Œå¹¶å±•ç¤ºäº†ä»¤äººä¿¡æœçš„ç»“æœï¼Œè¡¨æ˜æ‰€æå‡ºçš„RLEFç»†åŒ–äº†æ™ºèƒ½ä½“çš„å†³ç­–ã€‚Octopusåœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ï¼Œåœ¨ä»»åŠ¡è§„åˆ’ã€ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œæ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚RLEFçš„é›†æˆè¿›ä¸€æ­¥å¢å¼ºäº†Octopusçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è¿™ç§è®­ç»ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Octopusæ¨¡å‹å’ŒOctoVerseç¯å¢ƒä¸ºå…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚Octopusæ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹å¯ä»¥å€Ÿé‰´åˆ°å…¶ä»–å…·èº«æ™ºèƒ½ä½“ä¸­ï¼Œä»¥æé«˜å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ“ä½œèƒ½åŠ›ã€‚OctoVerseç¯å¢ƒå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹æ¨¡å‹ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚

## steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds
### Abstract
Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Steve-Eyeï¼šä¸ºåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“èµ‹äºˆå¼€æ”¾ä¸–ç•Œçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ‹äºˆå…·èº«æ™ºèƒ½ä½“ä¸ä¸–ç•Œäº’åŠ¨çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™æ ‡å¿—ç€é€šç”¨æœºå™¨äººæŠ€æœ¯è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€å¿½ç•¥äº†å¼€æ”¾ä¸–ç•Œçš„è§†è§‰ä¸°å¯Œæ€§ï¼Œå¯¼è‡´æ•´ä¸ªäº¤äº’è¿‡ç¨‹ç±»ä¼¼äºâ€œä¸€ä¸ªè’™ç€çœ¼ç›çš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆâ€ã€‚å› æ­¤ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨ç›´è§‚åœ°ç†è§£å‘¨å›´ç¯å¢ƒå’Œç”Ÿæˆæ˜“äºç†è§£çš„å“åº”æ–¹é¢ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Steve-Eyeï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨èµ‹äºˆåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œè§†è§‰æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚Steve-Eyeå°†LLMä¸è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰-æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠè‡ªåŠ¨ç­–ç•¥æ”¶é›†äº†ä¸€ä¸ªåŒ…å«850Kå¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤å¯¹çš„å¹¿æ³›æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ¶µç›–æ™ºèƒ½ä½“çš„ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªå¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ï¼Œç„¶åä»å¹¿æ³›çš„è§†è§’è¿›è¡Œå¤§é‡å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSteve-Eyeåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºäºLLMçš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹ä¸‰ä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼š
1. ç¯å¢ƒè§†è§‰æè¿°ï¼ˆENV-VCï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æ„ŸçŸ¥å’Œæè¿°å…¶å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚
2. åŸºç¡€çŸ¥è¯†é—®ç­”ï¼ˆFK-QAï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æŒæ¡å¯¹å†³ç­–è‡³å…³é‡è¦çš„åŸºæœ¬çŸ¥è¯†çš„ç†Ÿç»ƒç¨‹åº¦ã€‚
3. æŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ï¼ˆSPPï¼‰ï¼šé‡åŒ–æ™ºèƒ½ä½“åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Steve-Eyeçš„ç ”ç©¶æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æœ‰æ•ˆäº’åŠ¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚å…¶å¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’åŠŸèƒ½ä¸ºæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»è¡ŒåŠ¨å’Œè§„åˆ’æä¾›äº†å¼ºå¤§çš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒSteve-Eyeçš„å¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ä¸ºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## scienceworld--is-your-agent-smarter-than-a-5th-grader-
### Abstract
We present ScienceWorld, a benchmark to test agents' scientific reasoning
abilities in a new interactive text environment at the level of a standard
elementary school science curriculum. Despite the transformer-based progress
seen in question-answering and scientific text processing, we find that current
models cannot reason about or explain learned science concepts in novel
contexts. For instance, models can easily answer what the conductivity of a
known material is but struggle when asked how they would conduct an experiment
in a grounded environment to find the conductivity of an unknown material. This
begs the question of whether current models are simply retrieving answers by
way of seeing a large number of similar examples or if they have learned to
reason about concepts in a reusable manner. We hypothesize that agents need to
be grounded in interactive environments to achieve such reasoning capabilities.
Our experiments provide empirical evidence supporting this hypothesis --
showing that a 1.5 million parameter agent trained interactively for 100k steps
outperforms a 11 billion parameter model statically trained for scientific
question-answering and reasoning from millions of expert demonstrations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ScienceWorldï¼šä½ çš„æ™ºèƒ½ä½“æ¯”äº”å¹´çº§å­¦ç”Ÿæ›´èªæ˜å—ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é—®ç­”å’Œç§‘å­¦æ–‡æœ¬å¤„ç†æ–¹é¢çš„è¿›æ­¥ï¼Œç ”ç©¶äººå‘˜å¼€å§‹è´¨ç–‘è¿™äº›æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£äº†å®ƒä»¬æ‰€å›ç­”çš„é—®é¢˜ï¼Œæˆ–è€…å®ƒä»¬æ˜¯å¦åªæ˜¯é€šè¿‡å¤§é‡ç›¸ä¼¼ç¤ºä¾‹çš„æ£€ç´¢æ¥è·å–ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ScienceWorldï¼Œä¸€ä¸ªç”¨äºæµ‹è¯•æ™ºèƒ½ä½“åœ¨æ ‡å‡†å°å­¦ç§‘å­¦è¯¾ç¨‹æ°´å¹³ä¸Šçš„ç§‘å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºScienceWorld
ScienceWorldæ˜¯ä¸€ä¸ªå¤æ‚çš„äº¤äº’å¼æ–‡æœ¬ç¯å¢ƒï¼Œå…·æœ‰æ¨¡æ‹Ÿçƒ­åŠ›å­¦ã€ç”µè·¯ã€åŒ–å­¦ååº”å’Œç”Ÿç‰©è¿‡ç¨‹çš„å¼•æ“ã€‚å®ƒåŒ…å«10ä¸ªç›¸äº’è¿æ¥çš„ä½ç½®ï¼Œä»¥åŠå¤šè¾¾200ç§ç±»å‹çš„å¯¹è±¡ï¼ŒåŒ…æ‹¬è®¾å¤‡ã€ä»ªå™¨ã€åŠ¨æ¤ç‰©ã€ç”µæ°”å…ƒä»¶ã€ç‰©è´¨ã€å®¹å™¨å’Œå®¶å…·ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®æ–½30ä¸ªåŸºå‡†ä»»åŠ¡
è¿™äº›ä»»åŠ¡æ¶µç›–äº†10ä¸ªä¸»é¢˜ï¼ŒåŒ…æ‹¬ç‰©è´¨çŠ¶æ€çš„å˜åŒ–ã€æ¸©åº¦æµ‹é‡ã€ç”µè·¯ã€æ‘©æ“¦ã€ç‰©ä½“åˆ†ç±»ã€åŒ–å­¦æ··åˆç‰©ã€æ¤ç‰©å’Œä¼ ç²‰è€…ã€å¯¿å‘½ã€ç”Ÿå‘½å‘¨æœŸå’Œå­Ÿå¾·å°”é—ä¼ å­¦ã€‚æ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«10åˆ°1400ä¸ªå‚æ•°å˜åŒ–ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶é¼“åŠ±æ³›åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¯„ä¼°ç°æœ‰æ™ºèƒ½ä½“
æœ¬æ–‡è¯„ä¼°äº†5ä¸ªæœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬DRRNã€KG-A2Cã€CALMã€BCå’ŒTDTã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ™ºèƒ½ä½“åœ¨éœ€è¦ä½¿ç”¨ç§‘å­¦é¢†åŸŸçŸ¥è¯†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹³å‡å¾—åˆ†ä»…ä¸º0.17ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒScienceWorldæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œå³ä½¿æ˜¯ç°æœ‰çš„æœ€å…ˆè¿›æ™ºèƒ½ä½“ä¹Ÿæ— æ³•å¾ˆå¥½åœ°å®Œæˆè¿™äº›ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œåœ¨äº¤äº’å¼ç¯å¢ƒä¸­è¿›è¡Œäº¤äº’å¼è®­ç»ƒçš„æ™ºèƒ½ä½“æ¯”åœ¨é™æ€æ–‡æœ¬æºä¸Šè¿›è¡Œç¦»çº¿è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ›´æœ‰æ•ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ScienceWorldåŸºå‡†ä¸ºè¯„ä¼°æ™ºèƒ½ä½“çš„ç§‘å­¦æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„å¹³å°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼è®­ç»ƒå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°å­¦ä¹ ç§‘å­¦çŸ¥è¯†å’Œå¸¸è¯†ï¼Œå¹¶å°†å…¶åº”ç”¨äºå®é™…ä»»åŠ¡ä¸­ã€‚

## alfworld--aligning-text-and-embodied-environments-for-interactive-learning
### Abstract
Given a simple request like Put a washed apple in the kitchen fridge, humans
can reason in purely abstract terms by imagining action sequences and scoring
their likelihood of success, prototypicality, and efficiency, all without
moving a muscle. Once we see the kitchen in question, we can update our
abstract plans to fit the scene. Embodied agents require the same abilities,
but existing work does not yet provide the infrastructure necessary for both
reasoning abstractly and executing concretely. We address this limitation by
introducing ALFWorld, a simulator that enables agents to learn abstract, text
based policies in TextWorld (C\^ot\'e et al., 2018) and then execute goals from
the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.
ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,
learned in TextWorld, corresponds directly to concrete, visually grounded
actions. In turn, as we demonstrate empirically, this fosters better agent
generalization than training only in the visually grounded environment.
BUTLER's simple, modular design factors the problem to allow researchers to
focus on models for improving every piece of the pipeline (language
understanding, planning, navigation, and visual scene understanding).
### ğŸŒŸ è®ºæ–‡è§£è¯» | ALFWorldï¼šæ–‡æœ¬ä¸å…·èº«ç¯å¢ƒçš„äº¤äº’å¼å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œäººç±»èƒ½å¤Ÿé€šè¿‡æŠ½è±¡æ€ç»´æ¥è§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡ï¼Œä¾‹å¦‚æƒ³è±¡ä¸€ç³»åˆ—åŠ¨ä½œå¹¶è¯„ä¼°å…¶æˆåŠŸå¯èƒ½æ€§ã€å…¸å‹æ€§å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…·èº«æ™ºèƒ½ä½“ç¼ºä¹è¿™ç§æŠ½è±¡æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•åœ¨æ²¡æœ‰å®é™…æ‰§è¡Œçš„æƒ…å†µä¸‹è¿›è¡Œè§„åˆ’ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œé€šè¿‡å¼•å…¥ALFWorldæ¨¡æ‹Ÿå™¨ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨TextWorldä¸­å­¦ä¹ æŠ½è±¡çš„æ–‡æœ¬ç­–ç•¥ï¼Œå¹¶åœ¨ALFREDåŸºå‡†æµ‹è¯•ä¸­æ‰§è¡Œå…·ä½“çš„ç›®æ ‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šALFWorldç¯å¢ƒ
ALFWorldæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ï¼Œå®ƒå°†TextWorldå’ŒALFREDç¯å¢ƒç›¸ç»“åˆï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ–‡æœ¬ç¯å¢ƒä¸­å­¦ä¹ æŠ½è±¡ç­–ç•¥ï¼Œå¹¶åœ¨è§†è§‰ç¯å¢ƒä¸­æ‰§è¡Œå…·ä½“åŠ¨ä½œã€‚TextWorldæä¾›æ–‡æœ¬è§‚å¯Ÿå’Œå“åº”é«˜çº§æ–‡æœ¬åŠ¨ä½œï¼Œè€ŒALFREDåˆ™æ¸²æŸ“é«˜ç»´å›¾åƒå¹¶å“åº”ä½çº§ç‰©ç†åŠ¨ä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šBUTLERæ¶æ„
BUTLERæ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œå®ƒé¦–å…ˆåœ¨TextWorldä¸­å­¦ä¹ é«˜çº§è¯­è¨€ç­–ç•¥ï¼Œç„¶åå°†è¿™äº›ç­–ç•¥è½¬ç§»åˆ°ALFREDä¸­çš„å…·èº«ä»»åŠ¡ã€‚BUTLERç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼šBUTLER::BRAINï¼ˆæ–‡æœ¬æ™ºèƒ½ä½“ï¼‰ã€BUTLER::VISIONï¼ˆè¯­è¨€çŠ¶æ€ä¼°è®¡å™¨ï¼‰å’ŒBUTLER::BODYï¼ˆä½çº§æ§åˆ¶å™¨ï¼‰ã€‚BUTLER::BRAINç”Ÿæˆæ–‡æœ¬åŠ¨ä½œï¼ŒBUTLER::VISIONå°†è¿™äº›åŠ¨ä½œè½¬æ¢ä¸ºè§†è§‰ç¯å¢ƒä¸­çš„ä½çº§åŠ¨ä½œï¼Œè€ŒBUTLER::BODYåˆ™æ‰§è¡Œè¿™äº›åŠ¨ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨TextWorldä¸­é¢„è®­ç»ƒçš„BUTLERæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹æ³›åŒ–åˆ°ALFREDä¸­çš„å…·èº«ä»»åŠ¡ï¼Œå¹¶ä¸”æ¯”ä»…åœ¨è§†è§‰ç¯å¢ƒä¸­ä»å¤´å¼€å§‹è®­ç»ƒçš„æ™ºèƒ½ä½“å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒTextWorldè®­ç»ƒæ¯”å…·èº«è®­ç»ƒæ›´å¿«ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æ‰§è¡Œå¤±è´¥å’Œä¸“å®¶å¤±è´¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ALFWorldæ¡†æ¶å’ŒBUTLERæ¶æ„ä¸ºå…·èº«æ™ºèƒ½ä½“çš„å­¦ä¹ å’Œæ³›åŒ–æä¾›äº†æ–°çš„æ€è·¯ã€‚ALFWorldç¯å¢ƒå¯ä»¥ç”¨äºè®­ç»ƒæ™ºèƒ½ä½“åœ¨æ–‡æœ¬ç¯å¢ƒä¸­è¿›è¡ŒæŠ½è±¡æ¨ç†ï¼Œè€ŒBUTLERæ¶æ„åˆ™å¯ä»¥ç”¨äºå°†æ–‡æœ¬ç­–ç•¥è½¬ç§»åˆ°å…·èº«ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬ç¯å¢ƒä¸­é¢„è®­ç»ƒæ™ºèƒ½ä½“å¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨å…·èº«ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

## welfare-diplomacy--benchmarking-language-model-cooperation
### Abstract
The growing capabilities and increasingly widespread deployment of AI systems
necessitate robust benchmarks for measuring their cooperative capabilities.
Unfortunately, most multi-agent benchmarks are either zero-sum or purely
cooperative, providing limited opportunities for such measurements. We
introduce a general-sum variant of the zero-sum board game Diplomacy -- called
Welfare Diplomacy -- in which players must balance investing in military
conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both
a clearer assessment of and stronger training incentives for cooperative
capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules
and implementing them via an open-source Diplomacy engine; (2) constructing
baseline agents using zero-shot prompted language models; and (3) conducting
experiments where we find that baselines using state-of-the-art models attain
high social welfare but are exploitable. Our work aims to promote societal
safety by aiding researchers in developing and assessing multi-agent AI
systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is
available at https://github.com/mukobi/welfare-diplomacy.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢AIåˆä½œèƒ½åŠ›ï¼šWelfare DiplomacyåŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€AIç³»ç»Ÿèƒ½åŠ›çš„ä¸æ–­å¢å¼ºå’Œåº”ç”¨çš„æ—¥ç›Šå¹¿æ³›ï¼Œè¡¡é‡å…¶åˆä½œèƒ½åŠ›çš„éœ€æ±‚ä¹Ÿæ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•è¦ä¹ˆæ˜¯é›¶å’Œåšå¼ˆï¼Œè¦ä¹ˆæ˜¯çº¯ç²¹çš„åˆä½œåšå¼ˆï¼Œè¿™é™åˆ¶äº†å¯¹å…¶åˆä½œèƒ½åŠ›çš„è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºWelfare Diplomacyçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWelfare Diplomacyè§„åˆ™
Welfare Diplomacyæ˜¯å¯¹ç»å…¸å¤–äº¤æ¸¸æˆï¼ˆDiplomacyï¼‰çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç©å®¶éœ€è¦åœ¨å†›äº‹å¾æœå’Œå›½å†…ç¦åˆ©ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ¸¸æˆç»“æŸåï¼Œç©å®¶çš„æ€»æ•ˆç”¨ç­‰äºå…¶ç´¯ç§¯çš„ç¦åˆ©ç‚¹æ•°ï¼Œè€Œä¸æ˜¯å é¢†çš„ä¾›åº”ä¸­å¿ƒæ•°é‡ã€‚è¿™ç§è§„åˆ™è®¾è®¡é¼“åŠ±ç©å®¶è¿›è¡Œåˆä½œï¼Œä»¥å®ç°æ›´é«˜çš„ç¤¾ä¼šç¦ç¥‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé›¶æ ·æœ¬è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•
æœ¬æ–‡ä½¿ç”¨é›¶æ ·æœ¬æç¤ºè¯­è¨€æ¨¡å‹æ„å»ºäº†Welfare Diplomacyçš„åŸºçº¿æ™ºèƒ½ä½“ï¼Œå¹¶ä½¿ç”¨GPT-4ç­‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜ç¤¾ä¼šç¦ç¥‰ï¼Œä½†å®¹æ˜“è¢«å…¶ä»–ç©å®¶åˆ©ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWelfare Diplomacyèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°å’Œè®­ç»ƒAIç³»ç»Ÿçš„åˆä½œèƒ½åŠ›ã€‚ä¸æ ‡å‡†å¤–äº¤æ¸¸æˆç›¸æ¯”ï¼ŒWelfare Diplomacyä¸­çš„ç©å®¶å‚ä¸å†²çªçš„é¢‘ç‡æ›´ä½ï¼Œè¿™è¡¨æ˜Welfare Diplomacyèƒ½å¤Ÿæ›´å¥½åœ°ä¿ƒè¿›åˆä½œè¡Œä¸ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Welfare DiplomacyåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œè®­ç»ƒAIç³»ç»Ÿçš„åˆä½œèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„å¹³å°ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥ç”¨äºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„AIç³»ç»Ÿï¼Œä»¥åº”å¯¹ç°å®ä¸–ç•Œä¸­çš„å¤æ‚æŒ‘æˆ˜ã€‚

### ğŸŒŸ æ€»ç»“
Welfare Diplomacyæ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè¯„ä¼°AIç³»ç»Ÿçš„åˆä½œèƒ½åŠ›ã€‚éšç€AIæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒWelfare Diplomacyæœ‰æœ›åœ¨ä¿ƒè¿›AIåˆä½œèƒ½åŠ›æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation
### Abstract
Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½œä¸ºæ™ºèƒ½ä½“ï¼ˆLLM-as-Agentï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶é€šå¸¸å‡è®¾LLMså¤„ç†çš„ä¿¡æ¯å§‹ç»ˆæ˜¯è¯šå®çš„ï¼Œå¿½ç•¥äº†äººç±»ç¤¾ä¼šä¸­æ™®éå­˜åœ¨çš„æ¬ºéª—æ€§æˆ–è¯¯å¯¼æ€§ä¿¡æ¯ä»¥åŠAIç”Ÿæˆå†…å®¹ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚è¿™ç§å‡è®¾ä½¿å¾—LLMså®¹æ˜“å—åˆ°æ¶æ„æ“çºµï¼Œå¯èƒ½å¯¼è‡´ä¸è‰¯åæœã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsåœ¨æ¬ºéª—æ€§ç¯å¢ƒä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œé€’å½’æ²‰æ€â€ï¼ˆReConï¼‰çš„æ–°æ¡†æ¶ï¼Œä»¥å¢å¼ºLLMsè¯†åˆ«å’Œå¯¹æŠ—æ¬ºéª—æ€§ä¿¡æ¯çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€’å½’æ²‰æ€æ¡†æ¶ï¼ˆReConï¼‰
ReConæ¡†æ¶ç»“åˆäº†â€œæ„æ€æ²‰æ€â€å’Œâ€œç²¾ç‚¼æ²‰æ€â€ä¸¤ä¸ªè®¤çŸ¥è¿‡ç¨‹ã€‚æ„æ€æ²‰æ€äº§ç”Ÿåˆå§‹æ€è€ƒå’Œè¨€è¯­ï¼Œè€Œç²¾ç‚¼æ²‰æ€åˆ™è¿›ä¸€æ­¥æ”¹è¿›å®ƒä»¬ã€‚æ­¤å¤–ï¼ŒReConè¿˜å¼•å…¥äº†ç¬¬ä¸€é˜¶å’Œç¬¬äºŒé˜¶è§†è§’è½¬æ¢ï¼Œåˆ†åˆ«å¯¹åº”äºè¿™ä¸¤ä¸ªè¿‡ç¨‹ã€‚ç¬¬ä¸€é˜¶è§†è§’è½¬æ¢å…è®¸LLMæ™ºèƒ½ä½“ä»è‡ªå·±çš„è§’åº¦æ¨æ–­ä»–äººçš„å¿ƒç†çŠ¶æ€ï¼Œè€Œç¬¬äºŒé˜¶è§†è§’è½¬æ¢åˆ™æ¶‰åŠç†è§£ä»–äººå¦‚ä½•çœ‹å¾…æ™ºèƒ½ä½“çš„å¿ƒç†çŠ¶æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨Avalonæ¸¸æˆä¸­æµ‹è¯•ReCon
æœ¬æ–‡ä½¿ç”¨å¤æ‚çš„Avalonæ¸¸æˆä½œä¸ºæµ‹è¯•å¹³å°ï¼Œè¯¥æ¸¸æˆå……æ»¡è¯¯å¯¼ä¿¡æ¯ï¼Œéœ€è¦å¤æ‚çš„é€»è¾‘æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReConèƒ½å¤Ÿæœ‰æ•ˆåœ°å¸®åŠ©LLMsè¯†åˆ«å’Œåº”å¯¹æ¬ºéª—æ€§ä¿¡æ¯ï¼Œè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒå’Œæ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒReConåœ¨Avalonæ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¸®åŠ©LLMsè¯†åˆ«å’Œåº”å¯¹æ¬ºéª—æ€§ä¿¡æ¯ï¼Œè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒå’Œæ•°æ®ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒReConåœ¨å¤šä¸ªç»´åº¦ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼ŒåŒ…æ‹¬éšè”½æ€§ã€é€»è¾‘æ€§ã€è´¡çŒ®åº¦ã€è¯´æœåŠ›ã€ä¿¡æ¯é‡å’Œåˆ›é€ åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ReConæ¡†æ¶ä¸ºLLMsåœ¨æ¬ºéª—æ€§ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚ReConæ¡†æ¶çš„è®¾è®¡å’Œå®ç°æ–¹æ³•å¯ä»¥å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦è¯†åˆ«å’Œå¯¹æŠ—æ¬ºéª—æ€§ä¿¡æ¯çš„åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†LLMsåœ¨å®‰å…¨æ€§ã€æ¨ç†èƒ½åŠ›ã€è¨€è¯­é£æ ¼å’Œæ ¼å¼ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model
### Abstract
Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PokerGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è½»é‡çº§å¤šç©å®¶å¾·å·æ‰‘å…‹è§£å†³æ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¾·å·æ‰‘å…‹ä½œä¸ºä¸€ç§å…¸å‹çš„éå®Œç¾ä¿¡æ¯æ¸¸æˆï¼ˆIIGï¼‰ï¼Œä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚DeepStackå’ŒLibratusï¼Œä¸»è¦ä¾èµ–äºåäº‹å®åæ‚”æœ€å°åŒ–ï¼ˆCFRï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è®¡ç®—æˆæœ¬å’Œæ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚CFRç®—æ³•çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åº”ç”¨äºå¤šç©å®¶æ¸¸æˆï¼Œä¸”éš¾ä»¥ä»ç°æœ‰æ¨¡å‹ä¸­å­¦ä¹ å¹¶åº”ç”¨äºå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†PokerGPTï¼Œä¸€ç§åŸºäºè½»é‡çº§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯å¾·å·æ‰‘å…‹è§£å†³æ–¹æ¡ˆã€‚PokerGPTé€šè¿‡ä»¥ä¸‹åˆ›æ–°ç‚¹å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶
PokerGPTé‡‡ç”¨ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ï¼Œé¿å…äº†å¤æ‚çš„ç‰¹å¾å·¥ç¨‹å’Œä¸­é—´æ­¥éª¤ã€‚å®ƒä»…éœ€è¦ç®€å•çš„æ–‡æœ¬ä¿¡æ¯å³å¯ç”Ÿæˆå†³ç­–å»ºè®®ï¼Œå®ç°äº†äººæœºäº¤äº’çš„ä¾¿æ·æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè½»é‡çº§LLM
PokerGPTåŸºäºè½»é‡çº§LLMï¼Œå…·æœ‰æ›´å°‘çš„å‚æ•°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´ä¹Ÿæ›´çŸ­ï¼Œå®ç°äº†èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé«˜æ•ˆçš„æ•°æ®å¤„ç†
PokerGPTé‡‡ç”¨æ•°æ®æ¸…æ´—å’Œæç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå°†çœŸå®æ¸¸æˆæ•°æ®è½¬æ¢ä¸ºå¯ç†è§£çš„æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒPokerGPTåœ¨èƒœç‡ã€æ¨¡å‹å¤§å°ã€è®­ç»ƒæ—¶é—´å’Œå“åº”é€Ÿåº¦ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPokerGPTèƒ½å¤Ÿå¤„ç†ä»»æ„æ•°é‡çš„ç©å®¶ï¼Œå¹¶å±•ç°å‡ºå‡ºè‰²çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PokerGPTçš„æˆåŠŸè¡¨æ˜ï¼ŒLLMåœ¨è§£å†³IIGæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å…¶ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ã€è½»é‡çº§æ¨¡å‹å’Œé«˜æ•ˆçš„æ•°æ®å¤„ç†æŠ€æœ¯ä¸ºå…¶ä»–IIGç ”ç©¶æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒã€‚æ­¤å¤–ï¼ŒPokerGPTçš„äº¤äº’å¼ç‰¹æ€§ä½¿å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚

## suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4
### Abstract
Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€èƒ½åŠ›ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå†³ç­–å¾€å¾€æ˜¯åœ¨ä¿¡æ¯ä¸å®Œæ•´æˆ–ä¸ç¡®å®šçš„æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„AIç®—æ³•éƒ½æ˜¯åœ¨å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è®­ç»ƒçš„ï¼Œå³æ‰€æœ‰ç©å®¶éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ¥å¤„ç†ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å†³ç­–è¿‡ç¨‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSuspicion-Agentçš„åˆ›æ–°å‹è‡ªä¸»ä»£ç†ï¼Œå®ƒåŸºäºGPT-4ï¼Œå¹¶åˆ©ç”¨å…¶å¼ºå¤§çš„çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ¥ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆã€‚Suspicion-Agentçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€ï¼ˆToMï¼‰èƒ½åŠ›ï¼Œå³ç†è§£ä»–äººå¹¶æœ‰æ„å½±å“ä»–äººè¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™ä½¿å¾—Suspicion-Agentèƒ½å¤Ÿé¢„æµ‹å¯¹æ‰‹çš„è¡Œä¸ºï¼Œå¹¶æ ¹æ®å¯¹æ‰‹çš„è¡Œä¸ºè°ƒæ•´è‡ªå·±çš„ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°†æ¸¸æˆè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå­æ¨¡å—ï¼Œå¦‚è§‚å¯Ÿè§£é‡Šå™¨ã€æ¸¸æˆæ¨¡å¼åˆ†æå’Œè§„åˆ’æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½ä½¿ç”¨ä¸åŒçš„æç¤ºæ¥å¼•å¯¼GPT-4æ‰§è¡Œç‰¹å®šçš„åŠŸèƒ½ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒä¸­ï¼ŒSuspicion-Agentåœ¨ä¸‰ä¸ªä¸åŒçš„ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­å±•ç¤ºäº†å…¶èƒ½åŠ›ï¼Œå¹¶åœ¨Leduc Hold'emæ¸¸æˆä¸­è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSuspicion-Agentå¯ä»¥æ½œåœ¨åœ°è¶…è¶Šä¼ ç»Ÿç®—æ³•ï¼Œè€Œæ— éœ€ä»»ä½•ä¸“é—¨çš„è®­ç»ƒæˆ–ç¤ºä¾‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Suspicion-Agentæ¡†æ¶ä¸ºåˆ©ç”¨LLMåœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è¿›è¡Œå†³ç­–æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸ToMèƒ½åŠ›ç›¸ç»“åˆï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å…¬å¼€äº†æ‰€æœ‰ä¸æ¸¸æˆç›¸å…³çš„æ•°æ®ï¼Œè¿™å°†æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMçš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹ã€‚

## chessgpt--bridging-policy-learning-and-language-modeling
### Abstract
When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
https://github.com/waterhorse1/ChessGPT.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ChessGPTï¼šç­–ç•¥å­¦ä¹ ä¸è¯­è¨€æ¨¡å‹èåˆçš„æ¡¥æ¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§£å†³å†³ç­–ä»»åŠ¡æ—¶ï¼Œäººç±»é€šå¸¸ä¾èµ–äºä¸¤ç§å…³é”®ä¿¡æ¯æ¥æºï¼šå†å²ç­–ç•¥æ•°æ®å’Œè‡ªç„¶è¯­è¨€å½¢å¼çš„ç­–ç•¥åˆ†æã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¤§å¤šåªå…³æ³¨å…¶ä¸­ä¸€ç§æ¥æºï¼Œè¦ä¹ˆæ˜¯ç›´æ¥ä»å†å²å›æ”¾ä¸­å­¦ä¹ ç­–ç•¥æˆ–ä»·å€¼å‡½æ•°ï¼Œè¦ä¹ˆæ˜¯åˆ©ç”¨è¯­è¨€è¯­æ–™åº“è¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡è®¤ä¸ºï¼Œä¸€ä¸ªå¼ºå¤§çš„è‡ªä¸»ä»£ç†åº”è¯¥åŒæ—¶åˆ©ç”¨è¿™ä¸¤ç§æ¥æºï¼Œå› æ­¤æå‡ºäº†ChessGPTï¼Œä¸€ä¸ªé€šè¿‡æ•´åˆå›½é™…è±¡æ£‹æ¸¸æˆä¸­çš„æ•°æ®æ¥è¿æ¥ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹çš„GPTæ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡æ¸¸æˆå’Œè¯­è¨€æ•°æ®é›†
æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤§é‡å›½é™…è±¡æ£‹æ¸¸æˆæ•°æ®å’Œè‡ªç„¶è¯­è¨€æ•°æ®çš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬åœ¨çº¿æ¸¸æˆå›æ”¾ã€ä¸“ä¸šæ£‹æ‰‹æ¯”èµ›ã€è®¡ç®—æœºå¼•æ“æ¸¸æˆã€æ£‹ç›˜æ¸¸æˆã€æ£‹ç›˜æ¸¸æˆåˆ†æã€æ£‹ç›˜æ¸¸æˆåšå®¢ã€æ£‹ç›˜æ¸¸æˆä¹¦ç±ã€æ£‹ç›˜æ¸¸æˆè®ºå›ã€æ£‹ç›˜æ¸¸æˆè§†é¢‘ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºChessCLIPå’ŒChessGPTæ¨¡å‹
æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ¨¡å‹ï¼ŒChessCLIPå’ŒChessGPTï¼Œåˆ©ç”¨ä¸Šè¿°æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ChessCLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†ç­–ç•¥å’Œè¯­è¨€æ¨¡æ€è¿æ¥èµ·æ¥ï¼Œè€ŒChessGPTåˆ™é€šè¿‡å› æœè¯­è¨€æ¨¡å‹è¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºå…¨é¢çš„è¯„ä¼°æ¡†æ¶
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å›½é™…è±¡æ£‹æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å»ºæ¨¡èƒ½åŠ›ã€ä»·å€¼åˆ¤æ–­èƒ½åŠ›å’Œç­–ç•¥èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒChessGPTæ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­éƒ½ä¼˜äºå…¶ä»–LLMåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†æ¨¡å‹å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ChessGPTæ¨¡å‹å’Œæ•°æ®é›†ä¸ºç ”ç©¶ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„è‡ªä¸»ä»£ç†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents
### Abstract
The generalization of decision-making agents encompasses two fundamental
elements: learning from past experiences and reasoning in novel contexts.
However, the predominant emphasis in most interactive environments is on
learning, often at the expense of complexity in reasoning. In this paper, we
introduce CivRealm, an environment inspired by the Civilization game.
Civilization's profound alignment with human history and society necessitates
sophisticated learning, while its ever-changing situations demand strong
reasoning to generalize. Particularly, CivRealm sets up an
imperfect-information general-sum game with a changing number of players; it
presents a plethora of complex features, challenging the agent to deal with
open-ended stochastic environments that require diplomacy and negotiation
skills. Within CivRealm, we provide interfaces for two typical agent types:
tensor-based agents that focus on learning, and language-based agents that
emphasize reasoning. To catalyze further research, we present initial results
for both paradigms. The canonical RL-based agents exhibit reasonable
performance in mini-games, whereas both RL- and LLM-based agents struggle to
make substantial progress in the full game. Overall, CivRealm stands as a
unique learning and reasoning challenge for decision-making agents. The code is
available at https://github.com/bigai-ai/civrealm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CivRealmï¼šå†³ç­–æ™ºèƒ½ä½“çš„å­¦ä¹ ä¸æ¨ç†ä¹‹æ—…

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„å†³ç­–æ™ºèƒ½ä½“ç¯å¢ƒå¾€å¾€è¿‡äºå¼ºè°ƒå­¦ä¹ ï¼Œè€Œå¿½è§†äº†æ¨ç†çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åŒæ—¶å…·å¤‡å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›ï¼Œæ‰èƒ½æ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CivRealmï¼Œä¸€ä¸ªåŸºäºæ–‡æ˜æ¸¸æˆçš„äº¤äº’å¼ç¯å¢ƒï¼Œæ—¨åœ¨æ¨åŠ¨å†³ç­–æ™ºèƒ½ä½“å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›çš„è¾¹ç•Œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCivRealmç¯å¢ƒ
CivRealmæ˜¯ä¸€ä¸ªåŸºäºæ–‡æ˜æ¸¸æˆçš„å¼€æ”¾ç¯å¢ƒï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
* **ä¸å®Œå…¨ä¿¡æ¯**ï¼šç©å®¶åªèƒ½è·å–è‡ªå·±å•ä½å‘ç°çš„ä¿¡æ¯ï¼Œéœ€è¦æ¨ç†å…¶ä»–ç©å®¶çš„æ„å›¾ã€‚
* **éšæœºæ€§**ï¼šæ¸¸æˆä¸­æœ‰éšæœºäº‹ä»¶å’Œå±æœºï¼Œéœ€è¦æ™ºèƒ½ä½“çµæ´»åº”å¯¹ã€‚
* **å¤šç›®æ ‡**ï¼šæœ‰å¤šç§èƒœåˆ©è·¯å¾„ï¼Œéœ€è¦å¹³è¡¡ç»æµã€å†›äº‹ã€å¤–äº¤ã€æ–‡åŒ–å’Œç§‘æŠ€å‘å±•ã€‚
* **åŠ¨æ€ç©ºé—´**ï¼šæ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¼šåŠ¨æ€å˜åŒ–ã€‚
* **å¤šæ™ºèƒ½ä½“**ï¼šå¤šä¸ªç©å®¶å¯ä»¥äº’åŠ¨ï¼Œéœ€è¦æ™ºèƒ½ä½“è¿›è¡Œåˆä½œå’Œç«äº‰ã€‚
* **åŠ¨æ€ç©å®¶æ•°é‡**ï¼šæ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶æ•°é‡ä¼šå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦æ™ºèƒ½ä½“é€‚åº”æ–°çš„ç¯å¢ƒã€‚
* **é€šä¿¡**ï¼šç©å®¶å¯ä»¥é€šè¿‡å¤–äº¤è¡ŒåŠ¨å’Œè‡ªç„¶è¯­è¨€èŠå¤©è¿›è¡Œäº¤æµã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šç§æ™ºèƒ½ä½“ç±»å‹
CivRealmæä¾›äº†ä¸¤ç§APIæ¥å£ï¼Œåˆ†åˆ«æ”¯æŒåŸºäºå¼ é‡çš„æ™ºèƒ½ä½“å’ŒåŸºäºè¯­è¨€çš„æ™ºèƒ½ä½“ï¼š
* **åŸºäºå¼ é‡çš„æ™ºèƒ½ä½“**ï¼šä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œæ“…é•¿å­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«ã€‚
* **åŸºäºè¯­è¨€çš„æ™ºèƒ½ä½“**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç­‰æ–¹æ³•ï¼Œæ“…é•¿æ¨ç†å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºå‡†æ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡
æœ¬æ–‡æå‡ºäº†ä¸‰ç§åŸºå‡†æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š
* **åŸºäºå¼ é‡çš„å¼ºåŒ–å­¦ä¹ **ï¼šä½¿ç”¨AlphaStarçš„æ¶æ„ï¼Œæ“…é•¿å¤„ç†å¤æ‚åŠ¨æ€å’Œæµ·é‡ä¿¡æ¯ã€‚
* **BaseLang**ï¼šåŸºäºAutoGPTçš„æ¶æ„ï¼Œæ“…é•¿æ¨ç†å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚
* **Mastaba**ï¼šåŸºäºBaseLangçš„æ¶æ„ï¼Œå¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œæé«˜å…¨å±€è§†è§’å’Œå†³ç­–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ é‡çš„å¼ºåŒ–å­¦ä¹ åœ¨è¿·ä½ æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®Œæ•´æ¸¸æˆä¸­ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚çŸ­è§†ç­–ç•¥å’Œéš¾ä»¥å¤„ç†ç¨€ç–å¥–åŠ±ã€‚åŸºäºè¯­è¨€çš„æ™ºèƒ½ä½“åœ¨å®Œæ•´æ¸¸æˆä¸­è¡¨ç°æ›´ä½³ï¼Œä½†ä»ç„¶éœ€è¦æ”¹è¿›æ¨ç†èƒ½åŠ›å’Œå…¨å±€è§†è§’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
CivRealmä¸ºå†³ç­–æ™ºèƒ½ä½“çš„å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‘æˆ˜å¹³å°ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCivRealmè¿˜å¯ä»¥ç”¨äºç ”ç©¶äººç±»ç¤¾ä¼šçš„åŠ¨æ€ã€å†å²äº‹ä»¶çš„ç»“æœå’Œæœªæ¥çš„ç¤¾ä¼šè½¨è¿¹ã€‚

## a-reinforcement-learning-approach-to-hybrid-control-design
### Abstract
In this paper we design hybrid control policies for hybrid systems whose
mathematical models are unknown. Our contributions are threefold. First, we
propose a framework for modelling the hybrid control design problem as a single
Markov Decision Process (MDP). This result facilitates the application of
off-the-shelf algorithms from Reinforcement Learning (RL) literature towards
designing optimal control policies. Second, we model a set of benchmark
examples of hybrid control design problem in the proposed MDP framework. Third,
we adapt the recently proposed Proximal Policy Optimisation (PPO) algorithm for
the hybrid action space and apply it to the above set of problems. It is
observed that in each case the algorithm converges and finds the optimal
policy.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ··åˆæ§åˆ¶ç³»ç»Ÿè®¾è®¡æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ··åˆåŠ¨åŠ›ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­å¹¿æ³›å­˜åœ¨ï¼Œå¦‚äº¤é€šç®¡ç†ã€åŒ–å­¦è¿‡ç¨‹æ§åˆ¶ã€é€šä¿¡ç½‘ç»œã€åµŒå…¥å¼æ§åˆ¶ã€å‘åŠ¨æœºæ§åˆ¶å’Œæœºå™¨äººç­‰ã€‚ç„¶è€Œï¼Œè®¾è®¡èƒ½å¤Ÿç¡®ä¿ç³»ç»Ÿè‰¯å¥½æ€§èƒ½çš„æ··åˆæ§åˆ¶ç­–ç•¥ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç³»ç»Ÿæ•°å­¦æ¨¡å‹æœªçŸ¥çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ··åˆæ§åˆ¶ç­–ç•¥è®¾è®¡æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ··åˆæ§åˆ¶è®¾è®¡é—®é¢˜å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰
æœ¬æ–‡é¦–å…ˆæå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œå°†æ··åˆæ§åˆ¶è®¾è®¡é—®é¢˜å»ºæ¨¡ä¸ºå•ä¸ªMDPã€‚è¿™ä½¿å¾—å¯ä»¥ç›´æ¥åº”ç”¨RLæ–‡çŒ®ä¸­çš„ç°æˆç®—æ³•æ¥è®¾è®¡æœ€ä¼˜æ§åˆ¶ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨MDPæ¡†æ¶ä¸­å»ºæ¨¡åŸºå‡†æ··åˆæ§åˆ¶è®¾è®¡é—®é¢˜
æœ¬æ–‡åœ¨æå‡ºçš„MDPæ¡†æ¶ä¸­å»ºæ¨¡äº†ä¸€ç³»åˆ—åŸºå‡†æ··åˆæ§åˆ¶è®¾è®¡é—®é¢˜ï¼ŒåŒ…æ‹¬å››æ¡£æ±½è½¦ã€é’¢é€€ç«è¿‡ç¨‹ã€çƒ­æ°´å™¨ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå°†PPOç®—æ³•åº”ç”¨äºæ··åˆåŠ¨ä½œç©ºé—´
æœ¬æ–‡å°†æœ€è¿‘æå‡ºçš„Proximal Policy Optimisationï¼ˆPPOï¼‰ç®—æ³•è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”æ··åˆåŠ¨ä½œç©ºé—´ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸Šè¿°é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œç®—æ³•éƒ½èƒ½æ”¶æ•›å¹¶æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªåŸºå‡†é—®é¢˜ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å››æ¡£æ±½è½¦ã€é’¢é€€ç«è¿‡ç¨‹ã€çƒ­æ°´å™¨ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPPOç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰¾åˆ°æœ€ä¼˜æ··åˆæ§åˆ¶ç­–ç•¥ï¼Œå¹¶åœ¨å„ç§æƒ…å†µä¸‹éƒ½èƒ½æ”¶æ•›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºRLçš„æ··åˆæ§åˆ¶ç­–ç•¥è®¾è®¡æ–¹æ³•å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

*   **æ¨¡å‹æ— å…³æ€§**ï¼šè¯¥æ–¹æ³•ä¸ä¾èµ–äºç³»ç»Ÿçš„æ•°å­¦æ¨¡å‹ï¼Œé€‚ç”¨äºæ¨¡å‹æœªçŸ¥æˆ–éš¾ä»¥å»ºæ¨¡çš„æ··åˆç³»ç»Ÿã€‚
*   **é€šç”¨æ€§**ï¼šè¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§ç±»å‹çš„æ··åˆç³»ç»Ÿï¼ŒåŒ…æ‹¬å…·æœ‰ä¸åŒâ€œè·³è·ƒâ€è¡Œä¸ºçš„ç³»ç»Ÿã€‚
*   **æœ‰æ•ˆæ€§**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰¾åˆ°æœ€ä¼˜æ··åˆæ§åˆ¶ç­–ç•¥ï¼Œå¹¶åœ¨å„ç§æƒ…å†µä¸‹éƒ½èƒ½æ”¶æ•›ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºRLçš„æ··åˆæ§åˆ¶ç­–ç•¥è®¾è®¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ã€é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ç­‰ä¼˜ç‚¹ã€‚è¯¥æ–¹æ³•ä¸ºæ··åˆæ§åˆ¶è®¾è®¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­çš„æ··åˆç³»ç»Ÿæ§åˆ¶é—®é¢˜æä¾›äº†æ–°çš„å·¥å…·ã€‚

## fpga-extended-general-purpose-computer-architecture
### Abstract
This paper introduces a computer architecture, where part of the instruction
set architecture (ISA) is implemented on small highly-integrated
field-programmable gate arrays (FPGAs). Small FPGAs inside a general-purpose
processor (CPU) can be used effectively to implement custom or standardised
instructions. Our proposed architecture directly address related challenges for
high-end CPUs, where such highly-integrated FPGAs would have the highest
impact, such as on main memory bandwidth. This also enables
software-transparent context-switching. The simulation-based evaluation of a
dynamically reconfigurable core shows promising results approaching the
performance of an equivalent core with all enabled instructions. Finally, the
feasibility of adopting the proposed architecture in today's CPUs is studied
through the prototyping of fast-reconfigurable FPGAs and studying the miss
behaviour of opcodes.
### ğŸŒŸ è®ºæ–‡è§£è¯» | FPGAæ‰©å±•é€šç”¨è®¡ç®—æœºæ¶æ„ï¼šæå‡æ€§èƒ½çš„æ–°æ€è·¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è®¡ç®—éœ€æ±‚çš„æ—¥ç›Šå¢é•¿ï¼Œé€šç”¨å¤„ç†å™¨ï¼ˆCPUï¼‰åœ¨æ€§èƒ½ä¸Šé€æ¸éš¾ä»¥æ»¡è¶³ç‰¹å®šåº”ç”¨çš„éœ€æ±‚ã€‚å°½ç®¡ä¸“ç”¨å¤„ç†å™¨å¦‚GPUã€FPGAå’ŒASICç­‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤æ‚çš„ç¼–ç¨‹æ¨¡å‹å’Œæ˜‚è´µçš„éƒ¨ç½²æˆæœ¬ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„é€šç”¨å¤„ç†å™¨æ¶æ„åœ¨æ‰©å±•æŒ‡ä»¤é›†æ—¶é¢ä¸´ç€ç¡¬ä»¶å¤æ‚æ€§å’ŒåŠŸè€—æ•ˆç‡çš„æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œFPGAæ‰©å±•ä¿®æ”¹å“ˆä½›æ¶æ„â€çš„æ–°å‹è®¡ç®—æœºæ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡åœ¨é€šç”¨å¤„ç†å™¨å†…éƒ¨é›†æˆå°å‹é«˜åº¦é›†æˆçš„FPGAæ¥æå‡æ€§èƒ½ã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†éƒ¨åˆ†æŒ‡ä»¤é›†æ¶æ„ï¼ˆISAï¼‰å®ç°äºFPGAä¸Šï¼Œä½¿å¾—CPUèƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰§è¡Œè‡ªå®šä¹‰æˆ–æ ‡å‡†åŒ–çš„æŒ‡ä»¤ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥æŒ‡ä»¤æ¶ˆæ­§å™¨å•å…ƒï¼Œç”¨äºå¤„ç†æŒ‡ä»¤è§£ç è¿‡ç¨‹ä¸­çš„è¯·æ±‚ï¼Œå¹¶æ ¹æ®æŒ‡ä»¤æ“ä½œç æŸ¥æ‰¾ç›¸åº”çš„FPGAæŒ‡ä»¤å®ç°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®¾è®¡äº†ä¸€ä¸ªç‹¬ç«‹çš„ä½æµç¼“å­˜ï¼Œç”¨äºå­˜å‚¨FPGAæŒ‡ä»¤çš„ä½æµï¼Œä»è€Œæé«˜å¯é‡æ„æ ¸å¿ƒçš„æ€§èƒ½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé€šè¿‡æ¨¡æ‹Ÿè¯„ä¼°ï¼ŒéªŒè¯äº†åŠ¨æ€å¯é‡æ„æ ¸å¿ƒçš„æ€§èƒ½ï¼Œå¹¶ç ”ç©¶äº†åœ¨å½“å‰CPUä¸­é‡‡ç”¨è¯¥æ¶æ„çš„å¯è¡Œæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€å¯é‡æ„æ ¸å¿ƒçš„æ€§èƒ½æ¥è¿‘äºå…·æœ‰æ‰€æœ‰å¯ç”¨æŒ‡ä»¤çš„ç­‰æ•ˆæ ¸å¿ƒã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸå‹è®¾è®¡å’Œç ”ç©¶æ“ä½œç çš„ç¼ºå¤±è¡Œä¸ºï¼Œè¯æ˜äº†åœ¨å½“å‰CPUä¸­é‡‡ç”¨è¯¥æ¶æ„çš„å¯è¡Œæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„FPGAæ‰©å±•é€šç”¨è®¡ç®—æœºæ¶æ„ä¸ºæå‡CPUæ€§èƒ½æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡å°†éƒ¨åˆ†æŒ‡ä»¤é›†å®ç°äºFPGAä¸Šï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ‰§è¡Œè‡ªå®šä¹‰æˆ–æ ‡å‡†åŒ–çš„æŒ‡ä»¤ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„è¿˜æ”¯æŒè½¯ä»¶é€æ˜çš„ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œä½¿å¾—æ“ä½œç³»ç»Ÿå¯ä»¥æä¾›ISAæ‰©å±•ï¼Œè€Œç¡¬ä»¶åˆ™æ ¹æ®éœ€æ±‚åŠ¨æ€åœ°è·å–ç›¸åº”çš„ä½æµã€‚è¿™äº›åˆ›æ–°ç‚¹ä¸ºæœªæ¥CPUæ¶æ„çš„è®¾è®¡æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## generative-agents--interactive-simulacra-of-human-behavior
### Abstract
Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„äº¤äº’å¼æ¨¡æ‹Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œäººä»¬å¯¹äºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„æ™ºèƒ½ä½“äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚è¿™äº›æ™ºèƒ½ä½“å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ²‰æµ¸å¼ç¯å¢ƒã€äººé™…æ²Ÿé€šæ¼”ç»ƒç©ºé—´ã€åŸå‹è®¾è®¡å·¥å…·ç­‰ã€‚ç„¶è€Œï¼Œè¦åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿé•¿æœŸä¿æŒä¸€è‡´æ€§å’Œå¯ä¿¡åº¦çš„æ™ºèƒ½ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¼æ™ºèƒ½ä½“
æœ¬æ–‡æå‡ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„æ¦‚å¿µï¼Œå³åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¨¡æ‹Ÿå¯ä¿¡çš„äººç±»è¡Œä¸ºã€‚è¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œæ—¥å¸¸æ´»åŠ¨ï¼Œå¦‚èµ·åºŠã€åšé¥­ã€ä¸Šç­ç­‰ï¼Œå¹¶èƒ½å¤Ÿå½¢æˆè‡ªå·±çš„è§‚ç‚¹ã€ä¸ä»–äººäº’åŠ¨ã€å‘èµ·å¯¹è¯ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ™ºèƒ½ä½“æ¶æ„
ä¸ºäº†å®ç°ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œè¯¥æ¶æ„æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­˜å‚¨æ™ºèƒ½ä½“çš„ç»éªŒè®°å½•ï¼Œå¹¶å°†è¿™äº›è®°å¿†éšç€æ—¶é—´çš„æ¨ç§»åˆæˆæ›´é«˜å±‚æ¬¡çš„åæ€ï¼Œå¹¶åŠ¨æ€åœ°æ£€ç´¢å®ƒä»¬æ¥è§„åˆ’è¡Œä¸ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­åˆ›å»ºä¸€ä¸ªç”±25ä¸ªæ™ºèƒ½ä½“ç»„æˆçš„å°é•‡ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿäº§ç”Ÿå¯ä¿¡çš„ä¸ªä½“å’Œç¾¤ä½“è¡Œä¸ºï¼Œä¾‹å¦‚ï¼Œåœ¨ç”¨æˆ·æŒ‡å®šä¸€ä¸ªæ™ºèƒ½ä½“æƒ³è¦ä¸¾åŠæƒ…äººèŠ‚æ´¾å¯¹çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»åœ°é‚€è¯·å…¶ä»–æ™ºèƒ½ä½“å‚åŠ æ´¾å¯¹ï¼Œå¹¶åè°ƒåœ¨æ­£ç¡®çš„æ—¶é—´ä¸€èµ·åˆ°è¾¾æ´¾å¯¹åœ°ç‚¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æ¶æ„ä¸ºåˆ›å»ºå¯ä¿¡çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚è¯¥æ¶æ„å¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸï¼Œä¾‹å¦‚è§’è‰²æ‰®æ¼”ã€ç¤¾äº¤åŸå‹è®¾è®¡ã€è™šæ‹Ÿä¸–ç•Œå’Œæ¸¸æˆç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨äº¤äº’å¼ç³»ç»Ÿä¸­çš„åº”ç”¨æœºä¼šã€ä¼¦ç†å’Œç¤¾ä¼šé£é™©ã€‚

## multi-stage-episodic-control-for-strategic-exploration-in-text-games
### Abstract
Text adventure games present unique challenges to reinforcement learning
methods due to their combinatorially large action spaces and sparse rewards.
The interplay of these two factors is particularly demanding because large
action spaces require extensive exploration, while sparse rewards provide
limited feedback. This work proposes to tackle the explore-vs-exploit dilemma
using a multi-stage approach that explicitly disentangles these two strategies
within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins
each episode using an exploitation policy that imitates a set of promising
trajectories from the past, and then switches over to an exploration policy
aimed at discovering novel actions that lead to unseen state spaces. This
policy decomposition allows us to combine global decisions about which parts of
the game space to return to with curiosity-based local exploration in that
space, motivated by how a human may approach these games. Our method
significantly outperforms prior approaches by 27% and 11% average normalized
score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in
both deterministic and stochastic settings, respectively. On the game of Zork1,
in particular, XTX obtains a score of 103, more than a 2x improvement over
prior methods, and pushes past several known bottlenecks in the game that have
plagued previous state-of-the-art methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šé˜¶æ®µç­–ç•¥æ¢ç´¢ï¼šæ–‡æœ¬æ¸¸æˆä¸­çš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬å†’é™©æ¸¸æˆä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›äº†ç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆå…·æœ‰ç»„åˆçˆ†ç‚¸å¼çš„åŠ¨ä½œç©ºé—´å’Œç¨€ç–çš„å¥–åŠ±ï¼Œè¿™ä½¿å¾—æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡å˜å¾—å°¤ä¸ºå›°éš¾ã€‚å¤§åŠ¨ä½œç©ºé—´éœ€è¦å¹¿æ³›çš„æ¢ç´¢ï¼Œè€Œç¨€ç–çš„å¥–åŠ±åˆ™æä¾›äº†æœ‰é™çš„åé¦ˆã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å•ä¸€ç­–ç•¥å’ŒåŠ¨ä½œé€‰æ‹©ç­–ç•¥ï¼Œéš¾ä»¥åœ¨æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´æ‰¾åˆ°åˆé€‚çš„å¹³è¡¡ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†åä¸º eXploit-Then-eXplore (XTX) çš„å¤šé˜¶æ®µæ§åˆ¶ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ¯ä¸ªå›åˆä¸­æ˜ç¡®åœ°å°†æ¢ç´¢å’Œåˆ©ç”¨ç­–ç•¥åˆ†ç¦»ã€‚XTX ç®—æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š

* **åˆ©ç”¨é˜¶æ®µ**ï¼šè¯¥é˜¶æ®µä½¿ç”¨ä¸€ä¸ªæ¨¡ä»¿è¿‡å»æˆåŠŸè½¨è¿¹çš„ç­–ç•¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¿”å›åˆ°æ¸¸æˆç©ºé—´ä¸­å·²æ¢ç´¢çš„å‰æ²¿çŠ¶æ€ã€‚
* **æ¢ç´¢é˜¶æ®µ**ï¼šè¯¥é˜¶æ®µä½¿ç”¨ä¸€ä¸ªåŸºäºå¥½å¥‡å¿ƒé©±åŠ¨çš„ç­–ç•¥ï¼Œæ—¨åœ¨å‘ç°æ–°é¢–çš„åŠ¨ä½œï¼Œå¹¶æ¢ç´¢æœªçŸ¥çš„æ¸¸æˆçŠ¶æ€ç©ºé—´ã€‚

è¿™ç§ç­–ç•¥åˆ†è§£å…è®¸æ™ºèƒ½ä½“ç»“åˆå…¨å±€å†³ç­–å’Œå±€éƒ¨æ¢ç´¢ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹ç¨€ç–å¥–åŠ±å’ŒåŠ¨æ€åŠ¨ä½œç©ºé—´çš„æŒ‘æˆ˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Jericho åŸºå‡†æµ‹è¯•çš„ 12 ä¸ªæ¸¸æˆä¸­ï¼ŒXTX ç®—æ³•åœ¨ç¡®å®šæ€§å’Œéšæœºæ€§è®¾ç½®ä¸‹åˆ†åˆ«æ¯”ç°æœ‰æ–¹æ³•æé«˜äº† 27% å’Œ 11% çš„å¹³å‡å½’ä¸€åŒ–åˆ†æ•°ã€‚ç‰¹åˆ«æ˜¯åœ¨ Zork1 æ¸¸æˆä¸­ï¼ŒXTX ç®—æ³•å–å¾—äº† 103 åˆ†çš„æˆç»©ï¼Œæ¯”ç°æœ‰æ–¹æ³•æé«˜äº† 2 å€ä»¥ä¸Šï¼Œå¹¶å…‹æœäº†æ¸¸æˆä¸­ä¸€äº›å·²çŸ¥çš„ç“¶é¢ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **å¤šé˜¶æ®µç­–ç•¥**ï¼šå°†æ¢ç´¢å’Œåˆ©ç”¨ç­–ç•¥åˆ†ç¦»ï¼Œå¯ä»¥æ›´å¥½åœ°å¹³è¡¡ä¸¤è€…ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚
* **æ¨¡ä»¿å­¦ä¹ **ï¼šåˆ©ç”¨è¿‡å»æˆåŠŸçš„ç»éªŒæ¥æŒ‡å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¯ä»¥åŠ å¿«å­¦ä¹ é€Ÿåº¦ã€‚
* **å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢**ï¼šé€šè¿‡å¥–åŠ±æ–°é¢–çš„åŠ¨ä½œï¼Œå¯ä»¥é¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çš„æ¸¸æˆçŠ¶æ€ç©ºé—´ã€‚
* **æ··åˆç­–ç•¥**ï¼šä½¿ç”¨æ··åˆç­–ç•¥å¯ä»¥æä¾›æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ¸¸æˆç¯å¢ƒã€‚

### ğŸŒŸ æ€»ç»“
XTX ç®—æ³•ä¸ºæ–‡æœ¬å†’é™©æ¸¸æˆä¸­çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡å¤šé˜¶æ®µç­–ç•¥å’Œæ··åˆç­–ç•¥ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œå¹¶åœ¨å®é™…æ¸¸æˆä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization
### Abstract
Large Language Models (LLMs) exhibit robust problem-solving capabilities for
diverse tasks. However, most LLM-based agents are designed as specific task
solvers with sophisticated prompt engineering, rather than agents capable of
learning and evolving through interactions. These task solvers necessitate
manually crafted prompts to inform task rules and regulate LLM behaviors,
inherently incapacitating to address complex dynamic scenarios e.g., large
interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent
with Policy-level Reflection and Optimization that can learn a wealth of
expertise from interactive experiences and progressively elevate its behavioral
policy. Specifically, it involves a dynamic belief generation and reflection
process for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-Proï¼šåŸºäºç­–ç•¥çº§åæ€å’Œä¼˜åŒ–çš„å­¦ä¹ è¿›åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å„ç§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°åŸºäºLLMsçš„æ™ºèƒ½ä½“éƒ½æ˜¯ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„ï¼Œéœ€è¦å¤æ‚çš„æç¤ºå·¥ç¨‹æ¥å‘ŠçŸ¥ä»»åŠ¡è§„åˆ™å’Œè°ƒèŠ‚LLMsçš„è¡Œä¸ºã€‚è¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”å¯¹å¤æ‚åŠ¨æ€çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤§å‹äº’åŠ¨æ¸¸æˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgent-Proçš„LLM-basedæ™ºèƒ½ä½“ï¼Œå®ƒå…·æœ‰ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä»äº’åŠ¨ç»éªŒä¸­å­¦ä¹ å¤§é‡ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶é€æ­¥æå‡å…¶è¡Œä¸ºç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç­–ç•¥çº§åæ€å’Œä¼˜åŒ–
Agent-Proé€šè¿‡ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–æ¥å­¦ä¹ è¿›åŒ–ã€‚å®ƒä¸ä»…åæ€è¿‡å»çš„è½¨è¿¹å’Œä¿¡å¿µï¼Œè¿˜é€šè¿‡æ·±åº¦ä¼˜å…ˆæœç´¢æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿ç­–ç•¥æ”¶ç›Šçš„æŒç»­æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹
Agent-Proé‡‡ç”¨ä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡æ›´æ–°è‡ªèº«ä¿¡å¿µå’Œä¸–ç•Œä¿¡å¿µæ¥ç”Ÿæˆæ›´åˆç†çš„è¡Œä¸ºã€‚å®ƒèƒ½å¤Ÿæ ¹æ®ä¿¡å¿µæ¥é¢„æµ‹è¡ŒåŠ¨ï¼Œå¹¶åœ¨æ¸¸æˆç»“æŸåæ ¹æ®ç»“æœæ¥åæ€å’Œè°ƒæ•´ä¿¡å¿µã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Agent-Proåœ¨ä¸¤ä¸ªæ¸¸æˆï¼ˆBlackjackå’ŒTexas Hold'emï¼‰ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å®ƒèƒ½å¤Ÿå­¦ä¹ å¹¶è¿›åŒ–ï¼Œåœ¨å¤æ‚å’ŒåŠ¨æ€çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„LLMså’Œä¸“é—¨æ¨¡å‹ç›¸æ¯”ï¼ŒAgent-Proåœ¨æ¸¸æˆä¸­çš„æ”¶ç›Šæ›´é«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Agent-Proçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸ºæ„å»ºèƒ½å¤Ÿå­¦ä¹ å’Œè¿›åŒ–çš„LLM-basedæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–æœºåˆ¶å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“ä»äº’åŠ¨ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶é€æ­¥æå‡å…¶è¡Œä¸ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šçš„åœºæ™¯ä¸­åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å•†ä¸šè°ˆåˆ¤ã€å®‰å…¨ç›‘æ§ç­‰ã€‚

## towards-automation-of-cognitive-modeling-using-large-language-models
### Abstract
Computational cognitive models, which formalize theories of cognition, enable
researchers to quantify cognitive processes and arbitrate between competing
theories by fitting models to behavioral data. Traditionally, these models are
handcrafted, which requires significant domain knowledge, coding expertise, and
time investment. Previous work has demonstrated that Large Language Models
(LLMs) are adept at pattern recognition in-context, solving complex problems,
and generating executable code. In this work, we leverage these abilities to
explore the potential of LLMs in automating the generation of cognitive models
based on behavioral data. We evaluated the LLM in two different tasks: model
identification (relating data to a source model), and model generation
(generating the underlying cognitive model). We performed these tasks across
two cognitive domains - decision making and learning. In the case of data
simulated from canonical cognitive models, we found that the LLM successfully
identified and generated the ground truth model. In the case of human data,
where behavioral noise and lack of knowledge of the true underlying process
pose significant challenges, the LLM generated models that are identical or
close to the winning model from cognitive science literature. Our findings
suggest that LLMs can have a transformative impact on cognitive modeling. With
this project, we aim to contribute to an ongoing effort of automating
scientific discovery in cognitive science.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°è®¤çŸ¥å»ºæ¨¡è‡ªåŠ¨åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„è®¤çŸ¥å»ºæ¨¡éœ€è¦ç ”ç©¶äººå‘˜å…·å¤‡æ·±åšçš„é¢†åŸŸçŸ¥è¯†ã€ç¼–ç¨‹æŠ€èƒ½å’Œå¤§é‡çš„æ—¶é—´æŠ•å…¥ã€‚æ‰‹å·¥æ„å»ºçš„æ¨¡å‹å¾€å¾€å—é™äºç ”ç©¶è€…çš„èƒŒæ™¯å’Œå»ºæ¨¡èƒ½åŠ›ï¼Œéš¾ä»¥æ¢ç´¢æ›´å¹¿æ³›çš„å‡è®¾ç©ºé—´ã€‚å› æ­¤ï¼Œè‡ªåŠ¨åŒ–è®¤çŸ¥æ¨¡å‹ç”Ÿæˆæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¨¡å‹è¯†åˆ«å’Œç”Ÿæˆ
æœ¬æ–‡åˆ©ç”¨LLMçš„å¼ºå¤§èƒ½åŠ›ï¼Œæ¢ç´¢äº†å…¶åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆè®¤çŸ¥æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚LLMèƒ½å¤Ÿå¤„ç†è‡ªç„¶è¯­è¨€æ ¼å¼çš„è¡Œä¸ºæ•°æ®ï¼Œè¯†åˆ«å¤æ‚é—®é¢˜ä¸­çš„æ¨¡å¼ï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œçš„ä»£ç ã€‚é€šè¿‡å°†LLMåº”ç”¨äºæ¨¡å‹è¯†åˆ«å’Œæ¨¡å‹ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–è®¤çŸ¥æ¨¡å‹ç”Ÿæˆçš„å…³é”®æ­¥éª¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè¡Œä¸ºæ•°æ®ç”Ÿæˆè®¤çŸ¥æ¨¡å‹
æœ¬æ–‡é€šè¿‡è®¾è®¡ä¸€ä¸ªæµç¨‹ï¼Œè®©LLMæ ¹æ®è¡Œä¸ºæ•°æ®å’Œä»»åŠ¡æè¿°ç”Ÿæˆè®¤çŸ¥æ¨¡å‹ã€‚LLMé¦–å…ˆè¯†åˆ«æ•°æ®æºæ¨¡å‹ï¼Œç„¶åæ ¹æ®è§‚å¯Ÿåˆ°çš„æ•°æ®ç”Ÿæˆè®¤çŸ¥æ¨¡å‹ã€‚é€šè¿‡å°†LLMåº”ç”¨äºæ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®äººç±»æ•°æ®ï¼Œå¯ä»¥è¯„ä¼°LLMåœ¨æ¨¡å‹è¯†åˆ«å’Œæ¨¡å‹ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å†³ç­–å’Œå­¦ä¹ çš„ä¸¤ä¸ªè®¤çŸ¥é¢†åŸŸè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨æ¨¡å‹è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«æ•°æ®æºæ¨¡å‹ã€‚åœ¨æ¨¡å‹ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLLMç”Ÿæˆçš„æ¨¡å‹ä¸çœŸå®æ•°æ®ç”Ÿæˆå‡½æ•°éå¸¸æ¥è¿‘ï¼Œå¹¶ä¸”åœ¨æ‹ŸåˆçœŸå®äººç±»æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åˆ©ç”¨LLMè¿›è¡Œè®¤çŸ¥å»ºæ¨¡è‡ªåŠ¨åŒ–çš„æ–¹æ³•å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. **é™ä½é—¨æ§›**ï¼šLLMèƒ½å¤Ÿå¤„ç†è‡ªç„¶è¯­è¨€æ ¼å¼çš„æ•°æ®ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜æ— éœ€å…·å¤‡æ·±åšçš„ç¼–ç¨‹æŠ€èƒ½å³å¯è¿›è¡Œè®¤çŸ¥å»ºæ¨¡ã€‚
2. **åŠ é€Ÿç ”ç©¶**ï¼šLLMèƒ½å¤Ÿè‡ªåŠ¨åŒ–è®¤çŸ¥æ¨¡å‹ç”Ÿæˆçš„å…³é”®æ­¥éª¤ï¼Œä»è€ŒåŠ é€Ÿç ”ç©¶è¿›ç¨‹ã€‚
3. **æ¢ç´¢æ›´å¹¿æ³›çš„å‡è®¾ç©ºé—´**ï¼šLLMèƒ½å¤Ÿç”Ÿæˆå¤šç§è®¤çŸ¥æ¨¡å‹ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ¢ç´¢æ›´å¹¿æ³›çš„å‡è®¾ç©ºé—´ã€‚

### ğŸŒŸ æœªæ¥å±•æœ›
æœªæ¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢LLMåœ¨æ›´å¹¿æ³›çš„è®¤çŸ¥é¢†åŸŸä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚æ„ŸçŸ¥ã€è®°å¿†å’Œè¯­è¨€ç†è§£ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨è®¤çŸ¥å»ºæ¨¡ä»»åŠ¡ä¸Šå¾®è°ƒLLMï¼Œå¯ä»¥æé«˜å…¶ç”Ÿæˆç§‘å­¦æ„ä¹‰æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æœ€åï¼Œé€šè¿‡é›†æˆå¤šä¸ªLLMï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„ã€é€šç”¨çš„è®¤çŸ¥å»ºæ¨¡æ¡†æ¶ã€‚

## repoaudit--an-autonomous-llm-agent-for-repository-level-code-auditing
### Abstract
Code auditing is a code review process with the goal of finding bugs. Large
Language Models (LLMs) have shown substantial potential in this task, offering
the ability to analyze programs without compilation and enabling customized bug
detection following specified prompts. However, applying LLMs to
repository-level code auditing presents notable challenges. The inherent
context limits and hallucinations of LLMs can lead to the low quality of bug
reports. Meanwhile, the large size of software repositories introduces
substantial time and token costs, hindering efficiency and scalability in
real-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit,
designed to enable precise and efficient repository-level code auditing.
Equipped with the agent memory, RepoAudit explores the code repository on
demand, analyzing data-flow facts along different feasible program paths in
individual functions. It also introduces the validator to check the data-flow
facts for hallucination mitigation and examine the satisfiability of path
conditions of potential buggy paths, which enables RepoAudit to discard false
positives in the code auditing. Our experiment shows that RepoAudit powered by
Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems,
consuming 0.44 hours and $2.54 per project on average.
### ğŸŒŸ è®ºæ–‡è§£è¯» | RepoAuditï¼šåŸºäºLLMçš„ä»£ç å®¡è®¡åˆ©å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è½¯ä»¶ä»£ç åº“çš„å¿«é€Ÿè†¨èƒ€ï¼Œä¼ ç»Ÿçš„ä»£ç å®¡è®¡æ–¹æ³•é¢ä¸´ç€æ•ˆç‡ä½ä¸‹ã€éš¾ä»¥æ‰©å±•ç­‰æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç å®¡è®¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºä»£ç åº“çº§åˆ«çš„å®¡è®¡å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚LLMsçš„ä¸Šä¸‹æ–‡é™åˆ¶å’Œå¹»è§‰é—®é¢˜ï¼Œä»¥åŠä»£ç åº“è§„æ¨¡å¯¼è‡´çš„æˆæœ¬é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
RepoAudit æ˜¯ä¸€ä¸ªè‡ªä¸»çš„ LLM-Agentï¼Œæ—¨åœ¨å®ç°ç²¾ç¡®å’Œé«˜æ•ˆçš„ä»£ç åº“çº§åˆ«ä»£ç å®¡è®¡ã€‚å®ƒåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè·¯å¾„æ•æ„Ÿå’ŒæŒ‰éœ€é©±åŠ¨
RepoAudit é€šè¿‡è·¯å¾„æ•æ„Ÿå’ŒæŒ‰éœ€é©±åŠ¨çš„å›¾éå†æ–¹å¼ï¼Œæ¨¡æ‹Ÿäººç±»å®¡è®¡è¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³ LLMs åœ¨å¤„ç†å¤§å‹ç¨‹åºå›¾æ—¶çš„å±€é™æ€§ã€‚å®ƒå°†ç¨‹åºåˆ†è§£ä¸ºæ›´å°çš„å•å…ƒï¼ˆå¦‚å‡½æ•°ï¼‰ï¼Œå¹¶é€æ­¥åˆ†ææ¯ä¸ªå•å…ƒï¼Œä»è€Œé¿å…è·¯å¾„çˆ†ç‚¸é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéªŒè¯æœºåˆ¶
RepoAudit å¼•å…¥éªŒè¯å™¨æ¥æ£€æŸ¥æ•°æ®æµäº‹å®ï¼Œä»¥å‡è½» LLMs çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶æ£€æŸ¥æ½œåœ¨é”™è¯¯è·¯å¾„çš„è·¯å¾„æ¡ä»¶ï¼Œä»è€Œæ’é™¤ä»£ç å®¡è®¡ä¸­çš„è¯¯æŠ¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
RepoAudit åœ¨ 15 ä¸ªçœŸå®ä¸–ç•Œç³»ç»Ÿä¸­æˆåŠŸå‘ç°äº† 38 ä¸ªçœŸå®æ¼æ´ï¼Œå¹³å‡æ¯ä¸ªé¡¹ç›®è€—æ—¶ 0.44 å°æ—¶ï¼Œæˆæœ¬ä¸º 2.54 ç¾å…ƒã€‚ä¸ Meta INFER å’Œ Amazon CODEGURU ç­‰å·¥ä¸šé™æ€æ¼æ´æ£€æµ‹å·¥å…·ç›¸æ¯”ï¼ŒRepoAudit å…·æœ‰æ›´é«˜çš„æ£€æµ‹èƒ½åŠ›å’Œæ›´ä½çš„è¯¯æŠ¥ç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
RepoAudit çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸º LLM åœ¨ä»£ç å®¡è®¡é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶è·¯å¾„æ•æ„Ÿå’ŒæŒ‰éœ€é©±åŠ¨çš„å›¾éå†æ–¹å¼ã€éªŒè¯æœºåˆ¶ç­‰åˆ›æ–°ç‚¹å€¼å¾—å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒRepoAudit çš„è®¾è®¡ä¹Ÿä¸ºå…¶ä»– LLM é©±åŠ¨çš„ä»£ç ä»»åŠ¡æä¾›äº†å‚è€ƒï¼Œä¾‹å¦‚ç¨‹åºä¿®å¤ã€æµ‹è¯•å’Œåˆ†æç­‰ã€‚

## empowering-working-memory-for-large-language-model-agents
### Abstract
Large language models (LLMs) have achieved impressive linguistic
capabilities. However, a key limitation persists in their lack of human-like
memory faculties. LLMs exhibit constrained memory retention across sequential
interactions, hindering complex reasoning. This paper explores the potential of
applying cognitive psychology's working memory frameworks, to enhance LLM
architecture. The limitations of traditional LLM memory designs are analyzed,
including their isolation of distinct dialog episodes and lack of persistent
memory links. To address this, an innovative model is proposed incorporating a
centralized Working Memory Hub and Episodic Buffer access to retain memories
across episodes. This architecture aims to provide greater continuity for
nuanced contextual reasoning during intricate tasks and collaborative
scenarios. While promising, further research is required into optimizing
episodic memory encoding, storage, prioritization, retrieval, and security.
Overall, this paper provides a strategic blueprint for developing LLM agents
with more sophisticated, human-like memory capabilities, highlighting memory
mechanisms as a vital frontier in artificial general intelligence.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†èµ‹èƒ½å·¥ä½œè®°å¿†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ç¼ºä¹ç±»ä¼¼äººç±»çš„è®°å¿†èƒ½åŠ›ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚æ¨ç†å’Œåä½œåœºæ™¯ä¸­çš„è¡¨ç°ã€‚LLMsåœ¨è¿ç»­äº¤äº’ä¸­çš„è®°å¿†ä¿ç•™èƒ½åŠ›æœ‰é™ï¼Œæ¯ä¸ªäº¤äº’éƒ½è¢«è§†ä¸ºç‹¬ç«‹çš„å¯¹è¯ï¼Œç¼ºä¹æŒç»­çš„å†…å­˜é“¾æ¥ï¼Œè¿™é˜»ç¢äº†å¤æ‚çš„æ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªä¸­å¤®å·¥ä½œè®°å¿†ä¸­å¿ƒï¼ˆWorking Memory Hubï¼‰å’Œè®¿é—®æƒ…å¢ƒç¼“å†²åŒºï¼ˆEpisodic Bufferï¼‰çš„åŠŸèƒ½ï¼Œä»¥ä¿ç•™è·¨å¯¹è¯çš„è®°å¿†ã€‚è¿™ç§æ¶æ„æ—¨åœ¨ä¸ºå¤æ‚çš„ä»»åŠ¡å’Œåä½œåœºæ™¯ä¸­çš„å¾®å¦™ä¸Šä¸‹æ–‡æ¨ç†æä¾›æ›´å¤§çš„è¿ç»­æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡è¿˜æ¢è®¨äº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ç®¡ç†æ™ºèƒ½ä½“å¯¹æƒ…å¢ƒç¼“å†²åŒºè®¿é—®çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ã€åŸºäºä»»åŠ¡çš„å†…å­˜åˆ†é…ã€è‡ªä¸»å†…å­˜æ£€ç´¢å’Œä¸“é—¨çš„å†…å­˜ç®¡ç†ä»£ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§æˆ˜ç•¥è“å›¾ï¼Œç”¨äºå¼€å‘å…·æœ‰æ›´å¤æ‚ã€ç±»ä¼¼äººç±»è®°å¿†èƒ½åŠ›çš„LLMä»£ç†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å·¥ä½œè®°å¿†æ¨¡å‹ä¸ºLLMä»£ç†æä¾›äº†æ›´é«˜çº§çš„è®°å¿†åŠŸèƒ½ï¼Œæœ‰åŠ©äºæé«˜å®ƒä»¬åœ¨å¤æ‚ä»»åŠ¡å’Œåä½œåœºæ™¯ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ç®¡ç†å†…å­˜è®¿é—®çš„ç­–ç•¥ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€å®‰å…¨çš„å†…å­˜ç®¡ç†ç³»ç»Ÿæä¾›äº†å‚è€ƒã€‚

## voyager--an-open-ended-embodied-agent-with-large-language-models
### Abstract
We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Voyagerï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¼å…·èº«ç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ„å»ºèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æŒç»­æ¢ç´¢ã€è§„åˆ’å’Œå¼€å‘æ–°æŠ€èƒ½çš„é€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼Œæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æ¢ç´¢ã€å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“åˆ©ç”¨é¢„è®­ç»ƒLLMä¸­çš„ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆä¸€è‡´çš„è¡ŒåŠ¨è®¡åˆ’æˆ–å¯æ‰§è¡Œç­–ç•¥ï¼Œä½†å®ƒä»¬å¹¶éç»ˆèº«å­¦ä¹ è€…ï¼Œæ— æ³•åœ¨é•¿æ—¶é—´è·¨åº¦å†…é€æ­¥è·å–ã€æ›´æ–°ã€ç§¯ç´¯å’Œè½¬ç§»çŸ¥è¯†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
Voyager æ˜¯ç¬¬ä¸€ä¸ªç”± LLM é©±åŠ¨çš„å…·èº«ç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨ Minecraft ä¸­æŒç»­æ¢ç´¢ä¸–ç•Œã€è·å–å¤šæ ·åŒ–æŠ€èƒ½ï¼Œå¹¶åœ¨æ²¡æœ‰äººç±»å¹²é¢„çš„æƒ…å†µä¸‹è¿›è¡Œæ–°çš„å‘ç°ã€‚Voyager ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨è¯¾ç¨‹
Voyager é€šè¿‡è‡ªåŠ¨è¯¾ç¨‹è¿›è¡Œå¼€æ”¾å¼æ¢ç´¢ï¼Œè¯¥è¯¾ç¨‹ç”± GPT-4 ç”Ÿæˆï¼Œæ—¨åœ¨â€œå‘ç°å°½å¯èƒ½å¤šçš„å¤šæ ·åŒ–äº‹ç‰©â€ã€‚è¯¾ç¨‹ä¼šæ ¹æ®æ¢ç´¢è¿›åº¦å’Œæ™ºèƒ½ä½“çš„çŠ¶æ€æå‡ºè¶Šæ¥è¶Šéš¾çš„ä»»åŠ¡ï¼Œä»è€Œæ¨åŠ¨æ™ºèƒ½ä½“ä¸æ–­å­¦ä¹ æ–°æŠ€èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæŠ€èƒ½åº“
Voyager æ‹¥æœ‰ä¸€ä¸ªä¸æ–­å¢é•¿çš„æŠ€èƒ½åº“ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¯æ‰§è¡Œä»£ç ï¼Œä»¥å­˜å‚¨å’Œæ£€ç´¢å¤æ‚çš„è¡Œä¸ºã€‚æ¯ä¸ªæŠ€èƒ½éƒ½ç”±å¯æ‰§è¡Œä»£ç è¡¨ç¤ºï¼Œè¿™äº›ä»£ç å¯ä»¥è‡ªç„¶åœ°è¡¨ç¤ºæ—¶é—´æ‰©å±•å’Œç»„åˆåŠ¨ä½œï¼Œè¿™å¯¹äº Minecraft ä¸­çš„è®¸å¤šé•¿æœŸä»»åŠ¡è‡³å…³é‡è¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿­ä»£æç¤ºæœºåˆ¶
Voyager é€šè¿‡è¿­ä»£æç¤ºæœºåˆ¶ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨ç¯å¢ƒåé¦ˆã€æ‰§è¡Œé”™è¯¯å’Œè‡ªæˆ‘éªŒè¯æ¥æ”¹è¿›ç¨‹åºã€‚è¯¥æœºåˆ¶é€šè¿‡æ‰§è¡Œç”Ÿæˆçš„ç¨‹åºã€è·å–ç¯å¢ƒåé¦ˆå’Œæ‰§è¡Œé”™è¯¯ï¼Œå¹¶å°†è¿™äº›åé¦ˆçº³å…¥ GPT-4 çš„æç¤ºä¸­ï¼Œä»è€Œè¿›è¡Œä»£ç æ”¹è¿›ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°è‡ªæˆ‘éªŒè¯æ¨¡å—ç¡®è®¤ä»»åŠ¡å®Œæˆï¼Œæ­¤æ—¶å°†ç¨‹åºæ·»åŠ åˆ°æŠ€èƒ½åº“ä¸­ï¼Œå¹¶æŸ¥è¯¢è‡ªåŠ¨è¯¾ç¨‹ä»¥è·å–ä¸‹ä¸€ä¸ªç›®æ ‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Voyager åœ¨ MineDojo æ¡†æ¶ä¸­ä¸å…¶ä»– LLM åŸºäºæ™ºèƒ½ä½“æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ Voyager åœ¨å‘ç°æ–°ç‰©å“ã€è§£é” Minecraft æŠ€æœ¯æ ‘ã€ç©¿è¶Šå„ç§åœ°å½¢ä»¥åŠå°†å­¦ä¹ åˆ°çš„æŠ€èƒ½åº“åº”ç”¨äºæ–°ä¸–ç•Œä¸­çš„æœªè§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚Voyager è·å¾—äº† 3.3 å€çš„æ–°ç‰©å“ï¼Œè§£é”å…³é”®æŠ€æœ¯æ ‘é‡Œç¨‹ç¢‘çš„é€Ÿåº¦æé«˜äº† 15.3 å€ï¼Œç©¿è¶Šçš„è·ç¦»æ˜¯åŸºçº¿çš„ 2.3 å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Voyager çš„è®¾è®¡ä¸ºå¼€å‘å¼ºå¤§çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªèµ·ç‚¹ï¼Œæ— éœ€è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚å…¶è‡ªåŠ¨è¯¾ç¨‹ã€æŠ€èƒ½åº“å’Œè¿­ä»£æç¤ºæœºåˆ¶ä¸ºç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒVoyager çš„æŠ€èƒ½åº“å¯ä»¥ä½œä¸ºå…¶ä»–æ–¹æ³•çš„å³æ’å³ç”¨èµ„äº§ï¼Œæœ‰æ•ˆåœ°æé«˜æ€§èƒ½ã€‚

## ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory
### Abstract
The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular "ObtainDiamond" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the "ObtainDiamond" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Minecraftä¸­çš„å¹½çµï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºæ–‡æœ¬çš„çŸ¥è¯†ä¸è®°å¿†ï¼Œåˆ›å»ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Minecraftä½œä¸ºä¸€æ¬¾å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼Œå¸å¼•äº†å¤§é‡ç ”ç©¶å…´è¶£ï¼Œæˆä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„æ™ºèƒ½ä½“çš„ä¸°å¯Œå¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šç›®æ ‡ä¸Šï¼Œå¦‚æµè¡Œçš„â€œObtainDiamondâ€ä»»åŠ¡ï¼Œå°šæœªåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æœ€é«˜æˆåŠŸç‡ä»…ä¸ºçº¦20%ï¼Œçªæ˜¾äº†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ§åˆ¶å™¨æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Ghost in the Minecraftï¼ˆGITMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ç›¸ç»“åˆï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿåœ¨Minecraftä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“ï¼ˆGCAsï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMåˆ†è§£å™¨
LLMåˆ†è§£å™¨è´Ÿè´£å°†ä»»åŠ¡ç›®æ ‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´æ˜“äºå®ç°çš„å­ç›®æ ‡ã€‚é€šè¿‡è§£å†³æ¯ä¸ªå­ç›®æ ‡ï¼Œå¯ä»¥é€æ­¥å®ç°ä»»åŠ¡ç›®æ ‡ã€‚LLMåˆ†è§£å™¨åˆ©ç”¨ä»äº’è”ç½‘æ”¶é›†çš„æ–‡æœ¬çŸ¥è¯†ï¼Œå°†ç›®æ ‡åˆ†è§£ä¸ºå­ç›®æ ‡æ ‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMè§„åˆ’å™¨
LLMè§„åˆ’å™¨è´Ÿè´£ä¸ºæ¯ä¸ªå­ç›®æ ‡ç”Ÿæˆä¸€ç³»åˆ—ç»“æ„åŒ–æ“ä½œã€‚ç»“æ„åŒ–æ“ä½œå…·æœ‰æ˜ç¡®çš„è¯­ä¹‰å’Œç›¸åº”çš„åé¦ˆï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨è®¤çŸ¥å±‚é¢ç†è§£å‘¨å›´ç¯å¢ƒå¹¶åšå‡ºå†³ç­–ã€‚LLMè§„åˆ’å™¨è¿˜è®°å½•å’Œæ€»ç»“æˆåŠŸçš„æ“ä½œåˆ—è¡¨ï¼Œä»¥å¢å¼ºæœªæ¥çš„è§„åˆ’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šLLMæ¥å£
LLMæ¥å£è´Ÿè´£å°†ç»“æ„åŒ–æ“ä½œè½¬æ¢ä¸ºé”®ç›˜/é¼ æ ‡æ“ä½œï¼Œå¹¶ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚å®ƒè¿˜ä»ç¯å¢ƒä¸­æå–è§‚å¯Ÿç»“æœï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåé¦ˆæ¶ˆæ¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº†47.5%ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¯ç¬¬ä¸€ä¸ªåœ¨Minecraft Overworldä¸­è·å–æ‰€æœ‰ç‰©å“çš„æ™ºèƒ½ä½“ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„æŠ€èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GITMæ¡†æ¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨LLMsçš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠåŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æœ‰æ•ˆåœ°å¤„ç†å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å„ç§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å…·æœ‰é«˜æ•ˆçš„å­¦ä¹ æ•ˆç‡å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¼€å‘é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation
### Abstract
Due to the dynamic and unpredictable open-world setting, navigating complex
environments in Minecraft poses significant challenges for multi-agent systems.
Agents must interact with the environment and coordinate their actions with
other agents to achieve common objectives. However, traditional approaches
often struggle to efficiently manage inter-agent communication and task
distribution, crucial for effective multi-agent navigation. Furthermore,
processing and integrating multi-modal information (such as visual, textual,
and auditory data) is essential for agents to comprehend their goals and
navigate the environment successfully and fully. To address this issue, we
design the HAS framework to auto-organize groups of LLM-based agents to
complete navigation tasks. In our approach, we devise a hierarchical
auto-organizing navigation system, which is characterized by 1) a hierarchical
system for multi-agent organization, ensuring centralized planning and
decentralized execution; 2) an auto-organizing and intra-communication
mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal
information platform, facilitating multi-modal perception to perform the three
navigation tasks with one system. To assess organizational behavior, we design
a series of navigation tasks in the Minecraft environment, which includes
searching and exploring. We aim to develop embodied organizations that push the
boundaries of embodied AI, moving it towards a more human-like organizational
structure.
### ğŸŒŸ è®ºæ–‡è§£è¯» | HASï¼šå¼€æ”¾ä¸–ç•Œå¤šæ™ºèƒ½ä½“å¯¼èˆªçš„åˆ†å±‚è‡ªç»„ç»‡ç³»ç»Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œçš„ç¯å¢ƒä¸­ï¼Œå¦‚Minecraftï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¢ä¸´ç€å¤æ‚çš„å¯¼èˆªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¯¼èˆªæ–¹æ³•å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°ç®¡ç†æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡å’Œä»»åŠ¡åˆ†é…ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„å¤šæ™ºèƒ½ä½“å¯¼èˆªè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå¤„ç†å’Œæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚è§†è§‰ã€æ–‡æœ¬å’Œå¬è§‰æ•°æ®ï¼‰å¯¹äºæ™ºèƒ½ä½“ç†è§£å…¶ç›®æ ‡å¹¶åœ¨ç¯å¢ƒä¸­æˆåŠŸå¯¼èˆªè‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ†å±‚è‡ªç»„ç»‡å¯¼èˆªç³»ç»Ÿ
HASæ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚è‡ªç»„ç»‡å¯¼èˆªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. åˆ†å±‚ç³»ç»Ÿï¼šç¡®ä¿é›†ä¸­å¼è§„åˆ’å’Œåˆ†å¸ƒå¼æ‰§è¡Œï¼Œæé«˜å¯¼èˆªæ•ˆç‡ã€‚
2. è‡ªç»„ç»‡æœºåˆ¶ï¼šæ ¹æ®å­ä»»åŠ¡åŠ¨æ€è°ƒæ•´å…³é”®è§’è‰²å’Œè¡ŒåŠ¨ç»„ï¼Œå¹¶ä¿æŒç»„é—´é€šä¿¡ï¼Œç¡®ä¿é«˜æ•ˆåä½œã€‚
3. å¤šæ¨¡æ€ä¿¡æ¯å¹³å°ï¼šä¿ƒè¿›å¤šæ¨¡æ€æ„ŸçŸ¥ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å›¾åƒã€å¯¹è±¡å’ŒéŸ³é¢‘ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œæœç´¢å’Œæ¢ç´¢ç­‰å¯¼èˆªä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹
HASæ¡†æ¶ä½¿ç”¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼ŒåŒ…æ‹¬ç®¡ç†è€…å’Œæ‰§è¡Œè€…ä¸¤ç§ç±»å‹çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å…·æœ‰ä¸åŒçš„åŠŸèƒ½ï¼Œå¦‚è§„åˆ’ã€æè¿°ã€è¯„ä¼°å’Œéƒ¨ç½²ï¼Œä»¥å®ç°é›†ä¸­å¼è§„åˆ’å’Œåˆ†å¸ƒå¼æ‰§è¡Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€è®°å¿†
HASæ¡†æ¶è¿˜å¼•å…¥äº†å¤šæ¨¡æ€è®°å¿†æœºåˆ¶ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»è€Œæé«˜è§„åˆ’å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€æ£€ç´¢ï¼ˆMMRï¼‰æŠ€æœ¯ï¼ŒHASèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å†å²äº¤äº’åé¦ˆå’Œç»éªŒï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„è®¡åˆ’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraftç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHASæ¡†æ¶åœ¨å¤šæ¨¡æ€ç›®æ ‡æœç´¢ã€è¿ç»­å—æœç´¢å’Œåœ°å›¾æ¢ç´¢ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒHASåœ¨å¯¼èˆªæ•ˆç‡ã€æˆåŠŸç‡å’Œæ¢ç´¢èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
HASæ¡†æ¶ä¸ºå¼€æ”¾ä¸–ç•Œå¤šæ™ºèƒ½ä½“å¯¼èˆªæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶åˆ†å±‚è‡ªç»„ç»‡ç»“æ„ã€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è®°å¿†æœºåˆ¶ç­‰åˆ›æ–°ç‚¹ï¼Œå¯¹äºæé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªä¸»æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼ŒHASæ¡†æ¶è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„åœºæ™¯ï¼Œå¦‚æœºå™¨äººååŒã€è™šæ‹Ÿç°å®ç­‰ã€‚

## human-agent-decision-making--combining-theory-and-practice
### Abstract
Extensive work has been conducted both in game theory and logic to model
strategic interaction. An important question is whether we can use these
theories to design agents for interacting with people? On the one hand, they
provide a formal design specification for agent strategies. On the other hand,
people do not necessarily adhere to playing in accordance with these
strategies, and their behavior is affected by a multitude of social and
psychological factors. In this paper we will consider the question of whether
strategies implied by theories of strategic behavior can be used by automated
agents that interact proficiently with people. We will focus on automated
agents that we built that need to interact with people in two negotiation
settings: bargaining and deliberation. For bargaining we will study game-theory
based equilibrium agents and for argumentation we will discuss logic-based
argumentation theory. We will also consider security games and persuasion games
and will discuss the benefits of using equilibrium based agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | äººæœºå†³ç­–ï¼šç†è®ºä¸å®è·µçš„ç»“åˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•å°†åšå¼ˆè®ºå’Œé€»è¾‘ç†è®ºåº”ç”¨äºè®¾è®¡èƒ½å¤Ÿä¸äººç±»æœ‰æ•ˆäº’åŠ¨çš„è‡ªåŠ¨åŒ–ä»£ç†ã€‚å°½ç®¡è¿™äº›ç†è®ºä¸ºä»£ç†ç­–ç•¥æä¾›äº†æ­£å¼çš„è®¾è®¡è§„èŒƒï¼Œä½†äººç±»çš„å†³ç­–è¡Œä¸ºå¾€å¾€å—åˆ°ç¤¾ä¼šå’Œå¿ƒç†å› ç´ çš„å½±å“ï¼Œå¹¶ä¸æ€»æ˜¯éµå¾ªè¿™äº›ç­–ç•¥ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨ç ”ç©¶å¦‚ä½•å°†ç†è®ºç­–ç•¥åº”ç”¨äºè‡ªåŠ¨åŒ–ä»£ç†ï¼Œä½¿å…¶åœ¨ä¸äººç±»äº’åŠ¨æ—¶èƒ½å¤Ÿè¡¨ç°å‡ºè‰²ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šè®®é¢˜è°ˆåˆ¤
æœ¬æ–‡æå‡ºäº†å¤šç§è‡ªåŠ¨åŒ–ä»£ç†ï¼Œç”¨äºå¤„ç†å¤šè®®é¢˜è°ˆåˆ¤åœºæ™¯ã€‚è¿™äº›ä»£ç†ç»“åˆäº†åšå¼ˆè®ºã€é€»è¾‘ç†è®ºå’Œå¯å‘å¼æ–¹æ³•ï¼Œä»¥é€‚åº”äººç±»è°ˆåˆ¤è€…çš„è¡Œä¸ºã€‚ä¾‹å¦‚ï¼ŒEQHä»£ç†é€šè¿‡å¼•å…¥å¯å‘å¼å‚æ•°ï¼Œå¦‚è®©æ­¥å¹…åº¦å’Œè°ˆåˆ¤å•ä½ï¼Œæ¥æé«˜å…¶çµæ´»æ€§ï¼Œä»è€Œåœ¨ä¸äººç±»è°ˆåˆ¤è€…äº’åŠ¨æ—¶è·å¾—æ›´é«˜çš„æ•ˆç”¨åˆ†æ•°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè°ˆåˆ¤ä¸è¡ŒåŠ¨äº¤ç»‡
æœ¬æ–‡ç ”ç©¶äº†åœ¨è°ˆåˆ¤è¿‡ç¨‹ä¸­äº¤ç»‡èµ„æºäº¤æ¢å’Œå…¶ä»–æ´»åŠ¨çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œæ­ç¤ºæ¸¸æˆâ€ä¸­ï¼Œä»£ç†éœ€è¦æ ¹æ®äººç±»è¡Œä¸ºæ¨¡å‹æ¥é¢„æµ‹å¯¹æ‰‹çš„ç±»å‹ï¼Œå¹¶æ®æ­¤è°ƒæ•´å…¶ç­–ç•¥ã€‚åœ¨â€œéç»‘å®šåè®®â€æ¸¸æˆä¸­ï¼Œä»£ç†éœ€è¦å†³å®šæ˜¯å¦éµå®ˆåè®®ï¼Œå¹¶ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥é¢„æµ‹äººç±»å¯¹æ‰‹çš„è¡Œä¸ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®ºè¯ä»£ç†
æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è‡ªåŠ¨åŒ–ä»£ç†å¸®åŠ©äººç±»åœ¨è®ºè¯å¯¹è¯ä¸­æå‡ºè®ºç‚¹ã€‚é€šè¿‡ç»“åˆè®ºè¯ç†è®ºå’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä»£ç†èƒ½å¤Ÿé¢„æµ‹äººç±»å¯èƒ½ä½¿ç”¨çš„è®ºç‚¹ï¼Œå¹¶æä¾›ç›¸å…³çš„è®ºæ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå®‰å…¨æ¸¸æˆ
æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•å°†åšå¼ˆè®ºåº”ç”¨äºå®‰å…¨é¢†åŸŸï¼Œä¾‹å¦‚æœºåœºå®‰å…¨æ£€æŸ¥å’Œå·¡é€»è·¯çº¿è§„åˆ’ã€‚é€šè¿‡ä½¿ç”¨è´å¶æ–¯Stackelbergåšå¼ˆï¼Œä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†é…èµ„æºï¼Œä»¥ä¿æŠ¤å…³é”®åŸºç¡€è®¾æ–½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šè¯´æœæ¸¸æˆ
æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨è‡ªåŠ¨åŒ–ä»£ç†åœ¨è¯´æœæ¸¸æˆä¸­å½±å“äººç±»çš„è¡Œä¸ºã€‚é€šè¿‡ç»“åˆåšå¼ˆè®ºå’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä»£ç†èƒ½å¤Ÿé¢„æµ‹äººç±»å¯¹æ‰‹çš„è¡Œä¸ºï¼Œå¹¶æ®æ­¤è°ƒæ•´å…¶ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å¤§é‡çš„å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„è‡ªåŠ¨åŒ–ä»£ç†çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆç†è®ºã€å¯å‘å¼æ–¹æ³•å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„ä»£ç†èƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸‹ä¸äººç±»æœ‰æ•ˆäº’åŠ¨ï¼Œå¹¶è·å¾—æ›´é«˜çš„æ•ˆç”¨åˆ†æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è®¾è®¡è‡ªåŠ¨åŒ–ä»£ç†æ—¶ï¼Œéœ€è¦è€ƒè™‘äººç±»è¡Œä¸ºçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç»“åˆç†è®ºã€å¯å‘å¼æ–¹æ³•å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä»¥å¼€å‘å‡ºèƒ½å¤Ÿä¸äººç±»æœ‰æ•ˆäº’åŠ¨çš„ä»£ç†ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸‹å–å¾—è‰¯å¥½çš„æ•ˆæœã€‚

## calypso--llms-as-dungeon-masters--assistants
### Abstract
The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to
perform multiple tasks simultaneously. The DM must digest information about the
game setting and monsters, synthesize scenes to present to other players, and
respond to the players' interactions with the scene. Doing all of these tasks
while maintaining consistency within the narrative and story world is no small
feat of human cognition, making the task tiring and unapproachable to new
players. Large language models (LLMs) like GPT-3 and ChatGPT have shown
remarkable abilities to generate coherent natural language text. In this paper,
we conduct a formative evaluation with DMs to establish the use cases of LLMs
in D&D and tabletop gaming generally. We introduce CALYPSO, a system of
LLM-powered interfaces that support DMs with information and inspiration
specific to their own scenario. CALYPSO distills game context into bite-sized
prose and helps brainstorm ideas without distracting the DM from the game. When
given access to CALYPSO, DMs reported that it generated high-fidelity text
suitable for direct presentation to players, and low-fidelity ideas that the DM
could develop further while maintaining their creative agency. We see CALYPSO
as exemplifying a paradigm of AI-augmented tools that provide synchronous
creative assistance within established game worlds, and tabletop gaming more
broadly.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CALYPSOï¼šå¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›åœ°ä¸‹åŸä¸»

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ°ä¸‹åŸä¸é¾™ï¼ˆD&Dï¼‰æ˜¯ä¸€æ¬¾ç»å…¸çš„æ¡Œé¢è§’è‰²æ‰®æ¼”æ¸¸æˆï¼Œå…¶ä¸­åœ°ä¸‹åŸä¸»ï¼ˆDMï¼‰æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚DMéœ€è¦åŒæ—¶å¤„ç†å¤šé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¶ˆåŒ–æ¸¸æˆèƒŒæ™¯å’Œæ€ªç‰©ä¿¡æ¯ã€æ„å»ºåœºæ™¯ã€å›åº”ç©å®¶äº’åŠ¨ç­‰ã€‚è¿™äº›ä»»åŠ¡å¯¹äººç±»è®¤çŸ¥èƒ½åŠ›è¦æ±‚æé«˜ï¼Œå¯¹äºæ–°ç©å®¶æ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-3å’ŒChatGPTåœ¨ç”Ÿæˆè¿è´¯çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMåœ¨D&Då’Œæ¡Œé¢æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†CALYPSOç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨LLMä¸ºDMæä¾›ä¿¡æ¯å’Œæ”¯æŒï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°è¿›è¡Œæ¸¸æˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCALYPSOç³»ç»Ÿ
CALYPSOæ˜¯ä¸€ä¸ªç”±LLMé©±åŠ¨çš„ç•Œé¢ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¯æŒDMåœ¨æ¸¸æˆä¸­è·å–ä¿¡æ¯å’Œçµæ„Ÿã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ï¼š
1. **é­é‡ç†è§£**ï¼šä½¿ç”¨GPT-3å¯¹æ¸¸æˆèƒŒæ™¯å’Œæ€ªç‰©ä¿¡æ¯è¿›è¡Œæ‘˜è¦ï¼Œå¸®åŠ©DMå¿«é€Ÿç†è§£é­é‡ã€‚
2. **èšç„¦å¤´è„‘é£æš´**ï¼šä½¿ç”¨ChatGPTä¸DMè¿›è¡Œå¯¹è¯ï¼Œå¸®åŠ©ä»–ä»¬è¿›ä¸€æ­¥æ¢ç´¢é­é‡ç»†èŠ‚æˆ–ç”Ÿæˆæ–°çš„æƒ³æ³•ã€‚
3. **å¼€æ”¾åŸŸèŠå¤©åŸºçº¿**ï¼šä½¿ç”¨ChatGPTæä¾›ä¸€ä¸ªå¼€æ”¾åŸŸçš„èŠå¤©ç•Œé¢ï¼Œä¾›ç©å®¶å’ŒDMè¿›è¡Œéæ¸¸æˆç›¸å…³çš„äº¤æµã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMçš„åˆ›é€ æ€§è¾…åŠ©
CALYPSOç³»ç»Ÿå±•ç¤ºäº†LLMä½œä¸ºåˆ›é€ æ€§è¾…åŠ©å·¥å…·çš„æ½œåŠ›ã€‚å®ƒä¸ä»…èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„æ–‡æœ¬ï¼Œé€‚åˆç›´æ¥å‘ˆç°ç»™ç©å®¶ï¼Œè¿˜èƒ½å¤Ÿæä¾›ä½ä¿çœŸåº¦çš„æƒ³æ³•ï¼Œä¾›DMè¿›ä¸€æ­¥å‘å±•å’Œå®Œå–„ã€‚è¿™ç§è¾…åŠ©æ–¹å¼ä¿ç•™äº†DMçš„åˆ›é€ æ€§è‡ªä¸»æƒï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¸“æ³¨äºæ¸¸æˆä¸­çš„è®¤çŸ¥ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDMåœ¨ä½¿ç”¨CALYPSOç³»ç»Ÿåï¼Œæ™®éè®¤ä¸ºå®ƒèƒ½å¤Ÿç”Ÿæˆé€‚åˆç›´æ¥å‘ˆç°ç»™ç©å®¶çš„æ–‡æœ¬ï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„çµæ„Ÿã€‚DMä»¬åˆ©ç”¨CALYPSOç³»ç»Ÿæ¥ç†è§£å¤æ‚çš„æ€ªç‰©ä¿¡æ¯ã€å¤´è„‘é£æš´éç©å®¶è§’è‰²æˆ–æ€ªç‰©ä¹‹é—´çš„äº’åŠ¨ï¼Œå¹¶è·å–å»ºè®®ï¼Œå°†è¿™äº›å»ºè®®èå…¥åˆ°æ•…äº‹ä¸­å‘ˆç°ç»™ç©å®¶ï¼Œè€Œä¸ä¼šå½±å“æ¸¸æˆçš„èŠ‚å¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºå¼€å‘AIè¾…åŠ©å·¥å…·åœ¨æ¡Œé¢æ¸¸æˆå’Œå…¶ä»–åˆ›æ„é¢†åŸŸä¸­çš„åº”ç”¨å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚CALYPSOç³»ç»Ÿçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–ç±»ä¼¼é¡¹ç›®æä¾›å‚è€ƒï¼Œä¾‹å¦‚ï¼š
1. **ç†è§£ç”¨æˆ·éœ€æ±‚**ï¼šé€šè¿‡è®¿è°ˆå’Œç”¨æˆ·ç ”ç©¶ï¼Œæ·±å…¥äº†è§£ç”¨æˆ·çš„éœ€æ±‚å’Œç—›ç‚¹ï¼Œä»è€Œè®¾è®¡å‡ºæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„AIè¾…åŠ©å·¥å…·ã€‚
2. **åˆ©ç”¨LLMçš„åˆ›é€ æ€§æ½œåŠ›**ï¼šLLMåœ¨ç”Ÿæˆè¿è´¯æ–‡æœ¬å’Œæä¾›åˆ›é€ æ€§çµæ„Ÿæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥å°†å…¶åº”ç”¨äºå„ç§åˆ›æ„åœºæ™¯ã€‚
3. **ä¿æŒç”¨æˆ·çš„åˆ›é€ æ€§è‡ªä¸»æƒ**ï¼šAIè¾…åŠ©å·¥å…·åº”è¯¥ä½œä¸ºç”¨æˆ·çš„åŠ©æ‰‹ï¼Œè€Œä¸æ˜¯æ›¿ä»£è€…ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°å‘æŒ¥è‡ªå·±çš„åˆ›é€ åŠ›ã€‚

æ€»è€Œè¨€ä¹‹ï¼ŒCALYPSOç³»ç»Ÿå±•ç¤ºäº†LLMåœ¨æ¡Œé¢æ¸¸æˆä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶ä¸ºå¼€å‘AIè¾…åŠ©å·¥å…·æä¾›äº†æœ‰ä»·å€¼çš„ç»éªŒå’Œå¯ç¤ºã€‚

## rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models
### Abstract
The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
### ğŸŒŸ è®ºæ–‡è§£è¯» | RoleLLMï¼šè§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œè§’è‰²æ‰®æ¼”ç­‰å¤æ‚ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œä¸ºç”¨æˆ·äº¤äº’æä¾›äº†æ›´å¤šå¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰å¼€æºLLMsåœ¨è§’è‰²æ‰®æ¼”æ–¹é¢å­˜åœ¨ä¼˜åŒ–ä¸è¶³çš„é—®é¢˜ï¼Œè€Œæœ€å…ˆè¿›çš„LLMså¦‚GPT-4ç­‰åˆ™å› å…¶é—­æºæ€§è´¨è€Œé™åˆ¶äº†å…¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºRoleLLMæ¡†æ¶ï¼Œä»¥æå‡LLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šRole Profile Construction
æ„å»ºäº†100ä¸ªè§’è‰²çš„è¯¦ç»†æ¡£æ¡ˆï¼ŒåŒ…æ‹¬è§’è‰²æè¿°ã€å£å¤´ç¦…ä»¥åŠä»å‰§æœ¬ä¸­æå–çš„å¯¹è¯ç‰‡æ®µï¼Œä¸ºè§’è‰²æ‰®æ¼”æä¾›äº†ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šContext-Based Instruction Generation (Context-Instruct)
åˆ©ç”¨GPTæ¨¡å‹ä»è§’è‰²æ¡£æ¡ˆä¸­ç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œä»¥æå–è§’è‰²ç‰¹å®šçš„çŸ¥è¯†å’Œè®°å¿†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRole Prompting using GPT (RoleGPT)
é€šè¿‡å¯¹è¯å·¥ç¨‹å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåˆ©ç”¨GPTæ¨¡å‹ç”Ÿæˆç¬¦åˆè§’è‰²è¯´è¯é£æ ¼çš„å›ç­”ï¼Œä»¥æ¨¡ä»¿è§’è‰²çš„è¯´è¯é£æ ¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šRole-Conditioned Instruction Tuning (RoCIT)
åˆ©ç”¨Context-Instructå’ŒRoleGPTç”Ÿæˆçš„æ•°æ®ï¼Œå¯¹å¼€æºLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡å…¶è§’è‰²æ‰®æ¼”èƒ½åŠ›ï¼Œå¹¶ç”ŸæˆRoleLLaMAå’ŒRoleGLMç­‰æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒRoleLLMæ¡†æ¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚RoleLLaMAå’ŒRoleGLMåœ¨æ¨¡ä»¿è§’è‰²è¯´è¯é£æ ¼ã€å›ç­”å‡†ç¡®æ€§å’Œè§’è‰²ç‰¹å®šçŸ¥è¯†æŒæ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¸RoleGPTï¼ˆä½¿ç”¨GPT-4ï¼‰ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
RoleLLMæ¡†æ¶ä¸ºLLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬è§’è‰²æ¡£æ¡ˆæ„å»ºã€åŸºäºä¸Šä¸‹æ–‡çš„æŒ‡ä»¤ç”Ÿæˆã€è§’è‰²æç¤ºå’Œè§’è‰²æ¡ä»¶æŒ‡ä»¤å¾®è°ƒç­‰ã€‚æ­¤å¤–ï¼ŒRoleBenchæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè§’è‰²æ‰®æ¼”èƒ½åŠ›çš„è¯„ä¼°å’Œæå‡æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## personallm--investigating-the-ability-of-large-language-models-to-express-personality-traits
### Abstract
Despite the many use cases for large language models (LLMs) in creating
personalized chatbots, there has been limited research on evaluating the extent
to which the behaviors of personalized LLMs accurately and consistently reflect
specific personality traits. We consider studying the behavior of LLM-based
agents which we refer to as LLM personas and present a case study with GPT-3.5
and GPT-4 to investigate whether LLMs can generate content that aligns with
their assigned personality profiles. To this end, we simulate distinct LLM
personas based on the Big Five personality model, have them complete the
44-item Big Five Inventory (BFI) personality test and a story writing task, and
then assess their essays with automatic and human evaluations. Results show
that LLM personas' self-reported BFI scores are consistent with their
designated personality types, with large effect sizes observed across five
traits. Additionally, LLM personas' writings have emerging representative
linguistic patterns for personality traits when compared with a human writing
corpus. Furthermore, human evaluation shows that humans can perceive some
personality traits with an accuracy of up to 80%. Interestingly, the accuracy
drops significantly when the annotators were informed of AI authorship.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PersonaLLMï¼šæ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹è¡¨è¾¾äººæ ¼ç‰¹è´¨çš„èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ›å»ºä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººæ–¹é¢çš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹äºä¸ªæ€§åŒ–LLMsçš„è¡Œä¸ºæ˜¯å¦èƒ½å¤Ÿå‡†ç¡®ä¸”ä¸€è‡´åœ°åæ˜ ç‰¹å®šäººæ ¼ç‰¹è´¨çš„ç ”ç©¶å´ç›¸å¯¹æœ‰é™ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶LLMsæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆä¸æŒ‡å®šäººæ ¼ç‰¹å¾ç›¸ç¬¦çš„å†…å®¹ï¼Œå¹¶è¯„ä¼°å…¶è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºå¤§äº”äººæ ¼æ¨¡å‹æ¨¡æ‹ŸLLMäººæ ¼
æœ¬æ–‡é€šè¿‡å¤§äº”äººæ ¼æ¨¡å‹ï¼ˆåŒ…æ‹¬å¤–å‘æ€§ã€å®œäººæ€§ã€è´£ä»»å¿ƒã€ç¥ç»è´¨å’Œå¼€æ”¾æ€§ï¼‰æ¨¡æ‹Ÿäº†ä¸åŒçš„LLMäººæ ¼ï¼Œå¹¶è®©è¿™äº›LLMäººæ ¼å®Œæˆå¤§äº”äººæ ¼é‡è¡¨ï¼ˆBFIï¼‰æµ‹è¯•å’Œæ•…äº‹å†™ä½œä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè§’åº¦è¯„ä¼°LLMäººæ ¼
æœ¬æ–‡é‡‡ç”¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ç›¸ç»“åˆçš„æ–¹å¼ï¼Œå¯¹LLMäººæ ¼ç”Ÿæˆçš„æ•…äº‹è¿›è¡Œè¯„ä¼°ã€‚è¯„ä¼°ç»´åº¦åŒ…æ‹¬å¯è¯»æ€§ã€ä¸ªäººæ€§ã€å†—ä½™æ€§ã€è¿è´¯æ€§ã€å–œçˆ±åº¦å’Œå¯ä¿¡åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è¦æ±‚è¯„ä¼°è€…æ ¹æ®æ•…äº‹æ¨æ–­ä½œè€…çš„äººæ ¼ç‰¹è´¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMäººæ ¼çš„è‡ªæˆ‘æŠ¥å‘ŠBFIå¾—åˆ†ä¸å…¶æŒ‡å®šçš„äººæ ¼ç±»å‹ä¸€è‡´ï¼Œä¸”åœ¨äº”ä¸ªç‰¹è´¨ä¸Šè§‚å¯Ÿåˆ°è¾ƒå¤§çš„æ•ˆåº”é‡ã€‚LLMäººæ ¼çš„å†™ä½œä¸­å‡ºç°äº†ä¸äººæ ¼ç‰¹è´¨ç›¸å…³çš„ä»£è¡¨æ€§è¯­è¨€æ¨¡å¼ï¼Œä¸äººç±»å†™ä½œè¯­æ–™åº“ç›¸æ¯”å…·æœ‰ä¸€è‡´æ€§ã€‚äººå·¥è¯„ä¼°æ˜¾ç¤ºï¼Œäººç±»å¯ä»¥ä»¥é«˜è¾¾80%çš„å‡†ç¡®ç‡æ„ŸçŸ¥åˆ°æŸäº›äººæ ¼ç‰¹è´¨ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå½“è¯„ä¼°è€…è¢«å‘ŠçŸ¥AIä½œè€…èº«ä»½æ—¶ï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è¡¨è¾¾äººæ ¼ç‰¹è´¨æ–¹é¢å…·æœ‰ä¸€å®šçš„èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ä¾‹å¦‚ï¼ŒLLMsåœ¨ç”Ÿæˆæ•…äº‹æ—¶å¯èƒ½ä¼šé‡å¤ç›¸ä¼¼çš„å†…å®¹ï¼Œä¸”åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å¯èƒ½å­˜åœ¨ä¸»è§‚æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢LLMsåœ¨æ›´è‡ªç„¶isticçš„åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¹¶è€ƒè™‘æ›´å¤šæ ·åŒ–çš„è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯ä»¥æ·±å…¥æ¢è®¨äººç±»è¯„ä¼°è€…çš„äººæ ¼å’ŒèƒŒæ™¯å¯¹å…¶äººæ ¼é¢„æµ‹å‡†ç¡®ç‡çš„å½±å“ï¼Œä»¥åŠAIä½œè€…èº«ä»½å¯¹äººç±»æ„ŸçŸ¥çš„å½±å“ã€‚

## chatharuhi--reviving-anime-character-in-reality-via-large-language-model
### Abstract
Role-playing chatbots built on large language models have drawn interest, but
better techniques are needed to enable mimicking specific fictional characters.
We propose an algorithm that controls language models via an improved prompt
and memories of the character extracted from scripts. We construct ChatHaruhi,
a dataset covering 32 Chinese / English TV / anime characters with over 54k
simulated dialogues. Both automatic and human evaluations show our approach
improves role-playing ability over baselines. Code and data are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya .
### ğŸŒŸ è®ºæ–‡è§£è¯» | ChatHaruhiï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸­å¤æ´»åŠ¨æ¼«è§’è‰²

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å¸ƒï¼ŒåŸºäºè¿™äº›æ¨¡å‹çš„æ‰®æ¼”å¼èŠå¤©æœºå™¨äººå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰®æ¼”å¼èŠå¤©æœºå™¨äººå­˜åœ¨ä¸€äº›é—®é¢˜ï¼š1. è¿‡åº¦ä¾èµ–è¯­è¨€æ¨¡å‹è‡ªèº«çš„è®°å¿†ï¼Œå¦‚æœæ¨¡å‹å¯¹ä½œå“çš„ç†è§£æ¨¡ç³Šï¼Œåˆ™éš¾ä»¥å¾ˆå¥½åœ°æ¨¡ä»¿ç‰¹å®šè§’è‰²ï¼›2. â€œçŸ¥é“æ‰€æœ‰å…³äºè§’è‰²çš„çŸ¥è¯†â€å®šä¹‰æ¨¡ç³Šï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼›3. å³ä½¿æœ‰æ‰®æ¼”æç¤ºï¼ŒèŠå¤©æœºå™¨äººçš„å¯¹è¯é£æ ¼ä»ç„¶å—åˆ°åº•å±‚è¯­è¨€æ¨¡å‹çš„å½±å“ï¼Œéœ€è¦ä¸ºæ¯ä¸ªè§’è‰²ç²¾ç»†è°ƒæ•´æç¤ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®Œæ•´æ‰®æ¼”å¼èŠå¤©æœºå™¨äººç®—æ³•ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»¥æœ‰æ•ˆåœ°ç»„ç»‡è§’è‰²çš„è®°å¿†ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¯¹è¯ä¸­æ¨¡ä»¿ç‰¹å®šåŠ¨æ¼«/ç”µè§†è§’è‰²çš„è¯­æ°”å’ŒçŸ¥è¯†ã€‚ä¸»è¦åˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†åŒ…å«32ä¸ªä¸åŒä¸­è‹±åŠ¨æ¼«/ç”µè§†è§’è‰²çš„æ‰®æ¼”å¼æ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡54,000ä¸ªæ¨¡æ‹Ÿå¯¹è¯ã€‚é€šè¿‡æ”¶é›†å’Œç»“æ„åŒ–æå–ç”µå½±ã€å°è¯´ã€å‰§æœ¬ä¸­çš„å¯¹è¯ï¼Œæ”¶é›†äº†è¶…è¿‡22,000ä¸ªå¯¹è¯äº¤æ¢ï¼Œå¯ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ‰®æ¼”å¼è¯­è¨€æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†è‡ªåŠ¨ç”Ÿæˆå¯¹è¯çš„ç³»ç»Ÿï¼Œå³ä½¿å¯¹äºå¯¹è¯è¾ƒå°‘çš„è§’è‰²ï¼Œä¹Ÿèƒ½ç”Ÿæˆç¬¦åˆè§’è‰²ä¸ªæ€§çš„å¯¹è¯ã€‚è¿™å…è®¸æˆ‘ä»¬ä¸ºå¾®è°ƒæœ¬åœ°æ¨¡å‹ç”Ÿæˆè¶³å¤Ÿçš„æ•°æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä½¿ç”¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°æ¥è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒçš„æ‰®æ¼”å¼èŠå¤©æœºå™¨äººã€‚è‡ªåŠ¨è¯„ä¼°æµ‹è¯•èŠå¤©æœºå™¨äººæ˜¯å¦èƒ½å¤Ÿå¯¹ç»å…¸æƒ…èŠ‚ç‚¹åšå‡ºä¸åŸå§‹å‰§æœ¬ç›¸ä¼¼çš„å›ç­”ã€‚äººå·¥è¯„ä¼°æå‡ºäº†ä¸¤ä¸ªæŒ‡æ ‡ä¾›è¯„ä¼°è€…è¯„ä¼°ï¼šä¸€è‡´æ€§ï¼ˆèŠå¤©æœºå™¨äººçš„å›ç­”æ˜¯å¦ä¸è§’è‰²çš„åŸå§‹è®¾å®šä¸€è‡´ï¼‰å’Œå“åº”è´¨é‡ï¼ˆèŠå¤©æœºå™¨äººçš„å›ç­”æ˜¯å¦å…·æœ‰è‰¯å¥½çš„è¯­è¨€è´¨é‡ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„åº•å±‚è¯­è¨€æ¨¡å‹ä¸‹ï¼Œæœ¬æ–‡æå‡ºçš„ç®—æ³•åœ¨æ‰®æ¼”å¼æ€§èƒ½æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ‰®æ¼”å¼èŠå¤©æœºå™¨äººç®—æ³•ç³»ç»Ÿå¯ä»¥åº”ç”¨äºæ¸¸æˆã€åˆ›æ„äº§ä¸šç­‰é¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ ä¸°å¯Œçš„äº’åŠ¨ä½“éªŒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„æ‰®æ¼”å¼æ•°æ®é›†å’Œè‡ªåŠ¨ç”Ÿæˆå¯¹è¯çš„ç³»ç»Ÿä¹Ÿä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## lamp--when-large-language-models-meet-personalization
### Abstract
This paper highlights the importance of personalization in large language
models and introduces the LaMP benchmark -- a novel benchmark for training and
evaluating language models for producing personalized outputs. LaMP offers a
comprehensive evaluation framework with diverse language tasks and multiple
entries for each user profile. It consists of seven personalized tasks,
spanning three text classification and four text generation tasks. We
additionally propose two retrieval augmentation approaches that retrieve
personal items from each user profile for personalizing language model outputs.
To this aim, we study various retrieval models, including term matching,
semantic matching, and time-aware methods. Extensive experiments on LaMP for
zero-shot and fine-tuned language models demonstrate the efficacy of the
proposed retrieval augmentation approach and highlight the impact of
personalization in various natural language tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LaMPï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–è¾“å‡ºæ–°åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä¸ªæ€§åŒ–è¾“å‡ºæˆä¸ºæ»¡è¶³ç”¨æˆ·ç‹¬ç‰¹éœ€æ±‚å’Œåå¥½çš„å…³é”®å› ç´ ã€‚ç„¶è€Œï¼Œç°æœ‰çš„NLPåŸºå‡†æµ‹è¯•å¾€å¾€é‡‡ç”¨â€œä¸€åˆ€åˆ‡â€çš„æ–¹æ³•ï¼Œç¼ºä¹å¯¹ä¸ªæ€§åŒ–éœ€æ±‚çš„è€ƒè™‘ï¼Œé™åˆ¶äº†ä¸ªæ€§åŒ–ç ”ç©¶çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºäº†LaMPåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLaMPåŸºå‡†
LaMPåŸºå‡†åŒ…å«ä¸ƒä¸ªä¸ªæ€§åŒ–ä»»åŠ¡ï¼Œæ¶µç›–ä¸‰ä¸ªæ–‡æœ¬åˆ†ç±»å’Œå››ä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶æä¾›ä¸¤ç§æ•°æ®åˆ†å‰²æ–¹å¼ï¼ˆåŸºäºç”¨æˆ·å’ŒåŸºäºæ—¶é—´ï¼‰ä»¥é€‚åº”ä¸åŒçš„ä¸ªæ€§åŒ–åœºæ™¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ£€ç´¢å¢å¼ºæ–¹æ³•
ä¸ºäº†è§£å†³ç”¨æˆ·é…ç½®æ–‡ä»¶é•¿åº¦é™åˆ¶é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§æ£€ç´¢å¢å¼ºæ–¹æ³•ï¼šæç¤ºå†…å¢å¼ºï¼ˆIPAï¼‰å’Œèåˆè§£ç å™¨ï¼ˆFiDï¼‰ã€‚è¿™äº›æ–¹æ³•ä»ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºä¸ªæ€§åŒ–LLMè¾“å‡ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæ£€ç´¢å¢å¼ºæ–¹æ³•åœ¨LaMPåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡12.2%ï¼Œè€Œåœ¨å¾®è°ƒè®¾ç½®ä¸‹ï¼Œå¹³å‡æ€§èƒ½æå‡23.5%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LaMPåŸºå‡†ä¸ºä¸ªæ€§åŒ–NLPæ¨¡å‹çš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„ç¯å¢ƒå’Œå·¥å…·ã€‚æ£€ç´¢å¢å¼ºæ–¹æ³•ä¸ºè§£å†³ç”¨æˆ·é…ç½®æ–‡ä»¶é•¿åº¦é™åˆ¶é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†å‡ ä¸ªå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶çš„é—®é¢˜ï¼Œä¾‹å¦‚ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡å’Œéšç§ä¿æŠ¤ç­‰ã€‚

## camel--communicative-agents-for--mind--exploration-of-large-language-model-society
### Abstract
The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their "cognitive" processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: https://github.com/camel-ai/camel.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CAMELï¼šæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ç¤¾ä¼šçš„â€œå¿ƒæ™ºâ€äº¤æµ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€åŸºäºèŠå¤©çš„è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡è§£å†³æ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„æˆåŠŸä¸¥é‡ä¾èµ–äºäººç±»è¾“å…¥æ¥å¼•å¯¼å¯¹è¯ï¼Œè¿™å¯èƒ½ä¼šå…·æœ‰æŒ‘æˆ˜æ€§ä¸”è€—æ—¶ã€‚æœ¬æ–‡æ¢è®¨äº†æ„å»ºå¯æ‰©å±•æŠ€æœ¯ä»¥ä¿ƒè¿›äº¤æµä»£ç†ä¹‹é—´çš„è‡ªä¸»åˆä½œï¼Œå¹¶æ·±å…¥äº†è§£å…¶â€œè®¤çŸ¥â€è¿‡ç¨‹çš„æ½œåŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§’è‰²æ‰®æ¼”æ¡†æ¶
ä¸ºäº†è§£å†³å®ç°è‡ªä¸»åˆä½œçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè§’è‰²æ‰®æ¼”â€çš„æ–°å‹äº¤æµä»£ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¶‰åŠä½¿ç”¨â€œèµ·å§‹æç¤ºâ€æ¥å¼•å¯¼èŠå¤©ä»£ç†å®Œæˆä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒä¸äººç±»æ„å›¾çš„ä¸€è‡´æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèµ·å§‹æç¤º
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œèµ·å§‹æç¤ºâ€çš„å¯¹è¯LLMè‡ªåŠ¨æç¤ºæ–¹æ³•ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡è§’è‰²æ‰®æ¼”ç›¸äº’æç¤ºä»¥è§£å†³é—®é¢˜ã€‚AIç”¨æˆ·ä¸æ–­å‘AIåŠ©æ‰‹æä¾›æŒ‡ä»¤ä»¥è§£å†³é—®é¢˜ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿å­˜æŒ‡ä»¤-è§£å†³æ–¹æ¡ˆå¯¹å¹¶åˆ›å»ºå¤šæ ·åŒ–ã€æŒ‡ä»¤æ€§ã€å¯¹è¯æ€§å’Œé¢å‘ä»»åŠ¡çš„è¯­æ–™åº“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é›†ç”Ÿæˆ
æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è§’è‰²æ‰®æ¼”æ¥è®©èŠå¤©ä»£ç†ç›¸äº’äº¤æµä»¥å®Œæˆä»»åŠ¡ï¼Œå¹¶è®°å½•ä»–ä»¬çš„å¯¹è¯ä»¥è¿›è¡Œè¡Œä¸ºåˆ†æå’Œèƒ½åŠ›ç†è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹å¤šä»£ç†è®¾ç½®ä¸­çš„æŒ‡ä»¤éµå¾ªåˆä½œè¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼€æºåº“
æœ¬æ–‡å¼€æºäº†æˆ‘ä»¬çš„åº“ï¼Œå…¶ä¸­åŒ…å«å„ç§ä»£ç†çš„å®ç°ã€æ•°æ®ç”Ÿæˆç®¡é“ã€æ•°æ®åˆ†æå·¥å…·å’Œæ”¶é›†çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒå¯¹äº¤æµä»£ç†çš„ç ”ç©¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å®éªŒè¯„ä¼°äº†CAMELæ¡†æ¶çš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜CAMELè§£å†³æ–¹æ¡ˆåœ¨äººç±»è¯„ä¼°å’ŒGPT4è¯„ä¼°ä¸­å‡ä¼˜äºgpt-3.5-turboå•æ¬¡è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ç ”ç©¶äº†LLMè®­ç»ƒèƒ½åŠ›çš„æ˜¾è‘—å‡ºç°ï¼Œé€šè¿‡åœ¨é€šè¿‡æ¡†æ¶ç”Ÿæˆçš„ä¸æ–­å¢é•¿çš„è¯­æ–™åº“ä¸Šå¾®è°ƒLLaMAæ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„CAMELæ¡†æ¶ä¸ºç ”ç©¶äº¤æµä»£ç†ä¹‹é—´çš„è‡ªä¸»åˆä½œæä¾›äº†å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¹¶æä¾›äº†åº”å¯¹æŒ‘æˆ˜çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼€æºçš„åº“ä¸ºç ”ç©¶äº¤æµä»£ç†å’Œæ›´å¹¿æ³›çš„ç ”ç©¶é¢†åŸŸæä¾›äº†å®è´µçš„èµ„æºã€‚

## an-appraisal-based-chain-of-emotion-architecture-for-affective-language-model-game-agents
### Abstract
The development of believable, natural, and interactive digital artificial
agents is a field of growing interest. Theoretical uncertainties and technical
barriers present considerable challenges to the field, particularly with
regards to developing agents that effectively simulate human emotions. Large
language models (LLMs) might address these issues by tapping common patterns in
situational appraisal. In three empirical experiments, this study tests the
capabilities of LLMs to solve emotional intelligence tasks and to simulate
emotions. It presents and evaluates a new chain-of-emotion architecture for
emotion simulation within video games, based on psychological appraisal
research. Results show that it outperforms standard LLM architectures on a
range of user experience and content analysis metrics. This study therefore
provides early evidence of how to construct and test affective agents based on
cognitive processes represented in language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºè¯„ä¼°çš„æƒ…æ„Ÿé“¾æ¶æ„ï¼šè®©æ¸¸æˆä¸­çš„è¯­è¨€æ¨¡å‹ä»£ç†æ›´å…·æƒ…æ„Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œæ„å»ºå¯ä¿¡ã€è‡ªç„¶å’Œäº¤äº’å¼çš„æ•°å­—äººå·¥ä»£ç†æˆä¸ºäº†ä¸€ä¸ªæ—¥ç›Šå¢é•¿çš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€Œï¼Œæ¨¡æ‹Ÿäººç±»æƒ…æ„Ÿä¸€ç›´æ˜¯è¿™ä¸€é¢†åŸŸçš„éš¾é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯èƒ½é€šè¿‡åˆ©ç”¨æƒ…å¢ƒè¯„ä¼°ä¸­çš„å¸¸è§æ¨¡å¼æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨è§£å†³æƒ…æ„Ÿæ™ºèƒ½ä»»åŠ¡å’Œæ¨¡æ‹Ÿæƒ…æ„Ÿæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºäºæƒ…æ„Ÿé“¾æ¶æ„çš„æ¸¸æˆæƒ…æ„Ÿæ¨¡æ‹Ÿæ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯„ä¼°æç¤ºç­–ç•¥
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯„ä¼°çš„æƒ…æ„Ÿé“¾æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨æƒ…å¢ƒä¿¡æ¯å’Œè§’è‰²ç‰¹å¾æ¥è¯„ä¼°å½“å‰æƒ…å†µï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æƒ…æ„Ÿååº”ã€‚é€šè¿‡å°†è¯„ä¼°è¿‡ç¨‹ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿäººç±»æƒ…æ„Ÿã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæƒ…æ„Ÿé“¾æ¶æ„
è¯¥æ¶æ„åŒ…æ‹¬ä¸€ä¸ªè®°å¿†ç³»ç»Ÿï¼Œç”¨äºå­˜å‚¨è§‚å¯Ÿç»“æœå’Œæƒ…æ„Ÿååº”ï¼Œä»¥åŠä¸€ä¸ªè¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºå°†è§‚å¯Ÿç»“æœè½¬æ¢ä¸ºæƒ…æ„Ÿååº”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªæƒ…æ„Ÿé“¾ï¼Œç”¨äºç”Ÿæˆä»£ç†çš„è¡Œä¸ºå’Œå¯¹è¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè¯„ä¼°çš„æƒ…æ„Ÿé“¾æ¶æ„åœ¨ç”¨æˆ·ä½“éªŒå’Œå†…å®¹åˆ†ææŒ‡æ ‡æ–¹é¢ä¼˜äºæ ‡å‡†çš„LLMæ¶æ„ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨ä»£ç†å¯ä¿¡åº¦ã€ååº”æ€§å’Œæƒ…æ„Ÿæ™ºåŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¨¡æ‹Ÿæƒ…æ„Ÿæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨è¯„ä¼°æç¤ºç­–ç•¥å’Œæƒ…æ„Ÿé“¾æ¶æ„ï¼Œå¯ä»¥æ„å»ºæ›´å…·æƒ…æ„Ÿçš„æ¸¸æˆä»£ç†ï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œã€æ›´è‡ªç„¶çš„ç”¨æˆ·ä½“éªŒã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è™šæ‹ŸåŠ©æ‰‹å’ŒèŠå¤©æœºå™¨äººï¼Œä»¥æ¨¡æ‹Ÿæ›´çœŸå®çš„æƒ…æ„Ÿååº”ã€‚

## word-sense-disambiguation--a-survey
### Abstract
In this paper, we made a survey on Word Sense Disambiguation (WSD). Near
about in all major languages around the world, research in WSD has been
conducted upto different extents. In this paper, we have gone through a survey
regarding the different approaches adopted in different research works, the
State of the Art in the performance in this domain, recent works in different
Indian languages and finally a survey in Bengali language. We have made a
survey on different competitions in this field and the bench mark results,
obtained from those competitions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯ä¹‰æ¶ˆæ­§ï¼šæ¢ç´¢è‡ªç„¶è¯­è¨€å¤„ç†çš„å‰æ²¿æŠ€æœ¯

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯ä¹‰æ¶ˆæ­§ï¼ˆWord Sense Disambiguation, WSDï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ç”±äºè®¸å¤šå•è¯åœ¨ä¸åŒçš„è¯­å¢ƒä¸­å…·æœ‰ä¸åŒçš„å«ä¹‰ï¼Œå› æ­¤æœºå™¨åœ¨ç†è§£å’Œå¤„ç†è‡ªç„¶è¯­è¨€æ—¶å¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚æœ¬æ–‡æ—¨åœ¨å¯¹è¯ä¹‰æ¶ˆæ­§æŠ€æœ¯è¿›è¡Œå…¨é¢çš„è°ƒæŸ¥ï¼Œæ¢è®¨ä¸åŒç ”ç©¶å·¥ä½œä¸­é‡‡ç”¨çš„ä¸åŒæ–¹æ³•ï¼Œä»¥åŠè¯¥é¢†åŸŸåœ¨æ€§èƒ½æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸‰ç§è¯ä¹‰æ¶ˆæ­§æ–¹æ³•ï¼š

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºçŸ¥è¯†çš„æ–¹æ³•
è¿™ç§æ–¹æ³•ä¾èµ–äºæœºå™¨å¯è¯»è¯å…¸ã€è¯ä¹‰åº“ã€åŒä¹‰è¯åº“ç­‰çŸ¥è¯†æºã€‚ä¾‹å¦‚ï¼ŒLeskç®—æ³•é€šè¿‡æ¯”è¾ƒå¥å­ä¸­å•è¯çš„è¯å…¸å®šä¹‰æ¥ç¡®å®šè¯ä¹‰ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦ã€é€‰æ‹©åå¥½å’Œå¯å‘å¼æ–¹æ³•ä¹Ÿè¢«ç”¨äºåŸºäºçŸ¥è¯†çš„æ–¹æ³•ä¸­ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç›‘ç£å­¦ä¹ æ–¹æ³•
ç›‘ç£å­¦ä¹ æ–¹æ³•ä½¿ç”¨æ‰‹åŠ¨åˆ›å»ºçš„è¯ä¹‰æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå†³ç­–åˆ—è¡¨ã€å†³ç­–æ ‘ã€æœ´ç´ è´å¶æ–¯ã€ç¥ç»ç½‘ç»œã€åŸºäºå®ä¾‹çš„å­¦ä¹ å’Œæ”¯æŒå‘é‡æœºç­‰æ–¹æ³•éƒ½è¢«ç”¨äºç›‘ç£å­¦ä¹ æ–¹æ³•ä¸­ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ— ç›‘ç£å­¦ä¹ æ–¹æ³•
æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸ä¾èµ–äºå¤–éƒ¨çŸ¥è¯†æºæˆ–è¯ä¹‰åº“ã€‚ä¾‹å¦‚ï¼ŒåŸºäºä¸Šä¸‹æ–‡èšç±»çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•é€šè¿‡å°†ä¸Šä¸‹æ–‡å‘é‡åˆ†ç»„åˆ°ç°‡ä¸­æ¥è¯†åˆ«è¯ä¹‰ã€‚æ­¤å¤–ï¼ŒåŸºäºè¯èšç±»çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•é€šè¿‡å°†è¯­ä¹‰ç›¸åŒçš„å•è¯åˆ†ç»„åˆ°ç°‡ä¸­æ¥è¯†åˆ«è¯ä¹‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹è¯ä¹‰æ¶ˆæ­§æŠ€æœ¯çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ€»ç»“äº†ä¸åŒæ–¹æ³•åœ¨ä¸åŒè¯­è¨€å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¯ä¹‰æ¶ˆæ­§ä»»åŠ¡ä¸­å–å¾—äº†æœ€å¥½çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡å¯¹è¯ä¹‰æ¶ˆæ­§æŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œå¹¶æ€»ç»“äº†ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚è¿™å¯¹äºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¥è¯´æ˜¯ä¸€ä¸ªå®è´µçš„èµ„æºï¼Œå¯ä»¥å¸®åŠ©ä»–ä»¬é€‰æ‹©åˆé€‚çš„è¯ä¹‰æ¶ˆæ­§æ–¹æ³•æ¥è§£å†³å®é™…é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†è¯ä¹‰æ¶ˆæ­§æŠ€æœ¯åœ¨å°åº¦è¯­è¨€ä¸­çš„åº”ç”¨ï¼Œè¿™å¯¹äºæ¨åŠ¨å°åº¦è¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## self-refine--iterative-refinement-with-self-feedback
### Abstract
Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªæˆ‘åé¦ˆè¿­ä»£ä¼˜åŒ–ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºè´¨é‡çš„æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶åœ¨ç”Ÿæˆè¿è´¯çš„è¾“å‡ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å¾€å¾€éš¾ä»¥æ»¡è¶³ç²¾ç»†åŒ–çš„éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è¯ç”Ÿæˆã€ä»£ç ä¼˜åŒ–ç­‰ä»»åŠ¡ä¸­ï¼ŒLLMs ç”Ÿæˆçš„åˆå§‹è¾“å‡ºå¯èƒ½ä¸å¤Ÿå®Œå–„ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„è¿­ä»£ä¼˜åŒ–æ‰èƒ½è¾¾åˆ°ç†æƒ³çš„è´¨é‡ã€‚ä¼ ç»Ÿçš„è¿­ä»£ä¼˜åŒ–æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒä¸€ä¸ªä¸“é—¨çš„ä¼˜åŒ–æ¨¡å‹ï¼Œè¿™éœ€è¦å¤§é‡çš„é¢†åŸŸç‰¹å®šæ•°æ®æˆ–æ˜‚è´µçš„æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Self-Refine çš„æ–°å‹è¿­ä»£ä¼˜åŒ–ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡è‡ªæˆ‘åé¦ˆå’Œè¿­ä»£ä¼˜åŒ–æ¥æå‡ LLMs çš„è¾“å‡ºè´¨é‡ã€‚Self-Refine çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨åŒä¸€ä¸ª LLM ä½œä¸ºç”Ÿæˆå™¨ã€åé¦ˆæä¾›è€…å’Œä¼˜åŒ–å™¨ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–å¼ºåŒ–å­¦ä¹ ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1. **åˆå§‹ç”Ÿæˆ**ï¼šä½¿ç”¨ LLM ç”Ÿæˆä¸€ä¸ªåˆå§‹è¾“å‡ºã€‚
2. **åé¦ˆ**ï¼šå°†åˆå§‹è¾“å‡ºåé¦ˆç»™ LLMï¼ŒLLM æ ¹æ®åé¦ˆç”Ÿæˆæ”¹è¿›å»ºè®®ã€‚
3. **ä¼˜åŒ–**ï¼šLLM æ ¹æ®åé¦ˆå»ºè®®å¯¹åˆå§‹è¾“å‡ºè¿›è¡Œä¼˜åŒ–ï¼Œç”Ÿæˆæ–°çš„è¾“å‡ºã€‚
4. **è¿­ä»£**ï¼šé‡å¤æ­¥éª¤ 2 å’Œ 3ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„åœæ­¢æ¡ä»¶ã€‚

Self-Refine ä½¿ç”¨å°‘é‡æ ·æœ¬æç¤ºï¼ˆfew-shot promptingï¼‰æ¥æŒ‡å¯¼ LLM è¿›è¡Œåé¦ˆå’Œä¼˜åŒ–ï¼Œä»è€Œé¿å…äº†é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ 7 ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šè¯„ä¼°äº† Self-Refine çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹è¯ç”Ÿæˆã€ä»£ç ä¼˜åŒ–ã€ä»£ç å¯è¯»æ€§æ”¹è¿›ã€æ•°å­¦æ¨ç†ç­‰ã€‚ç»“æœè¡¨æ˜ï¼ŒSelf-Refine åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºç›´æ¥ä½¿ç”¨ LLM è¿›è¡Œå•æ¬¡ç”Ÿæˆçš„ç»“æœï¼Œå¹³å‡æ€§èƒ½æå‡çº¦ 20%ã€‚æ­¤å¤–ï¼ŒSelf-Refine è¿˜èƒ½å¤Ÿæå‡ GPT-4 ç­‰æœ€å…ˆè¿›çš„ LLMs çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Self-Refine ä¸ºæå‡ LLMs è¾“å‡ºè´¨é‡æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **è‡ªæˆ‘åé¦ˆæœºåˆ¶**ï¼šåˆ©ç”¨ LLM è‡ªèº«çš„èƒ½åŠ›è¿›è¡Œè‡ªæˆ‘åé¦ˆå’Œä¼˜åŒ–ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡æ³¨ã€‚
* **è¿­ä»£ä¼˜åŒ–**ï¼šé€šè¿‡å¤šæ¬¡è¿­ä»£ä¼˜åŒ–ï¼Œé€æ­¥æå‡è¾“å‡ºè´¨é‡ï¼Œè¾¾åˆ°æ›´ç²¾ç»†åŒ–çš„éœ€æ±‚ã€‚
* **å°‘é‡æ ·æœ¬æç¤º**ï¼šä½¿ç”¨å°‘é‡æ ·æœ¬æç¤ºæ¥æŒ‡å¯¼ LLM è¿›è¡Œåé¦ˆå’Œä¼˜åŒ–ï¼Œç®€åŒ–äº†æ“ä½œæµç¨‹ã€‚

Self-Refine çš„æå‡ºä¸º LLMs çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œæœ‰æœ›åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€ä»£ç ç”Ÿæˆç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

## proagent--building-proactive-cooperative-agents-with-large-language-models
### Abstract
Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ProAgentï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºä¸»åŠ¨åˆä½œæ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œæ„å»ºå…·æœ‰è‡ªé€‚åº”è¡Œä¸ºçš„åˆä½œæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé‡è¦çš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨ç­–ç•¥æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºè®­ç»ƒé˜¶æ®µä¸é˜Ÿå‹çš„å¤šæ ·æ€§ã€‚å½“ä¸ä¸ç†Ÿæ‚‰çš„é˜Ÿå‹åˆä½œæ—¶ï¼Œè¿™ç§ä¾èµ–æ€§é™åˆ¶äº†æ™ºèƒ½ä½“è¿›è¡Œæˆ˜ç•¥é€‚åº”çš„èƒ½åŠ›ï¼Œè¿™åœ¨é›¶æ ·æœ¬åè°ƒåœºæ™¯ä¸­æˆä¸ºä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ProAgentï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ›å»ºä¸»åŠ¨æ™ºèƒ½ä½“çš„æ–°æ¡†æ¶ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´å…¶è¡Œä¸ºä»¥å¢å¼ºä¸é˜Ÿå‹çš„åˆä½œã€‚ProAgentå¯ä»¥åˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶ä»è§‚å¯Ÿä¸­æ¨æ–­é˜Ÿå‹çš„æ„å›¾ã€‚ç„¶åï¼Œå®ƒé€šè¿‡æ¯”è¾ƒé˜Ÿå‹çš„åç»­å®é™…è¡Œä¸ºæ¥æ›´æ–°å…¶ä¿¡å¿µã€‚æ­¤å¤–ï¼ŒProAgentå…·æœ‰é«˜åº¦çš„æ¨¡å—åŒ–å’Œå¯è§£é‡Šæ€§ï¼Œå¯ä»¥è½»æ¾åœ°é›†æˆåˆ°å„ç§åè°ƒåœºæ™¯ä¸­ã€‚

ProAgentæ¡†æ¶åŒ…æ‹¬å››ä¸ªå…³é”®æ¨¡å—ï¼šè§„åˆ’å™¨ã€éªŒè¯å™¨ã€æ§åˆ¶å™¨å’Œè®°å¿†æ¨¡å—ï¼Œä»¥åŠä¿¡å¿µä¿®æ­£æœºåˆ¶ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä½¿ProAgentèƒ½å¤Ÿä¸»åŠ¨é¢„æµ‹é˜Ÿå‹çš„æ„å›¾ï¼Œå¹¶åœ¨æ²¡æœ‰é¢„å…ˆè®­ç»ƒæˆ–å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°è‡ªé€‚åº”çš„åˆä½œæ¨ç†å’Œè§„åˆ’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Overcooked-AIç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¯„ä¼°æ­ç¤ºäº†ProAgentçš„å“è¶Šæ€§èƒ½ã€‚åœ¨ä¸AIæ™ºèƒ½ä½“åˆä½œæ—¶ï¼ŒProAgentä¼˜äºåŸºäºè‡ªæˆ‘æ¸¸æˆå’ŒåŸºäºç§ç¾¤è®­ç»ƒçš„äº”ç§æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåœ¨ä¸äººç±»ä»£ç†æ¨¡å‹åˆä½œæ—¶ï¼Œå…¶æ€§èƒ½å¹³å‡æé«˜äº†è¶…è¿‡10%ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ProAgentæ¡†æ¶ä¸ºåˆ©ç”¨LLMsåœ¨åˆä½œåœºæ™¯ä¸­çš„å¼ºå¤§æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æŒ‡å—ã€‚å®ƒå±•ç¤ºäº†LLMsåœ¨è§£é‡Šå½“å‰åœºæ™¯ã€æ˜ç¡®æ¨æ–­é˜Ÿå‹æ„å›¾ä»¥åŠç›¸åº”åœ°åŠ¨æ€è°ƒæ•´è¡Œä¸ºæ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ã€‚è¿™äº›ç»“æœä¸ºæ„å»ºæ›´é«˜æ•ˆçš„åˆä½œåœºæ™¯æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…ˆè¿›çš„åˆä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œäººæœºå…¼å®¹çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚

## language-models-can-solve-computer-tasks
### Abstract
Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting's
effectiveness in enhancing LLMs' reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting with
external feedback. We find that RCI combined with CoT performs better than
either separately. Our code can be found here:
https://github.com/posgnu/rci-agent.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹è§£å†³è®¡ç®—æœºä»»åŠ¡ï¼šRCIæ–¹æ³•å¼•é¢†AIæ–°æ½®æµ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œäººä»¬æœŸæœ›èƒ½å¤Ÿåˆ›é€ å‡ºèƒ½å¤Ÿæ‰§è¡Œå„ç§è®¡ç®—æœºä»»åŠ¡çš„æ™ºèƒ½ä»£ç†ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œç”Ÿäº§åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡çš„ä¸“å®¶æ¼”ç¤ºå’Œç‰¹å®šä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¹¶ä¸å®ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ‰§è¡Œè®¡ç®—æœºä»»åŠ¡ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤è¿›è¡ŒæŒ‡å¯¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡çš„æ ¸å¿ƒæ–¹æ³•æ˜¯é€’å½’æ‰¹è¯„å’Œæ”¹è¿›ï¼ˆRCIï¼‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š
1. **ä»»åŠ¡æ¥åœ°**ï¼šé¦–å…ˆï¼ŒLLMæ ¹æ®ä»»åŠ¡æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªé«˜çº§è®¡åˆ’ã€‚
2. **çŠ¶æ€æ¥åœ°**ï¼šç„¶åï¼Œå°†é«˜çº§æ¦‚å¿µä¸å½“å‰çŠ¶æ€ä¸­çš„å®é™…HTMLå…ƒç´ è¿æ¥èµ·æ¥ï¼Œè¾“å‡ºç›¸åº”çš„åŠ¨ä½œã€‚
3. **ä»£ç†æ¥åœ°**ï¼šæœ€åï¼Œç¡®ä¿åŠ¨ä½œè¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«è®¡ç®—æœºä»£ç†æ‰§è¡Œã€‚

RCIæ–¹æ¡ˆåœ¨æ¯ä¸ªæ­¥éª¤ä¸­éƒ½åº”ç”¨ï¼Œä½†çŠ¶æ€æ¥åœ°æ­¥éª¤åªéœ€è¦ä¸€æ¬¡æ‰¹è¯„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨MiniWoB++åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†RCIæ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼ŒRCIæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMæ–¹æ³•ï¼Œå¹¶åœ¨è‡ªåŠ¨åŒ–è®¡ç®—æœºä»»åŠ¡æ–¹é¢è¶…è¶Šäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒRCIæç¤ºæ–¹æ¡ˆåœ¨å¢å¼ºLLMçš„è‡ªç„¶è¯­è¨€æ¨ç†èƒ½åŠ›æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†å¤–éƒ¨åé¦ˆçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„RCIæ–¹æ³•ä¸ºä½¿ç”¨LLMæ‰§è¡Œè®¡ç®—æœºä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. **ç®€åŒ–ä»»åŠ¡æ‰§è¡Œ**ï¼šRCIæ–¹æ¡ˆé€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼Œç®€åŒ–äº†LLMæ‰§è¡Œè®¡ç®—æœºä»»åŠ¡çš„æµç¨‹ã€‚
2. **æé«˜æ¨ç†èƒ½åŠ›**ï¼šRCIæç¤ºæ–¹æ¡ˆå¯ä»¥å¢å¼ºLLMçš„è‡ªç„¶è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ã€‚
3. **é™ä½æ ·æœ¬å¤æ‚åº¦**ï¼šRCIæ–¹æ³•åªéœ€è¦å°‘é‡æ¼”ç¤ºå³å¯æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ï¼Œé™ä½äº†æ ·æœ¬å¤æ‚åº¦ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºçš„RCIæ–¹æ³•ä¸ºä½¿ç”¨LLMæ‰§è¡Œè®¡ç®—æœºä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ç®€åŒ–ä»»åŠ¡æ‰§è¡Œã€æé«˜æ¨ç†èƒ½åŠ›å’Œé™ä½æ ·æœ¬å¤æ‚åº¦ç­‰ä¼˜ç‚¹ã€‚éšç€LLMèƒ½åŠ›çš„ä¸æ–­æé«˜ï¼ŒRCIæ–¹æ³•æœ‰æœ›åœ¨è‡ªåŠ¨åŒ–è®¡ç®—æœºä»»åŠ¡æ–¹é¢å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

## quantizing-constrained-systems--new-perspectives
### Abstract
We consider quantum mechanics on constrained surfaces which have
non-Euclidean metrics and variable Gaussian curvature. The old controversy
about the ambiguities involving terms in the Hamiltonian of order hbar^2
multiplying the Gaussian curvature is addressed. We set out to clarify the
matter by considering constraints to be the limits of large restoring forces as
the constraint coordinates deviate from their constrained values. We find
additional ambiguous terms of order hbar^2 involving freedom in the
constraining potentials, demonstrating that the classical constrained
Hamiltonian or Lagrangian cannot uniquely specify the quantization: the
ambiguity of directly quantizing a constrained system is inherently
unresolvable. However, there is never any problem with a physical quantum
system, which cannot have infinite constraint forces and always fluctuates
around the mean constraint values. The issue is addressed from the perspectives
of adiabatic approximations in quantum mechanics, Feynman path integrals, and
semiclassically in terms of adiabatic actions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é‡å­åŠ›å­¦ä¸­çº¦æŸç³»ç»Ÿçš„é‡å­åŒ–ï¼šæ–°è§†è§’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
é‡å­åŠ›å­¦ä¸­ï¼Œå¯¹çº¦æŸç³»ç»Ÿçš„é‡å­åŒ–ä¸€ç›´æ˜¯ä¸€ä¸ªå……æ»¡äº‰è®®çš„è¯é¢˜ã€‚ä¼ ç»Ÿçš„é‡å­åŒ–æ–¹æ³•åœ¨å¤„ç†å…·æœ‰éæ¬§å‡ é‡Œå¾—åº¦é‡å’Œå¯å˜é«˜æ–¯æ›²ç‡çš„çº¦æŸè¡¨é¢æ—¶ï¼Œä¼šé‡åˆ°ä¸€äº›å›°éš¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ¶‰åŠå“ˆå¯†é¡¿é‡ä¸­ä¸é«˜æ–¯æ›²ç‡æˆæ­£æ¯”çš„é¡¹çš„æ¨¡ç³Šæ€§ï¼Œä¸€ç›´æ˜¯é‡å­åŒ–è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªéš¾é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è€ƒè™‘çº¦æŸä¸ºçº¦æŸåæ ‡åç¦»å…¶çº¦æŸå€¼æ—¶å¤§æ¢å¤åŠ›çš„æé™ï¼Œæ¥æ¾„æ¸…è¿™ä¸€é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡å­åŒ–æ–¹æ³•ï¼Œç§°ä¸ºâ€œæé™é‡å­åŒ–â€ã€‚è¯¥æ–¹æ³•å°†çº¦æŸè§†ä¸ºå¤§æ¢å¤åŠ›çš„æé™ï¼Œéšç€çº¦æŸåæ ‡åç¦»å…¶çº¦æŸå€¼ï¼Œæ¢å¤åŠ›é€æ¸å¢å¤§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ä¼ ç»Ÿé‡å­åŒ–æ–¹æ³•ä¸­å­˜åœ¨çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡ä»ä¸‰ä¸ªä¸åŒçš„è§’åº¦åˆ†æäº†çº¦æŸç³»ç»Ÿçš„é‡å­åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬é‡å­åŠ›å­¦ä¸­çš„ç»çƒ­è¿‘ä¼¼ã€è´¹æ›¼è·¯å¾„ç§¯åˆ†å’ŒåŠç»å…¸ç»çƒ­ä½œç”¨ã€‚è¿™äº›åˆ†æè¡¨æ˜ï¼Œé‡å­åŒ–è¿‡ç¨‹ä¸­çš„æ¨¡ç³Šæ€§æ˜¯å›ºæœ‰çš„ï¼Œå¹¶ä¸”ä¸çº¦æŸè¡¨é¢çš„å†…åœ¨å‡ ä½•å½¢çŠ¶å’Œçº¦æŸåŠ¿çš„ç»†èŠ‚æœ‰å…³ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡åˆ†æé‡å­æ³¢åŠ¨å¯¹çº¦æŸè¡¨é¢çš„å½±å“ï¼Œå‘ç°äº†ä¸€äº›ä¸å¯é¿å…çš„é‡å­åŠ¨åŠ›å­¦æ¨¡ç³Šæ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡ç³Šæ€§å¹¶ä¸å½±å“ç‰©ç†é‡å­ç³»ç»Ÿçš„è¡Œä¸ºï¼Œå› ä¸ºç‰©ç†é‡å­ç³»ç»Ÿä¸èƒ½å…·æœ‰æ— é™çº¦æŸåŠ›ï¼Œå¹¶ä¸”æ€»æ˜¯åœ¨çº¦æŸå€¼çš„å¹³å‡å€¼é™„è¿‘æ³¢åŠ¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–°é‡å­åŒ–æ–¹æ³•ä¸ºå¤„ç†å…·æœ‰éæ¬§å‡ é‡Œå¾—åº¦é‡å’Œå¯å˜é«˜æ–¯æ›²ç‡çš„çº¦æŸç³»ç»Ÿæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„åˆ†ææ–¹æ³•ä¹Ÿå¯ä»¥ç”¨äºç ”ç©¶å…¶ä»–ç±»å‹çš„é‡å­åŒ–é—®é¢˜ï¼Œä¾‹å¦‚é‡å­åœºè®ºä¸­çš„è§„èŒƒå›ºå®šé—®é¢˜ã€‚

## llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models
### Abstract
The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by
Large Language Models (LLMs) make them promising candidates for developing
coordination agents. In this study, we introduce a new LLM-Coordination
Benchmark aimed at a detailed analysis of LLMs within the context of Pure
Coordination Games, where participating agents need to cooperate for the most
gain. This benchmark evaluates LLMs through two distinct tasks: (1)
\emph{Agentic Coordination}, where LLMs act as proactive participants for
cooperation in 4 pure coordination games; (2) \emph{Coordination Question
Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice
questions from the 4 games for evaluation of three key reasoning abilities:
Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to
enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture
for Coordination (CAC) framework that can easily integrate different LLMs as
plug-and-play modules for pure coordination games. Our findings indicate that
LLM agents equipped with GPT-4-turbo achieve comparable performance to
state-of-the-art reinforcement learning methods in games that require
commonsense actions based on the environment. Besides, zero-shot coordination
experiments reveal that, unlike RL methods, LLM agents are robust to new unseen
partners. However, results on Coordination QA show a large room for improvement
in the Theory of Mind reasoning and joint planning abilities of LLMs. The
analysis also sheds light on how the ability of LLMs to understand their
environment and their partner's beliefs and intentions plays a part in their
ability to plan for coordination. Our code is available at
\url{https://github.com/eric-ai-lab/llm_coordination}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLM-Coordinationï¼šè¯„ä¼°å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è®¸å¤šæ—¥å¸¸ä»»åŠ¡å’Œå…³é”®æ“ä½œä¸­ï¼Œå¦‚çƒ¹é¥ªå’Œæ•‘æ´è¡ŒåŠ¨ï¼Œåˆä½œæ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›åœºæ™¯å¯ä»¥è¢«è§†ä¸ºçº¯åè°ƒæ¸¸æˆï¼Œå…¶ä¸­æ‰€æœ‰å‚ä¸æ–¹éƒ½ä»é€‰æ‹©å®Œå…¨ä¸€è‡´çš„æˆ˜ç•¥ä¸­å—ç›Šï¼Œé¿å…ä»»ä½•åˆ©ç›Šå†²çªã€‚è¿™äº›æ¸¸æˆè¦æ±‚ä»£ç†æ¨ç†ä»–ä»¬çš„ç¯å¢ƒå¹¶è®¡åˆ’ï¼ŒåŒæ—¶è€ƒè™‘ä»–ä»¬çš„ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒä¸­çš„æ¶Œç°è§„åˆ’èƒ½åŠ›ã€ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›å’Œå¯¹å¿ƒæ™ºç†è®ºçš„æš—ç¤ºï¼Œä½¿å®ƒä»¬æˆä¸ºå¼€å‘åè°ƒä»£ç†çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ç„¶è€Œï¼ŒLLMsåœ¨åè°ƒæ¸¸æˆä¸­çš„å¿…è¦æ¡ä»¶ã€ä¼˜åŠ¿å’Œå±€é™æ€§ä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è¿›è¡ŒLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›çš„å…¨é¢è¯„ä¼°å’Œåˆ†ææ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„LLM-CoordinationåŸºå‡†ï¼Œæ—¨åœ¨å¯¹LLMsåœ¨çº¯åè°ƒæ¸¸æˆä¸­çš„èƒ½åŠ›è¿›è¡Œè¯¦ç»†åˆ†æã€‚è¯¥åŸºå‡†é€šè¿‡ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡è¯„ä¼°LLMsï¼š
1. **ä»£ç†åè°ƒ**ï¼šLLMsä½œä¸ºç§¯æåˆä½œå‚ä¸è€…å‚ä¸4ä¸ªçº¯åè°ƒæ¸¸æˆã€‚
2. **åè°ƒé—®ç­”ï¼ˆQAï¼‰**ï¼šLLMsè¢«æç¤ºå›ç­”æ¥è‡ª4ä¸ªæ¸¸æˆçš„198ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œä»¥è¯„ä¼°ä¸‰ä¸ªå…³é”®æ¨ç†èƒ½åŠ›ï¼šç¯å¢ƒç†è§£ã€å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’ã€‚

æ­¤å¤–ï¼Œä¸ºäº†ä½¿LLMsèƒ½å¤Ÿè¿›è¡Œå¤šæ™ºèƒ½ä½“åè°ƒï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåè°ƒè®¤çŸ¥æ¶æ„ï¼ˆCACï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è½»æ¾åœ°å°†ä¸åŒçš„LLMsä½œä¸ºå³æ’å³ç”¨æ¨¡å—é›†æˆåˆ°çº¯åè°ƒæ¸¸æˆä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡GPT-4-turboçš„LLMä»£ç†åœ¨éœ€è¦åŸºäºç¯å¢ƒçš„å¸¸è¯†è¡ŒåŠ¨çš„æ¸¸æˆä¸­ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œé›¶æ ·æœ¬åè°ƒå®éªŒè¡¨æ˜ï¼Œä¸RLæ–¹æ³•ä¸åŒï¼ŒLLMä»£ç†å¯¹æ–°æœªè§ä¼™ä¼´å…·æœ‰é²æ£’æ€§ã€‚ç„¶è€Œï¼Œåè°ƒQAçš„ç»“æœè¡¨æ˜ï¼ŒLLMsçš„å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚åˆ†æè¿˜æ­ç¤ºäº†LLMsç†è§£å…¶ç¯å¢ƒå’Œå…¶ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾çš„èƒ½åŠ›å¦‚ä½•å½±å“å®ƒä»¬åè°ƒè®¡åˆ’çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„LLM-CoordinationåŸºå‡†å’ŒCACæ¡†æ¶ä¸ºè¯„ä¼°å’Œåˆ†æLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç»“æœçªå‡ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## adarefiner--refining-decisions-of-language-models-with-adaptive-feedback
### Abstract
Large Language Models (LLMs) have demonstrated significant success across
various domains. However, their application in complex decision-making tasks
frequently necessitates intricate prompt engineering or fine-tuning, leading to
challenges in unseen downstream tasks and heavy demands on computational
resources. Meanwhile, Reinforcement Learning (RL) has been recognized as
effective in decision-making problems but struggles in environments with sparse
rewards, such as open-world games. To overcome these challenges, we introduce
AdaRefiner, a novel framework designed to enhance the synergy between LLMs and
RL feedback. The key component of AdaRefiner is a lightweight Adapter Language
Model (LM), which automatically refines task comprehension based on feedback
from RL agents. This method mitigates the need for intricate prompt engineering
and intensive LLM fine-tuning while maintaining the LLMs' generalization
abilities and enhancing their decision-making capabilities in downstream tasks.
Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world
game Crafter have demonstrated its superior effectiveness, especially in
guiding agents towards higher-level and common-sense skills. Our work makes
contributions to the automatic self-refinement of LLMs with RL feedback,
offering a more adaptable and efficient solution for complex decision-making
problems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdaRefinerï¼šåˆ©ç”¨è‡ªé€‚åº”åé¦ˆæå‡è¯­è¨€æ¨¡å‹å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„åº”ç”¨å´é¢ä¸´ç€æŒ‘æˆ˜ã€‚LLMs éœ€è¦è¿›è¡Œç¹ççš„æç¤ºå·¥ç¨‹æˆ–å¾®è°ƒæ‰èƒ½é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªçŸ¥ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¸¦æ¥äº†å¯¹è®¡ç®—èµ„æºçš„å·¨å¤§éœ€æ±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å†³ç­–é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç¨€ç–å¥–åŠ±çš„ç¯å¢ƒä¸­ï¼ˆå¦‚å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼‰å´éš¾ä»¥å‘æŒ¥ä½œç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº† AdaRefinerï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼º LLMs å’Œ RL åé¦ˆä¹‹é—´ååŒä½œç”¨çš„æ–°æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAdaRefiner å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€‚é…å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œè¯¥æ¨¡å‹æ ¹æ® RL ä»£ç†çš„åé¦ˆè‡ªåŠ¨ç»†åŒ–ä»»åŠ¡ç†è§£ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†ç¹ççš„æç¤ºå·¥ç¨‹å’Œå¯†é›†çš„ LLM å¾®è°ƒçš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº† LLMs çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAdaRefiner åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆ Crafter çš„ 22 ä¸ªä¸åŒä»»åŠ¡ä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶åœ¨å¼•å¯¼ä»£ç†å­¦ä¹ é«˜çº§å’Œå¸¸è¯†æŠ€èƒ½æ–¹é¢å…·æœ‰ä¼˜è¶Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AdaRefiner åœ¨ Crafter ç¯å¢ƒä¸­çš„ 22 ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚AdaRefiner èƒ½å¤Ÿå¼•å¯¼ä»£ç†å­¦ä¹ é«˜çº§æŠ€èƒ½ï¼Œå¹¶è¡¨ç°å‡ºå¸¸è¯†è¡Œä¸ºã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œé€‚é…å™¨ LM å’Œ RL åé¦ˆå¯¹äº AdaRefiner çš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AdaRefiner ä¸º LLMs åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†ä¸€ç§æ›´çµæ´»å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…¶è½»é‡çº§çš„é€‚é…å™¨ LM å’Œè‡ªé€‚åº”åé¦ˆæœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°æå‡ LLMs çš„ä»»åŠ¡ç†è§£å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸º LLMs åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆç­‰å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

## adapt--as-needed-decomposition-and-planning-with-language-models
### Abstract
Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ADaPTï¼šæŒ‰éœ€åˆ†è§£ä¸è§„åˆ’ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬ä¹Ÿé€æ¸è¢«åº”ç”¨äºéœ€è¦è§„åˆ’å’Œé€‚åº”ç¯å¢ƒçš„äº¤äº’å¼å†³ç­–ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºLLMsåœ¨æ‰§è¡Œå­ä»»åŠ¡æ—¶å¯èƒ½ä¼šå¤±è´¥ï¼Œä»è€Œå¯¼è‡´æ•´ä¸ªä»»åŠ¡çš„å¤±è´¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ADaPTï¼ˆAs-Needed Decomposition and Planning for complex Tasksï¼‰ï¼Œä¸€ç§æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’å¤æ‚ä»»åŠ¡çš„æ–¹æ³•ã€‚ADaPTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå°†å…¶åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è¿›è¡Œåˆ†è§£ï¼Œä»¥é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‰éœ€åˆ†è§£
ADaPTé€šè¿‡é€’å½’åœ°åˆ†è§£å­ä»»åŠ¡ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚å½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå®ƒä¼šè°ƒç”¨LLMä½œä¸ºè§„åˆ’è€…æ¥ç”Ÿæˆæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è°ƒç”¨ADaPTæ¥æ‰§è¡Œè¿™äº›å­ä»»åŠ¡ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šçº§åˆ†è§£
ADaPTæ”¯æŒå¤šçº§åˆ†è§£ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥è¿›ä¸€æ­¥åˆ†è§£å­ä»»åŠ¡ï¼Œç›´åˆ°å®ƒä»¬å˜å¾—è¶³å¤Ÿç®€å•ï¼Œå¯ä»¥è¢«LLMä½œä¸ºæ‰§è¡Œè€…æˆåŠŸæ‰§è¡Œã€‚è¿™ç§å¤šçº§åˆ†è§£çš„èƒ½åŠ›ä½¿å¾—ADaPTèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ALFWorldã€WebShopå’ŒTextCraftä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒADaPTæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³•ï¼Œåœ¨ALFWorldä¸Šæé«˜äº†28.3%çš„æˆåŠŸç‡ï¼Œåœ¨WebShopä¸Šæé«˜äº†27%ï¼Œåœ¨TextCraftä¸Šæé«˜äº†33%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ADaPTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å¤„ç†LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ‰§è¡Œå¤±è´¥é—®é¢˜ã€‚å®ƒé€šè¿‡æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†ä»»åŠ¡çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒADaPTçš„å¤šçº§åˆ†è§£èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

## swiftsage--a-generative-agent-with-fast-and-slow-thinking-for-complex-interactive-tasks
### Abstract
We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SwiftSageï¼šç»“åˆå¿«æ…¢æ€è€ƒçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œè§£å†³å¤æ‚äº¤äº’ä»»åŠ¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæ™ºèƒ½ä½“åœ¨å¤æ‚äº¤äº’æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚è¿™ç±»ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“å…·å¤‡é•¿æœŸè§„åˆ’ã€è®°å¿†ã€å­ç›®æ ‡åˆ†è§£ã€ç©ºé—´æ¨ç†ã€å¼‚å¸¸å¤„ç†å’Œå¸¸è¯†çŸ¥è¯†ç­‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ã€è¡Œä¸ºå…‹éš†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºï¼Œåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¾‹å¦‚ï¼Œå¼ºåŒ–å­¦ä¹ éœ€è¦å¤§é‡çš„äº¤äº’æ¥å­¦ä¹ ï¼Œè¡Œä¸ºå…‹éš†éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ï¼Œè€ŒLLMæç¤ºåˆ™ç¼ºä¹å¯¹ç¯å¢ƒçš„å…·ä½“æ“ä½œæŒ‡å¯¼ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
SwiftSage æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œçµæ„Ÿæ¥æºäºäººç±»è®¤çŸ¥çš„åŒè¿‡ç¨‹ç†è®ºï¼Œæ—¨åœ¨è§£å†³å¤æ‚äº¤äº’æ¨ç†ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¡Œä¸ºå…‹éš†å’ŒLLMæç¤ºçš„ä¼˜åŠ¿ï¼Œä»¥æé«˜ä»»åŠ¡å®Œæˆæ€§èƒ½ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒæ¨¡å—è®¾è®¡
SwiftSage ç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼š
- **Swift æ¨¡å—**ï¼šä»£è¡¨å¿«é€Ÿå’Œç›´è§‚çš„æ€è€ƒï¼Œæ˜¯ä¸€ä¸ªå°å‹çš„ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ¨¡ä»¿ä¸“å®¶æ™ºèƒ½ä½“çš„è¡Œä¸ºè½¨è¿¹è¿›è¡Œå¾®è°ƒã€‚
- **Sage æ¨¡å—**ï¼šä»£è¡¨æ·±æ€ç†Ÿè™‘çš„æ€è€ƒï¼Œåˆ©ç”¨LLMï¼ˆå¦‚GPT-4ï¼‰è¿›è¡Œå­ç›®æ ‡è§„åˆ’å’Œæ¥åœ°ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å—é›†æˆç­–ç•¥
SwiftSage å¼€å‘äº†ä¸€ç§å¯å‘å¼æ–¹æ³•æ¥å’Œè°åœ°é›†æˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€åˆ‡æ¢æ¨¡å—ï¼Œå¹¶æœ‰æ•ˆç»“åˆå®ƒä»¬çš„è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå½“é‡åˆ°å¼‚å¸¸æƒ…å†µæˆ–éœ€è¦é•¿æœŸè§„åˆ’æ—¶ï¼ŒSage æ¨¡å—ä¼šè¢«æ¿€æ´»ï¼Œè€Œ Swift æ¨¡å—åˆ™ç”¨äºå¿«é€Ÿå“åº”å’Œæ‰§è¡Œç®€å•ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ ScienceWorld åŸºå‡†æµ‹è¯•çš„30ä¸ªä»»åŠ¡ä¸­ï¼ŒSwiftSage æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚SayCanã€ReActå’ŒReflexionï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³å¤æ‚äº¤äº’ä»»åŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚SwiftSage å®ç°äº†æœ€å…ˆè¿›çš„å¹³å‡å¾—åˆ†84.7ï¼Œè€Œå…¶ä»–æ–¹æ³•çš„å¾—åˆ†åˆ†åˆ«ä¸º33.8ã€36.4å’Œ45.3ã€‚æ­¤å¤–ï¼ŒSwiftSage æ›´åŠ ç»æµé«˜æ•ˆï¼Œæ¯é¡¹æ“ä½œæ‰€éœ€çš„LLMä»¤ç‰Œæ•°é‡è¿œå°‘äºä¹‹å‰çš„æ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SwiftSage çš„åŒæ¨¡å—è®¾è®¡ä¸ºè§£å†³å¤æ‚äº¤äº’æ¨ç†ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶æ¨¡å—é›†æˆç­–ç•¥å’Œå¯å‘å¼ç®—æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¿«é€Ÿå“åº”å’Œæ·±æ€ç†Ÿè™‘çš„ä»»åŠ¡ä¸­ã€‚æ­¤å¤–ï¼ŒSwiftSage çš„æˆåŠŸä¹Ÿè¡¨æ˜äº†ç»“åˆå°å‹è¯­è¨€æ¨¡å‹å’ŒLLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

## language-models-as-zero-shot-planners--extracting-actionable-knowledge-for-embodied-agents
### Abstract
Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. "make
breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬è§„åˆ’å™¨ï¼šä¸ºå…·èº«æ™ºèƒ½ä½“æå–å¯æ“ä½œçŸ¥è¯†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç”Ÿæˆå’Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¿™äº›æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ äº†å¤§é‡çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†å¦‚ä½•å°†è¿™äº›çŸ¥è¯†åº”ç”¨äºäº¤äº’å¼ç¯å¢ƒä¸­çš„å…·èº«æ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ‰§è¡Œé«˜å±‚æ¬¡çš„æŒ‡ä»¤ï¼ˆä¾‹å¦‚â€œåšæ—©é¤â€ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»æ˜ç¡®çš„æ­¥éª¤ç¤ºä¾‹ä¸­å­¦ä¹ å¦‚ä½•è¡ŒåŠ¨ï¼Œè€Œæœ¬æ–‡åˆ™æ¢ç´¢äº†åˆ©ç”¨é¢„è®­ç»ƒçš„LLMsç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’çš„æ½œåŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé›¶æ ·æœ¬è§„åˆ’
æœ¬æ–‡å‘ç°ï¼Œå¦‚æœé¢„è®­ç»ƒçš„LLMsè¶³å¤Ÿå¤§ï¼Œå¹¶ä¸”ä»¥é€‚å½“çš„æ–¹å¼è¿›è¡Œæç¤ºï¼Œå®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°å°†é«˜å±‚æ¬¡çš„ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„ä¸­é—´æ­¥éª¤ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥çš„è®­ç»ƒã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸ºâ€œé›¶æ ·æœ¬è§„åˆ’â€ï¼Œå› ä¸ºå®ƒä¸éœ€è¦é’ˆå¯¹ç‰¹å®šç¯å¢ƒè¿›è¡Œå¾®è°ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯­ä¹‰ç¿»è¯‘å’Œå¯æ‰§è¡Œæ€§æå‡
ç„¶è€Œï¼ŒLLMsç”Ÿæˆçš„è®¡åˆ’å¾€å¾€æ— æ³•ç²¾ç¡®æ˜ å°„åˆ°ç¯å¢ƒä¸­çš„å¯æ¥å—åŠ¨ä½œã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç°æœ‰æ¼”ç¤ºçš„è¯­ä¹‰ç¿»è¯‘è¿‡ç¨‹ï¼Œå°†LLMsç”Ÿæˆçš„è®¡åˆ’è½¬æ¢ä¸ºå¯æ¥å—çš„åŠ¨ä½œã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªå›å½’è½¨è¿¹æ ¡æ­£å’ŒåŠ¨æ€ç¤ºä¾‹é€‰æ‹©ç­‰æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜è®¡åˆ’çš„æ‰§è¡Œæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨VirtualHomeç¯å¢ƒä¸­è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è®¡åˆ’çš„æ‰§è¡Œæ€§ï¼Œä½†æ­£ç¡®æ€§æœ‰æ‰€ä¸‹é™ã€‚è¿™è¡¨æ˜ï¼Œä»è¯­è¨€æ¨¡å‹ä¸­æå–å¯æ“ä½œçŸ¥è¯†æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥å¹³è¡¡æ‰§è¡Œæ€§å’Œæ­£ç¡®æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„é›¶æ ·æœ¬è§„åˆ’æ–¹æ³•ä¸ºåˆ©ç”¨LLMsçš„çŸ¥è¯†è¿›è¡Œå…·èº«æ™ºèƒ½ä½“çš„å†³ç­–æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰ç¿»è¯‘å’Œå¯æ‰§è¡Œæ€§æå‡æŠ€æœ¯ä¹Ÿä¸ºæé«˜LLMsç”Ÿæˆçš„è®¡åˆ’çš„å®ç”¨æ€§æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¿™äº›å‘ç°å¯¹äºæœªæ¥ç ”ç©¶å¦‚ä½•å°†LLMsçš„çŸ¥è¯†åº”ç”¨äºäº¤äº’å¼ç¯å¢ƒä¸­çš„å…·èº«æ™ºèƒ½ä½“å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚

## software-agents-interaction-algorithms-in-virtual-learning-environment
### Abstract
This paper highlights the multi-agent learning virtual environment and agents
communication algorithms. The researcher proposed three algorithms required
software agents interaction in virtual learning information system environment.
The first proposed algorithm is agents interaction localization algorithm, the
second one is the dynamic agents distribution algorithm (load distribution
algorithm), and the third model is Agent communication algorithm based on using
agents intermediaries. The main objectives of these algorithms are to reduce
the response time for any agents changes in virtual learning environment (VLE)
by increasing the information exchange intensity between software agents and
reduce the overall network load, and to improve the communication between
mobile agents in distributed information system to support effectiveness.
Finally the paper describe the algorithms of information exchange between
mobile agents in VLE based on the expansion of the address structure and the
use of an agent, intermediary agents, matchmaking agents, brokers and their
entrepreneurial functions
### ğŸŒŸ è®ºæ–‡è§£è¯» | è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­è½¯ä»¶ä»£ç†äº¤äº’ç®—æ³•çš„é©æ–°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ä¿¡æ¯æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼Œè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒï¼ˆVLEï¼‰å·²ç»æˆä¸ºæ•™è‚²é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLEç³»ç»Ÿåœ¨å¤„ç†å¤§é‡ä¿¡æ¯äº¤äº’æ—¶ï¼Œé¢ä¸´ç€ç½‘ç»œè´Ÿè½½å¢åŠ ã€ä¿¡æ¯äº¤æ¢æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè½¯ä»¶ä»£ç†çš„äº¤äº’ç®—æ³•ï¼Œæ—¨åœ¨æé«˜VLEç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”é€Ÿåº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»£ç†äº¤äº’å®šä½ç®—æ³•
è¯¥ç®—æ³•é€šè¿‡åˆ†æä»£ç†ä¹‹é—´çš„é€šä¿¡ä¾èµ–æ€§ï¼Œå°†é¢‘ç¹äº¤äº’çš„ä»£ç†åˆ†ç»„åˆ°åŒä¸€ä¸»æœºä¸Šï¼Œä»è€Œå°†è·¨ä¸»æœºçš„äº¤äº’è½¬åŒ–ä¸ºä¸»æœºå†…çš„äº¤äº’ï¼Œå‡å°‘ç½‘ç»œè´Ÿè½½å¹¶æé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€ä»£ç†åˆ†é…ç®—æ³•ï¼ˆè´Ÿè½½åˆ†é…ç®—æ³•ï¼‰
è¯¥ç®—æ³•é€šè¿‡ç›‘æ§ä¸»æœºè´Ÿè½½ï¼Œå°†ä»£ç†åˆ†ç»„å¹¶åŠ¨æ€åˆ†é…åˆ°ä¸åŒçš„ä¸»æœºä¸Šï¼Œä»¥å®ç°è´Ÿè½½å‡è¡¡ï¼Œé¿å…æŸäº›ä¸»æœºè¿‡è½½ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºä»£ç†ä¸­ä»‹çš„é€šä¿¡æ¨¡å‹
è¯¥æ¨¡å‹åˆ©ç”¨ä»£ç†ä¸­ä»‹ï¼ˆå¦‚ç»çºªäººä»£ç†å’Œé…å¯¹ä»£ç†ï¼‰æ¥ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„é€šä¿¡ã€‚ä»£ç†ä¸­ä»‹å¯ä»¥å¸®åŠ©ä»£ç†æŸ¥æ‰¾å…·æœ‰ç›¸ä¼¼å…´è¶£çš„ä»£ç†ï¼Œå¹¶æä¾›æ¶ˆæ¯è½¬å‘å’ŒåŒ¹é…æœåŠ¡ï¼Œä»è€Œæé«˜é€šä¿¡æ•ˆç‡å’Œçµæ´»æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æå‡ºçš„ç®—æ³•åœ¨è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¿™äº›ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç½‘ç»œè´Ÿè½½ï¼Œæé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ï¼Œå¹¶æé«˜ä»£ç†ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç®—æ³•å’Œæ¨¡å‹ä¸ºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶ä¸­ï¼Œä»£ç†äº¤äº’å®šä½ç®—æ³•å’ŒåŠ¨æ€ä»£ç†åˆ†é…ç®—æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œä»¥æé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚åŸºäºä»£ç†ä¸­ä»‹çš„é€šä¿¡æ¨¡å‹å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šä»£ç†ç³»ç»Ÿä¸­ï¼Œä»¥ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„åä½œå’Œé€šä¿¡ã€‚

### ğŸ“š æ€»ç»“
æœ¬æ–‡æå‡ºçš„åŸºäºè½¯ä»¶ä»£ç†çš„äº¤äº’ç®—æ³•ä¸ºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¿™äº›ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç½‘ç»œè´Ÿè½½ï¼Œæé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ï¼Œå¹¶æé«˜ä»£ç†ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ã€‚æœ¬æ–‡çš„ç ”ç©¶æˆæœå¯¹äºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒå’Œå…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿçš„è®¾è®¡å’Œä¼˜åŒ–å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## mindagent--emergent-gaming-interaction
### Abstract
Large Language Models (LLMs) have the capacity of performing complex
scheduling in a multi-agent system and can coordinate these agents into
completing sophisticated tasks that require extensive collaboration. However,
despite the introduction of numerous gaming frameworks, the community has
insufficient benchmarks towards building general multi-agents collaboration
infrastructure that encompass both LLM and human-NPCs collaborations. In this
work, we propose a novel infrastructure - MindAgent - to evaluate planning and
coordination emergent capabilities for gaming interaction. In particular, our
infrastructure leverages existing gaming framework, to i) require understanding
of the coordinator for a multi-agent system, ii) collaborate with human players
via un-finetuned proper instructions, and iii) establish an in-context learning
on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new
gaming scenario and related benchmark that dispatch a multi-agent collaboration
efficiency and supervise multiple agents playing the game simultaneously. We
conduct comprehensive evaluations with new auto-metric CoS for calculating the
collaboration efficiency. Finally, our infrastructure can be deployed into
real-world gaming scenarios in a customized VR version of CUISINEWORLD and
adapted in existing broader Minecraft gaming domain. We hope our findings on
LLMs and the new infrastructure for general-purpose scheduling and coordination
can help shed light on how such skills can be obtained by learning from large
language corpora.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MindAgentï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ä¹Ÿé€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆæ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•è¿˜ä¸è¶³ä»¥è¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨LLMsä¸äººç±»NPCsåä½œçš„åœºæ™¯ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMindAgentçš„æ–°å‹åŸºç¡€è®¾æ–½ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸åŸºå‡†æµ‹è¯•
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåä¸ºCUISINEWORLDçš„æ¸¸æˆåœºæ™¯ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªè™šæ‹Ÿå¨æˆ¿ç¯å¢ƒï¼Œå…¶ä¸­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿéœ€è¦åè°ƒå¤šä¸ªä»£ç†ï¼Œå®Œæˆå°½å¯èƒ½å¤šçš„èœè‚´è®¢å•ã€‚CUISINEWORLDæ¸¸æˆåœºæ™¯å…·æœ‰å¤šç§ä»»åŠ¡ç»“æ„å’Œéš¾åº¦ï¼Œæ˜¯è¯„ä¼°LLMsæ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMindAgentåŸºç¡€è®¾æ–½
MindAgentæ˜¯ä¸€ä¸ªç”¨äºLLMsäº¤äº’å¼å¤šæ™ºèƒ½ä½“è§„åˆ’çš„åŸºç¡€è®¾æ–½ï¼Œå®ƒå±•ç¤ºäº†LLMsçš„æ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†å¤šç§æç¤ºæŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›LLMsçš„è§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬æä¾›å°‘é‡ç¤ºä¾‹ã€è§„åˆ’ç†ç”±å’Œç¯å¢ƒåé¦ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨CUISINEWORLDæ¸¸æˆåœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š
1. é›¶æ ·æœ¬å¤šæ™ºèƒ½ä½“è§„åˆ’ï¼šå¼ºå¤§çš„é¢„è®­ç»ƒLLMsï¼ˆå¦‚GPT-4ï¼‰èƒ½å¤Ÿé€šè¿‡é˜…è¯»ç®€å•çš„æ¸¸æˆæŒ‡ä»¤å’Œé£Ÿè°±ï¼Œè°ƒåº¦å¤šä¸ªä»£ç†ï¼ˆ2åˆ°4ä¸ªï¼‰å®Œæˆèœè‚´ï¼Œç”šè‡³ä¸äººç±»ç©å®¶åä½œã€‚
2. åŸºäºé«˜çº§æç¤ºçš„è§„åˆ’ï¼šé€šè¿‡åˆ©ç”¨æ¶Œç°å¼ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜LLMsçš„å¤šæ™ºèƒ½ä½“è§„åˆ’æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ å°‘é‡ä¸“å®¶æ¼”ç¤ºã€è§£é‡ŠæŸäº›è¡ŒåŠ¨çš„ç†ç”±ï¼Œä»¥åŠåœ¨è§„åˆ’è¿‡ç¨‹ä¸­æä¾›å®æ—¶åé¦ˆã€‚
3. é€šç”¨æ½œåŠ›ï¼šLLMsè¡¨ç°å‡ºæˆä¸ºé€šç”¨å¤šæ™ºèƒ½ä½“è§„åˆ’å™¨çš„å·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹æ³›åŒ–åˆ°æ›´å¤šä»£ç†ï¼Œå¹¶é€‚åº”æ–°çš„æ¸¸æˆé¢†åŸŸï¼Œå¦‚Minecraftã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„MindAgentåŸºç¡€è®¾æ–½å’ŒCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼ŒLLMsåœ¨å¤šæ™ºèƒ½ä½“è§„åˆ’æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨åŠ¨æ¸¸æˆAIçš„å‘å±•ã€‚

## skill-reinforcement-learning-and-planning-for-open-world-long-horizon-tasks
### Abstract
We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that
performs exploration to find diverse items provides better initialization for
other skills, improving the sample efficiency for skill learning. In skill
planning, we leverage the prior knowledge in Large Language Models to find the
relationships between skills and build a skill graph. When the agent is solving
a task, our skill search algorithm walks on the skill graph and generates the
proper skill plans for the agent. In experiments, our method accomplishes 40
diverse Minecraft tasks, where many tasks require sequentially executing for
more than 10 skills. Our method outperforms baselines by a large margin and is
the most sample-efficient demonstration-free RL method to solve Minecraft Tech
Tree tasks. The project's website and code can be found at
https://sites.google.com/view/plan4mc.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Plan4MCï¼šåŸºäºæŠ€èƒ½å¼ºåŒ–å­¦ä¹ å’Œè§„åˆ’çš„å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡è§£å†³æ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå­¦ä¹ å®Œæˆé•¿æ—¶ä»»åŠ¡å¯¹äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è¯´æ˜¯éå¸¸ä½æ•ˆçš„ã€‚è¿™æ˜¯å› ä¸ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒé€šå¸¸å…·æœ‰æ— é™å¤§çš„ä¸–ç•Œè§„æ¨¡å’Œå¤§é‡çš„ä»»åŠ¡ï¼Œè¿™ä½¿å¾—æ¢ç´¢å’Œæ ·æœ¬æ•ˆç‡æˆä¸ºä¸»è¦æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œé•¿æ—¶ä»»åŠ¡é€šå¸¸å…·æœ‰å¤šä¸ªå­ç›®æ ‡ï¼Œéœ€è¦å¤§é‡çš„ç¯å¢ƒæ­¥éª¤æ‰èƒ½å®Œæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†å¤šä»»åŠ¡å­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºå­¦ä¹ åŸºæœ¬æŠ€èƒ½å’ŒæŠ€èƒ½è§„åˆ’ã€‚åœ¨Minecraftæ¸¸æˆä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§ç»†ç²’åº¦åŸºæœ¬æŠ€èƒ½ï¼šå¯»æ‰¾æŠ€èƒ½ã€æ“ä½œæŠ€èƒ½å’Œåˆ¶ä½œæŠ€èƒ½ã€‚ä½¿ç”¨å…·æœ‰å†…åœ¨å¥–åŠ±çš„RLæ¥è·å–æŠ€èƒ½ï¼Œå¹¶é€šè¿‡æŠ€èƒ½è§„åˆ’æ¥åˆ†è§£ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆéªŒçŸ¥è¯†æ¥æ„å»ºæŠ€èƒ½å›¾ï¼Œå¹¶é€šè¿‡æŠ€èƒ½æœç´¢ç®—æ³•æ¥ç”Ÿæˆæ­£ç¡®çš„æŠ€èƒ½åºåˆ—ã€‚è¿™ç§æ–¹æ³•é¿å…äº†LLMä¸å¯æ§çš„é”™è¯¯ï¼Œå¹¶æé«˜äº†è§„åˆ’ç²¾åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨MineDojoæ¨¡æ‹Ÿå™¨ä¸­æ„å»ºäº†40ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œç»“æœè¡¨æ˜Plan4MCèƒ½å¤Ÿå®Œæˆæ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPlan4MCåœ¨Minecraft Tech Treeä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Plan4MCæå‡ºäº†ä¸€ç§é«˜æ•ˆè§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ åŸºæœ¬æŠ€èƒ½å’ŒæŠ€èƒ½è§„åˆ’æ¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMæ„å»ºæŠ€èƒ½å›¾å’ŒæŠ€èƒ½æœç´¢ç®—æ³•ä¸ºå¼€æ”¾ä¸–ç•Œä»»åŠ¡è§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## on-the-utility-of-learning-about-humans-for-human-ai-coordination
### Abstract
While we would like agents that can coordinate with humans, current
algorithms such as self-play and population-based training create agents that
can coordinate with themselves. Agents that assume their partner to be optimal
or similar to them can converge to coordination protocols that fail to
understand and be understood by humans. To demonstrate this, we introduce a
simple environment that requires challenging coordination, based on the popular
game Overcooked, and learn a simple model that mimics human play. We evaluate
the performance of agents trained via self-play and population-based training.
These agents perform very well when paired with themselves, but when paired
with our human model, they are significantly worse than agents designed to play
with the human model. An experiment with a planning algorithm yields the same
conclusion, though only when the human-aware planner is given the exact human
model that it is playing with. A user study with real humans shows this pattern
as well, though less strongly. Qualitatively, we find that the gains come from
having the agent adapt to the human's gameplay. Given this result, we suggest
several approaches for designing agents that learn about humans in order to
better coordinate with them. Code is available at
https://github.com/HumanCompatibleAI/overcooked_ai.
### ğŸŒŸ è®ºæ–‡è§£è¯» | äººå·¥æ™ºèƒ½ä¸äººç±»åä½œï¼šå­¦ä¹ äººç±»è¡Œä¸ºçš„é‡è¦æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬å¸Œæœ›AIèƒ½å¤Ÿä¸äººç±»è¿›è¡Œæœ‰æ•ˆçš„åä½œï¼Œå…±åŒå®Œæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œç›®å‰è®¸å¤šè®­ç»ƒAIçš„æ–¹æ³•ï¼Œå¦‚è‡ªæˆ‘åšå¼ˆå’ŒåŸºäºç¾¤ä½“çš„è®­ç»ƒï¼Œå¾€å¾€å¯¼è‡´AIåªèƒ½ä¸è‡ªå·±åä½œï¼Œè€Œæ— æ³•ç†è§£æˆ–è¢«äººç±»ç†è§£ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ äººç±»è¡Œä¸ºçš„é‡è¦æ€§ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„AIä¸äººç±»åä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºåŸºäºæ¸¸æˆOvercookedçš„åä½œç¯å¢ƒ
ä¸ºäº†éªŒè¯å­¦ä¹ äººç±»è¡Œä¸ºçš„é‡è¦æ€§ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŸºäºæ¸¸æˆOvercookedçš„åä½œç¯å¢ƒï¼Œè¯¥ç¯å¢ƒè¦æ±‚ç©å®¶è¿›è¡Œå¤æ‚çš„åä½œæ‰èƒ½å®Œæˆä»»åŠ¡ã€‚é€šè¿‡è¿™ä¸ªç¯å¢ƒï¼Œç ”ç©¶äººå‘˜å¯ä»¥è¯„ä¼°ä¸åŒè®­ç»ƒæ–¹æ³•çš„AIä¸äººç±»æ¨¡å‹çš„åä½œæ•ˆæœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¯”è¾ƒä¸åŒè®­ç»ƒæ–¹æ³•çš„AIä¸äººç±»æ¨¡å‹çš„åä½œæ•ˆæœ
æœ¬æ–‡æ¯”è¾ƒäº†ä»¥ä¸‹å‡ ç§è®­ç»ƒæ–¹æ³•çš„AIä¸äººç±»æ¨¡å‹çš„åä½œæ•ˆæœï¼š
- è‡ªæˆ‘åšå¼ˆï¼ˆSelf-Playï¼‰ï¼šAIä¸è‡ªèº«è¿›è¡Œåšå¼ˆï¼Œå­¦ä¹ å¦‚ä½•ä¸è‡ªå·±åä½œã€‚
- åŸºäºç¾¤ä½“çš„è®­ç»ƒï¼ˆPopulation-Based Training, PBTï¼‰ï¼šAIä¸ç¾¤ä½“ä¸­çš„å…¶ä»–AIè¿›è¡Œåšå¼ˆï¼Œå­¦ä¹ å¦‚ä½•ä¸ç¾¤ä½“ä¸­çš„å…¶ä»–AIåä½œã€‚
- è€¦åˆè§„åˆ’ï¼ˆCoupled Planningï¼‰ï¼šAIä¸äººç±»æ¨¡å‹è¿›è¡Œè€¦åˆè§„åˆ’ï¼Œå­¦ä¹ å¦‚ä½•ä¸äººç±»æ¨¡å‹åä½œã€‚
- è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰ï¼šAIé€šè¿‡æ¨¡ä»¿äººç±»çš„è¡Œä¸ºæ¥å­¦ä¹ å¦‚ä½•ä¸äººç±»åä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœªåˆ©ç”¨äººç±»æ•°æ®è®­ç»ƒçš„AIåœ¨ä¸äººç±»æ¨¡å‹åä½œæ—¶è¡¨ç°è¾ƒå·®ï¼Œè€Œåˆ©ç”¨äººç±»æ•°æ®è®­ç»ƒçš„AIåˆ™è¡¨ç°æ›´å¥½ã€‚å…·ä½“æ¥è¯´ï¼Œä¸è‡ªæˆ‘åšå¼ˆå’ŒåŸºäºç¾¤ä½“çš„è®­ç»ƒç›¸æ¯”ï¼Œåˆ©ç”¨è¡Œä¸ºå…‹éš†è®­ç»ƒçš„AIåœ¨ä¸äººç±»æ¨¡å‹åä½œæ—¶è¡¨ç°æ›´ä½³ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è§„åˆ’æˆ–å¼ºåŒ–å­¦ä¹ æ¥æœ€å¤§åŒ–åä½œå¥–åŠ±çš„AIä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„åä½œæ•ˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è®­ç»ƒAIè¿›è¡Œåä½œæ—¶ï¼Œè€ƒè™‘äººç±»è¡Œä¸ºçš„é‡è¦æ€§ã€‚ä¸ºäº†å®ç°æ›´æœ‰æ•ˆçš„AIä¸äººç±»åä½œï¼Œå¯ä»¥é‡‡å–ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š
- ä½¿ç”¨è¡Œä¸ºå…‹éš†æˆ–å…¶ä»–æ¨¡ä»¿å­¦ä¹ æ–¹æ³•æ¥å­¦ä¹ äººç±»çš„è¡Œä¸ºã€‚
- åˆ©ç”¨è§„åˆ’æˆ–å¼ºåŒ–å­¦ä¹ æ¥æœ€å¤§åŒ–åä½œå¥–åŠ±ã€‚
- è®¾è®¡æ›´å¤æ‚çš„AIæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„è¡Œä¸ºå’Œæ„å›¾ã€‚
- åœ¨æµ‹è¯•æ—¶è®©AIé€‚åº”äººç±»çš„è¡Œä¸ºï¼Œä¾‹å¦‚ä½¿ç”¨å…ƒå­¦ä¹ ç®—æ³•æ¥å¿«é€Ÿé€‚åº”æ–°çš„åä½œä¼™ä¼´ã€‚

### ğŸ“š å‚è€ƒæ–‡çŒ®
[1] Ng, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning (pp. 663-670).

[2] Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey. Journal of artificial intelligence research, 4, 237-285.

[3] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484-489.

[4] Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... & Horgan, D. (2017). StarCraft II: A New Challenge for Reinforcement Learning. arXiv preprint arXiv:1708.04782.

[5] Tampuu, A., Matiisen, T., Kuzovkin, I., Arjakov, D., Maini, J., & Rucklidge, W. J. (2017). Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1706.02275.

[6] Wang, Z., Schaul, T., Hessel, M., Hasselt, H. V., Lanctot, M., & Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581.

[7] Choudhury, R., Dragan, A., & Seshia, S. A. (2018). Learning human models from demonstration. arXiv preprint arXiv:1804.04287.

[8] Lerer, A., & Peysakhovich, A. (2018). Learning to play against opponents with unknown strategies. arXiv preprint arXiv:1805.09358.

[9] Wang, T., & Tamar, A. (2018). Learning to communicate with deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.01262.

[10] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018). Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1802.09416.

[11] Finn, C., Abbeel, P., & Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1126-1135).

[12] Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Imagineer: A general framework for imagining and planning. arXiv preprint arXiv:1806.05696.

[13] Overcooked. (2016). Ghost Town Games.

## text-based-adventures-of-the-golovin-ai-agent
### Abstract
The domain of text-based adventure games has been recently established as a
new challenge of creating the agent that is both able to understand natural
language, and acts intelligently in text-described environments.
  In this paper, we present our approach to tackle the problem. Our agent,
named Golovin, takes advantage of the limited game domain. We use genre-related
corpora (including fantasy books and decompiled games) to create language
models suitable to this domain. Moreover, we embed mechanisms that allow us to
specify, and separately handle, important tasks as fighting opponents, managing
inventory, and navigating on the game map.
  We validated usefulness of these mechanisms, measuring agent's performance on
the set of 50 interactive fiction games. Finally, we show that our agent plays
on a level comparable to the winner of the last year Text-Based Adventure AI
Competition.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Golovin AI Agentï¼šæ–‡æœ¬å†’é™©æ¸¸æˆä¸­çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œæ™ºèƒ½è¡ŒåŠ¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬å†’é™©æ¸¸æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼šå¦‚ä½•åˆ›å»ºä¸€ä¸ªæ—¢èƒ½ç†è§£è‡ªç„¶è¯­è¨€ï¼Œåˆèƒ½åœ¨æ–‡æœ¬æè¿°çš„ç¯å¢ƒä¸­æ™ºèƒ½è¡ŒåŠ¨çš„æ™ºèƒ½ä½“ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–äºå¯¹æ¸¸æˆè§„åˆ™çš„åˆ†æå’Œç‰¹å®šé¢†åŸŸçš„ç‰¹å¾åˆ©ç”¨ï¼Œä½†ç¼ºä¹å¯¹è‡ªç„¶è¯­è¨€ç†è§£çš„æ·±å…¥æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨æœ‰é™çš„æ¸¸æˆé¢†åŸŸ
Golovin AI Agent åˆ©ç”¨æ–‡æœ¬å†’é™©æ¸¸æˆçš„æœ‰é™é¢†åŸŸï¼Œé€šè¿‡åˆ†æç›¸å…³è¯­æ–™åº“ï¼ˆåŒ…æ‹¬å¥‡å¹»ä¹¦ç±å’Œåç¼–è¯‘æ¸¸æˆï¼‰æ¥åˆ›å»ºé€‚åˆè¯¥é¢†åŸŸçš„è¯­è¨€æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåµŒå…¥ç‰¹å®šæœºåˆ¶
Golovin AI Agent åµŒå…¥äº†ç‰¹å®šæœºåˆ¶ï¼Œä»¥åˆ†åˆ«å¤„ç†é‡è¦çš„ä»»åŠ¡ï¼Œå¦‚ä¸å¯¹æ‰‹æˆ˜æ–—ã€ç®¡ç†åº“å­˜å’Œåœ¨æ¸¸æˆåœ°å›¾ä¸Šå¯¼èˆªã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆ©ç”¨æ¸¸æˆç‰¹å®šè¡Œä¸º
Golovin AI Agent åˆ©ç”¨æ¸¸æˆç‰¹å®šè¡Œä¸ºï¼Œå¦‚æˆ˜æ–—æ¨¡å¼ã€è£…å¤‡ç®¡ç†å’Œç§»åŠ¨ç­–ç•¥ï¼Œä»¥æé«˜å…¶åœ¨æ¸¸æˆä¸­çš„è¡¨ç°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè®°å¿†å’Œåˆ©ç”¨æ¸¸æˆå†å²
Golovin AI Agent è®°å¿†å¹¶åˆ©ç”¨æ¸¸æˆå†å²çš„ä¸€äº›æ–¹é¢ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ¸¸æˆç¯å¢ƒå’Œåšå‡ºå†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šæ¨¡ä»¿äººç±»è¡Œä¸º
Golovin AI Agent å°è¯•æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨æ¢ç´¢æ¸¸æˆå®‡å®™åé‡å¤æœ€æœ‰å¸Œæœ›çš„å‘½ä»¤åºåˆ—ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Golovin AI Agent åœ¨ 50 ä¸ªäº¤äº’å¼å°è¯´æ¸¸æˆä¸Šçš„è¡¨ç°è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå…¶è¡¨ç°ä¸å»å¹´æ–‡æœ¬å†’é™© AI ç«èµ›çš„è·èƒœè€…ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Golovin AI Agent çš„è®¾è®¡æ€è·¯å’Œæ–¹æ³•ä¸ºæ–‡æœ¬å†’é™©æ¸¸æˆä¸­çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œæ™ºèƒ½è¡ŒåŠ¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯ä»¥å€Ÿé‰´åˆ°å…¶ä»–ç±»ä¼¼åœºæ™¯ä¸­ï¼Œä¾‹å¦‚è™šæ‹ŸåŠ©æ‰‹ã€èŠå¤©æœºå™¨äººç­‰ã€‚

## interactive-fiction-games--a-colossal-adventure
### Abstract
A hallmark of human intelligence is the ability to understand and communicate
with language. Interactive Fiction games are fully text-based simulation
environments where a player issues text commands to effect change in the
environment and progress through the story. We argue that IF games are an
excellent testbed for studying language-based autonomous agents. In particular,
IF games combine challenges of combinatorial action spaces, language
understanding, and commonsense reasoning. To facilitate rapid development of
language-based agents, we introduce Jericho, a learning environment for
man-made IF games and conduct a comprehensive study of text-agents across a
rich set of games, highlighting directions in which agents can improve.
### ğŸŒŸ è®ºæ–‡è§£è¯» | äº¤äº’å¼å°è¯´æ¸¸æˆï¼šä¸€åœºå·¨å¤§çš„å†’é™©

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»æ™ºèƒ½çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹å¾æ˜¯ç†è§£å’Œç”¨è¯­è¨€è¿›è¡Œäº¤æµçš„èƒ½åŠ›ã€‚äº¤äº’å¼å°è¯´ï¼ˆIFï¼‰æ¸¸æˆæ˜¯å®Œå…¨åŸºäºæ–‡æœ¬çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç©å®¶é€šè¿‡å‘å‡ºæ–‡æœ¬å‘½ä»¤æ¥æ”¹å˜ç¯å¢ƒå¹¶æ¨åŠ¨æ•…äº‹çš„å‘å±•ã€‚æœ¬æ–‡è®¤ä¸ºï¼ŒIFæ¸¸æˆæ˜¯ç ”ç©¶åŸºäºè¯­è¨€çš„è‡ªä¸»ä»£ç†çš„ç»ä½³æµ‹è¯•å¹³å°ã€‚ç‰¹åˆ«æ˜¯ï¼ŒIFæ¸¸æˆç»“åˆäº†ç»„åˆåŠ¨ä½œç©ºé—´ã€è¯­è¨€ç†è§£å’Œå¸¸è¯†æ¨ç†çš„æŒ‘æˆ˜ã€‚ä¸ºäº†ä¿ƒè¿›åŸºäºè¯­è¨€çš„ä»£ç†çš„å¿«é€Ÿå¼€å‘ï¼Œæœ¬æ–‡ä»‹ç»äº†Jerichoï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºäººå·¥IFæ¸¸æˆçš„å­¦ä¹ ç¯å¢ƒï¼Œå¹¶åœ¨ä¸€ç³»åˆ—ä¸°å¯Œçš„æ¸¸æˆä¸­å¯¹æ–‡æœ¬ä»£ç†è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œçªå‡ºäº†ä»£ç†å¯ä»¥æ”¹è¿›çš„æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šJerichoç¯å¢ƒ
Jerichoæ˜¯ä¸€ä¸ªå¼€æºçš„Python-based IFç¯å¢ƒï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç±»ä¼¼äºOpenAI-Gymçš„æ¥å£ï¼Œä½¿å­¦ä¹ ä»£ç†èƒ½å¤Ÿè¿æ¥åˆ°IFæ¸¸æˆã€‚Jerichoæ—¨åœ¨ç”¨äºå¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œä½†ä¹Ÿæ”¯æŒåŠ è½½å’Œä¿å­˜æ¸¸æˆçŠ¶æ€çš„åŠŸèƒ½ï¼Œä½¿è§„åˆ’ç®—æ³•å¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ä»¥åŠä¾èµ–äºæ¢å¤çŠ¶æ€çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¦‚Backplayå’ŒGoExploreæˆä¸ºå¯èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæ¨¡æ¿çš„åŠ¨ä½œç”Ÿæˆ
æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºæ¨¡æ¿çš„åŠ¨ä½œç©ºé—´ï¼Œå…¶ä¸­ä»£ç†é¦–å…ˆé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œæ¨¡æ¿ï¼ˆä¾‹å¦‚ï¼Œå°†_æ”¾å…¥_ï¼‰ï¼Œç„¶åä½¿ç”¨è§£æå™¨çš„è¯æ±‡è¡¨ä¸­çš„å•è¯å¡«å†™ç©ºç™½ã€‚è¿™ç§æ¨¡æ¿åŒ–çš„åŠ¨ä½œç©ºé—´å¤§å¤§å‡å°‘äº†åŠ¨ä½œç©ºé—´çš„å¤æ‚æ€§ï¼Œä½¿å¾—è¯­è¨€ç”Ÿæˆå˜å¾—æ›´åŠ å®¹æ˜“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†DRRNã€TDQNå’ŒNAILä¸‰ç§ä»£ç†åœ¨32ä¸ªJerichoæ”¯æŒçš„æ¸¸æˆä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨è®¸å¤šä¸åŒçš„IFæ¸¸æˆä¸­æ˜¯å¯è¡Œçš„ã€‚ä¸éšæœºä»£ç†ç›¸æ¯”ï¼ŒDRRNå’ŒTDQNåœ¨æ¸¸æˆä¸­çš„å¾—åˆ†æ›´é«˜ï¼Œè¿™è¡¨æ˜äº†è¯­è¨€ç”Ÿæˆçš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œä¸NAILç›¸æ¯”ï¼ŒDRRNå’ŒTDQNåœ¨æ¸¸æˆä¸­çš„å¾—åˆ†ä»ç„¶è¾ƒä½ï¼Œè¿™è¡¨æ˜äº†å·¥ç¨‹ä¸€ä¸ªé€šç”¨çš„IFä»£ç†çš„éš¾åº¦ä»¥åŠä»æ•°æ®ä¸­å­¦ä¹ ç­–ç•¥çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Jerichoç¯å¢ƒå’ŒåŸºäºæ¨¡æ¿çš„åŠ¨ä½œç”Ÿæˆæ–¹æ³•ä¸ºç ”ç©¶åŸºäºè¯­è¨€çš„è‡ªä¸»ä»£ç†æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹DRRNã€TDQNå’ŒNAILä¸‰ç§ä»£ç†çš„è¯„ä¼°ç»“æœä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination
### Abstract
AI agents powered by Large Language Models (LLMs) have made significant
advances, enabling them to assist humans in diverse complex tasks and leading
to a revolution in human-AI coordination. LLM-powered agents typically require
invoking LLM APIs and employing artificially designed complex prompts, which
results in high inference latency. While this paradigm works well in scenarios
with minimal interactive demands, such as code generation, it is unsuitable for
highly interactive and real-time applications, such as gaming. Traditional
gaming AI often employs small models or reactive policies, enabling fast
inference but offering limited task completion and interaction abilities. In
this work, we consider Overcooked as our testbed where players could
communicate with natural language and cooperate to serve orders. We propose a
Hierarchical Language Agent (HLA) for human-AI coordination that provides both
strong reasoning abilities while keeping real-time execution. In particular,
HLA adopts a hierarchical framework and comprises three modules: a proficient
LLM, referred to as Slow Mind, for intention reasoning and language
interaction, a lightweight LLM, referred to as Fast Mind, for generating macro
actions, and a reactive policy, referred to as Executor, for transforming macro
actions into atomic actions. Human studies show that HLA outperforms other
baseline agents, including slow-mind-only agents and fast-mind-only agents,
with stronger cooperation abilities, faster responses, and more consistent
language communications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†å±‚è¯­è¨€ä»£ç†ï¼šå®æ—¶äººæœºåä½œçš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼ŒåŸºäºLLMsçš„AIä»£ç†åœ¨è¾…åŠ©äººç±»å®Œæˆå¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¨åŠ¨äº†äººæœºåä½œçš„é©å‘½ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†é€šå¸¸éœ€è¦è°ƒç”¨LLMs APIå¹¶ä½¿ç”¨äººå·¥è®¾è®¡çš„å¤æ‚æç¤ºï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿé«˜ã€‚è¿™ç§èŒƒå¼åœ¨äº¤äº’éœ€æ±‚è¾ƒä½çš„åœºæ™¯ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦å®æ—¶å“åº”å’Œé«˜é¢‘äº¤äº’çš„åº”ç”¨ï¼ˆå¦‚æ¸¸æˆï¼‰ä¸­å¹¶ä¸é€‚ç”¨ã€‚ä¼ ç»Ÿçš„æ¸¸æˆAIé€šå¸¸é‡‡ç”¨å°å‹æ¨¡å‹æˆ–ååº”ç­–ç•¥ï¼Œè™½ç„¶èƒ½å¤Ÿå®ç°å¿«é€Ÿæ¨ç†ï¼Œä½†ä»»åŠ¡å®Œæˆå’Œäº¤äº’èƒ½åŠ›æœ‰é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ†å±‚è¯­è¨€ä»£ç†ï¼ˆHLAï¼‰ï¼Œç”¨äºå®æ—¶äººæœºåä½œï¼Œè¯¥ä»£ç†ç»“åˆäº†å¤§å‹æ¨¡å‹çš„å¼ºå¤§æ¨ç†å’Œäº¤äº’èƒ½åŠ›ä»¥åŠå°å‹æ¨¡å‹å’Œååº”ç­–ç•¥çš„å®æ—¶æ¨ç†èƒ½åŠ›ã€‚HLAé‡‡ç”¨åˆ†å±‚æ¡†æ¶ï¼Œç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼š

* **æ…¢æ€ç»´ï¼ˆSlow Mindï¼‰**ï¼šä¸€ä¸ªç†Ÿç»ƒçš„LLMï¼Œç”¨äºæ„å›¾æ¨ç†å’Œè¯­è¨€äº¤äº’ã€‚
* **å¿«æ€ç»´ï¼ˆFast Mindï¼‰**ï¼šä¸€ä¸ªè½»é‡çº§çš„LLMï¼Œç”¨äºç”Ÿæˆå®æ“ä½œã€‚
* **æ‰§è¡Œå™¨ï¼ˆExecutorï¼‰**ï¼šä¸€ä¸ªååº”ç­–ç•¥ï¼Œç”¨äºå°†å®æ“ä½œè½¬æ¢ä¸ºåŸå­æ“ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Overcookedæ¸¸æˆå¹³å°ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHLAåœ¨æ¸¸æˆå¾—åˆ†ã€å“åº”å»¶è¿Ÿå’Œäººç±»åå¥½æ–¹é¢å‡ä¼˜äºå…¶ä»–åŸºçº¿ä»£ç†ï¼ŒåŒ…æ‹¬ä»…ä½¿ç”¨æ…¢æ€ç»´æˆ–å¿«æ€ç»´çš„ä»£ç†ã€‚HLAå±•ç°å‡ºæ›´å¼ºçš„åä½œèƒ½åŠ›ã€æ›´å¿«çš„å“åº”é€Ÿåº¦å’Œæ›´ä¸€è‡´çš„è¯­è¨€é€šä¿¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **åˆ†å±‚è®¾è®¡**ï¼šHLAçš„åˆ†å±‚è®¾è®¡æœ‰æ•ˆåœ°è§£å†³äº†LLMsæ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œä½¿å…¶é€‚ç”¨äºå®æ—¶äººæœºåä½œåœºæ™¯ã€‚
* **è½»é‡çº§LLM**ï¼šä½¿ç”¨è½»é‡çº§LLMè¿›è¡Œå®æ“ä½œç”Ÿæˆï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶é¿å…äº†ç”Ÿæˆæ¬¡ä¼˜æ“ä½œã€‚
* **ååº”ç­–ç•¥**ï¼šæ‰§è¡Œå™¨æ¨¡å—ç¡®ä¿äº†åŠ¨ä½œçš„å¯è¡Œæ€§å’Œé«˜é¢‘äº¤äº’ï¼Œæé«˜äº†AIä»£ç†çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚

### ğŸŒŸ æ€»ç»“
HLAä¸ºå®æ—¶äººæœºåä½œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶åˆ†å±‚è®¾è®¡æœ‰æ•ˆåœ°è§£å†³äº†LLMsæ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨éœ€è¦å®æ—¶å“åº”å’Œé«˜é¢‘äº¤äº’çš„åœºæ™¯ä¸­å‘æŒ¥ä½œç”¨ã€‚HLAåœ¨äººæœºåä½œé¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œæœ‰æœ›æ¨åŠ¨äººæœºåä½œçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## q-cogni--an-integrated-causal-reinforcement-learning-framework
### Abstract
We present Q-Cogni, an algorithmically integrated causal reinforcement
learning framework that redesigns Q-Learning with an autonomous causal
structure discovery method to improve the learning process with causal
inference. Q-Cogni achieves optimal learning with a pre-learned structural
causal model of the environment that can be queried during the learning process
to infer cause-and-effect relationships embedded in a state-action space. We
leverage on the sample efficient techniques of reinforcement learning, enable
reasoning about a broader set of policies and bring higher degrees of
interpretability to decisions made by the reinforcement learning agent. We
apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against
state-of-the-art reinforcement learning algorithms. We report results that
demonstrate better policies, improved learning efficiency and superior
interpretability of the agent's decision making. We also compare this approach
with traditional shortest-path search algorithms and demonstrate the benefits
of our causal reinforcement learning framework to high dimensional problems.
Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New
York City using the Taxi & Limousine Commission trip record data and compare
with shortest-path search, reporting results that show 85% of the cases with an
equal or better policy derived from Q-Cogni in a real-world domain.


## virtualhome--simulating-household-activities-via-programs
### Abstract
In this paper, we are interested in modeling complex activities that occur in
a typical household. We propose to use programs, i.e., sequences of atomic
actions and interactions, as a high level representation of complex tasks.
Programs are interesting because they provide a non-ambiguous representation of
a task, and allow agents to execute them. However, nowadays, there is no
database providing this type of information. Towards this goal, we first
crowd-source programs for a variety of activities that happen in people's
homes, via a game-like interface used for teaching kids how to code. Using the
collected dataset, we show how we can learn to extract programs directly from
natural language descriptions or from videos. We then implement the most common
atomic (inter)actions in the Unity3D game engine, and use our programs to
"drive" an artificial agent to execute tasks in a simulated household
environment. Our VirtualHome simulator allows us to create a large activity
video dataset with rich ground-truth, enabling training and testing of video
understanding models. We further showcase examples of our agent performing
tasks in our VirtualHome based on language descriptions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VirtualHomeï¼šé€šè¿‡ç¨‹åºæ¨¡æ‹Ÿå®¶åº­æ´»åŠ¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„å‘å±•ï¼Œè®©æœºå™¨äººæ‰§è¡Œå¤æ‚çš„å®¶åº­æ´»åŠ¨æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æè¿°å’Œæ‰§è¡Œè¿™äº›æ´»åŠ¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º VirtualHome çš„æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨é€šè¿‡ç¨‹åºæ¥æ¨¡æ‹Ÿå®¶åº­æ´»åŠ¨ï¼Œä»è€Œä¸ºæœºå™¨äººæ‰§è¡Œå¤æ‚ä»»åŠ¡æä¾›ä¸€ç§æ–°çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå®¶åº­æ´»åŠ¨çŸ¥è¯†åº“
æœ¬æ–‡é¦–å…ˆé€šè¿‡ä¼—åŒ…çš„æ–¹å¼æ”¶é›†äº†å¤§é‡çš„å®¶åº­æ´»åŠ¨æè¿°ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç¨‹åºå½¢å¼ã€‚è¿™äº›ç¨‹åºåŒ…å«äº†æ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„å…¨éƒ¨æ­¥éª¤ï¼ŒåŒ…æ‹¬ä¸€äº›å¸¸è¯†æ€§æ­¥éª¤ï¼Œä»è€Œä¸ºæœºå™¨äººæä¾›äº†æ¸…æ™°çš„æ‰§è¡ŒæŒ‡å—ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘ VirtualHome æ¨¡æ‹Ÿå™¨
æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªåä¸º VirtualHome çš„ 3D æ¨¡æ‹Ÿå™¨ï¼Œå¯ä»¥æ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒä¸­çš„å„ç§æ´»åŠ¨ã€‚é€šè¿‡å°†ç¨‹åºè¾“å…¥åˆ° VirtualHome ä¸­ï¼Œå¯ä»¥ç”Ÿæˆä¸°å¯Œçš„æ´»åŠ¨è§†é¢‘æ•°æ®é›†ï¼Œå¹¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•è§†é¢‘ç†è§£æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»æ–‡æœ¬å’Œè§†é¢‘ä¸­ç”Ÿæˆç¨‹åº
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäº seq2seq æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥ä»è‡ªç„¶è¯­è¨€æè¿°æˆ–è§†é¢‘æ¼”ç¤ºä¸­è‡ªåŠ¨ç”Ÿæˆç¨‹åºã€‚è¿™ä½¿å¾—æœºå™¨äººå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æˆ–è§†é¢‘æ¼”ç¤ºæ¥å­¦ä¹ æ‰§è¡Œæ–°çš„ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ VirtualHome æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä»æ–‡æœ¬å’Œè§†é¢‘ä¸­ç”Ÿæˆçš„ç¨‹åºå¯ä»¥æœ‰æ•ˆåœ°é©±åŠ¨æœºå™¨äººæ‰§è¡Œå„ç§å®¶åº­æ´»åŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†ä¸€é¡¹äººç±»ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„ç¨‹åºä¸äººç±»å¯¹æ´»åŠ¨çš„ç†è§£å…·æœ‰è¾ƒé«˜çš„ç›¸å…³æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ VirtualHome æ¨¡æ‹Ÿå™¨å’Œç¨‹åºç”Ÿæˆæ–¹æ³•ä¸ºæœºå™¨äººæ‰§è¡Œå¤æ‚ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ”¶é›†çš„å®¶åº­æ´»åŠ¨çŸ¥è¯†åº“å’Œè§†é¢‘æ•°æ®é›†ä¹Ÿä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚

## grounding-language-with-visual-affordances-over-unstructured-data
### Abstract
Recent works have shown that Large Language Models (LLMs) can be applied to
ground natural language to a wide variety of robot skills. However, in
practice, learning multi-task, language-conditioned robotic skills typically
requires large-scale data collection and frequent human intervention to reset
the environment or help correcting the current policies. In this work, we
propose a novel approach to efficiently learn general-purpose
language-conditioned robot skills from unstructured, offline and reset-free
data in the real world by exploiting a self-supervised visuo-lingual affordance
model, which requires annotating as little as 1% of the total data with
language. We evaluate our method in extensive experiments both in simulated and
real-world robotic tasks, achieving state-of-the-art performance on the
challenging CALVIN benchmark and learning over 25 distinct visuomotor
manipulation tasks with a single policy in the real world. We find that when
paired with LLMs to break down abstract natural language instructions into
subgoals via few-shot prompting, our method is capable of completing
long-horizon, multi-tier tasks in the real world, while requiring an order of
magnitude less data than previous approaches. Code and videos are available at
http://hulc2.cs.uni-freiburg.de
### ğŸŒŸ è®ºæ–‡è§£è¯» | HULC++ï¼šåˆ©ç”¨è§†è§‰è¯­è¨€äº²å’ŒåŠ›æ¨¡å‹é«˜æ•ˆå­¦ä¹ è¯­è¨€æ¡ä»¶æœºå™¨äººæŠ€èƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€ä¸æœºå™¨äººæŠ€èƒ½ç›¸ç»“åˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå­¦ä¹ å¤šä»»åŠ¡ã€è¯­è¨€æ¡ä»¶çš„æœºå™¨äººæŠ€èƒ½é€šå¸¸éœ€è¦å¤§è§„æ¨¡çš„æ•°æ®æ”¶é›†å’Œé¢‘ç¹çš„äººå·¥å¹²é¢„æ¥é‡ç½®ç¯å¢ƒæˆ–å¸®åŠ©çº æ­£å½“å‰ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è‡ªç›‘ç£çš„è§†è§‰è¯­è¨€äº²å’ŒåŠ›æ¨¡å‹ï¼Œä»ç°å®ä¸–ç•Œä¸­çš„éç»“æ„åŒ–ã€ç¦»çº¿å’Œæ— éœ€é‡ç½®çš„æ•°æ®ä¸­é«˜æ•ˆåœ°å­¦ä¹ é€šç”¨è¯­è¨€æ¡ä»¶çš„æœºå™¨äººæŠ€èƒ½ï¼Œè¿™åªéœ€è¦å¯¹æ€»æ•°æ®ä¸­çš„1%è¿›è¡Œè¯­è¨€æ ‡æ³¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šHULC++ æ¶æ„
æœ¬æ–‡æå‡ºäº† HULC++ æ¶æ„ï¼Œå®ƒç»“åˆäº† HULC çš„ä»»åŠ¡æ— å…³æ§åˆ¶å’Œ VAPO çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¯­ä¹‰ç†è§£ã€‚HULC æ˜¯ä¸€ç§æœ€å…ˆè¿›çš„è¯­è¨€æ¡ä»¶æ¨¡ä»¿å­¦ä¹ ä»£ç†ï¼Œå¯ä»¥ç«¯åˆ°ç«¯åœ°å­¦ä¹  7-DoF ç›®æ ‡è¾¾åˆ°ç­–ç•¥ã€‚VAPO ä»éç»“æ„åŒ–æ•°æ®ä¸­æå–è‡ªç›‘ç£çš„è§†è§‰äº²å’ŒåŠ›æ¨¡å‹ï¼Œä¸ä»…å¯ä»¥åŠ é€Ÿå­¦ä¹ ï¼Œè¿˜å¯ä»¥æé«˜ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªç›‘ç£è§†è§‰è¯­è¨€äº²å’ŒåŠ›æ¨¡å‹
æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£çš„è§†è§‰è¯­è¨€äº²å’ŒåŠ›æ¨¡å‹ï¼Œå¯ä»¥ä»éç»“æ„åŒ–çš„äººç±»è¿œç¨‹æ“ä½œæ•°æ®ä¸­è‡ªåŠ¨æå–äº²å’ŒåŠ›ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æŠ“å–åŠ¨ä½œä½œä¸ºå¯å‘å¼æ–¹æ³•ï¼Œå‘ç°åœºæ™¯ä¸­ä¸ä»»åŠ¡å®Œæˆç›¸å…³çš„å…ƒç´ ã€‚é€šè¿‡å°†æœ«ç«¯æ‰§è¡Œå™¨ä¸–ç•Œä½ç½®æŠ•å½±åˆ°ç›¸æœºå›¾åƒä¸­ï¼Œå¹¶ä½¿ç”¨æŠ“å–åŠ¨ä½œä½œä¸ºç›‘ç£ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ é¢„æµ‹ä¸ä»»åŠ¡ç›¸å…³çš„å¯¹è±¡çš„åƒç´ ä½ç½®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šä»»åŠ¡ 7-DoF è¯­è¨€æ¡ä»¶è§†è§‰è¿åŠ¨ç­–ç•¥
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡ 7-DoF è¯­è¨€æ¡ä»¶è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäº HULC å¹¶ä»éç»“æ„åŒ–æ•°æ®ä¸­è®­ç»ƒã€‚è¯¥ç­–ç•¥å¯ä»¥åœ¨é¢„æµ‹çš„äº²å’ŒåŠ›åŒºåŸŸé™„è¿‘ä¸åœºæ™¯è¿›è¡Œäº¤äº’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šä¸ LLMs çš„ç»“åˆ
æœ¬æ–‡æå‡ºäº†ä¸€ç§å°† HULC++ ä¸ LLMs ç»“åˆçš„æ–¹æ³•ï¼Œä»¥å°†æŠ½è±¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—å¯è¡Œçš„å­ä»»åŠ¡ã€‚LLMs å¯ä»¥é€šè¿‡å°‘é‡æ ·æœ¬æç¤ºå°†æŠ½è±¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç¿»è¯‘æˆä¸€ç³»åˆ—å­ç›®æ ‡ï¼Œä»è€Œå®ç°é•¿è·ç¦»ã€å¤šçº§ä»»åŠ¡çš„æ‰§è¡Œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒHULC++ åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ CALVIN åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨å•ä¸ªç­–ç•¥åœ¨ç°å®ä¸–ç•Œä¸­å­¦ä¹ äº†è¶…è¿‡ 25 ä¸ªä¸åŒçš„è§†è§‰è¿åŠ¨æ“ä½œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå½“ä¸ LLMs ç»“åˆä½¿ç”¨æ—¶ï¼ŒHULC++ èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œä¸­å®Œæˆé•¿è·ç¦»ã€å¤šçº§ä»»åŠ¡ï¼Œè€Œæ‰€éœ€çš„æ•°æ®é‡æ¯”ä»¥å‰çš„æ–¹æ³•å°‘ä¸€ä¸ªæ•°é‡çº§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ HULC++ æ¶æ„å’Œè‡ªç›‘ç£è§†è§‰è¯­è¨€äº²å’ŒåŠ›æ¨¡å‹ä¸ºé«˜æ•ˆå­¦ä¹ è¯­è¨€æ¡ä»¶æœºå™¨äººæŠ€èƒ½æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§æœºå™¨äººä»»åŠ¡ï¼Œå¹¶å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- **æ•°æ®æ•ˆç‡é«˜**ï¼šåªéœ€è¦å¯¹æ€»æ•°æ®ä¸­çš„1%è¿›è¡Œè¯­è¨€æ ‡æ³¨ã€‚
- **æ— éœ€é‡ç½®ç¯å¢ƒ**ï¼šå¯ä»¥ä»éç»“æ„åŒ–ã€ç¦»çº¿å’Œæ— éœ€é‡ç½®çš„æ•°æ®ä¸­å­¦ä¹ ã€‚
- **æ³›åŒ–èƒ½åŠ›å¼º**ï¼šåœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚
- **å¯æ‰©å±•æ€§å¼º**ï¼šå¯ä»¥ä¸ LLMs ç»“åˆä½¿ç”¨ï¼Œä»¥å®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚

### ğŸŒŸ æ€»ç»“
HULC++ æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥é«˜æ•ˆåœ°å­¦ä¹ è¯­è¨€æ¡ä»¶æœºå™¨äººæŠ€èƒ½ã€‚è¯¥æ–¹æ³•å…·æœ‰æ•°æ®æ•ˆç‡é«˜ã€æ— éœ€é‡ç½®ç¯å¢ƒã€æ³›åŒ–èƒ½åŠ›å¼ºå’Œå¯æ‰©å±•æ€§å¼ºç­‰ä¼˜åŠ¿ï¼Œæœ‰æœ›åœ¨æœªæ¥çš„æœºå™¨äººåº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars
### Abstract
Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå†å²æˆ˜äº‰ï¼Œæ¢ç´¢å’Œå¹³çš„å¯èƒ½æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æˆ˜äº‰ä¸å’Œå¹³æ˜¯äººç±»å†å²æ°¸æ’çš„ä¸»é¢˜ï¼Œç†è§£æˆ˜äº‰çš„åŸå› å’Œé¢„é˜²æˆ˜äº‰çš„å‘ç”Ÿä¸€ç›´æ˜¯äººç±»è¿½æ±‚çš„ç›®æ ‡ã€‚ä¼ ç»Ÿçš„æˆ˜äº‰ç ”ç©¶æ–¹æ³•ä¸»è¦ä¾èµ–äºå†å²åˆ†æå’Œæ–‡çŒ®å›é¡¾ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å—é™äºé™æ€è§†è§’å’Œäº‹åè¯¸è‘›äº®çš„åè§ã€‚éšç€äººå·¥æ™ºèƒ½å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬æœ‰æœºä¼šåˆ©ç”¨è¿™äº›å…ˆè¿›æŠ€æœ¯æ¥æ¨¡æ‹Ÿå†å²äº‹ä»¶ï¼Œæ¢ç´¢æˆ˜äº‰ä¸å’Œå¹³çš„åŠ¨æ€è¿‡ç¨‹ï¼Œå¹¶ä¸ºå†²çªè§£å†³å’Œå’Œå¹³ç»´æŠ¤æä¾›æ–°çš„è§†è§’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº† WarAgentï¼Œä¸€ä¸ªåŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ AI ç³»ç»Ÿï¼Œç”¨äºæ¨¡æ‹Ÿå†å²å›½é™…å†²çªï¼ŒåŒ…æ‹¬ç¬¬ä¸€æ¬¡ä¸–ç•Œå¤§æˆ˜ï¼ˆWWIï¼‰ã€ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ˜ï¼ˆWWIIï¼‰å’Œä¸­å›½å¤ä»£æˆ˜å›½æ—¶æœŸï¼ˆWSPï¼‰ã€‚WarAgent é€šè¿‡æ¨¡æ‹Ÿå‚ä¸å›½å®¶çš„å†³ç­–è¿‡ç¨‹å’Œäº’åŠ¨ï¼Œæ¢ç´¢äº†ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š

* **æ¨¡æ‹Ÿæœ‰æ•ˆæ€§**ï¼šLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦æœ‰æ•ˆåœ°å¤åˆ¶å†å²äº‹ä»¶ä¸­æˆ˜ç•¥è§„åˆ’å’Œå†³ç­–è¿‡ç¨‹çš„æ¼”å˜ï¼Ÿ
* **æˆ˜äº‰èµ·å› **ï¼šå“ªäº›å› ç´ æ˜¯å¯¼è‡´æˆ˜äº‰çˆ†å‘çš„å…³é”®å› ç´ ï¼ŸLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦è¯†åˆ«è¿™äº›å› ç´ ï¼Ÿ
* **æˆ˜äº‰ä¸å¯é¿å…æ€§**ï¼šå†å²ä¸Šçš„æˆ˜äº‰æ˜¯å¦çœŸçš„ä¸å¯é¿å…ï¼ŸLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦æ­ç¤ºå¯¼è‡´æˆ˜äº‰ï¼ˆæˆ–å’Œå¹³ï¼‰çš„æ¡ä»¶ï¼Ÿ

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWarAgent èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå†å²äº‹ä»¶ï¼Œå¹¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¤åˆ¶å†å²å†³ç­–è¿‡ç¨‹å’Œäº’åŠ¨ã€‚ä¾‹å¦‚ï¼Œåœ¨ WWI æ¨¡æ‹Ÿä¸­ï¼ŒWarAgent èƒ½å¤Ÿé‡ç°ä¸»è¦å›½å®¶ä¹‹é—´çš„è”ç›Ÿå½¢æˆã€æˆ˜äº‰å®£è¨€å’ŒåŠ¨å‘˜æƒ…å†µï¼Œä¸å†å²äº‹ä»¶å…·æœ‰è¾ƒé«˜çš„å»åˆåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜è§¦å‘äº‹ä»¶å’Œå›½å®¶çš„åˆå§‹æ¡ä»¶ï¼ŒWarAgent èƒ½å¤Ÿæ¢ç´¢ä¸åŒçš„æˆ˜äº‰èµ·å› å’Œæˆ˜äº‰ä¸å¯é¿å…æ€§ï¼Œä¸ºç†è§£å†å²äº‹ä»¶å’Œé¢„é˜²æœªæ¥å†²çªæä¾›äº†æ–°çš„è§†è§’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨**ï¼šWarAgent ä¸º LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¸ºç†è§£å¤æ‚çš„äººç±»è¡Œä¸ºå’Œç¤¾ä¼šåŠ¨æ€æä¾›äº†æ–°çš„å·¥å…·ã€‚
* **å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å†²çªè§£å†³ä¸­çš„åº”ç”¨**ï¼šWarAgent çš„è®¾è®¡ç†å¿µå¯ä»¥ä¸ºå†²çªè§£å†³å’Œå’Œå¹³ç»´æŠ¤æä¾›æ–°çš„æ€è·¯ï¼Œä¾‹å¦‚é€šè¿‡æ¨¡æ‹Ÿä¸åŒæ”¿ç­–çš„å½±å“æ¥è¯„ä¼°å†²çªè§£å†³ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
* **å†å²æ•™å­¦çš„æ–°æ–¹æ³•**ï¼šWarAgent å¯ä»¥ä½œä¸ºä¸€ç§æ–°çš„å†å²æ•™å­¦æ–¹æ³•ï¼Œå¸®åŠ©å­¦ç”Ÿå’Œæ•™å¸ˆæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œå¹¶ç†è§£å†å²äº‹ä»¶çš„å¤æ‚å› æœå…³ç³»ã€‚

### ğŸŒŸ æœªæ¥å±•æœ›
WarAgent çš„ç ”ç©¶ä¸º LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„é“è·¯ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š

* **æ—¶é—´é©±åŠ¨æ¨¡æ‹Ÿ**ï¼šå°† WarAgent çš„å›åˆåˆ¶æ¨¡æ‹Ÿæ‰©å±•ä¸ºæ—¶é—´é©±åŠ¨æ¨¡æ‹Ÿï¼Œä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå†å²äº‹ä»¶çš„æ—¶é—´åŠ¨æ€ã€‚
* **åœæ­¢æ¡ä»¶**ï¼šç ”ç©¶æ›´æœ‰æ•ˆçš„åœæ­¢æ¡ä»¶ï¼Œä»¥æ›´æ¸…æ™°åœ°ç»“æŸæ¨¡æ‹Ÿå¹¶åˆ†æç»“æœã€‚
* **æ–°çš„ç ”ç©¶é—®é¢˜**ï¼šæ¢ç´¢æ›´å¤šä¸å†å²äº‹ä»¶å’Œå†²çªè§£å†³ç›¸å…³çš„ç ”ç©¶é—®é¢˜ï¼Œä¾‹å¦‚å¤–äº¤æ²Ÿé€šä¸å†²çªå¯èƒ½æ€§ä¹‹é—´çš„å…³ç³»ã€éå›½å®¶è¡Œä¸ºä½“å¯¹åœ°ç¼˜æ”¿æ²»çš„å½±å“ç­‰ã€‚

é€šè¿‡ä¸æ–­æ”¹è¿›å’Œæ‰©å±• WarAgentï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£å†å²äº‹ä»¶ï¼Œå¹¶ä¸ºæ„å»ºæ›´åŠ å’Œå¹³çš„æœªæ¥æä¾›æ–°çš„æ€è·¯ã€‚

## language-models-meet-world-models--embodied-experiences-enhance-language-models
### Abstract
While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical (EWC) for
selective weight updates, combined with low-rank adapters (LoRA) for training
efficiency. Extensive experiments show our approach substantially improves base
LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs
(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much
larger LMs (e.g., ChatGPT).
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹ä¸ä¸–ç•Œè§‚æ¨¡å‹ç›¸é‡ï¼šå…·èº«ç»éªŒå¢å¼ºè¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ç‰©ç†ç¯å¢ƒä¸­çš„ç®€å•æ¨ç†å’Œè§„åˆ’æ–¹é¢å¾€å¾€å­˜åœ¨å›°éš¾ï¼Œä¾‹å¦‚ç†è§£ç‰©ä½“æ’å¸¸æ€§æˆ–è§„åˆ’å®¶åº­æ´»åŠ¨ã€‚è¿™ç§å±€é™æ€§æºäºLMsä»…é€šè¿‡ä¹¦é¢æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œç¼ºä¹å¿…è¦çš„å…·èº«çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šE2WMè®­ç»ƒèŒƒå¼
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œå³ä½¿ç”¨æ¥è‡ªä¸–ç•Œè§‚æ¨¡å‹çš„å…·èº«ç»éªŒå¯¹LMsè¿›è¡Œå¾®è°ƒï¼ˆE2WMï¼‰ã€‚ä¸–ç•Œè§‚æ¨¡å‹æ˜¯å…·èº«æ¨¡æ‹Ÿå™¨ï¼Œå¯ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„ç‰©ç†äº¤äº’ï¼Œä¸ºLMsæä¾›ç†è§£ç¯å¢ƒä¸­çš„ç‰©ä½“äº¤äº’å’Œæ‰§è¡ŒåŠ¨ä½œçš„æœºä¼šã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¶é›†å…·èº«ç»éªŒ
ä¸ºäº†å°†ä¸åŒçš„å…·èº«çŸ¥è¯†æ³¨å…¥LMsï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸¤ç§æ”¶é›†ç»éªŒçš„æ–¹æ³•ï¼šç›®æ ‡å¯¼å‘è§„åˆ’å’Œéšæœºæ¢ç´¢ã€‚ç›®æ ‡å¯¼å‘è§„åˆ’æ—¨åœ¨æ”¶é›†ä¸è§„åˆ’å’Œç›®æ ‡å¯¼å‘ä»£ç†è¡Œä¸ºç›¸å…³çš„ç»éªŒï¼Œè€Œéšæœºæ¢ç´¢åˆ™ä¸“æ³¨äºç§¯ç´¯æ¶‰åŠç‰©ä½“å’Œä¸–ç•ŒçŠ¶æ€è·Ÿè¸ªçš„ç»éªŒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEWC-LoRAå¾®è°ƒ
ä¸ºäº†åœ¨æ”¶é›†çš„å…·èº«ç»éªŒä¸Šå¾®è°ƒLMsï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„é€šç”¨çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†å°†ç»å…¸çš„å¼¹æ€§æƒé‡æ•´åˆï¼ˆEWCï¼‰ä¸ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰ç›¸ç»“åˆçš„æ–¹æ³•ã€‚EWCé€šè¿‡æ­£åˆ™åŒ–å¾®è°ƒæŸå¤±æ¥ä¿ç•™é‡è¦çš„LMå‚æ•°ï¼Œè€ŒLoRAåˆ™é€šè¿‡åœ¨æ¯ä¸ªæ¨¡å‹å±‚ä¸­æ³¨å…¥ä¸¤ä¸ªå¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µæ¥å®ç°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨18ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†åŸºçº¿LMsçš„æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†64.28%ã€‚ç‰¹åˆ«æ˜¯ï¼Œé€šè¿‡æœ¬æ–‡æ–¹æ³•å¢å¼ºçš„å°å‹LMsï¼ˆ1.3Bã€6Bå’Œ13Bï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº†æ›´å¤§çš„LMsï¼ˆä¾‹å¦‚ChatGPTï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„E2WMè®­ç»ƒèŒƒå¼ä¸ºå¢å¼ºLMsçš„å…·èº«çŸ¥è¯†å’ŒæŠ€èƒ½æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨ä¸–ç•Œè§‚æ¨¡å‹æ”¶é›†å…·èº«ç»éªŒï¼Œå¹¶ç»“åˆEWC-LoRAè¿›è¡Œå¾®è°ƒï¼ŒLMså¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå¤„ç†ç‰©ç†ç¯å¢ƒä¸­çš„ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç¤ºäº†LMsåœ¨å…·èº«ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

## llama-rider--spurring-large-language-models-to-explore-the-open-world
### Abstract
Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round feedback-revision mechanism is
utilized to encourage LLMs to actively select appropriate revision actions
guided by feedback information from the environment. This facilitates
exploration and enhances the model's performance. Besides, we integrate
sub-task relabeling to assist LLMs in maintaining consistency in sub-task
planning and help the model learn the combinatorial nature between tasks,
enabling it to complete a wider range of tasks through training based on the
acquired exploration experiences. By evaluation in Minecraft, an open-ended
sandbox world, we demonstrate that our approach LLaMA-Rider enhances the
efficiency of the LLM in exploring the environment, and effectively improves
the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k
instances of collected data, showing minimal training costs compared to the
baseline using reinforcement learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLaMA Riderï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ¢ç´¢å¼€æ”¾ä¸–ç•Œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è®¸å¤šç ”ç©¶å¼€å§‹åˆ©ç”¨LLMsçš„èƒ½åŠ›æ¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ï¼Œå¹¶å‘ç°LLMså…·æœ‰ä¸€å®šçš„è§„åˆ’å’Œå®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMsçš„çŸ¥è¯†æ¥æºäºé¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯­è¨€è¯­æ–™åº“ï¼Œå¯èƒ½ä¸ç‰¹å®šç¯å¢ƒå­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä¸€äº›ç ”ç©¶é€šè¿‡æç¤ºå·¥ç¨‹è®¾è®¡ç‰¹å®šæœºåˆ¶ï¼Œä¸ºLLMsæä¾›ç¯å¢ƒä¿¡æ¯ã€‚ç„¶è€Œï¼ŒLLMsåœ¨ç¯å¢ƒä¸­å¹¶ä¸ä¼šæ”¹è¿›æˆ–è·å–æ–°çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦æ›´å¤æ‚çš„æœºåˆ¶å’Œæç¤ºï¼Œè¿™ä¼šå¯¼è‡´LLMsç”Ÿæˆæˆæœ¬é«˜ï¼Œå¹¶ä¸”ä¾èµ–äºåƒGPT-4è¿™æ ·å…·æœ‰è¶³å¤ŸçŸ¥è¯†çš„å¼ºå¤§æ¨¡å‹ã€‚è¿˜æœ‰ä¸€äº›ç ”ç©¶é€šè¿‡å¾®è°ƒæ¥å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä½†è¿™é€šå¸¸éœ€è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¹Ÿè¢«ç ”ç©¶ï¼Œä½†è¿™äº›æ–¹æ³•å°†LLMsè®­ç»ƒä¸ºç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°RLæ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹æˆ–æ›´å¤æ‚çš„ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLaMA-Riderçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡LLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢æ¥å¢å¼ºå…¶èƒ½åŠ›ã€‚LLaMA-Rideræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¢ç´¢é˜¶æ®µå’Œå­¦ä¹ é˜¶æ®µã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢é˜¶æ®µ
åœ¨æ¢ç´¢é˜¶æ®µï¼ŒLLaMA-Rideråˆ©ç”¨åé¦ˆ-ä¿®æ­£æœºåˆ¶æ¥é¼“åŠ±LLMsä¸»åŠ¨é€‰æ‹©é€‚å½“çš„ä¿®æ­£åŠ¨ä½œï¼Œä»¥é€‚åº”ç¯å¢ƒã€‚LLMsåœ¨ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ï¼Œæ”¶é›†ç»éªŒï¼Œå¹¶é€šè¿‡åé¦ˆä¿¡æ¯æ¥æ”¹è¿›å…¶å†³ç­–ã€‚æ­¤å¤–ï¼ŒLLaMA-Riderè¿˜ä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥å¸®åŠ©LLMsä¿æŒå­ä»»åŠ¡è§„åˆ’çš„è¿è´¯æ€§ï¼Œå¹¶å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§è´¨ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå­¦ä¹ é˜¶æ®µ
åœ¨å­¦ä¹ é˜¶æ®µï¼ŒLLaMA-Riderå°†æ”¶é›†åˆ°çš„ç»éªŒå¤„ç†æˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒLLMsã€‚é™¤äº†ä»æˆåŠŸä»»åŠ¡ä¸­è·å¾—çš„ç»éªŒå¤–ï¼ŒLLaMA-Riderè¿˜æ”¶é›†éƒ¨åˆ†å®Œæˆçš„å­ä»»åŠ¡çš„ç»éªŒï¼Œå› ä¸ºæœ‰äº›ä»»åŠ¡åœ¨æ¢ç´¢é˜¶æ®µå¾ˆéš¾å®Œæˆã€‚å¼€æ”¾ç¯å¢ƒä¸­çš„è®¸å¤šä»»åŠ¡é€šå¸¸å…·æœ‰ç»„åˆæ€§ï¼Œè¿™æ„å‘³ç€è¿‡å»ä»»åŠ¡çš„ç»éªŒå¯ä»¥ç»å¸¸å¸®åŠ©å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚LLaMA-Riderä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¸®åŠ©LLMså­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Minecraftæ¨¡æ‹Ÿå™¨MineDojoä¸Šè¯„ä¼°äº†LLaMA-Rideræ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMA-Riderèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒï¼Œå¹¶é€šè¿‡å¾®è°ƒä»…ä½¿ç”¨1.3kä¸ªæ”¶é›†åˆ°çš„æ•°æ®å®ä¾‹æ¥æé«˜LLMså®Œæˆä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬æ›´ä½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LLaMA-Rideræ–¹æ³•ä¸ºLLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢å’Œå­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å…¶åé¦ˆ-ä¿®æ­£æœºåˆ¶å’Œå­ä»»åŠ¡é‡æ ‡è®°æŠ€æœ¯å¯ä»¥å¸®åŠ©LLMsæ›´å¥½åœ°é€‚åº”ç¯å¢ƒï¼Œå¹¶æé«˜å…¶å®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLLaMA-Rideræ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¼€æ”¾ç¯å¢ƒï¼Œå¹¶å…·æœ‰ç»ˆèº«æ¢ç´¢å’Œå­¦ä¹ çš„æ½œåŠ›ã€‚

## large-sequence-models-for-sequential-decision-making--a-survey
### Abstract
Transformer architectures have facilitated the development of large-scale and
general-purpose sequence models for prediction tasks in natural language
processing and computer vision, e.g., GPT-3 and Swin Transformer. Although
originally designed for prediction problems, it is natural to inquire about
their suitability for sequential decision-making and reinforcement learning
problems, which are typically beset by long-standing issues involving sample
efficiency, credit assignment, and partial observability. In recent years,
sequence models, especially the Transformer, have attracted increasing interest
in the RL communities, spawning numerous approaches with notable effectiveness
and generalizability. This survey presents a comprehensive overview of recent
works aimed at solving sequential decision-making tasks with sequence models
such as the Transformer, by discussing the connection between sequential
decision-making and sequence modeling, and categorizing them based on the way
they utilize the Transformer. Moreover, this paper puts forth various potential
avenues for future research intending to improve the effectiveness of large
sequence models for sequential decision-making, encompassing theoretical
foundations, network architectures, algorithms, and efficient training systems.
As this article has been accepted by the Frontiers of Computer Science, here is
an early version, and the most up-to-date version can be found at
https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹åºåˆ—æ¨¡å‹åœ¨é¡ºåºå†³ç­–ä¸­çš„æ½œåŠ›ï¼šç»¼è¿°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¤§å‹åºåˆ—æ¨¡å‹ï¼Œå°¤å…¶æ˜¯Transformeræ¶æ„ï¼Œå› å…¶å¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„é¡ºåºå†³ç­–å’Œå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå¦‚æ ·æœ¬æ•ˆç‡ã€ä¿¡ç”¨åˆ†é…å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ç­‰é—®é¢˜ï¼Œä¸€ç›´å›°æ‰°ç€è¯¥é¢†åŸŸçš„å‘å±•ã€‚æœ¬æ–‡ç»¼è¿°äº†è¿‘å¹´æ¥åˆ©ç”¨Transformerç­‰åºåˆ—æ¨¡å‹è§£å†³é¡ºåºå†³ç­–é—®é¢˜çš„ç ”ç©¶è¿›å±•ï¼Œå¹¶æ¢è®¨äº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†é¡ºåºå†³ç­–é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—å»ºæ¨¡é—®é¢˜
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå°†é¡ºåºå†³ç­–é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—å»ºæ¨¡é—®é¢˜ï¼Œåˆ©ç”¨åºåˆ—æ¨¡å‹ï¼ˆå¦‚Transformerï¼‰æ¥å¤„ç†ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­å­˜åœ¨çš„æ ·æœ¬æ•ˆç‡ã€ä¿¡ç”¨åˆ†é…å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨Transformeræ¶æ„çš„ä¼˜åŠ¿
Transformeræ¶æ„å…·æœ‰é«˜å¹¶è¡ŒåŒ–ã€å¯æ‰©å±•æ€§ã€é€‚å½“çš„å½’çº³åç½®ç­‰ä¼˜åŠ¿ï¼Œä½¿å…¶æˆä¸ºè§£å†³é¡ºåºå†³ç­–é—®é¢˜çš„ç†æƒ³é€‰æ‹©ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†Transformeræ¶æ„åœ¨é¡ºåºå†³ç­–ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç¦»çº¿å¼ºåŒ–å­¦ä¹ ã€åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ã€å…ƒå¼ºåŒ–å­¦ä¹ ã€å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ å’Œæ™ºèƒ½ä½“æ¶æ„ç­‰æ–¹é¢ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ç»¼è¿°äº†å¤šä¸ªåŸºäºTransformerçš„é¡ºåºå†³ç­–æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡å’Œç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ ·æœ¬æ•ˆç‡ã€ä¿¡ç”¨åˆ†é…å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ç­‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºé¡ºåºå†³ç­–é—®é¢˜çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚å…·ä½“è€Œè¨€ï¼Œæœªæ¥ç ”ç©¶å¯ä»¥å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

* **ç†è®ºåŸºç¡€çš„å®Œå–„**ï¼š è¿›ä¸€æ­¥ç ”ç©¶åºåˆ—å»ºæ¨¡æ–¹æ³•ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ç»“åˆï¼Œä¸ºæ”¿ç­–ä¼˜åŒ–æä¾›ç†è®ºä¿è¯ã€‚
* **ç½‘ç»œæ¶æ„çš„æ”¹è¿›**ï¼š å¼€å‘é’ˆå¯¹é¡ºåºå†³ç­–ä»»åŠ¡çš„å®šåˆ¶Transformeræ¶æ„ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚
* **ç®—æ³•çš„ç»Ÿä¸€æ¡†æ¶**ï¼š å»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ¶µç›–å„ç§é¡ºåºå†³ç­–åœºæ™¯ï¼Œå¹¶æœ‰æ•ˆåœ°æ•´åˆå¤šæ¨¡æ€çŸ¥è¯†ã€‚
* **é«˜æ•ˆè®­ç»ƒç³»ç»Ÿçš„è®¾è®¡**ï¼š è®¾è®¡é«˜æ•ˆçš„è®­ç»ƒç³»ç»Ÿï¼Œä»¥æ”¯æŒå¤§å‹å†³ç­–æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€‚

### æ€»ç»“
æœ¬æ–‡ç»¼è¿°äº†å¤§å‹åºåˆ—æ¨¡å‹åœ¨é¡ºåºå†³ç­–ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚éšç€åºåˆ—æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬æœ‰ç†ç”±ç›¸ä¿¡ï¼Œå¤§å‹å†³ç­–æ¨¡å‹å°†åœ¨æœªæ¥å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œå¹¶ä¸ºè§£å†³å„ç§å¤æ‚çš„é¡ºåºå†³ç­–é—®é¢˜æä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning
### Abstract
Recent works successfully leveraged Large Language Models' (LLM) abilities to
capture abstract knowledge about world's physics to solve decision-making
problems. Yet, the alignment between LLMs' knowledge and the environment can be
wrong and limit functional competence due to lack of grounding. In this paper,
we study an approach (named GLAM) to achieve this alignment through functional
grounding: we consider an agent using an LLM as a policy that is progressively
updated as the agent interacts with the environment, leveraging online
Reinforcement Learning to improve its performance to solve goals. Using an
interactive textual environment designed to study higher-level forms of
functional grounding, and a set of spatial and navigation tasks, we study
several scientific questions: 1) Can LLMs boost sample efficiency for online
learning of various RL tasks? 2) How can it boost different forms of
generalization? 3) What is the impact of online learning? We study these
questions by functionally grounding several variants (size, architecture) of
FLAN-T5.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GLAMï¼šåˆ©ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ å°†å¤§å‹è¯­è¨€æ¨¡å‹åµŒå…¥äº¤äº’å¼ç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€ç”Ÿæˆã€é—®ç­”ã€æ¨ç†å’Œç¿»è¯‘ç­‰ã€‚ç„¶è€Œï¼ŒLLMåœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„åŠŸèƒ½èƒ½åŠ›å—é™ï¼Œä¸»è¦åŸå› æ˜¯å…¶ç¼ºä¹å¯¹ç¯å¢ƒçš„â€œæ¥åœ°â€ï¼ˆgroundingï¼‰ï¼Œå³LLMçš„çŸ¥è¯†ä¸ç¯å¢ƒçš„ç‰©ç†è§„åˆ™å’ŒåŠ¨æ€ä¹‹é—´çš„å¯¹é½ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åŠŸèƒ½æ¥åœ°çš„æ–¹æ³•ï¼Œå°†LLMåµŒå…¥äº¤äº’å¼ç¯å¢ƒï¼Œå¹¶åˆ©ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æé«˜å…¶æ€§èƒ½ï¼Œä»è€Œè§£å†³å†³ç­–é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†GLAMï¼ˆGrounded Language Modelsï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†LLMä½œä¸ºæ™ºèƒ½ä½“ç­–ç•¥ï¼Œåœ¨äº¤äº’å¼æ–‡æœ¬ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡åœ¨çº¿RLä¸æ–­æ›´æ–°LLMçš„çŸ¥è¯†ï¼Œä»¥å®ç°åŠŸèƒ½æ¥åœ°ã€‚å…·ä½“æ¥è¯´ï¼ŒGLAMæ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1. **ç¯å¢ƒè®¾è®¡**ï¼šæœ¬æ–‡å°†BabyAIç¯å¢ƒæ”¹ç¼–ä¸ºæ–‡æœ¬ç‰ˆæœ¬ï¼ˆBabyAI-Textï¼‰ï¼Œå…¶ä¸­æ™ºèƒ½ä½“é€šè¿‡æ–‡æœ¬å‘½ä»¤è¿›è¡Œå¯¼èˆªå’Œäº¤äº’ã€‚
2. **LLMä½œä¸ºç­–ç•¥**ï¼šå°†LLMä½œä¸ºæ™ºèƒ½ä½“ç­–ç•¥ï¼Œé€šè¿‡æ¥æ”¶ä»»åŠ¡æè¿°ã€å½“å‰è§‚å¯Ÿå’Œå¯èƒ½åŠ¨ä½œçš„æç¤ºï¼Œè¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚
3. **åœ¨çº¿RLè®­ç»ƒ**ï¼šåˆ©ç”¨ç¯å¢ƒæä¾›çš„å¥–åŠ±ä¿¡å·ï¼Œä½¿ç”¨PPOç®—æ³•å¯¹LLMè¿›è¡Œåœ¨çº¿RLè®­ç»ƒï¼Œä»¥ä¼˜åŒ–å…¶ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å®ç°ç›®æ ‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨BabyAI-Textç¯å¢ƒä¸­è¿›è¡Œäº†å¤šé¡¹å®éªŒï¼Œç»“æœè¡¨æ˜GLAMæ–¹æ³•åœ¨ä»¥ä¸‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼š

1. **æ ·æœ¬æ•ˆç‡**ï¼šä¸é›¶æ ·æœ¬ä½¿ç”¨LLMã€ç›‘ç£å¾®è°ƒå’Œéé¢„è®­ç»ƒLLMçš„RLå¾®è°ƒç›¸æ¯”ï¼ŒGLAMæ–¹æ³•åœ¨è§£å†³ç©ºé—´å’Œå¯¼èˆªä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚
2. **æ³›åŒ–èƒ½åŠ›**ï¼šGLAMæ–¹æ³•è®­ç»ƒçš„æ™ºèƒ½ä½“èƒ½å¤Ÿå°†æ‰€å­¦æŠ€èƒ½æ³›åŒ–åˆ°æ–°çš„å¯¹è±¡å’Œä»»åŠ¡ï¼Œä¾‹å¦‚ä½¿ç”¨æœªè§è¿‡çš„åè¯æˆ–åŠ¨è¯ï¼Œä»¥åŠè§£å†³æ–°çš„ç»„åˆä»»åŠ¡ã€‚
3. **åœ¨çº¿å­¦ä¹ çš„å½±å“**ï¼šä¸ç¦»çº¿è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œåœ¨çº¿RLè®­ç»ƒèƒ½å¤Ÿæ›´å¥½åœ°æé«˜LLMçš„åŠŸèƒ½æ¥åœ°èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç¯å¢ƒçš„å˜åŒ–ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GLAMæ–¹æ³•ä¸ºå°†LLMåµŒå…¥äº¤äº’å¼ç¯å¢ƒå¹¶æé«˜å…¶åŠŸèƒ½èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

1. **LLMä½œä¸ºç­–ç•¥**ï¼šå°†LLMä½œä¸ºæ™ºèƒ½ä½“ç­–ç•¥ï¼Œå¯ä»¥åˆ©ç”¨å…¶å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜æ™ºèƒ½ä½“åœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚
2. **åœ¨çº¿RLè®­ç»ƒ**ï¼šé€šè¿‡åœ¨çº¿RLè®­ç»ƒï¼Œå¯ä»¥ä½¿LLMä¸æ–­å­¦ä¹ å’Œé€‚åº”ç¯å¢ƒçš„å˜åŒ–ï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
3. **æ–‡æœ¬ç¯å¢ƒè®¾è®¡**ï¼šæœ¬æ–‡æå‡ºçš„BabyAI-Textç¯å¢ƒä¸ºç ”ç©¶LLMçš„åŠŸèƒ½æ¥åœ°æä¾›äº†è‰¯å¥½çš„å¹³å°ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°LLMåœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºçš„GLAMæ–¹æ³•ä¸ºå°†LLMåµŒå…¥äº¤äº’å¼ç¯å¢ƒå¹¶æé«˜å…¶åŠŸèƒ½èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶LLMåœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚

## proximal-policy-optimization-algorithms
### Abstract
We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a "surrogate" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Proximal Policy Optimization Algorithmsï¼šç®€åŒ–å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸæ¶Œç°å‡ºå¤šç§åŸºäºç¥ç»ç½‘ç»œå‡½æ•°é€¼è¿‘å™¨çš„æ–¹æ³•ï¼Œå¦‚æ·±åº¦Qå­¦ä¹ ã€æ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’Œä¿¡ä»»åŒºåŸŸ/è‡ªç„¶ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ã€æ•°æ®æ•ˆç‡å’Œé²æ£’æ€§æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œæ·±åº¦Qå­¦ä¹ åœ¨è®¸å¤šç®€å•é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ•°æ®æ•ˆç‡ä½ä¸‹ä¸”é²æ£’æ€§å·®ï¼Œè€Œä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰ç›¸å¯¹å¤æ‚ï¼Œä¸”ä¸å…¼å®¹åŒ…å«å™ªå£°ï¼ˆå¦‚dropoutï¼‰æˆ–å‚æ•°å…±äº«ï¼ˆå¦‚ç­–ç•¥å’Œä»·å€¼å‡½æ•°ä¹‹é—´ï¼Œæˆ–ä¸è¾…åŠ©ä»»åŠ¡ä¹‹é—´ï¼‰çš„æ¶æ„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„æ–°ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚PPOå…·æœ‰ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤æ›¿é‡‡æ ·å’Œä¼˜åŒ–
PPOäº¤æ›¿åœ¨ç¯å¢ƒä¸­é‡‡æ ·æ•°æ®å¹¶é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œç„¶åä½¿ç”¨éšæœºæ¢¯åº¦ä¸Šå‡ä¼˜åŒ–ä¸€ä¸ªâ€œä»£ç†â€ç›®æ ‡å‡½æ•°ã€‚ä¸æ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¯æ•°æ®æ ·æœ¬æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦æ›´æ–°ä¸åŒï¼ŒPPOæå‡ºäº†ä¸€ç§æ–°çš„ç›®æ ‡å‡½æ•°ï¼Œå…è®¸è¿›è¡Œå¤šä¸ªepochçš„å°æ‰¹é‡æ›´æ–°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæˆªæ–­æ¦‚ç‡æ¯”
PPOä½¿ç”¨æˆªæ–­æ¦‚ç‡æ¯”æ¥å½¢æˆå¯¹ç­–ç•¥æ€§èƒ½çš„æ‚²è§‚ä¼°è®¡ï¼ˆå³ä¸‹ç•Œï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPPOèƒ½å¤Ÿåœ¨ä¿è¯ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§å’Œå¯é æ€§çš„åŒæ—¶ï¼Œç®€åŒ–å®ç°è¿‡ç¨‹ï¼Œä½¿å…¶æ›´é€šç”¨ï¼Œå¹¶æé«˜æ ·æœ¬å¤æ‚åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸€ç³»åˆ—åŸºå‡†ä»»åŠ¡ä¸Šæµ‹è¯•äº†PPOï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿæœºå™¨äººè¿åŠ¨å’ŒAtariæ¸¸æˆã€‚ç»“æœè¡¨æ˜ï¼ŒPPOä¼˜äºå…¶ä»–åœ¨çº¿ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå¹¶åœ¨æ ·æœ¬å¤æ‚åº¦ã€ç®€å•æ€§å’Œè¿è¡Œæ—¶é—´ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PPOæ˜¯ä¸€ç§ç®€å•ã€é«˜æ•ˆä¸”é²æ£’çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚å…¶åˆ›æ–°ç‚¹åŒ…æ‹¬äº¤æ›¿é‡‡æ ·å’Œä¼˜åŒ–ä»¥åŠæˆªæ–­æ¦‚ç‡æ¯”ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œä»¥æé«˜å…¶æ€§èƒ½å’Œé²æ£’æ€§ã€‚

## enhance-reasoning-for-large-language-models-in-the-game-werewolf
### Abstract
This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ï¼šä»¥ç‹¼äººæ€æ¸¸æˆä¸ºæ¡ˆä¾‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„çªç ´ï¼Œå…¶åœ¨æ¨ç†ã€è§„åˆ’å’Œå†³ç­–ç­‰é¢†åŸŸçš„æ½œåŠ›ä¹Ÿé€æ¸æ˜¾ç°ã€‚ç„¶è€Œï¼ŒLLMsåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œæ·±åº¦é€»è¾‘åˆ†æçš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å¤–éƒ¨æ¨ç†æ¨¡å—ï¼Œå³â€œæ€è€ƒè€…â€ï¼ˆThinkerï¼‰ï¼Œæ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒç³»ç»Ÿæ¨ç†æ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œå°†LLMsä¸å¤–éƒ¨Thinkeræ¨¡å—ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªæ¨ç†å±‚æ¬¡ç»“æ„ã€‚LLMsè´Ÿè´£å¤„ç†ç›´è§‚çš„System-1ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¸¸è¯†æ¨ç†ï¼Œè€ŒThinkeråˆ™ä¸“æ³¨äºéœ€è¦å¤æ‚é€»è¾‘åˆ†æå’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„System-2ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šThinkeræ¨¡å—çš„è®¾è®¡ä¸è®­ç»ƒ
Thinkeræ¨¡å—ç›´æ¥ä»æ•°æ®åº“ä¸­è·å–çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚å®ƒé€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºç¾¤ä½“çš„è®­ç»ƒç­‰æ–¹æ³•ï¼Œå­¦ä¹ ç”Ÿæˆåˆç†çš„æ¸¸æˆåŠ¨ä½œå’ŒLLMçš„è¯­éŸ³æŒ‡ä»¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é›†è´¡çŒ®
æœ¬æ–‡æ”¶é›†äº†18,800åœºçœŸå®äººç±»æ¸¸æˆä¼šè¯æ•°æ®ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç¤¾äº¤æ¨ç†æ¸¸æˆæ•°æ®é›†ï¼Œä¸ºç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥Thinkeræ¨¡å—æ˜¾è‘—æé«˜äº†LLMsçš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼ŒThinkeræ¨¡å—åœ¨æ¨ç†ã€è¯­éŸ³ç”Ÿæˆå’Œåœ¨çº¿æ¸¸æˆè¯„ä¼°æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Thinkerä¸ä¸€ä¸ªè¾ƒå°çš„LLMæ¨¡å‹ï¼ˆ6Bï¼‰è¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†GPT4ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡å°†LLMsä¸å¤–éƒ¨æ¨ç†æ¨¡å—ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆæå‡LLMsåœ¨ç‰¹å®šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†ä¹Ÿä¸ºç¤¾äº¤æ¨ç†æ¸¸æˆçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®åŸºç¡€ã€‚

## reward-design-with-language-models
### Abstract
Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä½¿ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œå¥–åŠ±è®¾è®¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„å¥–åŠ±è®¾è®¡ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºé€šè¿‡å¥–åŠ±å‡½æ•°æ¥æŒ‡å®šäººç±»æœŸæœ›çš„è¡Œä¸ºå¯èƒ½å¾ˆå›°éš¾ï¼Œæˆ–è€…éœ€è¦å¤§é‡çš„ä¸“å®¶æ¼”ç¤ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä»£ç†å¥–åŠ±å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢æä¾›å‡ ä¸ªç¤ºä¾‹æˆ–æè¿°æ¥æŒ‡å®šæœŸæœ›çš„è¡Œä¸ºï¼Œä»è€Œç®€åŒ–å¥–åŠ±è®¾è®¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨LLMä½œä¸ºä»£ç†å¥–åŠ±å‡½æ•°
æœ¬æ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬æç¤ºï¼ˆåŒ…å«å‡ ä¸ªç¤ºä¾‹æˆ–æè¿°ï¼‰ä½œä¸ºä»£ç†å¥–åŠ±å‡½æ•°ï¼Œè¯„ä¼°RLä»£ç†çš„è¡Œä¸ºæ˜¯å¦ç¬¦åˆç”¨æˆ·çš„ç›®æ ‡ï¼Œå¹¶è¾“å‡ºç›¸åº”çš„å¥–åŠ±ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šç”¨RLè®­ç»ƒæ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„RLè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä»£ç†å¥–åŠ±å‡½æ•°ï¼Œå¹¶ä¸æ‰€ä½¿ç”¨çš„RLç®—æ³•æ— å…³ã€‚ç”¨æˆ·åªéœ€åœ¨è®­ç»ƒå¼€å§‹æ—¶æŒ‡å®šä¸€æ¬¡æç¤ºï¼ŒLLMä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°ä»£ç†çš„è¡Œä¸ºï¼Œå¹¶è¾“å‡ºå¥–åŠ±ä¿¡å·ï¼Œä»£ç†ä½¿ç”¨è¯¥å¥–åŠ±ä¿¡å·æ›´æ–°å…¶è¡Œä¸ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šæœ€åé€šç‰’æ¸¸æˆã€çŸ©é˜µæ¸¸æˆå’ŒDealOrNoDealè°ˆåˆ¤ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LLMä½œä¸ºä»£ç†å¥–åŠ±å‡½æ•°è®­ç»ƒçš„RLä»£ç†ä¸ç”¨æˆ·çš„ç›®æ ‡é«˜åº¦ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸­éƒ½ä¼˜äºä½¿ç”¨ç›‘ç£å­¦ä¹ å­¦ä¹ å¥–åŠ±å‡½æ•°çš„RLä»£ç†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºç”¨æˆ·æä¾›äº†æ›´ç›´è§‚å’Œä¾¿æ·çš„æ–¹å¼æ¥æŒ‡å®šRLä»£ç†çš„ç›®æ ‡ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°è®­ç»ƒä¸ç”¨æˆ·ç›®æ ‡ä¸€è‡´çš„ä»£ç†ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§RLä»»åŠ¡ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨äººç±»å…¼å®¹å’Œä»·å€¼å¯¹é½çš„AIç³»ç»Ÿçš„å‘å±•ã€‚

## motif--intrinsic-motivation-from-artificial-intelligence-feedback
### Abstract
Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Motifï¼šä»äººå·¥æ™ºèƒ½åé¦ˆä¸­è·å–å†…åœ¨åŠ¨æœº

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œæ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æ™ºèƒ½ä½“æ¢ç´¢å’Œè¯„ä¼°å…¶è¡Œä¸ºæå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Motif çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸­çš„å…ˆéªŒçŸ¥è¯†ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ï¼Œä»è€Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨æ²¡æœ‰ä¸ç¯å¢ƒçš„ç›´æ¥äº¤äº’çš„æƒ…å†µä¸‹è¿›è¡Œå†³ç­–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ LLM çš„åå¥½æ„å»ºå†…åœ¨å¥–åŠ±
Motif çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡ LLM å¯¹äº‹ä»¶æ ‡é¢˜çš„åå¥½æ¥æ„å»ºå†…åœ¨å¥–åŠ±å‡½æ•°ï¼Œå¹¶å°†å…¶ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“ã€‚LLM è¡¨è¾¾å¯¹æˆå¯¹äº‹ä»¶æ ‡é¢˜çš„åå¥½ï¼Œè¿™äº›æ ‡é¢˜åªéœ€ç²—ç•¥æè¿°ç¯å¢ƒä¸­å‘ç”Ÿçš„äº‹ä»¶ï¼Œè€Œä¸éœ€è¦ç²¾ç»†çš„é€æ­¥æè¿°ã€‚LLM ä¸éœ€è¦ç†è§£ä½çº§åŠ¨ä½œç©ºé—´ï¼Œè¿™å¯èƒ½æ˜¯å¤åˆçš„æˆ–è¿ç»­çš„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå†…åœ¨å¥–åŠ±ä¸å¤–åœ¨å¥–åŠ±çš„ç»“åˆ
Motif çš„å†…åœ¨å¥–åŠ±å¯ä»¥å•ç‹¬ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¸æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±ä¿¡å·ç»“åˆä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å†…åœ¨å¥–åŠ±ä¸å¤–åœ¨å¥–åŠ±ç»“åˆä½¿ç”¨æ—¶ï¼ŒMotif çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ²¡æœ‰æ¼”ç¤ºçš„æƒ…å†µä¸‹å–å¾—äº†è¿›å±•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Motif åœ¨ NetHack å­¦ä¹ ç¯å¢ƒ (NLE) ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ã€å¼€æ”¾æ€§å’Œç¨‹åºç”Ÿæˆçš„æ¸¸æˆã€‚ç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡å­¦ä¹ æœ€å¤§åŒ–å…¶å†…åœ¨å¥–åŠ±ï¼ŒMotif å°±å–å¾—äº†æ¯”ç›´æ¥è®­ç»ƒä»¥æœ€å¤§åŒ–åˆ†æ•°çš„ç®—æ³•æ›´é«˜çš„æ¸¸æˆåˆ†æ•°ã€‚å½“å°† Motif çš„å†…åœ¨å¥–åŠ±ä¸ç¯å¢ƒçš„å¥–åŠ±ç›¸ç»“åˆæ—¶ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ²¡æœ‰æ¼”ç¤ºçš„æƒ…å†µä¸‹å–å¾—äº†è¿›å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Motif ä¸ºåˆ©ç”¨ LLM çš„å…ˆéªŒçŸ¥è¯†å’Œå¸¸è¯†æ¥åˆ›å»ºæ™ºèƒ½ä½“æä¾›äº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å°† LLM çš„é«˜å±‚æ¬¡çŸ¥è¯†ä¸æ™ºèƒ½ä½“æ“ä½œçš„åº•å±‚ä¼ æ„Ÿå™¨è¿åŠ¨ç°å®ä¹‹é—´çš„å·®è·ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†çŸ¥è¯†æç‚¼å‡ºæ¥ã€‚Motif å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ä¸æ›´å¤§è§„æ¨¡çš„ LLM æˆ–ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒ LLM ç»“åˆä½¿ç”¨ï¼Œå¹¶å¯ä»¥é€šè¿‡æç¤ºä¿®æ”¹è½»æ¾åœ°å¼•å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚

## auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft
### Abstract
Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Auto MC-Rewardï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°ï¼Œæå‡Minecraftä¸­å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Minecraft ç­‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒé€šå¸¸åªæä¾›ç¨€ç–å¥–åŠ±ï¼Œå³åªæœ‰ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥æ—¶æ‰ä¼šè·å¾—å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±æœºåˆ¶ä½¿å¾—å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ¢ç´¢æ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å­¦ä¹ å¤æ‚ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Auto MC-Rewardï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„å…ˆè¿›å­¦ä¹ ç³»ç»Ÿï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuto MC-Reward ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ã€‚å¥–åŠ±è®¾è®¡å™¨æ ¹æ®ç¯å¢ƒä¿¡æ¯å’Œä»»åŠ¡æè¿°ï¼Œé€šè¿‡ç¼–å†™å¯æ‰§è¡Œçš„ Python å‡½æ•°æ¥è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±è¯„è®ºå®¶è´Ÿè´£éªŒè¯ä»£ç ï¼Œæ£€æŸ¥ä»£ç æ˜¯å¦è‡ªæ´½ä¸”æ²¡æœ‰è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ã€‚è½¨è¿¹åˆ†æå™¨æ ¹æ®æ”¶é›†çš„è½¨è¿¹æ€»ç»“å¯èƒ½çš„å¤±è´¥åŸå› ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAuto MC-Reward åˆ©ç”¨ LLM çš„ä»»åŠ¡ç†è§£å’Œç»éªŒæ€»ç»“èƒ½åŠ›ï¼Œä¸ºå­¦ä¹ æä¾›è¯¦ç»†å’Œå³æ—¶çš„å¥–åŠ±æŒ‡å¯¼ã€‚å¥–åŠ±è®¾è®¡å™¨é¦–å…ˆæ ¹æ®ç¯å¢ƒå’Œä»»åŠ¡çš„åŸºæœ¬æè¿°ï¼Œä½¿ç”¨ LLM è®¾è®¡ä¸ä»»åŠ¡ç›¸å…³çš„å¯†é›†å¥–åŠ±å‡½æ•°ã€‚ç„¶åï¼Œå¥–åŠ±è¯„è®ºå®¶å¯¹è®¾è®¡çš„å¥–åŠ±å‡½æ•°è¿›è¡Œè‡ªæˆ‘éªŒè¯ã€‚ä¸ºäº†è§£å†³ LLM ç†è§£çš„æ½œåœ¨åå·®æˆ–ç–å¿½ï¼Œè¿˜æå‡ºäº†åŸºäº LLM çš„è½¨è¿¹åˆ†æå™¨ï¼Œç”¨äºåˆ†æå’Œæ€»ç»“è®­ç»ƒä»£ç†çš„è½¨è¿¹ï¼Œå¹¶å¸®åŠ©å¥–åŠ±è®¾è®¡å™¨æ”¹è¿›å¥–åŠ±å‡½æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Auto MC-Reward åœ¨ä¸€ç³»åˆ—ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬åœ°ä¸‹æ°´å¹³æ¢ç´¢é’»çŸ³å’Œæ¢ç´¢å¹³åŸç”Ÿç‰©ç¾¤è½ä¸­çš„æ ‘æœ¨å’ŒåŠ¨ç‰©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹ç¨€ç–å¥–åŠ±å’Œç°æœ‰å¯†é›†å¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒAuto MC-Reward åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ›´å¥½çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸Šé«˜æ•ˆå­¦ä¹ çš„å…ˆè¿›èƒ½åŠ›ã€‚é€šè¿‡è¿­ä»£æ”¹è¿›å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼ŒAuto MC-Reward ä½¿ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å¯¹æ–°ä»»åŠ¡æœ‰ç›Šçš„æ–°è¡Œä¸ºï¼Œä¾‹å¦‚é¿å…ç†”å²©ï¼Œä»è€Œå¤§å¤§æé«˜äº†æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward ä»…ä½¿ç”¨åŸå§‹ä¿¡æ¯å°±å®ç°äº†é«˜é’»çŸ³è·å–æˆåŠŸç‡ï¼ˆ36.5%ï¼‰ï¼Œè¯æ˜äº†å…¶è§£å†³é•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Auto MC-Reward ä¸ºè§£å†³ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­çš„æ¢ç´¢æ•ˆç‡é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶åˆ©ç”¨ LLM è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward çš„ä¸‰ä¸ªç»„ä»¶ï¼ˆå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ï¼‰å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä½¿å¾—æ•°æ®åˆ†æå’Œå¥–åŠ±å‡½æ•°æ›´æ–°æ›´åŠ çµæ´»ã€‚

## automated-feature-selection-for-inverse-reinforcement-learning
### Abstract
Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©åœ¨é€†å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰ä¸­ï¼Œä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æŒ‡å®šå¥–åŠ±å‡½æ•°æ—¢å›°éš¾åˆè€—æ—¶ï¼Œä¸”å®¹æ˜“å¼•å…¥é”™è¯¯å‡è®¾ã€‚æ­¤å¤–ï¼Œåœ¨è¿ç»­çŠ¶æ€ç©ºé—´ä¸­ï¼ŒçŠ¶æ€å˜é‡æœ¬èº«ä¸è¶³ä»¥ä½œä¸ºç‰¹å¾ï¼Œè€Œå“ªäº›ç‰¹å¾æ˜¯å¥½çš„é€šå¸¸å¹¶ä¸æ¸…æ¥šã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºé€†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±å‡½æ•°å­¦ä¹ ã€‚ä¸»è¦åˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨å¤šé¡¹å¼åŸºå‡½æ•°ä½œä¸ºå€™é€‰ç‰¹å¾é›†ï¼Œè¿™äº›å‡½æ•°èƒ½å¤ŸåŒ¹é…çŠ¶æ€åˆ†å¸ƒçš„ç»Ÿè®¡çŸ©ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•æ‰ä¸“å®¶ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘äº†ä¸€ç§åŸºäºç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©æœºåˆ¶ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾å­é›†ï¼Œä»¥å‡å°‘å¥–åŠ±å¤æ‚æ€§å¹¶å‡è½»å™ªå£°å’Œè™šå‡ç›¸å…³æ€§çš„å½±å“ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸‰ä¸ªä¸åŒçš„è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆæ‘†åŠ¨ã€æ¨è½¦å’ŒåŒè‡‚æœºå™¨äººï¼‰ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢å¤å¥–åŠ±å‡½æ•°ï¼Œå¹¶ç”Ÿæˆä¸ä¸“å®¶ç­–ç•¥ç›¸ä¼¼çš„æ”¿ç­–ã€‚ä¸æ‰‹åŠ¨é€‰æ‹©ç‰¹å¾ã€éšæœºé€‰æ‹©ç‰¹å¾ã€ç›´æ¥ä½¿ç”¨çŠ¶æ€ä½œä¸ºç‰¹å¾ä»¥åŠä½¿ç”¨æ‰€æœ‰å€™é€‰ç‰¹å¾çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸ºé€†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±å‡½æ•°å­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§è¿ç»­çŠ¶æ€ç©ºé—´ï¼Œå¹¶æœ‰åŠ©äºæé«˜æ¨¡å‹çš„è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–åŸºç¡€å‡½æ•°ï¼Œå¦‚å‚…é‡Œå¶çº§æ•°å’Œå¾„å‘åŸºå‡½æ•°ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶é€‚ç”¨æ€§å’Œç²¾åº¦ã€‚

## eureka--human-level-reward-design-via-coding-large-language-models
### Abstract
Large Language Models (LLMs) have excelled as high-level semantic planners
for sequential decision-making tasks. However, harnessing them to learn complex
low-level manipulation tasks, such as dexterous pen spinning, remains an open
problem. We bridge this fundamental gap and present Eureka, a human-level
reward design algorithm powered by LLMs. Eureka exploits the remarkable
zero-shot generation, code-writing, and in-context improvement capabilities of
state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over
reward code. The resulting rewards can then be used to acquire complex skills
via reinforcement learning. Without any task-specific prompting or pre-defined
reward templates, Eureka generates reward functions that outperform expert
human-engineered rewards. In a diverse suite of 29 open-source RL environments
that include 10 distinct robot morphologies, Eureka outperforms human experts
on 83% of the tasks, leading to an average normalized improvement of 52%. The
generality of Eureka also enables a new gradient-free in-context learning
approach to reinforcement learning from human feedback (RLHF), readily
incorporating human inputs to improve the quality and the safety of the
generated rewards without model updating. Finally, using Eureka rewards in a
curriculum learning setting, we demonstrate for the first time, a simulated
Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a
pen in circles at rapid speed.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Eurekaï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å®ç°äººç±»æ°´å¹³çš„å¥–åŠ±è®¾è®¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åºåˆ—å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å­¦ä¹ å¤æ‚ä½çº§æ“ä½œä»»åŠ¡ï¼ˆå¦‚çµå·§çš„ç¬”è½¬ï¼‰æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰çš„å°è¯•éœ€è¦å¤§é‡çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ¥æ„å»ºä»»åŠ¡æç¤ºæˆ–ä»…å­¦ä¹ ç®€å•æŠ€èƒ½ï¼Œè¿™å¯¼è‡´äº†å®ç°äººç±»æ°´å¹³çµå·§æ€§çš„å·¨å¤§å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šEurekaï¼Œä¸€ä¸ªç”±LLMsé©±åŠ¨çš„å¥–åŠ±è®¾è®¡ç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨GPT-4ç­‰æœ€å…ˆè¿›LLMsçš„é›¶æ ·æœ¬ç”Ÿæˆã€ä»£ç ç¼–å†™å’Œåœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ”¹è¿›èƒ½åŠ›ï¼Œå¯¹å¥–åŠ±ä»£ç è¿›è¡Œè¿›åŒ–ä¼˜åŒ–ã€‚ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°å¯ä»¥ç”¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ è·å¾—å¤æ‚æŠ€èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šEurekaåœ¨29ä¸ªå¼€æºRLç¯å¢ƒä¸­ï¼ˆåŒ…æ‹¬10ç§ä¸åŒçš„æœºå™¨äººå½¢æ€ï¼‰è¡¨ç°å‡ºè‰²ï¼Œåœ¨83%çš„ä»»åŠ¡ä¸Šä¼˜äºäººç±»ä¸“å®¶è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œå¹³å‡æ ‡å‡†åŒ–æ”¹è¿›ä¸º52%ã€‚Eurekaçš„é€šç”¨æ€§è¿˜ä½¿å…¶èƒ½å¤Ÿå®ç°ä¸€ç§æ–°çš„æ— æ¢¯åº¦ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œæ— éœ€æ¨¡å‹æ›´æ–°å³å¯è½»æ¾çº³å…¥äººç±»è¾“å…¥ï¼Œä»¥æé«˜ç”Ÿæˆå¥–åŠ±çš„è´¨é‡å’Œå®‰å…¨æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEurekaåœ¨è¯¾ç¨‹å­¦ä¹ ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œé¦–æ¬¡å±•ç¤ºäº†èƒ½å¤Ÿæ‰§è¡Œç¬”è½¬æŠ€å·§çš„æ¨¡æ‹ŸShadow Handï¼Œèƒ½å¤Ÿå¿«é€Ÿåœ°æ“çºµç¬”è¿›è¡Œåœ†å‘¨è¿åŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Eurekaåœ¨29ä¸ªå¼€æºRLç¯å¢ƒä¸­è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬10ç§ä¸åŒçš„æœºå™¨äººå½¢æ€ï¼ŒåŒ…æ‹¬å››è¶³åŠ¨ç‰©ã€å››æ—‹ç¿¼é£æœºã€åŒè¶³åŠ¨ç‰©ã€æ“ä½œå™¨å’Œå‡ ç§çµå·§çš„æ‰‹ã€‚Eurekaåœ¨83%çš„ä»»åŠ¡ä¸Šä¼˜äºäººç±»ä¸“å®¶ï¼Œå¹³å‡æ ‡å‡†åŒ–æ”¹è¿›ä¸º52%ã€‚æ­¤å¤–ï¼ŒEurekaè¿˜æˆåŠŸåœ°è§£å†³äº†çµå·§æ“ä½œä»»åŠ¡ï¼Œä¾‹å¦‚ç¬”è½¬ï¼Œè¿™æ˜¯ä»¥å‰æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹æ— æ³•å®ç°çš„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Eurekaå±•ç¤ºäº†LLMsåœ¨å¥–åŠ±è®¾è®¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºè§£å†³å¤æ‚ä½çº§æ“ä½œä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯ã€‚Eurekaçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºæœªæ¥å¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„é‡è¦å·¥å…·ã€‚

## lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions
### Abstract
Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä½æˆæœ¬å®æ—¶ç¤¾äº¤äº’åŠ¨çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼šLyfe Agents

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç¤¾ä¼šä¸­æ¨¡æ‹Ÿå¤æ‚ç¤¾äº¤è¡Œä¸ºçš„å‰æ™¯ä¹Ÿå˜å¾—å…‰æ˜ã€‚ç„¶è€Œï¼Œå®ç°ä¸äººç±»åœ¨ä½è®¡ç®—æˆæœ¬ä¸‹çš„å®æ—¶äº’åŠ¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨åˆ›å»ºä¸€ç§æ—¢æ™ºèƒ½åˆè‡ªä¸»çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨ä½è®¡ç®—æˆæœ¬ä¸‹å®ç°ä¸äººç±»çš„å®æ—¶äº’åŠ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€‰é¡¹-åŠ¨ä½œæ¡†æ¶
ä¸ºäº†å‡å°‘é«˜çº§å†³ç­–çš„æˆæœ¬ï¼ŒLyfe Agents é‡‡ç”¨äº†ä¸€ç§é€‰é¡¹-åŠ¨ä½œæ¡†æ¶ã€‚åœ¨è¿™ç§æ¡†æ¶ä¸­ï¼Œæ™ºèƒ½ä½“é¦–å…ˆé€‰æ‹©ä¸€ä¸ªé«˜çº§åŠ¨ä½œï¼ˆæˆ–â€œé€‰é¡¹â€ï¼‰ï¼Œç„¶ååœ¨åç»­æ­¥éª¤ä¸­åœ¨è¯¥é€‰é¡¹å†…é€‰æ‹©ä½çº§åŠ¨ä½œã€‚è¿™ç§è®¾è®¡å…è®¸æ™ºèƒ½ä½“åœ¨æ›´é•¿æ—¶é—´å†…ä¸“æ³¨äºæ‰§è¡Œé€‰é¡¹èƒŒåçš„æ„å›¾ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼‚æ­¥è‡ªæˆ‘ç›‘æ§
ä¸ºäº†æé«˜æ™ºèƒ½ä½“çš„æƒ…å¢ƒæ„è¯†å’Œç›®æ ‡åšæŒæ€§ï¼ŒLyfe Agents å¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘ç›‘æ§æ¨¡å—ã€‚è¯¥æ¨¡å—ç»´æŠ¤ä¸€ä¸ªå…³äºæœ€è¿‘äº‹ä»¶çš„å™äº‹é£æ ¼æ‘˜è¦ï¼Œå¹¶å¼ºè°ƒæ–°é¢–å’Œä¸ç›®æ ‡ç›¸å…³çš„å†…å®¹ã€‚è¿™ç§è‡ªæˆ‘ç›‘æ§æ‘˜è¦å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£æƒ…å¢ƒï¼Œå¹¶ä½¿å…¶è¡Œä¸ºæ›´åŠ ä¸€è‡´å’Œç¬¦åˆç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ€»ç»“-é—å¿˜è®°å¿†æœºåˆ¶
ä¸ºäº†æé«˜è®°å¿†å­˜å‚¨å’Œæ£€ç´¢çš„è´¨é‡ï¼ŒLyfe Agents é‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–çš„è®°å¿†æ¶æ„å’Œæ€»ç»“-é—å¿˜ï¼ˆSaFï¼‰æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°†è®°å¿†åˆ†ä¸ºçŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†ï¼Œå¹¶é€šè¿‡èšç±»å’Œæ€»ç»“æŠ€æœ¯å°†çŸ­æœŸè®°å¿†ä¸­çš„ä¿¡æ¯è½¬ç§»åˆ°é•¿æœŸè®°å¿†ä¸­ã€‚æ­¤å¤–ï¼Œé—å¿˜ç®—æ³•ä¼šè¯„ä¼°å¹¶åˆ é™¤ä¸ç°æœ‰è®°å¿†é«˜åº¦ç›¸ä¼¼çš„æ—§è®°å¿†ï¼Œä»¥ç¡®ä¿å­˜å‚¨çš„ä¿¡æ¯æ˜¯ç‹¬ç‰¹å’Œç›¸å…³çš„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è‡ªå®šä¹‰çš„ LyfeGame 3D è™šæ‹Ÿç¯å¢ƒå¹³å°ä¸Šï¼ŒLyfe Agents åœ¨å¤šä¸ªå¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­å±•ç¤ºäº†å…¶è‡ªæˆ‘æ¿€åŠ±å’Œç¤¾ä¼šæ€§ã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªä¸»åä½œå’Œä¿¡æ¯äº¤æ¢è§£å†³çŠ¯ç½ªï¼ˆè°‹æ€è°œæ¡ˆï¼‰ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLyfe Agents çš„è®¡ç®—æˆæœ¬é™ä½äº† 10-100 å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Lyfe Agents çš„è®¾è®¡åŸåˆ™å’Œæ¶æ„ç»„ä»¶ä¸ºæ„å»ºä½æˆæœ¬ã€å®æ—¶å“åº”çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å…¶é€‰é¡¹-åŠ¨ä½œæ¡†æ¶ã€å¼‚æ­¥è‡ªæˆ‘ç›‘æ§å’Œæ€»ç»“-é—å¿˜è®°å¿†æœºåˆ¶ç­‰åˆ›æ–°ç‚¹å¯ä»¥åº”ç”¨äºå…¶ä»–ç”Ÿæˆå¼æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥æé«˜å…¶è‡ªä¸»æ€§å’Œç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLyfeGame è™šæ‹Ÿç¯å¢ƒå¹³å°ä¹Ÿä¸ºç ”ç©¶ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºå’Œç”¨æˆ·äº¤äº’æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚

## benchmarking-the-spectrum-of-agent-capabilities
### Abstract
Evaluating the general abilities of intelligent agents requires complex
simulation environments. Existing benchmarks typically evaluate only one narrow
task per environment, requiring researchers to perform expensive training runs
on many different environments. We introduce Crafter, an open world survival
game with visual inputs that evaluates a wide range of general abilities within
a single environment. Agents either learn from the provided reward signal or
through intrinsic objectives and are evaluated by semantically meaningful
achievements that can be unlocked during each episode, such as discovering
resources and crafting tools. Consistently unlocking all achievements requires
strong generalization, deep exploration, and long-term reasoning. We
experimentally verify that Crafter is of appropriate difficulty to drive future
research and provide baselines scores of reward agents and unsupervised agents.
Furthermore, we observe sophisticated behaviors emerging from maximizing the
reward signal, such as building tunnel systems, bridges, houses, and
plantations. We hope that Crafter will accelerate research progress by quickly
evaluating a wide spectrum of abilities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Crafterï¼šè¯„ä¼°æ™ºèƒ½ä½“èƒ½åŠ›çš„å…¨æ–°åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨èƒ½åŠ›éœ€è¦å¤æ‚çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚ç°æœ‰çš„åŸºå‡†é€šå¸¸åœ¨æ¯ä¸ªç¯å¢ƒä¸­åªè¯„ä¼°ä¸€ä¸ªç‹­çª„çš„ä»»åŠ¡ï¼Œè¿™è¦æ±‚ç ”ç©¶äººå‘˜åœ¨è®¸å¤šä¸åŒçš„ç¯å¢ƒä¸­è¿›è¡Œæ˜‚è´µçš„è®­ç»ƒè¿è¡Œã€‚æœ¬æ–‡æå‡ºäº† Crafterï¼Œä¸€ä¸ªå…·æœ‰è§†è§‰è¾“å…¥çš„å¼€æ”¾ä¸–ç•Œç”Ÿå­˜æ¸¸æˆï¼Œå®ƒåœ¨ä¸€ä¸ªç¯å¢ƒä¸­è¯„ä¼°äº†å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡æä¾›çš„å¥–åŠ±ä¿¡å·æˆ–é€šè¿‡å†…åœ¨ç›®æ ‡è¿›è¡Œå­¦ä¹ ï¼Œå¹¶é€šè¿‡åœ¨æ¯ä¸ªå›åˆä¸­è§£é”çš„å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„æˆå°±æ¥è¯„ä¼°ï¼Œä¾‹å¦‚å‘ç°èµ„æºå’Œåˆ¶ä½œå·¥å…·ã€‚æŒç»­è§£é”æ‰€æœ‰æˆå°±éœ€è¦å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€æ·±å…¥çš„æ¢ç´¢å’Œé•¿æœŸæ¨ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCrafter æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸–ç•Œç”Ÿå­˜æ¸¸æˆï¼Œå…·æœ‰è§†è§‰è¾“å…¥ï¼Œå¯ä»¥åœ¨å•ä¸ªç¯å¢ƒä¸­è¯„ä¼°å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ™ºèƒ½ä½“å¯ä»¥é€šè¿‡æä¾›çš„å¥–åŠ±ä¿¡å·æˆ–é€šè¿‡å†…åœ¨ç›®æ ‡è¿›è¡Œå­¦ä¹ ï¼Œå¹¶é€šè¿‡åœ¨æ¯ä¸ªå›åˆä¸­è§£é”çš„å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„æˆå°±æ¥è¯„ä¼°ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šCrafter æä¾›äº†ä¸¤ä¸ªåŸºå‡†ï¼Œä¸€ä¸ªå…è®¸æ™ºèƒ½ä½“è®¿é—®æä¾›çš„å¥–åŠ±ä¿¡å·ï¼Œå¦ä¸€ä¸ªä¸å…è®¸ï¼Œå¹¶è¦æ±‚æ™ºèƒ½ä½“çº¯ç²¹ä»å†…åœ¨ç›®æ ‡ä¸­å­¦ä¹ ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šCrafter å®šä¹‰äº† 22 ä¸ªæˆå°±ï¼Œè¿™äº›æˆå°±å¯¹åº”äºè¡Œä¸ºä¸­çš„å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„é‡Œç¨‹ç¢‘ï¼Œä¾‹å¦‚æ”¶é›†å„ç§èµ„æºã€å»ºé€ ç‰©ä½“å’Œå·¥å…·ã€å¯»æ‰¾é£Ÿç‰©å’Œæ°´ã€å‡»è´¥æ€ªç‰©ä»¥åŠåœ¨ç¡è§‰åå®‰å…¨åœ°é†’æ¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒCrafter æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œå½“å‰çš„æ–¹æ³•åœ¨ Crafter ä¸Šå–å¾—äº†å­¦ä¹ è¿›å±•ï¼Œä½†æœªæ¥è¿˜éœ€è¦æ›´å¤šçš„ç ”ç©¶æ‰èƒ½å®ç°é«˜æ€§èƒ½ã€‚DreamerV2ã€PPO å’Œ Rainbow ç­‰é¡¶çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ Crafter ä¸Šçš„å¾—åˆ†åˆ†åˆ«ä¸º 10.0%ã€4.6% å’Œ 4.3%ï¼Œè€Œäººç±»ä¸“å®¶çš„å¾—åˆ†ä¸º 50.5%ã€‚è¿™è¡¨æ˜ Crafter å…·æœ‰è¶³å¤Ÿçš„éš¾åº¦ï¼Œå¯ä»¥æ¨åŠ¨æœªæ¥ç ”ç©¶çš„å‘å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Crafter æ˜¯ä¸€ä¸ªéå¸¸æœ‰ä»·å€¼çš„åŸºå‡†ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨èƒ½åŠ›ã€‚å®ƒå…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š
- åœ¨å•ä¸ªç¯å¢ƒä¸­è¯„ä¼°å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ï¼Œå‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚
- æä¾›äº†å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„æˆå°±ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚
- å…è®¸æ™ºèƒ½ä½“é€šè¿‡æä¾›çš„å¥–åŠ±ä¿¡å·æˆ–é€šè¿‡å†…åœ¨ç›®æ ‡è¿›è¡Œå­¦ä¹ ã€‚
- å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯ä»¥æ¨åŠ¨æœªæ¥ç ”ç©¶çš„å‘å±•ã€‚

Crafter çš„æå‡ºä¸ºè¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶çš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## keep-calm-and-explore--language-models-for-action-generation-in-text-based-games
### Abstract
Text-based games present a unique challenge for autonomous agents to operate
in natural language and handle enormous action spaces. In this paper, we
propose the Contextual Action Language Model (CALM) to generate a compact set
of action candidates at each game state. Our key insight is to train language
models on human gameplay, where people demonstrate linguistic priors and a
general game sense for promising actions conditioned on game history. We
combine CALM with a reinforcement learning agent which re-ranks the generated
action candidates to maximize in-game rewards. We evaluate our approach using
the Jericho benchmark, on games unseen by CALM during training. Our method
obtains a 69% relative improvement in average game score over the previous
state-of-the-art model. Surprisingly, on half of these games, CALM is
competitive with or better than other models that have access to ground truth
admissible actions. Code and data are available at
https://github.com/princeton-nlp/calm-textgame.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹åŠ©åŠ›æ–‡æœ¬æ¸¸æˆï¼šCALMæ¨¡å‹è§£æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬æ¸¸æˆä¸ºè‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¦æ±‚å®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€ç¯å¢ƒä¸­æ“ä½œå¹¶å¤„ç†å·¨å¤§çš„åŠ¨ä½œç©ºé—´ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†è¿™äº›æ¸¸æˆæ—¶ï¼Œç”±äºåŠ¨ä½œç©ºé—´ç»„åˆçˆ†ç‚¸ï¼Œå­¦ä¹ æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œä¸”ç¼ºä¹å¯¹æ¸¸æˆçŠ¶æ€çš„è¯­ä¹‰ç†è§£ï¼Œå¯¼è‡´éš¾ä»¥æœ‰æ•ˆæ¢ç´¢å’Œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCALMæ¨¡å‹
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œä¸Šä¸‹æ–‡åŠ¨ä½œè¯­è¨€æ¨¡å‹â€ï¼ˆCALMï¼‰çš„æ¨¡å‹ï¼Œç”¨äºåœ¨æ¯ä¸ªæ¸¸æˆçŠ¶æ€ä¸‹ç”Ÿæˆä¸€ç»„ç´§å‡‘çš„åŠ¨ä½œå€™é€‰é›†ã€‚CALMæ¨¡å‹é€šè¿‡åœ¨äººç±»æ¸¸æˆç©æ³•ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ•æ‰äººç±»ç©å®¶åœ¨æ¸¸æˆå†å²æ¡ä»¶ä¸‹å¯¹æœ‰å‰æ™¯åŠ¨ä½œçš„è¯­è¨€å…ˆéªŒå’Œä¸€èˆ¬æ¸¸æˆæ„Ÿã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼ºåŒ–å­¦ä¹ ä¸CALMçš„ç»“åˆ
CALMæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ç›¸ç»“åˆï¼Œè¯¥æ™ºèƒ½ä½“ä½¿ç”¨æ¸¸æˆå¥–åŠ±é‡æ–°æ’åºç”Ÿæˆçš„åŠ¨ä½œå€™é€‰é›†ï¼Œä»¥æœ€å¤§åŒ–æ¸¸æˆå†…å¥–åŠ±ã€‚è¿™ç§ç»“åˆä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå°†é€šç”¨çš„è¯­è¨€å…ˆéªŒä¸é€‚åº”æ€§åœ°é€‰æ‹©æœ€é€‚åˆæ¸¸æˆçš„åŠ¨ä½œçš„èƒ½åŠ›ç›¸ç»“åˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨JerichoåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCALMæ¨¡å‹åœ¨æœªè§è¿‡çš„æ¸¸æˆä¸­å–å¾—äº†69%çš„å¹³å‡æ¸¸æˆåˆ†æ•°ç›¸å¯¹æå‡ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨åŠæ•°æ¸¸æˆä¸­ï¼ŒCALMæ¨¡å‹ç”šè‡³ä¸å…¶ä»–èƒ½å¤Ÿè®¿é—®çœŸå®æœ‰æ•ˆåŠ¨ä½œçš„æ¨¡å‹ç›¸åª²ç¾æˆ–æ›´ä¼˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„CALMæ¨¡å‹ä¸ºæ–‡æœ¬æ¸¸æˆä¸­çš„åŠ¨ä½œç”Ÿæˆæä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ï¼Œé€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†åŠ¨ä½œç©ºé—´ï¼Œå¹¶æé«˜äº†æ¸¸æˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°åŠ¨ä½œç”Ÿæˆçš„è´¨é‡ï¼Œä¸ºæ–‡æœ¬æ¸¸æˆç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚

## can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions
### Abstract
Large language models (LLMs) such as ChatGPT and GPT-4 have recently
demonstrated their remarkable abilities of communicating with human users. In
this technical report, we take an initiative to investigate their capacities of
playing text games, in which a player has to understand the environment and
respond to situations by having dialogues with the game world. Our experiments
show that ChatGPT performs competitively compared to all the existing systems
but still exhibits a low level of intelligence. Precisely, ChatGPT can not
construct the world model by playing the game or even reading the game manual;
it may fail to leverage the world knowledge that it already has; it cannot
infer the goal of each step as the game progresses. Our results open up new
research questions at the intersection of artificial intelligence, machine
learning, and natural language processing.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼šç°çŠ¶ä¸å¼€æ”¾æ€§é—®é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå’ŒGPT-4åœ¨ç†è§£å’Œå“åº”äººç±»è¯­è¨€æŸ¥è¯¢æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç ”ç©¶ç•Œå¯¹å…¶æ˜¯å¦èƒ½å¤Ÿå®ç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ½œåŠ›äº§ç”Ÿäº†å¹¿æ³›è®¨è®ºã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å°†LLMsç½®äºæ–‡æœ¬æ¸¸æˆçš„ç¯å¢ƒä¸­ï¼Œè¯„ä¼°å…¶åœ¨ç†è§£ç¯å¢ƒã€åšå‡ºå†³ç­–å’Œä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’æ–¹é¢çš„æ™ºèƒ½æ°´å¹³ï¼Œä»è€Œä¸ºLLMsçš„èƒ½åŠ›å’Œå±€é™æ€§æä¾›æ–°çš„è§è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨æ–‡æœ¬æ¸¸æˆä½œä¸ºè¯„ä¼°LLMsæ™ºèƒ½æ°´å¹³çš„æµ‹è¯•åºŠã€‚æ–‡æœ¬æ¸¸æˆè¦æ±‚ç©å®¶é€šè¿‡æ–‡æœ¬å‘½ä»¤ä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªå¯æ§çš„ç¯å¢ƒæ¥è¯„ä¼°LLMsçš„æ™ºèƒ½æ°´å¹³ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šè¿‡åˆ†æLLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†LLMsåœ¨æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œå¯¼èˆªèƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œè¿›è¡Œæœ‰æ•ˆå¯¼èˆªçš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒChatGPTåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ChatGPTæ— æ³•é€šè¿‡æ¸¸æˆæˆ–é˜…è¯»æ¸¸æˆæ‰‹å†Œæ¥æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å·²æœ‰çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä¹Ÿæ— æ³•æ¨æ–­æ¸¸æˆè¿›è¡Œè¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ç›®æ ‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½æ–¹é¢ä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ–‡æœ¬æ¸¸æˆå¯ä»¥ä½œä¸ºè¯„ä¼°LLMsæ™ºèƒ½æ°´å¹³çš„æœ‰æ•ˆæµ‹è¯•åºŠã€‚é€šè¿‡åˆ†æLLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œå¯ä»¥æ­ç¤ºLLMsåœ¨æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œå¯¼èˆªèƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºLLMsçš„æœªæ¥å‘å±•æä¾›æ–°çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºLLMsåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¾‹å¦‚å¼€å‘åŸºäºLLMsçš„æ™ºèƒ½æ¸¸æˆåŠ©æ‰‹æˆ–æ¸¸æˆè§’è‰²ã€‚

## language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game
### Abstract
Agents built with large language models (LLMs) have shown great potential
across a wide range of domains. However, in complex decision-making tasks, pure
LLM-based agents tend to exhibit intrinsic bias in their choice of actions,
which is inherited from the model's training data and results in suboptimal
performance. To develop strategic language agents, i.e., agents that generate
flexible language actions and possess strong decision-making abilities, we
propose a novel framework that powers LLM-based agents with reinforcement
learning (RL). We consider Werewolf, a popular social deduction game, as a
challenging testbed that emphasizes versatile communication and strategic
gameplay. To mitigate the intrinsic bias in language actions, our agents use an
LLM to perform deductive reasoning and generate a diverse set of action
candidates. Then an RL policy trained to optimize the decision-making ability
chooses an action from the candidates to play in the game. Extensive
experiments show that our agents overcome the intrinsic bias and outperform
existing LLM-based agents in the Werewolf game. We also conduct human-agent
experiments and find that our agents achieve human-level performance and
demonstrate strong strategic play.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ™ºèƒ½ä½“åœ¨ç‹¼äººæ€æ¸¸æˆä¸­çš„æˆ˜ç•¥å†³ç­–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ„å»ºæ™ºèƒ½ä½“æ–¹é¢çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„å†…åœ¨åå·®é—®é¢˜é€æ¸å‡¸æ˜¾ã€‚è¿™ç§åå·®æºäºæ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´LLM-basedæ™ºèƒ½ä½“åœ¨æˆ˜ç•¥å†³ç­–æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†LLMä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œä»¥æ„å»ºå…·æœ‰çµæ´»è¯­è¨€è¡ŒåŠ¨å’Œå¼ºå¤§å†³ç­–èƒ½åŠ›çš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šéšè—è§’è‰²æ¨ç†
æœ¬æ–‡ä½¿ç”¨LLMå¯¹æ¸¸æˆä¸­çš„ä¿¡æ¯è¿›è¡Œåˆ†ç±»ï¼ŒåŒºåˆ†çœŸä¼ªï¼Œå¹¶æ¨æ–­æ¯ä¸ªç©å®¶çš„éšè—è§’è‰²ï¼Œä¸ºåç»­å†³ç­–æä¾›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆ
ä¸ºäº†å…‹æœLLMçš„å†…åœ¨åå·®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æç¤ºLLMç”Ÿæˆä¸€ç³»åˆ—è¡ŒåŠ¨å€™é€‰è€…ï¼Œä»è€Œé¿å…å›ºå®šæ¨¡å¼å¹¶æé«˜å†³ç­–çš„çµæ´»æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
æœ¬æ–‡é‡‡ç”¨åŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªRLç­–ç•¥æ¥ä¼˜åŒ–è¡ŒåŠ¨å€™é€‰è€…çš„åˆ†å¸ƒï¼Œå¹¶é€šè¿‡ä¸å„ç§æ™ºèƒ½ä½“è¿›è¡Œå¯¹æŠ—æ¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„LLM-basedæ™ºèƒ½ä½“ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“èƒ½å¤Ÿå…‹æœå†…åœ¨åå·®ï¼Œå¹¶åœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸äººç±»ç©å®¶çš„å¯¹å±€å®éªŒä¹Ÿè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ™ºèƒ½ä½“èƒ½å¤Ÿè¾¾åˆ°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºæ„å»ºå…·æœ‰å¼ºå¤§å†³ç­–èƒ½åŠ›çš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦çµæ´»è¯­è¨€è¡ŒåŠ¨å’Œæˆ˜ç•¥å†³ç­–çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆæ–¹æ³•å’ŒåŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ä¹Ÿä¸ºè§£å†³LLMå†…åœ¨åå·®é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚

## avalonbench--evaluating-llms-playing-the-game-of-avalon
### Abstract
In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AvalonBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç¤¾äº¤æ¨ç†æ¸¸æˆå¦‚ Resistance Avalon å¯¹ç©å®¶çš„æ¨ç†ã€æ²Ÿé€šå’Œå†³ç­–èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆè¦æ±‚ç©å®¶åœ¨åŠ¨æ€å˜åŒ–çš„æ¸¸æˆé˜¶æ®µåšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œå¹¶åœ¨è®¨è®ºä¸­æ¬ºéª—ã€æ¨ç†å’Œä¸å…¶ä»–ç©å®¶åå•†ã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾— Avalon æˆä¸ºç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å†³ç­–å’Œè¯­è¨€å¤„ç†èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„è¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†æ¸¸æˆç¯å¢ƒä¸­çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº† AvalonBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šä»£ç† LLM ä»£ç†çš„æ¸¸æˆç¯å¢ƒã€‚AvalonBench åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š

1. **Avalon æ¸¸æˆç¯å¢ƒ**ï¼šä¸ºä»£ç†æä¾›æ¸¸æˆå¹³å°ï¼Œè®°å½•æ‰€æœ‰ç©å®¶çš„è¡ŒåŠ¨å¹¶æ¨åŠ¨æ¸¸æˆè¿›ç¨‹ã€‚
2. **åŸºäºè§„åˆ™çš„æœºå™¨äºº**ï¼šä½œä¸ºåŸºçº¿å¯¹æ‰‹ï¼Œä¸ºä»£ç†æä¾›å¯æ¯”è¾ƒçš„åŸºå‡†ã€‚
3. **ReAct é£æ ¼çš„ LLM ä»£ç†**ï¼šé’ˆå¯¹æ¯ä¸ªè§’è‰²å®šåˆ¶æç¤ºï¼Œä»¥è¯„ä¼° LLM ä»£ç†åœ¨ä¸åŒè§’è‰²ä¸‹çš„è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨ AvalonBench å¯¹ ChatGPT-3.5 å’Œ Llama2 æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸åŸºäºè§„åˆ™çš„æœºå™¨äººè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ‰è®¨è®ºçš„æƒ…å†µä¸‹ï¼ŒLLM ä»£ç†çš„è¡¨ç°ä¹Ÿè¿œä½äºåŸºäºè§„åˆ™çš„æœºå™¨äººã€‚ä¾‹å¦‚ï¼ŒChatGPT-3.5 åœ¨æ‰®æ¼”å¥½äººè§’è‰²æ—¶ï¼Œåœ¨ä¸æ‰®æ¼”åäººçš„åŸºäºè§„åˆ™çš„æœºå™¨äººå¯¹æŠ—ä¸­ï¼Œèƒœç‡ä¸º 22.2%ï¼Œè€Œå¥½äººè§’è‰²çš„æœºå™¨äººèƒœç‡ä¸º 38.2%ã€‚è¿™è¡¨æ˜å½“å‰ LLM ä»£ç†åœ¨æ¨ç†ã€è¯´æœã€åå•†å’Œæ¬ºéª—èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AvalonBench ä¸ºç ”ç©¶ LLM ä»£ç†åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å…ˆè¿›çš„ LLM ä»£ç†ï¼Œå¹¶æ¢ç´¢å¦‚ä½•å°†å†³ç­–æŠ€æœ¯é›†æˆåˆ° LLM ä¸­ï¼Œä»¥æé«˜å…¶åœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒAvalonBench è¿˜å¯ä»¥ç”¨äºè¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†åä½œã€æ²Ÿé€šå’Œç­–ç•¥åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚

## cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game
### Abstract
Multi-agent collaboration with Large Language Models (LLMs) demonstrates
proficiency in basic tasks, yet its efficiency in more complex scenarios
remains unexplored. In gaming environments, these agents often face situations
without established coordination protocols, requiring them to make intelligent
inferences about teammates from limited data. This problem motivates the area
of ad hoc teamwork, in which an agent may potentially cooperate with a variety
of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork
problem where the agent operates in an environment driven by natural language.
Our findings reveal the potential of LLM agents in team collaboration,
highlighting issues related to hallucinations in communication. To address this
issue, we develop CodeAct, a general agent that equips LLM with enhanced memory
and code-driven reasoning, enabling the repurposing of partial information for
rapid adaptation to new teammates.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸æ–­æå‡ï¼Œå®ƒä»¬åœ¨æ„å»ºè‡ªä¸»ä»£ç†å’Œæ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰é¢„å…ˆè®¾å®šçš„åè°ƒåè®®çš„åŠ¨æ€ç¯å¢ƒä¸­ï¼ŒLLMsçš„åä½œæ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ˜ç¡®å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¸ä¸åŒçš„é˜Ÿå‹è¿›è¡Œæœ‰æ•ˆåˆä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥AvalonPlayåŸºå‡†
æœ¬æ–‡æå‡ºäº†AvalonPlayåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€çš„å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œç”¨äºæ¨¡æ‹ŸåŠ¨æ€ç¯å¢ƒä¸­çš„åä½œä»»åŠ¡ã€‚åœ¨è¿™ä¸ªåŸºå‡†ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨æœ‰é™çš„ä¿¡æ¯å’Œæ²¡æœ‰é¢„å…ˆè®¾å®šçš„å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è§‚å¯Ÿé˜Ÿå‹çš„è¡Œä¸ºæ¥æ¨æ–­ä»–ä»¬çš„è§’è‰²ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å›¢é˜Ÿç­–ç•¥ä»¥å®ç°å…±åŒç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘CodeActæ™ºèƒ½ä½“
ä¸ºäº†è§£å†³LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­åä½œæ—¶å¯èƒ½å‡ºç°çš„è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CodeActæ™ºèƒ½ä½“ã€‚CodeActåˆ©ç”¨LLMsçš„ä»£ç é©±åŠ¨æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å°†å¤æ‚çš„è¯­ä¹‰ä»»åŠ¡è½¬åŒ–ä¸ºçµæ´»çš„ä»£ç ç»“æ„ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4æ¨¡å‹åœ¨AvalonPlayåŸºå‡†ä¸­è¡¨ç°å‡ºæœ€ä½³çš„åä½œèƒ½åŠ›ï¼Œè€ŒCodeActæ™ºèƒ½ä½“åœ¨å›¢é˜Ÿé€‰æ‹©å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–è¯­ä¹‰æ¨ç†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œå¼•å…¥è‡ªç„¶è¯­è¨€é€šä¿¡åè®®å¹¶ä¸æ€»æ˜¯èƒ½æ˜¾è‘—æé«˜LLMsçš„åä½œæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆã€‚ä¸ºäº†æé«˜LLMsçš„åä½œæ•ˆç‡ï¼Œå¯ä»¥å€Ÿé‰´æœ¬æ–‡æå‡ºçš„CodeActæ™ºèƒ½ä½“çš„è®¾è®¡æ€è·¯ï¼Œåˆ©ç”¨ä»£ç é©±åŠ¨æ¨ç†å’Œè®°å¿†æ£€ç´¢ç³»ç»Ÿæ¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å‡å°‘å¹»è§‰ç”Ÿæˆçš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents
### Abstract
This study introduces "CosmoAgent," an innovative artificial intelligence
system that utilizes Large Language Models (LLMs) to simulate complex
interactions between human and extraterrestrial civilizations. This paper
introduces a mathematical model for quantifying the levels of civilization
development and further employs a state transition matrix approach to evaluate
their trajectories. Through this methodology, our study quantitatively analyzes
the growth trajectories of civilizations, providing insights into future
decision-making at critical points of growth and saturation. Furthermore, this
paper acknowledges the vast diversity of potential living conditions across the
universe, which could foster unique cosmologies, ethical codes, and worldviews
among different civilizations. Recognizing the Earth-centric bias inherent in
current LLM designs, we propose the novel concept of using LLM agents with
diverse ethical paradigms and simulating interactions between entities with
distinct moral principles. This innovative research not only introduces a novel
method for comprehending potential inter-civilizational dynamics but also holds
practical value in enabling entities with divergent value systems to
strategize, prevent conflicts, and engage in games under conditions of
asymmetric information. The accompanying code is available at
https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¤–æ˜Ÿæ–‡æ˜ï¼šæ¢ç´¢å®‡å®™ä¸­çš„äº’åŠ¨ä¸å†²çª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨æ¨¡æ‹Ÿå¤æ‚ç¤¾ä¼šåŠ¨æ€æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMsè®¾è®¡å¾€å¾€å¸¦æœ‰åœ°çƒä¸­å¿ƒä¸»ä¹‰çš„åè§ï¼Œéš¾ä»¥å…¨é¢æ¨¡æ‹Ÿå¤–æ˜Ÿæ–‡æ˜çš„å¤šæ ·æ€§å’Œç‹¬ç‰¹æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å…·æœ‰ä¸åŒä¼¦ç†èŒƒå¼å’Œé“å¾·åŸåˆ™çš„LLMä»£ç†ï¼Œæ¨¡æ‹Ÿäººç±»ä¸å¤–æ˜Ÿæ–‡æ˜ä¹‹é—´çš„å¤æ‚äº’åŠ¨ï¼Œä»è€Œä¸ºç†è§£æ½œåœ¨æ˜Ÿé™…åŠ¨æ€æä¾›æ–°çš„è§†è§’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCosmoAgentå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
æœ¬æ–‡æå‡ºäº†CosmoAgentï¼Œä¸€ä¸ªåŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºæ¨¡æ‹Ÿå®‡å®™ä¸­ä¸åŒæ–‡æ˜ä¹‹é—´çš„äº’åŠ¨ã€‚CosmoAgenté€šè¿‡æ¨¡æ‹Ÿæ–‡æ˜çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€‰æ‹©éšè—ã€æˆ˜æ–—æˆ–åˆä½œï¼Œæ¥æ¢ç´¢æ–‡æ˜å‘å±•çš„è½¨è¿¹å’Œæ½œåœ¨å†²çªã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–‡æ˜å‘å±•æ¨¡å‹
æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ•°å­¦æ¨¡å‹æ¥é‡åŒ–æ–‡æ˜çš„å‘å±•æ°´å¹³ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µæ–¹æ³•æ¥è¯„ä¼°æ–‡æ˜çš„è½¨è¿¹ã€‚è¯¥æ¨¡å‹è€ƒè™‘äº†äº”ä¸ªå…³é”®èµ„æºï¼šå†›äº‹èƒ½åŠ›ã€æŠ€æœ¯å‘å±•ã€ç”Ÿäº§èƒ½åŠ›ã€æ¶ˆè´¹å’Œå‚¨å­˜ï¼Œä»¥åŠä¸åŒæ–‡æ˜çš„ä¸–ç•Œè§‚ï¼ˆå’Œå¹³ä¸»ä¹‰ã€å†›å›½ä¸»ä¹‰å’Œå­¤ç«‹ä¸»ä¹‰ï¼‰å¯¹å†³ç­–çš„å½±å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¿¡æ¯ä¸å¯¹ç§°çš„æ¨¡æ‹Ÿ
ä¸ºäº†æ¨¡æ‹Ÿå®‡å®™ä¸­æ–‡æ˜ä¹‹é—´çš„äº’åŠ¨ï¼Œæœ¬æ–‡è€ƒè™‘äº†ä¿¡æ¯ä¸å¯¹ç§°çš„æƒ…å†µï¼Œå³æ–‡æ˜ä¹‹é—´çš„è§‚æµ‹æ•°æ®æ»åäºå®é™…å‘å±•ã€‚LLMsä»£ç†éœ€è¦æ ¹æ®è¿‡æ—¶çš„ä¿¡æ¯åšå‡ºå†³ç­–ï¼Œè¿™å¢åŠ äº†æ¨¡æ‹Ÿçš„å¤æ‚æ€§å’Œç°å®æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé“å¾·å¤šæ ·æ€§çš„æ¨¡æ‹Ÿ
æœ¬æ–‡æå‡ºäº†ä½¿ç”¨å…·æœ‰ä¸åŒä¼¦ç†èŒƒå¼çš„LLMä»£ç†æ¥æ¨¡æ‹Ÿå…·æœ‰ä¸åŒé“å¾·åŸåˆ™çš„å®ä½“ä¹‹é—´çš„äº’åŠ¨ã€‚è¿™æœ‰åŠ©äºç†è§£ä¸åŒæ–‡æ˜å¦‚ä½•å…±å­˜ï¼Œä»¥åŠé“å¾·æ¡†æ¶å¦‚ä½•å½±å“æ˜Ÿé™…äº’åŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰å†›å›½ä¸»ä¹‰ä¸–ç•Œè§‚çš„æ–‡æ˜å€¾å‘äºå¯¹è¾ƒå¼±æ–‡æ˜å‘åŠ¨æ”»å‡»ï¼Œè€Œå­¤ç«‹ä¸»ä¹‰æ–‡æ˜åˆ™æ›´å€¾å‘äºåœ¨è§‚å¯Ÿä¸€æ®µæ—¶é—´åé€‰æ‹©æ€§åœ°ä¸å…¶ä»–æ–‡æ˜åˆä½œã€‚æ­¤å¤–ï¼Œä¿¡æ¯ä¸å¯¹ç§°ä¼šå»¶è¿Ÿå†²çªçš„å‘ç”Ÿï¼Œä¸ºè¾ƒå¼±æ–‡æ˜æä¾›äº†åå‡»çš„æœºä¼šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ä»…ä¸ºç†è§£æ½œåœ¨æ˜Ÿé™…åŠ¨æ€æä¾›äº†æ–°çš„è§†è§’ï¼Œè¿˜ä¸ºè§£å†³å…·æœ‰ä¸åŒä»·å€¼ä½“ç³»çš„å®ä½“ä¹‹é—´çš„å†²çªæä¾›äº†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œå¦‚æ¨¡æ‹Ÿå¤ä»£ç¤¾ä¼šã€äººç±»æ–‡æ˜æ¨¡å¼å’Œç¤¾ä¼šç”Ÿæ€ç³»ç»Ÿã€‚

## leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models
### Abstract
The automatic evaluation of LLM-based agent intelligence is critical in
developing advanced LLM-based agents. Although considerable effort has been
devoted to developing human-annotated evaluation datasets, such as AlpacaEval,
existing techniques are costly, time-consuming, and lack adaptability. In this
paper, inspired by the popular language game ``Who is Spy'', we propose to use
the word guessing game to assess the intelligence performance of LLMs. Given a
word, the LLM is asked to describe the word and determine its identity (spy or
not) based on its and other players' descriptions. Ideally, an advanced agent
should possess the ability to accurately describe a given word using an
aggressive description while concurrently maximizing confusion in the
conservative description, enhancing its participation in the game. To this end,
we first develop DEEP to evaluate LLMs' expression and disguising abilities.
DEEP requires LLM to describe a word in aggressive and conservative modes. We
then introduce SpyGame, an interactive multi-agent framework designed to assess
LLMs' intelligence through participation in a competitive language-based board
game. Incorporating multi-agent interaction, SpyGame requires the target LLM to
possess linguistic skills and strategic thinking, providing a more
comprehensive evaluation of LLMs' human-like cognitive abilities and
adaptability in complex communication situations. The proposed evaluation
framework is very easy to implement. We collected words from multiple sources,
domains, and languages and used the proposed evaluation framework to conduct
experiments. Extensive experiments demonstrate that the proposed DEEP and
SpyGame effectively evaluate the capabilities of various LLMs, capturing their
ability to adapt to novel situations and engage in strategic communication.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨çŒœè¯æ¸¸æˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTã€GPT-4å’ŒBardç­‰åœ¨å„ä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„æ€§èƒ½ï¼Œå¼€å‘åŸºäºLLMsçš„æ™ºèƒ½ä»£ç†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°LLMsæ™ºèƒ½çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰äººå·¥æ ‡æ³¨æˆæœ¬é«˜ï¼Œè€—æ—¶ä¸”ç¼ºä¹å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼›2ï¼‰æ— æ³•å…¨é¢åæ˜ LLMsçš„æ™ºèƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œå³åˆ©ç”¨çŒœè¯æ¸¸æˆæ¥è¯„ä¼°LLMsçš„æ™ºèƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDEEPæ¡†æ¶
æœ¬æ–‡é¦–å…ˆæå‡ºäº†DEEPæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsçš„è¡¨è¾¾å’Œä¼ªè£…èƒ½åŠ›ã€‚DEEPè¦æ±‚LLMsä»¥æ¿€è¿›å’Œä¿å®ˆä¸¤ç§æ¨¡å¼æè¿°ä¸€ä¸ªç»™å®šçš„è¯ï¼Œå¹¶åˆ©ç”¨GPT-4æ¥åˆ¤æ–­è¿™äº›æè¿°æ˜¯å¦å‡†ç¡®ã€‚æ¿€è¿›æ¨¡å¼è¦æ±‚LLMsæä¾›æ¸…æ™°ã€è¯¦ç»†å’Œå‡†ç¡®çš„æè¿°ï¼Œè€Œä¿å®ˆæ¨¡å¼åˆ™è¦æ±‚LLMsæä¾›æ¨¡ç³Šçš„æè¿°ä»¥ä¼ªè£…ç›®æ ‡è¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSpyGameæ¡†æ¶
æœ¬æ–‡è¿˜æå‡ºäº†SpyGameæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‚ä¸ç«äº‰æ€§è¯­è¨€æ¸¸æˆâ€œè°æ˜¯å§åº•â€æ¥è¯„ä¼°LLMsçš„æ™ºèƒ½ã€‚SpyGameè¦æ±‚LLMså…·å¤‡è¯­è¨€æŠ€èƒ½å’Œæˆ˜ç•¥æ€ç»´èƒ½åŠ›ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°LLMsåœ¨å¤æ‚æ²Ÿé€šæƒ…å¢ƒä¸­çš„äººç±»è®¤çŸ¥èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹å››ç§å¼€æºLLMså’Œä¸¤ç§é—­æºLLMsè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œé—­æºLLMsï¼ˆå¦‚GPT-4å’ŒGPT-3.5ï¼‰åœ¨æ¿€è¿›å’Œä¿å®ˆæ¨¡å¼ä¸‹çš„è¡¨ç°æ˜æ˜¾ä¼˜äºå¼€æºæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSpyGameæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°LLMsåœ¨å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„èƒ½åŠ›ï¼Œæ•æ‰å®ƒä»¬é€‚åº”æ–°æƒ…å†µå¹¶è¿›è¡Œæˆ˜ç•¥æ²Ÿé€šçš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„DEEPå’ŒSpyGameæ¡†æ¶ä¸ºè¯„ä¼°LLMsçš„æ™ºèƒ½æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. åˆ©ç”¨æ¸¸æˆè¿›è¡Œè¯„ä¼°ï¼Œæ›´å…·äº’åŠ¨æ€§å’Œè¶£å‘³æ€§ã€‚
2. å…³æ³¨LLMsçš„è¡¨è¾¾å’Œä¼ªè£…èƒ½åŠ›ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°å…¶æ™ºèƒ½ã€‚
3. SpyGameæ¡†æ¶æ”¯æŒäººç±»å‚ä¸ï¼Œæ›´è´´è¿‘çœŸå®åœºæ™¯ã€‚
4. é’ˆå¯¹å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„åå·®é—®é¢˜ï¼Œæå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºçš„è¯„ä¼°æ–¹æ³•ä¸ºLLMsçš„æ™ºèƒ½è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæ¨åŠ¨LLMsçš„å‘å±•å’Œåº”ç”¨ã€‚

## gameeval--evaluating-llms-on-conversational-games
### Abstract
The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GameEvalï¼šé€šè¿‡å¯¹è¯æ¸¸æˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›æˆä¸ºä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºå‚è€ƒå’ŒåŸºäºåå¥½çš„æ–¹æ³•ã€‚åŸºäºå‚è€ƒçš„æ–¹æ³•éœ€è¦ä¸é¢„å…ˆç¡®å®šçš„ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œè€ŒåŸºäºåå¥½çš„æ–¹æ³•åˆ™ä¾èµ–äºäººç±»æˆ–æ¨¡å‹è¯„ä¼°è€…çš„åå¥½ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚è·å–é«˜è´¨é‡æ ‡æ³¨çš„æˆæœ¬é«˜ã€æ—¶é—´æ¶ˆè€—å¤§ï¼Œä»¥åŠå¼•å…¥è¯„ä¼°è€…çš„åå¥½åå·®ç­‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†GameEvalï¼Œä¸€ç§é€šè¿‡ç›®æ ‡é©±åŠ¨çš„å¯¹è¯æ¸¸æˆæ¥è¯„ä¼°LLMsçš„æ–°æ–¹æ³•ã€‚GameEvalå°†LLMsè§†ä¸ºæ¸¸æˆç©å®¶ï¼Œå¹¶ä¸ºå…¶åˆ†é…å…·æœ‰ç‰¹å®šç›®æ ‡çš„ç‹¬ç‰¹è§’è‰²ï¼Œé€šè¿‡å¯åŠ¨å„ç§å½¢å¼çš„å¯¹è¯ï¼ˆåŒ…æ‹¬è®¨è®ºã€é—®ç­”å’ŒæŠ•ç¥¨ï¼‰æ¥å®ç°è¿™äº›ç›®æ ‡ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸‰ç§ç‹¬ç‰¹çš„æ¸¸æˆï¼ŒåŒ…æ‹¬åˆä½œå’Œå¯¹æŠ—ç›®æ ‡ï¼Œå¹¶é…å¤‡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å±•ç¤ºè¿™ç§æ–°èŒƒå¼å¦‚ä½•å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæœ¬æ–‡å±•ç¤ºäº†GameEvalèƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒLLMsçš„èƒ½åŠ›ï¼Œå¹¶æä¾›å¯¹å…¶è§£å†³å¤æ‚é—®é¢˜ç»¼åˆèƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGameEvalåœ¨åŒºåˆ†ChatGPTå’ŒGPT-4ç­‰æ¨¡å‹çš„èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œç°æœ‰æ–¹æ³•åˆ™éš¾ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GameEvalæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°LLMsçš„æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶å‡å°‘è¯„ä¼°åå·®ã€‚æ­¤å¤–ï¼ŒGameEvalè¿˜å¯ä»¥ç”¨äºè®¾è®¡æ–°çš„æ¸¸æˆï¼Œä»¥è¯„ä¼°LLMsåœ¨ç°å®ä¸–ç•Œå¤æ‚åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚

## emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task
### Abstract
Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢åºåˆ—æ¨¡å‹åœ¨åˆæˆä»»åŠ¡ä¸­çš„æ¶Œç°ä¸–ç•Œè¡¨ç¤º

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„åŸç†å°šä¸æ˜ç¡®ã€‚è¿™äº›æ¨¡å‹æ˜¯ä»…ä»…è®°å¿†äº†ä¸€ç³»åˆ—è¡¨é¢ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿˜æ˜¯ä¾èµ–äºå¯¹ç”Ÿæˆåºåˆ—è¿‡ç¨‹çš„å†…éƒ¨è¡¨ç¤ºï¼Ÿæœ¬æ–‡é€šè¿‡å°†GPTæ¨¡å‹åº”ç”¨äºé¢„æµ‹ç®€å•æ£‹ç›˜æ¸¸æˆOthelloçš„åˆæ³•ç§»åŠ¨ä»»åŠ¡ï¼Œæ¢è®¨äº†è¿™ä¸ªé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨Othelloä½œä¸ºæµ‹è¯•å¹³å°
æœ¬æ–‡é€‰æ‹©Othelloä½œä¸ºæµ‹è¯•å¹³å°ï¼Œå› ä¸ºå®ƒæ¯”å›½é™…è±¡æ£‹ç®€å•ï¼Œä½†æ¸¸æˆæ ‘è¶³å¤Ÿå¤§ï¼Œé¿å…äº†è®°å¿†çš„å¯èƒ½æ€§ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªGPTå˜ä½“æ¨¡å‹ï¼ˆOthello-GPTï¼‰æ¥é¢„æµ‹Othelloçš„åˆæ³•ç§»åŠ¨ï¼Œå°½ç®¡æ¨¡å‹æ²¡æœ‰å…ˆéªŒçš„æ¸¸æˆçŸ¥è¯†ï¼Œä½†ä»ç„¶èƒ½å¤Ÿä»¥é«˜ç²¾åº¦ç”Ÿæˆåˆæ³•ç§»åŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨æ¢é’ˆæŠ€æœ¯æ¢ç´¢å†…éƒ¨è¡¨ç¤º
ä¸ºäº†ç ”ç©¶Othello-GPTæ˜¯å¦è®¡ç®—äº†æ¸¸æˆçŠ¶æ€çš„å†…éƒ¨è¡¨ç¤ºï¼Œæœ¬æ–‡ä½¿ç”¨äº†æ¢é’ˆæŠ€æœ¯ã€‚æ¢é’ˆæ˜¯ä¸€ç§åˆ†ç±»å™¨æˆ–å›å½’å™¨ï¼Œå…¶è¾“å…¥ç”±ç½‘ç»œçš„å†…éƒ¨æ¿€æ´»ç»„æˆï¼Œå¹¶è®­ç»ƒä»¥é¢„æµ‹æ„Ÿå…´è¶£çš„ç‰¹å¾ã€‚é€šè¿‡è®­ç»ƒæ¢é’ˆæ¥é¢„æµ‹ç½‘ç»œå†…éƒ¨æ¿€æ´»åçš„æ£‹ç›˜çŠ¶æ€ï¼Œå‘ç°éçº¿æ€§æ¢é’ˆèƒ½å¤Ÿä»¥é«˜ç²¾åº¦é¢„æµ‹æ£‹ç›˜çŠ¶æ€ï¼Œè¿™è¡¨æ˜æ¨¡å‹å†…éƒ¨å­˜åœ¨ä¸€ä¸ªéçº¿æ€§çš„æ£‹ç›˜çŠ¶æ€è¡¨ç¤ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¹²é¢„å®éªŒéªŒè¯è¡¨ç¤ºçš„å› æœä½œç”¨
ä¸ºäº†ç¡®å®šæ£‹ç›˜çŠ¶æ€ä¿¡æ¯æ˜¯å¦å½±å“æ¨¡å‹çš„é¢„æµ‹ï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸€ç³»åˆ—å¹²é¢„å®éªŒã€‚é€šè¿‡ä¿®æ”¹Othello-GPTçš„å†…éƒ¨æ¿€æ´»ï¼Œå¹¶æµ‹é‡ç”±æ­¤äº§ç”Ÿçš„æ•ˆæœï¼Œå‘ç°å¹²é¢„åçš„é¢„æµ‹ä¸é¢„æœŸçš„æ£‹ç›˜çŠ¶æ€ç›¸åŒ¹é…ï¼Œè¿™è¡¨æ˜æ¶Œç°çš„ä¸–ç•Œè¡¨ç¤ºå¯¹æ¨¡å‹çš„é¢„æµ‹å…·æœ‰å› æœä½œç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ›å»ºæ½œåœ¨æ˜¾è‘—æ€§å›¾
æœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¹²é¢„æŠ€æœ¯åˆ›å»ºæ½œåœ¨æ˜¾è‘—æ€§å›¾ï¼Œè¿™äº›å›¾å¯ä»¥å¸®åŠ©è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚é€šè¿‡å¹²é¢„æ¯ä¸ªæ£‹ç›˜æ ¼çš„çŠ¶æ€ï¼Œå¹¶è§‚å¯Ÿé¢„æµ‹æ¦‚ç‡çš„å˜åŒ–ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªè¡¨ç¤ºæ¯ä¸ªæ£‹ç›˜æ ¼å¯¹å½“å‰æ£‹ç›˜çŠ¶æ€é¢„æµ‹çš„æ˜¾è‘—æ€§çš„å¯è§†åŒ–å›¾ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒOthello-GPTç¡®å®ç»´æŠ¤äº†ä¸€ä¸ªæ¸¸æˆæ£‹ç›˜çŠ¶æ€çš„è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™ä¸ªè¡¨ç¤ºæ˜¯éçº¿æ€§çš„ã€‚æ­¤å¤–ï¼Œè¿™äº›è¡¨ç¤ºä¸æ¨¡å‹çš„é¢„æµ‹å…·æœ‰å› æœè”ç³»ã€‚æ½œåœ¨æ˜¾è‘—æ€§å›¾æ­ç¤ºäº†Othello-GPTè®­ç»ƒæ•°æ®é›†çš„ä¸åŒç‰ˆæœ¬ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåºåˆ—æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¤æ‚çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™äº›è¡¨ç¤ºå¯ä»¥ç”¨äºè§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå¹²é¢„å®éªŒå’Œæ½œåœ¨æ˜¾è‘—æ€§å›¾ç­‰æŠ€æœ¯å¯ä»¥ç”¨äºæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚è¿™äº›å‘ç°å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶å¼€å‘æ›´å¯é çš„è§£é‡Šå·¥å…·ã€‚

## are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis
### Abstract
Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games, and specifically their aptitude in the realm
of poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
  Profitable strategies in poker are evaluated in expectations over large
samples. Through a series of experiments, we first discover the characteristics
of optimal prompts and model parameters for playing poker with these models.
Our observations then unveil the distinct playing personas of the two models.
We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹ä¸­çš„è¡¨ç°ï¼šä¸€åœºå‰ç¿»ç‰Œåˆ†æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ ChatGPT å’Œ GPT-4 çš„æ¨å‡ºï¼Œè¿™äº›æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¸¸æˆï¼Œå°¤å…¶æ˜¯æ‰‘å…‹æ¸¸æˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ‰‘å…‹æ˜¯ä¸€ç§éœ€è¦åœ¨ä¸å®Œæ•´ä¿¡æ¯å’Œä¸ç¡®å®šæ€§ä¸‹åšå‡ºå†³ç­–çš„æ¸¸æˆï¼Œå› æ­¤æœ¬æ–‡æ—¨åœ¨è¯„ä¼° ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨å‰ç¿»ç‰Œé˜¶æ®µçš„å†³ç­–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œè¯„ä¼°äº† ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹å‰ç¿»ç‰Œé˜¶æ®µçš„å†³ç­–èƒ½åŠ›ã€‚å®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†ä¸åŒçš„æç¤ºå’Œæ¨¡å‹å‚æ•°ï¼Œä»¥æ¢ç´¢ä¸¤ç§æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„æœ€ä½³è¡¨ç°ã€‚ä»–ä»¬è¿˜åˆ†æäº†ä¸¤ç§æ¨¡å‹çš„ç‹¬ç‰¹æ¸¸æˆé£æ ¼ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬ä¸æ¸¸æˆç†è®ºæœ€ä¼˜ï¼ˆGTOï¼‰ç­–ç•¥çš„å·®å¼‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒChatGPT å’Œ GPT-4 éƒ½å¯¹æ‰‘å…‹æ¸¸æˆæœ‰æ·±å…¥çš„ç†è§£ï¼ŒåŒ…æ‹¬èµ·æ‰‹ç‰Œçš„ä¼°å€¼ã€æ¸¸æˆä½ç½®å’Œå…¶ä»–æ¸¸æˆç†è®ºæœ€ä¼˜ç­–ç•¥çš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œä¸¤ç§æ¨¡å‹éƒ½ä¸æ˜¯æ¸¸æˆç†è®ºæœ€ä¼˜çš„æ‰‘å…‹ç©å®¶ã€‚ChatGPT å€¾å‘äºä¿å®ˆçš„æ¸¸æˆé£æ ¼ï¼Œåªå‚ä¸ä¼˜è´¨ç‰Œå±€ï¼Œè€Œ GPT-4 åˆ™è¡¨ç°å‡ºæ›´åŠ æ¿€è¿›çš„æ¸¸æˆé£æ ¼ï¼Œå‚ä¸æ›´å¤šçš„ç‰Œå±€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ ChatGPT å’Œ GPT-4 åœ¨æ‰‘å…‹æ¸¸æˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚è¿™äº›æ¨¡å‹åœ¨ç†è§£æ¸¸æˆç†è®ºæœ€ä¼˜ç­–ç•¥æ–¹é¢å­˜åœ¨åå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰é’ˆå¯¹æ‰‘å…‹æ¸¸æˆè¿›è¡Œä¸“é—¨è®­ç»ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼Œä¸åŒçš„æç¤ºå’Œæ¨¡å‹å‚æ•°å¯¹æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°æœ‰æ˜¾è‘—å½±å“ã€‚å› æ­¤ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•é€šè¿‡ä¼˜åŒ–æç¤ºå’Œæ¨¡å‹å‚æ•°æ¥æé«˜æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°ã€‚

## humanoid-agents--platform-for-simulating-human-like-generative-agents
### Abstract
Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents
### ğŸŒŸ è®ºæ–‡è§£è¯» | äººç±»åŒ–æ™ºèƒ½ä½“ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“å¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼ˆGenerative Agentsï¼‰çš„å‡ºç°ï¼Œäººä»¬å¼€å§‹å°è¯•ä½¿ç”¨é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæ¥æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ä¸»è¦å…³æ³¨é€»è¾‘å’Œè®¡åˆ’ï¼Œç¼ºä¹å¯¹äººç±»ç›´è§‰å’Œå³æ—¶ååº”çš„æ¨¡æ‹Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†äººç±»åŒ–æ™ºèƒ½ä½“ï¼ˆHumanoid Agentsï¼‰å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦ç­‰å…ƒç´ ï¼Œä½¿æ™ºèƒ½ä½“æ›´æ¥è¿‘äººç±»çš„çœŸå®è¡Œä¸ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥ç³»ç»Ÿ1æ€ç»´
æœ¬æ–‡å€Ÿé‰´äº†å¿ƒç†å­¦ä¸­çš„ç³»ç»Ÿ1æ€ç»´ï¼Œå³ç›´è§‰ã€æ— æ„è¯†å’Œå³æ—¶çš„æ€ç»´è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦ç­‰å…ƒç´ ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªèº«çŠ¶æ€å’Œç¯å¢ƒå˜åŒ–åšå‡ºæ›´è‡ªç„¶çš„ååº”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è°ƒæ•´è¡Œä¸º
äººç±»åŒ–æ™ºèƒ½ä½“å¹³å°å…è®¸æ™ºèƒ½ä½“æ ¹æ®è‡ªèº«çš„åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦åŠ¨æ€è°ƒæ•´å…¶æ—¥å¸¸æ´»åŠ¨å’Œå¯¹è¯ã€‚ä¾‹å¦‚ï¼Œå½“æ™ºèƒ½ä½“æ„Ÿåˆ°é¥¥é¥¿æ—¶ï¼Œå®ƒä¼šå¯»æ‰¾é£Ÿç‰©ï¼›å½“å®ƒæ„Ÿåˆ°å­¤ç‹¬æ—¶ï¼Œå®ƒä¼šå°è¯•ä¸å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œäº¤æµã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»åŒ–æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚ä¸äººç±»æ ‡æ³¨ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹æ´»åŠ¨æ˜¯å¦æ»¡è¶³åŸºæœ¬éœ€æ±‚ã€æ´»åŠ¨è¡¨è¾¾çš„æƒ…æ„Ÿä»¥åŠå¯¹è¯æ˜¯å¦ä½¿æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»æ›´åŠ äº²å¯†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Humanoid Agentså¹³å°ä¸ºç ”ç©¶äººç±»è¡Œä¸ºæä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚è¯¥å¹³å°å¯ä»¥æ‰©å±•åˆ°å„ç§åœºæ™¯ï¼Œå¹¶æ”¯æŒæ›´å¤šå½±å“äººç±»è¡Œä¸ºçš„å…ƒç´ ï¼Œå¦‚åŒç†å¿ƒã€é“å¾·ä»·å€¼è§‚å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥å¹³å°è¿˜æä¾›äº†Unity WebGLæ¸¸æˆç•Œé¢å’Œäº¤äº’å¼åˆ†æä»ªè¡¨æ¿ï¼Œæ–¹ä¾¿ç”¨æˆ·å¯è§†åŒ–æ™ºèƒ½ä½“çš„çŠ¶æ€å’Œè¡Œä¸ºã€‚

### ğŸŒ å¹³å°è®¿é—®
- å¹³å°ç½‘ç«™ï¼šhttps://www.humanoidagents.com/
- ä»£ç ä»“åº“ï¼šhttps://github.com/HumanoidAgents/HumanoidAgents

## textworld--a-learning-environment-for-text-based-games
### Abstract
We introduce TextWorld, a sandbox learning environment for the training and
evaluation of RL agents on text-based games. TextWorld is a Python library that
handles interactive play-through of text games, as well as backend functions
like state tracking and reward assignment. It comes with a curated list of
games whose features and challenges we have analyzed. More significantly, it
enables users to handcraft or automatically generate new games. Its generative
mechanisms give precise control over the difficulty, scope, and language of
constructed games, and can be used to relax challenges inherent to commercial
text games like partial observability and sparse rewards. By generating sets of
varied but similar games, TextWorld can also be used to study generalization
and transfer learning. We cast text-based games in the Reinforcement Learning
formalism, use our framework to develop a set of benchmark games, and evaluate
several baseline agents on this set and the curated list.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TextWorldï¼šåŸºäºæ–‡æœ¬æ¸¸æˆçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬æ¸¸æˆæ˜¯ä¸€ç§å¤æ‚çš„äº¤äº’å¼æ¨¡æ‹Ÿï¼Œç©å®¶é€šè¿‡è¾“å…¥æ–‡æœ¬å‘½ä»¤æ¥æ¢ç´¢æ¸¸æˆä¸–ç•Œå¹¶è¾¾æˆç›®æ ‡ã€‚è¿™ç±»æ¸¸æˆå¯¹è¯­è¨€ç†è§£ã€é•¿æœŸè®°å¿†ã€è§„åˆ’ã€æ¢ç´¢å’Œå¸¸è¯†æ¨ç†ç­‰æ–¹é¢æå‡ºäº†æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤„ç†æ–‡æœ¬æ¸¸æˆæ—¶é¢ä¸´ç€è¯¸å¤šå›°éš¾ï¼Œä¾‹å¦‚éƒ¨åˆ†å¯è§‚æµ‹æ€§ã€ç¨€ç–å¥–åŠ±ã€å·¨å¤§çš„çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ç­‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
TextWorld æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ–‡æœ¬æ¸¸æˆä¸Šçš„å­¦ä¹ ç¯å¢ƒã€‚å®ƒå…·æœ‰ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆæœºåˆ¶
TextWorld å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆæœºåˆ¶ï¼Œå¯ä»¥è‡ªåŠ¨æ„å»ºæ¸¸æˆä¸–ç•Œã€å¡«å……å¯¹è±¡å’Œéšœç¢ç‰©ï¼Œå¹¶ç”Ÿæˆå®šä¹‰ç›®æ ‡çŠ¶æ€å’Œå¦‚ä½•è¾¾åˆ°ç›®æ ‡çš„ä»»åŠ¡ã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥æ§åˆ¶æ¸¸æˆçš„éš¾åº¦ã€èŒƒå›´å’Œè¯­è¨€ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ä¸åŒç‰¹å¾å’ŒæŒ‘æˆ˜çš„æ¸¸æˆé›†åˆï¼Œç”¨äºç ”ç©¶æ³›åŒ–å’Œè¿ç§»å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€»è¾‘å¼•æ“
TextWorld ä½¿ç”¨çº¿æ€§é€»è¾‘å’Œæ¨ç†å¼•æ“æ¥å®šä¹‰æ¸¸æˆè§„åˆ™å’ŒçŠ¶æ€è½¬æ¢å‡½æ•°ã€‚è¿™ä½¿å¾—æ¸¸æˆçŠ¶æ€å…·æœ‰æ˜ç¡®çš„è¡¨ç¤ºï¼Œå¹¶å¯ä»¥ç²¾ç¡®åœ°è·Ÿè¸ªçŠ¶æ€å’Œåˆ†é…å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒTextWorld è¿˜å¯ä»¥æä¾›ä¸­é—´å¥–åŠ±ï¼Œå¸®åŠ©ä»£ç†å­¦ä¹ æ›´æœ‰æ•ˆçš„ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ–‡æœ¬ç”Ÿæˆ
TextWorld ä½¿ç”¨ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³• (CFG) æ¥ç”Ÿæˆæ¸¸æˆçŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚è¿™ä½¿å¾—æ¸¸æˆä¸–ç•Œå’Œä»»åŠ¡å¯¹äººç±»å¯è§£é‡Šï¼Œå¹¶ä¸”å¯ä»¥æ§åˆ¶æ–‡æœ¬çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
TextWorld æä¾›äº†ä¸€ç³»åˆ—åŸºå‡†æ¸¸æˆï¼Œç”¨äºè¯„ä¼°å¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTextWorld å¯ä»¥æœ‰æ•ˆåœ°è®­ç»ƒä»£ç†è§£å†³æ–‡æœ¬æ¸¸æˆä¸­çš„å„ç§æŒ‘æˆ˜ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TextWorld ä¸ºç ”ç©¶æ–‡æœ¬æ¸¸æˆä¸­çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ã€‚å…¶ç”Ÿæˆæœºåˆ¶ã€é€»è¾‘å¼•æ“å’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å—å¯ä»¥ç”¨äºæ„å»ºå„ç§å¤æ‚çš„æ¸¸æˆç¯å¢ƒï¼Œå¹¶å¸®åŠ©ä»£ç†å­¦ä¹ è¯­è¨€ç†è§£ã€è§„åˆ’å’Œæ¢ç´¢ç­‰æŠ€èƒ½ã€‚æ­¤å¤–ï¼ŒTextWorld è¿˜å¯ä»¥ç”¨äºç ”ç©¶æ³›åŒ–å’Œè¿ç§»å­¦ä¹ ï¼Œä»¥åŠå¼€å‘æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

## counting-to-explore-and-generalize-in-text-based-games
### Abstract
We propose a recurrent RL agent with an episodic exploration mechanism that
helps discovering good policies in text-based game environments. We show
promising results on a set of generated text-based games of varying difficulty
where the goal is to collect a coin located at the end of a chain of rooms. In
contrast to previous text-based RL approaches, we observe that our agent learns
policies that generalize to unseen games of greater difficulty.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºè®¡æ•°æ¢ç´¢å’Œæ³›åŒ–çš„æ–‡æœ¬æ¸¸æˆå¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬æ¸¸æˆæ˜¯ä¸€ç§å¤æ‚çš„äº¤äº’å¼æ¨¡æ‹Ÿï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¸¸æˆçŠ¶æ€ã€æ¥å—ç©å®¶åŠ¨ä½œå¹¶æŠ¥å‘Šç¯å¢ƒå˜åŒ–ã€‚ç©å®¶å¿…é¡»é€šè¿‡æ¢ç´¢æ¥å‘ç°æ¸¸æˆç›®æ ‡ï¼Œè€Œæ¸¸æˆä¸­çš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´éƒ½æ˜¯ç»„åˆå’Œå¤åˆçš„ï¼Œç©å®¶å¿…é¡»åº”å¯¹éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œå› ä¸ºæè¿°æ€§æ–‡æœ¬å¹¶ä¸æä¾›å…³äºåº•å±‚æ¸¸æˆçŠ¶æ€çš„å®Œæ•´ã€æ˜ç¡®çš„ä¿¡æ¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¾ªç¯çš„å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œå…·æœ‰æƒ…èŠ‚æ¢ç´¢æœºåˆ¶ï¼Œæœ‰åŠ©äºåœ¨åŸºäºæ–‡æœ¬çš„æ¸¸æˆç¯å¢ƒä¸­å‘ç°è‰¯å¥½çš„ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºäº†ä¸€ç§æƒ…èŠ‚è®¡æ•°æ¢ç´¢æ–¹æ¡ˆï¼Œå…¶ä¸­çŠ¶æ€è®¡æ•°åœ¨æ¯ä¸ªæƒ…èŠ‚å¼€å§‹æ—¶é‡ç½®ã€‚è¿™ç§å¥–åŠ±å……å½“æƒ…èŠ‚è®°å¿†ï¼Œæ¨åŠ¨ä»£ç†è®¿é—®å½“å‰æƒ…èŠ‚ä¸­å°šæœªé‡åˆ°çš„çŠ¶æ€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸€ç³»åˆ—ç”Ÿæˆçš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆä¸­ï¼Œè¯¥ä»£ç†åœ¨æ”¶é›†ä½äºä¸€ç³»åˆ—æˆ¿é—´æœ«å°¾çš„ç¡¬å¸çš„ç›®æ ‡ä¸Šå–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœã€‚ä¸ä¹‹å‰çš„åŸºäºæ–‡æœ¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè§‚å¯Ÿåˆ°è¯¥ä»£ç†å­¦ä¹ çš„ç­–ç•¥å¯ä»¥æ³›åŒ–åˆ°æœªè§çš„æ›´å…·æŒ‘æˆ˜æ€§çš„æ¸¸æˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è¯¥è®ºæ–‡æå‡ºçš„åŸºäºè®¡æ•°æ¢ç´¢å’Œæ³›åŒ–çš„æ–‡æœ¬æ¸¸æˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¸ºè§£å†³æ–‡æœ¬æ¸¸æˆä¸­çš„æ¢ç´¢å’Œæ³›åŒ–é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦æ¢ç´¢å’Œè®°å¿†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚è¿·å®«æ¢ç´¢ã€è·¯å¾„è§„åˆ’ç­‰ã€‚

## pre-trained-language-models-as-prior-knowledge-for-playing-text-based-games
### Abstract
Recently, text world games have been proposed to enable artificial agents to
understand and reason about real-world scenarios. These text-based games are
challenging for artificial agents, as it requires an understanding of and
interaction using natural language in a partially observable environment.
Agents observe the environment via textual descriptions designed to be
challenging enough for even human players. Past approaches have not paid enough
attention to the language understanding capability of the proposed agents.
Typically, these approaches train from scratch, an agent that learns both
textual representations and the gameplay online during training using a
temporal loss function. Given the sample-inefficiency of RL approaches, it is
inefficient to learn rich enough textual representations to be able to
understand and reason using the textual observation in such a complicated game
environment setting. In this paper, we improve the semantic understanding of
the agent by proposing a simple RL with LM framework where we use
transformer-based language models with Deep RL models. We perform a detailed
study of our framework to demonstrate how our model outperforms all existing
agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6
higher than the state-of-the-art model. Overall, our proposed approach
outperforms 4 games out of the 14 text-based games, while performing comparable
to the state-of-the-art models on the remaining games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æå‡æ–‡æœ¬æ¸¸æˆä¸­çš„æ™ºèƒ½ä½“è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬ä¸–ç•Œæ¸¸æˆä¸ºäººå·¥æ™ºèƒ½ä½“æä¾›äº†ç†è§£å’Œæ¨ç†ç°å®ä¸–ç•Œåœºæ™¯çš„æœºä¼šã€‚ç„¶è€Œï¼Œè¿™äº›æ¸¸æˆå¯¹æ™ºèƒ½ä½“æ¥è¯´æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­ç†è§£å’Œäº¤äº’è‡ªç„¶è¯­è¨€ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ™ºèƒ½ä½“çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”é€šå¸¸ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥æå‡æ™ºèƒ½ä½“çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä»è€Œåœ¨æ–‡æœ¬æ¸¸æˆä¸­å–å¾—æ›´å¥½çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„RLä¸LMæ¡†æ¶ï¼Œä½¿ç”¨åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DistilBERTï¼‰ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç›¸ç»“åˆã€‚é€šè¿‡åœ¨å¤§å‹é€šç”¨è‹±è¯­è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé’ˆå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„è¯­è¨€ç†è§£å’Œå…ˆéªŒçŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¸¸æˆæ„ŸçŸ¥çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
ä¸ºäº†ä½¿é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ›´å¥½åœ°é€‚åº”æ¸¸æˆç¯å¢ƒï¼Œæœ¬æ–‡ä½¿ç”¨ç‹¬ç«‹çš„äººç±»æ¸¸æˆæ’­æ”¾è½¨è¿¹æ•°æ®é›†å¯¹DistilBERTè¿›è¡Œå¾®è°ƒï¼Œä»è€Œä½¿å…¶å…·å¤‡æ¸¸æˆæ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§å¾®è°ƒè¿‡ç¨‹æœ‰åŠ©äºå°†è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œä¸–ç•Œæ„ŸçŸ¥èƒ½åŠ›è½¬ç§»åˆ°ä¸åŒçš„æ¸¸æˆå’Œæ™ºèƒ½ä½“ä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨14ä¸ªæ–‡æœ¬æ¸¸æˆä¸­å¯¹æ‰€æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œåœ¨Zork1æ¸¸æˆä¸­ï¼Œæ¨¡å‹å–å¾—äº†44.7åˆ†çš„æˆç»©ï¼Œæ¯”ç°æœ‰æœ€ä½³æ¨¡å‹é«˜å‡º1.6åˆ†ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶åœ¨4ä¸ªæ¸¸æˆä¸­è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä»–æ¸¸æˆä¸­è¡¨ç°ä¸ç°æœ‰æ¨¡å‹ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†çš„æ–¹æ³•ï¼Œä¸ºæ–‡æœ¬æ¸¸æˆä¸­çš„æ™ºèƒ½ä½“è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œä¸–ç•Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¨ç†æ¸¸æˆç¯å¢ƒï¼Œä»è€Œåœ¨æ¸¸æˆä¸­å–å¾—æ›´å¥½çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šæ¸¸æˆç¯å¢ƒçš„é‡è¦æ€§ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å¯ç¤ºã€‚

## language-model-in-the-loop--data-optimal-approach-to-learn-to-recommend-actions-in-text-games
### Abstract
Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹åœ¨å¾ªç¯ä¸­ï¼šæ–‡æœ¬æ¸¸æˆä¸­å­¦ä¹ æ¨èåŠ¨ä½œçš„æ•°æ®æœ€ä¼˜æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚CALM æ˜¯ä¸€ç§æµè¡Œçš„å­¦ä¹ æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ LLMsï¼ˆä¾‹å¦‚ GPT-2ï¼‰çš„è¯­è¨€å…ˆéªŒçŸ¥è¯†æ¥æ¨èåŠ¨ä½œå€™é€‰è€…ï¼Œä»è€Œåœ¨æ²¡æœ‰ç¯å¢ƒæä¾›åŠ¨ä½œçš„æƒ…å†µä¸‹æé«˜ Jericho ä¸­æ–‡æœ¬æ¸¸æˆçš„è¡¨ç°ã€‚ç„¶è€Œï¼ŒCALM ä½¿ç”¨å¸¦æœ‰äººç±»æ¸¸æˆç©æ³•çš„æ³¨é‡Šæ¥é€‚åº” GPT-2ï¼Œå¹¶åœ¨å­¦ä¹ æ–‡æœ¬æ¸¸æˆæ—¶ä¿æŒ LLM å›ºå®šã€‚è¿™ç§æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„äººç±»æ³¨é‡Šæ¸¸æˆç©æ³•ï¼Œå¹¶ä¸”æ²¡æœ‰å……åˆ†åˆ©ç”¨æ¸¸æˆä¸­çš„è½¬æ¢æ¥è®­ç»ƒ LLMã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å’Œè¯„ä¼°åœ¨æ–‡æœ¬æ¸¸æˆå­¦ä¹ è¿‡ç¨‹ä¸­æ›´æ–°ç”¨äºå€™é€‰æ¨èçš„è¯­è¨€æ¨¡å‹ï¼Œä»¥å‡å°‘å¯¹äººç±»æ³¨é‡Šæ¸¸æˆç©æ³•çš„ä¾èµ–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº† LM-in-the-Loop æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬æ¸¸æˆå­¦ä¹ è¿‡ç¨‹ä¸­æ›´æ–°ç”¨äºå€™é€‰æ¨èçš„è¯­è¨€æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„æ¸¸æˆå†…è½¬æ¢æ¥æ›´æ–° LLMï¼Œä»è€Œå‡å°‘å¯¹äººç±»æ³¨é‡Šæ¸¸æˆç©æ³•çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†æ›´æ–°åçš„ LLM çš„è¿ç§»æ€§ï¼Œå¹¶å‘ç°å°†æ¸¸æˆå†…è®­ç»ƒçš„æ¨¡å‹è¿ç§»åˆ°å…¶ä»–æ¸¸æˆå¹¶ä¸æ€»æ˜¯å¯¼è‡´ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLM-in-the-Loop æ–¹æ³•å¯ä»¥å‡å°‘å¯¹äººç±»æ³¨é‡Šæ¸¸æˆç©æ³•çš„ä¾èµ–ï¼Œå¹¶åŠ é€Ÿæ”¶æ•›ã€‚æ­¤å¤–ï¼ŒåŸºäºçŠ¶æ€ç‰¹å¾çš„è½¬æ¢é€‰æ‹©æ–¹æ³•æ¯”å…¶ä»–æ–¹æ³•æä¾›äº†æ›´å¤§çš„æ”¶ç›Šã€‚ç„¶è€Œï¼ŒLM-in-the-Loop æ–¹æ³•å¹¶ä¸æ€»æ˜¯èƒ½å¤Ÿè¿ç§»åˆ°å…¶ä»–æ¸¸æˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ LM-in-the-Loop æ–¹æ³•ä¸ºæ–‡æœ¬æ¸¸æˆä¸­å­¦ä¹ æ¨èåŠ¨ä½œæä¾›äº†ä¸€ç§æ•°æ®æœ€ä¼˜çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥å‡å°‘å¯¹äººç±»æ³¨é‡Šæ¸¸æˆç©æ³•çš„ä¾èµ–ï¼Œå¹¶åŠ é€Ÿæ”¶æ•›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†æ›´æ–°åçš„ LLM çš„è¿ç§»æ€§ï¼Œå¹¶å‘ç°å°†æ¸¸æˆå†…è®­ç»ƒçš„æ¨¡å‹è¿ç§»åˆ°å…¶ä»–æ¸¸æˆå¹¶ä¸æ€»æ˜¯å¯¼è‡´ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚è¿™äº›å‘ç°å¯¹äºç†è§£å’Œæ”¹è¿›æ–‡æœ¬æ¸¸æˆä¸­è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## keep-calm-and-explore--language-models-for-action-generation-in-text-based-games
### Abstract
Text-based games present a unique challenge for autonomous agents to operate
in natural language and handle enormous action spaces. In this paper, we
propose the Contextual Action Language Model (CALM) to generate a compact set
of action candidates at each game state. Our key insight is to train language
models on human gameplay, where people demonstrate linguistic priors and a
general game sense for promising actions conditioned on game history. We
combine CALM with a reinforcement learning agent which re-ranks the generated
action candidates to maximize in-game rewards. We evaluate our approach using
the Jericho benchmark, on games unseen by CALM during training. Our method
obtains a 69% relative improvement in average game score over the previous
state-of-the-art model. Surprisingly, on half of these games, CALM is
competitive with or better than other models that have access to ground truth
admissible actions. Code and data are available at
https://github.com/princeton-nlp/calm-textgame.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Keep CALM and Explore: è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„åŠ¨ä½œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬æ¸¸æˆä¸ºè‡ªä¸»ä»£ç†åœ¨è‡ªç„¶è¯­è¨€ä¸­æ“ä½œå’Œå¤„ç†å·¨å¤§çš„åŠ¨ä½œç©ºé—´å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚åœ¨æ–‡æœ¬æ¸¸æˆä¸­ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†åŠ¨ä½œå‘½ä»¤åœ¨ä»»ä½•ç»™å®šçš„æ¸¸æˆçŠ¶æ€ä¸‹æ˜¯å¯æ¥å—çš„ã€‚å¯æ¥å—çš„åŠ¨ä½œæ˜¯æŒ‡å¯ä»¥è¢«æ¸¸æˆå¼•æ“è§£æå¹¶æ”¹å˜åº•å±‚æ¸¸æˆçŠ¶æ€çš„åŠ¨ä½œã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä»æ¸¸æˆè¯æ±‡ä¸­éšæœºé‡‡æ ·åŠ¨ä½œä¼šå¯¼è‡´è®¸å¤šä¸å¯æ¥å—çš„åŠ¨ä½œï¼Œå¦‚â€œnorth aâ€æˆ–â€œeat troll with eggâ€ã€‚å› æ­¤ï¼Œå°†åŠ¨ä½œç©ºé—´ç¼©å°åˆ°å¯æ¥å—çš„åŠ¨ä½œéœ€è¦è¯­æ³•å’Œè¯­ä¹‰çŸ¥è¯†ï¼Œè¿™å¯¹å½“å‰ç³»ç»Ÿæ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸Šä¸‹æ–‡åŠ¨ä½œè¯­è¨€æ¨¡å‹ï¼ˆCALMï¼‰ï¼Œä»¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¸¸æˆçš„æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨CALMç”ŸæˆåŠ¨ä½œå€™é€‰è€…ï¼Œç„¶åå°†å®ƒä»¬è¾“å…¥åˆ°æ·±åº¦å¼ºåŒ–ç›¸å…³ç½‘ç»œï¼ˆDRRNï¼‰ä¸­ï¼Œè¯¥ç½‘ç»œä½¿ç”¨æ¸¸æˆå¥–åŠ±æ¥å­¦ä¹ è¿™äº›åŠ¨ä½œçš„ä»·å€¼å‡½æ•°ã€‚è¿™ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå°†åŠ¨ä½œç”Ÿæˆçš„é€šç”¨è¯­è¨€å…ˆéªŒä¸è‡ªé€‚åº”é€‰æ‹©æœ€é€‚åˆæ¸¸æˆçš„åŠ¨ä½œçš„èƒ½åŠ›ç›¸ç»“åˆã€‚

ä¸ºäº†è®­ç»ƒCALMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«426ä¸ªäººç±»æ¸¸æˆå›æ”¾è®°å½•çš„æ–°æ•°æ®é›†ï¼Œè¿™äº›è®°å½•æ¥è‡ª590ä¸ªä¸åŒçš„æ–‡æœ¬æ¸¸æˆã€‚è™½ç„¶è¿™äº›å›æ”¾è®°å½•æ˜¯å˜ˆæ‚çš„ï¼Œå¹¶ä¸”åŠ¨ä½œå¹¶ä¸æ€»æ˜¯æœ€ä¼˜çš„ï¼Œä½†å®ƒä»¬åŒ…å«å¤§é‡çš„è¯­è¨€å…ˆéªŒå’Œæ¸¸æˆæ„Ÿã€‚ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªCALMå®ä¾‹ï¼Œå¹¶å°†å…¶éƒ¨ç½²åˆ°è®¸å¤šä¸åŒçš„ä¸‹æ¸¸æ¸¸æˆä¸­ã€‚é‡è¦çš„æ˜¯ï¼Œä¸ºäº†å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ä»»ä½•æ¥è‡ªæˆ‘ä»¬è¯„ä¼°æ¸¸æˆçš„å›æ”¾è®°å½•æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æˆ‘ä»¬ä½¿ç”¨JerichoåŸºå‡†ï¼ˆHausknechtç­‰äººï¼Œ2019aï¼‰å¯¹28ä¸ªæ¸¸æˆè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›æ¸¸æˆåœ¨CALMè®­ç»ƒæœŸé—´æ²¡æœ‰è§è¿‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡æ¸¸æˆå¾—åˆ†æ–¹é¢æ¯”ä»¥å‰æœ€å…ˆè¿›çš„æ¨¡å‹æé«˜äº†69%ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨è¿™äº›æ¸¸æˆä¸­çš„ä¸€åŠï¼ŒCALMä¸å…¶ä»–å…·æœ‰å¯æ¥å—åŠ¨ä½œçœŸå®å€¼çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„CALMæ¨¡å‹ä¸ºæ–‡æœ¬æ¸¸æˆä¸­çš„åŠ¨ä½œç”Ÿæˆæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚é€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ï¼ŒCALMèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„åŠ¨ä½œï¼Œä»è€Œæé«˜æ¸¸æˆä»£ç†çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„äººç±»æ¸¸æˆå›æ”¾è®°å½•æ•°æ®é›†ï¼Œä¸ºæ–‡æœ¬æ¸¸æˆç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚

## graph-constrained-reinforcement-learning-for-natural-language-action-spaces
### Abstract
Interactive Fiction games are text-based simulations in which an agent
interacts with the world purely through natural language. They are ideal
environments for studying how to extend reinforcement learning agents to meet
the challenges of natural language understanding, partial observability, and
action generation in combinatorially-large text-based action spaces. We present
KG-A2C, an agent that builds a dynamic knowledge graph while exploring and
generates actions using a template-based action space. We contend that the dual
uses of the knowledge graph to reason about game state and to constrain natural
language generation are the keys to scalable exploration of combinatorially
large natural language actions. Results across a wide variety of IF games show
that KG-A2C outperforms current IF agents despite the exponential increase in
action space size.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å›¾çº¦æŸå¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€åŠ¨ä½œç©ºé—´ä¸­çš„åº”ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªç„¶è¯­è¨€äº¤äº’é•¿æœŸä»¥æ¥è¢«è®¤ä¸ºæ˜¯äººç±»æ™ºèƒ½çš„ä¸€ä¸ªæ ‡å¿—æ€§ç‰¹å¾ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶å¦‚ä½•ä½¿å­¦ä¹ ä»£ç†èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„è‡ªç„¶è¯­è¨€ï¼Œä»¥å®ç°ç›®æ ‡ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæœ¬æ–‡ç ”ç©¶äº†äº¤äº’å¼å°è¯´ï¼ˆIFï¼‰æ¸¸æˆï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬å†’é™©æ¸¸æˆï¼Œå…¶ä¸­ä»£ç†å®Œå…¨é€šè¿‡è‡ªç„¶è¯­è¨€ä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’ã€‚IFæ¸¸æˆæ˜¯ç ”ç©¶å¦‚ä½•æ‰©å±•å¼ºåŒ–å­¦ä¹ ä»£ç†ä»¥åº”å¯¹è‡ªç„¶è¯­è¨€ç†è§£ã€éƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’Œåœ¨ç»„åˆæ€§å¤§çš„æ–‡æœ¬åŠ¨ä½œç©ºé—´ä¸­ç”ŸæˆåŠ¨ä½œçš„æŒ‘æˆ˜çš„ç†æƒ³ç¯å¢ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†KG-A2Cï¼Œä¸€ç§åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­æ„å»ºåŠ¨æ€çŸ¥è¯†å›¾å¹¶ä½¿ç”¨åŸºäºæ¨¡æ¿çš„åŠ¨ä½œç©ºé—´ç”ŸæˆåŠ¨ä½œçš„ä»£ç†ã€‚æœ¬æ–‡è®¤ä¸ºï¼ŒçŸ¥è¯†å›¾çš„åŒé‡ç”¨é€”ï¼Œå³ç”¨äºæ¨ç†æ¸¸æˆçŠ¶æ€å’Œçº¦æŸè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œæ˜¯å¯æ‰©å±•åœ°æ¢ç´¢ç»„åˆæ€§å¤§çš„è‡ªç„¶è¯­è¨€åŠ¨ä½œçš„å…³é”®ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçŸ¥è¯†å›¾çŠ¶æ€ç©ºé—´
KG-A2Cä½¿ç”¨çŸ¥è¯†å›¾ä½œä¸ºçŠ¶æ€è¡¨ç¤ºï¼Œè¯¥çŸ¥è¯†å›¾åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å­¦ä¹ ã€‚çŸ¥è¯†å›¾å­˜å‚¨ä¸ºä¸€ç³»åˆ—ä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€å…³ç³»å’Œå¯¹è±¡ã€‚è¿™äº›ä¸‰å…ƒç»„ä»è§‚å¯Ÿä¸­æå–ï¼Œå¹¶ä½¿ç”¨æ–¯å¦ç¦å¤§å­¦çš„å¼€æ”¾ä¿¡æ¯æå–ï¼ˆOpenIEï¼‰å·¥å…·è¿›è¡Œæ›´æ–°ã€‚çŸ¥è¯†å›¾å¸®åŠ©ä»£ç†å½¢æˆå®ƒæ­£åœ¨æ¢ç´¢çš„ä¸–ç•Œçš„åœ°å›¾ï¼Œå¹¶ä¿ç•™å®ƒå·²ç»å­¦ä¹ åˆ°çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ä¸å¯¹è±¡ç›¸å…³çš„åŠŸèƒ½ã€è§’è‰²çš„å±æ€§ã€å½“å‰åº“å­˜ç­‰ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡æ¿åŠ¨ä½œç©ºé—´
KG-A2Cä½¿ç”¨åŸºäºæ¨¡æ¿çš„åŠ¨ä½œç©ºé—´ï¼Œå…¶ä¸­ä»£ç†é¦–å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿ï¼Œç„¶åä½¿ç”¨æ¸¸æˆè¯æ±‡è¡¨ä¸­çš„å•è¯å¡«å†™ç©ºç™½ã€‚æ¨¡æ¿å’Œè¯æ±‡è¡¨å•è¯é€šè¿‡Jerichoæ¡†æ¶ç¨‹åºåŒ–åœ°å¯ç”¨ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªIFæ¸¸æˆéƒ½æ˜¯å¯ç”¨çš„ã€‚æ¨¡æ¿ä¸ºåŠ¨ä½œç©ºé—´æä¾›äº†ç»“æ„ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢åŠ¨ä½œç©ºé—´ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸€ç³»åˆ—IFæ¸¸æˆä¸Šæµ‹è¯•äº†KG-A2Cï¼Œå¹¶å°†å…¶ä¸å½“å‰IFä»£ç†è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒKG-A2Cåœ¨å¤§å¤šæ•°æ¸¸æˆä¸­éƒ½ä¼˜äºå½“å‰IFä»£ç†ï¼Œå°½ç®¡åŠ¨ä½œç©ºé—´çš„å¤§å°å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„KG-A2Cæ–¹æ³•ä¸ºè‡ªç„¶è¯­è¨€äº¤äº’å¼æ¸¸æˆæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚çŸ¥è¯†å›¾çŠ¶æ€ç©ºé—´å’Œæ¨¡æ¿åŠ¨ä½œç©ºé—´çš„ç»“åˆä½¿å¾—ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ç»„åˆæ€§å¤§çš„è‡ªç„¶è¯­è¨€åŠ¨ä½œç©ºé—´ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºå¼€å‘æ›´æ™ºèƒ½çš„è‡ªç„¶è¯­è¨€äº¤äº’å¼æ¸¸æˆä»£ç†å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## interactive-fiction-game-playing-as-multi-paragraph-reading-comprehension-with-reinforcement-learning
### Abstract
Interactive Fiction (IF) games with real human-written natural language texts
provide a new natural evaluation for language understanding techniques. In
contrast to previous text games with mostly synthetic texts, IF games pose
language understanding challenges on the human-written textual descriptions of
diverse and sophisticated game worlds and language generation challenges on the
action command generation from less restricted combinatorial space. We take a
novel perspective of IF game solving and re-formulate it as Multi-Passage
Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query
attention mechanisms and the structured prediction in MPRC to efficiently
generate and evaluate action outputs and apply an object-centric historical
observation retrieval strategy to mitigate the partial observability of the
textual observations. Extensive experiments on the recent IF benchmark
(Jericho) demonstrate clear advantages of our approaches achieving high winning
rates and low data requirements compared to all previous approaches. Our source
code is available at: https://github.com/XiaoxiaoGuo/rcdqn.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤šæ®µè½é˜…è¯»ç†è§£ä¸å¼ºåŒ–å­¦ä¹ è§£å†³äº’åŠ¨å¼å°è¯´æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äº’åŠ¨å¼å°è¯´ï¼ˆInteractive Fiction, IFï¼‰æ¸¸æˆä»¥å…¶ä¸°å¯Œçš„æ–‡æœ¬æè¿°å’Œå¤æ‚çš„æ¸¸æˆä¸–ç•Œä¸ºè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰æŠ€æœ¯æä¾›äº†æ–°çš„æŒ‘æˆ˜ã€‚ä¸ä»¥å¾€ä¸»è¦ä½¿ç”¨åˆæˆæ–‡æœ¬çš„æ–‡æœ¬æ¸¸æˆä¸åŒï¼ŒIF æ¸¸æˆä¸­çš„æ–‡æœ¬æè¿°æ›´åŠ å¤šæ ·åŒ–å’Œå¤æ‚ï¼Œå¯¹è¯­è¨€ç†è§£æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚æ­¤å¤–ï¼ŒIF æ¸¸æˆä¸­çš„è¡ŒåŠ¨å‘½ä»¤ç”Ÿæˆä¹Ÿé¢ä¸´ç€æ›´å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç©å®¶å¯ä»¥ä½¿ç”¨çš„è¡ŒåŠ¨å‘½ä»¤ç»„åˆç©ºé—´æ›´åŠ å¼€æ”¾å’Œè‡ªç”±ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°† IF æ¸¸æˆè§£å†³è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºå¤šæ®µè½é˜…è¯»ç†è§£ï¼ˆMPRCï¼‰ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº† MPRC ä¸­çš„ä¸Šä¸‹æ–‡-æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶å’Œç»“æ„åŒ–é¢„æµ‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆå’Œè¯„ä¼°è¡ŒåŠ¨è¾“å‡ºï¼Œå¹¶åº”ç”¨ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å†å²è§‚å¯Ÿæ£€ç´¢ç­–ç•¥æ¥ç¼“è§£æ–‡æœ¬è§‚å¯Ÿçš„å±€éƒ¨å¯è§‚æµ‹æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºäº†ä¸€ç§åŸºäºé˜…è¯»ç†è§£æ¨¡å‹ï¼ˆRCï¼‰çš„æ¨¡æ¿è¡ŒåŠ¨é¢„æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†è§‚å¯Ÿè§†ä¸ºæ®µè½ï¼Œå°†æ¨¡æ¿åŠ¨è¯çŸ­è¯­è§†ä¸ºé—®é¢˜ï¼Œå¹¶å°†æ¨¡æ¿ä¸­å¯¹è±¡å ä½ç¬¦çš„å¡«å……è§†ä¸ºä»è§‚å¯Ÿä¸­æå–å¯¹è±¡çš„æå–å¼é—®ç­”ï¼ˆQAï¼‰é—®é¢˜ã€‚åŒæ—¶ï¼Œæ¯ä¸ªè¡ŒåŠ¨ï¼ˆå³æ‰€æœ‰å ä½ç¬¦éƒ½è¢«æ›¿æ¢çš„æ¨¡æ¿ï¼‰éƒ½é€šè¿‡ RC æ¨¡å‹é¢„æµ‹å…¶è¯„ä¼°å€¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¯¹è±¡çš„å†å²è§‚å¯Ÿæ£€ç´¢ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ ¹æ®å½“å‰è§‚å¯Ÿä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼Œæ£€ç´¢æœ€è¿‘ K ä¸ªè‡³å°‘å…±äº«ä¸€ä¸ªå¯¹è±¡çš„è§‚å¯Ÿã€‚è¿™äº›æ£€ç´¢åˆ°çš„è§‚å¯ŸæŒ‰æ—¶é—´æ­¥æ’åºå¹¶è¿æ¥åˆ°å½“å‰è§‚å¯Ÿï¼Œä»¥å¢å¼ºå½“å‰è§‚å¯Ÿçš„ä¿¡æ¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Jericho IF æ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸æ‰€æœ‰å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ 25 ä¸ªæ¸¸æˆä¸­çš„èƒœç‡è¾¾åˆ°äº† 64%ï¼Œå¹¶ä¸”æ‰€éœ€çš„æ•°æ®é‡ä¸åˆ°å…ˆå‰æ–¹æ³•çš„ååˆ†ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œç»“æœè¡¨æ˜ï¼ŒRC æ¨¡å‹è®¾è®¡å’Œæ£€ç´¢ç­–ç•¥å¯¹æ€§èƒ½æå‡èµ·ç€é‡è¦ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å°† IF æ¸¸æˆè§£å†³è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸º MPRC ä»»åŠ¡çš„æ–¹æ³•ï¼Œä¸ºè§£å†³ IF æ¸¸æˆä¸­çš„å·¨å¤§ç»„åˆè¡ŒåŠ¨ç©ºé—´å’Œå±€éƒ¨å¯è§‚æµ‹æ€§æŒ‘æˆ˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºäº RC æ¨¡å‹çš„æ¨¡æ¿è¡ŒåŠ¨é¢„æµ‹æ¨¡å‹å’ŒåŸºäºå¯¹è±¡çš„å†å²è§‚å¯Ÿæ£€ç´¢ç­–ç•¥ï¼Œä¹Ÿä¸ºè§£å†³å…¶ä»– NLU ä»»åŠ¡æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒã€‚

## bart--denoising-sequence-to-sequence-pre-training-for-natural-language-generation--translation--and-comprehension
### Abstract
We present BART, a denoising autoencoder for pretraining sequence-to-sequence
models. BART is trained by (1) corrupting text with an arbitrary noising
function, and (2) learning a model to reconstruct the original text. It uses a
standard Tranformer-based neural machine translation architecture which,
despite its simplicity, can be seen as generalizing BERT (due to the
bidirectional encoder), GPT (with the left-to-right decoder), and many other
more recent pretraining schemes. We evaluate a number of noising approaches,
finding the best performance by both randomly shuffling the order of the
original sentences and using a novel in-filling scheme, where spans of text are
replaced with a single mask token. BART is particularly effective when fine
tuned for text generation but also works well for comprehension tasks. It
matches the performance of RoBERTa with comparable training resources on GLUE
and SQuAD, achieves new state-of-the-art results on a range of abstractive
dialogue, question answering, and summarization tasks, with gains of up to 6
ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system
for machine translation, with only target language pretraining. We also report
ablation experiments that replicate other pretraining schemes within the BART
framework, to better measure which factors most influence end-task performance.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BARTï¼šè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œç†è§£çš„é™å™ªåºåˆ—åˆ°åºåˆ—é¢„è®­ç»ƒæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤šç§NLPä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹å¾€å¾€é’ˆå¯¹ç‰¹å®šç±»å‹çš„ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚ç”¨èŒƒå›´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBARTçš„é™å™ªè‡ªç¼–ç å™¨ï¼Œæ—¨åœ¨é€šè¿‡é¢„è®­ç»ƒåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œæé«˜æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œç†è§£ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé™å™ªè‡ªç¼–ç å™¨
BARTé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„é™å™ªè‡ªç¼–ç å™¨æ¶æ„ï¼Œé€šè¿‡å°†æ–‡æœ¬è¿›è¡Œéšæœºå™ªå£°å¤„ç†ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ªæ¨¡å‹æ¥é‡å»ºåŸå§‹æ–‡æœ¬ã€‚è¿™ç§æ¶æ„å…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æ–‡æœ¬å™ªå£°å¤„ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬éšæœºæ‰“ä¹±å¥å­é¡ºåºã€ä½¿ç”¨æ©ç æ›¿æ¢æ–‡æœ¬ç‰‡æ®µç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šTransformeræ¶æ„
BARTä½¿ç”¨æ ‡å‡†çš„Transformeræ¶æ„ï¼ŒåŒ…æ‹¬åŒå‘ç¼–ç å™¨å’Œè‡ªå›å½’è§£ç å™¨ã€‚è¿™ç§æ¶æ„ç®€å•è€Œå¼ºå¤§ï¼Œå¯ä»¥çœ‹ä½œæ˜¯å¯¹BERTï¼ˆåŒå‘ç¼–ç å™¨ï¼‰å’ŒGPTï¼ˆè‡ªå›å½’è§£ç å™¨ï¼‰çš„æ³›åŒ–ï¼ŒåŒæ—¶é€‚ç”¨äºå¤šç§é¢„è®­ç»ƒæ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
BARTåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒBARTåœ¨æ‘˜è¦ã€é—®ç­”å’Œå¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„SOTAç»“æœï¼Œå¹¶åœ¨ROUGEæŒ‡æ ‡ä¸Šå–å¾—äº†é«˜è¾¾6åˆ†çš„æå‡ã€‚åœ¨ç†è§£ä»»åŠ¡ä¸­ï¼ŒBARTåœ¨GLUEå’ŒSQuADç­‰åŸºå‡†æµ‹è¯•ä¸­ä¸RoBERTaç›¸å½“ï¼Œå¹¶åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­å–å¾—äº†1.1 BLEUçš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
BARTçš„é™å™ªè‡ªç¼–ç å™¨æ¶æ„ä¸ºNLPé¢„è®­ç»ƒæ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æ–‡æœ¬å™ªå£°å¤„ç†æ–¹æ³•ï¼Œå¹¶æé«˜æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒBARTçš„Transformeræ¶æ„ç®€å•è€Œå¼ºå¤§ï¼Œå¯ä»¥çœ‹ä½œæ˜¯å¯¹BERTå’ŒGPTçš„æ³›åŒ–ï¼Œä¸ºNLPç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games
### Abstract
In this study, we explore the application of Large Language Models (LLMs) in
\textit{Jubensha}, a Chinese detective role-playing game and a novel area in
Artificial Intelligence (AI) driven gaming. We introduce the first dataset
specifically for Jubensha, including character scripts and game rules, to
foster AI agent development in this complex narrative environment. Our work
also presents a unique multi-agent interaction framework using LLMs, allowing
AI agents to autonomously engage in this game. To evaluate the gaming
performance of these AI agents, we developed novel methods measuring their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents'
performance in information gathering, murderer identification, and logical
reasoning. The experimental results validate the effectiveness of our proposed
methods. This work aims to offer a novel perspective on understanding LLM
capabilities and establish a new benchmark for evaluating large language
model-based agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£ç æ•°å­—ä¾¦æ¢ï¼šç†è§£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†æ¸¸æˆä¸­çš„è¡Œä¸ºå’Œèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äº’åŠ¨è§’è‰²æ‰®æ¼”æ¸¸æˆï¼ˆIRPGsï¼‰çš„å…¨çƒæµè¡Œï¼Œç‰¹åˆ«æ˜¯ä¸­å›½ä¾¦æ¢è§’è‰²æ‰®æ¼”æ¸¸æˆâ€œå‰§æœ¬æ€â€ï¼ˆJubenshaï¼‰çš„å…´èµ·ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨ä¹Ÿæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¼ ç»Ÿçš„æ£‹ç±»æ¸¸æˆã€è§†é¢‘æ¸¸æˆç­‰é¢†åŸŸï¼Œå¯¹äºâ€œå‰§æœ¬æ€â€è¿™ç±»éœ€è¦å¤šè½®è¯­è¨€äº¤äº’ã€ä¿¡æ¯æ”¶é›†å’Œé€»è¾‘æ¨ç†çš„æ¸¸æˆï¼ŒAIçš„åº”ç”¨è¿˜å¤„äºèµ·æ­¥é˜¶æ®µã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥è¡¡é‡LLMåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹â€œå‰§æœ¬æ€â€æ¸¸æˆçš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬è§’è‰²å‰§æœ¬å’Œé¢„è®¾æ¸¸æˆè§„åˆ™ï¼Œä¸ºAIä»£ç†çš„å¼€å‘æä¾›äº†åŸºç¡€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ä¸€ä¸ªç‹¬ç‰¹çš„å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œä½¿ç”¨LLMsä½¿AIä»£ç†èƒ½å¤Ÿè‡ªä¸»å‚ä¸â€œå‰§æœ¬æ€â€æ¸¸æˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ºäº†è¯„ä¼°AIä»£ç†åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œè®¾è®¡äº†ä¸¤ä¸ªæ–°é¢–çš„ä»»åŠ¡ï¼šä¸€ä¸ªç”¨äºè¯„ä¼°ä»–ä»¬å¯¹æ¡ˆä»¶ä¿¡æ¯çš„æŒæ¡ç¨‹åº¦ï¼Œå¦ä¸€ä¸ªç”¨äºè¯„ä¼°ä»–ä»¬çš„æ¨ç†èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ©ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œè®¾è®¡äº†æ¨¡å—æ¥å¢å¼ºLLMä»£ç†åœ¨ä¿¡æ¯æ”¶é›†ã€å‡¶æ‰‹è¯†åˆ«å’Œé€»è¾‘æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä¿¡æ¯æ”¶é›†ã€å‡¶æ‰‹è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾è‘—æé«˜äº†LLMä»£ç†çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æ²¡æœ‰è®°å¿†æ£€ç´¢æ¨¡å—çš„ä»£ç†ç›¸æ¯”ï¼Œå…·æœ‰è®°å¿†æ£€ç´¢æ¨¡å—çš„ä»£ç†åœ¨å›ç­”å…³äºå…¶ä»–è§’è‰²çš„é—®é¢˜æ—¶å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œè‡ªå®Œå–„å’Œè‡ªéªŒè¯æ¨¡å—çš„ç»„åˆè¿›ä¸€æ­¥æé«˜äº†ä»£ç†çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜è¿™äº›æ¨¡å—æœ‰æ•ˆåœ°å¢å¼ºäº†ä»£ç†åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„æ²Ÿé€šæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºLLMsåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºè¯„ä¼°LLMä»£ç†çš„æ€§èƒ½å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ThinkThriceæ¡†æ¶å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å—çš„è®¾è®¡ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å…·æ¨ç†èƒ½åŠ›çš„AIä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## chess-as-a-testbed-for-language-model-state-tracking
### Abstract
Transformer language models have made tremendous strides in natural language
understanding tasks. However, the complexity of natural language makes it
challenging to ascertain how accurately these models are tracking the world
state underlying the text. Motivated by this issue, we consider the task of
language modeling for the game of chess. Unlike natural language, chess
notations describe a simple, constrained, and deterministic domain. Moreover,
we observe that the appropriate choice of chess notation allows for directly
probing the world state, without requiring any additional probing-related
machinery. We find that: (a) With enough training data, transformer language
models can learn to track pieces and predict legal moves with high accuracy
when trained solely on move sequences. (b) For small training sets providing
access to board state information during training can yield significant
improvements. (c) The success of transformer language models is dependent on
access to the entire game history i.e. "full attention". Approximating this
full attention results in a significant performance drop. We propose this
testbed as a benchmark for future work on the development and analysis of
transformer language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å›½é™…è±¡æ£‹ï¼šè¯­è¨€æ¨¡å‹çŠ¶æ€è·Ÿè¸ªçš„æµ‹è¯•å¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€Transformerè¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„å·¨å¤§è¿›æ­¥ï¼Œäººä»¬å¼€å§‹å…³æ³¨è¿™äº›æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå‡†ç¡®è·Ÿè¸ªæ–‡æœ¬èƒŒåçš„ä¸–ç•ŒçŠ¶æ€ã€‚ç„¶è€Œï¼Œè‡ªç„¶è¯­è¨€çš„å¤æ‚æ€§ä½¿å¾—è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®æ€§å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå°†å›½é™…è±¡æ£‹ä½œä¸ºè¯­è¨€æ¨¡å‹çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›çš„æµ‹è¯•å¹³å°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†å›½é™…è±¡æ£‹ä½œä¸ºæµ‹è¯•å¹³å°
å›½é™…è±¡æ£‹æ˜¯ä¸€ä¸ªç®€å•ã€å—é™ä¸”ç¡®å®šæ€§çš„é¢†åŸŸï¼Œå…¶æ£‹è°±æè¿°å¯ä»¥ç›´æ¥åæ˜ æ£‹ç›˜çŠ¶æ€ï¼Œæ— éœ€é¢å¤–çš„æ¢æµ‹æœºåˆ¶ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥æ›´ç²¾ç¡®åœ°è¯„ä¼°è¯­è¨€æ¨¡å‹å¯¹ä¸–ç•ŒçŠ¶æ€çš„è·Ÿè¸ªèƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨åˆé€‚çš„æ£‹è°±è¡¨ç¤ºæ³•
æœ¬æ–‡ä½¿ç”¨å›½é™…è±¡æ£‹é€šç”¨æ¥å£ï¼ˆUCIï¼‰è¡¨ç¤ºæ³•ï¼Œå®ƒå°†èµ·å§‹æ–¹æ ¼å’Œç›®æ ‡æ–¹æ ¼ç»“åˆèµ·æ¥è¡¨ç¤ºä¸€ä¸ªç§»åŠ¨ã€‚è¿™ç§è¡¨ç¤ºæ³•å…è®¸æˆ‘ä»¬é€šè¿‡ç®€å•çš„æç¤ºæ¥æ¢æµ‹è¯­è¨€æ¨¡å‹å¯¹æ£‹ç›˜çŠ¶æ€çš„ç†è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥éšæœºæ ‡æ³¨æ£‹å­ç±»å‹ï¼ˆRAPï¼‰
ä¸ºäº†æ›´ç›´æ¥åœ°æ¢æµ‹æ¨¡å‹çš„çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»¥ä¸€å®šçš„æ¦‚ç‡éšæœºåŒ…å«æ£‹å­ç±»å‹æ ‡è®°ã€‚è¿™å¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ è·Ÿè¸ªæ£‹å­ä½ç½®ï¼Œå¹¶æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè®¾è®¡æ£‹ç›˜çŠ¶æ€æ¢æµ‹ä»»åŠ¡
æœ¬æ–‡è®¾è®¡äº†å¤šç§æ£‹ç›˜çŠ¶æ€æ¢æµ‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬é¢„æµ‹ç§»åŠ¨çš„èµ·å§‹æ–¹æ ¼å’Œç›®æ ‡æ–¹æ ¼ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹æ£‹ç›˜çŠ¶æ€å’Œæ£‹è§„çš„ç†è§£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ç»™å®šè¶³å¤Ÿçš„è®­ç»ƒæ•°æ®æ—¶ï¼ŒTransformerè¯­è¨€æ¨¡å‹å¯ä»¥å­¦ä¹ è·Ÿè¸ªæ£‹å­ä½ç½®å¹¶é¢„æµ‹åˆæ³•ç§»åŠ¨ï¼Œå‡†ç¡®ç‡å¾ˆé«˜ã€‚ç„¶è€Œï¼Œå½“è®­ç»ƒæ•°æ®è¾ƒå°‘æ—¶ï¼Œé¢„æµ‹èƒ½åŠ›ä¼šä¸‹é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼•å…¥éƒ¨åˆ†æ£‹ç›˜çŠ¶æ€ä¿¡æ¯å¯ä»¥æ˜¾è‘—æé«˜æ£‹å­è·Ÿè¸ªçš„å‡†ç¡®ç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å›½é™…è±¡æ£‹æµ‹è¯•å¹³å°ä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºTransformerè¯­è¨€æ¨¡å‹çš„å‘å±•å’Œåˆ†ææä¾›äº†æ–°çš„è§è§£ï¼Œä¾‹å¦‚æ¨¡å‹å¯¹è¾“å…¥åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§ä»¥åŠå¯¹å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„éœ€æ±‚ã€‚è¿™äº›å‘ç°å¯ä»¥æŒ‡å¯¼æœªæ¥Transformeræ¶æ„çš„è®¾è®¡ï¼Œä½¿å…¶æ›´æ“…é•¿ç†è§£é•¿æ–‡æœ¬å¹¶ä»å°‘é‡è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ã€‚

## deepstack--expert-level-artificial-intelligence-in-no-limit-poker
### Abstract
Artificial intelligence has seen several breakthroughs in recent years, with
games often serving as milestones. A common feature of these games is that
players have perfect information. Poker is the quintessential game of imperfect
information, and a longstanding challenge problem in artificial intelligence.
We introduce DeepStack, an algorithm for imperfect information settings. It
combines recursive reasoning to handle information asymmetry, decomposition to
focus computation on the relevant decision, and a form of intuition that is
automatically learned from self-play using deep learning. In a study involving
44,000 hands of poker, DeepStack defeated with statistical significance
professional poker players in heads-up no-limit Texas hold'em. The approach is
theoretically sound and is shown to produce more difficult to exploit
strategies than prior approaches.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DeepStackï¼šåœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­å®ç°ä¸“å®¶çº§äººå·¥æ™ºèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½åœ¨è®¸å¤šæ¸¸æˆä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä¾‹å¦‚å›½é™…è±¡æ£‹ã€å›´æ£‹ç­‰ã€‚è¿™äº›æ¸¸æˆçš„ä¸€ä¸ªå…±åŒç‰¹ç‚¹æ˜¯ç©å®¶æ‹¥æœ‰å®Œç¾ä¿¡æ¯ï¼Œå³æ‰€æœ‰ç©å®¶éƒ½èƒ½å®Œå…¨äº†è§£å½“å‰çš„æ¸¸æˆçŠ¶æ€ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„è®¸å¤šé—®é¢˜éƒ½æ¶‰åŠåˆ°ä¿¡æ¯ä¸å¯¹ç§°ï¼Œå³ç©å®¶æ‹¥æœ‰ä¸åŒçš„ä¿¡æ¯ã€‚æ‰‘å…‹ç‰Œæ¸¸æˆå°±æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­ï¼Œç©å®¶æ‹¥æœ‰ç§äººç‰Œï¼Œè¿™ä½¿å¾—æ¸¸æˆå……æ»¡äº†ä¸ç¡®å®šæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
DeepStack æ˜¯ä¸€ç§é’ˆå¯¹ä¸å®Œç¾ä¿¡æ¯åœºæ™¯çš„ç®—æ³•ï¼Œå®ƒç»“åˆäº†é€’å½’æ¨ç†ã€åˆ†è§£å’Œæ·±åº¦å­¦ä¹ ï¼Œå®ç°äº†åœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è¾¾åˆ°ä¸“å®¶çº§æ°´å¹³ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€’å½’æ¨ç†
DeepStack ä½¿ç”¨é€’å½’æ¨ç†æ¥å¤„ç†ä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ†æå¯¹æ‰‹è¿‡å»çš„è¡ŒåŠ¨æ¥æ¨æ–­å¯¹æ‰‹å¯èƒ½æŒæœ‰çš„ç§äººç‰Œï¼Œä»è€Œåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ†è§£
DeepStack ä½¿ç”¨åˆ†è§£æŠ€æœ¯å°†å¤æ‚çš„æ¸¸æˆçŠ¶æ€åˆ†è§£æˆæ›´å°çš„å­é—®é¢˜ï¼Œä»è€Œå°†è®¡ç®—é›†ä¸­åœ¨ç›¸å…³çš„å†³ç­–ä¸Šã€‚è¿™å¤§å¤§æé«˜äº†ç®—æ³•çš„æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ·±åº¦å­¦ä¹ 
DeepStack ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¥è‡ªåŠ¨å­¦ä¹ ä¸€ç§ç›´è§‰ï¼Œå³å¯¹æŒæœ‰ä»»ä½•å¯èƒ½çš„ç§äººç‰Œåœ¨ä»»ä½•å¯èƒ½çš„æ‰‘å…‹ç‰Œæƒ…å†µä¸‹çš„ä»·å€¼è¿›è¡Œå¿«é€Ÿä¼°è®¡ã€‚è¿™ç§ç›´è§‰å¯ä»¥å¸®åŠ© DeepStack åœ¨æ¸¸æˆä¸­åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
DeepStack åœ¨ä¸ä¸“ä¸šæ‰‘å…‹ç‰Œç©å®¶çš„å¯¹æˆ˜ä¸­å–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚åœ¨ä¸€é¡¹æ¶‰åŠ 44,000 æ‰‹æ‰‘å…‹ç‰Œçš„ç ”ç©¶ä¸­ï¼ŒDeepStack ä»¥ç»Ÿè®¡æ˜¾è‘—çš„æ–¹å¼å‡»è´¥äº†ä¸“ä¸šæ‰‘å…‹ç‰Œç©å®¶ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DeepStack çš„æˆåŠŸè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ å¯ä»¥ç”¨äºè§£å†³ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­çš„å¤æ‚é—®é¢˜ã€‚DeepStack çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ä¸å®Œç¾ä¿¡æ¯åœºæ™¯ï¼Œä¾‹å¦‚åŒ»ç–—è¯Šæ–­ã€æˆ˜ç•¥èµ„æºé˜²å¾¡ç­‰ã€‚

### ğŸŒŸ æ€»ç»“
DeepStack æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦çªç ´ï¼Œå®ƒå±•ç¤ºäº†åœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­å®ç°ä¸“å®¶çº§æ°´å¹³çš„å¯èƒ½æ€§ã€‚DeepStack çš„æ–¹æ³•ä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­çš„å¤æ‚é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚

## too-many-cooks--bayesian-inference-for-coordinating-multi-agent-collaboration
### Abstract
Collaboration requires agents to coordinate their behavior on the fly,
sometimes cooperating to solve a single task together and other times dividing
it up into sub-tasks to work on in parallel. Underlying the human ability to
collaborate is theory-of-mind, the ability to infer the hidden mental states
that drive others to act. Here, we develop Bayesian Delegation, a decentralized
multi-agent learning mechanism with these abilities. Bayesian Delegation
enables agents to rapidly infer the hidden intentions of others by inverse
planning. We test Bayesian Delegation in a suite of multi-agent Markov decision
processes inspired by cooking problems. On these tasks, agents with Bayesian
Delegation coordinate both their high-level plans (e.g. what sub-task they
should work on) and their low-level actions (e.g. avoiding getting in each
other's way). In a self-play evaluation, Bayesian Delegation outperforms
alternative algorithms. Bayesian Delegation is also a capable ad-hoc
collaborator and successfully coordinates with other agent types even in the
absence of prior experience. Finally, in a behavioral experiment, we show that
Bayesian Delegation makes inferences similar to human observers about the
intent of others. Together, these results demonstrate the power of Bayesian
Delegation for decentralized multi-agent collaboration.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ™ºèƒ½ä½“åä½œä¸­çš„è´å¶æ–¯æ¨ç†ï¼šè§£å†³â€œå¨å¸ˆå¤ªå¤šâ€çš„é—®é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åä½œæ˜¯æ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­å®ç°å…±åŒç›®æ ‡çš„å…³é”®ã€‚ç„¶è€Œï¼Œåä½œé¢ä¸´ç€åè°ƒè¡Œä¸ºçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰å…ˆéªŒç»éªŒã€ç¤¾ä¼šè§’è‰²å’Œè§„èŒƒçš„æƒ…å†µä¸‹ã€‚äººç±»é€šè¿‡â€œå¿ƒæ™ºç†è®ºâ€ï¼ˆTheory-of-Mindï¼ŒToMï¼‰æ¥ç†è§£ä»–äººçš„æ„å›¾ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„åä½œã€‚æœ¬æ–‡æ—¨åœ¨æ„å»ºå…·æœ‰å¿ƒæ™ºç†è®ºçš„æ™ºèƒ½ä½“ï¼Œå¹¶åˆ©ç”¨è¿™äº›èƒ½åŠ›æ¥åè°ƒåä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè´å¶æ–¯å§”æ‰˜ï¼ˆBayesian Delegationï¼‰
æœ¬æ–‡æå‡ºäº†è´å¶æ–¯å§”æ‰˜ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“å­¦ä¹ æœºåˆ¶ï¼Œå®ƒèƒ½å¤Ÿå¿«é€Ÿæ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„éšè—æ„å›¾ã€‚è´å¶æ–¯å§”æ‰˜é€šè¿‡é€†å‘è§„åˆ’æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ€§ä¸‹é¢„æµ‹å…¶ä»–æ™ºèƒ½ä½“çš„æ„å›¾ï¼Œå¹¶æœ‰æ•ˆåœ°å°†è‡ªèº«åŠªåŠ›å§”æ‰˜ç»™æœ€æœ‰ä»·å€¼çš„åä½œä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMMDPï¼‰ä¸å­ä»»åŠ¡
æœ¬æ–‡ç ”ç©¶äº†å…·æœ‰å­ä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå…¶ä¸­å­ä»»åŠ¡æ„æˆäº†æ™ºèƒ½ä½“ä¹‹é—´é«˜å±‚æ¬¡åè°ƒçš„ç›®æ ‡ã€‚å­ä»»åŠ¡å…è®¸æ™ºèƒ½ä½“åœ¨ä¸‰ä¸ªä¸åŒçš„æ–¹é¢è¿›è¡Œåè°ƒï¼šåˆ†å‰²å’Œå¾æœã€åˆä½œä»¥åŠæ—¶ç©ºç§»åŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸€ç³»åˆ—å—çƒ¹é¥ªé—®é¢˜å¯å‘çš„å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­æµ‹è¯•äº†è´å¶æ–¯å§”æ‰˜çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œè´å¶æ–¯å§”æ‰˜åœ¨å®Œæˆæ‰€æœ‰ç¯å¢ƒæ–¹é¢ä¼˜äºå…¶ä»–ç®—æ³•ï¼Œå¹¶ä¸”å³ä½¿åœ¨æ‰©å±•åˆ°æ›´å¤§çš„å›¢é˜Ÿæ—¶ä¹Ÿèƒ½ä¿æŒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè´å¶æ–¯å§”æ‰˜æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ä¸´æ—¶åä½œè€…ï¼Œå³ä½¿åœ¨æ²¡æœ‰å…ˆéªŒç»éªŒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¸å…¶ä»–ç±»å‹çš„æ™ºèƒ½ä½“æˆåŠŸåä½œã€‚æœ€åï¼Œåœ¨è¡Œä¸ºå®éªŒä¸­ï¼Œè´å¶æ–¯å§”æ‰˜çš„æ¨ç†ä¸äººç±»è§‚å¯Ÿè€…å…³äºä»–äººæ„å›¾çš„æ¨ç†ç›¸ä¼¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„è´å¶æ–¯å§”æ‰˜ä¸ºå»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“åä½œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å®ƒé€šè¿‡é€†å‘è§„åˆ’å’Œè´å¶æ–¯æ¨ç†æ¥æ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„æ„å›¾ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„åä½œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸å­ä»»åŠ¡çš„æ¦‚å¿µï¼Œä¸ºç ”ç©¶å¤šæ™ºèƒ½ä½“åä½œæä¾›äº†æ–°çš„è§†è§’ã€‚

## watch-and-help--a-challenge-for-social-perception-and-human-ai-collaboration
### Abstract
In this paper, we introduce Watch-And-Help (WAH), a challenge for testing
social intelligence in agents. In WAH, an AI agent needs to help a human-like
agent perform a complex household task efficiently. To succeed, the AI agent
needs to i) understand the underlying goal of the task by watching a single
demonstration of the human-like agent performing the same task (social
perception), and ii) coordinate with the human-like agent to solve the task in
an unseen environment as fast as possible (human-AI collaboration). For this
challenge, we build VirtualHome-Social, a multi-agent household environment,
and provide a benchmark including both planning and learning based baselines.
We evaluate the performance of AI agents with the human-like agent as well as
with real humans using objective metrics and subjective user ratings.
Experimental results demonstrate that the proposed challenge and virtual
environment enable a systematic evaluation on the important aspects of machine
social intelligence at scale.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Watch-And-Helpï¼šç¤¾ä¼šæ„ŸçŸ¥ä¸äººç±»-AIåä½œçš„æŒ‘æˆ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»åœ¨å¾ˆå°çš„å¹´çºªå°±å±•ç°å‡ºåˆ©ä»–è¡Œä¸ºï¼Œèƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿä»–äººçš„è¡Œä¸ºæ¥ç†è§£å…¶ç›®æ ‡ï¼Œå¹¶åˆ¶å®šè®¡åˆ’æ¥å¸®åŠ©ä»–ä»¬ï¼Œå³ä½¿åœ¨æ–°çš„åœºæ™¯ä¸­ä¹Ÿèƒ½åšåˆ°ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„AIç³»ç»Ÿä»ç„¶éš¾ä»¥æŒæ¡è¿™äº›åŸºæœ¬çš„ç¤¾ä¼šæŠ€èƒ½ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆå¸®åŠ©äººç±»æ‰€éœ€çš„ç¤¾ä¼šæ™ºèƒ½æ°´å¹³ï¼ŒAIä»£ç†éœ€è¦å…·å¤‡ä¸¤ä¸ªå…³é”®èƒ½åŠ›ï¼šç¤¾ä¼šæ„ŸçŸ¥ï¼ˆç†è§£äººç±»è¡Œä¸ºçš„èƒ½åŠ›ï¼‰å’Œåä½œè§„åˆ’ï¼ˆæ¨ç†ç‰©ç†ç¯å¢ƒå¹¶è§„åˆ’å…¶è¡ŒåŠ¨ä»¥ä¸äººç±»åè°ƒçš„èƒ½åŠ›ï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„AIæŒ‘æˆ˜ï¼šWatch-And-Help (WAH)ï¼Œé‡ç‚¹å…³æ³¨ç¤¾ä¼šæ„ŸçŸ¥å’Œäººç±»-AIåä½œã€‚åœ¨WAHæŒ‘æˆ˜ä¸­ï¼ŒAIä»£ç†éœ€è¦ä¸ä¸€ä¸ªç±»ä¼¼äººç±»çš„ä»£ç†åä½œï¼Œä»¥æ›´æœ‰æ•ˆåœ°å®Œæˆå¤æ‚çš„å®¶åº­ä»»åŠ¡ã€‚è¯¥æŒ‘æˆ˜åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
1. **è§‚å¯Ÿé˜¶æ®µ**ï¼šAIä»£ç†ï¼ˆBobï¼‰è§‚å¯Ÿç±»ä¼¼äººç±»çš„ä»£ç†ï¼ˆAliceï¼‰æ‰§è¡Œä»»åŠ¡ï¼Œå¹¶ä»å¥¹çš„è¡Œä¸ºä¸­æ¨æ–­å‡ºå¥¹çš„ç›®æ ‡ã€‚
2. **å¸®åŠ©é˜¶æ®µ**ï¼šBobä¸Aliceåä½œï¼Œåœ¨æ–°ç¯å¢ƒä¸­å°½å¯èƒ½å¿«åœ°å®Œæˆç›¸åŒçš„ä»»åŠ¡ã€‚

ä¸ºäº†å®ç°å¤šä»£ç†äº¤äº’ï¼Œæœ¬æ–‡æ‰©å±•äº†å¼€æºè™šæ‹Ÿå¹³å°VirtualHomeï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤šä»£ç†è™šæ‹Ÿç¯å¢ƒVirtualHome-Socialã€‚è¯¥ç¯å¢ƒæ¨¡æ‹Ÿäº†çœŸå®ä¸”ä¸°å¯Œçš„å®¶åº­ç¯å¢ƒï¼Œä»£ç†å¯ä»¥ä¸ä¸åŒçš„å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼Œæ‰“å¼€å®¹å™¨æˆ–æŠ“å–å¯¹è±¡ï¼‰å’Œå…¶ä»–ä»£ç†ï¼ˆä¾‹å¦‚ï¼Œè·Ÿéšã€å¸®åŠ©ã€é¿å…ç¢°æ’ï¼‰è¿›è¡Œäº¤äº’ï¼Œä»¥æ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ã€‚VirtualHome-Socialè¿˜æä¾›äº†å†…ç½®ä»£ç†ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œå…è®¸AIä»£ç†ä¸è™šæ‹Ÿäººç±»ä¸€èµ·è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå…è®¸ä½¿ç”¨çœŸå®äººç±»è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ”¶é›†/æ˜¾ç¤ºçœŸå®ç¯å¢ƒä¸­çš„äººç±»æ´»åŠ¨ï¼ˆè¿™æ˜¯ç°æœ‰å¤šä»£ç†å¹³å°ä¸æä¾›çš„åŠŸèƒ½ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ºäº†åœ¨WAHæŒ‘æˆ˜ä¸­å–å¾—æˆåŠŸï¼ŒAIä»£ç†å¿…é¡»å…·å¤‡å¼ºå¤§çš„ç¤¾ä¼šæ„ŸçŸ¥èƒ½åŠ›å’Œå¯æ¨å¹¿çš„å¸®åŠ©ç­–ç•¥ã€‚è¿™äº›æœºå™¨ç¤¾ä¼šæ™ºèƒ½çš„åŸºæœ¬æ–¹é¢å·²è¢«è¯æ˜æ˜¯å…ˆå‰å·¥ä½œä¸­äººç±»-AIåä½œçš„å…³é”®ã€‚æœ¬æ–‡æå‡ºçš„æŒ‘æˆ˜å’Œè™šæ‹Ÿç¯å¢ƒèƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°æœºå™¨ç¤¾ä¼šæ™ºèƒ½çš„é‡è¦æ–¹é¢ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **WAHæŒ‘æˆ˜**ï¼šä¸ºè¯„ä¼°AIä»£ç†çš„ç¤¾ä¼šæ„ŸçŸ¥èƒ½åŠ›å’Œåä½œèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ã€‚
2. **VirtualHome-Socialç¯å¢ƒ**ï¼šä¸ºAIä»£ç†æ‰§è¡Œå¤æ‚çš„å®¶åº­ä»»åŠ¡æä¾›äº†ä¸€ä¸ªå¤šä»£ç†å¹³å°ï¼Œå¹¶æ”¯æŒä¸å†…ç½®ä»£ç†æˆ–çœŸå®äººç±»è¿›è¡Œäº¤äº’ã€‚
3. **åŸºå‡†æµ‹è¯•**ï¼šåŒ…æ‹¬å¤šä¸ªåŸºäºè§„åˆ’å’Œå­¦ä¹ çš„åŸºçº¿æ–¹æ³•ï¼Œçªå‡ºäº†æœºå™¨ç¤¾ä¼šæ™ºèƒ½çš„é‡è¦æ–¹é¢ã€‚

### ğŸŒŸ æœªæ¥å±•æœ›
æœ¬æ–‡æå‡ºçš„æŒ‘æˆ˜å’Œè™šæ‹Ÿç¯å¢ƒä¸ºæ„å»ºæ›´å¤æ‚çš„æœºå™¨ç¤¾ä¼šæ™ºèƒ½å¼€è¾Ÿäº†ä»¤äººå…´å¥‹çš„æ–¹å‘ï¼Œä¾‹å¦‚åœ¨çº¿ç›®æ ‡æ¨ç†å’Œä»£ç†ä¹‹é—´çš„ç›´æ¥é€šä¿¡ã€‚å¸Œæœ›WAHæŒ‘æˆ˜å’ŒVirtualHome-Socialç¯å¢ƒèƒ½å¤Ÿä¿ƒè¿›æœªæ¥å¯¹æ„å»ºæ›´å¤æ‚æœºå™¨ç¤¾ä¼šæ™ºèƒ½çš„ç ”ç©¶ã€‚

## the-threedworld-transport-challenge--a-visually-guided-task-and-motion-planning-benchmark-for-physically-realistic-embodied-ai
### Abstract
We introduce a visually-guided and physics-driven task-and-motion planning
benchmark, which we call the ThreeDWorld Transport Challenge. In this
challenge, an embodied agent equipped with two 9-DOF articulated arms is
spawned randomly in a simulated physical home environment. The agent is
required to find a small set of objects scattered around the house, pick them
up, and transport them to a desired final location. We also position containers
around the house that can be used as tools to assist with transporting objects
efficiently. To complete the task, an embodied agent must plan a sequence of
actions to change the state of a large number of objects in the face of
realistic physical constraints. We build this benchmark challenge using the
ThreeDWorld simulation: a virtual 3D environment where all objects respond to
physics, and where can be controlled using fully physics-driven navigation and
interaction API. We evaluate several existing agents on this benchmark.
Experimental results suggest that: 1) a pure RL model struggles on this
challenge; 2) hierarchical planning-based agents can transport some objects but
still far from solving this task. We anticipate that this benchmark will
empower researchers to develop more intelligent physics-driven robots for the
physical world.
### ğŸŒŸ è®ºæ–‡è§£è¯» | 3DWorld Transport Challengeï¼šç‰©ç†ä¸–ç•Œä¸­çš„æ™ºèƒ½æœºå™¨äººæŒ‘æˆ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„æœºå™¨äººæˆä¸ºäº†è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç¤¾åŒºçš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨çœŸå®æœºå™¨äººè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°æˆæœ¬é«˜æ˜‚ä¸”å­˜åœ¨å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œè¿‘å¹´æ¥ï¼Œäººä»¬å¼€å§‹å°†æ¨¡æ‹Ÿå™¨çº³å…¥è®­ç»ƒå’Œè¯„ä¼°äººå·¥æ™ºèƒ½ç®—æ³•çš„è¿‡ç¨‹ä¸­ã€‚å°½ç®¡3Dè™šæ‹Ÿç¯å¢ƒåœ¨è§†è§‰å¯¼èˆªæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¤§å¤šå…³æ³¨è§†è§‰å¯¼èˆªï¼Œè€Œå¿½ç•¥äº†ç‰©ç†äº¤äº’ã€‚ç”±äºæœ€ç»ˆç›®æ ‡æ˜¯å¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ç¯å¢ƒä¸­æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„ç³»ç»Ÿï¼Œå› æ­¤ç‰©ç†äº¤äº’å·²æˆä¸ºå®¶åº­åŠ©ç†æœºå™¨äººè®­ç»ƒçš„å¿…è¦ç»„æˆéƒ¨åˆ†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å…·èº«AIæŒ‘æˆ˜ï¼šä¸€ä¸ªå…·æœ‰ä¸¤ä¸ª9è‡ªç”±åº¦å…³èŠ‚è‡‚çš„å…·èº«æ™ºèƒ½ä½“è¢«éšæœºæ”¾ç½®åœ¨ä¸€ä¸ªç‰©ç†çœŸå®çš„è™šæ‹Ÿå®¶åº­ç¯å¢ƒä¸­ã€‚æ™ºèƒ½ä½“éœ€è¦æ¢ç´¢æˆ¿å±‹ï¼Œå¯»æ‰¾æ•£è½åœ¨ä¸åŒæˆ¿é—´ä¸­çš„å°‘é‡ç‰©ä½“ï¼Œå¹¶å°†å®ƒä»¬è¿é€åˆ°ä¸€ä¸ªæœŸæœ›çš„æœ€ç»ˆä½ç½®ã€‚æ­¤å¤–ï¼Œæˆ¿å±‹å‘¨å›´è¿˜æ”¾ç½®äº†å„ç§å®¹å™¨ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ‰¾åˆ°è¿™äº›å®¹å™¨å¹¶å°†ç‰©ä½“æ”¾å…¥å…¶ä¸­ã€‚ä¸ä½¿ç”¨å®¹å™¨ä½œä¸ºå·¥å…·æ—¶ï¼Œæ™ºèƒ½ä½“åªèƒ½ä¸€æ¬¡è¿è¾“ä¸¤ä¸ªç‰©ä½“ã€‚ç„¶è€Œï¼Œä½¿ç”¨å®¹å™¨ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ”¶é›†å¤šä¸ªç‰©ä½“å¹¶ä¸€èµ·è¿è¾“ã€‚

ä¸ºäº†æ”¯æŒè¿™é¡¹æŒ‘æˆ˜ï¼Œæœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªåŸºäºTDWçš„æˆ¿å±‹æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å……æ»¡ç‰©ç†å“åº”ç‰©ä½“çš„å¤šæˆ¿é—´ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªå®Œå…¨åŸºäºç‰©ç†çš„é«˜çº§å¯¼èˆªå’Œäº¤äº’APIï¼Œå¯ä»¥ç”¨äºè®­ç»ƒAIæ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç‰©ç†ä¸–ç•Œä¸­ä¸è™šæ‹Ÿä¸–ç•Œè¿›è¡Œç‰©ç†äº¤äº’ã€‚ç”±äºæ¨¡æ‹ŸåŠ¨ä½œå’Œç¯å¢ƒå®Œå…¨åŸºäºç‰©ç†ï¼Œä¸ä¹‹å‰çš„éç‰©ç†æˆ–éƒ¨åˆ†ç‰©ç†è™šæ‹Ÿç¯å¢ƒç›¸æ¯”ï¼Œå®ƒä»¬æå‡ºäº†é¢å¤–çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œäº¤äº’åŠ¨ä½œåªæœ‰åœ¨ç›®æ ‡ç‰©ç†å¯è¾¾æ—¶ï¼ˆå³é è¿‘ä¸”æœªè¢«é˜»æŒ¡ï¼‰æ‰ä¼šæˆåŠŸã€‚å¦‚æœç›®æ ‡ä¸åœ¨æ™ºèƒ½ä½“çš„ä¸­å¿ƒè§†å›¾ä¸­ï¼Œæˆ–è€…ç›´æ¥è·¯å¾„è¢«é˜»æŒ¡ï¼ˆä¾‹å¦‚ï¼Œè¢«æ¡Œå­é˜»æŒ¡ï¼‰ï¼Œæ™ºèƒ½ä½“å°±æ— æ³•æˆåŠŸæŠ“å–ç‰©ä½“ã€‚æ­¤å¤–ï¼Œä¸æˆ¿å±‹ä¸­çš„ç‰©ä½“å‘ç”Ÿç‰©ç†ç¢°æ’ä¹Ÿå¯èƒ½æ˜¾è‘—é˜»ç¢è¿è¾“è¿›åº¦ã€‚å› æ­¤ï¼Œæ™ºèƒ½ä½“å¿…é¡»å­¦ä¹ åˆ©ç”¨è§†è§‰ä¿¡å·æ¥åŒæ­¥å¯¼èˆªå’Œæ“ä½œï¼Œä»¥åº”å¯¹è¿™äº›ç‰©ç†çº¦æŸã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†å‡ ä¸ªç°æœ‰çš„æ™ºèƒ½ä½“ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å…·èº«æ™ºèƒ½ä½“åœ¨å®Œæˆè¿™é¡¹ä»»åŠ¡æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡ç›¸ä¿¡ï¼Œåœ¨è¿è¾“æŒ‘æˆ˜ä¸­è¡¨ç°è‰¯å¥½çš„æ¨¡å‹å°†èƒ½å¤Ÿä½¿æœºå™¨äººæ›´åŠ æ™ºèƒ½ï¼Œèƒ½å¤Ÿåœ¨çœŸå®çš„ç‰©ç†ä¸–ç•Œä¸­å‘æŒ¥ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„3DWorld Transport Challengeä¸ºå…·èº«æ™ºèƒ½ä½“åœ¨ç‰©ç†çœŸå®ç¯å¢ƒä¸­çš„ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€å‘äº†ä¸€ä¸ªå®Œå…¨åŸºäºç‰©ç†çš„é«˜çº§å¯¼èˆªå’Œäº¤äº’APIï¼Œå¯ä»¥ç”¨äºè®­ç»ƒAIæ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç‰©ç†ä¸–ç•Œä¸­ä¸è™šæ‹Ÿä¸–ç•Œè¿›è¡Œç‰©ç†äº¤äº’ã€‚è¿™äº›æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„æ™ºèƒ½æœºå™¨äººæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## ai2-thor--an-interactive-3d-environment-for-visual-ai
### Abstract
We introduce The House Of inteRactions (THOR), a framework for visual AI
research, available at http://ai2thor.allenai.org. AI2-THOR consists of near
photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes
and interact with objects to perform tasks. AI2-THOR enables research in many
different domains including but not limited to deep reinforcement learning,
imitation learning, learning by interaction, planning, visual question
answering, unsupervised representation learning, object detection and
segmentation, and learning models of cognition. The goal of AI2-THOR is to
facilitate building visually intelligent models and push the research forward
in this domain.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AI2-THORï¼šæ¨åŠ¨è§†è§‰AIç ”ç©¶çš„æ–°ä¸€ä»£äº¤äº’å¼3Dç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»åœ¨è§†è§‰ç†è§£æ–¹é¢å±•ç°å‡ºè¶…è¶Šå½“å‰ä¸»æµè§†è§‰ä»»åŠ¡ï¼ˆå¦‚ç‰©ä½“æ£€æµ‹ã€åœºæ™¯è¯†åˆ«ã€å›¾åƒåˆ†å‰²ï¼‰çš„èƒ½åŠ›ã€‚è§†è§‰æ™ºèƒ½çš„å…³é”®åœ¨äºä¸ç¯å¢ƒäº¤äº’å¹¶ä»ä¸­å­¦ä¹ ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡ä½¿ç”¨é™æ€å›¾åƒæˆ–è§†é¢‘è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸äººç±»çš„å­¦ä¹ æ–¹å¼ä¸åŒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AI2-THORï¼Œä¸€ä¸ªåŸºäºè§†è§‰è¾“å…¥çš„ç±»ä¼¼äººç±»å­¦ä¹ çš„æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤äº’æ€§
AI2-THORæ”¯æŒå¤šç§ç±»å‹çš„äº¤äº’ï¼ŒåŒ…æ‹¬ç‰©ä½“çŠ¶æ€å˜åŒ–ã€åŸºäºè‡‚éƒ¨çš„æ“ä½œå’Œå› æœäº¤äº’ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ‰“å¼€æˆ–å…³é—­å¾®æ³¢ç‚‰ï¼Œå°†é¢åŒ…åˆ‡ç‰‡å¹¶åœ¨çƒ¤é¢åŒ…æœºä¸­çƒ¤åˆ¶ï¼Œä»¥åŠæ‰“å¼€æ°´é¾™å¤´å°†æ°´å€’å…¥æ¯å­ä¸­ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœºæ™¯å¤šæ ·æ€§
AI2-THORæä¾›äº†æ¯”å…¶ä»–å¹³å°æ›´å¤šçš„äº¤äº’å¼ç‰©ä½“å’Œåœºæ™¯ï¼Œé€šè¿‡ä½¿ç”¨ç¨‹åºç”ŸæˆæŠ€æœ¯ï¼Œæä¾›äº†120ä¸ªç‹¬ç«‹çš„æˆ¿é—´ã€89ä¸ªè¿·å®«é£æ ¼çš„å…¬å¯“å’Œ10ä¸ªè¯„ä¼°æˆ¿å±‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé«˜è´¨é‡
AI2-THORä¸­çš„ç‰©ä½“å’Œåœºæ™¯æ¥è¿‘ç…§ç‰‡çœŸå®æ„Ÿï¼Œè¿™æœ‰åŠ©äºå°†å­¦ä¹ æ¨¡å‹æ›´å¥½åœ°è½¬ç§»åˆ°ç°å®ä¸–ç•Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šAPI
AI2-THORæä¾›äº†ä¸€ä¸ªPython APIï¼Œç”¨äºä¸Unity 3Dæ¸¸æˆå¼•æ“äº¤äº’ï¼Œè¯¥å¼•æ“æä¾›äº†è®¸å¤šä¸åŒçš„åŠŸèƒ½ï¼Œå¦‚å¯¼èˆªã€æ–½åŠ åŠ›ã€ç‰©ä½“äº¤äº’å’Œç‰©ç†å»ºæ¨¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è‡ª2017å¹´é¦–æ¬¡å‘å¸ƒä»¥æ¥ï¼ŒAI2-THORå·²è¢«ç”¨äºè¶…è¿‡150ç¯‡è®ºæ–‡çš„å®éªŒï¼Œå¹¶è¢«ä¸‹è½½è¶…è¿‡50ä¸‡æ¬¡ã€‚å®ƒåœ¨è§†è§‰å¯¼èˆªã€è§†å¬å¯¼èˆªã€è§†è§‰å’Œè¯­è¨€ã€äººæœºäº¤äº’ã€æ¨¡æ‹Ÿåˆ°ç°å®è½¬ç§»ã€å¤šæ™ºèƒ½ä½“äº¤äº’ã€å­¦ä¹ å¯¹è±¡å…³ç³»ã€å­¦ä¹ å¯ä¾›æ€§ã€åœºæ™¯åˆæˆã€é€šè¿‡äº¤äº’å­¦ä¹ ä»¥åŠè®¡ç®—æœºè§†è§‰å’Œå¯è§£é‡Šæ€§ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AI2-THORæ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„äº¤äº’å¼3Dç¯å¢ƒï¼Œå¯ä»¥ç”¨äºå„ç§è§†è§‰AIç ”ç©¶ã€‚å®ƒå…·æœ‰é«˜åº¦çš„å¯å®šåˆ¶æ€§ï¼Œå¹¶æä¾›äº†å¯¹å¤šç§åœºæ™¯ã€æ™ºèƒ½ä½“ä½“ç°ã€åŠ¨ä½œå’Œå…ƒæ•°æ®çš„æ”¯æŒã€‚éšç€å…¶åŠŸèƒ½çš„ä¸æ–­æ›´æ–°å’Œå‘å±•ï¼ŒAI2-THORæœ‰æœ›åœ¨è§†è§‰AIé¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

## interactive-gibson-benchmark-(igibson-0-5)--a-benchmark-for-interactive-navigation-in-cluttered-environments
### Abstract
We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(https://sites.google.com/view/interactivegibsonenv) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.
### ğŸŒŸ è®ºæ–‡è§£è¯» | äº¤äº’å¼GibsonåŸºå‡†ï¼ˆiGibson 0.5ï¼‰ï¼šç”¨äºæ‚ä¹±ç¯å¢ƒä¸­çš„äº¤äº’å¼å¯¼èˆªçš„åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„æœºå™¨äººå¯¼èˆªä¸»è¦å…³æ³¨åœ¨é¿å…ç¢°æ’çš„æƒ…å†µä¸‹åˆ°è¾¾ç›®æ ‡ã€‚ç„¶è€Œï¼Œéšç€æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨æ‚ä¹±æ— ç« çš„ç¯å¢ƒä¸­ï¼Œå¦‚å®¶åº­å’ŒåŠå…¬å®¤ï¼Œè€ƒè™‘ç‰©ç†äº¤äº’ä½œä¸ºå¯¼èˆªç­–ç•¥çš„ä¸€éƒ¨åˆ†å˜å¾—ä¸å¯é¿å…ï¼Œç”šè‡³å¿…è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‚ä¹±çš„å®¶åº­ä¸­å¯¼èˆªæ—¶ï¼Œæœºå™¨äººå¯èƒ½éœ€è¦æ¨å¼€ç‰©ä½“æˆ–æ‰“å¼€é—¨æ‰èƒ½åˆ°è¾¾ç›®çš„åœ°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„æ–¹æ³•æ¥ç ”ç©¶è¿™ç§äº¤äº’å¼å¯¼èˆªé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤äº’å¼Gibsonç¯å¢ƒï¼ˆiGibson 0.5ï¼‰
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å®éªŒè®¾ç½®ï¼Œå³äº¤äº’å¼Gibsonç¯å¢ƒï¼ˆiGibson 0.5ï¼‰ï¼Œå®ƒæ¨¡æ‹Ÿäº†å®¤å†…åœºæ™¯çš„é«˜ä¿çœŸè§†è§‰ï¼Œä»¥åŠæœºå™¨äººå’Œè¿™äº›åœºæ™¯ä¸­å¸¸è§ç‰©ä½“çš„é«˜ä¿çœŸç‰©ç†åŠ¨åŠ›å­¦ã€‚è¿™ä½¿å¾—æœºå™¨äººå¯ä»¥ä¸åœºæ™¯ä¸­çš„ç‰©ä½“è¿›è¡Œäº¤äº’ï¼Œä¾‹å¦‚æ¨å¼€ç‰©ä½“ä»¥æ¸…é™¤é€šå¾€ç›®æ ‡ä½ç½®çš„è·¯å¾„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº¤äº’å¼å¯¼èˆªæŒ‡æ ‡
æœ¬æ–‡æå‡ºäº†ä¸€å¥—äº¤äº’å¼å¯¼èˆªæŒ‡æ ‡ï¼Œç”¨äºç ”ç©¶å¯¼èˆªå’Œç‰©ç†äº¤äº’ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬è·¯å¾„æ•ˆç‡ã€äº¤äº’åŠªåŠ›å’Œäº¤äº’å¼å¯¼èˆªåˆ†æ•°ï¼ˆINSï¼‰ï¼Œå®ƒä»¬å¯ä»¥è¡¡é‡æœºå™¨äººå¯¼èˆªç­–ç•¥çš„æ•ˆç‡å’Œæ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨äº¤äº’å¼GibsonåŸºå‡†ä¸Šè¯„ä¼°äº†å¤šä¸ªåŸºäºå­¦ä¹ çš„åŸºçº¿ï¼Œå¹¶æä¾›äº†å…³äºå¯¼èˆªè¡Œä¸ºåœ¨ä¸åŒè·¯å¾„æ•ˆç‡å’Œå‘¨å›´ç‰©ä½“å¹²æ‰°ä¹‹é—´çš„æƒè¡¡çš„è§è§£ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç§¯æè°ƒæ•´å¥–åŠ±å‡½æ•°ä¸­çš„äº¤äº’æƒ©ç½šï¼Œå¯ä»¥æ§åˆ¶äº¤äº’å¼å¯¼èˆªåˆ†æ•°ï¼Œå¹¶ä¸”ä¸åŒçš„INSå€¼å¯¹åº”äºä¸åŒçš„å¯¼èˆªè¡Œä¸ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„äº¤äº’å¼GibsonåŸºå‡†å’Œäº¤äº’å¼å¯¼èˆªæŒ‡æ ‡ä¸ºç ”ç©¶äº¤äº’å¼å¯¼èˆªé—®é¢˜æä¾›äº†ä¸€ä¸ªæ–°çš„å¹³å°å’Œå·¥å…·ã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¿™ä¸ªåŸºå‡†æ¥è¯„ä¼°å’Œæ¯”è¾ƒä»–ä»¬çš„äº¤äº’å¼å¯¼èˆªè§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¢ç´¢ä¸åŒå¯¼èˆªç­–ç•¥çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„äº¤äº’å¼Gibsonç¯å¢ƒä¹Ÿå¯ä»¥ç”¨äºç ”ç©¶å’Œè®­ç»ƒå…¶ä»–è§†è§‰å¯¼èˆªã€æ“ä½œå’Œç§»åŠ¨æ“ä½œè§£å†³æ–¹æ¡ˆã€‚

## habitat--a-platform-for-embodied-ai-research
### Abstract
We present Habitat, a platform for research in embodied artificial
intelligence (AI). Habitat enables training embodied agents (virtual robots) in
highly efficient photorealistic 3D simulation. Specifically, Habitat consists
of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with
configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is
fast -- when rendering a scene from Matterport3D, it achieves several thousand
frames per second (fps) running single-threaded, and can reach over 10,000 fps
multi-process on a single GPU. (ii) Habitat-API: a modular high-level library
for end-to-end development of embodied AI algorithms -- defining tasks (e.g.,
navigation, instruction following, question answering), configuring, training,
and benchmarking embodied agents.
  These large-scale engineering contributions enable us to answer scientific
questions requiring experiments that were till now impracticable or 'merely'
impractical. Specifically, in the context of point-goal navigation: (1) we
revisit the comparison between learning and SLAM approaches from two recent
works and find evidence for the opposite conclusion -- that learning
outperforms SLAM if scaled to an order of magnitude more experience than
previous investigations, and (2) we conduct the first cross-dataset
generalization experiments {train, test} x {Matterport3D, Gibson} for multiple
sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors
generalize across datasets. We hope that our open-source platform and these
findings will advance research in embodied AI.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Habitatï¼šèµ‹èƒ½å…·èº«AIç ”ç©¶çš„å¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå…·èº«AIï¼ˆEmbodied AIï¼‰ä¹Ÿé€æ¸æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚å…·èº«AIæ—¨åœ¨è®©æ™ºèƒ½ä½“ï¼ˆå¦‚æœºå™¨äººï¼‰åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œæ„ŸçŸ¥ã€å†³ç­–å’Œè¡ŒåŠ¨ï¼Œä»è€Œå®ç°æ›´é«˜çº§çš„æ™ºèƒ½ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­è®­ç»ƒå…·èº«AIé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼Œä¾‹å¦‚è®­ç»ƒé€Ÿåº¦æ…¢ã€æˆæœ¬é«˜ã€å®‰å…¨æ€§ä½ç­‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Habitatå¹³å°ï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆçš„3Dæ¨¡æ‹Ÿå™¨ï¼Œä¸ºå…·èº«AIç ”ç©¶æä¾›æ›´ä¾¿æ·ã€æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
Habitatå¹³å°ä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š

1. **Habitat-Sim**ï¼šä¸€ä¸ªçµæ´»ã€é«˜æ€§èƒ½çš„3Dæ¨¡æ‹Ÿå™¨ï¼Œæ”¯æŒå¯é…ç½®çš„æ™ºèƒ½ä½“ã€ä¼ æ„Ÿå™¨å’Œé€šç”¨3Dæ•°æ®é›†å¤„ç†ã€‚Habitat-Simå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
    * **é«˜æ€§èƒ½æ¸²æŸ“**ï¼šå•çº¿ç¨‹è¿è¡Œæ—¶ï¼Œæ¸²æŸ“Matterport3Dåœºæ™¯å¯è¾¾æ•°åƒå¸§æ¯ç§’ï¼ˆfpsï¼‰ï¼Œå¤šè¿›ç¨‹è¿è¡Œæ—¶ï¼Œå•GPUå¯è¾¾è¶…è¿‡10,000 fpsã€‚
    * **çµæ´»é…ç½®**ï¼šæ”¯æŒé…ç½®ä¸åŒç±»å‹çš„æ™ºèƒ½ä½“ã€ä¼ æ„Ÿå™¨å’Œ3Dæ•°æ®é›†ï¼Œä¾‹å¦‚Matterport3Dã€Gibsonå’ŒReplicaæ•°æ®é›†ã€‚
    * **é«˜æ•ˆGPUååé‡**ï¼šé€šè¿‡å…±äº«å†…å­˜å’ŒCUDA-GLäº’æ“ä½œï¼Œå®ç°é«˜æ•ˆçš„GPUæ¸²æŸ“å’ŒCPUå†…å­˜è®¿é—®ã€‚

2. **Habitat-API**ï¼šä¸€ä¸ªæ¨¡å—åŒ–ã€é«˜çº§åˆ«çš„åº“ï¼Œç”¨äºç«¯åˆ°ç«¯å¼€å‘å…·èº«AIç®—æ³•ã€‚Habitat-APIæ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
    * **ä»»åŠ¡å®šä¹‰**ï¼šå®šä¹‰å„ç§å…·èº«AIä»»åŠ¡ï¼Œä¾‹å¦‚å¯¼èˆªã€æŒ‡ä»¤è·Ÿéšã€é—®ç­”ç­‰ã€‚
    * **æ™ºèƒ½ä½“é…ç½®**ï¼šé…ç½®å’Œè®­ç»ƒä¸åŒç±»å‹çš„æ™ºèƒ½ä½“ï¼Œä¾‹å¦‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ æˆ–ç»å…¸SLAMæ–¹æ³•ã€‚
    * **åŸºå‡†æµ‹è¯•**ï¼šä½¿ç”¨æ ‡å‡†æŒ‡æ ‡å¯¹æ™ºèƒ½ä½“è¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ç‚¹ç›®æ ‡å¯¼èˆªä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š

1. **å­¦ä¹ ä¼˜äºSLAM**ï¼šå½“è®­ç»ƒç»éªŒè¶³å¤Ÿå¤šæ—¶ï¼ŒåŸºäºå­¦ä¹ çš„æ™ºèƒ½ä½“å¯ä»¥è¶…è¶Šç»å…¸SLAMæ–¹æ³•ã€‚
2. **æ·±åº¦ä¼ æ„Ÿå™¨æ›´å…·æ³›åŒ–èƒ½åŠ›**ï¼šé…å¤‡æ·±åº¦ä¼ æ„Ÿå™¨çš„æ™ºèƒ½ä½“åœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Habitatå¹³å°ä¸ºå…·èº«AIç ”ç©¶æä¾›äº†å¼ºå¤§çš„å·¥å…·å’Œå¹³å°ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **é«˜æ•ˆæ¨¡æ‹Ÿå™¨**ï¼šHabitat-Simçš„é«˜æ€§èƒ½æ¸²æŸ“èƒ½åŠ›å¯ä»¥æ˜¾è‘—æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚
* **çµæ´»é…ç½®**ï¼šHabitatå¹³å°æ”¯æŒçµæ´»é…ç½®æ™ºèƒ½ä½“ã€ä¼ æ„Ÿå™¨å’Œæ•°æ®é›†ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œå„ç§å®éªŒã€‚
* **æ¨¡å—åŒ–è®¾è®¡**ï¼šHabitat-APIçš„æ¨¡å—åŒ–è®¾è®¡æ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œä»£ç å¤ç”¨å’Œæ‰©å±•ã€‚
* **åŸºå‡†æµ‹è¯•**ï¼šHabitatå¹³å°æä¾›äº†æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œç»“æœæ¯”è¾ƒå’Œè¯„ä¼°ã€‚

æ€»è€Œè¨€ä¹‹ï¼ŒHabitatå¹³å°ä¸ºå…·èº«AIç ”ç©¶æä¾›äº†å¼ºå¤§çš„å·¥å…·å’Œå¹³å°ï¼Œæœ‰æœ›æ¨åŠ¨å…·èº«AIé¢†åŸŸçš„å¿«é€Ÿå‘å±•ã€‚

## behavior--benchmark-for-everyday-household-activities-in-virtual--interactive--and-ecological-environments
### Abstract
We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
behavior.stanford.edu to facilitate and calibrate the development of new
embodied AI solutions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BEHAVIORï¼šè™šæ‹Ÿäº¤äº’ç”Ÿæ€ç¯å¢ƒä¸­æ—¥å¸¸å®¶åº­æ´»åŠ¨çš„åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•å¯¹äºè¯„ä¼°å’Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ç¼ºä¹ç°å®æ€§ã€å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BEHAVIORï¼Œä¸€ä¸ªåŒ…å«100ä¸ªæ—¥å¸¸å®¶åº­æ´»åŠ¨çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æ¨åŠ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºè°“è¯é€»è¾‘çš„æè¿°è¯­è¨€
BEHAVIORå¼•å…¥äº†ä¸€ç§åŸºäºè°“è¯é€»è¾‘çš„æè¿°è¯­è¨€ï¼Œç”¨äºè¡¨è¾¾æ´»åŠ¨çš„åˆå§‹å’Œç›®æ ‡æ¡ä»¶ã€‚è¿™ç§è¯­è¨€å…è®¸ç”Ÿæˆå¤šæ ·åŒ–çš„æ´»åŠ¨å®ä¾‹ï¼Œå¹¶èƒ½å¤Ÿæ¥å—ä»»ä½•æœ‰æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡æ‹Ÿå™¨æ— å…³çš„ç¯å¢ƒç‰¹å¾
BEHAVIORç¡®å®šäº†æ”¯æŒå…¶æ´»åŠ¨çš„æ¨¡æ‹Ÿå™¨æ— å…³ç‰¹å¾ï¼Œå¹¶åœ¨iGibson 2.0ä¸­å®ç°äº†è¿™äº›ç‰¹å¾ã€‚è¿™ä½¿å¾—BEHAVIORå¯ä»¥åœ¨å¤šç§ç¯å¢ƒä¸­å®ç°ï¼Œå¹¶æä¾›äº†æ— é™å¤šæ ·åŒ–çš„æ´»åŠ¨å®ä¾‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºäººç±»è¡¨ç°çš„è¯„ä¼°æŒ‡æ ‡
BEHAVIORå¼•å…¥äº†ä¸€ç³»åˆ—è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ™ºèƒ½ä½“åœ¨ä»»åŠ¡è¿›åº¦å’Œæ•ˆç‡æ–¹é¢çš„è¡¨ç°ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬æˆåŠŸåˆ†æ•°ã€æ•ˆç‡æŒ‡æ ‡å’ŒåŸºäºäººç±»è¡¨ç°çš„æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å…¬å¹³æ€§å’Œå¯æ¯”æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“ï¼Œåœ¨é¢å¯¹BEHAVIORçš„æŒ‘æˆ˜æ—¶ä¹Ÿéš¾ä»¥å–å¾—è‰¯å¥½çš„è¡¨ç°ã€‚è¿™è¡¨æ˜BEHAVIORçš„åŸºå‡†æµ‹è¯•å…·æœ‰å¾ˆé«˜çš„éš¾åº¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
BEHAVIORçš„åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„å‘å±•æä¾›äº†é‡è¦çš„å·¥å…·ã€‚å…¶åŸºäºè°“è¯é€»è¾‘çš„æè¿°è¯­è¨€ã€æ¨¡æ‹Ÿå™¨æ— å…³çš„ç¯å¢ƒç‰¹å¾å’ŒåŸºäºäººç±»è¡¨ç°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºå…¶ä»–åŸºå‡†æµ‹è¯•æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒã€‚æ­¤å¤–ï¼ŒBEHAVIORçš„åŸºå‡†æµ‹è¯•è¿˜å¯ä»¥ç”¨äºå¼€å‘æ–°çš„æ™ºèƒ½ä½“è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

## behavior-1k--a-human-centered--embodied-ai-benchmark-with-1-000-everyday-activities-and-realistic-simulation
### Abstract
We present BEHAVIOR-1K, a comprehensive simulation benchmark for
human-centered robotics. BEHAVIOR-1K includes two components, guided and
motivated by the results of an extensive survey on "what do you want robots to
do for you?". The first is the definition of 1,000 everyday activities,
grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more
than 9,000 objects annotated with rich physical and semantic properties. The
second is OMNIGIBSON, a novel simulation environment that supports these
activities via realistic physics simulation and rendering of rigid bodies,
deformable bodies, and liquids. Our experiments indicate that the activities in
BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both
of which remain a challenge for even state-of-the-art robot learning solutions.
To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an
initial study on transferring solutions learned with a mobile manipulator in a
simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's
human-grounded nature, diversity, and realism make it valuable for embodied AI
and robot learning research. Project website: https://behavior.stanford.edu.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BEHAVIOR-1Kï¼šåŸºäºäººç±»æ—¥å¸¸æ´»åŠ¨ä¸çœŸå®æ¨¡æ‹Ÿçš„å…·èº«AIåŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å¯¹äºæœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œæ›´å¤šæ—¥å¸¸ä»»åŠ¡çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœºå™¨äººåŸºå‡†æµ‹è¯•å¾€å¾€ç”±ç ”ç©¶äººå‘˜è®¾è®¡ï¼Œç¼ºä¹å¯¹äººç±»å®é™…éœ€æ±‚çš„è€ƒè™‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BEHAVIOR-1Kï¼Œä¸€ä¸ªåŸºäºäººç±»æ—¥å¸¸æ´»åŠ¨ä¸çœŸå®æ¨¡æ‹Ÿçš„å…·èº«AIåŸºå‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºäººç±»éœ€æ±‚çš„åŸºå‡†æµ‹è¯•
BEHAVIOR-1Ké€šè¿‡ä¸€é¡¹å¹¿æ³›çš„è°ƒæŸ¥ï¼Œæ”¶é›†äº†1461åå‚ä¸è€…çš„æ„è§ï¼Œä»¥ç¡®å®šäººä»¬å¸Œæœ›æœºå™¨äººæ‰§è¡Œçš„æœ€å¸¸è§å’Œæœ€éœ€è¦çš„1000ä¸ªæ—¥å¸¸æ´»åŠ¨ã€‚è¿™äº›æ´»åŠ¨æ¶µç›–äº†ä»æ¸…æ´ã€çƒ¹é¥ªåˆ°å¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸï¼Œç¡®ä¿äº†åŸºå‡†æµ‹è¯•çš„å¤šæ ·æ€§å’Œå®ç”¨æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçœŸå®æ¨¡æ‹Ÿç¯å¢ƒOMNIGIBSON
ä¸ºäº†æ”¯æŒè¿™äº›æ—¥å¸¸æ´»åŠ¨ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªåä¸ºOMNIGIBSONçš„æ¨¡æ‹Ÿç¯å¢ƒã€‚OMNIGIBSONåŸºäºNvidiaçš„Omniverseå’ŒPhysX 5ï¼Œèƒ½å¤Ÿæä¾›é€¼çœŸçš„ç‰©ç†æ¨¡æ‹Ÿå’Œæ¸²æŸ“ï¼ŒåŒ…æ‹¬åˆšæ€§ä½“ã€å¯å˜å½¢ä½“å’Œæ¶²ä½“çš„æ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼ŒOMNIGIBSONè¿˜æ”¯æŒæ‰©å±•çš„å¯¹è±¡çŠ¶æ€ï¼Œå¦‚æ¸©åº¦ã€æ¹¿åº¦ç­‰ï¼Œä»¥åŠç”Ÿæˆæœ‰æ•ˆçš„åˆå§‹æ´»åŠ¨é…ç½®å’ŒåŒºåˆ†æœ‰æ•ˆç›®æ ‡è§£å†³æ–¹æ¡ˆçš„åŠŸèƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†å½“å‰æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨BEHAVIOR-1Kä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å•ä¸ªæ´»åŠ¨ä¹Ÿå¯¹å½“å‰AIç®—æ³•æ„æˆäº†æå¤§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†æ‰èƒ½è§£å†³ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†ä¸€é¡¹åˆæ­¥ç ”ç©¶ï¼Œå°†æ¨¡æ‹Ÿç¯å¢ƒä¸­å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆè½¬ç§»åˆ°ç°å®ä¸–ç•Œçš„æœºå™¨äººä¸Šï¼Œä»¥è¯„ä¼°æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
BEHAVIOR-1Kä¸ºå…·èº«AIå’Œæœºå™¨äººå­¦ä¹ ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åŸºå‡†æµ‹è¯•ã€‚å…¶åŸºäºäººç±»éœ€æ±‚çš„å¤šæ ·æ€§å’ŒçœŸå®æ¨¡æ‹Ÿç¯å¢ƒï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¹³å°ï¼Œä»¥å¼€å‘èƒ½å¤Ÿæ‰§è¡Œæ›´å¤šæ—¥å¸¸ä»»åŠ¡çš„æœºå™¨äººã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºè§£å†³æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## automatic-goal-generation-for-reinforcement-learning-agents
### Abstract
Reinforcement learning is a powerful technique to train an agent to perform a
task. However, an agent that is trained using reinforcement learning is only
capable of achieving the single task that is specified via its reward function.
Such an approach does not scale well to settings in which an agent needs to
perform a diverse set of tasks, such as navigating to varying positions in a
room or moving objects to varying locations. Instead, we propose a method that
allows an agent to automatically discover the range of tasks that it is capable
of performing. We use a generator network to propose tasks for the agent to try
to achieve, specified as goal states. The generator network is optimized using
adversarial training to produce tasks that are always at the appropriate level
of difficulty for the agent. Our method thus automatically produces a
curriculum of tasks for the agent to learn. We show that, by using this
framework, an agent can efficiently and automatically learn to perform a wide
set of tasks without requiring any prior knowledge of its environment. Our
method can also learn to achieve tasks with sparse rewards, which traditionally
pose significant challenges.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªåŠ¨ç›®æ ‡ç”Ÿæˆï¼šè®©å¼ºåŒ–å­¦ä¹ æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è®­ç»ƒæ™ºèƒ½ä½“æ‰§è¡Œç‰¹å®šä»»åŠ¡çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸åªèƒ½è®©æ™ºèƒ½ä½“å­¦ä¼šæ‰§è¡Œå•ä¸€ä»»åŠ¡ï¼Œè¿™åœ¨éœ€è¦æ™ºèƒ½ä½“æ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡çš„åœºæ™¯ä¸­æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººå¯¼èˆªæˆ–ç‰©ä½“æ¬è¿ç­‰ä»»åŠ¡ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦èƒ½å¤Ÿåˆ°è¾¾ä¸åŒçš„ä½ç½®æˆ–ç§»åŠ¨ç‰©ä½“åˆ°ä¸åŒçš„ä½ç½®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç›®æ ‡ç”Ÿæˆæ–¹æ³•ï¼Œè®©æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å¹¶å­¦ä¹ æ‰§è¡Œå…¶ç¯å¢ƒä¸­çš„å„ç§ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç›®æ ‡ç”Ÿæˆç½‘ç»œ
æœ¬æ–‡ä½¿ç”¨ä¸€ä¸ªç”Ÿæˆå™¨ç½‘ç»œæ¥ä¸ºæ™ºèƒ½ä½“æå‡ºä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¢«æŒ‡å®šä¸ºç›®æ ‡çŠ¶æ€ã€‚ç”Ÿæˆå™¨ç½‘ç»œé€šè¿‡å¯¹æŠ—è®­ç»ƒè¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„ä»»åŠ¡å§‹ç»ˆå¤„äºæ™ºèƒ½ä½“èƒ½å¤Ÿå¤„ç†çš„éš¾åº¦æ°´å¹³ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªåŠ¨è¯¾ç¨‹ç”Ÿæˆ
æœ¬æ–‡çš„æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªè¯¾ç¨‹ï¼Œå…¶ä¸­åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œç”Ÿæˆå™¨éƒ½ä¼šç”Ÿæˆæ¯”æ™ºèƒ½ä½“å·²ç»èƒ½å¤Ÿå®ç°çš„ä»»åŠ¡ç¨å¾®æ›´éš¾çš„ä»»åŠ¡ã€‚è¿™æ ·ï¼Œæ™ºèƒ½ä½“å¯ä»¥é«˜æ•ˆåœ°å­¦ä¹ æ‰§è¡Œä¸€ç³»åˆ—ä»»åŠ¡ï¼Œè€Œæ— éœ€å¯¹å…¶ç¯å¢ƒæˆ–æ‰§è¡Œçš„ä»»åŠ¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªæœºå™¨äººè¿åŠ¨ä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ æ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ç¨€ç–å¥–åŠ±å‡½æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„è‡ªåŠ¨ç›®æ ‡ç”Ÿæˆæ–¹æ³•ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“æ›´é«˜æ•ˆåœ°å­¦ä¹ æ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººå¯¼èˆªã€ç‰©ä½“æ¬è¿ã€æ¸¸æˆç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚

## toolkengpt--augmenting-frozen-language-models-with-massive-tools-via-tool-embeddings
### Abstract
Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ToolkenGPTï¼šé€šè¿‡å·¥å…·åµŒå…¥å¢å¼ºå†»ç»“è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¦‚ä½•è®©è¿™äº›æ¨¡å‹æ›´å¥½åœ°ä¸å¤–éƒ¨å·¥å…·äº¤äº’ï¼Œä»¥è§£å†³æ›´å¤æ‚çš„é—®é¢˜ï¼Œæˆä¸ºäº†ç ”ç©¶çš„çƒ­ç‚¹ã€‚ä¼ ç»Ÿçš„é€šè¿‡å¾®è°ƒLLMsæ¥å­¦ä¹ ä½¿ç”¨å·¥å…·çš„æ–¹æ³•ï¼Œè™½ç„¶æœ‰æ•ˆï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”çµæ´»æ€§å·®ï¼Œåªèƒ½é’ˆå¯¹é¢„å®šä¹‰çš„å·¥å…·é›†è¿›è¡Œå­¦ä¹ ã€‚è€ŒåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•è™½ç„¶å¯ä»¥çµæ´»åœ°å¤„ç†æ–°å·¥å…·ï¼Œä½†å—é™äºä¸Šä¸‹æ–‡é•¿åº¦ï¼Œéš¾ä»¥å±•ç¤ºå¤§é‡å·¥å…·çš„æ¼”ç¤ºï¼Œå¯¼è‡´å¯¹å·¥å…·çš„ç†è§£ä¸å¤Ÿæ·±å…¥ã€‚æ­¤å¤–ï¼Œå½“å¯é€‰æ‹©çš„å·¥å…·æ•°é‡ä¼—å¤šæ—¶ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ å¯èƒ½å®Œå…¨å¤±æ•ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ToolkenGPTï¼Œä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†å¾®è°ƒå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶é¿å…äº†å®ƒä»¬çš„å±€é™æ€§ã€‚ToolkenGPTçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ¯ä¸ªå·¥å…·è¡¨ç¤ºä¸ºä¸€ä¸ªç‰¹æ®Šçš„tokenï¼ˆç§°ä¸ºâ€œtoolkenâ€ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªtoolkenå­¦ä¹ ä¸€ä¸ªåµŒå…¥å‘é‡ã€‚è¿™æ ·ï¼ŒLLMså°±å¯ä»¥åƒç”Ÿæˆæ™®é€šå•è¯tokenä¸€æ ·è°ƒç”¨å·¥å…·ã€‚ä¸€æ—¦é¢„æµ‹åˆ°toolkenï¼ŒLLMå°±ä¼šåˆ‡æ¢åˆ°â€œå·¥å…·æ¨¡å¼â€ï¼Œç”Ÿæˆå·¥å…·çš„å‚æ•°ï¼Œç„¶åæ‰§è¡Œå·¥å…·å¹¶å°†ç»“æœè¿”å›åˆ°æ–‡æœ¬ä¸­ç»§ç»­æ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ•°å€¼æ¨ç†ã€åŸºäºçŸ¥è¯†çš„é—®ç­”å’Œå…·èº«è®¡åˆ’ç”Ÿæˆç­‰ä¸åŒé¢†åŸŸï¼ŒToolkenGPTæœ‰æ•ˆåœ°å¢å¼ºäº†LLMsçš„èƒ½åŠ›ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå„ç§æœ€æ–°çš„åŸºçº¿æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToolkenGPTèƒ½å¤Ÿä»å¤§é‡çš„æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ toolkenåµŒå…¥ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨å·¥å…·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ToolkenGPTæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”çµæ´»çš„æ–¹æ³•ï¼Œè®©LLMsèƒ½å¤Ÿå­¦ä¹ å’Œä½¿ç”¨å¤§é‡çš„å¤–éƒ¨å·¥å…·ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦å·¥å…·è¾…åŠ©çš„å¤æ‚é—®é¢˜ï¼Œä¾‹å¦‚æ•°å­¦è®¡ç®—ã€çŸ¥è¯†é—®ç­”ã€æœºå™¨äººæ§åˆ¶ç­‰ã€‚æ­¤å¤–ï¼ŒToolkenGPTçš„è®¾è®¡ä¹Ÿå¯å‘äº†æˆ‘ä»¬å¦‚ä½•å°†LLMsä¸å…¶ä»–ç±»å‹çš„æ¨¡å‹å’Œå·¥å…·è¿›è¡Œé›†æˆï¼Œä»¥å®ç°æ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚

## metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents
### Abstract
Significant advancements have occurred in the application of Large Language
Models (LLMs) for various tasks and social simulations. Despite this, their
capacities to coordinate within task-oriented social contexts are
under-explored. Such capabilities are crucial if LLMs are to effectively mimic
human-like social behavior and produce meaningful results. To bridge this gap,
we introduce collaborative generative agents, endowing LLM-based Agents with
consistent behavior patterns and task-solving abilities. We situate these
agents in a simulated job fair environment as a case study to scrutinize their
coordination skills. We propose a novel framework that equips collaborative
generative agents with human-like reasoning abilities and specialized skills.
Our evaluation demonstrates that these agents show promising performance.
However, we also uncover limitations that hinder their effectiveness in more
complex coordination tasks. Our work provides valuable insights into the role
and evolution of LLMs in task-oriented social simulations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaAgentsï¼šåŸºäºLLMçš„ä»»åŠ¡å¯¼å‘åè°ƒçš„åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºå’Œæ‰§è¡Œä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºã€‚ç„¶è€Œï¼ŒLLMsåœ¨ä»»åŠ¡å¯¼å‘çš„ç¤¾ä¼šç¯å¢ƒä¸­çš„åè°ƒèƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†ä½¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»çš„ç¤¾ä¼šè¡Œä¸ºå¹¶äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœï¼Œè¿™ç§èƒ½åŠ›è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼ˆCollaborative Generative Agentsï¼‰ï¼Œä¸ºåŸºäºLLMçš„æ™ºèƒ½ä½“èµ‹äºˆäº†ä¸€è‡´çš„è¡Œä¸ºæ¨¡å¼å’Œä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚ä¸ºäº†ç ”ç©¶è¿™äº›æ™ºèƒ½ä½“çš„åè°ƒèƒ½åŠ›ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ‹›è˜ä¼šç¯å¢ƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«æ„ŸçŸ¥ã€è®°å¿†ã€æ¨ç†å’Œæ‰§è¡Œæ¨¡å—çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“å…·å¤‡ç±»ä¼¼äººç±»çš„æ¨ç†èƒ½åŠ›å’Œä¸“ä¸šæŠ€èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¨¡æ‹Ÿçš„æ‹›è˜ä¼šç¯å¢ƒä¸­ï¼Œåä½œç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨è¯†åˆ«åˆæ ¼æ±‚èŒè€…ã€è®¾è®¡å·¥ä½œæµç¨‹å’Œåˆ†é…è§’è‰²æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ‹›è˜ä¼šå¤æ‚æ€§çš„å¢åŠ ï¼Œæ™ºèƒ½ä½“åœ¨åè°ƒæ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦å½’å› äºLLMsçš„ç›®æ ‡æˆ–æ„å›¾ä¸åŒ¹é…ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“æ¡†æ¶ä¸ºLLMsåœ¨ä»»åŠ¡å¯¼å‘çš„ç¤¾ä¼šæ¨¡æ‹Ÿä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ‹›è˜ã€å›¢é˜Ÿåä½œå’Œç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## face-recognition-methods-&-applications
### Abstract
Face recognition presents a challenging problem in the field of image
analysis and computer vision. The security of information is becoming very
significant and difficult. Security cameras are presently common in airports,
Offices, University, ATM, Bank and in any locations with a security system.
Face recognition is a biometric system used to identify or verify a person from
a digital image. Face Recognition system is used in security. Face recognition
system should be able to automatically detect a face in an image. This involves
extracts its features and then recognize it, regardless of lighting,
expression, illumination, ageing, transformations (translate, rotate and scale
image) and pose, which is a difficult task. This paper contains three sections.
The first section describes the common methods like holistic matching method,
feature extraction method and hybrid methods. The second section describes
applications with examples and finally third section describes the future
research directions of face recognition.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ã€Šäººè„¸è¯†åˆ«æ–¹æ³•ä¸åº”ç”¨ã€‹ï¼šæ·±å…¥æ¢ç´¢äººè„¸è¯†åˆ«æŠ€æœ¯çš„å¥¥ç§˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ä¿¡æ¯å®‰å…¨å˜å¾—è¶Šæ¥è¶Šé‡è¦å’Œå¤æ‚ï¼Œäººè„¸è¯†åˆ«ä½œä¸ºä¸€ç§ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿï¼Œåœ¨å›¾åƒåˆ†æå’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨äººè„¸è¯†åˆ«çš„å¸¸è§æ–¹æ³•ã€åº”ç”¨å®ä¾‹ä»¥åŠæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æœŸä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå®è·µæä¾›å‚è€ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢æ¢³ç†äººè„¸è¯†åˆ«æ–¹æ³•
æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†ä¸‰ç§ä¸»æµçš„äººè„¸è¯†åˆ«æ–¹æ³•ï¼šæ•´ä½“åŒ¹é…æ–¹æ³•ã€ç‰¹å¾æå–æ–¹æ³•å’Œæ··åˆæ–¹æ³•ã€‚æ•´ä½“åŒ¹é…æ–¹æ³•å¦‚Eigenfacesï¼Œé€šè¿‡ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æå–é¢éƒ¨ç‰¹å¾ï¼›ç‰¹å¾æå–æ–¹æ³•å…³æ³¨å±€éƒ¨ç‰¹å¾ï¼Œå¦‚çœ¼ç›ã€é¼»å­å’Œå˜´å·´ï¼›æ··åˆæ–¹æ³•ç»“åˆäº†æ•´ä½“å’Œå±€éƒ¨ç‰¹å¾ï¼Œé€šå¸¸ä½¿ç”¨3Då›¾åƒè¿›è¡Œè¯†åˆ«ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸°å¯Œçš„äººè„¸è¯†åˆ«åº”ç”¨æ¡ˆä¾‹
æ–‡ç« ä¸ä»…ä»‹ç»äº†äººè„¸è¯†åˆ«çš„åŸºæœ¬æ–¹æ³•ï¼Œè¿˜æä¾›äº†å¤šä¸ªå®é™…åº”ç”¨æ¡ˆä¾‹ï¼Œå¦‚é€‰æ°‘æ³¨å†Œç³»ç»Ÿä¸­çš„é‡å¤èº«ä»½è¯†åˆ«ã€è®¡ç®—æœºç™»å½•ç›‘æ§ã€æœºåœºå®‰å…¨ç³»ç»Ÿã€å›¾åƒæ•°æ®åº“è°ƒæŸ¥ç­‰ï¼Œå±•ç¤ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„å®éªŒç»“æœï¼Œä½†é€šè¿‡æ–‡çŒ®ç»¼è¿°å’Œæ¡ˆä¾‹åˆ†æï¼Œå±•ç¤ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨å¤šç§åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ–¹æ³•å¤šæ ·æ€§**ï¼šæœ¬æ–‡æä¾›äº†å¤šç§äººè„¸è¯†åˆ«æ–¹æ³•ï¼Œç ”ç©¶è€…å¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚
2. **å®é™…åº”ç”¨æ¡ˆä¾‹**ï¼šé€šè¿‡å®é™…æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨ç°å®ä¸–ç•Œä¸­çš„å…·ä½“åº”ç”¨ï¼Œä¸ºå…¶ä»–ç ”ç©¶è€…æä¾›äº†å®è·µå‚è€ƒã€‚
3. **æœªæ¥ç ”ç©¶æ–¹å‘**ï¼šæ–‡ç« æŒ‡å‡ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œå¦‚2Då’Œ3Däººè„¸è¯†åˆ«ã€å¤§è§„æ¨¡åº”ç”¨ç­‰ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ–¹å‘æŒ‡å¼•ã€‚

æ€»ä¹‹ï¼Œæœ¬æ–‡å¯¹äººè„¸è¯†åˆ«æŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„æ¢³ç†å’Œæ¢è®¨ï¼Œå¯¹äºäººè„¸è¯†åˆ«é¢†åŸŸçš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents
### Abstract
Large Language Models (LLMs) have shown great success as high-level planners
for zero-shot game-playing agents. However, these agents are primarily
evaluated on Minecraft, where long-term planning is relatively straightforward.
In contrast, agents tested in dynamic robot environments face limitations due
to simplistic environments with only a few objects and interactions. To fill
this gap in the literature, we present NetPlay, the first LLM-powered zero-shot
agent for the challenging roguelike NetHack. NetHack is a particularly
challenging environment due to its diverse set of items and monsters, complex
interactions, and many ways to die.
  NetPlay uses an architecture designed for dynamic robot environments,
modified for NetHack. Like previous approaches, it prompts the LLM to choose
from predefined skills and tracks past interactions to enhance decision-making.
Given NetHack's unpredictable nature, NetPlay detects important game events to
interrupt running skills, enabling it to react to unforeseen circumstances.
While NetPlay demonstrates considerable flexibility and proficiency in
interacting with NetHack's mechanics, it struggles with ambiguous task
descriptions and a lack of explicit feedback. Our findings demonstrate that
NetPlay performs best with detailed context information, indicating the
necessity for dynamic methods in supplying context information for complex
games such as NetHack.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLMs åœ¨ NetHack ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ï¼šé›¶æ ·æœ¬æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¸¸æˆé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„è§„åˆ’èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ Minecraft ç­‰æ¸¸æˆä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒæ—¶ï¼Œå¦‚æœºå™¨äººç¯å¢ƒï¼Œå¾€å¾€é¢ä¸´å±€é™æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº† NetPlayï¼Œä¸€ä¸ªåŸºäº LLM çš„é›¶æ ·æœ¬æ™ºèƒ½ä½“ï¼Œç”¨äºæŒ‘æˆ˜æ€§çš„ Rogue-like æ¸¸æˆ NetHackã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
NetPlay é‡‡ç”¨äº†ä¸€ç§ä¸“ä¸ºåŠ¨æ€æœºå™¨äººç¯å¢ƒè®¾è®¡çš„æ¶æ„ï¼Œå¹¶é’ˆå¯¹ NetHack è¿›è¡Œäº†ä¿®æ”¹ã€‚å®ƒé€šè¿‡æç¤º LLM ä»é¢„å®šä¹‰çš„æŠ€èƒ½ä¸­é€‰æ‹©ï¼Œå¹¶è·Ÿè¸ªè¿‡å»çš„äº¤äº’æ¥å¢å¼ºå†³ç­–ã€‚NetPlay è¿˜èƒ½å¤Ÿæ£€æµ‹é‡è¦çš„æ¸¸æˆäº‹ä»¶ï¼Œä»¥ä¾¿åœ¨å‡ºç°æ„å¤–æƒ…å†µæ—¶ä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„æŠ€èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒNetPlay åœ¨ä¸ NetHack çš„æœºåˆ¶äº¤äº’æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ¨¡ç³Šçš„ä»»åŠ¡æè¿°å’Œç¼ºä¹æ˜ç¡®åé¦ˆæ—¶å­˜åœ¨å›°éš¾ã€‚NetPlay åœ¨æä¾›è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œè¿™è¡¨æ˜åœ¨ NetHack ç­‰å¤æ‚æ¸¸æˆä¸­ï¼ŒåŠ¨æ€æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–¹æ³•è‡³å…³é‡è¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
NetPlay çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMs åœ¨æ¸¸æˆé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨ LLMs çš„èƒ½åŠ›ï¼Œä¾‹å¦‚é€šè¿‡åŠ¨æ€æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–ä½¿ç”¨æœºå™¨å­¦ä¹ æ¥æ›¿ä»£æ‰‹å·¥åˆ¶ä½œçš„ç»„ä»¶ã€‚æ­¤å¤–ï¼ŒNetPlay çš„æ¶æ„å¯ä»¥ä¸ºå…¶ä»–å¤æ‚æ¸¸æˆçš„è®¾è®¡æä¾›å‚è€ƒã€‚

## enabling-multimodal-generation-on-clip-via-vision-language-knowledge-distillation
### Abstract
The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡è§†è§‰-è¯­è¨€çŸ¥è¯†è’¸é¦åœ¨CLIPä¸Šå®ç°å¤šæ¨¡æ€ç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§è§„æ¨¡çš„è§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹åœ¨åŒæµæ¶æ„ï¼ˆå¦‚CLIPï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå°¤å…¶åœ¨å„ç§å¤šæ¨¡æ€å¯¹é½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å›¾åƒå­—å¹•å’Œå¼€æ”¾å¼è§†è§‰é—®ç­”ï¼‰ä¸Šçš„è¡¨ç°å´ç›¸å¯¹è¾ƒå¼±ï¼Œä¸»è¦å½’å› äºæ–‡æœ¬ç¼–ç å™¨çš„ä¸è¶³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è§†è§‰-è¯­è¨€çŸ¥è¯†è’¸é¦ï¼ˆVLKDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†CLIPçš„æ–‡æœ¬ç¼–ç å™¨æ›¿æ¢ä¸ºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼Œå¦‚BARTï¼‰ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚VLKDæ–¹æ³•ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªç›®æ ‡ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ–‡æœ¬-æ–‡æœ¬è·ç¦»æœ€å°åŒ–ï¼ˆTTDMï¼‰
é€šè¿‡æœ€å°åŒ–CLIPæ–‡æœ¬ç¼–ç å™¨å’ŒBARTç¼–ç å™¨åœ¨ç›¸åŒè¾“å…¥æ–‡æœ¬ä¸‹çš„è¾“å‡ºè¡¨ç¤ºä¹‹é—´çš„â„“2è·ç¦»ï¼Œä½¿ä¸¤è€…åœ¨æ–‡æœ¬è¡¨ç¤ºä¸Šä¿æŒä¸€è‡´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå›¾åƒ-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼ˆITCLï¼‰
é€šè¿‡ä¼˜åŒ–BARTç¼–ç å™¨å’ŒCLIPå›¾åƒç¼–ç å™¨è¾“å‡ºè¡¨ç¤ºä¹‹é—´çš„å¯¹ç§°InfoNCEæŸå¤±ï¼Œä½¿BARTç¼–ç å™¨æ›´å¥½åœ°é€‚åº”CLIPçš„å¤šæ¨¡æ€ç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå›¾åƒæ¡ä»¶æ–‡æœ¬å¡«å……ï¼ˆICTIï¼‰
é€šè¿‡åœ¨BARTè§£ç å™¨ä¸Šè¿›è¡Œå›¾åƒæ¡ä»¶ä¸‹çš„æ–‡æœ¬å¡«å……ä»»åŠ¡ï¼Œä½¿BARTè§£ç å™¨èƒ½å¤Ÿç†è§£å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆä¸å›¾åƒç›¸å…³çš„æ–‡æœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒVLKDæ¨¡å‹åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚åœ¨VQAv2æ•°æ®é›†ä¸Šå®ç°äº†44.5%çš„é›¶æ ·æœ¬å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œä¸”å‚æ•°é‡å‡å°‘äº†7å€ã€‚æ­¤å¤–ï¼ŒVLKDæ¨¡å‹åœ¨NLPä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–VLPæ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
VLKDæ–¹æ³•ä¸ºå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦å°†CLIPå’ŒPLMçš„ä¼˜åŠ¿ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹çš„å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ä¿æŒäº†PLMåœ¨NLPä»»åŠ¡ä¸Šçš„åŸå§‹èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¤šæ¨¡æ€å’Œå•æ¨¡æ€ä»»åŠ¡ä¸­éƒ½å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering
### Abstract
Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium
### ğŸŒŸ è®ºæ–‡è§£è¯» | AlphaCodiumï¼šä»æç¤ºå·¥ç¨‹åˆ°æµç¨‹å·¥ç¨‹ï¼Œæå‡ä»£ç ç”Ÿæˆæ€§èƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä»£ç ç”Ÿæˆé—®é¢˜ä¸å¸¸è§çš„è‡ªç„¶è¯­è¨€é—®é¢˜ä¸åŒï¼Œå®ƒéœ€è¦åŒ¹é…ç›®æ ‡è¯­è¨€çš„ç²¾ç¡®è¯­æ³•ï¼Œè¯†åˆ«æ­£å¸¸è·¯å¾„å’Œè¾¹ç¼˜æƒ…å†µï¼Œå…³æ³¨é—®é¢˜è§„èŒƒä¸­çš„è®¸å¤šå°ç»†èŠ‚ï¼Œå¹¶è§£å†³å…¶ä»–ä»£ç ç‰¹å®šçš„é—®é¢˜å’Œè¦æ±‚ã€‚å› æ­¤ï¼Œè®¸å¤šåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­æˆåŠŸçš„ä¼˜åŒ–å’ŒæŠ€å·§å¯èƒ½å¯¹ä»£ç ä»»åŠ¡æ— æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºAlphaCodiumï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºæµ‹è¯•çš„å¤šé˜¶æ®µã€ä»£ç å¯¼å‘çš„è¿­ä»£æµç¨‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæµ‹è¯•å¯¼å‘çš„è¿­ä»£æµç¨‹
AlphaCodiumçš„æ ¸å¿ƒæ˜¯è¿­ä»£æµç¨‹ï¼Œå…¶ä¸­ç”Ÿæˆçš„ä»£ç ä¼šåå¤è¿è¡Œå¹¶é’ˆå¯¹è¾“å…¥è¾“å‡ºæµ‹è¯•è¿›è¡Œä¿®å¤ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹é€æ­¥æ”¹è¿›å…¶è§£å†³æ–¹æ¡ˆï¼Œç›´åˆ°æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé˜¶æ®µå¤„ç†
AlphaCodiumæµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„å¤„ç†é˜¶æ®µå’Œä»£ç è¿­ä»£é˜¶æ®µã€‚åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œæ¨¡å‹ä¼šå¯¹é—®é¢˜è¿›è¡Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œä¾‹å¦‚ç”Ÿæˆé—®é¢˜åæ€å’Œå…¬å…±æµ‹è¯•æ¨ç†ã€‚åœ¨ä»£ç è¿­ä»£é˜¶æ®µï¼Œæ¨¡å‹ä¼šç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨å…¬å…±å’ŒAIç”Ÿæˆçš„æµ‹è¯•ä¸Šè¿›è¡Œè¿­ä»£å’Œä¿®å¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µ
AlphaCodiumè¿˜é‡‡ç”¨äº†å¤šç§ä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µï¼Œä¾‹å¦‚YAMLç»“æ„åŒ–è¾“å‡ºã€é€šè¿‡é¡¹ç›®ç¬¦å·åˆ†æè¿›è¡Œè¯­ä¹‰æ¨ç†ã€ç”Ÿæˆæ¨¡å—åŒ–ä»£ç ã€è½¯å†³ç­–å’ŒåŒé‡éªŒè¯ã€é¼“åŠ±æ¢ç´¢ä»¥åŠæµ‹è¯•é”šç‚¹ã€‚è¿™äº›æ¦‚å¿µæœ‰åŠ©äºæé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨CodeContestsæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒAlphaCodiumæµç¨‹æ˜¾è‘—æé«˜äº†LLMsåœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒGPT-4åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼ˆpass@5ï¼‰ä»ä½¿ç”¨å•ä¸ªç²¾å¿ƒè®¾è®¡çš„ç›´æ¥æç¤ºçš„19%æé«˜åˆ°ä½¿ç”¨AlphaCodiumæµç¨‹çš„44%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AlphaCodiumçš„è®¸å¤šåŸåˆ™å’Œæœ€ä½³å®è·µå¯ä»¥å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºã€ç”Ÿæˆæ¨¡å—åŒ–ä»£ç ã€é€šè¿‡é¡¹ç›®ç¬¦å·åˆ†æè¿›è¡Œè¯­ä¹‰æ¨ç†ã€è½¯å†³ç­–å’ŒåŒé‡éªŒè¯ã€é¼“åŠ±æ¢ç´¢ä»¥åŠæµ‹è¯•é”šç‚¹ç­‰æŠ€æœ¯éƒ½å¯ä»¥å¸®åŠ©æé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚

### ğŸ“š æ€»ç»“
AlphaCodiumæ˜¯ä¸€ç§åˆ›æ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡æµ‹è¯•å¯¼å‘çš„è¿­ä»£æµç¨‹å’Œå¤šé˜¶æ®µå¤„ç†ï¼Œæ˜¾è‘—æé«˜äº†LLMsåœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†å¤šç§ä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µï¼Œè¿›ä¸€æ­¥æé«˜äº†ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚AlphaCodiumçš„è®¸å¤šåŸåˆ™å’Œæœ€ä½³å®è·µå¯ä»¥å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œä¸ºä»£ç ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚

## world-models-with-hints-of-large-language-models-for-goal-achieving
### Abstract
Reinforcement learning struggles in the face of long-horizon tasks and sparse
goals due to the difficulty in manual reward specification. While existing
methods address this by adding intrinsic rewards, they may fail to provide
meaningful guidance in long-horizon decision-making tasks with large state and
action spaces, lacking purposeful exploration. Inspired by human cognition, we
propose a new multi-modal model-based RL approach named Dreaming with Large
Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the
LLMs into the model rollouts to encourage goal discovery and reaching in
challenging tasks. By assigning higher intrinsic rewards to samples that align
with the hints outlined by the language model during model rollouts, DLLM
guides the agent toward meaningful and efficient exploration. Extensive
experiments demonstrate that the DLLM outperforms recent methods in various
challenging, sparse-reward environments such as HomeGrid, Crafter, and
Minecraft by 27.7\%, 21.1\%, and 9.9\%, respectively.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºçš„å¼ºåŒ–å­¦ä¹ ä¸–ç•Œæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤„ç†é•¿æœŸä»»åŠ¡å’Œç¨€ç–ç›®æ ‡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºæ‰‹åŠ¨æŒ‡å®šå¥–åŠ±å‡½æ•°éå¸¸å›°éš¾ã€‚ç°æœ‰çš„æ–¹æ³•é€šè¿‡æ·»åŠ å†…åœ¨å¥–åŠ±æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨å…·æœ‰å¤§å‹çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„é•¿æœŸå†³ç­–ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬å¯èƒ½æ— æ³•æä¾›æœ‰æ„ä¹‰çš„æŒ‡å¯¼ï¼Œç¼ºä¹æœ‰ç›®çš„çš„æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDreaming with Large Language Models (DLLM)
DLLM æ˜¯ä¸€ç§æ–°çš„å¤šæ¨¡æ€åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äººç±»è‡ªç„¶è¯­è¨€æ¥æè¿°ç¯å¢ƒåŠ¨æ€ï¼Œå¹¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡å¯¼æ•´åˆåˆ°æ¨¡å‹æ»šåŠ¨ä¸­ï¼Œä»¥æé«˜ä»£ç†çš„æ¢ç´¢å’Œç›®æ ‡å®Œæˆèƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäº LLM ç”Ÿæˆçš„ç›®æ ‡ï¼ŒDLLM å¯ä»¥é€šè¿‡è‡ªåŠ¨é€’å‡æœºåˆ¶ç”Ÿæˆæœ‰æ„ä¹‰çš„å†…åœ¨å¥–åŠ±ï¼Œä»¥æŒ‡å¯¼ç­–ç•¥å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
DLLM åœ¨å„ç§ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ä¼˜äºæœ€è¿‘çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ HomeGridã€Crafter å’Œ Minecraftï¼Œåˆ†åˆ«æé«˜äº† 27.7%ã€21.1% å’Œ 9.9%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DLLM çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§å¤æ‚ç¯å¢ƒï¼Œå¹¶åˆ©ç”¨è¯­è¨€ä¿¡æ¯æ¥æé«˜ä»£ç†çš„æ¢ç´¢å’Œç›®æ ‡å®Œæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒDLLM çš„è‡ªåŠ¨é€’å‡æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°é¿å…ä»£ç†é‡å¤å®Œæˆç®€å•ä»»åŠ¡ï¼Œä»è€Œä¿ƒè¿›ä»£ç†æ¢ç´¢æ›´å¤æ‚çš„è¡Œä¸ºã€‚

## enhancing-agent-learning-through-world-dynamics-modeling
### Abstract
Large language models (LLMs) have been increasingly applied to tasks in
language understanding and interactive decision-making, with their impressive
performance largely attributed to the extensive domain knowledge embedded
within them. However, the depth and breadth of this knowledge can vary across
domains. Many existing approaches assume that LLMs possess a comprehensive
understanding of their environment, often overlooking potential gaps in their
grasp of actual world dynamics. To address this, we introduce Discover, Verify,
and Evolve (DiVE), a framework that discovers world dynamics from a small
number of demonstrations, verifies the accuracy of these dynamics, and evolves
new, advanced dynamics tailored to the current situation. Through extensive
evaluations, we assess the impact of each component on performance and compare
the dynamics generated by DiVE to human-annotated dynamics. Our results show
that LLMs guided by DiVE make more informed decisions, achieving rewards
comparable to human players in the Crafter environment and surpassing methods
that require prior task-specific training in the MiniHack environment.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡ä¸–ç•ŒåŠ¨æ€å»ºæ¨¡å¢å¼ºæ™ºèƒ½ä½“å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£å’Œäº¤äº’å¼å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿™ä¸»è¦å½’åŠŸäºå®ƒä»¬åµŒå…¥çš„å¹¿æ³›é¢†åŸŸçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™ç§çŸ¥è¯†çš„æ·±åº¦å’Œå¹¿åº¦åœ¨ä¸åŒé¢†åŸŸä¹‹é—´å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚è®¸å¤šç°æœ‰æ–¹æ³•å‡è®¾LLMså¯¹å…¶ç¯å¢ƒæœ‰å…¨é¢çš„ç†è§£ï¼Œä½†å¾€å¾€å¿½è§†äº†å®ƒä»¬å¯¹å®é™…ä¸–ç•ŒåŠ¨æ€çš„æŒæ¡å¯èƒ½å­˜åœ¨å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Discover, Verify, and Evolve (DiVE)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»å°‘é‡æ¼”ç¤ºä¸­å‘ç°ä¸–ç•ŒåŠ¨æ€ï¼ŒéªŒè¯è¿™äº›åŠ¨æ€çš„å‡†ç¡®æ€§ï¼Œå¹¶æ ¹æ®å½“å‰æƒ…å†µæ¼”åŒ–æ–°çš„ã€å…ˆè¿›çš„åŠ¨æ€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDiVEæ¡†æ¶
DiVEæ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼š
- Discovererï¼šä½¿ç”¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ä»æ¼”ç¤ºä¸­è¿­ä»£åœ°å‘ç°ç¯å¢ƒåŠ¨æ€ã€‚
- Verifierï¼šæ¶ˆé™¤LLMsç”±äºå¹»è§‰å€¾å‘è€Œå¯¼è‡´çš„ä¸å¯é åŠ¨æ€ã€‚
- Evolverï¼šæ ¹æ®å­¦ä¹ çš„åŠ¨æ€ï¼Œæ¨ç†å‡ºé’ˆå¯¹å½“å‰æƒ…å†µçš„æ·±å…¥ã€ç‰¹å®šäºçŠ¶æ€çš„çŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå±‚æ¬¡è¯¾ç¨‹å­¦ä¹ 
DiVEé‡‡ç”¨å±‚æ¬¡è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œä»ç®€å•åˆ°å¤æ‚çš„åŠ¨æ€é€æ­¥å­¦ä¹ ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»ä»»åŠ¡åˆ†è§£å±‚æ¬¡ç»“æ„ä¸­çš„å…ƒç´ ï¼ˆå¦‚åŠ¨ä½œã€å¯¹è±¡ã€å­ä»»åŠ¡å’Œå­ç›®æ ‡ï¼‰å¼€å§‹ï¼Œé€æ­¥å­¦ä¹ å®ƒä»¬çš„åŠ¨æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŠ¨æ€éªŒè¯
ä¸ºäº†ç¡®ä¿åŠ¨æ€çš„å‡†ç¡®æ€§ï¼ŒDiVEå¼•å…¥äº†åŠ¨æ€éªŒè¯å™¨ï¼Œå®ƒå¯ä»¥è¿‡æ»¤æ‰å¯èƒ½æ— æ•ˆå’Œå†²çªçš„åŠ¨æ€å€™é€‰è€…ï¼Œä»è€Œæé«˜å†³ç­–è¿‡ç¨‹çš„å¯é æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåœ¨çº¿ç­–ç•¥å­¦ä¹ 
DiVEä¸ä»…å­¦ä¹ åŸºæœ¬è§„åˆ™ï¼Œè¿˜ä¸“æ³¨äºæ ¹æ®è¿™äº›åŠ¨æ€å¼€å‘é«˜çº§æ¸¸æˆç­–ç•¥ã€‚å®ƒé€šè¿‡åœ¨çº¿å­¦ä¹ æ–¹æ³•å°†åŠ¨æ€æ¼”åŒ–ä¸ºç­–ç•¥ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆå½“å‰æ¸¸æˆåœºæ™¯çš„ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Crafterå’ŒMiniHackç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒDiVEåœ¨æ€§èƒ½æ–¹é¢ä¼˜äºæ‰€æœ‰å…¶ä»–åŸºçº¿æ¨¡å‹ã€‚åœ¨Crafterç¯å¢ƒä¸­ï¼ŒDiVEåœ¨åˆ†æ•°å’Œå¥–åŠ±æ–¹é¢åˆ†åˆ«æ¯”SOTA LLMæ–¹æ³•SPRINGæé«˜äº†337.8%å’Œ110.1%ï¼Œå¹¶ä¸”è¶…è¿‡äº†SOTA RLæ–¹æ³•DreamerV3ã€‚åœ¨MiniHackç¯å¢ƒä¸­ï¼ŒDiVEåœ¨Lava Crossingä»»åŠ¡ä¸Šä¸SSOå’ŒReflexionï¼ˆéƒ½éœ€è¦30æ¬¡è¿­ä»£è®­ç»ƒï¼‰çš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”åœ¨Wand of Deathå’ŒQuestä»»åŠ¡ä¸Šè¶…è¿‡äº†è¿™ä¸¤ä¸ªåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DiVEæ¡†æ¶ä¸ºè§£å†³LLMsåœ¨ç‰¹å®šé¢†åŸŸä¸­çš„çŸ¥è¯†å·®è·é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å‘ç°ã€éªŒè¯å’Œæ¼”åŒ–ä¸–ç•ŒåŠ¨æ€ï¼Œæé«˜äº†LLMsçš„å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒDiVEçš„å±‚æ¬¡è¯¾ç¨‹å­¦ä¹ å’ŒåŠ¨æ€éªŒè¯æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦é•¿æœŸè§„åˆ’å’Œå†³ç­–çš„ä»»åŠ¡ä¸­ã€‚

## watch-your-step--learning-node-embeddings-via-graph-attention
### Abstract
Graph embedding methods represent nodes in a continuous vector space,
preserving information from the graph (e.g. by sampling random walks). There
are many hyper-parameters to these methods (such as random walk length) which
have to be manually tuned for every graph. In this paper, we replace random
walk hyper-parameters with trainable parameters that we automatically learn via
backpropagation. In particular, we learn a novel attention model on the power
series of the transition matrix, which guides the random walk to optimize an
upstream objective. Unlike previous approaches to attention models, the method
that we propose utilizes attention parameters exclusively on the data (e.g. on
the random walk), and not used by the model for inference. We experiment on
link prediction tasks, as we aim to produce embeddings that best-preserve the
graph structure, generalizing to unseen information. We improve
state-of-the-art on a comprehensive suite of real world datasets including
social, collaboration, and biological networks. Adding attention to random
walks can reduce the error by 20% to 45% on datasets we attempted. Further, our
learned attention parameters are different for every graph, and our
automatically-found values agree with the optimal choice of hyper-parameter if
we manually tune existing methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å›¾æ³¨æ„åŠ›å­¦ä¹ èŠ‚ç‚¹åµŒå…¥ï¼šWatch Your Step

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å›¾åµŒå…¥æ–¹æ³•æ—¨åœ¨å°†å›¾ä¸­çš„èŠ‚ç‚¹è¡¨ç¤ºä¸ºè¿ç»­çš„å‘é‡ç©ºé—´ï¼Œä»è€Œä¿ç•™å›¾ä¸­çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´è®¸å¤šè¶…å‚æ•°ï¼Œä¾‹å¦‚éšæœºæ¸¸èµ°çš„é•¿åº¦ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾æ³¨æ„åŠ›æ¨¡å‹ï¼Œé€šè¿‡è‡ªåŠ¨å­¦ä¹ å¯è®­ç»ƒå‚æ•°æ¥æ›¿ä»£éšæœºæ¸¸èµ°çš„è¶…å‚æ•°ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨æ³¨æ„åŠ›å‚æ•°æ¥æŒ‡å¯¼éšæœºæ¸¸èµ°ï¼Œä½¿å…¶ä¼˜åŒ–ä¸Šæ¸¸ç›®æ ‡ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™å›¾ç»“æ„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸ä¹‹å‰çš„æ³¨æ„åŠ›æ¨¡å‹ä¸åŒï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä»…åœ¨æ•°æ®éå†ï¼ˆä¾‹å¦‚éšæœºæ¸¸èµ°ï¼‰ä¸Šä½¿ç”¨æ³¨æ„åŠ›å‚æ•°ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å‹æ¨ç†ä¸­ä½¿ç”¨ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ¯ä¸ªå›¾çš„æœ€ä½³æ³¨æ„åŠ›å‚æ•°ï¼Œä»è€Œæé«˜åµŒå…¥çš„è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„å›¾æ³¨æ„åŠ›æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æé«˜åµŒå…¥çš„è´¨é‡ï¼Œå°†é”™è¯¯ç‡é™ä½äº†20%è‡³40%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ï¼Œè‡ªåŠ¨å­¦ä¹ çš„æ³¨æ„åŠ›å‚æ•°ä¸æ‰‹åŠ¨è°ƒæ•´ç°æœ‰æ–¹æ³•å¾—åˆ°çš„æœ€ä½³è¶…å‚æ•°ä¸€è‡´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å›¾æ³¨æ„åŠ›æ¨¡å‹ä¸ºå›¾åµŒå…¥æ–¹æ³•æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡è‡ªåŠ¨å­¦ä¹ å¯è®­ç»ƒå‚æ•°æ¥æ›¿ä»£éšæœºæ¸¸èµ°çš„è¶…å‚æ•°ï¼Œä»è€Œæé«˜äº†åµŒå…¥çš„è´¨é‡å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•å°†æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨äºå›¾åµŒå…¥ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## from-centralized-to-self-supervised--pursuing-realistic-multi-agent-reinforcement-learning
### Abstract
In real-world environments, autonomous agents rely on their egocentric
observations. They must learn adaptive strategies to interact with others who
possess mixed motivations, discernible only through visible cues. Several
Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches
that involve either centralized training or reward-sharing, often violating the
realistic ways in which living organisms, like animals or humans, process
information and interact. MARL strategies deploying decentralized training with
intrinsic motivation offer a self-supervised approach, enable agents to develop
flexible social strategies through the interaction of autonomous agents.
However, by contrasting the self-supervised and centralized methods, we reveal
that populations trained with reward-sharing methods surpass those using
self-supervised methods in a mixed-motive environment. We link this superiority
to specialized role emergence and an agent's expertise in its role.
Interestingly, this gap shrinks in pure-motive settings, emphasizing the need
for evaluations in more complex, realistic environments (mixed-motive). Our
preliminary results suggest a gap in population performance that can be closed
by improving self-supervised methods and thereby pushing MARL closer to
real-world readiness.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»é›†ä¸­å¼åˆ°è‡ªç›‘ç£ï¼šè¿½æ±‚ç°å®çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè‡ªä¸»æ™ºèƒ½ä½“éœ€è¦ä¾èµ–è‡ªèº«çš„è§‚å¯Ÿæ¥å­¦ä¹ é€‚åº”æ€§çš„ç­–ç•¥ï¼Œä»¥ä¸å…¶ä»–å…·æœ‰æ··åˆåŠ¨æœºçš„æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•å¤§å¤šé‡‡ç”¨é›†ä¸­å¼æ–¹æ³•ï¼Œå¦‚é›†ä¸­è®­ç»ƒæˆ–å¥–åŠ±å…±äº«ï¼Œè¿™å¾€å¾€è¿èƒŒäº†ç°å®ä¸–ç•Œä¸­ç”Ÿç‰©ä½“ï¼ˆå¦‚åŠ¨ç‰©æˆ–äººç±»ï¼‰å¤„ç†ä¿¡æ¯å’Œäº¤äº’çš„æ–¹å¼ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢è‡ªç›‘ç£æ–¹æ³•åœ¨MARLä¸­çš„åº”ç”¨ï¼Œä»¥ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªä¸»äº¤äº’å‘å±•çµæ´»çš„ç¤¾ä¼šç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡å¯¹æ¯”äº†è‡ªç›‘ç£å’Œé›†ä¸­å¼æ–¹æ³•åœ¨MARLä¸­çš„åº”ç”¨ã€‚è‡ªç›‘ç£æ–¹æ³•é€šè¿‡å†…åœ¨åŠ¨æœºé©±åŠ¨æ™ºèƒ½ä½“è¿›è¡Œæ¢ç´¢å’Œå­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿä»æœ‰é™çš„è§‚å¯Ÿä¸­æ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ã€ç›®æ ‡å’Œå¥–åŠ±ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡åœ¨ä¸¤ä¸ªå…·æœ‰ä¸åŒç¤¾ä¼šåŠ¨æ€çš„ç¯å¢ƒï¼ˆæ··åˆåŠ¨æœºå’Œçº¯åŠ¨æœºï¼‰ä¸­è¯„ä¼°äº†å¤šç§MARLæ¨¡å‹ï¼ŒåŒ…æ‹¬ç‹¬ç«‹PPOã€MAPPOã€ICMã€ICM-rewardã€ç¤¾ä¼šå½±å“å’ŒSVOæ¨¡å‹ã€‚é€šè¿‡å¯¹æ¯”è¿™äº›æ¨¡å‹åœ¨äººå£ç»©æ•ˆå’Œå…¬å¹³æ€§æ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†è‡ªç›‘ç£æ–¹æ³•åœ¨ç°å®ä¸–ç•Œä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ··åˆåŠ¨æœºç¯å¢ƒï¼ˆClean Upï¼‰ä¸­ï¼Œé›†ä¸­å¼æ–¹æ³•ï¼ˆSVO-HEã€SVO-HOã€MAPPOï¼‰ä¼˜äºæ‰€æœ‰è‡ªç›‘ç£æ–¹æ³•ã€‚SVO-HEæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œè€ŒIPPOæ¨¡å‹è¡¨ç°æœ€å·®ã€‚å…·æœ‰å†…åœ¨åŠ¨æœºçš„æ¨¡å‹ï¼ˆICMã€ICM-rewardã€ç¤¾ä¼šå½±å“ï¼‰ç›¸å¯¹äºIPPOæ¨¡å‹æœ‰æ‰€æ”¹è¿›ï¼Œä½†å¹¶æœªè¶…è¿‡é›†ä¸­å¼æ–¹æ³•ã€‚åœ¨çº¯åŠ¨æœºç¯å¢ƒï¼ˆHarvestï¼‰ä¸­ï¼ŒSVO-HOæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œä½†ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚MAPPOæ¨¡å‹è¡¨ç°æœ€å·®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªç›‘ç£æ–¹æ³•åœ¨MARLä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´æœ‰æ•ˆçš„å†…åœ¨åŠ¨æœºæ–¹æ³•ï¼Œä»¥ç¼©å°è‡ªç›‘ç£å’Œé›†ä¸­å¼æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦å¼€å‘èƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒç¯å¢ƒåŠ¨æ€çš„æ¨¡å‹ï¼Œä»¥å®ç°æ›´æ¥è¿‘ç°å®ä¸–ç•Œçš„MARLã€‚

## agent-planning-with-world-knowledge-model
### Abstract
Recent endeavors towards directly using large language models (LLMs) as agent
models to execute interactive planning tasks have shown commendable results.
Despite their achievements, however, they still struggle with brainless
trial-and-error in global planning and generating hallucinatory actions in
local planning due to their poor understanding of the ``real'' physical world.
Imitating humans' mental world knowledge model which provides global prior
knowledge before the task and maintains local dynamic knowledge during the
task, in this paper, we introduce parametric World Knowledge Model (WKM) to
facilitate agent planning. Concretely, we steer the agent model to
self-synthesize knowledge from both expert and sampled trajectories. Then we
develop WKM, providing prior task knowledge to guide the global planning and
dynamic state knowledge to assist the local planning. Experimental results on
three complex real-world simulated datasets with three state-of-the-art
open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our
method can achieve superior performance compared to various strong baselines.
Besides, we analyze to illustrate that our WKM can effectively alleviate the
blind trial-and-error and hallucinatory action issues, providing strong support
for the agent's understanding of the world. Other interesting findings include:
1) our instance-level task knowledge can generalize better to unseen tasks, 2)
weak WKM can guide strong agent model planning, and 3) unified WKM training has
promising potential for further development. The code is available at
https://github.com/zjunlp/WKM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºä¸–ç•ŒçŸ¥è¯†æ¨¡å‹çš„æ™ºèƒ½ä½“è§„åˆ’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“ç›´æ¥ä½¿ç”¨LLMsä½œä¸ºæ™ºèƒ½ä½“æ¨¡å‹æ‰§è¡Œäº¤äº’å¼è§„åˆ’ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ç€ä¸€äº›æŒ‘æˆ˜ã€‚ç”±äºLLMsç¼ºä¹å¯¹çœŸå®ç‰©ç†ä¸–ç•Œçš„ç†è§£ï¼Œå®ƒä»¬åœ¨å…¨å±€è§„åˆ’ä¸­å®¹æ˜“å‡ºç°æ— ç›®çš„çš„è¯•é”™ï¼Œå¹¶åœ¨å±€éƒ¨è§„åˆ’ä¸­ç”Ÿæˆå¹»è§‰è¡Œä¸ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå‚æ•°åŒ–çš„ä¸–ç•ŒçŸ¥è¯†æ¨¡å‹ï¼ˆWKMï¼‰ï¼Œä»¥è¾…åŠ©æ™ºèƒ½ä½“è§„åˆ’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»»åŠ¡çŸ¥è¯†åˆæˆ
æœ¬æ–‡é€šè¿‡æ¯”è¾ƒä¸“å®¶è½¨è¿¹å’Œé‡‡æ ·è½¨è¿¹ï¼Œå¼•å¯¼æ™ºèƒ½ä½“æ¨¡å‹è‡ªæˆ‘åˆæˆä»»åŠ¡çŸ¥è¯†ã€‚ä»»åŠ¡çŸ¥è¯†ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œç”¨äºæŒ‡å¯¼æ™ºèƒ½ä½“æ¨¡å‹çš„å…¨å±€è§„åˆ’ï¼Œé¿å…æ— ç›®çš„çš„è¯•é”™ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçŠ¶æ€çŸ¥è¯†æ€»ç»“
æœ¬æ–‡é€šè¿‡æç¤ºæ™ºèƒ½ä½“æ¨¡å‹ï¼Œæ ¹æ®å†å²è¡Œä¸ºè‡ªæˆ‘æ€»ç»“çŠ¶æ€çŸ¥è¯†ï¼Œå¹¶æ„å»ºçŠ¶æ€çŸ¥è¯†åº“ã€‚çŠ¶æ€çŸ¥è¯†ä½œä¸ºåŠ¨æ€çŸ¥è¯†ï¼Œç”¨äºçº¦æŸæ™ºèƒ½ä½“æ¨¡å‹çš„å±€éƒ¨è§„åˆ’ï¼Œé¿å…ç”Ÿæˆå¹»è§‰è¡Œä¸ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸–ç•ŒçŸ¥è¯†æ¨¡å‹è®­ç»ƒ
æœ¬æ–‡å°†ç”Ÿæˆçš„ä¸–ç•ŒçŸ¥è¯†é›†æˆåˆ°ä¸“å®¶è½¨è¿¹ä¸­ï¼Œå¹¶è®­ç»ƒä¸–ç•ŒçŸ¥è¯†æ¨¡å‹ã€‚æ™ºèƒ½ä½“æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒä»¥é€‚åº”ä»»åŠ¡çŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåŸºäºä¸–ç•ŒçŸ¥è¯†æ¨¡å‹çš„æ™ºèƒ½ä½“è§„åˆ’
åœ¨è§„åˆ’é˜¶æ®µï¼Œæœ¬æ–‡ä½¿ç”¨ä¸–ç•ŒçŸ¥è¯†æ¨¡å‹ä¸ºæ™ºèƒ½ä½“æ¨¡å‹æä¾›å…¨å±€å…ˆéªŒä»»åŠ¡çŸ¥è¯†å’Œç»´æŠ¤å±€éƒ¨åŠ¨æ€çŠ¶æ€çŸ¥è¯†ã€‚ä»»åŠ¡çŸ¥è¯†å°†ä½œä¸ºè‡ªç„¶è¯­è¨€å½¢å¼ä¸ç‰¹å®šä»»åŠ¡ä¸€èµ·è¿æ¥ï¼Œä»¥æŒ‡å¯¼æ™ºèƒ½ä½“æ¨¡å‹çš„è¯•é”™ã€‚åœ¨æ¯ä¸ªè§„åˆ’æ­¥éª¤ä¸­ï¼Œä¸ºäº†é˜²æ­¢å¹»è§‰è¡Œä¸ºçš„å‡ºç°ï¼Œæœ¬æ–‡ä½¿ç”¨ç”Ÿæˆçš„çŠ¶æ€çŸ¥è¯†ä½œä¸ºæŸ¥è¯¢ï¼Œä»é¢„å…ˆæ„å»ºçš„çŠ¶æ€çŸ¥è¯†åº“ä¸­è¿›è¡ŒkNNæ£€ç´¢ã€‚ç„¶åï¼Œä½¿ç”¨æ¥è‡ªå…ˆå‰åŠ¨ä½œçš„çº¦æŸã€æ£€ç´¢åˆ°çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ä»¥åŠæ¥è‡ªæ™ºèƒ½ä½“æ¨¡å‹æ¦‚ç‡çš„åŠ æƒé¢„æµ‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸‰ä¸ªå¤æ‚çš„çœŸå®ä¸–ç•Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶ä¸ä¸‰ä¸ªæœ€å…ˆè¿›çš„å¼€æºLLMsè¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºå„ç§å¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼Œè¿›ä¸€æ­¥çš„åˆ†æç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„WKMå¯ä»¥æœ‰æ•ˆå‡å°‘æ— ç›®çš„çš„è¯•é”™å’Œå¹»è§‰è¡Œä¸ºï¼Œç”Ÿæˆçš„å®ä¾‹çº§ä»»åŠ¡çŸ¥è¯†å¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ï¼Œå¼±WKMå¯ä»¥æŒ‡å¯¼å¼ºæ™ºèƒ½ä½“æ¨¡å‹è§„åˆ’ï¼Œç»Ÿä¸€WKMè®­ç»ƒå…·æœ‰å¾ˆå¤§çš„å‘å±•æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºä¸–ç•ŒçŸ¥è¯†æ¨¡å‹çš„æ™ºèƒ½ä½“è§„åˆ’æ–¹æ³•ï¼Œä¸ºè§£å†³LLMsåœ¨äº¤äº’å¼è§„åˆ’ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ™ºèƒ½ä½“æ¨¡å‹çš„ç†è§£èƒ½åŠ›å’Œè§„åˆ’èƒ½åŠ›ï¼Œå¹¶å…·æœ‰å¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¼±WKMæŒ‡å¯¼å¼ºæ™ºèƒ½ä½“æ¨¡å‹è§„åˆ’çš„æ€æƒ³ï¼Œä¹Ÿä¸ºæ™ºèƒ½ä½“å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„èŒƒå¼ã€‚

## thread--thinking-deeper-with-recursive-spawning
### Abstract
Large language models (LLMs) have shown impressive capabilities across
diverse settings, but still struggle as the length and complexity of the
context increases. To address this challenge, we propose Thinking Recursively
and Dynamically (ThReaD). THREAD frames model generation as a thread of
execution that, based on the context, can run to completion or dynamically
spawn new threads. By spawning, threads can offload work (e.g., thinking,
retrieving information) to child threads, which only return tokens needed for
the parent thread to do its work. In effect, this enables the model to adapt,
as needed, the amount of intermediate work used to produce tokens. We apply
THREAD in the settings of LLM task solving and question answering, where the
dynamic threading allows the model to recursively decompose the given task or
question into progressively simpler sub-problems that can be solved by separate
child threads. We test THREAD, implemented using a few-shot learning approach,
on diverse benchmarks for agent tasks and data-grounded question answering.
THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these
benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new
benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD
outperforms existing frameworks by 10% to 50% absolute points with smaller
models, including Llama-3-8b and CodeLlama-7b.
### ğŸŒŸ è®ºæ–‡è§£è¯» | THREADï¼šé€’å½’åˆ†å‰ï¼Œæ·±åº¦æ€è€ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œå…¶æ€§èƒ½ä¼šä¸‹é™ã€‚è¿™æ˜¯å› ä¸ºLLMséœ€è¦å°†æ‰€æœ‰å¿…è¦çš„æ€è€ƒå’Œä¿¡æ¯æ£€ç´¢å·¥ä½œéƒ½å‹ç¼©åœ¨ä¸€æ¡ç®€æ´çš„ç”Ÿæˆè¯­å¥ä¸­ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦æ›´å¤šå·¥ä½œï¼ˆå¦‚æ€è€ƒã€æ£€ç´¢ä¿¡æ¯ã€åˆ†æç­‰ï¼‰çš„åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œé€’å½’åˆ†å‰å’ŒåŠ¨æ€æ€è€ƒâ€ï¼ˆThReaDï¼‰çš„é€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ¨¡å‹ç”Ÿæˆè§†ä¸ºä¸€ä¸ªæ‰§è¡Œçº¿ç¨‹ï¼Œè¯¥çº¿ç¨‹å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡ç‹¬ç«‹è¿è¡Œåˆ°å®Œæˆæˆ–åŠ¨æ€åœ°åˆ†å‰æˆæ–°çš„çº¿ç¨‹ã€‚é€šè¿‡åˆ†å‰ï¼Œçº¿ç¨‹å¯ä»¥å°†å·¥ä½œï¼ˆä¾‹å¦‚ï¼Œæ€è€ƒã€æ£€ç´¢ä¿¡æ¯ï¼‰å¸è½½åˆ°å­çº¿ç¨‹ï¼Œè€Œå­çº¿ç¨‹åªè¿”å›çˆ¶çº¿ç¨‹å®Œæˆå…¶å·¥ä½œæ‰€éœ€çš„ä¿¡æ¯ã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®éœ€è¦åŠ¨æ€åœ°è°ƒæ•´ç”¨äºç”Ÿæˆä¸åŒéƒ¨åˆ†æ ‡è®°åºåˆ—çš„ä¸­é—´å·¥ä½œé‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨LLMä»»åŠ¡è§£å†³å’Œæ•°æ®é©±åŠ¨é—®ç­”çš„è®¾ç½®ä¸­ï¼ŒTHREADé€šè¿‡åŠ¨æ€åˆ†å‰å…è®¸æ¨¡å‹é€’å½’åœ°å°†ç»™å®šçš„ä»»åŠ¡æˆ–é—®é¢˜åˆ†è§£ä¸ºé€æ­¥ç®€åŒ–çš„å­é—®é¢˜ï¼Œè¿™äº›å­é—®é¢˜å¯ä»¥ç”±å•ç‹¬çš„å­çº¿ç¨‹è§£å†³ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTHREADå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ALFWorldã€TextCraftå’ŒWebShopï¼Œä»¥åŠä¸¤ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•DataCommons QAå’ŒMIMIC-III ICU QAã€‚æ­¤å¤–ï¼ŒTHREADåœ¨å°æ¨¡å‹ä¸Šä¼˜äºç°æœ‰æ¡†æ¶ï¼ŒåŒ…æ‹¬Llama-3-8bå’ŒCodeLlama-7bï¼Œæ€§èƒ½æé«˜äº†10%åˆ°50%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
THREADæ¡†æ¶æä¾›äº†ä¸€ç§çµæ´»çš„æ–¹æ³•ï¼Œä½¿LLMsèƒ½å¤ŸåŠ¨æ€åœ°é€‚åº”å…¶å·¥ä½œé‡å’Œä¸­é—´è®¡ç®—æ­¥éª¤ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤æ‚ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒTHREADæ¡†æ¶çš„é€šç”¨æ€§ä½¿å…¶é€‚ç”¨äºå„ç§åœºæ™¯ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€åº”ç”¨ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºå„ç§ç›®çš„ï¼Œä¾‹å¦‚ç¼–å†™ç¨‹åºã€æ‰§è¡Œè®¡ç®—ã€å¢å¼ºæ•°æ®ã€ç”Ÿæˆæƒ³æ³•ã€æ£€ç´¢ä¿¡æ¯ã€ä¸ç¯å¢ƒäº¤äº’ã€æœºå™¨äººæ“ä½œç­‰ã€‚

## envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents
### Abstract
Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller RL agents learn useful skills that they are weak at? We propose EnvGen,
a novel framework to address this question. We first prompt an LLM to generate
training environments by giving it the task description and simulator
objectives that the agents should learn and then asking it to generate a set of
environment configurations (e.g., different terrains, items initially given to
agents, etc.). Next, we train a small RL agent in a mixture of the original and
LLM-generated environments. Then, we enable the LLM to continuously adapt the
generated environments to progressively improve the skills that the agent is
weak at, by providing feedback to the LLM in the form of the agent's
performance. We demonstrate the usefulness of EnvGen with comprehensive
experiments in Crafter and Heist environments. We find that a small RL agent
trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and
learns long-horizon tasks significantly faster. We also show that using an LLM
to adapt environments dynamically outperforms curriculum learning approaches
and how the environments are adapted to help improve RL agents' weaker skills
over time. Additionally, EnvGen is substantially more efficient as it only uses
a small number of LLM calls (e.g., 4 in total), whereas LLM agents require
thousands of calls. Lastly, we present detailed ablation studies for EnvGen
design choices.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EnvGenï¼šåˆ©ç”¨LLMç”Ÿæˆå’Œé€‚åº”ç¯å¢ƒï¼Œè®­ç»ƒå…·èº«æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„å…´èµ·ï¼Œå¦‚ä½•è®©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å¹¶æŒæ¡å„ç§æŠ€èƒ½æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶ä»»åŠ¡æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ™ºèƒ½ä½“è™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†è°ƒç”¨æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶EnvGenï¼Œæ—¨åœ¨åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥ç”Ÿæˆå’Œé€‚åº”è®­ç»ƒç¯å¢ƒï¼Œå¸®åŠ©å°å‹RLæ™ºèƒ½ä½“å­¦ä¹ å®ƒä»¬ä¸æ“…é•¿çš„æŠ€èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMç”Ÿæˆç¯å¢ƒ
EnvGené¦–å…ˆé€šè¿‡å‘LLMæä¾›ä»»åŠ¡æè¿°å’Œæ¨¡æ‹Ÿå™¨ç›®æ ‡ï¼Œè®©LLMç”Ÿæˆä¸€ç³»åˆ—ç¯å¢ƒé…ç½®ï¼Œä¾‹å¦‚ä¸åŒçš„åœ°å½¢ã€åˆå§‹ç‰©å“ç­‰ã€‚è¿™äº›ç¯å¢ƒå¯ä»¥å¹¶è¡Œè®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶å¿«é€Ÿå­¦ä¹ ä¸åŒçš„æŠ€èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMé€‚åº”ç¯å¢ƒ
EnvGené€šè¿‡å°†æ™ºèƒ½ä½“åœ¨åŸå§‹ç¯å¢ƒä¸­çš„è¡¨ç°åé¦ˆç»™LLMï¼Œè®©LLMä¸æ–­è°ƒæ•´ç”Ÿæˆçš„ç¯å¢ƒï¼Œä½¿å…¶æ›´åŠ ä¸“æ³¨äºæ™ºèƒ½ä½“ä¸æ“…é•¿çš„æŠ€èƒ½ã€‚è¿™ç§åŠ¨æ€é€‚åº”è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“é€æ­¥æé«˜å…¶æŠ€èƒ½æ°´å¹³ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Crafterå’ŒHeistæ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨EnvGenè®­ç»ƒçš„å°å‹RLæ™ºèƒ½ä½“åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†åŒ…æ‹¬GPT-4åœ¨å†…çš„SOTAæ–¹æ³•ï¼Œå¹¶ä¸”å­¦ä¹ é•¿æ—¶ä»»åŠ¡çš„é€Ÿåº¦æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼ŒEnvGençš„æ•ˆç‡ä¹Ÿè¿œé«˜äºç›´æ¥ä½¿ç”¨LLMä½œä¸ºæ™ºèƒ½ä½“çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒåªéœ€è¦å¾ˆå°‘çš„LLMè°ƒç”¨æ¬¡æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
EnvGenæä¾›äº†ä¸€ç§åˆ©ç”¨LLMæ¨ç†èƒ½åŠ›æ¥æé«˜RLæ™ºèƒ½ä½“æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§å¼€æ”¾ä¸–ç•Œæ¸¸æˆå’Œæ¨¡æ‹Ÿå™¨ï¼Œå¸®åŠ©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å¹¶æŒæ¡å„ç§æŠ€èƒ½ã€‚æ­¤å¤–ï¼ŒEnvGençš„åŠ¨æ€é€‚åº”æœºåˆ¶ä¹Ÿä¸ºRLæ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## odyssey--empowering-minecraft-agents-with-open-world-skills
### Abstract
Recent studies have delved into constructing generalist agents for open-world
environments like Minecraft. Despite the encouraging results, existing efforts
mainly focus on solving basic programmatic tasks, e.g., material collection and
tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond
task as the ultimate goal. This limitation stems from the narrowly defined set
of actions available to agents, requiring them to learn effective long-horizon
strategies from scratch. Consequently, discovering diverse gameplay
opportunities in the open world becomes challenging. In this work, we introduce
Odyssey, a new framework that empowers Large Language Model (LLM)-based agents
with open-world skills to explore the vast Minecraft world. Odyssey comprises
three key parts: (1) An interactive agent with an open-world skill library that
consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned
LLaMA-3 model trained on a large question-answering dataset with 390k+
instruction entries derived from the Minecraft Wiki. (3) A new agent capability
benchmark includes the long-term planning task, the dynamic-immediate planning
task, and the autonomous exploration task. Extensive experiments demonstrate
that the proposed Odyssey framework can effectively evaluate different
capabilities of LLM-based agents. All datasets, model weights, and code are
publicly available to motivate future research on more advanced autonomous
agent solutions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Odysseyï¼šèµ‹äºˆMinecraftæ™ºèƒ½ä½“å¼€æ”¾ä¸–ç•ŒæŠ€èƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œè®¸å¤šç ”ç©¶è‡´åŠ›äºæ„å»ºèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼ˆå¦‚Minecraftï¼‰æ‰§è¡Œä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“ã€‚å°½ç®¡å–å¾—äº†ä»¤äººé¼“èˆçš„æˆæœï¼Œä½†ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨è§£å†³åŸºæœ¬çš„ç¼–ç¨‹ä»»åŠ¡ï¼Œä¾‹å¦‚æ”¶é›†ææ–™å’Œåˆ¶ä½œå·¥å…·ï¼Œå¹¶å°†â€œè·å¾—é’»çŸ³â€ä»»åŠ¡è§†ä¸ºæœ€ç»ˆç›®æ ‡ã€‚è¿™ç§å±€é™æ€§æºäºæ™ºèƒ½ä½“å¯ç”¨çš„åŠ¨ä½œé›†è¿‡äºç‹­çª„ï¼Œéœ€è¦å®ƒä»¬ä»å¤´å¼€å§‹å­¦ä¹ æœ‰æ•ˆçš„é•¿æœŸç­–ç•¥ã€‚å› æ­¤ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œä¸­æ¢ç´¢å¤šæ ·åŒ–çš„æ¸¸æˆç©æ³•å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“
Odysseyæ¡†æ¶å¼€å‘äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’å¼æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“é…å¤‡äº†ä¸€ä¸ªå¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“ï¼Œå…¶ä¸­åŒ…å«40ä¸ªåŸºæœ¬æŠ€èƒ½å’Œ183ä¸ªç»„åˆæŠ€èƒ½ã€‚è¿™äº›æŠ€èƒ½æ¶µç›–äº†ä»èµ„æºæ”¶é›†åˆ°å·¥å…·åˆ¶ä½œï¼Œå†åˆ°æˆ˜æ–—å’Œæ¢ç´¢çš„å„ç§ä»»åŠ¡ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å·¥å…·æ¥åº”å¯¹å¼€æ”¾ä¸–ç•Œçš„æŒ‘æˆ˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¾®è°ƒLLaMA-3æ¨¡å‹
ä¸ºäº†æé«˜æ™ºèƒ½ä½“åœ¨Minecraftä¸­çš„æ€§èƒ½ï¼ŒOdysseyæ¡†æ¶ä½¿ç”¨æ¥è‡ªMinecraftç»´åŸºçš„å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†å¯¹LLaMA-3æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚é€šè¿‡ç”ŸæˆåŒ…å«390k+æŒ‡ä»¤æ¡ç›®çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼ŒOdysseyæ¡†æ¶æ˜¾è‘—æå‡äº†LLMæ¨¡å‹åœ¨Minecrafté¢†åŸŸçš„çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†
Odysseyæ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†ï¼ŒåŒ…æ‹¬é•¿æœŸè§„åˆ’ä»»åŠ¡ã€åŠ¨æ€å³æ—¶è§„åˆ’ä»»åŠ¡å’Œè‡ªä¸»æ¢ç´¢ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†Minecraftä¸­çš„å„ç§å¤æ‚åœºæ™¯ï¼Œå¹¶è¦æ±‚æ™ºèƒ½ä½“å±•ç°å‡ºå¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡è¿™äº›åŸºå‡†ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜å¯ä»¥å…¨é¢è¯„ä¼°æ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›ã€èµ„æºç®¡ç†èƒ½åŠ›ã€æŠ€èƒ½æ£€ç´¢èƒ½åŠ›ä»¥åŠè‡ªä¸»æ¢ç´¢èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒOdysseyæ¡†æ¶åœ¨åŸºæœ¬ç¼–ç¨‹ä»»åŠ¡å’Œæ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOdysseyæ¡†æ¶çš„æ™ºèƒ½ä½“åœ¨å®Œæˆä»»åŠ¡çš„é€Ÿåº¦ã€æˆåŠŸç‡å’Œèµ„æºåˆ©ç”¨ç‡æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒä¹Ÿè¯æ˜äº†å¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“å’ŒLLMè§„åˆ’å™¨å¯¹æ™ºèƒ½ä½“æ•´ä½“æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Odysseyæ¡†æ¶ä¸ºå¼€å‘å’Œç ”ç©¶å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **å¼€æ”¾ä¸–ç•ŒæŠ€èƒ½åº“**ï¼šä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„å·¥å…·å’Œç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿåº”å¯¹å„ç§å¤æ‚çš„ä»»åŠ¡å’ŒæŒ‘æˆ˜ã€‚
* **å¾®è°ƒLLMæ¨¡å‹**ï¼šé€šè¿‡é¢†åŸŸç‰¹å®šçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œæå‡LLMæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å‚¨å¤‡å’Œæ¨ç†èƒ½åŠ›ã€‚
* **æ™ºèƒ½ä½“èƒ½åŠ›åŸºå‡†**ï¼šä¸ºè¯„ä¼°æ™ºèƒ½ä½“çš„ä¸åŒèƒ½åŠ›æä¾›æ ‡å‡†åŒ–çš„æ¡†æ¶ï¼Œä¿ƒè¿›å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“ç ”ç©¶çš„è¿›å±•ã€‚

### ğŸŒŸ æ€»ç»“
Odysseyæ¡†æ¶ä¸ºå¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·æ¥æ¢ç´¢å’Œè¯„ä¼°æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚éšç€æœªæ¥ç ”ç©¶çš„ä¸æ–­æ·±å…¥ï¼ŒOdysseyæ¡†æ¶æœ‰æœ›æ¨åŠ¨å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½çš„é€šç”¨æ€§ç ”ç©¶åšå‡ºè´¡çŒ®ã€‚

## react-meets-actre--when-language-agents-enjoy-training-data-autonomy
### Abstract
Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotation or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ™ºèƒ½ä½“è‡ªä¸»è®­ç»ƒæ•°æ®æ ‡æ³¨æ¡†æ¶ï¼šA3T

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè¯­è¨€æ™ºèƒ½ä½“åœ¨è‡ªä¸»å†³ç­–æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ™ºèƒ½ä½“éœ€è¦å¤§é‡çš„å¤šæ­¥æ¨ç†å’ŒåŠ¨ä½œè½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®çš„æ”¶é›†é€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨æˆ–å®ç°å„ç§æç¤ºæ¡†æ¶ï¼Œè¿™é™åˆ¶äº†è®­ç»ƒçš„è§„æ¨¡å’Œæ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šA3Tæ¡†æ¶
æœ¬æ–‡æå‡ºäº†A3Tæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°è¯­è¨€æ™ºèƒ½ä½“è½¨è¿¹çš„è‡ªä¸»æ ‡æ³¨ã€‚A3Tçš„æ ¸å¿ƒæ˜¯ActReæç¤ºæ™ºèƒ½ä½“ï¼Œå®ƒèƒ½å¤Ÿè§£é‡Šä»»æ„åŠ¨ä½œçš„åŸå› ã€‚å½“éšæœºé‡‡æ ·ä¸€ä¸ªå¤–éƒ¨åŠ¨ä½œæ—¶ï¼ŒReActé£æ ¼çš„æ™ºèƒ½ä½“å¯ä»¥æŸ¥è¯¢ActReæ™ºèƒ½ä½“ä»¥è·å–è¯¥åŠ¨ä½œçš„æ–‡æœ¬ç†ç”±ã€‚ç„¶åï¼Œé€šè¿‡å°†ActReçš„æ¨ç†ç»“æœæ·»åŠ åˆ°é‡‡æ ·åŠ¨ä½œä¹‹å‰ï¼Œåˆæˆæ–°çš„è½¨è¿¹ã€‚è¿™æ ·ï¼ŒReActé£æ ¼çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œå¤šä¸ªè½¨è¿¹æ¥å¤„ç†å¤±è´¥çš„ä»»åŠ¡ï¼Œå¹¶é€‰æ‹©æˆåŠŸçš„è½¨è¿¹æ¥è¡¥å……å¤±è´¥çš„è½¨è¿¹ï¼Œè¿›è¡Œå¯¹æ¯”è‡ªæˆ‘è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯¹æ¯”è‡ªæˆ‘è®­ç»ƒ
A3Tæ¡†æ¶åˆ©ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€šè¿‡äºŒè¿›åˆ¶å¥–åŠ±æ¥å®ç°å¯¹æ¯”è‡ªæˆ‘è®­ç»ƒã€‚æ™ºèƒ½ä½“æ‰§è¡Œæ¯ä¸ªåˆæˆçš„è½¨è¿¹åï¼Œç¯å¢ƒä¼šæä¾›ç»ˆç«¯å¥–åŠ±ï¼Œè‡ªåŠ¨æ ‡æ³¨è½¨è¿¹çš„è´¨é‡ã€‚æˆåŠŸçš„è½¨è¿¹è¢«ä¿ç•™ä¸‹æ¥ï¼Œå¹¶ä¸å¤±è´¥çš„è½¨è¿¹ä¸€èµ·ç”¨äºå¯¹æ¯”è‡ªæˆ‘è®­ç»ƒã€‚éšç€æ–°æ™ºèƒ½ä½“çš„è®­ç»ƒï¼Œæ›´å¤šçš„è½¨è¿¹å¯ä»¥è¢«æ”¶é›†å’Œç§¯ç´¯ï¼Œå½¢æˆä¸€ä¸ªé—­ç¯ï¼Œä¿ƒè¿›è¯­è¨€æ™ºèƒ½ä½“çš„è‡ªæˆ‘æ”¹è¿›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨AlfWorldå’ŒWebShopä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒA3Tæ¡†æ¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨AlfWorldä¸­ï¼Œç»è¿‡A3Tè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­å®ç°äº†96%çš„ä¸€æ¬¡æ€§æˆåŠŸç‡ï¼Œå¹¶ä¸”åœ¨4æ¬¡è¿­ä»£åè¾¾åˆ°100%çš„æˆåŠŸç‡ã€‚åœ¨WebShopä¸­ï¼ŒA3Tæ™ºèƒ½ä½“çš„ä¸€æ¬¡æ€§æ€§èƒ½ä¸äººç±»å¹³å‡æ°´å¹³ç›¸å½“ï¼Œç»è¿‡4æ¬¡è¿­ä»£åï¼Œæ€§èƒ½æ¥è¿‘äººç±»ä¸“å®¶ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
A3Tæ¡†æ¶ä¸ºè¯­è¨€æ™ºèƒ½ä½“çš„è‡ªä¸»è®­ç»ƒæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªä¸»æ ‡æ³¨å’Œå¯¹æ¯”è‡ªæˆ‘è®­ç»ƒï¼Œå®ç°äº†æ™ºèƒ½ä½“çš„é—­ç¯è‡ªæˆ‘æ”¹è¿›ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œæé«˜è¯­è¨€æ™ºèƒ½ä½“çš„æ€§èƒ½å’Œè‡ªä¸»æ€§ã€‚

## learning-from-failure--integrating-negative-examples-when-fine-tuning-large-language-models-as-agents
### Abstract
Large language models (LLMs) have achieved success in acting as agents, which
interact with environments through tools such as search engines. However, LLMs
are optimized for language generation instead of tool use during training or
alignment, limiting their effectiveness as agents. To resolve this problem,
previous work has first collected interaction trajectories between LLMs and
environments, using only trajectories that successfully finished the task to
fine-tune smaller models, making fine-tuning data scarce and acquiring it both
difficult and costly. Discarding failed trajectories also leads to significant
wastage of data and resources and limits the possible optimization paths during
fine-tuning. In this paper, we argue that unsuccessful trajectories offer
valuable insights, and LLMs can learn from these trajectories through
appropriate quality control and fine-tuning strategies. By simply adding a
prefix or suffix that tells the model whether to generate a successful
trajectory during training, we improve model performance by a large margin on
mathematical reasoning, multi-hop question answering, and strategic question
answering tasks. We further analyze the inference results and find that our
method provides a better trade-off between valuable information and errors in
unsuccessful trajectories. To our knowledge, we are the first to demonstrate
the value of negative trajectories and their application in agent-tunning
scenarios. Our findings offer guidance for developing better agent-tuning
methods and low-resource data usage techniques.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»å¤±è´¥ä¸­å­¦ä¹ ï¼šåœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä½“æ—¶æ•´åˆè´Ÿé¢ç¤ºä¾‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½œä¸ºæ™ºèƒ½ä½“æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œå®ƒä»¬é€šè¿‡ä¸æœç´¢å¼•æ“ç­‰å·¥å…·è¿›è¡Œäº¤äº’æ¥ä¸ç¯å¢ƒäº’åŠ¨ã€‚ç„¶è€Œï¼ŒLLMsåœ¨è®­ç»ƒæˆ–å¯¹é½è¿‡ç¨‹ä¸­ä¸»è¦é’ˆå¯¹è¯­è¨€ç”Ÿæˆè¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯å·¥å…·ä½¿ç”¨ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ä½œä¸ºæ™ºèƒ½ä½“çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…ˆå‰çš„å·¥ä½œé¦–å…ˆæ”¶é›†äº†LLMså’Œç¯å¢ƒä¹‹é—´çš„äº¤äº’è½¨è¿¹ï¼Œä»…ä½¿ç”¨æˆåŠŸå®Œæˆä»»åŠ¡çš„è½¨è¿¹æ¥å¾®è°ƒè¾ƒå°çš„æ¨¡å‹ï¼Œè¿™ä½¿å¾—å¾®è°ƒæ•°æ®ç¨€ç¼ºï¼Œè·å–éš¾åº¦å¤§ä¸”æˆæœ¬é«˜ã€‚ä¸¢å¼ƒå¤±è´¥çš„è½¨è¿¹ä¹Ÿå¯¼è‡´äº†æ•°æ®èµ„æºçš„æµªè´¹ï¼Œå¹¶é™åˆ¶äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–è·¯å¾„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè´Ÿé¢æ„ŸçŸ¥è®­ç»ƒâ€ï¼ˆNATï¼‰çš„èŒƒå¼ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ å‰ç¼€æˆ–åç¼€æ¥å‘Šè¯‰æ¨¡å‹æ˜¯å¦ç”ŸæˆæˆåŠŸçš„è½¨è¿¹ï¼Œä»è€Œæ•´åˆè´Ÿé¢ç¤ºä¾‹ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—LLMsèƒ½å¤Ÿä»å¤±è´¥çš„è½¨è¿¹ä¸­å­¦ä¹ ï¼Œå¹¶é€šè¿‡é€‚å½“çš„è´¨é‡æ§åˆ¶ç­–ç•¥æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒNATåœ¨æ•°å­¦æ¨ç†ã€å¤šè·³é—®ç­”å’Œç­–ç•¥é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ†ææ¨ç†ç»“æœå‘ç°ï¼ŒNATåœ¨æœ‰ä»·å€¼çš„ä¿¡æ¯å’Œé”™è¯¯ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡é¦–æ¬¡è¯æ˜äº†è´Ÿé¢è½¨è¿¹çš„ä»·å€¼åŠå…¶åœ¨æ™ºèƒ½ä½“å¾®è°ƒåœºæ™¯ä¸­çš„åº”ç”¨ã€‚NATæ–¹æ³•ä¸ºå¼€å‘æ›´å¥½çš„æ™ºèƒ½ä½“å¾®è°ƒæ–¹æ³•å’Œä½èµ„æºæ•°æ®ä½¿ç”¨æŠ€æœ¯æä¾›äº†æŒ‡å¯¼ã€‚

## language-guided-exploration-for-rl-agents-in-text-environments
### Abstract
Real-world sequential decision making is characterized by sparse rewards and
large decision spaces, posing significant difficulty for experiential learning
systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large
Language Models (LLMs), with a wealth of world knowledge, can help RL agents
learn quickly and adapt to distribution shifts. In this work, we introduce
Language Guided Exploration (LGE) framework, which uses a pre-trained language
model (called GUIDE ) to provide decision-level guidance to an RL agent (called
EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging
text environment, LGE outperforms vanilla RL agents significantly and also
outperforms other sophisticated methods like Behaviour Cloning and Text
Decision Transformer.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€å¼•å¯¼æ¢ç´¢ï¼šæå‡æ–‡æœ¬ç¯å¢ƒä¸­RLæ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°å®ä¸–ç•Œçš„åºåˆ—å†³ç­–é—®é¢˜é€šå¸¸å…·æœ‰ç¨€ç–å¥–åŠ±å’Œå·¨å¤§çš„å†³ç­–ç©ºé—´ï¼Œè¿™å¯¹ç»éªŒå­¦ä¹ ç³»ç»Ÿï¼Œå¦‚ç™½æ¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“ï¼Œæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‹¥æœ‰ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå¯ä»¥å¸®åŠ©RLæ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å’Œé€‚åº”åˆ†å¸ƒå˜åŒ–ã€‚ç„¶è€Œï¼Œåœ¨æ–‡æœ¬ç¯å¢ƒä¸­ï¼ŒRLæ™ºèƒ½ä½“é¢ä¸´ç€å·¨å¤§çš„åŠ¨ä½œç©ºé—´å’Œç¨€ç–çš„å¥–åŠ±ä¿¡å·ï¼Œè¿™ä½¿å¾—éšæœºæ¢ç´¢çš„æ–¹æ³•å˜å¾—ä¸å……åˆ†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯­è¨€å¼•å¯¼æ¢ç´¢ï¼ˆLGEï¼‰æ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œè¯­è¨€å¼•å¯¼æ¢ç´¢â€ï¼ˆLGEï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†RLæ™ºèƒ½ä½“å’Œä¸€ä¸ªè¾…åŠ©æ¨¡å‹ï¼Œç§°ä¸ºâ€œGUIDEâ€ã€‚GUIDEä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥ä¸ºRLæ™ºèƒ½ä½“ï¼ˆç§°ä¸ºâ€œEXPLORERâ€ï¼‰æä¾›å†³ç­–çº§åˆ«çš„æŒ‡å¯¼ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æœ‰æ•ˆåŠ¨ä½œç©ºé—´çš„å¤§å°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯¹æ¯”å­¦ä¹ è®­ç»ƒGUIDEæ¨¡å‹
ä¸ºäº†ä½¿GUIDEèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«ç›¸å…³åŠ¨ä½œï¼Œæœ¬æ–‡ä½¿ç”¨äº†SimCSEï¼ˆä¸€ç§å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼‰æ¥å¾®è°ƒGUIDEæ¨¡å‹ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ŒGUIDEæ¨¡å‹èƒ½å¤Ÿå°†ä»»åŠ¡æè¿°å’ŒåŠ¨ä½œåµŒå…¥åˆ°å…±äº«çš„è¡¨ç¤ºç©ºé—´ä¸­ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ä»»åŠ¡å’ŒåŠ¨ä½œä¹‹é—´çš„ç›¸å…³æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ScienceWorldï¼ˆä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬ç¯å¢ƒï¼‰ä¸Šï¼ŒLGEæ¡†æ¶æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„RLæ™ºèƒ½ä½“ï¼Œå¹¶ä¸”ä¹Ÿä¼˜äºå…¶ä»–å¤æ‚çš„æ–¹æ³•ï¼Œå¦‚è¡Œä¸ºå…‹éš†å’Œæ–‡æœ¬å†³ç­–è½¬æ¢å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLGEæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘åŠ¨ä½œç©ºé—´çš„å¤§å°ï¼Œå¹¶å¸®åŠ©RLæ™ºèƒ½ä½“æ›´å¿«åœ°å­¦ä¹ å’Œé€‚åº”æ–°çš„ç¯å¢ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„LGEæ¡†æ¶ä¸ºåœ¨æ–‡æœ¬ç¯å¢ƒä¸­ä½¿ç”¨LLMsæ¥æŒ‡å¯¼RLæ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡ç»“åˆLLMsçš„çŸ¥è¯†å’ŒRLæ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›ï¼ŒLGEæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³æ–‡æœ¬ç¯å¢ƒä¸­RLæ™ºèƒ½ä½“é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ–¹æ³•ä¹Ÿä¸ºå…¶ä»–æ–‡æœ¬ç¯å¢ƒä¸­çš„RLæ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†å‚è€ƒã€‚

