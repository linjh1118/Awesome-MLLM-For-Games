# Paper List of geo_cite

- [22/05] **ARLO: A Framework for Automated Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2205.10416v1)] [[Code/Page]()] [[TLDR/Notes](#arlo--a-framework-for-automated-reinforcement-learning)]

- [23/07] **Challenges and Applications of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2307.10169v1)] [[Code/Page]()] [[TLDR/Notes](#challenges-and-applications-of-large-language-models)]

- [23/09] **Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf**  
[[Paper](http://arxiv.org/pdf/2309.04658v2)] [[Code/Page]()] [[TLDR/Notes](#exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf)]

- [23/12] **Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach**  
[[Paper](http://arxiv.org/pdf/2312.11865v3)] [[Code/Page]()] [[TLDR/Notes](#large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach)]

- [24/02] **PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2402.01118v3)] [[Code/Page](https://github.com/git-disl/PokeLLMon.)] [[TLDR/Notes](#pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models)]

- [23/02] **Cooperative Open-ended Learning Framework for Zero-shot Coordination**  
[[Paper](http://arxiv.org/pdf/2302.04831v4)] [[Code/Page](https://sites.google.com/view/cole-2023.)] [[TLDR/Notes](#cooperative-open-ended-learning-framework-for-zero-shot-coordination)]

- [23/02] **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**  
[[Paper](http://arxiv.org/pdf/2302.01560v3)] [[Code/Page](https://github.com/CraftJarvis/MC-Planner.)] [[TLDR/Notes](#describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents)]

- [23/02] **Guiding Pretraining in Reinforcement Learning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.06692v2)] [[Code/Page](https://github.com/yuqingd/ellm.)] [[TLDR/Notes](#guiding-pretraining-in-reinforcement-learning-with-large-language-models)]

- [23/12] **Creative Agents: Empowering Agents with Imagination for Creative Tasks**  
[[Paper](http://arxiv.org/pdf/2312.02519v1)] [[Code/Page](https://github.com/PKU-RL/Creative-Agents).)] [[TLDR/Notes](#creative-agents--empowering-agents-with-imagination-for-creative-tasks)]

- [23/10] **Octopus: Embodied Vision-Language Programmer from Environmental Feedback**  
[[Paper](http://arxiv.org/pdf/2310.08588v2)] [[Code/Page]()] [[TLDR/Notes](#octopus--embodied-vision-language-programmer-from-environmental-feedback)]

- [23/10] **Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds**  
[[Paper](http://arxiv.org/pdf/2310.13255v2)] [[Code/Page]()] [[TLDR/Notes](#steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds)]

- [22/03] **ScienceWorld: Is your Agent Smarter than a 5th Grader?**  
[[Paper](http://arxiv.org/pdf/2203.07540v2)] [[Code/Page]()] [[TLDR/Notes](#scienceworld--is-your-agent-smarter-than-a-5th-grader-)]

- [20/10] **ALFWorld: Aligning Text and Embodied Environments for Interactive Learning**  
[[Paper](http://arxiv.org/pdf/2010.03768v2)] [[Code/Page]()] [[TLDR/Notes](#alfworld--aligning-text-and-embodied-environments-for-interactive-learning)]

- [23/10] **Welfare Diplomacy: Benchmarking Language Model Cooperation**  
[[Paper](http://arxiv.org/pdf/2310.08901v1)] [[Code/Page](https://github.com/mukobi/welfare-diplomacy.)] [[TLDR/Notes](#welfare-diplomacy--benchmarking-language-model-cooperation)]

- [23/10] **Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation**  
[[Paper](http://arxiv.org/pdf/2310.01320v3)] [[Code/Page]()] [[TLDR/Notes](#avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation)]

- [24/01] **PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2401.06781v1)] [[Code/Page]()] [[TLDR/Notes](#pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model)]

- [23/09] **Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4**  
[[Paper](http://arxiv.org/pdf/2309.17277v3)] [[Code/Page]()] [[TLDR/Notes](#suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4)]

- [23/06] **ChessGPT: Bridging Policy Learning and Language Modeling**  
[[Paper](http://arxiv.org/pdf/2306.09200v2)] [[Code/Page](https://github.com/waterhorse1/ChessGPT.)] [[TLDR/Notes](#chessgpt--bridging-policy-learning-and-language-modeling)]

- [24/01] **CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents**  
[[Paper](http://arxiv.org/pdf/2401.10568v2)] [[Code/Page](https://github.com/bigai-ai/civrealm.)] [[TLDR/Notes](#civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents)]

- [20/09] **A reinforcement learning approach to hybrid control design**  
[[Paper](http://arxiv.org/pdf/2009.00821v1)] [[Code/Page]()] [[TLDR/Notes](#a-reinforcement-learning-approach-to-hybrid-control-design)]

- [22/03] **FPGA-extended General Purpose Computer Architecture**  
[[Paper](http://arxiv.org/pdf/2203.10359v3)] [[Code/Page]()] [[TLDR/Notes](#fpga-extended-general-purpose-computer-architecture)]

- [23/04] **Generative Agents: Interactive Simulacra of Human Behavior**  
[[Paper](http://arxiv.org/pdf/2304.03442v2)] [[Code/Page]()] [[TLDR/Notes](#generative-agents--interactive-simulacra-of-human-behavior)]

- [22/01] **Multi-Stage Episodic Control for Strategic Exploration in Text Games**  
[[Paper](http://arxiv.org/pdf/2201.01251v3)] [[Code/Page]()] [[TLDR/Notes](#multi-stage-episodic-control-for-strategic-exploration-in-text-games)]

- [24/02] **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**  
[[Paper](http://arxiv.org/pdf/2402.17574v3)] [[Code/Page]()] [[TLDR/Notes](#agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization)]

- [25/02] **Towards Automation of Cognitive Modeling using Large Language Models**  
[[Paper](http://arxiv.org/pdf/2502.00879v1)] [[Code/Page]()] [[TLDR/Notes](#towards-automation-of-cognitive-modeling-using-large-language-models)]

- [25/01] **RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing**  
[[Paper](http://arxiv.org/pdf/2501.18160v2)] [[Code/Page]()] [[TLDR/Notes](#repoaudit--an-autonomous-llm-agent-for-repository-level-code-auditing)]

- [23/12] **Empowering Working Memory for Large Language Model Agents**  
[[Paper](http://arxiv.org/pdf/2312.17259v2)] [[Code/Page]()] [[TLDR/Notes](#empowering-working-memory-for-large-language-model-agents)]

- [23/05] **Voyager: An Open-Ended Embodied Agent with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.16291v2)] [[Code/Page](https://voyager.minedojo.org/.)] [[TLDR/Notes](#voyager--an-open-ended-embodied-agent-with-large-language-models)]

- [23/05] **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**  
[[Paper](http://arxiv.org/pdf/2305.17144v2)] [[Code/Page](https://github.com/OpenGVLab/GITM.)] [[TLDR/Notes](#ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory)]

- [24/03] **Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation**  
[[Paper](http://arxiv.org/pdf/2403.08282v2)] [[Code/Page]()] [[TLDR/Notes](#hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation)]

- [16/06] **Human-Agent Decision-making: Combining Theory and Practice**  
[[Paper](http://arxiv.org/pdf/1606.07514v1)] [[Code/Page]()] [[TLDR/Notes](#human-agent-decision-making--combining-theory-and-practice)]

- [23/08] **CALYPSO: LLMs as Dungeon Masters' Assistants**  
[[Paper](http://arxiv.org/pdf/2308.07540v1)] [[Code/Page]()] [[TLDR/Notes](#calypso--llms-as-dungeon-masters--assistants)]

- [23/10] **RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.00746v3)] [[Code/Page]()] [[TLDR/Notes](#rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models)]

- [23/05] **PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits**  
[[Paper](http://arxiv.org/pdf/2305.02547v5)] [[Code/Page]()] [[TLDR/Notes](#personallm--investigating-the-ability-of-large-language-models-to-express-personality-traits)]

- [23/08] **ChatHaruhi: Reviving Anime Character in Reality via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2308.09597v1)] [[Code/Page](https://github.com/LC1332/Chat-Haruhi-Suzumiya)] [[TLDR/Notes](#chatharuhi--reviving-anime-character-in-reality-via-large-language-model)]

- [23/04] **LaMP: When Large Language Models Meet Personalization**  
[[Paper](http://arxiv.org/pdf/2304.11406v4)] [[Code/Page]()] [[TLDR/Notes](#lamp--when-large-language-models-meet-personalization)]

- [23/03] **CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society**  
[[Paper](http://arxiv.org/pdf/2303.17760v2)] [[Code/Page](https://github.com/camel-ai/camel.)] [[TLDR/Notes](#camel--communicative-agents-for--mind--exploration-of-large-language-model-society)]

- [23/09] **An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents**  
[[Paper](http://arxiv.org/pdf/2309.05076v1)] [[Code/Page]()] [[TLDR/Notes](#an-appraisal-based-chain-of-emotion-architecture-for-affective-language-model-game-agents)]

- [15/08] **Word sense disambiguation: a survey**  
[[Paper](http://arxiv.org/pdf/1508.01346v1)] [[Code/Page]()] [[TLDR/Notes](#word-sense-disambiguation--a-survey)]

- [23/03] **Self-Refine: Iterative Refinement with Self-Feedback**  
[[Paper](http://arxiv.org/pdf/2303.17651v2)] [[Code/Page]()] [[TLDR/Notes](#self-refine--iterative-refinement-with-self-feedback)]

- [23/08] **ProAgent: Building Proactive Cooperative Agents with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2308.11339v3)] [[Code/Page](https://pku-proagent.github.io}.)] [[TLDR/Notes](#proagent--building-proactive-cooperative-agents-with-large-language-models)]

- [23/03] **Language Models can Solve Computer Tasks**  
[[Paper](http://arxiv.org/pdf/2303.17491v3)] [[Code/Page](https://github.com/posgnu/rci-agent.)] [[TLDR/Notes](#language-models-can-solve-computer-tasks)]

- [25/00] **Quantizing Constrained Systems: New Perspectives**  
[[Paper](http://arxiv.org/pdf/quant-ph/9810037v1)] [[Code/Page]()] [[TLDR/Notes](#quantizing-constrained-systems--new-perspectives)]

- [23/10] **LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03903v2)] [[Code/Page](https://github.com/eric-ai-lab/llm_coordination}.)] [[TLDR/Notes](#llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models)]

- [23/09] **AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback**  
[[Paper](http://arxiv.org/pdf/2309.17176v3)] [[Code/Page]()] [[TLDR/Notes](#adarefiner--refining-decisions-of-language-models-with-adaptive-feedback)]

- [23/11] **ADaPT: As-Needed Decomposition and Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2311.05772v2)] [[Code/Page]()] [[TLDR/Notes](#adapt--as-needed-decomposition-and-planning-with-language-models)]

- [23/05] **SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks**  
[[Paper](http://arxiv.org/pdf/2305.17390v2)] [[Code/Page]()] [[TLDR/Notes](#swiftsage--a-generative-agent-with-fast-and-slow-thinking-for-complex-interactive-tasks)]

- [22/01] **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2201.07207v2)] [[Code/Page](https://huangwl18.github.io/language-planner)] [[TLDR/Notes](#language-models-as-zero-shot-planners--extracting-actionable-knowledge-for-embodied-agents)]

- [14/03] **Software Agents Interaction Algorithms in Virtual Learning Environment**  
[[Paper](http://arxiv.org/pdf/1403.5734v2)] [[Code/Page]()] [[TLDR/Notes](#software-agents-interaction-algorithms-in-virtual-learning-environment)]

- [23/09] **MindAgent: Emergent Gaming Interaction**  
[[Paper](http://arxiv.org/pdf/2309.09971v2)] [[Code/Page]()] [[TLDR/Notes](#mindagent--emergent-gaming-interaction)]

- [23/03] **Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks**  
[[Paper](http://arxiv.org/pdf/2303.16563v2)] [[Code/Page](https://sites.google.com/view/plan4mc.)] [[TLDR/Notes](#skill-reinforcement-learning-and-planning-for-open-world-long-horizon-tasks)]

- [19/10] **On the Utility of Learning about Humans for Human-AI Coordination**  
[[Paper](http://arxiv.org/pdf/1910.05789v2)] [[Code/Page](https://github.com/HumanCompatibleAI/overcooked_ai.)] [[TLDR/Notes](#on-the-utility-of-learning-about-humans-for-human-ai-coordination)]

- [17/05] **Text-based Adventures of the Golovin AI Agent**  
[[Paper](http://arxiv.org/pdf/1705.05637v1)] [[Code/Page]()] [[TLDR/Notes](#text-based-adventures-of-the-golovin-ai-agent)]

- [19/09] **Interactive Fiction Games: A Colossal Adventure**  
[[Paper](http://arxiv.org/pdf/1909.05398v3)] [[Code/Page]()] [[TLDR/Notes](#interactive-fiction-games--a-colossal-adventure)]

- [23/12] **LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination**  
[[Paper](http://arxiv.org/pdf/2312.15224v2)] [[Code/Page]()] [[TLDR/Notes](#llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination)]

- [23/02] **Q-Cogni: An Integrated Causal Reinforcement Learning Framework**  
[[Paper](http://arxiv.org/pdf/2302.13240v1)] [[Code/Page]()] [[TLDR/Notes](#q-cogni--an-integrated-causal-reinforcement-learning-framework)]

- [18/06] **VirtualHome: Simulating Household Activities via Programs**  
[[Paper](http://arxiv.org/pdf/1806.07011v1)] [[Code/Page]()] [[TLDR/Notes](#virtualhome--simulating-household-activities-via-programs)]

- [22/10] **Grounding Language with Visual Affordances over Unstructured Data**  
[[Paper](http://arxiv.org/pdf/2210.01911v3)] [[Code/Page](http://hulc2.cs.uni-freiburg.de)] [[TLDR/Notes](#grounding-language-with-visual-affordances-over-unstructured-data)]

- [23/11] **War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars**  
[[Paper](http://arxiv.org/pdf/2311.17227v2)] [[Code/Page](https://github.com/agiresearch/WarAgent}.)] [[TLDR/Notes](#war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars)]

- [23/05] **Language Models Meet World Models: Embodied Experiences Enhance Language Models**  
[[Paper](http://arxiv.org/pdf/2305.10626v3)] [[Code/Page]()] [[TLDR/Notes](#language-models-meet-world-models--embodied-experiences-enhance-language-models)]

- [23/10] **LLaMA Rider: Spurring Large Language Models to Explore the Open World**  
[[Paper](http://arxiv.org/pdf/2310.08922v1)] [[Code/Page]()] [[TLDR/Notes](#llama-rider--spurring-large-language-models-to-explore-the-open-world)]

- [23/06] **Large Sequence Models for Sequential Decision-Making: A Survey**  
[[Paper](http://arxiv.org/pdf/2306.13945v1)] [[Code/Page](https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5)] [[TLDR/Notes](#large-sequence-models-for-sequential-decision-making--a-survey)]

- [23/02] **Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2302.02662v4)] [[Code/Page]()] [[TLDR/Notes](#grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning)]

- [17/07] **Proximal Policy Optimization Algorithms**  
[[Paper](http://arxiv.org/pdf/1707.06347v2)] [[Code/Page]()] [[TLDR/Notes](#proximal-policy-optimization-algorithms)]

- [24/02] **Enhance Reasoning for Large Language Models in the Game Werewolf**  
[[Paper](http://arxiv.org/pdf/2402.02330v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-reasoning-for-large-language-models-in-the-game-werewolf)]

- [23/03] **Reward Design with Language Models**  
[[Paper](http://arxiv.org/pdf/2303.00001v1)] [[Code/Page]()] [[TLDR/Notes](#reward-design-with-language-models)]

- [23/10] **Motif: Intrinsic Motivation from Artificial Intelligence Feedback**  
[[Paper](http://arxiv.org/pdf/2310.00166v1)] [[Code/Page]()] [[TLDR/Notes](#motif--intrinsic-motivation-from-artificial-intelligence-feedback)]

- [23/12] **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**  
[[Paper](http://arxiv.org/pdf/2312.09238v2)] [[Code/Page]()] [[TLDR/Notes](#auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft)]

- [24/03] **Automated Feature Selection for Inverse Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2403.15079v1)] [[Code/Page](https://sites.google.com/view/feature4irl.)] [[TLDR/Notes](#automated-feature-selection-for-inverse-reinforcement-learning)]

- [23/10] **Eureka: Human-Level Reward Design via Coding Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.12931v2)] [[Code/Page]()] [[TLDR/Notes](#eureka--human-level-reward-design-via-coding-large-language-models)]

- [23/10] **Lyfe Agents: Generative agents for low-cost real-time social interactions**  
[[Paper](http://arxiv.org/pdf/2310.02172v1)] [[Code/Page]()] [[TLDR/Notes](#lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions)]

- [21/09] **Benchmarking the Spectrum of Agent Capabilities**  
[[Paper](http://arxiv.org/pdf/2109.06780v2)] [[Code/Page]()] [[TLDR/Notes](#benchmarking-the-spectrum-of-agent-capabilities)]

- [20/10] **Keep CALM and Explore: Language Models for Action Generation in Text-based Games**  
[[Paper](http://arxiv.org/pdf/2010.02903v1)] [[Code/Page](https://github.com/princeton-nlp/calm-textgame.)] [[TLDR/Notes](#keep-calm-and-explore--language-models-for-action-generation-in-text-based-games)]

- [23/04] **Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions**  
[[Paper](http://arxiv.org/pdf/2304.02868v1)] [[Code/Page]()] [[TLDR/Notes](#can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions)]

- [23/10] **Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game**  
[[Paper](http://arxiv.org/pdf/2310.18940v3)] [[Code/Page]()] [[TLDR/Notes](#language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game)]

- [23/10] **AvalonBench: Evaluating LLMs Playing the Game of Avalon**  
[[Paper](http://arxiv.org/pdf/2310.05036v3)] [[Code/Page]()] [[TLDR/Notes](#avalonbench--evaluating-llms-playing-the-game-of-avalon)]

- [23/12] **Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game**  
[[Paper](http://arxiv.org/pdf/2312.17515v1)] [[Code/Page]()] [[TLDR/Notes](#cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game)]

- [24/02] **What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents**  
[[Paper](http://arxiv.org/pdf/2402.13184v5)] [[Code/Page](https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.)] [[TLDR/Notes](#what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents)]

- [23/10] **Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.20499v2)] [[Code/Page]()] [[TLDR/Notes](#leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models)]

- [23/08] **GameEval: Evaluating LLMs on Conversational Games**  
[[Paper](http://arxiv.org/pdf/2308.10032v1)] [[Code/Page](https://github.com/GameEval/GameEval.)] [[TLDR/Notes](#gameeval--evaluating-llms-on-conversational-games)]

- [22/10] **Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task**  
[[Paper](http://arxiv.org/pdf/2210.13382v5)] [[Code/Page]()] [[TLDR/Notes](#emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task)]

- [23/08] **Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis**  
[[Paper](http://arxiv.org/pdf/2308.12466v2)] [[Code/Page]()] [[TLDR/Notes](#are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis)]

- [23/10] **Humanoid Agents: Platform for Simulating Human-like Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.05418v1)] [[Code/Page](https://www.humanoidagents.com/)] [[TLDR/Notes](#humanoid-agents--platform-for-simulating-human-like-generative-agents)]

- [18/06] **TextWorld: A Learning Environment for Text-based Games**  
[[Paper](http://arxiv.org/pdf/1806.11532v2)] [[Code/Page]()] [[TLDR/Notes](#textworld--a-learning-environment-for-text-based-games)]

- [18/06] **Counting to Explore and Generalize in Text-based Games**  
[[Paper](http://arxiv.org/pdf/1806.11525v2)] [[Code/Page]()] [[TLDR/Notes](#counting-to-explore-and-generalize-in-text-based-games)]

- [21/07] **Pre-trained Language Models as Prior Knowledge for Playing Text-based Games**  
[[Paper](http://arxiv.org/pdf/2107.08408v2)] [[Code/Page]()] [[TLDR/Notes](#pre-trained-language-models-as-prior-knowledge-for-playing-text-based-games)]

- [23/11] **Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games**  
[[Paper](http://arxiv.org/pdf/2311.07687v1)] [[Code/Page]()] [[TLDR/Notes](#language-model-in-the-loop--data-optimal-approach-to-learn-to-recommend-actions-in-text-games)]

- [20/10] **Keep CALM and Explore: Language Models for Action Generation in Text-based Games**  
[[Paper](http://arxiv.org/pdf/2010.02903v1)] [[Code/Page](https://github.com/princeton-nlp/calm-textgame.)] [[TLDR/Notes](#keep-calm-and-explore--language-models-for-action-generation-in-text-based-games)]

- [20/01] **Graph Constrained Reinforcement Learning for Natural Language Action Spaces**  
[[Paper](http://arxiv.org/pdf/2001.08837v1)] [[Code/Page]()] [[TLDR/Notes](#graph-constrained-reinforcement-learning-for-natural-language-action-spaces)]

- [20/10] **Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2010.02386v1)] [[Code/Page](https://github.com/XiaoxiaoGuo/rcdqn.)] [[TLDR/Notes](#interactive-fiction-game-playing-as-multi-paragraph-reading-comprehension-with-reinforcement-learning)]

- [19/10] **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**  
[[Paper](http://arxiv.org/pdf/1910.13461v1)] [[Code/Page]()] [[TLDR/Notes](#bart--denoising-sequence-to-sequence-pre-training-for-natural-language-generation--translation--and-comprehension)]

- [23/12] **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**  
[[Paper](http://arxiv.org/pdf/2312.00746v2)] [[Code/Page]()] [[TLDR/Notes](#deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games)]

- [21/02] **Chess as a Testbed for Language Model State Tracking**  
[[Paper](http://arxiv.org/pdf/2102.13249v2)] [[Code/Page]()] [[TLDR/Notes](#chess-as-a-testbed-for-language-model-state-tracking)]

- [17/01] **DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker**  
[[Paper](http://arxiv.org/pdf/1701.01724v3)] [[Code/Page]()] [[TLDR/Notes](#deepstack--expert-level-artificial-intelligence-in-no-limit-poker)]

- [20/03] **Too many cooks: Bayesian inference for coordinating multi-agent collaboration**  
[[Paper](http://arxiv.org/pdf/2003.11778v2)] [[Code/Page]()] [[TLDR/Notes](#too-many-cooks--bayesian-inference-for-coordinating-multi-agent-collaboration)]

- [20/10] **Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration**  
[[Paper](http://arxiv.org/pdf/2010.09890v2)] [[Code/Page]()] [[TLDR/Notes](#watch-and-help--a-challenge-for-social-perception-and-human-ai-collaboration)]

- [21/03] **The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI**  
[[Paper](http://arxiv.org/pdf/2103.14025v1)] [[Code/Page]()] [[TLDR/Notes](#the-threedworld-transport-challenge--a-visually-guided-task-and-motion-planning-benchmark-for-physically-realistic-embodied-ai)]

- [17/12] **AI2-THOR: An Interactive 3D Environment for Visual AI**  
[[Paper](http://arxiv.org/pdf/1712.05474v4)] [[Code/Page](http://ai2thor.allenai.org.)] [[TLDR/Notes](#ai2-thor--an-interactive-3d-environment-for-visual-ai)]

- [19/10] **Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments**  
[[Paper](http://arxiv.org/pdf/1910.14442v3)] [[Code/Page](https://sites.google.com/view/interactivegibsonenv))] [[TLDR/Notes](#interactive-gibson-benchmark-(igibson-0-5)--a-benchmark-for-interactive-navigation-in-cluttered-environments)]

- [19/04] **Habitat: A Platform for Embodied AI Research**  
[[Paper](http://arxiv.org/pdf/1904.01201v2)] [[Code/Page]()] [[TLDR/Notes](#habitat--a-platform-for-embodied-ai-research)]

- [21/08] **BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments**  
[[Paper](http://arxiv.org/pdf/2108.03332v1)] [[Code/Page]()] [[TLDR/Notes](#behavior--benchmark-for-everyday-household-activities-in-virtual--interactive--and-ecological-environments)]

- [24/03] **BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation**  
[[Paper](http://arxiv.org/pdf/2403.09227v1)] [[Code/Page](https://behavior.stanford.edu.)] [[TLDR/Notes](#behavior-1k--a-human-centered--embodied-ai-benchmark-with-1-000-everyday-activities-and-realistic-simulation)]

- [17/05] **Automatic Goal Generation for Reinforcement Learning Agents**  
[[Paper](http://arxiv.org/pdf/1705.06366v5)] [[Code/Page]()] [[TLDR/Notes](#automatic-goal-generation-for-reinforcement-learning-agents)]

- [23/05] **ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings**  
[[Paper](http://arxiv.org/pdf/2305.11554v4)] [[Code/Page]()] [[TLDR/Notes](#toolkengpt--augmenting-frozen-language-models-with-massive-tools-via-tool-embeddings)]

- [23/10] **MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.06500v1)] [[Code/Page]()] [[TLDR/Notes](#metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents)]

- [14/03] **Face Recognition Methods & Applications**  
[[Paper](http://arxiv.org/pdf/1403.0485v1)] [[Code/Page]()] [[TLDR/Notes](#face-recognition-methods-&-applications)]

- [24/03] **Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents**  
[[Paper](http://arxiv.org/pdf/2403.00690v1)] [[Code/Page]()] [[TLDR/Notes](#playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents)]

- [22/03] **Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation**  
[[Paper](http://arxiv.org/pdf/2203.06386v2)] [[Code/Page]()] [[TLDR/Notes](#enabling-multimodal-generation-on-clip-via-vision-language-knowledge-distillation)]

- [24/01] **Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering**  
[[Paper](http://arxiv.org/pdf/2401.08500v1)] [[Code/Page](https://github.com/Codium-ai/AlphaCodium)] [[TLDR/Notes](#code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering)]

- [24/06] **World Models with Hints of Large Language Models for Goal Achieving**  
[[Paper](http://arxiv.org/pdf/2406.07381v1)] [[Code/Page]()] [[TLDR/Notes](#world-models-with-hints-of-large-language-models-for-goal-achieving)]

- [24/07] **Enhancing Agent Learning through World Dynamics Modeling**  
[[Paper](http://arxiv.org/pdf/2407.17695v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-agent-learning-through-world-dynamics-modeling)]

- [17/10] **Watch Your Step: Learning Node Embeddings via Graph Attention**  
[[Paper](http://arxiv.org/pdf/1710.09599v2)] [[Code/Page]()] [[TLDR/Notes](#watch-your-step--learning-node-embeddings-via-graph-attention)]

- [23/12] **From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2312.08662v1)] [[Code/Page]()] [[TLDR/Notes](#from-centralized-to-self-supervised--pursuing-realistic-multi-agent-reinforcement-learning)]

- [24/05] **Agent Planning with World Knowledge Model**  
[[Paper](http://arxiv.org/pdf/2405.14205v4)] [[Code/Page](https://github.com/zjunlp/WKM.)] [[TLDR/Notes](#agent-planning-with-world-knowledge-model)]

- [24/05] **THREAD: Thinking Deeper with Recursive Spawning**  
[[Paper](http://arxiv.org/pdf/2405.17402v1)] [[Code/Page]()] [[TLDR/Notes](#thread--thinking-deeper-with-recursive-spawning)]

- [24/03] **EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2403.12014v2)] [[Code/Page]()] [[TLDR/Notes](#envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents)]

- [24/07] **Odyssey: Empowering Minecraft Agents with Open-World Skills**  
[[Paper](http://arxiv.org/pdf/2407.15325v2)] [[Code/Page]()] [[TLDR/Notes](#odyssey--empowering-minecraft-agents-with-open-world-skills)]

- [24/03] **ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy**  
[[Paper](http://arxiv.org/pdf/2403.14589v3)] [[Code/Page]()] [[TLDR/Notes](#react-meets-actre--when-language-agents-enjoy-training-data-autonomy)]

- [24/02] **Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents**  
[[Paper](http://arxiv.org/pdf/2402.11651v2)] [[Code/Page]()] [[TLDR/Notes](#learning-from-failure--integrating-negative-examples-when-fine-tuning-large-language-models-as-agents)]

- [24/03] **Language Guided Exploration for RL Agents in Text Environments**  
[[Paper](http://arxiv.org/pdf/2403.03141v1)] [[Code/Page]()] [[TLDR/Notes](#language-guided-exploration-for-rl-agents-in-text-environments)]



# TLDR/Notes
## the-arcade-learning-environment--an-evaluation-platform-for-general-agents
### Abstract
In this article we introduce the Arcade Learning Environment (ALE): both a
challenge problem and a platform and methodology for evaluating the development
of general, domain-independent AI technology. ALE provides an interface to
hundreds of Atari 2600 game environments, each one different, interesting, and
designed to be a challenge for human players. ALE presents significant research
challenges for reinforcement learning, model learning, model-based planning,
imitation learning, transfer learning, and intrinsic motivation. Most
importantly, it provides a rigorous testbed for evaluating and comparing
approaches to these problems. We illustrate the promise of ALE by developing
and benchmarking domain-independent agents designed using well-established AI
techniques for both reinforcement learning and planning. In doing so, we also
propose an evaluation methodology made possible by ALE, reporting empirical
results on over 55 different games. All of the software, including the
benchmark agents, is publicly available.
### 🌟 论文解读 | 通用智能体评估平台：Arcade Learning Environment

### 📌 背景痛点/本文动机
长期以来，人工智能领域的一个目标就是开发能够在各种任务和领域中通用的算法，而无需针对特定领域进行定制。然而，如何评估这些通用智能体的性能一直是一个挑战。传统的评估方法往往只在一小部分参数化的基准问题上进行，容易导致方法过拟合，并且忽略了将算法转移到新领域所需的专家努力。

### 🚀 核心方法
本文提出了Arcade Learning Environment (ALE)，这是一个新的挑战问题、平台和实验方法，用于评估通用智能体的性能。ALE提供了一个接口，可以与数百个Atari 2600游戏环境进行交互，每个游戏环境都不同、有趣，并且对人类玩家来说都是一种挑战。ALE为强化学习、模型学习、基于模型的规划、模仿学习、迁移学习和内在动机等领域的研究带来了重大挑战。最重要的是，它提供了一个严格的测试平台，用于评估和比较这些问题的解决方案。

### 📈 实验结果
本文通过开发和基准测试使用已建立的AI技术设计的域无关智能体，展示了ALE的潜力。这些智能体分别用于强化学习和规划。实验结果表明，虽然一些学习进展已经在Atari 2600游戏中实现，但仍有大量工作要做。不同的方法在不同的游戏中表现良好，但没有一种方法在所有游戏中都表现良好。一些游戏特别具有挑战性，例如需要高级规划的Montezuma’s Revenge游戏。

### 💬 可借鉴之处
ALE为评估和比较通用智能体的性能提供了一个强大的平台。它可以帮助研究人员开发更通用的算法，并推动人工智能领域的发展。此外，ALE还可以用于研究各种人工智能技术，例如强化学习、规划、模仿学习和迁移学习。

## arlo--a-framework-for-automated-reinforcement-learning
### Abstract
Automated Reinforcement Learning (AutoRL) is a relatively new area of
research that is gaining increasing attention. The objective of AutoRL consists
in easing the employment of Reinforcement Learning (RL) techniques for the
broader public by alleviating some of its main challenges, including data
collection, algorithm selection, and hyper-parameter tuning. In this work, we
propose a general and flexible framework, namely ARLO: Automated Reinforcement
Learning Optimizer, to construct automated pipelines for AutoRL. Based on this,
we propose a pipeline for offline and one for online RL, discussing the
components, interaction, and highlighting the difference between the two
settings. Furthermore, we provide a Python implementation of such pipelines,
released as an open-source library. Our implementation has been tested on an
illustrative LQG domain and on classic MuJoCo environments, showing the ability
to reach competitive performances requiring limited human intervention. We also
showcase the full pipeline on a realistic dam environment, automatically
performing the feature selection and the model generation tasks.
### 🌟 论文解读 | ARLO：自动化强化学习的框架

### 📌 背景痛点/本文动机
强化学习（RL）在解决复杂控制问题方面取得了显著成果，如自动驾驶、机器人操作和金融。然而，RL 技术的广泛应用受到数据收集、算法选择和超参数调整等挑战的限制。这使得 RL 技术难以被非专家用户使用。

### 🚀 核心方法
本文提出了 ARLO（自动化强化学习优化器）框架，旨在构建自动化管道以解决上述挑战。ARLO 框架包括两个主要管道：离线 RL 管道和在线 RL 管道。

#### 💡 创新点1：离线 RL 管道
离线 RL 管道包括数据生成、数据准备、特征工程、策略生成和策略评估五个阶段。该管道利用预先收集的数据集进行策略学习，无需与环境直接交互。

#### 💡 创新点2：在线 RL 管道
在线 RL 管道包括特征工程、策略生成和策略评估三个阶段。该管道直接与环境交互，实时收集数据并更新策略。

### 📈 实验结果
本文在 LQG 域和 MuJoCo 环境上测试了 ARLO 框架。结果表明，ARLO 框架能够自动选择最佳的超参数配置，并在有限的人工干预下达到竞争性的性能。

### 💬 可借鉴之处
ARLO 框架为自动化强化学习提供了一种通用的解决方案，具有以下可借鉴之处：

* **模块化设计**： ARLO 框架采用模块化设计，方便用户根据具体问题添加或修改管道中的阶段。
* **自动化超参数调整**： ARLO 框架能够自动选择最佳的超参数配置，无需人工干预。
* **开源实现**： ARLO 框架的开源实现为用户提供了方便的实验平台。

### 🌟 总结
ARLO 框架为自动化强化学习提供了一种通用的解决方案，有助于降低 RL 技术的使用门槛，使其更易于被非专家用户使用。

## challenges-and-applications-of-large-language-models
### Abstract
Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.
### 🌟 论文解读 | 大型语言模型：挑战与应用

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在短短几年内从无到有，迅速成为机器学习领域的主流。然而，由于该领域发展迅速，识别剩余的挑战和已经取得成功的应用领域变得困难。本文旨在建立一个系统的开放问题和应用成功案例的集合，以便机器学习研究人员可以更快地理解该领域的当前状态并提高效率。

### 🚀 核心方法
本文将LLMs的挑战分为三个主要类别：“设计”、“行为”和“科学”。

#### 设计挑战
* **数据集难以理解**： 预训练数据集的规模使得任何个人都无法彻底阅读或对包含的文档进行质量评估。
* **分词器依赖性**： 分词器引入了多个挑战，例如计算开销、语言依赖性、处理新词、固定词汇量、信息丢失和低人类可解释性。
* **高预训练成本**： 训练单个LLM可能需要数十万小时的计算时间，成本高达数百万美元，并消耗相当于几个典型美国家庭一年的能源量。
* **微调开销**： 预训练LLMs在大量和多样化的文本数据集上，可能导致结果模型难以明确捕获特定任务数据集的分布特性。
* **高推理延迟**： LLMs的推理延迟仍然很高，因为低并行性和大型内存占用。

#### 行为挑战
* **提示脆弱性**： 提示的语法和语义可以对模型的输出产生重大影响。
* **幻觉**： LLMs经常产生包含不准确信息的幻觉，这些信息可能难以检测，因为文本流畅自然。
* **行为错位**： LLMs经常生成与人类价值观或意图不一致的输出，这可能导致意外或负面的后果。

#### 科学挑战
* **过时的知识**： 预训练期间学习的事实性信息可能包含不准确或随时间过时的信息。
* **基于静态、人工编写的真实值评估**： 静态基准随着时间的推移变得不那么有用，因为模型的性能不断提高，而更新它们通常依赖于人工编写的真实值。
* **难以区分生成文本和人工编写文本**： 难以区分LLMs生成的文本和人工编写的文本。
* **无法通过规模解决的任务**： 一些任务似乎无法通过进一步扩展数据/模型规模来解决。
* **缺乏实验设计**： 许多论文没有进行受控实验（消融实验），这尤其成问题，因为它们具有很大的设计空间。
* **缺乏可重复性**： LLM研究的可重复性存在两个独特的问题：训练运行的重复性和闭源API服务模型的推理重复性。

### 📈 实验结果
本文没有提供具体的实验结果，而是对LLMs的挑战和应用进行了全面的概述。

### 💬 可借鉴之处
本文为LLMs的挑战和应用提供了一个全面的概述，为研究人员和实践者提供了宝贵的见解。本文还强调了LLMs的局限性，并提出了未来研究的方向。

## exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf
### Abstract
Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.
### 🌟 论文解读 | 探索大型语言模型在沟通游戏中的应用：以狼人杀为例的实证研究

### 📌 背景痛点/本文动机
沟通游戏，如狼人杀，是一种重要的研究工具，可以用来探索经济学、社会科学和人工智能等领域中的各种问题。然而，现有的AI代理在玩这类游戏时，要么对语言的使用有严格的限制，要么需要大量的人工标注数据，这使得AI代理在自然地玩这类游戏方面仍然面临挑战。

### 🚀 核心方法
本文提出了一种无需微调的大型语言模型（LLM）框架，用于玩沟通游戏，并以狼人杀为例进行了实证研究。该框架的核心方法包括：

💡 创新点1：历史信息收集
为了解决LLM的上下文长度限制问题，本文提出了一种从三个角度（新鲜度、信息量和完整性）收集历史信息的方法。具体来说，该方法包括：
- 收集最近的K条消息；
- 使用规则匹配和启发式指标选择最有信息量的N条消息；
- 通过回答问题的方式，从整个历史中提取更多信息。

💡 创新点2：经验学习
为了使LLM能够从经验中学习，本文提出了一种非参数学习机制。具体来说，该方法包括：
- 在每轮游戏结束后，收集所有玩家的响应、反思和得分，形成经验池；
- 在新的一轮游戏中，根据当前情况从经验池中检索最相关的经验，并从中提取建议，以指导LLM的推理。

### 📈 实验结果
实验结果表明，本文提出的框架能够有效地玩狼人杀游戏，并且能够从经验中学习，而无需微调LLM的参数。此外，实验中还观察到一些策略性行为，如信任、对抗、伪装和领导，这些行为并非预先编程，而是自发地从LLM中涌现出来的。

### 💬 可借鉴之处
本文提出的框架和方法为使用LLM玩沟通游戏提供了一种新的思路，并为进一步研究LLM在沟通游戏中的应用提供了有价值的参考。此外，本文提出的经验学习机制也可以应用于其他领域，例如对话系统和推荐系统。

## large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach
### Abstract
StarCraft II is a challenging benchmark for AI agents due to the necessity of
both precise micro level operations and strategic macro awareness. Previous
works, such as Alphastar and SCC, achieve impressive performance on tackling
StarCraft II , however, still exhibit deficiencies in long term strategic
planning and strategy interpretability. Emerging large language model (LLM)
agents, such as Voyage and MetaGPT, presents the immense potential in solving
intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs
on StarCraft II, a highly complex RTS game.To conveniently take full advantage
of LLMs` reasoning abilities, we first develop textual StratCraft II
environment, called TextStarCraft II, which LLM agent can interact. Secondly,
we propose a Chain of Summarization method, including single frame
summarization for processing raw observations and multi frame summarization for
analyzing game information, providing command recommendations, and generating
strategic decisions. Our experiment consists of two parts: first, an evaluation
by human experts, which includes assessing the LLMs`s mastery of StarCraft II
knowledge and the performance of LLM agents in the game; second, the in game
performance of LLM agents, encompassing aspects like win rate and the impact of
Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the
relevant knowledge and complex planning abilities needed to address StarCraft
II scenarios; 2. Human experts consider the performance of LLM agents to be
close to that of an average player who has played StarCraft II for eight years;
3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)
difficulty level. We have open sourced the code and released demo videos of LLM
agent playing StarCraft II.
### 🌟 论文解读 | 大型语言模型在星际争霸II中的表现：基准测试与摘要链方法

### 📌 背景痛点/本文动机
星际争霸II（StarCraft II）是一款极具挑战性的实时战略游戏，要求玩家在微观操作和宏观战略规划之间取得平衡。尽管之前的AI研究，如AlphaStar和SCC，在星际争霸II中取得了令人印象深刻的成果，但它们在长期战略规划和策略可解释性方面仍存在不足。随着大型语言模型（LLM）在解决复杂任务方面的潜力日益显现，本文旨在验证LLM在星际争霸II中的能力。

### 🚀 核心方法
💡 创新点1：TextStarCraft II环境
为了充分利用LLM的推理能力，本文开发了一个名为TextStarCraft II的文本环境，LLM代理可以与之交互。该环境将星际争霸II的复杂游戏动态转换为文本格式，允许LLM代理通过语言命令执行宏观战略行动。

💡 创新点2：摘要链（CoS）方法
本文提出了摘要链（CoS）方法，包括单帧摘要和多帧摘要。单帧摘要用于处理原始观察数据，而多帧摘要用于分析游戏信息，提供命令建议并生成战略决策。CoS方法通过信息压缩、推理加速和全局理解，增强了LLM代理在处理复杂信息和做出战略决策方面的能力。

### 📈 实验结果
实验结果表明，LLM具备解决星际争霸II场景所需的相关知识和复杂规划能力。人类专家认为，LLM代理在游戏中的表现接近于玩了八年星际争霸II的平均玩家。此外，LLM代理能够在Harder（Lv5）难度级别下击败内置AI。

### 💬 可借鉴之处
本文提出的TextStarCraft II环境和CoS方法为评估LLM在实时战略决策和长期规划方面的能力提供了一个新的基准。此外，本文的研究结果表明，LLM在解决复杂任务方面具有巨大潜力，并为未来在星际争霸II和其他实时战略游戏中的AI研究提供了有价值的见解。

## pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models
### Abstract
We introduce PokeLLMon, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pokemon
battles. The design of PokeLLMon incorporates three key strategies: (i)
In-context reinforcement learning that instantly consumes text-based feedback
derived from battles to iteratively refine the policy; (ii) Knowledge-augmented
generation that retrieves external knowledge to counteract hallucination and
enables the agent to act timely and properly; (iii) Consistent action
generation to mitigate the panic switching phenomenon when the agent faces a
powerful opponent and wants to elude the battle. We show that online battles
against human demonstrates PokeLLMon's human-like battle strategies and
just-in-time decision making, achieving 49% of win rate in the Ladder
competitions and 56% of win rate in the invited battles. Our implementation and
playable battle logs are available at: https://github.com/git-disl/PokeLLMon.
### 🌟 论文解读 | PokeLLMon：基于大型语言模型实现人类水平的宝可梦战斗AI

### 📌 背景痛点/本文动机
随着生成式AI和大型语言模型（LLMs）在自然语言处理（NLP）任务上的成功，人们开始探索LLMs如何自主地在物理世界中行动，将生成空间从文本扩展到行动，这被认为是追求通用人工智能（AGI）的关键范式。游戏是开发LLM-based代理与虚拟环境交互的合适测试平台。战术战斗游戏，如宝可梦战斗，因其状态和动作空间离散、回合制格式、战略性和复杂性，成为评估LLMs游戏能力的理想基准。

### 🚀 核心方法
💡 创新点1：上下文强化学习（ICRL）
为了解决LLMs在宝可梦战斗中出现的幻觉问题，论文提出了ICRL策略。ICRL利用战斗中即时生成的文本反馈作为“奖励”，在无需训练的情况下迭代优化动作生成策略。通过分析前一轮的行动和相应的文本反馈，代理能够不断调整其策略，从而更好地应对战斗中的变化。

💡 创新点2：知识增强生成（KAG）
为了进一步减少幻觉，论文引入了KAG策略。KAG通过检索外部知识，如类型优势/劣势关系和技能/能力效果，来增强生成过程。这些知识来源于宝可梦游戏中的宝可梦图鉴（Pokédex），它提供了关于宝可梦类型、技能和能力的详细信息。通过将外部知识添加到状态描述中，代理能够更准确地理解战斗情况，并做出更明智的决策。

💡 创新点3：一致性行动生成
为了解决代理在面对强大对手时出现的恐慌切换现象，论文提出了一致性行动生成策略。该策略通过多次独立生成行动并投票选出最一致的行动，来减少行动的不一致性。这种方法有助于代理在面对压力时保持冷静，避免过度思考和恐慌，从而做出更稳定的决策。

### 📈 实验结果
在线战斗结果表明，PokeLLMon在梯子比赛中取得了49%的胜率，在邀请比赛中取得了56%的胜率，展现出与人类玩家相当的比赛能力和策略。然而，PokeLLMon在面对人类玩家的消耗策略和欺骗技巧时也存在弱点，这表明未来需要进一步改进其长期规划和对手行为预测能力。

### 💬 可借鉴之处
PokeLLMon的设计和实现为LLMs在游戏领域的应用提供了新的思路。ICRL、KAG和一致性行动生成策略可以应用于其他游戏，帮助LLMs更好地理解和应对游戏中的挑战。此外，PokeLLMon的实验结果也揭示了LLMs在游戏中的优势和局限性，为未来研究和开发提供了有价值的参考。

## cooperative-open-ended-learning-framework-for-zero-shot-coordination
### Abstract
Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.
### 🌟 论文解读 | 零样本协调的协作开放学习框架

### 📌 背景痛点/本文动机
在合作人工智能（AI）中，零样本协调（ZSC）是一个重大挑战，即如何有效地与各种未见过的伙伴进行协调。传统的自我博弈（SP）方法虽然可以收敛到游戏的均衡状态，但往往形成特定的行为和惯例，难以适应与未见过的策略进行协调。为了克服SP的局限性，许多ZSC方法通过引入基于群体的训练（PBT）来促进策略或行为的多样性，以提高策略的适应性。然而，当优化固定的人口级目标时，群体内策略的协调能力可能不会得到提高，导致所谓的“合作不兼容性”。

### 🚀 核心方法
💡 创新点1：图形形式游戏（GFGs）和偏好图形形式游戏（P-GFGs）
本文提出了图形形式游戏（GFGs）和偏好图形形式游戏（P-GFGs）的概念，将合作任务重新表述为图形形式，以便更有效地评估和识别学习过程中的合作不兼容性。在GFGs中，策略被表征为节点，节点之间的边权重表示两个相关策略的平均合作收益。通过利用GFGs的子图，即偏好图形形式游戏（P-GFGs），可以进一步分析每个节点在图中的最大合作收益，从而评估合作不兼容性并识别无法协作的策略。

💡 创新点2：协作开放学习框架（COLE）
为了解决合作不兼容性问题，本文提出了协作开放学习框架（COLE），该框架从图形理论的角度构建了开放的目标，以评估和识别每个策略的合作能力。COLE框架通过迭代生成新的策略，这些策略近似于P-GFGs的经验游戏场景的最佳反应。此外，本文还提出了一种实用的算法COLESV，该算法结合了博弈论和图论的知识，并证明了该算法可以有效地克服合作不兼容性。

### 📈 实验结果
在Overcooked游戏环境中进行的实验结果表明，本文提出的方法在协调不同级别的伙伴时优于当前最先进的方法。此外，通过分析GFGs和P-GFGs，COLESV的学习过程揭示了该框架可以有效地克服合作不兼容性。

### 💬 可借鉴之处
本文提出的GFGs和P-GFGs的概念以及COLE框架为解决合作不兼容性问题提供了一种新的思路。此外，本文提出的COLESV算法在实际应用中具有很好的效果，可以为其他合作AI任务提供参考。

## describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents
### Abstract
We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
"$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.
### 🌟 论文解读 | 基于大型语言模型的交互式规划，助力开放世界多任务智能体

### 📌 背景痛点/本文动机
在开放世界环境中，多任务智能体面临着两大挑战：1）执行计划需要精确的多步推理，因为任务具有长期性；2）传统的规划器在排序复杂的计划中的并行子目标时，没有考虑当前智能体完成给定子任务的难易程度，导致生成的计划可能效率低下甚至不可行。

### 🚀 核心方法
本文提出了“描述、解释、规划和选择”（DEPS）的交互式规划方法，基于大型语言模型（LLMs）来解决上述挑战。

💡 创新点1：描述、解释和规划
DEPS 通过集成计划执行过程的描述和提供自我解释的反馈，更好地纠正初始 LLM 生成的计划中的错误。当遇到失败时，描述器会总结当前情况并发送给 LLM，LLM 作为解释器定位错误，然后根据描述器和解释器的信息更新计划。

💡 创新点2：目标选择器
DEPS 包含一个可训练的目标选择器模块，该模块根据完成每个并行候选子目标的估计步骤对它们进行排序，从而细化初始计划。选择器使用预测剩余时间步数来完成每个目标，并根据当前状态选择最接近的目标。

### 📈 实验结果
实验结果表明，DEPS 在开放世界环境（如 Minecraft）中取得了显著的成果，能够稳健地完成 70 多个任务，并且整体性能几乎翻倍。此外，DEPS 在非开放世界环境（如 ALFWorld 和桌面操作）中也表现出良好的效果。

### 💬 可借鉴之处
DEPS 的交互式规划方法为开放世界多任务智能体的开发提供了新的思路。通过集成描述、解释和规划，以及使用目标选择器，DEPS 能够生成更可靠和高效的计划，从而提高智能体在开放世界环境中的任务完成能力。

## guiding-pretraining-in-reinforcement-learning-with-large-language-models
### Abstract
Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.
### 🌟 论文解读 | 利用大型语言模型引导强化学习的预训练

### 📌 背景痛点/本文动机
强化学习算法在缺乏密集、良好形状的奖励函数时通常会遇到困难。内在动机探索方法通过奖励代理访问新颖状态或转换来解决这一限制，但在大多数发现的新颖性对下游任务无关紧要的大型环境中，这些方法提供的益处有限。本文提出了一种方法，该方法使用来自文本语料库的背景知识来塑造探索。这种方法称为ELLM（使用大型语言模型进行探索），它奖励代理实现由语言模型提出的与代理当前状态描述相关的目标。通过利用大规模语言模型预训练，ELLM引导代理朝着人类有意义且可能有用的行为发展，而无需人工干预。

### 🚀 核心方法
💡 创新点1：利用大型语言模型（LLM）的背景知识来塑造探索。LLM是概率文本模型，其预测编码了丰富的关于人类常识知识和文化习俗的信息。ELLM通过查询LLM来获取可能的目标，并奖励代理实现这些建议，从而引导探索朝着完成多样化、上下文敏感和人类有意义的目标。

💡 创新点2：使用LLM生成的目标作为内在奖励函数。ELLM通过测量LLM生成的目标与环境中代理转换的描述之间的语义相似性来计算奖励。当转换的描述与目标描述足够接近时，代理将获得与相似度成比例的奖励。

### 📈 实验结果
本文在Crafter游戏环境和Housekeep机器人模拟器中评估了ELLM。结果表明，ELLM训练的代理在预训练期间对常识行为的覆盖范围更好，并且在下游任务上的性能通常与基线相当或有所提高。

### 💬 可借鉴之处
本文提出的方法可以用于引导强化学习代理在缺乏外部定义的奖励的情况下学习有用的行为。通过利用LLM的背景知识，ELLM可以引导代理朝着人类有意义且可能有用的行为发展，从而提高强化学习算法的性能。此外，本文还探讨了LLM性能对提示选择、状态和转换描述的敏感性，并提出了改进LLM性能的潜在方法。

## creative-agents--empowering-agents-with-imagination-for-creative-tasks
### Abstract
We study building embodied agents for open-ended creative tasks. While
existing methods build instruction-following agents that can perform diverse
open-ended tasks, none of them demonstrates creativity -- the ability to give
novel and diverse task solutions implicit in the language instructions. This
limitation comes from their inability to convert abstract language instructions
into concrete task goals in the environment and perform long-horizon planning
for such complicated goals. Given the observation that humans perform creative
tasks with the help of imagination, we propose a class of solutions for
creative agents, where the controller is enhanced with an imaginator that
generates detailed imaginations of task outcomes conditioned on language
instructions. We introduce several approaches to implementing the components of
creative agents. We implement the imaginator with either a large language model
for textual imagination or a diffusion model for visual imagination. The
controller can either be a behavior-cloning policy learned from data or a
pre-trained foundation model generating executable codes in the environment. We
benchmark creative tasks with the challenging open-world game Minecraft, where
the agents are asked to create diverse buildings given free-form language
instructions. In addition, we propose novel evaluation metrics for open-ended
creative tasks utilizing GPT-4V, which holds many advantages over existing
metrics. We perform a detailed experimental analysis of creative agents,
showing that creative agents are the first AI agents accomplishing diverse
building creation in the survival mode of Minecraft. Our benchmark and models
are open-source for future research on creative agents
(https://github.com/PKU-RL/Creative-Agents).
### 🌟 论文解读 | 创意智能体：赋予智能体想象力以完成创意任务

### 📌 背景痛点/本文动机
现有的智能体大多只能执行预定义的任务，缺乏处理开放性任务的能力，尤其是那些需要创造力的任务。例如，在Minecraft游戏中，现有的智能体可以执行简单的指令，如“收集石头”或“建造一个雪人”，但无法完成更复杂的创意任务，如“建造一个砂岩宫殿”。这是因为它们无法将抽象的语言指令转换为具体的任务目标，并执行长期规划。

### 🚀 核心方法
本文提出了“创意智能体”的概念，通过赋予智能体想象力来处理开放性创意任务。创意智能体由两个主要部分组成：想象器和控制器。

💡 创新点1：想象器
想象器负责根据语言指令生成任务结果的详细想象。本文提出了两种实现想象器的方法：
- **文本想象**：使用大型语言模型（LLM）如GPT-4，通过Chain-of-Thought（CoT）技术生成文本想象。
- **视觉想象**：使用扩散模型如Stable Diffusion，生成与文本描述相符的视觉想象。

💡 创新点2：控制器
控制器负责将想象转换为可执行的计划。本文提出了两种实现控制器的方法：
- **行为克隆控制器**：从环境中学习行为克隆策略，将图像想象转换为建筑蓝图，并执行相应的动作。
- **基于GPT-4(V)的控制器**：利用GPT-4(V)的视觉语言理解和代码生成能力，直接生成可执行代码来完成任务。

### 📈 实验结果
本文在Minecraft游戏中进行了实验，结果表明，创意智能体能够根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。其中，使用扩散模型进行视觉想象，并结合GPT-4(V)进行控制的智能体表现最佳。

### 💬 可借鉴之处
本文提出的创意智能体框架为开放性学习和创意AI智能体研究提供了新的思路和方法。未来可以进一步探索如何提高行为克隆控制器的性能，以及如何增强智能体的创造力。

## octopus--embodied-vision-language-programmer-from-environmental-feedback
### Abstract
Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. When integrated into an embodied agent,
existing embodied VLM works either output detailed action sequences at the
manipulation level or only provide plans at an abstract level, leaving a gap
between high-level planning and real-world manipulation. To bridge this gap, we
introduce Octopus, an embodied vision-language programmer that uses executable
code generation as a medium to connect planning and manipulation. Octopus is
designed to 1) proficiently comprehend an agent's visual and textual task
objectives, 2) formulate intricate action sequences, and 3) generate executable
code. To facilitate Octopus model development, we introduce OctoVerse: a suite
of environments tailored for benchmarking vision-based code generators on a
wide spectrum of tasks, ranging from mundane daily chores in simulators to
sophisticated interactions in complex video games such as Grand Theft Auto
(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an
explorative agent that generates training data, i.e., action blueprints and
corresponding executable code. We also collect feedback that enables an
enhanced training scheme called Reinforcement Learning with Environmental
Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's
functionality and present compelling results, showing that the proposed RLEF
refines the agent's decision-making. By open-sourcing our simulation
environments, dataset, and model architecture, we aspire to ignite further
innovation and foster collaborative applications within the broader embodied AI
community.
### 🌟 论文解读 | Octopus：基于环境反馈的具身视觉-语言编程器

### 📌 背景痛点/本文动机
随着大型视觉-语言模型（VLMs）在多模态感知和推理方面取得显著进展，将它们集成到具身智能体中成为可能。然而，现有的具身VLM工作要么在操作层面输出详细的动作序列，要么仅在抽象层面提供计划，导致高级规划和现实世界操作之间存在差距。本文旨在解决这个问题，提出了一种名为Octopus的具身视觉-语言编程器，它使用可执行代码生成作为连接规划和操作的媒介。

### 🚀 核心方法
💡 创新点1：Octopus能够熟练地理解智能体的视觉和文本任务目标，制定复杂的动作序列，并生成可执行代码。
💡 创新点2：为了促进Octopus模型的发展，本文引入了OctoVerse，这是一套为在各种任务上评估基于视觉的代码生成器而量身定制的环境，包括从模拟器中的日常家务到复杂视频游戏（如GTA和Minecraft）中的复杂交互。
💡 创新点3：为了训练Octopus，本文利用GPT-4控制一个探索性智能体，生成训练数据，即动作蓝图和相应的可执行代码。同时，收集反馈，以实现一种称为环境反馈强化学习（RLEF）的增强训练方案。

### 📈 实验结果
通过一系列实验，本文展示了Octopus的功能，并展示了令人信服的结果，表明所提出的RLEF细化了智能体的决策。Octopus在各种场景中表现出强大的适应性，在任务规划、代码生成和执行方面优于现有模型。RLEF的集成进一步增强了Octopus的性能，展示了这种训练方法的有效性。

### 💬 可借鉴之处
本文提出的Octopus模型和OctoVerse环境为具身视觉-语言编程领域提供了新的思路和方法。Octopus模型的设计和训练过程可以借鉴到其他具身智能体中，以提高其在现实世界中的操作能力。OctoVerse环境可以用于评估和比较不同的具身视觉-语言编程模型，推动该领域的研究和发展。

## steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds
### Abstract
Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
### 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。

### 🚀 核心方法
为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。

### 📈 实验结果
实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：
1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。
2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。
3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。

### 💬 可借鉴之处
Steve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。

## scienceworld--is-your-agent-smarter-than-a-5th-grader-
### Abstract
We present ScienceWorld, a benchmark to test agents' scientific reasoning
abilities in a new interactive text environment at the level of a standard
elementary school science curriculum. Despite the transformer-based progress
seen in question-answering and scientific text processing, we find that current
models cannot reason about or explain learned science concepts in novel
contexts. For instance, models can easily answer what the conductivity of a
known material is but struggle when asked how they would conduct an experiment
in a grounded environment to find the conductivity of an unknown material. This
begs the question of whether current models are simply retrieving answers by
way of seeing a large number of similar examples or if they have learned to
reason about concepts in a reusable manner. We hypothesize that agents need to
be grounded in interactive environments to achieve such reasoning capabilities.
Our experiments provide empirical evidence supporting this hypothesis --
showing that a 1.5 million parameter agent trained interactively for 100k steps
outperforms a 11 billion parameter model statically trained for scientific
question-answering and reasoning from millions of expert demonstrations.
### 🌟 论文解读 | ScienceWorld：你的智能体比五年级学生更聪明吗？

### 📌 背景痛点/本文动机
随着大型语言模型在问答和科学文本处理方面的进步，研究人员开始质疑这些模型是否真正理解了它们所回答的问题，或者它们是否只是通过大量相似示例的检索来获取答案。为了解决这个问题，本文提出了ScienceWorld，一个用于测试智能体在标准小学科学课程水平上的科学推理能力的基准。

### 🚀 核心方法
💡 创新点1：构建ScienceWorld
ScienceWorld是一个复杂的交互式文本环境，具有模拟热力学、电路、化学反应和生物过程的引擎。它包含10个相互连接的位置，以及多达200种类型的对象，包括设备、仪器、动植物、电气元件、物质、容器和家具等。

💡 创新点2：实施30个基准任务
这些任务涵盖了10个主题，包括物质状态的变化、温度测量、电路、摩擦、物体分类、化学混合物、植物和传粉者、寿命、生命周期和孟德尔遗传学。每个任务都包含10到1400个参数变化，以防止过拟合并鼓励泛化。

💡 创新点3：评估现有智能体
本文评估了5个最先进的强化学习和语言模型智能体，包括DRRN、KG-A2C、CALM、BC和TDT。结果表明，这些智能体在需要使用科学领域知识的任务上表现不佳，平均得分仅为0.17。

### 📈 实验结果
实验结果表明，ScienceWorld是一个具有挑战性的基准，即使是现有的最先进智能体也无法很好地完成这些任务。此外，实验还发现，在交互式环境中进行交互式训练的智能体比在静态文本源上进行离线训练的大型语言模型更有效。

### 💬 可借鉴之处
本文提出的ScienceWorld基准为评估智能体的科学推理能力提供了一个新的平台。此外，本文的研究结果表明，交互式训练可以帮助智能体更好地学习科学知识和常识，并将其应用于实际任务中。

## alfworld--aligning-text-and-embodied-environments-for-interactive-learning
### Abstract
Given a simple request like Put a washed apple in the kitchen fridge, humans
can reason in purely abstract terms by imagining action sequences and scoring
their likelihood of success, prototypicality, and efficiency, all without
moving a muscle. Once we see the kitchen in question, we can update our
abstract plans to fit the scene. Embodied agents require the same abilities,
but existing work does not yet provide the infrastructure necessary for both
reasoning abstractly and executing concretely. We address this limitation by
introducing ALFWorld, a simulator that enables agents to learn abstract, text
based policies in TextWorld (C\^ot\'e et al., 2018) and then execute goals from
the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.
ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,
learned in TextWorld, corresponds directly to concrete, visually grounded
actions. In turn, as we demonstrate empirically, this fosters better agent
generalization than training only in the visually grounded environment.
BUTLER's simple, modular design factors the problem to allow researchers to
focus on models for improving every piece of the pipeline (language
understanding, planning, navigation, and visual scene understanding).
### 🌟 论文解读 | ALFWorld：文本与具身环境的交互式学习

### 📌 背景痛点/本文动机
在现实世界中，人类能够通过抽象思维来规划和执行任务，例如想象一系列动作并评估其成功可能性、典型性和效率。然而，现有的具身智能体缺乏这种抽象推理能力，无法在没有实际执行的情况下进行规划。本文旨在解决这一局限性，通过引入ALFWorld模拟器，使智能体能够在TextWorld中学习抽象的文本策略，并在ALFRED基准测试中执行具体的目标。

### 🚀 核心方法
💡 创新点1：ALFWorld环境
ALFWorld是一个模拟器，它将TextWorld和ALFRED环境相结合，使智能体能够在文本环境中学习抽象策略，并在视觉环境中执行具体动作。TextWorld提供文本观察和响应高级文本动作，而ALFRED则渲染高维图像并响应低级物理动作。

💡 创新点2：BUTLER架构
BUTLER是一个智能体，它首先在TextWorld中学习高级语言策略，然后将这些策略转移到ALFRED中的具身任务。BUTLER由三个模块组成：BUTLER::BRAIN（文本智能体）、BUTLER::VISION（语言状态估计器）和BUTLER::BODY（低级控制器）。BUTLER::BRAIN生成文本动作，BUTLER::VISION将这些动作转换为视觉环境中的低级动作，而BUTLER::BODY则执行这些动作。

### 📈 实验结果
实验结果表明，在TextWorld中预训练的BUTLER智能体能够在零样本的情况下泛化到ALFRED中的具身任务，并且比仅在视觉环境中从头开始训练的智能体具有更好的泛化能力。此外，TextWorld训练比具身训练更快，并且能够更好地处理执行失败和专家失败。

### 💬 可借鉴之处
本文提出的ALFWorld框架和BUTLER架构为具身智能体的学习和泛化提供了新的思路。ALFWorld环境可以用于训练智能体在文本环境中进行抽象推理，而BUTLER架构则可以用于将文本策略转移到具身任务。此外，本文的研究结果表明，在文本环境中预训练智能体可以显著提高其在具身环境中的泛化能力。

## welfare-diplomacy--benchmarking-language-model-cooperation
### Abstract
The growing capabilities and increasingly widespread deployment of AI systems
necessitate robust benchmarks for measuring their cooperative capabilities.
Unfortunately, most multi-agent benchmarks are either zero-sum or purely
cooperative, providing limited opportunities for such measurements. We
introduce a general-sum variant of the zero-sum board game Diplomacy -- called
Welfare Diplomacy -- in which players must balance investing in military
conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both
a clearer assessment of and stronger training incentives for cooperative
capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules
and implementing them via an open-source Diplomacy engine; (2) constructing
baseline agents using zero-shot prompted language models; and (3) conducting
experiments where we find that baselines using state-of-the-art models attain
high social welfare but are exploitable. Our work aims to promote societal
safety by aiding researchers in developing and assessing multi-agent AI
systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is
available at https://github.com/mukobi/welfare-diplomacy.
### 🌟 论文解读 | 探索AI合作能力：Welfare Diplomacy基准测试

### 📌 背景痛点/本文动机
随着AI系统能力的不断增强和应用的日益广泛，衡量其合作能力的需求也日益增长。然而，现有的多智能体基准测试要么是零和博弈，要么是纯粹的合作博弈，这限制了对其合作能力的评估。本文提出了一个名为Welfare Diplomacy的基准测试，旨在解决这一问题。

### 🚀 核心方法
💡 创新点1：Welfare Diplomacy规则
Welfare Diplomacy是对经典外交游戏（Diplomacy）的改进版本，玩家需要在军事征服和国内福利之间进行权衡。游戏结束后，玩家的总效用等于其累积的福利点数，而不是占领的供应中心数量。这种规则设计鼓励玩家进行合作，以实现更高的社会福祉。

💡 创新点2：零样本语言模型基准测试
本文使用零样本提示语言模型构建了Welfare Diplomacy的基线智能体，并使用GPT-4等最先进的模型进行了基准测试。实验结果表明，这些模型能够实现高社会福祉，但容易被其他玩家利用。

### 📈 实验结果
实验结果表明，Welfare Diplomacy能够有效地评估和训练AI系统的合作能力。与标准外交游戏相比，Welfare Diplomacy中的玩家参与冲突的频率更低，这表明Welfare Diplomacy能够更好地促进合作行为。

### 💬 可借鉴之处
本文提出的Welfare Diplomacy基准测试为评估和训练AI系统的合作能力提供了一个新的平台。该基准测试可以用于开发更安全、更可靠的AI系统，以应对现实世界中的复杂挑战。

### 🌟 总结
Welfare Diplomacy是一个很有前景的基准测试，可以帮助研究人员更好地理解和评估AI系统的合作能力。随着AI技术的不断发展，Welfare Diplomacy有望在促进AI合作能力方面发挥重要作用。

## avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation
### Abstract
Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
### 🌟 论文解读 | Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在作为智能体（LLM-as-Agent）方面取得了显著进展。然而，这些研究通常假设LLMs处理的信息始终是诚实的，忽略了人类社会中普遍存在的欺骗性或误导性信息以及AI生成内容中的潜在问题。这种假设使得LLMs容易受到恶意操纵，可能导致不良后果。本文旨在探索LLMs在欺骗性环境中的潜力，并提出了一个名为“递归沉思”（ReCon）的新框架，以增强LLMs识别和对抗欺骗性信息的能力。

### 🚀 核心方法
💡 创新点1：递归沉思框架（ReCon）
ReCon框架结合了“构思沉思”和“精炼沉思”两个认知过程。构思沉思产生初始思考和言语，而精炼沉思则进一步改进它们。此外，ReCon还引入了第一阶和第二阶视角转换，分别对应于这两个过程。第一阶视角转换允许LLM智能体从自己的角度推断他人的心理状态，而第二阶视角转换则涉及理解他人如何看待智能体的心理状态。

💡 创新点2：在Avalon游戏中测试ReCon
本文使用复杂的Avalon游戏作为测试平台，该游戏充满误导信息，需要复杂的逻辑推理。实验结果表明，ReCon能够有效地帮助LLMs识别和应对欺骗性信息，而无需额外的微调和数据。

### 📈 实验结果
实验结果表明，ReCon在Avalon游戏中表现出色，能够帮助LLMs识别和应对欺骗性信息，而无需额外的微调和数据。与基线方法相比，ReCon在多个维度上都有显著提升，包括隐蔽性、逻辑性、贡献度、说服力、信息量和创造力。

### 💬 可借鉴之处
本文提出的ReCon框架为LLMs在欺骗性环境中的应用提供了新的思路。ReCon框架的设计和实现方法可以借鉴到其他需要识别和对抗欺骗性信息的场景中。此外，本文还讨论了LLMs在安全性、推理能力、言语风格和格式等方面的局限性，为未来的研究提供了有价值的见解。

## pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model
### Abstract
Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.
### 🌟 论文解读 | PokerGPT：基于大型语言模型的轻量级多玩家德州扑克解决方案

### 📌 背景痛点/本文动机
德州扑克作为一种典型的非完美信息游戏（IIG），一直是人工智能研究的重要目标。然而，现有的解决方案，如DeepStack和Libratus，主要依赖于反事实后悔最小化（CFR）算法，该算法在计算成本和扩展性方面存在局限性。CFR算法的计算成本高昂，难以应用于多玩家游戏，且难以从现有模型中学习并应用于其他现实世界应用。

### 🚀 核心方法
本文提出了PokerGPT，一种基于轻量级大型语言模型（LLM）的端到端德州扑克解决方案。PokerGPT通过以下创新点克服了现有方法的局限性：

💡 创新点1：端到端学习框架
PokerGPT采用端到端学习框架，避免了复杂的特征工程和中间步骤。它仅需要简单的文本信息即可生成决策建议，实现了人机交互的便捷性。

💡 创新点2：轻量级LLM
PokerGPT基于轻量级LLM，具有更少的参数和更快的推理速度，同时训练时间也更短，实现了资源的有效利用。

💡 创新点3：高效的数据处理
PokerGPT采用数据清洗和提示工程技术，将真实游戏数据转换为可理解的文本提示，并使用强化学习人类反馈技术进行微调，提高了模型性能。

### 📈 实验结果
实验结果表明，PokerGPT在胜率、模型大小、训练时间和响应速度等方面均优于现有方法。此外，PokerGPT能够处理任意数量的玩家，并展现出出色的灵活性和适应性。

### 💬 可借鉴之处
PokerGPT的成功表明，LLM在解决IIG方面具有巨大潜力。其端到端学习框架、轻量级模型和高效的数据处理技术为其他IIG研究提供了可借鉴的经验。此外，PokerGPT的交互式特性使其在现实世界应用中具有广阔前景。

## suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4
### Abstract
Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.
### 🌟 论文解读 | 利用GPT-4的“心智理论”能力玩不完美信息游戏

### 📌 背景痛点/本文动机
在现实世界中，决策往往是在信息不完整或不确定的情况下进行的。然而，大多数现有的AI算法都是在完美信息游戏中训练的，即所有玩家都能看到所有信息。这限制了它们在现实世界中的应用。本文旨在探索如何利用大型语言模型（LLM）的知识和推理能力来处理不完美信息游戏，从而更好地模拟现实世界的决策过程。

### 🚀 核心方法
本文提出了一个名为Suspicion-Agent的创新型自主代理，它基于GPT-4，并利用其强大的知识检索和推理能力来玩不完美信息游戏。Suspicion-Agent的核心创新点包括：

💡 创新点1：利用GPT-4的“心智理论”（ToM）能力，即理解他人并有意影响他人行为的能力。这使得Suspicion-Agent能够预测对手的行为，并根据对手的行为调整自己的策略。

💡 创新点2：将游戏过程分解为多个子模块，如观察解释器、游戏模式分析和规划模块。每个模块都使用不同的提示来引导GPT-4执行特定的功能，从而实现更有效的决策。

### 📈 实验结果
在实验中，Suspicion-Agent在三个不同的不完美信息游戏中展示了其能力，并在Leduc Hold'em游戏中进行了定量评估。结果表明，Suspicion-Agent可以潜在地超越传统算法，而无需任何专门的训练或示例。

### 💬 可借鉴之处
本文提出的Suspicion-Agent框架为利用LLM在不完美信息游戏中进行决策提供了一个新的思路。其核心思想是将LLM的知识和推理能力与ToM能力相结合，从而实现更有效的决策。此外，本文还公开了所有与游戏相关的数据，这将有助于研究人员更好地理解LLM的能力，并开发更有效的模型。

## chessgpt--bridging-policy-learning-and-language-modeling
### Abstract
When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
https://github.com/waterhorse1/ChessGPT.
### 🌟 论文解读 | ChessGPT：策略学习与语言模型融合的桥梁

### 📌 背景痛点/本文动机
在解决决策任务时，人类通常依赖于两种关键信息来源：历史策略数据和自然语言形式的策略分析。然而，现有的研究大多只关注其中一种来源，要么是直接从历史回放中学习策略或价值函数，要么是利用语言语料库进行语言模型训练。本文认为，一个强大的自主代理应该同时利用这两种来源，因此提出了ChessGPT，一个通过整合国际象棋游戏中的数据来连接策略学习和语言模型的GPT模型。

### 🚀 核心方法
💡 创新点1：构建大规模游戏和语言数据集
本文构建了一个包含大量国际象棋游戏数据和自然语言数据的综合数据集，包括在线游戏回放、专业棋手比赛、计算机引擎游戏、棋盘游戏、棋盘游戏分析、棋盘游戏博客、棋盘游戏书籍、棋盘游戏论坛、棋盘游戏视频等。

💡 创新点2：提出ChessCLIP和ChessGPT模型
本文提出了两种模型，ChessCLIP和ChessGPT，利用上述数据集进行训练。ChessCLIP通过对比学习将策略和语言模态连接起来，而ChessGPT则通过因果语言模型进行策略学习。

💡 创新点3：提出全面的评估框架
本文设计了一个全面的评估框架，用于评估语言模型在国际象棋方面的能力，包括建模能力、价值判断能力和策略能力。

### 📈 实验结果
实验结果表明，ChessGPT模型在所有评估任务中都优于其他LLM基线模型，证明了模型和数据集的有效性。

### 💬 可借鉴之处
本文提出的ChessGPT模型和数据集为研究策略学习和语言模型之间的相互作用提供了新的思路和方法，并为开发更强大的自主代理提供了新的可能性。

## civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents
### Abstract
The generalization of decision-making agents encompasses two fundamental
elements: learning from past experiences and reasoning in novel contexts.
However, the predominant emphasis in most interactive environments is on
learning, often at the expense of complexity in reasoning. In this paper, we
introduce CivRealm, an environment inspired by the Civilization game.
Civilization's profound alignment with human history and society necessitates
sophisticated learning, while its ever-changing situations demand strong
reasoning to generalize. Particularly, CivRealm sets up an
imperfect-information general-sum game with a changing number of players; it
presents a plethora of complex features, challenging the agent to deal with
open-ended stochastic environments that require diplomacy and negotiation
skills. Within CivRealm, we provide interfaces for two typical agent types:
tensor-based agents that focus on learning, and language-based agents that
emphasize reasoning. To catalyze further research, we present initial results
for both paradigms. The canonical RL-based agents exhibit reasonable
performance in mini-games, whereas both RL- and LLM-based agents struggle to
make substantial progress in the full game. Overall, CivRealm stands as a
unique learning and reasoning challenge for decision-making agents. The code is
available at https://github.com/bigai-ai/civrealm.
### 🌟 论文解读 | CivRealm：决策智能体的学习与推理之旅

### 📌 背景痛点/本文动机
传统的决策智能体环境往往过于强调学习，而忽视了推理的重要性。然而，在实际应用中，智能体需要同时具备学习和推理能力，才能更好地适应复杂多变的环境。为了解决这个问题，本文提出了CivRealm，一个基于文明游戏的交互式环境，旨在推动决策智能体学习和推理能力的边界。

### 🚀 核心方法
💡 创新点1：CivRealm环境
CivRealm是一个基于文明游戏的开放环境，具有以下特点：
* **不完全信息**：玩家只能获取自己单位发现的信息，需要推理其他玩家的意图。
* **随机性**：游戏中有随机事件和危机，需要智能体灵活应对。
* **多目标**：有多种胜利路径，需要平衡经济、军事、外交、文化和科技发展。
* **动态空间**：游戏过程中，玩家的状态和动作空间会动态变化。
* **多智能体**：多个玩家可以互动，需要智能体进行合作和竞争。
* **动态玩家数量**：游戏过程中，玩家数量会发生变化，需要智能体适应新的环境。
* **通信**：玩家可以通过外交行动和自然语言聊天进行交流。

💡 创新点2：支持多种智能体类型
CivRealm提供了两种API接口，分别支持基于张量的智能体和基于语言的智能体：
* **基于张量的智能体**：使用深度强化学习等方法，擅长学习和模式识别。
* **基于语言的智能体**：使用大型语言模型等方法，擅长推理和自然语言处理。

💡 创新点3：基准方法和评估指标
本文提出了三种基准方法，包括：
* **基于张量的强化学习**：使用AlphaStar的架构，擅长处理复杂动态和海量信息。
* **BaseLang**：基于AutoGPT的架构，擅长推理和自然语言处理。
* **Mastaba**：基于BaseLang的架构，引入层次结构，提高全局视角和决策能力。

### 📈 实验结果
实验结果表明，基于张量的强化学习在迷你游戏中表现良好，但在完整游戏中仍然存在局限性，例如短视策略和难以处理稀疏奖励。基于语言的智能体在完整游戏中表现更佳，但仍然需要改进推理能力和全局视角。

### 💬 可借鉴之处
CivRealm为决策智能体的学习和推理能力提供了一个独特的挑战平台，可以用于评估和改进智能体的性能。此外，CivRealm还可以用于研究人类社会的动态、历史事件的结果和未来的社会轨迹。

## a-reinforcement-learning-approach-to-hybrid-control-design
### Abstract
In this paper we design hybrid control policies for hybrid systems whose
mathematical models are unknown. Our contributions are threefold. First, we
propose a framework for modelling the hybrid control design problem as a single
Markov Decision Process (MDP). This result facilitates the application of
off-the-shelf algorithms from Reinforcement Learning (RL) literature towards
designing optimal control policies. Second, we model a set of benchmark
examples of hybrid control design problem in the proposed MDP framework. Third,
we adapt the recently proposed Proximal Policy Optimisation (PPO) algorithm for
the hybrid action space and apply it to the above set of problems. It is
observed that in each case the algorithm converges and finds the optimal
policy.
### 🌟 论文解读 | 基于强化学习的混合控制系统设计新方法

### 📌 背景痛点/本文动机
混合动力系统在现实世界中广泛存在，如交通管理、化学过程控制、通信网络、嵌入式控制、发动机控制和机器人等。然而，设计能够确保系统良好性能的混合控制策略一直是一个挑战，尤其是在系统数学模型未知的情况下。本文旨在解决这一问题，提出了一种基于强化学习（RL）的混合控制策略设计方法。

### 🚀 核心方法
💡 创新点1：将混合控制设计问题建模为马尔可夫决策过程（MDP）
本文首先提出了一种框架，将混合控制设计问题建模为单个MDP。这使得可以直接应用RL文献中的现成算法来设计最优控制策略。

💡 创新点2：在MDP框架中建模基准混合控制设计问题
本文在提出的MDP框架中建模了一系列基准混合控制设计问题，包括四档汽车、钢退火过程、热水器等。

💡 创新点3：将PPO算法应用于混合动作空间
本文将最近提出的Proximal Policy Optimisation（PPO）算法进行了调整，以适应混合动作空间，并将其应用于上述问题。实验结果表明，在每种情况下，算法都能收敛并找到最优策略。

### 📈 实验结果
本文在多个基准问题上进行了实验，包括四档汽车、钢退火过程、热水器等。实验结果表明，PPO算法能够有效地找到最优混合控制策略，并在各种情况下都能收敛。

### 💬 可借鉴之处
本文提出的基于RL的混合控制策略设计方法具有以下可借鉴之处：

*   **模型无关性**：该方法不依赖于系统的数学模型，适用于模型未知或难以建模的混合系统。
*   **通用性**：该方法可以应用于各种类型的混合系统，包括具有不同“跳跃”行为的系统。
*   **有效性**：实验结果表明，该方法能够有效地找到最优混合控制策略，并在各种情况下都能收敛。

### 🌟 总结
本文提出了一种基于RL的混合控制策略设计方法，该方法具有模型无关性、通用性和有效性等优点。该方法为混合控制设计提供了一种新的思路，并为解决现实世界中的混合系统控制问题提供了新的工具。

## fpga-extended-general-purpose-computer-architecture
### Abstract
This paper introduces a computer architecture, where part of the instruction
set architecture (ISA) is implemented on small highly-integrated
field-programmable gate arrays (FPGAs). Small FPGAs inside a general-purpose
processor (CPU) can be used effectively to implement custom or standardised
instructions. Our proposed architecture directly address related challenges for
high-end CPUs, where such highly-integrated FPGAs would have the highest
impact, such as on main memory bandwidth. This also enables
software-transparent context-switching. The simulation-based evaluation of a
dynamically reconfigurable core shows promising results approaching the
performance of an equivalent core with all enabled instructions. Finally, the
feasibility of adopting the proposed architecture in today's CPUs is studied
through the prototyping of fast-reconfigurable FPGAs and studying the miss
behaviour of opcodes.
### 🌟 论文解读 | FPGA扩展通用计算机架构：提升性能的新思路

### 📌 背景痛点/本文动机
随着计算需求的日益增长，通用处理器（CPU）在性能上逐渐难以满足特定应用的需求。尽管专用处理器如GPU、FPGA和ASIC等在特定任务上表现出色，但它们通常需要复杂的编程模型和昂贵的部署成本。此外，现有的通用处理器架构在扩展指令集时面临着硬件复杂性和功耗效率的挑战。

### 🚀 核心方法
本文提出了一种名为“FPGA扩展修改哈佛架构”的新型计算机架构，旨在通过在通用处理器内部集成小型高度集成的FPGA来提升性能。主要创新点包括：

💡 创新点1：将部分指令集架构（ISA）实现于FPGA上，使得CPU能够有效地执行自定义或标准化的指令。
💡 创新点2：引入指令消歧器单元，用于处理指令解码过程中的请求，并根据指令操作码查找相应的FPGA指令实现。
💡 创新点3：设计了一个独立的位流缓存，用于存储FPGA指令的位流，从而提高可重构核心的性能。
💡 创新点4：通过模拟评估，验证了动态可重构核心的性能，并研究了在当前CPU中采用该架构的可行性。

### 📈 实验结果
实验结果表明，动态可重构核心的性能接近于具有所有启用指令的等效核心。此外，通过原型设计和研究操作码的缺失行为，证明了在当前CPU中采用该架构的可行性。

### 💬 可借鉴之处
本文提出的FPGA扩展通用计算机架构为提升CPU性能提供了一种新的思路。通过将部分指令集实现于FPGA上，可以有效地执行自定义或标准化的指令，从而提高性能。此外，该架构还支持软件透明的上下文切换，使得操作系统可以提供ISA扩展，而硬件则根据需求动态地获取相应的位流。这些创新点为未来CPU架构的设计提供了重要的参考价值。

## generative-agents--interactive-simulacra-of-human-behavior
### Abstract
Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
### 🌟 论文解读 | 生成式智能体：模拟人类行为的交互式模拟

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，人们对于能够模拟人类行为的智能体产生了浓厚的兴趣。这些智能体可以应用于各种场景，例如沉浸式环境、人际沟通演练空间、原型设计工具等。然而，要创建一个能够长期保持一致性和可信度的智能体仍然是一个挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：生成式智能体
本文提出了生成式智能体的概念，即利用生成模型模拟可信的人类行为。这些智能体能够进行日常活动，如起床、做饭、上班等，并能够形成自己的观点、与他人互动、发起对话等。

💡 创新点2：智能体架构
为了实现生成式智能体，本文提出了一种新的架构，该架构扩展了大型语言模型，使其能够存储智能体的经验记录，并将这些记忆随着时间的推移合成更高层次的反思，并动态地检索它们来规划行为。

### 📈 实验结果
本文通过在模拟环境中创建一个由25个智能体组成的小镇，展示了生成式智能体的潜力。实验结果表明，这些智能体能够产生可信的个体和群体行为，例如，在用户指定一个智能体想要举办情人节派对的情况下，智能体能够自主地邀请其他智能体参加派对，并协调在正确的时间一起到达派对地点。

### 💬 可借鉴之处
本文提出的生成式智能体架构为创建可信的人类行为模拟提供了新的思路。该架构可以应用于各种领域，例如角色扮演、社交原型设计、虚拟世界和游戏等。此外，本文还讨论了生成式智能体在交互式系统中的应用机会、伦理和社会风险。

## multi-stage-episodic-control-for-strategic-exploration-in-text-games
### Abstract
Text adventure games present unique challenges to reinforcement learning
methods due to their combinatorially large action spaces and sparse rewards.
The interplay of these two factors is particularly demanding because large
action spaces require extensive exploration, while sparse rewards provide
limited feedback. This work proposes to tackle the explore-vs-exploit dilemma
using a multi-stage approach that explicitly disentangles these two strategies
within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins
each episode using an exploitation policy that imitates a set of promising
trajectories from the past, and then switches over to an exploration policy
aimed at discovering novel actions that lead to unseen state spaces. This
policy decomposition allows us to combine global decisions about which parts of
the game space to return to with curiosity-based local exploration in that
space, motivated by how a human may approach these games. Our method
significantly outperforms prior approaches by 27% and 11% average normalized
score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in
both deterministic and stochastic settings, respectively. On the game of Zork1,
in particular, XTX obtains a score of 103, more than a 2x improvement over
prior methods, and pushes past several known bottlenecks in the game that have
plagued previous state-of-the-art methods.
### 🌟 论文解读 | 多阶段策略探索：文本游戏中的强化学习新突破

### 📌 背景痛点/本文动机
文本冒险游戏为强化学习算法提供了独特的测试平台，但同时也带来了巨大的挑战。这些游戏具有组合爆炸式的动作空间和稀疏的奖励，这使得探索与利用之间的平衡变得尤为困难。大动作空间需要广泛的探索，而稀疏的奖励则提供了有限的反馈。现有的方法通常采用单一策略和动作选择策略，难以在探索与利用之间找到合适的平衡点。

### 🚀 核心方法
本文提出了名为 eXploit-Then-eXplore (XTX) 的多阶段控制算法，该算法在每个回合中明确地将探索和利用策略分离。XTX 算法包含两个阶段：

* **利用阶段**：该阶段使用一个模仿过去成功轨迹的策略，使智能体能够返回到游戏空间中已探索的前沿状态。
* **探索阶段**：该阶段使用一个基于好奇心驱动的策略，旨在发现新颖的动作，并探索未知的游戏状态空间。

这种策略分解允许智能体结合全局决策和局部探索，从而更好地应对稀疏奖励和动态动作空间的挑战。

### 📈 实验结果
在 Jericho 基准测试的 12 个游戏中，XTX 算法在确定性和随机性设置下分别比现有方法提高了 27% 和 11% 的平均归一化分数。特别是在 Zork1 游戏中，XTX 算法取得了 103 分的成绩，比现有方法提高了 2 倍以上，并克服了游戏中一些已知的瓶颈。

### 💬 可借鉴之处
* **多阶段策略**：将探索和利用策略分离，可以更好地平衡两者之间的关系，从而提高学习效率。
* **模仿学习**：利用过去成功的经验来指导智能体的行为，可以加快学习速度。
* **好奇心驱动探索**：通过奖励新颖的动作，可以鼓励智能体探索未知的游戏状态空间。
* **混合策略**：使用混合策略可以提供更细粒度的控制，从而更好地适应不同的游戏环境。

### 🌟 总结
XTX 算法为文本冒险游戏中的强化学习提供了一种新的思路，通过多阶段策略和混合策略，有效地解决了探索与利用之间的平衡问题，并在实际游戏中取得了显著的性能提升。

## agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization
### Abstract
Large Language Models (LLMs) exhibit robust problem-solving capabilities for
diverse tasks. However, most LLM-based agents are designed as specific task
solvers with sophisticated prompt engineering, rather than agents capable of
learning and evolving through interactions. These task solvers necessitate
manually crafted prompts to inform task rules and regulate LLM behaviors,
inherently incapacitating to address complex dynamic scenarios e.g., large
interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent
with Policy-level Reflection and Optimization that can learn a wealth of
expertise from interactive experiences and progressively elevate its behavioral
policy. Specifically, it involves a dynamic belief generation and reflection
process for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
### 🌟 论文解读 | Agent-Pro：基于策略级反思和优化的学习进化

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在解决各种任务方面表现出强大的能力，但大多数基于LLMs的智能体都是为特定任务设计的，需要复杂的提示工程来告知任务规则和调节LLMs的行为。这使得它们难以应对复杂动态的场景，例如大型互动游戏。本文提出了一种名为Agent-Pro的LLM-based智能体，它具有策略级反思和优化能力，可以从互动经验中学习大量专业知识，并逐步提升其行为策略。

### 🚀 核心方法
💡 创新点1：策略级反思和优化
Agent-Pro通过策略级反思和优化来学习进化。它不仅反思过去的轨迹和信念，还通过深度优先搜索来优化策略，确保策略收益的持续提升。

💡 创新点2：信念感知决策过程
Agent-Pro采用信念感知决策过程，通过更新自身信念和世界信念来生成更合理的行为。它能够根据信念来预测行动，并在游戏结束后根据结果来反思和调整信念。

### 📈 实验结果
Agent-Pro在两个游戏（Blackjack和Texas Hold'em）中进行了评估，结果表明它能够学习并进化，在复杂和动态的场景中表现出色。与传统的LLMs和专门模型相比，Agent-Pro在游戏中的收益更高。

### 💬 可借鉴之处
Agent-Pro的设计理念和方法为构建能够学习和进化的LLM-based智能体提供了新的思路。其策略级反思和优化机制可以帮助智能体从互动经验中学习，并逐步提升其行为策略。此外，信念感知决策过程可以帮助智能体在不确定的场景中做出更合理的决策。这些方法可以应用于各种复杂的任务，例如商业谈判、安全监控等。

## towards-automation-of-cognitive-modeling-using-large-language-models
### Abstract
Computational cognitive models, which formalize theories of cognition, enable
researchers to quantify cognitive processes and arbitrate between competing
theories by fitting models to behavioral data. Traditionally, these models are
handcrafted, which requires significant domain knowledge, coding expertise, and
time investment. Previous work has demonstrated that Large Language Models
(LLMs) are adept at pattern recognition in-context, solving complex problems,
and generating executable code. In this work, we leverage these abilities to
explore the potential of LLMs in automating the generation of cognitive models
based on behavioral data. We evaluated the LLM in two different tasks: model
identification (relating data to a source model), and model generation
(generating the underlying cognitive model). We performed these tasks across
two cognitive domains - decision making and learning. In the case of data
simulated from canonical cognitive models, we found that the LLM successfully
identified and generated the ground truth model. In the case of human data,
where behavioral noise and lack of knowledge of the true underlying process
pose significant challenges, the LLM generated models that are identical or
close to the winning model from cognitive science literature. Our findings
suggest that LLMs can have a transformative impact on cognitive modeling. With
this project, we aim to contribute to an ongoing effort of automating
scientific discovery in cognitive science.
### 🌟 论文解读 | 利用大型语言模型实现认知建模自动化

### 📌 背景痛点/本文动机
传统的认知建模需要研究人员具备深厚的领域知识、编程技能和大量的时间投入。手工构建的模型往往受限于研究者的背景和建模能力，难以探索更广泛的假设空间。因此，自动化认知模型生成成为了一个重要的研究方向。

### 🚀 核心方法
💡 创新点1：利用大型语言模型（LLM）进行模型识别和生成
本文利用LLM的强大能力，探索了其在自动化生成认知模型方面的潜力。LLM能够处理自然语言格式的行为数据，识别复杂问题中的模式，并生成可执行的代码。通过将LLM应用于模型识别和模型生成任务，可以自动化认知模型生成的关键步骤。

💡 创新点2：基于行为数据生成认知模型
本文通过设计一个流程，让LLM根据行为数据和任务描述生成认知模型。LLM首先识别数据源模型，然后根据观察到的数据生成认知模型。通过将LLM应用于模拟数据和真实人类数据，可以评估LLM在模型识别和模型生成任务中的表现。

### 📈 实验结果
本文在决策和学习的两个认知领域进行了实验。结果表明，LLM在模型识别任务中表现出色，能够准确地识别数据源模型。在模型生成任务中，LLM生成的模型与真实数据生成函数非常接近，并且在拟合真实人类数据方面表现出色。

### 💬 可借鉴之处
本文提出的利用LLM进行认知建模自动化的方法具有以下可借鉴之处：
1. **降低门槛**：LLM能够处理自然语言格式的数据，使得研究人员无需具备深厚的编程技能即可进行认知建模。
2. **加速研究**：LLM能够自动化认知模型生成的关键步骤，从而加速研究进程。
3. **探索更广泛的假设空间**：LLM能够生成多种认知模型，帮助研究人员探索更广泛的假设空间。

### 🌟 未来展望
未来研究可以进一步探索LLM在更广泛的认知领域中的应用，例如感知、记忆和语言理解。此外，通过在认知建模任务上微调LLM，可以提高其生成科学意义模型的准确性。最后，通过集成多个LLM，可以创建一个完全自动化的、通用的认知建模框架。

## repoaudit--an-autonomous-llm-agent-for-repository-level-code-auditing
### Abstract
Code auditing is a code review process with the goal of finding bugs. Large
Language Models (LLMs) have shown substantial potential in this task, offering
the ability to analyze programs without compilation and enabling customized bug
detection following specified prompts. However, applying LLMs to
repository-level code auditing presents notable challenges. The inherent
context limits and hallucinations of LLMs can lead to the low quality of bug
reports. Meanwhile, the large size of software repositories introduces
substantial time and token costs, hindering efficiency and scalability in
real-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit,
designed to enable precise and efficient repository-level code auditing.
Equipped with the agent memory, RepoAudit explores the code repository on
demand, analyzing data-flow facts along different feasible program paths in
individual functions. It also introduces the validator to check the data-flow
facts for hallucination mitigation and examine the satisfiability of path
conditions of potential buggy paths, which enables RepoAudit to discard false
positives in the code auditing. Our experiment shows that RepoAudit powered by
Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems,
consuming 0.44 hours and $2.54 per project on average.
### 🌟 论文解读 | RepoAudit：基于LLM的代码审计利器

### 📌 背景痛点/本文动机
随着软件代码库的快速膨胀，传统的代码审计方法面临着效率低下、难以扩展等挑战。大型语言模型（LLMs）在代码审计方面展现出巨大潜力，但直接应用于代码库级别的审计存在局限性，例如LLMs的上下文限制和幻觉问题，以及代码库规模导致的成本问题。

### 🚀 核心方法
RepoAudit 是一个自主的 LLM-Agent，旨在实现精确和高效的代码库级别代码审计。它包含三个主要组件：

💡 创新点1：路径敏感和按需驱动
RepoAudit 通过路径敏感和按需驱动的图遍历方式，模拟人类审计过程，有效解决 LLMs 在处理大型程序图时的局限性。它将程序分解为更小的单元（如函数），并逐步分析每个单元，从而避免路径爆炸问题。

💡 创新点2：验证机制
RepoAudit 引入验证器来检查数据流事实，以减轻 LLMs 的幻觉问题，并检查潜在错误路径的路径条件，从而排除代码审计中的误报。

### 📈 实验结果
RepoAudit 在 15 个真实世界系统中成功发现了 38 个真实漏洞，平均每个项目耗时 0.44 小时，成本为 2.54 美元。与 Meta INFER 和 Amazon CODEGURU 等工业静态漏洞检测工具相比，RepoAudit 具有更高的检测能力和更低的误报率。

### 💬 可借鉴之处
RepoAudit 的设计理念和方法为 LLM 在代码审计领域的应用提供了新的思路，其路径敏感和按需驱动的图遍历方式、验证机制等创新点值得借鉴。此外，RepoAudit 的设计也为其他 LLM 驱动的代码任务提供了参考，例如程序修复、测试和分析等。

## empowering-working-memory-for-large-language-model-agents
### Abstract
Large language models (LLMs) have achieved impressive linguistic
capabilities. However, a key limitation persists in their lack of human-like
memory faculties. LLMs exhibit constrained memory retention across sequential
interactions, hindering complex reasoning. This paper explores the potential of
applying cognitive psychology's working memory frameworks, to enhance LLM
architecture. The limitations of traditional LLM memory designs are analyzed,
including their isolation of distinct dialog episodes and lack of persistent
memory links. To address this, an innovative model is proposed incorporating a
centralized Working Memory Hub and Episodic Buffer access to retain memories
across episodes. This architecture aims to provide greater continuity for
nuanced contextual reasoning during intricate tasks and collaborative
scenarios. While promising, further research is required into optimizing
episodic memory encoding, storage, prioritization, retrieval, and security.
Overall, this paper provides a strategic blueprint for developing LLM agents
with more sophisticated, human-like memory capabilities, highlighting memory
mechanisms as a vital frontier in artificial general intelligence.
### 🌟 论文解读 | 为大型语言模型代理赋能工作记忆

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在语言理解和生成方面取得了显著进展，但它们缺乏类似人类的记忆能力，限制了它们在复杂推理和协作场景中的表现。LLMs在连续交互中的记忆保留能力有限，每个交互都被视为独立的对话，缺乏持续的内存链接，这阻碍了复杂的推理。

### 🚀 核心方法
💡 创新点1：本文提出了一种创新的模型，该模型包含一个中央工作记忆中心（Working Memory Hub）和访问情境缓冲区（Episodic Buffer）的功能，以保留跨对话的记忆。这种架构旨在为复杂的任务和协作场景中的微妙上下文推理提供更大的连续性。

💡 创新点2：本文还探讨了在多智能体系统中管理智能体对情境缓冲区访问的策略，包括基于角色的访问控制、基于任务的内存分配、自主内存检索和专门的内存管理代理。

### 📈 实验结果
本文没有提供具体的实验结果，而是提出了一种战略蓝图，用于开发具有更复杂、类似人类记忆能力的LLM代理。

### 💬 可借鉴之处
本文提出的工作记忆模型为LLM代理提供了更高级的记忆功能，有助于提高它们在复杂任务和协作场景中的表现。此外，本文还探讨了在多智能体系统中管理内存访问的策略，为开发更高效、安全的内存管理系统提供了参考。

## voyager--an-open-ended-embodied-agent-with-large-language-models
### Abstract
We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.
### 🌟 论文解读 | Voyager：基于大型语言模型的开放式具身终身学习智能体

### 📌 背景痛点/本文动机
构建能够在开放世界中持续探索、规划和开发新技能的通用具身智能体，是人工智能领域的一大挑战。传统的强化学习和模仿学习方法在探索、可解释性和泛化方面存在局限性。近年来，基于大型语言模型（LLM）的智能体利用预训练LLM中的世界知识生成一致的行动计划或可执行策略，但它们并非终身学习者，无法在长时间跨度内逐步获取、更新、积累和转移知识。

### 🚀 核心方法
Voyager 是第一个由 LLM 驱动的具身终身学习智能体，能够在 Minecraft 中持续探索世界、获取多样化技能，并在没有人类干预的情况下进行新的发现。Voyager 由三个关键组件组成：

💡 创新点1：自动课程
Voyager 通过自动课程进行开放式探索，该课程由 GPT-4 生成，旨在“发现尽可能多的多样化事物”。课程会根据探索进度和智能体的状态提出越来越难的任务，从而推动智能体不断学习新技能。

💡 创新点2：技能库
Voyager 拥有一个不断增长的技能库，用于存储和检索可执行代码，以存储和检索复杂的行为。每个技能都由可执行代码表示，这些代码可以自然地表示时间扩展和组合动作，这对于 Minecraft 中的许多长期任务至关重要。

💡 创新点3：迭代提示机制
Voyager 通过迭代提示机制生成可执行代码，该机制利用环境反馈、执行错误和自我验证来改进程序。该机制通过执行生成的程序、获取环境反馈和执行错误，并将这些反馈纳入 GPT-4 的提示中，从而进行代码改进。这个过程会重复进行，直到自我验证模块确认任务完成，此时将程序添加到技能库中，并查询自动课程以获取下一个目标。

### 📈 实验结果
Voyager 在 MineDojo 框架中与其他 LLM 基于智能体技术进行了比较，结果表明 Voyager 在发现新物品、解锁 Minecraft 技术树、穿越各种地形以及将学习到的技能库应用于新世界中的未见任务方面表现出色。Voyager 获得了 3.3 倍的新物品，解锁关键技术树里程碑的速度提高了 15.3 倍，穿越的距离是基线的 2.3 倍。

### 💬 可借鉴之处
Voyager 的设计为开发强大的通用智能体提供了一个起点，无需调整模型参数。其自动课程、技能库和迭代提示机制为终身学习智能体的开发提供了新的思路。此外，Voyager 的技能库可以作为其他方法的即插即用资产，有效地提高性能。

## ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory
### Abstract
The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular "ObtainDiamond" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the "ObtainDiamond" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.
### 🌟 论文解读 | Minecraft中的幽灵：通过大型语言模型和基于文本的知识与记忆，创建开放世界环境中的通用能力智能体

### 📌 背景痛点/本文动机
Minecraft作为一款开放世界游戏，吸引了大量研究兴趣，成为开发能够在开放世界中运行的智能体的丰富平台。然而，目前的研究主要集中在特定目标上，如流行的“ObtainDiamond”任务，尚未在更广泛的任务上展现出有效的泛化能力。此外，现有方法在“ObtainDiamond”任务上的最高成功率仅为约20%，突显了现有基于强化学习（RL）的控制器方法的局限性。为了解决这些挑战，本文提出了Ghost in the Minecraft（GITM）框架，该框架将大型语言模型（LLMs）与基于文本的知识和记忆相结合，旨在创建能够在Minecraft中运行的通用能力智能体（GCAs）。

### 🚀 核心方法
💡 创新点1：LLM分解器
LLM分解器负责将任务目标分解为一系列更易于实现的子目标。通过解决每个子目标，可以逐步实现任务目标。LLM分解器利用从互联网收集的文本知识，将目标分解为子目标树。

💡 创新点2：LLM规划器
LLM规划器负责为每个子目标生成一系列结构化操作。结构化操作具有明确的语义和相应的反馈，使LLMs能够在认知层面理解周围环境并做出决策。LLM规划器还记录和总结成功的操作列表，以增强未来的规划。

💡 创新点3：LLM接口
LLM接口负责将结构化操作转换为键盘/鼠标操作，并与环境进行交互。它还从环境中提取观察结果，并将其转换为反馈消息。

### 📈 实验结果
本文的实验结果表明，基于LLM的智能体在“ObtainDiamond”任务上的成功率显著提高，达到了47.5%，超过了现有的方法。此外，该智能体是第一个在Minecraft Overworld中获取所有物品的智能体，展示了其广泛的技能。

### 💬 可借鉴之处
本文提出的GITM框架为开发能够在开放世界中运行的通用能力智能体提供了一种新的思路。通过利用LLMs的常识和推理能力，以及基于文本的知识和记忆，该框架能够使智能体有效地处理开放世界环境中的各种挑战。此外，该框架还具有高效的学习效率和良好的泛化能力，使其在开发通用能力智能体方面具有巨大的潜力。

## hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation
### Abstract
Due to the dynamic and unpredictable open-world setting, navigating complex
environments in Minecraft poses significant challenges for multi-agent systems.
Agents must interact with the environment and coordinate their actions with
other agents to achieve common objectives. However, traditional approaches
often struggle to efficiently manage inter-agent communication and task
distribution, crucial for effective multi-agent navigation. Furthermore,
processing and integrating multi-modal information (such as visual, textual,
and auditory data) is essential for agents to comprehend their goals and
navigate the environment successfully and fully. To address this issue, we
design the HAS framework to auto-organize groups of LLM-based agents to
complete navigation tasks. In our approach, we devise a hierarchical
auto-organizing navigation system, which is characterized by 1) a hierarchical
system for multi-agent organization, ensuring centralized planning and
decentralized execution; 2) an auto-organizing and intra-communication
mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal
information platform, facilitating multi-modal perception to perform the three
navigation tasks with one system. To assess organizational behavior, we design
a series of navigation tasks in the Minecraft environment, which includes
searching and exploring. We aim to develop embodied organizations that push the
boundaries of embodied AI, moving it towards a more human-like organizational
structure.
### 🌟 论文解读 | HAS：开放世界多智能体导航的分层自组织系统

### 📌 背景痛点/本文动机
在开放世界的环境中，如Minecraft，多智能体系统面临着复杂的导航挑战。传统的导航方法往往难以有效地管理智能体之间的通信和任务分配，这对于有效的多智能体导航至关重要。此外，处理和整合多模态信息（如视觉、文本和听觉数据）对于智能体理解其目标并在环境中成功导航至关重要。

### 🚀 核心方法
💡 创新点1：分层自组织导航系统
HAS框架设计了一个分层自组织导航系统，该系统具有以下特点：
1. 分层系统：确保集中式规划和分布式执行，提高导航效率。
2. 自组织机制：根据子任务动态调整关键角色和行动组，并保持组间通信，确保高效协作。
3. 多模态信息平台：促进多模态感知，使系统能够处理图像、对象和音频目标，并执行搜索和探索等导航任务。

💡 创新点2：多模态语言模型
HAS框架使用了多模态语言模型（MLM），包括管理者和执行者两种类型的模型。这些模型具有不同的功能，如规划、描述、评估和部署，以实现集中式规划和分布式执行。

💡 创新点3：多模态记忆
HAS框架还引入了多模态记忆机制，用于存储和检索多模态信息，从而提高规划准确性和一致性。通过检索增强生成（RAG）和多模态检索（MMR）技术，HAS能够有效地利用历史交互反馈和经验，生成更准确的计划。

### 📈 实验结果
在Minecraft环境中进行的实验表明，HAS框架在多模态目标搜索、连续块搜索和地图探索等任务中取得了最先进的性能。与基线方法相比，HAS在导航效率、成功率和探索能力方面均有显著提升。

### 💬 可借鉴之处
HAS框架为开放世界多智能体导航提供了一种新的思路和方法。其分层自组织结构、多模态语言模型和多模态记忆机制等创新点，对于提高多智能体系统的自主性、效率和适应性具有重要意义。此外，HAS框架还可以应用于其他需要多智能体协作的场景，如机器人协同、虚拟现实等。

## human-agent-decision-making--combining-theory-and-practice
### Abstract
Extensive work has been conducted both in game theory and logic to model
strategic interaction. An important question is whether we can use these
theories to design agents for interacting with people? On the one hand, they
provide a formal design specification for agent strategies. On the other hand,
people do not necessarily adhere to playing in accordance with these
strategies, and their behavior is affected by a multitude of social and
psychological factors. In this paper we will consider the question of whether
strategies implied by theories of strategic behavior can be used by automated
agents that interact proficiently with people. We will focus on automated
agents that we built that need to interact with people in two negotiation
settings: bargaining and deliberation. For bargaining we will study game-theory
based equilibrium agents and for argumentation we will discuss logic-based
argumentation theory. We will also consider security games and persuasion games
and will discuss the benefits of using equilibrium based agents.
### 🌟 论文解读 | 人机决策：理论与实践的结合

### 📌 背景痛点/本文动机
本文探讨了如何将博弈论和逻辑理论应用于设计能够与人类有效互动的自动化代理。尽管这些理论为代理策略提供了正式的设计规范，但人类的决策行为往往受到社会和心理因素的影响，并不总是遵循这些策略。因此，本文旨在研究如何将理论策略应用于自动化代理，使其在与人类互动时能够表现出色。

### 🚀 核心方法
💡 创新点1：多议题谈判
本文提出了多种自动化代理，用于处理多议题谈判场景。这些代理结合了博弈论、逻辑理论和启发式方法，以适应人类谈判者的行为。例如，EQH代理通过引入启发式参数，如让步幅度和谈判单位，来提高其灵活性，从而在与人类谈判者互动时获得更高的效用分数。

💡 创新点2：谈判与行动交织
本文研究了在谈判过程中交织资源交换和其他活动的场景。例如，在“揭示游戏”中，代理需要根据人类行为模型来预测对手的类型，并据此调整其策略。在“非绑定协议”游戏中，代理需要决定是否遵守协议，并使用机器学习技术来预测人类对手的行为。

💡 创新点3：论证代理
本文探讨了如何利用自动化代理帮助人类在论证对话中提出论点。通过结合论证理论和机器学习技术，代理能够预测人类可能使用的论点，并提供相关的论据。

💡 创新点4：安全游戏
本文研究了如何将博弈论应用于安全领域，例如机场安全检查和巡逻路线规划。通过使用贝叶斯Stackelberg博弈，代理能够有效地分配资源，以保护关键基础设施。

💡 创新点5：说服游戏
本文研究了如何利用自动化代理在说服游戏中影响人类的行为。通过结合博弈论和机器学习技术，代理能够预测人类对手的行为，并据此调整其策略。

### 📈 实验结果
本文通过大量的实验验证了所提出的自动化代理的有效性。结果表明，结合理论、启发式方法和机器学习技术的代理能够在多种场景下与人类有效互动，并获得更高的效用分数。

### 💬 可借鉴之处
本文的研究结果表明，在设计自动化代理时，需要考虑人类行为的复杂性和多样性。通过结合理论、启发式方法和机器学习技术，可以开发出能够与人类有效互动的代理，并在各种场景下取得良好的效果。

## calypso--llms-as-dungeon-masters--assistants
### Abstract
The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to
perform multiple tasks simultaneously. The DM must digest information about the
game setting and monsters, synthesize scenes to present to other players, and
respond to the players' interactions with the scene. Doing all of these tasks
while maintaining consistency within the narrative and story world is no small
feat of human cognition, making the task tiring and unapproachable to new
players. Large language models (LLMs) like GPT-3 and ChatGPT have shown
remarkable abilities to generate coherent natural language text. In this paper,
we conduct a formative evaluation with DMs to establish the use cases of LLMs
in D&D and tabletop gaming generally. We introduce CALYPSO, a system of
LLM-powered interfaces that support DMs with information and inspiration
specific to their own scenario. CALYPSO distills game context into bite-sized
prose and helps brainstorm ideas without distracting the DM from the game. When
given access to CALYPSO, DMs reported that it generated high-fidelity text
suitable for direct presentation to players, and low-fidelity ideas that the DM
could develop further while maintaining their creative agency. We see CALYPSO
as exemplifying a paradigm of AI-augmented tools that provide synchronous
creative assistance within established game worlds, and tabletop gaming more
broadly.
### 🌟 论文解读 | CALYPSO：大型语言模型助力地下城主

### 📌 背景痛点/本文动机
地下城与龙（D&D）是一款经典的桌面角色扮演游戏，其中地下城主（DM）扮演着至关重要的角色。DM需要同时处理多项任务，包括消化游戏背景和怪物信息、构建场景、回应玩家互动等。这些任务对人类认知能力要求极高，对于新玩家来说尤其具有挑战性。大型语言模型（LLM）如GPT-3和ChatGPT在生成连贯的自然语言文本方面表现出色。本文旨在探索LLM在D&D和桌面游戏中的应用，并提出了CALYPSO系统，该系统利用LLM为DM提供信息和支持，帮助他们更好地进行游戏。

### 🚀 核心方法
💡 创新点1：CALYPSO系统
CALYPSO是一个由LLM驱动的界面系统，旨在支持DM在游戏中获取信息和灵感。它包括三个主要功能：
1. **遭遇理解**：使用GPT-3对游戏背景和怪物信息进行摘要，帮助DM快速理解遭遇。
2. **聚焦头脑风暴**：使用ChatGPT与DM进行对话，帮助他们进一步探索遭遇细节或生成新的想法。
3. **开放域聊天基线**：使用ChatGPT提供一个开放域的聊天界面，供玩家和DM进行非游戏相关的交流。

💡 创新点2：LLM的创造性辅助
CALYPSO系统展示了LLM作为创造性辅助工具的潜力。它不仅能够生成高保真度的文本，适合直接呈现给玩家，还能够提供低保真度的想法，供DM进一步发展和完善。这种辅助方式保留了DM的创造性自主权，使他们能够更好地专注于游戏中的认知任务。

### 📈 实验结果
研究结果表明，DM在使用CALYPSO系统后，普遍认为它能够生成适合直接呈现给玩家的文本，并提供有价值的灵感。DM们利用CALYPSO系统来理解复杂的怪物信息、头脑风暴非玩家角色或怪物之间的互动，并获取建议，将这些建议融入到故事中呈现给玩家，而不会影响游戏的节奏。

### 💬 可借鉴之处
本文的研究结果对于开发AI辅助工具在桌面游戏和其他创意领域中的应用具有重要的启示意义。CALYPSO系统的设计理念和方法可以为其他类似项目提供参考，例如：
1. **理解用户需求**：通过访谈和用户研究，深入了解用户的需求和痛点，从而设计出更符合用户需求的AI辅助工具。
2. **利用LLM的创造性潜力**：LLM在生成连贯文本和提供创造性灵感方面具有巨大潜力，可以将其应用于各种创意场景。
3. **保持用户的创造性自主权**：AI辅助工具应该作为用户的助手，而不是替代者，帮助用户更好地发挥自己的创造力。

总而言之，CALYPSO系统展示了LLM在桌面游戏中的应用潜力，并为开发AI辅助工具提供了有价值的经验和启示。

## rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models
### Abstract
The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
### 🌟 论文解读 | RoleLLM：解锁大型语言模型的角色扮演能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的兴起，角色扮演等复杂任务成为可能，为用户交互提供了更多可能性。然而，现有开源LLMs在角色扮演方面存在优化不足的问题，而最先进的LLMs如GPT-4等则因其闭源性质而限制了其在角色扮演方面的应用。本文旨在解决这些问题，提出RoleLLM框架，以提升LLMs的角色扮演能力。

### 🚀 核心方法
💡 创新点1：Role Profile Construction
构建了100个角色的详细档案，包括角色描述、口头禅以及从剧本中提取的对话片段，为角色扮演提供了丰富的背景知识。

💡 创新点2：Context-Based Instruction Generation (Context-Instruct)
利用GPT模型从角色档案中生成高质量的问答对，以提取角色特定的知识和记忆。

💡 创新点3：Role Prompting using GPT (RoleGPT)
通过对话工程和检索增强技术，利用GPT模型生成符合角色说话风格的回答，以模仿角色的说话风格。

💡 创新点4：Role-Conditioned Instruction Tuning (RoCIT)
利用Context-Instruct和RoleGPT生成的数据，对开源LLMs进行微调，以提升其角色扮演能力，并生成RoleLLaMA和RoleGLM等模型。

### 📈 实验结果
实验结果表明，RoleLLM框架在角色扮演方面取得了显著成果。RoleLLaMA和RoleGLM在模仿角色说话风格、回答准确性和角色特定知识掌握方面表现出色，甚至在某些情况下与RoleGPT（使用GPT-4）相当。

### 💬 可借鉴之处
RoleLLM框架为LLMs的角色扮演能力提升提供了新的思路和方法，其创新点包括角色档案构建、基于上下文的指令生成、角色提示和角色条件指令微调等。此外，RoleBench数据集的构建也为角色扮演能力的评估和提升提供了重要的参考。

## personallm--investigating-the-ability-of-large-language-models-to-express-personality-traits
### Abstract
Despite the many use cases for large language models (LLMs) in creating
personalized chatbots, there has been limited research on evaluating the extent
to which the behaviors of personalized LLMs accurately and consistently reflect
specific personality traits. We consider studying the behavior of LLM-based
agents which we refer to as LLM personas and present a case study with GPT-3.5
and GPT-4 to investigate whether LLMs can generate content that aligns with
their assigned personality profiles. To this end, we simulate distinct LLM
personas based on the Big Five personality model, have them complete the
44-item Big Five Inventory (BFI) personality test and a story writing task, and
then assess their essays with automatic and human evaluations. Results show
that LLM personas' self-reported BFI scores are consistent with their
designated personality types, with large effect sizes observed across five
traits. Additionally, LLM personas' writings have emerging representative
linguistic patterns for personality traits when compared with a human writing
corpus. Furthermore, human evaluation shows that humans can perceive some
personality traits with an accuracy of up to 80%. Interestingly, the accuracy
drops significantly when the annotators were informed of AI authorship.
### 🌟 论文解读 | PersonaLLM：探究大型语言模型表达人格特质的能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在创建个性化聊天机器人方面的广泛应用，对于个性化LLMs的行为是否能够准确且一致地反映特定人格特质的研究却相对有限。本文旨在探究LLMs是否能够生成与指定人格特征相符的内容，并评估其表现。

### 🚀 核心方法
💡 创新点1：基于大五人格模型模拟LLM人格
本文通过大五人格模型（包括外向性、宜人性、责任心、神经质和开放性）模拟了不同的LLM人格，并让这些LLM人格完成大五人格量表（BFI）测试和故事写作任务。

💡 创新点2：多角度评估LLM人格
本文采用自动评估和人工评估相结合的方式，对LLM人格生成的故事进行评估。评估维度包括可读性、个人性、冗余性、连贯性、喜爱度和可信度。此外，还要求评估者根据故事推断作者的人格特质。

### 📈 实验结果
实验结果表明，LLM人格的自我报告BFI得分与其指定的人格类型一致，且在五个特质上观察到较大的效应量。LLM人格的写作中出现了与人格特质相关的代表性语言模式，与人类写作语料库相比具有一致性。人工评估显示，人类可以以高达80%的准确率感知到某些人格特质。有趣的是，当评估者被告知AI作者身份时，准确率显著下降。

### 💬 可借鉴之处
本文的研究结果表明，LLMs在表达人格特质方面具有一定的能力，但仍存在一些局限性。例如，LLMs在生成故事时可能会重复相似的内容，且在评估过程中可能存在主观性。未来研究可以进一步探索LLMs在更自然istic的场景下的表现，并考虑更多样化的语言和文化背景。此外，研究还可以深入探讨人类评估者的人格和背景对其人格预测准确率的影响，以及AI作者身份对人类感知的影响。

## chatharuhi--reviving-anime-character-in-reality-via-large-language-model
### Abstract
Role-playing chatbots built on large language models have drawn interest, but
better techniques are needed to enable mimicking specific fictional characters.
We propose an algorithm that controls language models via an improved prompt
and memories of the character extracted from scripts. We construct ChatHaruhi,
a dataset covering 32 Chinese / English TV / anime characters with over 54k
simulated dialogues. Both automatic and human evaluations show our approach
improves role-playing ability over baselines. Code and data are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya .
### 🌟 论文解读 | ChatHaruhi：通过大型语言模型在现实中复活动漫角色

### 📌 背景痛点/本文动机
随着ChatGPT等大型语言模型的发布，基于这些模型的扮演式聊天机器人引起了广泛关注。然而，现有的扮演式聊天机器人存在一些问题：1. 过度依赖语言模型自身的记忆，如果模型对作品的理解模糊，则难以很好地模仿特定角色；2. “知道所有关于角色的知识”定义模糊，容易产生幻觉；3. 即使有扮演提示，聊天机器人的对话风格仍然受到底层语言模型的影响，需要为每个角色精细调整提示。

### 🚀 核心方法
本文提出了一个基于大型语言模型的完整扮演式聊天机器人算法系统，该系统可以有效地组织角色的记忆，使语言模型能够在对话中模仿特定动漫/电视角色的语气和知识。主要创新点如下：

💡 创新点1：构建了包含32个不同中英动漫/电视角色的扮演式数据集，涵盖超过54,000个模拟对话。通过收集和结构化提取电影、小说、剧本中的对话，收集了超过22,000个对话交换，可用于训练和评估扮演式语言模型。

💡 创新点2：设计了自动生成对话的系统，即使对于对话较少的角色，也能生成符合角色个性的对话。这允许我们为微调本地模型生成足够的数据。

💡 创新点3：使用自动和人工评估来评估和比较不同的扮演式聊天机器人。自动评估测试聊天机器人是否能够对经典情节点做出与原始剧本相似的回答。人工评估提出了两个指标供评估者评估：一致性（聊天机器人的回答是否与角色的原始设定一致）和响应质量（聊天机器人的回答是否具有良好的语言质量）。

### 📈 实验结果
实验结果表明，在相同的底层语言模型下，本文提出的算法在扮演式性能方面优于基线模型。

### 💬 可借鉴之处
本文提出的扮演式聊天机器人算法系统可以应用于游戏、创意产业等领域，为用户提供更加丰富的互动体验。此外，本文构建的扮演式数据集和自动生成对话的系统也为相关研究提供了有价值的参考。

## lamp--when-large-language-models-meet-personalization
### Abstract
This paper highlights the importance of personalization in large language
models and introduces the LaMP benchmark -- a novel benchmark for training and
evaluating language models for producing personalized outputs. LaMP offers a
comprehensive evaluation framework with diverse language tasks and multiple
entries for each user profile. It consists of seven personalized tasks,
spanning three text classification and four text generation tasks. We
additionally propose two retrieval augmentation approaches that retrieve
personal items from each user profile for personalizing language model outputs.
To this aim, we study various retrieval models, including term matching,
semantic matching, and time-aware methods. Extensive experiments on LaMP for
zero-shot and fine-tuned language models demonstrate the efficacy of the
proposed retrieval augmentation approach and highlight the impact of
personalization in various natural language tasks.
### 🌟 论文解读 | LaMP：大型语言模型个性化输出新基准

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理（NLP）领域的广泛应用，个性化输出成为满足用户独特需求和偏好的关键因素。然而，现有的NLP基准测试往往采用“一刀切”的方法，缺乏对个性化需求的考虑，限制了个性化研究的发展。本文提出了LaMP基准，旨在评估LLMs在个性化文本分类和生成任务中的性能。

### 🚀 核心方法
💡 创新点1：LaMP基准
LaMP基准包含七个个性化任务，涵盖三个文本分类和四个文本生成任务，并提供两种数据分割方式（基于用户和基于时间）以适应不同的个性化场景。

💡 创新点2：检索增强方法
为了解决用户配置文件长度限制问题，本文提出了两种检索增强方法：提示内增强（IPA）和融合解码器（FiD）。这些方法从用户配置文件中检索相关信息，并将其用于个性化LLM输出。

### 📈 实验结果
实验结果表明，检索增强方法在LaMP基准上取得了显著的性能提升。在零样本设置下，平均性能提升12.2%，而在微调设置下，平均性能提升23.5%。

### 💬 可借鉴之处
LaMP基准为个性化NLP模型的研究提供了丰富的环境和工具。检索增强方法为解决用户配置文件长度限制问题提供了一种有效解决方案。此外，本文还提出了几个值得进一步研究的问题，例如个性化文本生成的评估指标和隐私保护等。

## camel--communicative-agents-for--mind--exploration-of-large-language-model-society
### Abstract
The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their "cognitive" processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: https://github.com/camel-ai/camel.
### 🌟 论文解读 | CAMEL：探索大型语言模型社会的“心智”交流

### 📌 背景痛点/本文动机
随着基于聊天的语言模型在复杂任务解决方面的快速发展，它们在解决复杂任务方面取得了显著进展。然而，这些模型的成功严重依赖于人类输入来引导对话，这可能会具有挑战性且耗时。本文探讨了构建可扩展技术以促进交流代理之间的自主合作，并深入了解其“认知”过程的潜力。

### 🚀 核心方法
💡 创新点1：角色扮演框架
为了解决实现自主合作的挑战，本文提出了一种名为“角色扮演”的新型交流代理框架。该框架涉及使用“起始提示”来引导聊天代理完成任务，同时保持与人类意图的一致性。

💡 创新点2：起始提示
本文提出了一种名为“起始提示”的对话LLM自动提示方法，使代理能够通过角色扮演相互提示以解决问题。AI用户不断向AI助手提供指令以解决问题，这使我们能够保存指令-解决方案对并创建多样化、指令性、对话性和面向任务的语料库。

💡 创新点3：数据集生成
本文展示了如何使用角色扮演来让聊天代理相互交流以完成任务，并记录他们的对话以进行行为分析和能力理解。特别是，我们对多代理设置中的指令遵循合作进行了全面研究。

💡 创新点4：开源库
本文开源了我们的库，其中包含各种代理的实现、数据生成管道、数据分析工具和收集的数据集，以支持对交流代理的研究。

### 📈 实验结果
本文通过实验评估了CAMEL框架的性能，结果表明CAMEL解决方案在人类评估和GPT4评估中均优于gpt-3.5-turbo单次解决方案。此外，本文还研究了LLM训练能力的显著出现，通过在通过框架生成的不断增长的语料库上微调LLaMA模型。

### 💬 可借鉴之处
本文提出的CAMEL框架为研究交流代理之间的自主合作提供了可扩展的方法，并提供了应对挑战的策略。此外，本文开源的库为研究交流代理和更广泛的研究领域提供了宝贵的资源。

## an-appraisal-based-chain-of-emotion-architecture-for-affective-language-model-game-agents
### Abstract
The development of believable, natural, and interactive digital artificial
agents is a field of growing interest. Theoretical uncertainties and technical
barriers present considerable challenges to the field, particularly with
regards to developing agents that effectively simulate human emotions. Large
language models (LLMs) might address these issues by tapping common patterns in
situational appraisal. In three empirical experiments, this study tests the
capabilities of LLMs to solve emotional intelligence tasks and to simulate
emotions. It presents and evaluates a new chain-of-emotion architecture for
emotion simulation within video games, based on psychological appraisal
research. Results show that it outperforms standard LLM architectures on a
range of user experience and content analysis metrics. This study therefore
provides early evidence of how to construct and test affective agents based on
cognitive processes represented in language models.
### 🌟 论文解读 | 基于评估的情感链架构：让游戏中的语言模型代理更具情感

### 📌 背景痛点/本文动机
随着人工智能技术的发展，构建可信、自然和交互式的数字人工代理成为了一个日益增长的研究领域。然而，模拟人类情感一直是这一领域的难题。大型语言模型（LLMs）可能通过利用情境评估中的常见模式来解决这些问题。本文研究了LLMs在解决情感智能任务和模拟情感方面的能力，并提出了一个新的基于情感链架构的游戏情感模拟方法。

### 🚀 核心方法
💡 创新点1：评估提示策略
本文提出了一种基于评估的情感链架构，该架构利用情境信息和角色特征来评估当前情况，并生成相应的情感反应。通过将评估过程与语言模型相结合，可以更准确地模拟人类情感。

💡 创新点2：情感链架构
该架构包括一个记忆系统，用于存储观察结果和情感反应，以及一个评估系统，用于将观察结果转换为情感反应。通过这种方式，可以创建一个情感链，用于生成代理的行为和对话。

### 📈 实验结果
实验结果表明，基于评估的情感链架构在用户体验和内容分析指标方面优于标准的LLM架构。用户研究表明，该架构在代理可信度、反应性和情感智力方面表现出色。

### 💬 可借鉴之处
本文的研究结果表明，LLMs在模拟情感方面具有巨大潜力。通过利用评估提示策略和情感链架构，可以构建更具情感的游戏代理，从而提供更丰富、更自然的用户体验。此外，该方法还可以应用于其他领域，例如虚拟助手和聊天机器人，以模拟更真实的情感反应。

## word-sense-disambiguation--a-survey
### Abstract
In this paper, we made a survey on Word Sense Disambiguation (WSD). Near
about in all major languages around the world, research in WSD has been
conducted upto different extents. In this paper, we have gone through a survey
regarding the different approaches adopted in different research works, the
State of the Art in the performance in this domain, recent works in different
Indian languages and finally a survey in Bengali language. We have made a
survey on different competitions in this field and the bench mark results,
obtained from those competitions.
### 🌟 论文解读 | 词义消歧：探索自然语言处理的前沿技术

### 📌 背景痛点/本文动机
词义消歧（Word Sense Disambiguation, WSD）是自然语言处理（NLP）领域中的一个重要挑战。由于许多单词在不同的语境中具有不同的含义，因此机器在理解和处理自然语言时往往会遇到困难。本文旨在对词义消歧技术进行全面的调查，探讨不同研究工作中采用的不同方法，以及该领域在性能方面的最新进展。

### 🚀 核心方法
本文主要介绍了三种词义消歧方法：

#### 💡 创新点1：基于知识的方法
这种方法依赖于机器可读词典、词义库、同义词库等知识源。例如，Lesk算法通过比较句子中单词的词典定义来确定词义。此外，语义相似度、选择偏好和启发式方法也被用于基于知识的方法中。

#### 💡 创新点2：监督学习方法
监督学习方法使用手动创建的词义标注数据来训练机器学习模型。例如，决策列表、决策树、朴素贝叶斯、神经网络、基于实例的学习和支持向量机等方法都被用于监督学习方法中。

#### 💡 创新点3：无监督学习方法
无监督学习方法不依赖于外部知识源或词义库。例如，基于上下文聚类的无监督学习方法通过将上下文向量分组到簇中来识别词义。此外，基于词聚类的无监督学习方法通过将语义相同的单词分组到簇中来识别词义。

### 📈 实验结果
本文对词义消歧技术的性能进行了评估，并总结了不同方法在不同语言和任务上的表现。结果表明，监督学习方法在词义消歧任务中取得了最好的性能。

### 💬 可借鉴之处
本文对词义消歧技术进行了全面的调查，并总结了不同方法的优缺点。这对于研究人员和开发者来说是一个宝贵的资源，可以帮助他们选择合适的词义消歧方法来解决实际问题。此外，本文还介绍了词义消歧技术在印度语言中的应用，这对于推动印度语言的自然语言处理研究具有重要意义。

## self-refine--iterative-refinement-with-self-feedback
### Abstract
Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach.
### 🌟 论文解读 | 自我反馈迭代优化：提升大型语言模型输出质量的新方法

### 📌 背景痛点/本文动机
大型语言模型（LLMs）虽然在生成连贯的输出方面表现出色，但在处理复杂任务时往往难以满足精细化的需求。例如，在对话生成、代码优化等任务中，LLMs 生成的初始输出可能不够完善，需要进一步的迭代优化才能达到理想的质量。传统的迭代优化方法通常需要训练一个专门的优化模型，这需要大量的领域特定数据或昂贵的标注，限制了其应用范围。

### 🚀 核心方法
本文提出了一种名为 Self-Refine 的新型迭代优化算法，该算法通过自我反馈和迭代优化来提升 LLMs 的输出质量。Self-Refine 的核心思想是使用同一个 LLM 作为生成器、反馈提供者和优化器，无需额外的训练数据或强化学习。具体步骤如下：

1. **初始生成**：使用 LLM 生成一个初始输出。
2. **反馈**：将初始输出反馈给 LLM，LLM 根据反馈生成改进建议。
3. **优化**：LLM 根据反馈建议对初始输出进行优化，生成新的输出。
4. **迭代**：重复步骤 2 和 3，直到达到预设的停止条件。

Self-Refine 使用少量样本提示（few-shot prompting）来指导 LLM 进行反馈和优化，从而避免了额外的训练过程。

### 📈 实验结果
本文在 7 个不同的任务上评估了 Self-Refine 的性能，包括对话生成、代码优化、代码可读性改进、数学推理等。结果表明，Self-Refine 在所有任务上都显著优于直接使用 LLM 进行单次生成的结果，平均性能提升约 20%。此外，Self-Refine 还能够提升 GPT-4 等最先进的 LLMs 的性能，证明了其在实际应用中的价值。

### 💬 可借鉴之处
Self-Refine 为提升 LLMs 输出质量提供了一种简单而有效的方法，具有以下可借鉴之处：

* **自我反馈机制**：利用 LLM 自身的能力进行自我反馈和优化，无需额外的训练数据或标注。
* **迭代优化**：通过多次迭代优化，逐步提升输出质量，达到更精细化的需求。
* **少量样本提示**：使用少量样本提示来指导 LLM 进行反馈和优化，简化了操作流程。

Self-Refine 的提出为 LLMs 的应用开辟了新的可能性，有望在自然语言处理、代码生成等领域发挥重要作用。

## proagent--building-proactive-cooperative-agents-with-large-language-models
### Abstract
Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.
### 🌟 论文解读 | ProAgent：基于大型语言模型构建主动合作智能体

### 📌 背景痛点/本文动机
在多智能体系统中，构建具有自适应行为的合作智能体是一个重要的目标。然而，现有的基于学习的方法在策略泛化方面存在局限性，因为它们依赖于训练阶段与队友的多样性。当与不熟悉的队友合作时，这种依赖性限制了智能体进行战略适应的能力，这在零样本协调场景中成为一个重大挑战。

### 🚀 核心方法
为了解决这个问题，本文提出了ProAgent，一个利用大型语言模型（LLMs）创建主动智能体的新框架，这些智能体能够动态地调整其行为以增强与队友的合作。ProAgent可以分析当前状态，并从观察中推断队友的意图。然后，它通过比较队友的后续实际行为来更新其信念。此外，ProAgent具有高度的模块化和可解释性，可以轻松地集成到各种协调场景中。

ProAgent框架包括四个关键模块：规划器、验证器、控制器和记忆模块，以及信念修正机制。这些模块协同工作，使ProAgent能够主动预测队友的意图，并在没有预先训练或微调的情况下实现自适应的合作推理和规划。

### 📈 实验结果
在Overcooked-AI环境中进行的实验评估揭示了ProAgent的卓越性能。在与AI智能体合作时，ProAgent优于基于自我游戏和基于种群训练的五种方法。此外，在与人类代理模型合作时，其性能平均提高了超过10%，与当前最先进的方法相比。

### 💬 可借鉴之处
ProAgent框架为利用LLMs在合作场景中的强大推理和规划能力提供了一个全面的指南。它展示了LLMs在解释当前场景、明确推断队友意图以及相应地动态调整行为方面的显著能力。这些结果为构建更高效的合作场景提供了有价值的见解，并为开发更先进的合作多智能体系统和人机兼容的AI系统铺平了道路。

## language-models-can-solve-computer-tasks
### Abstract
Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting's
effectiveness in enhancing LLMs' reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting with
external feedback. We find that RCI combined with CoT performs better than
either separately. Our code can be found here:
https://github.com/posgnu/rci-agent.
### 🌟 论文解读 | 语言模型解决计算机任务：RCI方法引领AI新潮流

### 📌 背景痛点/本文动机
随着人工智能的发展，人们期望能够创造出能够执行各种计算机任务的智能代理，从而提高效率和生产力。然而，现有的方法往往需要大量的专家演示和特定任务的奖励函数，这在实际应用中并不实用。本文提出了一种新的方法，使用预训练的大型语言模型（LLM）来执行计算机任务，并通过自然语言命令进行指导。

### 🚀 核心方法
本文的核心方法是递归批评和改进（RCI）方案，该方案通过以下步骤实现：
1. **任务接地**：首先，LLM根据任务文本生成一个高级计划。
2. **状态接地**：然后，将高级概念与当前状态中的实际HTML元素连接起来，输出相应的动作。
3. **代理接地**：最后，确保动作输出格式正确，可以被计算机代理执行。

RCI方案在每个步骤中都应用，但状态接地步骤只需要一次批评。

### 📈 实验结果
本文在MiniWoB++基准测试中评估了RCI方法，结果表明，RCI方法显著优于现有的LLM方法，并在自动化计算机任务方面超越了监督学习和强化学习方法。此外，RCI提示方案在增强LLM的自然语言推理能力方面也表现出色，超过了外部反馈的链式思维（CoT）提示。

### 💬 可借鉴之处
本文提出的RCI方法为使用LLM执行计算机任务提供了一种新的思路，具有以下可借鉴之处：
1. **简化任务执行**：RCI方案通过将任务分解为三个步骤，简化了LLM执行计算机任务的流程。
2. **提高推理能力**：RCI提示方案可以增强LLM的自然语言推理能力，使其能够更好地理解和执行任务。
3. **降低样本复杂度**：RCI方法只需要少量演示即可泛化到未见过的任务，降低了样本复杂度。

### 🌟 总结
本文提出的RCI方法为使用LLM执行计算机任务提供了一种新的思路，具有简化任务执行、提高推理能力和降低样本复杂度等优点。随着LLM能力的不断提高，RCI方法有望在自动化计算机任务方面发挥更大的作用。

## quantizing-constrained-systems--new-perspectives
### Abstract
We consider quantum mechanics on constrained surfaces which have
non-Euclidean metrics and variable Gaussian curvature. The old controversy
about the ambiguities involving terms in the Hamiltonian of order hbar^2
multiplying the Gaussian curvature is addressed. We set out to clarify the
matter by considering constraints to be the limits of large restoring forces as
the constraint coordinates deviate from their constrained values. We find
additional ambiguous terms of order hbar^2 involving freedom in the
constraining potentials, demonstrating that the classical constrained
Hamiltonian or Lagrangian cannot uniquely specify the quantization: the
ambiguity of directly quantizing a constrained system is inherently
unresolvable. However, there is never any problem with a physical quantum
system, which cannot have infinite constraint forces and always fluctuates
around the mean constraint values. The issue is addressed from the perspectives
of adiabatic approximations in quantum mechanics, Feynman path integrals, and
semiclassically in terms of adiabatic actions.
### 🌟 论文解读 | 量子力学中约束系统的量子化：新视角

### 📌 背景痛点/本文动机
量子力学中，对约束系统的量子化一直是一个充满争议的话题。传统的量子化方法在处理具有非欧几里得度量和可变高斯曲率的约束表面时，会遇到一些困难。特别是，涉及哈密顿量中与高斯曲率成正比的项的模糊性，一直是量子化过程中的一个难题。本文旨在通过考虑约束为约束坐标偏离其约束值时大恢复力的极限，来澄清这一问题。

### 🚀 核心方法
💡 创新点1：本文提出了一种新的量子化方法，称为“极限量子化”。该方法将约束视为大恢复力的极限，随着约束坐标偏离其约束值，恢复力逐渐增大。这种方法能够有效地解决传统量子化方法中存在的模糊性问题。

💡 创新点2：本文从三个不同的角度分析了约束系统的量子化问题，包括量子力学中的绝热近似、费曼路径积分和半经典绝热作用。这些分析表明，量子化过程中的模糊性是固有的，并且与约束表面的内在几何形状和约束势的细节有关。

### 📈 实验结果
本文通过分析量子波动对约束表面的影响，发现了一些不可避免的量子动力学模糊性。然而，这些模糊性并不影响物理量子系统的行为，因为物理量子系统不能具有无限约束力，并且总是在约束值的平均值附近波动。

### 💬 可借鉴之处
本文提出的新量子化方法为处理具有非欧几里得度量和可变高斯曲率的约束系统提供了一种新的思路。此外，本文的分析方法也可以用于研究其他类型的量子化问题，例如量子场论中的规范固定问题。

## llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models
### Abstract
The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by
Large Language Models (LLMs) make them promising candidates for developing
coordination agents. In this study, we introduce a new LLM-Coordination
Benchmark aimed at a detailed analysis of LLMs within the context of Pure
Coordination Games, where participating agents need to cooperate for the most
gain. This benchmark evaluates LLMs through two distinct tasks: (1)
\emph{Agentic Coordination}, where LLMs act as proactive participants for
cooperation in 4 pure coordination games; (2) \emph{Coordination Question
Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice
questions from the 4 games for evaluation of three key reasoning abilities:
Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to
enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture
for Coordination (CAC) framework that can easily integrate different LLMs as
plug-and-play modules for pure coordination games. Our findings indicate that
LLM agents equipped with GPT-4-turbo achieve comparable performance to
state-of-the-art reinforcement learning methods in games that require
commonsense actions based on the environment. Besides, zero-shot coordination
experiments reveal that, unlike RL methods, LLM agents are robust to new unseen
partners. However, results on Coordination QA show a large room for improvement
in the Theory of Mind reasoning and joint planning abilities of LLMs. The
analysis also sheds light on how the ability of LLMs to understand their
environment and their partner's beliefs and intentions plays a part in their
ability to plan for coordination. Our code is available at
\url{https://github.com/eric-ai-lab/llm_coordination}.
### 🌟 论文解读 | LLM-Coordination：评估和分析大型语言模型的多智能体协调能力

### 📌 背景痛点/本文动机
在许多日常任务和关键操作中，如烹饪和救援行动，合作是至关重要的。这些场景可以被视为纯协调游戏，其中所有参与方都从选择完全一致的战略中受益，避免任何利益冲突。这些游戏要求代理推理他们的环境并计划，同时考虑他们的伙伴的信念和意图。大型语言模型（LLMs）最近在物理和虚拟环境中的涌现规划能力、令人印象深刻的推理能力和对心智理论的暗示，使它们成为开发协调代理的有希望的候选者。然而，LLMs在协调游戏中的必要条件、优势和局限性仍然不清楚。本文旨在通过进行LLMs的多智能体协调能力的全面评估和分析来弥合这一差距。

### 🚀 核心方法
本文提出了一个新的LLM-Coordination基准，旨在对LLMs在纯协调游戏中的能力进行详细分析。该基准通过两个不同的任务评估LLMs：
1. **代理协调**：LLMs作为积极合作参与者参与4个纯协调游戏。
2. **协调问答（QA）**：LLMs被提示回答来自4个游戏的198个多项选择题，以评估三个关键推理能力：环境理解、心智理论推理和联合规划。

此外，为了使LLMs能够进行多智能体协调，本文引入了一个协调认知架构（CAC）框架，该框架可以轻松地将不同的LLMs作为即插即用模块集成到纯协调游戏中。

### 📈 实验结果
实验结果表明，配备GPT-4-turbo的LLM代理在需要基于环境的常识行动的游戏中，其性能与最先进的强化学习方法相当。此外，零样本协调实验表明，与RL方法不同，LLM代理对新未见伙伴具有鲁棒性。然而，协调QA的结果表明，LLMs的心智理论推理和联合规划能力还有很大的改进空间。分析还揭示了LLMs理解其环境和其伙伴的信念和意图的能力如何影响它们协调计划的能力。

### 💬 可借鉴之处
本文提出的LLM-Coordination基准和CAC框架为评估和分析LLMs的多智能体协调能力提供了一个有价值的工具。此外，本文的结果突出了LLMs在协调任务中的优势和局限性，并为未来研究提供了有价值的见解。

## adarefiner--refining-decisions-of-language-models-with-adaptive-feedback
### Abstract
Large Language Models (LLMs) have demonstrated significant success across
various domains. However, their application in complex decision-making tasks
frequently necessitates intricate prompt engineering or fine-tuning, leading to
challenges in unseen downstream tasks and heavy demands on computational
resources. Meanwhile, Reinforcement Learning (RL) has been recognized as
effective in decision-making problems but struggles in environments with sparse
rewards, such as open-world games. To overcome these challenges, we introduce
AdaRefiner, a novel framework designed to enhance the synergy between LLMs and
RL feedback. The key component of AdaRefiner is a lightweight Adapter Language
Model (LM), which automatically refines task comprehension based on feedback
from RL agents. This method mitigates the need for intricate prompt engineering
and intensive LLM fine-tuning while maintaining the LLMs' generalization
abilities and enhancing their decision-making capabilities in downstream tasks.
Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world
game Crafter have demonstrated its superior effectiveness, especially in
guiding agents towards higher-level and common-sense skills. Our work makes
contributions to the automatic self-refinement of LLMs with RL feedback,
offering a more adaptable and efficient solution for complex decision-making
problems.
### 🌟 论文解读 | AdaRefiner：利用自适应反馈提升语言模型决策能力

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在各个领域取得了显著的成功，但在复杂决策任务中的应用却面临着挑战。LLMs 需要进行繁琐的提示工程或微调才能适应特定任务，这限制了其在未知下游任务中的泛化能力，并带来了对计算资源的巨大需求。另一方面，强化学习（RL）在决策问题中表现出色，但在稀疏奖励的环境中（如开放世界游戏）却难以发挥作用。为了克服这些挑战，本文提出了 AdaRefiner，一个旨在增强 LLMs 和 RL 反馈之间协同作用的新框架。

### 🚀 核心方法
💡 创新点1：AdaRefiner 引入了一个轻量级的适配器语言模型（LM），该模型根据 RL 代理的反馈自动细化任务理解。这种方法减少了繁琐的提示工程和密集的 LLM 微调的需求，同时保持了 LLMs 的泛化能力，并增强了它们在下游任务中的决策能力。

💡 创新点2：AdaRefiner 在开放世界游戏 Crafter 的 22 个不同任务中进行了实证评估，结果表明其在引导代理学习高级和常识技能方面具有优越的有效性。

### 📈 实验结果
AdaRefiner 在 Crafter 环境中的 22 个任务上进行了评估，结果表明其性能优于最先进的基线方法。AdaRefiner 能够引导代理学习高级技能，并表现出常识行为。消融研究表明，适配器 LM 和 RL 反馈对于 AdaRefiner 的有效性至关重要。

### 💬 可借鉴之处
AdaRefiner 为 LLMs 在复杂决策任务中的应用提供了一种更灵活和高效的解决方案。其轻量级的适配器 LM 和自适应反馈机制可以有效地提升 LLMs 的任务理解和决策能力，为 LLMs 在开放世界游戏等复杂环境中的应用开辟了新的可能性。

## adapt--as-needed-decomposition-and-planning-with-language-models
### Abstract
Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
### 🌟 论文解读 | ADaPT：按需分解与规划，提升大型语言模型在复杂任务中的表现

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们也逐渐被应用于需要规划和适应环境的交互式决策任务中。然而，现有的方法在处理复杂任务时面临着挑战，因为LLMs在执行子任务时可能会失败，从而导致整个任务的失败。

### 🚀 核心方法
为了解决上述问题，本文提出了ADaPT（As-Needed Decomposition and Planning for complex Tasks），一种按需分解和规划复杂任务的方法。ADaPT的核心思想是，当LLM作为执行者无法执行子任务时，将其分解为更小的子任务，并递归地进行分解，以适应任务的复杂性和LLM的能力。

#### 💡 创新点1：按需分解
ADaPT通过递归地分解子任务，动态地适应任务的复杂性和LLM的能力。当LLM作为执行者无法执行子任务时，它会调用LLM作为规划者来生成更小的子任务，并递归地调用ADaPT来执行这些子任务。

#### 💡 创新点2：多级分解
ADaPT支持多级分解，这意味着它可以进一步分解子任务，直到它们变得足够简单，可以被LLM作为执行者成功执行。这种多级分解的能力使得ADaPT能够处理更复杂的任务，并提高任务的成功率。

### 📈 实验结果
在ALFWorld、WebShop和TextCraft三个数据集上进行的实验结果表明，ADaPT显著优于现有的强基线方法，在ALFWorld上提高了28.3%的成功率，在WebShop上提高了27%，在TextCraft上提高了33%。

### 💬 可借鉴之处
ADaPT提供了一种有效的方法来处理LLMs在复杂任务中的执行失败问题。它通过按需分解和规划，动态地适应任务的复杂性和LLM的能力，从而提高了任务的成功率。此外，ADaPT的多级分解能力使其能够处理更复杂的任务，并提高任务的成功率。

## swiftsage--a-generative-agent-with-fast-and-slow-thinking-for-complex-interactive-tasks
### Abstract
We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.
### 🌟 论文解读 | SwiftSage：结合快慢思考的生成式智能体，解决复杂交互任务

### 📌 背景痛点/本文动机
随着人工智能的发展，智能体在复杂交互推理任务中的能力越来越受到关注。这类任务要求智能体具备长期规划、记忆、子目标分解、空间推理、异常处理和常识知识等能力。然而，现有的方法，如强化学习、行为克隆和大型语言模型（LLM）提示，在处理复杂任务时存在局限性。例如，强化学习需要大量的交互来学习，行为克隆难以泛化到未见过的任务，而LLM提示则缺乏对环境的具体操作指导。

### 🚀 核心方法
SwiftSage 提出了一个新颖的智能体框架，灵感来源于人类认知的双过程理论，旨在解决复杂交互推理任务。该框架结合了行为克隆和LLM提示的优势，以提高任务完成性能。

#### 💡 创新点1：双模块设计
SwiftSage 由两个主要模块组成：
- **Swift 模块**：代表快速和直观的思考，是一个小型的编码器-解码器语言模型，通过模仿专家智能体的行为轨迹进行微调。
- **Sage 模块**：代表深思熟虑的思考，利用LLM（如GPT-4）进行子目标规划和接地。

#### 💡 创新点2：模块集成策略
SwiftSage 开发了一种启发式方法来和谐地集成这两个模块，根据任务需求动态切换模块，并有效结合它们的输出。例如，当遇到异常情况或需要长期规划时，Sage 模块会被激活，而 Swift 模块则用于快速响应和执行简单任务。

### 📈 实验结果
在 ScienceWorld 基准测试的30个任务中，SwiftSage 显著优于其他方法，如SayCan、ReAct和Reflexion，证明了其在解决复杂交互任务方面的有效性。SwiftSage 实现了最先进的平均得分84.7，而其他方法的得分分别为33.8、36.4和45.3。此外，SwiftSage 更加经济高效，每项操作所需的LLM令牌数量远少于之前的方法。

### 💬 可借鉴之处
SwiftSage 的双模块设计为解决复杂交互推理任务提供了一种新的思路。其模块集成策略和启发式算法可以应用于其他需要快速响应和深思熟虑的任务中。此外，SwiftSage 的成功也表明了结合小型语言模型和LLM在复杂推理任务中的潜力。

## language-models-as-zero-shot-planners--extracting-actionable-knowledge-for-embodied-agents
### Abstract
Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. "make
breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner
### 🌟 论文解读 | 语言模型作为零样本规划器：为具身智能体提取可操作知识

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在语言生成和理解方面取得了显著进展。这些模型在训练过程中学习了大量的世界知识，但如何将这些知识应用于交互式环境中的具身智能体，使其能够执行高层次的指令（例如“做早餐”）仍然是一个挑战。现有的研究主要集中在从明确的步骤示例中学习如何行动，而本文则探索了利用预训练的LLMs直接生成可执行行动计划的潜力。

### 🚀 核心方法
💡 创新点1：零样本规划
本文发现，如果预训练的LLMs足够大，并且以适当的方式进行提示，它们可以有效地将高层次的任务分解为可执行的中间步骤，而无需进一步的训练。这种方法被称为“零样本规划”，因为它不需要针对特定环境进行微调。

💡 创新点2：语义翻译和可执行性提升
然而，LLMs生成的计划往往无法精确映射到环境中的可接受动作。为了解决这个问题，本文提出了一种基于现有演示的语义翻译过程，将LLMs生成的计划转换为可接受的动作。此外，还引入了自回归轨迹校正和动态示例选择等技术，以进一步提高计划的执行性。

### 📈 实验结果
在VirtualHome环境中进行的评估表明，该方法显著提高了计划的执行性，但正确性有所下降。这表明，从语言模型中提取可操作知识是一个有前景的方向，但仍需进一步研究以平衡执行性和正确性。

### 💬 可借鉴之处
本文提出的零样本规划方法为利用LLMs的知识进行具身智能体的决策提供了新的思路。此外，语义翻译和可执行性提升技术也为提高LLMs生成的计划的实用性提供了有效的方法。这些发现对于未来研究如何将LLMs的知识应用于交互式环境中的具身智能体具有重要的启示意义。

## software-agents-interaction-algorithms-in-virtual-learning-environment
### Abstract
This paper highlights the multi-agent learning virtual environment and agents
communication algorithms. The researcher proposed three algorithms required
software agents interaction in virtual learning information system environment.
The first proposed algorithm is agents interaction localization algorithm, the
second one is the dynamic agents distribution algorithm (load distribution
algorithm), and the third model is Agent communication algorithm based on using
agents intermediaries. The main objectives of these algorithms are to reduce
the response time for any agents changes in virtual learning environment (VLE)
by increasing the information exchange intensity between software agents and
reduce the overall network load, and to improve the communication between
mobile agents in distributed information system to support effectiveness.
Finally the paper describe the algorithms of information exchange between
mobile agents in VLE based on the expansion of the address structure and the
use of an agent, intermediary agents, matchmaking agents, brokers and their
entrepreneurial functions
### 🌟 论文解读 | 虚拟学习环境中软件代理交互算法的革新

### 📌 背景痛点/本文动机
随着信息技术的飞速发展，虚拟学习环境（VLE）已经成为教育领域的重要工具。然而，现有的VLE系统在处理大量信息交互时，面临着网络负载增加、信息交换效率低下等问题。为了解决这些问题，本文提出了一种基于软件代理的交互算法，旨在提高VLE系统的效率和响应速度。

### 🚀 核心方法
💡 创新点1：代理交互定位算法
该算法通过分析代理之间的通信依赖性，将频繁交互的代理分组到同一主机上，从而将跨主机的交互转化为主机内的交互，减少网络负载并提高信息交换效率。

💡 创新点2：动态代理分配算法（负载分配算法）
该算法通过监控主机负载，将代理分组并动态分配到不同的主机上，以实现负载均衡，避免某些主机过载，从而提高系统的稳定性和性能。

💡 创新点3：基于代理中介的通信模型
该模型利用代理中介（如经纪人代理和配对代理）来促进代理之间的通信。代理中介可以帮助代理查找具有相似兴趣的代理，并提供消息转发和匹配服务，从而提高通信效率和灵活性。

### 📈 实验结果
本文提出的算法在虚拟学习环境中进行了实验验证，结果表明，这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。

### 💬 可借鉴之处
本文提出的算法和模型为虚拟学习环境的设计和优化提供了新的思路和方法。其中，代理交互定位算法和动态代理分配算法可以应用于其他分布式系统中，以提高系统的效率和性能。基于代理中介的通信模型可以应用于其他多代理系统中，以促进代理之间的协作和通信。

### 📚 总结
本文提出的基于软件代理的交互算法为虚拟学习环境的设计和优化提供了新的思路和方法。这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。本文的研究成果对于虚拟学习环境和其他分布式系统的设计和优化具有重要的参考价值。

## mindagent--emergent-gaming-interaction
### Abstract
Large Language Models (LLMs) have the capacity of performing complex
scheduling in a multi-agent system and can coordinate these agents into
completing sophisticated tasks that require extensive collaboration. However,
despite the introduction of numerous gaming frameworks, the community has
insufficient benchmarks towards building general multi-agents collaboration
infrastructure that encompass both LLM and human-NPCs collaborations. In this
work, we propose a novel infrastructure - MindAgent - to evaluate planning and
coordination emergent capabilities for gaming interaction. In particular, our
infrastructure leverages existing gaming framework, to i) require understanding
of the coordinator for a multi-agent system, ii) collaborate with human players
via un-finetuned proper instructions, and iii) establish an in-context learning
on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new
gaming scenario and related benchmark that dispatch a multi-agent collaboration
efficiency and supervise multiple agents playing the game simultaneously. We
conduct comprehensive evaluations with new auto-metric CoS for calculating the
collaboration efficiency. Finally, our infrastructure can be deployed into
real-world gaming scenarios in a customized VR version of CUISINEWORLD and
adapted in existing broader Minecraft gaming domain. We hope our findings on
LLMs and the new infrastructure for general-purpose scheduling and coordination
can help shed light on how such skills can be obtained by learning from large
language corpora.
### 🌟 论文解读 | MindAgent：大型语言模型在游戏交互中的涌现式规划与协调能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在各个领域的应用日益广泛，其在多智能体系统中的规划与协调能力也逐渐受到关注。然而，现有的游戏框架和基准测试还不足以评估LLMs在游戏交互中的涌现式规划与协调能力，尤其是在LLMs与人类NPCs协作的场景下。本文旨在解决这个问题，提出了一种名为MindAgent的新型基础设施，用于评估LLMs在游戏交互中的规划与协调能力。

### 🚀 核心方法
💡 创新点1：CUISINEWORLD游戏场景与基准测试
本文设计了一个名为CUISINEWORLD的游戏场景，模拟了一个虚拟厨房环境，其中多智能体系统需要协调多个代理，完成尽可能多的菜肴订单。CUISINEWORLD游戏场景具有多种任务结构和难度，是评估LLMs涌现式多智能体规划能力的理想测试平台。

💡 创新点2：MindAgent基础设施
MindAgent是一个用于LLMs交互式多智能体规划的基础设施，它展示了LLMs的涌现式多智能体规划能力，并引入了多种提示技术，以促进LLMs的规划能力，包括提供少量示例、规划理由和环境反馈。

### 📈 实验结果
本文在CUISINEWORLD游戏场景中进行了广泛的实验，结果表明：
1. 零样本多智能体规划：强大的预训练LLMs（如GPT-4）能够通过阅读简单的游戏指令和食谱，调度多个代理（2到4个）完成菜肴，甚至与人类玩家协作。
2. 基于高级提示的规划：通过利用涌现式上下文学习能力，可以显著提高LLMs的多智能体规划性能。例如，添加少量专家演示、解释某些行动的理由，以及在规划过程中提供实时反馈。
3. 通用潜力：LLMs表现出成为通用多智能体规划器的巨大潜力，因为它能够通过少量示例泛化到更多代理，并适应新的游戏领域，如Minecraft。

### 💬 可借鉴之处
本文提出的MindAgent基础设施和CUISINEWORLD游戏场景为评估LLMs在游戏交互中的涌现式规划与协调能力提供了新的思路和方法。此外，本文的研究结果也表明，LLMs在多智能体规划方面具有巨大的潜力，有望在未来推动游戏AI的发展。

## skill-reinforcement-learning-and-planning-for-open-world-long-horizon-tasks
### Abstract
We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that
performs exploration to find diverse items provides better initialization for
other skills, improving the sample efficiency for skill learning. In skill
planning, we leverage the prior knowledge in Large Language Models to find the
relationships between skills and build a skill graph. When the agent is solving
a task, our skill search algorithm walks on the skill graph and generates the
proper skill plans for the agent. In experiments, our method accomplishes 40
diverse Minecraft tasks, where many tasks require sequentially executing for
more than 10 skills. Our method outperforms baselines by a large margin and is
the most sample-efficient demonstration-free RL method to solve Minecraft Tech
Tree tasks. The project's website and code can be found at
https://sites.google.com/view/plan4mc.
### 🌟 论文解读 | Plan4MC：基于技能强化学习和规划的开放世界长时任务解决方案

### 📌 背景痛点/本文动机
在开放世界环境中，学习完成长时任务对于强化学习（RL）来说是非常低效的。这是因为开放世界环境通常具有无限大的世界规模和大量的任务，这使得探索和样本效率成为主要挑战。此外，长时任务通常具有多个子目标，需要大量的环境步骤才能完成。

### 🚀 核心方法
💡 创新点1：将多任务学习问题转化为学习基本技能和技能规划。在Minecraft游戏中，我们提出了三种细粒度基本技能：寻找技能、操作技能和制作技能。使用具有内在奖励的RL来获取技能，并通过技能规划来分解任务。

💡 创新点2：利用大型语言模型（LLM）的先验知识来构建技能图，并通过技能搜索算法来生成正确的技能序列。这种方法避免了LLM不可控的错误，并提高了规划精度。

### 📈 实验结果
在MineDojo模拟器中构建了40个多样化的任务，结果表明Plan4MC能够完成所有任务，并且显著优于基线方法。此外，Plan4MC在Minecraft Tech Tree任务中表现出更高的样本效率。

### 💬 可借鉴之处
Plan4MC提出了一种高效解决开放世界长时任务的方法，通过学习基本技能和技能规划来提高样本效率。此外，利用LLM构建技能图和技能搜索算法为开放世界任务规划提供了一种新的思路。

## on-the-utility-of-learning-about-humans-for-human-ai-coordination
### Abstract
While we would like agents that can coordinate with humans, current
algorithms such as self-play and population-based training create agents that
can coordinate with themselves. Agents that assume their partner to be optimal
or similar to them can converge to coordination protocols that fail to
understand and be understood by humans. To demonstrate this, we introduce a
simple environment that requires challenging coordination, based on the popular
game Overcooked, and learn a simple model that mimics human play. We evaluate
the performance of agents trained via self-play and population-based training.
These agents perform very well when paired with themselves, but when paired
with our human model, they are significantly worse than agents designed to play
with the human model. An experiment with a planning algorithm yields the same
conclusion, though only when the human-aware planner is given the exact human
model that it is playing with. A user study with real humans shows this pattern
as well, though less strongly. Qualitatively, we find that the gains come from
having the agent adapt to the human's gameplay. Given this result, we suggest
several approaches for designing agents that learn about humans in order to
better coordinate with them. Code is available at
https://github.com/HumanCompatibleAI/overcooked_ai.
### 🌟 论文解读 | 人工智能与人类协作：学习人类行为的重要性

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，我们希望AI能够与人类进行有效的协作，共同完成任务。然而，目前许多训练AI的方法，如自我博弈和基于群体的训练，往往导致AI只能与自己协作，而无法理解或被人类理解。本文旨在探讨在训练过程中学习人类行为的重要性，以实现更有效的AI与人类协作。

### 🚀 核心方法
💡 创新点1：构建基于游戏Overcooked的协作环境
为了验证学习人类行为的重要性，本文构建了一个基于游戏Overcooked的协作环境，该环境要求玩家进行复杂的协作才能完成任务。通过这个环境，研究人员可以评估不同训练方法的AI与人类模型的协作效果。

💡 创新点2：比较不同训练方法的AI与人类模型的协作效果
本文比较了以下几种训练方法的AI与人类模型的协作效果：
- 自我博弈（Self-Play）：AI与自身进行博弈，学习如何与自己协作。
- 基于群体的训练（Population-Based Training, PBT）：AI与群体中的其他AI进行博弈，学习如何与群体中的其他AI协作。
- 耦合规划（Coupled Planning）：AI与人类模型进行耦合规划，学习如何与人类模型协作。
- 行为克隆（Behavior Cloning）：AI通过模仿人类的行为来学习如何与人类协作。

### 📈 实验结果
实验结果表明，未利用人类数据训练的AI在与人类模型协作时表现较差，而利用人类数据训练的AI则表现更好。具体来说，与自我博弈和基于群体的训练相比，利用行为克隆训练的AI在与人类模型协作时表现更佳。此外，利用规划或强化学习来最大化协作奖励的AI也表现出更好的协作效果。

### 💬 可借鉴之处
本文的研究结果表明，在训练AI进行协作时，考虑人类行为的重要性。为了实现更有效的AI与人类协作，可以采取以下几种方法：
- 使用行为克隆或其他模仿学习方法来学习人类的行为。
- 利用规划或强化学习来最大化协作奖励。
- 设计更复杂的AI模型，使其能够更好地理解人类的行为和意图。
- 在测试时让AI适应人类的行为，例如使用元学习算法来快速适应新的协作伙伴。

### 📚 参考文献
[1] Ng, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning (pp. 663-670).

[2] Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey. Journal of artificial intelligence research, 4, 237-285.

[3] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484-489.

[4] Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... & Horgan, D. (2017). StarCraft II: A New Challenge for Reinforcement Learning. arXiv preprint arXiv:1708.04782.

[5] Tampuu, A., Matiisen, T., Kuzovkin, I., Arjakov, D., Maini, J., & Rucklidge, W. J. (2017). Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1706.02275.

[6] Wang, Z., Schaul, T., Hessel, M., Hasselt, H. V., Lanctot, M., & Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581.

[7] Choudhury, R., Dragan, A., & Seshia, S. A. (2018). Learning human models from demonstration. arXiv preprint arXiv:1804.04287.

[8] Lerer, A., & Peysakhovich, A. (2018). Learning to play against opponents with unknown strategies. arXiv preprint arXiv:1805.09358.

[9] Wang, T., & Tamar, A. (2018). Learning to communicate with deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.01262.

[10] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018). Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1802.09416.

[11] Finn, C., Abbeel, P., & Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1126-1135).

[12] Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Imagineer: A general framework for imagining and planning. arXiv preprint arXiv:1806.05696.

[13] Overcooked. (2016). Ghost Town Games.

## text-based-adventures-of-the-golovin-ai-agent
### Abstract
The domain of text-based adventure games has been recently established as a
new challenge of creating the agent that is both able to understand natural
language, and acts intelligently in text-described environments.
  In this paper, we present our approach to tackle the problem. Our agent,
named Golovin, takes advantage of the limited game domain. We use genre-related
corpora (including fantasy books and decompiled games) to create language
models suitable to this domain. Moreover, we embed mechanisms that allow us to
specify, and separately handle, important tasks as fighting opponents, managing
inventory, and navigating on the game map.
  We validated usefulness of these mechanisms, measuring agent's performance on
the set of 50 interactive fiction games. Finally, we show that our agent plays
on a level comparable to the winner of the last year Text-Based Adventure AI
Competition.
### 🌟 论文解读 | Golovin AI Agent：文本冒险游戏中的自然语言理解和智能行动

### 📌 背景痛点/本文动机
文本冒险游戏为人工智能领域带来了新的挑战：如何创建一个既能理解自然语言，又能在文本描述的环境中智能行动的智能体。现有的解决方案通常依赖于对游戏规则的分析和特定领域的特征利用，但缺乏对自然语言理解的深入探索。

### 🚀 核心方法
💡 创新点1：利用有限的游戏领域
Golovin AI Agent 利用文本冒险游戏的有限领域，通过分析相关语料库（包括奇幻书籍和反编译游戏）来创建适合该领域的语言模型。

💡 创新点2：嵌入特定机制
Golovin AI Agent 嵌入了特定机制，以分别处理重要的任务，如与对手战斗、管理库存和在游戏地图上导航。

💡 创新点3：利用游戏特定行为
Golovin AI Agent 利用游戏特定行为，如战斗模式、装备管理和移动策略，以提高其在游戏中的表现。

💡 创新点4：记忆和利用游戏历史
Golovin AI Agent 记忆并利用游戏历史的一些方面，以更好地理解游戏环境和做出决策。

💡 创新点5：模仿人类行为
Golovin AI Agent 尝试模仿人类行为，例如在探索游戏宇宙后重复最有希望的命令序列。

### 📈 实验结果
Golovin AI Agent 在 50 个交互式小说游戏上的表现进行了验证，结果显示其表现与去年文本冒险 AI 竞赛的获胜者相当。

### 💬 可借鉴之处
Golovin AI Agent 的设计思路和方法为文本冒险游戏中的自然语言理解和智能行动提供了新的思路，可以借鉴到其他类似场景中，例如虚拟助手、聊天机器人等。

## interactive-fiction-games--a-colossal-adventure
### Abstract
A hallmark of human intelligence is the ability to understand and communicate
with language. Interactive Fiction games are fully text-based simulation
environments where a player issues text commands to effect change in the
environment and progress through the story. We argue that IF games are an
excellent testbed for studying language-based autonomous agents. In particular,
IF games combine challenges of combinatorial action spaces, language
understanding, and commonsense reasoning. To facilitate rapid development of
language-based agents, we introduce Jericho, a learning environment for
man-made IF games and conduct a comprehensive study of text-agents across a
rich set of games, highlighting directions in which agents can improve.
### 🌟 论文解读 | 交互式小说游戏：一场巨大的冒险

### 📌 背景痛点/本文动机
人类智能的一个显著特征是理解和用语言进行交流的能力。交互式小说（IF）游戏是完全基于文本的模拟环境，玩家通过发出文本命令来改变环境并推动故事的发展。本文认为，IF游戏是研究基于语言的自主代理的绝佳测试平台。特别是，IF游戏结合了组合动作空间、语言理解和常识推理的挑战。为了促进基于语言的代理的快速开发，本文介绍了Jericho，这是一个用于人工IF游戏的学习环境，并在一系列丰富的游戏中对文本代理进行了全面的研究，突出了代理可以改进的方向。

### 🚀 核心方法
💡 创新点1：Jericho环境
Jericho是一个开源的Python-based IF环境，它提供了一个类似于OpenAI-Gym的接口，使学习代理能够连接到IF游戏。Jericho旨在用于强化学习代理，但也支持加载和保存游戏状态的功能，使规划算法如蒙特卡洛树搜索以及依赖于恢复状态的强化学习方法如Backplay和GoExplore成为可能。

💡 创新点2：基于模板的动作生成
本文引入了一种基于模板的动作空间，其中代理首先选择一个动作模板（例如，将_放入_），然后使用解析器的词汇表中的单词填写空白。这种模板化的动作空间大大减少了动作空间的复杂性，使得语言生成变得更加容易。

### 📈 实验结果
本文评估了DRRN、TDQN和NAIL三种代理在32个Jericho支持的游戏上的表现。结果表明，强化学习在许多不同的IF游戏中是可行的。与随机代理相比，DRRN和TDQN在游戏中的得分更高，这表明了语言生成的重要性。然而，与NAIL相比，DRRN和TDQN在游戏中的得分仍然较低，这表明了工程一个通用的IF代理的难度以及从数据中学习策略的潜力。

### 💬 可借鉴之处
本文提出的Jericho环境和基于模板的动作生成方法为研究基于语言的自主代理提供了新的思路。此外，本文对DRRN、TDQN和NAIL三种代理的评估结果为未来研究提供了有价值的参考。

## llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination
### Abstract
AI agents powered by Large Language Models (LLMs) have made significant
advances, enabling them to assist humans in diverse complex tasks and leading
to a revolution in human-AI coordination. LLM-powered agents typically require
invoking LLM APIs and employing artificially designed complex prompts, which
results in high inference latency. While this paradigm works well in scenarios
with minimal interactive demands, such as code generation, it is unsuitable for
highly interactive and real-time applications, such as gaming. Traditional
gaming AI often employs small models or reactive policies, enabling fast
inference but offering limited task completion and interaction abilities. In
this work, we consider Overcooked as our testbed where players could
communicate with natural language and cooperate to serve orders. We propose a
Hierarchical Language Agent (HLA) for human-AI coordination that provides both
strong reasoning abilities while keeping real-time execution. In particular,
HLA adopts a hierarchical framework and comprises three modules: a proficient
LLM, referred to as Slow Mind, for intention reasoning and language
interaction, a lightweight LLM, referred to as Fast Mind, for generating macro
actions, and a reactive policy, referred to as Executor, for transforming macro
actions into atomic actions. Human studies show that HLA outperforms other
baseline agents, including slow-mind-only agents and fast-mind-only agents,
with stronger cooperation abilities, faster responses, and more consistent
language communications.
### 🌟 论文解读 | 基于大型语言模型的分层语言代理：实时人机协作的突破

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的兴起，基于LLMs的AI代理在辅助人类完成复杂任务方面取得了显著进展，推动了人机协作的革命。然而，这些代理通常需要调用LLMs API并使用人工设计的复杂提示，导致推理延迟高。这种范式在交互需求较低的场景（如代码生成）中表现良好，但在需要实时响应和高频交互的应用（如游戏）中并不适用。传统的游戏AI通常采用小型模型或反应策略，虽然能够实现快速推理，但任务完成和交互能力有限。

### 🚀 核心方法
本文提出了一个分层语言代理（HLA），用于实时人机协作，该代理结合了大型模型的强大推理和交互能力以及小型模型和反应策略的实时推理能力。HLA采用分层框架，由三个模块组成：

* **慢思维（Slow Mind）**：一个熟练的LLM，用于意图推理和语言交互。
* **快思维（Fast Mind）**：一个轻量级的LLM，用于生成宏操作。
* **执行器（Executor）**：一个反应策略，用于将宏操作转换为原子操作。

### 📈 实验结果
在Overcooked游戏平台上进行的实验表明，HLA在游戏得分、响应延迟和人类偏好方面均优于其他基线代理，包括仅使用慢思维或快思维的代理。HLA展现出更强的协作能力、更快的响应速度和更一致的语言通信。

### 💬 可借鉴之处
* **分层设计**：HLA的分层设计有效地解决了LLMs推理延迟高的问题，使其适用于实时人机协作场景。
* **轻量级LLM**：使用轻量级LLM进行宏操作生成，提高了推理速度，同时避免了生成次优操作。
* **反应策略**：执行器模块确保了动作的可行性和高频交互，提高了AI代理的实时响应能力。

### 🌟 总结
HLA为实时人机协作提供了一种新的解决方案，其分层设计有效地解决了LLMs推理延迟高的问题，使其能够在需要实时响应和高频交互的场景中发挥作用。HLA在人机协作领域的应用前景广阔，有望推动人机协作的进一步发展。

## q-cogni--an-integrated-causal-reinforcement-learning-framework
### Abstract
We present Q-Cogni, an algorithmically integrated causal reinforcement
learning framework that redesigns Q-Learning with an autonomous causal
structure discovery method to improve the learning process with causal
inference. Q-Cogni achieves optimal learning with a pre-learned structural
causal model of the environment that can be queried during the learning process
to infer cause-and-effect relationships embedded in a state-action space. We
leverage on the sample efficient techniques of reinforcement learning, enable
reasoning about a broader set of policies and bring higher degrees of
interpretability to decisions made by the reinforcement learning agent. We
apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against
state-of-the-art reinforcement learning algorithms. We report results that
demonstrate better policies, improved learning efficiency and superior
interpretability of the agent's decision making. We also compare this approach
with traditional shortest-path search algorithms and demonstrate the benefits
of our causal reinforcement learning framework to high dimensional problems.
Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New
York City using the Taxi & Limousine Commission trip record data and compare
with shortest-path search, reporting results that show 85% of the cases with an
equal or better policy derived from Q-Cogni in a real-world domain.


## virtualhome--simulating-household-activities-via-programs
### Abstract
In this paper, we are interested in modeling complex activities that occur in
a typical household. We propose to use programs, i.e., sequences of atomic
actions and interactions, as a high level representation of complex tasks.
Programs are interesting because they provide a non-ambiguous representation of
a task, and allow agents to execute them. However, nowadays, there is no
database providing this type of information. Towards this goal, we first
crowd-source programs for a variety of activities that happen in people's
homes, via a game-like interface used for teaching kids how to code. Using the
collected dataset, we show how we can learn to extract programs directly from
natural language descriptions or from videos. We then implement the most common
atomic (inter)actions in the Unity3D game engine, and use our programs to
"drive" an artificial agent to execute tasks in a simulated household
environment. Our VirtualHome simulator allows us to create a large activity
video dataset with rich ground-truth, enabling training and testing of video
understanding models. We further showcase examples of our agent performing
tasks in our VirtualHome based on language descriptions.
### 🌟 论文解读 | VirtualHome：通过程序模拟家庭活动

### 📌 背景痛点/本文动机
随着人工智能和机器人技术的发展，让机器人执行复杂的家庭活动成为可能。然而，如何有效地描述和执行这些活动仍然是一个挑战。本文提出了一个名为 VirtualHome 的模拟器，旨在通过程序来模拟家庭活动，从而为机器人执行复杂任务提供一种新的方法。

### 🚀 核心方法
💡 创新点1：构建家庭活动知识库
本文首先通过众包的方式收集了大量的家庭活动描述，并将其转化为程序形式。这些程序包含了执行任务所需的全部步骤，包括一些常识性步骤，从而为机器人提供了清晰的执行指南。

💡 创新点2：开发 VirtualHome 模拟器
本文开发了一个名为 VirtualHome 的 3D 模拟器，可以模拟家庭环境中的各种活动。通过将程序输入到 VirtualHome 中，可以生成丰富的活动视频数据集，并用于训练和测试视频理解模型。

💡 创新点3：从文本和视频中生成程序
本文提出了一个基于 seq2seq 模型的方法，可以从自然语言描述或视频演示中自动生成程序。这使得机器人可以通过自然语言或视频演示来学习执行新的任务。

### 📈 实验结果
本文在 VirtualHome 模拟器上进行了实验，结果表明，从文本和视频中生成的程序可以有效地驱动机器人执行各种家庭活动。此外，本文还进行了一项人类研究，结果表明，生成的程序与人类对活动的理解具有较高的相关性。

### 💬 可借鉴之处
本文提出的 VirtualHome 模拟器和程序生成方法为机器人执行复杂任务提供了一种新的思路。此外，本文收集的家庭活动知识库和视频数据集也为相关研究提供了宝贵的资源。

## grounding-language-with-visual-affordances-over-unstructured-data
### Abstract
Recent works have shown that Large Language Models (LLMs) can be applied to
ground natural language to a wide variety of robot skills. However, in
practice, learning multi-task, language-conditioned robotic skills typically
requires large-scale data collection and frequent human intervention to reset
the environment or help correcting the current policies. In this work, we
propose a novel approach to efficiently learn general-purpose
language-conditioned robot skills from unstructured, offline and reset-free
data in the real world by exploiting a self-supervised visuo-lingual affordance
model, which requires annotating as little as 1% of the total data with
language. We evaluate our method in extensive experiments both in simulated and
real-world robotic tasks, achieving state-of-the-art performance on the
challenging CALVIN benchmark and learning over 25 distinct visuomotor
manipulation tasks with a single policy in the real world. We find that when
paired with LLMs to break down abstract natural language instructions into
subgoals via few-shot prompting, our method is capable of completing
long-horizon, multi-tier tasks in the real world, while requiring an order of
magnitude less data than previous approaches. Code and videos are available at
http://hulc2.cs.uni-freiburg.de
### 🌟 论文解读 | HULC++：利用视觉语言亲和力模型高效学习语言条件机器人技能

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在将自然语言与机器人技能相结合方面取得了显著进展。然而，在实践中，学习多任务、语言条件的机器人技能通常需要大规模的数据收集和频繁的人工干预来重置环境或帮助纠正当前策略。本文提出了一种新颖的方法，通过利用自监督的视觉语言亲和力模型，从现实世界中的非结构化、离线和无需重置的数据中高效地学习通用语言条件的机器人技能，这只需要对总数据中的1%进行语言标注。

### 🚀 核心方法
💡 创新点1：HULC++ 架构
本文提出了 HULC++ 架构，它结合了 HULC 的任务无关控制和 VAPO 的以对象为中心的语义理解。HULC 是一种最先进的语言条件模仿学习代理，可以端到端地学习 7-DoF 目标达到策略。VAPO 从非结构化数据中提取自监督的视觉亲和力模型，不仅可以加速学习，还可以提高下游控制策略的泛化能力。

💡 创新点2：自监督视觉语言亲和力模型
本文提出了一种自监督的视觉语言亲和力模型，可以从非结构化的人类远程操作数据中自动提取亲和力。该模型利用抓取动作作为启发式方法，发现场景中与任务完成相关的元素。通过将末端执行器世界位置投影到相机图像中，并使用抓取动作作为监督，模型可以学习预测与任务相关的对象的像素位置。

💡 创新点3：多任务 7-DoF 语言条件视觉运动策略
本文提出了一种多任务 7-DoF 语言条件视觉运动策略，该策略基于 HULC 并从非结构化数据中训练。该策略可以在预测的亲和力区域附近与场景进行交互。

💡 创新点4：与 LLMs 的结合
本文提出了一种将 HULC++ 与 LLMs 结合的方法，以将抽象的自然语言指令分解为一系列可行的子任务。LLMs 可以通过少量样本提示将抽象的自然语言指令翻译成一系列子目标，从而实现长距离、多级任务的执行。

### 📈 实验结果
本文在模拟和现实世界的机器人任务中进行了广泛的实验，结果表明，HULC++ 在具有挑战性的 CALVIN 基准测试中取得了最先进的性能，并使用单个策略在现实世界中学习了超过 25 个不同的视觉运动操作任务。此外，当与 LLMs 结合使用时，HULC++ 能够在现实世界中完成长距离、多级任务，而所需的数据量比以前的方法少一个数量级。

### 💬 可借鉴之处
本文提出的 HULC++ 架构和自监督视觉语言亲和力模型为高效学习语言条件机器人技能提供了一种新颖的方法。该方法可以应用于各种机器人任务，并具有以下优势：
- **数据效率高**：只需要对总数据中的1%进行语言标注。
- **无需重置环境**：可以从非结构化、离线和无需重置的数据中学习。
- **泛化能力强**：在模拟和现实世界的机器人任务中取得了最先进的性能。
- **可扩展性强**：可以与 LLMs 结合使用，以完成更复杂的任务。

### 🌟 总结
HULC++ 是一种很有前景的方法，可以高效地学习语言条件机器人技能。该方法具有数据效率高、无需重置环境、泛化能力强和可扩展性强等优势，有望在未来的机器人应用中发挥重要作用。

## war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars
### Abstract
Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
### 🌟 论文解读 | 利用大型语言模型模拟历史战争，探索和平的可能性

### 📌 背景痛点/本文动机
战争与和平是人类历史永恒的主题，理解战争的原因和预防战争的发生一直是人类追求的目标。传统的战争研究方法主要依赖于历史分析和文献回顾，但这些方法往往受限于静态视角和事后诸葛亮的偏见。随着人工智能和大型语言模型（LLM）的快速发展，我们有机会利用这些先进技术来模拟历史事件，探索战争与和平的动态过程，并为冲突解决和和平维护提供新的视角。

### 🚀 核心方法
本文提出了 WarAgent，一个基于 LLM 的多智能体 AI 系统，用于模拟历史国际冲突，包括第一次世界大战（WWI）、第二次世界大战（WWII）和中国古代战国时期（WSP）。WarAgent 通过模拟参与国家的决策过程和互动，探索了以下关键问题：

* **模拟有效性**：LLM 基于多智能体系统能否有效地复制历史事件中战略规划和决策过程的演变？
* **战争起因**：哪些因素是导致战争爆发的关键因素？LLM 基于多智能体系统能否识别这些因素？
* **战争不可避免性**：历史上的战争是否真的不可避免？LLM 基于多智能体系统能否揭示导致战争（或和平）的条件？

### 📈 实验结果
实验结果表明，WarAgent 能够有效地模拟历史事件，并在一定程度上复制历史决策过程和互动。例如，在 WWI 模拟中，WarAgent 能够重现主要国家之间的联盟形成、战争宣言和动员情况，与历史事件具有较高的吻合度。此外，通过改变触发事件和国家的初始条件，WarAgent 能够探索不同的战争起因和战争不可避免性，为理解历史事件和预防未来冲突提供了新的视角。

### 💬 可借鉴之处
* **LLM 在历史模拟中的应用**：WarAgent 为 LLM 在历史模拟中的应用提供了新的思路，为理解复杂的人类行为和社会动态提供了新的工具。
* **多智能体系统在冲突解决中的应用**：WarAgent 的设计理念可以为冲突解决和和平维护提供新的思路，例如通过模拟不同政策的影响来评估冲突解决策略的有效性。
* **历史教学的新方法**：WarAgent 可以作为一种新的历史教学方法，帮助学生和教师探索“如果”场景，并理解历史事件的复杂因果关系。

### 🌟 未来展望
WarAgent 的研究为 LLM 在历史模拟中的应用开辟了新的道路，未来可以进一步探索以下方向：

* **时间驱动模拟**：将 WarAgent 的回合制模拟扩展为时间驱动模拟，以更准确地模拟历史事件的时间动态。
* **停止条件**：研究更有效的停止条件，以更清晰地结束模拟并分析结果。
* **新的研究问题**：探索更多与历史事件和冲突解决相关的研究问题，例如外交沟通与冲突可能性之间的关系、非国家行为体对地缘政治的影响等。

通过不断改进和扩展 WarAgent，我们可以更好地理解历史事件，并为构建更加和平的未来提供新的思路。

## language-models-meet-world-models--embodied-experiences-enhance-language-models
### Abstract
While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical (EWC) for
selective weight updates, combined with low-rank adapters (LoRA) for training
efficiency. Extensive experiments show our approach substantially improves base
LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs
(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much
larger LMs (e.g., ChatGPT).
### 🌟 论文解读 | 语言模型与世界观模型相遇：具身经验增强语言模型

### 📌 背景痛点/本文动机
尽管大型语言模型（LMs）在众多任务中表现出色，但它们在物理环境中的简单推理和规划方面往往存在困难，例如理解物体恒常性或规划家庭活动。这种局限性源于LMs仅通过书面文本进行训练，缺乏必要的具身知识和技能。

### 🚀 核心方法
💡 创新点1：E2WM训练范式
本文提出了一种新的训练范式，即使用来自世界观模型的具身经验对LMs进行微调（E2WM）。世界观模型是具身模拟器，可以模拟真实世界环境中的物理交互，为LMs提供理解环境中的物体交互和执行动作的机会。

💡 创新点2：收集具身经验
为了将不同的具身知识注入LMs，本文介绍了两种收集经验的方法：目标导向规划和随机探索。目标导向规划旨在收集与规划和目标导向代理行为相关的经验，而随机探索则专注于积累涉及物体和世界状态跟踪的经验。

💡 创新点3：EWC-LoRA微调
为了在收集的具身经验上微调LMs，同时保留其原始的通用知识和能力，本文提出了将经典的弹性权重整合（EWC）与低秩适配器（LoRA）相结合的方法。EWC通过正则化微调损失来保留重要的LM参数，而LoRA则通过在每个模型层中注入两个可训练的低秩矩阵来实现参数高效的微调。

### 📈 实验结果
实验结果表明，本文的方法在18个下游任务上显著提高了基线LMs的性能，平均提高了64.28%。特别是，通过本文方法增强的小型LMs（1.3B、6B和13B）在许多任务上甚至超过了更大的LMs（例如ChatGPT）。

### 💬 可借鉴之处
本文提出的E2WM训练范式为增强LMs的具身知识和技能提供了一种有效的方法。通过使用世界观模型收集具身经验，并结合EWC-LoRA进行微调，LMs可以更好地理解和处理物理环境中的任务。这种方法在多个下游任务上取得了显著的性能提升，并展示了LMs在具身任务中的潜力。

## llama-rider--spurring-large-language-models-to-explore-the-open-world
### Abstract
Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round feedback-revision mechanism is
utilized to encourage LLMs to actively select appropriate revision actions
guided by feedback information from the environment. This facilitates
exploration and enhances the model's performance. Besides, we integrate
sub-task relabeling to assist LLMs in maintaining consistency in sub-task
planning and help the model learn the combinatorial nature between tasks,
enabling it to complete a wider range of tasks through training based on the
acquired exploration experiences. By evaluation in Minecraft, an open-ended
sandbox world, we demonstrate that our approach LLaMA-Rider enhances the
efficiency of the LLM in exploring the environment, and effectively improves
the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k
instances of collected data, showing minimal training costs compared to the
baseline using reinforcement learning.
### 🌟 论文解读 | LLaMA Rider：激发大型语言模型探索开放世界

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在模拟人类智能方面取得了显著进展。许多研究开始利用LLMs的能力来帮助智能体在环境中进行决策，并发现LLMs具有一定的规划和完成任务的能力。然而，LLMs的知识来源于预训练时使用的语言语料库，可能与特定环境存在差异。为了将LLMs与实际环境相结合，一些研究通过提示工程设计特定机制，为LLMs提供环境信息。然而，LLMs在环境中并不会改进或获取新知识。此外，对于更复杂的任务，需要更复杂的机制和提示，这会导致LLMs生成成本高，并且依赖于像GPT-4这样具有足够知识的强大模型。还有一些研究通过微调来将LLMs与实际环境相结合，但这通常需要依赖于特定任务的训练数据集。强化学习（RL）方法也被研究，但这些方法将LLMs训练为特定任务的策略，并且我们发现RL方法难以扩展到更大的模型或更复杂的任务。

### 🚀 核心方法
本文提出了一种名为LLaMA-Rider的方法，旨在通过LLMs在开放环境中的探索来增强其能力。LLaMA-Rider是一个两阶段的学习框架，包括探索阶段和学习阶段。

#### 💡 创新点1：探索阶段
在探索阶段，LLaMA-Rider利用反馈-修正机制来鼓励LLMs主动选择适当的修正动作，以适应环境。LLMs在环境中进行探索，收集经验，并通过反馈信息来改进其决策。此外，LLaMA-Rider还使用子任务重标记来帮助LLMs保持子任务规划的连贯性，并学习任务之间的组合性质。

#### 💡 创新点2：学习阶段
在学习阶段，LLaMA-Rider将收集到的经验处理成数据集，并使用监督微调（SFT）来训练LLMs。除了从成功任务中获得的经验外，LLaMA-Rider还收集部分完成的子任务的经验，因为有些任务在探索阶段很难完成。开放环境中的许多任务通常具有组合性，这意味着过去任务的经验可以经常帮助完成其他任务。LLaMA-Rider使用子任务重标记来提高数据利用率，并帮助LLMs学习任务之间的组合性。

### 📈 实验结果
本文在Minecraft模拟器MineDojo上评估了LLaMA-Rider方法。实验结果表明，LLaMA-Rider能够有效地探索环境，并通过微调仅使用1.3k个收集到的数据实例来提高LLMs完成任务的能力，与使用强化学习的方法相比，训练成本更低。

### 💬 可借鉴之处
LLaMA-Rider方法为LLMs在开放环境中的探索和学习提供了一种有效的方法。其反馈-修正机制和子任务重标记技术可以帮助LLMs更好地适应环境，并提高其完成任务的能力。此外，LLaMA-Rider方法还可以扩展到其他开放环境，并具有终身探索和学习的潜力。

## large-sequence-models-for-sequential-decision-making--a-survey
### Abstract
Transformer architectures have facilitated the development of large-scale and
general-purpose sequence models for prediction tasks in natural language
processing and computer vision, e.g., GPT-3 and Swin Transformer. Although
originally designed for prediction problems, it is natural to inquire about
their suitability for sequential decision-making and reinforcement learning
problems, which are typically beset by long-standing issues involving sample
efficiency, credit assignment, and partial observability. In recent years,
sequence models, especially the Transformer, have attracted increasing interest
in the RL communities, spawning numerous approaches with notable effectiveness
and generalizability. This survey presents a comprehensive overview of recent
works aimed at solving sequential decision-making tasks with sequence models
such as the Transformer, by discussing the connection between sequential
decision-making and sequence modeling, and categorizing them based on the way
they utilize the Transformer. Moreover, this paper puts forth various potential
avenues for future research intending to improve the effectiveness of large
sequence models for sequential decision-making, encompassing theoretical
foundations, network architectures, algorithms, and efficient training systems.
As this article has been accepted by the Frontiers of Computer Science, here is
an early version, and the most up-to-date version can be found at
https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5
### 🌟 论文解读 | 大型序列模型在顺序决策中的潜力：综述

### 📌 背景痛点/本文动机
随着深度学习在自然语言处理和计算机视觉领域的广泛应用，大型序列模型，尤其是Transformer架构，因其强大的预测能力和泛化能力而备受关注。然而，传统的顺序决策和强化学习问题，如样本效率、信用分配和部分可观察性等问题，一直困扰着该领域的发展。本文综述了近年来利用Transformer等序列模型解决顺序决策问题的研究进展，并探讨了未来研究的潜在方向。

### 🚀 核心方法
💡 创新点1：将顺序决策问题转化为序列建模问题
本文提出了一种新的思路，将顺序决策问题转化为序列建模问题，利用序列模型（如Transformer）来处理。这种方法可以有效地解决传统强化学习方法中存在的样本效率、信用分配和部分可观察性问题。

💡 创新点2：利用Transformer架构的优势
Transformer架构具有高并行化、可扩展性、适当的归纳偏置等优势，使其成为解决顺序决策问题的理想选择。本文详细介绍了Transformer架构在顺序决策中的应用，包括离线强化学习、基于模型的强化学习、元强化学习、多智能体强化学习、目标条件强化学习和智能体架构等方面。

### 📈 实验结果
本文综述了多个基于Transformer的顺序决策模型，并在多个任务和环境中进行了实验。结果表明，这些模型在样本效率、信用分配和部分可观察性等方面取得了显著的性能提升，并展现出良好的泛化能力。

### 💬 可借鉴之处
本文为顺序决策问题的研究提供了新的思路和方法，并指出了未来研究的潜在方向。具体而言，未来研究可以关注以下几个方面：

* **理论基础的完善**： 进一步研究序列建模方法与传统强化学习方法的结合，为政策优化提供理论保证。
* **网络架构的改进**： 开发针对顺序决策任务的定制Transformer架构，以提高模型性能。
* **算法的统一框架**： 建立一个统一的框架，涵盖各种顺序决策场景，并有效地整合多模态知识。
* **高效训练系统的设计**： 设计高效的训练系统，以支持大型决策模型的训练，并优化训练效率。

### 总结
本文综述了大型序列模型在顺序决策中的应用，并探讨了未来研究的潜在方向。随着序列模型和强化学习技术的不断发展，我们有理由相信，大型决策模型将在未来发挥越来越重要的作用，并为解决各种复杂的顺序决策问题提供新的解决方案。

## grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning
### Abstract
Recent works successfully leveraged Large Language Models' (LLM) abilities to
capture abstract knowledge about world's physics to solve decision-making
problems. Yet, the alignment between LLMs' knowledge and the environment can be
wrong and limit functional competence due to lack of grounding. In this paper,
we study an approach (named GLAM) to achieve this alignment through functional
grounding: we consider an agent using an LLM as a policy that is progressively
updated as the agent interacts with the environment, leveraging online
Reinforcement Learning to improve its performance to solve goals. Using an
interactive textual environment designed to study higher-level forms of
functional grounding, and a set of spatial and navigation tasks, we study
several scientific questions: 1) Can LLMs boost sample efficiency for online
learning of various RL tasks? 2) How can it boost different forms of
generalization? 3) What is the impact of online learning? We study these
questions by functionally grounding several variants (size, architecture) of
FLAN-T5.
### 🌟 论文解读 | GLAM：利用在线强化学习将大型语言模型嵌入交互式环境

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLM）在自然语言处理领域取得了显著进展，展现出强大的能力，例如自然语言生成、问答、推理和翻译等。然而，LLM在交互式环境中的功能能力受限，主要原因是其缺乏对环境的“接地”（grounding），即LLM的知识与环境的物理规则和动态之间的对齐不足。本文旨在通过功能接地的方法，将LLM嵌入交互式环境，并利用在线强化学习（RL）来提高其性能，从而解决决策问题。

### 🚀 核心方法
本文提出了GLAM（Grounded Language Models）方法，该方法将LLM作为智能体策略，在交互式文本环境中进行训练，并通过在线RL不断更新LLM的知识，以实现功能接地。具体来说，GLAM方法包括以下步骤：

1. **环境设计**：本文将BabyAI环境改编为文本版本（BabyAI-Text），其中智能体通过文本命令进行导航和交互。
2. **LLM作为策略**：将LLM作为智能体策略，通过接收任务描述、当前观察和可能动作的提示，输出动作的概率分布。
3. **在线RL训练**：利用环境提供的奖励信号，使用PPO算法对LLM进行在线RL训练，以优化其策略，使其能够更好地实现目标。

### 📈 实验结果
本文在BabyAI-Text环境中进行了多项实验，结果表明GLAM方法在以下方面取得了显著成果：

1. **样本效率**：与零样本使用LLM、监督微调和非预训练LLM的RL微调相比，GLAM方法在解决空间和导航任务方面表现出更高的样本效率。
2. **泛化能力**：GLAM方法训练的智能体能够将所学技能泛化到新的对象和任务，例如使用未见过的名词或动词，以及解决新的组合任务。
3. **在线学习的影响**：与离线行为克隆相比，在线RL训练能够更好地提高LLM的功能接地能力，使其能够更好地适应环境的变化。

### 💬 可借鉴之处
本文提出的GLAM方法为将LLM嵌入交互式环境并提高其功能能力提供了一种有效的方法。该方法具有以下可借鉴之处：

1. **LLM作为策略**：将LLM作为智能体策略，可以利用其强大的语言理解和生成能力，提高智能体在交互式环境中的表现。
2. **在线RL训练**：通过在线RL训练，可以使LLM不断学习和适应环境的变化，提高其泛化能力和鲁棒性。
3. **文本环境设计**：本文提出的BabyAI-Text环境为研究LLM的功能接地提供了良好的平台，可以用于评估LLM在交互式环境中的表现。

总而言之，本文提出的GLAM方法为将LLM嵌入交互式环境并提高其功能能力提供了一种有效的方法，并为未来研究LLM在交互式环境中的应用提供了新的思路。

## proximal-policy-optimization-algorithms
### Abstract
We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a "surrogate" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.
### 🌟 论文解读 | Proximal Policy Optimization Algorithms：简化强化学习中的策略梯度方法

### 📌 背景痛点/本文动机
近年来，强化学习领域涌现出多种基于神经网络函数逼近器的方法，如深度Q学习、标准策略梯度方法和信任区域/自然策略梯度方法。然而，这些方法在可扩展性、数据效率和鲁棒性方面仍有改进空间。例如，深度Q学习在许多简单问题上表现不佳，标准策略梯度方法数据效率低下且鲁棒性差，而信任区域策略优化（TRPO）相对复杂，且不兼容包含噪声（如dropout）或参数共享（如策略和价值函数之间，或与辅助任务之间）的架构。

### 🚀 核心方法
本文提出了一种名为近端策略优化（PPO）的新策略梯度方法，旨在解决上述问题。PPO具有以下创新点：

💡 创新点1：交替采样和优化
PPO交替在环境中采样数据并通过与环境的交互，然后使用随机梯度上升优化一个“代理”目标函数。与标准策略梯度方法每数据样本执行一次梯度更新不同，PPO提出了一种新的目标函数，允许进行多个epoch的小批量更新。

💡 创新点2：截断概率比
PPO使用截断概率比来形成对策略性能的悲观估计（即下界）。通过这种方式，PPO能够在保证策略更新的稳定性和可靠性的同时，简化实现过程，使其更通用，并提高样本复杂度。

### 📈 实验结果
本文在一系列基准任务上测试了PPO，包括模拟机器人运动和Atari游戏。结果表明，PPO优于其他在线策略梯度方法，并在样本复杂度、简单性和运行时间之间取得了良好的平衡。

### 💬 可借鉴之处
PPO是一种简单、高效且鲁棒的策略梯度方法，适用于各种强化学习任务。其创新点包括交替采样和优化以及截断概率比，这些方法可以应用于其他策略梯度方法，以提高其性能和鲁棒性。

## enhance-reasoning-for-large-language-models-in-the-game-werewolf
### Abstract
This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
### 🌟 论文解读 | 大型语言模型推理能力提升：以狼人杀游戏为案例

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理（NLP）任务上的突破，其在推理、规划和决策等领域的潜力也逐渐显现。然而，LLMs在处理复杂推理任务时仍面临挑战，尤其是在需要领域特定知识和深度逻辑分析的任务中。本文旨在通过引入外部推理模块，即“思考者”（Thinker），来增强LLM的推理能力，使其在特定任务中表现更佳。

### 🚀 核心方法
💡 创新点1：双系统推理框架
本文提出了一个创新的框架，将LLMs与外部Thinker模块相结合，形成了一个推理层次结构。LLMs负责处理直观的System-1任务，如自然语言处理和常识推理，而Thinker则专注于需要复杂逻辑分析和领域特定知识的System-2任务。

💡 创新点2：Thinker模块的设计与训练
Thinker模块直接从数据库中获取知识，并采用各种优化技术进行训练。它通过模仿学习、强化学习和基于群体的训练等方法，学习生成合理的游戏动作和LLM的语音指令。

💡 创新点3：数据集贡献
本文收集了18,800场真实人类游戏会话数据，构建了迄今为止最大的社交推理游戏数据集，为研究提供了宝贵资源。

### 📈 实验结果
实验结果表明，引入Thinker模块显著提高了LLMs的推理和生成能力。在狼人杀游戏中，Thinker模块在推理、语音生成和在线游戏评估方面均表现出色。此外，通过将Thinker与一个较小的LLM模型（6B）进行微调，其性能甚至超过了GPT4。

### 💬 可借鉴之处
本文提出的框架和方法为LLMs在复杂推理任务中的应用提供了新的思路。通过将LLMs与外部推理模块相结合，可以有效提升LLMs在特定领域的推理能力，使其在更多实际应用中发挥更大的作用。此外，本文构建的大规模数据集也为社交推理游戏的研究提供了重要的数据基础。

## reward-design-with-language-models
### Abstract
Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning
### 🌟 论文解读 | 使用语言模型进行奖励设计

### 📌 背景痛点/本文动机
强化学习（RL）中的奖励设计一直是一个挑战，因为通过奖励函数来指定人类期望的行为可能很困难，或者需要大量的专家演示。本文提出了一种新的方法，使用大型语言模型（LLM）作为代理奖励函数，用户可以通过自然语言界面提供几个示例或描述来指定期望的行为，从而简化奖励设计。

### 🚀 核心方法
💡 创新点1：使用LLM作为代理奖励函数
本文的核心思想是利用LLM的上下文学习能力，将用户提供的文本提示（包含几个示例或描述）作为代理奖励函数，评估RL代理的行为是否符合用户的目标，并输出相应的奖励信号。

💡 创新点2：通用RL训练框架
本文提出了一种通用的RL训练框架，该框架利用代理奖励函数，并与所使用的RL算法无关。用户只需在训练开始时指定一次提示，LLM会在训练过程中评估代理的行为，并输出奖励信号，代理使用该奖励信号更新其行为。

### 📈 实验结果
本文在三个任务中评估了该方法的有效性：最后通牒游戏、矩阵游戏和DealOrNoDeal谈判任务。结果表明，使用LLM作为代理奖励函数训练的RL代理与用户的目标高度一致，并且在所有三个任务中都优于使用监督学习学习奖励函数的RL代理。

### 💬 可借鉴之处
本文提出的方法为用户提供了更直观和便捷的方式来指定RL代理的目标，并且可以有效地训练与用户目标一致的代理。该方法可以应用于各种RL任务，并有望推动人类兼容和价值对齐的AI系统的发展。

## motif--intrinsic-motivation-from-artificial-intelligence-feedback
### Abstract
Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
### 🌟 论文解读 | Motif：从人工智能反馈中获取内在动机

### 📌 背景痛点/本文动机
在复杂环境中，没有先验知识的智能体探索和评估其行为极具挑战性。本文提出了一种名为 Motif 的新方法，旨在将大型语言模型 (LLM) 中的先验知识与智能体进行交互，从而帮助智能体在没有与环境的直接交互的情况下进行决策。

### 🚀 核心方法
💡 创新点1：利用 LLM 的偏好构建内在奖励
Motif 的核心思想是，通过 LLM 对事件标题的偏好来构建内在奖励函数，并将其用于强化学习训练智能体。LLM 表达对成对事件标题的偏好，这些标题只需粗略描述环境中发生的事件，而不需要精细的逐步描述。LLM 不需要理解低级动作空间，这可能是复合的或连续的。

💡 创新点2：内在奖励与外在奖励的结合
Motif 的内在奖励可以单独使用，也可以与来自环境的奖励信号结合使用。实验表明，当内在奖励与外在奖励结合使用时，Motif 的性能显著优于现有方法，并在没有演示的情况下取得了进展。

### 📈 实验结果
Motif 在 NetHack 学习环境 (NLE) 上进行了评估，这是一个具有挑战性、开放性和程序生成的游戏。结果表明，仅通过学习最大化其内在奖励，Motif 就取得了比直接训练以最大化分数的算法更高的游戏分数。当将 Motif 的内在奖励与环境的奖励相结合时，该方法显著优于现有方法，并在没有演示的情况下取得了进展。

### 💬 可借鉴之处
Motif 为利用 LLM 的先验知识和常识来创建智能体提供了一种通用的方法。它通过将 LLM 的高层次知识与智能体操作的底层传感器运动现实之间的差距，从而有效地将知识提炼出来。Motif 具有可扩展性，可以与更大规模的 LLM 或特定领域的微调 LLM 结合使用，并可以通过提示修改轻松地引导智能体的行为。

## auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft
### Abstract
Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.
### 🌟 论文解读 | Auto MC-Reward：利用大型语言模型自动设计密集奖励函数，提升Minecraft中强化学习的效率

### 📌 背景痛点/本文动机
Minecraft 等强化学习环境通常只提供稀疏奖励，即只有任务完成或失败时才会获得奖励。这种奖励机制使得强化学习代理在探索效率方面面临挑战，难以学习复杂任务。为了解决这个问题，本文提出了 Auto MC-Reward，一个利用大型语言模型 (LLM) 自动设计密集奖励函数的先进学习系统，从而提高学习效率。

### 🚀 核心方法
💡 创新点1：Auto MC-Reward 由三个关键组件组成：奖励设计器、奖励评论家和轨迹分析器。奖励设计器根据环境信息和任务描述，通过编写可执行的 Python 函数来设计奖励函数。奖励评论家负责验证代码，检查代码是否自洽且没有语法和语义错误。轨迹分析器根据收集的轨迹总结可能的失败原因，并提供改进建议。

💡 创新点2：Auto MC-Reward 利用 LLM 的任务理解和经验总结能力，为学习提供详细和即时的奖励指导。奖励设计器首先根据环境和任务的基本描述，使用 LLM 设计与任务相关的密集奖励函数。然后，奖励评论家对设计的奖励函数进行自我验证。为了解决 LLM 理解的潜在偏差或疏忽，还提出了基于 LLM 的轨迹分析器，用于分析和总结训练代理的轨迹，并帮助奖励设计器改进奖励函数。

### 📈 实验结果
Auto MC-Reward 在一系列代表性基准测试中进行了验证，包括地下水平探索钻石和探索平原生物群落中的树木和动物。实验结果表明，与原始稀疏奖励和现有密集奖励方法相比，Auto MC-Reward 在这些任务上取得了显著更好的结果，显示出其在稀疏奖励任务上高效学习的先进能力。通过迭代改进奖励函数的设计，Auto MC-Reward 使代理能够有效地学习对新任务有益的新行为，例如避免熔岩，从而大大提高了成功率。此外，Auto MC-Reward 仅使用原始信息就实现了高钻石获取成功率（36.5%），证明了其解决长期任务的能力。

### 💬 可借鉴之处
Auto MC-Reward 为解决稀疏奖励任务中的探索效率问题提供了一种新的思路。其利用 LLM 自动设计密集奖励函数的方法，可以有效地提高强化学习代理的学习效率。此外，Auto MC-Reward 的三个组件（奖励设计器、奖励评论家和轨迹分析器）可以独立运行，使得数据分析和奖励函数更新更加灵活。

## automated-feature-selection-for-inverse-reinforcement-learning
### Abstract
Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.
### 🌟 论文解读 | 自动化特征选择在逆强化学习中的应用

### 📌 背景痛点/本文动机
在逆强化学习（IRL）中，从专家演示中学习奖励函数是一个关键步骤。然而，手动指定奖励函数既困难又耗时，且容易引入错误假设。此外，在连续状态空间中，状态变量本身不足以作为特征，而哪些特征是好的通常并不清楚。

### 🚀 核心方法
本文提出了一个自动化特征选择方法，用于逆强化学习中的奖励函数学习。主要创新点如下：

💡 创新点1：使用多项式基函数作为候选特征集，这些函数能够匹配状态分布的统计矩，从而有效地捕捉专家策略。

💡 创新点2：开发了一种基于相关性的特征选择机制，自动选择最相关的特征子集，以减少奖励复杂性并减轻噪声和虚假相关性的影响。

### 📈 实验结果
在三个不同的连续控制任务（摆动、推车和双臂机器人）上进行的实验表明，该方法能够有效地恢复奖励函数，并生成与专家策略相似的政策。与手动选择特征、随机选择特征、直接使用状态作为特征以及使用所有候选特征的方法相比，该方法在所有任务中都取得了更好的性能。

### 💬 可借鉴之处
本文提出的自动化特征选择方法为逆强化学习中的奖励函数学习提供了一种有效且高效的解决方案。该方法可以应用于各种连续状态空间，并有助于提高模型的解释性和鲁棒性。此外，该方法还可以扩展到其他基础函数，如傅里叶级数和径向基函数，以进一步提高其适用性和精度。

## eureka--human-level-reward-design-via-coding-large-language-models
### Abstract
Large Language Models (LLMs) have excelled as high-level semantic planners
for sequential decision-making tasks. However, harnessing them to learn complex
low-level manipulation tasks, such as dexterous pen spinning, remains an open
problem. We bridge this fundamental gap and present Eureka, a human-level
reward design algorithm powered by LLMs. Eureka exploits the remarkable
zero-shot generation, code-writing, and in-context improvement capabilities of
state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over
reward code. The resulting rewards can then be used to acquire complex skills
via reinforcement learning. Without any task-specific prompting or pre-defined
reward templates, Eureka generates reward functions that outperform expert
human-engineered rewards. In a diverse suite of 29 open-source RL environments
that include 10 distinct robot morphologies, Eureka outperforms human experts
on 83% of the tasks, leading to an average normalized improvement of 52%. The
generality of Eureka also enables a new gradient-free in-context learning
approach to reinforcement learning from human feedback (RLHF), readily
incorporating human inputs to improve the quality and the safety of the
generated rewards without model updating. Finally, using Eureka rewards in a
curriculum learning setting, we demonstrate for the first time, a simulated
Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a
pen in circles at rapid speed.
### 🌟 论文解读 | Eureka：通过大型语言模型实现人类水平的奖励设计

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在序列决策任务中表现出色，但在学习复杂低级操作任务（如灵巧的笔转）方面仍存在挑战。现有的尝试需要大量的领域专业知识来构建任务提示或仅学习简单技能，这导致了实现人类水平灵巧性的巨大差距。

### 🚀 核心方法
💡 创新点1：Eureka，一个由LLMs驱动的奖励设计算法，通过利用GPT-4等最先进LLMs的零样本生成、代码编写和在上下文中的改进能力，对奖励代码进行进化优化。生成的奖励函数可以用于通过强化学习获得复杂技能。

💡 创新点2：Eureka在29个开源RL环境中（包括10种不同的机器人形态）表现出色，在83%的任务上优于人类专家设计的奖励函数，平均标准化改进为52%。Eureka的通用性还使其能够实现一种新的无梯度上下文学习方法，用于从人类反馈中进行强化学习（RLHF），无需模型更新即可轻松纳入人类输入，以提高生成奖励的质量和安全性。

💡 创新点3：Eureka在课程学习环境中使用，首次展示了能够执行笔转技巧的模拟Shadow Hand，能够快速地操纵笔进行圆周运动。

### 📈 实验结果
Eureka在29个开源RL环境中进行了全面评估，包括10种不同的机器人形态，包括四足动物、四旋翼飞机、双足动物、操作器和几种灵巧的手。Eureka在83%的任务上优于人类专家，平均标准化改进为52%。此外，Eureka还成功地解决了灵巧操作任务，例如笔转，这是以前手动奖励工程无法实现的。

### 💬 可借鉴之处
Eureka展示了LLMs在奖励设计中的潜力，为解决复杂低级操作任务提供了新的思路。Eureka的通用性和可扩展性使其成为未来强化学习研究的重要工具。

## lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions
### Abstract
Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.
### 🌟 论文解读 | 低成本实时社交互动的生成式智能体：Lyfe Agents

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在模拟人类行为方面的潜力日益显现，生成式智能体在虚拟社会中模拟复杂社交行为的前景也变得光明。然而，实现与人类在低计算成本下的实时互动仍然是一个挑战。本文旨在创建一种既智能又自主的生成式智能体，能够在低计算成本下实现与人类的实时互动。

### 🚀 核心方法
💡 创新点1：选项-动作框架
为了减少高级决策的成本，Lyfe Agents 采用了一种选项-动作框架。在这种框架中，智能体首先选择一个高级动作（或“选项”），然后在后续步骤中在该选项内选择低级动作。这种设计允许智能体在更长时间内专注于执行选项背后的意图，从而降低成本并提高效率。

💡 创新点2：异步自我监控
为了提高智能体的情境意识和目标坚持性，Lyfe Agents 引入了一个自我监控模块。该模块维护一个关于最近事件的叙事风格摘要，并强调新颖和与目标相关的内容。这种自我监控摘要帮助智能体更好地理解情境，并使其行为更加一致和符合目标。

💡 创新点3：总结-遗忘记忆机制
为了提高记忆存储和检索的质量，Lyfe Agents 采用了一种层次化的记忆架构和总结-遗忘（SaF）方法。这种方法将记忆分为短期记忆和长期记忆，并通过聚类和总结技术将短期记忆中的信息转移到长期记忆中。此外，遗忘算法会评估并删除与现有记忆高度相似的旧记忆，以确保存储的信息是独特和相关的。

### 📈 实验结果
在自定义的 LyfeGame 3D 虚拟环境平台上，Lyfe Agents 在多个多智能体场景中展示了其自我激励和社会性。例如，智能体能够通过自主协作和信息交换解决犯罪（谋杀谜案）。此外，与现有方法相比，Lyfe Agents 的计算成本降低了 10-100 倍。

### 💬 可借鉴之处
Lyfe Agents 的设计原则和架构组件为构建低成本、实时响应的生成式智能体提供了有价值的参考。其选项-动作框架、异步自我监控和总结-遗忘记忆机制等创新点可以应用于其他生成式智能体框架，以提高其自主性和社交推理能力。此外，LyfeGame 虚拟环境平台也为研究生成式智能体的社会行为和用户交互提供了有价值的工具。

## benchmarking-the-spectrum-of-agent-capabilities
### Abstract
Evaluating the general abilities of intelligent agents requires complex
simulation environments. Existing benchmarks typically evaluate only one narrow
task per environment, requiring researchers to perform expensive training runs
on many different environments. We introduce Crafter, an open world survival
game with visual inputs that evaluates a wide range of general abilities within
a single environment. Agents either learn from the provided reward signal or
through intrinsic objectives and are evaluated by semantically meaningful
achievements that can be unlocked during each episode, such as discovering
resources and crafting tools. Consistently unlocking all achievements requires
strong generalization, deep exploration, and long-term reasoning. We
experimentally verify that Crafter is of appropriate difficulty to drive future
research and provide baselines scores of reward agents and unsupervised agents.
Furthermore, we observe sophisticated behaviors emerging from maximizing the
reward signal, such as building tunnel systems, bridges, houses, and
plantations. We hope that Crafter will accelerate research progress by quickly
evaluating a wide spectrum of abilities.
### 🌟 论文解读 | Crafter：评估智能体能力的全新基准

### 📌 背景痛点/本文动机
评估智能体的通用能力需要复杂的模拟环境。现有的基准通常在每个环境中只评估一个狭窄的任务，这要求研究人员在许多不同的环境中进行昂贵的训练运行。本文提出了 Crafter，一个具有视觉输入的开放世界生存游戏，它在一个环境中评估了广泛的通用能力。智能体可以通过提供的奖励信号或通过内在目标进行学习，并通过在每个回合中解锁的具有语义意义的成就来评估，例如发现资源和制作工具。持续解锁所有成就需要强大的泛化能力、深入的探索和长期推理。

### 🚀 核心方法
💡 创新点1：Crafter 是一个开放世界生存游戏，具有视觉输入，可以在单个环境中评估广泛的通用能力。
💡 创新点2：智能体可以通过提供的奖励信号或通过内在目标进行学习，并通过在每个回合中解锁的具有语义意义的成就来评估。
💡 创新点3：Crafter 提供了两个基准，一个允许智能体访问提供的奖励信号，另一个不允许，并要求智能体纯粹从内在目标中学习。
💡 创新点4：Crafter 定义了 22 个成就，这些成就对应于行为中的具有语义意义的里程碑，例如收集各种资源、建造物体和工具、寻找食物和水、击败怪物以及在睡觉后安全地醒来。

### 📈 实验结果
实验结果表明，Crafter 是一个具有挑战性的基准，当前的方法在 Crafter 上取得了学习进展，但未来还需要更多的研究才能实现高性能。DreamerV2、PPO 和 Rainbow 等顶级强化学习算法在 Crafter 上的得分分别为 10.0%、4.6% 和 4.3%，而人类专家的得分为 50.5%。这表明 Crafter 具有足够的难度，可以推动未来研究的发展。

### 💬 可借鉴之处
Crafter 是一个非常有价值的基准，可以用于评估智能体的通用能力。它具有以下优点：
- 在单个环境中评估广泛的通用能力，减少了计算需求。
- 提供了具有语义意义的成就，可以更全面地评估智能体的能力。
- 允许智能体通过提供的奖励信号或通过内在目标进行学习。
- 具有挑战性，可以推动未来研究的发展。

Crafter 的提出为评估智能体的通用能力提供了一个新的基准，并为未来研究的发展提供了有价值的参考。

## keep-calm-and-explore--language-models-for-action-generation-in-text-based-games
### Abstract
Text-based games present a unique challenge for autonomous agents to operate
in natural language and handle enormous action spaces. In this paper, we
propose the Contextual Action Language Model (CALM) to generate a compact set
of action candidates at each game state. Our key insight is to train language
models on human gameplay, where people demonstrate linguistic priors and a
general game sense for promising actions conditioned on game history. We
combine CALM with a reinforcement learning agent which re-ranks the generated
action candidates to maximize in-game rewards. We evaluate our approach using
the Jericho benchmark, on games unseen by CALM during training. Our method
obtains a 69% relative improvement in average game score over the previous
state-of-the-art model. Surprisingly, on half of these games, CALM is
competitive with or better than other models that have access to ground truth
admissible actions. Code and data are available at
https://github.com/princeton-nlp/calm-textgame.
### 🌟 论文解读 | 语言模型助力文本游戏：CALM模型解析

### 📌 背景痛点/本文动机
文本游戏为自主智能体提供了独特的挑战，要求它们在自然语言环境中操作并处理巨大的动作空间。现有的强化学习模型在处理这些游戏时，由于动作空间组合爆炸，学习收敛速度慢，且缺乏对游戏状态的语义理解，导致难以有效探索和选择最优动作。

### 🚀 核心方法
💡 创新点1：CALM模型
本文提出了一个名为“上下文动作语言模型”（CALM）的模型，用于在每个游戏状态下生成一组紧凑的动作候选集。CALM模型通过在人类游戏玩法上进行训练，捕捉人类玩家在游戏历史条件下对有前景动作的语言先验和一般游戏感。

💡 创新点2：强化学习与CALM的结合
CALM模型与强化学习智能体相结合，该智能体使用游戏奖励重新排序生成的动作候选集，以最大化游戏内奖励。这种结合使得模型能够将通用的语言先验与适应性地选择最适合游戏的动作的能力相结合。

### 📈 实验结果
在Jericho基准测试中，CALM模型在未见过的游戏中取得了69%的平均游戏分数相对提升，超过了之前最先进的模型。令人惊讶的是，在半数游戏中，CALM模型甚至与其他能够访问真实有效动作的模型相媲美或更优。

### 💬 可借鉴之处
本文提出的CALM模型为文本游戏中的动作生成提供了一个新的思路，通过结合语言模型和强化学习，有效地缩小了动作空间，并提高了游戏性能。此外，本文还引入了一个新的数据集，用于评估动作生成的质量，为文本游戏研究提供了宝贵的资源。

## can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions
### Abstract
Large language models (LLMs) such as ChatGPT and GPT-4 have recently
demonstrated their remarkable abilities of communicating with human users. In
this technical report, we take an initiative to investigate their capacities of
playing text games, in which a player has to understand the environment and
respond to situations by having dialogues with the game world. Our experiments
show that ChatGPT performs competitively compared to all the existing systems
but still exhibits a low level of intelligence. Precisely, ChatGPT can not
construct the world model by playing the game or even reading the game manual;
it may fail to leverage the world knowledge that it already has; it cannot
infer the goal of each step as the game progresses. Our results open up new
research questions at the intersection of artificial intelligence, machine
learning, and natural language processing.
### 🌟 论文解读 | 大型语言模型在文本游戏中的表现：现状与开放性问题

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）如ChatGPT和GPT-4在理解和响应人类语言查询方面展现出令人印象深刻的能力，研究界对其是否能够实现通用人工智能（AGI）的潜力产生了广泛讨论。本文旨在通过将LLMs置于文本游戏的环境中，评估其在理解环境、做出决策和与游戏世界进行交互方面的智能水平，从而为LLMs的能力和局限性提供新的见解。

### 🚀 核心方法
💡 创新点1：使用文本游戏作为评估LLMs智能水平的测试床。文本游戏要求玩家通过文本命令与游戏世界进行交互，从而提供了一个可控的环境来评估LLMs的智能水平。
💡 创新点2：通过分析LLMs在文本游戏中的表现，揭示了LLMs在构建世界模型、推断目标和导航能力方面的局限性。研究发现，尽管LLMs在文本游戏中的表现优于现有系统，但它们仍然缺乏构建世界模型、推断目标和进行有效导航的能力。

### 📈 实验结果
实验结果表明，ChatGPT在文本游戏中的表现优于现有系统，但仍然存在一些局限性。ChatGPT无法通过游戏或阅读游戏手册来构建世界模型，无法充分利用已有的世界知识，也无法推断游戏进行过程中每一步的目标。这些结果表明，LLMs在实现人类水平的智能方面仍然存在一些挑战。

### 💬 可借鉴之处
本文的研究结果表明，文本游戏可以作为评估LLMs智能水平的有效测试床。通过分析LLMs在文本游戏中的表现，可以揭示LLMs在构建世界模型、推断目标和导航能力方面的局限性，并为LLMs的未来发展提供新的方向。此外，本文的研究结果也为LLMs在游戏领域的应用提供了新的思路，例如开发基于LLMs的智能游戏助手或游戏角色。

## language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game
### Abstract
Agents built with large language models (LLMs) have shown great potential
across a wide range of domains. However, in complex decision-making tasks, pure
LLM-based agents tend to exhibit intrinsic bias in their choice of actions,
which is inherited from the model's training data and results in suboptimal
performance. To develop strategic language agents, i.e., agents that generate
flexible language actions and possess strong decision-making abilities, we
propose a novel framework that powers LLM-based agents with reinforcement
learning (RL). We consider Werewolf, a popular social deduction game, as a
challenging testbed that emphasizes versatile communication and strategic
gameplay. To mitigate the intrinsic bias in language actions, our agents use an
LLM to perform deductive reasoning and generate a diverse set of action
candidates. Then an RL policy trained to optimize the decision-making ability
chooses an action from the candidates to play in the game. Extensive
experiments show that our agents overcome the intrinsic bias and outperform
existing LLM-based agents in the Werewolf game. We also conduct human-agent
experiments and find that our agents achieve human-level performance and
demonstrate strong strategic play.
### 🌟 论文解读 | 语言智能体在狼人杀游戏中的战略决策

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在构建智能体方面的广泛应用，其在复杂决策任务中表现出的内在偏差问题逐渐凸显。这种偏差源于模型训练数据，导致LLM-based智能体在战略决策方面表现不佳。为了解决这个问题，本文提出了一种新的框架，将LLM与强化学习（RL）相结合，以构建具有灵活语言行动和强大决策能力的战略语言智能体。

### 🚀 核心方法
💡 创新点1：隐藏角色推理
本文使用LLM对游戏中的信息进行分类，区分真伪，并推断每个玩家的隐藏角色，为后续决策提供基础。

💡 创新点2：多样化行动生成
为了克服LLM的内在偏差，本文提出了一种多样化行动生成方法，通过提示LLM生成一系列行动候选者，从而避免固定模式并提高决策的灵活性。

💡 创新点3：基于群体的强化学习训练
本文采用基于群体的强化学习训练方法，通过训练一个RL策略来优化行动候选者的分布，并通过与各种智能体进行对抗来提高策略的鲁棒性。

### 📈 实验结果
本文在狼人杀游戏中进行了广泛的实验，结果表明，与现有的LLM-based智能体相比，本文提出的战略语言智能体能够克服内在偏差，并在游戏中表现出更强的战略决策能力。此外，与人类玩家的对局实验也表明，本文提出的智能体能够达到人类水平的游戏表现。

### 💬 可借鉴之处
本文提出的框架为构建具有强大决策能力的战略语言智能体提供了一种新的思路，其核心方法可以应用于其他需要灵活语言行动和战略决策的场景。此外，本文提出的多样化行动生成方法和基于群体的强化学习训练方法也为解决LLM内在偏差问题提供了新的思路。

## avalonbench--evaluating-llms-playing-the-game-of-avalon
### Abstract
In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
### 🌟 论文解读 | AvalonBench：评估大型语言模型在社交推理游戏中的表现

### 📌 背景痛点/本文动机
社交推理游戏如 Resistance Avalon 对玩家的推理、沟通和决策能力提出了挑战。这些游戏要求玩家在动态变化的游戏阶段做出明智的决策，并在讨论中欺骗、推理和与其他玩家协商。这些特点使得 Avalon 成为研究大型语言模型（LLM）代理的决策和语言处理能力的理想测试平台。然而，目前缺乏一个全面的评估 LLM 代理在多代理游戏环境中的性能的基准测试平台。

### 🚀 核心方法
本文提出了 AvalonBench，一个专门用于评估多代理 LLM 代理的游戏环境。AvalonBench 包含以下三个关键组成部分：

1. **Avalon 游戏环境**：为代理提供游戏平台，记录所有玩家的行动并推动游戏进程。
2. **基于规则的机器人**：作为基线对手，为代理提供可比较的基准。
3. **ReAct 风格的 LLM 代理**：针对每个角色定制提示，以评估 LLM 代理在不同角色下的表现。

### 📈 实验结果
本文使用 AvalonBench 对 ChatGPT-3.5 和 Llama2 模型进行了评估，并与基于规则的机器人进行了比较。结果显示，即使在有讨论的情况下，LLM 代理的表现也远低于基于规则的机器人。例如，ChatGPT-3.5 在扮演好人角色时，在与扮演坏人的基于规则的机器人对抗中，胜率为 22.2%，而好人角色的机器人胜率为 38.2%。这表明当前 LLM 代理在推理、说服、协商和欺骗能力方面存在明显差距。

### 💬 可借鉴之处
AvalonBench 为研究 LLM 代理在社交推理游戏中的表现提供了一个有价值的测试平台。该平台可以帮助研究人员开发更先进的 LLM 代理，并探索如何将决策技术集成到 LLM 中，以提高其在复杂游戏环境中的表现。此外，AvalonBench 还可以用于评估 LLM 代理在多代理协作、沟通和策略制定方面的能力。

## cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game
### Abstract
Multi-agent collaboration with Large Language Models (LLMs) demonstrates
proficiency in basic tasks, yet its efficiency in more complex scenarios
remains unexplored. In gaming environments, these agents often face situations
without established coordination protocols, requiring them to make intelligent
inferences about teammates from limited data. This problem motivates the area
of ad hoc teamwork, in which an agent may potentially cooperate with a variety
of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork
problem where the agent operates in an environment driven by natural language.
Our findings reveal the potential of LLM agents in team collaboration,
highlighting issues related to hallucinations in communication. To address this
issue, we develop CodeAct, a general agent that equips LLM with enhanced memory
and code-driven reasoning, enabling the repurposing of partial information for
rapid adaptation to new teammates.
### 🌟 论文解读 | 探索大型语言模型在动态环境中的协作能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在推理和泛化能力上的不断提升，它们在构建自主代理和推动人工智能领域的发展方面展现出巨大潜力。然而，在多智能体协作中，特别是在没有预先设定的协调协议的动态环境中，LLMs的协作效率仍然是一个未充分探索的领域。本文旨在研究LLMs在动态环境中的协作能力，特别是在没有明确团队策略的情况下，如何与不同的队友进行有效合作。

### 🚀 核心方法
💡 创新点1：引入AvalonPlay基准
本文提出了AvalonPlay基准，这是一个基于自然语言的多智能体平台，用于模拟动态环境中的协作任务。在这个基准中，智能体需要在有限的信息和没有预先设定的团队策略的情况下，通过观察队友的行为来推断他们的角色，并动态调整团队策略以实现共同目标。

💡 创新点2：开发CodeAct智能体
为了解决LLMs在动态环境中协作时可能出现的记忆遗忘和幻觉生成等问题，本文提出了CodeAct智能体。CodeAct利用LLMs的代码驱动推理能力，通过将复杂的语义任务转化为灵活的代码结构，从而提高智能体在动态环境中的协作效率。

### 📈 实验结果
实验结果表明，GPT-4模型在AvalonPlay基准中表现出最佳的协作能力，而CodeAct智能体在团队选择准确性方面优于其他语义推理方法。此外，实验还发现，引入自然语言通信协议并不总是能显著提高LLMs的协作效率。

### 💬 可借鉴之处
本文的研究结果表明，LLMs在动态环境中的协作能力仍然面临一些挑战，如记忆遗忘和幻觉生成。为了提高LLMs的协作效率，可以借鉴本文提出的CodeAct智能体的设计思路，利用代码驱动推理和记忆检索系统来增强智能体的推理能力和信息处理能力。此外，还可以进一步研究如何减少幻觉生成的影响，并探索LLMs在现实世界场景中的应用。

## what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents
### Abstract
This study introduces "CosmoAgent," an innovative artificial intelligence
system that utilizes Large Language Models (LLMs) to simulate complex
interactions between human and extraterrestrial civilizations. This paper
introduces a mathematical model for quantifying the levels of civilization
development and further employs a state transition matrix approach to evaluate
their trajectories. Through this methodology, our study quantitatively analyzes
the growth trajectories of civilizations, providing insights into future
decision-making at critical points of growth and saturation. Furthermore, this
paper acknowledges the vast diversity of potential living conditions across the
universe, which could foster unique cosmologies, ethical codes, and worldviews
among different civilizations. Recognizing the Earth-centric bias inherent in
current LLM designs, we propose the novel concept of using LLM agents with
diverse ethical paradigms and simulating interactions between entities with
distinct moral principles. This innovative research not only introduces a novel
method for comprehending potential inter-civilizational dynamics but also holds
practical value in enabling entities with divergent value systems to
strategize, prevent conflicts, and engage in games under conditions of
asymmetric information. The accompanying code is available at
https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.
### 🌟 论文解读 | 用大型语言模型模拟外星文明：探索宇宙中的互动与冲突

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，其在模拟复杂社会动态方面的潜力日益凸显。然而，现有的LLMs设计往往带有地球中心主义的偏见，难以全面模拟外星文明的多样性和独特性。本文旨在通过引入具有不同伦理范式和道德原则的LLM代理，模拟人类与外星文明之间的复杂互动，从而为理解潜在星际动态提供新的视角。

### 🚀 核心方法
💡 创新点1：CosmoAgent多智能体系统
本文提出了CosmoAgent，一个基于LLMs的多智能体系统，用于模拟宇宙中不同文明之间的互动。CosmoAgent通过模拟文明的决策过程，包括选择隐藏、战斗或合作，来探索文明发展的轨迹和潜在冲突。

💡 创新点2：文明发展模型
本文引入了一个数学模型来量化文明的发展水平，并使用状态转移矩阵方法来评估文明的轨迹。该模型考虑了五个关键资源：军事能力、技术发展、生产能力、消费和储存，以及不同文明的世界观（和平主义、军国主义和孤立主义）对决策的影响。

💡 创新点3：信息不对称的模拟
为了模拟宇宙中文明之间的互动，本文考虑了信息不对称的情况，即文明之间的观测数据滞后于实际发展。LLMs代理需要根据过时的信息做出决策，这增加了模拟的复杂性和现实性。

💡 创新点4：道德多样性的模拟
本文提出了使用具有不同伦理范式的LLM代理来模拟具有不同道德原则的实体之间的互动。这有助于理解不同文明如何共存，以及道德框架如何影响星际互动。

### 📈 实验结果
实验结果表明，具有军国主义世界观的文明倾向于对较弱文明发动攻击，而孤立主义文明则更倾向于在观察一段时间后选择性地与其他文明合作。此外，信息不对称会延迟冲突的发生，为较弱文明提供了反击的机会。

### 💬 可借鉴之处
本文的研究不仅为理解潜在星际动态提供了新的视角，还为解决具有不同价值体系的实体之间的冲突提供了策略。此外，本文的研究方法可以应用于其他领域，如模拟古代社会、人类文明模式和社会生态系统。

## leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models
### Abstract
The automatic evaluation of LLM-based agent intelligence is critical in
developing advanced LLM-based agents. Although considerable effort has been
devoted to developing human-annotated evaluation datasets, such as AlpacaEval,
existing techniques are costly, time-consuming, and lack adaptability. In this
paper, inspired by the popular language game ``Who is Spy'', we propose to use
the word guessing game to assess the intelligence performance of LLMs. Given a
word, the LLM is asked to describe the word and determine its identity (spy or
not) based on its and other players' descriptions. Ideally, an advanced agent
should possess the ability to accurately describe a given word using an
aggressive description while concurrently maximizing confusion in the
conservative description, enhancing its participation in the game. To this end,
we first develop DEEP to evaluate LLMs' expression and disguising abilities.
DEEP requires LLM to describe a word in aggressive and conservative modes. We
then introduce SpyGame, an interactive multi-agent framework designed to assess
LLMs' intelligence through participation in a competitive language-based board
game. Incorporating multi-agent interaction, SpyGame requires the target LLM to
possess linguistic skills and strategic thinking, providing a more
comprehensive evaluation of LLMs' human-like cognitive abilities and
adaptability in complex communication situations. The proposed evaluation
framework is very easy to implement. We collected words from multiple sources,
domains, and languages and used the proposed evaluation framework to conduct
experiments. Extensive experiments demonstrate that the proposed DEEP and
SpyGame effectively evaluate the capabilities of various LLMs, capturing their
ability to adapt to novel situations and engage in strategic communication.
### 🌟 论文解读 | 利用猜词游戏评估大型语言模型的智能

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）如ChatGPT、GPT-4和Bard等在各个任务中展现出惊人的性能，开发基于LLMs的智能代理变得越来越重要。然而，现有的评估LLMs智能的方法存在两个主要问题：1）人工标注成本高，耗时且缺乏可扩展性和适应性；2）无法全面反映LLMs的智能。为了解决这些问题，本文提出了一种新的评估方法，即利用猜词游戏来评估LLMs的智能。

### 🚀 核心方法
💡 创新点1：DEEP框架
本文首先提出了DEEP框架，用于评估LLMs的表达和伪装能力。DEEP要求LLMs以激进和保守两种模式描述一个给定的词，并利用GPT-4来判断这些描述是否准确。激进模式要求LLMs提供清晰、详细和准确的描述，而保守模式则要求LLMs提供模糊的描述以伪装目标词。

💡 创新点2：SpyGame框架
本文还提出了SpyGame框架，这是一个交互式多智能体框架，旨在通过参与竞争性语言游戏“谁是卧底”来评估LLMs的智能。SpyGame要求LLMs具备语言技能和战略思维能力，从而更全面地评估LLMs在复杂沟通情境中的人类认知能力和适应性。

### 📈 实验结果
本文对四种开源LLMs和两种闭源LLMs进行了实验，结果表明，闭源LLMs（如GPT-4和GPT-3.5）在激进和保守模式下的表现明显优于开源模型。此外，SpyGame框架能够有效地评估LLMs在多智能体交互中的能力，捕捉它们适应新情况并进行战略沟通的能力。

### 💬 可借鉴之处
本文提出的DEEP和SpyGame框架为评估LLMs的智能提供了一种新的方法，具有以下可借鉴之处：
1. 利用游戏进行评估，更具互动性和趣味性。
2. 关注LLMs的表达和伪装能力，更全面地评估其智能。
3. SpyGame框架支持人类参与，更贴近真实场景。
4. 针对多智能体交互中的偏差问题，提出了有效的解决方案。

总而言之，本文提出的评估方法为LLMs的智能评估提供了新的思路，有助于推动LLMs的发展和应用。

## gameeval--evaluating-llms-on-conversational-games
### Abstract
The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.
### 🌟 论文解读 | GameEval：通过对话游戏评估大型语言模型

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，如何有效地评估这些模型的能力成为一个挑战。现有的评估方法主要分为两类：基于参考和基于偏好的方法。基于参考的方法需要与预先确定的答案进行比较，而基于偏好的方法则依赖于人类或模型评估者的偏好。这两种方法都存在局限性，例如获取高质量标注的成本高、时间消耗大，以及引入评估者的偏好偏差等。

### 🚀 核心方法
本文提出了GameEval，一种通过目标驱动的对话游戏来评估LLMs的新方法。GameEval将LLMs视为游戏玩家，并为其分配具有特定目标的独特角色，通过启动各种形式的对话（包括讨论、问答和投票）来实现这些目标。本文设计了三种独特的游戏，包括合作和对抗目标，并配备了相应的评估指标，以展示这种新范式如何全面评估模型性能。

### 📈 实验结果
通过广泛的实验，本文展示了GameEval能够有效地区分不同LLMs的能力，并提供对其解决复杂问题综合能力的全面评估。实验结果表明，GameEval在区分ChatGPT和GPT-4等模型的能力方面表现出色，而现有方法则难以做到这一点。

### 💬 可借鉴之处
GameEval提供了一种新的评估LLMs的方法，可以更全面地评估模型的能力，并减少评估偏差。此外，GameEval还可以用于设计新的游戏，以评估LLMs在现实世界复杂场景中的能力。

## emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task
### Abstract
Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
### 🌟 论文解读 | 探索序列模型在合成任务中的涌现世界表示

### 📌 背景痛点/本文动机
语言模型展现出惊人的能力，但其背后的原理尚不明确。这些模型是仅仅记忆了一系列表面统计信息，还是依赖于对生成序列过程的内部表示？本文通过将GPT模型应用于预测简单棋盘游戏Othello的合法移动任务，探讨了这个问题。

### 🚀 核心方法
💡 创新点1：使用Othello作为测试平台
本文选择Othello作为测试平台，因为它比国际象棋简单，但游戏树足够大，避免了记忆的可能性。通过训练一个GPT变体模型（Othello-GPT）来预测Othello的合法移动，尽管模型没有先验的游戏知识，但仍然能够以高精度生成合法移动。

💡 创新点2：使用探针技术探索内部表示
为了研究Othello-GPT是否计算了游戏状态的内部表示，本文使用了探针技术。探针是一种分类器或回归器，其输入由网络的内部激活组成，并训练以预测感兴趣的特征。通过训练探针来预测网络内部激活后的棋盘状态，发现非线性探针能够以高精度预测棋盘状态，这表明模型内部存在一个非线性的棋盘状态表示。

💡 创新点3：干预实验验证表示的因果作用
为了确定棋盘状态信息是否影响模型的预测，本文进行了一系列干预实验。通过修改Othello-GPT的内部激活，并测量由此产生的效果，发现干预后的预测与预期的棋盘状态相匹配，这表明涌现的世界表示对模型的预测具有因果作用。

💡 创新点4：创建潜在显著性图
本文还展示了如何使用干预技术创建潜在显著性图，这些图可以帮助解释模型的预测。通过干预每个棋盘格的状态，并观察预测概率的变化，可以生成一个表示每个棋盘格对当前棋盘状态预测的显著性的可视化图。

### 📈 实验结果
实验结果表明，Othello-GPT确实维护了一个游戏棋盘状态的表示，并且这个表示是非线性的。此外，这些表示与模型的预测具有因果联系。潜在显著性图揭示了Othello-GPT训练数据集的不同版本之间的显著差异。

### 💬 可借鉴之处
本文的研究结果表明，序列模型可以学习到复杂的内部表示，并且这些表示可以用于解释模型的预测。此外，干预实验和潜在显著性图等技术可以用于更好地理解模型的内部工作机制。这些发现对于自然语言处理等领域的研究具有重要意义，可以帮助我们更好地理解语言模型的内部表示，并开发更可靠的解释工具。

## are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis
### Abstract
Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games, and specifically their aptitude in the realm
of poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
  Profitable strategies in poker are evaluated in expectations over large
samples. Through a series of experiments, we first discover the characteristics
of optimal prompts and model parameters for playing poker with these models.
Our observations then unveil the distinct playing personas of the two models.
We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
### 🌟 论文解读 | ChatGPT 和 GPT-4 在德州扑克中的表现：一场前翻牌分析

### 📌 背景痛点/本文动机
随着 ChatGPT 和 GPT-4 的推出，这些模型在各种任务中表现出色，但在游戏，尤其是扑克游戏方面的能力尚未得到充分探索。扑克是一种需要在不完整信息和不确定性下做出决策的游戏，因此本文旨在评估 ChatGPT 和 GPT-4 在德州扑克中的表现，特别是它们在前翻牌阶段的决策能力。

### 🚀 核心方法
本文通过一系列实验，评估了 ChatGPT 和 GPT-4 在德州扑克前翻牌阶段的决策能力。实验中，研究人员使用了不同的提示和模型参数，以探索两种模型在扑克游戏中的最佳表现。他们还分析了两种模型的独特游戏风格，并比较了它们与游戏理论最优（GTO）策略的差异。

### 📈 实验结果
实验结果表明，ChatGPT 和 GPT-4 都对扑克游戏有深入的理解，包括起手牌的估值、游戏位置和其他游戏理论最优策略的细节。然而，两种模型都不是游戏理论最优的扑克玩家。ChatGPT 倾向于保守的游戏风格，只参与优质牌局，而 GPT-4 则表现出更加激进的游戏风格，参与更多的牌局。

### 💬 可借鉴之处
本文的研究结果表明，尽管 ChatGPT 和 GPT-4 在扑克游戏方面表现出色，但它们仍然存在局限性。这些模型在理解游戏理论最优策略方面存在偏差，这可能是由于它们在训练过程中没有针对扑克游戏进行专门训练。此外，本文的研究结果也表明，不同的提示和模型参数对模型在扑克游戏中的表现有显著影响。因此，未来的研究可以探索如何通过优化提示和模型参数来提高模型在扑克游戏中的表现。

## humanoid-agents--platform-for-simulating-human-like-generative-agents
### Abstract
Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents
### 🌟 论文解读 | 人类化智能体：模拟人类行为的生成式智能体平台

### 📌 背景痛点/本文动机
随着生成式智能体（Generative Agents）的出现，人们开始尝试使用高级自然语言处理系统来模拟人类行为。然而，现有的生成式智能体主要关注逻辑和计划，缺乏对人类直觉和即时反应的模拟。为了解决这个问题，本文提出了人类化智能体（Humanoid Agents）平台，旨在通过引入基本需求、情感和关系亲密度等元素，使智能体更接近人类的真实行为。

### 🚀 核心方法
💡 创新点1：引入系统1思维
本文借鉴了心理学中的系统1思维，即直觉、无意识和即时的思维过程。通过引入基本需求、情感和关系亲密度等元素，使智能体能够根据自身状态和环境变化做出更自然的反应。

💡 创新点2：动态调整行为
人类化智能体平台允许智能体根据自身的基本需求、情感和关系亲密度动态调整其日常活动和对话。例如，当智能体感到饥饿时，它会寻找食物；当它感到孤独时，它会尝试与其他智能体进行交流。

### 📈 实验结果
实验结果表明，人类化智能体能够有效地模拟人类行为。与人类标注相比，该系统能够准确预测活动是否满足基本需求、活动表达的情感以及对话是否使智能体之间的关系更加亲密。

### 💬 可借鉴之处
本文提出的Humanoid Agents平台为研究人类行为提供了一个有价值的工具。该平台可以扩展到各种场景，并支持更多影响人类行为的元素，如同理心、道德价值观和文化背景。此外，该平台还提供了Unity WebGL游戏界面和交互式分析仪表板，方便用户可视化智能体的状态和行为。

### 🌐 平台访问
- 平台网站：https://www.humanoidagents.com/
- 代码仓库：https://github.com/HumanoidAgents/HumanoidAgents

## textworld--a-learning-environment-for-text-based-games
### Abstract
We introduce TextWorld, a sandbox learning environment for the training and
evaluation of RL agents on text-based games. TextWorld is a Python library that
handles interactive play-through of text games, as well as backend functions
like state tracking and reward assignment. It comes with a curated list of
games whose features and challenges we have analyzed. More significantly, it
enables users to handcraft or automatically generate new games. Its generative
mechanisms give precise control over the difficulty, scope, and language of
constructed games, and can be used to relax challenges inherent to commercial
text games like partial observability and sparse rewards. By generating sets of
varied but similar games, TextWorld can also be used to study generalization
and transfer learning. We cast text-based games in the Reinforcement Learning
formalism, use our framework to develop a set of benchmark games, and evaluate
several baseline agents on this set and the curated list.
### 🌟 论文解读 | TextWorld：基于文本游戏的强化学习环境

### 📌 背景痛点/本文动机
文本游戏是一种复杂的交互式模拟，玩家通过输入文本命令来探索游戏世界并达成目标。这类游戏对语言理解、长期记忆、规划、探索和常识推理等方面提出了挑战。然而，现有的强化学习算法在处理文本游戏时面临着诸多困难，例如部分可观测性、稀疏奖励、巨大的状态空间和动作空间等。

### 🚀 核心方法
TextWorld 是一个用于训练和评估强化学习代理在文本游戏上的学习环境。它具有以下创新点：

💡 创新点1：生成机制
TextWorld 具有强大的生成机制，可以自动构建游戏世界、填充对象和障碍物，并生成定义目标状态和如何达到目标的任务。这使得研究人员可以控制游戏的难度、范围和语言，并生成具有不同特征和挑战的游戏集合，用于研究泛化和迁移学习。

💡 创新点2：逻辑引擎
TextWorld 使用线性逻辑和推理引擎来定义游戏规则和状态转换函数。这使得游戏状态具有明确的表示，并可以精确地跟踪状态和分配奖励。此外，TextWorld 还可以提供中间奖励，帮助代理学习更有效的策略。

💡 创新点3：文本生成
TextWorld 使用上下文无关文法 (CFG) 来生成游戏状态的自然语言描述。这使得游戏世界和任务对人类可解释，并且可以控制文本的复杂性和多样性。

### 📈 实验结果
TextWorld 提供了一系列基准游戏，用于评估强化学习代理的性能。实验结果表明，TextWorld 可以有效地训练代理解决文本游戏中的各种挑战，并展现出良好的泛化能力。

### 💬 可借鉴之处
TextWorld 为研究文本游戏中的强化学习提供了一个强大的工具。其生成机制、逻辑引擎和文本生成模块可以用于构建各种复杂的游戏环境，并帮助代理学习语言理解、规划和探索等技能。此外，TextWorld 还可以用于研究泛化和迁移学习，以及开发新的强化学习算法。

## counting-to-explore-and-generalize-in-text-based-games
### Abstract
We propose a recurrent RL agent with an episodic exploration mechanism that
helps discovering good policies in text-based game environments. We show
promising results on a set of generated text-based games of varying difficulty
where the goal is to collect a coin located at the end of a chain of rooms. In
contrast to previous text-based RL approaches, we observe that our agent learns
policies that generalize to unseen games of greater difficulty.
### 🌟 论文解读 | 基于计数探索和泛化的文本游戏强化学习

### 📌 背景痛点/本文动机
文本游戏是一种复杂的交互式模拟，使用自然语言描述游戏状态、接受玩家动作并报告环境变化。玩家必须通过探索来发现游戏目标，而游戏中的观察和动作空间都是组合和复合的，玩家必须应对部分可观察性，因为描述性文本并不提供关于底层游戏状态的完整、明确的信息。

### 🚀 核心方法
💡 创新点1：提出了一种基于循环的强化学习代理，具有情节探索机制，有助于在基于文本的游戏环境中发现良好的策略。
💡 创新点2：提出了一种情节计数探索方案，其中状态计数在每个情节开始时重置。这种奖励充当情节记忆，推动代理访问当前情节中尚未遇到的状态。

### 📈 实验结果
在一系列生成的基于文本的游戏中，该代理在收集位于一系列房间末尾的硬币的目标上取得了有希望的结果。与之前的基于文本的强化学习方法相比，观察到该代理学习的策略可以泛化到未见的更具挑战性的游戏。

### 💬 可借鉴之处
该论文提出的基于计数探索和泛化的文本游戏强化学习方法，为解决文本游戏中的探索和泛化问题提供了一种新的思路。该方法可以应用于其他需要探索和记忆能力的强化学习任务，例如迷宫探索、路径规划等。

## pre-trained-language-models-as-prior-knowledge-for-playing-text-based-games
### Abstract
Recently, text world games have been proposed to enable artificial agents to
understand and reason about real-world scenarios. These text-based games are
challenging for artificial agents, as it requires an understanding of and
interaction using natural language in a partially observable environment.
Agents observe the environment via textual descriptions designed to be
challenging enough for even human players. Past approaches have not paid enough
attention to the language understanding capability of the proposed agents.
Typically, these approaches train from scratch, an agent that learns both
textual representations and the gameplay online during training using a
temporal loss function. Given the sample-inefficiency of RL approaches, it is
inefficient to learn rich enough textual representations to be able to
understand and reason using the textual observation in such a complicated game
environment setting. In this paper, we improve the semantic understanding of
the agent by proposing a simple RL with LM framework where we use
transformer-based language models with Deep RL models. We perform a detailed
study of our framework to demonstrate how our model outperforms all existing
agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6
higher than the state-of-the-art model. Overall, our proposed approach
outperforms 4 games out of the 14 text-based games, while performing comparable
to the state-of-the-art models on the remaining games.
### 🌟 论文解读 | 利用预训练语言模型提升文本游戏中的智能体表现

### 📌 背景痛点/本文动机
文本世界游戏为人工智能体提供了理解和推理现实世界场景的机会。然而，这些游戏对智能体来说极具挑战性，因为它们需要在部分可观察的环境中理解和交互自然语言。现有的方法往往忽略了智能体的语言理解能力，并且通常从头开始训练，导致样本效率低下。本文旨在通过引入预训练语言模型来提升智能体的语义理解能力，从而在文本游戏中取得更好的表现。

### 🚀 核心方法
💡 创新点1：预训练语言模型作为先验知识
本文提出了一种简单的RL与LM框架，使用基于Transformer的语言模型（如DistilBERT）与深度强化学习模型相结合。通过在大型通用英语语料库上进行预训练，然后针对特定下游任务进行微调，预训练语言模型能够为智能体提供丰富的语言理解和先验知识。

💡 创新点2：游戏感知的预训练语言模型
为了使预训练语言模型更好地适应游戏环境，本文使用独立的人类游戏播放轨迹数据集对DistilBERT进行微调，从而使其具备游戏感知能力。这种微调过程有助于将语言模型的知识和世界感知能力转移到不同的游戏和智能体中。

### 📈 实验结果
本文在14个文本游戏中对所提出的框架进行了评估，结果显示，在Zork1游戏中，模型取得了44.7分的成绩，比现有最佳模型高出1.6分。总体而言，该框架在4个游戏中超越了现有模型，并在其他游戏中表现与现有模型相当。

### 💬 可借鉴之处
本文提出的预训练语言模型作为先验知识的方法，为文本游戏中的智能体设计提供了新的思路。通过利用预训练语言模型的知识和世界感知能力，智能体能够更好地理解和推理游戏环境，从而在游戏中取得更好的表现。此外，本文还强调了微调预训练语言模型以适应特定游戏环境的重要性，这为未来研究提供了有价值的启示。

## language-model-in-the-loop--data-optimal-approach-to-learn-to-recommend-actions-in-text-games
### Abstract
Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.
### 🌟 论文解读 | 语言模型在循环中：文本游戏中学习推荐动作的数据最优方法

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在语言理解基准测试中表现出色。CALM 是一种流行的学习方法，它利用 LLMs（例如 GPT-2）的语言先验知识来推荐动作候选者，从而在没有环境提供动作的情况下提高 Jericho 中文本游戏的表现。然而，CALM 使用带有人类游戏玩法的注释来适应 GPT-2，并在学习文本游戏时保持 LLM 固定。这种方法依赖于昂贵的人类注释游戏玩法，并且没有充分利用游戏中的转换来训练 LLM。本文旨在探索和评估在文本游戏学习过程中更新用于候选推荐的语言模型，以减少对人类注释游戏玩法的依赖。

### 🚀 核心方法
本文提出了 LM-in-the-Loop 方法，该方法在文本游戏学习过程中更新用于候选推荐的语言模型。具体来说，该方法使用精心选择的游戏内转换来更新 LLM，从而减少对人类注释游戏玩法的依赖。此外，本文还分析了更新后的 LLM 的迁移性，并发现将游戏内训练的模型迁移到其他游戏并不总是导致一致的性能提升。

### 📈 实验结果
实验结果表明，LM-in-the-Loop 方法可以减少对人类注释游戏玩法的依赖，并加速收敛。此外，基于状态特征的转换选择方法比其他方法提供了更大的收益。然而，LM-in-the-Loop 方法并不总是能够迁移到其他游戏。

### 💬 可借鉴之处
本文提出的 LM-in-the-Loop 方法为文本游戏中学习推荐动作提供了一种数据最优的方法。该方法可以减少对人类注释游戏玩法的依赖，并加速收敛。此外，本文还分析了更新后的 LLM 的迁移性，并发现将游戏内训练的模型迁移到其他游戏并不总是导致一致的性能提升。这些发现对于理解和改进文本游戏中语言模型的性能具有重要意义。

## keep-calm-and-explore--language-models-for-action-generation-in-text-based-games
### Abstract
Text-based games present a unique challenge for autonomous agents to operate
in natural language and handle enormous action spaces. In this paper, we
propose the Contextual Action Language Model (CALM) to generate a compact set
of action candidates at each game state. Our key insight is to train language
models on human gameplay, where people demonstrate linguistic priors and a
general game sense for promising actions conditioned on game history. We
combine CALM with a reinforcement learning agent which re-ranks the generated
action candidates to maximize in-game rewards. We evaluate our approach using
the Jericho benchmark, on games unseen by CALM during training. Our method
obtains a 69% relative improvement in average game score over the previous
state-of-the-art model. Surprisingly, on half of these games, CALM is
competitive with or better than other models that have access to ground truth
admissible actions. Code and data are available at
https://github.com/princeton-nlp/calm-textgame.
### 🌟 论文解读 | Keep CALM and Explore: 语言模型在文本游戏中的动作生成

### 📌 背景痛点/本文动机
文本游戏为自主代理在自然语言中操作和处理巨大的动作空间带来了独特的挑战。在文本游戏中，只有一小部分动作命令在任何给定的游戏状态下是可接受的。可接受的动作是指可以被游戏引擎解析并改变底层游戏状态的动作。例如，在图1中，我们可以观察到，从游戏词汇中随机采样动作会导致许多不可接受的动作，如“north a”或“eat troll with egg”。因此，将动作空间缩小到可接受的动作需要语法和语义知识，这对当前系统来说是一个挑战。

### 🚀 核心方法
本文提出了上下文动作语言模型（CALM），以缓解这一挑战。具体来说，在游戏的每个步骤中，我们使用CALM生成动作候选者，然后将它们输入到深度强化相关网络（DRRN）中，该网络使用游戏奖励来学习这些动作的价值函数。这使我们的模型能够将动作生成的通用语言先验与自适应选择最适合游戏的动作的能力相结合。

为了训练CALM，我们引入了一个包含426个人类游戏回放记录的新数据集，这些记录来自590个不同的文本游戏。虽然这些回放记录是嘈杂的，并且动作并不总是最优的，但它们包含大量的语言先验和游戏感。使用这个数据集，我们训练了一个CALM实例，并将其部署到许多不同的下游游戏中。重要的是，为了展示我们方法的泛化能力，我们没有使用任何来自我们评估游戏的回放记录来训练语言模型。

### 📈 实验结果
我们使用Jericho基准（Hausknecht等人，2019a）对28个游戏进行了评估，这些游戏在CALM训练期间没有见过。我们的方法在平均游戏得分方面比以前最先进的模型提高了69%。令人惊讶的是，在这些游戏中的一半，CALM与其他具有可接受动作真实值的方法具有竞争力或更好。

### 💬 可借鉴之处
本文提出的CALM模型为文本游戏中的动作生成提供了一种有效的方法。通过结合语言模型和强化学习，CALM能够生成高质量、上下文相关的动作，从而提高游戏代理的性能。此外，本文还引入了一个新的人类游戏回放记录数据集，为文本游戏研究提供了宝贵的资源。

## graph-constrained-reinforcement-learning-for-natural-language-action-spaces
### Abstract
Interactive Fiction games are text-based simulations in which an agent
interacts with the world purely through natural language. They are ideal
environments for studying how to extend reinforcement learning agents to meet
the challenges of natural language understanding, partial observability, and
action generation in combinatorially-large text-based action spaces. We present
KG-A2C, an agent that builds a dynamic knowledge graph while exploring and
generates actions using a template-based action space. We contend that the dual
uses of the knowledge graph to reason about game state and to constrain natural
language generation are the keys to scalable exploration of combinatorially
large natural language actions. Results across a wide variety of IF games show
that KG-A2C outperforms current IF agents despite the exponential increase in
action space size.
### 🌟 论文解读 | 图约束强化学习在自然语言动作空间中的应用

### 📌 背景痛点/本文动机
自然语言交互长期以来被认为是人类智能的一个标志性特征。本文旨在研究如何使学习代理能够理解和生成与上下文相关的自然语言，以实现目标。为了实现这一目标，本文研究了交互式小说（IF）游戏，这是一种文本冒险游戏，其中代理完全通过自然语言与游戏世界进行交互。IF游戏是研究如何扩展强化学习代理以应对自然语言理解、部分可观察性和在组合性大的文本动作空间中生成动作的挑战的理想环境。

### 🚀 核心方法
本文提出了KG-A2C，一种在探索过程中构建动态知识图并使用基于模板的动作空间生成动作的代理。本文认为，知识图的双重用途，即用于推理游戏状态和约束自然语言生成，是可扩展地探索组合性大的自然语言动作的关键。

#### 💡 创新点1：知识图状态空间
KG-A2C使用知识图作为状态表示，该知识图在探索过程中学习。知识图存储为一系列三元组，包括主体、关系和对象。这些三元组从观察中提取，并使用斯坦福大学的开放信息提取（OpenIE）工具进行更新。知识图帮助代理形成它正在探索的世界的地图，并保留它已经学习到的信息，例如与对象相关的功能、角色的属性、当前库存等。

#### 💡 创新点2：模板动作空间
KG-A2C使用基于模板的动作空间，其中代理首先选择一个模板，然后使用游戏词汇表中的单词填写空白。模板和词汇表单词通过Jericho框架程序化地可用，因此对于每个IF游戏都是可用的。模板为动作空间提供了结构，使得代理能够有效地探索动作空间。

### 📈 实验结果
本文在一系列IF游戏上测试了KG-A2C，并将其与当前IF代理进行了比较。结果表明，KG-A2C在大多数游戏中都优于当前IF代理，尽管动作空间的大小呈指数级增长。

### 💬 可借鉴之处
本文提出的KG-A2C方法为自然语言交互式游戏提供了一个有效的解决方案。知识图状态空间和模板动作空间的结合使得代理能够有效地探索组合性大的自然语言动作空间。本文的研究结果对于开发更智能的自然语言交互式游戏代理具有重要的参考价值。

## interactive-fiction-game-playing-as-multi-paragraph-reading-comprehension-with-reinforcement-learning
### Abstract
Interactive Fiction (IF) games with real human-written natural language texts
provide a new natural evaluation for language understanding techniques. In
contrast to previous text games with mostly synthetic texts, IF games pose
language understanding challenges on the human-written textual descriptions of
diverse and sophisticated game worlds and language generation challenges on the
action command generation from less restricted combinatorial space. We take a
novel perspective of IF game solving and re-formulate it as Multi-Passage
Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query
attention mechanisms and the structured prediction in MPRC to efficiently
generate and evaluate action outputs and apply an object-centric historical
observation retrieval strategy to mitigate the partial observability of the
textual observations. Extensive experiments on the recent IF benchmark
(Jericho) demonstrate clear advantages of our approaches achieving high winning
rates and low data requirements compared to all previous approaches. Our source
code is available at: https://github.com/XiaoxiaoGuo/rcdqn.
### 🌟 论文解读 | 利用多段落阅读理解与强化学习解决互动式小说游戏

### 📌 背景痛点/本文动机
互动式小说（Interactive Fiction, IF）游戏以其丰富的文本描述和复杂的游戏世界为自然语言理解（NLU）技术提供了新的挑战。与以往主要使用合成文本的文本游戏不同，IF 游戏中的文本描述更加多样化和复杂，对语言理解提出了更高的要求。此外，IF 游戏中的行动命令生成也面临着更大的挑战，因为玩家可以使用的行动命令组合空间更加开放和自由。

### 🚀 核心方法
💡 创新点1：将 IF 游戏解决过程重新定义为多段落阅读理解（MPRC）任务。这种方法利用了 MPRC 中的上下文-查询注意力机制和结构化预测，可以有效地生成和评估行动输出，并应用以对象为中心的历史观察检索策略来缓解文本观察的局部可观测性。

💡 创新点2：提出了一种基于阅读理解模型（RC）的模板行动预测模型。该模型将观察视为段落，将模板动词短语视为问题，并将模板中对象占位符的填充视为从观察中提取对象的提取式问答（QA）问题。同时，每个行动（即所有占位符都被替换的模板）都通过 RC 模型预测其评估值。

💡 创新点3：提出了一种基于对象的历史观察检索策略。该方法根据当前观察中检测到的对象，检索最近 K 个至少共享一个对象的观察。这些检索到的观察按时间步排序并连接到当前观察，以增强当前观察的信息。

### 📈 实验结果
在 Jericho IF 游戏基准测试中，与所有先前的方法相比，本文提出的方法在 25 个游戏中的胜率达到了 64%，并且所需的数据量不到先前方法的十分之一。此外，本文还进行了消融研究，结果表明，RC 模型设计和检索策略对性能提升起着重要作用。

### 💬 可借鉴之处
本文提出的将 IF 游戏解决过程重新定义为 MPRC 任务的方法，为解决 IF 游戏中的巨大组合行动空间和局部可观测性挑战提供了一种新的思路。此外，本文提出的基于 RC 模型的模板行动预测模型和基于对象的历史观察检索策略，也为解决其他 NLU 任务提供了可借鉴的经验。

## bart--denoising-sequence-to-sequence-pre-training-for-natural-language-generation--translation--and-comprehension
### Abstract
We present BART, a denoising autoencoder for pretraining sequence-to-sequence
models. BART is trained by (1) corrupting text with an arbitrary noising
function, and (2) learning a model to reconstruct the original text. It uses a
standard Tranformer-based neural machine translation architecture which,
despite its simplicity, can be seen as generalizing BERT (due to the
bidirectional encoder), GPT (with the left-to-right decoder), and many other
more recent pretraining schemes. We evaluate a number of noising approaches,
finding the best performance by both randomly shuffling the order of the
original sentences and using a novel in-filling scheme, where spans of text are
replaced with a single mask token. BART is particularly effective when fine
tuned for text generation but also works well for comprehension tasks. It
matches the performance of RoBERTa with comparable training resources on GLUE
and SQuAD, achieves new state-of-the-art results on a range of abstractive
dialogue, question answering, and summarization tasks, with gains of up to 6
ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system
for machine translation, with only target language pretraining. We also report
ablation experiments that replicate other pretraining schemes within the BART
framework, to better measure which factors most influence end-task performance.
### 🌟 论文解读 | BART：自然语言生成、翻译和理解的降噪序列到序列预训练模型

### 📌 背景痛点/本文动机
自然语言处理（NLP）领域近年来取得了显著进展，自监督学习方法在多种NLP任务中取得了令人瞩目的成果。然而，现有的预训练模型往往针对特定类型的任务进行优化，限制了其通用性和适用范围。本文提出了一种名为BART的降噪自编码器，旨在通过预训练序列到序列模型，提高模型在自然语言生成、翻译和理解等任务上的性能。

### 🚀 核心方法
💡 创新点1：降噪自编码器
BART采用了一种新颖的降噪自编码器架构，通过将文本进行随机噪声处理，然后学习一个模型来重建原始文本。这种架构具有很高的灵活性，可以应用于各种文本噪声处理方法，包括随机打乱句子顺序、使用掩码替换文本片段等。

💡 创新点2：Transformer架构
BART使用标准的Transformer架构，包括双向编码器和自回归解码器。这种架构简单而强大，可以看作是对BERT（双向编码器）和GPT（自回归解码器）的泛化，同时适用于多种预训练方案。

### 📈 实验结果
BART在多个NLP任务上取得了优异的性能。在文本生成任务中，BART在摘要、问答和对话生成等任务上取得了新的SOTA结果，并在ROUGE指标上取得了高达6分的提升。在理解任务中，BART在GLUE和SQuAD等基准测试中与RoBERTa相当，并在机器翻译任务中取得了1.1 BLEU的提升。

### 💬 可借鉴之处
BART的降噪自编码器架构为NLP预训练模型提供了一种新的思路，可以应用于各种文本噪声处理方法，并提高模型在多种任务上的性能。此外，BART的Transformer架构简单而强大，可以看作是对BERT和GPT的泛化，为NLP研究提供了新的方向。

## deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games
### Abstract
In this study, we explore the application of Large Language Models (LLMs) in
\textit{Jubensha}, a Chinese detective role-playing game and a novel area in
Artificial Intelligence (AI) driven gaming. We introduce the first dataset
specifically for Jubensha, including character scripts and game rules, to
foster AI agent development in this complex narrative environment. Our work
also presents a unique multi-agent interaction framework using LLMs, allowing
AI agents to autonomously engage in this game. To evaluate the gaming
performance of these AI agents, we developed novel methods measuring their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents'
performance in information gathering, murderer identification, and logical
reasoning. The experimental results validate the effectiveness of our proposed
methods. This work aims to offer a novel perspective on understanding LLM
capabilities and establish a new benchmark for evaluating large language
model-based agents.
### 🌟 论文解读 | 解码数字侦探：理解大型语言模型在多智能体推理游戏中的行为和能力

### 📌 背景痛点/本文动机
随着互动角色扮演游戏（IRPGs）的全球流行，特别是中国侦探角色扮演游戏“剧本杀”（Jubensha）的兴起，人工智能（AI）在游戏领域的应用也日益受到关注。然而，现有的AI研究主要集中在传统的棋类游戏、视频游戏等领域，对于“剧本杀”这类需要多轮语言交互、信息收集和逻辑推理的游戏，AI的应用还处于起步阶段。本文旨在探索大型语言模型（LLMs）在“剧本杀”游戏中的应用，并建立一个新的评估基准，以衡量LLM在复杂叙事环境中的能力。

### 🚀 核心方法
💡 创新点1：构建了首个专门针对“剧本杀”游戏的中文数据集，包括角色剧本和预设游戏规则，为AI代理的开发提供了基础。
💡 创新点2：设计了一个独特的多智能体交互框架，使用LLMs使AI代理能够自主参与“剧本杀”游戏。
💡 创新点3：为了评估AI代理在“剧本杀”游戏中的表现，设计了两个新颖的任务：一个用于评估他们对案件信息的掌握程度，另一个用于评估他们的推理能力。
💡 创新点4：利用最新的上下文学习技术，设计了模块来增强LLM代理在信息收集、凶手识别和逻辑推理方面的性能。

### 📈 实验结果
实验结果表明，本文提出的方法在信息收集、凶手识别和推理能力方面显著提高了LLM代理的性能。具体来说，与没有记忆检索模块的代理相比，具有记忆检索模块的代理在回答关于其他角色的问题时准确率显著提高。此外，自完善和自验证模块的组合进一步提高了代理的准确率，表明这些模块有效地增强了代理在“剧本杀”游戏中的沟通效率。

### 💬 可借鉴之处
本文的研究为LLMs在复杂叙事环境中的应用提供了新的视角，并为评估LLM代理的性能建立了新的基准。此外，本文提出的ThinkThrice框架和上下文学习模块的设计，为开发更智能、更具推理能力的AI代理提供了有价值的参考。

## chess-as-a-testbed-for-language-model-state-tracking
### Abstract
Transformer language models have made tremendous strides in natural language
understanding tasks. However, the complexity of natural language makes it
challenging to ascertain how accurately these models are tracking the world
state underlying the text. Motivated by this issue, we consider the task of
language modeling for the game of chess. Unlike natural language, chess
notations describe a simple, constrained, and deterministic domain. Moreover,
we observe that the appropriate choice of chess notation allows for directly
probing the world state, without requiring any additional probing-related
machinery. We find that: (a) With enough training data, transformer language
models can learn to track pieces and predict legal moves with high accuracy
when trained solely on move sequences. (b) For small training sets providing
access to board state information during training can yield significant
improvements. (c) The success of transformer language models is dependent on
access to the entire game history i.e. "full attention". Approximating this
full attention results in a significant performance drop. We propose this
testbed as a benchmark for future work on the development and analysis of
transformer language models.
### 🌟 论文解读 | 国际象棋：语言模型状态跟踪的测试平台

### 📌 背景痛点/本文动机
随着Transformer语言模型在自然语言理解任务上的巨大进步，人们开始关注这些模型是否能够准确跟踪文本背后的世界状态。然而，自然语言的复杂性使得评估模型的准确性变得困难。为了解决这个问题，本文提出将国际象棋作为语言模型状态跟踪能力的测试平台。

### 🚀 核心方法
💡 创新点1：将国际象棋作为测试平台
国际象棋是一个简单、受限且确定性的领域，其棋谱描述可以直接反映棋盘状态，无需额外的探测机制。这使得我们可以更精确地评估语言模型对世界状态的跟踪能力。

💡 创新点2：使用合适的棋谱表示法
本文使用国际象棋通用接口（UCI）表示法，它将起始方格和目标方格结合起来表示一个移动。这种表示法允许我们通过简单的提示来探测语言模型对棋盘状态的理解。

💡 创新点3：引入随机标注棋子类型（RAP）
为了更直接地探测模型的状态跟踪能力，本文提出在训练过程中以一定的概率随机包含棋子类型标记。这可以帮助模型学习跟踪棋子位置，并提高语言模型的性能。

💡 创新点4：设计棋盘状态探测任务
本文设计了多种棋盘状态探测任务，包括预测移动的起始方格和目标方格，以评估模型对棋盘状态和棋规的理解。

### 📈 实验结果
实验结果表明，当给定足够的训练数据时，Transformer语言模型可以学习跟踪棋子位置并预测合法移动，准确率很高。然而，当训练数据较少时，预测能力会下降。在这种情况下，引入部分棋盘状态信息可以显著提高棋子跟踪的准确率。

### 💬 可借鉴之处
本文提出的国际象棋测试平台为评估语言模型的状态跟踪能力提供了一个有效的工具。此外，本文的研究结果也为Transformer语言模型的发展和分析提供了新的见解，例如模型对输入分布变化的鲁棒性以及对完整上下文信息的需求。这些发现可以指导未来Transformer架构的设计，使其更擅长理解长文本并从少量训练数据中学习。

## deepstack--expert-level-artificial-intelligence-in-no-limit-poker
### Abstract
Artificial intelligence has seen several breakthroughs in recent years, with
games often serving as milestones. A common feature of these games is that
players have perfect information. Poker is the quintessential game of imperfect
information, and a longstanding challenge problem in artificial intelligence.
We introduce DeepStack, an algorithm for imperfect information settings. It
combines recursive reasoning to handle information asymmetry, decomposition to
focus computation on the relevant decision, and a form of intuition that is
automatically learned from self-play using deep learning. In a study involving
44,000 hands of poker, DeepStack defeated with statistical significance
professional poker players in heads-up no-limit Texas hold'em. The approach is
theoretically sound and is shown to produce more difficult to exploit
strategies than prior approaches.
### 🌟 论文解读 | DeepStack：在不完美信息游戏中实现专家级人工智能

### 📌 背景痛点/本文动机
近年来，人工智能在许多游戏中取得了突破性进展，例如国际象棋、围棋等。这些游戏的一个共同特点是玩家拥有完美信息，即所有玩家都能完全了解当前的游戏状态。然而，现实世界中的许多问题都涉及到信息不对称，即玩家拥有不同的信息。扑克牌游戏就是一个典型的例子，玩家拥有私人牌，这使得游戏充满了不确定性。

### 🚀 核心方法
DeepStack 是一种针对不完美信息场景的算法，它结合了递归推理、分解和深度学习，实现了在不完美信息游戏中达到专家级水平。

💡 创新点1：递归推理
DeepStack 使用递归推理来处理信息不对称问题。它通过分析对手过去的行动来推断对手可能持有的私人牌，从而做出更明智的决策。

💡 创新点2：分解
DeepStack 使用分解技术将复杂的游戏状态分解成更小的子问题，从而将计算集中在相关的决策上。这大大提高了算法的效率。

💡 创新点3：深度学习
DeepStack 使用深度学习来自动学习一种直觉，即对持有任何可能的私人牌在任何可能的扑克牌情况下的价值进行快速估计。这种直觉可以帮助 DeepStack 在游戏中做出更明智的决策。

### 📈 实验结果
DeepStack 在与专业扑克牌玩家的对战中取得了显著的优势。在一项涉及 44,000 手扑克牌的研究中，DeepStack 以统计显著的方式击败了专业扑克牌玩家。

### 💬 可借鉴之处
DeepStack 的成功表明，深度学习可以用于解决不完美信息游戏中的复杂问题。DeepStack 的方法可以应用于其他不完美信息场景，例如医疗诊断、战略资源防御等。

### 🌟 总结
DeepStack 是人工智能领域的一个重要突破，它展示了在不完美信息游戏中实现专家级水平的可能性。DeepStack 的方法为解决现实世界中的复杂问题提供了新的思路。

## too-many-cooks--bayesian-inference-for-coordinating-multi-agent-collaboration
### Abstract
Collaboration requires agents to coordinate their behavior on the fly,
sometimes cooperating to solve a single task together and other times dividing
it up into sub-tasks to work on in parallel. Underlying the human ability to
collaborate is theory-of-mind, the ability to infer the hidden mental states
that drive others to act. Here, we develop Bayesian Delegation, a decentralized
multi-agent learning mechanism with these abilities. Bayesian Delegation
enables agents to rapidly infer the hidden intentions of others by inverse
planning. We test Bayesian Delegation in a suite of multi-agent Markov decision
processes inspired by cooking problems. On these tasks, agents with Bayesian
Delegation coordinate both their high-level plans (e.g. what sub-task they
should work on) and their low-level actions (e.g. avoiding getting in each
other's way). In a self-play evaluation, Bayesian Delegation outperforms
alternative algorithms. Bayesian Delegation is also a capable ad-hoc
collaborator and successfully coordinates with other agent types even in the
absence of prior experience. Finally, in a behavioral experiment, we show that
Bayesian Delegation makes inferences similar to human observers about the
intent of others. Together, these results demonstrate the power of Bayesian
Delegation for decentralized multi-agent collaboration.
### 🌟 论文解读 | 多智能体协作中的贝叶斯推理：解决“厨师太多”的问题

### 📌 背景痛点/本文动机
协作是智能体在多智能体环境中实现共同目标的关键。然而，协作面临着协调行为的挑战，尤其是在没有先验经验、社会角色和规范的情况下。人类通过“心智理论”（Theory-of-Mind，ToM）来理解他人的意图，从而实现高效的协作。本文旨在构建具有心智理论的智能体，并利用这些能力来协调协作。

### 🚀 核心方法
💡 创新点1：贝叶斯委托（Bayesian Delegation）
本文提出了贝叶斯委托，这是一种新的去中心化多智能体学习机制，它能够快速推断其他智能体的隐藏意图。贝叶斯委托通过逆向规划来实现这一点，使智能体能够在不确定性下预测其他智能体的意图，并有效地将自身努力委托给最有价值的协作任务。

💡 创新点2：多智能体马尔可夫决策过程（MMDP）与子任务
本文研究了具有子任务的多智能体马尔可夫决策过程，其中子任务构成了智能体之间高层次协调的目标。子任务允许智能体在三个不同的方面进行协调：分割和征服、合作以及时空移动。

### 📈 实验结果
本文在一系列受烹饪问题启发的多智能体马尔可夫决策过程中测试了贝叶斯委托的性能。结果表明，贝叶斯委托在完成所有环境方面优于其他算法，并且即使在扩展到更大的团队时也能保持性能。此外，贝叶斯委托是一个有效的临时协作者，即使在没有先验经验的情况下也能与其他类型的智能体成功协作。最后，在行为实验中，贝叶斯委托的推理与人类观察者关于他人意图的推理相似。

### 💬 可借鉴之处
本文提出的贝叶斯委托为去中心化多智能体协作提供了一种有效的方法。它通过逆向规划和贝叶斯推理来推断其他智能体的意图，从而实现高效的协作。此外，本文还探讨了多智能体马尔可夫决策过程与子任务的概念，为研究多智能体协作提供了新的视角。

## watch-and-help--a-challenge-for-social-perception-and-human-ai-collaboration
### Abstract
In this paper, we introduce Watch-And-Help (WAH), a challenge for testing
social intelligence in agents. In WAH, an AI agent needs to help a human-like
agent perform a complex household task efficiently. To succeed, the AI agent
needs to i) understand the underlying goal of the task by watching a single
demonstration of the human-like agent performing the same task (social
perception), and ii) coordinate with the human-like agent to solve the task in
an unseen environment as fast as possible (human-AI collaboration). For this
challenge, we build VirtualHome-Social, a multi-agent household environment,
and provide a benchmark including both planning and learning based baselines.
We evaluate the performance of AI agents with the human-like agent as well as
with real humans using objective metrics and subjective user ratings.
Experimental results demonstrate that the proposed challenge and virtual
environment enable a systematic evaluation on the important aspects of machine
social intelligence at scale.
### 🌟 论文解读 | Watch-And-Help：社会感知与人类-AI协作的挑战

### 📌 背景痛点/本文动机
人类在很小的年纪就展现出利他行为，能够通过观察他人的行为来理解其目标，并制定计划来帮助他们，即使在新的场景中也能做到。然而，目前最先进的AI系统仍然难以掌握这些基本的社会技能。为了实现有效帮助人类所需的社会智能水平，AI代理需要具备两个关键能力：社会感知（理解人类行为的能力）和协作规划（推理物理环境并规划其行动以与人类协调的能力）。

### 🚀 核心方法
本文提出了一个新的AI挑战：Watch-And-Help (WAH)，重点关注社会感知和人类-AI协作。在WAH挑战中，AI代理需要与一个类似人类的代理协作，以更有效地完成复杂的家庭任务。该挑战分为两个阶段：
1. **观察阶段**：AI代理（Bob）观察类似人类的代理（Alice）执行任务，并从她的行为中推断出她的目标。
2. **帮助阶段**：Bob与Alice协作，在新环境中尽可能快地完成相同的任务。

为了实现多代理交互，本文扩展了开源虚拟平台VirtualHome，并构建了一个多代理虚拟环境VirtualHome-Social。该环境模拟了真实且丰富的家庭环境，代理可以与不同的对象（例如，打开容器或抓取对象）和其他代理（例如，跟随、帮助、避免碰撞）进行交互，以执行复杂的任务。VirtualHome-Social还提供了内置代理，模拟人类行为，允许AI代理与虚拟人类一起进行训练和测试，并提供了一个接口，允许使用真实人类进行评估，并收集/显示真实环境中的人类活动（这是现有多代理平台不提供的功能）。

### 📈 实验结果
实验结果表明，为了在WAH挑战中取得成功，AI代理必须具备强大的社会感知能力和可推广的帮助策略。这些机器社会智能的基本方面已被证明是先前工作中人类-AI协作的关键。本文提出的挑战和虚拟环境能够系统地评估机器社会智能的重要方面。

### 💬 可借鉴之处
1. **WAH挑战**：为评估AI代理的社会感知能力和协作能力提供了一个新的框架。
2. **VirtualHome-Social环境**：为AI代理执行复杂的家庭任务提供了一个多代理平台，并支持与内置代理或真实人类进行交互。
3. **基准测试**：包括多个基于规划和学习的基线方法，突出了机器社会智能的重要方面。

### 🌟 未来展望
本文提出的挑战和虚拟环境为构建更复杂的机器社会智能开辟了令人兴奋的方向，例如在线目标推理和代理之间的直接通信。希望WAH挑战和VirtualHome-Social环境能够促进未来对构建更复杂机器社会智能的研究。

## the-threedworld-transport-challenge--a-visually-guided-task-and-motion-planning-benchmark-for-physically-realistic-embodied-ai
### Abstract
We introduce a visually-guided and physics-driven task-and-motion planning
benchmark, which we call the ThreeDWorld Transport Challenge. In this
challenge, an embodied agent equipped with two 9-DOF articulated arms is
spawned randomly in a simulated physical home environment. The agent is
required to find a small set of objects scattered around the house, pick them
up, and transport them to a desired final location. We also position containers
around the house that can be used as tools to assist with transporting objects
efficiently. To complete the task, an embodied agent must plan a sequence of
actions to change the state of a large number of objects in the face of
realistic physical constraints. We build this benchmark challenge using the
ThreeDWorld simulation: a virtual 3D environment where all objects respond to
physics, and where can be controlled using fully physics-driven navigation and
interaction API. We evaluate several existing agents on this benchmark.
Experimental results suggest that: 1) a pure RL model struggles on this
challenge; 2) hierarchical planning-based agents can transport some objects but
still far from solving this task. We anticipate that this benchmark will
empower researchers to develop more intelligent physics-driven robots for the
physical world.
### 🌟 论文解读 | 3DWorld Transport Challenge：物理世界中的智能机器人挑战

### 📌 背景痛点/本文动机
随着人工智能和机器人技术的不断发展，能够在物理世界中感知和行动的机器人成为了计算机视觉和机器人社区的重要目标。然而，直接使用真实机器人进行训练和评估成本高昂且存在安全风险。因此，近年来，人们开始将模拟器纳入训练和评估人工智能算法的过程中。尽管3D虚拟环境在视觉导航方面取得了显著进展，但它们大多关注视觉导航，而忽略了物理交互。由于最终目标是开发能够在物理环境中感知和行动的系统，因此物理交互已成为家庭助理机器人训练的必要组成部分。

### 🚀 核心方法
本文提出了一个新的具身AI挑战：一个具有两个9自由度关节臂的具身智能体被随机放置在一个物理真实的虚拟家庭环境中。智能体需要探索房屋，寻找散落在不同房间中的少量物体，并将它们运送到一个期望的最终位置。此外，房屋周围还放置了各种容器，智能体可以找到这些容器并将物体放入其中。不使用容器作为工具时，智能体只能一次运输两个物体。然而，使用容器，智能体可以收集多个物体并一起运输。

为了支持这项挑战，本文创建了一个基于TDW的房屋数据集，其中包含充满物理响应物体的多房间环境。此外，还开发了一个完全基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。由于模拟动作和环境完全基于物理，与之前的非物理或部分物理虚拟环境相比，它们提出了额外的挑战。例如，交互动作只有在目标物理可达时（即靠近且未被阻挡）才会成功。如果目标不在智能体的中心视图中，或者直接路径被阻挡（例如，被桌子阻挡），智能体就无法成功抓取物体。此外，与房屋中的物体发生物理碰撞也可能显著阻碍运输进度。因此，智能体必须学习利用视觉信号来同步导航和操作，以应对这些物理约束。

### 📈 实验结果
本文评估了几个现有的智能体，实验结果表明，现有的具身智能体在完成这项任务方面都存在困难。本文相信，在运输挑战中表现良好的模型将能够使机器人更加智能，能够在真实的物理世界中发挥作用。

### 💬 可借鉴之处
本文提出的3DWorld Transport Challenge为具身智能体在物理真实环境中的任务和运动规划能力提供了一个新的评估标准。此外，本文还开发了一个完全基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。这些成果为开发能够在物理世界中感知和行动的智能机器人提供了新的思路和方法。

## ai2-thor--an-interactive-3d-environment-for-visual-ai
### Abstract
We introduce The House Of inteRactions (THOR), a framework for visual AI
research, available at http://ai2thor.allenai.org. AI2-THOR consists of near
photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes
and interact with objects to perform tasks. AI2-THOR enables research in many
different domains including but not limited to deep reinforcement learning,
imitation learning, learning by interaction, planning, visual question
answering, unsupervised representation learning, object detection and
segmentation, and learning models of cognition. The goal of AI2-THOR is to
facilitate building visually intelligent models and push the research forward
in this domain.
### 🌟 论文解读 | AI2-THOR：推动视觉AI研究的新一代交互式3D环境

### 📌 背景痛点/本文动机
人类在视觉理解方面展现出超越当前主流视觉任务（如物体检测、场景识别、图像分割）的能力。视觉智能的关键在于与环境交互并从中学习。然而，当前最先进的计算机视觉模型通常是通过使用静态图像或视频进行训练，这与人类的学习方式不同。为了解决这个问题，本文提出了AI2-THOR，一个基于视觉输入的类似人类学习的框架。

### 🚀 核心方法
💡 创新点1：交互性
AI2-THOR支持多种类型的交互，包括物体状态变化、基于臂部的操作和因果交互。例如，可以打开或关闭微波炉，将面包切片并在烤面包机中烤制，以及打开水龙头将水倒入杯子中。

💡 创新点2：场景多样性
AI2-THOR提供了比其他平台更多的交互式物体和场景，通过使用程序生成技术，提供了120个独立的房间、89个迷宫风格的公寓和10个评估房屋。

💡 创新点3：高质量
AI2-THOR中的物体和场景接近照片真实感，这有助于将学习模型更好地转移到现实世界。

💡 创新点4：API
AI2-THOR提供了一个Python API，用于与Unity 3D游戏引擎交互，该引擎提供了许多不同的功能，如导航、施加力、物体交互和物理建模。

### 📈 实验结果
自2017年首次发布以来，AI2-THOR已被用于超过150篇论文的实验，并被下载超过50万次。它在视觉导航、视听导航、视觉和语言、人机交互、模拟到现实转移、多智能体交互、学习对象关系、学习可供性、场景合成、通过交互学习以及计算机视觉和可解释性等领域取得了显著成果。

### 💬 可借鉴之处
AI2-THOR是一个功能强大的交互式3D环境，可以用于各种视觉AI研究。它具有高度的可定制性，并提供了对多种场景、智能体体现、动作和元数据的支持。随着其功能的不断更新和发展，AI2-THOR有望在视觉AI领域发挥更大的作用。

## interactive-gibson-benchmark-(igibson-0-5)--a-benchmark-for-interactive-navigation-in-cluttered-environments
### Abstract
We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(https://sites.google.com/view/interactivegibsonenv) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.
### 🌟 论文解读 | 交互式Gibson基准（iGibson 0.5）：用于杂乱环境中的交互式导航的基准

### 📌 背景痛点/本文动机
传统的机器人导航主要关注在避免碰撞的情况下到达目标。然而，随着机器人越来越多地部署在杂乱无章的环境中，如家庭和办公室，考虑物理交互作为导航策略的一部分变得不可避免，甚至必要。例如，在杂乱的家庭中导航时，机器人可能需要推开物体或打开门才能到达目的地。本文提出了一个系统的方法来研究这种交互式导航问题。

### 🚀 核心方法
💡 创新点1：交互式Gibson环境（iGibson 0.5）
本文提出了一个新的实验设置，即交互式Gibson环境（iGibson 0.5），它模拟了室内场景的高保真视觉，以及机器人和这些场景中常见物体的高保真物理动力学。这使得机器人可以与场景中的物体进行交互，例如推开物体以清除通往目标位置的路径。

💡 创新点2：交互式导航指标
本文提出了一套交互式导航指标，用于研究导航和物理交互之间的相互作用。这些指标包括路径效率、交互努力和交互式导航分数（INS），它们可以衡量机器人导航策略的效率和效果。

### 📈 实验结果
本文在交互式Gibson基准上评估了多个基于学习的基线，并提供了关于导航行为在不同路径效率和周围物体干扰之间的权衡的见解。结果表明，通过积极调整奖励函数中的交互惩罚，可以控制交互式导航分数，并且不同的INS值对应于不同的导航行为。

### 💬 可借鉴之处
本文提出的交互式Gibson基准和交互式导航指标为研究交互式导航问题提供了一个新的平台和工具。研究人员可以利用这个基准来评估和比较他们的交互式导航解决方案，并探索不同导航策略的权衡。此外，本文提出的交互式Gibson环境也可以用于研究和训练其他视觉导航、操作和移动操作解决方案。

## habitat--a-platform-for-embodied-ai-research
### Abstract
We present Habitat, a platform for research in embodied artificial
intelligence (AI). Habitat enables training embodied agents (virtual robots) in
highly efficient photorealistic 3D simulation. Specifically, Habitat consists
of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with
configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is
fast -- when rendering a scene from Matterport3D, it achieves several thousand
frames per second (fps) running single-threaded, and can reach over 10,000 fps
multi-process on a single GPU. (ii) Habitat-API: a modular high-level library
for end-to-end development of embodied AI algorithms -- defining tasks (e.g.,
navigation, instruction following, question answering), configuring, training,
and benchmarking embodied agents.
  These large-scale engineering contributions enable us to answer scientific
questions requiring experiments that were till now impracticable or 'merely'
impractical. Specifically, in the context of point-goal navigation: (1) we
revisit the comparison between learning and SLAM approaches from two recent
works and find evidence for the opposite conclusion -- that learning
outperforms SLAM if scaled to an order of magnitude more experience than
previous investigations, and (2) we conduct the first cross-dataset
generalization experiments {train, test} x {Matterport3D, Gibson} for multiple
sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors
generalize across datasets. We hope that our open-source platform and these
findings will advance research in embodied AI.
### 🌟 论文解读 | Habitat：赋能具身AI研究的平台

### 📌 背景痛点/本文动机
随着深度学习在计算机视觉和自然语言处理领域的快速发展，具身AI（Embodied AI）也逐渐成为研究热点。具身AI旨在让智能体（如机器人）在真实环境中进行感知、决策和行动，从而实现更高级的智能。然而，在真实世界中训练具身AI面临着诸多挑战，例如训练速度慢、成本高、安全性低等。因此，本文提出了Habitat平台，旨在通过高效的3D模拟器，为具身AI研究提供更便捷、更高效的解决方案。

### 🚀 核心方法
Habitat平台主要由两部分组成：

1. **Habitat-Sim**：一个灵活、高性能的3D模拟器，支持可配置的智能体、传感器和通用3D数据集处理。Habitat-Sim具有以下特点：
    * **高性能渲染**：单线程运行时，渲染Matterport3D场景可达数千帧每秒（fps），多进程运行时，单GPU可达超过10,000 fps。
    * **灵活配置**：支持配置不同类型的智能体、传感器和3D数据集，例如Matterport3D、Gibson和Replica数据集。
    * **高效GPU吞吐量**：通过共享内存和CUDA-GL互操作，实现高效的GPU渲染和CPU内存访问。

2. **Habitat-API**：一个模块化、高级别的库，用于端到端开发具身AI算法。Habitat-API支持以下功能：
    * **任务定义**：定义各种具身AI任务，例如导航、指令跟随、问答等。
    * **智能体配置**：配置和训练不同类型的智能体，例如通过模仿学习、强化学习或经典SLAM方法。
    * **基准测试**：使用标准指标对智能体进行评估和比较。

### 📈 实验结果
本文在点目标导航任务上进行了实验，结果表明：

1. **学习优于SLAM**：当训练经验足够多时，基于学习的智能体可以超越经典SLAM方法。
2. **深度传感器更具泛化能力**：配备深度传感器的智能体在不同数据集之间具有更好的泛化能力。

### 💬 可借鉴之处
Habitat平台为具身AI研究提供了强大的工具和平台，具有以下可借鉴之处：

* **高效模拟器**：Habitat-Sim的高性能渲染能力可以显著提高训练速度，降低训练成本。
* **灵活配置**：Habitat平台支持灵活配置智能体、传感器和数据集，方便研究人员进行各种实验。
* **模块化设计**：Habitat-API的模块化设计方便研究人员进行代码复用和扩展。
* **基准测试**：Habitat平台提供了标准化的基准测试方法，方便研究人员进行结果比较和评估。

总而言之，Habitat平台为具身AI研究提供了强大的工具和平台，有望推动具身AI领域的快速发展。

## behavior--benchmark-for-everyday-household-activities-in-virtual--interactive--and-ecological-environments
### Abstract
We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
behavior.stanford.edu to facilitate and calibrate the development of new
embodied AI solutions.
### 🌟 论文解读 | BEHAVIOR：虚拟交互生态环境中日常家庭活动的基准

### 📌 背景痛点/本文动机
随着人工智能技术的发展，模拟环境中的基准测试对于评估和推动智能体在现实世界中的表现至关重要。然而，现有的基准测试往往缺乏现实性、多样性和复杂性，无法全面评估智能体在真实世界中的能力。为了解决这个问题，本文提出了BEHAVIOR，一个包含100个日常家庭活动的基准测试，旨在模拟真实世界中的挑战，并推动智能体在现实世界中的发展。

### 🚀 核心方法
💡 创新点1：基于谓词逻辑的描述语言
BEHAVIOR引入了一种基于谓词逻辑的描述语言，用于表达活动的初始和目标条件。这种语言允许生成多样化的活动实例，并能够接受任何有意义的解决方案。

💡 创新点2：模拟器无关的环境特征
BEHAVIOR确定了支持其活动的模拟器无关特征，并在iGibson 2.0中实现了这些特征。这使得BEHAVIOR可以在多种环境中实现，并提供了无限多样化的活动实例。

💡 创新点3：基于人类表现的评估指标
BEHAVIOR引入了一系列评估指标，用于衡量智能体在任务进度和效率方面的表现。这些指标包括成功分数、效率指标和基于人类表现的指标，以确保评估的公平性和可比性。

### 📈 实验结果
实验结果表明，即使是当前最先进的智能体，在面对BEHAVIOR的挑战时也难以取得良好的表现。这表明BEHAVIOR的基准测试具有很高的难度，能够有效地评估智能体在现实世界中的能力。

### 💬 可借鉴之处
BEHAVIOR的基准测试为评估和推动智能体在现实世界中的发展提供了重要的工具。其基于谓词逻辑的描述语言、模拟器无关的环境特征和基于人类表现的评估指标，为其他基准测试提供了可借鉴的经验。此外，BEHAVIOR的基准测试还可以用于开发新的智能体解决方案，并推动人工智能技术的发展。

## behavior-1k--a-human-centered--embodied-ai-benchmark-with-1-000-everyday-activities-and-realistic-simulation
### Abstract
We present BEHAVIOR-1K, a comprehensive simulation benchmark for
human-centered robotics. BEHAVIOR-1K includes two components, guided and
motivated by the results of an extensive survey on "what do you want robots to
do for you?". The first is the definition of 1,000 everyday activities,
grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more
than 9,000 objects annotated with rich physical and semantic properties. The
second is OMNIGIBSON, a novel simulation environment that supports these
activities via realistic physics simulation and rendering of rigid bodies,
deformable bodies, and liquids. Our experiments indicate that the activities in
BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both
of which remain a challenge for even state-of-the-art robot learning solutions.
To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an
initial study on transferring solutions learned with a mobile manipulator in a
simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's
human-grounded nature, diversity, and realism make it valuable for embodied AI
and robot learning research. Project website: https://behavior.stanford.edu.
### 🌟 论文解读 | BEHAVIOR-1K：基于人类日常活动与真实模拟的具身AI基准

### 📌 背景痛点/本文动机
随着人工智能和机器人技术的快速发展，人们对于机器人能够执行更多日常任务的需求日益增长。然而，现有的机器人基准测试往往由研究人员设计，缺乏对人类实际需求的考虑。为了解决这个问题，本文提出了BEHAVIOR-1K，一个基于人类日常活动与真实模拟的具身AI基准。

### 🚀 核心方法
💡 创新点1：基于人类需求的基准测试
BEHAVIOR-1K通过一项广泛的调查，收集了1461名参与者的意见，以确定人们希望机器人执行的最常见和最需要的1000个日常活动。这些活动涵盖了从清洁、烹饪到娱乐等多个领域，确保了基准测试的多样性和实用性。

💡 创新点2：真实模拟环境OMNIGIBSON
为了支持这些日常活动，本文开发了一个名为OMNIGIBSON的模拟环境。OMNIGIBSON基于Nvidia的Omniverse和PhysX 5，能够提供逼真的物理模拟和渲染，包括刚性体、可变形体和液体的模拟。此外，OMNIGIBSON还支持扩展的对象状态，如温度、湿度等，以及生成有效的初始活动配置和区分有效目标解决方案的功能。

### 📈 实验结果
本文评估了当前最先进的强化学习算法在BEHAVIOR-1K中的表现。结果表明，即使是单个活动也对当前AI算法构成了极大的挑战，并且需要大量的领域知识才能解决。此外，本文还进行了一项初步研究，将模拟环境中学习的解决方案转移到现实世界的机器人上，以评估模拟与现实之间的差距。

### 💬 可借鉴之处
BEHAVIOR-1K为具身AI和机器人学习研究提供了一个有价值的基准测试。其基于人类需求的多样性和真实模拟环境，为研究人员提供了一个平台，以开发能够执行更多日常任务的机器人。此外，本文的研究结果也为解决模拟与现实之间的差距提供了有价值的见解。

## automatic-goal-generation-for-reinforcement-learning-agents
### Abstract
Reinforcement learning is a powerful technique to train an agent to perform a
task. However, an agent that is trained using reinforcement learning is only
capable of achieving the single task that is specified via its reward function.
Such an approach does not scale well to settings in which an agent needs to
perform a diverse set of tasks, such as navigating to varying positions in a
room or moving objects to varying locations. Instead, we propose a method that
allows an agent to automatically discover the range of tasks that it is capable
of performing. We use a generator network to propose tasks for the agent to try
to achieve, specified as goal states. The generator network is optimized using
adversarial training to produce tasks that are always at the appropriate level
of difficulty for the agent. Our method thus automatically produces a
curriculum of tasks for the agent to learn. We show that, by using this
framework, an agent can efficiently and automatically learn to perform a wide
set of tasks without requiring any prior knowledge of its environment. Our
method can also learn to achieve tasks with sparse rewards, which traditionally
pose significant challenges.
### 🌟 论文解读 | 自动目标生成：让强化学习更高效

### 📌 背景痛点/本文动机
强化学习（RL）是一种强大的训练智能体执行特定任务的技术。然而，传统的强化学习方法通常只能让智能体学会执行单一任务，这在需要智能体执行多样化任务的场景中显得力不从心。例如，在机器人导航或物体搬运等任务中，智能体需要能够到达不同的位置或移动物体到不同的位置。为了解决这个问题，本文提出了一种自动目标生成方法，让智能体能够自动发现并学习执行其环境中的各种任务。

### 🚀 核心方法
💡 创新点1：目标生成网络
本文使用一个生成器网络来为智能体提出任务，这些任务被指定为目标状态。生成器网络通过对抗训练进行优化，以确保生成的任务始终处于智能体能够处理的难度水平。

💡 创新点2：自动课程生成
本文的方法自动生成一个课程，其中在每个步骤中，生成器都会生成比智能体已经能够实现的任务稍微更难的任务。这样，智能体可以高效地学习执行一系列任务，而无需对其环境或执行的任务有任何先验知识。

### 📈 实验结果
本文在多个机器人运动任务中进行了实验，结果表明，与传统的强化学习方法相比，本文的方法能够更快地学习执行各种任务，并且能够有效地处理稀疏奖励函数。

### 💬 可借鉴之处
本文提出的自动目标生成方法为强化学习提供了一种新的思路，可以帮助智能体更高效地学习执行多样化任务。该方法可以应用于各种场景，例如机器人导航、物体搬运、游戏等。此外，本文的方法还可以与其他多目标强化学习方法相结合，进一步提高学习效率。

## toolkengpt--augmenting-frozen-language-models-with-massive-tools-via-tool-embeddings
### Abstract
Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
### 🌟 论文解读 | ToolkenGPT：通过工具嵌入增强冻结语言模型

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在各个领域的广泛应用，如何让这些模型更好地与外部工具交互，以解决更复杂的问题，成为了研究的热点。传统的通过微调LLMs来学习使用工具的方法，虽然有效，但成本高昂且灵活性差，只能针对预定义的工具集进行学习。而基于上下文学习的方法虽然可以灵活地处理新工具，但受限于上下文长度，难以展示大量工具的演示，导致对工具的理解不够深入。此外，当可选择的工具数量众多时，上下文学习可能完全失效。

### 🚀 核心方法
本文提出了ToolkenGPT，一种新的方法，它结合了微调和上下文学习的优点，同时避免了它们的局限性。ToolkenGPT的核心思想是将每个工具表示为一个特殊的token（称为“toolken”），并为每个toolken学习一个嵌入向量。这样，LLMs就可以像生成普通单词token一样调用工具。一旦预测到toolken，LLM就会切换到“工具模式”，生成工具的参数，然后执行工具并将结果返回到文本中继续推理。

### 📈 实验结果
在数值推理、基于知识的问答和具身计划生成等不同领域，ToolkenGPT有效地增强了LLMs的能力，并显著优于各种最新的基线方法。实验结果表明，ToolkenGPT能够从大量的演示数据中学习toolken嵌入，从而更好地理解和使用工具。

### 💬 可借鉴之处
ToolkenGPT提供了一种高效且灵活的方法，让LLMs能够学习和使用大量的外部工具。这种方法可以应用于各种需要工具辅助的复杂问题，例如数学计算、知识问答、机器人控制等。此外，ToolkenGPT的设计也启发了我们如何将LLMs与其他类型的模型和工具进行集成，以实现更强大的功能。

## metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents
### Abstract
Significant advancements have occurred in the application of Large Language
Models (LLMs) for various tasks and social simulations. Despite this, their
capacities to coordinate within task-oriented social contexts are
under-explored. Such capabilities are crucial if LLMs are to effectively mimic
human-like social behavior and produce meaningful results. To bridge this gap,
we introduce collaborative generative agents, endowing LLM-based Agents with
consistent behavior patterns and task-solving abilities. We situate these
agents in a simulated job fair environment as a case study to scrutinize their
coordination skills. We propose a novel framework that equips collaborative
generative agents with human-like reasoning abilities and specialized skills.
Our evaluation demonstrates that these agents show promising performance.
However, we also uncover limitations that hinder their effectiveness in more
complex coordination tasks. Our work provides valuable insights into the role
and evolution of LLMs in task-oriented social simulations.
### 🌟 论文解读 | MetaAgents：基于LLM的任务导向协调的协作生成式智能体

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们在模拟人类行为和执行任务方面的能力日益增强。然而，LLMs在任务导向的社会环境中的协调能力尚未得到充分探索。为了使LLMs能够有效地模拟人类的社会行为并产生有意义的结果，这种能力至关重要。

### 🚀 核心方法
本文提出了协作生成式智能体（Collaborative Generative Agents），为基于LLM的智能体赋予了一致的行为模式和任务解决能力。为了研究这些智能体的协调能力，本文构建了一个模拟的招聘会环境，并提出了一个包含感知、记忆、推理和执行模块的框架。该框架使协作生成式智能体具备类似人类的推理能力和专业技能。

### 📈 实验结果
在模拟的招聘会环境中，协作生成式智能体在识别合格求职者、设计工作流程和分配角色方面表现出良好的性能。然而，随着招聘会复杂性的增加，智能体在协调方面遇到了挑战，这主要归因于LLMs的目标或意图不匹配。

### 💬 可借鉴之处
本文提出的协作生成式智能体框架为LLMs在任务导向的社会模拟中的应用提供了有价值的见解。该框架可以应用于各种场景，例如招聘、团队协作和社交网络模拟。此外，本文还揭示了LLMs在协调任务中面临的挑战，为未来的研究提供了方向。

## face-recognition-methods-&-applications
### Abstract
Face recognition presents a challenging problem in the field of image
analysis and computer vision. The security of information is becoming very
significant and difficult. Security cameras are presently common in airports,
Offices, University, ATM, Bank and in any locations with a security system.
Face recognition is a biometric system used to identify or verify a person from
a digital image. Face Recognition system is used in security. Face recognition
system should be able to automatically detect a face in an image. This involves
extracts its features and then recognize it, regardless of lighting,
expression, illumination, ageing, transformations (translate, rotate and scale
image) and pose, which is a difficult task. This paper contains three sections.
The first section describes the common methods like holistic matching method,
feature extraction method and hybrid methods. The second section describes
applications with examples and finally third section describes the future
research directions of face recognition.
### 🌟 论文解读 | 《人脸识别方法与应用》：深入探索人脸识别技术的奥秘

### 📌 背景痛点/本文动机
随着信息安全变得越来越重要和复杂，人脸识别作为一种生物识别系统，在图像分析和计算机视觉领域面临着巨大的挑战。本文旨在探讨人脸识别的常见方法、应用实例以及未来的研究方向，以期为相关领域的研究和实践提供参考。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全面梳理人脸识别方法
本文详细介绍了三种主流的人脸识别方法：整体匹配方法、特征提取方法和混合方法。整体匹配方法如Eigenfaces，通过主成分分析（PCA）提取面部特征；特征提取方法关注局部特征，如眼睛、鼻子和嘴巴；混合方法结合了整体和局部特征，通常使用3D图像进行识别。

💡 创新点2：丰富的人脸识别应用案例
文章不仅介绍了人脸识别的基本方法，还提供了多个实际应用案例，如选民注册系统中的重复身份识别、计算机登录监控、机场安全系统、图像数据库调查等，展示了人脸识别技术在各个领域的广泛应用。

### 📈 实验结果
本文没有详细描述具体的实验结果，但通过文献综述和案例分析，展示了人脸识别技术在多种场景下的有效性和实用性。

### 💬 可借鉴之处
1. **方法多样性**：本文提供了多种人脸识别方法，研究者可以根据具体应用场景选择合适的方法。
2. **实际应用案例**：通过实际案例，展示了人脸识别技术在现实世界中的具体应用，为其他研究者提供了实践参考。
3. **未来研究方向**：文章指出了人脸识别技术的未来发展方向，如2D和3D人脸识别、大规模应用等，为后续研究提供了方向指引。

总之，本文对人脸识别技术进行了全面的梳理和探讨，对于人脸识别领域的研究者和工程师具有很高的参考价值。

## playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents
### Abstract
Large Language Models (LLMs) have shown great success as high-level planners
for zero-shot game-playing agents. However, these agents are primarily
evaluated on Minecraft, where long-term planning is relatively straightforward.
In contrast, agents tested in dynamic robot environments face limitations due
to simplistic environments with only a few objects and interactions. To fill
this gap in the literature, we present NetPlay, the first LLM-powered zero-shot
agent for the challenging roguelike NetHack. NetHack is a particularly
challenging environment due to its diverse set of items and monsters, complex
interactions, and many ways to die.
  NetPlay uses an architecture designed for dynamic robot environments,
modified for NetHack. Like previous approaches, it prompts the LLM to choose
from predefined skills and tracks past interactions to enhance decision-making.
Given NetHack's unpredictable nature, NetPlay detects important game events to
interrupt running skills, enabling it to react to unforeseen circumstances.
While NetPlay demonstrates considerable flexibility and proficiency in
interacting with NetHack's mechanics, it struggles with ambiguous task
descriptions and a lack of explicit feedback. Our findings demonstrate that
NetPlay performs best with detailed context information, indicating the
necessity for dynamic methods in supplying context information for complex
games such as NetHack.
### 🌟 论文解读 | LLMs 在 NetHack 中的潜力与局限性：零样本智能体

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在游戏领域展现出强大的规划能力，尤其是在 Minecraft 等游戏中。然而，这些模型在处理复杂、动态的环境时，如机器人环境，往往面临局限性。为了填补这一空白，本文提出了 NetPlay，一个基于 LLM 的零样本智能体，用于挑战性的 Rogue-like 游戏 NetHack。

### 🚀 核心方法
NetPlay 采用了一种专为动态机器人环境设计的架构，并针对 NetHack 进行了修改。它通过提示 LLM 从预定义的技能中选择，并跟踪过去的交互来增强决策。NetPlay 还能够检测重要的游戏事件，以便在出现意外情况时中断正在执行的技能。

### 📈 实验结果
实验结果表明，NetPlay 在与 NetHack 的机制交互方面表现出色，但在处理模糊的任务描述和缺乏明确反馈时存在困难。NetPlay 在提供详细上下文信息的情况下表现最佳，这表明在 NetHack 等复杂游戏中，动态提供上下文信息的方法至关重要。

### 💬 可借鉴之处
NetPlay 的研究结果表明，LLMs 在游戏领域具有巨大的潜力，但仍然存在局限性。未来研究可以探索如何更好地利用 LLMs 的能力，例如通过动态提供上下文信息或使用机器学习来替代手工制作的组件。此外，NetPlay 的架构可以为其他复杂游戏的设计提供参考。

## enabling-multimodal-generation-on-clip-via-vision-language-knowledge-distillation
### Abstract
The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.
### 🌟 论文解读 | 通过视觉-语言知识蒸馏在CLIP上实现多模态生成

### 📌 背景痛点/本文动机
近年来，大规模的视觉-语言预训练（VLP）模型在双流架构（如CLIP）上取得了显著成果，尤其在各种多模态对齐任务中表现出色。然而，这些模型在多模态生成任务（如图像字幕和开放式视觉问答）上的表现却相对较弱，主要归因于文本编码器的不足。

### 🚀 核心方法
为了解决这一问题，本文提出了视觉-语言知识蒸馏（VLKD）方法，通过将CLIP的文本编码器替换为预训练的语言模型（PLM，如BART），从而增强模型的多模态生成能力。VLKD方法主要包括以下三个目标：

💡 创新点1：文本-文本距离最小化（TTDM）
通过最小化CLIP文本编码器和BART编码器在相同输入文本下的输出表示之间的ℓ2距离，使两者在文本表示上保持一致。

💡 创新点2：图像-文本对比学习（ITCL）
通过优化BART编码器和CLIP图像编码器输出表示之间的对称InfoNCE损失，使BART编码器更好地适应CLIP的多模态空间。

💡 创新点3：图像条件文本填充（ICTI）
通过在BART解码器上进行图像条件下的文本填充任务，使BART解码器能够理解多模态信息，并生成与图像相关的文本。

### 📈 实验结果
实验结果表明，VLKD模型在多模态生成任务上表现出色，例如在VQAv2数据集上实现了44.5%的零样本准确率，超过了之前最先进的零样本模型，且参数量减少了7倍。此外，VLKD模型在NLP任务上的表现也优于其他VLP模型，证明了该方法的有效性。

### 💬 可借鉴之处
VLKD方法为多模态生成任务提供了一种高效且有效的解决方案，通过知识蒸馏将CLIP和PLM的优势相结合，实现了在零样本和微调设置下的多模态生成能力。此外，该方法还保持了PLM在NLP任务上的原始能力，使其在多模态和单模态任务中都具有广泛的应用前景。

## code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering
### Abstract
Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium
### 🌟 论文解读 | AlphaCodium：从提示工程到流程工程，提升代码生成性能

### 📌 背景痛点/本文动机
代码生成问题与常见的自然语言问题不同，它需要匹配目标语言的精确语法，识别正常路径和边缘情况，关注问题规范中的许多小细节，并解决其他代码特定的问题和要求。因此，许多在自然语言生成中成功的优化和技巧可能对代码任务无效。本文提出了一种新的代码生成方法，称为AlphaCodium，它是一种基于测试的多阶段、代码导向的迭代流程，旨在提高大型语言模型（LLMs）在代码问题上的性能。

### 🚀 核心方法
💡 创新点1：测试导向的迭代流程
AlphaCodium的核心是迭代流程，其中生成的代码会反复运行并针对输入输出测试进行修复。这种方法允许模型逐步改进其解决方案，直到找到正确的答案。

💡 创新点2：多阶段处理
AlphaCodium流程分为两个主要阶段：预处理阶段和代码迭代阶段。在预处理阶段，模型会对问题进行自然语言推理，例如生成问题反思和公共测试推理。在代码迭代阶段，模型会生成代码解决方案，并在公共和AI生成的测试上进行迭代和修复。

💡 创新点3：代码导向的设计概念
AlphaCodium还采用了多种代码导向的设计概念，例如YAML结构化输出、通过项目符号分析进行语义推理、生成模块化代码、软决策和双重验证、鼓励探索以及测试锚点。这些概念有助于提高代码生成的质量和效率。

### 📈 实验结果
在CodeContests数据集上进行的实验表明，AlphaCodium流程显著提高了LLMs在代码问题上的性能。例如，GPT-4在验证集上的准确率（pass@5）从使用单个精心设计的直接提示的19%提高到使用AlphaCodium流程的44%。

### 💬 可借鉴之处
AlphaCodium的许多原则和最佳实践可以广泛应用于一般的代码生成任务。例如，使用结构化输出、生成模块化代码、通过项目符号分析进行语义推理、软决策和双重验证、鼓励探索以及测试锚点等技术都可以帮助提高代码生成的质量和效率。

### 📚 总结
AlphaCodium是一种创新的代码生成方法，它通过测试导向的迭代流程和多阶段处理，显著提高了LLMs在代码问题上的性能。该方法还采用了多种代码导向的设计概念，进一步提高了代码生成的质量和效率。AlphaCodium的许多原则和最佳实践可以广泛应用于一般的代码生成任务，为代码生成领域的研究和应用提供了新的思路和方向。

## world-models-with-hints-of-large-language-models-for-goal-achieving
### Abstract
Reinforcement learning struggles in the face of long-horizon tasks and sparse
goals due to the difficulty in manual reward specification. While existing
methods address this by adding intrinsic rewards, they may fail to provide
meaningful guidance in long-horizon decision-making tasks with large state and
action spaces, lacking purposeful exploration. Inspired by human cognition, we
propose a new multi-modal model-based RL approach named Dreaming with Large
Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the
LLMs into the model rollouts to encourage goal discovery and reaching in
challenging tasks. By assigning higher intrinsic rewards to samples that align
with the hints outlined by the language model during model rollouts, DLLM
guides the agent toward meaningful and efficient exploration. Extensive
experiments demonstrate that the DLLM outperforms recent methods in various
challenging, sparse-reward environments such as HomeGrid, Crafter, and
Minecraft by 27.7\%, 21.1\%, and 9.9\%, respectively.
### 🌟 论文解读 | 利用大型语言模型提示的强化学习世界模型

### 📌 背景痛点/本文动机
强化学习（RL）在处理长期任务和稀疏目标时面临挑战，因为手动指定奖励函数非常困难。现有的方法通过添加内在奖励来解决这一问题，但在具有大型状态和动作空间的长期决策任务中，它们可能无法提供有意义的指导，缺乏有目的的探索。

### 🚀 核心方法
💡 创新点1：Dreaming with Large Language Models (DLLM)
DLLM 是一种新的多模态基于模型强化学习（MBRL）方法，它利用人类自然语言来描述环境动态，并将大型语言模型（LLM）的指导整合到模型滚动中，以提高代理的探索和目标完成能力。

💡 创新点2：基于 LLM 生成的目标，DLLM 可以通过自动递减机制生成有意义的内在奖励，以指导策略学习。

### 📈 实验结果
DLLM 在各种稀疏奖励环境中优于最近的方法，包括 HomeGrid、Crafter 和 Minecraft，分别提高了 27.7%、21.1% 和 9.9%。

### 💬 可借鉴之处
DLLM 的方法可以应用于各种复杂环境，并利用语言信息来提高代理的探索和目标完成能力。此外，DLLM 的自动递减机制可以有效地避免代理重复完成简单任务，从而促进代理探索更复杂的行为。

## enhancing-agent-learning-through-world-dynamics-modeling
### Abstract
Large language models (LLMs) have been increasingly applied to tasks in
language understanding and interactive decision-making, with their impressive
performance largely attributed to the extensive domain knowledge embedded
within them. However, the depth and breadth of this knowledge can vary across
domains. Many existing approaches assume that LLMs possess a comprehensive
understanding of their environment, often overlooking potential gaps in their
grasp of actual world dynamics. To address this, we introduce Discover, Verify,
and Evolve (DiVE), a framework that discovers world dynamics from a small
number of demonstrations, verifies the accuracy of these dynamics, and evolves
new, advanced dynamics tailored to the current situation. Through extensive
evaluations, we assess the impact of each component on performance and compare
the dynamics generated by DiVE to human-annotated dynamics. Our results show
that LLMs guided by DiVE make more informed decisions, achieving rewards
comparable to human players in the Crafter environment and surpassing methods
that require prior task-specific training in the MiniHack environment.
### 🌟 论文解读 | 通过世界动态建模增强智能体学习

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在语言理解和交互式决策任务中表现出色，这主要归功于它们嵌入的广泛领域知识。然而，这种知识的深度和广度在不同领域之间可能存在差异。许多现有方法假设LLMs对其环境有全面的理解，但往往忽视了它们对实际世界动态的掌握可能存在差距。为了解决这个问题，本文提出了Discover, Verify, and Evolve (DiVE)框架，该框架从少量演示中发现世界动态，验证这些动态的准确性，并根据当前情况演化新的、先进的动态。

### 🚀 核心方法
💡 创新点1：DiVE框架
DiVE框架由三个主要组件组成：
- Discoverer：使用课程学习方法从演示中迭代地发现环境动态。
- Verifier：消除LLMs由于幻觉倾向而导致的不可靠动态。
- Evolver：根据学习的动态，推理出针对当前情况的深入、特定于状态的知识。

💡 创新点2：层次课程学习
DiVE采用层次课程学习方法，从简单到复杂的动态逐步学习，从而更有效地学习。具体来说，它从任务分解层次结构中的元素（如动作、对象、子任务和子目标）开始，逐步学习它们的动态。

💡 创新点3：动态验证
为了确保动态的准确性，DiVE引入了动态验证器，它可以过滤掉可能无效和冲突的动态候选者，从而提高决策过程的可靠性。

💡 创新点4：在线策略学习
DiVE不仅学习基本规则，还专注于根据这些动态开发高级游戏策略。它通过在线学习方法将动态演化为策略，从而生成更符合当前游戏场景的策略。

### 📈 实验结果
在Crafter和MiniHack环境中进行的实验表明，DiVE在性能方面优于所有其他基线模型。在Crafter环境中，DiVE在分数和奖励方面分别比SOTA LLM方法SPRING提高了337.8%和110.1%，并且超过了SOTA RL方法DreamerV3。在MiniHack环境中，DiVE在Lava Crossing任务上与SSO和Reflexion（都需要30次迭代训练）的性能相当，并且在Wand of Death和Quest任务上超过了这两个基线。

### 💬 可借鉴之处
DiVE框架为解决LLMs在特定领域中的知识差距问题提供了一种有效的方法。它通过发现、验证和演化世界动态，提高了LLMs的决策能力。此外，DiVE的层次课程学习和动态验证方法可以应用于其他需要长期规划和决策的任务中。

## watch-your-step--learning-node-embeddings-via-graph-attention
### Abstract
Graph embedding methods represent nodes in a continuous vector space,
preserving information from the graph (e.g. by sampling random walks). There
are many hyper-parameters to these methods (such as random walk length) which
have to be manually tuned for every graph. In this paper, we replace random
walk hyper-parameters with trainable parameters that we automatically learn via
backpropagation. In particular, we learn a novel attention model on the power
series of the transition matrix, which guides the random walk to optimize an
upstream objective. Unlike previous approaches to attention models, the method
that we propose utilizes attention parameters exclusively on the data (e.g. on
the random walk), and not used by the model for inference. We experiment on
link prediction tasks, as we aim to produce embeddings that best-preserve the
graph structure, generalizing to unseen information. We improve
state-of-the-art on a comprehensive suite of real world datasets including
social, collaboration, and biological networks. Adding attention to random
walks can reduce the error by 20% to 45% on datasets we attempted. Further, our
learned attention parameters are different for every graph, and our
automatically-found values agree with the optimal choice of hyper-parameter if
we manually tune existing methods.
### 🌟 论文解读 | 图注意力学习节点嵌入：Watch Your Step

### 📌 背景痛点/本文动机
图嵌入方法旨在将图中的节点表示为连续的向量空间，从而保留图中的信息。然而，这些方法通常需要手动调整许多超参数，例如随机游走的长度，这限制了它们的效率和实用性。

### 🚀 核心方法
💡 创新点1：本文提出了一种新的图注意力模型，通过自动学习可训练参数来替代随机游走的超参数。这种方法利用注意力参数来指导随机游走，使其优化上游目标，从而更好地保留图结构。

💡 创新点2：与之前的注意力模型不同，本文提出的方法仅在数据遍历（例如随机游走）上使用注意力参数，而不是在模型推理中使用。这使得模型能够自动学习每个图的最佳注意力参数，从而提高嵌入的质量。

### 📈 实验结果
本文在链接预测任务上进行了实验，结果表明，与现有的方法相比，本文提出的图注意力模型能够显著提高嵌入的质量，将错误率降低了20%至40%。此外，本文还发现，自动学习的注意力参数与手动调整现有方法得到的最佳超参数一致。

### 💬 可借鉴之处
本文提出的图注意力模型为图嵌入方法提供了一种新的思路，通过自动学习可训练参数来替代随机游走的超参数，从而提高了嵌入的质量和效率。此外，本文还展示了如何将注意力机制应用于图嵌入，为未来的研究提供了新的方向。

## from-centralized-to-self-supervised--pursuing-realistic-multi-agent-reinforcement-learning
### Abstract
In real-world environments, autonomous agents rely on their egocentric
observations. They must learn adaptive strategies to interact with others who
possess mixed motivations, discernible only through visible cues. Several
Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches
that involve either centralized training or reward-sharing, often violating the
realistic ways in which living organisms, like animals or humans, process
information and interact. MARL strategies deploying decentralized training with
intrinsic motivation offer a self-supervised approach, enable agents to develop
flexible social strategies through the interaction of autonomous agents.
However, by contrasting the self-supervised and centralized methods, we reveal
that populations trained with reward-sharing methods surpass those using
self-supervised methods in a mixed-motive environment. We link this superiority
to specialized role emergence and an agent's expertise in its role.
Interestingly, this gap shrinks in pure-motive settings, emphasizing the need
for evaluations in more complex, realistic environments (mixed-motive). Our
preliminary results suggest a gap in population performance that can be closed
by improving self-supervised methods and thereby pushing MARL closer to
real-world readiness.
### 🌟 论文解读 | 从集中式到自监督：追求现实的多智能体强化学习

### 📌 背景痛点/本文动机
在现实世界中，自主智能体需要依赖自身的观察来学习适应性的策略，以与其他具有混合动机的智能体进行交互。然而，现有的多智能体强化学习（MARL）方法大多采用集中式方法，如集中训练或奖励共享，这往往违背了现实世界中生物体（如动物或人类）处理信息和交互的方式。本文旨在探索自监督方法在MARL中的应用，以使智能体能够通过自主交互发展灵活的社会策略。

### 🚀 核心方法
💡 创新点1：本文对比了自监督和集中式方法在MARL中的应用。自监督方法通过内在动机驱动智能体进行探索和学习，使其能够从有限的观察中推断其他智能体的状态、目标和奖励。

💡 创新点2：本文在两个具有不同社会动态的环境（混合动机和纯动机）中评估了多种MARL模型，包括独立PPO、MAPPO、ICM、ICM-reward、社会影响和SVO模型。通过对比这些模型在人口绩效和公平性方面的表现，揭示了自监督方法在现实世界中的潜力和局限性。

### 📈 实验结果
在混合动机环境（Clean Up）中，集中式方法（SVO-HE、SVO-HO、MAPPO）优于所有自监督方法。SVO-HE模型表现最佳，而IPPO模型表现最差。具有内在动机的模型（ICM、ICM-reward、社会影响）相对于IPPO模型有所改进，但并未超过集中式方法。在纯动机环境（Harvest）中，SVO-HO模型表现最佳，但与其他模型相比没有显著差异。MAPPO模型表现最差。

### 💬 可借鉴之处
本文的研究结果表明，自监督方法在MARL中具有巨大的潜力，但仍需进一步改进。未来研究可以探索更有效的内在动机方法，以缩小自监督和集中式方法之间的差距。此外，还需要开发能够灵活适应不同环境动态的模型，以实现更接近现实世界的MARL。

## agent-planning-with-world-knowledge-model
### Abstract
Recent endeavors towards directly using large language models (LLMs) as agent
models to execute interactive planning tasks have shown commendable results.
Despite their achievements, however, they still struggle with brainless
trial-and-error in global planning and generating hallucinatory actions in
local planning due to their poor understanding of the ``real'' physical world.
Imitating humans' mental world knowledge model which provides global prior
knowledge before the task and maintains local dynamic knowledge during the
task, in this paper, we introduce parametric World Knowledge Model (WKM) to
facilitate agent planning. Concretely, we steer the agent model to
self-synthesize knowledge from both expert and sampled trajectories. Then we
develop WKM, providing prior task knowledge to guide the global planning and
dynamic state knowledge to assist the local planning. Experimental results on
three complex real-world simulated datasets with three state-of-the-art
open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our
method can achieve superior performance compared to various strong baselines.
Besides, we analyze to illustrate that our WKM can effectively alleviate the
blind trial-and-error and hallucinatory action issues, providing strong support
for the agent's understanding of the world. Other interesting findings include:
1) our instance-level task knowledge can generalize better to unseen tasks, 2)
weak WKM can guide strong agent model planning, and 3) unified WKM training has
promising potential for further development. The code is available at
https://github.com/zjunlp/WKM.
### 🌟 论文解读 | 基于世界知识模型的智能体规划

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在自然语言处理任务中取得了显著进展。然而，当直接使用LLMs作为智能体模型执行交互式规划任务时，它们仍然面临着一些挑战。由于LLMs缺乏对真实物理世界的理解，它们在全局规划中容易出现无目的的试错，并在局部规划中生成幻觉行为。为了解决这个问题，本文提出了一个参数化的世界知识模型（WKM），以辅助智能体规划。

### 🚀 核心方法
💡 创新点1：任务知识合成
本文通过比较专家轨迹和采样轨迹，引导智能体模型自我合成任务知识。任务知识作为先验知识，用于指导智能体模型的全局规划，避免无目的的试错。

💡 创新点2：状态知识总结
本文通过提示智能体模型，根据历史行为自我总结状态知识，并构建状态知识库。状态知识作为动态知识，用于约束智能体模型的局部规划，避免生成幻觉行为。

💡 创新点3：世界知识模型训练
本文将生成的世界知识集成到专家轨迹中，并训练世界知识模型。智能体模型需要重新训练以适应任务知识。

💡 创新点4：基于世界知识模型的智能体规划
在规划阶段，本文使用世界知识模型为智能体模型提供全局先验任务知识和维护局部动态状态知识。任务知识将作为自然语言形式与特定任务一起连接，以指导智能体模型的试错。在每个规划步骤中，为了防止幻觉行为的出现，本文使用生成的状态知识作为查询，从预先构建的状态知识库中进行kNN检索。然后，使用来自先前动作的约束、检索到的下一个动作的概率以及来自智能体模型概率的加权预测来预测下一个动作。

### 📈 实验结果
本文在三个复杂的真实世界模拟数据集上进行了实验，并与三个最先进的开源LLMs进行了比较。实验结果表明，本文的方法在性能上优于各种强基线。此外，进一步的分析结果表明，本文的WKM可以有效减少无目的的试错和幻觉行为，生成的实例级任务知识可以更好地泛化到未见过的任务，弱WKM可以指导强智能体模型规划，统一WKM训练具有很大的发展潜力。

### 💬 可借鉴之处
本文提出的基于世界知识模型的智能体规划方法，为解决LLMs在交互式规划任务中的挑战提供了一种新的思路。该方法可以有效地提高智能体模型的理解能力和规划能力，并具有很好的泛化能力。此外，本文提出的弱WKM指导强智能体模型规划的思想，也为智能体学习提供了一种新的范式。

## thread--thinking-deeper-with-recursive-spawning
### Abstract
Large language models (LLMs) have shown impressive capabilities across
diverse settings, but still struggle as the length and complexity of the
context increases. To address this challenge, we propose Thinking Recursively
and Dynamically (ThReaD). THREAD frames model generation as a thread of
execution that, based on the context, can run to completion or dynamically
spawn new threads. By spawning, threads can offload work (e.g., thinking,
retrieving information) to child threads, which only return tokens needed for
the parent thread to do its work. In effect, this enables the model to adapt,
as needed, the amount of intermediate work used to produce tokens. We apply
THREAD in the settings of LLM task solving and question answering, where the
dynamic threading allows the model to recursively decompose the given task or
question into progressively simpler sub-problems that can be solved by separate
child threads. We test THREAD, implemented using a few-shot learning approach,
on diverse benchmarks for agent tasks and data-grounded question answering.
THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these
benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new
benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD
outperforms existing frameworks by 10% to 50% absolute points with smaller
models, including Llama-3-8b and CodeLlama-7b.
### 🌟 论文解读 | THREAD：递归分叉，深度思考

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在处理复杂任务时，随着上下文长度和复杂性的增加，其性能会下降。这是因为LLMs需要将所有必要的思考和信息检索工作都压缩在一条简洁的生成语句中，这限制了它们在需要更多工作（如思考、检索信息、分析等）的场景中的有效性。

### 🚀 核心方法
本文提出了一个名为“递归分叉和动态思考”（ThReaD）的通用框架，该框架将模型生成视为一个执行线程，该线程可以根据上下文独立运行到完成或动态地分叉成新的线程。通过分叉，线程可以将工作（例如，思考、检索信息）卸载到子线程，而子线程只返回父线程完成其工作所需的信息。这使模型能够根据需要动态地调整用于生成不同部分标记序列的中间工作量。

### 📈 实验结果
在LLM任务解决和数据驱动问答的设置中，THREAD通过动态分叉允许模型递归地将给定的任务或问题分解为逐步简化的子问题，这些子问题可以由单独的子线程解决。在多个基准测试中，THREAD实现了最先进的性能，包括ALFWorld、TextCraft和WebShop，以及两个新的基准测试DataCommons QA和MIMIC-III ICU QA。此外，THREAD在小模型上优于现有框架，包括Llama-3-8b和CodeLlama-7b，性能提高了10%到50%。

### 💬 可借鉴之处
THREAD框架提供了一种灵活的方法，使LLMs能够动态地适应其工作量和中间计算步骤，从而更好地处理复杂任务。此外，THREAD框架的通用性使其适用于各种场景，包括多模态应用，并且可以用于各种目的，例如编写程序、执行计算、增强数据、生成想法、检索信息、与环境交互、机器人操作等。

## envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents
### Abstract
Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller RL agents learn useful skills that they are weak at? We propose EnvGen,
a novel framework to address this question. We first prompt an LLM to generate
training environments by giving it the task description and simulator
objectives that the agents should learn and then asking it to generate a set of
environment configurations (e.g., different terrains, items initially given to
agents, etc.). Next, we train a small RL agent in a mixture of the original and
LLM-generated environments. Then, we enable the LLM to continuously adapt the
generated environments to progressively improve the skills that the agent is
weak at, by providing feedback to the LLM in the form of the agent's
performance. We demonstrate the usefulness of EnvGen with comprehensive
experiments in Crafter and Heist environments. We find that a small RL agent
trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and
learns long-horizon tasks significantly faster. We also show that using an LLM
to adapt environments dynamically outperforms curriculum learning approaches
and how the environments are adapted to help improve RL agents' weaker skills
over time. Additionally, EnvGen is substantially more efficient as it only uses
a small number of LLM calls (e.g., 4 in total), whereas LLM agents require
thousands of calls. Lastly, we present detailed ablation studies for EnvGen
design choices.
### 🌟 论文解读 | EnvGen：利用LLM生成和适应环境，训练具身智能体

### 📌 背景痛点/本文动机
随着具身智能体在开放世界游戏中的兴起，如何让智能体快速学习并掌握各种技能成为了一个挑战。传统的强化学习（RL）方法在处理长时任务时效率低下，而直接使用大型语言模型（LLM）作为智能体虽然性能强大，但调用成本高昂。本文提出了一种新的框架EnvGen，旨在利用LLM的推理能力来生成和适应训练环境，帮助小型RL智能体学习它们不擅长的技能。

### 🚀 核心方法
💡 创新点1：LLM生成环境
EnvGen首先通过向LLM提供任务描述和模拟器目标，让LLM生成一系列环境配置，例如不同的地形、初始物品等。这些环境可以并行训练智能体，使其快速学习不同的技能。

💡 创新点2：LLM适应环境
EnvGen通过将智能体在原始环境中的表现反馈给LLM，让LLM不断调整生成的环境，使其更加专注于智能体不擅长的技能。这种动态适应过程可以帮助智能体逐步提高其技能水平。

### 📈 实验结果
在Crafter和Heist游戏环境中进行的实验表明，使用EnvGen训练的小型RL智能体在性能上超过了包括GPT-4在内的SOTA方法，并且学习长时任务的速度显著提高。此外，EnvGen的效率也远高于直接使用LLM作为智能体的方法，因为它只需要很少的LLM调用次数。

### 💬 可借鉴之处
EnvGen提供了一种利用LLM推理能力来提高RL智能体性能的有效方法。它可以应用于各种开放世界游戏和模拟器，帮助智能体快速学习并掌握各种技能。此外，EnvGen的动态适应机制也为RL智能体的训练提供了一种新的思路。

## odyssey--empowering-minecraft-agents-with-open-world-skills
### Abstract
Recent studies have delved into constructing generalist agents for open-world
environments like Minecraft. Despite the encouraging results, existing efforts
mainly focus on solving basic programmatic tasks, e.g., material collection and
tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond
task as the ultimate goal. This limitation stems from the narrowly defined set
of actions available to agents, requiring them to learn effective long-horizon
strategies from scratch. Consequently, discovering diverse gameplay
opportunities in the open world becomes challenging. In this work, we introduce
Odyssey, a new framework that empowers Large Language Model (LLM)-based agents
with open-world skills to explore the vast Minecraft world. Odyssey comprises
three key parts: (1) An interactive agent with an open-world skill library that
consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned
LLaMA-3 model trained on a large question-answering dataset with 390k+
instruction entries derived from the Minecraft Wiki. (3) A new agent capability
benchmark includes the long-term planning task, the dynamic-immediate planning
task, and the autonomous exploration task. Extensive experiments demonstrate
that the proposed Odyssey framework can effectively evaluate different
capabilities of LLM-based agents. All datasets, model weights, and code are
publicly available to motivate future research on more advanced autonomous
agent solutions.
### 🌟 论文解读 | Odyssey：赋予Minecraft智能体开放世界技能

### 📌 背景痛点/本文动机
近年来，许多研究致力于构建能够在开放世界环境中（如Minecraft）执行任务的通用智能体。尽管取得了令人鼓舞的成果，但现有工作主要集中在解决基本的编程任务，例如收集材料和制作工具，并将“获得钻石”任务视为最终目标。这种局限性源于智能体可用的动作集过于狭窄，需要它们从头开始学习有效的长期策略。因此，在开放世界中探索多样化的游戏玩法变得具有挑战性。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：开放世界技能库
Odyssey框架开发了一个基于大型语言模型（LLM）的交互式智能体，该智能体配备了一个开放世界技能库，其中包含40个基本技能和183个组合技能。这些技能涵盖了从资源收集到工具制作，再到战斗和探索的各种任务，为智能体提供了丰富的工具来应对开放世界的挑战。

💡 创新点2：微调LLaMA-3模型
为了提高智能体在Minecraft中的性能，Odyssey框架使用来自Minecraft维基的大规模问答数据集对LLaMA-3模型进行了微调。通过生成包含390k+指令条目的训练数据集，并使用LoRA技术进行高效训练，Odyssey框架显著提升了LLM模型在Minecraft领域的知识储备和推理能力。

💡 创新点3：智能体能力基准
Odyssey框架引入了一个新的智能体能力基准，包括长期规划任务、动态即时规划任务和自主探索任务。这些任务涵盖了Minecraft中的各种复杂场景，并要求智能体展现出多样化的解决方案。通过这些基准任务，研究人员可以全面评估智能体的规划能力、资源管理能力、技能检索能力以及自主探索能力。

### 📈 实验结果
实验结果表明，Odyssey框架在基本编程任务和智能体能力基准任务上都取得了显著的性能提升。与现有方法相比，Odyssey框架的智能体在完成任务的速度、成功率和资源利用率方面都表现出色。此外，消融实验也证明了开放世界技能库和LLM规划器对智能体整体性能的关键作用。

### 💬 可借鉴之处
Odyssey框架为开发和研究开放世界智能体提供了一个全面的框架，具有以下可借鉴之处：

* **开放世界技能库**：为智能体提供丰富的工具和策略，使其能够应对各种复杂的任务和挑战。
* **微调LLM模型**：通过领域特定的数据集进行微调，提升LLM模型在特定领域的知识储备和推理能力。
* **智能体能力基准**：为评估智能体的不同能力提供标准化的框架，促进开放世界智能体研究的进展。

### 🌟 总结
Odyssey框架为开放世界智能体的发展开辟了新的可能性，并为研究人员提供了一个强大的工具来探索和评估智能体的能力。随着未来研究的不断深入，Odyssey框架有望推动开放世界智能体技术的进一步发展，并为人工智能的通用性研究做出贡献。

## react-meets-actre--when-language-agents-enjoy-training-data-autonomy
### Abstract
Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotation or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
### 🌟 论文解读 | 语言智能体自主训练数据标注框架：A3T

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，语言智能体在自主决策方面展现出巨大的潜力。然而，训练这些智能体需要大量的多步推理和动作轨迹作为训练数据，而这些数据的收集通常需要大量的人工标注或实现各种提示框架，这限制了训练的规模和效率。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：A3T框架
本文提出了A3T框架，该框架能够实现语言智能体轨迹的自主标注。A3T的核心是ActRe提示智能体，它能够解释任意动作的原因。当随机采样一个外部动作时，ReAct风格的智能体可以查询ActRe智能体以获取该动作的文本理由。然后，通过将ActRe的推理结果添加到采样动作之前，合成新的轨迹。这样，ReAct风格的智能体可以执行多个轨迹来处理失败的任务，并选择成功的轨迹来补充失败的轨迹，进行对比自我训练。

💡 创新点2：对比自我训练
A3T框架利用策略梯度方法，通过二进制奖励来实现对比自我训练。智能体执行每个合成的轨迹后，环境会提供终端奖励，自动标注轨迹的质量。成功的轨迹被保留下来，并与失败的轨迹一起用于对比自我训练。随着新智能体的训练，更多的轨迹可以被收集和积累，形成一个闭环，促进语言智能体的自我改进。

### 📈 实验结果
在AlfWorld和WebShop两个基准测试中，A3T框架取得了显著的性能提升。在AlfWorld中，经过A3T训练的智能体在未见过的场景中实现了96%的一次性成功率，并且在4次迭代后达到100%的成功率。在WebShop中，A3T智能体的一次性性能与人类平均水平相当，经过4次迭代后，性能接近人类专家。

### 💬 可借鉴之处
A3T框架为语言智能体的自主训练提供了一种有效的方法，通过自主标注和对比自我训练，实现了智能体的闭环自我改进。这种方法可以应用于各种场景，提高语言智能体的性能和自主性。

## learning-from-failure--integrating-negative-examples-when-fine-tuning-large-language-models-as-agents
### Abstract
Large language models (LLMs) have achieved success in acting as agents, which
interact with environments through tools such as search engines. However, LLMs
are optimized for language generation instead of tool use during training or
alignment, limiting their effectiveness as agents. To resolve this problem,
previous work has first collected interaction trajectories between LLMs and
environments, using only trajectories that successfully finished the task to
fine-tune smaller models, making fine-tuning data scarce and acquiring it both
difficult and costly. Discarding failed trajectories also leads to significant
wastage of data and resources and limits the possible optimization paths during
fine-tuning. In this paper, we argue that unsuccessful trajectories offer
valuable insights, and LLMs can learn from these trajectories through
appropriate quality control and fine-tuning strategies. By simply adding a
prefix or suffix that tells the model whether to generate a successful
trajectory during training, we improve model performance by a large margin on
mathematical reasoning, multi-hop question answering, and strategic question
answering tasks. We further analyze the inference results and find that our
method provides a better trade-off between valuable information and errors in
unsuccessful trajectories. To our knowledge, we are the first to demonstrate
the value of negative trajectories and their application in agent-tunning
scenarios. Our findings offer guidance for developing better agent-tuning
methods and low-resource data usage techniques.
### 🌟 论文解读 | 从失败中学习：在微调大型语言模型作为智能体时整合负面示例

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在作为智能体方面取得了成功，它们通过与搜索引擎等工具进行交互来与环境互动。然而，LLMs在训练或对齐过程中主要针对语言生成进行优化，而不是工具使用，这限制了它们作为智能体的有效性。为了解决这个问题，先前的工作首先收集了LLMs和环境之间的交互轨迹，仅使用成功完成任务的轨迹来微调较小的模型，这使得微调数据稀缺，获取难度大且成本高。丢弃失败的轨迹也导致了数据资源的浪费，并限制了微调过程中的优化路径。

### 🚀 核心方法
本文提出了一种名为“负面感知训练”（NAT）的范式，通过在训练过程中添加前缀或后缀来告诉模型是否生成成功的轨迹，从而整合负面示例。这种方法使得LLMs能够从失败的轨迹中学习，并通过适当的质量控制策略来提高模型性能。

### 📈 实验结果
实验结果表明，NAT在数学推理、多跳问答和策略问答任务上显著提高了模型性能。此外，分析推理结果发现，NAT在有价值的信息和错误之间提供了更好的权衡。

### 💬 可借鉴之处
本文首次证明了负面轨迹的价值及其在智能体微调场景中的应用。NAT方法为开发更好的智能体微调方法和低资源数据使用技术提供了指导。

## language-guided-exploration-for-rl-agents-in-text-environments
### Abstract
Real-world sequential decision making is characterized by sparse rewards and
large decision spaces, posing significant difficulty for experiential learning
systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large
Language Models (LLMs), with a wealth of world knowledge, can help RL agents
learn quickly and adapt to distribution shifts. In this work, we introduce
Language Guided Exploration (LGE) framework, which uses a pre-trained language
model (called GUIDE ) to provide decision-level guidance to an RL agent (called
EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging
text environment, LGE outperforms vanilla RL agents significantly and also
outperforms other sophisticated methods like Behaviour Cloning and Text
Decision Transformer.
### 🌟 论文解读 | 语言引导探索：提升文本环境中RL智能体的决策能力

### 📌 背景痛点/本文动机
现实世界的序列决策问题通常具有稀疏奖励和巨大的决策空间，这对经验学习系统，如白板强化学习（RL）智能体，构成了重大挑战。大型语言模型（LLMs）拥有丰富的世界知识，可以帮助RL智能体快速学习和适应分布变化。然而，在文本环境中，RL智能体面临着巨大的动作空间和稀疏的奖励信号，这使得随机探索的方法变得不充分。

### 🚀 核心方法
💡 创新点1：语言引导探索（LGE）框架
本文提出了一个名为“语言引导探索”（LGE）的框架，该框架结合了RL智能体和一个辅助模型，称为“GUIDE”。GUIDE使用预训练的语言模型来为RL智能体（称为“EXPLORER”）提供决策级别的指导，从而显著减少有效动作空间的大小。

💡 创新点2：对比学习训练GUIDE模型
为了使GUIDE能够有效地识别相关动作，本文使用了SimCSE（一种对比学习框架）来微调GUIDE模型。通过对比学习，GUIDE模型能够将任务描述和动作嵌入到共享的表示空间中，从而更好地理解任务和动作之间的相关性。

### 📈 实验结果
在ScienceWorld（一个具有挑战性的文本环境）上，LGE框架显著优于传统的RL智能体，并且也优于其他复杂的方法，如行为克隆和文本决策转换器。实验结果表明，LGE框架能够有效地减少动作空间的大小，并帮助RL智能体更快地学习和适应新的环境。

### 💬 可借鉴之处
本文提出的LGE框架为在文本环境中使用LLMs来指导RL智能体提供了一种新的思路。通过结合LLMs的知识和RL智能体的学习能力，LGE框架能够有效地解决文本环境中RL智能体面临的挑战。此外，本文提出的对比学习训练方法也为其他文本环境中的RL智能体训练提供了参考。

