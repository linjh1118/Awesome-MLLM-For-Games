
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>57</th>
      <td>VirtualHome: Simulating Household Activities via Programs</td>
      <td>In this paper, we are interested in modeling complex activities that occur in<br>a typical household. We propose to use programs, i.e., sequences of atomic<br>actions and interactions, as a high level representation of complex tasks.<br>Programs are interesting because they provide a non-ambiguous representation of<br>a task, and allow agents to execute them. However, nowadays, there is no<br>database providing this type of information. Towards this goal, we first<br>crowd-source programs for a variety of activities that happen in people's<br>homes, via a game-like interface used for teaching kids how to code. Using the<br>collected dataset, we show how we can learn to extract programs directly from<br>natural language descriptions or from videos. We then implement the most common<br>atomic (inter)actions in the Unity3D game engine, and use our programs to<br>"drive" an artificial agent to execute tasks in a simulated household<br>environment. Our VirtualHome simulator allows us to create a large activity<br>video dataset with rich ground-truth, enabling training and testing of video<br>understanding models. We further showcase examples of our agent performing<br>tasks in our VirtualHome based on language descriptions.</td>
      <td>## 🌟 论文解读 | VirtualHome：通过程序模拟家庭活动<br><br>## 📌 背景痛点/本文动机<br>随着人工智能和机器人技术的发展，让机器人执行复杂的家庭活动成为可能。然而，如何有效地描述和执行这些活动仍然是一个挑战。本文提出了一个名为 VirtualHome 的模拟器，旨在通过程序来模拟家庭活动，从而为机器人执行复杂任务提供一种新的方法。<br><br>## 🚀 核心方法<br>💡 创新点1：构建家庭活动知识库<br>本文首先通过众包的方式收集了大量的家庭活动描述，并将其转化为程序形式。这些程序包含了执行任务所需的全部步骤，包括一些常识性步骤，从而为机器人提供了清晰的执行指南。<br><br>💡 创新点2：开发 VirtualHome 模拟器<br>本文开发了一个名为 VirtualHome 的 3D 模拟器，可以模拟家庭环境中的各种活动。通过将程序输入到 VirtualHome 中，可以生成丰富的活动视频数据集，并用于训练和测试视频理解模型。<br><br>💡 创新点3：从文本和视频中生成程序<br>本文提出了一个基于 seq2seq 模型的方法，可以从自然语言描述或视频演示中自动生成程序。这使得机器人可以通过自然语言或视频演示来学习执行新的任务。<br><br>## 📈 实验结果<br>本文在 VirtualHome 模拟器上进行了实验，结果表明，从文本和视频中生成的程序可以有效地驱动机器人执行各种家庭活动。此外，本文还进行了一项人类研究，结果表明，生成的程序与人类对活动的理解具有较高的相关性。<br><br>## 💬 可借鉴之处<br>本文提出的 VirtualHome 模拟器和程序生成方法为机器人执行复杂任务提供了一种新的思路。此外，本文收集的家庭活动知识库和视频数据集也为相关研究提供了宝贵的资源。</td>
    </tr>
    <tr>
      <th>116</th>
      <td>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents</td>
      <td>Recent SOTA approaches for embodied learning via interaction directly employ<br>large language models (LLMs) as agents to determine the next steps in an<br>environment. Due to their world knowledge and reasoning capabilities, LLM<br>agents achieve stronger performance than previous smaller agents based on<br>reinforcement learning (RL); however, frequently calling LLMs is slow and<br>expensive. Instead of directly employing LLMs as agents, can we use LLMs'<br>reasoning capabilities to adaptively create training environments to help<br>smaller RL agents learn useful skills that they are weak at? We propose EnvGen,<br>a novel framework to address this question. We first prompt an LLM to generate<br>training environments by giving it the task description and simulator<br>objectives that the agents should learn and then asking it to generate a set of<br>environment configurations (e.g., different terrains, items initially given to<br>agents, etc.). Next, we train a small RL agent in a mixture of the original and<br>LLM-generated environments. Then, we enable the LLM to continuously adapt the<br>generated environments to progressively improve the skills that the agent is<br>weak at, by providing feedback to the LLM in the form of the agent's<br>performance. We demonstrate the usefulness of EnvGen with comprehensive<br>experiments in Crafter and Heist environments. We find that a small RL agent<br>trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and<br>learns long-horizon tasks significantly faster. We also show that using an LLM<br>to adapt environments dynamically outperforms curriculum learning approaches<br>and how the environments are adapted to help improve RL agents' weaker skills<br>over time. Additionally, EnvGen is substantially more efficient as it only uses<br>a small number of LLM calls (e.g., 4 in total), whereas LLM agents require<br>thousands of calls. Lastly, we present detailed ablation studies for EnvGen<br>design choices.</td>
      <td>## 🌟 论文解读 | EnvGen：利用LLM生成和适应环境，训练具身智能体<br><br>## 📌 背景痛点/本文动机<br>随着具身智能体在开放世界游戏中的兴起，如何让智能体快速学习并掌握各种技能成为了一个挑战。传统的强化学习（RL）方法在处理长时任务时效率低下，而直接使用大型语言模型（LLM）作为智能体虽然性能强大，但调用成本高昂。本文提出了一种新的框架EnvGen，旨在利用LLM的推理能力来生成和适应训练环境，帮助小型RL智能体学习它们不擅长的技能。<br><br>## 🚀 核心方法<br>💡 创新点1：LLM生成环境<br>EnvGen首先通过向LLM提供任务描述和模拟器目标，让LLM生成一系列环境配置，例如不同的地形、初始物品等。这些环境可以并行训练智能体，使其快速学习不同的技能。<br><br>💡 创新点2：LLM适应环境<br>EnvGen通过将智能体在原始环境中的表现反馈给LLM，让LLM不断调整生成的环境，使其更加专注于智能体不擅长的技能。这种动态适应过程可以帮助智能体逐步提高其技能水平。<br><br>## 📈 实验结果<br>在Crafter和Heist游戏环境中进行的实验表明，使用EnvGen训练的小型RL智能体在性能上超过了包括GPT-4在内的SOTA方法，并且学习长时任务的速度显著提高。此外，EnvGen的效率也远高于直接使用LLM作为智能体的方法，因为它只需要很少的LLM调用次数。<br><br>## 💬 可借鉴之处<br>EnvGen提供了一种利用LLM推理能力来提高RL智能体性能的有效方法。它可以应用于各种开放世界游戏和模拟器，帮助智能体快速学习并掌握各种技能。此外，EnvGen的动态适应机制也为RL智能体的训练提供了一种新的思路。</td>
    </tr>
    <tr>
      <th>35</th>
      <td>ChatHaruhi: Reviving Anime Character in Reality via Large Language Model</td>
      <td>Role-playing chatbots built on large language models have drawn interest, but<br>better techniques are needed to enable mimicking specific fictional characters.<br>We propose an algorithm that controls language models via an improved prompt<br>and memories of the character extracted from scripts. We construct ChatHaruhi,<br>a dataset covering 32 Chinese / English TV / anime characters with over 54k<br>simulated dialogues. Both automatic and human evaluations show our approach<br>improves role-playing ability over baselines. Code and data are available at<br>https://github.com/LC1332/Chat-Haruhi-Suzumiya .</td>
      <td>## 🌟 论文解读 | ChatHaruhi：通过大型语言模型在现实中复活动漫角色<br><br>## 📌 背景痛点/本文动机<br>随着ChatGPT等大型语言模型的发布，基于这些模型的扮演式聊天机器人引起了广泛关注。然而，现有的扮演式聊天机器人存在一些问题：1. 过度依赖语言模型自身的记忆，如果模型对作品的理解模糊，则难以很好地模仿特定角色；2. “知道所有关于角色的知识”定义模糊，容易产生幻觉；3. 即使有扮演提示，聊天机器人的对话风格仍然受到底层语言模型的影响，需要为每个角色精细调整提示。<br><br>## 🚀 核心方法<br>本文提出了一个基于大型语言模型的完整扮演式聊天机器人算法系统，该系统可以有效地组织角色的记忆，使语言模型能够在对话中模仿特定动漫/电视角色的语气和知识。主要创新点如下：<br><br>💡 创新点1：构建了包含32个不同中英动漫/电视角色的扮演式数据集，涵盖超过54,000个模拟对话。通过收集和结构化提取电影、小说、剧本中的对话，收集了超过22,000个对话交换，可用于训练和评估扮演式语言模型。<br><br>💡 创新点2：设计了自动生成对话的系统，即使对于对话较少的角色，也能生成符合角色个性的对话。这允许我们为微调本地模型生成足够的数据。<br><br>💡 创新点3：使用自动和人工评估来评估和比较不同的扮演式聊天机器人。自动评估测试聊天机器人是否能够对经典情节点做出与原始剧本相似的回答。人工评估提出了两个指标供评估者评估：一致性（聊天机器人的回答是否与角色的原始设定一致）和响应质量（聊天机器人的回答是否具有良好的语言质量）。<br><br>## 📈 实验结果<br>实验结果表明，在相同的底层语言模型下，本文提出的算法在扮演式性能方面优于基线模型。<br><br>## 💬 可借鉴之处<br>本文提出的扮演式聊天机器人算法系统可以应用于游戏、创意产业等领域，为用户提供更加丰富的互动体验。此外，本文构建的扮演式数据集和自动生成对话的系统也为相关研究提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Empowering Working Memory for Large Language Model Agents</td>
      <td>Large language models (LLMs) have achieved impressive linguistic<br>capabilities. However, a key limitation persists in their lack of human-like<br>memory faculties. LLMs exhibit constrained memory retention across sequential<br>interactions, hindering complex reasoning. This paper explores the potential of<br>applying cognitive psychology's working memory frameworks, to enhance LLM<br>architecture. The limitations of traditional LLM memory designs are analyzed,<br>including their isolation of distinct dialog episodes and lack of persistent<br>memory links. To address this, an innovative model is proposed incorporating a<br>centralized Working Memory Hub and Episodic Buffer access to retain memories<br>across episodes. This architecture aims to provide greater continuity for<br>nuanced contextual reasoning during intricate tasks and collaborative<br>scenarios. While promising, further research is required into optimizing<br>episodic memory encoding, storage, prioritization, retrieval, and security.<br>Overall, this paper provides a strategic blueprint for developing LLM agents<br>with more sophisticated, human-like memory capabilities, highlighting memory<br>mechanisms as a vital frontier in artificial general intelligence.</td>
      <td>## 🌟 论文解读 | 为大型语言模型代理赋能工作记忆<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在语言理解和生成方面取得了显著进展，但它们缺乏类似人类的记忆能力，限制了它们在复杂推理和协作场景中的表现。LLMs在连续交互中的记忆保留能力有限，每个交互都被视为独立的对话，缺乏持续的内存链接，这阻碍了复杂的推理。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了一种创新的模型，该模型包含一个中央工作记忆中心（Working Memory Hub）和访问情境缓冲区（Episodic Buffer）的功能，以保留跨对话的记忆。这种架构旨在为复杂的任务和协作场景中的微妙上下文推理提供更大的连续性。<br><br>💡 创新点2：本文还探讨了在多智能体系统中管理智能体对情境缓冲区访问的策略，包括基于角色的访问控制、基于任务的内存分配、自主内存检索和专门的内存管理代理。<br><br>## 📈 实验结果<br>本文没有提供具体的实验结果，而是提出了一种战略蓝图，用于开发具有更复杂、类似人类记忆能力的LLM代理。<br><br>## 💬 可借鉴之处<br>本文提出的工作记忆模型为LLM代理提供了更高级的记忆功能，有助于提高它们在复杂任务和协作场景中的表现。此外，本文还探讨了在多智能体系统中管理内存访问的策略，为开发更高效、安全的内存管理系统提供了参考。</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Multi-Stage Episodic Control for Strategic Exploration in Text Games</td>
      <td>Text adventure games present unique challenges to reinforcement learning<br>methods due to their combinatorially large action spaces and sparse rewards.<br>The interplay of these two factors is particularly demanding because large<br>action spaces require extensive exploration, while sparse rewards provide<br>limited feedback. This work proposes to tackle the explore-vs-exploit dilemma<br>using a multi-stage approach that explicitly disentangles these two strategies<br>within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins<br>each episode using an exploitation policy that imitates a set of promising<br>trajectories from the past, and then switches over to an exploration policy<br>aimed at discovering novel actions that lead to unseen state spaces. This<br>policy decomposition allows us to combine global decisions about which parts of<br>the game space to return to with curiosity-based local exploration in that<br>space, motivated by how a human may approach these games. Our method<br>significantly outperforms prior approaches by 27% and 11% average normalized<br>score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in<br>both deterministic and stochastic settings, respectively. On the game of Zork1,<br>in particular, XTX obtains a score of 103, more than a 2x improvement over<br>prior methods, and pushes past several known bottlenecks in the game that have<br>plagued previous state-of-the-art methods.</td>
      <td>## 🌟 论文解读 | 多阶段策略探索：文本游戏中的强化学习新突破<br><br>## 📌 背景痛点/本文动机<br>文本冒险游戏为强化学习算法提供了独特的测试平台，但同时也带来了巨大的挑战。这些游戏具有组合爆炸式的动作空间和稀疏的奖励，这使得探索与利用之间的平衡变得尤为困难。大动作空间需要广泛的探索，而稀疏的奖励则提供了有限的反馈。现有的方法通常采用单一策略和动作选择策略，难以在探索与利用之间找到合适的平衡点。<br><br>## 🚀 核心方法<br>本文提出了名为 eXploit-Then-eXplore (XTX) 的多阶段控制算法，该算法在每个回合中明确地将探索和利用策略分离。XTX 算法包含两个阶段：<br><br>* **利用阶段**：该阶段使用一个模仿过去成功轨迹的策略，使智能体能够返回到游戏空间中已探索的前沿状态。<br>* **探索阶段**：该阶段使用一个基于好奇心驱动的策略，旨在发现新颖的动作，并探索未知的游戏状态空间。<br><br>这种策略分解允许智能体结合全局决策和局部探索，从而更好地应对稀疏奖励和动态动作空间的挑战。<br><br>## 📈 实验结果<br>在 Jericho 基准测试的 12 个游戏中，XTX 算法在确定性和随机性设置下分别比现有方法提高了 27% 和 11% 的平均归一化分数。特别是在 Zork1 游戏中，XTX 算法取得了 103 分的成绩，比现有方法提高了 2 倍以上，并克服了游戏中一些已知的瓶颈。<br><br>## 💬 可借鉴之处<br>* **多阶段策略**：将探索和利用策略分离，可以更好地平衡两者之间的关系，从而提高学习效率。<br>* **模仿学习**：利用过去成功的经验来指导智能体的行为，可以加快学习速度。<br>* **好奇心驱动探索**：通过奖励新颖的动作，可以鼓励智能体探索未知的游戏状态空间。<br>* **混合策略**：使用混合策略可以提供更细粒度的控制，从而更好地适应不同的游戏环境。<br><br>## 🌟 总结<br>XTX 算法为文本冒险游戏中的强化学习提供了一种新的思路，通过多阶段策略和混合策略，有效地解决了探索与利用之间的平衡问题，并在实际游戏中取得了显著的性能提升。</td>
    </tr>
    <tr>
      <th>62</th>
      <td>Large Sequence Models for Sequential Decision-Making: A Survey</td>
      <td>Transformer architectures have facilitated the development of large-scale and<br>general-purpose sequence models for prediction tasks in natural language<br>processing and computer vision, e.g., GPT-3 and Swin Transformer. Although<br>originally designed for prediction problems, it is natural to inquire about<br>their suitability for sequential decision-making and reinforcement learning<br>problems, which are typically beset by long-standing issues involving sample<br>efficiency, credit assignment, and partial observability. In recent years,<br>sequence models, especially the Transformer, have attracted increasing interest<br>in the RL communities, spawning numerous approaches with notable effectiveness<br>and generalizability. This survey presents a comprehensive overview of recent<br>works aimed at solving sequential decision-making tasks with sequence models<br>such as the Transformer, by discussing the connection between sequential<br>decision-making and sequence modeling, and categorizing them based on the way<br>they utilize the Transformer. Moreover, this paper puts forth various potential<br>avenues for future research intending to improve the effectiveness of large<br>sequence models for sequential decision-making, encompassing theoretical<br>foundations, network architectures, algorithms, and efficient training systems.<br>As this article has been accepted by the Frontiers of Computer Science, here is<br>an early version, and the most up-to-date version can be found at<br>https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5</td>
      <td>## 🌟 论文解读 | 大型序列模型在顺序决策中的潜力：综述<br><br>## 📌 背景痛点/本文动机<br>随着深度学习在自然语言处理和计算机视觉领域的广泛应用，大型序列模型，尤其是Transformer架构，因其强大的预测能力和泛化能力而备受关注。然而，传统的顺序决策和强化学习问题，如样本效率、信用分配和部分可观察性等问题，一直困扰着该领域的发展。本文综述了近年来利用Transformer等序列模型解决顺序决策问题的研究进展，并探讨了未来研究的潜在方向。<br><br>## 🚀 核心方法<br>💡 创新点1：将顺序决策问题转化为序列建模问题<br>本文提出了一种新的思路，将顺序决策问题转化为序列建模问题，利用序列模型（如Transformer）来处理。这种方法可以有效地解决传统强化学习方法中存在的样本效率、信用分配和部分可观察性问题。<br><br>💡 创新点2：利用Transformer架构的优势<br>Transformer架构具有高并行化、可扩展性、适当的归纳偏置等优势，使其成为解决顺序决策问题的理想选择。本文详细介绍了Transformer架构在顺序决策中的应用，包括离线强化学习、基于模型的强化学习、元强化学习、多智能体强化学习、目标条件强化学习和智能体架构等方面。<br><br>## 📈 实验结果<br>本文综述了多个基于Transformer的顺序决策模型，并在多个任务和环境中进行了实验。结果表明，这些模型在样本效率、信用分配和部分可观察性等方面取得了显著的性能提升，并展现出良好的泛化能力。<br><br>## 💬 可借鉴之处<br>本文为顺序决策问题的研究提供了新的思路和方法，并指出了未来研究的潜在方向。具体而言，未来研究可以关注以下几个方面：<br><br>* **理论基础的完善**： 进一步研究序列建模方法与传统强化学习方法的结合，为政策优化提供理论保证。<br>* **网络架构的改进**： 开发针对顺序决策任务的定制Transformer架构，以提高模型性能。<br>* **算法的统一框架**： 建立一个统一的框架，涵盖各种顺序决策场景，并有效地整合多模态知识。<br>* **高效训练系统的设计**： 设计高效的训练系统，以支持大型决策模型的训练，并优化训练效率。<br><br>## 总结<br>本文综述了大型序列模型在顺序决策中的应用，并探讨了未来研究的潜在方向。随着序列模型和强化学习技术的不断发展，我们有理由相信，大型决策模型将在未来发挥越来越重要的作用，并为解决各种复杂的顺序决策问题提供新的解决方案。</td>
    </tr>
    <tr>
      <th>86</th>
      <td>Pre-trained Language Models as Prior Knowledge for Playing Text-based Games</td>
      <td>Recently, text world games have been proposed to enable artificial agents to<br>understand and reason about real-world scenarios. These text-based games are<br>challenging for artificial agents, as it requires an understanding of and<br>interaction using natural language in a partially observable environment.<br>Agents observe the environment via textual descriptions designed to be<br>challenging enough for even human players. Past approaches have not paid enough<br>attention to the language understanding capability of the proposed agents.<br>Typically, these approaches train from scratch, an agent that learns both<br>textual representations and the gameplay online during training using a<br>temporal loss function. Given the sample-inefficiency of RL approaches, it is<br>inefficient to learn rich enough textual representations to be able to<br>understand and reason using the textual observation in such a complicated game<br>environment setting. In this paper, we improve the semantic understanding of<br>the agent by proposing a simple RL with LM framework where we use<br>transformer-based language models with Deep RL models. We perform a detailed<br>study of our framework to demonstrate how our model outperforms all existing<br>agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6<br>higher than the state-of-the-art model. Overall, our proposed approach<br>outperforms 4 games out of the 14 text-based games, while performing comparable<br>to the state-of-the-art models on the remaining games.</td>
      <td>## 🌟 论文解读 | 利用预训练语言模型提升文本游戏中的智能体表现<br><br>## 📌 背景痛点/本文动机<br>文本世界游戏为人工智能体提供了理解和推理现实世界场景的机会。然而，这些游戏对智能体来说极具挑战性，因为它们需要在部分可观察的环境中理解和交互自然语言。现有的方法往往忽略了智能体的语言理解能力，并且通常从头开始训练，导致样本效率低下。本文旨在通过引入预训练语言模型来提升智能体的语义理解能力，从而在文本游戏中取得更好的表现。<br><br>## 🚀 核心方法<br>💡 创新点1：预训练语言模型作为先验知识<br>本文提出了一种简单的RL与LM框架，使用基于Transformer的语言模型（如DistilBERT）与深度强化学习模型相结合。通过在大型通用英语语料库上进行预训练，然后针对特定下游任务进行微调，预训练语言模型能够为智能体提供丰富的语言理解和先验知识。<br><br>💡 创新点2：游戏感知的预训练语言模型<br>为了使预训练语言模型更好地适应游戏环境，本文使用独立的人类游戏播放轨迹数据集对DistilBERT进行微调，从而使其具备游戏感知能力。这种微调过程有助于将语言模型的知识和世界感知能力转移到不同的游戏和智能体中。<br><br>## 📈 实验结果<br>本文在14个文本游戏中对所提出的框架进行了评估，结果显示，在Zork1游戏中，模型取得了44.7分的成绩，比现有最佳模型高出1.6分。总体而言，该框架在4个游戏中超越了现有模型，并在其他游戏中表现与现有模型相当。<br><br>## 💬 可借鉴之处<br>本文提出的预训练语言模型作为先验知识的方法，为文本游戏中的智能体设计提供了新的思路。通过利用预训练语言模型的知识和世界感知能力，智能体能够更好地理解和推理游戏环境，从而在游戏中取得更好的表现。此外，本文还强调了微调预训练语言模型以适应特定游戏环境的重要性，这为未来研究提供了有价值的启示。</td>
    </tr>
    <tr>
      <th>19</th>
      <td>CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents</td>
      <td>The generalization of decision-making agents encompasses two fundamental<br>elements: learning from past experiences and reasoning in novel contexts.<br>However, the predominant emphasis in most interactive environments is on<br>learning, often at the expense of complexity in reasoning. In this paper, we<br>introduce CivRealm, an environment inspired by the Civilization game.<br>Civilization's profound alignment with human history and society necessitates<br>sophisticated learning, while its ever-changing situations demand strong<br>reasoning to generalize. Particularly, CivRealm sets up an<br>imperfect-information general-sum game with a changing number of players; it<br>presents a plethora of complex features, challenging the agent to deal with<br>open-ended stochastic environments that require diplomacy and negotiation<br>skills. Within CivRealm, we provide interfaces for two typical agent types:<br>tensor-based agents that focus on learning, and language-based agents that<br>emphasize reasoning. To catalyze further research, we present initial results<br>for both paradigms. The canonical RL-based agents exhibit reasonable<br>performance in mini-games, whereas both RL- and LLM-based agents struggle to<br>make substantial progress in the full game. Overall, CivRealm stands as a<br>unique learning and reasoning challenge for decision-making agents. The code is<br>available at https://github.com/bigai-ai/civrealm.</td>
      <td>## 🌟 论文解读 | CivRealm：决策智能体的学习与推理之旅<br><br>## 📌 背景痛点/本文动机<br>传统的决策智能体环境往往过于强调学习，而忽视了推理的重要性。然而，在实际应用中，智能体需要同时具备学习和推理能力，才能更好地适应复杂多变的环境。为了解决这个问题，本文提出了CivRealm，一个基于文明游戏的交互式环境，旨在推动决策智能体学习和推理能力的边界。<br><br>## 🚀 核心方法<br>💡 创新点1：CivRealm环境<br>CivRealm是一个基于文明游戏的开放环境，具有以下特点：<br>* **不完全信息**：玩家只能获取自己单位发现的信息，需要推理其他玩家的意图。<br>* **随机性**：游戏中有随机事件和危机，需要智能体灵活应对。<br>* **多目标**：有多种胜利路径，需要平衡经济、军事、外交、文化和科技发展。<br>* **动态空间**：游戏过程中，玩家的状态和动作空间会动态变化。<br>* **多智能体**：多个玩家可以互动，需要智能体进行合作和竞争。<br>* **动态玩家数量**：游戏过程中，玩家数量会发生变化，需要智能体适应新的环境。<br>* **通信**：玩家可以通过外交行动和自然语言聊天进行交流。<br><br>💡 创新点2：支持多种智能体类型<br>CivRealm提供了两种API接口，分别支持基于张量的智能体和基于语言的智能体：<br>* **基于张量的智能体**：使用深度强化学习等方法，擅长学习和模式识别。<br>* **基于语言的智能体**：使用大型语言模型等方法，擅长推理和自然语言处理。<br><br>💡 创新点3：基准方法和评估指标<br>本文提出了三种基准方法，包括：<br>* **基于张量的强化学习**：使用AlphaStar的架构，擅长处理复杂动态和海量信息。<br>* **BaseLang**：基于AutoGPT的架构，擅长推理和自然语言处理。<br>* **Mastaba**：基于BaseLang的架构，引入层次结构，提高全局视角和决策能力。<br><br>## 📈 实验结果<br>实验结果表明，基于张量的强化学习在迷你游戏中表现良好，但在完整游戏中仍然存在局限性，例如短视策略和难以处理稀疏奖励。基于语言的智能体在完整游戏中表现更佳，但仍然需要改进推理能力和全局视角。<br><br>## 💬 可借鉴之处<br>CivRealm为决策智能体的学习和推理能力提供了一个独特的挑战平台，可以用于评估和改进智能体的性能。此外，CivRealm还可以用于研究人类社会的动态、历史事件的结果和未来的社会轨迹。</td>
    </tr>
    <tr>
      <th>32</th>
      <td>CALYPSO: LLMs as Dungeon Masters' Assistants</td>
      <td>The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to<br>perform multiple tasks simultaneously. The DM must digest information about the<br>game setting and monsters, synthesize scenes to present to other players, and<br>respond to the players' interactions with the scene. Doing all of these tasks<br>while maintaining consistency within the narrative and story world is no small<br>feat of human cognition, making the task tiring and unapproachable to new<br>players. Large language models (LLMs) like GPT-3 and ChatGPT have shown<br>remarkable abilities to generate coherent natural language text. In this paper,<br>we conduct a formative evaluation with DMs to establish the use cases of LLMs<br>in D&D and tabletop gaming generally. We introduce CALYPSO, a system of<br>LLM-powered interfaces that support DMs with information and inspiration<br>specific to their own scenario. CALYPSO distills game context into bite-sized<br>prose and helps brainstorm ideas without distracting the DM from the game. When<br>given access to CALYPSO, DMs reported that it generated high-fidelity text<br>suitable for direct presentation to players, and low-fidelity ideas that the DM<br>could develop further while maintaining their creative agency. We see CALYPSO<br>as exemplifying a paradigm of AI-augmented tools that provide synchronous<br>creative assistance within established game worlds, and tabletop gaming more<br>broadly.</td>
      <td>## 🌟 论文解读 | CALYPSO：大型语言模型助力地下城主<br><br>## 📌 背景痛点/本文动机<br>地下城与龙（D&D）是一款经典的桌面角色扮演游戏，其中地下城主（DM）扮演着至关重要的角色。DM需要同时处理多项任务，包括消化游戏背景和怪物信息、构建场景、回应玩家互动等。这些任务对人类认知能力要求极高，对于新玩家来说尤其具有挑战性。大型语言模型（LLM）如GPT-3和ChatGPT在生成连贯的自然语言文本方面表现出色。本文旨在探索LLM在D&D和桌面游戏中的应用，并提出了CALYPSO系统，该系统利用LLM为DM提供信息和支持，帮助他们更好地进行游戏。<br><br>## 🚀 核心方法<br>💡 创新点1：CALYPSO系统<br>CALYPSO是一个由LLM驱动的界面系统，旨在支持DM在游戏中获取信息和灵感。它包括三个主要功能：<br>1. **遭遇理解**：使用GPT-3对游戏背景和怪物信息进行摘要，帮助DM快速理解遭遇。<br>2. **聚焦头脑风暴**：使用ChatGPT与DM进行对话，帮助他们进一步探索遭遇细节或生成新的想法。<br>3. **开放域聊天基线**：使用ChatGPT提供一个开放域的聊天界面，供玩家和DM进行非游戏相关的交流。<br><br>💡 创新点2：LLM的创造性辅助<br>CALYPSO系统展示了LLM作为创造性辅助工具的潜力。它不仅能够生成高保真度的文本，适合直接呈现给玩家，还能够提供低保真度的想法，供DM进一步发展和完善。这种辅助方式保留了DM的创造性自主权，使他们能够更好地专注于游戏中的认知任务。<br><br>## 📈 实验结果<br>研究结果表明，DM在使用CALYPSO系统后，普遍认为它能够生成适合直接呈现给玩家的文本，并提供有价值的灵感。DM们利用CALYPSO系统来理解复杂的怪物信息、头脑风暴非玩家角色或怪物之间的互动，并获取建议，将这些建议融入到故事中呈现给玩家，而不会影响游戏的节奏。<br><br>## 💬 可借鉴之处<br>本文的研究结果对于开发AI辅助工具在桌面游戏和其他创意领域中的应用具有重要的启示意义。CALYPSO系统的设计理念和方法可以为其他类似项目提供参考，例如：<br>1. **理解用户需求**：通过访谈和用户研究，深入了解用户的需求和痛点，从而设计出更符合用户需求的AI辅助工具。<br>2. **利用LLM的创造性潜力**：LLM在生成连贯文本和提供创造性灵感方面具有巨大潜力，可以将其应用于各种创意场景。<br>3. **保持用户的创造性自主权**：AI辅助工具应该作为用户的助手，而不是替代者，帮助用户更好地发挥自己的创造力。<br><br>总而言之，CALYPSO系统展示了LLM在桌面游戏中的应用潜力，并为开发AI辅助工具提供了有价值的经验和启示。</td>
    </tr>
    <tr>
      <th>12</th>
      <td>ScienceWorld: Is your Agent Smarter than a 5th Grader?</td>
      <td>We present ScienceWorld, a benchmark to test agents' scientific reasoning<br>abilities in a new interactive text environment at the level of a standard<br>elementary school science curriculum. Despite the transformer-based progress<br>seen in question-answering and scientific text processing, we find that current<br>models cannot reason about or explain learned science concepts in novel<br>contexts. For instance, models can easily answer what the conductivity of a<br>known material is but struggle when asked how they would conduct an experiment<br>in a grounded environment to find the conductivity of an unknown material. This<br>begs the question of whether current models are simply retrieving answers by<br>way of seeing a large number of similar examples or if they have learned to<br>reason about concepts in a reusable manner. We hypothesize that agents need to<br>be grounded in interactive environments to achieve such reasoning capabilities.<br>Our experiments provide empirical evidence supporting this hypothesis --<br>showing that a 1.5 million parameter agent trained interactively for 100k steps<br>outperforms a 11 billion parameter model statically trained for scientific<br>question-answering and reasoning from millions of expert demonstrations.</td>
      <td>## 🌟 论文解读 | ScienceWorld：你的智能体比五年级学生更聪明吗？<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型在问答和科学文本处理方面的进步，研究人员开始质疑这些模型是否真正理解了它们所回答的问题，或者它们是否只是通过大量相似示例的检索来获取答案。为了解决这个问题，本文提出了ScienceWorld，一个用于测试智能体在标准小学科学课程水平上的科学推理能力的基准。<br><br>## 🚀 核心方法<br>💡 创新点1：构建ScienceWorld<br>ScienceWorld是一个复杂的交互式文本环境，具有模拟热力学、电路、化学反应和生物过程的引擎。它包含10个相互连接的位置，以及多达200种类型的对象，包括设备、仪器、动植物、电气元件、物质、容器和家具等。<br><br>💡 创新点2：实施30个基准任务<br>这些任务涵盖了10个主题，包括物质状态的变化、温度测量、电路、摩擦、物体分类、化学混合物、植物和传粉者、寿命、生命周期和孟德尔遗传学。每个任务都包含10到1400个参数变化，以防止过拟合并鼓励泛化。<br><br>💡 创新点3：评估现有智能体<br>本文评估了5个最先进的强化学习和语言模型智能体，包括DRRN、KG-A2C、CALM、BC和TDT。结果表明，这些智能体在需要使用科学领域知识的任务上表现不佳，平均得分仅为0.17。<br><br>## 📈 实验结果<br>实验结果表明，ScienceWorld是一个具有挑战性的基准，即使是现有的最先进智能体也无法很好地完成这些任务。此外，实验还发现，在交互式环境中进行交互式训练的智能体比在静态文本源上进行离线训练的大型语言模型更有效。<br><br>## 💬 可借鉴之处<br>本文提出的ScienceWorld基准为评估智能体的科学推理能力提供了一个新的平台。此外，本文的研究结果表明，交互式训练可以帮助智能体更好地学习科学知识和常识，并将其应用于实际任务中。</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Guiding Pretraining in Reinforcement Learning with Large Language Models</td>
      <td>Reinforcement learning algorithms typically struggle in the absence of a<br>dense, well-shaped reward function. Intrinsically motivated exploration methods<br>address this limitation by rewarding agents for visiting novel states or<br>transitions, but these methods offer limited benefits in large environments<br>where most discovered novelty is irrelevant for downstream tasks. We describe a<br>method that uses background knowledge from text corpora to shape exploration.<br>This method, called ELLM (Exploring with LLMs) rewards an agent for achieving<br>goals suggested by a language model prompted with a description of the agent's<br>current state. By leveraging large-scale language model pretraining, ELLM<br>guides agents toward human-meaningful and plausibly useful behaviors without<br>requiring a human in the loop. We evaluate ELLM in the Crafter game environment<br>and the Housekeep robotic simulator, showing that ELLM-trained agents have<br>better coverage of common-sense behaviors during pretraining and usually match<br>or improve performance on a range of downstream tasks. Code available at<br>https://github.com/yuqingd/ellm.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型引导强化学习的预训练<br><br>## 📌 背景痛点/本文动机<br>强化学习算法在缺乏密集、良好形状的奖励函数时通常会遇到困难。内在动机探索方法通过奖励代理访问新颖状态或转换来解决这一限制，但在大多数发现的新颖性对下游任务无关紧要的大型环境中，这些方法提供的益处有限。本文提出了一种方法，该方法使用来自文本语料库的背景知识来塑造探索。这种方法称为ELLM（使用大型语言模型进行探索），它奖励代理实现由语言模型提出的与代理当前状态描述相关的目标。通过利用大规模语言模型预训练，ELLM引导代理朝着人类有意义且可能有用的行为发展，而无需人工干预。<br><br>## 🚀 核心方法<br>💡 创新点1：利用大型语言模型（LLM）的背景知识来塑造探索。LLM是概率文本模型，其预测编码了丰富的关于人类常识知识和文化习俗的信息。ELLM通过查询LLM来获取可能的目标，并奖励代理实现这些建议，从而引导探索朝着完成多样化、上下文敏感和人类有意义的目标。<br><br>💡 创新点2：使用LLM生成的目标作为内在奖励函数。ELLM通过测量LLM生成的目标与环境中代理转换的描述之间的语义相似性来计算奖励。当转换的描述与目标描述足够接近时，代理将获得与相似度成比例的奖励。<br><br>## 📈 实验结果<br>本文在Crafter游戏环境和Housekeep机器人模拟器中评估了ELLM。结果表明，ELLM训练的代理在预训练期间对常识行为的覆盖范围更好，并且在下游任务上的性能通常与基线相当或有所提高。<br><br>## 💬 可借鉴之处<br>本文提出的方法可以用于引导强化学习代理在缺乏外部定义的奖励的情况下学习有用的行为。通过利用LLM的背景知识，ELLM可以引导代理朝着人类有意义且可能有用的行为发展，从而提高强化学习算法的性能。此外，本文还探讨了LLM性能对提示选择、状态和转换描述的敏感性，并提出了改进LLM性能的潜在方法。</td>
    </tr>
    <tr>
      <th>38</th>
      <td>An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents</td>
      <td>The development of believable, natural, and interactive digital artificial<br>agents is a field of growing interest. Theoretical uncertainties and technical<br>barriers present considerable challenges to the field, particularly with<br>regards to developing agents that effectively simulate human emotions. Large<br>language models (LLMs) might address these issues by tapping common patterns in<br>situational appraisal. In three empirical experiments, this study tests the<br>capabilities of LLMs to solve emotional intelligence tasks and to simulate<br>emotions. It presents and evaluates a new chain-of-emotion architecture for<br>emotion simulation within video games, based on psychological appraisal<br>research. Results show that it outperforms standard LLM architectures on a<br>range of user experience and content analysis metrics. This study therefore<br>provides early evidence of how to construct and test affective agents based on<br>cognitive processes represented in language models.</td>
      <td>## 🌟 论文解读 | 基于评估的情感链架构：让游戏中的语言模型代理更具情感<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的发展，构建可信、自然和交互式的数字人工代理成为了一个日益增长的研究领域。然而，模拟人类情感一直是这一领域的难题。大型语言模型（LLMs）可能通过利用情境评估中的常见模式来解决这些问题。本文研究了LLMs在解决情感智能任务和模拟情感方面的能力，并提出了一个新的基于情感链架构的游戏情感模拟方法。<br><br>## 🚀 核心方法<br>💡 创新点1：评估提示策略<br>本文提出了一种基于评估的情感链架构，该架构利用情境信息和角色特征来评估当前情况，并生成相应的情感反应。通过将评估过程与语言模型相结合，可以更准确地模拟人类情感。<br><br>💡 创新点2：情感链架构<br>该架构包括一个记忆系统，用于存储观察结果和情感反应，以及一个评估系统，用于将观察结果转换为情感反应。通过这种方式，可以创建一个情感链，用于生成代理的行为和对话。<br><br>## 📈 实验结果<br>实验结果表明，基于评估的情感链架构在用户体验和内容分析指标方面优于标准的LLM架构。用户研究表明，该架构在代理可信度、反应性和情感智力方面表现出色。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，LLMs在模拟情感方面具有巨大潜力。通过利用评估提示策略和情感链架构，可以构建更具情感的游戏代理，从而提供更丰富、更自然的用户体验。此外，该方法还可以应用于其他领域，例如虚拟助手和聊天机器人，以模拟更真实的情感反应。</td>
    </tr>
    <tr>
      <th>16</th>
      <td>PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model</td>
      <td>Poker, also known as Texas Hold'em, has always been a typical research target<br>within imperfect information games (IIGs). IIGs have long served as a measure<br>of artificial intelligence (AI) development. Representative prior works, such<br>as DeepStack and Libratus heavily rely on counterfactual regret minimization<br>(CFR) to tackle heads-up no-limit Poker. However, it is challenging for<br>subsequent researchers to learn CFR from previous models and apply it to other<br>real-world applications due to the expensive computational cost of CFR<br>iterations. Additionally, CFR is difficult to apply to multi-player games due<br>to the exponential growth of the game tree size. In this work, we introduce<br>PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number<br>of players and gaining high win rates, established on a lightweight large<br>language model (LLM). PokerGPT only requires simple textual information of<br>Poker games for generating decision-making advice, thus guaranteeing the<br>convenient interaction between AI and humans. We mainly transform a set of<br>textual records acquired from real games into prompts, and use them to<br>fine-tune a lightweight pre-trained LLM using reinforcement learning human<br>feedback technique. To improve fine-tuning performance, we conduct prompt<br>engineering on raw data, including filtering useful information, selecting<br>behaviors of players with high win rates, and further processing them into<br>textual instruction using multiple prompt engineering techniques. Through the<br>experiments, we demonstrate that PokerGPT outperforms previous approaches in<br>terms of win rate, model size, training time, and response speed, indicating<br>the great potential of LLMs in solving IIGs.</td>
      <td>## 🌟 论文解读 | PokerGPT：基于大型语言模型的轻量级多玩家德州扑克解决方案<br><br>## 📌 背景痛点/本文动机<br>德州扑克作为一种典型的非完美信息游戏（IIG），一直是人工智能研究的重要目标。然而，现有的解决方案，如DeepStack和Libratus，主要依赖于反事实后悔最小化（CFR）算法，该算法在计算成本和扩展性方面存在局限性。CFR算法的计算成本高昂，难以应用于多玩家游戏，且难以从现有模型中学习并应用于其他现实世界应用。<br><br>## 🚀 核心方法<br>本文提出了PokerGPT，一种基于轻量级大型语言模型（LLM）的端到端德州扑克解决方案。PokerGPT通过以下创新点克服了现有方法的局限性：<br><br>💡 创新点1：端到端学习框架<br>PokerGPT采用端到端学习框架，避免了复杂的特征工程和中间步骤。它仅需要简单的文本信息即可生成决策建议，实现了人机交互的便捷性。<br><br>💡 创新点2：轻量级LLM<br>PokerGPT基于轻量级LLM，具有更少的参数和更快的推理速度，同时训练时间也更短，实现了资源的有效利用。<br><br>💡 创新点3：高效的数据处理<br>PokerGPT采用数据清洗和提示工程技术，将真实游戏数据转换为可理解的文本提示，并使用强化学习人类反馈技术进行微调，提高了模型性能。<br><br>## 📈 实验结果<br>实验结果表明，PokerGPT在胜率、模型大小、训练时间和响应速度等方面均优于现有方法。此外，PokerGPT能够处理任意数量的玩家，并展现出出色的灵活性和适应性。<br><br>## 💬 可借鉴之处<br>PokerGPT的成功表明，LLM在解决IIG方面具有巨大潜力。其端到端学习框架、轻量级模型和高效的数据处理技术为其他IIG研究提供了可借鉴的经验。此外，PokerGPT的交互式特性使其在现实世界应用中具有广阔前景。</td>
    </tr>
    <tr>
      <th>102</th>
      <td>BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</td>
      <td>We present BEHAVIOR-1K, a comprehensive simulation benchmark for<br>human-centered robotics. BEHAVIOR-1K includes two components, guided and<br>motivated by the results of an extensive survey on "what do you want robots to<br>do for you?". The first is the definition of 1,000 everyday activities,<br>grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more<br>than 9,000 objects annotated with rich physical and semantic properties. The<br>second is OMNIGIBSON, a novel simulation environment that supports these<br>activities via realistic physics simulation and rendering of rigid bodies,<br>deformable bodies, and liquids. Our experiments indicate that the activities in<br>BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both<br>of which remain a challenge for even state-of-the-art robot learning solutions.<br>To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an<br>initial study on transferring solutions learned with a mobile manipulator in a<br>simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's<br>human-grounded nature, diversity, and realism make it valuable for embodied AI<br>and robot learning research. Project website: https://behavior.stanford.edu.</td>
      <td>## 🌟 论文解读 | BEHAVIOR-1K：基于人类日常活动与真实模拟的具身AI基准<br><br>## 📌 背景痛点/本文动机<br>随着人工智能和机器人技术的快速发展，人们对于机器人能够执行更多日常任务的需求日益增长。然而，现有的机器人基准测试往往由研究人员设计，缺乏对人类实际需求的考虑。为了解决这个问题，本文提出了BEHAVIOR-1K，一个基于人类日常活动与真实模拟的具身AI基准。<br><br>## 🚀 核心方法<br>💡 创新点1：基于人类需求的基准测试<br>BEHAVIOR-1K通过一项广泛的调查，收集了1461名参与者的意见，以确定人们希望机器人执行的最常见和最需要的1000个日常活动。这些活动涵盖了从清洁、烹饪到娱乐等多个领域，确保了基准测试的多样性和实用性。<br><br>💡 创新点2：真实模拟环境OMNIGIBSON<br>为了支持这些日常活动，本文开发了一个名为OMNIGIBSON的模拟环境。OMNIGIBSON基于Nvidia的Omniverse和PhysX 5，能够提供逼真的物理模拟和渲染，包括刚性体、可变形体和液体的模拟。此外，OMNIGIBSON还支持扩展的对象状态，如温度、湿度等，以及生成有效的初始活动配置和区分有效目标解决方案的功能。<br><br>## 📈 实验结果<br>本文评估了当前最先进的强化学习算法在BEHAVIOR-1K中的表现。结果表明，即使是单个活动也对当前AI算法构成了极大的挑战，并且需要大量的领域知识才能解决。此外，本文还进行了一项初步研究，将模拟环境中学习的解决方案转移到现实世界的机器人上，以评估模拟与现实之间的差距。<br><br>## 💬 可借鉴之处<br>BEHAVIOR-1K为具身AI和机器人学习研究提供了一个有价值的基准测试。其基于人类需求的多样性和真实模拟环境，为研究人员提供了一个平台，以开发能够执行更多日常任务的机器人。此外，本文的研究结果也为解决模拟与现实之间的差距提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>65</th>
      <td>Enhance Reasoning for Large Language Models in the Game Werewolf</td>
      <td>This paper presents an innovative framework that integrates Large Language<br>Models (LLMs) with an external Thinker module to enhance the reasoning<br>capabilities of LLM-based agents. Unlike augmenting LLMs with prompt<br>engineering, Thinker directly harnesses knowledge from databases and employs<br>various optimization techniques. The framework forms a reasoning hierarchy<br>where LLMs handle intuitive System-1 tasks such as natural language processing,<br>while the Thinker focuses on cognitive System-2 tasks that require complex<br>logical analysis and domain-specific knowledge. Our framework is presented<br>using a 9-player Werewolf game that demands dual-system reasoning. We introduce<br>a communication protocol between LLMs and the Thinker, and train the Thinker<br>using data from 18800 human sessions and reinforcement learning. Experiments<br>demonstrate the framework's effectiveness in deductive reasoning, speech<br>generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to<br>surpass GPT4 when integrated with the Thinker. This paper also contributes the<br>largest dataset for social deduction games to date.</td>
      <td>## 🌟 论文解读 | 大型语言模型推理能力提升：以狼人杀游戏为案例<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理（NLP）任务上的突破，其在推理、规划和决策等领域的潜力也逐渐显现。然而，LLMs在处理复杂推理任务时仍面临挑战，尤其是在需要领域特定知识和深度逻辑分析的任务中。本文旨在通过引入外部推理模块，即“思考者”（Thinker），来增强LLM的推理能力，使其在特定任务中表现更佳。<br><br>## 🚀 核心方法<br>💡 创新点1：双系统推理框架<br>本文提出了一个创新的框架，将LLMs与外部Thinker模块相结合，形成了一个推理层次结构。LLMs负责处理直观的System-1任务，如自然语言处理和常识推理，而Thinker则专注于需要复杂逻辑分析和领域特定知识的System-2任务。<br><br>💡 创新点2：Thinker模块的设计与训练<br>Thinker模块直接从数据库中获取知识，并采用各种优化技术进行训练。它通过模仿学习、强化学习和基于群体的训练等方法，学习生成合理的游戏动作和LLM的语音指令。<br><br>💡 创新点3：数据集贡献<br>本文收集了18,800场真实人类游戏会话数据，构建了迄今为止最大的社交推理游戏数据集，为研究提供了宝贵资源。<br><br>## 📈 实验结果<br>实验结果表明，引入Thinker模块显著提高了LLMs的推理和生成能力。在狼人杀游戏中，Thinker模块在推理、语音生成和在线游戏评估方面均表现出色。此外，通过将Thinker与一个较小的LLM模型（6B）进行微调，其性能甚至超过了GPT4。<br><br>## 💬 可借鉴之处<br>本文提出的框架和方法为LLMs在复杂推理任务中的应用提供了新的思路。通过将LLMs与外部推理模块相结合，可以有效提升LLMs在特定领域的推理能力，使其在更多实际应用中发挥更大的作用。此外，本文构建的大规模数据集也为社交推理游戏的研究提供了重要的数据基础。</td>
    </tr>
    <tr>
      <th>46</th>
      <td>ADaPT: As-Needed Decomposition and Planning with Language Models</td>
      <td>Large Language Models (LLMs) are increasingly being used for interactive<br>decision-making tasks requiring planning and adapting to the environment.<br>Recent works employ LLMs-as-agents in broadly two ways: iteratively determining<br>the next action (iterative executors) or generating plans and executing<br>sub-tasks using LLMs (plan-and-execute). However, these methods struggle with<br>task complexity, as the inability to execute any sub-task may lead to task<br>failure. To address these shortcomings, we introduce As-Needed Decomposition<br>and Planning for complex Tasks (ADaPT), an approach that explicitly plans and<br>decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute<br>them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity<br>and LLM capability. Our results demonstrate that ADaPT substantially<br>outperforms established strong baselines, achieving success rates up to 28.3%<br>higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel<br>compositional dataset that we introduce. Through extensive analysis, we<br>illustrate the importance of multilevel decomposition and establish that ADaPT<br>dynamically adjusts to the capabilities of the executor LLM as well as to task<br>complexity.</td>
      <td>## 🌟 论文解读 | ADaPT：按需分解与规划，提升大型语言模型在复杂任务中的表现<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们也逐渐被应用于需要规划和适应环境的交互式决策任务中。然而，现有的方法在处理复杂任务时面临着挑战，因为LLMs在执行子任务时可能会失败，从而导致整个任务的失败。<br><br>## 🚀 核心方法<br>为了解决上述问题，本文提出了ADaPT（As-Needed Decomposition and Planning for complex Tasks），一种按需分解和规划复杂任务的方法。ADaPT的核心思想是，当LLM作为执行者无法执行子任务时，将其分解为更小的子任务，并递归地进行分解，以适应任务的复杂性和LLM的能力。<br><br>### 💡 创新点1：按需分解<br>ADaPT通过递归地分解子任务，动态地适应任务的复杂性和LLM的能力。当LLM作为执行者无法执行子任务时，它会调用LLM作为规划者来生成更小的子任务，并递归地调用ADaPT来执行这些子任务。<br><br>### 💡 创新点2：多级分解<br>ADaPT支持多级分解，这意味着它可以进一步分解子任务，直到它们变得足够简单，可以被LLM作为执行者成功执行。这种多级分解的能力使得ADaPT能够处理更复杂的任务，并提高任务的成功率。<br><br>## 📈 实验结果<br>在ALFWorld、WebShop和TextCraft三个数据集上进行的实验结果表明，ADaPT显著优于现有的强基线方法，在ALFWorld上提高了28.3%的成功率，在WebShop上提高了27%，在TextCraft上提高了33%。<br><br>## 💬 可借鉴之处<br>ADaPT提供了一种有效的方法来处理LLMs在复杂任务中的执行失败问题。它通过按需分解和规划，动态地适应任务的复杂性和LLM的能力，从而提高了任务的成功率。此外，ADaPT的多级分解能力使其能够处理更复杂的任务，并提高任务的成功率。</td>
    </tr>
    <tr>
      <th>77</th>
      <td>Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game</td>
      <td>Multi-agent collaboration with Large Language Models (LLMs) demonstrates<br>proficiency in basic tasks, yet its efficiency in more complex scenarios<br>remains unexplored. In gaming environments, these agents often face situations<br>without established coordination protocols, requiring them to make intelligent<br>inferences about teammates from limited data. This problem motivates the area<br>of ad hoc teamwork, in which an agent may potentially cooperate with a variety<br>of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork<br>problem where the agent operates in an environment driven by natural language.<br>Our findings reveal the potential of LLM agents in team collaboration,<br>highlighting issues related to hallucinations in communication. To address this<br>issue, we develop CodeAct, a general agent that equips LLM with enhanced memory<br>and code-driven reasoning, enabling the repurposing of partial information for<br>rapid adaptation to new teammates.</td>
      <td>## 🌟 论文解读 | 探索大型语言模型在动态环境中的协作能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在推理和泛化能力上的不断提升，它们在构建自主代理和推动人工智能领域的发展方面展现出巨大潜力。然而，在多智能体协作中，特别是在没有预先设定的协调协议的动态环境中，LLMs的协作效率仍然是一个未充分探索的领域。本文旨在研究LLMs在动态环境中的协作能力，特别是在没有明确团队策略的情况下，如何与不同的队友进行有效合作。<br><br>## 🚀 核心方法<br>💡 创新点1：引入AvalonPlay基准<br>本文提出了AvalonPlay基准，这是一个基于自然语言的多智能体平台，用于模拟动态环境中的协作任务。在这个基准中，智能体需要在有限的信息和没有预先设定的团队策略的情况下，通过观察队友的行为来推断他们的角色，并动态调整团队策略以实现共同目标。<br><br>💡 创新点2：开发CodeAct智能体<br>为了解决LLMs在动态环境中协作时可能出现的记忆遗忘和幻觉生成等问题，本文提出了CodeAct智能体。CodeAct利用LLMs的代码驱动推理能力，通过将复杂的语义任务转化为灵活的代码结构，从而提高智能体在动态环境中的协作效率。<br><br>## 📈 实验结果<br>实验结果表明，GPT-4模型在AvalonPlay基准中表现出最佳的协作能力，而CodeAct智能体在团队选择准确性方面优于其他语义推理方法。此外，实验还发现，引入自然语言通信协议并不总是能显著提高LLMs的协作效率。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，LLMs在动态环境中的协作能力仍然面临一些挑战，如记忆遗忘和幻觉生成。为了提高LLMs的协作效率，可以借鉴本文提出的CodeAct智能体的设计思路，利用代码驱动推理和记忆检索系统来增强智能体的推理能力和信息处理能力。此外，还可以进一步研究如何减少幻觉生成的影响，并探索LLMs在现实世界场景中的应用。</td>
    </tr>
    <tr>
      <th>20</th>
      <td>A reinforcement learning approach to hybrid control design</td>
      <td>In this paper we design hybrid control policies for hybrid systems whose<br>mathematical models are unknown. Our contributions are threefold. First, we<br>propose a framework for modelling the hybrid control design problem as a single<br>Markov Decision Process (MDP). This result facilitates the application of<br>off-the-shelf algorithms from Reinforcement Learning (RL) literature towards<br>designing optimal control policies. Second, we model a set of benchmark<br>examples of hybrid control design problem in the proposed MDP framework. Third,<br>we adapt the recently proposed Proximal Policy Optimisation (PPO) algorithm for<br>the hybrid action space and apply it to the above set of problems. It is<br>observed that in each case the algorithm converges and finds the optimal<br>policy.</td>
      <td>## 🌟 论文解读 | 基于强化学习的混合控制系统设计新方法<br><br>## 📌 背景痛点/本文动机<br>混合动力系统在现实世界中广泛存在，如交通管理、化学过程控制、通信网络、嵌入式控制、发动机控制和机器人等。然而，设计能够确保系统良好性能的混合控制策略一直是一个挑战，尤其是在系统数学模型未知的情况下。本文旨在解决这一问题，提出了一种基于强化学习（RL）的混合控制策略设计方法。<br><br>## 🚀 核心方法<br>💡 创新点1：将混合控制设计问题建模为马尔可夫决策过程（MDP）<br>本文首先提出了一种框架，将混合控制设计问题建模为单个MDP。这使得可以直接应用RL文献中的现成算法来设计最优控制策略。<br><br>💡 创新点2：在MDP框架中建模基准混合控制设计问题<br>本文在提出的MDP框架中建模了一系列基准混合控制设计问题，包括四档汽车、钢退火过程、热水器等。<br><br>💡 创新点3：将PPO算法应用于混合动作空间<br>本文将最近提出的Proximal Policy Optimisation（PPO）算法进行了调整，以适应混合动作空间，并将其应用于上述问题。实验结果表明，在每种情况下，算法都能收敛并找到最优策略。<br><br>## 📈 实验结果<br>本文在多个基准问题上进行了实验，包括四档汽车、钢退火过程、热水器等。实验结果表明，PPO算法能够有效地找到最优混合控制策略，并在各种情况下都能收敛。<br><br>## 💬 可借鉴之处<br>本文提出的基于RL的混合控制策略设计方法具有以下可借鉴之处：<br><br>*   **模型无关性**：该方法不依赖于系统的数学模型，适用于模型未知或难以建模的混合系统。<br>*   **通用性**：该方法可以应用于各种类型的混合系统，包括具有不同“跳跃”行为的系统。<br>*   **有效性**：实验结果表明，该方法能够有效地找到最优混合控制策略，并在各种情况下都能收敛。<br><br>## 🌟 总结<br>本文提出了一种基于RL的混合控制策略设计方法，该方法具有模型无关性、通用性和有效性等优点。该方法为混合控制设计提供了一种新的思路，并为解决现实世界中的混合系统控制问题提供了新的工具。</td>
    </tr>
    <tr>
      <th>85</th>
      <td>Counting to Explore and Generalize in Text-based Games</td>
      <td>We propose a recurrent RL agent with an episodic exploration mechanism that<br>helps discovering good policies in text-based game environments. We show<br>promising results on a set of generated text-based games of varying difficulty<br>where the goal is to collect a coin located at the end of a chain of rooms. In<br>contrast to previous text-based RL approaches, we observe that our agent learns<br>policies that generalize to unseen games of greater difficulty.</td>
      <td>## 🌟 论文解读 | 基于计数探索和泛化的文本游戏强化学习<br><br>## 📌 背景痛点/本文动机<br>文本游戏是一种复杂的交互式模拟，使用自然语言描述游戏状态、接受玩家动作并报告环境变化。玩家必须通过探索来发现游戏目标，而游戏中的观察和动作空间都是组合和复合的，玩家必须应对部分可观察性，因为描述性文本并不提供关于底层游戏状态的完整、明确的信息。<br><br>## 🚀 核心方法<br>💡 创新点1：提出了一种基于循环的强化学习代理，具有情节探索机制，有助于在基于文本的游戏环境中发现良好的策略。<br>💡 创新点2：提出了一种情节计数探索方案，其中状态计数在每个情节开始时重置。这种奖励充当情节记忆，推动代理访问当前情节中尚未遇到的状态。<br><br>## 📈 实验结果<br>在一系列生成的基于文本的游戏中，该代理在收集位于一系列房间末尾的硬币的目标上取得了有希望的结果。与之前的基于文本的强化学习方法相比，观察到该代理学习的策略可以泛化到未见的更具挑战性的游戏。<br><br>## 💬 可借鉴之处<br>该论文提出的基于计数探索和泛化的文本游戏强化学习方法，为解决文本游戏中的探索和泛化问题提供了一种新的思路。该方法可以应用于其他需要探索和记忆能力的强化学习任务，例如迷宫探索、路径规划等。</td>
    </tr>
    <tr>
      <th>21</th>
      <td>FPGA-extended General Purpose Computer Architecture</td>
      <td>This paper introduces a computer architecture, where part of the instruction<br>set architecture (ISA) is implemented on small highly-integrated<br>field-programmable gate arrays (FPGAs). Small FPGAs inside a general-purpose<br>processor (CPU) can be used effectively to implement custom or standardised<br>instructions. Our proposed architecture directly address related challenges for<br>high-end CPUs, where such highly-integrated FPGAs would have the highest<br>impact, such as on main memory bandwidth. This also enables<br>software-transparent context-switching. The simulation-based evaluation of a<br>dynamically reconfigurable core shows promising results approaching the<br>performance of an equivalent core with all enabled instructions. Finally, the<br>feasibility of adopting the proposed architecture in today's CPUs is studied<br>through the prototyping of fast-reconfigurable FPGAs and studying the miss<br>behaviour of opcodes.</td>
      <td>## 🌟 论文解读 | FPGA扩展通用计算机架构：提升性能的新思路<br><br>## 📌 背景痛点/本文动机<br>随着计算需求的日益增长，通用处理器（CPU）在性能上逐渐难以满足特定应用的需求。尽管专用处理器如GPU、FPGA和ASIC等在特定任务上表现出色，但它们通常需要复杂的编程模型和昂贵的部署成本。此外，现有的通用处理器架构在扩展指令集时面临着硬件复杂性和功耗效率的挑战。<br><br>## 🚀 核心方法<br>本文提出了一种名为“FPGA扩展修改哈佛架构”的新型计算机架构，旨在通过在通用处理器内部集成小型高度集成的FPGA来提升性能。主要创新点包括：<br><br>💡 创新点1：将部分指令集架构（ISA）实现于FPGA上，使得CPU能够有效地执行自定义或标准化的指令。<br>💡 创新点2：引入指令消歧器单元，用于处理指令解码过程中的请求，并根据指令操作码查找相应的FPGA指令实现。<br>💡 创新点3：设计了一个独立的位流缓存，用于存储FPGA指令的位流，从而提高可重构核心的性能。<br>💡 创新点4：通过模拟评估，验证了动态可重构核心的性能，并研究了在当前CPU中采用该架构的可行性。<br><br>## 📈 实验结果<br>实验结果表明，动态可重构核心的性能接近于具有所有启用指令的等效核心。此外，通过原型设计和研究操作码的缺失行为，证明了在当前CPU中采用该架构的可行性。<br><br>## 💬 可借鉴之处<br>本文提出的FPGA扩展通用计算机架构为提升CPU性能提供了一种新的思路。通过将部分指令集实现于FPGA上，可以有效地执行自定义或标准化的指令，从而提高性能。此外，该架构还支持软件透明的上下文切换，使得操作系统可以提供ISA扩展，而硬件则根据需求动态地获取相应的位流。这些创新点为未来CPU架构的设计提供了重要的参考价值。</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4</td>
      <td>Unlike perfect information games, where all elements are known to every<br>player, imperfect information games emulate the real-world complexities of<br>decision-making under uncertain or incomplete information. GPT-4, the recent<br>breakthrough in large language models (LLMs) trained on massive passive data,<br>is notable for its knowledge retrieval and reasoning abilities. This paper<br>delves into the applicability of GPT-4's learned knowledge for imperfect<br>information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an<br>innovative agent that leverages GPT-4's capabilities for performing in<br>imperfect information games. With proper prompt engineering to achieve<br>different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable<br>adaptability across a range of imperfect information card games. Importantly,<br>GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it<br>can understand others and intentionally impact others' behavior. Leveraging<br>this, we design a planning strategy that enables GPT-4 to competently play<br>against different opponents, adapting its gameplay style as needed, while<br>requiring only the game rules and descriptions of observations as input. In the<br>experiments, we qualitatively showcase the capabilities of Suspicion-Agent<br>across three different imperfect information games and then quantitatively<br>evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can<br>potentially outperform traditional algorithms designed for imperfect<br>information games, without any specialized training or examples. In order to<br>encourage and foster deeper insights within the community, we make our<br>game-related data publicly available.</td>
      <td>## 🌟 论文解读 | 利用GPT-4的“心智理论”能力玩不完美信息游戏<br><br>## 📌 背景痛点/本文动机<br>在现实世界中，决策往往是在信息不完整或不确定的情况下进行的。然而，大多数现有的AI算法都是在完美信息游戏中训练的，即所有玩家都能看到所有信息。这限制了它们在现实世界中的应用。本文旨在探索如何利用大型语言模型（LLM）的知识和推理能力来处理不完美信息游戏，从而更好地模拟现实世界的决策过程。<br><br>## 🚀 核心方法<br>本文提出了一个名为Suspicion-Agent的创新型自主代理，它基于GPT-4，并利用其强大的知识检索和推理能力来玩不完美信息游戏。Suspicion-Agent的核心创新点包括：<br><br>💡 创新点1：利用GPT-4的“心智理论”（ToM）能力，即理解他人并有意影响他人行为的能力。这使得Suspicion-Agent能够预测对手的行为，并根据对手的行为调整自己的策略。<br><br>💡 创新点2：将游戏过程分解为多个子模块，如观察解释器、游戏模式分析和规划模块。每个模块都使用不同的提示来引导GPT-4执行特定的功能，从而实现更有效的决策。<br><br>## 📈 实验结果<br>在实验中，Suspicion-Agent在三个不同的不完美信息游戏中展示了其能力，并在Leduc Hold'em游戏中进行了定量评估。结果表明，Suspicion-Agent可以潜在地超越传统算法，而无需任何专门的训练或示例。<br><br>## 💬 可借鉴之处<br>本文提出的Suspicion-Agent框架为利用LLM在不完美信息游戏中进行决策提供了一个新的思路。其核心思想是将LLM的知识和推理能力与ToM能力相结合，从而实现更有效的决策。此外，本文还公开了所有与游戏相关的数据，这将有助于研究人员更好地理解LLM的能力，并开发更有效的模型。</td>
    </tr>
    <tr>
      <th>96</th>
      <td>Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration</td>
      <td>In this paper, we introduce Watch-And-Help (WAH), a challenge for testing<br>social intelligence in agents. In WAH, an AI agent needs to help a human-like<br>agent perform a complex household task efficiently. To succeed, the AI agent<br>needs to i) understand the underlying goal of the task by watching a single<br>demonstration of the human-like agent performing the same task (social<br>perception), and ii) coordinate with the human-like agent to solve the task in<br>an unseen environment as fast as possible (human-AI collaboration). For this<br>challenge, we build VirtualHome-Social, a multi-agent household environment,<br>and provide a benchmark including both planning and learning based baselines.<br>We evaluate the performance of AI agents with the human-like agent as well as<br>with real humans using objective metrics and subjective user ratings.<br>Experimental results demonstrate that the proposed challenge and virtual<br>environment enable a systematic evaluation on the important aspects of machine<br>social intelligence at scale.</td>
      <td>## 🌟 论文解读 | Watch-And-Help：社会感知与人类-AI协作的挑战<br><br>## 📌 背景痛点/本文动机<br>人类在很小的年纪就展现出利他行为，能够通过观察他人的行为来理解其目标，并制定计划来帮助他们，即使在新的场景中也能做到。然而，目前最先进的AI系统仍然难以掌握这些基本的社会技能。为了实现有效帮助人类所需的社会智能水平，AI代理需要具备两个关键能力：社会感知（理解人类行为的能力）和协作规划（推理物理环境并规划其行动以与人类协调的能力）。<br><br>## 🚀 核心方法<br>本文提出了一个新的AI挑战：Watch-And-Help (WAH)，重点关注社会感知和人类-AI协作。在WAH挑战中，AI代理需要与一个类似人类的代理协作，以更有效地完成复杂的家庭任务。该挑战分为两个阶段：<br>1. **观察阶段**：AI代理（Bob）观察类似人类的代理（Alice）执行任务，并从她的行为中推断出她的目标。<br>2. **帮助阶段**：Bob与Alice协作，在新环境中尽可能快地完成相同的任务。<br><br>为了实现多代理交互，本文扩展了开源虚拟平台VirtualHome，并构建了一个多代理虚拟环境VirtualHome-Social。该环境模拟了真实且丰富的家庭环境，代理可以与不同的对象（例如，打开容器或抓取对象）和其他代理（例如，跟随、帮助、避免碰撞）进行交互，以执行复杂的任务。VirtualHome-Social还提供了内置代理，模拟人类行为，允许AI代理与虚拟人类一起进行训练和测试，并提供了一个接口，允许使用真实人类进行评估，并收集/显示真实环境中的人类活动（这是现有多代理平台不提供的功能）。<br><br>## 📈 实验结果<br>实验结果表明，为了在WAH挑战中取得成功，AI代理必须具备强大的社会感知能力和可推广的帮助策略。这些机器社会智能的基本方面已被证明是先前工作中人类-AI协作的关键。本文提出的挑战和虚拟环境能够系统地评估机器社会智能的重要方面。<br><br>## 💬 可借鉴之处<br>1. **WAH挑战**：为评估AI代理的社会感知能力和协作能力提供了一个新的框架。<br>2. **VirtualHome-Social环境**：为AI代理执行复杂的家庭任务提供了一个多代理平台，并支持与内置代理或真实人类进行交互。<br>3. **基准测试**：包括多个基于规划和学习的基线方法，突出了机器社会智能的重要方面。<br><br>## 🌟 未来展望<br>本文提出的挑战和虚拟环境为构建更复杂的机器社会智能开辟了令人兴奋的方向，例如在线目标推理和代理之间的直接通信。希望WAH挑战和VirtualHome-Social环境能够促进未来对构建更复杂机器社会智能的研究。</td>
    </tr>
    <tr>
      <th>111</th>
      <td>Enhancing Agent Learning through World Dynamics Modeling</td>
      <td>Large language models (LLMs) have been increasingly applied to tasks in<br>language understanding and interactive decision-making, with their impressive<br>performance largely attributed to the extensive domain knowledge embedded<br>within them. However, the depth and breadth of this knowledge can vary across<br>domains. Many existing approaches assume that LLMs possess a comprehensive<br>understanding of their environment, often overlooking potential gaps in their<br>grasp of actual world dynamics. To address this, we introduce Discover, Verify,<br>and Evolve (DiVE), a framework that discovers world dynamics from a small<br>number of demonstrations, verifies the accuracy of these dynamics, and evolves<br>new, advanced dynamics tailored to the current situation. Through extensive<br>evaluations, we assess the impact of each component on performance and compare<br>the dynamics generated by DiVE to human-annotated dynamics. Our results show<br>that LLMs guided by DiVE make more informed decisions, achieving rewards<br>comparable to human players in the Crafter environment and surpassing methods<br>that require prior task-specific training in the MiniHack environment.</td>
      <td>## 🌟 论文解读 | 通过世界动态建模增强智能体学习<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在语言理解和交互式决策任务中表现出色，这主要归功于它们嵌入的广泛领域知识。然而，这种知识的深度和广度在不同领域之间可能存在差异。许多现有方法假设LLMs对其环境有全面的理解，但往往忽视了它们对实际世界动态的掌握可能存在差距。为了解决这个问题，本文提出了Discover, Verify, and Evolve (DiVE)框架，该框架从少量演示中发现世界动态，验证这些动态的准确性，并根据当前情况演化新的、先进的动态。<br><br>## 🚀 核心方法<br>💡 创新点1：DiVE框架<br>DiVE框架由三个主要组件组成：<br>- Discoverer：使用课程学习方法从演示中迭代地发现环境动态。<br>- Verifier：消除LLMs由于幻觉倾向而导致的不可靠动态。<br>- Evolver：根据学习的动态，推理出针对当前情况的深入、特定于状态的知识。<br><br>💡 创新点2：层次课程学习<br>DiVE采用层次课程学习方法，从简单到复杂的动态逐步学习，从而更有效地学习。具体来说，它从任务分解层次结构中的元素（如动作、对象、子任务和子目标）开始，逐步学习它们的动态。<br><br>💡 创新点3：动态验证<br>为了确保动态的准确性，DiVE引入了动态验证器，它可以过滤掉可能无效和冲突的动态候选者，从而提高决策过程的可靠性。<br><br>💡 创新点4：在线策略学习<br>DiVE不仅学习基本规则，还专注于根据这些动态开发高级游戏策略。它通过在线学习方法将动态演化为策略，从而生成更符合当前游戏场景的策略。<br><br>## 📈 实验结果<br>在Crafter和MiniHack环境中进行的实验表明，DiVE在性能方面优于所有其他基线模型。在Crafter环境中，DiVE在分数和奖励方面分别比SOTA LLM方法SPRING提高了337.8%和110.1%，并且超过了SOTA RL方法DreamerV3。在MiniHack环境中，DiVE在Lava Crossing任务上与SSO和Reflexion（都需要30次迭代训练）的性能相当，并且在Wand of Death和Quest任务上超过了这两个基线。<br><br>## 💬 可借鉴之处<br>DiVE框架为解决LLMs在特定领域中的知识差距问题提供了一种有效的方法。它通过发现、验证和演化世界动态，提高了LLMs的决策能力。此外，DiVE的层次课程学习和动态验证方法可以应用于其他需要长期规划和决策的任务中。</td>
    </tr>
    <tr>
      <th>5</th>
      <td>PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models</td>
      <td>We introduce PokeLLMon, the first LLM-embodied agent that achieves<br>human-parity performance in tactical battle games, as demonstrated in Pokemon<br>battles. The design of PokeLLMon incorporates three key strategies: (i)<br>In-context reinforcement learning that instantly consumes text-based feedback<br>derived from battles to iteratively refine the policy; (ii) Knowledge-augmented<br>generation that retrieves external knowledge to counteract hallucination and<br>enables the agent to act timely and properly; (iii) Consistent action<br>generation to mitigate the panic switching phenomenon when the agent faces a<br>powerful opponent and wants to elude the battle. We show that online battles<br>against human demonstrates PokeLLMon's human-like battle strategies and<br>just-in-time decision making, achieving 49% of win rate in the Ladder<br>competitions and 56% of win rate in the invited battles. Our implementation and<br>playable battle logs are available at: https://github.com/git-disl/PokeLLMon.</td>
      <td>## 🌟 论文解读 | PokeLLMon：基于大型语言模型实现人类水平的宝可梦战斗AI<br><br>## 📌 背景痛点/本文动机<br>随着生成式AI和大型语言模型（LLMs）在自然语言处理（NLP）任务上的成功，人们开始探索LLMs如何自主地在物理世界中行动，将生成空间从文本扩展到行动，这被认为是追求通用人工智能（AGI）的关键范式。游戏是开发LLM-based代理与虚拟环境交互的合适测试平台。战术战斗游戏，如宝可梦战斗，因其状态和动作空间离散、回合制格式、战略性和复杂性，成为评估LLMs游戏能力的理想基准。<br><br>## 🚀 核心方法<br>💡 创新点1：上下文强化学习（ICRL）<br>为了解决LLMs在宝可梦战斗中出现的幻觉问题，论文提出了ICRL策略。ICRL利用战斗中即时生成的文本反馈作为“奖励”，在无需训练的情况下迭代优化动作生成策略。通过分析前一轮的行动和相应的文本反馈，代理能够不断调整其策略，从而更好地应对战斗中的变化。<br><br>💡 创新点2：知识增强生成（KAG）<br>为了进一步减少幻觉，论文引入了KAG策略。KAG通过检索外部知识，如类型优势/劣势关系和技能/能力效果，来增强生成过程。这些知识来源于宝可梦游戏中的宝可梦图鉴（Pokédex），它提供了关于宝可梦类型、技能和能力的详细信息。通过将外部知识添加到状态描述中，代理能够更准确地理解战斗情况，并做出更明智的决策。<br><br>💡 创新点3：一致性行动生成<br>为了解决代理在面对强大对手时出现的恐慌切换现象，论文提出了一致性行动生成策略。该策略通过多次独立生成行动并投票选出最一致的行动，来减少行动的不一致性。这种方法有助于代理在面对压力时保持冷静，避免过度思考和恐慌，从而做出更稳定的决策。<br><br>## 📈 实验结果<br>在线战斗结果表明，PokeLLMon在梯子比赛中取得了49%的胜率，在邀请比赛中取得了56%的胜率，展现出与人类玩家相当的比赛能力和策略。然而，PokeLLMon在面对人类玩家的消耗策略和欺骗技巧时也存在弱点，这表明未来需要进一步改进其长期规划和对手行为预测能力。<br><br>## 💬 可借鉴之处<br>PokeLLMon的设计和实现为LLMs在游戏领域的应用提供了新的思路。ICRL、KAG和一致性行动生成策略可以应用于其他游戏，帮助LLMs更好地理解和应对游戏中的挑战。此外，PokeLLMon的实验结果也揭示了LLMs在游戏中的优势和局限性，为未来研究和开发提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>44</th>
      <td>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</td>
      <td>The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by<br>Large Language Models (LLMs) make them promising candidates for developing<br>coordination agents. In this study, we introduce a new LLM-Coordination<br>Benchmark aimed at a detailed analysis of LLMs within the context of Pure<br>Coordination Games, where participating agents need to cooperate for the most<br>gain. This benchmark evaluates LLMs through two distinct tasks: (1)<br>\emph{Agentic Coordination}, where LLMs act as proactive participants for<br>cooperation in 4 pure coordination games; (2) \emph{Coordination Question<br>Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice<br>questions from the 4 games for evaluation of three key reasoning abilities:<br>Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to<br>enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture<br>for Coordination (CAC) framework that can easily integrate different LLMs as<br>plug-and-play modules for pure coordination games. Our findings indicate that<br>LLM agents equipped with GPT-4-turbo achieve comparable performance to<br>state-of-the-art reinforcement learning methods in games that require<br>commonsense actions based on the environment. Besides, zero-shot coordination<br>experiments reveal that, unlike RL methods, LLM agents are robust to new unseen<br>partners. However, results on Coordination QA show a large room for improvement<br>in the Theory of Mind reasoning and joint planning abilities of LLMs. The<br>analysis also sheds light on how the ability of LLMs to understand their<br>environment and their partner's beliefs and intentions plays a part in their<br>ability to plan for coordination. Our code is available at<br>\url{https://github.com/eric-ai-lab/llm_coordination}.</td>
      <td>## 🌟 论文解读 | LLM-Coordination：评估和分析大型语言模型的多智能体协调能力<br><br>## 📌 背景痛点/本文动机<br>在许多日常任务和关键操作中，如烹饪和救援行动，合作是至关重要的。这些场景可以被视为纯协调游戏，其中所有参与方都从选择完全一致的战略中受益，避免任何利益冲突。这些游戏要求代理推理他们的环境并计划，同时考虑他们的伙伴的信念和意图。大型语言模型（LLMs）最近在物理和虚拟环境中的涌现规划能力、令人印象深刻的推理能力和对心智理论的暗示，使它们成为开发协调代理的有希望的候选者。然而，LLMs在协调游戏中的必要条件、优势和局限性仍然不清楚。本文旨在通过进行LLMs的多智能体协调能力的全面评估和分析来弥合这一差距。<br><br>## 🚀 核心方法<br>本文提出了一个新的LLM-Coordination基准，旨在对LLMs在纯协调游戏中的能力进行详细分析。该基准通过两个不同的任务评估LLMs：<br>1. **代理协调**：LLMs作为积极合作参与者参与4个纯协调游戏。<br>2. **协调问答（QA）**：LLMs被提示回答来自4个游戏的198个多项选择题，以评估三个关键推理能力：环境理解、心智理论推理和联合规划。<br><br>此外，为了使LLMs能够进行多智能体协调，本文引入了一个协调认知架构（CAC）框架，该框架可以轻松地将不同的LLMs作为即插即用模块集成到纯协调游戏中。<br><br>## 📈 实验结果<br>实验结果表明，配备GPT-4-turbo的LLM代理在需要基于环境的常识行动的游戏中，其性能与最先进的强化学习方法相当。此外，零样本协调实验表明，与RL方法不同，LLM代理对新未见伙伴具有鲁棒性。然而，协调QA的结果表明，LLMs的心智理论推理和联合规划能力还有很大的改进空间。分析还揭示了LLMs理解其环境和其伙伴的信念和意图的能力如何影响它们协调计划的能力。<br><br>## 💬 可借鉴之处<br>本文提出的LLM-Coordination基准和CAC框架为评估和分析LLMs的多智能体协调能力提供了一个有价值的工具。此外，本文的结果突出了LLMs在协调任务中的优势和局限性，并为未来研究提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>52</th>
      <td>On the Utility of Learning about Humans for Human-AI Coordination</td>
      <td>While we would like agents that can coordinate with humans, current<br>algorithms such as self-play and population-based training create agents that<br>can coordinate with themselves. Agents that assume their partner to be optimal<br>or similar to them can converge to coordination protocols that fail to<br>understand and be understood by humans. To demonstrate this, we introduce a<br>simple environment that requires challenging coordination, based on the popular<br>game Overcooked, and learn a simple model that mimics human play. We evaluate<br>the performance of agents trained via self-play and population-based training.<br>These agents perform very well when paired with themselves, but when paired<br>with our human model, they are significantly worse than agents designed to play<br>with the human model. An experiment with a planning algorithm yields the same<br>conclusion, though only when the human-aware planner is given the exact human<br>model that it is playing with. A user study with real humans shows this pattern<br>as well, though less strongly. Qualitatively, we find that the gains come from<br>having the agent adapt to the human's gameplay. Given this result, we suggest<br>several approaches for designing agents that learn about humans in order to<br>better coordinate with them. Code is available at<br>https://github.com/HumanCompatibleAI/overcooked_ai.</td>
      <td>## 🌟 论文解读 | 人工智能与人类协作：学习人类行为的重要性<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，我们希望AI能够与人类进行有效的协作，共同完成任务。然而，目前许多训练AI的方法，如自我博弈和基于群体的训练，往往导致AI只能与自己协作，而无法理解或被人类理解。本文旨在探讨在训练过程中学习人类行为的重要性，以实现更有效的AI与人类协作。<br><br>## 🚀 核心方法<br>💡 创新点1：构建基于游戏Overcooked的协作环境<br>为了验证学习人类行为的重要性，本文构建了一个基于游戏Overcooked的协作环境，该环境要求玩家进行复杂的协作才能完成任务。通过这个环境，研究人员可以评估不同训练方法的AI与人类模型的协作效果。<br><br>💡 创新点2：比较不同训练方法的AI与人类模型的协作效果<br>本文比较了以下几种训练方法的AI与人类模型的协作效果：<br>- 自我博弈（Self-Play）：AI与自身进行博弈，学习如何与自己协作。<br>- 基于群体的训练（Population-Based Training, PBT）：AI与群体中的其他AI进行博弈，学习如何与群体中的其他AI协作。<br>- 耦合规划（Coupled Planning）：AI与人类模型进行耦合规划，学习如何与人类模型协作。<br>- 行为克隆（Behavior Cloning）：AI通过模仿人类的行为来学习如何与人类协作。<br><br>## 📈 实验结果<br>实验结果表明，未利用人类数据训练的AI在与人类模型协作时表现较差，而利用人类数据训练的AI则表现更好。具体来说，与自我博弈和基于群体的训练相比，利用行为克隆训练的AI在与人类模型协作时表现更佳。此外，利用规划或强化学习来最大化协作奖励的AI也表现出更好的协作效果。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，在训练AI进行协作时，考虑人类行为的重要性。为了实现更有效的AI与人类协作，可以采取以下几种方法：<br>- 使用行为克隆或其他模仿学习方法来学习人类的行为。<br>- 利用规划或强化学习来最大化协作奖励。<br>- 设计更复杂的AI模型，使其能够更好地理解人类的行为和意图。<br>- 在测试时让AI适应人类的行为，例如使用元学习算法来快速适应新的协作伙伴。<br><br>## 📚 参考文献<br>[1] Ng, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning (pp. 663-670).<br><br>[2] Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey. Journal of artificial intelligence research, 4, 237-285.<br><br>[3] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484-489.<br><br>[4] Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... & Horgan, D. (2017). StarCraft II: A New Challenge for Reinforcement Learning. arXiv preprint arXiv:1708.04782.<br><br>[5] Tampuu, A., Matiisen, T., Kuzovkin, I., Arjakov, D., Maini, J., & Rucklidge, W. J. (2017). Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1706.02275.<br><br>[6] Wang, Z., Schaul, T., Hessel, M., Hasselt, H. V., Lanctot, M., & Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581.<br><br>[7] Choudhury, R., Dragan, A., & Seshia, S. A. (2018). Learning human models from demonstration. arXiv preprint arXiv:1804.04287.<br><br>[8] Lerer, A., & Peysakhovich, A. (2018). Learning to play against opponents with unknown strategies. arXiv preprint arXiv:1805.09358.<br><br>[9] Wang, T., & Tamar, A. (2018). Learning to communicate with deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.01262.<br><br>[10] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018). Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1802.09416.<br><br>[11] Finn, C., Abbeel, P., & Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1126-1135).<br><br>[12] Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Imagineer: A general framework for imagining and planning. arXiv preprint arXiv:1806.05696.<br><br>[13] Overcooked. (2016). Ghost Town Games.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Welfare Diplomacy: Benchmarking Language Model Cooperation</td>
      <td>The growing capabilities and increasingly widespread deployment of AI systems<br>necessitate robust benchmarks for measuring their cooperative capabilities.<br>Unfortunately, most multi-agent benchmarks are either zero-sum or purely<br>cooperative, providing limited opportunities for such measurements. We<br>introduce a general-sum variant of the zero-sum board game Diplomacy -- called<br>Welfare Diplomacy -- in which players must balance investing in military<br>conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both<br>a clearer assessment of and stronger training incentives for cooperative<br>capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules<br>and implementing them via an open-source Diplomacy engine; (2) constructing<br>baseline agents using zero-shot prompted language models; and (3) conducting<br>experiments where we find that baselines using state-of-the-art models attain<br>high social welfare but are exploitable. Our work aims to promote societal<br>safety by aiding researchers in developing and assessing multi-agent AI<br>systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is<br>available at https://github.com/mukobi/welfare-diplomacy.</td>
      <td>## 🌟 论文解读 | 探索AI合作能力：Welfare Diplomacy基准测试<br><br>## 📌 背景痛点/本文动机<br>随着AI系统能力的不断增强和应用的日益广泛，衡量其合作能力的需求也日益增长。然而，现有的多智能体基准测试要么是零和博弈，要么是纯粹的合作博弈，这限制了对其合作能力的评估。本文提出了一个名为Welfare Diplomacy的基准测试，旨在解决这一问题。<br><br>## 🚀 核心方法<br>💡 创新点1：Welfare Diplomacy规则<br>Welfare Diplomacy是对经典外交游戏（Diplomacy）的改进版本，玩家需要在军事征服和国内福利之间进行权衡。游戏结束后，玩家的总效用等于其累积的福利点数，而不是占领的供应中心数量。这种规则设计鼓励玩家进行合作，以实现更高的社会福祉。<br><br>💡 创新点2：零样本语言模型基准测试<br>本文使用零样本提示语言模型构建了Welfare Diplomacy的基线智能体，并使用GPT-4等最先进的模型进行了基准测试。实验结果表明，这些模型能够实现高社会福祉，但容易被其他玩家利用。<br><br>## 📈 实验结果<br>实验结果表明，Welfare Diplomacy能够有效地评估和训练AI系统的合作能力。与标准外交游戏相比，Welfare Diplomacy中的玩家参与冲突的频率更低，这表明Welfare Diplomacy能够更好地促进合作行为。<br><br>## 💬 可借鉴之处<br>本文提出的Welfare Diplomacy基准测试为评估和训练AI系统的合作能力提供了一个新的平台。该基准测试可以用于开发更安全、更可靠的AI系统，以应对现实世界中的复杂挑战。<br><br>## 🌟 总结<br>Welfare Diplomacy是一个很有前景的基准测试，可以帮助研究人员更好地理解和评估AI系统的合作能力。随着AI技术的不断发展，Welfare Diplomacy有望在促进AI合作能力方面发挥重要作用。</td>
    </tr>
    <tr>
      <th>84</th>
      <td>TextWorld: A Learning Environment for Text-based Games</td>
      <td>We introduce TextWorld, a sandbox learning environment for the training and<br>evaluation of RL agents on text-based games. TextWorld is a Python library that<br>handles interactive play-through of text games, as well as backend functions<br>like state tracking and reward assignment. It comes with a curated list of<br>games whose features and challenges we have analyzed. More significantly, it<br>enables users to handcraft or automatically generate new games. Its generative<br>mechanisms give precise control over the difficulty, scope, and language of<br>constructed games, and can be used to relax challenges inherent to commercial<br>text games like partial observability and sparse rewards. By generating sets of<br>varied but similar games, TextWorld can also be used to study generalization<br>and transfer learning. We cast text-based games in the Reinforcement Learning<br>formalism, use our framework to develop a set of benchmark games, and evaluate<br>several baseline agents on this set and the curated list.</td>
      <td>## 🌟 论文解读 | TextWorld：基于文本游戏的强化学习环境<br><br>## 📌 背景痛点/本文动机<br>文本游戏是一种复杂的交互式模拟，玩家通过输入文本命令来探索游戏世界并达成目标。这类游戏对语言理解、长期记忆、规划、探索和常识推理等方面提出了挑战。然而，现有的强化学习算法在处理文本游戏时面临着诸多困难，例如部分可观测性、稀疏奖励、巨大的状态空间和动作空间等。<br><br>## 🚀 核心方法<br>TextWorld 是一个用于训练和评估强化学习代理在文本游戏上的学习环境。它具有以下创新点：<br><br>💡 创新点1：生成机制<br>TextWorld 具有强大的生成机制，可以自动构建游戏世界、填充对象和障碍物，并生成定义目标状态和如何达到目标的任务。这使得研究人员可以控制游戏的难度、范围和语言，并生成具有不同特征和挑战的游戏集合，用于研究泛化和迁移学习。<br><br>💡 创新点2：逻辑引擎<br>TextWorld 使用线性逻辑和推理引擎来定义游戏规则和状态转换函数。这使得游戏状态具有明确的表示，并可以精确地跟踪状态和分配奖励。此外，TextWorld 还可以提供中间奖励，帮助代理学习更有效的策略。<br><br>💡 创新点3：文本生成<br>TextWorld 使用上下文无关文法 (CFG) 来生成游戏状态的自然语言描述。这使得游戏世界和任务对人类可解释，并且可以控制文本的复杂性和多样性。<br><br>## 📈 实验结果<br>TextWorld 提供了一系列基准游戏，用于评估强化学习代理的性能。实验结果表明，TextWorld 可以有效地训练代理解决文本游戏中的各种挑战，并展现出良好的泛化能力。<br><br>## 💬 可借鉴之处<br>TextWorld 为研究文本游戏中的强化学习提供了一个强大的工具。其生成机制、逻辑引擎和文本生成模块可以用于构建各种复杂的游戏环境，并帮助代理学习语言理解、规划和探索等技能。此外，TextWorld 还可以用于研究泛化和迁移学习，以及开发新的强化学习算法。</td>
    </tr>
    <tr>
      <th>71</th>
      <td>Lyfe Agents: Generative agents for low-cost real-time social interactions</td>
      <td>Highly autonomous generative agents powered by large language models promise<br>to simulate intricate social behaviors in virtual societies. However, achieving<br>real-time interactions with humans at a low computational cost remains<br>challenging. Here, we introduce Lyfe Agents. They combine low-cost with<br>real-time responsiveness, all while remaining intelligent and goal-oriented.<br>Key innovations include: (1) an option-action framework, reducing the cost of<br>high-level decisions; (2) asynchronous self-monitoring for better<br>self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing<br>critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation<br>and sociability across several multi-agent scenarios in our custom LyfeGame 3D<br>virtual environment platform. When equipped with our brain-inspired techniques,<br>Lyfe Agents can exhibit human-like self-motivated social reasoning. For<br>example, the agents can solve a crime (a murder mystery) through autonomous<br>collaboration and information exchange. Meanwhile, our techniques enabled Lyfe<br>Agents to operate at a computational cost 10-100 times lower than existing<br>alternatives. Our findings underscore the transformative potential of<br>autonomous generative agents to enrich human social experiences in virtual<br>worlds.</td>
      <td>## 🌟 论文解读 | 低成本实时社交互动的生成式智能体：Lyfe Agents<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在模拟人类行为方面的潜力日益显现，生成式智能体在虚拟社会中模拟复杂社交行为的前景也变得光明。然而，实现与人类在低计算成本下的实时互动仍然是一个挑战。本文旨在创建一种既智能又自主的生成式智能体，能够在低计算成本下实现与人类的实时互动。<br><br>## 🚀 核心方法<br>💡 创新点1：选项-动作框架<br>为了减少高级决策的成本，Lyfe Agents 采用了一种选项-动作框架。在这种框架中，智能体首先选择一个高级动作（或“选项”），然后在后续步骤中在该选项内选择低级动作。这种设计允许智能体在更长时间内专注于执行选项背后的意图，从而降低成本并提高效率。<br><br>💡 创新点2：异步自我监控<br>为了提高智能体的情境意识和目标坚持性，Lyfe Agents 引入了一个自我监控模块。该模块维护一个关于最近事件的叙事风格摘要，并强调新颖和与目标相关的内容。这种自我监控摘要帮助智能体更好地理解情境，并使其行为更加一致和符合目标。<br><br>💡 创新点3：总结-遗忘记忆机制<br>为了提高记忆存储和检索的质量，Lyfe Agents 采用了一种层次化的记忆架构和总结-遗忘（SaF）方法。这种方法将记忆分为短期记忆和长期记忆，并通过聚类和总结技术将短期记忆中的信息转移到长期记忆中。此外，遗忘算法会评估并删除与现有记忆高度相似的旧记忆，以确保存储的信息是独特和相关的。<br><br>## 📈 实验结果<br>在自定义的 LyfeGame 3D 虚拟环境平台上，Lyfe Agents 在多个多智能体场景中展示了其自我激励和社会性。例如，智能体能够通过自主协作和信息交换解决犯罪（谋杀谜案）。此外，与现有方法相比，Lyfe Agents 的计算成本降低了 10-100 倍。<br><br>## 💬 可借鉴之处<br>Lyfe Agents 的设计原则和架构组件为构建低成本、实时响应的生成式智能体提供了有价值的参考。其选项-动作框架、异步自我监控和总结-遗忘记忆机制等创新点可以应用于其他生成式智能体框架，以提高其自主性和社交推理能力。此外，LyfeGame 虚拟环境平台也为研究生成式智能体的社会行为和用户交互提供了有价值的工具。</td>
    </tr>
    <tr>
      <th>93</th>
      <td>Chess as a Testbed for Language Model State Tracking</td>
      <td>Transformer language models have made tremendous strides in natural language<br>understanding tasks. However, the complexity of natural language makes it<br>challenging to ascertain how accurately these models are tracking the world<br>state underlying the text. Motivated by this issue, we consider the task of<br>language modeling for the game of chess. Unlike natural language, chess<br>notations describe a simple, constrained, and deterministic domain. Moreover,<br>we observe that the appropriate choice of chess notation allows for directly<br>probing the world state, without requiring any additional probing-related<br>machinery. We find that: (a) With enough training data, transformer language<br>models can learn to track pieces and predict legal moves with high accuracy<br>when trained solely on move sequences. (b) For small training sets providing<br>access to board state information during training can yield significant<br>improvements. (c) The success of transformer language models is dependent on<br>access to the entire game history i.e. "full attention". Approximating this<br>full attention results in a significant performance drop. We propose this<br>testbed as a benchmark for future work on the development and analysis of<br>transformer language models.</td>
      <td>## 🌟 论文解读 | 国际象棋：语言模型状态跟踪的测试平台<br><br>## 📌 背景痛点/本文动机<br>随着Transformer语言模型在自然语言理解任务上的巨大进步，人们开始关注这些模型是否能够准确跟踪文本背后的世界状态。然而，自然语言的复杂性使得评估模型的准确性变得困难。为了解决这个问题，本文提出将国际象棋作为语言模型状态跟踪能力的测试平台。<br><br>## 🚀 核心方法<br>💡 创新点1：将国际象棋作为测试平台<br>国际象棋是一个简单、受限且确定性的领域，其棋谱描述可以直接反映棋盘状态，无需额外的探测机制。这使得我们可以更精确地评估语言模型对世界状态的跟踪能力。<br><br>💡 创新点2：使用合适的棋谱表示法<br>本文使用国际象棋通用接口（UCI）表示法，它将起始方格和目标方格结合起来表示一个移动。这种表示法允许我们通过简单的提示来探测语言模型对棋盘状态的理解。<br><br>💡 创新点3：引入随机标注棋子类型（RAP）<br>为了更直接地探测模型的状态跟踪能力，本文提出在训练过程中以一定的概率随机包含棋子类型标记。这可以帮助模型学习跟踪棋子位置，并提高语言模型的性能。<br><br>💡 创新点4：设计棋盘状态探测任务<br>本文设计了多种棋盘状态探测任务，包括预测移动的起始方格和目标方格，以评估模型对棋盘状态和棋规的理解。<br><br>## 📈 实验结果<br>实验结果表明，当给定足够的训练数据时，Transformer语言模型可以学习跟踪棋子位置并预测合法移动，准确率很高。然而，当训练数据较少时，预测能力会下降。在这种情况下，引入部分棋盘状态信息可以显著提高棋子跟踪的准确率。<br><br>## 💬 可借鉴之处<br>本文提出的国际象棋测试平台为评估语言模型的状态跟踪能力提供了一个有效的工具。此外，本文的研究结果也为Transformer语言模型的发展和分析提供了新的见解，例如模型对输入分布变化的鲁棒性以及对完整上下文信息的需求。这些发现可以指导未来Transformer架构的设计，使其更擅长理解长文本并从少量训练数据中学习。</td>
    </tr>
    <tr>
      <th>53</th>
      <td>Text-based Adventures of the Golovin AI Agent</td>
      <td>The domain of text-based adventure games has been recently established as a<br>new challenge of creating the agent that is both able to understand natural<br>language, and acts intelligently in text-described environments.<br>  In this paper, we present our approach to tackle the problem. Our agent,<br>named Golovin, takes advantage of the limited game domain. We use genre-related<br>corpora (including fantasy books and decompiled games) to create language<br>models suitable to this domain. Moreover, we embed mechanisms that allow us to<br>specify, and separately handle, important tasks as fighting opponents, managing<br>inventory, and navigating on the game map.<br>  We validated usefulness of these mechanisms, measuring agent's performance on<br>the set of 50 interactive fiction games. Finally, we show that our agent plays<br>on a level comparable to the winner of the last year Text-Based Adventure AI<br>Competition.</td>
      <td>## 🌟 论文解读 | Golovin AI Agent：文本冒险游戏中的自然语言理解和智能行动<br><br>## 📌 背景痛点/本文动机<br>文本冒险游戏为人工智能领域带来了新的挑战：如何创建一个既能理解自然语言，又能在文本描述的环境中智能行动的智能体。现有的解决方案通常依赖于对游戏规则的分析和特定领域的特征利用，但缺乏对自然语言理解的深入探索。<br><br>## 🚀 核心方法<br>💡 创新点1：利用有限的游戏领域<br>Golovin AI Agent 利用文本冒险游戏的有限领域，通过分析相关语料库（包括奇幻书籍和反编译游戏）来创建适合该领域的语言模型。<br><br>💡 创新点2：嵌入特定机制<br>Golovin AI Agent 嵌入了特定机制，以分别处理重要的任务，如与对手战斗、管理库存和在游戏地图上导航。<br><br>💡 创新点3：利用游戏特定行为<br>Golovin AI Agent 利用游戏特定行为，如战斗模式、装备管理和移动策略，以提高其在游戏中的表现。<br><br>💡 创新点4：记忆和利用游戏历史<br>Golovin AI Agent 记忆并利用游戏历史的一些方面，以更好地理解游戏环境和做出决策。<br><br>💡 创新点5：模仿人类行为<br>Golovin AI Agent 尝试模仿人类行为，例如在探索游戏宇宙后重复最有希望的命令序列。<br><br>## 📈 实验结果<br>Golovin AI Agent 在 50 个交互式小说游戏上的表现进行了验证，结果显示其表现与去年文本冒险 AI 竞赛的获胜者相当。<br><br>## 💬 可借鉴之处<br>Golovin AI Agent 的设计思路和方法为文本冒险游戏中的自然语言理解和智能行动提供了新的思路，可以借鉴到其他类似场景中，例如虚拟助手、聊天机器人等。</td>
    </tr>
    <tr>
      <th>107</th>
      <td>Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents</td>
      <td>Large Language Models (LLMs) have shown great success as high-level planners<br>for zero-shot game-playing agents. However, these agents are primarily<br>evaluated on Minecraft, where long-term planning is relatively straightforward.<br>In contrast, agents tested in dynamic robot environments face limitations due<br>to simplistic environments with only a few objects and interactions. To fill<br>this gap in the literature, we present NetPlay, the first LLM-powered zero-shot<br>agent for the challenging roguelike NetHack. NetHack is a particularly<br>challenging environment due to its diverse set of items and monsters, complex<br>interactions, and many ways to die.<br>  NetPlay uses an architecture designed for dynamic robot environments,<br>modified for NetHack. Like previous approaches, it prompts the LLM to choose<br>from predefined skills and tracks past interactions to enhance decision-making.<br>Given NetHack's unpredictable nature, NetPlay detects important game events to<br>interrupt running skills, enabling it to react to unforeseen circumstances.<br>While NetPlay demonstrates considerable flexibility and proficiency in<br>interacting with NetHack's mechanics, it struggles with ambiguous task<br>descriptions and a lack of explicit feedback. Our findings demonstrate that<br>NetPlay performs best with detailed context information, indicating the<br>necessity for dynamic methods in supplying context information for complex<br>games such as NetHack.</td>
      <td>## 🌟 论文解读 | LLMs 在 NetHack 中的潜力与局限性：零样本智能体<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在游戏领域展现出强大的规划能力，尤其是在 Minecraft 等游戏中。然而，这些模型在处理复杂、动态的环境时，如机器人环境，往往面临局限性。为了填补这一空白，本文提出了 NetPlay，一个基于 LLM 的零样本智能体，用于挑战性的 Rogue-like 游戏 NetHack。<br><br>## 🚀 核心方法<br>NetPlay 采用了一种专为动态机器人环境设计的架构，并针对 NetHack 进行了修改。它通过提示 LLM 从预定义的技能中选择，并跟踪过去的交互来增强决策。NetPlay 还能够检测重要的游戏事件，以便在出现意外情况时中断正在执行的技能。<br><br>## 📈 实验结果<br>实验结果表明，NetPlay 在与 NetHack 的机制交互方面表现出色，但在处理模糊的任务描述和缺乏明确反馈时存在困难。NetPlay 在提供详细上下文信息的情况下表现最佳，这表明在 NetHack 等复杂游戏中，动态提供上下文信息的方法至关重要。<br><br>## 💬 可借鉴之处<br>NetPlay 的研究结果表明，LLMs 在游戏领域具有巨大的潜力，但仍然存在局限性。未来研究可以探索如何更好地利用 LLMs 的能力，例如通过动态提供上下文信息或使用机器学习来替代手工制作的组件。此外，NetPlay 的架构可以为其他复杂游戏的设计提供参考。</td>
    </tr>
    <tr>
      <th>97</th>
      <td>The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI</td>
      <td>We introduce a visually-guided and physics-driven task-and-motion planning<br>benchmark, which we call the ThreeDWorld Transport Challenge. In this<br>challenge, an embodied agent equipped with two 9-DOF articulated arms is<br>spawned randomly in a simulated physical home environment. The agent is<br>required to find a small set of objects scattered around the house, pick them<br>up, and transport them to a desired final location. We also position containers<br>around the house that can be used as tools to assist with transporting objects<br>efficiently. To complete the task, an embodied agent must plan a sequence of<br>actions to change the state of a large number of objects in the face of<br>realistic physical constraints. We build this benchmark challenge using the<br>ThreeDWorld simulation: a virtual 3D environment where all objects respond to<br>physics, and where can be controlled using fully physics-driven navigation and<br>interaction API. We evaluate several existing agents on this benchmark.<br>Experimental results suggest that: 1) a pure RL model struggles on this<br>challenge; 2) hierarchical planning-based agents can transport some objects but<br>still far from solving this task. We anticipate that this benchmark will<br>empower researchers to develop more intelligent physics-driven robots for the<br>physical world.</td>
      <td>## 🌟 论文解读 | 3DWorld Transport Challenge：物理世界中的智能机器人挑战<br><br>## 📌 背景痛点/本文动机<br>随着人工智能和机器人技术的不断发展，能够在物理世界中感知和行动的机器人成为了计算机视觉和机器人社区的重要目标。然而，直接使用真实机器人进行训练和评估成本高昂且存在安全风险。因此，近年来，人们开始将模拟器纳入训练和评估人工智能算法的过程中。尽管3D虚拟环境在视觉导航方面取得了显著进展，但它们大多关注视觉导航，而忽略了物理交互。由于最终目标是开发能够在物理环境中感知和行动的系统，因此物理交互已成为家庭助理机器人训练的必要组成部分。<br><br>## 🚀 核心方法<br>本文提出了一个新的具身AI挑战：一个具有两个9自由度关节臂的具身智能体被随机放置在一个物理真实的虚拟家庭环境中。智能体需要探索房屋，寻找散落在不同房间中的少量物体，并将它们运送到一个期望的最终位置。此外，房屋周围还放置了各种容器，智能体可以找到这些容器并将物体放入其中。不使用容器作为工具时，智能体只能一次运输两个物体。然而，使用容器，智能体可以收集多个物体并一起运输。<br><br>为了支持这项挑战，本文创建了一个基于TDW的房屋数据集，其中包含充满物理响应物体的多房间环境。此外，还开发了一个完全基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。由于模拟动作和环境完全基于物理，与之前的非物理或部分物理虚拟环境相比，它们提出了额外的挑战。例如，交互动作只有在目标物理可达时（即靠近且未被阻挡）才会成功。如果目标不在智能体的中心视图中，或者直接路径被阻挡（例如，被桌子阻挡），智能体就无法成功抓取物体。此外，与房屋中的物体发生物理碰撞也可能显著阻碍运输进度。因此，智能体必须学习利用视觉信号来同步导航和操作，以应对这些物理约束。<br><br>## 📈 实验结果<br>本文评估了几个现有的智能体，实验结果表明，现有的具身智能体在完成这项任务方面都存在困难。本文相信，在运输挑战中表现良好的模型将能够使机器人更加智能，能够在真实的物理世界中发挥作用。<br><br>## 💬 可借鉴之处<br>本文提出的3DWorld Transport Challenge为具身智能体在物理真实环境中的任务和运动规划能力提供了一个新的评估标准。此外，本文还开发了一个完全基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。这些成果为开发能够在物理世界中感知和行动的智能机器人提供了新的思路和方法。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</td>
      <td>StarCraft II is a challenging benchmark for AI agents due to the necessity of<br>both precise micro level operations and strategic macro awareness. Previous<br>works, such as Alphastar and SCC, achieve impressive performance on tackling<br>StarCraft II , however, still exhibit deficiencies in long term strategic<br>planning and strategy interpretability. Emerging large language model (LLM)<br>agents, such as Voyage and MetaGPT, presents the immense potential in solving<br>intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs<br>on StarCraft II, a highly complex RTS game.To conveniently take full advantage<br>of LLMs` reasoning abilities, we first develop textual StratCraft II<br>environment, called TextStarCraft II, which LLM agent can interact. Secondly,<br>we propose a Chain of Summarization method, including single frame<br>summarization for processing raw observations and multi frame summarization for<br>analyzing game information, providing command recommendations, and generating<br>strategic decisions. Our experiment consists of two parts: first, an evaluation<br>by human experts, which includes assessing the LLMs`s mastery of StarCraft II<br>knowledge and the performance of LLM agents in the game; second, the in game<br>performance of LLM agents, encompassing aspects like win rate and the impact of<br>Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the<br>relevant knowledge and complex planning abilities needed to address StarCraft<br>II scenarios; 2. Human experts consider the performance of LLM agents to be<br>close to that of an average player who has played StarCraft II for eight years;<br>3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)<br>difficulty level. We have open sourced the code and released demo videos of LLM<br>agent playing StarCraft II.</td>
      <td>## 🌟 论文解读 | 大型语言模型在星际争霸II中的表现：基准测试与摘要链方法<br><br>## 📌 背景痛点/本文动机<br>星际争霸II（StarCraft II）是一款极具挑战性的实时战略游戏，要求玩家在微观操作和宏观战略规划之间取得平衡。尽管之前的AI研究，如AlphaStar和SCC，在星际争霸II中取得了令人印象深刻的成果，但它们在长期战略规划和策略可解释性方面仍存在不足。随着大型语言模型（LLM）在解决复杂任务方面的潜力日益显现，本文旨在验证LLM在星际争霸II中的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：TextStarCraft II环境<br>为了充分利用LLM的推理能力，本文开发了一个名为TextStarCraft II的文本环境，LLM代理可以与之交互。该环境将星际争霸II的复杂游戏动态转换为文本格式，允许LLM代理通过语言命令执行宏观战略行动。<br><br>💡 创新点2：摘要链（CoS）方法<br>本文提出了摘要链（CoS）方法，包括单帧摘要和多帧摘要。单帧摘要用于处理原始观察数据，而多帧摘要用于分析游戏信息，提供命令建议并生成战略决策。CoS方法通过信息压缩、推理加速和全局理解，增强了LLM代理在处理复杂信息和做出战略决策方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，LLM具备解决星际争霸II场景所需的相关知识和复杂规划能力。人类专家认为，LLM代理在游戏中的表现接近于玩了八年星际争霸II的平均玩家。此外，LLM代理能够在Harder（Lv5）难度级别下击败内置AI。<br><br>## 💬 可借鉴之处<br>本文提出的TextStarCraft II环境和CoS方法为评估LLM在实时战略决策和长期规划方面的能力提供了一个新的基准。此外，本文的研究结果表明，LLM在解决复杂任务方面具有巨大潜力，并为未来在星际争霸II和其他实时战略游戏中的AI研究提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>76</th>
      <td>AvalonBench: Evaluating LLMs Playing the Game of Avalon</td>
      <td>In this paper, we explore the potential of Large Language Models (LLMs)<br>Agents in playing the strategic social deduction game, Resistance Avalon.<br>Players in Avalon are challenged not only to make informed decisions based on<br>dynamically evolving game phases, but also to engage in discussions where they<br>must deceive, deduce, and negotiate with other players. These characteristics<br>make Avalon a compelling test-bed to study the decision-making and<br>language-processing capabilities of LLM Agents. To facilitate research in this<br>line, we introduce AvalonBench - a comprehensive game environment tailored for<br>evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game<br>environment for Avalon, (2) rule-based bots as baseline opponents, and (3)<br>ReAct-style LLM agents with tailored prompts for each role. Notably, our<br>evaluations based on AvalonBench highlight a clear capability gap. For<br>instance, models like ChatGPT playing good-role got a win rate of 22.2% against<br>rule-based bots playing evil, while good-role bot achieves 38.2% win rate in<br>the same setting. We envision AvalonBench could be a good test-bed for<br>developing more advanced LLMs (with self-playing) and agent frameworks that can<br>effectively model the layered complexities of such game environments.</td>
      <td>## 🌟 论文解读 | AvalonBench：评估大型语言模型在社交推理游戏中的表现<br><br>## 📌 背景痛点/本文动机<br>社交推理游戏如 Resistance Avalon 对玩家的推理、沟通和决策能力提出了挑战。这些游戏要求玩家在动态变化的游戏阶段做出明智的决策，并在讨论中欺骗、推理和与其他玩家协商。这些特点使得 Avalon 成为研究大型语言模型（LLM）代理的决策和语言处理能力的理想测试平台。然而，目前缺乏一个全面的评估 LLM 代理在多代理游戏环境中的性能的基准测试平台。<br><br>## 🚀 核心方法<br>本文提出了 AvalonBench，一个专门用于评估多代理 LLM 代理的游戏环境。AvalonBench 包含以下三个关键组成部分：<br><br>1. **Avalon 游戏环境**：为代理提供游戏平台，记录所有玩家的行动并推动游戏进程。<br>2. **基于规则的机器人**：作为基线对手，为代理提供可比较的基准。<br>3. **ReAct 风格的 LLM 代理**：针对每个角色定制提示，以评估 LLM 代理在不同角色下的表现。<br><br>## 📈 实验结果<br>本文使用 AvalonBench 对 ChatGPT-3.5 和 Llama2 模型进行了评估，并与基于规则的机器人进行了比较。结果显示，即使在有讨论的情况下，LLM 代理的表现也远低于基于规则的机器人。例如，ChatGPT-3.5 在扮演好人角色时，在与扮演坏人的基于规则的机器人对抗中，胜率为 22.2%，而好人角色的机器人胜率为 38.2%。这表明当前 LLM 代理在推理、说服、协商和欺骗能力方面存在明显差距。<br><br>## 💬 可借鉴之处<br>AvalonBench 为研究 LLM 代理在社交推理游戏中的表现提供了一个有价值的测试平台。该平台可以帮助研究人员开发更先进的 LLM 代理，并探索如何将决策技术集成到 LLM 中，以提高其在复杂游戏环境中的表现。此外，AvalonBench 还可以用于评估 LLM 代理在多代理协作、沟通和策略制定方面的能力。</td>
    </tr>
    <tr>
      <th>60</th>
      <td>Language Models Meet World Models: Embodied Experiences Enhance Language Models</td>
      <td>While large language models (LMs) have shown remarkable capabilities across<br>numerous tasks, they often struggle with simple reasoning and planning in<br>physical environments, such as understanding object permanence or planning<br>household activities. The limitation arises from the fact that LMs are trained<br>only on written text and miss essential embodied knowledge and skills. In this<br>paper, we propose a new paradigm of enhancing LMs by finetuning them with world<br>models, to gain diverse embodied knowledge while retaining their general<br>language capabilities. Our approach deploys an embodied agent in a world model,<br>particularly a simulator of the physical world (VirtualHome), and acquires a<br>diverse set of embodied experiences through both goal-oriented planning and<br>random exploration. These experiences are then used to finetune LMs to teach<br>diverse abilities of reasoning and acting in the physical world, e.g., planning<br>and completing goals, object permanence and tracking, etc. Moreover, it is<br>desirable to preserve the generality of LMs during finetuning, which<br>facilitates generalizing the embodied knowledge across tasks rather than being<br>tied to specific simulations. We thus further introduce the classical (EWC) for<br>selective weight updates, combined with low-rank adapters (LoRA) for training<br>efficiency. Extensive experiments show our approach substantially improves base<br>LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs<br>(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much<br>larger LMs (e.g., ChatGPT).</td>
      <td>## 🌟 论文解读 | 语言模型与世界观模型相遇：具身经验增强语言模型<br><br>## 📌 背景痛点/本文动机<br>尽管大型语言模型（LMs）在众多任务中表现出色，但它们在物理环境中的简单推理和规划方面往往存在困难，例如理解物体恒常性或规划家庭活动。这种局限性源于LMs仅通过书面文本进行训练，缺乏必要的具身知识和技能。<br><br>## 🚀 核心方法<br>💡 创新点1：E2WM训练范式<br>本文提出了一种新的训练范式，即使用来自世界观模型的具身经验对LMs进行微调（E2WM）。世界观模型是具身模拟器，可以模拟真实世界环境中的物理交互，为LMs提供理解环境中的物体交互和执行动作的机会。<br><br>💡 创新点2：收集具身经验<br>为了将不同的具身知识注入LMs，本文介绍了两种收集经验的方法：目标导向规划和随机探索。目标导向规划旨在收集与规划和目标导向代理行为相关的经验，而随机探索则专注于积累涉及物体和世界状态跟踪的经验。<br><br>💡 创新点3：EWC-LoRA微调<br>为了在收集的具身经验上微调LMs，同时保留其原始的通用知识和能力，本文提出了将经典的弹性权重整合（EWC）与低秩适配器（LoRA）相结合的方法。EWC通过正则化微调损失来保留重要的LM参数，而LoRA则通过在每个模型层中注入两个可训练的低秩矩阵来实现参数高效的微调。<br><br>## 📈 实验结果<br>实验结果表明，本文的方法在18个下游任务上显著提高了基线LMs的性能，平均提高了64.28%。特别是，通过本文方法增强的小型LMs（1.3B、6B和13B）在许多任务上甚至超过了更大的LMs（例如ChatGPT）。<br><br>## 💬 可借鉴之处<br>本文提出的E2WM训练范式为增强LMs的具身知识和技能提供了一种有效的方法。通过使用世界观模型收集具身经验，并结合EWC-LoRA进行微调，LMs可以更好地理解和处理物理环境中的任务。这种方法在多个下游任务上取得了显著的性能提升，并展示了LMs在具身任务中的潜力。</td>
    </tr>
    <tr>
      <th>100</th>
      <td>Habitat: A Platform for Embodied AI Research</td>
      <td>We present Habitat, a platform for research in embodied artificial<br>intelligence (AI). Habitat enables training embodied agents (virtual robots) in<br>highly efficient photorealistic 3D simulation. Specifically, Habitat consists<br>of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with<br>configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is<br>fast -- when rendering a scene from Matterport3D, it achieves several thousand<br>frames per second (fps) running single-threaded, and can reach over 10,000 fps<br>multi-process on a single GPU. (ii) Habitat-API: a modular high-level library<br>for end-to-end development of embodied AI algorithms -- defining tasks (e.g.,<br>navigation, instruction following, question answering), configuring, training,<br>and benchmarking embodied agents.<br>  These large-scale engineering contributions enable us to answer scientific<br>questions requiring experiments that were till now impracticable or 'merely'<br>impractical. Specifically, in the context of point-goal navigation: (1) we<br>revisit the comparison between learning and SLAM approaches from two recent<br>works and find evidence for the opposite conclusion -- that learning<br>outperforms SLAM if scaled to an order of magnitude more experience than<br>previous investigations, and (2) we conduct the first cross-dataset<br>generalization experiments {train, test} x {Matterport3D, Gibson} for multiple<br>sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors<br>generalize across datasets. We hope that our open-source platform and these<br>findings will advance research in embodied AI.</td>
      <td>## 🌟 论文解读 | Habitat：赋能具身AI研究的平台<br><br>## 📌 背景痛点/本文动机<br>随着深度学习在计算机视觉和自然语言处理领域的快速发展，具身AI（Embodied AI）也逐渐成为研究热点。具身AI旨在让智能体（如机器人）在真实环境中进行感知、决策和行动，从而实现更高级的智能。然而，在真实世界中训练具身AI面临着诸多挑战，例如训练速度慢、成本高、安全性低等。因此，本文提出了Habitat平台，旨在通过高效的3D模拟器，为具身AI研究提供更便捷、更高效的解决方案。<br><br>## 🚀 核心方法<br>Habitat平台主要由两部分组成：<br><br>1. **Habitat-Sim**：一个灵活、高性能的3D模拟器，支持可配置的智能体、传感器和通用3D数据集处理。Habitat-Sim具有以下特点：<br>    * **高性能渲染**：单线程运行时，渲染Matterport3D场景可达数千帧每秒（fps），多进程运行时，单GPU可达超过10,000 fps。<br>    * **灵活配置**：支持配置不同类型的智能体、传感器和3D数据集，例如Matterport3D、Gibson和Replica数据集。<br>    * **高效GPU吞吐量**：通过共享内存和CUDA-GL互操作，实现高效的GPU渲染和CPU内存访问。<br><br>2. **Habitat-API**：一个模块化、高级别的库，用于端到端开发具身AI算法。Habitat-API支持以下功能：<br>    * **任务定义**：定义各种具身AI任务，例如导航、指令跟随、问答等。<br>    * **智能体配置**：配置和训练不同类型的智能体，例如通过模仿学习、强化学习或经典SLAM方法。<br>    * **基准测试**：使用标准指标对智能体进行评估和比较。<br><br>## 📈 实验结果<br>本文在点目标导航任务上进行了实验，结果表明：<br><br>1. **学习优于SLAM**：当训练经验足够多时，基于学习的智能体可以超越经典SLAM方法。<br>2. **深度传感器更具泛化能力**：配备深度传感器的智能体在不同数据集之间具有更好的泛化能力。<br><br>## 💬 可借鉴之处<br>Habitat平台为具身AI研究提供了强大的工具和平台，具有以下可借鉴之处：<br><br>* **高效模拟器**：Habitat-Sim的高性能渲染能力可以显著提高训练速度，降低训练成本。<br>* **灵活配置**：Habitat平台支持灵活配置智能体、传感器和数据集，方便研究人员进行各种实验。<br>* **模块化设计**：Habitat-API的模块化设计方便研究人员进行代码复用和扩展。<br>* **基准测试**：Habitat平台提供了标准化的基准测试方法，方便研究人员进行结果比较和评估。<br><br>总而言之，Habitat平台为具身AI研究提供了强大的工具和平台，有望推动具身AI领域的快速发展。</td>
    </tr>
    <tr>
      <th>75</th>
      <td>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</td>
      <td>Agents built with large language models (LLMs) have shown great potential<br>across a wide range of domains. However, in complex decision-making tasks, pure<br>LLM-based agents tend to exhibit intrinsic bias in their choice of actions,<br>which is inherited from the model's training data and results in suboptimal<br>performance. To develop strategic language agents, i.e., agents that generate<br>flexible language actions and possess strong decision-making abilities, we<br>propose a novel framework that powers LLM-based agents with reinforcement<br>learning (RL). We consider Werewolf, a popular social deduction game, as a<br>challenging testbed that emphasizes versatile communication and strategic<br>gameplay. To mitigate the intrinsic bias in language actions, our agents use an<br>LLM to perform deductive reasoning and generate a diverse set of action<br>candidates. Then an RL policy trained to optimize the decision-making ability<br>chooses an action from the candidates to play in the game. Extensive<br>experiments show that our agents overcome the intrinsic bias and outperform<br>existing LLM-based agents in the Werewolf game. We also conduct human-agent<br>experiments and find that our agents achieve human-level performance and<br>demonstrate strong strategic play.</td>
      <td>## 🌟 论文解读 | 语言智能体在狼人杀游戏中的战略决策<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在构建智能体方面的广泛应用，其在复杂决策任务中表现出的内在偏差问题逐渐凸显。这种偏差源于模型训练数据，导致LLM-based智能体在战略决策方面表现不佳。为了解决这个问题，本文提出了一种新的框架，将LLM与强化学习（RL）相结合，以构建具有灵活语言行动和强大决策能力的战略语言智能体。<br><br>## 🚀 核心方法<br>💡 创新点1：隐藏角色推理<br>本文使用LLM对游戏中的信息进行分类，区分真伪，并推断每个玩家的隐藏角色，为后续决策提供基础。<br><br>💡 创新点2：多样化行动生成<br>为了克服LLM的内在偏差，本文提出了一种多样化行动生成方法，通过提示LLM生成一系列行动候选者，从而避免固定模式并提高决策的灵活性。<br><br>💡 创新点3：基于群体的强化学习训练<br>本文采用基于群体的强化学习训练方法，通过训练一个RL策略来优化行动候选者的分布，并通过与各种智能体进行对抗来提高策略的鲁棒性。<br><br>## 📈 实验结果<br>本文在狼人杀游戏中进行了广泛的实验，结果表明，与现有的LLM-based智能体相比，本文提出的战略语言智能体能够克服内在偏差，并在游戏中表现出更强的战略决策能力。此外，与人类玩家的对局实验也表明，本文提出的智能体能够达到人类水平的游戏表现。<br><br>## 💬 可借鉴之处<br>本文提出的框架为构建具有强大决策能力的战略语言智能体提供了一种新的思路，其核心方法可以应用于其他需要灵活语言行动和战略决策的场景。此外，本文提出的多样化行动生成方法和基于群体的强化学习训练方法也为解决LLM内在偏差问题提供了新的思路。</td>
    </tr>
    <tr>
      <th>119</th>
      <td>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</td>
      <td>Large language models (LLMs) have achieved success in acting as agents, which<br>interact with environments through tools such as search engines. However, LLMs<br>are optimized for language generation instead of tool use during training or<br>alignment, limiting their effectiveness as agents. To resolve this problem,<br>previous work has first collected interaction trajectories between LLMs and<br>environments, using only trajectories that successfully finished the task to<br>fine-tune smaller models, making fine-tuning data scarce and acquiring it both<br>difficult and costly. Discarding failed trajectories also leads to significant<br>wastage of data and resources and limits the possible optimization paths during<br>fine-tuning. In this paper, we argue that unsuccessful trajectories offer<br>valuable insights, and LLMs can learn from these trajectories through<br>appropriate quality control and fine-tuning strategies. By simply adding a<br>prefix or suffix that tells the model whether to generate a successful<br>trajectory during training, we improve model performance by a large margin on<br>mathematical reasoning, multi-hop question answering, and strategic question<br>answering tasks. We further analyze the inference results and find that our<br>method provides a better trade-off between valuable information and errors in<br>unsuccessful trajectories. To our knowledge, we are the first to demonstrate<br>the value of negative trajectories and their application in agent-tunning<br>scenarios. Our findings offer guidance for developing better agent-tuning<br>methods and low-resource data usage techniques.</td>
      <td>## 🌟 论文解读 | 从失败中学习：在微调大型语言模型作为智能体时整合负面示例<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在作为智能体方面取得了成功，它们通过与搜索引擎等工具进行交互来与环境互动。然而，LLMs在训练或对齐过程中主要针对语言生成进行优化，而不是工具使用，这限制了它们作为智能体的有效性。为了解决这个问题，先前的工作首先收集了LLMs和环境之间的交互轨迹，仅使用成功完成任务的轨迹来微调较小的模型，这使得微调数据稀缺，获取难度大且成本高。丢弃失败的轨迹也导致了数据资源的浪费，并限制了微调过程中的优化路径。<br><br>## 🚀 核心方法<br>本文提出了一种名为“负面感知训练”（NAT）的范式，通过在训练过程中添加前缀或后缀来告诉模型是否生成成功的轨迹，从而整合负面示例。这种方法使得LLMs能够从失败的轨迹中学习，并通过适当的质量控制策略来提高模型性能。<br><br>## 📈 实验结果<br>实验结果表明，NAT在数学推理、多跳问答和策略问答任务上显著提高了模型性能。此外，分析推理结果发现，NAT在有价值的信息和错误之间提供了更好的权衡。<br><br>## 💬 可借鉴之处<br>本文首次证明了负面轨迹的价值及其在智能体微调场景中的应用。NAT方法为开发更好的智能体微调方法和低资源数据使用技术提供了指导。</td>
    </tr>
    <tr>
      <th>94</th>
      <td>DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker</td>
      <td>Artificial intelligence has seen several breakthroughs in recent years, with<br>games often serving as milestones. A common feature of these games is that<br>players have perfect information. Poker is the quintessential game of imperfect<br>information, and a longstanding challenge problem in artificial intelligence.<br>We introduce DeepStack, an algorithm for imperfect information settings. It<br>combines recursive reasoning to handle information asymmetry, decomposition to<br>focus computation on the relevant decision, and a form of intuition that is<br>automatically learned from self-play using deep learning. In a study involving<br>44,000 hands of poker, DeepStack defeated with statistical significance<br>professional poker players in heads-up no-limit Texas hold'em. The approach is<br>theoretically sound and is shown to produce more difficult to exploit<br>strategies than prior approaches.</td>
      <td>## 🌟 论文解读 | DeepStack：在不完美信息游戏中实现专家级人工智能<br><br>## 📌 背景痛点/本文动机<br>近年来，人工智能在许多游戏中取得了突破性进展，例如国际象棋、围棋等。这些游戏的一个共同特点是玩家拥有完美信息，即所有玩家都能完全了解当前的游戏状态。然而，现实世界中的许多问题都涉及到信息不对称，即玩家拥有不同的信息。扑克牌游戏就是一个典型的例子，玩家拥有私人牌，这使得游戏充满了不确定性。<br><br>## 🚀 核心方法<br>DeepStack 是一种针对不完美信息场景的算法，它结合了递归推理、分解和深度学习，实现了在不完美信息游戏中达到专家级水平。<br><br>💡 创新点1：递归推理<br>DeepStack 使用递归推理来处理信息不对称问题。它通过分析对手过去的行动来推断对手可能持有的私人牌，从而做出更明智的决策。<br><br>💡 创新点2：分解<br>DeepStack 使用分解技术将复杂的游戏状态分解成更小的子问题，从而将计算集中在相关的决策上。这大大提高了算法的效率。<br><br>💡 创新点3：深度学习<br>DeepStack 使用深度学习来自动学习一种直觉，即对持有任何可能的私人牌在任何可能的扑克牌情况下的价值进行快速估计。这种直觉可以帮助 DeepStack 在游戏中做出更明智的决策。<br><br>## 📈 实验结果<br>DeepStack 在与专业扑克牌玩家的对战中取得了显著的优势。在一项涉及 44,000 手扑克牌的研究中，DeepStack 以统计显著的方式击败了专业扑克牌玩家。<br><br>## 💬 可借鉴之处<br>DeepStack 的成功表明，深度学习可以用于解决不完美信息游戏中的复杂问题。DeepStack 的方法可以应用于其他不完美信息场景，例如医疗诊断、战略资源防御等。<br><br>## 🌟 总结<br>DeepStack 是人工智能领域的一个重要突破，它展示了在不完美信息游戏中实现专家级水平的可能性。DeepStack 的方法为解决现实世界中的复杂问题提供了新的思路。</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Cooperative Open-ended Learning Framework for Zero-shot Coordination</td>
      <td>Zero-shot coordination in cooperative artificial intelligence (AI) remains a<br>significant challenge, which means effectively coordinating with a wide range<br>of unseen partners. Previous algorithms have attempted to address this<br>challenge by optimizing fixed objectives within a population to improve<br>strategy or behaviour diversity. However, these approaches can result in a loss<br>of learning and an inability to cooperate with certain strategies within the<br>population, known as cooperative incompatibility. To address this issue, we<br>propose the Cooperative Open-ended LEarning (COLE) framework, which constructs<br>open-ended objectives in cooperative games with two players from the<br>perspective of graph theory to assess and identify the cooperative ability of<br>each strategy. We further specify the framework and propose a practical<br>algorithm that leverages knowledge from game theory and graph theory.<br>Furthermore, an analysis of the learning process of the algorithm shows that it<br>can efficiently overcome cooperative incompatibility. The experimental results<br>in the Overcooked game environment demonstrate that our method outperforms<br>current state-of-the-art methods when coordinating with different-level<br>partners. Our demo is available at https://sites.google.com/view/cole-2023.</td>
      <td>## 🌟 论文解读 | 零样本协调的协作开放学习框架<br><br>## 📌 背景痛点/本文动机<br>在合作人工智能（AI）中，零样本协调（ZSC）是一个重大挑战，即如何有效地与各种未见过的伙伴进行协调。传统的自我博弈（SP）方法虽然可以收敛到游戏的均衡状态，但往往形成特定的行为和惯例，难以适应与未见过的策略进行协调。为了克服SP的局限性，许多ZSC方法通过引入基于群体的训练（PBT）来促进策略或行为的多样性，以提高策略的适应性。然而，当优化固定的人口级目标时，群体内策略的协调能力可能不会得到提高，导致所谓的“合作不兼容性”。<br><br>## 🚀 核心方法<br>💡 创新点1：图形形式游戏（GFGs）和偏好图形形式游戏（P-GFGs）<br>本文提出了图形形式游戏（GFGs）和偏好图形形式游戏（P-GFGs）的概念，将合作任务重新表述为图形形式，以便更有效地评估和识别学习过程中的合作不兼容性。在GFGs中，策略被表征为节点，节点之间的边权重表示两个相关策略的平均合作收益。通过利用GFGs的子图，即偏好图形形式游戏（P-GFGs），可以进一步分析每个节点在图中的最大合作收益，从而评估合作不兼容性并识别无法协作的策略。<br><br>💡 创新点2：协作开放学习框架（COLE）<br>为了解决合作不兼容性问题，本文提出了协作开放学习框架（COLE），该框架从图形理论的角度构建了开放的目标，以评估和识别每个策略的合作能力。COLE框架通过迭代生成新的策略，这些策略近似于P-GFGs的经验游戏场景的最佳反应。此外，本文还提出了一种实用的算法COLESV，该算法结合了博弈论和图论的知识，并证明了该算法可以有效地克服合作不兼容性。<br><br>## 📈 实验结果<br>在Overcooked游戏环境中进行的实验结果表明，本文提出的方法在协调不同级别的伙伴时优于当前最先进的方法。此外，通过分析GFGs和P-GFGs，COLESV的学习过程揭示了该框架可以有效地克服合作不兼容性。<br><br>## 💬 可借鉴之处<br>本文提出的GFGs和P-GFGs的概念以及COLE框架为解决合作不兼容性问题提供了一种新的思路。此外，本文提出的COLESV算法在实际应用中具有很好的效果，可以为其他合作AI任务提供参考。</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</td>
      <td>Due to the dynamic and unpredictable open-world setting, navigating complex<br>environments in Minecraft poses significant challenges for multi-agent systems.<br>Agents must interact with the environment and coordinate their actions with<br>other agents to achieve common objectives. However, traditional approaches<br>often struggle to efficiently manage inter-agent communication and task<br>distribution, crucial for effective multi-agent navigation. Furthermore,<br>processing and integrating multi-modal information (such as visual, textual,<br>and auditory data) is essential for agents to comprehend their goals and<br>navigate the environment successfully and fully. To address this issue, we<br>design the HAS framework to auto-organize groups of LLM-based agents to<br>complete navigation tasks. In our approach, we devise a hierarchical<br>auto-organizing navigation system, which is characterized by 1) a hierarchical<br>system for multi-agent organization, ensuring centralized planning and<br>decentralized execution; 2) an auto-organizing and intra-communication<br>mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal<br>information platform, facilitating multi-modal perception to perform the three<br>navigation tasks with one system. To assess organizational behavior, we design<br>a series of navigation tasks in the Minecraft environment, which includes<br>searching and exploring. We aim to develop embodied organizations that push the<br>boundaries of embodied AI, moving it towards a more human-like organizational<br>structure.</td>
      <td>## 🌟 论文解读 | HAS：开放世界多智能体导航的分层自组织系统<br><br>## 📌 背景痛点/本文动机<br>在开放世界的环境中，如Minecraft，多智能体系统面临着复杂的导航挑战。传统的导航方法往往难以有效地管理智能体之间的通信和任务分配，这对于有效的多智能体导航至关重要。此外，处理和整合多模态信息（如视觉、文本和听觉数据）对于智能体理解其目标并在环境中成功导航至关重要。<br><br>## 🚀 核心方法<br>💡 创新点1：分层自组织导航系统<br>HAS框架设计了一个分层自组织导航系统，该系统具有以下特点：<br>1. 分层系统：确保集中式规划和分布式执行，提高导航效率。<br>2. 自组织机制：根据子任务动态调整关键角色和行动组，并保持组间通信，确保高效协作。<br>3. 多模态信息平台：促进多模态感知，使系统能够处理图像、对象和音频目标，并执行搜索和探索等导航任务。<br><br>💡 创新点2：多模态语言模型<br>HAS框架使用了多模态语言模型（MLM），包括管理者和执行者两种类型的模型。这些模型具有不同的功能，如规划、描述、评估和部署，以实现集中式规划和分布式执行。<br><br>💡 创新点3：多模态记忆<br>HAS框架还引入了多模态记忆机制，用于存储和检索多模态信息，从而提高规划准确性和一致性。通过检索增强生成（RAG）和多模态检索（MMR）技术，HAS能够有效地利用历史交互反馈和经验，生成更准确的计划。<br><br>## 📈 实验结果<br>在Minecraft环境中进行的实验表明，HAS框架在多模态目标搜索、连续块搜索和地图探索等任务中取得了最先进的性能。与基线方法相比，HAS在导航效率、成功率和探索能力方面均有显著提升。<br><br>## 💬 可借鉴之处<br>HAS框架为开放世界多智能体导航提供了一种新的思路和方法。其分层自组织结构、多模态语言模型和多模态记忆机制等创新点，对于提高多智能体系统的自主性、效率和适应性具有重要意义。此外，HAS框架还可以应用于其他需要多智能体协作的场景，如机器人协同、虚拟现实等。</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
      <td>Like humans, large language models (LLMs) do not always generate the best<br>output on their first try. Motivated by how humans refine their written text,<br>we introduce Self-Refine, an approach for improving initial outputs from LLMs<br>through iterative feedback and refinement. The main idea is to generate an<br>initial output using an LLMs; then, the same LLMs provides feedback for its<br>output and uses it to refine itself, iteratively. Self-Refine does not require<br>any supervised training data, additional training, or reinforcement learning,<br>and instead uses a single LLM as the generator, refiner, and feedback provider.<br>We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response<br>generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,<br>and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine<br>are preferred by humans and automatic metrics over those generated with the<br>same LLM using conventional one-step generation, improving by ~20% absolute on<br>average in task performance. Our work demonstrates that even state-of-the-art<br>LLMs like GPT-4 can be further improved at test time using our simple,<br>standalone approach.</td>
      <td>## 🌟 论文解读 | 自我反馈迭代优化：提升大型语言模型输出质量的新方法<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）虽然在生成连贯的输出方面表现出色，但在处理复杂任务时往往难以满足精细化的需求。例如，在对话生成、代码优化等任务中，LLMs 生成的初始输出可能不够完善，需要进一步的迭代优化才能达到理想的质量。传统的迭代优化方法通常需要训练一个专门的优化模型，这需要大量的领域特定数据或昂贵的标注，限制了其应用范围。<br><br>## 🚀 核心方法<br>本文提出了一种名为 Self-Refine 的新型迭代优化算法，该算法通过自我反馈和迭代优化来提升 LLMs 的输出质量。Self-Refine 的核心思想是使用同一个 LLM 作为生成器、反馈提供者和优化器，无需额外的训练数据或强化学习。具体步骤如下：<br><br>1. **初始生成**：使用 LLM 生成一个初始输出。<br>2. **反馈**：将初始输出反馈给 LLM，LLM 根据反馈生成改进建议。<br>3. **优化**：LLM 根据反馈建议对初始输出进行优化，生成新的输出。<br>4. **迭代**：重复步骤 2 和 3，直到达到预设的停止条件。<br><br>Self-Refine 使用少量样本提示（few-shot prompting）来指导 LLM 进行反馈和优化，从而避免了额外的训练过程。<br><br>## 📈 实验结果<br>本文在 7 个不同的任务上评估了 Self-Refine 的性能，包括对话生成、代码优化、代码可读性改进、数学推理等。结果表明，Self-Refine 在所有任务上都显著优于直接使用 LLM 进行单次生成的结果，平均性能提升约 20%。此外，Self-Refine 还能够提升 GPT-4 等最先进的 LLMs 的性能，证明了其在实际应用中的价值。<br><br>## 💬 可借鉴之处<br>Self-Refine 为提升 LLMs 输出质量提供了一种简单而有效的方法，具有以下可借鉴之处：<br><br>* **自我反馈机制**：利用 LLM 自身的能力进行自我反馈和优化，无需额外的训练数据或标注。<br>* **迭代优化**：通过多次迭代优化，逐步提升输出质量，达到更精细化的需求。<br>* **少量样本提示**：使用少量样本提示来指导 LLM 进行反馈和优化，简化了操作流程。<br><br>Self-Refine 的提出为 LLMs 的应用开辟了新的可能性，有望在自然语言处理、代码生成等领域发挥重要作用。</td>
    </tr>
    <tr>
      <th>59</th>
      <td>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</td>
      <td>Can we avoid wars at the crossroads of history? This question has been<br>pursued by individuals, scholars, policymakers, and organizations throughout<br>human history. In this research, we attempt to answer the question based on the<br>recent advances of Artificial Intelligence (AI) and Large Language Models<br>(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to<br>simulate the participating countries, their decisions, and the consequences, in<br>historical international conflicts, including the World War I (WWI), the World<br>War II (WWII), and the Warring States Period (WSP) in Ancient China. By<br>evaluating the simulation effectiveness, we examine the advancements and<br>limitations of cutting-edge AI systems' abilities in studying complex<br>collective human behaviors such as international conflicts under diverse<br>settings. In these simulations, the emergent interactions among agents also<br>offer a novel perspective for examining the triggers and conditions that lead<br>to war. Our findings offer data-driven and AI-augmented insights that can<br>redefine how we approach conflict resolution and peacekeeping strategies. The<br>implications stretch beyond historical analysis, offering a blueprint for using<br>AI to understand human history and possibly prevent future international<br>conflicts. Code and data are available at<br>\url{https://github.com/agiresearch/WarAgent}.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型模拟历史战争，探索和平的可能性<br><br>## 📌 背景痛点/本文动机<br>战争与和平是人类历史永恒的主题，理解战争的原因和预防战争的发生一直是人类追求的目标。传统的战争研究方法主要依赖于历史分析和文献回顾，但这些方法往往受限于静态视角和事后诸葛亮的偏见。随着人工智能和大型语言模型（LLM）的快速发展，我们有机会利用这些先进技术来模拟历史事件，探索战争与和平的动态过程，并为冲突解决和和平维护提供新的视角。<br><br>## 🚀 核心方法<br>本文提出了 WarAgent，一个基于 LLM 的多智能体 AI 系统，用于模拟历史国际冲突，包括第一次世界大战（WWI）、第二次世界大战（WWII）和中国古代战国时期（WSP）。WarAgent 通过模拟参与国家的决策过程和互动，探索了以下关键问题：<br><br>* **模拟有效性**：LLM 基于多智能体系统能否有效地复制历史事件中战略规划和决策过程的演变？<br>* **战争起因**：哪些因素是导致战争爆发的关键因素？LLM 基于多智能体系统能否识别这些因素？<br>* **战争不可避免性**：历史上的战争是否真的不可避免？LLM 基于多智能体系统能否揭示导致战争（或和平）的条件？<br><br>## 📈 实验结果<br>实验结果表明，WarAgent 能够有效地模拟历史事件，并在一定程度上复制历史决策过程和互动。例如，在 WWI 模拟中，WarAgent 能够重现主要国家之间的联盟形成、战争宣言和动员情况，与历史事件具有较高的吻合度。此外，通过改变触发事件和国家的初始条件，WarAgent 能够探索不同的战争起因和战争不可避免性，为理解历史事件和预防未来冲突提供了新的视角。<br><br>## 💬 可借鉴之处<br>* **LLM 在历史模拟中的应用**：WarAgent 为 LLM 在历史模拟中的应用提供了新的思路，为理解复杂的人类行为和社会动态提供了新的工具。<br>* **多智能体系统在冲突解决中的应用**：WarAgent 的设计理念可以为冲突解决和和平维护提供新的思路，例如通过模拟不同政策的影响来评估冲突解决策略的有效性。<br>* **历史教学的新方法**：WarAgent 可以作为一种新的历史教学方法，帮助学生和教师探索“如果”场景，并理解历史事件的复杂因果关系。<br><br>## 🌟 未来展望<br>WarAgent 的研究为 LLM 在历史模拟中的应用开辟了新的道路，未来可以进一步探索以下方向：<br><br>* **时间驱动模拟**：将 WarAgent 的回合制模拟扩展为时间驱动模拟，以更准确地模拟历史事件的时间动态。<br>* **停止条件**：研究更有效的停止条件，以更清晰地结束模拟并分析结果。<br>* **新的研究问题**：探索更多与历史事件和冲突解决相关的研究问题，例如外交沟通与冲突可能性之间的关系、非国家行为体对地缘政治的影响等。<br><br>通过不断改进和扩展 WarAgent，我们可以更好地理解历史事件，并为构建更加和平的未来提供新的思路。</td>
    </tr>
    <tr>
      <th>99</th>
      <td>Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments</td>
      <td>We present Interactive Gibson Benchmark, the first comprehensive benchmark<br>for training and evaluating Interactive Navigation: robot navigation strategies<br>where physical interaction with objects is allowed and even encouraged to<br>accomplish a task. For example, the robot can move objects if needed in order<br>to clear a path leading to the goal location. Our benchmark comprises two novel<br>elements: 1) a new experimental setup, the Interactive Gibson Environment<br>(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high<br>fidelity physical dynamics of the robot and common objects found in these<br>scenes; 2) a set of Interactive Navigation metrics which allows one to study<br>the interplay between navigation and physical interaction. We present and<br>evaluate multiple learning-based baselines in Interactive Gibson, and provide<br>insights into regimes of navigation with different trade-offs between<br>navigation path efficiency and disturbance of surrounding objects. We make our<br>benchmark publicly<br>available(https://sites.google.com/view/interactivegibsonenv) and encourage<br>researchers from all disciplines in robotics (e.g. planning, learning, control)<br>to propose, evaluate, and compare their Interactive Navigation solutions in<br>Interactive Gibson.</td>
      <td>## 🌟 论文解读 | 交互式Gibson基准（iGibson 0.5）：用于杂乱环境中的交互式导航的基准<br><br>## 📌 背景痛点/本文动机<br>传统的机器人导航主要关注在避免碰撞的情况下到达目标。然而，随着机器人越来越多地部署在杂乱无章的环境中，如家庭和办公室，考虑物理交互作为导航策略的一部分变得不可避免，甚至必要。例如，在杂乱的家庭中导航时，机器人可能需要推开物体或打开门才能到达目的地。本文提出了一个系统的方法来研究这种交互式导航问题。<br><br>## 🚀 核心方法<br>💡 创新点1：交互式Gibson环境（iGibson 0.5）<br>本文提出了一个新的实验设置，即交互式Gibson环境（iGibson 0.5），它模拟了室内场景的高保真视觉，以及机器人和这些场景中常见物体的高保真物理动力学。这使得机器人可以与场景中的物体进行交互，例如推开物体以清除通往目标位置的路径。<br><br>💡 创新点2：交互式导航指标<br>本文提出了一套交互式导航指标，用于研究导航和物理交互之间的相互作用。这些指标包括路径效率、交互努力和交互式导航分数（INS），它们可以衡量机器人导航策略的效率和效果。<br><br>## 📈 实验结果<br>本文在交互式Gibson基准上评估了多个基于学习的基线，并提供了关于导航行为在不同路径效率和周围物体干扰之间的权衡的见解。结果表明，通过积极调整奖励函数中的交互惩罚，可以控制交互式导航分数，并且不同的INS值对应于不同的导航行为。<br><br>## 💬 可借鉴之处<br>本文提出的交互式Gibson基准和交互式导航指标为研究交互式导航问题提供了一个新的平台和工具。研究人员可以利用这个基准来评估和比较他们的交互式导航解决方案，并探索不同导航策略的权衡。此外，本文提出的交互式Gibson环境也可以用于研究和训练其他视觉导航、操作和移动操作解决方案。</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</td>
      <td>Large vision-language models (VLMs) have achieved substantial progress in<br>multimodal perception and reasoning. When integrated into an embodied agent,<br>existing embodied VLM works either output detailed action sequences at the<br>manipulation level or only provide plans at an abstract level, leaving a gap<br>between high-level planning and real-world manipulation. To bridge this gap, we<br>introduce Octopus, an embodied vision-language programmer that uses executable<br>code generation as a medium to connect planning and manipulation. Octopus is<br>designed to 1) proficiently comprehend an agent's visual and textual task<br>objectives, 2) formulate intricate action sequences, and 3) generate executable<br>code. To facilitate Octopus model development, we introduce OctoVerse: a suite<br>of environments tailored for benchmarking vision-based code generators on a<br>wide spectrum of tasks, ranging from mundane daily chores in simulators to<br>sophisticated interactions in complex video games such as Grand Theft Auto<br>(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an<br>explorative agent that generates training data, i.e., action blueprints and<br>corresponding executable code. We also collect feedback that enables an<br>enhanced training scheme called Reinforcement Learning with Environmental<br>Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's<br>functionality and present compelling results, showing that the proposed RLEF<br>refines the agent's decision-making. By open-sourcing our simulation<br>environments, dataset, and model architecture, we aspire to ignite further<br>innovation and foster collaborative applications within the broader embodied AI<br>community.</td>
      <td>## 🌟 论文解读 | Octopus：基于环境反馈的具身视觉-语言编程器<br><br>## 📌 背景痛点/本文动机<br>随着大型视觉-语言模型（VLMs）在多模态感知和推理方面取得显著进展，将它们集成到具身智能体中成为可能。然而，现有的具身VLM工作要么在操作层面输出详细的动作序列，要么仅在抽象层面提供计划，导致高级规划和现实世界操作之间存在差距。本文旨在解决这个问题，提出了一种名为Octopus的具身视觉-语言编程器，它使用可执行代码生成作为连接规划和操作的媒介。<br><br>## 🚀 核心方法<br>💡 创新点1：Octopus能够熟练地理解智能体的视觉和文本任务目标，制定复杂的动作序列，并生成可执行代码。<br>💡 创新点2：为了促进Octopus模型的发展，本文引入了OctoVerse，这是一套为在各种任务上评估基于视觉的代码生成器而量身定制的环境，包括从模拟器中的日常家务到复杂视频游戏（如GTA和Minecraft）中的复杂交互。<br>💡 创新点3：为了训练Octopus，本文利用GPT-4控制一个探索性智能体，生成训练数据，即动作蓝图和相应的可执行代码。同时，收集反馈，以实现一种称为环境反馈强化学习（RLEF）的增强训练方案。<br><br>## 📈 实验结果<br>通过一系列实验，本文展示了Octopus的功能，并展示了令人信服的结果，表明所提出的RLEF细化了智能体的决策。Octopus在各种场景中表现出强大的适应性，在任务规划、代码生成和执行方面优于现有模型。RLEF的集成进一步增强了Octopus的性能，展示了这种训练方法的有效性。<br><br>## 💬 可借鉴之处<br>本文提出的Octopus模型和OctoVerse环境为具身视觉-语言编程领域提供了新的思路和方法。Octopus模型的设计和训练过程可以借鉴到其他具身智能体中，以提高其在现实世界中的操作能力。OctoVerse环境可以用于评估和比较不同的具身视觉-语言编程模型，推动该领域的研究和发展。</td>
    </tr>
    <tr>
      <th>66</th>
      <td>Reward Design with Language Models</td>
      <td>Reward design in reinforcement learning (RL) is challenging since specifying<br>human notions of desired behavior may be difficult via reward functions or<br>require many expert demonstrations. Can we instead cheaply design rewards using<br>a natural language interface? This paper explores how to simplify reward design<br>by prompting a large language model (LLM) such as GPT-3 as a proxy reward<br>function, where the user provides a textual prompt containing a few examples<br>(few-shot) or a description (zero-shot) of the desired behavior. Our approach<br>leverages this proxy reward function in an RL framework. Specifically, users<br>specify a prompt once at the beginning of training. During training, the LLM<br>evaluates an RL agent's behavior against the desired behavior described by the<br>prompt and outputs a corresponding reward signal. The RL agent then uses this<br>reward to update its behavior. We evaluate whether our approach can train<br>agents aligned with user objectives in the Ultimatum Game, matrix games, and<br>the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents<br>trained with our framework are well-aligned with the user's objectives and<br>outperform RL agents trained with reward functions learned via supervised<br>learning</td>
      <td>## 🌟 论文解读 | 使用语言模型进行奖励设计<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）中的奖励设计一直是一个挑战，因为通过奖励函数来指定人类期望的行为可能很困难，或者需要大量的专家演示。本文提出了一种新的方法，使用大型语言模型（LLM）作为代理奖励函数，用户可以通过自然语言界面提供几个示例或描述来指定期望的行为，从而简化奖励设计。<br><br>## 🚀 核心方法<br>💡 创新点1：使用LLM作为代理奖励函数<br>本文的核心思想是利用LLM的上下文学习能力，将用户提供的文本提示（包含几个示例或描述）作为代理奖励函数，评估RL代理的行为是否符合用户的目标，并输出相应的奖励信号。<br><br>💡 创新点2：通用RL训练框架<br>本文提出了一种通用的RL训练框架，该框架利用代理奖励函数，并与所使用的RL算法无关。用户只需在训练开始时指定一次提示，LLM会在训练过程中评估代理的行为，并输出奖励信号，代理使用该奖励信号更新其行为。<br><br>## 📈 实验结果<br>本文在三个任务中评估了该方法的有效性：最后通牒游戏、矩阵游戏和DealOrNoDeal谈判任务。结果表明，使用LLM作为代理奖励函数训练的RL代理与用户的目标高度一致，并且在所有三个任务中都优于使用监督学习学习奖励函数的RL代理。<br><br>## 💬 可借鉴之处<br>本文提出的方法为用户提供了更直观和便捷的方式来指定RL代理的目标，并且可以有效地训练与用户目标一致的代理。该方法可以应用于各种RL任务，并有望推动人类兼容和价值对齐的AI系统的发展。</td>
    </tr>
    <tr>
      <th>110</th>
      <td>World Models with Hints of Large Language Models for Goal Achieving</td>
      <td>Reinforcement learning struggles in the face of long-horizon tasks and sparse<br>goals due to the difficulty in manual reward specification. While existing<br>methods address this by adding intrinsic rewards, they may fail to provide<br>meaningful guidance in long-horizon decision-making tasks with large state and<br>action spaces, lacking purposeful exploration. Inspired by human cognition, we<br>propose a new multi-modal model-based RL approach named Dreaming with Large<br>Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the<br>LLMs into the model rollouts to encourage goal discovery and reaching in<br>challenging tasks. By assigning higher intrinsic rewards to samples that align<br>with the hints outlined by the language model during model rollouts, DLLM<br>guides the agent toward meaningful and efficient exploration. Extensive<br>experiments demonstrate that the DLLM outperforms recent methods in various<br>challenging, sparse-reward environments such as HomeGrid, Crafter, and<br>Minecraft by 27.7\%, 21.1\%, and 9.9\%, respectively.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型提示的强化学习世界模型<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）在处理长期任务和稀疏目标时面临挑战，因为手动指定奖励函数非常困难。现有的方法通过添加内在奖励来解决这一问题，但在具有大型状态和动作空间的长期决策任务中，它们可能无法提供有意义的指导，缺乏有目的的探索。<br><br>## 🚀 核心方法<br>💡 创新点1：Dreaming with Large Language Models (DLLM)<br>DLLM 是一种新的多模态基于模型强化学习（MBRL）方法，它利用人类自然语言来描述环境动态，并将大型语言模型（LLM）的指导整合到模型滚动中，以提高代理的探索和目标完成能力。<br><br>💡 创新点2：基于 LLM 生成的目标，DLLM 可以通过自动递减机制生成有意义的内在奖励，以指导策略学习。<br><br>## 📈 实验结果<br>DLLM 在各种稀疏奖励环境中优于最近的方法，包括 HomeGrid、Crafter 和 Minecraft，分别提高了 27.7%、21.1% 和 9.9%。<br><br>## 💬 可借鉴之处<br>DLLM 的方法可以应用于各种复杂环境，并利用语言信息来提高代理的探索和目标完成能力。此外，DLLM 的自动递减机制可以有效地避免代理重复完成简单任务，从而促进代理探索更复杂的行为。</td>
    </tr>
    <tr>
      <th>63</th>
      <td>Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning</td>
      <td>Recent works successfully leveraged Large Language Models' (LLM) abilities to<br>capture abstract knowledge about world's physics to solve decision-making<br>problems. Yet, the alignment between LLMs' knowledge and the environment can be<br>wrong and limit functional competence due to lack of grounding. In this paper,<br>we study an approach (named GLAM) to achieve this alignment through functional<br>grounding: we consider an agent using an LLM as a policy that is progressively<br>updated as the agent interacts with the environment, leveraging online<br>Reinforcement Learning to improve its performance to solve goals. Using an<br>interactive textual environment designed to study higher-level forms of<br>functional grounding, and a set of spatial and navigation tasks, we study<br>several scientific questions: 1) Can LLMs boost sample efficiency for online<br>learning of various RL tasks? 2) How can it boost different forms of<br>generalization? 3) What is the impact of online learning? We study these<br>questions by functionally grounding several variants (size, architecture) of<br>FLAN-T5.</td>
      <td>## 🌟 论文解读 | GLAM：利用在线强化学习将大型语言模型嵌入交互式环境<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLM）在自然语言处理领域取得了显著进展，展现出强大的能力，例如自然语言生成、问答、推理和翻译等。然而，LLM在交互式环境中的功能能力受限，主要原因是其缺乏对环境的“接地”（grounding），即LLM的知识与环境的物理规则和动态之间的对齐不足。本文旨在通过功能接地的方法，将LLM嵌入交互式环境，并利用在线强化学习（RL）来提高其性能，从而解决决策问题。<br><br>## 🚀 核心方法<br>本文提出了GLAM（Grounded Language Models）方法，该方法将LLM作为智能体策略，在交互式文本环境中进行训练，并通过在线RL不断更新LLM的知识，以实现功能接地。具体来说，GLAM方法包括以下步骤：<br><br>1. **环境设计**：本文将BabyAI环境改编为文本版本（BabyAI-Text），其中智能体通过文本命令进行导航和交互。<br>2. **LLM作为策略**：将LLM作为智能体策略，通过接收任务描述、当前观察和可能动作的提示，输出动作的概率分布。<br>3. **在线RL训练**：利用环境提供的奖励信号，使用PPO算法对LLM进行在线RL训练，以优化其策略，使其能够更好地实现目标。<br><br>## 📈 实验结果<br>本文在BabyAI-Text环境中进行了多项实验，结果表明GLAM方法在以下方面取得了显著成果：<br><br>1. **样本效率**：与零样本使用LLM、监督微调和非预训练LLM的RL微调相比，GLAM方法在解决空间和导航任务方面表现出更高的样本效率。<br>2. **泛化能力**：GLAM方法训练的智能体能够将所学技能泛化到新的对象和任务，例如使用未见过的名词或动词，以及解决新的组合任务。<br>3. **在线学习的影响**：与离线行为克隆相比，在线RL训练能够更好地提高LLM的功能接地能力，使其能够更好地适应环境的变化。<br><br>## 💬 可借鉴之处<br>本文提出的GLAM方法为将LLM嵌入交互式环境并提高其功能能力提供了一种有效的方法。该方法具有以下可借鉴之处：<br><br>1. **LLM作为策略**：将LLM作为智能体策略，可以利用其强大的语言理解和生成能力，提高智能体在交互式环境中的表现。<br>2. **在线RL训练**：通过在线RL训练，可以使LLM不断学习和适应环境的变化，提高其泛化能力和鲁棒性。<br>3. **文本环境设计**：本文提出的BabyAI-Text环境为研究LLM的功能接地提供了良好的平台，可以用于评估LLM在交互式环境中的表现。<br><br>总而言之，本文提出的GLAM方法为将LLM嵌入交互式环境并提高其功能能力提供了一种有效的方法，并为未来研究LLM在交互式环境中的应用提供了新的思路。</td>
    </tr>
    <tr>
      <th>48</th>
      <td>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents</td>
      <td>Can world knowledge learned by large language models (LLMs) be used to act in<br>interactive environments? In this paper, we investigate the possibility of<br>grounding high-level tasks, expressed in natural language (e.g. "make<br>breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While<br>prior work focused on learning from explicit step-by-step examples of how to<br>act, we surprisingly find that if pre-trained LMs are large enough and prompted<br>appropriately, they can effectively decompose high-level tasks into mid-level<br>plans without any further training. However, the plans produced naively by LLMs<br>often cannot map precisely to admissible actions. We propose a procedure that<br>conditions on existing demonstrations and semantically translates the plans to<br>admissible actions. Our evaluation in the recent VirtualHome environment shows<br>that the resulting method substantially improves executability over the LLM<br>baseline. The conducted human evaluation reveals a trade-off between<br>executability and correctness but shows a promising sign towards extracting<br>actionable knowledge from language models. Website at<br>https://huangwl18.github.io/language-planner</td>
      <td>## 🌟 论文解读 | 语言模型作为零样本规划器：为具身智能体提取可操作知识<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在语言生成和理解方面取得了显著进展。这些模型在训练过程中学习了大量的世界知识，但如何将这些知识应用于交互式环境中的具身智能体，使其能够执行高层次的指令（例如“做早餐”）仍然是一个挑战。现有的研究主要集中在从明确的步骤示例中学习如何行动，而本文则探索了利用预训练的LLMs直接生成可执行行动计划的潜力。<br><br>## 🚀 核心方法<br>💡 创新点1：零样本规划<br>本文发现，如果预训练的LLMs足够大，并且以适当的方式进行提示，它们可以有效地将高层次的任务分解为可执行的中间步骤，而无需进一步的训练。这种方法被称为“零样本规划”，因为它不需要针对特定环境进行微调。<br><br>💡 创新点2：语义翻译和可执行性提升<br>然而，LLMs生成的计划往往无法精确映射到环境中的可接受动作。为了解决这个问题，本文提出了一种基于现有演示的语义翻译过程，将LLMs生成的计划转换为可接受的动作。此外，还引入了自回归轨迹校正和动态示例选择等技术，以进一步提高计划的执行性。<br><br>## 📈 实验结果<br>在VirtualHome环境中进行的评估表明，该方法显著提高了计划的执行性，但正确性有所下降。这表明，从语言模型中提取可操作知识是一个有前景的方向，但仍需进一步研究以平衡执行性和正确性。<br><br>## 💬 可借鉴之处<br>本文提出的零样本规划方法为利用LLMs的知识进行具身智能体的决策提供了新的思路。此外，语义翻译和可执行性提升技术也为提高LLMs生成的计划的实用性提供了有效的方法。这些发现对于未来研究如何将LLMs的知识应用于交互式环境中的具身智能体具有重要的启示意义。</td>
    </tr>
    <tr>
      <th>54</th>
      <td>Interactive Fiction Games: A Colossal Adventure</td>
      <td>A hallmark of human intelligence is the ability to understand and communicate<br>with language. Interactive Fiction games are fully text-based simulation<br>environments where a player issues text commands to effect change in the<br>environment and progress through the story. We argue that IF games are an<br>excellent testbed for studying language-based autonomous agents. In particular,<br>IF games combine challenges of combinatorial action spaces, language<br>understanding, and commonsense reasoning. To facilitate rapid development of<br>language-based agents, we introduce Jericho, a learning environment for<br>man-made IF games and conduct a comprehensive study of text-agents across a<br>rich set of games, highlighting directions in which agents can improve.</td>
      <td>## 🌟 论文解读 | 交互式小说游戏：一场巨大的冒险<br><br>## 📌 背景痛点/本文动机<br>人类智能的一个显著特征是理解和用语言进行交流的能力。交互式小说（IF）游戏是完全基于文本的模拟环境，玩家通过发出文本命令来改变环境并推动故事的发展。本文认为，IF游戏是研究基于语言的自主代理的绝佳测试平台。特别是，IF游戏结合了组合动作空间、语言理解和常识推理的挑战。为了促进基于语言的代理的快速开发，本文介绍了Jericho，这是一个用于人工IF游戏的学习环境，并在一系列丰富的游戏中对文本代理进行了全面的研究，突出了代理可以改进的方向。<br><br>## 🚀 核心方法<br>💡 创新点1：Jericho环境<br>Jericho是一个开源的Python-based IF环境，它提供了一个类似于OpenAI-Gym的接口，使学习代理能够连接到IF游戏。Jericho旨在用于强化学习代理，但也支持加载和保存游戏状态的功能，使规划算法如蒙特卡洛树搜索以及依赖于恢复状态的强化学习方法如Backplay和GoExplore成为可能。<br><br>💡 创新点2：基于模板的动作生成<br>本文引入了一种基于模板的动作空间，其中代理首先选择一个动作模板（例如，将_放入_），然后使用解析器的词汇表中的单词填写空白。这种模板化的动作空间大大减少了动作空间的复杂性，使得语言生成变得更加容易。<br><br>## 📈 实验结果<br>本文评估了DRRN、TDQN和NAIL三种代理在32个Jericho支持的游戏上的表现。结果表明，强化学习在许多不同的IF游戏中是可行的。与随机代理相比，DRRN和TDQN在游戏中的得分更高，这表明了语言生成的重要性。然而，与NAIL相比，DRRN和TDQN在游戏中的得分仍然较低，这表明了工程一个通用的IF代理的难度以及从数据中学习策略的潜力。<br><br>## 💬 可借鉴之处<br>本文提出的Jericho环境和基于模板的动作生成方法为研究基于语言的自主代理提供了新的思路。此外，本文对DRRN、TDQN和NAIL三种代理的评估结果为未来研究提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>72</th>
      <td>Benchmarking the Spectrum of Agent Capabilities</td>
      <td>Evaluating the general abilities of intelligent agents requires complex<br>simulation environments. Existing benchmarks typically evaluate only one narrow<br>task per environment, requiring researchers to perform expensive training runs<br>on many different environments. We introduce Crafter, an open world survival<br>game with visual inputs that evaluates a wide range of general abilities within<br>a single environment. Agents either learn from the provided reward signal or<br>through intrinsic objectives and are evaluated by semantically meaningful<br>achievements that can be unlocked during each episode, such as discovering<br>resources and crafting tools. Consistently unlocking all achievements requires<br>strong generalization, deep exploration, and long-term reasoning. We<br>experimentally verify that Crafter is of appropriate difficulty to drive future<br>research and provide baselines scores of reward agents and unsupervised agents.<br>Furthermore, we observe sophisticated behaviors emerging from maximizing the<br>reward signal, such as building tunnel systems, bridges, houses, and<br>plantations. We hope that Crafter will accelerate research progress by quickly<br>evaluating a wide spectrum of abilities.</td>
      <td>## 🌟 论文解读 | Crafter：评估智能体能力的全新基准<br><br>## 📌 背景痛点/本文动机<br>评估智能体的通用能力需要复杂的模拟环境。现有的基准通常在每个环境中只评估一个狭窄的任务，这要求研究人员在许多不同的环境中进行昂贵的训练运行。本文提出了 Crafter，一个具有视觉输入的开放世界生存游戏，它在一个环境中评估了广泛的通用能力。智能体可以通过提供的奖励信号或通过内在目标进行学习，并通过在每个回合中解锁的具有语义意义的成就来评估，例如发现资源和制作工具。持续解锁所有成就需要强大的泛化能力、深入的探索和长期推理。<br><br>## 🚀 核心方法<br>💡 创新点1：Crafter 是一个开放世界生存游戏，具有视觉输入，可以在单个环境中评估广泛的通用能力。<br>💡 创新点2：智能体可以通过提供的奖励信号或通过内在目标进行学习，并通过在每个回合中解锁的具有语义意义的成就来评估。<br>💡 创新点3：Crafter 提供了两个基准，一个允许智能体访问提供的奖励信号，另一个不允许，并要求智能体纯粹从内在目标中学习。<br>💡 创新点4：Crafter 定义了 22 个成就，这些成就对应于行为中的具有语义意义的里程碑，例如收集各种资源、建造物体和工具、寻找食物和水、击败怪物以及在睡觉后安全地醒来。<br><br>## 📈 实验结果<br>实验结果表明，Crafter 是一个具有挑战性的基准，当前的方法在 Crafter 上取得了学习进展，但未来还需要更多的研究才能实现高性能。DreamerV2、PPO 和 Rainbow 等顶级强化学习算法在 Crafter 上的得分分别为 10.0%、4.6% 和 4.3%，而人类专家的得分为 50.5%。这表明 Crafter 具有足够的难度，可以推动未来研究的发展。<br><br>## 💬 可借鉴之处<br>Crafter 是一个非常有价值的基准，可以用于评估智能体的通用能力。它具有以下优点：<br>- 在单个环境中评估广泛的通用能力，减少了计算需求。<br>- 提供了具有语义意义的成就，可以更全面地评估智能体的能力。<br>- 允许智能体通过提供的奖励信号或通过内在目标进行学习。<br>- 具有挑战性，可以推动未来研究的发展。<br><br>Crafter 的提出为评估智能体的通用能力提供了一个新的基准，并为未来研究的发展提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>41</th>
      <td>ProAgent: Building Proactive Cooperative Agents with Large Language Models</td>
      <td>Building agents with adaptive behavior in cooperative tasks stands as a<br>paramount goal in the realm of multi-agent systems. Current approaches to<br>developing cooperative agents rely primarily on learning-based methods, whose<br>policy generalization depends heavily on the diversity of teammates they<br>interact with during the training phase. Such reliance, however, constrains the<br>agents' capacity for strategic adaptation when cooperating with unfamiliar<br>teammates, which becomes a significant challenge in zero-shot coordination<br>scenarios. To address this challenge, we propose ProAgent, a novel framework<br>that harnesses large language models (LLMs) to create proactive agents capable<br>of dynamically adapting their behavior to enhance cooperation with teammates.<br>ProAgent can analyze the present state, and infer the intentions of teammates<br>from observations. It then updates its beliefs in alignment with the teammates'<br>subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of<br>modularity and interpretability, making it easily integrated into various of<br>coordination scenarios. Experimental evaluations conducted within the<br>Overcooked-AI environment unveil the remarkable performance superiority of<br>ProAgent, outperforming five methods based on self-play and population-based<br>training when cooperating with AI agents. Furthermore, in partnered with human<br>proxy models, its performance exhibits an average improvement exceeding 10%<br>compared to the current state-of-the-art method. For more information about our<br>project, please visit~\url{https://pku-proagent.github.io}.</td>
      <td>## 🌟 论文解读 | ProAgent：基于大型语言模型构建主动合作智能体<br><br>## 📌 背景痛点/本文动机<br>在多智能体系统中，构建具有自适应行为的合作智能体是一个重要的目标。然而，现有的基于学习的方法在策略泛化方面存在局限性，因为它们依赖于训练阶段与队友的多样性。当与不熟悉的队友合作时，这种依赖性限制了智能体进行战略适应的能力，这在零样本协调场景中成为一个重大挑战。<br><br>## 🚀 核心方法<br>为了解决这个问题，本文提出了ProAgent，一个利用大型语言模型（LLMs）创建主动智能体的新框架，这些智能体能够动态地调整其行为以增强与队友的合作。ProAgent可以分析当前状态，并从观察中推断队友的意图。然后，它通过比较队友的后续实际行为来更新其信念。此外，ProAgent具有高度的模块化和可解释性，可以轻松地集成到各种协调场景中。<br><br>ProAgent框架包括四个关键模块：规划器、验证器、控制器和记忆模块，以及信念修正机制。这些模块协同工作，使ProAgent能够主动预测队友的意图，并在没有预先训练或微调的情况下实现自适应的合作推理和规划。<br><br>## 📈 实验结果<br>在Overcooked-AI环境中进行的实验评估揭示了ProAgent的卓越性能。在与AI智能体合作时，ProAgent优于基于自我游戏和基于种群训练的五种方法。此外，在与人类代理模型合作时，其性能平均提高了超过10%，与当前最先进的方法相比。<br><br>## 💬 可借鉴之处<br>ProAgent框架为利用LLMs在合作场景中的强大推理和规划能力提供了一个全面的指南。它展示了LLMs在解释当前场景、明确推断队友意图以及相应地动态调整行为方面的显著能力。这些结果为构建更高效的合作场景提供了有价值的见解，并为开发更先进的合作多智能体系统和人机兼容的AI系统铺平了道路。</td>
    </tr>
    <tr>
      <th>73</th>
      <td>Keep CALM and Explore: Language Models for Action Generation in Text-based Games</td>
      <td>Text-based games present a unique challenge for autonomous agents to operate<br>in natural language and handle enormous action spaces. In this paper, we<br>propose the Contextual Action Language Model (CALM) to generate a compact set<br>of action candidates at each game state. Our key insight is to train language<br>models on human gameplay, where people demonstrate linguistic priors and a<br>general game sense for promising actions conditioned on game history. We<br>combine CALM with a reinforcement learning agent which re-ranks the generated<br>action candidates to maximize in-game rewards. We evaluate our approach using<br>the Jericho benchmark, on games unseen by CALM during training. Our method<br>obtains a 69% relative improvement in average game score over the previous<br>state-of-the-art model. Surprisingly, on half of these games, CALM is<br>competitive with or better than other models that have access to ground truth<br>admissible actions. Code and data are available at<br>https://github.com/princeton-nlp/calm-textgame.</td>
      <td>## 🌟 论文解读 | 语言模型助力文本游戏：CALM模型解析<br><br>## 📌 背景痛点/本文动机<br>文本游戏为自主智能体提供了独特的挑战，要求它们在自然语言环境中操作并处理巨大的动作空间。现有的强化学习模型在处理这些游戏时，由于动作空间组合爆炸，学习收敛速度慢，且缺乏对游戏状态的语义理解，导致难以有效探索和选择最优动作。<br><br>## 🚀 核心方法<br>💡 创新点1：CALM模型<br>本文提出了一个名为“上下文动作语言模型”（CALM）的模型，用于在每个游戏状态下生成一组紧凑的动作候选集。CALM模型通过在人类游戏玩法上进行训练，捕捉人类玩家在游戏历史条件下对有前景动作的语言先验和一般游戏感。<br><br>💡 创新点2：强化学习与CALM的结合<br>CALM模型与强化学习智能体相结合，该智能体使用游戏奖励重新排序生成的动作候选集，以最大化游戏内奖励。这种结合使得模型能够将通用的语言先验与适应性地选择最适合游戏的动作的能力相结合。<br><br>## 📈 实验结果<br>在Jericho基准测试中，CALM模型在未见过的游戏中取得了69%的平均游戏分数相对提升，超过了之前最先进的模型。令人惊讶的是，在半数游戏中，CALM模型甚至与其他能够访问真实有效动作的模型相媲美或更优。<br><br>## 💬 可借鉴之处<br>本文提出的CALM模型为文本游戏中的动作生成提供了一个新的思路，通过结合语言模型和强化学习，有效地缩小了动作空间，并提高了游戏性能。此外，本文还引入了一个新的数据集，用于评估动作生成的质量，为文本游戏研究提供了宝贵的资源。</td>
    </tr>
    <tr>
      <th>90</th>
      <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
      <td>Interactive Fiction (IF) games with real human-written natural language texts<br>provide a new natural evaluation for language understanding techniques. In<br>contrast to previous text games with mostly synthetic texts, IF games pose<br>language understanding challenges on the human-written textual descriptions of<br>diverse and sophisticated game worlds and language generation challenges on the<br>action command generation from less restricted combinatorial space. We take a<br>novel perspective of IF game solving and re-formulate it as Multi-Passage<br>Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query<br>attention mechanisms and the structured prediction in MPRC to efficiently<br>generate and evaluate action outputs and apply an object-centric historical<br>observation retrieval strategy to mitigate the partial observability of the<br>textual observations. Extensive experiments on the recent IF benchmark<br>(Jericho) demonstrate clear advantages of our approaches achieving high winning<br>rates and low data requirements compared to all previous approaches. Our source<br>code is available at: https://github.com/XiaoxiaoGuo/rcdqn.</td>
      <td>## 🌟 论文解读 | 利用多段落阅读理解与强化学习解决互动式小说游戏<br><br>## 📌 背景痛点/本文动机<br>互动式小说（Interactive Fiction, IF）游戏以其丰富的文本描述和复杂的游戏世界为自然语言理解（NLU）技术提供了新的挑战。与以往主要使用合成文本的文本游戏不同，IF 游戏中的文本描述更加多样化和复杂，对语言理解提出了更高的要求。此外，IF 游戏中的行动命令生成也面临着更大的挑战，因为玩家可以使用的行动命令组合空间更加开放和自由。<br><br>## 🚀 核心方法<br>💡 创新点1：将 IF 游戏解决过程重新定义为多段落阅读理解（MPRC）任务。这种方法利用了 MPRC 中的上下文-查询注意力机制和结构化预测，可以有效地生成和评估行动输出，并应用以对象为中心的历史观察检索策略来缓解文本观察的局部可观测性。<br><br>💡 创新点2：提出了一种基于阅读理解模型（RC）的模板行动预测模型。该模型将观察视为段落，将模板动词短语视为问题，并将模板中对象占位符的填充视为从观察中提取对象的提取式问答（QA）问题。同时，每个行动（即所有占位符都被替换的模板）都通过 RC 模型预测其评估值。<br><br>💡 创新点3：提出了一种基于对象的历史观察检索策略。该方法根据当前观察中检测到的对象，检索最近 K 个至少共享一个对象的观察。这些检索到的观察按时间步排序并连接到当前观察，以增强当前观察的信息。<br><br>## 📈 实验结果<br>在 Jericho IF 游戏基准测试中，与所有先前的方法相比，本文提出的方法在 25 个游戏中的胜率达到了 64%，并且所需的数据量不到先前方法的十分之一。此外，本文还进行了消融研究，结果表明，RC 模型设计和检索策略对性能提升起着重要作用。<br><br>## 💬 可借鉴之处<br>本文提出的将 IF 游戏解决过程重新定义为 MPRC 任务的方法，为解决 IF 游戏中的巨大组合行动空间和局部可观测性挑战提供了一种新的思路。此外，本文提出的基于 RC 模型的模板行动预测模型和基于对象的历史观察检索策略，也为解决其他 NLU 任务提供了可借鉴的经验。</td>
    </tr>
    <tr>
      <th>80</th>
      <td>GameEval: Evaluating LLMs on Conversational Games</td>
      <td>The rapid advancements in large language models (LLMs) have presented<br>challenges in evaluating those models. Existing evaluation methods are either<br>reference-based or preference based, which inevitably need human intervention<br>or introduce test bias caused by evaluator models. In this paper, we propose<br>GameEval, a novel approach to evaluating LLMs through goal-driven<br>conversational games, overcoming the limitations of previous methods. GameEval<br>treats LLMs as game players and assigns them distinct roles with specific goals<br>achieved by launching conversations of various forms, including discussion,<br>question answering, and voting. We design three unique games with cooperative<br>or adversarial objectives, accompanied by corresponding evaluation metrics, to<br>show how this new paradigm comprehensively evaluates model performance.Through<br>extensive experiments, we show that GameEval can effectively differentiate the<br>capabilities of various LLMs, providing a comprehensive assessment of their<br>integrated abilities to solve complex problems. Our public anonymous code is<br>available at https://github.com/GameEval/GameEval.</td>
      <td>## 🌟 论文解读 | GameEval：通过对话游戏评估大型语言模型<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，如何有效地评估这些模型的能力成为一个挑战。现有的评估方法主要分为两类：基于参考和基于偏好的方法。基于参考的方法需要与预先确定的答案进行比较，而基于偏好的方法则依赖于人类或模型评估者的偏好。这两种方法都存在局限性，例如获取高质量标注的成本高、时间消耗大，以及引入评估者的偏好偏差等。<br><br>## 🚀 核心方法<br>本文提出了GameEval，一种通过目标驱动的对话游戏来评估LLMs的新方法。GameEval将LLMs视为游戏玩家，并为其分配具有特定目标的独特角色，通过启动各种形式的对话（包括讨论、问答和投票）来实现这些目标。本文设计了三种独特的游戏，包括合作和对抗目标，并配备了相应的评估指标，以展示这种新范式如何全面评估模型性能。<br><br>## 📈 实验结果<br>通过广泛的实验，本文展示了GameEval能够有效地区分不同LLMs的能力，并提供对其解决复杂问题综合能力的全面评估。实验结果表明，GameEval在区分ChatGPT和GPT-4等模型的能力方面表现出色，而现有方法则难以做到这一点。<br><br>## 💬 可借鉴之处<br>GameEval提供了一种新的评估LLMs的方法，可以更全面地评估模型的能力，并减少评估偏差。此外，GameEval还可以用于设计新的游戏，以评估LLMs在现实世界复杂场景中的能力。</td>
    </tr>
    <tr>
      <th>115</th>
      <td>THREAD: Thinking Deeper with Recursive Spawning</td>
      <td>Large language models (LLMs) have shown impressive capabilities across<br>diverse settings, but still struggle as the length and complexity of the<br>context increases. To address this challenge, we propose Thinking Recursively<br>and Dynamically (ThReaD). THREAD frames model generation as a thread of<br>execution that, based on the context, can run to completion or dynamically<br>spawn new threads. By spawning, threads can offload work (e.g., thinking,<br>retrieving information) to child threads, which only return tokens needed for<br>the parent thread to do its work. In effect, this enables the model to adapt,<br>as needed, the amount of intermediate work used to produce tokens. We apply<br>THREAD in the settings of LLM task solving and question answering, where the<br>dynamic threading allows the model to recursively decompose the given task or<br>question into progressively simpler sub-problems that can be solved by separate<br>child threads. We test THREAD, implemented using a few-shot learning approach,<br>on diverse benchmarks for agent tasks and data-grounded question answering.<br>THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these<br>benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new<br>benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD<br>outperforms existing frameworks by 10% to 50% absolute points with smaller<br>models, including Llama-3-8b and CodeLlama-7b.</td>
      <td>## 🌟 论文解读 | THREAD：递归分叉，深度思考<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在处理复杂任务时，随着上下文长度和复杂性的增加，其性能会下降。这是因为LLMs需要将所有必要的思考和信息检索工作都压缩在一条简洁的生成语句中，这限制了它们在需要更多工作（如思考、检索信息、分析等）的场景中的有效性。<br><br>## 🚀 核心方法<br>本文提出了一个名为“递归分叉和动态思考”（ThReaD）的通用框架，该框架将模型生成视为一个执行线程，该线程可以根据上下文独立运行到完成或动态地分叉成新的线程。通过分叉，线程可以将工作（例如，思考、检索信息）卸载到子线程，而子线程只返回父线程完成其工作所需的信息。这使模型能够根据需要动态地调整用于生成不同部分标记序列的中间工作量。<br><br>## 📈 实验结果<br>在LLM任务解决和数据驱动问答的设置中，THREAD通过动态分叉允许模型递归地将给定的任务或问题分解为逐步简化的子问题，这些子问题可以由单独的子线程解决。在多个基准测试中，THREAD实现了最先进的性能，包括ALFWorld、TextCraft和WebShop，以及两个新的基准测试DataCommons QA和MIMIC-III ICU QA。此外，THREAD在小模型上优于现有框架，包括Llama-3-8b和CodeLlama-7b，性能提高了10%到50%。<br><br>## 💬 可借鉴之处<br>THREAD框架提供了一种灵活的方法，使LLMs能够动态地适应其工作量和中间计算步骤，从而更好地处理复杂任务。此外，THREAD框架的通用性使其适用于各种场景，包括多模态应用，并且可以用于各种目的，例如编写程序、执行计算、增强数据、生成想法、检索信息、与环境交互、机器人操作等。</td>
    </tr>
    <tr>
      <th>64</th>
      <td>Proximal Policy Optimization Algorithms</td>
      <td>We propose a new family of policy gradient methods for reinforcement<br>learning, which alternate between sampling data through interaction with the<br>environment, and optimizing a "surrogate" objective function using stochastic<br>gradient ascent. Whereas standard policy gradient methods perform one gradient<br>update per data sample, we propose a novel objective function that enables<br>multiple epochs of minibatch updates. The new methods, which we call proximal<br>policy optimization (PPO), have some of the benefits of trust region policy<br>optimization (TRPO), but they are much simpler to implement, more general, and<br>have better sample complexity (empirically). Our experiments test PPO on a<br>collection of benchmark tasks, including simulated robotic locomotion and Atari<br>game playing, and we show that PPO outperforms other online policy gradient<br>methods, and overall strikes a favorable balance between sample complexity,<br>simplicity, and wall-time.</td>
      <td>## 🌟 论文解读 | Proximal Policy Optimization Algorithms：简化强化学习中的策略梯度方法<br><br>## 📌 背景痛点/本文动机<br>近年来，强化学习领域涌现出多种基于神经网络函数逼近器的方法，如深度Q学习、标准策略梯度方法和信任区域/自然策略梯度方法。然而，这些方法在可扩展性、数据效率和鲁棒性方面仍有改进空间。例如，深度Q学习在许多简单问题上表现不佳，标准策略梯度方法数据效率低下且鲁棒性差，而信任区域策略优化（TRPO）相对复杂，且不兼容包含噪声（如dropout）或参数共享（如策略和价值函数之间，或与辅助任务之间）的架构。<br><br>## 🚀 核心方法<br>本文提出了一种名为近端策略优化（PPO）的新策略梯度方法，旨在解决上述问题。PPO具有以下创新点：<br><br>💡 创新点1：交替采样和优化<br>PPO交替在环境中采样数据并通过与环境的交互，然后使用随机梯度上升优化一个“代理”目标函数。与标准策略梯度方法每数据样本执行一次梯度更新不同，PPO提出了一种新的目标函数，允许进行多个epoch的小批量更新。<br><br>💡 创新点2：截断概率比<br>PPO使用截断概率比来形成对策略性能的悲观估计（即下界）。通过这种方式，PPO能够在保证策略更新的稳定性和可靠性的同时，简化实现过程，使其更通用，并提高样本复杂度。<br><br>## 📈 实验结果<br>本文在一系列基准任务上测试了PPO，包括模拟机器人运动和Atari游戏。结果表明，PPO优于其他在线策略梯度方法，并在样本复杂度、简单性和运行时间之间取得了良好的平衡。<br><br>## 💬 可借鉴之处<br>PPO是一种简单、高效且鲁棒的策略梯度方法，适用于各种强化学习任务。其创新点包括交替采样和优化以及截断概率比，这些方法可以应用于其他策略梯度方法，以提高其性能和鲁棒性。</td>
    </tr>
    <tr>
      <th>61</th>
      <td>LLaMA Rider: Spurring Large Language Models to Explore the Open World</td>
      <td>Recently, various studies have leveraged Large Language Models (LLMs) to help<br>decision-making and planning in environments, and try to align the LLMs'<br>knowledge with the world conditions. Nonetheless, the capacity of LLMs to<br>continuously acquire environmental knowledge and adapt in an open world remains<br>uncertain. In this paper, we propose an approach to spur LLMs to explore the<br>open world, gather experiences, and learn to improve their task-solving<br>capabilities. In this approach, a multi-round feedback-revision mechanism is<br>utilized to encourage LLMs to actively select appropriate revision actions<br>guided by feedback information from the environment. This facilitates<br>exploration and enhances the model's performance. Besides, we integrate<br>sub-task relabeling to assist LLMs in maintaining consistency in sub-task<br>planning and help the model learn the combinatorial nature between tasks,<br>enabling it to complete a wider range of tasks through training based on the<br>acquired exploration experiences. By evaluation in Minecraft, an open-ended<br>sandbox world, we demonstrate that our approach LLaMA-Rider enhances the<br>efficiency of the LLM in exploring the environment, and effectively improves<br>the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k<br>instances of collected data, showing minimal training costs compared to the<br>baseline using reinforcement learning.</td>
      <td>## 🌟 论文解读 | LLaMA Rider：激发大型语言模型探索开放世界<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在模拟人类智能方面取得了显著进展。许多研究开始利用LLMs的能力来帮助智能体在环境中进行决策，并发现LLMs具有一定的规划和完成任务的能力。然而，LLMs的知识来源于预训练时使用的语言语料库，可能与特定环境存在差异。为了将LLMs与实际环境相结合，一些研究通过提示工程设计特定机制，为LLMs提供环境信息。然而，LLMs在环境中并不会改进或获取新知识。此外，对于更复杂的任务，需要更复杂的机制和提示，这会导致LLMs生成成本高，并且依赖于像GPT-4这样具有足够知识的强大模型。还有一些研究通过微调来将LLMs与实际环境相结合，但这通常需要依赖于特定任务的训练数据集。强化学习（RL）方法也被研究，但这些方法将LLMs训练为特定任务的策略，并且我们发现RL方法难以扩展到更大的模型或更复杂的任务。<br><br>## 🚀 核心方法<br>本文提出了一种名为LLaMA-Rider的方法，旨在通过LLMs在开放环境中的探索来增强其能力。LLaMA-Rider是一个两阶段的学习框架，包括探索阶段和学习阶段。<br><br>### 💡 创新点1：探索阶段<br>在探索阶段，LLaMA-Rider利用反馈-修正机制来鼓励LLMs主动选择适当的修正动作，以适应环境。LLMs在环境中进行探索，收集经验，并通过反馈信息来改进其决策。此外，LLaMA-Rider还使用子任务重标记来帮助LLMs保持子任务规划的连贯性，并学习任务之间的组合性质。<br><br>### 💡 创新点2：学习阶段<br>在学习阶段，LLaMA-Rider将收集到的经验处理成数据集，并使用监督微调（SFT）来训练LLMs。除了从成功任务中获得的经验外，LLaMA-Rider还收集部分完成的子任务的经验，因为有些任务在探索阶段很难完成。开放环境中的许多任务通常具有组合性，这意味着过去任务的经验可以经常帮助完成其他任务。LLaMA-Rider使用子任务重标记来提高数据利用率，并帮助LLMs学习任务之间的组合性。<br><br>## 📈 实验结果<br>本文在Minecraft模拟器MineDojo上评估了LLaMA-Rider方法。实验结果表明，LLaMA-Rider能够有效地探索环境，并通过微调仅使用1.3k个收集到的数据实例来提高LLMs完成任务的能力，与使用强化学习的方法相比，训练成本更低。<br><br>## 💬 可借鉴之处<br>LLaMA-Rider方法为LLMs在开放环境中的探索和学习提供了一种有效的方法。其反馈-修正机制和子任务重标记技术可以帮助LLMs更好地适应环境，并提高其完成任务的能力。此外，LLaMA-Rider方法还可以扩展到其他开放环境，并具有终身探索和学习的潜力。</td>
    </tr>
    <tr>
      <th>69</th>
      <td>Automated Feature Selection for Inverse Reinforcement Learning</td>
      <td>Inverse reinforcement learning (IRL) is an imitation learning approach to<br>learning reward functions from expert demonstrations. Its use avoids the<br>difficult and tedious procedure of manual reward specification while retaining<br>the generalization power of reinforcement learning. In IRL, the reward is<br>usually represented as a linear combination of features. In continuous state<br>spaces, the state variables alone are not sufficiently rich to be used as<br>features, but which features are good is not known in general. To address this<br>issue, we propose a method that employs polynomial basis functions to form a<br>candidate set of features, which are shown to allow the matching of statistical<br>moments of state distributions. Feature selection is then performed for the<br>candidates by leveraging the correlation between trajectory probabilities and<br>feature expectations. We demonstrate the approach's effectiveness by recovering<br>reward functions that capture expert policies across non-linear control tasks<br>of increasing complexity. Code, data, and videos are available at<br>https://sites.google.com/view/feature4irl.</td>
      <td>## 🌟 论文解读 | 自动化特征选择在逆强化学习中的应用<br><br>## 📌 背景痛点/本文动机<br>在逆强化学习（IRL）中，从专家演示中学习奖励函数是一个关键步骤。然而，手动指定奖励函数既困难又耗时，且容易引入错误假设。此外，在连续状态空间中，状态变量本身不足以作为特征，而哪些特征是好的通常并不清楚。<br><br>## 🚀 核心方法<br>本文提出了一个自动化特征选择方法，用于逆强化学习中的奖励函数学习。主要创新点如下：<br><br>💡 创新点1：使用多项式基函数作为候选特征集，这些函数能够匹配状态分布的统计矩，从而有效地捕捉专家策略。<br><br>💡 创新点2：开发了一种基于相关性的特征选择机制，自动选择最相关的特征子集，以减少奖励复杂性并减轻噪声和虚假相关性的影响。<br><br>## 📈 实验结果<br>在三个不同的连续控制任务（摆动、推车和双臂机器人）上进行的实验表明，该方法能够有效地恢复奖励函数，并生成与专家策略相似的政策。与手动选择特征、随机选择特征、直接使用状态作为特征以及使用所有候选特征的方法相比，该方法在所有任务中都取得了更好的性能。<br><br>## 💬 可借鉴之处<br>本文提出的自动化特征选择方法为逆强化学习中的奖励函数学习提供了一种有效且高效的解决方案。该方法可以应用于各种连续状态空间，并有助于提高模型的解释性和鲁棒性。此外，该方法还可以扩展到其他基础函数，如傅里叶级数和径向基函数，以进一步提高其适用性和精度。</td>
    </tr>
    <tr>
      <th>89</th>
      <td>Graph Constrained Reinforcement Learning for Natural Language Action Spaces</td>
      <td>Interactive Fiction games are text-based simulations in which an agent<br>interacts with the world purely through natural language. They are ideal<br>environments for studying how to extend reinforcement learning agents to meet<br>the challenges of natural language understanding, partial observability, and<br>action generation in combinatorially-large text-based action spaces. We present<br>KG-A2C, an agent that builds a dynamic knowledge graph while exploring and<br>generates actions using a template-based action space. We contend that the dual<br>uses of the knowledge graph to reason about game state and to constrain natural<br>language generation are the keys to scalable exploration of combinatorially<br>large natural language actions. Results across a wide variety of IF games show<br>that KG-A2C outperforms current IF agents despite the exponential increase in<br>action space size.</td>
      <td>## 🌟 论文解读 | 图约束强化学习在自然语言动作空间中的应用<br><br>## 📌 背景痛点/本文动机<br>自然语言交互长期以来被认为是人类智能的一个标志性特征。本文旨在研究如何使学习代理能够理解和生成与上下文相关的自然语言，以实现目标。为了实现这一目标，本文研究了交互式小说（IF）游戏，这是一种文本冒险游戏，其中代理完全通过自然语言与游戏世界进行交互。IF游戏是研究如何扩展强化学习代理以应对自然语言理解、部分可观察性和在组合性大的文本动作空间中生成动作的挑战的理想环境。<br><br>## 🚀 核心方法<br>本文提出了KG-A2C，一种在探索过程中构建动态知识图并使用基于模板的动作空间生成动作的代理。本文认为，知识图的双重用途，即用于推理游戏状态和约束自然语言生成，是可扩展地探索组合性大的自然语言动作的关键。<br><br>### 💡 创新点1：知识图状态空间<br>KG-A2C使用知识图作为状态表示，该知识图在探索过程中学习。知识图存储为一系列三元组，包括主体、关系和对象。这些三元组从观察中提取，并使用斯坦福大学的开放信息提取（OpenIE）工具进行更新。知识图帮助代理形成它正在探索的世界的地图，并保留它已经学习到的信息，例如与对象相关的功能、角色的属性、当前库存等。<br><br>### 💡 创新点2：模板动作空间<br>KG-A2C使用基于模板的动作空间，其中代理首先选择一个模板，然后使用游戏词汇表中的单词填写空白。模板和词汇表单词通过Jericho框架程序化地可用，因此对于每个IF游戏都是可用的。模板为动作空间提供了结构，使得代理能够有效地探索动作空间。<br><br>## 📈 实验结果<br>本文在一系列IF游戏上测试了KG-A2C，并将其与当前IF代理进行了比较。结果表明，KG-A2C在大多数游戏中都优于当前IF代理，尽管动作空间的大小呈指数级增长。<br><br>## 💬 可借鉴之处<br>本文提出的KG-A2C方法为自然语言交互式游戏提供了一个有效的解决方案。知识图状态空间和模板动作空间的结合使得代理能够有效地探索组合性大的自然语言动作空间。本文的研究结果对于开发更智能的自然语言交互式游戏代理具有重要的参考价值。</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。<br><br>## 🚀 核心方法<br>为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：<br>1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。<br>2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。<br>3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。<br><br>## 💬 可借鉴之处<br>Steve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Voyager: An Open-Ended Embodied Agent with Large Language Models</td>
      <td>We introduce Voyager, the first LLM-powered embodied lifelong learning agent<br>in Minecraft that continuously explores the world, acquires diverse skills, and<br>makes novel discoveries without human intervention. Voyager consists of three<br>key components: 1) an automatic curriculum that maximizes exploration, 2) an<br>ever-growing skill library of executable code for storing and retrieving<br>complex behaviors, and 3) a new iterative prompting mechanism that incorporates<br>environment feedback, execution errors, and self-verification for program<br>improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses<br>the need for model parameter fine-tuning. The skills developed by Voyager are<br>temporally extended, interpretable, and compositional, which compounds the<br>agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,<br>Voyager shows strong in-context lifelong learning capability and exhibits<br>exceptional proficiency in playing Minecraft. It obtains 3.3x more unique<br>items, travels 2.3x longer distances, and unlocks key tech tree milestones up<br>to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill<br>library in a new Minecraft world to solve novel tasks from scratch, while other<br>techniques struggle to generalize. We open-source our full codebase and prompts<br>at https://voyager.minedojo.org/.</td>
      <td>## 🌟 论文解读 | Voyager：基于大型语言模型的开放式具身终身学习智能体<br><br>## 📌 背景痛点/本文动机<br>构建能够在开放世界中持续探索、规划和开发新技能的通用具身智能体，是人工智能领域的一大挑战。传统的强化学习和模仿学习方法在探索、可解释性和泛化方面存在局限性。近年来，基于大型语言模型（LLM）的智能体利用预训练LLM中的世界知识生成一致的行动计划或可执行策略，但它们并非终身学习者，无法在长时间跨度内逐步获取、更新、积累和转移知识。<br><br>## 🚀 核心方法<br>Voyager 是第一个由 LLM 驱动的具身终身学习智能体，能够在 Minecraft 中持续探索世界、获取多样化技能，并在没有人类干预的情况下进行新的发现。Voyager 由三个关键组件组成：<br><br>💡 创新点1：自动课程<br>Voyager 通过自动课程进行开放式探索，该课程由 GPT-4 生成，旨在“发现尽可能多的多样化事物”。课程会根据探索进度和智能体的状态提出越来越难的任务，从而推动智能体不断学习新技能。<br><br>💡 创新点2：技能库<br>Voyager 拥有一个不断增长的技能库，用于存储和检索可执行代码，以存储和检索复杂的行为。每个技能都由可执行代码表示，这些代码可以自然地表示时间扩展和组合动作，这对于 Minecraft 中的许多长期任务至关重要。<br><br>💡 创新点3：迭代提示机制<br>Voyager 通过迭代提示机制生成可执行代码，该机制利用环境反馈、执行错误和自我验证来改进程序。该机制通过执行生成的程序、获取环境反馈和执行错误，并将这些反馈纳入 GPT-4 的提示中，从而进行代码改进。这个过程会重复进行，直到自我验证模块确认任务完成，此时将程序添加到技能库中，并查询自动课程以获取下一个目标。<br><br>## 📈 实验结果<br>Voyager 在 MineDojo 框架中与其他 LLM 基于智能体技术进行了比较，结果表明 Voyager 在发现新物品、解锁 Minecraft 技术树、穿越各种地形以及将学习到的技能库应用于新世界中的未见任务方面表现出色。Voyager 获得了 3.3 倍的新物品，解锁关键技术树里程碑的速度提高了 15.3 倍，穿越的距离是基线的 2.3 倍。<br><br>## 💬 可借鉴之处<br>Voyager 的设计为开发强大的通用智能体提供了一个起点，无需调整模型参数。其自动课程、技能库和迭代提示机制为终身学习智能体的开发提供了新的思路。此外，Voyager 的技能库可以作为其他方法的即插即用资产，有效地提高性能。</td>
    </tr>
    <tr>
      <th>105</th>
      <td>MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents</td>
      <td>Significant advancements have occurred in the application of Large Language<br>Models (LLMs) for various tasks and social simulations. Despite this, their<br>capacities to coordinate within task-oriented social contexts are<br>under-explored. Such capabilities are crucial if LLMs are to effectively mimic<br>human-like social behavior and produce meaningful results. To bridge this gap,<br>we introduce collaborative generative agents, endowing LLM-based Agents with<br>consistent behavior patterns and task-solving abilities. We situate these<br>agents in a simulated job fair environment as a case study to scrutinize their<br>coordination skills. We propose a novel framework that equips collaborative<br>generative agents with human-like reasoning abilities and specialized skills.<br>Our evaluation demonstrates that these agents show promising performance.<br>However, we also uncover limitations that hinder their effectiveness in more<br>complex coordination tasks. Our work provides valuable insights into the role<br>and evolution of LLMs in task-oriented social simulations.</td>
      <td>## 🌟 论文解读 | MetaAgents：基于LLM的任务导向协调的协作生成式智能体<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们在模拟人类行为和执行任务方面的能力日益增强。然而，LLMs在任务导向的社会环境中的协调能力尚未得到充分探索。为了使LLMs能够有效地模拟人类的社会行为并产生有意义的结果，这种能力至关重要。<br><br>## 🚀 核心方法<br>本文提出了协作生成式智能体（Collaborative Generative Agents），为基于LLM的智能体赋予了一致的行为模式和任务解决能力。为了研究这些智能体的协调能力，本文构建了一个模拟的招聘会环境，并提出了一个包含感知、记忆、推理和执行模块的框架。该框架使协作生成式智能体具备类似人类的推理能力和专业技能。<br><br>## 📈 实验结果<br>在模拟的招聘会环境中，协作生成式智能体在识别合格求职者、设计工作流程和分配角色方面表现出良好的性能。然而，随着招聘会复杂性的增加，智能体在协调方面遇到了挑战，这主要归因于LLMs的目标或意图不匹配。<br><br>## 💬 可借鉴之处<br>本文提出的协作生成式智能体框架为LLMs在任务导向的社会模拟中的应用提供了有价值的见解。该框架可以应用于各种场景，例如招聘、团队协作和社交网络模拟。此外，本文还揭示了LLMs在协调任务中面临的挑战，为未来的研究提供了方向。</td>
    </tr>
    <tr>
      <th>49</th>
      <td>Software Agents Interaction Algorithms in Virtual Learning Environment</td>
      <td>This paper highlights the multi-agent learning virtual environment and agents<br>communication algorithms. The researcher proposed three algorithms required<br>software agents interaction in virtual learning information system environment.<br>The first proposed algorithm is agents interaction localization algorithm, the<br>second one is the dynamic agents distribution algorithm (load distribution<br>algorithm), and the third model is Agent communication algorithm based on using<br>agents intermediaries. The main objectives of these algorithms are to reduce<br>the response time for any agents changes in virtual learning environment (VLE)<br>by increasing the information exchange intensity between software agents and<br>reduce the overall network load, and to improve the communication between<br>mobile agents in distributed information system to support effectiveness.<br>Finally the paper describe the algorithms of information exchange between<br>mobile agents in VLE based on the expansion of the address structure and the<br>use of an agent, intermediary agents, matchmaking agents, brokers and their<br>entrepreneurial functions</td>
      <td>## 🌟 论文解读 | 虚拟学习环境中软件代理交互算法的革新<br><br>## 📌 背景痛点/本文动机<br>随着信息技术的飞速发展，虚拟学习环境（VLE）已经成为教育领域的重要工具。然而，现有的VLE系统在处理大量信息交互时，面临着网络负载增加、信息交换效率低下等问题。为了解决这些问题，本文提出了一种基于软件代理的交互算法，旨在提高VLE系统的效率和响应速度。<br><br>## 🚀 核心方法<br>💡 创新点1：代理交互定位算法<br>该算法通过分析代理之间的通信依赖性，将频繁交互的代理分组到同一主机上，从而将跨主机的交互转化为主机内的交互，减少网络负载并提高信息交换效率。<br><br>💡 创新点2：动态代理分配算法（负载分配算法）<br>该算法通过监控主机负载，将代理分组并动态分配到不同的主机上，以实现负载均衡，避免某些主机过载，从而提高系统的稳定性和性能。<br><br>💡 创新点3：基于代理中介的通信模型<br>该模型利用代理中介（如经纪人代理和配对代理）来促进代理之间的通信。代理中介可以帮助代理查找具有相似兴趣的代理，并提供消息转发和匹配服务，从而提高通信效率和灵活性。<br><br>## 📈 实验结果<br>本文提出的算法在虚拟学习环境中进行了实验验证，结果表明，这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。<br><br>## 💬 可借鉴之处<br>本文提出的算法和模型为虚拟学习环境的设计和优化提供了新的思路和方法。其中，代理交互定位算法和动态代理分配算法可以应用于其他分布式系统中，以提高系统的效率和性能。基于代理中介的通信模型可以应用于其他多代理系统中，以促进代理之间的协作和通信。<br><br>## 📚 总结<br>本文提出的基于软件代理的交互算法为虚拟学习环境的设计和优化提供了新的思路和方法。这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。本文的研究成果对于虚拟学习环境和其他分布式系统的设计和优化具有重要的参考价值。</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</td>
      <td>We investigate the challenge of task planning for multi-task embodied agents<br>in open-world environments. Two main difficulties are identified: 1) executing<br>plans in an open-world environment (e.g., Minecraft) necessitates accurate and<br>multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla<br>planners do not consider how easy the current agent can achieve a given<br>sub-task when ordering parallel sub-goals within a complicated plan, the<br>resulting plan could be inefficient or even infeasible. To this end, we propose<br>"\( \underline{D} \)escribe, \( \underline{E} \)xplain, \( \underline{P} \)lan and<br>\( \underline{S} \)elect" (\( \textbf{DEPS} \)), an interactive planning approach based<br>on Large Language Models (LLMs). DEPS facilitates better error correction on<br>initial LLM-generated \( \textit{plan} \) by integrating \( \textit{description} \) of<br>the plan execution process and providing self-\( \textit{explanation} \) of<br>feedback when encountering failures during the extended planning phases.<br>Furthermore, it includes a goal \( \textit{selector} \), which is a trainable<br>module that ranks parallel candidate sub-goals based on the estimated steps of<br>completion, consequently refining the initial plan. Our experiments mark the<br>milestone of the first zero-shot multi-task agent that can robustly accomplish<br>70+ Minecraft tasks and nearly double the overall performances. Further testing<br>reveals our method's general effectiveness in popularly adopted non-open-ended<br>domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and<br>exploratory studies detail how our design beats the counterparts and provide a<br>promising update on the \( \texttt{ObtainDiamond} \) grand challenge with our<br>approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的交互式规划，助力开放世界多任务智能体<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，多任务智能体面临着两大挑战：1）执行计划需要精确的多步推理，因为任务具有长期性；2）传统的规划器在排序复杂的计划中的并行子目标时，没有考虑当前智能体完成给定子任务的难易程度，导致生成的计划可能效率低下甚至不可行。<br><br>## 🚀 核心方法<br>本文提出了“描述、解释、规划和选择”（DEPS）的交互式规划方法，基于大型语言模型（LLMs）来解决上述挑战。<br><br>💡 创新点1：描述、解释和规划<br>DEPS 通过集成计划执行过程的描述和提供自我解释的反馈，更好地纠正初始 LLM 生成的计划中的错误。当遇到失败时，描述器会总结当前情况并发送给 LLM，LLM 作为解释器定位错误，然后根据描述器和解释器的信息更新计划。<br><br>💡 创新点2：目标选择器<br>DEPS 包含一个可训练的目标选择器模块，该模块根据完成每个并行候选子目标的估计步骤对它们进行排序，从而细化初始计划。选择器使用预测剩余时间步数来完成每个目标，并根据当前状态选择最接近的目标。<br><br>## 📈 实验结果<br>实验结果表明，DEPS 在开放世界环境（如 Minecraft）中取得了显著的成果，能够稳健地完成 70 多个任务，并且整体性能几乎翻倍。此外，DEPS 在非开放世界环境（如 ALFWorld 和桌面操作）中也表现出良好的效果。<br><br>## 💬 可借鉴之处<br>DEPS 的交互式规划方法为开放世界多任务智能体的开发提供了新的思路。通过集成描述、解释和规划，以及使用目标选择器，DEPS 能够生成更可靠和高效的计划，从而提高智能体在开放世界环境中的任务完成能力。</td>
    </tr>
    <tr>
      <th>103</th>
      <td>Automatic Goal Generation for Reinforcement Learning Agents</td>
      <td>Reinforcement learning is a powerful technique to train an agent to perform a<br>task. However, an agent that is trained using reinforcement learning is only<br>capable of achieving the single task that is specified via its reward function.<br>Such an approach does not scale well to settings in which an agent needs to<br>perform a diverse set of tasks, such as navigating to varying positions in a<br>room or moving objects to varying locations. Instead, we propose a method that<br>allows an agent to automatically discover the range of tasks that it is capable<br>of performing. We use a generator network to propose tasks for the agent to try<br>to achieve, specified as goal states. The generator network is optimized using<br>adversarial training to produce tasks that are always at the appropriate level<br>of difficulty for the agent. Our method thus automatically produces a<br>curriculum of tasks for the agent to learn. We show that, by using this<br>framework, an agent can efficiently and automatically learn to perform a wide<br>set of tasks without requiring any prior knowledge of its environment. Our<br>method can also learn to achieve tasks with sparse rewards, which traditionally<br>pose significant challenges.</td>
      <td>## 🌟 论文解读 | 自动目标生成：让强化学习更高效<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）是一种强大的训练智能体执行特定任务的技术。然而，传统的强化学习方法通常只能让智能体学会执行单一任务，这在需要智能体执行多样化任务的场景中显得力不从心。例如，在机器人导航或物体搬运等任务中，智能体需要能够到达不同的位置或移动物体到不同的位置。为了解决这个问题，本文提出了一种自动目标生成方法，让智能体能够自动发现并学习执行其环境中的各种任务。<br><br>## 🚀 核心方法<br>💡 创新点1：目标生成网络<br>本文使用一个生成器网络来为智能体提出任务，这些任务被指定为目标状态。生成器网络通过对抗训练进行优化，以确保生成的任务始终处于智能体能够处理的难度水平。<br><br>💡 创新点2：自动课程生成<br>本文的方法自动生成一个课程，其中在每个步骤中，生成器都会生成比智能体已经能够实现的任务稍微更难的任务。这样，智能体可以高效地学习执行一系列任务，而无需对其环境或执行的任务有任何先验知识。<br><br>## 📈 实验结果<br>本文在多个机器人运动任务中进行了实验，结果表明，与传统的强化学习方法相比，本文的方法能够更快地学习执行各种任务，并且能够有效地处理稀疏奖励函数。<br><br>## 💬 可借鉴之处<br>本文提出的自动目标生成方法为强化学习提供了一种新的思路，可以帮助智能体更高效地学习执行多样化任务。该方法可以应用于各种场景，例如机器人导航、物体搬运、游戏等。此外，本文的方法还可以与其他多目标强化学习方法相结合，进一步提高学习效率。</td>
    </tr>
    <tr>
      <th>78</th>
      <td>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</td>
      <td>This study introduces "CosmoAgent," an innovative artificial intelligence<br>system that utilizes Large Language Models (LLMs) to simulate complex<br>interactions between human and extraterrestrial civilizations. This paper<br>introduces a mathematical model for quantifying the levels of civilization<br>development and further employs a state transition matrix approach to evaluate<br>their trajectories. Through this methodology, our study quantitatively analyzes<br>the growth trajectories of civilizations, providing insights into future<br>decision-making at critical points of growth and saturation. Furthermore, this<br>paper acknowledges the vast diversity of potential living conditions across the<br>universe, which could foster unique cosmologies, ethical codes, and worldviews<br>among different civilizations. Recognizing the Earth-centric bias inherent in<br>current LLM designs, we propose the novel concept of using LLM agents with<br>diverse ethical paradigms and simulating interactions between entities with<br>distinct moral principles. This innovative research not only introduces a novel<br>method for comprehending potential inter-civilizational dynamics but also holds<br>practical value in enabling entities with divergent value systems to<br>strategize, prevent conflicts, and engage in games under conditions of<br>asymmetric information. The accompanying code is available at<br>https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.</td>
      <td>## 🌟 论文解读 | 用大型语言模型模拟外星文明：探索宇宙中的互动与冲突<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，其在模拟复杂社会动态方面的潜力日益凸显。然而，现有的LLMs设计往往带有地球中心主义的偏见，难以全面模拟外星文明的多样性和独特性。本文旨在通过引入具有不同伦理范式和道德原则的LLM代理，模拟人类与外星文明之间的复杂互动，从而为理解潜在星际动态提供新的视角。<br><br>## 🚀 核心方法<br>💡 创新点1：CosmoAgent多智能体系统<br>本文提出了CosmoAgent，一个基于LLMs的多智能体系统，用于模拟宇宙中不同文明之间的互动。CosmoAgent通过模拟文明的决策过程，包括选择隐藏、战斗或合作，来探索文明发展的轨迹和潜在冲突。<br><br>💡 创新点2：文明发展模型<br>本文引入了一个数学模型来量化文明的发展水平，并使用状态转移矩阵方法来评估文明的轨迹。该模型考虑了五个关键资源：军事能力、技术发展、生产能力、消费和储存，以及不同文明的世界观（和平主义、军国主义和孤立主义）对决策的影响。<br><br>💡 创新点3：信息不对称的模拟<br>为了模拟宇宙中文明之间的互动，本文考虑了信息不对称的情况，即文明之间的观测数据滞后于实际发展。LLMs代理需要根据过时的信息做出决策，这增加了模拟的复杂性和现实性。<br><br>💡 创新点4：道德多样性的模拟<br>本文提出了使用具有不同伦理范式的LLM代理来模拟具有不同道德原则的实体之间的互动。这有助于理解不同文明如何共存，以及道德框架如何影响星际互动。<br><br>## 📈 实验结果<br>实验结果表明，具有军国主义世界观的文明倾向于对较弱文明发动攻击，而孤立主义文明则更倾向于在观察一段时间后选择性地与其他文明合作。此外，信息不对称会延迟冲突的发生，为较弱文明提供了反击的机会。<br><br>## 💬 可借鉴之处<br>本文的研究不仅为理解潜在星际动态提供了新的视角，还为解决具有不同价值体系的实体之间的冲突提供了策略。此外，本文的研究方法可以应用于其他领域，如模拟古代社会、人类文明模式和社会生态系统。</td>
    </tr>
    <tr>
      <th>117</th>
      <td>Odyssey: Empowering Minecraft Agents with Open-World Skills</td>
      <td>Recent studies have delved into constructing generalist agents for open-world<br>environments like Minecraft. Despite the encouraging results, existing efforts<br>mainly focus on solving basic programmatic tasks, e.g., material collection and<br>tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond<br>task as the ultimate goal. This limitation stems from the narrowly defined set<br>of actions available to agents, requiring them to learn effective long-horizon<br>strategies from scratch. Consequently, discovering diverse gameplay<br>opportunities in the open world becomes challenging. In this work, we introduce<br>Odyssey, a new framework that empowers Large Language Model (LLM)-based agents<br>with open-world skills to explore the vast Minecraft world. Odyssey comprises<br>three key parts: (1) An interactive agent with an open-world skill library that<br>consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned<br>LLaMA-3 model trained on a large question-answering dataset with 390k+<br>instruction entries derived from the Minecraft Wiki. (3) A new agent capability<br>benchmark includes the long-term planning task, the dynamic-immediate planning<br>task, and the autonomous exploration task. Extensive experiments demonstrate<br>that the proposed Odyssey framework can effectively evaluate different<br>capabilities of LLM-based agents. All datasets, model weights, and code are<br>publicly available to motivate future research on more advanced autonomous<br>agent solutions.</td>
      <td>## 🌟 论文解读 | Odyssey：赋予Minecraft智能体开放世界技能<br><br>## 📌 背景痛点/本文动机<br>近年来，许多研究致力于构建能够在开放世界环境中（如Minecraft）执行任务的通用智能体。尽管取得了令人鼓舞的成果，但现有工作主要集中在解决基本的编程任务，例如收集材料和制作工具，并将“获得钻石”任务视为最终目标。这种局限性源于智能体可用的动作集过于狭窄，需要它们从头开始学习有效的长期策略。因此，在开放世界中探索多样化的游戏玩法变得具有挑战性。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：开放世界技能库<br>Odyssey框架开发了一个基于大型语言模型（LLM）的交互式智能体，该智能体配备了一个开放世界技能库，其中包含40个基本技能和183个组合技能。这些技能涵盖了从资源收集到工具制作，再到战斗和探索的各种任务，为智能体提供了丰富的工具来应对开放世界的挑战。<br><br>💡 创新点2：微调LLaMA-3模型<br>为了提高智能体在Minecraft中的性能，Odyssey框架使用来自Minecraft维基的大规模问答数据集对LLaMA-3模型进行了微调。通过生成包含390k+指令条目的训练数据集，并使用LoRA技术进行高效训练，Odyssey框架显著提升了LLM模型在Minecraft领域的知识储备和推理能力。<br><br>💡 创新点3：智能体能力基准<br>Odyssey框架引入了一个新的智能体能力基准，包括长期规划任务、动态即时规划任务和自主探索任务。这些任务涵盖了Minecraft中的各种复杂场景，并要求智能体展现出多样化的解决方案。通过这些基准任务，研究人员可以全面评估智能体的规划能力、资源管理能力、技能检索能力以及自主探索能力。<br><br>## 📈 实验结果<br>实验结果表明，Odyssey框架在基本编程任务和智能体能力基准任务上都取得了显著的性能提升。与现有方法相比，Odyssey框架的智能体在完成任务的速度、成功率和资源利用率方面都表现出色。此外，消融实验也证明了开放世界技能库和LLM规划器对智能体整体性能的关键作用。<br><br>## 💬 可借鉴之处<br>Odyssey框架为开发和研究开放世界智能体提供了一个全面的框架，具有以下可借鉴之处：<br><br>* **开放世界技能库**：为智能体提供丰富的工具和策略，使其能够应对各种复杂的任务和挑战。<br>* **微调LLM模型**：通过领域特定的数据集进行微调，提升LLM模型在特定领域的知识储备和推理能力。<br>* **智能体能力基准**：为评估智能体的不同能力提供标准化的框架，促进开放世界智能体研究的进展。<br><br>## 🌟 总结<br>Odyssey框架为开放世界智能体的发展开辟了新的可能性，并为研究人员提供了一个强大的工具来探索和评估智能体的能力。随着未来研究的不断深入，Odyssey框架有望推动开放世界智能体技术的进一步发展，并为人工智能的通用性研究做出贡献。</td>
    </tr>
    <tr>
      <th>45</th>
      <td>AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback</td>
      <td>Large Language Models (LLMs) have demonstrated significant success across<br>various domains. However, their application in complex decision-making tasks<br>frequently necessitates intricate prompt engineering or fine-tuning, leading to<br>challenges in unseen downstream tasks and heavy demands on computational<br>resources. Meanwhile, Reinforcement Learning (RL) has been recognized as<br>effective in decision-making problems but struggles in environments with sparse<br>rewards, such as open-world games. To overcome these challenges, we introduce<br>AdaRefiner, a novel framework designed to enhance the synergy between LLMs and<br>RL feedback. The key component of AdaRefiner is a lightweight Adapter Language<br>Model (LM), which automatically refines task comprehension based on feedback<br>from RL agents. This method mitigates the need for intricate prompt engineering<br>and intensive LLM fine-tuning while maintaining the LLMs' generalization<br>abilities and enhancing their decision-making capabilities in downstream tasks.<br>Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world<br>game Crafter have demonstrated its superior effectiveness, especially in<br>guiding agents towards higher-level and common-sense skills. Our work makes<br>contributions to the automatic self-refinement of LLMs with RL feedback,<br>offering a more adaptable and efficient solution for complex decision-making<br>problems.</td>
      <td>## 🌟 论文解读 | AdaRefiner：利用自适应反馈提升语言模型决策能力<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在各个领域取得了显著的成功，但在复杂决策任务中的应用却面临着挑战。LLMs 需要进行繁琐的提示工程或微调才能适应特定任务，这限制了其在未知下游任务中的泛化能力，并带来了对计算资源的巨大需求。另一方面，强化学习（RL）在决策问题中表现出色，但在稀疏奖励的环境中（如开放世界游戏）却难以发挥作用。为了克服这些挑战，本文提出了 AdaRefiner，一个旨在增强 LLMs 和 RL 反馈之间协同作用的新框架。<br><br>## 🚀 核心方法<br>💡 创新点1：AdaRefiner 引入了一个轻量级的适配器语言模型（LM），该模型根据 RL 代理的反馈自动细化任务理解。这种方法减少了繁琐的提示工程和密集的 LLM 微调的需求，同时保持了 LLMs 的泛化能力，并增强了它们在下游任务中的决策能力。<br><br>💡 创新点2：AdaRefiner 在开放世界游戏 Crafter 的 22 个不同任务中进行了实证评估，结果表明其在引导代理学习高级和常识技能方面具有优越的有效性。<br><br>## 📈 实验结果<br>AdaRefiner 在 Crafter 环境中的 22 个任务上进行了评估，结果表明其性能优于最先进的基线方法。AdaRefiner 能够引导代理学习高级技能，并表现出常识行为。消融研究表明，适配器 LM 和 RL 反馈对于 AdaRefiner 的有效性至关重要。<br><br>## 💬 可借鉴之处<br>AdaRefiner 为 LLMs 在复杂决策任务中的应用提供了一种更灵活和高效的解决方案。其轻量级的适配器 LM 和自适应反馈机制可以有效地提升 LLMs 的任务理解和决策能力，为 LLMs 在开放世界游戏等复杂环境中的应用开辟了新的可能性。</td>
    </tr>
    <tr>
      <th>50</th>
      <td>MindAgent: Emergent Gaming Interaction</td>
      <td>Large Language Models (LLMs) have the capacity of performing complex<br>scheduling in a multi-agent system and can coordinate these agents into<br>completing sophisticated tasks that require extensive collaboration. However,<br>despite the introduction of numerous gaming frameworks, the community has<br>insufficient benchmarks towards building general multi-agents collaboration<br>infrastructure that encompass both LLM and human-NPCs collaborations. In this<br>work, we propose a novel infrastructure - MindAgent - to evaluate planning and<br>coordination emergent capabilities for gaming interaction. In particular, our<br>infrastructure leverages existing gaming framework, to i) require understanding<br>of the coordinator for a multi-agent system, ii) collaborate with human players<br>via un-finetuned proper instructions, and iii) establish an in-context learning<br>on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new<br>gaming scenario and related benchmark that dispatch a multi-agent collaboration<br>efficiency and supervise multiple agents playing the game simultaneously. We<br>conduct comprehensive evaluations with new auto-metric CoS for calculating the<br>collaboration efficiency. Finally, our infrastructure can be deployed into<br>real-world gaming scenarios in a customized VR version of CUISINEWORLD and<br>adapted in existing broader Minecraft gaming domain. We hope our findings on<br>LLMs and the new infrastructure for general-purpose scheduling and coordination<br>can help shed light on how such skills can be obtained by learning from large<br>language corpora.</td>
      <td>## 🌟 论文解读 | MindAgent：大型语言模型在游戏交互中的涌现式规划与协调能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在各个领域的应用日益广泛，其在多智能体系统中的规划与协调能力也逐渐受到关注。然而，现有的游戏框架和基准测试还不足以评估LLMs在游戏交互中的涌现式规划与协调能力，尤其是在LLMs与人类NPCs协作的场景下。本文旨在解决这个问题，提出了一种名为MindAgent的新型基础设施，用于评估LLMs在游戏交互中的规划与协调能力。<br><br>## 🚀 核心方法<br>💡 创新点1：CUISINEWORLD游戏场景与基准测试<br>本文设计了一个名为CUISINEWORLD的游戏场景，模拟了一个虚拟厨房环境，其中多智能体系统需要协调多个代理，完成尽可能多的菜肴订单。CUISINEWORLD游戏场景具有多种任务结构和难度，是评估LLMs涌现式多智能体规划能力的理想测试平台。<br><br>💡 创新点2：MindAgent基础设施<br>MindAgent是一个用于LLMs交互式多智能体规划的基础设施，它展示了LLMs的涌现式多智能体规划能力，并引入了多种提示技术，以促进LLMs的规划能力，包括提供少量示例、规划理由和环境反馈。<br><br>## 📈 实验结果<br>本文在CUISINEWORLD游戏场景中进行了广泛的实验，结果表明：<br>1. 零样本多智能体规划：强大的预训练LLMs（如GPT-4）能够通过阅读简单的游戏指令和食谱，调度多个代理（2到4个）完成菜肴，甚至与人类玩家协作。<br>2. 基于高级提示的规划：通过利用涌现式上下文学习能力，可以显著提高LLMs的多智能体规划性能。例如，添加少量专家演示、解释某些行动的理由，以及在规划过程中提供实时反馈。<br>3. 通用潜力：LLMs表现出成为通用多智能体规划器的巨大潜力，因为它能够通过少量示例泛化到更多代理，并适应新的游戏领域，如Minecraft。<br><br>## 💬 可借鉴之处<br>本文提出的MindAgent基础设施和CUISINEWORLD游戏场景为评估LLMs在游戏交互中的涌现式规划与协调能力提供了新的思路和方法。此外，本文的研究结果也表明，LLMs在多智能体规划方面具有巨大的潜力，有望在未来推动游戏AI的发展。</td>
    </tr>
    <tr>
      <th>118</th>
      <td>ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy</td>
      <td>Language agents have demonstrated autonomous decision-making abilities by<br>reasoning with foundation models. Recently, efforts have been made to train<br>language agents for performance improvement, with multi-step reasoning and<br>action trajectories as the training data. However, collecting such trajectories<br>still requires considerable human effort, by either artificial annotation or<br>implementations of diverse prompting frameworks. In this work, we propose<br>A\( ^3 \)T, a framework that enables the Autonomous Annotation of Agent<br>Trajectories in the style of ReAct. The central role is an ActRe prompting<br>agent, which explains the reason for an arbitrary action. When randomly<br>sampling an external action, the ReAct-style agent could query the ActRe agent<br>with the action to obtain its textual rationales. Novel trajectories are then<br>synthesized by prepending the posterior reasoning from ActRe to the sampled<br>action. In this way, the ReAct-style agent executes multiple trajectories for<br>the failed tasks, and selects the successful ones to supplement its failed<br>trajectory for contrastive self-training. Realized by policy gradient methods<br>with binarized rewards, the contrastive self-training with accumulated<br>trajectories facilitates a closed loop for multiple rounds of language agent<br>self-improvement. We conduct experiments using QLoRA fine-tuning with the<br>open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with<br>A\( ^3 \)T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative<br>rounds. In WebShop, the 1-shot performance of the A\( ^3 \)T agent matches human<br>average, and 4 rounds of iterative refinement lead to the performance<br>approaching human experts. A\( ^3 \)T agents significantly outperform existing<br>techniques, including prompting with GPT-4, advanced agent frameworks, and<br>fully fine-tuned LLMs.</td>
      <td>## 🌟 论文解读 | 语言智能体自主训练数据标注框架：A3T<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，语言智能体在自主决策方面展现出巨大的潜力。然而，训练这些智能体需要大量的多步推理和动作轨迹作为训练数据，而这些数据的收集通常需要大量的人工标注或实现各种提示框架，这限制了训练的规模和效率。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：A3T框架<br>本文提出了A3T框架，该框架能够实现语言智能体轨迹的自主标注。A3T的核心是ActRe提示智能体，它能够解释任意动作的原因。当随机采样一个外部动作时，ReAct风格的智能体可以查询ActRe智能体以获取该动作的文本理由。然后，通过将ActRe的推理结果添加到采样动作之前，合成新的轨迹。这样，ReAct风格的智能体可以执行多个轨迹来处理失败的任务，并选择成功的轨迹来补充失败的轨迹，进行对比自我训练。<br><br>💡 创新点2：对比自我训练<br>A3T框架利用策略梯度方法，通过二进制奖励来实现对比自我训练。智能体执行每个合成的轨迹后，环境会提供终端奖励，自动标注轨迹的质量。成功的轨迹被保留下来，并与失败的轨迹一起用于对比自我训练。随着新智能体的训练，更多的轨迹可以被收集和积累，形成一个闭环，促进语言智能体的自我改进。<br><br>## 📈 实验结果<br>在AlfWorld和WebShop两个基准测试中，A3T框架取得了显著的性能提升。在AlfWorld中，经过A3T训练的智能体在未见过的场景中实现了96%的一次性成功率，并且在4次迭代后达到100%的成功率。在WebShop中，A3T智能体的一次性性能与人类平均水平相当，经过4次迭代后，性能接近人类专家。<br><br>## 💬 可借鉴之处<br>A3T框架为语言智能体的自主训练提供了一种有效的方法，通过自主标注和对比自我训练，实现了智能体的闭环自我改进。这种方法可以应用于各种场景，提高语言智能体的性能和自主性。</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization</td>
      <td>Large Language Models (LLMs) exhibit robust problem-solving capabilities for<br>diverse tasks. However, most LLM-based agents are designed as specific task<br>solvers with sophisticated prompt engineering, rather than agents capable of<br>learning and evolving through interactions. These task solvers necessitate<br>manually crafted prompts to inform task rules and regulate LLM behaviors,<br>inherently incapacitating to address complex dynamic scenarios e.g., large<br>interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent<br>with Policy-level Reflection and Optimization that can learn a wealth of<br>expertise from interactive experiences and progressively elevate its behavioral<br>policy. Specifically, it involves a dynamic belief generation and reflection<br>process for policy evolution. Rather than action-level reflection, Agent-Pro<br>iteratively reflects on past trajectories and beliefs, fine-tuning its<br>irrational beliefs for a better policy. Moreover, a depth-first search is<br>employed for policy optimization, ensuring continual enhancement in policy<br>payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,<br>outperforming vanilla LLM and specialized models. Our results show Agent-Pro<br>can learn and evolve in complex and dynamic scenes, which also benefits<br>numerous LLM-based applications.</td>
      <td>## 🌟 论文解读 | Agent-Pro：基于策略级反思和优化的学习进化<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在解决各种任务方面表现出强大的能力，但大多数基于LLMs的智能体都是为特定任务设计的，需要复杂的提示工程来告知任务规则和调节LLMs的行为。这使得它们难以应对复杂动态的场景，例如大型互动游戏。本文提出了一种名为Agent-Pro的LLM-based智能体，它具有策略级反思和优化能力，可以从互动经验中学习大量专业知识，并逐步提升其行为策略。<br><br>## 🚀 核心方法<br>💡 创新点1：策略级反思和优化<br>Agent-Pro通过策略级反思和优化来学习进化。它不仅反思过去的轨迹和信念，还通过深度优先搜索来优化策略，确保策略收益的持续提升。<br><br>💡 创新点2：信念感知决策过程<br>Agent-Pro采用信念感知决策过程，通过更新自身信念和世界信念来生成更合理的行为。它能够根据信念来预测行动，并在游戏结束后根据结果来反思和调整信念。<br><br>## 📈 实验结果<br>Agent-Pro在两个游戏（Blackjack和Texas Hold'em）中进行了评估，结果表明它能够学习并进化，在复杂和动态的场景中表现出色。与传统的LLMs和专门模型相比，Agent-Pro在游戏中的收益更高。<br><br>## 💬 可借鉴之处<br>Agent-Pro的设计理念和方法为构建能够学习和进化的LLM-based智能体提供了新的思路。其策略级反思和优化机制可以帮助智能体从互动经验中学习，并逐步提升其行为策略。此外，信念感知决策过程可以帮助智能体在不确定的场景中做出更合理的决策。这些方法可以应用于各种复杂的任务，例如商业谈判、安全监控等。</td>
    </tr>
    <tr>
      <th>106</th>
      <td>Face Recognition Methods & Applications</td>
      <td>Face recognition presents a challenging problem in the field of image<br>analysis and computer vision. The security of information is becoming very<br>significant and difficult. Security cameras are presently common in airports,<br>Offices, University, ATM, Bank and in any locations with a security system.<br>Face recognition is a biometric system used to identify or verify a person from<br>a digital image. Face Recognition system is used in security. Face recognition<br>system should be able to automatically detect a face in an image. This involves<br>extracts its features and then recognize it, regardless of lighting,<br>expression, illumination, ageing, transformations (translate, rotate and scale<br>image) and pose, which is a difficult task. This paper contains three sections.<br>The first section describes the common methods like holistic matching method,<br>feature extraction method and hybrid methods. The second section describes<br>applications with examples and finally third section describes the future<br>research directions of face recognition.</td>
      <td>## 🌟 论文解读 | 《人脸识别方法与应用》：深入探索人脸识别技术的奥秘<br><br>## 📌 背景痛点/本文动机<br>随着信息安全变得越来越重要和复杂，人脸识别作为一种生物识别系统，在图像分析和计算机视觉领域面临着巨大的挑战。本文旨在探讨人脸识别的常见方法、应用实例以及未来的研究方向，以期为相关领域的研究和实践提供参考。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：全面梳理人脸识别方法<br>本文详细介绍了三种主流的人脸识别方法：整体匹配方法、特征提取方法和混合方法。整体匹配方法如Eigenfaces，通过主成分分析（PCA）提取面部特征；特征提取方法关注局部特征，如眼睛、鼻子和嘴巴；混合方法结合了整体和局部特征，通常使用3D图像进行识别。<br><br>💡 创新点2：丰富的人脸识别应用案例<br>文章不仅介绍了人脸识别的基本方法，还提供了多个实际应用案例，如选民注册系统中的重复身份识别、计算机登录监控、机场安全系统、图像数据库调查等，展示了人脸识别技术在各个领域的广泛应用。<br><br>## 📈 实验结果<br>本文没有详细描述具体的实验结果，但通过文献综述和案例分析，展示了人脸识别技术在多种场景下的有效性和实用性。<br><br>## 💬 可借鉴之处<br>1. **方法多样性**：本文提供了多种人脸识别方法，研究者可以根据具体应用场景选择合适的方法。<br>2. **实际应用案例**：通过实际案例，展示了人脸识别技术在现实世界中的具体应用，为其他研究者提供了实践参考。<br>3. **未来研究方向**：文章指出了人脸识别技术的未来发展方向，如2D和3D人脸识别、大规模应用等，为后续研究提供了方向指引。<br><br>总之，本文对人脸识别技术进行了全面的梳理和探讨，对于人脸识别领域的研究者和工程师具有很高的参考价值。</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Towards Automation of Cognitive Modeling using Large Language Models</td>
      <td>Computational cognitive models, which formalize theories of cognition, enable<br>researchers to quantify cognitive processes and arbitrate between competing<br>theories by fitting models to behavioral data. Traditionally, these models are<br>handcrafted, which requires significant domain knowledge, coding expertise, and<br>time investment. Previous work has demonstrated that Large Language Models<br>(LLMs) are adept at pattern recognition in-context, solving complex problems,<br>and generating executable code. In this work, we leverage these abilities to<br>explore the potential of LLMs in automating the generation of cognitive models<br>based on behavioral data. We evaluated the LLM in two different tasks: model<br>identification (relating data to a source model), and model generation<br>(generating the underlying cognitive model). We performed these tasks across<br>two cognitive domains - decision making and learning. In the case of data<br>simulated from canonical cognitive models, we found that the LLM successfully<br>identified and generated the ground truth model. In the case of human data,<br>where behavioral noise and lack of knowledge of the true underlying process<br>pose significant challenges, the LLM generated models that are identical or<br>close to the winning model from cognitive science literature. Our findings<br>suggest that LLMs can have a transformative impact on cognitive modeling. With<br>this project, we aim to contribute to an ongoing effort of automating<br>scientific discovery in cognitive science.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型实现认知建模自动化<br><br>## 📌 背景痛点/本文动机<br>传统的认知建模需要研究人员具备深厚的领域知识、编程技能和大量的时间投入。手工构建的模型往往受限于研究者的背景和建模能力，难以探索更广泛的假设空间。因此，自动化认知模型生成成为了一个重要的研究方向。<br><br>## 🚀 核心方法<br>💡 创新点1：利用大型语言模型（LLM）进行模型识别和生成<br>本文利用LLM的强大能力，探索了其在自动化生成认知模型方面的潜力。LLM能够处理自然语言格式的行为数据，识别复杂问题中的模式，并生成可执行的代码。通过将LLM应用于模型识别和模型生成任务，可以自动化认知模型生成的关键步骤。<br><br>💡 创新点2：基于行为数据生成认知模型<br>本文通过设计一个流程，让LLM根据行为数据和任务描述生成认知模型。LLM首先识别数据源模型，然后根据观察到的数据生成认知模型。通过将LLM应用于模拟数据和真实人类数据，可以评估LLM在模型识别和模型生成任务中的表现。<br><br>## 📈 实验结果<br>本文在决策和学习的两个认知领域进行了实验。结果表明，LLM在模型识别任务中表现出色，能够准确地识别数据源模型。在模型生成任务中，LLM生成的模型与真实数据生成函数非常接近，并且在拟合真实人类数据方面表现出色。<br><br>## 💬 可借鉴之处<br>本文提出的利用LLM进行认知建模自动化的方法具有以下可借鉴之处：<br>1. **降低门槛**：LLM能够处理自然语言格式的数据，使得研究人员无需具备深厚的编程技能即可进行认知建模。<br>2. **加速研究**：LLM能够自动化认知模型生成的关键步骤，从而加速研究进程。<br>3. **探索更广泛的假设空间**：LLM能够生成多种认知模型，帮助研究人员探索更广泛的假设空间。<br><br>## 🌟 未来展望<br>未来研究可以进一步探索LLM在更广泛的认知领域中的应用，例如感知、记忆和语言理解。此外，通过在认知建模任务上微调LLM，可以提高其生成科学意义模型的准确性。最后，通过集成多个LLM，可以创建一个完全自动化的、通用的认知建模框架。</td>
    </tr>
    <tr>
      <th>51</th>
      <td>Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks</td>
      <td>We study building multi-task agents in open-world environments. Without human<br>demonstrations, learning to accomplish long-horizon tasks in a large open-world<br>environment with reinforcement learning (RL) is extremely inefficient. To<br>tackle this challenge, we convert the multi-task learning problem into learning<br>basic skills and planning over the skills. Using the popular open-world game<br>Minecraft as the testbed, we propose three types of fine-grained basic skills,<br>and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that<br>performs exploration to find diverse items provides better initialization for<br>other skills, improving the sample efficiency for skill learning. In skill<br>planning, we leverage the prior knowledge in Large Language Models to find the<br>relationships between skills and build a skill graph. When the agent is solving<br>a task, our skill search algorithm walks on the skill graph and generates the<br>proper skill plans for the agent. In experiments, our method accomplishes 40<br>diverse Minecraft tasks, where many tasks require sequentially executing for<br>more than 10 skills. Our method outperforms baselines by a large margin and is<br>the most sample-efficient demonstration-free RL method to solve Minecraft Tech<br>Tree tasks. The project's website and code can be found at<br>https://sites.google.com/view/plan4mc.</td>
      <td>## 🌟 论文解读 | Plan4MC：基于技能强化学习和规划的开放世界长时任务解决方案<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，学习完成长时任务对于强化学习（RL）来说是非常低效的。这是因为开放世界环境通常具有无限大的世界规模和大量的任务，这使得探索和样本效率成为主要挑战。此外，长时任务通常具有多个子目标，需要大量的环境步骤才能完成。<br><br>## 🚀 核心方法<br>💡 创新点1：将多任务学习问题转化为学习基本技能和技能规划。在Minecraft游戏中，我们提出了三种细粒度基本技能：寻找技能、操作技能和制作技能。使用具有内在奖励的RL来获取技能，并通过技能规划来分解任务。<br><br>💡 创新点2：利用大型语言模型（LLM）的先验知识来构建技能图，并通过技能搜索算法来生成正确的技能序列。这种方法避免了LLM不可控的错误，并提高了规划精度。<br><br>## 📈 实验结果<br>在MineDojo模拟器中构建了40个多样化的任务，结果表明Plan4MC能够完成所有任务，并且显著优于基线方法。此外，Plan4MC在Minecraft Tech Tree任务中表现出更高的样本效率。<br><br>## 💬 可借鉴之处<br>Plan4MC提出了一种高效解决开放世界长时任务的方法，通过学习基本技能和技能规划来提高样本效率。此外，利用LLM构建技能图和技能搜索算法为开放世界任务规划提供了一种新的思路。</td>
    </tr>
    <tr>
      <th>87</th>
      <td>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games</td>
      <td>Large Language Models (LLMs) have demonstrated superior performance in<br>language understanding benchmarks. CALM, a popular approach, leverages<br>linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to<br>improve the performance in text games in Jericho without environment-provided<br>actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps<br>the LLM fixed during the learning of the text based games. In this work, we<br>explore and evaluate updating LLM used for candidate recommendation during the<br>learning of the text based game as well to mitigate the reliance on the human<br>annotated gameplays, which are costly to acquire. We observe that by updating<br>the LLM during learning using carefully selected in-game transitions, we can<br>reduce the dependency on using human annotated game plays for fine-tuning the<br>LLMs. We conducted further analysis to study the transferability of the updated<br>LLMs and observed that transferring in-game trained models to other games did<br>not result in a consistent transfer.</td>
      <td>## 🌟 论文解读 | 语言模型在循环中：文本游戏中学习推荐动作的数据最优方法<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在语言理解基准测试中表现出色。CALM 是一种流行的学习方法，它利用 LLMs（例如 GPT-2）的语言先验知识来推荐动作候选者，从而在没有环境提供动作的情况下提高 Jericho 中文本游戏的表现。然而，CALM 使用带有人类游戏玩法的注释来适应 GPT-2，并在学习文本游戏时保持 LLM 固定。这种方法依赖于昂贵的人类注释游戏玩法，并且没有充分利用游戏中的转换来训练 LLM。本文旨在探索和评估在文本游戏学习过程中更新用于候选推荐的语言模型，以减少对人类注释游戏玩法的依赖。<br><br>## 🚀 核心方法<br>本文提出了 LM-in-the-Loop 方法，该方法在文本游戏学习过程中更新用于候选推荐的语言模型。具体来说，该方法使用精心选择的游戏内转换来更新 LLM，从而减少对人类注释游戏玩法的依赖。此外，本文还分析了更新后的 LLM 的迁移性，并发现将游戏内训练的模型迁移到其他游戏并不总是导致一致的性能提升。<br><br>## 📈 实验结果<br>实验结果表明，LM-in-the-Loop 方法可以减少对人类注释游戏玩法的依赖，并加速收敛。此外，基于状态特征的转换选择方法比其他方法提供了更大的收益。然而，LM-in-the-Loop 方法并不总是能够迁移到其他游戏。<br><br>## 💬 可借鉴之处<br>本文提出的 LM-in-the-Loop 方法为文本游戏中学习推荐动作提供了一种数据最优的方法。该方法可以减少对人类注释游戏玩法的依赖，并加速收敛。此外，本文还分析了更新后的 LLM 的迁移性，并发现将游戏内训练的模型迁移到其他游戏并不总是导致一致的性能提升。这些发现对于理解和改进文本游戏中语言模型的性能具有重要意义。</td>
    </tr>
    <tr>
      <th>18</th>
      <td>ChessGPT: Bridging Policy Learning and Language Modeling</td>
      <td>When solving decision-making tasks, humans typically depend on information<br>from two key sources: (1) Historical policy data, which provides interaction<br>replay from the environment, and (2) Analytical insights in natural language<br>form, exposing the invaluable thought process or strategic considerations.<br>Despite this, the majority of preceding research focuses on only one source:<br>they either use historical replay exclusively to directly learn policy or value<br>functions, or engaged in language model training utilizing mere language<br>corpus. In this paper, we argue that a powerful autonomous agent should cover<br>both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning<br>and language modeling by integrating data from these two sources in Chess<br>games. Specifically, we build a large-scale game and language dataset related<br>to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and<br>ChessGPT, integrating policy learning and language modeling. Finally, we<br>propose a full evaluation framework for evaluating language model's chess<br>ability. Experimental results validate our model and dataset's effectiveness.<br>We open source our code, model, and dataset at<br>https://github.com/waterhorse1/ChessGPT.</td>
      <td>## 🌟 论文解读 | ChessGPT：策略学习与语言模型融合的桥梁<br><br>## 📌 背景痛点/本文动机<br>在解决决策任务时，人类通常依赖于两种关键信息来源：历史策略数据和自然语言形式的策略分析。然而，现有的研究大多只关注其中一种来源，要么是直接从历史回放中学习策略或价值函数，要么是利用语言语料库进行语言模型训练。本文认为，一个强大的自主代理应该同时利用这两种来源，因此提出了ChessGPT，一个通过整合国际象棋游戏中的数据来连接策略学习和语言模型的GPT模型。<br><br>## 🚀 核心方法<br>💡 创新点1：构建大规模游戏和语言数据集<br>本文构建了一个包含大量国际象棋游戏数据和自然语言数据的综合数据集，包括在线游戏回放、专业棋手比赛、计算机引擎游戏、棋盘游戏、棋盘游戏分析、棋盘游戏博客、棋盘游戏书籍、棋盘游戏论坛、棋盘游戏视频等。<br><br>💡 创新点2：提出ChessCLIP和ChessGPT模型<br>本文提出了两种模型，ChessCLIP和ChessGPT，利用上述数据集进行训练。ChessCLIP通过对比学习将策略和语言模态连接起来，而ChessGPT则通过因果语言模型进行策略学习。<br><br>💡 创新点3：提出全面的评估框架<br>本文设计了一个全面的评估框架，用于评估语言模型在国际象棋方面的能力，包括建模能力、价值判断能力和策略能力。<br><br>## 📈 实验结果<br>实验结果表明，ChessGPT模型在所有评估任务中都优于其他LLM基线模型，证明了模型和数据集的有效性。<br><br>## 💬 可借鉴之处<br>本文提出的ChessGPT模型和数据集为研究策略学习和语言模型之间的相互作用提供了新的思路和方法，并为开发更强大的自主代理提供了新的可能性。</td>
    </tr>
    <tr>
      <th>47</th>
      <td>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</td>
      <td>We introduce SwiftSage, a novel agent framework inspired by the dual-process<br>theory of human cognition, designed to excel in action planning for complex<br>interactive reasoning tasks. SwiftSage integrates the strengths of behavior<br>cloning and prompting large language models (LLMs) to enhance task completion<br>performance. The framework comprises two primary modules: the Swift module,<br>representing fast and intuitive thinking, and the Sage module, emulating<br>deliberate thought processes. The Swift module is a small encoder-decoder LM<br>fine-tuned on the oracle agent's action trajectories, while the Sage module<br>employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a<br>heuristic method to harmoniously integrate the two modules, resulting in a more<br>efficient and robust problem-solving process. In 30 tasks from the ScienceWorld<br>benchmark, SwiftSage significantly outperforms other methods such as SayCan,<br>ReAct, and Reflexion, demonstrating its effectiveness in solving complex<br>interactive tasks.</td>
      <td>## 🌟 论文解读 | SwiftSage：结合快慢思考的生成式智能体，解决复杂交互任务<br><br>## 📌 背景痛点/本文动机<br>随着人工智能的发展，智能体在复杂交互推理任务中的能力越来越受到关注。这类任务要求智能体具备长期规划、记忆、子目标分解、空间推理、异常处理和常识知识等能力。然而，现有的方法，如强化学习、行为克隆和大型语言模型（LLM）提示，在处理复杂任务时存在局限性。例如，强化学习需要大量的交互来学习，行为克隆难以泛化到未见过的任务，而LLM提示则缺乏对环境的具体操作指导。<br><br>## 🚀 核心方法<br>SwiftSage 提出了一个新颖的智能体框架，灵感来源于人类认知的双过程理论，旨在解决复杂交互推理任务。该框架结合了行为克隆和LLM提示的优势，以提高任务完成性能。<br><br>### 💡 创新点1：双模块设计<br>SwiftSage 由两个主要模块组成：<br>- **Swift 模块**：代表快速和直观的思考，是一个小型的编码器-解码器语言模型，通过模仿专家智能体的行为轨迹进行微调。<br>- **Sage 模块**：代表深思熟虑的思考，利用LLM（如GPT-4）进行子目标规划和接地。<br><br>### 💡 创新点2：模块集成策略<br>SwiftSage 开发了一种启发式方法来和谐地集成这两个模块，根据任务需求动态切换模块，并有效结合它们的输出。例如，当遇到异常情况或需要长期规划时，Sage 模块会被激活，而 Swift 模块则用于快速响应和执行简单任务。<br><br>## 📈 实验结果<br>在 ScienceWorld 基准测试的30个任务中，SwiftSage 显著优于其他方法，如SayCan、ReAct和Reflexion，证明了其在解决复杂交互任务方面的有效性。SwiftSage 实现了最先进的平均得分84.7，而其他方法的得分分别为33.8、36.4和45.3。此外，SwiftSage 更加经济高效，每项操作所需的LLM令牌数量远少于之前的方法。<br><br>## 💬 可借鉴之处<br>SwiftSage 的双模块设计为解决复杂交互推理任务提供了一种新的思路。其模块集成策略和启发式算法可以应用于其他需要快速响应和深思熟虑的任务中。此外，SwiftSage 的成功也表明了结合小型语言模型和LLM在复杂推理任务中的潜力。</td>
    </tr>
    <tr>
      <th>42</th>
      <td>Language Models can Solve Computer Tasks</td>
      <td>Agents capable of carrying out general tasks on a computer can improve<br>efficiency and productivity by automating repetitive tasks and assisting in<br>complex problem-solving. Ideally, such agents should be able to solve new<br>computer tasks presented to them through natural language commands. However,<br>previous approaches to this problem require large amounts of expert<br>demonstrations and task-specific reward functions, both of which are<br>impractical for new tasks. In this work, we show that a pre-trained large<br>language model (LLM) agent can execute computer tasks guided by natural<br>language using a simple prompting scheme where the agent Recursively Criticizes<br>and Improves its output (RCI). The RCI approach significantly outperforms<br>existing LLM methods for automating computer tasks and surpasses supervised<br>learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++<br>benchmark. We compare multiple LLMs and find that RCI with the<br>InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful<br>of demonstrations per task rather than tens of thousands, and without a<br>task-specific reward function. Furthermore, we demonstrate RCI prompting's<br>effectiveness in enhancing LLMs' reasoning abilities on a suite of natural<br>language reasoning tasks, outperforming chain of thought (CoT) prompting with<br>external feedback. We find that RCI combined with CoT performs better than<br>either separately. Our code can be found here:<br>https://github.com/posgnu/rci-agent.</td>
      <td>## 🌟 论文解读 | 语言模型解决计算机任务：RCI方法引领AI新潮流<br><br>## 📌 背景痛点/本文动机<br>随着人工智能的发展，人们期望能够创造出能够执行各种计算机任务的智能代理，从而提高效率和生产力。然而，现有的方法往往需要大量的专家演示和特定任务的奖励函数，这在实际应用中并不实用。本文提出了一种新的方法，使用预训练的大型语言模型（LLM）来执行计算机任务，并通过自然语言命令进行指导。<br><br>## 🚀 核心方法<br>本文的核心方法是递归批评和改进（RCI）方案，该方案通过以下步骤实现：<br>1. **任务接地**：首先，LLM根据任务文本生成一个高级计划。<br>2. **状态接地**：然后，将高级概念与当前状态中的实际HTML元素连接起来，输出相应的动作。<br>3. **代理接地**：最后，确保动作输出格式正确，可以被计算机代理执行。<br><br>RCI方案在每个步骤中都应用，但状态接地步骤只需要一次批评。<br><br>## 📈 实验结果<br>本文在MiniWoB++基准测试中评估了RCI方法，结果表明，RCI方法显著优于现有的LLM方法，并在自动化计算机任务方面超越了监督学习和强化学习方法。此外，RCI提示方案在增强LLM的自然语言推理能力方面也表现出色，超过了外部反馈的链式思维（CoT）提示。<br><br>## 💬 可借鉴之处<br>本文提出的RCI方法为使用LLM执行计算机任务提供了一种新的思路，具有以下可借鉴之处：<br>1. **简化任务执行**：RCI方案通过将任务分解为三个步骤，简化了LLM执行计算机任务的流程。<br>2. **提高推理能力**：RCI提示方案可以增强LLM的自然语言推理能力，使其能够更好地理解和执行任务。<br>3. **降低样本复杂度**：RCI方法只需要少量演示即可泛化到未见过的任务，降低了样本复杂度。<br><br>## 🌟 总结<br>本文提出的RCI方法为使用LLM执行计算机任务提供了一种新的思路，具有简化任务执行、提高推理能力和降低样本复杂度等优点。随着LLM能力的不断提高，RCI方法有望在自动化计算机任务方面发挥更大的作用。</td>
    </tr>
    <tr>
      <th>91</th>
      <td>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</td>
      <td>We present BART, a denoising autoencoder for pretraining sequence-to-sequence<br>models. BART is trained by (1) corrupting text with an arbitrary noising<br>function, and (2) learning a model to reconstruct the original text. It uses a<br>standard Tranformer-based neural machine translation architecture which,<br>despite its simplicity, can be seen as generalizing BERT (due to the<br>bidirectional encoder), GPT (with the left-to-right decoder), and many other<br>more recent pretraining schemes. We evaluate a number of noising approaches,<br>finding the best performance by both randomly shuffling the order of the<br>original sentences and using a novel in-filling scheme, where spans of text are<br>replaced with a single mask token. BART is particularly effective when fine<br>tuned for text generation but also works well for comprehension tasks. It<br>matches the performance of RoBERTa with comparable training resources on GLUE<br>and SQuAD, achieves new state-of-the-art results on a range of abstractive<br>dialogue, question answering, and summarization tasks, with gains of up to 6<br>ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system<br>for machine translation, with only target language pretraining. We also report<br>ablation experiments that replicate other pretraining schemes within the BART<br>framework, to better measure which factors most influence end-task performance.</td>
      <td>## 🌟 论文解读 | BART：自然语言生成、翻译和理解的降噪序列到序列预训练模型<br><br>## 📌 背景痛点/本文动机<br>自然语言处理（NLP）领域近年来取得了显著进展，自监督学习方法在多种NLP任务中取得了令人瞩目的成果。然而，现有的预训练模型往往针对特定类型的任务进行优化，限制了其通用性和适用范围。本文提出了一种名为BART的降噪自编码器，旨在通过预训练序列到序列模型，提高模型在自然语言生成、翻译和理解等任务上的性能。<br><br>## 🚀 核心方法<br>💡 创新点1：降噪自编码器<br>BART采用了一种新颖的降噪自编码器架构，通过将文本进行随机噪声处理，然后学习一个模型来重建原始文本。这种架构具有很高的灵活性，可以应用于各种文本噪声处理方法，包括随机打乱句子顺序、使用掩码替换文本片段等。<br><br>💡 创新点2：Transformer架构<br>BART使用标准的Transformer架构，包括双向编码器和自回归解码器。这种架构简单而强大，可以看作是对BERT（双向编码器）和GPT（自回归解码器）的泛化，同时适用于多种预训练方案。<br><br>## 📈 实验结果<br>BART在多个NLP任务上取得了优异的性能。在文本生成任务中，BART在摘要、问答和对话生成等任务上取得了新的SOTA结果，并在ROUGE指标上取得了高达6分的提升。在理解任务中，BART在GLUE和SQuAD等基准测试中与RoBERTa相当，并在机器翻译任务中取得了1.1 BLEU的提升。<br><br>## 💬 可借鉴之处<br>BART的降噪自编码器架构为NLP预训练模型提供了一种新的思路，可以应用于各种文本噪声处理方法，并提高模型在多种任务上的性能。此外，BART的Transformer架构简单而强大，可以看作是对BERT和GPT的泛化，为NLP研究提供了新的方向。</td>
    </tr>
    <tr>
      <th>36</th>
      <td>LaMP: When Large Language Models Meet Personalization</td>
      <td>This paper highlights the importance of personalization in large language<br>models and introduces the LaMP benchmark -- a novel benchmark for training and<br>evaluating language models for producing personalized outputs. LaMP offers a<br>comprehensive evaluation framework with diverse language tasks and multiple<br>entries for each user profile. It consists of seven personalized tasks,<br>spanning three text classification and four text generation tasks. We<br>additionally propose two retrieval augmentation approaches that retrieve<br>personal items from each user profile for personalizing language model outputs.<br>To this aim, we study various retrieval models, including term matching,<br>semantic matching, and time-aware methods. Extensive experiments on LaMP for<br>zero-shot and fine-tuned language models demonstrate the efficacy of the<br>proposed retrieval augmentation approach and highlight the impact of<br>personalization in various natural language tasks.</td>
      <td>## 🌟 论文解读 | LaMP：大型语言模型个性化输出新基准<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理（NLP）领域的广泛应用，个性化输出成为满足用户独特需求和偏好的关键因素。然而，现有的NLP基准测试往往采用“一刀切”的方法，缺乏对个性化需求的考虑，限制了个性化研究的发展。本文提出了LaMP基准，旨在评估LLMs在个性化文本分类和生成任务中的性能。<br><br>## 🚀 核心方法<br>💡 创新点1：LaMP基准<br>LaMP基准包含七个个性化任务，涵盖三个文本分类和四个文本生成任务，并提供两种数据分割方式（基于用户和基于时间）以适应不同的个性化场景。<br><br>💡 创新点2：检索增强方法<br>为了解决用户配置文件长度限制问题，本文提出了两种检索增强方法：提示内增强（IPA）和融合解码器（FiD）。这些方法从用户配置文件中检索相关信息，并将其用于个性化LLM输出。<br><br>## 📈 实验结果<br>实验结果表明，检索增强方法在LaMP基准上取得了显著的性能提升。在零样本设置下，平均性能提升12.2%，而在微调设置下，平均性能提升23.5%。<br><br>## 💬 可借鉴之处<br>LaMP基准为个性化NLP模型的研究提供了丰富的环境和工具。检索增强方法为解决用户配置文件长度限制问题提供了一种有效解决方案。此外，本文还提出了几个值得进一步研究的问题，例如个性化文本生成的评估指标和隐私保护等。</td>
    </tr>
    <tr>
      <th>101</th>
      <td>BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</td>
      <td>We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in<br>simulation, spanning a range of everyday household chores such as cleaning,<br>maintenance, and food preparation. These activities are designed to be<br>realistic, diverse, and complex, aiming to reproduce the challenges that agents<br>must face in the real world. Building such a benchmark poses three fundamental<br>difficulties for each activity: definition (it can differ by time, place, or<br>person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these<br>with three innovations. First, we propose an object-centric, predicate<br>logic-based description language for expressing an activity's initial and goal<br>conditions, enabling generation of diverse instances for any activity. Second,<br>we identify the simulator-agnostic features required by an underlying<br>environment to support BEHAVIOR, and demonstrate its realization in one such<br>simulator. Third, we introduce a set of metrics to measure task progress and<br>efficiency, absolute and relative to human demonstrators. We include 500 human<br>demonstrations in virtual reality (VR) to serve as the human ground truth. Our<br>experiments demonstrate that even state of the art embodied AI solutions<br>struggle with the level of realism, diversity, and complexity imposed by the<br>activities in our benchmark. We make BEHAVIOR publicly available at<br>behavior.stanford.edu to facilitate and calibrate the development of new<br>embodied AI solutions.</td>
      <td>## 🌟 论文解读 | BEHAVIOR：虚拟交互生态环境中日常家庭活动的基准<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的发展，模拟环境中的基准测试对于评估和推动智能体在现实世界中的表现至关重要。然而，现有的基准测试往往缺乏现实性、多样性和复杂性，无法全面评估智能体在真实世界中的能力。为了解决这个问题，本文提出了BEHAVIOR，一个包含100个日常家庭活动的基准测试，旨在模拟真实世界中的挑战，并推动智能体在现实世界中的发展。<br><br>## 🚀 核心方法<br>💡 创新点1：基于谓词逻辑的描述语言<br>BEHAVIOR引入了一种基于谓词逻辑的描述语言，用于表达活动的初始和目标条件。这种语言允许生成多样化的活动实例，并能够接受任何有意义的解决方案。<br><br>💡 创新点2：模拟器无关的环境特征<br>BEHAVIOR确定了支持其活动的模拟器无关特征，并在iGibson 2.0中实现了这些特征。这使得BEHAVIOR可以在多种环境中实现，并提供了无限多样化的活动实例。<br><br>💡 创新点3：基于人类表现的评估指标<br>BEHAVIOR引入了一系列评估指标，用于衡量智能体在任务进度和效率方面的表现。这些指标包括成功分数、效率指标和基于人类表现的指标，以确保评估的公平性和可比性。<br><br>## 📈 实验结果<br>实验结果表明，即使是当前最先进的智能体，在面对BEHAVIOR的挑战时也难以取得良好的表现。这表明BEHAVIOR的基准测试具有很高的难度，能够有效地评估智能体在现实世界中的能力。<br><br>## 💬 可借鉴之处<br>BEHAVIOR的基准测试为评估和推动智能体在现实世界中的发展提供了重要的工具。其基于谓词逻辑的描述语言、模拟器无关的环境特征和基于人类表现的评估指标，为其他基准测试提供了可借鉴的经验。此外，BEHAVIOR的基准测试还可以用于开发新的智能体解决方案，并推动人工智能技术的发展。</td>
    </tr>
    <tr>
      <th>108</th>
      <td>Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</td>
      <td>The recent large-scale vision-language pre-training (VLP) of dual-stream<br>architectures (e.g., CLIP) with a tremendous amount of image-text pair data,<br>has shown its superiority on various multimodal alignment tasks. Despite its<br>success, the resulting models are not capable of multimodal generative tasks<br>due to the weak text encoder. To tackle this problem, we propose to augment the<br>dual-stream VLP model with a textual pre-trained language model (PLM) via<br>vision-language knowledge distillation (VLKD), enabling the capability for<br>multimodal generation. VLKD is pretty data- and computation-efficient compared<br>to the pre-training from scratch. Experimental results show that the resulting<br>model has strong zero-shot performance on multimodal generation tasks, such as<br>open-ended visual question answering and image captioning. For example, it<br>achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous<br>state-of-the-art zero-shot model with \( 7\times \) fewer parameters. Furthermore,<br>the original textual language understanding and generation ability of the PLM<br>is maintained after VLKD, which makes our model versatile for both multimodal<br>and unimodal tasks.</td>
      <td>## 🌟 论文解读 | 通过视觉-语言知识蒸馏在CLIP上实现多模态生成<br><br>## 📌 背景痛点/本文动机<br>近年来，大规模的视觉-语言预训练（VLP）模型在双流架构（如CLIP）上取得了显著成果，尤其在各种多模态对齐任务中表现出色。然而，这些模型在多模态生成任务（如图像字幕和开放式视觉问答）上的表现却相对较弱，主要归因于文本编码器的不足。<br><br>## 🚀 核心方法<br>为了解决这一问题，本文提出了视觉-语言知识蒸馏（VLKD）方法，通过将CLIP的文本编码器替换为预训练的语言模型（PLM，如BART），从而增强模型的多模态生成能力。VLKD方法主要包括以下三个目标：<br><br>💡 创新点1：文本-文本距离最小化（TTDM）<br>通过最小化CLIP文本编码器和BART编码器在相同输入文本下的输出表示之间的ℓ2距离，使两者在文本表示上保持一致。<br><br>💡 创新点2：图像-文本对比学习（ITCL）<br>通过优化BART编码器和CLIP图像编码器输出表示之间的对称InfoNCE损失，使BART编码器更好地适应CLIP的多模态空间。<br><br>💡 创新点3：图像条件文本填充（ICTI）<br>通过在BART解码器上进行图像条件下的文本填充任务，使BART解码器能够理解多模态信息，并生成与图像相关的文本。<br><br>## 📈 实验结果<br>实验结果表明，VLKD模型在多模态生成任务上表现出色，例如在VQAv2数据集上实现了44.5%的零样本准确率，超过了之前最先进的零样本模型，且参数量减少了7倍。此外，VLKD模型在NLP任务上的表现也优于其他VLP模型，证明了该方法的有效性。<br><br>## 💬 可借鉴之处<br>VLKD方法为多模态生成任务提供了一种高效且有效的解决方案，通过知识蒸馏将CLIP和PLM的优势相结合，实现了在零样本和微调设置下的多模态生成能力。此外，该方法还保持了PLM在NLP任务上的原始能力，使其在多模态和单模态任务中都具有广泛的应用前景。</td>
    </tr>
    <tr>
      <th>114</th>
      <td>Agent Planning with World Knowledge Model</td>
      <td>Recent endeavors towards directly using large language models (LLMs) as agent<br>models to execute interactive planning tasks have shown commendable results.<br>Despite their achievements, however, they still struggle with brainless<br>trial-and-error in global planning and generating hallucinatory actions in<br>local planning due to their poor understanding of the ``real'' physical world.<br>Imitating humans' mental world knowledge model which provides global prior<br>knowledge before the task and maintains local dynamic knowledge during the<br>task, in this paper, we introduce parametric World Knowledge Model (WKM) to<br>facilitate agent planning. Concretely, we steer the agent model to<br>self-synthesize knowledge from both expert and sampled trajectories. Then we<br>develop WKM, providing prior task knowledge to guide the global planning and<br>dynamic state knowledge to assist the local planning. Experimental results on<br>three complex real-world simulated datasets with three state-of-the-art<br>open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our<br>method can achieve superior performance compared to various strong baselines.<br>Besides, we analyze to illustrate that our WKM can effectively alleviate the<br>blind trial-and-error and hallucinatory action issues, providing strong support<br>for the agent's understanding of the world. Other interesting findings include:<br>1) our instance-level task knowledge can generalize better to unseen tasks, 2)<br>weak WKM can guide strong agent model planning, and 3) unified WKM training has<br>promising potential for further development. The code is available at<br>https://github.com/zjunlp/WKM.</td>
      <td>## 🌟 论文解读 | 基于世界知识模型的智能体规划<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在自然语言处理任务中取得了显著进展。然而，当直接使用LLMs作为智能体模型执行交互式规划任务时，它们仍然面临着一些挑战。由于LLMs缺乏对真实物理世界的理解，它们在全局规划中容易出现无目的的试错，并在局部规划中生成幻觉行为。为了解决这个问题，本文提出了一个参数化的世界知识模型（WKM），以辅助智能体规划。<br><br>## 🚀 核心方法<br>💡 创新点1：任务知识合成<br>本文通过比较专家轨迹和采样轨迹，引导智能体模型自我合成任务知识。任务知识作为先验知识，用于指导智能体模型的全局规划，避免无目的的试错。<br><br>💡 创新点2：状态知识总结<br>本文通过提示智能体模型，根据历史行为自我总结状态知识，并构建状态知识库。状态知识作为动态知识，用于约束智能体模型的局部规划，避免生成幻觉行为。<br><br>💡 创新点3：世界知识模型训练<br>本文将生成的世界知识集成到专家轨迹中，并训练世界知识模型。智能体模型需要重新训练以适应任务知识。<br><br>💡 创新点4：基于世界知识模型的智能体规划<br>在规划阶段，本文使用世界知识模型为智能体模型提供全局先验任务知识和维护局部动态状态知识。任务知识将作为自然语言形式与特定任务一起连接，以指导智能体模型的试错。在每个规划步骤中，为了防止幻觉行为的出现，本文使用生成的状态知识作为查询，从预先构建的状态知识库中进行kNN检索。然后，使用来自先前动作的约束、检索到的下一个动作的概率以及来自智能体模型概率的加权预测来预测下一个动作。<br><br>## 📈 实验结果<br>本文在三个复杂的真实世界模拟数据集上进行了实验，并与三个最先进的开源LLMs进行了比较。实验结果表明，本文的方法在性能上优于各种强基线。此外，进一步的分析结果表明，本文的WKM可以有效减少无目的的试错和幻觉行为，生成的实例级任务知识可以更好地泛化到未见过的任务，弱WKM可以指导强智能体模型规划，统一WKM训练具有很大的发展潜力。<br><br>## 💬 可借鉴之处<br>本文提出的基于世界知识模型的智能体规划方法，为解决LLMs在交互式规划任务中的挑战提供了一种新的思路。该方法可以有效地提高智能体模型的理解能力和规划能力，并具有很好的泛化能力。此外，本文提出的弱WKM指导强智能体模型规划的思想，也为智能体学习提供了一种新的范式。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Challenges and Applications of Large Language Models</td>
      <td>Large Language Models (LLMs) went from non-existent to ubiquitous in the<br>machine learning discourse within a few years. Due to the fast pace of the<br>field, it is difficult to identify the remaining challenges and already<br>fruitful application areas. In this paper, we aim to establish a systematic set<br>of open problems and application successes so that ML researchers can<br>comprehend the field's current state more quickly and become productive.</td>
      <td>## 🌟 论文解读 | 大型语言模型：挑战与应用<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在短短几年内从无到有，迅速成为机器学习领域的主流。然而，由于该领域发展迅速，识别剩余的挑战和已经取得成功的应用领域变得困难。本文旨在建立一个系统的开放问题和应用成功案例的集合，以便机器学习研究人员可以更快地理解该领域的当前状态并提高效率。<br><br>## 🚀 核心方法<br>本文将LLMs的挑战分为三个主要类别：“设计”、“行为”和“科学”。<br><br>### 设计挑战<br>* **数据集难以理解**： 预训练数据集的规模使得任何个人都无法彻底阅读或对包含的文档进行质量评估。<br>* **分词器依赖性**： 分词器引入了多个挑战，例如计算开销、语言依赖性、处理新词、固定词汇量、信息丢失和低人类可解释性。<br>* **高预训练成本**： 训练单个LLM可能需要数十万小时的计算时间，成本高达数百万美元，并消耗相当于几个典型美国家庭一年的能源量。<br>* **微调开销**： 预训练LLMs在大量和多样化的文本数据集上，可能导致结果模型难以明确捕获特定任务数据集的分布特性。<br>* **高推理延迟**： LLMs的推理延迟仍然很高，因为低并行性和大型内存占用。<br><br>### 行为挑战<br>* **提示脆弱性**： 提示的语法和语义可以对模型的输出产生重大影响。<br>* **幻觉**： LLMs经常产生包含不准确信息的幻觉，这些信息可能难以检测，因为文本流畅自然。<br>* **行为错位**： LLMs经常生成与人类价值观或意图不一致的输出，这可能导致意外或负面的后果。<br><br>### 科学挑战<br>* **过时的知识**： 预训练期间学习的事实性信息可能包含不准确或随时间过时的信息。<br>* **基于静态、人工编写的真实值评估**： 静态基准随着时间的推移变得不那么有用，因为模型的性能不断提高，而更新它们通常依赖于人工编写的真实值。<br>* **难以区分生成文本和人工编写文本**： 难以区分LLMs生成的文本和人工编写的文本。<br>* **无法通过规模解决的任务**： 一些任务似乎无法通过进一步扩展数据/模型规模来解决。<br>* **缺乏实验设计**： 许多论文没有进行受控实验（消融实验），这尤其成问题，因为它们具有很大的设计空间。<br>* **缺乏可重复性**： LLM研究的可重复性存在两个独特的问题：训练运行的重复性和闭源API服务模型的推理重复性。<br><br>## 📈 实验结果<br>本文没有提供具体的实验结果，而是对LLMs的挑战和应用进行了全面的概述。<br><br>## 💬 可借鉴之处<br>本文为LLMs的挑战和应用提供了一个全面的概述，为研究人员和实践者提供了宝贵的见解。本文还强调了LLMs的局限性，并提出了未来研究的方向。</td>
    </tr>
    <tr>
      <th>79</th>
      <td>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</td>
      <td>The automatic evaluation of LLM-based agent intelligence is critical in<br>developing advanced LLM-based agents. Although considerable effort has been<br>devoted to developing human-annotated evaluation datasets, such as AlpacaEval,<br>existing techniques are costly, time-consuming, and lack adaptability. In this<br>paper, inspired by the popular language game ``Who is Spy'', we propose to use<br>the word guessing game to assess the intelligence performance of LLMs. Given a<br>word, the LLM is asked to describe the word and determine its identity (spy or<br>not) based on its and other players' descriptions. Ideally, an advanced agent<br>should possess the ability to accurately describe a given word using an<br>aggressive description while concurrently maximizing confusion in the<br>conservative description, enhancing its participation in the game. To this end,<br>we first develop DEEP to evaluate LLMs' expression and disguising abilities.<br>DEEP requires LLM to describe a word in aggressive and conservative modes. We<br>then introduce SpyGame, an interactive multi-agent framework designed to assess<br>LLMs' intelligence through participation in a competitive language-based board<br>game. Incorporating multi-agent interaction, SpyGame requires the target LLM to<br>possess linguistic skills and strategic thinking, providing a more<br>comprehensive evaluation of LLMs' human-like cognitive abilities and<br>adaptability in complex communication situations. The proposed evaluation<br>framework is very easy to implement. We collected words from multiple sources,<br>domains, and languages and used the proposed evaluation framework to conduct<br>experiments. Extensive experiments demonstrate that the proposed DEEP and<br>SpyGame effectively evaluate the capabilities of various LLMs, capturing their<br>ability to adapt to novel situations and engage in strategic communication.</td>
      <td>## 🌟 论文解读 | 利用猜词游戏评估大型语言模型的智能<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）如ChatGPT、GPT-4和Bard等在各个任务中展现出惊人的性能，开发基于LLMs的智能代理变得越来越重要。然而，现有的评估LLMs智能的方法存在两个主要问题：1）人工标注成本高，耗时且缺乏可扩展性和适应性；2）无法全面反映LLMs的智能。为了解决这些问题，本文提出了一种新的评估方法，即利用猜词游戏来评估LLMs的智能。<br><br>## 🚀 核心方法<br>💡 创新点1：DEEP框架<br>本文首先提出了DEEP框架，用于评估LLMs的表达和伪装能力。DEEP要求LLMs以激进和保守两种模式描述一个给定的词，并利用GPT-4来判断这些描述是否准确。激进模式要求LLMs提供清晰、详细和准确的描述，而保守模式则要求LLMs提供模糊的描述以伪装目标词。<br><br>💡 创新点2：SpyGame框架<br>本文还提出了SpyGame框架，这是一个交互式多智能体框架，旨在通过参与竞争性语言游戏“谁是卧底”来评估LLMs的智能。SpyGame要求LLMs具备语言技能和战略思维能力，从而更全面地评估LLMs在复杂沟通情境中的人类认知能力和适应性。<br><br>## 📈 实验结果<br>本文对四种开源LLMs和两种闭源LLMs进行了实验，结果表明，闭源LLMs（如GPT-4和GPT-3.5）在激进和保守模式下的表现明显优于开源模型。此外，SpyGame框架能够有效地评估LLMs在多智能体交互中的能力，捕捉它们适应新情况并进行战略沟通的能力。<br><br>## 💬 可借鉴之处<br>本文提出的DEEP和SpyGame框架为评估LLMs的智能提供了一种新的方法，具有以下可借鉴之处：<br>1. 利用游戏进行评估，更具互动性和趣味性。<br>2. 关注LLMs的表达和伪装能力，更全面地评估其智能。<br>3. SpyGame框架支持人类参与，更贴近真实场景。<br>4. 针对多智能体交互中的偏差问题，提出了有效的解决方案。<br><br>总而言之，本文提出的评估方法为LLMs的智能评估提供了新的思路，有助于推动LLMs的发展和应用。</td>
    </tr>
    <tr>
      <th>56</th>
      <td>Q-Cogni: An Integrated Causal Reinforcement Learning Framework</td>
      <td>We present Q-Cogni, an algorithmically integrated causal reinforcement<br>learning framework that redesigns Q-Learning with an autonomous causal<br>structure discovery method to improve the learning process with causal<br>inference. Q-Cogni achieves optimal learning with a pre-learned structural<br>causal model of the environment that can be queried during the learning process<br>to infer cause-and-effect relationships embedded in a state-action space. We<br>leverage on the sample efficient techniques of reinforcement learning, enable<br>reasoning about a broader set of policies and bring higher degrees of<br>interpretability to decisions made by the reinforcement learning agent. We<br>apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against<br>state-of-the-art reinforcement learning algorithms. We report results that<br>demonstrate better policies, improved learning efficiency and superior<br>interpretability of the agent's decision making. We also compare this approach<br>with traditional shortest-path search algorithms and demonstrate the benefits<br>of our causal reinforcement learning framework to high dimensional problems.<br>Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New<br>York City using the Taxi & Limousine Commission trip record data and compare<br>with shortest-path search, reporting results that show 85% of the cases with an<br>equal or better policy derived from Q-Cogni in a real-world domain.</td>
      <td></td>
    </tr>
    <tr>
      <th>37</th>
      <td>CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</td>
      <td>The rapid advancement of chat-based language models has led to remarkable<br>progress in complex task-solving. However, their success heavily relies on<br>human input to guide the conversation, which can be challenging and<br>time-consuming. This paper explores the potential of building scalable<br>techniques to facilitate autonomous cooperation among communicative agents, and<br>provides insight into their "cognitive" processes. To address the challenges of<br>achieving autonomous cooperation, we propose a novel communicative agent<br>framework named role-playing. Our approach involves using inception prompting<br>to guide chat agents toward task completion while maintaining consistency with<br>human intentions. We showcase how role-playing can be used to generate<br>conversational data for studying the behaviors and capabilities of a society of<br>agents, providing a valuable resource for investigating conversational language<br>models. In particular, we conduct comprehensive studies on<br>instruction-following cooperation in multi-agent settings. Our contributions<br>include introducing a novel communicative agent framework, offering a scalable<br>approach for studying the cooperative behaviors and capabilities of multi-agent<br>systems, and open-sourcing our library to support research on communicative<br>agents and beyond: https://github.com/camel-ai/camel.</td>
      <td>## 🌟 论文解读 | CAMEL：探索大型语言模型社会的“心智”交流<br><br>## 📌 背景痛点/本文动机<br>随着基于聊天的语言模型在复杂任务解决方面的快速发展，它们在解决复杂任务方面取得了显著进展。然而，这些模型的成功严重依赖于人类输入来引导对话，这可能会具有挑战性且耗时。本文探讨了构建可扩展技术以促进交流代理之间的自主合作，并深入了解其“认知”过程的潜力。<br><br>## 🚀 核心方法<br>💡 创新点1：角色扮演框架<br>为了解决实现自主合作的挑战，本文提出了一种名为“角色扮演”的新型交流代理框架。该框架涉及使用“起始提示”来引导聊天代理完成任务，同时保持与人类意图的一致性。<br><br>💡 创新点2：起始提示<br>本文提出了一种名为“起始提示”的对话LLM自动提示方法，使代理能够通过角色扮演相互提示以解决问题。AI用户不断向AI助手提供指令以解决问题，这使我们能够保存指令-解决方案对并创建多样化、指令性、对话性和面向任务的语料库。<br><br>💡 创新点3：数据集生成<br>本文展示了如何使用角色扮演来让聊天代理相互交流以完成任务，并记录他们的对话以进行行为分析和能力理解。特别是，我们对多代理设置中的指令遵循合作进行了全面研究。<br><br>💡 创新点4：开源库<br>本文开源了我们的库，其中包含各种代理的实现、数据生成管道、数据分析工具和收集的数据集，以支持对交流代理的研究。<br><br>## 📈 实验结果<br>本文通过实验评估了CAMEL框架的性能，结果表明CAMEL解决方案在人类评估和GPT4评估中均优于gpt-3.5-turbo单次解决方案。此外，本文还研究了LLM训练能力的显著出现，通过在通过框架生成的不断增长的语料库上微调LLaMA模型。<br><br>## 💬 可借鉴之处<br>本文提出的CAMEL框架为研究交流代理之间的自主合作提供了可扩展的方法，并提供了应对挑战的策略。此外，本文开源的库为研究交流代理和更广泛的研究领域提供了宝贵的资源。</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Word sense disambiguation: a survey</td>
      <td>In this paper, we made a survey on Word Sense Disambiguation (WSD). Near<br>about in all major languages around the world, research in WSD has been<br>conducted upto different extents. In this paper, we have gone through a survey<br>regarding the different approaches adopted in different research works, the<br>State of the Art in the performance in this domain, recent works in different<br>Indian languages and finally a survey in Bengali language. We have made a<br>survey on different competitions in this field and the bench mark results,<br>obtained from those competitions.</td>
      <td>## 🌟 论文解读 | 词义消歧：探索自然语言处理的前沿技术<br><br>## 📌 背景痛点/本文动机<br>词义消歧（Word Sense Disambiguation, WSD）是自然语言处理（NLP）领域中的一个重要挑战。由于许多单词在不同的语境中具有不同的含义，因此机器在理解和处理自然语言时往往会遇到困难。本文旨在对词义消歧技术进行全面的调查，探讨不同研究工作中采用的不同方法，以及该领域在性能方面的最新进展。<br><br>## 🚀 核心方法<br>本文主要介绍了三种词义消歧方法：<br><br>### 💡 创新点1：基于知识的方法<br>这种方法依赖于机器可读词典、词义库、同义词库等知识源。例如，Lesk算法通过比较句子中单词的词典定义来确定词义。此外，语义相似度、选择偏好和启发式方法也被用于基于知识的方法中。<br><br>### 💡 创新点2：监督学习方法<br>监督学习方法使用手动创建的词义标注数据来训练机器学习模型。例如，决策列表、决策树、朴素贝叶斯、神经网络、基于实例的学习和支持向量机等方法都被用于监督学习方法中。<br><br>### 💡 创新点3：无监督学习方法<br>无监督学习方法不依赖于外部知识源或词义库。例如，基于上下文聚类的无监督学习方法通过将上下文向量分组到簇中来识别词义。此外，基于词聚类的无监督学习方法通过将语义相同的单词分组到簇中来识别词义。<br><br>## 📈 实验结果<br>本文对词义消歧技术的性能进行了评估，并总结了不同方法在不同语言和任务上的表现。结果表明，监督学习方法在词义消歧任务中取得了最好的性能。<br><br>## 💬 可借鉴之处<br>本文对词义消歧技术进行了全面的调查，并总结了不同方法的优缺点。这对于研究人员和开发者来说是一个宝贵的资源，可以帮助他们选择合适的词义消歧方法来解决实际问题。此外，本文还介绍了词义消歧技术在印度语言中的应用，这对于推动印度语言的自然语言处理研究具有重要意义。</td>
    </tr>
    <tr>
      <th>58</th>
      <td>Grounding Language with Visual Affordances over Unstructured Data</td>
      <td>Recent works have shown that Large Language Models (LLMs) can be applied to<br>ground natural language to a wide variety of robot skills. However, in<br>practice, learning multi-task, language-conditioned robotic skills typically<br>requires large-scale data collection and frequent human intervention to reset<br>the environment or help correcting the current policies. In this work, we<br>propose a novel approach to efficiently learn general-purpose<br>language-conditioned robot skills from unstructured, offline and reset-free<br>data in the real world by exploiting a self-supervised visuo-lingual affordance<br>model, which requires annotating as little as 1% of the total data with<br>language. We evaluate our method in extensive experiments both in simulated and<br>real-world robotic tasks, achieving state-of-the-art performance on the<br>challenging CALVIN benchmark and learning over 25 distinct visuomotor<br>manipulation tasks with a single policy in the real world. We find that when<br>paired with LLMs to break down abstract natural language instructions into<br>subgoals via few-shot prompting, our method is capable of completing<br>long-horizon, multi-tier tasks in the real world, while requiring an order of<br>magnitude less data than previous approaches. Code and videos are available at<br>http://hulc2.cs.uni-freiburg.de</td>
      <td>## 🌟 论文解读 | HULC++：利用视觉语言亲和力模型高效学习语言条件机器人技能<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在将自然语言与机器人技能相结合方面取得了显著进展。然而，在实践中，学习多任务、语言条件的机器人技能通常需要大规模的数据收集和频繁的人工干预来重置环境或帮助纠正当前策略。本文提出了一种新颖的方法，通过利用自监督的视觉语言亲和力模型，从现实世界中的非结构化、离线和无需重置的数据中高效地学习通用语言条件的机器人技能，这只需要对总数据中的1%进行语言标注。<br><br>## 🚀 核心方法<br>💡 创新点1：HULC++ 架构<br>本文提出了 HULC++ 架构，它结合了 HULC 的任务无关控制和 VAPO 的以对象为中心的语义理解。HULC 是一种最先进的语言条件模仿学习代理，可以端到端地学习 7-DoF 目标达到策略。VAPO 从非结构化数据中提取自监督的视觉亲和力模型，不仅可以加速学习，还可以提高下游控制策略的泛化能力。<br><br>💡 创新点2：自监督视觉语言亲和力模型<br>本文提出了一种自监督的视觉语言亲和力模型，可以从非结构化的人类远程操作数据中自动提取亲和力。该模型利用抓取动作作为启发式方法，发现场景中与任务完成相关的元素。通过将末端执行器世界位置投影到相机图像中，并使用抓取动作作为监督，模型可以学习预测与任务相关的对象的像素位置。<br><br>💡 创新点3：多任务 7-DoF 语言条件视觉运动策略<br>本文提出了一种多任务 7-DoF 语言条件视觉运动策略，该策略基于 HULC 并从非结构化数据中训练。该策略可以在预测的亲和力区域附近与场景进行交互。<br><br>💡 创新点4：与 LLMs 的结合<br>本文提出了一种将 HULC++ 与 LLMs 结合的方法，以将抽象的自然语言指令分解为一系列可行的子任务。LLMs 可以通过少量样本提示将抽象的自然语言指令翻译成一系列子目标，从而实现长距离、多级任务的执行。<br><br>## 📈 实验结果<br>本文在模拟和现实世界的机器人任务中进行了广泛的实验，结果表明，HULC++ 在具有挑战性的 CALVIN 基准测试中取得了最先进的性能，并使用单个策略在现实世界中学习了超过 25 个不同的视觉运动操作任务。此外，当与 LLMs 结合使用时，HULC++ 能够在现实世界中完成长距离、多级任务，而所需的数据量比以前的方法少一个数量级。<br><br>## 💬 可借鉴之处<br>本文提出的 HULC++ 架构和自监督视觉语言亲和力模型为高效学习语言条件机器人技能提供了一种新颖的方法。该方法可以应用于各种机器人任务，并具有以下优势：<br>- **数据效率高**：只需要对总数据中的1%进行语言标注。<br>- **无需重置环境**：可以从非结构化、离线和无需重置的数据中学习。<br>- **泛化能力强**：在模拟和现实世界的机器人任务中取得了最先进的性能。<br>- **可扩展性强**：可以与 LLMs 结合使用，以完成更复杂的任务。<br><br>## 🌟 总结<br>HULC++ 是一种很有前景的方法，可以高效地学习语言条件机器人技能。该方法具有数据效率高、无需重置环境、泛化能力强和可扩展性强等优势，有望在未来的机器人应用中发挥重要作用。</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation</td>
      <td>Recent breakthroughs in large language models (LLMs) have brought remarkable<br>success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is<br>that the information processed by LLMs is consistently honest, neglecting the<br>pervasive deceptive or misleading information in human society and AI-generated<br>content. This oversight makes LLMs susceptible to malicious manipulations,<br>potentially resulting in detrimental outcomes. This study utilizes the<br>intricate Avalon game as a testbed to explore LLMs' potential in deceptive<br>environments. Avalon, full of misinformation and requiring sophisticated logic,<br>manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'<br>recursive thinking and perspective-taking in the Avalon game, we introduce a<br>novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to<br>identify and counteract deceptive information. ReCon combines formulation and<br>refinement contemplation processes; formulation contemplation produces initial<br>thoughts and speech, while refinement contemplation further polishes them.<br>Additionally, we incorporate first-order and second-order perspective<br>transitions into these processes respectively. Specifically, the first-order<br>allows an LLM agent to infer others' mental states, and the second-order<br>involves understanding how others perceive the agent's mental state. After<br>integrating ReCon with different LLMs, extensive experiment results from the<br>Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around<br>deceptive information without extra fine-tuning and data. Finally, we offer a<br>possible explanation for the efficacy of ReCon and explore the current<br>limitations of LLMs in terms of safety, reasoning, speaking style, and format,<br>potentially furnishing insights for subsequent research.</td>
      <td>## 🌟 论文解读 | Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在作为智能体（LLM-as-Agent）方面取得了显著进展。然而，这些研究通常假设LLMs处理的信息始终是诚实的，忽略了人类社会中普遍存在的欺骗性或误导性信息以及AI生成内容中的潜在问题。这种假设使得LLMs容易受到恶意操纵，可能导致不良后果。本文旨在探索LLMs在欺骗性环境中的潜力，并提出了一个名为“递归沉思”（ReCon）的新框架，以增强LLMs识别和对抗欺骗性信息的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：递归沉思框架（ReCon）<br>ReCon框架结合了“构思沉思”和“精炼沉思”两个认知过程。构思沉思产生初始思考和言语，而精炼沉思则进一步改进它们。此外，ReCon还引入了第一阶和第二阶视角转换，分别对应于这两个过程。第一阶视角转换允许LLM智能体从自己的角度推断他人的心理状态，而第二阶视角转换则涉及理解他人如何看待智能体的心理状态。<br><br>💡 创新点2：在Avalon游戏中测试ReCon<br>本文使用复杂的Avalon游戏作为测试平台，该游戏充满误导信息，需要复杂的逻辑推理。实验结果表明，ReCon能够有效地帮助LLMs识别和应对欺骗性信息，而无需额外的微调和数据。<br><br>## 📈 实验结果<br>实验结果表明，ReCon在Avalon游戏中表现出色，能够帮助LLMs识别和应对欺骗性信息，而无需额外的微调和数据。与基线方法相比，ReCon在多个维度上都有显著提升，包括隐蔽性、逻辑性、贡献度、说服力、信息量和创造力。<br><br>## 💬 可借鉴之处<br>本文提出的ReCon框架为LLMs在欺骗性环境中的应用提供了新的思路。ReCon框架的设计和实现方法可以借鉴到其他需要识别和对抗欺骗性信息的场景中。此外，本文还讨论了LLMs在安全性、推理能力、言语风格和格式等方面的局限性，为未来的研究提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>83</th>
      <td>Humanoid Agents: Platform for Simulating Human-like Generative Agents</td>
      <td>Just as computational simulations of atoms, molecules and cells have shaped<br>the way we study the sciences, true-to-life simulations of human-like agents<br>can be valuable tools for studying human behavior. We propose Humanoid Agents,<br>a system that guides Generative Agents to behave more like humans by<br>introducing three elements of System 1 processing: Basic needs (e.g. hunger,<br>health and energy), Emotion and Closeness in Relationships. Humanoid Agents are<br>able to use these dynamic elements to adapt their daily activities and<br>conversations with other agents, as supported with empirical experiments. Our<br>system is designed to be extensible to various settings, three of which we<br>demonstrate, as well as to other elements influencing human behavior (e.g.<br>empathy, moral values and cultural background). Our platform also includes a<br>Unity WebGL game interface for visualization and an interactive analytics<br>dashboard to show agent statuses over time. Our platform is available on<br>https://www.humanoidagents.com/ and code is on<br>https://github.com/HumanoidAgents/HumanoidAgents</td>
      <td>## 🌟 论文解读 | 人类化智能体：模拟人类行为的生成式智能体平台<br><br>## 📌 背景痛点/本文动机<br>随着生成式智能体（Generative Agents）的出现，人们开始尝试使用高级自然语言处理系统来模拟人类行为。然而，现有的生成式智能体主要关注逻辑和计划，缺乏对人类直觉和即时反应的模拟。为了解决这个问题，本文提出了人类化智能体（Humanoid Agents）平台，旨在通过引入基本需求、情感和关系亲密度等元素，使智能体更接近人类的真实行为。<br><br>## 🚀 核心方法<br>💡 创新点1：引入系统1思维<br>本文借鉴了心理学中的系统1思维，即直觉、无意识和即时的思维过程。通过引入基本需求、情感和关系亲密度等元素，使智能体能够根据自身状态和环境变化做出更自然的反应。<br><br>💡 创新点2：动态调整行为<br>人类化智能体平台允许智能体根据自身的基本需求、情感和关系亲密度动态调整其日常活动和对话。例如，当智能体感到饥饿时，它会寻找食物；当它感到孤独时，它会尝试与其他智能体进行交流。<br><br>## 📈 实验结果<br>实验结果表明，人类化智能体能够有效地模拟人类行为。与人类标注相比，该系统能够准确预测活动是否满足基本需求、活动表达的情感以及对话是否使智能体之间的关系更加亲密。<br><br>## 💬 可借鉴之处<br>本文提出的Humanoid Agents平台为研究人类行为提供了一个有价值的工具。该平台可以扩展到各种场景，并支持更多影响人类行为的元素，如同理心、道德价值观和文化背景。此外，该平台还提供了Unity WebGL游戏界面和交互式分析仪表板，方便用户可视化智能体的状态和行为。<br><br>## 🌐 平台访问<br>- 平台网站：https://www.humanoidagents.com/<br>- 代码仓库：https://github.com/HumanoidAgents/HumanoidAgents</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
      <td>Believable proxies of human behavior can empower interactive applications<br>ranging from immersive environments to rehearsal spaces for interpersonal<br>communication to prototyping tools. In this paper, we introduce generative<br>agents--computational software agents that simulate believable human behavior.<br>Generative agents wake up, cook breakfast, and head to work; artists paint,<br>while authors write; they form opinions, notice each other, and initiate<br>conversations; they remember and reflect on days past as they plan the next<br>day. To enable generative agents, we describe an architecture that extends a<br>large language model to store a complete record of the agent's experiences<br>using natural language, synthesize those memories over time into higher-level<br>reflections, and retrieve them dynamically to plan behavior. We instantiate<br>generative agents to populate an interactive sandbox environment inspired by<br>The Sims, where end users can interact with a small town of twenty five agents<br>using natural language. In an evaluation, these generative agents produce<br>believable individual and emergent social behaviors: for example, starting with<br>only a single user-specified notion that one agent wants to throw a Valentine's<br>Day party, the agents autonomously spread invitations to the party over the<br>next two days, make new acquaintances, ask each other out on dates to the<br>party, and coordinate to show up for the party together at the right time. We<br>demonstrate through ablation that the components of our agent<br>architecture--observation, planning, and reflection--each contribute critically<br>to the believability of agent behavior. By fusing large language models with<br>computational, interactive agents, this work introduces architectural and<br>interaction patterns for enabling believable simulations of human behavior.</td>
      <td>## 🌟 论文解读 | 生成式智能体：模拟人类行为的交互式模拟<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，人们对于能够模拟人类行为的智能体产生了浓厚的兴趣。这些智能体可以应用于各种场景，例如沉浸式环境、人际沟通演练空间、原型设计工具等。然而，要创建一个能够长期保持一致性和可信度的智能体仍然是一个挑战。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：生成式智能体<br>本文提出了生成式智能体的概念，即利用生成模型模拟可信的人类行为。这些智能体能够进行日常活动，如起床、做饭、上班等，并能够形成自己的观点、与他人互动、发起对话等。<br><br>💡 创新点2：智能体架构<br>为了实现生成式智能体，本文提出了一种新的架构，该架构扩展了大型语言模型，使其能够存储智能体的经验记录，并将这些记忆随着时间的推移合成更高层次的反思，并动态地检索它们来规划行为。<br><br>## 📈 实验结果<br>本文通过在模拟环境中创建一个由25个智能体组成的小镇，展示了生成式智能体的潜力。实验结果表明，这些智能体能够产生可信的个体和群体行为，例如，在用户指定一个智能体想要举办情人节派对的情况下，智能体能够自主地邀请其他智能体参加派对，并协调在正确的时间一起到达派对地点。<br><br>## 💬 可借鉴之处<br>本文提出的生成式智能体架构为创建可信的人类行为模拟提供了新的思路。该架构可以应用于各种领域，例如角色扮演、社交原型设计、虚拟世界和游戏等。此外，本文还讨论了生成式智能体在交互式系统中的应用机会、伦理和社会风险。</td>
    </tr>
    <tr>
      <th>26</th>
      <td>RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing</td>
      <td>Code auditing is a code review process with the goal of finding bugs. Large<br>Language Models (LLMs) have shown substantial potential in this task, offering<br>the ability to analyze programs without compilation and enabling customized bug<br>detection following specified prompts. However, applying LLMs to<br>repository-level code auditing presents notable challenges. The inherent<br>context limits and hallucinations of LLMs can lead to the low quality of bug<br>reports. Meanwhile, the large size of software repositories introduces<br>substantial time and token costs, hindering efficiency and scalability in<br>real-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit,<br>designed to enable precise and efficient repository-level code auditing.<br>Equipped with the agent memory, RepoAudit explores the code repository on<br>demand, analyzing data-flow facts along different feasible program paths in<br>individual functions. It also introduces the validator to check the data-flow<br>facts for hallucination mitigation and examine the satisfiability of path<br>conditions of potential buggy paths, which enables RepoAudit to discard false<br>positives in the code auditing. Our experiment shows that RepoAudit powered by<br>Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems,<br>consuming 0.44 hours and $2.54 per project on average.</td>
      <td>## 🌟 论文解读 | RepoAudit：基于LLM的代码审计利器<br><br>## 📌 背景痛点/本文动机<br>随着软件代码库的快速膨胀，传统的代码审计方法面临着效率低下、难以扩展等挑战。大型语言模型（LLMs）在代码审计方面展现出巨大潜力，但直接应用于代码库级别的审计存在局限性，例如LLMs的上下文限制和幻觉问题，以及代码库规模导致的成本问题。<br><br>## 🚀 核心方法<br>RepoAudit 是一个自主的 LLM-Agent，旨在实现精确和高效的代码库级别代码审计。它包含三个主要组件：<br><br>💡 创新点1：路径敏感和按需驱动<br>RepoAudit 通过路径敏感和按需驱动的图遍历方式，模拟人类审计过程，有效解决 LLMs 在处理大型程序图时的局限性。它将程序分解为更小的单元（如函数），并逐步分析每个单元，从而避免路径爆炸问题。<br><br>💡 创新点2：验证机制<br>RepoAudit 引入验证器来检查数据流事实，以减轻 LLMs 的幻觉问题，并检查潜在错误路径的路径条件，从而排除代码审计中的误报。<br><br>## 📈 实验结果<br>RepoAudit 在 15 个真实世界系统中成功发现了 38 个真实漏洞，平均每个项目耗时 0.44 小时，成本为 2.54 美元。与 Meta INFER 和 Amazon CODEGURU 等工业静态漏洞检测工具相比，RepoAudit 具有更高的检测能力和更低的误报率。<br><br>## 💬 可借鉴之处<br>RepoAudit 的设计理念和方法为 LLM 在代码审计领域的应用提供了新的思路，其路径敏感和按需驱动的图遍历方式、验证机制等创新点值得借鉴。此外，RepoAudit 的设计也为其他 LLM 驱动的代码任务提供了参考，例如程序修复、测试和分析等。</td>
    </tr>
    <tr>
      <th>82</th>
      <td>Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis</td>
      <td>Since the introduction of ChatGPT and GPT-4, these models have been tested<br>across a large number of tasks. Their adeptness across domains is evident, but<br>their aptitude in playing games, and specifically their aptitude in the realm<br>of poker has remained unexplored. Poker is a game that requires decision making<br>under uncertainty and incomplete information. In this paper, we put ChatGPT and<br>GPT-4 through the poker test and evaluate their poker skills. Our findings<br>reveal that while both models display an advanced understanding of poker,<br>encompassing concepts like the valuation of starting hands, playing positions<br>and other intricacies of game theory optimal (GTO) poker, both ChatGPT and<br>GPT-4 are NOT game theory optimal poker players.<br>  Profitable strategies in poker are evaluated in expectations over large<br>samples. Through a series of experiments, we first discover the characteristics<br>of optimal prompts and model parameters for playing poker with these models.<br>Our observations then unveil the distinct playing personas of the two models.<br>We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This<br>exploration then sheds light on the divergent poker tactics of the two models:<br>ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker<br>vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which<br>means that it has a propensity to only engage with premium hands and folds a<br>majority of hands. When subjected to the same directive, GPT-4 plays like a<br>maniac, showcasing a loose and aggressive style of play. Both strategies,<br>although relatively advanced, are not game theory optimal.</td>
      <td>## 🌟 论文解读 | ChatGPT 和 GPT-4 在德州扑克中的表现：一场前翻牌分析<br><br>## 📌 背景痛点/本文动机<br>随着 ChatGPT 和 GPT-4 的推出，这些模型在各种任务中表现出色，但在游戏，尤其是扑克游戏方面的能力尚未得到充分探索。扑克是一种需要在不完整信息和不确定性下做出决策的游戏，因此本文旨在评估 ChatGPT 和 GPT-4 在德州扑克中的表现，特别是它们在前翻牌阶段的决策能力。<br><br>## 🚀 核心方法<br>本文通过一系列实验，评估了 ChatGPT 和 GPT-4 在德州扑克前翻牌阶段的决策能力。实验中，研究人员使用了不同的提示和模型参数，以探索两种模型在扑克游戏中的最佳表现。他们还分析了两种模型的独特游戏风格，并比较了它们与游戏理论最优（GTO）策略的差异。<br><br>## 📈 实验结果<br>实验结果表明，ChatGPT 和 GPT-4 都对扑克游戏有深入的理解，包括起手牌的估值、游戏位置和其他游戏理论最优策略的细节。然而，两种模型都不是游戏理论最优的扑克玩家。ChatGPT 倾向于保守的游戏风格，只参与优质牌局，而 GPT-4 则表现出更加激进的游戏风格，参与更多的牌局。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，尽管 ChatGPT 和 GPT-4 在扑克游戏方面表现出色，但它们仍然存在局限性。这些模型在理解游戏理论最优策略方面存在偏差，这可能是由于它们在训练过程中没有针对扑克游戏进行专门训练。此外，本文的研究结果也表明，不同的提示和模型参数对模型在扑克游戏中的表现有显著影响。因此，未来的研究可以探索如何通过优化提示和模型参数来提高模型在扑克游戏中的表现。</td>
    </tr>
    <tr>
      <th>98</th>
      <td>AI2-THOR: An Interactive 3D Environment for Visual AI</td>
      <td>We introduce The House Of inteRactions (THOR), a framework for visual AI<br>research, available at http://ai2thor.allenai.org. AI2-THOR consists of near<br>photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes<br>and interact with objects to perform tasks. AI2-THOR enables research in many<br>different domains including but not limited to deep reinforcement learning,<br>imitation learning, learning by interaction, planning, visual question<br>answering, unsupervised representation learning, object detection and<br>segmentation, and learning models of cognition. The goal of AI2-THOR is to<br>facilitate building visually intelligent models and push the research forward<br>in this domain.</td>
      <td>## 🌟 论文解读 | AI2-THOR：推动视觉AI研究的新一代交互式3D环境<br><br>## 📌 背景痛点/本文动机<br>人类在视觉理解方面展现出超越当前主流视觉任务（如物体检测、场景识别、图像分割）的能力。视觉智能的关键在于与环境交互并从中学习。然而，当前最先进的计算机视觉模型通常是通过使用静态图像或视频进行训练，这与人类的学习方式不同。为了解决这个问题，本文提出了AI2-THOR，一个基于视觉输入的类似人类学习的框架。<br><br>## 🚀 核心方法<br>💡 创新点1：交互性<br>AI2-THOR支持多种类型的交互，包括物体状态变化、基于臂部的操作和因果交互。例如，可以打开或关闭微波炉，将面包切片并在烤面包机中烤制，以及打开水龙头将水倒入杯子中。<br><br>💡 创新点2：场景多样性<br>AI2-THOR提供了比其他平台更多的交互式物体和场景，通过使用程序生成技术，提供了120个独立的房间、89个迷宫风格的公寓和10个评估房屋。<br><br>💡 创新点3：高质量<br>AI2-THOR中的物体和场景接近照片真实感，这有助于将学习模型更好地转移到现实世界。<br><br>💡 创新点4：API<br>AI2-THOR提供了一个Python API，用于与Unity 3D游戏引擎交互，该引擎提供了许多不同的功能，如导航、施加力、物体交互和物理建模。<br><br>## 📈 实验结果<br>自2017年首次发布以来，AI2-THOR已被用于超过150篇论文的实验，并被下载超过50万次。它在视觉导航、视听导航、视觉和语言、人机交互、模拟到现实转移、多智能体交互、学习对象关系、学习可供性、场景合成、通过交互学习以及计算机视觉和可解释性等领域取得了显著成果。<br><br>## 💬 可借鉴之处<br>AI2-THOR是一个功能强大的交互式3D环境，可以用于各种视觉AI研究。它具有高度的可定制性，并提供了对多种场景、智能体体现、动作和元数据的支持。随着其功能的不断更新和发展，AI2-THOR有望在视觉AI领域发挥更大的作用。</td>
    </tr>
    <tr>
      <th>92</th>
      <td>Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</td>
      <td>In this study, we explore the application of Large Language Models (LLMs) in<br>\textit{Jubensha}, a Chinese detective role-playing game and a novel area in<br>Artificial Intelligence (AI) driven gaming. We introduce the first dataset<br>specifically for Jubensha, including character scripts and game rules, to<br>foster AI agent development in this complex narrative environment. Our work<br>also presents a unique multi-agent interaction framework using LLMs, allowing<br>AI agents to autonomously engage in this game. To evaluate the gaming<br>performance of these AI agents, we developed novel methods measuring their<br>mastery of case information and reasoning skills. Furthermore, we incorporated<br>the latest advancements in in-context learning to improve the agents'<br>performance in information gathering, murderer identification, and logical<br>reasoning. The experimental results validate the effectiveness of our proposed<br>methods. This work aims to offer a novel perspective on understanding LLM<br>capabilities and establish a new benchmark for evaluating large language<br>model-based agents.</td>
      <td>## 🌟 论文解读 | 解码数字侦探：理解大型语言模型在多智能体推理游戏中的行为和能力<br><br>## 📌 背景痛点/本文动机<br>随着互动角色扮演游戏（IRPGs）的全球流行，特别是中国侦探角色扮演游戏“剧本杀”（Jubensha）的兴起，人工智能（AI）在游戏领域的应用也日益受到关注。然而，现有的AI研究主要集中在传统的棋类游戏、视频游戏等领域，对于“剧本杀”这类需要多轮语言交互、信息收集和逻辑推理的游戏，AI的应用还处于起步阶段。本文旨在探索大型语言模型（LLMs）在“剧本杀”游戏中的应用，并建立一个新的评估基准，以衡量LLM在复杂叙事环境中的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：构建了首个专门针对“剧本杀”游戏的中文数据集，包括角色剧本和预设游戏规则，为AI代理的开发提供了基础。<br>💡 创新点2：设计了一个独特的多智能体交互框架，使用LLMs使AI代理能够自主参与“剧本杀”游戏。<br>💡 创新点3：为了评估AI代理在“剧本杀”游戏中的表现，设计了两个新颖的任务：一个用于评估他们对案件信息的掌握程度，另一个用于评估他们的推理能力。<br>💡 创新点4：利用最新的上下文学习技术，设计了模块来增强LLM代理在信息收集、凶手识别和逻辑推理方面的性能。<br><br>## 📈 实验结果<br>实验结果表明，本文提出的方法在信息收集、凶手识别和推理能力方面显著提高了LLM代理的性能。具体来说，与没有记忆检索模块的代理相比，具有记忆检索模块的代理在回答关于其他角色的问题时准确率显著提高。此外，自完善和自验证模块的组合进一步提高了代理的准确率，表明这些模块有效地增强了代理在“剧本杀”游戏中的沟通效率。<br><br>## 💬 可借鉴之处<br>本文的研究为LLMs在复杂叙事环境中的应用提供了新的视角，并为评估LLM代理的性能建立了新的基准。此外，本文提出的ThinkThrice框架和上下文学习模块的设计，为开发更智能、更具推理能力的AI代理提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>55</th>
      <td>LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination</td>
      <td>AI agents powered by Large Language Models (LLMs) have made significant<br>advances, enabling them to assist humans in diverse complex tasks and leading<br>to a revolution in human-AI coordination. LLM-powered agents typically require<br>invoking LLM APIs and employing artificially designed complex prompts, which<br>results in high inference latency. While this paradigm works well in scenarios<br>with minimal interactive demands, such as code generation, it is unsuitable for<br>highly interactive and real-time applications, such as gaming. Traditional<br>gaming AI often employs small models or reactive policies, enabling fast<br>inference but offering limited task completion and interaction abilities. In<br>this work, we consider Overcooked as our testbed where players could<br>communicate with natural language and cooperate to serve orders. We propose a<br>Hierarchical Language Agent (HLA) for human-AI coordination that provides both<br>strong reasoning abilities while keeping real-time execution. In particular,<br>HLA adopts a hierarchical framework and comprises three modules: a proficient<br>LLM, referred to as Slow Mind, for intention reasoning and language<br>interaction, a lightweight LLM, referred to as Fast Mind, for generating macro<br>actions, and a reactive policy, referred to as Executor, for transforming macro<br>actions into atomic actions. Human studies show that HLA outperforms other<br>baseline agents, including slow-mind-only agents and fast-mind-only agents,<br>with stronger cooperation abilities, faster responses, and more consistent<br>language communications.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的分层语言代理：实时人机协作的突破<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的兴起，基于LLMs的AI代理在辅助人类完成复杂任务方面取得了显著进展，推动了人机协作的革命。然而，这些代理通常需要调用LLMs API并使用人工设计的复杂提示，导致推理延迟高。这种范式在交互需求较低的场景（如代码生成）中表现良好，但在需要实时响应和高频交互的应用（如游戏）中并不适用。传统的游戏AI通常采用小型模型或反应策略，虽然能够实现快速推理，但任务完成和交互能力有限。<br><br>## 🚀 核心方法<br>本文提出了一个分层语言代理（HLA），用于实时人机协作，该代理结合了大型模型的强大推理和交互能力以及小型模型和反应策略的实时推理能力。HLA采用分层框架，由三个模块组成：<br><br>* **慢思维（Slow Mind）**：一个熟练的LLM，用于意图推理和语言交互。<br>* **快思维（Fast Mind）**：一个轻量级的LLM，用于生成宏操作。<br>* **执行器（Executor）**：一个反应策略，用于将宏操作转换为原子操作。<br><br>## 📈 实验结果<br>在Overcooked游戏平台上进行的实验表明，HLA在游戏得分、响应延迟和人类偏好方面均优于其他基线代理，包括仅使用慢思维或快思维的代理。HLA展现出更强的协作能力、更快的响应速度和更一致的语言通信。<br><br>## 💬 可借鉴之处<br>* **分层设计**：HLA的分层设计有效地解决了LLMs推理延迟高的问题，使其适用于实时人机协作场景。<br>* **轻量级LLM**：使用轻量级LLM进行宏操作生成，提高了推理速度，同时避免了生成次优操作。<br>* **反应策略**：执行器模块确保了动作的可行性和高频交互，提高了AI代理的实时响应能力。<br><br>## 🌟 总结<br>HLA为实时人机协作提供了一种新的解决方案，其分层设计有效地解决了LLMs推理延迟高的问题，使其能够在需要实时响应和高频交互的场景中发挥作用。HLA在人机协作领域的应用前景广阔，有望推动人机协作的进一步发展。</td>
    </tr>
    <tr>
      <th>43</th>
      <td>Quantizing Constrained Systems: New Perspectives</td>
      <td>We consider quantum mechanics on constrained surfaces which have<br>non-Euclidean metrics and variable Gaussian curvature. The old controversy<br>about the ambiguities involving terms in the Hamiltonian of order hbar^2<br>multiplying the Gaussian curvature is addressed. We set out to clarify the<br>matter by considering constraints to be the limits of large restoring forces as<br>the constraint coordinates deviate from their constrained values. We find<br>additional ambiguous terms of order hbar^2 involving freedom in the<br>constraining potentials, demonstrating that the classical constrained<br>Hamiltonian or Lagrangian cannot uniquely specify the quantization: the<br>ambiguity of directly quantizing a constrained system is inherently<br>unresolvable. However, there is never any problem with a physical quantum<br>system, which cannot have infinite constraint forces and always fluctuates<br>around the mean constraint values. The issue is addressed from the perspectives<br>of adiabatic approximations in quantum mechanics, Feynman path integrals, and<br>semiclassically in terms of adiabatic actions.</td>
      <td>## 🌟 论文解读 | 量子力学中约束系统的量子化：新视角<br><br>## 📌 背景痛点/本文动机<br>量子力学中，对约束系统的量子化一直是一个充满争议的话题。传统的量子化方法在处理具有非欧几里得度量和可变高斯曲率的约束表面时，会遇到一些困难。特别是，涉及哈密顿量中与高斯曲率成正比的项的模糊性，一直是量子化过程中的一个难题。本文旨在通过考虑约束为约束坐标偏离其约束值时大恢复力的极限，来澄清这一问题。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了一种新的量子化方法，称为“极限量子化”。该方法将约束视为大恢复力的极限，随着约束坐标偏离其约束值，恢复力逐渐增大。这种方法能够有效地解决传统量子化方法中存在的模糊性问题。<br><br>💡 创新点2：本文从三个不同的角度分析了约束系统的量子化问题，包括量子力学中的绝热近似、费曼路径积分和半经典绝热作用。这些分析表明，量子化过程中的模糊性是固有的，并且与约束表面的内在几何形状和约束势的细节有关。<br><br>## 📈 实验结果<br>本文通过分析量子波动对约束表面的影响，发现了一些不可避免的量子动力学模糊性。然而，这些模糊性并不影响物理量子系统的行为，因为物理量子系统不能具有无限约束力，并且总是在约束值的平均值附近波动。<br><br>## 💬 可借鉴之处<br>本文提出的新量子化方法为处理具有非欧几里得度量和可变高斯曲率的约束系统提供了一种新的思路。此外，本文的分析方法也可以用于研究其他类型的量子化问题，例如量子场论中的规范固定问题。</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        