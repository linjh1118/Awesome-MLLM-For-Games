
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game</td>
      <td>The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred<br>interest in complex multimodal reasoning tasks in the real-world and virtual<br>environment, which require coordinating multiple abilities, including visual<br>perception, visual reasoning, spatial awareness, and target deduction. However,<br>existing evaluations primarily assess the final task completion, often<br>degrading assessments to isolated abilities such as visual grounding and visual<br>question answering. Less attention is given to comprehensively and<br>quantitatively analyzing reasoning process in multimodal environments, which is<br>crucial for understanding model behaviors and underlying reasoning mechanisms<br>beyond merely task success. To address this, we introduce MM-Escape, an<br>extensible benchmark for investigating multimodal reasoning, inspired by<br>real-world escape games. MM-Escape emphasizes intermediate model behaviors<br>alongside final task completion. To achieve this, we develop EscapeCraft, a<br>customizable and open environment that enables models to engage in free-form<br>exploration for assessing multimodal reasoning. Extensive experiments show that<br>MLLMs, regardless of scale, can successfully complete the simplest room escape<br>tasks, with some exhibiting human-like exploration strategies. Yet, performance<br>dramatically drops as task difficulty increases. Moreover, we observe that<br>performance bottlenecks vary across models, revealing distinct failure modes<br>and limitations in their multimodal reasoning abilities, such as repetitive<br>trajectories without adaptive exploration, getting stuck in corners due to poor<br>visual spatial awareness, and ineffective use of acquired props, such as the<br>key. We hope our work sheds light on new challenges in multimodal reasoning,<br>and uncovers potential improvements in MLLMs capabilities.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ï¼Ÿé€šè¿‡å¯æ‰©å±•çš„é€ƒè„±æ¸¸æˆè¿›è¡Œè¯„ä¼°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å¯¹ç°å®ä¸–ç•Œå’Œè™šæ‹Ÿç¯å¢ƒä¸­å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åè°ƒå¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€è§†è§‰æ¨ç†ã€ç©ºé—´æ„è¯†å’Œç›®æ ‡æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨æœ€ç»ˆçš„ä»»åŠ¡å®Œæˆæƒ…å†µï¼Œå¾€å¾€å°†è¯„ä¼°é™çº§ä¸ºå­¤ç«‹çš„èƒ½åŠ›ï¼Œå¦‚è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ã€‚è¾ƒå°‘å…³æ³¨åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­å…¨é¢å’Œå®šé‡åœ°åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œè¿™å¯¹äºç†è§£æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†æœºåˆ¶è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MM-Escapeï¼Œä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†ï¼Œç”¨äºç ”ç©¶å¤šæ¨¡æ€æ¨ç†ï¼Œçµæ„Ÿæ¥è‡ªç°å®ä¸–ç•Œçš„é€ƒè„±æ¸¸æˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMM-EscapeåŸºå‡†<br>MM-Escapeå¼ºè°ƒæœ€ç»ˆä»»åŠ¡å®Œæˆå’Œä¸­é—´æ¨¡å‹è¡Œä¸ºã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæœ¬æ–‡å¼€å‘äº†EscapeCraftï¼Œä¸€ä¸ªå¯å®šåˆ¶çš„å¼€æ”¾ç¯å¢ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè‡ªç”±æ¢ç´¢ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨ç†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šEscapeCraftç¯å¢ƒ<br>EscapeCraftæ˜¯ä¸€ä¸ªå¯å®šåˆ¶çš„å¼€æ”¾ç¯å¢ƒï¼Œå…è®¸æ¨¡å‹è¿›è¡Œè‡ªç”±æ¢ç´¢ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨ç†ã€‚å®ƒæ”¯æŒè‡ªå®šä¹‰å’Œå¯æ‰©å±•çš„åœºæ™¯ç”Ÿæˆï¼Œå¹¶å®šä¹‰äº†ä»»åŠ¡çš„æ¨ç†è·¯å¾„ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºè§„æ¨¡å¤§å°ï¼ŒMLLMséƒ½èƒ½æˆåŠŸå®Œæˆæœ€ç®€å•çš„æˆ¿é—´é€ƒè„±ä»»åŠ¡ï¼Œå…¶ä¸­ä¸€äº›æ¨¡å‹è¡¨ç°å‡ºç±»ä¼¼äººç±»çš„æ¢ç´¢ç­–ç•¥ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡éš¾åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿåˆ°æ€§èƒ½ç“¶é¢ˆå› æ¨¡å‹è€Œå¼‚ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸åŒå¤±è´¥æ¨¡å¼å’Œå±€é™æ€§ï¼Œä¾‹å¦‚æ²¡æœ‰è‡ªé€‚åº”æ¢ç´¢çš„é‡å¤è½¨è¿¹ã€ç”±äºè§†è§‰ç©ºé—´æ„è¯†å·®è€Œé™·å…¥è§’è½ï¼Œä»¥åŠæ— æ•ˆåœ°ä½¿ç”¨è·å¾—çš„é“å…·ï¼Œå¦‚é’¥åŒ™ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„MM-EscapeåŸºå‡†å’ŒEscapeCraftç¯å¢ƒä¸ºè¯„ä¼°å’Œæ”¹è¿›MLLMsçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚é€šè¿‡å…³æ³¨ä¸­é—´æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥æ›´å…¨é¢åœ°ç†è§£æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†æœºåˆ¶ï¼Œä»è€Œæ¨åŠ¨MLLMsåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>1</th>
      <td>From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons</td>
      <td>We examine the capability of Multimodal Large Language Models (MLLMs) to<br>tackle diverse domains that extend beyond the traditional language and vision<br>tasks these models are typically trained on. Specifically, our focus lies in<br>areas such as Embodied AI, Games, UI Control, and Planning. To this end, we<br>introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).<br>GEA is a single unified model capable of grounding itself across these varied<br>domains through a multi-embodiment action tokenizer. GEA is trained with<br>supervised learning on a large dataset of embodied experiences and with online<br>RL in interactive simulators. We explore the data and algorithmic choices<br>necessary to develop such a model. Our findings reveal the importance of<br>training with cross-domain data and online RL for building generalist agents.<br>The final GEA model achieves strong generalization performance to unseen tasks<br>across diverse benchmarks compared to other generalist models and<br>benchmark-specific approaches.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åˆ°é€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼šæ–¹æ³•ä¸ç»éªŒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­è¨€å’Œè§†è§‰ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åªåœ¨ç‰¹å®šçš„è®­ç»ƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œéš¾ä»¥åº”å¯¹å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œåœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢MLLMsåœ¨æ›´å¹¿æ³›é¢†åŸŸä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å…·èº«AIã€æ¸¸æˆã€UIæ§åˆ¶å’Œè§„åˆ’ç­‰é¢†åŸŸã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼ˆGEAï¼‰<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†MLLMsè½¬åŒ–ä¸ºé€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼ˆGEAï¼‰çš„æ–¹æ³•ã€‚GEAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å¤šå…·èº«åŠ¨ä½œåˆ†è¯å™¨åœ¨å„ä¸ªé¢†åŸŸä¸­è¿›è¡Œè‡ªæˆ‘å®šä½ã€‚GEAé€šè¿‡åœ¨å¤§è§„æ¨¡å…·èº«ç»éªŒæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šå…·èº«åŠ¨ä½œåˆ†è¯å™¨<br>ä¸ºäº†ä½¿GEAèƒ½å¤Ÿæ§åˆ¶å¤šæ ·åŒ–çš„å…·èº«å½¢æ€ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„å­¦ä¹ åˆ†è¯æœºåˆ¶ï¼Œç”¨äºå¤„ç†æ‰€æœ‰è¿ç»­å’Œç¦»æ•£çš„åŠ¨ä½œç©ºé—´ã€‚è¿™ç§æœºåˆ¶å°†è¿ç»­åŠ¨ä½œæ˜ å°„åˆ°ä¸€ç³»åˆ—æ–°çš„åˆ†è¯ï¼Œä»è€Œä½¿å¾—MLLMsèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œè¿™äº›åŠ¨ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>GEAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ç°æœ‰çš„é€šç”¨æ™ºèƒ½ä½“æ¨¡å‹ç›¸æ¯”ï¼Œç”šè‡³åœ¨æŸäº›ç‰¹å®šé¢†åŸŸä¸­çš„è¡¨ç°ä¹Ÿä¼˜äºæˆ–æ¥è¿‘äºä¸“ä¸šç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨CALVINæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGEAåœ¨å¤„ç†æœªè§è¿‡çš„æŒ‡ä»¤å’ŒèƒŒæ™¯æ—¶è¾¾åˆ°äº†90%çš„æˆåŠŸç‡ï¼Œæ¯”ç±»ä¼¼æ–¹æ³•é«˜å‡ºè¿‘10%ï¼Œå¹¶ä¸”ä¸ä¸“ä¸šç³»ç»Ÿçš„æ€§èƒ½ç›¸å½“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è·¨é¢†åŸŸæ•°æ®è¿›è¡Œè®­ç»ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ å¯¹äºæ„å»ºé€šç”¨æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†é¢„è®­ç»ƒMLLMsåœ¨GEAæ¶æ„ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡å¯¹æ€§èƒ½çš„å½±å“ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥å¼€å‘æ›´å¼ºå¤§çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºå’ŒæŒ‡å¯¼ã€‚</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis</td>
      <td>Recent advances in diffusion models have demonstrated exceptional<br>capabilities in image and video generation, further improving the effectiveness<br>of 4D synthesis. Existing 4D generation methods can generate high-quality 4D<br>objects or scenes based on user-friendly conditions, benefiting the gaming and<br>video industries. However, these methods struggle to synthesize significant<br>object deformation of complex 4D transitions and interactions within scenes. To<br>address this challenge, we propose Trans4D, a novel text-to-4D synthesis<br>framework that enables realistic complex scene transitions. Specifically, we<br>first use multi-modal large language models (MLLMs) to produce a physic-aware<br>scene description for 4D scene initialization and effective transition timing<br>planning. Then we propose a geometry-aware 4D transition network to realize a<br>complex scene-level 4D transition based on the plan, which involves expressive<br>geometrical object deformation. Extensive experiments demonstrate that Trans4D<br>consistently outperforms existing state-of-the-art methods in generating 4D<br>scenes with accurate and high-quality transitions, validating its<br>effectiveness. Code: https://github.com/YangLing0818/Trans4D</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Trans4Dï¼šåŸºäºæ–‡æœ¬çš„4Dåœºæ™¯åˆæˆæ–°æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢çš„çªç ´ï¼Œ4DåˆæˆæŠ€æœ¯ä¹Ÿå¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚ç°æœ‰çš„4Dç”Ÿæˆæ–¹æ³•å¯ä»¥åŸºäºç”¨æˆ·å‹å¥½çš„æ¡ä»¶ç”Ÿæˆé«˜è´¨é‡çš„4Då¯¹è±¡æˆ–åœºæ™¯ï¼Œä¸ºæ¸¸æˆå’Œè§†é¢‘è¡Œä¸šå¸¦æ¥äº†å·¨å¤§ä»·å€¼ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åˆæˆå¤æ‚4Dè¿‡æ¸¡å’Œåœºæ™¯å†…äº¤äº’ä¸­çš„æ˜¾è‘—å¯¹è±¡å˜å½¢æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç‰©ç†æ„ŸçŸ¥çš„4Dè¿‡æ¸¡è§„åˆ’<br>Trans4Dåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆç‰©ç†æ„ŸçŸ¥çš„åœºæ™¯æè¿°ï¼Œç”¨äº4Dåœºæ™¯åˆå§‹åŒ–å’Œæœ‰æ•ˆçš„è¿‡æ¸¡æ—¶é—´è§„åˆ’ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç¡®ä¿4Dåœºæ™¯çš„ç‰©ç†åˆç†æ€§å’Œç©ºé—´å…³ç³»çš„æ­£ç¡®æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå‡ ä½•æ„ŸçŸ¥çš„4Dè¿‡æ¸¡ç½‘ç»œ<br>Trans4Dæå‡ºäº†ä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥çš„è¿‡æ¸¡ç½‘ç»œï¼ˆTransNetï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è§„åˆ’åŠ¨æ€åœ°ç¡®å®š4Dåœºæ™¯ä¸­æ¯ä¸ªç‚¹äº‘æ˜¯å¦åº”è¯¥å‡ºç°æˆ–æ¶ˆå¤±ã€‚è¿™ä½¿å¾—Trans4Dèƒ½å¤Ÿè‡ªç„¶åœ°å¤„ç†å¤§è§„æ¨¡çš„å¯¹è±¡å˜å½¢ï¼Œä¾‹å¦‚å¯¼å¼¹å˜æˆçˆ†ç‚¸äº‘ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Trans4Dåœ¨ç”Ÿæˆå…·æœ‰å‡†ç¡®å’Œé«˜è´¨é‡è¿‡æ¸¡çš„4Dåœºæ™¯æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Trans4Dä¸º4Dåœºæ™¯åˆæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶ç‰©ç†æ„ŸçŸ¥çš„è¿‡æ¸¡è§„åˆ’å’Œå‡ ä½•æ„ŸçŸ¥çš„è¿‡æ¸¡ç½‘ç»œä¸ºç”Ÿæˆå…·æœ‰å¤æ‚äº¤äº’å’Œæ˜¾è‘—å˜å½¢çš„4Dåœºæ™¯æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒTrans4Dçš„è®­ç»ƒç­–ç•¥å¹³è¡¡äº†æ•ˆç‡å’Œè´¨é‡ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ç”Ÿæˆé«˜è´¨é‡çš„4Dåœºæ™¯ã€‚</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ING-VP: MLLMs cannot Play Easy Vision-based Games Yet</td>
      <td>As multimodal large language models (MLLMs) continue to demonstrate<br>increasingly competitive performance across a broad spectrum of tasks, more<br>intricate and comprehensive benchmarks have been developed to assess these<br>cutting-edge models. These benchmarks introduce new challenges to core<br>capabilities such as perception, reasoning, and planning. However, existing<br>multimodal benchmarks fall short in providing a focused evaluation of<br>multi-step planning based on spatial relationships in images. To bridge this<br>gap, we present ING-VP, the first INteractive Game-based Vision Planning<br>benchmark, specifically designed to evaluate the spatial imagination and<br>multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,<br>encompassing 300 levels, each with 6 unique configurations. A single model<br>engages in over 60,000 rounds of interaction. The benchmark framework allows<br>for multiple comparison settings, including image-text vs. text-only inputs,<br>single-step vs. multi-step reasoning, and with-history vs. without-history<br>conditions, offering valuable insights into the model's capabilities. We<br>evaluated numerous state-of-the-art MLLMs, with the highest-performing model,<br>Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the<br>anticipated standard. This work aims to provide a specialized evaluation<br>framework to drive advancements in MLLMs' capacity for complex spatial<br>reasoning and planning. The code is publicly available at<br>https://github.com/Thisisus7/ING-VP.git.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ING-VPï¼šå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è§„åˆ’ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºè¶Šæ¥è¶Šå¼ºçš„ç«äº‰åŠ›ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„å¤æ‚æ€§å’Œå…¨é¢æ€§çš„åŸºå‡†æµ‹è¯•ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°åŸºäºå›¾åƒç©ºé—´å…³ç³»çš„å¤šæ­¥è§„åˆ’èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†ING-VPï¼Œä¸€ä¸ªåŸºäºäº¤äº’å¼æ¸¸æˆçš„è§†è§‰è§„åˆ’åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šING-VPåŸºå‡†æµ‹è¯•<br>ING-VPåŸºå‡†æµ‹è¯•åŒ…å«6ä¸ªä¸åŒçš„æ¸¸æˆï¼Œæ¶µç›–300ä¸ªå…³å¡ï¼Œæ¯ä¸ªå…³å¡æœ‰6ç§ç‹¬ç‰¹çš„é…ç½®ã€‚å•ä¸ªæ¨¡å‹å‚ä¸è¶…è¿‡60,000è½®äº¤äº’ã€‚åŸºå‡†æµ‹è¯•æ¡†æ¶å…è®¸å¤šç§æ¯”è¾ƒè®¾ç½®ï¼ŒåŒ…æ‹¬å›¾åƒ-æ–‡æœ¬ä¸ä»…æ–‡æœ¬è¾“å…¥ã€å•æ­¥ä¸å¤šæ­¥æ¨ç†ä»¥åŠå¸¦å†å²ä¸ä¸å¸¦å†å²æ¡ä»¶ï¼Œä¸ºæ¨¡å‹çš„èƒ½åŠ›æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°MLLMsçš„èƒ½åŠ›<br>æœ¬æ–‡è¯„ä¼°äº†ä¼—å¤šæœ€å…ˆè¿›çš„MLLMsï¼Œå‘ç°å³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹Claude-3.5 Sonnetï¼Œå…¶å¹³å‡å‡†ç¡®ç‡ä¹Ÿåªæœ‰3.37%ï¼Œè¿œä½äºé¢„æœŸæ ‡å‡†ã€‚è¿™è¡¨æ˜å½“å‰çš„MLLMsåœ¨ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥è§„åˆ’èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹Claude-3.5 Sonnetï¼Œåœ¨ING-VPåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¹Ÿåªæœ‰3.37%ï¼Œè¿œä½äºäººç±»åœ¨è¿™äº›ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™è¡¨æ˜å½“å‰çš„MLLMsåœ¨ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥è§„åˆ’èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>ING-VPåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°MLLMsçš„ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªä¸“é—¨çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨MLLMsåœ¨å¤æ‚ç©ºé—´æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æ–¹é¢çš„è¿›æ­¥ã€‚æ­¤å¤–ï¼ŒING-VPåŸºå‡†æµ‹è¯•çš„ç»“æœä¹Ÿä¸ºMLLMsçš„æœªæ¥è®¾è®¡å’Œè®­ç»ƒç­–ç•¥æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs</td>
      <td>Humans spontaneously use increasingly efficient language as interactions<br>progress, by adapting and forming ad-hoc conventions. This phenomenon has been<br>studied extensively using reference games, showing properties of human language<br>that go beyond relaying intents. It remains unexplored whether multimodal large<br>language models (MLLMs) similarly increase communication efficiency during<br>interactions, and what mechanisms they may adopt for this purpose. We introduce<br>ICCA, an automated framework to evaluate such conversational adaptation as an<br>in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and<br>observe that while they may understand the increasingly efficient language of<br>their interlocutor, they do not spontaneously make their own language more<br>efficient over time. This latter ability can only be elicited in some models<br>(e.g., GPT-4) with heavy-handed prompting. This shows that this property of<br>linguistic interaction does not arise from current training regimes, even<br>though it is a common hallmark of human language. ICCA is available at<br>https://github.com/lil-lab/ICCA.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹è¯é€‚åº”æ€§è¯„ä¼°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>äººç±»åœ¨äº¤æµè¿‡ç¨‹ä¸­ä¼šè‡ªå‘åœ°ä½¿ç”¨è¶Šæ¥è¶Šé«˜æ•ˆçš„è¯­è¨€ï¼Œé€šè¿‡é€‚åº”å’Œå½¢æˆä¸´æ—¶çš„çº¦å®šæ¥æé«˜æ²Ÿé€šæ•ˆç‡ã€‚è¿™ç§ç°è±¡åœ¨å‚è€ƒæ¸¸æˆä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†äººç±»è¯­è¨€çš„ç‰¹æ€§ï¼Œè¶…è¶Šäº†ä»…ä»…ä¼ è¾¾æ„å›¾çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯å¦åœ¨äº¤äº’è¿‡ç¨‹ä¸­åŒæ ·èƒ½å¤Ÿæé«˜æ²Ÿé€šæ•ˆç‡ï¼Œä»¥åŠå®ƒä»¬å¯èƒ½é‡‡ç”¨å“ªäº›æœºåˆ¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ICCAï¼ˆIn-context Conversational Adaptationï¼‰ï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°MLLMsåœ¨ä¸Šä¸‹æ–‡ä¸­çš„å¯¹è¯é€‚åº”æ€§ã€‚ICCAä½¿ç”¨äººç±»-äººç±»å‚è€ƒæ¸¸æˆäº¤äº’çš„è¯­æ–™åº“ï¼Œå…è®¸å®Œå…¨è‡ªåŠ¨åŒ–çš„è¯„ä¼°ï¼Œæ— éœ€è¿›ä¸€æ­¥çš„äººç±»äº¤äº’ï¼Œä½¿å…¶æ˜“äºéƒ¨ç½²ä»¥åˆ†ææ–°æ¨¡å‹ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†æ¨¡å‹ä¸äººç±»äº¤äº’çš„åœºæ™¯ï¼Œé€šè¿‡æ¯”è¾ƒæ¨¡å‹è¡Œä¸ºçš„å˜åŒ–æ¥è¯„ä¼°å…¶å¯¹è¯é€‚åº”æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡è¯„ä¼°äº†äº”ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„MLLMsï¼šIDEFICSã€LLaVa-1.5ã€GPT4-visionã€Gemini 1.0 Pro Visionå’ŒClaude 3 opusã€‚ç ”ç©¶å‘ç°ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½éš¾ä»¥è‡ªå‘åœ°å¼•å…¥çº¦å®šå¹¶ä½œä¸ºè¯´è¯è€…è¿›è¡Œé€‚åº”ã€‚é€šè¿‡ä¸ºæ¯ä¸ªæ¨¡å‹è®¾è®¡ç‰¹å®šçš„æŒ‡ä»¤ï¼Œå¯ä»¥éƒ¨åˆ†è§£å†³è¿™ä¸€é—®é¢˜ã€‚æœ€å¼ºçš„æ¨¡å‹ï¼ˆGPT4ã€Geminiå’ŒClaudeï¼‰å¯ä»¥é€æ¸ä½¿ç”¨æ›´çŸ­çš„æ¶ˆæ¯ï¼ˆè·å¾—è¯æ±‡æ•ˆç‡ï¼‰ï¼Œä½†ä»ç„¶éš¾ä»¥å®ç°æ”¶æ•›æˆ–ç¨³å®šæ€§ï¼Œè¿™é˜»ç¢äº†çœŸæ­£é«˜æ•ˆæ²Ÿé€šçš„å‡ºç°ã€‚å½“ä½œä¸ºå¬ä¼—æ—¶ï¼ŒGPT4æ˜¾ç¤ºå‡ºæ¥è¿‘äººç±»çš„é€‚åº”è¶‹åŠ¿ï¼Œéšç€äº¤äº’çš„è¿›è¡Œï¼Œå…¶å‡†ç¡®æ€§ä¸æ–­æé«˜ï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™è¡¨ç°å‡ºè¾ƒå°‘çš„è¿™ç§è¡Œä¸ºæˆ–ä»…åœ¨ç®€åŒ–çš„è®¾ç½®ä¸‹è¡¨ç°å‡ºè¿™ç§è¡Œä¸ºã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰çš„MLLMså¯èƒ½è¢«åŠ¨åœ°ç†è§£å…¶å¯¹è¯è€…çš„ä¸æ–­å‘å±•å˜åŒ–çš„è¯­è¨€ï¼Œä½†å®ƒä»¬é€‚åº”è‡ªèº«è¯­è¨€ä»¥æé«˜æ²Ÿé€šæ•ˆç‡çš„èƒ½åŠ›å¹¶æ²¡æœ‰è‡ªç„¶åœ°ä»å®ƒä»¬çš„è®­ç»ƒæˆ–æŒ‡ä»¤è°ƒæ•´ä¸­äº§ç”Ÿã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†é‡è¦çš„æ–¹å‘ï¼ŒåŒ…æ‹¬æé«˜æ¨¡å‹çš„è‡ªå‘è¯­è¨€æ•ˆç‡ã€ä¿æŒè¯­è¨€ä¸€è‡´æ€§ã€é¿å…è¿‡åº¦é‡å¤å€¾å‘ä»¥åŠå¤„ç†å•ä¸ªæŸ¥è¯¢ä¸­çš„æ›´å¤šå›¾åƒã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>MLVU: Benchmarking Multi-task Long Video Understanding</td>
      <td>The evaluation of Long Video Understanding (LVU) performance poses an<br>important but challenging research problem. Despite previous efforts, the<br>existing video understanding benchmarks are severely constrained by several<br>issues, especially the insufficient lengths of videos, a lack of diversity in<br>video types and evaluation tasks, and the inappropriateness for evaluating LVU<br>performances. To address the above problems, we propose a new benchmark called<br>MLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and<br>in-depth evaluation of LVU. MLVU presents the following critical values:<br>\textit{1)} The substantial and flexible extension of video lengths, which<br>enables the benchmark to evaluate LVU performance across a wide range of<br>durations. \textit{2)} The inclusion of various video genres, e.g., movies,<br>surveillance footage, egocentric videos, cartoons, game videos, etc., which<br>reflects the models' LVU performances in different scenarios. \textit{3)} The<br>development of diversified evaluation tasks, which enables a comprehensive<br>examination of MLLMs' key abilities in long-video understanding. The empirical<br>study with 23 latest MLLMs reveals significant room for improvement in today's<br>technique, as all existing methods struggle with most of the evaluation tasks<br>and exhibit severe performance degradation when handling longer videos.<br>Additionally, it suggests that factors such as context length,<br>image-understanding ability, and the choice of LLM backbone can play critical<br>roles in future advancements. We anticipate that MLVU will advance the research<br>of long video understanding by providing a comprehensive and in-depth analysis<br>of MLLMs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MLVUï¼šå¤šä»»åŠ¡é•¿è§†é¢‘ç†è§£åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€èƒ½åŠ›ä¸Šçš„æ‰©å±•ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†ä¸»è¦é’ˆå¯¹çŸ­è§†é¢‘ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°MLLMsåœ¨å¤„ç†é•¿è§†é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰åŸºå‡†åœ¨è§†é¢‘ç±»å‹å’Œè¯„ä¼°ä»»åŠ¡çš„å¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”è¯„ä¼°ä»»åŠ¡çš„è®¾è®¡å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨é•¿è§†é¢‘ä¸­çš„å¤æ‚ä¿¡æ¯ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†é¢‘é•¿åº¦çš„å¤§å¹…æ‰©å±•<br>MLVUåŸºå‡†åŒ…å«æ—¶é•¿ä»3åˆ†é’Ÿåˆ°2å°æ—¶ä¸ç­‰çš„å¤šæ ·åŒ–é•¿è§†é¢‘ï¼Œå¹³å‡è§†é¢‘é•¿åº¦çº¦ä¸º15åˆ†é’Ÿï¼Œè¿œè¶…ç°æœ‰åŸºå‡†ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªè§†é¢‘è¢«è¿›ä¸€æ­¥åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œä»¥ä¾¿é’ˆå¯¹ä¸åŒç‰‡æ®µåˆ›å»ºè¯„ä¼°ä»»åŠ¡ï¼Œä»è€Œçµæ´»è¯„ä¼°MLLMsåœ¨ä¸åŒè§†é¢‘é•¿åº¦ä¸‹çš„è¡¨ç°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–çš„è§†é¢‘ç±»å‹<br>MLVUåŸºå‡†æ¶µç›–äº†å¤šç§è§†é¢‘ç±»å‹ï¼ŒåŒ…æ‹¬ç”µå½±ã€ç”Ÿæ´»è®°å½•ã€ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ã€ä½“è‚²ã€æ•™ç¨‹ã€ç›‘æ§å½•åƒç­‰çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œä»¥åŠåŠ¨ç”»ç³»åˆ—å’Œæ¸¸æˆè§†é¢‘ç­‰æ¨¡æ‹Ÿè§†é¢‘ã€‚è¿™ç§å¤šæ ·æ€§ä½¿å¾—MLLMsåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„è¡¨ç°å¯ä»¥å¾—åˆ°å…¨é¢è¯„ä¼°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ ·åŒ–çš„è¯„ä¼°ä»»åŠ¡<br>MLVUåŸºå‡†åŒ…å«9ä¸ªä¸åŒçš„è¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsåœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„å…³é”®èƒ½åŠ›ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ï¼Œä»¥åŠéœ€è¦åˆ©ç”¨æ•´ä¸ªè§†é¢‘çš„å…¨å±€ä¿¡æ¯æˆ–ç‰¹å®šç‰‡æ®µçš„å±€éƒ¨ä¿¡æ¯çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¶‰åŠå±€éƒ¨ä¿¡æ¯çš„ä»»åŠ¡éƒ½å¸¦æœ‰æ˜ç¡®çš„ä¸Šä¸‹æ–‡æ ‡æ³¨ï¼Œè¦æ±‚MLLMsèƒ½å¤Ÿå‡†ç¡®å®šä½æˆ–æ¨æ–­é•¿è§†é¢‘ä¸­çš„ç›¸å…³ç‰‡æ®µã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>é€šè¿‡å¯¹23ä¸ªæœ€æ–°çš„MLLMsè¿›è¡Œå®è¯ç ”ç©¶ï¼ŒMLVUåŸºå‡†æ­ç¤ºäº†ç°æœ‰æŠ€æœ¯åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ‰€æœ‰ç°æœ‰æ–¹æ³•åœ¨å¤§å¤šæ•°è¯„ä¼°ä»»åŠ¡ä¸Šéƒ½è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ›´é•¿çš„è§†é¢‘æ—¶æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ã€å›¾åƒç†è§£èƒ½åŠ›å’ŒLLMéª¨å¹²çš„é€‰æ‹©ç­‰å› ç´ å¯¹æœªæ¥é•¿è§†é¢‘ç†è§£æŠ€æœ¯çš„è¿›æ­¥èµ·ç€å…³é”®ä½œç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MLVUåŸºå‡†ä¸ºé•¿è§†é¢‘ç†è§£ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢è€Œæ·±å…¥çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜äº†è§£MLLMsåœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œå¹¶ä¸ºæœªæ¥çš„æŠ€æœ¯æ”¹è¿›æä¾›æ–¹å‘ã€‚æ­¤å¤–ï¼ŒMLVUåŸºå‡†çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡è¯„ä¼°åŸºå‡†çš„å¼€å‘æä¾›å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>6</th>
      <td>A Survey on Large Language Model-Based Game Agents</td>
      <td>The development of game agents holds a critical role in advancing towards<br>Artificial General Intelligence (AGI). The progress of LLMs and their<br>multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve<br>and empower game agents with human-like decision-making capabilities in complex<br>computer game environments. This paper provides a comprehensive overview of<br>LLM-based game agents from a holistic viewpoint. First, we introduce the<br>conceptual architecture of LLM-based game agents, centered around six essential<br>functional components: perception, memory, thinking, role-playing, action, and<br>learning. Second, we survey existing representative LLM-based game agents<br>documented in the literature with respect to methodologies and adaptation<br>agility across six genres of games, including adventure, communication,<br>competition, cooperation, simulation, and crafting & exploration games.<br>Finally, we present an outlook of future research and development directions in<br>this burgeoning field. A curated list of relevant papers is maintained and made<br>accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ¸¸æˆä»£ç†ï¼šè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„æ¢ç´¢<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…¶å¤šæ¨¡æ€ç‰ˆæœ¬ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆä»£ç†å¤§å¤šåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œåœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­ç¼ºä¹ç±»ä¼¼äººç±»çš„å†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨å¦‚ä½•åˆ©ç”¨LLMsæ„å»ºå…·æœ‰äººç±»å†³ç­–èƒ½åŠ›çš„æ¸¸æˆä»£ç†ï¼Œä»¥æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºåŸºäºLLMsçš„æ¸¸æˆä»£ç†ï¼ˆLLMGAsï¼‰ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å…¶å…­ä¸ªæ ¸å¿ƒåŠŸèƒ½ç»„ä»¶ï¼š<br><br>1. **æ„ŸçŸ¥**ï¼šLLMGAsé€šè¿‡å¤šç§æ–¹å¼æ„ŸçŸ¥æ¸¸æˆç¯å¢ƒï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰ã€å£°éŸ³ç­‰ï¼Œä»¥è·å–å¿…è¦çš„æ¸¸æˆçŠ¶æ€ä¿¡æ¯ã€‚<br>2. **è®°å¿†**ï¼šLLMGAsåˆ©ç”¨è®°å¿†ç³»ç»Ÿå­˜å‚¨è¿‡å»çš„è§‚å¯Ÿã€æ€è€ƒå’Œè¡ŒåŠ¨ï¼Œä»¥ä¾¿åœ¨æœªæ¥çš„å†³ç­–ä¸­æ£€ç´¢å’Œåˆ©ç”¨ã€‚<br>3. **æ€è€ƒ**ï¼šLLMGAsé€šè¿‡æ¨ç†å’Œè§„åˆ’è¿›è¡Œå†³ç­–ï¼Œæ¨ç†åŒ…æ‹¬æ¼”ç»ã€å½’çº³å’Œå‡è®¾æ¨ç†ï¼Œè§„åˆ’åˆ™å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡ã€‚<br>4. **è§’è‰²æ‰®æ¼”**ï¼šLLMGAsèƒ½å¤Ÿæ¨¡æ‹Ÿæ¸¸æˆä¸­çš„ä¸åŒè§’è‰²ï¼Œç”Ÿæˆç¬¦åˆè§’è‰²ç‰¹å¾å’Œç›®æ ‡çš„å¯¹è¯å’Œè¡Œä¸ºã€‚<br>5. **è¡ŒåŠ¨**ï¼šLLMGAså°†ç”Ÿæˆçš„æ–‡æœ¬å†³ç­–è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨ï¼Œä»¥ä¸æ¸¸æˆç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚<br>6. **å­¦ä¹ **ï¼šLLMGAsé€šè¿‡ç§¯ç´¯æ¸¸æˆç»éªŒå’Œç¯å¢ƒåé¦ˆï¼Œä¸æ–­æ”¹è¿›å…¶è®¤çŸ¥å’Œæ¸¸æˆèƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹ç°æœ‰æ–‡çŒ®è¿›è¡Œäº†ç»¼è¿°ï¼Œæ¶µç›–äº†å…­ç§ç±»å‹çš„æ¸¸æˆï¼ŒåŒ…æ‹¬å†’é™©ã€æ²Ÿé€šã€ç«äº‰ã€åˆä½œã€æ¨¡æ‹Ÿå’Œåˆ¶ä½œä¸æ¢ç´¢æ¸¸æˆã€‚å¯¹äºæ¯ç§ç±»å‹çš„æ¸¸æˆï¼Œæœ¬æ–‡åˆ†æäº†æŠ€æœ¯æŒ‘æˆ˜ã€æ”¯æŒçš„æ¸¸æˆç¯å¢ƒå’Œå¸¸ç”¨çš„ä¼˜åŒ–ç­–ç•¥ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„LLMGAsæ¡†æ¶ä¸ºæ„å»ºå…·æœ‰äººç±»å†³ç­–èƒ½åŠ›çš„æ¸¸æˆä»£ç†æä¾›äº†é‡è¦çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„ä¸‰ä¸ªæ–¹å‘ï¼š<br><br>1. **ç¯å¢ƒä¸­çš„LLMsæ¥åœ°**ï¼šé€šè¿‡å¤šæ¨¡æ€æ„ŸçŸ¥ã€å¤–éƒ¨å¯ç”¨æ€§å‡½æ•°ã€ç¯å¢ƒåé¦ˆå’Œç»éªŒå­¦ä¹ ï¼Œä½¿LLMsæ›´å¥½åœ°é€‚åº”ç°å®ç¯å¢ƒã€‚<br>2. **é€šè¿‡æ¸¸æˆç©çŸ¥è¯†å‘ç°**ï¼šåˆ©ç”¨æ¸¸æˆæœºåˆ¶ä¸­çš„å¤æ‚çŸ¥è¯†ï¼Œè®¾è®¡èƒ½å¤Ÿä»è§‚å¯Ÿå’Œç»éªŒä¸­æå–åº•å±‚çŸ¥è¯†çš„LLMä»£ç†ã€‚<br>3. **ä»£ç†ç¤¾ä¼šæ¨¡æ‹Ÿ**ï¼šåœ¨ç°å®æ¸¸æˆç¯å¢ƒä¸­å¼€å‘æ›´å¥½çš„è®¤çŸ¥æ¶æ„ï¼Œæ¨¡æ‹Ÿæ›´å¤æ‚çš„ç¤¾äº¤äº’åŠ¨å’Œåˆä½œï¼Œä»¥æ›´å¥½åœ°ç†è§£å’Œè¡¨ç¤ºå¤æ‚çš„äººç±»äº¤äº’ã€‚<br><br>## ğŸ“š å‚è€ƒèµ„æ–™<br>[1] The Embodied Cognition Hypothesis<br>[2] ChatGPT<br>[3] GPT-4V<br>[4] Gemini<br>[5] LLM-based agent (LLMA)<br>[6] Pre-existing knowledge<br>[7] Real-world learning<br>[8] Real-world experience<br>[9] Chess<br>[10] Poker<br>[11] Atari games<br>[12] StarCraft II<br>[13] Minecraft<br>[14] DOTA II<br>[15] Reinforcement Learning (RL)<br>[16] Behavior-level policy learning<br>[17] Cognitive abilities<br>[18] Reasoning<br>[19] LLMs survey papers<br>[20] LLMAs survey papers<br>[21] LLMGAs survey papers<br>[22] Game development survey papers<br>[23] Game agents survey papers<br>[24] LLMGAs taxonomy<br>[25] Game categories<br>[26] Technical challenges<br>[27] Game environments<br>[28] Optimization strategies<br>[29] Future research directions<br>[30] LLMGAs in games<br>[31] Perception module<br>[32] Memory module<br>[33] Role-playing module<br>[34] Thinking module<br>[35] Action module<br>[36] Learning module<br>[37] Adventure games<br>[38] Communication games<br>[39] Competition games<br>[40] Cooperation games<br>[41] Simulation games<br>[42] Crafting & exploration games<br>[43] Evaluation metrics<br>[44] Grounding LLMs in environments<br>[45] Knowledge discovery through game-playing<br>[46] Agent society simulation</td>
    </tr>
    <tr>
      <th>7</th>
      <td>GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data</td>
      <td>Multimodal Large Language Models (MLLMs) are typically assessed using<br>expensive annotated multimodal benchmarks, which often lag behind the rapidly<br>evolving demands of MLLM evaluation. This paper outlines and validates<br>GenCeption, a novel, annotation-free evaluation method that requires only<br>unimodal data to measure inter-modality semantic coherence and inversely<br>assesses MLLMs' tendency to hallucinate. This approach eliminates the need for<br>costly data annotation, minimizes the risk of training data contamination, is<br>expected to result in slower benchmark saturation, and avoids the illusion of<br>emerging abilities. Inspired by the DrawCeption game, GenCeption begins with a<br>non-textual sample and proceeds through iterative description and generation<br>steps. The semantic drift across iterations is quantified using the GC@T<br>metric. While GenCeption is principally applicable to MLLMs across various<br>modalities, this paper focuses on its implementation and validation for Vision<br>LLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption<br>benchmark for evaluating VLLMs, and compare the performance of several popular<br>VLLMs and human annotators. Our empirical results validate GenCeption's<br>effectiveness, demonstrating strong correlations with established VLLM<br>benchmarks. VLLMs still significantly lag behind human performance and struggle<br>especially with text-intensive tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GenCeptionï¼šä¸€ç§æ— éœ€æ ‡æ³¨çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºæ˜‚è´µçš„å¤šæ¨¡æ€æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€æ»åäºMLLMsçš„å¿«é€Ÿæ¼”è¿›éœ€æ±‚ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•è¿˜å­˜åœ¨ä»¥ä¸‹æŒ‘æˆ˜ï¼š<br>1. ä¾èµ–é«˜è´¨é‡æ ‡æ³¨çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ•æ‰MLLMsçš„åŠ¨æ€å‘å±•ã€‚<br>2. ä½¿ç”¨ç¦»æ•£æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰è¯„ä¼°ï¼Œå¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹èƒ½åŠ›çš„è¯¯åˆ¤ã€‚<br>3. è¯„ä¼°åˆ†æ•°å¯èƒ½æ— æ³•åæ˜ çœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®å¯èƒ½å—åˆ°åŸºå‡†æ•°æ®é›†çš„æ±¡æŸ“ã€‚<br>4. è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªæ¨¡æ€çš„å†…å®¹å¯èƒ½å¹¶ä¸éœ€è¦å›ç­”åŸºå‡†é—®é¢˜ï¼Œå› ä¸ºç­”æ¡ˆå¯ä»¥ä»é—®é¢˜æˆ–MLLMsçš„é¢„è®­ç»ƒçŸ¥è¯†ä¸­æ¨æ–­å‡ºæ¥ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†GenCeptionï¼Œä¸€ç§æ–°é¢–çš„æ— æ ‡æ³¨è¯„ä¼°æ–¹æ³•ï¼Œå®ƒåªéœ€è¦å•æ¨¡æ€æ•°æ®æ¥è¡¡é‡è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œè¯„ä¼°MLLMsçš„å¹»è§‰å€¾å‘ã€‚GenCeptionçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š<br>1. **æ— æ ‡æ³¨è¯„ä¼°**ï¼šæ— éœ€æ˜‚è´µçš„æ•°æ®æ ‡æ³¨ï¼Œåªéœ€ä½¿ç”¨å•æ¨¡æ€æ•°æ®é›†å³å¯è¿›è¡Œè¯„ä¼°ã€‚<br>2. **è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§**ï¼šé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œæè¿°éæ–‡æœ¬æ ·æœ¬ï¼Œè¯„ä¼°MLLMsåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚<br>3. **å¹»è§‰å€¾å‘è¯„ä¼°**ï¼šé€šè¿‡è¡¡é‡è¯­ä¹‰æ¼‚ç§»ï¼Œé—´æ¥è¯„ä¼°MLLMsçš„å¹»è§‰å€¾å‘ã€‚<br>4. **è¿ç»­æŒ‡æ ‡**ï¼šä½¿ç”¨GC@TæŒ‡æ ‡ï¼Œæä¾›æ›´ç»†ç²’åº¦çš„è¯„ä¼°ï¼Œé¿å…å¯¹æ¨¡å‹èƒ½åŠ›çš„è¯¯åˆ¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨MMECeptionåŸºå‡†ä¸Šè¯„ä¼°äº†ä¸ƒç§æµè¡Œçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰ï¼Œå¹¶ä¸äººç±»æ ‡æ³¨è€…çš„è¡¨ç°è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒGenCeptionä¸ç°æœ‰VLLMåŸºå‡†å…·æœ‰å¼ºç›¸å…³æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°VLLMsçš„è¯­ä¹‰ä¸€è‡´æ€§å’Œå¹»è§‰å€¾å‘ã€‚æ­¤å¤–ï¼ŒVLLMsåœ¨æ–‡æœ¬å¯†é›†å‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ç„¶æ˜¾è‘—è½åäºäººç±»ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GenCeptionæä¾›äº†ä¸€ç§æ–°é¢–çš„æ— æ ‡æ³¨è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°MLLMsçš„è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œå¹»è§‰å€¾å‘ã€‚è¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š<br>1. **æˆæœ¬æ•ˆç›Š**ï¼šæ— éœ€æ˜‚è´µçš„æ•°æ®æ ‡æ³¨ï¼Œé™ä½äº†è¯„ä¼°æˆæœ¬ã€‚<br>2. **åŠ¨æ€é€‚åº”**ï¼šèƒ½å¤Ÿé€‚åº”MLLMsçš„å¿«é€Ÿæ¼”è¿›éœ€æ±‚ã€‚<br>3. **å…¨é¢è¯„ä¼°**ï¼šèƒ½å¤Ÿè¯„ä¼°MLLMsçš„å¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬æè¿°ã€ç”Ÿæˆå’Œæ¨ç†ç­‰ã€‚<br>4. **å¯æ‰©å±•æ€§**ï¼šå¯ä»¥æ‰©å±•åˆ°å…¶ä»–éæ–‡æœ¬æ¨¡æ€ï¼Œå¦‚éŸ³é¢‘ã€è§†é¢‘å’Œå›¾å½¢ç­‰ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>GenCeptionä¸ºMLLMsçš„è¯„ä¼°æä¾›äº†ä¸€ç§æ–°é¢–ä¸”æœ‰æ•ˆçš„æ— æ ‡æ³¨æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°MLLMsçš„è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œå¹»è§‰å€¾å‘ã€‚è¯¥æ–¹æ³•å…·æœ‰æˆæœ¬æ•ˆç›Šã€åŠ¨æ€é€‚åº”ã€å…¨é¢è¯„ä¼°å’Œå¯æ‰©å±•æ€§ç­‰ä¼˜åŠ¿ï¼Œä¸ºMLLMsçš„è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>8</th>
      <td>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</td>
      <td>We present PCA-Bench, a multimodal decision-making benchmark for evaluating<br>the integrated capabilities of Multimodal Large Language Models (MLLMs).<br>Departing from previous benchmarks focusing on simplistic tasks and individual<br>model capability, PCA-Bench introduces three complex scenarios: autonomous<br>driving, domestic robotics, and open-world games. Given task instructions and<br>diverse contexts, the model is required to seamlessly integrate multiple<br>capabilities of Perception, Cognition, and Action in a reasoning chain to make<br>accurate decisions. Moreover, PCA-Bench features error localization<br>capabilities, scrutinizing model inaccuracies in areas such as perception,<br>knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To<br>balance accuracy and efficiency in evaluation, we propose PCA-Eval, an<br>automatic evaluation protocol, and assess 10 prevalent MLLMs. The results<br>reveal significant performance disparities between open-source models and<br>powerful proprietary models like GPT-4 Vision. To address this, we introduce<br>Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing<br>instruction tuning examples in multimodal embodied environments. EIE generates<br>7,510 training examples in PCA-Bench and enhances the performance of<br>open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision<br>accuracy), thereby validating the effectiveness of EIE. Our findings suggest<br>that robust MLLMs like GPT4-Vision show promise for decision-making in embodied<br>agents, opening new avenues for MLLM research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | PCA-Benchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­çš„å†³ç­–èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¾€å¾€åªå…³æ³¨å•ä¸ªæ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹åœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢çš„ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ç¼ºä¹å¯¹æ¨¡å‹é”™è¯¯è¿›è¡Œå®šä½çš„èƒ½åŠ›ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šæ¨¡å‹åœ¨å“ªäº›æ–¹é¢éœ€è¦æ”¹è¿›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPCA-Bench<br>æœ¬æ–‡æå‡ºäº†PCA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­å†³ç­–èƒ½åŠ›çš„å¤šæ¨¡æ€å†³ç­–åŸºå‡†ã€‚PCA-Benchå¼•å…¥äº†ä¸‰ä¸ªå¤æ‚çš„åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®ä»»åŠ¡æŒ‡ä»¤å’Œä¸åŒçš„ä¸Šä¸‹æ–‡ï¼Œæ— ç¼åœ°æ•´åˆæ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œä»¥åšå‡ºå‡†ç¡®çš„å†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPCA-Eval<br>ä¸ºäº†å¹³è¡¡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†PCA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ã€‚PCA-Evalåˆ©ç”¨LLMså¼ºå¤§çš„è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œæ ¹æ®æ•°æ®æ³¨é‡Šä¸­çš„é”šç‚¹ä¿¡æ¯ï¼Œè‡ªåŠ¨è¿›è¡Œé”™è¯¯å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCA-Evalä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ï¼Œå¹³å‡Kappaç³»æ•°è¾¾åˆ°0.8+ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEmbodied-Instruction-Evolution (EIE)<br>ä¸ºäº†è§£å†³PCA-Benchæ•°æ®é›†æ ‡æ³¨å·¥ä½œé‡å¤§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Embodied-Instruction-Evolution (EIE)æ¡†æ¶ã€‚EIEåˆ©ç”¨LLMsè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹ï¼Œä»è€Œå‡å°‘äº†äººå·¥åŠ³åŠ¨ï¼Œå¹¶æé«˜äº†PCA-Benchçš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4 Visionåœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºMLLMsã€‚EIEæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜å¼€æºMLLMsçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†GPT-4 Visionã€‚PCA-Evalèƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½æ¨¡å‹é”™è¯¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„PCA-Benchå’ŒPCA-Evalä¸ºè¯„ä¼°MLLMsçš„å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚EIEæ¡†æ¶ä¸ºè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„MLLMsåœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„å†³ç­–èƒ½åŠ›å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä¸ºMLLMsçš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        