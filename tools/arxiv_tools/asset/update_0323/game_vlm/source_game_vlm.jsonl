{"title":"How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game","authors":"Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu","summary":"The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.","url":"http:\/\/arxiv.org\/abs\/2503.10042v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.10042v1","published":1741841323000,"comment":null,"pdf_text":"How Do Multimodal Large Language Models Handle Complex Multimodal\nReasoning? Placing Them in An Extensible Escape Game\nZiyue Wang1 ♠*, Yurui Dong3*, Fuwen Luo1, Minyuan Ruan1, Zhili Cheng1,\nChi Chen1, Peng Li2 B, Yang Liu1,2 B\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n3School of Management, Fudan University, Shanghai, China\nConfiguration \nFiles\nScene \nData\nAutomatic \nGeneration\nTarget 2: Post-Game Debriefing (Optional)\nQ 3: Piece Together the \nWhole Story\n- The room appeared to be a \ncross between a living area \nand a storage space, likely \nused for daily life … \nQ1: Describe the \nRoom\n- The first room where \nI started had a clean, \nminimalistic design, with \nmodern furniture …\nQ2: Recall the \nEscape Process\n- Some notable items \nI observed:  Small \nplants and a key to \nopen box for …\nDoor (Room 1 Exit & \nRoom 2 Entrance)\nRoom 1 \nRoom 2 \nGame Target:  \nExit of Room 2\nInitial \nPosition\nKey Prop\nEngine \nRendering\nAction:\ninteract\nread\njump\nrotate\ngrab\nforward\ntilt\nTarget 1: Room Escaping\nFigure 1. Illustration of our proposed room escape environment EscapeCraft, which allows us to generate customized room scenes (left)\nand define groundtruth reasoning path of tasks (right). Based on EscapeCraft, we create MM-Escape benchmark, targeting at evaluating\nboth the task completion performance and the entire multimodal reasoning process of MLLMs.\nAbstract\nThe rapid advancing of Multimodal Large Language Mod-\nels (MLLMs) has spurred interest in complex multimodal\nreasoning tasks in the real-world and virtual environment,\nwhich require coordinating multiple abilities, including vi-\nsual perception, visual reasoning, spatial awareness, and\ntarget deduction. However, existing evaluations primarily\nassess the final task completion, often degrading assess-\nments to isolated abilities such as visual grounding and vi-\nsual question answering. Less attention is given to compre-\nhensively and quantitatively analyzing reasoning process\nin multimodal environments, which is crucial for under-\nstanding model behaviors and underlying reasoning mech-\nanisms beyond merely task success. To address this, we\nintroduce MM-Escape, an extensible benchmark for inves-\ntigating multimodal reasoning, inspired by real-world es-\ncape games. MM-Escape emphasizes intermediate model\n*Equal contribution, ♠Project lead, B Corresponding author\nbehaviors alongside final task completion. To achieve this,\nwe develop EscapeCraft, a customizable and open environ-\nment that enables models to engage in free-form exploration\nfor assessing multimodal reasoning. Extensive experiments\nshow that MLLMs, regardless of scale, can successfully\ncomplete the simplest room escape tasks, with some exhibit-\ning human-like exploration strategies.\nYet, performance\ndramatically drops as task difficulty increases. Moreover,\nwe observe that performance bottlenecks vary across mod-\nels, revealing distinct failure modes and limitations in their\nmultimodal reasoning abilities, such as repetitive trajecto-\nries without adaptive exploration, getting stuck in corners\ndue to poor visual spatial awareness, and ineffective use of\nacquired props, such as the key. We hope our work sheds\nlight on new challenges in multimodal reasoning, and un-\ncovers potential improvements in MLLMs capabilities. 1 2\n1GitHub repo: https:\/\/github.com\/THUNLP-MT\/EscapeCraft.\n2Home page: https:\/\/thunlp-mt.github.io\/EscapeCraft.\narXiv:2503.10042v1  [cs.CV]  13 Mar 2025\n1. Introduction\nThe rapid development of Large Language Models (LLMs)\nand Multimodal Large Language Models (MLLMs) have\ndriven the advancement of diverse multimodal systems and\napplications for academic research [2, 42], industrial engi-\nneering [15], and everyday assistance [13, 34]. Multimodal\nreasoning is essential for these applications that require in-\ntegrating multiple abilities such as visual perception, spatial\nawareness, and visual grounding [35]. For example, it en-\nhances autonomous driving by improving the holistic under-\nstanding of multi-view information and localization, which\nare essential for vehicle actions and planning [10, 12], and\nalso advances the general-purpose assistants in better per-\nforming visual and multimodal tasks in the wild [17, 21].\nDespite significant attention and effort towards improv-\ning multimodal reasoning abilities of MLLMs [22, 35, 47],\ncomprehensive evaluation remains underexplored for two\nkey reasons. First, fundamental tasks such as visual ground-\ning [8, 46, 48] and image captioning [1, 25] are conducted\nin constraint environment and straightforward objectives,\nreducing the need for autonomous exploration. They pri-\nmarily focus on identifying correct answers, such as bound-\ning boxes and objects, without requiring coordinating mul-\ntiple multimodal abilities. Second, while multimodal tasks\nin open-world settings [20, 26, 29] involve complex envi-\nronments and objectives, they emphasize final task comple-\ntion, often measured by success rate [18]. This results in a\nlack of profound analysis over the reasoning process, lead-\ning to potentially inaccurate assessments of multimodal rea-\nsoning capabilities. Moreover, some open-world tasks pro-\nvide structured knowledge libraries [9, 36] that standard-\nize the reasoning mechanisms. These consequently limit\nthe autonomy of models to conduct multimodal reasoning,\nmaking the reasoning more reliant on predefined knowledge\nrather than exploration of multimodal surroundings.\nWe argue that in open multimodal environment, includ-\ning real-world settings and virtual simulators, complex mul-\ntimodal reasoning should not be solely assessed by task\ncompletion results or isolated tasks. Instead, it is more prac-\ntical and realistic to examine how models autonomously\ncoordinate across multiple multimodal reasoning abilities.\nRecently, open environments such as Habitat [30], AI2-\nTHOR[16], and OsWorld[40] are widely discussed, where\nMLLMs are required to exhibit complex reasoning skills in-\ncluding visual searching, spatial understanding, tool utiliza-\ntion, and long-term decision-making. However, as summa-\nrize in Table 1, there is a constraint on tasks or environments\nespecially designed for evaluating multimodal reasoning in\nopen-ended and interactive settings, leaving the evaluation\nof complex multimodal reasoning underestimated.\nIn this paper, we introduce MM-Escape, an extensible\nbenchmark for evaluating complex multimodal reasoning,\ninspired by real-world escape games. To achieve this, we\ndevelop EscapeCraft, a customizable open environment\nthat engages models in free-form exploration through the\nroom escape task, assisting in comprehensive assessments\nof their multimodal reasoning abilities. MM-Escape pro-\nvides solutions to the above issues of excessive concerns\non task completion and simplified objectives, by emphasiz-\ning intermediate behaviors alongside final task completion.\nIt measures the entire reasoning process consists of com-\npound abilities. While our designed games are intuitive for\nhuman players, they pose significant challenges for models\nregarding reasoning paths and interaction behaviors, expos-\ning key limitations in current multimodal reasoning capabil-\nities. As an additional bonus, we introduce post-game de-\nbriefing task, which is feasible only for the most challenging\nsettings. It requires models to recall the escape process and\nreconstruct the story via collected clues, which assist in the\nfuture assessment of more complicated reasoning tasks.\nWe conduct comprehensive experiments and derive two\nkey findings. First, although the overall performance re-\nmains far from human-level, recent MLLMs demonstrate\nstrong multimodal reasoning ability. Notably, models like\nGPT-4o and Gemini-1.5-Pro achieve high escape rates and\nexhibit human-like exploration and reasoning strategies.\nWhile their interaction success rates are halved with steps\nmore than doubled compared to human results. Second, as\ngame difficulty increases, the model performance sharply\ndeclines, revealing distinct failure modes across models.\nFor instance, GPT-4o tends to repeat actions and get trapped\nin historical trajectories when reasoning paths grow longer,\nGemini struggles with spatial awareness and often gets\nstuck in corners. We also thoroughly discuss post-game de-\nbriefing and other extensible settings in Section 5. These\nfindings highlight the need for greater attentions toward\ncomprehensive analysis and improvement of multimodal\nreasoning abilities. Our contributions are as follows:\n• We introduce a benchmark, MM-Escape, to advance\ncomprehensive evaluation of multimodal reasoning for\nMLLMs, by quantitatively evaluating intermediate rea-\nsoning process alongside task completion performance.\n• Our benchmark features free-form exploration, requiring\nmodels to autonomously coordinate multiple multimodal\nreasoning abilities in the multimodal room escape task.\n• We thoroughly investigate model behaviors using MM-\nEscape and identify distinct limitations across models.\nOur analysis provides detailed insights, highlighting fu-\nture optimization and potential real-world applications.\n2. Related Work\n2.1. Complex Reasoning Abilities of MLLMs\nRecent research on MLLMs has moved beyond addressing\nsimple tasks, such as image captioning and image retrieval,\nand instead focuses on enhancing model abilities towards\nBenchmark\nScenario\nTask\nMultimodal\nEnvironment Type\nURP\nPA\nTextWorld [5]\nText Game\nSimplified Text Games\n✗\nOpen Environment\n✓\n✗\nEscapeBench [28]\nText Game\nRoom Escape\n✗\nOpen Environment\n✓\n✓\nOpenEQA [23]\nVideo\nEgo-centric QA\n✓\nControlled Environment\n✗\n✗\nEgoSchema [24]\nVideo\nEgo-centric QA\n✓\nControlled Environment\n✗\n✗\nVSI-Bench [44]\nVideo\nVisual-Spatial QA\n✓\nControlled Environment\n✗\n✗\nMineRL [14]\nVideo Game\nGoals in Minecraft\n✓\nOpen Environment\n✗\n✗\nOSWorld [40]\nOS Environment\nComputer Use\n✓\nOpen Environment\n✗\n✗\nALFRED [31]\nEmbodied Environment\nHousehold Task\n✓\nOpen Environment\n✗\n✗\nEmbodiedBench [45]\nEmbodied Environment\nHousehold Task\n✓\nOpen Environment\n✗\n✗\nMM-Escape (Ours)\nEmbodied Environment\nRoom Escape\n✓\nOpen Environment\n✓\n✓\nTable 1. Benchmarks aiming at evaluating complex reasoning ability of models. URP means unpredictable reasoning path. PA means\nprocess assessment.\nmore complex tasks and more realistic scenarios. For ex-\nample, MLLMs can operate digital devices such as comput-\ners [27, 43] and mobile phones [33, 39], play video games\nrequiring long action sequences [3, 6], and perform robotic\nmanipulation in the real world [11, 49]. Unlike tasks such\nas visual question answering and visual grounding, which\nhave standardized answers, these complex tasks allow di-\nverse solutions and engage in open environments. As a re-\nsult, multimodal reasoning ability is becoming increasingly\nimportant for achieving more advanced targets.\n2.2. Complex Multimodal Reasoning Evaluation\nResearchers have made efforts to evaluate the visual reason-\ning ability of MLLMs in complex tasks which require mod-\nels to coordinate multiple multimodal abilities [23, 24, 44].\nHowever, many of them leverage videos with predefined\nperspectives and trajectories as input. Models cannot in-\nteract with environments by themselves.\nSome other works focus on complex open worlds, such\nas Minecraft [19, 20, 29, 32, 37, 38], to evaluate model abil-\nities of multimodal reasoning. However, two issues still ex-\nist. First, due to clearly defined game rules, standardized\nguides are available from websites like Minecraft Wiki 3,\nand training datasets of game demonstrations [14], making\nit feasible for models to reason based on their commonsense\nor relying on external knowledge rather than observations\nduring free-form exploration. Second, evaluations are often\noutcome-oriented, for example, focusing on specific goals\nlike obtaining a diamond in Minecraft, while how models\nexplore the open world is ignored. Similar issues exist in\nother complex open-ended tasks in open worlds like com-\nputer use [40] and robotic manipulation [31, 45].\nCompared to existing benchmarks, our work features in\nthree aspects: a) we support sufficient interactions between\nmodels and environments; b) environments can be flexibly\nconfigured, preventing models from solely relying external\nknowledge to achieve the goals; c) reasoning processes are\nalso evaluated alongside final results. These enable us to\nunderstand model abilities more comprehensively.\n3https:\/\/minecraft.wiki\/\n2.3. Multimodal v.s. Pure Text Environments\nPrevious works investigated reasoning ability of models in\npure text scenarios by allowing models to interact with en-\nvironments via text interfaces [5, 28, 41]. However, some\nmultimodal information cannot be easily and precisely ex-\npressed in verbal forms, hindering reasoning ability based\non multimodal information to be evaluated in pure text en-\nvironments. Additionally, models enjoy enhanced decision-\nmaking autonomy within multimodal environments com-\npared to text environments. For example, models may de-\ncide their rotation angles or travel distances based on mul-\ntimodal inputs when requiring to observe target objects or\nnearby surroundings, but it is hard to make such decisions\nbased on text descriptions. Therefore, we believe that mul-\ntimodal environments are essential for evaluating reason-\ning abilities of models, and propose EscapeCraft and MM-\nEscape to address the above issues.\n3. MM-Escape\nIn this paper, we address comprehensive investigation of\ncomplex multimodal reasoning for MLLMs by introducing\nMM-Escape, an extensible benchmark. To support this, we\ndesign EscapeCraft, a highly customizable and open envi-\nronment that engages models in free-form exploration for\nquantitatively assessing the intermediate multimodal rea-\nsoning process, as depicted in Figure 1. We highlight that\ncomplex multimodal reasoning requires an autonomous in-\ntegration of multiple multimodal abilities, and that evalu-\nating the reasoning process is as crucial as assessing final\ntask completion. We will first describe our introduced tasks\nin the following section, and detail the construction of our\nbenchmark along with its corresponding environment.\n3.1. Task Definition\nInspired by real-world game, room escape, we propose\nto place MLLMs in an escape escape scenario to inves-\ntigate their multimodal reasoning abilities in an open-\nenvironment, where models should autonomously explore\nmultimodal surroundings, and reason toward both tempo-\nrary and long-term goals. We name the introduced task,\nmultimodal room escape. It consists of a mandatory tar-\n1\n2\n3\n5\n6\n7\n8\n9\n10\n14\n15\n1\n12\nRotate to \nright side   \nto explore \nother areas\n& look for \ninteractive \nobjects or \nclues.\nContinue \nrotating to \nfully explore \nthe room.\nRotate a bit \nto have a \nbetter view \nof the TV \nand the \ntable.\nAdjust view to face \nthe TV, look for \nclues or interactive \nelements.\nMove forward \n& grab the key.\nMove to-\nwards the \ndoor & try to \nunlock it \nwith the key.\nLook \nslightly up, \nexpecting \nthe door to \nbe in view.\nTry to locate \nthe door.\nTry to use the \nkey on the door.\nThe door is \ncentered now. \nTrying to \nunlock it \nusing the key.\nNow, I have the key, \nI‘ll turn around to \nface the door and \ntry using it.\nContinue \nrotating to\nexplore …\n5\n7\n8\n9\n6\n4\n0\n2\n3\n10\n15\n11 12 13\n14\n1\nModel \ntrajectory\nStep \nindex\nModel \nposition\nObservation \ndirection\nModel-generated \nrationales\nAcquisition of \nthe key\nAttempt to \nunlock\nContinue \nrotating to\nexplore …\n0\n4\nStill can‘t \nsee the\ndoor. \nSlightly \nadjust \nview.\nFigure 2. An example of success escape path from Gemini-1.5-pro. For the first four steps, it rotates, without moving, to observe the\nsurroundings, and then starts moving towards the direct on the observed tables in the following steps. For step 5 and 6, we zoom in to the\nkey to make it clearer. The key is distorted because the view of model is tilting downwards to observe objects on the table. From step 10\nto 14, the model is already close to the door, and is slowly rotating and tilting to locate the door.\nget, the room escaping task, aiming at escaping a locked\nroom, and an optional target the post-game debriefing task,\nrequiring to reconstruct the story discovered during the es-\ncaping. Our multimodal room escape task assesses the en-\ntire reasoning process rather than solely focusing on final\ngame completion.\nRoom Escaping Task.\nThis task presents a ultimate goal\nof exiting the room, as shown in Figure 1 (Game Target) and\na detailed example is demonstrated in Figure 2. It requires\nmodels to fully explore and interact with the multimodal\nenvironment, search for props and clues, identify the exit,\nand correctly use props to unlock the door. We do not ex-\nploit step-by-step instructions to restricted model actions,\nensuring them to freely explore the environment without\nconstraints, and automatically deduce current or short-term\ngoals. This task evaluates the integrated multimodal rea-\nsoning ability, including object recognition, visual search,\nvisual reasoning, target deduction, spatial reasoning, and\nprop utilization. Please refer to Supplementary Materials\nSec. I for detailed discussion over full required abilities.\nPost-game debriefing task differs from the room escap-\ning task that involves reasoning about past experiences, cur-\nrent states, and future plans, as it takes place after the game\nis completed4. It serves as an optional tasks because its pre-\nliminary requirement is to collect all props in the game and\n4This task is also common in real-world escape game.\nexit the room successfully, which is quite challenging for\nmost of current models. Therefore, for a fair comparison,\nwe only apply this target to cases where models correctly\ncomplete the game as shown in Figure 1. It emphasizes log-\nicality and consistency of reconstruction, requiring models\nto reflect on the past experiences and events.\n3.2. Construction and Design of Environment\nWe develop EscapeCraft to place models in an escape game,\na free-form exploration environment, to comprehensively\ninvestigate complex multimodal reasoning ability of mod-\nels. It supports customizable and extensible scene genera-\ntion for our escape game with minimal manual effort.\nRoom Scene Generation\nTo support room escape task,\nwe generate diverse and interactable room environments,\nwhich requires efficient and large-scale scene data gen-\neration and rendering.\nWe develop an environment, Es-\ncapeCraft, by extending ProcTHOR [7] and Legent [4],\nwhich are originally labor-intensive regarding the scene\ngeneration process. We enable automatic size adaptation\nto predefined or customized configurations, such as room\nscale, number of rooms, and required furniture, by incorpo-\nrating 3D furniture models with annotated size information.\nCritical objects are made fully interactable to serve as props\nand clues to assist in completing the task. The flexibility and\ninteractivity of objects allow for automatic large-scale 3D\ndiffuculty-1\ndiffuculty-2\ndiffuculty-3\ndiffuculty-n\nFigure 3. Illustration of difficulties. This figure shows required\npaths for the reasoning process of success escape concerning each\nlevels. The levels can be customized and extended as depicted by\nthe “difficulty-n” example, and as demonstrated in Figure 1 (lower\nright part).\nroom generation following the requirement of our bench-\nmark. EscapeCraft is highly customizable and extensible,\nmaking it well-suited for the room escape task. Detailed\nconstruction can be found in Supplementary Material C.1.\nAction Space\nWe define three types of actions, moving,\nview adjustment, and interaction. The moving action, i.e.\nmoving forward, allows the model to change its position to\nperceive objects at different depth. View adjustment enables\nperception from different angles and facilitates object selec-\ntion for interaction, including horizontal or vertical rotation,\nand looking at specific coordinates. Interaction actions con-\ntain grabbing, using, reading, and inputting, allowing mod-\nels to obtain and utilize props from the environment, and\nprocess messages displayed by the props. These actions can\nbe executed individually or integratedly in a multi-action\nway. See Supplementary Materials Sec. C.1.2 for details.\nInventory system\nTo enable model players to acquire and\nutilize props within the environment, we design an inven-\ntory system that allows models to store and manage ac-\nquired items, access detailed information about them, and\nuse them as needed. This system assists models in effec-\ntively using props, and successfully escaping the room.\n3.3. MM-Escape Benchmark\nGame Settings.\nWe introduce an automatic reasoning\nchain generation procedure by configuring the Prop Chain.\nIn detail, this is a singly linked list representing the ordered\nsequence of items and interactions required to complete the\ngame. Each node in the chain corresponds to an interac-\ntive element, such as a key, a locked box, or a note with a\npassword, where the tail node represents the exit point of\nthe game. To construct a complete escape game setting, we\nannotate the links between nodes in the prop chain to define\nthe ways to obtain different props (such as unlimited acqui-\nsition or requiring a key to open, etc.) and their inclusion\nrelationships (for example, a key can be placed in a box).\nSee Supplementary Material Sec. C.2 for details.\nFollowing this, we employ varying difficulty levels in\nMM-Escape to facilitate in-depth assessments of complex\nmultimodal reasoning. Shown in Figure 3, difficulty is pri-\nmarily determined by the predefined prop chain of a game,\nwhere longer chains correspond to higher difficulty. We de-\nfine three standard difficulty levels for individual rooms:\n• Difficulty-1: The simplest one-hop reasoning path where\nno props are needed to unlock the door. Models can exit\nby locating the door and interacting with it directly.\n• Difficulty-2: A two-hop reasoning path requiring an addi-\ntional key or password compared to Difficulty-1. Models\nshould search for the key or password and interact with it\nto unlock the door.\n• Difficulty-3: A three-hop reasoning path requiring both a\npassword and a key, with one additional hop to Difficulty-\n2. This level challenges models with spatial reasoning,\nvisual search, and prop utilization.\nSince the prop chain can grow infinitely, our difficulty levels\nare inherently extendable. Moerover, the type of questions\nor tasks in each reasoning hop are customizable and inter-\nchangeable, further enhancing the difficulty and flexibility\nof MM-Escape. We also explore some extended settings\nthat incorporate with other tasks, such as embodied QA and\nvisual logical reasoning, with case studies in Section 5.2\nAdditionally, to further investigate the behavior and\ncomplex multimodal reasoning abilities of MLLMs, we in-\ntroduce a multi-room setting by combining two standard\nsingle rooms.\nWe create multiple multi-room combina-\ntions, each containing two individual rooms. The config-\nurations include: two Difficulty-1 rooms, two Difficulty-2\nrooms, and a mixed setting of Difficulty-1 and Difficulty-\n2. In the multi-room setting, models start in the first room,\nwhich has only one exit. Upon successfully exiting, models\nenter the second room and search for the final game exit.\nHowever, this setting presents a greater challenge than the\nsingle-room scenario, as there are two doors in the second\nroom, requiring models to distinguish between exit and en-\ntrance based on their corresponding surroundings.\nStatistics of MM-Escape\nFor individual room settings,\nwe generated 11 scenes for each of Difficulty-1 and\nDifficulty-2, and 21 scenes for Difficulty-3. As there are\ntwo types of props required by Difficulty-3, we enable a\nkey-first and a password-first prop chains. For multi-room\nsettings, we generated 10 scenes for the three different com-\nbinations introduced above.\nThere are totally 63 scenes\nfor standard evaluation of our benchmark, which could be\nfurther extended for future research without adapting the\nenvironment. These scenes are categorized into four dis-\ntinct styles: living room (14), kitchen (19), bathroom (19),\nand bedroom (11).\nAnd different objects are automati-\ncally placed within each scene to correspond to its specific\nstyle. On average, difficulty-1 scenes contain 20.18 objects,\ndifficulty-2 contains 14.55, and difficulty-3 contains 15.24.\nModels\nDifficulty-1\nDifficulty-2\nDifficulty-3\nAVG\nER (%)↑\nER\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nHuman\n100.00\n5.73\n95.45\n0.19\n100.00\n100.00\n13.64\n81.81\n0.19\n100.00\n100.00\n21.45\n75.45\n0.19\n100.00\nGPT-4o\n100.00\n11.27\n37.82\n0.42\n72.73\n81.82\n36.73\n36.73\n0.26\n71.36\n90.00\n50.19\n31.36\n0.35\n81.36\nGemini-1.5-pro\n81.82\n21.18\n49.18\n0.39\n54.55\n90.91\n47.82\n14.89\n0.44\n46.82\n74.49\n73.18\n10.43\n0.48\n61.06\nClaude 3.5 Sonnet\n72.73\n22.09\n30.64\n0.36\n45.45\n54.55\n57.45\n20.64\n0.17\n39.61\n54.83\n82.36\n16.21\n0.22\n52.60\nLlama-3.2-11b-vision\n63.64\n23.55\n31.36\n0.35\n0.00\n27.27\n75.00\n3.16\n0.44\n0.00\n27.27\n100.00\n3.55\n0.32\n21.21\nQwen-VL-Max\n18.18\n42.64\n11.36\n0.05\n0.00\n27.27\n75.00\n3.51\n0.15\n9.09\n18.18\n94.18\n2.72\n0.31\n9.09\nPhi-3-vision-128k\n0.00\n50.00\n0.00\n0.01\n0.00\n0.00\n75.00\n0.00\n0.02\n0.00\n0.00\n100.00\n0.00\n0.01\n0.00\nTable 2. Results of standard single room setting. Prop: Prop Gain; Steps: average steps used to complete the game; Grab SR: the precision\nof grabbing; Grab Ratio: the portion of grabbing actions regarding the total consumed steps. Note that Difficulty-1 requires no prop, and\nthe prop gain is therefore omitted for this setting. The max allowed steps are 50, 75, 100 for Difficulty-1, -2, -3 respectively.\nModels\nDifficulty-1 & Difficulty-1\nDifficulty-1 & Difficulty-2\nDifficulty-2 & Difficulty-2\nER(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nGPT-4o\n75.00\n35.50\n34.25\n0.32\n90.00\n100.00\n34.90\n35.52\n0.31\n70.00\n80.00\n39.50\n42.32\n0.37\nGemini-1.5-pro\n22.22\n40.22\n22.89\n0.38\n40.00\n50.00\n56.60\n16.79\n0.05\n60.00\n80.00\n60.00\n22.71\n0.34\nLlama-3.2-11b-vision\n55.56\n31.00\n36.25\n0.36\n10.00\n60.00\n66.40\n4.40\n0.40\n10.00\n40.00\n76.80\n27.00\n0.19\nClaude 3.5 Sonnet\n22.22\n45.22\n10.62\n0.08\n20.00\n20.00\n71.90\n6.75\n0.09\n10.00\n10.00\n80.00\n23.20\n0.06\nQwen-VL-max\n22.22\n40.33\n12.96\n0.16\n30.00\n50.00\n57.70\n42.30\n0.28\n0.00\n10.00\n80.00\n23.66\n0.32\nTable 3. Performance on multi-room setting for different room scenes. To assist in the more challenging setting, we provide models with\na full successful escape path from Room 1 (9 steps) for self-reflection when they try to unlock Room 2. Hence, the Prop Gain (Prop\n(%)) in the results refers only to Room 2. Further challenges of escaping from the very beginning of multi-room setting are discussed in\nSupplementary Material F.\nThe objects are logically arranged in a manner consistent\nwith real-life settings and randomly distributed within the\nrooms to serve as part of the tasks for models.\nMetrics of MM-Escape\nTo comprehensively evaluate\ncomplex multimodal reasoning ability, we propose a set of\nmetrics for the intermediate process, alongside an indica-\ntor for final task completion. For the room escape task, we\nuse average escape rate (ER) as the indicator of game com-\npletion, and design five metrics for measuring intermediate\ninteractions, including prop gain, average steps, grab count,\ngrab success rate, and grab ratio. Denoting the total steps\nas S, amount of succeeded grabbing action as N T P\ngrab, grab\nsuccess rate as GSR, grab ratio as Rgrab we have,\nProp Gain =\nN T P\ngrab\nP Prop count,\n(1)\nGSR =\nN T P\ngrab\nP Grabbing actions,\n(2)\nRgrab =\nP Grabbing actions\nS\n,\n(3)\nThe debriefing task is only applied to cases where mod-\nels successfully escaped as a bonus, to further investigate\nthe multimodal understanding and reasoning process. Be-\ncause models that successfully complete the game tend to\nachieve high scores regarding metrics mentioned above. We\nemploy large language models as evaluator for this task, as-\nsessing the consistency ([0, 1]) of reconstructed stories with\nthe groundtruth stories.\n4. Experiments\nEvaluation Setups.\nWe investigate both open-source\nmodels and propriety models of different scales, and eval-\nuate their multimodal reasoning ability in level-wise ap-\nproaches as outlined in Section 3.3. Specifically, we employ\nboth single room and multi-room settings. The multi-room\nsetting can be viewed as combinations of two single rooms,\nwith the second room featuring two doors—one for entry\nand one for exit. For robust comparison, we set the temper-\nature to 0 to eliminate token decoding diversity. The prompt\ntemplates used for evaluation are detailed in Supplementary\nMaterial Sec. J. Since the game can grow infinitely, we limit\nthe maximum steps for each difficulties as 50, 75, and 100\nrespectively, for quantitative measurements. We slightly in-\ncrease the max step to 80 for the most challenging multi-\nroom setting.\nMain Results of Game Completion\nResults of standard\nsingle room setting, along with manual evaluation are re-\nported in Table 2. See Supplementary Material Sec. B for\ndetails of human evaluation.\nThe results indicate that model performances falls signif-\nicantly behind human capabilities across all metrics, high-\nlighting the remarkable gap between current multimodal\nreasoning abilities and human-level capabilities. Among\nthe models, GPT-4o demonstrates the strongest overall per-\nformance across all difficulty levels, achieving the highest\naverage escape rate (81.36%), followed by Gemini-1.5-pro\n(61.06%). Other models limited success in task comple-\ntion, except for Phi-3-vision-128k, which fails considering\nFigure 4. Analysis on the grabbing behaviors with respect to the escape rate and the visibility of the exit at initial location.\nthe task completion. Claude 3.5 Sonnet and Llama-3.2-11b-\nvision achieve moderate ER, but their performance drops\nsignificantly in Difficulty-2 and -3, suggesting weaknesses\nin multi-hop multimodal reasoning.\nFor the intermediate process, GPT-4o efficiently com-\npletes tasks with relatively fewer steps while maintaining\na high grabbing success rate.\nNotably, Gemini-1.5-pro\nachieves a lower escape rate (ER) in Difficulty-2 compared\nto GPT-4o, yet it presents the highest Prop Gain, suggesting\na strong visual search ability. For models do not survive the\ntask completion, we can still measure their multimodal rea-\nsoning ability in terms of Prop Gain, GSR and Grab Ratio.\nLlama-3.2-11b-vision and Qwen-VL-Max receive identical\nProp Gain, while the later presents higher GSR with a lower\nGrab Ratio. This implies that Qwen-VL-Max can precisely\nidentify scenes with useful props, while Llama-3.2 is likely\nto adopt a greedy policy to increase grab actions so as to\nfind more props, which is often unintentionally.\nFor the more challenging multi-room setting, we can\nderive similar conclusions.\nAdditionally, we notice that\nby providing a successful path of the first room helps the\nmodel to better conduct multimodal reasoning in our task\nfor most models, but the improvements present in differ-\nent forms. For example, GPT-4o achieves an ER of 90% for\nDifficulty-2 when bootstrapped by a first room of Difficulty-\n1, and prop gain of Gemini and Llama 3.2 is improved in\nDifficulty-2 & -2 combination compared with the setting of\na single room of Difficulty-2.\n5. Analysis and Discussions\nTakeaway Observations\n• Distinct Human-Like Behavioral Patterns: Models ex-\nhibit unique behaviors in room escape task. Gemini tends\nto remain in a fixed location, typically the starting point,\nscanning its surroundings before taking action. In con-\nModels\nDifficulty-2\nDifficulty-3\n#Key\n#Exit\n#PW\n#Key\n#Exit\nstep\nClaude\n59.60\n62.60\n5.25\n20.67\n84.80\nGPT\n16.25\n39.10\n11.80\n17.23\n50.40\nGemini\n16.50\n50.90\n20.67\n38.13\n65.70\ncost\nClaude\n0.91\n0.23\n0.66\n0.50\n0.46\nGPT\n0.68\n0.40\n0.36\n0.28\n0.40\nGemini\n0.62\n0.63\n0.49\n0.29\n0.31\nTable 4. steps taken to to key prop and door. & cost of key props.\n#PW: step\/cost to obtain passwords. cost of # represents the pro-\nportion of steps the model spends searching for # in the entire path.\ntrast, GPT-4o first observes a wider range for a global un-\nderstanding of the environment.\n• Diverse Observation Strategies: Some models deviate\nfrom a conventional front-facing view. Gemini frequently\nemploys a downward-facing perspective to inspect ob-\njects like tables and chairs, whereas GPT-4o predomi-\nnantly relies on a front-facing view.\n• Common failure modes:\nWe observe some common\nreasons leading to unsuccessful escape, such as move-\nment failures and interaction challenges. Movement fail-\nures vary across models, including repetitive trajectories\n(GPT-4o) and getting trapped (Gemini and Claude). Inter-\naction challenges include action combination failure (Phi-\n3 and Qwen-VL) and imprecise object specifying.\n• Adherence to Long-Term Goals: Most models remain\ngoal-oriented, focusing on locating the exit and clues or\nprops that could assist in escaping. However, Phi-3 con-\nsistently fails to generate valid object interactions and\ndoes not recognize doors, unlike other models, which en-\ngage with doors upon detection.\nPlease refer to Supplementary Material Sec. A for full ver-\nsion of overall observations.\nLook at \nthe \nnumbers \non the \nwall to \nsee if \nthey \nprovide \nany clues \nfor the \npassword.\nMoving \ncloser to \nthe table \nto inspect \nthe \nobjects on \nit and the \ncabinet \nbehind it. \nThe \nnumbers \n‘3422’ on \nthe wall \nmight be a \nclue, so I‘ll \nkeep them \nin mind……\nMoving closer to the table and focusing on the strawberry \nto try grabbing it again. The items on the table don't seem \ninteractable, so I'll try the cabinet next.\nMoving \ncloser \nto the \ngame \ncontroll\ner and \nattempti\nng to \ngrab it \nagain. I \nthink \nI'm \nclose \nenough \nnow.\nTurn around to \nface the rest \nof the room and \ninspect the \nobjects on the \ntable.\nRotate to the \nright to get a \nbetter view of \nthe room and \nidentify other \npotential clues \nor objects……\n   ……\nMove slightly closer to the table to grab the \nobject on it……\nMove closer to the table to grab the key……\nModel trajectory\nModel position\nObservation direction\nModel-generated rationales\nObserving\n(a) Case of Gemini\n(b) Case of GPT-4o\n(b) Case of GPT-4o (moving password figure away from the door)\nFigure 5. The extended scenario where the required password is displayed via a numerical pattern on the wall, rather than explicitly written\non notes. GPT-4o completes reading it at once and exits within five steps, while Gemini struggles to repetitively search the room. Moving\nthe pattern away from the door further challenges GPT-4o, leading to a failure of escaping.\n5.1. Analysis of Entire Path\nWe investigate three three key questions in this section: Q1)\nHow many steps required to obtain props? Q2) How many\nsteps are needed to exit the room after acquiring the core\nprop (key or password)? Q3) What is the relationship be-\ntween grab success rate and escape outcome for each test?\nFor Q1, GPT-4o presents a significant advantage in step\ncounts required to obtain the key followed by Gemini as\nshown in Table 4. While Claude requires fewer steps to\nfind props in Difficulty-3, this comes at the cost of a sig-\nnificant decrease in escape rate. The superior performance\nin locating and obtaining the core props can be attributed\nto better understanding of task objectives and the holistic\nenvironment, and its enhanced reasoning abilities in this\ncontext. For Q2, Gemini can locate and acquire the key\nat a lower cost in difficulty-2, but GPT-4o outperforms in\nDifficulty-3, which is more complex. GPT-4o benefits from\nits prior memory and understanding of the room environ-\nment, gained in the process of obtaining key props, which\naids it to efficiently locate the exit and escape with fewer\nsteps compared to other models. For Q3, escape success\nis positively correlated with grab success rate (GSR), as\nshown in Figure 4(a).\nHigher GSR implies that models\nhave experienced more successful interactions with the en-\nvironment, potentially indicating a clearer understanding of\nthe overall environment and ultimate goals for our task.\nWhile GSR declines with difficulty, the scores of GPT-4o\nand Claude 3.5 remain relatively stable compared to others,\nwith less variation in grabbing behavior and GSR across dif-\nficulties. The low GSR of Qwen in difficulty-2 and -3 can\nbe partly caused by the ineffective perception of the envi-\nronment, inferior reasoning and interacting decision in this\ncomplex tasks, while the ow GSR of Llama 3.2 is limited\nby its input registration of only one image at a time.\nPlease refer to Supplementary Materials Sec. E for de-\ntailed discussion, and Sec. D for additional examination\nabout the moving distance and arrangement of the room.\n5.2. The Extensibility of EscapeCraft\nWe provide an extended case study in this section. We also\ndiscussion of fully autonomous version of multi-room set-\nting in Supplementary Materials Sec. F, and an additional\ncustomizations of escaping path in Supplementary Materi-\nals Sec. G.\nWe introduced an extended scenario where the required\npassword is displayed via a numerical pattern on the wall,\nrather than explicitly written on notes, as shown in Fig-\nure 5.\nModels should recognize the pattern on the wall\n(password) and infer its relevance to the door. When pat-\ntern appears near the door, GPT-4o quickly identifies it and\nexit in the following five steps, while Gemini, despite see-\ning the pattern, failed to recognize it as the password and\ninstead searched the room exhaustively repeatedly.\nWe further move the pattern away from the location\nof the door, and observe unchanged behaviors of Gemini.\nHowever, GPT-4o performs differently, by repeatedly mov-\ning between the bed and the wall without recognizing the\npassword. It also failed to interact with the door until the\ngame stops by the max allowance, revealing limitations in\nits long-term reasoning and spatial reasoning.\n5.3. Analysis on Post-game Debriefing\nThe post-game debriefing task requires models to recall\ntheir escape process and obtained clues, and reconstruct the\nwhole stories. As successful escape is necessary for post-\ngame debriefing, we only evaluate models with high suc-\ncess rate, that is, GPT-4o and Gemini-1.5-pro. Results show\nthat both models fall short of ability of retelling the stories.\nModels pay strong attention to the processes which are di-\nrectly related to the completion of room escaping, such as\npassword acquisition. Meanwhile, they ignore background\nstories which are less important but also helpful for escap-\ning. For the reason of limited model abilities, this may be a\neffective strategy to complete tasks. However, with the en-\nhancement of model abilities in the future, it is necessary to\nimprove model ability of memorization of background in-\nformation. For the experiment results, please refer to Sup-\nplementary Material Sec. H.\n6. Conclusions\nIn this paper, we introduce MM-Escape, an extensible\nbenchmark for investigating multimodal reasoning, inspired\nby real-world escape games. We also develop EscapeCraft\nthat enables models to engage in free-form exploration for\nassessing multimodal reasoning, to construction our bench-\nmark.\nWe find that MLLMs can successfully complete\nthe simplest level of tasks, and some models even exhibit-\ning human-like behaviors and strategies. However, perfor-\nmance dramatically drops as task difficulty increases, while\nhuman testers consistently succeed.\nMM-Escape reveals\ndistinct failure modes across models, such as repetitive tra-\njectories without adaptive exploration, trapped in corners\nwithout good spatial awareness. We hope our work sheds\nlight on new challenges, and uncovers potential improve-\nments for MLLMs.\nContributions\nZiyue Wang: Design of the escape process and post-game\ndebriefing, implementation of EscapeCraft, all reported ex-\nperiments. Paper writing: all sections and figures.\nYurui Dong: Design of the escape process and post-game\ndebriefing, construction of 3D environment, all engineering\nand coding works. Paper writing: method sections and ap-\npendix sections, figures of case study.\nFuwen Luo: Design of the escape process, design and ex-\nperiments of post-game debriefing. Paper writing: related\nworks, post-game debriefing experiments and analysis.\nMinyuan Ruan: Implementation of EscapeCraft, scene\ngeneration, construction of homepage. Paper writing: anal-\nysis, human evaluations, figures of analysis.\nZhili Cheng: Construction of 3D environment, design of\nroom escape process.\nChi Chen: Design of room escape process and post-game\ndebriefing, support on experiments.\nPeng Li: Project supervision, advising of all designs, engi-\nneering, experiments, and paper writing.\nYang Liu:\nProject supervision,\nadvising of all de-\nsigns,\nengineering,\nexperiments,\nand\npaper\nwriting.\nReferences\n[1] Shuang Bai and Shan An. A survey on automatic image cap-\ntion generation. Neurocomputing, 311:291–304, 2018. 2\n[2] Manojit Bhattacharya, Soumen Pal, Srijan Chatterjee, Sang-\nSoo Lee, and Chiranjib Chakraborty. Large language model\nto multimodal large language model: A journey to shape\nthe biological macromolecules to biological sciences and\nmedicine. Molecular Therapy-Nucleic Acids, 35(3), 2024.\n2\n[3] Peng Chen, Pi Bu, Jun Song, Yuan Gao, and Bo Zheng.\nCan vlms play action role-playing games? take black myth\nwukong as a study case, 2024. 3\n[4] Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An\nLiu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, and\nMaosong Sun. Legent: Open platform for embodied agents.\narXiv preprint arXiv:2404.18243, 2024. 4\n[5] Marc-Alexandre Cˆot´e, Akos K´ad´ar, Xingdi Yuan, Ben Ky-\nbartas, Tavian Barnes, Emery Fine, James Moore, Matthew\nHausknecht,\nLayla El Asri,\nMahmoud Adada,\net al.\nTextworld: A learning environment for text-based games. In\nComputer Games: 7th Workshop, CGW 2018, Held in Con-\njunction with the 27th International Conference on Artificial\nIntelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018,\nRevised Selected Papers 7, pages 41–75. Springer, 2019. 3\n[6] Adrian de Wynter. Will gpt-4 run doom? IEEE Transactions\non Games, pages 1–10, 2024. 3\n[7] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,\nKiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,\nAniruddha Kembhavi, and Roozbeh Mottaghi.\nProcthor:\nLarge-scale embodied ai using procedural generation. Ad-\nvances in Neural Information Processing Systems, 35:5982–\n5994, 2022. 4, 1\n[8] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang\nZhou, and Houqiang Li. Transvg: End-to-end visual ground-\ning with transformers.\nIn Proceedings of the IEEE\/CVF\nInternational Conference on Computer Vision, pages 1769–\n1779, 2021. 2\n[9] Shiyao Ding and Takayuki Ito.\nMinellama: Llama with\nretrieval-augmented generation as a decision maker in\nminecraft. In International Conference on Practical Appli-\ncations of Agents and Multi-Agent Systems, pages 97–108.\nSpringer, 2024. 2\n[10] Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei\nZhang, and Xiaomeng Li. Holistic autonomous driving un-\nderstanding by bird’s-eye-view injected multi-modal large\nmodels.\nIn Proceedings of the IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13668–\n13677, 2024. 2\n[11] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence. Palm-e: an embodied multimodal lan-\nguage model. In Proceedings of the 40th International Con-\nference on Machine Learning. JMLR.org, 2023. 3\n[12] Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei\nWang, Hongqing Chu, and Bingzhao Gao. Mllm-sul: Multi-\nmodal large language model for semantic scene understand-\ning and localization in traffic scenarios, 2024. 2\n[13] Hao Fei, Xiangtai Li, Haotian Liu, Fuxiao Liu, Zhuosheng\nZhang, Hanwang Zhang, and Shuicheng Yan. From mul-\ntimodal llm to human-level ai: Modality, instruction, rea-\nsoning and beyond. In Proceedings of the 32nd ACM In-\nternational Conference on Multimedia, pages 11289–11291,\n2024. 2\n[14] William H Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov.\nMinerl: a large-scale dataset of minecraft\ndemonstrations.\nIn Proceedings of the 28th International\nJoint Conference on Artificial Intelligence, pages 2442–\n2448, 2019. 3\n[15] Sagar Jose, Khanh TP Nguyen, Kamal Medjaher, Ryad Ze-\nmouri, M´elanie L´evesque, and Antoine Tahan. Advancing\nmultimodal diagnostics: Integrating industrial textual data\nand domain knowledge with large language models. Expert\nSystems with Applications, 255:124603, 2024. 2\n[16] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,\nLuca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,\nDaniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv:1712.05474,\n2017. 2\n[17] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, Jianfeng Gao, et al.\nMultimodal\nfoundation models: From specialists to general-purpose as-\nsistants. Foundations and Trends® in Computer Graphics\nand Vision, 16(1-2):1–214, 2024. 2\n[18] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou,\nYu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and\nJifeng Dai. Auto mc-reward: Automated dense reward de-\nsign with large language models for minecraft. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 16426–16435, 2024. 2\n[19] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dong-\nmei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodal\nmemory empowered agents excel in long-horizon tasks. In\nAdvances in Neural Information Processing Systems, pages\n49881–49913. Curran Associates, Inc., 2024. 3\n[20] Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu,\nand Wei Yang. Juewu-mc: Playing minecraft with sample-\nefficient hierarchical reinforcement learning. arXiv preprint\narXiv:2112.04907, 2021. 2, 3\n[21] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li,\nTianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\net al. Llava-plus: Learning to use tools for creating multi-\nmodal agents. In European Conference on Computer Vision,\npages 126–142. Springer, 2024. 2\n[22] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering.\nAdvances\nin Neural Information Processing Systems, 35:2507–2521,\n2022. 2\n[23] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav\nPutta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal,\nPaul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al.\nOpeneqa: Embodied question answering in the era of foun-\ndation models. In Proceedings of the IEEE\/CVF conference\non computer vision and pattern recognition, pages 16488–\n16498, 2024. 3\n[24] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra\nMalik. Egoschema: A diagnostic benchmark for very long-\nform video language understanding. Advances in Neural In-\nformation Processing Systems, 36:46212–46244, 2023. 3\n[25] Ron Mokady, Amir Hertz, and Amit H. Bermano.\nClip-\nCap: CLIP prefix for image captioning.\nArXiv preprint,\nabs\/2111.09734, 2021. 2\n[26] Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Yifu\nYuan, Zibin Dong, Jinyi Liu, MingZhi Li, Yuzheng Zhuang,\nand YAN ZHENG.\nPeria: Perceive, reason, imagine, act\nvia holistic language and vision planning for manipulation.\nAdvances in Neural Information Processing Systems, 37:\n17541–17571, 2025. 2\n[27] OpenAI. Computer-using agent: Introducing a universal in-\nterface for ai to interact with the digital world. 2025. 3\n[28] Cheng Qian, Peixuan Han, Qinyu Luo, Bingxiang He, Xiusi\nChen, Yuji Zhang, Hongyi Du, Jiarui Yao, Xiaocheng Yang,\nDenghui Zhang, Yunzhu Li, and Heng Ji.\nEscapebench:\nPushing language models to think outside the box, 2024. 3\n[29] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu\nSheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A\nmulti-modal open-ended embodied system in minecraft via\nactive perception. In 2024 IEEE\/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 16307–\n16316. IEEE, 2024. 2, 3\n[30] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv\nBatra. Habitat: A Platform for Embodied AI Research. In\nProceedings of the IEEE\/CVF International Conference on\nComputer Vision (ICCV), 2019. 2\n[31] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan\nBisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer,\nand Dieter Fox.\nAlfred:\nA benchmark for interpreting\ngrounded instructions for everyday tasks. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020. 3\n[32] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023. 3\n[33] Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang,\nFei Huang, and Jitao Sang. Mobile-agent-v: Learning mo-\nbile device operation through video-guided multi-agent col-\nlaboration, 2025. 3\n[34] Yuqing Wang and Yun Zhao. Gemini in reasoning: Unveiling\ncommonsense in multimodal large language models. arXiv\npreprint arXiv:2312.17661, 2023. 2\n[35] Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Hait-\neng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng\nYou, and Hongxia Yang. Exploring the reasoning abilities\nof multimodal large language models (mllms): A compre-\nhensive survey on emerging trends in multimodal reasoning.\narXiv preprint arXiv:2401.06805, 2024. 2\n[36] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma,\nand Yitao Liang. Rat: Retrieval augmented thoughts elicit\ncontext-aware reasoning and verification in long-horizon\ngeneration.\nIn NeurIPS 2024 Workshop on Open-World\nAgents. 2\n[37] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xi-\naojian Ma, and Yitao Liang.\nDescribe, explain, plan\nand select: Interactive planning with large language mod-\nels enables open-world multi-task agents.\narXiv preprint\narXiv:2302.01560, 2023. 3\n[38] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zi-\nlong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang.\nJarvis-1:\nOpen-world multi-task agents with memory-\naugmented multimodal language models. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 47(3):1894–\n1907, 2025. 3\n[39] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang,\nMing Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-\ne: Self-evolving mobile assistant for complex tasks. arXiv\npreprint arXiv:2501.11733, 2025. 3\n[40] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li,\nSiheng Zhao, Ruisheng Cao, Jing Hua Toh, Zhoujun Cheng,\nDongchan Shin, Fangyu Lei, et al. Osworld: Benchmark-\ning multimodal agents for open-ended tasks in real computer\nenvironments. Advances in Neural Information Processing\nSystems, 37:52040–52094, 2025. 2, 3\n[41] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong\nWang, Weidong Liu, and Yang Liu. Exploring large language\nmodels for communication games: An empirical study on\nwerewolf. arXiv preprint arXiv:2309.04658, 2023. 3\n[42] Yibo Yan, Shen Wang, Jiahao Huo, Jingheng Ye, Zhendong\nChu, Xuming Hu, Philip S Yu, Carla Gomes, Bart Selman,\nand Qingsong Wen. Position: Multimodal large language\nmodels can significantly advance scientific reasoning. arXiv\npreprint arXiv:2502.02871, 2025. 2\n[43] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret,\nShunyu Yao, Karthik Narasimhan, and Ofir Press.\nSwe-\nagent: Agent-computer interfaces enable automated software\nengineering.\nAdvances in Neural Information Processing\nSystems, 37:50528–50652, 2024. 3\n[44] Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han,\nLi Fei-Fei, and Saining Xie. Thinking in space: How mul-\ntimodal large language models see, remember, and recall\nspaces. arXiv preprint arXiv:2412.14171, 2024. 3\n[45] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng\nQian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella,\nMarziyeh Movahedi, Manling Li, et al.\nEmbodiedbench:\nComprehensive benchmarking multi-modal large language\nmodels for vision-driven embodied agents. arXiv preprint\narXiv:2502.09560, 2025. 3\n[46] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing\nHuang, Dong Yu, and Jiebo Luo.\nA fast and accurate\none-stage approach to visual grounding. In Proceedings of\nthe IEEE\/CVF international conference on computer vision,\npages 4683–4693, 2019. 2\n[47] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang.\nMm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023. 2\n[48] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69–85. Springer, 2016.\n2\n[49] Yichen Zhu, Zhicai Ou, Xiaofeng Mou, and Jian Tang.\nRetrieval-augmented embodied agents.\nIn Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17985–17995, 2024. 3\nHow Do Multimodal Large Language Models Handle Complex Multimodal\nReasoning? Placing Them in An Extensible Escape Game\nSupplementary Material\nA. Full Takeaway Observations\n• Distinct human-like behavioral patterns: There are dis-\ntinct behavioral patterns among different models in the\nroom escape task. GPT-4o and Gemini exhibit the most\nhuman-like behavior, but the patterns differ from each\nother. For example, GPT-4o demonstrating stable view-\npoint control and deliberate movements. At beginning\nsteps, it tends to walk around with large distances and\nobserve the surroundings in a wide range. While Gemini\ntends to look around before taking further actions to move\nto other locations. Some other models tend to shift their\nperspectives arbitrarily, leading to inefficient exploration.\nGemini frequently gets stuck, struggling with spatial nav-\nigation, while Phi-3 exhibits a tendency to continuously\nrotate in place with minimal actual movement, hindering\neffective exploration.\n• Robustness towards system prompts: Most of the models\nare faithful to the system instruction. They are aware of\nthe ultimate target, to locate the door and make a way\nout, and are not easily distracted by findings during the\nexploration, except for Phi-3 who always fail to generate\nrequired actions with valid and interactable items.\n• Common failure modes: However, significant limitations\npersist across all model, including GPT-4o sometimes.\nA common failure mode is inaccurate object positioning\nwithin the field of view. Models often fail to center the\ntarget object, which is indicated by a guiding red dot in\nour environment, precisely suggesting the objects to in-\nteract with. This often leads to unsuccessful interactions\nsuch as grabbing or entering for password. Furthermore,\nsome models struggle with tool utilization, particularly in\ncases requiring abstract reasoning, such as correctly ap-\nplying a password to unlock a door.\nB. Human Evaluation\nWe conduct manual evaluation on MM-Escape, and re-\nport detailed results in Table 5. Human participants in Es-\ncapeCraft exhibit a clear understanding of how to complete\ntasks efficiently. By observing objects in the environment,\nthey can make reasonable judgments about which items to\npick up, leading to a higher success rate in effective item\nacquisition and usage. Additionally, when unable to open\ndoors or interactable objects, humans are more adept at\npromptly shifting their approach to seek alternative clues in\nthe environment rather than getting stuck. In terms of spa-\ntial awareness, they demonstrate a strong ability to perceive\nthe relative positions of objects, enabling logically reason-\nable, smoother and more coordinated exploratory actions.\nAcross Difficulty-1 to 3, human participants consistently\nidentify all necessary items with less interaction attempts\ncompared with MLLMs, and successfully complete all the\ntasks within a limited number of steps.\nC. Construction Details\nC.1. Environment Construction\nC.1.1. Room Generation\nWe adopted the automated 3D room generation method\nProcTHOR [7], with additional improvements to enhance\nits flexibility and applicability regarding diverse type of\nscenes. Following Procthor, we generate 3D environments\nthat can simulate diverse real-world scenes, such as bed-\nrooms, living rooms, and offices by maintaining collections\nof typical objects that are common in different scenes. For\ninstance, desks in offices, workbenches in laboratories, and\nother representative objects of corresponding scenes. We\nenable automatic creation of 3D rooms from the collections\nof each scenes, ensuring that the generated rooms accu-\nrately reflect their respective environments.\nWe use a configuration file to generate each room, spec-\nifying the items along with required styles, positions, sizes,\nand interactivity.\nThis enables precise control over the\nplacement of prop objects, ensuring that they are arranged\nin a manner aligning with real-world expectations on spatial\narrangement.\nBenefits of the Automated 3D Room Generation include:\n• Diversity and Complexity: By automatically generating a\nvariety of 3D rooms, we can provide the model with di-\nverse environments, ensuring that it is capable of handling\nvarious layouts, objects, and puzzle elements. This diver-\nsity is critical in assessing the model’s ability to reason\nin different scenarios, evaluating its performance when\nconfronted with unknown and complex situations. More-\nover, the ability to create different configurations on the\nfly means the model will not be limited to predefined en-\nvironments, which helps to prevent overfitting to specific\nroom layouts.\n• Enhanced Realism:\nUnlike manually designed fixed\nscenes, automatically generated 3D environments can\nsimulate more natural and irregular spatial layouts. This\nis essential for training and evaluating agents on spatial\nreasoning, pathfinding, and interaction skills. By incor-\nporating a wide range of room designs, we create more\nMetrics\nScene\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nLevel1\nSteps\n10\n3\n7\n7\n5\n5\n6\n3\n7\n4\n6\nProp Gain(%)\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\nGrab Count\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nGrab Success(%)\n50.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\nLevel2\nSteps\n23\n17\n10\n8\n9\n13\n15\n8\n16\n20\n11\nProp Gain(%)\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\nGrab Count\n3\n3\n2\n2\n2\n3\n3\n2\n3\n3\n2\nGrab Success(%)\n66.67\n66.67\n100.00\n100.00\n100.00\n66.67\n66.67\n100.00\n66.67\n66.67\n100.00\nLevel3 (note-key)\nSteps\n22\n20\n21\n17\n23\n27\n18\n16\n22\n23\n27\nProp Gain(%)\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\nGrab Count\n4\n5\n4\n4\n4\n5\n3\n3\n4\n4\n5\nGrab Success(%)\n75\n60\n75\n75\n75\n60\n100\n100\n75\n75\n60\nLevel3 (key-note)\nSteps\n-\n22\n21\n19\n18\n20\n24\n16\n27\n17\n18\nProp Gain(%)\n-\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\nGrab Count\n-\n4\n4\n4\n5\n5\n6\n4\n5\n4\n4\nGrab Success(%)\n-\n75\n75\n75\n60\n60\n50\n75\n60\n75\n75\nTable 5. Detailed results for various levels.Since humans completed all escape tasks in the evaluation, the escape rate is 100% and not\nreflected above.\nrealistic scenarios in which agents must navigate, interact\nwith objects, and solve problems, similar to real-world\nchallenges.\n• Efficiency: The automated generation of 3D rooms sig-\nnificantly improves the efficiency of the testing process.\nWhether for debugging model performance or conduct-\ning large-scale evaluations, the ability to generate various\nenvironments quickly eliminates the time-consuming and\ntedious process of manually creating scenes. This allows\nfor faster iteration and more comprehensive testing with-\nout the bottleneck of scene creation.\n• Evaluation Robustness: In escape room-style games, the\ndiversity of room layouts and puzzles directly influences\nthe game’s difficulty. By automating the scene generation\nprocess, we enable the model to train and be evaluated\nin a wide array of environments, which helps enhance its\nrobustness. This diversity allows the model to develop\nbetter strategies for handling new and unexpected chal-\nlenges, improving its ability to generalize across different\nscenarios.\n• Prevention of Cheating and Overfitting: One of the ma-\njor advantages of generating an infinite variety of scenes\nis the prevention of both cheating and overfitting. Fixed\ntesting environments often lead to overfitting, where a\nmodel can ”learn” to exploit certain patterns or repeti-\ntive features of the environment. In contrast, each au-\ntomatically generated room is unique, with random ele-\nments that require the model to demonstrate true problem-\nsolving abilities in previously unseen configurations. This\nensures that the model cannot simply memorize the envi-\nronment but must adapt its strategies to succeed.\n• Adaptive Adjustment: Another key feature of our ap-\nproach is the ability to dynamically adjust the agent’s\nstarting position and other environmental variables. This\nfeature allows us to test how the agent performs under dif-\nferent initial conditions, such as varying the agent’s start-\ning location, the distribution of objects, or the complexity\nof the puzzle. These adjustments enable a more compre-\nhensive assessment of the agent’s performance, providing\ndeeper insights into its ability to adapt and solve problems\nin diverse situations.\nThe automated 3D room generation framework we devel-\noped not only enhances the diversity and realism of test-\ning environments but also optimizes the efficiency of large-\nscale evaluations. By providing a mechanism for dynami-\ncally altering the environment and agent conditions, it of-\nfers a more robust and fair evaluation process, ensuring that\nmodels are evaluated under realistic, varied, and challeng-\ning conditions.\nC.1.2. Action Space\nIn our EscapeCraft environment, the agent is allowed to\nperform a set of actions that facilitate its interaction with\nthe environment. These actions include moving forward, ro-\ntating right, rotating down, looking at specific coordinates,\ngrabbing objects, and interacting with elements in the envi-\nronment. Each of these actions plays a distinct role in en-\nabling the agent to explore and solve tasks within the escape\nscenario.\n• Moving Forward: This action specifies the distance the\nagent needs to travel along its current heading.\nThe\nagent’s movement is controlled by the distance parame-\nter, which dictates how far it should move in a straight\nline.\n• Rotating Right: This action specifies the angle by which\nthe agent should rotate to the right. The agent can adjust\nits orientation by a specified angular increment, which al-\nlows it to navigate through the environment by changing\nits field of view.\n• Rotating Down: Similar to rotating right, this action al-\nlows the agent to lower its head by a specified angle. This\naction is crucial for examining objects at different vertical\nlevels, contributing to a more thorough exploration of the\nenvironment.\n• Looking At: The ”looking at” action involves orienting\nthe agent’s view towards a specific coordinate within its\ncurrent field of view. This coordinate is represented in a\nrelative manner, with the center of the field of view de-\nnoted as (0.5, 0.5). By specifying the target coordinates,\nthe agent can focus on particular objects or areas of inter-\nest in the environment.\n• Grabbing: The grabbing action indicates if the agent\nwants to pick up an object or interact with an item within\nits proximity. This action is typically used when the agent\nidentifies an object that can be picked up or manipulated,\nallowing it to add that item to its inventory or interact with\nit to get crucial information.\n• Interacting: The interaction action is multifaceted and de-\npends on the context of the object the agent is engaging\nwith. Interactions fall into three primary categories:\n1. Item Usage: The agent can use items from its inven-\ntory by referencing the unique ID of an item, such as a\nkey, tool, or piece of equipment that it has previously\nobtained. In these cases, the agent specifies the item\nID and applies it to relevant objects in the environment\n(e.g., using a key to unlock a door).\n2. Text Input: Some interactions require the agent to in-\nput text, such as a password to unlock a combination\nlock. These textual inputs are necessary to progress in\nthe environment when dealing with specific security\nmechanisms.\n3. Read: When the agent wants to know the detailed in-\nformation of an item in its inventory (e.g., reading the\ncontent recorded in a note), it can use this parameter\nand provide the ID of the corresponding item to the\nitem usage field.\nA special case arises when the agent performs the grab\naction and leaves the interaction input empty. In this in-\nstance, it indicates the agent’s intent to pick up an item\nwithin the field of view, without specifying a particular\nitem to interact with. This action is used when the agent\nis trying to collect objects that are relevant to its escape\nmission.\nThroughout the agent’s exploration, its interactions with\nthe environment yield varying types of feedback. The envi-\nronment is populated with different types of objects, classi-\nfied as follows:\n• Non-Escape Related Props: These are objects within the\nenvironment that do not directly contribute to the agent’s\nescape objectives. Interactions with these items provide\nno information or progress.\n• Collectible Items: These items can be obtained and added\nto the agent’s inventory, providing critical information\nand\/or utility for the agent’s tasks. Upon collection, the\nagent gains knowledge of the item’s identity and its asso-\nciated attributes.\n• Locked Props:\nThese include objects such as locked\ndoors, chests, or other secured items. When the agent in-\nteracts with a locked object in the early stages, it receives\na prompt indicating the type of item required to unlock\nit. Upon obtaining the corresponding item (e.g., a key,\na code, or another unlocking mechanism), the agent can\nuse the appropriate item from its inventory to unlock the\nobject by specifying its ID or providing the required in-\nput (e.g., entering a password). Once these items are un-\nlocked, the agent will immediately obtain the props con-\ntained in them and be informed of the simple information\nof the items obtained.\nThese interaction dynamics are crucial for the agent’s\nprogression in the environment, as they form the basis\nfor decision-making, object management, and problem-\nsolving. The design of these interactions reflects the need\nfor both exploration and strategy, with the agent needing to\nacquire, manage, and apply various items in order to navi-\ngate and ultimately escape the environment.\nC.2. Data Construction\nC.2.1. Prop Chain\nWe proposed a procedural generation approach for con-\nstructing game settings tailored to overcome the inherent\nlimitations of current language models, such as restricted\ncontext length and reduced reasoning capabilities. To ad-\ndress these constraints, we propose the concept of Prop\nChain, a singly linked list that organizes interactive game\nelements in a sequence, ensuring a coherent flow of game-\nplay interactions. Each node in the linked list corresponds\nto a distinct interactive item or action, such as a key, a\nlocked box, or a note with a password. The tail node of\nthe chain signifies the game’s exit point, thereby serving as\nthe conclusion of the sequence. Table 6 shows the the Prop\nChain for the Difficulty-3 Level.\nIn our implementation of the Prop Chain, we initially\nfocus on a set of fundamental game elements: a key, a\nlocked box (which can only be opened with a key or pass-\nword), a note (carrying both password and story-related in-\nformation), and an exit (which is locked and requires either\na key or password to access). These components are used to\nconstruct a series of interconnected nodes, where each item\nor action is represented by a node in the chain. The links be-\ntween the nodes define the relationships between the props\nand the ways in which they can be obtained or used dur-\ning the gameplay. For instance, some props may be freely\naccessible, while others require specific conditions, such as\npossessing a key to unlock a box, or using a specific pass-\nword to open the exit door.\nThe relationships between nodes can be annotated to\nspecify different modes of interaction. For example, a key\ncan be placed within a box, requiring the player to first un-\nlock the box before acquiring the key. Additionally, certain\nnodes may include complex conditions, such as a note that\nreveals the password needed to open the exit, thereby incor-\nporating both narrative and functional elements within the\ngame.\nEach node has an additional show property set to indi-\ncate whether the item should appear directly in the scene\n(for example, a key placed in a box only needs to show the\nbox in the scene, while a key that can be directly obtained\nindependently needs to be shown in the scene), allowing us\nto determine which props need to be generated in the 3D\nscene by reading the game settings.\nWhile our initial focus on a limited set of props and\ninteractions—such as the key, locked box, note, and\nexit—suffices for creating a variety of escape game settings\nthat challenge current language models, the system is highly\nextensible. The procedural nature of Prop Chain allows\nfor the seamless integration of new props, interactions, and\nunlocking mechanisms. As such, the framework can eas-\nily accommodate additional types of interactive items, more\nintricate unlock conditions, and customized gameplay me-\nchanics in future iterations. This scalability ensures that the\napproach remains adaptable to more complex and diverse\ngame scenarios, further enhancing its applicability for test-\ning language models in a variety of settings.\nThe Prop Chain framework provides a robust and flexi-\nble methodology for the procedural generation of game set-\ntings. By focusing on a set of core interactive elements and\ndefining their relationships within a linked list structure, we\nhave developed a scalable approach that can evolve to in-\ncorporate new game dynamics and meet the increasing de-\nmands of future language models.\nD. Analysis of Moving Distance\nWe calculated the optimal distance required for escape tasks\nin each scene and compared it with the real distance trav-\neled by the models. Contrary to our expectations, the dis-\ntance models travelled does not exhibit a significant correla-\ntion with the distance among key props and the door within\nthe scene shown in Table 7. This discrepancy may be at-\ntributed to the model’s lack of holistic environmental per-\nception, which prevents models from further reasoning and\nplanning based on current and ultimate goal, thereby failing\nto generate an effective and optimal route to complete the\ntask.\nE. Analysis of Grabbing Behaviors\nIn Figure 4 (b)(c)(d), we analyzed three performance met-\nrics—steps, GRS, and Rgrab—during the model task com-\npletion process under Visibility of Exits at initial locations\nand orientations . The results indicate that, under common\ntrends, the ability to see the exit from the initial position\naids the model in escaping the room with fewer steps.It\naligns with our intuition, as the exit, crucially related to\nthe ultimate task goal, plays a significant role in model’s\nvisual recognition, reasoning and interaction with the envi-\nronment to collect information. However, there exist excep-\ntions. In Difficulty 1, many models that perform well still\nshow low GSR and higher step counts despite being able to\nsee the exit initially. Therefore they do not interact directly\nwith the exit door at the very beginning, but instead choose\nto rotate around and gather information about the environ-\nment for reasoning and taking action. This is also evident in\nDifficulty-2 and -3, where these models, after acquiring the\nkey prop, can locate the exit and escape more efficiently, as\nreflected in better performance in terms of GRS, Rgrab and\nsteps.\nWe further raise three questions for the analysis of the\nreasoning process during escaping: i) How many steps it\ncosts to obtain props? ii) How many steps it costs to exit\nthe room after obtaining the core prop (key or password to\nthe door)? iii) What is the relationship between grab success\nrate (GSR) and escape outcome for each test?\nFor question 1, GPT-4o demonstrates a significant ad-\nvantage in the number of steps required to obtain the key\nfollowed by Gemini as shown in Table 4. Although Claude\nrequires fewer average steps to find props in Difficulty-3,\nthis comes at the cost of a significant decrease in escape\nrate. The superior performance in locating and obtaining the\ncore prop can be attributed to model’s better understanding\nof task objectives and the environment in the escape room,\nas well as its enhanced reasoning abilities in this context.\nFor question 2, Gemini is able to locate and acquire the\nkey at lower cost in difficulty-2. But in difficulty-3 which\nis more complex, GPT-4o performs better. It finds the core\nID\nType\nUnlock Method\nContents\nShow\nbox 1\nbox\npassword (password 1)\nkey 1, note 2\ntrue\nkey 1\nkey\n-\nfalse\nnote 1\npaper\n-\npassword (password 1)\ntrue\nnote 2\npaper\n-\nsome story\nfalse\npassword 1\npassword\n-\n-\nfalse\nexit\nexit\nkey(key 1)\n-\n-\nTable 6. Representation of the Prop Chain for the Difficulty-3 Level. The level includes a sequence of interactive props where only box 1\nand note 1 are visible in the room. The gameplay progression follows a structured sequence: the agent first discovers note 1, which\ncontains the password 1 needed to unlock box 1. Inside box 1, the agent retrieves key 1 and note 2, the latter of which contains a story\nelement of the game. Finally, the agent uses key 1 to unlock the exit and complete the game.\nGPT\nGemini\nClaude\nLLaMA\nQwen\nCorrelation\n- 0.06\n0.06\n0.49\n0.63\n- 0.48\nTable 7. Correlation between optimal distance and model moving\ndistance.\nprop with fewer steps and its prior memory and understand-\ning of the room environment—gained in the process of ob-\ntaining key props—aids it to locate the exit and escape using\neven fewer steps compared to other models.\nFor question 3, we observe that escape success is pos-\nitively correlated with GSR, as shown in Figure 4(a). A\nhigher Grab SR implies that models have experienced more\nsuccessful interactions with the environment. It potentially\nindicates a clearer understanding of the overall environment\nand ultimate goals within the room escape task, leading to\na higher success rate. As difficulty increases, the Grab SR\nof most models declines, and many of them fail to escape.\nHowever, GPT-4o and Claude 3.5 remain relatively stable,\nwith less variation in grabbing behavior and success rate\nacross difficulty settings compared to others. The low suc-\ncess rate of Qwen, and Llama 3.2 11B in difficulty 2 and 3\ncan be partly attributed to their inability to effectively per-\nceive the environment, reason and make appropriate object\ninteraction choices in more complex tasks.\nF. Discussion of Fully Autonomous Multi-room\nEscape\nWe discussed a simplified multi-room setting in Table 3.\nWe further study how models behavior in this section. The\nER of GPT-4o decreases to only 50% on average for the\nsettings of applying Difficulty-2 to room 2. The grabbing\nbehaviors also change, where both the Grab SR and Grab\nRatio decreases. Similar trends are observed for Gemini\nand Claude. These indicate that models can learn from a\nsuccessful escape history. We also note that by setting the\ntwo rooms to the same difficulty level further helps models\nModels\nDifficulty-3-note-key\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nGPT-4o\n72.73\n100.00\n47.18\n33.82\n0.42\nGemini-1.5-pro\n63.64\n86.36\n61.27\n16.06\n0.51\nClaude 3.5 Sonnet\n36.36\n40.91\n78.55\n10.03\n0.27\nModels\nDifficulty-3-key-note\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nGPT-4o\n70.00\n80.00\n53.20\n28.90\n0.29\nClaude 3.5 Sonnet\n37.50\n68.75\n88.14\n22.05\n0.15\nGemini-1.5-pro\n30.00\n60.00\n87.70\n4.79\n0.46\nTable 8.\nDetailed results of note-key and key-note settings of\nDifficulty-3.\nto escape, while different levels do not benefit as expected.\nG. Discussion of Customizing Difficulties\nWe enable two different settings of Difficulty-3, a key-note\nsetting and a note-key setting. We observe that human an-\nnotators perform equally for both settings (from Table 5),\nwhile some models present preferences regarding the key-\nfirst and the note-first (i.e. the password-first), as shown in\nTable 8. Gemini presents an approaching GPT-4o level re-\nsults in the note-key setting, while scores the worst in all\ncalculated metrics among the three reported models, pre-\nsenting a preference towards searching for the note rather\nthan recognizing and interacting with the key. Additionally,\nClaude scores higher in Grab SR regarding the key-first set-\nting than the note-first setting, potentially indicating a better\nattention on the key (directly used to unlock the door) than\non the note (with clues, implicitly assists with the escape\nprocess).\nFor the multi-room setting, whose results are reported\nin Table 3, we further extend the experiments to a full au-\ntonomous scenario to require models to escape both room\nin order all by themselves. This means, the first room no\nlonger serves as a bootstrapping guidance. We notice a per-\nformance drop both in the escape rate and the grabbing be-\nhaviors.\nH. Experiment Results of Post-game Debrief-\ning\nWe choose models with top-2 escaping rate for evaluation,\nthat is, GPT-4o and Gemini-1.5-pro. We first leverage the\ndebriefing prompt, as shown in Table 13, to get recovered\nstories from models. Then we use GPT-4o to assess the re-\ncovered stories according to their similarities to our given\ngroundtruth stories, then assign a score between 0 and 5 to\neach recovered story. As shown in Table 9, average scores\nof both models are low. We further show a case in Table 10.\nWe can see that models can recall and describe their es-\ncape process relatively well, but they do not remember clues\nabout the background stories well, which leads to the failure\nof restoring the full picture of stories.\nModel\nGPT-4o\nGemini-1.5-pro\nAverage Score\n0.33\n0.44\nTable 9. Average scores of models on post-game debriefing task.\nScores are between 0 and 5.\nI. Required Abilities\nIn our MM-Escape Benchmark, the free-form task chal-\nlenges models with the following abilities:\n• Visual search. The model should have a prior knowledge\non the potential locations of useful clues and objects, rea-\nson about its current field of view and make decisions to\nexplore the environment efficiently.\n• Active perception. Due to the limited field of view, the\nmodel cannot acquire all relevant information at once. It\nmust actively adjust the perspective by moving and rotat-\ning its viewpoint, allowing is to perceive from different\nangles and effectively identify key objects, such as the\nkey, the password and the door.\n• Spatial reasoning. The model is required to deduce spa-\ntial depth and objects in the room. It must infer hidden or\nobscured elements, estimate distances, and determine the\nmost appropriate actions based on the spatial configura-\ntion of its surroundings it observed. The model should\nleverage its understanding of space to make decisions\nabout movement, interaction, and problem-solving, pro-\ngressively advancing toward the escape objective.\n• Target deduction. The model is only given the objective\nto escape the room, without explicit instructions on how\nto achieve it. Hence, it should autonomously infer nec-\nessary actions through interaction and exploration, or de-\nGroundtruth\nThe story revolves around a secret society that hides within\nthe walls of an ancient mansion. Once you enter, you must\nfollow cryptic clues and navigate through a maze of rituals\nand secret rooms. The society’s true purpose is far darker\nthan its intellectual allure, and the deeper you go, the harder\nit is to escape. The goal is to uncover the society’s myster-\nies and find a way out before becoming part of its twisted\nlegacy.\nRecovered Story from GPT-4o\nThe room appeared to be a deliberately designed escape\nchallenge, possibly created for entertainment or as a test of\nproblem-solving skills. The presence of the locked box and\nthe notes suggested a sequence of puzzles that needed to be\nsolved to escape.\nThe sequence of events began with the discovery of Note 1,\nwhich provided the numerical code ”9926.” This code was\nused to unlock the box on the table, revealing Key 1 and\nNote 2. The key was then used to unlock the door, allowing\nfor the escape from the room.\nThe story on Note 2, while not explicitly detailed, might\nhave provided a narrative or thematic context for the escape\nchallenge, perhaps hinting at the creator’s intentions or the\npurpose of the room. The overall experience was one of ex-\nploration, deduction, and the satisfaction of solving a series\nof interconnected puzzles to achieve freedom.\nRecovered Story from Gemini-1.5-pro\nThe room was clearly designed as an escape room chal-\nlenge. The purpose was to test my problem-solving skills\nand ability to find and interpret clues. The creator of the\nchallenge remains unknown, but the sterile and impersonal\nnature of the room suggests a deliberate design choice. The\nsequence of events was as follows: I started in the center\nof the room, explored the cabinet near the door, and then\nthe sofa and stool. I then found the note with the num-\nber ”1148”. Realizing the box on the side table was likely\npassword-protected, I used the number from the note to un-\nlock it. Inside, I found the key (key 1) and a second note\n(note 2). I then used the key to unlock the door and escape\nthe room. The story on note 2 likely served as a distraction\nor perhaps a thematic element to the escape room’s design,\nbut ultimately, the key to escaping was the numerical code\n”1148”.\nTable 10. A case of post-game debriefing.\ncompose the ultimate objective into feasible and short-\nterm goals.\n• Prop Utilization Capability. In the escape room environ-\nment, the model must maintain awareness of the items\nin its inventory and determine the appropriate moments\nto use them. Effective utilization of these props is cru-\ncial for navigating the complex environment and achiev-\ning the escape objective.\n• Long-term reasoning. Successfully escaping the room re-\nquires a prolonged sequence of interactions. The model\nmust analyze and integrate long-form text-image data\nacross multiple key interaction steps to make informed\ndecisions.\nJ. Prompt Template\nSystem Prompt\nThe System Prompt consists of two pri-\nmary components: the Instruction Prompt and the Op-\neration Prompt.\nThe Instruction Prompt provides the\nmodel with contextual information regarding the current en-\nvironment, its overarching objective, and the approach re-\nquired to achieve this objective. In contrast, the Operation\nPrompt delineates, in precise detail, the permissible actions\nand exploratory methods that the model can employ within\nthe environment. Additionally, it specifies the format and\nstructure of the structured data that the model is expected\nto generate in response. The complete prompt is shown in\ntable 11.\nStep Prompt\nThe Step Prompt is designed to provide\nfeedback to the model regarding the outcome of its previ-\nous interaction with the environment (if an interaction was\nattempted). Simultaneously, it informs the model in real-\ntime about the items currently available in its inventory for\npotential use. Additionally, the prompt serves as a direc-\ntive, encouraging the model to continue exploration or en-\ngage in further interactions.The complete prompt is shown\nin table 12.\nDebriefing Prompt\nThe Story Recovery Prompt is used\nto guide the model to recall and infer the background and\nstory of the entire game based on the interaction records af-\nter the model successfully escapes the room. The model is\nguided to describe the room environment, recall the items\nthat may contain information or clues, and finally piece to-\ngether the whole story to complete the story recovery. The\ncomplete prompt is shown in table 13.\nInstruction Prompt\nYou find yourself locked inside a room, and your ultimate goal is to escape the room. i.e. the room escape game.\nYou can explore the room, interact with objects, inspect items, and resolve puzzles. If you find doors locked or uninteractable,\nyou probably need to search for keys or passwords to unlock the door when interacting with the environment. You can adopt\nthe following actions to explore the room and interact with objects:\nOperation Prompt\n- move forward: float, ranged between [-10, 10]. This is the number of meters you want to move forward (negative value\nmeans moving backward).\n- rotate right: float, ranged between [-180, 180]. This is the number of degrees you want to turn right (negative value means\nturn left).\n- rotate down: float, ranged between [-90, 90]. This is the angle you want to adjust your view vertically. Positive value means\nlooking downward, while a negative value means looking upward. Angle 0 means looking straight ahead.\n- jump: bool, whether you want to jump (can be used together with moving forward), e.g., True represents the action ”to\njump”.\n- look at: list[x: foat, y: float], the range of x and y is [0, 1]. This parameter is the coordinates of the point in the image you\nwant to look at. For reference, the coordinates of the upper left corner of the scene are (0, 0) and the coordinates of the lower\nright corner are (1, 1). Also to mention that there are on clues on the ceiling.\n- grab: bool, whether you require to interact with the object located exactly at the center of the scene (marked by a red dot).\ne.g., to grab the key or to interact with (or open) a box at the center of the scene, set grab=True. The red dot assists in locating\nthe object you require to interact with. You might need to adjust the view or move closer to ensure the red dot is on your\ntarget object, through the rotate right, rotate down, and move forward actions. To successfully grab an object, you should\ncenter the object via the red dot and be in a certain distance to it. If the grabbing fails, try move closer towards the object.\nIf it fails multiple times at the same position, you should be aware that not all objects are interactable, do not get stucked in\nuninteractable position.\n- interactions : dict:{”use item id”: str, this is the item id you require to view or use (when used together with grab=True, it\nmeans to use this item to interact with the target object you want to grab, e.g. using item id of the key to open the door in the\nscene), ”input”: str, this is the message you want to input when interacting with the center object}.\n- read: str, this is the item id that you want to get detailed information from your bag.\n- rationale: str, represents the rationale of your action. This should explain your decision-making process and help the agent\nunderstand your thinking process.\nYou need to return data in the following format of JSON string to interact with the scene:\n{\n‘‘move forward’’: float,\n‘‘rotate right’’: float,\n‘‘rotate down’’: float,\n‘‘jump’’: bool,\n‘‘look at’’: [x: float, y: float],\n‘‘grab’’: bool,\n‘‘interactions’’: {\n‘‘use item id’’: str,\n‘‘input’’: str\n},\n‘‘read’’: str,\n‘‘rationale’’: str\n}\nAll of the above operations are optional. If no value is passed in, the interactive operation will not be performed.\nYou must follow the above instructions and don’t say anything else except for the JSON string of operations.\nTable 11. The System Prompt\nInteraction Result\n{interaction result}\n===\nInventory\nThe items in your bag usable include:\n{bag desc}\n===\nStep Prompt\nPlease determine the next action(s) that could help you ob-\nserve the room or obtain useful tools or clues.\nIf you find yourself stuck in a corner, try turn around by\npassing rotate right.\nYou need to return data in the following format of\nJSON string to interact with the scene and don’t say any-\nthing else:\n{\n‘‘move forward’’: float,\n‘‘rotate right’’: float,\n‘‘rotate down’’: float,\n‘‘jump’’: bool,\n‘‘look at’’: [x: float, y: float],\n‘‘grab’’: bool,\n‘‘interactions’’: {\n‘‘use item id’’: str,\n‘‘input’’: str\n},\n‘‘read’’: str,\n‘‘rationale’’: str\n}\nTable 12. The Step Prompt\nStory Recovery\nYou have successfully escaped the room. Now, reconstruct\nthe entire story based on the items you discovered during the\ngame and the overall environment you observed. Follow the\nsteps below to guide your recollection and piece together\nthe full narrative.\nDescribe the room environment\nStep 1: Describe the room environment ”Begin by describ-\ning the room where you started. What did the room look\nlike? What was the overall atmosphere? Were there any\nnotable features, such as furniture, lighting, or strange ob-\njects? Include sensory details like smells, sounds, and the\narrangement of the room. This will help set the scene for\nthe story.”\nRecall the items that may contain\nStep 2: Recall the items that may contain information or\nclues ”Think back to the objects you found throughout the\ngame. What items did you come across? Were any of them\nunusual or seemed important? These could include physical\nitems like keys, notes, or devices, or even abstract clues like\nsymbols or markings on the wall. Reflect on how each item\nmight have connected to the next step in your escape.”\nPiece together the whole story\nStep 3: Piece together the whole story ”Now, use the in-\nformation from the room description and the items you’ve\nfound to piece together the full story. What was the purpose\nof the room? Who or what might have created the escape\nchallenge, and why? What was the sequence of events that\nled you to the escape? Try to connect the dots between the\nenvironment, the clues, and the items you encountered, and\nreconstruct the narrative from start to finish.”\nTable 13. The Story Recovery Prompt\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game.pdf"}
{"title":"From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons","authors":"Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, Alexander Toshev","summary":"We examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches.","url":"http:\/\/arxiv.org\/abs\/2412.08442v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.08442v1","published":1733929585000,"comment":null,"pdf_text":"From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons\nAndrew Szot*1,2\nBogdan Mazoure∗1\nOmar Attia1\nAleksei Timofeev1\nHarsh Agrawal1\nDevon Hjelm1\nZhe Gan1\nZsolt Kira2\nAlexander Toshev1\n1 Apple, 2 Georgia Tech\na.szot@apple.com, toshev@apple.com\nAbstract\nWe examine the capability of Multimodal Large Language\nModels (MLLMs) to tackle diverse domains that extend be-\nyond the traditional language and vision tasks these models\nare typically trained on. Specifically, our focus lies in areas\nsuch as Embodied AI, Games, UI Control, and Planning.\nTo this end, we introduce a process of adapting an MLLM\nto a Generalist Embodied Agent (GEA). GEA is a single\nunified model capable of grounding itself across these var-\nied domains through a multi-embodiment action tokenizer.\nGEA is trained with supervised learning on a large dataset\nof embodied experiences and with online RL in interactive\nsimulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal\nthe importance of training with cross-domain data and on-\nline RL for building generalist agents. The final GEA model\nachieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist\nmodels and benchmark-specific approaches.\n1. Introduction\nFoundation Models have demonstrated broad capabilities\nacross language and image understanding tasks [5, 16, 30,\n33, 42, 43, 45, 46, 51, 58, 66, 95, 103]. In particular, Mul-\ntimodal LLMs (MLLMs) – multimodal foundation models\ntrained on vast amounts of textual and image data – excel at\ntasks that are natural to their text and image training modal-\nities. As an extension of MLLMs, Vision-Language-Action\nmodels have been successfully applied in robotics and em-\nbodied AI [3, 11, 18, 80], as well as agents for the web\n[27, 37, 75, 102] and user interface (UI) control [28, 68, 89].\nThese applications have demonstrated that MLLMs can\nbe successfully applied to diverse domains for the purpose\nof controlling various embodiments like robots, playing\ngames, and controlling devices via UIs. As many of these\ndomains share similarities, it is natural to ask how a single\n*Core contributor\nManipulation\nPlanning\nVideo Games\nMLLM\nGEA-Base\nGEA\nPerformance on Mobile Manipulation Task \n0%\n57%\n83%\nSFT\nOnline RL\nUI Control\nNavigation\nSuccess Rate\nSet an alarm for 6am\nDestroy space invaders\nCollect the gold coin\nPickup the gray key\nPut an apple in the drawer\nHammer the nail\nMove the slider left\nPick the orange\nFigure 1. The Generalist Embodied Agent (GEA) is a multimodal\nLLM-based agent that can complete tasks from natural language\ninstructions across a variety of domains and embodiments span-\nning manipulation, planning, game playing, and UI control. A\npretrained MLLM is finetuned with supervised finetuning (SFT)\non a large dataset of embodied experiences. The final GEA model\nis then finetuned with reinforcement learning (RL). GEA achieves\ncompetitive results in generalization to unseen settings.\nagent can be trained to be generally proficient in all of these\ndomains. This is a challenging problem as many of these\ntasks require physics and geometric reasoning, their embod-\niments are either static or share morphologies via a mobile\nmanipulator, their applications require long-horizon plan-\nning, and many are partially observable and require reason-\ning over long sequences of observations. In addition, train-\ning with combined data across domains with these similar-\nities may yield cross-domain benefits, where a single agent\nmay outperform agents trained on individual domains.\nIn this work, we demonstrate an approach for adapting\nan MLLM into a single Generalist Embodied Agent (GEA)\nto solve a vast number of tasks across diverse domains span-\nning manipulation, navigation, video game playing, and UI\ncontrol. To enable GEA to control diverse embodiments,\n1\narXiv:2412.08442v1  [cs.LG]  11 Dec 2024\nwe learn a unified learned tokenization mechanism across\nall continuous and discrete action spaces. As Figure 1 il-\nlustrates, we then employ supervised finetuning (SFT) [87]\nto adapt a pretrained MLLM to predict actions from trajec-\ntories of agents successfully completing tasks. This SFT\ndataset spans over 2.2 million trajectories from diverse col-\nlection methods like human labelers or learned policies.\nWhile this SFT process produces a capable agent, it suffers\nfrom an inherent lack of data, specifically data diversity, and\nrarely exhibits robustness to mistakes. We thus also train\nGEA with a second stage of online reinforcement learning\n(RL) training over a subset of the domains where the agent\ncollects and learns from data in interactive simulators.\nWe demonstrate that GEA exhibits strong generalist\ncapabilities. Specifically, it reaches state-of-the-art perfor-\nmance across many benchmarks against other generalist\nagents and even outperforms or closely matches bespoke\nspecialist systems. For example, in the CALVIN manipula-\ntion benchmark [60], GEA reaches 90% success rate while\noperating on unseen instructions and background, which is\nnearly 10% higher than similar methods [48] and closely\nmatches the performance of specialist systems [35]. In a\nHabitat mobile pick task [78], GEA achieves 83% success\nin unseen scenes, outperforming a policy trained with RL\non the ground truth simulator state. Similarly, in Procgen\nvideo games [15] GEA reaches 44% of expert score, which\nis almost 20% higher than prior specialist [59] models.\nWe also analyze the relationship between the generalist\ncapabilities of GEA and its training data and base MLLM.\nWe demonstrate that training on the combined data from\na diverse set of domains for SFT provides a cross-domain\nperformance boost over using only per-domain data. Fi-\nnally, we explore the role of RL and online data collection\nfor building generalist agents and empirically demonstrate\nthe benefits of online RL over prior approaches of iterative\nSFT or offline RL.\nAs a further contribution to the community, we will re-\nlease the code for training and evaluating GEA along with\nthe GEA model itself. We will add the link to the code and\nmodel to this paper when they are ready for release.\n2. Related Work\nPrior works have explored building generalist agents by\ntraining policies on large multi-task datasets, illustrating\nthe importance of scaling interactive data to create capable\nmulti-task agents [20, 69, 88]. Additionally, prior works\nhave studied new architectures for generalist agents [24,\n86], while others focus on applying generalist agents to\nrobotic contexts [9, 10, 83]. Some research also investigates\ngeneralist models in domain-specific benchmarks [34, 82]\nor in cross-embodiment scenarios [17, 65]. Like GEA, these\napproaches leverage extensive data, yet our work empha-\nsizes the importance of adapting a pretrained MLLM via\nboth finetuning and online RL. For example, differences be-\ntween GEA and Gato [69], are that GEA leverages RL, uti-\nlizes a pretrained MLLM, learns a multi-embodiment action\ntokenizer, and focuses on evaluating generalization to new\ntask settings. As a result, GEA empirically outperforms\nGato in many settings.\nLike GEA, some prior work focuses on adapting\nMLLMs as agents. Works have proposed domain-specific\npipelines for using the zero-shot capabilities of MLLMs for\ndecision-making [3, 31, 50, 85, 91, 100], while our work\nfocuses on finetuning MLLMs.\nOther works investigate\nschemes for finetuning MLLMs for decision-making and\nthe benefits of doing so, but also in the context of specific\ndomains [48, 76, 80, 81]. Prior work also finetunes MLLMs\nas generalist agents [11, 36, 63]. However, our work shows\nresults across more diverse domains and studies the impor-\ntance of supervised learning with multi-domain data and RL\nfinetuning. Architecturally, GEA is different from Open-\nVLA [36] in that it uses a learned multi-embodiment ac-\ntion tokenizer as opposed to a uniform discretization, which\nprior work has demonstrated to perform better [81]. More-\nover, works have explored the value of finetuning MLLMs\nas UI agents [4, 19, 23, 62, 97]. While GEA explores adapt-\ning MLLMs as policies, there are also other ways to lever-\nage MLLMs such as via reward models [55, 56], world\nmodels [90], or environment generation [94].\nMore broadly, prior work has demonstrated how LLMs\ncan be used for agents that can reason and interact.\nVarious works focus on training LLM agents through\nspecific pipelines [101]. Similar to how GEA finetunes for\ndecision-making capabilities not present in the base LLM,\nother works demonstrate the value of finetuning LLMs for\nreasoning and problem-solving capabilities [77, 92]. Prior\nworks also demonstrate the value of finetuning LLMs with\nself-generated data [54], mistakes from the LLM [74], and\nwith RL [39].\nLikewise, GEA shows the importance of\nusing such RL training, beyond just supervised learning, to\ncreate a capable agent.\n3. Generalist Embodied Agent\n3.1. Problem Settings\nWe focus on language-specified tasks with visual observa-\ntions. Specifically, we consider the goal-specified Partially-\nObservable Markov Decision Processes (POMDPs) [8] with\nobservation space O, action space A, goal space G, and a\nreward model R. For brevity, we omit other elements of\nthe MDP. In our settings, G is represented by a textual de-\nscription of the task to solve. Observations consist of RGB\nimages from the agent, which can either be from an agent\ncamera in the case of embodied AI applications or screen-\nshots in the case of video games or UI interactions.\nWe consider a diverse set of environment types, which\n2\nVisual Encoder\nLLM\n￼k1\nt\n￼k2\nt\n￼kM\nt\nAction Tokens\nMulti-Embodiment Action \nDe-Tokenizer\nAction for \nEnvironment\n￼k1\nt\n￼kM−1\nt\nJoint velocity\nDelta joint position\nEnd-effector\nJump, left, …\nForward, left, …\nTap 23 47\nDiscrete \nControl\nContinuous \nControl\nUnified Token \nOutput Space\nVisual Bridge\nAgent: Fetch mobile robot. Actions: delta \njoint control… Instruction: pick an apple\n(Prompt)\n(Instruction)\nObservation History\nStatic Manipulation\nMobile Manipulation\nStatic Manipulation\nUI Control\nNavigation\nVideo Games\ndx dy dz \n[28,\n73]\n “move left”\n[278, 276]\nLLM Tokenizer\nResidual VQ-VAE Encoder\nTruncate for \nenvironment\n￼at\n￼ot\nGEA Component\nFigure 2. GEA utilizes a pretrained MLLM together with a multi-embodiment action tokenizer to enable a generalist agent to operate across\na wide range of domains, embodiments, and action spaces. GEA takes as input information about the embodiment and desired task with the\nembodiment prompt and instruction and the observation visuals (bottom). It produces a sequence of action tokens in the LLM vocabulary,\nwhich are decoded by the multi-embodiment action detokenizer into an action for the appropriate embodiment and action space.\nwe refer to as domains (see Table 1 for examples). These\ndomains specify a diverse set of action spaces spanning var-\nious robotic control spaces, high-level primitives, and com-\nputer UI interfaces. Our goal is to learn one policy that\noperates over a number of environments, which we denote\nby Mi = (Oi, Ai, Gi, Ri) for each environment i ∈E.\n3.2. GEA Architecture\nThe Generalist Embodied Agent (GEA) performs tasks by\nproducing actions that are executed in an environment con-\nditioned on observations, past actions, and a task descrip-\ntion. More formally, for timestep t in environment Mi, the\nGEA model takes as input an environment specific prompt\nPi, followed by a task instruction I ∈Gi and up to c inter-\nleaved previous observations and actions ot−c, at−c, . . . , ot\nfrom Oi and Ai. From these inputs, the GEA model pre-\ndicts action at ∈Ai to execute in the environment. The\nprompt provides information about the environment and\nembodiment to control. The task instruction is a natural\nlanguage description of the task the agent is to execute.\nMulti-Embodiment Action Tokenizer. We study Gen-\neralist Embodied Agent (GEA) adapted from an MLLM. As\nMLLMs naturally consume only text and images and gen-\nerate only text, we follow the findings of Szot et al. [81] to\nmodify the LLM vocabulary to represent actions. First, we\nrepresent all actions across {Mi}, i ∈E with two vocab-\nularies: Vdisc for discrete actions and Vcont for continuous\nactions, so that the final vocabulary is V = Vdisc ∪Vcont.\nA discrete action is described by language, and then this\nlanguage is tokenized into a sequence of text tokens repre-\nsenting this action. Vdisc is defined as all such text token\nsequences representing the discrete actions.\nAs continuous actions are not readily expressed as text,\nwe use a learned action tokenizer that maps each continuous\naction into a sequence of new tokens, whose vocabulary we\ndenote by Vcont. Details of how we train this tokenizer are in\nSection 4.2. We replace the |Vcont| most infrequently used\ntokens in the original LLM vocabulary with Vcont.\n4. Training\nGEA starts from a base MLLM and first trains a continu-\nous action tokenizer. As depicted in Figure 3, the MLLM\nis adapted to GEA-Base by supervised finetuning on em-\nbodied experiences. Next, GEA-Base is adapted to the full\nGEA model through supervised and reinforcement learning.\n4.1. Base MLLM\nThe primary consideration for selecting a base model be-\nyond its inherent vision-language strength is its ability to\nscale to long contexts as embodied data consists of long tra-\njectories of interleaved observations and actions. We thus\nbuild GEA off LLaVA-OneVision [44], a model specifically\ntrained to handle sequences of images through interleaved\nimage\/text pairs and videos which extends to GEA operat-\ning over a history of observations.\n4.2. Continuous Multi-Embodiment Tokenizer\nTo obtain a vocabulary Vcont for continuous actions and\na tokenizer\/de-tokenizer for these actions, we follow Szot\n3\net al. [81] and train a Residual VQ-VAE (RVQ) [40] model\nover action vectors. The RVQ model is a variational au-\ntoencoder that leverages a sequence of discrete embeddings\nto represent the data. Specifically, it encodes an action a\nas a sequence of M tokens k1(a), . . . , kM(a), where each\ntoken denotes a code from a learned vocabulary V m\ncont, for\nm ∈1, . . . , M.\nA key feature of RVQ is that the mth\nvocabulary is trained to encode the residual of the action\nafter it has been encoded with vocabularies 1, . . . , m −1.\nThis hierarchical encoding makes RVQ effective in pre-\ncisely representing continuous actions with a minimal num-\nber of discrete tokens. The final continuous action vocab-\nulary used by GEA is the union of the RVQ vocabularies\nVcont = S\nm V m\ncont.\nThe main distinction from Szot et al. [81] is that we train\na single tokenizer\/de-tokenizer across all continuous action\nspaces. As shown in Table 1, these spaces cover a variety\nof robotic control types, including end-effector, joint veloc-\nity, and joint position control. To facilitate training a unified\nRVQ, we pad all action vectors to the maximum action di-\nmension. During inference, we decode the predicted action\ntokens and then truncate the output to match the dimension-\nality of the specific embodiment’s action space. We use 2\ncodebooks each with 512 tokens and a token vector dimen-\nsion of 1024. Additional details on action tokenization are\nin Appendix A.\n4.3. Stage 1: Supervised-Instruction Finetuning\nThe first step of GEA is to use supervised-instruction\nfinetuning (SFT) to adapt the base MLLM for embodied\ndecision-making (left side of Fig. 3). We use a collection\nD = S\ni∈E Di of demonstration datasets from all environ-\nments E. During this stage, we use a standard cross-entropy\nloss over actions in the case of interactive data or responses\nin the case of vision-language data. As is typically done in\nMLLM training, we maximize the negative log-likelihood\nof predicting these output tokens for each example:\nLSFT(D) = −\nX\n(I,ot−c:t,at−c:t)∈D\nlog p(at|P, I, ot−c, at−c, . . . , ot) (1)\nWe then train the base MLLM over all the above datasets\nwith SFT to obtain the GEA-Base model.\nTraining Details. The entire GEA-Base model is ini-\ntialized from the base MLLM. We train for 75k updates us-\ning AdamW [53] with cosine learning rate decay and lin-\near learning rate warmup for the first 10% of training steps;\nlearning rate of 1e−5; global batch size of 256 and an ob-\nservation context length of c = 3. Training takes around 2\ndays on 8 nodes of 8xH100 GPUs (see Appendix B).\n4.4. Stage 2: Online Reinforcement Learning\nWhile the previously described SFT training process pro-\nduces a capable GEA-Base agent, it is only trained on a\nlimited set of expert trajectories, which rarely demonstrate\nPretrained \nMLLM\nGEA-Base\nSFT on interactive data\nGEA\nSFT on interactive data\nOnline RL in simulation\nTrain entire \nMLLM\nLoRA finetune LLM\nFigure 3. GEA training stages. First, a MLLM is adapted to GEA-\nBase by finetuning the entire MLLM with SFT on interactive data.\nNext, GEA-Base is finetuned jointly with online RL (PPO) and\nSFT on the original data with LoRA.\ndiverse behaviors like error recovery. Hence, we propose\nto utilize online RL for some of the environments. In a\nsecond stage of training, we continue to train GEA-Base\nwith RL in addition to SFT to obtain the final GEA model\n(right side of Fig. 3). For online RL, we train the GEA-Base\nagent with PPO [72], whose optimization loss is denoted by\nLPPO(Mi) for each environment in which we have a simu-\nlator i ∈EPPO ⊂E. We combine this objective with the SFT\nobjective from Eq. (1) to obtain the final GEA objective:\nLGEA =\nX\ni∈EPPO\nLPPO(Mi) + λ\nX\ni∈E\nLSFT(Di)\n(2)\nwhere we weight the SFT loss by λ = 0.1 to emphasize\nthe RL loss. This RL+SFT training stage on the GEA-Base\nmodel produces the final model we refer to as GEA.\nPPO Details. The GEA value function for RL is an MLP\nnetwork which is initialized from scratch. It takes as input\nthe MLLM final layer activations at the observation token\nstep just before the action tokens and an average pooled\nrepresentation of the visual embeddings from the MLLM\nvision encoder. The critic also optionally takes any privi-\nleged state information about the task since the critic is only\nused during training, and not during inference.\nTo stabilize RL training across numerous environments,\nwe use PopArt [84] return normalization to account for\nthe diverse reward distributions across these environments.\nSince the output space of the LLM can consist of many pos-\nsible tokens, we use constrained decoding to force the au-\ntoregressive action sampling to be within the action space\nfor the environment.\nFor continuous action tasks, this\namounts to constraining the output to the learned contin-\nuous action tokens.\nFor the discrete control tasks this\namounts to constraining the output to the valid language ac-\ntions (for example, ”pick apple” or ”right”). To account for\ndifferences in the valid distribution of actions per environ-\nment, we normalize the entropy for PPO so a single entropy\ncoefficient can apply to all tasks.\nTraining Details.\nSince GEA-Base already obtains\nsome success, and RL introduces GPU memory overhead\nvia environments simulated on the GPU, we use LoRA [29]\nto finetune the LLM while freezing all other components.\nAll environments use a rollout length of 128, a learning rate\n4\nDataset Name\nDomain\nAction Type\nEmbodiment Type\n# Trajs\nData Source\nOpenX [63]\nStatic Manip\nCont. Various\n22 Various Robots\n1.2M\nVarious\nMeta-World [98]\nStatic Manip.\nCont. EE+Gripper\nSawyer\n45k\nScripted\nCALVIN [60]\nStatic Manip.\nCont. EE+Gripper\nFranka Arm\n18k\nHuman\nManiskill [22]\nStatic Manip.\nCont. Joint Velocity\nROKAE xMate3Pro\n5k\nMotion Planner\nHabitat Pick [78]\nMobile Manip.\nCont. Joint Position + Base\nFetch\n50k\nRL Expert\nHabitat Place [78]\nMobile Manip.\nCont. Joint Position+Base\nFetch\n50k\nRL Expert\nHabitat Nav [78]\nNavigation\nCont. Velocity\nFetch\n13k\nShortest Path\nBabyAI [14]\nNavigation\nDiscrete\nVirtual\n50k\nShortest Path\nLangR [80]\nPlanning\nDiscrete\nFetch\n150k\nRL Expert\nProcgen [15]\nVideo Games\nDiscrete\nVirtual\n320k\nRL Expert\nAtari [7]\nVideo Games\nDiscrete\nVirtual\n286k\nRL Expert\nAndroidControl [47]\nUI Control\nMixed\nVirtual\n14k\nHuman\nTable 1. Overview of the embodied datasets used for training GEA. The actions in each dataset can be either discrete or continuous with a\nspecific control space for the continuous actions. The embodiment type describes the agent being controlled. Each trajectory in a dataset\nrefers to a sequence of images and actions. The data source refers to the collection method for these trajectories.\nof 3e−4, an entropy coefficient of 1e−4, and a value func-\ntion learning loss of 1.5e−4. RL finetuning uses 8 nodes\nof 8xH100 GPUs with 4 parallel environments per GPU.\nEnvironments are partitioned per node into 3 of the GPUs\nrunning HabPick, 3 running Procgen, and 2 running LangR.\nThe SFT per-device batch size is 2. We train for 100M cu-\nmulative steps across all tasks, which takes around 1 day.\nFull RL training details are in Appendix C.\n5. Datasets and Environments\nWe use a diverse set of domains with associated environ-\nments and datasets (see Table 1). This section introduces\nthese domains followed by an explanation of how we use\nthem in stage 1 and 2 of our training procedure.\nStatic Manipulation.\nThese datasets are of a fixed\nrobot manipulator interacting with objects. Some of these\ndatasets are simulated table top interactions with rigid-body\nobjects such as Meta-World [99], CALVIN [60] and Man-\niskill [22]. We also leverage a large dataset of interactions\non real robot platforms [63]. These datasets span a vari-\nety of control spaces in end-effector and joint control. The\ncamera is typically mounted in a static position so that the\nworkspace and robot arm are fully visible.\nMobile Manipulation. We also investigate setups where\nthe robot manipulator moves via a mobile base. We use the\nobject rearrangement tasks from the Habitat platform [71]\nfor datasets in these tasks. These datasets cover object pick-\ning and placing tasks where the robot starts up to 2 meters\naway from the object. The robot has to coordinate moving\nits base and arm to successfully pick up the object. These\ndatasets involve first person egocentric cameras.\nNavigation. We also use datasets of navigation in iso-\nlation. We use datasets of simulated robot navigation in\nHabitat. We also use navigation in grid-world environments\nfrom BabyAI [14]. Both datasets were collected with short-\nest path experts.\nVideo games. We use datasets from two standard bench-\nmarks for decision making in video games, Procgen [15]\nand Atari [7]. Both datasets were collected by RL agents\nwhich were separately trained to solve each individual\ngame. We convert these tasks to be language-conditioned\nby providing the game name along with a short description\nof the game’s objective and rules. We only train on success-\nful trajectories.\nPlanning. We use a dataset of successful episodes in\nthe LangR dataset [80]. In this task the agent has to se-\nlect between skill primitives that accomplish long horizon\nlanguage-specified rearrangement tasks for a home robot.\nUI Control. We use the AndroidControl [47] dataset of\nUI interaction in Android devices spanning 833 apps. The\nactions are combinations of tap actions specified by screen\ncoordinates and text typing.\nVision language instruction data. To improve general-\nization of the model, we also include data used for train-\ning the original MLLM, which prior work has found is\nuseful when finetuning MLLMs as control policies [11].\nWe used the following datasets of text and images without\nany actions: VQAv2 [21], OKVQA [57], A-OKVQA [73],\nGQA [32] and the LLaVA-Instruct-150k dataset [52].\nStage 1 Training: SFT Data. To obtain embodied data\nfor Stage 1 Training (see Sec. 4.3), we collect a large dataset\nof language-conditioned behaviors from all of the above\ndomains consisting of 2.2M trajectories.\nAll trajectories\nare successful examples of language-conditioned behaviors\nwith visual observations. The data is from diverse collec-\ntion sources such as human demonstrations, RL-based poli-\ncies, and motion planners. The dataset is diverse and spans\nthousands of distinct tasks and many embodiments (see Ap-\n5\nGEA\nPrior Work\n# Tasks\nGeneralization Type\nManipulation\nMeta-World\n94.7\n84 MLLM+IL [81]S\n87.0 [69]G\n45\nobject positions\nCALVIN (ABC →D)\n90.0\n82.4 MLLM+IL [48]S\n92.2 IL+pointcloud[35]\n34*\ninstructions, background\nManiskill\n13.6\n6.5 IL [22]S\n47.8 IL+PPO [22]S\n5\nobject positions\nHabitat Pick\n82.5\n29 IL [81]S\n81.0 RL + sim stateS\n20\nhouse\nHabitat Place\n93.5\n95.5 RL + sim stateS\n10\nhouse\nVideo Games\nProcgen\n44.0\n25 [59]S\n16\nbackground\nAtari\n32.7\n31 [69]G\n85 Offline RL [41]S\n44\nnone\nNavigation\nHabitat Nav\n62.5\n72 [78]S\n10\nhouse\nBabyAI\n91.1\n93.2 [69]G\n17*\ninstructions, grid state\nUI Control\nAndroidControl\n57.3\n45 GPT-4o+SoM [93]G\n35*\ninstructions\nPlanning\nLangR\n50.0\n51 MLLM+RL[81]S\n10*\ninstructions, house\nTable 2. Zero-shot generalization of GEA to new tasks in terms of success rate % and in the video games tasks % of expert performance. We\ncompare against prior works consisting of domain specialists (with superscript “S”) that are trained on only data from that benchmark and\ndomain generalists (with superscript “G”) that are trained on data from several benchmarks. Bold indicates best, underline close second,\nand gray coloring that the method assumes access to additional input modalities like pointcloud or ground truth simulator state, meaning it\nis not a fair comparison to GEA. The “Prior Work” column also gives details about how the methods were trained (IL or RL) and if they\nassume additional input modalities. The “# Tasks” column gives a general indication of the number of distinct evaluation settings with a\n“*” indicating each task also has diverse language instructions.\npendix D for full details).\nStage 2 Training: RL Environments.\nFor Stage 2\nonline RL (see Sec. 4.4) we use environments from the\nthree domains of Habitat Pick [78], Language Rearrenge-\nment (LangR) [80], and Procgen [15].\nThus, we define\nEPPO = {HabPick, LangR, ProcGen}.\nHabitat Pick and\nLangR are simulated in the Habitat platform [71] and have\nreward functions for achieving and making progress to-\nwards the goal. In Procgen, we use all 16 games for RL and\nuse the game specific reward functions (see Appendix C for\nfull details).\n6. Empirical Evaluation\nWe empirically demonstrate the ability of GEA as a general-\nist agent that can generalize to new instructions and settings\nacross diverse embodiments and domains. We assess the\nrole of the RL training in achieving this goal. In ablations,\nwe study the impact of scaling data between multiple do-\nmains, compare RL to other forms of policy collected data,\nthe impact of the the base MLLM.\n6.1. GEA Generalization Capabilities\nIn this section, we evaluate the generalization capabilities\nof the final GEA model. We use the associated benchmarks\nfrom the datasets in Table 1, which span manipulation, nav-\nigation, video games, UI control, and planning.\nThese\nbenchmarks evaluate agents in new settings not present in\nthe training data, such as new object positions, scenes, vi-\nsual backgrounds, tasks, or instructions. All benchmarks\nspecify evaluation instructions in natural language and re-\nquire the agent to operate from visual observations.\nWe report the “online” performance of GEA, meaning\nwe evaluate its performance in an interactive simulator. The\nonly exception is AndroidControl, where we instead check\nthe correspondence with a ground truth test trajectory [68].\nEach benchmark also evaluates agents over many distinct\ntasks.\nFor example, in the Procgen video game bench-\nmark, we report the average Procgen performance over all\n16 Procgen games each of which is an entirely different\nvideo game. Full evaluation details are in Appendix E.\nWe seek to comprehensively frame the empirical per-\nformance of GEA relative to prior work on our evaluated\n6\nHabitat\nPick\nProcgen\nLangR\nAll Other\nGEA\n82.5\n44.0\n50.0\n70.5\nGEA-Base\n60.5\n36.1\n15.5\n69.5\nTable 3. Effect of stage 2 RL training on GEA-Base (7b model).\nTasks included in RL training increase their generalization per-\nformance. Performance on the remaining tasks slightly increases\nthanks to continued SFT training.\nbenchmarks.\nFirst, in all benchmarks, we use only im-\nages as observations without using any privileged informa-\ntion such as the simulation state or additional observations\nsuch as 3D point clouds. Second, we evaluate our single\nGEA model across all environments, which is referred to as\na generalist. Some of the approaches we compare against\nare trained on data from a single environment, and we re-\nfer to those as specialists. In other comparative approaches,\nthe split between train or test data is unclear. For exam-\nple, Gato [69] is a generalist model like GEA, yet evaluates\nand trains on some less diverse tasks with their own train-\ning datasets, which are not released. Appendix F.1 discusses\nthese connections in detail.\nTable 2 summarizes the comparative evaluation. GEA\nexcels at manipulation tasks, either exceeding or match-\ning the performance of specialist models. For example, in\nMeta-World GEA greatly outperforms both specialist and\ngeneralist models trained for this task with a 7% absolute in-\ncrease relative to the best-performing baseline. In CALVIN,\nGEA outperforms a variety of recent specialist models.\nGEA also performs closely to the specialist 3D Diffuser Ac-\ntor method [35], which uses a manipulation-specific action\nrepresentation of end-effector key points and uses a depth\ncamera to represent the scene as a 3D feature cloud. GEA\nonly uses the third PoV RGB camera and does not use an\naction or observation space specific to table-top manipula-\ntion. GEA outperforms the baseline in Habitat Pick and\nclosely matches it in Habitat Place despite these baselines\nbeing trained with the ground truth simulator state. In Man-\niskill, the difficult, often occluded, camera view results in\nlow overall success rates, yet GEA outperforms other re-\nsults that only use IL. However, GEA is outperformed by\nmethods that use RL in this benchmark.\nIn video game benchmarks, GEA outperforms the spe-\ncialist model baseline in Procgen. In Atari, GEA outper-\nforms the generalist Gato model [69]. However, in Atari,\nGEA is outperformed by Multi-Game DT [41], which uses\noffline RL from suboptimal demonstrations. In Atari, GEA\ndoes not learn with offline or online RL.\nIn the BabyAI navigation benchmark, GEA is close in\nperformance to Gato [69] despite GEA using RGB ren-\nderings of the top-down view rather than any underlying\nstate information and 100x fewer demonstrations for this\nHabitat\nPick\nCALVIN Procgen\nAndroid\nControl\nBabyAI\nGEA-Base\n57.0\n48.0\n24.5\n50.5\n84.7\nDomain Specific\n54.5\n35.5\n23.7\n48.9\n82.1\nOnly LLM\n9.5\n0.0\n7.6\n26.4\n49.4\nOnly VisEncoder\n34.5\n13.0\n24.5\n28.3\n70.6\nNone\n9.0\n0.0\n7.4\n14.1\n44.4\nTable 4. We present results using LLaVA-OneVision-500m as the\nMLLM base (row 1) as well as training with only domain specific\ndata (row 2). We also present results for a transformer of the same\narchitecture but only the LLM subnet is initialized (row 3), or the\nvisual encoder (row 4), or neither (row 5).\nenvironment. In Habitat Nav, GEA underperforms an RL-\ntrained expert. This gap could be due to the context of GEA\nonly consisting of the previous three observations, which\ncould limit its ability in partially observable settings. In\nUI control, another discrete action task, GEA outperforms\nGPT-4o [64] with set of marks [93] generated by a UI detec-\ntion model. This demonstrates that GEA benefits from be-\ning specialized for interactive decision-making even against\na powerful LLM and specialized perception system. Fi-\nnally, in the discrete control benchmark of the LangR plan-\nning task, GEA closely matches the performance of a spe-\ncialist baseline, which trains on only this task with RL.\nComparative Benefit of RL to SFT. Training with\nRL was important to achieving these strong results. Ta-\nble 3 compares the performance of GEA-Base, which is\nonly trained with SFT on expert demonstrations, and GEA,\nwhich is additionally trained with a second stage of RL. The\nresults show that the success rate of GEA greatly increases\non Habitat Pick, Procgen, and LangR, which are the envi-\nronments where RL was used. Furthermore, the average\nsuccess rate across all the other tasks is not influenced by\nRL due to the continued joint SFT training.\n6.2. GEA Training and Model Analysis\nIn this section, we explore the relationship between the gen-\neralist capabilities of GEA and its training data. We analyze\nthe role of embodied SFT data for adapting the MLLM for\ninteraction and the importance of online data. We also eval-\nuate the importance of the base MLLM. For all results in\nthis section, we train all models with 32% of the original\nembodied SFT data and 40k updates to ease the computa-\ntional burden of the analysis. We also evaluate on the tasks\nfrom Table 2 with a reduced number of evaluation episodes\nwith around 200 episodes per benchmark. Appendix F.2 de-\nscribes this analysis evaluation and dataset in detail.\nImpact of Multi-Domain Data. We evaluate training a\ngeneralist model on data from all diverse domains, versus\ntraining a model only on data from the particular target do-\nmain. Specifically, we train the smaller LLaVA-OneVision-\n500m model on either data from a single domain (“Domain\n7\nMLLM\nSuccess SFT\nMLLM\nOffline RL\nMLLM\nOnline RL\nGEA-Base\nGEA-Base\nSuccess SFT\nGEA-Base\nOffline RL\nGEA-Base\nOnline RL\n0\n20\n40\n60\n80\nSuccess (%)\n17\n23\n41\n57\n46\n48\n83\nFigure 4. Online learning in Habitat Pick. MLLM methods fine-\ntune LLaVA-OV while other methods finetune GEA-Base.\nSpecific”) or data across all domains (“GEA-Base ”). Com-\nparing these options in Table 4 shows that across all the\nbenchmarks, training with all the data is beneficial. How-\never, the gain is smaller in some of the domains like An-\ndroid Control and Procgen, likely due to less overlap with\nthe other training domains as opposed to the wealth of ma-\nnipulation data we train with.\nAppendix G.4 contains a\nmore granular investigation into the pairwise transferabil-\nity between each dataset and also supports this conclusion\nthat GEA benefits from multi-domain data.\nImpact of Policy-Collected Data.\nNext, we explore\nthe role of learning from data sources beyond SFT on ex-\npert demonstrations. While GEA-Base is a capable embod-\nied policy, it is only trained on successful demonstrations.\nThese demonstrations rarely exhibit recovery behaviors or\nrobustness to non-expert behaviors. Unlike typical MLLM\napplications such as visual question answering, in interac-\ntive tasks, agents trained with expert data can suffer from\nthe problem of “covariate shift” where small agent errors\ncause the observation distribution to shift from the expert’s\nand for errors to compound [70].\nWe analyze how GEA-Base can be trained with addi-\ntional data to improve performance in the Habitat Pick task\nand compare the following alternatives. First, GEA-Base\nSuccess SFT collects 10k successful examples in the en-\nvironment with the GEA-Base policy and then trains on\nthese successes with supervised learning. GEA-Base Offline\nRL collects 10k trajectories consisting of both successes\nand failures, both labeled with the dense Habitat Pick re-\nward, and then trains on these with the IQL offline-RL algo-\nrithm [38]. GEA Online RL finetunes GEA-Base with PPO,\nwhich leverages online interactions with the simulator like\nthe GEA Stage-2 training (but omits the joint SFT loss). We\nagain use the smaller base LLaVA-OneVision-500m model\nfor these experiments. Figure 4 shows the results of these\nvariations.\nA main takeaway message is the strong impact of on-\nline RL on top of a finetuned MLLM, which increases the\nsuccess of GEA-Base from 57% to 83%, despite the lat-\nter being trained on 50k successful Habitat Pick demonstra-\ntions. Online RL outperforms both Success SFT and IQL\nLangR\nProcgen\nAndroid Control\nHabitat Pick\nCALVIN\nBabyAI\n0\n20\n40\n60\n80\nSuccess (%)\nLLaVA-OV-500m\nLLaVA-OV-7b\nLLaVA-1.5-7b\nMM1.5-1.2b\nMM1.5-6.4b\nFigure 5. Analyzing the impact of training GEA with different\nbase MLLMs with different parameter counts.\noffline RL, highlighting the need for online interactions. It\nis worth noting that applying success SFT and offline RL\non top of GEA-Base decreases the model’s performance,\nwhich could be due to the lack of diverse data.\nThese results further show that online RL is beneficial\nwhen applied on top of a finetuned model with domain\ndata. Online RL alone is unable to bring the performance\nof the base MLLM to GEA-Base without the stage 1 SFT.\nAppendix G.2 explores this further and shows that GEA-\nBase is also far more sample efficient with RL than the base\nMLLM. This analysis demonstrates it is important for the\nGEA model to use RL in combination with SFT.\nImpact of pretrained MLLM. We assess the impor-\ntance of the pretrained MLLM in the GEA architecture. To\ndo so, we present results using a base model that has an ar-\nchitecture identical to the pretrained MLLM. However, in-\nstead of initializing the full model with LLaVA-OneVision\nwe initialize only the LLM or the visual encoder with the\ncorresponding subnet weights.\nThe results in Table 4 show that the full MLLM has a\nsubstantial impact on performance. Although this isn’t sur-\nprising, it is important to note that the visual encoder ini-\ntialization seems to have a stronger impact on the final per-\nformance compared to the LLM. We conjecture that this is\nbecause the benchmarks require visual generalization, and\ntraining the LLaVA-OneVision SigLIP visual encoder from\nscratch with only the embodied data is challenging.\nFurther, we analyze the impact of the base MLLM size\non the GEA-Base. We use two classes of backbone mod-\nels, LLaVA-OneVision [44] and MM1.5 [58], and use two\nsizes for each of them (0.5B and 7B for the former, and\n1.2B and 6.4B for the latter). As shown in Figure 5, in\nour agentic applications increasing model capacity leads\nto stronger performance, both within and also across\nbackbone model class. Interestingly, the class of models\nhas little effect as we see very similar performance across\nmultiple backbone models. The three different 7B models\nhave been trained on different web-scale data. Nevertheless,\ntheir difference has a negligible impact on GEA.\nAdditionally, in Appendix G.3, we show that GEA train-\ning and evaluation is robust to the random seed.\n8\n7. Conclusion\nIn this work, we studied how finetuning pretrained MLLMs\nwith large-scale embodied experience via expert trajecto-\nries and online RL unlocks their ability to act as Generalist\nEmbodied Agent. To interface with diverse embodiments,\nGEA uses a learned action tokenizer. We illustrate the im-\nportance of RL finetuning for GEA, which results in com-\npetitive results across a variety of domains spanning manip-\nulation, video games, navigation, UI control, and planning.\nWhile GEA demonstrates impressive abilities across a\nwide diversity of tasks, it is still not at the level of a foun-\ndation model for decision-making like similar models for\nlanguage and vision [1]. GEA cannot control arbitrary em-\nbodiments and operate in arbitrary environments zero-shot.\nFurthermore, the performance of GEA in some domains,\nsuch as Maniskill, Atari, and AndroidControl, is far from\nperfect. Extending RL to these environments could be a\nsolution. Future work can continue to scale GEA to more\ntasks to extend its generalist capabilities.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 9\n[2] Rishabh Agarwal, Dale Schuurmans, and Mohammad\nNorouzi.\nAn optimistic perspective on offline reinforce-\nment learning.\nIn International conference on machine\nlearning, pages 104–114. PMLR, 2020. 14\n[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,\net al.\nDo as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691,\n2022. 1, 2\n[4] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr,\nSergey Levine, and Aviral Kumar. Digirl: Training in-the-\nwild device-control agents with autonomous reinforcement\nlearning. arXiv preprint arXiv:2406.11896, 2024. 2\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. arXiv preprint arXiv:2308.12966,\n2023. 1\n[6] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling.\nThe arcade learning environment: An evaluation platform\nfor general agents.\nJournal of Artificial Intelligence Re-\nsearch, 47:253–279, 2013. 16\n[7] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael\nBowling. The arcade learning environment: An evaluation\nplatform for general agents.\nJournal of Artificial Intelli-\ngence Research, 47:253–279, 2013. 5\n[8] Richard Bellman. A markovian decision process. Indiana\nUniv. Math. J., 6:679–684, 1957. 2\n[9] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao,\nColine Devin, Alex X Lee, Maria Bauza, Todor Davchev,\nYuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: A\nself-improving foundation agent for robotic manipulation.\narXiv preprint arXiv:2306.11706, 2023. 2\n[10] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine\nHsu, et al. Rt-1: Robotics transformer for real-world con-\ntrol at scale. arXiv preprint arXiv:2212.06817, 2022. 2\n[11] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:\nVision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint arXiv:2307.15818, 2023. 1,\n2, 5\n[12] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada,\nSaurabh Kumar, and Marc G. Bellemare. Dopamine: A\nResearch Framework for Deep Reinforcement Learning.\n2018. 16\n[13] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\nAditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforce-\nment learning via sequence modeling. Advances in neural\ninformation processing systems, 34:15084–15097, 2021. 17\n[14] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem\nLahlou, Lucas Willems, Chitwan Saharia, Thien Huu\nNguyen, and Yoshua Bengio. BabyAI: First steps towards\ngrounded language learning with a human in the loop. In\nICLR, 2019. 5, 15, 16\n[15] Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schul-\nman. Leveraging procedural generation to benchmark rein-\nforcement learning. In International conference on machine\nlearning, pages 2048–2056. PMLR, 2020. 2, 5, 6, 14, 16,\n17\n[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 1\n[17] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and\nSergey Levine. Scaling cross-embodied learning: One pol-\nicy for manipulation, navigation, locomotion and aviation.\narXiv preprint arXiv:2408.11812, 2024. 2\n[18] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-\nE: An embodied multimodal language model.\narXiv\npreprint arXiv:2303.03378, 2023. 1\n[19] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Mat-\nsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin\nGur. Multimodal web navigation with instruction-finetuned\nfoundation models.\narXiv preprint arXiv:2305.11854,\n2023. 2\n[20] Quentin Gallou´edec, Edward Beeching, Cl´ement Romac,\nand Emmanuel Dellandr´ea. Jack of all trades, master of\nsome, a multi-purpose transformer agent. arXiv preprint\narXiv:2402.09844, 2024. 2\n9\n[21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the v in vqa matter: El-\nevating the role of image understanding in visual ques-\ntion answering. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 6904–6913,\n2017. 5\n[22] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang\nLiu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei,\nYunchao Yao, et al.\nManiskill2:\nA unified bench-\nmark for generalizable manipulation skills. arXiv preprint\narXiv:2302.04659, 2023. 5, 6, 15, 16, 17\n[23] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksandra\nFaust.\nA real-world webagent with planning, long con-\ntext understanding, and program synthesis. arXiv preprint\narXiv:2307.12856, 2023. 2\n[24] Siddhant Haldar, Zhuoran Peng, and Lerrel Pinto. Baku:\nAn efficient transformer for multi-task policy learning.\narXiv preprint arXiv:2406.07539, 2024. 2, 16\n[25] Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2:\nScalable, robust world models for continuous control. arXiv\npreprint arXiv:2310.16828, 2023. 17\n[26] Abhinav Narayan Harish, Larry Heck, Josiah P Hanna,\nZsolt Kira, and Andrew Szot. Reinforcement learning via\nauxiliary task distillation. In European Conference on Com-\nputer Vision, pages 214–230. Springer, 2025. 14, 16, 17\n[27] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong\nDai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. We-\nbvoyager: Building an end-to-end web agent with large\nmultimodal models.\narXiv preprint arXiv:2401.13919,\n2024. 1\n[28] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\nWenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, et al.\nCogagent: A visual language\nmodel for gui agents.\nIn Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\npages 14281–14290, 2024. 1\n[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4\n[30] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Barun Patra, Qiang Liu, Kriti\nAggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary,\nSubhojit Som, Xia Song, and Furu Wei. Language is not\nall you need: Aligning perception with language models,\n2023. 1\n[31] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\nMordatch.\nLanguage models as zero-shot planners: Ex-\ntracting actionable knowledge for embodied agents. In In-\nternational conference on machine learning, pages 9118–\n9147. PMLR, 2022. 2\n[32] Drew A. Hudson and Christopher D. Manning. Gqa: a new\ndataset for compositional question answering over real-\nworld images. In CVPR, 2019. 5\n[33] IDEFICS.\nIntroducing idefics:\nAn open reproduction\nof state-of-the-art visual language model.\nhttps:\/\/\nhuggingface.co\/blog\/idefics, 2023. 1\n[34] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,\nYongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anand-\nkumar, Yuke Zhu, and Linxi Fan.\nVima: General robot\nmanipulation with multimodal prompts.\narXiv preprint\narXiv:2210.03094, 2022. 2\n[35] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragki-\nadaki.\n3d diffuser actor: Policy diffusion with 3d scene\nrepresentations. arXiv preprint arXiv:2402.10885, 2024. 2,\n6, 7, 17\n[36] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted\nXiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov,\nEthan Foster, Grace Lam, Pannag Sanketi, et al.\nOpen-\nvla: An open-source vision-language-action model. arXiv\npreprint arXiv:2406.09246, 2024. 2\n[37] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur,\nMing Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan\nZhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwe-\nbarena: Evaluating multimodal agents on realistic visual\nweb tasks. arXiv preprint arXiv:2401.13649, 2024. 1\n[38] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline\nreinforcement learning with implicit q-learning.\narXiv\npreprint arXiv:2110.06169, 2021. 8\n[39] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su,\nJohn D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal,\nColton Bishop, Rebecca Roelofs, et al. Training language\nmodels to self-correct via reinforcement learning.\narXiv\npreprint arXiv:2409.12917, 2024. 2\n[40] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and\nWook-Shin Han.\nAutoregressive image generation using\nresidual quantization.\nIn Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11523–11532, 2022. 4, 13\n[41] Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee,\nDaniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fis-\ncher, Eric Jang, Henryk Michalewski, et al. Multi-game\ndecision transformers.\narXiv preprint arXiv:2205.15241,\n2022. 6, 7, 14, 16, 17\n[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nFanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.\nMimic-it: Multi-modal in-context instruction tuning. arXiv\npreprint arXiv:2306.05425, 2023. 1\n[43] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter: A multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 1\n[44] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng\nLi, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li. Llava-onevision: Easy visual task transfer.\narXiv preprint arXiv:2408.03326, 2024. 3, 8\n[45] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, and Jianfeng Gao.\nMultimodal\nfoundation models: From specialists to general-purpose as-\nsistants. arXiv preprint arXiv:2309.10020, 2023. 1\n[46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\n10\nfrozen image encoders and large language models, 2023.\n1\n[47] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo\nCampbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On\nthe effects of data scale on computer control agents. arXiv\npreprint arXiv:2406.03679, 2024. 5, 15, 16, 18\n[48] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie\nXu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang,\nHuaping Liu, et al. Vision-language foundation models as\neffective robot imitators. arXiv preprint arXiv:2311.01378,\n2023. 2, 6, 15, 17\n[49] Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh\nAgrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff\nNichols, Yinfei Yang, and Zhe Gan. Ferret-ui 2: Master-\ning universal user interface understanding across platforms.\narXiv preprint arXiv:2410.18967, 2024. 18\n[50] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy Zeng.\nCode as policies: Language model programs for embod-\nied control.\nIn 2023 IEEE International Conference on\nRobotics and Automation (ICRA), pages 9493–9500. IEEE,\n2023. 2\n[51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning, 2023. 1\n[52] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 5\n[53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 4,\n13\n[54] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale,\nHarsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao\nSun, et al. Improve mathematical reasoning in language\nmodels by automated process supervision. arXiv preprint\narXiv:2406.06592, 2024. 2\n[55] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-\nAn Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. Eureka: Human-level\nreward design via coding large language models.\narXiv\npreprint arXiv:2310.12931, 2023. 2\n[56] Yecheng Jason Ma, William Liang, Hung-Ju Wang, Sam\nWang, Yuke Zhu, Linxi Fan, Osbert Bastani, and Dinesh\nJayaraman. Dreureka: Language model guided sim-to-real\ntransfer. arXiv preprint arXiv:2406.01967, 2024. 2\n[57] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings\nof the IEEE\/cvf conference on computer vision and pattern\nrecognition, pages 3195–3204, 2019. 5\n[58] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training.\narXiv preprint arXiv:2403.09611, 2024. 1, 8\n[59] Ishita Mediratta, Qingfei You, Minqi Jiang, and Roberta\nRaileanu. The generalization gap in offline reinforcement\nlearning. arXiv preprint arXiv:2312.05742, 2023. 2, 6, 15,\n16, 17\n[60] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wol-\nfram Burgard.\nCalvin:\nA benchmark for language-\nconditioned policy learning for long-horizon robot manip-\nulation tasks. IEEE Robotics and Automation Letters, 7(3):\n7327–7334, 2022. 2, 5, 15, 16\n[61] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\nRiedmiller. Playing atari with deep reinforcement learning.\narXiv preprint arXiv:1312.5602, 2013. 14\n[62] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse, Shan-\ntanu Jain, Vineet Kosaraju, William Saunders, et al. We-\nbgpt:\nBrowser-assisted question-answering with human\nfeedback. arXiv preprint arXiv:2112.09332, 2021. 2\n[63] Abby O’Neill, Abdul Rehman, Abhinav Gupta, Abhiram\nMaddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham\nLee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al.\nOpen x-embodiment: Robotic learning datasets and rt-x\nmodels. arXiv preprint arXiv:2310.08864, 2023. 2, 5\n[64] OpenAI.\nGpt-4o\nsystem\ncard.\narXiv\npreprint\narXiv:2410.21276, 2024. 7\n[65] Austin Patel and Shuran Song. Get-zero: Graph embodi-\nment transformer for zero-shot embodiment generalization.\narXiv preprint arXiv:2407.15002, 2024. 2\n[66] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 1\n[67] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giu-\nlia Vezzani, John Schulman, Emanuel Todorov, and Sergey\nLevine.\nLearning complex dexterous manipulation with\ndeep reinforcement learning and demonstrations.\narXiv\npreprint arXiv:1709.10087, 2017. 17\n[68] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana\nRiva, and Timothy Lillicrap. Androidinthewild: A large-\nscale dataset for android device control. Advances in Neu-\nral Information Processing Systems, 36, 2024. 1, 6\n[69] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron,\nMai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-\ngenberg, et al.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022. 2, 6, 7, 16, 17, 18\n[70] St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A\nreduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the four-\nteenth international conference on artificial intelligence\nand statistics, pages 627–635. JMLR Workshop and Con-\nference Proceedings, 2011. 8\n[71] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub,\nJia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and\nDhruv Batra. Habitat: A Platform for Embodied AI Re-\nsearch. ICCV, 2019. 5, 6\n[72] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347, 2017. 4\n11\n[73] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In European conference on computer vision,\npages 146–162. Springer, 2022. 5\n[74] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg,\nVirginia Smith, and Aviral Kumar. Rl on incorrect synthetic\ndata scales the efficiency of llm math reasoning by eight-\nfold. arXiv preprint arXiv:2406.14532, 2024. 2\n[75] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,\nPanupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Ken-\nton Lee, and Kristina N Toutanova. From pixels to ui ac-\ntions: Learning to follow instructions via graphical user in-\nterfaces. NeurIPS, 2023. 1\n[76] Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du, and\nHuazhe Xu. Unleashing the power of pre-trained language\nmodels for offline reinforcement learning. arXiv preprint\narXiv:2310.20587, 2023. 2\n[77] Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh\nAnand, Piyush Patil, Xavier Garcia, Peter J Liu, James\nHarrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human\ndata: Scaling self-training for problem-solving with lan-\nguage models. arXiv preprint arXiv:2312.06585, 2023. 2\n[78] Andrew Szot, Alexander Clegg, Eric Undersander, Erik\nWijmans,\nYili\nZhao,\nJohn\nTurner,\nNoah\nMaestre,\nMustafa Mukadam, Devendra Singh Chaplot, Oleksandr\nMaksymets, et al. Habitat 2.0: Training home assistants\nto rearrange their habitat. Advances in Neural Information\nProcessing Systems, 34, 2021. 2, 5, 6, 14, 15, 16, 17\n[79] Andrew Szot, Karmesh Yadav, Alex Clegg, Vincent-\nPierre Berges, Aaron Gokaslan, Angel Chang, Manolis\nSavva, Zsolt Kira, and Dhruv Batra.\nHabitat rearrange-\nment challenge 2022.\nhttps:\/\/aihabitat.org\/\nchallenge\/2022_rearrange, 2022. 14\n[80] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan\nMazoure, Walter Talbott, Katherine Metcalf, Natalie Mack-\nraz, Devon Hjelm, and Alexander Toshev. Large language\nmodels as generalizable policies for embodied tasks. arXiv\npreprint arXiv:2310.17722, 2023. 1, 2, 5, 6, 14, 15\n[81] Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon\nHjelm, Zsolt Kira, and Alexander Toshev. Grounding mul-\ntimodal large language models in actions. arXiv preprint\narXiv:2406.07904, 2024. 2, 3, 4, 6, 14, 16, 17, 18\n[82] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder\nBaveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie\nBradley-Schmieg, Michael Chang, Natalie Clay, Adrian\nCollister, et al. Human-timescale adaptation in an open-\nended task space. arXiv preprint arXiv:2301.07608, 2023.\n2\n[83] Octo Model Team, Dibya Ghosh, Homer Walke, Karl\nPertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey\nHejna, Tobias Kreiman, Charles Xu, et al.\nOcto:\nAn open-source generalist robot policy.\narXiv preprint\narXiv:2405.12213, 2024. 2\n[84] Hado P Van Hasselt,\nArthur Guez,\nMatteo Hessel,\nVolodymyr Mnih, and David Silver. Learning values across\nmany orders of magnitude. Advances in neural information\nprocessing systems, 29, 2016. 4\n[85] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023.\n2\n[86] Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He.\nScaling proprioceptive-visual learning with heterogeneous\npre-trained transformers. arXiv preprint arXiv:2409.20537,\n2024. 2\n[87] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learn-\ners. arXiv preprint arXiv:2109.01652, 2021. 2\n[88] Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Roge-\nrio Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba,\nAshish Kapoor, and Shuang Ma. Is imitation all you need?\ngeneralized decision-making with dual-phase training. In\nProceedings of the IEEE\/CVF International Conference on\nComputer Vision, pages 16221–16231, 2023. 2\n[89] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao\nYu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang,\nand Yunxin Liu. Empowering llm to use smartphone for\nintelligent task automation. arXiv e-prints, pages arXiv–\n2308, 2023. 1\n[90] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen,\nJiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and\nTao Kong.\nUnleashing large-scale video generative pre-\ntraining for visual robot manipulation.\narXiv preprint\narXiv:2312.13139, 2023. 2\n[91] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lep-\nert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon\nRusinkiewicz, and Thomas Funkhouser. Tidybot: Person-\nalized robot assistance with large language models.\nAu-\ntonomous Robots, 47(8):1087–1102, 2023. 2\n[92] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan,\nTimothy P Lillicrap, Kenji Kawaguchi, and Michael Shieh.\nMonte carlo tree search boosts reasoning via iterative pref-\nerence learning. arXiv preprint arXiv:2405.00451, 2024.\n2\n[93] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan\nLi, and Jianfeng Gao.\nSet-of-mark prompting unleashes\nextraordinary visual grounding in gpt-4v. arXiv preprint\narXiv:2310.11441, 2023. 6, 7\n[94] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Al-\nvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay\nKrishna, Lingjie Liu, et al. Holodeck: Language guided\ngeneration of 3d embodied ai environments. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 16227–16237, 2024. 2\n[95] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 1\n[96] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav,\nAustin Wang, Mukul Khanna, Theophile Gervet, Tsung-\nYen Yang, Vidhi Jain, Alexander William Clegg, John\n12\nTurner, et al.\nHomerobot: Open-vocabulary mobile ma-\nnipulation. arXiv preprint arXiv:2306.11565, 2023. 17\n[97] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers,\nAmanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe\nGan. Ferret-ui: Grounded mobile ui understanding with\nmultimodal llms.\nIn European Conference on Computer\nVision, pages 240–255. Springer, 2025. 2\n[98] Tianhe Yu, Deirdre Quillen, Zhanpeng He, R. Julian, Karol\nHausman, Chelsea Finn, and S. Levine. Meta-world: A\nbenchmark and evaluation for multi-task and meta rein-\nforcement learning. In CoRL, 2019. 5\n[99] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine. Meta-\nworld: A benchmark and evaluation for multi-task and meta\nreinforcement learning. In Conference on Robot Learning\n(CoRL), 2019. 5, 14\n[100] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-\nmanski, Adrian Wong, Stefan Welker, Federico Tombari,\nAveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. So-\ncratic models: Composing zero-shot multimodal reasoning\nwith language. arXiv preprint arXiv:2204.00598, 2022. 2\n[101] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu,\nWeiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yi-\nhao Feng, Zuxin Liu, et al. Agentohana: Design unified\ndata and training pipeline for effective agent learning. arXiv\npreprint arXiv:2402.15506, 2024. 2\n[102] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu\nSu. Gpt-4v (ision) is a generalist web agent, if grounded.\narXiv preprint arXiv:2401.01614, 2024. 1\n[103] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1\nA. Continuous Multi-Embodiment Tokenizer\nDetails\nAs stated in Section 4.2, we train a Residual VQ-VAE\n(RVQ) model to convert continuous actions from diverse\nrobot embodiments into a shared discrete token space. In\nour experiments, we use 2 codebooks, each with 512 to-\nkens, and the token vector dimension is 1024. Actions are\nencoded into these latent codebooks via a 4-layer MLP with\n4096 hidden dimensions per layer. The decoder uses the\nsame MLP architecture but now inputs the 1024-dimension\nlatent and outputs the padded action. Refer to Lee et al.\n[40] for further details about the RVQ method. We map\nthese 512 tokens for both of the 2 codebooks to tokens\n30000 −30512 of the LLM vocabulary. Since these tokens\nare non-English tokens for all the LLMs we consider and all\nour tasks use English instructions, we use this same token\nrange for all MLLM experiments. All actions are padded to\nbe 21 dimensions during tokenization. During detokeniza-\ntion, the 21 dimension continuous vector is truncated from\nthe right to fit the expected action dimension for the envi-\nronment.\nThe RVQ tokenizer is trained with MSE loss using all\ndatasets with continuous actions from the overall set of\ndatasets used to train GEA described in Appendix D. The\ncommitment loss from the RVQ is weighted by 1.0 relative\nto the MSE loss. The tokenizer is trained with a per-GPU\nbatch size of 256 across 8 H100 GPUs. The model is then\ntrained for 15k updates using the AdamW optimizer [53]\nwith a cosine learning rate decaying to 0 at the end of train-\ning and a linear learning rate warmup schedule for the first\n10% of training. At this number of updates, the valida-\ntion MSE loss on unseen actions had largely plateaued at\n≈0.0038 averaged across all datasets and 0.002 −0.007\ndepending on the dataset. This trained RVQ tokenizer was\nthen used to train and evaluate all models with continuous\nactions.\nB. SFT Training Additional Details\nTo limit the number of tokens per frame to be processed\nby the LLM, we use the “video” encoding strategy from\nLLaVA-OneVision. This does not use the AnyRes tech-\nnique and also applies bilinear interpolation to reduce the\nnumber of tokens per image. This results in 196 tokens\nfor each image after being processed by the visual encoder\nand downsampled. For the training sequence format de-\nscribed in Section 3.2, we use the following prompt format\nwhere per-domain strings are manually defined and substi-\ntuted into each bracketed statement.\nUser:\nAgent: <agent description>.\nActions:\n13\nSimulator: <simulation platform>\nCamera: <camera details>\nInstruction: <task instruction>\nAgent:\nWhen forming training batches, we randomly sample trajec-\ntories, and then randomly sample a span of the appropriate\ncontext length from that trajectory, and then use the sam-\npled span of observation and actions as an element of the\ndata batch. In the case of the interactive data, the model\nis only trained to predict the action tokens; the loss for the\nprompt, instruction, and image tokens is masked out.\nFor the details about the datasets used in this training\nprocess, see Appendix D.\nC. RL Training Additional Details\nIn this section, we list additional RL training details omitted\nfrom Section 4.4. The value function is a 4-layer MLP with\na hidden dimension of 2048. The value function takes as\ninput a single vector from the mean-pooled visual tokens\nfrom the visual encoder, the final activation of the LLM for\nthe observation, and any task-specific state information that\nis available. For LoRA finetuning, we use a value of r =\n128, α = 32, and a dropout value of 0.1. We use GAE\nreturn estimation with τ = 0.95 and a discount factor of\nγ = 0.999.\nWe run RL with the Habitat Pick, LangR, and Procgen\nenvironments. Each GPU runs a different benchmark in-\nstance. Both Habitat Pick and Procgen are allocated to run\non 50% more GPUs than LangR because we found that\nLangR learns much faster with RL compared to the other\ntasks. Per benchmark variation, such as different episodes\nin Habitat Pick and different games in Procgen, are equally\ndivided between the GPUs assigned to that benchmark. On\neach GPU, a parallelized set of 6 environments are running\nfor batched environment experience collection. We update\nwith 2 PPO epochs, 6 minibatches per epoch, and a clip pa-\nrameter of 0.2. We also use the AdamW optimizer for RL\ntraining but without any learning rate schedule. We now\ndetail the RL considerations specific to each environment.\nIn Habitat Pick, we use the same environment details as\nfrom prior works using the same task [26, 79, 81]. The task\nrequires the agent to pick up a target object specified by\nname. Specifically, the agent starts within 2 meters of the\ntarget object and faces towards the receptacle the target ob-\nject is on but with random noise N(0, 1.57) applied to the\ndirection of facing directly at the receptacle. The task ends\nin failure if the agent excessively collides with the scene,\ndrops the object, or picks up the wrong object. The maxi-\nmum number of steps per episode is 300 steps. We use the\nsame reward as the RL-trained pick skill from Szot et al.\n[78] where a dense reward is provided for moving the end-\neffector closer to the object, a positive bonus for picking up\nthe object, and then negative penalties for collisions. We\nuse the 50k training episodes from Szot et al. [79]. Note\nthat these training episodes used for RL training are dis-\ntinct from the episodes used for testing, which are in unseen\nhouse layouts.\nFor the LangR environment, we use the RL environment\ndetails from Szot et al. [80]. The reward function for this en-\nvironment involves a sparse reward for completing the task,\nsubgoal rewards for completing individual parts of the task,\nand a slack penalty to encourage completing the task faster.\nThis environment has a maximum of 32 steps per task. In\nLangR, memory is important because the agent must ex-\nplore to find certain objects. We therefore increased the\nnumber of visual observations in the context to 16 for the\nLangR RL training. We achieve this by increasing the vi-\nsual encoder bilinear interpolation factor to produce only\n32 visual tokens per image observation. Implementing such\nenvironment-specific policy tweaks is simpler in the RL\ntraining phase because each GPU worker runs a distinct en-\nvironment.\nFor Procgen, we use the RL environment details from\nCobbe et al. [15]. Importantly, we train over all 16 of the\nProcgen games at once using RL. We use the standard per-\ngame reward functions.\nD. Dataset Details\nMetaWorld: We use the Metaworld simulator [99] with the\npre-defined MT-45 split, which consists of 45 training table-\ntop manipulation tasks using a Sawyer XYZ arm. Each of\nthe 45 tasks has a language instruction associated to it, e.g.\n”Open the drawer”, which we use in all experiments. As in-\nputs to the generalist agent, we use RGB observations from\nthe corner3 camera resized to GEA resolution. We use\nno proprioceptive information.\nThe dataset consists of 500 trajectories from each of the\n45 train tasks. To construct the train, validation and test\ndatasets, we rollout the scripted expert policy 10 times for\neach task’s starting state until the first success, to obtain\nsufficient data diversity. We then assign some of the starting\nstates from the 45 training tasks to the validation set and the\nremaining trajectories to the training dataset.\nAtari: We use the DQN replay dataset [2], which con-\nsists of 50 million transitions collected while training the\nDQN [61] algorithm on each of the 44 environments sep-\narately.\nBased on the findings of Multi-Game Decision\nTransformer [41], we construct the SFT training dataset to\nbe the top-10% of the DQN replay data by trajectory re-\nturns, over 5 random seeds and 50 splits. We do not use\n100% of the data since it is suboptimal to include low-return\ntrajectories in the SFT dataset. However, if one would like\nto run offline RL on the data, they should include all of the\navailable data.\n14\nBabyAI: This dataset consists of 50k trajectories split\nequally between each of the BabyAI tasks from Chevalier-\nBoisvert et al. [14].\nThese trajectories are collected by\nthe shortest path expert provided in the code release for\nChevalier-Boisvert et al. [14]. The observations are top-\ndown RGB renderings of the image at 336×336 resolution.\nProcgen: We use the training datasets across all Proc-\ngen games provided from Mediratta et al. [59]. This dataset\nconsists of 10M observation-action transitions between all\nthe games collected by a PPO expert policy that was indi-\nvidually trained on each game. These transitions amount to\naround 320k trajectories.\nCALVIN: We use the standard CALVIN ABC →D\ndataset provided by Mees et al. [60], which are language-\nlabeled task instructions collected by human teleoperation\nof the robot. This dataset consists of around 18k demonstra-\ntions. The observations are 200×200 RGB renderings from\nthe robot head camera. Note that unlike prior work [48], we\ndo not use the gripper camera or any robot proprioception\nfrom this dataset. The actions in the dataset are 6D end-\neffector control for the relative orientation and position and\nthe gripper state.\nLangR: For this work, we collect 150k demonstrations\nfor each of the 150k unique training episodes defined by\nSzot et al. [80]. We collect this data by utilizing the RL-\ntrained policy from [80]. This policy achieves high perfor-\nmance on the training set of instructions (around 98% suc-\ncess rate), and we collect 1 successful demonstration per\ntraining episode. The observations are 336 × 336 RGB im-\nages from the robot head camera. The actions are between\n70 high-level skills that include picking up objects by name,\nnavigating to receptacles, placing on receptacles by name,\nand opening and closing receptacles by name.\nHabitat Pick\/Place: We collect 50k demonstrations for\neach of these tasks via an expert policy trained with RL.\nWe use the same setup as from the skill training in Szot\net al. [78] but operate from an object class to pickup rather\nthan the original geometric goal task specification. This ex-\npert policy was trained with the ground truth simulator state,\nconsisting of the relative position of the target object to the\nrobot’s end-effector. The expert is trained for 100M steps\nuntil convergence. The performance of both experts on the\nunseen episodes are displayed in Table 2 as baselines.\nFor the online learning experiments from Section 6.2, we\nconstruct the training dataset as follows. We use the GEA-\nBase checkpoint to collect 10,000 trajectories with action\nsampling to allow for suboptimal trajectories to be added\nto the data. The rollouts policy has a success rate of about\n50%, meaning that half of the trajectories can be used for\nSFT from successful trajectories, and all of them can be\nused for offline RL. When training the SFT policy, we re-\nstrict the loss to be optimized only over successful trajecto-\nries, as SFT cannot learn from suboptimal data. In addition\nto observations and actions, we also log the reward values\nwhich are required for offline RL.\nHabitat Nav: We collect 13k shortest path demonstra-\ntions of an agent navigating to receptacles by name in the\nhouse. The navigation is performed by an oracle shortest\npath agent.\nManiskill: These datasets are from the released im-\nitation learning datasets from Gu et al. [22].\nThey are\ngenerated via a motion planning algorithm. We only uti-\nlize the RGB 3rd-person RGB camera. We generate data\nfor the “StackCube”, “PegInsertionSide”, “PlugCharger”,\n“PushCube”, and “PickCube” tasks. The dataset from Gu\net al. [22] has 1k demonstrations for each of these tasks.\nAndroid Control: This is the dataset from Li et al. [47]\nconsisting of 14k human demonstrations of using apps to\naccomplish UI control tasks. Each trajectory has a unique\nlanguage instruction. The data spans 833 apps. The original\nimages are 1080×1920, corresponding to the phone screen\nsize. Since we do not use any AnyRes techniques in the\nMLLM visual encoder, we resize the images to be square\nbefore inputting them into the MLLM visual encoder. Us-\ning AnyRes with image crops to account for these image\naspect ratios could improve the performance of GEA. Ac-\ntions are generally represented as the name of the action\ntype (like “tap”, “scroll”, or “input text”) followed by the\nargument for that action where applicable. For tap action ar-\nguments, we discretize the original 1080×1920 screen into\n50 × 50 patches. A tap action argument is represented as\ntwo integers representing the horizontal and vertical patch\ncoordinates where the tap occurred on the screen. All these\nactions are represented as textual tokens and are separate\nfrom the continuous action tokens.\nOpenX: The Open X-Embodiment dataset consists of\nnumerous individual datasets spanning different robots.\nWe include the following individual datasets from OpenX:\nAustin Buds dataset, Austin Sailor dataset, Austin Sir-\nius dataset, Berkeley Cable Routing, CMU Stretch, DLR\nEDAN Shared Control, Fractal, IAMLab CMU Pickup In-\nsert, Jaco Play, Kuka, UCSD Kitchen Dataset, UTAustin\nMutex, Dobbe, FMB, RoboSet and Spoc.\nVision language instruction data: We use the datasets\ndescribed in Section 5.\nWhen sampling batches for any SFT training, we weight\neach dataset defined in Table 1 equally, except OpenX is up-\nsampled 2x, Procgen 2x, and all the VQA data is upsampled\n3x.\nE. Evaluation Details\nOverall, we use the standard evaluation settings per environ-\nment as defined by the prior work. We detail the evaluation\nsettings for each environment below:\nMetaWorld:\nWe evaluate the generalization perfor-\nmance of our agent on Metaworld unseen starting states\n15\nover 5 episodes each, totaling to 450 evaluation trajectories\nin total. We use the simulator’s notion of success to define\nthe success rate.\nAtari:\nWe evaluate the in-distribution performance\nof our model on 44 Atari games, by using the same\nsetup as the DQN replay dataset, which in turn relies on\nthe Dopamine [12] framework.\nSpecifically, we use the\n{GameID}NoFramskip-v4 version of the ALE simula-\ntor [6]. We conduct 10 rollouts over all 44 Atari games\nand average their respective human-normalized scores. The\nhuman-normalized scores follow the same protocol as [41,\n69]:\nscorenormalized(s) =\n|s −scorerandom|\nscorehuman −scorerandom\nThrough our experiments, we have found that GEA-Base\nperforms better according to the human-normalized average\nscore than GEA (40.3 vs 32.71). The result is not surpris-\ning, as we do not perform any Atari online RL finetuning\non the GEA-Base model, and hence, performance can de-\ngrade at the expense of much better performance on Habitat\nPick and Procgen. To solve this issue, one should co-train\non all tasks that support fast online simulation. However,\nsince Atari is structurally similar to Procgen, and Procgen\nsupports procedural level generation to test generalization,\nwe choose not to perform online RL finetuning on Atari.\nBabyAI: We evaluate each task from Chevalier-Boisvert\net al. [14] and evaluate over 100 random episodes for each\nof the 17 tasks, resulting in 1700 total episodes for the num-\nbers reported in Table 2. Each episode has a different envi-\nronment state and new language instruction.\nProcgen: We follow the test evaluation setting Medi-\nratta et al. [59] which uses the “easy” mode setting of the\ngame. We evaluate 50 episodes for each of the 16 games.\nLike Atari, the performance is evaluated in Procgen via the\nmin-max normalized per-game scores from a random policy\nand the a PPO expert policy using the reported scores from\nCobbe et al. [15].\nCALVIN: We use the ABC →D evaluation setting.\nThis means that during our evaluation, the table background\nand the language instructions are unseen. We report results\nover the full 1k evaluation episodes defined by Mees et al.\n[60].\nLangR: We follow the standard evaluation settings from\nSzot et al. [78]. Like the original work, our numbers are\nreported over the 9 unseen language instruction splits. We\nevaluate in 100 episodes for each of the 9 evaluation splits.\nEach test episode is also in an unseen house layout.\nHabitat Pick\/Place\/Nav: We follow the evaluation split\nand settings from Szot et al. [78] and evaluate the agent for\n500 episodes in unseen house layouts. This version of the\ntask where the agent has to operate from a language instruc-\ntion of which object to pick is a harder version of the orig-\ninal geometric goal task and has been employed by prior\nworks [26, 81].\nManiskill: We use the standard evaluation settings from\nGu et al. [22]. Aligning with the generated dataset for Man-\niskill, we evaluate in the “StackCube”, “PegInsertionSide”,\n“PlugCharger”, “PushCube”, and “PickCube” tasks. Each\nof the 5 tasks are evaluated for 100 episodes each.\nAndroid Control: We evaluate on the full 1,540 test\nepisodes from Li et al. [47]. We measure the per-step suc-\ncess rate, meaning the percent of the time the agent predicts\nthe right action based on the ground truth test episode. This\nis distinct from the episode level success rate which requires\nthe agent to predict every action in the trajectory correctly.\nWe report the success rate under the more challenging set-\nting of following high-level instructions only, without per-\nstep low-level instructions. We consider an action as pre-\ndicted correctly if it is within 5 of the horizontal and vertical\npatches defined in the dataset generation from Appendix D.\nAny entered text is considered correct if the correct text is\ncontained in the entered text or the entered text is contained\nin the correct text.\nF. Further Experimental Details\nF.1. Additional Baseline Details\nFor each benchmark, to the best of our knowledge, we re-\nport the method from prior work with the highest perfor-\nmance. In this section, we add more details about these\nbaselines from prior works in Table 2 and any differences in\nthe evaluation settings and method assumptions from GEA.\nWe source methods from prior work that train a single pol-\nicy over all tasks in a benchmark. We consider methods\nthat only train on tasks from a single benchmark as “spe-\ncialist” and methods that train across multiple benchmarks\nas “generalist”. Note that this means we do not compare\nagainst methods that train policies on individual tasks from\nthe benchmark. For example, in Procgen, we do not com-\npare to the performance of Cobbe et al. [15] since a separate\npolicy is trained for each of the 16 tasks. Instead, we only\ncompare to methods that train a single policy across multi-\nple tasks.\nMeta-World: Many prior works report performance on\nthe Meta-World benchmark [24], but fewer works train and\nevaluate over the full set of 45 tasks. Gato [69] trains over\nall the Meta-World tasks and reports an average perfor-\nmance of 87.0% success rate.\n1 Unlike GEA, Gato also\ntakes as input the proprioceptive state in Meta-World. Reed\net al. [69] also does not clarify if the Meta-World evaluation\nis performed over unseen state configurations or using the\nsame states seen during training. GEA is evaluated on un-\nseen state configurations. The GEA Meta-World numbers\n1We reference the Gato numbers from Table 8 of the TMLR paper ver-\nsion: https:\/\/openreview.net\/pdf?id=1ikK0kHjvj\n16\n0\n5000 10000 15000 20000 25000 30000 35000 40000\nUpdates\n1.0\n1.2\n1.4\n1.6\n1.8\nValidation Loss\nSeed 0\nSeed 1\nSeed 2\n(a) Validation Loss (GEA-Base-500m).\nHabPick\nProcgen\nCALVIN\nBabyAI\nMeta-\nWorld\nAndroid\nControl\nSeed 0\n46.5\n29.0\n44.5\n82.6\n82.7\n48.4\nSeed 1\n49.0\n25.4\n42.5\n80.0\n86.7\n50.1\nSeed 2\n53.0\n27.3\n44.5\n82.4\n88.4\n49.2\nCombined\n49.5 ± 3.3\n27.2 ± 1.8\n43.8 ± 1.2\n81.7 ± 1.5\n85.9 ± 3.0\n49.2 ± 0.8\n(b) Evaluation success rates (GEA-Base-500m).\nFigure 6. Variance in training jobs and evaluation for GEA. We train three different random seeds for GEA-Base-500m. The left shows the\nvalidation loss during SFT is similar between each random seed. The right shows the resulting online evaluation for these three random\nseeds across six of the benchmarks, along with the combined averages and standard deviation per benchmark. While the standard deviation\nis low, it is still a couple of percent on some benchmarks despite the validation loss being very similar.\nare reported in the same setting as Szot et al. [81] with the\nsame inputs.\nCALVIN: To the best of our knowledge, there are no\ngeneralist agents that report the performance on CALVIN\nin addition to other benchmarks. RoboFlamingo [48] also\nadapts an MLLM for control by finetuning it with super-\nvised learning.\nWe include this specialist agent since it\nalso leverages finetuning MLLMs, to demonstrate the gains\nfrom scaling to a generalist model with GEA. Compared to\nGEA, RoboFlamingo uses the gripper camera, image aug-\nmentations during training and a longer context length. 3D\nDiffuser Actor [35] is the state-of-the-art specialist system\nfor CALVIN ABC →D setting and narrowly outperforms\nGEA. However, as mentioned in the main text, 3D Diffuser\nActor also assumes input to 3D pointclouds features. This\nmethod uses the head and gripper RGBD cameras to ex-\ntra pointclouds of the scene. These pointclouds are then\nconverted into a 3D feature cloud using a pretrained CLIP\nmodel. This method also simplifies the problem by com-\npressing longer sequences of actions into end-effector key-\nposes.\nManiskill: We compare to the numbers using RGBD in\nTable 2 and 3 of Gu et al. [22]. Unlike these numbers, GEA\nuses only RGB inputs and no depth images. These results\ntrain a single policy per-task which is technically narrower\nthan our definition of “Specialist Agent” which requires\ntraining one policy on all tasks from the benchmark. How-\never, we still include these baselines to situate our Maniskill\nresults. GEA outperforms doing imitation learning with the\nexact same demonstrations as from Table 2 of Gu et al. [22].\nThe superior results of 47.8% success are from Table 3 of\nGu et al. [22], which train with DAPG [67] and PPO. GEA\ndoes not train with RL in this task. Hansen et al. [25] re-\nports higher success rates in Maniskill2 tasks, but uses the\nground truth state information instead of visual observations\nand trains a single policy per individual Maniskill2.\nHabitat Pick: Other methods that report performance\non the version of the Habitat Pick task that requires pick-\ning from the object name typically achieve low success\nrates [26, 81, 96]. We thus also compare to the expert policy\nthat was used to generate the Habitat Pick training dataset\nas described in Appendix D. Note that this expert policy\nwas not trained in the evaluation scenes and thus also must\ngeneralize to unseen scenes. This expert policy has perfor-\nmance on par with pick skills trained in Habitat that oper-\nates from the much stronger assumption of a geometric goal\ninput [78]. The specialist numbers from [81] also finetune\na MLLM with imitation learning and are evaluated in the\nexact same setting as GEA.\nHabitat Place: We follow the same evaluation criteria as\nHabitat Pick and compare to the expert policy trained with\nRL that was used to generate the expert demonstrations as\ndescribed in Appendix D.\nProcgen: We compare against the BC test numbers\nfrom Figure 2 of Mediratta et al. [59], which use the same\ndatasets as our setup. While Gato [69] also reports num-\nbers in Procgen, we are not able to compare to these num-\nbers because Gato reports performance relative to unknown\nscore of the data collection policy. To the best of our knowl-\nedge the score of the data collection policy is not released.\nThus, it is unclear how the Gato Procgen performance is\nnormalized according to the standard Procgen normaliza-\ntion scores [15] rendering a direct comparison impossible.\nAtari:\nFor a generalist system, we compare with\nGato [69] which as Table 8 shows of Reed et al. [69]\nshows, achieves 30.9 normalized score. Multi-Game De-\ncision Transformers [41] achieves 85 normalized score in\nthe same scoring setting. This method was trained with of-\nfline RL based on conditioning the training on the reward-\nto-go [13].\nHabitat Nav: We compare against the success rate of\nthe navigation policy from Szot et al. [78]. This policy is\ntrained with RL on the same training set of episodes and\nevaluated on the same testing set of episodes as GEA. How-\n17\never, this policy operates on the geometric goal specification\nof the receptacle rather than the receptacle name. Addition-\nally, this policy also takes an egomotion sensor as input,\nwhereas GEA does not, simplifying the problem.\nBabyAI: We compare against Gato [69] which as Ta-\nble 8 in Reed et al. [69] shows, achieves 93.2 normalized\nscore. While Reed et al. [69] does not clarify this detail,\npresumably the normalization is with respect to a perfect\nexpert policy, so the normalized score is equal to the suc-\ncess rate. Gato trains with far more data than GEA with\n4.61M episodes.\nAndroidControl: We compare against using a Set-of-\nMark prompting with GPT-4o. The Set-of-Mark was imple-\nmented using the Ferret-UI model [49] for UI element de-\ntection to generate the marks with GPT-4o to determine the\naction. GEA and this baseline are evaluated under the same\nsuccess criteria. We do not compare to the numbers from\nthe original AndroidControl paper [47] since it uses differ-\nent evaluation criteria from ours described in Appendix E\nand the code for the evaluation in Li et al. [47] is not publi-\ncally released.\nLangR: We compare against the state-of-the-art num-\nbers from Szot et al. [81]. This method was trained with RL\nover the same set of training episodes as GEA.\nF.2. Ablation Analysis Setting\nFor the analysis experiments, we used a reduced subset\nof the total dataset. Specifically, we use the Meta-World,\nCALVIN, Habitat Pick, BabyAI, Procgen, and Android-\nControl datasets. Datasets from the other domains are ex-\ncluded.\nWhen training methods in the analysis setting, we keep\nall the same settings as from the main GEA experiments\nwith the hyperparameters described in Appendix B. How-\never, we reduce the number of updates to 40k and use a\nglobal batch size of 256 across 2 nodes of 8 H100 GPUs\neach.\nAll results in the analysis section are reported in\nthe LLaVA-OneVision-500m setting unless specified oth-\nerwise.\nG. Further Results\nG.1. Benchmark Per-Task Success Rate\nWe breakdown the performance of the GEA model reported\nin Table 2 per individual task for the benchmarks of Meta-\nWorld (Table 5), CALVIN (Table 6), Procgen (Table 7),\nManiskill (Table 8), LangR (Table 9), and BabyAI (Ta-\nble 10).\nG.2. Habitat Pick RL Finetuning\nComplimenting the results demonstrating the value of on-\nline learning from Figure 4, in this section, we compare the\nRL sample efficiency of GEA-Base versus the base MLLM.\nFigure 7 shows that doing RL from the GEA-Base model is\nfar more sample efficient and converges to much higher per-\nformance than doing RL on the MLLM model. The GEA-\nBase model is trained with SFT on demonstrations from the\nHabitat Pick task, so it is expected that its performance will\nstart higher. However, the MLLM model is never able to\nmake up for the performance gap, even with continued RL\nfinetuning.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment Steps\n1e8\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nMLLM\nGEA-Base\nFigure 7. Success rate on the Habitat Pick task of GEA-Base and\nthe base MLLM when finetuning with online RL. Displayed are\nsuccess rates on the training dataset used in the RL process.\nG.3. Model Variance Analysis\nIn this section, we analyze the sensitivity of training GEA-\nBase-500m with different random seeds. Specifically, we\nvary the random seed used to initialize the model, all as-\npects of the algorithm, and dataset sampling. We then train\nthe GEA-Base-500m model in the analysis setting from Ap-\npendix F.2. The results in Figure 6 demonstrate that while\nthe training and validation curves are very similar. The on-\nline evaluation performance of GEA-Base-500m does have\nsome variance per random seed within a couple of percent-\nage points.\nG.4. Domain Transfer Analysis\nIn Figure 8 we study how performance transfers between\ndomains by training the GEA-Base-500m model on every\npossible pair of datasets from BabyAI, CALVIN, Habitat\nPick, Android Control and Procgen. We train the model in\nthe same analysis setting as from Appendix F.2. We com-\npare the performance of the dataset pair to only training on\none of the datasets (the same as the “Domain Specific” re-\nsults from Table 4). The results in Figure 8, with more blue\ncolors for negative transfer and more red colors for posi-\ntive transfer, show that some domains such as Procgen and\nCALVIN enormously benefit from data in other domains.\nAndroid Control slightly benefits from Procgen, another\ndiscrete control task. On the other hand, Habitat Pick and\nBabyAI have negative transfer from a variety of tasks. De-\n18\nspite this negative transfer, Table 4 still demonstrates that\ntraining on all the data across all these domains improves\nthe performance.\nHabitat\nPick\nCALVIN\nProcgen\nAndroid\nControl\nBabyAI\nEvaluation Domain\nHabitat\nPick\nCALVIN\nProcgen\nAndroid\nControl\nBabyAI\nAdditional Training Domain\n1.00\n1.15\n1.16\n1.02\n0.90\n0.75\n1.00\n1.16\n0.98\n0.82\n1.04\n1.24\n1.00\n1.06\n1.00\n0.97\n1.20\n1.25\n1.00\n0.95\n0.92\n1.06\n1.33\n1.02\n1.00\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\nSuccess Relative to Domain Specific\nFigure 8. Each square represents the success rate of GEA-Base-\n500m trained with the datasets from the two domains indicated by\nthe column and row name and evaluated on the domain indicated\nby the column. Success rates are scaled by training on only data\nfrom that domain, meaning each column is normalized relative to\nthe diagonal. A more blue color means negative transfer and a\nmore red color means positive transfer.\nGEA\nassembly\n86.0\nbasketball\n100.0\nbutton-press-topdown\n100.0\nbutton-press-topdown-wall\n100.0\nbutton-press\n100.0\nbutton-press-wall\n92.0\ncoffee-button\n100.0\ncoffee-pull\n100.0\ncoffee-push\n100.0\ndial-turn\n100.0\ndisassemble\n88.0\ndoor-close\n100.0\ndoor-open\n100.0\ndrawer-close\n100.0\ndrawer-open\n100.0\nfaucet-open\n100.0\nfaucet-close\n100.0\nhammer\n100.0\nhandle-press-side\n100.0\nhandle-press\n100.0\nhandle-pull-side\n80.0\nhandle-pull\n100.0\nlever-pull\n80.0\npeg-insert-side\n100.0\npeg-unplug-side\n98.0\npick-out-of-hole\n60.0\npick-place\n92.0\npick-place-wall\n82.0\nplate-slide\n100.0\nplate-slide-side\n96.0\nplate-slide-back\n100.0\nplate-slide-back-side\n100.0\npush-back\n90.0\npush\n100.0\npush-wall\n100.0\nreach\n60.0\nreach-wall\n94.0\nshelf-place\n98.0\nsoccer\n76.0\nstick-push\n100.0\nstick-pull\n98.0\nsweep-into\n90.0\nsweep\n100.0\nwindow-open\n100.0\nwindow-close\n100.0\nTable 5. Meta-World per-task success rate breakdown.\n19\nGEA\nturn off led\n87.0\nmove slider left\n100.0\nrotate red block right\n69.0\nopen drawer\n100.0\nrotate red block left\n95.5\npush pink block right\n100.0\npush blue block right\n65.2\npush red block left\n85.7\npush pink block left\n87.1\npush red block right\n31.0\npush blue block left\n88.6\npush into drawer\n86.7\nrotate pink block left\n100.0\nturn on lightbulb\n97.6\nrotate pink block right\n93.5\nrotate blue block right\n78.6\nturn off lightbulb\n97.1\nlift blue block table\n100.0\nclose drawer\n100.0\nrotate blue block left\n95.8\nmove slider right\n95.4\nturn on led\n96.4\nlift blue block slider\n63.3\nlift pink block table\n100.0\nlift red block slider\n73.1\nlift red block table\n83.3\nlift pink block slider\n96.0\nTable 6. CALVIN per-task success rate breakdown. Note the tasks\nare not equally represented in the evaluation episodes.\nGEA\nbigfish\n43.1\nbossfight\n54.2\ncaveflyer\n27.7\nchaser\n54.0\ncoinrun\n66.0\ndodgeball\n3.4\nfruitbot\n84.8\nheist\n32.0\nleaper\n26.0\nmaze\n58.0\nminer\n27.0\nclimber\n46.2\nninja\n62.0\nplunder\n4.5\njumper\n50.0\nTable 7. Procgen per-game score breakdown.\nGEA\nStackCube\n4.0\nPegInsertionSide\n0.0\nPlugCharger\n0.0\nPushCube\n52.0\nPickCube\n12.0\nTable 8. Maniskill per-task score breakdown.\nGEA\nrephrasing\n84.0\nreferring expressions\n16.0\nspatial relationships\n0.0\ncontext\n34.0\nirrelevant text\n86.0\nmultiple rearrangements\n82.0\nnovel objects\n96.0\nmultiple objects\n0.0\nconditional instructions\n52.0\nTable 9. LangR per-task score breakdown.\nGEA\nGoToRedBallGrey\n88.0\nGoToRedBall\n97.0\nGoToRedBallNoDists\n100.0\nGoToObj\n100.0\nGoToObjS4\n100.0\nGoToLocalS8N7\n93.0\nGoToRedBlueBall\n92.0\nGoToDoor\n100.0\nOpen\n76.0\nOpenRedDoor\n100.0\nOpenDoorColor\n100.0\nOpenDoorLoc\n80.0\nOpenTwoDoors\n100.0\nOpenRedBlueDoors\n100.0\nPickup\n41.0\nPickupLoc\n88.0\nPickupDist\n94.0\nTable 10. BabyAI per-task score breakdown.\n20\nGEA\nAlien\n6.1\nAmidar\n0.0\nAssault\n86.5\nAsterix\n2.3\nAtlantis\n0.0\nBankHeist\n8.9\nBattleZone\n39.2\nBeamRider\n3.1\nBoxing\n0.0\nBreakout\n0.0\nCentipede\n50.0\nChopperCommand\n30.2\nCrazyClimber\n0.0\nDemonAttack\n20.8\nDoubleDunk\n300.0\nEnduro\n0.0\nFishingDerby\n0.0\nFreeway\n81.1\nFrostbite\n2.7\nGopher\n0.0\nGravitar\n0.0\nHero\n0.0\nIceHockey\n0.0\nJamesbond\n0.0\nKangaroo\n38.5\nKrull\n373.0\nKungFuMaster\n0.0\nMsPacman\n27.9\nNameThisGame\n0.0\nPhoenix\n0.0\nPong\n0.0\nQbert\n0.6\nRiverraid\n0.0\nRoadRunner\n33.0\nRobotank\n307.2\nSeaquest\n0.3\nSpaceInvaders\n52.7\nStarGunner\n1.4\nTimePilot\n0.0\nUpNDown\n0.0\nVideoPinball\n0.0\nWizardOfWor\n0.0\nYarsRevenge\n0.0\nZaxxon\n0.0\nTable 11. Atari success rate breakdown.\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons.pdf"}
{"title":"Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis","authors":"Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, Wentao Zhang","summary":"Recent advances in diffusion models have demonstrated exceptional\ncapabilities in image and video generation, further improving the effectiveness\nof 4D synthesis. Existing 4D generation methods can generate high-quality 4D\nobjects or scenes based on user-friendly conditions, benefiting the gaming and\nvideo industries. However, these methods struggle to synthesize significant\nobject deformation of complex 4D transitions and interactions within scenes. To\naddress this challenge, we propose Trans4D, a novel text-to-4D synthesis\nframework that enables realistic complex scene transitions. Specifically, we\nfirst use multi-modal large language models (MLLMs) to produce a physic-aware\nscene description for 4D scene initialization and effective transition timing\nplanning. Then we propose a geometry-aware 4D transition network to realize a\ncomplex scene-level 4D transition based on the plan, which involves expressive\ngeometrical object deformation. Extensive experiments demonstrate that Trans4D\nconsistently outperforms existing state-of-the-art methods in generating 4D\nscenes with accurate and high-quality transitions, validating its\neffectiveness. Code: https:\/\/github.com\/YangLing0818\/Trans4D","url":"http:\/\/arxiv.org\/abs\/2410.07155v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.07155v1","published":1728496563000,"comment":"Project: https:\/\/github.com\/YangLing0818\/Trans4D","pdf_text":"Preprint.\nTRANS4D: REALISTIC GEOMETRY-AWARE TRANSI-\nTION FOR COMPOSITIONAL TEXT-TO-4D SYNTHESIS\nBohan Zeng1∗, Ling Yang1†∗, Siyu Li1, Jiaming Liu1, Zixiang Zhang1, Juanxi Tian1,\nKaixin Zhu1, Yongzhen Guo1, Fu-Yun Wang2, Minkai Xu3, Stefano Ermon3, Wentao Zhang1†\n1Peking University\n2The Chinese University of Hong Kong\n3Stanford University\nhttps:\/\/github.com\/YangLing0818\/Trans4D\nABSTRACT\nRecent advances in diffusion models have demonstrated exceptional capabilities in\nimage and video generation, further improving the effectiveness of 4D synthesis.\nExisting 4D generation methods can generate high-quality 4D objects or scenes\nbased on user-friendly conditions, benefiting the gaming and video industries.\nHowever, these methods struggle to synthesize significant object deformation of\ncomplex 4D transitions and interactions within scenes. To address this challenge,\nwe propose TRANS4D, a novel text-to-4D synthesis framework that enables re-\nalistic complex scene transitions. Specifically, we first use multi-modal large\nlanguage models (MLLMs) to produce a physic-aware scene description for 4D\nscene initialization and effective transition timing planning. Then we propose a\ngeometry-aware 4D transition network to realize a complex scene-level 4D transi-\ntion based on the plan, which involves expressive geometrical object deformation.\nExtensive experiments demonstrate that TRANS4D consistently outperforms exist-\ning state-of-the-art methods in generating 4D scenes with accurate and high-quality\ntransitions, validating its effectiveness.\n1\nINTRODUCTION\nRecent diffusion model (DM) advances have revolutionized video and 3D synthesis. By harnessing\nthe generative capability of DM, video generation methods (Liu et al., 2024b; Bao et al., 2024) have\nachieved high-quality video production that meets commercial standards. DreamFusion (Poole et al.,\n2023) introduced Score Distillation Sampling (SDS) to guide NeRF model optimization, marking a\nsignificant breakthrough in high-fidelity 3D generation.\nBuilding on these remarkable breakthroughs, 4D generation methods have demonstrated impressive\nperformance. These methods can be broadly categorized into three types: text-to-4D (Singer et al.,\n2023; Bahmani et al., 2024b; Zheng et al., 2024; Ling et al., 2024), single-image-to-4D (Zhao et al.,\n2023; Zheng et al., 2024), and monocular-video-to-4D (Ren et al., 2023; Jiang et al., 2024; Yin et al.,\n2023; Zeng et al., 2024; Zhang et al., 2024b; Wang et al., 2024a). Text-to-4D and Image-to-4D\nmethods (Yu et al., 2024; Bahmani et al., 2024b; Zheng et al., 2024) combine video and multi-view\ngeneration models with SDS to synthesize 4D objects, though the motion remains limited due to\ncurrent constraints in video generation models. Monocular-video-to-4D methods (Jiang et al., 2024;\nWang et al., 2024a) utilize prior dynamics from video conditions to achieve high-quality 4D object\nsynthesis with large-scale and natural motion, constrained by the requirement for videos with clear\nforeground subjects that are difficult to obtain. However, these methods primarily address local\ndeformations of individual objects and fall short of generating complex 4D scenes that involve global\ninteractions between multiple objects.\nRather than merely focusing on 4D object generation, text-to-4D methods like Comp4D (Xu et al.,\n2024) and monocular-video-to-4D methods such as Dreamscene4D (Chu et al., 2024) have achieved\n4D scene generation. These methods still use deformation networks to adjust local coordinates and\nsimulate movements of objects within 4D scenes, similar to 4D object generation methods. However,\n∗Contributed equally.\n†Corresponding authors: yangling0818@163.com, wentao.zhang@pku.edu.cn\n1\narXiv:2410.07155v1  [cs.CV]  9 Oct 2024\nPreprint.\nView 1\nView 2\nView 1\nView 2\n“The missile collided with the plane and exploded”\nComp4D\nTrans4D\nTime\nFigure 1: Comparing our TRANS4D with Comp4D (Xu et al., 2024) in 4D scene transition generation.\ndeformation networks are limited in handling significant object deformation in the 4D scene, which\ncomplicates the generation of 4D transitions with complex interactions, such as a missile transforming\ninto an exploded cloud or a magician conjuring a dancer.\nTo address these challenges, we propose a text-to-4D method TRANS4D, which leverages multimodal\nlarge language models (MLLMs) for geometry-aware 4D scene planning, and introduces a Transition\nNetwork to simulate significant objects deformation within the generated 4D scenes. Unlike existing\nMLLMs that primarily describe or recognize input conditions, or methods like Comp4D (Xu et al.,\n2024) that focus on basic object trajectory function, we propose Physics-aware 4D Transition\nPlanning method that enables MLLMs to generate detailed physical 4D information, including\ninitial positions, movement and rotation speeds, and transition times. This allows for more precise\n4D scene initialization and transition management. The Transition Network further realizes the\ntransition process by predicting whether each point in the 3DGS model should appear or disappear\nat a specific time t. This capability ensures great control over transitions, enabling large-scale\nobject transformations to be handled naturally and seamlessly, such as a missile transforming into an\nexploded cloud. As demonstrated in Fig. 1, our method achieves more natural and coherent 4D scene\nsynthesis with complex interactions than existing text-to-4D scene generation techniques.\nThe main contributions of TRANS4D can be summarized as:\n• In this work, we introduce a text-to-4D generation method called TRANS4D, which enables\ncomplex 4D scene synthesis and facilitates geometry-aware 4D scene transitions. Even\nif the 4D scene contains complex interactions or significant deformation among multiple\nobjects, our method can stably generate high-quality 4D scenes.\n• We present a Physics-aware 4D Transition Planning method, which sequentially leverages\nMLLM to perform physics-aware prompt expansion and transition planning. This approach\nensures effective and reasonable initialization for 4D scene generation.\n• We propose a geometry-aware Transition Network that achieves natural and smooth\ngeometry-aware transitions in 4D scenes.\n• Comprehensive experiments demonstrate that our TRANS4D generates more realistic and\nhigh-quality complex 4D scenes than existing baseline methods.\n2\nPreprint.\n2\nBACKGROUND & PROBLEM STATEMENT\n2.1\n4D CONTENT GENERATION\nResearch on 4D content generation begins with reconstructing dynamic 3D representations based\non multi-view videos. Existing 4D reconstruction models (Pumarola et al., 2021; Wu et al., 2024a;\nHuang et al., 2024) achieve realistic 4D generation by extending 3D models such as NeRF and 3DGS.\nHowever, obtaining multi-view videos for 4D synthesis is challenging. Recently, more researchers\nhave focused on 4D generation using simpler conditions, and these methods can be broadly divided\ninto three categories: text-to-4D, image-to-4D, and monocular-video-to-4D. The text-to-4D (Singer\net al., 2023; Bahmani et al., 2024b; Ling et al., 2024; Yu et al., 2024) and image-to-4D (Zhao et al.,\n2023; Zheng et al., 2024) methods are the first to be explored by researchers, typically extending\n3D objects into 4D objects using SDS loss based on pretrained video DM. However, due to the\nlimitations of SDS loss based on video DM, the dynamics of these 4D objects often seem unrealistic.\nSubsequently, some methods (Yin et al., 2023; Jiang et al., 2024; Zeng et al., 2024; Zhang et al.,\n2024b; Wang et al., 2024a) leverage monocular video as a condition to generate high-quality and\nnaturally dynamic 4D objects. Nevertheless, generating 4D scenes remains challenging for these\nmethods, as they often require monocular videos with clear foreground subjects, which are difficult\nto obtain. The text-to-4D method (Xu et al., 2024; Bahmani et al., 2024a), and the monocular-video-\nto-4D method (Chu et al., 2024), can generate 4D scenes, but they struggle with situations involving\ngeometrical 4D scene transitions. To address this, we propose TRANS4D, which enables the stable\nand convenient generation of 4D scenes with physical 4D transitions.\n2.2\nGENERATION WITH LARGE LANGUAGE MODEL\nInspired by the advancements in LLMs and MLLMs (Touvron et al., 2023; Liu et al., 2024a; Lin\net al., 2023a; Hong et al., 2023; Qi et al., 2024), many works have leveraged these models to achieve\nhigher-quality generation. In image generation (Dong et al., 2023; Zeng et al., 2023b; Yang et al.,\n2024a; Hu et al., 2024; Han et al., 2024; Berman & Peysakhovich, 2024) and image editing (Fu et al.,\n2024; Li et al., 2024b; Jin et al., 2024; Tian et al., 2024a; Yang et al., 2024b), LLMs are first utilized\nto enhance the quality of output images. Thanks to the powerful planning abilities of LLMs, these\nimage generation and editing methods can handle more complex scenarios. Subsequently, with the\nresearch surge sparked by Sora (Liu et al., 2024b), more and more video generation methods (Zeng\net al., 2023c; Bao et al., 2024; Wu et al., 2024c; Tian et al., 2024c; Maaz et al., 2024) and storytelling\napproaches (Soldan et al., 2021; Tian et al., 2024b; Yang et al., 2024c) have harnessed the impressive\ncapabilities of LLMs to achieve coherent and realistic video synthesis, significantly contributing to\nthe multimedia industry’s development. Furthermore, with advancements in text-to-3D techniques\n(Poole et al., 2023; Lin et al., 2023b; Wang et al., 2024b; Zeng et al., 2023a; Liang et al., 2024), some\n3D (Sun et al., 2023; Feng et al., 2023; Li et al., 2024a; Chen et al., 2024b; Zhou et al., 2024) and\neven 4D (Xu et al., 2024; Wang et al., 2024a; Chu et al., 2024) generation methods now involve LLMs\nto produce high-fidelity 3D or 4D outputs with complex geometrical structures based on simple\nconditions. However, simultaneously planning temporal progression and spatial layout remains\nchallenging for existing LLM and MLLM methods, making generating highly complex 4D scenes\ndifficult. In this work, we equip MLLMs with enhanced capabilities for 4D planning, enabling more\neffective generation of complex 4D scenes.\n2.3\nTRANSITION GENERATION\nAccording to the current research landscape, video transition synthesis is less explored than the more\npopular text-to-video and image-to-video generation methods. However, this direction is crucial in\ngenerating complex scenes and long stories. Scene transitions link two consecutive periods smoothly\nthrough location, setting, or camera viewpoint changes. This seamless transition ensures the coherent\nprogression of the scene or story. Before video scene transitions, related research primarily focused\non non-deep learning algorithms with fixed patterns, as well as Morphing (Wolberg, 1998; Shechtman\net al., 2010) that identify pixel-level similarities and generative models (Van Den Oord et al., 2017;\nGal et al., 2022) that leverage latent features of linear networks to achieve smooth and reliable\ntransitions. Recent works (Chen et al., 2023; Ouyang et al., 2024; Xing et al., 2024; Feng et al.,\n2024; Zhang et al., 2024a) have advanced the field by enabling smooth and creative video transitions,\n3\nPreprint.\n(a) Full Pipeline of Trans4D\n(b) Physics-aware 4D Transition Planning\n(c) Transition Network for 4D Scene Transition\nPhysics-aware 4D \nTransition Planning\nProvided \ntextual \nprompts\n𝑦\nDetailed 4D \nscene data\n𝑦𝑖\n𝑝𝑜𝑠𝜂\n𝑠𝑝𝑑𝜈\n𝑎𝑛𝑔𝜙\n𝑟𝑜𝑡𝜛\n𝑇𝑡𝑟𝑎𝑛𝑠\n4D Scenes \nInitialization\n“The missile collided with the plane and exploded”\n× 𝑀\n3DGS models\nInitial 4D scene\n𝑡\nTransition Network\n𝑝trans\nmodify the opacity of each by 𝑝𝑡𝑟𝑎𝑛𝑠\nchoose whether each point appears with 𝑝𝑡𝑟𝑎𝑛𝑠\nMain objects: Missile (0), airplane (1), smoke after \nexplosion (2).\nExpansion description: The missile's initial \ncoordinates are (-2.0, 0.0, 0.0). The airplane's \ninitial coordinates are (2.0, 0.0, 0.0). The missile \nmoves along the x-axis at a speed of 3\/48 per \nframe. The airplane moves in the opposite \ndirection along the x-axis at a speed of -3\/48 per \nframe. …… In time 0.6, an explosion happens, \nSmoke appears, and the missile disappears. The \nmissile (number 0) transitions into smoke (number \n2) during the time interval from 0.54 to 0.64.\nPrompt Expansion \npos = [(-2.0,0.0,0.0), \n(2.0,0.0,0.0),(0.0,0.0,0.\n0),],\nspd = [[(3.0,0.0,0.0), \n(0.0,0.0,0.0)], [(-\n3.0,0.0,0.0),(-1.0,-\n1.0,0.0)],[(0.0, 0.0, \n0.0)],],\nPlanning Result \nPlanning By Trans4D \n𝑻𝒔𝒑𝒅 = [[0.6],[0.6],[],],\nang = \n[(0,0,0),(0,180,0),(0,0,0),],\nrot = [[(10, 0, 0)],[(0, 0, \n0),(0,0,20),(0,0,0)],[(0,0,0)],],\n𝑻𝒓𝒐𝒕 = [[],[0.6, 0.64],[],],\n                    ……\n𝑻𝒕𝒓𝒂𝒏𝒔 = [(0.54, 0.64)]\nPlanning By \nComp4D \nScene \nDecomposition, \nPrompt \nAugmentation, \nand Trajectory \nDesigned by \nLLM.\nTrajectory \nfunction \n𝑥= 𝑓(·)\nLinear layers\nSigmoid\n4D Scene Transition\noptimize the Deformation \nnetwork, and the \nTransition network\n4D Transition During Training\n4D Transition During Inference\n𝑡\n𝑡\nFigure 2: Overview of our TRANS4D, consisting of physics-aware 4D Transition Planning and\nTransition Network that enable 4D scene generation with complex interaction.\npaving the way for the creation of story-level, long-form videos. In addition to the video transition,\nour work first involves the geometry-aware transition into the text-to-4D synthesis.\n3\nTRANS4D\nOur TRANS4D is designed to achieve reasonable physical 4D scene transitions, as illustrated in\nFig. 2(a). This section will explain how TRANS4D performs physics-aware 4D scene planning and\naccomplishes geometry-aware 4D transitions.\n3.1\nPRELIMINARIES\n3D Gaussian Splatting.\n3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) G consists of N\nGaussian points {gi, i = 1, 2, ..., N}, and each point is defined with a center position µ, covariance Σ,\nopacity α, and color c. Each point gi is represented by a Gaussian distribution, and during rendering,\nthe formula can be expressed as:\nG(x) =\nN\nX\ni=1\nαi · ci · exp\n\u0012\n−1\n2(x −µi)⊤Σ−1\ni (x −µi)\n\u0013\n,\n(1)\nwhere x is an arbitrary position in space during the rendering process.\nText-to-4D Generation.\nBefore introducing our method, defining the input and output of the\ntext-to-4D scene generation task is essential. In this work, the input is a text prompt y, and the output\nis a 4D scene represented by M 3D Gaussian Splatting (3DGS) models {Gi, i = 1, 2, ..., M}, along\nwith a deformation network {Di, i = 1, 2, ..., M} corresponding to each 3DGS model. Typically, the\n4\nPreprint.\ndeformation network is represented by a multi-layer perceptron (MLP):\nD(x, q, t) = (∆xt, ∆qt),\nxt = x + ∆xt,\nqt = q + ∆qt,\n(2)\nwhere x and q denote the arbitrary position and orientation within the 3DGS model, and xt and qt\nrepresent the corresponding position and orientation at time t.\n3.2\nPHYSICS-AWARE 4D TRANSITION PLANNING\nTo optimize the 4D scene effectively, it is crucial to plan the placement and trajectories of objects\nwithin the generated scene based on the textual prompts y. This process includes determining\nwhich objects to generate, as well as specifying their initial positions η, movement speeds v, initial\norientation angles ϕ, rotational speeds ϖ, and scene transition times Ttrans. Unlike Comp4D (Xu\net al., 2024), which only uses LLMs to predict simple trajectory functions for 4D synthesis, our\nTRANS4D method leverages MLLM vision-language priors and introduces a physics-aware prompt\nexpansion and transition planning approach. This advancement facilitates more reliable and complex\ninitialization of 4D scenes.\nPhysics-aware Prompt Expansion and Transition Planning.\nThe target of 4D planning is to\nderive spatial and temporal information from a given textual prompt. However, spatiotemporal data\nin a 4D scene are abstract and complex, making it difficult for LLMs or MLLMs to directly interpret\nand generate accurate physics-aware 4D scene data from a simple textual prompt. To overcome\nthis challenge, we propose a physics-aware 4D prompt expansion and transition planning method.\nFirst, the method applies physical principles to analyze the original prompt, deriving spatiotemporal\ninformation and decomposing it into scene prompts {yi, i = 1, 2, ..., M}. These prompts guide the\ncreation of 3D objects within the scene. By utilizing both these prompts and the language-vision\npriors of MLLM, we extend the original textual input into a comprehensive, physics-aware scene\ndescription for the target 4D scene. This description provides specific details, including the placement\nof objects, their movements, and rotations along the x, y, and z axes over time, as well as key events\n(e.g., changes in motion speed or the appearance and disappearance of objects). By converting this\ndescription into a specific data format, the desired 4D scene data is obtained. As illustrated in Fig 2(b),\nthis method enables MLLM to generate detailed and physically plausible 4D scene data, including η,\nv, ϕ, ϖ, and Ttrans. The detailed reasoning prompts are provided in the Appendix.\nInitialization of 4D Scene.\nBased on the {yi, i = 1, 2, ..., M} obtained through the planning\nmethod, we utilize SDS with text-to-image generation model (Ye et al., 2023) to guide basic 3DGS\nmodels {Gi, i = 1, 2, ..., M} synthesis. Using the planning 4D scene data, We calculate the transfor-\nmation function for any position within these 3DGS models at each time t as:\nx = R(ϕ + ϖ · t)xξ + η + v · t\n(3)\nwhere R denotes the rotation matrix, x represents the arbitrary position in the 3DGS model, and xξ is\nthe coordinate of x when the 3DGS model is at (0, 0, 0). By integrating {Gi, i = 1, 2, ..., M} with\nthe transformation function, we obtain an initial 4D scene.\nAfter obtaining the physics-aware planning, we use geometry-aware 4D transitions to effectively\nvisualize the physical dynamics derived from this planning. In the next section, we detail how our\nproposed transition network realizes these geometry-aware 4D transitions.\n3.3\nGEOMETRY-AWARE 4D TRANSITION\nBy utilizing the initial 4D scene and the deformation network, we can achieve 4D scene synthesis\nin certain scenarios through global object positioning and local dynamics. However, depending\nexclusively on movement is limited, as it cannot support geometry-aware 4D transitions that involve\nsignificant object deformation, such as the appearance or disappearance of objects in a 4D scene.\nTo overcome this limitation, we propose a geometry-aware Transition Network (TransNet), which is\na multi-layer perceptron (MLP) with a Sigmoid activation function at the output layer. As shown\nin Fig. 2(c), TransNet takes the position of the point cloud and the time t as inputs, and processes\nthem through several linear layers to produce an intermediate output. This intermediate output is then\n5\nPreprint.\nscaled by a coefficient wtrans before inputting into the final Sigmoid function. The final output of\nTransNet denotes as ptrans, which lies between 0 and 1 and serves as a reference for 4D transition.\nptrans = σ(wtrans · h(xt, qt, t)),\n(4)\nwhere h(xt, qt, t) represents the intermediate output from the linear layers of TransNet, σ is the\nSigmoid activation function, and wtrans is a scaling coefficient, typically set to 10 or higher, to\namplify the changes of the point cloud over time t.\nDuring the training stage, to ensure that TransNet is differentiable, we modify the opacity of each\npoint cloud by multiplying the opacity α directly with ptrans. During the inference stage, to ensure a\nnoticeable transition, ptrans is used to determine whether each Gaussian point of the 3DGS model\nappears in the 4D scene. This method enables a smooth and natural 4D scene transition. The\ncalculation process is as follows:\nB =\n\u001a1,\nwith probability ptrans,\n0,\nwith probability 1 −ptrans,\n(5)\nWhen B = 1, the point cloud appears in the 4D scene; otherwise, it does not. Compared to manually\nconstraining the number of points in the 3DGS model at different time intervals, TransNet allows\nfor flexible and rational control of point variations during the transition process, effectively achieving\ndesired geometry-aware 4D scene transitions.\n3.4\nEFFICIENT 4D TRAINING AND REFINEMENT\nConventional text-to-4D optimization strategies typically rely on SDS loss based on video DM to\nproduce 4D results with reliable dynamics, which incurs high computational costs. To efficiently\nachieve high-fidelity 4D scene synthesis with realistic dynamics, we optimize TRANS4D in two\nphases: first, we train the deformation network and TransNet using 3DGS models with a relatively\nsmall number of point clouds, minimizing costs even with SDS based on video DM. Then, we refine\n3DGS models, allowing for increased point cloud counts with lower computational overhead.\nDuring the training of the deformation network and TransNet, the number of points in each 3DGS\nmodel is fixed at 20,000. We represent the rendered images of the 4D scene over 16 consecutive\ntimes t as {I1, I2, ..., I16}. For SDS loss, noise is added to the rendered images, represented as\nI1\nt , I2\nt , ..., I16\nt\nat timestep t′. We optimize deformation network and TransNet using SDS based on\nvideo DM ϵvid, which can be expressed as:\n∇θdynLSDS−vid({I1, I2, ..., I16}, y) =\nEt′,ϵ\n\u0014\nw(t′)\n\u0010\nϵvid({I1\nt′, I2\nt′, ..., I16\nt′ }, y, t′) −ϵ\n\u0011∂{I1, I2, . . . , I16}\n∂θdyn\n\u0015\n,\n(6)\nwhere θdyn represents the parameters of deformation network and TransNet. During this training\nstage, the points in the 3DGS model are neither cloned nor split, ensuring efficient training of both\nnetworks. To further enhance the quality of the 4D scenes, we use an SDS loss based on text-to-image\nDM ϵimg to supervise further optimization of the 3DGS model. At this stage, the points in the 3DGS\nmodel are cloned and split for the refinement:\n∇θGLSDS(I, y) = Et′,ϵ\n\u0014\nw(t′)\n\u0010\nϵimg(I, y, t′) −ϵ\n\u0011 ∂I\n∂θG\n\u0015\n,\n(7)\nwhere I represents the rendered result of the 4D scene at a random time t, and θG represents the\nparameters of the 3DGS model. Meanwhile, the inputs to the deformation network and TransNet\nconsist solely of the positions of the 3DGS model’s points. Therefore, even after the refinement stage,\nwhile the 3DGS models in the 4D scene become more detailed and realistic, the dynamics of the 4D\nscene remain unaffected.\n6\nPreprint.\nConsistent4D\n4D-fy\nConsistent4D\n4D-fy\nDream-in-4D\nComp4D\nDream-in-4D\nComp4D\nThe magician conjured a dancer\nOurs\nOurs\nA pigeon appeared from a top hat\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\nView 1\nView 2\nView 1\nView 2\n𝒕\n𝒕\nFigure 3: Qualitative comparison with previous baseline methods (Bahmani et al., 2024b; Zheng et al.,\n2024; Jiang et al., 2024; Xu et al., 2024). Our method achieves smoother geometric 4D transitions\nand produces more realistic object interactions within 4D scenes.\n7\nPreprint.\nTable 1: Quantitive comparison of text-to-4D generation.\nMetrics\nConsistent4D\n4D-fy\nDream-in-4D\nComp4D\nTRANS4D (Ours)\nQAlign-vid-quality ↑\n2.275\n3.017\n3.035\n2.961\n3.226\nQAlign-vid-aesthetic ↑\n1.924\n2.089\n2.111\n1.774\n2.148\nVid-MLLM-metrics ↑\n0.5931\n0.4347\n0.5063\n0.5532\n0.6483\nCLIP-score ↑\n0.2836\n0.2661\n0.2607\n0.2757\n0.2941\nUser study ↑\n0.72\n0.64\n0.67\n0.59\n0.78\n4\nEXPERIMENTS\nImplementation Details.\nIn this work, all experiments are conducted on four A100-SXM4-80GB\nGPUs. In Stage 1, we optimize for 5000 steps using the Adam optimizer (Kingma, 2014) to obtain\nthe 3DGS models. In Stage 2, we perform 4500 optimization steps to train the deformation network\nand the Transition Network. During the refinement phase, we further optimize the 3DGS models for\nobjects that cannot be represented in high quality with only 20000 points (e.g., complex structures\nlike ”volcano”). This refinement is performed over 4000 steps using the SDS loss. We ensure a fair\ncomparison by using the same models across all methods for both generation and supervision. For\nany use of a text-to-image generation model, we use Stable Diffusion 2.1 (Rombach et al., 2022); for\nany use of a multiview generation model, we use MVDream (Shi et al., 2024); and for any use of a\ntext-to-video generation model, we use VideoCraft (Chen et al., 2024a).\nBaseline Methods.\nTo validate the effectiveness of our method in generating complex 4D scenes\nwith geometry-aware 4D transitions, we compare it with several different 4D generation methods.\nThese methods include text-to-4D-object methods 4D-fy (Bahmani et al., 2024b) and Dream-in-4D\n(Zheng et al., 2024), a monocular-video-to-4D-object method Consistent4D (Jiang et al., 2024), and\na text-to-4D-scene method Comp4D (Xu et al., 2024).\nMetrics.\nDue to the lack of visual ground truth in text-to-4D generation tasks, we employ QAlign-\nvid-quality and QAlign-vid-aesthetic metrics (Wu et al., 2024b) to evaluate the quality and aesthetics\nof the generated 4D scenes. To assess the semantic alignment of the generated results, we utilize the\nCLIP-score (Park et al., 2021) and MLLM-score. Additionally, we conduct a user study to enhance\nthe credibility of our comparison results. More details on QAlign-vid-quality, QAlign-vid-aesthetic,\nCLIP-score, MLLM-score, and the user study are provided in the Appendix.\n4.1\nTEXT-TO-4D SYNTHESIS\nQuantitative Results.\nTo assess the effectiveness of TRANS4D in complex 4D scene synthesis,\nwe utilize 30 complex textual prompts for 4D scene synthesis. Most of these prompts involve\ngeometry-aware transitions, with the specific prompts detailed in the supplementary material. As\nshown in Table 1, TRANS4D surpasses other methods across all metrics. The text-to-4D methods,\n4D-fy and Dream-in-4D, achieve high scores on the metrics utilized Q-align, demonstrating their\nability to generate high-quality 4D scenes. However, they perform poorly on the CLIP and MLLM\nscores, highlighting that it remains challenging for them to generate 4D scenes that accurately align\nwith the input text. Additionally, our TRANS4D achieved the highest score in the user study, further\nvalidating its effectiveness.\nQualitative Results.\nTo intuitively demonstrate the superiority of our method in generating complex\n4D scenes, that have significant object deformations, we conduct a qualitative comparison with other\nbaseline models. As shown in Fig. 3, the rendered videos of the 4D outputs generated by our\nmethod exhibit the most reasonable and high quality. Additionally, while 4D-fy and Dream-in-4D\nalso produce high-quality visual outputs, these text-to-4D-object generation methods struggle to\ncreate 4D scenes with coherent dynamics based on textual requirements. Lastly, the results from\nConsistent4D indicate that monocular-video-to-4D generation methods perform better for simple 4D\nobject generation. However, when the monocular video involves complex dynamics and interactions\n(as in the visualization example, ”The magician conjured a dancer”), these methods struggle to\nproduce satisfactory 4D outputs. Moreover, acquiring a monocular video with both clear subjects\n8\nPreprint.\nFigure 4: Additional user study for model analysis.\nView 1\nView 2\nw\/o Physics-aware \nprompt expansion\nLLM w\/ Physics-aware \nprompt expansion\nfull Physics-aware 4D \nplanning method\nFigure 5: Ablation study of Physics-aware 4D Transition Planning method.\nand reasonable dynamics is inherently challenging. Therefore, our TRANS4D is currently the most\nconvenient and reliable method for generating complex 4D scenes.\n4.2\nMODEL ANALYSIS\nTo highlight the key contributions of TRANS4D, including Physics-aware 4D Transition Planning\nand the Transition Network, we conduct additional user studies to demonstrate the effectiveness of\nour proposed models. Furthermore, we incorporate visual comparisons to showcase the necessity and\nbenefits of refinement.\nRationality of MLLM-planned Trajectory.\nWe have demonstrated that our method for initial-\nizing 4D scenes outperforms the simple function-based method Comp4D. To further showcase\nthe advantages of our Physics-aware 4D Transition Planning method, we conduct an experiment\nwhere volunteers evaluate videos generated from three different initialization methods: (1) Without\nPhysics-aware prompt expansion: the MLLM receives only one example (including input text and 4D\ndata) to generate 4D scenes based on other input texts; (2) Utilizing an LLM to predict the 4D data\nwith Physics-aware prompt expansion; and (3) Our complete Physics-aware 4D Transition Planning\nmethod. As shown in Fig. 4(a), without Physics-aware prompt expansion, the MLLM struggles to\ngenerate plausible 4D data for scene initialization, resulting in poor outcomes. This underscores the\nimportance of physics-aware prompt expansion. Moreover, when we utilize the LLM to produce\nthe 4D data with Physics-aware prompt expansion, the predicted 4D data lack precision due to the\nabsence of vision-language priors. As illustrated in Fig. 5, incorporating the full Physics-aware 4D\n9\nPreprint.\nw\/o Transition Network\nadjust the opacity by\nutilize             to \ndetermine whether a \npoint appears \nAn angelic girl is becoming a puppet of the devil.\nFigure 6: Ablation study of Transition Network.\nw\/o refinement\nw\/ refinement\nView 1\nView 2\nFigure 7: Ablation study of refinement.\nTransition Planning method significantly enhances the results, highlighting its ability to enrich our\napproach with prior knowledge for more reasonable scene initialization.\nGeometrical Expressiveness.\nTo better observe the effects of the transition network, we decelerate\nthe geometric-aware 4D transition process, allowing volunteers to discern the transition effects. We\nprovide the volunteers with three different videos representing various transition methods: (1) without\nusing the transition network; (2) using the transition network, where ptrans is multiplied by the\nopacity; and (3) using the transition network, where ptrans determines which points should appear.\nThe volunteers are asked to evaluate which process appears more natural. As shown in Fig. 4(b), it\nis evident that the majority of volunteers find the transitions incorporating the transition network to\nbe more natural, with the point selection method receiving the highest scores due to the clearer and\nmore distinct transition. We demonstrate the generated results in Fig. 6, which highlights the pivotal\nsignificance of the proposed transition network in this study.\nEfficiency and Quality of Refinement.\nWhen a 4D scene contains over 200,000 point clouds,\ndirectly supervising it with video SDS loss consumes 80GB or more GPU memory limit, while\nleading to suboptimal quality. In contrast, by separating the training process, we reduce memory\nusage to around 50GB, almost halving the requirement, while significantly improving the quality\nof the generated 4D scene. Specifically, we initially represent the 4D scene using minimal point\nclouds while training the deformation and transition networks. Then, we apply a refinement process\nto improve the quality of each 3DGS model by increasing the number of point clouds as needed.\nThis stepwise training manages memory efficiently while producing high-quality 4D scenes. As\ndemonstrated in Fig. 7, for massive 3D objects like “volcano erupting”, sparse point clouds cannot\nrepresent them effectively. Hence, refining such 3D objects is essential. In conclusion, our training\nstrategy balances efficiency and quality, enabling the generation of high-quality 4D scenes with\nrelatively limited computational resources.\n10\nPreprint.\n5\nCONCLUSION AND FUTURE WORK\nIn this work, we propose TRANS4D, a novel text-to-4D scene generation method that produces high-\nquality 4D scenes involving complex object interactions and significant deformations. Specifically,\nwe introduce a Physics-aware 4D Transition Planning method, which enables MLLM to initialize\nrealistic 4D scenes with multiple interacting objects. To facilitate geometry-aware transitions in\nthe generated 4D scene, we design a Transition Network that dynamically determines whether each\npoint cloud in the 4D scene should appear or disappear, allowing our method to handle substantial\nobject deformations naturally. Our experiments demonstrate that TRANS4D consistently generates\nhigh-quality 4D scenes with complex interactions and smooth, geometry-aware transitions.\nFor future work, We will continue to improve the quality of multi-object interactions in 4D scenes,\nwhich will help achieve more realistic 4D scene generation, and support the development of the video\nmultimedia and gaming industries.\nREFERENCES\nSherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu,\nJeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-\n4d generation. In ECCV, 2024a.\nSherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter\nWonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy:\nText-to-4d generation using hybrid score distillation sampling. In CVPR, 2024b.\nFan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao,\nShilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-\nvideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.\nWilliam Berman and Alexander Peysakhovich. Mumu: Bootstrapping multimodal image generation\nfrom text-to-image data. arXiv preprint arXiv:2406.18790, 2024.\nHaoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR,\n2024a.\nSijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan,\nand Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning\nand planning. In CVPR, 2024b.\nXinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang,\nDahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative\ntransition and prediction. In ICLR, 2023.\nWen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene\ngeneration from monocular videos. arXiv preprint arXiv:2405.02280, 2024.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and\ncreation. In ICLR, 2023.\nYao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J Black. Posegpt:\nChatting about 3d human pose. arXiv preprint arXiv:2311.18836, 2023.\nYutang Feng, Sicheng Gao, Yuxiang Bao, Xiaodi Wang, Shumin Han, Juan Zhang, Baochang Zhang,\nand Angela Yao. Wave: Warping ddim inversion features for zero-shot text-to-video editing. In\nECCV, 2024.\nTsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding\ninstruction-based image editing via multimodal large language models. In ICLR, 2024.\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nStylegan-nada: Clip-guided domain adaptation of image generators. TOG, 2022.\n11\nPreprint.\nYucheng Han, Rui Wang, Chi Zhang, Juntao Hu, Pei Cheng, Bin Fu, and Hanwang Zhang. Emma:\nYour text-to-image diffusion model can secretly accept multi-modal prompts. arXiv preprint\narXiv:2406.09162, 2024.\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang\nGan. 3d-llm: Injecting the 3d world into large language models. In NeurIPS, 2023.\nXiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models\nwith llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.\nYi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs:\nSparse-controlled gaussian splatting for editable dynamic scenes. In CVPR, 2024.\nYanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. Consistent4d: Consistent 360° dynamic\nobject generation from monocular video. In ICLR, 2024.\nYing Jin, Pengyang Ling, Xiaoyi Dong, Pan Zhang, Jiaqi Wang, and Dahua Lin. Reasonpix2pix:\nInstruction reasoning dataset for advanced image editing. arXiv preprint arXiv:2405.11190, 2024.\nBernhard Kerbl, Johannes Hanika, Thomas M¨uller, Harshavardhan Kondapaneni, Francesco Di Gia-\ncomo, Thomas Leimk¨uhler, Chris Chaitanya, Matthias Nießner, and Roland Heged¨us. 3d gaussian\nsplatting for real-time radiance field rendering. In SIGGRAPH, 2023.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nHong Li, Yutang Feng, Song Xue, Xuhui Liu, Bohan Zeng, Shanglin Li, Boyu Liu, Jianzhuang Liu,\nShumin Han, and Baochang Zhang. Uv-idm: Identity-conditioned latent diffusion model for face\nuv-texture generation. In CVPR, 2024a.\nShanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao\nHu, Jianzhuang Liu, et al. Zone: Zero-shot instruction-guided local editing. In CVPR, 2024b.\nYixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer:\nTowards high-fidelity text-to-3d generation via interval score matching. In CVPR, 2024.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, 2023b.\nHuan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your\ngaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In CVPR, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS,\n2024a.\nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang,\nHanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and\nopportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024b.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image\nand video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024.\nYichen Ouyang, Hao Zhao, Gaoang Wang, et al. Flexifilm: Long video generation with flexible\nconditions. arXiv preprint arXiv:2404.18620, 2024.\nDong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for\ncompositional text-to-image synthesis. In NeurIPS, 2021.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. In ICLR, 2023.\n12\nPreprint.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural\nradiance fields for dynamic scenes. In CVPR, 2021.\nZekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and\nKaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. arXiv\npreprint arXiv:2402.17766, 2024.\nJiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaus-\nsian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nEli Shechtman, Alex Rav-Acha, Michal Irani, and Steve Seitz. Regenerative morphing. In CVPR,\n2010.\nYichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. In The Twelfth International Conference on Learning Representations,\n2024.\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation.\narXiv preprint arXiv:2301.11280, 2023.\nMattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, and Bernard Ghanem. Vlg-net: Video-\nlanguage graph matching network for video grounding. In ICCV, 2021.\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt:\nProcedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945, 2023.\nBozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou,\nXi Chen, and Huajun Chen. Instructedit: Instruction-based knowledge editing for large language\nmodels. arXiv preprint arXiv:2402.16123, 2024a.\nChangyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen,\nLewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling\nvia multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024b.\nYe Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen\nYu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation.\narXiv preprint arXiv:2406.04277, 2024c.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.\nYikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single\ngenerated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. arXiv preprint\narXiv:2405.16822, 2024a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In\nNeurIPS, 2024b.\nGeorge Wolberg. Image morphing: a survey. The visual computer, 1998.\nGuanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian,\nand Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR,\n2024a.\nHaoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao,\nAnnan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via\ndiscrete text-defined levels. In ICML, 2024b.\n13\nPreprint.\nJialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long.\nivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv:2405.15223,\n2024c.\nJinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong.\nTooncrafter: Generative cartoon interpolation. arXiv preprint arXiv:2405.17933, 2024.\nDejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N Plataniotis,\nand Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint\narXiv:2403.16993, 2024.\nLing Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering\ntext-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML,\n2024a.\nLing Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan.\nEditworld: Simulating world dynamics for instruction-following image editing. arXiv preprint\narXiv:2405.14785, 2024b.\nShuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen.\nSeed-story: Multimodal long story generation with large language model.\narXiv preprint\narXiv:2407.08683, 2024c.\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt\nadapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and\nFei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality\ncollaboration. In CVPR, 2024.\nYuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content\ngeneration with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023.\nHeng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A\nJeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via\nvideo diffusion models. arXiv preprint arXiv:2406.07472, 2024.\nBohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang,\nJianzhuang Liu, and Baochang Zhang. Ipdreamer: Appearance-controllable 3d object generation\nwith image prompts. arXiv preprint arXiv:2310.05375, 2023a.\nBohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu,\nand Baochang Zhang. Controllable mind visual diffusion model. arXiv preprint arXiv:2305.10135,\n2023b.\nBohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, and Baochang Zhang.\nFace animation with an attribute-guided diffusion model. arXiv preprint arXiv:2304.03199, 2023c.\nYifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun\nCao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. arXiv preprint\narXiv:2403.14939, 2024.\nBowen Zhang, Xiaofei Xie, Haotian Lu, Na Ma, Tianlin Li, and Qing Guo. Mavin: Multi-action video\ngeneration with diffusion models via transition video infilling. arXiv preprint arXiv:2405.18003,\n2024a.\nHaiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion:\nMulti-view video diffusion model for 4d generation. arXiv preprint arXiv:2405.20674, 2024b.\nYuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124:\nAnimating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023.\nYufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. A unified\napproach for text-and image-guided 4d scene generation. In CVPR, 2024.\n14\nPreprint.\nXiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun,\nand Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided\ngenerative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024.\n15\nPreprint.\nA\nAPPENDIX\nIn Appendix A.1, we provide detailed information of our evaluation metrics. Appendix A.2 outlines\nthe specific prompts and process of the Physics-aware 4D Transition Planning method. Finally, in\nAppendix A.3, we present the textual prompts used for evaluation along with additional 4D scenes\ngenerated by TRANS4D.\nTable 2: 4D scene decomposition.\nYou are a 4D Scene Decomposing Agent\nYour task is to decompose the 4D scene into several\nappropriate parts based on the prompt provided by the\nuser. Unlike 3D scene generation methods that only need to\nsplit according to the content of the prompt, you need to\nanalyze the possible physical dynamic that may occur in\nthe provided prompt from both temporal and spatial\ndimensions. Concurrently, based on the analysis results,\ndecompose the provided prompt into several prompts in the\ntime-space dimension.\n--- Main Object ---\nThese objects’ prompts will be used for generating 3D objects\nfirst, and then add time dimension to generate a complete\n4D scene.\nTherefore, if the same object undergoes significant physical\nchanges over time, it should be considered as two separate\nmain objects.\nThe scene’s background is blank, and only moving objects,\nsuddenly appearing objects like clouds and smoke, and\nobjects undergoing shape changes, such as melting or\nbreaking, need to be considered.\n--- Examples ---\n......\nA.1\nDETAILS OF METRICS\nIn this section, we provide a more detailed explanation of the metrics and user studies discussed in\nthe main paper.\nQAlign-vid-quality and QAlign-vid-aesthetic.\nQ-Align (Wu et al., 2024b) is a large multi-modal\nmodel fine-tuned from mPLUG-Owl2 (Ye et al., 2024) using extensive image and video quality-\nassessment datasets. It has demonstrated strong alignment with human judgment on existing quality\nassessment benchmarks. In line with Comp4D (Xu et al., 2024), we use Q-Align to evaluate the\nquality of the generated 4D scenes. Specifically, we input rendering videos of 4D scenes produced by\nvarious methods from viewpoints of -120◦, -60◦, 0◦, 60◦, 120◦, and 180◦into Q-Align. The output\nscores from Q-Align range from 1 (worst) to 5 (best). We calculate the average score of these outputs\nto compare the performance of different 4D generation methods quantitatively.\nCLIP score.\nThe CLIP score (Park et al., 2021) is a widely used metric for evaluating the correlation\nbetween input textual prompts and generated images. Following the approach in 4D-fy (Bahmani\net al., 2024b), we calculate the CLIP score between the frames of the rendered videos and the input\ntextual prompts. Due to the complexity of 4D scene generation, which involves significant object\ndynamics, we use the maximum CLIP score obtained across all frames of each rendered video as the\n16\nPreprint.\nTable 3: Complete Scene Expansion Description\nYou are an Efficient Scene Expansion Agent.\nYour task is to use these decompositional main objects and the\nprompt to expand the provided prompt into a complete\nphysics-aware 4D scene description.\n--- Scene ---\nThe scene is a 4D video clip composed of the main objects\nextracted earlier. The scene information should include:\n- The initial position of each object, represented in the form\n[x, y, z].\n- The movement path of the objects defines the movement vector\nper frame. Each object can have multiple movement segments.\n- The time points when movements start or stop.\n- The initial rotation angle of the objects is expressed in\ndegrees as [rx, ry, rz] (rotation along the x, y, and z\naxes respectively).\n- The rotation path of the objects, defining the rotation\nchange per frame.\n- The time intervals when rotations occur.\n- The time states of the objects, such as when they appear,\ndisappear, or transform at specific times.\n- The transformation relationships between objects, specifying\nwhich objects transform into each other during certain\ntime intervals and when these transformations occur.\nThe time points are represented within a single 4D segment,\nwith 0 indicating the start and 1 indicating the end.\nOther states use decimals to specify the exact time point\nwithin the segment.\nThe scene’s center is [0, 0, 0], and the range for each\ncoordinate axis within the scene is [-1, 1]. Positions\noutside this range are considered outside the scene.\nObjects can enter the scene from outside, but each main\nobject must appear within the scene at some point.\n--- Examples ---\n......\nrepresentative score. To evaluate their performance, we compare the average CLIP scores of rendered\nvideos generated by different methods.\nMLLM score.\nAlthough the CLIP score is a commonly used metric to evaluate semantic alignment,\nit can not fully analyze the reasonability of rendered videos. To more effectively evaluate the semantic\nalignment of the generated 4D results, we propose the MLLM score which leverages the vision-\nlanguage knowledge of GPT4o to evaluate the correlation between the rendered videos and the input\ntextual prompts. Specifically, we present the rendered videos and the provided textual prompts for\nthe ChatGPT-4o. The specific prompt provided for ChatGPT-4o scoring the semantic alignment as:\n“We provide several <video> clips along with a <text prompt>. The videos represent rendered 4D\nscenes from specific viewpoints. Please evaluate the 4D scenes generated by different methods based\non the alignment between the video and the text prompt, as well as the overall video quality, and\nassign a score between 0 and 1.”\n17\nPreprint.\nTable 4: The specific prompt for obtaining 4D planning data.\nYou are a 4D data production Agent.\nYour task is to transfer the complete 4D scene description\ninto precise 4D planning data.\nThe output should be in the json format:\n{\n\"sample\": {\n\"obj_prompt\": [\n\"List of objects involved in the scenario\"],\n\"TrajParams\": {\n\"init_pos\": [\n[x, y, z] \/\/ Initial positions of objects in 3D\nspace],\n\"move_list\": [\n[\n[dx, dy, dz], \/\/ Movement vector\n[dx, dy, dz] \/\/ Additional movement after an\nevent\n] ],\n\"move_time\": [\n[time] \/\/ List of times when movements occur or\nstop],\n\"init_angle\": [\n[rx, ry, rz] \/\/ Initial rotation angles (degrees)\nof objects along x, y, z axes],\n\"rotations\": [\n[\n[rx, ry, rz], \/\/ Rotation vector per frame\n[rx, ry, rz] \/\/ Optional: Additional rotation\nafter an event\n] ],\n\"rotations_time\": [\n[start_time, end_time] \/\/ Times when rotations\noccur],\n......\n\"trans_list\": [\n[obj_index, transition_obj_index] \/\/ Objects that\ntransition into each other],\n\"trans_period\": [\n[start_time, end_time] \/\/ The time period when the\ntransition occurs.]\n}\n}\n}\nUser study.\nFor unsupervised text-to-4D-scene generation, the user study is the most convincing\nmetric. To further validate the effectiveness of our method, we conduct a comprehensive user study\ninvolving 80 volunteers. Each volunteer is randomly provided 10 test examples from the testing\ndataset introduced in this work. For each example, volunteers are asked to judge whether the generated\nresults from various 4D methods successfully achieve the desired 4D synthesis based on the given\ntext inputs. Volunteers rate each result on a scale from 0 to 1, where a score closer to 1 indicates\nbetter alignment with the expected outcome.\n18\nPreprint.\nTable 5: Textual prompts used in the user study.\nThe missile collided with the plane and exploded.\nA cavalry charged two shield-bearing infantry.\nThe magician conjured a dancer.\nThe ice block melts into water.\nThe volcano erupted violently.\nThe tree fell after being cut by the harvester.\nThe water balloon burst on impact.\nThe clock struck midnight.\nThe egg cracked open.\nThe spaceship took off from Earth and entered space.\nThe tornado formed over the plains.\nThe butterfly emerged from the cocoon.\nThe snowflake melted on the tongue.\nThe fish jumped out of the water.\nThe corn kernels pop into popcorn.\nThe moon appeared from behind the clouds.\nA pigeon appeared from a top hat.\nAn angelic girl is becoming a puppet of the devil.\nAn explosion occurs while a wizard is brewing a magic potion.\nA sage caused a gigantic flower to bloom.\nThree worshippers pray for the appearance of an angel.\nA zombie crawls out of the tombstone.\nA dragon breathes fire onto a knight’s shield.\nA giant cracks the ground with its heavy footsteps.\nA knight draws a glowing sword from a stone.\nA sorcerer opens a portal to another dimension.\nA ghost passes through a wall, leaving behind a cold mist.\nA castle tower collapses after being struck by lightning.\nA violin plays itself, filling the air with haunting melodies.\nThe appearance of the sun clears the fog.\nA.2\nMORE DETAILS OF OUR MODEL\nPhysics-aware 4D planning.\nMultimodal Large Language Models (MLLMs), leveraging their\nvision-language priors, have the potential to generate reasonable and natural spatiotemporal data.\nIn this work, we leverage the spatiotemporal awareness of MLLMs to achieve impressive 4D scene\ninitialization and 4D transition planning. During the process of obtaining 4D data, we require the\nMLLM to ensure that the generated plans are consistent with physical principles and geometrically\ncoherent, thereby guaranteeing both physical plausibility and the correctness of spatial relationships.\nSpecifically, Tables 2, 3, and 4, present the detailed prompts for scene decomposition, physics-aware\n4D prompt expansion, and 4D planning data, respectively.\nA.3\nADDITIONAL RESULTS\nTextual prompts used for comparison.\nIn Table. 5, We provide the specific textual prompts used\nin quantitative comparison.\ncomparison results.\nIn Fig. 8, we provide more generated 4D scenes of TRANS4D, to further\ndemonstrate the effectiveness of our method.\n19\nPreprint.\nIce cube melts into water\nThree worshippers pray for the appearance of an angel.\nThe tree cut off by a harvester.\nGreen flames ignited during the wizard's process of brewing the potion.\nFigure 8: Additional generated 4D results, our TRANS4D can consistently produce high-quality 4D\nscenes.\n20\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis.pdf"}
{"title":"ING-VP: MLLMs cannot Play Easy Vision-based Games Yet","authors":"Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, Ge Zhang","summary":"As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps:\/\/github.com\/Thisisus7\/ING-VP.git.","url":"http:\/\/arxiv.org\/abs\/2410.06555v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.06555v1","published":1728451058000,"comment":"49 pages, 12 figures","pdf_text":"Preprint\nING-VP: MLLMS CANNOT PLAY EASY VISION-BASED\nGAMES YET\nHaoran Zhang1∗, Hangyu Guo1∗, Shuyue Guo1, Meng Cao3,\nWenhao Huang1,2, Jiaheng Liu1†, Ge Zhang1,2†\n1M-A-P,\n2Bytedance.Inc,\n3MBZUAI\nhaor7@outlook.com, zhangge.eli@bytedance.com\nABSTRACT\nAs multimodal large language models (MLLMs) continue to demonstrate increas-\ningly competitive performance across a broad spectrum of tasks, more intricate\nand comprehensive benchmarks have been developed to assess these cutting-edge\nmodels. These benchmarks introduce new challenges to core capabilities such\nas perception, reasoning, and planning. However, existing multimodal bench-\nmarks fall short in providing a focused evaluation of multi-step planning based\non spatial relationships in images. To bridge this gap, we present ING-VP,\nthe first INteractive Game-based Vision Planning benchmark, specifically de-\nsigned to evaluate the spatial imagination and multi-step reasoning abilities of\nMLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with\n6 unique configurations. A single model engages in over 60,000 rounds of in-\nteraction. The benchmark framework allows for multiple comparison settings,\nincluding image-text vs. text-only inputs, single-step vs. multi-step reasoning,\nand with-history vs. without-history conditions, offering valuable insights into\nthe model’s capabilities. We evaluated numerous state-of-the-art MLLMs, with\nthe highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy\nof only 3.37%, far below the anticipated standard. This work aims to provide\na specialized evaluation framework to drive advancements in MLLMs’ capacity\nfor complex spatial reasoning and planning. The code is publicly available at\nhttps:\/\/github.com\/Thisisus7\/ING-VP.git.\nw\/ history & w\/o history\nImage-text & Text-only\nMulti-step & One-step\nPerception\nText Understanding\nSpatial Imagination\nPlanning\nReasoning\nING-VP\nBench\n6 Games\n3 Comparisons\n          Capacity                              Evaluation\nMultimodal & Interactive Environment\nPrompts\nImage\/text\nLevels\nModel\n1.question\n1\/4.initial state\nGame Env\n2.output\nDatabase\n2.instruction\n3.next state\n4.next state\n4.history\nConsider the multi-step process with a history-enabled setting:\nThe model ingests game data as input and generates an output in response to the specific prompt.\n1.\nThis output is relayed to a database, which extracts the corresponding instructions and sends them to the game\nenvironment.\n2.\nThe game environment updates its state based on these instructions, returning the new game data to the database.\n3.\nThis updated data, together with the prompt, is used as the next input for the model, which continues to generate\noutputs for the database.\n4.\nThis cycle—steps 2 through 4—is repeated until the model successfully completes the level or the step limit is\nreached.\n5.\nFigure 1: The overview of ING-VP benchmark.ING-VP comprises 6 distinct games, conducts 3\ncomparative analyses across 6 experimental settings, and evaluates 5 key capabilities of MLLMs.\nAdditionally, it offers a highly efficient interactive environment for both inference and analysis.\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated remarkable capabilities in natural language\nprocessing, generation, and even textual complex reasoning and planning (Zhao et al., 2023). Building\nupon this powerful foundation of LLMs, integrating visual inputs has led to the development of even\n∗Equal Contributions, † Corresponding Authors.\n1\narXiv:2410.06555v1  [cs.CL]  9 Oct 2024\nPreprint\nmore powerful models (OpenAI, 2024; Anil et al., 2023a), a.k.a multimodal large language models\n(MLLMs).\nDespite demonstrating impressive performance in handling most general multimodal tasks, the\neffectiveness of MLLM in multimodal reasoning and planning still remains unclear. Moreover, recent\nstudies (Lu et al., 2024; Dai et al., 2024) indicate that vision-language training might degrade the\ntextual capabilities of MLLMs, suggesting that MLLMs built upon LLMs could be impaired when\nadapted to multimodal reasoning and planning tasks. Consequently, there is an urgent need for a\ntest that incorporates multimodal complex reasoning and planning cases to guide the subsequent\nenhancements of MLLMs.\nTo address this issue, existing studies generally utilize visual question answering (VQA) (Antol\net al., 2015; Kafle & Kanan, 2017) and game-based evaluations (Wu et al., 2023; Bellemare et al.,\n2013) to assess the visual reasoning capabilities of MLLMs. In general, VQA necessitates a verified\nground-truth answer that relies on human annotations. But acquiring these annotations is both\ncostly and time-consuming. Moreover, the absence of interaction and planning in typical VQA tasks\nposes difficulties in evaluating the reasoning and planning capabilities of advanced MLLMs. The\ntasks presented in these benchmarks are overly simplistic (Yue et al., 2023) or only test reasoning\nwithin domain-specific knowledge (Yue et al., 2023; Zhang et al., 2024a), which mainly evaluates\nthe LLM knowledge of MLLMs rather than the perception, reasoning, and planning of MLLMs.\nTherefore, recent studies (Xu et al., 2024; Chia et al., 2024) prompt MLLMs to interact with digital\ngame environments, which are measured by game outcomes and scores, leading to the game-based\nevaluation. Unlike VQA tasks, these methods can evaluate the multi-step reasoning capabilities\nand even spatial imagination of MLLMs, which is crucial function of human cognition, allowing\nus to interact with realistic environments (Wu et al., 2024). Despite the effectiveness, these works\nare typically restricted to individual games with complex rules, involve time-consuming evaluation\nepisodes, and fail to effectively assess the models’ generalization capabilities in multimodal planning.\nConsidering these challenges, our goal is to develop a generalizable and efficient benchmark to\nevaluate the multi-step planning abilities of MLLMs, providing insights for subsequent improvements\nof MLLMs with complex multi-step reasoning.\nTo fill this gap, in this paper, we introduce the INteractive Game-based Vision Planning benchmark\n(ING-VP), meticulously focusing on evaluating the spatial imagination and multi-step reasoning\nabilities of MLLMs. Figure 1 shows games, evaluation settings, and the interactive process in our\nING-VP. To construct our ING-VP, we initially collect six games featuring easily understandable\nrules. In each game, we collect 50 levels, each comprising both an image and a text representation of\nthe current state, providing vision and textual inputs for MLLMs, as illustrated in Figure 2. To assess\nthe spatial imagination and planning capabilities of MLLMs, we establish six experimental settings,\nwhich prompt the models to perform single-step and multi-step reasoning, with or without historical\ninteraction. During the evaluation, we employ MLLMs to interact within the environment until the\ngame is completed. To evaluate model performance comprehensively, beyond merely determining\nwhether a model can finish a game, we also use the model’s action efficiency and the remaining steps\nto complete the game as evaluation metrics.\nWith our ING-VP, we test 15 open- and closed-source MLLMs and analyze their performance on\nour test cases. We first support the benchmark designed to evaluate the multi-step reasoning and\nspatial imagination capabilities of MLLMs — ING-VP bench. Then we analyze these capabilities of\ncurrent open- and closed-source MLLMs, despite a performance gap, the leading open-source model,\nInternVL2-Llama3-76B, achieves an accuracy of 2.50%, ranking just behind Claude-3.5 Sonnet,\nGPT-4o, and Gemini-1.5 Pro. Notably, its performance significantly surpasses that of GPT-4o mini,\nwhich stands at 1.05%, and GPT-4v, which records a mere 0.32%. We also conduct a detailed analysis\nof these models’ performance, the evidence shows that:\n• The inability to process the relative positions of elements is one of the primary issues with\nMLLM perception.\n• Even the most advanced MLLMs have very limited planning capabilities, far below the\nperformance of ordinary humans on these simple tasks.\n• Current models tend to generate instructions that are much longer than necessary to complete\nthe levels. While this can improve accuracy on simple levels, it also indirectly reveals that\nMLLMs are “uncertain” about the correct solution.\n2\nPreprint\nWhile most tasks in the ING-VP benchmark are straightforward for humans, they pose significant\nchallenges for MLLMs, even the top-performing model, Claude-3.5 Sonnet, achieving an average\naccuracy of just 3.37%. We reveal that current MLLMs generally lack spatial imagination and\nmulti-step planning abilities, and offer a new perspective on the capability requirements for MLLMs.\nSokoban\nInstruction:\naction: L(Left); R(Right); U(Up); D(Down)\nundo: return to a specified history state\nMaze\nInstruction:\naction: L(Left); R(Right); U(Up); D(Down)\n8-queens\nInstruction:\naction: [x,y], coordinates of chess pieces\nundo: return to a specified history state\nHanoi\nInstruction:\naction: “{x}{y}”, move the top disk from rod x\nto rod y,  smaller on larger\nSudoku\n15-puzzle\nInstruction:\naction: {number}, if the number is around\nthe empty space, , they will swap positions.\nInstruction:\naction: \"{row}{column}\": {number}\nundo: return to a specified history state\nImage\nText\nIntroduction:\nA puzzle game where the player navigates a\nwarehouse, \npushing \nboxes \nto \ndesignated\nstorage locations.\nGoal:\nMove all boxes onto\nthe storage locations.\nImage\nIntroduction:\nA puzzle game where the player navigate\nthrough a complex labyrinth, finding the path\nto the exit.\nText\nGoal:\nMove the red square\nto the green square.\nImage\nText\nIntroduction:\nA chess problem that challenges the player to\nplace eight queens on an 8x8 chessboard so\nthat no two queens threaten each other.\nGoal:\nNo two queens can\nshare the same row,\ncolumn and diagonal.\nImage\nText\nIntroduction:\nA puzzle where the objective is to move a stack\nof disks from one rod to another, following\nspecific rules about disk placement.\nGoal:\nMove all disks to \nthe rod D.\nImage\nText\nIntroduction:\nA sliding puzzle consisting of a 4x4 grid with 15\nnumbered tiles and one empty space.\nGoal:\nSlide the tiles to\narrange them in\nnumerical order.\nImage\nText\nIntroduction:\nA logic-based number-placement puzzle.\nGoal:\nFill a 9×9 grid with digits\nso that each column,\nrow, and 3×3 sub-grid\ncontains all digits from 1\nto 9 without repetition.\nFigure 2: ING-VP examples sampled from each game. Includes pictures and text representations of\nSokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, and 15-puzzle.\n2\nRELATED WORK\nMultimodal Large Language Models. LLMs (Achiam et al., 2023; Anil et al., 2023b) have\ndemonstrated their ability of generating human-like texts to understand and respond to complex\ninstructional queries. The successes of LLMs has elicited the burgeoning proliferation of multi-modal\nLLMs (Alayrac et al., 2022; Li et al., 2023b; Liu et al., 2024; Sun et al., 2024; Jin et al., 2023b),\nwhich is designed to process and integrate multiple types of data. The primary attempt Flamingo\n(Alayrac et al., 2022) endows visual-language models with in-context few-shot learning capabilities\nby trained on large-scale interleaved text-image data. BLIP2 (Li et al., 2023b) designs a Q-Former\narchitecture to align the visual-textual knowledge during the pre-training phase. LLaVA (Liu et al.,\n2024) collect GPT-4 generated multimodal language-image instruction-following data and train a\ngeneral-purpose visual-language assistant. Beyond multimodal understanding, EMU-2 (Sun et al.,\n2024) and LaVIT (Jin et al., 2023b) take one step further and act as generative multimodal model to\nsupport visual prompting and object-grounded generation.\nMLLM Benchmarks. The development of MLLMs has highlighted the critical need of benchmarks\nfor thorough evaluations. Although traditional visual-language tasks (e.g., visual question answering\n(Antol et al., 2015; Kafle & Kanan, 2017) and image captioning (Lin et al., 2014; Plummer et al.,\n2015)) can be used as evaluation benchmarks, they are too strict and require the exact match with\nthe ground-truth answers. To this end, LVLM-eHub (Xu et al., 2023) and LAMM (Yin et al., 2024)\nreformulate exiting public datasets as evaluation samples and employ human annotators or GPT\nto assess the quality. MME (Li et al., 2024), MMBench (Liu et al., 2023b) and SEED-Bench (Li\net al., 2024) construct multiple-choice questions to mitigate the subjectivity and instability of GPT\nevaluation. MMMU (Yue et al., 2024) evaluate the advanced perception and reasoning of MLLMs on\nspecific domains (e.g., science, business).\nGame-based Evaluations. Digital games are acknowledged as essential in the pursuit of artificial\ngeneral intelligence since they present complex challenges requiring advanced reasoning and cognitive\n3\nPreprint\nskills. These challenges make digital games an ideal benchmark for evaluating the capabilities of\nMLLMs (Wu et al., 2023; Bellemare et al., 2013; Hu et al., 2024; Sweetser, 2024; Xu et al., 2024)\nincluding the environment perception (Hong et al., 2023; Akoury et al., 2023), memory construction\n(Zhu et al., 2023; Zhang et al., 2024b; Ding et al., 2023; Park et al., 2022; Liu et al., 2023a), reasoning\n(Liu et al., 2023a; Wang et al., 2023a; Qian et al., 2023; Huang et al., 2022) and decision-making\n(Chen et al., 2023; Zhou et al., 2023; Jin et al., 2023a; Qian et al., 2023). Several methods focus on\nsemantic-level perception of environmental elements including locations, objects or actions in games.\nThey either use basic text input of user ideas (Li et al., 2023a) or game state variables and dialogues\n(Akoury et al., 2023; Park et al., 2022; 2023). Role-based inputs, e.g., the inclusion of character,\nstory, role-related information (Hong et al., 2023; Wang et al., 2023b) and skills (Gong et al., 2023)\nare often included. TorchCraft (Synnaeve et al., 2016) is presented to use real-time strategy games\nsuch as StarCraft: Brood War to serve as a benchmark for AI research. The Chess game has long\nbeen employed as an AI testing ground (Noever et al., 2020; St¨ockl, 2021; Toshniwal et al., 2022).\nChess Transformer (Noever et al., 2020) fine-tunes GPT-2 to generate plausible strategies and learns\ncomplex gameplay. Recent works (Taesiri et al., 2022; 2024) formulate the bug detection problem\nas a question-answering task and leverage the zero-shot capabilities of LLMs for video game bug\ndetection. R2-PLAY (Xu et al., 2024) constructs a multimodal game instruction tuning dataset to\nfacilitate the “read-to-play” capability of LLMs. PuzzleVQA (Chia et al., 2024) demonstrates that\nexisting MLLMs exhibit substantial challenges when solving puzzles that demand visual perception,\ninductive reasoning, and deductive reasoning. Beyond the benchmark setting, we additionally develop\nan interactive environment to assess the ability of multimodal models to perform spatial reasoning\nand multi-step inference based on visual details.\n3\nTHE ING-VP BENCHMARK\n3.1\nOVERVIEW OF ING-VP\nWe introduce ING-VP benchmark, a new interactive game-based vision planning benchmark designed\nto measure the multi-step reasoning and spatial imagination capabilities of MLLMs. The benchmark\nencompasses 6 distinct settings, 6 games, and 50 levels per game, the core mechanisms are depicted\nin Figure 1. To mitigate data leakage and ensure problem solvability, the majority of our levels\nare algorithmically generated and verified. Representative examples of each game are illustrated in\nFigure 2.\nING-VP features 6 games that are conceptually simple yet cognitively challenging: Sokoban, Maze,\nSudoku, 8-queens, Tower of Hanoi, and 15-puzzle. The simplicity lies in the easily comprehensible\nrules and the ability to encapsulate complete level information within a single image, facilitating\ncomprehensive reasoning. The challenge stems from the requirement for models to precisely capture\ncore visual elements and their spatial relationships, necessitating multi-step reasoning to successfully\ncomplete each level. We meticulously craft 6 reasoning settings, enabling researchers to systematically\nidentify the strengths and limitations of target models through comparative analysis of performance\nacross these settings.\n3.2\nSIX INFERENCE SETTINGS\nOne-step: Image and Text-only Settings In the One-step with Image setting, we provide the\nmodel solely with an image depicting the initial game state and prompt it to generate comprehensive\ninstructions for level completion. The One-step Text-only setting follows an identical approach,\nwith the key distinction being the replacement of the image input with its corresponding textual\nrepresentation.\nMulti-step: Image and Text-only Settings (without History) In the Multi-step with Image setting,\nwe provide the model with an image of the current game state at each inference round. After the\nmodel outputs a single-step instruction, this instruction is fed into the game as input, causing the game\nstate to change and generate a new image. This new image then serves as the model’s input for the\nnext step. The Multi-step Text-only setting follows the same process, but uses textual representations\nas the model’s input.\nMulti-step: Image and Text-only Settings (with History) The key distinction in these settings is the\ninclusion of the model’s historical outputs as part of the prompt in each interaction. Additionally, for\n4\nPreprint\nSokoban, Sudoku, and N-queens, we add an undo option, allowing the model to freely revert to any\nprevious state. This enhancement applies to both the Image and Text-only variants of the Multi-step\nsetting.\n3.3\nGAME SELECTION\nWe chose six games that are widely recognized, have straightforward rules, and operate in a determin-\nistic environment, making them ideal representatives for our study. In a deterministic environment,\nthe outcome of every action taken by an agent is predictable and certain. Such an environment can\nbe formally defined using a Markov Decision Process (MDP). The model employs a strategy π to\ndetermine the next action at based on the current state st and all previous actions a0:t−1, represented\nas:\nat = π(st, a0:t−1)\n(1)\nThe planning process of MLLMs can be expressed as:\nS′ = π(S, A, G, n)\n(2)\nWhere S′ is the future sequence of states, which terminates upon achieving the goal G or exhausting\nthe available moves n; S is the current sequence of states; A represents the current sequence of\nactions.\n3.4\nDATA COLLECTION\nSokoban. It involves pushing crates onto designated storage locations within a warehouse maze. We\nselect 50 levels from the Sasquatch dataset 1. To mitigate difficulty and prevent data leakage, we\nemploy the A-star algorithm to constrain each level to a maximum of 8 steps for completion.\nMaze. The Maze game challenges players to navigate from a starting point to a target through a\nnetwork of paths. We employ a Depth-First Search (DFS) algorithm to automatically generate 50\nsolvable levels, each with an 11x11 grid size. We also constrain the solution length to a maximum of\n8 steps.\n8-Queens. The 8-Queens puzzle challenges people to place eight queens on an 8x8 chessboard such\nthat no two queens threaten each other. N-Queens is a special game due to its standard formulation:\nmodels could potentially solve it without visual input, relying solely on memorized patterns from\ntraining data. To ensure that visual reasoning is essential, we modify the puzzle by manually placing\nthe first queen in a different position for each level. The image presented to the MLLMs shows this\ninitial configuration, requiring them to reason from this starting point to complete the puzzle.\nSudoku. Sudoku is a logic-based number placement puzzle that requires filling a 9x9 grid such that\neach row, column, and 3x3 subgrid contains all digits from 1 to 9 without repetition. A well-formed\nSudoku puzzle with a unique solution requires a minimum of 17 initial clues. For our benchmark,\nwe curate a set of 50 puzzles with each puzzle contain 71 clues from a Kaggle dataset 2 , ensuring\neach puzzle meets this criterion. We then manually generate corresponding images for each level to\nmaintain consistency with our benchmark’s visual reasoning focus.\nHanoi The Tower of Hanoi is a classic mathematical puzzle that involves transferring a stack of disks\nof varying diameters from one rod to another, adhering to the constraint that a larger disk must never\nbe placed atop a smaller one. In our implementation, each problem instance consists of four rods and\nfive disks, with an optimal solution requiring a minimum of 8 moves.\n15-Puzzle It’s a classical sliding tile puzzle comprising a 4x4 grid with 15 numbered tiles and one\nvacant space. The objective is to rearrange the tiles into numerical order through a series of sliding\nmovements. In our implementation, we employ the Breadth-First Search (BFS) algorithm to explore\nsolution paths, constraining the search depth to 8 moves as previous games.\n1http:\/\/www.abelmartin.com\/rj\/sokobanJS\/Skinner\/David%20W.%20Skinner%\n20-%20Sokoban.htm\n2https:\/\/www.kaggle.com\/datasets\/informoney\/4-million-sudoku-puzzles-easytohard\n5\nPreprint\nImage-text\nText-only\nMulti-step\nOne-step\nMulti-step\nOne-step\nModel\nMetric\nw\/o history\nw\/ history\nw\/o history\nw\/ history\nOverall\nClosed Source Model\nAcc.\n0.30\n0.30\n7.00\n2.30\n2.30\n8.00\n3.37\nComp.\n3.90\n4.30\n21.90\n4.90\n5.20\n16.80\n9.50\nClaude-3.5 Sonnet\nEff.\n26.90\n23.10\n48.40\n17.60\n18.50\n42.00\n29.42\nAcc.\n3.30\n2.00\n0.30\n3.30\n3.30\n4.30\n2.75\nComp.\n6.70\n5.20\n12.90\n5.80\n5.40\n13.80\n8.30\nGPT-4o\nEff.\n19.20\n14.20\n33.70\n18.70\n18.30\n47.80\n25.32\nAcc.\n1.00\n0.30\n2.70\n5.70\n4.30\n2.30\n2.72\nComp.\n5.90\n3.80\n9.60\n8.20\n6.50\n8.50\n7.08\nGemini-1.5-Pro\nEff.\n34.70\n27.80\n42.80\n19.50\n18.50\n37.70\n30.17\nAcc.\n0.70\n0.30\n0.00\n2.00\n2.30\n1.00\n1.05\nComp.\n3.40\n3.40\n6.60\n5.20\n5.90\n8.90\n5.57\nGPT-4o mini\nEff.\n13.20\n8.20\n35.20\n19.50\n17.30\n40.10\n22.25\nAcc.\n0.00\n0.00\n1.30\n0.00\n0.30\n0.30\n0.32\nComp.\n2.90\n2.90\n4.30\n2.60\n3.00\n3.40\n3.18\nGPT-4V\nEff.\n8.80\n7.20\n5.50\n16.80\n17.40\n8.50\n10.70\nAcc.\nnull\nnull\nnull\n2.30\n2.30\n1.00\n1.87\nComp.\nnull\nnull\nnull\n4.80\n4.80\n9.10\n6.23\nGPT-4 Turbo\nEff.\nnull\nnull\nnull\n12.20\n12.30\n41.00\n21.83\nAcc.\nnull\nnull\nnull\n2.30\n2.30\n1.00\n1.87\nComp.\nnull\nnull\nnull\n4.80\n4.80\n10.70\n5.07\nClaude-3 Opus\nEff.\nnull\nnull\nnull\n12.40\n12.30\n40.80\n21.83\nOpen Source Model\nAcc.\n2.67\n2.33\n3.00\n2.33\n1.67\n3.00\n2.50\nComp.\n9.07\n6.28\n8.30\n8.32\n8.03\n5.88\n7.65\nInternVL2-Llama3-76B\nEff.\n17.55\n15.13\n36.18\n21.13\n29.30\n32.95\n25.58\nAcc.\n2.33\n1.33\n1.67\n1.67\n2.00\n2.33\n1.89\nComp.\n4.80\n5.22\n5.65\n5.25\n5.27\n5.22\n5.23\nInternvl2-26B\nEff.\n10.58\n9.22\n11.93\n10.22\n9.27\n16.72\n11.32\nAcc.\n1.67\n1.67\n2.67\n1.00\n2.00\n1.67\n1.78\nComp.\n5.68\n5.43\n7.87\n5.03\n4.08\n8.08\n6.03\nInternvl2-40B\nEff.\n18.37\n12.98\n22.22\n15.33\n15.22\n34.16\n18.82\nAcc.\n1.33\n0.67\n2.00\n1.67\n1.33\n2.00\n1.50\nComp.\n5.90\n5.68\n6.58\n5.68\n5.02\n7.63\n6.08\nCogvlm2-19B\nEff.\n15.75\n16.45\n27.12\n13.75\n12.85\n31.37\n19.55\nAcc.\n1.00\n0.33\n0.33\n1.33\n0.67\n1.67\n0.89\nComp.\n2.60\n2.58\n3.33\n2.63\n2.50\n3.83\n2.91\nInternvl2-8B\nEff.\n5.90\n5.27\n4.97\n3.05\n4.27\n6.03\n4.91\nAcc.\n0.67\n0.33\n0.00\n0.33\n0.33\n0.67\n0.39\nComp.\n6.30\n6.30\n4.57\n5.80\n6.00\n4.18\n5.53\nInternvl-Chat-v1.5\nEff.\n14.90\n14.22\n25.68\n11.70\n10.87\n27.27\n17.44\nAcc.\n0.67\n0.33\n1.00\n0.33\n0.00\n0.00\n0.39\nComp.\n3.47\n2.72\n3.65\n2.68\n4.18\n3.92\n3.44\ndeepseek-VL\nEff.\n11.80\n11.22\n16.40\n8.38\n9.57\n15.90\n12.21\nAcc.\n0.33\n0\n0\n0.67\n0.33\n0\n0.22\nComp.\n3.78\n3.33\n4.17\n3.62\n2.68\n4.22\n3.63\nMiniCPM-V2.6\nEff.\n11.18\n10.62\n17.73\n10.08\n6.37\n21.88\n12.98\nTable 1: Main results for the best-performing MLLMs (LLMs).\n4\nEXPERIMENTS\nWe conduct a comprehensive evaluation of both open-source and closed-source MLLMs, employ a\nzero-shot setting to faithfully emulate the human puzzle-solving process, given the unique nature of\nour tasks. A uniform set of prompts was applied across all models. The complete set of 36 prompts is\npresented in the Appendix B.\n4.1\nBASELINES\nMLLMs. We consider a comprehensive suite of mainstream large multimodal models. Closed-source\nmodels include GPT-4o, GPT-4o Mini, GPT-4v, GPT-4 Turbo, Claude-3.5 Sonnet, Claude-3 Opus,\n6\nPreprint\nand Gemini-1.5 Pro. Open-source models consist of CogVLM2-19B, DeepSeek-VL, Internvl-Chat-\nv1.5, Internvl2-8B, Internvl2-26B, Internvl2-40B, InternVL2-Llama3-76B, and MiniCPM-V2.6. We\nutilize each model’s official API for closed-source systems or the publicly available checkpoint for\nopen-source implementations, More information of these models can be found in the Appendix A.\nEvaluation. We present a systematic interactive environment for evaluating all MLLMs, where\nmodels interact with the game environment until either completing the task or exhausting the allotted\nsteps. We constrain the model’s output action instructions to JSON format through prompts and\nextract them using regular expressions. The correctly extracted instructions are then used as input\nfor the game environment. After the game state changes, the new state is fed back to the model\nfor the next round of inference. We employ three metrics: accuracy, completion degree, and action\nefficiency. (1) Accuracy is our main metric, it measures whether the model can complete the task\nwithin the specified number of steps. (2) Completion degree is determined by the final state of the\ngame environment after interaction with the model. The closer the final state is to the cleared state,\nthe higher the score; if it deviates, the score decreases accordingly. (3) Action efficiency represents\nwhether each instruction output by the model effectuates a change in the game state. The computation\nmethod for action efficiency is as follows:\nAction Efficiency =\nPn\ni=1\n# of efficient actions for level i\n# of total actions for level i\nn\n4.2\nMAIN RESULTS\nIn this section, we examine the spatial reasoning and planning abilities of current MLLMs using the\nING-VP benchmark. The results are presented in Table 1, please see the Appendix C for the complete\nresults.Our key observations are as follows:\n55.2%\n2.9%\n41.9%\na. Image-text\n58.0%\n42.0%\nb. Text-only\nHanoi\nMaze\n15-puzzle\n8-queens\nSokoban\nSudoku\n0.0\n20.0\n40.0\na\na\na\na\na\na\nb\nb\nb\nb\nb\nb\nError Types\nPerceptual Errors\nTextual Understanding Errors\nPlanning Errors\nFigure 3: Error distribution over Claude-3.5 Son-\nnet’s 555 errors across different tasks and settings.\nThe ING-VP benchmark poses a substantial\nchallenge to current MLLMs: Even the most\nadvanced model, Claude-3.5 Sonnet, achieves\nan accuracy of only 3.37%. In contrast, an av-\nerage human can easily complete all of these\ntasks (8-queens is an exception), highlighting a\nsignificant gap between model performance and\nhuman capabilities on the ING-VP benchmark.\nPerformance disparity between open-source\nand closed-source models persists: While the\nperformance of closed-source models on ING-\nVP is far from satisfactory, they still outperform\nthe open-source models. The best-performing\nopen-source model, InternVL2-Llama3-76B,\nachieves an accuracy of 2.50%, which remains\nlower than Claude-3.5 Sonnet, GPT-4o and\nGemini-1.5 Pro.\nFor MLLMs, the greatest challenge in per-\nception is understanding location informa-\ntion. According to our observations of the infer-\nence results, the most advanced models, such as\nClaude-3.5 Sonnet and GPT-4o, can generally\nidentify the elements present and even count the\nquantity of each in the Sokoban game. However, they struggle to accurately determine precise\nlocation information, leading to very low inference accuracy and degree of task completion.\nMerely breaking down the steps is unhelpful and may even be counterproductive. In text-only\ntasks, Claude-3.5 Sonnet and GPT-4o achieve accuracy rates of 2.30% and 3.30%, respectively, in\nthe multi-step setting, which are lower than their 8.00% and 4.30% accuracy in the one-step setting.\nFor the ING-VP benchmark, thinking step by step does not work and even has a negative effect. We\n7\nPreprint\nbelieve that MLLMs rely heavily on pattern matching based on prior training data, generating outputs\nfrom similar inputs rather than engaging in actual planning.\n4.3\nFINE-GRAINED ANALYSIS\n0\n50\nAcc.\n0\n50\n0\n50\nComp.\n0\n50\n4 step\n8 step\n12 step\n16 step\nClaude-3.5 Sonnet\n25\n50\nEff.\n4 step\n8 step\n12 step\n16 step\nGPT-4o\n25\n50\nImage-text Multi-step w\/o history\nImage-text Multi-step w\/ history\nImage-text One-step\nText-only Multi-step w\/o history\nText-only Multi-step w\/ history\nText-only One-step\nOverall\nFigure 4: Maze level accuracy of Claude-3.5 Son-\nnet and GPT-4o across 4 difficulty levels.\nIn this section, We conduct a comprehensive\nrange of analyses to explore the generative capa-\nbilities of MLLMs in a broader context, while\nalso dissecting the nuanced output tendencies\nof current models. We hope our results can pro-\nvides valuable insights that can inform future\nmodel design and training strategies.\nError Analysis. We collate and analyze 555\nerrors (image-text: 279, text-only: 276) made\nby Claude-3.5 Sonnet in one-step setting, as il-\nlustrated in Figure 3. It is important to note that\nwhile we categorize each case under distinct\nerror types, in many instances the model exhib-\nited errors in both comprehension and reasoning.\nOur classification follows contextual cues: when\nthe model provided invalid instructions from the\noutset, we labele it as an understanding error.\nConversely, if the model deviated from the cor-\nrect solution at an intermediate step, we classify it as a reasoning error. Below, we summarize key\nobservations based on these error types:\n• Perceptual Errors (55.2%\/–%): These errors occur exclusively in the image-text setting.\nWhile current models are generally able to recognize overall attributes of an image—such\nas identifying the game genre and its components, their ability to accurately interpret fine\ndetails, including the specific size and precise location of each element, remains limited\n(e.g., see Figure 7. This perceptual limitation represents a major contributor to the elevated\nerror rates in this setting.\n• Textual Understanding Errors (2.9%\/58.0%): Textual understanding errors manifest in\ntwo main forms: a misinterpretation of specific prompts or an inability to correctly parse\ndata structures or character matrices used to represent game levels in the text-only setting\n(as shown in Figure 8). These errors indicate that the model struggles to generalize its\nunderstanding when presented with text structures not commonly encountered in its training\ndata.\n• Planning Errors (41.9%\/42.0%): Planning errors constitute another major issue for Claude-\n3.5 Sonnet. In these cases, the model initially provides plausible steps but eventually fails\ndue to its inability to correctly track or judge the game state after several steps (see Figure 9).\nThis suggests a breakdown in maintaining consistent reasoning over multi-step processes.\n• Other Errors: During error analysis, we observe that Claude-3.5 Sonnet and GPT-4o never\nrefused to answer queries, and all responses were accurately extracted. However, models\nsuch as GPT-4V displayed issues like refusal to respond or failure to adhere to the required\nresponse format, which hindered our ability to retrieve the outputs.\nPlanning Capacity Analysis. We select the game where models performed best—Maze—and\nintroduced three additional difficulty levels: 4 steps, 12 steps, and 16 steps, by adjusting only the\nnumber of moves required to complete the level, while maintaining the same level structure. This\nallowed us to closely examine the planning capabilities of the most advanced MLLMs, Claude-3.5\nSonnet and GPT-4o, as shown in FIgure 4. Our findings showed a significant decline in both accuracy\nand completion degree as the number of required steps increased. However, action efficiency, which\nemphasizes perception and judgment of the current state, was not notably affected, since modifying\nthe step count without altering the overall layout had little impact on this metric.\nComparative Analysis.\nWe compare the results across different metrics, settings, and models,\naiming to highlight the characteristics of current MLLMs.\n8\nPreprint\nFigure 5: An example showcasing Claude 3.5-Sonnet with a fixed output paradigm.\n• Results differ across metrics. Of the three metrics provided by ING-VP, accuracy—being\nthe most stringent—typically yields the lowest scores. The primary reason action efficiency\nis often significantly higher than both completion rate and accuracy is that models frequently\ngenerate instructions that alter the game state, but these changes have minimal impact on\nsuccessfully completing the level. A notable example is Gemini-1.5 Pro, which achieves an\naverage action efficiency of 76.52% on the 15-puzzle, yet only 0.67% and 3.42% in accuracy\nand completion rate, respectively.\n• Image-text vs. Text-only. Comparing the performance of each model in the image-text and\ntext-only settings, we found that most test subjects performed better in the text-only setting.\nThis highlights that limitations in image comprehension remain a key factor constraining\nthe performance of MLLMs.\n• Multi-step vs. One-step. According to the results in Table 3, for most models, multi-step\nsetting improves accuracy compared to one-step. However, there are exceptions, such as\nClaude-3.5 Sonnet. We compare the output of Claude-3.5 Sonnet and GPT-4o and find that,\ndespite we set the same parameters for closed-source models, Claude-3.5 Sonnet’s sampling\nstrategy is more fixed than GPT-4o’s. As a result, when the model produces an invalid\naction in a certain state, it tends to repeatedly generate the same action until all attempts are\nexhausted. GPT-4o, being more flexible, is better at generating diverse responses. Therefore,\nalthough Claude-3.5 Sonnet performs better than GPT-4o in one-step tasks, the opposite is\ntrue for multi-step tasks. One example is shown in Figure 5.\n• With-history vs. Without-history. In our tasks, incorporating the model’s historical output\nas the input for subsequent rounds did not lead to improved performance. Additionally, we\nintroduce an undo option for Sokoban, Sudoku, and N-Queens in the with-history setting.\nInterestingly, despite the models frequently reaching a state where undoing moves was\nnecessary to complete the level, almost none utilized this feature. This suggests that the\nmodels struggle with processing precise positional information and are unable to accurately\nassess whether the current state is solvable.\n5\nTWO THINKING ABOUT PLANNING\nA holistic approach may outperform a divide-and-conquer strategy. When humans are tasked\nwith completing a planning problem, whether in a single or multi-step process, it typically involves\nthree key phases: understanding the goal, devising a plan, and breaking down the steps. Large\n9\nPreprint\nmodels should operate similarly, yet when presented with the same game level, their outputs differ\nsignificantly between one-step and multi-step settings, as highlighted in Table 1. Notably, even the\ninitial steps diverge between the two approaches. To explore the planning capabilities of the model\nfurther, we employ two methods to adjust the multi-step output:\n• Step-wise Best of N (BoN): The model generates ten candidate responses at each step, with\nthe most frequent answer selected as the final output.\n• Forced Planning: The model is required to complete its entire plan before producing a final\nanswer, akin to the one-step setting.\nFigure 6: An example of results for the Claude-3.5 Sonnet in four settings.\nFIgure 6 illustrates an example of these methods in action, despite these adjustments, the multi-step\napproach failed to match the performance of the one-step setting. This suggests that, for the large\nmodels, even when given identical image, one-step and multi-step tasks are fundamentally different,\nwith the former better eliciting the model’s planning capabilities.\nSmall changes in the prompt phrasing can substantially influence the model’s planning effec-\ntiveness. A thorough comparison of single-step and multi-step outputs reveals not only differences\nbut also distinct tendencies. For instance, in Maze and Sokoban games, Claude-3.5 Sonnet favors ”U\n(Up)” and ”D (Down)” in the one-step mode, whereas it prefers ”L (Left)” and ”R (Right)” in the\nmulti-step mode. Given that most of the prompt wording remains consistent between the two settings,\nour results indicate that subtle variations can profoundly affect the model’s response distribution. We\nleave more detailed experiments as future work.\n6\nCONCLUSION\nIn this work, we introduce ING-VP, an interactive game-based vision planning benchmark designed\nto evaluate the spatial imagination and planning capabilities of MLLMs. Our experimental results\nreveal that even the most advanced MLLMs struggle to achieve satisfactory performance on game\ntasks that humans find trivial. This underperformance stems from multiple factors: existing models\noften fail to generate accurate perceptions of images, and they face even greater challenges in making\ninferences and plans based on their understanding. We believe that ING-VP is of noteworthy to\nthe community’s deeper understanding of MLLMs, and can also advance MLLMs’ capabilities in\ncomprehension and planning within visual contexts.\nLIMITATIONS\nDespite its strengths, ING-VP has certain limitations. We deliberately omit difficulty grading settings.\nIncluding simpler levels would significantly increase the likelihood of models completing tasks by\nchance after sufficient steps, potentially compromising the reliability of our results. Conversely,\nincorporating more challenging levels would yield little insight, given that MLLMs already struggle\nwith current difficulty levels, and could negatively impact inference efficiency. Furthermore, ING-VP\ndoes not exhaustively cover all possible game types. Instead, we focus on selecting well-known\nand representative games to ensure relevance and broad applicability. Finally, to address efficiency\nconcerns, we do not use images of previous states as input in the multi-step with history setting.\nThese considerations provide clear directions for future enhancements to our benchmark.\n10\nPreprint\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nNader Akoury, Qian Yang, and Mohit Iyyer. A framework for exploring player perceptions of llm-\ngenerated dialogue in commercial video games. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 2295–2311, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin\nJohnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler,\nTimothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald\nBarham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan\nDoherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha\nGoel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka,\nBecca Roelofs, Ana¨ıs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran\nKazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of\nhighly capable multimodal models. CoRR, abs\/2312.11805, 2023a.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023b.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international\nconference on computer vision, pp. 2425–2433, 2015.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253–279, 2013.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\nLu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and explor-\ning emergent behaviors. In The Twelfth International Conference on Learning Representations,\n2023.\nYew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puz-\nzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual\npatterns. arXiv preprint arXiv:2403.13315, 2024.\nWenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki,\nMohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms.\narXiv preprint, 2024.\nShiying Ding, Xinyi Chen, Yan Fang, Wenrui Liu, Yiwu Qiu, and Chunlei Chai. Designgpt: Multi-\nagent collaboration in design. In 2023 16th International Symposium on Computational Intelligence\nand Design (ISCID), pp. 204–208. IEEE, 2023.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng,\nSong-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction.\narXiv preprint arXiv:2309.09971, 2023.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\n11\nPreprint\nSihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu.\nA survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022.\nChuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, and Jianlong Fu.\nAlphablock: Embodied finetuning for vision-language reasoning in robot manipulation. arXiv\npreprint arXiv:2305.18898, 2023a.\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru\nSong, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual\ntokenization. arXiv preprint arXiv:2309.04669, 2023b.\nKushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In\nProceedings of the IEEE international conference on computer vision, pp. 1965–1973, 2017.\nBohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan.\nSeed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pp. 13299–13308, 2024.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Com-\nmunicative agents for” mind” exploration of large language model society. Advances in Neural\nInformation Processing Systems, 36:51991–52008, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning, pp. 19730–19742. PMLR, 2023b.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740–755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36, 2024.\nJijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, and Yu Wang. Llm-powered\nhierarchical language agent for real-time human-ai coordination. arXiv preprint arXiv:2312.15224,\n2023a.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\narXiv preprint arXiv:2307.06281, 2023b.\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,\nZhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.\nDeepseek-vl: Towards real-world vision-language understanding. CoRR, abs\/2403.05525, 2024.\nDavid Noever, Matt Ciolino, and Josh Kalin. The chess transformer: Mastering play using generative\nlanguage models. arXiv preprint arXiv:2008.04057, 2020.\nOpenAI. Gpt-4o system card. CoRR, 2024.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Social simulacra: Creating populated prototypes for social computing systems. In\nProceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pp.\n1–18, 2022.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and technology, pp. 1–22, 2023.\n12\nPreprint\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE international conference on computer\nvision, pp. 2641–2649, 2015.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6, 2023.\nAndreas St¨ockl. Watching a language model learning chess. In Proceedings of the International\nConference on Recent Advances in Natural Language Processing (RANLP 2021), pp. 1369–1379,\n2021.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context\nlearners. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npp. 14398–14409, 2024.\nPenny Sweetser. Large language models and video games: A preliminary scoping review. In\nProceedings of the 6th ACM Conference on Conversational User Interfaces, pp. 1–8, 2024.\nGabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timoth´ee Lacroix, Zeming\nLin, Florian Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on\nreal-time strategy games. arXiv preprint arXiv:1611.00625, 2016.\nMohammad Reza Taesiri, Finlay Macklon, Yihe Wang, Hengshuo Shen, and Cor-Paul Bezemer.\nLarge language models are pretty good zero-shot video game bug detectors. arXiv preprint\narXiv:2210.02506, 2022.\nMohammad Reza Taesiri, Tianjun Feng, Cor-Paul Bezemer, and Anh Nguyen. Glitchbench: Can\nlarge multimodal models detect video game glitches? In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22444–22455, 2024.\nShubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as a testbed for\nlanguage model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 11385–11393, 2022.\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei\nWang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through\nrecursive contemplation. arXiv preprint arXiv:2310.01320, 2023a.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,\nHongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting,\nand enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746,\n2023b.\nWenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei.\nVisualization-of-thought elicits spatial reasoning in large language models.\narXiv preprint\narXiv:2404.03622, 2024.\nYue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as\nintelligent agents. arXiv preprint arXiv:2310.01557, 2023.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models. arXiv preprint arXiv:2306.09265, 2023.\nXinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and B¨orje F Karlsson.\nA survey on game playing agents and large models: Methods, applications, and challenges. arXiv\npreprint arXiv:2403.10249, 2024.\nZhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang,\nZhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multi-modal instruction-tuning\ndataset, framework, and benchmark. Advances in Neural Information Processing Systems, 36,\n2024.\n13\nPreprint\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,\nBoyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for\nexpert AGI. CoRR, abs\/2311.16502, 2023.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal under-\nstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 9556–9567, 2024.\nGe Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang\nCheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi\nLi, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang,\nWenhu Chen, and Jie Fu. CMMMU: A chinese massive multi-discipline multimodal understanding\nbenchmark. CoRR, abs\/2401.11944, 2024a.\nJesse Zhang, Karl Pertsch, Jiahui Zhang, and Joseph J Lim. Sprint: Scalable policy pre-training\nvia language instruction relabeling. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 9168–9175. IEEE, 2024b.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and\nJi-Rong Wen. A survey of large language models. CoRR, abs\/2303.18223, 2023.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian\nZhang, Jing Chen, Ruipu Wu, Shuai Wang, et al.\nAgents: An open-source framework for\nautonomous language agents. arXiv preprint arXiv:2309.07870, 2023.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenvironments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n14\nPreprint\nA\nMODEL LIST\nList of all models involved in the ING-VP.\nOrganization\nModel\nAccess\nClosed Source Model\nOpenAI\nGPT-4o\nhttps:\/\/openai.com\/index\/hello-gpt-4o\/\nGPT-4o mini\nhttps:\/\/openai.com\/index\/gpt-4o-mini-advancing-cost-efficient-intelligence\/\nGPT-4v\nhttps:\/\/openai.com\/index\/gpt-4v-system-card\/\nGPT-4 Turbo\nhttps:\/\/platform.openai.com\/docs\/models\/gpt-4-turbo-and-gpt-4\nAnthropic\nClaude-3.5 Sonnet\nhttps:\/\/www.anthropic.com\/news\/claude-3-5-sonnet\nClaude-3 Opus\nhttps:\/\/www.anthropic.com\/news\/claude-3-family\nGoogle Deepmind\nGemini-1.5 Pro\nhttps:\/\/deepmind.google\/technologies\/gemini\/pro\/\nOpen Source Model\nShanghai AI Laboratory\nInternVL2-Llama3-76B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-Llama3-76B\nInternVL2-40B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-40B\nInternVL2-26B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-26B\nInternVL2-8B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-8B\nInternVL-Chat-V1-5\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL-Chat-V1-5\nZhipu AI\nCogVLM2-Llama3-chat-19B\nhttps:\/\/github.com\/THUDM\/CogVLM2\nDeepSeek-AI\nDeepSeek-VL-7B-chat\nhttps:\/\/github.com\/deepseek-ai\/DeepSeek-VL\nModelBest Inc\nMiniCPM-V 2.6\nhttps:\/\/github.com\/OpenBMB\/MiniCPM-V\nTable 2: List of all models involved in the ING-VP.\nB\nPROMPTS\nThe following is the comprehensive list of 36 prompts utilized in our experiments.\nB.1\nMULTI-STEP WITH IMAGE WITHOUT HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an image of a level of the Tower of\nHanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\nPlease use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means move\nthe disk on rod-x to rod-y\nInstruction:\nPlease output only one step and your output must meet required format {”output”: ”{rod-\nx}{rod-y}”} and not output anything else:\n15\nPreprint\nMaze\nSystem:\nYou are a player of Maze game. And you will be given an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. Red area: your current position.\n2. Green area: destination.\n3. Black area: wall, unable to pass.\n4. White area: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\nInstruction:\nPlease output only one step, and your output must be one of the following: {”output”: ”L”}\nor {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”} and not output anything else:\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given an image of a level of the n-puzzle\ngame.\nPlease finish the n-puzzle based on the image provided.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\nInstruction:\nPlease output only one step and your output must meet required format {”output”: number}.\nPlease do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given an image of a level of the n-queens\ngame.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\n16\nPreprint\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n5. If your chess piece violates the three rules, it will be ignored.\nInstruction:\nPlease output only one step and your output must meet required format {”output”: [row,\ncol]}, and not output anything else:\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given an image of a level of the Sokoban\ngame.\nYour task is to complete this level by outputting movement instructions based on this image\none step at a time.\nObjective: Move all boxes onto the designated storage locations (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\n4. goal: push all the boxes onto the docks\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\nInstruction:\nPlease output only one step, and your output must be one of the following: ”output”: ”L” or\n”output”: ”R” or ”output”: ”U” or ”output”: ”D” and not output anything else:\n17\nPreprint\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given an image of a level of the Sudoku\ngame.\nPlease finish the sudoku puzzle based on the image provided, one step at a time.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nOutput Instructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number}}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nInstruction:\nPlease output only one step and your output must meet required format {”output”:\n{”{row}{column}”: {number}}}, and not output anything else:\nB.2\nMULTI-STEP TEXT-ONLY WITHOUT HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an dictionary representation of a\nlevel of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D\n2. And 5 disks: a, b, c, d, e; for size: a ¿ b ¿ c ¿ d ¿ e\n3. Your task is to move all the disks to rod ”D”\n4. Only one disk can be moved at a time\n5. Only the top disk can be moved\n6. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\nPlease use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means move\nthe disk on rod-x to rod-y\nInstruction:\nDictionary representation:\n{text-representation-path}\nPlease output only one step based on the given rules and dictionary representation, and\nyour output must meet required format {”output”: ”{rod-x}{rod-y}”}. Please do not output\nanything else.\n18\nPreprint\nMaze\nSystem:\nYou are a player of Maze game. And you will be given a text matrix of a level of the Maze\ngame.\nYour task is to move from your current position through the floor to the destination.\nInformation of text matrix:\n1. ’S’: your current position.\n2. ’X’: destination.\n3. ’+’: wall, unable to pass.\n4. ’ ’: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\nInstruction:\nText matrix:\n{text-representation-path}\nPlease output only one step based on the given rules and text matrix, and your output must be\none of the following: {”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”:\n”D”}. Please do not output anything else.\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given a list representation of a level of\nthe n-puzzle game.\nPlease finish the n-puzzle based on the list representation provided.\nIllustration of given list representation:\n1. The main list represents the board of size 4 * 4;\n2. The main list contains 4 sublist, each sublist represents a row, and contains 4\nelements;\n3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty\nspace is represented as 0;\n4. The goal is to rearrange the elements to [[1,2,3,4], [5,6,7,8], [9,10,11,12],\n[13,14,15,0]]\n5. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\nInstruction:\nList representation:\n{text-representation-path}\nPlease output only one step based on given list representation and your output must meet\nrequired format {”output”: number}. Please do not output anything else.\n19\nPreprint\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given a coordinate of the existing queens\nof a level of the n-queens game.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen is already given, so do not include it in your answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n5. If your chess piece violates the three rules, it will be ignored.\nInstruction:\nThe coordinate of the existing queens (including the first queen):\n{text-representation-path}\n1. first number: row index, range from 0 to 7\n2. second number: column index, range from 0 to 7\nPlease output only one step based on given coordinate and your output must meet required\nformat {”output”: [row, col]}. And do not output anything else.\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given a text matrix of a level of the\nSokoban game.\nYour task is to complete this level by outputting movement instructions based on the given\ntext matrix one step at a time.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration of given text matrix:\n1. ’.’: dock\n2. ’$’: box\n3. ’*’: box on the dock (can also be pushed)\n20\nPreprint\n4. ’@’: worker (or agent)\n5. ’+’: worker on the dock\n6. ’ ’: floor\n7. ’#’: wall\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\nInstruction:\nText matrix:\n{text-representation-path}\nPlease output only one step based on text matrix, and your output must be one of the following:\n{”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”}. And do not\noutput anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given a number string of a level of the\nSudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nIllustration of the given number string:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nInstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: ”output”: ”rowcolumn”: number.\n3. The range of row and column are 0-8, the range of number is 1-9.\nInstruction:\nNumber string:\n{text-representation-path}\nPlease output only one step based on given number string and your output must meet required\nformat {”output”: {”{row}{column}”: {number}}}. And do not output anything else:\n21\nPreprint\nB.3\nMULTI-STEP WITH IMAGE WITH HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an image of a level of the Tower of\nHanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means\nmove the disk on rod-x to rod-y,\n2. This is a multi-turn conversation. The conversation history provided below may be\nhelpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step and your output must meet required format {”output”: ”{rod-\nx}{rod-y}”} and not output anything else:\nMaze\nSystem:\nYou are a player of Maze game. And you will be given an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. Red area: your current position.\n2. Green area: destination.\n3. Black area: wall, unable to pass.\n4. White area: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\n22\nPreprint\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step, and your output must be one of the following: {”output”: ”L”}\nor {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”} and not output anything else:\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given an image of a level of the n-puzzle\ngame.\nPlease finish the n-puzzle based on the image provided.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\n3. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step and your output must meet required format {”output”: number}.\nPlease do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given an image of a level of the n-queens\ngame.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n23\nPreprint\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}.\n5. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n6. If your chess piece violates the three rules, it will be ignored.\n7. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\nconversation-history-path\nPlease output only one step and your output must meet required format {”output”: [row,\ncol]}, and not output anything else:\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given an image of a level of the Sokoban\ngame.\nYour task is to complete this level by outputting movement instructions based on this image\none step at a time.\nObjective: Move all boxes onto the designated storage locations (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\n4. goal: push all the boxes onto the docks\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n24\nPreprint\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step, and your output must be one of the following: ”output”: ”L” or\n”output”: ”R” or ”output”: ”U” or ”output”: ”D” and not output anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given an image of a level of the Sudoku\ngame.\nPlease finish the sudoku puzzle based on the image provided, one step at a time.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nOutput Instructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number}}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\n4. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n5. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nPlease output only one step and your output must meet required format {”output”:\n{”{row}{column}”: {number}}}, and not output anything else:\nB.4\nMULTI-STEP TEXT-ONLY WITH HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an dictionary representation of a\nlevel of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D\n2. And 5 disks: a, b, c, d, e; for size: a ¿ b ¿ c ¿ d ¿ e\n3. Your task is to move all the disks to rod ”D”\n4. Only one disk can be moved at a time\n25\nPreprint\n5. Only the top disk can be moved\n6. At no time should a large disk be placed on top of a small disk.\nInstructions:\n1. Use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means\nmove the disk on rod-x to rod-y\n2. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nDictionary representation:\n{text-representation-path}\nConversation history:\n{conversation-history-path}\nPlease output only one step based on the given rules and dictionary representation, and\nyour output must meet required format {”output”: ”{rod-x}{rod-y}”}. Please do not output\nanything else.\nMaze\nSystem:\nYou are a player of Maze game. And you will be given a text matrix of a level of the Maze\ngame.\nYour task is to move from your current position through the floor to the destination.\nInformation of text matrix:\n1. ’S’: your current position.\n2. ’X’: destination.\n3. ’+’: wall, unable to pass.\n4. ’ ’: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nText matrix:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on the given rules and text matrix, and your output must be\none of the following: {”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”:\n”D”}. Please do not output anything else.\n26\nPreprint\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given a list representation of a level of\nthe n-puzzle game.\nPlease finish the n-puzzle based on the list representation provided.\nIllustration of given list representation:\n1. The main list represents the board of size 4 * 4;\n2. The main list contains 4 sublist, each sublist represents a row, and contains 4\nelements;\n3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty\nspace is represented as 0;\n4. The goal is to rearrange the elements to [[1,2,3,4], [5,6,7,8], [9,10,11,12],\n[13,14,15,0]]\n5. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\n3. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nList representation:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on given list representation and your output must meet\nrequired format {”output”: number}. Please do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given a coordinate of the existing queens\nof a level of the n-queens game.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen is already given, so do not include it in your answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n27\nPreprint\n5. If your chess piece violates the three rules, it will be ignored.\n6. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThe coordinate of the existing queens (including the first queen):\n{text-representation-path}\n1. first number: row index, range from 0 to 7\n2. second number: column index, range from 0 to 7\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on given coordinate and your output must meet required\nformat {”output”: [row, col]}. And do not output anything else.\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given a text matrix of a level of the\nSokoban game.\nYour task is to complete this level by outputting movement instructions based on the given\ntext matrix one step at a time.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration of given text matrix:\n1. ’.’: dock\n2. ’$’: box\n3. ’*’: box on the dock (can also be pushed)\n4. ’@’: worker (or agent)\n5. ’+’: worker on the dock\n6. ’ ’: floor\n7. ’#’: wall\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n28\nPreprint\n3. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nText matrix:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on text matrix, and your output must be one of the following:\n{”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”}. And do not\noutput anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given a number string of a level of the\nSudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nIllustration of the given number string:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nInstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: ”output”: ”rowcolumn”: number.\n3. The range of row and column are 0-8, the range of number is 1-9.\n4. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n5. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nNumber string:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\n29\nPreprint\nPlease output only one step based on given number string and your output must meet required\nformat {”output”: {”{row}{column}”: {number}}}. And do not output anything else:\nB.5\nONE-STEP WITH IMAGE\nHanoi\nThis is an image of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nRules:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nNote:\n1. Use JSON as your output format: {”output”: [”AC”, ”AD”, ...]}, which means move\nthe top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.\nYour answer:\nMaze\nThis is an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. red area: your current position\n2. green area: destination\n3. black area: wall, unable to pass\n4. white area: floor, able to pass\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\n15-puzzle\nThis is an image of a level of the n-puzzle game.\nYour task is to generate a list of numbers to complete the n-puzzle problem.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: [number1, number2, number3, ...]}.\n30\nPreprint\n2. THe number1, number2, ... means if number1 is around the empty space, they will\nswap positions first; after that, if number2 is around the empty space, number2 and\nthe empty space will swap positions too, and so on.\nYour answer:\n8-queens\nThis is an image of a level of the n-queens game.\nYour task is to generate a list of coordinates to complete the n-queens problem on a board\nwhere the first queen is already placed.\nFollow these rules: Each queen must be placed in such a way that no two queens threaten\neach other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nNote:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Your output should be in the JSON format: {”output”: [[row-x1, col-y1], [row-x2,\ncol-y2], ...]}. Each [row-x, col-y] means the coordinate you want to place your\npiece.\n5. If your chess piece violates the three rules, it will be ignored.\nYour answer:\nSokoban\nThis is an image of a level of the Sokoban game.\nYour task is to complete this level by outputting movement instructions based on this image.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\n31\nPreprint\nYour answer:\nSudoku\nThis is an image of a level of the Sudoku game.\nPlease finish the sudoku puzzle based on the image provided.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\ninstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number},\n”{row}{column}”: {number}, ...}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nYour answer:\nB.6\nONE-STEP TEXT-ONLY\nHanoi\nThis is an dictionary representation of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nDictionary representation:\n{text-representation-path}\nRules:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nNote:\n1. Use JSON as your output format: {”output”: [”AC”, ”AD”, ...]}, which means move\nthe top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.\nYour answer:\nMaze\nThis is an dictionary representation of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nDictionary representation:\n{text-representation-path}\nRules:\n1. red area: your current position\n2. green area: destination\n32\nPreprint\n3. black area: wall, unable to pass\n4. white area: floor, able to pass\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\n15-puzzle\nThis is a list representation of a level of the n-puzzle game.\nPlease finish the n-puzzle based on the list representation provi\nded.\nList representation:\n{text-representation-path}\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: [number1, number2, number3, ...]}.\n2. THe number1, number2, ... means if number1 is around the empty space, they will\nswap positions first; after that, if number2 is around the empty space, number2 and\nthe empty space will swap positions too, and so on.\nYour answer:\n8-queens\nThis is a level of the n-queens game.\nYour task is to generate coordinates to complete the n-queens problem on a board where the\nfirst queen is already placed.\nThe coordinate of the first queen:\n{text-representation-path}\nFollow these rules: Each queen must be placed in such a way that no two queens threaten\neach other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nNote:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n33\nPreprint\n4. Your output should be in the JSON format: {”output”: [[row-x1, col-y1], [row-x2,\ncol-y2], ...]}. Each [row-x, col-y] means the coordinate you want to place your\npiece.\n5. If your chess piece violates the three rules, it will be ignored.\nYour answer:\nSokoban\nThis is a text matrix of a level of the Sokoban game.\nYour task is to complete this level by outputting movement instructions based on this text\nmatrix.\nText matrix:\n{text-representation-path}\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\nSudoku\nThis is a number string of a level of the Sudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nNumber string:\n{text-representation-path}\nIllustration:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n34\nPreprint\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\ninstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number},\n”{row}{column}”: {number}, ...}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nYour answer:\nC\nDETAILED RESULTS\nModel\nSetting\nMaze Sokoban N-queens N-puzzle Hanoi Sudoku Overall\nClosed Source Model\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 27.80\n9.50\n0.00\n2.50\n0.50\n0.00\n6.70\nMulti-step\nw\/o history\nEff.\n5.60\n47.00\n3.30\n58.10\n0.60\n0.50\n19.20\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 17.20\n9.80\n0.00\n4.50\n0.00\n0.00\n5.20\nMulti-step\nw\/ history\nEff.\n18.60\n26.30\n3.00\n37.60\n0.00\n0.00\n14.20\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 36.50\n3.50\n4.00\n1.80\n0.20\n31.20\n12.90\nImage-text\nOne-step\nEff.\n38.20\n52.50\n58.80\n27.90\n12.70 11.90\n33.70\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 25.20\n6.50\n0.60\n1.00\n1.20\n0.00\n5.80\nMulti-step\nw\/o history\nEff.\n10.60\n9.60\n1.80\n89.40\n0.90\n0.10\n18.70\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 23.20\n6.00\n0.00\n1.80\n1.50\n0.00\n5.40\nMulti-step\nw\/ history\nEff.\n11.60\n5.20\n1.80\n89.50\n1.50\n0.10\n18.30\nAcc. 12.00\n0.00\n8.00\n4.00\n0.00\n2.00\n4.30\nComp. 27.50\n4.50\n12.00\n10.50\n5.00\n23.00\n13.80\nText-only\nOne-step\nEff.\n31.40\n49.70\n72.00\n43.20\n62.30 28.30\n47.80\nAcc. 14.33\n0.00\n1.33\n0.67\n0.00\n0.33\n2.75\nComp. 26.23\n6.63\n2.77\n3.68\n1.40\n9.03\n8.30\nGPT-4o\nAverage\nEff.\n19.33\n31.72\n23.45\n57.62\n13.00\n6.82\n25.32\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 4.80\n7.20\n0.00\n4.20\n1.00\n0.00\n2.90\n35\nPreprint\nMulti-step\nw\/o history\nEff.\n3.60\n15.40\n2.80\n27.30\n2.50\n1.40\n8.80\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 7.50\n5.20\n0.30\n3.50\n1.00\n0.00\n2.90\nMulti-step\nw\/ history\nEff.\n13.60\n0.50\n2.80\n22.90\n2.40\n1.10\n7.20\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.30\nComp. 19.20\n6.50\n0.00\n0.00\n0.00\n0.00\n4.30\nImage-text One-step\nEff.\n32.90\n0.00\n0.00\n0.00\n0.00\n0.00\n5.50\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 4.50\n7.50\n0.00\n1.00\n2.50\n0.00\n2.60\nMulti-step\nw\/o history\nEff.\n3.60\n5.60\n1.80\n86.60\n2.00\n1.00\n16.80\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 5.00\n8.00\n0.30\n2.00\n2.50\n0.00\n3.00\nMulti-step\nw\/ history\nEff.\n9.30\n5.60\n1.80\n86.00\n1.10\n0.80\n17.40\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 13.80\n6.80\n0.00\n0.00\n0.00\n0.00\n3.40\nText-only\nOne-step\nEff.\n7.90\n43.10\n0.00\n0.00\n0.00\n0.00\n8.50\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.32\nComp. 9.13\n6.87\n0.10\n1.78\n1.17\n0.00\n3.18\nGPT-4V\nAverage\nEff.\n11.82\n11.70\n1.53\n37.13\n1.33\n0.72\n10.70\nAcc.\n4.00\n0.00\n0.00\n2.00\n0.00\n0.00\n1.00\nComp. 19.80\n9.00\n0.30\n4.00\n2.00\n0.00\n5.90\nMulti-step\nw\/o history\nEff.\n25.10\n57.10\n3.30\n95.90\n23.80\n3.00\n34.70\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 10.50\n6.50\n0.00\n5.20\n0.50\n0.00\n3.80\nMulti-step\nw\/ history\nEff.\n20.20\n48.40\n4.10\n87.50\n5.30\n1.40\n27.80\nAcc. 10.00\n6.00\n0.00\n0.00\n0.00\n0.00\n2.70\nComp. 20.80\n13.80\n4.00\n3.20\n4.80\n11.00\n9.60\nImage-text\nOne-step\nEff.\n35.30\n58.80\n61.00\n55.70\n38.60\n7.10\n42.80\nAcc. 34.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.70\nComp. 43.20\n4.50\n0.60\n0.80\n0.20\n0.00\n8.20\nMulti-step\nw\/o history\nEff.\n16.10\n3.40\n2.50\n94.00\n0.40\n0.60\n19.50\nAcc. 26.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.30\nComp. 32.80\n4.00\n0.30\n0.80\n1.00\n0.00\n6.50\nMulti-step\nw\/ history\nEff.\n8.60\n3.40\n2.30\n95.60\n0.50\n0.50\n18.50\nAcc. 10.00\n2.00\n2.00\n0.00\n0.00\n0.00\n2.30\n36\nPreprint\nComp. 24.80\n6.20\n2.00\n6.50\n3.00\n8.20\n8.50\nText-only\nOne-step\nEff.\n33.70\n55.50\n64.80\n30.40\n37.50\n4.50\n37.70\nAcc. 14.00\n0.00\n1.33\n0.67\n0.00\n0.33\n2.72\nComp. 25.32\n7.33\n1.20\n3.42\n1.92\n3.20\n7.08\nAverage\nEff.\n23.17\n37.77\n23.00\n76.52\n17.68\n2.85\n30.17\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\nComp. 10.00\n5.80\n0.00\n1.80\n3.00\n0.00\n3.40\nMulti-step\nw\/o history\nEff.\n2.70\n21.40\n1.70\n35.60\n17.40\n0.40\n13.20\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 8.50\n6.50\n0.00\n2.00\n3.50\n0.00\n3.40\nMulti-step\nw\/ history\nEff.\n2.00\n0.00\n1.40\n23.50\n22.10\n0.40\n8.20\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 20.80\n8.20\n0.00\n3.00\n6.80\n1.00\n6.60\nImage-text\nOne-step\nEff.\n34.80\n64.70\n58.20\n32.80\n18.50\n2.50\n35.20\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n,2.0\nComp. 22.00\n7.00\n0.30\n0.20\n1.80\n0.00\n5.20\nMulti-step\nw\/o history\nEff.\n7.80\n8.30\n1.60\n95.30\n3.50\n0.40\n19.50\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 23.80\n8.80\n0.00\n1.20\n1.50\n0.00\n5.90\nMulti-step\nw\/ history\nEff.\n8.40\n4.90\n1.80\n86.10\n2.50\n0.30\n17.30\nAcc.\n2.00\n0.00\n2.00\n0.00\n0.00\n2.00\n1.00\nComp. 26.20\n6.50\n2.00\n4.80\n4.20\n9.80\n8.90\nText-only\nOne-step\nEff.\n27.70\n52.40\n70.80\n36.00\n42.70 11.20\n40.10\nAcc.\n5.67\n0.00\n0.33\n0.00\n0.00\n0.33\n1.05\nComp. 18.55\n7.13\n0.38\n2.17\n3.47\n1.80\n5.57\nGPT-4o mini\nAverage\nEff.\n13.90\n25.28\n22.58\n51.55\n17.78\n2.53\n22.25\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 10.00\n5.80\n0.00\n7.20\n0.20\n0.00\n3.90\nMulti-step\nw\/o history\nEff.\n25.10\n33.60\n2.00\n86.80\n8.40\n5.50\n26.90\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 8.00\n6.20\n0.00\n11.20\n0.20\n0.00\n4.30\nMulti-step\nw\/ history\nEff.\n37.00\n37.30\n2.80\n49.10\n9.00\n3.60\n23.10\nAcc. 28.00\n2.00\n4.00\n4.00\n0.00\n4.00\n7.00\nComp. 55.00\n5.50\n4.00\n13.80\n6.50\n46.60\n21.90\nImage-text\nOne-step\nEff.\n51.30\n63.40\n60.20\n52.50\n26.40 36.90\n48.40\n37\nPreprint\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.80\n6.80\n2.30\n0.20\n1.20\n0.00\n4.90\nMulti-step\nw\/o history\nEff.\n4.40\n4.60\n1.60\n92.40\n1.80\n0.60\n17.60\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 21.20\n6.20\n1.40\n0.00\n2.50\n0.00\n5.20\nMulti-step\nw\/ history\nEff.\n4.60\n4.00\n1.70\n98.80\n1.60\n0.40\n18.50\nAcc. 28.00\n0.00\n18.00\n2.00\n0.00\n0.00\n8.00\nComp. 41.20\n10.00\n28.00\n11.50\n8.20\n1.60\n16.80\nText-only\nOne-step\nEff.\n37.50\n61.70\n76.80\n41.30\n30.90\n3.80\n42.00\nAcc. 14.00\n0.33\n3.67\n1.67\n0.00\n0.67\n3.37\nComp. 25.70\n6.75\n5.95\n7.32\n3.13\n8.03\n9.50\nClaude-3.5 Sonnet\nAverage\nEff.\n26.65\n34.10\n24.18\n70.15\n13.02\n8.47\n29.42\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.20\n8.00\n0.90\n0.50\n1.50\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n5.40\n3.10\n1.70\n62.80\n1.20\n0.40\n12.40\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 19.20\n7.20\n0.60\n0.00\n1.50\n0.00\n4.80\nMulti-step\nw\/ history\nEff.\n4.50\n3.60\n1.90\n62.00\n1.30\n0.60\n12.30\nAcc.\n0.00\n2.00\n2.00\n2.00\n0.00\n0.00\n1.00\nComp. 40.50\n4.80\n6.00\n7.50\n4.20\n1.20\n10.70\nText-only\nOne-step\nEff.\n40.20\n55.70\n71.00\n47.00\n27.70\n3.00\n40.80\nAcc.\n9.33\n0.67\n0.67\n0.67\n0.00\n0.00\n1.87\nComp. 25.97\n6.67\n2.50\n2.67\n2.40\n0.40\n5.07\nClaude-3 Opus\nAverage\nEff.\n16.70\n20.80\n24.87\n57.27\n10.07\n1.33\n21.83\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.20\n8.00\n0.90\n0.20\n1.50\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n5.30\n3.10\n1.70\n61.80\n1.20\n0.40\n12.20\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 19.20\n7.20\n0.60\n0.00\n1.50\n0.00\n4.80\nMulti-step\nw\/ history\nEff.\n4.50\n3.60\n1.80\n62.00\n1.30\n0.60\n12.30\nAcc.\n0.00\n2.00\n2.00\n2.00\n0.00\n0.00\n1.00\nComp. 35.00\n5.00\n2.00\n7.00\n4.20\n1.20\n9.10\nText-only\nOne-step\nEff.\n38.70\n56.30\n69.00\n48.60\n29.80\n3.80\n41.00\nAcc.\n9.33\n0.67\n0.67\n0.67\n0.00\n0.00\n1.87\nComp. 24.13\n6.73\n1.17\n2.40\n2.40\n0.40\n6.23\n38\nPreprint\nGPT-4 Turbo\nAverage\nEff.\n16.17\n21.00\n24.17\n57.47\n10.77\n1.60\n21.83\nOpen Source Model\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 15.20\n6.50\n0.00\n1.00\n0.00\n0.00\n3.78\nMulti-step\nw\/o history\nEff.\n22.50\n25.10\n8.20\n11.30\n0.00\n0.00\n11.18\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 13.20\n6.80\n0.00\n0.00\n0.00\n0.00\n3.33\nMulti-step\nw\/ history\nEff.\n19.80\n26.00\n7.30\n9.80\n0.80\n0.00\n10.62\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 16.80\n4.50\n1.20\n0.00\n2.50\n0.00\n4.17\nImage-text\nOne-step\nEff.\n25.20\n22.60\n33.80\n5.30\n19.50\n0.00\n17.73\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 14.00\n5.50\n0.00\n1.20\n1.00\n0.00\n3.62\nMulti-step\nw\/o history\nEff.\n16.30\n27.70\n3.20\n12.10\n1.20\n0.00\n10.08\nAcc.\n0.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 11.20\n4.00\n0.10\n0.80\n0.00\n0.00\n2.68\nMulti-step\nw\/ history\nEff.\n13.30\n17.80\n1.60\n5.50\n0.00\n0.00\n6.37\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 14.00\n5.80\n2.00\n0.00\n3.50\n0.00\n4.22\nText-only\nOne-step\nEff.\n24.10\n57.20\n30.00\n0.00\n20.00\n0.00\n21.88\nAcc.\n1.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.22\nComp. 14.07\n5.52\n0.55\n0.50\n1.17\n0.00\n3.63\nMiniCPM-V2.6\nAverage\nEff.\n20.20\n29.40\n14.02\n7.33\n6.92\n0.00\n12.98\nAcc.\n6.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 11.00\n4.50\n0.10\n0.00\n0.00\n0.00\n2.60\nMulti-step\nw\/o history\nEff.\n17.90\n11.10\n3.40\n3.00\n0.00\n0.00\n5.90\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 10.00\n5.50\n0.00\n0.00\n0.00\n0.00\n2.58\nMulti-step\nw\/ history\nEff.\n16.60\n10.90\n2.80\n1.30\n0.00\n0.00\n5.27\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 14.20\n5.00\n0.00\n0.80\n0.00\n0.00\n3.33\nImage-text\nOne-step\nEff.\n10.40\n12.00\n3.30\n4.10\n0.00\n0.00\n4.97\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 11.80\n4.00\n0.00\n0.00\n0.00\n0.00\n2.63\nMulti-step\nw\/o history\nEff.\n13.20\n2.30\n0.00\n2.80\n0.00\n0.00\n3.05\n39\nPreprint\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 9.20\n5.80\n0.00\n0.00\n0.00\n0.00\n2.50\nMulti-step\nw\/ history\nEff.\n13.10\n6.60\n1.50\n4.40\n0.00\n0.00\n4.27\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 15.80\n7.20\n0.00\n0.00\n0.00\n0.00\n3.83\nText-only\nOne-step\nEff.\n17.70\n10.10\n0.00\n8.40\n0.00\n0.00\n6.03\nAcc.\n5.33\n0.00\n0.00\n0.00\n0.00\n0.00\n0.89\nComp. 12.00\n5.33\n0.02\n0.13\n0.00\n0.00\n2.91\nInternvl2-8B\nAverage\nEff.\n14.82\n8.83\n1.83\n4.00\n0.00\n0.00\n4.91\nAcc. 12.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 18.50\n7.80\n0.30\n2.00\n0.20\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n25.00\n19.70\n2.20\n14.90\n1.20\n0.50\n10.58\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 19.00\n8.00\n0.30\n2.50\n1.50\n0.00\n5.22\nMulti-step\nw\/ history\nEff.\n23.10\n17.00\n1.90\n12.10\n1.20\n0.00\n9.22\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 21.20\n8.50\n1.20\n2.50\n0.50\n0.00\n5.65\nImage-text\nOne-step\nEff.\n27.70\n18.60\n15.50\n7.40\n2.40\n0.00\n11.93\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 20.20\n9.50\n0.00\n1.80\n0.00\n0.00\n5.25\nMulti-step\nw\/o history\nEff.\n21.80\n19.80\n2.50\n16.00\n0.00\n1.20\n10.22\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 20.00\n10.50\n0.10\n1.00\n0.00\n0.00\n5.27\nMulti-step\nw\/ history\nEff.\n20.10\n21.20\n1.30\n10.80\n0.00\n2.20\n9.27\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 22.00\n8.00\n0.00\n0.80\n0.50\n0.00\n5.22\nText-only\nOne-step\nEff.\n25.40\n14.10\n43.30\n14.90\n1.60\n1.00\n16.72\nAcc. 11.00\n0.33\n0.00\n0.00\n0.00\n0.00\n1.89\nComp. 20.15\n8.72\n0.32\n1.77\n0.45\n0.00\n5.23\nInternvl2-26B\nAverage\nEff.\n23.85\n18.40\n11.12\n12.68\n1.07\n0.82\n11.32\nAcc.\n8.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 21.20\n6.80\n0.60\n4.50\n1.00\n0.00\n5.68\nMulti-step\nw\/o history\nEff.\n33.50\n40.40\n1.60\n29.70\n0.80\n4.20\n18.37\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 18.80\n7.00\n0.30\n6.50\n0.00\n0.00\n5.43\n40\nPreprint\nMulti-step\nw\/ history\nEff.\n31.00\n18.40\n1.20\n24.30\n0.00\n3.00\n12.98\nAcc. 16.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.67\nComp. 30.00\n7.50\n2.00\n6.50\n1.20\n0.00\n7.87\nImage-text\nOne-step\nEff.\n42.40\n42.60\n18.80\n28.10\n1.40\n0.00\n22.22\nAcc.\n4.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 19.20\n9.80\n0.00\n1.20\n0.00\n0.00\n5.03\nMulti-step\nw\/o history\nEff.\n27.20\n41.10\n0.80\n20.90\n0.00\n2.00\n15.33\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 12.50\n7.50\n0.00\n4.00\n0.50\n0.00\n4.08\nMulti-step\nw\/ history\nEff.\n24.10\n39.00\n1.80\n25.80\n0.60\n0.00\n15.22\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 23.80\n9.50\n4.00\n11.20\n0.00\n0.00\n8.08\nText-only\nOne-step\nEff.\n29.90\n44.70\n55.40\n38.40\n2.40\n0,6\n34.16\nAcc. 10.00\n0.67\n0.00\n0.00\n0.00\n0.00\n1.78\nComp. 20.92\n8.02\n1.15\n5.65\n0.45\n0.00\n6.03\nInternvl2-40B\nAverage\nEff.\n31.35\n37.70\n13.27\n27.87\n0.87\n1.84\n18.82\nAcc.\n0.00\n4.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 28.30\n9.50\n0.00\n0.00\n0.00\n0.00\n6.30\nMulti-step\nw\/o history\nEff.\n36.40\n42.20\n3.20\n6.70\n0.00\n0.90\n14.90\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 29.20\n8.00\n0.60\n0.00\n0.00\n0.00\n6.30\nMulti-step\nw\/ history\nEff.\n38.80\n39.30\n3.10\n3.20\n0.00\n0.90\n14.22\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 18.50\n6.50\n1.20\n1.00\n0.20\n0.00\n4.57\nImage-text\nOne-step\nEff.\n26.00\n36.70\n58.10\n26.30\n1.10\n5.90\n25.68\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 27.00\n7.00\n0.60\n0.00\n0.20\n0.00\n5.80\nMulti-step\nw\/o history\nEff.\n34.50\n27.60\n4.20\n3.00\n0.40\n0.50\n11.70\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 29.50\n6.50\n0.00\n0.00\n0.00\n0.00\n6.00\nMulti-step\nw\/ history\nEff.\n39.10\n22.10\n1.60\n2.40\n0.00\n0.00\n10.87\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 13.50\n4.80\n4.00\n1.80\n1.00\n0.00\n4.18\nText-only\nOne-step\nEff.\n23.90\n38.70\n59.50\n33.30\n2.40\n5.80\n27.27\nAcc.\n1.67\n0.67\n0.00\n0.00\n0.00\n0.00\n0.39\n41\nPreprint\nComp. 24.33\n7.05\n1.07\n0.47\n0.23\n0.00\n5.53\nAverage\nEff.\n33.12\n34.43\n21.62\n12.48\n0.65\n2.33\n17.44\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 9.00\n8.00\n0.10\n2.50\n1.20\n0.00\n3.47\nMulti-step\nw\/o history\nEff.\n14.40\n28.00\n2.50\n24.80\n0.50\n0.60\n11.80\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 8.50\n6.00\n0.60\n1.00\n0.20\n0.00\n2.72\nMulti-step\nw\/ history\nEff.\n13.40\n26.60\n1.70\n25.00\n0.60\n0.00\n11.22\nAcc.\n6.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 10.80\n6.50\n0.60\n4.00\n0.00\n0.00\n3.65\nImage-text\nOne-step\nEff.\n16.70\n30.50\n29.90\n21.30\n0.00\n0.00\n16.40\nAcc.\n0.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 8.00\n6.80\n0.00\n0.80\n0.50\n0.00\n2.68\nMulti-step\nw\/o history\nEff.\n11.50\n32.00\n2.40\n3.30\n1.10\n0.00\n8.38\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 11.80\n5.80\n0.00\n6.50\n1.00\n0.00\n4.18\nMulti-step\nw\/ history\nEff.\n16.10\n25.40\n0.00\n15.20\n0.70\n0.00\n9.57\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 12.00\n7.50\n0.00\n4.00\n0.00\n0.00\n3.92\nText-only\nOne-step\nEff.\n17.60\n27.00\n15.50\n35.30\n0.00\n0.00\n15.90\nAcc.\n2.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.39\nComp. 10.02\n6.77\n0.22\n3.13\n0.48\n0.00\n3.44\nDeepSeek-VL\nAverage\nEff.\n14.95\n28.25\n8.67\n20.82\n0.48\n0.10\n12.21\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 25.50\n7.50\n0.00\n1.20\n1.20\n0.00\n5.90\nMulti-step\nw\/o history\nEff.\n33.10\n45.30\n1.10\n11.10\n2.80\n1.10\n15.75\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 23.80\n8.50\n0.00\n0.80\n1.00\n0.00\n5.68\nMulti-step\nw\/ history\nEff.\n33.20\n47.70\n0.90\n14.10\n2.10\n0.70\n16.45\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 27.00\n8.80\n1.20\n2.50\n0.00\n0.00\n6.58\nImage-text\nOne-step\nEff.\n39.50\n39.80\n47.50\n28.80\n0.00\n7.10\n27.12\nAcc.\n8.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 24.50\n6.00\n0.60\n3.00\n0.00\n0.00\n5.68\nMulti-step\nw\/o history\nEff.\n33.80\n30.50\n3.60\n14.60\n0.00\n0.00\n13.75\n42\nPreprint\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 20.50\n7.00\n0.60\n1.00\n1.00\n0.00\n5.02\nMulti-step\nw\/ history\nEff.\n31.00\n31.40\n2.30\n10.50\n0.50\n1.40\n12.85\nAcc. 10.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 26.00\n7.50\n0.60\n10.50\n1.20\n0.00\n7.63\nText-only\nOne-step\nEff.\n37.20\n40.20\n61.10\n36.80\n5.10\n7.80\n31.37\nAcc.\n8.33\n0.67\n0.00\n0.00\n0.00\n0.00\n1.50\nComp. 24.55\n7.55\n0.50\n3.17\n0.73\n0.00\n6.08\nCogvlm2-19B\nAverage\nEff.\n34.63\n39.15\n19.42\n19.32\n1.75\n3.02\n19.55\nAcc. 12.00\n4.00\n0.00\n0.00\n0.00\n0.00\n2.67\nComp. 40.50\n8.80\n0.10\n4.50\n0.50\n0.00\n9.07\nMulti-step\nw\/o history\nEff.\n47.20\n33.60\n4.20\n17.80\n1.50\n1.00\n17.55\nAcc. 12.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 30.20\n4.50\n0.30\n2.50\n0.20\n0.00\n6.28\nMulti-step\nw\/ history\nEff.\n38.60\n28.90\n3.90\n15.30\n1.60\n2.50\n15.13\nAcc. 18.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.00\nComp. 42.20\n4.80\n0.60\n1.20\n1.00\n0.00\n8.30\nImage-text One-step\nEff.\n50.90\n55.40\n42.30\n50.00\n8.60\n9.90\n36.18\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 41.80\n6.80\n0.30\n0.80\n0.20\n0.00\n8.32\nMulti-step\nw\/o history\nEff.\n49.40\n49.20\n5.50\n20.70\n1.90\n0.10\n21.13\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 37.00\n9.50\n0.00\n1.20\n0.50\n0.00\n8.03\nMulti-step\nw\/ history\nEff.\n41.00\n52.50\n4.90\n55.30\n10.00 12.10\n29.30\nAcc. 14.00\n4.00\n0.00\n0.00\n0.00\n0.00\n3.00\nComp. 18.80\n9.00\n2.00\n4.50\n1.00\n0.00\n5.88\nText-only\nOne-step\nEff.\n29.00\n52.00\n47.70\n49.00\n9.90\n10.10\n32.95\nAcc. 13.33\n1.67\n0.00\n0.00\n0.00\n0.00\n2.50\nComp. 35.08\n7.23\n0.55\n2.45\n0.57\n0.00\n7.65\nInternVL2-\nLlama3-76B\nAverage\nEff.\n42.68\n45.27\n18.08\n34.68\n5.58\n5.95\n25.38\nTable 3: Results for all of the MLLMs\nD\nCASE STUDY\n43\nPreprint\nFigure 7: A sample case of perceptual error. Sokoban — One-step — Image-text — Level 2.\n44\nPreprint\nFigure 8: A sample case of textual understanding error. Hanoi — Multi-step — Text-only — With-\nhistory — Level 13.\n45\nPreprint\nFigure 9: A sample case of three errors. 8-queens — One-step — Image-text — Level 8.\n46\nPreprint\nFigure 10: A sample case of output comparison. Maze — Multi-step — Image-text — Without-\nhistory — Level 33.\n47\nPreprint\nFigure 11: A sample case of output comparison. 15-Puzzle — Multi-step — Image-text — Without-\nhistory — Level 45.\n48\nPreprint\nFigure 12: A sample case of output comparison. Sudoku — One-step — Image-text — Level 2.\n49\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ING-VP: MLLMs cannot Play Easy Vision-based Games Yet.pdf"}
{"title":"Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs","authors":"Yilun Hua, Yoav Artzi","summary":"Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps:\/\/github.com\/lil-lab\/ICCA.","url":"http:\/\/arxiv.org\/abs\/2408.01417v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.01417v1","published":1722621117000,"comment":"Accepted to COLM 2024","pdf_text":"Published as a conference paper at COLM 2024\nTalk Less, Interact Better: Evaluating In-context\nConversational Adaptation in Multimodal LLMs\nYilun Hua and Yoav Artzi\nDepartment of Computer Science and Cornell Tech\nCornell University\n{yilunhua, yoav}@cs.cornell.edu\nAbstract\nHumans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon\nhas been studied extensively using reference games, showing properties of\nhuman language that go beyond relaying intents. It remains unexplored\nwhether multimodal large language models (MLLMs) similarly increase\ncommunication efficiency during interactions, and what mechanisms they\nmay adopt for this purpose. We introduce ICCA, an automated framework\nto evaluate such conversational adaptation as an in-context behavior in\nMLLMs. We evaluate several state-of-the-art MLLMs, and observe that\nwhile they may understand the increasingly efficient language of their\ninterlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property\nof linguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language.\n1\nIntroduction\nHuman interlocutors adapt to each other during interactions, developing increasingly\nefficient ways to refer to concepts and objects. Hawkins et al. (2020b) exemplify this via\ncommunication between a nurse and a bed-ridden patient at home. Initially, the patient\nmay refer to a medicine with the medicine for my back pain in a small blue medicine bottle ..., but\nafter a week of care, they are likely to just ask for their back meds. This increase in efficiency\nrelies on the interlocutors forming ad-hoc linguistic conventions: the mutually understood,\nconcise phrases to communicate referential content. This phenomenon has been repeatedly\nobserved and characterized in controlled studies using repeated reference games (Figure 1;\ne.g., Krauss & Weinheimer, 1964; Brennan & Clark, 1996; Hawkins et al., 2020a).\nWe study this ability in multimodal large language models (MLLMs). LLMs and MLLMs are\nwell positioned to acquire this behavior and display it spontaneously in interactions. They\nare trained on large amounts of human language data, in which this behavior is common\nand the history of an ongoing interaction is often retained, thereby explicitly keeping the\ninformation needed at hand. Beyond the scientific question, such ad-hoc adaptation has\nsignificant application impacts: enabling more natural interactions, reducing the costs\ninvolved in conversations (e.g., using shorter utterances to communicate the same amount\nof information), and increasing the accuracy of relaying intent.\nWe propose ICCA,1 an automated framework to evaluate and characterize the ability of\nmodels to form ad-hoc conventions. ICCA uses a corpus of human-human reference game\ninteractions, allowing for completely automated evaluation, which does not require further\nhuman interaction, making it easy to deploy for the analysis of new models. The interaction\nfollows the standard repeated reference game setup (Clark & Wilkes-Gibbs, 1986), where\na speaker refers to an image within a shared context of images, and a listener resolves the\n1ICCA stands for In-context Conversational Adaptation.\n1\narXiv:2408.01417v1  [cs.CL]  2 Aug 2024\nPublished as a conference paper at COLM 2024\nPhoto with a bowl of 3 \nbananas with pokadot \nA\nB\nC\nD\nImage B\nA bowl full of mixed fruit, \nblack background\nImage A\nContext\nRepetition 1\nRepetition 6\nblack\nImage A\npokadot\nImage B\nFigure 1: Illustration of a reference game. The speaker (blue) and listener (orange) observe a\nshared set of images.2The interaction progresses in six repetitions, each includes a trial for\nevery context image. In each trial, the speaker describes a target image, and the listener has\nto select the correct target given the description only. For simplicity, this figure omits the\nfeedback on listener actions. This interaction illustrates some of the effects of convention\nformation: the descriptions become shorter as the interaction progresses, and lexical choices\nconverge to a subset of the words used in earlier repetitions.\nreference to select one of the images, ideally the one originally referred to. Figure 1 illustrates\nthe scenario. We focus on in-context adaptation – as the interaction progresses, the entire\nhistory is retained in-context. Core to our approach is comparing the changes in model\nbehavior, either as speaker or listener, throughout an interaction to the changes observed\nin humans. We measure different properties that have been shown to be influenced by\nconvention formation: utterance length, lexical convergence, and selection accuracy.\nWe apply our approach to five representative MLLMs: IDEFICS (Huggingface, 2023), LLaVa-\n1.5 (Liu et al., 2023a), GPT4-vision (OpenAI et al., 2024), Gemini 1.0 Pro Vision (Google, 2023),\nand Claude 3 opus (Anthropic, 2024). We find that all models struggle to spontaneously\nintroduce conventions and adapt as speakers. Prompt engineering an explicit in-context\ninstruction specific to the reference game scenario can address this to some degree. The\nstrongest models (GPT4, Gemini, and Claude) can then gradually use shorter messages\n(gaining lexical efficiency) but still struggle with convergence or stability, which hinders\nthe emergence of truly efficient communication. When acting as a listener, GPT4 displays\nadaptation trends close to humans, improving its accuracy as the interaction progresses,\nwhile other models show this behavior to a lesser degree or only under some simplified\nsetups. Overall, we show that while today’s MLLMs may passively understand the evolv-\ning language of their interlocutor, the ability to adapt their own language for efficient\ncommunication does not naturally emerge from their training or instruction-tuning. This\noutlines important future research problems. We release ICCA under the MIT license at\nhttps:\/\/github.com\/lil-lab\/ICCA.\n2\nBackground and Related Work\nRepeated Reference Games\nA reference game is an interaction where a speaker and a\nlistener (i.e., a dyad) interact over a shared context. The shared context is a set of images.\nThe speaker describes a target image. The target designation is only revealed to the speaker.\nThe listener has to select an image following the speaker’s description. Each participant\nsees the images in a different order, so they cannot use position information to communicate\nthe referent. Reference games have been used extensively in the study of computational\nmodels, including recently to evaluate visual abstraction (Ji et al., 2022) and conversational\naptitude (Chalamalasetti et al., 2023).\nA repeated reference game (Figure 1) includes multiple repetitions. Each repetition has\none trial for each image in the shared context. The listener receives feedback after every\n2In Figure 1, we show the shared context only once for compactness. In our experiments, we shuffle\nand show the context for each trial, but also experiment with showing it only once.\n2\nPublished as a conference paper at COLM 2024\ntrial, which indicates the correct selection. The repetition and feedback allow the dyad\nto form message agreements over the repeating stimuli (i.e., the speaker would naturally\nuse gradually shorter but related messages across repetitions, and the listener learns what\nthey refer to). ICCA’s repeated reference games are developed based on the setup and\ndata from Hawkins et al. (2020b). The shared context includes four images, and there are\nsix repetitions, giving a total of 24 trials. The order of images is shuffled across the trials.\nWe refer to this as the standard setup. Hawkins et al. (2020b) only uses this standard setup.\nBeyond this standard setup, we design variants to further disentangle the types and causes\nof model failures for in-context LLM adaptation (Section 4 and Section 5).\nAd-hoc Adaptation in Interactions\nExisting literature shows that humans are inclined\nto reduce the effort needed to convey their intended information and for their audience\nto comprehend it, leading to efficient communication (e.g., Zipf, 1949; Gibson et al., 2019;\nYin et al., 2024). When human individuals interact through dialogue, this is manifested by\ndeveloping and using ad-hoc linguistic conventions. This phenomenon has been observed\nwith repeated reference games (Krauss & Weinheimer, 1964; 1966; Clark & Wilkes-Gibbs,\n1986; Hawkins et al., 2020a), and related interaction scenarios (Haber et al., 2019). Studies\nhave also shown various properties of these conventions, such as arbitrariness, stability,\nstickiness, and convergence (Lewis, 1969; Brennan & Clark, 1996; Markman & Makin, 1998;\nHawkins et al., 2020a; Eliav et al., 2023). This adaptation was modeled with the pragmatic\nrational speech act model (RSA; Goodman & Frank, 2016), leading to development of\nmodels that replicate this behavior in reference games (e.g., Monroe et al., 2017; McDowell\n& Goodman, 2019; White et al., 2020) and use it to improve model performance on other\ntasks (e.g., Andreas & Klein, 2016; Fried et al., 2018). Adaptation was studied beyond\nreference games, showing how the complexity of the scenario influences how conventions\nmanifest in the language (Effenberger et al., 2021).\nAd-hoc conventions are a particular instantiation of the broader phenomenon of common\nground, which is defined as the mutually recognized shared information between the par-\nticipants of a conversation (Clark & Brennan, 1991; Lewis, 1969; Stalnaker, 2002). Common\nground has been studied extensively, with focus on both human cognition (Clark, 1996;\nHorton & Gerrig, 2016) and machine reasoning (Cohen & Levesque; Grosz & Sidner; Traum,\n1994; Del Tredici et al., 2022; Shaikh et al., 2024; Andukuri et al., 2024; Testoni & Fern´andez,\n2024).\nModel Adaptation\nAdapting models during an interaction to improve communication\nefficiency or success is relatively understudied. Hawkins et al. (2020b) proposes a continual\nlearning method for CNN-RNN models to gain communication efficiency in repeated refer-\nence games through continual weight updates. Zhu et al. (2021) proposes explicitly training\nmodels for ad-hoc adaptation through meta-learning. We focus on the in-context capabilities\nof LLMs and MLLMs, which offer an update-free route for adaptation that is particularly\ncompelling given the costs of updating large models. Our use of in-context learning differs\nfrom how this mechanism is usually used either by providing instructions (Ouyang et al.,\n2022) or few-shot examples (Brown et al., 2020). While reference games can be seen as\nrelated to the few-shot approach, ad-hoc adaptation is not about replicating patterns, but\nshowing change and adaptation over time.\n3\nThe ICCA Framework\nICCA uses a dataset of human-human interactions, and allows to easily customize different\nparts of the interaction. This flexibility enables different research questions. For example,\nin Section 5, we customize the interaction structure to analyze how well models handle\nlong interactions with multiple images interleaved in them. ICCA supports studies with the\nmodel acting either as speaker or listener, and includes several metrics to track different\nproperties of adaptation during the interaction.\nICCA is fully automated and easily applicable to new MLLMs. Our design does not\nrequire collecting new data or human studies, but instead uses Hawkins et al. (2020b)’s\nhuman-human interaction data to simulate a human interacting with an MLLM. Each\n3\nPublished as a conference paper at COLM 2024\ninteraction in the dataset was collected under the standard setup (Section 2) and uses a\nvisually challenging reference context, consisting of four similar images (Figure 1). The\ndataset contains 54 human-human interactions, which we incorporate into ICCA.\nA repeated reference game interaction R is a sequence of tuples ⟨(Ci, ci, si, li, fi)⟩n\ni=1, where\nCi is the set of images forming the reference context, ci is the index of the target image, si is\nthe speaker utterance, li is the listener selection, and fi is the feedback based on the listener’s\nselection. At every trial t, with the evaluated model as either the listener or speaker, ICCA\nconstructs the MLLM prompt from an instruction text I, the history (i.e., all prior trials\nR[: t]), and the stimuli for the current trial with a user-defined pre-processing function F.\nUpon receiving the model’s response, ICCA computes the feedback ft to help the next trial.\nThe function F prepares the prompt depending on the experiment, and is key to the flexibility\nof ICCA. For example, when evaluating a model as a listener under the standard setup,\nthe function input would be F(I, R[: t], Ct, st), and it would format and concatenate all the\nelements in order. Figure 7 in the appendix shows an example prompt. ICCA allows to\neasily modify this standard setup, for example by processing the data such that F drops all\nreference contexts except C1, thereby creating a simpler input where the images appear only\nonce at the beginning of the interaction.\nICCA simulates the interlocutor of the model evaluated either with a deterministic counter-\npart or by having another model take the role. A deterministic speaker outputs the messages\nfrom recorded human interactions dataset, showing predetermined, realistic trajectories of\nmessage shortening over time, but it does not adapt its language based on the listener’s\nselections. It can be considered as a “convention comprehension” task, potentially more\nchallenging due to the non-adapting speaker messages. We use this simulated speaker\nfor our model-as-listener experiments (Section 5) because it exposes the model listener to\nbehaviors and linguistic conventions naturally occurring in human interactions. Inversely,\nfor our speaker experiments (Section 4), we use a high-performance model listener (GPT4),\nwhich we observe to have performance similar to human listeners (Section 5).\nWe evaluate model behavior with adaptations of the metrics used in human studies with\nreference games (Hawkins et al., 2020a;b). For listener experiments, we follow Hawkins\net al. (2020b) and report the average accuracy in each repetition. Speaker experiments are\nevaluated using several metrics. We report the average message length and the listener’s\naccuracy in each repetition. Additionally, we evaluate the similarity between corresponding\nmessages from consecutive repetitions. While this was done using GloVe embeddings (Pen-\nnington et al., 2014) in past work (Hawkins et al., 2020a), we design a new metric called\nWord Novelty Rate (WNR), which is sensitive to exact word choices. WNR is a modified\nword error rate that only counts insertions and substitutions, and ignores deletions. It is\nmotivated by how people naturally drop words from their messages as the interaction\nprogresses (Hawkins et al., 2020a), whereas additions and substitutions of words often\nreflect important changes in information based on our observations. Compared to GloVe,\nWNR is more sensitive to lexical inconsistencies that can increase the listener’s cognitive\nload. Appendix A presents more details of our metrics, including a comparison of WNR\nand embedding-based similarity metrics and a variant of WNR that is not normalized by\nmessage length (referred to as Word Novelty Distance).\n4\nModel-as-speaker Experiments\nWe study model behavior as speaker with five state-of-the-art vision MLLMs: IDEFICS-\n80b-instruct,3 LLaVa-1.5-13b, GPT4-vision, Gemini 1.0 Pro Vision, and Claude 3 opus.\nThroughout all speaker experiments, we customize the data to only show the referential\ncontext once at the beginning of the interaction, so there is no shuffling of context through-\nout the interaction. We engineer prompts for each model individually to best evaluate\nits capability. We use GPT4 as the listener. It exhibits high performance in our listener\nexperiments (Section 5), especially when the context appears only once at the beginning,\n3IDEFICS is an open-source reproduction of Flamingo (Alayrac et al., 2022).\n4\nPublished as a conference paper at COLM 2024\n20\n40\n60\n80\n100\nAcc %\nS1: Standard Speaker\nS2: Gricean Instruction\nS3: Explicit Instruction\nS4: Explicit Instruc-\ntion+Consistency Request\n5\n10\n15\n20\nMsg len (tokens)\n1\n2\n3\n4\n5\n6\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nWNR\n1\n2\n3\n4\n5\n6\nRepetition #\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\nIDEFICS\nLLaVa\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 2: Speaker experiments. Margins of errors are bootstrapped 95% CIs.\nso it is a suitable substitute for a human listener in this study. Appendix A.3 provides\nimplementation details.\nWe design four speaker variants by modifying the instruction I. The variants were developed\nthroughout our experiments, by observing the difficulty of models to present patterns similar\nto human linguistic behavior. The variants instruct the model to display the convention\nformation behavior observed in human speakers in increasingly explicit and specific ways:\nS1: Standard Speaker The standard speaker setup (Section 2). The model speaker only\nreceives the basic game instruction, with no mention of communication efficiency.\nFigure 6 in the appendix shows an example prompt.\nS2: Gricean Instruction A relatively light-handed and general way to introduce the\nexpected convention formation behavior is to explicitly instruct the model to follow\nthe Gricean quantity maxim. This kind of instruction is not specific to reference\ngames, and does not explicitly mention message length. Its focus is information,\nand it entails that cooperative interlocutors would provide enough information\nto identify the referent but would not make the message more informative than\nnecessary. We add additional instructions based on the maxim and further instruct\nthe model to think about how the amount of information needed may change as more trials\nare completed and based on the listener’s performance in previous trials.4,5\nS3: Explicit Instruction We instruct the model to explicitly reduce message length as\nthe interaction progresses. Unlike S1 and S2, this instruction is specific to reference\ngames, as language adaptation in other scenarios is not necessarily accompanied by\nlength reduction (Effenberger et al., 2021). We add to S1 an explicit instruction to\n4The models did not show substantial improvement without this additional instruction.\n5Not the exact prompt used for experiments; shortened and revised for illustrative purposes.\n5\nPublished as a conference paper at COLM 2024\nreduce utterance length: as more trials are completed and as the listener understands you\nbetter, gradually condense your messages, making them shorter and shorter every trial.5\nS4: Explicit Instruction + Consistency Request Convention formation in reference\ngames is not only characterized by reduction in utterance length, but also by lexical\nconsistency. This variant explicitly instructs the model to follow this pattern. Similar\nto S3, it is specific to the repeated reference game setup and its use of a repeating\ncontext. We add to S3 the instruction: when creating a shorter message for an image, try\nto extract salient tokens from the previous messages for this image rather than introducing\nnew words. The short messages should still allow the listener to choose the target correctly.\nFor each image, when you reach a message that cannot be further shortened, you should\nkeep using that message for the rest of the game.5\nFigure 2 shows the results for all variants, along with properties of the human messages\nfrom Hawkins et al. (2020b), which were collected using the standard setup. We report\nmean message length, WNR, and listener accuracy for each repetition. Overall, all models\nfail to spontaneously improve communication efficiency. It is only with fairly heavy-handed\ninstruction that GPT4, Gemini, and Claude show adaptation trends similar to humans.\nVariant S1 shows that without any explicit instruction, the models show trends that are\nfar from human behavior. GPT4, Gemini, and Claude generate longer utterances in later\nrepetitions. IDEFICS and LLaVa maintain consistent message lengths. But, upon close\ninspection, we observe they simply tend to repeat previously used messages for the same\nimage. This explains their very low and almost constant WNR. We analyze this further in\nSection 6. Also, the messages of IDEFICS and LLaVa are less effective in distinguishing the\ntarget images, as shown by the lower listener accuracies. This demonstrates the inability of\nthese models to correct their behavior based on feedback. No models show WNR trends\nsimilar to humans. GPT4, Gemini, and Claude constantly introduce new words as the game\nprogresses, as shown by higher WNR curves, even though we do not use token sampling for\ndecoding. Gemini shows a downward trend, but achieves this by the undesirable practice of\nmaking its messages longer every trial while making a relatively constant number of word\ninsertions or changes, so a bigger portion of the message is maintained.6\nGricean instruction (S2) leads GPT4 and Claude to reduce message length over time, though\nGPT4’s reduction is far from humans’. Both models have WNR curves significantly higher\nthan humans. As their messages shorten, they still frequently introduce new words and\ndo not stabilize the messages, a behavior adverse to communication efficiency. We further\ndiscuss this issue with S3, where more models display this issue.\nS3’s explicit instruction has no impact on IDEFICS and LLaVa. Both continue repeating\nmessages, failing to follow the instructions. Gemini and GPT4 show decreasing message\nlength trends similar to humans but still have longer messages than humans throughout.\nClaude eventually produces messages as short as humans but starts with much longer\nmessages than other models. All models showing message shortening frequently introduce\nnew words as they shorten the messages. Even when the message is too short to be\ncondensed, the models may adopt new words next time without changing the message\nlength. Figure 8 in the appendix exemplifies these behaviors. Such behaviors deviate from\nthe stability property of conventions and the observation that human messages show high\nconsistency and convergence. The gap between the WNR curves of these models and\nhumans illustrates this issue. While these models show increasing lexical efficiency, the use\nof new words reduces communication efficiency, burdening the listener to reason about the\nwords that did not appear the last time the image was referenced. We further discuss this\nissue in Section 6.\nS4 addresses the consistency issue but requires further explicit instruction. The final prompt\nelicits from GPT4, Gemini, and Claude both length reduction and message convergence, as\nobserved with humans. However, the S4 prompt is very specific to ICCA’s setup and does\nnot generalize beyond reference games. Heavy-handed prompt interventions like this are\n6This is due WNR’s length-normalization, which is critical to reflect similarity (Appendix A.2).\n6\nPublished as a conference paper at COLM 2024\nalso known to cause unintended model behaviors (Shaikh et al., 2024). Prompt engineering\nis not likely to be the solution.\n5\nModel-as-listener Experiments\nListener experiments follow a setup similar to the speaker experiments as far as models\nand prompt optimization (Section 4). Gemini, LLaVa, and Claude cap the number of input\nimages, limiting their use in some of our listener variants. Overall, we design four main\nvariants, each implemented through the pre-processing function F. Our design process is\niterative, with some variants designed based on the behavior observed with earlier ones.\nThroughout the listener variants, we keep the instruction I largely constant and about the\nrole of the listener. We vary how we display the referential context.\nThe listener action space is more limited, simply requiring the model to select the referenced\nimage. We focus on evaluating model accuracy, similar to how listener behavior is charac-\nterized in human studies. Figure 3 visualizes the behaviors we observe. We also include\nhuman listener accuracy trends as reference to model accuracies.\nThe starting point for the listener study is the standard reference game setup (Section 2):\nL1: Standard Listener\nImages are shuffled and re-displayed for each trial, so each\nimage will potentially have a new label relative to previous trials.\nL1 requires a growing number of images in the prompt as the interaction progresses. With\nsix repetitions of four trials and a context of four images, the maximum number of images\nin the prompt at the end of the interaction is 96. Gemini, LLaVa, and Claude can take at\nmost 16, 4, and 20 images, so we only use GPT4 and IDEFICS with L1.\nWe expect an effective model to exploit the conversation history to reason about the human\nspeaker’s conventionalized ways of speaking. Even if the model starts with low accuracy, it\nhas the opportunity to improve because the prompt at later stages includes feedback for its\nchoices, and as the messages conventionalize, later messages for an image are often exact\nrepetitions, albeit with the referential context shuffled.\nBoth GPT4 and IDEFICS do significantly worse than humans (Figure 3, left). As expected,\nhumans demonstrate strong performance to start with, and show an upward trend in ac-\ncuracy, as the interlocutors adapt to each other. GPT4 is significantly worse than humans,\nthough performing fairly well. It shows a marginal improvement trend (88.9%→92.5% in\nrepetition 5), but it is not significant, and weakens in the last repetition (91.2%). IDEFICS\nis much worse immediately in the beginning (46.8%), and rather than improving, its per-\nformance deteriorates as the interaction progresses, reaching random chance in the later\ntrials. This happens even though it is receiving an increasing amount of information that\nshould allow it to improve its performance. A possible cause for this trend is the dramatic\nincrease in the prompt size, especially as more and more images are added, as the interaction\nprogresses. We further discuss this issue and its potential causes in Section 6.\n5.1\nHistory and Context Impact\nFollowing the observations with L1, we design three variants with simplified referential\ncontexts and history to better understand how well the models handle the interaction history\nand the referential context:\nL2: No History Each of the 24 trials is given to the model in isolation, without any\nhistory. The model input includes the context of four images and the speaker\nutterance, as well as the basic game instruction. This variant reveals the extent\nto which the model can reason about the ad-hoc conventions formed in repeated\nhuman-human interaction without access to the history in which they were formed.\nL3: Images Once\nA potential challenge of L1 is the large number of images in the\nprompt. A model’s architecture or training may not be suitable for handling a\nlarge number of images, and LLM prompt length is known to adversely influence\n7\nPublished as a conference paper at COLM 2024\n1\n2\n3\n4\n5\n6\n25\n50\n75\n100\nAcc %\nL1: Standard Listener\n1\n2\n3\n4\n5\n6\nRepetition #\nL2: No History\n1\n2\n3\n4\n5\n6\nL3: Images Once\n1\n2\n3\n4\n5\n6\nL4: No Shuffle\nIDEFICS\nLLaVa\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 3: Listener experiments. Margins of Error are 95% bootstrapped CIs.\nperformance (Liu et al., 2023b). L3 uses a shorter history, by only providing the\nreferential context (four images) to the model once in the first trial. Image labels are\npersistent across all trials, which also avoids the impact of shuffling. Unlike L2, this\nvariant includes the complete message, selection, and feedback history.\nL4: No Shuffle The four images appear every trial similar to L1 but are not shuffled\nacross trials. L4 shows the effects of image shuffle if compared with L1. It also\nshows the effect of image quantity if compared with L3, which does not involve\nshuffling either.\nL2 and L3 need only four images in the prompt, allowing us to test all the models.\nThe no-history variant (L2) reveals different performance trends for IDEFICS and GPT4\ncompared to the standard setup (L1). IDEFICS is still not doing great (45.8% on the first\nrepetition), but performance largely remains consistent across repetitions, indicating that\npossibly the complexity of the prompt is at the root of its downward trend in the L1 scenario.\nGPT4 starts with similar performance (87.5%) to its results in L1, but then shows a slight\ndownward trend (83.8% at the end), in contrast to the initial upward trend in L1. This\nindicates that the gradually conventionalized, shorter messages a human speaker uses tend\nto be more difficult for the model to resolve and that the conversation history can be an\neffective remedy. Gemini (80.2→78.8%), LLaVa (72.7→71.3%), and Claude (57.4→55.5%)\nshow similar trends to GPT4 from Repetition 1 to 6, with overall lower accuracies.\nThe benefit of history becomes more conspicuous when the referential context is only given\nonce (L3). This variant dramatically simplifies the prompt, but retains the information\nneeded for convention formation (i.e., prior message, selection, and feedback). All the\nmodels show an upward trajectory as the interactions progress. GPT4 and Claude are\nthe strongest, eventually reaching 100% accuracy, matching human listeners’ performance\n(99.54%). This suggests that all models can associate the current message with the relevant\nprior messages, thereby increasing their prediction accuracy as the interaction progresses.\nAdmittedly, because an image always has the same label throughout the game under this\nsetup, the models may do well by simply drawing associations between an image’s label\nand the messages that have referred to that image (i.e., label-message associations). Under\nthis mechanism, the model can improve its accuracy without reasoning about the actual\nvisual input. We explore this possible mechanism with more game variants in Appendix D.\nNonetheless, this experiment shows that MLLMs possess some capabilities for increasingly\nefficient communication with humans when acting as the listener.\nThe no-shuffle (L4) experiment also provides key insights. GPT4’s accuracy is similar to\nthat in L3 and higher than that in L1. This demonstrates sensitivity to image shuffling and\nthe constantly changing image labels. GPT4 in L3 and L4 may be relying to some degree on\ntext similarity between repetitions by exploiting the label-message associations, rather than\ngrounding to the visual input. We study this further in Appendix D. IDEFICS’ performance\n8\nPublished as a conference paper at COLM 2024\non the other hand is different from both L3 and L1, showing an almost unnoticeable trend\nup. This shows that it is not only the shuffling but also the increase in the number of images\nthat IDEFICS cannot seem to handle well. We further discuss this issue in Section 6.\n6\nDiscussion\nOur studies point to various issues that likely hinder specific models from displaying\ncommunication efficiency gains, and point out directions for future works.\nTendency to Repeat Messages\nIn the speaker study, IDEFICS and LLaVa tend to repeat\nthe first message they use for each image, showing no adaptation. To further study how\nmuch these models prefer patterns of repetition, we design a test that uses these models\nfor language modeling rather than text generation. We construct two transcripts for each\nof the 54 human-human interactions in our original dataset. One is the original transcript\nfrom human-human interactions, showing the natural evolution of messages. The other is\na manipulated transcript where the speaker repeats the messages from Repetition 1 in all\nthe later repetitions. We calculate the log probability and perplexity IDEFICS and LLaVa\nassign to these transcripts, count the number of times one type of transcript has better log-\nprobability\/perplexity than the other, and apply a sign test. We find that the manipulated\ntranscript showing message repetitions consistently receives higher log probability and\nlower perplexity for all 54 interactions, showing the models’ significant tendency towards\nrepeated patterns (sign test p-values are near zero). Unfortunately, this experiment cannot\nbe done with GPT4, Gemini, or Claude due to API limitations.\nLexical Efficiency ̸= Communication Efficiency\nThe convergence of human speaker mes-\nsages for a particular image to a short, stable convention often takes the form of extracting\nsalient tokens from the previous message and sticking to the same message once it becomes\nvery short (Hawkins et al., 2020a). Unless directly instructed to do so through a highly\nengineered prompt (S4), GPT4, Gemini, and Claude often introduce new words when\nshortening their messages or even when the messages cannot be further shortened, as\nshown in the S3 explicit instruction variant. Such inconsistency with human behaviors is\nproblematic. When messages for the same image do not converge, no conventions can form\nand the listener will likely need additional cognitive effort to process the previously unseen\nwords. Intuitively, even if a new message is semantically similar to a previous one by using\nsynonyms, resolving it still likely entail a greater cognitive load than an exact repetition.\nMoreover, when new words describing a new aspect of an image are introduced after a\nfew rounds of relatively similar messages, they violate the human listener’s expectation,\npotentially leading to miscommunication and slower response (Metzing & Brennan, 2003).\nPerformance Degradation with Many-image Inputs\nAmong the models that support a\nlarge number of images, IDEFICS performs much worse as the number of images increases.\nEven though the images in L4 are not shuffled across trials, which could have allowed the\nmodel to exploit label-message associations as an efficient way to gain high accuracy, the\nmodel still had much lower accuracy than when the images only appear once (L3) (Figure 3).\nWhen the history contains a growing number of images that are shuffled between trials (L1),\nIDEFICS shows an even worse accuracy trend.\nA likely hypothesis is that a greater number of images creates challenges for capturing the\ndependency between specific visual input and textual cues, which can manifest as failures to\nassociate an image’s label with the actual content of the image. In a qualitative experiment,\nwe supply a sequence of images and their labels as input to IDEFICS, and instruct it to\ndescribe Image [X]. We observe that IDEFICS can describe the correct image easily when we\ngive up to four labeled images, but often makes mistakes as the number increases (Figure 9\nin the appendix). Therefore, even though IDEFICS is designed and trained to support\nmulti-image inference,7 its multi-image capabilities do not generalize beyond a few images.\n7The Flamingo architecture behind IDEFICS supports an arbitrary number of images as input and\nit is trained on interleaved texts and multiple images (Huggingface, 2023).\n9\nPublished as a conference paper at COLM 2024\nAnother potential cause is the Flamingo architecture behind IDEFICS. Each token’s cross-\nmodal attention only applies to the visual features of the last image that precedes it, rather\nthan all the images. Information about other images can only be indirectly accessed through\nself-attention on the hidden states of their respective <image> tokens in the text sequence.\nThis architecture may degrade the ability of the model to reason about images, depending\non their locations in the input. When the target is not the last image, Image D, the message\ntokens will not have direct cross-modal attention to the target’s visual features, which may\nhurt IDEFICS’ prediction. In L3, where IDEFICS did perform well, this limitation might have\nbeen mitigated by the strong textual cues and label-message associations, whereas having\nmore images may distract IDEFICS from these cues and thus manifests this limitation.\n7\nConclusion\nICCA provides a perspective into the performance of today’s MLLMs that is missing in\nexisting benchmarking, and can be easily applied to new MLLMs without collecting new\nhuman data. We observe that state-of-the-art models lack the in-context abilities to adapt\ntheir own language for efficient communication, even though they may sometimes perform\nbetter while passively receiving increasingly efficient language from their interlocutor. This\nissue is fundamental because, unlike humans, the models do not perceive the effort or\ncost needed for communication, thus having no inherent reason to reduce them. It is\nstill surprising though, given that LLMs\/MLLMs have successfully displayed many other\nhuman behaviors and impressive abilities in various applications, by learning from the large\namounts of human data, where adaptation for efficiency is common. Overall, the current\nparadigm for creating LLMs fails to address the need for conversational adaptation and\nfuture research is needed on improving their abilities to spontaneously improve language\nefficiency, maintain language consistency for the same referent, avoid excessive tendency\nfor repetitions, and handle more images in a single query.\nAcknowledgments\nThis research was supported by ARO W911NF21-1-0106, NSF under grant No. 1750499, a\ngift from Open Philanthropy, and a gift from Apple. We thank Robert Hawkins and Marten\nvan Schijndel for insightful discussions. We thank the anonymous reviewers and the area\nchair for their valuable feedback.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Has-\nson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman\nRing, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Ne-\nmatzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan.\nFlamingo:\na visual language model\nfor few-shot learning.\nIn Proceedings of Advances in Neural Information Processing\nSystems, 2022. URL https:\/\/proceedings.neurips.cc\/paper files\/paper\/2022\/file\/\n960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf.\nJacob Andreas and Dan Klein. Reasoning about pragmatics with neural listeners and\nspeakers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,\n2016. URL https:\/\/www.aclweb.org\/anthology\/D16-1125.\nChinmaya Andukuri, Jan-Philipp Fr¨anken, Tobias Gerstenberg, and Noah D. Goodman.\nStar-gate: Teaching language models to ask clarifying questions. arXiv, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2403.19154.\nAnthropic. Introducing the next generation of Claude, 2024. URL https:\/\/www.anthropic.\ncom\/news\/claude-3-family.\n10\nPublished as a conference paper at COLM 2024\nSusan E. Brennan and Herbert H. Clark. Conceptual pacts and lexical choice in conversation.\nJournal of Experimental Psychology: Learning, Memory, and Cognition, 22, 1996.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. In Proceedings of Advances in Neural Information Processing Systems, vol-\nume 33, 2020. URL https:\/\/proceedings.neurips.cc\/paper files\/paper\/2020\/file\/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nKranti Chalamalasetti, Jana G¨otze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler,\nand David Schlangen. Clembench: Using game play to evaluate chat-optimized language\nmodels as conversational agents. arXiv, 2023. URL https:\/\/arxiv.org\/abs\/2305.13455.\nHerbert H. Clark. Common ground. Cambridge University Press, 1996.\nHerbert H. Clark and Susan E. Brennan. Grounding in communication. In Perspectives on\nSocially Shared Cognition. American Psychological Association, 1991.\nHerbert H. Clark and Deanna Wilkes-Gibbs.\nReferring as a collaborative process.\nCognition, 22, 1986.\nURL https:\/\/www.sciencedirect.com\/science\/article\/pii\/\n0010027786900107.\nPhilip R. Cohen and Hector J. Levesque. Rational Interaction as the Basis for Communication.\nThe MIT Press. URL https:\/\/doi.org\/10.7551\/mitpress\/3839.003.0014.\nMarco Del Tredici, Xiaoyu Shen, Gianni Barlacchi, Bill Byrne, and Adri`a de Gispert. From\nrewriting to remembering: Common ground for conversational qa models. 2022. URL\nhttps:\/\/arxiv.org\/abs\/2204.03930.\nAnna Effenberger, Rhia Singh, Eva Yan, Alane Suhr, and Yoav Artzi. Analysis of language\nchange in collaborative instruction following. In Findings of the Association for Computa-\ntional Linguistics: EMNLP, 2021. URL https:\/\/aclanthology.org\/2021.findings-emnlp.\n239.pdf.\nRon Eliav, Anya Ji, Yoav Artzi, and Robert Hawkins. Semantic uncertainty guides the\nextension of conventions to new referents. arXiv, 2023. URL http:\/\/arxiv.org\/abs\/2305.\n06539.\nDaniel Fried, Jacob Andreas, and Dan Klein. Unified pragmatic models for generating\nand following instructions. In Proceedings of the Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, 2018. URL\nhttps:\/\/www.aclweb.org\/anthology\/N18-1177.\nEdward Gibson, Richard Futrell, Steven P. Piantadosi, Isabelle Dautriche, Kyle Mahowald,\nLeon Bergen, and Roger Levy. How efficiency shapes human language. Trends in Cog-\nnitive Sciences, 23, 2019. URL https:\/\/www.sciencedirect.com\/science\/article\/pii\/\nS1364661319300580.\nNoah D. Goodman and Michael C. Frank. Pragmatic language interpretation as probabilistic\ninference. Trends in Cognitive Sciences, 20, 2016. URL https:\/\/api.semanticscholar.org\/\nCorpusID:3632786.\nGoogle. Introducing Gemini: Our largest and most capable AI model, 2023. URL https:\n\/\/blog.google\/technology\/ai\/google-gemini-ai\/.\nBarbara J. Grosz and Candace L. Sidner. Plans for discourse. In Intentions in Communication.\nThe MIT Press. URL https:\/\/doi.org\/10.7551\/mitpress\/3839.003.0022.\n11\nPublished as a conference paper at COLM 2024\nJanosch Haber, Tim Baumg¨artner, Ece Takmaz, Lieke Gelderloos, Elia Bruni, and Raquel\nFern´andez. The PhotoBook dataset: Building common ground through visually-grounded\ndialogue. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\n2019. URL https:\/\/aclanthology.org\/P19-1184v2.pdf.\nRobert Hawkins, Michael C. Frank, and Noah D. Goodman. Characterizing the dynamics\nof learning in repeated reference games. Cognitive Science, 44, 2020a. URL https:\/\/\nonlinelibrary.wiley.com\/doi\/abs\/10.1111\/cogs.12845.\nRobert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. Continual adaptation\nfor efficient machine communication. In Proceedings of the Conference on Computational\nNatural Language Learning, November 2020b.\nWilliam S. Horton and Richard J. Gerrig. Revisiting the memory-based processing approach\nto common ground. Topics in Cognitive Science, 8, 2016. URL https:\/\/onlinelibrary.\nwiley.com\/doi\/full\/10.1111\/tops.12216.\nHuggingface. HuggingFaceM4\/idefics-80b-instruct, 2023. URL https:\/\/huggingface.co\/\nHuggingFaceM4\/idefics-80b-instruct.\nAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins,\nand Yoav Artzi. Abstract visual reasoning with tangram shapes. In Proceedings of the\nConference on Empirical Methods in Natural Language Processing, December 2022. URL\nhttps:\/\/aclanthology.org\/2022.emnlp-main.38.\nRobert M. Krauss and Sidney Weinheimer. Changes in reference phrases as a function of\nfrequency of usage in social interaction: A preliminary study. Psychonomic Science, 1, 1964.\nURL https:\/\/link.springer.com\/article\/10.3758\/BF03342817.\nRobert M. Krauss and Sidney Weinheimer. Concurrent feedback, confirmation, and the\nencoding of referents in verbal communication. Journal of Personality and Social Psychology,\n4, 1966.\nDavid Kellogg Lewis. Convention: A Philosophical Study. Wiley-Blackwell, 1969.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. arXiv, 2023a. URL https:\/\/arxiv.org\/pdf\/2310.03744.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts. Transactions\nof the Association for Computational Linguistics, 12, 2023b. URL https:\/\/aclanthology.org\/\n2024.tacl-1.9.pdf.\nArthur B. Markman and Valerie S. Makin. Referential communication and category acquisi-\ntion. Journal of Experimental Psychology: General, 127, 1998.\nBill McDowell and Noah D. Goodman. Learning from omission. In Proceedings of the Annual\nMeeting of the Association for Computational Linguistics, 2019. URL https:\/\/aclanthology.\norg\/P19-1059.pdf.\nCharles Metzing and Susan E. Brennan. When conceptual pacts are broken: Partner-specific\neffects on the comprehension of referring expressions. Journal of Memory and Language, 49,\n2003. URL https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0749596X03000287.\nWill Monroe, Robert Hawkins, Noah D. Goodman, and Christopher Potts.\nColors in\ncontext: A pragmatic neural model for grounded language understanding. Transactions\nof the Association for Computational Linguistics, 5, 2017. URL https:\/\/aclanthology.org\/\nQ17-1023.pdf.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat,\nRed Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao,\nMohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\n12\nPublished as a conference paper at COLM 2024\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brak-\nman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie\nCampbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen,\nMark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty\nEleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim´on Posada Fishman,\nJuston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun\nGogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han,\nJeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade\nHickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali\nKamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,\nJong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt\nKnight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis,\nKyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,\nJade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam\nManning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg\nMurk, David M´ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan,\nRichard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino,\nJoe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,\nAlex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute\nPeres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle\nPokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real,\nKendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted\nSanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schul-\nman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian\nSohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet,\nAmin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cer´on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C. J. Weinmann,\nAkila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu,\nMichael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,\nRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang\nZhuang, William Zhuk, and Barret Zoph. GPT-4 Technical Report. arXiv, 2024. URL\nhttp:\/\/arxiv.org\/abs\/2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder,\nPaul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow\ninstructions with human feedback. In Proceedings of the Conference on Neural Information\nProcessing Systems, 2022.\nURL https:\/\/proceedings.neurips.cc\/paper files\/paper\/\n2022\/file\/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors\nfor word representation. In Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, 2014. URL http:\/\/www.aclweb.org\/anthology\/D14-1162.\n13\nPublished as a conference paper at COLM 2024\nOmar Shaikh, Kristina Gligoric, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, and Dan\nJurafsky. Grounding gaps in language model generations. In Proceedings of the Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, June 2024. URL https:\/\/aclanthology.org\/2024.naacl-long.348.\nRobert Stalnaker. Common ground. Linguistics and Philosophy, 25, 2002. URL https:\n\/\/link.springer.com\/article\/10.1023\/A:1020867916902.\nAlberto Testoni and Raquel Fern´andez. Asking the right question at the right time: Human\nand model uncertainty guidance to ask clarification questions. In Proceedings of the\nConference of the European Chapter of the Association for Computational Linguistics, 2024. URL\nhttps:\/\/aclanthology.org\/2024.eacl-long.16.pdf.\nDavid Traum. A Computational Theory of Grounding in Natural Language Conversation. PhD\nthesis, University of Rochester, 1994.\nJulia White, Jesse Mu, and Noah D. Goodman. Learning to refer informatively by amortizing\npragmatic reasoning. In Proceedings of the Annual Meeting of the Cognitive Science Society,\n2020. URL https:\/\/cognitivesciencesociety.org\/cogsci20\/papers\/0175\/0175.pdf.\nKayo Yin, Terry Regier, and Dan Klein. American sign language handshapes reflect pressures\nfor communicative efficiency. arXiv, 2024. URL https:\/\/arxiv.org\/abs\/2406.04024.\nHao Zhu, Graham Neubig, and Yonatan Bisk. Few-shot language coordination by modeling\ntheory of mind. In Proceedings of the International Conference on Machine Learning, 2021.\nURL https:\/\/proceedings.mlr.press\/v139\/zhu21d\/zhu21d.pdf.\nGeorge Kingsley Zipf. Human Behavior and the Principle of Least Effort: An Introduction to\nHuman Ecology. Addison-Wesley Press, 1949.\n14\nPublished as a conference paper at COLM 2024\nA\nImplementation Details\nA.1\nMessage Length Measurement\nWe measure the length of the generated messages by counting the number of tokens. Because\ndifferent MLLMs have different tokenizers, we choose to only use IDEFICS’s tokenizer for\nmessage length calculations for all models.\nA.2\nWord Novelty Rate\nWord Novelty Rate is a modified Word Error Rate, which only counts insertions and substi-\ntutions, and ignores deletions. The number of insertions and substitutions is normalized by\nthe length of the reference message, as done in the standard Word Error Rate calculation.\nFor two messages from Repetition N-1 and N, we use the message from Repetition N-1\nas the reference and the one from Repetition N as the hypothesis. We follow Hawkins\net al. (2020a)’s metric design and drop most function words to only consider open-class\ncontent words (nouns, adjectives, verbs, and adverbs) as well as pronouns, numbers, and\nadpositions.\nWNR addresses the limitation of metrics using cosine similarity between averaged embed-\ndings (e.g., GloVe), which operate in the semantic space, for example as in Hawkins et al.\n(2020a). Semantic similarity between messages is not sensitive to some lexical changes,\nignoring the importance of exact word choice in convention formation. For example, once a\nconvention is formed to use the car to refer to an image, changing it to a semantically similar\nmessage, the automobile, violates the stability property of conventions, which may increase\nthe listener’s cognitive load. Empirically, we find that WNR produces results consistent\nwith Hawkins et al. (2020a)’s averaged GloVe embedding similarity, as shown in Figure 4\n(WNR moves in the opposite direction to the GloVe-based similarity because WNR directly\nmeasures dissimilarity).\n1\n2\n3\n4\n5\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nRepetition #\nGloVe-Sim\nWNR\nFigure 4: GloVe embedding similarity and WNR between messages from consecutive\nrepetitions. Every increase in GloVe embedding similarity is captured by a corresponding\ndecrease in WNR, and vice versa. Margins of Error are 95% bootstrapped CIs.\nWe also report a variant of WNR, which is not normalized by length. We refer to this variant\nas Word Novelty Distance (WND). WND is sometimes more interpretable because each\n15\nPublished as a conference paper at COLM 2024\nunit difference in it directly corresponds to a word insertion or deletion. However, because\nWND is small between any two short messages, it can be harder to interpret when messages\nare short. Figure 5 reports WND for our speaker experiments.\n1\n2\n3\n4\n5\n6\n0\n1\n2\n3\n4\n5\n6\n7\nWND\nS1: Standard Speaker\n1\n2\n3\n4\n5\n6\nRepetition #\nS2: Gricean Instruction\n1\n2\n3\n4\n5\n6\nS3: Explicit Instruction\n1\n2\n3\n4\n5\n6\nS4: Explicit Instruc-\ntion+Consistency Request\nIDEFICS\nLLava\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 5: Word Novelty Distance for speaker experiments. Margins of Error are 95%\nbootstrapped CIs.\nA.3\nMLLM Implementation Details\nThe exact versions of the MLLMs used are idefics-80b-instruct, llava-1.5-13b, gpt-4-1106-\nvision-preview, Gemini 1.0 Pro Vision, and claude-3-opus-20240229. For IDEFICS, we use\n8-bit quantization to fit the 80b model into 3 A6000 GPUs. For all MLLMs, we use a decoding\ntemperature of 0 to avoid the uncertainty caused by sampling, which was also the default\nfor IDEFICS and LLaVa. We use the models’ default values for other hyperparameters.\nThe GPT4 listener used to evaluate model speakers follows the listener interaction setup in\nL3: Images Once. L3 is the scenario where GPT4 shows its best performance and almost\nperfectly matches human performance.\nLLaVa only supports taking 1 image as input. To bypass this constraint, we merge the 4\nimages in the referential context into 1 image, using a 2-by-2 grid. Instead of using image\nlabels (A, B, C, D), experiments with LLaVa refer to an original image using its location in\nthe merged image (top right, top left, bottom right, and bottom left).\nWe conduct prompt engineering for each model individually to find the most suitable\nphrasing of the instructions. The prompt engineering was done over the pilot study dataset\nreleased in Hawkins et al. (2020b)’s official Github repository. The pilot study dataset is\ndistinct from the 54 human interactions in ICCA that we use for evaluation. This pilot study\ndataset contains human-human interactions on easy referential contexts, where the target\nimages are more easily distinguishable to humans with very short messages. For this reason,\nthe human interactions in this pilot study dataset show less language adaptation, and we\nonly use it for prompt engineering and not evaluation.\n16\nPublished as a conference paper at COLM 2024\nB\nExample Prompts\nModel-as-speaker Prompt:\n[System] Play a game with a listener. This game consists of multiple trials and 4 images (labeled A,\nB, C, D). You will act as the speaker in this game. In each trial, one of the images is given as the\ntarget. You will generate a message to tell the listener which image is the target without mentioning\nany image label. The listener will try to choose the target correctly based on your message. You will\nknow which image the listener guesses, so you may adjust your messages based on the listener’s\naccuracy. Your reply should only contain the message and be shorter than 20 words. Do not mention\nany image’s label (A, B, C, D) in your message.\nImage A: <img1>\nImage B: <img2>\nImage C: <img3>\nImage D: <img4>\nTrial 1, the target is Image B.\n[Speaker] Message: Two bananas and apples in a white bowl on polka dot cloth.\n[System] The listener correctly answered Image B.\nTrial 2, the target is Image A.\n[Speaker] Message: Bananas on a floral cloth with an Amazon box.\n[System] The listener correctly answered Image B.\nTrial 3, the target is Image D.\n[Speaker] Message: Bananas and apples in a bowl, striped cloth underneath.\n[System] The listener mistakenly answered Image C.\nTrial 4, the target is Image C.\n[Speaker] Message: Assorted fruit in a painted ceramic bowl.\n[System] The listener correctly answered Image C.\nTrial 5, the target is Image B.\n[Speaker] Message:\nFigure 6: Example prompt for model-as-speaker standard setup. The prompt is shortened\nand revised for illustrative purposes.\n17\nPublished as a conference paper at COLM 2024\nModel-as-listener Prompt:\n[System] Play a game with multiple trials involving the same set of images. In each trial, I will refer\nto one of the images with a message. You will guess which image I’m referring to. If present, the\nhistory of the previous trials may help you better understand how I refer to specific images.\nTrial 1\nImage A: <img4>\nImage B: <img2>\nImage C: <img3>\nImage D: <img1>\nWhich image is this message referring to: The photo with ‘amazon’ on the back, with 2 bananas.\n[Listener] Image D\n[System] Correct. I was referring to Image D.\nTrial 2\nImage A: <img1>\nImage B: <img3>\nImage C: <img2>\nImage D: <img4>\nWhich image is this message referring to: Photo with a bowl of 3 bananas with a pokadot background.\n[Listener] Image B\n[System] Wrong. I was referring to Image C.\nTrial 3\nImage A: <img3>\nImage B: <img1>\nImage C: <img2>\nImage D: <img4>\nWhich image is this message referring to: A bowl full of mixed fruit, black background.\n[Listener] Image A\n[System] Correct. I was referring to Image A.\nTrial 4\nImage A: <img3>\nImage B: <img4>\nImage C: <img2>\nImage D: <img1>\nWhich image is this message referring to: Photo with a bowl of 3 bananas with stripes and other\npatterns in the background.\n[Listener] Image B\n[System] Correct. I was referring to Image B.\nTrial 5\nImage A: <img3>\nImage B: <img2>\nImage C: <img4>\nImage D: <img1>\nWhich image is this message referring to: Amazon in back with 2 bananas.\n[Listener] Image\nFigure 7: Example prompt for model-as-listener standard setup. The prompt is shortened\nand revised for illustrative purposes.\n18\nPublished as a conference paper at COLM 2024\nC\nFigures for Error Analysis\nHuman Messages\n1. dirty truck going towards the bridge\n2. dirty truck going to bridge\n3. how is this even guessing, it’s so easy, dirty truck going to bridge\n4. dirty truck\n5. dirty truck \n6. dirty truck\nGPT4 Messages\n1. construction bridge with a concrete mixer truck underneath\n2. bridge construction with a white truck\n3. bridge work, dirty truck below\n4. bridge construction\n5. overpass work\n6. concrete mixer\nFigure 8: Evolution of an image’s corresponding messages across 6 repetitions in S3. Both\nthe human speaker and the GPT4 speaker show length reduction, but GPT4’s messages are\nmore inconsistent.\nImage A:\nImage B:\nImage C:\nImage D:\nImage E:\nImage F:\nImage G:\nImage H:\nUser: describe Image D.\nIDEFICS: a truck is parked in a parking lot.\nFigure 9: Example of IDEFICS’ issue with associating images with their correct labels. The\ninput is a sequence of images interleaved with their labels and a prompt to ask the model to\ndescribe Image [X]. We tried different formats to label the images, such as <img> is Image A\nand Image A: <img> but none helped IDEFICS produce reliable results as the number of\nimages increases.\n19\nPublished as a conference paper at COLM 2024\nD\nImage Grounding Impact\n1\n2\n3\n4\n5\n6\n25\n50\n75\n100\nL5: Images Masked\n1\n2\n3\n4\n5\n6\nRepetition #\nL6: Images Misleading\n1\n2\n3\n4\n5\n6\nL7: Images Mis-\nleading in Rep. 6\nIDEFICS\nllava\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 10: Average accuracy for variants L5-7. Margins of Error are 95% bootstrapped CIs.\nThe results with L3 (Section 5.1) raised concerns about whether the models are using images\neffectively or just exploiting label-message association. We develop three variants to study\nthis:\nL5: Images Masked Each image is replaced by an image mask, where all pixels have\nthe black color, (0, 0, 0). The image order is persistent, so the same label refers\nalways to the same underlying image that is masked. For the first four trials in\nRepetition 1, the model has to make a guess. Regardless of the guess, the correct\ntarget is given by the system feedback as usual. Theoretically, the model can use\nthe feedback for later repetitions. For a model to succeed in this variant, it must\nassociate the messages with the image labels since no actual images are given. This\nvariant tests the extent to which a model can exploit label-message associations\nthrough in-context learning as a way of convention understanding.\nL6:\nImages Misleading This variant complements L5 to further study the impact of\ntext signals and images. The setup here follows L3, showing the referential context\nonce, except that we manipulate the images to be misleading. For the manipulation,\nwe shuffle the images when presenting them to the model listener at the beginning\nof the game, but we do not change the gold image labels in the system. Therefore,\nImage [X] from the speaker and the system’s perspective is likely different from the\nImage [X] from the listener’s perspective, and so on. For example, using the context\nin Figure 11, the model may see the message Photo with a bowl of 3 bananas with\npokadot and choose Image C but the system and the speaker would always think the\nimage that features bananas and polka dot is Image B, since we shuffled the images\nwithout updating the gold labels accordingly. Then the system feedback would be\nwrong, the correct answer is Image B. To succeed under this setting, the model must\nlearn to ignore the images and just associate all the descriptions related to polka\ndot (for example) with the label Image B.\nL7:\nImages Misleading in Rep. 6 This variant further tests if the models’ previous\npromising performance in L3 comes from just exploiting the textual signals (the\nlabel-message associations) and ignoring the visual input. We show the context\nonce similar to L3 at the beginning, but then show shuffled versions during each\ntrial in the last repetition, without updating the gold labels (same manipulation as\nL6). We hypothesize that if the model tends to ignore the image input and rely on\ntextual associations in the conversation history, this manipulation will have little\nimpact on its prediction or the accuracy calculated based on the old gold label.\nThis variant requires 20 images in the last trial so it cannot be used with LLaVa or\nGemini.\n20\nPublished as a conference paper at COLM 2024\nWhen models cannot utilize the image input (L5 and L6), all models start with around\nor below random chance accuracies (25%). As the interaction progresses, models exploit\npast messages and feedback via label-message associations to improve performance and\ndisplay a trend of improvement. L5 and L6 results together demonstrate that exploiting\nlabel-message associations while ignoring the images can easily emerge as an effective\nmechanism for MLLMs’ convention understanding behavior, provided that the image\nreferent has a consistent textual label.\nInjecting misleading images in the last repetition (L7) leads to a significant drop in per-\nformance relative to previous repetitions for IDEFICS (87.5→69.4%), GPT4 (99.5→27.3%),\nand Claude (98.6→22.2%). The drop is particularly conspicuous for GPT4 and Claude,\nwhere performance goes down to around random chance. This suggests that GPT4 and\nClaude do not overly exploit the consistent textual associations from the history, because\notherwise they would not have been ‘misled’ by the manipulations in the last repetition.\nThis is desirable because it shows that the visual input matters to these models’ output.\nOn the other hand, while IDEFICS also shows a drop in performance, it is not as strong as\nwhat the other two models show. This indicates that although IDEFICS is not completely\nignoring the visual input, it does exploit the textual associations beyond what is desired.\nA\nB\nC\nD\nD\nC\nB\nA\nTrial 1\nSpeaker: Photo with a \nbowl of 3 bananas with \npokadot\nListener: Image C\nSpeaker: wrong, I was \nreferring to Image B. \nSpeaker’s Perspective\nListener’s Perspective\nFigure 11: Example Trial 1 from L6: Images Misleading\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs.pdf"}
{"title":"MLVU: Benchmarking Multi-task Long Video Understanding","authors":"Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu","summary":"The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.","url":"http:\/\/arxiv.org\/abs\/2406.04264v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2406.04264v3","published":1717693772000,"comment":null,"pdf_text":"MLVU: Benchmarking Multi-task Long Video Understanding\nJunjie Zhou* 1,2, Yan Shu∗1, Bo Zhao∗1,3, Boya Wu1, Zhengyang Liang1, Shitao Xiao1,\nMinghao Qin1, Xi Yang1, Yongping Xiong2, Bo Zhang4, Tiejun Huang1,5, Zheng Liu† 1\n1 Beijing Academy of Artificial Intelligence, 2 Beijing University of Posts and Telecommunications,\n3 Shanghai Jiao Tong University, 4 Zhejiang University, 5 Peking University\n{junjiebupt, bozhaonanjing, zhengliu1026}@gmail.com\nAbstract\nThe evaluation of Long Video Understanding (LVU) perfor-\nmance poses an important but challenging research problem.\nDespite previous efforts, the existing video understanding\nbenchmarks are severely constrained by several issues, es-\npecially the insufficient lengths of videos, a lack of diversity\nin video types and evaluation tasks, and the inappropriate-\nness for evaluating LVU performances. To address the above\nproblems, we propose a new benchmark called MLVU (Multi-\ntask Long Video Understanding Benchmark) for the compre-\nhensive and in-depth evaluation of LVU. MLVU presents\nthe following critical values: 1) The substantial and flex-\nible extension of video lengths, which enables the bench-\nmark to evaluate LVU performance across a wide range of\ndurations. 2) The inclusion of various video genres, e.g.,\nmovies, surveillance footage, egocentric videos, cartoons,\ngame videos, etc., which reflects the models’ LVU perfor-\nmances in different scenarios. 3) The development of di-\nversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs’ key abilities in long-video under-\nstanding. The empirical study with 23 latest MLLMs reveals\nsignificant room for improvement in today’s technique, as all\nexisting methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling\nlonger videos. Additionally, it suggests that factors such as\ncontext length, image-understanding ability, and the choice\nof LLM backbone can play critical roles in future advance-\nments. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive\nand in-depth analysis of MLLMs.\n1. Introduction\nLarge language models (LLMs) are growing into a general\nsolution for numerous AI tasks [6, 45]. In recent years,\nit becomes increasingly emphasized to extend LLMs with\nmulti-modal capabilities and thus bring the Multi-modal\n*Co-first authors\n†Corresponding author\nLLM, namely, MLLM. Remarkably, it has been made pos-\nsible for today’s MLLMs to perceive information in texts,\nimages, videos, etc., and solve complicated problems in\nphysical environments [1, 44]. Along with the development\nof MLLMs, new benchmarks are continuously created to\nfacilitate comprehensive and in-depth analysis of MLLMs\n[12, 26, 32, 57].\nHowever, it remains a great challenge to evaluate the\nMLLMs’ long-video understanding (LVU) performances\ngiven the following limitations. Firstly, the majority of exist-\ning video understanding benchmarks are made up of short\nvideos [19, 22, 26, 36, 52], whose lengths can be merely a\nfew seconds. As a result, they are insufficient to reflect the\nMLLMs’ long-video understanding capabilities. Secondly,\nthere is a notable lack of diversity in both video genres and\nevaluation tasks. Existing benchmarks often concentrate on\na single video type, such as egocentric videos [15, 34], or\nfocus on one specific task, like captioning [52]. These limi-\ntations hinder comprehensive evaluation of LVU capabilities.\nLast but not least, many previous evaluation tasks are not\nproperly designed for LVU, as they can be solved without\nusing the complex information from long videos. For ex-\nample, many questions are simply about one single frame\nin the long videos [41, 60]. Besides, numerous others are\nabout popular movies and celebrities [13, 27], which can be\nanswered directly by MLLMs based on the textual prompts.\nConceptually, MLLMs are expected to handle any type of\nlong video and accomplish any related tasks. Therefore, the\nevaluation of LVU should emphasize two important prop-\nerties: length and diversity. Furthermore, it is crucial that\nthe evaluation tasks are specifically designed to leverage the\ncomplex information inherent in long videos, addressing\nthe shortcomings of previous benchmarks. Based on these\nprinciples, we propose a novel benchmark called MLVU\n(Mult-task Long Video Understanding Benchmark), which\npresents the following critical advantages.\n• It makes a substantial extension for the video length.\nMLVU is created based on long videos of diversified\nlengths, ranging from 3 minutes to 2 hours. The average\n1\narXiv:2406.04264v3  [cs.CV]  1 Jan 2025\nBenchmarks\n#Videos\n#QA\nPairs\nLen. (s)\nClose-\nEnded\nOpen-\nEnded\nVarious\nGenres\nMulti-\nLevel\nMulti-\nDimension\nReferring\nQA\nNExT-QA [50]\n1,000\n8,564\n39.5\n✓\n✓\n✓\n✗\n✗\n✗\nTVQA [21]\n15,253\n15,253\n11.2\n✓\n✗\n✗\n✗\n✗\n✗\nMSRVTT-QA [52]\n2,900\n72,821\n15.2\n✓\n✗\n✗\n✗\n✗\n✗\nMVBench [26]\n3,641\n4,000\n16.0\n✓\n✗\n✓\n✗\n✗\n✗\nMovie101 [58]\n101\n-\n6144\n✗\n✓\n✗\n✗\n✗\n✗\nEgoSchema [34]\n5,063\n5,063\n180\n✓\n✗\n✗\n✗\n✗\n✗\nMovieChat-1K [41]\n130\n1,950\n500\n✓\n✓\n✗\n✗\n✓\n✗\nVideo-MME∗[13]\n900\n2,700\n1024\n✓\n✗\n✓\n✓\n✗\n✗\nLongVideoBench∗[49]\n3,763\n6,678\n473\n✓\n✗\n✓\n✓\n✗\n✓\nMLVU\n1,730\n3,102\n930\n✓\n✓\n✓\n✓\n✓\n✓\nTable 1. Comparison of MLVU with existing benchmarks, including the number of videos (#Videos), number of QA pairs (#QA pairs),\naverage video length (Len.), presence of Close-Ended tasks, presence of Open-Ended tasks, inclusion of various video genres (Various\nGenres), coverage of multiple duration levels (Multi-Level), inclusion of multiple dimensions of LVU tasks (Multi-Dimension), and\nquestions involving local information with clear referring context rather than direct timestamps [41] or well-known narrative elements [17, 27]\n(Referring QA). The first block represents short video understanding benchmarks, and the second block represents long video understanding\nbenchmarks. ∗denotes work concurrent with MLVU.\nvideo length is about 15 minutes, which makes it much\nlonger than most of the existing benchmarks. Additionally,\neach video is further segmented so that evaluation tasks\ncan be created w.r.t. different video clips (e.g., summa-\nrization for the first 3 minutes, the first 6 minutes, and the\nentire duration of the video). Therefore, it is able to flex-\nibly evaluate the MLLMs’ performance across different\nvideo lengths.\n• It encompasses a wide variety of video genres. MLVU\nincludes diverse real-world videos, such as movies, life\nrecords, and egocentric videos. Additionally, it features\ntypical simulated videos like games and cartoons. This di-\nversity allows for a comprehensive assessment of MLLMs’\nperformance across various application scenarios.\n• It introduces diversified evaluation tasks tailored for\nLVU. MLVU comprises 9 distinct tasks that collectively\nassess a wide range of MLLMs’ LVU capabilities. On\none hand, it includes both multiple-choice and open-ended\ngeneration tasks, reflecting the models’ performance in\nhandling different task formats. On the other hand, some\ntasks are designed to leverage global information from\nentire videos, while others require the use of specific local\ninformation from certain clips. Moreover, all questions in-\nvolving local information are annotated with unambiguous\ncontext, requiring MLLMs to accurately locate or infer the\nappropriate clips within long videos.\nTable 1 shows that MLVU provides a more comprehen-\nsive evaluation of LVU compared to existing and concurrent\nbenchmarks. We extensively investigate 23 popular MLLMs\nwith MLVU, which brings in several critical insights. Firstly,\nlong-video understanding remains a technically challenging\nproblem for the existing MLLMs. While GPT-4o1 achieves\nthe leading performance in the experiment, it only attains\n1https:\/\/openai.com\/index\/hello-gpt-4o\/\nan average score of 54.5% in multi-choice tasks. All meth-\nods struggle with tasks requiring fine-grained information\nfrom entire videos, such as action counting, ordering, and\nsummarization. Secondly, recent open-source long video\nMLLMs have made significant strides in LVU [11, 40, 60].\nThese advancements have improved the models’ capability to\nprocess extended visual sequences, thereby closing the gap\nwith leading proprietary models in recent months. Finally,\nthe empirical results underscore influential factors in LVU,\nsuch as the extension of context length, the improvement\nof image understanding ability, and the utilization of strong\nLLM-backbones. In addition to the benchmark’s overall\nconclusion, individual tasks enable fine-grained analysis of\nMLLMs’ performances in each specialized aspects. There-\nfore, we anticipate the benchmark to assist in improving\nMLLMs’ long-video understanding capabilities by provid-\ning insights into their current strengths and weaknesses.\n2. Related Work\nMultimodal Large Language Models.\nMultimodal large\nlanguage models (MLLMs) have attracted significant interest\nfrom both academia and industry. Recent advancements in\nthis field have been achieved by integrating LLM backbones\nwith visual encoders and adapters, and fine-tuning the en-\ntire architecture through visual instruction tuning [8, 29, 63].\nBased on the same philosophy, MLLMs have been further de-\nveloped for video processing using video instruction datasets\nand specialized video adapters [25, 26, 28, 33, 54, 59]. How-\never, most existing models are optimized for short videos,\ntypically under one minute, due to the difficulty in estab-\nlishing sufficient context for longer videos. To address this\nchallenge, researchers have explored compact video repre-\nsentations or extended the context length of MLLMs. For\ninstance, LLaMa-Vid [27] compresses each video frame\n2\nAction\nRomance\nScience Fiction\nSuspense\nWar\nComedy\nAction\nFamily\nFantasy\nAdventure\nGeography\nAnimal\nScience\nCooking\nHandcraft\nSoftware\nBasketball\nFootball\nPingpong\nVolleyball\nBadminton\nTravel\nPet\nFood\nDaily Life\nOutdoor\nIndoor\nIndoor\nOutdoor\nCartoon\nGame\nMovie\nTV Series\nDocumentary\nTutorial\nSport\nLife Record\nEgo-centric\nSurveillance\nSimulation\n0\n100\n200\n300\n400\n500\n(3, 5]\n(5, 8]\n(8, 11]\n(11, 15]\n(15, 30]\n(30, 120]\n(120, )\nNumber of Videos\nVideo Duration (min)\nPlot QA\n589\nEgo Reasoning\n405\nNeedle QA\n415\nSub-Scene\nCaptioning\n247\nAnomaly Recognition\n239\nTopic Reasoning\n355\nVideo\nSummarization\n257\nAction \nCount\n266\nAction\nOrder\n329\nMulti-Detail Task\nOpen-ended Task\nClose-ended Task\nHolistic Task \nSingle-Detail Task\nFigure 1. Statistical Overview of our MLVU benchmark. Left: Video genres included in MLVU; Top Right: Distribution of video duration;\nBottom Right: Task types and their counts in MLVU.\ninto two tokens, enabling the model to handle videos sev-\neral hours long. Methods like MovieChat [41] and MA-\nLMM [16] introduce specialized memory components for\nrecursive video processing. Furthermore, approaches such\nas LWM [30], LongVA [60], and Video-XL [40] are de-\nsigned to extend the context length of MLLMs, facilitating\nthe processing of longer video inputs. Additionally, it is\nalso explored to make selective usage of frames or clips\nfrom long videos based on retrievers or agents [38, 46, 53].\nDespite these progresses, it remains an open problem for\nMLLMs to effectively handle long videos.\nVideo Understanding Benchmarks.\nWith the unprece-\ndented interest in MLLMs, the creation of benchmarks for\nthese models has become increasingly emphasized (as ad-\nvanced by MMMU [57], MME [12], and many other pio-\nneering works). In video understanding, the research com-\nmunity has made significant efforts as well, particularly for\nshort videos. There are specialized benchmarks for temporal\nperception [48, 56], action understanding [47, 48], video\nclassification [18], video reasoning [50, 51], and video cap-\ntioning [35, 52]. Recently, MVBench [26] provides a com-\nprehensive short-video benchmark to evaluate general capa-\nbilities via question-answering. For long video understand-\ning, people seek to leverage long-form videos, like movies,\nto create benchmarks.\nFor example, LLaMA-Vid [27]\ndeveloped a movie question-answering dataset based on\nMovieNet [17]. Despite using long videos, many questions\nfocus on well-known narrative elements, allowing them to be\nanswered without analyzing the video’s content. In contrast,\nMovieChat [41] avoids specific character names or plot de-\ntails in its questions. However, since each question provides\na specific timestamp, the tasks can be reduced to short-video\nor image understanding problems. Beyond movies, there\nare task-specific benchmarks like EgoSchema [34], which\npresents video reasoning tasks using first-person footage\nfrom Ego4D [15]. These specialized benchmarks, however,\nfocus on a single aspect of MLLMs rather than offering a\ncomprehensive analysis of long video understanding. There-\nfore, it is essential to develop a comprehensive benchmark\nwith carefully designed tasks to effectively evaluate MLLMs’\ncapabilities in understanding long videos.\n3. MLVU: Multi-task Long Video Understand-\ning Benchmark\nIn this section, we start with an overview of MLVU, which\nhighlights its constitution and explains its values over the\nprevious works. Then, we discuss how each evaluation task\nis constructed in MLVU.\n3.1. Overview\nMLVU is a multi-task benchmark consisting of 3,102 ques-\ntions across 9 categories, specifically designed for long video\nunderstanding. It is divided into a dev set and a test set,\ncontaining 2,593 and 509 questions, respectively. The bench-\nmark is distinguished by the following features.\nDiversified Video Categories. MLVU offers a compre-\nhensive collection of videos across various categories (Fig-\nure 1 Left). These include typical real-world videos such as\nmovies, documentaries, TV series, egocentric videos, life\nrecords, sports, tutorials, and surveillance footage. Addition-\nally, it features significant simulated videos from animated\nseries and game videos.\nSubstantial Extension of Video Length. MLVU is made\n3\nup of videos of diversified lengths, spanning from 3 min to\nmore than 2 hours (Figure 1 Top Right). Besides, each video\nis further partitioned as incremental segments, e.g., the first\n3 min, the first 6 min, and the entire video, where tasks are\ncreated for each individual segment. Thus, the MLLMs can\nbe flexibly evaluated across different video lengths.\nDiversified Evaluation Tasks. MLVU also provides a\ndiverse array of evaluation tasks, which are closely aligned\nwith the common visual capabilities of MLLMs, such as\nreasoning, captioning, recognition, perception, and summa-\nrization (Figure 1 Bottom Right). All the tasks are tailored\nfor LVU. That is to say, the tasks need to be solved based\non the in-depth understanding of video. Some of tasks are\nto examine whether the global information from the entire\nvideo can be effectively utilized (holistic LVU); while oth-\ners focus on whether the MLLMs can make precise usage\nof proper local information within the long video (detail\nLVU). Additionally, both multi-choice and free-form gener-\nation tasks are included in MLVU, which help to examine\nMLLMs’ capabilities in handling different task formats.\n3.2. Construction of MLVU\nThe evaluation tasks of MLVU can be categorized into three\ntypes: 1) holistic LVU, which needs to be solved by mak-\ning use of the global information from the entire video; 2)\nsingle-detail LVU, which relies on leveraging one critical\nplot within the long video; and 3) multi-detail LVU, which\nnecessitates the joint utilization of multiple plots within the\nlong video. The construction process of MLVU is discussed\nw.r.t the above three categories. To facilitate the discussion,\nwe define ULVC (Universal Long Video Collection) as the\nuniversal collection of long videos from various sources\n(more details about ULVC are presented in Appendix C).\n3.2.1. Holistic LVU\nTopic Reasoning (TR). The topic reasoning task requires\nMLLMs to respond to questions about the principal subject\nof a long video, as shown with Figure 2 (a). This includes\nelements such as the video’s genre, pivotal events, or pri-\nmary settings. All questions and answers undergo manual\nannotation2, resulting in a total of 355 questions. TR tasks\nare formatted as multiple-choice questions, with the model’s\nperformance assessed based on accuracy.\nAnomaly Recognition (AR). The anomaly recognition task\ninvolves identifying the anomalous behavior within a surveil-\nlance footage (Figure 2 b). We leverage the surveillance\nvideo clips from UCF Crime dataset [43] for this task. The\nselected video clips are longer than three minutes. We create\n239 questions based on the original annotations provided by\nthe dataset. The AR task is also conducted in the multiple-\nchoice format, whose performance is measured by accuracy.\n2Detailed information and annotation guidelines for annotators are pre-\nsented in Appendix F.\nVideo Summarization (VS). This task requires MLLMs\nto summarize the key events in a long video (Figure 2 c).\nWe select the narrative-rich videos from ULVC for this task,\nincluding movies, TV series, documentaries, life records,\nand animated series. There are 257 selected videos in total,\nwhose summaries are manually annotated. During evalua-\ntion, the MLLMs are prompted with \"Please summarize the\nmain content of this video\". We employ GPT-4 to assess the\ngenerated summaries by comparing with the annotation re-\nsults. Details about annotation and evaluation are presented\nin Appendix F.3 and G.3.\n3.2.2. Single-Detail LVU\nNeedle\nQuestion-Answering\n(NQA).\nNeedle-In-the-\nHaystack-Search (NIHS) is a popular evaluation task for\nlong-context LLM [31]. Taking the inspiration from NIHS,\nwe create Needle Question-Answering (NQA), shown as\nFigure 2 (d). In this task, the MLLM is required to answer a\nquestion related to a specific segment (referred as needle)\nwithin a long video (referred as background video). The\nneedles are short video clips sampled from WebVid [5]\nand Clevrer [55], while the background videos are sampled\nfrom our ULVC. The needle is randomly inserted into\nthe background video, where a question-answer pair is\nannotated. By incorporating necessary details, the question\ncan always correspond to the needle without ambiguity.\nDuring evaluation, the MLLM needs to infer the location\nof the needle based on the details provided in the question,\nand solve the problem on top of the needle’s information.\nThe NQA task is structured as multiple-choice, whose\nperformance is measured by accuracy.\nEgo Reasoning (ER). Ego-centric videos capture a series\nof consecutive actions from a first-person perspective. The\nMLLM needs to reason for a question about a specific be-\nhavior in the video, e.g., predicting for the event which is\ncorrelated or satisfies a certain causal relationship with the\nbehavior (Figure 2 e). Both videos and QA annotations are\ncollected from the NLQ task of Ego4D [15]. The ER task is\nstructured as multiple-choice, with a total of 405 questions\ncreated for this task.\nPlot Question-Answering (PQA). In this task, the MLLM\nneeds to reason for questions about a plot in a narrative\nvideo, shown as Figure 2 (f). The video is sampled from the\nmovies, TV series, and animated series in our ULVC. There\nare 589 question-answer pairs created by manual annotation.\nDuring annotation, the human annotators are asked to only\nprovide necessary details about the plot but not to suggest\nany objective hints, e.g., the two characters in the example\nvideo are referred as cat and mouse, rather than Tom and\nJerry. Therefore, it can prevent the question from being\nshort-cut by the MLLM’s common-sense knowledge (more\ndetails about PQA can be found in the Appendix F.6).\nSub-Scene Captioning (SSC). In this task, the MLLM needs\nto generate the caption for a sub-scene in a long video. The\n4\nFigure 2. Examples of MLVU. There are nine tasks designed to evaluate the holistic, single-detail, and multi-detail LVU capabilities of\nMLLMs. The MLLMs are asked to solve the problem (with the ground-truth answers marked in blue) based on the long video input and\ntextual prompt. For multiple-choice questions, we set 4 candidates in the dev set and 6 candidates in the test set.\n5\nlong videos in SSC are sampled from the Movie101 dataset\n[58], while the questions and answers are manually anno-\ntated. During annotation, the human annotator is asked\nto provide a detailed description for the sub-scene as the\nground-truth answer. Besides, they need to offer necessary\nclues in their questions such that the referred sub-scenes\ncan be identified without ambiguity. During evaluation, we\nemploy GPT-4 [1] to measure the quality of caption in com-\nparison with the ground-truth. Details about annotation and\nevaluation are presented in Appendix F.7 and G.3.\n3.2.3. Multi-Detail LVU\nAction Order (AO). In this task, the MLLM needs to predict\nthe right order for a sequence of actions (Figure 2 h). The\nactions are presented by short video clips, called probes.\nThe probes are formulated in two different ways. One is\nmade up of clips from the Kinetics dataset [18], where each\nclip represents a distinct action. The other one is from the\nconsecutive clips of an action in the ActivityNet-Caption\ndataset [20]. The probes are inserted into a long background\nvideo, which is sampled from ULVC. There are 329 AO\nquestions in total. The task is structured as a multiple-choice\nprblem, where the right order is selected from the misleading\noptions provided by the annotator.\nAction Count (AC). This task requires the MLLM to count\nthe occurrences of an action within a long video (Figure 2\ni). Each action corresponds to multiple short probe clips\nsampled from the Kinetics dataset [18]. The probes of an ac-\ntion are inserted into a long background video sampled from\nULVC. We also perform manual examination to ensure that\nthe inserted action does not exist in the original background\nvideo. A total of 266 evaluation instances have been created.\nThe AC task is structured as a multiple-choice problem, with\nperformance measured by accuracy.\n4. Experiments and Analysis\n4.1. Settings\nWe conduct a comprehensive investigation of 23 MLLMs us-\ning our MLVU benchmark, encompassing both open-source\nand proprietary models. The experimental MLLMs are di-\nvided into three categories: 1) Image MLLMs, primarily\nfine-tuned using image-related instructions; 2) Short Video\nMLLMs, fine-tuned with short-video related instructions;\nand 3) Long Video MLLMs, optimized for long-video under-\nstanding capability. For Image MLLMs, we leverage their\nmulti-image inference capabilities to process segmented\nframes from original videos. For Video MLLMs, we employ\neither a uniform sampling strategy or a frame rate sampling\nstrategy for video processing. All models are evaluated\nbased on their official implementations or available APIs,\nwith evaluations conducted in a zero-shot manner. More\ndetails about the evaluation are provided in Appendix G.\n4.2. Main Results\nThe overall evaluation results for all investigated MLLMs\nin the MLVU test set are shown in Table 2 (with dev set re-\nsults in Appendix B). Individual performances are reported\nfor each task, while average performances are provided\nfor multiple-choice (M-Avg) and generation tasks (G-Avg).\nFrom the results, we derive three primary conclusions:\n1) The proprietary model GPT-4o [37] achieves opti-\nmal performance in our benchmark. It leads in multiple-\nchoice tasks with an M-Avg of 54.5%(within 0-100%) and\nexcels in generation tasks with a G-Avg of 5.87 (within\n0.0-10.0), outperforming all other methods.\n2) Recent advances in LVU have achieved significant\nprogress, and the gap between open-source long video\nMLLMs and GPT-4o on close-ended tasks is narrowing.\nBefore June 2024, the best open-source long video MLLMs,\nMiniGPT4-Video [3], lagged significantly behind GPT-4o.\nHowever, recent models [11, 24, 40, 60] have made substan-\ntial progress. For instance, LLaVA-Onevision trails GPT-4o\nby only 2.8% in M-Avg. These models have improved their\nability to handle long visual sequences, achieving significant\nadvancements in single-detail (e.g., NQA) and multi-detail\n(e.g., AC) tasks compared to previous open-source models.\n3) Existing methods still struggle to handle most tasks\nin our benchmark. For instance, GPT-4o only achieves\n42.9% in the needle question-answering (NQA) task. In\ncontrast, analogous tasks in the text domain, such as NIHS\n(Needle-In-the-Haystack-Search) and Passkey Retrieval, are\neffectively handled by many existing long LLMs [14, 61].\nAdditionally, GPT-4o shows even less reliability in tasks like\nego-reasoning (ER), action ordering (AO), and action count\n(AC), with most baseline methods performing even worse.\nThese observations indicate that long-video understanding\nremains a significant challenge for today’s MLLMs.\nIn addition to the primary conclusions from the overall\nperformances, we can also make the following interesting\nobservations about the individual tasks.\n4) The close-ended holistic tasks present much higher\ndifferentiation than other tasks. These tasks, i.e., topic\nreasoning (TR) and anomaly recognition (AR), show sig-\nnificant variance in performance across different models.\nProprietary MLLMs, like GPT-4o, and superior open-source\nmodels, such as InternVL-2 [8], VideoLLaMA2 [9], and\nLLaVA-OneVision [24], can accurately solve these prob-\nlems. Meanwhile, many other popular MLLMs still fail to\ngenerate meaningful performances. Since these tasks only\nrequire an overall understanding of long videos, they can\nserve as a preliminary indicator of MLLMs’ long video un-\nderstanding (LVU) ability.\n5) It’s challenging to deal with tasks that require nu-\nanced understanding of multiple details. Although several\nMLLMs can handle single-detail LVU tasks to some ex-\ntent, their performances suffer from catastrophic degradation\n6\nMethods\nDate\nInput\nHolistic\nSingle Detail\nMulti Detail\nM-Avg\nG-Avg\nTR\nAR\nVS∗\nNQA\nER\nPQA\nSSC∗\nAO\nAC\nFull mark\n–\n–\n100\n100\n10\n100\n100\n100\n10\n100\n100\n100\n10\nRandom\n–\n–\n16.7\n16.7\n–\n16.7\n16.7\n16.7\n–\n16.7\n16.7\n16.7\n–\nImage MLLMs\nOtter-I [23]\n2023-05\n16 frm\n17.6\n17.9\n2.03\n16.7\n17.0\n18.0\n3.90\n15.7\n16.7\n17.1\n2.97\nLLaVA-1.6 [29]\n2024-01\n16 frm\n63.7\n17.9\n2.00\n13.3\n26.4\n30.0\n4.20\n21.4\n16.7\n27.1\n3.10\nInternVL-2 [8]\n2024-07\n16 frm\n85.7\n51.3\n2.55\n48.3\n47.2\n52.0\n5.25\n32.9\n15.0\n47.5\n3.90\nClaude-3-Opus† [2]\n2024-03\n16 frm\n53.8\n30.8\n2.83\n14.0\n17.0\n20.0\n3.67\n10.0\n6.7\n21.8\n3.25\nQwen-VL-Max† [4]\n2024-01\n16 frm\n75.8\n53.8\n3.00\n15.0\n26.4\n4.84\n20.0\n20.7\n11.7\n32.2\n3.92\nShort Video MLLMs\nOtter-V [23]\n2023-05\n16 frm\n16.5\n12.8\n2.18\n16.7\n22.6\n22.0\n4.20\n12.9\n13.3\n16.7\n3.19\nmPLUG-Owl-V [54]\n2023-04\n16 frm\n25.3\n15.4\n2.20\n6.7\n13.2\n22.0\n5.01\n14.3\n20.0\n16.7\n3.61\nVideoChat [25]\n2023-05\n16 frm\n26.4\n12.8\n2.15\n18.3\n17.0\n22.0\n4.90\n15.7\n11.7\n17.7\n3.53\nVideo-LLaMA-2 [59]\n2024-08\n16 frm\n52.7\n12.8\n2.23\n13.3\n17.0\n12.0\n4.87\n15.7\n8.3\n18.8\n3.55\nVideoChat2-HD [26]\n2024-06\n16 frm\n74.7\n43.6\n2.83\n35.0\n34.0\n30.0\n5.14\n21.4\n23.3\n37.4\n3.99\nVideo-LLaVA [28]\n2023-11\n8 frm\n70.3\n38.5\n20.9\n2.30\n26.4\n26.0\n5.06\n20.0\n21.7\n29.3\n3.68\nShareGPT4Video [7]\n2024-05\n16 frm\n73.6\n25.6\n2.53\n31.7\n45.3\n38.0\n4.72\n17.1\n8.3\n34.2\n3.63\nVideoLLaMA2 [9]\n2024-06\n16 frm\n80.2\n53.8\n2.80\n36.7\n54.7\n54.0\n5.09\n42.9\n16.7\n48.4\n3.95\nLong Video MLLMs\nMovieChat [41]\n2023-07\n2048 frm\n18.7\n10.3\n2.30\n23.3\n15.1\n16.0\n3.24\n17.1\n15.0\n16.5\n2.77\nMovie-LLM [42]\n2024-03\n1 fps\n27.5\n25.6\n2.10\n10.0\n11.3\n16.0\n4.93\n20.0\n21.7\n18.9\n3.52\nLLaMA-VID [27]\n2023-11\n1 fps\n20.9\n23.1\n2.70\n21.7\n11.3\n16.0\n4.15\n18.6\n15.0\n18.1\n3.43\nMA-LMM [16]\n2024-04\n1000 frm\n44.0\n23.1\n3.04\n13.3\n30.2\n14.0\n4.61\n18.6\n13.3\n22.4\n3.83\nMiniGPT4-Video [3]\n2024-04\n90 frm\n64.9\n46.2\n2.50\n20.0\n30.2\n30.0\n4.27\n15.7\n15.0\n31.7\n3.39\nLongVA [60]\n2024-06\n256 frm\n81.3\n41.0\n2.90\n46.7\n39.6\n46.0\n4.92\n17.1\n23.3\n42.1\n3.91\nVideo-CCAM [11]\n2024-08\n96 frm\n79.1\n38.5\n2.65\n45.0\n52.8\n56.0\n4.49\n24.3\n26.7\n46.1\n3.57\nVideo-XL [40]\n2024-09\n256 frm\n78.0\n28.2\n3.40\n50.0\n41.5\n46.0\n5.02\n48.6\n31.7\n46.3\n4.21\nLLaVA-Onevision [24]\n2024-08\n32 frm\n83.5\n56.4\n3.75\n46.7\n58.4\n58.0\n5.09\n35.7\n23.3\n51.7\n4.42\nGPT-4o† [37]\n2024-05\n0.5 fps\n83.7\n68.8\n4.94\n42.9\n47.8\n57.1\n6.80\n46.2\n35.0\n54.5\n5.87\nTable 2. The overall performances on MLVU test set, including the holistic LVU tasks, the single-detail LVU tasks, and multi-detail LVU\ntasks. Date: the release date of the MLLM. M-Avg: the average performance of multiple-choice tasks; G-Avg: the average performance of\ngeneration tasks (marked by ∗). Two input strategies are used by the MLLMs in evaluation: Uniform Sampling (N frm), which evenly\nsamples N frames from the video; Frame Rate Sampling (N fps), which samples N frames per second. † denotes proprietary models.\nwhen addressing multi-detail LVU tasks. Most methods,\nexcept for GPT-4o and Video-XL [40], fail entirely in ac-\ntion order (AO) and action count (AC) tasks. Additionally,\nmost approaches struggle with summarization tasks, which\nrequire recalling multiple nuanced details from long videos.\nAs a brief conclusion, although today’s MLLMs can deal\nwith some preliminary LVU tasks, it remains a tough chal-\nlenge to achieve an in-depth understanding of nuanced infor-\nmation within long videos.\n4.3. Further Analysis\n6) Longer videos are more challenging for MLLMs.\nWe evaluate MLLMs’ performances across various video\nlengths. For this purpose, we introduce a derivative dataset\nalongside MLVU, called MLVU Time-ladder. In this dataset,\nthe same kinds of evaluation tasks are created for videos of\nvariant lengths, including 180s, 360s, and 600s (more details\npresented in Appendix D). As shown in Figure 3, the per-\nformances of all models tend to decline as the video length\ngrows, which indicates that the existing MLLMs’ LVU abili-\nties are severely constrained by the video length. Moreover,\nthe short video model Video-LLaMA-2 [59] maintains a cer-\ntain level of LVU ability at 3 minutes, but its performance\napproaches random results at 10 minutes.\n7) The performance of recent advanced long video\nMLLMs remains robust regardless of the position of the\nreferring clip within the long video. In single-detail tasks,\nthe referring clip denotes the specific segment of the long\nvideo that is referenced or inferred to answer a question.\nAs shown in Figure 4, we categorize clip positions into\nfour intervals and assess model performance on two single-\ndetail tasks: ego reasoning (ER) and plot question-answering\n(PQA). Recent long video MLLMs, such as LongVA [60]\nand Video-XL [40], maintain consistent performance re-\n7\nImpact of Context Length\nImpact of IU\nImpact of LLM\nModel\nContext Len. M-Avg\nModel\nMMMU (Val) M-Avg\nModel\nLLM\nM-Avg\nMGV\n16\n24.2\nOtter-I\n32.2\n17.1\nVLM2 Vicuna-7B 13.3\n90\n31.7↑7.5 LLaVA-1.6\n35.8\n27.1↑10.0\nVicuna-13B 18.8↑5.5\nGPT-4o\n16\n45.8\nGPT-4V\n58.1\n43.3\nMGV\nLLaMA-7B 20.6\n256\n54.5↑8.7 GPT-4o\n63.8\n45.8↑2.5\nMistral-7B 31.7 ↑11.1\nTable 3. Detailed discussions about the impact from context length, image understanding (IU) ability, and LLM Backbone. For the IU\nimpact experiment, we used 16-frame uniform sampling for both GPT-4V and GPT-4o. MGV: MiniGPT4-Video, VLM2: Video-LLaMA-2.\n180s\n360s\n600s\n20\n30\n40\n50\nAccuracy (%)\nVideo-XL\nVideoChat2\nMiniGPT4-Video\nVideo-LLAMA-2\nFigure 3. Experimental performance on varying video lengths. The\nevaluated metric is the average accuracy across five multiple-choice\ntasks involving local information: NQA, ER, PQA, AC, and AO.\n20\n28\n36\n44\nLongVA\nVideoXL\nVideoChat2 VideoLLaVA\n20\n30\n40\n50\nLongVA\nVideoXL\nVideoChat2 VideoLLaVA\nEgo Reasoning\nPlot QA\n(0, 25] \n(25, 50] \n(50, 75] \n(75, 100] \nVideoXL\nLongVA\nVideoChat2 VideoLLaVA\nVideoXL\nLongVA\nVideoChat2 VideoLLaVA\n44\n36\n28\n20\n50\n40\n30\n20\nFigure 4. Model performance across different referring clip posi-\ntions, spanning from the beginning to the end of the entire video.\ngardless of the referring clip’s position within the video.\nConversely, short video MLLMs are more sensitive to clip\nlocation. This indicates that recent advancements in long\nvideo MLLMs enhance both reliable clue retrieval and effec-\ntive reasoning from extended visual sequences.\n8) The challenge of multi-detail tasks increases with\nthe number of details. We analyzed model performance\non the action count (AC) task by grouping questions based\non the number of probes (which correspond to details) and\nevaluating the average performance within these groups. As\nshown in Figure 5, performance significantly declines across\nall models as the number of probes increases. This indi-\ncates that current MLLMs face substantial difficulties com-\nprehending and processing multiple details simultaneously,\nhighlighting a critical area for future improvement in long\nvideo understanding capabilities.\n9) Context Length, Image-Understanding ability, and\n[1,2]\n[3,4]\n[5,6]\nNumber of Probes\n20\n25\n30\n35\n40\nAccuracy (%)\nGPT-4o\nVideo-XL\nLLaVA-Onevision\nVideoChat2\nLongVA\nFigure 5. Model performance on the action count (AC) task in\nrelation to the number of probes.\nthe choice of LLM Backbones are key factors in LVU\nperformance. As shown in Table 3, we conducted abla-\ntion experiments on several factors affecting MLLMs, using\nM-Avg as the evaluation metric. First, we examined the\nmodels’ handling of different context lengths. Specifically,\nwe increased MiniGPT4-Video’s input from 16 to 90 frames\nand GPT-4o’s input from 16 to 256 frames (as shown on the\nleft side of Table 3). Both models showed consistent perfor-\nmance improvements with longer input lengths. To assess the\nimpact of image understanding (IU) capabilities, we referred\nto the results from MMMU [57] (presented in the middle of\nTable 3). It is evident that MLLMs’ LVU performance gener-\nally aligns with their IU performance in MMMU. Finally, we\ncompared MLLMs using different backbones (depicted on\nthe right side of Table 3). The findings indicate that LVU per-\nformance improves with larger (Vicuna-13B vs. Vicuna-7B)\nand more advanced backbones (Mistral-7B vs. Llama-2-\n7B). These observations indicate that LVU is the result of\nmultiple complex factors, with the ability to perceive longer\nvideos and effectively utilize the perceived information being\ncrucial for the improvement of LVU.\n5. Conclusion\nThis paper presents MLVU, a novel benchmark for the\nassessment of long video understanding.\nWith several\ncritical innovations: the substantial extension of video\nlengths, the inclusion of various video genres, and the\ndevelopment of diversified LVU-oriented evaluation tasks,\nthe new benchmark is able provide a comprehensive and\nin-depth analysis for MLLMs’ long-video understanding\nperformance. The empirical study on MLVU reveals LVU\n8\nremains a technically challenging problem for today’s\nstate-of-the-art MLLMs. Future advancements may call\nfor the joint optimization of complex factors, such as\ncontext length, image understanding ability, and even LLM\nbackbones. We anticipate this benchmark will facilitate\nfuture research in long-video understanding of MLLMs.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023. 1,\n6, 3\n[2] Anthropic. Claude 3. https:\/\/www.anthropic.com\/\nnews\/claude-3-family, 2024. 7, 2\n[3] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Es-\nsam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elho-\nseiny. Minigpt4-video: Advancing multimodal llms for video\nunderstanding with interleaved visual-textual tokens. arXiv\npreprint arXiv:2404.03413, 2024. 6, 7, 2\n[4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-\naodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.\nQwen technical report.\narXiv preprint arXiv:2309.16609,\n2023. 7, 2\n[5] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.\nFrozen in time: A joint video and image encoder for end-to-\nend retrieval. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision, pages 1728–1738, 2021. 4,\n3\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877–1901, 2020. 1\n[7] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang,\nYuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu\nTang, et al. Sharegpt4video: Improving video understand-\ning and generation with better captions.\narXiv preprint\narXiv:2406.04325, 2024. 7, 2\n[8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei\nGao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo,\nZheng Ma, et al. How far are we to gpt-4v? closing the gap\nto commercial multimodal models with open-source suites.\narXiv preprint arXiv:2404.16821, 2024. 2, 6, 7\n[9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin\nLi, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang\nLuo, Deli Zhao, et al. Videollama 2: Advancing spatial-\ntemporal modeling and audio understanding in video-llms.\narXiv preprint arXiv:2406.07476, 2024. 6, 7, 2\n[10] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. Minedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, 35:\n18343–18362, 2022. 1\n[11] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu,\nand Hui Wang. Video-ccam: Enhancing video-language un-\nderstanding with causal cross-attention masks for short and\nlong videos. arXiv preprint arXiv:2408.14023, 2024. 2, 6, 7\n[12] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Meng-\ndan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing\nSun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehen-\nsive evaluation benchmark for multimodal large language\nmodels. arXiv preprint arXiv:2306.13394, 2023. 1, 3\n[13] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai\nRen, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\nShen, Mengdan Zhang, et al. Video-mme: The first-ever\ncomprehensive evaluation benchmark of multi-modal llms in\nvideo analysis. arXiv preprint arXiv:2405.21075, 2024. 1, 2\n[14] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh\nHajishirzi, Yoon Kim, and Hao Peng. Data engineering for\nscaling language models to 128k context. arXiv preprint\narXiv:2402.10171, 2024. 6\n[15] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary\nChavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,\nHao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the\nworld in 3,000 hours of egocentric video. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18995–19012, 2022. 1, 3, 4\n[16] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xue-\nfei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam\nLim.\nMa-lmm:\nMemory-augmented large multimodal\nmodel for long-term video understanding. arXiv preprint\narXiv:2404.05726, 2024. 3, 7, 2\n[17] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua\nLin. Movienet: A holistic dataset for movie understanding. In\nComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part IV 16,\npages 709–727. Springer, 2020. 2, 3\n[18] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 3, 6\n[19] Muhammad Uzair Khattak, Muhammad Ferjad Naeem,\nJameel Hassan, Muzammal Naseer, Federico Tombari, Fa-\nhad Shahbaz Khan, and Salman Khan. Complex video rea-\nsoning and robustness evaluation suite for video-lmms. arXiv\npreprint arXiv:2405.03690, 2024. 1\n[20] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles.\nDense-captioning events in videos.\nIn Proceedings of the IEEE international conference on\ncomputer vision, pages 706–715, 2017. 6\n[21] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa:\nLocalized, compositional video question answering. arXiv\npreprint arXiv:1809.01696, 2018. 2, 4\n[22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 1\n[23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\n9\nwith in-context instruction tuning. CoRR, abs\/2305.03726,\n2023. 7, 2\n[24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\nHao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun-\nyuan Li. Llava-onevision: Easy visual task transfer. arXiv\npreprint arXiv:2408.03326, 2024. 6, 7\n[25] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355, 2023. 2, 7\n[26] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\nMvbench: A comprehensive multi-modal video understand-\ning benchmark. arXiv preprint arXiv:2311.17005, 2023. 1,\n2, 3, 7\n[27] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An\nimage is worth 2 tokens in large language models. arXiv\npreprint arXiv:2311.17043, 2023. 1, 2, 3, 7\n[28] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\nLi Yuan.\nVideo-llava: Learning united visual represen-\ntation by alignment before projection.\narXiv preprint\narXiv:2311.10122, 2023. 2, 7\n[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2023. 2, 7\n[30] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 3\n[31] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\nLost in the middle: How language models use long con-\ntexts.\nTransactions of the Association for Computational\nLinguistics, 12:157–173, 2024. 4\n[32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n1\n[33] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\nFahad Shahbaz Khan. Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models. arXiv\npreprint arXiv:2306.05424, 2023. 2\n[34] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra\nMalik. Egoschema: A diagnostic benchmark for very long-\nform video language understanding.\nAdvances in Neural\nInformation Processing Systems, 36, 2023. 1, 2, 3\n[35] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m:\nLearning a text-video embedding by watching hundred mil-\nlion narrated video clips. In Proceedings of the IEEE\/CVF\ninternational conference on computer vision, pages 2630–\n2640, 2019. 3\n[36] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan,\nDongdong Chen, and Li Yuan. Video-bench: A comprehen-\nsive benchmark and toolkit for evaluating video-based large\nlanguage models. arXiv preprint arXiv:2311.16103, 2023. 1\n[37] OpenAI.\nGpt-4o.\nhttps:\/\/openai.com\/index\/\nhello-gpt-4o\/, 2024. 6, 7, 2\n[38] Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang,\nYi Wang, Yu Qiao, and Hongsheng Li. Retrieving-to-answer:\nZero-shot video question answering with frozen large lan-\nguage models. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision, pages 272–283, 2023. 3\n[39] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu\nHou.\nTimechat: A time-sensitive multimodal large lan-\nguage model for long video understanding. arXiv preprint\narXiv:2312.02051, 2023. 2\n[40] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie\nZhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long\nvision language model for hour-scale video understanding.\narXiv preprint arXiv:2409.14485, 2024. 2, 3, 6, 7\n[41] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang,\nHaoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu,\nJenq-Neng Hwang, et al. Moviechat: From dense token to\nsparse memory for long video understanding. arXiv preprint\narXiv:2307.16449, 2023. 1, 2, 3, 7, 4\n[42] Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang,\nGang Yu, Jiayuan Fan, and Tao Chen. Moviellm: Enhancing\nlong video understanding with ai-generated movies. arXiv\npreprint arXiv:2403.01422, 2024. 7, 2\n[43] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world\nanomaly detection in surveillance videos. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pages 6479–6488, 2018. 4, 1, 2\n[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 1\n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 1\n[46] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-\nLevy. Videoagent: Long-form video understanding with large\nlanguage model as agent. arXiv preprint arXiv:2403.10517,\n2024. 3\n[47] Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu,\nJaemin Cho, Zineng Tang, Mohit Bansal, and Heng Ji. Pax-\nion: Patching action knowledge in video-language founda-\ntion models.\nAdvances in Neural Information Processing\nSystems, 36, 2023. 3\n[48] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,\nand Chuang Gan. Star: A benchmark for situated reasoning\nin real-world videos. In Thirty-fifth conference on neural\ninformation processing systems datasets and benchmarks\ntrack (Round 2), 2021. 3\n[49] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li.\nLongvideobench:\nA benchmark for long-context inter-\nleaved video-language understanding.\narXiv preprint\narXiv:2407.15754, 2024. 2\n[50] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.\nNext-qa: Next phase of question-answering to explaining tem-\nporal actions. In Proceedings of the IEEE\/CVF conference\n10\non computer vision and pattern recognition, pages 9777–\n9786, 2021. 2, 3\n[51] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan\nZhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa:\nTowards surprising video comprehension.\narXiv preprint\narXiv:2306.14899, 2023. 3\n[52] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288–5296, 2016. 1, 2, 3\n[53] Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, and Yan\nLu. Retrieval-based video language model for efficient long\nvideo question answering. arXiv preprint arXiv:2312.04931,\n2023. 3\n[54] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 2, 7\n[55] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun\nWu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer:\nCollision events for video representation and reasoning. In\nInternational Conference on Learning Representations, 2019.\n4\n[56] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yuet-\ning Zhuang, and Dacheng Tao. Activitynet-qa: A dataset\nfor understanding complex web videos via question answer-\ning. In Proceedings of the AAAI Conference on Artificial\nIntelligence, pages 9127–9134, 2019. 3\n[57] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren,\nYuxuan Sun, et al. Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert agi.\narXiv preprint arXiv:2311.16502, 2023. 1, 3, 8\n[58] Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang,\nand Qin Jin. Movie101: A new movie understanding bench-\nmark.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), pages 4669–4684, 2023. 2, 6, 1\n[59] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858, 2023. 2, 7\n[60] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng,\nJingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan,\nChunyuan Li, and Ziwei Liu. Long context transfer from\nlanguage to vision. arXiv preprint arXiv:2406.16852, 2024.\n1, 2, 3, 6, 7\n[61] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu,\nJunhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai,\nShuo Wang, Zhiyuan Liu, et al.\n∞bench: Extending\nlong context evaluation beyond 100k tokens. arXiv preprint\narXiv:2402.13718, 2024. 6\n[62] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa\nWang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li,\net al. Languagebind: Extending video-language pretraining\nto n-modality by language-based semantic alignment. arXiv\npreprint arXiv:2310.01852, 2023. 6\n[63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMinigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language mod-\nels. In The Twelfth International Conference on Learning\nRepresentations, 2023. 2\n11\nMLVU: Benchmarking Multi-task Long Video Understanding\nSupplementary Material\nA. Overview of Appendix\n• B: Evaluation Results on MLVU Dev Set\n• C: Collecting Details of our Universal Long Video Col-\nlection (ULVC)\n• D: Details of the MLVU Time-Ladder\n• E Detailed Division of Dev and Test Sets in MLVU\n• F: Annotation Details of MLVU\n• G: Details of Baselines and the Evaluation Process.\n• H: Explorations of Video Retrieval Augmented Gener-\nation\n• I: More Visualized Examples of MLVU\nB. Evaluation Results on MLVU Dev Set\nThe evaluation results of the baselines on the MLVU dev set\nare detailed in Table 2. Notably, the multiple-choice ques-\ntions in the MLVU dev set present four options, whereas the\nMLVU test set offers six, making the latter more challenging\nand discriminative.\nC. Collecting Details of our Universal Long\nVideo Collection (ULVC)\nIn the initial stage of our Multi-task Long Video Under-\nstanding (MLVU) benchmark creation, we first collected\nlong-form videos from a variety of sources to form our Uni-\nversal Long Video Collection (ULVC). The entirety of the\nlong videos incorporated into our MLVU benchmark were\nselected, edited, or synthesized from ULVC.\nSpecifically, our ULVC includes a diverse set of 986\nlong videos. This collection features 168 movies from the\nMovie101 [58] and MovieChat [41] datasets, along with 60\ndocumentaries from MovieChat [41]. It also contains 65\ngame videos from MineDojo [10], 239 surveillance videos\nfrom UCF-Crime [43], and 100 ego-centric videos from\nEgo4D [15]. Additionally, we independently collected 72\ncartoons, 92 TV series, 60 tutorial videos, 60 sports videos,\nand 70 life records.\nIt’s important to clarify that the quantity of videos in the\nULVC does not directly correspond to the number of videos\nand questions in our MLVU benchmark, which are 1,730\nand 3,102 respectively. For example, a two-hour movie\nfrom the ULVC might be utilized in its entirety for the Sub-\nScene Captioning task, or it could be segmented into several\napproximately 10-minute clips for the Video Summarization\ntask, or even used as a background video for synthetic video\ngeneration. Moreover, a single video could be annotated\nwith multiple questions simultaneously.\nD. Details of the MLVU Time-Ladder\nAs discussed in Section 3.1, most tasks in our MLVU are\nsubject to segment-level annotation. This approach provides\nus with the flexibility to adjust the length of the video with-\nout requiring additional human annotators. Building on this\nstrategy, as mentioned in Section 4.3, we have generated\na derivative dataset, MLVU Time-Ladder, which includes\nvideos of varying durations - specifically 3, 6, and 10 min-\nutes. This dataset allows us to investigate how video duration\nimpacts LVU task difficulty.\nSpecifically, during the annotation process of the VS\ntask, we guided annotators to delineate the summarization\nin accordance with the initial 3 and 6-minute segments. For\nthe PQA and SSC tasks, we requested annotators to identify\nthe segments within the extended video where the pertinent\nanswers are located. In the case of the ego reasoning task, the\nEgo4D dataset [15] already comprises the intervals where\nthe answers reside. Lastly, for the synthetic tasks of NQA,\nAO, and AC, we possess the capability to directly generate\nthe necessary video lengths.\nE. Detailed Division of Dev and Test Sets in\nMLVU\nOur MLVU comprises a total of 3,102 questions, divided\ninto a dev set with 2,593 questions and a test set with 509\nquestions. We present the detailed distribution of questions\nfor each task in Table 1.\nTask\nDev\nTest\nTotal\nTopic Reasoning\n264\n91\n355\nAnomaly Recognition\n200\n39\n239\nVideo Summarization\n217\n40\n257\nNeedle QA\n355\n60\n415\nEgo Reasoning\n352\n53\n405\nPlot QA\n539\n50\n589\nSub-Scene Captioning\n201\n46\n247\nAction Order\n259\n70\n329\nAction Count\n206\n60\n266\nTable 1. Detailed Distribution of Questions in the MLVU Dataset\nAcross Dev and Test Sets for Each Task.\n1\nMethods\nDate\nInput\nHolistic\nSingle Detail\nMulti Detail\nM-Avg\nG-Avg\nTR\nAR\nVS∗\nNQA\nER\nPQA\nSSC∗\nAO\nAC\nFull mark\n–\n–\n100\n100\n10\n100\n100\n100\n10\n100\n100\n100\n10\nRandom\n–\n–\n25\n25\n–\n25\n25\n25\n–\n25\n25\n25\n–\nImage MLLMs\nOtter-I [23]\n2023-05\n16 frm\n25.0\n25.0\n2.18\n25.1\n25.0\n24.9\n4.12\n13.1\n25.2\n23.3\n3.15\nLLaVA-1.6 [29]\n2024-01\n16 frm\n60.6\n41.0\n2.11\n43.1\n38.4\n41.0\n4.35\n25.5\n25.7\n39.3\n3.23\nClaude-3-Opus† [2]\n2024-03\n16 frm\n67.2\n43.5\n3.11\n21.6\n40.2\n47.8\n3.66\n18.2\n16.7\n36.5\n3.39\nQwen-VL-Max† [4]\n2024-01\n16 frm\n67.4\n63.5\n2.71\n40.3\n40.9\n43.3\n5.21\n25.0\n14.8\n42.2\n3.96\nShort Video MLLMs\nOtter-V [23]\n2023-05\n16 frm\n24.6\n26.0\n2.38\n28.2\n27.6\n22.3\n4.23\n15.1\n26.7\n24.4\n3.31\nmPLUG-Owl-V [54]\n2023-04\n16 frm\n28.0\n25.0\n2.36\n24.5\n31.8\n27.3\n5.31\n21.2\n23.3\n25.9\n3.84\nVideoChat [25]\n2023-05\n16 frm\n33.0\n32.0\n2.31\n27.0\n32.1\n27.6\n5.01\n24.3\n28.6\n29.2\n3.66\nVideo-LLaMA-2 [59]\n2024-08\n16 frm\n54.5\n41.5\n2.34\n39.4\n33.5\n35.4\n5.22\n18.5\n25.7\n35.5\n3.78\nVideoChat2-HD [26]\n2024-06\n16 frm\n74.6\n51.5\n2.57\n42.0\n47.4\n43.8\n5.04\n22.8\n29.6\n44.5\n3.81\nVideo-LLaVA [28]\n2023-11\n8 frm\n71.6\n57.0\n2.43\n53.2\n45.2\n48.4\n5.25\n20.1\n35.9\n47.3\n3.84\nShareGPT4Video [7]\n2024-05\n16 frm\n75.8\n51.5\n2.52\n47.6\n43.2\n48.4\n5.02\n34.0\n23.3\n46.4\n3.77\nVideoLLaMA2 [9]\n2024-06\n16 frm\n74.6\n64.5\n2.79\n49.9\n43.8\n45.1\n5.18\n34.0\n27.4\n48.5\n3.99\nLong Video MLLMs\nMovieChat [41]\n2023-07\n2048 frm\n29.5\n25.0\n2.33\n24.2\n24.7\n25.8\n3.23\n28.6\n22.8\n25.8\n2.78\nMovie-LLM [42]\n2024-03\n1 fps\n30.0\n29.0\n2.88\n29.6\n24.7\n24.1\n5.00\n20.5\n24.8\n26.1\n3.94\nTimeChat [39]\n2023-12\n96 frm\n23.1\n27.0\n2.54\n24.5\n28.4\n25.8\n4.29\n24.7\n32.0\n30.9\n3.42\nLLaMA-VID [27]\n2023-11\n1 fps\n50.8\n34.5\n3.22\n30.1\n32.7\n32.5\n5.22\n23.9\n27.8\n33.2\n4.22\nMA-LMM [16]\n2024-04\n1000 frm\n51.9\n35.5\n2.12\n43.1\n38.9\n35.8\n4.80\n25.1\n24.3\n36.4\n3.46\nMiniGPT4-Video [3]\n2024-04\n90 frm\n70.9\n52.5\n2.64\n49.0\n48.6\n44.5\n4.07\n23.2\n23.0\n44.5\n3.36\nLongVA [60]\n2024-06\n256 frm\n83.3\n58.5\n3.39\n69.3\n50.0\n67.2\n5.26\n38.6\n27.2\n56.3\n4.33\nVideo-CCAM [11]\n2024-08\n96 frm\n84.9\n66.0\n2.84\n73.2\n60.5\n66.1\n5.19\n42.1\n38.4\n63.1\n4.01\nVideo-XL [40]\n2024-09\n256 frm\n80.3\n54.5\n3.25\n73.8\n57.4\n67.9\n5.02\n68.3\n40.3\n64.9\n4.14\nGPT-4o† [37]\n2024-05\n0.5 fps\n87.4\n74.5\n4.90\n64.8\n57.1\n65.1\n6.69\n56.7\n46.3\n64.6\n5.80\nTable 2. The overall performances on MLVU dev set, including the holistic LVU tasks (TR: Topic Reasoning, AR: Anomaly Recognition,\nVS: Video Summary), the single-detail LVU tasks (NQA: Needle QA, ER: Ego Reasoning, PQA: Plot QA, SSC: Sub-Scene Captioning),\nand multi-detail LVU tasks (AO: Action Order, AC: Action Count). M-Avg: the average performance of multiple-choice tasks; G-Avg: the\naverage performance of generation tasks (marked by ∗). Two input strategies are used by the MLLMs in evaluation: Uniform Sampling\n(N frm), which evenly samples N frames from the video; Frame Rate Sampling (N fps), which samples N frames per second. † denotes\nproprietary models.\nF. Annotation Details of MLVU\nF.1. Topic Reasoning (TR).\nThe questions and corresponding answers for the TR task\nwere meticulously annotated by human annotators, following\nthe specific guidelines illustrated in Figure 1. We required\nthe annotators to design questions related to the reasoning\nof the video topic, rather than focusing on the creation of\nquestions about minor details. More visualized examples of\nTR task can be found in Figure 10.\nF.2. Anomaly Recognition (AR).\nThe anomaly recognition task did not involve manual annota-\ntion. We utilized videos exceeding three minutes in duration,\nextracted from the UCF-Crime dataset [43]. We also modi-\nfied the original labels to fit a multiple-choice format.\nF.3. Video Summarization (VS).\nThe ground truth data for the VS task were derived from\nmanual annotations. We instructed the annotators to use\npronouns instead of specific character names in all annota-\ntions. This guideline stemmed from the inherent constraints\nof most existing MLLMs, which generally lacked the capac-\nity to process audio or subtitles. This made it difficult for\n2\nAnnotation Guidelines for Topic Reasoning\n1. Task Description: Your task is to formulate a question that pertains to the genre and key content\nof a given long video, and then provide the corresponding answer.\n2. Question Requirements:\n-\nYour questions should be centered around the core content of the video, rather than focusing on\nminor details.\n-\nSuitable topics for questions include the genre of the video, the main events or themes, the\nprimary environmental setting, the depicted weather conditions, and the time period or timeline.\n3. Question Format:\n-\nQuestions should be structured in a multiple-choice format. Each question should have one\ncorrect answer and three plausible, yet incorrect, distractor options.\n4. Question Examples (for reference only, not limited):\n- What genre does this movie\/video fall into?\n- Where does the main scene in the video take place?\n- What is the main event being narrated in the video?\n- What is the protagonist in the video accomplishing?\nFigure 1. Annotation Guidelines for the Topic Reasoning Task.\nthese models to identify specific characters. The annotation\ninstructions and examples provided to the annotators are\nelaborated in Figure 2. More visualized examples of VS task\ncan be found in Figure 10.\nF.4. Needle Question-Answering (NQA).\nWe leveraged the GPT-4 [1] and the detailed video cap-\ntion data from the WebVid dataset [5] to facilitate a semi-\nautomated generation of annotated questions and answers\nfor the NQA task. Initially, we selected video clips from\nWebVid, which we refered to as needle clips. The corre-\nsponding captions of these needle clips were then fed into\nGPT-4, which generated question-answer pairs based on\nthe information encapsulated in the captions. The specific\nprompt provided to GPT-4 is depicted in Figure 3. The\ngenerated questions were carefully crafted to focus on a par-\nticular detail within the needle clip. These questions were\nstructured to incorporate the maximum number of hints to\neffectively guide MLLMs in grounding the content of the\nneedle within the context of the longer video. Following\nthis, we randomly selected longer background videos from\nour ULVC and manually ensured that the scene indicated\nby the needle’s question did not feature in these background\nvideos. The final step involves integrating the needle into\nthe longer video, thereby producing the final needle question\nvideo. More visualized examples of NQA task can be found\nin Figure 11.\nF.5. Ego Reasoning (ER).\nThe video resources, questions, and correct responses used in\nthe ER task were derived from the Natural Language Queries\n(NLQ) task within the Ego4D dataset [15]. This data was\nrestructured to fit a multiple-choice question format.\nF.6. Plot Question-Answering (PQA).\nThe PQA task’s questions and answers were annotated by\nhuman annotators, following specific guidelines illustrated in\nFigure 4. We instructed the annotators to craft questions that\nprobe into the intricate plot details encapsulated within the\nvideos. These questions were designed to encompass both\nperception and reasoning aspects. We stipulated that both\nquestions and their corresponding answers should avoid the\nuse of specific character names or any objective hints, and\nshould instead utilize pronouns. This approach was strate-\ngized to prevent potential information leakage, given that\nMLLMs often demonstrate a familiarity with the storylines\nof well-known movies and TV series. Such common-sense\nknowledge could potentially allow the MLLMs to answer\nquestions correctly without the essential requirement of ana-\nlyzing the input video.\nNonetheless, the complexity of character interactions and\nactions in longer videos poses a challenge to conveying\nplot details using only pronouns and feature descriptions.\nPrevious datasets for plot question answering that avoided\nthe use of character names often resulted in compromised\n3\nAnnotation Guidelines for Video Summarization\n1. Task Description: Your task is to provide a comprehensive summary of the key events occurring within a\nvideo clip that ranges from 3 to 15 minutes in length.\n2. Annotation Requirements:\n-\nThe annotation should encapsulate the principal events portrayed in the video, structured in chronological\norder.\n-\nRefrain from using specific character names in the annotation. Instead, all characters should be referred to\nusing pronouns and identified by their unique attributes or roles, such as attire, age, profession, etc. For\ninstance, characters could be described as an \"elderly individual\" (age), a \"medical professional\"\n(profession), among others.\n-\nDisregard audio-related information, such as dialogues between characters. The summaries should be\nderived exclusively from the visual content presented in the video.\n3. Annotation Template:\n-\nInitiate your summary by outlining the overall content of the video: the event being narrated or the video's\nmain theme.\n-\nSubsequently, chronologically depict the key events that unfold in the video. The aim is to provide a clear\nand concise description of the main content, events, and scenes exhibited in the video.\n4. Annotation Examples:\n-\nCartoon: This is a video about a cartoon sponge‘s whimsical adventures. The video begins with a cartoon\nsponge rushing into a house to converse with a cartoon starfish on a rocking chair. The sponge then heads\nto a concert hall where he watches a performance, during which a cartoon animal on a throne reprimands a\ncartoon octopus who continues his act. Later, the cartoon sponge and a cartoon squirrel are seen flying and\nconversing in the air. The sponge also encounters a cartoon shark preparing to drink coffee and a cartoon\nlobster sailing on a sponge, after which the lobster chases the sponge away.\n-\nMovie \/ TV Series: This is a video depicting a dramatic narrative. The video starts with a man singing into\na microphone, with a few other men playing instruments behind him. The scene changes to someone\npushing open a door and walking into a room where others are resting. She then opens another door, enters\na room and starts arguing with the singing man, which results in a fight. Next, the woman drives the man\naway, which results in a car crash. The car then falls off a bridge and gets hit by another car. The screen\ngoes black and then lights up again, revealing a bookshelf filled with books at the end.\n-\nDocumentary: This is a documentary about forest animals and ecology. The video begins by showing\nscenes of fish, butterflies, orangutans, and birds in the forest. Then, the video depicts two birds\ncooperatively building a nest on a rock. As it starts to rain in the forest, a hatchling is born. The two birds\ncatch bugs and frogs in the forest and feed them to the newborn. The camera follows the direction of the\nflowing river, which converges to form a spectacular waterfall. The video ends with a calm sea and beach,\nwith a large flock of seabirds flying over the sea, hunting for prey close to the water.\nFigure 2. Annotation Guidelines for the Video Summarization Task.\nquestion diversity and tended towards generalized queries.\nWe illustrate this through a comparative analysis of TVQA\n[21], Moviechat [41], and our PQA dataset’s question word\nclouds in Figure 5. While TVQA provides a diverse range\nof questions, it does so by employing specific character\nnames. In contrast, Moviechat avoids character names, but\nits questions are frequently overly broad, lack specific plot\ndetails, and exhibit diminished diversity. Our PQA dataset\nsuccessfully navigates these challenges, offering a diverse\nrange of questions without resorting to the use of character\nnames. More visualized examples of PQA task can be found\nin Figure 11.\nF.7. Sub-Scene Captioning (SSC).\nIn the development process of the SSC task, we employed\nhuman annotators to generate both prompts and standard\ncaption data. The specific guidelines provided to annotators\nare illustrated in Figure 6. Initially, the annotators identi-\n4\nPrompt for Generating Needle Questions\nYou are a question setter. Your task is to evaluate the\nparticipants’ ability to capture detailed information from an\nextremely long video. The participants will receive a lengthy\nand content-rich video, and you are required to ask a question\nabout a specific piece of information from the video.\nI will provide you with a description of the segment that\nneeds to be questioned at the end. Your question must include\nas much contextual information as possible to help the\nparticipants\nlocate the source of the\ninformation.\nThe\ndescription I provide generally contains multiple clues, and\nyou should ask questions targeting different clues. Your\nquestion should be in a multiple-choice format, necessitating\nthe provision of at least four choices, including the correct\nanswer. Depending on the depth of information in the\nsegment description, you can craft between 1 to 3 distinct\nquestions.\nPlease provide the questions in the JSON format as follows...\nHere is the description of the segment that needs to be\nquestioned...\nFigure 3. The prompt provided to GPT-4 in the process of creating\nthe question-answer pair for the Needle Question-Answering task.\nfied a specific, easily referable sub-scene within a lengthy\nmovie. Subsequently, they crafted a prompt replete with\nadequate clues to reference this scene, ensuring the unique-\nness of these clues throughout the entire film. To prevent\nany leakage of information, the prompt was designed to ex-\nclude any character-specific names or objective hints, instead\nincorporating rich descriptive details to allude to the plot.\nFollowing this, the annotators produced a detailed caption\nfor this sub-scene, and deconstructed the caption into multi-\nple, non-redundant \"scoring points\" to facilitate quantitative\nassessment (the details of the evaluation metric can be found\nin Appendix G.3). More visualized examples of PQA task\ncan be found in Figure 12.\nF.8. Action Order (AO).\nThe videos, questions, and answers for the action order\ntask were all synthetically generated. In order to maintain\nthe high quality of our evaluation data, we adopted a dual-\nstrategy approach. Firstly, we selected actions for the probe\nvideos that were not commonly seen in most films, such as\nmaking jewelry and water skiing. Secondly, in the selection\nof background videos, we conducted a cursory review of the\nvideo content to further ensure that the actions referenced in\nthe questions were not present in the video. This rigorous\nmethodology ensured the reliability of our data.\nF.9. Action Count (AC).\nThe process of data acquisition and annotation for the action\ncount task closely mirrored that of the action order task. All\nvideos, questions, and answers were synthetically generated.\nWe employed a strategy consistent with the action order task\nto ensure the validity and reliability of our evaluation data.\nG. Details of Baselines and the Evaluation Pro-\ncess\nG.1. Baselines\nIn this section, we outline the primary baselines evaluated\non our MLVU. For image-based MLLMs, most models lack\nmulti-image inference capabilities. Therefore, we select\nOtter-I, LLaVA-1.6, and InternVL, which have official multi-\nimage implementations. Additionally, we include two pro-\nprietary models—Claude-3-Opus and Qwen-VL-Max—that\noffer APIs for multi-image inference. For the available mod-\nels, we determine the maximum input frames based on their\nLLM context length. Claude and Qwen support a maximum\nof approximately 20 images, so we choose 16 frames to\nensure fair comparisons. Regarding video MLLMs, we use\ndefault frame sampling strategies. For example, VideoChat2\nuniformly samples 16 frames, while LLaMA-Vid samples\n1 frame per second. Specifically, GPT-4o can handle up to\napproximately 500 images at a resolution of 512×512 pixels.\nThus, we select a sampling rate of 0.5 fps to accommodate\nmost of our videos.\nG.2. Inference Detatils\nWe have developed two templates specifically for Multiple-\nChoice and Generation tasks, as illustrated in Figure 7. Dis-\ntinct system prompts were designed to accommodate the\ndifferences between video-based and image-based MLLMs.\nConsidering the variances in task requirements, we incorpo-\nrated “option prediction guidance” into the Multiple-Choice\ntemplate to aid in option extraction. Conversely, in Genera-\ntion tasks, we do not implement any additional interventions\nbut employ fixed-question guidance to enable models to\nrespond to diverse task questions. In our evaluation, the tem-\nplates are seamlessly integrated into the evaluation code of\nopen-release models or available API of proprietary models.\nG.3. Evaluation Metrics\nFor the evaluation of Multiple Choice tasks, we directly\ncompute absolute accuracy by matching the predicted op-\ntion with the ground truth. In Generation tasks, we develop\nmultiple criteria for assessment and employ GPT-4 to rank\nthe alignment between generated texts and the provided an-\nswers. As illustrated in Figure 8, we use “Accuracy” and\n“Relevance” to benchmark Sub-scene Captioning, and “Com-\npleteness” and “Reliability” to evaluate the capabilities of\nVideo Summary.\n5\nAnnotation Guidelines for Plot Question-Answering\n1. Task Description: Your task is to generate questions and answers based on the plot events depicted in\nvarious media, including movie, TV series, and cartoon animations.\n2. Question Requirements:\n-\nThe questions should target specific details or events within the given video. Both factual and inferential\nquestions are encouraged.\n-\nAvoid using specific character names in the questions. Instead, use pronouns or identify characters by\nunique attributes or roles (e.g., attire, age, profession).\n-\nEnsure that the plot referred to in your question is unique within the long video. Avoid using vague\ndescriptions that can apply to multiple instances (like \"eating\"). Instead, refer to unique scenes or add\nenough details to specify the exact event.\n3. Question Format:\n-\nQuestions should be structured in a multiple-choice format. Each question should have one correct answer\nand three plausible, yet incorrect, distractor options.\n4. Examples of Questions (for reference only, not limited):\n-\nHow does the character in the small boat end up?\n-\nHow did the warship and the small boat approach each other?\n-\nWhy didn't the old man buy the chicken?\n-\nWhat mode of transportation did the old man take in the end?\n-\nWhat was the young woman doing when she drove to the airport?\nFigure 4. Annotation Guidelines for the Plot Question-Answering Task.\nFigure 5. Word Cloud Comparison of questions in TVQA test set (left), MovieChat test set (middle), and our PQA (right). Notably,\nTVQA’s character-specific names require LLMs to recognize characters, risking reliance on pre-existing knowledge. In contrast, MovieChat\nquestions are less diverse. Our PQA addresses these issues, providing enhanced usability and reliability.\nH. Explorations of Video Retrieval Augmented\nGeneration\nAs discussed in Section 4.3, most MLLMs are adversely\naffected by video length. Drawing inspiration from the\nuse of Retrieval Augmented Generation (RAG) in video\nunderstanding, we have developed a zero-shot RAG strategy\nand seamlessly integrated it into existing MLLMs. Table 3\ndisplays the performance comparison between the baseline\nmodels and the models employing our RAG strategy. It is\nnoteworthy that all methods benefit from the RAG strategy\nin Needle QA, Ego Reasoning, and Plot QA. Conversely,\nminimal improvement is observed in Action Count, and a\ndecrease is noted in Action Order and Overall Reasoning.\nThis is primarily because RAG facilitates the retrieval of\ndetail-oriented video clips, which makes models more likely\nto focus on answer-related cues in specific single-detail rea-\nsoning tasks. However, RAG exhibits limited capabilities\nin multi-detail reasoning and holistic understanding tasks,\nwhich require global perception and knowledge aggregation.\nThe pipeline of our video retrieval augmented generation\nis illustrated in Figure 9. Initially, a long video is uniformly\ndivided into N video clips, each containing C frames. Sub-\nsequently, we employ a robust video feature extraction tool,\nLanguageBind [62] to extract clip embeddings FI ∈RN×d,\nwhere d represents the dimension of each clip embedding.\n6\nAnnotation Guidelines for Sub-Scene Captioning\n1. Task Description: You are required to provide a detailed caption for a specific scene in a long movie and\nclearly provide a unique prompt that can point to this scene.\n2. Prompt Requirements:\n-\nThe clue in the prompt should direct to a specific and singular scene in the movie.\n-\nEnsure that the prompt does not contain specific character names or movie-specific terms.\n-\nThe scene to be described should generally not exceed 1 minute.\n3. Caption Requirements:\n-\nAvoid using specific character names in the captions. Instead, use pronouns or identify characters by unique\nattributes or roles (e.g., attire, age, profession).\n-\nProvide a caption and a list of unique plot details as scoring points, ensuring there's no repetition of details\nalready present in the prompt.\n4. Examples:\n-\nExample (1):\n-\nPrompt: Please describe the situation after the man at the door takes off his hat and throws it away.\n-\nCaption: The hat flies into the room and is kicked into the large clock by the man in black who stands up.\n-\nScoring points: \"The hat flies into the room\", \"is kicked into the large clock\", \"by the man in black who\nstands up”\n-\nExample (2):\n-\nPrompt: Please describe the reaction of the short-haired man when the long-haired man took out the urn.\n-\nCaption: The short-haired man stood up, held the urn in his hands, and pressed his forehead against the\nmouth of the urn, unable to hold back his tears.\n-\nScoring points: \"The short-haired man stood up\", \"held the urn in his hands\", \"pressed his forehead\nagainst the mouth of the urn\", \"unable to hold back his tears\"\nFigure 6. Annotation Guidelines for the Sub-Scene Captioning Task.\nWe then compute the similarities between FI and the text\nembedding FT , concatenating the top K clips to enhance the\nmodel’s capability for question-answering. Given that many\nVideo MLLMs are limited to processing only 16 frames,\nwe have adjusted the settings for C and K to accommodate\nvideo retrieval in 16-second intervals. As discussed below,\nthe RAG strategy excels in detail-oriented tasks but shows\nlimitations in global understanding tasks. Moreover, it is\nrelatively inefficient, requiring more than one minute to com-\nplete the process. Consequently, more effective approaches\nneed to be developed for long video understanding tasks,\nand we aim to address this in future work.\nI. More Visualized Examples of MLVU.\nWe present additional visualizations of our MLVU annota-\ntion examples in Figures 10, 11, and 12.\n7\nFigure 7. Inference template for our MLVU.\n8\nEvaluation Prompt For Sub-Scene Captioning Task\n##TASK DESCRIPTION: You are required to evaluate a respondent's answer based on a provided question, some scoring points,\nand the respondent's answer. You should provide two scores. The first is the accuracy score, which should range from 1 to 5. The\nsecond is the relevance score, which should also range from 1 to 5. Below are the criteria for each scoring category.\n##ACCURACY Scoring Criteria:\nEvaluate the respondent's answer against specific scoring points as follows:\nScore 1: The response completely misses the scoring point.\nScore 3: The response mentions content related to the scoring point but is not entirely correct.\nScore 5: The response accurately addresses the scoring point.\nCalculate the average score across all scoring points to determine the final accuracy score.\n##RELEVANCE Scoring Criteria:\nAssess how the respondent's answer relates to the original question:\nScore 1: The response is completely off-topic from the question.\nScore 2: The response is partially related to the question but contains a significant amount of irrelevant content.\nScore 3: The response primarily addresses the question, but the respondent seems uncertain about their own answer.\nScore 4: The response mostly addresses the question and the respondent appears confident in their answer.\nScore 5: The response is fully focused on addressing the question with no irrelevant content and demonstrates complete certainty.\n##INSTRUCTION:\n1. Evaluate ACCURACY: First, assess and score each scoring point based on the respondent's answer. Calculate the average of these\nscores to establish the final accuracy score. Provide a detailed rationale before assigning your score.\n2. Evaluate RELEVANCE: Assess the relevance of the respondent’s answer to the question. Note that when evaluating relevance, the\ncorrectness of the answer is not considered; focus solely on how relevant the answer is to the question. Provide a comprehensive\nrationale before assigning your score.\n3. Output Scores in JSON Format: Present the scores in JSON format as follows...\nEvaluation Prompt For Video Summarization Task\n##TASK DESCRIPTION:\nYou are required to evaluate the performance of the respondent in the video summarization task based on the standard answer and the\nrespondent's answer. You should provide two scores. The first is the COMPLETENESS score, which should range from 1 to 5. The\nsecond is the RELIABILITY score, which should also range from 1 to 5. Below are the criteria for each scoring category:\n##COMPLETENESS Scoring Criteria:\nThe completeness score focuses on whether the summary covers all key points and main information from the video.\nScore 1: The summary hardly covers any of the main content or key points of the video.\nScore 2: The summary covers some of the main content and key points but misses many.\nScore 3: The summary covers most of the main content and key points.\nScore 4: The summary is very comprehensive, covering most to nearly all of the main content and key points.\nScore 5: The summary completely covers all the main content and key points of the video.\n##CORRECTNESS Scoring Criteria:\nThe correctness score evaluates the correctness and clarity of the video summary. It checks for factual errors, misleading statements,\nand contradictions with the video content. If the respondent's answer includes details that are not present in the standard answer, as\nlong as these details do not conflict with the correct answer and are reasonable, points should not be deducted.\nScore 1: Contains multiple factual errors and contradictions; presentation is confusing.\nScore 2: Includes several errors and some contradictions; needs clearer presentation.\nScore 3: Generally accurate with minor errors; minimal contradictions; reasonably clear presentation.\nScore 4: Very accurate with negligible inaccuracies; no contradictions; clear and fluent presentation.\nScore 5: Completely accurate with no errors or contradictions; presentation is clear and easy to understand.\n##INSTRUCTION:\n1. Evaluate COMPLETENESS: First, analyze the respondent's answer according to the scoring criteria, then provide an integer score\nbetween 1 and 5 based on sufficient evidence.\n2. Evaluate CORRECTNESS : First, analyze the respondent's answer according to the scoring criteria, then provide an integer score\nbetween 1 and 5 based on sufficient evidence.\n3. Output Scores in JSON Format: Present the scores in JSON format as follows...\nFigure 8. Detailed prompt for evaluation of generation tasks in MLVU.\n9\nFigure 9. Pipeline of our video retrieval augmented generation strategy.\n10\nModel\nSettings Needle QA Ego Rea. Plot QA Action Or. Action Co. Anomaly Rec. Topic Rea.\nLLaVA-B\n-\n43.1\n38.4\n41.0\n25.5\n25.7\n41.0\n60.6\nLLaVA-R\nC=2,K=8\n50.7\n45.7\n49.7\n26.3\n26.7\n40.8\n59.8\nC=4,K=4\n53.5\n43.5\n50.6\n25.9\n29.6\n39.9\n58.5\nC=8,K=2\n55.2\n42.6\n50.3\n25.1\n30.1\n40.6\n59.5\nInternVL-B\n-\n52.7\n43.5\n54.4\n32.8\n23.8\n67.0\n78.8\nInternVL-R\nC=2,K=8\n77.2\n52.6\n61.4\n30.1\n36.4\n57.9\n69.2\nC=4,K=4\n76.3\n51.4\n59.9\n29.3\n36.9\n58.3\n69.4\nC=8,K=2\n77.8\n48.9\n61.6\n31.7\n33.0\n60.2\n62.3\nVideo-LLaMA-B\n-\n39.4\n33.5\n35.4\n18.5\n25.7\n41.5\n54.5\nVideo-LLaMA-R\nC=2,K=8\n61.4\n42.6\n38.8\n17.4\n17.5\n35.7\n48.5\nC=4,K=4\n58.9\n42.6\n39.1\n17.8\n23.8\n36.0\n49.3\nC=8,K=2\n62.0\n38.4\n36.2\n25.5\n18.0\n38.5\n51.0\nVideoChat2-B\n-\n42.0\n47.4\n43.8\n22.8\n29.6\n51.5\n74.6\nVideoChat2-R\nC=2,K=8\n72.1\n53.7\n55.5\n21.6\n30.1\n45.8\n68.2\nC=4,K=4\n72.4\n55.4\n53.4\n22.4\n31.1\n45.3\n68.9\nC=8,K=2\n73.8\n53.1\n55.3\n22.0\n31.6\n46.6\n69.7\nMiniGPT4-Video-B\n-\n49.0\n48.6\n44.5\n23.2\n23.0\n52.5\n70.9\nMiniGPT4-Video-R\nC=2,K=8\n60.6\n44.3\n47.4\n23.2\n23.7\n42.8\n60.9\nC=4,K=4\n60.3\n44.6\n46.9\n26.3\n23.8\n42.6\n60.7\nC=8,K=2\n56.3\n44.6\n46.6\n27.4\n24.8\n45.0\n47.5\nTable 3. Quantitative results on video Retrieval Augmented Generation. “model-B” and “model-R” denote Baseline and RAG models\nrespectively. We evaluate two image MLLMs and three video MLLMs in different settings.\n11\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nQuestion: What type of film is this?\n(A) Mystery \n(B) Action \n(C) Comedy \n(D) Romance\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nQuestion: What is this video about?\n(A) A person in the game taking care of pets \n(B) A person in the game building a structure by the lake\n(C) A person in the game planting trees by the lake (D) A documentary about humans and nature\nQuestion: Where is the main setting of the video?\n(A) Desert      (B) Grassland\n(C) Outside the house \n(D) Inside the house\nTopic Reasoning\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nPrompt: Please summarize the main content of this video.\nThe video begins with two men talking in a dimly lit room. After one of the men leaves, he enters another\nhouse where an elderly woman is present. They engage in conversation, and the elderly woman appears sad.\nIn another scene, two women are talking, and one of them takes car keys and leaves. She arrives at another\nlocation and talks with a woman and a man. Subsequently, one of the women makes a phone call.\nVideo Summarization\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nPrompt: Please summarize the main content of this video.\nThe video starts with a man singing into a microphone, with a few other men playing instruments behind\nhim. The scene changes to someone pushing open a door and walking into a room where others are\nresting. She then opens another door, enters a room and starts arguing with the singing man, which results\nin a fight. Next, the woman drives the man away, which results in a car crash. The car then falls off a\nbridge and gets hit by another car. The screen goes black and then lights up again, revealing a bookshelf\nfilled with books at the end.\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nFigure 10. More Examples of Topic Reasoning and Video Summarization Tasks.\n12\nQuestion: Where is the senior businessman having a serious conversation on the cell phone?\n(A) In a park\n(B) By the sea shore\n(C) In his office \n(D) At a restaurant\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nNeedle Question-Answering\nQuestion: What are the little girl and her grandmother doing together?\n(A) Watching TV\n(B) Playing a game\n(C) Reading a children‘s book\n(D) Eating dinner\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nQuestion: What happened after the person with the yellow stripe arrived at the camp?\n(A) He went to eat \n(B) He went hunting \n(C) He went to war \n(D) He started a fight with the person holding the pipe\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nPlot Question Answering\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nQuestion: What color is the table lamp in the background of the scene where a man and a women \nare chatting?\n(A) Black\n(B) White\n(C) Green \n(D) Yellow\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nFigure 11. More Examples of Needle Question Answering and Plot Question Answering Tasks.\n13\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nPrompt: Please describe the situation after the woman in red walked to the window of the bridal shop.\nAnswer: The woman in red took a picture with her camera. As the photo slowly slid out, she looked \ndown at it.  \nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nSub-Scene Captioning\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nPrompt: Please describe the process of a man alone in a room looking for a camera.\nThe man raises his cue stick to find the angle, then turns around and walks to a statue where \nhe finds the camera.  \nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nFigure 12. More Examples of Sub-Scene Captioning.\n14\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MLVU: Benchmarking Multi-task Long Video Understanding.pdf"}
{"title":"A Survey on Large Language Model-Based Game Agents","authors":"Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu","summary":"The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence (AGI). The progress of LLMs and their\nmultimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve\nand empower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview of\nLLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essential\nfunctional components: perception, memory, thinking, role-playing, action, and\nlearning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation\nagility across six genres of games, including adventure, communication,\ncompetition, cooperation, simulation, and crafting & exploration games.\nFinally, we present an outlook of future research and development directions in\nthis burgeoning field. A curated list of relevant papers is maintained and made\naccessible at: https:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.","url":"http:\/\/arxiv.org\/abs\/2404.02039v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2404.02039v1","published":1712072058000,"comment":null,"pdf_text":"A Survey on Large Language Model-Based Game Agents\nSihao Hu†, Tiansheng Huang†, Fatih ˙Ilhan†, Selim Tekin†,\nGaowen Liu‡, Ramana Kompella‡, Ling Liu†\n†Georgia Institute of Technology,\n‡Cisco Research\nQ sihaohu@gatech.edu, ling.liu@gatech.cc.edu\nAbstract\nThe development of game agents holds a critical role in advancing towards Ar-\ntificial General Intelligence (AGI). The progress of LLMs and their multimodal\ncounterparts (MLLMs) offers an unprecedented opportunity to evolve and em-\npower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview\nof LLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essen-\ntial functional components: perception, memory, thinking, role-playing, action,\nand learning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation agility\nacross six genres of games, including adventure, communication, competition, co-\noperation, simulation, and crafting & exploration games. Finally, we present\nan outlook of future research and development directions in this burgeoning\nfield. A curated list of relevant papers is maintained and made accessible at:\nhttps:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n1\nIntroduction\nIntelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor\nactivity.\n— The Embodied Cognition Hypothesis [1]\nLarge language models (LLMs), exemplified by ChatGPT [2], represent an important milestone in\nnatural language understanding (NLU) and generative artificial intelligence (Gen-AI). Empowered by\ngenerative training over massive data of diverse web sources with hundreds of billions of parameters,\nLLMs demonstrate astonishing capabilities of generalizing knowledge from huge text corpus data\nand displaying conversational intelligence in natural language with human-level NLU performance.\nThe emergence of multimodal LLMs (MLLMs), such as GPT-4V [3] and Gemini [4], marks another\nmilestone, enabling LLMs to perceive and understand visual input. We conjecture that the success\nof LLM technologies fuels an unprecedented opportunity in the pursuit of human-like Artificial\nGeneral Intelligence (AGI): the cognitive capabilities previously thought to be exclusive to humans,\nsuch as reasoning, planning, and reflection, with a degree of self-control, self-understanding, and\nself-improving, are now achievable by integrating appropriately prompting of LLMs with built-in\ncognitive intelligence.\nWe define an LLM-based agent (LLMA) as an intelligent entity that employs LLMs1 as a core\ncomponent to conduct human-like decision-making process [5]. Even though LLMAs are capable\nof cognitive processing similar to human, a distinction between existing LLMAs and human-like\n1In this paper, LLMs refers to both large language models (LLMs) and multimodal large language models\n(MLLMs).\narXiv:2404.02039v1  [cs.AI]  2 Apr 2024\nAGI is evident: current LLMAs rely on decoding and generalizing pre-existing knowledge derived\nfrom pre-training data [6], while AGI is capable of discovering and learning new knowledge through\nexperimentation and experience in real world [7; 8]. Inspired by the process of intelligence devel-\nopment in human infants, the embodied cognition hypothesis [1] posits that the intelligence of an\nagent emerges from observing and interacting its environment, i.e., grounding the agent in a world\nthat integrates physical, social, and linguistic experiences is vital for fostering conditions conducive\nto the development of human-like intelligence.\nDigital games are recognized as ideal environments for cultivating AI agents due to their complexity,\ndiversity, controllability, safety and reproducibility. Games, ranging from classical chess and poker\ngames [9; 10; 11] to modern video games like Atari games [12], StarCraft II [13], Minecraft [14]\nand DOTA II [15], have been long instrumental in advancing AI research. Unlike traditional\nReinforcement Learning (RL)-based agents [10; 16; 17; 18] that make decisions with the goal\nof maximizing expected rewards through behavior-level policy learning, constructing LLM-based\ngame agents (LLMGAs) capable of employing cognitive abilities to gain fundamental insights into\ngameplay, potentially aligns more closely with the pursuit of AGI.\nPrevious survey papers on LLMs [19; 20; 21] or LLMAs [22; 23; 24] mainly focus on reviewing\nexisting LLMs developed in industry and academic research teams, as well as the general applications\nof LLMAs, paying less attention to the field of game agents. Concurrent survey papers [25; 26] place\na notable emphasis on the game development and cover a limited number of publications on LLMGAs.\nTo bridge this gap, this paper attempts to conduct a comprehensive and systematic survey on recent\ndevelopments in LLMGAs. Specifically, this survey is organized into three synergistic parts: First, we\nprovide a unified reference framework, in which we describe the essential modules for constructing\nLLMGAs, covering six core functional components: perception, memory, thinking, role-playing,\naction and learning. Second, we introduce a taxonomy that categorizes existing literature into six game\ncategories, including adventure, competition, cooperation, simulation, and crafting & exploration.\nFor each category, we describe the technical challenges, the supporting game environments, as well as\nthe commonly used optimization strategies. In the third and final part, we envision different directions\nof future advancement of LLMGAs.\nIn summary, this survey paper serves as a comprehensive review of the literature on LLMGAs,\noffering a taxonomy of six game categories to enhance understanding and facilitate the development\nand assessment of various LLMGAs. It aims to catalyze progress within this nascent research\narea and to inspire further innovation in research and development of LLMGAs. Given that this\nis a new and burgeoning research field, this survey paper will be continuously updated to keep\ntrack of the latest studies. A curated list of relevant literature is maintained and accessible at\nhttps:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n2\nA Unified Architecture for LLMGAs\nFigure 1 provides an conceptual architecture of LLMGAs that consist of the six essential functional\ncomponents and their workflow: For each game step, the perception module captures game state\ninformation, providing the necessary data for the agent to understand its current environment. The\nthinking module processes the perceived information, generating thoughts based on reasoning,\nplanning, and reflection for informed decision-making. Memory serves as an external storage, where\npast experiences, knowledge and curated skills are retained and can be retrieved for future use.\nThe role-playing module enables the agent to simulate specific roles within the game, exhibiting\nbelievable behaviors that align with each role’s characteristics and objectives. The action module\ntranslates the generated text decisions into executable actions, allowing the agent to interact and\nmanipulate game elements effectively. The learning module continuously improves the agent’s\ncognitive and game-playing abilities through accumulated experience and interaction within the game\nenvironments.\n2.1\nPerception\nPerception acts like the agent’s sense organs, such as eyes, with their primary role being to perceive\ninput from a multimodal domain that encompasses various modalities, including text, visuals, sound,\ntouch, etc. Efficient and robust perception functions are critical to empower a game agent to accurately\ncapture the important game state information for decision-making.\n2\nFigure 1: The conceptual architecture of LLMGAs. At each game step, the perception module\nperceives the multimodal information from the game environment, including textual, images, symbolic\nstates, and so on. The agent retrieves essential memories from the memory module and take them\nalong with perceived information as input for thinking (reasoning, planning, and reflection), enabling\nitself to formulate strategies and make informed decisions. The role-playing module affects the\ndecision-making process to ensure that the agent’s behavior aligns with its designated character. Then\nthe action module translates generated action descriptions into executable and admissible actions\nfor altering game states at the next game step. Finally, the learning module serves to continuously\nimprove the agent’s cognitive and game-playing abilities through accumulated gameplay experience.\nAll text-based games, regardless whether they are text adventure games, such as Zork I [27], or\ncommunication games, such as Werewolf [28], are described using natural languages and can be\ndirectly tackled by LLMs. In contrast, for videos games there are three primary ways to enable agents\nperceive the game state:\n1. State variable access: Some game environments [29; 30; 31; 14; 32] support to access symbolic\nstate variables via internal APIs. For example, a Pokémon in Pokémon battles [30] can be\nrepresented by the state variables of species, statistics, status, and moves, without relying on\nany visual information. In Minecraft, Mineflayer [33] provides high-level APIs to access the\nlocal environment state, such as positions, blocks, inventory. The state values are filled into\ndesigned prompt templates to form textual descriptions of game states. However, not all the\ngames support internal APIs, and describing games merely with symbolic states can result in\ninformation loss, especially for games that require detailed visual information to fully capture\nthe gameplay experience, like Red Dead Redemption 2 [34] and StarCraft II [29].\n2. External visual encoder: To solve text-only problem, existing studies equips LLMs with\nexternal visual encoders to translate visual information into textual observations. For example,\nCoELA [35] and LLMPlanner [36] adopt object detectors to recognize objects within the agent’s\nfield of view in embodied environments. The CLIP [37] visual encoder and its variants are\nwidely used for mapping images into pre-defined text descriptions [38; 39; 40]. For example,\nJARVIS-1 [39] uses MineCLIP [41] to select the most similar text description from a set of\n1,000 Minecraft text data entries for images; ELLM [40] adopts ClipCap [42] as the captioner\nfor visual observations: it maps CLIP embedding to a 10-token sequence, which are fed as the\nprefix for GPT-2 to generate the whole caption.\n3. Multimodal LLMs (MLLMs): Visual encoders fall short in generalizability for unseen sce-\nnarios or objects, as they primarily rely on predefined text descriptions for classification. In\ncomparison, MLLMs align visual and textual information in a unified representation space\nand decode them into natural languages, thereby enabling better generalizability across un-\n3\nknown scenarios. General-purpose MLLMs like GPT-4V [3] are adopted in the game-playing of\nRDR2 [34], Doom [43], Minecraft [44] and simulated Embodied household [45] to directly per-\nceive visual observations for decision-making or generating text data as the perception module,\nbut usually need error correction mechanisms [45; 34] with feedback from the environments\nto address inaccuracies; Game-specific MLLMs involve supervised learning on multimodal\ninstruction data generated by experts, such as GATO [46] and SteveEye [47], or learned from\nenvironmental feedback through RL such as Octopus [45].\nIn summary, for video games, accessing symbolic states requires the support of internal APIs. External\nvisual encoders suffer from limited generalizability, as they cannot fully cover all scenarios or objects,\nespecially those without predefined textual descriptions. Although general-purpose MLLMs address\nabove-mentioned issues, they are still insufficient for distinguish fine-grained details like the relative\npositions of target objects, and struggles to understand game-specific concepts [34]. Grounding and\ndisciplining MLLMs with game experience and feedback [45] is a promising way to enable better\nperception and understanding for games.\nPerception\nText game\nZork Series [27; 48], ScienceWorld [49],\nALFworld [50], Diplomacy [51],\nWereworlf [28], Avalone [52], etc.\nVideo game\nState variable access\nStarCraft II [29], Pokémon Battles [30],\nOvercooked [31], Poker [53; 54],\nChess [55], Civilization [56], etc.\nVisual encoder\nELLM [40], LLMPlanner [36], CoELA [35],\nMineCLIP [41], Jarvis-1 [39], etc.\nMLLMs\nSteveEye [47], Octopus [45], GCC [34],\nPlayDoom [43], GATO [46], etc.\nFigure 2: Mind map for the perception module.\n2.2\nMemory\nHumans rely on memory systems to memorize prior experiences for recalling, learning, and applying\nknowledge in future scenarios. Similarly, LLMGAs necessitate memory systems to ensure operational\ncoherence and efficiency, serving as a repository for their past observations, thoughts, actions, and\nskills, from which agents retrieve essential information for strategy formulation and decision-making.\nFrom a perspective of cognitive science [57; 5], human’s memory mechanism can be divided as\nworking memory and long-term memory, where working memory stores an agent’s current context,\nand long-term memory stores the agents past experience and thoughts. For LLMGAs, working\nmemory can be regarded as the context of LLMs, and the term \"memory\" here refers to the long-term\nmemory, which acts as an external storage. Long-term memory stores episodic memories [58] such as\nobservation streams [59] and previous game trajectories [28; 60; 61] generated through the perception\nmodule; high-level semantic memories [62] that represents the agents’ understanding of itself and\nthe game world [59; 63], generated through the thinking module; and procedural memories [64],\nwhich represent curated skill stored as code [65; 34] or plans [66; 39].\nRetrieval: As memories accumulate over time, the majority of them distract from decision-making.\nRetrieval serves as an essential role to filter out through and pass the most relevant memories to the\nagent. Memory records are typically stored as key-value pairs. In semantic retrieval, the process\ninvolves calculating the semantic similarity between the representations of a query and the memory\nkeys, and selecting the memory values with the highest similarity to the query object. The query\nobject can be various forms, such as self-instructed questions [59], task-triggered questions [65],\npredefined questions [28], or visual observations [39]. Specifically, in Voyager [65]’s memory system,\nthe keys are program descriptions, and the values are the previously executed successful program\ncodes. In JARVIS-1 [39], the keys are composed of task descriptions paired with observations in\nimages, while the values represent the previously executed plans. Additionally, to simulate the\nhuman forgetting mechanism, Generative Agents [59] take into account recency and importance,\nwhere recency is calculated using an exponential decay function over game hours, and importance is\nevaluated by the LLM to differentiate mundane details from core information.\n4\nMemory\nFunctionality\nEpisodic memory\nGenerativeAgents [59], Xu et\nal. [28], ProAgent [61], etc.\nSemantic memory\nGenerativeAgents [59], Reflexion [63], etc.\nProcedural memory\nVoyager [65], GTIM [66],\nJARVIS-1 [39], Craddle [34], etc.\nRetrieval\nText\nGenerativeAgents [59], Voyager [65],\nGTIM [66], Craddle [34], etc.\nMulti-modal\nJARVIS-1 [39], CoELA [35], HAS [67], etc.\nFigure 3: Mind map for the memory module.\n2.3\nRole-playing\nRole-playing enables agents to assume diverse characters or roles within the game, generating\nbelievable conversations and behaviors appropriate to the given roles. Many games feature role-\nplaying elements [59; 28; 52] where players assume specific roles and engage in game-playing from\nthe perspective of the characters, leading to immersive gaming experiences. Role-playing is also\nimportant for building Non-Player Characters (NPCs) [68] and game assistants [69], as well as for\ngenerating dialogues [70].\nIt has been proved that assigning different personality types can largely influence the generative\nstyle of LLMs [71; 70]. Role-playing can enhance the vividness [72], personalization [73] and\nproficiency [74] of LLMAs, and generating dialogues with affective information makes agents’\nbehavior more believable [75; 76]. For role-playing, the simplest way is to directly insert natural\nlanguage descriptions of a role’s identity, such as character traits, hobbies, occupation and social\nrelationships, as initial memories for the agent [59]. Evaluations show that providing few-shot\ndialogue examples or fine-tuning can further enhance the role-playing performance in conversational\ntasks [70; 77]. Recent advanced approaches such as CharacterLLM [78] build imaginary experience\nfrom characters’ profiles, and fine-tune LLMs with these experiences to enable agents to exhibit\nconsistent personalities and express emotions.\n2.4\nThinking\nThinking is the cognitive process of analyzing and integrating information. In this section, we\nintroduce two primary thinking methods for decision-making: reasoning and planning. Reasoning\ninvolves using deduction, induction, and abduction to generalize observations, derive conclusions,\nand infer explanations. In comparison, planning strategizes decision steps to achieve complex and\nlong-horizon goals.\n2.4.1\nReasoning\nReasoning [79] is a process that starts from observation, factual evidence, and previous thoughts,\nthen progresses through analyzing and synthesizing these elements to deduce specific conclusions\nfrom general principles (deduction), infer general principles from specific instances (induction), or\nformulate the most likely explanations (abduction). Reasoning is fundamental to human cognition,\nenabling individuals to make sense of the world, solve problems, and make informed decisions.\nLLMGAs [80; 56; 43; 30] adopt general-purpose reasoning approaches [81; 82; 80; 83; 84] to analyze\ninformation logically, providing informative insights for decision-making: ReAct [80] introduce\nreasoning to condition the generation of action with few-shot prompting; CoT [81] and Zero CoT [82]\ndecompose the entire thinking process into multiple chained thoughts, enabling the step-by-step\nelucidation of complex problems; SC [84], ToT [83] and GoT [85] follow a multi-path reasoning\nparadigm: SC conducts multiple times independent reasoning and choose the result with the highest\nfrequency as the final output; ToT [83] and GoT [85] organize reasoning paths into tree and graph-like\nstructures to enhance the reasoning ability.\n5\nReflection [63; 38; 86] can be recognized as a special type of reasoning that usually occurs after feed-\nback from previous trials is provided. It involves the agent analyzing its own actions, decisions, and\nthought processes, and considering how these could be improved based on the feedback received. This\nreflective process allows for the adjustment and refinement of strategies and behaviors, contributing to\nlearning and development over time. Specifically, Reflexion [63], DEPS [38], AgentPro [61], ProA-\ngent [87] identify errors and inefficiencies in past failed attempts through self-reflection and reuse the\nthoughts to enhance the performance in subsequent trials. Moreover, RCI [88], Self-Refine [86] and\nGPTLens [89] demonstrates that the feedback can not only come from environments, but also LLMs\nthemselves, and iteratively refine the results by incorporating self-generated reflection.\nIn game playing, Hu et al. [30] discover that when encountering a powerful opponent, CoT can\nintroduce panic feelings, causing the agent to act inconsistently, such as switching to different\nPokémon in consecutive turns. In comparison, SC alleviates the issue by voting for the most consistent\naction; Theory-of-Mind [90; 91] (ToM) thinking involves inferring others’ intentions from a shifted\nperspective, and demonstrate enhancement in imperfect information games like Poker [54] and\nenables LLMGAs recognize partners’ intention for assistance in cooperation games [92]. Moreover,\nreflecting on the surface observations or experiences can provide high-level, abstract thoughts, which\nhelps the agent act more reasonably and believably [59].\n2.4.2\nPlanning\nHumans utilize planning as a strategic tool to address and manage challenging and long-term tasks.\nFor LLMGAs, planning involves the decomposition of a complex task into simpler, executable subtask\nset. Existing planning approaches can be categorized as goal-free planning and goal-conditioned\nplanning, based on whether a predefined goal is necessary for the planning process.\nGoal-free planning: Open-ended games usually do not have prefixed goals for players to achieve.\nGenerating a goal plan saves the agent from being overwhelmed by numerous possible actions.\nExisting goal-free planning approaches [59; 93; 65; 40; 94; 95] primarily instruct LLMs to generate\ngoal plans. Voyager [65], ELLM [40], SPRING [94] and AdaRefiner [95] prompt LLMs with agent’s\nstates such as hunger, inventory, and equipment, and local observations for generating suitable\nnext goals; OMNI [93] prompts LLMs to select interesting and learnable tasks for agent to explore\nthe open-world. In simulation games, a long-term daily plan can effectively prevent incoherent\nbehaviors [59; 30]. Generative Agents [59] utilize a top-down approach for generating a one-day\nplan for human-simulacra agents, starting with a broad initial plan for the day, then breaking it down\ninto more detailed action plans. After planning, agents can choose to either continue with the plan or\nreact to its dynamic local environment.\nGoal-conditioned planning: A goal-conditioned planner recursively translates a goal, task, or\ninstruction into a set of subgoals until it reaches structured actions. Goal-conditioned planning is\nused for addressing long-horizon and complex tasks such as crafting tools [38; 66] or performing\nquests [36; 96; 97]. Existing studies primarily instruct LLMs to generate plans. ZeroShotPlanner [98]\nand LLMPlanner [36] prompts LLMs with zero-shot or few-shot examples for planning; Given\nthe difficulty in generating a correct plan on the first attempt, GITM [66] and JARVIS-1 [39]\nleverage external knowledge, such as item crafting recipes to enhance planning, and also incorporate\nenvironmental feedback such as error messages to refine the initial plan; DEPS [38] introduce error\ncorrection on initial plans by integrating description of the plan execution and self-explanation of\nfeedback when encountering failures; Adapt [96] and SwiftSAGE [97] adaptively decompose tasks\nwith LLMs when encountering execution failures; S-Agents [99], HAS [67] and MindAgents [100]\noperate in a hierarchical cooperation structure in which a LLM planner dispatches sub-tasks to\nmulti-agents for efficient execution.\n2.5\nAction\nAfter humans make decisions to take actions, they control their bodies, such as hands, to execute\nthese actions, translating cognitive decisions into physical movements that interact with the world\naround them. The action module serves as the hands of the LLMGAs, translating language-described\ndecisions into executable actions in the game environment, enabling the agents to interact with and\nalter their surroundings or game states. Different games necessitate different levels of granularity in\nagents’ output actions. For games requiring manipulative control like RDR2 [34], Minecraft [14]\n6\nThinking\nReasoning\nCoT [81], Zero-CoT [81], ReAct [80],\nSC [84], ToM [91], GenerativeAgents [59],\nReflexion [63], DEPS [38], ProA-\ngent [87], RCI [88], Self-Refine [86], etc.\nPlanning\nGoal-free planning\nGenerative Agents [59], Voy-\nager [65], OMNI [93], ELLM [40],\nSPRING [94], AdaRefiner [95], etc.\nGoal-conditioned\nplanning\nZeroShotPlanner [98], LLMPlan-\nner [36], DEPS [38], GTIM [66],\nPlan4MC [101], SwiftSAGE [97],\nAdapt [96], S-Agents [99], etc.\nFigure 4: Mind map for the thinking module.\nand Overcooked [102], a low-level action like keyboard or mouse operation is required. In contrast,\ngames without manipulative control like text adventure games [103; 104], Pokémon battles [30] and\nPoker [53] directly facilitate the execution of high-level actions.\nLLMs typically generate high-level actions rather than low-level actions. Therefore, for games with\nmanipulative control, a translation module is required to translate LLM-generated action into low-\nlevel actions. Existing studies adopt heuristics [92; 105; 66; 33; 59] or RL policies [40; 101; 106] for\ntranslating a high-level action into low-level action sequences. Heuristic-based translation generates\nlow-level movements using path-finding algorithms, along with manipulative actions. For example,\nin Overcooked, given a high-level action \"chop tomato\", the translation module identifies the shortest\npath to the target with a breadth-first search algorithm and identifies a sequence of movements along\nwith the chop action [92; 105]; In Minecraft, the high-level \"approach\" action uses an A∗algorithm\nfor path-finding and executes low-level actions like jump, move and fall in four directions [66; 33];\nIn comparison, RL-based approaches [41; 40; 101; 106] train language-conditioned RL policies that\ntake observations and high-level actions as input to generate low-level actions, rewarded based on the\nsemantic similarity between the goals and the agent’s transitions.\nGames without manipulative control can be divided as parser-based games [27; 107] and choice-\nbased games [30]. Parser-based games require LLMs to generate an action word by word, wherease\nchoice-based games only need LLMs to select from a set of given actions. For parser-based games,\nZeroShotPlanner [98] proposes semantic translation that maps LLM-generated free-form actions\nto the semantically similar, admissible actions; SayCan [108] calculates the probability of each\nadmissible action using the chain rule by multiplying the conditional generation probability of each\nsuccessive string given the previous string.\n2.6\nLearning\nHumans are able to refine their cognitive abilities and acquire knowledge by interacting with the\nphysical world, gaining hands-on experience through direct engagement with their environments.\nSimilarly, an LLMGA’s learning process involves improving its cognitive and game-playing abilities\nover time, based on the experiences and feedback received from the game environment.\nLLMs encode a wealth of semantic knowledge about the world while lack of real experience within\nenvironments, i.e., they are ungrounded [108]. The majority of existing LLMGAs adopt frozen LLMs\nto play games, relying on carefully designed prompts [28; 109] or external knowledge [30; 39; 66].\nIn comparison, enable LLMGAs to learn in environments is crucial, since it closely mirrors the way\nhumans acquire knowledge through interacting with the real world. Existing learning approaches\ncan be divided into three categories: in-context feedback learning, supervised fine-tuning and\nreinforcement learning.\nIn-context feedback learning: Feedback represents a type of evaluation for previous strategies. By\nincluding feedback from environments into context, LLMs are able to iteratively \"reinforce\" strategy\ngeneration without updating weights [63; 65; 66; 30]. Specifically, Reflexion [63] and DEPS [38]\ngenerate self-reflection\/explanation on the feedback like failure signal and reuses the thought for the\nnext trail; Voyager [65] and GTIM [66] iteratively prompt LLMs to re-generate action code with error\nmessages; Hu et al. [30] uses manually generated feedback such as the HP change across consecutive\n7\nturns as evaluation for previous actions; Furthermore, existing works [63; 86; 86] demonstrate\nfeedback cannot only comes from the game environments, but also from LLMs themselves.\nSupervised fine-tuning: Supervised fine-tuning [110; 111] gathers high quality experience to fine-\ntune LLMs, based on the assumption that such experiences encompass environmental knowledge.\nSpecifically, E2WM [110] collects embodied experience in VirtualHome with Monte Carlo Tree\nSearch and random exploration; LLAMARider [111] gathers experience in Minecraft via self-\nreflection with feedback. Both of them demonstrate that fine-tuning on the collected experience\nenhances capability of LLMs on solving tasks within the environment. Moreover, imitation learning-\nbased approaches like GATO [46], LID [112], SwiftSAGE [97] and Octopus [45] fine-tune LMs\nusing expert or oracle trajectories to enhance their performance as policies.\nReinforcement Learning: Existing RL-based LLMGAs can be divided into three categories: (1)\nLLM as actor: GLAM [113] is grounded in the BabyAI-text environment as a policy to select\nnext action (four movements), training through online RL [114]; (2) LLM as planner: Existing\nstudies such as SayCan [108], Plan4MC [101], RL-GPT [106], ELLM [40], follow a hierarchical\nparadigm that integrates fixed LLMs as high-level planners with separate low-level RL policies to\nexecute actions. In comparison, another line of research involves fine-tuning large language model\n(LLM) planners based on rewards received from the environment, such as Octopus [45]; (3) LLM\nas presenter: LMs can be co-trained with RL policies to produce consistent dialogues that reflect\nthe intentions of policy models, especially in communication games such as Diplomacy [51] and\nWerewolf [115]; (4) LLM for reward design: LLMs can directly serve as reward models [116],\nprovide annotations for training reward models [117], or generate and refine reward functions for\nguiding RL agent training [118; 119; 120].\nLearning\nIn-context feed-\nback learning\nReflexion [63], DEPS [38], Voyager [65],\nRCI [88], PokéLLMon [30], etc.\nSupervised\nfine-tuning\nE2WM [110], LlaMARider [111],\nLID [112], SwiftSAGE [97],\nAdaRefiner [95], etc.\nReinforcement\nLearning\nLLM as actor\nGLAM [113], etc.\nLLM as planner\nSayCan [108], ELLM [40],\nPlan4MC [101], RL-GPT [106], etc.\nLLM as presentor\nCicero [51], Thinker [115], etc.\nReward design\nRewardDesign [116], Motif [117],\nMC-Reward [118], Eureka [120], etc.\nFigure 5: Mind map for the learning module.\n3\nLLMGAs in Games\nWe categorize existing studies into six categories based on the main characteristics of the games they\nsupport, including adventure, communication, competition, cooperation, simulation, and crafting &\nexploration. Figure 6 illustrates the core gameplay mechanics associated with its genre:\n• Adventure: Adventure games emphasize story-driven gameplay, where players explore environ-\nments, solve quests and interact with characters and objects to progress the game. Representative\ngames: Zork I [27] and Red Dead Redemption 2 (RDR2) [34].\n• Communication: Communication games revolve through the turns of communication, negotiation,\ndeduction and even deceptions among multiple players. Representative games: Werewolf [28] and\nDiplomacy [51].\n• Competition: Competition games pit players against each other in challenges that test skill or\nstrategy, aiming to outperform others for victory. Representative games: StarCraft II [29] and\nPokémon Battles [30].\n8\nFigure 6: The depiction of six game categories.\n• Cooperation: Cooperation games are designed around players working together towards common\ngoals, emphasizing teamwork, collaborative problem-solving, and shared achievements. Represen-\ntative games: Overcooked [102].\n• Simulation: Simulation games replicate real-world events in detail, allowing players to experience\nand manage scenarios ranging from building a civilization or living another life. Representative\ngames: The Sims [59; 121] and Civilization [56].\n• Crafting & Exploration: Crafting & Exploration games provide open worlds where players gather\nresources, craft items, and exploring within expansive environments, encouraging creativity and\ndiscovery. Representative games: Minecraft [14] and Crafter [122].\nWe summarize existing studies on LLMGAs in Table 1. In this section, we will walk through six game\ncategories, highlighting key findings and methodologies employed in the current research landscape.\n3.1\nAdventure Games\nAdventure games typically progress through storylines or quests. We categorize existing works into\ntwo types based on modality: text-based adventure games and video adventure games.\nText adventure games: A text adventure game provides a text-based environment in which players\nuse text commands to interact with the world, exploring and completing quests. TextWorld [137] is a\ngenerator of synthetic text games [138; 103] with varying difficulty levels by adjusting parameters\nsuch as the numbers of rooms and objects, quest length and complexity; Jericho [104] is a collection\nof 56 human-made games originally designed for human players, covering fictions such as the\nZork series [27; 48] and Hitchhiker’s Guide to the Galaxy [139]; ALFWorld [50] is aligned to\nthe embodied environment ALFRED [140], where agents are requested to accomplish six types\nof household tasks; ScienceWorld [49] simulates a primary school science curriculum, such as\nthermodynamics and electrical circuits. To complete a quest, an agent needs to navigate to specific\nrooms, obtain necessary items, conduct experiments, and analyze the results; BabyAI-Text [113] is\na text extension of BabyAI [141], a procedurally generated minigrid environment where an agent\nnavigates and interacts with objects.\nDue to the lack of graphics, text games rely on the commonsense knowledge as a prior for how\nto interact with the environment. In parser-based text games, generating a three-word sentence\nwith a small vocabulary of size 1,000 leads to 1 billion combinatorial candidates. Pre-trained LMs\nfeaturing human knowledge can effectively narrow down the action space and thus have been widely\nutilized as linguistic priors for guiding RL agents [123; 142; 143; 144]. Recently, LLMGAs are\nemployed to playing text adventure games: Tsai et al.[124] suggest that the game-playing ability of\nGPT-3.5 is on par with state-of-the-art (SOTA) reinforcement learning (RL) approaches[145; 146],\nbut it is incapable of constructing the entire map of a partially-known environment; REACT [80]\nand Reflexion [63] prompt LLMs to generate additional reasoning and reflection to condition the\ngeneration of actions; To solve challenging tasks, Adapt [96] and SwiftSage [97] adopt an LLM\nplanner to decompose complex tasks into subgoals as needed; GLAM [113] leverage online RL to\nground an LLM in BabyAI-text as a policy.\nVideo adventure game: Red Dead Redemption 2 (RDR2) is a 3D action-adventure game in which\nplayers assume the role of an outlaw, and follow the storyline of his life as part of a criminal gang.\nThe game features an important characteristic, i.e., it guides the player what to do next with instant\ninstructions. Cradle [34] is an LLMGA that perceives the game screen, analyzes instructions, generate\naction plans and controls the character through mouse\/keyboard operations using GPT-4V.\n9\nTable 1: Comparison among existing LLMGAs. FT denotes Fine-Tuning.\nStudies\nCategory\nGame\nBase Model\nFT\nModality\nCALM [123]\nAdventure\nJericho\nGPT-2\n✓\nTxt\nCanPlayWell [124]\nAdventure\nZork I\nGPT-3.5\n✗\nTxt\nReAct [80]\nAdventure\nALFWorld\nPaLM\n✗\nTxt\nReflexion [63]\nAdventure\nALFWorld\nGPT-3\n✗\nTxt\nADAPT [96]\nAdventure\nALFWorld\nGPT-3.5\n✗\nTxt\nSwiftSAGE [97]\nAdventure\nScienceWorld\nGPT-4 & T5\n✓\nText\nGLAM [113]\nAdventure\nBabyAI-Text\nFLAN-T5\n✓\nTxt\nCradle [34]\nAdventure\nRDR2\nGPT-4V\n✗\nTxt & Img\nXu et al. [28]\nCommunication\nWerewolf\nGPT-3.5\n✗\nTxt\nXu et al. [125]\nCommunication\nWerewolf\nGPT-4\n✗\nTxt\nThinker [115]\nCommunication\nWerewolf\nChatGLM-6B\n✓\nText\nReCon [52]\nCommunication\nAvalone\nGPT-4\n✗\nTxt\nAvaloneBench [126]\nCommunication\nAvalone\nGPT-3.5\n✗\nTxt\nCodeAct [127]\nCommunication\nAvalone\nGPT-4\n✗\nTxt\nCicero [51]\nCommunication\nDiplomacy\nBART\n✓\nTxt\nWarAgent [109]\nCommunication\nDiplomacy-like\nGPT-4\n✗\nTxt\nCosmoAgent [128]\nCommunication\nDiplomacy-like\nGPT-4\n✗\nTxt\nDEEP [129]\nCommunication\nWord Guess\nGPT-4\n✗\nTxt\nGameEval [130]\nCommunication\nWord Guess\nGPT-4\n✗\nTxt\nPokéLLMon [30]\nCompetition\nPokémon Battles\nGPT-4\n✗\nTxt\nCoS [29]\nCompetition\nStarCraft II\nGPT-3.5\n✗\nTxt\nSwarmBrain [131]\nCompetition\nStarCraft II\nGPT-3.5\n✗\nTxt\nChessGPT [55]\nCompetition\nChess\nRedPajama-3B\n✓\nPGN\nOthelloGPT [132]\nCompetition\nOthello\nGPT\n✓\nPGN\nPokerGPT [53]\nCompetition\nTexas Hold’em\nOPT-1.3B\n✓\nTxt\nGoodPoker [133]\nCompetition\nTexas Hold’em\nGPT-4\n✗\nTxt\nSuspicionAgent [54]\nCompetition\nLeduc Hold’em\nGPT-4\n✗\nTxt\nAgentPro [61]\nCompetition\nLeduc Hold’em\nGPT-4\n✗\nTxt\nLLM-Co [92]\nCooperation\nOvercooked\nGPT-4\n✗\nTxt\nMindAgent [100]\nCooperation\nOvercooked\nGPT-4\n✗\nTxt\nProAgent [87]\nCooperation\nOvercooked\n-\n✗\nTxt\nHLA [105]\nCooperation\nOvercooked\nGPT-3.5&LLaMA2\n✗\nTxt\nS-Agents [99]\nCooperation\nMinecraft\nGPT-4\n✗\nTxt\nHAC [67]\nCooperation\nMinecraft\nGPT-4V\n✗\nTxt & Img\nCoELA [35]\nCooperation\nTDW-T&WAH\nGPT-4\n✗\nTxt & Img\nGenerativeAgents [59]\nHuman Simulation\nSmallVille\nGPT-3.5\n✗\nTxt\nHumanoidAgents [134]\nHuman Simulation\nSocial\nGPT-3.5\n✗\nTxt\nLyfeAgent [121]\nHuman Simulation\nLyfe Game\nGPT-3.5\n✗\nTxt\nAgentSims [135]\nHuman Simulation\nAgentSims\n-\n✗\nTxt\nCivRealm [56]\nCivil. Simulation\nCivilization\n-\n✗\nTxt\nZeroShotPlanner [98]\nEmbodied Simulation\nVirtualHome\nGPT-3\n✗\nTxt\nLLMPlanner [36]\nEmbodied Simulation\nALFRED\nGPT-3\n✗\nTxt & Img\nE2WM [110]\nEmbodied Simulation\nVirtualHome\nLLaMA-13B\n✓\nTxt\nOctopus [45]\nEmbodied Simulation\nBehavior-1K\nCLIP & MPT-7B\n✓\nTxt & Img\nVoyager [65]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt\nDEPS [38]\nCraft & Explore\nMinecraft\nMineCLIP & GPT-4\n✗\nTxt & Img\nGTIM [66]\nCraft & Explore\nMinecraft\nGPT-3.5\n✗\nTxt\nJARVIS-1 [39]\nCraft & Explore\nMinecraft\nMineCLIP & GPT4\n✗\nTxt & Img\nPlan4MC [101]\nCraft & Explore\nMinecraft\nGPT-3.5\n✗\nTxt & Img\nRL-GPT [106]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt & Img\nMineDoJo [41]\nCraft & Explore\nMinecraft\nMineCLIP\n✓\nTxt & Img\nLLaMARider [111]\nCraft & Explore\nMinecraft\nLLaMA-2-70B\n✓\nTxt & Img\nSteveEye [136]\nCraft & Explore\nMinecraft\nCLIP & LLaMA2\n✓\nTxt & Img\nCreativeAgent [44]\nCraft & Explore\nMinecraft\nGPT-4V\n✗\nTxt & Img\nMCReward [44]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt & Img\nELLM [40]\nCraft & Explore\nCrafter\nCodex\n✗\nTxt & Img\nSPRING [94]\nCraft & Explore\nCrafter\nGPT-4\n✗\nTxt & Img\nAdaRefiner [95]\nCraft & Explore\nCrafter\nLLaMA2 & GPT-4\n✓\nTxt & Img\nOMNI [93]\nCraft & Explore\nCrafter\nGPT-3\n✗\nTxt & Img\nPlayDoom [43]\nOthers\nDoom\nGPT-4V\n✗\nTxt & Img\nGATO [46]\nOthers\nAtari\nGATO\n✓\nImg\nMotif [117]\nOthers\nNetHack\nLlaMA-2\n✗\nTxt\n10\n3.2\nCommunication Games\nCommunication (or conversational) games revolve through the turns of communication, negotiation,\ndeduction and deception among multiple players. The challenge of communication games lies in\ninferring others’ intention behind ambiguous or misleading language utterances, and hiding one’s\nown intention if necessary.\nWerewolf: The game pits two groups against each other, i.e., werewolves and non-werewolves\n(villagers, witch, guard and seer), and alternates between night phases, where werewolves secretly\nattack, and day phases, where survivors discuss and vote to eliminate suspects. The witch, guard,\nand seer each possess unique abilities. Xu et al. [28] propose to retrieve and reflect on historical\ncommunications for enhancement, and observe that GPT-3.5 demonstrates strategic behaviors such\nas trust, confrontation, camouflage, and leadership. Xu et al. [125] employ a RL policy to select the\noptimal action from among the diverse actions generated by LLMs, aiming to overcome the LLMs’\nprior preference for specific actions. Wu et al. [115] introduce a RL policy to generate the next\naction by taking as input the reasoning generated by the LLM, and employ another LLM to generate\ndescriptions aligned to the action.\nAvalon: The game progresses through rounds of discussion and voting to decide who participates in\nthe quests. The goal for the good team is to successfully complete quests, while the bad team aims\nto secretly sabotage these quests or identify the role of Merlin, who knows the identities of the bad\nplayers. Light et al. [126] suggest that GPT-3.5 struggles to formulate and execute simple strategies\nand sometimes reveals its own bad identity. Wang et al. [52] introduce a reasoning approach that\ntakes into account first-order and second-order perspective shifts to combat pervasive misinformation.\nTo combat hallucination, Shi et al. [127] propose to generate reasoning substeps in a code format that\nare interpreted as actions subsequently.\nDiplomatic games: Diplomacy is the first diplomatic board game from the 1950s where players\nassume the roles of seven powers striving to conquer Europe during WW1. Each turn is marked\nby private negotiations, trust-building, and tactical coordination among players. Cicero [51] is a\nhuman-level agent in Diplomacy that integrates a RL policy for planning and a BART [147] model\nconditioned on the plan for generating consistent negotiation messages; WarAgent [109] simulates\nthe participating countries, decisions, and consequences in WW I and WW II; CosmoAgent [128]\nmimics the communication, conflicts, and cooperation among various universal civilizations.\nOthers: Studies have demonstrated the game-playing abilities of LLMs in various games, including\nSpyGame (Who is Spy) [130; 129], Ask-Guess [130; 129], Tofu Kingdom [130], and Murder Mystery\nGames [148], known as Jubensha in Chinese.\n3.3\nCompetition Games\nCompetition games, governed by strict rules, challenge agents with varied-level opponents, demanding\nadvanced reasoning and skills. Competition games serves as benchmarks for evaluating reasoning and\nplanning abilities of LLMGAs directly against human players. Reaching human-level performance is\na crucial achievement that highlights the agent’s prowess in complex decision-making and strategic\nimplementation.\nStarCraft II: StarCraft II is a real-time strategy game in which players are tasked with gathering\nresources, building bases, creating armies, and engaging in combats to defeat the opponent. Ma et\nal. [29] introduce TextStarCraft II, a natural language interface that enables LLMs to play StarCraft\nII and Chain-of-Summarization for efficient reasoning and decision-making; SwarmBrain [131]\nintroduce an Overmind Intelligence Matrix for high-level strategic planning and a Swarm ReflexNet\nfor rapid tactical responses. These LLM-based agents exhibit comparable performance against the\ngame’s built-in AI at high difficulty levels.\nPokémon Battle: Pokémon battles are turn-based tactical games, with two players each sending out\none Pokémon and choosing either to attack or switch Pokémon each turn. Hu et al. [30] introduce an\nenvironment that enables LLMs to play Pokémon battles and a human-level agent PokéLLMon that\nconsumes instant feedback to iteratively refine the policy, retrieves external knowledge to combat\nhallucination, and generates consistent actions to alleviate the panic switching problem caused by\nCoT [81] reasoning.\n11\nChess: Feng et al. [55] introduce a large-scale chess gameplay dataset stored in Portable Game\nNotation format [149] and ChessGPT fine-tuned on mixed chess and language datasets to support\nboard state evaluation and chess playing; Toshniwal et al. [150] and Li et al. [132] discover that\nLMs trained to predict next move in chess are capable of tracking the state of the board given a\nmove sequence, i.e., LMs are capable of playing blindfolded. This suggests that LMs do not merely\nmemorize surface statistics but also learn a causal model of the sequence-generating process.\nPoker: In Texas Hold’em, Gupta et al. [151] observe that GPT-4 plays like an advanced yet aggressive\nplayer who raises with a wide range of hands pre-flop, avoids limping, and exhibits unconventional\nplay; PokerGPT [53] demonstrates that OPT-1.3B [152] with supervised fine-tuning and RLHF [2]\ncan achieve comparable performance to a RL-based method Alphaholdem [153] with significantly\nless training cost: 9.5 GPU hours compared to Alphaholdem’s 580 GPU hours. Guo et al. [54]\nand Zhang et al. [61] demonstrate that prompting LLMs to predict opponents’ thoughts, known as\nTheory of Mind [90; 91], results in significant improvements in Texas Hold’em, BlackJack and Leduc\nHold’em [11].\n3.4\nCooperation Games\nCooperation among individuals can enhance the efficiency and effectiveness of task accomplishment.\nThere are primarily three types of cooperative tasks in games: (1) Cooperative cooking [102; 100;\n154] requires agents collaborate to cook and deliver as many dishes as possible within the given time.\nTo prepare an onion soup in Overcooked-AI [102], two agents need to load three onions into a cooker,\nstarting a cooking process that lasts 20 time steps, and transfer the soup to a plate for delivery; (2)\nEmbodied household cooperation [155; 156] requires agents to collaboratively accomplish tasks\nlike transporting as many objects as possible to the goal position in embodied environments with\npartial observation [107; 157]; (3) Cooperative crafting [99; 100] & exploration [67] in Minecraft\ncan be accelerated through cooperation between multiple agents. Existing cooperative game settings\ncan be categorized into decentralized and centralized cooperation.\nDecentralized cooperation: A decentralized structure is a democratic structure (\n) where\nthere is no central task dispatcher. In Overcooked, the ability to infer the partner’s intent and next\naction based on the its historical actions, known as Theory-of-Mind, is crucial to prevent conflicts.\nAgashe et al. [92] show that GPT-4 is able to recognize and offer assistance to partners in need,\nand show robustness in adjusting to different partners. ProAgent [87] introduces a belief correction\nmodule to rectify incorrect beliefs on partners and consistently outperforms RL approaches [114;\n31; 158]. Moreover, HLA [105] integrates a proficient LLM and a lightweight LLM to balance\nefficacy and efficiency in real-time human-agent interaction; In partially-observable embodied\nenvironments, CoELA [35] introduce an efficient communication module to determine what and\nwhen to communicate, exhibiting better performance compare to MCTS-based and rule-based\nplanners on Watch-and-Help [155] and TDW Transport tasks [156].\nCentralized cooperation: In Minecraft, S-agents [99] and MindAgents [100] adopts a centralized\ndispatcher\/planner to decompose a challenging goal into subtasks and dispatches them to agents for\nexecution, forming a hierarchical architecture. HAS [67] introduces an auto-organizing mechanism\nto dynamically adjust key roles and action groups during cooperation, and an intra-communication\nmechanism to ensure efficient collaboration.\n3.5\nSimulation Games\nSimulation games provide simulated environments for real-world events or scenarios, enabling players\nto experience realistic interactions and decision-making in open-ended game playing. Existing studies\ncan be categorized as human & social simulation, civilization simulation and embodied simulation.\nHuman and social simulation: Generative Agents [59] marks the first LLM-based human simulation\nexperiment that leverages LLMs’ prior knowledge to simulate human-like daily life and social\nactivities. Specifically, GPT-3.5 assumes the roles of 25 generative agents with unique persona and\nsocial relationship, residing in a virtual small town. A cognitive architecture is introduced to support\nagents in remembering, retrieving, reflecting, planning, and acting within dynamic environments.\nDuring the two-day simulation, emergent behaviors like exchanging information, forming new\nrelationships and coordinating joint activities are observed.\n12\nOn the basis of Generative Agents, Humanoid Agents [134] further considers the effects of states like\nbasic needs (e.g., hunger, health, and energy), emotions, and closeness in relationships on agents’\nbehavior generation; For other simulation environments, AgentSims [135] is a programmable and\nextendable environment; LyfeGame [121] is an 3D virtual small town in Japan. Three experimental\nscenarios are designed to assess the social behaviors of LLM-based agents, including a murder\nmystery, a high school activity fair, and a patient-in-distress scenario.\nCivilization simulation: CivRealm [56] is a game environment based on Civilization [32], where\neach player governs a civilization simulating the progress of human history. As an open-ended game,\nit features diverse victory conditions, requiring players to strategically develop the economy, military,\ndiplomacy, culture, and technology of their civilizations. Mastaba [56] introduces an advisor and an\nAutoGPT [159]-style worker, where the advisor aids in generating context-specific objectives while\nthe workers handle the execution of these goals through generated actions. Experiments show that\nthe advisor brings an advantage at the early game stage, yet the advantage diminishes as the game\nprogresses.\nEmbodied simulation: In simulated 3D environments, embodied agents perceives their surround-\nings from egocentric perception similar to human and engage with realistic objects to carry out\na wide range of tasks by following instructions like \"Rinse off a mug and place it in the coffee\nmaker\". Existing benchmarks include AI2-THOR [160], Virtual Home [107], ALFRED [140], iGib-\nson [161], Habitat [162], ThreeDWorld [156], Behavior [163] and Behavior-1K [164]. Existing\napproaches [36; 98; 108; 110] primarily adopt LLMs as planners to decompose an instruction into\naction plans. Specifically, ZeroShotPlanner [98] prompts LLMs in zero-shot manner for planning;\nSayCan [108] uses a learned affordance function to assist LLMs in selecting valid actions during\nplanning; LLMPlanner [36] adopts an KNN retriever to select few-shot examples and dynamically\nre-plan based on the observation in the current environment; E2WM [110] fine-tunes an LLM with\nembodied experience collected through action space search and random exploration to enhance the\nunderstanding of the environments.\n3.6\nCrafting & Exploration Games\nMinecraft and Crafter are two game environments that have been widely studied for game agents with\na focus on crafting & exploration. Minecraft [14] is a 3D sandbox game that offer players the great\nfreedom to traverse a world made up of blocky, pixelated landscapes, facilitated by the procedurally\ngenerated worlds. The resouce-based crafting system enables players to transform collected materials\ninto tools, build elaborate structures and complex machines. Crafter [122] is a 2D open-world game\nthat mirrors the survival mode of Minecraft. It challenges players to manage their resources carefully\nto ensure sufficient water, food, and rest, while also defending against threats like zombies. The\ngame’s world is also procedurally generated for the exploration purpose, and it includes 22 tasks for\nplayers to accomplish.\nExisting agents can be divided as goal-conditioned agents that implement the task given an instruction\n(crafting), or autonomous exploration agents that navigate within the open-world based on self-\ndetermined objectives (exploration).\nCrafting: The key challenge in crafting tasks lies in their complexity: agents must gather diverse\nmaterials scattered across the world and understand intricate recipes and the sequential steps involved.\nConsequently, planning is widely employed to address crafting tasks. Existing agent design such\nas DEPS [38], GITM [66], JARVIS-1 [39], Plan4MC [101], RL-GPT [106] and S-agents [99]\nmainly follow a paradigm that adopts LLMs as a planner to decompose the goal into subgoals and\nfurther generate action plans for each sub-goals. Specifically, DEPS introduce error correction on\ninitial plans by integrating description of the plan execution and self-explanation of feedback when\nencountering failures; GITM [66] leverages external knowledges like item crafting\/smelting recipes,\nand is equipped with a long-term memory to maintain common reference plans for encountered\nobjectives; JARVIS-1 [39] chains MineCLIP [41] and an LLM together to perceive multimodal input\nand utilizes a multimodal memory to store experiences; Plan4MC [101] and RL-GPT [106] integrate\nthe LLM planner with a low-level RL policy for action execution; S-agents [99] and HAS [67]\ndispatches subtasks to multiple agents for cooperatively task execution;\nExploration: Navigating through procedurally generated world without specific goals can overwhelm\nagents with numerous possible actions. Previous works leverage curriculum learning [165] to identify\n13\nsuitable tasks while now LLMs can directly act as goal generators. In Minecraft, Voyager [65]\nadopts an automatic curriculum in a self-directed way [166], i.e., it asks LLM to generate goals\nthat adapts to the agent’s current state, inventory, acquired skills, and environment. In Crafter,\nOMNI [93] utilizes LLMs to determine interesting tasks for curriculum design, overcoming the\nprevious challenge of quantifying \"interest\". ELLM [40], SPRING [94] and AdaRefiner [95] prompt\nLLMs to generate goals for agents. Specifically, ELLM [40] queries LLMs for next goals given an\nagent’s current context, and rewards agents for accomplishing those suggestions in the sparse-reward\nsetting; SPRING [94] uses LLMs to summarize useful knowledge from the Crafter paper [122] and\nprogressively prompts the LLM to generate next action; On the basis of ELLM, AdaRefiner [95]\ncascades a learnable lightweight LLM with fixed LLMs for better goal plan generation.\n3.7\nEvaluation\nThe evaluation metrics for game agents vary across different games. In Table 2, we summarize\nthe metrics for several representative games. For games with specific task instructions, such as\nALFWorld, ScienceWorld, BabyAI, RDR2, ALFRED, VirtualHome, and crafting tasks in Minecraft\nand Crafter, the task success rate is usually adopted as the primary metric; for competition games,\nwin rate, game score, and Elo rating are common metrics; for communication games that separate\nplayers into adversarial teams such as Werewolf and Avalone, win rate can be used as the metric.\nFor human\/social simulation experiments, human evaluators are typically recruited to assess the\nbelievability of behaviors exhibited by LLM-based agents.\nTable 2: Evaluation metric used in representative games\nGame\nMetric\nJericho\nGame score [123]\nALFWorld\nTask success rate [80; 96]\nScienceWorld\nTask score [97]\nBabyAI\/BabyAI-Text\nTask success rate [93; 113]\nRDR2\nTask success rate [34]\nWerewolf\nWin rate [115; 28; 125], Voting accuracy [115]\nAvalone\nWin rate [52; 127; 126]\nDiplomacy\nPlayer score [51]\nStarCraft II\nWin rate [29; 131]\nPokémon Battles\nWin rate [30], Battle score [30]\nChess\nElo rating [55], Move score [55]\nPoker\nWin rate [54; 53], # of win\/loss chips [54], mbb\/hand [53]\nOvercooked\nReward value [92; 87], Success rate [100; 105]\nHuman Simulation\nHuman evaluation [59; 134]\nCivilization\nTask success rate [56], Game score [56], # of techs & units [56]\nALFRED\nTask success rate [36]\nVirtualHome\nTask success rate [98; 167], Executability [98; 167]\nMinecraft\nTask success rate [38; 65; 66], Map coverage [65], # of items [65]\nCrafter\nTask success rate [40; 93]\n4\nConclusion and Future Directions\nIn this paper, we conduct a systematic literature review of existing studies on LLMGAs, examining\ntwo primary aspects: (1) Construction of LLMGAs, where we elaborate on six essential components,\nincluding perception, memory, thinking, role-playing, action, and learning; (2) LLMGAs in six game\ncategories, including adventure, communication, competition, cooperation, simulation, and crafting\n& exploration, where we detail the game environments and common strategies adopted by game\nagents associated with each type. Finally, we identify three potential future directions for this new\nresearch field:\nGrounding LLMs in environments: LLMs pre-trained only on text corpus data are not grounded\nin real environments, i.e., they are not really aware of the consequences of their generations on\nphysical process [168]. Consequently, ungrounded LLMs generate inadmissible actions [108], exhibit\n14\na gap between high-level intentions and intricate game control [34], especially in the absence of\nvisual perception abilities [29] and feedback, and thus largely rely on manually-designed prompts.\nExisting efforts have been made toward grounding LLMs through multimodal perception [45], the\nadopt of external affordance functions [108], feedback from environments [30], and experiencing in\nenvironments [110; 40]. However, the current progress in grounding techniques remains limited and\nfall shorts for the requirements of real-world application [169]. Games, serving as controllable and\nsafe environments, are ideal testbeds for developing grounding techniques that can make LLMs more\ninline with sophisticated environments.\nKnowledge discovery through game-playing: Current studies [65; 30; 29] primarily remain at\nthe stage of utilizing the pre-existing knowledge encoded in LLMs for game-playing. Although\nsome studies propose to leverage game-playing experiences [110; 111; 40] to ground and enhance\nLLM-based agents, these agents are still incapable of extracting underlying knowledge below the\nsurfaces of observations and experience. Knowledge discovery is not simply about learning to act\neffectively, but to understand fundamental principles and causal model of gameplay mechanisms just\nlike human. We believe that gameplay mechanisms with complex extrinsic knowledge are essential\ntestbeds for designing such agents, and knowledge discovery via experiencing in environments might\nrepresent a critical step toward the pursuit of AGI.\nAgent society simulation: The social simulation experiment of Generative Agents [59] has demon-\nstrated that LLM-based agents are promising for believable simulacra of human. Emergent human-like\nsocial behaviors are observed like information diffusion, forming new relationship and coordination\nfor social activities, with the support of a novel cognitive architecture. However, as human beings are\nfar more sophisticated, with complicated mental processes, emotional depth, and advanced social\nskills, it would be an intriguing avenue for future research to develop better cognitive architectures and\nmore nuanced simulations of social interactions and cooperation [170] in realistic game environments\nto foster deeper understanding and representation of complex human interactions.\nAcknowledgements\nAuthors would like to thank Yunxiang Yan and Vijayraj Shanmugaraj for their assistance in collecting\npapers. This research is partially sponsored by the NSF CISE grants 2302720, 2312758, 2038029, a\nGTRI PhD Fellowship, an IBM faculty award, and a grant from CISCO Edge AI program.\nReferences\n[1] Smith, L., M. Gasser. The development of embodied cognition: Six lessons from babies.\nArtificial life, 11(1-2):13–29, 2005.\n[2] Ouyang, L., J. Wu, X. Jiang, et al. Training language models to follow instructions with human\nfeedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\n[3] Achiam, J., S. Adler, S. Agarwal, et al.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023.\n[4] Team, G., R. Anil, S. Borgeaud, et al. Gemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805, 2023.\n[5] Sumers, T. R., S. Yao, K. Narasimhan, et al. Cognitive architectures for language agents. arXiv\npreprint arXiv:2309.02427, 2023.\n[6] Delétang, G., A. Ruoss, P.-A. Duquenne, et al. Language modeling is compression. arXiv\npreprint arXiv:2309.10668, 2023.\n[7] Goertzel, B., C. Pennachin. Artificial general intelligence, vol. 2. Springer, 2007.\n[8] Goertzel, B. Artificial general intelligence: concept, state of the art, and future prospects.\nJournal of Artificial General Intelligence, 5(1):1, 2014.\n[9] IBM. Deep blue. https:\/\/www.ibm.com\/history\/deep-blue.\n15\n[10] Silver, D., A. Huang, C. J. Maddison, et al. Mastering the game of go with deep neural\nnetworks and tree search. nature, 529(7587):484–489, 2016.\n[11] Southey, F., M. P. Bowling, B. Larson, et al. Bayes’ bluff: Opponent modelling in poker. arXiv\npreprint arXiv:1207.1411, 2012.\n[12] Bellemare, M. G., Y. Naddaf, J. Veness, et al. The arcade learning environment: An evaluation\nplatform for general agents. Journal of Artificial Intelligence Research, 47:253–279, 2013.\n[13] Vinyals, O., T. Ewalds, S. Bartunov, et al. Starcraft ii: A new challenge for reinforcement\nlearning. arXiv preprint arXiv:1708.04782, 2017.\n[14] Mojang Studios. Minecraft. https:\/\/www.minecraft.net\/en-us.\n[15] Dota 2. https:\/\/www.dota2.com\/home.\n[16] Silver, D., J. Schrittwieser, K. Simonyan, et al. Mastering the game of go without human\nknowledge. nature, 550(7676):354–359, 2017.\n[17] OpenAI. Openai five. https:\/\/openai.com\/research\/openai-five, 2018. Accessed\non: yyyy-mm-dd.\n[18] DeepMind.\nAlphastar:\nMastering\nthe\nreal-time\nstrategy\ngame\nstarcraft\nii.\nhttps:\/\/deepmind.google\/discover\/blog\/\nalphastar-mastering-the-real-time-strategy-game-starcraft-ii, 2019.\n[19] Zhao, W. X., K. Zhou, J. Li, et al. A survey of large language models. arXiv preprint\narXiv:2303.18223, 2023.\n[20] Chang, Y., X. Wang, J. Wang, et al. A survey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology, 2023.\n[21] Kaddour, J., J. Harris, M. Mozes, et al. Challenges and applications of large language models.\narXiv preprint arXiv:2307.10169, 2023.\n[22] Xi, Z., W. Chen, X. Guo, et al. The rise and potential of large language model based agents: A\nsurvey. arXiv preprint arXiv:2309.07864, 2023.\n[23] Wang, L., C. Ma, X. Feng, et al. A survey on large language model based autonomous agents.\narXiv preprint arXiv:2308.11432, 2023.\n[24] Gao, C., X. Lan, N. Li, et al. Large language models empowered agent-based modeling and\nsimulation: A survey and perspectives. arXiv preprint arXiv:2312.11970, 2023.\n[25] Gallotta, R., G. Todd, M. Zammit, et al. Large language models and games: A survey and\nroadmap. arXiv preprint arXiv:2402.18659, 2024.\n[26] Sweetser, P. Large language models and video games: A preliminary scoping review. arXiv\npreprint arXiv:2403.02613, 2024.\n[27] Infocom. Zork I. http:\/\/ifdb.tads.org\/viewgame?id=0dbnusxunq7fw5ro, 1980.\n[28] Xu, Y., S. Wang, P. Li, et al. Exploring large language models for communication games: An\nempirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.\n[29] Ma, W., Q. Mi, X. Yan, et al. Large language models play starcraft ii: Benchmarks and a chain\nof summarization approach. arXiv preprint arXiv:2312.11865, 2023.\n[30] Hu, S., T. Huang, L. Liu. Pokéllmon: A human-parity agent for pokémon battles with large\nlanguage models, 2024.\n[31] Li, Y., S. Zhang, J. Sun, et al. Cooperative open-ended learning framework for zero-shot\ncoordination. arXiv preprint arXiv:2302.04831, 2023.\n[32] Freeciv-web contributors. Freeciv-web. https:\/\/github.com\/freeciv\/freeciv-web,\n2023.\n16\n[33] PrismarineJS. Mineflayer: Create minecraft bots with a powerful, stable, and high level\njavascript api. https:\/\/github.com\/PrismarineJS\/mineflayer, 2013.\n[34] Tan, W., Z. Ding, W. Zhang, et al. Towards general computer control: A multimodal agent for\nred dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.\n[35] Zhang, H., W. Du, J. Shan, et al. Building cooperative embodied agents modularly with large\nlanguage models. arXiv preprint arXiv:2307.02485, 2023.\n[36] Song, C. H., J. Wu, C. Washington, et al. Llm-planner: Few-shot grounded planning for\nembodied agents with large language models. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision, pages 2998–3009. 2023.\n[37] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural\nlanguage supervision. In International conference on machine learning, pages 8748–8763.\nPMLR, 2021.\n[38] Wang, Z., S. Cai, A. Liu, et al. Describe, explain, plan and select: Interactive planning with\nlarge language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,\n2023.\n[39] —. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language\nmodels. arXiv preprint arXiv:2311.05997, 2023.\n[40] Du, Y., O. Watkins, Z. Wang, et al. Guiding pretraining in reinforcement learning with\nlarge language models. In International Conference on Machine Learning, pages 8657–8677.\nPMLR, 2023.\n[41] Fan, L., G. Wang, Y. Jiang, et al. Minedojo: Building open-ended embodied agents with\ninternet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343–\n18362, 2022.\n[42] Mokady, R., A. Hertz, A. H. Bermano. Clipcap: Clip prefix for image captioning. arXiv\npreprint arXiv:2111.09734, 2021.\n[43] de Wynter, A. Will gpt-4 run doom? arXiv preprint arXiv:2403.05468, 2024.\n[44] Zhang, C., P. Cai, Y. Fu, et al. Creative agents: Empowering agents with imagination for\ncreative tasks. arXiv preprint arXiv:2312.02519, 2023.\n[45] Yang, J., Y. Dong, S. Liu, et al. Octopus: Embodied vision-language programmer from\nenvironmental feedback. arXiv preprint arXiv:2310.08588, 2023.\n[46] Reed, S., K. Zolna, E. Parisotto, et al. A generalist agent. arXiv preprint arXiv:2205.06175,\n2022.\n[47] Zheng, S., Y. Feng, Z. Lu, et al. Steve-eye: Equipping llm-based embodied agents with\nvisual perception in open worlds. In The Twelfth International Conference on Learning\nRepresentations. 2023.\n[48] Infocom. Zork III. http:\/\/ifdb.tads.org\/viewgame?id=vrsot1zgy1wfcdru, 1982.\n[49] Wang, R., P. Jansen, M.-A. Côté, et al. Scienceworld: Is your agent smarter than a 5th grader?\narXiv preprint arXiv:2203.07540, 2022.\n[50] Shridhar, M., X. Yuan, M.-A. Côté, et al. Alfworld: Aligning text and embodied environments\nfor interactive learning. arXiv preprint arXiv:2010.03768, 2020.\n[51] (FAIR)†, M. F. A. R. D. T., A. Bakhtin, N. Brown, et al. Human-level play in the game of\ndiplomacy by combining language models with strategic reasoning. Science, 378(6624):1067–\n1074, 2022.\n[52] Wang, S., C. Liu, Z. Zheng, et al. Avalon’s game of thoughts: Battle against deception through\nrecursive contemplation. arXiv preprint arXiv:2310.01320, 2023.\n17\n[53] Huang, C., Y. Cao, Y. Wen, et al. Pokergpt: An end-to-end lightweight solver for multi-player\ntexas hold’em via large language model. arXiv preprint arXiv:2401.06781, 2024.\n[54] Guo, J., B. Yang, P. Yoo, et al. Suspicion-agent: Playing imperfect information games with\ntheory of mind aware gpt-4. arXiv preprint arXiv:2309.17277, 2023.\n[55] Feng, X., Y. Luo, Z. Wang, et al. Chessgpt: Bridging policy learning and language modeling.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[56] Qi, S., S. Chen, Y. Li, et al. Civrealm: A learning and reasoning odyssey in civilization for\ndecision-making agents. arXiv preprint arXiv:2401.10568, 2024.\n[57] Atkinson, R. C., R. M. Shiffrin. Human memory: A proposed system and its control processes.\nIn Psychology of learning and motivation, vol. 2, pages 89–195. Elsevier, 1968.\n[58] Nuxoll, A. M., J. E. Laird. Extending cognitive architecture with episodic memory. In AAAI,\npages 1560–1564. 2007.\n[59] Park, J. S., J. O’Brien, C. J. Cai, et al. Generative agents: Interactive simulacra of human\nbehavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and\nTechnology, pages 1–22. 2023.\n[60] Tuyls, J., S. Yao, S. Kakade, et al. Multi-stage episodic control for strategic exploration in text\ngames. arXiv preprint arXiv:2201.01251, 2022.\n[61] Zhang, W., K. Tang, H. Wu, et al. Agent-pro: Learning to evolve via policy-level reflection\nand optimization. arXiv preprint arXiv:2402.17574, 2024.\n[62] Lindes, P., J. E. Laird. Toward integrating cognitive linguistics and cognitive language\nprocessing. In Proceedings of the 14th International Conference on Cognitive Modeling\n(ICCM). 2016.\n[63] Shinn, N., B. Labash, A. Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n[64] Lum, J. A., G. Conti-Ramsden, D. Page, et al. Working, declarative and procedural memory in\nspecific language impairment. cortex, 48(9):1138–1154, 2012.\n[65] Wang, G., Y. Xie, Y. Jiang, et al. Voyager: An open-ended embodied agent with large language\nmodels. arXiv preprint arXiv:2305.16291, 2023.\n[66] Zhu, X., Y. Chen, H. Tian, et al. Ghost in the minecraft: Generally capable agents for open-\nworld enviroments via large language models with text-based knowledge and memory. arXiv\npreprint arXiv:2305.17144, 2023.\n[67] Zhao, Z., K. Chen, D. Guo, et al. Hierarchical auto-organizing system for open-ended multi-\nagent navigation. arXiv preprint arXiv:2403.08282, 2024.\n[68] Uluda˘glı, M. Ç., K. O˘guz. Non-player character decision-making in computer games. Artificial\nIntelligence Review, 56(12):14159–14191, 2023.\n[69] Zhu, A., L. Martin, A. Head, et al. Calypso: Llms as dungeon master’s assistants. In Proceed-\nings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,\nvol. 19, pages 380–390. 2023.\n[70] Wang, Z. M., Z. Peng, H. Que, et al. Rolellm: Benchmarking, eliciting, and enhancing\nrole-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023.\n[71] Jiang, H., X. Zhang, X. Cao, et al. Personallm: Investigating the ability of gpt-3.5 to express\npersonality traits and gender differences. arXiv preprint arXiv:2305.02547, 2023.\n[72] Li, C., Z. Leng, C. Yan, et al. Chatharuhi: Reviving anime character in reality via large\nlanguage model. arXiv preprint arXiv:2308.09597, 2023.\n18\n[73] Salemi, A., S. Mysore, M. Bendersky, et al. Lamp: When large language models meet\npersonalization. arXiv preprint arXiv:2304.11406, 2023.\n[74] Li, G., H. Hammoud, H. Itani, et al. Camel: Communicative agents for\" mind\" exploration of\nlarge language model society. Advances in Neural Information Processing Systems, 36, 2024.\n[75] Croissant, M., M. Frister, G. Schofield, et al. An appraisal-based chain-of-emotion architecture\nfor affective language model game agents. arXiv preprint arXiv:2309.05076, 2023.\n[76] Huang, J.-t., M. H. Lam, E. J. Li, et al. Emotionally numb or empathetic? evaluating how llms\nfeel using emotionbench. arXiv preprint arXiv:2308.03656, 2023.\n[77] Tu, Q., S. Fan, Z. Tian, et al. Charactereval: A chinese benchmark for role-playing conversa-\ntional agent evaluation. arXiv preprint arXiv:2401.01275, 2024.\n[78] Shao, Y., L. Li, J. Dai, et al. Character-llm: A trainable agent for role-playing. arXiv preprint\narXiv:2310.10158, 2023.\n[79] Huang, J., K. C.-C. Chang. Towards reasoning in large language models: A survey. arXiv\npreprint arXiv:2212.10403, 2022.\n[80] Yao, S., J. Zhao, D. Yu, et al. React: Synergizing reasoning and acting in language models.\narXiv preprint arXiv:2210.03629, 2022.\n[81] Wei, J., X. Wang, D. Schuurmans, et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models. Advances in Neural Information Processing Systems, 35:24824–24837,\n2022.\n[82] Kojima, T., S. S. Gu, M. Reid, et al. Large language models are zero-shot reasoners. Advances\nin neural information processing systems, 35:22199–22213, 2022.\n[83] Yao, S., D. Yu, J. Zhao, et al. Tree of thoughts: Deliberate problem solving with large language\nmodels. arXiv preprint arXiv:2305.10601, 2023.\n[84] Wang, X., J. Wei, D. Schuurmans, et al. Self-consistency improves chain of thought reasoning\nin language models. arXiv preprint arXiv:2203.11171, 2022.\n[85] Besta, M., N. Blach, A. Kubicek, et al. Graph of thoughts: Solving elaborate problems with\nlarge language models. arXiv preprint arXiv:2308.09687, 2023.\n[86] Madaan, A., N. Tandon, P. Gupta, et al. Self-refine: Iterative refinement with self-feedback.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[87] Zhang, C., K. Yang, S. Hu, et al. Proagent: Building proactive cooperative ai with large\nlanguage models. arXiv preprint arXiv:2308.11339, 2023.\n[88] Kim, G., P. Baldi, S. McAleer. Language models can solve computer tasks. Advances in\nNeural Information Processing Systems, 36, 2024.\n[89] Hu, S., T. Huang, F. ˙Ilhan, et al. Large language model-powered smart contract vulnerability\ndetection: New perspectives. arXiv preprint arXiv:2310.01152, 2023.\n[90] Frith, C., U. Frith. Theory of mind. Current biology, 15(17):R644–R645, 2005.\n[91] Kosinski, M. Theory of mind may have spontaneously emerged in large language models.\narXiv preprint arXiv:2302.02083, 2023.\n[92] Agashe, S., Y. Fan, X. E. Wang. Evaluating multi-agent coordination abilities in large language\nmodels. arXiv preprint arXiv:2310.03903, 2023.\n[93] Zhang, J., J. Lehman, K. Stanley, et al. Omni: Open-endedness via models of human notions\nof interestingness. arXiv preprint arXiv:2306.01711, 2023.\n[94] Wu, Y., S. Y. Min, S. Prabhumoye, et al. Spring: Studying papers and reasoning to play games.\nAdvances in Neural Information Processing Systems, 36, 2024.\n19\n[95] Zhang, W., Z. Lu. Adarefiner: Refining decisions of language models with adaptive feedback,\n2023.\n[96] Prasad, A., A. Koller, M. Hartmann, et al. Adapt: As-needed decomposition and planning with\nlanguage models. arXiv preprint arXiv:2311.05772, 2023.\n[97] Lin, B. Y., Y. Fu, K. Yang, et al. Swiftsage: A generative agent with fast and slow thinking for\ncomplex interactive tasks. Advances in Neural Information Processing Systems, 36, 2024.\n[98] Huang, W., P. Abbeel, D. Pathak, et al. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International Conference on Machine Learning,\npages 9118–9147. PMLR, 2022.\n[99] Chen, J., Y. Jiang, J. Lu, et al. S-agents: self-organizing agents in open-ended environment.\narXiv preprint arXiv:2402.04578, 2024.\n[100] Gong, R., Q. Huang, X. Ma, et al. Mindagent: Emergent gaming interaction. arXiv preprint\narXiv:2309.09971, 2023.\n[101] Yuan, H., C. Zhang, H. Wang, et al. Plan4mc: Skill reinforcement learning and planning for\nopen-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\n[102] Carroll, M., R. Shah, M. K. Ho, et al. On the utility of learning about humans for human-ai\ncoordination. Advances in neural information processing systems, 32, 2019.\n[103] Microsoft Research. First textworld problems: The competition using text-based games to\nadvance capabilities of ai agents, 2019.\n[104] Hausknecht, M., P. Ammanabrolu, M.-A. Côté, et al. Interactive fiction games: A colossal\nadventure. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pages\n7903–7910. 2020.\n[105] Liu, J., C. Yu, J. Gao, et al. Llm-powered hierarchical language agent for real-time human-ai\ncoordination. arXiv preprint arXiv:2312.15224, 2023.\n[106] Liu, S., H. Yuan, M. Hu, et al. Rl-gpt: Integrating reinforcement learning and code-as-policy.\narXiv preprint arXiv:2402.19299, 2024.\n[107] Puig, X., K. Ra, M. Boben, et al. Virtualhome: Simulating household activities via programs.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n8494–8502. 2018.\n[108] Ahn, M., A. Brohan, N. Brown, et al. Do as i can, not as i say: Grounding language in robotic\naffordances. arXiv preprint arXiv:2204.01691, 2022.\n[109] Hua, W., L. Fan, L. Li, et al. War and peace (waragent): Large language model-based\nmulti-agent simulation of world wars. arXiv preprint arXiv:2311.17227, 2023.\n[110] Xiang, J., T. Tao, Y. Gu, et al. Language models meet world models: Embodied experiences\nenhance language models. Advances in neural information processing systems, 36, 2024.\n[111] Feng, Y., Y. Wang, J. Liu, et al. Llama rider: Spurring large language models to explore the\nopen world. arXiv preprint arXiv:2310.08922, 2023.\n[112] Li, S., X. Puig, C. Paxton, et al. Pre-trained language models for interactive decision-making.\nAdvances in Neural Information Processing Systems, 35:31199–31212, 2022.\n[113] Carta, T., C. Romac, T. Wolf, et al. Grounding large language models in interactive environ-\nments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.\n[114] Schulman, J., F. Wolski, P. Dhariwal, et al. Proximal policy optimization algorithms. arXiv\npreprint arXiv:1707.06347, 2017.\n[115] Wu, S., L. Zhu, T. Yang, et al. Enhance reasoning for large language models in the game\nwerewolf. arXiv preprint arXiv:2402.02330, 2024.\n20\n[116] Kwon, M., S. M. Xie, K. Bullard, et al. Reward design with language models. arXiv preprint\narXiv:2303.00001, 2023.\n[117] Klissarov, M., P. D’Oro, S. Sodhani, et al. Motif: Intrinsic motivation from artificial intelligence\nfeedback. arXiv preprint arXiv:2310.00166, 2023.\n[118] Li, H., X. Yang, Z. Wang, et al. Auto mc-reward: Automated dense reward design with large\nlanguage models for minecraft. arXiv preprint arXiv:2312.09238, 2023.\n[119] Xie, T., S. Zhao, C. H. Wu, et al. Text2reward: Automated dense reward function generation\nfor reinforcement learning. arXiv preprint arXiv:2309.11489, 2023.\n[120] Ma, Y. J., W. Liang, G. Wang, et al. Eureka: Human-level reward design via coding large\nlanguage models. arXiv preprint arXiv:2310.12931, 2023.\n[121] Kaiya, Z., M. Naim, J. Kondic, et al. Lyfe agents: Generative agents for low-cost real-time\nsocial interactions. arXiv preprint arXiv:2310.02172, 2023.\n[122] Hafner, D. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780,\n2021.\n[123] Yao, S., R. Rao, M. Hausknecht, et al. Keep calm and explore: Language models for action\ngeneration in text-based games. arXiv preprint arXiv:2010.02903, 2020.\n[124] Tsai, C. F., X. Zhou, S. S. Liu, et al. Can large language models play text games well? current\nstate-of-the-art and open questions. arXiv preprint arXiv:2304.02868, 2023.\n[125] Xu, Z., C. Yu, F. Fang, et al. Language agents with reinforcement learning for strategic play in\nthe werewolf game. arXiv preprint arXiv:2310.18940, 2023.\n[126] Light, J., M. Cai, S. Shen, et al. Avalonbench: Evaluating llms playing the game of avalon.\narXiv e-prints, pages arXiv–2310, 2023.\n[127] Shi, Z., M. Fang, S. Zheng, et al. Cooperation on the fly: Exploring language agents for ad\nhoc teamwork in the avalon game. arXiv preprint arXiv:2312.17515, 2023.\n[128] Jin, M., B. Wang, Z. Xue, et al. What if llms have different world views: Simulating alien\ncivilizations with llm-based agents. arXiv preprint arXiv:2402.13184, 2024.\n[129] Liang, T., Z. He, J.-t. Huang, et al. Leveraging word guessing games to assess the intelligence\nof large language models. arXiv preprint arXiv:2310.20499, 2023.\n[130] Qiao, D., C. Wu, Y. Liang, et al. Gameeval: Evaluating llms on conversational games. arXiv\npreprint arXiv:2308.10032, 2023.\n[131] Shao, X., W. Jiang, F. Zuo, et al. Swarmbrain: Embodied agent for real-time strategy game\nstarcraft ii via large language models. arXiv preprint arXiv:2401.17749, 2024.\n[132] Li, K., A. K. Hopkins, D. Bau, et al. Emergent world representations: Exploring a sequence\nmodel trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022.\n[133] Gupta, A. Are chatgpt and gpt-4 good poker players?–a pre-flop analysis. arXiv preprint\narXiv:2308.12466, 2023.\n[134] Wang, Z., Y. Y. Chiu, Y. C. Chiu. Humanoid agents: Platform for simulating human-like\ngenerative agents. arXiv preprint arXiv:2310.05418, 2023.\n[135] Lin, J., H. Zhao, A. Zhang, et al. Agentsims: An open-source sandbox for large language\nmodel evaluation. arXiv preprint arXiv:2308.04026, 2023.\n[136] Stengel-Eskin, E., A. Prasad, M. Bansal. Regal: Refactoring programs to discover generalizable\nabstractions. arXiv preprint arXiv:2401.16467, 2024.\n21\n[137] Côté, M.-A., A. Kádár, X. Yuan, et al. Textworld: A learning environment for text-based\ngames. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th\nInternational Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13,\n2018, Revised Selected Papers 7, pages 41–75. Springer, 2019.\n[138] Yuan, X., M.-A. Côté, A. Sordoni, et al. Counting to explore and generalize in text-based\ngames. arXiv preprint arXiv:1806.11525, 2018.\n[139] BBC.\nThe hitchhiker’s guide to the galaxy text adventure:\n30th anniversary edi-\ntion. https:\/\/www.bbc.co.uk\/programmes\/articles\/1g84m0sXpnNCv84GpN2PLZG\/\nthe-game-30th-anniversary-edition.\n[140] Shridhar, M., J. Thomason, D. Gordon, et al. Alfred: A benchmark for interpreting grounded\ninstructions for everyday tasks. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pages 10740–10749. 2020.\n[141] Chevalier-Boisvert, M., D. Bahdanau, S. Lahlou, et al. Babyai: A platform to study the sample\nefficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.\n[142] Singh, I., G. Singh, A. Modi. Pre-trained language models as prior knowledge for playing\ntext-based games. arXiv preprint arXiv:2107.08408, 2021.\n[143] Sudhakar, A. V., P. Parthasarathi, J. Rajendran, et al. Language model-in-the-loop: Data opti-\nmal approach to learn-to-recommend actions in text games. arXiv preprint arXiv:2311.07687,\n2023.\n[144] Shi, Z., Y. Xu, M. Fang, et al. Self-imitation learning for action generation in text-based\ngames. In Proceedings of the 17th Conference of the European Chapter of the Association for\nComputational Linguistics, pages 703–726. 2023.\n[145] Ammanabrolu, P., M. Hausknecht. Graph constrained reinforcement learning for natural\nlanguage action spaces. arXiv preprint arXiv:2001.08837, 2020.\n[146] Guo, X., M. Yu, Y. Gao, et al. Interactive fiction game playing as multi-paragraph reading\ncomprehension with reinforcement learning. arXiv preprint arXiv:2010.02386, 2020.\n[147] Lewis, M., Y. Liu, N. Goyal, et al. Bart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,\n2019.\n[148] Wu, D., H. Shi, Z. Sun, et al. Deciphering digital detectives: Understanding llm behaviors and\ncapabilities in multi-agent mystery games. arXiv preprint arXiv:2312.00746, 2023.\n[149] Wikipedia. Portable game notation. https:\/\/en.wikipedia.org\/wiki\/Portable_Game_\nNotation, 2023.\n[150] Toshniwal, S., S. Wiseman, K. Livescu, et al. Chess as a testbed for language model state\ntracking. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pages\n11385–11393. 2022.\n[151] Gupta, A. Are chatgpt and gpt-4 good poker players?–a pre-flop analysis. arXiv preprint\narXiv:2308.12466, 2023.\n[152] Zhang, S., S. Roller, N. Goyal, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[153] Zhao, E., R. Yan, J. Li, et al. Alphaholdem: High-performance artificial intelligence for\nheads-up no-limit poker via end-to-end reinforcement learning. In Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 36, pages 4689–4697. 2022.\n[154] Wu, S. A., R. E. Wang, J. A. Evans, et al. Too many cooks: Bayesian inference for coordinating\nmulti-agent collaboration. Topics in Cognitive Science, 13(2):414–432, 2021.\n[155] Puig, X., T. Shu, S. Li, et al. Watch-and-help: A challenge for social perception and human-ai\ncollaboration. arXiv preprint arXiv:2010.09890, 2020.\n22\n[156] Gan, C., S. Zhou, J. Schwartz, et al. The threedworld transport challenge: A visually guided\ntask-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint\narXiv:2103.14025, 2021.\n[157] Gan, C., J. Schwartz, S. Alter, et al. Threedworld: A platform for interactive multi-modal\nphysical simulation. arXiv preprint arXiv:2007.04954, 2020.\n[158] Zhao, R., J. Song, Y. Yuan, et al. Maximum entropy population-based training for zero-shot\nhuman-ai coordination. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 37, pages 6145–6153. 2023.\n[159] Significant Gravitas. AutoGPT.\n[160] Kolve, E., R. Mottaghi, W. Han, et al. Ai2-thor: An interactive 3d environment for visual ai.\narXiv preprint arXiv:1712.05474, 2017.\n[161] Xia, F., W. B. Shen, C. Li, et al. Interactive gibson benchmark: A benchmark for interactive\nnavigation in cluttered environments. IEEE Robotics and Automation Letters, 5(2):713–720,\n2020.\n[162] Savva, M., A. Kadian, O. Maksymets, et al. Habitat: A platform for embodied ai research. In\nProceedings of the IEEE\/CVF international conference on computer vision, pages 9339–9347.\n2019.\n[163] Srivastava, S., C. Li, M. Lingelbach, et al. Behavior: Benchmark for everyday household\nactivities in virtual, interactive, and ecological environments. In Conference on robot learning,\npages 477–490. PMLR, 2022.\n[164] Li, C., R. Zhang, J. Wong, et al. Behavior-1k: A benchmark for embodied ai with 1,000\neveryday activities and realistic simulation. In Conference on Robot Learning, pages 80–93.\nPMLR, 2023.\n[165] Florensa, C., D. Held, X. Geng, et al. Automatic goal generation for reinforcement learning\nagents. In International conference on machine learning, pages 1515–1528. PMLR, 2018.\n[166] Wang, Y., Y. Kordi, S. Mishra, et al. Self-instruct: Aligning language model with self generated\ninstructions. arXiv preprint arXiv:2212.10560, 2022.\n[167] Hao, S., T. Liu, Z. Wang, et al. Toolkengpt: Augmenting frozen language models with massive\ntools via tool embeddings. Advances in neural information processing systems, 36, 2024.\n[168] Bender, E. M., A. Koller. Climbing towards nlu: On meaning, form, and understanding in the\nage of data. In Proceedings of the 58th annual meeting of the association for computational\nlinguistics, pages 5185–5198. 2020.\n[169] Bommasani, R., D. A. Hudson, E. Adeli, et al. On the opportunities and risks of foundation\nmodels. arXiv preprint arXiv:2108.07258, 2021.\n[170] Li, Y., Y. Zhang, L. Sun. Metaagents: Simulating interactions of human behaviors for\nllm-based task-oriented coordination via collaborative generative agents. arXiv preprint\narXiv:2310.06500, 2023.\n23\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/A Survey on Large Language Model-Based Game Agents.pdf"}
{"title":"GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data","authors":"Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang","summary":"Multimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is\nexpected to result in slower benchmark saturation, and avoids the illusion of\nemerging abilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision\nLLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption\nbenchmark for evaluating VLLMs, and compare the performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lag behind human performance and struggle\nespecially with text-intensive tasks.","url":"http:\/\/arxiv.org\/abs\/2402.14973v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.14973v4","published":1708636924000,"comment":"Published by Computer Speech & Language\n  (https:\/\/doi.org\/10.1016\/j.csl.2025.101785). Source code and Leaderboard:\n  https:\/\/github.com\/llcresearch\/GenCeption","pdf_text":"GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data\nLele Caoa,b,∗, Valentin Buchnerb,c,∗, Zineb Senaneb,d,e,f, Fangkai Yangd\naMicrosoft Gaming (ABK), Stockholm, Sweden\nbEQT Group (Motherbrain), Stockholm, Sweden\ncChapter Two, Stockholm, Sweden\ndKTH Royal Institute of Technology, Stockholm, Sweden\neT´el´ecom Paris, Palaiseau, France\nfFever Energy, Stockholm, Sweden\nAbstract\nMultimodal Large Language Models (MLLMs) are typically assessed using expensive annotated multimodal bench-\nmarks, which often lag behind the rapidly evolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only unimodal data to measure inter-modality\nsemantic coherence and inversely assesses MLLMs’ tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is expected to result in slower benchmark\nsaturation, and avoids the illusion of emerging abilities. Inspired by the DrawCeption game, GenCeption begins with\na non-textual sample and proceeds through iterative description and generation steps. The semantic drift across iter-\nations is quantified using the GC@T metric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision LLMs (VLLMs). Based on the Gen-\nCeption method, we establish the MMECeption benchmark for evaluating VLLMs, and compare the performance of\nseveral popular VLLMs and human annotators. Our empirical results validate GenCeption’s effectiveness, demonstrat-\ning strong correlations with established VLLM benchmarks. VLLMs still significantly lag behind human performance\nand struggle especially with text-intensive tasks.\nKeywords:\nmultimodal large language model, evaluation, benchmark\n1. Introduction\nLarge Language Models (LLMs) demonstrate exceptional abilities in natural language understanding, reasoning,\nand problem-solving. Multimodal LLMs (MLLMs) enhance these capabilities by incorporating multiple modalities,\nwith the visual modality being predominant and highly commercialized (Achiam et al., 2023; Liu et al., 2023b; Jiang\net al., 2023; Ye et al., 2023). Building on LLMs, MLLMs integrate non-textual modalities, enabling richer interactions\nand broader applications in real-world scenarios. However, there is a lack of comprehensive evaluation methods to\ncompare different MLLM architectures and training approaches (Fu et al., 2023).\nIn response, the community has developed several MLLM benchmarks, as detailed by Xu et al. (2022); Dai et al.\n(2023); Wang et al. (2023); Ye et al. (2023); Li et al. (2023c); Zhao et al. (2023). They primarily focus on the visual\n(i.e., image) and textual input modality due to that VLLMs (Vision LLMs)1 are the most widely used and readily\n∗Equal contribution. Corresponding author: Lele Cao (lelecao@microsoft.com)\nSource code and leaderboard: https:\/\/github.com\/llcresearch\/GenCeption\nThis work was initiated during the author’s tenure at EQT Motherbrain, with significant parts completed independently thereafter. It represents\npersonal research conducted outside the scope of employment responsibilities. Relevant employers have been informed and have provided consent\nfor publication.\n1Vision Large Language Models (VLLMs) are a specialized subclass of Multimodal Large Language Models (MLLMs) designed to integrate\nvisual and textual modalities for tasks such as image captioning, visual question answering, and multimodal reasoning. While VLLMs are generally\ncapable of processing various visual data types, their most common input is images, owing to the abundance of annotated image-text datasets and\nthe maturity of image processing technologies.\nPreprint published by Computer Speech & Language [https:\/\/doi.org\/10.1016\/j.csl.2025.101785]\narXiv:2402.14973v4  [cs.CL]  5 Mar 2025\nPlease write a clear, \nprecise, detailed, and \nconcise description \nof the image ...\nText Prompt: PDesc\nImage \n(or other \nmodality)\nX\nVLLM\n(Vision LLM)\nDescription \nText: Qt\nGenerate an image \naccording to the \nfollowing description:\nImage generation \nprompt (textual): PGen\n(t-1)\n(t)\nIt shows a happy \ndog ...\nIt shows a \nhappy dog ...\nIm age Gener ator  \n(e.g., DALL·E 3)\nX\nGenrated \nImage\n(t)\nReplace           with        \nX\n(t-1)\nX\n(t)\nand start the next \niteration (t+1)\nPGen\n(t)\nGen(        )\nF (         ,          )\nPDesc X\n(t-1)\nFigure 1: An illustration of the t-th iteration in the GenCeption evaluation procedure for VLLMs. Using the image modality as an example, the\nprocess begins with an existing image X(0) sourced from a unimodal image dataset for the first iteration (t=1). The VLLM provides a detailed\ndescription of the image, which is then used by an image generator to produce X(t).\navailable MLLMs on the market. However, these benchmarks face common challenges:\n(1) They predominantly rely on multimodal datasets that demand high-quality annotations, which is costly and\nrestrictive in capturing the evolving capabilities of MLLMs (Fu et al., 2023). This has been shown to result\nin increasing speed in benchmark saturation while contemporary models still struggle on trivial real-world\ntasks (Kiela et al., 2021). Emerging methods like CrossCheckGPT (Sun et al., 2024), designed specifically\nfor MLLM evaluation via cross-system consistency, provide a more relevant, annotation-free alternative. On a\nbroader scope, methods like PRD (Li et al., 2023b) focus on LLM evaluation through peer-based rankings and\nmay be further adapted for MLLM evaluation tasks.\n(2) MLLM evaluation benchmarks that rely on discrete metrics like accuracy may falsely suggest emergent abilities\nand do not allow predictable projections of performance improvements from model scaling (Schaeffer et al.,\n2023).\n(3) The evaluation scores may not reflect true performance on real-world tasks due to potential contamination of\nMLLM training data by benchmark datasets, as reported for LLM pretraining corpora (Dodge et al., 2021; Yang\net al., 2023).\n(4) The content of one modality is often not needed to answer benchmark questions, as the answer can often be\ninferred from the question or the MLLM’s pretraining knowledge.\nAs a consequence of both (3) and (4), some MLLMs can excel on vision QA benchmarks without even being provided\nthe image that is associated with the question. Existing solutions either only tackle a subset of these challenges, or\nfocus on specific tasks such as image captioning (Lee et al., 2024).\nWe propose GenCeption to address all highlighted challenges involved in the evaluation of task-agnostic MLLMs.\nGenCeption is designed to be a general evaluation framework that can be applied across modalities. To validate its\neffectiveness, this paper focuses on Vision LLMs (VLLMs), leveraging the visual modality as an illustrative example.\nGenCeption addresses challenge (1) by relying on easily accessible unimodal datasets, which allows for cost-effective\nand scalable benchmark creation. Relying on unimodal datasets additionally addresses challenge (3) and (4), as it\nallows to easily use previously unseen datasets for MLLM evaluation, and enforces the relevance of the provided\nmodality for excelling at this task. To tackle challenge (2), GenCeption uses the continuous GC@T metric, providing\na more nuanced evaluation compared to discrete metrics, allowing for better projections of performance improvements\nand avoiding the mirage of emergent abilities.\nOn a high and general level, GenCeption assesses MLLMs’ ability to consistently maintain semantic coherence\nacross modalities by iteratively generating and describing non-textual samples and measures the continuous GC@T\nmetric. This approach simultaneously evaluates the MLLM’s tendency to hallucinate, as this inversely correlates with\nsemantic coherence. Further, an MLLM’s ability to provide complete yet concise descriptions of non-textual samples\nmeasures a diverse range of specialized abilities. For instance, to perform well at describing an image using a limited\n2\nnumber of tokens, it is advantageous to be able to reason over people’s emotions and intentions behind their actions,\ninfer the current and preceding weather, count objects, and recognize artistic styles. This list can be extended to\nvarious abilities depending on the non-textual modality and the content of the samples used during the GenCeption\nprocess. The main contributions of this paper are the following:\n• Proposing GenCeption, an evaluation method that principally allows for using unlabeled unimodal datasets for\nMLLM evaluation.\n• Introducing MMECeption, a Vision LLM (VLLM) evaluation benchmark utilizing the GenCeption method.\nMMECeption uses the images from the MME benchmark (Fu et al., 2023), but without their annotated question\nand answer pairs.\n• Evaluating seven leading VLLMs on the MMECeption benchmark and comparing results with other popular\nVLLM benchmarks and human performance.\nWe will elaborate on the proposed implementation of the GenCeption method, detail our experimental setup, and\ndiscuss our findings.\n2. GenCeption\nOur approach, GenCeption, is inspired by a multi-player game DrawCeption2 (a.k.a., Scrawl or Whispernary). In\nthis game, the first player is given an image and describes it verbally to the next player. This player then attempts to\nrecreate the image based on the description. The cycle repeats, often resulting in amusing deviations from the original\nimage. The challenge and objective are to maintain the initial information through iterative transitions between verbal\ndescriptions and drawings. Similarly, a proficient Multimodal Language Model (MLLM), which models multiple\nmodalities such as text and images, should excel at this task. Recognizing that MLLMs can encompass modalities\nbeyond just visual cues, such as audio and graphs, we name our approach GenCeption, covering a broader scope\nthan the visually-centric DrawCeption. For the sake of clarity and alignment with our experiments, we will focus on\nVLLMs in the remainder of this section to walk through the GenCeption approach.\nWhile it may not be possible to preserve the initial information perfectly due to varying levels of richness, accuracy,\nand ambiguity in different modalities, a more capable MLLM will minimize the semantic drift from the original\ninput. This contrasts with common benchmarks that aim for complete saturation, highlighting a key advantage of\nthe GenCeption framework: the creation of benchmarks that are more challenging to saturate. With complex initial\nsamples, such as images of real-world scenes or graphs with numerous nodes and edges, this may even result in\nimpossible-to-saturate benchmarks. Aiming for minimum rather than no semantic drift, this would allow to rank\nMLLMs relative to each other while continuously leaving space for more performant models.\n2.1. Procedure\nUnlike existing MLLM benchmarks (often focused on VLLMs) that rely on multimodal samples, GenCeption\nis designed to operate on unimodal datasets, significantly streamlining dataset acquisition efforts. For illustrative\npurposes, we employ the image modality as a representative non-textual modality throughout this exposition. Let\nus consider an image dataset D comprising images X1, X2, . . . , XN, similar to well-established datasets like Ima-\ngeNet (Deng et al., 2009), CIFAR (Krizhevsky et al., 2009), and STL (Coates et al., 2011). Without loss of generality,\nany image from D is denoted as X.\nGenCeption operates iteratively, from t = 1 to a pre-defined maximum iteration t = T. Each iteration, as depicted\nin Figure 1, begins with an image X(t−1) and yields a new image X(t). The first iteration (t = 1) starts with the original\nimage X(0) from D. During any given iteration t, the VLLM receives a textual prompt PDesc (Table 1), instructing the\nVLLM to generate a comprehensive description Qt for the input image X(t−1):\nQt := F(PDesc, X(t−1)), where F denotes the generation function of any VLLMs.\n(1)\n2https:\/\/wikipedia.org\/wiki\/drawception\n3\nPlease write a clear, precise, detailed, and concise description of all elements in the image. Focus on accurately depicting\nvarious aspects, including but not limited to the colors, shapes, positions, styles, texts and the relationships between dif-\nferent objects and subjects in the image. Your description should be thorough enough to guide a professional in recreating\nthis image solely based on your textual representation. Remember, only include descriptive texts that directly pertain to\nthe contents of the image. You must complete the description using less than 500 words.\nTable 1: The fixed textual prompt PDesc instructs the MLLM to produce a description of the input X(t−1).\nFollowing this, an image generation prompt P(t)\nGen is constructed as “Generate an image that fully and precisely reflects this\ndescription: < Qt >”. This prompt guides a pretrained image generation model, such as DALL·E (Ramesh et al., 2021)\nor Imagen (DeepMind, 2023), to create a new image, X(t):\nX(t) := Gen(P(t)\nGen),\n(2)\nwhere Gen(·) signifies the chosen image generator. Each subsequent iteration t+1 starts with the image X(t) generated\nin the previous iteration. Upon completing all iterations, we obtain a series of T + 1 images: X(0), X(1), . . . , X(T), with\nthe initial image being the original and the rest sequentially produced across the iterations.\nThe textual prompt PDesc is intentionally kept short and concise to minimize potential variations in model be-\nhaviours due to susceptibility to prompt composition (Loya et al., 2023).\n2.2. Metric: GC@T\nOur primary objective is to measure the semantic divergence of each generated image X(t) (for t ∈{1, 2, . . . , T})\nfrom the original image X(0). We use a pretrained image encoder, such as ViT (Dosovitskiy et al., 2021), to transform\nall images, resulting in T +1 image embeddings denoted as z(0), z(1), . . . , z(T), where z(t) := Enc(X(t)). We then compute\nthe cosine similarity between z(0) and each z(t) (for t ∈{1, 2, . . . , T}), yielding T similarity scores: s(1), s(2), . . . , s(T).\nHere, s(t) ∈[−1.0, 1.0] approximates the level of semantic drift in the t-th iteration of the GenCeption procedure.\nTo quantify the overall speed and magnitude of semantic drift, we calculate the GenCeption score over T iterations,\ndenoted as GC@T ∈[−1.0, 1.0], as follows:\nGC@T :=\nPT\nt=1(t · s(t))\nPT\nt=1 t\n.\n(3)\nThis is a normalized and continuous metric that weights later iterations more heavily for two reasons: (1) similar\nto the DrawCeption game, the last image’s deviation from the initial image is most telling; (2) we aim to capture\nperformance and dynamics across the entire iterative sequence. A high GC@T value signifies an exceptional ability\nto maintain inter-modal (text-image) semantic congruence, effectively curbing the propensity for rapid or extensive\ndeviation from the semantics encapsulated in the original image. Notably, GC@1 is equivalent to s(1). For the pseudo\ncode detailing the GenCeption procedure and the calculation of the average GC@T metric over the entire dataset D,\nsee Algorithm 1.\nFor the special case of VLLMs that are evaluated in this study, we additionally replace using ViT embeddings and\ncosine similarity with the Frechet Inception Distance (FID), a metric commonly used to evaluate image generation\nmodels (Heusel et al., 2017). The FID is calculated between the original dataset of images D(0), and the images\ngenerated from the respective dataset using the GenCeption process D(t), yielding T FID scores: fid(1), fid(2), . . . , fid(T).\nThe GCFID@T score is then calculated as:\nGCFID@T :=\nPT\nt=1(t · fid(t))\nPT\nt=1 t\n.\n(4)\nAs the FID indicates a distance rather than a similarity between two sets of images, a lower distance indicates\nbetter performance, and consequently a lower GCFID@T score indicates a more capable VLLM.\n4\nAlgorithm 1: Calculate GC@T via GenCeption for a specific VLLM under evaluation\nInput: VLLM to be evaluated, a unimodal dataset D: X(0)\n1 , . . . , X(0)\nn , . . . , X(0)\nN , fixed textual prompt PDesc, a\nsample generator Gen(·), and a sample encoder Enc(·)\nOutput: Average GC@T metric over D\nParameter: The number of iterations T\n1: GC@T = 0\n2: for (n = 1; n ≤N; n + +) do\n3:\nz(0) := Enc(X(0)\nn );\n4:\nfor (t = 1; t ≤T; t + +) do\n5:\nGenerate description Qt for X(t−1)\nn\nusing (1);\n6:\nCreate P(t)\nGen using Qt;\n7:\nGenerate X(t)\nn according to (2);\n8:\ns(t) := CosineSimilarity(z(0), Enc(X(t)\nn ));\n9:\nend\n10:\nCalculate GC@T += PT\nt=1(t · s(t))\/ PT\nt=1 t; (3)\n11: end\n12: return GC@T \/ N;\n3. Experiments\nWe run several extensive experiments to validate the GenCeption method by comparing the GC@T scores achieved\nby several VLLMs to the scores they achieve on carefully crafted established benchmarks and to average human per-\nformance. Although GenCeption’s design merely requires unimodal image datasets, we employ the same data as\nused by a recent and well-validated MLLM benchmark, MME (Fu et al., 2023). While we discard the annotated\nquestion-answer pairs associated with the images in this benchmark, this provides us with the ability (1) to facilitate\ndirect comparison with metrics that include textual QA (question-answering) annotations, and (2) to enable a detailed\nassessment of MLLM performance across MME’s 14 meticulously crafted sample categories. Attributing this newly\ncreated benchmark to the MME dataset and the GenCeption method, we refer to it as the MMECeption benchmark.\nWe select seven VLLMs – Gemini1.5-Pro (Reid et al., 2024), GPT-4o (OpenAI, 2024) , GPT-4V (Achiam et al.,\n2023), Claude3-Opus (Anthropic, 2023), LLaVA-7B\/13B (Liu et al., 2023b) and mPLUG-Owl2 (Ye et al., 2023)\n– based on their superior performance on the OpenCompass multimodal leaderboard (OpenCompass, 2023), which\nincorporates a comprehensive set of benchmarks like MME (Fu et al., 2023), HallusionBench (Liu et al., 2023a),\nMMStar (Chen et al., 2024), SeedBench (Li et al., 2023a), and AI2D (Kembhavi et al., 2016). We use DALL·E as the\ndefault image generation model. To prevent potential bias towards OpenAI-developed VLLMs, which might have had\naccess to DALL·E-generated images during their training, we perform an additional evaluation of all VLLMs on the\nGC@1 score using Imagen2 as an image generation model. We set the temperature parameter to 0 in both the VLLMs\nand image generators to minimize the stochasticity in model outputs.\nAs humans are well versed at integrating vision and language modalities, we aim to quantify average human\nperformance on the MMECeption benchmark. As the GenCeption procedure is a labor-intensive and time-consuming\ntask for humans, we randomly select 5 images from each MME category, and by providing human annotators with\nthe same prompts as defined in Table 1, collect results and calculate the GC@1 metric. Five human annotators (3\nmaster students, 1 lecturer, and 1 artist) were recruited to describe one image of each category such that each image\nin a category is described by a different person to mitigate personal performance differences. The annotators were\ngiven 14 weeks to perform this task and were awarded a generous reimbursement of €40 each to ensure sufficient\ndedication. All annotators were either native English speakers or fluent at a professional level.\n3.1. Quantitative results\nWe partition the 14 MME categories into two groups based on content type: visual-intensive (10 categories: ex-\nistence, count, position, color, poster, celebrity, scene, landmark, artwork, and commonsense reasoning) and textual-\nintensive (4 categories: code reasoning, numerical calculation, text translation, and OCR). GC@3 scores on the\n5\nSample group\n& category\nGemini1.5-Pro\nClaude3-Opus\nGPT-4o\nGPT-4V\nmPLUG-Owl2\nLLaVA-13B\nLLaVA-7B\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nvisual-intensive samples\nExistence\n190.0\n0.437\n269.8\n183.3\n0.382\n273.7\n195.0\n0.400\n266.5\n175.0\n0.422\n265.1\n185.0\n0.323\n296.9\n195.0\n0.305\n322.0\n195.0\n0.308\n318.1\nCount\n148.3\n0.389\n272.6\n116.7\n0.348\n285.3\n190.0\n0.388\n277.5\n153.3\n0.404\n277.4\n160.0\n0.299\n316.1\n165.0\n0.294\n319.7\n148.3\n0.253\n326.1\nPosition\n105.0\n0.357\n253.7\n76.7\n0.357\n266.1\n145.0\n0.398\n260.6\n85.0\n0.408\n253.3\n75.0\n0.306\n294.0\n135.0\n0.255\n298.9\n123.3\n0.285\n286.0\nColor\n175.0\n0.474\n234.8\n118.3\n0.385\n267.6\n180.0\n0.421\n246.1\n141.7\n0.403\n243.7\n138.3\n0.290\n310.1\n165.0\n0.300\n305.6\n170.0\n0.284\n304.5\nPoster\n175.2\n0.374\n206.0\n149.7\n0.360\n206.0\n192.2\n0.335\n203.7\n187.8\n0.324\n209.4\n154.8\n0.243\n209.2\n163.6\n0.215\n240.9\n154.1\n0.214\n244.3\nCelebrity\n169.4\n0.362\n191.0\n77.6\n0.317\n192.5\n46.8\n0.331\n193.3\n53.5\n0.332\n189.1\n167.9\n0.232\n211.3\n144.4\n0.206\n223.7\n153.2\n0.188\n233.6\nScene\n147.0\n0.423\n173.7\n149.8\n0.374\n174.7\n148.5\n0.401\n171.5\n141.2\n0.393\n173.7\n157.8\n0.299\n194.4\n162.8\n0.277\n198.0\n160.8\n0.266\n196.3\nLandmark\n176.8\n0.375\n182.1\n113.0\n0.344\n188.9\n175.5\n0.372\n182.0\n104.0\n0.353\n182.6\n158.8\n0.275\n206.0\n150.8\n0.242\n224.3\n154.8\n0.252\n214.4\nArtwork\n152.2\n0.412\n171.1\n136.8\n0.385\n170.4\n144.0\n0.415\n169.2\n115.0\n0.421\n170.3\n136.0\n0.252\n202.2\n98.8\n0.212\n213.3\n110.0\n0.210\n215.3\nCommonsense\n150.0\n0.464\n216.5\n115.0\n0.432\n210.4\n174.3\n0.448\n213.9\n155.0\n0.471\n208.1\n127.9\n0.353\n237.2\n115.7\n0.334\n248.9\n117.1\n0.294\n254.4\nVisual Mean\n158.9\n0.407\n217.1\n123.7\n0.368\n223.6\n159.1\n0.391\n218.4\n131.2\n0.393\n217.3\n146.2\n0.287\n247.7\n149.7\n0.264\n259.5\n148.7\n0.255\n259.3\nVisual Rank\n2\n1\n1\n7\n4\n4\n1\n3\n3\n6\n2\n2\n5\n5\n5\n3\n6\n7\n5\n7\n6\ntextual-intensive\nCode reasoning\n117.5\n0.213\n310.0\n70.0\n0.245\n267.4\n182.5\n0.255\n299.7\n147.5\n0.193\n302.9\n65.0\n0.176\n327.6\n55.0\n0.144\n323.5\n50.0\n0.107\n398.2\nNumerical calc.\n110.0\n0.268\n346.5\n67.5\n0.229\n349.3\n170.0\n0.282\n346.4\n80.0\n0.240\n322.5\n45.0\n0.192\n362.0\n35.0\n0.195\n367.4\n50.0\n0.155\n366.0\nText translation\n162.5\n0.240\n334.6\n45.0\n0.236\n362.5\n192.5\n0.211\n326.9\n55.0\n0.157\n368.0\n112.5\n0.081\n365.2\n85.0\n0.116\n352.3\n65.0\n0.111\n424.4\nOCR\n170.0\n0.367\n233.2\n167.5\n0.362\n245.5\n192.5\n0.362\n246.2\n177.5\n0.393\n238.0\n102.5\n0.276\n255.4\n95.0\n0.239\n270.6\n65.0\n0.222\n283.7\nTextual Mean\n140.0\n0.272\n306.1\n87.5\n0.268\n306.2\n184.4\n0.278\n304.8\n115.0\n0.246\n307.9\n81.3\n0.181\n327.6\n67.5\n0.174\n328.5\n57.5\n0.149\n368.1\nTextual Rank\n2\n2\n2\n4\n3\n3\n1\n1\n1\n3\n4\n4\n5\n5\n5\n6\n6\n6\n7\n7\n7\nOverall Mean\n153.5\n0.368\n242.5\n113.3\n0.340\n247.2\n166.3\n0.359\n243.1\n126.5\n0.351\n243.2\n127.6\n0.257\n270.5\n126.1\n0.238\n279.2\n122.6\n0.225\n290.1\nOverall Rank\n2\n1\n1\n7\n4\n4\n1\n2\n2\n4\n3\n3\n3\n5\n5\n5\n6\n6\n6\n7\n7\nHallusionBench*\n45.2 (rank=3)\n37.8 (rank=4)\n51.7 (rank=1)\n46.5 (rank=2)\n25.7 (rank=6)\n24.5 (rank=7)\n27.6 (rank=5)\nMMStar*\n38.6 (rank=5)\n45.7 (rank=3)\n61.6 (rank=1)\n47.7 (rank=2)\n34.8 (rank=6)\n40.1 (rank=4)\n34.6 (rank=7)\nSEEDBench (Test)*\n70.7 (rank=3)\n64.0 (rank=7)\n76.4 (rank=1)\n71.6 (rank=2)\n64.5 (rank=6)\n67.9 (rank=4)\n66.4 (rank=5)\nAI2D*\n70.2 (rank=4)\n70.6 (rank=3)\n82.2 (rank=1)\n75.5 (rank=2)\n55.7 (rank=7)\n61.3 (rank=5)\n55.9 (rank=6)\nOpenCompass*\n62.7 (rank=3)\n57.7 (rank=4)\n66.3 (rank=1)\n63.3 (rank=2)\n46.3 (rank=7)\n48.8 (rank=5)\n46.7 (rank=6)\n* Results are sourced from https:\/\/huggingface.co\/spaces\/opencompass\/open_vlm_leaderboard as of 2024-04-25 (except GPT-4o) and 2024-05-23 (for GPT-4o).\n↓Results are obtained using FID to measure the similarity between images. https:\/\/github.com\/GaParmar\/clean-fid\nTable 2: Evaluation results of GC@3, MME, HallusionBench and OpenCompass on visual(Vis)-intensive and textual(Text)-intensive images. Best results per metric and category (over different\nMLLMs) are bolded.\n6\nFigure 2: Correlation Matrix of GC@1 and GC@3 scores on MMECeption, and several other benchmarks.\nMMECeption benchmark and accuracy on the original MME benchmark are reported per category and as aggregations\nin Table 2. Additionally, we include the scores and ranks of all evaluated VLLMs on the OpenCompass (OpenCom-\npass, 2023), MME (Fu et al., 2023), HallusionBench (Liu et al., 2023a), MMStar (Chen et al., 2024), SeedBench (Li\net al., 2023a), and AI2D (Kembhavi et al., 2016) leaderboards. Notably, Gemini1.5-Pro leads our rankings, followed\nby GPT-4o, GPT-4v, Claude3-Opus, mPLUG-Owl2, and LLaVA-13B\/7B. The GenCeption method shows robustness\nto the similarity metric used, as the overall ranking remains identical when using cosine similarity or FID distance for\ncalculating GC@T scores.\nGC@1\nGC@3\nMME\nHallusionBench\nGC@1 (essentially the same as s(1))\n-\n0.99\n0.46\n0.92\nGC@3\n0.99\n-\n0.52\n0.93\ns(3) (GC@3 without weighting by t)\n0.98\n0.99\n0.46\n0.94\nCrossCheckGPT (Image-to-text)\n0.96\n0.94\n0.42\n0.97\nTable 3: Correlation matrix comparing GC@1, GC@3, s(3) (GC@3 without temporal weighting), and CrossCheckGPT with established bench-\nmarks MME and HallusionBench.\nFigure 2 presents a correlation matrix among GC@T scores and the benchmarks mentioned above, where the\noverall GC@T scores are averaged over the GC@T scores of all MME categories. The strong correlations with the\nOpenCompass scores incorporating the results of multiple meticulously crafted benchmarks indicate that MMECep-\ntion provides a comprehensive evaluation that may complement existing benchmarks. Further, GenCeption appears\nto effectively measure a VLLM’s tendency to hallucinate, as demonstrated by the strong correlations with Hallu-\nsionBench. While these observations are further emphasized by the correlations with MMStar and AI2D, the only\nmoderate correlations with MME and SEEDBench provide more nuanced insights. As MME displays these mod-\nerate correlations also with the other benchmarks, it can be reasoned that it measures dimensions supplement to\nthose measured by other benchmarks and GenCeption. SEEDBench on the other hand correlates strongly with other\nbenchmarks, but only moderately with GC@T scores. This indicates that SEEDBench measures aspects that are also\nmeasured by other benchmarks, but fail to be captured by GenCeption. Future research could focus on identifying\nthese aspects to potentially incorporate them into GenCeption.\nOne of the key strengths of GenCeption lies in its annotation-free evaluation methodology, a concept also reflected\nin emerging evaluation methods such as CrossCheckGPT Sun et al. (2024). CrossCheckGPT ranks hallucinations\nby evaluating the consistency of outputs across independent MLLM models. In 3, we analyze the correlation of\nCrossCheckGPT with GenCeption, MME, and HallusionBench scores. The results show strong correlations between\nCrossCheckGPT and both GenCeption and HallusionBench, affirming its capability to capture key evaluative met-\n7\nSample group\n& category\nGemini1.5-Pro\nClaude3-Opus\nGPT-4o\nGPT-4V\nmPLUG-Owl2\nLLaVA-13B\nLLaVA-7B\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nvisual-intensive samples\nExistence\n0.505\n0.529\n0.500\n0.532\n0.536\n0.521\n0.505\n0.530\n0.427\n0.515\n0.416\n0.485\n0.418\n0.506\nCount\n0.456\n0.489\n0.466\n0.490\n0.456\n0.494\n0.498\n0.506\n0.378\n0.463\n0.408\n0.466\n0.341\n0.416\nPosition\n0.511\n0.491\n0.495\n0.480\n0.469\n0.460\n0.501\n0.473\n0.346\n0.452\n0.359\n0.454\n0.350\n0.402\nColor\n0.545\n0.525\n0.489\n0.501\n0.480\n0.473\n0.506\n0.490\n0.345\n0.471\n0.420\n0.457\n0.318\n0.436\nPoster\n0.455\n0.388\n0.450\n0.381\n0.445\n0.383\n0.444\n0.365\n0.338\n0.357\n0.303\n0.312\n0.305\n0.266\nCelebrity\n0.417\n0.384\n0.424\n0.382\n0.418\n0.373\n0.433\n0.389\n0.319\n0.336\n0.284\n0.317\n0.263\n0.313\nScene\n0.511\n0.490\n0.504\n0.478\n0.482\n0.474\n0.497\n0.474\n0.385\n0.417\n0.355\n0.404\n0.350\n0.392\nLandmark\n0.500\n0.485\n0.460\n0.492\n0.494\n0.479\n0.458\n0.480\n0.363\n0.351\n0.376\n0.357\n0.334\n0.333\nArtwork\n0.494\n0.454\n0.508\n0.461\n0.500\n0.455\n0.504\n0.455\n0.333\n0.385\n0.308\n0.333\n0.294\n0.304\nCommon.\n0.545\n0.531\n0.535\n0.507\n0.562\n0.526\n0.563\n0.535\n0.425\n0.493\n0.429\n0.473\n0.417\n0.458\nVis Mean\n0.494\n0.477\n0.483\n0.470\n0.484\n0.464\n0.491\n0.470\n0.366\n0.424\n0.366\n0.406\n0.339\n0.383\nVis Rank\n1\n1\n4\n2\n3\n4\n2\n2\n5\n5\n5\n6\n7\n7\ntextual-intensive\nCode\n0.364\n0.177\n0.304\n0.180\n0.395\n0.179\n0.333\n0.263\n0.281\n0.100\n0.260\n0.168\n0.186\n0.108\nNumerical\n0.322\n0.417\n0.333\n0.389\n0.366\n0.456\n0.325\n0.383\n0.322\n0.225\n0.336\n0.265\n0.259\n0.222\nText trans.\n0.396\n0.227\n0.356\n0.258\n0.444\n0.277\n0.359\n0.238\n0.173\n0.052\n0.200\n0.118\n0.212\n0.073\nOCR\n0.462\n0.500\n0.486\n0.448\n0.421\n0.441\n0.482\n0.417\n0.358\n0.384\n0.368\n0.385\n0.351\n0.320\nText Mean\n0.386\n0.330\n0.370\n0.319\n0.407\n0.338\n0.375\n0.325\n0.284\n0.190\n0.291\n0.234\n0.252\n0.181\nText Rank\n2\n2\n4\n4\n1\n1\n3\n3\n6\n6\n5\n5\n7\n7\nOverall Mean\n0.463\n0.435\n0.451\n0.427\n0.462\n0.428\n0.458\n0.428\n0.343\n0.357\n0.344\n0.357\n0.314\n0.325\nOverall Rank\n1\n1\n4\n4\n2\n2\n3\n2\n6\n5\n5\n5\n7\n7\nTable 4: The impact of different image encoders, DALL·E 3 (Dalle3) vs. Imagen 2 (Imgn2), on GC@1 score. Best results per configuration and\ncategory (over different VLLMs) are bolded.\nSample group\n& category\nGemini1.5\n-Pro\nClaude3\n-Opus\nGPT-4o\nGPT-4V\nmPLUG\n-Owl2\nLLaVA\n-13B\nLLaVA\n-7B\nHuman\n∆% between\nhuman & best\nvisual-intensive samples\nExistence\n0.5841\n0.4563\n0.5578\n0.5434\n0.3967\n0.3524\n0.3782\n0.6402\n+ 9.6045%\nCount\n0.4140\n0.3799\n0.2725\n0.4882\n0.2364\n0.3535\n0.2038\n0.5476\n+ 12.1671%\nPosition\n0.5546\n0.4959\n0.4086\n0.5639\n0.3527\n0.4285\n0.3899\n0.6409\n+ 13.6549%\nColor\n0.7081\n0.6206\n0.6139\n0.5516\n0.4047\n0.4314\n0.3506\n0.8380\n+ 18.3449%\nPoster\n0.5046\n0.4362\n0.4939\n0.4681\n0.3998\n0.3208\n0.2905\n0.5456\n+ 8.1252%\nCelebrity\n0.4182\n0.3988\n0.4369\n0.4447\n0.3714\n0.2545\n0.2160\n0.4671\n+ 5.0371%\nScene\n0.6080\n0.5828\n0.5229\n0.5919\n0.4842\n0.3906\n0.4057\n0.6236\n+ 2.5658%\nLandmark\n0.4903\n0.4932\n0.5236\n0.5702\n0.3613\n0.4174\n0.3845\n0.6045\n+ 6.0154%\nArtwork\n0.3725\n0.5304\n0.5297\n0.5252\n0.2938\n0.2924\n0.2336\n0.5421\n+ 2.2059%\nCommonsense\n0.4338\n0.5375\n0.5047\n0.4012\n0.3244\n0.4153\n0.3532\n0.6417\n+ 19.3860%\nVisual Mean\n0.5088\n0.4932\n0.4865\n0.5148\n0.3625\n0.3657\n0.3206\n0.6091\n+ 9.7107%\ntext-intensive\nCode reasoning\n0.3689\n0.4085\n0.4043\n0.3690\n0.2923\n0.2963\n0.1975\n0.5376\n+ 31.6034%\nNumerical calc.\n0.3652\n0.3958\n0.3940\n0.4241\n0.3474\n0.4409\n0.3423\n0.5160\n+ 17.0333%\nText translation\n0.4480\n0.3949\n0.4333\n0.3803\n0.0931\n0.2372\n0.1981\n0.6196\n+ 38.3036%\nOCR\n0.4382\n0.4329\n0.3334\n0.4455\n0.2663\n0.3371\n0.2912\n0.4696\n+ 5.4097%\nTextual Mean\n0.4051\n0.4080\n0.3913\n0.4047\n0.2498\n0.3279\n0.2573\n0.5357\n+ 23.0875%\nOverall Mean\n0.4792\n0.4688\n0.4593\n0.4834\n0.3303\n0.3549\n0.3025\n0.5882\n+ 13.5327%\nTable 5: The performance of VLLMs and humans on the GC@1 metric, evaluated using 5 randomly drawn images per sample\/image category. The\nbest performance achieved by an VLLM is underlined.\nrics. Notably, CrossCheckGPT exhibits a weaker correlation with MME, which is likely because the GenCeption\nbenchmark is developed using MME image samples, making it inherently more aligned with the MME framework.\nGC@T scores, as defined in Equation (3), are weighted by a temporal factor t. To examine the impact of this\n8\nweighting, we conducted an ablation study where the weighting mechanism was removed, effectively transforming\nGC@T into s(T). Table 3 demonstrates that s(3) retains a high correlation with GC@3, yet its correlation with MME\ndiminishes compared to the weighted version, while its alignment with HallusionBench remains consistent. Further-\nmore, unweighted scores correlate with MME in a uniform manner across different iterations, whereas the weighted\nscores show a progressive increase in correlation with MME as more iterations are applied. This indicates that tem-\nporal weighting amplifies later iterations’ influence, emphasizing cumulative semantic shifts captured by MME’s\niterative design. Generally, stronger correlation with MME is desirable as it validates the alignment between GenCep-\ntion’s metrics and an established benchmark, reinforcing GenCeption’s ability to assess iterative semantic coherence\neffectively and reliably.\nTable 4 compares GC@1 scores using different image generators, OpenAI’s DALL·E 3 (Ramesh et al., 2021)\nand Google DeepMind’s Imagen2 (DeepMind, 2023). Independent of image generator used, the rankings remain\nunchanged, except that on visual-intensive samples only, Claude3-Opus scores equally with GPT-4V. This provides\nevidence that even though DALL·E 3, GPT-4o, and GPT-4V were developed and trained by OpenAI, neither of\nOpenAI’s models has an advantage over non-OpenAI VLLMs.\nTable 5 shows human performance on a subset of 5 randomly drawn images per category compared to the VLLM\nperformance on the same subset of samples. It can be observed that humans outperform all VLLMs, with especially\nstrong differences in performance for the text-intensive categories. The worst performance, relative to humans, is\nachieved on the code reasoning and text translation categories, the former containing images of code snippets and the\nlatter of phrases written in simplified Chinese characters. The relatively best performance by VLLMs is achieved on\nthe scene and artwork categories, which contain every-day life photos and popular artworks. This demonstrates that\nthere is still substantial space for performance improvement, and that compared to humans, VLLMs still lack relevant\ncompetences. It must be noted that human performance here does not constitute an upper bound in possible scores to\nachieve, and that future generations of VLLMs may well outperform humans.\n3.2. Qualitative Results\nWe qualitatively inspect our results by visualizing generated images together with their cosine similarity and\nGC@T scores for two seed images across different categories, as shown in Figure 3. This visualization highlights\nthe correlation between these scores and the visual characteristics of the images relative to the seed image. A key\nobservation is that later iterations show an increased tendency to produce imagery deviating from the seed image, as\nindicated by lower GC@T scores. This serves as an additional qualitative validation of the GenCeption method and\nthe MMECeption benchmark, as using VLLMs scoring higher on the MMECeption benchmark results in generated\nimages that preserve more information from the seed image. For a wider range of examples across MME image\ncategories and corresponding descriptions from each evaluated VLLM, readers are referred to Appendix A.\n4. Discussion and Future Directions\nThis study validates the GenCeption method with a focus on the visual modality primarily because (1) VLLMs\nare the most widely used and readily available MLLMs on the market, and (2) image generation and embedding\ntools have reached a mature and highly commercialized stage compared to other modalities. However, GenCeption\nis designed to be modality-agnostic. The same iterative procedure (i.e., describing a unimodal sample and then re-\ngenerating it from the description) can, in principle, be applied to other non-text modalities like audio and video.\nThe requirement is that (1) a generation model exists for the given modality, and (2) there is a suitable encoder to\nquantify the similarity between the original sample and the regenerated one. Moreover, recent advancements have\nintroduced multimodal LLMs capable of both generating and interpreting multiple modalities simultaneously, such as\nShow-o (Xie et al., 2024), Emu3 (Wang et al., 2024), and JanusPro (Chen et al., 2025). In these cases, GenCeption\ncould leverage the same MLLM for both description and generation tasks, serving as a particularly valuable approach\nfor directly measuring modality consistency within such unified multimodal systems.\nFuture research is invited to adapt GenCeption to other non-text modalities, such as audio, video, and graphs. For\ninstance, the framework can be initiated with a dataset of audio samples, and MLLMs can iteratively generate and\ndescribe the audio content. Similarly, for video and graph data, the process can involve generating textual descriptions\nof short video clips or graph structures and their recreation. While the core iterative process of GenCeption remains\napplicable, these extensions require careful exploration of modality-specific generation and embedding models.\n9\nSeed Im age\n0.569\n0.362\n0.271\n0.334\n0.057\n0.123\n0.266\n0.194\n0.100\nGC@3=0.351\nGC@1=0.569\nGC@3=0.136\nGC@1=0.334\nGC@3=0.159\nGC@1=0.266\nt=1\nt=2\nt=3\nfrom \n\" Color \"  \ncategory\n0.384\n0.158\n0.260\nGC@3=0.247\nGC@1=0.384\n0.707\n0.484\n0.400\nGC@3=0.477\nGC@1=0.707\n0.428\n0.299\n0.351\nGC@3=0.347\nGC@1=0.428\nGC@3=0.286\nGC@1=0.426\n0.426\n0.312\n0.222\nt=1\nt=2\nt=3\nGC@3=0.296\nGC@1=0.224\n0.224\n0.272\n0.336\nGC@3=0.246\nGC@1=0.430\n0.430\n0.276\n0.164\nSeed Im age\nfrom \n\" OCR\"  \ncategory\nGC@3=0.264\nGC@1=0.382\n0.382\n0.202\n0.266\nGC@3=0.383\nGC@1=0.375\n0.375\n0.394\n0.379\nGC@3=0.276\nGC@1=0.373\n0.373\n0.266\n0.251\nFigure 3: Demonstration of GenCeption evaluation procedure: the images generated over 3 GenCeption iterations for several MLLMs. The\nsimilarity s(t) scores (to the seed image) are shown on the top of images; GC@1 and GC@3 scores are printed on the bottom of the first and third\nimage, respectively.\nThe broad skill assessment provided by GenCeption goes along with the limitation that it is difficult to assess\nwhich skills contribute most to a high GC@T score. Our analysis indicates that contemporary VLLMs perform\npoorly on text-intensive tasks while excelling in describing scenes and artworks. Future research could investigate\nthis in a more fine-grained manner by creating datasets requiring specialized skills. For example, datasets could\ninclude images of complex emotions, dynamic movements, mechanical processes, or user interfaces. Additionally,\ncombining GenCeption with specifically designed similarity metrics may offer more detailed insights into specific\nMLLM abilities.\n5. Conclusion\nIn this paper, we introduce GenCeption to enhance the evaluation of rapidly evolving Multimodal Language\nModels (MLLMs). The GenCeption method attempts to address key limitations of existing MLLM benchmarks,\nsuch as costly data annotation, leading questions, the illusion of emergent abilities, and, as it allows to use newly\ncreated images without annotation, training data contamination. Further, it is expected to result in slower benchmark\nsaturation. Being adaptable to different modalities, the GenCeption method can deliver value as a unified MLLM\nevaluation method that complements existing MLLM benchmarks.\nOur empirical validation using the MMECeption benchmark shows that GenCeption effectively assesses semantic\ncoherence and consistency across modalities, aligning with established VLLM benchmarks. By assessing humans on\nthe MMECeption task, we demonstrate that current VLLMs significantly lag behind human performance, particularly\n10\nwhen working with text-intensive images. Future work is encouraged to refine and extend this framework across a\nwider range of modalities, datasets, and similarity metrics.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al., 2023.\nGPT-4 technical report. arXiv preprint arXiv:2303.08774 .\nAnthropic, 2023. Model card for claude. URL: https:\/\/www-cdn.anthropic.com\/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627\/\nModel_Card_Claude_3.pdf. accessed: [13.05.2024].\nChen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al., 2024. Are we on the right way for evaluating\nlarge vision-language models? arXiv preprint arXiv:2403.20330 .\nChen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., 2025. Janus-Pro: Unified multimodal understanding and generation with\ndata and model scaling. arXiv preprint arXiv:2501.17811.\nCoates, A., Ng, A., Lee, H., 2011. An analysis of single-layer networks in unsupervised feature learning, in: Proceedings of the fourteenth\ninternational conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings. pp. 215–223.\nDai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S., 2023. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint:2305.06500 .\nDeepMind, 2023. Imagegen2. https:\/\/deepmind.google\/technologies\/imagen-2\/. Accessed: [13.05.2024].\nDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. ImageNet: A large-scale hierarchical image database, in: 2009 IEEE conference\non computer vision and pattern recognition, pp. 248–255.\nDodge, J., Sap, M., Marasovi´c, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., Gardner, M., 2021. Documenting large webtext corpora:\nA case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758 .\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,\nUszkoreit, J., Houlsby, N., 2021. An image is worth 16x16 words: Transformers for image recognition at scale, in: International Conference on\nLearning Representations.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al., 2023. MME: A comprehensive evaluation\nbenchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 .\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S., 2017. Gans trained by a two time-scale update rule converge to a local nash\nequilibrium. Advances in neural information processing systems 30.\nJiang, Y., Chan, C., Chen, M., Wang, W., 2023.\nLion: Adversarial distillation of closed-source large language model.\narXiv preprint\narXiv:2305.12870 .\nKembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., Farhadi, A., 2016. A diagram is worth a dozen images, in: Computer Vision–ECCV\n2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, Springer. pp. 235–251.\nKiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., et al., 2021. Dynabench: Rethinking\nbenchmarking in nlp, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 4110–4124.\nKrizhevsky, A., Hinton, G., et al., 2009. Learning multiple layers of features from tiny images. Technical Report. Massachusetts Institute of\nTechnology and New York University.\nLee, Y., Park, I., Kang, M., 2024. FLEUR: An explainable reference-free evaluation metric for image captioning using a large multimodal model,\nin: Ku, L.W., Martins, A., Srikumar, V. (Eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), Association for Computational Linguistics, Bangkok, Thailand. pp. 3732–3746. URL: https:\/\/aclanthology.\norg\/2024.acl-long.205, doi:10.18653\/v1\/2024.acl-long.205.\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y., 2023a. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv\npreprint arXiv:2307.16125 .\nLi, R., Patel, T., Du, X., 2023b. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762 .\nLi, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R., 2023c. Evaluating object hallucination in large vision-language models. arXiv\npreprint:2305.10355 .\nLiu, F., Guan, T., Li, Z., Chen, L., Yacoob, Y., Manocha, D., Zhou, T., 2023a. Hallusionbench: You see what you think? or you think what\nyou see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint\narXiv:2310.14566 .\nLiu, H., Li, C., Li, Y., Lee, Y.J., 2023b. Improved baselines with visual instruction tuning, in: NeurIPS 2023 Workshop on Instruction Tuning and\nInstruction Following.\nLoya, M., Sinha, D.A., Futrell, R., 2023. Exploring the sensitivity of llms’ decision-making capabilities: Insights from prompt variation and\nhyperparameters. arXiv preprint arXiv:2312.17476 .\nOpenAI, 2024. Hello gpt-4o. OpenAI Technical Report Available at https:\/\/openai.com\/index\/hello-gpt-4o\/.\nOpenCompass, 2023.\nOpenCompass: A universal evaluation platform for foundation models.\nhttps:\/\/github.com\/open-compass\/\nopencompass.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I., 2021. Zero-shot text-to-image generation, in: Interna-\ntional Conference on Machine Learning, PMLR. pp. 8821–8831.\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al., 2024.\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 .\nSchaeffer, R., Miranda, B., Koyejo, S., 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004 .\nSun, G., Manakul, P., Liusie, A., Pipatanakul, K., Zhang, C., Woodland, P., Gales, M., 2024. Crosscheckgpt: Universal hallucination ranking for\nmultimodal foundation models. arXiv preprint arXiv:2405.13684 .\n11\nWang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al., 2023. Visionllm: Large language model is also\nan open-ended decoder for vision-centric tasks. arXiv preprint:2305.11175 .\nWang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al., 2024. Emu3: Next-token prediction is all you\nneed. arXiv preprint arXiv:2409.18869.\nXie, J., Mao, W., Bai, Z., Zhang, D.J., Wang, W., Lin, K.Q., Gu, Y., Chen, Z., Yang, Z., Shou, M.Z., 2024. Show-o: One single transformer to\nunify multimodal understanding and generation. arXiv preprint arXiv:2408.12528.\nXu, Z., Shen, Y., Huang, L., 2022. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint:2212.10773 .\nYang, S., Chiang, W.L., Zheng, L., Gonzalez, J.E., Stoica, I., 2023. Rethinking benchmark and contamination for language models with rephrased\nsamples. arXiv preprint arXiv:2311.04850 .\nYe, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J., 2023. mplug-owl2: Revolutionizing multi-modal large language\nmodel with modality collaboration. arXiv preprint:2311.04257 .\nZhao, Y., Pang, T., Du, C., Yang, X., Li, C., Cheung, N.M., Lin, M., 2023. On evaluating adversarial robustness of large vision-language models.\narXiv preprint:2305.16934 .\nAppendix A. GenCeption Demonstration\nTo provide a comprehensive, intuitive and qualitative understanding of the GenCeption procedure and GC@T\nmetric, we illustrate the input, output, intermediate artifacts, similarity scores, and GC@T values throughout the\nGenCeption process. Examples from the visual-intensive and textual-intensive groups are showcased in Figures B.5\nand B.6, respectively. The corresponding seed images and their metadata are presented in Figure A.4.\nVisual-Intensive \nSeed input image X\nExistence category: \n0016.png\n(0)\nTextual-Intensive \nSeed input image X\nCode_r easoning \ncategory: 0016.png\n(0)\nFigure A.4: Example seed images from the visually (Figure B.5) and textually (Figure B.6) intensive groups, along with their associated metadata.\nAppendix B. Dataset and Reproducibility\nIn Sections 1, 2.1, 2.2 and 3 of the main paper, we cite the creators of all artifacts used. Detailed citations can\nbe found in references. The MME dataset is not directly downloadable, and is released for research purposes only\nupon a request from authors to gain access to it. It does not contain any personally identifying information, as the\nquestions regard visual aspects of the images. We followed the guidelines provided by the authors and respected the\nintended terms of use. The specific licenses and terms for the use and distribution of publicly available artifacts can\nbe found in the corresponding original papers or GitHub repositories, as cited. As per this research work and aligning\nwith the MME copyrights, we are not releasing this asset. Regarding the created artifacts, we introduce a new metric\ncalled GC@T, and detail its creation and intended use in Section 2.2 of the main paper. Our study exclusively utilizes\nimages from the MME dataset, omitting textual QA annotations, and generates textual data in the form of English\ndescriptions as part of our methodology. Given the nature of our research centered on quantifying the inter-modality\ncoherence and consistency, we do not apply any data splits. Due to limitations in terms of computational resources,\nthe metrics reported in Table 2 are from a single run.\nIn our study, we adopt several state-of-the-art models to facilitate our experiments, including Gemini1.5-Pro,\nGPT-4o, GPT-4V, Claude3-Opus, LLaVa-13B, LLaVa-7B, and mPLUG-Ow12 for text description generation, ViT\nfor image embedding, and DALL·E 3 and Imagen2 for image generation, adhering to default parameter settings as\n12\ns   =0.24\ns   =0.31\ns   =0.24\ns   =0.33\nt=1\nThe image depicts an indoor scene, primarily \nfeaturing a domestic cat standing on the hood of a \nblack car. The cat is positioned slightly off-center to \nthe left of the frame, facing towards the right with \nits body profile visible. It has a striped coat with a \nmix of gray, black, and white fur, and is wearing a \ncollar with a visible tag. The cat's eyes are wide \nopen, and its ears are perked up, suggesting \nalertness. The car has a glossy finish, reflecting \nsome of the surrounding objects and lights. The \nhood of the car slopes downwards from left to \nright, and the car's headlights are visible at the \nbottom of the frame, with the left headlight \npartially cut off by the edge of the image. In the \nbackground, there is a garage setting with various \nitems. To the left, there is a white cabinet with \nmultiple drawers, and on top of it, there are several \nobjects including a white lamp with a conical shade, a \nred object that could be a toolbox, and boxes with \nvisible text, one of which reads \"flippo.\" There is also \na bicycle partially visible behind the cat, leaning \nagainst the cabinet, with only the handlebars and \npart of the frame shown. To the right of the cat, \n(1)\nQ\n(1)\nX\nThe image depicts a well-organized garage interior with a focus on a classic \ncar and a cat. The scene is illuminated by a strip of fluorescent lighting \nabove, casting a cool, bright light across the space. In the foreground, a \ndomestic cat with grey and black stripes and a white underbelly is perched \natop a white cabinet with a smooth finish. The cat wears a collar and looks \nto the right with a focused gaze. Its tail hangs off the edge of the cabinet, \nsuggesting a relaxed posture. The cabinet is part of a larger storage \nsystem against the wall, featuring drawers and doors with simple round \nknobs. On the wall above the cabinet, a pegboard holds an array of neatly \narranged tools and equipment, including wrenches, screwdrivers, pliers, \nhammers, and other hand tools, all with various handle colors such as blue, \nblack, and wood. There are also shelves with jars, spray bottles, and \ncontainers, some clear and others opaque, likely containing automotive or \nworkshop-related substances. To the left of the cabinet, a bicycle is \npartially visible, leaning against the wall with its front wheel and handlebar \nshowing. A cardboard box labeled \"Flippo\" sits on the cabinet, and a wooden \ncrate is on the floor to the left of the cabinet, both suggesting storage or \nongoing projects. The centerpiece of the image is a portion of a classic \ncar, occupying the right side of the frame. The car's body is a glossy black \nwith chrome accents and a sleek design, indicative of a vintage luxury or \nsports model. The visible wheel has a detailed rim design with a central logo \ncap, and the tire is branded \"MICHELIN.\" The car's front bumper and \nheadlight are partially shown, with the headlight emitting a soft glow, \nreflecting off the polished car surface and the floor. Beneath the car and \nextending into the foreground is a patterned area rug with a geometric \ndesign featuring circles and ellipses in shades of pink and black, providing a \ncontrast to the otherwise industrial setting. In the bottom right corner, \nthere are two metallic cans or containers, one overturned, and a stack of \nwhat appears to be round metal parts or discs. The overall atmosphere of \nthe image is one of order and attention to detail, with a sense of personal \npride in the maintenance of both the car and the workspace. The presence \nof the cat adds a touch of domesticity and life to the otherwise inanimate \nsetting.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image depicts a well-organized garage \ninterior with a vintage car and a cat. The scene is \nilluminated by natural light coming from the right, \ncasting shadows to the left. In the foreground, a \nclassic black car with chrome details takes center \nstage. It has a prominent grille, round headlamps, \nand a shiny bumper. On the car's hood sits a tabby \ncat with a distinctive striped pattern, looking \ndirectly at the viewer with a neutral expression. \nBehind the car, a wall-mounted shelving unit \nspans the width of the image. The shelves are \nstocked with uniformly sized containers, mostly in \nshades of gray and white, creating a \nmonochromatic look. Some containers have labels, \nbut the text is not legible. On the top shelf, there \nare gray storage boxes and a wooden crate with \nvisible text that appears to be upside down. To \nthe right of the shelving unit, a bicycle is mounted \non the wall, and below it, a cardboard box with \nvisible branding sits on the floor. The wall to the \nleft features a pegboard with neatly arranged \ntools, including wrenches and screwdrivers. Above \nthe pegboard, an orange extension cord hangs \nfrom a hook. The cabinetry below the shelving \nconsists of white drawers and cabinets with \nround, black handles. ... ...\n(3)\nQ\n(1)\ns   =0.30\n(2)\n(3)\n(1)\ns   =0.33\n(2)\n(3)\nThe image features a cat standing on \nthe hood of a black car. The cat is \npositioned towards the center of the \nhood, with its front paws on the hood \nand its back paws on the roof. The car \nis parked in a garage, and the cat \nappears to be curious about its \nsurroundings. In the garage, there are \nseveral items scattered around. A \nbicycle is located on the left side of the \ngarage, and a chair is placed near the \ncenter. A few bottles can be seen on \nthe right side of the garage, and a cup \nis located towards the top right corner. \nAdditionally, a book is placed on the left \nside of the garage, and a potted plant \nis situated on the right side.\n(1)\nQ\nThe image features a black BMW car \nparked in a garage. The car is positioned in \nthe center of the garage, with a bicycle \nparked next to it on the left side. There are \nseveral bottles scattered around the \ngarage, with some placed near the car and \nothers near the bicycle. In addition to the \ncar and bicycle, there are two chairs in the \ngarage. One chair is located on the left side, \nwhile the other is on the right side. A dining \ntable can be seen in the background, placed \nnear the right side of the garage. A potted \nplant is also present in the garage, situated \non the left side near the bicycle. The overall \nscene appears to be a cozy and organized \ngarage space.\n(2)\nQ\nThe image features a large, modern, and \nluxurious BMW car parked in a garage. \nThe car is positioned in the center of the \ngarage, surrounded by various bottles \nand wine glasses. There are at least 15 \nwine glasses scattered around the \ngarage, with some placed closer to the \ncar and others further away. The bottles \nare also dispersed throughout the \ngarage, with some near the car and \nothers positioned at different locations. In \naddition to the car and wine glasses, \nthere are two bicycles in the garage. One \nbicycle is located near the left side of the \ngarage, while the other is positioned \ncloser to the right side. The bicycles are \nplaced near the car, creating a sense of \na well-organized and functional garage \nspace.\n(3)\nQ\ns   =0.15\ns   =0.44\n(1)\ns   =0.18\n(2)\n(3)\nThe image features a cat sitting on \ntop of a car. The cat is positioned in \nthe center of the car, with its body \nstretched out, occupying a \nsignificant portion of the vehicle's \nsurface. The car is black, and the \ncat appears to be enjoying its time \non the hood. In the background, \nthere are several bottles scattered \naround, with some placed closer to \nthe left side of the car and others \nfurther away. Additionally, there is a \npotted plant located near the right \nedge of the image, adding a touch of \ngreenery to the scene.\n(1)\nQ\nThe image features a cat lying on the hood \nof a car. The cat is positioned in the center \nof the scene, occupying a significant \nportion of the car's surface. The car is \nparked on a driveway, with a potted plant \nlocated to the left of the car. There are \nnumerous bottles scattered around the \nscene, with some placed near the car and \nothers further away. The bottles vary in \nsize and position, creating a sense of \ndisarray. Additionally, there is a book \nlocated on the right side of the image, \npossibly indicating that someone was \nreading or studying nearby.\n(2)\nQ\nThe image features a gray car parked in a \ndriveway, with a cat sitting on top of it. \nThe cat is positioned in the middle of the \ncar, occupying a significant portion of the \nvehicle's surface. The car is surrounded \nby various objects, including several \npotted plants placed around the driveway. \nThere are three potted plants on the left \nside of the car, with one located near the \nfront, another in the middle, and the third \none further back. On the right side of the \ncar, there are two potted plants, one \ncloser to the front and the other further \nback. Additionally, there are two bottles in \nthe scene, one near the front of the car \nand the other one further back. A book \ncan also be seen on the ground, close to \nthe right side of the car.\n(3)\nQ\ns   =0.31\ns   =0.24\n(2)\n(3)\nThe image features a black car \nwith a cat sitting on its hood. The \ncat is positioned towards the center \nof the car, looking at the camera. \nThe car's hood is slightly curved, \nproviding a comfortable surface for \nthe cat to sit on. In the background, \nthere are two bicycles, one located \non the left side of the car and the \nother on the right side. A chair can \nbe seen in the middle of the scene, \nand a bottle is placed on the left side \nof the car. Additionally, there is a \nbook resting on the left side of the \ncar, and a clock is visible on the \nright side.\n(1)\nQ\nThe image features a black car \nparked in a room, with a cat sitting on \nthe hood of the car. The room also \ncontains a bicycle, a clock on the wall, \nand a chair. There are several bottles \nscattered around the room, with one \nnear the car, another near the \nbicycle, and a few more placed in \ndifferent areas. Additionally, there \nare two books in the room, one near \nthe car and the other closer to the \nbicycle.\n(2)\nQ\nThe image depicts a room with a black \nsports car parked inside, surrounded by \nvarious objects. The car is positioned in \nthe center of the room, with a cat sitting \non the hood. There are two chairs in the \nroom, one located on the left side and the \nother on the right side.  A clock is mounted \non the wall, and a book can be seen placed \non the floor. Additionally, there are several \nbottles scattered throughout the room, \nwith some placed on the floor and others \non surfaces. A bicycle is also present in \nthe room, located on the right side. The \nroom appears to be a mix of a living space \nand a garage, with the car being the main \nfocus. The presence of the cat on the \nhood adds a touch of warmth and liveliness \nto the scene.\n(3)\nQ\n(1)\ns   =0.20\ns   =0.23\ns   =0.31\nt=1\nThe image depicts a domestic scene inside a \ngarage. The primary focus is a tabby cat with a \nstriped gray and black coat, white paws, and a \nyellow collar with a tag. The cat is standing on \nthe hood of a black car, facing slightly to the \nleft. The cars hood is shiny, reflecting the cat \nand the surrounding light.In the background, \nthere is a workbench area with several \nelements:1. A beige cabinet with multiple \ndrawers and a countertop.2. On the \ncountertop, there is a cardboard box labeled \n\"Fragile\" in red text, a red bottle, and a few \nother indistinct items.3. Above the countertop, \nthere is a pegboard with various tools and items \nhanging, including a bicycle tire and a hose.To \nthe left of the workbench, there are two white \nlamps with beige lampshades, one of which is \npartially obscured. Below the lamps, there is a \npurple cloth or towel on the floor.The garage \nfloor is gray and appears to be made of \nconcrete. The lighting in the garage is relatively \ndim, with the primary light source coming from \nthe ceiling, casting shadows and reflections on \nthe cars surface.Overall, the scene is a typical \ngarage setting with a mix of storage, tools, and \na vehicle, with the cat adding a lively element to \nthe otherwise static environment.\n(1)\nQ\n(1)\nX\nThe image depicts a well-organized garage with various elements \narranged meticulously. Foreground: - A black classic car is parked \ninside the garage, positioned slightly to the left. The cars front is \nfacing outward, and its headlights and grille are visible. - A grey \ntabby cat with a yellow collar and a bell is sitting on the cars hood, \nlooking slightly to the right.- On the floor in front of the car, \nthere are several small items: a small cardboard box, a red ball, a \nred bottle, and a white mug with a red stripe. Background: - The \nback wall of the garage is adorned with a pegboard filled with \nvarious tools. The tools include wrenches, screwdrivers, pliers, \nand other hand tools, all neatly hung.- Above the pegboard, there \nare two framed pictures: one of a cat and another of a \nlandscape.- To the right of the pegboard, there is a set of beige \ncabinets with multiple drawers and a countertop. On the \ncountertop, there are several items including a red bottle, a \nyellow bottle, a grey toolbox, and a few other small containers.- \nBelow the countertop, there are more drawers and a small open \nspace where a coiled hose is stored.- To the right of the \ncabinets, there is a large red cylindrical container, possibly a \ntrash can, with a few items stacked on top of it.- In front of the \nred container, there is a cardboard box labeled \"FRAGILE\" in red \ntext.- A pink cloth is draped on the floor near the cardboard box. \nCeiling: - Two industrial-style pendant lights hang from the ceiling, \nilluminating the garage. The light on the left casts a warm glow on \nthe door and the cat. Left Wall: - A bicycle is mounted on the \nwall, positioned above the car.- Below the bicycle, there is a \nframed picture of a tiger. Door: - A beige door with a small window \nis situated in the center of the back wall. The door has a brass \ndoorknob and is closed.The overall scene is well-lit, with a warm, \ninviting atmosphere, and the garage is depicted as clean and \norderly. The cat sitting on the car adds a touch of liveliness to the \notherwise static environment.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image depicts a well-organized garage \nwith a classic car, various tools, and other \nitems. Here is a detailed description: \nBackground and Setting: - The garage has \na concrete floor and a light gray wall.- Two \nlarge, industrial-style pendant lights hang \nfrom the wooden ceiling beams, illuminating \nthe scene.- On the back wall, there are two \nframed tiger portraits, one above the \nother, positioned centrally above the car. \nCentral Element: - A classic black car with a \nwhite racing stripe down the center is \nparked in the middle of the garage. The cars \nfront is facing the viewer.- A gray cat with a \nyellow collar sits on the cars hood, slightly to \nthe right of the center. Tools and Storage: - \nBehind the car, a large pegboard is mounted \non the wall, filled with various tools such as \nwrenches, pliers, and screwdrivers, all \nneatly organized.- To the left of the \npegboard, there is a gray metal tool chest \nwith multiple drawers. On top of the chest, \nthere are several items including a red oil \ncan, a white mug, and a few small bottles.- \nTo the right of the pegboard, there is a tall, \nopen shelving unit with various items such as \na red gas can, a brown box, and other \nmiscellaneous objects. Additional Elements: \n... ...\n(3)\nQ\n(1)\ns   =0.29\n(2)\n(3)\ns   =0.39\ns   =0.29\nt=1\nA tabby cat with short brown and black \nstriped fur and white paws stands on the \nhood of a black car in a cluttered garage. \nThe cat is positioned slightly to the left of \nthe center of the image, with its body \nangled towards the left and its head turned \nto look over its right shoulder. It has a \nyellow collar with a round tag. The car's \nhood reflects a light source, creating a \nbright spot near the center. The garage is \nfilled with various items, including: a tall lamp \nwith a beige shade on a cabinet to the left; \na white plastic storage drawer unit with \nmiscellaneous objects on top; a red and \nblack kettlebell; a cardboard box labeled \n\"Huggies\"; metal shelves holding tools and \nother equipment; a bicycle hanging on the \nwall; and a red fire extinguisher. The floor is \ngray concrete, and a pair of purple slippers \nlies near the front left corner of the car.\n(1)\nQ\n(1)\nX\nA tabby cat with a yellow collar and a gold tag sits on the \nhood of a black vintage car in a cluttered garage. The cat \nis the central focus of the image, positioned slightly to the \nleft with its body angled towards the camera and its head \nturned slightly to the right. Its eyes are wide and alert, \nlooking directly at the viewer. The cat's fur is a mix of \nbrown, black, and orange stripes, with white patches on its \nchest and paws. The car is a classic sedan with chrome \nbumpers and headlights. The hood is long and slopes down \ntowards the front, with the cat perched near the highest \npoint. A wrench, a red toolbox, and a box of spark plugs \nare scattered on the hood around the cat. The car's \ninterior is not visible, but the driver's side door is open, \nrevealing a glimpse of the black leather seat. The garage is \nfilled with an assortment of tools, equipment, and storage \ncontainers. The walls are lined with metal shelves holding \nvarious tools, including wrenches, screwdrivers, and \nhammers. Plastic bins and cardboard boxes are stacked on \nthe shelves and on the floor. A white refrigerator stands \nagainst the wall to the left of the car, and a fire \nextinguisher is mounted on the wall to the right. A single \nhanging lamp with a grey metal shade illuminates the scene, \ncasting a warm glow on the cat and the car. The overall \natmosphere is one of organized chaos, with the cat \nproviding a calm and curious presence amidst the clutter.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nA long-haired calico cat with vibrant green eyes \nsits majestically on the hood of a classic black \ncar, positioned centrally within a cluttered \ngarage. The cat's fur is a striking mix of orange, \nbrown, and black, with white accents around its \neyes and chest. It wears a yellow collar adorned \nwith a small gold tag. Its gaze is directed \nupwards, exuding an air of curiosity and \nconfidence. The car, with its gleaming chrome \ngrille and single exposed headlight, occupies the \nforeground. Its hood is open, revealing a complex \nengine bay. A red toolbox rests on the engine, \naccompanied by an assortment of tools including \nwrenches, pliers, and screwdrivers, scattered \naround the cat. A white rag is casually draped \nover the edge of the hood. The background of \nthe garage is a symphony of organized chaos. \nWooden shelves line the walls, overflowing with \ntools, boxes, and miscellaneous items. A vintage \nmint-green refrigerator stands out, adorned with \nmagnets and a fire extinguisher hanging beside it. \nFluorescent lights illuminate the space from \nabove, casting a warm glow on the scene. The \noverall atmosphere is one of a well-used and loved \nworkspace, where the feline companion adds a \ntouch of charm and personality.\n(3)\nQ\n(1)\ns   =0.46\n(2)\n(3)\ns   =0.33\ns   =0.33\nt=1\nThe image depicts a tabby cat sitting on the hood of \na black car in what appears to be a cluttered \ngarage or workshop. The cat has gray and white \nfur with distinct stripes and markings. It is wearing a \nblue harness and is positioned near the front \nwindshield of the vehicle. The car seems to be an \nolder model, possibly from the 1990s or early \n2000s, based on its styling. The hood is slightly \ndented and shows signs of wear. In the background, \nvarious storage boxes, shelves, and miscellaneous \nitems can be seen, indicating a somewhat \ndisorganized space. On the shelves behind the car, \nthere are several cardboard boxes labeled \"Naylor\" \nin black marker or print. The boxes are stacked \nhaphazardly, with some appearing to be open or \npartially filled. Next to the boxes, there is a red \nplastic storage container and what looks like a \nrolled-up mat or carpet. The lighting in the scene is \nartificial, likely from overhead fluorescent bulbs, \ncasting a slightly harsh and shadowy atmosphere. \nThe concrete floor and metal shelving units \ncontribute to the industrial feel of the setting. The \noverall composition draws the viewer's attention to \nthe cat as the central subject, with its striking \nappearance and unexpected presence on the car \nhood creating a sense of curiosity and whimsy \namidst the cluttered surroundings. The juxtaposition \nof the feline against the mechanical backdrop adds \nan element of contrast and interest to the image.\n(1)\nQ\n(1)\nX\nThe image depicts a cat lounging on the hood of a vintage black \nChevrolet car in what appears to be a cluttered garage or storage \nspace. The cat has a primarily brown and black tabby coat with \nwhite patches on its chest and paws. It is wearing a blue harness \nand lying comfortably on a red toolbox or storage container placed \non the car's hood. The Chevrolet has a classic 1970s design, with \na rectangular front grille and headlights. The car's license plate \nreads \"NAYLJOR\". Various tools, boxes, and other objects are \nscattered around and on top of the vehicle, creating a \ndisorganized, lived-in atmosphere. In the background, there are \nnumerous cardboard boxes stacked on metal shelving units, \nreaching from floor to ceiling. The boxes have handwritten labels \nsuch as \"Collide\", \"Balloon\", and \"Hallo Ween\", suggesting they \ncontain holiday decorations or party supplies. The shelves appear \nto be in a state of disarray, with some boxes haphazardly placed \nor leaning against each other. The garage floor is concrete, and a \ncoiled green hose can be seen in the foreground, along with a pair \nof worn leather shoes or boots. The lighting in the space appears \nto be a mix of natural light filtering in from an unseen source and \nartificial overhead lighting, casting shadows and highlighting the \ntextures of the various objects. Overall, the image conveys a \nsense of organized chaos, with the cat serving as a focal point \namidst the clutter. The juxtaposition of the sleek, classic car and \nthe jumbled storage space creates an intriguing visual contrast, \ninviting the viewer to explore the details and imagine the stories \nbehind the eclectic collection of objects.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nIn this whimsical image, a calico cat sits atop the \nhood of a vintage Chevrolet car inside what \nappears to be a cluttered garage or workshop. \nThe cat, with its distinctive orange, black and white \nfur pattern, is wearing a blue harness and looks \ndirectly at the camera. The car is a light gray \n1970s model Chevrolet, possibly an Impala or \nCaprice, with chrome bumpers and trim. Its hood \nornament is visible. In front of the car are a red \ntoolbox, a green coiled hose, and work boots on the \nconcrete floor. The garage walls are covered in \nweathered wooden shelves holding an array of \nboxes, cans and miscellaneous items. Handwritten \nsigns on cardboard label the contents of the \nshelves, with words like \"Bulbs\", \"Canvas\", \n\"Globes\", \"Tape\" and \"Balloons\". A fluorescent \nshop light illuminates the scene from above. The \noverall color palette is muted, dominated by the \nbrowns and grays of the wood and concrete, \npunctuated by the cat's vibrant fur and small spots \nof color from the objects on the shelves and floor. \nThe composition draws the eye to the cat as the \ncentral subject, with the lines of the car and \nshelves leading toward it. The densely packed \nbackground creates a sense of organized chaos \nthat contrasts with the cat calmly perched on the \nclean lines of the classic car.\n(3)\nQ\n(1)\ns   =0.30\n(2)\n(3)\nthere is a cluttered workbench with indistinct items, and above it, there is a pegboard with hanging tools, although \nthe details of the tools are not clear. The floor of the garage is concrete, and there is a pink mat with circular \npatterns partially visible in the lower left corner of the image. The lighting in the scene appears to be artificial, \nwith the source likely coming from the left side, as indicated by the shadows and the illuminated areas on the cat \nand the car. The overall ambiance of the image is that of a typical home garage with a mix of utility and disarray.\nt=1\n(1)\nX\n(2)\nX\n(3)\nX\nt=2\nt=3\nt=1\n(1)\nX\n(2)\nX\n(3)\nX\nt=2\nt=3\nt=1\n(1)\nX\n(2)\nX\n(3)\nX\nt=2\nt=3\nFigure B.5: Illustration of a 3-iteration GenCeption procedure run on a visual-intensive image (from “existence” category) to evaluate 7 VLLMs.\nEach iteration t shows the generated image X(t), the description Q(t) of the preceding image X(t−1), and the similarity score s(t) relative to X(0).\nThe GC@3 metric for each VLLM is also presented. Hallucinated elements within descriptions Q(1) and Q(2) as compared to the seed image are\nindicated with red underlining.\n13\ns   =0.11\ns   =0.22\nt=1\nThe image displays a snippet of computer code against a \nwhite background. The code is written in a programming \nlanguage that resembles Python, based on its syntax and \nstructure. At the top of the image, there is a variable \nassignment statement with the syntax \"x=5\". The variable \n\"x\" is assigned the integer value 5. The text is in a plain font, \nwith the variable \"x\" and the integer \"5\" both colored in \npurple. Below the variable assignment, there is a conditional \nstatement starting with \"if x>10:\". The \"if\" keyword is in bold \nand colored in blue, the variable \"x\" is in purple, the \ngreater-than symbol (>) is in black, and the integer \"10\" is in \npurple, followed by a colon in black. The condition checks if \nthe value of \"x\" is greater than 10. Indented beneath the \n\"if\" statement, there is a \"print\" function call with the syntax \n\"print(\\'x is larger than 10\\')\". The \"print\" keyword is in bold \nand blue, the parentheses are in black, and the string inside \nthe parentheses is in red, enclosed in single quotes. Following \nthe \"if\" block, there is an \"else:\" statement, aligned with the \n\"if\" statement above it. The \"else\" keyword is in bold and \nblue, followed by a colon in black. Indented beneath the \"else\" \nstatement, there is another \"print\" function call with the \nsyntax \"print(\\'x is smaller than 10\\')\". The structure is \nidentical to the previous \"print\" function call, with the \"print\" \nkeyword in bold and blue, the parentheses in black, and the \nstring inside the parentheses in red, enclosed in single quotes. \nThe text of the string differs, stating \\'x is smaller than \n(1)\nQ\n(1)\nX\nThe image depicts a stylized representation of a code snippet \nwithin a rounded rectangle that resembles a smartphone or a \ntablet. The background of this rectangle is a deep blue color, and \nit has a lighter blue border with a darker blue outline. The top of \nthe rectangle features a simplified representation of a device\\'s \ninterface with three circular \"buttons\": one red on the left, one \nblue in the middle, and one purple on the right. These buttons are \naligned horizontally and are placed near the top edge within the \nlighter blue border. The main body of the image contains text and \nsymbols that mimic the syntax and structure of a programming \nlanguage, primarily in white with some elements in pink, purple, and \nred. The text is arranged to resemble a block of code with \nconditional statements and is not syntactically correct in any real \nprogramming language. At the top, in the center of the blue \nbackground, is the word \"IF\" in white, followed by a red \"-0\". \nBelow this, the code snippet starts with \"IF X 1:10;X\" in white, \nwhere \"X\" is larger and in pink, and \"1:10;\" is in purple. Each line of \nthe code block is preceded by a pink \">\", and the lines are indented \nat various levels to suggest nested conditions or loops. The first line \nof code reads \"print\\'s x Larger Than-----\" with \"print\" in white \nand the rest in pink, followed by a rightward arrow in white. The \nsecond line has a red \"X\" followed by \"1.5<2:X X\" with \"1.5<2:\" in \npurple and the \"X\"s in pink. The third line reads \"\\\\\"-x\" is Large \nThAn 1\/\" with \"-x\" in red and the rest in pink, followed by a \nrightward arrow in white. The fourth line has a white \"(-)\" followed \n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image depicts a well-organized garage \nwith a classic car, various tools, and other \nitems. Here is a detailed description: \nBackground and Setting: - The garage has a \nconcrete floor and a light gray wall.- Two \nlarge, industrial-style pendant lights hang \nfrom the wooden ceiling beams, illuminating the \nscene.- On the back wall, there are two \nframed tiger portraits, one above the other, \npositioned centrally above the car. Central \nElement: - A classic black car with a white \nracing stripe down the center is parked in the \nmiddle of the garage. The cars front is facing \nthe viewer.- A gray cat with a yellow collar \nsits on the cars hood, slightly to the right of \nthe center. Tools and Storage: - Behind the \ncar, a large pegboard is mounted on the wall, \nfilled with various tools such as wrenches, \npliers, and screwdrivers, all neatly organized.- \nTo the left of the pegboard, there is a gray \nmetal tool chest with multiple drawers. On top \nof the chest, there are several items including \na red oil can, a white mug, and a few small \nbottles.- To the right of the pegboard, there \nis a tall, open shelving unit with various items \nsuch as a red gas can, ... ...\n(3)\nQ\n(1)\ns   =0.22\n(2)\n(3)\n10\\'. The indentation for the \"print\" statements under both the \"if\" and \"else\" blocks suggests a block structure typical in \nPython, where the indented code is executed based on the condition of the \"if\" statement. The font used for the code \nappears to be a monospaced font, which is commonly used for programming to ensure that each character occupies the \nsame amount of horizontal space, aiding in the readability of the code. The image does not contain any additional elements or \ndecorations outside the code snippet. There are no line numbers or any other annotations. The simplicity of the image \nfocuses the viewer\\'s attention solely on the code itself.\nby \"print LaRG6EX\" with \"print\" in white and \"LaRG6EX\" in pink, followed by a rightward arrow in white. The fifth line reads \n\"(xnt)\\\\\"is Lang ThAn s(>1)\" with \"(xnt)\" in red, \"\\\\\"is Lang ThAn s\" in pink, and \"(>1)\" in purple. The sixth line has a red \"X\" \nfollowed by \"1.0<6:X X\" with \"1.0<6:\" in purple and the \"X\"s in pink. The final line at the bottom reads \"(else: \\'s XLESSE',\ns   =0.33\ns   =0.25\nt=1\nThe image displays a code snippet written in a \nprogramming language, likely Python. The code \nis presented against a white background. The \ncode begins by assigning the value 5 to the \nvariable x. This line is not indented, indicating it \nis outside any control structure. Next, there is \nan if statement that checks the condition x > \n10. If x is greater than 10, the code block \nunder the if statement will be executed. This \nblock consists of a single line, indented by 4 \nspaces, which calls the print() function with \nthe string argument 'x is larger than 10'. \nAfter the if block, there is an else statement. \nThe code block under the else statement will \nbe executed if the condition x > 10 is not met \n(i.e., x is less than or equal to 10). This block \nalso consists of a single line, indented by 4 \nspaces, which calls the print() function with \nthe string argument 'x is smaller than 10'. The \ncode snippet demonstrates a basic conditional \nflow control using an if-else statement, where \ndifferent actions are taken based on whether \n(1)\nQ\n(1)\nX\nThe image displays what appears to be a snippet of code or \nprogramming language, likely Lisp or a similar dialect. The \ncode is presented against a gradient background that \ntransitions from dark blue on the left to a lighter teal color \non the right side of the rectangular frame. The code itself \nis color-coded, with different elements displayed in distinct \nhues for readability. The majority of the text is in white, \nwhile certain elements like parentheses, mathematical \noperators, and specific commands are highlighted in colors \nsuch as light blue, pink, red, and yellow. The code snippet \ncontains several lines, each starting with a combination of \nletters, numbers and symbols. Some notable elements \ninclude: - Variables or function names like \"efe\", \"ette\", \n\"excte\", \"sxcrcrete\" - Mathematical operations and \ncomparisons, e.g., \"i = 5\", \"x xr\", \"1 - 1 - > -'e\" - Conditional \nstatements or function calls like \"r e |-|-t\", \"l = Le:e e r\" - \nPrint or output commands such as \"print\", \"prrtean\" - \nLoops or iterative structures, e.g., \"x *\\'er\", \"ess; 4 \nspaest: ndedt:\" The code appears to be indented using \nspaces to denote hierarchy and scope, a common practice in \n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image depicts a close-up view of what appears \nto be a section of computer code displayed on a \nscreen with rounded corners, resembling a mobile \ndevice or tablet interface. The screen has a dark, \nslightly purplish background gradient. The code is \nwritten in a monospaced font, with keywords and \nspecial characters highlighted in various colors \nagainst the dark background. The text is primarily in \nshades of pink, blue, green and white. The code \nsnippet seems to be written in a programming \nlanguage, likely a functional or declarative language \nbased on the syntax. It includes function definitions, \nvariable assignments, list manipulations, and \nconditional statements. Some notable elements in the \ncode include: - Variable assignments using syntax like \n\"s -variables, ...\" - Function definitions like \n\"gmarattonaes ))\", \"cs)--. ...\"  - List manipulations \nwith syntax like \"c se!)(dis.\" and \"ss .<(',\" - \nConditional statements using \"?\", \":-\" and \"=\" \noperators - Mathematical operations like \"-)\", \"+)\", \n\"-(\" and \"=-\" - Special characters and keywords like \n\"!\", \"@\", \"$\", \"->\", \":-\", \"=\", \"?\", \".\", \",\", \"()\", \"[]\" \nThe code snippet is quite dense and fills up the entire \nvisible screen area. The use of bright colors for \nsyntax highlighting against the dark background \ncreates a visually striking, almost neon-like contrast \ntypical of many code editor color schemes. ... ...\n(3)\nQ\n(1)\ns   =0.45\n(2)\n(3)\nmany programming languages. The rectangular frame featuring the code has rounded corners and a thin, \ndark blue outline. The entire image has a slight shadow effect, giving it a sense of depth against the purple \nbackground.\ns   =0.02\ns   =0.05\nt=1\nThe image displays a Python code snippet \nconsisting of four lines against a white \nbackground. The first line, \"x=5\", is written in \ngreen font and assigns the integer value 5 to \nthe variable x.The second line, \"if x>10:\", is \nwritten in a dark green font and initiates a \nconditional statement, checking if the value of x \nis greater than 10.The third line, \"print('x is \nlarger than 10')\", is indented and written in a \nmaroon font. This line is part of the if \nstatement's body and will only execute if the \ncondition in the second line is true. It prints the \nstring \"x is larger than 10\" to the console.The \nfourth line, \"else:\", is written in a dark green \nfont and marks the beginning of the else block, \nwhich executes only if the condition in the if \nstatement is false.The fifth line, \"print('x is \nsmaller than 10')\", is indented and written in a \nmaroon font. This line is part of the else block \nand prints the string \"x is smaller than 10\" to \nthe console since the condition in the if \nstatement (x > 10) is not true for x = 5.\n(1)\nQ\n(1)\nX\nThe image presents a block of pseudocode \nresembling the Python programming language, set \nagainst a light background. The code is displayed \nwithin a rectangular region with a dark green fill and \nrounded corners. The text \"X-X->110\" is positioned \nat the top center of the rectangle in a bold, white, \nsans-serif font. Below this title, the pseudocode is \nstructured with indentation to indicate code blocks. \nThe code primarily uses a light green color, with \nspecific elements highlighted in white. The first line of \ncode defines a variable using the syntax \"- = (:tangle, \ntas(\". The subsequent lines involve conditional \nstatements and variable assignments, utilizing \nkeywords like \"if\", \"else\", and comparisons such as \n\"larger than\".  Several lines are partially obscured by \nblack rectangles, suggesting redacted or hidden \ninformation. The final line of code, \"X x is latger than \n10)\", is positioned outside the main code block and is \nentirely in white. The overall style of the image evokes \na technical or programming-related theme, with the \ncolor scheme and font choices contributing to a digital \naesthetic.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image presents a three-dimensional cube \ncomposed of green computer code against a lighter \ngreen background. The code is written in a \nmonospaced font, typical of programming \nenvironments. The cube's edges are slightly \nblurred, giving it a soft, almost ethereal \nappearance. The code itself is dense and complex, \nfeaturing a mix of letters, numbers, and symbols. \nParentheses and colons are used extensively, \nsuggesting a structured and hierarchical \norganization of the code. Words like \"angle,\" \n\"triangle,\" and \"circle\" hint at possible geometric \ncalculations or graphical operations. Other terms \nlike \"color,\" \"text,\" and \"image\" suggest potential \nimage processing or display functionalities.The code \nis predominantly green, but certain sections are \nhighlighted in red. These red sections appear to be \nvariable names or function calls, possibly indicating \npoints of interest or specific instructions within the \ncode. The contrast between green and red draws \nthe viewer's attention to these elements, \nsuggesting their importance within the overall \nstructure.The cube's perspective is such that the \nviewer is looking slightly down upon it, with the top \nface angled towards the front. ... ...\n(3)\nQ\n(1)\ns   =0.07\n(2)\n(3)\ns   =0.14\ns   =0.16\nt=1\nThe image displays a snippet of Python code. The \ncode is formatted with syntax highlighting, where \ndifferent elements of the code are colored \ndifferently for clarity.1. Variable Assignment: - The \nfirst line of the code is `x=5`. - The variable `x` is \nassigned the value `5`. - The text `x=5` is colored in \ngreen.2. Conditional Statement: - The second line \nstarts with the keyword `if`, followed by the condition \n`x>10`. - The `if` keyword is colored in purple. - The \ncondition `x>10` is colored in green. - The line ends \nwith a colon `:`, which is also colored in green.3. \nIndented Block for `if` Statement: - The third line is \nindented and contains the `print` function: `print('x is \nlarger than 10')`. - The `print` keyword is colored in \nblue. - The string `'x is larger than 10'` is enclosed in \nsingle quotes and is colored in red.4. Else Statement: \n- The fourth line starts with the keyword `else`, \nfollowed by a colon `:`. - The `else` keyword is colored \nin purple. - The colon `:` is colored in green.5. \nIndented Block for `else` Statement: - The fifth line \nis indented and contains the `print` function: `print('x \nis smaller than 10')`. - The `print` keyword is colored \nin blue. - The string `'x is smaller than 10'` is enclosed \nin single quotes and is colored in red.The code \n(1)\nQ\n(1)\nX\nThe image features a large, stylized letter \"X\" at its center, \ncomposed of vibrant, overlapping, and interlocking shapes. The \n\"X\" is primarily made up of bold, neon colors including green, blue, \npurple, and pink, with a gradient effect that transitions smoothly \nbetween these colors. The shapes forming the \"X\" are thick and \nrounded, giving it a modern and dynamic appearance.Surrounding \nthe \"X\" are numerous lines of text and symbols, resembling a mix \nof programming code and mathematical expressions. The text is \nwritten in various colors, including black, red, green, blue, and \npurple, and is distributed evenly around the \"X\". The text includes \na variety of characters such as letters, numbers, parentheses, \nbrackets, and mathematical operators.Key elements of the text \ninclude: - Top left: \"exee?xs?>(fix)-1?))ex x(ffxi-(efint:3))x\" - Top \nright: \"iff-)xx?2((=x(>25)5))\" - Center left: \"dhtp:(jxx \nys)is.)x0)( elestesex)w())\" - Center right: \"xy?)(efnt) \n(elese))(x)?\" - Bottom left: \"erdtrt,>) (p:yy)1st??\" - Bottom right: \n\"ncitteter?(ftx 10 x.? prittere:((les:e:S\"There are also several \nstandalone symbols and expressions scattered around the \"X\": - \nTop left corner: \"xt?3\" - Bottom left corner: \"oo>>==\" - Bottom \nright corner: \"exrt:-x?)\" - Center left: \"pbit:=))\" - Center right: \n\"iht(>)\"The overall style of the image is highly graphical and \nabstract, with a strong emphasis on bright, contrasting colors and \na playful, almost chaotic arrangement of text and symbols. The \nbackground is a plain, light color, which helps to highlight the vivid \ncolors and intricate details of the \"X\" and surrounding text. The \nimage combines elements of digital art, typography, and coding, \ncreating a visually engaging and complex composition.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image features a large, stylized letter \n\"X\" at its center, composed of multiple \nlayers and vibrant colors. The \"X\" is \nconstructed from a series of overlapping, \nrounded rectangular shapes that create a \nthree-dimensional effect. Each layer of the \n\"X\" is a different color, transitioning \nsmoothly from one to another, including \nshades of green, blue, purple, pink, and red. \nThe colors are bright and saturated, giving \nthe \"X\" a dynamic and eye-catching \nappearance.The background of the image is \na light, muted green, which contrasts with \nthe vivid colors of the \"X.\" Surrounding the \n\"X\" are numerous small geometric shapes \nand symbols, including circles, squares, \nrectangles, and lines, in various colors that \nmatch the palette of the \"X.\" These shapes \nare densely packed around the \"X,\" creating \na complex and intricate pattern that extends \noutward from the center.The geometric \nshapes and symbols are arranged in a \nseemingly random yet harmonious manner, \nwith some shapes overlapping and others \nspaced apart. ... ...\n(3)\nQ\n(1)\ns   =0.31\n(2)\n(3)\nstructure is as follows: - The variable `x` is assigned the value `5`. - An `if` statement checks if `x` is \ngreater than `10`. - If true, it prints `x is larger than 10`. - If false, it prints `x is smaller than 10`.The \nindentation is consistent, with the `print` statements indented to indicate they are within the `if` and `else` \nblocks, respectively. The syntax highlighting helps differentiate between keywords, variables, and strings.\ns   =0.08\ns   =0.17\nt=1\nThe image features a white background \nwith a code snippet displayed on it. The \ncode snippet is written in a programming \nlanguage, and it appears to be a \ncomparison of two variables. The first \nvariable is compared to 10, while the \nsecond variable is compared to 10. The \ncomparison is done using the \"if\" \nstatement, which is represented by the \n\"if\" keyword. The code snippet also \nincludes the \"print\" function, which is \nused to output the results of the \ncomparison. The \"print\" function is \nrepresented by the \"print\" keyword. \nThe comparison is performed using the \n\"<\" operator, which is represented by \nthe \"<\" symbol. The code snippet is \nwritten in a clear and concise manner, \nmaking it easy to understand.\n(1)\nQ\n(1)\nX\nThe image features a white and \nblack computer screen \ndisplaying a code snippet. The \ncode snippet is written in a \nprogramming language, and it \nappears to be a conditional \nstatement. The statement is \nchecking if a variable is less \nthan 10, and if so, it returns a \nspecific value. The code is \nwritten in a clear and concise \nmanner, making it easy to \nunderstand.\n(2)\nX\n(3)\nX\nt=2\nt=3\n(2)\nQ\nThe image features a black tablet with a \nwhite screen displaying a computer \nprogram. The program appears to be a \nlanguage conversion program, as indicated \nby the text on the screen. The text is \nwritten in a programming language, with \nvarious instructions and codes \ndisplayed.\\nThe program seems to be in \nthe process of converting a string of \ncharacters. The string is displayed in the \nmiddle of the screen, occupying a \nsignificant portion of the space. The \ncharacters are arranged in a line, with \nsome characters appearing more \nfrequently than others.\\nThe tablet is \npositioned on a surface, possibly a desk, \nwith the screen facing the viewer. The \nprogram appears to be in use, as the \ncharacters are being converted and \ndisplayed on the screen.\n(3)\nQ\n(1)\ns   =0.19\n(2)\n(3)\ns   =0.07\ns   =0.01\n(1)\ns   =0.05\n(2)\n(3)\nThe image features a white \nbackground with two red \ntext boxes placed side by \nside. The first text box \nreads \"box is larger,\" while \nthe second text box reads \n\"box is smaller.\" The two \ntext boxes are positioned in \nsuch a way that they are \nclearly visible and easy to \nread.\n(1)\nQ\nThe image features a red box with the \nwords \"BOX IS LARGER\" written on it. \nThe box is placed on a white \nbackground, making it stand out \nprominently. The text is in bold and \ncapitalized, emphasizing the message. \nThe box itself appears to be a simple, \nrectangular shape with a flat base and \nsides. The contrast between the red \nbox and the white background further \nhighlights the message and draws \nattention to the box\\'s size.\n(2)\nQ\nThe image features a red box \nwith the words \"Box is Lager\" \nwritten on it. The box is placed on \na white background, making the \nred color stand out. The text is in \nbold and white, emphasizing the \nmessage. The box appears to be \na square, with its sides and \ncorners forming a perfect \nrectangle. The overall \ncomposition of the image is simple \nand straightforward, focusing on \nthe message conveyed by the \nbox.\n(3)\nQ\ns   =0.03\ns   =0.11\n(2)\n(3)\nThe image displays a red text \nbox with two sentences \nwritten in it. The sentences \nare written in a contrasting \ncolor, making them stand out \nagainst the red background. \nThe sentences are placed in \nthe middle of the image, with \none sentence positioned slightly \nhigher than the other. The \ntext appears to be written in a \nmodern, stylish font, which \nadds to the overall visual \nappeal of the image.\n(1)\nQ\nThe image features a red square with \na collection of letters and numbers \narranged in a visually appealing manner. \nThe letters are written in a variety of \ncolors, including red, white, and black, \nwhile the numbers are displayed in a mix \nof red and white. The arrangement of \nthe letters and numbers creates a \nsense of depth and complexity, making \nthe image visually engaging. The red \nsquare serves as a backdrop for the \nintricate arrangement of letters and \nnumbers, drawing attention to the \nartistic display.\n(2)\nQ\nThe image features a large, \nintricate puzzle made up of various \ncolored blocks. The puzzle is \npredominantly red, with some black \nand white elements. The puzzle is \nplaced on a red background, which \nadds to the overall visual appeal of \nthe scene.\\n\\nThe puzzle is \ncomposed of numerous small \nsquares, which are arranged in a \ncomplex pattern. The individual \nblocks are of different shapes and \nsizes, creating a visually engaging \nand challenging puzzle. The puzzle is \ndisplayed prominently in the image, \ndrawing the viewer's attention to its \nintricate design.\n(3)\nQ\n(1)\ns   =0.17\nt=1\n(1)\nX\n(2)\nX\n(3)\nX\nt=2\nt=3\nt=1\n(1)\nX\n(2)\nX\n(3)\nX\nt=2\nt=3\nthe variable x is greater than 10 or not. The code is displayed in a monospaced font, with syntax \nhighlighting. The if and else keywords are highlighted in a purple color, the print keyword is highlighted \nin blue, and the string arguments are highlighted in green. The indentation is clearly visible, aiding in \nunderstanding the structure of the code.\nFigure B.6: Illustration of a 3-iteration GenCeption procedure run on a textual-intensive image (from “code reasoning” category) to evaluate 7\nVLLMs. Each iteration t shows the generated image X(t), the description Q(t) of the preceding image X(t−1), and the similarity score s(t) relative\nto X(0). The GC@3 metric for each VLLM is also presented. Hallucinated elements within descriptions Q(1) as compared to the seed image are\nindicated with red underlining.\n14\noutlined in their original specifications. As we only evaluated existing models but did not train new models, no\nhyperparamter tuning was applicaple. The text descriptions generated by GPT-4V\/4o, Claude3 and Gemini1.5 are\nobtained through API calls, while experiments involving the other models are conducted on A100 GPUs, totaling\napproximately 96 GPU hours. Image generation was also performed via a call to OpenAI’s DALL-E 3 API. To\ncompute the GC@T metric, we employ the cosine similarity metric from the Scikit-learn library (Version 1.4.0).\nAppendix C. Human Annotators\nAs described in Section 3, we benchmarked the performance of humans at the GC@T task. The 5 human anno-\ntators were recruited from the authors’ social circle and were being made aware that they contributed to a research\nproject, with the specific goal of the research project only being disclosed after participation. They were provided\nwith the same instruction prompt as the MLLMs, and given 14 weeks to complete the task. This task does not in-\nvolve sharing any personal information and the images were carefully evaluated to not be offensive in any way. The\nparticipants gave consent that their annotations could be used for scientific research and included in research papers.\nAppendix D. AI Assistants\nAI assistants like GitHub Copilot, ChatGPT, and Perplexity were used to support writing the necessary codebase\nand find efficient ways to express complex concepts. AI assistant suggestions were always carefully evaluated for\ncorrectness and only used after human revision.\n15\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data.pdf"}
{"title":"PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain","authors":"Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang","summary":"We present PCA-Bench, a multimodal decision-making benchmark for evaluating\nthe integrated capabilities of Multimodal Large Language Models (MLLMs).\nDeparting from previous benchmarks focusing on simplistic tasks and individual\nmodel capability, PCA-Bench introduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. Given task instructions and\ndiverse contexts, the model is required to seamlessly integrate multiple\ncapabilities of Perception, Cognition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench features error localization\ncapabilities, scrutinizing model inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the reliability of deploying MLLMs. To\nbalance accuracy and efficiency in evaluation, we propose PCA-Eval, an\nautomatic evaluation protocol, and assess 10 prevalent MLLMs. The results\nreveal significant performance disparities between open-source models and\npowerful proprietary models like GPT-4 Vision. To address this, we introduce\nEmbodied-Instruction-Evolution (EIE), an automatic framework for synthesizing\ninstruction tuning examples in multimodal embodied environments. EIE generates\n7,510 training examples in PCA-Bench and enhances the performance of\nopen-source MLLMs, occasionally surpassing GPT-4 Vision (+3\\% in decision\naccuracy), thereby validating the effectiveness of EIE. Our findings suggest\nthat robust MLLMs like GPT4-Vision show promise for decision-making in embodied\nagents, opening new avenues for MLLM research.","url":"http:\/\/arxiv.org\/abs\/2402.15527v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.15527v1","published":1708499398000,"comment":"Code and Data released at https:\/\/github.com\/pkunlp-icler\/PCA-EVAL.\n  Leaderboard at: https:\/\/docs.qq.com\/sheet\/DVUd4WUpGRHRqUnNV. This article\n  supersedes its workshop version arxiv: 2310.02071. arXiv admin note: text\n  overlap with arXiv:2310.02071","pdf_text":"PCA-Bench: Evaluating Multimodal Large Language Models in\nPerception-Cognition-Action Chain\nLiang Chen1, Yichi Zhang1, Shuhuai Ren1, Haozhe Zhao1, Zefan Cai1, Yuchi Wang1,\nPeiyi Wang1, Xiangdi Meng1, Tianyu Liu2, Baobao Chang1\n1 National Key Laboratory for Multimedia Information Processing, Peking University\n2 Alibaba Group\n{leo.liang.chen, yczhang, shuhuai_ren}@stu.pku.edu.cn\ntianyu0421@alibaba-inc.com, chbb@pku.edu.cn\n PCA-EVAL\nPCA-Bench-V1\nAbstract\nWe present PCA-Bench, a multimodal decision-\nmaking benchmark for evaluating the inte-\ngrated capabilities of Multimodal Large Lan-\nguage Models (MLLMs). Departing from pre-\nvious benchmarks focusing on simplistic tasks\nand individual model capability, PCA-Bench in-\ntroduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world\ngames. Given task instructions and diverse con-\ntexts, the model is required to seamlessly inte-\ngrate multiple capabilities of Perception, Cog-\nnition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench fea-\ntures error localization capabilities, scrutinizing\nmodel inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the\nreliability of deploying MLLMs. To balance\naccuracy and efficiency in evaluation, we pro-\npose PCA-Eval, an automatic evaluation pro-\ntocol, and assess 10 prevalent MLLMs. The\nresults reveal significant performance dispari-\nties between open-source models and powerful\nproprietary models like GPT-4 Vision. To ad-\ndress this, we introduce Embodied-Instruction-\nEvolution (EIE), an automatic framework for\nsynthesizing instruction tuning examples in\nmultimodal embodied environments. EIE gen-\nerates 7,510 training examples in PCA-Bench\nand enhances the performance of open-source\nMLLMs, occasionally surpassing GPT-4 Vi-\nsion (+3% in decision accuracy), thereby val-\nidating the effectiveness of EIE. Our findings\nsuggest that robust MLLMs like GPT4-Vision\nshow promise for decision-making in embod-\nied agents, opening new avenues for MLLM\nresearch.\n1\nIntroduction\nMultimodal Large Language Models (MLLMs)\nhave demonstrated remarkable capabilities in tack-\nling complex tasks that necessitate a chain of in-\ntegrated skills, including visual perception, world\nknowledge, reasoning, action, and more (OpenAI,\nYou are a driving assistant.  Based on current image, \nwhat should you do if you want to drive forward safely?\nA. Slow down \nB. Keep driving \nC. Call the police \nPlease first describe the image and related information, \nthen think step by step, and finally give the anwser.\nVisual Feature Extractor\nMultimodal LLM\nThe image depicts a wintry scene with a road covered in snow. The sky \nis clear, and no other vehicles on the road. There's a road sign visible, \nwith a symbol suggesting caution due to a slippery road surface.\nThe snow leads a landscape with sparse vegetation. Clear sky\nindicating it's likely during the day with good visibility. The road itself \nis not cleared of snow, showing only tire tracks indicating that some \nvehicles have passed before. The road sign warns for icy or snowy \nconditions.\nGiven these factors, the most appropriate action to take if you want to \ndrive forward safely would be:\nA. Slow down\nVisual\nEmbeddings\nLanguage\nEmbeddings\n......\nPerception\nCognition\nAction\nPerceiving Input\nReasoning\nMaking Decision\nFigure 1: Example of decision making with MLLMs in\nthe Perception-Cognition-Action Chain.\n2023; Dai et al., 2023a; Liu et al., 2023b; Li et al.,\n2023c; Zhao et al., 2023).\nHowever, current MLLM benchmarks often eval-\nuate these capabilities individually (Fu et al., 2023;\nLiu et al., 2023e), overlooking the significant in-\ntegrated potential that Large Language Models\n(LLMs) contribute to multimodal models. While\nsome benchmarks like MMMU (Yue et al., 2023)\nand MathVista (Lu et al., 2023a) require abilities\nfrom both the vision and language part, they lack\nerror localization techniques beyond accuracy as-\nsessments. This complicates identifying which part\nof the MLLM malfunctioned when making mis-\ntakes—whether it was the visual or the language\ncomponent—and determines which aspect requires\nenhancement to enhance overall performance.\nTo address the challenges of insufficient inte-\ngrated benchmarking and error localization prob-\nlems, we introduce PCA-Bench. It arises with\narXiv:2402.15527v1  [cs.CL]  21 Feb 2024\nMLLM’s applications in embodied AI and decision\nmaking, where models called agents need to first\nprocess multimodal observation from different en-\nvironments, reason with the current situation and\ngoal, and finally make an action from a given ac-\ntion space. The abilities in the complex decision\nmaking process can be abstracted to Perception,\nCognition and Action according to the Perception-\nAction loop (Fuster, 2004) in Cognitive Science, a\nfundamental concept that describes how organisms\nprocess sensory information to interact with their\nenvironment through actions, offering a compre-\nhensive framework for assessment. Figure 1 shows\nhow MLLMs make decisions in the PCA chain.\nThe instances in PCA-Bench are from three in-\nfluential domains in embodied decision-making:\nautonomous driving, domestic robotics, and open-\nworld gaming. As shown in Figure 2, each in-\nstance is annotated by human annotators with a\n6-element tuple: <image, question, action candi-\ndates, answer, reason, key concept>. The last three\nelements serve as anchors for error localization for\nAction, Cognition and Perception, correspondingly.\nPCA-Eval is an anchor-based evaluation pro-\ntocol, designed to automatically conduct error lo-\ncalization utilizing the powerful semantic parsing\nability of LLMs and the anchor information in data\nannotation. In the past, such localization was both\nlabor-intensive and time-consuming. PCA-Eval\nwith strong LLMs like GPT4 demonstrates a strong\nkappa correlation with human assessments, reach-\ning 0.8+ average kappa coefficients for perception,\ncognition, and action scores. The anchor-based\nevaluation provides the LLMs with groundtruth\nanswers for each sub-score, preventing the sys-\ntematic bias of LLM evaluators, such as position\nbias (Wang et al., 2023b; Zheng et al., 2023) in\nthe pair-wise evaluation and verbosity bias (Zheng\net al., 2023) in simple preference evaluation. We\nalso compared open state-of-the-art LLMs in PCA-\nEval. Though they lag behind close ones in align-\nment with human assessments, we see large im-\nprovement when the model scales up. We believe\nthat with specific training for error localization and\nimproved general ability of open LLMs in the fu-\nture, they would be more suitable evaluation tools\nfor the reproducible and transparent characteristics.\nAiming at scaling up PCA-Bench, using LLM\nto synthesize training examples is an increasingly\npopular method for enhancing models without ad-\nditional human involvement. We expand this ap-\nproach to generate more samples following the\nFigure 2: Instances of PCA-Bench in 3 domains.\nPCA guideline. Unlike text-based instruction gen-\neration methods like Self-Instruct (Wang et al.,\n2023c), generating instructions in embodied envi-\nronments poses distinct challenges. It demands not\nonly the creation of textual instructions but also the\ngeneration of corresponding precise observations.\nTo address these challenges, we propose Embod-\nied Instruction Evolution (EIE), which integrates\nexternal environments with LLMs, thereby extend-\ning the LLMs’ ability to data synthesize across\nvarious embodied environments, contributing to\n7,510 training data in PCA-Bench.\nWe conduct comprehensive experiments and\nanalysis on PCA-Bench, our findings are summa-\nrized as follows:\n1. Visual perception and reasoning with world\nknowledge are two core abilities for an MLLM\nto make correct decisions in PCA-Bench. GPT4-\nVision shows strong zero-shot cross-modal reason-\ning ability for embodied decision-making tasks,\nsurpassing open-source MLLMs and even Tool-\nUsing LLM-agent.\n2. EIE could generate training samples signifi-\ncantly enhancing the performance of open-source\nMLLMs (surpassing GPT-4V at some scores), vali-\ndating the effectiveness of the method.\n3.\nPCA-Eval serves as a good error locator.\nAbove the high average kappa coefficient (0.8+)\nwith human assessments and its ability to pinpoint\nthe error source, it can effectively distinguishes\nwhether a model’s correct decisions are fluky or\nthrough genuine understanding. This leads to a bet-\nter ensemble metric for MLLM evaluation named\nGenuine PCA Score.\n2\nPCA-Bench\n2.1\nProblem Definition\nMultimodal decision-making problems are com-\nmonly formalized with a partially observable\nMarkov decision process. For MLLMs F tested in\nPCA-Bench, we care about given the multi-modal\nobservation o ∈O, the goal description g, a subset\nof candidates actions AC ⊆A, whether the model\ncould make correct action a ∈AC and give proper\nreasoning process r.\nF(g, o, AC) = (a, r)\n(1)\nAs shown in Figure 2, each instance in the bench-\nmark is a 6-element tuple: <image, question, ac-\ntion candidates, answer, reason, key concept>.\nThe image is collected from various embodied en-\nvironments, including transportation scenes, house-\nkeeper environments, and Minecraft. Questions,\naction candidates, and answers are derived from\nreal tasks within the corresponding environment.\nThe reasons explain why the answer is the best\nchoice for the current image, while the key concept\nhighlights the most question-related aspect of the\nimage.\nUnlike traditional visual question-answering\ndatasets that emphasize visual perception (e.g.,\nVQA (Goyal et al., 2017)) or visual reasoning\n(e.g., NLVR (Suhr et al., 2017)), PCA-Bench man-\ndates accurate observation perception, complex\ntask decomposition, and understanding the out-\ncomes of various actions simultaneously. Com-\npared to embodied simulation environments such as\nALFRED (Shridhar et al., 2020) and Minedojo (Fan\net al., 2022), PCA-Bench stands out for its focus\non high-level actions, proving to be more effec-\ntive for evaluating MLLMs. This is because high-\nlevel actions, which can be readily translated or\nprogrammed into low-level actions within their re-\nspective domains, are inherently more accessible\nto LLMs. The high-level actions are more compre-\nhensible for LLMs than the direct low-level actions\nlike action vectors in the simulation environments\nbecause (1) the high-level actions are in the form\nof natural languages, making it easier for LLMs\nto understand the meaning and connect with world\nknowledge. (2) LLMs are not grounded with low-\nlevel actions during the pretraining or finetuning\nstage, making it hard for LLMs to understand the\nconsequences of executing an action.\nTo answer a question in PCA-Bench, the agent\nmust possess the following abilities: (1) Percep-\ntion: Accurately identify the concept related to\nthe question within the image; (2) Cognition: En-\ngage in reasoning based on image perception and\nworldly knowledge; (3) Action: Comprehend the\npotential actions, selecting the one that best aligns\nwith the outcome of the reasoning process. A de-\nficiency in any of these abilities would possibly\nresult in an incorrect answer, posing a significant\nchallenge to the more integrated capabilities of\nMLLMs.\n2.2\nPCA-Eval\nFor each instance, we prompt the model to deliver\nan answer comprising a reasoning process r, and\na final action a, represented as < r, a >. By com-\nparing the model prediction with the ground truth\nanswer, we can obtain a fine-grained diagnosis of\nthe decision making process as follows:\nPerception Score (P-Score) measures the model’s\naccuracy in perceiving the observation. It is com-\nputed based on whether the agent’s reasoning pro-\ncess r includes the key concept of the instance. A\nscore of 1 is assigned if at least one question-related\nkey concept is described by the agent; otherwise,\nit is 0. For the top example in Figure 2, the agent\nshould output “clear road” or “no car visible” or\nother semantically equivalent concepts in its de-\nscription of the image to get the perception score.\nParsing the model’s output and determining\nwhether it entails the key concept using shallow\nfeatures of the sentence is not trivial. We leverage\nLLM to conduct entailment detection, which turns\nout to have a high alignment with human judgment.\nCognition Score (C-Score) assesses the model’s\nability to reason, comprehend, and make informed\ndecisions based on the perceived input data and\nworld knowledge. The score is 1 if the reasoning\nprocess is correct, otherwise the score is 0. For\nthe instance in Figure 2, the agent should link the\n“clear road” to the action “keep driving” based on\ntransportation commonsense to get the score.\nAction Score (A-Score) measures the model’s abil-\nity to generate appropriate and effective responses\nor actions based on the perceived input data and\nthe cognitive understanding of the context. The\nscore is assigned a value of 1 if the agent selects\nthe correct action; otherwise, the score is set to 0.\n2.3\nAutomatic Evaluation\nRecent advancements have seen researchers har-\nnessing powerful LLMs for the evaluation of the\noutput of language models. Studies have revealed\nthat the outcomes from LLMs could exhibit re-\nmarkable alignment with human judgments (Zheng\net al., 2023; Wang et al., 2023b,a). In our investiga-\ntion, we employed GPT-4 to automatically evaluate\nperception, cognition, and action scores based on\nthe model’s outputs. Our findings underscore a\nsignificant agreement between GPT-4 scoring and\nhuman evaluation results. This is substantiated by\nCohen-Kappa coefficients of 0.71, 0.82, and 0.94\nfor perception, cognition, and action evaluations,\nrespectively. Experiments of human evaluation and\ncomparison of open LLMs are in section 4.1. For a\ndetailed description of our evaluation tool, kindly\nrefer to Appendix D.\n2.4\nBenchmark Dataset Overview\nFor the test set, the examples are written by 3 hu-\nman experts for each domain. There are no over-\nlapped environmental observations between the\ntraining and test sets. The details of the human\nannotation pipeline can be found in Appendix B.\nWe introduce the three domains encompassed by\nour dataset as follows:\nAutonomous Driving.\nIn the autonomous driv-\ning domain, instances are derived from real-world\ntransportation scenes, which requires the agent to\nhave particular abilities such as traffic sign recogni-\ntion, obstacle detection, and decision-making at in-\ntersections. The dataset aims to evaluate an agent’s\nability to perceive and interpret visual informa-\ntion while making safe and efficient driving deci-\nsions. The images are collected from TT100K (Zhu\net al., 2016) dataset and annotators are instructed\nto propose an image-conditioned question that is\ngrounded with real actions of vehicles.\nDomestic Robot.\nThe domestic assistance do-\nmain features instances from the ALFRED (Shrid-\nhar et al., 2020; Kolve et al., 2017) environment,\nwhich simulates a housekeeper robot performing\ntasks within a household setting. These tasks may\ninclude object manipulation, navigation, and inter-\naction with various appliances. The environment\nassesses an agent’s ability to understand and exe-\ncute complex instructions while navigating and in-\nteracting with a dynamic environment. Annotators\nare asked to select one image from the randomly\ngenerated scenes in the environment, propose a\nquestion related to the items on the scene, and an-\nnotate the full information of the instance.\nTopology Graph: Harvest beef using iron sword\nCraft 2 Iron Ingot\nCollect 2 Wood\nCraft 1 Stick\nFind a Cow\nKill a Cow\nCraft an Iron Sword\nCollect 2 Iron Ore\nFigure 3: Illustration of task topology graph. Events in\ngreen represent the leaf nodes of the graph.\nOpen-World Game.\nIn the open-world game do-\nmain, instances are sourced from the Minecraft en-\nvironment, where agents are tasked with exploring,\ncrafting, and surviving in a procedurally generated\nworld. This dataset evaluates an agent’s ability to\nreason and plan actions within a complex, open-\nended environment, which often requires long-term\nstrategizing and adaptability. Annotators receive\npredefined tasks from MineDojo (Fan et al., 2022)\nas a reference during the task generation phase. For\neach task, we instruct the annotator to sketch a task\ntopology graph, exemplified in Figure 3. The task\nshould be completed under the topological order of\nthe graph, where the event located in the leaf nodes\nshould be finished first. Each node in the task topol-\nogy graph can be viewed as a step in the sequential\ndecision. We list the in-domain task distribution in\nAppendix A.\n2.5\nEmbodied Instruction Evolution\nThe PCA-Bench benchmark also includes subset\nof automatic generated samples by Embodied In-\nstruction Evolution(EIE), which is used as training\nset in our experiment.\nThe annotation of PCA-Bench examples is a\nlabor-intensive task. As illustrated in Figure 4, we\nintroduce Embodied Instruction Evolution (EIE),\na method for automatically augmenting examples\nin the PCA-Bench format using Large Language\nModels, such as ChatGPT. This process involves\nfour key steps:\n1) Setup of Programmable Interface: Estab-\nlish a programmable interface with a corresponding\ntemplate, ensuring that observations in the embod-\nied environment can be generated based on specific\nparameters.\n2) Generation of Seed Tasks: Create initial\nseed tasks for each environment. These tasks are\nrepresentative of the general challenges an agent\nFigure 4: Pipeline of the Embodied Instruction Evolution method.\nmight encounter. We provide ChatGPT with sam-\nple tasks and enable it to generate additional seed\ntasks.\n3) Task Specification and Template Filling:\nFor each seed task, we instruct ChatGPT to break\ndown the task into multiple subtasks, following its\nevent topology graph (as seen in Figure 3). This\napproach mimics the multi-step decision-making\nprocess. After determining the subtask names, we\nuse the LLM to populate the environment parame-\nter templates created in Step 1 for each subtask.\n4) Observation Generation and Filtering:\nGenerate observations for the environment and im-\nplement an automatic process to filter out invalid\ninstances. The filled templates may contain er-\nrors, such as incorrect creature names or impos-\nsible items, leading to errors during environment\ncreation. When such errors occur, the affected tem-\nplates are automatically filtered out. For domains\nwithout programmable environments (autonomous\ndriving), step 1 and step 4 are not needed, we col-\nlect real traffic images and utilize GPT4-Vision to\ngenerate seed task based on the image content.\nEIE leverages the capabilities of Large Language\nModels to reduce manual labor and improve the\ndiversity and scalability of PCA-Bench.\n3\nExperiments\n3.1\nTracks\nZero Shot End-to-End.\nThe test set of PCA-\nBench serves as an effective tool for comparing\nthe embodied decision-making and cross-modal\nreasoning capabilities of various Multimodal Lan-\nguage Learning Models (MLLMs). In this evalu-\nation, the same images and prompts are provided\nto each model under test. Additionally, to address\nthe challenge of perceiving certain non-visual in-\nformation from images, details such as “items in\nhand” and “items in inventory”, particularly rele-\nvant in domestic and gaming domains, are directly\nincluded in the question prompts.\nIn our analysis, we benchmark the performance\nof the most recently open-sourced models, includ-\ning LLaVA1.5 and Qwen-VL-Chat, as well as the\nAPI-only GPT4-V model. All models are evalu-\nated using their default inference configurations to\nensure a fair and standardized comparison.\nFinetuning with EIE.\nIn this track, we extend\nthe capabilities of open-source MLLMs by fine-\ntuning them with the training set generated through\nour Embodied Instruction Evolution (EIE) method.\nAfter the fine-tuning process, these trained models\nare subjected to the test set of PCA-Bench. We\nfinetune the LLaVA-7b\/13b, MMICL and Qwen-\nVL-Chat models on the training set for 5 epochs.\nThe training details are in Appendix E.\nZero Shot Modality Conversion.\nIn this track,\nwe introduce and compare a new baseline, termed\nHOLMES, which utilizes LLM without multi-\nmodal perception capabilities. Instead, HOLMES\nrelies on modality conversion APIs for embodied\ndecision-making processes. Within the HOLMES\nframework, the LLM must continuously invoke\nvarious APIs, retrieving and processing return in-\nformation about the environment. The HOLMES\nmethod is illustrated in Figure 7 from Appendix.\nModel\nSize\nTraffic\nDomestic\nGame\nAverage\nP\nC\nA\nP\nC\nA\nP\nC\nA\nP\nC\nA\nMiniGPT4 (Zhu et al., 2023)†\n7B\n0.45\n0.37\n0.48\n0.81\n0.38\n0.38\n0.38\n0.14\n0.27\n0.55\n0.30\n0.38\nLLaVA1.5 (Liu et al., 2023b)†\n7B\n0.44\n0.44\n0.53\n0.92\n0.48\n0.44\n0.8\n0.35\n0.39\n0.72\n0.42\n0.45\nQwen-VL-Chat (Bai et al., 2023)†\n7B\n0.53\n0.36\n0.62\n0.77\n0.41\n0.44\n0.39\n0.18\n0.25\n0.56\n0.33\n0.44\nMiniGPT4 (Zhu et al., 2023)†\n13B\n0.41\n0.37\n0.5\n0.85\n0.35\n0.33\n0.41\n0.22\n0.33\n0.56\n0.31\n0.39\nInstructBLIP (Dai et al., 2023b)†\n13B\n0.36\n0.41\n0.42\n0.90\n0.44\n0.39\n0.33\n0.25\n0.24\n0.53\n0.37\n0.35\nMMICL (Zhao et al., 2023)†\n13B\n0.31\n0.49\n0.47\n0.81\n0.3\n0.33\n0.41\n0.18\n0.27\n0.51\n0.32\n0.36\nSPHINX-v1 (Lin et al., 2023)†\n13B\n0.46\n0.48\n0.61\n0.95\n0.55\n0.31\n0.71\n0.35\n0.43\n0.71\n0.46\n0.45\nLLaVA1.5 (Liu et al., 2023b)†\n13B\n0.49\n0.56\n0.61\n0.95\n0.62\n0.46\n0.74\n0.45\n0.51\n0.73\n0.54\n0.53\nQwen-VL-Chat-PLUS (Bai et al., 2023)‡\nUNK\n0.57\n0.56\n0.65\n0.86\n0.44\n0.43\n0.68\n0.47\n0.49\n0.70\n0.49\n0.52\nGPT-4V (OpenAI, 2023)‡\nUNK\n0.73\n0.72\n0.74\n0.96\n0.66\n0.62\n0.88\n0.72\n0.69\n0.86\n0.7\n0.68\nTable 1: Zero Shot results on the full test set of PCA-Bench. Highest scores in each line are bold while second\nhighest scores are underlined. Models with † are fully open-source. Models with ‡ only provide API to access. P, C,\nand A represent Perception, Cognition, and Action Scores, respectively.\nFigure 5: Performance comparsion between models’ zero-shot results and models’ finetuned results with the data\ngenerated by Embodied-Instruct-Evolution (EIE) method. EIE improves the performance on all domains for both\nLLaVA1.5-7b and Qwen-VL-Chat models. Results of LLavA1.5-13B and MMICL are in Figure 13 from appendix.\nWe evaluate two LLMs in this track: ChatGPT-\n3.5-Turbo and GPT-4-0613, comparing their per-\nformances against the advanced GPT-4-Vision. Im-\nplementation details of the HOLMES framework\nand the APIs are provided in Appendix C.\n3.2\nEvaluation and Metrics\nWe use our PCA-Eval evaluation tool proposed in\nSection 2.3 to automatically assess the output of dif-\nferent models through three lenses: perception (P-\nScore), cognition (C-Score), and action (A-Score).\n3.3\nMain Results\nZero Shot Results.\nThe results of the zero-shot\nend-to-end track are shown in Table 1. Among\nall MLLMs, GPT4-V, outperforms existing open-\nsource models by achieving the highest scores of\n0.86, 0.7, and 0.68 in the perception, cognition, and\naction dimensions respectively. This performance\nrepresents a 15% action score improvement over\nits strongest open-source counterpart, LLaVA1.5-\n13B. The impressive performance of GPT4-V is\nprimarily attributed to its exceptional ability to per-\nceive visual information across different domains\nand the world knowledge in the language model,\nparticularly in the challenging game domain.\nImpact of Finetuning with EIE.\nThe results of\nthe fine-tuning track are illustrated in Figure 5. Our\nEIE method has been found to significantly en-\nhance the general decision-making abilities of vari-\nous models, encompassing perception, cognition,\nand action. Notably, it has led to an average in-\ncrease of 0.24 and 0.19 in action scores for the\nLLaVA1.5-7b and Qwen-VL-Chat models, respec-\ntively. Results for LLaVA1.5-13b and MMICL are\nillustrated in Figure 13, also showing improved\nperformance when trained with EIE. We note that\nthere exist reasoning or perception errors in some\nof the generated sample due to the hallucination\nproblem of LLM generated content, however they\ndo not influence the overall performance. In some\ncases, these sub-scores have matched or even sur-\npassed those of the GPT4-V model, demonstrating\nthe potential of the EIE to scale up and apply to\ndifferent environments.\nComparison Between End-to-End and Modality\nConversion Method\nIn the zero-shot modality\nconversion track, we conduct an analysis and com-\nparison of the outputs generated by the End2End\nMethod\nModel\nTraffic\nDomestic\nGame\nAverage\nP\nC\nA\nP\nC\nA\nP\nC\nA\nP\nC\nA\nEnd-to-End\nGPT-4V\n0.75\n0.73\n0.78\n0.81\n0.69\n0.67\n0.95\n0.79\n0.77\n0.84\n0.74\n0.74\nHOLMES\nChatGPT\n0.75\n0.68\n0.66\n0.88\n0.52\n0.50\n0.78\n0.40\n0.36\n0.80\n0.53\n0.51\nGPT4\n0.87\n0.82\n0.82\n0.85\n0.61\n0.56\n0.91\n0.77\n0.74\n0.88\n0.73\n0.71\nTable 2: Comparison between End-to-End (MLLM) and HOLMES (LLM+API) methods on a subset of PCA-Bench\nwith API annotation.\nmethod with GPT4-V, as well as the HOLMES\nmethod with GPT4 and ChatGPT-3.5 in Table 2.\nThe results show that the HOLMES system\nbased on GPT4 achieves 0.71 Action Score, which\nis on par with GPT4-V’s performance (0.74). This\nindicates that, overall, the HOLMES system is able\nto accurately understand the task goal, split the\nlarger goal into multiple smaller steps, and cor-\nrectly invoke the relevant APIs to accomplish each\nstep. Specifically, the HOLMES system based on\nGPT4 can recognize the key concepts in a task, and\nperceive the state and environment of these con-\ncepts through the results returned by APIs. Conse-\nquently, the system achieves an average Perception\nScore of 0.88, which even outperforms GPT4-V’s\n0.84. However, compared to End2End methods,\nHOLMES relies on multi-step reasoning for the\nfinal decision, in which reasoning errors tend to\naccumulate, and thus achieves a lower Cognition\nScore in both Domestic and Game domains.\nOn the other hand, we also find that the End2End\nmethod effectively mitigates information loss dur-\ning the modality conversion process. As illustrated\nin Figure 8 from Appendix, an image depicts a\nroad with several nearby cars. GPT4-V is capable\nof discerning that the street is not crowded, thereby\nsuggesting that the driver can continue driving.\nConversely, GPT4-HOLMES, while being aware\nof the number of cars, lacks information about their\nspatial relation, leading it to recommend slowing\ndown because of the existence of 14 cars. This\nsuggests that the End2End method is superior in\nperceiving certain visual features that are not cap-\ntured by the APIs. Conversely, some specialized\nAPIs, such as traffic sign detection, outperform\nGPT4-V in tasks like traffic sign detection, as they\nare specifically trained for this task. This could en-\nable the HOLMES method to gather more accurate\ninformation than the End2End model.\nEvaluator Model\nKappa Coefficients\nP\nC\nA\nGPT4†\n0.71\n0.82\n0.94\nQwen1.5-72B-Chat†\n0.30\n0.49\n0.60\nQwen1.5-14B-Chat†\n0.16\n0.24\n0.16\nQwen1.5-7B-Chat†\n0.20\n0.11\n0.06\nTable 3: Comparison of Open† and Close† LLMs as\nEvaluators. Kappa coefficients of Qwens increase when\nthe model scales up.\n4\nDiscussion\n4.1\nStrong LLMs are Good Error Locators.\nAs shown in Table 3, we compare the scoring kappa\ncoefficients with human assessments for different\nLLMs. We randomly select 300 model outputs\nequally from different domains and ask 3 human\nexperts to give perception, cognition, and action\nscores. The final result is based on the majority\nof three annotators. The result underscores a sig-\nnificant agreement between GPT-4 scoring and hu-\nman evaluation results. This is substantiated by\nCohen-Kappa coefficients of 0.71, 0.82, and 0.94\nfor perception, cognition, and action evaluations.\nWe also compare open models as evaluators.\nWe choose one of the best open LLMs, Qwen1.51\nseries from 7B, 14B to 72B version. Currently\nopen LLMs tend to give wrongly high judgments in\nall sub-scores. Although currently trailing behind\nGPT-4 in performance, we anticipate that with tar-\ngeted training focused on error identification and\nenhancements in the overall capabilities of open\nLLMs, these models will become more effective\nevaluation tools compared to closed models. This\nis primarily due to the reproducible and transpar-\nent nature of open models, which offer significant\nadvantages in the development of evaluation tools.\n1https:\/\/huggingface.co\/collections\/Qwen\n4.2\nGenuine PCA Score\nPCA-Eval could pinpoint cases where the MLLM\ngets the correct answer by a fluke where perception\nor cognition score is 0 but the action score is 1. It\nexplains why for some models, the action score is\nhigher than perception and cognition scores. For in-\nstance, a model might opt for a conservative action,\nsuch as slowing down, even without accurately rec-\nognizing snowy weather in the image, resulting in\na fluky correct action. In another scenario, if the\nmodel exhibits a preference for a specific choice\nindex, it will attain a high action score provided\nthat the evaluation dataset contains a substantial\nnumber of correct choices matching the preferred\nindex, a phenomenon attributable to the positional\nbiases inherent in both the model and the dataset.\nTo overcome the mentioned bias when evaluating\nthe genuine ability of MLLM, we propose a new\nmetric Genuine PCA Score. It is equal to one\nif the perception, cogntion and action scores are\nall 1 for one model’s response to a question. We\nfind that for all models, there exists significant gap\n(>10%) between the action score and genuine PCA\nscore in average, revealing that relying on single\nmetric such as choice accuracy is very problematic\nwhen conducting model evaluation. In our online\nleaderboard, both average action score and average\ngenuine PCA score are considered when ranking\nthe candidate models.\n4.3\nAlignment between Agent Decisions and\nHuman Values\nWe have observed instances where the decisions\nmade by the agent contradict human values. Con-\nsider the scenario depicted in Figure 9 from Ap-\npendix. The image illustrates a crosswalk with-\nout pedestrians. The appropriate response would\nbe slowing down, as caution is paramount when\napproaching a crosswalk, regardless of the pres-\nence or absence of pedestrians. However, upon\nprocessing the information that the crosswalk is\nempty, ChatGPT suggests that maintaining the cur-\nrent speed is the optimal action, arguing that the\nabsence of pedestrians eliminates the need to slow\ndown. The rationale provided by ChatGPT is logi-\ncal, yet it does not align with human values.\n5\nRelated Work\nMLLM Benchmark.\nIn recent times, there\nhave been several benchmarks built for evaluating\nMLLMs, such as MMBench, MME, Seed-Bench,\nPOPE (Liu et al., 2023e; Fu et al., 2023; Li et al.,\n2023a,e) that assess MLLMs performance from\nmultiple fine-grained dimensions.\nVisit-Bench,\nLVLM-eHub, M3IT (Bitton et al., 2023; Xu et al.,\n2023; Li et al., 2023c) focus on the general in-\nstruction following ability. General VQA tasks\nlike OKVQA, VQAv2, Vizwiz, ScienceQA, VSR\nand IconQA (Marino et al., 2019; Agrawal et al.,\n2015; Gurari et al., 2018; Lu et al., 2022; Liu et al.,\n2023a; Lu et al., 2021) focus on visual understand-\ning. MMMU, MathVista, LLaVA-benchmark and\nMM-Vet (Yue et al., 2023; Lu et al., 2023a; Liu\net al., 2023c; Yu et al., 2023) require abilities from\nthe vision part and specific knowledge in the lan-\nguage part. A lack of error localization techniques\nbeyond accuracy assessments is among current\nbenchmarks. This complicates identifying which\npart of the MLLM malfunctioned when making\nmistakes. Unlike prior work, PCA-Bench is more\nrelevant to evaluate MLLMs’ ability to utilize inte-\ngrated abilities to solve one task and make explain-\nable decisions via error localization.\nLLM Agent and Embodied Decision Making.\nUsing LLMs to empower the AI agents (Xi et al.,\n2023; Liu et al., 2023d; Park et al., 2023; Wang\net al., 2023d) becomes more and more promis-\ning. Specifically, we can employ LLMs to enhance\nthe decision making ability of the agents (Nakano\net al., 2022; Yao et al., 2022; Li et al., 2023d;\nSong et al., 2023; Li et al., 2023b), expanding\ntheir perception and action space through strate-\ngies like tool utilization (Schick et al., 2023; Qin\net al., 2023; Lu et al., 2023b). This line of research\ndivides the entire decision-making process into two\nphases: (1) information seeking, usually involving\nMLLMs to verbalize the current status of AI agents\nin the vision-based environment with natural lan-\nguage; (2) reasoning and planning with text-based\nLLMs to decide what the AI agent should do in\nthe next step with textual clues. Although LLM-\nbased agents demonstrate reasoning and planning\nabilities through techniques like Chain of Thought\nor problem decomposition (Wei et al., 2023; Yao\net al., 2023; Kojima et al., 2022), they inherently\nlack visual perception, and are limited to the dis-\ncrete textual content. Therefore, integrating mul-\ntimodal information can offer agents a broader\ncontext and a more precise understanding, such\nas PaLM-E (Driess et al., 2023), enhancing their\nenvironmental perception. However, there is still\nlarge gap deploying MLLM in various embodied\nenvironments due to the lack of appropriate bench-\nmark and interface linking those two domains while\nPCA-Bench is an attempt towards that goal.\n6\nConclusion\nIn this paper, we introduce PCA-Bench, a mul-\ntimodal benchmark designed to assess the inte-\ngrated decision-making capabilities of MLLMs.\nThis benchmark features PCA-EVAL, a novel fine-\ngrained automatic evaluation tool that diagnoses\ndecision making processes from three critical per-\nspectives: perception, cognition, and action. To\nenhance the decision making ability from data per-\nspective, we propose the Embodied Instruction Evo-\nlution method to automatically synthesize instruc-\ntion examples from different environments, which\nhas been proven effective in our main experiments.\nWe believe that powerful MLLMs pave a new and\npromising way toward decision making in embod-\nied environments and we hope PCA-Bench could\nserve as a good benchmark in evaluation and error\nlocalization for MLLMs’ development.\n7\nLimitations\nThe current scope of PCA-Bench is confined to\nmerely three domains in static environments. One\nof our future works aims to broaden this scope\nto encompass more domains and dynamic embod-\nied environments where MLLMs could keep get-\nting feedback, which is closer to real embodied\nAI scenarios.\nWe do not apply different infer-\nence enhancement methods like In-Context Learn-\ning and Reflection in the decision making process\nof MLLMs. We just use the simplest prompting\nmethod and leave the exploration of a better cross-\nmodal Chain-of-Thought method for future studies.\nCurrently, PCA-Eval shows the best consistency\nwith human evaluators when using powerful close\nLLM GPT4, which would bring additional cost to\nthe user of PCA-Eval. We plan to develop and re-\nlease an open error locator for error localization in\nthe benchmark in the future.\nReferences\nAishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Mar-\ngaret Mitchell, C. Lawrence Zitnick, Devi Parikh,\nand Dhruv Batra. 2015. Vqa: Visual question an-\nswering. International Journal of Computer Vision,\n123:4 – 31.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023.\nQwen-vl:\nA frontier\nlarge vision-language model with versatile abilities.\nArXiv, abs\/2308.12966.\nYonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,\nWanrong Zhu, Anas Awadalla, Josh Gardner, Rohan\nTaori, and Ludwig Schmidt. 2023. Visit-bench: A\nbenchmark for vision-language instruction following\ninspired by real-world use.\nLiang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao,\nZefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, and\nBaobao Chang. 2023. Towards end-to-end embod-\nied decision making via multi-modal large language\nmodel: Explorations with gpt4-vision and beyond.\nArXiv.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023a. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Albert Li, Pascale Fung, and Steven C. H.\nHoi. 2023b. Instructblip: Towards general-purpose\nvision-language models with instruction tuning.\nArXiv, abs\/2305.06500.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\n2023. Palm-e: An embodied multimodal language\nmodel.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-\ndlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anand-\nkumar. 2022.\nMinedojo:\nBuilding open-ended\nembodied agents with internet-scale knowledge.\nIn Thirty-sixth Conference on Neural Information\nProcessing\nSystems\nDatasets\nand\nBenchmarks\nTrack.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, et al. 2023. Mme: A compre-\nhensive evaluation benchmark for multimodal large\nlanguage models. arXiv preprint arXiv:2306.13394.\nJoaquin M. Fuster. 2004. Upper processing stages of\nthe perception–action cycle.\nTrends in Cognitive\nSciences, 8(4):143–145.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017.\nMaking the V in\nVQA matter: Elevating the role of image under-\nstanding in visual question answering.\nIn 2017\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July\n21-26, 2017, pages 6325–6334.\nDanna Gurari, Qing Li, Abigale Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jef-\nfrey P. Bigham. 2018. Vizwiz grand challenge: An-\nswering visual questions from blind people. 2018\nIEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 3608–3617.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang,\nSeonghyeon Ye, Jamin Shin, and Minjoon Seo. 2023.\nThe cot collection: Improving zero-shot and few-shot\nlearning of language models via chain-of-thought\nfine-tuning.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners.\nAdvances\nin neural information processing systems, 35:22199–\n22213.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-\nderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon,\nYuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017.\nAI2-THOR: An Interactive 3D Environment for Vi-\nsual AI. arXiv.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. 2023a. Seed-bench: Bench-\nmarking multimodal llms with generative compre-\nhension. ArXiv, abs\/2307.16125.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani\nItani, Dmitrii Khizbullin, and Bernard Ghanem.\n2023b. Camel: Communicative agents for \"mind\"\nexploration of large language model society.\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi\nWang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu.\n2023c. M3it: A large-scale dataset towards multi-\nmodal multilingual instruction tuning. arXiv preprint\narXiv:2306.04387.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023d. Api-\nbank: A benchmark for tool-augmented llms.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,\nWayne Xin Zhao, and Ji-Rong Wen. 2023e. Eval-\nuating object hallucination in large vision-language\nmodels.\nZiyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\nQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao,\nKeqin Chen, Jiaming Han, Siyuan Huang, Yichi\nZhang, Xuming He, Hongsheng Li, and Yu Qiao.\n2023. Sphinx: The joint mixing of weights, tasks,\nand visual embeddings for multi-modal large lan-\nguage models.\nFangyu Liu, Guy Edward Toh Emerson, and Nigel Col-\nlier. 2023a. Visual spatial reasoning. Transactions of\nthe Association for Computational Linguistics.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual instruction tuning.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023c. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny\nZhou, Andrew M Dai, Diyi Yang, and Soroush\nVosoughi. 2023d. Training socially aligned language\nmodels in simulated human society. arXiv preprint\narXiv:2305.16960.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. 2023e. Mm-\nbench: Is your multi-modal model an all-around\nplayer? arXiv preprint arXiv:2307.06281.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun\nyue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. 2023a.\nMathvista: Evaluating math reasoning in visual con-\ntexts with gpt-4v, bard, and other large multimodal\nmodels. ArXiv, abs\/2310.02255.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. 2022. Learn to explain: Multi-\nmodal reasoning via thought chains for science ques-\ntion answering. In The 36th Conference on Neural\nInformation Processing Systems (NeurIPS).\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2023b. Chameleon: Plug-and-play\ncompositional reasoning with large language models.\narXiv preprint arXiv:2304.09842.\nPan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou\nZhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and\nSong-Chun Zhu. 2021. Iconqa: A new benchmark\nfor abstract diagram understanding and visual lan-\nguage reasoning. In The 35th Conference on Neural\nInformation Processing Systems (NeurIPS) Track on\nDatasets and Benchmarks.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019.\nOK-VQA: A vi-\nsual question answering benchmark requiring exter-\nnal knowledge. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA, June 16-20, 2019, pages 3195–\n3204.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nOpenAI. 2023. Gpt-4v(ision) system card.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S\nBernstein. 2023.\nGenerative agents: Interactive\nsimulacra of human behavior.\narXiv preprint\narXiv:2304.03442.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, et al. 2023.\nTool\nlearning with foundation models.\narXiv preprint\narXiv:2304.08354.\nJoseph Redmon and Ali Farhadi. 2018. Yolov3: An\nincremental improvement. arXiv.\nShuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai\nZheng, Mu Li, Alex Smola, and Xu Sun. 2023.\nPrompt pre-training with twenty-thousand classes for\nopen-vocabulary visual recognition. arXiv preprint\narXiv:2304.04704.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. ALFRED: A\nBenchmark for Interpreting Grounded Instructions\nfor Everyday Tasks. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nYifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu,\nHan Qian, Mingbo Song, Hailiang Huang, Cheng\nLi, Ke Wang, Rong Yao, Ye Tian, and Sujian Li.\n2023. Restgpt: Connecting large language models\nwith real-world restful apis.\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.\n2017. A corpus of natural language for visual rea-\nsoning. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 217–223.\nPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai\nLin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. 2023a.\nMaking large language models better reasoners with\nalignment. arXiv preprint arXiv:2309.02144.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023b. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023c. Self-instruct: Aligning language\nmodels with self-generated instructions.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and\nYitao Liang. 2023d.\nDescribe, explain, plan and\nselect: Interactive planning with large language mod-\nels enables open-world multi-task agents. ArXiv,\nabs\/2302.01560.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\nSenjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,\nXiao Wang, Limao Xiong, Yuhao Zhou, Weiran\nWang, Changhao Jiang, Yicheng Zou, Xiangyang\nLiu, Zhangyue Yin, Shihan Dou, Rongxiang Weng,\nWensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan\nZheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui.\n2023. The rise and potential of large language model\nbased agents: A survey.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao,\nShuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang,\nYu Qiao, and Ping Luo. 2023. Lvlm-ehub: A com-\nprehensive evaluation benchmark for large vision-\nlanguage models. arXiv preprint arXiv:2306.09265.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nWeiran Yao, Shelby Heinecke, Juan Carlos Niebles,\nZhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy,\nZeyuan Chen, Jianguo Zhang, Devansh Arpit, et al.\n2023. Retroformer: Retrospective large language\nagents with policy gradient optimization.\narXiv\npreprint arXiv:2308.02151.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Li-\njuan Wang. 2023. Mm-vet: Evaluating large mul-\ntimodal models for integrated capabilities. ArXiv,\nabs\/2308.02490.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,\nHuan Sun, Yu Su, and Wenhu Chen. 2023. Mmmu:\nA massive multi-discipline multimodal understand-\ning and reasoning benchmark for expert agi.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023.\nMulti-\nmodal chain-of-thought reasoning in language mod-\nels. arXiv preprint arXiv:2302.00923.\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian\nMa, Kaikai An, Liang Chen, Zixuan Liu, Sheng\nWang, Wenjuan Han, and Baobao Chang. 2023.\nMmicl: Empowering vision-language model with\nmulti-modal in-context learning.\narXiv preprint\narXiv:2309.07915.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nZhe Zhu, Dun Liang, Songhai Zhang, Xiaolei Huang,\nBaoli Li, and Shimin Hu. 2016.\nTraffic-sign de-\ntection and classification in the wild.\nIn The\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nA\nExamples of PCA-Bench\nA.1\nData Distribution\nFigure 6: Domain and required ability distribution of\nPCA-Bench.\nThe PCA-Bench’s data distribution across var-\nious domains is outlined in Figure 6. For the Au-\ntonomous Driving domain, instances are grouped\nby their respective task types. In the Domestic\nRobot domain, instances are grouped by their loca-\ntions. In the Open-World Game domain, instances\nare grouped by the tasks they aim to accomplish.\nB\nHuman Annotation Pipelines\nThe annotation process consists of two stages: (1)\nDataset Annotation, and (2) Dataset Refinement.\nDuring the initial stage, three annotators are as-\nsigned to each domain, adhering strictly to the re-\nspective annotation guidelines. They first pinpoint\nthe source images from each domain that are in-\nformative and meaningful so that they can write\nquestions for each image. The annotators have the\nresponsibility to ensure every question has only\none correct answer and accurate rationales. In the\nsubsequent stage, annotators are instructed to scru-\ntinize the output actions and rationales presented by\nChatGPT and check the annotations. This process\naims to address the challenge of multiple correct\nanswers, as ChatGPT can furnish comprehensive\nexplanations for its actions. These explanations\nassist annotators in assessing the acceptability of\nChatGPT’s response, particularly when it deviates\nfrom the established ground truth answer. This en-\nables annotators to refine annotations to ensure the\npresence of a single correct answer.\nB.1\nPCA-Bench Examples\nWe list three examples of each domain from PCA-\nBench, as shown in Figure 10, 11, and 12.\nC\nZero Shot Modality Conversion:\nHOLMES\nTo optimize the evaluation process of HOLMES2\nmethod, we pre-execute all relevant APIs for each\ninstance within a selected subset of 300 instances\nfrom the PCA-Bench test set, recording the results\nfor individual instances. This method enables im-\nmediate access to specific API results, eliminating\nthe need to rerun the model for each evaluation\ninstance.\nTraffic Domain.\nBelow is the API description\nfor the traffic domain.\n1 # API Description for Traffic Domain:\n2 def detect_traffic_sign ():\n3\n\"\"\"\n4\nDetects traffic signs in the image.\n5\n:return: list of detected traffic\nsigns and coordinates , e.g. ['stop\n','max speed limit ']\n6\n\"\"\"\n7\npass\n8\n9 def object_detection ():\n10\n\"\"\"\n11\nDetects objects in the image.\n12\n:return: dict of detected objects\nand number of the objects , e.g. {'\ncar ':10, 'person ':1}\n13\n\"\"\"\n14\npass\n15\n16 def ocr():\n17\n\"\"\"\n18\nPerforms OCR on the image.\n19\n:return: list of detected text , e.g.\n['Changjiang road ', 'Right lane\nclosure ']\n20\n\"\"\"\n21\npass\n22\n23 def image_caption ():\n24\n\"\"\"\n25\nGenerates a caption for the image.\n26\n:return: caption , e.g. 'A red car\ndriving down the street '\n27\n\"\"\"\n28\npass\n29\n30 def weather_detection ():\n31\n\"\"\"\n32\nDetect current weather.\n33\n:return: weather , e.g. 'rainy ' or '\nclear '\n34\n\"\"\"\n35\npass\n• detect_traffic_sign(): The detection of road\ntraffic signs model utilize YOLO (Redmon and\nFarhadi, 2018) which trained on the Tsinghua-\nTencent 100K dataset (Zhu et al., 2016). TT100K\n2Originally proposed in an early version of this pa-\nper (Chen et al., 2023)\nWhat is the best action to take when you are \ndriving on the highway? \nActions: A. Slow down B. Keep driving C. Stop \nthe car D. Change to other lane\nI will first call the detect_traffic_sign() API to \ncheck if there are any traffic signs in the \nimage that could inform the best action.\ndetect_traffic_sign() = None\nSince there are no traffic signs detected, I will now \ncall the object_detection() API to check for any \nobjects in the image that could inform the best action.\nobject_detection() = None\nSince there are no objects detected, I will now call \nthe caption() API to get a general description of the \nimage, which might help in determining the best \naction.\ncaption() = The image features a long, empty \nhighway.\nBased on the provided caption, it seems that the \nhighway is not congested. Therefore, the best \naction to take when driving on the highway in \nthis situation would be: B. Keep driving\nAutonomous Driving\nYou are a powerful game assistant in Minecraft. \nYour goal is to Harvest wool. Based on current \ncondition, what is the best action to do next? \nActions:  A. Find sheep B. Shear sheep C. Craft \nshears D. Find iron ore E. Craft iron ingot\nIn order to answer the question, I would first need to \ncheck if I already have shears in the inventory. So I \nwould call list_inventory_information().\nlist_inventory_information() = [(\"shears\", 1)]\nGiven that the inventory does have shears but no \nnearby sheep are detected, the best action to do next \nwould be: A. Find Sheep\nOpen-World Game\nSince the inventory contains shears, then the next \naction would be dependent on the presence of sheep. \nI would then call `list_nearby_mobs()` to check if \nthere are sheep nearby \nlist_nearby_mobs() = None\nYou are a domestic robot that helps me do \nhousework. Your goal is to heat a cup of milk. What \nis the best action to take next? \nActions: A. Find a microwave. B. Put a cup of milk \ninto the microwave. C. Find milk. D. Pick up a cup \nof milk.\nI will first call the object_detection_alfred() API to \ncheck for any relevant objects in the current view.\nobject_detection_alfred() =\n['StoveTopDoor1',\n'Lettuce',\n'StoveBurner',\n'SaltShaker',\n'CounterTop',\n'Spatula', 'Pot', 'StoveKnob', 'StoveBase1', 'Cabinet',\n'DishSponge',\n'Microwave',\n'Pan',\n'PepperShaker',\n'Ladle',\n'Floor',\n'SoapBottle',\n'Dishwasher',\n'Sink',\n'Ceiling', 'Bread', 'Mug']\nSince I already have a cup of milk in my inventory, \nthe best action to take next is:\nB. Put a cup of milk into the microwave.\nDomestic Robot\nNow that I have detected a microwave and a mug in \nthe current view, I will call list_inventory_alfred() to \ncheck if I already have milk in my inventory.\nlist_items_in_inventory_alfred() = [“cup of milk”]\nImage\nUser\nLLM\nModel\/API Hub\nFigure 7: Three examples of HOLMES solving questions from different domains of PCA-Bench.\ncomprises 100,000 images encompassing 30,000\ninstances of traffic signs. The end-to-end YOLO\nenables simultaneous detection and classification\nof traffic signs.\n• object_detection(): Objects demanding atten-\ntion during vehicle operation primarily encompass\ncars, pedestrians, and bicycles. A surfeit of vehi-\ncles can lead to traffic congestion, while the pres-\nence of pedestrians or bicycles ahead necessitates\ncars to decelerate and proceed cautiously. Hence,\nthe object_detection() API predominantly identi-\nfies three key object categories: cars, pedestrians,\nand bicycles. We utilize PMOP (Ren et al., 2023),\na model trained on vision-language models through\nthe prompt pre-training method, which enables the\ndetection and counting of the three mentioned ob-\njectives by modifying specific class names.\n• ocr(): We employ PaddleOCR3 to extract tex-\ntual information from images, providing crucial\nroad data for real-time navigation.\n• image_caption(): To initially streamline the\nroad information within the image, we employ the\nBLIP2-flan-t5-xl to generate an initial caption for\nthe picture. This caption, derived from basic im-\nage data, is then utilized as input for the model to\nfacilitate decision-making.\n3https:\/\/github.com\/PaddlePaddle\/PaddleOCR\/\ntree\/release\/2.7\n• weather_detection(): Weather detection lever-\nages a pre-trained ResNet50 model4, derived from\na dataset of more than 70,000 weather records. This\nmodel extracts weather information from provided\nimages to inform decision-making.\nDomestic Robot Domain.\nBelow is the API de-\nscription for the Domestic Robot domain.\n1 #API Description for Domestic Robot\nDomain\n2 def object\\_detection ():\n3\n\"\"\"\n4\nDetects objects in current view ,\nwhich you don't need do find.\n5\n:return: list of detected objects , e\n.g. ['chair ','table ']\n6\n\"\"\"\n7\npass\n8\n9 def list_items_in_hands ():\n10\n\"\"\"\n11\nLists items in your hand , which you\ndon't need to pick up\n12\n:return: list of items in hand , e.g.\n['coffee cup ','milk ']\n13\n\"\"\"\n14\npass\nGame Domain.\nBelow is the API description for\nthe Game domain (Minedojo).\n1 #API Description for Game Domain\n2 def list_nearby_mobs_in_minecraft ():\n4https:\/\/github.com\/mengxianglong123\/\nweather-recognition\n3\n\"\"\"\n4\nLists nearby mobs in Minecraft.\n5\n:return: list of nearby mobs , e.g.\n['creeper ', 'pig ']\n6\n\"\"\"\n7\npass\n8\n9 def list_inventory_information ():\n10\n\"\"\"\n11\nLists inventory information of the\nplayer in Minecraft.\n12\n:return: list of inventory\ninformation with number , e.g. [('\ndiamond ', 64), ('iron ', 32)]\n13\n\"\"\"\n14\npass\nNote that within the Domestic Robot Domain\nand Game Domain, APIs can be directly accessed\nwithin the virtual environment, allowing for the per-\nception of the surrounding objects and the current\npicture context.\nWhat is the best action to take when you are driving? \nActions: A. Slow down B. Keep driving C. Stop the \ncar D. Change to other lane\nAutonomous Driving\nGPT4-V\nThe traffic appears to be moving smoothly without \nany apparent obstructions. The traffic signals are not \nvisible in the image, so there's no indication that you \nneed to slow down or stop.\nThe best action to take when driving in the scenario \npictured is: B. Keep driving\nGPT4\nBased on the object detection results, there are 14 \ncars in the image. It's better to be cautious as the road \nmight be congested with cars. The best action to take \nis: A. Slow down.\n……\nFigure 8: A Comparison between GPT4-V and GPT4-\nHOLMES\nWhat is the best action to take when you are driving? \nActions: A. Slow down B. Keep driving C. Stop the \ncar D. Change to other lane\nAutonomous Driving\nChatGPT\nBased on the provided caption and the result of object \ndetection, the image depicts an empty crosswalk with \nno humans or cars. Therefore, the best action is B: \nKeep driving, as it's not necessary to slow down.\nobject_detection() = {‘car’:0,’human’:0, ‘bicycle’:0}\ncaption() = An empty crosswalk.\n……\nFigure 9: An case showing the value mis-alignment\nbetween of agent and human’s decision.\nD\nAutomatic Evaluation\nWe utilize the template as shown in Table 4 to\nquery GPT-4, aiming to evaluate its responses and\nassign scores for perception, cognition, and action.\nBy feeding both the agent’s output and the ground\ntruth answer to GPT-4, based on this template, we\ncan then extract the three distinct scores from the\nconclusion of GPT-4’s response.\nBased on current image, what is the best \naction to take when you are driving on road?\n(B) Drive close to the left side\n(A) Drive close to the right side\n(B) Drive close to the left side (C) Call 911\nBased on the image, The right side of the road \nis closed, so you should drive close to the left.\nright lane closure\nimage：\nQuetsion：\nBased on current image, What should you do \nif you want to drive forward safely?\nAction candidates：\nAnswer：\n(A) Slow down\n(A) Slow down (B) Keep driving (C) Call the \npolice\nReason：\nKey Concept：\nThe current heavy snowfall is causing \nslippery roads. So you should slow down.\nSnowy weather\nimage：\nQuetsion：\nAction candidates：\nAnswer：\nReason：\nKey Concept：\nimage：\nQuetsion：\nYou are at 20km\/h now. What should you do \nif you want to overtake the car in front of you?\nAction candidates：\nAnswer：\n(B) You can't overtake now\n(A) Speed up to 50km\/h (B) You can't \novertake now (C) Turn Left\nReason：\nKey Concept：\nYou did not exceed the speed limit. But road \nis congested. So you can't overtake now.\ncongested road\nFigure 10: Three examples of PCA-Bench in the autonomous driving domain.\nYou are in a room, the goal is to water the \nplant. What is the best action to take next?\n(C) Use sprinkler water the plant\n(A) Find a sprinkler (B) Pick up the sprinkler \n(C) Use sprinkler water the plant\nThere is a house plant in front of us. We have \na sprinkler in hands. Therefore, our next step \nis to water the plant using the sprinkler.\nplant, sprinkler\nimage：\nQuetsion：\nYou are in a room, the goal is to wash the cup. \nWhat is the best action to take next?\nAction candidates：\nAnswer：\n(A) Find the cup\n(A) Find the cup (B) Find the dish sponge \n(C) Find a sink (D) Open the faucet\nReason：\nKey Concept：\nThere is no cup in front of us. Therefore, our \nnext step is to find a sink. \ncup, dish sponge, sink\nimage：\nQuetsion：\nAction candidates：\nAnswer：\nReason：\nKey Concept：\nimage：\nQuetsion：\nYou are in a room, the goal is to cook rice. \nWhat is the best action to take next? \nAction candidates：\nAnswer：\n(A) Go to the kitchen\n(A) Go to the kitchen (B) Pick up rice \n(C) Pick up pot (D) Boil water\nReason：\nKey Concept：\nWe can infer from the objects around us that \nwe are in the living room so our next step is to \ngo to the kitchen\nrice, kitchen, living room\nFigure 11: Three examples of PCA-Bench in the domestic robot domain.\n(C) craft iron sword \n(A) find cow (B) kill cow (C) craft iron sword \n(D) find iron ore (E) craft iron ingot\nYou need a cow and kill it with an iron sword for \nbeef. You're near a cow, but lack an iron sword. \nSo, craft one using two iron ingots and a stick. \nYou have two iron ingots, and two sticks so you \ncan craft an iron sword.\nCow nearby, No iron sword, 2 sticks,2 iron ingots\nimage：\nQuetsion：\nAction candidates：\nAnswer：\nReason：\nKey Concept：\nTo harvest beef using an iron sword, based on \nthe image, which is your next action? \n(A) find planks \n(A) find planks (B) craft crafting table (C) place \ncrafting table\nTo place a crafting table in front of you, your \ninventory must have a crafting table. To get a \ncrafting table in your inventory, you need to \nmake one. You need 4 planks to make a crafting \ntable. Since you have 3, find one more first.\nNo crafting table, 3 planks\nimage：\nQuetsion：\nAction candidates：\nAnswer：\nReason：\nKey Concept：\nTo place a crafting table in front of you, based on \nthe image, which is your next action? \n(B) milk cow\n(A) find cow (B) milk cow (C) craft bucket (D) \nfind iron ore (E) craft iron ingot\nTo get milk, you need to find a cow, and milk it \nwith a bucket. There is a cow nearby, and you \nhave a bucket, so you can milk the cow.\nCow nearby, Have bucket\nimage：\nQuetsion：\nAction candidates：\nAnswer：\nReason：\nKey Concept：\nTo harvest milk, which is your next action? \nFigure 12: Three examples of PCA-Bench in the open-world game domain.\n[Question]: {question}\n[Action Choices]: {actions}\n[Agent Answer]: {model_output}\n[Correct Action]: {true_action}\n[Key Concepts]: {key_concept}\n[Reference Reasoning Process]: {reason}\n[System]\nWe would like you to access the agent’s performance in the multimodal reasoning task about\ndomain. In this task, the agent is given an image, a [Question], and several candidate [Action\nChoices], and is asked to give an [Agent Answer] for the [Question]. The [Agent Answer]\nencapsulates the agent’s perception of the image’s [Key Concepts], the agent’s cognition reasoning\nprocess and the final selected action.\nWe request you to give three types of scores for the agent’s [Agent Answer] in comparison to the\ngiven [Key Concepts], [Reference Reasoning Process] and [Correct Action]:\n1. action score: If the selected action in the [Agent Answer] matches that of the [Correct Action],\nthe action score is 1; otherwise, it is 0.\n2. perception score: This score evaluates the model’s capability to perceive and interpret observa-\ntions. It is contingent on whether the [Agent Answer] includes any of the [Key Concepts] of the\ninstance. If it accurately describes any one of the [Key Concepts], the score is 1; otherwise, it is 0.\n3. cognition score: This score gauges the model’s ability to reason, comprehend, and make\ninformed decisions based on perceived input data and world knowledge. If the reasoning process\nin the [Agent Answer] aligns with the [Reference Reasoning Process], the score is 1; otherwise, it\nis 0.\nPlease note that there are only scores of 0 and 1.\nYou should carefully compare the [Agent Answer] with the [Correct Action], [Key Concepts] and\n[Reference Reasoning Process] to give your assessment.\nYou need first to give your assessment evidence and then the scores.\nYour output MUST contain 6 lines with the following format:\naction assessment evidence: (assessment evidence here)\naction score: (score here)\nperception assessment evidence: (assessment evidence here)\nperception score: (score here)\ncognition assessment evidence: (assessment evidence here)\ncognition score: (score here)\nTable 4: The template of querying GPT-4.\nE\nTraining Details\nTable 5 shows the specific parameters used for fine-\ntuning in different models. The PCA results on\nthe three domains of PCA bench before and after\nfine-tuning different models are shown in Figure\n13.\nFigure 13: Performance comparsion between models’ zero-shot results and models’ finetuned results with the data\ngenerated by Embodied-Instruct-Evolution (EIE) method. EIE improves the performance on all domains for both\nLLaVA1.5-13b and MMICL models.\nModel\nParameter\nValue\nLearning Rate\n2e-4\nUse Lora Finetuning?\nYes\nLora Rank\n8\nLora Alpha\n32\nQwen-VL-Chat\/LLaVA1.5-7\/13b\nGlobal Batchsize\n20\nWeight Decay\n0\nTrain Epochs\n5\nLr Scheduler Type\nCosine\nWarmup Ratio\n0.03\nLearning Rate\n5e-4\nUse Lora Finetuning?\nNo\nGlobal Batchsize\n20\nMMICL\nWeight Decay\n5e-4\nTrain Epochs\n5\nLr Scheduler Type\nLinear\nWarmup Ratio\n0.2\nTable 5: Training details for different models with EIE.\nF\nDoes Chain-of-Thought Finetuning\nImprove Cross-modal Reasoning?\nUnlike vanilla finetuning, which solely focuses on\ndelivering direct answers, Chain-of-Thought Fine-\ntuning necessitates the model to first articulate its\nreasoning before presenting the answer. This ap-\nproach has been demonstrated to be a highly effec-\ntive instruction tuning paradigm for LLMs (Chung\net al., 2022; Kim et al., 2023). We have incorpo-\nrated this methodology in our previous finetuning\nexperiments.\nTo further evaluate its impact, we conducted an\nablation study where the reasoning process was\nomitted from the target output during the training\nof MLLMs. We then assessed the variations in\naction scores on the test set. As depicted in Fig-\nure 14, to our surprise, the figures suggest that\nChain-of-Thought finetuning exerts a relatively mi-\nnor influence when compared to conventional label\nfinetuning. We have noticed that similar phenom-\nena has been identified by Zhang et al. (2023) that\nstandard CoT finetuning does not work for MLLMs\nin their explorations.\nWe think there are three potential explanations:\n1) Task Variation: Contrary to mathematics datasets\nlike GSM8K, the current task doesn’t require multi-\nstep complex reasoning to arrive at the final answer\nand the automatic generated CoTs have noise. 2)\nModality Discrepancy: The CoT capability, inher-\nent in LLMs, is only moderately adjusted for vi-\nsual input for current open-source MLLMs. This\nadaptation process could potentially impair the rea-\nsoning ability. 3) Short Cut in Pretraining: We\nthink a deeper reason might lie in the short-cut\nduring pretraining period of current open-source\nMLLMs, which are pretrained on simple image\ncaption task in a large scale. Those captions are\nusually short and lose a lot of information about the\noriginal image. What’s more important is that the\nreasoning ability of LLM is not utilized during the\npretraining stage, which might hurt the reasoning\nability of LLM during the SFT period. We defer\nto future research how to effectively harness the\nCoT capabilities of LLMs to enhance embodied\ndecision-making processes.\nFigure 14: Action scores changes when training without\nreasoning process for different models. The benefit of\nCoT finetuning is not consistent among models. Blue\nmeans difference of action score for LLaVA1.5-7b, Or-\nange means difference of action score for LLaVA1.5-\n13b and green means difference of action score for\nQwen-VL-Chat.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain.pdf"}
