# Paper List of Terms(MLLM+game)
- [25/03] **How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game**  
[[Paper](http://arxiv.org/pdf/2503.10042v1)] [[Code/Page]()] [[TLDR/Notes](#how-do-multimodal-large-language-models-handle-complex-multimodal-reasoning?-placing-them-in-an-extensible-escape-game)]

- [24/12] **From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons**  
[[Paper](http://arxiv.org/pdf/2412.08442v1)] [[Code/Page]()] [[TLDR/Notes](#from-multimodal-llms-to-generalist-embodied-agents-methods-and-lessons)]

- [24/10] **Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis**  
[[Paper](http://arxiv.org/pdf/2410.07155v1)] [[Code/Page](https://github.com/YangLing0818/Trans4D)] [[TLDR/Notes](#trans4d-realistic-geometry-aware-transition-for-compositional-text-to-4d-synthesis)]

- [24/10] **ING-VP: MLLMs cannot Play Easy Vision-based Games Yet**  
[[Paper](http://arxiv.org/pdf/2410.06555v1)] [[Code/Page](https://github.com/Thisisus7/ING-VP.git.)] [[TLDR/Notes](#ing-vp-mllms-cannot-play-easy-vision-based-games-yet)]

- [24/08] **Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs**  
[[Paper](http://arxiv.org/pdf/2408.01417v1)] [[Code/Page](https://github.com/lil-lab/ICCA.)] [[TLDR/Notes](#talk-less,-interact-better-evaluating-in-context-conversational-adaptation-in-multimodal-llms)]

- [24/06] **MLVU: Benchmarking Multi-task Long Video Understanding**  
[[Paper](http://arxiv.org/pdf/2406.04264v3)] [[Code/Page]()] [[TLDR/Notes](#mlvu-benchmarking-multi-task-long-video-understanding)]

- [24/04] **A Survey on Large Language Model-Based Game Agents**  
[[Paper](http://arxiv.org/pdf/2404.02039v1)] [[Code/Page](https://github.com/git-disl/awesome-LLM-game-agent-papers.)] [[TLDR/Notes](#a-survey-on-large-language-model-based-game-agents)]

- [24/02] **GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data**  
[[Paper](http://arxiv.org/pdf/2402.14973v4)] [[Code/Page]()] [[TLDR/Notes](#genception-evaluate-vision-llms-with-unlabeled-unimodal-data)]

- [24/02] **PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain**  
[[Paper](http://arxiv.org/pdf/2402.15527v1)] [[Code/Page]()] [[TLDR/Notes](#pca-bench-evaluating-multimodal-large-language-models-in-perception-cognition-action-chain)]



# TLDR/Notes
## How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game
### Abstract
The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred
interest in complex multimodal reasoning tasks in the real-world and virtual
environment, which require coordinating multiple abilities, including visual
perception, visual reasoning, spatial awareness, and target deduction. However,
existing evaluations primarily assess the final task completion, often
degrading assessments to isolated abilities such as visual grounding and visual
question answering. Less attention is given to comprehensively and
quantitatively analyzing reasoning process in multimodal environments, which is
crucial for understanding model behaviors and underlying reasoning mechanisms
beyond merely task success. To address this, we introduce MM-Escape, an
extensible benchmark for investigating multimodal reasoning, inspired by
real-world escape games. MM-Escape emphasizes intermediate model behaviors
alongside final task completion. To achieve this, we develop EscapeCraft, a
customizable and open environment that enables models to engage in free-form
exploration for assessing multimodal reasoning. Extensive experiments show that
MLLMs, regardless of scale, can successfully complete the simplest room escape
tasks, with some exhibiting human-like exploration strategies. Yet, performance
dramatically drops as task difficulty increases. Moreover, we observe that
performance bottlenecks vary across models, revealing distinct failure modes
and limitations in their multimodal reasoning abilities, such as repetitive
trajectories without adaptive exploration, getting stuck in corners due to poor
visual spatial awareness, and ineffective use of acquired props, such as the
key. We hope our work sheds light on new challenges in multimodal reasoning,
and uncovers potential improvements in MLLMs capabilities.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ï¼Ÿé€šè¿‡å¯æ‰©å±•çš„é€ƒè„±æ¸¸æˆè¿›è¡Œè¯„ä¼°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å¯¹ç°å®ä¸–ç•Œå’Œè™šæ‹Ÿç¯å¢ƒä¸­å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åè°ƒå¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€è§†è§‰æ¨ç†ã€ç©ºé—´æ„è¯†å’Œç›®æ ‡æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨æœ€ç»ˆçš„ä»»åŠ¡å®Œæˆæƒ…å†µï¼Œå¾€å¾€å°†è¯„ä¼°é™çº§ä¸ºå­¤ç«‹çš„èƒ½åŠ›ï¼Œå¦‚è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ã€‚è¾ƒå°‘å…³æ³¨åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­å…¨é¢å’Œå®šé‡åœ°åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œè¿™å¯¹äºç†è§£æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†æœºåˆ¶è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MM-Escapeï¼Œä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†ï¼Œç”¨äºç ”ç©¶å¤šæ¨¡æ€æ¨ç†ï¼Œçµæ„Ÿæ¥è‡ªç°å®ä¸–ç•Œçš„é€ƒè„±æ¸¸æˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMM-EscapeåŸºå‡†
MM-Escapeå¼ºè°ƒæœ€ç»ˆä»»åŠ¡å®Œæˆå’Œä¸­é—´æ¨¡å‹è¡Œä¸ºã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæœ¬æ–‡å¼€å‘äº†EscapeCraftï¼Œä¸€ä¸ªå¯å®šåˆ¶çš„å¼€æ”¾ç¯å¢ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè‡ªç”±æ¢ç´¢ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šEscapeCraftç¯å¢ƒ
EscapeCraftæ˜¯ä¸€ä¸ªå¯å®šåˆ¶çš„å¼€æ”¾ç¯å¢ƒï¼Œå…è®¸æ¨¡å‹è¿›è¡Œè‡ªç”±æ¢ç´¢ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨ç†ã€‚å®ƒæ”¯æŒè‡ªå®šä¹‰å’Œå¯æ‰©å±•çš„åœºæ™¯ç”Ÿæˆï¼Œå¹¶å®šä¹‰äº†ä»»åŠ¡çš„æ¨ç†è·¯å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºè§„æ¨¡å¤§å°ï¼ŒMLLMséƒ½èƒ½æˆåŠŸå®Œæˆæœ€ç®€å•çš„æˆ¿é—´é€ƒè„±ä»»åŠ¡ï¼Œå…¶ä¸­ä¸€äº›æ¨¡å‹è¡¨ç°å‡ºç±»ä¼¼äººç±»çš„æ¢ç´¢ç­–ç•¥ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡éš¾åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿåˆ°æ€§èƒ½ç“¶é¢ˆå› æ¨¡å‹è€Œå¼‚ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸åŒå¤±è´¥æ¨¡å¼å’Œå±€é™æ€§ï¼Œä¾‹å¦‚æ²¡æœ‰è‡ªé€‚åº”æ¢ç´¢çš„é‡å¤è½¨è¿¹ã€ç”±äºè§†è§‰ç©ºé—´æ„è¯†å·®è€Œé™·å…¥è§’è½ï¼Œä»¥åŠæ— æ•ˆåœ°ä½¿ç”¨è·å¾—çš„é“å…·ï¼Œå¦‚é’¥åŒ™ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„MM-EscapeåŸºå‡†å’ŒEscapeCraftç¯å¢ƒä¸ºè¯„ä¼°å’Œæ”¹è¿›MLLMsçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚é€šè¿‡å…³æ³¨ä¸­é—´æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥æ›´å…¨é¢åœ°ç†è§£æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†æœºåˆ¶ï¼Œä»è€Œæ¨åŠ¨MLLMsåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„å‘å±•ã€‚

## From Multimodal LLMs to Generalist Embodied Agents Methods and Lessons
### Abstract
We examine the capability of Multimodal Large Language Models (MLLMs) to
tackle diverse domains that extend beyond the traditional language and vision
tasks these models are typically trained on. Specifically, our focus lies in
areas such as Embodied AI, Games, UI Control, and Planning. To this end, we
introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).
GEA is a single unified model capable of grounding itself across these varied
domains through a multi-embodiment action tokenizer. GEA is trained with
supervised learning on a large dataset of embodied experiences and with online
RL in interactive simulators. We explore the data and algorithmic choices
necessary to develop such a model. Our findings reveal the importance of
training with cross-domain data and online RL for building generalist agents.
The final GEA model achieves strong generalization performance to unseen tasks
across diverse benchmarks compared to other generalist models and
benchmark-specific approaches.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åˆ°é€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼šæ–¹æ³•ä¸ç»éªŒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­è¨€å’Œè§†è§‰ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åªåœ¨ç‰¹å®šçš„è®­ç»ƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œéš¾ä»¥åº”å¯¹å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œåœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢MLLMsåœ¨æ›´å¹¿æ³›é¢†åŸŸä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å…·èº«AIã€æ¸¸æˆã€UIæ§åˆ¶å’Œè§„åˆ’ç­‰é¢†åŸŸã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼ˆGEAï¼‰
æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†MLLMsè½¬åŒ–ä¸ºé€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼ˆGEAï¼‰çš„æ–¹æ³•ã€‚GEAæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å¤šå…·èº«åŠ¨ä½œåˆ†è¯å™¨åœ¨å„ä¸ªé¢†åŸŸä¸­è¿›è¡Œè‡ªæˆ‘å®šä½ã€‚GEAé€šè¿‡åœ¨å¤§è§„æ¨¡å…·èº«ç»éªŒæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šå…·èº«åŠ¨ä½œåˆ†è¯å™¨
ä¸ºäº†ä½¿GEAèƒ½å¤Ÿæ§åˆ¶å¤šæ ·åŒ–çš„å…·èº«å½¢æ€ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„å­¦ä¹ åˆ†è¯æœºåˆ¶ï¼Œç”¨äºå¤„ç†æ‰€æœ‰è¿ç»­å’Œç¦»æ•£çš„åŠ¨ä½œç©ºé—´ã€‚è¿™ç§æœºåˆ¶å°†è¿ç»­åŠ¨ä½œæ˜ å°„åˆ°ä¸€ç³»åˆ—æ–°çš„åˆ†è¯ï¼Œä»è€Œä½¿å¾—MLLMsèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œè¿™äº›åŠ¨ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
GEAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ç°æœ‰çš„é€šç”¨æ™ºèƒ½ä½“æ¨¡å‹ç›¸æ¯”ï¼Œç”šè‡³åœ¨æŸäº›ç‰¹å®šé¢†åŸŸä¸­çš„è¡¨ç°ä¹Ÿä¼˜äºæˆ–æ¥è¿‘äºä¸“ä¸šç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨CALVINæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGEAåœ¨å¤„ç†æœªè§è¿‡çš„æŒ‡ä»¤å’ŒèƒŒæ™¯æ—¶è¾¾åˆ°äº†90%çš„æˆåŠŸç‡ï¼Œæ¯”ç±»ä¼¼æ–¹æ³•é«˜å‡ºè¿‘10%ï¼Œå¹¶ä¸”ä¸ä¸“ä¸šç³»ç»Ÿçš„æ€§èƒ½ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è·¨é¢†åŸŸæ•°æ®è¿›è¡Œè®­ç»ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ å¯¹äºæ„å»ºé€šç”¨æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†é¢„è®­ç»ƒMLLMsåœ¨GEAæ¶æ„ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡å¯¹æ€§èƒ½çš„å½±å“ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥å¼€å‘æ›´å¼ºå¤§çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºå’ŒæŒ‡å¯¼ã€‚

## Trans4D Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis
### Abstract
Recent advances in diffusion models have demonstrated exceptional
capabilities in image and video generation, further improving the effectiveness
of 4D synthesis. Existing 4D generation methods can generate high-quality 4D
objects or scenes based on user-friendly conditions, benefiting the gaming and
video industries. However, these methods struggle to synthesize significant
object deformation of complex 4D transitions and interactions within scenes. To
address this challenge, we propose Trans4D, a novel text-to-4D synthesis
framework that enables realistic complex scene transitions. Specifically, we
first use multi-modal large language models (MLLMs) to produce a physic-aware
scene description for 4D scene initialization and effective transition timing
planning. Then we propose a geometry-aware 4D transition network to realize a
complex scene-level 4D transition based on the plan, which involves expressive
geometrical object deformation. Extensive experiments demonstrate that Trans4D
consistently outperforms existing state-of-the-art methods in generating 4D
scenes with accurate and high-quality transitions, validating its
effectiveness. Code: https://github.com/YangLing0818/Trans4D
### ğŸŒŸ è®ºæ–‡è§£è¯» | Trans4Dï¼šåŸºäºæ–‡æœ¬çš„4Dåœºæ™¯åˆæˆæ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢çš„çªç ´ï¼Œ4DåˆæˆæŠ€æœ¯ä¹Ÿå¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚ç°æœ‰çš„4Dç”Ÿæˆæ–¹æ³•å¯ä»¥åŸºäºç”¨æˆ·å‹å¥½çš„æ¡ä»¶ç”Ÿæˆé«˜è´¨é‡çš„4Då¯¹è±¡æˆ–åœºæ™¯ï¼Œä¸ºæ¸¸æˆå’Œè§†é¢‘è¡Œä¸šå¸¦æ¥äº†å·¨å¤§ä»·å€¼ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åˆæˆå¤æ‚4Dè¿‡æ¸¡å’Œåœºæ™¯å†…äº¤äº’ä¸­çš„æ˜¾è‘—å¯¹è±¡å˜å½¢æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç‰©ç†æ„ŸçŸ¥çš„4Dè¿‡æ¸¡è§„åˆ’
Trans4Dåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆç‰©ç†æ„ŸçŸ¥çš„åœºæ™¯æè¿°ï¼Œç”¨äº4Dåœºæ™¯åˆå§‹åŒ–å’Œæœ‰æ•ˆçš„è¿‡æ¸¡æ—¶é—´è§„åˆ’ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç¡®ä¿4Dåœºæ™¯çš„ç‰©ç†åˆç†æ€§å’Œç©ºé—´å…³ç³»çš„æ­£ç¡®æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå‡ ä½•æ„ŸçŸ¥çš„4Dè¿‡æ¸¡ç½‘ç»œ
Trans4Dæå‡ºäº†ä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥çš„è¿‡æ¸¡ç½‘ç»œï¼ˆTransNetï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è§„åˆ’åŠ¨æ€åœ°ç¡®å®š4Dåœºæ™¯ä¸­æ¯ä¸ªç‚¹äº‘æ˜¯å¦åº”è¯¥å‡ºç°æˆ–æ¶ˆå¤±ã€‚è¿™ä½¿å¾—Trans4Dèƒ½å¤Ÿè‡ªç„¶åœ°å¤„ç†å¤§è§„æ¨¡çš„å¯¹è±¡å˜å½¢ï¼Œä¾‹å¦‚å¯¼å¼¹å˜æˆçˆ†ç‚¸äº‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Trans4Dåœ¨ç”Ÿæˆå…·æœ‰å‡†ç¡®å’Œé«˜è´¨é‡è¿‡æ¸¡çš„4Dåœºæ™¯æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Trans4Dä¸º4Dåœºæ™¯åˆæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶ç‰©ç†æ„ŸçŸ¥çš„è¿‡æ¸¡è§„åˆ’å’Œå‡ ä½•æ„ŸçŸ¥çš„è¿‡æ¸¡ç½‘ç»œä¸ºç”Ÿæˆå…·æœ‰å¤æ‚äº¤äº’å’Œæ˜¾è‘—å˜å½¢çš„4Dåœºæ™¯æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒTrans4Dçš„è®­ç»ƒç­–ç•¥å¹³è¡¡äº†æ•ˆç‡å’Œè´¨é‡ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ç”Ÿæˆé«˜è´¨é‡çš„4Dåœºæ™¯ã€‚

## ING-VP MLLMs cannot Play Easy Vision-based Games Yet
### Abstract
As multimodal large language models (MLLMs) continue to demonstrate
increasingly competitive performance across a broad spectrum of tasks, more
intricate and comprehensive benchmarks have been developed to assess these
cutting-edge models. These benchmarks introduce new challenges to core
capabilities such as perception, reasoning, and planning. However, existing
multimodal benchmarks fall short in providing a focused evaluation of
multi-step planning based on spatial relationships in images. To bridge this
gap, we present ING-VP, the first INteractive Game-based Vision Planning
benchmark, specifically designed to evaluate the spatial imagination and
multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,
encompassing 300 levels, each with 6 unique configurations. A single model
engages in over 60,000 rounds of interaction. The benchmark framework allows
for multiple comparison settings, including image-text vs. text-only inputs,
single-step vs. multi-step reasoning, and with-history vs. without-history
conditions, offering valuable insights into the model's capabilities. We
evaluated numerous state-of-the-art MLLMs, with the highest-performing model,
Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the
anticipated standard. This work aims to provide a specialized evaluation
framework to drive advancements in MLLMs' capacity for complex spatial
reasoning and planning. The code is publicly available at
https://github.com/Thisisus7/ING-VP.git.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ING-VPï¼šå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è§„åˆ’ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºè¶Šæ¥è¶Šå¼ºçš„ç«äº‰åŠ›ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„å¤æ‚æ€§å’Œå…¨é¢æ€§çš„åŸºå‡†æµ‹è¯•ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°åŸºäºå›¾åƒç©ºé—´å…³ç³»çš„å¤šæ­¥è§„åˆ’èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†ING-VPï¼Œä¸€ä¸ªåŸºäºäº¤äº’å¼æ¸¸æˆçš„è§†è§‰è§„åˆ’åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šING-VPåŸºå‡†æµ‹è¯•
ING-VPåŸºå‡†æµ‹è¯•åŒ…å«6ä¸ªä¸åŒçš„æ¸¸æˆï¼Œæ¶µç›–300ä¸ªå…³å¡ï¼Œæ¯ä¸ªå…³å¡æœ‰6ç§ç‹¬ç‰¹çš„é…ç½®ã€‚å•ä¸ªæ¨¡å‹å‚ä¸è¶…è¿‡60,000è½®äº¤äº’ã€‚åŸºå‡†æµ‹è¯•æ¡†æ¶å…è®¸å¤šç§æ¯”è¾ƒè®¾ç½®ï¼ŒåŒ…æ‹¬å›¾åƒ-æ–‡æœ¬ä¸ä»…æ–‡æœ¬è¾“å…¥ã€å•æ­¥ä¸å¤šæ­¥æ¨ç†ä»¥åŠå¸¦å†å²ä¸ä¸å¸¦å†å²æ¡ä»¶ï¼Œä¸ºæ¨¡å‹çš„èƒ½åŠ›æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°MLLMsçš„èƒ½åŠ›
æœ¬æ–‡è¯„ä¼°äº†ä¼—å¤šæœ€å…ˆè¿›çš„MLLMsï¼Œå‘ç°å³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹Claude-3.5 Sonnetï¼Œå…¶å¹³å‡å‡†ç¡®ç‡ä¹Ÿåªæœ‰3.37%ï¼Œè¿œä½äºé¢„æœŸæ ‡å‡†ã€‚è¿™è¡¨æ˜å½“å‰çš„MLLMsåœ¨ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥è§„åˆ’èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹Claude-3.5 Sonnetï¼Œåœ¨ING-VPåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¹Ÿåªæœ‰3.37%ï¼Œè¿œä½äºäººç±»åœ¨è¿™äº›ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™è¡¨æ˜å½“å‰çš„MLLMsåœ¨ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥è§„åˆ’èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ING-VPåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°MLLMsçš„ç©ºé—´æƒ³è±¡åŠ›å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªä¸“é—¨çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨MLLMsåœ¨å¤æ‚ç©ºé—´æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æ–¹é¢çš„è¿›æ­¥ã€‚æ­¤å¤–ï¼ŒING-VPåŸºå‡†æµ‹è¯•çš„ç»“æœä¹Ÿä¸ºMLLMsçš„æœªæ¥è®¾è®¡å’Œè®­ç»ƒç­–ç•¥æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## Talk Less, Interact Better Evaluating In-context Conversational Adaptation in Multimodal LLMs
### Abstract
Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹è¯é€‚åº”æ€§è¯„ä¼°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»åœ¨äº¤æµè¿‡ç¨‹ä¸­ä¼šè‡ªå‘åœ°ä½¿ç”¨è¶Šæ¥è¶Šé«˜æ•ˆçš„è¯­è¨€ï¼Œé€šè¿‡é€‚åº”å’Œå½¢æˆä¸´æ—¶çš„çº¦å®šæ¥æé«˜æ²Ÿé€šæ•ˆç‡ã€‚è¿™ç§ç°è±¡åœ¨å‚è€ƒæ¸¸æˆä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†äººç±»è¯­è¨€çš„ç‰¹æ€§ï¼Œè¶…è¶Šäº†ä»…ä»…ä¼ è¾¾æ„å›¾çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯å¦åœ¨äº¤äº’è¿‡ç¨‹ä¸­åŒæ ·èƒ½å¤Ÿæé«˜æ²Ÿé€šæ•ˆç‡ï¼Œä»¥åŠå®ƒä»¬å¯èƒ½é‡‡ç”¨å“ªäº›æœºåˆ¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ICCAï¼ˆIn-context Conversational Adaptationï¼‰ï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°MLLMsåœ¨ä¸Šä¸‹æ–‡ä¸­çš„å¯¹è¯é€‚åº”æ€§ã€‚ICCAä½¿ç”¨äººç±»-äººç±»å‚è€ƒæ¸¸æˆäº¤äº’çš„è¯­æ–™åº“ï¼Œå…è®¸å®Œå…¨è‡ªåŠ¨åŒ–çš„è¯„ä¼°ï¼Œæ— éœ€è¿›ä¸€æ­¥çš„äººç±»äº¤äº’ï¼Œä½¿å…¶æ˜“äºéƒ¨ç½²ä»¥åˆ†ææ–°æ¨¡å‹ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿäº†æ¨¡å‹ä¸äººç±»äº¤äº’çš„åœºæ™¯ï¼Œé€šè¿‡æ¯”è¾ƒæ¨¡å‹è¡Œä¸ºçš„å˜åŒ–æ¥è¯„ä¼°å…¶å¯¹è¯é€‚åº”æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†äº”ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„MLLMsï¼šIDEFICSã€LLaVa-1.5ã€GPT4-visionã€Gemini 1.0 Pro Visionå’ŒClaude 3 opusã€‚ç ”ç©¶å‘ç°ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½éš¾ä»¥è‡ªå‘åœ°å¼•å…¥çº¦å®šå¹¶ä½œä¸ºè¯´è¯è€…è¿›è¡Œé€‚åº”ã€‚é€šè¿‡ä¸ºæ¯ä¸ªæ¨¡å‹è®¾è®¡ç‰¹å®šçš„æŒ‡ä»¤ï¼Œå¯ä»¥éƒ¨åˆ†è§£å†³è¿™ä¸€é—®é¢˜ã€‚æœ€å¼ºçš„æ¨¡å‹ï¼ˆGPT4ã€Geminiå’ŒClaudeï¼‰å¯ä»¥é€æ¸ä½¿ç”¨æ›´çŸ­çš„æ¶ˆæ¯ï¼ˆè·å¾—è¯æ±‡æ•ˆç‡ï¼‰ï¼Œä½†ä»ç„¶éš¾ä»¥å®ç°æ”¶æ•›æˆ–ç¨³å®šæ€§ï¼Œè¿™é˜»ç¢äº†çœŸæ­£é«˜æ•ˆæ²Ÿé€šçš„å‡ºç°ã€‚å½“ä½œä¸ºå¬ä¼—æ—¶ï¼ŒGPT4æ˜¾ç¤ºå‡ºæ¥è¿‘äººç±»çš„é€‚åº”è¶‹åŠ¿ï¼Œéšç€äº¤äº’çš„è¿›è¡Œï¼Œå…¶å‡†ç¡®æ€§ä¸æ–­æé«˜ï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™è¡¨ç°å‡ºè¾ƒå°‘çš„è¿™ç§è¡Œä¸ºæˆ–ä»…åœ¨ç®€åŒ–çš„è®¾ç½®ä¸‹è¡¨ç°å‡ºè¿™ç§è¡Œä¸ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰çš„MLLMså¯èƒ½è¢«åŠ¨åœ°ç†è§£å…¶å¯¹è¯è€…çš„ä¸æ–­å‘å±•å˜åŒ–çš„è¯­è¨€ï¼Œä½†å®ƒä»¬é€‚åº”è‡ªèº«è¯­è¨€ä»¥æé«˜æ²Ÿé€šæ•ˆç‡çš„èƒ½åŠ›å¹¶æ²¡æœ‰è‡ªç„¶åœ°ä»å®ƒä»¬çš„è®­ç»ƒæˆ–æŒ‡ä»¤è°ƒæ•´ä¸­äº§ç”Ÿã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†é‡è¦çš„æ–¹å‘ï¼ŒåŒ…æ‹¬æé«˜æ¨¡å‹çš„è‡ªå‘è¯­è¨€æ•ˆç‡ã€ä¿æŒè¯­è¨€ä¸€è‡´æ€§ã€é¿å…è¿‡åº¦é‡å¤å€¾å‘ä»¥åŠå¤„ç†å•ä¸ªæŸ¥è¯¢ä¸­çš„æ›´å¤šå›¾åƒã€‚

## MLVU Benchmarking Multi-task Long Video Understanding
### Abstract
The evaluation of Long Video Understanding (LVU) performance poses an
important but challenging research problem. Despite previous efforts, the
existing video understanding benchmarks are severely constrained by several
issues, especially the insufficient lengths of videos, a lack of diversity in
video types and evaluation tasks, and the inappropriateness for evaluating LVU
performances. To address the above problems, we propose a new benchmark called
MLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and
in-depth evaluation of LVU. MLVU presents the following critical values:
\textit{1)} The substantial and flexible extension of video lengths, which
enables the benchmark to evaluate LVU performance across a wide range of
durations. \textit{2)} The inclusion of various video genres, e.g., movies,
surveillance footage, egocentric videos, cartoons, game videos, etc., which
reflects the models' LVU performances in different scenarios. \textit{3)} The
development of diversified evaluation tasks, which enables a comprehensive
examination of MLLMs' key abilities in long-video understanding. The empirical
study with 23 latest MLLMs reveals significant room for improvement in today's
technique, as all existing methods struggle with most of the evaluation tasks
and exhibit severe performance degradation when handling longer videos.
Additionally, it suggests that factors such as context length,
image-understanding ability, and the choice of LLM backbone can play critical
roles in future advancements. We anticipate that MLVU will advance the research
of long video understanding by providing a comprehensive and in-depth analysis
of MLLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MLVUï¼šå¤šä»»åŠ¡é•¿è§†é¢‘ç†è§£åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€èƒ½åŠ›ä¸Šçš„æ‰©å±•ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†ä¸»è¦é’ˆå¯¹çŸ­è§†é¢‘ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°MLLMsåœ¨å¤„ç†é•¿è§†é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰åŸºå‡†åœ¨è§†é¢‘ç±»å‹å’Œè¯„ä¼°ä»»åŠ¡çš„å¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”è¯„ä¼°ä»»åŠ¡çš„è®¾è®¡å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨é•¿è§†é¢‘ä¸­çš„å¤æ‚ä¿¡æ¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†é¢‘é•¿åº¦çš„å¤§å¹…æ‰©å±•
MLVUåŸºå‡†åŒ…å«æ—¶é•¿ä»3åˆ†é’Ÿåˆ°2å°æ—¶ä¸ç­‰çš„å¤šæ ·åŒ–é•¿è§†é¢‘ï¼Œå¹³å‡è§†é¢‘é•¿åº¦çº¦ä¸º15åˆ†é’Ÿï¼Œè¿œè¶…ç°æœ‰åŸºå‡†ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªè§†é¢‘è¢«è¿›ä¸€æ­¥åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œä»¥ä¾¿é’ˆå¯¹ä¸åŒç‰‡æ®µåˆ›å»ºè¯„ä¼°ä»»åŠ¡ï¼Œä»è€Œçµæ´»è¯„ä¼°MLLMsåœ¨ä¸åŒè§†é¢‘é•¿åº¦ä¸‹çš„è¡¨ç°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–çš„è§†é¢‘ç±»å‹
MLVUåŸºå‡†æ¶µç›–äº†å¤šç§è§†é¢‘ç±»å‹ï¼ŒåŒ…æ‹¬ç”µå½±ã€ç”Ÿæ´»è®°å½•ã€ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ã€ä½“è‚²ã€æ•™ç¨‹ã€ç›‘æ§å½•åƒç­‰çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œä»¥åŠåŠ¨ç”»ç³»åˆ—å’Œæ¸¸æˆè§†é¢‘ç­‰æ¨¡æ‹Ÿè§†é¢‘ã€‚è¿™ç§å¤šæ ·æ€§ä½¿å¾—MLLMsåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„è¡¨ç°å¯ä»¥å¾—åˆ°å…¨é¢è¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ ·åŒ–çš„è¯„ä¼°ä»»åŠ¡
MLVUåŸºå‡†åŒ…å«9ä¸ªä¸åŒçš„è¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsåœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„å…³é”®èƒ½åŠ›ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ï¼Œä»¥åŠéœ€è¦åˆ©ç”¨æ•´ä¸ªè§†é¢‘çš„å…¨å±€ä¿¡æ¯æˆ–ç‰¹å®šç‰‡æ®µçš„å±€éƒ¨ä¿¡æ¯çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¶‰åŠå±€éƒ¨ä¿¡æ¯çš„ä»»åŠ¡éƒ½å¸¦æœ‰æ˜ç¡®çš„ä¸Šä¸‹æ–‡æ ‡æ³¨ï¼Œè¦æ±‚MLLMsèƒ½å¤Ÿå‡†ç¡®å®šä½æˆ–æ¨æ–­é•¿è§†é¢‘ä¸­çš„ç›¸å…³ç‰‡æ®µã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¯¹23ä¸ªæœ€æ–°çš„MLLMsè¿›è¡Œå®è¯ç ”ç©¶ï¼ŒMLVUåŸºå‡†æ­ç¤ºäº†ç°æœ‰æŠ€æœ¯åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ‰€æœ‰ç°æœ‰æ–¹æ³•åœ¨å¤§å¤šæ•°è¯„ä¼°ä»»åŠ¡ä¸Šéƒ½è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ›´é•¿çš„è§†é¢‘æ—¶æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ã€å›¾åƒç†è§£èƒ½åŠ›å’ŒLLMéª¨å¹²çš„é€‰æ‹©ç­‰å› ç´ å¯¹æœªæ¥é•¿è§†é¢‘ç†è§£æŠ€æœ¯çš„è¿›æ­¥èµ·ç€å…³é”®ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MLVUåŸºå‡†ä¸ºé•¿è§†é¢‘ç†è§£ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢è€Œæ·±å…¥çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜äº†è§£MLLMsåœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œå¹¶ä¸ºæœªæ¥çš„æŠ€æœ¯æ”¹è¿›æä¾›æ–¹å‘ã€‚æ­¤å¤–ï¼ŒMLVUåŸºå‡†çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡è¯„ä¼°åŸºå‡†çš„å¼€å‘æä¾›å‚è€ƒã€‚

## A Survey on Large Language Model-Based Game Agents
### Abstract
The development of game agents holds a critical role in advancing towards
Artificial General Intelligence (AGI). The progress of LLMs and their
multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve
and empower game agents with human-like decision-making capabilities in complex
computer game environments. This paper provides a comprehensive overview of
LLM-based game agents from a holistic viewpoint. First, we introduce the
conceptual architecture of LLM-based game agents, centered around six essential
functional components: perception, memory, thinking, role-playing, action, and
learning. Second, we survey existing representative LLM-based game agents
documented in the literature with respect to methodologies and adaptation
agility across six genres of games, including adventure, communication,
competition, cooperation, simulation, and crafting & exploration games.
Finally, we present an outlook of future research and development directions in
this burgeoning field. A curated list of relevant papers is maintained and made
accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ¸¸æˆä»£ç†ï¼šè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…¶å¤šæ¨¡æ€ç‰ˆæœ¬ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆä»£ç†å¤§å¤šåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œåœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­ç¼ºä¹ç±»ä¼¼äººç±»çš„å†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨å¦‚ä½•åˆ©ç”¨LLMsæ„å»ºå…·æœ‰äººç±»å†³ç­–èƒ½åŠ›çš„æ¸¸æˆä»£ç†ï¼Œä»¥æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºåŸºäºLLMsçš„æ¸¸æˆä»£ç†ï¼ˆLLMGAsï¼‰ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å…¶å…­ä¸ªæ ¸å¿ƒåŠŸèƒ½ç»„ä»¶ï¼š

1. **æ„ŸçŸ¥**ï¼šLLMGAsé€šè¿‡å¤šç§æ–¹å¼æ„ŸçŸ¥æ¸¸æˆç¯å¢ƒï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰ã€å£°éŸ³ç­‰ï¼Œä»¥è·å–å¿…è¦çš„æ¸¸æˆçŠ¶æ€ä¿¡æ¯ã€‚
2. **è®°å¿†**ï¼šLLMGAsåˆ©ç”¨è®°å¿†ç³»ç»Ÿå­˜å‚¨è¿‡å»çš„è§‚å¯Ÿã€æ€è€ƒå’Œè¡ŒåŠ¨ï¼Œä»¥ä¾¿åœ¨æœªæ¥çš„å†³ç­–ä¸­æ£€ç´¢å’Œåˆ©ç”¨ã€‚
3. **æ€è€ƒ**ï¼šLLMGAsé€šè¿‡æ¨ç†å’Œè§„åˆ’è¿›è¡Œå†³ç­–ï¼Œæ¨ç†åŒ…æ‹¬æ¼”ç»ã€å½’çº³å’Œå‡è®¾æ¨ç†ï¼Œè§„åˆ’åˆ™å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡ã€‚
4. **è§’è‰²æ‰®æ¼”**ï¼šLLMGAsèƒ½å¤Ÿæ¨¡æ‹Ÿæ¸¸æˆä¸­çš„ä¸åŒè§’è‰²ï¼Œç”Ÿæˆç¬¦åˆè§’è‰²ç‰¹å¾å’Œç›®æ ‡çš„å¯¹è¯å’Œè¡Œä¸ºã€‚
5. **è¡ŒåŠ¨**ï¼šLLMGAså°†ç”Ÿæˆçš„æ–‡æœ¬å†³ç­–è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨ï¼Œä»¥ä¸æ¸¸æˆç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚
6. **å­¦ä¹ **ï¼šLLMGAsé€šè¿‡ç§¯ç´¯æ¸¸æˆç»éªŒå’Œç¯å¢ƒåé¦ˆï¼Œä¸æ–­æ”¹è¿›å…¶è®¤çŸ¥å’Œæ¸¸æˆèƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹ç°æœ‰æ–‡çŒ®è¿›è¡Œäº†ç»¼è¿°ï¼Œæ¶µç›–äº†å…­ç§ç±»å‹çš„æ¸¸æˆï¼ŒåŒ…æ‹¬å†’é™©ã€æ²Ÿé€šã€ç«äº‰ã€åˆä½œã€æ¨¡æ‹Ÿå’Œåˆ¶ä½œä¸æ¢ç´¢æ¸¸æˆã€‚å¯¹äºæ¯ç§ç±»å‹çš„æ¸¸æˆï¼Œæœ¬æ–‡åˆ†æäº†æŠ€æœ¯æŒ‘æˆ˜ã€æ”¯æŒçš„æ¸¸æˆç¯å¢ƒå’Œå¸¸ç”¨çš„ä¼˜åŒ–ç­–ç•¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„LLMGAsæ¡†æ¶ä¸ºæ„å»ºå…·æœ‰äººç±»å†³ç­–èƒ½åŠ›çš„æ¸¸æˆä»£ç†æä¾›äº†é‡è¦çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„ä¸‰ä¸ªæ–¹å‘ï¼š

1. **ç¯å¢ƒä¸­çš„LLMsæ¥åœ°**ï¼šé€šè¿‡å¤šæ¨¡æ€æ„ŸçŸ¥ã€å¤–éƒ¨å¯ç”¨æ€§å‡½æ•°ã€ç¯å¢ƒåé¦ˆå’Œç»éªŒå­¦ä¹ ï¼Œä½¿LLMsæ›´å¥½åœ°é€‚åº”ç°å®ç¯å¢ƒã€‚
2. **é€šè¿‡æ¸¸æˆç©çŸ¥è¯†å‘ç°**ï¼šåˆ©ç”¨æ¸¸æˆæœºåˆ¶ä¸­çš„å¤æ‚çŸ¥è¯†ï¼Œè®¾è®¡èƒ½å¤Ÿä»è§‚å¯Ÿå’Œç»éªŒä¸­æå–åº•å±‚çŸ¥è¯†çš„LLMä»£ç†ã€‚
3. **ä»£ç†ç¤¾ä¼šæ¨¡æ‹Ÿ**ï¼šåœ¨ç°å®æ¸¸æˆç¯å¢ƒä¸­å¼€å‘æ›´å¥½çš„è®¤çŸ¥æ¶æ„ï¼Œæ¨¡æ‹Ÿæ›´å¤æ‚çš„ç¤¾äº¤äº’åŠ¨å’Œåˆä½œï¼Œä»¥æ›´å¥½åœ°ç†è§£å’Œè¡¨ç¤ºå¤æ‚çš„äººç±»äº¤äº’ã€‚

### ğŸ“š å‚è€ƒèµ„æ–™
[1] The Embodied Cognition Hypothesis
[2] ChatGPT
[3] GPT-4V
[4] Gemini
[5] LLM-based agent (LLMA)
[6] Pre-existing knowledge
[7] Real-world learning
[8] Real-world experience
[9] Chess
[10] Poker
[11] Atari games
[12] StarCraft II
[13] Minecraft
[14] DOTA II
[15] Reinforcement Learning (RL)
[16] Behavior-level policy learning
[17] Cognitive abilities
[18] Reasoning
[19] LLMs survey papers
[20] LLMAs survey papers
[21] LLMGAs survey papers
[22] Game development survey papers
[23] Game agents survey papers
[24] LLMGAs taxonomy
[25] Game categories
[26] Technical challenges
[27] Game environments
[28] Optimization strategies
[29] Future research directions
[30] LLMGAs in games
[31] Perception module
[32] Memory module
[33] Role-playing module
[34] Thinking module
[35] Action module
[36] Learning module
[37] Adventure games
[38] Communication games
[39] Competition games
[40] Cooperation games
[41] Simulation games
[42] Crafting & exploration games
[43] Evaluation metrics
[44] Grounding LLMs in environments
[45] Knowledge discovery through game-playing
[46] Agent society simulation

## GenCeption Evaluate Vision LLMs with Unlabeled Unimodal Data
### Abstract
Multimodal Large Language Models (MLLMs) are typically assessed using
expensive annotated multimodal benchmarks, which often lag behind the rapidly
evolving demands of MLLM evaluation. This paper outlines and validates
GenCeption, a novel, annotation-free evaluation method that requires only
unimodal data to measure inter-modality semantic coherence and inversely
assesses MLLMs' tendency to hallucinate. This approach eliminates the need for
costly data annotation, minimizes the risk of training data contamination, is
expected to result in slower benchmark saturation, and avoids the illusion of
emerging abilities. Inspired by the DrawCeption game, GenCeption begins with a
non-textual sample and proceeds through iterative description and generation
steps. The semantic drift across iterations is quantified using the GC@T
metric. While GenCeption is principally applicable to MLLMs across various
modalities, this paper focuses on its implementation and validation for Vision
LLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption
benchmark for evaluating VLLMs, and compare the performance of several popular
VLLMs and human annotators. Our empirical results validate GenCeption's
effectiveness, demonstrating strong correlations with established VLLM
benchmarks. VLLMs still significantly lag behind human performance and struggle
especially with text-intensive tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GenCeptionï¼šä¸€ç§æ— éœ€æ ‡æ³¨çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºæ˜‚è´µçš„å¤šæ¨¡æ€æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€æ»åäºMLLMsçš„å¿«é€Ÿæ¼”è¿›éœ€æ±‚ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•è¿˜å­˜åœ¨ä»¥ä¸‹æŒ‘æˆ˜ï¼š
1. ä¾èµ–é«˜è´¨é‡æ ‡æ³¨çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ•æ‰MLLMsçš„åŠ¨æ€å‘å±•ã€‚
2. ä½¿ç”¨ç¦»æ•£æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰è¯„ä¼°ï¼Œå¯èƒ½å¯¼è‡´å¯¹æ¨¡å‹èƒ½åŠ›çš„è¯¯åˆ¤ã€‚
3. è¯„ä¼°åˆ†æ•°å¯èƒ½æ— æ³•åæ˜ çœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®å¯èƒ½å—åˆ°åŸºå‡†æ•°æ®é›†çš„æ±¡æŸ“ã€‚
4. è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªæ¨¡æ€çš„å†…å®¹å¯èƒ½å¹¶ä¸éœ€è¦å›ç­”åŸºå‡†é—®é¢˜ï¼Œå› ä¸ºç­”æ¡ˆå¯ä»¥ä»é—®é¢˜æˆ–MLLMsçš„é¢„è®­ç»ƒçŸ¥è¯†ä¸­æ¨æ–­å‡ºæ¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†GenCeptionï¼Œä¸€ç§æ–°é¢–çš„æ— æ ‡æ³¨è¯„ä¼°æ–¹æ³•ï¼Œå®ƒåªéœ€è¦å•æ¨¡æ€æ•°æ®æ¥è¡¡é‡è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œè¯„ä¼°MLLMsçš„å¹»è§‰å€¾å‘ã€‚GenCeptionçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
1. **æ— æ ‡æ³¨è¯„ä¼°**ï¼šæ— éœ€æ˜‚è´µçš„æ•°æ®æ ‡æ³¨ï¼Œåªéœ€ä½¿ç”¨å•æ¨¡æ€æ•°æ®é›†å³å¯è¿›è¡Œè¯„ä¼°ã€‚
2. **è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§**ï¼šé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œæè¿°éæ–‡æœ¬æ ·æœ¬ï¼Œè¯„ä¼°MLLMsåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚
3. **å¹»è§‰å€¾å‘è¯„ä¼°**ï¼šé€šè¿‡è¡¡é‡è¯­ä¹‰æ¼‚ç§»ï¼Œé—´æ¥è¯„ä¼°MLLMsçš„å¹»è§‰å€¾å‘ã€‚
4. **è¿ç»­æŒ‡æ ‡**ï¼šä½¿ç”¨GC@TæŒ‡æ ‡ï¼Œæä¾›æ›´ç»†ç²’åº¦çš„è¯„ä¼°ï¼Œé¿å…å¯¹æ¨¡å‹èƒ½åŠ›çš„è¯¯åˆ¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨MMECeptionåŸºå‡†ä¸Šè¯„ä¼°äº†ä¸ƒç§æµè¡Œçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰ï¼Œå¹¶ä¸äººç±»æ ‡æ³¨è€…çš„è¡¨ç°è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒGenCeptionä¸ç°æœ‰VLLMåŸºå‡†å…·æœ‰å¼ºç›¸å…³æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°VLLMsçš„è¯­ä¹‰ä¸€è‡´æ€§å’Œå¹»è§‰å€¾å‘ã€‚æ­¤å¤–ï¼ŒVLLMsåœ¨æ–‡æœ¬å¯†é›†å‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ç„¶æ˜¾è‘—è½åäºäººç±»ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GenCeptionæä¾›äº†ä¸€ç§æ–°é¢–çš„æ— æ ‡æ³¨è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°MLLMsçš„è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œå¹»è§‰å€¾å‘ã€‚è¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
1. **æˆæœ¬æ•ˆç›Š**ï¼šæ— éœ€æ˜‚è´µçš„æ•°æ®æ ‡æ³¨ï¼Œé™ä½äº†è¯„ä¼°æˆæœ¬ã€‚
2. **åŠ¨æ€é€‚åº”**ï¼šèƒ½å¤Ÿé€‚åº”MLLMsçš„å¿«é€Ÿæ¼”è¿›éœ€æ±‚ã€‚
3. **å…¨é¢è¯„ä¼°**ï¼šèƒ½å¤Ÿè¯„ä¼°MLLMsçš„å¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬æè¿°ã€ç”Ÿæˆå’Œæ¨ç†ç­‰ã€‚
4. **å¯æ‰©å±•æ€§**ï¼šå¯ä»¥æ‰©å±•åˆ°å…¶ä»–éæ–‡æœ¬æ¨¡æ€ï¼Œå¦‚éŸ³é¢‘ã€è§†é¢‘å’Œå›¾å½¢ç­‰ã€‚

### ğŸŒŸ æ€»ç»“
GenCeptionä¸ºMLLMsçš„è¯„ä¼°æä¾›äº†ä¸€ç§æ–°é¢–ä¸”æœ‰æ•ˆçš„æ— æ ‡æ³¨æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°MLLMsçš„è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œå¹»è§‰å€¾å‘ã€‚è¯¥æ–¹æ³•å…·æœ‰æˆæœ¬æ•ˆç›Šã€åŠ¨æ€é€‚åº”ã€å…¨é¢è¯„ä¼°å’Œå¯æ‰©å±•æ€§ç­‰ä¼˜åŠ¿ï¼Œä¸ºMLLMsçš„è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## PCA-Bench Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain
### Abstract
We present PCA-Bench, a multimodal decision-making benchmark for evaluating
the integrated capabilities of Multimodal Large Language Models (MLLMs).
Departing from previous benchmarks focusing on simplistic tasks and individual
model capability, PCA-Bench introduces three complex scenarios: autonomous
driving, domestic robotics, and open-world games. Given task instructions and
diverse contexts, the model is required to seamlessly integrate multiple
capabilities of Perception, Cognition, and Action in a reasoning chain to make
accurate decisions. Moreover, PCA-Bench features error localization
capabilities, scrutinizing model inaccuracies in areas such as perception,
knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To
balance accuracy and efficiency in evaluation, we propose PCA-Eval, an
automatic evaluation protocol, and assess 10 prevalent MLLMs. The results
reveal significant performance disparities between open-source models and
powerful proprietary models like GPT-4 Vision. To address this, we introduce
Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing
instruction tuning examples in multimodal embodied environments. EIE generates
7,510 training examples in PCA-Bench and enhances the performance of
open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision
accuracy), thereby validating the effectiveness of EIE. Our findings suggest
that robust MLLMs like GPT4-Vision show promise for decision-making in embodied
agents, opening new avenues for MLLM research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PCA-Benchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­çš„å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¾€å¾€åªå…³æ³¨å•ä¸ªæ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹åœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢çš„ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ç¼ºä¹å¯¹æ¨¡å‹é”™è¯¯è¿›è¡Œå®šä½çš„èƒ½åŠ›ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šæ¨¡å‹åœ¨å“ªäº›æ–¹é¢éœ€è¦æ”¹è¿›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPCA-Bench
æœ¬æ–‡æå‡ºäº†PCA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­å†³ç­–èƒ½åŠ›çš„å¤šæ¨¡æ€å†³ç­–åŸºå‡†ã€‚PCA-Benchå¼•å…¥äº†ä¸‰ä¸ªå¤æ‚çš„åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®ä»»åŠ¡æŒ‡ä»¤å’Œä¸åŒçš„ä¸Šä¸‹æ–‡ï¼Œæ— ç¼åœ°æ•´åˆæ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œä»¥åšå‡ºå‡†ç¡®çš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPCA-Eval
ä¸ºäº†å¹³è¡¡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†PCA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ã€‚PCA-Evalåˆ©ç”¨LLMså¼ºå¤§çš„è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œæ ¹æ®æ•°æ®æ³¨é‡Šä¸­çš„é”šç‚¹ä¿¡æ¯ï¼Œè‡ªåŠ¨è¿›è¡Œé”™è¯¯å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCA-Evalä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ï¼Œå¹³å‡Kappaç³»æ•°è¾¾åˆ°0.8+ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEmbodied-Instruction-Evolution (EIE)
ä¸ºäº†è§£å†³PCA-Benchæ•°æ®é›†æ ‡æ³¨å·¥ä½œé‡å¤§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Embodied-Instruction-Evolution (EIE)æ¡†æ¶ã€‚EIEåˆ©ç”¨LLMsè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹ï¼Œä»è€Œå‡å°‘äº†äººå·¥åŠ³åŠ¨ï¼Œå¹¶æé«˜äº†PCA-Benchçš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4 Visionåœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºMLLMsã€‚EIEæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜å¼€æºMLLMsçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†GPT-4 Visionã€‚PCA-Evalèƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½æ¨¡å‹é”™è¯¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„PCA-Benchå’ŒPCA-Evalä¸ºè¯„ä¼°MLLMsçš„å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚EIEæ¡†æ¶ä¸ºè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„MLLMsåœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„å†³ç­–èƒ½åŠ›å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä¸ºMLLMsçš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

