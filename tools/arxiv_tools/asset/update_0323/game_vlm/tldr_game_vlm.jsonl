{"title":"How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game","authors":"Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu","summary":"The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.","url":"http:\/\/arxiv.org\/abs\/2503.10042v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.10042v1","published":1741841323000,"comment":null,"pdf_text":"How Do Multimodal Large Language Models Handle Complex Multimodal\nReasoning? Placing Them in An Extensible Escape Game\nZiyue Wang1 ♠*, Yurui Dong3*, Fuwen Luo1, Minyuan Ruan1, Zhili Cheng1,\nChi Chen1, Peng Li2 B, Yang Liu1,2 B\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n3School of Management, Fudan University, Shanghai, China\nConfiguration \nFiles\nScene \nData\nAutomatic \nGeneration\nTarget 2: Post-Game Debriefing (Optional)\nQ 3: Piece Together the \nWhole Story\n- The room appeared to be a \ncross between a living area \nand a storage space, likely \nused for daily life … \nQ1: Describe the \nRoom\n- The first room where \nI started had a clean, \nminimalistic design, with \nmodern furniture …\nQ2: Recall the \nEscape Process\n- Some notable items \nI observed:  Small \nplants and a key to \nopen box for …\nDoor (Room 1 Exit & \nRoom 2 Entrance)\nRoom 1 \nRoom 2 \nGame Target:  \nExit of Room 2\nInitial \nPosition\nKey Prop\nEngine \nRendering\nAction:\ninteract\nread\njump\nrotate\ngrab\nforward\ntilt\nTarget 1: Room Escaping\nFigure 1. Illustration of our proposed room escape environment EscapeCraft, which allows us to generate customized room scenes (left)\nand define groundtruth reasoning path of tasks (right). Based on EscapeCraft, we create MM-Escape benchmark, targeting at evaluating\nboth the task completion performance and the entire multimodal reasoning process of MLLMs.\nAbstract\nThe rapid advancing of Multimodal Large Language Mod-\nels (MLLMs) has spurred interest in complex multimodal\nreasoning tasks in the real-world and virtual environment,\nwhich require coordinating multiple abilities, including vi-\nsual perception, visual reasoning, spatial awareness, and\ntarget deduction. However, existing evaluations primarily\nassess the final task completion, often degrading assess-\nments to isolated abilities such as visual grounding and vi-\nsual question answering. Less attention is given to compre-\nhensively and quantitatively analyzing reasoning process\nin multimodal environments, which is crucial for under-\nstanding model behaviors and underlying reasoning mech-\nanisms beyond merely task success. To address this, we\nintroduce MM-Escape, an extensible benchmark for inves-\ntigating multimodal reasoning, inspired by real-world es-\ncape games. MM-Escape emphasizes intermediate model\n*Equal contribution, ♠Project lead, B Corresponding author\nbehaviors alongside final task completion. To achieve this,\nwe develop EscapeCraft, a customizable and open environ-\nment that enables models to engage in free-form exploration\nfor assessing multimodal reasoning. Extensive experiments\nshow that MLLMs, regardless of scale, can successfully\ncomplete the simplest room escape tasks, with some exhibit-\ning human-like exploration strategies.\nYet, performance\ndramatically drops as task difficulty increases. Moreover,\nwe observe that performance bottlenecks vary across mod-\nels, revealing distinct failure modes and limitations in their\nmultimodal reasoning abilities, such as repetitive trajecto-\nries without adaptive exploration, getting stuck in corners\ndue to poor visual spatial awareness, and ineffective use of\nacquired props, such as the key. We hope our work sheds\nlight on new challenges in multimodal reasoning, and un-\ncovers potential improvements in MLLMs capabilities. 1 2\n1GitHub repo: https:\/\/github.com\/THUNLP-MT\/EscapeCraft.\n2Home page: https:\/\/thunlp-mt.github.io\/EscapeCraft.\narXiv:2503.10042v1  [cs.CV]  13 Mar 2025\n1. Introduction\nThe rapid development of Large Language Models (LLMs)\nand Multimodal Large Language Models (MLLMs) have\ndriven the advancement of diverse multimodal systems and\napplications for academic research [2, 42], industrial engi-\nneering [15], and everyday assistance [13, 34]. Multimodal\nreasoning is essential for these applications that require in-\ntegrating multiple abilities such as visual perception, spatial\nawareness, and visual grounding [35]. For example, it en-\nhances autonomous driving by improving the holistic under-\nstanding of multi-view information and localization, which\nare essential for vehicle actions and planning [10, 12], and\nalso advances the general-purpose assistants in better per-\nforming visual and multimodal tasks in the wild [17, 21].\nDespite significant attention and effort towards improv-\ning multimodal reasoning abilities of MLLMs [22, 35, 47],\ncomprehensive evaluation remains underexplored for two\nkey reasons. First, fundamental tasks such as visual ground-\ning [8, 46, 48] and image captioning [1, 25] are conducted\nin constraint environment and straightforward objectives,\nreducing the need for autonomous exploration. They pri-\nmarily focus on identifying correct answers, such as bound-\ning boxes and objects, without requiring coordinating mul-\ntiple multimodal abilities. Second, while multimodal tasks\nin open-world settings [20, 26, 29] involve complex envi-\nronments and objectives, they emphasize final task comple-\ntion, often measured by success rate [18]. This results in a\nlack of profound analysis over the reasoning process, lead-\ning to potentially inaccurate assessments of multimodal rea-\nsoning capabilities. Moreover, some open-world tasks pro-\nvide structured knowledge libraries [9, 36] that standard-\nize the reasoning mechanisms. These consequently limit\nthe autonomy of models to conduct multimodal reasoning,\nmaking the reasoning more reliant on predefined knowledge\nrather than exploration of multimodal surroundings.\nWe argue that in open multimodal environment, includ-\ning real-world settings and virtual simulators, complex mul-\ntimodal reasoning should not be solely assessed by task\ncompletion results or isolated tasks. Instead, it is more prac-\ntical and realistic to examine how models autonomously\ncoordinate across multiple multimodal reasoning abilities.\nRecently, open environments such as Habitat [30], AI2-\nTHOR[16], and OsWorld[40] are widely discussed, where\nMLLMs are required to exhibit complex reasoning skills in-\ncluding visual searching, spatial understanding, tool utiliza-\ntion, and long-term decision-making. However, as summa-\nrize in Table 1, there is a constraint on tasks or environments\nespecially designed for evaluating multimodal reasoning in\nopen-ended and interactive settings, leaving the evaluation\nof complex multimodal reasoning underestimated.\nIn this paper, we introduce MM-Escape, an extensible\nbenchmark for evaluating complex multimodal reasoning,\ninspired by real-world escape games. To achieve this, we\ndevelop EscapeCraft, a customizable open environment\nthat engages models in free-form exploration through the\nroom escape task, assisting in comprehensive assessments\nof their multimodal reasoning abilities. MM-Escape pro-\nvides solutions to the above issues of excessive concerns\non task completion and simplified objectives, by emphasiz-\ning intermediate behaviors alongside final task completion.\nIt measures the entire reasoning process consists of com-\npound abilities. While our designed games are intuitive for\nhuman players, they pose significant challenges for models\nregarding reasoning paths and interaction behaviors, expos-\ning key limitations in current multimodal reasoning capabil-\nities. As an additional bonus, we introduce post-game de-\nbriefing task, which is feasible only for the most challenging\nsettings. It requires models to recall the escape process and\nreconstruct the story via collected clues, which assist in the\nfuture assessment of more complicated reasoning tasks.\nWe conduct comprehensive experiments and derive two\nkey findings. First, although the overall performance re-\nmains far from human-level, recent MLLMs demonstrate\nstrong multimodal reasoning ability. Notably, models like\nGPT-4o and Gemini-1.5-Pro achieve high escape rates and\nexhibit human-like exploration and reasoning strategies.\nWhile their interaction success rates are halved with steps\nmore than doubled compared to human results. Second, as\ngame difficulty increases, the model performance sharply\ndeclines, revealing distinct failure modes across models.\nFor instance, GPT-4o tends to repeat actions and get trapped\nin historical trajectories when reasoning paths grow longer,\nGemini struggles with spatial awareness and often gets\nstuck in corners. We also thoroughly discuss post-game de-\nbriefing and other extensible settings in Section 5. These\nfindings highlight the need for greater attentions toward\ncomprehensive analysis and improvement of multimodal\nreasoning abilities. Our contributions are as follows:\n• We introduce a benchmark, MM-Escape, to advance\ncomprehensive evaluation of multimodal reasoning for\nMLLMs, by quantitatively evaluating intermediate rea-\nsoning process alongside task completion performance.\n• Our benchmark features free-form exploration, requiring\nmodels to autonomously coordinate multiple multimodal\nreasoning abilities in the multimodal room escape task.\n• We thoroughly investigate model behaviors using MM-\nEscape and identify distinct limitations across models.\nOur analysis provides detailed insights, highlighting fu-\nture optimization and potential real-world applications.\n2. Related Work\n2.1. Complex Reasoning Abilities of MLLMs\nRecent research on MLLMs has moved beyond addressing\nsimple tasks, such as image captioning and image retrieval,\nand instead focuses on enhancing model abilities towards\nBenchmark\nScenario\nTask\nMultimodal\nEnvironment Type\nURP\nPA\nTextWorld [5]\nText Game\nSimplified Text Games\n✗\nOpen Environment\n✓\n✗\nEscapeBench [28]\nText Game\nRoom Escape\n✗\nOpen Environment\n✓\n✓\nOpenEQA [23]\nVideo\nEgo-centric QA\n✓\nControlled Environment\n✗\n✗\nEgoSchema [24]\nVideo\nEgo-centric QA\n✓\nControlled Environment\n✗\n✗\nVSI-Bench [44]\nVideo\nVisual-Spatial QA\n✓\nControlled Environment\n✗\n✗\nMineRL [14]\nVideo Game\nGoals in Minecraft\n✓\nOpen Environment\n✗\n✗\nOSWorld [40]\nOS Environment\nComputer Use\n✓\nOpen Environment\n✗\n✗\nALFRED [31]\nEmbodied Environment\nHousehold Task\n✓\nOpen Environment\n✗\n✗\nEmbodiedBench [45]\nEmbodied Environment\nHousehold Task\n✓\nOpen Environment\n✗\n✗\nMM-Escape (Ours)\nEmbodied Environment\nRoom Escape\n✓\nOpen Environment\n✓\n✓\nTable 1. Benchmarks aiming at evaluating complex reasoning ability of models. URP means unpredictable reasoning path. PA means\nprocess assessment.\nmore complex tasks and more realistic scenarios. For ex-\nample, MLLMs can operate digital devices such as comput-\ners [27, 43] and mobile phones [33, 39], play video games\nrequiring long action sequences [3, 6], and perform robotic\nmanipulation in the real world [11, 49]. Unlike tasks such\nas visual question answering and visual grounding, which\nhave standardized answers, these complex tasks allow di-\nverse solutions and engage in open environments. As a re-\nsult, multimodal reasoning ability is becoming increasingly\nimportant for achieving more advanced targets.\n2.2. Complex Multimodal Reasoning Evaluation\nResearchers have made efforts to evaluate the visual reason-\ning ability of MLLMs in complex tasks which require mod-\nels to coordinate multiple multimodal abilities [23, 24, 44].\nHowever, many of them leverage videos with predefined\nperspectives and trajectories as input. Models cannot in-\nteract with environments by themselves.\nSome other works focus on complex open worlds, such\nas Minecraft [19, 20, 29, 32, 37, 38], to evaluate model abil-\nities of multimodal reasoning. However, two issues still ex-\nist. First, due to clearly defined game rules, standardized\nguides are available from websites like Minecraft Wiki 3,\nand training datasets of game demonstrations [14], making\nit feasible for models to reason based on their commonsense\nor relying on external knowledge rather than observations\nduring free-form exploration. Second, evaluations are often\noutcome-oriented, for example, focusing on specific goals\nlike obtaining a diamond in Minecraft, while how models\nexplore the open world is ignored. Similar issues exist in\nother complex open-ended tasks in open worlds like com-\nputer use [40] and robotic manipulation [31, 45].\nCompared to existing benchmarks, our work features in\nthree aspects: a) we support sufficient interactions between\nmodels and environments; b) environments can be flexibly\nconfigured, preventing models from solely relying external\nknowledge to achieve the goals; c) reasoning processes are\nalso evaluated alongside final results. These enable us to\nunderstand model abilities more comprehensively.\n3https:\/\/minecraft.wiki\/\n2.3. Multimodal v.s. Pure Text Environments\nPrevious works investigated reasoning ability of models in\npure text scenarios by allowing models to interact with en-\nvironments via text interfaces [5, 28, 41]. However, some\nmultimodal information cannot be easily and precisely ex-\npressed in verbal forms, hindering reasoning ability based\non multimodal information to be evaluated in pure text en-\nvironments. Additionally, models enjoy enhanced decision-\nmaking autonomy within multimodal environments com-\npared to text environments. For example, models may de-\ncide their rotation angles or travel distances based on mul-\ntimodal inputs when requiring to observe target objects or\nnearby surroundings, but it is hard to make such decisions\nbased on text descriptions. Therefore, we believe that mul-\ntimodal environments are essential for evaluating reason-\ning abilities of models, and propose EscapeCraft and MM-\nEscape to address the above issues.\n3. MM-Escape\nIn this paper, we address comprehensive investigation of\ncomplex multimodal reasoning for MLLMs by introducing\nMM-Escape, an extensible benchmark. To support this, we\ndesign EscapeCraft, a highly customizable and open envi-\nronment that engages models in free-form exploration for\nquantitatively assessing the intermediate multimodal rea-\nsoning process, as depicted in Figure 1. We highlight that\ncomplex multimodal reasoning requires an autonomous in-\ntegration of multiple multimodal abilities, and that evalu-\nating the reasoning process is as crucial as assessing final\ntask completion. We will first describe our introduced tasks\nin the following section, and detail the construction of our\nbenchmark along with its corresponding environment.\n3.1. Task Definition\nInspired by real-world game, room escape, we propose\nto place MLLMs in an escape escape scenario to inves-\ntigate their multimodal reasoning abilities in an open-\nenvironment, where models should autonomously explore\nmultimodal surroundings, and reason toward both tempo-\nrary and long-term goals. We name the introduced task,\nmultimodal room escape. It consists of a mandatory tar-\n1\n2\n3\n5\n6\n7\n8\n9\n10\n14\n15\n1\n12\nRotate to \nright side   \nto explore \nother areas\n& look for \ninteractive \nobjects or \nclues.\nContinue \nrotating to \nfully explore \nthe room.\nRotate a bit \nto have a \nbetter view \nof the TV \nand the \ntable.\nAdjust view to face \nthe TV, look for \nclues or interactive \nelements.\nMove forward \n& grab the key.\nMove to-\nwards the \ndoor & try to \nunlock it \nwith the key.\nLook \nslightly up, \nexpecting \nthe door to \nbe in view.\nTry to locate \nthe door.\nTry to use the \nkey on the door.\nThe door is \ncentered now. \nTrying to \nunlock it \nusing the key.\nNow, I have the key, \nI‘ll turn around to \nface the door and \ntry using it.\nContinue \nrotating to\nexplore …\n5\n7\n8\n9\n6\n4\n0\n2\n3\n10\n15\n11 12 13\n14\n1\nModel \ntrajectory\nStep \nindex\nModel \nposition\nObservation \ndirection\nModel-generated \nrationales\nAcquisition of \nthe key\nAttempt to \nunlock\nContinue \nrotating to\nexplore …\n0\n4\nStill can‘t \nsee the\ndoor. \nSlightly \nadjust \nview.\nFigure 2. An example of success escape path from Gemini-1.5-pro. For the first four steps, it rotates, without moving, to observe the\nsurroundings, and then starts moving towards the direct on the observed tables in the following steps. For step 5 and 6, we zoom in to the\nkey to make it clearer. The key is distorted because the view of model is tilting downwards to observe objects on the table. From step 10\nto 14, the model is already close to the door, and is slowly rotating and tilting to locate the door.\nget, the room escaping task, aiming at escaping a locked\nroom, and an optional target the post-game debriefing task,\nrequiring to reconstruct the story discovered during the es-\ncaping. Our multimodal room escape task assesses the en-\ntire reasoning process rather than solely focusing on final\ngame completion.\nRoom Escaping Task.\nThis task presents a ultimate goal\nof exiting the room, as shown in Figure 1 (Game Target) and\na detailed example is demonstrated in Figure 2. It requires\nmodels to fully explore and interact with the multimodal\nenvironment, search for props and clues, identify the exit,\nand correctly use props to unlock the door. We do not ex-\nploit step-by-step instructions to restricted model actions,\nensuring them to freely explore the environment without\nconstraints, and automatically deduce current or short-term\ngoals. This task evaluates the integrated multimodal rea-\nsoning ability, including object recognition, visual search,\nvisual reasoning, target deduction, spatial reasoning, and\nprop utilization. Please refer to Supplementary Materials\nSec. I for detailed discussion over full required abilities.\nPost-game debriefing task differs from the room escap-\ning task that involves reasoning about past experiences, cur-\nrent states, and future plans, as it takes place after the game\nis completed4. It serves as an optional tasks because its pre-\nliminary requirement is to collect all props in the game and\n4This task is also common in real-world escape game.\nexit the room successfully, which is quite challenging for\nmost of current models. Therefore, for a fair comparison,\nwe only apply this target to cases where models correctly\ncomplete the game as shown in Figure 1. It emphasizes log-\nicality and consistency of reconstruction, requiring models\nto reflect on the past experiences and events.\n3.2. Construction and Design of Environment\nWe develop EscapeCraft to place models in an escape game,\na free-form exploration environment, to comprehensively\ninvestigate complex multimodal reasoning ability of mod-\nels. It supports customizable and extensible scene genera-\ntion for our escape game with minimal manual effort.\nRoom Scene Generation\nTo support room escape task,\nwe generate diverse and interactable room environments,\nwhich requires efficient and large-scale scene data gen-\neration and rendering.\nWe develop an environment, Es-\ncapeCraft, by extending ProcTHOR [7] and Legent [4],\nwhich are originally labor-intensive regarding the scene\ngeneration process. We enable automatic size adaptation\nto predefined or customized configurations, such as room\nscale, number of rooms, and required furniture, by incorpo-\nrating 3D furniture models with annotated size information.\nCritical objects are made fully interactable to serve as props\nand clues to assist in completing the task. The flexibility and\ninteractivity of objects allow for automatic large-scale 3D\ndiffuculty-1\ndiffuculty-2\ndiffuculty-3\ndiffuculty-n\nFigure 3. Illustration of difficulties. This figure shows required\npaths for the reasoning process of success escape concerning each\nlevels. The levels can be customized and extended as depicted by\nthe “difficulty-n” example, and as demonstrated in Figure 1 (lower\nright part).\nroom generation following the requirement of our bench-\nmark. EscapeCraft is highly customizable and extensible,\nmaking it well-suited for the room escape task. Detailed\nconstruction can be found in Supplementary Material C.1.\nAction Space\nWe define three types of actions, moving,\nview adjustment, and interaction. The moving action, i.e.\nmoving forward, allows the model to change its position to\nperceive objects at different depth. View adjustment enables\nperception from different angles and facilitates object selec-\ntion for interaction, including horizontal or vertical rotation,\nand looking at specific coordinates. Interaction actions con-\ntain grabbing, using, reading, and inputting, allowing mod-\nels to obtain and utilize props from the environment, and\nprocess messages displayed by the props. These actions can\nbe executed individually or integratedly in a multi-action\nway. See Supplementary Materials Sec. C.1.2 for details.\nInventory system\nTo enable model players to acquire and\nutilize props within the environment, we design an inven-\ntory system that allows models to store and manage ac-\nquired items, access detailed information about them, and\nuse them as needed. This system assists models in effec-\ntively using props, and successfully escaping the room.\n3.3. MM-Escape Benchmark\nGame Settings.\nWe introduce an automatic reasoning\nchain generation procedure by configuring the Prop Chain.\nIn detail, this is a singly linked list representing the ordered\nsequence of items and interactions required to complete the\ngame. Each node in the chain corresponds to an interac-\ntive element, such as a key, a locked box, or a note with a\npassword, where the tail node represents the exit point of\nthe game. To construct a complete escape game setting, we\nannotate the links between nodes in the prop chain to define\nthe ways to obtain different props (such as unlimited acqui-\nsition or requiring a key to open, etc.) and their inclusion\nrelationships (for example, a key can be placed in a box).\nSee Supplementary Material Sec. C.2 for details.\nFollowing this, we employ varying difficulty levels in\nMM-Escape to facilitate in-depth assessments of complex\nmultimodal reasoning. Shown in Figure 3, difficulty is pri-\nmarily determined by the predefined prop chain of a game,\nwhere longer chains correspond to higher difficulty. We de-\nfine three standard difficulty levels for individual rooms:\n• Difficulty-1: The simplest one-hop reasoning path where\nno props are needed to unlock the door. Models can exit\nby locating the door and interacting with it directly.\n• Difficulty-2: A two-hop reasoning path requiring an addi-\ntional key or password compared to Difficulty-1. Models\nshould search for the key or password and interact with it\nto unlock the door.\n• Difficulty-3: A three-hop reasoning path requiring both a\npassword and a key, with one additional hop to Difficulty-\n2. This level challenges models with spatial reasoning,\nvisual search, and prop utilization.\nSince the prop chain can grow infinitely, our difficulty levels\nare inherently extendable. Moerover, the type of questions\nor tasks in each reasoning hop are customizable and inter-\nchangeable, further enhancing the difficulty and flexibility\nof MM-Escape. We also explore some extended settings\nthat incorporate with other tasks, such as embodied QA and\nvisual logical reasoning, with case studies in Section 5.2\nAdditionally, to further investigate the behavior and\ncomplex multimodal reasoning abilities of MLLMs, we in-\ntroduce a multi-room setting by combining two standard\nsingle rooms.\nWe create multiple multi-room combina-\ntions, each containing two individual rooms. The config-\nurations include: two Difficulty-1 rooms, two Difficulty-2\nrooms, and a mixed setting of Difficulty-1 and Difficulty-\n2. In the multi-room setting, models start in the first room,\nwhich has only one exit. Upon successfully exiting, models\nenter the second room and search for the final game exit.\nHowever, this setting presents a greater challenge than the\nsingle-room scenario, as there are two doors in the second\nroom, requiring models to distinguish between exit and en-\ntrance based on their corresponding surroundings.\nStatistics of MM-Escape\nFor individual room settings,\nwe generated 11 scenes for each of Difficulty-1 and\nDifficulty-2, and 21 scenes for Difficulty-3. As there are\ntwo types of props required by Difficulty-3, we enable a\nkey-first and a password-first prop chains. For multi-room\nsettings, we generated 10 scenes for the three different com-\nbinations introduced above.\nThere are totally 63 scenes\nfor standard evaluation of our benchmark, which could be\nfurther extended for future research without adapting the\nenvironment. These scenes are categorized into four dis-\ntinct styles: living room (14), kitchen (19), bathroom (19),\nand bedroom (11).\nAnd different objects are automati-\ncally placed within each scene to correspond to its specific\nstyle. On average, difficulty-1 scenes contain 20.18 objects,\ndifficulty-2 contains 14.55, and difficulty-3 contains 15.24.\nModels\nDifficulty-1\nDifficulty-2\nDifficulty-3\nAVG\nER (%)↑\nER\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nHuman\n100.00\n5.73\n95.45\n0.19\n100.00\n100.00\n13.64\n81.81\n0.19\n100.00\n100.00\n21.45\n75.45\n0.19\n100.00\nGPT-4o\n100.00\n11.27\n37.82\n0.42\n72.73\n81.82\n36.73\n36.73\n0.26\n71.36\n90.00\n50.19\n31.36\n0.35\n81.36\nGemini-1.5-pro\n81.82\n21.18\n49.18\n0.39\n54.55\n90.91\n47.82\n14.89\n0.44\n46.82\n74.49\n73.18\n10.43\n0.48\n61.06\nClaude 3.5 Sonnet\n72.73\n22.09\n30.64\n0.36\n45.45\n54.55\n57.45\n20.64\n0.17\n39.61\n54.83\n82.36\n16.21\n0.22\n52.60\nLlama-3.2-11b-vision\n63.64\n23.55\n31.36\n0.35\n0.00\n27.27\n75.00\n3.16\n0.44\n0.00\n27.27\n100.00\n3.55\n0.32\n21.21\nQwen-VL-Max\n18.18\n42.64\n11.36\n0.05\n0.00\n27.27\n75.00\n3.51\n0.15\n9.09\n18.18\n94.18\n2.72\n0.31\n9.09\nPhi-3-vision-128k\n0.00\n50.00\n0.00\n0.01\n0.00\n0.00\n75.00\n0.00\n0.02\n0.00\n0.00\n100.00\n0.00\n0.01\n0.00\nTable 2. Results of standard single room setting. Prop: Prop Gain; Steps: average steps used to complete the game; Grab SR: the precision\nof grabbing; Grab Ratio: the portion of grabbing actions regarding the total consumed steps. Note that Difficulty-1 requires no prop, and\nthe prop gain is therefore omitted for this setting. The max allowed steps are 50, 75, 100 for Difficulty-1, -2, -3 respectively.\nModels\nDifficulty-1 & Difficulty-1\nDifficulty-1 & Difficulty-2\nDifficulty-2 & Difficulty-2\nER(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nGPT-4o\n75.00\n35.50\n34.25\n0.32\n90.00\n100.00\n34.90\n35.52\n0.31\n70.00\n80.00\n39.50\n42.32\n0.37\nGemini-1.5-pro\n22.22\n40.22\n22.89\n0.38\n40.00\n50.00\n56.60\n16.79\n0.05\n60.00\n80.00\n60.00\n22.71\n0.34\nLlama-3.2-11b-vision\n55.56\n31.00\n36.25\n0.36\n10.00\n60.00\n66.40\n4.40\n0.40\n10.00\n40.00\n76.80\n27.00\n0.19\nClaude 3.5 Sonnet\n22.22\n45.22\n10.62\n0.08\n20.00\n20.00\n71.90\n6.75\n0.09\n10.00\n10.00\n80.00\n23.20\n0.06\nQwen-VL-max\n22.22\n40.33\n12.96\n0.16\n30.00\n50.00\n57.70\n42.30\n0.28\n0.00\n10.00\n80.00\n23.66\n0.32\nTable 3. Performance on multi-room setting for different room scenes. To assist in the more challenging setting, we provide models with\na full successful escape path from Room 1 (9 steps) for self-reflection when they try to unlock Room 2. Hence, the Prop Gain (Prop\n(%)) in the results refers only to Room 2. Further challenges of escaping from the very beginning of multi-room setting are discussed in\nSupplementary Material F.\nThe objects are logically arranged in a manner consistent\nwith real-life settings and randomly distributed within the\nrooms to serve as part of the tasks for models.\nMetrics of MM-Escape\nTo comprehensively evaluate\ncomplex multimodal reasoning ability, we propose a set of\nmetrics for the intermediate process, alongside an indica-\ntor for final task completion. For the room escape task, we\nuse average escape rate (ER) as the indicator of game com-\npletion, and design five metrics for measuring intermediate\ninteractions, including prop gain, average steps, grab count,\ngrab success rate, and grab ratio. Denoting the total steps\nas S, amount of succeeded grabbing action as N T P\ngrab, grab\nsuccess rate as GSR, grab ratio as Rgrab we have,\nProp Gain =\nN T P\ngrab\nP Prop count,\n(1)\nGSR =\nN T P\ngrab\nP Grabbing actions,\n(2)\nRgrab =\nP Grabbing actions\nS\n,\n(3)\nThe debriefing task is only applied to cases where mod-\nels successfully escaped as a bonus, to further investigate\nthe multimodal understanding and reasoning process. Be-\ncause models that successfully complete the game tend to\nachieve high scores regarding metrics mentioned above. We\nemploy large language models as evaluator for this task, as-\nsessing the consistency ([0, 1]) of reconstructed stories with\nthe groundtruth stories.\n4. Experiments\nEvaluation Setups.\nWe investigate both open-source\nmodels and propriety models of different scales, and eval-\nuate their multimodal reasoning ability in level-wise ap-\nproaches as outlined in Section 3.3. Specifically, we employ\nboth single room and multi-room settings. The multi-room\nsetting can be viewed as combinations of two single rooms,\nwith the second room featuring two doors—one for entry\nand one for exit. For robust comparison, we set the temper-\nature to 0 to eliminate token decoding diversity. The prompt\ntemplates used for evaluation are detailed in Supplementary\nMaterial Sec. J. Since the game can grow infinitely, we limit\nthe maximum steps for each difficulties as 50, 75, and 100\nrespectively, for quantitative measurements. We slightly in-\ncrease the max step to 80 for the most challenging multi-\nroom setting.\nMain Results of Game Completion\nResults of standard\nsingle room setting, along with manual evaluation are re-\nported in Table 2. See Supplementary Material Sec. B for\ndetails of human evaluation.\nThe results indicate that model performances falls signif-\nicantly behind human capabilities across all metrics, high-\nlighting the remarkable gap between current multimodal\nreasoning abilities and human-level capabilities. Among\nthe models, GPT-4o demonstrates the strongest overall per-\nformance across all difficulty levels, achieving the highest\naverage escape rate (81.36%), followed by Gemini-1.5-pro\n(61.06%). Other models limited success in task comple-\ntion, except for Phi-3-vision-128k, which fails considering\nFigure 4. Analysis on the grabbing behaviors with respect to the escape rate and the visibility of the exit at initial location.\nthe task completion. Claude 3.5 Sonnet and Llama-3.2-11b-\nvision achieve moderate ER, but their performance drops\nsignificantly in Difficulty-2 and -3, suggesting weaknesses\nin multi-hop multimodal reasoning.\nFor the intermediate process, GPT-4o efficiently com-\npletes tasks with relatively fewer steps while maintaining\na high grabbing success rate.\nNotably, Gemini-1.5-pro\nachieves a lower escape rate (ER) in Difficulty-2 compared\nto GPT-4o, yet it presents the highest Prop Gain, suggesting\na strong visual search ability. For models do not survive the\ntask completion, we can still measure their multimodal rea-\nsoning ability in terms of Prop Gain, GSR and Grab Ratio.\nLlama-3.2-11b-vision and Qwen-VL-Max receive identical\nProp Gain, while the later presents higher GSR with a lower\nGrab Ratio. This implies that Qwen-VL-Max can precisely\nidentify scenes with useful props, while Llama-3.2 is likely\nto adopt a greedy policy to increase grab actions so as to\nfind more props, which is often unintentionally.\nFor the more challenging multi-room setting, we can\nderive similar conclusions.\nAdditionally, we notice that\nby providing a successful path of the first room helps the\nmodel to better conduct multimodal reasoning in our task\nfor most models, but the improvements present in differ-\nent forms. For example, GPT-4o achieves an ER of 90% for\nDifficulty-2 when bootstrapped by a first room of Difficulty-\n1, and prop gain of Gemini and Llama 3.2 is improved in\nDifficulty-2 & -2 combination compared with the setting of\na single room of Difficulty-2.\n5. Analysis and Discussions\nTakeaway Observations\n• Distinct Human-Like Behavioral Patterns: Models ex-\nhibit unique behaviors in room escape task. Gemini tends\nto remain in a fixed location, typically the starting point,\nscanning its surroundings before taking action. In con-\nModels\nDifficulty-2\nDifficulty-3\n#Key\n#Exit\n#PW\n#Key\n#Exit\nstep\nClaude\n59.60\n62.60\n5.25\n20.67\n84.80\nGPT\n16.25\n39.10\n11.80\n17.23\n50.40\nGemini\n16.50\n50.90\n20.67\n38.13\n65.70\ncost\nClaude\n0.91\n0.23\n0.66\n0.50\n0.46\nGPT\n0.68\n0.40\n0.36\n0.28\n0.40\nGemini\n0.62\n0.63\n0.49\n0.29\n0.31\nTable 4. steps taken to to key prop and door. & cost of key props.\n#PW: step\/cost to obtain passwords. cost of # represents the pro-\nportion of steps the model spends searching for # in the entire path.\ntrast, GPT-4o first observes a wider range for a global un-\nderstanding of the environment.\n• Diverse Observation Strategies: Some models deviate\nfrom a conventional front-facing view. Gemini frequently\nemploys a downward-facing perspective to inspect ob-\njects like tables and chairs, whereas GPT-4o predomi-\nnantly relies on a front-facing view.\n• Common failure modes:\nWe observe some common\nreasons leading to unsuccessful escape, such as move-\nment failures and interaction challenges. Movement fail-\nures vary across models, including repetitive trajectories\n(GPT-4o) and getting trapped (Gemini and Claude). Inter-\naction challenges include action combination failure (Phi-\n3 and Qwen-VL) and imprecise object specifying.\n• Adherence to Long-Term Goals: Most models remain\ngoal-oriented, focusing on locating the exit and clues or\nprops that could assist in escaping. However, Phi-3 con-\nsistently fails to generate valid object interactions and\ndoes not recognize doors, unlike other models, which en-\ngage with doors upon detection.\nPlease refer to Supplementary Material Sec. A for full ver-\nsion of overall observations.\nLook at \nthe \nnumbers \non the \nwall to \nsee if \nthey \nprovide \nany clues \nfor the \npassword.\nMoving \ncloser to \nthe table \nto inspect \nthe \nobjects on \nit and the \ncabinet \nbehind it. \nThe \nnumbers \n‘3422’ on \nthe wall \nmight be a \nclue, so I‘ll \nkeep them \nin mind……\nMoving closer to the table and focusing on the strawberry \nto try grabbing it again. The items on the table don't seem \ninteractable, so I'll try the cabinet next.\nMoving \ncloser \nto the \ngame \ncontroll\ner and \nattempti\nng to \ngrab it \nagain. I \nthink \nI'm \nclose \nenough \nnow.\nTurn around to \nface the rest \nof the room and \ninspect the \nobjects on the \ntable.\nRotate to the \nright to get a \nbetter view of \nthe room and \nidentify other \npotential clues \nor objects……\n   ……\nMove slightly closer to the table to grab the \nobject on it……\nMove closer to the table to grab the key……\nModel trajectory\nModel position\nObservation direction\nModel-generated rationales\nObserving\n(a) Case of Gemini\n(b) Case of GPT-4o\n(b) Case of GPT-4o (moving password figure away from the door)\nFigure 5. The extended scenario where the required password is displayed via a numerical pattern on the wall, rather than explicitly written\non notes. GPT-4o completes reading it at once and exits within five steps, while Gemini struggles to repetitively search the room. Moving\nthe pattern away from the door further challenges GPT-4o, leading to a failure of escaping.\n5.1. Analysis of Entire Path\nWe investigate three three key questions in this section: Q1)\nHow many steps required to obtain props? Q2) How many\nsteps are needed to exit the room after acquiring the core\nprop (key or password)? Q3) What is the relationship be-\ntween grab success rate and escape outcome for each test?\nFor Q1, GPT-4o presents a significant advantage in step\ncounts required to obtain the key followed by Gemini as\nshown in Table 4. While Claude requires fewer steps to\nfind props in Difficulty-3, this comes at the cost of a sig-\nnificant decrease in escape rate. The superior performance\nin locating and obtaining the core props can be attributed\nto better understanding of task objectives and the holistic\nenvironment, and its enhanced reasoning abilities in this\ncontext. For Q2, Gemini can locate and acquire the key\nat a lower cost in difficulty-2, but GPT-4o outperforms in\nDifficulty-3, which is more complex. GPT-4o benefits from\nits prior memory and understanding of the room environ-\nment, gained in the process of obtaining key props, which\naids it to efficiently locate the exit and escape with fewer\nsteps compared to other models. For Q3, escape success\nis positively correlated with grab success rate (GSR), as\nshown in Figure 4(a).\nHigher GSR implies that models\nhave experienced more successful interactions with the en-\nvironment, potentially indicating a clearer understanding of\nthe overall environment and ultimate goals for our task.\nWhile GSR declines with difficulty, the scores of GPT-4o\nand Claude 3.5 remain relatively stable compared to others,\nwith less variation in grabbing behavior and GSR across dif-\nficulties. The low GSR of Qwen in difficulty-2 and -3 can\nbe partly caused by the ineffective perception of the envi-\nronment, inferior reasoning and interacting decision in this\ncomplex tasks, while the ow GSR of Llama 3.2 is limited\nby its input registration of only one image at a time.\nPlease refer to Supplementary Materials Sec. E for de-\ntailed discussion, and Sec. D for additional examination\nabout the moving distance and arrangement of the room.\n5.2. The Extensibility of EscapeCraft\nWe provide an extended case study in this section. We also\ndiscussion of fully autonomous version of multi-room set-\nting in Supplementary Materials Sec. F, and an additional\ncustomizations of escaping path in Supplementary Materi-\nals Sec. G.\nWe introduced an extended scenario where the required\npassword is displayed via a numerical pattern on the wall,\nrather than explicitly written on notes, as shown in Fig-\nure 5.\nModels should recognize the pattern on the wall\n(password) and infer its relevance to the door. When pat-\ntern appears near the door, GPT-4o quickly identifies it and\nexit in the following five steps, while Gemini, despite see-\ning the pattern, failed to recognize it as the password and\ninstead searched the room exhaustively repeatedly.\nWe further move the pattern away from the location\nof the door, and observe unchanged behaviors of Gemini.\nHowever, GPT-4o performs differently, by repeatedly mov-\ning between the bed and the wall without recognizing the\npassword. It also failed to interact with the door until the\ngame stops by the max allowance, revealing limitations in\nits long-term reasoning and spatial reasoning.\n5.3. Analysis on Post-game Debriefing\nThe post-game debriefing task requires models to recall\ntheir escape process and obtained clues, and reconstruct the\nwhole stories. As successful escape is necessary for post-\ngame debriefing, we only evaluate models with high suc-\ncess rate, that is, GPT-4o and Gemini-1.5-pro. Results show\nthat both models fall short of ability of retelling the stories.\nModels pay strong attention to the processes which are di-\nrectly related to the completion of room escaping, such as\npassword acquisition. Meanwhile, they ignore background\nstories which are less important but also helpful for escap-\ning. For the reason of limited model abilities, this may be a\neffective strategy to complete tasks. However, with the en-\nhancement of model abilities in the future, it is necessary to\nimprove model ability of memorization of background in-\nformation. For the experiment results, please refer to Sup-\nplementary Material Sec. H.\n6. Conclusions\nIn this paper, we introduce MM-Escape, an extensible\nbenchmark for investigating multimodal reasoning, inspired\nby real-world escape games. We also develop EscapeCraft\nthat enables models to engage in free-form exploration for\nassessing multimodal reasoning, to construction our bench-\nmark.\nWe find that MLLMs can successfully complete\nthe simplest level of tasks, and some models even exhibit-\ning human-like behaviors and strategies. However, perfor-\nmance dramatically drops as task difficulty increases, while\nhuman testers consistently succeed.\nMM-Escape reveals\ndistinct failure modes across models, such as repetitive tra-\njectories without adaptive exploration, trapped in corners\nwithout good spatial awareness. We hope our work sheds\nlight on new challenges, and uncovers potential improve-\nments for MLLMs.\nContributions\nZiyue Wang: Design of the escape process and post-game\ndebriefing, implementation of EscapeCraft, all reported ex-\nperiments. Paper writing: all sections and figures.\nYurui Dong: Design of the escape process and post-game\ndebriefing, construction of 3D environment, all engineering\nand coding works. Paper writing: method sections and ap-\npendix sections, figures of case study.\nFuwen Luo: Design of the escape process, design and ex-\nperiments of post-game debriefing. Paper writing: related\nworks, post-game debriefing experiments and analysis.\nMinyuan Ruan: Implementation of EscapeCraft, scene\ngeneration, construction of homepage. Paper writing: anal-\nysis, human evaluations, figures of analysis.\nZhili Cheng: Construction of 3D environment, design of\nroom escape process.\nChi Chen: Design of room escape process and post-game\ndebriefing, support on experiments.\nPeng Li: Project supervision, advising of all designs, engi-\nneering, experiments, and paper writing.\nYang Liu:\nProject supervision,\nadvising of all de-\nsigns,\nengineering,\nexperiments,\nand\npaper\nwriting.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nHow Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game\n```\n#### 2. 论文摘要\n```\nThe rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.\n```\n\n#### 3. 论文全文\n```\nHow Do Multimodal Large Language Models Handle Complex Multimodal\nReasoning? Placing Them in An Extensible Escape Game\nZiyue Wang1 ♠*, Yurui Dong3*, Fuwen Luo1, Minyuan Ruan1, Zhili Cheng1,\nChi Chen1, Peng Li2 B, Yang Liu1,2 B\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n3School of Management, Fudan University, Shanghai, China\nConfiguration \nFiles\nScene \nData\nAutomatic \nGeneration\nTarget 2: Post-Game Debriefing (Optional)\nQ 3: Piece Together the \nWhole Story\n- The room appeared to be a \ncross between a living area \nand a storage space, likely \nused for daily life … \nQ1: Describe the \nRoom\n- The first room where \nI started had a clean, \nminimalistic design, with \nmodern furniture …\nQ2: Recall the \nEscape Process\n- Some notable items \nI observed:  Small \nplants and a key to \nopen box for …\nDoor (Room 1 Exit & \nRoom 2 Entrance)\nRoom 1 \nRoom 2 \nGame Target:  \nExit of Room 2\nInitial \nPosition\nKey Prop\nEngine \nRendering\nAction:\ninteract\nread\njump\nrotate\ngrab\nforward\ntilt\nTarget 1: Room Escaping\nFigure 1. Illustration of our proposed room escape environment EscapeCraft, which allows us to generate customized room scenes (left)\nand define groundtruth reasoning path of tasks (right). Based on EscapeCraft, we create MM-Escape benchmark, targeting at evaluating\nboth the task completion performance and the entire multimodal reasoning process of MLLMs.\nAbstract\nThe rapid advancing of Multimodal Large Language Mod-\nels (MLLMs) has spurred interest in complex multimodal\nreasoning tasks in the real-world and virtual environment,\nwhich require coordinating multiple abilities, including vi-\nsual perception, visual reasoning, spatial awareness, and\ntarget deduction. However, existing evaluations primarily\nassess the final task completion, often degrading assess-\nments to isolated abilities such as visual grounding and vi-\nsual question answering. Less attention is given to compre-\nhensively and quantitatively analyzing reasoning process\nin multimodal environments, which is crucial for under-\nstanding model behaviors and underlying reasoning mech-\nanisms beyond merely task success. To address this, we\nintroduce MM-Escape, an extensible benchmark for inves-\ntigating multimodal reasoning, inspired by real-world es-\ncape games. MM-Escape emphasizes intermediate model\n*Equal contribution, ♠Project lead, B Corresponding author\nbehaviors alongside final task completion. To achieve this,\nwe develop EscapeCraft, a customizable and open environ-\nment that enables models to engage in free-form exploration\nfor assessing multimodal reasoning. Extensive experiments\nshow that MLLMs, regardless of scale, can successfully\ncomplete the simplest room escape tasks, with some exhibit-\ning human-like exploration strategies.\nYet, performance\ndramatically drops as task difficulty increases. Moreover,\nwe observe that performance bottlenecks vary across mod-\nels, revealing distinct failure modes and limitations in their\nmultimodal reasoning abilities, such as repetitive trajecto-\nries without adaptive exploration, getting stuck in corners\ndue to poor visual spatial awareness, and ineffective use of\nacquired props, such as the key. We hope our work sheds\nlight on new challenges in multimodal reasoning, and un-\ncovers potential improvements in MLLMs capabilities. 1 2\n1GitHub repo: https:\/\/github.com\/THUNLP-MT\/EscapeCraft.\n2Home page: https:\/\/thunlp-mt.github.io\/EscapeCraft.\narXiv:2503.10042v1  [cs.CV]  13 Mar 2025\n1. Introduction\nThe rapid development of Large Language Models (LLMs)\nand Multimodal Large Language Models (MLLMs) have\ndriven the advancement of diverse multimodal systems and\napplications for academic research [2, 42], industrial engi-\nneering [15], and everyday assistance [13, 34]. Multimodal\nreasoning is essential for these applications that require in-\ntegrating multiple abilities such as visual perception, spatial\nawareness, and visual grounding [35]. For example, it en-\nhances autonomous driving by improving the holistic under-\nstanding of multi-view information and localization, which\nare essential for vehicle actions and planning [10, 12], and\nalso advances the general-purpose assistants in better per-\nforming visual and multimodal tasks in the wild [17, 21].\nDespite significant attention and effort towards improv-\ning multimodal reasoning abilities of MLLMs [22, 35, 47],\ncomprehensive evaluation remains underexplored for two\nkey reasons. First, fundamental tasks such as visual ground-\ning [8, 46, 48] and image captioning [1, 25] are conducted\nin constraint environment and straightforward objectives,\nreducing the need for autonomous exploration. They pri-\nmarily focus on identifying correct answers, such as bound-\ning boxes and objects, without requiring coordinating mul-\ntiple multimodal abilities. Second, while multimodal tasks\nin open-world settings [20, 26, 29] involve complex envi-\nronments and objectives, they emphasize final task comple-\ntion, often measured by success rate [18]. This results in a\nlack of profound analysis over the reasoning process, lead-\ning to potentially inaccurate assessments of multimodal rea-\nsoning capabilities. Moreover, some open-world tasks pro-\nvide structured knowledge libraries [9, 36] that standard-\nize the reasoning mechanisms. These consequently limit\nthe autonomy of models to conduct multimodal reasoning,\nmaking the reasoning more reliant on predefined knowledge\nrather than exploration of multimodal surroundings.\nWe argue that in open multimodal environment, includ-\ning real-world settings and virtual simulators, complex mul-\ntimodal reasoning should not be solely assessed by task\ncompletion results or isolated tasks. Instead, it is more prac-\ntical and realistic to examine how models autonomously\ncoordinate across multiple multimodal reasoning abilities.\nRecently, open environments such as Habitat [30], AI2-\nTHOR[16], and OsWorld[40] are widely discussed, where\nMLLMs are required to exhibit complex reasoning skills in-\ncluding visual searching, spatial understanding, tool utiliza-\ntion, and long-term decision-making. However, as summa-\nrize in Table 1, there is a constraint on tasks or environments\nespecially designed for evaluating multimodal reasoning in\nopen-ended and interactive settings, leaving the evaluation\nof complex multimodal reasoning underestimated.\nIn this paper, we introduce MM-Escape, an extensible\nbenchmark for evaluating complex multimodal reasoning,\ninspired by real-world escape games. To achieve this, we\ndevelop EscapeCraft, a customizable open environment\nthat engages models in free-form exploration through the\nroom escape task, assisting in comprehensive assessments\nof their multimodal reasoning abilities. MM-Escape pro-\nvides solutions to the above issues of excessive concerns\non task completion and simplified objectives, by emphasiz-\ning intermediate behaviors alongside final task completion.\nIt measures the entire reasoning process consists of com-\npound abilities. While our designed games are intuitive for\nhuman players, they pose significant challenges for models\nregarding reasoning paths and interaction behaviors, expos-\ning key limitations in current multimodal reasoning capabil-\nities. As an additional bonus, we introduce post-game de-\nbriefing task, which is feasible only for the most challenging\nsettings. It requires models to recall the escape process and\nreconstruct the story via collected clues, which assist in the\nfuture assessment of more complicated reasoning tasks.\nWe conduct comprehensive experiments and derive two\nkey findings. First, although the overall performance re-\nmains far from human-level, recent MLLMs demonstrate\nstrong multimodal reasoning ability. Notably, models like\nGPT-4o and Gemini-1.5-Pro achieve high escape rates and\nexhibit human-like exploration and reasoning strategies.\nWhile their interaction success rates are halved with steps\nmore than doubled compared to human results. Second, as\ngame difficulty increases, the model performance sharply\ndeclines, revealing distinct failure modes across models.\nFor instance, GPT-4o tends to repeat actions and get trapped\nin historical trajectories when reasoning paths grow longer,\nGemini struggles with spatial awareness and often gets\nstuck in corners. We also thoroughly discuss post-game de-\nbriefing and other extensible settings in Section 5. These\nfindings highlight the need for greater attentions toward\ncomprehensive analysis and improvement of multimodal\nreasoning abilities. Our contributions are as follows:\n• We introduce a benchmark, MM-Escape, to advance\ncomprehensive evaluation of multimodal reasoning for\nMLLMs, by quantitatively evaluating intermediate rea-\nsoning process alongside task completion performance.\n• Our benchmark features free-form exploration, requiring\nmodels to autonomously coordinate multiple multimodal\nreasoning abilities in the multimodal room escape task.\n• We thoroughly investigate model behaviors using MM-\nEscape and identify distinct limitations across models.\nOur analysis provides detailed insights, highlighting fu-\nture optimization and potential real-world applications.\n2. Related Work\n2.1. Complex Reasoning Abilities of MLLMs\nRecent research on MLLMs has moved beyond addressing\nsimple tasks, such as image captioning and image retrieval,\nand instead focuses on enhancing model abilities towards\nBenchmark\nScenario\nTask\nMultimodal\nEnvironment Type\nURP\nPA\nTextWorld [5]\nText Game\nSimplified Text Games\n✗\nOpen Environment\n✓\n✗\nEscapeBench [28]\nText Game\nRoom Escape\n✗\nOpen Environment\n✓\n✓\nOpenEQA [23]\nVideo\nEgo-centric QA\n✓\nControlled Environment\n✗\n✗\nEgoSchema [24]\nVideo\nEgo-centric QA\n✓\nControlled Environment\n✗\n✗\nVSI-Bench [44]\nVideo\nVisual-Spatial QA\n✓\nControlled Environment\n✗\n✗\nMineRL [14]\nVideo Game\nGoals in Minecraft\n✓\nOpen Environment\n✗\n✗\nOSWorld [40]\nOS Environment\nComputer Use\n✓\nOpen Environment\n✗\n✗\nALFRED [31]\nEmbodied Environment\nHousehold Task\n✓\nOpen Environment\n✗\n✗\nEmbodiedBench [45]\nEmbodied Environment\nHousehold Task\n✓\nOpen Environment\n✗\n✗\nMM-Escape (Ours)\nEmbodied Environment\nRoom Escape\n✓\nOpen Environment\n✓\n✓\nTable 1. Benchmarks aiming at evaluating complex reasoning ability of models. URP means unpredictable reasoning path. PA means\nprocess assessment.\nmore complex tasks and more realistic scenarios. For ex-\nample, MLLMs can operate digital devices such as comput-\ners [27, 43] and mobile phones [33, 39], play video games\nrequiring long action sequences [3, 6], and perform robotic\nmanipulation in the real world [11, 49]. Unlike tasks such\nas visual question answering and visual grounding, which\nhave standardized answers, these complex tasks allow di-\nverse solutions and engage in open environments. As a re-\nsult, multimodal reasoning ability is becoming increasingly\nimportant for achieving more advanced targets.\n2.2. Complex Multimodal Reasoning Evaluation\nResearchers have made efforts to evaluate the visual reason-\ning ability of MLLMs in complex tasks which require mod-\nels to coordinate multiple multimodal abilities [23, 24, 44].\nHowever, many of them leverage videos with predefined\nperspectives and trajectories as input. Models cannot in-\nteract with environments by themselves.\nSome other works focus on complex open worlds, such\nas Minecraft [19, 20, 29, 32, 37, 38], to evaluate model abil-\nities of multimodal reasoning. However, two issues still ex-\nist. First, due to clearly defined game rules, standardized\nguides are available from websites like Minecraft Wiki 3,\nand training datasets of game demonstrations [14], making\nit feasible for models to reason based on their commonsense\nor relying on external knowledge rather than observations\nduring free-form exploration. Second, evaluations are often\noutcome-oriented, for example, focusing on specific goals\nlike obtaining a diamond in Minecraft, while how models\nexplore the open world is ignored. Similar issues exist in\nother complex open-ended tasks in open worlds like com-\nputer use [40] and robotic manipulation [31, 45].\nCompared to existing benchmarks, our work features in\nthree aspects: a) we support sufficient interactions between\nmodels and environments; b) environments can be flexibly\nconfigured, preventing models from solely relying external\nknowledge to achieve the goals; c) reasoning processes are\nalso evaluated alongside final results. These enable us to\nunderstand model abilities more comprehensively.\n3https:\/\/minecraft.wiki\/\n2.3. Multimodal v.s. Pure Text Environments\nPrevious works investigated reasoning ability of models in\npure text scenarios by allowing models to interact with en-\nvironments via text interfaces [5, 28, 41]. However, some\nmultimodal information cannot be easily and precisely ex-\npressed in verbal forms, hindering reasoning ability based\non multimodal information to be evaluated in pure text en-\nvironments. Additionally, models enjoy enhanced decision-\nmaking autonomy within multimodal environments com-\npared to text environments. For example, models may de-\ncide their rotation angles or travel distances based on mul-\ntimodal inputs when requiring to observe target objects or\nnearby surroundings, but it is hard to make such decisions\nbased on text descriptions. Therefore, we believe that mul-\ntimodal environments are essential for evaluating reason-\ning abilities of models, and propose EscapeCraft and MM-\nEscape to address the above issues.\n3. MM-Escape\nIn this paper, we address comprehensive investigation of\ncomplex multimodal reasoning for MLLMs by introducing\nMM-Escape, an extensible benchmark. To support this, we\ndesign EscapeCraft, a highly customizable and open envi-\nronment that engages models in free-form exploration for\nquantitatively assessing the intermediate multimodal rea-\nsoning process, as depicted in Figure 1. We highlight that\ncomplex multimodal reasoning requires an autonomous in-\ntegration of multiple multimodal abilities, and that evalu-\nating the reasoning process is as crucial as assessing final\ntask completion. We will first describe our introduced tasks\nin the following section, and detail the construction of our\nbenchmark along with its corresponding environment.\n3.1. Task Definition\nInspired by real-world game, room escape, we propose\nto place MLLMs in an escape escape scenario to inves-\ntigate their multimodal reasoning abilities in an open-\nenvironment, where models should autonomously explore\nmultimodal surroundings, and reason toward both tempo-\nrary and long-term goals. We name the introduced task,\nmultimodal room escape. It consists of a mandatory tar-\n1\n2\n3\n5\n6\n7\n8\n9\n10\n14\n15\n1\n12\nRotate to \nright side   \nto explore \nother areas\n& look for \ninteractive \nobjects or \nclues.\nContinue \nrotating to \nfully explore \nthe room.\nRotate a bit \nto have a \nbetter view \nof the TV \nand the \ntable.\nAdjust view to face \nthe TV, look for \nclues or interactive \nelements.\nMove forward \n& grab the key.\nMove to-\nwards the \ndoor & try to \nunlock it \nwith the key.\nLook \nslightly up, \nexpecting \nthe door to \nbe in view.\nTry to locate \nthe door.\nTry to use the \nkey on the door.\nThe door is \ncentered now. \nTrying to \nunlock it \nusing the key.\nNow, I have the key, \nI‘ll turn around to \nface the door and \ntry using it.\nContinue \nrotating to\nexplore …\n5\n7\n8\n9\n6\n4\n0\n2\n3\n10\n15\n11 12 13\n14\n1\nModel \ntrajectory\nStep \nindex\nModel \nposition\nObservation \ndirection\nModel-generated \nrationales\nAcquisition of \nthe key\nAttempt to \nunlock\nContinue \nrotating to\nexplore …\n0\n4\nStill can‘t \nsee the\ndoor. \nSlightly \nadjust \nview.\nFigure 2. An example of success escape path from Gemini-1.5-pro. For the first four steps, it rotates, without moving, to observe the\nsurroundings, and then starts moving towards the direct on the observed tables in the following steps. For step 5 and 6, we zoom in to the\nkey to make it clearer. The key is distorted because the view of model is tilting downwards to observe objects on the table. From step 10\nto 14, the model is already close to the door, and is slowly rotating and tilting to locate the door.\nget, the room escaping task, aiming at escaping a locked\nroom, and an optional target the post-game debriefing task,\nrequiring to reconstruct the story discovered during the es-\ncaping. Our multimodal room escape task assesses the en-\ntire reasoning process rather than solely focusing on final\ngame completion.\nRoom Escaping Task.\nThis task presents a ultimate goal\nof exiting the room, as shown in Figure 1 (Game Target) and\na detailed example is demonstrated in Figure 2. It requires\nmodels to fully explore and interact with the multimodal\nenvironment, search for props and clues, identify the exit,\nand correctly use props to unlock the door. We do not ex-\nploit step-by-step instructions to restricted model actions,\nensuring them to freely explore the environment without\nconstraints, and automatically deduce current or short-term\ngoals. This task evaluates the integrated multimodal rea-\nsoning ability, including object recognition, visual search,\nvisual reasoning, target deduction, spatial reasoning, and\nprop utilization. Please refer to Supplementary Materials\nSec. I for detailed discussion over full required abilities.\nPost-game debriefing task differs from the room escap-\ning task that involves reasoning about past experiences, cur-\nrent states, and future plans, as it takes place after the game\nis completed4. It serves as an optional tasks because its pre-\nliminary requirement is to collect all props in the game and\n4This task is also common in real-world escape game.\nexit the room successfully, which is quite challenging for\nmost of current models. Therefore, for a fair comparison,\nwe only apply this target to cases where models correctly\ncomplete the game as shown in Figure 1. It emphasizes log-\nicality and consistency of reconstruction, requiring models\nto reflect on the past experiences and events.\n3.2. Construction and Design of Environment\nWe develop EscapeCraft to place models in an escape game,\na free-form exploration environment, to comprehensively\ninvestigate complex multimodal reasoning ability of mod-\nels. It supports customizable and extensible scene genera-\ntion for our escape game with minimal manual effort.\nRoom Scene Generation\nTo support room escape task,\nwe generate diverse and interactable room environments,\nwhich requires efficient and large-scale scene data gen-\neration and rendering.\nWe develop an environment, Es-\ncapeCraft, by extending ProcTHOR [7] and Legent [4],\nwhich are originally labor-intensive regarding the scene\ngeneration process. We enable automatic size adaptation\nto predefined or customized configurations, such as room\nscale, number of rooms, and required furniture, by incorpo-\nrating 3D furniture models with annotated size information.\nCritical objects are made fully interactable to serve as props\nand clues to assist in completing the task. The flexibility and\ninteractivity of objects allow for automatic large-scale 3D\ndiffuculty-1\ndiffuculty-2\ndiffuculty-3\ndiffuculty-n\nFigure 3. Illustration of difficulties. This figure shows required\npaths for the reasoning process of success escape concerning each\nlevels. The levels can be customized and extended as depicted by\nthe “difficulty-n” example, and as demonstrated in Figure 1 (lower\nright part).\nroom generation following the requirement of our bench-\nmark. EscapeCraft is highly customizable and extensible,\nmaking it well-suited for the room escape task. Detailed\nconstruction can be found in Supplementary Material C.1.\nAction Space\nWe define three types of actions, moving,\nview adjustment, and interaction. The moving action, i.e.\nmoving forward, allows the model to change its position to\nperceive objects at different depth. View adjustment enables\nperception from different angles and facilitates object selec-\ntion for interaction, including horizontal or vertical rotation,\nand looking at specific coordinates. Interaction actions con-\ntain grabbing, using, reading, and inputting, allowing mod-\nels to obtain and utilize props from the environment, and\nprocess messages displayed by the props. These actions can\nbe executed individually or integratedly in a multi-action\nway. See Supplementary Materials Sec. C.1.2 for details.\nInventory system\nTo enable model players to acquire and\nutilize props within the environment, we design an inven-\ntory system that allows models to store and manage ac-\nquired items, access detailed information about them, and\nuse them as needed. This system assists models in effec-\ntively using props, and successfully escaping the room.\n3.3. MM-Escape Benchmark\nGame Settings.\nWe introduce an automatic reasoning\nchain generation procedure by configuring the Prop Chain.\nIn detail, this is a singly linked list representing the ordered\nsequence of items and interactions required to complete the\ngame. Each node in the chain corresponds to an interac-\ntive element, such as a key, a locked box, or a note with a\npassword, where the tail node represents the exit point of\nthe game. To construct a complete escape game setting, we\nannotate the links between nodes in the prop chain to define\nthe ways to obtain different props (such as unlimited acqui-\nsition or requiring a key to open, etc.) and their inclusion\nrelationships (for example, a key can be placed in a box).\nSee Supplementary Material Sec. C.2 for details.\nFollowing this, we employ varying difficulty levels in\nMM-Escape to facilitate in-depth assessments of complex\nmultimodal reasoning. Shown in Figure 3, difficulty is pri-\nmarily determined by the predefined prop chain of a game,\nwhere longer chains correspond to higher difficulty. We de-\nfine three standard difficulty levels for individual rooms:\n• Difficulty-1: The simplest one-hop reasoning path where\nno props are needed to unlock the door. Models can exit\nby locating the door and interacting with it directly.\n• Difficulty-2: A two-hop reasoning path requiring an addi-\ntional key or password compared to Difficulty-1. Models\nshould search for the key or password and interact with it\nto unlock the door.\n• Difficulty-3: A three-hop reasoning path requiring both a\npassword and a key, with one additional hop to Difficulty-\n2. This level challenges models with spatial reasoning,\nvisual search, and prop utilization.\nSince the prop chain can grow infinitely, our difficulty levels\nare inherently extendable. Moerover, the type of questions\nor tasks in each reasoning hop are customizable and inter-\nchangeable, further enhancing the difficulty and flexibility\nof MM-Escape. We also explore some extended settings\nthat incorporate with other tasks, such as embodied QA and\nvisual logical reasoning, with case studies in Section 5.2\nAdditionally, to further investigate the behavior and\ncomplex multimodal reasoning abilities of MLLMs, we in-\ntroduce a multi-room setting by combining two standard\nsingle rooms.\nWe create multiple multi-room combina-\ntions, each containing two individual rooms. The config-\nurations include: two Difficulty-1 rooms, two Difficulty-2\nrooms, and a mixed setting of Difficulty-1 and Difficulty-\n2. In the multi-room setting, models start in the first room,\nwhich has only one exit. Upon successfully exiting, models\nenter the second room and search for the final game exit.\nHowever, this setting presents a greater challenge than the\nsingle-room scenario, as there are two doors in the second\nroom, requiring models to distinguish between exit and en-\ntrance based on their corresponding surroundings.\nStatistics of MM-Escape\nFor individual room settings,\nwe generated 11 scenes for each of Difficulty-1 and\nDifficulty-2, and 21 scenes for Difficulty-3. As there are\ntwo types of props required by Difficulty-3, we enable a\nkey-first and a password-first prop chains. For multi-room\nsettings, we generated 10 scenes for the three different com-\nbinations introduced above.\nThere are totally 63 scenes\nfor standard evaluation of our benchmark, which could be\nfurther extended for future research without adapting the\nenvironment. These scenes are categorized into four dis-\ntinct styles: living room (14), kitchen (19), bathroom (19),\nand bedroom (11).\nAnd different objects are automati-\ncally placed within each scene to correspond to its specific\nstyle. On average, difficulty-1 scenes contain 20.18 objects,\ndifficulty-2 contains 14.55, and difficulty-3 contains 15.24.\nModels\nDifficulty-1\nDifficulty-2\nDifficulty-3\nAVG\nER (%)↑\nER\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER\n(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nHuman\n100.00\n5.73\n95.45\n0.19\n100.00\n100.00\n13.64\n81.81\n0.19\n100.00\n100.00\n21.45\n75.45\n0.19\n100.00\nGPT-4o\n100.00\n11.27\n37.82\n0.42\n72.73\n81.82\n36.73\n36.73\n0.26\n71.36\n90.00\n50.19\n31.36\n0.35\n81.36\nGemini-1.5-pro\n81.82\n21.18\n49.18\n0.39\n54.55\n90.91\n47.82\n14.89\n0.44\n46.82\n74.49\n73.18\n10.43\n0.48\n61.06\nClaude 3.5 Sonnet\n72.73\n22.09\n30.64\n0.36\n45.45\n54.55\n57.45\n20.64\n0.17\n39.61\n54.83\n82.36\n16.21\n0.22\n52.60\nLlama-3.2-11b-vision\n63.64\n23.55\n31.36\n0.35\n0.00\n27.27\n75.00\n3.16\n0.44\n0.00\n27.27\n100.00\n3.55\n0.32\n21.21\nQwen-VL-Max\n18.18\n42.64\n11.36\n0.05\n0.00\n27.27\n75.00\n3.51\n0.15\n9.09\n18.18\n94.18\n2.72\n0.31\n9.09\nPhi-3-vision-128k\n0.00\n50.00\n0.00\n0.01\n0.00\n0.00\n75.00\n0.00\n0.02\n0.00\n0.00\n100.00\n0.00\n0.01\n0.00\nTable 2. Results of standard single room setting. Prop: Prop Gain; Steps: average steps used to complete the game; Grab SR: the precision\nof grabbing; Grab Ratio: the portion of grabbing actions regarding the total consumed steps. Note that Difficulty-1 requires no prop, and\nthe prop gain is therefore omitted for this setting. The max allowed steps are 50, 75, 100 for Difficulty-1, -2, -3 respectively.\nModels\nDifficulty-1 & Difficulty-1\nDifficulty-1 & Difficulty-2\nDifficulty-2 & Difficulty-2\nER(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nER(%)↑\nProp\n(%)↑\nSteps↓\nGrab\nSR (%)↑\nGrab\nRatio\nGPT-4o\n75.00\n35.50\n34.25\n0.32\n90.00\n100.00\n34.90\n35.52\n0.31\n70.00\n80.00\n39.50\n42.32\n0.37\nGemini-1.5-pro\n22.22\n40.22\n22.89\n0.38\n40.00\n50.00\n56.60\n16.79\n0.05\n60.00\n80.00\n60.00\n22.71\n0.34\nLlama-3.2-11b-vision\n55.56\n31.00\n36.25\n0.36\n10.00\n60.00\n66.40\n4.40\n0.40\n10.00\n40.00\n76.80\n27.00\n0.19\nClaude 3.5 Sonnet\n22.22\n45.22\n10.62\n0.08\n20.00\n20.00\n71.90\n6.75\n0.09\n10.00\n10.00\n80.00\n23.20\n0.06\nQwen-VL-max\n22.22\n40.33\n12.96\n0.16\n30.00\n50.00\n57.70\n42.30\n0.28\n0.00\n10.00\n80.00\n23.66\n0.32\nTable 3. Performance on multi-room setting for different room scenes. To assist in the more challenging setting, we provide models with\na full successful escape path from Room 1 (9 steps) for self-reflection when they try to unlock Room 2. Hence, the Prop Gain (Prop\n(%)) in the results refers only to Room 2. Further challenges of escaping from the very beginning of multi-room setting are discussed in\nSupplementary Material F.\nThe objects are logically arranged in a manner consistent\nwith real-life settings and randomly distributed within the\nrooms to serve as part of the tasks for models.\nMetrics of MM-Escape\nTo comprehensively evaluate\ncomplex multimodal reasoning ability, we propose a set of\nmetrics for the intermediate process, alongside an indica-\ntor for final task completion. For the room escape task, we\nuse average escape rate (ER) as the indicator of game com-\npletion, and design five metrics for measuring intermediate\ninteractions, including prop gain, average steps, grab count,\ngrab success rate, and grab ratio. Denoting the total steps\nas S, amount of succeeded grabbing action as N T P\ngrab, grab\nsuccess rate as GSR, grab ratio as Rgrab we have,\nProp Gain =\nN T P\ngrab\nP Prop count,\n(1)\nGSR =\nN T P\ngrab\nP Grabbing actions,\n(2)\nRgrab =\nP Grabbing actions\nS\n,\n(3)\nThe debriefing task is only applied to cases where mod-\nels successfully escaped as a bonus, to further investigate\nthe multimodal understanding and reasoning process. Be-\ncause models that successfully complete the game tend to\nachieve high scores regarding metrics mentioned above. We\nemploy large language models as evaluator for this task, as-\nsessing the consistency ([0, 1]) of reconstructed stories with\nthe groundtruth stories.\n4. Experiments\nEvaluation Setups.\nWe investigate both open-source\nmodels and propriety models of different scales, and eval-\nuate their multimodal reasoning ability in level-wise ap-\nproaches as outlined in Section 3.3. Specifically, we employ\nboth single room and multi-room settings. The multi-room\nsetting can be viewed as combinations of two single rooms,\nwith the second room featuring two doors—one for entry\nand one for exit. For robust comparison, we set the temper-\nature to 0 to eliminate token decoding diversity. The prompt\ntemplates used for evaluation are detailed in Supplementary\nMaterial Sec. J. Since the game can grow infinitely, we limit\nthe maximum steps for each difficulties as 50, 75, and 100\nrespectively, for quantitative measurements. We slightly in-\ncrease the max step to 80 for the most challenging multi-\nroom setting.\nMain Results of Game Completion\nResults of standard\nsingle room setting, along with manual evaluation are re-\nported in Table 2. See Supplementary Material Sec. B for\ndetails of human evaluation.\nThe results indicate that model performances falls signif-\nicantly behind human capabilities across all metrics, high-\nlighting the remarkable gap between current multimodal\nreasoning abilities and human-level capabilities. Among\nthe models, GPT-4o demonstrates the strongest overall per-\nformance across all difficulty levels, achieving the highest\naverage escape rate (81.36%), followed by Gemini-1.5-pro\n(61.06%). Other models limited success in task comple-\ntion, except for Phi-3-vision-128k, which fails considering\nFigure 4. Analysis on the grabbing behaviors with respect to the escape rate and the visibility of the exit at initial location.\nthe task completion. Claude 3.5 Sonnet and Llama-3.2-11b-\nvision achieve moderate ER, but their performance drops\nsignificantly in Difficulty-2 and -3, suggesting weaknesses\nin multi-hop multimodal reasoning.\nFor the intermediate process, GPT-4o efficiently com-\npletes tasks with relatively fewer steps while maintaining\na high grabbing success rate.\nNotably, Gemini-1.5-pro\nachieves a lower escape rate (ER) in Difficulty-2 compared\nto GPT-4o, yet it presents the highest Prop Gain, suggesting\na strong visual search ability. For models do not survive the\ntask completion, we can still measure their multimodal rea-\nsoning ability in terms of Prop Gain, GSR and Grab Ratio.\nLlama-3.2-11b-vision and Qwen-VL-Max receive identical\nProp Gain, while the later presents higher GSR with a lower\nGrab Ratio. This implies that Qwen-VL-Max can precisely\nidentify scenes with useful props, while Llama-3.2 is likely\nto adopt a greedy policy to increase grab actions so as to\nfind more props, which is often unintentionally.\nFor the more challenging multi-room setting, we can\nderive similar conclusions.\nAdditionally, we notice that\nby providing a successful path of the first room helps the\nmodel to better conduct multimodal reasoning in our task\nfor most models, but the improvements present in differ-\nent forms. For example, GPT-4o achieves an ER of 90% for\nDifficulty-2 when bootstrapped by a first room of Difficulty-\n1, and prop gain of Gemini and Llama 3.2 is improved in\nDifficulty-2 & -2 combination compared with the setting of\na single room of Difficulty-2.\n5. Analysis and Discussions\nTakeaway Observations\n• Distinct Human-Like Behavioral Patterns: Models ex-\nhibit unique behaviors in room escape task. Gemini tends\nto remain in a fixed location, typically the starting point,\nscanning its surroundings before taking action. In con-\nModels\nDifficulty-2\nDifficulty-3\n#Key\n#Exit\n#PW\n#Key\n#Exit\nstep\nClaude\n59.60\n62.60\n5.25\n20.67\n84.80\nGPT\n16.25\n39.10\n11.80\n17.23\n50.40\nGemini\n16.50\n50.90\n20.67\n38.13\n65.70\ncost\nClaude\n0.91\n0.23\n0.66\n0.50\n0.46\nGPT\n0.68\n0.40\n0.36\n0.28\n0.40\nGemini\n0.62\n0.63\n0.49\n0.29\n0.31\nTable 4. steps taken to to key prop and door. & cost of key props.\n#PW: step\/cost to obtain passwords. cost of # represents the pro-\nportion of steps the model spends searching for # in the entire path.\ntrast, GPT-4o first observes a wider range for a global un-\nderstanding of the environment.\n• Diverse Observation Strategies: Some models deviate\nfrom a conventional front-facing view. Gemini frequently\nemploys a downward-facing perspective to inspect ob-\njects like tables and chairs, whereas GPT-4o predomi-\nnantly relies on a front-facing view.\n• Common failure modes:\nWe observe some common\nreasons leading to unsuccessful escape, such as move-\nment failures and interaction challenges. Movement fail-\nures vary across models, including repetitive trajectories\n(GPT-4o) and getting trapped (Gemini and Claude). Inter-\naction challenges include action combination failure (Phi-\n3 and Qwen-VL) and imprecise object specifying.\n• Adherence to Long-Term Goals: Most models remain\ngoal-oriented, focusing on locating the exit and clues or\nprops that could assist in escaping. However, Phi-3 con-\nsistently fails to generate valid object interactions and\ndoes not recognize doors, unlike other models, which en-\ngage with doors upon detection.\nPlease refer to Supplementary Material Sec. A for full ver-\nsion of overall observations.\nLook at \nthe \nnumbers \non the \nwall to \nsee if \nthey \nprovide \nany clues \nfor the \npassword.\nMoving \ncloser to \nthe table \nto inspect \nthe \nobjects on \nit and the \ncabinet \nbehind it. \nThe \nnumbers \n‘3422’ on \nthe wall \nmight be a \nclue, so I‘ll \nkeep them \nin mind……\nMoving closer to the table and focusing on the strawberry \nto try grabbing it again. The items on the table don't seem \ninteractable, so I'll try the cabinet next.\nMoving \ncloser \nto the \ngame \ncontroll\ner and \nattempti\nng to \ngrab it \nagain. I \nthink \nI'm \nclose \nenough \nnow.\nTurn around to \nface the rest \nof the room and \ninspect the \nobjects on the \ntable.\nRotate to the \nright to get a \nbetter view of \nthe room and \nidentify other \npotential clues \nor objects……\n   ……\nMove slightly closer to the table to grab the \nobject on it……\nMove closer to the table to grab the key……\nModel trajectory\nModel position\nObservation direction\nModel-generated rationales\nObserving\n(a) Case of Gemini\n(b) Case of GPT-4o\n(b) Case of GPT-4o (moving password figure away from the door)\nFigure 5. The extended scenario where the required password is displayed via a numerical pattern on the wall, rather than explicitly written\non notes. GPT-4o completes reading it at once and exits within five steps, while Gemini struggles to repetitively search the room. Moving\nthe pattern away from the door further challenges GPT-4o, leading to a failure of escaping.\n5.1. Analysis of Entire Path\nWe investigate three three key questions in this section: Q1)\nHow many steps required to obtain props? Q2) How many\nsteps are needed to exit the room after acquiring the core\nprop (key or password)? Q3) What is the relationship be-\ntween grab success rate and escape outcome for each test?\nFor Q1, GPT-4o presents a significant advantage in step\ncounts required to obtain the key followed by Gemini as\nshown in Table 4. While Claude requires fewer steps to\nfind props in Difficulty-3, this comes at the cost of a sig-\nnificant decrease in escape rate. The superior performance\nin locating and obtaining the core props can be attributed\nto better understanding of task objectives and the holistic\nenvironment, and its enhanced reasoning abilities in this\ncontext. For Q2, Gemini can locate and acquire the key\nat a lower cost in difficulty-2, but GPT-4o outperforms in\nDifficulty-3, which is more complex. GPT-4o benefits from\nits prior memory and understanding of the room environ-\nment, gained in the process of obtaining key props, which\naids it to efficiently locate the exit and escape with fewer\nsteps compared to other models. For Q3, escape success\nis positively correlated with grab success rate (GSR), as\nshown in Figure 4(a).\nHigher GSR implies that models\nhave experienced more successful interactions with the en-\nvironment, potentially indicating a clearer understanding of\nthe overall environment and ultimate goals for our task.\nWhile GSR declines with difficulty, the scores of GPT-4o\nand Claude 3.5 remain relatively stable compared to others,\nwith less variation in grabbing behavior and GSR across dif-\nficulties. The low GSR of Qwen in difficulty-2 and -3 can\nbe partly caused by the ineffective perception of the envi-\nronment, inferior reasoning and interacting decision in this\ncomplex tasks, while the ow GSR of Llama 3.2 is limited\nby its input registration of only one image at a time.\nPlease refer to Supplementary Materials Sec. E for de-\ntailed discussion, and Sec. D for additional examination\nabout the moving distance and arrangement of the room.\n5.2. The Extensibility of EscapeCraft\nWe provide an extended case study in this section. We also\ndiscussion of fully autonomous version of multi-room set-\nting in Supplementary Materials Sec. F, and an additional\ncustomizations of escaping path in Supplementary Materi-\nals Sec. G.\nWe introduced an extended scenario where the required\npassword is displayed via a numerical pattern on the wall,\nrather than explicitly written on notes, as shown in Fig-\nure 5.\nModels should recognize the pattern on the wall\n(password) and infer its relevance to the door. When pat-\ntern appears near the door, GPT-4o quickly identifies it and\nexit in the following five steps, while Gemini, despite see-\ning the pattern, failed to recognize it as the password and\ninstead searched the room exhaustively repeatedly.\nWe further move the pattern away from the location\nof the door, and observe unchanged behaviors of Gemini.\nHowever, GPT-4o performs differently, by repeatedly mov-\ning between the bed and the wall without recognizing the\npassword. It also failed to interact with the door until the\ngame stops by the max allowance, revealing limitations in\nits long-term reasoning and spatial reasoning.\n5.3. Analysis on Post-game Debriefing\nThe post-game debriefing task requires models to recall\ntheir escape process and obtained clues, and reconstruct the\nwhole stories. As successful escape is necessary for post-\ngame debriefing, we only evaluate models with high suc-\ncess rate, that is, GPT-4o and Gemini-1.5-pro. Results show\nthat both models fall short of ability of retelling the stories.\nModels pay strong attention to the processes which are di-\nrectly related to the completion of room escaping, such as\npassword acquisition. Meanwhile, they ignore background\nstories which are less important but also helpful for escap-\ning. For the reason of limited model abilities, this may be a\neffective strategy to complete tasks. However, with the en-\nhancement of model abilities in the future, it is necessary to\nimprove model ability of memorization of background in-\nformation. For the experiment results, please refer to Sup-\nplementary Material Sec. H.\n6. Conclusions\nIn this paper, we introduce MM-Escape, an extensible\nbenchmark for investigating multimodal reasoning, inspired\nby real-world escape games. We also develop EscapeCraft\nthat enables models to engage in free-form exploration for\nassessing multimodal reasoning, to construction our bench-\nmark.\nWe find that MLLMs can successfully complete\nthe simplest level of tasks, and some models even exhibit-\ning human-like behaviors and strategies. However, perfor-\nmance dramatically drops as task difficulty increases, while\nhuman testers consistently succeed.\nMM-Escape reveals\ndistinct failure modes across models, such as repetitive tra-\njectories without adaptive exploration, trapped in corners\nwithout good spatial awareness. We hope our work sheds\nlight on new challenges, and uncovers potential improve-\nments for MLLMs.\nContributions\nZiyue Wang: Design of the escape process and post-game\ndebriefing, implementation of EscapeCraft, all reported ex-\nperiments. Paper writing: all sections and figures.\nYurui Dong: Design of the escape process and post-game\ndebriefing, construction of 3D environment, all engineering\nand coding works. Paper writing: method sections and ap-\npendix sections, figures of case study.\nFuwen Luo: Design of the escape process, design and ex-\nperiments of post-game debriefing. Paper writing: related\nworks, post-game debriefing experiments and analysis.\nMinyuan Ruan: Implementation of EscapeCraft, scene\ngeneration, construction of homepage. Paper writing: anal-\nysis, human evaluations, figures of analysis.\nZhili Cheng: Construction of 3D environment, design of\nroom escape process.\nChi Chen: Design of room escape process and post-game\ndebriefing, support on experiments.\nPeng Li: Project supervision, advising of all designs, engi-\nneering, experiments, and paper writing.\nYang Liu:\nProject supervision,\nadvising of all de-\nsigns,\nengineering,\nexperiments,\nand\npaper\nwriting.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 多模态大型语言模型如何处理复杂的多模态推理？通过可扩展的逃脱游戏进行评估\n\n## 📌 背景痛点\/本文动机\n随着多模态大型语言模型（MLLMs）的快速发展，人们对现实世界和虚拟环境中复杂的多模态推理任务产生了浓厚的兴趣。这些任务需要协调多种能力，包括视觉感知、视觉推理、空间意识和目标推理。然而，现有的评估主要关注最终的任务完成情况，往往将评估降级为孤立的能力，如视觉定位和视觉问答。较少关注在多模态环境中全面和定量地分析推理过程，这对于理解模型行为和推理机制至关重要。为了解决这个问题，本文提出了MM-Escape，一个可扩展的基准，用于研究多模态推理，灵感来自现实世界的逃脱游戏。\n\n## 🚀 核心方法\n💡 创新点1：MM-Escape基准\nMM-Escape强调最终任务完成和中间模型行为。为了实现这一点，本文开发了EscapeCraft，一个可定制的开放环境，使模型能够进行自由探索，以评估多模态推理。\n\n💡 创新点2：EscapeCraft环境\nEscapeCraft是一个可定制的开放环境，允许模型进行自由探索，以评估多模态推理。它支持自定义和可扩展的场景生成，并定义了任务的推理路径。\n\n## 📈 实验结果\n实验结果表明，无论规模大小，MLLMs都能成功完成最简单的房间逃脱任务，其中一些模型表现出类似人类的探索策略。然而，随着任务难度的增加，性能急剧下降。此外，观察到性能瓶颈因模型而异，揭示了它们在多模态推理能力方面的不同失败模式和局限性，例如没有自适应探索的重复轨迹、由于视觉空间意识差而陷入角落，以及无效地使用获得的道具，如钥匙。\n\n## 💬 可借鉴之处\n本文提出的MM-Escape基准和EscapeCraft环境为评估和改进MLLMs的多模态推理能力提供了新的思路和方法。通过关注中间模型行为和推理过程，可以更全面地理解模型行为和推理机制，从而推动MLLMs在多模态推理任务上的发展。","llm_summary_res_status":200}
{"title":"From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons","authors":"Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, Alexander Toshev","summary":"We examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches.","url":"http:\/\/arxiv.org\/abs\/2412.08442v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.08442v1","published":1733929585000,"comment":null,"pdf_text":"From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons\nAndrew Szot*1,2\nBogdan Mazoure∗1\nOmar Attia1\nAleksei Timofeev1\nHarsh Agrawal1\nDevon Hjelm1\nZhe Gan1\nZsolt Kira2\nAlexander Toshev1\n1 Apple, 2 Georgia Tech\na.szot@apple.com, toshev@apple.com\nAbstract\nWe examine the capability of Multimodal Large Language\nModels (MLLMs) to tackle diverse domains that extend be-\nyond the traditional language and vision tasks these models\nare typically trained on. Specifically, our focus lies in areas\nsuch as Embodied AI, Games, UI Control, and Planning.\nTo this end, we introduce a process of adapting an MLLM\nto a Generalist Embodied Agent (GEA). GEA is a single\nunified model capable of grounding itself across these var-\nied domains through a multi-embodiment action tokenizer.\nGEA is trained with supervised learning on a large dataset\nof embodied experiences and with online RL in interactive\nsimulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal\nthe importance of training with cross-domain data and on-\nline RL for building generalist agents. The final GEA model\nachieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist\nmodels and benchmark-specific approaches.\n1. Introduction\nFoundation Models have demonstrated broad capabilities\nacross language and image understanding tasks [5, 16, 30,\n33, 42, 43, 45, 46, 51, 58, 66, 95, 103]. In particular, Mul-\ntimodal LLMs (MLLMs) – multimodal foundation models\ntrained on vast amounts of textual and image data – excel at\ntasks that are natural to their text and image training modal-\nities. As an extension of MLLMs, Vision-Language-Action\nmodels have been successfully applied in robotics and em-\nbodied AI [3, 11, 18, 80], as well as agents for the web\n[27, 37, 75, 102] and user interface (UI) control [28, 68, 89].\nThese applications have demonstrated that MLLMs can\nbe successfully applied to diverse domains for the purpose\nof controlling various embodiments like robots, playing\ngames, and controlling devices via UIs. As many of these\ndomains share similarities, it is natural to ask how a single\n*Core contributor\nManipulation\nPlanning\nVideo Games\nMLLM\nGEA-Base\nGEA\nPerformance on Mobile Manipulation Task \n0%\n57%\n83%\nSFT\nOnline RL\nUI Control\nNavigation\nSuccess Rate\nSet an alarm for 6am\nDestroy space invaders\nCollect the gold coin\nPickup the gray key\nPut an apple in the drawer\nHammer the nail\nMove the slider left\nPick the orange\nFigure 1. The Generalist Embodied Agent (GEA) is a multimodal\nLLM-based agent that can complete tasks from natural language\ninstructions across a variety of domains and embodiments span-\nning manipulation, planning, game playing, and UI control. A\npretrained MLLM is finetuned with supervised finetuning (SFT)\non a large dataset of embodied experiences. The final GEA model\nis then finetuned with reinforcement learning (RL). GEA achieves\ncompetitive results in generalization to unseen settings.\nagent can be trained to be generally proficient in all of these\ndomains. This is a challenging problem as many of these\ntasks require physics and geometric reasoning, their embod-\niments are either static or share morphologies via a mobile\nmanipulator, their applications require long-horizon plan-\nning, and many are partially observable and require reason-\ning over long sequences of observations. In addition, train-\ning with combined data across domains with these similar-\nities may yield cross-domain benefits, where a single agent\nmay outperform agents trained on individual domains.\nIn this work, we demonstrate an approach for adapting\nan MLLM into a single Generalist Embodied Agent (GEA)\nto solve a vast number of tasks across diverse domains span-\nning manipulation, navigation, video game playing, and UI\ncontrol. To enable GEA to control diverse embodiments,\n1\narXiv:2412.08442v1  [cs.LG]  11 Dec 2024\nwe learn a unified learned tokenization mechanism across\nall continuous and discrete action spaces. As Figure 1 il-\nlustrates, we then employ supervised finetuning (SFT) [87]\nto adapt a pretrained MLLM to predict actions from trajec-\ntories of agents successfully completing tasks. This SFT\ndataset spans over 2.2 million trajectories from diverse col-\nlection methods like human labelers or learned policies.\nWhile this SFT process produces a capable agent, it suffers\nfrom an inherent lack of data, specifically data diversity, and\nrarely exhibits robustness to mistakes. We thus also train\nGEA with a second stage of online reinforcement learning\n(RL) training over a subset of the domains where the agent\ncollects and learns from data in interactive simulators.\nWe demonstrate that GEA exhibits strong generalist\ncapabilities. Specifically, it reaches state-of-the-art perfor-\nmance across many benchmarks against other generalist\nagents and even outperforms or closely matches bespoke\nspecialist systems. For example, in the CALVIN manipula-\ntion benchmark [60], GEA reaches 90% success rate while\noperating on unseen instructions and background, which is\nnearly 10% higher than similar methods [48] and closely\nmatches the performance of specialist systems [35]. In a\nHabitat mobile pick task [78], GEA achieves 83% success\nin unseen scenes, outperforming a policy trained with RL\non the ground truth simulator state. Similarly, in Procgen\nvideo games [15] GEA reaches 44% of expert score, which\nis almost 20% higher than prior specialist [59] models.\nWe also analyze the relationship between the generalist\ncapabilities of GEA and its training data and base MLLM.\nWe demonstrate that training on the combined data from\na diverse set of domains for SFT provides a cross-domain\nperformance boost over using only per-domain data. Fi-\nnally, we explore the role of RL and online data collection\nfor building generalist agents and empirically demonstrate\nthe benefits of online RL over prior approaches of iterative\nSFT or offline RL.\nAs a further contribution to the community, we will re-\nlease the code for training and evaluating GEA along with\nthe GEA model itself. We will add the link to the code and\nmodel to this paper when they are ready for release.\n2. Related Work\nPrior works have explored building generalist agents by\ntraining policies on large multi-task datasets, illustrating\nthe importance of scaling interactive data to create capable\nmulti-task agents [20, 69, 88]. Additionally, prior works\nhave studied new architectures for generalist agents [24,\n86], while others focus on applying generalist agents to\nrobotic contexts [9, 10, 83]. Some research also investigates\ngeneralist models in domain-specific benchmarks [34, 82]\nor in cross-embodiment scenarios [17, 65]. Like GEA, these\napproaches leverage extensive data, yet our work empha-\nsizes the importance of adapting a pretrained MLLM via\nboth finetuning and online RL. For example, differences be-\ntween GEA and Gato [69], are that GEA leverages RL, uti-\nlizes a pretrained MLLM, learns a multi-embodiment action\ntokenizer, and focuses on evaluating generalization to new\ntask settings. As a result, GEA empirically outperforms\nGato in many settings.\nLike GEA, some prior work focuses on adapting\nMLLMs as agents. Works have proposed domain-specific\npipelines for using the zero-shot capabilities of MLLMs for\ndecision-making [3, 31, 50, 85, 91, 100], while our work\nfocuses on finetuning MLLMs.\nOther works investigate\nschemes for finetuning MLLMs for decision-making and\nthe benefits of doing so, but also in the context of specific\ndomains [48, 76, 80, 81]. Prior work also finetunes MLLMs\nas generalist agents [11, 36, 63]. However, our work shows\nresults across more diverse domains and studies the impor-\ntance of supervised learning with multi-domain data and RL\nfinetuning. Architecturally, GEA is different from Open-\nVLA [36] in that it uses a learned multi-embodiment ac-\ntion tokenizer as opposed to a uniform discretization, which\nprior work has demonstrated to perform better [81]. More-\nover, works have explored the value of finetuning MLLMs\nas UI agents [4, 19, 23, 62, 97]. While GEA explores adapt-\ning MLLMs as policies, there are also other ways to lever-\nage MLLMs such as via reward models [55, 56], world\nmodels [90], or environment generation [94].\nMore broadly, prior work has demonstrated how LLMs\ncan be used for agents that can reason and interact.\nVarious works focus on training LLM agents through\nspecific pipelines [101]. Similar to how GEA finetunes for\ndecision-making capabilities not present in the base LLM,\nother works demonstrate the value of finetuning LLMs for\nreasoning and problem-solving capabilities [77, 92]. Prior\nworks also demonstrate the value of finetuning LLMs with\nself-generated data [54], mistakes from the LLM [74], and\nwith RL [39].\nLikewise, GEA shows the importance of\nusing such RL training, beyond just supervised learning, to\ncreate a capable agent.\n3. Generalist Embodied Agent\n3.1. Problem Settings\nWe focus on language-specified tasks with visual observa-\ntions. Specifically, we consider the goal-specified Partially-\nObservable Markov Decision Processes (POMDPs) [8] with\nobservation space O, action space A, goal space G, and a\nreward model R. For brevity, we omit other elements of\nthe MDP. In our settings, G is represented by a textual de-\nscription of the task to solve. Observations consist of RGB\nimages from the agent, which can either be from an agent\ncamera in the case of embodied AI applications or screen-\nshots in the case of video games or UI interactions.\nWe consider a diverse set of environment types, which\n2\nVisual Encoder\nLLM\n￼k1\nt\n￼k2\nt\n￼kM\nt\nAction Tokens\nMulti-Embodiment Action \nDe-Tokenizer\nAction for \nEnvironment\n￼k1\nt\n￼kM−1\nt\nJoint velocity\nDelta joint position\nEnd-effector\nJump, left, …\nForward, left, …\nTap 23 47\nDiscrete \nControl\nContinuous \nControl\nUnified Token \nOutput Space\nVisual Bridge\nAgent: Fetch mobile robot. Actions: delta \njoint control… Instruction: pick an apple\n(Prompt)\n(Instruction)\nObservation History\nStatic Manipulation\nMobile Manipulation\nStatic Manipulation\nUI Control\nNavigation\nVideo Games\ndx dy dz \n[28,\n73]\n “move left”\n[278, 276]\nLLM Tokenizer\nResidual VQ-VAE Encoder\nTruncate for \nenvironment\n￼at\n￼ot\nGEA Component\nFigure 2. GEA utilizes a pretrained MLLM together with a multi-embodiment action tokenizer to enable a generalist agent to operate across\na wide range of domains, embodiments, and action spaces. GEA takes as input information about the embodiment and desired task with the\nembodiment prompt and instruction and the observation visuals (bottom). It produces a sequence of action tokens in the LLM vocabulary,\nwhich are decoded by the multi-embodiment action detokenizer into an action for the appropriate embodiment and action space.\nwe refer to as domains (see Table 1 for examples). These\ndomains specify a diverse set of action spaces spanning var-\nious robotic control spaces, high-level primitives, and com-\nputer UI interfaces. Our goal is to learn one policy that\noperates over a number of environments, which we denote\nby Mi = (Oi, Ai, Gi, Ri) for each environment i ∈E.\n3.2. GEA Architecture\nThe Generalist Embodied Agent (GEA) performs tasks by\nproducing actions that are executed in an environment con-\nditioned on observations, past actions, and a task descrip-\ntion. More formally, for timestep t in environment Mi, the\nGEA model takes as input an environment specific prompt\nPi, followed by a task instruction I ∈Gi and up to c inter-\nleaved previous observations and actions ot−c, at−c, . . . , ot\nfrom Oi and Ai. From these inputs, the GEA model pre-\ndicts action at ∈Ai to execute in the environment. The\nprompt provides information about the environment and\nembodiment to control. The task instruction is a natural\nlanguage description of the task the agent is to execute.\nMulti-Embodiment Action Tokenizer. We study Gen-\neralist Embodied Agent (GEA) adapted from an MLLM. As\nMLLMs naturally consume only text and images and gen-\nerate only text, we follow the findings of Szot et al. [81] to\nmodify the LLM vocabulary to represent actions. First, we\nrepresent all actions across {Mi}, i ∈E with two vocab-\nularies: Vdisc for discrete actions and Vcont for continuous\nactions, so that the final vocabulary is V = Vdisc ∪Vcont.\nA discrete action is described by language, and then this\nlanguage is tokenized into a sequence of text tokens repre-\nsenting this action. Vdisc is defined as all such text token\nsequences representing the discrete actions.\nAs continuous actions are not readily expressed as text,\nwe use a learned action tokenizer that maps each continuous\naction into a sequence of new tokens, whose vocabulary we\ndenote by Vcont. Details of how we train this tokenizer are in\nSection 4.2. We replace the |Vcont| most infrequently used\ntokens in the original LLM vocabulary with Vcont.\n4. Training\nGEA starts from a base MLLM and first trains a continu-\nous action tokenizer. As depicted in Figure 3, the MLLM\nis adapted to GEA-Base by supervised finetuning on em-\nbodied experiences. Next, GEA-Base is adapted to the full\nGEA model through supervised and reinforcement learning.\n4.1. Base MLLM\nThe primary consideration for selecting a base model be-\nyond its inherent vision-language strength is its ability to\nscale to long contexts as embodied data consists of long tra-\njectories of interleaved observations and actions. We thus\nbuild GEA off LLaVA-OneVision [44], a model specifically\ntrained to handle sequences of images through interleaved\nimage\/text pairs and videos which extends to GEA operat-\ning over a history of observations.\n4.2. Continuous Multi-Embodiment Tokenizer\nTo obtain a vocabulary Vcont for continuous actions and\na tokenizer\/de-tokenizer for these actions, we follow Szot\n3\net al. [81] and train a Residual VQ-VAE (RVQ) [40] model\nover action vectors. The RVQ model is a variational au-\ntoencoder that leverages a sequence of discrete embeddings\nto represent the data. Specifically, it encodes an action a\nas a sequence of M tokens k1(a), . . . , kM(a), where each\ntoken denotes a code from a learned vocabulary V m\ncont, for\nm ∈1, . . . , M.\nA key feature of RVQ is that the mth\nvocabulary is trained to encode the residual of the action\nafter it has been encoded with vocabularies 1, . . . , m −1.\nThis hierarchical encoding makes RVQ effective in pre-\ncisely representing continuous actions with a minimal num-\nber of discrete tokens. The final continuous action vocab-\nulary used by GEA is the union of the RVQ vocabularies\nVcont = S\nm V m\ncont.\nThe main distinction from Szot et al. [81] is that we train\na single tokenizer\/de-tokenizer across all continuous action\nspaces. As shown in Table 1, these spaces cover a variety\nof robotic control types, including end-effector, joint veloc-\nity, and joint position control. To facilitate training a unified\nRVQ, we pad all action vectors to the maximum action di-\nmension. During inference, we decode the predicted action\ntokens and then truncate the output to match the dimension-\nality of the specific embodiment’s action space. We use 2\ncodebooks each with 512 tokens and a token vector dimen-\nsion of 1024. Additional details on action tokenization are\nin Appendix A.\n4.3. Stage 1: Supervised-Instruction Finetuning\nThe first step of GEA is to use supervised-instruction\nfinetuning (SFT) to adapt the base MLLM for embodied\ndecision-making (left side of Fig. 3). We use a collection\nD = S\ni∈E Di of demonstration datasets from all environ-\nments E. During this stage, we use a standard cross-entropy\nloss over actions in the case of interactive data or responses\nin the case of vision-language data. As is typically done in\nMLLM training, we maximize the negative log-likelihood\nof predicting these output tokens for each example:\nLSFT(D) = −\nX\n(I,ot−c:t,at−c:t)∈D\nlog p(at|P, I, ot−c, at−c, . . . , ot) (1)\nWe then train the base MLLM over all the above datasets\nwith SFT to obtain the GEA-Base model.\nTraining Details. The entire GEA-Base model is ini-\ntialized from the base MLLM. We train for 75k updates us-\ning AdamW [53] with cosine learning rate decay and lin-\near learning rate warmup for the first 10% of training steps;\nlearning rate of 1e−5; global batch size of 256 and an ob-\nservation context length of c = 3. Training takes around 2\ndays on 8 nodes of 8xH100 GPUs (see Appendix B).\n4.4. Stage 2: Online Reinforcement Learning\nWhile the previously described SFT training process pro-\nduces a capable GEA-Base agent, it is only trained on a\nlimited set of expert trajectories, which rarely demonstrate\nPretrained \nMLLM\nGEA-Base\nSFT on interactive data\nGEA\nSFT on interactive data\nOnline RL in simulation\nTrain entire \nMLLM\nLoRA finetune LLM\nFigure 3. GEA training stages. First, a MLLM is adapted to GEA-\nBase by finetuning the entire MLLM with SFT on interactive data.\nNext, GEA-Base is finetuned jointly with online RL (PPO) and\nSFT on the original data with LoRA.\ndiverse behaviors like error recovery. Hence, we propose\nto utilize online RL for some of the environments. In a\nsecond stage of training, we continue to train GEA-Base\nwith RL in addition to SFT to obtain the final GEA model\n(right side of Fig. 3). For online RL, we train the GEA-Base\nagent with PPO [72], whose optimization loss is denoted by\nLPPO(Mi) for each environment in which we have a simu-\nlator i ∈EPPO ⊂E. We combine this objective with the SFT\nobjective from Eq. (1) to obtain the final GEA objective:\nLGEA =\nX\ni∈EPPO\nLPPO(Mi) + λ\nX\ni∈E\nLSFT(Di)\n(2)\nwhere we weight the SFT loss by λ = 0.1 to emphasize\nthe RL loss. This RL+SFT training stage on the GEA-Base\nmodel produces the final model we refer to as GEA.\nPPO Details. The GEA value function for RL is an MLP\nnetwork which is initialized from scratch. It takes as input\nthe MLLM final layer activations at the observation token\nstep just before the action tokens and an average pooled\nrepresentation of the visual embeddings from the MLLM\nvision encoder. The critic also optionally takes any privi-\nleged state information about the task since the critic is only\nused during training, and not during inference.\nTo stabilize RL training across numerous environments,\nwe use PopArt [84] return normalization to account for\nthe diverse reward distributions across these environments.\nSince the output space of the LLM can consist of many pos-\nsible tokens, we use constrained decoding to force the au-\ntoregressive action sampling to be within the action space\nfor the environment.\nFor continuous action tasks, this\namounts to constraining the output to the learned contin-\nuous action tokens.\nFor the discrete control tasks this\namounts to constraining the output to the valid language ac-\ntions (for example, ”pick apple” or ”right”). To account for\ndifferences in the valid distribution of actions per environ-\nment, we normalize the entropy for PPO so a single entropy\ncoefficient can apply to all tasks.\nTraining Details.\nSince GEA-Base already obtains\nsome success, and RL introduces GPU memory overhead\nvia environments simulated on the GPU, we use LoRA [29]\nto finetune the LLM while freezing all other components.\nAll environments use a rollout length of 128, a learning rate\n4\nDataset Name\nDomain\nAction Type\nEmbodiment Type\n# Trajs\nData Source\nOpenX [63]\nStatic Manip\nCont. Various\n22 Various Robots\n1.2M\nVarious\nMeta-World [98]\nStatic Manip.\nCont. EE+Gripper\nSawyer\n45k\nScripted\nCALVIN [60]\nStatic Manip.\nCont. EE+Gripper\nFranka Arm\n18k\nHuman\nManiskill [22]\nStatic Manip.\nCont. Joint Velocity\nROKAE xMate3Pro\n5k\nMotion Planner\nHabitat Pick [78]\nMobile Manip.\nCont. Joint Position + Base\nFetch\n50k\nRL Expert\nHabitat Place [78]\nMobile Manip.\nCont. Joint Position+Base\nFetch\n50k\nRL Expert\nHabitat Nav [78]\nNavigation\nCont. Velocity\nFetch\n13k\nShortest Path\nBabyAI [14]\nNavigation\nDiscrete\nVirtual\n50k\nShortest Path\nLangR [80]\nPlanning\nDiscrete\nFetch\n150k\nRL Expert\nProcgen [15]\nVideo Games\nDiscrete\nVirtual\n320k\nRL Expert\nAtari [7]\nVideo Games\nDiscrete\nVirtual\n286k\nRL Expert\nAndroidControl [47]\nUI Control\nMixed\nVirtual\n14k\nHuman\nTable 1. Overview of the embodied datasets used for training GEA. The actions in each dataset can be either discrete or continuous with a\nspecific control space for the continuous actions. The embodiment type describes the agent being controlled. Each trajectory in a dataset\nrefers to a sequence of images and actions. The data source refers to the collection method for these trajectories.\nof 3e−4, an entropy coefficient of 1e−4, and a value func-\ntion learning loss of 1.5e−4. RL finetuning uses 8 nodes\nof 8xH100 GPUs with 4 parallel environments per GPU.\nEnvironments are partitioned per node into 3 of the GPUs\nrunning HabPick, 3 running Procgen, and 2 running LangR.\nThe SFT per-device batch size is 2. We train for 100M cu-\nmulative steps across all tasks, which takes around 1 day.\nFull RL training details are in Appendix C.\n5. Datasets and Environments\nWe use a diverse set of domains with associated environ-\nments and datasets (see Table 1). This section introduces\nthese domains followed by an explanation of how we use\nthem in stage 1 and 2 of our training procedure.\nStatic Manipulation.\nThese datasets are of a fixed\nrobot manipulator interacting with objects. Some of these\ndatasets are simulated table top interactions with rigid-body\nobjects such as Meta-World [99], CALVIN [60] and Man-\niskill [22]. We also leverage a large dataset of interactions\non real robot platforms [63]. These datasets span a vari-\nety of control spaces in end-effector and joint control. The\ncamera is typically mounted in a static position so that the\nworkspace and robot arm are fully visible.\nMobile Manipulation. We also investigate setups where\nthe robot manipulator moves via a mobile base. We use the\nobject rearrangement tasks from the Habitat platform [71]\nfor datasets in these tasks. These datasets cover object pick-\ning and placing tasks where the robot starts up to 2 meters\naway from the object. The robot has to coordinate moving\nits base and arm to successfully pick up the object. These\ndatasets involve first person egocentric cameras.\nNavigation. We also use datasets of navigation in iso-\nlation. We use datasets of simulated robot navigation in\nHabitat. We also use navigation in grid-world environments\nfrom BabyAI [14]. Both datasets were collected with short-\nest path experts.\nVideo games. We use datasets from two standard bench-\nmarks for decision making in video games, Procgen [15]\nand Atari [7]. Both datasets were collected by RL agents\nwhich were separately trained to solve each individual\ngame. We convert these tasks to be language-conditioned\nby providing the game name along with a short description\nof the game’s objective and rules. We only train on success-\nful trajectories.\nPlanning. We use a dataset of successful episodes in\nthe LangR dataset [80]. In this task the agent has to se-\nlect between skill primitives that accomplish long horizon\nlanguage-specified rearrangement tasks for a home robot.\nUI Control. We use the AndroidControl [47] dataset of\nUI interaction in Android devices spanning 833 apps. The\nactions are combinations of tap actions specified by screen\ncoordinates and text typing.\nVision language instruction data. To improve general-\nization of the model, we also include data used for train-\ning the original MLLM, which prior work has found is\nuseful when finetuning MLLMs as control policies [11].\nWe used the following datasets of text and images without\nany actions: VQAv2 [21], OKVQA [57], A-OKVQA [73],\nGQA [32] and the LLaVA-Instruct-150k dataset [52].\nStage 1 Training: SFT Data. To obtain embodied data\nfor Stage 1 Training (see Sec. 4.3), we collect a large dataset\nof language-conditioned behaviors from all of the above\ndomains consisting of 2.2M trajectories.\nAll trajectories\nare successful examples of language-conditioned behaviors\nwith visual observations. The data is from diverse collec-\ntion sources such as human demonstrations, RL-based poli-\ncies, and motion planners. The dataset is diverse and spans\nthousands of distinct tasks and many embodiments (see Ap-\n5\nGEA\nPrior Work\n# Tasks\nGeneralization Type\nManipulation\nMeta-World\n94.7\n84 MLLM+IL [81]S\n87.0 [69]G\n45\nobject positions\nCALVIN (ABC →D)\n90.0\n82.4 MLLM+IL [48]S\n92.2 IL+pointcloud[35]\n34*\ninstructions, background\nManiskill\n13.6\n6.5 IL [22]S\n47.8 IL+PPO [22]S\n5\nobject positions\nHabitat Pick\n82.5\n29 IL [81]S\n81.0 RL + sim stateS\n20\nhouse\nHabitat Place\n93.5\n95.5 RL + sim stateS\n10\nhouse\nVideo Games\nProcgen\n44.0\n25 [59]S\n16\nbackground\nAtari\n32.7\n31 [69]G\n85 Offline RL [41]S\n44\nnone\nNavigation\nHabitat Nav\n62.5\n72 [78]S\n10\nhouse\nBabyAI\n91.1\n93.2 [69]G\n17*\ninstructions, grid state\nUI Control\nAndroidControl\n57.3\n45 GPT-4o+SoM [93]G\n35*\ninstructions\nPlanning\nLangR\n50.0\n51 MLLM+RL[81]S\n10*\ninstructions, house\nTable 2. Zero-shot generalization of GEA to new tasks in terms of success rate % and in the video games tasks % of expert performance. We\ncompare against prior works consisting of domain specialists (with superscript “S”) that are trained on only data from that benchmark and\ndomain generalists (with superscript “G”) that are trained on data from several benchmarks. Bold indicates best, underline close second,\nand gray coloring that the method assumes access to additional input modalities like pointcloud or ground truth simulator state, meaning it\nis not a fair comparison to GEA. The “Prior Work” column also gives details about how the methods were trained (IL or RL) and if they\nassume additional input modalities. The “# Tasks” column gives a general indication of the number of distinct evaluation settings with a\n“*” indicating each task also has diverse language instructions.\npendix D for full details).\nStage 2 Training: RL Environments.\nFor Stage 2\nonline RL (see Sec. 4.4) we use environments from the\nthree domains of Habitat Pick [78], Language Rearrenge-\nment (LangR) [80], and Procgen [15].\nThus, we define\nEPPO = {HabPick, LangR, ProcGen}.\nHabitat Pick and\nLangR are simulated in the Habitat platform [71] and have\nreward functions for achieving and making progress to-\nwards the goal. In Procgen, we use all 16 games for RL and\nuse the game specific reward functions (see Appendix C for\nfull details).\n6. Empirical Evaluation\nWe empirically demonstrate the ability of GEA as a general-\nist agent that can generalize to new instructions and settings\nacross diverse embodiments and domains. We assess the\nrole of the RL training in achieving this goal. In ablations,\nwe study the impact of scaling data between multiple do-\nmains, compare RL to other forms of policy collected data,\nthe impact of the the base MLLM.\n6.1. GEA Generalization Capabilities\nIn this section, we evaluate the generalization capabilities\nof the final GEA model. We use the associated benchmarks\nfrom the datasets in Table 1, which span manipulation, nav-\nigation, video games, UI control, and planning.\nThese\nbenchmarks evaluate agents in new settings not present in\nthe training data, such as new object positions, scenes, vi-\nsual backgrounds, tasks, or instructions. All benchmarks\nspecify evaluation instructions in natural language and re-\nquire the agent to operate from visual observations.\nWe report the “online” performance of GEA, meaning\nwe evaluate its performance in an interactive simulator. The\nonly exception is AndroidControl, where we instead check\nthe correspondence with a ground truth test trajectory [68].\nEach benchmark also evaluates agents over many distinct\ntasks.\nFor example, in the Procgen video game bench-\nmark, we report the average Procgen performance over all\n16 Procgen games each of which is an entirely different\nvideo game. Full evaluation details are in Appendix E.\nWe seek to comprehensively frame the empirical per-\nformance of GEA relative to prior work on our evaluated\n6\nHabitat\nPick\nProcgen\nLangR\nAll Other\nGEA\n82.5\n44.0\n50.0\n70.5\nGEA-Base\n60.5\n36.1\n15.5\n69.5\nTable 3. Effect of stage 2 RL training on GEA-Base (7b model).\nTasks included in RL training increase their generalization per-\nformance. Performance on the remaining tasks slightly increases\nthanks to continued SFT training.\nbenchmarks.\nFirst, in all benchmarks, we use only im-\nages as observations without using any privileged informa-\ntion such as the simulation state or additional observations\nsuch as 3D point clouds. Second, we evaluate our single\nGEA model across all environments, which is referred to as\na generalist. Some of the approaches we compare against\nare trained on data from a single environment, and we re-\nfer to those as specialists. In other comparative approaches,\nthe split between train or test data is unclear. For exam-\nple, Gato [69] is a generalist model like GEA, yet evaluates\nand trains on some less diverse tasks with their own train-\ning datasets, which are not released. Appendix F.1 discusses\nthese connections in detail.\nTable 2 summarizes the comparative evaluation. GEA\nexcels at manipulation tasks, either exceeding or match-\ning the performance of specialist models. For example, in\nMeta-World GEA greatly outperforms both specialist and\ngeneralist models trained for this task with a 7% absolute in-\ncrease relative to the best-performing baseline. In CALVIN,\nGEA outperforms a variety of recent specialist models.\nGEA also performs closely to the specialist 3D Diffuser Ac-\ntor method [35], which uses a manipulation-specific action\nrepresentation of end-effector key points and uses a depth\ncamera to represent the scene as a 3D feature cloud. GEA\nonly uses the third PoV RGB camera and does not use an\naction or observation space specific to table-top manipula-\ntion. GEA outperforms the baseline in Habitat Pick and\nclosely matches it in Habitat Place despite these baselines\nbeing trained with the ground truth simulator state. In Man-\niskill, the difficult, often occluded, camera view results in\nlow overall success rates, yet GEA outperforms other re-\nsults that only use IL. However, GEA is outperformed by\nmethods that use RL in this benchmark.\nIn video game benchmarks, GEA outperforms the spe-\ncialist model baseline in Procgen. In Atari, GEA outper-\nforms the generalist Gato model [69]. However, in Atari,\nGEA is outperformed by Multi-Game DT [41], which uses\noffline RL from suboptimal demonstrations. In Atari, GEA\ndoes not learn with offline or online RL.\nIn the BabyAI navigation benchmark, GEA is close in\nperformance to Gato [69] despite GEA using RGB ren-\nderings of the top-down view rather than any underlying\nstate information and 100x fewer demonstrations for this\nHabitat\nPick\nCALVIN Procgen\nAndroid\nControl\nBabyAI\nGEA-Base\n57.0\n48.0\n24.5\n50.5\n84.7\nDomain Specific\n54.5\n35.5\n23.7\n48.9\n82.1\nOnly LLM\n9.5\n0.0\n7.6\n26.4\n49.4\nOnly VisEncoder\n34.5\n13.0\n24.5\n28.3\n70.6\nNone\n9.0\n0.0\n7.4\n14.1\n44.4\nTable 4. We present results using LLaVA-OneVision-500m as the\nMLLM base (row 1) as well as training with only domain specific\ndata (row 2). We also present results for a transformer of the same\narchitecture but only the LLM subnet is initialized (row 3), or the\nvisual encoder (row 4), or neither (row 5).\nenvironment. In Habitat Nav, GEA underperforms an RL-\ntrained expert. This gap could be due to the context of GEA\nonly consisting of the previous three observations, which\ncould limit its ability in partially observable settings. In\nUI control, another discrete action task, GEA outperforms\nGPT-4o [64] with set of marks [93] generated by a UI detec-\ntion model. This demonstrates that GEA benefits from be-\ning specialized for interactive decision-making even against\na powerful LLM and specialized perception system. Fi-\nnally, in the discrete control benchmark of the LangR plan-\nning task, GEA closely matches the performance of a spe-\ncialist baseline, which trains on only this task with RL.\nComparative Benefit of RL to SFT. Training with\nRL was important to achieving these strong results. Ta-\nble 3 compares the performance of GEA-Base, which is\nonly trained with SFT on expert demonstrations, and GEA,\nwhich is additionally trained with a second stage of RL. The\nresults show that the success rate of GEA greatly increases\non Habitat Pick, Procgen, and LangR, which are the envi-\nronments where RL was used. Furthermore, the average\nsuccess rate across all the other tasks is not influenced by\nRL due to the continued joint SFT training.\n6.2. GEA Training and Model Analysis\nIn this section, we explore the relationship between the gen-\neralist capabilities of GEA and its training data. We analyze\nthe role of embodied SFT data for adapting the MLLM for\ninteraction and the importance of online data. We also eval-\nuate the importance of the base MLLM. For all results in\nthis section, we train all models with 32% of the original\nembodied SFT data and 40k updates to ease the computa-\ntional burden of the analysis. We also evaluate on the tasks\nfrom Table 2 with a reduced number of evaluation episodes\nwith around 200 episodes per benchmark. Appendix F.2 de-\nscribes this analysis evaluation and dataset in detail.\nImpact of Multi-Domain Data. We evaluate training a\ngeneralist model on data from all diverse domains, versus\ntraining a model only on data from the particular target do-\nmain. Specifically, we train the smaller LLaVA-OneVision-\n500m model on either data from a single domain (“Domain\n7\nMLLM\nSuccess SFT\nMLLM\nOffline RL\nMLLM\nOnline RL\nGEA-Base\nGEA-Base\nSuccess SFT\nGEA-Base\nOffline RL\nGEA-Base\nOnline RL\n0\n20\n40\n60\n80\nSuccess (%)\n17\n23\n41\n57\n46\n48\n83\nFigure 4. Online learning in Habitat Pick. MLLM methods fine-\ntune LLaVA-OV while other methods finetune GEA-Base.\nSpecific”) or data across all domains (“GEA-Base ”). Com-\nparing these options in Table 4 shows that across all the\nbenchmarks, training with all the data is beneficial. How-\never, the gain is smaller in some of the domains like An-\ndroid Control and Procgen, likely due to less overlap with\nthe other training domains as opposed to the wealth of ma-\nnipulation data we train with.\nAppendix G.4 contains a\nmore granular investigation into the pairwise transferabil-\nity between each dataset and also supports this conclusion\nthat GEA benefits from multi-domain data.\nImpact of Policy-Collected Data.\nNext, we explore\nthe role of learning from data sources beyond SFT on ex-\npert demonstrations. While GEA-Base is a capable embod-\nied policy, it is only trained on successful demonstrations.\nThese demonstrations rarely exhibit recovery behaviors or\nrobustness to non-expert behaviors. Unlike typical MLLM\napplications such as visual question answering, in interac-\ntive tasks, agents trained with expert data can suffer from\nthe problem of “covariate shift” where small agent errors\ncause the observation distribution to shift from the expert’s\nand for errors to compound [70].\nWe analyze how GEA-Base can be trained with addi-\ntional data to improve performance in the Habitat Pick task\nand compare the following alternatives. First, GEA-Base\nSuccess SFT collects 10k successful examples in the en-\nvironment with the GEA-Base policy and then trains on\nthese successes with supervised learning. GEA-Base Offline\nRL collects 10k trajectories consisting of both successes\nand failures, both labeled with the dense Habitat Pick re-\nward, and then trains on these with the IQL offline-RL algo-\nrithm [38]. GEA Online RL finetunes GEA-Base with PPO,\nwhich leverages online interactions with the simulator like\nthe GEA Stage-2 training (but omits the joint SFT loss). We\nagain use the smaller base LLaVA-OneVision-500m model\nfor these experiments. Figure 4 shows the results of these\nvariations.\nA main takeaway message is the strong impact of on-\nline RL on top of a finetuned MLLM, which increases the\nsuccess of GEA-Base from 57% to 83%, despite the lat-\nter being trained on 50k successful Habitat Pick demonstra-\ntions. Online RL outperforms both Success SFT and IQL\nLangR\nProcgen\nAndroid Control\nHabitat Pick\nCALVIN\nBabyAI\n0\n20\n40\n60\n80\nSuccess (%)\nLLaVA-OV-500m\nLLaVA-OV-7b\nLLaVA-1.5-7b\nMM1.5-1.2b\nMM1.5-6.4b\nFigure 5. Analyzing the impact of training GEA with different\nbase MLLMs with different parameter counts.\noffline RL, highlighting the need for online interactions. It\nis worth noting that applying success SFT and offline RL\non top of GEA-Base decreases the model’s performance,\nwhich could be due to the lack of diverse data.\nThese results further show that online RL is beneficial\nwhen applied on top of a finetuned model with domain\ndata. Online RL alone is unable to bring the performance\nof the base MLLM to GEA-Base without the stage 1 SFT.\nAppendix G.2 explores this further and shows that GEA-\nBase is also far more sample efficient with RL than the base\nMLLM. This analysis demonstrates it is important for the\nGEA model to use RL in combination with SFT.\nImpact of pretrained MLLM. We assess the impor-\ntance of the pretrained MLLM in the GEA architecture. To\ndo so, we present results using a base model that has an ar-\nchitecture identical to the pretrained MLLM. However, in-\nstead of initializing the full model with LLaVA-OneVision\nwe initialize only the LLM or the visual encoder with the\ncorresponding subnet weights.\nThe results in Table 4 show that the full MLLM has a\nsubstantial impact on performance. Although this isn’t sur-\nprising, it is important to note that the visual encoder ini-\ntialization seems to have a stronger impact on the final per-\nformance compared to the LLM. We conjecture that this is\nbecause the benchmarks require visual generalization, and\ntraining the LLaVA-OneVision SigLIP visual encoder from\nscratch with only the embodied data is challenging.\nFurther, we analyze the impact of the base MLLM size\non the GEA-Base. We use two classes of backbone mod-\nels, LLaVA-OneVision [44] and MM1.5 [58], and use two\nsizes for each of them (0.5B and 7B for the former, and\n1.2B and 6.4B for the latter). As shown in Figure 5, in\nour agentic applications increasing model capacity leads\nto stronger performance, both within and also across\nbackbone model class. Interestingly, the class of models\nhas little effect as we see very similar performance across\nmultiple backbone models. The three different 7B models\nhave been trained on different web-scale data. Nevertheless,\ntheir difference has a negligible impact on GEA.\nAdditionally, in Appendix G.3, we show that GEA train-\ning and evaluation is robust to the random seed.\n8\n7. Conclusion\nIn this work, we studied how finetuning pretrained MLLMs\nwith large-scale embodied experience via expert trajecto-\nries and online RL unlocks their ability to act as Generalist\nEmbodied Agent. To interface with diverse embodiments,\nGEA uses a learned action tokenizer. We illustrate the im-\nportance of RL finetuning for GEA, which results in com-\npetitive results across a variety of domains spanning manip-\nulation, video games, navigation, UI control, and planning.\nWhile GEA demonstrates impressive abilities across a\nwide diversity of tasks, it is still not at the level of a foun-\ndation model for decision-making like similar models for\nlanguage and vision [1]. GEA cannot control arbitrary em-\nbodiments and operate in arbitrary environments zero-shot.\nFurthermore, the performance of GEA in some domains,\nsuch as Maniskill, Atari, and AndroidControl, is far from\nperfect. Extending RL to these environments could be a\nsolution. Future work can continue to scale GEA to more\ntasks to extend its generalist capabilities.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nFrom Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons\n```\n#### 2. 论文摘要\n```\nWe examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches.\n```\n\n#### 3. 论文全文\n```\nFrom Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons\nAndrew Szot*1,2\nBogdan Mazoure∗1\nOmar Attia1\nAleksei Timofeev1\nHarsh Agrawal1\nDevon Hjelm1\nZhe Gan1\nZsolt Kira2\nAlexander Toshev1\n1 Apple, 2 Georgia Tech\na.szot@apple.com, toshev@apple.com\nAbstract\nWe examine the capability of Multimodal Large Language\nModels (MLLMs) to tackle diverse domains that extend be-\nyond the traditional language and vision tasks these models\nare typically trained on. Specifically, our focus lies in areas\nsuch as Embodied AI, Games, UI Control, and Planning.\nTo this end, we introduce a process of adapting an MLLM\nto a Generalist Embodied Agent (GEA). GEA is a single\nunified model capable of grounding itself across these var-\nied domains through a multi-embodiment action tokenizer.\nGEA is trained with supervised learning on a large dataset\nof embodied experiences and with online RL in interactive\nsimulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal\nthe importance of training with cross-domain data and on-\nline RL for building generalist agents. The final GEA model\nachieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist\nmodels and benchmark-specific approaches.\n1. Introduction\nFoundation Models have demonstrated broad capabilities\nacross language and image understanding tasks [5, 16, 30,\n33, 42, 43, 45, 46, 51, 58, 66, 95, 103]. In particular, Mul-\ntimodal LLMs (MLLMs) – multimodal foundation models\ntrained on vast amounts of textual and image data – excel at\ntasks that are natural to their text and image training modal-\nities. As an extension of MLLMs, Vision-Language-Action\nmodels have been successfully applied in robotics and em-\nbodied AI [3, 11, 18, 80], as well as agents for the web\n[27, 37, 75, 102] and user interface (UI) control [28, 68, 89].\nThese applications have demonstrated that MLLMs can\nbe successfully applied to diverse domains for the purpose\nof controlling various embodiments like robots, playing\ngames, and controlling devices via UIs. As many of these\ndomains share similarities, it is natural to ask how a single\n*Core contributor\nManipulation\nPlanning\nVideo Games\nMLLM\nGEA-Base\nGEA\nPerformance on Mobile Manipulation Task \n0%\n57%\n83%\nSFT\nOnline RL\nUI Control\nNavigation\nSuccess Rate\nSet an alarm for 6am\nDestroy space invaders\nCollect the gold coin\nPickup the gray key\nPut an apple in the drawer\nHammer the nail\nMove the slider left\nPick the orange\nFigure 1. The Generalist Embodied Agent (GEA) is a multimodal\nLLM-based agent that can complete tasks from natural language\ninstructions across a variety of domains and embodiments span-\nning manipulation, planning, game playing, and UI control. A\npretrained MLLM is finetuned with supervised finetuning (SFT)\non a large dataset of embodied experiences. The final GEA model\nis then finetuned with reinforcement learning (RL). GEA achieves\ncompetitive results in generalization to unseen settings.\nagent can be trained to be generally proficient in all of these\ndomains. This is a challenging problem as many of these\ntasks require physics and geometric reasoning, their embod-\niments are either static or share morphologies via a mobile\nmanipulator, their applications require long-horizon plan-\nning, and many are partially observable and require reason-\ning over long sequences of observations. In addition, train-\ning with combined data across domains with these similar-\nities may yield cross-domain benefits, where a single agent\nmay outperform agents trained on individual domains.\nIn this work, we demonstrate an approach for adapting\nan MLLM into a single Generalist Embodied Agent (GEA)\nto solve a vast number of tasks across diverse domains span-\nning manipulation, navigation, video game playing, and UI\ncontrol. To enable GEA to control diverse embodiments,\n1\narXiv:2412.08442v1  [cs.LG]  11 Dec 2024\nwe learn a unified learned tokenization mechanism across\nall continuous and discrete action spaces. As Figure 1 il-\nlustrates, we then employ supervised finetuning (SFT) [87]\nto adapt a pretrained MLLM to predict actions from trajec-\ntories of agents successfully completing tasks. This SFT\ndataset spans over 2.2 million trajectories from diverse col-\nlection methods like human labelers or learned policies.\nWhile this SFT process produces a capable agent, it suffers\nfrom an inherent lack of data, specifically data diversity, and\nrarely exhibits robustness to mistakes. We thus also train\nGEA with a second stage of online reinforcement learning\n(RL) training over a subset of the domains where the agent\ncollects and learns from data in interactive simulators.\nWe demonstrate that GEA exhibits strong generalist\ncapabilities. Specifically, it reaches state-of-the-art perfor-\nmance across many benchmarks against other generalist\nagents and even outperforms or closely matches bespoke\nspecialist systems. For example, in the CALVIN manipula-\ntion benchmark [60], GEA reaches 90% success rate while\noperating on unseen instructions and background, which is\nnearly 10% higher than similar methods [48] and closely\nmatches the performance of specialist systems [35]. In a\nHabitat mobile pick task [78], GEA achieves 83% success\nin unseen scenes, outperforming a policy trained with RL\non the ground truth simulator state. Similarly, in Procgen\nvideo games [15] GEA reaches 44% of expert score, which\nis almost 20% higher than prior specialist [59] models.\nWe also analyze the relationship between the generalist\ncapabilities of GEA and its training data and base MLLM.\nWe demonstrate that training on the combined data from\na diverse set of domains for SFT provides a cross-domain\nperformance boost over using only per-domain data. Fi-\nnally, we explore the role of RL and online data collection\nfor building generalist agents and empirically demonstrate\nthe benefits of online RL over prior approaches of iterative\nSFT or offline RL.\nAs a further contribution to the community, we will re-\nlease the code for training and evaluating GEA along with\nthe GEA model itself. We will add the link to the code and\nmodel to this paper when they are ready for release.\n2. Related Work\nPrior works have explored building generalist agents by\ntraining policies on large multi-task datasets, illustrating\nthe importance of scaling interactive data to create capable\nmulti-task agents [20, 69, 88]. Additionally, prior works\nhave studied new architectures for generalist agents [24,\n86], while others focus on applying generalist agents to\nrobotic contexts [9, 10, 83]. Some research also investigates\ngeneralist models in domain-specific benchmarks [34, 82]\nor in cross-embodiment scenarios [17, 65]. Like GEA, these\napproaches leverage extensive data, yet our work empha-\nsizes the importance of adapting a pretrained MLLM via\nboth finetuning and online RL. For example, differences be-\ntween GEA and Gato [69], are that GEA leverages RL, uti-\nlizes a pretrained MLLM, learns a multi-embodiment action\ntokenizer, and focuses on evaluating generalization to new\ntask settings. As a result, GEA empirically outperforms\nGato in many settings.\nLike GEA, some prior work focuses on adapting\nMLLMs as agents. Works have proposed domain-specific\npipelines for using the zero-shot capabilities of MLLMs for\ndecision-making [3, 31, 50, 85, 91, 100], while our work\nfocuses on finetuning MLLMs.\nOther works investigate\nschemes for finetuning MLLMs for decision-making and\nthe benefits of doing so, but also in the context of specific\ndomains [48, 76, 80, 81]. Prior work also finetunes MLLMs\nas generalist agents [11, 36, 63]. However, our work shows\nresults across more diverse domains and studies the impor-\ntance of supervised learning with multi-domain data and RL\nfinetuning. Architecturally, GEA is different from Open-\nVLA [36] in that it uses a learned multi-embodiment ac-\ntion tokenizer as opposed to a uniform discretization, which\nprior work has demonstrated to perform better [81]. More-\nover, works have explored the value of finetuning MLLMs\nas UI agents [4, 19, 23, 62, 97]. While GEA explores adapt-\ning MLLMs as policies, there are also other ways to lever-\nage MLLMs such as via reward models [55, 56], world\nmodels [90], or environment generation [94].\nMore broadly, prior work has demonstrated how LLMs\ncan be used for agents that can reason and interact.\nVarious works focus on training LLM agents through\nspecific pipelines [101]. Similar to how GEA finetunes for\ndecision-making capabilities not present in the base LLM,\nother works demonstrate the value of finetuning LLMs for\nreasoning and problem-solving capabilities [77, 92]. Prior\nworks also demonstrate the value of finetuning LLMs with\nself-generated data [54], mistakes from the LLM [74], and\nwith RL [39].\nLikewise, GEA shows the importance of\nusing such RL training, beyond just supervised learning, to\ncreate a capable agent.\n3. Generalist Embodied Agent\n3.1. Problem Settings\nWe focus on language-specified tasks with visual observa-\ntions. Specifically, we consider the goal-specified Partially-\nObservable Markov Decision Processes (POMDPs) [8] with\nobservation space O, action space A, goal space G, and a\nreward model R. For brevity, we omit other elements of\nthe MDP. In our settings, G is represented by a textual de-\nscription of the task to solve. Observations consist of RGB\nimages from the agent, which can either be from an agent\ncamera in the case of embodied AI applications or screen-\nshots in the case of video games or UI interactions.\nWe consider a diverse set of environment types, which\n2\nVisual Encoder\nLLM\n￼k1\nt\n￼k2\nt\n￼kM\nt\nAction Tokens\nMulti-Embodiment Action \nDe-Tokenizer\nAction for \nEnvironment\n￼k1\nt\n￼kM−1\nt\nJoint velocity\nDelta joint position\nEnd-effector\nJump, left, …\nForward, left, …\nTap 23 47\nDiscrete \nControl\nContinuous \nControl\nUnified Token \nOutput Space\nVisual Bridge\nAgent: Fetch mobile robot. Actions: delta \njoint control… Instruction: pick an apple\n(Prompt)\n(Instruction)\nObservation History\nStatic Manipulation\nMobile Manipulation\nStatic Manipulation\nUI Control\nNavigation\nVideo Games\ndx dy dz \n[28,\n73]\n “move left”\n[278, 276]\nLLM Tokenizer\nResidual VQ-VAE Encoder\nTruncate for \nenvironment\n￼at\n￼ot\nGEA Component\nFigure 2. GEA utilizes a pretrained MLLM together with a multi-embodiment action tokenizer to enable a generalist agent to operate across\na wide range of domains, embodiments, and action spaces. GEA takes as input information about the embodiment and desired task with the\nembodiment prompt and instruction and the observation visuals (bottom). It produces a sequence of action tokens in the LLM vocabulary,\nwhich are decoded by the multi-embodiment action detokenizer into an action for the appropriate embodiment and action space.\nwe refer to as domains (see Table 1 for examples). These\ndomains specify a diverse set of action spaces spanning var-\nious robotic control spaces, high-level primitives, and com-\nputer UI interfaces. Our goal is to learn one policy that\noperates over a number of environments, which we denote\nby Mi = (Oi, Ai, Gi, Ri) for each environment i ∈E.\n3.2. GEA Architecture\nThe Generalist Embodied Agent (GEA) performs tasks by\nproducing actions that are executed in an environment con-\nditioned on observations, past actions, and a task descrip-\ntion. More formally, for timestep t in environment Mi, the\nGEA model takes as input an environment specific prompt\nPi, followed by a task instruction I ∈Gi and up to c inter-\nleaved previous observations and actions ot−c, at−c, . . . , ot\nfrom Oi and Ai. From these inputs, the GEA model pre-\ndicts action at ∈Ai to execute in the environment. The\nprompt provides information about the environment and\nembodiment to control. The task instruction is a natural\nlanguage description of the task the agent is to execute.\nMulti-Embodiment Action Tokenizer. We study Gen-\neralist Embodied Agent (GEA) adapted from an MLLM. As\nMLLMs naturally consume only text and images and gen-\nerate only text, we follow the findings of Szot et al. [81] to\nmodify the LLM vocabulary to represent actions. First, we\nrepresent all actions across {Mi}, i ∈E with two vocab-\nularies: Vdisc for discrete actions and Vcont for continuous\nactions, so that the final vocabulary is V = Vdisc ∪Vcont.\nA discrete action is described by language, and then this\nlanguage is tokenized into a sequence of text tokens repre-\nsenting this action. Vdisc is defined as all such text token\nsequences representing the discrete actions.\nAs continuous actions are not readily expressed as text,\nwe use a learned action tokenizer that maps each continuous\naction into a sequence of new tokens, whose vocabulary we\ndenote by Vcont. Details of how we train this tokenizer are in\nSection 4.2. We replace the |Vcont| most infrequently used\ntokens in the original LLM vocabulary with Vcont.\n4. Training\nGEA starts from a base MLLM and first trains a continu-\nous action tokenizer. As depicted in Figure 3, the MLLM\nis adapted to GEA-Base by supervised finetuning on em-\nbodied experiences. Next, GEA-Base is adapted to the full\nGEA model through supervised and reinforcement learning.\n4.1. Base MLLM\nThe primary consideration for selecting a base model be-\nyond its inherent vision-language strength is its ability to\nscale to long contexts as embodied data consists of long tra-\njectories of interleaved observations and actions. We thus\nbuild GEA off LLaVA-OneVision [44], a model specifically\ntrained to handle sequences of images through interleaved\nimage\/text pairs and videos which extends to GEA operat-\ning over a history of observations.\n4.2. Continuous Multi-Embodiment Tokenizer\nTo obtain a vocabulary Vcont for continuous actions and\na tokenizer\/de-tokenizer for these actions, we follow Szot\n3\net al. [81] and train a Residual VQ-VAE (RVQ) [40] model\nover action vectors. The RVQ model is a variational au-\ntoencoder that leverages a sequence of discrete embeddings\nto represent the data. Specifically, it encodes an action a\nas a sequence of M tokens k1(a), . . . , kM(a), where each\ntoken denotes a code from a learned vocabulary V m\ncont, for\nm ∈1, . . . , M.\nA key feature of RVQ is that the mth\nvocabulary is trained to encode the residual of the action\nafter it has been encoded with vocabularies 1, . . . , m −1.\nThis hierarchical encoding makes RVQ effective in pre-\ncisely representing continuous actions with a minimal num-\nber of discrete tokens. The final continuous action vocab-\nulary used by GEA is the union of the RVQ vocabularies\nVcont = S\nm V m\ncont.\nThe main distinction from Szot et al. [81] is that we train\na single tokenizer\/de-tokenizer across all continuous action\nspaces. As shown in Table 1, these spaces cover a variety\nof robotic control types, including end-effector, joint veloc-\nity, and joint position control. To facilitate training a unified\nRVQ, we pad all action vectors to the maximum action di-\nmension. During inference, we decode the predicted action\ntokens and then truncate the output to match the dimension-\nality of the specific embodiment’s action space. We use 2\ncodebooks each with 512 tokens and a token vector dimen-\nsion of 1024. Additional details on action tokenization are\nin Appendix A.\n4.3. Stage 1: Supervised-Instruction Finetuning\nThe first step of GEA is to use supervised-instruction\nfinetuning (SFT) to adapt the base MLLM for embodied\ndecision-making (left side of Fig. 3). We use a collection\nD = S\ni∈E Di of demonstration datasets from all environ-\nments E. During this stage, we use a standard cross-entropy\nloss over actions in the case of interactive data or responses\nin the case of vision-language data. As is typically done in\nMLLM training, we maximize the negative log-likelihood\nof predicting these output tokens for each example:\nLSFT(D) = −\nX\n(I,ot−c:t,at−c:t)∈D\nlog p(at|P, I, ot−c, at−c, . . . , ot) (1)\nWe then train the base MLLM over all the above datasets\nwith SFT to obtain the GEA-Base model.\nTraining Details. The entire GEA-Base model is ini-\ntialized from the base MLLM. We train for 75k updates us-\ning AdamW [53] with cosine learning rate decay and lin-\near learning rate warmup for the first 10% of training steps;\nlearning rate of 1e−5; global batch size of 256 and an ob-\nservation context length of c = 3. Training takes around 2\ndays on 8 nodes of 8xH100 GPUs (see Appendix B).\n4.4. Stage 2: Online Reinforcement Learning\nWhile the previously described SFT training process pro-\nduces a capable GEA-Base agent, it is only trained on a\nlimited set of expert trajectories, which rarely demonstrate\nPretrained \nMLLM\nGEA-Base\nSFT on interactive data\nGEA\nSFT on interactive data\nOnline RL in simulation\nTrain entire \nMLLM\nLoRA finetune LLM\nFigure 3. GEA training stages. First, a MLLM is adapted to GEA-\nBase by finetuning the entire MLLM with SFT on interactive data.\nNext, GEA-Base is finetuned jointly with online RL (PPO) and\nSFT on the original data with LoRA.\ndiverse behaviors like error recovery. Hence, we propose\nto utilize online RL for some of the environments. In a\nsecond stage of training, we continue to train GEA-Base\nwith RL in addition to SFT to obtain the final GEA model\n(right side of Fig. 3). For online RL, we train the GEA-Base\nagent with PPO [72], whose optimization loss is denoted by\nLPPO(Mi) for each environment in which we have a simu-\nlator i ∈EPPO ⊂E. We combine this objective with the SFT\nobjective from Eq. (1) to obtain the final GEA objective:\nLGEA =\nX\ni∈EPPO\nLPPO(Mi) + λ\nX\ni∈E\nLSFT(Di)\n(2)\nwhere we weight the SFT loss by λ = 0.1 to emphasize\nthe RL loss. This RL+SFT training stage on the GEA-Base\nmodel produces the final model we refer to as GEA.\nPPO Details. The GEA value function for RL is an MLP\nnetwork which is initialized from scratch. It takes as input\nthe MLLM final layer activations at the observation token\nstep just before the action tokens and an average pooled\nrepresentation of the visual embeddings from the MLLM\nvision encoder. The critic also optionally takes any privi-\nleged state information about the task since the critic is only\nused during training, and not during inference.\nTo stabilize RL training across numerous environments,\nwe use PopArt [84] return normalization to account for\nthe diverse reward distributions across these environments.\nSince the output space of the LLM can consist of many pos-\nsible tokens, we use constrained decoding to force the au-\ntoregressive action sampling to be within the action space\nfor the environment.\nFor continuous action tasks, this\namounts to constraining the output to the learned contin-\nuous action tokens.\nFor the discrete control tasks this\namounts to constraining the output to the valid language ac-\ntions (for example, ”pick apple” or ”right”). To account for\ndifferences in the valid distribution of actions per environ-\nment, we normalize the entropy for PPO so a single entropy\ncoefficient can apply to all tasks.\nTraining Details.\nSince GEA-Base already obtains\nsome success, and RL introduces GPU memory overhead\nvia environments simulated on the GPU, we use LoRA [29]\nto finetune the LLM while freezing all other components.\nAll environments use a rollout length of 128, a learning rate\n4\nDataset Name\nDomain\nAction Type\nEmbodiment Type\n# Trajs\nData Source\nOpenX [63]\nStatic Manip\nCont. Various\n22 Various Robots\n1.2M\nVarious\nMeta-World [98]\nStatic Manip.\nCont. EE+Gripper\nSawyer\n45k\nScripted\nCALVIN [60]\nStatic Manip.\nCont. EE+Gripper\nFranka Arm\n18k\nHuman\nManiskill [22]\nStatic Manip.\nCont. Joint Velocity\nROKAE xMate3Pro\n5k\nMotion Planner\nHabitat Pick [78]\nMobile Manip.\nCont. Joint Position + Base\nFetch\n50k\nRL Expert\nHabitat Place [78]\nMobile Manip.\nCont. Joint Position+Base\nFetch\n50k\nRL Expert\nHabitat Nav [78]\nNavigation\nCont. Velocity\nFetch\n13k\nShortest Path\nBabyAI [14]\nNavigation\nDiscrete\nVirtual\n50k\nShortest Path\nLangR [80]\nPlanning\nDiscrete\nFetch\n150k\nRL Expert\nProcgen [15]\nVideo Games\nDiscrete\nVirtual\n320k\nRL Expert\nAtari [7]\nVideo Games\nDiscrete\nVirtual\n286k\nRL Expert\nAndroidControl [47]\nUI Control\nMixed\nVirtual\n14k\nHuman\nTable 1. Overview of the embodied datasets used for training GEA. The actions in each dataset can be either discrete or continuous with a\nspecific control space for the continuous actions. The embodiment type describes the agent being controlled. Each trajectory in a dataset\nrefers to a sequence of images and actions. The data source refers to the collection method for these trajectories.\nof 3e−4, an entropy coefficient of 1e−4, and a value func-\ntion learning loss of 1.5e−4. RL finetuning uses 8 nodes\nof 8xH100 GPUs with 4 parallel environments per GPU.\nEnvironments are partitioned per node into 3 of the GPUs\nrunning HabPick, 3 running Procgen, and 2 running LangR.\nThe SFT per-device batch size is 2. We train for 100M cu-\nmulative steps across all tasks, which takes around 1 day.\nFull RL training details are in Appendix C.\n5. Datasets and Environments\nWe use a diverse set of domains with associated environ-\nments and datasets (see Table 1). This section introduces\nthese domains followed by an explanation of how we use\nthem in stage 1 and 2 of our training procedure.\nStatic Manipulation.\nThese datasets are of a fixed\nrobot manipulator interacting with objects. Some of these\ndatasets are simulated table top interactions with rigid-body\nobjects such as Meta-World [99], CALVIN [60] and Man-\niskill [22]. We also leverage a large dataset of interactions\non real robot platforms [63]. These datasets span a vari-\nety of control spaces in end-effector and joint control. The\ncamera is typically mounted in a static position so that the\nworkspace and robot arm are fully visible.\nMobile Manipulation. We also investigate setups where\nthe robot manipulator moves via a mobile base. We use the\nobject rearrangement tasks from the Habitat platform [71]\nfor datasets in these tasks. These datasets cover object pick-\ning and placing tasks where the robot starts up to 2 meters\naway from the object. The robot has to coordinate moving\nits base and arm to successfully pick up the object. These\ndatasets involve first person egocentric cameras.\nNavigation. We also use datasets of navigation in iso-\nlation. We use datasets of simulated robot navigation in\nHabitat. We also use navigation in grid-world environments\nfrom BabyAI [14]. Both datasets were collected with short-\nest path experts.\nVideo games. We use datasets from two standard bench-\nmarks for decision making in video games, Procgen [15]\nand Atari [7]. Both datasets were collected by RL agents\nwhich were separately trained to solve each individual\ngame. We convert these tasks to be language-conditioned\nby providing the game name along with a short description\nof the game’s objective and rules. We only train on success-\nful trajectories.\nPlanning. We use a dataset of successful episodes in\nthe LangR dataset [80]. In this task the agent has to se-\nlect between skill primitives that accomplish long horizon\nlanguage-specified rearrangement tasks for a home robot.\nUI Control. We use the AndroidControl [47] dataset of\nUI interaction in Android devices spanning 833 apps. The\nactions are combinations of tap actions specified by screen\ncoordinates and text typing.\nVision language instruction data. To improve general-\nization of the model, we also include data used for train-\ning the original MLLM, which prior work has found is\nuseful when finetuning MLLMs as control policies [11].\nWe used the following datasets of text and images without\nany actions: VQAv2 [21], OKVQA [57], A-OKVQA [73],\nGQA [32] and the LLaVA-Instruct-150k dataset [52].\nStage 1 Training: SFT Data. To obtain embodied data\nfor Stage 1 Training (see Sec. 4.3), we collect a large dataset\nof language-conditioned behaviors from all of the above\ndomains consisting of 2.2M trajectories.\nAll trajectories\nare successful examples of language-conditioned behaviors\nwith visual observations. The data is from diverse collec-\ntion sources such as human demonstrations, RL-based poli-\ncies, and motion planners. The dataset is diverse and spans\nthousands of distinct tasks and many embodiments (see Ap-\n5\nGEA\nPrior Work\n# Tasks\nGeneralization Type\nManipulation\nMeta-World\n94.7\n84 MLLM+IL [81]S\n87.0 [69]G\n45\nobject positions\nCALVIN (ABC →D)\n90.0\n82.4 MLLM+IL [48]S\n92.2 IL+pointcloud[35]\n34*\ninstructions, background\nManiskill\n13.6\n6.5 IL [22]S\n47.8 IL+PPO [22]S\n5\nobject positions\nHabitat Pick\n82.5\n29 IL [81]S\n81.0 RL + sim stateS\n20\nhouse\nHabitat Place\n93.5\n95.5 RL + sim stateS\n10\nhouse\nVideo Games\nProcgen\n44.0\n25 [59]S\n16\nbackground\nAtari\n32.7\n31 [69]G\n85 Offline RL [41]S\n44\nnone\nNavigation\nHabitat Nav\n62.5\n72 [78]S\n10\nhouse\nBabyAI\n91.1\n93.2 [69]G\n17*\ninstructions, grid state\nUI Control\nAndroidControl\n57.3\n45 GPT-4o+SoM [93]G\n35*\ninstructions\nPlanning\nLangR\n50.0\n51 MLLM+RL[81]S\n10*\ninstructions, house\nTable 2. Zero-shot generalization of GEA to new tasks in terms of success rate % and in the video games tasks % of expert performance. We\ncompare against prior works consisting of domain specialists (with superscript “S”) that are trained on only data from that benchmark and\ndomain generalists (with superscript “G”) that are trained on data from several benchmarks. Bold indicates best, underline close second,\nand gray coloring that the method assumes access to additional input modalities like pointcloud or ground truth simulator state, meaning it\nis not a fair comparison to GEA. The “Prior Work” column also gives details about how the methods were trained (IL or RL) and if they\nassume additional input modalities. The “# Tasks” column gives a general indication of the number of distinct evaluation settings with a\n“*” indicating each task also has diverse language instructions.\npendix D for full details).\nStage 2 Training: RL Environments.\nFor Stage 2\nonline RL (see Sec. 4.4) we use environments from the\nthree domains of Habitat Pick [78], Language Rearrenge-\nment (LangR) [80], and Procgen [15].\nThus, we define\nEPPO = {HabPick, LangR, ProcGen}.\nHabitat Pick and\nLangR are simulated in the Habitat platform [71] and have\nreward functions for achieving and making progress to-\nwards the goal. In Procgen, we use all 16 games for RL and\nuse the game specific reward functions (see Appendix C for\nfull details).\n6. Empirical Evaluation\nWe empirically demonstrate the ability of GEA as a general-\nist agent that can generalize to new instructions and settings\nacross diverse embodiments and domains. We assess the\nrole of the RL training in achieving this goal. In ablations,\nwe study the impact of scaling data between multiple do-\nmains, compare RL to other forms of policy collected data,\nthe impact of the the base MLLM.\n6.1. GEA Generalization Capabilities\nIn this section, we evaluate the generalization capabilities\nof the final GEA model. We use the associated benchmarks\nfrom the datasets in Table 1, which span manipulation, nav-\nigation, video games, UI control, and planning.\nThese\nbenchmarks evaluate agents in new settings not present in\nthe training data, such as new object positions, scenes, vi-\nsual backgrounds, tasks, or instructions. All benchmarks\nspecify evaluation instructions in natural language and re-\nquire the agent to operate from visual observations.\nWe report the “online” performance of GEA, meaning\nwe evaluate its performance in an interactive simulator. The\nonly exception is AndroidControl, where we instead check\nthe correspondence with a ground truth test trajectory [68].\nEach benchmark also evaluates agents over many distinct\ntasks.\nFor example, in the Procgen video game bench-\nmark, we report the average Procgen performance over all\n16 Procgen games each of which is an entirely different\nvideo game. Full evaluation details are in Appendix E.\nWe seek to comprehensively frame the empirical per-\nformance of GEA relative to prior work on our evaluated\n6\nHabitat\nPick\nProcgen\nLangR\nAll Other\nGEA\n82.5\n44.0\n50.0\n70.5\nGEA-Base\n60.5\n36.1\n15.5\n69.5\nTable 3. Effect of stage 2 RL training on GEA-Base (7b model).\nTasks included in RL training increase their generalization per-\nformance. Performance on the remaining tasks slightly increases\nthanks to continued SFT training.\nbenchmarks.\nFirst, in all benchmarks, we use only im-\nages as observations without using any privileged informa-\ntion such as the simulation state or additional observations\nsuch as 3D point clouds. Second, we evaluate our single\nGEA model across all environments, which is referred to as\na generalist. Some of the approaches we compare against\nare trained on data from a single environment, and we re-\nfer to those as specialists. In other comparative approaches,\nthe split between train or test data is unclear. For exam-\nple, Gato [69] is a generalist model like GEA, yet evaluates\nand trains on some less diverse tasks with their own train-\ning datasets, which are not released. Appendix F.1 discusses\nthese connections in detail.\nTable 2 summarizes the comparative evaluation. GEA\nexcels at manipulation tasks, either exceeding or match-\ning the performance of specialist models. For example, in\nMeta-World GEA greatly outperforms both specialist and\ngeneralist models trained for this task with a 7% absolute in-\ncrease relative to the best-performing baseline. In CALVIN,\nGEA outperforms a variety of recent specialist models.\nGEA also performs closely to the specialist 3D Diffuser Ac-\ntor method [35], which uses a manipulation-specific action\nrepresentation of end-effector key points and uses a depth\ncamera to represent the scene as a 3D feature cloud. GEA\nonly uses the third PoV RGB camera and does not use an\naction or observation space specific to table-top manipula-\ntion. GEA outperforms the baseline in Habitat Pick and\nclosely matches it in Habitat Place despite these baselines\nbeing trained with the ground truth simulator state. In Man-\niskill, the difficult, often occluded, camera view results in\nlow overall success rates, yet GEA outperforms other re-\nsults that only use IL. However, GEA is outperformed by\nmethods that use RL in this benchmark.\nIn video game benchmarks, GEA outperforms the spe-\ncialist model baseline in Procgen. In Atari, GEA outper-\nforms the generalist Gato model [69]. However, in Atari,\nGEA is outperformed by Multi-Game DT [41], which uses\noffline RL from suboptimal demonstrations. In Atari, GEA\ndoes not learn with offline or online RL.\nIn the BabyAI navigation benchmark, GEA is close in\nperformance to Gato [69] despite GEA using RGB ren-\nderings of the top-down view rather than any underlying\nstate information and 100x fewer demonstrations for this\nHabitat\nPick\nCALVIN Procgen\nAndroid\nControl\nBabyAI\nGEA-Base\n57.0\n48.0\n24.5\n50.5\n84.7\nDomain Specific\n54.5\n35.5\n23.7\n48.9\n82.1\nOnly LLM\n9.5\n0.0\n7.6\n26.4\n49.4\nOnly VisEncoder\n34.5\n13.0\n24.5\n28.3\n70.6\nNone\n9.0\n0.0\n7.4\n14.1\n44.4\nTable 4. We present results using LLaVA-OneVision-500m as the\nMLLM base (row 1) as well as training with only domain specific\ndata (row 2). We also present results for a transformer of the same\narchitecture but only the LLM subnet is initialized (row 3), or the\nvisual encoder (row 4), or neither (row 5).\nenvironment. In Habitat Nav, GEA underperforms an RL-\ntrained expert. This gap could be due to the context of GEA\nonly consisting of the previous three observations, which\ncould limit its ability in partially observable settings. In\nUI control, another discrete action task, GEA outperforms\nGPT-4o [64] with set of marks [93] generated by a UI detec-\ntion model. This demonstrates that GEA benefits from be-\ning specialized for interactive decision-making even against\na powerful LLM and specialized perception system. Fi-\nnally, in the discrete control benchmark of the LangR plan-\nning task, GEA closely matches the performance of a spe-\ncialist baseline, which trains on only this task with RL.\nComparative Benefit of RL to SFT. Training with\nRL was important to achieving these strong results. Ta-\nble 3 compares the performance of GEA-Base, which is\nonly trained with SFT on expert demonstrations, and GEA,\nwhich is additionally trained with a second stage of RL. The\nresults show that the success rate of GEA greatly increases\non Habitat Pick, Procgen, and LangR, which are the envi-\nronments where RL was used. Furthermore, the average\nsuccess rate across all the other tasks is not influenced by\nRL due to the continued joint SFT training.\n6.2. GEA Training and Model Analysis\nIn this section, we explore the relationship between the gen-\neralist capabilities of GEA and its training data. We analyze\nthe role of embodied SFT data for adapting the MLLM for\ninteraction and the importance of online data. We also eval-\nuate the importance of the base MLLM. For all results in\nthis section, we train all models with 32% of the original\nembodied SFT data and 40k updates to ease the computa-\ntional burden of the analysis. We also evaluate on the tasks\nfrom Table 2 with a reduced number of evaluation episodes\nwith around 200 episodes per benchmark. Appendix F.2 de-\nscribes this analysis evaluation and dataset in detail.\nImpact of Multi-Domain Data. We evaluate training a\ngeneralist model on data from all diverse domains, versus\ntraining a model only on data from the particular target do-\nmain. Specifically, we train the smaller LLaVA-OneVision-\n500m model on either data from a single domain (“Domain\n7\nMLLM\nSuccess SFT\nMLLM\nOffline RL\nMLLM\nOnline RL\nGEA-Base\nGEA-Base\nSuccess SFT\nGEA-Base\nOffline RL\nGEA-Base\nOnline RL\n0\n20\n40\n60\n80\nSuccess (%)\n17\n23\n41\n57\n46\n48\n83\nFigure 4. Online learning in Habitat Pick. MLLM methods fine-\ntune LLaVA-OV while other methods finetune GEA-Base.\nSpecific”) or data across all domains (“GEA-Base ”). Com-\nparing these options in Table 4 shows that across all the\nbenchmarks, training with all the data is beneficial. How-\never, the gain is smaller in some of the domains like An-\ndroid Control and Procgen, likely due to less overlap with\nthe other training domains as opposed to the wealth of ma-\nnipulation data we train with.\nAppendix G.4 contains a\nmore granular investigation into the pairwise transferabil-\nity between each dataset and also supports this conclusion\nthat GEA benefits from multi-domain data.\nImpact of Policy-Collected Data.\nNext, we explore\nthe role of learning from data sources beyond SFT on ex-\npert demonstrations. While GEA-Base is a capable embod-\nied policy, it is only trained on successful demonstrations.\nThese demonstrations rarely exhibit recovery behaviors or\nrobustness to non-expert behaviors. Unlike typical MLLM\napplications such as visual question answering, in interac-\ntive tasks, agents trained with expert data can suffer from\nthe problem of “covariate shift” where small agent errors\ncause the observation distribution to shift from the expert’s\nand for errors to compound [70].\nWe analyze how GEA-Base can be trained with addi-\ntional data to improve performance in the Habitat Pick task\nand compare the following alternatives. First, GEA-Base\nSuccess SFT collects 10k successful examples in the en-\nvironment with the GEA-Base policy and then trains on\nthese successes with supervised learning. GEA-Base Offline\nRL collects 10k trajectories consisting of both successes\nand failures, both labeled with the dense Habitat Pick re-\nward, and then trains on these with the IQL offline-RL algo-\nrithm [38]. GEA Online RL finetunes GEA-Base with PPO,\nwhich leverages online interactions with the simulator like\nthe GEA Stage-2 training (but omits the joint SFT loss). We\nagain use the smaller base LLaVA-OneVision-500m model\nfor these experiments. Figure 4 shows the results of these\nvariations.\nA main takeaway message is the strong impact of on-\nline RL on top of a finetuned MLLM, which increases the\nsuccess of GEA-Base from 57% to 83%, despite the lat-\nter being trained on 50k successful Habitat Pick demonstra-\ntions. Online RL outperforms both Success SFT and IQL\nLangR\nProcgen\nAndroid Control\nHabitat Pick\nCALVIN\nBabyAI\n0\n20\n40\n60\n80\nSuccess (%)\nLLaVA-OV-500m\nLLaVA-OV-7b\nLLaVA-1.5-7b\nMM1.5-1.2b\nMM1.5-6.4b\nFigure 5. Analyzing the impact of training GEA with different\nbase MLLMs with different parameter counts.\noffline RL, highlighting the need for online interactions. It\nis worth noting that applying success SFT and offline RL\non top of GEA-Base decreases the model’s performance,\nwhich could be due to the lack of diverse data.\nThese results further show that online RL is beneficial\nwhen applied on top of a finetuned model with domain\ndata. Online RL alone is unable to bring the performance\nof the base MLLM to GEA-Base without the stage 1 SFT.\nAppendix G.2 explores this further and shows that GEA-\nBase is also far more sample efficient with RL than the base\nMLLM. This analysis demonstrates it is important for the\nGEA model to use RL in combination with SFT.\nImpact of pretrained MLLM. We assess the impor-\ntance of the pretrained MLLM in the GEA architecture. To\ndo so, we present results using a base model that has an ar-\nchitecture identical to the pretrained MLLM. However, in-\nstead of initializing the full model with LLaVA-OneVision\nwe initialize only the LLM or the visual encoder with the\ncorresponding subnet weights.\nThe results in Table 4 show that the full MLLM has a\nsubstantial impact on performance. Although this isn’t sur-\nprising, it is important to note that the visual encoder ini-\ntialization seems to have a stronger impact on the final per-\nformance compared to the LLM. We conjecture that this is\nbecause the benchmarks require visual generalization, and\ntraining the LLaVA-OneVision SigLIP visual encoder from\nscratch with only the embodied data is challenging.\nFurther, we analyze the impact of the base MLLM size\non the GEA-Base. We use two classes of backbone mod-\nels, LLaVA-OneVision [44] and MM1.5 [58], and use two\nsizes for each of them (0.5B and 7B for the former, and\n1.2B and 6.4B for the latter). As shown in Figure 5, in\nour agentic applications increasing model capacity leads\nto stronger performance, both within and also across\nbackbone model class. Interestingly, the class of models\nhas little effect as we see very similar performance across\nmultiple backbone models. The three different 7B models\nhave been trained on different web-scale data. Nevertheless,\ntheir difference has a negligible impact on GEA.\nAdditionally, in Appendix G.3, we show that GEA train-\ning and evaluation is robust to the random seed.\n8\n7. Conclusion\nIn this work, we studied how finetuning pretrained MLLMs\nwith large-scale embodied experience via expert trajecto-\nries and online RL unlocks their ability to act as Generalist\nEmbodied Agent. To interface with diverse embodiments,\nGEA uses a learned action tokenizer. We illustrate the im-\nportance of RL finetuning for GEA, which results in com-\npetitive results across a variety of domains spanning manip-\nulation, video games, navigation, UI control, and planning.\nWhile GEA demonstrates impressive abilities across a\nwide diversity of tasks, it is still not at the level of a foun-\ndation model for decision-making like similar models for\nlanguage and vision [1]. GEA cannot control arbitrary em-\nbodiments and operate in arbitrary environments zero-shot.\nFurthermore, the performance of GEA in some domains,\nsuch as Maniskill, Atari, and AndroidControl, is far from\nperfect. Extending RL to these environments could be a\nsolution. Future work can continue to scale GEA to more\ntasks to extend its generalist capabilities.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 从多模态大型语言模型到通用具身智能体：方法与经验\n\n## 📌 背景痛点\/本文动机\n随着人工智能技术的不断发展，多模态大型语言模型（MLLMs）在语言和视觉理解任务中展现出强大的能力。然而，这些模型通常只在特定的训练领域表现出色，难以应对多样化的任务和场景。本文旨在探索MLLMs在更广泛领域中的应用潜力，特别是具身AI、游戏、UI控制和规划等领域。\n\n## 🚀 核心方法\n💡 创新点1：通用具身智能体（GEA）\n本文提出了一种将MLLMs转化为通用具身智能体（GEA）的方法。GEA是一个统一的模型，能够通过多具身动作分词器在各个领域中进行自我定位。GEA通过在大规模具身经验数据集上进行监督学习和在线强化学习（RL）进行训练。\n\n💡 创新点2：多具身动作分词器\n为了使GEA能够控制多样化的具身形态，本文引入了一种统一的学习分词机制，用于处理所有连续和离散的动作空间。这种机制将连续动作映射到一系列新的分词，从而使得MLLMs能够理解和执行这些动作。\n\n## 📈 实验结果\nGEA在多个基准测试中展现出强大的泛化能力，与现有的通用智能体模型相比，甚至在某些特定领域中的表现也优于或接近于专业系统。例如，在CALVIN操作基准测试中，GEA在处理未见过的指令和背景时达到了90%的成功率，比类似方法高出近10%，并且与专业系统的性能相当。\n\n## 💬 可借鉴之处\n本文的研究结果表明，使用跨领域数据进行训练和在线强化学习对于构建通用智能体至关重要。此外，本文还强调了预训练MLLMs在GEA架构中的重要性，以及模型规模对性能的影响。这些发现为未来开发更强大的通用智能体提供了重要的启示和指导。","llm_summary_res_status":200}
{"title":"Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis","authors":"Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, Wentao Zhang","summary":"Recent advances in diffusion models have demonstrated exceptional\ncapabilities in image and video generation, further improving the effectiveness\nof 4D synthesis. Existing 4D generation methods can generate high-quality 4D\nobjects or scenes based on user-friendly conditions, benefiting the gaming and\nvideo industries. However, these methods struggle to synthesize significant\nobject deformation of complex 4D transitions and interactions within scenes. To\naddress this challenge, we propose Trans4D, a novel text-to-4D synthesis\nframework that enables realistic complex scene transitions. Specifically, we\nfirst use multi-modal large language models (MLLMs) to produce a physic-aware\nscene description for 4D scene initialization and effective transition timing\nplanning. Then we propose a geometry-aware 4D transition network to realize a\ncomplex scene-level 4D transition based on the plan, which involves expressive\ngeometrical object deformation. Extensive experiments demonstrate that Trans4D\nconsistently outperforms existing state-of-the-art methods in generating 4D\nscenes with accurate and high-quality transitions, validating its\neffectiveness. Code: https:\/\/github.com\/YangLing0818\/Trans4D","url":"http:\/\/arxiv.org\/abs\/2410.07155v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.07155v1","published":1728496563000,"comment":"Project: https:\/\/github.com\/YangLing0818\/Trans4D","pdf_text":"Preprint.\nTRANS4D: REALISTIC GEOMETRY-AWARE TRANSI-\nTION FOR COMPOSITIONAL TEXT-TO-4D SYNTHESIS\nBohan Zeng1∗, Ling Yang1†∗, Siyu Li1, Jiaming Liu1, Zixiang Zhang1, Juanxi Tian1,\nKaixin Zhu1, Yongzhen Guo1, Fu-Yun Wang2, Minkai Xu3, Stefano Ermon3, Wentao Zhang1†\n1Peking University\n2The Chinese University of Hong Kong\n3Stanford University\nhttps:\/\/github.com\/YangLing0818\/Trans4D\nABSTRACT\nRecent advances in diffusion models have demonstrated exceptional capabilities in\nimage and video generation, further improving the effectiveness of 4D synthesis.\nExisting 4D generation methods can generate high-quality 4D objects or scenes\nbased on user-friendly conditions, benefiting the gaming and video industries.\nHowever, these methods struggle to synthesize significant object deformation of\ncomplex 4D transitions and interactions within scenes. To address this challenge,\nwe propose TRANS4D, a novel text-to-4D synthesis framework that enables re-\nalistic complex scene transitions. Specifically, we first use multi-modal large\nlanguage models (MLLMs) to produce a physic-aware scene description for 4D\nscene initialization and effective transition timing planning. Then we propose a\ngeometry-aware 4D transition network to realize a complex scene-level 4D transi-\ntion based on the plan, which involves expressive geometrical object deformation.\nExtensive experiments demonstrate that TRANS4D consistently outperforms exist-\ning state-of-the-art methods in generating 4D scenes with accurate and high-quality\ntransitions, validating its effectiveness.\n1\nINTRODUCTION\nRecent diffusion model (DM) advances have revolutionized video and 3D synthesis. By harnessing\nthe generative capability of DM, video generation methods (Liu et al., 2024b; Bao et al., 2024) have\nachieved high-quality video production that meets commercial standards. DreamFusion (Poole et al.,\n2023) introduced Score Distillation Sampling (SDS) to guide NeRF model optimization, marking a\nsignificant breakthrough in high-fidelity 3D generation.\nBuilding on these remarkable breakthroughs, 4D generation methods have demonstrated impressive\nperformance. These methods can be broadly categorized into three types: text-to-4D (Singer et al.,\n2023; Bahmani et al., 2024b; Zheng et al., 2024; Ling et al., 2024), single-image-to-4D (Zhao et al.,\n2023; Zheng et al., 2024), and monocular-video-to-4D (Ren et al., 2023; Jiang et al., 2024; Yin et al.,\n2023; Zeng et al., 2024; Zhang et al., 2024b; Wang et al., 2024a). Text-to-4D and Image-to-4D\nmethods (Yu et al., 2024; Bahmani et al., 2024b; Zheng et al., 2024) combine video and multi-view\ngeneration models with SDS to synthesize 4D objects, though the motion remains limited due to\ncurrent constraints in video generation models. Monocular-video-to-4D methods (Jiang et al., 2024;\nWang et al., 2024a) utilize prior dynamics from video conditions to achieve high-quality 4D object\nsynthesis with large-scale and natural motion, constrained by the requirement for videos with clear\nforeground subjects that are difficult to obtain. However, these methods primarily address local\ndeformations of individual objects and fall short of generating complex 4D scenes that involve global\ninteractions between multiple objects.\nRather than merely focusing on 4D object generation, text-to-4D methods like Comp4D (Xu et al.,\n2024) and monocular-video-to-4D methods such as Dreamscene4D (Chu et al., 2024) have achieved\n4D scene generation. These methods still use deformation networks to adjust local coordinates and\nsimulate movements of objects within 4D scenes, similar to 4D object generation methods. However,\n∗Contributed equally.\n†Corresponding authors: yangling0818@163.com, wentao.zhang@pku.edu.cn\n1\narXiv:2410.07155v1  [cs.CV]  9 Oct 2024\nPreprint.\nView 1\nView 2\nView 1\nView 2\n“The missile collided with the plane and exploded”\nComp4D\nTrans4D\nTime\nFigure 1: Comparing our TRANS4D with Comp4D (Xu et al., 2024) in 4D scene transition generation.\ndeformation networks are limited in handling significant object deformation in the 4D scene, which\ncomplicates the generation of 4D transitions with complex interactions, such as a missile transforming\ninto an exploded cloud or a magician conjuring a dancer.\nTo address these challenges, we propose a text-to-4D method TRANS4D, which leverages multimodal\nlarge language models (MLLMs) for geometry-aware 4D scene planning, and introduces a Transition\nNetwork to simulate significant objects deformation within the generated 4D scenes. Unlike existing\nMLLMs that primarily describe or recognize input conditions, or methods like Comp4D (Xu et al.,\n2024) that focus on basic object trajectory function, we propose Physics-aware 4D Transition\nPlanning method that enables MLLMs to generate detailed physical 4D information, including\ninitial positions, movement and rotation speeds, and transition times. This allows for more precise\n4D scene initialization and transition management. The Transition Network further realizes the\ntransition process by predicting whether each point in the 3DGS model should appear or disappear\nat a specific time t. This capability ensures great control over transitions, enabling large-scale\nobject transformations to be handled naturally and seamlessly, such as a missile transforming into an\nexploded cloud. As demonstrated in Fig. 1, our method achieves more natural and coherent 4D scene\nsynthesis with complex interactions than existing text-to-4D scene generation techniques.\nThe main contributions of TRANS4D can be summarized as:\n• In this work, we introduce a text-to-4D generation method called TRANS4D, which enables\ncomplex 4D scene synthesis and facilitates geometry-aware 4D scene transitions. Even\nif the 4D scene contains complex interactions or significant deformation among multiple\nobjects, our method can stably generate high-quality 4D scenes.\n• We present a Physics-aware 4D Transition Planning method, which sequentially leverages\nMLLM to perform physics-aware prompt expansion and transition planning. This approach\nensures effective and reasonable initialization for 4D scene generation.\n• We propose a geometry-aware Transition Network that achieves natural and smooth\ngeometry-aware transitions in 4D scenes.\n• Comprehensive experiments demonstrate that our TRANS4D generates more realistic and\nhigh-quality complex 4D scenes than existing baseline methods.\n2\nPreprint.\n2\nBACKGROUND & PROBLEM STATEMENT\n2.1\n4D CONTENT GENERATION\nResearch on 4D content generation begins with reconstructing dynamic 3D representations based\non multi-view videos. Existing 4D reconstruction models (Pumarola et al., 2021; Wu et al., 2024a;\nHuang et al., 2024) achieve realistic 4D generation by extending 3D models such as NeRF and 3DGS.\nHowever, obtaining multi-view videos for 4D synthesis is challenging. Recently, more researchers\nhave focused on 4D generation using simpler conditions, and these methods can be broadly divided\ninto three categories: text-to-4D, image-to-4D, and monocular-video-to-4D. The text-to-4D (Singer\net al., 2023; Bahmani et al., 2024b; Ling et al., 2024; Yu et al., 2024) and image-to-4D (Zhao et al.,\n2023; Zheng et al., 2024) methods are the first to be explored by researchers, typically extending\n3D objects into 4D objects using SDS loss based on pretrained video DM. However, due to the\nlimitations of SDS loss based on video DM, the dynamics of these 4D objects often seem unrealistic.\nSubsequently, some methods (Yin et al., 2023; Jiang et al., 2024; Zeng et al., 2024; Zhang et al.,\n2024b; Wang et al., 2024a) leverage monocular video as a condition to generate high-quality and\nnaturally dynamic 4D objects. Nevertheless, generating 4D scenes remains challenging for these\nmethods, as they often require monocular videos with clear foreground subjects, which are difficult\nto obtain. The text-to-4D method (Xu et al., 2024; Bahmani et al., 2024a), and the monocular-video-\nto-4D method (Chu et al., 2024), can generate 4D scenes, but they struggle with situations involving\ngeometrical 4D scene transitions. To address this, we propose TRANS4D, which enables the stable\nand convenient generation of 4D scenes with physical 4D transitions.\n2.2\nGENERATION WITH LARGE LANGUAGE MODEL\nInspired by the advancements in LLMs and MLLMs (Touvron et al., 2023; Liu et al., 2024a; Lin\net al., 2023a; Hong et al., 2023; Qi et al., 2024), many works have leveraged these models to achieve\nhigher-quality generation. In image generation (Dong et al., 2023; Zeng et al., 2023b; Yang et al.,\n2024a; Hu et al., 2024; Han et al., 2024; Berman & Peysakhovich, 2024) and image editing (Fu et al.,\n2024; Li et al., 2024b; Jin et al., 2024; Tian et al., 2024a; Yang et al., 2024b), LLMs are first utilized\nto enhance the quality of output images. Thanks to the powerful planning abilities of LLMs, these\nimage generation and editing methods can handle more complex scenarios. Subsequently, with the\nresearch surge sparked by Sora (Liu et al., 2024b), more and more video generation methods (Zeng\net al., 2023c; Bao et al., 2024; Wu et al., 2024c; Tian et al., 2024c; Maaz et al., 2024) and storytelling\napproaches (Soldan et al., 2021; Tian et al., 2024b; Yang et al., 2024c) have harnessed the impressive\ncapabilities of LLMs to achieve coherent and realistic video synthesis, significantly contributing to\nthe multimedia industry’s development. Furthermore, with advancements in text-to-3D techniques\n(Poole et al., 2023; Lin et al., 2023b; Wang et al., 2024b; Zeng et al., 2023a; Liang et al., 2024), some\n3D (Sun et al., 2023; Feng et al., 2023; Li et al., 2024a; Chen et al., 2024b; Zhou et al., 2024) and\neven 4D (Xu et al., 2024; Wang et al., 2024a; Chu et al., 2024) generation methods now involve LLMs\nto produce high-fidelity 3D or 4D outputs with complex geometrical structures based on simple\nconditions. However, simultaneously planning temporal progression and spatial layout remains\nchallenging for existing LLM and MLLM methods, making generating highly complex 4D scenes\ndifficult. In this work, we equip MLLMs with enhanced capabilities for 4D planning, enabling more\neffective generation of complex 4D scenes.\n2.3\nTRANSITION GENERATION\nAccording to the current research landscape, video transition synthesis is less explored than the more\npopular text-to-video and image-to-video generation methods. However, this direction is crucial in\ngenerating complex scenes and long stories. Scene transitions link two consecutive periods smoothly\nthrough location, setting, or camera viewpoint changes. This seamless transition ensures the coherent\nprogression of the scene or story. Before video scene transitions, related research primarily focused\non non-deep learning algorithms with fixed patterns, as well as Morphing (Wolberg, 1998; Shechtman\net al., 2010) that identify pixel-level similarities and generative models (Van Den Oord et al., 2017;\nGal et al., 2022) that leverage latent features of linear networks to achieve smooth and reliable\ntransitions. Recent works (Chen et al., 2023; Ouyang et al., 2024; Xing et al., 2024; Feng et al.,\n2024; Zhang et al., 2024a) have advanced the field by enabling smooth and creative video transitions,\n3\nPreprint.\n(a) Full Pipeline of Trans4D\n(b) Physics-aware 4D Transition Planning\n(c) Transition Network for 4D Scene Transition\nPhysics-aware 4D \nTransition Planning\nProvided \ntextual \nprompts\n𝑦\nDetailed 4D \nscene data\n𝑦𝑖\n𝑝𝑜𝑠𝜂\n𝑠𝑝𝑑𝜈\n𝑎𝑛𝑔𝜙\n𝑟𝑜𝑡𝜛\n𝑇𝑡𝑟𝑎𝑛𝑠\n4D Scenes \nInitialization\n“The missile collided with the plane and exploded”\n× 𝑀\n3DGS models\nInitial 4D scene\n𝑡\nTransition Network\n𝑝trans\nmodify the opacity of each by 𝑝𝑡𝑟𝑎𝑛𝑠\nchoose whether each point appears with 𝑝𝑡𝑟𝑎𝑛𝑠\nMain objects: Missile (0), airplane (1), smoke after \nexplosion (2).\nExpansion description: The missile's initial \ncoordinates are (-2.0, 0.0, 0.0). The airplane's \ninitial coordinates are (2.0, 0.0, 0.0). The missile \nmoves along the x-axis at a speed of 3\/48 per \nframe. The airplane moves in the opposite \ndirection along the x-axis at a speed of -3\/48 per \nframe. …… In time 0.6, an explosion happens, \nSmoke appears, and the missile disappears. The \nmissile (number 0) transitions into smoke (number \n2) during the time interval from 0.54 to 0.64.\nPrompt Expansion \npos = [(-2.0,0.0,0.0), \n(2.0,0.0,0.0),(0.0,0.0,0.\n0),],\nspd = [[(3.0,0.0,0.0), \n(0.0,0.0,0.0)], [(-\n3.0,0.0,0.0),(-1.0,-\n1.0,0.0)],[(0.0, 0.0, \n0.0)],],\nPlanning Result \nPlanning By Trans4D \n𝑻𝒔𝒑𝒅 = [[0.6],[0.6],[],],\nang = \n[(0,0,0),(0,180,0),(0,0,0),],\nrot = [[(10, 0, 0)],[(0, 0, \n0),(0,0,20),(0,0,0)],[(0,0,0)],],\n𝑻𝒓𝒐𝒕 = [[],[0.6, 0.64],[],],\n                    ……\n𝑻𝒕𝒓𝒂𝒏𝒔 = [(0.54, 0.64)]\nPlanning By \nComp4D \nScene \nDecomposition, \nPrompt \nAugmentation, \nand Trajectory \nDesigned by \nLLM.\nTrajectory \nfunction \n𝑥= 𝑓(·)\nLinear layers\nSigmoid\n4D Scene Transition\noptimize the Deformation \nnetwork, and the \nTransition network\n4D Transition During Training\n4D Transition During Inference\n𝑡\n𝑡\nFigure 2: Overview of our TRANS4D, consisting of physics-aware 4D Transition Planning and\nTransition Network that enable 4D scene generation with complex interaction.\npaving the way for the creation of story-level, long-form videos. In addition to the video transition,\nour work first involves the geometry-aware transition into the text-to-4D synthesis.\n3\nTRANS4D\nOur TRANS4D is designed to achieve reasonable physical 4D scene transitions, as illustrated in\nFig. 2(a). This section will explain how TRANS4D performs physics-aware 4D scene planning and\naccomplishes geometry-aware 4D transitions.\n3.1\nPRELIMINARIES\n3D Gaussian Splatting.\n3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) G consists of N\nGaussian points {gi, i = 1, 2, ..., N}, and each point is defined with a center position µ, covariance Σ,\nopacity α, and color c. Each point gi is represented by a Gaussian distribution, and during rendering,\nthe formula can be expressed as:\nG(x) =\nN\nX\ni=1\nαi · ci · exp\n\u0012\n−1\n2(x −µi)⊤Σ−1\ni (x −µi)\n\u0013\n,\n(1)\nwhere x is an arbitrary position in space during the rendering process.\nText-to-4D Generation.\nBefore introducing our method, defining the input and output of the\ntext-to-4D scene generation task is essential. In this work, the input is a text prompt y, and the output\nis a 4D scene represented by M 3D Gaussian Splatting (3DGS) models {Gi, i = 1, 2, ..., M}, along\nwith a deformation network {Di, i = 1, 2, ..., M} corresponding to each 3DGS model. Typically, the\n4\nPreprint.\ndeformation network is represented by a multi-layer perceptron (MLP):\nD(x, q, t) = (∆xt, ∆qt),\nxt = x + ∆xt,\nqt = q + ∆qt,\n(2)\nwhere x and q denote the arbitrary position and orientation within the 3DGS model, and xt and qt\nrepresent the corresponding position and orientation at time t.\n3.2\nPHYSICS-AWARE 4D TRANSITION PLANNING\nTo optimize the 4D scene effectively, it is crucial to plan the placement and trajectories of objects\nwithin the generated scene based on the textual prompts y. This process includes determining\nwhich objects to generate, as well as specifying their initial positions η, movement speeds v, initial\norientation angles ϕ, rotational speeds ϖ, and scene transition times Ttrans. Unlike Comp4D (Xu\net al., 2024), which only uses LLMs to predict simple trajectory functions for 4D synthesis, our\nTRANS4D method leverages MLLM vision-language priors and introduces a physics-aware prompt\nexpansion and transition planning approach. This advancement facilitates more reliable and complex\ninitialization of 4D scenes.\nPhysics-aware Prompt Expansion and Transition Planning.\nThe target of 4D planning is to\nderive spatial and temporal information from a given textual prompt. However, spatiotemporal data\nin a 4D scene are abstract and complex, making it difficult for LLMs or MLLMs to directly interpret\nand generate accurate physics-aware 4D scene data from a simple textual prompt. To overcome\nthis challenge, we propose a physics-aware 4D prompt expansion and transition planning method.\nFirst, the method applies physical principles to analyze the original prompt, deriving spatiotemporal\ninformation and decomposing it into scene prompts {yi, i = 1, 2, ..., M}. These prompts guide the\ncreation of 3D objects within the scene. By utilizing both these prompts and the language-vision\npriors of MLLM, we extend the original textual input into a comprehensive, physics-aware scene\ndescription for the target 4D scene. This description provides specific details, including the placement\nof objects, their movements, and rotations along the x, y, and z axes over time, as well as key events\n(e.g., changes in motion speed or the appearance and disappearance of objects). By converting this\ndescription into a specific data format, the desired 4D scene data is obtained. As illustrated in Fig 2(b),\nthis method enables MLLM to generate detailed and physically plausible 4D scene data, including η,\nv, ϕ, ϖ, and Ttrans. The detailed reasoning prompts are provided in the Appendix.\nInitialization of 4D Scene.\nBased on the {yi, i = 1, 2, ..., M} obtained through the planning\nmethod, we utilize SDS with text-to-image generation model (Ye et al., 2023) to guide basic 3DGS\nmodels {Gi, i = 1, 2, ..., M} synthesis. Using the planning 4D scene data, We calculate the transfor-\nmation function for any position within these 3DGS models at each time t as:\nx = R(ϕ + ϖ · t)xξ + η + v · t\n(3)\nwhere R denotes the rotation matrix, x represents the arbitrary position in the 3DGS model, and xξ is\nthe coordinate of x when the 3DGS model is at (0, 0, 0). By integrating {Gi, i = 1, 2, ..., M} with\nthe transformation function, we obtain an initial 4D scene.\nAfter obtaining the physics-aware planning, we use geometry-aware 4D transitions to effectively\nvisualize the physical dynamics derived from this planning. In the next section, we detail how our\nproposed transition network realizes these geometry-aware 4D transitions.\n3.3\nGEOMETRY-AWARE 4D TRANSITION\nBy utilizing the initial 4D scene and the deformation network, we can achieve 4D scene synthesis\nin certain scenarios through global object positioning and local dynamics. However, depending\nexclusively on movement is limited, as it cannot support geometry-aware 4D transitions that involve\nsignificant object deformation, such as the appearance or disappearance of objects in a 4D scene.\nTo overcome this limitation, we propose a geometry-aware Transition Network (TransNet), which is\na multi-layer perceptron (MLP) with a Sigmoid activation function at the output layer. As shown\nin Fig. 2(c), TransNet takes the position of the point cloud and the time t as inputs, and processes\nthem through several linear layers to produce an intermediate output. This intermediate output is then\n5\nPreprint.\nscaled by a coefficient wtrans before inputting into the final Sigmoid function. The final output of\nTransNet denotes as ptrans, which lies between 0 and 1 and serves as a reference for 4D transition.\nptrans = σ(wtrans · h(xt, qt, t)),\n(4)\nwhere h(xt, qt, t) represents the intermediate output from the linear layers of TransNet, σ is the\nSigmoid activation function, and wtrans is a scaling coefficient, typically set to 10 or higher, to\namplify the changes of the point cloud over time t.\nDuring the training stage, to ensure that TransNet is differentiable, we modify the opacity of each\npoint cloud by multiplying the opacity α directly with ptrans. During the inference stage, to ensure a\nnoticeable transition, ptrans is used to determine whether each Gaussian point of the 3DGS model\nappears in the 4D scene. This method enables a smooth and natural 4D scene transition. The\ncalculation process is as follows:\nB =\n\u001a1,\nwith probability ptrans,\n0,\nwith probability 1 −ptrans,\n(5)\nWhen B = 1, the point cloud appears in the 4D scene; otherwise, it does not. Compared to manually\nconstraining the number of points in the 3DGS model at different time intervals, TransNet allows\nfor flexible and rational control of point variations during the transition process, effectively achieving\ndesired geometry-aware 4D scene transitions.\n3.4\nEFFICIENT 4D TRAINING AND REFINEMENT\nConventional text-to-4D optimization strategies typically rely on SDS loss based on video DM to\nproduce 4D results with reliable dynamics, which incurs high computational costs. To efficiently\nachieve high-fidelity 4D scene synthesis with realistic dynamics, we optimize TRANS4D in two\nphases: first, we train the deformation network and TransNet using 3DGS models with a relatively\nsmall number of point clouds, minimizing costs even with SDS based on video DM. Then, we refine\n3DGS models, allowing for increased point cloud counts with lower computational overhead.\nDuring the training of the deformation network and TransNet, the number of points in each 3DGS\nmodel is fixed at 20,000. We represent the rendered images of the 4D scene over 16 consecutive\ntimes t as {I1, I2, ..., I16}. For SDS loss, noise is added to the rendered images, represented as\nI1\nt , I2\nt , ..., I16\nt\nat timestep t′. We optimize deformation network and TransNet using SDS based on\nvideo DM ϵvid, which can be expressed as:\n∇θdynLSDS−vid({I1, I2, ..., I16}, y) =\nEt′,ϵ\n\u0014\nw(t′)\n\u0010\nϵvid({I1\nt′, I2\nt′, ..., I16\nt′ }, y, t′) −ϵ\n\u0011∂{I1, I2, . . . , I16}\n∂θdyn\n\u0015\n,\n(6)\nwhere θdyn represents the parameters of deformation network and TransNet. During this training\nstage, the points in the 3DGS model are neither cloned nor split, ensuring efficient training of both\nnetworks. To further enhance the quality of the 4D scenes, we use an SDS loss based on text-to-image\nDM ϵimg to supervise further optimization of the 3DGS model. At this stage, the points in the 3DGS\nmodel are cloned and split for the refinement:\n∇θGLSDS(I, y) = Et′,ϵ\n\u0014\nw(t′)\n\u0010\nϵimg(I, y, t′) −ϵ\n\u0011 ∂I\n∂θG\n\u0015\n,\n(7)\nwhere I represents the rendered result of the 4D scene at a random time t, and θG represents the\nparameters of the 3DGS model. Meanwhile, the inputs to the deformation network and TransNet\nconsist solely of the positions of the 3DGS model’s points. Therefore, even after the refinement stage,\nwhile the 3DGS models in the 4D scene become more detailed and realistic, the dynamics of the 4D\nscene remain unaffected.\n6\nPreprint.\nConsistent4D\n4D-fy\nConsistent4D\n4D-fy\nDream-in-4D\nComp4D\nDream-in-4D\nComp4D\nThe magician conjured a dancer\nOurs\nOurs\nA pigeon appeared from a top hat\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\nView 1\nView 2\nView 1\nView 2\n𝒕\n𝒕\nFigure 3: Qualitative comparison with previous baseline methods (Bahmani et al., 2024b; Zheng et al.,\n2024; Jiang et al., 2024; Xu et al., 2024). Our method achieves smoother geometric 4D transitions\nand produces more realistic object interactions within 4D scenes.\n7\nPreprint.\nTable 1: Quantitive comparison of text-to-4D generation.\nMetrics\nConsistent4D\n4D-fy\nDream-in-4D\nComp4D\nTRANS4D (Ours)\nQAlign-vid-quality ↑\n2.275\n3.017\n3.035\n2.961\n3.226\nQAlign-vid-aesthetic ↑\n1.924\n2.089\n2.111\n1.774\n2.148\nVid-MLLM-metrics ↑\n0.5931\n0.4347\n0.5063\n0.5532\n0.6483\nCLIP-score ↑\n0.2836\n0.2661\n0.2607\n0.2757\n0.2941\nUser study ↑\n0.72\n0.64\n0.67\n0.59\n0.78\n4\nEXPERIMENTS\nImplementation Details.\nIn this work, all experiments are conducted on four A100-SXM4-80GB\nGPUs. In Stage 1, we optimize for 5000 steps using the Adam optimizer (Kingma, 2014) to obtain\nthe 3DGS models. In Stage 2, we perform 4500 optimization steps to train the deformation network\nand the Transition Network. During the refinement phase, we further optimize the 3DGS models for\nobjects that cannot be represented in high quality with only 20000 points (e.g., complex structures\nlike ”volcano”). This refinement is performed over 4000 steps using the SDS loss. We ensure a fair\ncomparison by using the same models across all methods for both generation and supervision. For\nany use of a text-to-image generation model, we use Stable Diffusion 2.1 (Rombach et al., 2022); for\nany use of a multiview generation model, we use MVDream (Shi et al., 2024); and for any use of a\ntext-to-video generation model, we use VideoCraft (Chen et al., 2024a).\nBaseline Methods.\nTo validate the effectiveness of our method in generating complex 4D scenes\nwith geometry-aware 4D transitions, we compare it with several different 4D generation methods.\nThese methods include text-to-4D-object methods 4D-fy (Bahmani et al., 2024b) and Dream-in-4D\n(Zheng et al., 2024), a monocular-video-to-4D-object method Consistent4D (Jiang et al., 2024), and\na text-to-4D-scene method Comp4D (Xu et al., 2024).\nMetrics.\nDue to the lack of visual ground truth in text-to-4D generation tasks, we employ QAlign-\nvid-quality and QAlign-vid-aesthetic metrics (Wu et al., 2024b) to evaluate the quality and aesthetics\nof the generated 4D scenes. To assess the semantic alignment of the generated results, we utilize the\nCLIP-score (Park et al., 2021) and MLLM-score. Additionally, we conduct a user study to enhance\nthe credibility of our comparison results. More details on QAlign-vid-quality, QAlign-vid-aesthetic,\nCLIP-score, MLLM-score, and the user study are provided in the Appendix.\n4.1\nTEXT-TO-4D SYNTHESIS\nQuantitative Results.\nTo assess the effectiveness of TRANS4D in complex 4D scene synthesis,\nwe utilize 30 complex textual prompts for 4D scene synthesis. Most of these prompts involve\ngeometry-aware transitions, with the specific prompts detailed in the supplementary material. As\nshown in Table 1, TRANS4D surpasses other methods across all metrics. The text-to-4D methods,\n4D-fy and Dream-in-4D, achieve high scores on the metrics utilized Q-align, demonstrating their\nability to generate high-quality 4D scenes. However, they perform poorly on the CLIP and MLLM\nscores, highlighting that it remains challenging for them to generate 4D scenes that accurately align\nwith the input text. Additionally, our TRANS4D achieved the highest score in the user study, further\nvalidating its effectiveness.\nQualitative Results.\nTo intuitively demonstrate the superiority of our method in generating complex\n4D scenes, that have significant object deformations, we conduct a qualitative comparison with other\nbaseline models. As shown in Fig. 3, the rendered videos of the 4D outputs generated by our\nmethod exhibit the most reasonable and high quality. Additionally, while 4D-fy and Dream-in-4D\nalso produce high-quality visual outputs, these text-to-4D-object generation methods struggle to\ncreate 4D scenes with coherent dynamics based on textual requirements. Lastly, the results from\nConsistent4D indicate that monocular-video-to-4D generation methods perform better for simple 4D\nobject generation. However, when the monocular video involves complex dynamics and interactions\n(as in the visualization example, ”The magician conjured a dancer”), these methods struggle to\nproduce satisfactory 4D outputs. Moreover, acquiring a monocular video with both clear subjects\n8\nPreprint.\nFigure 4: Additional user study for model analysis.\nView 1\nView 2\nw\/o Physics-aware \nprompt expansion\nLLM w\/ Physics-aware \nprompt expansion\nfull Physics-aware 4D \nplanning method\nFigure 5: Ablation study of Physics-aware 4D Transition Planning method.\nand reasonable dynamics is inherently challenging. Therefore, our TRANS4D is currently the most\nconvenient and reliable method for generating complex 4D scenes.\n4.2\nMODEL ANALYSIS\nTo highlight the key contributions of TRANS4D, including Physics-aware 4D Transition Planning\nand the Transition Network, we conduct additional user studies to demonstrate the effectiveness of\nour proposed models. Furthermore, we incorporate visual comparisons to showcase the necessity and\nbenefits of refinement.\nRationality of MLLM-planned Trajectory.\nWe have demonstrated that our method for initial-\nizing 4D scenes outperforms the simple function-based method Comp4D. To further showcase\nthe advantages of our Physics-aware 4D Transition Planning method, we conduct an experiment\nwhere volunteers evaluate videos generated from three different initialization methods: (1) Without\nPhysics-aware prompt expansion: the MLLM receives only one example (including input text and 4D\ndata) to generate 4D scenes based on other input texts; (2) Utilizing an LLM to predict the 4D data\nwith Physics-aware prompt expansion; and (3) Our complete Physics-aware 4D Transition Planning\nmethod. As shown in Fig. 4(a), without Physics-aware prompt expansion, the MLLM struggles to\ngenerate plausible 4D data for scene initialization, resulting in poor outcomes. This underscores the\nimportance of physics-aware prompt expansion. Moreover, when we utilize the LLM to produce\nthe 4D data with Physics-aware prompt expansion, the predicted 4D data lack precision due to the\nabsence of vision-language priors. As illustrated in Fig. 5, incorporating the full Physics-aware 4D\n9\nPreprint.\nw\/o Transition Network\nadjust the opacity by\nutilize             to \ndetermine whether a \npoint appears \nAn angelic girl is becoming a puppet of the devil.\nFigure 6: Ablation study of Transition Network.\nw\/o refinement\nw\/ refinement\nView 1\nView 2\nFigure 7: Ablation study of refinement.\nTransition Planning method significantly enhances the results, highlighting its ability to enrich our\napproach with prior knowledge for more reasonable scene initialization.\nGeometrical Expressiveness.\nTo better observe the effects of the transition network, we decelerate\nthe geometric-aware 4D transition process, allowing volunteers to discern the transition effects. We\nprovide the volunteers with three different videos representing various transition methods: (1) without\nusing the transition network; (2) using the transition network, where ptrans is multiplied by the\nopacity; and (3) using the transition network, where ptrans determines which points should appear.\nThe volunteers are asked to evaluate which process appears more natural. As shown in Fig. 4(b), it\nis evident that the majority of volunteers find the transitions incorporating the transition network to\nbe more natural, with the point selection method receiving the highest scores due to the clearer and\nmore distinct transition. We demonstrate the generated results in Fig. 6, which highlights the pivotal\nsignificance of the proposed transition network in this study.\nEfficiency and Quality of Refinement.\nWhen a 4D scene contains over 200,000 point clouds,\ndirectly supervising it with video SDS loss consumes 80GB or more GPU memory limit, while\nleading to suboptimal quality. In contrast, by separating the training process, we reduce memory\nusage to around 50GB, almost halving the requirement, while significantly improving the quality\nof the generated 4D scene. Specifically, we initially represent the 4D scene using minimal point\nclouds while training the deformation and transition networks. Then, we apply a refinement process\nto improve the quality of each 3DGS model by increasing the number of point clouds as needed.\nThis stepwise training manages memory efficiently while producing high-quality 4D scenes. As\ndemonstrated in Fig. 7, for massive 3D objects like “volcano erupting”, sparse point clouds cannot\nrepresent them effectively. Hence, refining such 3D objects is essential. In conclusion, our training\nstrategy balances efficiency and quality, enabling the generation of high-quality 4D scenes with\nrelatively limited computational resources.\n10\nPreprint.\n5\nCONCLUSION AND FUTURE WORK\nIn this work, we propose TRANS4D, a novel text-to-4D scene generation method that produces high-\nquality 4D scenes involving complex object interactions and significant deformations. Specifically,\nwe introduce a Physics-aware 4D Transition Planning method, which enables MLLM to initialize\nrealistic 4D scenes with multiple interacting objects. To facilitate geometry-aware transitions in\nthe generated 4D scene, we design a Transition Network that dynamically determines whether each\npoint cloud in the 4D scene should appear or disappear, allowing our method to handle substantial\nobject deformations naturally. Our experiments demonstrate that TRANS4D consistently generates\nhigh-quality 4D scenes with complex interactions and smooth, geometry-aware transitions.\nFor future work, We will continue to improve the quality of multi-object interactions in 4D scenes,\nwhich will help achieve more realistic 4D scene generation, and support the development of the video\nmultimedia and gaming industries.\nREFERENCES\nSherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu,\nJeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-\n4d generation. In ECCV, 2024a.\nSherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter\nWonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy:\nText-to-4d generation using hybrid score distillation sampling. In CVPR, 2024b.\nFan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao,\nShilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-\nvideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.\nWilliam Berman and Alexander Peysakhovich. Mumu: Bootstrapping multimodal image generation\nfrom text-to-image data. arXiv preprint arXiv:2406.18790, 2024.\nHaoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR,\n2024a.\nSijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan,\nand Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning\nand planning. In CVPR, 2024b.\nXinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang,\nDahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative\ntransition and prediction. In ICLR, 2023.\nWen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene\ngeneration from monocular videos. arXiv preprint arXiv:2405.02280, 2024.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and\ncreation. In ICLR, 2023.\nYao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J Black. Posegpt:\nChatting about 3d human pose. arXiv preprint arXiv:2311.18836, 2023.\nYutang Feng, Sicheng Gao, Yuxiang Bao, Xiaodi Wang, Shumin Han, Juan Zhang, Baochang Zhang,\nand Angela Yao. Wave: Warping ddim inversion features for zero-shot text-to-video editing. In\nECCV, 2024.\nTsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding\ninstruction-based image editing via multimodal large language models. In ICLR, 2024.\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nStylegan-nada: Clip-guided domain adaptation of image generators. TOG, 2022.\n11\nPreprint.\nYucheng Han, Rui Wang, Chi Zhang, Juntao Hu, Pei Cheng, Bin Fu, and Hanwang Zhang. Emma:\nYour text-to-image diffusion model can secretly accept multi-modal prompts. arXiv preprint\narXiv:2406.09162, 2024.\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang\nGan. 3d-llm: Injecting the 3d world into large language models. In NeurIPS, 2023.\nXiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models\nwith llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.\nYi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs:\nSparse-controlled gaussian splatting for editable dynamic scenes. In CVPR, 2024.\nYanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. Consistent4d: Consistent 360° dynamic\nobject generation from monocular video. In ICLR, 2024.\nYing Jin, Pengyang Ling, Xiaoyi Dong, Pan Zhang, Jiaqi Wang, and Dahua Lin. Reasonpix2pix:\nInstruction reasoning dataset for advanced image editing. arXiv preprint arXiv:2405.11190, 2024.\nBernhard Kerbl, Johannes Hanika, Thomas M¨uller, Harshavardhan Kondapaneni, Francesco Di Gia-\ncomo, Thomas Leimk¨uhler, Chris Chaitanya, Matthias Nießner, and Roland Heged¨us. 3d gaussian\nsplatting for real-time radiance field rendering. In SIGGRAPH, 2023.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nHong Li, Yutang Feng, Song Xue, Xuhui Liu, Bohan Zeng, Shanglin Li, Boyu Liu, Jianzhuang Liu,\nShumin Han, and Baochang Zhang. Uv-idm: Identity-conditioned latent diffusion model for face\nuv-texture generation. In CVPR, 2024a.\nShanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao\nHu, Jianzhuang Liu, et al. Zone: Zero-shot instruction-guided local editing. In CVPR, 2024b.\nYixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer:\nTowards high-fidelity text-to-3d generation via interval score matching. In CVPR, 2024.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, 2023b.\nHuan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your\ngaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In CVPR, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS,\n2024a.\nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang,\nHanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and\nopportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024b.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image\nand video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024.\nYichen Ouyang, Hao Zhao, Gaoang Wang, et al. Flexifilm: Long video generation with flexible\nconditions. arXiv preprint arXiv:2404.18620, 2024.\nDong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for\ncompositional text-to-image synthesis. In NeurIPS, 2021.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. In ICLR, 2023.\n12\nPreprint.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural\nradiance fields for dynamic scenes. In CVPR, 2021.\nZekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and\nKaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. arXiv\npreprint arXiv:2402.17766, 2024.\nJiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaus-\nsian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nEli Shechtman, Alex Rav-Acha, Michal Irani, and Steve Seitz. Regenerative morphing. In CVPR,\n2010.\nYichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. In The Twelfth International Conference on Learning Representations,\n2024.\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation.\narXiv preprint arXiv:2301.11280, 2023.\nMattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, and Bernard Ghanem. Vlg-net: Video-\nlanguage graph matching network for video grounding. In ICCV, 2021.\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt:\nProcedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945, 2023.\nBozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou,\nXi Chen, and Huajun Chen. Instructedit: Instruction-based knowledge editing for large language\nmodels. arXiv preprint arXiv:2402.16123, 2024a.\nChangyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen,\nLewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling\nvia multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024b.\nYe Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen\nYu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation.\narXiv preprint arXiv:2406.04277, 2024c.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.\nYikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single\ngenerated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. arXiv preprint\narXiv:2405.16822, 2024a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In\nNeurIPS, 2024b.\nGeorge Wolberg. Image morphing: a survey. The visual computer, 1998.\nGuanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian,\nand Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR,\n2024a.\nHaoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao,\nAnnan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via\ndiscrete text-defined levels. In ICML, 2024b.\n13\nPreprint.\nJialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long.\nivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv:2405.15223,\n2024c.\nJinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong.\nTooncrafter: Generative cartoon interpolation. arXiv preprint arXiv:2405.17933, 2024.\nDejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N Plataniotis,\nand Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint\narXiv:2403.16993, 2024.\nLing Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering\ntext-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML,\n2024a.\nLing Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan.\nEditworld: Simulating world dynamics for instruction-following image editing. arXiv preprint\narXiv:2405.14785, 2024b.\nShuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen.\nSeed-story: Multimodal long story generation with large language model.\narXiv preprint\narXiv:2407.08683, 2024c.\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt\nadapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and\nFei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality\ncollaboration. In CVPR, 2024.\nYuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content\ngeneration with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023.\nHeng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A\nJeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via\nvideo diffusion models. arXiv preprint arXiv:2406.07472, 2024.\nBohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang,\nJianzhuang Liu, and Baochang Zhang. Ipdreamer: Appearance-controllable 3d object generation\nwith image prompts. arXiv preprint arXiv:2310.05375, 2023a.\nBohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu,\nand Baochang Zhang. Controllable mind visual diffusion model. arXiv preprint arXiv:2305.10135,\n2023b.\nBohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, and Baochang Zhang.\nFace animation with an attribute-guided diffusion model. arXiv preprint arXiv:2304.03199, 2023c.\nYifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun\nCao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. arXiv preprint\narXiv:2403.14939, 2024.\nBowen Zhang, Xiaofei Xie, Haotian Lu, Na Ma, Tianlin Li, and Qing Guo. Mavin: Multi-action video\ngeneration with diffusion models via transition video infilling. arXiv preprint arXiv:2405.18003,\n2024a.\nHaiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion:\nMulti-view video diffusion model for 4d generation. arXiv preprint arXiv:2405.20674, 2024b.\nYuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124:\nAnimating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023.\nYufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. A unified\napproach for text-and image-guided 4d scene generation. In CVPR, 2024.\n14\nPreprint.\nXiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun,\nand Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided\ngenerative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024.\n15\nPreprint.\nA\nAPPENDIX\nIn Appendix A.1, we provide detailed information of our evaluation metrics. Appendix A.2 outlines\nthe specific prompts and process of the Physics-aware 4D Transition Planning method. Finally, in\nAppendix A.3, we present the textual prompts used for evaluation along with additional 4D scenes\ngenerated by TRANS4D.\nTable 2: 4D scene decomposition.\nYou are a 4D Scene Decomposing Agent\nYour task is to decompose the 4D scene into several\nappropriate parts based on the prompt provided by the\nuser. Unlike 3D scene generation methods that only need to\nsplit according to the content of the prompt, you need to\nanalyze the possible physical dynamic that may occur in\nthe provided prompt from both temporal and spatial\ndimensions. Concurrently, based on the analysis results,\ndecompose the provided prompt into several prompts in the\ntime-space dimension.\n--- Main Object ---\nThese objects’ prompts will be used for generating 3D objects\nfirst, and then add time dimension to generate a complete\n4D scene.\nTherefore, if the same object undergoes significant physical\nchanges over time, it should be considered as two separate\nmain objects.\nThe scene’s background is blank, and only moving objects,\nsuddenly appearing objects like clouds and smoke, and\nobjects undergoing shape changes, such as melting or\nbreaking, need to be considered.\n--- Examples ---\n......\nA.1\nDETAILS OF METRICS\nIn this section, we provide a more detailed explanation of the metrics and user studies discussed in\nthe main paper.\nQAlign-vid-quality and QAlign-vid-aesthetic.\nQ-Align (Wu et al., 2024b) is a large multi-modal\nmodel fine-tuned from mPLUG-Owl2 (Ye et al., 2024) using extensive image and video quality-\nassessment datasets. It has demonstrated strong alignment with human judgment on existing quality\nassessment benchmarks. In line with Comp4D (Xu et al., 2024), we use Q-Align to evaluate the\nquality of the generated 4D scenes. Specifically, we input rendering videos of 4D scenes produced by\nvarious methods from viewpoints of -120◦, -60◦, 0◦, 60◦, 120◦, and 180◦into Q-Align. The output\nscores from Q-Align range from 1 (worst) to 5 (best). We calculate the average score of these outputs\nto compare the performance of different 4D generation methods quantitatively.\nCLIP score.\nThe CLIP score (Park et al., 2021) is a widely used metric for evaluating the correlation\nbetween input textual prompts and generated images. Following the approach in 4D-fy (Bahmani\net al., 2024b), we calculate the CLIP score between the frames of the rendered videos and the input\ntextual prompts. Due to the complexity of 4D scene generation, which involves significant object\ndynamics, we use the maximum CLIP score obtained across all frames of each rendered video as the\n16\nPreprint.\nTable 3: Complete Scene Expansion Description\nYou are an Efficient Scene Expansion Agent.\nYour task is to use these decompositional main objects and the\nprompt to expand the provided prompt into a complete\nphysics-aware 4D scene description.\n--- Scene ---\nThe scene is a 4D video clip composed of the main objects\nextracted earlier. The scene information should include:\n- The initial position of each object, represented in the form\n[x, y, z].\n- The movement path of the objects defines the movement vector\nper frame. Each object can have multiple movement segments.\n- The time points when movements start or stop.\n- The initial rotation angle of the objects is expressed in\ndegrees as [rx, ry, rz] (rotation along the x, y, and z\naxes respectively).\n- The rotation path of the objects, defining the rotation\nchange per frame.\n- The time intervals when rotations occur.\n- The time states of the objects, such as when they appear,\ndisappear, or transform at specific times.\n- The transformation relationships between objects, specifying\nwhich objects transform into each other during certain\ntime intervals and when these transformations occur.\nThe time points are represented within a single 4D segment,\nwith 0 indicating the start and 1 indicating the end.\nOther states use decimals to specify the exact time point\nwithin the segment.\nThe scene’s center is [0, 0, 0], and the range for each\ncoordinate axis within the scene is [-1, 1]. Positions\noutside this range are considered outside the scene.\nObjects can enter the scene from outside, but each main\nobject must appear within the scene at some point.\n--- Examples ---\n......\nrepresentative score. To evaluate their performance, we compare the average CLIP scores of rendered\nvideos generated by different methods.\nMLLM score.\nAlthough the CLIP score is a commonly used metric to evaluate semantic alignment,\nit can not fully analyze the reasonability of rendered videos. To more effectively evaluate the semantic\nalignment of the generated 4D results, we propose the MLLM score which leverages the vision-\nlanguage knowledge of GPT4o to evaluate the correlation between the rendered videos and the input\ntextual prompts. Specifically, we present the rendered videos and the provided textual prompts for\nthe ChatGPT-4o. The specific prompt provided for ChatGPT-4o scoring the semantic alignment as:\n“We provide several <video> clips along with a <text prompt>. The videos represent rendered 4D\nscenes from specific viewpoints. Please evaluate the 4D scenes generated by different methods based\non the alignment between the video and the text prompt, as well as the overall video quality, and\nassign a score between 0 and 1.”\n17\nPreprint.\nTable 4: The specific prompt for obtaining 4D planning data.\nYou are a 4D data production Agent.\nYour task is to transfer the complete 4D scene description\ninto precise 4D planning data.\nThe output should be in the json format:\n{\n\"sample\": {\n\"obj_prompt\": [\n\"List of objects involved in the scenario\"],\n\"TrajParams\": {\n\"init_pos\": [\n[x, y, z] \/\/ Initial positions of objects in 3D\nspace],\n\"move_list\": [\n[\n[dx, dy, dz], \/\/ Movement vector\n[dx, dy, dz] \/\/ Additional movement after an\nevent\n] ],\n\"move_time\": [\n[time] \/\/ List of times when movements occur or\nstop],\n\"init_angle\": [\n[rx, ry, rz] \/\/ Initial rotation angles (degrees)\nof objects along x, y, z axes],\n\"rotations\": [\n[\n[rx, ry, rz], \/\/ Rotation vector per frame\n[rx, ry, rz] \/\/ Optional: Additional rotation\nafter an event\n] ],\n\"rotations_time\": [\n[start_time, end_time] \/\/ Times when rotations\noccur],\n......\n\"trans_list\": [\n[obj_index, transition_obj_index] \/\/ Objects that\ntransition into each other],\n\"trans_period\": [\n[start_time, end_time] \/\/ The time period when the\ntransition occurs.]\n}\n}\n}\nUser study.\nFor unsupervised text-to-4D-scene generation, the user study is the most convincing\nmetric. To further validate the effectiveness of our method, we conduct a comprehensive user study\ninvolving 80 volunteers. Each volunteer is randomly provided 10 test examples from the testing\ndataset introduced in this work. For each example, volunteers are asked to judge whether the generated\nresults from various 4D methods successfully achieve the desired 4D synthesis based on the given\ntext inputs. Volunteers rate each result on a scale from 0 to 1, where a score closer to 1 indicates\nbetter alignment with the expected outcome.\n18\nPreprint.\nTable 5: Textual prompts used in the user study.\nThe missile collided with the plane and exploded.\nA cavalry charged two shield-bearing infantry.\nThe magician conjured a dancer.\nThe ice block melts into water.\nThe volcano erupted violently.\nThe tree fell after being cut by the harvester.\nThe water balloon burst on impact.\nThe clock struck midnight.\nThe egg cracked open.\nThe spaceship took off from Earth and entered space.\nThe tornado formed over the plains.\nThe butterfly emerged from the cocoon.\nThe snowflake melted on the tongue.\nThe fish jumped out of the water.\nThe corn kernels pop into popcorn.\nThe moon appeared from behind the clouds.\nA pigeon appeared from a top hat.\nAn angelic girl is becoming a puppet of the devil.\nAn explosion occurs while a wizard is brewing a magic potion.\nA sage caused a gigantic flower to bloom.\nThree worshippers pray for the appearance of an angel.\nA zombie crawls out of the tombstone.\nA dragon breathes fire onto a knight’s shield.\nA giant cracks the ground with its heavy footsteps.\nA knight draws a glowing sword from a stone.\nA sorcerer opens a portal to another dimension.\nA ghost passes through a wall, leaving behind a cold mist.\nA castle tower collapses after being struck by lightning.\nA violin plays itself, filling the air with haunting melodies.\nThe appearance of the sun clears the fog.\nA.2\nMORE DETAILS OF OUR MODEL\nPhysics-aware 4D planning.\nMultimodal Large Language Models (MLLMs), leveraging their\nvision-language priors, have the potential to generate reasonable and natural spatiotemporal data.\nIn this work, we leverage the spatiotemporal awareness of MLLMs to achieve impressive 4D scene\ninitialization and 4D transition planning. During the process of obtaining 4D data, we require the\nMLLM to ensure that the generated plans are consistent with physical principles and geometrically\ncoherent, thereby guaranteeing both physical plausibility and the correctness of spatial relationships.\nSpecifically, Tables 2, 3, and 4, present the detailed prompts for scene decomposition, physics-aware\n4D prompt expansion, and 4D planning data, respectively.\nA.3\nADDITIONAL RESULTS\nTextual prompts used for comparison.\nIn Table. 5, We provide the specific textual prompts used\nin quantitative comparison.\ncomparison results.\nIn Fig. 8, we provide more generated 4D scenes of TRANS4D, to further\ndemonstrate the effectiveness of our method.\n19\nPreprint.\nIce cube melts into water\nThree worshippers pray for the appearance of an angel.\nThe tree cut off by a harvester.\nGreen flames ignited during the wizard's process of brewing the potion.\nFigure 8: Additional generated 4D results, our TRANS4D can consistently produce high-quality 4D\nscenes.\n20\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nTrans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis\n```\n#### 2. 论文摘要\n```\nRecent advances in diffusion models have demonstrated exceptional\ncapabilities in image and video generation, further improving the effectiveness\nof 4D synthesis. Existing 4D generation methods can generate high-quality 4D\nobjects or scenes based on user-friendly conditions, benefiting the gaming and\nvideo industries. However, these methods struggle to synthesize significant\nobject deformation of complex 4D transitions and interactions within scenes. To\naddress this challenge, we propose Trans4D, a novel text-to-4D synthesis\nframework that enables realistic complex scene transitions. Specifically, we\nfirst use multi-modal large language models (MLLMs) to produce a physic-aware\nscene description for 4D scene initialization and effective transition timing\nplanning. Then we propose a geometry-aware 4D transition network to realize a\ncomplex scene-level 4D transition based on the plan, which involves expressive\ngeometrical object deformation. Extensive experiments demonstrate that Trans4D\nconsistently outperforms existing state-of-the-art methods in generating 4D\nscenes with accurate and high-quality transitions, validating its\neffectiveness. Code: https:\/\/github.com\/YangLing0818\/Trans4D\n```\n\n#### 3. 论文全文\n```\nPreprint.\nTRANS4D: REALISTIC GEOMETRY-AWARE TRANSI-\nTION FOR COMPOSITIONAL TEXT-TO-4D SYNTHESIS\nBohan Zeng1∗, Ling Yang1†∗, Siyu Li1, Jiaming Liu1, Zixiang Zhang1, Juanxi Tian1,\nKaixin Zhu1, Yongzhen Guo1, Fu-Yun Wang2, Minkai Xu3, Stefano Ermon3, Wentao Zhang1†\n1Peking University\n2The Chinese University of Hong Kong\n3Stanford University\nhttps:\/\/github.com\/YangLing0818\/Trans4D\nABSTRACT\nRecent advances in diffusion models have demonstrated exceptional capabilities in\nimage and video generation, further improving the effectiveness of 4D synthesis.\nExisting 4D generation methods can generate high-quality 4D objects or scenes\nbased on user-friendly conditions, benefiting the gaming and video industries.\nHowever, these methods struggle to synthesize significant object deformation of\ncomplex 4D transitions and interactions within scenes. To address this challenge,\nwe propose TRANS4D, a novel text-to-4D synthesis framework that enables re-\nalistic complex scene transitions. Specifically, we first use multi-modal large\nlanguage models (MLLMs) to produce a physic-aware scene description for 4D\nscene initialization and effective transition timing planning. Then we propose a\ngeometry-aware 4D transition network to realize a complex scene-level 4D transi-\ntion based on the plan, which involves expressive geometrical object deformation.\nExtensive experiments demonstrate that TRANS4D consistently outperforms exist-\ning state-of-the-art methods in generating 4D scenes with accurate and high-quality\ntransitions, validating its effectiveness.\n1\nINTRODUCTION\nRecent diffusion model (DM) advances have revolutionized video and 3D synthesis. By harnessing\nthe generative capability of DM, video generation methods (Liu et al., 2024b; Bao et al., 2024) have\nachieved high-quality video production that meets commercial standards. DreamFusion (Poole et al.,\n2023) introduced Score Distillation Sampling (SDS) to guide NeRF model optimization, marking a\nsignificant breakthrough in high-fidelity 3D generation.\nBuilding on these remarkable breakthroughs, 4D generation methods have demonstrated impressive\nperformance. These methods can be broadly categorized into three types: text-to-4D (Singer et al.,\n2023; Bahmani et al., 2024b; Zheng et al., 2024; Ling et al., 2024), single-image-to-4D (Zhao et al.,\n2023; Zheng et al., 2024), and monocular-video-to-4D (Ren et al., 2023; Jiang et al., 2024; Yin et al.,\n2023; Zeng et al., 2024; Zhang et al., 2024b; Wang et al., 2024a). Text-to-4D and Image-to-4D\nmethods (Yu et al., 2024; Bahmani et al., 2024b; Zheng et al., 2024) combine video and multi-view\ngeneration models with SDS to synthesize 4D objects, though the motion remains limited due to\ncurrent constraints in video generation models. Monocular-video-to-4D methods (Jiang et al., 2024;\nWang et al., 2024a) utilize prior dynamics from video conditions to achieve high-quality 4D object\nsynthesis with large-scale and natural motion, constrained by the requirement for videos with clear\nforeground subjects that are difficult to obtain. However, these methods primarily address local\ndeformations of individual objects and fall short of generating complex 4D scenes that involve global\ninteractions between multiple objects.\nRather than merely focusing on 4D object generation, text-to-4D methods like Comp4D (Xu et al.,\n2024) and monocular-video-to-4D methods such as Dreamscene4D (Chu et al., 2024) have achieved\n4D scene generation. These methods still use deformation networks to adjust local coordinates and\nsimulate movements of objects within 4D scenes, similar to 4D object generation methods. However,\n∗Contributed equally.\n†Corresponding authors: yangling0818@163.com, wentao.zhang@pku.edu.cn\n1\narXiv:2410.07155v1  [cs.CV]  9 Oct 2024\nPreprint.\nView 1\nView 2\nView 1\nView 2\n“The missile collided with the plane and exploded”\nComp4D\nTrans4D\nTime\nFigure 1: Comparing our TRANS4D with Comp4D (Xu et al., 2024) in 4D scene transition generation.\ndeformation networks are limited in handling significant object deformation in the 4D scene, which\ncomplicates the generation of 4D transitions with complex interactions, such as a missile transforming\ninto an exploded cloud or a magician conjuring a dancer.\nTo address these challenges, we propose a text-to-4D method TRANS4D, which leverages multimodal\nlarge language models (MLLMs) for geometry-aware 4D scene planning, and introduces a Transition\nNetwork to simulate significant objects deformation within the generated 4D scenes. Unlike existing\nMLLMs that primarily describe or recognize input conditions, or methods like Comp4D (Xu et al.,\n2024) that focus on basic object trajectory function, we propose Physics-aware 4D Transition\nPlanning method that enables MLLMs to generate detailed physical 4D information, including\ninitial positions, movement and rotation speeds, and transition times. This allows for more precise\n4D scene initialization and transition management. The Transition Network further realizes the\ntransition process by predicting whether each point in the 3DGS model should appear or disappear\nat a specific time t. This capability ensures great control over transitions, enabling large-scale\nobject transformations to be handled naturally and seamlessly, such as a missile transforming into an\nexploded cloud. As demonstrated in Fig. 1, our method achieves more natural and coherent 4D scene\nsynthesis with complex interactions than existing text-to-4D scene generation techniques.\nThe main contributions of TRANS4D can be summarized as:\n• In this work, we introduce a text-to-4D generation method called TRANS4D, which enables\ncomplex 4D scene synthesis and facilitates geometry-aware 4D scene transitions. Even\nif the 4D scene contains complex interactions or significant deformation among multiple\nobjects, our method can stably generate high-quality 4D scenes.\n• We present a Physics-aware 4D Transition Planning method, which sequentially leverages\nMLLM to perform physics-aware prompt expansion and transition planning. This approach\nensures effective and reasonable initialization for 4D scene generation.\n• We propose a geometry-aware Transition Network that achieves natural and smooth\ngeometry-aware transitions in 4D scenes.\n• Comprehensive experiments demonstrate that our TRANS4D generates more realistic and\nhigh-quality complex 4D scenes than existing baseline methods.\n2\nPreprint.\n2\nBACKGROUND & PROBLEM STATEMENT\n2.1\n4D CONTENT GENERATION\nResearch on 4D content generation begins with reconstructing dynamic 3D representations based\non multi-view videos. Existing 4D reconstruction models (Pumarola et al., 2021; Wu et al., 2024a;\nHuang et al., 2024) achieve realistic 4D generation by extending 3D models such as NeRF and 3DGS.\nHowever, obtaining multi-view videos for 4D synthesis is challenging. Recently, more researchers\nhave focused on 4D generation using simpler conditions, and these methods can be broadly divided\ninto three categories: text-to-4D, image-to-4D, and monocular-video-to-4D. The text-to-4D (Singer\net al., 2023; Bahmani et al., 2024b; Ling et al., 2024; Yu et al., 2024) and image-to-4D (Zhao et al.,\n2023; Zheng et al., 2024) methods are the first to be explored by researchers, typically extending\n3D objects into 4D objects using SDS loss based on pretrained video DM. However, due to the\nlimitations of SDS loss based on video DM, the dynamics of these 4D objects often seem unrealistic.\nSubsequently, some methods (Yin et al., 2023; Jiang et al., 2024; Zeng et al., 2024; Zhang et al.,\n2024b; Wang et al., 2024a) leverage monocular video as a condition to generate high-quality and\nnaturally dynamic 4D objects. Nevertheless, generating 4D scenes remains challenging for these\nmethods, as they often require monocular videos with clear foreground subjects, which are difficult\nto obtain. The text-to-4D method (Xu et al., 2024; Bahmani et al., 2024a), and the monocular-video-\nto-4D method (Chu et al., 2024), can generate 4D scenes, but they struggle with situations involving\ngeometrical 4D scene transitions. To address this, we propose TRANS4D, which enables the stable\nand convenient generation of 4D scenes with physical 4D transitions.\n2.2\nGENERATION WITH LARGE LANGUAGE MODEL\nInspired by the advancements in LLMs and MLLMs (Touvron et al., 2023; Liu et al., 2024a; Lin\net al., 2023a; Hong et al., 2023; Qi et al., 2024), many works have leveraged these models to achieve\nhigher-quality generation. In image generation (Dong et al., 2023; Zeng et al., 2023b; Yang et al.,\n2024a; Hu et al., 2024; Han et al., 2024; Berman & Peysakhovich, 2024) and image editing (Fu et al.,\n2024; Li et al., 2024b; Jin et al., 2024; Tian et al., 2024a; Yang et al., 2024b), LLMs are first utilized\nto enhance the quality of output images. Thanks to the powerful planning abilities of LLMs, these\nimage generation and editing methods can handle more complex scenarios. Subsequently, with the\nresearch surge sparked by Sora (Liu et al., 2024b), more and more video generation methods (Zeng\net al., 2023c; Bao et al., 2024; Wu et al., 2024c; Tian et al., 2024c; Maaz et al., 2024) and storytelling\napproaches (Soldan et al., 2021; Tian et al., 2024b; Yang et al., 2024c) have harnessed the impressive\ncapabilities of LLMs to achieve coherent and realistic video synthesis, significantly contributing to\nthe multimedia industry’s development. Furthermore, with advancements in text-to-3D techniques\n(Poole et al., 2023; Lin et al., 2023b; Wang et al., 2024b; Zeng et al., 2023a; Liang et al., 2024), some\n3D (Sun et al., 2023; Feng et al., 2023; Li et al., 2024a; Chen et al., 2024b; Zhou et al., 2024) and\neven 4D (Xu et al., 2024; Wang et al., 2024a; Chu et al., 2024) generation methods now involve LLMs\nto produce high-fidelity 3D or 4D outputs with complex geometrical structures based on simple\nconditions. However, simultaneously planning temporal progression and spatial layout remains\nchallenging for existing LLM and MLLM methods, making generating highly complex 4D scenes\ndifficult. In this work, we equip MLLMs with enhanced capabilities for 4D planning, enabling more\neffective generation of complex 4D scenes.\n2.3\nTRANSITION GENERATION\nAccording to the current research landscape, video transition synthesis is less explored than the more\npopular text-to-video and image-to-video generation methods. However, this direction is crucial in\ngenerating complex scenes and long stories. Scene transitions link two consecutive periods smoothly\nthrough location, setting, or camera viewpoint changes. This seamless transition ensures the coherent\nprogression of the scene or story. Before video scene transitions, related research primarily focused\non non-deep learning algorithms with fixed patterns, as well as Morphing (Wolberg, 1998; Shechtman\net al., 2010) that identify pixel-level similarities and generative models (Van Den Oord et al., 2017;\nGal et al., 2022) that leverage latent features of linear networks to achieve smooth and reliable\ntransitions. Recent works (Chen et al., 2023; Ouyang et al., 2024; Xing et al., 2024; Feng et al.,\n2024; Zhang et al., 2024a) have advanced the field by enabling smooth and creative video transitions,\n3\nPreprint.\n(a) Full Pipeline of Trans4D\n(b) Physics-aware 4D Transition Planning\n(c) Transition Network for 4D Scene Transition\nPhysics-aware 4D \nTransition Planning\nProvided \ntextual \nprompts\n𝑦\nDetailed 4D \nscene data\n𝑦𝑖\n𝑝𝑜𝑠𝜂\n𝑠𝑝𝑑𝜈\n𝑎𝑛𝑔𝜙\n𝑟𝑜𝑡𝜛\n𝑇𝑡𝑟𝑎𝑛𝑠\n4D Scenes \nInitialization\n“The missile collided with the plane and exploded”\n× 𝑀\n3DGS models\nInitial 4D scene\n𝑡\nTransition Network\n𝑝trans\nmodify the opacity of each by 𝑝𝑡𝑟𝑎𝑛𝑠\nchoose whether each point appears with 𝑝𝑡𝑟𝑎𝑛𝑠\nMain objects: Missile (0), airplane (1), smoke after \nexplosion (2).\nExpansion description: The missile's initial \ncoordinates are (-2.0, 0.0, 0.0). The airplane's \ninitial coordinates are (2.0, 0.0, 0.0). The missile \nmoves along the x-axis at a speed of 3\/48 per \nframe. The airplane moves in the opposite \ndirection along the x-axis at a speed of -3\/48 per \nframe. …… In time 0.6, an explosion happens, \nSmoke appears, and the missile disappears. The \nmissile (number 0) transitions into smoke (number \n2) during the time interval from 0.54 to 0.64.\nPrompt Expansion \npos = [(-2.0,0.0,0.0), \n(2.0,0.0,0.0),(0.0,0.0,0.\n0),],\nspd = [[(3.0,0.0,0.0), \n(0.0,0.0,0.0)], [(-\n3.0,0.0,0.0),(-1.0,-\n1.0,0.0)],[(0.0, 0.0, \n0.0)],],\nPlanning Result \nPlanning By Trans4D \n𝑻𝒔𝒑𝒅 = [[0.6],[0.6],[],],\nang = \n[(0,0,0),(0,180,0),(0,0,0),],\nrot = [[(10, 0, 0)],[(0, 0, \n0),(0,0,20),(0,0,0)],[(0,0,0)],],\n𝑻𝒓𝒐𝒕 = [[],[0.6, 0.64],[],],\n                    ……\n𝑻𝒕𝒓𝒂𝒏𝒔 = [(0.54, 0.64)]\nPlanning By \nComp4D \nScene \nDecomposition, \nPrompt \nAugmentation, \nand Trajectory \nDesigned by \nLLM.\nTrajectory \nfunction \n𝑥= 𝑓(·)\nLinear layers\nSigmoid\n4D Scene Transition\noptimize the Deformation \nnetwork, and the \nTransition network\n4D Transition During Training\n4D Transition During Inference\n𝑡\n𝑡\nFigure 2: Overview of our TRANS4D, consisting of physics-aware 4D Transition Planning and\nTransition Network that enable 4D scene generation with complex interaction.\npaving the way for the creation of story-level, long-form videos. In addition to the video transition,\nour work first involves the geometry-aware transition into the text-to-4D synthesis.\n3\nTRANS4D\nOur TRANS4D is designed to achieve reasonable physical 4D scene transitions, as illustrated in\nFig. 2(a). This section will explain how TRANS4D performs physics-aware 4D scene planning and\naccomplishes geometry-aware 4D transitions.\n3.1\nPRELIMINARIES\n3D Gaussian Splatting.\n3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) G consists of N\nGaussian points {gi, i = 1, 2, ..., N}, and each point is defined with a center position µ, covariance Σ,\nopacity α, and color c. Each point gi is represented by a Gaussian distribution, and during rendering,\nthe formula can be expressed as:\nG(x) =\nN\nX\ni=1\nαi · ci · exp\n\u0012\n−1\n2(x −µi)⊤Σ−1\ni (x −µi)\n\u0013\n,\n(1)\nwhere x is an arbitrary position in space during the rendering process.\nText-to-4D Generation.\nBefore introducing our method, defining the input and output of the\ntext-to-4D scene generation task is essential. In this work, the input is a text prompt y, and the output\nis a 4D scene represented by M 3D Gaussian Splatting (3DGS) models {Gi, i = 1, 2, ..., M}, along\nwith a deformation network {Di, i = 1, 2, ..., M} corresponding to each 3DGS model. Typically, the\n4\nPreprint.\ndeformation network is represented by a multi-layer perceptron (MLP):\nD(x, q, t) = (∆xt, ∆qt),\nxt = x + ∆xt,\nqt = q + ∆qt,\n(2)\nwhere x and q denote the arbitrary position and orientation within the 3DGS model, and xt and qt\nrepresent the corresponding position and orientation at time t.\n3.2\nPHYSICS-AWARE 4D TRANSITION PLANNING\nTo optimize the 4D scene effectively, it is crucial to plan the placement and trajectories of objects\nwithin the generated scene based on the textual prompts y. This process includes determining\nwhich objects to generate, as well as specifying their initial positions η, movement speeds v, initial\norientation angles ϕ, rotational speeds ϖ, and scene transition times Ttrans. Unlike Comp4D (Xu\net al., 2024), which only uses LLMs to predict simple trajectory functions for 4D synthesis, our\nTRANS4D method leverages MLLM vision-language priors and introduces a physics-aware prompt\nexpansion and transition planning approach. This advancement facilitates more reliable and complex\ninitialization of 4D scenes.\nPhysics-aware Prompt Expansion and Transition Planning.\nThe target of 4D planning is to\nderive spatial and temporal information from a given textual prompt. However, spatiotemporal data\nin a 4D scene are abstract and complex, making it difficult for LLMs or MLLMs to directly interpret\nand generate accurate physics-aware 4D scene data from a simple textual prompt. To overcome\nthis challenge, we propose a physics-aware 4D prompt expansion and transition planning method.\nFirst, the method applies physical principles to analyze the original prompt, deriving spatiotemporal\ninformation and decomposing it into scene prompts {yi, i = 1, 2, ..., M}. These prompts guide the\ncreation of 3D objects within the scene. By utilizing both these prompts and the language-vision\npriors of MLLM, we extend the original textual input into a comprehensive, physics-aware scene\ndescription for the target 4D scene. This description provides specific details, including the placement\nof objects, their movements, and rotations along the x, y, and z axes over time, as well as key events\n(e.g., changes in motion speed or the appearance and disappearance of objects). By converting this\ndescription into a specific data format, the desired 4D scene data is obtained. As illustrated in Fig 2(b),\nthis method enables MLLM to generate detailed and physically plausible 4D scene data, including η,\nv, ϕ, ϖ, and Ttrans. The detailed reasoning prompts are provided in the Appendix.\nInitialization of 4D Scene.\nBased on the {yi, i = 1, 2, ..., M} obtained through the planning\nmethod, we utilize SDS with text-to-image generation model (Ye et al., 2023) to guide basic 3DGS\nmodels {Gi, i = 1, 2, ..., M} synthesis. Using the planning 4D scene data, We calculate the transfor-\nmation function for any position within these 3DGS models at each time t as:\nx = R(ϕ + ϖ · t)xξ + η + v · t\n(3)\nwhere R denotes the rotation matrix, x represents the arbitrary position in the 3DGS model, and xξ is\nthe coordinate of x when the 3DGS model is at (0, 0, 0). By integrating {Gi, i = 1, 2, ..., M} with\nthe transformation function, we obtain an initial 4D scene.\nAfter obtaining the physics-aware planning, we use geometry-aware 4D transitions to effectively\nvisualize the physical dynamics derived from this planning. In the next section, we detail how our\nproposed transition network realizes these geometry-aware 4D transitions.\n3.3\nGEOMETRY-AWARE 4D TRANSITION\nBy utilizing the initial 4D scene and the deformation network, we can achieve 4D scene synthesis\nin certain scenarios through global object positioning and local dynamics. However, depending\nexclusively on movement is limited, as it cannot support geometry-aware 4D transitions that involve\nsignificant object deformation, such as the appearance or disappearance of objects in a 4D scene.\nTo overcome this limitation, we propose a geometry-aware Transition Network (TransNet), which is\na multi-layer perceptron (MLP) with a Sigmoid activation function at the output layer. As shown\nin Fig. 2(c), TransNet takes the position of the point cloud and the time t as inputs, and processes\nthem through several linear layers to produce an intermediate output. This intermediate output is then\n5\nPreprint.\nscaled by a coefficient wtrans before inputting into the final Sigmoid function. The final output of\nTransNet denotes as ptrans, which lies between 0 and 1 and serves as a reference for 4D transition.\nptrans = σ(wtrans · h(xt, qt, t)),\n(4)\nwhere h(xt, qt, t) represents the intermediate output from the linear layers of TransNet, σ is the\nSigmoid activation function, and wtrans is a scaling coefficient, typically set to 10 or higher, to\namplify the changes of the point cloud over time t.\nDuring the training stage, to ensure that TransNet is differentiable, we modify the opacity of each\npoint cloud by multiplying the opacity α directly with ptrans. During the inference stage, to ensure a\nnoticeable transition, ptrans is used to determine whether each Gaussian point of the 3DGS model\nappears in the 4D scene. This method enables a smooth and natural 4D scene transition. The\ncalculation process is as follows:\nB =\n\u001a1,\nwith probability ptrans,\n0,\nwith probability 1 −ptrans,\n(5)\nWhen B = 1, the point cloud appears in the 4D scene; otherwise, it does not. Compared to manually\nconstraining the number of points in the 3DGS model at different time intervals, TransNet allows\nfor flexible and rational control of point variations during the transition process, effectively achieving\ndesired geometry-aware 4D scene transitions.\n3.4\nEFFICIENT 4D TRAINING AND REFINEMENT\nConventional text-to-4D optimization strategies typically rely on SDS loss based on video DM to\nproduce 4D results with reliable dynamics, which incurs high computational costs. To efficiently\nachieve high-fidelity 4D scene synthesis with realistic dynamics, we optimize TRANS4D in two\nphases: first, we train the deformation network and TransNet using 3DGS models with a relatively\nsmall number of point clouds, minimizing costs even with SDS based on video DM. Then, we refine\n3DGS models, allowing for increased point cloud counts with lower computational overhead.\nDuring the training of the deformation network and TransNet, the number of points in each 3DGS\nmodel is fixed at 20,000. We represent the rendered images of the 4D scene over 16 consecutive\ntimes t as {I1, I2, ..., I16}. For SDS loss, noise is added to the rendered images, represented as\nI1\nt , I2\nt , ..., I16\nt\nat timestep t′. We optimize deformation network and TransNet using SDS based on\nvideo DM ϵvid, which can be expressed as:\n∇θdynLSDS−vid({I1, I2, ..., I16}, y) =\nEt′,ϵ\n\u0014\nw(t′)\n\u0010\nϵvid({I1\nt′, I2\nt′, ..., I16\nt′ }, y, t′) −ϵ\n\u0011∂{I1, I2, . . . , I16}\n∂θdyn\n\u0015\n,\n(6)\nwhere θdyn represents the parameters of deformation network and TransNet. During this training\nstage, the points in the 3DGS model are neither cloned nor split, ensuring efficient training of both\nnetworks. To further enhance the quality of the 4D scenes, we use an SDS loss based on text-to-image\nDM ϵimg to supervise further optimization of the 3DGS model. At this stage, the points in the 3DGS\nmodel are cloned and split for the refinement:\n∇θGLSDS(I, y) = Et′,ϵ\n\u0014\nw(t′)\n\u0010\nϵimg(I, y, t′) −ϵ\n\u0011 ∂I\n∂θG\n\u0015\n,\n(7)\nwhere I represents the rendered result of the 4D scene at a random time t, and θG represents the\nparameters of the 3DGS model. Meanwhile, the inputs to the deformation network and TransNet\nconsist solely of the positions of the 3DGS model’s points. Therefore, even after the refinement stage,\nwhile the 3DGS models in the 4D scene become more detailed and realistic, the dynamics of the 4D\nscene remain unaffected.\n6\nPreprint.\nConsistent4D\n4D-fy\nConsistent4D\n4D-fy\nDream-in-4D\nComp4D\nDream-in-4D\nComp4D\nThe magician conjured a dancer\nOurs\nOurs\nA pigeon appeared from a top hat\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\n𝒕𝟎\n𝒕𝟏\n𝒕𝟐\nView 1\nView 2\nView 1\nView 2\n𝒕\n𝒕\nFigure 3: Qualitative comparison with previous baseline methods (Bahmani et al., 2024b; Zheng et al.,\n2024; Jiang et al., 2024; Xu et al., 2024). Our method achieves smoother geometric 4D transitions\nand produces more realistic object interactions within 4D scenes.\n7\nPreprint.\nTable 1: Quantitive comparison of text-to-4D generation.\nMetrics\nConsistent4D\n4D-fy\nDream-in-4D\nComp4D\nTRANS4D (Ours)\nQAlign-vid-quality ↑\n2.275\n3.017\n3.035\n2.961\n3.226\nQAlign-vid-aesthetic ↑\n1.924\n2.089\n2.111\n1.774\n2.148\nVid-MLLM-metrics ↑\n0.5931\n0.4347\n0.5063\n0.5532\n0.6483\nCLIP-score ↑\n0.2836\n0.2661\n0.2607\n0.2757\n0.2941\nUser study ↑\n0.72\n0.64\n0.67\n0.59\n0.78\n4\nEXPERIMENTS\nImplementation Details.\nIn this work, all experiments are conducted on four A100-SXM4-80GB\nGPUs. In Stage 1, we optimize for 5000 steps using the Adam optimizer (Kingma, 2014) to obtain\nthe 3DGS models. In Stage 2, we perform 4500 optimization steps to train the deformation network\nand the Transition Network. During the refinement phase, we further optimize the 3DGS models for\nobjects that cannot be represented in high quality with only 20000 points (e.g., complex structures\nlike ”volcano”). This refinement is performed over 4000 steps using the SDS loss. We ensure a fair\ncomparison by using the same models across all methods for both generation and supervision. For\nany use of a text-to-image generation model, we use Stable Diffusion 2.1 (Rombach et al., 2022); for\nany use of a multiview generation model, we use MVDream (Shi et al., 2024); and for any use of a\ntext-to-video generation model, we use VideoCraft (Chen et al., 2024a).\nBaseline Methods.\nTo validate the effectiveness of our method in generating complex 4D scenes\nwith geometry-aware 4D transitions, we compare it with several different 4D generation methods.\nThese methods include text-to-4D-object methods 4D-fy (Bahmani et al., 2024b) and Dream-in-4D\n(Zheng et al., 2024), a monocular-video-to-4D-object method Consistent4D (Jiang et al., 2024), and\na text-to-4D-scene method Comp4D (Xu et al., 2024).\nMetrics.\nDue to the lack of visual ground truth in text-to-4D generation tasks, we employ QAlign-\nvid-quality and QAlign-vid-aesthetic metrics (Wu et al., 2024b) to evaluate the quality and aesthetics\nof the generated 4D scenes. To assess the semantic alignment of the generated results, we utilize the\nCLIP-score (Park et al., 2021) and MLLM-score. Additionally, we conduct a user study to enhance\nthe credibility of our comparison results. More details on QAlign-vid-quality, QAlign-vid-aesthetic,\nCLIP-score, MLLM-score, and the user study are provided in the Appendix.\n4.1\nTEXT-TO-4D SYNTHESIS\nQuantitative Results.\nTo assess the effectiveness of TRANS4D in complex 4D scene synthesis,\nwe utilize 30 complex textual prompts for 4D scene synthesis. Most of these prompts involve\ngeometry-aware transitions, with the specific prompts detailed in the supplementary material. As\nshown in Table 1, TRANS4D surpasses other methods across all metrics. The text-to-4D methods,\n4D-fy and Dream-in-4D, achieve high scores on the metrics utilized Q-align, demonstrating their\nability to generate high-quality 4D scenes. However, they perform poorly on the CLIP and MLLM\nscores, highlighting that it remains challenging for them to generate 4D scenes that accurately align\nwith the input text. Additionally, our TRANS4D achieved the highest score in the user study, further\nvalidating its effectiveness.\nQualitative Results.\nTo intuitively demonstrate the superiority of our method in generating complex\n4D scenes, that have significant object deformations, we conduct a qualitative comparison with other\nbaseline models. As shown in Fig. 3, the rendered videos of the 4D outputs generated by our\nmethod exhibit the most reasonable and high quality. Additionally, while 4D-fy and Dream-in-4D\nalso produce high-quality visual outputs, these text-to-4D-object generation methods struggle to\ncreate 4D scenes with coherent dynamics based on textual requirements. Lastly, the results from\nConsistent4D indicate that monocular-video-to-4D generation methods perform better for simple 4D\nobject generation. However, when the monocular video involves complex dynamics and interactions\n(as in the visualization example, ”The magician conjured a dancer”), these methods struggle to\nproduce satisfactory 4D outputs. Moreover, acquiring a monocular video with both clear subjects\n8\nPreprint.\nFigure 4: Additional user study for model analysis.\nView 1\nView 2\nw\/o Physics-aware \nprompt expansion\nLLM w\/ Physics-aware \nprompt expansion\nfull Physics-aware 4D \nplanning method\nFigure 5: Ablation study of Physics-aware 4D Transition Planning method.\nand reasonable dynamics is inherently challenging. Therefore, our TRANS4D is currently the most\nconvenient and reliable method for generating complex 4D scenes.\n4.2\nMODEL ANALYSIS\nTo highlight the key contributions of TRANS4D, including Physics-aware 4D Transition Planning\nand the Transition Network, we conduct additional user studies to demonstrate the effectiveness of\nour proposed models. Furthermore, we incorporate visual comparisons to showcase the necessity and\nbenefits of refinement.\nRationality of MLLM-planned Trajectory.\nWe have demonstrated that our method for initial-\nizing 4D scenes outperforms the simple function-based method Comp4D. To further showcase\nthe advantages of our Physics-aware 4D Transition Planning method, we conduct an experiment\nwhere volunteers evaluate videos generated from three different initialization methods: (1) Without\nPhysics-aware prompt expansion: the MLLM receives only one example (including input text and 4D\ndata) to generate 4D scenes based on other input texts; (2) Utilizing an LLM to predict the 4D data\nwith Physics-aware prompt expansion; and (3) Our complete Physics-aware 4D Transition Planning\nmethod. As shown in Fig. 4(a), without Physics-aware prompt expansion, the MLLM struggles to\ngenerate plausible 4D data for scene initialization, resulting in poor outcomes. This underscores the\nimportance of physics-aware prompt expansion. Moreover, when we utilize the LLM to produce\nthe 4D data with Physics-aware prompt expansion, the predicted 4D data lack precision due to the\nabsence of vision-language priors. As illustrated in Fig. 5, incorporating the full Physics-aware 4D\n9\nPreprint.\nw\/o Transition Network\nadjust the opacity by\nutilize             to \ndetermine whether a \npoint appears \nAn angelic girl is becoming a puppet of the devil.\nFigure 6: Ablation study of Transition Network.\nw\/o refinement\nw\/ refinement\nView 1\nView 2\nFigure 7: Ablation study of refinement.\nTransition Planning method significantly enhances the results, highlighting its ability to enrich our\napproach with prior knowledge for more reasonable scene initialization.\nGeometrical Expressiveness.\nTo better observe the effects of the transition network, we decelerate\nthe geometric-aware 4D transition process, allowing volunteers to discern the transition effects. We\nprovide the volunteers with three different videos representing various transition methods: (1) without\nusing the transition network; (2) using the transition network, where ptrans is multiplied by the\nopacity; and (3) using the transition network, where ptrans determines which points should appear.\nThe volunteers are asked to evaluate which process appears more natural. As shown in Fig. 4(b), it\nis evident that the majority of volunteers find the transitions incorporating the transition network to\nbe more natural, with the point selection method receiving the highest scores due to the clearer and\nmore distinct transition. We demonstrate the generated results in Fig. 6, which highlights the pivotal\nsignificance of the proposed transition network in this study.\nEfficiency and Quality of Refinement.\nWhen a 4D scene contains over 200,000 point clouds,\ndirectly supervising it with video SDS loss consumes 80GB or more GPU memory limit, while\nleading to suboptimal quality. In contrast, by separating the training process, we reduce memory\nusage to around 50GB, almost halving the requirement, while significantly improving the quality\nof the generated 4D scene. Specifically, we initially represent the 4D scene using minimal point\nclouds while training the deformation and transition networks. Then, we apply a refinement process\nto improve the quality of each 3DGS model by increasing the number of point clouds as needed.\nThis stepwise training manages memory efficiently while producing high-quality 4D scenes. As\ndemonstrated in Fig. 7, for massive 3D objects like “volcano erupting”, sparse point clouds cannot\nrepresent them effectively. Hence, refining such 3D objects is essential. In conclusion, our training\nstrategy balances efficiency and quality, enabling the generation of high-quality 4D scenes with\nrelatively limited computational resources.\n10\nPreprint.\n5\nCONCLUSION AND FUTURE WORK\nIn this work, we propose TRANS4D, a novel text-to-4D scene generation method that produces high-\nquality 4D scenes involving complex object interactions and significant deformations. Specifically,\nwe introduce a Physics-aware 4D Transition Planning method, which enables MLLM to initialize\nrealistic 4D scenes with multiple interacting objects. To facilitate geometry-aware transitions in\nthe generated 4D scene, we design a Transition Network that dynamically determines whether each\npoint cloud in the 4D scene should appear or disappear, allowing our method to handle substantial\nobject deformations naturally. Our experiments demonstrate that TRANS4D consistently generates\nhigh-quality 4D scenes with complex interactions and smooth, geometry-aware transitions.\nFor future work, We will continue to improve the quality of multi-object interactions in 4D scenes,\nwhich will help achieve more realistic 4D scene generation, and support the development of the video\nmultimedia and gaming industries.\nREFERENCES\nSherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu,\nJeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-\n4d generation. In ECCV, 2024a.\nSherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter\nWonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy:\nText-to-4d generation using hybrid score distillation sampling. In CVPR, 2024b.\nFan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao,\nShilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-\nvideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.\nWilliam Berman and Alexander Peysakhovich. Mumu: Bootstrapping multimodal image generation\nfrom text-to-image data. arXiv preprint arXiv:2406.18790, 2024.\nHaoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR,\n2024a.\nSijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan,\nand Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning\nand planning. In CVPR, 2024b.\nXinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang,\nDahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative\ntransition and prediction. In ICLR, 2023.\nWen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene\ngeneration from monocular videos. arXiv preprint arXiv:2405.02280, 2024.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and\ncreation. In ICLR, 2023.\nYao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J Black. Posegpt:\nChatting about 3d human pose. arXiv preprint arXiv:2311.18836, 2023.\nYutang Feng, Sicheng Gao, Yuxiang Bao, Xiaodi Wang, Shumin Han, Juan Zhang, Baochang Zhang,\nand Angela Yao. Wave: Warping ddim inversion features for zero-shot text-to-video editing. In\nECCV, 2024.\nTsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding\ninstruction-based image editing via multimodal large language models. In ICLR, 2024.\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nStylegan-nada: Clip-guided domain adaptation of image generators. TOG, 2022.\n11\nPreprint.\nYucheng Han, Rui Wang, Chi Zhang, Juntao Hu, Pei Cheng, Bin Fu, and Hanwang Zhang. Emma:\nYour text-to-image diffusion model can secretly accept multi-modal prompts. arXiv preprint\narXiv:2406.09162, 2024.\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang\nGan. 3d-llm: Injecting the 3d world into large language models. In NeurIPS, 2023.\nXiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models\nwith llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.\nYi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs:\nSparse-controlled gaussian splatting for editable dynamic scenes. In CVPR, 2024.\nYanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. Consistent4d: Consistent 360° dynamic\nobject generation from monocular video. In ICLR, 2024.\nYing Jin, Pengyang Ling, Xiaoyi Dong, Pan Zhang, Jiaqi Wang, and Dahua Lin. Reasonpix2pix:\nInstruction reasoning dataset for advanced image editing. arXiv preprint arXiv:2405.11190, 2024.\nBernhard Kerbl, Johannes Hanika, Thomas M¨uller, Harshavardhan Kondapaneni, Francesco Di Gia-\ncomo, Thomas Leimk¨uhler, Chris Chaitanya, Matthias Nießner, and Roland Heged¨us. 3d gaussian\nsplatting for real-time radiance field rendering. In SIGGRAPH, 2023.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nHong Li, Yutang Feng, Song Xue, Xuhui Liu, Bohan Zeng, Shanglin Li, Boyu Liu, Jianzhuang Liu,\nShumin Han, and Baochang Zhang. Uv-idm: Identity-conditioned latent diffusion model for face\nuv-texture generation. In CVPR, 2024a.\nShanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xiuhui Liu, Jiaming Liu, Lin Li, Xu Tang, Yao\nHu, Jianzhuang Liu, et al. Zone: Zero-shot instruction-guided local editing. In CVPR, 2024b.\nYixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer:\nTowards high-fidelity text-to-3d generation via interval score matching. In CVPR, 2024.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, 2023b.\nHuan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your\ngaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In CVPR, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS,\n2024a.\nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang,\nHanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and\nopportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024b.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image\nand video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024.\nYichen Ouyang, Hao Zhao, Gaoang Wang, et al. Flexifilm: Long video generation with flexible\nconditions. arXiv preprint arXiv:2404.18620, 2024.\nDong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for\ncompositional text-to-image synthesis. In NeurIPS, 2021.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. In ICLR, 2023.\n12\nPreprint.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural\nradiance fields for dynamic scenes. In CVPR, 2021.\nZekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and\nKaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. arXiv\npreprint arXiv:2402.17766, 2024.\nJiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaus-\nsian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nEli Shechtman, Alex Rav-Acha, Michal Irani, and Steve Seitz. Regenerative morphing. In CVPR,\n2010.\nYichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. In The Twelfth International Conference on Learning Representations,\n2024.\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation.\narXiv preprint arXiv:2301.11280, 2023.\nMattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, and Bernard Ghanem. Vlg-net: Video-\nlanguage graph matching network for video grounding. In ICCV, 2021.\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt:\nProcedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945, 2023.\nBozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou,\nXi Chen, and Huajun Chen. Instructedit: Instruction-based knowledge editing for large language\nmodels. arXiv preprint arXiv:2402.16123, 2024a.\nChangyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen,\nLewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling\nvia multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024b.\nYe Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen\nYu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation.\narXiv preprint arXiv:2406.04277, 2024c.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.\nYikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single\ngenerated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. arXiv preprint\narXiv:2405.16822, 2024a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In\nNeurIPS, 2024b.\nGeorge Wolberg. Image morphing: a survey. The visual computer, 1998.\nGuanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian,\nand Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR,\n2024a.\nHaoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao,\nAnnan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via\ndiscrete text-defined levels. In ICML, 2024b.\n13\nPreprint.\nJialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long.\nivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv:2405.15223,\n2024c.\nJinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong.\nTooncrafter: Generative cartoon interpolation. arXiv preprint arXiv:2405.17933, 2024.\nDejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N Plataniotis,\nand Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint\narXiv:2403.16993, 2024.\nLing Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering\ntext-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML,\n2024a.\nLing Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan.\nEditworld: Simulating world dynamics for instruction-following image editing. arXiv preprint\narXiv:2405.14785, 2024b.\nShuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen.\nSeed-story: Multimodal long story generation with large language model.\narXiv preprint\narXiv:2407.08683, 2024c.\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt\nadapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and\nFei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality\ncollaboration. In CVPR, 2024.\nYuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content\ngeneration with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023.\nHeng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A\nJeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via\nvideo diffusion models. arXiv preprint arXiv:2406.07472, 2024.\nBohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang,\nJianzhuang Liu, and Baochang Zhang. Ipdreamer: Appearance-controllable 3d object generation\nwith image prompts. arXiv preprint arXiv:2310.05375, 2023a.\nBohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu,\nand Baochang Zhang. Controllable mind visual diffusion model. arXiv preprint arXiv:2305.10135,\n2023b.\nBohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, and Baochang Zhang.\nFace animation with an attribute-guided diffusion model. arXiv preprint arXiv:2304.03199, 2023c.\nYifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun\nCao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. arXiv preprint\narXiv:2403.14939, 2024.\nBowen Zhang, Xiaofei Xie, Haotian Lu, Na Ma, Tianlin Li, and Qing Guo. Mavin: Multi-action video\ngeneration with diffusion models via transition video infilling. arXiv preprint arXiv:2405.18003,\n2024a.\nHaiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion:\nMulti-view video diffusion model for 4d generation. arXiv preprint arXiv:2405.20674, 2024b.\nYuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124:\nAnimating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023.\nYufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. A unified\napproach for text-and image-guided 4d scene generation. In CVPR, 2024.\n14\nPreprint.\nXiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun,\nand Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided\ngenerative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024.\n15\nPreprint.\nA\nAPPENDIX\nIn Appendix A.1, we provide detailed information of our evaluation metrics. Appendix A.2 outlines\nthe specific prompts and process of the Physics-aware 4D Transition Planning method. Finally, in\nAppendix A.3, we present the textual prompts used for evaluation along with additional 4D scenes\ngenerated by TRANS4D.\nTable 2: 4D scene decomposition.\nYou are a 4D Scene Decomposing Agent\nYour task is to decompose the 4D scene into several\nappropriate parts based on the prompt provided by the\nuser. Unlike 3D scene generation methods that only need to\nsplit according to the content of the prompt, you need to\nanalyze the possible physical dynamic that may occur in\nthe provided prompt from both temporal and spatial\ndimensions. Concurrently, based on the analysis results,\ndecompose the provided prompt into several prompts in the\ntime-space dimension.\n--- Main Object ---\nThese objects’ prompts will be used for generating 3D objects\nfirst, and then add time dimension to generate a complete\n4D scene.\nTherefore, if the same object undergoes significant physical\nchanges over time, it should be considered as two separate\nmain objects.\nThe scene’s background is blank, and only moving objects,\nsuddenly appearing objects like clouds and smoke, and\nobjects undergoing shape changes, such as melting or\nbreaking, need to be considered.\n--- Examples ---\n......\nA.1\nDETAILS OF METRICS\nIn this section, we provide a more detailed explanation of the metrics and user studies discussed in\nthe main paper.\nQAlign-vid-quality and QAlign-vid-aesthetic.\nQ-Align (Wu et al., 2024b) is a large multi-modal\nmodel fine-tuned from mPLUG-Owl2 (Ye et al., 2024) using extensive image and video quality-\nassessment datasets. It has demonstrated strong alignment with human judgment on existing quality\nassessment benchmarks. In line with Comp4D (Xu et al., 2024), we use Q-Align to evaluate the\nquality of the generated 4D scenes. Specifically, we input rendering videos of 4D scenes produced by\nvarious methods from viewpoints of -120◦, -60◦, 0◦, 60◦, 120◦, and 180◦into Q-Align. The output\nscores from Q-Align range from 1 (worst) to 5 (best). We calculate the average score of these outputs\nto compare the performance of different 4D generation methods quantitatively.\nCLIP score.\nThe CLIP score (Park et al., 2021) is a widely used metric for evaluating the correlation\nbetween input textual prompts and generated images. Following the approach in 4D-fy (Bahmani\net al., 2024b), we calculate the CLIP score between the frames of the rendered videos and the input\ntextual prompts. Due to the complexity of 4D scene generation, which involves significant object\ndynamics, we use the maximum CLIP score obtained across all frames of each rendered video as the\n16\nPreprint.\nTable 3: Complete Scene Expansion Description\nYou are an Efficient Scene Expansion Agent.\nYour task is to use these decompositional main objects and the\nprompt to expand the provided prompt into a complete\nphysics-aware 4D scene description.\n--- Scene ---\nThe scene is a 4D video clip composed of the main objects\nextracted earlier. The scene information should include:\n- The initial position of each object, represented in the form\n[x, y, z].\n- The movement path of the objects defines the movement vector\nper frame. Each object can have multiple movement segments.\n- The time points when movements start or stop.\n- The initial rotation angle of the objects is expressed in\ndegrees as [rx, ry, rz] (rotation along the x, y, and z\naxes respectively).\n- The rotation path of the objects, defining the rotation\nchange per frame.\n- The time intervals when rotations occur.\n- The time states of the objects, such as when they appear,\ndisappear, or transform at specific times.\n- The transformation relationships between objects, specifying\nwhich objects transform into each other during certain\ntime intervals and when these transformations occur.\nThe time points are represented within a single 4D segment,\nwith 0 indicating the start and 1 indicating the end.\nOther states use decimals to specify the exact time point\nwithin the segment.\nThe scene’s center is [0, 0, 0], and the range for each\ncoordinate axis within the scene is [-1, 1]. Positions\noutside this range are considered outside the scene.\nObjects can enter the scene from outside, but each main\nobject must appear within the scene at some point.\n--- Examples ---\n......\nrepresentative score. To evaluate their performance, we compare the average CLIP scores of rendered\nvideos generated by different methods.\nMLLM score.\nAlthough the CLIP score is a commonly used metric to evaluate semantic alignment,\nit can not fully analyze the reasonability of rendered videos. To more effectively evaluate the semantic\nalignment of the generated 4D results, we propose the MLLM score which leverages the vision-\nlanguage knowledge of GPT4o to evaluate the correlation between the rendered videos and the input\ntextual prompts. Specifically, we present the rendered videos and the provided textual prompts for\nthe ChatGPT-4o. The specific prompt provided for ChatGPT-4o scoring the semantic alignment as:\n“We provide several <video> clips along with a <text prompt>. The videos represent rendered 4D\nscenes from specific viewpoints. Please evaluate the 4D scenes generated by different methods based\non the alignment between the video and the text prompt, as well as the overall video quality, and\nassign a score between 0 and 1.”\n17\nPreprint.\nTable 4: The specific prompt for obtaining 4D planning data.\nYou are a 4D data production Agent.\nYour task is to transfer the complete 4D scene description\ninto precise 4D planning data.\nThe output should be in the json format:\n{\n\"sample\": {\n\"obj_prompt\": [\n\"List of objects involved in the scenario\"],\n\"TrajParams\": {\n\"init_pos\": [\n[x, y, z] \/\/ Initial positions of objects in 3D\nspace],\n\"move_list\": [\n[\n[dx, dy, dz], \/\/ Movement vector\n[dx, dy, dz] \/\/ Additional movement after an\nevent\n] ],\n\"move_time\": [\n[time] \/\/ List of times when movements occur or\nstop],\n\"init_angle\": [\n[rx, ry, rz] \/\/ Initial rotation angles (degrees)\nof objects along x, y, z axes],\n\"rotations\": [\n[\n[rx, ry, rz], \/\/ Rotation vector per frame\n[rx, ry, rz] \/\/ Optional: Additional rotation\nafter an event\n] ],\n\"rotations_time\": [\n[start_time, end_time] \/\/ Times when rotations\noccur],\n......\n\"trans_list\": [\n[obj_index, transition_obj_index] \/\/ Objects that\ntransition into each other],\n\"trans_period\": [\n[start_time, end_time] \/\/ The time period when the\ntransition occurs.]\n}\n}\n}\nUser study.\nFor unsupervised text-to-4D-scene generation, the user study is the most convincing\nmetric. To further validate the effectiveness of our method, we conduct a comprehensive user study\ninvolving 80 volunteers. Each volunteer is randomly provided 10 test examples from the testing\ndataset introduced in this work. For each example, volunteers are asked to judge whether the generated\nresults from various 4D methods successfully achieve the desired 4D synthesis based on the given\ntext inputs. Volunteers rate each result on a scale from 0 to 1, where a score closer to 1 indicates\nbetter alignment with the expected outcome.\n18\nPreprint.\nTable 5: Textual prompts used in the user study.\nThe missile collided with the plane and exploded.\nA cavalry charged two shield-bearing infantry.\nThe magician conjured a dancer.\nThe ice block melts into water.\nThe volcano erupted violently.\nThe tree fell after being cut by the harvester.\nThe water balloon burst on impact.\nThe clock struck midnight.\nThe egg cracked open.\nThe spaceship took off from Earth and entered space.\nThe tornado formed over the plains.\nThe butterfly emerged from the cocoon.\nThe snowflake melted on the tongue.\nThe fish jumped out of the water.\nThe corn kernels pop into popcorn.\nThe moon appeared from behind the clouds.\nA pigeon appeared from a top hat.\nAn angelic girl is becoming a puppet of the devil.\nAn explosion occurs while a wizard is brewing a magic potion.\nA sage caused a gigantic flower to bloom.\nThree worshippers pray for the appearance of an angel.\nA zombie crawls out of the tombstone.\nA dragon breathes fire onto a knight’s shield.\nA giant cracks the ground with its heavy footsteps.\nA knight draws a glowing sword from a stone.\nA sorcerer opens a portal to another dimension.\nA ghost passes through a wall, leaving behind a cold mist.\nA castle tower collapses after being struck by lightning.\nA violin plays itself, filling the air with haunting melodies.\nThe appearance of the sun clears the fog.\nA.2\nMORE DETAILS OF OUR MODEL\nPhysics-aware 4D planning.\nMultimodal Large Language Models (MLLMs), leveraging their\nvision-language priors, have the potential to generate reasonable and natural spatiotemporal data.\nIn this work, we leverage the spatiotemporal awareness of MLLMs to achieve impressive 4D scene\ninitialization and 4D transition planning. During the process of obtaining 4D data, we require the\nMLLM to ensure that the generated plans are consistent with physical principles and geometrically\ncoherent, thereby guaranteeing both physical plausibility and the correctness of spatial relationships.\nSpecifically, Tables 2, 3, and 4, present the detailed prompts for scene decomposition, physics-aware\n4D prompt expansion, and 4D planning data, respectively.\nA.3\nADDITIONAL RESULTS\nTextual prompts used for comparison.\nIn Table. 5, We provide the specific textual prompts used\nin quantitative comparison.\ncomparison results.\nIn Fig. 8, we provide more generated 4D scenes of TRANS4D, to further\ndemonstrate the effectiveness of our method.\n19\nPreprint.\nIce cube melts into water\nThree worshippers pray for the appearance of an angel.\nThe tree cut off by a harvester.\nGreen flames ignited during the wizard's process of brewing the potion.\nFigure 8: Additional generated 4D results, our TRANS4D can consistently produce high-quality 4D\nscenes.\n20\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Trans4D：基于文本的4D场景合成新框架\n\n## 📌 背景痛点\/本文动机\n随着扩散模型在图像和视频生成方面的突破，4D合成技术也得到了快速发展。现有的4D生成方法可以基于用户友好的条件生成高质量的4D对象或场景，为游戏和视频行业带来了巨大价值。然而，这些方法在合成复杂4D过渡和场景内交互中的显著对象变形方面仍然存在挑战。\n\n## 🚀 核心方法\n💡 创新点1：物理感知的4D过渡规划\nTrans4D利用多模态大型语言模型（MLLMs）生成物理感知的场景描述，用于4D场景初始化和有效的过渡时间规划。这种方法能够确保4D场景的物理合理性和空间关系的正确性。\n\n💡 创新点2：几何感知的4D过渡网络\nTrans4D提出了一个几何感知的过渡网络（TransNet），它能够根据规划动态地确定4D场景中每个点云是否应该出现或消失。这使得Trans4D能够自然地处理大规模的对象变形，例如导弹变成爆炸云。\n\n## 📈 实验结果\nTrans4D在生成具有准确和高质量过渡的4D场景方面始终优于现有的最先进方法，验证了其有效性。\n\n## 💬 可借鉴之处\nTrans4D为4D场景合成提供了一种新的思路，其物理感知的过渡规划和几何感知的过渡网络为生成具有复杂交互和显著变形的4D场景提供了有效的方法。此外，Trans4D的训练策略平衡了效率和质量，使其能够在有限的计算资源下生成高质量的4D场景。","llm_summary_res_status":200}
{"title":"ING-VP: MLLMs cannot Play Easy Vision-based Games Yet","authors":"Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, Ge Zhang","summary":"As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps:\/\/github.com\/Thisisus7\/ING-VP.git.","url":"http:\/\/arxiv.org\/abs\/2410.06555v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.06555v1","published":1728451058000,"comment":"49 pages, 12 figures","pdf_text":"Preprint\nING-VP: MLLMS CANNOT PLAY EASY VISION-BASED\nGAMES YET\nHaoran Zhang1∗, Hangyu Guo1∗, Shuyue Guo1, Meng Cao3,\nWenhao Huang1,2, Jiaheng Liu1†, Ge Zhang1,2†\n1M-A-P,\n2Bytedance.Inc,\n3MBZUAI\nhaor7@outlook.com, zhangge.eli@bytedance.com\nABSTRACT\nAs multimodal large language models (MLLMs) continue to demonstrate increas-\ningly competitive performance across a broad spectrum of tasks, more intricate\nand comprehensive benchmarks have been developed to assess these cutting-edge\nmodels. These benchmarks introduce new challenges to core capabilities such\nas perception, reasoning, and planning. However, existing multimodal bench-\nmarks fall short in providing a focused evaluation of multi-step planning based\non spatial relationships in images. To bridge this gap, we present ING-VP,\nthe first INteractive Game-based Vision Planning benchmark, specifically de-\nsigned to evaluate the spatial imagination and multi-step reasoning abilities of\nMLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with\n6 unique configurations. A single model engages in over 60,000 rounds of in-\nteraction. The benchmark framework allows for multiple comparison settings,\nincluding image-text vs. text-only inputs, single-step vs. multi-step reasoning,\nand with-history vs. without-history conditions, offering valuable insights into\nthe model’s capabilities. We evaluated numerous state-of-the-art MLLMs, with\nthe highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy\nof only 3.37%, far below the anticipated standard. This work aims to provide\na specialized evaluation framework to drive advancements in MLLMs’ capacity\nfor complex spatial reasoning and planning. The code is publicly available at\nhttps:\/\/github.com\/Thisisus7\/ING-VP.git.\nw\/ history & w\/o history\nImage-text & Text-only\nMulti-step & One-step\nPerception\nText Understanding\nSpatial Imagination\nPlanning\nReasoning\nING-VP\nBench\n6 Games\n3 Comparisons\n          Capacity                              Evaluation\nMultimodal & Interactive Environment\nPrompts\nImage\/text\nLevels\nModel\n1.question\n1\/4.initial state\nGame Env\n2.output\nDatabase\n2.instruction\n3.next state\n4.next state\n4.history\nConsider the multi-step process with a history-enabled setting:\nThe model ingests game data as input and generates an output in response to the specific prompt.\n1.\nThis output is relayed to a database, which extracts the corresponding instructions and sends them to the game\nenvironment.\n2.\nThe game environment updates its state based on these instructions, returning the new game data to the database.\n3.\nThis updated data, together with the prompt, is used as the next input for the model, which continues to generate\noutputs for the database.\n4.\nThis cycle—steps 2 through 4—is repeated until the model successfully completes the level or the step limit is\nreached.\n5.\nFigure 1: The overview of ING-VP benchmark.ING-VP comprises 6 distinct games, conducts 3\ncomparative analyses across 6 experimental settings, and evaluates 5 key capabilities of MLLMs.\nAdditionally, it offers a highly efficient interactive environment for both inference and analysis.\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated remarkable capabilities in natural language\nprocessing, generation, and even textual complex reasoning and planning (Zhao et al., 2023). Building\nupon this powerful foundation of LLMs, integrating visual inputs has led to the development of even\n∗Equal Contributions, † Corresponding Authors.\n1\narXiv:2410.06555v1  [cs.CL]  9 Oct 2024\nPreprint\nmore powerful models (OpenAI, 2024; Anil et al., 2023a), a.k.a multimodal large language models\n(MLLMs).\nDespite demonstrating impressive performance in handling most general multimodal tasks, the\neffectiveness of MLLM in multimodal reasoning and planning still remains unclear. Moreover, recent\nstudies (Lu et al., 2024; Dai et al., 2024) indicate that vision-language training might degrade the\ntextual capabilities of MLLMs, suggesting that MLLMs built upon LLMs could be impaired when\nadapted to multimodal reasoning and planning tasks. Consequently, there is an urgent need for a\ntest that incorporates multimodal complex reasoning and planning cases to guide the subsequent\nenhancements of MLLMs.\nTo address this issue, existing studies generally utilize visual question answering (VQA) (Antol\net al., 2015; Kafle & Kanan, 2017) and game-based evaluations (Wu et al., 2023; Bellemare et al.,\n2013) to assess the visual reasoning capabilities of MLLMs. In general, VQA necessitates a verified\nground-truth answer that relies on human annotations. But acquiring these annotations is both\ncostly and time-consuming. Moreover, the absence of interaction and planning in typical VQA tasks\nposes difficulties in evaluating the reasoning and planning capabilities of advanced MLLMs. The\ntasks presented in these benchmarks are overly simplistic (Yue et al., 2023) or only test reasoning\nwithin domain-specific knowledge (Yue et al., 2023; Zhang et al., 2024a), which mainly evaluates\nthe LLM knowledge of MLLMs rather than the perception, reasoning, and planning of MLLMs.\nTherefore, recent studies (Xu et al., 2024; Chia et al., 2024) prompt MLLMs to interact with digital\ngame environments, which are measured by game outcomes and scores, leading to the game-based\nevaluation. Unlike VQA tasks, these methods can evaluate the multi-step reasoning capabilities\nand even spatial imagination of MLLMs, which is crucial function of human cognition, allowing\nus to interact with realistic environments (Wu et al., 2024). Despite the effectiveness, these works\nare typically restricted to individual games with complex rules, involve time-consuming evaluation\nepisodes, and fail to effectively assess the models’ generalization capabilities in multimodal planning.\nConsidering these challenges, our goal is to develop a generalizable and efficient benchmark to\nevaluate the multi-step planning abilities of MLLMs, providing insights for subsequent improvements\nof MLLMs with complex multi-step reasoning.\nTo fill this gap, in this paper, we introduce the INteractive Game-based Vision Planning benchmark\n(ING-VP), meticulously focusing on evaluating the spatial imagination and multi-step reasoning\nabilities of MLLMs. Figure 1 shows games, evaluation settings, and the interactive process in our\nING-VP. To construct our ING-VP, we initially collect six games featuring easily understandable\nrules. In each game, we collect 50 levels, each comprising both an image and a text representation of\nthe current state, providing vision and textual inputs for MLLMs, as illustrated in Figure 2. To assess\nthe spatial imagination and planning capabilities of MLLMs, we establish six experimental settings,\nwhich prompt the models to perform single-step and multi-step reasoning, with or without historical\ninteraction. During the evaluation, we employ MLLMs to interact within the environment until the\ngame is completed. To evaluate model performance comprehensively, beyond merely determining\nwhether a model can finish a game, we also use the model’s action efficiency and the remaining steps\nto complete the game as evaluation metrics.\nWith our ING-VP, we test 15 open- and closed-source MLLMs and analyze their performance on\nour test cases. We first support the benchmark designed to evaluate the multi-step reasoning and\nspatial imagination capabilities of MLLMs — ING-VP bench. Then we analyze these capabilities of\ncurrent open- and closed-source MLLMs, despite a performance gap, the leading open-source model,\nInternVL2-Llama3-76B, achieves an accuracy of 2.50%, ranking just behind Claude-3.5 Sonnet,\nGPT-4o, and Gemini-1.5 Pro. Notably, its performance significantly surpasses that of GPT-4o mini,\nwhich stands at 1.05%, and GPT-4v, which records a mere 0.32%. We also conduct a detailed analysis\nof these models’ performance, the evidence shows that:\n• The inability to process the relative positions of elements is one of the primary issues with\nMLLM perception.\n• Even the most advanced MLLMs have very limited planning capabilities, far below the\nperformance of ordinary humans on these simple tasks.\n• Current models tend to generate instructions that are much longer than necessary to complete\nthe levels. While this can improve accuracy on simple levels, it also indirectly reveals that\nMLLMs are “uncertain” about the correct solution.\n2\nPreprint\nWhile most tasks in the ING-VP benchmark are straightforward for humans, they pose significant\nchallenges for MLLMs, even the top-performing model, Claude-3.5 Sonnet, achieving an average\naccuracy of just 3.37%. We reveal that current MLLMs generally lack spatial imagination and\nmulti-step planning abilities, and offer a new perspective on the capability requirements for MLLMs.\nSokoban\nInstruction:\naction: L(Left); R(Right); U(Up); D(Down)\nundo: return to a specified history state\nMaze\nInstruction:\naction: L(Left); R(Right); U(Up); D(Down)\n8-queens\nInstruction:\naction: [x,y], coordinates of chess pieces\nundo: return to a specified history state\nHanoi\nInstruction:\naction: “{x}{y}”, move the top disk from rod x\nto rod y,  smaller on larger\nSudoku\n15-puzzle\nInstruction:\naction: {number}, if the number is around\nthe empty space, , they will swap positions.\nInstruction:\naction: \"{row}{column}\": {number}\nundo: return to a specified history state\nImage\nText\nIntroduction:\nA puzzle game where the player navigates a\nwarehouse, \npushing \nboxes \nto \ndesignated\nstorage locations.\nGoal:\nMove all boxes onto\nthe storage locations.\nImage\nIntroduction:\nA puzzle game where the player navigate\nthrough a complex labyrinth, finding the path\nto the exit.\nText\nGoal:\nMove the red square\nto the green square.\nImage\nText\nIntroduction:\nA chess problem that challenges the player to\nplace eight queens on an 8x8 chessboard so\nthat no two queens threaten each other.\nGoal:\nNo two queens can\nshare the same row,\ncolumn and diagonal.\nImage\nText\nIntroduction:\nA puzzle where the objective is to move a stack\nof disks from one rod to another, following\nspecific rules about disk placement.\nGoal:\nMove all disks to \nthe rod D.\nImage\nText\nIntroduction:\nA sliding puzzle consisting of a 4x4 grid with 15\nnumbered tiles and one empty space.\nGoal:\nSlide the tiles to\narrange them in\nnumerical order.\nImage\nText\nIntroduction:\nA logic-based number-placement puzzle.\nGoal:\nFill a 9×9 grid with digits\nso that each column,\nrow, and 3×3 sub-grid\ncontains all digits from 1\nto 9 without repetition.\nFigure 2: ING-VP examples sampled from each game. Includes pictures and text representations of\nSokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, and 15-puzzle.\n2\nRELATED WORK\nMultimodal Large Language Models. LLMs (Achiam et al., 2023; Anil et al., 2023b) have\ndemonstrated their ability of generating human-like texts to understand and respond to complex\ninstructional queries. The successes of LLMs has elicited the burgeoning proliferation of multi-modal\nLLMs (Alayrac et al., 2022; Li et al., 2023b; Liu et al., 2024; Sun et al., 2024; Jin et al., 2023b),\nwhich is designed to process and integrate multiple types of data. The primary attempt Flamingo\n(Alayrac et al., 2022) endows visual-language models with in-context few-shot learning capabilities\nby trained on large-scale interleaved text-image data. BLIP2 (Li et al., 2023b) designs a Q-Former\narchitecture to align the visual-textual knowledge during the pre-training phase. LLaVA (Liu et al.,\n2024) collect GPT-4 generated multimodal language-image instruction-following data and train a\ngeneral-purpose visual-language assistant. Beyond multimodal understanding, EMU-2 (Sun et al.,\n2024) and LaVIT (Jin et al., 2023b) take one step further and act as generative multimodal model to\nsupport visual prompting and object-grounded generation.\nMLLM Benchmarks. The development of MLLMs has highlighted the critical need of benchmarks\nfor thorough evaluations. Although traditional visual-language tasks (e.g., visual question answering\n(Antol et al., 2015; Kafle & Kanan, 2017) and image captioning (Lin et al., 2014; Plummer et al.,\n2015)) can be used as evaluation benchmarks, they are too strict and require the exact match with\nthe ground-truth answers. To this end, LVLM-eHub (Xu et al., 2023) and LAMM (Yin et al., 2024)\nreformulate exiting public datasets as evaluation samples and employ human annotators or GPT\nto assess the quality. MME (Li et al., 2024), MMBench (Liu et al., 2023b) and SEED-Bench (Li\net al., 2024) construct multiple-choice questions to mitigate the subjectivity and instability of GPT\nevaluation. MMMU (Yue et al., 2024) evaluate the advanced perception and reasoning of MLLMs on\nspecific domains (e.g., science, business).\nGame-based Evaluations. Digital games are acknowledged as essential in the pursuit of artificial\ngeneral intelligence since they present complex challenges requiring advanced reasoning and cognitive\n3\nPreprint\nskills. These challenges make digital games an ideal benchmark for evaluating the capabilities of\nMLLMs (Wu et al., 2023; Bellemare et al., 2013; Hu et al., 2024; Sweetser, 2024; Xu et al., 2024)\nincluding the environment perception (Hong et al., 2023; Akoury et al., 2023), memory construction\n(Zhu et al., 2023; Zhang et al., 2024b; Ding et al., 2023; Park et al., 2022; Liu et al., 2023a), reasoning\n(Liu et al., 2023a; Wang et al., 2023a; Qian et al., 2023; Huang et al., 2022) and decision-making\n(Chen et al., 2023; Zhou et al., 2023; Jin et al., 2023a; Qian et al., 2023). Several methods focus on\nsemantic-level perception of environmental elements including locations, objects or actions in games.\nThey either use basic text input of user ideas (Li et al., 2023a) or game state variables and dialogues\n(Akoury et al., 2023; Park et al., 2022; 2023). Role-based inputs, e.g., the inclusion of character,\nstory, role-related information (Hong et al., 2023; Wang et al., 2023b) and skills (Gong et al., 2023)\nare often included. TorchCraft (Synnaeve et al., 2016) is presented to use real-time strategy games\nsuch as StarCraft: Brood War to serve as a benchmark for AI research. The Chess game has long\nbeen employed as an AI testing ground (Noever et al., 2020; St¨ockl, 2021; Toshniwal et al., 2022).\nChess Transformer (Noever et al., 2020) fine-tunes GPT-2 to generate plausible strategies and learns\ncomplex gameplay. Recent works (Taesiri et al., 2022; 2024) formulate the bug detection problem\nas a question-answering task and leverage the zero-shot capabilities of LLMs for video game bug\ndetection. R2-PLAY (Xu et al., 2024) constructs a multimodal game instruction tuning dataset to\nfacilitate the “read-to-play” capability of LLMs. PuzzleVQA (Chia et al., 2024) demonstrates that\nexisting MLLMs exhibit substantial challenges when solving puzzles that demand visual perception,\ninductive reasoning, and deductive reasoning. Beyond the benchmark setting, we additionally develop\nan interactive environment to assess the ability of multimodal models to perform spatial reasoning\nand multi-step inference based on visual details.\n3\nTHE ING-VP BENCHMARK\n3.1\nOVERVIEW OF ING-VP\nWe introduce ING-VP benchmark, a new interactive game-based vision planning benchmark designed\nto measure the multi-step reasoning and spatial imagination capabilities of MLLMs. The benchmark\nencompasses 6 distinct settings, 6 games, and 50 levels per game, the core mechanisms are depicted\nin Figure 1. To mitigate data leakage and ensure problem solvability, the majority of our levels\nare algorithmically generated and verified. Representative examples of each game are illustrated in\nFigure 2.\nING-VP features 6 games that are conceptually simple yet cognitively challenging: Sokoban, Maze,\nSudoku, 8-queens, Tower of Hanoi, and 15-puzzle. The simplicity lies in the easily comprehensible\nrules and the ability to encapsulate complete level information within a single image, facilitating\ncomprehensive reasoning. The challenge stems from the requirement for models to precisely capture\ncore visual elements and their spatial relationships, necessitating multi-step reasoning to successfully\ncomplete each level. We meticulously craft 6 reasoning settings, enabling researchers to systematically\nidentify the strengths and limitations of target models through comparative analysis of performance\nacross these settings.\n3.2\nSIX INFERENCE SETTINGS\nOne-step: Image and Text-only Settings In the One-step with Image setting, we provide the\nmodel solely with an image depicting the initial game state and prompt it to generate comprehensive\ninstructions for level completion. The One-step Text-only setting follows an identical approach,\nwith the key distinction being the replacement of the image input with its corresponding textual\nrepresentation.\nMulti-step: Image and Text-only Settings (without History) In the Multi-step with Image setting,\nwe provide the model with an image of the current game state at each inference round. After the\nmodel outputs a single-step instruction, this instruction is fed into the game as input, causing the game\nstate to change and generate a new image. This new image then serves as the model’s input for the\nnext step. The Multi-step Text-only setting follows the same process, but uses textual representations\nas the model’s input.\nMulti-step: Image and Text-only Settings (with History) The key distinction in these settings is the\ninclusion of the model’s historical outputs as part of the prompt in each interaction. Additionally, for\n4\nPreprint\nSokoban, Sudoku, and N-queens, we add an undo option, allowing the model to freely revert to any\nprevious state. This enhancement applies to both the Image and Text-only variants of the Multi-step\nsetting.\n3.3\nGAME SELECTION\nWe chose six games that are widely recognized, have straightforward rules, and operate in a determin-\nistic environment, making them ideal representatives for our study. In a deterministic environment,\nthe outcome of every action taken by an agent is predictable and certain. Such an environment can\nbe formally defined using a Markov Decision Process (MDP). The model employs a strategy π to\ndetermine the next action at based on the current state st and all previous actions a0:t−1, represented\nas:\nat = π(st, a0:t−1)\n(1)\nThe planning process of MLLMs can be expressed as:\nS′ = π(S, A, G, n)\n(2)\nWhere S′ is the future sequence of states, which terminates upon achieving the goal G or exhausting\nthe available moves n; S is the current sequence of states; A represents the current sequence of\nactions.\n3.4\nDATA COLLECTION\nSokoban. It involves pushing crates onto designated storage locations within a warehouse maze. We\nselect 50 levels from the Sasquatch dataset 1. To mitigate difficulty and prevent data leakage, we\nemploy the A-star algorithm to constrain each level to a maximum of 8 steps for completion.\nMaze. The Maze game challenges players to navigate from a starting point to a target through a\nnetwork of paths. We employ a Depth-First Search (DFS) algorithm to automatically generate 50\nsolvable levels, each with an 11x11 grid size. We also constrain the solution length to a maximum of\n8 steps.\n8-Queens. The 8-Queens puzzle challenges people to place eight queens on an 8x8 chessboard such\nthat no two queens threaten each other. N-Queens is a special game due to its standard formulation:\nmodels could potentially solve it without visual input, relying solely on memorized patterns from\ntraining data. To ensure that visual reasoning is essential, we modify the puzzle by manually placing\nthe first queen in a different position for each level. The image presented to the MLLMs shows this\ninitial configuration, requiring them to reason from this starting point to complete the puzzle.\nSudoku. Sudoku is a logic-based number placement puzzle that requires filling a 9x9 grid such that\neach row, column, and 3x3 subgrid contains all digits from 1 to 9 without repetition. A well-formed\nSudoku puzzle with a unique solution requires a minimum of 17 initial clues. For our benchmark,\nwe curate a set of 50 puzzles with each puzzle contain 71 clues from a Kaggle dataset 2 , ensuring\neach puzzle meets this criterion. We then manually generate corresponding images for each level to\nmaintain consistency with our benchmark’s visual reasoning focus.\nHanoi The Tower of Hanoi is a classic mathematical puzzle that involves transferring a stack of disks\nof varying diameters from one rod to another, adhering to the constraint that a larger disk must never\nbe placed atop a smaller one. In our implementation, each problem instance consists of four rods and\nfive disks, with an optimal solution requiring a minimum of 8 moves.\n15-Puzzle It’s a classical sliding tile puzzle comprising a 4x4 grid with 15 numbered tiles and one\nvacant space. The objective is to rearrange the tiles into numerical order through a series of sliding\nmovements. In our implementation, we employ the Breadth-First Search (BFS) algorithm to explore\nsolution paths, constraining the search depth to 8 moves as previous games.\n1http:\/\/www.abelmartin.com\/rj\/sokobanJS\/Skinner\/David%20W.%20Skinner%\n20-%20Sokoban.htm\n2https:\/\/www.kaggle.com\/datasets\/informoney\/4-million-sudoku-puzzles-easytohard\n5\nPreprint\nImage-text\nText-only\nMulti-step\nOne-step\nMulti-step\nOne-step\nModel\nMetric\nw\/o history\nw\/ history\nw\/o history\nw\/ history\nOverall\nClosed Source Model\nAcc.\n0.30\n0.30\n7.00\n2.30\n2.30\n8.00\n3.37\nComp.\n3.90\n4.30\n21.90\n4.90\n5.20\n16.80\n9.50\nClaude-3.5 Sonnet\nEff.\n26.90\n23.10\n48.40\n17.60\n18.50\n42.00\n29.42\nAcc.\n3.30\n2.00\n0.30\n3.30\n3.30\n4.30\n2.75\nComp.\n6.70\n5.20\n12.90\n5.80\n5.40\n13.80\n8.30\nGPT-4o\nEff.\n19.20\n14.20\n33.70\n18.70\n18.30\n47.80\n25.32\nAcc.\n1.00\n0.30\n2.70\n5.70\n4.30\n2.30\n2.72\nComp.\n5.90\n3.80\n9.60\n8.20\n6.50\n8.50\n7.08\nGemini-1.5-Pro\nEff.\n34.70\n27.80\n42.80\n19.50\n18.50\n37.70\n30.17\nAcc.\n0.70\n0.30\n0.00\n2.00\n2.30\n1.00\n1.05\nComp.\n3.40\n3.40\n6.60\n5.20\n5.90\n8.90\n5.57\nGPT-4o mini\nEff.\n13.20\n8.20\n35.20\n19.50\n17.30\n40.10\n22.25\nAcc.\n0.00\n0.00\n1.30\n0.00\n0.30\n0.30\n0.32\nComp.\n2.90\n2.90\n4.30\n2.60\n3.00\n3.40\n3.18\nGPT-4V\nEff.\n8.80\n7.20\n5.50\n16.80\n17.40\n8.50\n10.70\nAcc.\nnull\nnull\nnull\n2.30\n2.30\n1.00\n1.87\nComp.\nnull\nnull\nnull\n4.80\n4.80\n9.10\n6.23\nGPT-4 Turbo\nEff.\nnull\nnull\nnull\n12.20\n12.30\n41.00\n21.83\nAcc.\nnull\nnull\nnull\n2.30\n2.30\n1.00\n1.87\nComp.\nnull\nnull\nnull\n4.80\n4.80\n10.70\n5.07\nClaude-3 Opus\nEff.\nnull\nnull\nnull\n12.40\n12.30\n40.80\n21.83\nOpen Source Model\nAcc.\n2.67\n2.33\n3.00\n2.33\n1.67\n3.00\n2.50\nComp.\n9.07\n6.28\n8.30\n8.32\n8.03\n5.88\n7.65\nInternVL2-Llama3-76B\nEff.\n17.55\n15.13\n36.18\n21.13\n29.30\n32.95\n25.58\nAcc.\n2.33\n1.33\n1.67\n1.67\n2.00\n2.33\n1.89\nComp.\n4.80\n5.22\n5.65\n5.25\n5.27\n5.22\n5.23\nInternvl2-26B\nEff.\n10.58\n9.22\n11.93\n10.22\n9.27\n16.72\n11.32\nAcc.\n1.67\n1.67\n2.67\n1.00\n2.00\n1.67\n1.78\nComp.\n5.68\n5.43\n7.87\n5.03\n4.08\n8.08\n6.03\nInternvl2-40B\nEff.\n18.37\n12.98\n22.22\n15.33\n15.22\n34.16\n18.82\nAcc.\n1.33\n0.67\n2.00\n1.67\n1.33\n2.00\n1.50\nComp.\n5.90\n5.68\n6.58\n5.68\n5.02\n7.63\n6.08\nCogvlm2-19B\nEff.\n15.75\n16.45\n27.12\n13.75\n12.85\n31.37\n19.55\nAcc.\n1.00\n0.33\n0.33\n1.33\n0.67\n1.67\n0.89\nComp.\n2.60\n2.58\n3.33\n2.63\n2.50\n3.83\n2.91\nInternvl2-8B\nEff.\n5.90\n5.27\n4.97\n3.05\n4.27\n6.03\n4.91\nAcc.\n0.67\n0.33\n0.00\n0.33\n0.33\n0.67\n0.39\nComp.\n6.30\n6.30\n4.57\n5.80\n6.00\n4.18\n5.53\nInternvl-Chat-v1.5\nEff.\n14.90\n14.22\n25.68\n11.70\n10.87\n27.27\n17.44\nAcc.\n0.67\n0.33\n1.00\n0.33\n0.00\n0.00\n0.39\nComp.\n3.47\n2.72\n3.65\n2.68\n4.18\n3.92\n3.44\ndeepseek-VL\nEff.\n11.80\n11.22\n16.40\n8.38\n9.57\n15.90\n12.21\nAcc.\n0.33\n0\n0\n0.67\n0.33\n0\n0.22\nComp.\n3.78\n3.33\n4.17\n3.62\n2.68\n4.22\n3.63\nMiniCPM-V2.6\nEff.\n11.18\n10.62\n17.73\n10.08\n6.37\n21.88\n12.98\nTable 1: Main results for the best-performing MLLMs (LLMs).\n4\nEXPERIMENTS\nWe conduct a comprehensive evaluation of both open-source and closed-source MLLMs, employ a\nzero-shot setting to faithfully emulate the human puzzle-solving process, given the unique nature of\nour tasks. A uniform set of prompts was applied across all models. The complete set of 36 prompts is\npresented in the Appendix B.\n4.1\nBASELINES\nMLLMs. We consider a comprehensive suite of mainstream large multimodal models. Closed-source\nmodels include GPT-4o, GPT-4o Mini, GPT-4v, GPT-4 Turbo, Claude-3.5 Sonnet, Claude-3 Opus,\n6\nPreprint\nand Gemini-1.5 Pro. Open-source models consist of CogVLM2-19B, DeepSeek-VL, Internvl-Chat-\nv1.5, Internvl2-8B, Internvl2-26B, Internvl2-40B, InternVL2-Llama3-76B, and MiniCPM-V2.6. We\nutilize each model’s official API for closed-source systems or the publicly available checkpoint for\nopen-source implementations, More information of these models can be found in the Appendix A.\nEvaluation. We present a systematic interactive environment for evaluating all MLLMs, where\nmodels interact with the game environment until either completing the task or exhausting the allotted\nsteps. We constrain the model’s output action instructions to JSON format through prompts and\nextract them using regular expressions. The correctly extracted instructions are then used as input\nfor the game environment. After the game state changes, the new state is fed back to the model\nfor the next round of inference. We employ three metrics: accuracy, completion degree, and action\nefficiency. (1) Accuracy is our main metric, it measures whether the model can complete the task\nwithin the specified number of steps. (2) Completion degree is determined by the final state of the\ngame environment after interaction with the model. The closer the final state is to the cleared state,\nthe higher the score; if it deviates, the score decreases accordingly. (3) Action efficiency represents\nwhether each instruction output by the model effectuates a change in the game state. The computation\nmethod for action efficiency is as follows:\nAction Efficiency =\nPn\ni=1\n# of efficient actions for level i\n# of total actions for level i\nn\n4.2\nMAIN RESULTS\nIn this section, we examine the spatial reasoning and planning abilities of current MLLMs using the\nING-VP benchmark. The results are presented in Table 1, please see the Appendix C for the complete\nresults.Our key observations are as follows:\n55.2%\n2.9%\n41.9%\na. Image-text\n58.0%\n42.0%\nb. Text-only\nHanoi\nMaze\n15-puzzle\n8-queens\nSokoban\nSudoku\n0.0\n20.0\n40.0\na\na\na\na\na\na\nb\nb\nb\nb\nb\nb\nError Types\nPerceptual Errors\nTextual Understanding Errors\nPlanning Errors\nFigure 3: Error distribution over Claude-3.5 Son-\nnet’s 555 errors across different tasks and settings.\nThe ING-VP benchmark poses a substantial\nchallenge to current MLLMs: Even the most\nadvanced model, Claude-3.5 Sonnet, achieves\nan accuracy of only 3.37%. In contrast, an av-\nerage human can easily complete all of these\ntasks (8-queens is an exception), highlighting a\nsignificant gap between model performance and\nhuman capabilities on the ING-VP benchmark.\nPerformance disparity between open-source\nand closed-source models persists: While the\nperformance of closed-source models on ING-\nVP is far from satisfactory, they still outperform\nthe open-source models. The best-performing\nopen-source model, InternVL2-Llama3-76B,\nachieves an accuracy of 2.50%, which remains\nlower than Claude-3.5 Sonnet, GPT-4o and\nGemini-1.5 Pro.\nFor MLLMs, the greatest challenge in per-\nception is understanding location informa-\ntion. According to our observations of the infer-\nence results, the most advanced models, such as\nClaude-3.5 Sonnet and GPT-4o, can generally\nidentify the elements present and even count the\nquantity of each in the Sokoban game. However, they struggle to accurately determine precise\nlocation information, leading to very low inference accuracy and degree of task completion.\nMerely breaking down the steps is unhelpful and may even be counterproductive. In text-only\ntasks, Claude-3.5 Sonnet and GPT-4o achieve accuracy rates of 2.30% and 3.30%, respectively, in\nthe multi-step setting, which are lower than their 8.00% and 4.30% accuracy in the one-step setting.\nFor the ING-VP benchmark, thinking step by step does not work and even has a negative effect. We\n7\nPreprint\nbelieve that MLLMs rely heavily on pattern matching based on prior training data, generating outputs\nfrom similar inputs rather than engaging in actual planning.\n4.3\nFINE-GRAINED ANALYSIS\n0\n50\nAcc.\n0\n50\n0\n50\nComp.\n0\n50\n4 step\n8 step\n12 step\n16 step\nClaude-3.5 Sonnet\n25\n50\nEff.\n4 step\n8 step\n12 step\n16 step\nGPT-4o\n25\n50\nImage-text Multi-step w\/o history\nImage-text Multi-step w\/ history\nImage-text One-step\nText-only Multi-step w\/o history\nText-only Multi-step w\/ history\nText-only One-step\nOverall\nFigure 4: Maze level accuracy of Claude-3.5 Son-\nnet and GPT-4o across 4 difficulty levels.\nIn this section, We conduct a comprehensive\nrange of analyses to explore the generative capa-\nbilities of MLLMs in a broader context, while\nalso dissecting the nuanced output tendencies\nof current models. We hope our results can pro-\nvides valuable insights that can inform future\nmodel design and training strategies.\nError Analysis. We collate and analyze 555\nerrors (image-text: 279, text-only: 276) made\nby Claude-3.5 Sonnet in one-step setting, as il-\nlustrated in Figure 3. It is important to note that\nwhile we categorize each case under distinct\nerror types, in many instances the model exhib-\nited errors in both comprehension and reasoning.\nOur classification follows contextual cues: when\nthe model provided invalid instructions from the\noutset, we labele it as an understanding error.\nConversely, if the model deviated from the cor-\nrect solution at an intermediate step, we classify it as a reasoning error. Below, we summarize key\nobservations based on these error types:\n• Perceptual Errors (55.2%\/–%): These errors occur exclusively in the image-text setting.\nWhile current models are generally able to recognize overall attributes of an image—such\nas identifying the game genre and its components, their ability to accurately interpret fine\ndetails, including the specific size and precise location of each element, remains limited\n(e.g., see Figure 7. This perceptual limitation represents a major contributor to the elevated\nerror rates in this setting.\n• Textual Understanding Errors (2.9%\/58.0%): Textual understanding errors manifest in\ntwo main forms: a misinterpretation of specific prompts or an inability to correctly parse\ndata structures or character matrices used to represent game levels in the text-only setting\n(as shown in Figure 8). These errors indicate that the model struggles to generalize its\nunderstanding when presented with text structures not commonly encountered in its training\ndata.\n• Planning Errors (41.9%\/42.0%): Planning errors constitute another major issue for Claude-\n3.5 Sonnet. In these cases, the model initially provides plausible steps but eventually fails\ndue to its inability to correctly track or judge the game state after several steps (see Figure 9).\nThis suggests a breakdown in maintaining consistent reasoning over multi-step processes.\n• Other Errors: During error analysis, we observe that Claude-3.5 Sonnet and GPT-4o never\nrefused to answer queries, and all responses were accurately extracted. However, models\nsuch as GPT-4V displayed issues like refusal to respond or failure to adhere to the required\nresponse format, which hindered our ability to retrieve the outputs.\nPlanning Capacity Analysis. We select the game where models performed best—Maze—and\nintroduced three additional difficulty levels: 4 steps, 12 steps, and 16 steps, by adjusting only the\nnumber of moves required to complete the level, while maintaining the same level structure. This\nallowed us to closely examine the planning capabilities of the most advanced MLLMs, Claude-3.5\nSonnet and GPT-4o, as shown in FIgure 4. Our findings showed a significant decline in both accuracy\nand completion degree as the number of required steps increased. However, action efficiency, which\nemphasizes perception and judgment of the current state, was not notably affected, since modifying\nthe step count without altering the overall layout had little impact on this metric.\nComparative Analysis.\nWe compare the results across different metrics, settings, and models,\naiming to highlight the characteristics of current MLLMs.\n8\nPreprint\nFigure 5: An example showcasing Claude 3.5-Sonnet with a fixed output paradigm.\n• Results differ across metrics. Of the three metrics provided by ING-VP, accuracy—being\nthe most stringent—typically yields the lowest scores. The primary reason action efficiency\nis often significantly higher than both completion rate and accuracy is that models frequently\ngenerate instructions that alter the game state, but these changes have minimal impact on\nsuccessfully completing the level. A notable example is Gemini-1.5 Pro, which achieves an\naverage action efficiency of 76.52% on the 15-puzzle, yet only 0.67% and 3.42% in accuracy\nand completion rate, respectively.\n• Image-text vs. Text-only. Comparing the performance of each model in the image-text and\ntext-only settings, we found that most test subjects performed better in the text-only setting.\nThis highlights that limitations in image comprehension remain a key factor constraining\nthe performance of MLLMs.\n• Multi-step vs. One-step. According to the results in Table 3, for most models, multi-step\nsetting improves accuracy compared to one-step. However, there are exceptions, such as\nClaude-3.5 Sonnet. We compare the output of Claude-3.5 Sonnet and GPT-4o and find that,\ndespite we set the same parameters for closed-source models, Claude-3.5 Sonnet’s sampling\nstrategy is more fixed than GPT-4o’s. As a result, when the model produces an invalid\naction in a certain state, it tends to repeatedly generate the same action until all attempts are\nexhausted. GPT-4o, being more flexible, is better at generating diverse responses. Therefore,\nalthough Claude-3.5 Sonnet performs better than GPT-4o in one-step tasks, the opposite is\ntrue for multi-step tasks. One example is shown in Figure 5.\n• With-history vs. Without-history. In our tasks, incorporating the model’s historical output\nas the input for subsequent rounds did not lead to improved performance. Additionally, we\nintroduce an undo option for Sokoban, Sudoku, and N-Queens in the with-history setting.\nInterestingly, despite the models frequently reaching a state where undoing moves was\nnecessary to complete the level, almost none utilized this feature. This suggests that the\nmodels struggle with processing precise positional information and are unable to accurately\nassess whether the current state is solvable.\n5\nTWO THINKING ABOUT PLANNING\nA holistic approach may outperform a divide-and-conquer strategy. When humans are tasked\nwith completing a planning problem, whether in a single or multi-step process, it typically involves\nthree key phases: understanding the goal, devising a plan, and breaking down the steps. Large\n9\nPreprint\nmodels should operate similarly, yet when presented with the same game level, their outputs differ\nsignificantly between one-step and multi-step settings, as highlighted in Table 1. Notably, even the\ninitial steps diverge between the two approaches. To explore the planning capabilities of the model\nfurther, we employ two methods to adjust the multi-step output:\n• Step-wise Best of N (BoN): The model generates ten candidate responses at each step, with\nthe most frequent answer selected as the final output.\n• Forced Planning: The model is required to complete its entire plan before producing a final\nanswer, akin to the one-step setting.\nFigure 6: An example of results for the Claude-3.5 Sonnet in four settings.\nFIgure 6 illustrates an example of these methods in action, despite these adjustments, the multi-step\napproach failed to match the performance of the one-step setting. This suggests that, for the large\nmodels, even when given identical image, one-step and multi-step tasks are fundamentally different,\nwith the former better eliciting the model’s planning capabilities.\nSmall changes in the prompt phrasing can substantially influence the model’s planning effec-\ntiveness. A thorough comparison of single-step and multi-step outputs reveals not only differences\nbut also distinct tendencies. For instance, in Maze and Sokoban games, Claude-3.5 Sonnet favors ”U\n(Up)” and ”D (Down)” in the one-step mode, whereas it prefers ”L (Left)” and ”R (Right)” in the\nmulti-step mode. Given that most of the prompt wording remains consistent between the two settings,\nour results indicate that subtle variations can profoundly affect the model’s response distribution. We\nleave more detailed experiments as future work.\n6\nCONCLUSION\nIn this work, we introduce ING-VP, an interactive game-based vision planning benchmark designed\nto evaluate the spatial imagination and planning capabilities of MLLMs. Our experimental results\nreveal that even the most advanced MLLMs struggle to achieve satisfactory performance on game\ntasks that humans find trivial. This underperformance stems from multiple factors: existing models\noften fail to generate accurate perceptions of images, and they face even greater challenges in making\ninferences and plans based on their understanding. We believe that ING-VP is of noteworthy to\nthe community’s deeper understanding of MLLMs, and can also advance MLLMs’ capabilities in\ncomprehension and planning within visual contexts.\nLIMITATIONS\nDespite its strengths, ING-VP has certain limitations. We deliberately omit difficulty grading settings.\nIncluding simpler levels would significantly increase the likelihood of models completing tasks by\nchance after sufficient steps, potentially compromising the reliability of our results. Conversely,\nincorporating more challenging levels would yield little insight, given that MLLMs already struggle\nwith current difficulty levels, and could negatively impact inference efficiency. Furthermore, ING-VP\ndoes not exhaustively cover all possible game types. Instead, we focus on selecting well-known\nand representative games to ensure relevance and broad applicability. Finally, to address efficiency\nconcerns, we do not use images of previous states as input in the multi-step with history setting.\nThese considerations provide clear directions for future enhancements to our benchmark.\n10\nPreprint\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nNader Akoury, Qian Yang, and Mohit Iyyer. A framework for exploring player perceptions of llm-\ngenerated dialogue in commercial video games. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 2295–2311, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin\nJohnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler,\nTimothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald\nBarham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan\nDoherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha\nGoel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka,\nBecca Roelofs, Ana¨ıs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran\nKazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of\nhighly capable multimodal models. CoRR, abs\/2312.11805, 2023a.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023b.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international\nconference on computer vision, pp. 2425–2433, 2015.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253–279, 2013.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\nLu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and explor-\ning emergent behaviors. In The Twelfth International Conference on Learning Representations,\n2023.\nYew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puz-\nzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual\npatterns. arXiv preprint arXiv:2403.13315, 2024.\nWenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki,\nMohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms.\narXiv preprint, 2024.\nShiying Ding, Xinyi Chen, Yan Fang, Wenrui Liu, Yiwu Qiu, and Chunlei Chai. Designgpt: Multi-\nagent collaboration in design. In 2023 16th International Symposium on Computational Intelligence\nand Design (ISCID), pp. 204–208. IEEE, 2023.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng,\nSong-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction.\narXiv preprint arXiv:2309.09971, 2023.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\n11\nPreprint\nSihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu.\nA survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022.\nChuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, and Jianlong Fu.\nAlphablock: Embodied finetuning for vision-language reasoning in robot manipulation. arXiv\npreprint arXiv:2305.18898, 2023a.\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru\nSong, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual\ntokenization. arXiv preprint arXiv:2309.04669, 2023b.\nKushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In\nProceedings of the IEEE international conference on computer vision, pp. 1965–1973, 2017.\nBohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan.\nSeed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pp. 13299–13308, 2024.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Com-\nmunicative agents for” mind” exploration of large language model society. Advances in Neural\nInformation Processing Systems, 36:51991–52008, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning, pp. 19730–19742. PMLR, 2023b.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740–755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36, 2024.\nJijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, and Yu Wang. Llm-powered\nhierarchical language agent for real-time human-ai coordination. arXiv preprint arXiv:2312.15224,\n2023a.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\narXiv preprint arXiv:2307.06281, 2023b.\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,\nZhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.\nDeepseek-vl: Towards real-world vision-language understanding. CoRR, abs\/2403.05525, 2024.\nDavid Noever, Matt Ciolino, and Josh Kalin. The chess transformer: Mastering play using generative\nlanguage models. arXiv preprint arXiv:2008.04057, 2020.\nOpenAI. Gpt-4o system card. CoRR, 2024.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Social simulacra: Creating populated prototypes for social computing systems. In\nProceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pp.\n1–18, 2022.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and technology, pp. 1–22, 2023.\n12\nPreprint\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE international conference on computer\nvision, pp. 2641–2649, 2015.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6, 2023.\nAndreas St¨ockl. Watching a language model learning chess. In Proceedings of the International\nConference on Recent Advances in Natural Language Processing (RANLP 2021), pp. 1369–1379,\n2021.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context\nlearners. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npp. 14398–14409, 2024.\nPenny Sweetser. Large language models and video games: A preliminary scoping review. In\nProceedings of the 6th ACM Conference on Conversational User Interfaces, pp. 1–8, 2024.\nGabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timoth´ee Lacroix, Zeming\nLin, Florian Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on\nreal-time strategy games. arXiv preprint arXiv:1611.00625, 2016.\nMohammad Reza Taesiri, Finlay Macklon, Yihe Wang, Hengshuo Shen, and Cor-Paul Bezemer.\nLarge language models are pretty good zero-shot video game bug detectors. arXiv preprint\narXiv:2210.02506, 2022.\nMohammad Reza Taesiri, Tianjun Feng, Cor-Paul Bezemer, and Anh Nguyen. Glitchbench: Can\nlarge multimodal models detect video game glitches? In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22444–22455, 2024.\nShubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as a testbed for\nlanguage model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 11385–11393, 2022.\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei\nWang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through\nrecursive contemplation. arXiv preprint arXiv:2310.01320, 2023a.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,\nHongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting,\nand enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746,\n2023b.\nWenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei.\nVisualization-of-thought elicits spatial reasoning in large language models.\narXiv preprint\narXiv:2404.03622, 2024.\nYue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as\nintelligent agents. arXiv preprint arXiv:2310.01557, 2023.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models. arXiv preprint arXiv:2306.09265, 2023.\nXinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and B¨orje F Karlsson.\nA survey on game playing agents and large models: Methods, applications, and challenges. arXiv\npreprint arXiv:2403.10249, 2024.\nZhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang,\nZhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multi-modal instruction-tuning\ndataset, framework, and benchmark. Advances in Neural Information Processing Systems, 36,\n2024.\n13\nPreprint\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,\nBoyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for\nexpert AGI. CoRR, abs\/2311.16502, 2023.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal under-\nstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 9556–9567, 2024.\nGe Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang\nCheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi\nLi, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang,\nWenhu Chen, and Jie Fu. CMMMU: A chinese massive multi-discipline multimodal understanding\nbenchmark. CoRR, abs\/2401.11944, 2024a.\nJesse Zhang, Karl Pertsch, Jiahui Zhang, and Joseph J Lim. Sprint: Scalable policy pre-training\nvia language instruction relabeling. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 9168–9175. IEEE, 2024b.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and\nJi-Rong Wen. A survey of large language models. CoRR, abs\/2303.18223, 2023.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian\nZhang, Jing Chen, Ruipu Wu, Shuai Wang, et al.\nAgents: An open-source framework for\nautonomous language agents. arXiv preprint arXiv:2309.07870, 2023.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenvironments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n14\nPreprint\nA\nMODEL LIST\nList of all models involved in the ING-VP.\nOrganization\nModel\nAccess\nClosed Source Model\nOpenAI\nGPT-4o\nhttps:\/\/openai.com\/index\/hello-gpt-4o\/\nGPT-4o mini\nhttps:\/\/openai.com\/index\/gpt-4o-mini-advancing-cost-efficient-intelligence\/\nGPT-4v\nhttps:\/\/openai.com\/index\/gpt-4v-system-card\/\nGPT-4 Turbo\nhttps:\/\/platform.openai.com\/docs\/models\/gpt-4-turbo-and-gpt-4\nAnthropic\nClaude-3.5 Sonnet\nhttps:\/\/www.anthropic.com\/news\/claude-3-5-sonnet\nClaude-3 Opus\nhttps:\/\/www.anthropic.com\/news\/claude-3-family\nGoogle Deepmind\nGemini-1.5 Pro\nhttps:\/\/deepmind.google\/technologies\/gemini\/pro\/\nOpen Source Model\nShanghai AI Laboratory\nInternVL2-Llama3-76B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-Llama3-76B\nInternVL2-40B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-40B\nInternVL2-26B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-26B\nInternVL2-8B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-8B\nInternVL-Chat-V1-5\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL-Chat-V1-5\nZhipu AI\nCogVLM2-Llama3-chat-19B\nhttps:\/\/github.com\/THUDM\/CogVLM2\nDeepSeek-AI\nDeepSeek-VL-7B-chat\nhttps:\/\/github.com\/deepseek-ai\/DeepSeek-VL\nModelBest Inc\nMiniCPM-V 2.6\nhttps:\/\/github.com\/OpenBMB\/MiniCPM-V\nTable 2: List of all models involved in the ING-VP.\nB\nPROMPTS\nThe following is the comprehensive list of 36 prompts utilized in our experiments.\nB.1\nMULTI-STEP WITH IMAGE WITHOUT HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an image of a level of the Tower of\nHanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\nPlease use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means move\nthe disk on rod-x to rod-y\nInstruction:\nPlease output only one step and your output must meet required format {”output”: ”{rod-\nx}{rod-y}”} and not output anything else:\n15\nPreprint\nMaze\nSystem:\nYou are a player of Maze game. And you will be given an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. Red area: your current position.\n2. Green area: destination.\n3. Black area: wall, unable to pass.\n4. White area: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\nInstruction:\nPlease output only one step, and your output must be one of the following: {”output”: ”L”}\nor {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”} and not output anything else:\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given an image of a level of the n-puzzle\ngame.\nPlease finish the n-puzzle based on the image provided.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\nInstruction:\nPlease output only one step and your output must meet required format {”output”: number}.\nPlease do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given an image of a level of the n-queens\ngame.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\n16\nPreprint\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n5. If your chess piece violates the three rules, it will be ignored.\nInstruction:\nPlease output only one step and your output must meet required format {”output”: [row,\ncol]}, and not output anything else:\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given an image of a level of the Sokoban\ngame.\nYour task is to complete this level by outputting movement instructions based on this image\none step at a time.\nObjective: Move all boxes onto the designated storage locations (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\n4. goal: push all the boxes onto the docks\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\nInstruction:\nPlease output only one step, and your output must be one of the following: ”output”: ”L” or\n”output”: ”R” or ”output”: ”U” or ”output”: ”D” and not output anything else:\n17\nPreprint\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given an image of a level of the Sudoku\ngame.\nPlease finish the sudoku puzzle based on the image provided, one step at a time.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nOutput Instructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number}}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nInstruction:\nPlease output only one step and your output must meet required format {”output”:\n{”{row}{column}”: {number}}}, and not output anything else:\nB.2\nMULTI-STEP TEXT-ONLY WITHOUT HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an dictionary representation of a\nlevel of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D\n2. And 5 disks: a, b, c, d, e; for size: a ¿ b ¿ c ¿ d ¿ e\n3. Your task is to move all the disks to rod ”D”\n4. Only one disk can be moved at a time\n5. Only the top disk can be moved\n6. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\nPlease use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means move\nthe disk on rod-x to rod-y\nInstruction:\nDictionary representation:\n{text-representation-path}\nPlease output only one step based on the given rules and dictionary representation, and\nyour output must meet required format {”output”: ”{rod-x}{rod-y}”}. Please do not output\nanything else.\n18\nPreprint\nMaze\nSystem:\nYou are a player of Maze game. And you will be given a text matrix of a level of the Maze\ngame.\nYour task is to move from your current position through the floor to the destination.\nInformation of text matrix:\n1. ’S’: your current position.\n2. ’X’: destination.\n3. ’+’: wall, unable to pass.\n4. ’ ’: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\nInstruction:\nText matrix:\n{text-representation-path}\nPlease output only one step based on the given rules and text matrix, and your output must be\none of the following: {”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”:\n”D”}. Please do not output anything else.\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given a list representation of a level of\nthe n-puzzle game.\nPlease finish the n-puzzle based on the list representation provided.\nIllustration of given list representation:\n1. The main list represents the board of size 4 * 4;\n2. The main list contains 4 sublist, each sublist represents a row, and contains 4\nelements;\n3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty\nspace is represented as 0;\n4. The goal is to rearrange the elements to [[1,2,3,4], [5,6,7,8], [9,10,11,12],\n[13,14,15,0]]\n5. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\nInstruction:\nList representation:\n{text-representation-path}\nPlease output only one step based on given list representation and your output must meet\nrequired format {”output”: number}. Please do not output anything else.\n19\nPreprint\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given a coordinate of the existing queens\nof a level of the n-queens game.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen is already given, so do not include it in your answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n5. If your chess piece violates the three rules, it will be ignored.\nInstruction:\nThe coordinate of the existing queens (including the first queen):\n{text-representation-path}\n1. first number: row index, range from 0 to 7\n2. second number: column index, range from 0 to 7\nPlease output only one step based on given coordinate and your output must meet required\nformat {”output”: [row, col]}. And do not output anything else.\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given a text matrix of a level of the\nSokoban game.\nYour task is to complete this level by outputting movement instructions based on the given\ntext matrix one step at a time.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration of given text matrix:\n1. ’.’: dock\n2. ’$’: box\n3. ’*’: box on the dock (can also be pushed)\n20\nPreprint\n4. ’@’: worker (or agent)\n5. ’+’: worker on the dock\n6. ’ ’: floor\n7. ’#’: wall\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\nInstruction:\nText matrix:\n{text-representation-path}\nPlease output only one step based on text matrix, and your output must be one of the following:\n{”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”}. And do not\noutput anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given a number string of a level of the\nSudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nIllustration of the given number string:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nInstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: ”output”: ”rowcolumn”: number.\n3. The range of row and column are 0-8, the range of number is 1-9.\nInstruction:\nNumber string:\n{text-representation-path}\nPlease output only one step based on given number string and your output must meet required\nformat {”output”: {”{row}{column}”: {number}}}. And do not output anything else:\n21\nPreprint\nB.3\nMULTI-STEP WITH IMAGE WITH HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an image of a level of the Tower of\nHanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means\nmove the disk on rod-x to rod-y,\n2. This is a multi-turn conversation. The conversation history provided below may be\nhelpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step and your output must meet required format {”output”: ”{rod-\nx}{rod-y}”} and not output anything else:\nMaze\nSystem:\nYou are a player of Maze game. And you will be given an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. Red area: your current position.\n2. Green area: destination.\n3. Black area: wall, unable to pass.\n4. White area: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\n22\nPreprint\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step, and your output must be one of the following: {”output”: ”L”}\nor {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”} and not output anything else:\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given an image of a level of the n-puzzle\ngame.\nPlease finish the n-puzzle based on the image provided.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\n3. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step and your output must meet required format {”output”: number}.\nPlease do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given an image of a level of the n-queens\ngame.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n23\nPreprint\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}.\n5. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n6. If your chess piece violates the three rules, it will be ignored.\n7. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\nconversation-history-path\nPlease output only one step and your output must meet required format {”output”: [row,\ncol]}, and not output anything else:\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given an image of a level of the Sokoban\ngame.\nYour task is to complete this level by outputting movement instructions based on this image\none step at a time.\nObjective: Move all boxes onto the designated storage locations (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\n4. goal: push all the boxes onto the docks\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n24\nPreprint\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step, and your output must be one of the following: ”output”: ”L” or\n”output”: ”R” or ”output”: ”U” or ”output”: ”D” and not output anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given an image of a level of the Sudoku\ngame.\nPlease finish the sudoku puzzle based on the image provided, one step at a time.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nOutput Instructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number}}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\n4. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n5. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nPlease output only one step and your output must meet required format {”output”:\n{”{row}{column}”: {number}}}, and not output anything else:\nB.4\nMULTI-STEP TEXT-ONLY WITH HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an dictionary representation of a\nlevel of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D\n2. And 5 disks: a, b, c, d, e; for size: a ¿ b ¿ c ¿ d ¿ e\n3. Your task is to move all the disks to rod ”D”\n4. Only one disk can be moved at a time\n25\nPreprint\n5. Only the top disk can be moved\n6. At no time should a large disk be placed on top of a small disk.\nInstructions:\n1. Use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means\nmove the disk on rod-x to rod-y\n2. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nDictionary representation:\n{text-representation-path}\nConversation history:\n{conversation-history-path}\nPlease output only one step based on the given rules and dictionary representation, and\nyour output must meet required format {”output”: ”{rod-x}{rod-y}”}. Please do not output\nanything else.\nMaze\nSystem:\nYou are a player of Maze game. And you will be given a text matrix of a level of the Maze\ngame.\nYour task is to move from your current position through the floor to the destination.\nInformation of text matrix:\n1. ’S’: your current position.\n2. ’X’: destination.\n3. ’+’: wall, unable to pass.\n4. ’ ’: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nText matrix:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on the given rules and text matrix, and your output must be\none of the following: {”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”:\n”D”}. Please do not output anything else.\n26\nPreprint\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given a list representation of a level of\nthe n-puzzle game.\nPlease finish the n-puzzle based on the list representation provided.\nIllustration of given list representation:\n1. The main list represents the board of size 4 * 4;\n2. The main list contains 4 sublist, each sublist represents a row, and contains 4\nelements;\n3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty\nspace is represented as 0;\n4. The goal is to rearrange the elements to [[1,2,3,4], [5,6,7,8], [9,10,11,12],\n[13,14,15,0]]\n5. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\n3. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nList representation:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on given list representation and your output must meet\nrequired format {”output”: number}. Please do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given a coordinate of the existing queens\nof a level of the n-queens game.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen is already given, so do not include it in your answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n27\nPreprint\n5. If your chess piece violates the three rules, it will be ignored.\n6. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThe coordinate of the existing queens (including the first queen):\n{text-representation-path}\n1. first number: row index, range from 0 to 7\n2. second number: column index, range from 0 to 7\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on given coordinate and your output must meet required\nformat {”output”: [row, col]}. And do not output anything else.\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given a text matrix of a level of the\nSokoban game.\nYour task is to complete this level by outputting movement instructions based on the given\ntext matrix one step at a time.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration of given text matrix:\n1. ’.’: dock\n2. ’$’: box\n3. ’*’: box on the dock (can also be pushed)\n4. ’@’: worker (or agent)\n5. ’+’: worker on the dock\n6. ’ ’: floor\n7. ’#’: wall\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n28\nPreprint\n3. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nText matrix:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on text matrix, and your output must be one of the following:\n{”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”}. And do not\noutput anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given a number string of a level of the\nSudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nIllustration of the given number string:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nInstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: ”output”: ”rowcolumn”: number.\n3. The range of row and column are 0-8, the range of number is 1-9.\n4. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n5. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nNumber string:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\n29\nPreprint\nPlease output only one step based on given number string and your output must meet required\nformat {”output”: {”{row}{column}”: {number}}}. And do not output anything else:\nB.5\nONE-STEP WITH IMAGE\nHanoi\nThis is an image of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nRules:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nNote:\n1. Use JSON as your output format: {”output”: [”AC”, ”AD”, ...]}, which means move\nthe top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.\nYour answer:\nMaze\nThis is an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. red area: your current position\n2. green area: destination\n3. black area: wall, unable to pass\n4. white area: floor, able to pass\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\n15-puzzle\nThis is an image of a level of the n-puzzle game.\nYour task is to generate a list of numbers to complete the n-puzzle problem.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: [number1, number2, number3, ...]}.\n30\nPreprint\n2. THe number1, number2, ... means if number1 is around the empty space, they will\nswap positions first; after that, if number2 is around the empty space, number2 and\nthe empty space will swap positions too, and so on.\nYour answer:\n8-queens\nThis is an image of a level of the n-queens game.\nYour task is to generate a list of coordinates to complete the n-queens problem on a board\nwhere the first queen is already placed.\nFollow these rules: Each queen must be placed in such a way that no two queens threaten\neach other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nNote:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Your output should be in the JSON format: {”output”: [[row-x1, col-y1], [row-x2,\ncol-y2], ...]}. Each [row-x, col-y] means the coordinate you want to place your\npiece.\n5. If your chess piece violates the three rules, it will be ignored.\nYour answer:\nSokoban\nThis is an image of a level of the Sokoban game.\nYour task is to complete this level by outputting movement instructions based on this image.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\n31\nPreprint\nYour answer:\nSudoku\nThis is an image of a level of the Sudoku game.\nPlease finish the sudoku puzzle based on the image provided.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\ninstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number},\n”{row}{column}”: {number}, ...}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nYour answer:\nB.6\nONE-STEP TEXT-ONLY\nHanoi\nThis is an dictionary representation of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nDictionary representation:\n{text-representation-path}\nRules:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nNote:\n1. Use JSON as your output format: {”output”: [”AC”, ”AD”, ...]}, which means move\nthe top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.\nYour answer:\nMaze\nThis is an dictionary representation of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nDictionary representation:\n{text-representation-path}\nRules:\n1. red area: your current position\n2. green area: destination\n32\nPreprint\n3. black area: wall, unable to pass\n4. white area: floor, able to pass\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\n15-puzzle\nThis is a list representation of a level of the n-puzzle game.\nPlease finish the n-puzzle based on the list representation provi\nded.\nList representation:\n{text-representation-path}\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: [number1, number2, number3, ...]}.\n2. THe number1, number2, ... means if number1 is around the empty space, they will\nswap positions first; after that, if number2 is around the empty space, number2 and\nthe empty space will swap positions too, and so on.\nYour answer:\n8-queens\nThis is a level of the n-queens game.\nYour task is to generate coordinates to complete the n-queens problem on a board where the\nfirst queen is already placed.\nThe coordinate of the first queen:\n{text-representation-path}\nFollow these rules: Each queen must be placed in such a way that no two queens threaten\neach other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nNote:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n33\nPreprint\n4. Your output should be in the JSON format: {”output”: [[row-x1, col-y1], [row-x2,\ncol-y2], ...]}. Each [row-x, col-y] means the coordinate you want to place your\npiece.\n5. If your chess piece violates the three rules, it will be ignored.\nYour answer:\nSokoban\nThis is a text matrix of a level of the Sokoban game.\nYour task is to complete this level by outputting movement instructions based on this text\nmatrix.\nText matrix:\n{text-representation-path}\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\nSudoku\nThis is a number string of a level of the Sudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nNumber string:\n{text-representation-path}\nIllustration:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n34\nPreprint\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\ninstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number},\n”{row}{column}”: {number}, ...}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nYour answer:\nC\nDETAILED RESULTS\nModel\nSetting\nMaze Sokoban N-queens N-puzzle Hanoi Sudoku Overall\nClosed Source Model\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 27.80\n9.50\n0.00\n2.50\n0.50\n0.00\n6.70\nMulti-step\nw\/o history\nEff.\n5.60\n47.00\n3.30\n58.10\n0.60\n0.50\n19.20\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 17.20\n9.80\n0.00\n4.50\n0.00\n0.00\n5.20\nMulti-step\nw\/ history\nEff.\n18.60\n26.30\n3.00\n37.60\n0.00\n0.00\n14.20\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 36.50\n3.50\n4.00\n1.80\n0.20\n31.20\n12.90\nImage-text\nOne-step\nEff.\n38.20\n52.50\n58.80\n27.90\n12.70 11.90\n33.70\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 25.20\n6.50\n0.60\n1.00\n1.20\n0.00\n5.80\nMulti-step\nw\/o history\nEff.\n10.60\n9.60\n1.80\n89.40\n0.90\n0.10\n18.70\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 23.20\n6.00\n0.00\n1.80\n1.50\n0.00\n5.40\nMulti-step\nw\/ history\nEff.\n11.60\n5.20\n1.80\n89.50\n1.50\n0.10\n18.30\nAcc. 12.00\n0.00\n8.00\n4.00\n0.00\n2.00\n4.30\nComp. 27.50\n4.50\n12.00\n10.50\n5.00\n23.00\n13.80\nText-only\nOne-step\nEff.\n31.40\n49.70\n72.00\n43.20\n62.30 28.30\n47.80\nAcc. 14.33\n0.00\n1.33\n0.67\n0.00\n0.33\n2.75\nComp. 26.23\n6.63\n2.77\n3.68\n1.40\n9.03\n8.30\nGPT-4o\nAverage\nEff.\n19.33\n31.72\n23.45\n57.62\n13.00\n6.82\n25.32\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 4.80\n7.20\n0.00\n4.20\n1.00\n0.00\n2.90\n35\nPreprint\nMulti-step\nw\/o history\nEff.\n3.60\n15.40\n2.80\n27.30\n2.50\n1.40\n8.80\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 7.50\n5.20\n0.30\n3.50\n1.00\n0.00\n2.90\nMulti-step\nw\/ history\nEff.\n13.60\n0.50\n2.80\n22.90\n2.40\n1.10\n7.20\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.30\nComp. 19.20\n6.50\n0.00\n0.00\n0.00\n0.00\n4.30\nImage-text One-step\nEff.\n32.90\n0.00\n0.00\n0.00\n0.00\n0.00\n5.50\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 4.50\n7.50\n0.00\n1.00\n2.50\n0.00\n2.60\nMulti-step\nw\/o history\nEff.\n3.60\n5.60\n1.80\n86.60\n2.00\n1.00\n16.80\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 5.00\n8.00\n0.30\n2.00\n2.50\n0.00\n3.00\nMulti-step\nw\/ history\nEff.\n9.30\n5.60\n1.80\n86.00\n1.10\n0.80\n17.40\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 13.80\n6.80\n0.00\n0.00\n0.00\n0.00\n3.40\nText-only\nOne-step\nEff.\n7.90\n43.10\n0.00\n0.00\n0.00\n0.00\n8.50\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.32\nComp. 9.13\n6.87\n0.10\n1.78\n1.17\n0.00\n3.18\nGPT-4V\nAverage\nEff.\n11.82\n11.70\n1.53\n37.13\n1.33\n0.72\n10.70\nAcc.\n4.00\n0.00\n0.00\n2.00\n0.00\n0.00\n1.00\nComp. 19.80\n9.00\n0.30\n4.00\n2.00\n0.00\n5.90\nMulti-step\nw\/o history\nEff.\n25.10\n57.10\n3.30\n95.90\n23.80\n3.00\n34.70\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 10.50\n6.50\n0.00\n5.20\n0.50\n0.00\n3.80\nMulti-step\nw\/ history\nEff.\n20.20\n48.40\n4.10\n87.50\n5.30\n1.40\n27.80\nAcc. 10.00\n6.00\n0.00\n0.00\n0.00\n0.00\n2.70\nComp. 20.80\n13.80\n4.00\n3.20\n4.80\n11.00\n9.60\nImage-text\nOne-step\nEff.\n35.30\n58.80\n61.00\n55.70\n38.60\n7.10\n42.80\nAcc. 34.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.70\nComp. 43.20\n4.50\n0.60\n0.80\n0.20\n0.00\n8.20\nMulti-step\nw\/o history\nEff.\n16.10\n3.40\n2.50\n94.00\n0.40\n0.60\n19.50\nAcc. 26.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.30\nComp. 32.80\n4.00\n0.30\n0.80\n1.00\n0.00\n6.50\nMulti-step\nw\/ history\nEff.\n8.60\n3.40\n2.30\n95.60\n0.50\n0.50\n18.50\nAcc. 10.00\n2.00\n2.00\n0.00\n0.00\n0.00\n2.30\n36\nPreprint\nComp. 24.80\n6.20\n2.00\n6.50\n3.00\n8.20\n8.50\nText-only\nOne-step\nEff.\n33.70\n55.50\n64.80\n30.40\n37.50\n4.50\n37.70\nAcc. 14.00\n0.00\n1.33\n0.67\n0.00\n0.33\n2.72\nComp. 25.32\n7.33\n1.20\n3.42\n1.92\n3.20\n7.08\nAverage\nEff.\n23.17\n37.77\n23.00\n76.52\n17.68\n2.85\n30.17\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\nComp. 10.00\n5.80\n0.00\n1.80\n3.00\n0.00\n3.40\nMulti-step\nw\/o history\nEff.\n2.70\n21.40\n1.70\n35.60\n17.40\n0.40\n13.20\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 8.50\n6.50\n0.00\n2.00\n3.50\n0.00\n3.40\nMulti-step\nw\/ history\nEff.\n2.00\n0.00\n1.40\n23.50\n22.10\n0.40\n8.20\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 20.80\n8.20\n0.00\n3.00\n6.80\n1.00\n6.60\nImage-text\nOne-step\nEff.\n34.80\n64.70\n58.20\n32.80\n18.50\n2.50\n35.20\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n,2.0\nComp. 22.00\n7.00\n0.30\n0.20\n1.80\n0.00\n5.20\nMulti-step\nw\/o history\nEff.\n7.80\n8.30\n1.60\n95.30\n3.50\n0.40\n19.50\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 23.80\n8.80\n0.00\n1.20\n1.50\n0.00\n5.90\nMulti-step\nw\/ history\nEff.\n8.40\n4.90\n1.80\n86.10\n2.50\n0.30\n17.30\nAcc.\n2.00\n0.00\n2.00\n0.00\n0.00\n2.00\n1.00\nComp. 26.20\n6.50\n2.00\n4.80\n4.20\n9.80\n8.90\nText-only\nOne-step\nEff.\n27.70\n52.40\n70.80\n36.00\n42.70 11.20\n40.10\nAcc.\n5.67\n0.00\n0.33\n0.00\n0.00\n0.33\n1.05\nComp. 18.55\n7.13\n0.38\n2.17\n3.47\n1.80\n5.57\nGPT-4o mini\nAverage\nEff.\n13.90\n25.28\n22.58\n51.55\n17.78\n2.53\n22.25\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 10.00\n5.80\n0.00\n7.20\n0.20\n0.00\n3.90\nMulti-step\nw\/o history\nEff.\n25.10\n33.60\n2.00\n86.80\n8.40\n5.50\n26.90\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 8.00\n6.20\n0.00\n11.20\n0.20\n0.00\n4.30\nMulti-step\nw\/ history\nEff.\n37.00\n37.30\n2.80\n49.10\n9.00\n3.60\n23.10\nAcc. 28.00\n2.00\n4.00\n4.00\n0.00\n4.00\n7.00\nComp. 55.00\n5.50\n4.00\n13.80\n6.50\n46.60\n21.90\nImage-text\nOne-step\nEff.\n51.30\n63.40\n60.20\n52.50\n26.40 36.90\n48.40\n37\nPreprint\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.80\n6.80\n2.30\n0.20\n1.20\n0.00\n4.90\nMulti-step\nw\/o history\nEff.\n4.40\n4.60\n1.60\n92.40\n1.80\n0.60\n17.60\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 21.20\n6.20\n1.40\n0.00\n2.50\n0.00\n5.20\nMulti-step\nw\/ history\nEff.\n4.60\n4.00\n1.70\n98.80\n1.60\n0.40\n18.50\nAcc. 28.00\n0.00\n18.00\n2.00\n0.00\n0.00\n8.00\nComp. 41.20\n10.00\n28.00\n11.50\n8.20\n1.60\n16.80\nText-only\nOne-step\nEff.\n37.50\n61.70\n76.80\n41.30\n30.90\n3.80\n42.00\nAcc. 14.00\n0.33\n3.67\n1.67\n0.00\n0.67\n3.37\nComp. 25.70\n6.75\n5.95\n7.32\n3.13\n8.03\n9.50\nClaude-3.5 Sonnet\nAverage\nEff.\n26.65\n34.10\n24.18\n70.15\n13.02\n8.47\n29.42\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.20\n8.00\n0.90\n0.50\n1.50\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n5.40\n3.10\n1.70\n62.80\n1.20\n0.40\n12.40\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 19.20\n7.20\n0.60\n0.00\n1.50\n0.00\n4.80\nMulti-step\nw\/ history\nEff.\n4.50\n3.60\n1.90\n62.00\n1.30\n0.60\n12.30\nAcc.\n0.00\n2.00\n2.00\n2.00\n0.00\n0.00\n1.00\nComp. 40.50\n4.80\n6.00\n7.50\n4.20\n1.20\n10.70\nText-only\nOne-step\nEff.\n40.20\n55.70\n71.00\n47.00\n27.70\n3.00\n40.80\nAcc.\n9.33\n0.67\n0.67\n0.67\n0.00\n0.00\n1.87\nComp. 25.97\n6.67\n2.50\n2.67\n2.40\n0.40\n5.07\nClaude-3 Opus\nAverage\nEff.\n16.70\n20.80\n24.87\n57.27\n10.07\n1.33\n21.83\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.20\n8.00\n0.90\n0.20\n1.50\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n5.30\n3.10\n1.70\n61.80\n1.20\n0.40\n12.20\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 19.20\n7.20\n0.60\n0.00\n1.50\n0.00\n4.80\nMulti-step\nw\/ history\nEff.\n4.50\n3.60\n1.80\n62.00\n1.30\n0.60\n12.30\nAcc.\n0.00\n2.00\n2.00\n2.00\n0.00\n0.00\n1.00\nComp. 35.00\n5.00\n2.00\n7.00\n4.20\n1.20\n9.10\nText-only\nOne-step\nEff.\n38.70\n56.30\n69.00\n48.60\n29.80\n3.80\n41.00\nAcc.\n9.33\n0.67\n0.67\n0.67\n0.00\n0.00\n1.87\nComp. 24.13\n6.73\n1.17\n2.40\n2.40\n0.40\n6.23\n38\nPreprint\nGPT-4 Turbo\nAverage\nEff.\n16.17\n21.00\n24.17\n57.47\n10.77\n1.60\n21.83\nOpen Source Model\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 15.20\n6.50\n0.00\n1.00\n0.00\n0.00\n3.78\nMulti-step\nw\/o history\nEff.\n22.50\n25.10\n8.20\n11.30\n0.00\n0.00\n11.18\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 13.20\n6.80\n0.00\n0.00\n0.00\n0.00\n3.33\nMulti-step\nw\/ history\nEff.\n19.80\n26.00\n7.30\n9.80\n0.80\n0.00\n10.62\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 16.80\n4.50\n1.20\n0.00\n2.50\n0.00\n4.17\nImage-text\nOne-step\nEff.\n25.20\n22.60\n33.80\n5.30\n19.50\n0.00\n17.73\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 14.00\n5.50\n0.00\n1.20\n1.00\n0.00\n3.62\nMulti-step\nw\/o history\nEff.\n16.30\n27.70\n3.20\n12.10\n1.20\n0.00\n10.08\nAcc.\n0.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 11.20\n4.00\n0.10\n0.80\n0.00\n0.00\n2.68\nMulti-step\nw\/ history\nEff.\n13.30\n17.80\n1.60\n5.50\n0.00\n0.00\n6.37\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 14.00\n5.80\n2.00\n0.00\n3.50\n0.00\n4.22\nText-only\nOne-step\nEff.\n24.10\n57.20\n30.00\n0.00\n20.00\n0.00\n21.88\nAcc.\n1.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.22\nComp. 14.07\n5.52\n0.55\n0.50\n1.17\n0.00\n3.63\nMiniCPM-V2.6\nAverage\nEff.\n20.20\n29.40\n14.02\n7.33\n6.92\n0.00\n12.98\nAcc.\n6.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 11.00\n4.50\n0.10\n0.00\n0.00\n0.00\n2.60\nMulti-step\nw\/o history\nEff.\n17.90\n11.10\n3.40\n3.00\n0.00\n0.00\n5.90\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 10.00\n5.50\n0.00\n0.00\n0.00\n0.00\n2.58\nMulti-step\nw\/ history\nEff.\n16.60\n10.90\n2.80\n1.30\n0.00\n0.00\n5.27\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 14.20\n5.00\n0.00\n0.80\n0.00\n0.00\n3.33\nImage-text\nOne-step\nEff.\n10.40\n12.00\n3.30\n4.10\n0.00\n0.00\n4.97\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 11.80\n4.00\n0.00\n0.00\n0.00\n0.00\n2.63\nMulti-step\nw\/o history\nEff.\n13.20\n2.30\n0.00\n2.80\n0.00\n0.00\n3.05\n39\nPreprint\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 9.20\n5.80\n0.00\n0.00\n0.00\n0.00\n2.50\nMulti-step\nw\/ history\nEff.\n13.10\n6.60\n1.50\n4.40\n0.00\n0.00\n4.27\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 15.80\n7.20\n0.00\n0.00\n0.00\n0.00\n3.83\nText-only\nOne-step\nEff.\n17.70\n10.10\n0.00\n8.40\n0.00\n0.00\n6.03\nAcc.\n5.33\n0.00\n0.00\n0.00\n0.00\n0.00\n0.89\nComp. 12.00\n5.33\n0.02\n0.13\n0.00\n0.00\n2.91\nInternvl2-8B\nAverage\nEff.\n14.82\n8.83\n1.83\n4.00\n0.00\n0.00\n4.91\nAcc. 12.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 18.50\n7.80\n0.30\n2.00\n0.20\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n25.00\n19.70\n2.20\n14.90\n1.20\n0.50\n10.58\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 19.00\n8.00\n0.30\n2.50\n1.50\n0.00\n5.22\nMulti-step\nw\/ history\nEff.\n23.10\n17.00\n1.90\n12.10\n1.20\n0.00\n9.22\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 21.20\n8.50\n1.20\n2.50\n0.50\n0.00\n5.65\nImage-text\nOne-step\nEff.\n27.70\n18.60\n15.50\n7.40\n2.40\n0.00\n11.93\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 20.20\n9.50\n0.00\n1.80\n0.00\n0.00\n5.25\nMulti-step\nw\/o history\nEff.\n21.80\n19.80\n2.50\n16.00\n0.00\n1.20\n10.22\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 20.00\n10.50\n0.10\n1.00\n0.00\n0.00\n5.27\nMulti-step\nw\/ history\nEff.\n20.10\n21.20\n1.30\n10.80\n0.00\n2.20\n9.27\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 22.00\n8.00\n0.00\n0.80\n0.50\n0.00\n5.22\nText-only\nOne-step\nEff.\n25.40\n14.10\n43.30\n14.90\n1.60\n1.00\n16.72\nAcc. 11.00\n0.33\n0.00\n0.00\n0.00\n0.00\n1.89\nComp. 20.15\n8.72\n0.32\n1.77\n0.45\n0.00\n5.23\nInternvl2-26B\nAverage\nEff.\n23.85\n18.40\n11.12\n12.68\n1.07\n0.82\n11.32\nAcc.\n8.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 21.20\n6.80\n0.60\n4.50\n1.00\n0.00\n5.68\nMulti-step\nw\/o history\nEff.\n33.50\n40.40\n1.60\n29.70\n0.80\n4.20\n18.37\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 18.80\n7.00\n0.30\n6.50\n0.00\n0.00\n5.43\n40\nPreprint\nMulti-step\nw\/ history\nEff.\n31.00\n18.40\n1.20\n24.30\n0.00\n3.00\n12.98\nAcc. 16.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.67\nComp. 30.00\n7.50\n2.00\n6.50\n1.20\n0.00\n7.87\nImage-text\nOne-step\nEff.\n42.40\n42.60\n18.80\n28.10\n1.40\n0.00\n22.22\nAcc.\n4.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 19.20\n9.80\n0.00\n1.20\n0.00\n0.00\n5.03\nMulti-step\nw\/o history\nEff.\n27.20\n41.10\n0.80\n20.90\n0.00\n2.00\n15.33\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 12.50\n7.50\n0.00\n4.00\n0.50\n0.00\n4.08\nMulti-step\nw\/ history\nEff.\n24.10\n39.00\n1.80\n25.80\n0.60\n0.00\n15.22\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 23.80\n9.50\n4.00\n11.20\n0.00\n0.00\n8.08\nText-only\nOne-step\nEff.\n29.90\n44.70\n55.40\n38.40\n2.40\n0,6\n34.16\nAcc. 10.00\n0.67\n0.00\n0.00\n0.00\n0.00\n1.78\nComp. 20.92\n8.02\n1.15\n5.65\n0.45\n0.00\n6.03\nInternvl2-40B\nAverage\nEff.\n31.35\n37.70\n13.27\n27.87\n0.87\n1.84\n18.82\nAcc.\n0.00\n4.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 28.30\n9.50\n0.00\n0.00\n0.00\n0.00\n6.30\nMulti-step\nw\/o history\nEff.\n36.40\n42.20\n3.20\n6.70\n0.00\n0.90\n14.90\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 29.20\n8.00\n0.60\n0.00\n0.00\n0.00\n6.30\nMulti-step\nw\/ history\nEff.\n38.80\n39.30\n3.10\n3.20\n0.00\n0.90\n14.22\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 18.50\n6.50\n1.20\n1.00\n0.20\n0.00\n4.57\nImage-text\nOne-step\nEff.\n26.00\n36.70\n58.10\n26.30\n1.10\n5.90\n25.68\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 27.00\n7.00\n0.60\n0.00\n0.20\n0.00\n5.80\nMulti-step\nw\/o history\nEff.\n34.50\n27.60\n4.20\n3.00\n0.40\n0.50\n11.70\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 29.50\n6.50\n0.00\n0.00\n0.00\n0.00\n6.00\nMulti-step\nw\/ history\nEff.\n39.10\n22.10\n1.60\n2.40\n0.00\n0.00\n10.87\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 13.50\n4.80\n4.00\n1.80\n1.00\n0.00\n4.18\nText-only\nOne-step\nEff.\n23.90\n38.70\n59.50\n33.30\n2.40\n5.80\n27.27\nAcc.\n1.67\n0.67\n0.00\n0.00\n0.00\n0.00\n0.39\n41\nPreprint\nComp. 24.33\n7.05\n1.07\n0.47\n0.23\n0.00\n5.53\nAverage\nEff.\n33.12\n34.43\n21.62\n12.48\n0.65\n2.33\n17.44\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 9.00\n8.00\n0.10\n2.50\n1.20\n0.00\n3.47\nMulti-step\nw\/o history\nEff.\n14.40\n28.00\n2.50\n24.80\n0.50\n0.60\n11.80\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 8.50\n6.00\n0.60\n1.00\n0.20\n0.00\n2.72\nMulti-step\nw\/ history\nEff.\n13.40\n26.60\n1.70\n25.00\n0.60\n0.00\n11.22\nAcc.\n6.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 10.80\n6.50\n0.60\n4.00\n0.00\n0.00\n3.65\nImage-text\nOne-step\nEff.\n16.70\n30.50\n29.90\n21.30\n0.00\n0.00\n16.40\nAcc.\n0.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 8.00\n6.80\n0.00\n0.80\n0.50\n0.00\n2.68\nMulti-step\nw\/o history\nEff.\n11.50\n32.00\n2.40\n3.30\n1.10\n0.00\n8.38\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 11.80\n5.80\n0.00\n6.50\n1.00\n0.00\n4.18\nMulti-step\nw\/ history\nEff.\n16.10\n25.40\n0.00\n15.20\n0.70\n0.00\n9.57\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 12.00\n7.50\n0.00\n4.00\n0.00\n0.00\n3.92\nText-only\nOne-step\nEff.\n17.60\n27.00\n15.50\n35.30\n0.00\n0.00\n15.90\nAcc.\n2.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.39\nComp. 10.02\n6.77\n0.22\n3.13\n0.48\n0.00\n3.44\nDeepSeek-VL\nAverage\nEff.\n14.95\n28.25\n8.67\n20.82\n0.48\n0.10\n12.21\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 25.50\n7.50\n0.00\n1.20\n1.20\n0.00\n5.90\nMulti-step\nw\/o history\nEff.\n33.10\n45.30\n1.10\n11.10\n2.80\n1.10\n15.75\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 23.80\n8.50\n0.00\n0.80\n1.00\n0.00\n5.68\nMulti-step\nw\/ history\nEff.\n33.20\n47.70\n0.90\n14.10\n2.10\n0.70\n16.45\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 27.00\n8.80\n1.20\n2.50\n0.00\n0.00\n6.58\nImage-text\nOne-step\nEff.\n39.50\n39.80\n47.50\n28.80\n0.00\n7.10\n27.12\nAcc.\n8.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 24.50\n6.00\n0.60\n3.00\n0.00\n0.00\n5.68\nMulti-step\nw\/o history\nEff.\n33.80\n30.50\n3.60\n14.60\n0.00\n0.00\n13.75\n42\nPreprint\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 20.50\n7.00\n0.60\n1.00\n1.00\n0.00\n5.02\nMulti-step\nw\/ history\nEff.\n31.00\n31.40\n2.30\n10.50\n0.50\n1.40\n12.85\nAcc. 10.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 26.00\n7.50\n0.60\n10.50\n1.20\n0.00\n7.63\nText-only\nOne-step\nEff.\n37.20\n40.20\n61.10\n36.80\n5.10\n7.80\n31.37\nAcc.\n8.33\n0.67\n0.00\n0.00\n0.00\n0.00\n1.50\nComp. 24.55\n7.55\n0.50\n3.17\n0.73\n0.00\n6.08\nCogvlm2-19B\nAverage\nEff.\n34.63\n39.15\n19.42\n19.32\n1.75\n3.02\n19.55\nAcc. 12.00\n4.00\n0.00\n0.00\n0.00\n0.00\n2.67\nComp. 40.50\n8.80\n0.10\n4.50\n0.50\n0.00\n9.07\nMulti-step\nw\/o history\nEff.\n47.20\n33.60\n4.20\n17.80\n1.50\n1.00\n17.55\nAcc. 12.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 30.20\n4.50\n0.30\n2.50\n0.20\n0.00\n6.28\nMulti-step\nw\/ history\nEff.\n38.60\n28.90\n3.90\n15.30\n1.60\n2.50\n15.13\nAcc. 18.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.00\nComp. 42.20\n4.80\n0.60\n1.20\n1.00\n0.00\n8.30\nImage-text One-step\nEff.\n50.90\n55.40\n42.30\n50.00\n8.60\n9.90\n36.18\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 41.80\n6.80\n0.30\n0.80\n0.20\n0.00\n8.32\nMulti-step\nw\/o history\nEff.\n49.40\n49.20\n5.50\n20.70\n1.90\n0.10\n21.13\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 37.00\n9.50\n0.00\n1.20\n0.50\n0.00\n8.03\nMulti-step\nw\/ history\nEff.\n41.00\n52.50\n4.90\n55.30\n10.00 12.10\n29.30\nAcc. 14.00\n4.00\n0.00\n0.00\n0.00\n0.00\n3.00\nComp. 18.80\n9.00\n2.00\n4.50\n1.00\n0.00\n5.88\nText-only\nOne-step\nEff.\n29.00\n52.00\n47.70\n49.00\n9.90\n10.10\n32.95\nAcc. 13.33\n1.67\n0.00\n0.00\n0.00\n0.00\n2.50\nComp. 35.08\n7.23\n0.55\n2.45\n0.57\n0.00\n7.65\nInternVL2-\nLlama3-76B\nAverage\nEff.\n42.68\n45.27\n18.08\n34.68\n5.58\n5.95\n25.38\nTable 3: Results for all of the MLLMs\nD\nCASE STUDY\n43\nPreprint\nFigure 7: A sample case of perceptual error. Sokoban — One-step — Image-text — Level 2.\n44\nPreprint\nFigure 8: A sample case of textual understanding error. Hanoi — Multi-step — Text-only — With-\nhistory — Level 13.\n45\nPreprint\nFigure 9: A sample case of three errors. 8-queens — One-step — Image-text — Level 8.\n46\nPreprint\nFigure 10: A sample case of output comparison. Maze — Multi-step — Image-text — Without-\nhistory — Level 33.\n47\nPreprint\nFigure 11: A sample case of output comparison. 15-Puzzle — Multi-step — Image-text — Without-\nhistory — Level 45.\n48\nPreprint\nFigure 12: A sample case of output comparison. Sudoku — One-step — Image-text — Level 2.\n49\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ING-VP: MLLMs cannot Play Easy Vision-based Games Yet.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nING-VP: MLLMs cannot Play Easy Vision-based Games Yet\n```\n#### 2. 论文摘要\n```\nAs multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps:\/\/github.com\/Thisisus7\/ING-VP.git.\n```\n\n#### 3. 论文全文\n```\nPreprint\nING-VP: MLLMS CANNOT PLAY EASY VISION-BASED\nGAMES YET\nHaoran Zhang1∗, Hangyu Guo1∗, Shuyue Guo1, Meng Cao3,\nWenhao Huang1,2, Jiaheng Liu1†, Ge Zhang1,2†\n1M-A-P,\n2Bytedance.Inc,\n3MBZUAI\nhaor7@outlook.com, zhangge.eli@bytedance.com\nABSTRACT\nAs multimodal large language models (MLLMs) continue to demonstrate increas-\ningly competitive performance across a broad spectrum of tasks, more intricate\nand comprehensive benchmarks have been developed to assess these cutting-edge\nmodels. These benchmarks introduce new challenges to core capabilities such\nas perception, reasoning, and planning. However, existing multimodal bench-\nmarks fall short in providing a focused evaluation of multi-step planning based\non spatial relationships in images. To bridge this gap, we present ING-VP,\nthe first INteractive Game-based Vision Planning benchmark, specifically de-\nsigned to evaluate the spatial imagination and multi-step reasoning abilities of\nMLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with\n6 unique configurations. A single model engages in over 60,000 rounds of in-\nteraction. The benchmark framework allows for multiple comparison settings,\nincluding image-text vs. text-only inputs, single-step vs. multi-step reasoning,\nand with-history vs. without-history conditions, offering valuable insights into\nthe model’s capabilities. We evaluated numerous state-of-the-art MLLMs, with\nthe highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy\nof only 3.37%, far below the anticipated standard. This work aims to provide\na specialized evaluation framework to drive advancements in MLLMs’ capacity\nfor complex spatial reasoning and planning. The code is publicly available at\nhttps:\/\/github.com\/Thisisus7\/ING-VP.git.\nw\/ history & w\/o history\nImage-text & Text-only\nMulti-step & One-step\nPerception\nText Understanding\nSpatial Imagination\nPlanning\nReasoning\nING-VP\nBench\n6 Games\n3 Comparisons\n          Capacity                              Evaluation\nMultimodal & Interactive Environment\nPrompts\nImage\/text\nLevels\nModel\n1.question\n1\/4.initial state\nGame Env\n2.output\nDatabase\n2.instruction\n3.next state\n4.next state\n4.history\nConsider the multi-step process with a history-enabled setting:\nThe model ingests game data as input and generates an output in response to the specific prompt.\n1.\nThis output is relayed to a database, which extracts the corresponding instructions and sends them to the game\nenvironment.\n2.\nThe game environment updates its state based on these instructions, returning the new game data to the database.\n3.\nThis updated data, together with the prompt, is used as the next input for the model, which continues to generate\noutputs for the database.\n4.\nThis cycle—steps 2 through 4—is repeated until the model successfully completes the level or the step limit is\nreached.\n5.\nFigure 1: The overview of ING-VP benchmark.ING-VP comprises 6 distinct games, conducts 3\ncomparative analyses across 6 experimental settings, and evaluates 5 key capabilities of MLLMs.\nAdditionally, it offers a highly efficient interactive environment for both inference and analysis.\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated remarkable capabilities in natural language\nprocessing, generation, and even textual complex reasoning and planning (Zhao et al., 2023). Building\nupon this powerful foundation of LLMs, integrating visual inputs has led to the development of even\n∗Equal Contributions, † Corresponding Authors.\n1\narXiv:2410.06555v1  [cs.CL]  9 Oct 2024\nPreprint\nmore powerful models (OpenAI, 2024; Anil et al., 2023a), a.k.a multimodal large language models\n(MLLMs).\nDespite demonstrating impressive performance in handling most general multimodal tasks, the\neffectiveness of MLLM in multimodal reasoning and planning still remains unclear. Moreover, recent\nstudies (Lu et al., 2024; Dai et al., 2024) indicate that vision-language training might degrade the\ntextual capabilities of MLLMs, suggesting that MLLMs built upon LLMs could be impaired when\nadapted to multimodal reasoning and planning tasks. Consequently, there is an urgent need for a\ntest that incorporates multimodal complex reasoning and planning cases to guide the subsequent\nenhancements of MLLMs.\nTo address this issue, existing studies generally utilize visual question answering (VQA) (Antol\net al., 2015; Kafle & Kanan, 2017) and game-based evaluations (Wu et al., 2023; Bellemare et al.,\n2013) to assess the visual reasoning capabilities of MLLMs. In general, VQA necessitates a verified\nground-truth answer that relies on human annotations. But acquiring these annotations is both\ncostly and time-consuming. Moreover, the absence of interaction and planning in typical VQA tasks\nposes difficulties in evaluating the reasoning and planning capabilities of advanced MLLMs. The\ntasks presented in these benchmarks are overly simplistic (Yue et al., 2023) or only test reasoning\nwithin domain-specific knowledge (Yue et al., 2023; Zhang et al., 2024a), which mainly evaluates\nthe LLM knowledge of MLLMs rather than the perception, reasoning, and planning of MLLMs.\nTherefore, recent studies (Xu et al., 2024; Chia et al., 2024) prompt MLLMs to interact with digital\ngame environments, which are measured by game outcomes and scores, leading to the game-based\nevaluation. Unlike VQA tasks, these methods can evaluate the multi-step reasoning capabilities\nand even spatial imagination of MLLMs, which is crucial function of human cognition, allowing\nus to interact with realistic environments (Wu et al., 2024). Despite the effectiveness, these works\nare typically restricted to individual games with complex rules, involve time-consuming evaluation\nepisodes, and fail to effectively assess the models’ generalization capabilities in multimodal planning.\nConsidering these challenges, our goal is to develop a generalizable and efficient benchmark to\nevaluate the multi-step planning abilities of MLLMs, providing insights for subsequent improvements\nof MLLMs with complex multi-step reasoning.\nTo fill this gap, in this paper, we introduce the INteractive Game-based Vision Planning benchmark\n(ING-VP), meticulously focusing on evaluating the spatial imagination and multi-step reasoning\nabilities of MLLMs. Figure 1 shows games, evaluation settings, and the interactive process in our\nING-VP. To construct our ING-VP, we initially collect six games featuring easily understandable\nrules. In each game, we collect 50 levels, each comprising both an image and a text representation of\nthe current state, providing vision and textual inputs for MLLMs, as illustrated in Figure 2. To assess\nthe spatial imagination and planning capabilities of MLLMs, we establish six experimental settings,\nwhich prompt the models to perform single-step and multi-step reasoning, with or without historical\ninteraction. During the evaluation, we employ MLLMs to interact within the environment until the\ngame is completed. To evaluate model performance comprehensively, beyond merely determining\nwhether a model can finish a game, we also use the model’s action efficiency and the remaining steps\nto complete the game as evaluation metrics.\nWith our ING-VP, we test 15 open- and closed-source MLLMs and analyze their performance on\nour test cases. We first support the benchmark designed to evaluate the multi-step reasoning and\nspatial imagination capabilities of MLLMs — ING-VP bench. Then we analyze these capabilities of\ncurrent open- and closed-source MLLMs, despite a performance gap, the leading open-source model,\nInternVL2-Llama3-76B, achieves an accuracy of 2.50%, ranking just behind Claude-3.5 Sonnet,\nGPT-4o, and Gemini-1.5 Pro. Notably, its performance significantly surpasses that of GPT-4o mini,\nwhich stands at 1.05%, and GPT-4v, which records a mere 0.32%. We also conduct a detailed analysis\nof these models’ performance, the evidence shows that:\n• The inability to process the relative positions of elements is one of the primary issues with\nMLLM perception.\n• Even the most advanced MLLMs have very limited planning capabilities, far below the\nperformance of ordinary humans on these simple tasks.\n• Current models tend to generate instructions that are much longer than necessary to complete\nthe levels. While this can improve accuracy on simple levels, it also indirectly reveals that\nMLLMs are “uncertain” about the correct solution.\n2\nPreprint\nWhile most tasks in the ING-VP benchmark are straightforward for humans, they pose significant\nchallenges for MLLMs, even the top-performing model, Claude-3.5 Sonnet, achieving an average\naccuracy of just 3.37%. We reveal that current MLLMs generally lack spatial imagination and\nmulti-step planning abilities, and offer a new perspective on the capability requirements for MLLMs.\nSokoban\nInstruction:\naction: L(Left); R(Right); U(Up); D(Down)\nundo: return to a specified history state\nMaze\nInstruction:\naction: L(Left); R(Right); U(Up); D(Down)\n8-queens\nInstruction:\naction: [x,y], coordinates of chess pieces\nundo: return to a specified history state\nHanoi\nInstruction:\naction: “{x}{y}”, move the top disk from rod x\nto rod y,  smaller on larger\nSudoku\n15-puzzle\nInstruction:\naction: {number}, if the number is around\nthe empty space, , they will swap positions.\nInstruction:\naction: \"{row}{column}\": {number}\nundo: return to a specified history state\nImage\nText\nIntroduction:\nA puzzle game where the player navigates a\nwarehouse, \npushing \nboxes \nto \ndesignated\nstorage locations.\nGoal:\nMove all boxes onto\nthe storage locations.\nImage\nIntroduction:\nA puzzle game where the player navigate\nthrough a complex labyrinth, finding the path\nto the exit.\nText\nGoal:\nMove the red square\nto the green square.\nImage\nText\nIntroduction:\nA chess problem that challenges the player to\nplace eight queens on an 8x8 chessboard so\nthat no two queens threaten each other.\nGoal:\nNo two queens can\nshare the same row,\ncolumn and diagonal.\nImage\nText\nIntroduction:\nA puzzle where the objective is to move a stack\nof disks from one rod to another, following\nspecific rules about disk placement.\nGoal:\nMove all disks to \nthe rod D.\nImage\nText\nIntroduction:\nA sliding puzzle consisting of a 4x4 grid with 15\nnumbered tiles and one empty space.\nGoal:\nSlide the tiles to\narrange them in\nnumerical order.\nImage\nText\nIntroduction:\nA logic-based number-placement puzzle.\nGoal:\nFill a 9×9 grid with digits\nso that each column,\nrow, and 3×3 sub-grid\ncontains all digits from 1\nto 9 without repetition.\nFigure 2: ING-VP examples sampled from each game. Includes pictures and text representations of\nSokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, and 15-puzzle.\n2\nRELATED WORK\nMultimodal Large Language Models. LLMs (Achiam et al., 2023; Anil et al., 2023b) have\ndemonstrated their ability of generating human-like texts to understand and respond to complex\ninstructional queries. The successes of LLMs has elicited the burgeoning proliferation of multi-modal\nLLMs (Alayrac et al., 2022; Li et al., 2023b; Liu et al., 2024; Sun et al., 2024; Jin et al., 2023b),\nwhich is designed to process and integrate multiple types of data. The primary attempt Flamingo\n(Alayrac et al., 2022) endows visual-language models with in-context few-shot learning capabilities\nby trained on large-scale interleaved text-image data. BLIP2 (Li et al., 2023b) designs a Q-Former\narchitecture to align the visual-textual knowledge during the pre-training phase. LLaVA (Liu et al.,\n2024) collect GPT-4 generated multimodal language-image instruction-following data and train a\ngeneral-purpose visual-language assistant. Beyond multimodal understanding, EMU-2 (Sun et al.,\n2024) and LaVIT (Jin et al., 2023b) take one step further and act as generative multimodal model to\nsupport visual prompting and object-grounded generation.\nMLLM Benchmarks. The development of MLLMs has highlighted the critical need of benchmarks\nfor thorough evaluations. Although traditional visual-language tasks (e.g., visual question answering\n(Antol et al., 2015; Kafle & Kanan, 2017) and image captioning (Lin et al., 2014; Plummer et al.,\n2015)) can be used as evaluation benchmarks, they are too strict and require the exact match with\nthe ground-truth answers. To this end, LVLM-eHub (Xu et al., 2023) and LAMM (Yin et al., 2024)\nreformulate exiting public datasets as evaluation samples and employ human annotators or GPT\nto assess the quality. MME (Li et al., 2024), MMBench (Liu et al., 2023b) and SEED-Bench (Li\net al., 2024) construct multiple-choice questions to mitigate the subjectivity and instability of GPT\nevaluation. MMMU (Yue et al., 2024) evaluate the advanced perception and reasoning of MLLMs on\nspecific domains (e.g., science, business).\nGame-based Evaluations. Digital games are acknowledged as essential in the pursuit of artificial\ngeneral intelligence since they present complex challenges requiring advanced reasoning and cognitive\n3\nPreprint\nskills. These challenges make digital games an ideal benchmark for evaluating the capabilities of\nMLLMs (Wu et al., 2023; Bellemare et al., 2013; Hu et al., 2024; Sweetser, 2024; Xu et al., 2024)\nincluding the environment perception (Hong et al., 2023; Akoury et al., 2023), memory construction\n(Zhu et al., 2023; Zhang et al., 2024b; Ding et al., 2023; Park et al., 2022; Liu et al., 2023a), reasoning\n(Liu et al., 2023a; Wang et al., 2023a; Qian et al., 2023; Huang et al., 2022) and decision-making\n(Chen et al., 2023; Zhou et al., 2023; Jin et al., 2023a; Qian et al., 2023). Several methods focus on\nsemantic-level perception of environmental elements including locations, objects or actions in games.\nThey either use basic text input of user ideas (Li et al., 2023a) or game state variables and dialogues\n(Akoury et al., 2023; Park et al., 2022; 2023). Role-based inputs, e.g., the inclusion of character,\nstory, role-related information (Hong et al., 2023; Wang et al., 2023b) and skills (Gong et al., 2023)\nare often included. TorchCraft (Synnaeve et al., 2016) is presented to use real-time strategy games\nsuch as StarCraft: Brood War to serve as a benchmark for AI research. The Chess game has long\nbeen employed as an AI testing ground (Noever et al., 2020; St¨ockl, 2021; Toshniwal et al., 2022).\nChess Transformer (Noever et al., 2020) fine-tunes GPT-2 to generate plausible strategies and learns\ncomplex gameplay. Recent works (Taesiri et al., 2022; 2024) formulate the bug detection problem\nas a question-answering task and leverage the zero-shot capabilities of LLMs for video game bug\ndetection. R2-PLAY (Xu et al., 2024) constructs a multimodal game instruction tuning dataset to\nfacilitate the “read-to-play” capability of LLMs. PuzzleVQA (Chia et al., 2024) demonstrates that\nexisting MLLMs exhibit substantial challenges when solving puzzles that demand visual perception,\ninductive reasoning, and deductive reasoning. Beyond the benchmark setting, we additionally develop\nan interactive environment to assess the ability of multimodal models to perform spatial reasoning\nand multi-step inference based on visual details.\n3\nTHE ING-VP BENCHMARK\n3.1\nOVERVIEW OF ING-VP\nWe introduce ING-VP benchmark, a new interactive game-based vision planning benchmark designed\nto measure the multi-step reasoning and spatial imagination capabilities of MLLMs. The benchmark\nencompasses 6 distinct settings, 6 games, and 50 levels per game, the core mechanisms are depicted\nin Figure 1. To mitigate data leakage and ensure problem solvability, the majority of our levels\nare algorithmically generated and verified. Representative examples of each game are illustrated in\nFigure 2.\nING-VP features 6 games that are conceptually simple yet cognitively challenging: Sokoban, Maze,\nSudoku, 8-queens, Tower of Hanoi, and 15-puzzle. The simplicity lies in the easily comprehensible\nrules and the ability to encapsulate complete level information within a single image, facilitating\ncomprehensive reasoning. The challenge stems from the requirement for models to precisely capture\ncore visual elements and their spatial relationships, necessitating multi-step reasoning to successfully\ncomplete each level. We meticulously craft 6 reasoning settings, enabling researchers to systematically\nidentify the strengths and limitations of target models through comparative analysis of performance\nacross these settings.\n3.2\nSIX INFERENCE SETTINGS\nOne-step: Image and Text-only Settings In the One-step with Image setting, we provide the\nmodel solely with an image depicting the initial game state and prompt it to generate comprehensive\ninstructions for level completion. The One-step Text-only setting follows an identical approach,\nwith the key distinction being the replacement of the image input with its corresponding textual\nrepresentation.\nMulti-step: Image and Text-only Settings (without History) In the Multi-step with Image setting,\nwe provide the model with an image of the current game state at each inference round. After the\nmodel outputs a single-step instruction, this instruction is fed into the game as input, causing the game\nstate to change and generate a new image. This new image then serves as the model’s input for the\nnext step. The Multi-step Text-only setting follows the same process, but uses textual representations\nas the model’s input.\nMulti-step: Image and Text-only Settings (with History) The key distinction in these settings is the\ninclusion of the model’s historical outputs as part of the prompt in each interaction. Additionally, for\n4\nPreprint\nSokoban, Sudoku, and N-queens, we add an undo option, allowing the model to freely revert to any\nprevious state. This enhancement applies to both the Image and Text-only variants of the Multi-step\nsetting.\n3.3\nGAME SELECTION\nWe chose six games that are widely recognized, have straightforward rules, and operate in a determin-\nistic environment, making them ideal representatives for our study. In a deterministic environment,\nthe outcome of every action taken by an agent is predictable and certain. Such an environment can\nbe formally defined using a Markov Decision Process (MDP). The model employs a strategy π to\ndetermine the next action at based on the current state st and all previous actions a0:t−1, represented\nas:\nat = π(st, a0:t−1)\n(1)\nThe planning process of MLLMs can be expressed as:\nS′ = π(S, A, G, n)\n(2)\nWhere S′ is the future sequence of states, which terminates upon achieving the goal G or exhausting\nthe available moves n; S is the current sequence of states; A represents the current sequence of\nactions.\n3.4\nDATA COLLECTION\nSokoban. It involves pushing crates onto designated storage locations within a warehouse maze. We\nselect 50 levels from the Sasquatch dataset 1. To mitigate difficulty and prevent data leakage, we\nemploy the A-star algorithm to constrain each level to a maximum of 8 steps for completion.\nMaze. The Maze game challenges players to navigate from a starting point to a target through a\nnetwork of paths. We employ a Depth-First Search (DFS) algorithm to automatically generate 50\nsolvable levels, each with an 11x11 grid size. We also constrain the solution length to a maximum of\n8 steps.\n8-Queens. The 8-Queens puzzle challenges people to place eight queens on an 8x8 chessboard such\nthat no two queens threaten each other. N-Queens is a special game due to its standard formulation:\nmodels could potentially solve it without visual input, relying solely on memorized patterns from\ntraining data. To ensure that visual reasoning is essential, we modify the puzzle by manually placing\nthe first queen in a different position for each level. The image presented to the MLLMs shows this\ninitial configuration, requiring them to reason from this starting point to complete the puzzle.\nSudoku. Sudoku is a logic-based number placement puzzle that requires filling a 9x9 grid such that\neach row, column, and 3x3 subgrid contains all digits from 1 to 9 without repetition. A well-formed\nSudoku puzzle with a unique solution requires a minimum of 17 initial clues. For our benchmark,\nwe curate a set of 50 puzzles with each puzzle contain 71 clues from a Kaggle dataset 2 , ensuring\neach puzzle meets this criterion. We then manually generate corresponding images for each level to\nmaintain consistency with our benchmark’s visual reasoning focus.\nHanoi The Tower of Hanoi is a classic mathematical puzzle that involves transferring a stack of disks\nof varying diameters from one rod to another, adhering to the constraint that a larger disk must never\nbe placed atop a smaller one. In our implementation, each problem instance consists of four rods and\nfive disks, with an optimal solution requiring a minimum of 8 moves.\n15-Puzzle It’s a classical sliding tile puzzle comprising a 4x4 grid with 15 numbered tiles and one\nvacant space. The objective is to rearrange the tiles into numerical order through a series of sliding\nmovements. In our implementation, we employ the Breadth-First Search (BFS) algorithm to explore\nsolution paths, constraining the search depth to 8 moves as previous games.\n1http:\/\/www.abelmartin.com\/rj\/sokobanJS\/Skinner\/David%20W.%20Skinner%\n20-%20Sokoban.htm\n2https:\/\/www.kaggle.com\/datasets\/informoney\/4-million-sudoku-puzzles-easytohard\n5\nPreprint\nImage-text\nText-only\nMulti-step\nOne-step\nMulti-step\nOne-step\nModel\nMetric\nw\/o history\nw\/ history\nw\/o history\nw\/ history\nOverall\nClosed Source Model\nAcc.\n0.30\n0.30\n7.00\n2.30\n2.30\n8.00\n3.37\nComp.\n3.90\n4.30\n21.90\n4.90\n5.20\n16.80\n9.50\nClaude-3.5 Sonnet\nEff.\n26.90\n23.10\n48.40\n17.60\n18.50\n42.00\n29.42\nAcc.\n3.30\n2.00\n0.30\n3.30\n3.30\n4.30\n2.75\nComp.\n6.70\n5.20\n12.90\n5.80\n5.40\n13.80\n8.30\nGPT-4o\nEff.\n19.20\n14.20\n33.70\n18.70\n18.30\n47.80\n25.32\nAcc.\n1.00\n0.30\n2.70\n5.70\n4.30\n2.30\n2.72\nComp.\n5.90\n3.80\n9.60\n8.20\n6.50\n8.50\n7.08\nGemini-1.5-Pro\nEff.\n34.70\n27.80\n42.80\n19.50\n18.50\n37.70\n30.17\nAcc.\n0.70\n0.30\n0.00\n2.00\n2.30\n1.00\n1.05\nComp.\n3.40\n3.40\n6.60\n5.20\n5.90\n8.90\n5.57\nGPT-4o mini\nEff.\n13.20\n8.20\n35.20\n19.50\n17.30\n40.10\n22.25\nAcc.\n0.00\n0.00\n1.30\n0.00\n0.30\n0.30\n0.32\nComp.\n2.90\n2.90\n4.30\n2.60\n3.00\n3.40\n3.18\nGPT-4V\nEff.\n8.80\n7.20\n5.50\n16.80\n17.40\n8.50\n10.70\nAcc.\nnull\nnull\nnull\n2.30\n2.30\n1.00\n1.87\nComp.\nnull\nnull\nnull\n4.80\n4.80\n9.10\n6.23\nGPT-4 Turbo\nEff.\nnull\nnull\nnull\n12.20\n12.30\n41.00\n21.83\nAcc.\nnull\nnull\nnull\n2.30\n2.30\n1.00\n1.87\nComp.\nnull\nnull\nnull\n4.80\n4.80\n10.70\n5.07\nClaude-3 Opus\nEff.\nnull\nnull\nnull\n12.40\n12.30\n40.80\n21.83\nOpen Source Model\nAcc.\n2.67\n2.33\n3.00\n2.33\n1.67\n3.00\n2.50\nComp.\n9.07\n6.28\n8.30\n8.32\n8.03\n5.88\n7.65\nInternVL2-Llama3-76B\nEff.\n17.55\n15.13\n36.18\n21.13\n29.30\n32.95\n25.58\nAcc.\n2.33\n1.33\n1.67\n1.67\n2.00\n2.33\n1.89\nComp.\n4.80\n5.22\n5.65\n5.25\n5.27\n5.22\n5.23\nInternvl2-26B\nEff.\n10.58\n9.22\n11.93\n10.22\n9.27\n16.72\n11.32\nAcc.\n1.67\n1.67\n2.67\n1.00\n2.00\n1.67\n1.78\nComp.\n5.68\n5.43\n7.87\n5.03\n4.08\n8.08\n6.03\nInternvl2-40B\nEff.\n18.37\n12.98\n22.22\n15.33\n15.22\n34.16\n18.82\nAcc.\n1.33\n0.67\n2.00\n1.67\n1.33\n2.00\n1.50\nComp.\n5.90\n5.68\n6.58\n5.68\n5.02\n7.63\n6.08\nCogvlm2-19B\nEff.\n15.75\n16.45\n27.12\n13.75\n12.85\n31.37\n19.55\nAcc.\n1.00\n0.33\n0.33\n1.33\n0.67\n1.67\n0.89\nComp.\n2.60\n2.58\n3.33\n2.63\n2.50\n3.83\n2.91\nInternvl2-8B\nEff.\n5.90\n5.27\n4.97\n3.05\n4.27\n6.03\n4.91\nAcc.\n0.67\n0.33\n0.00\n0.33\n0.33\n0.67\n0.39\nComp.\n6.30\n6.30\n4.57\n5.80\n6.00\n4.18\n5.53\nInternvl-Chat-v1.5\nEff.\n14.90\n14.22\n25.68\n11.70\n10.87\n27.27\n17.44\nAcc.\n0.67\n0.33\n1.00\n0.33\n0.00\n0.00\n0.39\nComp.\n3.47\n2.72\n3.65\n2.68\n4.18\n3.92\n3.44\ndeepseek-VL\nEff.\n11.80\n11.22\n16.40\n8.38\n9.57\n15.90\n12.21\nAcc.\n0.33\n0\n0\n0.67\n0.33\n0\n0.22\nComp.\n3.78\n3.33\n4.17\n3.62\n2.68\n4.22\n3.63\nMiniCPM-V2.6\nEff.\n11.18\n10.62\n17.73\n10.08\n6.37\n21.88\n12.98\nTable 1: Main results for the best-performing MLLMs (LLMs).\n4\nEXPERIMENTS\nWe conduct a comprehensive evaluation of both open-source and closed-source MLLMs, employ a\nzero-shot setting to faithfully emulate the human puzzle-solving process, given the unique nature of\nour tasks. A uniform set of prompts was applied across all models. The complete set of 36 prompts is\npresented in the Appendix B.\n4.1\nBASELINES\nMLLMs. We consider a comprehensive suite of mainstream large multimodal models. Closed-source\nmodels include GPT-4o, GPT-4o Mini, GPT-4v, GPT-4 Turbo, Claude-3.5 Sonnet, Claude-3 Opus,\n6\nPreprint\nand Gemini-1.5 Pro. Open-source models consist of CogVLM2-19B, DeepSeek-VL, Internvl-Chat-\nv1.5, Internvl2-8B, Internvl2-26B, Internvl2-40B, InternVL2-Llama3-76B, and MiniCPM-V2.6. We\nutilize each model’s official API for closed-source systems or the publicly available checkpoint for\nopen-source implementations, More information of these models can be found in the Appendix A.\nEvaluation. We present a systematic interactive environment for evaluating all MLLMs, where\nmodels interact with the game environment until either completing the task or exhausting the allotted\nsteps. We constrain the model’s output action instructions to JSON format through prompts and\nextract them using regular expressions. The correctly extracted instructions are then used as input\nfor the game environment. After the game state changes, the new state is fed back to the model\nfor the next round of inference. We employ three metrics: accuracy, completion degree, and action\nefficiency. (1) Accuracy is our main metric, it measures whether the model can complete the task\nwithin the specified number of steps. (2) Completion degree is determined by the final state of the\ngame environment after interaction with the model. The closer the final state is to the cleared state,\nthe higher the score; if it deviates, the score decreases accordingly. (3) Action efficiency represents\nwhether each instruction output by the model effectuates a change in the game state. The computation\nmethod for action efficiency is as follows:\nAction Efficiency =\nPn\ni=1\n# of efficient actions for level i\n# of total actions for level i\nn\n4.2\nMAIN RESULTS\nIn this section, we examine the spatial reasoning and planning abilities of current MLLMs using the\nING-VP benchmark. The results are presented in Table 1, please see the Appendix C for the complete\nresults.Our key observations are as follows:\n55.2%\n2.9%\n41.9%\na. Image-text\n58.0%\n42.0%\nb. Text-only\nHanoi\nMaze\n15-puzzle\n8-queens\nSokoban\nSudoku\n0.0\n20.0\n40.0\na\na\na\na\na\na\nb\nb\nb\nb\nb\nb\nError Types\nPerceptual Errors\nTextual Understanding Errors\nPlanning Errors\nFigure 3: Error distribution over Claude-3.5 Son-\nnet’s 555 errors across different tasks and settings.\nThe ING-VP benchmark poses a substantial\nchallenge to current MLLMs: Even the most\nadvanced model, Claude-3.5 Sonnet, achieves\nan accuracy of only 3.37%. In contrast, an av-\nerage human can easily complete all of these\ntasks (8-queens is an exception), highlighting a\nsignificant gap between model performance and\nhuman capabilities on the ING-VP benchmark.\nPerformance disparity between open-source\nand closed-source models persists: While the\nperformance of closed-source models on ING-\nVP is far from satisfactory, they still outperform\nthe open-source models. The best-performing\nopen-source model, InternVL2-Llama3-76B,\nachieves an accuracy of 2.50%, which remains\nlower than Claude-3.5 Sonnet, GPT-4o and\nGemini-1.5 Pro.\nFor MLLMs, the greatest challenge in per-\nception is understanding location informa-\ntion. According to our observations of the infer-\nence results, the most advanced models, such as\nClaude-3.5 Sonnet and GPT-4o, can generally\nidentify the elements present and even count the\nquantity of each in the Sokoban game. However, they struggle to accurately determine precise\nlocation information, leading to very low inference accuracy and degree of task completion.\nMerely breaking down the steps is unhelpful and may even be counterproductive. In text-only\ntasks, Claude-3.5 Sonnet and GPT-4o achieve accuracy rates of 2.30% and 3.30%, respectively, in\nthe multi-step setting, which are lower than their 8.00% and 4.30% accuracy in the one-step setting.\nFor the ING-VP benchmark, thinking step by step does not work and even has a negative effect. We\n7\nPreprint\nbelieve that MLLMs rely heavily on pattern matching based on prior training data, generating outputs\nfrom similar inputs rather than engaging in actual planning.\n4.3\nFINE-GRAINED ANALYSIS\n0\n50\nAcc.\n0\n50\n0\n50\nComp.\n0\n50\n4 step\n8 step\n12 step\n16 step\nClaude-3.5 Sonnet\n25\n50\nEff.\n4 step\n8 step\n12 step\n16 step\nGPT-4o\n25\n50\nImage-text Multi-step w\/o history\nImage-text Multi-step w\/ history\nImage-text One-step\nText-only Multi-step w\/o history\nText-only Multi-step w\/ history\nText-only One-step\nOverall\nFigure 4: Maze level accuracy of Claude-3.5 Son-\nnet and GPT-4o across 4 difficulty levels.\nIn this section, We conduct a comprehensive\nrange of analyses to explore the generative capa-\nbilities of MLLMs in a broader context, while\nalso dissecting the nuanced output tendencies\nof current models. We hope our results can pro-\nvides valuable insights that can inform future\nmodel design and training strategies.\nError Analysis. We collate and analyze 555\nerrors (image-text: 279, text-only: 276) made\nby Claude-3.5 Sonnet in one-step setting, as il-\nlustrated in Figure 3. It is important to note that\nwhile we categorize each case under distinct\nerror types, in many instances the model exhib-\nited errors in both comprehension and reasoning.\nOur classification follows contextual cues: when\nthe model provided invalid instructions from the\noutset, we labele it as an understanding error.\nConversely, if the model deviated from the cor-\nrect solution at an intermediate step, we classify it as a reasoning error. Below, we summarize key\nobservations based on these error types:\n• Perceptual Errors (55.2%\/–%): These errors occur exclusively in the image-text setting.\nWhile current models are generally able to recognize overall attributes of an image—such\nas identifying the game genre and its components, their ability to accurately interpret fine\ndetails, including the specific size and precise location of each element, remains limited\n(e.g., see Figure 7. This perceptual limitation represents a major contributor to the elevated\nerror rates in this setting.\n• Textual Understanding Errors (2.9%\/58.0%): Textual understanding errors manifest in\ntwo main forms: a misinterpretation of specific prompts or an inability to correctly parse\ndata structures or character matrices used to represent game levels in the text-only setting\n(as shown in Figure 8). These errors indicate that the model struggles to generalize its\nunderstanding when presented with text structures not commonly encountered in its training\ndata.\n• Planning Errors (41.9%\/42.0%): Planning errors constitute another major issue for Claude-\n3.5 Sonnet. In these cases, the model initially provides plausible steps but eventually fails\ndue to its inability to correctly track or judge the game state after several steps (see Figure 9).\nThis suggests a breakdown in maintaining consistent reasoning over multi-step processes.\n• Other Errors: During error analysis, we observe that Claude-3.5 Sonnet and GPT-4o never\nrefused to answer queries, and all responses were accurately extracted. However, models\nsuch as GPT-4V displayed issues like refusal to respond or failure to adhere to the required\nresponse format, which hindered our ability to retrieve the outputs.\nPlanning Capacity Analysis. We select the game where models performed best—Maze—and\nintroduced three additional difficulty levels: 4 steps, 12 steps, and 16 steps, by adjusting only the\nnumber of moves required to complete the level, while maintaining the same level structure. This\nallowed us to closely examine the planning capabilities of the most advanced MLLMs, Claude-3.5\nSonnet and GPT-4o, as shown in FIgure 4. Our findings showed a significant decline in both accuracy\nand completion degree as the number of required steps increased. However, action efficiency, which\nemphasizes perception and judgment of the current state, was not notably affected, since modifying\nthe step count without altering the overall layout had little impact on this metric.\nComparative Analysis.\nWe compare the results across different metrics, settings, and models,\naiming to highlight the characteristics of current MLLMs.\n8\nPreprint\nFigure 5: An example showcasing Claude 3.5-Sonnet with a fixed output paradigm.\n• Results differ across metrics. Of the three metrics provided by ING-VP, accuracy—being\nthe most stringent—typically yields the lowest scores. The primary reason action efficiency\nis often significantly higher than both completion rate and accuracy is that models frequently\ngenerate instructions that alter the game state, but these changes have minimal impact on\nsuccessfully completing the level. A notable example is Gemini-1.5 Pro, which achieves an\naverage action efficiency of 76.52% on the 15-puzzle, yet only 0.67% and 3.42% in accuracy\nand completion rate, respectively.\n• Image-text vs. Text-only. Comparing the performance of each model in the image-text and\ntext-only settings, we found that most test subjects performed better in the text-only setting.\nThis highlights that limitations in image comprehension remain a key factor constraining\nthe performance of MLLMs.\n• Multi-step vs. One-step. According to the results in Table 3, for most models, multi-step\nsetting improves accuracy compared to one-step. However, there are exceptions, such as\nClaude-3.5 Sonnet. We compare the output of Claude-3.5 Sonnet and GPT-4o and find that,\ndespite we set the same parameters for closed-source models, Claude-3.5 Sonnet’s sampling\nstrategy is more fixed than GPT-4o’s. As a result, when the model produces an invalid\naction in a certain state, it tends to repeatedly generate the same action until all attempts are\nexhausted. GPT-4o, being more flexible, is better at generating diverse responses. Therefore,\nalthough Claude-3.5 Sonnet performs better than GPT-4o in one-step tasks, the opposite is\ntrue for multi-step tasks. One example is shown in Figure 5.\n• With-history vs. Without-history. In our tasks, incorporating the model’s historical output\nas the input for subsequent rounds did not lead to improved performance. Additionally, we\nintroduce an undo option for Sokoban, Sudoku, and N-Queens in the with-history setting.\nInterestingly, despite the models frequently reaching a state where undoing moves was\nnecessary to complete the level, almost none utilized this feature. This suggests that the\nmodels struggle with processing precise positional information and are unable to accurately\nassess whether the current state is solvable.\n5\nTWO THINKING ABOUT PLANNING\nA holistic approach may outperform a divide-and-conquer strategy. When humans are tasked\nwith completing a planning problem, whether in a single or multi-step process, it typically involves\nthree key phases: understanding the goal, devising a plan, and breaking down the steps. Large\n9\nPreprint\nmodels should operate similarly, yet when presented with the same game level, their outputs differ\nsignificantly between one-step and multi-step settings, as highlighted in Table 1. Notably, even the\ninitial steps diverge between the two approaches. To explore the planning capabilities of the model\nfurther, we employ two methods to adjust the multi-step output:\n• Step-wise Best of N (BoN): The model generates ten candidate responses at each step, with\nthe most frequent answer selected as the final output.\n• Forced Planning: The model is required to complete its entire plan before producing a final\nanswer, akin to the one-step setting.\nFigure 6: An example of results for the Claude-3.5 Sonnet in four settings.\nFIgure 6 illustrates an example of these methods in action, despite these adjustments, the multi-step\napproach failed to match the performance of the one-step setting. This suggests that, for the large\nmodels, even when given identical image, one-step and multi-step tasks are fundamentally different,\nwith the former better eliciting the model’s planning capabilities.\nSmall changes in the prompt phrasing can substantially influence the model’s planning effec-\ntiveness. A thorough comparison of single-step and multi-step outputs reveals not only differences\nbut also distinct tendencies. For instance, in Maze and Sokoban games, Claude-3.5 Sonnet favors ”U\n(Up)” and ”D (Down)” in the one-step mode, whereas it prefers ”L (Left)” and ”R (Right)” in the\nmulti-step mode. Given that most of the prompt wording remains consistent between the two settings,\nour results indicate that subtle variations can profoundly affect the model’s response distribution. We\nleave more detailed experiments as future work.\n6\nCONCLUSION\nIn this work, we introduce ING-VP, an interactive game-based vision planning benchmark designed\nto evaluate the spatial imagination and planning capabilities of MLLMs. Our experimental results\nreveal that even the most advanced MLLMs struggle to achieve satisfactory performance on game\ntasks that humans find trivial. This underperformance stems from multiple factors: existing models\noften fail to generate accurate perceptions of images, and they face even greater challenges in making\ninferences and plans based on their understanding. We believe that ING-VP is of noteworthy to\nthe community’s deeper understanding of MLLMs, and can also advance MLLMs’ capabilities in\ncomprehension and planning within visual contexts.\nLIMITATIONS\nDespite its strengths, ING-VP has certain limitations. We deliberately omit difficulty grading settings.\nIncluding simpler levels would significantly increase the likelihood of models completing tasks by\nchance after sufficient steps, potentially compromising the reliability of our results. Conversely,\nincorporating more challenging levels would yield little insight, given that MLLMs already struggle\nwith current difficulty levels, and could negatively impact inference efficiency. Furthermore, ING-VP\ndoes not exhaustively cover all possible game types. Instead, we focus on selecting well-known\nand representative games to ensure relevance and broad applicability. Finally, to address efficiency\nconcerns, we do not use images of previous states as input in the multi-step with history setting.\nThese considerations provide clear directions for future enhancements to our benchmark.\n10\nPreprint\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nNader Akoury, Qian Yang, and Mohit Iyyer. A framework for exploring player perceptions of llm-\ngenerated dialogue in commercial video games. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 2295–2311, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin\nJohnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler,\nTimothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald\nBarham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan\nDoherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha\nGoel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka,\nBecca Roelofs, Ana¨ıs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran\nKazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of\nhighly capable multimodal models. CoRR, abs\/2312.11805, 2023a.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023b.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international\nconference on computer vision, pp. 2425–2433, 2015.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253–279, 2013.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\nLu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and explor-\ning emergent behaviors. In The Twelfth International Conference on Learning Representations,\n2023.\nYew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puz-\nzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual\npatterns. arXiv preprint arXiv:2403.13315, 2024.\nWenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki,\nMohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms.\narXiv preprint, 2024.\nShiying Ding, Xinyi Chen, Yan Fang, Wenrui Liu, Yiwu Qiu, and Chunlei Chai. Designgpt: Multi-\nagent collaboration in design. In 2023 16th International Symposium on Computational Intelligence\nand Design (ISCID), pp. 204–208. IEEE, 2023.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng,\nSong-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction.\narXiv preprint arXiv:2309.09971, 2023.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\n11\nPreprint\nSihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu.\nA survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022.\nChuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, and Jianlong Fu.\nAlphablock: Embodied finetuning for vision-language reasoning in robot manipulation. arXiv\npreprint arXiv:2305.18898, 2023a.\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru\nSong, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual\ntokenization. arXiv preprint arXiv:2309.04669, 2023b.\nKushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In\nProceedings of the IEEE international conference on computer vision, pp. 1965–1973, 2017.\nBohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan.\nSeed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pp. 13299–13308, 2024.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Com-\nmunicative agents for” mind” exploration of large language model society. Advances in Neural\nInformation Processing Systems, 36:51991–52008, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning, pp. 19730–19742. PMLR, 2023b.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740–755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36, 2024.\nJijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, and Yu Wang. Llm-powered\nhierarchical language agent for real-time human-ai coordination. arXiv preprint arXiv:2312.15224,\n2023a.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\narXiv preprint arXiv:2307.06281, 2023b.\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,\nZhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.\nDeepseek-vl: Towards real-world vision-language understanding. CoRR, abs\/2403.05525, 2024.\nDavid Noever, Matt Ciolino, and Josh Kalin. The chess transformer: Mastering play using generative\nlanguage models. arXiv preprint arXiv:2008.04057, 2020.\nOpenAI. Gpt-4o system card. CoRR, 2024.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Social simulacra: Creating populated prototypes for social computing systems. In\nProceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pp.\n1–18, 2022.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and technology, pp. 1–22, 2023.\n12\nPreprint\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE international conference on computer\nvision, pp. 2641–2649, 2015.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6, 2023.\nAndreas St¨ockl. Watching a language model learning chess. In Proceedings of the International\nConference on Recent Advances in Natural Language Processing (RANLP 2021), pp. 1369–1379,\n2021.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context\nlearners. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npp. 14398–14409, 2024.\nPenny Sweetser. Large language models and video games: A preliminary scoping review. In\nProceedings of the 6th ACM Conference on Conversational User Interfaces, pp. 1–8, 2024.\nGabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timoth´ee Lacroix, Zeming\nLin, Florian Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on\nreal-time strategy games. arXiv preprint arXiv:1611.00625, 2016.\nMohammad Reza Taesiri, Finlay Macklon, Yihe Wang, Hengshuo Shen, and Cor-Paul Bezemer.\nLarge language models are pretty good zero-shot video game bug detectors. arXiv preprint\narXiv:2210.02506, 2022.\nMohammad Reza Taesiri, Tianjun Feng, Cor-Paul Bezemer, and Anh Nguyen. Glitchbench: Can\nlarge multimodal models detect video game glitches? In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22444–22455, 2024.\nShubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as a testbed for\nlanguage model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 11385–11393, 2022.\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei\nWang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through\nrecursive contemplation. arXiv preprint arXiv:2310.01320, 2023a.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,\nHongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting,\nand enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746,\n2023b.\nWenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei.\nVisualization-of-thought elicits spatial reasoning in large language models.\narXiv preprint\narXiv:2404.03622, 2024.\nYue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as\nintelligent agents. arXiv preprint arXiv:2310.01557, 2023.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models. arXiv preprint arXiv:2306.09265, 2023.\nXinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and B¨orje F Karlsson.\nA survey on game playing agents and large models: Methods, applications, and challenges. arXiv\npreprint arXiv:2403.10249, 2024.\nZhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang,\nZhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multi-modal instruction-tuning\ndataset, framework, and benchmark. Advances in Neural Information Processing Systems, 36,\n2024.\n13\nPreprint\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,\nBoyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for\nexpert AGI. CoRR, abs\/2311.16502, 2023.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal under-\nstanding and reasoning benchmark for expert agi. In Proceedings of the IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 9556–9567, 2024.\nGe Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang\nCheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi\nLi, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang,\nWenhu Chen, and Jie Fu. CMMMU: A chinese massive multi-discipline multimodal understanding\nbenchmark. CoRR, abs\/2401.11944, 2024a.\nJesse Zhang, Karl Pertsch, Jiahui Zhang, and Joseph J Lim. Sprint: Scalable policy pre-training\nvia language instruction relabeling. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 9168–9175. IEEE, 2024b.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and\nJi-Rong Wen. A survey of large language models. CoRR, abs\/2303.18223, 2023.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian\nZhang, Jing Chen, Ruipu Wu, Shuai Wang, et al.\nAgents: An open-source framework for\nautonomous language agents. arXiv preprint arXiv:2309.07870, 2023.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenvironments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n14\nPreprint\nA\nMODEL LIST\nList of all models involved in the ING-VP.\nOrganization\nModel\nAccess\nClosed Source Model\nOpenAI\nGPT-4o\nhttps:\/\/openai.com\/index\/hello-gpt-4o\/\nGPT-4o mini\nhttps:\/\/openai.com\/index\/gpt-4o-mini-advancing-cost-efficient-intelligence\/\nGPT-4v\nhttps:\/\/openai.com\/index\/gpt-4v-system-card\/\nGPT-4 Turbo\nhttps:\/\/platform.openai.com\/docs\/models\/gpt-4-turbo-and-gpt-4\nAnthropic\nClaude-3.5 Sonnet\nhttps:\/\/www.anthropic.com\/news\/claude-3-5-sonnet\nClaude-3 Opus\nhttps:\/\/www.anthropic.com\/news\/claude-3-family\nGoogle Deepmind\nGemini-1.5 Pro\nhttps:\/\/deepmind.google\/technologies\/gemini\/pro\/\nOpen Source Model\nShanghai AI Laboratory\nInternVL2-Llama3-76B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-Llama3-76B\nInternVL2-40B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-40B\nInternVL2-26B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-26B\nInternVL2-8B\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL2-8B\nInternVL-Chat-V1-5\nhttps:\/\/huggingface.co\/OpenGVLab\/InternVL-Chat-V1-5\nZhipu AI\nCogVLM2-Llama3-chat-19B\nhttps:\/\/github.com\/THUDM\/CogVLM2\nDeepSeek-AI\nDeepSeek-VL-7B-chat\nhttps:\/\/github.com\/deepseek-ai\/DeepSeek-VL\nModelBest Inc\nMiniCPM-V 2.6\nhttps:\/\/github.com\/OpenBMB\/MiniCPM-V\nTable 2: List of all models involved in the ING-VP.\nB\nPROMPTS\nThe following is the comprehensive list of 36 prompts utilized in our experiments.\nB.1\nMULTI-STEP WITH IMAGE WITHOUT HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an image of a level of the Tower of\nHanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\nPlease use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means move\nthe disk on rod-x to rod-y\nInstruction:\nPlease output only one step and your output must meet required format {”output”: ”{rod-\nx}{rod-y}”} and not output anything else:\n15\nPreprint\nMaze\nSystem:\nYou are a player of Maze game. And you will be given an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. Red area: your current position.\n2. Green area: destination.\n3. Black area: wall, unable to pass.\n4. White area: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\nInstruction:\nPlease output only one step, and your output must be one of the following: {”output”: ”L”}\nor {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”} and not output anything else:\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given an image of a level of the n-puzzle\ngame.\nPlease finish the n-puzzle based on the image provided.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\nInstruction:\nPlease output only one step and your output must meet required format {”output”: number}.\nPlease do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given an image of a level of the n-queens\ngame.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\n16\nPreprint\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n5. If your chess piece violates the three rules, it will be ignored.\nInstruction:\nPlease output only one step and your output must meet required format {”output”: [row,\ncol]}, and not output anything else:\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given an image of a level of the Sokoban\ngame.\nYour task is to complete this level by outputting movement instructions based on this image\none step at a time.\nObjective: Move all boxes onto the designated storage locations (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\n4. goal: push all the boxes onto the docks\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\nInstruction:\nPlease output only one step, and your output must be one of the following: ”output”: ”L” or\n”output”: ”R” or ”output”: ”U” or ”output”: ”D” and not output anything else:\n17\nPreprint\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given an image of a level of the Sudoku\ngame.\nPlease finish the sudoku puzzle based on the image provided, one step at a time.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nOutput Instructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number}}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nInstruction:\nPlease output only one step and your output must meet required format {”output”:\n{”{row}{column}”: {number}}}, and not output anything else:\nB.2\nMULTI-STEP TEXT-ONLY WITHOUT HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an dictionary representation of a\nlevel of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D\n2. And 5 disks: a, b, c, d, e; for size: a ¿ b ¿ c ¿ d ¿ e\n3. Your task is to move all the disks to rod ”D”\n4. Only one disk can be moved at a time\n5. Only the top disk can be moved\n6. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\nPlease use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means move\nthe disk on rod-x to rod-y\nInstruction:\nDictionary representation:\n{text-representation-path}\nPlease output only one step based on the given rules and dictionary representation, and\nyour output must meet required format {”output”: ”{rod-x}{rod-y}”}. Please do not output\nanything else.\n18\nPreprint\nMaze\nSystem:\nYou are a player of Maze game. And you will be given a text matrix of a level of the Maze\ngame.\nYour task is to move from your current position through the floor to the destination.\nInformation of text matrix:\n1. ’S’: your current position.\n2. ’X’: destination.\n3. ’+’: wall, unable to pass.\n4. ’ ’: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\nInstruction:\nText matrix:\n{text-representation-path}\nPlease output only one step based on the given rules and text matrix, and your output must be\none of the following: {”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”:\n”D”}. Please do not output anything else.\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given a list representation of a level of\nthe n-puzzle game.\nPlease finish the n-puzzle based on the list representation provided.\nIllustration of given list representation:\n1. The main list represents the board of size 4 * 4;\n2. The main list contains 4 sublist, each sublist represents a row, and contains 4\nelements;\n3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty\nspace is represented as 0;\n4. The goal is to rearrange the elements to [[1,2,3,4], [5,6,7,8], [9,10,11,12],\n[13,14,15,0]]\n5. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\nInstruction:\nList representation:\n{text-representation-path}\nPlease output only one step based on given list representation and your output must meet\nrequired format {”output”: number}. Please do not output anything else.\n19\nPreprint\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given a coordinate of the existing queens\nof a level of the n-queens game.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen is already given, so do not include it in your answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n5. If your chess piece violates the three rules, it will be ignored.\nInstruction:\nThe coordinate of the existing queens (including the first queen):\n{text-representation-path}\n1. first number: row index, range from 0 to 7\n2. second number: column index, range from 0 to 7\nPlease output only one step based on given coordinate and your output must meet required\nformat {”output”: [row, col]}. And do not output anything else.\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given a text matrix of a level of the\nSokoban game.\nYour task is to complete this level by outputting movement instructions based on the given\ntext matrix one step at a time.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration of given text matrix:\n1. ’.’: dock\n2. ’$’: box\n3. ’*’: box on the dock (can also be pushed)\n20\nPreprint\n4. ’@’: worker (or agent)\n5. ’+’: worker on the dock\n6. ’ ’: floor\n7. ’#’: wall\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\nInstruction:\nText matrix:\n{text-representation-path}\nPlease output only one step based on text matrix, and your output must be one of the following:\n{”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”}. And do not\noutput anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given a number string of a level of the\nSudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nIllustration of the given number string:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nInstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: ”output”: ”rowcolumn”: number.\n3. The range of row and column are 0-8, the range of number is 1-9.\nInstruction:\nNumber string:\n{text-representation-path}\nPlease output only one step based on given number string and your output must meet required\nformat {”output”: {”{row}{column}”: {number}}}. And do not output anything else:\n21\nPreprint\nB.3\nMULTI-STEP WITH IMAGE WITH HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an image of a level of the Tower of\nHanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means\nmove the disk on rod-x to rod-y,\n2. This is a multi-turn conversation. The conversation history provided below may be\nhelpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step and your output must meet required format {”output”: ”{rod-\nx}{rod-y}”} and not output anything else:\nMaze\nSystem:\nYou are a player of Maze game. And you will be given an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. Red area: your current position.\n2. Green area: destination.\n3. Black area: wall, unable to pass.\n4. White area: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\n22\nPreprint\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step, and your output must be one of the following: {”output”: ”L”}\nor {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”} and not output anything else:\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given an image of a level of the n-puzzle\ngame.\nPlease finish the n-puzzle based on the image provided.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nOutput Instructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\n3. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step and your output must meet required format {”output”: number}.\nPlease do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given an image of a level of the n-queens\ngame.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n23\nPreprint\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}.\n5. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n6. If your chess piece violates the three rules, it will be ignored.\n7. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\nconversation-history-path\nPlease output only one step and your output must meet required format {”output”: [row,\ncol]}, and not output anything else:\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given an image of a level of the Sokoban\ngame.\nYour task is to complete this level by outputting movement instructions based on this image\none step at a time.\nObjective: Move all boxes onto the designated storage locations (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\n4. goal: push all the boxes onto the docks\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n24\nPreprint\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step, and your output must be one of the following: ”output”: ”L” or\n”output”: ”R” or ”output”: ”U” or ”output”: ”D” and not output anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given an image of a level of the Sudoku\ngame.\nPlease finish the sudoku puzzle based on the image provided, one step at a time.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nOutput Instructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number}}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\n4. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n5. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nPlease output only one step and your output must meet required format {”output”:\n{”{row}{column}”: {number}}}, and not output anything else:\nB.4\nMULTI-STEP TEXT-ONLY WITH HISTORY\nHanoi\nSystem:\nYou are a player of Hanoi game. And you will be given an dictionary representation of a\nlevel of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nYou must follow the rules of Hanoi game:\n1. There are 4 rods: A, B, C, D\n2. And 5 disks: a, b, c, d, e; for size: a ¿ b ¿ c ¿ d ¿ e\n3. Your task is to move all the disks to rod ”D”\n4. Only one disk can be moved at a time\n25\nPreprint\n5. Only the top disk can be moved\n6. At no time should a large disk be placed on top of a small disk.\nInstructions:\n1. Use JSON as your output format: {”output”: ”{rod-x}{rod-y}”}, which means\nmove the disk on rod-x to rod-y\n2. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nDictionary representation:\n{text-representation-path}\nConversation history:\n{conversation-history-path}\nPlease output only one step based on the given rules and dictionary representation, and\nyour output must meet required format {”output”: ”{rod-x}{rod-y}”}. Please do not output\nanything else.\nMaze\nSystem:\nYou are a player of Maze game. And you will be given a text matrix of a level of the Maze\ngame.\nYour task is to move from your current position through the floor to the destination.\nInformation of text matrix:\n1. ’S’: your current position.\n2. ’X’: destination.\n3. ’+’: wall, unable to pass.\n4. ’ ’: floor, able to pass.\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n3. Output only one move at a time, wait for confirmation before proceeding to the next\nstep.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nText matrix:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on the given rules and text matrix, and your output must be\none of the following: {”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”:\n”D”}. Please do not output anything else.\n26\nPreprint\n15-puzzle\nSystem:\nYou are a player of n-puzzle game. And you will be given a list representation of a level of\nthe n-puzzle game.\nPlease finish the n-puzzle based on the list representation provided.\nIllustration of given list representation:\n1. The main list represents the board of size 4 * 4;\n2. The main list contains 4 sublist, each sublist represents a row, and contains 4\nelements;\n3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty\nspace is represented as 0;\n4. The goal is to rearrange the elements to [[1,2,3,4], [5,6,7,8], [9,10,11,12],\n[13,14,15,0]]\n5. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: number}.\n2. if the number is around the empty space, they will swap positions.\n3. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nList representation:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on given list representation and your output must meet\nrequired format {”output”: number}. Please do not output anything else.\n8-queens\nSystem:\nYou are a player of n-queens game. And you will be given a coordinate of the existing queens\nof a level of the n-queens game.\nYour task is to generate coordinates one at a time to complete the n-queens problem on a\nboard where the first queen is already placed.\nRules: Each queen must be placed in such a way that no two queens threaten each other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nInstructions:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen is already given, so do not include it in your answer.\n4. Output the coordinates of each queen one at a time in the JSON format: {”output”:\n[row, col]}\n27\nPreprint\n5. If your chess piece violates the three rules, it will be ignored.\n6. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nThe coordinate of the existing queens (including the first queen):\n{text-representation-path}\n1. first number: row index, range from 0 to 7\n2. second number: column index, range from 0 to 7\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on given coordinate and your output must meet required\nformat {”output”: [row, col]}. And do not output anything else.\nSokoban\nSystem:\nYou are a player of Sokoban game. And you will be given a text matrix of a level of the\nSokoban game.\nYour task is to complete this level by outputting movement instructions based on the given\ntext matrix one step at a time.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration of given text matrix:\n1. ’.’: dock\n2. ’$’: box\n3. ’*’: box on the dock (can also be pushed)\n4. ’@’: worker (or agent)\n5. ’+’: worker on the dock\n6. ’ ’: floor\n7. ’#’: wall\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. Use JSON as your output format: {”output”: ”L”} or {”output”: ”R”} or {”output”:\n”U”} or {”output”: ”D”}.\n28\nPreprint\n3. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n4. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nText matrix:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\nPlease output only one step based on text matrix, and your output must be one of the following:\n{”output”: ”L”} or {”output”: ”R”} or {”output”: ”U”} or {”output”: ”D”}. And do not\noutput anything else:\nSudoku\nSystem:\nYou are a player of Sudoku game. And you will be given a number string of a level of the\nSudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nIllustration of the given number string:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\nInstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: ”output”: ”rowcolumn”: number.\n3. The range of row and column are 0-8, the range of number is 1-9.\n4. If you think you are in an irreversible error state and want to return to the state at a\ncertain step in history, use: ”{”output”: {number}}”, where {number} is the step\nnumber.\n5. You will obtain a multi-turn conversation. The conversation history provided below\nmay be helpful to you.\nInstruction:\nNumber string:\n{text-representation-path}\nThis is a multi-turn conversation. The conversation history provided below may be helpful to\nyou.\nConversation history:\n{conversation-history-path}\n29\nPreprint\nPlease output only one step based on given number string and your output must meet required\nformat {”output”: {”{row}{column}”: {number}}}. And do not output anything else:\nB.5\nONE-STEP WITH IMAGE\nHanoi\nThis is an image of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the image provided.\nRules:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nNote:\n1. Use JSON as your output format: {”output”: [”AC”, ”AD”, ...]}, which means move\nthe top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.\nYour answer:\nMaze\nThis is an image of a level of the Maze game.\nYour task is to move from your current position through the floor to the destination.\nRules:\n1. red area: your current position\n2. green area: destination\n3. black area: wall, unable to pass\n4. white area: floor, able to pass\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\n15-puzzle\nThis is an image of a level of the n-puzzle game.\nYour task is to generate a list of numbers to complete the n-puzzle problem.\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: [number1, number2, number3, ...]}.\n30\nPreprint\n2. THe number1, number2, ... means if number1 is around the empty space, they will\nswap positions first; after that, if number2 is around the empty space, number2 and\nthe empty space will swap positions too, and so on.\nYour answer:\n8-queens\nThis is an image of a level of the n-queens game.\nYour task is to generate a list of coordinates to complete the n-queens problem on a board\nwhere the first queen is already placed.\nFollow these rules: Each queen must be placed in such a way that no two queens threaten\neach other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nNote:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n4. Your output should be in the JSON format: {”output”: [[row-x1, col-y1], [row-x2,\ncol-y2], ...]}. Each [row-x, col-y] means the coordinate you want to place your\npiece.\n5. If your chess piece violates the three rules, it will be ignored.\nYour answer:\nSokoban\nThis is an image of a level of the Sokoban game.\nYour task is to complete this level by outputting movement instructions based on this image.\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\n31\nPreprint\nYour answer:\nSudoku\nThis is an image of a level of the Sudoku game.\nPlease finish the sudoku puzzle based on the image provided.\nRules:\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\ninstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number},\n”{row}{column}”: {number}, ...}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nYour answer:\nB.6\nONE-STEP TEXT-ONLY\nHanoi\nThis is an dictionary representation of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nDictionary representation:\n{text-representation-path}\nRules:\n1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e\n2. Your task is to move all the disks to rod ”D”\n3. Only one disk can be moved at a time\n4. Only the top disk can be moved\n5. At no time should a large disk be placed on top of a small disk.\nNote:\n1. Use JSON as your output format: {”output”: [”AC”, ”AD”, ...]}, which means move\nthe top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.\nYour answer:\nMaze\nThis is an dictionary representation of a level of the Tower of Hanoi game.\nPlease finish the Tower of Hanoi puzzle based on the dictionary representation provided.\nDictionary representation:\n{text-representation-path}\nRules:\n1. red area: your current position\n2. green area: destination\n32\nPreprint\n3. black area: wall, unable to pass\n4. white area: floor, able to pass\nOutput Instructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\n15-puzzle\nThis is a list representation of a level of the n-puzzle game.\nPlease finish the n-puzzle based on the list representation provi\nded.\nList representation:\n{text-representation-path}\nRules:\n1. The board is a square grid of size 4 * 4;\n2. The board contains 15 numbered tiles and one empty space;\n3. The goal is to rearrange the tiles so that they are in ascending order from the top left\ncorner of the board;\n4. Valid moves are up, down, left, and right.\nInstructions:\n1. Use JSON as your output format: {”output”: [number1, number2, number3, ...]}.\n2. THe number1, number2, ... means if number1 is around the empty space, they will\nswap positions first; after that, if number2 is around the empty space, number2 and\nthe empty space will swap positions too, and so on.\nYour answer:\n8-queens\nThis is a level of the n-queens game.\nYour task is to generate coordinates to complete the n-queens problem on a board where the\nfirst queen is already placed.\nThe coordinate of the first queen:\n{text-representation-path}\nFollow these rules: Each queen must be placed in such a way that no two queens threaten\neach other.\n1. No two queens can share the same row.\n2. No two queens can share the same column.\n3. No two queens can share the same diagonal.\nNote:\n1. An 8 x 8 chessboard with 8 queens.\n2. The coordinate range is from 0 to 7.\n3. The position of the first queen (red color) is already given, so do not include it in\nyour answer.\n33\nPreprint\n4. Your output should be in the JSON format: {”output”: [[row-x1, col-y1], [row-x2,\ncol-y2], ...]}. Each [row-x, col-y] means the coordinate you want to place your\npiece.\n5. If your chess piece violates the three rules, it will be ignored.\nYour answer:\nSokoban\nThis is a text matrix of a level of the Sokoban game.\nYour task is to complete this level by outputting movement instructions based on this text\nmatrix.\nText matrix:\n{text-representation-path}\nObjective: Move all boxes onto the docks (goals).\nRules:\n1. Movement: The player can move up (U), down (D), left (L), or right (R).\n2. Pushing Boxes: The player can push one box at a time by moving towards it. Boxes\ncan only be pushed, not pulled.\n3. Grid Limitations: The player and boxes can only move into empty spaces. Walls\nand other boxes block movement.\nRestrictions:\n1. A box cannot be pushed if there is another box or a wall directly behind it.\n2. The player cannot move through boxes or walls.\nIllustration:\n1. dashed grid: dock\n2. yellow box: box on the dock (can also be pushed)\n3. brown box: box on the floor\nInstructions:\n1. Provide movement instructions using only the 4 letters: ”L” (left), ”R” (right), ”U”\n(up), ”D” (down).\n2. For example, if you want to move two cells down, three cells to the right, one cell\nup, and two cells to the left, the example output: {”output”: ”DDRRRULL”}\nYour answer:\nSudoku\nThis is a number string of a level of the Sudoku game.\nPlease finish the sudoku puzzle based on the number string provided, one step at a time.\nNumber string:\n{text-representation-path}\nIllustration:\n1. This string contains 81 numbers in total, ranges from 0 to 9.\n2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges\nfrom 1 to 9.\n3. the first number is the top left number, the last number is the bottom right number.\nRules:\n34\nPreprint\n1. In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9\nexactly once without repeating.\n2. You need to determine the number to fill in the blank based on the existing numbers.\ninstructions:\n1. The top left number is at row 0, column 0; the bottom right number is at row 8,\ncolumn 8.\n2. Use JSON as your output format: {”output”: {”{row}{column}”: {number},\n”{row}{column}”: {number}, ...}}.\n3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.\nYour answer:\nC\nDETAILED RESULTS\nModel\nSetting\nMaze Sokoban N-queens N-puzzle Hanoi Sudoku Overall\nClosed Source Model\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 27.80\n9.50\n0.00\n2.50\n0.50\n0.00\n6.70\nMulti-step\nw\/o history\nEff.\n5.60\n47.00\n3.30\n58.10\n0.60\n0.50\n19.20\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 17.20\n9.80\n0.00\n4.50\n0.00\n0.00\n5.20\nMulti-step\nw\/ history\nEff.\n18.60\n26.30\n3.00\n37.60\n0.00\n0.00\n14.20\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 36.50\n3.50\n4.00\n1.80\n0.20\n31.20\n12.90\nImage-text\nOne-step\nEff.\n38.20\n52.50\n58.80\n27.90\n12.70 11.90\n33.70\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 25.20\n6.50\n0.60\n1.00\n1.20\n0.00\n5.80\nMulti-step\nw\/o history\nEff.\n10.60\n9.60\n1.80\n89.40\n0.90\n0.10\n18.70\nAcc. 20.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.30\nComp. 23.20\n6.00\n0.00\n1.80\n1.50\n0.00\n5.40\nMulti-step\nw\/ history\nEff.\n11.60\n5.20\n1.80\n89.50\n1.50\n0.10\n18.30\nAcc. 12.00\n0.00\n8.00\n4.00\n0.00\n2.00\n4.30\nComp. 27.50\n4.50\n12.00\n10.50\n5.00\n23.00\n13.80\nText-only\nOne-step\nEff.\n31.40\n49.70\n72.00\n43.20\n62.30 28.30\n47.80\nAcc. 14.33\n0.00\n1.33\n0.67\n0.00\n0.33\n2.75\nComp. 26.23\n6.63\n2.77\n3.68\n1.40\n9.03\n8.30\nGPT-4o\nAverage\nEff.\n19.33\n31.72\n23.45\n57.62\n13.00\n6.82\n25.32\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 4.80\n7.20\n0.00\n4.20\n1.00\n0.00\n2.90\n35\nPreprint\nMulti-step\nw\/o history\nEff.\n3.60\n15.40\n2.80\n27.30\n2.50\n1.40\n8.80\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 7.50\n5.20\n0.30\n3.50\n1.00\n0.00\n2.90\nMulti-step\nw\/ history\nEff.\n13.60\n0.50\n2.80\n22.90\n2.40\n1.10\n7.20\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.30\nComp. 19.20\n6.50\n0.00\n0.00\n0.00\n0.00\n4.30\nImage-text One-step\nEff.\n32.90\n0.00\n0.00\n0.00\n0.00\n0.00\n5.50\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 4.50\n7.50\n0.00\n1.00\n2.50\n0.00\n2.60\nMulti-step\nw\/o history\nEff.\n3.60\n5.60\n1.80\n86.60\n2.00\n1.00\n16.80\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 5.00\n8.00\n0.30\n2.00\n2.50\n0.00\n3.00\nMulti-step\nw\/ history\nEff.\n9.30\n5.60\n1.80\n86.00\n1.10\n0.80\n17.40\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 13.80\n6.80\n0.00\n0.00\n0.00\n0.00\n3.40\nText-only\nOne-step\nEff.\n7.90\n43.10\n0.00\n0.00\n0.00\n0.00\n8.50\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.32\nComp. 9.13\n6.87\n0.10\n1.78\n1.17\n0.00\n3.18\nGPT-4V\nAverage\nEff.\n11.82\n11.70\n1.53\n37.13\n1.33\n0.72\n10.70\nAcc.\n4.00\n0.00\n0.00\n2.00\n0.00\n0.00\n1.00\nComp. 19.80\n9.00\n0.30\n4.00\n2.00\n0.00\n5.90\nMulti-step\nw\/o history\nEff.\n25.10\n57.10\n3.30\n95.90\n23.80\n3.00\n34.70\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 10.50\n6.50\n0.00\n5.20\n0.50\n0.00\n3.80\nMulti-step\nw\/ history\nEff.\n20.20\n48.40\n4.10\n87.50\n5.30\n1.40\n27.80\nAcc. 10.00\n6.00\n0.00\n0.00\n0.00\n0.00\n2.70\nComp. 20.80\n13.80\n4.00\n3.20\n4.80\n11.00\n9.60\nImage-text\nOne-step\nEff.\n35.30\n58.80\n61.00\n55.70\n38.60\n7.10\n42.80\nAcc. 34.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.70\nComp. 43.20\n4.50\n0.60\n0.80\n0.20\n0.00\n8.20\nMulti-step\nw\/o history\nEff.\n16.10\n3.40\n2.50\n94.00\n0.40\n0.60\n19.50\nAcc. 26.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.30\nComp. 32.80\n4.00\n0.30\n0.80\n1.00\n0.00\n6.50\nMulti-step\nw\/ history\nEff.\n8.60\n3.40\n2.30\n95.60\n0.50\n0.50\n18.50\nAcc. 10.00\n2.00\n2.00\n0.00\n0.00\n0.00\n2.30\n36\nPreprint\nComp. 24.80\n6.20\n2.00\n6.50\n3.00\n8.20\n8.50\nText-only\nOne-step\nEff.\n33.70\n55.50\n64.80\n30.40\n37.50\n4.50\n37.70\nAcc. 14.00\n0.00\n1.33\n0.67\n0.00\n0.33\n2.72\nComp. 25.32\n7.33\n1.20\n3.42\n1.92\n3.20\n7.08\nAverage\nEff.\n23.17\n37.77\n23.00\n76.52\n17.68\n2.85\n30.17\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\nComp. 10.00\n5.80\n0.00\n1.80\n3.00\n0.00\n3.40\nMulti-step\nw\/o history\nEff.\n2.70\n21.40\n1.70\n35.60\n17.40\n0.40\n13.20\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\nComp. 8.50\n6.50\n0.00\n2.00\n3.50\n0.00\n3.40\nMulti-step\nw\/ history\nEff.\n2.00\n0.00\n1.40\n23.50\n22.10\n0.40\n8.20\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 20.80\n8.20\n0.00\n3.00\n6.80\n1.00\n6.60\nImage-text\nOne-step\nEff.\n34.80\n64.70\n58.20\n32.80\n18.50\n2.50\n35.20\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n,2.0\nComp. 22.00\n7.00\n0.30\n0.20\n1.80\n0.00\n5.20\nMulti-step\nw\/o history\nEff.\n7.80\n8.30\n1.60\n95.30\n3.50\n0.40\n19.50\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 23.80\n8.80\n0.00\n1.20\n1.50\n0.00\n5.90\nMulti-step\nw\/ history\nEff.\n8.40\n4.90\n1.80\n86.10\n2.50\n0.30\n17.30\nAcc.\n2.00\n0.00\n2.00\n0.00\n0.00\n2.00\n1.00\nComp. 26.20\n6.50\n2.00\n4.80\n4.20\n9.80\n8.90\nText-only\nOne-step\nEff.\n27.70\n52.40\n70.80\n36.00\n42.70 11.20\n40.10\nAcc.\n5.67\n0.00\n0.33\n0.00\n0.00\n0.33\n1.05\nComp. 18.55\n7.13\n0.38\n2.17\n3.47\n1.80\n5.57\nGPT-4o mini\nAverage\nEff.\n13.90\n25.28\n22.58\n51.55\n17.78\n2.53\n22.25\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 10.00\n5.80\n0.00\n7.20\n0.20\n0.00\n3.90\nMulti-step\nw\/o history\nEff.\n25.10\n33.60\n2.00\n86.80\n8.40\n5.50\n26.90\nAcc.\n0.00\n0.00\n0.00\n2.00\n0.00\n0.00\n0.30\nComp. 8.00\n6.20\n0.00\n11.20\n0.20\n0.00\n4.30\nMulti-step\nw\/ history\nEff.\n37.00\n37.30\n2.80\n49.10\n9.00\n3.60\n23.10\nAcc. 28.00\n2.00\n4.00\n4.00\n0.00\n4.00\n7.00\nComp. 55.00\n5.50\n4.00\n13.80\n6.50\n46.60\n21.90\nImage-text\nOne-step\nEff.\n51.30\n63.40\n60.20\n52.50\n26.40 36.90\n48.40\n37\nPreprint\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.80\n6.80\n2.30\n0.20\n1.20\n0.00\n4.90\nMulti-step\nw\/o history\nEff.\n4.40\n4.60\n1.60\n92.40\n1.80\n0.60\n17.60\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 21.20\n6.20\n1.40\n0.00\n2.50\n0.00\n5.20\nMulti-step\nw\/ history\nEff.\n4.60\n4.00\n1.70\n98.80\n1.60\n0.40\n18.50\nAcc. 28.00\n0.00\n18.00\n2.00\n0.00\n0.00\n8.00\nComp. 41.20\n10.00\n28.00\n11.50\n8.20\n1.60\n16.80\nText-only\nOne-step\nEff.\n37.50\n61.70\n76.80\n41.30\n30.90\n3.80\n42.00\nAcc. 14.00\n0.33\n3.67\n1.67\n0.00\n0.67\n3.37\nComp. 25.70\n6.75\n5.95\n7.32\n3.13\n8.03\n9.50\nClaude-3.5 Sonnet\nAverage\nEff.\n26.65\n34.10\n24.18\n70.15\n13.02\n8.47\n29.42\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.20\n8.00\n0.90\n0.50\n1.50\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n5.40\n3.10\n1.70\n62.80\n1.20\n0.40\n12.40\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 19.20\n7.20\n0.60\n0.00\n1.50\n0.00\n4.80\nMulti-step\nw\/ history\nEff.\n4.50\n3.60\n1.90\n62.00\n1.30\n0.60\n12.30\nAcc.\n0.00\n2.00\n2.00\n2.00\n0.00\n0.00\n1.00\nComp. 40.50\n4.80\n6.00\n7.50\n4.20\n1.20\n10.70\nText-only\nOne-step\nEff.\n40.20\n55.70\n71.00\n47.00\n27.70\n3.00\n40.80\nAcc.\n9.33\n0.67\n0.67\n0.67\n0.00\n0.00\n1.87\nComp. 25.97\n6.67\n2.50\n2.67\n2.40\n0.40\n5.07\nClaude-3 Opus\nAverage\nEff.\n16.70\n20.80\n24.87\n57.27\n10.07\n1.33\n21.83\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 18.20\n8.00\n0.90\n0.20\n1.50\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n5.30\n3.10\n1.70\n61.80\n1.20\n0.40\n12.20\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.30\nComp. 19.20\n7.20\n0.60\n0.00\n1.50\n0.00\n4.80\nMulti-step\nw\/ history\nEff.\n4.50\n3.60\n1.80\n62.00\n1.30\n0.60\n12.30\nAcc.\n0.00\n2.00\n2.00\n2.00\n0.00\n0.00\n1.00\nComp. 35.00\n5.00\n2.00\n7.00\n4.20\n1.20\n9.10\nText-only\nOne-step\nEff.\n38.70\n56.30\n69.00\n48.60\n29.80\n3.80\n41.00\nAcc.\n9.33\n0.67\n0.67\n0.67\n0.00\n0.00\n1.87\nComp. 24.13\n6.73\n1.17\n2.40\n2.40\n0.40\n6.23\n38\nPreprint\nGPT-4 Turbo\nAverage\nEff.\n16.17\n21.00\n24.17\n57.47\n10.77\n1.60\n21.83\nOpen Source Model\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 15.20\n6.50\n0.00\n1.00\n0.00\n0.00\n3.78\nMulti-step\nw\/o history\nEff.\n22.50\n25.10\n8.20\n11.30\n0.00\n0.00\n11.18\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 13.20\n6.80\n0.00\n0.00\n0.00\n0.00\n3.33\nMulti-step\nw\/ history\nEff.\n19.80\n26.00\n7.30\n9.80\n0.80\n0.00\n10.62\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 16.80\n4.50\n1.20\n0.00\n2.50\n0.00\n4.17\nImage-text\nOne-step\nEff.\n25.20\n22.60\n33.80\n5.30\n19.50\n0.00\n17.73\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 14.00\n5.50\n0.00\n1.20\n1.00\n0.00\n3.62\nMulti-step\nw\/o history\nEff.\n16.30\n27.70\n3.20\n12.10\n1.20\n0.00\n10.08\nAcc.\n0.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 11.20\n4.00\n0.10\n0.80\n0.00\n0.00\n2.68\nMulti-step\nw\/ history\nEff.\n13.30\n17.80\n1.60\n5.50\n0.00\n0.00\n6.37\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 14.00\n5.80\n2.00\n0.00\n3.50\n0.00\n4.22\nText-only\nOne-step\nEff.\n24.10\n57.20\n30.00\n0.00\n20.00\n0.00\n21.88\nAcc.\n1.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.22\nComp. 14.07\n5.52\n0.55\n0.50\n1.17\n0.00\n3.63\nMiniCPM-V2.6\nAverage\nEff.\n20.20\n29.40\n14.02\n7.33\n6.92\n0.00\n12.98\nAcc.\n6.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 11.00\n4.50\n0.10\n0.00\n0.00\n0.00\n2.60\nMulti-step\nw\/o history\nEff.\n17.90\n11.10\n3.40\n3.00\n0.00\n0.00\n5.90\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 10.00\n5.50\n0.00\n0.00\n0.00\n0.00\n2.58\nMulti-step\nw\/ history\nEff.\n16.60\n10.90\n2.80\n1.30\n0.00\n0.00\n5.27\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 14.20\n5.00\n0.00\n0.80\n0.00\n0.00\n3.33\nImage-text\nOne-step\nEff.\n10.40\n12.00\n3.30\n4.10\n0.00\n0.00\n4.97\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 11.80\n4.00\n0.00\n0.00\n0.00\n0.00\n2.63\nMulti-step\nw\/o history\nEff.\n13.20\n2.30\n0.00\n2.80\n0.00\n0.00\n3.05\n39\nPreprint\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 9.20\n5.80\n0.00\n0.00\n0.00\n0.00\n2.50\nMulti-step\nw\/ history\nEff.\n13.10\n6.60\n1.50\n4.40\n0.00\n0.00\n4.27\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 15.80\n7.20\n0.00\n0.00\n0.00\n0.00\n3.83\nText-only\nOne-step\nEff.\n17.70\n10.10\n0.00\n8.40\n0.00\n0.00\n6.03\nAcc.\n5.33\n0.00\n0.00\n0.00\n0.00\n0.00\n0.89\nComp. 12.00\n5.33\n0.02\n0.13\n0.00\n0.00\n2.91\nInternvl2-8B\nAverage\nEff.\n14.82\n8.83\n1.83\n4.00\n0.00\n0.00\n4.91\nAcc. 12.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 18.50\n7.80\n0.30\n2.00\n0.20\n0.00\n4.80\nMulti-step\nw\/o history\nEff.\n25.00\n19.70\n2.20\n14.90\n1.20\n0.50\n10.58\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 19.00\n8.00\n0.30\n2.50\n1.50\n0.00\n5.22\nMulti-step\nw\/ history\nEff.\n23.10\n17.00\n1.90\n12.10\n1.20\n0.00\n9.22\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 21.20\n8.50\n1.20\n2.50\n0.50\n0.00\n5.65\nImage-text\nOne-step\nEff.\n27.70\n18.60\n15.50\n7.40\n2.40\n0.00\n11.93\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 20.20\n9.50\n0.00\n1.80\n0.00\n0.00\n5.25\nMulti-step\nw\/o history\nEff.\n21.80\n19.80\n2.50\n16.00\n0.00\n1.20\n10.22\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 20.00\n10.50\n0.10\n1.00\n0.00\n0.00\n5.27\nMulti-step\nw\/ history\nEff.\n20.10\n21.20\n1.30\n10.80\n0.00\n2.20\n9.27\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 22.00\n8.00\n0.00\n0.80\n0.50\n0.00\n5.22\nText-only\nOne-step\nEff.\n25.40\n14.10\n43.30\n14.90\n1.60\n1.00\n16.72\nAcc. 11.00\n0.33\n0.00\n0.00\n0.00\n0.00\n1.89\nComp. 20.15\n8.72\n0.32\n1.77\n0.45\n0.00\n5.23\nInternvl2-26B\nAverage\nEff.\n23.85\n18.40\n11.12\n12.68\n1.07\n0.82\n11.32\nAcc.\n8.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 21.20\n6.80\n0.60\n4.50\n1.00\n0.00\n5.68\nMulti-step\nw\/o history\nEff.\n33.50\n40.40\n1.60\n29.70\n0.80\n4.20\n18.37\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 18.80\n7.00\n0.30\n6.50\n0.00\n0.00\n5.43\n40\nPreprint\nMulti-step\nw\/ history\nEff.\n31.00\n18.40\n1.20\n24.30\n0.00\n3.00\n12.98\nAcc. 16.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.67\nComp. 30.00\n7.50\n2.00\n6.50\n1.20\n0.00\n7.87\nImage-text\nOne-step\nEff.\n42.40\n42.60\n18.80\n28.10\n1.40\n0.00\n22.22\nAcc.\n4.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 19.20\n9.80\n0.00\n1.20\n0.00\n0.00\n5.03\nMulti-step\nw\/o history\nEff.\n27.20\n41.10\n0.80\n20.90\n0.00\n2.00\n15.33\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 12.50\n7.50\n0.00\n4.00\n0.50\n0.00\n4.08\nMulti-step\nw\/ history\nEff.\n24.10\n39.00\n1.80\n25.80\n0.60\n0.00\n15.22\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 23.80\n9.50\n4.00\n11.20\n0.00\n0.00\n8.08\nText-only\nOne-step\nEff.\n29.90\n44.70\n55.40\n38.40\n2.40\n0,6\n34.16\nAcc. 10.00\n0.67\n0.00\n0.00\n0.00\n0.00\n1.78\nComp. 20.92\n8.02\n1.15\n5.65\n0.45\n0.00\n6.03\nInternvl2-40B\nAverage\nEff.\n31.35\n37.70\n13.27\n27.87\n0.87\n1.84\n18.82\nAcc.\n0.00\n4.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 28.30\n9.50\n0.00\n0.00\n0.00\n0.00\n6.30\nMulti-step\nw\/o history\nEff.\n36.40\n42.20\n3.20\n6.70\n0.00\n0.90\n14.90\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 29.20\n8.00\n0.60\n0.00\n0.00\n0.00\n6.30\nMulti-step\nw\/ history\nEff.\n38.80\n39.30\n3.10\n3.20\n0.00\n0.90\n14.22\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 18.50\n6.50\n1.20\n1.00\n0.20\n0.00\n4.57\nImage-text\nOne-step\nEff.\n26.00\n36.70\n58.10\n26.30\n1.10\n5.90\n25.68\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 27.00\n7.00\n0.60\n0.00\n0.20\n0.00\n5.80\nMulti-step\nw\/o history\nEff.\n34.50\n27.60\n4.20\n3.00\n0.40\n0.50\n11.70\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 29.50\n6.50\n0.00\n0.00\n0.00\n0.00\n6.00\nMulti-step\nw\/ history\nEff.\n39.10\n22.10\n1.60\n2.40\n0.00\n0.00\n10.87\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 13.50\n4.80\n4.00\n1.80\n1.00\n0.00\n4.18\nText-only\nOne-step\nEff.\n23.90\n38.70\n59.50\n33.30\n2.40\n5.80\n27.27\nAcc.\n1.67\n0.67\n0.00\n0.00\n0.00\n0.00\n0.39\n41\nPreprint\nComp. 24.33\n7.05\n1.07\n0.47\n0.23\n0.00\n5.53\nAverage\nEff.\n33.12\n34.43\n21.62\n12.48\n0.65\n2.33\n17.44\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 9.00\n8.00\n0.10\n2.50\n1.20\n0.00\n3.47\nMulti-step\nw\/o history\nEff.\n14.40\n28.00\n2.50\n24.80\n0.50\n0.60\n11.80\nAcc.\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 8.50\n6.00\n0.60\n1.00\n0.20\n0.00\n2.72\nMulti-step\nw\/ history\nEff.\n13.40\n26.60\n1.70\n25.00\n0.60\n0.00\n11.22\nAcc.\n6.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nComp. 10.80\n6.50\n0.60\n4.00\n0.00\n0.00\n3.65\nImage-text\nOne-step\nEff.\n16.70\n30.50\n29.90\n21.30\n0.00\n0.00\n16.40\nAcc.\n0.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.33\nComp. 8.00\n6.80\n0.00\n0.80\n0.50\n0.00\n2.68\nMulti-step\nw\/o history\nEff.\n11.50\n32.00\n2.40\n3.30\n1.10\n0.00\n8.38\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 11.80\n5.80\n0.00\n6.50\n1.00\n0.00\n4.18\nMulti-step\nw\/ history\nEff.\n16.10\n25.40\n0.00\n15.20\n0.70\n0.00\n9.57\nAcc.\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nComp. 12.00\n7.50\n0.00\n4.00\n0.00\n0.00\n3.92\nText-only\nOne-step\nEff.\n17.60\n27.00\n15.50\n35.30\n0.00\n0.00\n15.90\nAcc.\n2.00\n0.33\n0.00\n0.00\n0.00\n0.00\n0.39\nComp. 10.02\n6.77\n0.22\n3.13\n0.48\n0.00\n3.44\nDeepSeek-VL\nAverage\nEff.\n14.95\n28.25\n8.67\n20.82\n0.48\n0.10\n12.21\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 25.50\n7.50\n0.00\n1.20\n1.20\n0.00\n5.90\nMulti-step\nw\/o history\nEff.\n33.10\n45.30\n1.10\n11.10\n2.80\n1.10\n15.75\nAcc.\n4.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.67\nComp. 23.80\n8.50\n0.00\n0.80\n1.00\n0.00\n5.68\nMulti-step\nw\/ history\nEff.\n33.20\n47.70\n0.90\n14.10\n2.10\n0.70\n16.45\nAcc. 12.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 27.00\n8.80\n1.20\n2.50\n0.00\n0.00\n6.58\nImage-text\nOne-step\nEff.\n39.50\n39.80\n47.50\n28.80\n0.00\n7.10\n27.12\nAcc.\n8.00\n2.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 24.50\n6.00\n0.60\n3.00\n0.00\n0.00\n5.68\nMulti-step\nw\/o history\nEff.\n33.80\n30.50\n3.60\n14.60\n0.00\n0.00\n13.75\n42\nPreprint\nAcc.\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.33\nComp. 20.50\n7.00\n0.60\n1.00\n1.00\n0.00\n5.02\nMulti-step\nw\/ history\nEff.\n31.00\n31.40\n2.30\n10.50\n0.50\n1.40\n12.85\nAcc. 10.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.00\nComp. 26.00\n7.50\n0.60\n10.50\n1.20\n0.00\n7.63\nText-only\nOne-step\nEff.\n37.20\n40.20\n61.10\n36.80\n5.10\n7.80\n31.37\nAcc.\n8.33\n0.67\n0.00\n0.00\n0.00\n0.00\n1.50\nComp. 24.55\n7.55\n0.50\n3.17\n0.73\n0.00\n6.08\nCogvlm2-19B\nAverage\nEff.\n34.63\n39.15\n19.42\n19.32\n1.75\n3.02\n19.55\nAcc. 12.00\n4.00\n0.00\n0.00\n0.00\n0.00\n2.67\nComp. 40.50\n8.80\n0.10\n4.50\n0.50\n0.00\n9.07\nMulti-step\nw\/o history\nEff.\n47.20\n33.60\n4.20\n17.80\n1.50\n1.00\n17.55\nAcc. 12.00\n2.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 30.20\n4.50\n0.30\n2.50\n0.20\n0.00\n6.28\nMulti-step\nw\/ history\nEff.\n38.60\n28.90\n3.90\n15.30\n1.60\n2.50\n15.13\nAcc. 18.00\n0.00\n0.00\n0.00\n0.00\n0.00\n3.00\nComp. 42.20\n4.80\n0.60\n1.20\n1.00\n0.00\n8.30\nImage-text One-step\nEff.\n50.90\n55.40\n42.30\n50.00\n8.60\n9.90\n36.18\nAcc. 14.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.33\nComp. 41.80\n6.80\n0.30\n0.80\n0.20\n0.00\n8.32\nMulti-step\nw\/o history\nEff.\n49.40\n49.20\n5.50\n20.70\n1.90\n0.10\n21.13\nAcc. 10.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.67\nComp. 37.00\n9.50\n0.00\n1.20\n0.50\n0.00\n8.03\nMulti-step\nw\/ history\nEff.\n41.00\n52.50\n4.90\n55.30\n10.00 12.10\n29.30\nAcc. 14.00\n4.00\n0.00\n0.00\n0.00\n0.00\n3.00\nComp. 18.80\n9.00\n2.00\n4.50\n1.00\n0.00\n5.88\nText-only\nOne-step\nEff.\n29.00\n52.00\n47.70\n49.00\n9.90\n10.10\n32.95\nAcc. 13.33\n1.67\n0.00\n0.00\n0.00\n0.00\n2.50\nComp. 35.08\n7.23\n0.55\n2.45\n0.57\n0.00\n7.65\nInternVL2-\nLlama3-76B\nAverage\nEff.\n42.68\n45.27\n18.08\n34.68\n5.58\n5.95\n25.38\nTable 3: Results for all of the MLLMs\nD\nCASE STUDY\n43\nPreprint\nFigure 7: A sample case of perceptual error. Sokoban — One-step — Image-text — Level 2.\n44\nPreprint\nFigure 8: A sample case of textual understanding error. Hanoi — Multi-step — Text-only — With-\nhistory — Level 13.\n45\nPreprint\nFigure 9: A sample case of three errors. 8-queens — One-step — Image-text — Level 8.\n46\nPreprint\nFigure 10: A sample case of output comparison. Maze — Multi-step — Image-text — Without-\nhistory — Level 33.\n47\nPreprint\nFigure 11: A sample case of output comparison. 15-Puzzle — Multi-step — Image-text — Without-\nhistory — Level 45.\n48\nPreprint\nFigure 12: A sample case of output comparison. Sudoku — One-step — Image-text — Level 2.\n49\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | ING-VP：大型多模态语言模型在视觉规划任务上的挑战\n\n## 📌 背景痛点\/本文动机\n随着多模态大型语言模型（MLLMs）在各类任务中展现出越来越强的竞争力，评估这些模型的复杂性和全面性的基准测试也变得越来越重要。然而，现有的多模态基准测试在评估基于图像空间关系的多步规划能力方面存在不足。为了填补这一空白，本文提出了ING-VP，一个基于交互式游戏的视觉规划基准测试，旨在评估MLLMs的空间想象力和多步推理能力。\n\n## 🚀 核心方法\n💡 创新点1：ING-VP基准测试\nING-VP基准测试包含6个不同的游戏，涵盖300个关卡，每个关卡有6种独特的配置。单个模型参与超过60,000轮交互。基准测试框架允许多种比较设置，包括图像-文本与仅文本输入、单步与多步推理以及带历史与不带历史条件，为模型的能力提供有价值的见解。\n\n💡 创新点2：评估MLLMs的能力\n本文评估了众多最先进的MLLMs，发现即使是表现最好的模型Claude-3.5 Sonnet，其平均准确率也只有3.37%，远低于预期标准。这表明当前的MLLMs在空间想象力和多步规划能力方面存在显著不足。\n\n## 📈 实验结果\n实验结果表明，即使是表现最好的模型Claude-3.5 Sonnet，在ING-VP基准测试上的平均准确率也只有3.37%，远低于人类在这些简单任务上的表现。这表明当前的MLLMs在空间想象力和多步规划能力方面存在显著不足。\n\n## 💬 可借鉴之处\nING-VP基准测试为评估MLLMs的空间想象力和多步推理能力提供了一个专门的评估框架，有助于推动MLLMs在复杂空间推理和规划能力方面的进步。此外，ING-VP基准测试的结果也为MLLMs的未来设计和训练策略提供了有价值的见解。","llm_summary_res_status":200}
{"title":"Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs","authors":"Yilun Hua, Yoav Artzi","summary":"Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps:\/\/github.com\/lil-lab\/ICCA.","url":"http:\/\/arxiv.org\/abs\/2408.01417v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.01417v1","published":1722621117000,"comment":"Accepted to COLM 2024","pdf_text":"Published as a conference paper at COLM 2024\nTalk Less, Interact Better: Evaluating In-context\nConversational Adaptation in Multimodal LLMs\nYilun Hua and Yoav Artzi\nDepartment of Computer Science and Cornell Tech\nCornell University\n{yilunhua, yoav}@cs.cornell.edu\nAbstract\nHumans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon\nhas been studied extensively using reference games, showing properties of\nhuman language that go beyond relaying intents. It remains unexplored\nwhether multimodal large language models (MLLMs) similarly increase\ncommunication efficiency during interactions, and what mechanisms they\nmay adopt for this purpose. We introduce ICCA, an automated framework\nto evaluate such conversational adaptation as an in-context behavior in\nMLLMs. We evaluate several state-of-the-art MLLMs, and observe that\nwhile they may understand the increasingly efficient language of their\ninterlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property\nof linguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language.\n1\nIntroduction\nHuman interlocutors adapt to each other during interactions, developing increasingly\nefficient ways to refer to concepts and objects. Hawkins et al. (2020b) exemplify this via\ncommunication between a nurse and a bed-ridden patient at home. Initially, the patient\nmay refer to a medicine with the medicine for my back pain in a small blue medicine bottle ..., but\nafter a week of care, they are likely to just ask for their back meds. This increase in efficiency\nrelies on the interlocutors forming ad-hoc linguistic conventions: the mutually understood,\nconcise phrases to communicate referential content. This phenomenon has been repeatedly\nobserved and characterized in controlled studies using repeated reference games (Figure 1;\ne.g., Krauss & Weinheimer, 1964; Brennan & Clark, 1996; Hawkins et al., 2020a).\nWe study this ability in multimodal large language models (MLLMs). LLMs and MLLMs are\nwell positioned to acquire this behavior and display it spontaneously in interactions. They\nare trained on large amounts of human language data, in which this behavior is common\nand the history of an ongoing interaction is often retained, thereby explicitly keeping the\ninformation needed at hand. Beyond the scientific question, such ad-hoc adaptation has\nsignificant application impacts: enabling more natural interactions, reducing the costs\ninvolved in conversations (e.g., using shorter utterances to communicate the same amount\nof information), and increasing the accuracy of relaying intent.\nWe propose ICCA,1 an automated framework to evaluate and characterize the ability of\nmodels to form ad-hoc conventions. ICCA uses a corpus of human-human reference game\ninteractions, allowing for completely automated evaluation, which does not require further\nhuman interaction, making it easy to deploy for the analysis of new models. The interaction\nfollows the standard repeated reference game setup (Clark & Wilkes-Gibbs, 1986), where\na speaker refers to an image within a shared context of images, and a listener resolves the\n1ICCA stands for In-context Conversational Adaptation.\n1\narXiv:2408.01417v1  [cs.CL]  2 Aug 2024\nPublished as a conference paper at COLM 2024\nPhoto with a bowl of 3 \nbananas with pokadot \nA\nB\nC\nD\nImage B\nA bowl full of mixed fruit, \nblack background\nImage A\nContext\nRepetition 1\nRepetition 6\nblack\nImage A\npokadot\nImage B\nFigure 1: Illustration of a reference game. The speaker (blue) and listener (orange) observe a\nshared set of images.2The interaction progresses in six repetitions, each includes a trial for\nevery context image. In each trial, the speaker describes a target image, and the listener has\nto select the correct target given the description only. For simplicity, this figure omits the\nfeedback on listener actions. This interaction illustrates some of the effects of convention\nformation: the descriptions become shorter as the interaction progresses, and lexical choices\nconverge to a subset of the words used in earlier repetitions.\nreference to select one of the images, ideally the one originally referred to. Figure 1 illustrates\nthe scenario. We focus on in-context adaptation – as the interaction progresses, the entire\nhistory is retained in-context. Core to our approach is comparing the changes in model\nbehavior, either as speaker or listener, throughout an interaction to the changes observed\nin humans. We measure different properties that have been shown to be influenced by\nconvention formation: utterance length, lexical convergence, and selection accuracy.\nWe apply our approach to five representative MLLMs: IDEFICS (Huggingface, 2023), LLaVa-\n1.5 (Liu et al., 2023a), GPT4-vision (OpenAI et al., 2024), Gemini 1.0 Pro Vision (Google, 2023),\nand Claude 3 opus (Anthropic, 2024). We find that all models struggle to spontaneously\nintroduce conventions and adapt as speakers. Prompt engineering an explicit in-context\ninstruction specific to the reference game scenario can address this to some degree. The\nstrongest models (GPT4, Gemini, and Claude) can then gradually use shorter messages\n(gaining lexical efficiency) but still struggle with convergence or stability, which hinders\nthe emergence of truly efficient communication. When acting as a listener, GPT4 displays\nadaptation trends close to humans, improving its accuracy as the interaction progresses,\nwhile other models show this behavior to a lesser degree or only under some simplified\nsetups. Overall, we show that while today’s MLLMs may passively understand the evolv-\ning language of their interlocutor, the ability to adapt their own language for efficient\ncommunication does not naturally emerge from their training or instruction-tuning. This\noutlines important future research problems. We release ICCA under the MIT license at\nhttps:\/\/github.com\/lil-lab\/ICCA.\n2\nBackground and Related Work\nRepeated Reference Games\nA reference game is an interaction where a speaker and a\nlistener (i.e., a dyad) interact over a shared context. The shared context is a set of images.\nThe speaker describes a target image. The target designation is only revealed to the speaker.\nThe listener has to select an image following the speaker’s description. Each participant\nsees the images in a different order, so they cannot use position information to communicate\nthe referent. Reference games have been used extensively in the study of computational\nmodels, including recently to evaluate visual abstraction (Ji et al., 2022) and conversational\naptitude (Chalamalasetti et al., 2023).\nA repeated reference game (Figure 1) includes multiple repetitions. Each repetition has\none trial for each image in the shared context. The listener receives feedback after every\n2In Figure 1, we show the shared context only once for compactness. In our experiments, we shuffle\nand show the context for each trial, but also experiment with showing it only once.\n2\nPublished as a conference paper at COLM 2024\ntrial, which indicates the correct selection. The repetition and feedback allow the dyad\nto form message agreements over the repeating stimuli (i.e., the speaker would naturally\nuse gradually shorter but related messages across repetitions, and the listener learns what\nthey refer to). ICCA’s repeated reference games are developed based on the setup and\ndata from Hawkins et al. (2020b). The shared context includes four images, and there are\nsix repetitions, giving a total of 24 trials. The order of images is shuffled across the trials.\nWe refer to this as the standard setup. Hawkins et al. (2020b) only uses this standard setup.\nBeyond this standard setup, we design variants to further disentangle the types and causes\nof model failures for in-context LLM adaptation (Section 4 and Section 5).\nAd-hoc Adaptation in Interactions\nExisting literature shows that humans are inclined\nto reduce the effort needed to convey their intended information and for their audience\nto comprehend it, leading to efficient communication (e.g., Zipf, 1949; Gibson et al., 2019;\nYin et al., 2024). When human individuals interact through dialogue, this is manifested by\ndeveloping and using ad-hoc linguistic conventions. This phenomenon has been observed\nwith repeated reference games (Krauss & Weinheimer, 1964; 1966; Clark & Wilkes-Gibbs,\n1986; Hawkins et al., 2020a), and related interaction scenarios (Haber et al., 2019). Studies\nhave also shown various properties of these conventions, such as arbitrariness, stability,\nstickiness, and convergence (Lewis, 1969; Brennan & Clark, 1996; Markman & Makin, 1998;\nHawkins et al., 2020a; Eliav et al., 2023). This adaptation was modeled with the pragmatic\nrational speech act model (RSA; Goodman & Frank, 2016), leading to development of\nmodels that replicate this behavior in reference games (e.g., Monroe et al., 2017; McDowell\n& Goodman, 2019; White et al., 2020) and use it to improve model performance on other\ntasks (e.g., Andreas & Klein, 2016; Fried et al., 2018). Adaptation was studied beyond\nreference games, showing how the complexity of the scenario influences how conventions\nmanifest in the language (Effenberger et al., 2021).\nAd-hoc conventions are a particular instantiation of the broader phenomenon of common\nground, which is defined as the mutually recognized shared information between the par-\nticipants of a conversation (Clark & Brennan, 1991; Lewis, 1969; Stalnaker, 2002). Common\nground has been studied extensively, with focus on both human cognition (Clark, 1996;\nHorton & Gerrig, 2016) and machine reasoning (Cohen & Levesque; Grosz & Sidner; Traum,\n1994; Del Tredici et al., 2022; Shaikh et al., 2024; Andukuri et al., 2024; Testoni & Fern´andez,\n2024).\nModel Adaptation\nAdapting models during an interaction to improve communication\nefficiency or success is relatively understudied. Hawkins et al. (2020b) proposes a continual\nlearning method for CNN-RNN models to gain communication efficiency in repeated refer-\nence games through continual weight updates. Zhu et al. (2021) proposes explicitly training\nmodels for ad-hoc adaptation through meta-learning. We focus on the in-context capabilities\nof LLMs and MLLMs, which offer an update-free route for adaptation that is particularly\ncompelling given the costs of updating large models. Our use of in-context learning differs\nfrom how this mechanism is usually used either by providing instructions (Ouyang et al.,\n2022) or few-shot examples (Brown et al., 2020). While reference games can be seen as\nrelated to the few-shot approach, ad-hoc adaptation is not about replicating patterns, but\nshowing change and adaptation over time.\n3\nThe ICCA Framework\nICCA uses a dataset of human-human interactions, and allows to easily customize different\nparts of the interaction. This flexibility enables different research questions. For example,\nin Section 5, we customize the interaction structure to analyze how well models handle\nlong interactions with multiple images interleaved in them. ICCA supports studies with the\nmodel acting either as speaker or listener, and includes several metrics to track different\nproperties of adaptation during the interaction.\nICCA is fully automated and easily applicable to new MLLMs. Our design does not\nrequire collecting new data or human studies, but instead uses Hawkins et al. (2020b)’s\nhuman-human interaction data to simulate a human interacting with an MLLM. Each\n3\nPublished as a conference paper at COLM 2024\ninteraction in the dataset was collected under the standard setup (Section 2) and uses a\nvisually challenging reference context, consisting of four similar images (Figure 1). The\ndataset contains 54 human-human interactions, which we incorporate into ICCA.\nA repeated reference game interaction R is a sequence of tuples ⟨(Ci, ci, si, li, fi)⟩n\ni=1, where\nCi is the set of images forming the reference context, ci is the index of the target image, si is\nthe speaker utterance, li is the listener selection, and fi is the feedback based on the listener’s\nselection. At every trial t, with the evaluated model as either the listener or speaker, ICCA\nconstructs the MLLM prompt from an instruction text I, the history (i.e., all prior trials\nR[: t]), and the stimuli for the current trial with a user-defined pre-processing function F.\nUpon receiving the model’s response, ICCA computes the feedback ft to help the next trial.\nThe function F prepares the prompt depending on the experiment, and is key to the flexibility\nof ICCA. For example, when evaluating a model as a listener under the standard setup,\nthe function input would be F(I, R[: t], Ct, st), and it would format and concatenate all the\nelements in order. Figure 7 in the appendix shows an example prompt. ICCA allows to\neasily modify this standard setup, for example by processing the data such that F drops all\nreference contexts except C1, thereby creating a simpler input where the images appear only\nonce at the beginning of the interaction.\nICCA simulates the interlocutor of the model evaluated either with a deterministic counter-\npart or by having another model take the role. A deterministic speaker outputs the messages\nfrom recorded human interactions dataset, showing predetermined, realistic trajectories of\nmessage shortening over time, but it does not adapt its language based on the listener’s\nselections. It can be considered as a “convention comprehension” task, potentially more\nchallenging due to the non-adapting speaker messages. We use this simulated speaker\nfor our model-as-listener experiments (Section 5) because it exposes the model listener to\nbehaviors and linguistic conventions naturally occurring in human interactions. Inversely,\nfor our speaker experiments (Section 4), we use a high-performance model listener (GPT4),\nwhich we observe to have performance similar to human listeners (Section 5).\nWe evaluate model behavior with adaptations of the metrics used in human studies with\nreference games (Hawkins et al., 2020a;b). For listener experiments, we follow Hawkins\net al. (2020b) and report the average accuracy in each repetition. Speaker experiments are\nevaluated using several metrics. We report the average message length and the listener’s\naccuracy in each repetition. Additionally, we evaluate the similarity between corresponding\nmessages from consecutive repetitions. While this was done using GloVe embeddings (Pen-\nnington et al., 2014) in past work (Hawkins et al., 2020a), we design a new metric called\nWord Novelty Rate (WNR), which is sensitive to exact word choices. WNR is a modified\nword error rate that only counts insertions and substitutions, and ignores deletions. It is\nmotivated by how people naturally drop words from their messages as the interaction\nprogresses (Hawkins et al., 2020a), whereas additions and substitutions of words often\nreflect important changes in information based on our observations. Compared to GloVe,\nWNR is more sensitive to lexical inconsistencies that can increase the listener’s cognitive\nload. Appendix A presents more details of our metrics, including a comparison of WNR\nand embedding-based similarity metrics and a variant of WNR that is not normalized by\nmessage length (referred to as Word Novelty Distance).\n4\nModel-as-speaker Experiments\nWe study model behavior as speaker with five state-of-the-art vision MLLMs: IDEFICS-\n80b-instruct,3 LLaVa-1.5-13b, GPT4-vision, Gemini 1.0 Pro Vision, and Claude 3 opus.\nThroughout all speaker experiments, we customize the data to only show the referential\ncontext once at the beginning of the interaction, so there is no shuffling of context through-\nout the interaction. We engineer prompts for each model individually to best evaluate\nits capability. We use GPT4 as the listener. It exhibits high performance in our listener\nexperiments (Section 5), especially when the context appears only once at the beginning,\n3IDEFICS is an open-source reproduction of Flamingo (Alayrac et al., 2022).\n4\nPublished as a conference paper at COLM 2024\n20\n40\n60\n80\n100\nAcc %\nS1: Standard Speaker\nS2: Gricean Instruction\nS3: Explicit Instruction\nS4: Explicit Instruc-\ntion+Consistency Request\n5\n10\n15\n20\nMsg len (tokens)\n1\n2\n3\n4\n5\n6\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nWNR\n1\n2\n3\n4\n5\n6\nRepetition #\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\nIDEFICS\nLLaVa\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 2: Speaker experiments. Margins of errors are bootstrapped 95% CIs.\nso it is a suitable substitute for a human listener in this study. Appendix A.3 provides\nimplementation details.\nWe design four speaker variants by modifying the instruction I. The variants were developed\nthroughout our experiments, by observing the difficulty of models to present patterns similar\nto human linguistic behavior. The variants instruct the model to display the convention\nformation behavior observed in human speakers in increasingly explicit and specific ways:\nS1: Standard Speaker The standard speaker setup (Section 2). The model speaker only\nreceives the basic game instruction, with no mention of communication efficiency.\nFigure 6 in the appendix shows an example prompt.\nS2: Gricean Instruction A relatively light-handed and general way to introduce the\nexpected convention formation behavior is to explicitly instruct the model to follow\nthe Gricean quantity maxim. This kind of instruction is not specific to reference\ngames, and does not explicitly mention message length. Its focus is information,\nand it entails that cooperative interlocutors would provide enough information\nto identify the referent but would not make the message more informative than\nnecessary. We add additional instructions based on the maxim and further instruct\nthe model to think about how the amount of information needed may change as more trials\nare completed and based on the listener’s performance in previous trials.4,5\nS3: Explicit Instruction We instruct the model to explicitly reduce message length as\nthe interaction progresses. Unlike S1 and S2, this instruction is specific to reference\ngames, as language adaptation in other scenarios is not necessarily accompanied by\nlength reduction (Effenberger et al., 2021). We add to S1 an explicit instruction to\n4The models did not show substantial improvement without this additional instruction.\n5Not the exact prompt used for experiments; shortened and revised for illustrative purposes.\n5\nPublished as a conference paper at COLM 2024\nreduce utterance length: as more trials are completed and as the listener understands you\nbetter, gradually condense your messages, making them shorter and shorter every trial.5\nS4: Explicit Instruction + Consistency Request Convention formation in reference\ngames is not only characterized by reduction in utterance length, but also by lexical\nconsistency. This variant explicitly instructs the model to follow this pattern. Similar\nto S3, it is specific to the repeated reference game setup and its use of a repeating\ncontext. We add to S3 the instruction: when creating a shorter message for an image, try\nto extract salient tokens from the previous messages for this image rather than introducing\nnew words. The short messages should still allow the listener to choose the target correctly.\nFor each image, when you reach a message that cannot be further shortened, you should\nkeep using that message for the rest of the game.5\nFigure 2 shows the results for all variants, along with properties of the human messages\nfrom Hawkins et al. (2020b), which were collected using the standard setup. We report\nmean message length, WNR, and listener accuracy for each repetition. Overall, all models\nfail to spontaneously improve communication efficiency. It is only with fairly heavy-handed\ninstruction that GPT4, Gemini, and Claude show adaptation trends similar to humans.\nVariant S1 shows that without any explicit instruction, the models show trends that are\nfar from human behavior. GPT4, Gemini, and Claude generate longer utterances in later\nrepetitions. IDEFICS and LLaVa maintain consistent message lengths. But, upon close\ninspection, we observe they simply tend to repeat previously used messages for the same\nimage. This explains their very low and almost constant WNR. We analyze this further in\nSection 6. Also, the messages of IDEFICS and LLaVa are less effective in distinguishing the\ntarget images, as shown by the lower listener accuracies. This demonstrates the inability of\nthese models to correct their behavior based on feedback. No models show WNR trends\nsimilar to humans. GPT4, Gemini, and Claude constantly introduce new words as the game\nprogresses, as shown by higher WNR curves, even though we do not use token sampling for\ndecoding. Gemini shows a downward trend, but achieves this by the undesirable practice of\nmaking its messages longer every trial while making a relatively constant number of word\ninsertions or changes, so a bigger portion of the message is maintained.6\nGricean instruction (S2) leads GPT4 and Claude to reduce message length over time, though\nGPT4’s reduction is far from humans’. Both models have WNR curves significantly higher\nthan humans. As their messages shorten, they still frequently introduce new words and\ndo not stabilize the messages, a behavior adverse to communication efficiency. We further\ndiscuss this issue with S3, where more models display this issue.\nS3’s explicit instruction has no impact on IDEFICS and LLaVa. Both continue repeating\nmessages, failing to follow the instructions. Gemini and GPT4 show decreasing message\nlength trends similar to humans but still have longer messages than humans throughout.\nClaude eventually produces messages as short as humans but starts with much longer\nmessages than other models. All models showing message shortening frequently introduce\nnew words as they shorten the messages. Even when the message is too short to be\ncondensed, the models may adopt new words next time without changing the message\nlength. Figure 8 in the appendix exemplifies these behaviors. Such behaviors deviate from\nthe stability property of conventions and the observation that human messages show high\nconsistency and convergence. The gap between the WNR curves of these models and\nhumans illustrates this issue. While these models show increasing lexical efficiency, the use\nof new words reduces communication efficiency, burdening the listener to reason about the\nwords that did not appear the last time the image was referenced. We further discuss this\nissue in Section 6.\nS4 addresses the consistency issue but requires further explicit instruction. The final prompt\nelicits from GPT4, Gemini, and Claude both length reduction and message convergence, as\nobserved with humans. However, the S4 prompt is very specific to ICCA’s setup and does\nnot generalize beyond reference games. Heavy-handed prompt interventions like this are\n6This is due WNR’s length-normalization, which is critical to reflect similarity (Appendix A.2).\n6\nPublished as a conference paper at COLM 2024\nalso known to cause unintended model behaviors (Shaikh et al., 2024). Prompt engineering\nis not likely to be the solution.\n5\nModel-as-listener Experiments\nListener experiments follow a setup similar to the speaker experiments as far as models\nand prompt optimization (Section 4). Gemini, LLaVa, and Claude cap the number of input\nimages, limiting their use in some of our listener variants. Overall, we design four main\nvariants, each implemented through the pre-processing function F. Our design process is\niterative, with some variants designed based on the behavior observed with earlier ones.\nThroughout the listener variants, we keep the instruction I largely constant and about the\nrole of the listener. We vary how we display the referential context.\nThe listener action space is more limited, simply requiring the model to select the referenced\nimage. We focus on evaluating model accuracy, similar to how listener behavior is charac-\nterized in human studies. Figure 3 visualizes the behaviors we observe. We also include\nhuman listener accuracy trends as reference to model accuracies.\nThe starting point for the listener study is the standard reference game setup (Section 2):\nL1: Standard Listener\nImages are shuffled and re-displayed for each trial, so each\nimage will potentially have a new label relative to previous trials.\nL1 requires a growing number of images in the prompt as the interaction progresses. With\nsix repetitions of four trials and a context of four images, the maximum number of images\nin the prompt at the end of the interaction is 96. Gemini, LLaVa, and Claude can take at\nmost 16, 4, and 20 images, so we only use GPT4 and IDEFICS with L1.\nWe expect an effective model to exploit the conversation history to reason about the human\nspeaker’s conventionalized ways of speaking. Even if the model starts with low accuracy, it\nhas the opportunity to improve because the prompt at later stages includes feedback for its\nchoices, and as the messages conventionalize, later messages for an image are often exact\nrepetitions, albeit with the referential context shuffled.\nBoth GPT4 and IDEFICS do significantly worse than humans (Figure 3, left). As expected,\nhumans demonstrate strong performance to start with, and show an upward trend in ac-\ncuracy, as the interlocutors adapt to each other. GPT4 is significantly worse than humans,\nthough performing fairly well. It shows a marginal improvement trend (88.9%→92.5% in\nrepetition 5), but it is not significant, and weakens in the last repetition (91.2%). IDEFICS\nis much worse immediately in the beginning (46.8%), and rather than improving, its per-\nformance deteriorates as the interaction progresses, reaching random chance in the later\ntrials. This happens even though it is receiving an increasing amount of information that\nshould allow it to improve its performance. A possible cause for this trend is the dramatic\nincrease in the prompt size, especially as more and more images are added, as the interaction\nprogresses. We further discuss this issue and its potential causes in Section 6.\n5.1\nHistory and Context Impact\nFollowing the observations with L1, we design three variants with simplified referential\ncontexts and history to better understand how well the models handle the interaction history\nand the referential context:\nL2: No History Each of the 24 trials is given to the model in isolation, without any\nhistory. The model input includes the context of four images and the speaker\nutterance, as well as the basic game instruction. This variant reveals the extent\nto which the model can reason about the ad-hoc conventions formed in repeated\nhuman-human interaction without access to the history in which they were formed.\nL3: Images Once\nA potential challenge of L1 is the large number of images in the\nprompt. A model’s architecture or training may not be suitable for handling a\nlarge number of images, and LLM prompt length is known to adversely influence\n7\nPublished as a conference paper at COLM 2024\n1\n2\n3\n4\n5\n6\n25\n50\n75\n100\nAcc %\nL1: Standard Listener\n1\n2\n3\n4\n5\n6\nRepetition #\nL2: No History\n1\n2\n3\n4\n5\n6\nL3: Images Once\n1\n2\n3\n4\n5\n6\nL4: No Shuffle\nIDEFICS\nLLaVa\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 3: Listener experiments. Margins of Error are 95% bootstrapped CIs.\nperformance (Liu et al., 2023b). L3 uses a shorter history, by only providing the\nreferential context (four images) to the model once in the first trial. Image labels are\npersistent across all trials, which also avoids the impact of shuffling. Unlike L2, this\nvariant includes the complete message, selection, and feedback history.\nL4: No Shuffle The four images appear every trial similar to L1 but are not shuffled\nacross trials. L4 shows the effects of image shuffle if compared with L1. It also\nshows the effect of image quantity if compared with L3, which does not involve\nshuffling either.\nL2 and L3 need only four images in the prompt, allowing us to test all the models.\nThe no-history variant (L2) reveals different performance trends for IDEFICS and GPT4\ncompared to the standard setup (L1). IDEFICS is still not doing great (45.8% on the first\nrepetition), but performance largely remains consistent across repetitions, indicating that\npossibly the complexity of the prompt is at the root of its downward trend in the L1 scenario.\nGPT4 starts with similar performance (87.5%) to its results in L1, but then shows a slight\ndownward trend (83.8% at the end), in contrast to the initial upward trend in L1. This\nindicates that the gradually conventionalized, shorter messages a human speaker uses tend\nto be more difficult for the model to resolve and that the conversation history can be an\neffective remedy. Gemini (80.2→78.8%), LLaVa (72.7→71.3%), and Claude (57.4→55.5%)\nshow similar trends to GPT4 from Repetition 1 to 6, with overall lower accuracies.\nThe benefit of history becomes more conspicuous when the referential context is only given\nonce (L3). This variant dramatically simplifies the prompt, but retains the information\nneeded for convention formation (i.e., prior message, selection, and feedback). All the\nmodels show an upward trajectory as the interactions progress. GPT4 and Claude are\nthe strongest, eventually reaching 100% accuracy, matching human listeners’ performance\n(99.54%). This suggests that all models can associate the current message with the relevant\nprior messages, thereby increasing their prediction accuracy as the interaction progresses.\nAdmittedly, because an image always has the same label throughout the game under this\nsetup, the models may do well by simply drawing associations between an image’s label\nand the messages that have referred to that image (i.e., label-message associations). Under\nthis mechanism, the model can improve its accuracy without reasoning about the actual\nvisual input. We explore this possible mechanism with more game variants in Appendix D.\nNonetheless, this experiment shows that MLLMs possess some capabilities for increasingly\nefficient communication with humans when acting as the listener.\nThe no-shuffle (L4) experiment also provides key insights. GPT4’s accuracy is similar to\nthat in L3 and higher than that in L1. This demonstrates sensitivity to image shuffling and\nthe constantly changing image labels. GPT4 in L3 and L4 may be relying to some degree on\ntext similarity between repetitions by exploiting the label-message associations, rather than\ngrounding to the visual input. We study this further in Appendix D. IDEFICS’ performance\n8\nPublished as a conference paper at COLM 2024\non the other hand is different from both L3 and L1, showing an almost unnoticeable trend\nup. This shows that it is not only the shuffling but also the increase in the number of images\nthat IDEFICS cannot seem to handle well. We further discuss this issue in Section 6.\n6\nDiscussion\nOur studies point to various issues that likely hinder specific models from displaying\ncommunication efficiency gains, and point out directions for future works.\nTendency to Repeat Messages\nIn the speaker study, IDEFICS and LLaVa tend to repeat\nthe first message they use for each image, showing no adaptation. To further study how\nmuch these models prefer patterns of repetition, we design a test that uses these models\nfor language modeling rather than text generation. We construct two transcripts for each\nof the 54 human-human interactions in our original dataset. One is the original transcript\nfrom human-human interactions, showing the natural evolution of messages. The other is\na manipulated transcript where the speaker repeats the messages from Repetition 1 in all\nthe later repetitions. We calculate the log probability and perplexity IDEFICS and LLaVa\nassign to these transcripts, count the number of times one type of transcript has better log-\nprobability\/perplexity than the other, and apply a sign test. We find that the manipulated\ntranscript showing message repetitions consistently receives higher log probability and\nlower perplexity for all 54 interactions, showing the models’ significant tendency towards\nrepeated patterns (sign test p-values are near zero). Unfortunately, this experiment cannot\nbe done with GPT4, Gemini, or Claude due to API limitations.\nLexical Efficiency ̸= Communication Efficiency\nThe convergence of human speaker mes-\nsages for a particular image to a short, stable convention often takes the form of extracting\nsalient tokens from the previous message and sticking to the same message once it becomes\nvery short (Hawkins et al., 2020a). Unless directly instructed to do so through a highly\nengineered prompt (S4), GPT4, Gemini, and Claude often introduce new words when\nshortening their messages or even when the messages cannot be further shortened, as\nshown in the S3 explicit instruction variant. Such inconsistency with human behaviors is\nproblematic. When messages for the same image do not converge, no conventions can form\nand the listener will likely need additional cognitive effort to process the previously unseen\nwords. Intuitively, even if a new message is semantically similar to a previous one by using\nsynonyms, resolving it still likely entail a greater cognitive load than an exact repetition.\nMoreover, when new words describing a new aspect of an image are introduced after a\nfew rounds of relatively similar messages, they violate the human listener’s expectation,\npotentially leading to miscommunication and slower response (Metzing & Brennan, 2003).\nPerformance Degradation with Many-image Inputs\nAmong the models that support a\nlarge number of images, IDEFICS performs much worse as the number of images increases.\nEven though the images in L4 are not shuffled across trials, which could have allowed the\nmodel to exploit label-message associations as an efficient way to gain high accuracy, the\nmodel still had much lower accuracy than when the images only appear once (L3) (Figure 3).\nWhen the history contains a growing number of images that are shuffled between trials (L1),\nIDEFICS shows an even worse accuracy trend.\nA likely hypothesis is that a greater number of images creates challenges for capturing the\ndependency between specific visual input and textual cues, which can manifest as failures to\nassociate an image’s label with the actual content of the image. In a qualitative experiment,\nwe supply a sequence of images and their labels as input to IDEFICS, and instruct it to\ndescribe Image [X]. We observe that IDEFICS can describe the correct image easily when we\ngive up to four labeled images, but often makes mistakes as the number increases (Figure 9\nin the appendix). Therefore, even though IDEFICS is designed and trained to support\nmulti-image inference,7 its multi-image capabilities do not generalize beyond a few images.\n7The Flamingo architecture behind IDEFICS supports an arbitrary number of images as input and\nit is trained on interleaved texts and multiple images (Huggingface, 2023).\n9\nPublished as a conference paper at COLM 2024\nAnother potential cause is the Flamingo architecture behind IDEFICS. Each token’s cross-\nmodal attention only applies to the visual features of the last image that precedes it, rather\nthan all the images. Information about other images can only be indirectly accessed through\nself-attention on the hidden states of their respective <image> tokens in the text sequence.\nThis architecture may degrade the ability of the model to reason about images, depending\non their locations in the input. When the target is not the last image, Image D, the message\ntokens will not have direct cross-modal attention to the target’s visual features, which may\nhurt IDEFICS’ prediction. In L3, where IDEFICS did perform well, this limitation might have\nbeen mitigated by the strong textual cues and label-message associations, whereas having\nmore images may distract IDEFICS from these cues and thus manifests this limitation.\n7\nConclusion\nICCA provides a perspective into the performance of today’s MLLMs that is missing in\nexisting benchmarking, and can be easily applied to new MLLMs without collecting new\nhuman data. We observe that state-of-the-art models lack the in-context abilities to adapt\ntheir own language for efficient communication, even though they may sometimes perform\nbetter while passively receiving increasingly efficient language from their interlocutor. This\nissue is fundamental because, unlike humans, the models do not perceive the effort or\ncost needed for communication, thus having no inherent reason to reduce them. It is\nstill surprising though, given that LLMs\/MLLMs have successfully displayed many other\nhuman behaviors and impressive abilities in various applications, by learning from the large\namounts of human data, where adaptation for efficiency is common. Overall, the current\nparadigm for creating LLMs fails to address the need for conversational adaptation and\nfuture research is needed on improving their abilities to spontaneously improve language\nefficiency, maintain language consistency for the same referent, avoid excessive tendency\nfor repetitions, and handle more images in a single query.\nAcknowledgments\nThis research was supported by ARO W911NF21-1-0106, NSF under grant No. 1750499, a\ngift from Open Philanthropy, and a gift from Apple. We thank Robert Hawkins and Marten\nvan Schijndel for insightful discussions. We thank the anonymous reviewers and the area\nchair for their valuable feedback.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nTalk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs\n```\n#### 2. 论文摘要\n```\nHumans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps:\/\/github.com\/lil-lab\/ICCA.\n```\n\n#### 3. 论文全文\n```\nPublished as a conference paper at COLM 2024\nTalk Less, Interact Better: Evaluating In-context\nConversational Adaptation in Multimodal LLMs\nYilun Hua and Yoav Artzi\nDepartment of Computer Science and Cornell Tech\nCornell University\n{yilunhua, yoav}@cs.cornell.edu\nAbstract\nHumans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon\nhas been studied extensively using reference games, showing properties of\nhuman language that go beyond relaying intents. It remains unexplored\nwhether multimodal large language models (MLLMs) similarly increase\ncommunication efficiency during interactions, and what mechanisms they\nmay adopt for this purpose. We introduce ICCA, an automated framework\nto evaluate such conversational adaptation as an in-context behavior in\nMLLMs. We evaluate several state-of-the-art MLLMs, and observe that\nwhile they may understand the increasingly efficient language of their\ninterlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property\nof linguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language.\n1\nIntroduction\nHuman interlocutors adapt to each other during interactions, developing increasingly\nefficient ways to refer to concepts and objects. Hawkins et al. (2020b) exemplify this via\ncommunication between a nurse and a bed-ridden patient at home. Initially, the patient\nmay refer to a medicine with the medicine for my back pain in a small blue medicine bottle ..., but\nafter a week of care, they are likely to just ask for their back meds. This increase in efficiency\nrelies on the interlocutors forming ad-hoc linguistic conventions: the mutually understood,\nconcise phrases to communicate referential content. This phenomenon has been repeatedly\nobserved and characterized in controlled studies using repeated reference games (Figure 1;\ne.g., Krauss & Weinheimer, 1964; Brennan & Clark, 1996; Hawkins et al., 2020a).\nWe study this ability in multimodal large language models (MLLMs). LLMs and MLLMs are\nwell positioned to acquire this behavior and display it spontaneously in interactions. They\nare trained on large amounts of human language data, in which this behavior is common\nand the history of an ongoing interaction is often retained, thereby explicitly keeping the\ninformation needed at hand. Beyond the scientific question, such ad-hoc adaptation has\nsignificant application impacts: enabling more natural interactions, reducing the costs\ninvolved in conversations (e.g., using shorter utterances to communicate the same amount\nof information), and increasing the accuracy of relaying intent.\nWe propose ICCA,1 an automated framework to evaluate and characterize the ability of\nmodels to form ad-hoc conventions. ICCA uses a corpus of human-human reference game\ninteractions, allowing for completely automated evaluation, which does not require further\nhuman interaction, making it easy to deploy for the analysis of new models. The interaction\nfollows the standard repeated reference game setup (Clark & Wilkes-Gibbs, 1986), where\na speaker refers to an image within a shared context of images, and a listener resolves the\n1ICCA stands for In-context Conversational Adaptation.\n1\narXiv:2408.01417v1  [cs.CL]  2 Aug 2024\nPublished as a conference paper at COLM 2024\nPhoto with a bowl of 3 \nbananas with pokadot \nA\nB\nC\nD\nImage B\nA bowl full of mixed fruit, \nblack background\nImage A\nContext\nRepetition 1\nRepetition 6\nblack\nImage A\npokadot\nImage B\nFigure 1: Illustration of a reference game. The speaker (blue) and listener (orange) observe a\nshared set of images.2The interaction progresses in six repetitions, each includes a trial for\nevery context image. In each trial, the speaker describes a target image, and the listener has\nto select the correct target given the description only. For simplicity, this figure omits the\nfeedback on listener actions. This interaction illustrates some of the effects of convention\nformation: the descriptions become shorter as the interaction progresses, and lexical choices\nconverge to a subset of the words used in earlier repetitions.\nreference to select one of the images, ideally the one originally referred to. Figure 1 illustrates\nthe scenario. We focus on in-context adaptation – as the interaction progresses, the entire\nhistory is retained in-context. Core to our approach is comparing the changes in model\nbehavior, either as speaker or listener, throughout an interaction to the changes observed\nin humans. We measure different properties that have been shown to be influenced by\nconvention formation: utterance length, lexical convergence, and selection accuracy.\nWe apply our approach to five representative MLLMs: IDEFICS (Huggingface, 2023), LLaVa-\n1.5 (Liu et al., 2023a), GPT4-vision (OpenAI et al., 2024), Gemini 1.0 Pro Vision (Google, 2023),\nand Claude 3 opus (Anthropic, 2024). We find that all models struggle to spontaneously\nintroduce conventions and adapt as speakers. Prompt engineering an explicit in-context\ninstruction specific to the reference game scenario can address this to some degree. The\nstrongest models (GPT4, Gemini, and Claude) can then gradually use shorter messages\n(gaining lexical efficiency) but still struggle with convergence or stability, which hinders\nthe emergence of truly efficient communication. When acting as a listener, GPT4 displays\nadaptation trends close to humans, improving its accuracy as the interaction progresses,\nwhile other models show this behavior to a lesser degree or only under some simplified\nsetups. Overall, we show that while today’s MLLMs may passively understand the evolv-\ning language of their interlocutor, the ability to adapt their own language for efficient\ncommunication does not naturally emerge from their training or instruction-tuning. This\noutlines important future research problems. We release ICCA under the MIT license at\nhttps:\/\/github.com\/lil-lab\/ICCA.\n2\nBackground and Related Work\nRepeated Reference Games\nA reference game is an interaction where a speaker and a\nlistener (i.e., a dyad) interact over a shared context. The shared context is a set of images.\nThe speaker describes a target image. The target designation is only revealed to the speaker.\nThe listener has to select an image following the speaker’s description. Each participant\nsees the images in a different order, so they cannot use position information to communicate\nthe referent. Reference games have been used extensively in the study of computational\nmodels, including recently to evaluate visual abstraction (Ji et al., 2022) and conversational\naptitude (Chalamalasetti et al., 2023).\nA repeated reference game (Figure 1) includes multiple repetitions. Each repetition has\none trial for each image in the shared context. The listener receives feedback after every\n2In Figure 1, we show the shared context only once for compactness. In our experiments, we shuffle\nand show the context for each trial, but also experiment with showing it only once.\n2\nPublished as a conference paper at COLM 2024\ntrial, which indicates the correct selection. The repetition and feedback allow the dyad\nto form message agreements over the repeating stimuli (i.e., the speaker would naturally\nuse gradually shorter but related messages across repetitions, and the listener learns what\nthey refer to). ICCA’s repeated reference games are developed based on the setup and\ndata from Hawkins et al. (2020b). The shared context includes four images, and there are\nsix repetitions, giving a total of 24 trials. The order of images is shuffled across the trials.\nWe refer to this as the standard setup. Hawkins et al. (2020b) only uses this standard setup.\nBeyond this standard setup, we design variants to further disentangle the types and causes\nof model failures for in-context LLM adaptation (Section 4 and Section 5).\nAd-hoc Adaptation in Interactions\nExisting literature shows that humans are inclined\nto reduce the effort needed to convey their intended information and for their audience\nto comprehend it, leading to efficient communication (e.g., Zipf, 1949; Gibson et al., 2019;\nYin et al., 2024). When human individuals interact through dialogue, this is manifested by\ndeveloping and using ad-hoc linguistic conventions. This phenomenon has been observed\nwith repeated reference games (Krauss & Weinheimer, 1964; 1966; Clark & Wilkes-Gibbs,\n1986; Hawkins et al., 2020a), and related interaction scenarios (Haber et al., 2019). Studies\nhave also shown various properties of these conventions, such as arbitrariness, stability,\nstickiness, and convergence (Lewis, 1969; Brennan & Clark, 1996; Markman & Makin, 1998;\nHawkins et al., 2020a; Eliav et al., 2023). This adaptation was modeled with the pragmatic\nrational speech act model (RSA; Goodman & Frank, 2016), leading to development of\nmodels that replicate this behavior in reference games (e.g., Monroe et al., 2017; McDowell\n& Goodman, 2019; White et al., 2020) and use it to improve model performance on other\ntasks (e.g., Andreas & Klein, 2016; Fried et al., 2018). Adaptation was studied beyond\nreference games, showing how the complexity of the scenario influences how conventions\nmanifest in the language (Effenberger et al., 2021).\nAd-hoc conventions are a particular instantiation of the broader phenomenon of common\nground, which is defined as the mutually recognized shared information between the par-\nticipants of a conversation (Clark & Brennan, 1991; Lewis, 1969; Stalnaker, 2002). Common\nground has been studied extensively, with focus on both human cognition (Clark, 1996;\nHorton & Gerrig, 2016) and machine reasoning (Cohen & Levesque; Grosz & Sidner; Traum,\n1994; Del Tredici et al., 2022; Shaikh et al., 2024; Andukuri et al., 2024; Testoni & Fern´andez,\n2024).\nModel Adaptation\nAdapting models during an interaction to improve communication\nefficiency or success is relatively understudied. Hawkins et al. (2020b) proposes a continual\nlearning method for CNN-RNN models to gain communication efficiency in repeated refer-\nence games through continual weight updates. Zhu et al. (2021) proposes explicitly training\nmodels for ad-hoc adaptation through meta-learning. We focus on the in-context capabilities\nof LLMs and MLLMs, which offer an update-free route for adaptation that is particularly\ncompelling given the costs of updating large models. Our use of in-context learning differs\nfrom how this mechanism is usually used either by providing instructions (Ouyang et al.,\n2022) or few-shot examples (Brown et al., 2020). While reference games can be seen as\nrelated to the few-shot approach, ad-hoc adaptation is not about replicating patterns, but\nshowing change and adaptation over time.\n3\nThe ICCA Framework\nICCA uses a dataset of human-human interactions, and allows to easily customize different\nparts of the interaction. This flexibility enables different research questions. For example,\nin Section 5, we customize the interaction structure to analyze how well models handle\nlong interactions with multiple images interleaved in them. ICCA supports studies with the\nmodel acting either as speaker or listener, and includes several metrics to track different\nproperties of adaptation during the interaction.\nICCA is fully automated and easily applicable to new MLLMs. Our design does not\nrequire collecting new data or human studies, but instead uses Hawkins et al. (2020b)’s\nhuman-human interaction data to simulate a human interacting with an MLLM. Each\n3\nPublished as a conference paper at COLM 2024\ninteraction in the dataset was collected under the standard setup (Section 2) and uses a\nvisually challenging reference context, consisting of four similar images (Figure 1). The\ndataset contains 54 human-human interactions, which we incorporate into ICCA.\nA repeated reference game interaction R is a sequence of tuples ⟨(Ci, ci, si, li, fi)⟩n\ni=1, where\nCi is the set of images forming the reference context, ci is the index of the target image, si is\nthe speaker utterance, li is the listener selection, and fi is the feedback based on the listener’s\nselection. At every trial t, with the evaluated model as either the listener or speaker, ICCA\nconstructs the MLLM prompt from an instruction text I, the history (i.e., all prior trials\nR[: t]), and the stimuli for the current trial with a user-defined pre-processing function F.\nUpon receiving the model’s response, ICCA computes the feedback ft to help the next trial.\nThe function F prepares the prompt depending on the experiment, and is key to the flexibility\nof ICCA. For example, when evaluating a model as a listener under the standard setup,\nthe function input would be F(I, R[: t], Ct, st), and it would format and concatenate all the\nelements in order. Figure 7 in the appendix shows an example prompt. ICCA allows to\neasily modify this standard setup, for example by processing the data such that F drops all\nreference contexts except C1, thereby creating a simpler input where the images appear only\nonce at the beginning of the interaction.\nICCA simulates the interlocutor of the model evaluated either with a deterministic counter-\npart or by having another model take the role. A deterministic speaker outputs the messages\nfrom recorded human interactions dataset, showing predetermined, realistic trajectories of\nmessage shortening over time, but it does not adapt its language based on the listener’s\nselections. It can be considered as a “convention comprehension” task, potentially more\nchallenging due to the non-adapting speaker messages. We use this simulated speaker\nfor our model-as-listener experiments (Section 5) because it exposes the model listener to\nbehaviors and linguistic conventions naturally occurring in human interactions. Inversely,\nfor our speaker experiments (Section 4), we use a high-performance model listener (GPT4),\nwhich we observe to have performance similar to human listeners (Section 5).\nWe evaluate model behavior with adaptations of the metrics used in human studies with\nreference games (Hawkins et al., 2020a;b). For listener experiments, we follow Hawkins\net al. (2020b) and report the average accuracy in each repetition. Speaker experiments are\nevaluated using several metrics. We report the average message length and the listener’s\naccuracy in each repetition. Additionally, we evaluate the similarity between corresponding\nmessages from consecutive repetitions. While this was done using GloVe embeddings (Pen-\nnington et al., 2014) in past work (Hawkins et al., 2020a), we design a new metric called\nWord Novelty Rate (WNR), which is sensitive to exact word choices. WNR is a modified\nword error rate that only counts insertions and substitutions, and ignores deletions. It is\nmotivated by how people naturally drop words from their messages as the interaction\nprogresses (Hawkins et al., 2020a), whereas additions and substitutions of words often\nreflect important changes in information based on our observations. Compared to GloVe,\nWNR is more sensitive to lexical inconsistencies that can increase the listener’s cognitive\nload. Appendix A presents more details of our metrics, including a comparison of WNR\nand embedding-based similarity metrics and a variant of WNR that is not normalized by\nmessage length (referred to as Word Novelty Distance).\n4\nModel-as-speaker Experiments\nWe study model behavior as speaker with five state-of-the-art vision MLLMs: IDEFICS-\n80b-instruct,3 LLaVa-1.5-13b, GPT4-vision, Gemini 1.0 Pro Vision, and Claude 3 opus.\nThroughout all speaker experiments, we customize the data to only show the referential\ncontext once at the beginning of the interaction, so there is no shuffling of context through-\nout the interaction. We engineer prompts for each model individually to best evaluate\nits capability. We use GPT4 as the listener. It exhibits high performance in our listener\nexperiments (Section 5), especially when the context appears only once at the beginning,\n3IDEFICS is an open-source reproduction of Flamingo (Alayrac et al., 2022).\n4\nPublished as a conference paper at COLM 2024\n20\n40\n60\n80\n100\nAcc %\nS1: Standard Speaker\nS2: Gricean Instruction\nS3: Explicit Instruction\nS4: Explicit Instruc-\ntion+Consistency Request\n5\n10\n15\n20\nMsg len (tokens)\n1\n2\n3\n4\n5\n6\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nWNR\n1\n2\n3\n4\n5\n6\nRepetition #\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\nIDEFICS\nLLaVa\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 2: Speaker experiments. Margins of errors are bootstrapped 95% CIs.\nso it is a suitable substitute for a human listener in this study. Appendix A.3 provides\nimplementation details.\nWe design four speaker variants by modifying the instruction I. The variants were developed\nthroughout our experiments, by observing the difficulty of models to present patterns similar\nto human linguistic behavior. The variants instruct the model to display the convention\nformation behavior observed in human speakers in increasingly explicit and specific ways:\nS1: Standard Speaker The standard speaker setup (Section 2). The model speaker only\nreceives the basic game instruction, with no mention of communication efficiency.\nFigure 6 in the appendix shows an example prompt.\nS2: Gricean Instruction A relatively light-handed and general way to introduce the\nexpected convention formation behavior is to explicitly instruct the model to follow\nthe Gricean quantity maxim. This kind of instruction is not specific to reference\ngames, and does not explicitly mention message length. Its focus is information,\nand it entails that cooperative interlocutors would provide enough information\nto identify the referent but would not make the message more informative than\nnecessary. We add additional instructions based on the maxim and further instruct\nthe model to think about how the amount of information needed may change as more trials\nare completed and based on the listener’s performance in previous trials.4,5\nS3: Explicit Instruction We instruct the model to explicitly reduce message length as\nthe interaction progresses. Unlike S1 and S2, this instruction is specific to reference\ngames, as language adaptation in other scenarios is not necessarily accompanied by\nlength reduction (Effenberger et al., 2021). We add to S1 an explicit instruction to\n4The models did not show substantial improvement without this additional instruction.\n5Not the exact prompt used for experiments; shortened and revised for illustrative purposes.\n5\nPublished as a conference paper at COLM 2024\nreduce utterance length: as more trials are completed and as the listener understands you\nbetter, gradually condense your messages, making them shorter and shorter every trial.5\nS4: Explicit Instruction + Consistency Request Convention formation in reference\ngames is not only characterized by reduction in utterance length, but also by lexical\nconsistency. This variant explicitly instructs the model to follow this pattern. Similar\nto S3, it is specific to the repeated reference game setup and its use of a repeating\ncontext. We add to S3 the instruction: when creating a shorter message for an image, try\nto extract salient tokens from the previous messages for this image rather than introducing\nnew words. The short messages should still allow the listener to choose the target correctly.\nFor each image, when you reach a message that cannot be further shortened, you should\nkeep using that message for the rest of the game.5\nFigure 2 shows the results for all variants, along with properties of the human messages\nfrom Hawkins et al. (2020b), which were collected using the standard setup. We report\nmean message length, WNR, and listener accuracy for each repetition. Overall, all models\nfail to spontaneously improve communication efficiency. It is only with fairly heavy-handed\ninstruction that GPT4, Gemini, and Claude show adaptation trends similar to humans.\nVariant S1 shows that without any explicit instruction, the models show trends that are\nfar from human behavior. GPT4, Gemini, and Claude generate longer utterances in later\nrepetitions. IDEFICS and LLaVa maintain consistent message lengths. But, upon close\ninspection, we observe they simply tend to repeat previously used messages for the same\nimage. This explains their very low and almost constant WNR. We analyze this further in\nSection 6. Also, the messages of IDEFICS and LLaVa are less effective in distinguishing the\ntarget images, as shown by the lower listener accuracies. This demonstrates the inability of\nthese models to correct their behavior based on feedback. No models show WNR trends\nsimilar to humans. GPT4, Gemini, and Claude constantly introduce new words as the game\nprogresses, as shown by higher WNR curves, even though we do not use token sampling for\ndecoding. Gemini shows a downward trend, but achieves this by the undesirable practice of\nmaking its messages longer every trial while making a relatively constant number of word\ninsertions or changes, so a bigger portion of the message is maintained.6\nGricean instruction (S2) leads GPT4 and Claude to reduce message length over time, though\nGPT4’s reduction is far from humans’. Both models have WNR curves significantly higher\nthan humans. As their messages shorten, they still frequently introduce new words and\ndo not stabilize the messages, a behavior adverse to communication efficiency. We further\ndiscuss this issue with S3, where more models display this issue.\nS3’s explicit instruction has no impact on IDEFICS and LLaVa. Both continue repeating\nmessages, failing to follow the instructions. Gemini and GPT4 show decreasing message\nlength trends similar to humans but still have longer messages than humans throughout.\nClaude eventually produces messages as short as humans but starts with much longer\nmessages than other models. All models showing message shortening frequently introduce\nnew words as they shorten the messages. Even when the message is too short to be\ncondensed, the models may adopt new words next time without changing the message\nlength. Figure 8 in the appendix exemplifies these behaviors. Such behaviors deviate from\nthe stability property of conventions and the observation that human messages show high\nconsistency and convergence. The gap between the WNR curves of these models and\nhumans illustrates this issue. While these models show increasing lexical efficiency, the use\nof new words reduces communication efficiency, burdening the listener to reason about the\nwords that did not appear the last time the image was referenced. We further discuss this\nissue in Section 6.\nS4 addresses the consistency issue but requires further explicit instruction. The final prompt\nelicits from GPT4, Gemini, and Claude both length reduction and message convergence, as\nobserved with humans. However, the S4 prompt is very specific to ICCA’s setup and does\nnot generalize beyond reference games. Heavy-handed prompt interventions like this are\n6This is due WNR’s length-normalization, which is critical to reflect similarity (Appendix A.2).\n6\nPublished as a conference paper at COLM 2024\nalso known to cause unintended model behaviors (Shaikh et al., 2024). Prompt engineering\nis not likely to be the solution.\n5\nModel-as-listener Experiments\nListener experiments follow a setup similar to the speaker experiments as far as models\nand prompt optimization (Section 4). Gemini, LLaVa, and Claude cap the number of input\nimages, limiting their use in some of our listener variants. Overall, we design four main\nvariants, each implemented through the pre-processing function F. Our design process is\niterative, with some variants designed based on the behavior observed with earlier ones.\nThroughout the listener variants, we keep the instruction I largely constant and about the\nrole of the listener. We vary how we display the referential context.\nThe listener action space is more limited, simply requiring the model to select the referenced\nimage. We focus on evaluating model accuracy, similar to how listener behavior is charac-\nterized in human studies. Figure 3 visualizes the behaviors we observe. We also include\nhuman listener accuracy trends as reference to model accuracies.\nThe starting point for the listener study is the standard reference game setup (Section 2):\nL1: Standard Listener\nImages are shuffled and re-displayed for each trial, so each\nimage will potentially have a new label relative to previous trials.\nL1 requires a growing number of images in the prompt as the interaction progresses. With\nsix repetitions of four trials and a context of four images, the maximum number of images\nin the prompt at the end of the interaction is 96. Gemini, LLaVa, and Claude can take at\nmost 16, 4, and 20 images, so we only use GPT4 and IDEFICS with L1.\nWe expect an effective model to exploit the conversation history to reason about the human\nspeaker’s conventionalized ways of speaking. Even if the model starts with low accuracy, it\nhas the opportunity to improve because the prompt at later stages includes feedback for its\nchoices, and as the messages conventionalize, later messages for an image are often exact\nrepetitions, albeit with the referential context shuffled.\nBoth GPT4 and IDEFICS do significantly worse than humans (Figure 3, left). As expected,\nhumans demonstrate strong performance to start with, and show an upward trend in ac-\ncuracy, as the interlocutors adapt to each other. GPT4 is significantly worse than humans,\nthough performing fairly well. It shows a marginal improvement trend (88.9%→92.5% in\nrepetition 5), but it is not significant, and weakens in the last repetition (91.2%). IDEFICS\nis much worse immediately in the beginning (46.8%), and rather than improving, its per-\nformance deteriorates as the interaction progresses, reaching random chance in the later\ntrials. This happens even though it is receiving an increasing amount of information that\nshould allow it to improve its performance. A possible cause for this trend is the dramatic\nincrease in the prompt size, especially as more and more images are added, as the interaction\nprogresses. We further discuss this issue and its potential causes in Section 6.\n5.1\nHistory and Context Impact\nFollowing the observations with L1, we design three variants with simplified referential\ncontexts and history to better understand how well the models handle the interaction history\nand the referential context:\nL2: No History Each of the 24 trials is given to the model in isolation, without any\nhistory. The model input includes the context of four images and the speaker\nutterance, as well as the basic game instruction. This variant reveals the extent\nto which the model can reason about the ad-hoc conventions formed in repeated\nhuman-human interaction without access to the history in which they were formed.\nL3: Images Once\nA potential challenge of L1 is the large number of images in the\nprompt. A model’s architecture or training may not be suitable for handling a\nlarge number of images, and LLM prompt length is known to adversely influence\n7\nPublished as a conference paper at COLM 2024\n1\n2\n3\n4\n5\n6\n25\n50\n75\n100\nAcc %\nL1: Standard Listener\n1\n2\n3\n4\n5\n6\nRepetition #\nL2: No History\n1\n2\n3\n4\n5\n6\nL3: Images Once\n1\n2\n3\n4\n5\n6\nL4: No Shuffle\nIDEFICS\nLLaVa\nGPT4\nGemini\nClaude\nHuman (Standard Setup)\nFigure 3: Listener experiments. Margins of Error are 95% bootstrapped CIs.\nperformance (Liu et al., 2023b). L3 uses a shorter history, by only providing the\nreferential context (four images) to the model once in the first trial. Image labels are\npersistent across all trials, which also avoids the impact of shuffling. Unlike L2, this\nvariant includes the complete message, selection, and feedback history.\nL4: No Shuffle The four images appear every trial similar to L1 but are not shuffled\nacross trials. L4 shows the effects of image shuffle if compared with L1. It also\nshows the effect of image quantity if compared with L3, which does not involve\nshuffling either.\nL2 and L3 need only four images in the prompt, allowing us to test all the models.\nThe no-history variant (L2) reveals different performance trends for IDEFICS and GPT4\ncompared to the standard setup (L1). IDEFICS is still not doing great (45.8% on the first\nrepetition), but performance largely remains consistent across repetitions, indicating that\npossibly the complexity of the prompt is at the root of its downward trend in the L1 scenario.\nGPT4 starts with similar performance (87.5%) to its results in L1, but then shows a slight\ndownward trend (83.8% at the end), in contrast to the initial upward trend in L1. This\nindicates that the gradually conventionalized, shorter messages a human speaker uses tend\nto be more difficult for the model to resolve and that the conversation history can be an\neffective remedy. Gemini (80.2→78.8%), LLaVa (72.7→71.3%), and Claude (57.4→55.5%)\nshow similar trends to GPT4 from Repetition 1 to 6, with overall lower accuracies.\nThe benefit of history becomes more conspicuous when the referential context is only given\nonce (L3). This variant dramatically simplifies the prompt, but retains the information\nneeded for convention formation (i.e., prior message, selection, and feedback). All the\nmodels show an upward trajectory as the interactions progress. GPT4 and Claude are\nthe strongest, eventually reaching 100% accuracy, matching human listeners’ performance\n(99.54%). This suggests that all models can associate the current message with the relevant\nprior messages, thereby increasing their prediction accuracy as the interaction progresses.\nAdmittedly, because an image always has the same label throughout the game under this\nsetup, the models may do well by simply drawing associations between an image’s label\nand the messages that have referred to that image (i.e., label-message associations). Under\nthis mechanism, the model can improve its accuracy without reasoning about the actual\nvisual input. We explore this possible mechanism with more game variants in Appendix D.\nNonetheless, this experiment shows that MLLMs possess some capabilities for increasingly\nefficient communication with humans when acting as the listener.\nThe no-shuffle (L4) experiment also provides key insights. GPT4’s accuracy is similar to\nthat in L3 and higher than that in L1. This demonstrates sensitivity to image shuffling and\nthe constantly changing image labels. GPT4 in L3 and L4 may be relying to some degree on\ntext similarity between repetitions by exploiting the label-message associations, rather than\ngrounding to the visual input. We study this further in Appendix D. IDEFICS’ performance\n8\nPublished as a conference paper at COLM 2024\non the other hand is different from both L3 and L1, showing an almost unnoticeable trend\nup. This shows that it is not only the shuffling but also the increase in the number of images\nthat IDEFICS cannot seem to handle well. We further discuss this issue in Section 6.\n6\nDiscussion\nOur studies point to various issues that likely hinder specific models from displaying\ncommunication efficiency gains, and point out directions for future works.\nTendency to Repeat Messages\nIn the speaker study, IDEFICS and LLaVa tend to repeat\nthe first message they use for each image, showing no adaptation. To further study how\nmuch these models prefer patterns of repetition, we design a test that uses these models\nfor language modeling rather than text generation. We construct two transcripts for each\nof the 54 human-human interactions in our original dataset. One is the original transcript\nfrom human-human interactions, showing the natural evolution of messages. The other is\na manipulated transcript where the speaker repeats the messages from Repetition 1 in all\nthe later repetitions. We calculate the log probability and perplexity IDEFICS and LLaVa\nassign to these transcripts, count the number of times one type of transcript has better log-\nprobability\/perplexity than the other, and apply a sign test. We find that the manipulated\ntranscript showing message repetitions consistently receives higher log probability and\nlower perplexity for all 54 interactions, showing the models’ significant tendency towards\nrepeated patterns (sign test p-values are near zero). Unfortunately, this experiment cannot\nbe done with GPT4, Gemini, or Claude due to API limitations.\nLexical Efficiency ̸= Communication Efficiency\nThe convergence of human speaker mes-\nsages for a particular image to a short, stable convention often takes the form of extracting\nsalient tokens from the previous message and sticking to the same message once it becomes\nvery short (Hawkins et al., 2020a). Unless directly instructed to do so through a highly\nengineered prompt (S4), GPT4, Gemini, and Claude often introduce new words when\nshortening their messages or even when the messages cannot be further shortened, as\nshown in the S3 explicit instruction variant. Such inconsistency with human behaviors is\nproblematic. When messages for the same image do not converge, no conventions can form\nand the listener will likely need additional cognitive effort to process the previously unseen\nwords. Intuitively, even if a new message is semantically similar to a previous one by using\nsynonyms, resolving it still likely entail a greater cognitive load than an exact repetition.\nMoreover, when new words describing a new aspect of an image are introduced after a\nfew rounds of relatively similar messages, they violate the human listener’s expectation,\npotentially leading to miscommunication and slower response (Metzing & Brennan, 2003).\nPerformance Degradation with Many-image Inputs\nAmong the models that support a\nlarge number of images, IDEFICS performs much worse as the number of images increases.\nEven though the images in L4 are not shuffled across trials, which could have allowed the\nmodel to exploit label-message associations as an efficient way to gain high accuracy, the\nmodel still had much lower accuracy than when the images only appear once (L3) (Figure 3).\nWhen the history contains a growing number of images that are shuffled between trials (L1),\nIDEFICS shows an even worse accuracy trend.\nA likely hypothesis is that a greater number of images creates challenges for capturing the\ndependency between specific visual input and textual cues, which can manifest as failures to\nassociate an image’s label with the actual content of the image. In a qualitative experiment,\nwe supply a sequence of images and their labels as input to IDEFICS, and instruct it to\ndescribe Image [X]. We observe that IDEFICS can describe the correct image easily when we\ngive up to four labeled images, but often makes mistakes as the number increases (Figure 9\nin the appendix). Therefore, even though IDEFICS is designed and trained to support\nmulti-image inference,7 its multi-image capabilities do not generalize beyond a few images.\n7The Flamingo architecture behind IDEFICS supports an arbitrary number of images as input and\nit is trained on interleaved texts and multiple images (Huggingface, 2023).\n9\nPublished as a conference paper at COLM 2024\nAnother potential cause is the Flamingo architecture behind IDEFICS. Each token’s cross-\nmodal attention only applies to the visual features of the last image that precedes it, rather\nthan all the images. Information about other images can only be indirectly accessed through\nself-attention on the hidden states of their respective <image> tokens in the text sequence.\nThis architecture may degrade the ability of the model to reason about images, depending\non their locations in the input. When the target is not the last image, Image D, the message\ntokens will not have direct cross-modal attention to the target’s visual features, which may\nhurt IDEFICS’ prediction. In L3, where IDEFICS did perform well, this limitation might have\nbeen mitigated by the strong textual cues and label-message associations, whereas having\nmore images may distract IDEFICS from these cues and thus manifests this limitation.\n7\nConclusion\nICCA provides a perspective into the performance of today’s MLLMs that is missing in\nexisting benchmarking, and can be easily applied to new MLLMs without collecting new\nhuman data. We observe that state-of-the-art models lack the in-context abilities to adapt\ntheir own language for efficient communication, even though they may sometimes perform\nbetter while passively receiving increasingly efficient language from their interlocutor. This\nissue is fundamental because, unlike humans, the models do not perceive the effort or\ncost needed for communication, thus having no inherent reason to reduce them. It is\nstill surprising though, given that LLMs\/MLLMs have successfully displayed many other\nhuman behaviors and impressive abilities in various applications, by learning from the large\namounts of human data, where adaptation for efficiency is common. Overall, the current\nparadigm for creating LLMs fails to address the need for conversational adaptation and\nfuture research is needed on improving their abilities to spontaneously improve language\nefficiency, maintain language consistency for the same referent, avoid excessive tendency\nfor repetitions, and handle more images in a single query.\nAcknowledgments\nThis research was supported by ARO W911NF21-1-0106, NSF under grant No. 1750499, a\ngift from Open Philanthropy, and a gift from Apple. We thank Robert Hawkins and Marten\nvan Schijndel for insightful discussions. We thank the anonymous reviewers and the area\nchair for their valuable feedback.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 多模态大语言模型中的对话适应性评估\n\n## 📌 背景痛点\/本文动机\n人类在交流过程中会自发地使用越来越高效的语言，通过适应和形成临时的约定来提高沟通效率。这种现象在参考游戏中得到了广泛的研究，揭示了人类语言的特性，超越了仅仅传达意图的功能。然而，目前尚不清楚多模态大语言模型（MLLMs）是否在交互过程中同样能够提高沟通效率，以及它们可能采用哪些机制来实现这一目标。\n\n## 🚀 核心方法\n本文提出了ICCA（In-context Conversational Adaptation），一个自动化的框架，用于评估MLLMs在上下文中的对话适应性。ICCA使用人类-人类参考游戏交互的语料库，允许完全自动化的评估，无需进一步的人类交互，使其易于部署以分析新模型。该框架模拟了模型与人类交互的场景，通过比较模型行为的变化来评估其对话适应性。\n\n## 📈 实验结果\n本文评估了五个具有代表性的MLLMs：IDEFICS、LLaVa-1.5、GPT4-vision、Gemini 1.0 Pro Vision和Claude 3 opus。研究发现，所有模型都难以自发地引入约定并作为说话者进行适应。通过为每个模型设计特定的指令，可以部分解决这一问题。最强的模型（GPT4、Gemini和Claude）可以逐渐使用更短的消息（获得词汇效率），但仍然难以实现收敛或稳定性，这阻碍了真正高效沟通的出现。当作为听众时，GPT4显示出接近人类的适应趋势，随着交互的进行，其准确性不断提高，而其他模型则表现出较少的这种行为或仅在简化的设置下表现出这种行为。\n\n## 💬 可借鉴之处\n本文的研究结果表明，尽管当前的MLLMs可能被动地理解其对话者的不断发展变化的语言，但它们适应自身语言以提高沟通效率的能力并没有自然地从它们的训练或指令调整中产生。这为未来的研究指明了重要的方向，包括提高模型的自发语言效率、保持语言一致性、避免过度重复倾向以及处理单个查询中的更多图像。","llm_summary_res_status":200}
{"title":"MLVU: Benchmarking Multi-task Long Video Understanding","authors":"Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu","summary":"The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.","url":"http:\/\/arxiv.org\/abs\/2406.04264v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2406.04264v3","published":1717693772000,"comment":null,"pdf_text":"MLVU: Benchmarking Multi-task Long Video Understanding\nJunjie Zhou* 1,2, Yan Shu∗1, Bo Zhao∗1,3, Boya Wu1, Zhengyang Liang1, Shitao Xiao1,\nMinghao Qin1, Xi Yang1, Yongping Xiong2, Bo Zhang4, Tiejun Huang1,5, Zheng Liu† 1\n1 Beijing Academy of Artificial Intelligence, 2 Beijing University of Posts and Telecommunications,\n3 Shanghai Jiao Tong University, 4 Zhejiang University, 5 Peking University\n{junjiebupt, bozhaonanjing, zhengliu1026}@gmail.com\nAbstract\nThe evaluation of Long Video Understanding (LVU) perfor-\nmance poses an important but challenging research problem.\nDespite previous efforts, the existing video understanding\nbenchmarks are severely constrained by several issues, es-\npecially the insufficient lengths of videos, a lack of diversity\nin video types and evaluation tasks, and the inappropriate-\nness for evaluating LVU performances. To address the above\nproblems, we propose a new benchmark called MLVU (Multi-\ntask Long Video Understanding Benchmark) for the compre-\nhensive and in-depth evaluation of LVU. MLVU presents\nthe following critical values: 1) The substantial and flex-\nible extension of video lengths, which enables the bench-\nmark to evaluate LVU performance across a wide range of\ndurations. 2) The inclusion of various video genres, e.g.,\nmovies, surveillance footage, egocentric videos, cartoons,\ngame videos, etc., which reflects the models’ LVU perfor-\nmances in different scenarios. 3) The development of di-\nversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs’ key abilities in long-video under-\nstanding. The empirical study with 23 latest MLLMs reveals\nsignificant room for improvement in today’s technique, as all\nexisting methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling\nlonger videos. Additionally, it suggests that factors such as\ncontext length, image-understanding ability, and the choice\nof LLM backbone can play critical roles in future advance-\nments. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive\nand in-depth analysis of MLLMs.\n1. Introduction\nLarge language models (LLMs) are growing into a general\nsolution for numerous AI tasks [6, 45]. In recent years,\nit becomes increasingly emphasized to extend LLMs with\nmulti-modal capabilities and thus bring the Multi-modal\n*Co-first authors\n†Corresponding author\nLLM, namely, MLLM. Remarkably, it has been made pos-\nsible for today’s MLLMs to perceive information in texts,\nimages, videos, etc., and solve complicated problems in\nphysical environments [1, 44]. Along with the development\nof MLLMs, new benchmarks are continuously created to\nfacilitate comprehensive and in-depth analysis of MLLMs\n[12, 26, 32, 57].\nHowever, it remains a great challenge to evaluate the\nMLLMs’ long-video understanding (LVU) performances\ngiven the following limitations. Firstly, the majority of exist-\ning video understanding benchmarks are made up of short\nvideos [19, 22, 26, 36, 52], whose lengths can be merely a\nfew seconds. As a result, they are insufficient to reflect the\nMLLMs’ long-video understanding capabilities. Secondly,\nthere is a notable lack of diversity in both video genres and\nevaluation tasks. Existing benchmarks often concentrate on\na single video type, such as egocentric videos [15, 34], or\nfocus on one specific task, like captioning [52]. These limi-\ntations hinder comprehensive evaluation of LVU capabilities.\nLast but not least, many previous evaluation tasks are not\nproperly designed for LVU, as they can be solved without\nusing the complex information from long videos. For ex-\nample, many questions are simply about one single frame\nin the long videos [41, 60]. Besides, numerous others are\nabout popular movies and celebrities [13, 27], which can be\nanswered directly by MLLMs based on the textual prompts.\nConceptually, MLLMs are expected to handle any type of\nlong video and accomplish any related tasks. Therefore, the\nevaluation of LVU should emphasize two important prop-\nerties: length and diversity. Furthermore, it is crucial that\nthe evaluation tasks are specifically designed to leverage the\ncomplex information inherent in long videos, addressing\nthe shortcomings of previous benchmarks. Based on these\nprinciples, we propose a novel benchmark called MLVU\n(Mult-task Long Video Understanding Benchmark), which\npresents the following critical advantages.\n• It makes a substantial extension for the video length.\nMLVU is created based on long videos of diversified\nlengths, ranging from 3 minutes to 2 hours. The average\n1\narXiv:2406.04264v3  [cs.CV]  1 Jan 2025\nBenchmarks\n#Videos\n#QA\nPairs\nLen. (s)\nClose-\nEnded\nOpen-\nEnded\nVarious\nGenres\nMulti-\nLevel\nMulti-\nDimension\nReferring\nQA\nNExT-QA [50]\n1,000\n8,564\n39.5\n✓\n✓\n✓\n✗\n✗\n✗\nTVQA [21]\n15,253\n15,253\n11.2\n✓\n✗\n✗\n✗\n✗\n✗\nMSRVTT-QA [52]\n2,900\n72,821\n15.2\n✓\n✗\n✗\n✗\n✗\n✗\nMVBench [26]\n3,641\n4,000\n16.0\n✓\n✗\n✓\n✗\n✗\n✗\nMovie101 [58]\n101\n-\n6144\n✗\n✓\n✗\n✗\n✗\n✗\nEgoSchema [34]\n5,063\n5,063\n180\n✓\n✗\n✗\n✗\n✗\n✗\nMovieChat-1K [41]\n130\n1,950\n500\n✓\n✓\n✗\n✗\n✓\n✗\nVideo-MME∗[13]\n900\n2,700\n1024\n✓\n✗\n✓\n✓\n✗\n✗\nLongVideoBench∗[49]\n3,763\n6,678\n473\n✓\n✗\n✓\n✓\n✗\n✓\nMLVU\n1,730\n3,102\n930\n✓\n✓\n✓\n✓\n✓\n✓\nTable 1. Comparison of MLVU with existing benchmarks, including the number of videos (#Videos), number of QA pairs (#QA pairs),\naverage video length (Len.), presence of Close-Ended tasks, presence of Open-Ended tasks, inclusion of various video genres (Various\nGenres), coverage of multiple duration levels (Multi-Level), inclusion of multiple dimensions of LVU tasks (Multi-Dimension), and\nquestions involving local information with clear referring context rather than direct timestamps [41] or well-known narrative elements [17, 27]\n(Referring QA). The first block represents short video understanding benchmarks, and the second block represents long video understanding\nbenchmarks. ∗denotes work concurrent with MLVU.\nvideo length is about 15 minutes, which makes it much\nlonger than most of the existing benchmarks. Additionally,\neach video is further segmented so that evaluation tasks\ncan be created w.r.t. different video clips (e.g., summa-\nrization for the first 3 minutes, the first 6 minutes, and the\nentire duration of the video). Therefore, it is able to flex-\nibly evaluate the MLLMs’ performance across different\nvideo lengths.\n• It encompasses a wide variety of video genres. MLVU\nincludes diverse real-world videos, such as movies, life\nrecords, and egocentric videos. Additionally, it features\ntypical simulated videos like games and cartoons. This di-\nversity allows for a comprehensive assessment of MLLMs’\nperformance across various application scenarios.\n• It introduces diversified evaluation tasks tailored for\nLVU. MLVU comprises 9 distinct tasks that collectively\nassess a wide range of MLLMs’ LVU capabilities. On\none hand, it includes both multiple-choice and open-ended\ngeneration tasks, reflecting the models’ performance in\nhandling different task formats. On the other hand, some\ntasks are designed to leverage global information from\nentire videos, while others require the use of specific local\ninformation from certain clips. Moreover, all questions in-\nvolving local information are annotated with unambiguous\ncontext, requiring MLLMs to accurately locate or infer the\nappropriate clips within long videos.\nTable 1 shows that MLVU provides a more comprehen-\nsive evaluation of LVU compared to existing and concurrent\nbenchmarks. We extensively investigate 23 popular MLLMs\nwith MLVU, which brings in several critical insights. Firstly,\nlong-video understanding remains a technically challenging\nproblem for the existing MLLMs. While GPT-4o1 achieves\nthe leading performance in the experiment, it only attains\n1https:\/\/openai.com\/index\/hello-gpt-4o\/\nan average score of 54.5% in multi-choice tasks. All meth-\nods struggle with tasks requiring fine-grained information\nfrom entire videos, such as action counting, ordering, and\nsummarization. Secondly, recent open-source long video\nMLLMs have made significant strides in LVU [11, 40, 60].\nThese advancements have improved the models’ capability to\nprocess extended visual sequences, thereby closing the gap\nwith leading proprietary models in recent months. Finally,\nthe empirical results underscore influential factors in LVU,\nsuch as the extension of context length, the improvement\nof image understanding ability, and the utilization of strong\nLLM-backbones. In addition to the benchmark’s overall\nconclusion, individual tasks enable fine-grained analysis of\nMLLMs’ performances in each specialized aspects. There-\nfore, we anticipate the benchmark to assist in improving\nMLLMs’ long-video understanding capabilities by provid-\ning insights into their current strengths and weaknesses.\n2. Related Work\nMultimodal Large Language Models.\nMultimodal large\nlanguage models (MLLMs) have attracted significant interest\nfrom both academia and industry. Recent advancements in\nthis field have been achieved by integrating LLM backbones\nwith visual encoders and adapters, and fine-tuning the en-\ntire architecture through visual instruction tuning [8, 29, 63].\nBased on the same philosophy, MLLMs have been further de-\nveloped for video processing using video instruction datasets\nand specialized video adapters [25, 26, 28, 33, 54, 59]. How-\never, most existing models are optimized for short videos,\ntypically under one minute, due to the difficulty in estab-\nlishing sufficient context for longer videos. To address this\nchallenge, researchers have explored compact video repre-\nsentations or extended the context length of MLLMs. For\ninstance, LLaMa-Vid [27] compresses each video frame\n2\nAction\nRomance\nScience Fiction\nSuspense\nWar\nComedy\nAction\nFamily\nFantasy\nAdventure\nGeography\nAnimal\nScience\nCooking\nHandcraft\nSoftware\nBasketball\nFootball\nPingpong\nVolleyball\nBadminton\nTravel\nPet\nFood\nDaily Life\nOutdoor\nIndoor\nIndoor\nOutdoor\nCartoon\nGame\nMovie\nTV Series\nDocumentary\nTutorial\nSport\nLife Record\nEgo-centric\nSurveillance\nSimulation\n0\n100\n200\n300\n400\n500\n(3, 5]\n(5, 8]\n(8, 11]\n(11, 15]\n(15, 30]\n(30, 120]\n(120, )\nNumber of Videos\nVideo Duration (min)\nPlot QA\n589\nEgo Reasoning\n405\nNeedle QA\n415\nSub-Scene\nCaptioning\n247\nAnomaly Recognition\n239\nTopic Reasoning\n355\nVideo\nSummarization\n257\nAction \nCount\n266\nAction\nOrder\n329\nMulti-Detail Task\nOpen-ended Task\nClose-ended Task\nHolistic Task \nSingle-Detail Task\nFigure 1. Statistical Overview of our MLVU benchmark. Left: Video genres included in MLVU; Top Right: Distribution of video duration;\nBottom Right: Task types and their counts in MLVU.\ninto two tokens, enabling the model to handle videos sev-\neral hours long. Methods like MovieChat [41] and MA-\nLMM [16] introduce specialized memory components for\nrecursive video processing. Furthermore, approaches such\nas LWM [30], LongVA [60], and Video-XL [40] are de-\nsigned to extend the context length of MLLMs, facilitating\nthe processing of longer video inputs. Additionally, it is\nalso explored to make selective usage of frames or clips\nfrom long videos based on retrievers or agents [38, 46, 53].\nDespite these progresses, it remains an open problem for\nMLLMs to effectively handle long videos.\nVideo Understanding Benchmarks.\nWith the unprece-\ndented interest in MLLMs, the creation of benchmarks for\nthese models has become increasingly emphasized (as ad-\nvanced by MMMU [57], MME [12], and many other pio-\nneering works). In video understanding, the research com-\nmunity has made significant efforts as well, particularly for\nshort videos. There are specialized benchmarks for temporal\nperception [48, 56], action understanding [47, 48], video\nclassification [18], video reasoning [50, 51], and video cap-\ntioning [35, 52]. Recently, MVBench [26] provides a com-\nprehensive short-video benchmark to evaluate general capa-\nbilities via question-answering. For long video understand-\ning, people seek to leverage long-form videos, like movies,\nto create benchmarks.\nFor example, LLaMA-Vid [27]\ndeveloped a movie question-answering dataset based on\nMovieNet [17]. Despite using long videos, many questions\nfocus on well-known narrative elements, allowing them to be\nanswered without analyzing the video’s content. In contrast,\nMovieChat [41] avoids specific character names or plot de-\ntails in its questions. However, since each question provides\na specific timestamp, the tasks can be reduced to short-video\nor image understanding problems. Beyond movies, there\nare task-specific benchmarks like EgoSchema [34], which\npresents video reasoning tasks using first-person footage\nfrom Ego4D [15]. These specialized benchmarks, however,\nfocus on a single aspect of MLLMs rather than offering a\ncomprehensive analysis of long video understanding. There-\nfore, it is essential to develop a comprehensive benchmark\nwith carefully designed tasks to effectively evaluate MLLMs’\ncapabilities in understanding long videos.\n3. MLVU: Multi-task Long Video Understand-\ning Benchmark\nIn this section, we start with an overview of MLVU, which\nhighlights its constitution and explains its values over the\nprevious works. Then, we discuss how each evaluation task\nis constructed in MLVU.\n3.1. Overview\nMLVU is a multi-task benchmark consisting of 3,102 ques-\ntions across 9 categories, specifically designed for long video\nunderstanding. It is divided into a dev set and a test set,\ncontaining 2,593 and 509 questions, respectively. The bench-\nmark is distinguished by the following features.\nDiversified Video Categories. MLVU offers a compre-\nhensive collection of videos across various categories (Fig-\nure 1 Left). These include typical real-world videos such as\nmovies, documentaries, TV series, egocentric videos, life\nrecords, sports, tutorials, and surveillance footage. Addition-\nally, it features significant simulated videos from animated\nseries and game videos.\nSubstantial Extension of Video Length. MLVU is made\n3\nup of videos of diversified lengths, spanning from 3 min to\nmore than 2 hours (Figure 1 Top Right). Besides, each video\nis further partitioned as incremental segments, e.g., the first\n3 min, the first 6 min, and the entire video, where tasks are\ncreated for each individual segment. Thus, the MLLMs can\nbe flexibly evaluated across different video lengths.\nDiversified Evaluation Tasks. MLVU also provides a\ndiverse array of evaluation tasks, which are closely aligned\nwith the common visual capabilities of MLLMs, such as\nreasoning, captioning, recognition, perception, and summa-\nrization (Figure 1 Bottom Right). All the tasks are tailored\nfor LVU. That is to say, the tasks need to be solved based\non the in-depth understanding of video. Some of tasks are\nto examine whether the global information from the entire\nvideo can be effectively utilized (holistic LVU); while oth-\ners focus on whether the MLLMs can make precise usage\nof proper local information within the long video (detail\nLVU). Additionally, both multi-choice and free-form gener-\nation tasks are included in MLVU, which help to examine\nMLLMs’ capabilities in handling different task formats.\n3.2. Construction of MLVU\nThe evaluation tasks of MLVU can be categorized into three\ntypes: 1) holistic LVU, which needs to be solved by mak-\ning use of the global information from the entire video; 2)\nsingle-detail LVU, which relies on leveraging one critical\nplot within the long video; and 3) multi-detail LVU, which\nnecessitates the joint utilization of multiple plots within the\nlong video. The construction process of MLVU is discussed\nw.r.t the above three categories. To facilitate the discussion,\nwe define ULVC (Universal Long Video Collection) as the\nuniversal collection of long videos from various sources\n(more details about ULVC are presented in Appendix C).\n3.2.1. Holistic LVU\nTopic Reasoning (TR). The topic reasoning task requires\nMLLMs to respond to questions about the principal subject\nof a long video, as shown with Figure 2 (a). This includes\nelements such as the video’s genre, pivotal events, or pri-\nmary settings. All questions and answers undergo manual\nannotation2, resulting in a total of 355 questions. TR tasks\nare formatted as multiple-choice questions, with the model’s\nperformance assessed based on accuracy.\nAnomaly Recognition (AR). The anomaly recognition task\ninvolves identifying the anomalous behavior within a surveil-\nlance footage (Figure 2 b). We leverage the surveillance\nvideo clips from UCF Crime dataset [43] for this task. The\nselected video clips are longer than three minutes. We create\n239 questions based on the original annotations provided by\nthe dataset. The AR task is also conducted in the multiple-\nchoice format, whose performance is measured by accuracy.\n2Detailed information and annotation guidelines for annotators are pre-\nsented in Appendix F.\nVideo Summarization (VS). This task requires MLLMs\nto summarize the key events in a long video (Figure 2 c).\nWe select the narrative-rich videos from ULVC for this task,\nincluding movies, TV series, documentaries, life records,\nand animated series. There are 257 selected videos in total,\nwhose summaries are manually annotated. During evalua-\ntion, the MLLMs are prompted with \"Please summarize the\nmain content of this video\". We employ GPT-4 to assess the\ngenerated summaries by comparing with the annotation re-\nsults. Details about annotation and evaluation are presented\nin Appendix F.3 and G.3.\n3.2.2. Single-Detail LVU\nNeedle\nQuestion-Answering\n(NQA).\nNeedle-In-the-\nHaystack-Search (NIHS) is a popular evaluation task for\nlong-context LLM [31]. Taking the inspiration from NIHS,\nwe create Needle Question-Answering (NQA), shown as\nFigure 2 (d). In this task, the MLLM is required to answer a\nquestion related to a specific segment (referred as needle)\nwithin a long video (referred as background video). The\nneedles are short video clips sampled from WebVid [5]\nand Clevrer [55], while the background videos are sampled\nfrom our ULVC. The needle is randomly inserted into\nthe background video, where a question-answer pair is\nannotated. By incorporating necessary details, the question\ncan always correspond to the needle without ambiguity.\nDuring evaluation, the MLLM needs to infer the location\nof the needle based on the details provided in the question,\nand solve the problem on top of the needle’s information.\nThe NQA task is structured as multiple-choice, whose\nperformance is measured by accuracy.\nEgo Reasoning (ER). Ego-centric videos capture a series\nof consecutive actions from a first-person perspective. The\nMLLM needs to reason for a question about a specific be-\nhavior in the video, e.g., predicting for the event which is\ncorrelated or satisfies a certain causal relationship with the\nbehavior (Figure 2 e). Both videos and QA annotations are\ncollected from the NLQ task of Ego4D [15]. The ER task is\nstructured as multiple-choice, with a total of 405 questions\ncreated for this task.\nPlot Question-Answering (PQA). In this task, the MLLM\nneeds to reason for questions about a plot in a narrative\nvideo, shown as Figure 2 (f). The video is sampled from the\nmovies, TV series, and animated series in our ULVC. There\nare 589 question-answer pairs created by manual annotation.\nDuring annotation, the human annotators are asked to only\nprovide necessary details about the plot but not to suggest\nany objective hints, e.g., the two characters in the example\nvideo are referred as cat and mouse, rather than Tom and\nJerry. Therefore, it can prevent the question from being\nshort-cut by the MLLM’s common-sense knowledge (more\ndetails about PQA can be found in the Appendix F.6).\nSub-Scene Captioning (SSC). In this task, the MLLM needs\nto generate the caption for a sub-scene in a long video. The\n4\nFigure 2. Examples of MLVU. There are nine tasks designed to evaluate the holistic, single-detail, and multi-detail LVU capabilities of\nMLLMs. The MLLMs are asked to solve the problem (with the ground-truth answers marked in blue) based on the long video input and\ntextual prompt. For multiple-choice questions, we set 4 candidates in the dev set and 6 candidates in the test set.\n5\nlong videos in SSC are sampled from the Movie101 dataset\n[58], while the questions and answers are manually anno-\ntated. During annotation, the human annotator is asked\nto provide a detailed description for the sub-scene as the\nground-truth answer. Besides, they need to offer necessary\nclues in their questions such that the referred sub-scenes\ncan be identified without ambiguity. During evaluation, we\nemploy GPT-4 [1] to measure the quality of caption in com-\nparison with the ground-truth. Details about annotation and\nevaluation are presented in Appendix F.7 and G.3.\n3.2.3. Multi-Detail LVU\nAction Order (AO). In this task, the MLLM needs to predict\nthe right order for a sequence of actions (Figure 2 h). The\nactions are presented by short video clips, called probes.\nThe probes are formulated in two different ways. One is\nmade up of clips from the Kinetics dataset [18], where each\nclip represents a distinct action. The other one is from the\nconsecutive clips of an action in the ActivityNet-Caption\ndataset [20]. The probes are inserted into a long background\nvideo, which is sampled from ULVC. There are 329 AO\nquestions in total. The task is structured as a multiple-choice\nprblem, where the right order is selected from the misleading\noptions provided by the annotator.\nAction Count (AC). This task requires the MLLM to count\nthe occurrences of an action within a long video (Figure 2\ni). Each action corresponds to multiple short probe clips\nsampled from the Kinetics dataset [18]. The probes of an ac-\ntion are inserted into a long background video sampled from\nULVC. We also perform manual examination to ensure that\nthe inserted action does not exist in the original background\nvideo. A total of 266 evaluation instances have been created.\nThe AC task is structured as a multiple-choice problem, with\nperformance measured by accuracy.\n4. Experiments and Analysis\n4.1. Settings\nWe conduct a comprehensive investigation of 23 MLLMs us-\ning our MLVU benchmark, encompassing both open-source\nand proprietary models. The experimental MLLMs are di-\nvided into three categories: 1) Image MLLMs, primarily\nfine-tuned using image-related instructions; 2) Short Video\nMLLMs, fine-tuned with short-video related instructions;\nand 3) Long Video MLLMs, optimized for long-video under-\nstanding capability. For Image MLLMs, we leverage their\nmulti-image inference capabilities to process segmented\nframes from original videos. For Video MLLMs, we employ\neither a uniform sampling strategy or a frame rate sampling\nstrategy for video processing. All models are evaluated\nbased on their official implementations or available APIs,\nwith evaluations conducted in a zero-shot manner. More\ndetails about the evaluation are provided in Appendix G.\n4.2. Main Results\nThe overall evaluation results for all investigated MLLMs\nin the MLVU test set are shown in Table 2 (with dev set re-\nsults in Appendix B). Individual performances are reported\nfor each task, while average performances are provided\nfor multiple-choice (M-Avg) and generation tasks (G-Avg).\nFrom the results, we derive three primary conclusions:\n1) The proprietary model GPT-4o [37] achieves opti-\nmal performance in our benchmark. It leads in multiple-\nchoice tasks with an M-Avg of 54.5%(within 0-100%) and\nexcels in generation tasks with a G-Avg of 5.87 (within\n0.0-10.0), outperforming all other methods.\n2) Recent advances in LVU have achieved significant\nprogress, and the gap between open-source long video\nMLLMs and GPT-4o on close-ended tasks is narrowing.\nBefore June 2024, the best open-source long video MLLMs,\nMiniGPT4-Video [3], lagged significantly behind GPT-4o.\nHowever, recent models [11, 24, 40, 60] have made substan-\ntial progress. For instance, LLaVA-Onevision trails GPT-4o\nby only 2.8% in M-Avg. These models have improved their\nability to handle long visual sequences, achieving significant\nadvancements in single-detail (e.g., NQA) and multi-detail\n(e.g., AC) tasks compared to previous open-source models.\n3) Existing methods still struggle to handle most tasks\nin our benchmark. For instance, GPT-4o only achieves\n42.9% in the needle question-answering (NQA) task. In\ncontrast, analogous tasks in the text domain, such as NIHS\n(Needle-In-the-Haystack-Search) and Passkey Retrieval, are\neffectively handled by many existing long LLMs [14, 61].\nAdditionally, GPT-4o shows even less reliability in tasks like\nego-reasoning (ER), action ordering (AO), and action count\n(AC), with most baseline methods performing even worse.\nThese observations indicate that long-video understanding\nremains a significant challenge for today’s MLLMs.\nIn addition to the primary conclusions from the overall\nperformances, we can also make the following interesting\nobservations about the individual tasks.\n4) The close-ended holistic tasks present much higher\ndifferentiation than other tasks. These tasks, i.e., topic\nreasoning (TR) and anomaly recognition (AR), show sig-\nnificant variance in performance across different models.\nProprietary MLLMs, like GPT-4o, and superior open-source\nmodels, such as InternVL-2 [8], VideoLLaMA2 [9], and\nLLaVA-OneVision [24], can accurately solve these prob-\nlems. Meanwhile, many other popular MLLMs still fail to\ngenerate meaningful performances. Since these tasks only\nrequire an overall understanding of long videos, they can\nserve as a preliminary indicator of MLLMs’ long video un-\nderstanding (LVU) ability.\n5) It’s challenging to deal with tasks that require nu-\nanced understanding of multiple details. Although several\nMLLMs can handle single-detail LVU tasks to some ex-\ntent, their performances suffer from catastrophic degradation\n6\nMethods\nDate\nInput\nHolistic\nSingle Detail\nMulti Detail\nM-Avg\nG-Avg\nTR\nAR\nVS∗\nNQA\nER\nPQA\nSSC∗\nAO\nAC\nFull mark\n–\n–\n100\n100\n10\n100\n100\n100\n10\n100\n100\n100\n10\nRandom\n–\n–\n16.7\n16.7\n–\n16.7\n16.7\n16.7\n–\n16.7\n16.7\n16.7\n–\nImage MLLMs\nOtter-I [23]\n2023-05\n16 frm\n17.6\n17.9\n2.03\n16.7\n17.0\n18.0\n3.90\n15.7\n16.7\n17.1\n2.97\nLLaVA-1.6 [29]\n2024-01\n16 frm\n63.7\n17.9\n2.00\n13.3\n26.4\n30.0\n4.20\n21.4\n16.7\n27.1\n3.10\nInternVL-2 [8]\n2024-07\n16 frm\n85.7\n51.3\n2.55\n48.3\n47.2\n52.0\n5.25\n32.9\n15.0\n47.5\n3.90\nClaude-3-Opus† [2]\n2024-03\n16 frm\n53.8\n30.8\n2.83\n14.0\n17.0\n20.0\n3.67\n10.0\n6.7\n21.8\n3.25\nQwen-VL-Max† [4]\n2024-01\n16 frm\n75.8\n53.8\n3.00\n15.0\n26.4\n4.84\n20.0\n20.7\n11.7\n32.2\n3.92\nShort Video MLLMs\nOtter-V [23]\n2023-05\n16 frm\n16.5\n12.8\n2.18\n16.7\n22.6\n22.0\n4.20\n12.9\n13.3\n16.7\n3.19\nmPLUG-Owl-V [54]\n2023-04\n16 frm\n25.3\n15.4\n2.20\n6.7\n13.2\n22.0\n5.01\n14.3\n20.0\n16.7\n3.61\nVideoChat [25]\n2023-05\n16 frm\n26.4\n12.8\n2.15\n18.3\n17.0\n22.0\n4.90\n15.7\n11.7\n17.7\n3.53\nVideo-LLaMA-2 [59]\n2024-08\n16 frm\n52.7\n12.8\n2.23\n13.3\n17.0\n12.0\n4.87\n15.7\n8.3\n18.8\n3.55\nVideoChat2-HD [26]\n2024-06\n16 frm\n74.7\n43.6\n2.83\n35.0\n34.0\n30.0\n5.14\n21.4\n23.3\n37.4\n3.99\nVideo-LLaVA [28]\n2023-11\n8 frm\n70.3\n38.5\n20.9\n2.30\n26.4\n26.0\n5.06\n20.0\n21.7\n29.3\n3.68\nShareGPT4Video [7]\n2024-05\n16 frm\n73.6\n25.6\n2.53\n31.7\n45.3\n38.0\n4.72\n17.1\n8.3\n34.2\n3.63\nVideoLLaMA2 [9]\n2024-06\n16 frm\n80.2\n53.8\n2.80\n36.7\n54.7\n54.0\n5.09\n42.9\n16.7\n48.4\n3.95\nLong Video MLLMs\nMovieChat [41]\n2023-07\n2048 frm\n18.7\n10.3\n2.30\n23.3\n15.1\n16.0\n3.24\n17.1\n15.0\n16.5\n2.77\nMovie-LLM [42]\n2024-03\n1 fps\n27.5\n25.6\n2.10\n10.0\n11.3\n16.0\n4.93\n20.0\n21.7\n18.9\n3.52\nLLaMA-VID [27]\n2023-11\n1 fps\n20.9\n23.1\n2.70\n21.7\n11.3\n16.0\n4.15\n18.6\n15.0\n18.1\n3.43\nMA-LMM [16]\n2024-04\n1000 frm\n44.0\n23.1\n3.04\n13.3\n30.2\n14.0\n4.61\n18.6\n13.3\n22.4\n3.83\nMiniGPT4-Video [3]\n2024-04\n90 frm\n64.9\n46.2\n2.50\n20.0\n30.2\n30.0\n4.27\n15.7\n15.0\n31.7\n3.39\nLongVA [60]\n2024-06\n256 frm\n81.3\n41.0\n2.90\n46.7\n39.6\n46.0\n4.92\n17.1\n23.3\n42.1\n3.91\nVideo-CCAM [11]\n2024-08\n96 frm\n79.1\n38.5\n2.65\n45.0\n52.8\n56.0\n4.49\n24.3\n26.7\n46.1\n3.57\nVideo-XL [40]\n2024-09\n256 frm\n78.0\n28.2\n3.40\n50.0\n41.5\n46.0\n5.02\n48.6\n31.7\n46.3\n4.21\nLLaVA-Onevision [24]\n2024-08\n32 frm\n83.5\n56.4\n3.75\n46.7\n58.4\n58.0\n5.09\n35.7\n23.3\n51.7\n4.42\nGPT-4o† [37]\n2024-05\n0.5 fps\n83.7\n68.8\n4.94\n42.9\n47.8\n57.1\n6.80\n46.2\n35.0\n54.5\n5.87\nTable 2. The overall performances on MLVU test set, including the holistic LVU tasks, the single-detail LVU tasks, and multi-detail LVU\ntasks. Date: the release date of the MLLM. M-Avg: the average performance of multiple-choice tasks; G-Avg: the average performance of\ngeneration tasks (marked by ∗). Two input strategies are used by the MLLMs in evaluation: Uniform Sampling (N frm), which evenly\nsamples N frames from the video; Frame Rate Sampling (N fps), which samples N frames per second. † denotes proprietary models.\nwhen addressing multi-detail LVU tasks. Most methods,\nexcept for GPT-4o and Video-XL [40], fail entirely in ac-\ntion order (AO) and action count (AC) tasks. Additionally,\nmost approaches struggle with summarization tasks, which\nrequire recalling multiple nuanced details from long videos.\nAs a brief conclusion, although today’s MLLMs can deal\nwith some preliminary LVU tasks, it remains a tough chal-\nlenge to achieve an in-depth understanding of nuanced infor-\nmation within long videos.\n4.3. Further Analysis\n6) Longer videos are more challenging for MLLMs.\nWe evaluate MLLMs’ performances across various video\nlengths. For this purpose, we introduce a derivative dataset\nalongside MLVU, called MLVU Time-ladder. In this dataset,\nthe same kinds of evaluation tasks are created for videos of\nvariant lengths, including 180s, 360s, and 600s (more details\npresented in Appendix D). As shown in Figure 3, the per-\nformances of all models tend to decline as the video length\ngrows, which indicates that the existing MLLMs’ LVU abili-\nties are severely constrained by the video length. Moreover,\nthe short video model Video-LLaMA-2 [59] maintains a cer-\ntain level of LVU ability at 3 minutes, but its performance\napproaches random results at 10 minutes.\n7) The performance of recent advanced long video\nMLLMs remains robust regardless of the position of the\nreferring clip within the long video. In single-detail tasks,\nthe referring clip denotes the specific segment of the long\nvideo that is referenced or inferred to answer a question.\nAs shown in Figure 4, we categorize clip positions into\nfour intervals and assess model performance on two single-\ndetail tasks: ego reasoning (ER) and plot question-answering\n(PQA). Recent long video MLLMs, such as LongVA [60]\nand Video-XL [40], maintain consistent performance re-\n7\nImpact of Context Length\nImpact of IU\nImpact of LLM\nModel\nContext Len. M-Avg\nModel\nMMMU (Val) M-Avg\nModel\nLLM\nM-Avg\nMGV\n16\n24.2\nOtter-I\n32.2\n17.1\nVLM2 Vicuna-7B 13.3\n90\n31.7↑7.5 LLaVA-1.6\n35.8\n27.1↑10.0\nVicuna-13B 18.8↑5.5\nGPT-4o\n16\n45.8\nGPT-4V\n58.1\n43.3\nMGV\nLLaMA-7B 20.6\n256\n54.5↑8.7 GPT-4o\n63.8\n45.8↑2.5\nMistral-7B 31.7 ↑11.1\nTable 3. Detailed discussions about the impact from context length, image understanding (IU) ability, and LLM Backbone. For the IU\nimpact experiment, we used 16-frame uniform sampling for both GPT-4V and GPT-4o. MGV: MiniGPT4-Video, VLM2: Video-LLaMA-2.\n180s\n360s\n600s\n20\n30\n40\n50\nAccuracy (%)\nVideo-XL\nVideoChat2\nMiniGPT4-Video\nVideo-LLAMA-2\nFigure 3. Experimental performance on varying video lengths. The\nevaluated metric is the average accuracy across five multiple-choice\ntasks involving local information: NQA, ER, PQA, AC, and AO.\n20\n28\n36\n44\nLongVA\nVideoXL\nVideoChat2 VideoLLaVA\n20\n30\n40\n50\nLongVA\nVideoXL\nVideoChat2 VideoLLaVA\nEgo Reasoning\nPlot QA\n(0, 25] \n(25, 50] \n(50, 75] \n(75, 100] \nVideoXL\nLongVA\nVideoChat2 VideoLLaVA\nVideoXL\nLongVA\nVideoChat2 VideoLLaVA\n44\n36\n28\n20\n50\n40\n30\n20\nFigure 4. Model performance across different referring clip posi-\ntions, spanning from the beginning to the end of the entire video.\ngardless of the referring clip’s position within the video.\nConversely, short video MLLMs are more sensitive to clip\nlocation. This indicates that recent advancements in long\nvideo MLLMs enhance both reliable clue retrieval and effec-\ntive reasoning from extended visual sequences.\n8) The challenge of multi-detail tasks increases with\nthe number of details. We analyzed model performance\non the action count (AC) task by grouping questions based\non the number of probes (which correspond to details) and\nevaluating the average performance within these groups. As\nshown in Figure 5, performance significantly declines across\nall models as the number of probes increases. This indi-\ncates that current MLLMs face substantial difficulties com-\nprehending and processing multiple details simultaneously,\nhighlighting a critical area for future improvement in long\nvideo understanding capabilities.\n9) Context Length, Image-Understanding ability, and\n[1,2]\n[3,4]\n[5,6]\nNumber of Probes\n20\n25\n30\n35\n40\nAccuracy (%)\nGPT-4o\nVideo-XL\nLLaVA-Onevision\nVideoChat2\nLongVA\nFigure 5. Model performance on the action count (AC) task in\nrelation to the number of probes.\nthe choice of LLM Backbones are key factors in LVU\nperformance. As shown in Table 3, we conducted abla-\ntion experiments on several factors affecting MLLMs, using\nM-Avg as the evaluation metric. First, we examined the\nmodels’ handling of different context lengths. Specifically,\nwe increased MiniGPT4-Video’s input from 16 to 90 frames\nand GPT-4o’s input from 16 to 256 frames (as shown on the\nleft side of Table 3). Both models showed consistent perfor-\nmance improvements with longer input lengths. To assess the\nimpact of image understanding (IU) capabilities, we referred\nto the results from MMMU [57] (presented in the middle of\nTable 3). It is evident that MLLMs’ LVU performance gener-\nally aligns with their IU performance in MMMU. Finally, we\ncompared MLLMs using different backbones (depicted on\nthe right side of Table 3). The findings indicate that LVU per-\nformance improves with larger (Vicuna-13B vs. Vicuna-7B)\nand more advanced backbones (Mistral-7B vs. Llama-2-\n7B). These observations indicate that LVU is the result of\nmultiple complex factors, with the ability to perceive longer\nvideos and effectively utilize the perceived information being\ncrucial for the improvement of LVU.\n5. Conclusion\nThis paper presents MLVU, a novel benchmark for the\nassessment of long video understanding.\nWith several\ncritical innovations: the substantial extension of video\nlengths, the inclusion of various video genres, and the\ndevelopment of diversified LVU-oriented evaluation tasks,\nthe new benchmark is able provide a comprehensive and\nin-depth analysis for MLLMs’ long-video understanding\nperformance. The empirical study on MLVU reveals LVU\n8\nremains a technically challenging problem for today’s\nstate-of-the-art MLLMs. Future advancements may call\nfor the joint optimization of complex factors, such as\ncontext length, image understanding ability, and even LLM\nbackbones. We anticipate this benchmark will facilitate\nfuture research in long-video understanding of MLLMs.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MLVU: Benchmarking Multi-task Long Video Understanding.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMLVU: Benchmarking Multi-task Long Video Understanding\n```\n#### 2. 论文摘要\n```\nThe evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.\n```\n\n#### 3. 论文全文\n```\nMLVU: Benchmarking Multi-task Long Video Understanding\nJunjie Zhou* 1,2, Yan Shu∗1, Bo Zhao∗1,3, Boya Wu1, Zhengyang Liang1, Shitao Xiao1,\nMinghao Qin1, Xi Yang1, Yongping Xiong2, Bo Zhang4, Tiejun Huang1,5, Zheng Liu† 1\n1 Beijing Academy of Artificial Intelligence, 2 Beijing University of Posts and Telecommunications,\n3 Shanghai Jiao Tong University, 4 Zhejiang University, 5 Peking University\n{junjiebupt, bozhaonanjing, zhengliu1026}@gmail.com\nAbstract\nThe evaluation of Long Video Understanding (LVU) perfor-\nmance poses an important but challenging research problem.\nDespite previous efforts, the existing video understanding\nbenchmarks are severely constrained by several issues, es-\npecially the insufficient lengths of videos, a lack of diversity\nin video types and evaluation tasks, and the inappropriate-\nness for evaluating LVU performances. To address the above\nproblems, we propose a new benchmark called MLVU (Multi-\ntask Long Video Understanding Benchmark) for the compre-\nhensive and in-depth evaluation of LVU. MLVU presents\nthe following critical values: 1) The substantial and flex-\nible extension of video lengths, which enables the bench-\nmark to evaluate LVU performance across a wide range of\ndurations. 2) The inclusion of various video genres, e.g.,\nmovies, surveillance footage, egocentric videos, cartoons,\ngame videos, etc., which reflects the models’ LVU perfor-\nmances in different scenarios. 3) The development of di-\nversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs’ key abilities in long-video under-\nstanding. The empirical study with 23 latest MLLMs reveals\nsignificant room for improvement in today’s technique, as all\nexisting methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling\nlonger videos. Additionally, it suggests that factors such as\ncontext length, image-understanding ability, and the choice\nof LLM backbone can play critical roles in future advance-\nments. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive\nand in-depth analysis of MLLMs.\n1. Introduction\nLarge language models (LLMs) are growing into a general\nsolution for numerous AI tasks [6, 45]. In recent years,\nit becomes increasingly emphasized to extend LLMs with\nmulti-modal capabilities and thus bring the Multi-modal\n*Co-first authors\n†Corresponding author\nLLM, namely, MLLM. Remarkably, it has been made pos-\nsible for today’s MLLMs to perceive information in texts,\nimages, videos, etc., and solve complicated problems in\nphysical environments [1, 44]. Along with the development\nof MLLMs, new benchmarks are continuously created to\nfacilitate comprehensive and in-depth analysis of MLLMs\n[12, 26, 32, 57].\nHowever, it remains a great challenge to evaluate the\nMLLMs’ long-video understanding (LVU) performances\ngiven the following limitations. Firstly, the majority of exist-\ning video understanding benchmarks are made up of short\nvideos [19, 22, 26, 36, 52], whose lengths can be merely a\nfew seconds. As a result, they are insufficient to reflect the\nMLLMs’ long-video understanding capabilities. Secondly,\nthere is a notable lack of diversity in both video genres and\nevaluation tasks. Existing benchmarks often concentrate on\na single video type, such as egocentric videos [15, 34], or\nfocus on one specific task, like captioning [52]. These limi-\ntations hinder comprehensive evaluation of LVU capabilities.\nLast but not least, many previous evaluation tasks are not\nproperly designed for LVU, as they can be solved without\nusing the complex information from long videos. For ex-\nample, many questions are simply about one single frame\nin the long videos [41, 60]. Besides, numerous others are\nabout popular movies and celebrities [13, 27], which can be\nanswered directly by MLLMs based on the textual prompts.\nConceptually, MLLMs are expected to handle any type of\nlong video and accomplish any related tasks. Therefore, the\nevaluation of LVU should emphasize two important prop-\nerties: length and diversity. Furthermore, it is crucial that\nthe evaluation tasks are specifically designed to leverage the\ncomplex information inherent in long videos, addressing\nthe shortcomings of previous benchmarks. Based on these\nprinciples, we propose a novel benchmark called MLVU\n(Mult-task Long Video Understanding Benchmark), which\npresents the following critical advantages.\n• It makes a substantial extension for the video length.\nMLVU is created based on long videos of diversified\nlengths, ranging from 3 minutes to 2 hours. The average\n1\narXiv:2406.04264v3  [cs.CV]  1 Jan 2025\nBenchmarks\n#Videos\n#QA\nPairs\nLen. (s)\nClose-\nEnded\nOpen-\nEnded\nVarious\nGenres\nMulti-\nLevel\nMulti-\nDimension\nReferring\nQA\nNExT-QA [50]\n1,000\n8,564\n39.5\n✓\n✓\n✓\n✗\n✗\n✗\nTVQA [21]\n15,253\n15,253\n11.2\n✓\n✗\n✗\n✗\n✗\n✗\nMSRVTT-QA [52]\n2,900\n72,821\n15.2\n✓\n✗\n✗\n✗\n✗\n✗\nMVBench [26]\n3,641\n4,000\n16.0\n✓\n✗\n✓\n✗\n✗\n✗\nMovie101 [58]\n101\n-\n6144\n✗\n✓\n✗\n✗\n✗\n✗\nEgoSchema [34]\n5,063\n5,063\n180\n✓\n✗\n✗\n✗\n✗\n✗\nMovieChat-1K [41]\n130\n1,950\n500\n✓\n✓\n✗\n✗\n✓\n✗\nVideo-MME∗[13]\n900\n2,700\n1024\n✓\n✗\n✓\n✓\n✗\n✗\nLongVideoBench∗[49]\n3,763\n6,678\n473\n✓\n✗\n✓\n✓\n✗\n✓\nMLVU\n1,730\n3,102\n930\n✓\n✓\n✓\n✓\n✓\n✓\nTable 1. Comparison of MLVU with existing benchmarks, including the number of videos (#Videos), number of QA pairs (#QA pairs),\naverage video length (Len.), presence of Close-Ended tasks, presence of Open-Ended tasks, inclusion of various video genres (Various\nGenres), coverage of multiple duration levels (Multi-Level), inclusion of multiple dimensions of LVU tasks (Multi-Dimension), and\nquestions involving local information with clear referring context rather than direct timestamps [41] or well-known narrative elements [17, 27]\n(Referring QA). The first block represents short video understanding benchmarks, and the second block represents long video understanding\nbenchmarks. ∗denotes work concurrent with MLVU.\nvideo length is about 15 minutes, which makes it much\nlonger than most of the existing benchmarks. Additionally,\neach video is further segmented so that evaluation tasks\ncan be created w.r.t. different video clips (e.g., summa-\nrization for the first 3 minutes, the first 6 minutes, and the\nentire duration of the video). Therefore, it is able to flex-\nibly evaluate the MLLMs’ performance across different\nvideo lengths.\n• It encompasses a wide variety of video genres. MLVU\nincludes diverse real-world videos, such as movies, life\nrecords, and egocentric videos. Additionally, it features\ntypical simulated videos like games and cartoons. This di-\nversity allows for a comprehensive assessment of MLLMs’\nperformance across various application scenarios.\n• It introduces diversified evaluation tasks tailored for\nLVU. MLVU comprises 9 distinct tasks that collectively\nassess a wide range of MLLMs’ LVU capabilities. On\none hand, it includes both multiple-choice and open-ended\ngeneration tasks, reflecting the models’ performance in\nhandling different task formats. On the other hand, some\ntasks are designed to leverage global information from\nentire videos, while others require the use of specific local\ninformation from certain clips. Moreover, all questions in-\nvolving local information are annotated with unambiguous\ncontext, requiring MLLMs to accurately locate or infer the\nappropriate clips within long videos.\nTable 1 shows that MLVU provides a more comprehen-\nsive evaluation of LVU compared to existing and concurrent\nbenchmarks. We extensively investigate 23 popular MLLMs\nwith MLVU, which brings in several critical insights. Firstly,\nlong-video understanding remains a technically challenging\nproblem for the existing MLLMs. While GPT-4o1 achieves\nthe leading performance in the experiment, it only attains\n1https:\/\/openai.com\/index\/hello-gpt-4o\/\nan average score of 54.5% in multi-choice tasks. All meth-\nods struggle with tasks requiring fine-grained information\nfrom entire videos, such as action counting, ordering, and\nsummarization. Secondly, recent open-source long video\nMLLMs have made significant strides in LVU [11, 40, 60].\nThese advancements have improved the models’ capability to\nprocess extended visual sequences, thereby closing the gap\nwith leading proprietary models in recent months. Finally,\nthe empirical results underscore influential factors in LVU,\nsuch as the extension of context length, the improvement\nof image understanding ability, and the utilization of strong\nLLM-backbones. In addition to the benchmark’s overall\nconclusion, individual tasks enable fine-grained analysis of\nMLLMs’ performances in each specialized aspects. There-\nfore, we anticipate the benchmark to assist in improving\nMLLMs’ long-video understanding capabilities by provid-\ning insights into their current strengths and weaknesses.\n2. Related Work\nMultimodal Large Language Models.\nMultimodal large\nlanguage models (MLLMs) have attracted significant interest\nfrom both academia and industry. Recent advancements in\nthis field have been achieved by integrating LLM backbones\nwith visual encoders and adapters, and fine-tuning the en-\ntire architecture through visual instruction tuning [8, 29, 63].\nBased on the same philosophy, MLLMs have been further de-\nveloped for video processing using video instruction datasets\nand specialized video adapters [25, 26, 28, 33, 54, 59]. How-\never, most existing models are optimized for short videos,\ntypically under one minute, due to the difficulty in estab-\nlishing sufficient context for longer videos. To address this\nchallenge, researchers have explored compact video repre-\nsentations or extended the context length of MLLMs. For\ninstance, LLaMa-Vid [27] compresses each video frame\n2\nAction\nRomance\nScience Fiction\nSuspense\nWar\nComedy\nAction\nFamily\nFantasy\nAdventure\nGeography\nAnimal\nScience\nCooking\nHandcraft\nSoftware\nBasketball\nFootball\nPingpong\nVolleyball\nBadminton\nTravel\nPet\nFood\nDaily Life\nOutdoor\nIndoor\nIndoor\nOutdoor\nCartoon\nGame\nMovie\nTV Series\nDocumentary\nTutorial\nSport\nLife Record\nEgo-centric\nSurveillance\nSimulation\n0\n100\n200\n300\n400\n500\n(3, 5]\n(5, 8]\n(8, 11]\n(11, 15]\n(15, 30]\n(30, 120]\n(120, )\nNumber of Videos\nVideo Duration (min)\nPlot QA\n589\nEgo Reasoning\n405\nNeedle QA\n415\nSub-Scene\nCaptioning\n247\nAnomaly Recognition\n239\nTopic Reasoning\n355\nVideo\nSummarization\n257\nAction \nCount\n266\nAction\nOrder\n329\nMulti-Detail Task\nOpen-ended Task\nClose-ended Task\nHolistic Task \nSingle-Detail Task\nFigure 1. Statistical Overview of our MLVU benchmark. Left: Video genres included in MLVU; Top Right: Distribution of video duration;\nBottom Right: Task types and their counts in MLVU.\ninto two tokens, enabling the model to handle videos sev-\neral hours long. Methods like MovieChat [41] and MA-\nLMM [16] introduce specialized memory components for\nrecursive video processing. Furthermore, approaches such\nas LWM [30], LongVA [60], and Video-XL [40] are de-\nsigned to extend the context length of MLLMs, facilitating\nthe processing of longer video inputs. Additionally, it is\nalso explored to make selective usage of frames or clips\nfrom long videos based on retrievers or agents [38, 46, 53].\nDespite these progresses, it remains an open problem for\nMLLMs to effectively handle long videos.\nVideo Understanding Benchmarks.\nWith the unprece-\ndented interest in MLLMs, the creation of benchmarks for\nthese models has become increasingly emphasized (as ad-\nvanced by MMMU [57], MME [12], and many other pio-\nneering works). In video understanding, the research com-\nmunity has made significant efforts as well, particularly for\nshort videos. There are specialized benchmarks for temporal\nperception [48, 56], action understanding [47, 48], video\nclassification [18], video reasoning [50, 51], and video cap-\ntioning [35, 52]. Recently, MVBench [26] provides a com-\nprehensive short-video benchmark to evaluate general capa-\nbilities via question-answering. For long video understand-\ning, people seek to leverage long-form videos, like movies,\nto create benchmarks.\nFor example, LLaMA-Vid [27]\ndeveloped a movie question-answering dataset based on\nMovieNet [17]. Despite using long videos, many questions\nfocus on well-known narrative elements, allowing them to be\nanswered without analyzing the video’s content. In contrast,\nMovieChat [41] avoids specific character names or plot de-\ntails in its questions. However, since each question provides\na specific timestamp, the tasks can be reduced to short-video\nor image understanding problems. Beyond movies, there\nare task-specific benchmarks like EgoSchema [34], which\npresents video reasoning tasks using first-person footage\nfrom Ego4D [15]. These specialized benchmarks, however,\nfocus on a single aspect of MLLMs rather than offering a\ncomprehensive analysis of long video understanding. There-\nfore, it is essential to develop a comprehensive benchmark\nwith carefully designed tasks to effectively evaluate MLLMs’\ncapabilities in understanding long videos.\n3. MLVU: Multi-task Long Video Understand-\ning Benchmark\nIn this section, we start with an overview of MLVU, which\nhighlights its constitution and explains its values over the\nprevious works. Then, we discuss how each evaluation task\nis constructed in MLVU.\n3.1. Overview\nMLVU is a multi-task benchmark consisting of 3,102 ques-\ntions across 9 categories, specifically designed for long video\nunderstanding. It is divided into a dev set and a test set,\ncontaining 2,593 and 509 questions, respectively. The bench-\nmark is distinguished by the following features.\nDiversified Video Categories. MLVU offers a compre-\nhensive collection of videos across various categories (Fig-\nure 1 Left). These include typical real-world videos such as\nmovies, documentaries, TV series, egocentric videos, life\nrecords, sports, tutorials, and surveillance footage. Addition-\nally, it features significant simulated videos from animated\nseries and game videos.\nSubstantial Extension of Video Length. MLVU is made\n3\nup of videos of diversified lengths, spanning from 3 min to\nmore than 2 hours (Figure 1 Top Right). Besides, each video\nis further partitioned as incremental segments, e.g., the first\n3 min, the first 6 min, and the entire video, where tasks are\ncreated for each individual segment. Thus, the MLLMs can\nbe flexibly evaluated across different video lengths.\nDiversified Evaluation Tasks. MLVU also provides a\ndiverse array of evaluation tasks, which are closely aligned\nwith the common visual capabilities of MLLMs, such as\nreasoning, captioning, recognition, perception, and summa-\nrization (Figure 1 Bottom Right). All the tasks are tailored\nfor LVU. That is to say, the tasks need to be solved based\non the in-depth understanding of video. Some of tasks are\nto examine whether the global information from the entire\nvideo can be effectively utilized (holistic LVU); while oth-\ners focus on whether the MLLMs can make precise usage\nof proper local information within the long video (detail\nLVU). Additionally, both multi-choice and free-form gener-\nation tasks are included in MLVU, which help to examine\nMLLMs’ capabilities in handling different task formats.\n3.2. Construction of MLVU\nThe evaluation tasks of MLVU can be categorized into three\ntypes: 1) holistic LVU, which needs to be solved by mak-\ning use of the global information from the entire video; 2)\nsingle-detail LVU, which relies on leveraging one critical\nplot within the long video; and 3) multi-detail LVU, which\nnecessitates the joint utilization of multiple plots within the\nlong video. The construction process of MLVU is discussed\nw.r.t the above three categories. To facilitate the discussion,\nwe define ULVC (Universal Long Video Collection) as the\nuniversal collection of long videos from various sources\n(more details about ULVC are presented in Appendix C).\n3.2.1. Holistic LVU\nTopic Reasoning (TR). The topic reasoning task requires\nMLLMs to respond to questions about the principal subject\nof a long video, as shown with Figure 2 (a). This includes\nelements such as the video’s genre, pivotal events, or pri-\nmary settings. All questions and answers undergo manual\nannotation2, resulting in a total of 355 questions. TR tasks\nare formatted as multiple-choice questions, with the model’s\nperformance assessed based on accuracy.\nAnomaly Recognition (AR). The anomaly recognition task\ninvolves identifying the anomalous behavior within a surveil-\nlance footage (Figure 2 b). We leverage the surveillance\nvideo clips from UCF Crime dataset [43] for this task. The\nselected video clips are longer than three minutes. We create\n239 questions based on the original annotations provided by\nthe dataset. The AR task is also conducted in the multiple-\nchoice format, whose performance is measured by accuracy.\n2Detailed information and annotation guidelines for annotators are pre-\nsented in Appendix F.\nVideo Summarization (VS). This task requires MLLMs\nto summarize the key events in a long video (Figure 2 c).\nWe select the narrative-rich videos from ULVC for this task,\nincluding movies, TV series, documentaries, life records,\nand animated series. There are 257 selected videos in total,\nwhose summaries are manually annotated. During evalua-\ntion, the MLLMs are prompted with \"Please summarize the\nmain content of this video\". We employ GPT-4 to assess the\ngenerated summaries by comparing with the annotation re-\nsults. Details about annotation and evaluation are presented\nin Appendix F.3 and G.3.\n3.2.2. Single-Detail LVU\nNeedle\nQuestion-Answering\n(NQA).\nNeedle-In-the-\nHaystack-Search (NIHS) is a popular evaluation task for\nlong-context LLM [31]. Taking the inspiration from NIHS,\nwe create Needle Question-Answering (NQA), shown as\nFigure 2 (d). In this task, the MLLM is required to answer a\nquestion related to a specific segment (referred as needle)\nwithin a long video (referred as background video). The\nneedles are short video clips sampled from WebVid [5]\nand Clevrer [55], while the background videos are sampled\nfrom our ULVC. The needle is randomly inserted into\nthe background video, where a question-answer pair is\nannotated. By incorporating necessary details, the question\ncan always correspond to the needle without ambiguity.\nDuring evaluation, the MLLM needs to infer the location\nof the needle based on the details provided in the question,\nand solve the problem on top of the needle’s information.\nThe NQA task is structured as multiple-choice, whose\nperformance is measured by accuracy.\nEgo Reasoning (ER). Ego-centric videos capture a series\nof consecutive actions from a first-person perspective. The\nMLLM needs to reason for a question about a specific be-\nhavior in the video, e.g., predicting for the event which is\ncorrelated or satisfies a certain causal relationship with the\nbehavior (Figure 2 e). Both videos and QA annotations are\ncollected from the NLQ task of Ego4D [15]. The ER task is\nstructured as multiple-choice, with a total of 405 questions\ncreated for this task.\nPlot Question-Answering (PQA). In this task, the MLLM\nneeds to reason for questions about a plot in a narrative\nvideo, shown as Figure 2 (f). The video is sampled from the\nmovies, TV series, and animated series in our ULVC. There\nare 589 question-answer pairs created by manual annotation.\nDuring annotation, the human annotators are asked to only\nprovide necessary details about the plot but not to suggest\nany objective hints, e.g., the two characters in the example\nvideo are referred as cat and mouse, rather than Tom and\nJerry. Therefore, it can prevent the question from being\nshort-cut by the MLLM’s common-sense knowledge (more\ndetails about PQA can be found in the Appendix F.6).\nSub-Scene Captioning (SSC). In this task, the MLLM needs\nto generate the caption for a sub-scene in a long video. The\n4\nFigure 2. Examples of MLVU. There are nine tasks designed to evaluate the holistic, single-detail, and multi-detail LVU capabilities of\nMLLMs. The MLLMs are asked to solve the problem (with the ground-truth answers marked in blue) based on the long video input and\ntextual prompt. For multiple-choice questions, we set 4 candidates in the dev set and 6 candidates in the test set.\n5\nlong videos in SSC are sampled from the Movie101 dataset\n[58], while the questions and answers are manually anno-\ntated. During annotation, the human annotator is asked\nto provide a detailed description for the sub-scene as the\nground-truth answer. Besides, they need to offer necessary\nclues in their questions such that the referred sub-scenes\ncan be identified without ambiguity. During evaluation, we\nemploy GPT-4 [1] to measure the quality of caption in com-\nparison with the ground-truth. Details about annotation and\nevaluation are presented in Appendix F.7 and G.3.\n3.2.3. Multi-Detail LVU\nAction Order (AO). In this task, the MLLM needs to predict\nthe right order for a sequence of actions (Figure 2 h). The\nactions are presented by short video clips, called probes.\nThe probes are formulated in two different ways. One is\nmade up of clips from the Kinetics dataset [18], where each\nclip represents a distinct action. The other one is from the\nconsecutive clips of an action in the ActivityNet-Caption\ndataset [20]. The probes are inserted into a long background\nvideo, which is sampled from ULVC. There are 329 AO\nquestions in total. The task is structured as a multiple-choice\nprblem, where the right order is selected from the misleading\noptions provided by the annotator.\nAction Count (AC). This task requires the MLLM to count\nthe occurrences of an action within a long video (Figure 2\ni). Each action corresponds to multiple short probe clips\nsampled from the Kinetics dataset [18]. The probes of an ac-\ntion are inserted into a long background video sampled from\nULVC. We also perform manual examination to ensure that\nthe inserted action does not exist in the original background\nvideo. A total of 266 evaluation instances have been created.\nThe AC task is structured as a multiple-choice problem, with\nperformance measured by accuracy.\n4. Experiments and Analysis\n4.1. Settings\nWe conduct a comprehensive investigation of 23 MLLMs us-\ning our MLVU benchmark, encompassing both open-source\nand proprietary models. The experimental MLLMs are di-\nvided into three categories: 1) Image MLLMs, primarily\nfine-tuned using image-related instructions; 2) Short Video\nMLLMs, fine-tuned with short-video related instructions;\nand 3) Long Video MLLMs, optimized for long-video under-\nstanding capability. For Image MLLMs, we leverage their\nmulti-image inference capabilities to process segmented\nframes from original videos. For Video MLLMs, we employ\neither a uniform sampling strategy or a frame rate sampling\nstrategy for video processing. All models are evaluated\nbased on their official implementations or available APIs,\nwith evaluations conducted in a zero-shot manner. More\ndetails about the evaluation are provided in Appendix G.\n4.2. Main Results\nThe overall evaluation results for all investigated MLLMs\nin the MLVU test set are shown in Table 2 (with dev set re-\nsults in Appendix B). Individual performances are reported\nfor each task, while average performances are provided\nfor multiple-choice (M-Avg) and generation tasks (G-Avg).\nFrom the results, we derive three primary conclusions:\n1) The proprietary model GPT-4o [37] achieves opti-\nmal performance in our benchmark. It leads in multiple-\nchoice tasks with an M-Avg of 54.5%(within 0-100%) and\nexcels in generation tasks with a G-Avg of 5.87 (within\n0.0-10.0), outperforming all other methods.\n2) Recent advances in LVU have achieved significant\nprogress, and the gap between open-source long video\nMLLMs and GPT-4o on close-ended tasks is narrowing.\nBefore June 2024, the best open-source long video MLLMs,\nMiniGPT4-Video [3], lagged significantly behind GPT-4o.\nHowever, recent models [11, 24, 40, 60] have made substan-\ntial progress. For instance, LLaVA-Onevision trails GPT-4o\nby only 2.8% in M-Avg. These models have improved their\nability to handle long visual sequences, achieving significant\nadvancements in single-detail (e.g., NQA) and multi-detail\n(e.g., AC) tasks compared to previous open-source models.\n3) Existing methods still struggle to handle most tasks\nin our benchmark. For instance, GPT-4o only achieves\n42.9% in the needle question-answering (NQA) task. In\ncontrast, analogous tasks in the text domain, such as NIHS\n(Needle-In-the-Haystack-Search) and Passkey Retrieval, are\neffectively handled by many existing long LLMs [14, 61].\nAdditionally, GPT-4o shows even less reliability in tasks like\nego-reasoning (ER), action ordering (AO), and action count\n(AC), with most baseline methods performing even worse.\nThese observations indicate that long-video understanding\nremains a significant challenge for today’s MLLMs.\nIn addition to the primary conclusions from the overall\nperformances, we can also make the following interesting\nobservations about the individual tasks.\n4) The close-ended holistic tasks present much higher\ndifferentiation than other tasks. These tasks, i.e., topic\nreasoning (TR) and anomaly recognition (AR), show sig-\nnificant variance in performance across different models.\nProprietary MLLMs, like GPT-4o, and superior open-source\nmodels, such as InternVL-2 [8], VideoLLaMA2 [9], and\nLLaVA-OneVision [24], can accurately solve these prob-\nlems. Meanwhile, many other popular MLLMs still fail to\ngenerate meaningful performances. Since these tasks only\nrequire an overall understanding of long videos, they can\nserve as a preliminary indicator of MLLMs’ long video un-\nderstanding (LVU) ability.\n5) It’s challenging to deal with tasks that require nu-\nanced understanding of multiple details. Although several\nMLLMs can handle single-detail LVU tasks to some ex-\ntent, their performances suffer from catastrophic degradation\n6\nMethods\nDate\nInput\nHolistic\nSingle Detail\nMulti Detail\nM-Avg\nG-Avg\nTR\nAR\nVS∗\nNQA\nER\nPQA\nSSC∗\nAO\nAC\nFull mark\n–\n–\n100\n100\n10\n100\n100\n100\n10\n100\n100\n100\n10\nRandom\n–\n–\n16.7\n16.7\n–\n16.7\n16.7\n16.7\n–\n16.7\n16.7\n16.7\n–\nImage MLLMs\nOtter-I [23]\n2023-05\n16 frm\n17.6\n17.9\n2.03\n16.7\n17.0\n18.0\n3.90\n15.7\n16.7\n17.1\n2.97\nLLaVA-1.6 [29]\n2024-01\n16 frm\n63.7\n17.9\n2.00\n13.3\n26.4\n30.0\n4.20\n21.4\n16.7\n27.1\n3.10\nInternVL-2 [8]\n2024-07\n16 frm\n85.7\n51.3\n2.55\n48.3\n47.2\n52.0\n5.25\n32.9\n15.0\n47.5\n3.90\nClaude-3-Opus† [2]\n2024-03\n16 frm\n53.8\n30.8\n2.83\n14.0\n17.0\n20.0\n3.67\n10.0\n6.7\n21.8\n3.25\nQwen-VL-Max† [4]\n2024-01\n16 frm\n75.8\n53.8\n3.00\n15.0\n26.4\n4.84\n20.0\n20.7\n11.7\n32.2\n3.92\nShort Video MLLMs\nOtter-V [23]\n2023-05\n16 frm\n16.5\n12.8\n2.18\n16.7\n22.6\n22.0\n4.20\n12.9\n13.3\n16.7\n3.19\nmPLUG-Owl-V [54]\n2023-04\n16 frm\n25.3\n15.4\n2.20\n6.7\n13.2\n22.0\n5.01\n14.3\n20.0\n16.7\n3.61\nVideoChat [25]\n2023-05\n16 frm\n26.4\n12.8\n2.15\n18.3\n17.0\n22.0\n4.90\n15.7\n11.7\n17.7\n3.53\nVideo-LLaMA-2 [59]\n2024-08\n16 frm\n52.7\n12.8\n2.23\n13.3\n17.0\n12.0\n4.87\n15.7\n8.3\n18.8\n3.55\nVideoChat2-HD [26]\n2024-06\n16 frm\n74.7\n43.6\n2.83\n35.0\n34.0\n30.0\n5.14\n21.4\n23.3\n37.4\n3.99\nVideo-LLaVA [28]\n2023-11\n8 frm\n70.3\n38.5\n20.9\n2.30\n26.4\n26.0\n5.06\n20.0\n21.7\n29.3\n3.68\nShareGPT4Video [7]\n2024-05\n16 frm\n73.6\n25.6\n2.53\n31.7\n45.3\n38.0\n4.72\n17.1\n8.3\n34.2\n3.63\nVideoLLaMA2 [9]\n2024-06\n16 frm\n80.2\n53.8\n2.80\n36.7\n54.7\n54.0\n5.09\n42.9\n16.7\n48.4\n3.95\nLong Video MLLMs\nMovieChat [41]\n2023-07\n2048 frm\n18.7\n10.3\n2.30\n23.3\n15.1\n16.0\n3.24\n17.1\n15.0\n16.5\n2.77\nMovie-LLM [42]\n2024-03\n1 fps\n27.5\n25.6\n2.10\n10.0\n11.3\n16.0\n4.93\n20.0\n21.7\n18.9\n3.52\nLLaMA-VID [27]\n2023-11\n1 fps\n20.9\n23.1\n2.70\n21.7\n11.3\n16.0\n4.15\n18.6\n15.0\n18.1\n3.43\nMA-LMM [16]\n2024-04\n1000 frm\n44.0\n23.1\n3.04\n13.3\n30.2\n14.0\n4.61\n18.6\n13.3\n22.4\n3.83\nMiniGPT4-Video [3]\n2024-04\n90 frm\n64.9\n46.2\n2.50\n20.0\n30.2\n30.0\n4.27\n15.7\n15.0\n31.7\n3.39\nLongVA [60]\n2024-06\n256 frm\n81.3\n41.0\n2.90\n46.7\n39.6\n46.0\n4.92\n17.1\n23.3\n42.1\n3.91\nVideo-CCAM [11]\n2024-08\n96 frm\n79.1\n38.5\n2.65\n45.0\n52.8\n56.0\n4.49\n24.3\n26.7\n46.1\n3.57\nVideo-XL [40]\n2024-09\n256 frm\n78.0\n28.2\n3.40\n50.0\n41.5\n46.0\n5.02\n48.6\n31.7\n46.3\n4.21\nLLaVA-Onevision [24]\n2024-08\n32 frm\n83.5\n56.4\n3.75\n46.7\n58.4\n58.0\n5.09\n35.7\n23.3\n51.7\n4.42\nGPT-4o† [37]\n2024-05\n0.5 fps\n83.7\n68.8\n4.94\n42.9\n47.8\n57.1\n6.80\n46.2\n35.0\n54.5\n5.87\nTable 2. The overall performances on MLVU test set, including the holistic LVU tasks, the single-detail LVU tasks, and multi-detail LVU\ntasks. Date: the release date of the MLLM. M-Avg: the average performance of multiple-choice tasks; G-Avg: the average performance of\ngeneration tasks (marked by ∗). Two input strategies are used by the MLLMs in evaluation: Uniform Sampling (N frm), which evenly\nsamples N frames from the video; Frame Rate Sampling (N fps), which samples N frames per second. † denotes proprietary models.\nwhen addressing multi-detail LVU tasks. Most methods,\nexcept for GPT-4o and Video-XL [40], fail entirely in ac-\ntion order (AO) and action count (AC) tasks. Additionally,\nmost approaches struggle with summarization tasks, which\nrequire recalling multiple nuanced details from long videos.\nAs a brief conclusion, although today’s MLLMs can deal\nwith some preliminary LVU tasks, it remains a tough chal-\nlenge to achieve an in-depth understanding of nuanced infor-\nmation within long videos.\n4.3. Further Analysis\n6) Longer videos are more challenging for MLLMs.\nWe evaluate MLLMs’ performances across various video\nlengths. For this purpose, we introduce a derivative dataset\nalongside MLVU, called MLVU Time-ladder. In this dataset,\nthe same kinds of evaluation tasks are created for videos of\nvariant lengths, including 180s, 360s, and 600s (more details\npresented in Appendix D). As shown in Figure 3, the per-\nformances of all models tend to decline as the video length\ngrows, which indicates that the existing MLLMs’ LVU abili-\nties are severely constrained by the video length. Moreover,\nthe short video model Video-LLaMA-2 [59] maintains a cer-\ntain level of LVU ability at 3 minutes, but its performance\napproaches random results at 10 minutes.\n7) The performance of recent advanced long video\nMLLMs remains robust regardless of the position of the\nreferring clip within the long video. In single-detail tasks,\nthe referring clip denotes the specific segment of the long\nvideo that is referenced or inferred to answer a question.\nAs shown in Figure 4, we categorize clip positions into\nfour intervals and assess model performance on two single-\ndetail tasks: ego reasoning (ER) and plot question-answering\n(PQA). Recent long video MLLMs, such as LongVA [60]\nand Video-XL [40], maintain consistent performance re-\n7\nImpact of Context Length\nImpact of IU\nImpact of LLM\nModel\nContext Len. M-Avg\nModel\nMMMU (Val) M-Avg\nModel\nLLM\nM-Avg\nMGV\n16\n24.2\nOtter-I\n32.2\n17.1\nVLM2 Vicuna-7B 13.3\n90\n31.7↑7.5 LLaVA-1.6\n35.8\n27.1↑10.0\nVicuna-13B 18.8↑5.5\nGPT-4o\n16\n45.8\nGPT-4V\n58.1\n43.3\nMGV\nLLaMA-7B 20.6\n256\n54.5↑8.7 GPT-4o\n63.8\n45.8↑2.5\nMistral-7B 31.7 ↑11.1\nTable 3. Detailed discussions about the impact from context length, image understanding (IU) ability, and LLM Backbone. For the IU\nimpact experiment, we used 16-frame uniform sampling for both GPT-4V and GPT-4o. MGV: MiniGPT4-Video, VLM2: Video-LLaMA-2.\n180s\n360s\n600s\n20\n30\n40\n50\nAccuracy (%)\nVideo-XL\nVideoChat2\nMiniGPT4-Video\nVideo-LLAMA-2\nFigure 3. Experimental performance on varying video lengths. The\nevaluated metric is the average accuracy across five multiple-choice\ntasks involving local information: NQA, ER, PQA, AC, and AO.\n20\n28\n36\n44\nLongVA\nVideoXL\nVideoChat2 VideoLLaVA\n20\n30\n40\n50\nLongVA\nVideoXL\nVideoChat2 VideoLLaVA\nEgo Reasoning\nPlot QA\n(0, 25] \n(25, 50] \n(50, 75] \n(75, 100] \nVideoXL\nLongVA\nVideoChat2 VideoLLaVA\nVideoXL\nLongVA\nVideoChat2 VideoLLaVA\n44\n36\n28\n20\n50\n40\n30\n20\nFigure 4. Model performance across different referring clip posi-\ntions, spanning from the beginning to the end of the entire video.\ngardless of the referring clip’s position within the video.\nConversely, short video MLLMs are more sensitive to clip\nlocation. This indicates that recent advancements in long\nvideo MLLMs enhance both reliable clue retrieval and effec-\ntive reasoning from extended visual sequences.\n8) The challenge of multi-detail tasks increases with\nthe number of details. We analyzed model performance\non the action count (AC) task by grouping questions based\non the number of probes (which correspond to details) and\nevaluating the average performance within these groups. As\nshown in Figure 5, performance significantly declines across\nall models as the number of probes increases. This indi-\ncates that current MLLMs face substantial difficulties com-\nprehending and processing multiple details simultaneously,\nhighlighting a critical area for future improvement in long\nvideo understanding capabilities.\n9) Context Length, Image-Understanding ability, and\n[1,2]\n[3,4]\n[5,6]\nNumber of Probes\n20\n25\n30\n35\n40\nAccuracy (%)\nGPT-4o\nVideo-XL\nLLaVA-Onevision\nVideoChat2\nLongVA\nFigure 5. Model performance on the action count (AC) task in\nrelation to the number of probes.\nthe choice of LLM Backbones are key factors in LVU\nperformance. As shown in Table 3, we conducted abla-\ntion experiments on several factors affecting MLLMs, using\nM-Avg as the evaluation metric. First, we examined the\nmodels’ handling of different context lengths. Specifically,\nwe increased MiniGPT4-Video’s input from 16 to 90 frames\nand GPT-4o’s input from 16 to 256 frames (as shown on the\nleft side of Table 3). Both models showed consistent perfor-\nmance improvements with longer input lengths. To assess the\nimpact of image understanding (IU) capabilities, we referred\nto the results from MMMU [57] (presented in the middle of\nTable 3). It is evident that MLLMs’ LVU performance gener-\nally aligns with their IU performance in MMMU. Finally, we\ncompared MLLMs using different backbones (depicted on\nthe right side of Table 3). The findings indicate that LVU per-\nformance improves with larger (Vicuna-13B vs. Vicuna-7B)\nand more advanced backbones (Mistral-7B vs. Llama-2-\n7B). These observations indicate that LVU is the result of\nmultiple complex factors, with the ability to perceive longer\nvideos and effectively utilize the perceived information being\ncrucial for the improvement of LVU.\n5. Conclusion\nThis paper presents MLVU, a novel benchmark for the\nassessment of long video understanding.\nWith several\ncritical innovations: the substantial extension of video\nlengths, the inclusion of various video genres, and the\ndevelopment of diversified LVU-oriented evaluation tasks,\nthe new benchmark is able provide a comprehensive and\nin-depth analysis for MLLMs’ long-video understanding\nperformance. The empirical study on MLVU reveals LVU\n8\nremains a technically challenging problem for today’s\nstate-of-the-art MLLMs. Future advancements may call\nfor the joint optimization of complex factors, such as\ncontext length, image understanding ability, and even LLM\nbackbones. We anticipate this benchmark will facilitate\nfuture research in long-video understanding of MLLMs.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | MLVU：多任务长视频理解基准\n\n## 📌 背景痛点\/本文动机\n随着大型语言模型（LLMs）在多模态能力上的扩展，多模态大型语言模型（MLLMs）在处理文本、图像和视频等多模态信息方面取得了显著进展。然而，现有的视频理解基准主要针对短视频，无法有效评估MLLMs在处理长视频方面的能力。此外，现有基准在视频类型和评估任务的多样性方面存在不足，且评估任务的设计往往无法充分利用长视频中的复杂信息。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1：视频长度的大幅扩展\nMLVU基准包含时长从3分钟到2小时不等的多样化长视频，平均视频长度约为15分钟，远超现有基准。此外，每个视频被进一步分割成多个片段，以便针对不同片段创建评估任务，从而灵活评估MLLMs在不同视频长度下的表现。\n\n💡 创新点2：多样化的视频类型\nMLVU基准涵盖了多种视频类型，包括电影、生活记录、第一人称视角视频、体育、教程、监控录像等真实世界视频，以及动画系列和游戏视频等模拟视频。这种多样性使得MLLMs在不同应用场景下的表现可以得到全面评估。\n\n💡 创新点3：多样化的评估任务\nMLVU基准包含9个不同的评估任务，旨在全面评估MLLMs在长视频理解方面的关键能力。这些任务包括多项选择题和开放式生成任务，以及需要利用整个视频的全局信息或特定片段的局部信息的任务。此外，所有涉及局部信息的任务都带有明确的上下文标注，要求MLLMs能够准确定位或推断长视频中的相关片段。\n\n## 📈 实验结果\n通过对23个最新的MLLMs进行实证研究，MLVU基准揭示了现有技术在长视频理解方面仍有很大的改进空间。所有现有方法在大多数评估任务上都表现不佳，并且在处理更长的视频时性能严重下降。此外，研究结果表明，上下文长度、图像理解能力和LLM骨干的选择等因素对未来长视频理解技术的进步起着关键作用。\n\n## 💬 可借鉴之处\nMLVU基准为长视频理解研究提供了一个全面而深入的评估框架，有助于研究人员了解MLLMs在长视频理解方面的优势和不足，并为未来的技术改进提供方向。此外，MLVU基准的设计理念和方法可以为其他多模态任务评估基准的开发提供参考。","llm_summary_res_status":200}
{"title":"A Survey on Large Language Model-Based Game Agents","authors":"Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu","summary":"The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence (AGI). The progress of LLMs and their\nmultimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve\nand empower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview of\nLLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essential\nfunctional components: perception, memory, thinking, role-playing, action, and\nlearning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation\nagility across six genres of games, including adventure, communication,\ncompetition, cooperation, simulation, and crafting & exploration games.\nFinally, we present an outlook of future research and development directions in\nthis burgeoning field. A curated list of relevant papers is maintained and made\naccessible at: https:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.","url":"http:\/\/arxiv.org\/abs\/2404.02039v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2404.02039v1","published":1712072058000,"comment":null,"pdf_text":"A Survey on Large Language Model-Based Game Agents\nSihao Hu†, Tiansheng Huang†, Fatih ˙Ilhan†, Selim Tekin†,\nGaowen Liu‡, Ramana Kompella‡, Ling Liu†\n†Georgia Institute of Technology,\n‡Cisco Research\nQ sihaohu@gatech.edu, ling.liu@gatech.cc.edu\nAbstract\nThe development of game agents holds a critical role in advancing towards Ar-\ntificial General Intelligence (AGI). The progress of LLMs and their multimodal\ncounterparts (MLLMs) offers an unprecedented opportunity to evolve and em-\npower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview\nof LLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essen-\ntial functional components: perception, memory, thinking, role-playing, action,\nand learning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation agility\nacross six genres of games, including adventure, communication, competition, co-\noperation, simulation, and crafting & exploration games. Finally, we present\nan outlook of future research and development directions in this burgeoning\nfield. A curated list of relevant papers is maintained and made accessible at:\nhttps:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n1\nIntroduction\nIntelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor\nactivity.\n— The Embodied Cognition Hypothesis [1]\nLarge language models (LLMs), exemplified by ChatGPT [2], represent an important milestone in\nnatural language understanding (NLU) and generative artificial intelligence (Gen-AI). Empowered by\ngenerative training over massive data of diverse web sources with hundreds of billions of parameters,\nLLMs demonstrate astonishing capabilities of generalizing knowledge from huge text corpus data\nand displaying conversational intelligence in natural language with human-level NLU performance.\nThe emergence of multimodal LLMs (MLLMs), such as GPT-4V [3] and Gemini [4], marks another\nmilestone, enabling LLMs to perceive and understand visual input. We conjecture that the success\nof LLM technologies fuels an unprecedented opportunity in the pursuit of human-like Artificial\nGeneral Intelligence (AGI): the cognitive capabilities previously thought to be exclusive to humans,\nsuch as reasoning, planning, and reflection, with a degree of self-control, self-understanding, and\nself-improving, are now achievable by integrating appropriately prompting of LLMs with built-in\ncognitive intelligence.\nWe define an LLM-based agent (LLMA) as an intelligent entity that employs LLMs1 as a core\ncomponent to conduct human-like decision-making process [5]. Even though LLMAs are capable\nof cognitive processing similar to human, a distinction between existing LLMAs and human-like\n1In this paper, LLMs refers to both large language models (LLMs) and multimodal large language models\n(MLLMs).\narXiv:2404.02039v1  [cs.AI]  2 Apr 2024\nAGI is evident: current LLMAs rely on decoding and generalizing pre-existing knowledge derived\nfrom pre-training data [6], while AGI is capable of discovering and learning new knowledge through\nexperimentation and experience in real world [7; 8]. Inspired by the process of intelligence devel-\nopment in human infants, the embodied cognition hypothesis [1] posits that the intelligence of an\nagent emerges from observing and interacting its environment, i.e., grounding the agent in a world\nthat integrates physical, social, and linguistic experiences is vital for fostering conditions conducive\nto the development of human-like intelligence.\nDigital games are recognized as ideal environments for cultivating AI agents due to their complexity,\ndiversity, controllability, safety and reproducibility. Games, ranging from classical chess and poker\ngames [9; 10; 11] to modern video games like Atari games [12], StarCraft II [13], Minecraft [14]\nand DOTA II [15], have been long instrumental in advancing AI research. Unlike traditional\nReinforcement Learning (RL)-based agents [10; 16; 17; 18] that make decisions with the goal\nof maximizing expected rewards through behavior-level policy learning, constructing LLM-based\ngame agents (LLMGAs) capable of employing cognitive abilities to gain fundamental insights into\ngameplay, potentially aligns more closely with the pursuit of AGI.\nPrevious survey papers on LLMs [19; 20; 21] or LLMAs [22; 23; 24] mainly focus on reviewing\nexisting LLMs developed in industry and academic research teams, as well as the general applications\nof LLMAs, paying less attention to the field of game agents. Concurrent survey papers [25; 26] place\na notable emphasis on the game development and cover a limited number of publications on LLMGAs.\nTo bridge this gap, this paper attempts to conduct a comprehensive and systematic survey on recent\ndevelopments in LLMGAs. Specifically, this survey is organized into three synergistic parts: First, we\nprovide a unified reference framework, in which we describe the essential modules for constructing\nLLMGAs, covering six core functional components: perception, memory, thinking, role-playing,\naction and learning. Second, we introduce a taxonomy that categorizes existing literature into six game\ncategories, including adventure, competition, cooperation, simulation, and crafting & exploration.\nFor each category, we describe the technical challenges, the supporting game environments, as well as\nthe commonly used optimization strategies. In the third and final part, we envision different directions\nof future advancement of LLMGAs.\nIn summary, this survey paper serves as a comprehensive review of the literature on LLMGAs,\noffering a taxonomy of six game categories to enhance understanding and facilitate the development\nand assessment of various LLMGAs. It aims to catalyze progress within this nascent research\narea and to inspire further innovation in research and development of LLMGAs. Given that this\nis a new and burgeoning research field, this survey paper will be continuously updated to keep\ntrack of the latest studies. A curated list of relevant literature is maintained and accessible at\nhttps:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n2\nA Unified Architecture for LLMGAs\nFigure 1 provides an conceptual architecture of LLMGAs that consist of the six essential functional\ncomponents and their workflow: For each game step, the perception module captures game state\ninformation, providing the necessary data for the agent to understand its current environment. The\nthinking module processes the perceived information, generating thoughts based on reasoning,\nplanning, and reflection for informed decision-making. Memory serves as an external storage, where\npast experiences, knowledge and curated skills are retained and can be retrieved for future use.\nThe role-playing module enables the agent to simulate specific roles within the game, exhibiting\nbelievable behaviors that align with each role’s characteristics and objectives. The action module\ntranslates the generated text decisions into executable actions, allowing the agent to interact and\nmanipulate game elements effectively. The learning module continuously improves the agent’s\ncognitive and game-playing abilities through accumulated experience and interaction within the game\nenvironments.\n2.1\nPerception\nPerception acts like the agent’s sense organs, such as eyes, with their primary role being to perceive\ninput from a multimodal domain that encompasses various modalities, including text, visuals, sound,\ntouch, etc. Efficient and robust perception functions are critical to empower a game agent to accurately\ncapture the important game state information for decision-making.\n2\nFigure 1: The conceptual architecture of LLMGAs. At each game step, the perception module\nperceives the multimodal information from the game environment, including textual, images, symbolic\nstates, and so on. The agent retrieves essential memories from the memory module and take them\nalong with perceived information as input for thinking (reasoning, planning, and reflection), enabling\nitself to formulate strategies and make informed decisions. The role-playing module affects the\ndecision-making process to ensure that the agent’s behavior aligns with its designated character. Then\nthe action module translates generated action descriptions into executable and admissible actions\nfor altering game states at the next game step. Finally, the learning module serves to continuously\nimprove the agent’s cognitive and game-playing abilities through accumulated gameplay experience.\nAll text-based games, regardless whether they are text adventure games, such as Zork I [27], or\ncommunication games, such as Werewolf [28], are described using natural languages and can be\ndirectly tackled by LLMs. In contrast, for videos games there are three primary ways to enable agents\nperceive the game state:\n1. State variable access: Some game environments [29; 30; 31; 14; 32] support to access symbolic\nstate variables via internal APIs. For example, a Pokémon in Pokémon battles [30] can be\nrepresented by the state variables of species, statistics, status, and moves, without relying on\nany visual information. In Minecraft, Mineflayer [33] provides high-level APIs to access the\nlocal environment state, such as positions, blocks, inventory. The state values are filled into\ndesigned prompt templates to form textual descriptions of game states. However, not all the\ngames support internal APIs, and describing games merely with symbolic states can result in\ninformation loss, especially for games that require detailed visual information to fully capture\nthe gameplay experience, like Red Dead Redemption 2 [34] and StarCraft II [29].\n2. External visual encoder: To solve text-only problem, existing studies equips LLMs with\nexternal visual encoders to translate visual information into textual observations. For example,\nCoELA [35] and LLMPlanner [36] adopt object detectors to recognize objects within the agent’s\nfield of view in embodied environments. The CLIP [37] visual encoder and its variants are\nwidely used for mapping images into pre-defined text descriptions [38; 39; 40]. For example,\nJARVIS-1 [39] uses MineCLIP [41] to select the most similar text description from a set of\n1,000 Minecraft text data entries for images; ELLM [40] adopts ClipCap [42] as the captioner\nfor visual observations: it maps CLIP embedding to a 10-token sequence, which are fed as the\nprefix for GPT-2 to generate the whole caption.\n3. Multimodal LLMs (MLLMs): Visual encoders fall short in generalizability for unseen sce-\nnarios or objects, as they primarily rely on predefined text descriptions for classification. In\ncomparison, MLLMs align visual and textual information in a unified representation space\nand decode them into natural languages, thereby enabling better generalizability across un-\n3\nknown scenarios. General-purpose MLLMs like GPT-4V [3] are adopted in the game-playing of\nRDR2 [34], Doom [43], Minecraft [44] and simulated Embodied household [45] to directly per-\nceive visual observations for decision-making or generating text data as the perception module,\nbut usually need error correction mechanisms [45; 34] with feedback from the environments\nto address inaccuracies; Game-specific MLLMs involve supervised learning on multimodal\ninstruction data generated by experts, such as GATO [46] and SteveEye [47], or learned from\nenvironmental feedback through RL such as Octopus [45].\nIn summary, for video games, accessing symbolic states requires the support of internal APIs. External\nvisual encoders suffer from limited generalizability, as they cannot fully cover all scenarios or objects,\nespecially those without predefined textual descriptions. Although general-purpose MLLMs address\nabove-mentioned issues, they are still insufficient for distinguish fine-grained details like the relative\npositions of target objects, and struggles to understand game-specific concepts [34]. Grounding and\ndisciplining MLLMs with game experience and feedback [45] is a promising way to enable better\nperception and understanding for games.\nPerception\nText game\nZork Series [27; 48], ScienceWorld [49],\nALFworld [50], Diplomacy [51],\nWereworlf [28], Avalone [52], etc.\nVideo game\nState variable access\nStarCraft II [29], Pokémon Battles [30],\nOvercooked [31], Poker [53; 54],\nChess [55], Civilization [56], etc.\nVisual encoder\nELLM [40], LLMPlanner [36], CoELA [35],\nMineCLIP [41], Jarvis-1 [39], etc.\nMLLMs\nSteveEye [47], Octopus [45], GCC [34],\nPlayDoom [43], GATO [46], etc.\nFigure 2: Mind map for the perception module.\n2.2\nMemory\nHumans rely on memory systems to memorize prior experiences for recalling, learning, and applying\nknowledge in future scenarios. Similarly, LLMGAs necessitate memory systems to ensure operational\ncoherence and efficiency, serving as a repository for their past observations, thoughts, actions, and\nskills, from which agents retrieve essential information for strategy formulation and decision-making.\nFrom a perspective of cognitive science [57; 5], human’s memory mechanism can be divided as\nworking memory and long-term memory, where working memory stores an agent’s current context,\nand long-term memory stores the agents past experience and thoughts. For LLMGAs, working\nmemory can be regarded as the context of LLMs, and the term \"memory\" here refers to the long-term\nmemory, which acts as an external storage. Long-term memory stores episodic memories [58] such as\nobservation streams [59] and previous game trajectories [28; 60; 61] generated through the perception\nmodule; high-level semantic memories [62] that represents the agents’ understanding of itself and\nthe game world [59; 63], generated through the thinking module; and procedural memories [64],\nwhich represent curated skill stored as code [65; 34] or plans [66; 39].\nRetrieval: As memories accumulate over time, the majority of them distract from decision-making.\nRetrieval serves as an essential role to filter out through and pass the most relevant memories to the\nagent. Memory records are typically stored as key-value pairs. In semantic retrieval, the process\ninvolves calculating the semantic similarity between the representations of a query and the memory\nkeys, and selecting the memory values with the highest similarity to the query object. The query\nobject can be various forms, such as self-instructed questions [59], task-triggered questions [65],\npredefined questions [28], or visual observations [39]. Specifically, in Voyager [65]’s memory system,\nthe keys are program descriptions, and the values are the previously executed successful program\ncodes. In JARVIS-1 [39], the keys are composed of task descriptions paired with observations in\nimages, while the values represent the previously executed plans. Additionally, to simulate the\nhuman forgetting mechanism, Generative Agents [59] take into account recency and importance,\nwhere recency is calculated using an exponential decay function over game hours, and importance is\nevaluated by the LLM to differentiate mundane details from core information.\n4\nMemory\nFunctionality\nEpisodic memory\nGenerativeAgents [59], Xu et\nal. [28], ProAgent [61], etc.\nSemantic memory\nGenerativeAgents [59], Reflexion [63], etc.\nProcedural memory\nVoyager [65], GTIM [66],\nJARVIS-1 [39], Craddle [34], etc.\nRetrieval\nText\nGenerativeAgents [59], Voyager [65],\nGTIM [66], Craddle [34], etc.\nMulti-modal\nJARVIS-1 [39], CoELA [35], HAS [67], etc.\nFigure 3: Mind map for the memory module.\n2.3\nRole-playing\nRole-playing enables agents to assume diverse characters or roles within the game, generating\nbelievable conversations and behaviors appropriate to the given roles. Many games feature role-\nplaying elements [59; 28; 52] where players assume specific roles and engage in game-playing from\nthe perspective of the characters, leading to immersive gaming experiences. Role-playing is also\nimportant for building Non-Player Characters (NPCs) [68] and game assistants [69], as well as for\ngenerating dialogues [70].\nIt has been proved that assigning different personality types can largely influence the generative\nstyle of LLMs [71; 70]. Role-playing can enhance the vividness [72], personalization [73] and\nproficiency [74] of LLMAs, and generating dialogues with affective information makes agents’\nbehavior more believable [75; 76]. For role-playing, the simplest way is to directly insert natural\nlanguage descriptions of a role’s identity, such as character traits, hobbies, occupation and social\nrelationships, as initial memories for the agent [59]. Evaluations show that providing few-shot\ndialogue examples or fine-tuning can further enhance the role-playing performance in conversational\ntasks [70; 77]. Recent advanced approaches such as CharacterLLM [78] build imaginary experience\nfrom characters’ profiles, and fine-tune LLMs with these experiences to enable agents to exhibit\nconsistent personalities and express emotions.\n2.4\nThinking\nThinking is the cognitive process of analyzing and integrating information. In this section, we\nintroduce two primary thinking methods for decision-making: reasoning and planning. Reasoning\ninvolves using deduction, induction, and abduction to generalize observations, derive conclusions,\nand infer explanations. In comparison, planning strategizes decision steps to achieve complex and\nlong-horizon goals.\n2.4.1\nReasoning\nReasoning [79] is a process that starts from observation, factual evidence, and previous thoughts,\nthen progresses through analyzing and synthesizing these elements to deduce specific conclusions\nfrom general principles (deduction), infer general principles from specific instances (induction), or\nformulate the most likely explanations (abduction). Reasoning is fundamental to human cognition,\nenabling individuals to make sense of the world, solve problems, and make informed decisions.\nLLMGAs [80; 56; 43; 30] adopt general-purpose reasoning approaches [81; 82; 80; 83; 84] to analyze\ninformation logically, providing informative insights for decision-making: ReAct [80] introduce\nreasoning to condition the generation of action with few-shot prompting; CoT [81] and Zero CoT [82]\ndecompose the entire thinking process into multiple chained thoughts, enabling the step-by-step\nelucidation of complex problems; SC [84], ToT [83] and GoT [85] follow a multi-path reasoning\nparadigm: SC conducts multiple times independent reasoning and choose the result with the highest\nfrequency as the final output; ToT [83] and GoT [85] organize reasoning paths into tree and graph-like\nstructures to enhance the reasoning ability.\n5\nReflection [63; 38; 86] can be recognized as a special type of reasoning that usually occurs after feed-\nback from previous trials is provided. It involves the agent analyzing its own actions, decisions, and\nthought processes, and considering how these could be improved based on the feedback received. This\nreflective process allows for the adjustment and refinement of strategies and behaviors, contributing to\nlearning and development over time. Specifically, Reflexion [63], DEPS [38], AgentPro [61], ProA-\ngent [87] identify errors and inefficiencies in past failed attempts through self-reflection and reuse the\nthoughts to enhance the performance in subsequent trials. Moreover, RCI [88], Self-Refine [86] and\nGPTLens [89] demonstrates that the feedback can not only come from environments, but also LLMs\nthemselves, and iteratively refine the results by incorporating self-generated reflection.\nIn game playing, Hu et al. [30] discover that when encountering a powerful opponent, CoT can\nintroduce panic feelings, causing the agent to act inconsistently, such as switching to different\nPokémon in consecutive turns. In comparison, SC alleviates the issue by voting for the most consistent\naction; Theory-of-Mind [90; 91] (ToM) thinking involves inferring others’ intentions from a shifted\nperspective, and demonstrate enhancement in imperfect information games like Poker [54] and\nenables LLMGAs recognize partners’ intention for assistance in cooperation games [92]. Moreover,\nreflecting on the surface observations or experiences can provide high-level, abstract thoughts, which\nhelps the agent act more reasonably and believably [59].\n2.4.2\nPlanning\nHumans utilize planning as a strategic tool to address and manage challenging and long-term tasks.\nFor LLMGAs, planning involves the decomposition of a complex task into simpler, executable subtask\nset. Existing planning approaches can be categorized as goal-free planning and goal-conditioned\nplanning, based on whether a predefined goal is necessary for the planning process.\nGoal-free planning: Open-ended games usually do not have prefixed goals for players to achieve.\nGenerating a goal plan saves the agent from being overwhelmed by numerous possible actions.\nExisting goal-free planning approaches [59; 93; 65; 40; 94; 95] primarily instruct LLMs to generate\ngoal plans. Voyager [65], ELLM [40], SPRING [94] and AdaRefiner [95] prompt LLMs with agent’s\nstates such as hunger, inventory, and equipment, and local observations for generating suitable\nnext goals; OMNI [93] prompts LLMs to select interesting and learnable tasks for agent to explore\nthe open-world. In simulation games, a long-term daily plan can effectively prevent incoherent\nbehaviors [59; 30]. Generative Agents [59] utilize a top-down approach for generating a one-day\nplan for human-simulacra agents, starting with a broad initial plan for the day, then breaking it down\ninto more detailed action plans. After planning, agents can choose to either continue with the plan or\nreact to its dynamic local environment.\nGoal-conditioned planning: A goal-conditioned planner recursively translates a goal, task, or\ninstruction into a set of subgoals until it reaches structured actions. Goal-conditioned planning is\nused for addressing long-horizon and complex tasks such as crafting tools [38; 66] or performing\nquests [36; 96; 97]. Existing studies primarily instruct LLMs to generate plans. ZeroShotPlanner [98]\nand LLMPlanner [36] prompts LLMs with zero-shot or few-shot examples for planning; Given\nthe difficulty in generating a correct plan on the first attempt, GITM [66] and JARVIS-1 [39]\nleverage external knowledge, such as item crafting recipes to enhance planning, and also incorporate\nenvironmental feedback such as error messages to refine the initial plan; DEPS [38] introduce error\ncorrection on initial plans by integrating description of the plan execution and self-explanation of\nfeedback when encountering failures; Adapt [96] and SwiftSAGE [97] adaptively decompose tasks\nwith LLMs when encountering execution failures; S-Agents [99], HAS [67] and MindAgents [100]\noperate in a hierarchical cooperation structure in which a LLM planner dispatches sub-tasks to\nmulti-agents for efficient execution.\n2.5\nAction\nAfter humans make decisions to take actions, they control their bodies, such as hands, to execute\nthese actions, translating cognitive decisions into physical movements that interact with the world\naround them. The action module serves as the hands of the LLMGAs, translating language-described\ndecisions into executable actions in the game environment, enabling the agents to interact with and\nalter their surroundings or game states. Different games necessitate different levels of granularity in\nagents’ output actions. For games requiring manipulative control like RDR2 [34], Minecraft [14]\n6\nThinking\nReasoning\nCoT [81], Zero-CoT [81], ReAct [80],\nSC [84], ToM [91], GenerativeAgents [59],\nReflexion [63], DEPS [38], ProA-\ngent [87], RCI [88], Self-Refine [86], etc.\nPlanning\nGoal-free planning\nGenerative Agents [59], Voy-\nager [65], OMNI [93], ELLM [40],\nSPRING [94], AdaRefiner [95], etc.\nGoal-conditioned\nplanning\nZeroShotPlanner [98], LLMPlan-\nner [36], DEPS [38], GTIM [66],\nPlan4MC [101], SwiftSAGE [97],\nAdapt [96], S-Agents [99], etc.\nFigure 4: Mind map for the thinking module.\nand Overcooked [102], a low-level action like keyboard or mouse operation is required. In contrast,\ngames without manipulative control like text adventure games [103; 104], Pokémon battles [30] and\nPoker [53] directly facilitate the execution of high-level actions.\nLLMs typically generate high-level actions rather than low-level actions. Therefore, for games with\nmanipulative control, a translation module is required to translate LLM-generated action into low-\nlevel actions. Existing studies adopt heuristics [92; 105; 66; 33; 59] or RL policies [40; 101; 106] for\ntranslating a high-level action into low-level action sequences. Heuristic-based translation generates\nlow-level movements using path-finding algorithms, along with manipulative actions. For example,\nin Overcooked, given a high-level action \"chop tomato\", the translation module identifies the shortest\npath to the target with a breadth-first search algorithm and identifies a sequence of movements along\nwith the chop action [92; 105]; In Minecraft, the high-level \"approach\" action uses an A∗algorithm\nfor path-finding and executes low-level actions like jump, move and fall in four directions [66; 33];\nIn comparison, RL-based approaches [41; 40; 101; 106] train language-conditioned RL policies that\ntake observations and high-level actions as input to generate low-level actions, rewarded based on the\nsemantic similarity between the goals and the agent’s transitions.\nGames without manipulative control can be divided as parser-based games [27; 107] and choice-\nbased games [30]. Parser-based games require LLMs to generate an action word by word, wherease\nchoice-based games only need LLMs to select from a set of given actions. For parser-based games,\nZeroShotPlanner [98] proposes semantic translation that maps LLM-generated free-form actions\nto the semantically similar, admissible actions; SayCan [108] calculates the probability of each\nadmissible action using the chain rule by multiplying the conditional generation probability of each\nsuccessive string given the previous string.\n2.6\nLearning\nHumans are able to refine their cognitive abilities and acquire knowledge by interacting with the\nphysical world, gaining hands-on experience through direct engagement with their environments.\nSimilarly, an LLMGA’s learning process involves improving its cognitive and game-playing abilities\nover time, based on the experiences and feedback received from the game environment.\nLLMs encode a wealth of semantic knowledge about the world while lack of real experience within\nenvironments, i.e., they are ungrounded [108]. The majority of existing LLMGAs adopt frozen LLMs\nto play games, relying on carefully designed prompts [28; 109] or external knowledge [30; 39; 66].\nIn comparison, enable LLMGAs to learn in environments is crucial, since it closely mirrors the way\nhumans acquire knowledge through interacting with the real world. Existing learning approaches\ncan be divided into three categories: in-context feedback learning, supervised fine-tuning and\nreinforcement learning.\nIn-context feedback learning: Feedback represents a type of evaluation for previous strategies. By\nincluding feedback from environments into context, LLMs are able to iteratively \"reinforce\" strategy\ngeneration without updating weights [63; 65; 66; 30]. Specifically, Reflexion [63] and DEPS [38]\ngenerate self-reflection\/explanation on the feedback like failure signal and reuses the thought for the\nnext trail; Voyager [65] and GTIM [66] iteratively prompt LLMs to re-generate action code with error\nmessages; Hu et al. [30] uses manually generated feedback such as the HP change across consecutive\n7\nturns as evaluation for previous actions; Furthermore, existing works [63; 86; 86] demonstrate\nfeedback cannot only comes from the game environments, but also from LLMs themselves.\nSupervised fine-tuning: Supervised fine-tuning [110; 111] gathers high quality experience to fine-\ntune LLMs, based on the assumption that such experiences encompass environmental knowledge.\nSpecifically, E2WM [110] collects embodied experience in VirtualHome with Monte Carlo Tree\nSearch and random exploration; LLAMARider [111] gathers experience in Minecraft via self-\nreflection with feedback. Both of them demonstrate that fine-tuning on the collected experience\nenhances capability of LLMs on solving tasks within the environment. Moreover, imitation learning-\nbased approaches like GATO [46], LID [112], SwiftSAGE [97] and Octopus [45] fine-tune LMs\nusing expert or oracle trajectories to enhance their performance as policies.\nReinforcement Learning: Existing RL-based LLMGAs can be divided into three categories: (1)\nLLM as actor: GLAM [113] is grounded in the BabyAI-text environment as a policy to select\nnext action (four movements), training through online RL [114]; (2) LLM as planner: Existing\nstudies such as SayCan [108], Plan4MC [101], RL-GPT [106], ELLM [40], follow a hierarchical\nparadigm that integrates fixed LLMs as high-level planners with separate low-level RL policies to\nexecute actions. In comparison, another line of research involves fine-tuning large language model\n(LLM) planners based on rewards received from the environment, such as Octopus [45]; (3) LLM\nas presenter: LMs can be co-trained with RL policies to produce consistent dialogues that reflect\nthe intentions of policy models, especially in communication games such as Diplomacy [51] and\nWerewolf [115]; (4) LLM for reward design: LLMs can directly serve as reward models [116],\nprovide annotations for training reward models [117], or generate and refine reward functions for\nguiding RL agent training [118; 119; 120].\nLearning\nIn-context feed-\nback learning\nReflexion [63], DEPS [38], Voyager [65],\nRCI [88], PokéLLMon [30], etc.\nSupervised\nfine-tuning\nE2WM [110], LlaMARider [111],\nLID [112], SwiftSAGE [97],\nAdaRefiner [95], etc.\nReinforcement\nLearning\nLLM as actor\nGLAM [113], etc.\nLLM as planner\nSayCan [108], ELLM [40],\nPlan4MC [101], RL-GPT [106], etc.\nLLM as presentor\nCicero [51], Thinker [115], etc.\nReward design\nRewardDesign [116], Motif [117],\nMC-Reward [118], Eureka [120], etc.\nFigure 5: Mind map for the learning module.\n3\nLLMGAs in Games\nWe categorize existing studies into six categories based on the main characteristics of the games they\nsupport, including adventure, communication, competition, cooperation, simulation, and crafting &\nexploration. Figure 6 illustrates the core gameplay mechanics associated with its genre:\n• Adventure: Adventure games emphasize story-driven gameplay, where players explore environ-\nments, solve quests and interact with characters and objects to progress the game. Representative\ngames: Zork I [27] and Red Dead Redemption 2 (RDR2) [34].\n• Communication: Communication games revolve through the turns of communication, negotiation,\ndeduction and even deceptions among multiple players. Representative games: Werewolf [28] and\nDiplomacy [51].\n• Competition: Competition games pit players against each other in challenges that test skill or\nstrategy, aiming to outperform others for victory. Representative games: StarCraft II [29] and\nPokémon Battles [30].\n8\nFigure 6: The depiction of six game categories.\n• Cooperation: Cooperation games are designed around players working together towards common\ngoals, emphasizing teamwork, collaborative problem-solving, and shared achievements. Represen-\ntative games: Overcooked [102].\n• Simulation: Simulation games replicate real-world events in detail, allowing players to experience\nand manage scenarios ranging from building a civilization or living another life. Representative\ngames: The Sims [59; 121] and Civilization [56].\n• Crafting & Exploration: Crafting & Exploration games provide open worlds where players gather\nresources, craft items, and exploring within expansive environments, encouraging creativity and\ndiscovery. Representative games: Minecraft [14] and Crafter [122].\nWe summarize existing studies on LLMGAs in Table 1. In this section, we will walk through six game\ncategories, highlighting key findings and methodologies employed in the current research landscape.\n3.1\nAdventure Games\nAdventure games typically progress through storylines or quests. We categorize existing works into\ntwo types based on modality: text-based adventure games and video adventure games.\nText adventure games: A text adventure game provides a text-based environment in which players\nuse text commands to interact with the world, exploring and completing quests. TextWorld [137] is a\ngenerator of synthetic text games [138; 103] with varying difficulty levels by adjusting parameters\nsuch as the numbers of rooms and objects, quest length and complexity; Jericho [104] is a collection\nof 56 human-made games originally designed for human players, covering fictions such as the\nZork series [27; 48] and Hitchhiker’s Guide to the Galaxy [139]; ALFWorld [50] is aligned to\nthe embodied environment ALFRED [140], where agents are requested to accomplish six types\nof household tasks; ScienceWorld [49] simulates a primary school science curriculum, such as\nthermodynamics and electrical circuits. To complete a quest, an agent needs to navigate to specific\nrooms, obtain necessary items, conduct experiments, and analyze the results; BabyAI-Text [113] is\na text extension of BabyAI [141], a procedurally generated minigrid environment where an agent\nnavigates and interacts with objects.\nDue to the lack of graphics, text games rely on the commonsense knowledge as a prior for how\nto interact with the environment. In parser-based text games, generating a three-word sentence\nwith a small vocabulary of size 1,000 leads to 1 billion combinatorial candidates. Pre-trained LMs\nfeaturing human knowledge can effectively narrow down the action space and thus have been widely\nutilized as linguistic priors for guiding RL agents [123; 142; 143; 144]. Recently, LLMGAs are\nemployed to playing text adventure games: Tsai et al.[124] suggest that the game-playing ability of\nGPT-3.5 is on par with state-of-the-art (SOTA) reinforcement learning (RL) approaches[145; 146],\nbut it is incapable of constructing the entire map of a partially-known environment; REACT [80]\nand Reflexion [63] prompt LLMs to generate additional reasoning and reflection to condition the\ngeneration of actions; To solve challenging tasks, Adapt [96] and SwiftSage [97] adopt an LLM\nplanner to decompose complex tasks into subgoals as needed; GLAM [113] leverage online RL to\nground an LLM in BabyAI-text as a policy.\nVideo adventure game: Red Dead Redemption 2 (RDR2) is a 3D action-adventure game in which\nplayers assume the role of an outlaw, and follow the storyline of his life as part of a criminal gang.\nThe game features an important characteristic, i.e., it guides the player what to do next with instant\ninstructions. Cradle [34] is an LLMGA that perceives the game screen, analyzes instructions, generate\naction plans and controls the character through mouse\/keyboard operations using GPT-4V.\n9\nTable 1: Comparison among existing LLMGAs. FT denotes Fine-Tuning.\nStudies\nCategory\nGame\nBase Model\nFT\nModality\nCALM [123]\nAdventure\nJericho\nGPT-2\n✓\nTxt\nCanPlayWell [124]\nAdventure\nZork I\nGPT-3.5\n✗\nTxt\nReAct [80]\nAdventure\nALFWorld\nPaLM\n✗\nTxt\nReflexion [63]\nAdventure\nALFWorld\nGPT-3\n✗\nTxt\nADAPT [96]\nAdventure\nALFWorld\nGPT-3.5\n✗\nTxt\nSwiftSAGE [97]\nAdventure\nScienceWorld\nGPT-4 & T5\n✓\nText\nGLAM [113]\nAdventure\nBabyAI-Text\nFLAN-T5\n✓\nTxt\nCradle [34]\nAdventure\nRDR2\nGPT-4V\n✗\nTxt & Img\nXu et al. [28]\nCommunication\nWerewolf\nGPT-3.5\n✗\nTxt\nXu et al. [125]\nCommunication\nWerewolf\nGPT-4\n✗\nTxt\nThinker [115]\nCommunication\nWerewolf\nChatGLM-6B\n✓\nText\nReCon [52]\nCommunication\nAvalone\nGPT-4\n✗\nTxt\nAvaloneBench [126]\nCommunication\nAvalone\nGPT-3.5\n✗\nTxt\nCodeAct [127]\nCommunication\nAvalone\nGPT-4\n✗\nTxt\nCicero [51]\nCommunication\nDiplomacy\nBART\n✓\nTxt\nWarAgent [109]\nCommunication\nDiplomacy-like\nGPT-4\n✗\nTxt\nCosmoAgent [128]\nCommunication\nDiplomacy-like\nGPT-4\n✗\nTxt\nDEEP [129]\nCommunication\nWord Guess\nGPT-4\n✗\nTxt\nGameEval [130]\nCommunication\nWord Guess\nGPT-4\n✗\nTxt\nPokéLLMon [30]\nCompetition\nPokémon Battles\nGPT-4\n✗\nTxt\nCoS [29]\nCompetition\nStarCraft II\nGPT-3.5\n✗\nTxt\nSwarmBrain [131]\nCompetition\nStarCraft II\nGPT-3.5\n✗\nTxt\nChessGPT [55]\nCompetition\nChess\nRedPajama-3B\n✓\nPGN\nOthelloGPT [132]\nCompetition\nOthello\nGPT\n✓\nPGN\nPokerGPT [53]\nCompetition\nTexas Hold’em\nOPT-1.3B\n✓\nTxt\nGoodPoker [133]\nCompetition\nTexas Hold’em\nGPT-4\n✗\nTxt\nSuspicionAgent [54]\nCompetition\nLeduc Hold’em\nGPT-4\n✗\nTxt\nAgentPro [61]\nCompetition\nLeduc Hold’em\nGPT-4\n✗\nTxt\nLLM-Co [92]\nCooperation\nOvercooked\nGPT-4\n✗\nTxt\nMindAgent [100]\nCooperation\nOvercooked\nGPT-4\n✗\nTxt\nProAgent [87]\nCooperation\nOvercooked\n-\n✗\nTxt\nHLA [105]\nCooperation\nOvercooked\nGPT-3.5&LLaMA2\n✗\nTxt\nS-Agents [99]\nCooperation\nMinecraft\nGPT-4\n✗\nTxt\nHAC [67]\nCooperation\nMinecraft\nGPT-4V\n✗\nTxt & Img\nCoELA [35]\nCooperation\nTDW-T&WAH\nGPT-4\n✗\nTxt & Img\nGenerativeAgents [59]\nHuman Simulation\nSmallVille\nGPT-3.5\n✗\nTxt\nHumanoidAgents [134]\nHuman Simulation\nSocial\nGPT-3.5\n✗\nTxt\nLyfeAgent [121]\nHuman Simulation\nLyfe Game\nGPT-3.5\n✗\nTxt\nAgentSims [135]\nHuman Simulation\nAgentSims\n-\n✗\nTxt\nCivRealm [56]\nCivil. Simulation\nCivilization\n-\n✗\nTxt\nZeroShotPlanner [98]\nEmbodied Simulation\nVirtualHome\nGPT-3\n✗\nTxt\nLLMPlanner [36]\nEmbodied Simulation\nALFRED\nGPT-3\n✗\nTxt & Img\nE2WM [110]\nEmbodied Simulation\nVirtualHome\nLLaMA-13B\n✓\nTxt\nOctopus [45]\nEmbodied Simulation\nBehavior-1K\nCLIP & MPT-7B\n✓\nTxt & Img\nVoyager [65]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt\nDEPS [38]\nCraft & Explore\nMinecraft\nMineCLIP & GPT-4\n✗\nTxt & Img\nGTIM [66]\nCraft & Explore\nMinecraft\nGPT-3.5\n✗\nTxt\nJARVIS-1 [39]\nCraft & Explore\nMinecraft\nMineCLIP & GPT4\n✗\nTxt & Img\nPlan4MC [101]\nCraft & Explore\nMinecraft\nGPT-3.5\n✗\nTxt & Img\nRL-GPT [106]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt & Img\nMineDoJo [41]\nCraft & Explore\nMinecraft\nMineCLIP\n✓\nTxt & Img\nLLaMARider [111]\nCraft & Explore\nMinecraft\nLLaMA-2-70B\n✓\nTxt & Img\nSteveEye [136]\nCraft & Explore\nMinecraft\nCLIP & LLaMA2\n✓\nTxt & Img\nCreativeAgent [44]\nCraft & Explore\nMinecraft\nGPT-4V\n✗\nTxt & Img\nMCReward [44]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt & Img\nELLM [40]\nCraft & Explore\nCrafter\nCodex\n✗\nTxt & Img\nSPRING [94]\nCraft & Explore\nCrafter\nGPT-4\n✗\nTxt & Img\nAdaRefiner [95]\nCraft & Explore\nCrafter\nLLaMA2 & GPT-4\n✓\nTxt & Img\nOMNI [93]\nCraft & Explore\nCrafter\nGPT-3\n✗\nTxt & Img\nPlayDoom [43]\nOthers\nDoom\nGPT-4V\n✗\nTxt & Img\nGATO [46]\nOthers\nAtari\nGATO\n✓\nImg\nMotif [117]\nOthers\nNetHack\nLlaMA-2\n✗\nTxt\n10\n3.2\nCommunication Games\nCommunication (or conversational) games revolve through the turns of communication, negotiation,\ndeduction and deception among multiple players. The challenge of communication games lies in\ninferring others’ intention behind ambiguous or misleading language utterances, and hiding one’s\nown intention if necessary.\nWerewolf: The game pits two groups against each other, i.e., werewolves and non-werewolves\n(villagers, witch, guard and seer), and alternates between night phases, where werewolves secretly\nattack, and day phases, where survivors discuss and vote to eliminate suspects. The witch, guard,\nand seer each possess unique abilities. Xu et al. [28] propose to retrieve and reflect on historical\ncommunications for enhancement, and observe that GPT-3.5 demonstrates strategic behaviors such\nas trust, confrontation, camouflage, and leadership. Xu et al. [125] employ a RL policy to select the\noptimal action from among the diverse actions generated by LLMs, aiming to overcome the LLMs’\nprior preference for specific actions. Wu et al. [115] introduce a RL policy to generate the next\naction by taking as input the reasoning generated by the LLM, and employ another LLM to generate\ndescriptions aligned to the action.\nAvalon: The game progresses through rounds of discussion and voting to decide who participates in\nthe quests. The goal for the good team is to successfully complete quests, while the bad team aims\nto secretly sabotage these quests or identify the role of Merlin, who knows the identities of the bad\nplayers. Light et al. [126] suggest that GPT-3.5 struggles to formulate and execute simple strategies\nand sometimes reveals its own bad identity. Wang et al. [52] introduce a reasoning approach that\ntakes into account first-order and second-order perspective shifts to combat pervasive misinformation.\nTo combat hallucination, Shi et al. [127] propose to generate reasoning substeps in a code format that\nare interpreted as actions subsequently.\nDiplomatic games: Diplomacy is the first diplomatic board game from the 1950s where players\nassume the roles of seven powers striving to conquer Europe during WW1. Each turn is marked\nby private negotiations, trust-building, and tactical coordination among players. Cicero [51] is a\nhuman-level agent in Diplomacy that integrates a RL policy for planning and a BART [147] model\nconditioned on the plan for generating consistent negotiation messages; WarAgent [109] simulates\nthe participating countries, decisions, and consequences in WW I and WW II; CosmoAgent [128]\nmimics the communication, conflicts, and cooperation among various universal civilizations.\nOthers: Studies have demonstrated the game-playing abilities of LLMs in various games, including\nSpyGame (Who is Spy) [130; 129], Ask-Guess [130; 129], Tofu Kingdom [130], and Murder Mystery\nGames [148], known as Jubensha in Chinese.\n3.3\nCompetition Games\nCompetition games, governed by strict rules, challenge agents with varied-level opponents, demanding\nadvanced reasoning and skills. Competition games serves as benchmarks for evaluating reasoning and\nplanning abilities of LLMGAs directly against human players. Reaching human-level performance is\na crucial achievement that highlights the agent’s prowess in complex decision-making and strategic\nimplementation.\nStarCraft II: StarCraft II is a real-time strategy game in which players are tasked with gathering\nresources, building bases, creating armies, and engaging in combats to defeat the opponent. Ma et\nal. [29] introduce TextStarCraft II, a natural language interface that enables LLMs to play StarCraft\nII and Chain-of-Summarization for efficient reasoning and decision-making; SwarmBrain [131]\nintroduce an Overmind Intelligence Matrix for high-level strategic planning and a Swarm ReflexNet\nfor rapid tactical responses. These LLM-based agents exhibit comparable performance against the\ngame’s built-in AI at high difficulty levels.\nPokémon Battle: Pokémon battles are turn-based tactical games, with two players each sending out\none Pokémon and choosing either to attack or switch Pokémon each turn. Hu et al. [30] introduce an\nenvironment that enables LLMs to play Pokémon battles and a human-level agent PokéLLMon that\nconsumes instant feedback to iteratively refine the policy, retrieves external knowledge to combat\nhallucination, and generates consistent actions to alleviate the panic switching problem caused by\nCoT [81] reasoning.\n11\nChess: Feng et al. [55] introduce a large-scale chess gameplay dataset stored in Portable Game\nNotation format [149] and ChessGPT fine-tuned on mixed chess and language datasets to support\nboard state evaluation and chess playing; Toshniwal et al. [150] and Li et al. [132] discover that\nLMs trained to predict next move in chess are capable of tracking the state of the board given a\nmove sequence, i.e., LMs are capable of playing blindfolded. This suggests that LMs do not merely\nmemorize surface statistics but also learn a causal model of the sequence-generating process.\nPoker: In Texas Hold’em, Gupta et al. [151] observe that GPT-4 plays like an advanced yet aggressive\nplayer who raises with a wide range of hands pre-flop, avoids limping, and exhibits unconventional\nplay; PokerGPT [53] demonstrates that OPT-1.3B [152] with supervised fine-tuning and RLHF [2]\ncan achieve comparable performance to a RL-based method Alphaholdem [153] with significantly\nless training cost: 9.5 GPU hours compared to Alphaholdem’s 580 GPU hours. Guo et al. [54]\nand Zhang et al. [61] demonstrate that prompting LLMs to predict opponents’ thoughts, known as\nTheory of Mind [90; 91], results in significant improvements in Texas Hold’em, BlackJack and Leduc\nHold’em [11].\n3.4\nCooperation Games\nCooperation among individuals can enhance the efficiency and effectiveness of task accomplishment.\nThere are primarily three types of cooperative tasks in games: (1) Cooperative cooking [102; 100;\n154] requires agents collaborate to cook and deliver as many dishes as possible within the given time.\nTo prepare an onion soup in Overcooked-AI [102], two agents need to load three onions into a cooker,\nstarting a cooking process that lasts 20 time steps, and transfer the soup to a plate for delivery; (2)\nEmbodied household cooperation [155; 156] requires agents to collaboratively accomplish tasks\nlike transporting as many objects as possible to the goal position in embodied environments with\npartial observation [107; 157]; (3) Cooperative crafting [99; 100] & exploration [67] in Minecraft\ncan be accelerated through cooperation between multiple agents. Existing cooperative game settings\ncan be categorized into decentralized and centralized cooperation.\nDecentralized cooperation: A decentralized structure is a democratic structure (\n) where\nthere is no central task dispatcher. In Overcooked, the ability to infer the partner’s intent and next\naction based on the its historical actions, known as Theory-of-Mind, is crucial to prevent conflicts.\nAgashe et al. [92] show that GPT-4 is able to recognize and offer assistance to partners in need,\nand show robustness in adjusting to different partners. ProAgent [87] introduces a belief correction\nmodule to rectify incorrect beliefs on partners and consistently outperforms RL approaches [114;\n31; 158]. Moreover, HLA [105] integrates a proficient LLM and a lightweight LLM to balance\nefficacy and efficiency in real-time human-agent interaction; In partially-observable embodied\nenvironments, CoELA [35] introduce an efficient communication module to determine what and\nwhen to communicate, exhibiting better performance compare to MCTS-based and rule-based\nplanners on Watch-and-Help [155] and TDW Transport tasks [156].\nCentralized cooperation: In Minecraft, S-agents [99] and MindAgents [100] adopts a centralized\ndispatcher\/planner to decompose a challenging goal into subtasks and dispatches them to agents for\nexecution, forming a hierarchical architecture. HAS [67] introduces an auto-organizing mechanism\nto dynamically adjust key roles and action groups during cooperation, and an intra-communication\nmechanism to ensure efficient collaboration.\n3.5\nSimulation Games\nSimulation games provide simulated environments for real-world events or scenarios, enabling players\nto experience realistic interactions and decision-making in open-ended game playing. Existing studies\ncan be categorized as human & social simulation, civilization simulation and embodied simulation.\nHuman and social simulation: Generative Agents [59] marks the first LLM-based human simulation\nexperiment that leverages LLMs’ prior knowledge to simulate human-like daily life and social\nactivities. Specifically, GPT-3.5 assumes the roles of 25 generative agents with unique persona and\nsocial relationship, residing in a virtual small town. A cognitive architecture is introduced to support\nagents in remembering, retrieving, reflecting, planning, and acting within dynamic environments.\nDuring the two-day simulation, emergent behaviors like exchanging information, forming new\nrelationships and coordinating joint activities are observed.\n12\nOn the basis of Generative Agents, Humanoid Agents [134] further considers the effects of states like\nbasic needs (e.g., hunger, health, and energy), emotions, and closeness in relationships on agents’\nbehavior generation; For other simulation environments, AgentSims [135] is a programmable and\nextendable environment; LyfeGame [121] is an 3D virtual small town in Japan. Three experimental\nscenarios are designed to assess the social behaviors of LLM-based agents, including a murder\nmystery, a high school activity fair, and a patient-in-distress scenario.\nCivilization simulation: CivRealm [56] is a game environment based on Civilization [32], where\neach player governs a civilization simulating the progress of human history. As an open-ended game,\nit features diverse victory conditions, requiring players to strategically develop the economy, military,\ndiplomacy, culture, and technology of their civilizations. Mastaba [56] introduces an advisor and an\nAutoGPT [159]-style worker, where the advisor aids in generating context-specific objectives while\nthe workers handle the execution of these goals through generated actions. Experiments show that\nthe advisor brings an advantage at the early game stage, yet the advantage diminishes as the game\nprogresses.\nEmbodied simulation: In simulated 3D environments, embodied agents perceives their surround-\nings from egocentric perception similar to human and engage with realistic objects to carry out\na wide range of tasks by following instructions like \"Rinse off a mug and place it in the coffee\nmaker\". Existing benchmarks include AI2-THOR [160], Virtual Home [107], ALFRED [140], iGib-\nson [161], Habitat [162], ThreeDWorld [156], Behavior [163] and Behavior-1K [164]. Existing\napproaches [36; 98; 108; 110] primarily adopt LLMs as planners to decompose an instruction into\naction plans. Specifically, ZeroShotPlanner [98] prompts LLMs in zero-shot manner for planning;\nSayCan [108] uses a learned affordance function to assist LLMs in selecting valid actions during\nplanning; LLMPlanner [36] adopts an KNN retriever to select few-shot examples and dynamically\nre-plan based on the observation in the current environment; E2WM [110] fine-tunes an LLM with\nembodied experience collected through action space search and random exploration to enhance the\nunderstanding of the environments.\n3.6\nCrafting & Exploration Games\nMinecraft and Crafter are two game environments that have been widely studied for game agents with\na focus on crafting & exploration. Minecraft [14] is a 3D sandbox game that offer players the great\nfreedom to traverse a world made up of blocky, pixelated landscapes, facilitated by the procedurally\ngenerated worlds. The resouce-based crafting system enables players to transform collected materials\ninto tools, build elaborate structures and complex machines. Crafter [122] is a 2D open-world game\nthat mirrors the survival mode of Minecraft. It challenges players to manage their resources carefully\nto ensure sufficient water, food, and rest, while also defending against threats like zombies. The\ngame’s world is also procedurally generated for the exploration purpose, and it includes 22 tasks for\nplayers to accomplish.\nExisting agents can be divided as goal-conditioned agents that implement the task given an instruction\n(crafting), or autonomous exploration agents that navigate within the open-world based on self-\ndetermined objectives (exploration).\nCrafting: The key challenge in crafting tasks lies in their complexity: agents must gather diverse\nmaterials scattered across the world and understand intricate recipes and the sequential steps involved.\nConsequently, planning is widely employed to address crafting tasks. Existing agent design such\nas DEPS [38], GITM [66], JARVIS-1 [39], Plan4MC [101], RL-GPT [106] and S-agents [99]\nmainly follow a paradigm that adopts LLMs as a planner to decompose the goal into subgoals and\nfurther generate action plans for each sub-goals. Specifically, DEPS introduce error correction on\ninitial plans by integrating description of the plan execution and self-explanation of feedback when\nencountering failures; GITM [66] leverages external knowledges like item crafting\/smelting recipes,\nand is equipped with a long-term memory to maintain common reference plans for encountered\nobjectives; JARVIS-1 [39] chains MineCLIP [41] and an LLM together to perceive multimodal input\nand utilizes a multimodal memory to store experiences; Plan4MC [101] and RL-GPT [106] integrate\nthe LLM planner with a low-level RL policy for action execution; S-agents [99] and HAS [67]\ndispatches subtasks to multiple agents for cooperatively task execution;\nExploration: Navigating through procedurally generated world without specific goals can overwhelm\nagents with numerous possible actions. Previous works leverage curriculum learning [165] to identify\n13\nsuitable tasks while now LLMs can directly act as goal generators. In Minecraft, Voyager [65]\nadopts an automatic curriculum in a self-directed way [166], i.e., it asks LLM to generate goals\nthat adapts to the agent’s current state, inventory, acquired skills, and environment. In Crafter,\nOMNI [93] utilizes LLMs to determine interesting tasks for curriculum design, overcoming the\nprevious challenge of quantifying \"interest\". ELLM [40], SPRING [94] and AdaRefiner [95] prompt\nLLMs to generate goals for agents. Specifically, ELLM [40] queries LLMs for next goals given an\nagent’s current context, and rewards agents for accomplishing those suggestions in the sparse-reward\nsetting; SPRING [94] uses LLMs to summarize useful knowledge from the Crafter paper [122] and\nprogressively prompts the LLM to generate next action; On the basis of ELLM, AdaRefiner [95]\ncascades a learnable lightweight LLM with fixed LLMs for better goal plan generation.\n3.7\nEvaluation\nThe evaluation metrics for game agents vary across different games. In Table 2, we summarize\nthe metrics for several representative games. For games with specific task instructions, such as\nALFWorld, ScienceWorld, BabyAI, RDR2, ALFRED, VirtualHome, and crafting tasks in Minecraft\nand Crafter, the task success rate is usually adopted as the primary metric; for competition games,\nwin rate, game score, and Elo rating are common metrics; for communication games that separate\nplayers into adversarial teams such as Werewolf and Avalone, win rate can be used as the metric.\nFor human\/social simulation experiments, human evaluators are typically recruited to assess the\nbelievability of behaviors exhibited by LLM-based agents.\nTable 2: Evaluation metric used in representative games\nGame\nMetric\nJericho\nGame score [123]\nALFWorld\nTask success rate [80; 96]\nScienceWorld\nTask score [97]\nBabyAI\/BabyAI-Text\nTask success rate [93; 113]\nRDR2\nTask success rate [34]\nWerewolf\nWin rate [115; 28; 125], Voting accuracy [115]\nAvalone\nWin rate [52; 127; 126]\nDiplomacy\nPlayer score [51]\nStarCraft II\nWin rate [29; 131]\nPokémon Battles\nWin rate [30], Battle score [30]\nChess\nElo rating [55], Move score [55]\nPoker\nWin rate [54; 53], # of win\/loss chips [54], mbb\/hand [53]\nOvercooked\nReward value [92; 87], Success rate [100; 105]\nHuman Simulation\nHuman evaluation [59; 134]\nCivilization\nTask success rate [56], Game score [56], # of techs & units [56]\nALFRED\nTask success rate [36]\nVirtualHome\nTask success rate [98; 167], Executability [98; 167]\nMinecraft\nTask success rate [38; 65; 66], Map coverage [65], # of items [65]\nCrafter\nTask success rate [40; 93]\n4\nConclusion and Future Directions\nIn this paper, we conduct a systematic literature review of existing studies on LLMGAs, examining\ntwo primary aspects: (1) Construction of LLMGAs, where we elaborate on six essential components,\nincluding perception, memory, thinking, role-playing, action, and learning; (2) LLMGAs in six game\ncategories, including adventure, communication, competition, cooperation, simulation, and crafting\n& exploration, where we detail the game environments and common strategies adopted by game\nagents associated with each type. Finally, we identify three potential future directions for this new\nresearch field:\nGrounding LLMs in environments: LLMs pre-trained only on text corpus data are not grounded\nin real environments, i.e., they are not really aware of the consequences of their generations on\nphysical process [168]. Consequently, ungrounded LLMs generate inadmissible actions [108], exhibit\n14\na gap between high-level intentions and intricate game control [34], especially in the absence of\nvisual perception abilities [29] and feedback, and thus largely rely on manually-designed prompts.\nExisting efforts have been made toward grounding LLMs through multimodal perception [45], the\nadopt of external affordance functions [108], feedback from environments [30], and experiencing in\nenvironments [110; 40]. However, the current progress in grounding techniques remains limited and\nfall shorts for the requirements of real-world application [169]. Games, serving as controllable and\nsafe environments, are ideal testbeds for developing grounding techniques that can make LLMs more\ninline with sophisticated environments.\nKnowledge discovery through game-playing: Current studies [65; 30; 29] primarily remain at\nthe stage of utilizing the pre-existing knowledge encoded in LLMs for game-playing. Although\nsome studies propose to leverage game-playing experiences [110; 111; 40] to ground and enhance\nLLM-based agents, these agents are still incapable of extracting underlying knowledge below the\nsurfaces of observations and experience. Knowledge discovery is not simply about learning to act\neffectively, but to understand fundamental principles and causal model of gameplay mechanisms just\nlike human. We believe that gameplay mechanisms with complex extrinsic knowledge are essential\ntestbeds for designing such agents, and knowledge discovery via experiencing in environments might\nrepresent a critical step toward the pursuit of AGI.\nAgent society simulation: The social simulation experiment of Generative Agents [59] has demon-\nstrated that LLM-based agents are promising for believable simulacra of human. Emergent human-like\nsocial behaviors are observed like information diffusion, forming new relationship and coordination\nfor social activities, with the support of a novel cognitive architecture. However, as human beings are\nfar more sophisticated, with complicated mental processes, emotional depth, and advanced social\nskills, it would be an intriguing avenue for future research to develop better cognitive architectures and\nmore nuanced simulations of social interactions and cooperation [170] in realistic game environments\nto foster deeper understanding and representation of complex human interactions.\nAcknowledgements\nAuthors would like to thank Yunxiang Yan and Vijayraj Shanmugaraj for their assistance in collecting\npapers. This research is partially sponsored by the NSF CISE grants 2302720, 2312758, 2038029, a\nGTRI PhD Fellowship, an IBM faculty award, and a grant from CISCO Edge AI program.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/A Survey on Large Language Model-Based Game Agents.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nA Survey on Large Language Model-Based Game Agents\n```\n#### 2. 论文摘要\n```\nThe development of game agents holds a critical role in advancing towards\nArtificial General Intelligence (AGI). The progress of LLMs and their\nmultimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve\nand empower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview of\nLLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essential\nfunctional components: perception, memory, thinking, role-playing, action, and\nlearning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation\nagility across six genres of games, including adventure, communication,\ncompetition, cooperation, simulation, and crafting & exploration games.\nFinally, we present an outlook of future research and development directions in\nthis burgeoning field. A curated list of relevant papers is maintained and made\naccessible at: https:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n```\n\n#### 3. 论文全文\n```\nA Survey on Large Language Model-Based Game Agents\nSihao Hu†, Tiansheng Huang†, Fatih ˙Ilhan†, Selim Tekin†,\nGaowen Liu‡, Ramana Kompella‡, Ling Liu†\n†Georgia Institute of Technology,\n‡Cisco Research\nQ sihaohu@gatech.edu, ling.liu@gatech.cc.edu\nAbstract\nThe development of game agents holds a critical role in advancing towards Ar-\ntificial General Intelligence (AGI). The progress of LLMs and their multimodal\ncounterparts (MLLMs) offers an unprecedented opportunity to evolve and em-\npower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview\nof LLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essen-\ntial functional components: perception, memory, thinking, role-playing, action,\nand learning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation agility\nacross six genres of games, including adventure, communication, competition, co-\noperation, simulation, and crafting & exploration games. Finally, we present\nan outlook of future research and development directions in this burgeoning\nfield. A curated list of relevant papers is maintained and made accessible at:\nhttps:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n1\nIntroduction\nIntelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor\nactivity.\n— The Embodied Cognition Hypothesis [1]\nLarge language models (LLMs), exemplified by ChatGPT [2], represent an important milestone in\nnatural language understanding (NLU) and generative artificial intelligence (Gen-AI). Empowered by\ngenerative training over massive data of diverse web sources with hundreds of billions of parameters,\nLLMs demonstrate astonishing capabilities of generalizing knowledge from huge text corpus data\nand displaying conversational intelligence in natural language with human-level NLU performance.\nThe emergence of multimodal LLMs (MLLMs), such as GPT-4V [3] and Gemini [4], marks another\nmilestone, enabling LLMs to perceive and understand visual input. We conjecture that the success\nof LLM technologies fuels an unprecedented opportunity in the pursuit of human-like Artificial\nGeneral Intelligence (AGI): the cognitive capabilities previously thought to be exclusive to humans,\nsuch as reasoning, planning, and reflection, with a degree of self-control, self-understanding, and\nself-improving, are now achievable by integrating appropriately prompting of LLMs with built-in\ncognitive intelligence.\nWe define an LLM-based agent (LLMA) as an intelligent entity that employs LLMs1 as a core\ncomponent to conduct human-like decision-making process [5]. Even though LLMAs are capable\nof cognitive processing similar to human, a distinction between existing LLMAs and human-like\n1In this paper, LLMs refers to both large language models (LLMs) and multimodal large language models\n(MLLMs).\narXiv:2404.02039v1  [cs.AI]  2 Apr 2024\nAGI is evident: current LLMAs rely on decoding and generalizing pre-existing knowledge derived\nfrom pre-training data [6], while AGI is capable of discovering and learning new knowledge through\nexperimentation and experience in real world [7; 8]. Inspired by the process of intelligence devel-\nopment in human infants, the embodied cognition hypothesis [1] posits that the intelligence of an\nagent emerges from observing and interacting its environment, i.e., grounding the agent in a world\nthat integrates physical, social, and linguistic experiences is vital for fostering conditions conducive\nto the development of human-like intelligence.\nDigital games are recognized as ideal environments for cultivating AI agents due to their complexity,\ndiversity, controllability, safety and reproducibility. Games, ranging from classical chess and poker\ngames [9; 10; 11] to modern video games like Atari games [12], StarCraft II [13], Minecraft [14]\nand DOTA II [15], have been long instrumental in advancing AI research. Unlike traditional\nReinforcement Learning (RL)-based agents [10; 16; 17; 18] that make decisions with the goal\nof maximizing expected rewards through behavior-level policy learning, constructing LLM-based\ngame agents (LLMGAs) capable of employing cognitive abilities to gain fundamental insights into\ngameplay, potentially aligns more closely with the pursuit of AGI.\nPrevious survey papers on LLMs [19; 20; 21] or LLMAs [22; 23; 24] mainly focus on reviewing\nexisting LLMs developed in industry and academic research teams, as well as the general applications\nof LLMAs, paying less attention to the field of game agents. Concurrent survey papers [25; 26] place\na notable emphasis on the game development and cover a limited number of publications on LLMGAs.\nTo bridge this gap, this paper attempts to conduct a comprehensive and systematic survey on recent\ndevelopments in LLMGAs. Specifically, this survey is organized into three synergistic parts: First, we\nprovide a unified reference framework, in which we describe the essential modules for constructing\nLLMGAs, covering six core functional components: perception, memory, thinking, role-playing,\naction and learning. Second, we introduce a taxonomy that categorizes existing literature into six game\ncategories, including adventure, competition, cooperation, simulation, and crafting & exploration.\nFor each category, we describe the technical challenges, the supporting game environments, as well as\nthe commonly used optimization strategies. In the third and final part, we envision different directions\nof future advancement of LLMGAs.\nIn summary, this survey paper serves as a comprehensive review of the literature on LLMGAs,\noffering a taxonomy of six game categories to enhance understanding and facilitate the development\nand assessment of various LLMGAs. It aims to catalyze progress within this nascent research\narea and to inspire further innovation in research and development of LLMGAs. Given that this\nis a new and burgeoning research field, this survey paper will be continuously updated to keep\ntrack of the latest studies. A curated list of relevant literature is maintained and accessible at\nhttps:\/\/github.com\/git-disl\/awesome-LLM-game-agent-papers.\n2\nA Unified Architecture for LLMGAs\nFigure 1 provides an conceptual architecture of LLMGAs that consist of the six essential functional\ncomponents and their workflow: For each game step, the perception module captures game state\ninformation, providing the necessary data for the agent to understand its current environment. The\nthinking module processes the perceived information, generating thoughts based on reasoning,\nplanning, and reflection for informed decision-making. Memory serves as an external storage, where\npast experiences, knowledge and curated skills are retained and can be retrieved for future use.\nThe role-playing module enables the agent to simulate specific roles within the game, exhibiting\nbelievable behaviors that align with each role’s characteristics and objectives. The action module\ntranslates the generated text decisions into executable actions, allowing the agent to interact and\nmanipulate game elements effectively. The learning module continuously improves the agent’s\ncognitive and game-playing abilities through accumulated experience and interaction within the game\nenvironments.\n2.1\nPerception\nPerception acts like the agent’s sense organs, such as eyes, with their primary role being to perceive\ninput from a multimodal domain that encompasses various modalities, including text, visuals, sound,\ntouch, etc. Efficient and robust perception functions are critical to empower a game agent to accurately\ncapture the important game state information for decision-making.\n2\nFigure 1: The conceptual architecture of LLMGAs. At each game step, the perception module\nperceives the multimodal information from the game environment, including textual, images, symbolic\nstates, and so on. The agent retrieves essential memories from the memory module and take them\nalong with perceived information as input for thinking (reasoning, planning, and reflection), enabling\nitself to formulate strategies and make informed decisions. The role-playing module affects the\ndecision-making process to ensure that the agent’s behavior aligns with its designated character. Then\nthe action module translates generated action descriptions into executable and admissible actions\nfor altering game states at the next game step. Finally, the learning module serves to continuously\nimprove the agent’s cognitive and game-playing abilities through accumulated gameplay experience.\nAll text-based games, regardless whether they are text adventure games, such as Zork I [27], or\ncommunication games, such as Werewolf [28], are described using natural languages and can be\ndirectly tackled by LLMs. In contrast, for videos games there are three primary ways to enable agents\nperceive the game state:\n1. State variable access: Some game environments [29; 30; 31; 14; 32] support to access symbolic\nstate variables via internal APIs. For example, a Pokémon in Pokémon battles [30] can be\nrepresented by the state variables of species, statistics, status, and moves, without relying on\nany visual information. In Minecraft, Mineflayer [33] provides high-level APIs to access the\nlocal environment state, such as positions, blocks, inventory. The state values are filled into\ndesigned prompt templates to form textual descriptions of game states. However, not all the\ngames support internal APIs, and describing games merely with symbolic states can result in\ninformation loss, especially for games that require detailed visual information to fully capture\nthe gameplay experience, like Red Dead Redemption 2 [34] and StarCraft II [29].\n2. External visual encoder: To solve text-only problem, existing studies equips LLMs with\nexternal visual encoders to translate visual information into textual observations. For example,\nCoELA [35] and LLMPlanner [36] adopt object detectors to recognize objects within the agent’s\nfield of view in embodied environments. The CLIP [37] visual encoder and its variants are\nwidely used for mapping images into pre-defined text descriptions [38; 39; 40]. For example,\nJARVIS-1 [39] uses MineCLIP [41] to select the most similar text description from a set of\n1,000 Minecraft text data entries for images; ELLM [40] adopts ClipCap [42] as the captioner\nfor visual observations: it maps CLIP embedding to a 10-token sequence, which are fed as the\nprefix for GPT-2 to generate the whole caption.\n3. Multimodal LLMs (MLLMs): Visual encoders fall short in generalizability for unseen sce-\nnarios or objects, as they primarily rely on predefined text descriptions for classification. In\ncomparison, MLLMs align visual and textual information in a unified representation space\nand decode them into natural languages, thereby enabling better generalizability across un-\n3\nknown scenarios. General-purpose MLLMs like GPT-4V [3] are adopted in the game-playing of\nRDR2 [34], Doom [43], Minecraft [44] and simulated Embodied household [45] to directly per-\nceive visual observations for decision-making or generating text data as the perception module,\nbut usually need error correction mechanisms [45; 34] with feedback from the environments\nto address inaccuracies; Game-specific MLLMs involve supervised learning on multimodal\ninstruction data generated by experts, such as GATO [46] and SteveEye [47], or learned from\nenvironmental feedback through RL such as Octopus [45].\nIn summary, for video games, accessing symbolic states requires the support of internal APIs. External\nvisual encoders suffer from limited generalizability, as they cannot fully cover all scenarios or objects,\nespecially those without predefined textual descriptions. Although general-purpose MLLMs address\nabove-mentioned issues, they are still insufficient for distinguish fine-grained details like the relative\npositions of target objects, and struggles to understand game-specific concepts [34]. Grounding and\ndisciplining MLLMs with game experience and feedback [45] is a promising way to enable better\nperception and understanding for games.\nPerception\nText game\nZork Series [27; 48], ScienceWorld [49],\nALFworld [50], Diplomacy [51],\nWereworlf [28], Avalone [52], etc.\nVideo game\nState variable access\nStarCraft II [29], Pokémon Battles [30],\nOvercooked [31], Poker [53; 54],\nChess [55], Civilization [56], etc.\nVisual encoder\nELLM [40], LLMPlanner [36], CoELA [35],\nMineCLIP [41], Jarvis-1 [39], etc.\nMLLMs\nSteveEye [47], Octopus [45], GCC [34],\nPlayDoom [43], GATO [46], etc.\nFigure 2: Mind map for the perception module.\n2.2\nMemory\nHumans rely on memory systems to memorize prior experiences for recalling, learning, and applying\nknowledge in future scenarios. Similarly, LLMGAs necessitate memory systems to ensure operational\ncoherence and efficiency, serving as a repository for their past observations, thoughts, actions, and\nskills, from which agents retrieve essential information for strategy formulation and decision-making.\nFrom a perspective of cognitive science [57; 5], human’s memory mechanism can be divided as\nworking memory and long-term memory, where working memory stores an agent’s current context,\nand long-term memory stores the agents past experience and thoughts. For LLMGAs, working\nmemory can be regarded as the context of LLMs, and the term \"memory\" here refers to the long-term\nmemory, which acts as an external storage. Long-term memory stores episodic memories [58] such as\nobservation streams [59] and previous game trajectories [28; 60; 61] generated through the perception\nmodule; high-level semantic memories [62] that represents the agents’ understanding of itself and\nthe game world [59; 63], generated through the thinking module; and procedural memories [64],\nwhich represent curated skill stored as code [65; 34] or plans [66; 39].\nRetrieval: As memories accumulate over time, the majority of them distract from decision-making.\nRetrieval serves as an essential role to filter out through and pass the most relevant memories to the\nagent. Memory records are typically stored as key-value pairs. In semantic retrieval, the process\ninvolves calculating the semantic similarity between the representations of a query and the memory\nkeys, and selecting the memory values with the highest similarity to the query object. The query\nobject can be various forms, such as self-instructed questions [59], task-triggered questions [65],\npredefined questions [28], or visual observations [39]. Specifically, in Voyager [65]’s memory system,\nthe keys are program descriptions, and the values are the previously executed successful program\ncodes. In JARVIS-1 [39], the keys are composed of task descriptions paired with observations in\nimages, while the values represent the previously executed plans. Additionally, to simulate the\nhuman forgetting mechanism, Generative Agents [59] take into account recency and importance,\nwhere recency is calculated using an exponential decay function over game hours, and importance is\nevaluated by the LLM to differentiate mundane details from core information.\n4\nMemory\nFunctionality\nEpisodic memory\nGenerativeAgents [59], Xu et\nal. [28], ProAgent [61], etc.\nSemantic memory\nGenerativeAgents [59], Reflexion [63], etc.\nProcedural memory\nVoyager [65], GTIM [66],\nJARVIS-1 [39], Craddle [34], etc.\nRetrieval\nText\nGenerativeAgents [59], Voyager [65],\nGTIM [66], Craddle [34], etc.\nMulti-modal\nJARVIS-1 [39], CoELA [35], HAS [67], etc.\nFigure 3: Mind map for the memory module.\n2.3\nRole-playing\nRole-playing enables agents to assume diverse characters or roles within the game, generating\nbelievable conversations and behaviors appropriate to the given roles. Many games feature role-\nplaying elements [59; 28; 52] where players assume specific roles and engage in game-playing from\nthe perspective of the characters, leading to immersive gaming experiences. Role-playing is also\nimportant for building Non-Player Characters (NPCs) [68] and game assistants [69], as well as for\ngenerating dialogues [70].\nIt has been proved that assigning different personality types can largely influence the generative\nstyle of LLMs [71; 70]. Role-playing can enhance the vividness [72], personalization [73] and\nproficiency [74] of LLMAs, and generating dialogues with affective information makes agents’\nbehavior more believable [75; 76]. For role-playing, the simplest way is to directly insert natural\nlanguage descriptions of a role’s identity, such as character traits, hobbies, occupation and social\nrelationships, as initial memories for the agent [59]. Evaluations show that providing few-shot\ndialogue examples or fine-tuning can further enhance the role-playing performance in conversational\ntasks [70; 77]. Recent advanced approaches such as CharacterLLM [78] build imaginary experience\nfrom characters’ profiles, and fine-tune LLMs with these experiences to enable agents to exhibit\nconsistent personalities and express emotions.\n2.4\nThinking\nThinking is the cognitive process of analyzing and integrating information. In this section, we\nintroduce two primary thinking methods for decision-making: reasoning and planning. Reasoning\ninvolves using deduction, induction, and abduction to generalize observations, derive conclusions,\nand infer explanations. In comparison, planning strategizes decision steps to achieve complex and\nlong-horizon goals.\n2.4.1\nReasoning\nReasoning [79] is a process that starts from observation, factual evidence, and previous thoughts,\nthen progresses through analyzing and synthesizing these elements to deduce specific conclusions\nfrom general principles (deduction), infer general principles from specific instances (induction), or\nformulate the most likely explanations (abduction). Reasoning is fundamental to human cognition,\nenabling individuals to make sense of the world, solve problems, and make informed decisions.\nLLMGAs [80; 56; 43; 30] adopt general-purpose reasoning approaches [81; 82; 80; 83; 84] to analyze\ninformation logically, providing informative insights for decision-making: ReAct [80] introduce\nreasoning to condition the generation of action with few-shot prompting; CoT [81] and Zero CoT [82]\ndecompose the entire thinking process into multiple chained thoughts, enabling the step-by-step\nelucidation of complex problems; SC [84], ToT [83] and GoT [85] follow a multi-path reasoning\nparadigm: SC conducts multiple times independent reasoning and choose the result with the highest\nfrequency as the final output; ToT [83] and GoT [85] organize reasoning paths into tree and graph-like\nstructures to enhance the reasoning ability.\n5\nReflection [63; 38; 86] can be recognized as a special type of reasoning that usually occurs after feed-\nback from previous trials is provided. It involves the agent analyzing its own actions, decisions, and\nthought processes, and considering how these could be improved based on the feedback received. This\nreflective process allows for the adjustment and refinement of strategies and behaviors, contributing to\nlearning and development over time. Specifically, Reflexion [63], DEPS [38], AgentPro [61], ProA-\ngent [87] identify errors and inefficiencies in past failed attempts through self-reflection and reuse the\nthoughts to enhance the performance in subsequent trials. Moreover, RCI [88], Self-Refine [86] and\nGPTLens [89] demonstrates that the feedback can not only come from environments, but also LLMs\nthemselves, and iteratively refine the results by incorporating self-generated reflection.\nIn game playing, Hu et al. [30] discover that when encountering a powerful opponent, CoT can\nintroduce panic feelings, causing the agent to act inconsistently, such as switching to different\nPokémon in consecutive turns. In comparison, SC alleviates the issue by voting for the most consistent\naction; Theory-of-Mind [90; 91] (ToM) thinking involves inferring others’ intentions from a shifted\nperspective, and demonstrate enhancement in imperfect information games like Poker [54] and\nenables LLMGAs recognize partners’ intention for assistance in cooperation games [92]. Moreover,\nreflecting on the surface observations or experiences can provide high-level, abstract thoughts, which\nhelps the agent act more reasonably and believably [59].\n2.4.2\nPlanning\nHumans utilize planning as a strategic tool to address and manage challenging and long-term tasks.\nFor LLMGAs, planning involves the decomposition of a complex task into simpler, executable subtask\nset. Existing planning approaches can be categorized as goal-free planning and goal-conditioned\nplanning, based on whether a predefined goal is necessary for the planning process.\nGoal-free planning: Open-ended games usually do not have prefixed goals for players to achieve.\nGenerating a goal plan saves the agent from being overwhelmed by numerous possible actions.\nExisting goal-free planning approaches [59; 93; 65; 40; 94; 95] primarily instruct LLMs to generate\ngoal plans. Voyager [65], ELLM [40], SPRING [94] and AdaRefiner [95] prompt LLMs with agent’s\nstates such as hunger, inventory, and equipment, and local observations for generating suitable\nnext goals; OMNI [93] prompts LLMs to select interesting and learnable tasks for agent to explore\nthe open-world. In simulation games, a long-term daily plan can effectively prevent incoherent\nbehaviors [59; 30]. Generative Agents [59] utilize a top-down approach for generating a one-day\nplan for human-simulacra agents, starting with a broad initial plan for the day, then breaking it down\ninto more detailed action plans. After planning, agents can choose to either continue with the plan or\nreact to its dynamic local environment.\nGoal-conditioned planning: A goal-conditioned planner recursively translates a goal, task, or\ninstruction into a set of subgoals until it reaches structured actions. Goal-conditioned planning is\nused for addressing long-horizon and complex tasks such as crafting tools [38; 66] or performing\nquests [36; 96; 97]. Existing studies primarily instruct LLMs to generate plans. ZeroShotPlanner [98]\nand LLMPlanner [36] prompts LLMs with zero-shot or few-shot examples for planning; Given\nthe difficulty in generating a correct plan on the first attempt, GITM [66] and JARVIS-1 [39]\nleverage external knowledge, such as item crafting recipes to enhance planning, and also incorporate\nenvironmental feedback such as error messages to refine the initial plan; DEPS [38] introduce error\ncorrection on initial plans by integrating description of the plan execution and self-explanation of\nfeedback when encountering failures; Adapt [96] and SwiftSAGE [97] adaptively decompose tasks\nwith LLMs when encountering execution failures; S-Agents [99], HAS [67] and MindAgents [100]\noperate in a hierarchical cooperation structure in which a LLM planner dispatches sub-tasks to\nmulti-agents for efficient execution.\n2.5\nAction\nAfter humans make decisions to take actions, they control their bodies, such as hands, to execute\nthese actions, translating cognitive decisions into physical movements that interact with the world\naround them. The action module serves as the hands of the LLMGAs, translating language-described\ndecisions into executable actions in the game environment, enabling the agents to interact with and\nalter their surroundings or game states. Different games necessitate different levels of granularity in\nagents’ output actions. For games requiring manipulative control like RDR2 [34], Minecraft [14]\n6\nThinking\nReasoning\nCoT [81], Zero-CoT [81], ReAct [80],\nSC [84], ToM [91], GenerativeAgents [59],\nReflexion [63], DEPS [38], ProA-\ngent [87], RCI [88], Self-Refine [86], etc.\nPlanning\nGoal-free planning\nGenerative Agents [59], Voy-\nager [65], OMNI [93], ELLM [40],\nSPRING [94], AdaRefiner [95], etc.\nGoal-conditioned\nplanning\nZeroShotPlanner [98], LLMPlan-\nner [36], DEPS [38], GTIM [66],\nPlan4MC [101], SwiftSAGE [97],\nAdapt [96], S-Agents [99], etc.\nFigure 4: Mind map for the thinking module.\nand Overcooked [102], a low-level action like keyboard or mouse operation is required. In contrast,\ngames without manipulative control like text adventure games [103; 104], Pokémon battles [30] and\nPoker [53] directly facilitate the execution of high-level actions.\nLLMs typically generate high-level actions rather than low-level actions. Therefore, for games with\nmanipulative control, a translation module is required to translate LLM-generated action into low-\nlevel actions. Existing studies adopt heuristics [92; 105; 66; 33; 59] or RL policies [40; 101; 106] for\ntranslating a high-level action into low-level action sequences. Heuristic-based translation generates\nlow-level movements using path-finding algorithms, along with manipulative actions. For example,\nin Overcooked, given a high-level action \"chop tomato\", the translation module identifies the shortest\npath to the target with a breadth-first search algorithm and identifies a sequence of movements along\nwith the chop action [92; 105]; In Minecraft, the high-level \"approach\" action uses an A∗algorithm\nfor path-finding and executes low-level actions like jump, move and fall in four directions [66; 33];\nIn comparison, RL-based approaches [41; 40; 101; 106] train language-conditioned RL policies that\ntake observations and high-level actions as input to generate low-level actions, rewarded based on the\nsemantic similarity between the goals and the agent’s transitions.\nGames without manipulative control can be divided as parser-based games [27; 107] and choice-\nbased games [30]. Parser-based games require LLMs to generate an action word by word, wherease\nchoice-based games only need LLMs to select from a set of given actions. For parser-based games,\nZeroShotPlanner [98] proposes semantic translation that maps LLM-generated free-form actions\nto the semantically similar, admissible actions; SayCan [108] calculates the probability of each\nadmissible action using the chain rule by multiplying the conditional generation probability of each\nsuccessive string given the previous string.\n2.6\nLearning\nHumans are able to refine their cognitive abilities and acquire knowledge by interacting with the\nphysical world, gaining hands-on experience through direct engagement with their environments.\nSimilarly, an LLMGA’s learning process involves improving its cognitive and game-playing abilities\nover time, based on the experiences and feedback received from the game environment.\nLLMs encode a wealth of semantic knowledge about the world while lack of real experience within\nenvironments, i.e., they are ungrounded [108]. The majority of existing LLMGAs adopt frozen LLMs\nto play games, relying on carefully designed prompts [28; 109] or external knowledge [30; 39; 66].\nIn comparison, enable LLMGAs to learn in environments is crucial, since it closely mirrors the way\nhumans acquire knowledge through interacting with the real world. Existing learning approaches\ncan be divided into three categories: in-context feedback learning, supervised fine-tuning and\nreinforcement learning.\nIn-context feedback learning: Feedback represents a type of evaluation for previous strategies. By\nincluding feedback from environments into context, LLMs are able to iteratively \"reinforce\" strategy\ngeneration without updating weights [63; 65; 66; 30]. Specifically, Reflexion [63] and DEPS [38]\ngenerate self-reflection\/explanation on the feedback like failure signal and reuses the thought for the\nnext trail; Voyager [65] and GTIM [66] iteratively prompt LLMs to re-generate action code with error\nmessages; Hu et al. [30] uses manually generated feedback such as the HP change across consecutive\n7\nturns as evaluation for previous actions; Furthermore, existing works [63; 86; 86] demonstrate\nfeedback cannot only comes from the game environments, but also from LLMs themselves.\nSupervised fine-tuning: Supervised fine-tuning [110; 111] gathers high quality experience to fine-\ntune LLMs, based on the assumption that such experiences encompass environmental knowledge.\nSpecifically, E2WM [110] collects embodied experience in VirtualHome with Monte Carlo Tree\nSearch and random exploration; LLAMARider [111] gathers experience in Minecraft via self-\nreflection with feedback. Both of them demonstrate that fine-tuning on the collected experience\nenhances capability of LLMs on solving tasks within the environment. Moreover, imitation learning-\nbased approaches like GATO [46], LID [112], SwiftSAGE [97] and Octopus [45] fine-tune LMs\nusing expert or oracle trajectories to enhance their performance as policies.\nReinforcement Learning: Existing RL-based LLMGAs can be divided into three categories: (1)\nLLM as actor: GLAM [113] is grounded in the BabyAI-text environment as a policy to select\nnext action (four movements), training through online RL [114]; (2) LLM as planner: Existing\nstudies such as SayCan [108], Plan4MC [101], RL-GPT [106], ELLM [40], follow a hierarchical\nparadigm that integrates fixed LLMs as high-level planners with separate low-level RL policies to\nexecute actions. In comparison, another line of research involves fine-tuning large language model\n(LLM) planners based on rewards received from the environment, such as Octopus [45]; (3) LLM\nas presenter: LMs can be co-trained with RL policies to produce consistent dialogues that reflect\nthe intentions of policy models, especially in communication games such as Diplomacy [51] and\nWerewolf [115]; (4) LLM for reward design: LLMs can directly serve as reward models [116],\nprovide annotations for training reward models [117], or generate and refine reward functions for\nguiding RL agent training [118; 119; 120].\nLearning\nIn-context feed-\nback learning\nReflexion [63], DEPS [38], Voyager [65],\nRCI [88], PokéLLMon [30], etc.\nSupervised\nfine-tuning\nE2WM [110], LlaMARider [111],\nLID [112], SwiftSAGE [97],\nAdaRefiner [95], etc.\nReinforcement\nLearning\nLLM as actor\nGLAM [113], etc.\nLLM as planner\nSayCan [108], ELLM [40],\nPlan4MC [101], RL-GPT [106], etc.\nLLM as presentor\nCicero [51], Thinker [115], etc.\nReward design\nRewardDesign [116], Motif [117],\nMC-Reward [118], Eureka [120], etc.\nFigure 5: Mind map for the learning module.\n3\nLLMGAs in Games\nWe categorize existing studies into six categories based on the main characteristics of the games they\nsupport, including adventure, communication, competition, cooperation, simulation, and crafting &\nexploration. Figure 6 illustrates the core gameplay mechanics associated with its genre:\n• Adventure: Adventure games emphasize story-driven gameplay, where players explore environ-\nments, solve quests and interact with characters and objects to progress the game. Representative\ngames: Zork I [27] and Red Dead Redemption 2 (RDR2) [34].\n• Communication: Communication games revolve through the turns of communication, negotiation,\ndeduction and even deceptions among multiple players. Representative games: Werewolf [28] and\nDiplomacy [51].\n• Competition: Competition games pit players against each other in challenges that test skill or\nstrategy, aiming to outperform others for victory. Representative games: StarCraft II [29] and\nPokémon Battles [30].\n8\nFigure 6: The depiction of six game categories.\n• Cooperation: Cooperation games are designed around players working together towards common\ngoals, emphasizing teamwork, collaborative problem-solving, and shared achievements. Represen-\ntative games: Overcooked [102].\n• Simulation: Simulation games replicate real-world events in detail, allowing players to experience\nand manage scenarios ranging from building a civilization or living another life. Representative\ngames: The Sims [59; 121] and Civilization [56].\n• Crafting & Exploration: Crafting & Exploration games provide open worlds where players gather\nresources, craft items, and exploring within expansive environments, encouraging creativity and\ndiscovery. Representative games: Minecraft [14] and Crafter [122].\nWe summarize existing studies on LLMGAs in Table 1. In this section, we will walk through six game\ncategories, highlighting key findings and methodologies employed in the current research landscape.\n3.1\nAdventure Games\nAdventure games typically progress through storylines or quests. We categorize existing works into\ntwo types based on modality: text-based adventure games and video adventure games.\nText adventure games: A text adventure game provides a text-based environment in which players\nuse text commands to interact with the world, exploring and completing quests. TextWorld [137] is a\ngenerator of synthetic text games [138; 103] with varying difficulty levels by adjusting parameters\nsuch as the numbers of rooms and objects, quest length and complexity; Jericho [104] is a collection\nof 56 human-made games originally designed for human players, covering fictions such as the\nZork series [27; 48] and Hitchhiker’s Guide to the Galaxy [139]; ALFWorld [50] is aligned to\nthe embodied environment ALFRED [140], where agents are requested to accomplish six types\nof household tasks; ScienceWorld [49] simulates a primary school science curriculum, such as\nthermodynamics and electrical circuits. To complete a quest, an agent needs to navigate to specific\nrooms, obtain necessary items, conduct experiments, and analyze the results; BabyAI-Text [113] is\na text extension of BabyAI [141], a procedurally generated minigrid environment where an agent\nnavigates and interacts with objects.\nDue to the lack of graphics, text games rely on the commonsense knowledge as a prior for how\nto interact with the environment. In parser-based text games, generating a three-word sentence\nwith a small vocabulary of size 1,000 leads to 1 billion combinatorial candidates. Pre-trained LMs\nfeaturing human knowledge can effectively narrow down the action space and thus have been widely\nutilized as linguistic priors for guiding RL agents [123; 142; 143; 144]. Recently, LLMGAs are\nemployed to playing text adventure games: Tsai et al.[124] suggest that the game-playing ability of\nGPT-3.5 is on par with state-of-the-art (SOTA) reinforcement learning (RL) approaches[145; 146],\nbut it is incapable of constructing the entire map of a partially-known environment; REACT [80]\nand Reflexion [63] prompt LLMs to generate additional reasoning and reflection to condition the\ngeneration of actions; To solve challenging tasks, Adapt [96] and SwiftSage [97] adopt an LLM\nplanner to decompose complex tasks into subgoals as needed; GLAM [113] leverage online RL to\nground an LLM in BabyAI-text as a policy.\nVideo adventure game: Red Dead Redemption 2 (RDR2) is a 3D action-adventure game in which\nplayers assume the role of an outlaw, and follow the storyline of his life as part of a criminal gang.\nThe game features an important characteristic, i.e., it guides the player what to do next with instant\ninstructions. Cradle [34] is an LLMGA that perceives the game screen, analyzes instructions, generate\naction plans and controls the character through mouse\/keyboard operations using GPT-4V.\n9\nTable 1: Comparison among existing LLMGAs. FT denotes Fine-Tuning.\nStudies\nCategory\nGame\nBase Model\nFT\nModality\nCALM [123]\nAdventure\nJericho\nGPT-2\n✓\nTxt\nCanPlayWell [124]\nAdventure\nZork I\nGPT-3.5\n✗\nTxt\nReAct [80]\nAdventure\nALFWorld\nPaLM\n✗\nTxt\nReflexion [63]\nAdventure\nALFWorld\nGPT-3\n✗\nTxt\nADAPT [96]\nAdventure\nALFWorld\nGPT-3.5\n✗\nTxt\nSwiftSAGE [97]\nAdventure\nScienceWorld\nGPT-4 & T5\n✓\nText\nGLAM [113]\nAdventure\nBabyAI-Text\nFLAN-T5\n✓\nTxt\nCradle [34]\nAdventure\nRDR2\nGPT-4V\n✗\nTxt & Img\nXu et al. [28]\nCommunication\nWerewolf\nGPT-3.5\n✗\nTxt\nXu et al. [125]\nCommunication\nWerewolf\nGPT-4\n✗\nTxt\nThinker [115]\nCommunication\nWerewolf\nChatGLM-6B\n✓\nText\nReCon [52]\nCommunication\nAvalone\nGPT-4\n✗\nTxt\nAvaloneBench [126]\nCommunication\nAvalone\nGPT-3.5\n✗\nTxt\nCodeAct [127]\nCommunication\nAvalone\nGPT-4\n✗\nTxt\nCicero [51]\nCommunication\nDiplomacy\nBART\n✓\nTxt\nWarAgent [109]\nCommunication\nDiplomacy-like\nGPT-4\n✗\nTxt\nCosmoAgent [128]\nCommunication\nDiplomacy-like\nGPT-4\n✗\nTxt\nDEEP [129]\nCommunication\nWord Guess\nGPT-4\n✗\nTxt\nGameEval [130]\nCommunication\nWord Guess\nGPT-4\n✗\nTxt\nPokéLLMon [30]\nCompetition\nPokémon Battles\nGPT-4\n✗\nTxt\nCoS [29]\nCompetition\nStarCraft II\nGPT-3.5\n✗\nTxt\nSwarmBrain [131]\nCompetition\nStarCraft II\nGPT-3.5\n✗\nTxt\nChessGPT [55]\nCompetition\nChess\nRedPajama-3B\n✓\nPGN\nOthelloGPT [132]\nCompetition\nOthello\nGPT\n✓\nPGN\nPokerGPT [53]\nCompetition\nTexas Hold’em\nOPT-1.3B\n✓\nTxt\nGoodPoker [133]\nCompetition\nTexas Hold’em\nGPT-4\n✗\nTxt\nSuspicionAgent [54]\nCompetition\nLeduc Hold’em\nGPT-4\n✗\nTxt\nAgentPro [61]\nCompetition\nLeduc Hold’em\nGPT-4\n✗\nTxt\nLLM-Co [92]\nCooperation\nOvercooked\nGPT-4\n✗\nTxt\nMindAgent [100]\nCooperation\nOvercooked\nGPT-4\n✗\nTxt\nProAgent [87]\nCooperation\nOvercooked\n-\n✗\nTxt\nHLA [105]\nCooperation\nOvercooked\nGPT-3.5&LLaMA2\n✗\nTxt\nS-Agents [99]\nCooperation\nMinecraft\nGPT-4\n✗\nTxt\nHAC [67]\nCooperation\nMinecraft\nGPT-4V\n✗\nTxt & Img\nCoELA [35]\nCooperation\nTDW-T&WAH\nGPT-4\n✗\nTxt & Img\nGenerativeAgents [59]\nHuman Simulation\nSmallVille\nGPT-3.5\n✗\nTxt\nHumanoidAgents [134]\nHuman Simulation\nSocial\nGPT-3.5\n✗\nTxt\nLyfeAgent [121]\nHuman Simulation\nLyfe Game\nGPT-3.5\n✗\nTxt\nAgentSims [135]\nHuman Simulation\nAgentSims\n-\n✗\nTxt\nCivRealm [56]\nCivil. Simulation\nCivilization\n-\n✗\nTxt\nZeroShotPlanner [98]\nEmbodied Simulation\nVirtualHome\nGPT-3\n✗\nTxt\nLLMPlanner [36]\nEmbodied Simulation\nALFRED\nGPT-3\n✗\nTxt & Img\nE2WM [110]\nEmbodied Simulation\nVirtualHome\nLLaMA-13B\n✓\nTxt\nOctopus [45]\nEmbodied Simulation\nBehavior-1K\nCLIP & MPT-7B\n✓\nTxt & Img\nVoyager [65]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt\nDEPS [38]\nCraft & Explore\nMinecraft\nMineCLIP & GPT-4\n✗\nTxt & Img\nGTIM [66]\nCraft & Explore\nMinecraft\nGPT-3.5\n✗\nTxt\nJARVIS-1 [39]\nCraft & Explore\nMinecraft\nMineCLIP & GPT4\n✗\nTxt & Img\nPlan4MC [101]\nCraft & Explore\nMinecraft\nGPT-3.5\n✗\nTxt & Img\nRL-GPT [106]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt & Img\nMineDoJo [41]\nCraft & Explore\nMinecraft\nMineCLIP\n✓\nTxt & Img\nLLaMARider [111]\nCraft & Explore\nMinecraft\nLLaMA-2-70B\n✓\nTxt & Img\nSteveEye [136]\nCraft & Explore\nMinecraft\nCLIP & LLaMA2\n✓\nTxt & Img\nCreativeAgent [44]\nCraft & Explore\nMinecraft\nGPT-4V\n✗\nTxt & Img\nMCReward [44]\nCraft & Explore\nMinecraft\nGPT-4\n✗\nTxt & Img\nELLM [40]\nCraft & Explore\nCrafter\nCodex\n✗\nTxt & Img\nSPRING [94]\nCraft & Explore\nCrafter\nGPT-4\n✗\nTxt & Img\nAdaRefiner [95]\nCraft & Explore\nCrafter\nLLaMA2 & GPT-4\n✓\nTxt & Img\nOMNI [93]\nCraft & Explore\nCrafter\nGPT-3\n✗\nTxt & Img\nPlayDoom [43]\nOthers\nDoom\nGPT-4V\n✗\nTxt & Img\nGATO [46]\nOthers\nAtari\nGATO\n✓\nImg\nMotif [117]\nOthers\nNetHack\nLlaMA-2\n✗\nTxt\n10\n3.2\nCommunication Games\nCommunication (or conversational) games revolve through the turns of communication, negotiation,\ndeduction and deception among multiple players. The challenge of communication games lies in\ninferring others’ intention behind ambiguous or misleading language utterances, and hiding one’s\nown intention if necessary.\nWerewolf: The game pits two groups against each other, i.e., werewolves and non-werewolves\n(villagers, witch, guard and seer), and alternates between night phases, where werewolves secretly\nattack, and day phases, where survivors discuss and vote to eliminate suspects. The witch, guard,\nand seer each possess unique abilities. Xu et al. [28] propose to retrieve and reflect on historical\ncommunications for enhancement, and observe that GPT-3.5 demonstrates strategic behaviors such\nas trust, confrontation, camouflage, and leadership. Xu et al. [125] employ a RL policy to select the\noptimal action from among the diverse actions generated by LLMs, aiming to overcome the LLMs’\nprior preference for specific actions. Wu et al. [115] introduce a RL policy to generate the next\naction by taking as input the reasoning generated by the LLM, and employ another LLM to generate\ndescriptions aligned to the action.\nAvalon: The game progresses through rounds of discussion and voting to decide who participates in\nthe quests. The goal for the good team is to successfully complete quests, while the bad team aims\nto secretly sabotage these quests or identify the role of Merlin, who knows the identities of the bad\nplayers. Light et al. [126] suggest that GPT-3.5 struggles to formulate and execute simple strategies\nand sometimes reveals its own bad identity. Wang et al. [52] introduce a reasoning approach that\ntakes into account first-order and second-order perspective shifts to combat pervasive misinformation.\nTo combat hallucination, Shi et al. [127] propose to generate reasoning substeps in a code format that\nare interpreted as actions subsequently.\nDiplomatic games: Diplomacy is the first diplomatic board game from the 1950s where players\nassume the roles of seven powers striving to conquer Europe during WW1. Each turn is marked\nby private negotiations, trust-building, and tactical coordination among players. Cicero [51] is a\nhuman-level agent in Diplomacy that integrates a RL policy for planning and a BART [147] model\nconditioned on the plan for generating consistent negotiation messages; WarAgent [109] simulates\nthe participating countries, decisions, and consequences in WW I and WW II; CosmoAgent [128]\nmimics the communication, conflicts, and cooperation among various universal civilizations.\nOthers: Studies have demonstrated the game-playing abilities of LLMs in various games, including\nSpyGame (Who is Spy) [130; 129], Ask-Guess [130; 129], Tofu Kingdom [130], and Murder Mystery\nGames [148], known as Jubensha in Chinese.\n3.3\nCompetition Games\nCompetition games, governed by strict rules, challenge agents with varied-level opponents, demanding\nadvanced reasoning and skills. Competition games serves as benchmarks for evaluating reasoning and\nplanning abilities of LLMGAs directly against human players. Reaching human-level performance is\na crucial achievement that highlights the agent’s prowess in complex decision-making and strategic\nimplementation.\nStarCraft II: StarCraft II is a real-time strategy game in which players are tasked with gathering\nresources, building bases, creating armies, and engaging in combats to defeat the opponent. Ma et\nal. [29] introduce TextStarCraft II, a natural language interface that enables LLMs to play StarCraft\nII and Chain-of-Summarization for efficient reasoning and decision-making; SwarmBrain [131]\nintroduce an Overmind Intelligence Matrix for high-level strategic planning and a Swarm ReflexNet\nfor rapid tactical responses. These LLM-based agents exhibit comparable performance against the\ngame’s built-in AI at high difficulty levels.\nPokémon Battle: Pokémon battles are turn-based tactical games, with two players each sending out\none Pokémon and choosing either to attack or switch Pokémon each turn. Hu et al. [30] introduce an\nenvironment that enables LLMs to play Pokémon battles and a human-level agent PokéLLMon that\nconsumes instant feedback to iteratively refine the policy, retrieves external knowledge to combat\nhallucination, and generates consistent actions to alleviate the panic switching problem caused by\nCoT [81] reasoning.\n11\nChess: Feng et al. [55] introduce a large-scale chess gameplay dataset stored in Portable Game\nNotation format [149] and ChessGPT fine-tuned on mixed chess and language datasets to support\nboard state evaluation and chess playing; Toshniwal et al. [150] and Li et al. [132] discover that\nLMs trained to predict next move in chess are capable of tracking the state of the board given a\nmove sequence, i.e., LMs are capable of playing blindfolded. This suggests that LMs do not merely\nmemorize surface statistics but also learn a causal model of the sequence-generating process.\nPoker: In Texas Hold’em, Gupta et al. [151] observe that GPT-4 plays like an advanced yet aggressive\nplayer who raises with a wide range of hands pre-flop, avoids limping, and exhibits unconventional\nplay; PokerGPT [53] demonstrates that OPT-1.3B [152] with supervised fine-tuning and RLHF [2]\ncan achieve comparable performance to a RL-based method Alphaholdem [153] with significantly\nless training cost: 9.5 GPU hours compared to Alphaholdem’s 580 GPU hours. Guo et al. [54]\nand Zhang et al. [61] demonstrate that prompting LLMs to predict opponents’ thoughts, known as\nTheory of Mind [90; 91], results in significant improvements in Texas Hold’em, BlackJack and Leduc\nHold’em [11].\n3.4\nCooperation Games\nCooperation among individuals can enhance the efficiency and effectiveness of task accomplishment.\nThere are primarily three types of cooperative tasks in games: (1) Cooperative cooking [102; 100;\n154] requires agents collaborate to cook and deliver as many dishes as possible within the given time.\nTo prepare an onion soup in Overcooked-AI [102], two agents need to load three onions into a cooker,\nstarting a cooking process that lasts 20 time steps, and transfer the soup to a plate for delivery; (2)\nEmbodied household cooperation [155; 156] requires agents to collaboratively accomplish tasks\nlike transporting as many objects as possible to the goal position in embodied environments with\npartial observation [107; 157]; (3) Cooperative crafting [99; 100] & exploration [67] in Minecraft\ncan be accelerated through cooperation between multiple agents. Existing cooperative game settings\ncan be categorized into decentralized and centralized cooperation.\nDecentralized cooperation: A decentralized structure is a democratic structure (\n) where\nthere is no central task dispatcher. In Overcooked, the ability to infer the partner’s intent and next\naction based on the its historical actions, known as Theory-of-Mind, is crucial to prevent conflicts.\nAgashe et al. [92] show that GPT-4 is able to recognize and offer assistance to partners in need,\nand show robustness in adjusting to different partners. ProAgent [87] introduces a belief correction\nmodule to rectify incorrect beliefs on partners and consistently outperforms RL approaches [114;\n31; 158]. Moreover, HLA [105] integrates a proficient LLM and a lightweight LLM to balance\nefficacy and efficiency in real-time human-agent interaction; In partially-observable embodied\nenvironments, CoELA [35] introduce an efficient communication module to determine what and\nwhen to communicate, exhibiting better performance compare to MCTS-based and rule-based\nplanners on Watch-and-Help [155] and TDW Transport tasks [156].\nCentralized cooperation: In Minecraft, S-agents [99] and MindAgents [100] adopts a centralized\ndispatcher\/planner to decompose a challenging goal into subtasks and dispatches them to agents for\nexecution, forming a hierarchical architecture. HAS [67] introduces an auto-organizing mechanism\nto dynamically adjust key roles and action groups during cooperation, and an intra-communication\nmechanism to ensure efficient collaboration.\n3.5\nSimulation Games\nSimulation games provide simulated environments for real-world events or scenarios, enabling players\nto experience realistic interactions and decision-making in open-ended game playing. Existing studies\ncan be categorized as human & social simulation, civilization simulation and embodied simulation.\nHuman and social simulation: Generative Agents [59] marks the first LLM-based human simulation\nexperiment that leverages LLMs’ prior knowledge to simulate human-like daily life and social\nactivities. Specifically, GPT-3.5 assumes the roles of 25 generative agents with unique persona and\nsocial relationship, residing in a virtual small town. A cognitive architecture is introduced to support\nagents in remembering, retrieving, reflecting, planning, and acting within dynamic environments.\nDuring the two-day simulation, emergent behaviors like exchanging information, forming new\nrelationships and coordinating joint activities are observed.\n12\nOn the basis of Generative Agents, Humanoid Agents [134] further considers the effects of states like\nbasic needs (e.g., hunger, health, and energy), emotions, and closeness in relationships on agents’\nbehavior generation; For other simulation environments, AgentSims [135] is a programmable and\nextendable environment; LyfeGame [121] is an 3D virtual small town in Japan. Three experimental\nscenarios are designed to assess the social behaviors of LLM-based agents, including a murder\nmystery, a high school activity fair, and a patient-in-distress scenario.\nCivilization simulation: CivRealm [56] is a game environment based on Civilization [32], where\neach player governs a civilization simulating the progress of human history. As an open-ended game,\nit features diverse victory conditions, requiring players to strategically develop the economy, military,\ndiplomacy, culture, and technology of their civilizations. Mastaba [56] introduces an advisor and an\nAutoGPT [159]-style worker, where the advisor aids in generating context-specific objectives while\nthe workers handle the execution of these goals through generated actions. Experiments show that\nthe advisor brings an advantage at the early game stage, yet the advantage diminishes as the game\nprogresses.\nEmbodied simulation: In simulated 3D environments, embodied agents perceives their surround-\nings from egocentric perception similar to human and engage with realistic objects to carry out\na wide range of tasks by following instructions like \"Rinse off a mug and place it in the coffee\nmaker\". Existing benchmarks include AI2-THOR [160], Virtual Home [107], ALFRED [140], iGib-\nson [161], Habitat [162], ThreeDWorld [156], Behavior [163] and Behavior-1K [164]. Existing\napproaches [36; 98; 108; 110] primarily adopt LLMs as planners to decompose an instruction into\naction plans. Specifically, ZeroShotPlanner [98] prompts LLMs in zero-shot manner for planning;\nSayCan [108] uses a learned affordance function to assist LLMs in selecting valid actions during\nplanning; LLMPlanner [36] adopts an KNN retriever to select few-shot examples and dynamically\nre-plan based on the observation in the current environment; E2WM [110] fine-tunes an LLM with\nembodied experience collected through action space search and random exploration to enhance the\nunderstanding of the environments.\n3.6\nCrafting & Exploration Games\nMinecraft and Crafter are two game environments that have been widely studied for game agents with\na focus on crafting & exploration. Minecraft [14] is a 3D sandbox game that offer players the great\nfreedom to traverse a world made up of blocky, pixelated landscapes, facilitated by the procedurally\ngenerated worlds. The resouce-based crafting system enables players to transform collected materials\ninto tools, build elaborate structures and complex machines. Crafter [122] is a 2D open-world game\nthat mirrors the survival mode of Minecraft. It challenges players to manage their resources carefully\nto ensure sufficient water, food, and rest, while also defending against threats like zombies. The\ngame’s world is also procedurally generated for the exploration purpose, and it includes 22 tasks for\nplayers to accomplish.\nExisting agents can be divided as goal-conditioned agents that implement the task given an instruction\n(crafting), or autonomous exploration agents that navigate within the open-world based on self-\ndetermined objectives (exploration).\nCrafting: The key challenge in crafting tasks lies in their complexity: agents must gather diverse\nmaterials scattered across the world and understand intricate recipes and the sequential steps involved.\nConsequently, planning is widely employed to address crafting tasks. Existing agent design such\nas DEPS [38], GITM [66], JARVIS-1 [39], Plan4MC [101], RL-GPT [106] and S-agents [99]\nmainly follow a paradigm that adopts LLMs as a planner to decompose the goal into subgoals and\nfurther generate action plans for each sub-goals. Specifically, DEPS introduce error correction on\ninitial plans by integrating description of the plan execution and self-explanation of feedback when\nencountering failures; GITM [66] leverages external knowledges like item crafting\/smelting recipes,\nand is equipped with a long-term memory to maintain common reference plans for encountered\nobjectives; JARVIS-1 [39] chains MineCLIP [41] and an LLM together to perceive multimodal input\nand utilizes a multimodal memory to store experiences; Plan4MC [101] and RL-GPT [106] integrate\nthe LLM planner with a low-level RL policy for action execution; S-agents [99] and HAS [67]\ndispatches subtasks to multiple agents for cooperatively task execution;\nExploration: Navigating through procedurally generated world without specific goals can overwhelm\nagents with numerous possible actions. Previous works leverage curriculum learning [165] to identify\n13\nsuitable tasks while now LLMs can directly act as goal generators. In Minecraft, Voyager [65]\nadopts an automatic curriculum in a self-directed way [166], i.e., it asks LLM to generate goals\nthat adapts to the agent’s current state, inventory, acquired skills, and environment. In Crafter,\nOMNI [93] utilizes LLMs to determine interesting tasks for curriculum design, overcoming the\nprevious challenge of quantifying \"interest\". ELLM [40], SPRING [94] and AdaRefiner [95] prompt\nLLMs to generate goals for agents. Specifically, ELLM [40] queries LLMs for next goals given an\nagent’s current context, and rewards agents for accomplishing those suggestions in the sparse-reward\nsetting; SPRING [94] uses LLMs to summarize useful knowledge from the Crafter paper [122] and\nprogressively prompts the LLM to generate next action; On the basis of ELLM, AdaRefiner [95]\ncascades a learnable lightweight LLM with fixed LLMs for better goal plan generation.\n3.7\nEvaluation\nThe evaluation metrics for game agents vary across different games. In Table 2, we summarize\nthe metrics for several representative games. For games with specific task instructions, such as\nALFWorld, ScienceWorld, BabyAI, RDR2, ALFRED, VirtualHome, and crafting tasks in Minecraft\nand Crafter, the task success rate is usually adopted as the primary metric; for competition games,\nwin rate, game score, and Elo rating are common metrics; for communication games that separate\nplayers into adversarial teams such as Werewolf and Avalone, win rate can be used as the metric.\nFor human\/social simulation experiments, human evaluators are typically recruited to assess the\nbelievability of behaviors exhibited by LLM-based agents.\nTable 2: Evaluation metric used in representative games\nGame\nMetric\nJericho\nGame score [123]\nALFWorld\nTask success rate [80; 96]\nScienceWorld\nTask score [97]\nBabyAI\/BabyAI-Text\nTask success rate [93; 113]\nRDR2\nTask success rate [34]\nWerewolf\nWin rate [115; 28; 125], Voting accuracy [115]\nAvalone\nWin rate [52; 127; 126]\nDiplomacy\nPlayer score [51]\nStarCraft II\nWin rate [29; 131]\nPokémon Battles\nWin rate [30], Battle score [30]\nChess\nElo rating [55], Move score [55]\nPoker\nWin rate [54; 53], # of win\/loss chips [54], mbb\/hand [53]\nOvercooked\nReward value [92; 87], Success rate [100; 105]\nHuman Simulation\nHuman evaluation [59; 134]\nCivilization\nTask success rate [56], Game score [56], # of techs & units [56]\nALFRED\nTask success rate [36]\nVirtualHome\nTask success rate [98; 167], Executability [98; 167]\nMinecraft\nTask success rate [38; 65; 66], Map coverage [65], # of items [65]\nCrafter\nTask success rate [40; 93]\n4\nConclusion and Future Directions\nIn this paper, we conduct a systematic literature review of existing studies on LLMGAs, examining\ntwo primary aspects: (1) Construction of LLMGAs, where we elaborate on six essential components,\nincluding perception, memory, thinking, role-playing, action, and learning; (2) LLMGAs in six game\ncategories, including adventure, communication, competition, cooperation, simulation, and crafting\n& exploration, where we detail the game environments and common strategies adopted by game\nagents associated with each type. Finally, we identify three potential future directions for this new\nresearch field:\nGrounding LLMs in environments: LLMs pre-trained only on text corpus data are not grounded\nin real environments, i.e., they are not really aware of the consequences of their generations on\nphysical process [168]. Consequently, ungrounded LLMs generate inadmissible actions [108], exhibit\n14\na gap between high-level intentions and intricate game control [34], especially in the absence of\nvisual perception abilities [29] and feedback, and thus largely rely on manually-designed prompts.\nExisting efforts have been made toward grounding LLMs through multimodal perception [45], the\nadopt of external affordance functions [108], feedback from environments [30], and experiencing in\nenvironments [110; 40]. However, the current progress in grounding techniques remains limited and\nfall shorts for the requirements of real-world application [169]. Games, serving as controllable and\nsafe environments, are ideal testbeds for developing grounding techniques that can make LLMs more\ninline with sophisticated environments.\nKnowledge discovery through game-playing: Current studies [65; 30; 29] primarily remain at\nthe stage of utilizing the pre-existing knowledge encoded in LLMs for game-playing. Although\nsome studies propose to leverage game-playing experiences [110; 111; 40] to ground and enhance\nLLM-based agents, these agents are still incapable of extracting underlying knowledge below the\nsurfaces of observations and experience. Knowledge discovery is not simply about learning to act\neffectively, but to understand fundamental principles and causal model of gameplay mechanisms just\nlike human. We believe that gameplay mechanisms with complex extrinsic knowledge are essential\ntestbeds for designing such agents, and knowledge discovery via experiencing in environments might\nrepresent a critical step toward the pursuit of AGI.\nAgent society simulation: The social simulation experiment of Generative Agents [59] has demon-\nstrated that LLM-based agents are promising for believable simulacra of human. Emergent human-like\nsocial behaviors are observed like information diffusion, forming new relationship and coordination\nfor social activities, with the support of a novel cognitive architecture. However, as human beings are\nfar more sophisticated, with complicated mental processes, emotional depth, and advanced social\nskills, it would be an intriguing avenue for future research to develop better cognitive architectures and\nmore nuanced simulations of social interactions and cooperation [170] in realistic game environments\nto foster deeper understanding and representation of complex human interactions.\nAcknowledgements\nAuthors would like to thank Yunxiang Yan and Vijayraj Shanmugaraj for their assistance in collecting\npapers. This research is partially sponsored by the NSF CISE grants 2302720, 2312758, 2038029, a\nGTRI PhD Fellowship, an IBM faculty award, and a grant from CISCO Edge AI program.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 基于大型语言模型的智能游戏代理：迈向通用人工智能的探索\n\n## 📌 背景痛点\/本文动机\n随着大型语言模型（LLMs）和其多模态版本（MLLMs）的快速发展，它们在自然语言理解和生成式人工智能方面取得了突破性进展。然而，现有的游戏代理大多基于强化学习（RL），在复杂游戏环境中缺乏类似人类的决策能力。本文旨在探讨如何利用LLMs构建具有人类决策能力的游戏代理，以推动通用人工智能（AGI）的发展。\n\n## 🚀 核心方法\n本文提出了一个统一的框架，用于构建基于LLMs的游戏代理（LLMGAs），并详细介绍了其六个核心功能组件：\n\n1. **感知**：LLMGAs通过多种方式感知游戏环境，包括文本、视觉、声音等，以获取必要的游戏状态信息。\n2. **记忆**：LLMGAs利用记忆系统存储过去的观察、思考和行动，以便在未来的决策中检索和利用。\n3. **思考**：LLMGAs通过推理和规划进行决策，推理包括演绎、归纳和假设推理，规划则将复杂任务分解为可执行的子任务。\n4. **角色扮演**：LLMGAs能够模拟游戏中的不同角色，生成符合角色特征和目标的对话和行为。\n5. **行动**：LLMGAs将生成的文本决策转换为可执行的行动，以与游戏环境进行交互。\n6. **学习**：LLMGAs通过积累游戏经验和环境反馈，不断改进其认知和游戏能力。\n\n## 📈 实验结果\n本文对现有文献进行了综述，涵盖了六种类型的游戏，包括冒险、沟通、竞争、合作、模拟和制作与探索游戏。对于每种类型的游戏，本文分析了技术挑战、支持的游戏环境和常用的优化策略。\n\n## 💬 可借鉴之处\n本文提出的LLMGAs框架为构建具有人类决策能力的游戏代理提供了重要的参考。此外，本文还指出了未来研究的三个方向：\n\n1. **环境中的LLMs接地**：通过多模态感知、外部可用性函数、环境反馈和经验学习，使LLMs更好地适应现实环境。\n2. **通过游戏玩知识发现**：利用游戏机制中的复杂知识，设计能够从观察和经验中提取底层知识的LLM代理。\n3. **代理社会模拟**：在现实游戏环境中开发更好的认知架构，模拟更复杂的社交互动和合作，以更好地理解和表示复杂的人类交互。\n\n## 📚 参考资料\n[1] The Embodied Cognition Hypothesis\n[2] ChatGPT\n[3] GPT-4V\n[4] Gemini\n[5] LLM-based agent (LLMA)\n[6] Pre-existing knowledge\n[7] Real-world learning\n[8] Real-world experience\n[9] Chess\n[10] Poker\n[11] Atari games\n[12] StarCraft II\n[13] Minecraft\n[14] DOTA II\n[15] Reinforcement Learning (RL)\n[16] Behavior-level policy learning\n[17] Cognitive abilities\n[18] Reasoning\n[19] LLMs survey papers\n[20] LLMAs survey papers\n[21] LLMGAs survey papers\n[22] Game development survey papers\n[23] Game agents survey papers\n[24] LLMGAs taxonomy\n[25] Game categories\n[26] Technical challenges\n[27] Game environments\n[28] Optimization strategies\n[29] Future research directions\n[30] LLMGAs in games\n[31] Perception module\n[32] Memory module\n[33] Role-playing module\n[34] Thinking module\n[35] Action module\n[36] Learning module\n[37] Adventure games\n[38] Communication games\n[39] Competition games\n[40] Cooperation games\n[41] Simulation games\n[42] Crafting & exploration games\n[43] Evaluation metrics\n[44] Grounding LLMs in environments\n[45] Knowledge discovery through game-playing\n[46] Agent society simulation","llm_summary_res_status":200}
{"title":"GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data","authors":"Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang","summary":"Multimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is\nexpected to result in slower benchmark saturation, and avoids the illusion of\nemerging abilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision\nLLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption\nbenchmark for evaluating VLLMs, and compare the performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lag behind human performance and struggle\nespecially with text-intensive tasks.","url":"http:\/\/arxiv.org\/abs\/2402.14973v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.14973v4","published":1708636924000,"comment":"Published by Computer Speech & Language\n  (https:\/\/doi.org\/10.1016\/j.csl.2025.101785). Source code and Leaderboard:\n  https:\/\/github.com\/llcresearch\/GenCeption","pdf_text":"GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data\nLele Caoa,b,∗, Valentin Buchnerb,c,∗, Zineb Senaneb,d,e,f, Fangkai Yangd\naMicrosoft Gaming (ABK), Stockholm, Sweden\nbEQT Group (Motherbrain), Stockholm, Sweden\ncChapter Two, Stockholm, Sweden\ndKTH Royal Institute of Technology, Stockholm, Sweden\neT´el´ecom Paris, Palaiseau, France\nfFever Energy, Stockholm, Sweden\nAbstract\nMultimodal Large Language Models (MLLMs) are typically assessed using expensive annotated multimodal bench-\nmarks, which often lag behind the rapidly evolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only unimodal data to measure inter-modality\nsemantic coherence and inversely assesses MLLMs’ tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is expected to result in slower benchmark\nsaturation, and avoids the illusion of emerging abilities. Inspired by the DrawCeption game, GenCeption begins with\na non-textual sample and proceeds through iterative description and generation steps. The semantic drift across iter-\nations is quantified using the GC@T metric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision LLMs (VLLMs). Based on the Gen-\nCeption method, we establish the MMECeption benchmark for evaluating VLLMs, and compare the performance of\nseveral popular VLLMs and human annotators. Our empirical results validate GenCeption’s effectiveness, demonstrat-\ning strong correlations with established VLLM benchmarks. VLLMs still significantly lag behind human performance\nand struggle especially with text-intensive tasks.\nKeywords:\nmultimodal large language model, evaluation, benchmark\n1. Introduction\nLarge Language Models (LLMs) demonstrate exceptional abilities in natural language understanding, reasoning,\nand problem-solving. Multimodal LLMs (MLLMs) enhance these capabilities by incorporating multiple modalities,\nwith the visual modality being predominant and highly commercialized (Achiam et al., 2023; Liu et al., 2023b; Jiang\net al., 2023; Ye et al., 2023). Building on LLMs, MLLMs integrate non-textual modalities, enabling richer interactions\nand broader applications in real-world scenarios. However, there is a lack of comprehensive evaluation methods to\ncompare different MLLM architectures and training approaches (Fu et al., 2023).\nIn response, the community has developed several MLLM benchmarks, as detailed by Xu et al. (2022); Dai et al.\n(2023); Wang et al. (2023); Ye et al. (2023); Li et al. (2023c); Zhao et al. (2023). They primarily focus on the visual\n(i.e., image) and textual input modality due to that VLLMs (Vision LLMs)1 are the most widely used and readily\n∗Equal contribution. Corresponding author: Lele Cao (lelecao@microsoft.com)\nSource code and leaderboard: https:\/\/github.com\/llcresearch\/GenCeption\nThis work was initiated during the author’s tenure at EQT Motherbrain, with significant parts completed independently thereafter. It represents\npersonal research conducted outside the scope of employment responsibilities. Relevant employers have been informed and have provided consent\nfor publication.\n1Vision Large Language Models (VLLMs) are a specialized subclass of Multimodal Large Language Models (MLLMs) designed to integrate\nvisual and textual modalities for tasks such as image captioning, visual question answering, and multimodal reasoning. While VLLMs are generally\ncapable of processing various visual data types, their most common input is images, owing to the abundance of annotated image-text datasets and\nthe maturity of image processing technologies.\nPreprint published by Computer Speech & Language [https:\/\/doi.org\/10.1016\/j.csl.2025.101785]\narXiv:2402.14973v4  [cs.CL]  5 Mar 2025\nPlease write a clear, \nprecise, detailed, and \nconcise description \nof the image ...\nText Prompt: PDesc\nImage \n(or other \nmodality)\nX\nVLLM\n(Vision LLM)\nDescription \nText: Qt\nGenerate an image \naccording to the \nfollowing description:\nImage generation \nprompt (textual): PGen\n(t-1)\n(t)\nIt shows a happy \ndog ...\nIt shows a \nhappy dog ...\nIm age Gener ator  \n(e.g., DALL·E 3)\nX\nGenrated \nImage\n(t)\nReplace           with        \nX\n(t-1)\nX\n(t)\nand start the next \niteration (t+1)\nPGen\n(t)\nGen(        )\nF (         ,          )\nPDesc X\n(t-1)\nFigure 1: An illustration of the t-th iteration in the GenCeption evaluation procedure for VLLMs. Using the image modality as an example, the\nprocess begins with an existing image X(0) sourced from a unimodal image dataset for the first iteration (t=1). The VLLM provides a detailed\ndescription of the image, which is then used by an image generator to produce X(t).\navailable MLLMs on the market. However, these benchmarks face common challenges:\n(1) They predominantly rely on multimodal datasets that demand high-quality annotations, which is costly and\nrestrictive in capturing the evolving capabilities of MLLMs (Fu et al., 2023). This has been shown to result\nin increasing speed in benchmark saturation while contemporary models still struggle on trivial real-world\ntasks (Kiela et al., 2021). Emerging methods like CrossCheckGPT (Sun et al., 2024), designed specifically\nfor MLLM evaluation via cross-system consistency, provide a more relevant, annotation-free alternative. On a\nbroader scope, methods like PRD (Li et al., 2023b) focus on LLM evaluation through peer-based rankings and\nmay be further adapted for MLLM evaluation tasks.\n(2) MLLM evaluation benchmarks that rely on discrete metrics like accuracy may falsely suggest emergent abilities\nand do not allow predictable projections of performance improvements from model scaling (Schaeffer et al.,\n2023).\n(3) The evaluation scores may not reflect true performance on real-world tasks due to potential contamination of\nMLLM training data by benchmark datasets, as reported for LLM pretraining corpora (Dodge et al., 2021; Yang\net al., 2023).\n(4) The content of one modality is often not needed to answer benchmark questions, as the answer can often be\ninferred from the question or the MLLM’s pretraining knowledge.\nAs a consequence of both (3) and (4), some MLLMs can excel on vision QA benchmarks without even being provided\nthe image that is associated with the question. Existing solutions either only tackle a subset of these challenges, or\nfocus on specific tasks such as image captioning (Lee et al., 2024).\nWe propose GenCeption to address all highlighted challenges involved in the evaluation of task-agnostic MLLMs.\nGenCeption is designed to be a general evaluation framework that can be applied across modalities. To validate its\neffectiveness, this paper focuses on Vision LLMs (VLLMs), leveraging the visual modality as an illustrative example.\nGenCeption addresses challenge (1) by relying on easily accessible unimodal datasets, which allows for cost-effective\nand scalable benchmark creation. Relying on unimodal datasets additionally addresses challenge (3) and (4), as it\nallows to easily use previously unseen datasets for MLLM evaluation, and enforces the relevance of the provided\nmodality for excelling at this task. To tackle challenge (2), GenCeption uses the continuous GC@T metric, providing\na more nuanced evaluation compared to discrete metrics, allowing for better projections of performance improvements\nand avoiding the mirage of emergent abilities.\nOn a high and general level, GenCeption assesses MLLMs’ ability to consistently maintain semantic coherence\nacross modalities by iteratively generating and describing non-textual samples and measures the continuous GC@T\nmetric. This approach simultaneously evaluates the MLLM’s tendency to hallucinate, as this inversely correlates with\nsemantic coherence. Further, an MLLM’s ability to provide complete yet concise descriptions of non-textual samples\nmeasures a diverse range of specialized abilities. For instance, to perform well at describing an image using a limited\n2\nnumber of tokens, it is advantageous to be able to reason over people’s emotions and intentions behind their actions,\ninfer the current and preceding weather, count objects, and recognize artistic styles. This list can be extended to\nvarious abilities depending on the non-textual modality and the content of the samples used during the GenCeption\nprocess. The main contributions of this paper are the following:\n• Proposing GenCeption, an evaluation method that principally allows for using unlabeled unimodal datasets for\nMLLM evaluation.\n• Introducing MMECeption, a Vision LLM (VLLM) evaluation benchmark utilizing the GenCeption method.\nMMECeption uses the images from the MME benchmark (Fu et al., 2023), but without their annotated question\nand answer pairs.\n• Evaluating seven leading VLLMs on the MMECeption benchmark and comparing results with other popular\nVLLM benchmarks and human performance.\nWe will elaborate on the proposed implementation of the GenCeption method, detail our experimental setup, and\ndiscuss our findings.\n2. GenCeption\nOur approach, GenCeption, is inspired by a multi-player game DrawCeption2 (a.k.a., Scrawl or Whispernary). In\nthis game, the first player is given an image and describes it verbally to the next player. This player then attempts to\nrecreate the image based on the description. The cycle repeats, often resulting in amusing deviations from the original\nimage. The challenge and objective are to maintain the initial information through iterative transitions between verbal\ndescriptions and drawings. Similarly, a proficient Multimodal Language Model (MLLM), which models multiple\nmodalities such as text and images, should excel at this task. Recognizing that MLLMs can encompass modalities\nbeyond just visual cues, such as audio and graphs, we name our approach GenCeption, covering a broader scope\nthan the visually-centric DrawCeption. For the sake of clarity and alignment with our experiments, we will focus on\nVLLMs in the remainder of this section to walk through the GenCeption approach.\nWhile it may not be possible to preserve the initial information perfectly due to varying levels of richness, accuracy,\nand ambiguity in different modalities, a more capable MLLM will minimize the semantic drift from the original\ninput. This contrasts with common benchmarks that aim for complete saturation, highlighting a key advantage of\nthe GenCeption framework: the creation of benchmarks that are more challenging to saturate. With complex initial\nsamples, such as images of real-world scenes or graphs with numerous nodes and edges, this may even result in\nimpossible-to-saturate benchmarks. Aiming for minimum rather than no semantic drift, this would allow to rank\nMLLMs relative to each other while continuously leaving space for more performant models.\n2.1. Procedure\nUnlike existing MLLM benchmarks (often focused on VLLMs) that rely on multimodal samples, GenCeption\nis designed to operate on unimodal datasets, significantly streamlining dataset acquisition efforts. For illustrative\npurposes, we employ the image modality as a representative non-textual modality throughout this exposition. Let\nus consider an image dataset D comprising images X1, X2, . . . , XN, similar to well-established datasets like Ima-\ngeNet (Deng et al., 2009), CIFAR (Krizhevsky et al., 2009), and STL (Coates et al., 2011). Without loss of generality,\nany image from D is denoted as X.\nGenCeption operates iteratively, from t = 1 to a pre-defined maximum iteration t = T. Each iteration, as depicted\nin Figure 1, begins with an image X(t−1) and yields a new image X(t). The first iteration (t = 1) starts with the original\nimage X(0) from D. During any given iteration t, the VLLM receives a textual prompt PDesc (Table 1), instructing the\nVLLM to generate a comprehensive description Qt for the input image X(t−1):\nQt := F(PDesc, X(t−1)), where F denotes the generation function of any VLLMs.\n(1)\n2https:\/\/wikipedia.org\/wiki\/drawception\n3\nPlease write a clear, precise, detailed, and concise description of all elements in the image. Focus on accurately depicting\nvarious aspects, including but not limited to the colors, shapes, positions, styles, texts and the relationships between dif-\nferent objects and subjects in the image. Your description should be thorough enough to guide a professional in recreating\nthis image solely based on your textual representation. Remember, only include descriptive texts that directly pertain to\nthe contents of the image. You must complete the description using less than 500 words.\nTable 1: The fixed textual prompt PDesc instructs the MLLM to produce a description of the input X(t−1).\nFollowing this, an image generation prompt P(t)\nGen is constructed as “Generate an image that fully and precisely reflects this\ndescription: < Qt >”. This prompt guides a pretrained image generation model, such as DALL·E (Ramesh et al., 2021)\nor Imagen (DeepMind, 2023), to create a new image, X(t):\nX(t) := Gen(P(t)\nGen),\n(2)\nwhere Gen(·) signifies the chosen image generator. Each subsequent iteration t+1 starts with the image X(t) generated\nin the previous iteration. Upon completing all iterations, we obtain a series of T + 1 images: X(0), X(1), . . . , X(T), with\nthe initial image being the original and the rest sequentially produced across the iterations.\nThe textual prompt PDesc is intentionally kept short and concise to minimize potential variations in model be-\nhaviours due to susceptibility to prompt composition (Loya et al., 2023).\n2.2. Metric: GC@T\nOur primary objective is to measure the semantic divergence of each generated image X(t) (for t ∈{1, 2, . . . , T})\nfrom the original image X(0). We use a pretrained image encoder, such as ViT (Dosovitskiy et al., 2021), to transform\nall images, resulting in T +1 image embeddings denoted as z(0), z(1), . . . , z(T), where z(t) := Enc(X(t)). We then compute\nthe cosine similarity between z(0) and each z(t) (for t ∈{1, 2, . . . , T}), yielding T similarity scores: s(1), s(2), . . . , s(T).\nHere, s(t) ∈[−1.0, 1.0] approximates the level of semantic drift in the t-th iteration of the GenCeption procedure.\nTo quantify the overall speed and magnitude of semantic drift, we calculate the GenCeption score over T iterations,\ndenoted as GC@T ∈[−1.0, 1.0], as follows:\nGC@T :=\nPT\nt=1(t · s(t))\nPT\nt=1 t\n.\n(3)\nThis is a normalized and continuous metric that weights later iterations more heavily for two reasons: (1) similar\nto the DrawCeption game, the last image’s deviation from the initial image is most telling; (2) we aim to capture\nperformance and dynamics across the entire iterative sequence. A high GC@T value signifies an exceptional ability\nto maintain inter-modal (text-image) semantic congruence, effectively curbing the propensity for rapid or extensive\ndeviation from the semantics encapsulated in the original image. Notably, GC@1 is equivalent to s(1). For the pseudo\ncode detailing the GenCeption procedure and the calculation of the average GC@T metric over the entire dataset D,\nsee Algorithm 1.\nFor the special case of VLLMs that are evaluated in this study, we additionally replace using ViT embeddings and\ncosine similarity with the Frechet Inception Distance (FID), a metric commonly used to evaluate image generation\nmodels (Heusel et al., 2017). The FID is calculated between the original dataset of images D(0), and the images\ngenerated from the respective dataset using the GenCeption process D(t), yielding T FID scores: fid(1), fid(2), . . . , fid(T).\nThe GCFID@T score is then calculated as:\nGCFID@T :=\nPT\nt=1(t · fid(t))\nPT\nt=1 t\n.\n(4)\nAs the FID indicates a distance rather than a similarity between two sets of images, a lower distance indicates\nbetter performance, and consequently a lower GCFID@T score indicates a more capable VLLM.\n4\nAlgorithm 1: Calculate GC@T via GenCeption for a specific VLLM under evaluation\nInput: VLLM to be evaluated, a unimodal dataset D: X(0)\n1 , . . . , X(0)\nn , . . . , X(0)\nN , fixed textual prompt PDesc, a\nsample generator Gen(·), and a sample encoder Enc(·)\nOutput: Average GC@T metric over D\nParameter: The number of iterations T\n1: GC@T = 0\n2: for (n = 1; n ≤N; n + +) do\n3:\nz(0) := Enc(X(0)\nn );\n4:\nfor (t = 1; t ≤T; t + +) do\n5:\nGenerate description Qt for X(t−1)\nn\nusing (1);\n6:\nCreate P(t)\nGen using Qt;\n7:\nGenerate X(t)\nn according to (2);\n8:\ns(t) := CosineSimilarity(z(0), Enc(X(t)\nn ));\n9:\nend\n10:\nCalculate GC@T += PT\nt=1(t · s(t))\/ PT\nt=1 t; (3)\n11: end\n12: return GC@T \/ N;\n3. Experiments\nWe run several extensive experiments to validate the GenCeption method by comparing the GC@T scores achieved\nby several VLLMs to the scores they achieve on carefully crafted established benchmarks and to average human per-\nformance. Although GenCeption’s design merely requires unimodal image datasets, we employ the same data as\nused by a recent and well-validated MLLM benchmark, MME (Fu et al., 2023). While we discard the annotated\nquestion-answer pairs associated with the images in this benchmark, this provides us with the ability (1) to facilitate\ndirect comparison with metrics that include textual QA (question-answering) annotations, and (2) to enable a detailed\nassessment of MLLM performance across MME’s 14 meticulously crafted sample categories. Attributing this newly\ncreated benchmark to the MME dataset and the GenCeption method, we refer to it as the MMECeption benchmark.\nWe select seven VLLMs – Gemini1.5-Pro (Reid et al., 2024), GPT-4o (OpenAI, 2024) , GPT-4V (Achiam et al.,\n2023), Claude3-Opus (Anthropic, 2023), LLaVA-7B\/13B (Liu et al., 2023b) and mPLUG-Owl2 (Ye et al., 2023)\n– based on their superior performance on the OpenCompass multimodal leaderboard (OpenCompass, 2023), which\nincorporates a comprehensive set of benchmarks like MME (Fu et al., 2023), HallusionBench (Liu et al., 2023a),\nMMStar (Chen et al., 2024), SeedBench (Li et al., 2023a), and AI2D (Kembhavi et al., 2016). We use DALL·E as the\ndefault image generation model. To prevent potential bias towards OpenAI-developed VLLMs, which might have had\naccess to DALL·E-generated images during their training, we perform an additional evaluation of all VLLMs on the\nGC@1 score using Imagen2 as an image generation model. We set the temperature parameter to 0 in both the VLLMs\nand image generators to minimize the stochasticity in model outputs.\nAs humans are well versed at integrating vision and language modalities, we aim to quantify average human\nperformance on the MMECeption benchmark. As the GenCeption procedure is a labor-intensive and time-consuming\ntask for humans, we randomly select 5 images from each MME category, and by providing human annotators with\nthe same prompts as defined in Table 1, collect results and calculate the GC@1 metric. Five human annotators (3\nmaster students, 1 lecturer, and 1 artist) were recruited to describe one image of each category such that each image\nin a category is described by a different person to mitigate personal performance differences. The annotators were\ngiven 14 weeks to perform this task and were awarded a generous reimbursement of €40 each to ensure sufficient\ndedication. All annotators were either native English speakers or fluent at a professional level.\n3.1. Quantitative results\nWe partition the 14 MME categories into two groups based on content type: visual-intensive (10 categories: ex-\nistence, count, position, color, poster, celebrity, scene, landmark, artwork, and commonsense reasoning) and textual-\nintensive (4 categories: code reasoning, numerical calculation, text translation, and OCR). GC@3 scores on the\n5\nSample group\n& category\nGemini1.5-Pro\nClaude3-Opus\nGPT-4o\nGPT-4V\nmPLUG-Owl2\nLLaVA-13B\nLLaVA-7B\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nvisual-intensive samples\nExistence\n190.0\n0.437\n269.8\n183.3\n0.382\n273.7\n195.0\n0.400\n266.5\n175.0\n0.422\n265.1\n185.0\n0.323\n296.9\n195.0\n0.305\n322.0\n195.0\n0.308\n318.1\nCount\n148.3\n0.389\n272.6\n116.7\n0.348\n285.3\n190.0\n0.388\n277.5\n153.3\n0.404\n277.4\n160.0\n0.299\n316.1\n165.0\n0.294\n319.7\n148.3\n0.253\n326.1\nPosition\n105.0\n0.357\n253.7\n76.7\n0.357\n266.1\n145.0\n0.398\n260.6\n85.0\n0.408\n253.3\n75.0\n0.306\n294.0\n135.0\n0.255\n298.9\n123.3\n0.285\n286.0\nColor\n175.0\n0.474\n234.8\n118.3\n0.385\n267.6\n180.0\n0.421\n246.1\n141.7\n0.403\n243.7\n138.3\n0.290\n310.1\n165.0\n0.300\n305.6\n170.0\n0.284\n304.5\nPoster\n175.2\n0.374\n206.0\n149.7\n0.360\n206.0\n192.2\n0.335\n203.7\n187.8\n0.324\n209.4\n154.8\n0.243\n209.2\n163.6\n0.215\n240.9\n154.1\n0.214\n244.3\nCelebrity\n169.4\n0.362\n191.0\n77.6\n0.317\n192.5\n46.8\n0.331\n193.3\n53.5\n0.332\n189.1\n167.9\n0.232\n211.3\n144.4\n0.206\n223.7\n153.2\n0.188\n233.6\nScene\n147.0\n0.423\n173.7\n149.8\n0.374\n174.7\n148.5\n0.401\n171.5\n141.2\n0.393\n173.7\n157.8\n0.299\n194.4\n162.8\n0.277\n198.0\n160.8\n0.266\n196.3\nLandmark\n176.8\n0.375\n182.1\n113.0\n0.344\n188.9\n175.5\n0.372\n182.0\n104.0\n0.353\n182.6\n158.8\n0.275\n206.0\n150.8\n0.242\n224.3\n154.8\n0.252\n214.4\nArtwork\n152.2\n0.412\n171.1\n136.8\n0.385\n170.4\n144.0\n0.415\n169.2\n115.0\n0.421\n170.3\n136.0\n0.252\n202.2\n98.8\n0.212\n213.3\n110.0\n0.210\n215.3\nCommonsense\n150.0\n0.464\n216.5\n115.0\n0.432\n210.4\n174.3\n0.448\n213.9\n155.0\n0.471\n208.1\n127.9\n0.353\n237.2\n115.7\n0.334\n248.9\n117.1\n0.294\n254.4\nVisual Mean\n158.9\n0.407\n217.1\n123.7\n0.368\n223.6\n159.1\n0.391\n218.4\n131.2\n0.393\n217.3\n146.2\n0.287\n247.7\n149.7\n0.264\n259.5\n148.7\n0.255\n259.3\nVisual Rank\n2\n1\n1\n7\n4\n4\n1\n3\n3\n6\n2\n2\n5\n5\n5\n3\n6\n7\n5\n7\n6\ntextual-intensive\nCode reasoning\n117.5\n0.213\n310.0\n70.0\n0.245\n267.4\n182.5\n0.255\n299.7\n147.5\n0.193\n302.9\n65.0\n0.176\n327.6\n55.0\n0.144\n323.5\n50.0\n0.107\n398.2\nNumerical calc.\n110.0\n0.268\n346.5\n67.5\n0.229\n349.3\n170.0\n0.282\n346.4\n80.0\n0.240\n322.5\n45.0\n0.192\n362.0\n35.0\n0.195\n367.4\n50.0\n0.155\n366.0\nText translation\n162.5\n0.240\n334.6\n45.0\n0.236\n362.5\n192.5\n0.211\n326.9\n55.0\n0.157\n368.0\n112.5\n0.081\n365.2\n85.0\n0.116\n352.3\n65.0\n0.111\n424.4\nOCR\n170.0\n0.367\n233.2\n167.5\n0.362\n245.5\n192.5\n0.362\n246.2\n177.5\n0.393\n238.0\n102.5\n0.276\n255.4\n95.0\n0.239\n270.6\n65.0\n0.222\n283.7\nTextual Mean\n140.0\n0.272\n306.1\n87.5\n0.268\n306.2\n184.4\n0.278\n304.8\n115.0\n0.246\n307.9\n81.3\n0.181\n327.6\n67.5\n0.174\n328.5\n57.5\n0.149\n368.1\nTextual Rank\n2\n2\n2\n4\n3\n3\n1\n1\n1\n3\n4\n4\n5\n5\n5\n6\n6\n6\n7\n7\n7\nOverall Mean\n153.5\n0.368\n242.5\n113.3\n0.340\n247.2\n166.3\n0.359\n243.1\n126.5\n0.351\n243.2\n127.6\n0.257\n270.5\n126.1\n0.238\n279.2\n122.6\n0.225\n290.1\nOverall Rank\n2\n1\n1\n7\n4\n4\n1\n2\n2\n4\n3\n3\n3\n5\n5\n5\n6\n6\n6\n7\n7\nHallusionBench*\n45.2 (rank=3)\n37.8 (rank=4)\n51.7 (rank=1)\n46.5 (rank=2)\n25.7 (rank=6)\n24.5 (rank=7)\n27.6 (rank=5)\nMMStar*\n38.6 (rank=5)\n45.7 (rank=3)\n61.6 (rank=1)\n47.7 (rank=2)\n34.8 (rank=6)\n40.1 (rank=4)\n34.6 (rank=7)\nSEEDBench (Test)*\n70.7 (rank=3)\n64.0 (rank=7)\n76.4 (rank=1)\n71.6 (rank=2)\n64.5 (rank=6)\n67.9 (rank=4)\n66.4 (rank=5)\nAI2D*\n70.2 (rank=4)\n70.6 (rank=3)\n82.2 (rank=1)\n75.5 (rank=2)\n55.7 (rank=7)\n61.3 (rank=5)\n55.9 (rank=6)\nOpenCompass*\n62.7 (rank=3)\n57.7 (rank=4)\n66.3 (rank=1)\n63.3 (rank=2)\n46.3 (rank=7)\n48.8 (rank=5)\n46.7 (rank=6)\n* Results are sourced from https:\/\/huggingface.co\/spaces\/opencompass\/open_vlm_leaderboard as of 2024-04-25 (except GPT-4o) and 2024-05-23 (for GPT-4o).\n↓Results are obtained using FID to measure the similarity between images. https:\/\/github.com\/GaParmar\/clean-fid\nTable 2: Evaluation results of GC@3, MME, HallusionBench and OpenCompass on visual(Vis)-intensive and textual(Text)-intensive images. Best results per metric and category (over different\nMLLMs) are bolded.\n6\nFigure 2: Correlation Matrix of GC@1 and GC@3 scores on MMECeption, and several other benchmarks.\nMMECeption benchmark and accuracy on the original MME benchmark are reported per category and as aggregations\nin Table 2. Additionally, we include the scores and ranks of all evaluated VLLMs on the OpenCompass (OpenCom-\npass, 2023), MME (Fu et al., 2023), HallusionBench (Liu et al., 2023a), MMStar (Chen et al., 2024), SeedBench (Li\net al., 2023a), and AI2D (Kembhavi et al., 2016) leaderboards. Notably, Gemini1.5-Pro leads our rankings, followed\nby GPT-4o, GPT-4v, Claude3-Opus, mPLUG-Owl2, and LLaVA-13B\/7B. The GenCeption method shows robustness\nto the similarity metric used, as the overall ranking remains identical when using cosine similarity or FID distance for\ncalculating GC@T scores.\nGC@1\nGC@3\nMME\nHallusionBench\nGC@1 (essentially the same as s(1))\n-\n0.99\n0.46\n0.92\nGC@3\n0.99\n-\n0.52\n0.93\ns(3) (GC@3 without weighting by t)\n0.98\n0.99\n0.46\n0.94\nCrossCheckGPT (Image-to-text)\n0.96\n0.94\n0.42\n0.97\nTable 3: Correlation matrix comparing GC@1, GC@3, s(3) (GC@3 without temporal weighting), and CrossCheckGPT with established bench-\nmarks MME and HallusionBench.\nFigure 2 presents a correlation matrix among GC@T scores and the benchmarks mentioned above, where the\noverall GC@T scores are averaged over the GC@T scores of all MME categories. The strong correlations with the\nOpenCompass scores incorporating the results of multiple meticulously crafted benchmarks indicate that MMECep-\ntion provides a comprehensive evaluation that may complement existing benchmarks. Further, GenCeption appears\nto effectively measure a VLLM’s tendency to hallucinate, as demonstrated by the strong correlations with Hallu-\nsionBench. While these observations are further emphasized by the correlations with MMStar and AI2D, the only\nmoderate correlations with MME and SEEDBench provide more nuanced insights. As MME displays these mod-\nerate correlations also with the other benchmarks, it can be reasoned that it measures dimensions supplement to\nthose measured by other benchmarks and GenCeption. SEEDBench on the other hand correlates strongly with other\nbenchmarks, but only moderately with GC@T scores. This indicates that SEEDBench measures aspects that are also\nmeasured by other benchmarks, but fail to be captured by GenCeption. Future research could focus on identifying\nthese aspects to potentially incorporate them into GenCeption.\nOne of the key strengths of GenCeption lies in its annotation-free evaluation methodology, a concept also reflected\nin emerging evaluation methods such as CrossCheckGPT Sun et al. (2024). CrossCheckGPT ranks hallucinations\nby evaluating the consistency of outputs across independent MLLM models. In 3, we analyze the correlation of\nCrossCheckGPT with GenCeption, MME, and HallusionBench scores. The results show strong correlations between\nCrossCheckGPT and both GenCeption and HallusionBench, affirming its capability to capture key evaluative met-\n7\nSample group\n& category\nGemini1.5-Pro\nClaude3-Opus\nGPT-4o\nGPT-4V\nmPLUG-Owl2\nLLaVA-13B\nLLaVA-7B\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nvisual-intensive samples\nExistence\n0.505\n0.529\n0.500\n0.532\n0.536\n0.521\n0.505\n0.530\n0.427\n0.515\n0.416\n0.485\n0.418\n0.506\nCount\n0.456\n0.489\n0.466\n0.490\n0.456\n0.494\n0.498\n0.506\n0.378\n0.463\n0.408\n0.466\n0.341\n0.416\nPosition\n0.511\n0.491\n0.495\n0.480\n0.469\n0.460\n0.501\n0.473\n0.346\n0.452\n0.359\n0.454\n0.350\n0.402\nColor\n0.545\n0.525\n0.489\n0.501\n0.480\n0.473\n0.506\n0.490\n0.345\n0.471\n0.420\n0.457\n0.318\n0.436\nPoster\n0.455\n0.388\n0.450\n0.381\n0.445\n0.383\n0.444\n0.365\n0.338\n0.357\n0.303\n0.312\n0.305\n0.266\nCelebrity\n0.417\n0.384\n0.424\n0.382\n0.418\n0.373\n0.433\n0.389\n0.319\n0.336\n0.284\n0.317\n0.263\n0.313\nScene\n0.511\n0.490\n0.504\n0.478\n0.482\n0.474\n0.497\n0.474\n0.385\n0.417\n0.355\n0.404\n0.350\n0.392\nLandmark\n0.500\n0.485\n0.460\n0.492\n0.494\n0.479\n0.458\n0.480\n0.363\n0.351\n0.376\n0.357\n0.334\n0.333\nArtwork\n0.494\n0.454\n0.508\n0.461\n0.500\n0.455\n0.504\n0.455\n0.333\n0.385\n0.308\n0.333\n0.294\n0.304\nCommon.\n0.545\n0.531\n0.535\n0.507\n0.562\n0.526\n0.563\n0.535\n0.425\n0.493\n0.429\n0.473\n0.417\n0.458\nVis Mean\n0.494\n0.477\n0.483\n0.470\n0.484\n0.464\n0.491\n0.470\n0.366\n0.424\n0.366\n0.406\n0.339\n0.383\nVis Rank\n1\n1\n4\n2\n3\n4\n2\n2\n5\n5\n5\n6\n7\n7\ntextual-intensive\nCode\n0.364\n0.177\n0.304\n0.180\n0.395\n0.179\n0.333\n0.263\n0.281\n0.100\n0.260\n0.168\n0.186\n0.108\nNumerical\n0.322\n0.417\n0.333\n0.389\n0.366\n0.456\n0.325\n0.383\n0.322\n0.225\n0.336\n0.265\n0.259\n0.222\nText trans.\n0.396\n0.227\n0.356\n0.258\n0.444\n0.277\n0.359\n0.238\n0.173\n0.052\n0.200\n0.118\n0.212\n0.073\nOCR\n0.462\n0.500\n0.486\n0.448\n0.421\n0.441\n0.482\n0.417\n0.358\n0.384\n0.368\n0.385\n0.351\n0.320\nText Mean\n0.386\n0.330\n0.370\n0.319\n0.407\n0.338\n0.375\n0.325\n0.284\n0.190\n0.291\n0.234\n0.252\n0.181\nText Rank\n2\n2\n4\n4\n1\n1\n3\n3\n6\n6\n5\n5\n7\n7\nOverall Mean\n0.463\n0.435\n0.451\n0.427\n0.462\n0.428\n0.458\n0.428\n0.343\n0.357\n0.344\n0.357\n0.314\n0.325\nOverall Rank\n1\n1\n4\n4\n2\n2\n3\n2\n6\n5\n5\n5\n7\n7\nTable 4: The impact of different image encoders, DALL·E 3 (Dalle3) vs. Imagen 2 (Imgn2), on GC@1 score. Best results per configuration and\ncategory (over different VLLMs) are bolded.\nSample group\n& category\nGemini1.5\n-Pro\nClaude3\n-Opus\nGPT-4o\nGPT-4V\nmPLUG\n-Owl2\nLLaVA\n-13B\nLLaVA\n-7B\nHuman\n∆% between\nhuman & best\nvisual-intensive samples\nExistence\n0.5841\n0.4563\n0.5578\n0.5434\n0.3967\n0.3524\n0.3782\n0.6402\n+ 9.6045%\nCount\n0.4140\n0.3799\n0.2725\n0.4882\n0.2364\n0.3535\n0.2038\n0.5476\n+ 12.1671%\nPosition\n0.5546\n0.4959\n0.4086\n0.5639\n0.3527\n0.4285\n0.3899\n0.6409\n+ 13.6549%\nColor\n0.7081\n0.6206\n0.6139\n0.5516\n0.4047\n0.4314\n0.3506\n0.8380\n+ 18.3449%\nPoster\n0.5046\n0.4362\n0.4939\n0.4681\n0.3998\n0.3208\n0.2905\n0.5456\n+ 8.1252%\nCelebrity\n0.4182\n0.3988\n0.4369\n0.4447\n0.3714\n0.2545\n0.2160\n0.4671\n+ 5.0371%\nScene\n0.6080\n0.5828\n0.5229\n0.5919\n0.4842\n0.3906\n0.4057\n0.6236\n+ 2.5658%\nLandmark\n0.4903\n0.4932\n0.5236\n0.5702\n0.3613\n0.4174\n0.3845\n0.6045\n+ 6.0154%\nArtwork\n0.3725\n0.5304\n0.5297\n0.5252\n0.2938\n0.2924\n0.2336\n0.5421\n+ 2.2059%\nCommonsense\n0.4338\n0.5375\n0.5047\n0.4012\n0.3244\n0.4153\n0.3532\n0.6417\n+ 19.3860%\nVisual Mean\n0.5088\n0.4932\n0.4865\n0.5148\n0.3625\n0.3657\n0.3206\n0.6091\n+ 9.7107%\ntext-intensive\nCode reasoning\n0.3689\n0.4085\n0.4043\n0.3690\n0.2923\n0.2963\n0.1975\n0.5376\n+ 31.6034%\nNumerical calc.\n0.3652\n0.3958\n0.3940\n0.4241\n0.3474\n0.4409\n0.3423\n0.5160\n+ 17.0333%\nText translation\n0.4480\n0.3949\n0.4333\n0.3803\n0.0931\n0.2372\n0.1981\n0.6196\n+ 38.3036%\nOCR\n0.4382\n0.4329\n0.3334\n0.4455\n0.2663\n0.3371\n0.2912\n0.4696\n+ 5.4097%\nTextual Mean\n0.4051\n0.4080\n0.3913\n0.4047\n0.2498\n0.3279\n0.2573\n0.5357\n+ 23.0875%\nOverall Mean\n0.4792\n0.4688\n0.4593\n0.4834\n0.3303\n0.3549\n0.3025\n0.5882\n+ 13.5327%\nTable 5: The performance of VLLMs and humans on the GC@1 metric, evaluated using 5 randomly drawn images per sample\/image category. The\nbest performance achieved by an VLLM is underlined.\nrics. Notably, CrossCheckGPT exhibits a weaker correlation with MME, which is likely because the GenCeption\nbenchmark is developed using MME image samples, making it inherently more aligned with the MME framework.\nGC@T scores, as defined in Equation (3), are weighted by a temporal factor t. To examine the impact of this\n8\nweighting, we conducted an ablation study where the weighting mechanism was removed, effectively transforming\nGC@T into s(T). Table 3 demonstrates that s(3) retains a high correlation with GC@3, yet its correlation with MME\ndiminishes compared to the weighted version, while its alignment with HallusionBench remains consistent. Further-\nmore, unweighted scores correlate with MME in a uniform manner across different iterations, whereas the weighted\nscores show a progressive increase in correlation with MME as more iterations are applied. This indicates that tem-\nporal weighting amplifies later iterations’ influence, emphasizing cumulative semantic shifts captured by MME’s\niterative design. Generally, stronger correlation with MME is desirable as it validates the alignment between GenCep-\ntion’s metrics and an established benchmark, reinforcing GenCeption’s ability to assess iterative semantic coherence\neffectively and reliably.\nTable 4 compares GC@1 scores using different image generators, OpenAI’s DALL·E 3 (Ramesh et al., 2021)\nand Google DeepMind’s Imagen2 (DeepMind, 2023). Independent of image generator used, the rankings remain\nunchanged, except that on visual-intensive samples only, Claude3-Opus scores equally with GPT-4V. This provides\nevidence that even though DALL·E 3, GPT-4o, and GPT-4V were developed and trained by OpenAI, neither of\nOpenAI’s models has an advantage over non-OpenAI VLLMs.\nTable 5 shows human performance on a subset of 5 randomly drawn images per category compared to the VLLM\nperformance on the same subset of samples. It can be observed that humans outperform all VLLMs, with especially\nstrong differences in performance for the text-intensive categories. The worst performance, relative to humans, is\nachieved on the code reasoning and text translation categories, the former containing images of code snippets and the\nlatter of phrases written in simplified Chinese characters. The relatively best performance by VLLMs is achieved on\nthe scene and artwork categories, which contain every-day life photos and popular artworks. This demonstrates that\nthere is still substantial space for performance improvement, and that compared to humans, VLLMs still lack relevant\ncompetences. It must be noted that human performance here does not constitute an upper bound in possible scores to\nachieve, and that future generations of VLLMs may well outperform humans.\n3.2. Qualitative Results\nWe qualitatively inspect our results by visualizing generated images together with their cosine similarity and\nGC@T scores for two seed images across different categories, as shown in Figure 3. This visualization highlights\nthe correlation between these scores and the visual characteristics of the images relative to the seed image. A key\nobservation is that later iterations show an increased tendency to produce imagery deviating from the seed image, as\nindicated by lower GC@T scores. This serves as an additional qualitative validation of the GenCeption method and\nthe MMECeption benchmark, as using VLLMs scoring higher on the MMECeption benchmark results in generated\nimages that preserve more information from the seed image. For a wider range of examples across MME image\ncategories and corresponding descriptions from each evaluated VLLM, readers are referred to Appendix A.\n4. Discussion and Future Directions\nThis study validates the GenCeption method with a focus on the visual modality primarily because (1) VLLMs\nare the most widely used and readily available MLLMs on the market, and (2) image generation and embedding\ntools have reached a mature and highly commercialized stage compared to other modalities. However, GenCeption\nis designed to be modality-agnostic. The same iterative procedure (i.e., describing a unimodal sample and then re-\ngenerating it from the description) can, in principle, be applied to other non-text modalities like audio and video.\nThe requirement is that (1) a generation model exists for the given modality, and (2) there is a suitable encoder to\nquantify the similarity between the original sample and the regenerated one. Moreover, recent advancements have\nintroduced multimodal LLMs capable of both generating and interpreting multiple modalities simultaneously, such as\nShow-o (Xie et al., 2024), Emu3 (Wang et al., 2024), and JanusPro (Chen et al., 2025). In these cases, GenCeption\ncould leverage the same MLLM for both description and generation tasks, serving as a particularly valuable approach\nfor directly measuring modality consistency within such unified multimodal systems.\nFuture research is invited to adapt GenCeption to other non-text modalities, such as audio, video, and graphs. For\ninstance, the framework can be initiated with a dataset of audio samples, and MLLMs can iteratively generate and\ndescribe the audio content. Similarly, for video and graph data, the process can involve generating textual descriptions\nof short video clips or graph structures and their recreation. While the core iterative process of GenCeption remains\napplicable, these extensions require careful exploration of modality-specific generation and embedding models.\n9\nSeed Im age\n0.569\n0.362\n0.271\n0.334\n0.057\n0.123\n0.266\n0.194\n0.100\nGC@3=0.351\nGC@1=0.569\nGC@3=0.136\nGC@1=0.334\nGC@3=0.159\nGC@1=0.266\nt=1\nt=2\nt=3\nfrom \n\" Color \"  \ncategory\n0.384\n0.158\n0.260\nGC@3=0.247\nGC@1=0.384\n0.707\n0.484\n0.400\nGC@3=0.477\nGC@1=0.707\n0.428\n0.299\n0.351\nGC@3=0.347\nGC@1=0.428\nGC@3=0.286\nGC@1=0.426\n0.426\n0.312\n0.222\nt=1\nt=2\nt=3\nGC@3=0.296\nGC@1=0.224\n0.224\n0.272\n0.336\nGC@3=0.246\nGC@1=0.430\n0.430\n0.276\n0.164\nSeed Im age\nfrom \n\" OCR\"  \ncategory\nGC@3=0.264\nGC@1=0.382\n0.382\n0.202\n0.266\nGC@3=0.383\nGC@1=0.375\n0.375\n0.394\n0.379\nGC@3=0.276\nGC@1=0.373\n0.373\n0.266\n0.251\nFigure 3: Demonstration of GenCeption evaluation procedure: the images generated over 3 GenCeption iterations for several MLLMs. The\nsimilarity s(t) scores (to the seed image) are shown on the top of images; GC@1 and GC@3 scores are printed on the bottom of the first and third\nimage, respectively.\nThe broad skill assessment provided by GenCeption goes along with the limitation that it is difficult to assess\nwhich skills contribute most to a high GC@T score. Our analysis indicates that contemporary VLLMs perform\npoorly on text-intensive tasks while excelling in describing scenes and artworks. Future research could investigate\nthis in a more fine-grained manner by creating datasets requiring specialized skills. For example, datasets could\ninclude images of complex emotions, dynamic movements, mechanical processes, or user interfaces. Additionally,\ncombining GenCeption with specifically designed similarity metrics may offer more detailed insights into specific\nMLLM abilities.\n5. Conclusion\nIn this paper, we introduce GenCeption to enhance the evaluation of rapidly evolving Multimodal Language\nModels (MLLMs). The GenCeption method attempts to address key limitations of existing MLLM benchmarks,\nsuch as costly data annotation, leading questions, the illusion of emergent abilities, and, as it allows to use newly\ncreated images without annotation, training data contamination. Further, it is expected to result in slower benchmark\nsaturation. Being adaptable to different modalities, the GenCeption method can deliver value as a unified MLLM\nevaluation method that complements existing MLLM benchmarks.\nOur empirical validation using the MMECeption benchmark shows that GenCeption effectively assesses semantic\ncoherence and consistency across modalities, aligning with established VLLM benchmarks. By assessing humans on\nthe MMECeption task, we demonstrate that current VLLMs significantly lag behind human performance, particularly\n10\nwhen working with text-intensive images. Future work is encouraged to refine and extend this framework across a\nwider range of modalities, datasets, and similarity metrics.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nGenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data\n```\n#### 2. 论文摘要\n```\nMultimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is\nexpected to result in slower benchmark saturation, and avoids the illusion of\nemerging abilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision\nLLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption\nbenchmark for evaluating VLLMs, and compare the performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lag behind human performance and struggle\nespecially with text-intensive tasks.\n```\n\n#### 3. 论文全文\n```\nGenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data\nLele Caoa,b,∗, Valentin Buchnerb,c,∗, Zineb Senaneb,d,e,f, Fangkai Yangd\naMicrosoft Gaming (ABK), Stockholm, Sweden\nbEQT Group (Motherbrain), Stockholm, Sweden\ncChapter Two, Stockholm, Sweden\ndKTH Royal Institute of Technology, Stockholm, Sweden\neT´el´ecom Paris, Palaiseau, France\nfFever Energy, Stockholm, Sweden\nAbstract\nMultimodal Large Language Models (MLLMs) are typically assessed using expensive annotated multimodal bench-\nmarks, which often lag behind the rapidly evolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only unimodal data to measure inter-modality\nsemantic coherence and inversely assesses MLLMs’ tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is expected to result in slower benchmark\nsaturation, and avoids the illusion of emerging abilities. Inspired by the DrawCeption game, GenCeption begins with\na non-textual sample and proceeds through iterative description and generation steps. The semantic drift across iter-\nations is quantified using the GC@T metric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision LLMs (VLLMs). Based on the Gen-\nCeption method, we establish the MMECeption benchmark for evaluating VLLMs, and compare the performance of\nseveral popular VLLMs and human annotators. Our empirical results validate GenCeption’s effectiveness, demonstrat-\ning strong correlations with established VLLM benchmarks. VLLMs still significantly lag behind human performance\nand struggle especially with text-intensive tasks.\nKeywords:\nmultimodal large language model, evaluation, benchmark\n1. Introduction\nLarge Language Models (LLMs) demonstrate exceptional abilities in natural language understanding, reasoning,\nand problem-solving. Multimodal LLMs (MLLMs) enhance these capabilities by incorporating multiple modalities,\nwith the visual modality being predominant and highly commercialized (Achiam et al., 2023; Liu et al., 2023b; Jiang\net al., 2023; Ye et al., 2023). Building on LLMs, MLLMs integrate non-textual modalities, enabling richer interactions\nand broader applications in real-world scenarios. However, there is a lack of comprehensive evaluation methods to\ncompare different MLLM architectures and training approaches (Fu et al., 2023).\nIn response, the community has developed several MLLM benchmarks, as detailed by Xu et al. (2022); Dai et al.\n(2023); Wang et al. (2023); Ye et al. (2023); Li et al. (2023c); Zhao et al. (2023). They primarily focus on the visual\n(i.e., image) and textual input modality due to that VLLMs (Vision LLMs)1 are the most widely used and readily\n∗Equal contribution. Corresponding author: Lele Cao (lelecao@microsoft.com)\nSource code and leaderboard: https:\/\/github.com\/llcresearch\/GenCeption\nThis work was initiated during the author’s tenure at EQT Motherbrain, with significant parts completed independently thereafter. It represents\npersonal research conducted outside the scope of employment responsibilities. Relevant employers have been informed and have provided consent\nfor publication.\n1Vision Large Language Models (VLLMs) are a specialized subclass of Multimodal Large Language Models (MLLMs) designed to integrate\nvisual and textual modalities for tasks such as image captioning, visual question answering, and multimodal reasoning. While VLLMs are generally\ncapable of processing various visual data types, their most common input is images, owing to the abundance of annotated image-text datasets and\nthe maturity of image processing technologies.\nPreprint published by Computer Speech & Language [https:\/\/doi.org\/10.1016\/j.csl.2025.101785]\narXiv:2402.14973v4  [cs.CL]  5 Mar 2025\nPlease write a clear, \nprecise, detailed, and \nconcise description \nof the image ...\nText Prompt: PDesc\nImage \n(or other \nmodality)\nX\nVLLM\n(Vision LLM)\nDescription \nText: Qt\nGenerate an image \naccording to the \nfollowing description:\nImage generation \nprompt (textual): PGen\n(t-1)\n(t)\nIt shows a happy \ndog ...\nIt shows a \nhappy dog ...\nIm age Gener ator  \n(e.g., DALL·E 3)\nX\nGenrated \nImage\n(t)\nReplace           with        \nX\n(t-1)\nX\n(t)\nand start the next \niteration (t+1)\nPGen\n(t)\nGen(        )\nF (         ,          )\nPDesc X\n(t-1)\nFigure 1: An illustration of the t-th iteration in the GenCeption evaluation procedure for VLLMs. Using the image modality as an example, the\nprocess begins with an existing image X(0) sourced from a unimodal image dataset for the first iteration (t=1). The VLLM provides a detailed\ndescription of the image, which is then used by an image generator to produce X(t).\navailable MLLMs on the market. However, these benchmarks face common challenges:\n(1) They predominantly rely on multimodal datasets that demand high-quality annotations, which is costly and\nrestrictive in capturing the evolving capabilities of MLLMs (Fu et al., 2023). This has been shown to result\nin increasing speed in benchmark saturation while contemporary models still struggle on trivial real-world\ntasks (Kiela et al., 2021). Emerging methods like CrossCheckGPT (Sun et al., 2024), designed specifically\nfor MLLM evaluation via cross-system consistency, provide a more relevant, annotation-free alternative. On a\nbroader scope, methods like PRD (Li et al., 2023b) focus on LLM evaluation through peer-based rankings and\nmay be further adapted for MLLM evaluation tasks.\n(2) MLLM evaluation benchmarks that rely on discrete metrics like accuracy may falsely suggest emergent abilities\nand do not allow predictable projections of performance improvements from model scaling (Schaeffer et al.,\n2023).\n(3) The evaluation scores may not reflect true performance on real-world tasks due to potential contamination of\nMLLM training data by benchmark datasets, as reported for LLM pretraining corpora (Dodge et al., 2021; Yang\net al., 2023).\n(4) The content of one modality is often not needed to answer benchmark questions, as the answer can often be\ninferred from the question or the MLLM’s pretraining knowledge.\nAs a consequence of both (3) and (4), some MLLMs can excel on vision QA benchmarks without even being provided\nthe image that is associated with the question. Existing solutions either only tackle a subset of these challenges, or\nfocus on specific tasks such as image captioning (Lee et al., 2024).\nWe propose GenCeption to address all highlighted challenges involved in the evaluation of task-agnostic MLLMs.\nGenCeption is designed to be a general evaluation framework that can be applied across modalities. To validate its\neffectiveness, this paper focuses on Vision LLMs (VLLMs), leveraging the visual modality as an illustrative example.\nGenCeption addresses challenge (1) by relying on easily accessible unimodal datasets, which allows for cost-effective\nand scalable benchmark creation. Relying on unimodal datasets additionally addresses challenge (3) and (4), as it\nallows to easily use previously unseen datasets for MLLM evaluation, and enforces the relevance of the provided\nmodality for excelling at this task. To tackle challenge (2), GenCeption uses the continuous GC@T metric, providing\na more nuanced evaluation compared to discrete metrics, allowing for better projections of performance improvements\nand avoiding the mirage of emergent abilities.\nOn a high and general level, GenCeption assesses MLLMs’ ability to consistently maintain semantic coherence\nacross modalities by iteratively generating and describing non-textual samples and measures the continuous GC@T\nmetric. This approach simultaneously evaluates the MLLM’s tendency to hallucinate, as this inversely correlates with\nsemantic coherence. Further, an MLLM’s ability to provide complete yet concise descriptions of non-textual samples\nmeasures a diverse range of specialized abilities. For instance, to perform well at describing an image using a limited\n2\nnumber of tokens, it is advantageous to be able to reason over people’s emotions and intentions behind their actions,\ninfer the current and preceding weather, count objects, and recognize artistic styles. This list can be extended to\nvarious abilities depending on the non-textual modality and the content of the samples used during the GenCeption\nprocess. The main contributions of this paper are the following:\n• Proposing GenCeption, an evaluation method that principally allows for using unlabeled unimodal datasets for\nMLLM evaluation.\n• Introducing MMECeption, a Vision LLM (VLLM) evaluation benchmark utilizing the GenCeption method.\nMMECeption uses the images from the MME benchmark (Fu et al., 2023), but without their annotated question\nand answer pairs.\n• Evaluating seven leading VLLMs on the MMECeption benchmark and comparing results with other popular\nVLLM benchmarks and human performance.\nWe will elaborate on the proposed implementation of the GenCeption method, detail our experimental setup, and\ndiscuss our findings.\n2. GenCeption\nOur approach, GenCeption, is inspired by a multi-player game DrawCeption2 (a.k.a., Scrawl or Whispernary). In\nthis game, the first player is given an image and describes it verbally to the next player. This player then attempts to\nrecreate the image based on the description. The cycle repeats, often resulting in amusing deviations from the original\nimage. The challenge and objective are to maintain the initial information through iterative transitions between verbal\ndescriptions and drawings. Similarly, a proficient Multimodal Language Model (MLLM), which models multiple\nmodalities such as text and images, should excel at this task. Recognizing that MLLMs can encompass modalities\nbeyond just visual cues, such as audio and graphs, we name our approach GenCeption, covering a broader scope\nthan the visually-centric DrawCeption. For the sake of clarity and alignment with our experiments, we will focus on\nVLLMs in the remainder of this section to walk through the GenCeption approach.\nWhile it may not be possible to preserve the initial information perfectly due to varying levels of richness, accuracy,\nand ambiguity in different modalities, a more capable MLLM will minimize the semantic drift from the original\ninput. This contrasts with common benchmarks that aim for complete saturation, highlighting a key advantage of\nthe GenCeption framework: the creation of benchmarks that are more challenging to saturate. With complex initial\nsamples, such as images of real-world scenes or graphs with numerous nodes and edges, this may even result in\nimpossible-to-saturate benchmarks. Aiming for minimum rather than no semantic drift, this would allow to rank\nMLLMs relative to each other while continuously leaving space for more performant models.\n2.1. Procedure\nUnlike existing MLLM benchmarks (often focused on VLLMs) that rely on multimodal samples, GenCeption\nis designed to operate on unimodal datasets, significantly streamlining dataset acquisition efforts. For illustrative\npurposes, we employ the image modality as a representative non-textual modality throughout this exposition. Let\nus consider an image dataset D comprising images X1, X2, . . . , XN, similar to well-established datasets like Ima-\ngeNet (Deng et al., 2009), CIFAR (Krizhevsky et al., 2009), and STL (Coates et al., 2011). Without loss of generality,\nany image from D is denoted as X.\nGenCeption operates iteratively, from t = 1 to a pre-defined maximum iteration t = T. Each iteration, as depicted\nin Figure 1, begins with an image X(t−1) and yields a new image X(t). The first iteration (t = 1) starts with the original\nimage X(0) from D. During any given iteration t, the VLLM receives a textual prompt PDesc (Table 1), instructing the\nVLLM to generate a comprehensive description Qt for the input image X(t−1):\nQt := F(PDesc, X(t−1)), where F denotes the generation function of any VLLMs.\n(1)\n2https:\/\/wikipedia.org\/wiki\/drawception\n3\nPlease write a clear, precise, detailed, and concise description of all elements in the image. Focus on accurately depicting\nvarious aspects, including but not limited to the colors, shapes, positions, styles, texts and the relationships between dif-\nferent objects and subjects in the image. Your description should be thorough enough to guide a professional in recreating\nthis image solely based on your textual representation. Remember, only include descriptive texts that directly pertain to\nthe contents of the image. You must complete the description using less than 500 words.\nTable 1: The fixed textual prompt PDesc instructs the MLLM to produce a description of the input X(t−1).\nFollowing this, an image generation prompt P(t)\nGen is constructed as “Generate an image that fully and precisely reflects this\ndescription: < Qt >”. This prompt guides a pretrained image generation model, such as DALL·E (Ramesh et al., 2021)\nor Imagen (DeepMind, 2023), to create a new image, X(t):\nX(t) := Gen(P(t)\nGen),\n(2)\nwhere Gen(·) signifies the chosen image generator. Each subsequent iteration t+1 starts with the image X(t) generated\nin the previous iteration. Upon completing all iterations, we obtain a series of T + 1 images: X(0), X(1), . . . , X(T), with\nthe initial image being the original and the rest sequentially produced across the iterations.\nThe textual prompt PDesc is intentionally kept short and concise to minimize potential variations in model be-\nhaviours due to susceptibility to prompt composition (Loya et al., 2023).\n2.2. Metric: GC@T\nOur primary objective is to measure the semantic divergence of each generated image X(t) (for t ∈{1, 2, . . . , T})\nfrom the original image X(0). We use a pretrained image encoder, such as ViT (Dosovitskiy et al., 2021), to transform\nall images, resulting in T +1 image embeddings denoted as z(0), z(1), . . . , z(T), where z(t) := Enc(X(t)). We then compute\nthe cosine similarity between z(0) and each z(t) (for t ∈{1, 2, . . . , T}), yielding T similarity scores: s(1), s(2), . . . , s(T).\nHere, s(t) ∈[−1.0, 1.0] approximates the level of semantic drift in the t-th iteration of the GenCeption procedure.\nTo quantify the overall speed and magnitude of semantic drift, we calculate the GenCeption score over T iterations,\ndenoted as GC@T ∈[−1.0, 1.0], as follows:\nGC@T :=\nPT\nt=1(t · s(t))\nPT\nt=1 t\n.\n(3)\nThis is a normalized and continuous metric that weights later iterations more heavily for two reasons: (1) similar\nto the DrawCeption game, the last image’s deviation from the initial image is most telling; (2) we aim to capture\nperformance and dynamics across the entire iterative sequence. A high GC@T value signifies an exceptional ability\nto maintain inter-modal (text-image) semantic congruence, effectively curbing the propensity for rapid or extensive\ndeviation from the semantics encapsulated in the original image. Notably, GC@1 is equivalent to s(1). For the pseudo\ncode detailing the GenCeption procedure and the calculation of the average GC@T metric over the entire dataset D,\nsee Algorithm 1.\nFor the special case of VLLMs that are evaluated in this study, we additionally replace using ViT embeddings and\ncosine similarity with the Frechet Inception Distance (FID), a metric commonly used to evaluate image generation\nmodels (Heusel et al., 2017). The FID is calculated between the original dataset of images D(0), and the images\ngenerated from the respective dataset using the GenCeption process D(t), yielding T FID scores: fid(1), fid(2), . . . , fid(T).\nThe GCFID@T score is then calculated as:\nGCFID@T :=\nPT\nt=1(t · fid(t))\nPT\nt=1 t\n.\n(4)\nAs the FID indicates a distance rather than a similarity between two sets of images, a lower distance indicates\nbetter performance, and consequently a lower GCFID@T score indicates a more capable VLLM.\n4\nAlgorithm 1: Calculate GC@T via GenCeption for a specific VLLM under evaluation\nInput: VLLM to be evaluated, a unimodal dataset D: X(0)\n1 , . . . , X(0)\nn , . . . , X(0)\nN , fixed textual prompt PDesc, a\nsample generator Gen(·), and a sample encoder Enc(·)\nOutput: Average GC@T metric over D\nParameter: The number of iterations T\n1: GC@T = 0\n2: for (n = 1; n ≤N; n + +) do\n3:\nz(0) := Enc(X(0)\nn );\n4:\nfor (t = 1; t ≤T; t + +) do\n5:\nGenerate description Qt for X(t−1)\nn\nusing (1);\n6:\nCreate P(t)\nGen using Qt;\n7:\nGenerate X(t)\nn according to (2);\n8:\ns(t) := CosineSimilarity(z(0), Enc(X(t)\nn ));\n9:\nend\n10:\nCalculate GC@T += PT\nt=1(t · s(t))\/ PT\nt=1 t; (3)\n11: end\n12: return GC@T \/ N;\n3. Experiments\nWe run several extensive experiments to validate the GenCeption method by comparing the GC@T scores achieved\nby several VLLMs to the scores they achieve on carefully crafted established benchmarks and to average human per-\nformance. Although GenCeption’s design merely requires unimodal image datasets, we employ the same data as\nused by a recent and well-validated MLLM benchmark, MME (Fu et al., 2023). While we discard the annotated\nquestion-answer pairs associated with the images in this benchmark, this provides us with the ability (1) to facilitate\ndirect comparison with metrics that include textual QA (question-answering) annotations, and (2) to enable a detailed\nassessment of MLLM performance across MME’s 14 meticulously crafted sample categories. Attributing this newly\ncreated benchmark to the MME dataset and the GenCeption method, we refer to it as the MMECeption benchmark.\nWe select seven VLLMs – Gemini1.5-Pro (Reid et al., 2024), GPT-4o (OpenAI, 2024) , GPT-4V (Achiam et al.,\n2023), Claude3-Opus (Anthropic, 2023), LLaVA-7B\/13B (Liu et al., 2023b) and mPLUG-Owl2 (Ye et al., 2023)\n– based on their superior performance on the OpenCompass multimodal leaderboard (OpenCompass, 2023), which\nincorporates a comprehensive set of benchmarks like MME (Fu et al., 2023), HallusionBench (Liu et al., 2023a),\nMMStar (Chen et al., 2024), SeedBench (Li et al., 2023a), and AI2D (Kembhavi et al., 2016). We use DALL·E as the\ndefault image generation model. To prevent potential bias towards OpenAI-developed VLLMs, which might have had\naccess to DALL·E-generated images during their training, we perform an additional evaluation of all VLLMs on the\nGC@1 score using Imagen2 as an image generation model. We set the temperature parameter to 0 in both the VLLMs\nand image generators to minimize the stochasticity in model outputs.\nAs humans are well versed at integrating vision and language modalities, we aim to quantify average human\nperformance on the MMECeption benchmark. As the GenCeption procedure is a labor-intensive and time-consuming\ntask for humans, we randomly select 5 images from each MME category, and by providing human annotators with\nthe same prompts as defined in Table 1, collect results and calculate the GC@1 metric. Five human annotators (3\nmaster students, 1 lecturer, and 1 artist) were recruited to describe one image of each category such that each image\nin a category is described by a different person to mitigate personal performance differences. The annotators were\ngiven 14 weeks to perform this task and were awarded a generous reimbursement of €40 each to ensure sufficient\ndedication. All annotators were either native English speakers or fluent at a professional level.\n3.1. Quantitative results\nWe partition the 14 MME categories into two groups based on content type: visual-intensive (10 categories: ex-\nistence, count, position, color, poster, celebrity, scene, landmark, artwork, and commonsense reasoning) and textual-\nintensive (4 categories: code reasoning, numerical calculation, text translation, and OCR). GC@3 scores on the\n5\nSample group\n& category\nGemini1.5-Pro\nClaude3-Opus\nGPT-4o\nGPT-4V\nmPLUG-Owl2\nLLaVA-13B\nLLaVA-7B\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nMME↑\nPACC\nGC@3↑\nCosine\nGC@3↓\nFID\nvisual-intensive samples\nExistence\n190.0\n0.437\n269.8\n183.3\n0.382\n273.7\n195.0\n0.400\n266.5\n175.0\n0.422\n265.1\n185.0\n0.323\n296.9\n195.0\n0.305\n322.0\n195.0\n0.308\n318.1\nCount\n148.3\n0.389\n272.6\n116.7\n0.348\n285.3\n190.0\n0.388\n277.5\n153.3\n0.404\n277.4\n160.0\n0.299\n316.1\n165.0\n0.294\n319.7\n148.3\n0.253\n326.1\nPosition\n105.0\n0.357\n253.7\n76.7\n0.357\n266.1\n145.0\n0.398\n260.6\n85.0\n0.408\n253.3\n75.0\n0.306\n294.0\n135.0\n0.255\n298.9\n123.3\n0.285\n286.0\nColor\n175.0\n0.474\n234.8\n118.3\n0.385\n267.6\n180.0\n0.421\n246.1\n141.7\n0.403\n243.7\n138.3\n0.290\n310.1\n165.0\n0.300\n305.6\n170.0\n0.284\n304.5\nPoster\n175.2\n0.374\n206.0\n149.7\n0.360\n206.0\n192.2\n0.335\n203.7\n187.8\n0.324\n209.4\n154.8\n0.243\n209.2\n163.6\n0.215\n240.9\n154.1\n0.214\n244.3\nCelebrity\n169.4\n0.362\n191.0\n77.6\n0.317\n192.5\n46.8\n0.331\n193.3\n53.5\n0.332\n189.1\n167.9\n0.232\n211.3\n144.4\n0.206\n223.7\n153.2\n0.188\n233.6\nScene\n147.0\n0.423\n173.7\n149.8\n0.374\n174.7\n148.5\n0.401\n171.5\n141.2\n0.393\n173.7\n157.8\n0.299\n194.4\n162.8\n0.277\n198.0\n160.8\n0.266\n196.3\nLandmark\n176.8\n0.375\n182.1\n113.0\n0.344\n188.9\n175.5\n0.372\n182.0\n104.0\n0.353\n182.6\n158.8\n0.275\n206.0\n150.8\n0.242\n224.3\n154.8\n0.252\n214.4\nArtwork\n152.2\n0.412\n171.1\n136.8\n0.385\n170.4\n144.0\n0.415\n169.2\n115.0\n0.421\n170.3\n136.0\n0.252\n202.2\n98.8\n0.212\n213.3\n110.0\n0.210\n215.3\nCommonsense\n150.0\n0.464\n216.5\n115.0\n0.432\n210.4\n174.3\n0.448\n213.9\n155.0\n0.471\n208.1\n127.9\n0.353\n237.2\n115.7\n0.334\n248.9\n117.1\n0.294\n254.4\nVisual Mean\n158.9\n0.407\n217.1\n123.7\n0.368\n223.6\n159.1\n0.391\n218.4\n131.2\n0.393\n217.3\n146.2\n0.287\n247.7\n149.7\n0.264\n259.5\n148.7\n0.255\n259.3\nVisual Rank\n2\n1\n1\n7\n4\n4\n1\n3\n3\n6\n2\n2\n5\n5\n5\n3\n6\n7\n5\n7\n6\ntextual-intensive\nCode reasoning\n117.5\n0.213\n310.0\n70.0\n0.245\n267.4\n182.5\n0.255\n299.7\n147.5\n0.193\n302.9\n65.0\n0.176\n327.6\n55.0\n0.144\n323.5\n50.0\n0.107\n398.2\nNumerical calc.\n110.0\n0.268\n346.5\n67.5\n0.229\n349.3\n170.0\n0.282\n346.4\n80.0\n0.240\n322.5\n45.0\n0.192\n362.0\n35.0\n0.195\n367.4\n50.0\n0.155\n366.0\nText translation\n162.5\n0.240\n334.6\n45.0\n0.236\n362.5\n192.5\n0.211\n326.9\n55.0\n0.157\n368.0\n112.5\n0.081\n365.2\n85.0\n0.116\n352.3\n65.0\n0.111\n424.4\nOCR\n170.0\n0.367\n233.2\n167.5\n0.362\n245.5\n192.5\n0.362\n246.2\n177.5\n0.393\n238.0\n102.5\n0.276\n255.4\n95.0\n0.239\n270.6\n65.0\n0.222\n283.7\nTextual Mean\n140.0\n0.272\n306.1\n87.5\n0.268\n306.2\n184.4\n0.278\n304.8\n115.0\n0.246\n307.9\n81.3\n0.181\n327.6\n67.5\n0.174\n328.5\n57.5\n0.149\n368.1\nTextual Rank\n2\n2\n2\n4\n3\n3\n1\n1\n1\n3\n4\n4\n5\n5\n5\n6\n6\n6\n7\n7\n7\nOverall Mean\n153.5\n0.368\n242.5\n113.3\n0.340\n247.2\n166.3\n0.359\n243.1\n126.5\n0.351\n243.2\n127.6\n0.257\n270.5\n126.1\n0.238\n279.2\n122.6\n0.225\n290.1\nOverall Rank\n2\n1\n1\n7\n4\n4\n1\n2\n2\n4\n3\n3\n3\n5\n5\n5\n6\n6\n6\n7\n7\nHallusionBench*\n45.2 (rank=3)\n37.8 (rank=4)\n51.7 (rank=1)\n46.5 (rank=2)\n25.7 (rank=6)\n24.5 (rank=7)\n27.6 (rank=5)\nMMStar*\n38.6 (rank=5)\n45.7 (rank=3)\n61.6 (rank=1)\n47.7 (rank=2)\n34.8 (rank=6)\n40.1 (rank=4)\n34.6 (rank=7)\nSEEDBench (Test)*\n70.7 (rank=3)\n64.0 (rank=7)\n76.4 (rank=1)\n71.6 (rank=2)\n64.5 (rank=6)\n67.9 (rank=4)\n66.4 (rank=5)\nAI2D*\n70.2 (rank=4)\n70.6 (rank=3)\n82.2 (rank=1)\n75.5 (rank=2)\n55.7 (rank=7)\n61.3 (rank=5)\n55.9 (rank=6)\nOpenCompass*\n62.7 (rank=3)\n57.7 (rank=4)\n66.3 (rank=1)\n63.3 (rank=2)\n46.3 (rank=7)\n48.8 (rank=5)\n46.7 (rank=6)\n* Results are sourced from https:\/\/huggingface.co\/spaces\/opencompass\/open_vlm_leaderboard as of 2024-04-25 (except GPT-4o) and 2024-05-23 (for GPT-4o).\n↓Results are obtained using FID to measure the similarity between images. https:\/\/github.com\/GaParmar\/clean-fid\nTable 2: Evaluation results of GC@3, MME, HallusionBench and OpenCompass on visual(Vis)-intensive and textual(Text)-intensive images. Best results per metric and category (over different\nMLLMs) are bolded.\n6\nFigure 2: Correlation Matrix of GC@1 and GC@3 scores on MMECeption, and several other benchmarks.\nMMECeption benchmark and accuracy on the original MME benchmark are reported per category and as aggregations\nin Table 2. Additionally, we include the scores and ranks of all evaluated VLLMs on the OpenCompass (OpenCom-\npass, 2023), MME (Fu et al., 2023), HallusionBench (Liu et al., 2023a), MMStar (Chen et al., 2024), SeedBench (Li\net al., 2023a), and AI2D (Kembhavi et al., 2016) leaderboards. Notably, Gemini1.5-Pro leads our rankings, followed\nby GPT-4o, GPT-4v, Claude3-Opus, mPLUG-Owl2, and LLaVA-13B\/7B. The GenCeption method shows robustness\nto the similarity metric used, as the overall ranking remains identical when using cosine similarity or FID distance for\ncalculating GC@T scores.\nGC@1\nGC@3\nMME\nHallusionBench\nGC@1 (essentially the same as s(1))\n-\n0.99\n0.46\n0.92\nGC@3\n0.99\n-\n0.52\n0.93\ns(3) (GC@3 without weighting by t)\n0.98\n0.99\n0.46\n0.94\nCrossCheckGPT (Image-to-text)\n0.96\n0.94\n0.42\n0.97\nTable 3: Correlation matrix comparing GC@1, GC@3, s(3) (GC@3 without temporal weighting), and CrossCheckGPT with established bench-\nmarks MME and HallusionBench.\nFigure 2 presents a correlation matrix among GC@T scores and the benchmarks mentioned above, where the\noverall GC@T scores are averaged over the GC@T scores of all MME categories. The strong correlations with the\nOpenCompass scores incorporating the results of multiple meticulously crafted benchmarks indicate that MMECep-\ntion provides a comprehensive evaluation that may complement existing benchmarks. Further, GenCeption appears\nto effectively measure a VLLM’s tendency to hallucinate, as demonstrated by the strong correlations with Hallu-\nsionBench. While these observations are further emphasized by the correlations with MMStar and AI2D, the only\nmoderate correlations with MME and SEEDBench provide more nuanced insights. As MME displays these mod-\nerate correlations also with the other benchmarks, it can be reasoned that it measures dimensions supplement to\nthose measured by other benchmarks and GenCeption. SEEDBench on the other hand correlates strongly with other\nbenchmarks, but only moderately with GC@T scores. This indicates that SEEDBench measures aspects that are also\nmeasured by other benchmarks, but fail to be captured by GenCeption. Future research could focus on identifying\nthese aspects to potentially incorporate them into GenCeption.\nOne of the key strengths of GenCeption lies in its annotation-free evaluation methodology, a concept also reflected\nin emerging evaluation methods such as CrossCheckGPT Sun et al. (2024). CrossCheckGPT ranks hallucinations\nby evaluating the consistency of outputs across independent MLLM models. In 3, we analyze the correlation of\nCrossCheckGPT with GenCeption, MME, and HallusionBench scores. The results show strong correlations between\nCrossCheckGPT and both GenCeption and HallusionBench, affirming its capability to capture key evaluative met-\n7\nSample group\n& category\nGemini1.5-Pro\nClaude3-Opus\nGPT-4o\nGPT-4V\nmPLUG-Owl2\nLLaVA-13B\nLLaVA-7B\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nDalle3\nImgn2\nvisual-intensive samples\nExistence\n0.505\n0.529\n0.500\n0.532\n0.536\n0.521\n0.505\n0.530\n0.427\n0.515\n0.416\n0.485\n0.418\n0.506\nCount\n0.456\n0.489\n0.466\n0.490\n0.456\n0.494\n0.498\n0.506\n0.378\n0.463\n0.408\n0.466\n0.341\n0.416\nPosition\n0.511\n0.491\n0.495\n0.480\n0.469\n0.460\n0.501\n0.473\n0.346\n0.452\n0.359\n0.454\n0.350\n0.402\nColor\n0.545\n0.525\n0.489\n0.501\n0.480\n0.473\n0.506\n0.490\n0.345\n0.471\n0.420\n0.457\n0.318\n0.436\nPoster\n0.455\n0.388\n0.450\n0.381\n0.445\n0.383\n0.444\n0.365\n0.338\n0.357\n0.303\n0.312\n0.305\n0.266\nCelebrity\n0.417\n0.384\n0.424\n0.382\n0.418\n0.373\n0.433\n0.389\n0.319\n0.336\n0.284\n0.317\n0.263\n0.313\nScene\n0.511\n0.490\n0.504\n0.478\n0.482\n0.474\n0.497\n0.474\n0.385\n0.417\n0.355\n0.404\n0.350\n0.392\nLandmark\n0.500\n0.485\n0.460\n0.492\n0.494\n0.479\n0.458\n0.480\n0.363\n0.351\n0.376\n0.357\n0.334\n0.333\nArtwork\n0.494\n0.454\n0.508\n0.461\n0.500\n0.455\n0.504\n0.455\n0.333\n0.385\n0.308\n0.333\n0.294\n0.304\nCommon.\n0.545\n0.531\n0.535\n0.507\n0.562\n0.526\n0.563\n0.535\n0.425\n0.493\n0.429\n0.473\n0.417\n0.458\nVis Mean\n0.494\n0.477\n0.483\n0.470\n0.484\n0.464\n0.491\n0.470\n0.366\n0.424\n0.366\n0.406\n0.339\n0.383\nVis Rank\n1\n1\n4\n2\n3\n4\n2\n2\n5\n5\n5\n6\n7\n7\ntextual-intensive\nCode\n0.364\n0.177\n0.304\n0.180\n0.395\n0.179\n0.333\n0.263\n0.281\n0.100\n0.260\n0.168\n0.186\n0.108\nNumerical\n0.322\n0.417\n0.333\n0.389\n0.366\n0.456\n0.325\n0.383\n0.322\n0.225\n0.336\n0.265\n0.259\n0.222\nText trans.\n0.396\n0.227\n0.356\n0.258\n0.444\n0.277\n0.359\n0.238\n0.173\n0.052\n0.200\n0.118\n0.212\n0.073\nOCR\n0.462\n0.500\n0.486\n0.448\n0.421\n0.441\n0.482\n0.417\n0.358\n0.384\n0.368\n0.385\n0.351\n0.320\nText Mean\n0.386\n0.330\n0.370\n0.319\n0.407\n0.338\n0.375\n0.325\n0.284\n0.190\n0.291\n0.234\n0.252\n0.181\nText Rank\n2\n2\n4\n4\n1\n1\n3\n3\n6\n6\n5\n5\n7\n7\nOverall Mean\n0.463\n0.435\n0.451\n0.427\n0.462\n0.428\n0.458\n0.428\n0.343\n0.357\n0.344\n0.357\n0.314\n0.325\nOverall Rank\n1\n1\n4\n4\n2\n2\n3\n2\n6\n5\n5\n5\n7\n7\nTable 4: The impact of different image encoders, DALL·E 3 (Dalle3) vs. Imagen 2 (Imgn2), on GC@1 score. Best results per configuration and\ncategory (over different VLLMs) are bolded.\nSample group\n& category\nGemini1.5\n-Pro\nClaude3\n-Opus\nGPT-4o\nGPT-4V\nmPLUG\n-Owl2\nLLaVA\n-13B\nLLaVA\n-7B\nHuman\n∆% between\nhuman & best\nvisual-intensive samples\nExistence\n0.5841\n0.4563\n0.5578\n0.5434\n0.3967\n0.3524\n0.3782\n0.6402\n+ 9.6045%\nCount\n0.4140\n0.3799\n0.2725\n0.4882\n0.2364\n0.3535\n0.2038\n0.5476\n+ 12.1671%\nPosition\n0.5546\n0.4959\n0.4086\n0.5639\n0.3527\n0.4285\n0.3899\n0.6409\n+ 13.6549%\nColor\n0.7081\n0.6206\n0.6139\n0.5516\n0.4047\n0.4314\n0.3506\n0.8380\n+ 18.3449%\nPoster\n0.5046\n0.4362\n0.4939\n0.4681\n0.3998\n0.3208\n0.2905\n0.5456\n+ 8.1252%\nCelebrity\n0.4182\n0.3988\n0.4369\n0.4447\n0.3714\n0.2545\n0.2160\n0.4671\n+ 5.0371%\nScene\n0.6080\n0.5828\n0.5229\n0.5919\n0.4842\n0.3906\n0.4057\n0.6236\n+ 2.5658%\nLandmark\n0.4903\n0.4932\n0.5236\n0.5702\n0.3613\n0.4174\n0.3845\n0.6045\n+ 6.0154%\nArtwork\n0.3725\n0.5304\n0.5297\n0.5252\n0.2938\n0.2924\n0.2336\n0.5421\n+ 2.2059%\nCommonsense\n0.4338\n0.5375\n0.5047\n0.4012\n0.3244\n0.4153\n0.3532\n0.6417\n+ 19.3860%\nVisual Mean\n0.5088\n0.4932\n0.4865\n0.5148\n0.3625\n0.3657\n0.3206\n0.6091\n+ 9.7107%\ntext-intensive\nCode reasoning\n0.3689\n0.4085\n0.4043\n0.3690\n0.2923\n0.2963\n0.1975\n0.5376\n+ 31.6034%\nNumerical calc.\n0.3652\n0.3958\n0.3940\n0.4241\n0.3474\n0.4409\n0.3423\n0.5160\n+ 17.0333%\nText translation\n0.4480\n0.3949\n0.4333\n0.3803\n0.0931\n0.2372\n0.1981\n0.6196\n+ 38.3036%\nOCR\n0.4382\n0.4329\n0.3334\n0.4455\n0.2663\n0.3371\n0.2912\n0.4696\n+ 5.4097%\nTextual Mean\n0.4051\n0.4080\n0.3913\n0.4047\n0.2498\n0.3279\n0.2573\n0.5357\n+ 23.0875%\nOverall Mean\n0.4792\n0.4688\n0.4593\n0.4834\n0.3303\n0.3549\n0.3025\n0.5882\n+ 13.5327%\nTable 5: The performance of VLLMs and humans on the GC@1 metric, evaluated using 5 randomly drawn images per sample\/image category. The\nbest performance achieved by an VLLM is underlined.\nrics. Notably, CrossCheckGPT exhibits a weaker correlation with MME, which is likely because the GenCeption\nbenchmark is developed using MME image samples, making it inherently more aligned with the MME framework.\nGC@T scores, as defined in Equation (3), are weighted by a temporal factor t. To examine the impact of this\n8\nweighting, we conducted an ablation study where the weighting mechanism was removed, effectively transforming\nGC@T into s(T). Table 3 demonstrates that s(3) retains a high correlation with GC@3, yet its correlation with MME\ndiminishes compared to the weighted version, while its alignment with HallusionBench remains consistent. Further-\nmore, unweighted scores correlate with MME in a uniform manner across different iterations, whereas the weighted\nscores show a progressive increase in correlation with MME as more iterations are applied. This indicates that tem-\nporal weighting amplifies later iterations’ influence, emphasizing cumulative semantic shifts captured by MME’s\niterative design. Generally, stronger correlation with MME is desirable as it validates the alignment between GenCep-\ntion’s metrics and an established benchmark, reinforcing GenCeption’s ability to assess iterative semantic coherence\neffectively and reliably.\nTable 4 compares GC@1 scores using different image generators, OpenAI’s DALL·E 3 (Ramesh et al., 2021)\nand Google DeepMind’s Imagen2 (DeepMind, 2023). Independent of image generator used, the rankings remain\nunchanged, except that on visual-intensive samples only, Claude3-Opus scores equally with GPT-4V. This provides\nevidence that even though DALL·E 3, GPT-4o, and GPT-4V were developed and trained by OpenAI, neither of\nOpenAI’s models has an advantage over non-OpenAI VLLMs.\nTable 5 shows human performance on a subset of 5 randomly drawn images per category compared to the VLLM\nperformance on the same subset of samples. It can be observed that humans outperform all VLLMs, with especially\nstrong differences in performance for the text-intensive categories. The worst performance, relative to humans, is\nachieved on the code reasoning and text translation categories, the former containing images of code snippets and the\nlatter of phrases written in simplified Chinese characters. The relatively best performance by VLLMs is achieved on\nthe scene and artwork categories, which contain every-day life photos and popular artworks. This demonstrates that\nthere is still substantial space for performance improvement, and that compared to humans, VLLMs still lack relevant\ncompetences. It must be noted that human performance here does not constitute an upper bound in possible scores to\nachieve, and that future generations of VLLMs may well outperform humans.\n3.2. Qualitative Results\nWe qualitatively inspect our results by visualizing generated images together with their cosine similarity and\nGC@T scores for two seed images across different categories, as shown in Figure 3. This visualization highlights\nthe correlation between these scores and the visual characteristics of the images relative to the seed image. A key\nobservation is that later iterations show an increased tendency to produce imagery deviating from the seed image, as\nindicated by lower GC@T scores. This serves as an additional qualitative validation of the GenCeption method and\nthe MMECeption benchmark, as using VLLMs scoring higher on the MMECeption benchmark results in generated\nimages that preserve more information from the seed image. For a wider range of examples across MME image\ncategories and corresponding descriptions from each evaluated VLLM, readers are referred to Appendix A.\n4. Discussion and Future Directions\nThis study validates the GenCeption method with a focus on the visual modality primarily because (1) VLLMs\nare the most widely used and readily available MLLMs on the market, and (2) image generation and embedding\ntools have reached a mature and highly commercialized stage compared to other modalities. However, GenCeption\nis designed to be modality-agnostic. The same iterative procedure (i.e., describing a unimodal sample and then re-\ngenerating it from the description) can, in principle, be applied to other non-text modalities like audio and video.\nThe requirement is that (1) a generation model exists for the given modality, and (2) there is a suitable encoder to\nquantify the similarity between the original sample and the regenerated one. Moreover, recent advancements have\nintroduced multimodal LLMs capable of both generating and interpreting multiple modalities simultaneously, such as\nShow-o (Xie et al., 2024), Emu3 (Wang et al., 2024), and JanusPro (Chen et al., 2025). In these cases, GenCeption\ncould leverage the same MLLM for both description and generation tasks, serving as a particularly valuable approach\nfor directly measuring modality consistency within such unified multimodal systems.\nFuture research is invited to adapt GenCeption to other non-text modalities, such as audio, video, and graphs. For\ninstance, the framework can be initiated with a dataset of audio samples, and MLLMs can iteratively generate and\ndescribe the audio content. Similarly, for video and graph data, the process can involve generating textual descriptions\nof short video clips or graph structures and their recreation. While the core iterative process of GenCeption remains\napplicable, these extensions require careful exploration of modality-specific generation and embedding models.\n9\nSeed Im age\n0.569\n0.362\n0.271\n0.334\n0.057\n0.123\n0.266\n0.194\n0.100\nGC@3=0.351\nGC@1=0.569\nGC@3=0.136\nGC@1=0.334\nGC@3=0.159\nGC@1=0.266\nt=1\nt=2\nt=3\nfrom \n\" Color \"  \ncategory\n0.384\n0.158\n0.260\nGC@3=0.247\nGC@1=0.384\n0.707\n0.484\n0.400\nGC@3=0.477\nGC@1=0.707\n0.428\n0.299\n0.351\nGC@3=0.347\nGC@1=0.428\nGC@3=0.286\nGC@1=0.426\n0.426\n0.312\n0.222\nt=1\nt=2\nt=3\nGC@3=0.296\nGC@1=0.224\n0.224\n0.272\n0.336\nGC@3=0.246\nGC@1=0.430\n0.430\n0.276\n0.164\nSeed Im age\nfrom \n\" OCR\"  \ncategory\nGC@3=0.264\nGC@1=0.382\n0.382\n0.202\n0.266\nGC@3=0.383\nGC@1=0.375\n0.375\n0.394\n0.379\nGC@3=0.276\nGC@1=0.373\n0.373\n0.266\n0.251\nFigure 3: Demonstration of GenCeption evaluation procedure: the images generated over 3 GenCeption iterations for several MLLMs. The\nsimilarity s(t) scores (to the seed image) are shown on the top of images; GC@1 and GC@3 scores are printed on the bottom of the first and third\nimage, respectively.\nThe broad skill assessment provided by GenCeption goes along with the limitation that it is difficult to assess\nwhich skills contribute most to a high GC@T score. Our analysis indicates that contemporary VLLMs perform\npoorly on text-intensive tasks while excelling in describing scenes and artworks. Future research could investigate\nthis in a more fine-grained manner by creating datasets requiring specialized skills. For example, datasets could\ninclude images of complex emotions, dynamic movements, mechanical processes, or user interfaces. Additionally,\ncombining GenCeption with specifically designed similarity metrics may offer more detailed insights into specific\nMLLM abilities.\n5. Conclusion\nIn this paper, we introduce GenCeption to enhance the evaluation of rapidly evolving Multimodal Language\nModels (MLLMs). The GenCeption method attempts to address key limitations of existing MLLM benchmarks,\nsuch as costly data annotation, leading questions, the illusion of emergent abilities, and, as it allows to use newly\ncreated images without annotation, training data contamination. Further, it is expected to result in slower benchmark\nsaturation. Being adaptable to different modalities, the GenCeption method can deliver value as a unified MLLM\nevaluation method that complements existing MLLM benchmarks.\nOur empirical validation using the MMECeption benchmark shows that GenCeption effectively assesses semantic\ncoherence and consistency across modalities, aligning with established VLLM benchmarks. By assessing humans on\nthe MMECeption task, we demonstrate that current VLLMs significantly lag behind human performance, particularly\n10\nwhen working with text-intensive images. Future work is encouraged to refine and extend this framework across a\nwider range of modalities, datasets, and similarity metrics.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | GenCeption：一种无需标注的多模态语言模型评估方法\n\n## 📌 背景痛点\/本文动机\n随着多模态大型语言模型（MLLMs）的快速发展，现有的评估方法主要依赖于昂贵的多模态标注数据集，这些数据集往往滞后于MLLMs的快速演进需求。此外，现有的评估方法还存在以下挑战：\n1. 依赖高质量标注的多模态数据集，成本高昂且难以捕捉MLLMs的动态发展。\n2. 使用离散指标（如准确率）评估，可能导致对模型能力的误判。\n3. 评估分数可能无法反映真实世界任务上的表现，因为训练数据可能受到基准数据集的污染。\n4. 评估过程中，一个模态的内容可能并不需要回答基准问题，因为答案可以从问题或MLLMs的预训练知识中推断出来。\n\n## 🚀 核心方法\n本文提出了GenCeption，一种新颖的无标注评估方法，它只需要单模态数据来衡量跨模态语义一致性和评估MLLMs的幻觉倾向。GenCeption的主要特点包括：\n1. **无标注评估**：无需昂贵的数据标注，只需使用单模态数据集即可进行评估。\n2. **跨模态语义一致性**：通过迭代生成和描述非文本样本，评估MLLMs在不同模态之间保持语义一致性的能力。\n3. **幻觉倾向评估**：通过衡量语义漂移，间接评估MLLMs的幻觉倾向。\n4. **连续指标**：使用GC@T指标，提供更细粒度的评估，避免对模型能力的误判。\n\n## 📈 实验结果\n本文在MMECeption基准上评估了七种流行的视觉语言模型（VLLMs），并与人类标注者的表现进行了比较。结果表明，GenCeption与现有VLLM基准具有强相关性，能够有效评估VLLMs的语义一致性和幻觉倾向。此外，VLLMs在文本密集型任务上的表现仍然显著落后于人类。\n\n## 💬 可借鉴之处\nGenCeption提供了一种新颖的无标注评估方法，可以有效地评估MLLMs的跨模态语义一致性和幻觉倾向。该方法具有以下优势：\n1. **成本效益**：无需昂贵的数据标注，降低了评估成本。\n2. **动态适应**：能够适应MLLMs的快速演进需求。\n3. **全面评估**：能够评估MLLMs的多种能力，包括描述、生成和推理等。\n4. **可扩展性**：可以扩展到其他非文本模态，如音频、视频和图形等。\n\n## 🌟 总结\nGenCeption为MLLMs的评估提供了一种新颖且有效的无标注方法，能够有效地评估MLLMs的跨模态语义一致性和幻觉倾向。该方法具有成本效益、动态适应、全面评估和可扩展性等优势，为MLLMs的评估提供了新的思路和方法。","llm_summary_res_status":200}
{"title":"PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain","authors":"Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang","summary":"We present PCA-Bench, a multimodal decision-making benchmark for evaluating\nthe integrated capabilities of Multimodal Large Language Models (MLLMs).\nDeparting from previous benchmarks focusing on simplistic tasks and individual\nmodel capability, PCA-Bench introduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. Given task instructions and\ndiverse contexts, the model is required to seamlessly integrate multiple\ncapabilities of Perception, Cognition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench features error localization\ncapabilities, scrutinizing model inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the reliability of deploying MLLMs. To\nbalance accuracy and efficiency in evaluation, we propose PCA-Eval, an\nautomatic evaluation protocol, and assess 10 prevalent MLLMs. The results\nreveal significant performance disparities between open-source models and\npowerful proprietary models like GPT-4 Vision. To address this, we introduce\nEmbodied-Instruction-Evolution (EIE), an automatic framework for synthesizing\ninstruction tuning examples in multimodal embodied environments. EIE generates\n7,510 training examples in PCA-Bench and enhances the performance of\nopen-source MLLMs, occasionally surpassing GPT-4 Vision (+3\\% in decision\naccuracy), thereby validating the effectiveness of EIE. Our findings suggest\nthat robust MLLMs like GPT4-Vision show promise for decision-making in embodied\nagents, opening new avenues for MLLM research.","url":"http:\/\/arxiv.org\/abs\/2402.15527v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.15527v1","published":1708499398000,"comment":"Code and Data released at https:\/\/github.com\/pkunlp-icler\/PCA-EVAL.\n  Leaderboard at: https:\/\/docs.qq.com\/sheet\/DVUd4WUpGRHRqUnNV. This article\n  supersedes its workshop version arxiv: 2310.02071. arXiv admin note: text\n  overlap with arXiv:2310.02071","pdf_text":"PCA-Bench: Evaluating Multimodal Large Language Models in\nPerception-Cognition-Action Chain\nLiang Chen1, Yichi Zhang1, Shuhuai Ren1, Haozhe Zhao1, Zefan Cai1, Yuchi Wang1,\nPeiyi Wang1, Xiangdi Meng1, Tianyu Liu2, Baobao Chang1\n1 National Key Laboratory for Multimedia Information Processing, Peking University\n2 Alibaba Group\n{leo.liang.chen, yczhang, shuhuai_ren}@stu.pku.edu.cn\ntianyu0421@alibaba-inc.com, chbb@pku.edu.cn\n PCA-EVAL\nPCA-Bench-V1\nAbstract\nWe present PCA-Bench, a multimodal decision-\nmaking benchmark for evaluating the inte-\ngrated capabilities of Multimodal Large Lan-\nguage Models (MLLMs). Departing from pre-\nvious benchmarks focusing on simplistic tasks\nand individual model capability, PCA-Bench in-\ntroduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world\ngames. Given task instructions and diverse con-\ntexts, the model is required to seamlessly inte-\ngrate multiple capabilities of Perception, Cog-\nnition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench fea-\ntures error localization capabilities, scrutinizing\nmodel inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the\nreliability of deploying MLLMs. To balance\naccuracy and efficiency in evaluation, we pro-\npose PCA-Eval, an automatic evaluation pro-\ntocol, and assess 10 prevalent MLLMs. The\nresults reveal significant performance dispari-\nties between open-source models and powerful\nproprietary models like GPT-4 Vision. To ad-\ndress this, we introduce Embodied-Instruction-\nEvolution (EIE), an automatic framework for\nsynthesizing instruction tuning examples in\nmultimodal embodied environments. EIE gen-\nerates 7,510 training examples in PCA-Bench\nand enhances the performance of open-source\nMLLMs, occasionally surpassing GPT-4 Vi-\nsion (+3% in decision accuracy), thereby val-\nidating the effectiveness of EIE. Our findings\nsuggest that robust MLLMs like GPT4-Vision\nshow promise for decision-making in embod-\nied agents, opening new avenues for MLLM\nresearch.\n1\nIntroduction\nMultimodal Large Language Models (MLLMs)\nhave demonstrated remarkable capabilities in tack-\nling complex tasks that necessitate a chain of in-\ntegrated skills, including visual perception, world\nknowledge, reasoning, action, and more (OpenAI,\nYou are a driving assistant.  Based on current image, \nwhat should you do if you want to drive forward safely?\nA. Slow down \nB. Keep driving \nC. Call the police \nPlease first describe the image and related information, \nthen think step by step, and finally give the anwser.\nVisual Feature Extractor\nMultimodal LLM\nThe image depicts a wintry scene with a road covered in snow. The sky \nis clear, and no other vehicles on the road. There's a road sign visible, \nwith a symbol suggesting caution due to a slippery road surface.\nThe snow leads a landscape with sparse vegetation. Clear sky\nindicating it's likely during the day with good visibility. The road itself \nis not cleared of snow, showing only tire tracks indicating that some \nvehicles have passed before. The road sign warns for icy or snowy \nconditions.\nGiven these factors, the most appropriate action to take if you want to \ndrive forward safely would be:\nA. Slow down\nVisual\nEmbeddings\nLanguage\nEmbeddings\n......\nPerception\nCognition\nAction\nPerceiving Input\nReasoning\nMaking Decision\nFigure 1: Example of decision making with MLLMs in\nthe Perception-Cognition-Action Chain.\n2023; Dai et al., 2023a; Liu et al., 2023b; Li et al.,\n2023c; Zhao et al., 2023).\nHowever, current MLLM benchmarks often eval-\nuate these capabilities individually (Fu et al., 2023;\nLiu et al., 2023e), overlooking the significant in-\ntegrated potential that Large Language Models\n(LLMs) contribute to multimodal models. While\nsome benchmarks like MMMU (Yue et al., 2023)\nand MathVista (Lu et al., 2023a) require abilities\nfrom both the vision and language part, they lack\nerror localization techniques beyond accuracy as-\nsessments. This complicates identifying which part\nof the MLLM malfunctioned when making mis-\ntakes—whether it was the visual or the language\ncomponent—and determines which aspect requires\nenhancement to enhance overall performance.\nTo address the challenges of insufficient inte-\ngrated benchmarking and error localization prob-\nlems, we introduce PCA-Bench. It arises with\narXiv:2402.15527v1  [cs.CL]  21 Feb 2024\nMLLM’s applications in embodied AI and decision\nmaking, where models called agents need to first\nprocess multimodal observation from different en-\nvironments, reason with the current situation and\ngoal, and finally make an action from a given ac-\ntion space. The abilities in the complex decision\nmaking process can be abstracted to Perception,\nCognition and Action according to the Perception-\nAction loop (Fuster, 2004) in Cognitive Science, a\nfundamental concept that describes how organisms\nprocess sensory information to interact with their\nenvironment through actions, offering a compre-\nhensive framework for assessment. Figure 1 shows\nhow MLLMs make decisions in the PCA chain.\nThe instances in PCA-Bench are from three in-\nfluential domains in embodied decision-making:\nautonomous driving, domestic robotics, and open-\nworld gaming. As shown in Figure 2, each in-\nstance is annotated by human annotators with a\n6-element tuple: <image, question, action candi-\ndates, answer, reason, key concept>. The last three\nelements serve as anchors for error localization for\nAction, Cognition and Perception, correspondingly.\nPCA-Eval is an anchor-based evaluation pro-\ntocol, designed to automatically conduct error lo-\ncalization utilizing the powerful semantic parsing\nability of LLMs and the anchor information in data\nannotation. In the past, such localization was both\nlabor-intensive and time-consuming. PCA-Eval\nwith strong LLMs like GPT4 demonstrates a strong\nkappa correlation with human assessments, reach-\ning 0.8+ average kappa coefficients for perception,\ncognition, and action scores. The anchor-based\nevaluation provides the LLMs with groundtruth\nanswers for each sub-score, preventing the sys-\ntematic bias of LLM evaluators, such as position\nbias (Wang et al., 2023b; Zheng et al., 2023) in\nthe pair-wise evaluation and verbosity bias (Zheng\net al., 2023) in simple preference evaluation. We\nalso compared open state-of-the-art LLMs in PCA-\nEval. Though they lag behind close ones in align-\nment with human assessments, we see large im-\nprovement when the model scales up. We believe\nthat with specific training for error localization and\nimproved general ability of open LLMs in the fu-\nture, they would be more suitable evaluation tools\nfor the reproducible and transparent characteristics.\nAiming at scaling up PCA-Bench, using LLM\nto synthesize training examples is an increasingly\npopular method for enhancing models without ad-\nditional human involvement. We expand this ap-\nproach to generate more samples following the\nFigure 2: Instances of PCA-Bench in 3 domains.\nPCA guideline. Unlike text-based instruction gen-\neration methods like Self-Instruct (Wang et al.,\n2023c), generating instructions in embodied envi-\nronments poses distinct challenges. It demands not\nonly the creation of textual instructions but also the\ngeneration of corresponding precise observations.\nTo address these challenges, we propose Embod-\nied Instruction Evolution (EIE), which integrates\nexternal environments with LLMs, thereby extend-\ning the LLMs’ ability to data synthesize across\nvarious embodied environments, contributing to\n7,510 training data in PCA-Bench.\nWe conduct comprehensive experiments and\nanalysis on PCA-Bench, our findings are summa-\nrized as follows:\n1. Visual perception and reasoning with world\nknowledge are two core abilities for an MLLM\nto make correct decisions in PCA-Bench. GPT4-\nVision shows strong zero-shot cross-modal reason-\ning ability for embodied decision-making tasks,\nsurpassing open-source MLLMs and even Tool-\nUsing LLM-agent.\n2. EIE could generate training samples signifi-\ncantly enhancing the performance of open-source\nMLLMs (surpassing GPT-4V at some scores), vali-\ndating the effectiveness of the method.\n3.\nPCA-Eval serves as a good error locator.\nAbove the high average kappa coefficient (0.8+)\nwith human assessments and its ability to pinpoint\nthe error source, it can effectively distinguishes\nwhether a model’s correct decisions are fluky or\nthrough genuine understanding. This leads to a bet-\nter ensemble metric for MLLM evaluation named\nGenuine PCA Score.\n2\nPCA-Bench\n2.1\nProblem Definition\nMultimodal decision-making problems are com-\nmonly formalized with a partially observable\nMarkov decision process. For MLLMs F tested in\nPCA-Bench, we care about given the multi-modal\nobservation o ∈O, the goal description g, a subset\nof candidates actions AC ⊆A, whether the model\ncould make correct action a ∈AC and give proper\nreasoning process r.\nF(g, o, AC) = (a, r)\n(1)\nAs shown in Figure 2, each instance in the bench-\nmark is a 6-element tuple: <image, question, ac-\ntion candidates, answer, reason, key concept>.\nThe image is collected from various embodied en-\nvironments, including transportation scenes, house-\nkeeper environments, and Minecraft. Questions,\naction candidates, and answers are derived from\nreal tasks within the corresponding environment.\nThe reasons explain why the answer is the best\nchoice for the current image, while the key concept\nhighlights the most question-related aspect of the\nimage.\nUnlike traditional visual question-answering\ndatasets that emphasize visual perception (e.g.,\nVQA (Goyal et al., 2017)) or visual reasoning\n(e.g., NLVR (Suhr et al., 2017)), PCA-Bench man-\ndates accurate observation perception, complex\ntask decomposition, and understanding the out-\ncomes of various actions simultaneously. Com-\npared to embodied simulation environments such as\nALFRED (Shridhar et al., 2020) and Minedojo (Fan\net al., 2022), PCA-Bench stands out for its focus\non high-level actions, proving to be more effec-\ntive for evaluating MLLMs. This is because high-\nlevel actions, which can be readily translated or\nprogrammed into low-level actions within their re-\nspective domains, are inherently more accessible\nto LLMs. The high-level actions are more compre-\nhensible for LLMs than the direct low-level actions\nlike action vectors in the simulation environments\nbecause (1) the high-level actions are in the form\nof natural languages, making it easier for LLMs\nto understand the meaning and connect with world\nknowledge. (2) LLMs are not grounded with low-\nlevel actions during the pretraining or finetuning\nstage, making it hard for LLMs to understand the\nconsequences of executing an action.\nTo answer a question in PCA-Bench, the agent\nmust possess the following abilities: (1) Percep-\ntion: Accurately identify the concept related to\nthe question within the image; (2) Cognition: En-\ngage in reasoning based on image perception and\nworldly knowledge; (3) Action: Comprehend the\npotential actions, selecting the one that best aligns\nwith the outcome of the reasoning process. A de-\nficiency in any of these abilities would possibly\nresult in an incorrect answer, posing a significant\nchallenge to the more integrated capabilities of\nMLLMs.\n2.2\nPCA-Eval\nFor each instance, we prompt the model to deliver\nan answer comprising a reasoning process r, and\na final action a, represented as < r, a >. By com-\nparing the model prediction with the ground truth\nanswer, we can obtain a fine-grained diagnosis of\nthe decision making process as follows:\nPerception Score (P-Score) measures the model’s\naccuracy in perceiving the observation. It is com-\nputed based on whether the agent’s reasoning pro-\ncess r includes the key concept of the instance. A\nscore of 1 is assigned if at least one question-related\nkey concept is described by the agent; otherwise,\nit is 0. For the top example in Figure 2, the agent\nshould output “clear road” or “no car visible” or\nother semantically equivalent concepts in its de-\nscription of the image to get the perception score.\nParsing the model’s output and determining\nwhether it entails the key concept using shallow\nfeatures of the sentence is not trivial. We leverage\nLLM to conduct entailment detection, which turns\nout to have a high alignment with human judgment.\nCognition Score (C-Score) assesses the model’s\nability to reason, comprehend, and make informed\ndecisions based on the perceived input data and\nworld knowledge. The score is 1 if the reasoning\nprocess is correct, otherwise the score is 0. For\nthe instance in Figure 2, the agent should link the\n“clear road” to the action “keep driving” based on\ntransportation commonsense to get the score.\nAction Score (A-Score) measures the model’s abil-\nity to generate appropriate and effective responses\nor actions based on the perceived input data and\nthe cognitive understanding of the context. The\nscore is assigned a value of 1 if the agent selects\nthe correct action; otherwise, the score is set to 0.\n2.3\nAutomatic Evaluation\nRecent advancements have seen researchers har-\nnessing powerful LLMs for the evaluation of the\noutput of language models. Studies have revealed\nthat the outcomes from LLMs could exhibit re-\nmarkable alignment with human judgments (Zheng\net al., 2023; Wang et al., 2023b,a). In our investiga-\ntion, we employed GPT-4 to automatically evaluate\nperception, cognition, and action scores based on\nthe model’s outputs. Our findings underscore a\nsignificant agreement between GPT-4 scoring and\nhuman evaluation results. This is substantiated by\nCohen-Kappa coefficients of 0.71, 0.82, and 0.94\nfor perception, cognition, and action evaluations,\nrespectively. Experiments of human evaluation and\ncomparison of open LLMs are in section 4.1. For a\ndetailed description of our evaluation tool, kindly\nrefer to Appendix D.\n2.4\nBenchmark Dataset Overview\nFor the test set, the examples are written by 3 hu-\nman experts for each domain. There are no over-\nlapped environmental observations between the\ntraining and test sets. The details of the human\nannotation pipeline can be found in Appendix B.\nWe introduce the three domains encompassed by\nour dataset as follows:\nAutonomous Driving.\nIn the autonomous driv-\ning domain, instances are derived from real-world\ntransportation scenes, which requires the agent to\nhave particular abilities such as traffic sign recogni-\ntion, obstacle detection, and decision-making at in-\ntersections. The dataset aims to evaluate an agent’s\nability to perceive and interpret visual informa-\ntion while making safe and efficient driving deci-\nsions. The images are collected from TT100K (Zhu\net al., 2016) dataset and annotators are instructed\nto propose an image-conditioned question that is\ngrounded with real actions of vehicles.\nDomestic Robot.\nThe domestic assistance do-\nmain features instances from the ALFRED (Shrid-\nhar et al., 2020; Kolve et al., 2017) environment,\nwhich simulates a housekeeper robot performing\ntasks within a household setting. These tasks may\ninclude object manipulation, navigation, and inter-\naction with various appliances. The environment\nassesses an agent’s ability to understand and exe-\ncute complex instructions while navigating and in-\nteracting with a dynamic environment. Annotators\nare asked to select one image from the randomly\ngenerated scenes in the environment, propose a\nquestion related to the items on the scene, and an-\nnotate the full information of the instance.\nTopology Graph: Harvest beef using iron sword\nCraft 2 Iron Ingot\nCollect 2 Wood\nCraft 1 Stick\nFind a Cow\nKill a Cow\nCraft an Iron Sword\nCollect 2 Iron Ore\nFigure 3: Illustration of task topology graph. Events in\ngreen represent the leaf nodes of the graph.\nOpen-World Game.\nIn the open-world game do-\nmain, instances are sourced from the Minecraft en-\nvironment, where agents are tasked with exploring,\ncrafting, and surviving in a procedurally generated\nworld. This dataset evaluates an agent’s ability to\nreason and plan actions within a complex, open-\nended environment, which often requires long-term\nstrategizing and adaptability. Annotators receive\npredefined tasks from MineDojo (Fan et al., 2022)\nas a reference during the task generation phase. For\neach task, we instruct the annotator to sketch a task\ntopology graph, exemplified in Figure 3. The task\nshould be completed under the topological order of\nthe graph, where the event located in the leaf nodes\nshould be finished first. Each node in the task topol-\nogy graph can be viewed as a step in the sequential\ndecision. We list the in-domain task distribution in\nAppendix A.\n2.5\nEmbodied Instruction Evolution\nThe PCA-Bench benchmark also includes subset\nof automatic generated samples by Embodied In-\nstruction Evolution(EIE), which is used as training\nset in our experiment.\nThe annotation of PCA-Bench examples is a\nlabor-intensive task. As illustrated in Figure 4, we\nintroduce Embodied Instruction Evolution (EIE),\na method for automatically augmenting examples\nin the PCA-Bench format using Large Language\nModels, such as ChatGPT. This process involves\nfour key steps:\n1) Setup of Programmable Interface: Estab-\nlish a programmable interface with a corresponding\ntemplate, ensuring that observations in the embod-\nied environment can be generated based on specific\nparameters.\n2) Generation of Seed Tasks: Create initial\nseed tasks for each environment. These tasks are\nrepresentative of the general challenges an agent\nFigure 4: Pipeline of the Embodied Instruction Evolution method.\nmight encounter. We provide ChatGPT with sam-\nple tasks and enable it to generate additional seed\ntasks.\n3) Task Specification and Template Filling:\nFor each seed task, we instruct ChatGPT to break\ndown the task into multiple subtasks, following its\nevent topology graph (as seen in Figure 3). This\napproach mimics the multi-step decision-making\nprocess. After determining the subtask names, we\nuse the LLM to populate the environment parame-\nter templates created in Step 1 for each subtask.\n4) Observation Generation and Filtering:\nGenerate observations for the environment and im-\nplement an automatic process to filter out invalid\ninstances. The filled templates may contain er-\nrors, such as incorrect creature names or impos-\nsible items, leading to errors during environment\ncreation. When such errors occur, the affected tem-\nplates are automatically filtered out. For domains\nwithout programmable environments (autonomous\ndriving), step 1 and step 4 are not needed, we col-\nlect real traffic images and utilize GPT4-Vision to\ngenerate seed task based on the image content.\nEIE leverages the capabilities of Large Language\nModels to reduce manual labor and improve the\ndiversity and scalability of PCA-Bench.\n3\nExperiments\n3.1\nTracks\nZero Shot End-to-End.\nThe test set of PCA-\nBench serves as an effective tool for comparing\nthe embodied decision-making and cross-modal\nreasoning capabilities of various Multimodal Lan-\nguage Learning Models (MLLMs). In this evalu-\nation, the same images and prompts are provided\nto each model under test. Additionally, to address\nthe challenge of perceiving certain non-visual in-\nformation from images, details such as “items in\nhand” and “items in inventory”, particularly rele-\nvant in domestic and gaming domains, are directly\nincluded in the question prompts.\nIn our analysis, we benchmark the performance\nof the most recently open-sourced models, includ-\ning LLaVA1.5 and Qwen-VL-Chat, as well as the\nAPI-only GPT4-V model. All models are evalu-\nated using their default inference configurations to\nensure a fair and standardized comparison.\nFinetuning with EIE.\nIn this track, we extend\nthe capabilities of open-source MLLMs by fine-\ntuning them with the training set generated through\nour Embodied Instruction Evolution (EIE) method.\nAfter the fine-tuning process, these trained models\nare subjected to the test set of PCA-Bench. We\nfinetune the LLaVA-7b\/13b, MMICL and Qwen-\nVL-Chat models on the training set for 5 epochs.\nThe training details are in Appendix E.\nZero Shot Modality Conversion.\nIn this track,\nwe introduce and compare a new baseline, termed\nHOLMES, which utilizes LLM without multi-\nmodal perception capabilities. Instead, HOLMES\nrelies on modality conversion APIs for embodied\ndecision-making processes. Within the HOLMES\nframework, the LLM must continuously invoke\nvarious APIs, retrieving and processing return in-\nformation about the environment. The HOLMES\nmethod is illustrated in Figure 7 from Appendix.\nModel\nSize\nTraffic\nDomestic\nGame\nAverage\nP\nC\nA\nP\nC\nA\nP\nC\nA\nP\nC\nA\nMiniGPT4 (Zhu et al., 2023)†\n7B\n0.45\n0.37\n0.48\n0.81\n0.38\n0.38\n0.38\n0.14\n0.27\n0.55\n0.30\n0.38\nLLaVA1.5 (Liu et al., 2023b)†\n7B\n0.44\n0.44\n0.53\n0.92\n0.48\n0.44\n0.8\n0.35\n0.39\n0.72\n0.42\n0.45\nQwen-VL-Chat (Bai et al., 2023)†\n7B\n0.53\n0.36\n0.62\n0.77\n0.41\n0.44\n0.39\n0.18\n0.25\n0.56\n0.33\n0.44\nMiniGPT4 (Zhu et al., 2023)†\n13B\n0.41\n0.37\n0.5\n0.85\n0.35\n0.33\n0.41\n0.22\n0.33\n0.56\n0.31\n0.39\nInstructBLIP (Dai et al., 2023b)†\n13B\n0.36\n0.41\n0.42\n0.90\n0.44\n0.39\n0.33\n0.25\n0.24\n0.53\n0.37\n0.35\nMMICL (Zhao et al., 2023)†\n13B\n0.31\n0.49\n0.47\n0.81\n0.3\n0.33\n0.41\n0.18\n0.27\n0.51\n0.32\n0.36\nSPHINX-v1 (Lin et al., 2023)†\n13B\n0.46\n0.48\n0.61\n0.95\n0.55\n0.31\n0.71\n0.35\n0.43\n0.71\n0.46\n0.45\nLLaVA1.5 (Liu et al., 2023b)†\n13B\n0.49\n0.56\n0.61\n0.95\n0.62\n0.46\n0.74\n0.45\n0.51\n0.73\n0.54\n0.53\nQwen-VL-Chat-PLUS (Bai et al., 2023)‡\nUNK\n0.57\n0.56\n0.65\n0.86\n0.44\n0.43\n0.68\n0.47\n0.49\n0.70\n0.49\n0.52\nGPT-4V (OpenAI, 2023)‡\nUNK\n0.73\n0.72\n0.74\n0.96\n0.66\n0.62\n0.88\n0.72\n0.69\n0.86\n0.7\n0.68\nTable 1: Zero Shot results on the full test set of PCA-Bench. Highest scores in each line are bold while second\nhighest scores are underlined. Models with † are fully open-source. Models with ‡ only provide API to access. P, C,\nand A represent Perception, Cognition, and Action Scores, respectively.\nFigure 5: Performance comparsion between models’ zero-shot results and models’ finetuned results with the data\ngenerated by Embodied-Instruct-Evolution (EIE) method. EIE improves the performance on all domains for both\nLLaVA1.5-7b and Qwen-VL-Chat models. Results of LLavA1.5-13B and MMICL are in Figure 13 from appendix.\nWe evaluate two LLMs in this track: ChatGPT-\n3.5-Turbo and GPT-4-0613, comparing their per-\nformances against the advanced GPT-4-Vision. Im-\nplementation details of the HOLMES framework\nand the APIs are provided in Appendix C.\n3.2\nEvaluation and Metrics\nWe use our PCA-Eval evaluation tool proposed in\nSection 2.3 to automatically assess the output of dif-\nferent models through three lenses: perception (P-\nScore), cognition (C-Score), and action (A-Score).\n3.3\nMain Results\nZero Shot Results.\nThe results of the zero-shot\nend-to-end track are shown in Table 1. Among\nall MLLMs, GPT4-V, outperforms existing open-\nsource models by achieving the highest scores of\n0.86, 0.7, and 0.68 in the perception, cognition, and\naction dimensions respectively. This performance\nrepresents a 15% action score improvement over\nits strongest open-source counterpart, LLaVA1.5-\n13B. The impressive performance of GPT4-V is\nprimarily attributed to its exceptional ability to per-\nceive visual information across different domains\nand the world knowledge in the language model,\nparticularly in the challenging game domain.\nImpact of Finetuning with EIE.\nThe results of\nthe fine-tuning track are illustrated in Figure 5. Our\nEIE method has been found to significantly en-\nhance the general decision-making abilities of vari-\nous models, encompassing perception, cognition,\nand action. Notably, it has led to an average in-\ncrease of 0.24 and 0.19 in action scores for the\nLLaVA1.5-7b and Qwen-VL-Chat models, respec-\ntively. Results for LLaVA1.5-13b and MMICL are\nillustrated in Figure 13, also showing improved\nperformance when trained with EIE. We note that\nthere exist reasoning or perception errors in some\nof the generated sample due to the hallucination\nproblem of LLM generated content, however they\ndo not influence the overall performance. In some\ncases, these sub-scores have matched or even sur-\npassed those of the GPT4-V model, demonstrating\nthe potential of the EIE to scale up and apply to\ndifferent environments.\nComparison Between End-to-End and Modality\nConversion Method\nIn the zero-shot modality\nconversion track, we conduct an analysis and com-\nparison of the outputs generated by the End2End\nMethod\nModel\nTraffic\nDomestic\nGame\nAverage\nP\nC\nA\nP\nC\nA\nP\nC\nA\nP\nC\nA\nEnd-to-End\nGPT-4V\n0.75\n0.73\n0.78\n0.81\n0.69\n0.67\n0.95\n0.79\n0.77\n0.84\n0.74\n0.74\nHOLMES\nChatGPT\n0.75\n0.68\n0.66\n0.88\n0.52\n0.50\n0.78\n0.40\n0.36\n0.80\n0.53\n0.51\nGPT4\n0.87\n0.82\n0.82\n0.85\n0.61\n0.56\n0.91\n0.77\n0.74\n0.88\n0.73\n0.71\nTable 2: Comparison between End-to-End (MLLM) and HOLMES (LLM+API) methods on a subset of PCA-Bench\nwith API annotation.\nmethod with GPT4-V, as well as the HOLMES\nmethod with GPT4 and ChatGPT-3.5 in Table 2.\nThe results show that the HOLMES system\nbased on GPT4 achieves 0.71 Action Score, which\nis on par with GPT4-V’s performance (0.74). This\nindicates that, overall, the HOLMES system is able\nto accurately understand the task goal, split the\nlarger goal into multiple smaller steps, and cor-\nrectly invoke the relevant APIs to accomplish each\nstep. Specifically, the HOLMES system based on\nGPT4 can recognize the key concepts in a task, and\nperceive the state and environment of these con-\ncepts through the results returned by APIs. Conse-\nquently, the system achieves an average Perception\nScore of 0.88, which even outperforms GPT4-V’s\n0.84. However, compared to End2End methods,\nHOLMES relies on multi-step reasoning for the\nfinal decision, in which reasoning errors tend to\naccumulate, and thus achieves a lower Cognition\nScore in both Domestic and Game domains.\nOn the other hand, we also find that the End2End\nmethod effectively mitigates information loss dur-\ning the modality conversion process. As illustrated\nin Figure 8 from Appendix, an image depicts a\nroad with several nearby cars. GPT4-V is capable\nof discerning that the street is not crowded, thereby\nsuggesting that the driver can continue driving.\nConversely, GPT4-HOLMES, while being aware\nof the number of cars, lacks information about their\nspatial relation, leading it to recommend slowing\ndown because of the existence of 14 cars. This\nsuggests that the End2End method is superior in\nperceiving certain visual features that are not cap-\ntured by the APIs. Conversely, some specialized\nAPIs, such as traffic sign detection, outperform\nGPT4-V in tasks like traffic sign detection, as they\nare specifically trained for this task. This could en-\nable the HOLMES method to gather more accurate\ninformation than the End2End model.\nEvaluator Model\nKappa Coefficients\nP\nC\nA\nGPT4†\n0.71\n0.82\n0.94\nQwen1.5-72B-Chat†\n0.30\n0.49\n0.60\nQwen1.5-14B-Chat†\n0.16\n0.24\n0.16\nQwen1.5-7B-Chat†\n0.20\n0.11\n0.06\nTable 3: Comparison of Open† and Close† LLMs as\nEvaluators. Kappa coefficients of Qwens increase when\nthe model scales up.\n4\nDiscussion\n4.1\nStrong LLMs are Good Error Locators.\nAs shown in Table 3, we compare the scoring kappa\ncoefficients with human assessments for different\nLLMs. We randomly select 300 model outputs\nequally from different domains and ask 3 human\nexperts to give perception, cognition, and action\nscores. The final result is based on the majority\nof three annotators. The result underscores a sig-\nnificant agreement between GPT-4 scoring and hu-\nman evaluation results. This is substantiated by\nCohen-Kappa coefficients of 0.71, 0.82, and 0.94\nfor perception, cognition, and action evaluations.\nWe also compare open models as evaluators.\nWe choose one of the best open LLMs, Qwen1.51\nseries from 7B, 14B to 72B version. Currently\nopen LLMs tend to give wrongly high judgments in\nall sub-scores. Although currently trailing behind\nGPT-4 in performance, we anticipate that with tar-\ngeted training focused on error identification and\nenhancements in the overall capabilities of open\nLLMs, these models will become more effective\nevaluation tools compared to closed models. This\nis primarily due to the reproducible and transpar-\nent nature of open models, which offer significant\nadvantages in the development of evaluation tools.\n1https:\/\/huggingface.co\/collections\/Qwen\n4.2\nGenuine PCA Score\nPCA-Eval could pinpoint cases where the MLLM\ngets the correct answer by a fluke where perception\nor cognition score is 0 but the action score is 1. It\nexplains why for some models, the action score is\nhigher than perception and cognition scores. For in-\nstance, a model might opt for a conservative action,\nsuch as slowing down, even without accurately rec-\nognizing snowy weather in the image, resulting in\na fluky correct action. In another scenario, if the\nmodel exhibits a preference for a specific choice\nindex, it will attain a high action score provided\nthat the evaluation dataset contains a substantial\nnumber of correct choices matching the preferred\nindex, a phenomenon attributable to the positional\nbiases inherent in both the model and the dataset.\nTo overcome the mentioned bias when evaluating\nthe genuine ability of MLLM, we propose a new\nmetric Genuine PCA Score. It is equal to one\nif the perception, cogntion and action scores are\nall 1 for one model’s response to a question. We\nfind that for all models, there exists significant gap\n(>10%) between the action score and genuine PCA\nscore in average, revealing that relying on single\nmetric such as choice accuracy is very problematic\nwhen conducting model evaluation. In our online\nleaderboard, both average action score and average\ngenuine PCA score are considered when ranking\nthe candidate models.\n4.3\nAlignment between Agent Decisions and\nHuman Values\nWe have observed instances where the decisions\nmade by the agent contradict human values. Con-\nsider the scenario depicted in Figure 9 from Ap-\npendix. The image illustrates a crosswalk with-\nout pedestrians. The appropriate response would\nbe slowing down, as caution is paramount when\napproaching a crosswalk, regardless of the pres-\nence or absence of pedestrians. However, upon\nprocessing the information that the crosswalk is\nempty, ChatGPT suggests that maintaining the cur-\nrent speed is the optimal action, arguing that the\nabsence of pedestrians eliminates the need to slow\ndown. The rationale provided by ChatGPT is logi-\ncal, yet it does not align with human values.\n5\nRelated Work\nMLLM Benchmark.\nIn recent times, there\nhave been several benchmarks built for evaluating\nMLLMs, such as MMBench, MME, Seed-Bench,\nPOPE (Liu et al., 2023e; Fu et al., 2023; Li et al.,\n2023a,e) that assess MLLMs performance from\nmultiple fine-grained dimensions.\nVisit-Bench,\nLVLM-eHub, M3IT (Bitton et al., 2023; Xu et al.,\n2023; Li et al., 2023c) focus on the general in-\nstruction following ability. General VQA tasks\nlike OKVQA, VQAv2, Vizwiz, ScienceQA, VSR\nand IconQA (Marino et al., 2019; Agrawal et al.,\n2015; Gurari et al., 2018; Lu et al., 2022; Liu et al.,\n2023a; Lu et al., 2021) focus on visual understand-\ning. MMMU, MathVista, LLaVA-benchmark and\nMM-Vet (Yue et al., 2023; Lu et al., 2023a; Liu\net al., 2023c; Yu et al., 2023) require abilities from\nthe vision part and specific knowledge in the lan-\nguage part. A lack of error localization techniques\nbeyond accuracy assessments is among current\nbenchmarks. This complicates identifying which\npart of the MLLM malfunctioned when making\nmistakes. Unlike prior work, PCA-Bench is more\nrelevant to evaluate MLLMs’ ability to utilize inte-\ngrated abilities to solve one task and make explain-\nable decisions via error localization.\nLLM Agent and Embodied Decision Making.\nUsing LLMs to empower the AI agents (Xi et al.,\n2023; Liu et al., 2023d; Park et al., 2023; Wang\net al., 2023d) becomes more and more promis-\ning. Specifically, we can employ LLMs to enhance\nthe decision making ability of the agents (Nakano\net al., 2022; Yao et al., 2022; Li et al., 2023d;\nSong et al., 2023; Li et al., 2023b), expanding\ntheir perception and action space through strate-\ngies like tool utilization (Schick et al., 2023; Qin\net al., 2023; Lu et al., 2023b). This line of research\ndivides the entire decision-making process into two\nphases: (1) information seeking, usually involving\nMLLMs to verbalize the current status of AI agents\nin the vision-based environment with natural lan-\nguage; (2) reasoning and planning with text-based\nLLMs to decide what the AI agent should do in\nthe next step with textual clues. Although LLM-\nbased agents demonstrate reasoning and planning\nabilities through techniques like Chain of Thought\nor problem decomposition (Wei et al., 2023; Yao\net al., 2023; Kojima et al., 2022), they inherently\nlack visual perception, and are limited to the dis-\ncrete textual content. Therefore, integrating mul-\ntimodal information can offer agents a broader\ncontext and a more precise understanding, such\nas PaLM-E (Driess et al., 2023), enhancing their\nenvironmental perception. However, there is still\nlarge gap deploying MLLM in various embodied\nenvironments due to the lack of appropriate bench-\nmark and interface linking those two domains while\nPCA-Bench is an attempt towards that goal.\n6\nConclusion\nIn this paper, we introduce PCA-Bench, a mul-\ntimodal benchmark designed to assess the inte-\ngrated decision-making capabilities of MLLMs.\nThis benchmark features PCA-EVAL, a novel fine-\ngrained automatic evaluation tool that diagnoses\ndecision making processes from three critical per-\nspectives: perception, cognition, and action. To\nenhance the decision making ability from data per-\nspective, we propose the Embodied Instruction Evo-\nlution method to automatically synthesize instruc-\ntion examples from different environments, which\nhas been proven effective in our main experiments.\nWe believe that powerful MLLMs pave a new and\npromising way toward decision making in embod-\nied environments and we hope PCA-Bench could\nserve as a good benchmark in evaluation and error\nlocalization for MLLMs’ development.\n7\nLimitations\nThe current scope of PCA-Bench is confined to\nmerely three domains in static environments. One\nof our future works aims to broaden this scope\nto encompass more domains and dynamic embod-\nied environments where MLLMs could keep get-\nting feedback, which is closer to real embodied\nAI scenarios.\nWe do not apply different infer-\nence enhancement methods like In-Context Learn-\ning and Reflection in the decision making process\nof MLLMs. We just use the simplest prompting\nmethod and leave the exploration of a better cross-\nmodal Chain-of-Thought method for future studies.\nCurrently, PCA-Eval shows the best consistency\nwith human evaluators when using powerful close\nLLM GPT4, which would bring additional cost to\nthe user of PCA-Eval. We plan to develop and re-\nlease an open error locator for error localization in\nthe benchmark in the future.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nPCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain\n```\n#### 2. 论文摘要\n```\nWe present PCA-Bench, a multimodal decision-making benchmark for evaluating\nthe integrated capabilities of Multimodal Large Language Models (MLLMs).\nDeparting from previous benchmarks focusing on simplistic tasks and individual\nmodel capability, PCA-Bench introduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. Given task instructions and\ndiverse contexts, the model is required to seamlessly integrate multiple\ncapabilities of Perception, Cognition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench features error localization\ncapabilities, scrutinizing model inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the reliability of deploying MLLMs. To\nbalance accuracy and efficiency in evaluation, we propose PCA-Eval, an\nautomatic evaluation protocol, and assess 10 prevalent MLLMs. The results\nreveal significant performance disparities between open-source models and\npowerful proprietary models like GPT-4 Vision. To address this, we introduce\nEmbodied-Instruction-Evolution (EIE), an automatic framework for synthesizing\ninstruction tuning examples in multimodal embodied environments. EIE generates\n7,510 training examples in PCA-Bench and enhances the performance of\nopen-source MLLMs, occasionally surpassing GPT-4 Vision (+3\\% in decision\naccuracy), thereby validating the effectiveness of EIE. Our findings suggest\nthat robust MLLMs like GPT4-Vision show promise for decision-making in embodied\nagents, opening new avenues for MLLM research.\n```\n\n#### 3. 论文全文\n```\nPCA-Bench: Evaluating Multimodal Large Language Models in\nPerception-Cognition-Action Chain\nLiang Chen1, Yichi Zhang1, Shuhuai Ren1, Haozhe Zhao1, Zefan Cai1, Yuchi Wang1,\nPeiyi Wang1, Xiangdi Meng1, Tianyu Liu2, Baobao Chang1\n1 National Key Laboratory for Multimedia Information Processing, Peking University\n2 Alibaba Group\n{leo.liang.chen, yczhang, shuhuai_ren}@stu.pku.edu.cn\ntianyu0421@alibaba-inc.com, chbb@pku.edu.cn\n PCA-EVAL\nPCA-Bench-V1\nAbstract\nWe present PCA-Bench, a multimodal decision-\nmaking benchmark for evaluating the inte-\ngrated capabilities of Multimodal Large Lan-\nguage Models (MLLMs). Departing from pre-\nvious benchmarks focusing on simplistic tasks\nand individual model capability, PCA-Bench in-\ntroduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world\ngames. Given task instructions and diverse con-\ntexts, the model is required to seamlessly inte-\ngrate multiple capabilities of Perception, Cog-\nnition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench fea-\ntures error localization capabilities, scrutinizing\nmodel inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the\nreliability of deploying MLLMs. To balance\naccuracy and efficiency in evaluation, we pro-\npose PCA-Eval, an automatic evaluation pro-\ntocol, and assess 10 prevalent MLLMs. The\nresults reveal significant performance dispari-\nties between open-source models and powerful\nproprietary models like GPT-4 Vision. To ad-\ndress this, we introduce Embodied-Instruction-\nEvolution (EIE), an automatic framework for\nsynthesizing instruction tuning examples in\nmultimodal embodied environments. EIE gen-\nerates 7,510 training examples in PCA-Bench\nand enhances the performance of open-source\nMLLMs, occasionally surpassing GPT-4 Vi-\nsion (+3% in decision accuracy), thereby val-\nidating the effectiveness of EIE. Our findings\nsuggest that robust MLLMs like GPT4-Vision\nshow promise for decision-making in embod-\nied agents, opening new avenues for MLLM\nresearch.\n1\nIntroduction\nMultimodal Large Language Models (MLLMs)\nhave demonstrated remarkable capabilities in tack-\nling complex tasks that necessitate a chain of in-\ntegrated skills, including visual perception, world\nknowledge, reasoning, action, and more (OpenAI,\nYou are a driving assistant.  Based on current image, \nwhat should you do if you want to drive forward safely?\nA. Slow down \nB. Keep driving \nC. Call the police \nPlease first describe the image and related information, \nthen think step by step, and finally give the anwser.\nVisual Feature Extractor\nMultimodal LLM\nThe image depicts a wintry scene with a road covered in snow. The sky \nis clear, and no other vehicles on the road. There's a road sign visible, \nwith a symbol suggesting caution due to a slippery road surface.\nThe snow leads a landscape with sparse vegetation. Clear sky\nindicating it's likely during the day with good visibility. The road itself \nis not cleared of snow, showing only tire tracks indicating that some \nvehicles have passed before. The road sign warns for icy or snowy \nconditions.\nGiven these factors, the most appropriate action to take if you want to \ndrive forward safely would be:\nA. Slow down\nVisual\nEmbeddings\nLanguage\nEmbeddings\n......\nPerception\nCognition\nAction\nPerceiving Input\nReasoning\nMaking Decision\nFigure 1: Example of decision making with MLLMs in\nthe Perception-Cognition-Action Chain.\n2023; Dai et al., 2023a; Liu et al., 2023b; Li et al.,\n2023c; Zhao et al., 2023).\nHowever, current MLLM benchmarks often eval-\nuate these capabilities individually (Fu et al., 2023;\nLiu et al., 2023e), overlooking the significant in-\ntegrated potential that Large Language Models\n(LLMs) contribute to multimodal models. While\nsome benchmarks like MMMU (Yue et al., 2023)\nand MathVista (Lu et al., 2023a) require abilities\nfrom both the vision and language part, they lack\nerror localization techniques beyond accuracy as-\nsessments. This complicates identifying which part\nof the MLLM malfunctioned when making mis-\ntakes—whether it was the visual or the language\ncomponent—and determines which aspect requires\nenhancement to enhance overall performance.\nTo address the challenges of insufficient inte-\ngrated benchmarking and error localization prob-\nlems, we introduce PCA-Bench. It arises with\narXiv:2402.15527v1  [cs.CL]  21 Feb 2024\nMLLM’s applications in embodied AI and decision\nmaking, where models called agents need to first\nprocess multimodal observation from different en-\nvironments, reason with the current situation and\ngoal, and finally make an action from a given ac-\ntion space. The abilities in the complex decision\nmaking process can be abstracted to Perception,\nCognition and Action according to the Perception-\nAction loop (Fuster, 2004) in Cognitive Science, a\nfundamental concept that describes how organisms\nprocess sensory information to interact with their\nenvironment through actions, offering a compre-\nhensive framework for assessment. Figure 1 shows\nhow MLLMs make decisions in the PCA chain.\nThe instances in PCA-Bench are from three in-\nfluential domains in embodied decision-making:\nautonomous driving, domestic robotics, and open-\nworld gaming. As shown in Figure 2, each in-\nstance is annotated by human annotators with a\n6-element tuple: <image, question, action candi-\ndates, answer, reason, key concept>. The last three\nelements serve as anchors for error localization for\nAction, Cognition and Perception, correspondingly.\nPCA-Eval is an anchor-based evaluation pro-\ntocol, designed to automatically conduct error lo-\ncalization utilizing the powerful semantic parsing\nability of LLMs and the anchor information in data\nannotation. In the past, such localization was both\nlabor-intensive and time-consuming. PCA-Eval\nwith strong LLMs like GPT4 demonstrates a strong\nkappa correlation with human assessments, reach-\ning 0.8+ average kappa coefficients for perception,\ncognition, and action scores. The anchor-based\nevaluation provides the LLMs with groundtruth\nanswers for each sub-score, preventing the sys-\ntematic bias of LLM evaluators, such as position\nbias (Wang et al., 2023b; Zheng et al., 2023) in\nthe pair-wise evaluation and verbosity bias (Zheng\net al., 2023) in simple preference evaluation. We\nalso compared open state-of-the-art LLMs in PCA-\nEval. Though they lag behind close ones in align-\nment with human assessments, we see large im-\nprovement when the model scales up. We believe\nthat with specific training for error localization and\nimproved general ability of open LLMs in the fu-\nture, they would be more suitable evaluation tools\nfor the reproducible and transparent characteristics.\nAiming at scaling up PCA-Bench, using LLM\nto synthesize training examples is an increasingly\npopular method for enhancing models without ad-\nditional human involvement. We expand this ap-\nproach to generate more samples following the\nFigure 2: Instances of PCA-Bench in 3 domains.\nPCA guideline. Unlike text-based instruction gen-\neration methods like Self-Instruct (Wang et al.,\n2023c), generating instructions in embodied envi-\nronments poses distinct challenges. It demands not\nonly the creation of textual instructions but also the\ngeneration of corresponding precise observations.\nTo address these challenges, we propose Embod-\nied Instruction Evolution (EIE), which integrates\nexternal environments with LLMs, thereby extend-\ning the LLMs’ ability to data synthesize across\nvarious embodied environments, contributing to\n7,510 training data in PCA-Bench.\nWe conduct comprehensive experiments and\nanalysis on PCA-Bench, our findings are summa-\nrized as follows:\n1. Visual perception and reasoning with world\nknowledge are two core abilities for an MLLM\nto make correct decisions in PCA-Bench. GPT4-\nVision shows strong zero-shot cross-modal reason-\ning ability for embodied decision-making tasks,\nsurpassing open-source MLLMs and even Tool-\nUsing LLM-agent.\n2. EIE could generate training samples signifi-\ncantly enhancing the performance of open-source\nMLLMs (surpassing GPT-4V at some scores), vali-\ndating the effectiveness of the method.\n3.\nPCA-Eval serves as a good error locator.\nAbove the high average kappa coefficient (0.8+)\nwith human assessments and its ability to pinpoint\nthe error source, it can effectively distinguishes\nwhether a model’s correct decisions are fluky or\nthrough genuine understanding. This leads to a bet-\nter ensemble metric for MLLM evaluation named\nGenuine PCA Score.\n2\nPCA-Bench\n2.1\nProblem Definition\nMultimodal decision-making problems are com-\nmonly formalized with a partially observable\nMarkov decision process. For MLLMs F tested in\nPCA-Bench, we care about given the multi-modal\nobservation o ∈O, the goal description g, a subset\nof candidates actions AC ⊆A, whether the model\ncould make correct action a ∈AC and give proper\nreasoning process r.\nF(g, o, AC) = (a, r)\n(1)\nAs shown in Figure 2, each instance in the bench-\nmark is a 6-element tuple: <image, question, ac-\ntion candidates, answer, reason, key concept>.\nThe image is collected from various embodied en-\nvironments, including transportation scenes, house-\nkeeper environments, and Minecraft. Questions,\naction candidates, and answers are derived from\nreal tasks within the corresponding environment.\nThe reasons explain why the answer is the best\nchoice for the current image, while the key concept\nhighlights the most question-related aspect of the\nimage.\nUnlike traditional visual question-answering\ndatasets that emphasize visual perception (e.g.,\nVQA (Goyal et al., 2017)) or visual reasoning\n(e.g., NLVR (Suhr et al., 2017)), PCA-Bench man-\ndates accurate observation perception, complex\ntask decomposition, and understanding the out-\ncomes of various actions simultaneously. Com-\npared to embodied simulation environments such as\nALFRED (Shridhar et al., 2020) and Minedojo (Fan\net al., 2022), PCA-Bench stands out for its focus\non high-level actions, proving to be more effec-\ntive for evaluating MLLMs. This is because high-\nlevel actions, which can be readily translated or\nprogrammed into low-level actions within their re-\nspective domains, are inherently more accessible\nto LLMs. The high-level actions are more compre-\nhensible for LLMs than the direct low-level actions\nlike action vectors in the simulation environments\nbecause (1) the high-level actions are in the form\nof natural languages, making it easier for LLMs\nto understand the meaning and connect with world\nknowledge. (2) LLMs are not grounded with low-\nlevel actions during the pretraining or finetuning\nstage, making it hard for LLMs to understand the\nconsequences of executing an action.\nTo answer a question in PCA-Bench, the agent\nmust possess the following abilities: (1) Percep-\ntion: Accurately identify the concept related to\nthe question within the image; (2) Cognition: En-\ngage in reasoning based on image perception and\nworldly knowledge; (3) Action: Comprehend the\npotential actions, selecting the one that best aligns\nwith the outcome of the reasoning process. A de-\nficiency in any of these abilities would possibly\nresult in an incorrect answer, posing a significant\nchallenge to the more integrated capabilities of\nMLLMs.\n2.2\nPCA-Eval\nFor each instance, we prompt the model to deliver\nan answer comprising a reasoning process r, and\na final action a, represented as < r, a >. By com-\nparing the model prediction with the ground truth\nanswer, we can obtain a fine-grained diagnosis of\nthe decision making process as follows:\nPerception Score (P-Score) measures the model’s\naccuracy in perceiving the observation. It is com-\nputed based on whether the agent’s reasoning pro-\ncess r includes the key concept of the instance. A\nscore of 1 is assigned if at least one question-related\nkey concept is described by the agent; otherwise,\nit is 0. For the top example in Figure 2, the agent\nshould output “clear road” or “no car visible” or\nother semantically equivalent concepts in its de-\nscription of the image to get the perception score.\nParsing the model’s output and determining\nwhether it entails the key concept using shallow\nfeatures of the sentence is not trivial. We leverage\nLLM to conduct entailment detection, which turns\nout to have a high alignment with human judgment.\nCognition Score (C-Score) assesses the model’s\nability to reason, comprehend, and make informed\ndecisions based on the perceived input data and\nworld knowledge. The score is 1 if the reasoning\nprocess is correct, otherwise the score is 0. For\nthe instance in Figure 2, the agent should link the\n“clear road” to the action “keep driving” based on\ntransportation commonsense to get the score.\nAction Score (A-Score) measures the model’s abil-\nity to generate appropriate and effective responses\nor actions based on the perceived input data and\nthe cognitive understanding of the context. The\nscore is assigned a value of 1 if the agent selects\nthe correct action; otherwise, the score is set to 0.\n2.3\nAutomatic Evaluation\nRecent advancements have seen researchers har-\nnessing powerful LLMs for the evaluation of the\noutput of language models. Studies have revealed\nthat the outcomes from LLMs could exhibit re-\nmarkable alignment with human judgments (Zheng\net al., 2023; Wang et al., 2023b,a). In our investiga-\ntion, we employed GPT-4 to automatically evaluate\nperception, cognition, and action scores based on\nthe model’s outputs. Our findings underscore a\nsignificant agreement between GPT-4 scoring and\nhuman evaluation results. This is substantiated by\nCohen-Kappa coefficients of 0.71, 0.82, and 0.94\nfor perception, cognition, and action evaluations,\nrespectively. Experiments of human evaluation and\ncomparison of open LLMs are in section 4.1. For a\ndetailed description of our evaluation tool, kindly\nrefer to Appendix D.\n2.4\nBenchmark Dataset Overview\nFor the test set, the examples are written by 3 hu-\nman experts for each domain. There are no over-\nlapped environmental observations between the\ntraining and test sets. The details of the human\nannotation pipeline can be found in Appendix B.\nWe introduce the three domains encompassed by\nour dataset as follows:\nAutonomous Driving.\nIn the autonomous driv-\ning domain, instances are derived from real-world\ntransportation scenes, which requires the agent to\nhave particular abilities such as traffic sign recogni-\ntion, obstacle detection, and decision-making at in-\ntersections. The dataset aims to evaluate an agent’s\nability to perceive and interpret visual informa-\ntion while making safe and efficient driving deci-\nsions. The images are collected from TT100K (Zhu\net al., 2016) dataset and annotators are instructed\nto propose an image-conditioned question that is\ngrounded with real actions of vehicles.\nDomestic Robot.\nThe domestic assistance do-\nmain features instances from the ALFRED (Shrid-\nhar et al., 2020; Kolve et al., 2017) environment,\nwhich simulates a housekeeper robot performing\ntasks within a household setting. These tasks may\ninclude object manipulation, navigation, and inter-\naction with various appliances. The environment\nassesses an agent’s ability to understand and exe-\ncute complex instructions while navigating and in-\nteracting with a dynamic environment. Annotators\nare asked to select one image from the randomly\ngenerated scenes in the environment, propose a\nquestion related to the items on the scene, and an-\nnotate the full information of the instance.\nTopology Graph: Harvest beef using iron sword\nCraft 2 Iron Ingot\nCollect 2 Wood\nCraft 1 Stick\nFind a Cow\nKill a Cow\nCraft an Iron Sword\nCollect 2 Iron Ore\nFigure 3: Illustration of task topology graph. Events in\ngreen represent the leaf nodes of the graph.\nOpen-World Game.\nIn the open-world game do-\nmain, instances are sourced from the Minecraft en-\nvironment, where agents are tasked with exploring,\ncrafting, and surviving in a procedurally generated\nworld. This dataset evaluates an agent’s ability to\nreason and plan actions within a complex, open-\nended environment, which often requires long-term\nstrategizing and adaptability. Annotators receive\npredefined tasks from MineDojo (Fan et al., 2022)\nas a reference during the task generation phase. For\neach task, we instruct the annotator to sketch a task\ntopology graph, exemplified in Figure 3. The task\nshould be completed under the topological order of\nthe graph, where the event located in the leaf nodes\nshould be finished first. Each node in the task topol-\nogy graph can be viewed as a step in the sequential\ndecision. We list the in-domain task distribution in\nAppendix A.\n2.5\nEmbodied Instruction Evolution\nThe PCA-Bench benchmark also includes subset\nof automatic generated samples by Embodied In-\nstruction Evolution(EIE), which is used as training\nset in our experiment.\nThe annotation of PCA-Bench examples is a\nlabor-intensive task. As illustrated in Figure 4, we\nintroduce Embodied Instruction Evolution (EIE),\na method for automatically augmenting examples\nin the PCA-Bench format using Large Language\nModels, such as ChatGPT. This process involves\nfour key steps:\n1) Setup of Programmable Interface: Estab-\nlish a programmable interface with a corresponding\ntemplate, ensuring that observations in the embod-\nied environment can be generated based on specific\nparameters.\n2) Generation of Seed Tasks: Create initial\nseed tasks for each environment. These tasks are\nrepresentative of the general challenges an agent\nFigure 4: Pipeline of the Embodied Instruction Evolution method.\nmight encounter. We provide ChatGPT with sam-\nple tasks and enable it to generate additional seed\ntasks.\n3) Task Specification and Template Filling:\nFor each seed task, we instruct ChatGPT to break\ndown the task into multiple subtasks, following its\nevent topology graph (as seen in Figure 3). This\napproach mimics the multi-step decision-making\nprocess. After determining the subtask names, we\nuse the LLM to populate the environment parame-\nter templates created in Step 1 for each subtask.\n4) Observation Generation and Filtering:\nGenerate observations for the environment and im-\nplement an automatic process to filter out invalid\ninstances. The filled templates may contain er-\nrors, such as incorrect creature names or impos-\nsible items, leading to errors during environment\ncreation. When such errors occur, the affected tem-\nplates are automatically filtered out. For domains\nwithout programmable environments (autonomous\ndriving), step 1 and step 4 are not needed, we col-\nlect real traffic images and utilize GPT4-Vision to\ngenerate seed task based on the image content.\nEIE leverages the capabilities of Large Language\nModels to reduce manual labor and improve the\ndiversity and scalability of PCA-Bench.\n3\nExperiments\n3.1\nTracks\nZero Shot End-to-End.\nThe test set of PCA-\nBench serves as an effective tool for comparing\nthe embodied decision-making and cross-modal\nreasoning capabilities of various Multimodal Lan-\nguage Learning Models (MLLMs). In this evalu-\nation, the same images and prompts are provided\nto each model under test. Additionally, to address\nthe challenge of perceiving certain non-visual in-\nformation from images, details such as “items in\nhand” and “items in inventory”, particularly rele-\nvant in domestic and gaming domains, are directly\nincluded in the question prompts.\nIn our analysis, we benchmark the performance\nof the most recently open-sourced models, includ-\ning LLaVA1.5 and Qwen-VL-Chat, as well as the\nAPI-only GPT4-V model. All models are evalu-\nated using their default inference configurations to\nensure a fair and standardized comparison.\nFinetuning with EIE.\nIn this track, we extend\nthe capabilities of open-source MLLMs by fine-\ntuning them with the training set generated through\nour Embodied Instruction Evolution (EIE) method.\nAfter the fine-tuning process, these trained models\nare subjected to the test set of PCA-Bench. We\nfinetune the LLaVA-7b\/13b, MMICL and Qwen-\nVL-Chat models on the training set for 5 epochs.\nThe training details are in Appendix E.\nZero Shot Modality Conversion.\nIn this track,\nwe introduce and compare a new baseline, termed\nHOLMES, which utilizes LLM without multi-\nmodal perception capabilities. Instead, HOLMES\nrelies on modality conversion APIs for embodied\ndecision-making processes. Within the HOLMES\nframework, the LLM must continuously invoke\nvarious APIs, retrieving and processing return in-\nformation about the environment. The HOLMES\nmethod is illustrated in Figure 7 from Appendix.\nModel\nSize\nTraffic\nDomestic\nGame\nAverage\nP\nC\nA\nP\nC\nA\nP\nC\nA\nP\nC\nA\nMiniGPT4 (Zhu et al., 2023)†\n7B\n0.45\n0.37\n0.48\n0.81\n0.38\n0.38\n0.38\n0.14\n0.27\n0.55\n0.30\n0.38\nLLaVA1.5 (Liu et al., 2023b)†\n7B\n0.44\n0.44\n0.53\n0.92\n0.48\n0.44\n0.8\n0.35\n0.39\n0.72\n0.42\n0.45\nQwen-VL-Chat (Bai et al., 2023)†\n7B\n0.53\n0.36\n0.62\n0.77\n0.41\n0.44\n0.39\n0.18\n0.25\n0.56\n0.33\n0.44\nMiniGPT4 (Zhu et al., 2023)†\n13B\n0.41\n0.37\n0.5\n0.85\n0.35\n0.33\n0.41\n0.22\n0.33\n0.56\n0.31\n0.39\nInstructBLIP (Dai et al., 2023b)†\n13B\n0.36\n0.41\n0.42\n0.90\n0.44\n0.39\n0.33\n0.25\n0.24\n0.53\n0.37\n0.35\nMMICL (Zhao et al., 2023)†\n13B\n0.31\n0.49\n0.47\n0.81\n0.3\n0.33\n0.41\n0.18\n0.27\n0.51\n0.32\n0.36\nSPHINX-v1 (Lin et al., 2023)†\n13B\n0.46\n0.48\n0.61\n0.95\n0.55\n0.31\n0.71\n0.35\n0.43\n0.71\n0.46\n0.45\nLLaVA1.5 (Liu et al., 2023b)†\n13B\n0.49\n0.56\n0.61\n0.95\n0.62\n0.46\n0.74\n0.45\n0.51\n0.73\n0.54\n0.53\nQwen-VL-Chat-PLUS (Bai et al., 2023)‡\nUNK\n0.57\n0.56\n0.65\n0.86\n0.44\n0.43\n0.68\n0.47\n0.49\n0.70\n0.49\n0.52\nGPT-4V (OpenAI, 2023)‡\nUNK\n0.73\n0.72\n0.74\n0.96\n0.66\n0.62\n0.88\n0.72\n0.69\n0.86\n0.7\n0.68\nTable 1: Zero Shot results on the full test set of PCA-Bench. Highest scores in each line are bold while second\nhighest scores are underlined. Models with † are fully open-source. Models with ‡ only provide API to access. P, C,\nand A represent Perception, Cognition, and Action Scores, respectively.\nFigure 5: Performance comparsion between models’ zero-shot results and models’ finetuned results with the data\ngenerated by Embodied-Instruct-Evolution (EIE) method. EIE improves the performance on all domains for both\nLLaVA1.5-7b and Qwen-VL-Chat models. Results of LLavA1.5-13B and MMICL are in Figure 13 from appendix.\nWe evaluate two LLMs in this track: ChatGPT-\n3.5-Turbo and GPT-4-0613, comparing their per-\nformances against the advanced GPT-4-Vision. Im-\nplementation details of the HOLMES framework\nand the APIs are provided in Appendix C.\n3.2\nEvaluation and Metrics\nWe use our PCA-Eval evaluation tool proposed in\nSection 2.3 to automatically assess the output of dif-\nferent models through three lenses: perception (P-\nScore), cognition (C-Score), and action (A-Score).\n3.3\nMain Results\nZero Shot Results.\nThe results of the zero-shot\nend-to-end track are shown in Table 1. Among\nall MLLMs, GPT4-V, outperforms existing open-\nsource models by achieving the highest scores of\n0.86, 0.7, and 0.68 in the perception, cognition, and\naction dimensions respectively. This performance\nrepresents a 15% action score improvement over\nits strongest open-source counterpart, LLaVA1.5-\n13B. The impressive performance of GPT4-V is\nprimarily attributed to its exceptional ability to per-\nceive visual information across different domains\nand the world knowledge in the language model,\nparticularly in the challenging game domain.\nImpact of Finetuning with EIE.\nThe results of\nthe fine-tuning track are illustrated in Figure 5. Our\nEIE method has been found to significantly en-\nhance the general decision-making abilities of vari-\nous models, encompassing perception, cognition,\nand action. Notably, it has led to an average in-\ncrease of 0.24 and 0.19 in action scores for the\nLLaVA1.5-7b and Qwen-VL-Chat models, respec-\ntively. Results for LLaVA1.5-13b and MMICL are\nillustrated in Figure 13, also showing improved\nperformance when trained with EIE. We note that\nthere exist reasoning or perception errors in some\nof the generated sample due to the hallucination\nproblem of LLM generated content, however they\ndo not influence the overall performance. In some\ncases, these sub-scores have matched or even sur-\npassed those of the GPT4-V model, demonstrating\nthe potential of the EIE to scale up and apply to\ndifferent environments.\nComparison Between End-to-End and Modality\nConversion Method\nIn the zero-shot modality\nconversion track, we conduct an analysis and com-\nparison of the outputs generated by the End2End\nMethod\nModel\nTraffic\nDomestic\nGame\nAverage\nP\nC\nA\nP\nC\nA\nP\nC\nA\nP\nC\nA\nEnd-to-End\nGPT-4V\n0.75\n0.73\n0.78\n0.81\n0.69\n0.67\n0.95\n0.79\n0.77\n0.84\n0.74\n0.74\nHOLMES\nChatGPT\n0.75\n0.68\n0.66\n0.88\n0.52\n0.50\n0.78\n0.40\n0.36\n0.80\n0.53\n0.51\nGPT4\n0.87\n0.82\n0.82\n0.85\n0.61\n0.56\n0.91\n0.77\n0.74\n0.88\n0.73\n0.71\nTable 2: Comparison between End-to-End (MLLM) and HOLMES (LLM+API) methods on a subset of PCA-Bench\nwith API annotation.\nmethod with GPT4-V, as well as the HOLMES\nmethod with GPT4 and ChatGPT-3.5 in Table 2.\nThe results show that the HOLMES system\nbased on GPT4 achieves 0.71 Action Score, which\nis on par with GPT4-V’s performance (0.74). This\nindicates that, overall, the HOLMES system is able\nto accurately understand the task goal, split the\nlarger goal into multiple smaller steps, and cor-\nrectly invoke the relevant APIs to accomplish each\nstep. Specifically, the HOLMES system based on\nGPT4 can recognize the key concepts in a task, and\nperceive the state and environment of these con-\ncepts through the results returned by APIs. Conse-\nquently, the system achieves an average Perception\nScore of 0.88, which even outperforms GPT4-V’s\n0.84. However, compared to End2End methods,\nHOLMES relies on multi-step reasoning for the\nfinal decision, in which reasoning errors tend to\naccumulate, and thus achieves a lower Cognition\nScore in both Domestic and Game domains.\nOn the other hand, we also find that the End2End\nmethod effectively mitigates information loss dur-\ning the modality conversion process. As illustrated\nin Figure 8 from Appendix, an image depicts a\nroad with several nearby cars. GPT4-V is capable\nof discerning that the street is not crowded, thereby\nsuggesting that the driver can continue driving.\nConversely, GPT4-HOLMES, while being aware\nof the number of cars, lacks information about their\nspatial relation, leading it to recommend slowing\ndown because of the existence of 14 cars. This\nsuggests that the End2End method is superior in\nperceiving certain visual features that are not cap-\ntured by the APIs. Conversely, some specialized\nAPIs, such as traffic sign detection, outperform\nGPT4-V in tasks like traffic sign detection, as they\nare specifically trained for this task. This could en-\nable the HOLMES method to gather more accurate\ninformation than the End2End model.\nEvaluator Model\nKappa Coefficients\nP\nC\nA\nGPT4†\n0.71\n0.82\n0.94\nQwen1.5-72B-Chat†\n0.30\n0.49\n0.60\nQwen1.5-14B-Chat†\n0.16\n0.24\n0.16\nQwen1.5-7B-Chat†\n0.20\n0.11\n0.06\nTable 3: Comparison of Open† and Close† LLMs as\nEvaluators. Kappa coefficients of Qwens increase when\nthe model scales up.\n4\nDiscussion\n4.1\nStrong LLMs are Good Error Locators.\nAs shown in Table 3, we compare the scoring kappa\ncoefficients with human assessments for different\nLLMs. We randomly select 300 model outputs\nequally from different domains and ask 3 human\nexperts to give perception, cognition, and action\nscores. The final result is based on the majority\nof three annotators. The result underscores a sig-\nnificant agreement between GPT-4 scoring and hu-\nman evaluation results. This is substantiated by\nCohen-Kappa coefficients of 0.71, 0.82, and 0.94\nfor perception, cognition, and action evaluations.\nWe also compare open models as evaluators.\nWe choose one of the best open LLMs, Qwen1.51\nseries from 7B, 14B to 72B version. Currently\nopen LLMs tend to give wrongly high judgments in\nall sub-scores. Although currently trailing behind\nGPT-4 in performance, we anticipate that with tar-\ngeted training focused on error identification and\nenhancements in the overall capabilities of open\nLLMs, these models will become more effective\nevaluation tools compared to closed models. This\nis primarily due to the reproducible and transpar-\nent nature of open models, which offer significant\nadvantages in the development of evaluation tools.\n1https:\/\/huggingface.co\/collections\/Qwen\n4.2\nGenuine PCA Score\nPCA-Eval could pinpoint cases where the MLLM\ngets the correct answer by a fluke where perception\nor cognition score is 0 but the action score is 1. It\nexplains why for some models, the action score is\nhigher than perception and cognition scores. For in-\nstance, a model might opt for a conservative action,\nsuch as slowing down, even without accurately rec-\nognizing snowy weather in the image, resulting in\na fluky correct action. In another scenario, if the\nmodel exhibits a preference for a specific choice\nindex, it will attain a high action score provided\nthat the evaluation dataset contains a substantial\nnumber of correct choices matching the preferred\nindex, a phenomenon attributable to the positional\nbiases inherent in both the model and the dataset.\nTo overcome the mentioned bias when evaluating\nthe genuine ability of MLLM, we propose a new\nmetric Genuine PCA Score. It is equal to one\nif the perception, cogntion and action scores are\nall 1 for one model’s response to a question. We\nfind that for all models, there exists significant gap\n(>10%) between the action score and genuine PCA\nscore in average, revealing that relying on single\nmetric such as choice accuracy is very problematic\nwhen conducting model evaluation. In our online\nleaderboard, both average action score and average\ngenuine PCA score are considered when ranking\nthe candidate models.\n4.3\nAlignment between Agent Decisions and\nHuman Values\nWe have observed instances where the decisions\nmade by the agent contradict human values. Con-\nsider the scenario depicted in Figure 9 from Ap-\npendix. The image illustrates a crosswalk with-\nout pedestrians. The appropriate response would\nbe slowing down, as caution is paramount when\napproaching a crosswalk, regardless of the pres-\nence or absence of pedestrians. However, upon\nprocessing the information that the crosswalk is\nempty, ChatGPT suggests that maintaining the cur-\nrent speed is the optimal action, arguing that the\nabsence of pedestrians eliminates the need to slow\ndown. The rationale provided by ChatGPT is logi-\ncal, yet it does not align with human values.\n5\nRelated Work\nMLLM Benchmark.\nIn recent times, there\nhave been several benchmarks built for evaluating\nMLLMs, such as MMBench, MME, Seed-Bench,\nPOPE (Liu et al., 2023e; Fu et al., 2023; Li et al.,\n2023a,e) that assess MLLMs performance from\nmultiple fine-grained dimensions.\nVisit-Bench,\nLVLM-eHub, M3IT (Bitton et al., 2023; Xu et al.,\n2023; Li et al., 2023c) focus on the general in-\nstruction following ability. General VQA tasks\nlike OKVQA, VQAv2, Vizwiz, ScienceQA, VSR\nand IconQA (Marino et al., 2019; Agrawal et al.,\n2015; Gurari et al., 2018; Lu et al., 2022; Liu et al.,\n2023a; Lu et al., 2021) focus on visual understand-\ning. MMMU, MathVista, LLaVA-benchmark and\nMM-Vet (Yue et al., 2023; Lu et al., 2023a; Liu\net al., 2023c; Yu et al., 2023) require abilities from\nthe vision part and specific knowledge in the lan-\nguage part. A lack of error localization techniques\nbeyond accuracy assessments is among current\nbenchmarks. This complicates identifying which\npart of the MLLM malfunctioned when making\nmistakes. Unlike prior work, PCA-Bench is more\nrelevant to evaluate MLLMs’ ability to utilize inte-\ngrated abilities to solve one task and make explain-\nable decisions via error localization.\nLLM Agent and Embodied Decision Making.\nUsing LLMs to empower the AI agents (Xi et al.,\n2023; Liu et al., 2023d; Park et al., 2023; Wang\net al., 2023d) becomes more and more promis-\ning. Specifically, we can employ LLMs to enhance\nthe decision making ability of the agents (Nakano\net al., 2022; Yao et al., 2022; Li et al., 2023d;\nSong et al., 2023; Li et al., 2023b), expanding\ntheir perception and action space through strate-\ngies like tool utilization (Schick et al., 2023; Qin\net al., 2023; Lu et al., 2023b). This line of research\ndivides the entire decision-making process into two\nphases: (1) information seeking, usually involving\nMLLMs to verbalize the current status of AI agents\nin the vision-based environment with natural lan-\nguage; (2) reasoning and planning with text-based\nLLMs to decide what the AI agent should do in\nthe next step with textual clues. Although LLM-\nbased agents demonstrate reasoning and planning\nabilities through techniques like Chain of Thought\nor problem decomposition (Wei et al., 2023; Yao\net al., 2023; Kojima et al., 2022), they inherently\nlack visual perception, and are limited to the dis-\ncrete textual content. Therefore, integrating mul-\ntimodal information can offer agents a broader\ncontext and a more precise understanding, such\nas PaLM-E (Driess et al., 2023), enhancing their\nenvironmental perception. However, there is still\nlarge gap deploying MLLM in various embodied\nenvironments due to the lack of appropriate bench-\nmark and interface linking those two domains while\nPCA-Bench is an attempt towards that goal.\n6\nConclusion\nIn this paper, we introduce PCA-Bench, a mul-\ntimodal benchmark designed to assess the inte-\ngrated decision-making capabilities of MLLMs.\nThis benchmark features PCA-EVAL, a novel fine-\ngrained automatic evaluation tool that diagnoses\ndecision making processes from three critical per-\nspectives: perception, cognition, and action. To\nenhance the decision making ability from data per-\nspective, we propose the Embodied Instruction Evo-\nlution method to automatically synthesize instruc-\ntion examples from different environments, which\nhas been proven effective in our main experiments.\nWe believe that powerful MLLMs pave a new and\npromising way toward decision making in embod-\nied environments and we hope PCA-Bench could\nserve as a good benchmark in evaluation and error\nlocalization for MLLMs’ development.\n7\nLimitations\nThe current scope of PCA-Bench is confined to\nmerely three domains in static environments. One\nof our future works aims to broaden this scope\nto encompass more domains and dynamic embod-\nied environments where MLLMs could keep get-\nting feedback, which is closer to real embodied\nAI scenarios.\nWe do not apply different infer-\nence enhancement methods like In-Context Learn-\ning and Reflection in the decision making process\nof MLLMs. We just use the simplest prompting\nmethod and leave the exploration of a better cross-\nmodal Chain-of-Thought method for future studies.\nCurrently, PCA-Eval shows the best consistency\nwith human evaluators when using powerful close\nLLM GPT4, which would bring additional cost to\nthe user of PCA-Eval. We plan to develop and re-\nlease an open error locator for error localization in\nthe benchmark in the future.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | PCA-Bench：评估多模态大语言模型在感知-认知-行动链中的决策能力\n\n## 📌 背景痛点\/本文动机\n随着多模态大语言模型（MLLMs）在处理复杂任务方面的能力日益增强，现有的评估基准往往只关注单个模型能力的评估，而忽略了模型在感知、认知和行动方面的综合能力。此外，现有的基准缺乏对模型错误进行定位的能力，这使得难以确定模型在哪些方面需要改进。\n\n## 🚀 核心方法\n💡 创新点1：PCA-Bench\n本文提出了PCA-Bench，这是一个用于评估MLLMs在感知-认知-行动链中决策能力的多模态决策基准。PCA-Bench引入了三个复杂的场景：自动驾驶、家庭机器人和开放世界游戏。在这些场景中，模型需要根据任务指令和不同的上下文，无缝地整合感知、认知和行动的能力，以做出准确的决策。\n\n💡 创新点2：PCA-Eval\n为了平衡评估的准确性和效率，本文提出了PCA-Eval，这是一个自动评估协议。PCA-Eval利用LLMs强大的语义解析能力，根据数据注释中的锚点信息，自动进行错误定位。实验结果表明，PCA-Eval与人类评估结果具有高度的一致性，平均Kappa系数达到0.8+。\n\n💡 创新点3：Embodied-Instruction-Evolution (EIE)\n为了解决PCA-Bench数据集标注工作量大的问题，本文提出了Embodied-Instruction-Evolution (EIE)框架。EIE利用LLMs自动合成多模态具身环境中的指令调整示例，从而减少了人工劳动，并提高了PCA-Bench的多样性和可扩展性。\n\n## 📈 实验结果\n实验结果表明，GPT-4 Vision在感知、认知和行动方面都表现出色，超过了现有的开源MLLMs。EIE方法能够显著提高开源MLLMs的性能，在某些指标上甚至超过了GPT-4 Vision。PCA-Eval能够有效地定位模型错误，从而提高了模型评估的可靠性。\n\n## 💬 可借鉴之处\n本文提出的PCA-Bench和PCA-Eval为评估MLLMs的决策能力提供了一个新的基准和评估工具。EIE框架为自动合成多模态具身环境中的指令调整示例提供了一种有效的方法。本文的研究结果表明，强大的MLLMs在具身智能体中的决策能力具有很大的潜力，为MLLMs的研究开辟了新的方向。","llm_summary_res_status":200}
