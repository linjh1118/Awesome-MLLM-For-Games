
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
      <th>query_answer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</td>
      <td>We present PCA-Bench, a multimodal decision-making benchmark for evaluating<br>the integrated capabilities of Multimodal Large Language Models (MLLMs).<br>Departing from previous benchmarks focusing on simplistic tasks and individual<br>model capability, PCA-Bench introduces three complex scenarios: autonomous<br>driving, domestic robotics, and open-world games. Given task instructions and<br>diverse contexts, the model is required to seamlessly integrate multiple<br>capabilities of Perception, Cognition, and Action in a reasoning chain to make<br>accurate decisions. Moreover, PCA-Bench features error localization<br>capabilities, scrutinizing model inaccuracies in areas such as perception,<br>knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To<br>balance accuracy and efficiency in evaluation, we propose PCA-Eval, an<br>automatic evaluation protocol, and assess 10 prevalent MLLMs. The results<br>reveal significant performance disparities between open-source models and<br>powerful proprietary models like GPT-4 Vision. To address this, we introduce<br>Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing<br>instruction tuning examples in multimodal embodied environments. EIE generates<br>7,510 training examples in PCA-Bench and enhances the performance of<br>open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision<br>accuracy), thereby validating the effectiveness of EIE. Our findings suggest<br>that robust MLLMs like GPT4-Vision show promise for decision-making in embodied<br>agents, opening new avenues for MLLM research.</td>
      <td>## 🌟 论文解读 | PCA-Bench：评估多模态大语言模型在感知-认知-行动链中的决策能力<br><br>## 📌 背景痛点/本文动机<br>随着多模态大语言模型（MLLMs）在处理复杂任务方面的能力日益增强，现有的评估基准往往只关注单个模型能力的评估，而忽略了模型在感知、认知和行动方面的综合能力。此外，现有的基准缺乏对模型错误进行定位的能力，这使得难以确定模型在哪些方面需要改进。<br><br>## 🚀 核心方法<br>💡 创新点1：PCA-Bench<br>本文提出了PCA-Bench，这是一个用于评估MLLMs在感知-认知-行动链中决策能力的多模态决策基准。PCA-Bench引入了三个复杂的场景：自动驾驶、家庭机器人和开放世界游戏。在这些场景中，模型需要根据任务指令和不同的上下文，无缝地整合感知、认知和行动的能力，以做出准确的决策。<br><br>💡 创新点2：PCA-Eval<br>为了平衡评估的准确性和效率，本文提出了PCA-Eval，这是一个自动评估协议。PCA-Eval利用LLMs强大的语义解析能力，根据数据注释中的锚点信息，自动进行错误定位。实验结果表明，PCA-Eval与人类评估结果具有高度的一致性，平均Kappa系数达到0.8+。<br><br>💡 创新点3：Embodied-Instruction-Evolution (EIE)<br>为了解决PCA-Bench数据集标注工作量大的问题，本文提出了Embodied-Instruction-Evolution (EIE)框架。EIE利用LLMs自动合成多模态具身环境中的指令调整示例，从而减少了人工劳动，并提高了PCA-Bench的多样性和可扩展性。<br><br>## 📈 实验结果<br>实验结果表明，GPT-4 Vision在感知、认知和行动方面都表现出色，超过了现有的开源MLLMs。EIE方法能够显著提高开源MLLMs的性能，在某些指标上甚至超过了GPT-4 Vision。PCA-Eval能够有效地定位模型错误，从而提高了模型评估的可靠性。<br><br>## 💬 可借鉴之处<br>本文提出的PCA-Bench和PCA-Eval为评估MLLMs的决策能力提供了一个新的基准和评估工具。EIE框架为自动合成多模态具身环境中的指令调整示例提供了一种有效的方法。本文的研究结果表明，强大的MLLMs在具身智能体中的决策能力具有很大的潜力，为MLLMs的研究开辟了新的方向。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是PCA-Bench，这是一个用于评估多模态大语言模型（MLLMs）在感知-认知-行动链中决策能力的多模态决策基准。PCA-Bench引入了三个复杂的场景：自动驾驶、家庭机器人和开放世界游戏。在这些场景中，模型需要根据任务指令和不同的上下文，无缝地整合感知、认知和行动的能力，以做出准确的决策。此外，PCA-Bench还具备错误定位能力，可以分析模型在感知、知识或推理等方面的不准确之处，从而提高模型部署的可靠性。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并没有明确说明PCA-Bench需要什么设备条件。然而，由于PCA-Bench是一个用于评估多模态大语言模型的基准，因此它可能需要高性能的计算设备，例如具有多个GPU和大量内存的服务器。此外，由于PCA-Bench涉及图像处理和自然语言处理等任务，因此它可能还需要具有强大计算能力的机器学习框架和库。<br><br>至于本文的模型训练和推理所使用的设备，论文中也没有明确说明。然而，由于论文中提到的模型包括GPT-4 Vision等大型模型，因此它们可能需要高性能的计算设备，例如具有多个GPU和大量内存的服务器。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>PCA-Bench是一个用于评估多模态大语言模型（MLLMs）在感知-认知-行动链中决策能力的多模态决策基准。它引入了三个复杂的场景：自动驾驶、家庭机器人和开放世界游戏。在这些场景中，模型需要根据任务指令和不同的上下文，无缝地整合感知、认知和行动的能力，以做出准确的决策。此外，PCA-Bench还具备错误定位能力，可以分析模型在感知、知识或推理等方面的不准确之处，从而提高模型部署的可靠性。<br><br>PCA-Bench的评估协议PCA-Eval是一个基于锚点的评估协议，它利用LLMs强大的语义解析能力，根据数据注释中的锚点信息，自动进行错误定位。实验结果表明，PCA-Eval与人类评估结果具有高度的一致性，平均Kappa系数达到0.8+。PCA-Eval能够有效地定位模型错误，从而提高了模型评估的可靠性。<br><br>PCA-Bench还引入了Embodied-Instruction-Evolution (EIE)框架，它利用LLMs自动合成多模态具身环境中的指令调整示例，从而减少了人工劳动，并提高了PCA-Bench的多样性和可扩展性。EIE框架为自动合成多模态具身环境中的指令调整示例提供了一种有效的方法。<br><br>PCA-Bench的实验结果表明，GPT-4 Vision在感知、认知和行动方面都表现出色，超过了现有的开源MLLMs。EIE方法能够显著提高开源MLLMs的性能，在某些指标上甚至超过了GPT-4 Vision。PCA-Eval能够有效地定位模型错误，从而提高了模型评估的可靠性。<br><br>综上所述，PCA-Bench是一个具有高质量评估协议和错误定位能力的基准，它能够有效地评估多模态大语言模型的决策能力。PCA-Bench的引入为评估MLLMs的决策能力提供了一个新的基准和评估工具，并为MLLMs的研究开辟了新的方向。</td>
    </tr>
    <tr>
      <th>1</th>
      <td>DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments</td>
      <td>Large Language Model~(LLM) based agents have been increasingly popular in<br>solving complex and dynamic tasks, which requires proper evaluation systems to<br>assess their capabilities. Nevertheless, existing benchmarks usually either<br>focus on single-objective tasks or use overly broad assessing metrics, failing<br>to provide a comprehensive inspection of the actual capabilities of LLM-based<br>agents in complicated decision-making tasks. To address these issues, we<br>introduce DSGBench, a more rigorous evaluation platform for strategic<br>decision-making. Firstly, it incorporates six complex strategic games which<br>serve as ideal testbeds due to their long-term and multi-dimensional<br>decision-making demands and flexibility in customizing tasks of various<br>difficulty levels or multiple targets. Secondly, DSGBench employs a<br>fine-grained evaluation scoring system which examines the decision-making<br>capabilities by looking into the performance in five specific dimensions and<br>offering a comprehensive assessment in a well-designed way. Furthermore,<br>DSGBench also incorporates an automated decision-tracking mechanism which<br>enables in-depth analysis of agent behaviour patterns and the changes in their<br>strategies. We demonstrate the advances of DSGBench by applying it to multiple<br>popular LLM-based agents and our results suggest that DSGBench provides<br>valuable insights in choosing LLM-based agents as well as improving their<br>future development. DSGBench is available at<br>https://github.com/DeciBrain-Group/DSGBench.</td>
      <td>## 🌟 论文解读 | DSGBench：评估大型语言模型在复杂决策环境中的战略决策能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）在解决复杂和动态任务方面的能力日益增强，评估这些模型在复杂决策任务中的实际能力变得至关重要。然而，现有的评估系统通常只关注单一目标任务或使用过于宽泛的评估指标，无法全面评估LLM模型在复杂决策任务中的实际能力。<br><br>## 🚀 核心方法<br>💡 创新点1：DSGBench是一个更严格的评估平台，用于评估战略决策能力。它包含了六个复杂的战略游戏，这些游戏因其长期和多维度的决策需求以及定制各种难度级别或多个目标的任务的灵活性而成为理想的测试平台。<br><br>💡 创新点2：DSGBench采用了一种细粒度的评估评分系统，通过考察在五个特定维度中的表现来检查决策能力，并以一种精心设计的方式提供全面的评估。此外，DSGBench还包含一个自动化的决策跟踪机制，能够深入分析代理的行为模式和策略的变化。<br><br>## 📈 实验结果<br>DSGBench通过应用于多个流行的LLM模型，展示了其在选择LLM模型以及改进其未来发展方面的价值。实验结果表明，DSGBench能够提供有价值的见解，帮助研究人员更好地理解LLM模型在不同决策环境中的表现。<br><br>## 💬 可借鉴之处<br>DSGBench为评估LLM模型在复杂决策环境中的战略决策能力提供了一个全面的框架。其细粒度的评估指标和决策跟踪机制可以帮助研究人员深入了解LLM模型的行为模式和策略变化，从而更好地改进模型的设计和开发。此外，DSGBench的灵活性和可定制性使其能够适应不同的评估需求，为LLM模型的研究和应用提供了有力的支持。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark名为DSGBench，它是一个用于评估大型语言模型（LLM）在复杂决策环境中战略决策能力的平台。DSGBench包含了六个复杂的战略游戏，这些游戏因其长期和多维度的决策需求以及定制各种难度级别或多个目标的任务的灵活性而成为理想的测试平台。DSGBench采用了一种细粒度的评估评分系统，通过考察在五个特定维度中的表现来检查决策能力，并以一种精心设计的方式提供全面的评估。此外，DSGBench还包含一个自动化的决策跟踪机制，能够深入分析代理的行为模式和策略的变化。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确指出DSGBench运行所需的设备条件，例如GPU数量和内存大小。通常这类评估平台需要较高的计算资源，特别是当涉及到多个LLM模型和复杂游戏环境时。至于本文中模型训练和推理所使用的设备，论文中也没有提供具体信息。不过，考虑到LLM模型的计算需求，通常需要高性能的计算集群，配备多块GPU和足够的内存。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>DSGBench采用了细粒度的评估评分系统，通过考察在五个特定维度中的表现来检查决策能力，并以一种精心设计的方式提供全面的评估。这种评估方式不仅关注结果，也关注过程，从而减少了reward hacking的可能性。此外，DSGBench还包含一个自动化的决策跟踪机制，能够深入分析代理的行为模式和策略的变化，这为RL类模型提供了更多的学习机会。因此，DSGBench的环境设计有利于RL类模型在这个benchmark上表现出色。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs</td>
      <td>The rapid advancement of large language models (LLMs) has accelerated their<br>application in reasoning, with strategic reasoning drawing increasing<br>attention. To evaluate LLMs' strategic reasoning capabilities, game theory,<br>with its concise structure, has become a preferred approach. However, current<br>research focuses on a limited selection of games, resulting in low coverage.<br>Classic game scenarios risk data leakage, and existing benchmarks often lack<br>extensibility, making them inadequate for evaluating state-of-the-art models.<br>To address these challenges, we propose TMGBench, a benchmark with<br>comprehensive game type coverage, novel scenarios, and flexible organization.<br>Specifically, we incorporate all 144 game types summarized by the<br>Robinson-Goforth topology of 2x2 games, constructed as classic games. We also<br>employ synthetic data generation to create diverse, higher-quality scenarios<br>through topic guidance and human inspection, referred to as story-based games.<br>Lastly, we provide a sustainable framework for increasingly powerful LLMs by<br>treating these games as atomic units and organizing them into more complex<br>forms via sequential, parallel, and nested structures. Our comprehensive<br>evaluation of mainstream LLMs covers tests on rational reasoning, robustness,<br>Theory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in<br>accuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,<br>OpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and<br>70.0% on sequential, parallel, and nested games, highlighting TMGBench's<br>challenges.</td>
      <td>## 🌟 论文解读 | TMGBench：评估大型语言模型战略推理能力的系统游戏基准<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，它们在推理任务中的应用日益广泛，其中战略推理能力尤其受到关注。然而，现有的研究往往局限于少数经典游戏，导致游戏类型的覆盖率低，且经典游戏场景存在数据泄露的风险。此外，现有的基准测试往往缺乏可扩展性，难以评估最先进的模型。为了解决这些问题，本文提出了TMGBench，一个具有全面游戏类型覆盖率、新颖场景和灵活组织方式的基准测试。<br><br>## 🚀 核心方法<br>💡 创新点1：全面的游戏类型覆盖率<br>TMGBench包含了由Robinson-Goforth拓扑结构总结的144种2x2游戏类型，涵盖了各种不同的游戏结构，包括经典游戏如囚徒困境等。<br><br>💡 创新点2：新颖的场景<br>为了解决经典游戏场景的数据泄露问题，TMGBench采用了合成数据生成技术，为每种经典游戏创建了五个不同的基于故事的场景，这些场景涵盖了商业、法律、交通等现实生活中的主题。<br><br>💡 创新点3：灵活的游戏组织方式<br>TMGBench将游戏视为原子单位，并通过顺序、并行和嵌套结构将它们组织成更复杂的游戏形式，以评估LLMs在并行、顺序和多层级决策方面的战略推理能力。<br><br>## 📈 实验结果<br>本文对主流LLMs进行了全面评估，包括理性推理、推理鲁棒性、心智理论（ToM）能力和复杂游戏形式的推理。结果表明，LLMs在战略推理过程的准确性和一致性方面仍存在缺陷，且对ToM的掌握程度也各不相同。OpenAI的最新推理模型o1-mini在顺序、并行和嵌套游戏上的准确率分别为66.6%、60.0%和70.0%，突显了TMGBench的挑战性。<br><br>## 💬 可借鉴之处<br>TMGBench为评估LLMs的战略推理能力提供了一个全面的基准测试，其创新点包括全面的游戏类型覆盖率、新颖的场景和灵活的游戏组织方式。此外，本文还揭示了LLMs在战略推理方面的缺陷，并提出了改进方向，为LLMs的研究和应用提供了有价值的参考。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一个名为TMGBench的基准测试，用于评估大型语言模型（LLMs）的战略推理能力。TMGBench具有以下特点：<br><br>* **全面的游戏类型覆盖率**：包含了由Robinson-Goforth拓扑结构总结的144种2x2游戏类型，涵盖了各种不同的游戏结构，包括经典游戏如囚徒困境等。<br>* **新颖的场景**：为了解决经典游戏场景的数据泄露问题，TMGBench采用了合成数据生成技术，为每种经典游戏创建了五个不同的基于故事的场景，这些场景涵盖了商业、法律、交通等现实生活中的主题。<br>* **灵活的游戏组织方式**：TMGBench将游戏视为原子单位，并通过顺序、并行和嵌套结构将它们组织成更复杂的游戏形式，以评估LLMs在并行、顺序和多层级决策方面的战略推理能力。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明TMGBench需要什么设备条件。但是，由于TMGBench包含了大量的游戏类型和场景，并且需要进行复杂的推理和评估，因此需要高性能的计算设备，例如多GPU服务器和大量内存。<br><br>论文中提到，模型训练和推理使用了OpenAI的GPT-4o模型，该模型是在高性能计算设备上训练的。具体使用的设备信息没有在论文中说明。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确说明TMGBench是否具有高质量的结果奖励或过程奖励。但是，由于TMGBench是基于游戏理论设计的，其奖励机制与游戏的结果和过程密切相关，因此不容易出现reward hacking的问题。<br><br>此外，TMGBench的复杂性和多样性也为RL类模型提供了挑战和机遇，可以促进RL类模型在战略推理方面的研究和应用。</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games</td>
      <td>Recent advancements in large language models (LLMs) have expanded their<br>capabilities beyond traditional text-based tasks to multimodal domains,<br>integrating visual, auditory, and textual data. While multimodal LLMs have been<br>extensively explored for high-level planning in domains like robotics and<br>games, their potential as low-level controllers remains largely untapped. In<br>this paper, we introduce a novel benchmark aimed at testing the emergent<br>capabilities of multimodal LLMs as low-level policies in Atari games. Unlike<br>traditional reinforcement learning (RL) methods that require training for each<br>new environment and reward function specification, these LLMs utilize<br>pre-existing multimodal knowledge to directly engage with game environments.<br>Our study assesses the performances of multiple multimodal LLMs against<br>traditional RL agents, human players, and random agents, focusing on their<br>ability to understand and interact with complex visual scenes and formulate<br>strategic responses. Our results show that these multimodal LLMs are not yet<br>capable of being zero-shot low-level policies. Furthermore, we see that this<br>is, in part, due to their visual and spatial reasoning. Additional results and<br>videos are available on our project webpage:<br>https://dev1nw.github.io/atari-gpt/.</td>
      <td>## 🌟 论文解读 | Atari-GPT：评估多模态大型语言模型在Atari游戏中的低级策略能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的突破，它们的能力已经扩展到了多模态领域，整合了视觉、听觉和文本数据。尽管多模态LLMs在机器人技术和游戏等领域的决策规划方面得到了广泛探索，但它们作为低级控制器的潜力尚未得到充分挖掘。本文旨在通过引入一个新的基准，测试多模态LLMs在Atari游戏中的低级策略能力，以填补这一研究空白。<br><br>## 🚀 核心方法<br>💡 创新点1：提出Atari-GPT基准<br>本文提出了Atari-GPT基准，旨在评估多模态LLMs在Atari游戏中的低级策略能力。该基准通过比较LLMs与传统强化学习（RL）代理、人类玩家和随机代理的性能，评估它们在理解复杂视觉场景和制定战略反应方面的能力。<br><br>💡 创新点2：评估视觉和空间推理能力<br>除了游戏性能评估，本文还通过一系列提示测试了LLMs的视觉理解、空间推理和战略直觉能力。这些测试旨在揭示LLMs在理解游戏环境方面的局限性，并为进一步改进提供方向。<br><br>## 📈 实验结果<br>实验结果表明，尽管LLMs在Atari游戏中的表现不如人类玩家或专门的RL模型，但它们仍然能够识别和与游戏帧中的关键元素进行交互。然而，它们作为低级控制器的性能仍然不佳，这可能是由于缺乏针对此任务的训练以及空间推理的困难。此外，实验还发现，LLMs的推理时间对于实时决策至关重要，而目前的多模态模型在速度方面仍存在不足。<br><br>## 💬 可借鉴之处<br>本文的研究结果为LLMs在低级控制任务中的应用提供了宝贵的见解。尽管LLMs尚未达到与人类或RL模型相匹配的水平，但它们在Atari游戏中的表现仍然值得注意。这表明LLMs具有适应性和潜力，可以扩展到其原始训练范围之外，为未来在需要详细环境交互和决策的应用中作为更通用的低级控制器提供了一瞥。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一个名为Atari-GPT的基准，旨在测试多模态大型语言模型（LLMs）在Atari游戏中的低级策略能力。这个基准通过比较LLMs与传统强化学习（RL）代理、人类玩家和随机代理的性能，评估它们在理解复杂视觉场景和制定战略反应方面的能力。实验结果表明，尽管LLMs在Atari游戏中的表现不如人类玩家或专门的RL模型，但它们仍然能够识别和与游戏帧中的关键元素进行交互。然而，它们作为低级控制器的性能仍然不佳，这可能是由于缺乏针对此任务的训练以及空间推理的困难。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确说明Atari-GPT基准所需的设备条件，例如GPU数量或内存大小。然而，由于LLMs的推理过程计算密集，因此需要高性能的计算设备来支持实时决策。论文中提到，实验使用了GPT-4V Turbo、GPT-4o、Gemini 1.5 Flash和Claude 3 Haiku等模型，这些模型在推理速度方面有所不同。例如，GPT-4o、Gemini 1.5 Flash和Claude 3 Haiku的平均推理时间在2-3秒之间，而GPT-4 Turbo的推理时间在5-7秒之间。因此，为了进行实时决策，需要选择推理速度较快的模型，并配备相应的计算设备。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>Atari-GPT基准使用的是Atari游戏环境，这些游戏通常具有明确的结果奖励，例如得分。然而，论文中并未提及是否使用了过程奖励。对于RL类模型来说，高质量的结果奖励和过程奖励都是重要的，因为它们可以帮助模型更好地学习游戏策略。尽管Atari游戏环境提供了结果奖励，但论文中的实验结果表明，LLMs在Atari游戏中的表现仍然不如人类玩家或专门的RL模型。这可能是因为LLMs缺乏针对此任务的训练以及空间推理的困难。因此，即使Atari游戏环境提供了结果奖励，LLMs在Atari-GPT基准上的表现仍然有待提高。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</td>
      <td>The advancement of Offline Reinforcement Learning (RL) and Offline<br>Multi-Agent Reinforcement Learning (MARL) critically depends on the<br>availability of high-quality, pre-collected offline datasets that represent<br>real-world complexities and practical applications. However, existing datasets<br>often fall short in their simplicity and lack of realism. To address this gap,<br>we propose Hokoff, a comprehensive set of pre-collected datasets that covers<br>both offline RL and offline MARL, accompanied by a robust framework, to<br>facilitate further research. This data is derived from Honor of Kings, a<br>recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate<br>nature, closely resembling real-life situations. Utilizing this framework, we<br>benchmark a variety of offline RL and offline MARL algorithms. We also<br>introduce a novel baseline algorithm tailored for the inherent hierarchical<br>action space of the game. We reveal the incompetency of current offline RL<br>approaches in handling task complexity, generalization and multi-task learning.</td>
      <td>## 🌟 论文解读 | Hokoff：基于王者荣耀的真实游戏数据集及其离线强化学习基准<br><br>## 📌 背景痛点/本文动机<br>离线强化学习（Offline RL）和离线多智能体强化学习（Offline MARL）的研究进展依赖于高质量、预先收集的离线数据集，这些数据集应代表现实世界的复杂性和实际应用。然而，现有的数据集往往过于简单，缺乏现实性。为了解决这个问题，本文提出了Hokoff，这是一套全面的预先收集的数据集，涵盖了离线RL和离线MARL，并伴随着一个强大的框架，以促进进一步的研究。这些数据来自王者荣耀，这是一款以其复杂性质而闻名的多人在线战斗竞技场（MOBA）游戏，与现实生活情况非常相似。<br><br>## 🚀 核心方法<br>💡 创新点1：基于王者荣耀的复杂环境<br>Hokoff的数据集来源于王者荣耀，这是一款拥有超过1亿日活跃玩家的全球最受欢迎的MOBA游戏之一。该游戏的复杂性远远超过其他数据集，展示了模拟现实世界场景的潜力。<br><br>💡 创新点2：开源、易用的框架<br>本文提出了一个开源、易用的框架，该框架包括离线RL（采样、训练和评估）的全面流程和一些有用的工具。基于该框架，我们发布了一系列丰富多样的数据集，这些数据集使用一系列具有不同设计因素的前训练模型生成，不仅适用于离线RL，也适用于离线MARL。<br><br>💡 创新点3：多级模型<br>为了确保不同算法的性能比较的有效性和公正性，本文提出了多级模型，包含多个具有不同水平的检查点。这些模型可以用于采样和评估，从而更准确地评估算法的性能。<br><br>💡 创新点4：多样化的数据集<br>Hokoff提供了多样化的数据集，包括多难度、多任务、泛化、异构队友和子任务等。这些数据集旨在模拟现实世界的复杂性和多样性，为离线RL和离线MARL的研究提供更真实的环境。<br><br>## 📈 实验结果<br>本文在Hokoff数据集上评估了多种离线RL和离线MARL算法，并提出了一个针对王者荣耀固有层次结构动作空间的新的基线算法。结果表明，当前的离线RL方法在处理任务复杂性、泛化和多任务学习方面存在不足。<br><br>## 💬 可借鉴之处<br>Hokoff数据集和框架为离线RL和离线MARL的研究提供了宝贵的资源。其多样化的数据集和强大的框架可以帮助研究人员更好地理解和评估离线学习算法的性能，并为解决现实世界问题提供新的思路和方法。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是Hokoff，它是一套基于王者荣耀游戏环境的离线强化学习（Offline RL）和离线多智能体强化学习（Offline MARL）数据集。Hokoff数据集来源于王者荣耀，这是一款拥有超过1亿日活跃玩家的全球最受欢迎的MOBA游戏之一。该游戏的复杂性远远超过其他数据集，展示了模拟现实世界场景的潜力。Hokoff数据集涵盖了多种难度、多任务、泛化、异构队友和子任务等，旨在模拟现实世界的复杂性和多样性，为离线RL和离线MARL的研究提供更真实的环境。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确指出Hokoff benchmark所需的设备条件，但根据论文内容，我们可以推测其所需的设备条件较高。由于王者荣耀游戏环境的复杂性，以及数据集的多样性，训练和推理过程可能需要大量的计算资源。因此，建议使用高性能的计算机，配备多个GPU和足够的内存。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>Hokoff benchmark的环境设计考虑了reward hacking的问题。王者荣耀游戏环境的奖励机制是零和可调奖励MOBA游戏，游戏回合的回报受到对手和游戏设置的影响很大，目标是获胜。因此，使用回合回报作为性能指标可能存在偏差。为了解决这个问题，Hokoff benchmark采用了多级模型，包含多个具有不同水平的检查点。这些模型可以用于采样和评估，从而更准确地评估算法的性能。此外，Hokoff benchmark还提供了多样化的数据集，包括多难度、多任务、泛化、异构队友和子任务等，旨在模拟现实世界的复杂性和多样性，为离线RL和离线MARL的研究提供更真实的环境。</td>
    </tr>
    <tr>
      <th>5</th>
      <td>A Benchmark Environment for Offline Reinforcement Learning in Racing Games</td>
      <td>Offline Reinforcement Learning (ORL) is a promising approach to reduce the<br>high sample complexity of traditional Reinforcement Learning (RL) by<br>eliminating the need for continuous environmental interactions. ORL exploits a<br>dataset of pre-collected transitions and thus expands the range of application<br>of RL to tasks in which the excessive environment queries increase training<br>time and decrease efficiency, such as in modern AAA games. This paper<br>introduces OfflineMania a novel environment for ORL research. It is inspired by<br>the iconic TrackMania series and developed using the Unity 3D game engine. The<br>environment simulates a single-agent racing game in which the objective is to<br>complete the track through optimal navigation. We provide a variety of datasets<br>to assess ORL performance. These datasets, created from policies of varying<br>ability and in different sizes, aim to offer a challenging testbed for<br>algorithm development and evaluation. We further establish a set of baselines<br>for a range of Online RL, ORL, and hybrid Offline to Online RL approaches using<br>our environment.</td>
      <td>## 🌟 论文解读 | 离线强化学习在赛车游戏中的新基准环境<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）在游戏行业中的应用日益广泛，但传统RL的高样本复杂度限制了其在复杂环境中的应用，如现代AAA游戏。离线强化学习（ORL）通过利用预先收集的过渡数据集来减少对环境交互的需求，从而降低了样本复杂度。然而，目前缺乏专门针对游戏环境，特别是赛车游戏，的ORL数据集和基准环境。<br><br>## 🚀 核心方法<br>💡 创新点1：引入OfflineMania环境<br>OfflineMania是一个基于Unity 3D游戏引擎开发的单代理赛车游戏环境，灵感来源于TrackMania系列游戏。该环境模拟了赛车游戏，目标是通过最优导航完成赛道。<br><br>💡 创新点2：提供多样化的数据集<br>为了评估ORL的性能，论文提供了多种数据集，这些数据集由不同能力水平的策略生成，并具有不同的规模。这些数据集旨在为算法开发和评估提供一个具有挑战性的测试平台。<br><br>💡 创新点3：建立基线<br>论文使用OfflineMania环境，为一系列在线RL、ORL和混合离线到在线RL方法建立了基线。这些基线有助于评估不同算法的性能，并促进ORL在游戏环境中的应用。<br><br>## 📈 实验结果<br>实验结果表明，IQL在大多数数据集上表现优于TD3BC和CQL。此外，SDBG和JSRL在离线到在线RL方法中表现出色，能够有效提高离线训练的政策的性能。<br><br>## 💬 可借鉴之处<br>OfflineMania环境和提供的数据集为ORL研究提供了一个有价值的测试平台。论文中的基线结果为评估不同ORL算法的性能提供了参考。此外，论文还探讨了离线到在线RL方法在游戏环境中的应用，为游戏开发人员提供了新的思路。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br>这篇论文提出了一个名为OfflineMania的基准环境，用于离线强化学习（ORL）研究。OfflineMania是一个基于Unity 3D游戏引擎开发的单代理赛车游戏环境，灵感来源于TrackMania系列游戏。该环境模拟了赛车游戏，目标是通过最优导航完成赛道。为了评估ORL的性能，论文提供了多种数据集，这些数据集由不同能力水平的策略生成，并具有不同的规模。这些数据集旨在为算法开发和评估提供一个具有挑战性的测试平台。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br>论文中提到，所有实验都是在配备Nvidia RTX 2070 GPU和AMD Ryzen 3600X处理器的系统上进行的。至于具体的设备条件，论文没有明确说明，但根据实验结果，我们可以推测，OfflineMania环境可能需要一定性能的GPU和CPU来支持训练和推理。例如，对于在线强化学习（Online RL）方法，PPO训练需要15百万环境交互和大约10小时的训练时间，而SAC训练需要3百万环境交互和大约20小时的训练时间。对于离线强化学习（ORL）方法，所有算法在300,000网络更新后大约需要1小时的训练时间。对于离线到在线强化学习（Offline to Online RL）方法，所有算法在300,000离线网络更新和1百万在线更新后大约需要4小时的训练时间。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br>OfflineMania环境使用了一个基于赛车游戏进展的奖励函数，该函数考虑了赛车在赛道上的位置和速度。奖励函数的设计旨在鼓励赛车通过最优导航完成赛道，同时惩罚与墙壁的碰撞。这种奖励函数的设计可以减少reward hacking的可能性，因为它不仅关注最终结果，还关注赛车在赛道上的行为。因此，OfflineMania环境可以支持RL类模型在这个benchmark上大放异彩，因为它提供了一个具有挑战性的测试平台，可以评估不同ORL算法的性能。</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</td>
      <td>StarCraft II is a challenging benchmark for AI agents due to the necessity of<br>both precise micro level operations and strategic macro awareness. Previous<br>works, such as Alphastar and SCC, achieve impressive performance on tackling<br>StarCraft II , however, still exhibit deficiencies in long term strategic<br>planning and strategy interpretability. Emerging large language model (LLM)<br>agents, such as Voyage and MetaGPT, presents the immense potential in solving<br>intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs<br>on StarCraft II, a highly complex RTS game.To conveniently take full advantage<br>of LLMs` reasoning abilities, we first develop textual StratCraft II<br>environment, called TextStarCraft II, which LLM agent can interact. Secondly,<br>we propose a Chain of Summarization method, including single frame<br>summarization for processing raw observations and multi frame summarization for<br>analyzing game information, providing command recommendations, and generating<br>strategic decisions. Our experiment consists of two parts: first, an evaluation<br>by human experts, which includes assessing the LLMs`s mastery of StarCraft II<br>knowledge and the performance of LLM agents in the game; second, the in game<br>performance of LLM agents, encompassing aspects like win rate and the impact of<br>Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the<br>relevant knowledge and complex planning abilities needed to address StarCraft<br>II scenarios; 2. Human experts consider the performance of LLM agents to be<br>close to that of an average player who has played StarCraft II for eight years;<br>3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)<br>difficulty level. We have open sourced the code and released demo videos of LLM<br>agent playing StarCraft II.</td>
      <td>## 🌟 论文解读 | 大型语言模型在星际争霸II中的表现：基准测试与摘要链方法<br><br>## 📌 背景痛点/本文动机<br>星际争霸II（StarCraft II）是一款极具挑战性的实时战略游戏，要求玩家在微观操作和宏观战略规划之间取得平衡。尽管之前的AI研究，如AlphaStar和SCC，在星际争霸II中取得了令人印象深刻的成果，但它们在长期战略规划和策略可解释性方面仍存在不足。随着大型语言模型（LLM）在解决复杂任务方面的潜力日益显现，本文旨在验证LLM在星际争霸II中的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：TextStarCraft II环境<br>为了充分利用LLM的推理能力，本文开发了一个名为TextStarCraft II的文本环境，LLM代理可以与之交互。该环境将星际争霸II的复杂游戏动态转换为文本格式，允许LLM代理通过语言命令执行宏观战略行动。<br><br>💡 创新点2：摘要链（CoS）方法<br>本文提出了摘要链（CoS）方法，包括单帧摘要和多帧摘要。单帧摘要用于处理原始观察数据，而多帧摘要用于分析游戏信息，提供命令建议并生成战略决策。CoS方法通过信息压缩、推理加速和全局理解，增强了LLM代理在处理复杂信息和做出战略决策方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，LLM具备解决星际争霸II场景所需的相关知识和复杂规划能力。人类专家认为，LLM代理在游戏中的表现接近于玩了八年星际争霸II的平均玩家。此外，LLM代理能够在Harder（Lv5）难度级别下击败内置AI。<br><br>## 💬 可借鉴之处<br>本文提出的TextStarCraft II环境和CoS方法为评估LLM在实时战略决策和长期规划方面的能力提供了一个新的基准。此外，本文的研究结果表明，LLM在解决复杂任务方面具有巨大潜力，并为未来在星际争霸II和其他实时战略游戏中的AI研究提供了有价值的见解。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是TextStarCraft II，这是一个专门为评估大型语言模型（LLM）在星际争霸II中的实时战略决策和长期规划能力而设计的文本环境。TextStarCraft II利用python-sc2框架将星际争霸II的复杂游戏动态转换为文本格式，允许LLM代理通过语言命令执行宏观战略行动。这个环境填补了现有星际争霸II环境中缺乏语言支持的空白，为评估LLM在实时战略决策和长期规划方面的能力提供了一个新的基准。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中提到，TextStarCraft II环境在家庭PC上进行了测试，只需要4GB的GPU内存。这意味着这个benchmark可以在相对低配置的设备上运行。至于模型训练和推理所使用的设备，论文中并没有明确说明。但是，根据论文中提到的实验结果，可以推测模型训练可能需要较高配置的设备，例如多GPU服务器，而推理则可以在家庭PC上进行。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>TextStarCraft II环境提供了一个奖励函数R，用于将代理行为与游戏目标对齐。该奖励函数根据比赛结果（输、平、赢）分别赋予{-1, 0, 1}的值。这种奖励机制可以有效地评估代理在游戏中的表现，并鼓励代理采取有利于获胜的策略。然而，论文中并没有详细说明奖励函数的具体设计，因此无法确定它是否能够完全避免reward hacking。尽管如此，TextStarCraft II环境为RL类模型提供了一个新的测试平台，有助于推动实时战略决策和长期规划方面的研究。</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI</td>
      <td>We introduce a visually-guided and physics-driven task-and-motion planning<br>benchmark, which we call the ThreeDWorld Transport Challenge. In this<br>challenge, an embodied agent equipped with two 9-DOF articulated arms is<br>spawned randomly in a simulated physical home environment. The agent is<br>required to find a small set of objects scattered around the house, pick them<br>up, and transport them to a desired final location. We also position containers<br>around the house that can be used as tools to assist with transporting objects<br>efficiently. To complete the task, an embodied agent must plan a sequence of<br>actions to change the state of a large number of objects in the face of<br>realistic physical constraints. We build this benchmark challenge using the<br>ThreeDWorld simulation: a virtual 3D environment where all objects respond to<br>physics, and where can be controlled using fully physics-driven navigation and<br>interaction API. We evaluate several existing agents on this benchmark.<br>Experimental results suggest that: 1) a pure RL model struggles on this<br>challenge; 2) hierarchical planning-based agents can transport some objects but<br>still far from solving this task. We anticipate that this benchmark will<br>empower researchers to develop more intelligent physics-driven robots for the<br>physical world.</td>
      <td>## 🌟 论文解读 | 3DWorld Transport Challenge：物理世界中的智能机器人挑战<br><br>## 📌 背景痛点/本文动机<br>随着人工智能和机器人技术的不断发展，能够在物理世界中感知和行动的机器人成为了计算机视觉和机器人社区的重要目标。然而，直接使用真实机器人进行训练和评估成本高昂且存在安全风险。因此，近年来，人们开始将模拟器纳入训练和评估人工智能算法的过程中。尽管3D虚拟环境在视觉导航方面取得了显著进展，但它们大多关注视觉导航，而忽略了物理交互。由于最终目标是开发能够在物理环境中感知和行动的系统，因此物理交互已成为家庭助理机器人训练的必要组成部分。<br><br>## 🚀 核心方法<br>本文提出了一个新的具身AI挑战：一个具有两个9自由度关节臂的具身智能体被随机放置在一个物理真实的虚拟家庭环境中。智能体需要探索房屋，寻找散落在不同房间中的少量物体，并将它们运送到一个期望的最终位置。此外，房屋周围还放置了各种容器，智能体可以找到这些容器并将物体放入其中。不使用容器作为工具时，智能体只能一次运输两个物体。然而，使用容器，智能体可以收集多个物体并一起运输。<br><br>为了支持这项挑战，本文创建了一个基于TDW的房屋数据集，其中包含充满物理响应物体的多房间环境。此外，还开发了一个完全基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。由于模拟动作和环境完全基于物理，与之前的非物理或部分物理虚拟环境相比，它们提出了额外的挑战。例如，交互动作只有在目标物理可达时（即靠近且未被阻挡）才会成功。如果目标不在智能体的中心视图中，或者直接路径被阻挡（例如，被桌子阻挡），智能体就无法成功抓取物体。此外，与房屋中的物体发生物理碰撞也可能显著阻碍运输进度。因此，智能体必须学习利用视觉信号来同步导航和操作，以应对这些物理约束。<br><br>## 📈 实验结果<br>本文评估了几个现有的智能体，实验结果表明，现有的具身智能体在完成这项任务方面都存在困难。本文相信，在运输挑战中表现良好的模型将能够使机器人更加智能，能够在真实的物理世界中发挥作用。<br><br>## 💬 可借鉴之处<br>本文提出的3DWorld Transport Challenge为具身智能体在物理真实环境中的任务和运动规划能力提供了一个新的评估标准。此外，本文还开发了一个完全基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。这些成果为开发能够在物理世界中感知和行动的智能机器人提供了新的思路和方法。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是“ThreeDWorld Transport Challenge”，它是一个视觉引导的任务和运动规划基准，用于评估具身智能体在物理真实环境中的能力。在这个挑战中，一个装备有两个9自由度关节臂的具身智能体被随机放置在一个模拟的物理家庭环境中。智能体需要探索房屋，寻找散落在不同房间中的少量物体，并将它们运送到一个期望的最终位置。此外，房屋周围还放置了各种容器，智能体可以找到这些容器并将物体放入其中。不使用容器作为工具时，智能体只能一次运输两个物体。然而，使用容器，智能体可以收集多个物体并一起运输。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并没有明确指出进行实验所需的设备条件，例如GPU数量和内存大小。然而，根据论文内容，我们可以推测进行实验可能需要高性能的计算设备，因为模拟器需要处理大量的物理交互和渲染任务。此外，训练和推理模型可能需要使用GPU来加速计算。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中提到，这个benchmark使用了一个基于物理的高级导航和交互API，可以用于训练AI智能体在虚拟物理世界中与虚拟世界进行物理交互。由于模拟动作和环境完全基于物理，与之前的非物理或部分物理虚拟环境相比，它们提出了额外的挑战。例如，交互动作只有在目标物理可达时（即靠近且未被阻挡）才会成功。如果目标不在智能体的中心视图中，或者直接路径被阻挡（例如，被桌子阻挡），智能体就无法成功抓取物体。此外，与房屋中的物体发生物理碰撞也可能显著阻碍运输进度。因此，智能体必须学习利用视觉信号来同步导航和操作，以应对这些物理约束。<br><br>这些挑战使得reward hacking变得困难，因为智能体需要真正理解和适应物理环境，而不仅仅是通过简单的策略来获得奖励。因此，这个benchmark支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</td>
      <td>We introduce a novel and extensible benchmark for large language models<br>(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.<br>The open-source game simulation code, available on GitHub, allows LLMs to<br>compete and generates detailed data files in JSON, CSV, TXT, and PNG formats<br>for leaderboard rankings and further analysis. We present the results of games<br>among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by<br>Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and<br>GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of<br>results from other LLMs. In total, we simulated 2,310 matches (5 sessions for<br>each pair among 7 LLMs and a random player) across three types of games, using<br>three distinct prompt types: list, illustration, and image. The results<br>revealed significant variations in LLM performance across different games and<br>prompt types, with analysis covering win and disqualification rates, missed<br>opportunity analysis, and invalid move analysis. The details of the leaderboard<br>and result matrix data are available as open-access data on GitHub. This study<br>enhances our understanding of LLMs' capabilities in playing games they were not<br>specifically trained for, helping to assess their rule comprehension and<br>strategic thinking. On the path to Artificial General Intelligence (AGI), this<br>study lays the groundwork for future exploration into their utility in complex<br>decision-making scenarios, illuminating their strategic thinking abilities and<br>offering directions for further inquiry into the limits of LLMs within<br>game-based frameworks.</td>
      <td>## 🌟 论文解读 | 通过基于网格的游戏竞赛评估大型语言模型：一个可扩展的LLM基准和排行榜<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在人工智能领域的快速发展，评估这些模型的能力和性能变得至关重要。传统的评估方法，如准确率、精确率等，已经无法全面评估LLMs的复杂能力。因此，需要新的基准来评估LLMs在规则理解、战略思考和决策制定等方面的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：基于网格的游戏竞赛<br>本文提出了一种新颖且可扩展的基准，通过网格游戏（如井字棋、四子棋和五子棋）来评估LLMs的能力。这些游戏需要模型理解规则、制定策略并做出决策，从而全面评估LLMs的能力。<br><br>💡 创新点2：开放源代码和排行榜<br>本文提供了一个开源的游戏模拟代码，允许LLMs进行竞赛，并生成详细的JSON、CSV、TXT和PNG格式的数据文件，用于排行榜排名和进一步分析。此外，本文还提供了一个排行榜，展示了不同LLMs在不同游戏和提示类型下的表现。<br><br>## 📈 实验结果<br>本文对七个领先的LLMs进行了评估，包括Anthropic的Claude 3.5 Sonnet和Claude 3 Sonnet、Google的Gemini 1.5 Pro和Gemini 1.5 Flash、OpenAI的GPT-4 Turbo和GPT-4o以及Meta的Llama3-70B。实验结果表明，LLMs在不同游戏和提示类型下的表现存在显著差异。例如，Claude 3.5 Sonnet在井字棋中表现出色，但在五子棋中表现较差。此外，LLMs在处理复杂和基于视觉的提示格式时也面临挑战。<br><br>## 💬 可借鉴之处<br>本文提出的基于网格的游戏竞赛基准为评估LLMs的能力提供了一个有价值的工具。此外，本文还提供了一个开放源代码和排行榜，方便研究人员进行进一步的分析和比较。本文的研究结果对于理解LLMs的能力和局限性具有重要意义，并为未来的研究和开发提供了方向。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一种新颖且可扩展的基准，用于评估大型语言模型（LLMs）的能力。该基准通过网格游戏（如井字棋、四子棋和五子棋）来评估LLMs的能力。这些游戏需要模型理解规则、制定策略并做出决策，从而全面评估LLMs的能力。该基准提供了一个开源的游戏模拟代码，允许LLMs进行竞赛，并生成详细的JSON、CSV、TXT和PNG格式的数据文件，用于排行榜排名和进一步分析。此外，该基准还提供了一个排行榜，展示了不同LLMs在不同游戏和提示类型下的表现。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确说明该benchmark所需的设备条件，例如GPU数量和内存大小。然而，由于该benchmark是基于网格游戏，并且需要与LLMs进行交互，因此可能需要一定的计算资源来支持LLMs的推理过程。论文中提到，为了访问模型，使用了Google和OpenAI提供的web APIs，以及Amazon Bedrock服务，利用了serverless AWS Lambda函数和API网关。这表明，该benchmark可能需要云服务提供商的支持，以便能够访问和运行LLMs。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中并未明确说明该benchmark是否具有高质量的结果奖励或过程奖励，以防止reward hacking。然而，由于该benchmark是基于网格游戏，并且需要LLMs理解规则、制定策略并做出决策，因此可能需要LLMs具备一定的战略思考和决策制定能力。这可能会使得RL类模型在该benchmark上表现出色，因为RL类模型通常擅长于通过试错学习和优化策略。然而，由于该benchmark的复杂性，LLMs可能需要大量的训练和调整才能在该benchmark上取得良好的表现。</td>
    </tr>
    <tr>
      <th>9</th>
      <td>MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields</td>
      <td>Previous research has demonstrated the advantages of integrating data from<br>multiple sources over traditional unimodal data, leading to the emergence of<br>numerous novel multimodal applications. We propose a multimodal classification<br>benchmark MuG with eight datasets that allows researchers to evaluate and<br>improve their models. These datasets are collected from four various genres of<br>games that cover tabular, textual, and visual modalities. We conduct<br>multi-aspect data analysis to provide insights into the benchmark, including<br>label balance ratios, percentages of missing features, distributions of data<br>within each modality, and the correlations between labels and input modalities.<br>We further present experimental results obtained by several state-of-the-art<br>unimodal classifiers and multimodal classifiers, which demonstrate the<br>challenging and multimodal-dependent properties of the benchmark. MuG is<br>released at https://github.com/lujiaying/MUG-Bench with the data, tutorials,<br>and implemented baselines.</td>
      <td>## 🌟 论文解读 | MuG：基于游戏数据的多元分类基准<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，多模态学习已经成为一个重要的研究方向。多模态学习能够整合来自不同来源的数据，从而提高模型的性能和鲁棒性。然而，目前缺乏一个全面的基准数据集，用于评估和改进多模态分类模型。本文提出了一个名为 MuG 的多模态分类基准，旨在解决这一问题。<br><br>## 🚀 核心方法<br>MuG 包含了来自四个不同游戏类型的八个数据集，涵盖了表格、文本和视觉三种模态。这些数据集经过必要的清洗、转换和修改，以便于研究人员使用。为了更好地理解 MuG 的特性，本文进行了多方面的数据分析，包括标签平衡比例、缺失特征百分比、每种模态中数据的分布以及标签与输入模态之间的相关性。<br><br>## 📈 实验结果<br>本文使用多个最先进的单模态分类器和多模态分类器在 MuG 上进行了实验。结果表明，多模态分类器在大多数情况下都优于单模态分类器，这表明 MuG 中的分类任务确实依赖于多模态信息。此外，本文还提出了一种新的基于图注意力网络的多模态分类器 MUGNET，它在性能上与现有的多模态分类器相当。<br><br>## 💬 可借鉴之处<br>MuG 基准数据集为研究人员提供了一个评估和改进多模态分类模型的重要工具。此外，本文提出的 MUGNET 模型也为多模态学习提供了一种新的思路。未来，研究人员可以利用 MuG 基准数据集进行更多关于多模态学习的研究，并探索新的应用场景。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark名为MuG，是一个多模态分类基准，包含了来自四个不同游戏类型的八个数据集，涵盖了表格、文本和视觉三种模态。这些数据集经过必要的清洗、转换和修改，以便于研究人员使用。MuG旨在提供一个全面的基准数据集，用于评估和改进多模态分类模型。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中提到，为了进行实验，使用了以下硬件配置：<br><br>- 16个Intel Xeon Gold 6254 CPU（每个CPU 18个核心）<br>- 1个24GB的TITAN RTX GPU<br><br>此外，论文中还提到，所有模型都使用相同的硬件进行训练和测试，并且为训练过程添加了8小时的时限，以反映现实世界的资源限制。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确提到MuG benchmark是否包含奖励机制，也没有提到是否支持RL类模型。因此，无法确定MuG benchmark是否适合RL类模型。</td>
    </tr>
    <tr>
      <th>10</th>
      <td>GlitchBench: Can large multimodal models detect video game glitches?</td>
      <td>Large multimodal models (LMMs) have evolved from large language models (LLMs)<br>to integrate multiple input modalities, such as visual inputs. This integration<br>augments the capacity of LLMs for tasks requiring visual comprehension and<br>reasoning. However, the extent and limitations of their enhanced abilities are<br>not fully understood, especially when it comes to real-world tasks. To address<br>this gap, we introduce GlitchBench, a novel benchmark derived from video game<br>quality assurance tasks, to test and evaluate the reasoning capabilities of<br>LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios<br>from video games and aims to challenge both the visual and linguistic reasoning<br>powers of LMMs in detecting and interpreting out-of-the-ordinary events. We<br>evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents<br>a new challenge for these models. Code and data are available at:<br>https://glitchbench.github.io/</td>
      <td>## 🌟 论文解读 | GlitchBench：大型多模态模型能否检测视频游戏中的错误？<br><br>## 📌 背景痛点/本文动机<br>随着大型多模态模型（LMMs）的不断发展，它们在视觉理解和推理方面的能力得到了显著提升。然而，这些模型在实际应用中的表现和局限性尚不明确。为了填补这一空白，本文提出了GlitchBench，一个基于视频游戏质量保证任务的基准测试，旨在评估LMMs在检测和解释异常事件方面的推理能力。<br><br>## 🚀 核心方法<br>💡 创新点1：GlitchBench数据集<br>GlitchBench数据集由593个游戏中的异常和错误场景组成，涵盖了205款不同类型的游戏。每个场景都包含一个视频片段、一个代表性帧、一个简短的描述以及一个指向Reddit上相关讨论的链接。此外，数据集还包括330个无错误的图像作为对比。<br><br>💡 创新点2：评估方法<br>本文评估了11个最先进的LMMs，包括GPT-4V和LLaVA，在GlitchBench上的表现。评估方法包括三个问题：<br>1. 这张图片有什么不寻常的地方？<br>2. 这张图片有什么问题？<br>3. 详细描述这张图片。<br>通过比较模型生成的文本与真实标签，评估模型在检测和解释异常事件方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，LMMs在检测违反简单物理定律的错误（如汽车在空中飞行）方面表现较好，但在检测更微妙的错误（如人体部位处于不可能的姿势）方面表现较差。GPT-4V在GlitchBench上表现最佳，准确率达到43.4%。然而，与无错误图像相比，模型在检测错误图像方面的准确率明显较低，这表明错误图像更具挑战性。<br><br>## 💬 可借鉴之处<br>本文提出的GlitchBench基准测试为评估LMMs在实际应用中的推理能力提供了一个有价值的工具。此外，本文的研究结果表明，LMMs在检测和解释异常事件方面仍存在局限性，需要进一步改进。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark名为GlitchBench，它是一个用于测试和评估大型多模态模型（LMMs）在检测和解释视频游戏中的异常事件（即错误）方面的推理能力的基准测试。GlitchBench数据集由593个游戏中的异常和错误场景组成，涵盖了205款不同类型的游戏。每个场景都包含一个视频片段、一个代表性帧、一个简短的描述以及一个指向Reddit上相关讨论的链接。此外，数据集还包括330个无错误的图像作为对比。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确指出运行GlitchBench所需的特定硬件配置。然而，考虑到LMMs通常需要大量的计算资源，运行这个benchmark可能需要高性能的GPU和足够的内存。至于模型训练和推理所使用的设备，论文中提到使用了11个最先进的LMMs，包括GPT-4V和LLaVA，但没有具体说明这些模型的训练和推理是在何种设备上进行的。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中并未提及GlitchBench是否设计有奖励机制，特别是针对强化学习（RL）类模型。GlitchBench主要是一个评估LMMs在检测和解释异常事件方面的推理能力的基准测试，它通过比较模型生成的文本与真实标签来评估模型的表现。因此，它更像是一个评估工具，而不是一个训练环境，所以可能没有设计专门的奖励机制来支持RL类模型。</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models</td>
      <td>While the situation has improved for text-only models, it again seems to be<br>the case currently that multimodal (text and image) models develop faster than<br>ways to evaluate them. In this paper, we bring a recently developed evaluation<br>paradigm from text models to multimodal models, namely evaluation through the<br>goal-oriented game (self) play, complementing reference-based and<br>preference-based evaluation. Specifically, we define games that challenge a<br>model's capability to represent a situation from visual information and align<br>such representations through dialogue. We find that the largest closed models<br>perform rather well on the games that we define, while even the best<br>open-weight models struggle with them. On further analysis, we find that the<br>exceptional deep captioning capabilities of the largest models drive some of<br>the performance. There is still room to grow for both kinds of models, ensuring<br>the continued relevance of the benchmark.</td>
      <td>## 🌟 论文解读 | 游戏化评估：探究大型多模态模型中的多模态和对话式接地<br><br>## 📌 背景痛点/本文动机<br>随着大型多模态模型（LMMs）的快速发展，现有的评估方法主要依赖于参考式评估，难以全面评估模型在复杂场景下的交互能力。本文旨在探索一种新的评估范式，即通过目标导向的游戏（自我）玩法来评估多模态模型，以补充现有的参考式和偏好式评估方法。<br><br>## 🚀 核心方法<br>💡 创新点1：将游戏化评估范式应用于多模态模型<br>本文借鉴了文本模型中新兴的游戏化评估方法，并将其应用于多模态模型。通过定义三种对话游戏（参考游戏、图像比较游戏和导航游戏），挑战模型从视觉信息中构建情境模型并通过对话进行对齐的能力。<br><br>💡 创新点2：构建多模态游戏框架<br>本文使用 clemgame/clembench 框架来实现游戏化评估。该框架通过自然语言提示模板来定义游戏目标，并通过程序化的游戏大师来控制游戏流程和评分规则。<br><br>## 📈 实验结果<br>实验结果表明，大型闭源模型在本文定义的游戏中表现良好，而即使是最好的开源模型也难以应对这些挑战。进一步分析发现，大型模型在深度图像描述方面的出色能力推动了部分性能提升。这表明，无论是闭源模型还是开源模型，都仍有很大的发展空间。<br><br>## 💬 可借鉴之处<br>本文提出的游戏化评估方法为多模态模型的评估提供了新的思路，可以帮助研究人员更全面地评估模型在复杂场景下的交互能力。此外，本文构建的多模态游戏框架也为其他研究人员提供了可复现的实验平台。<br><br>## 📚 参考文献<br>* Chalamalasetti, V., et al. (2023). clemgame: A framework for evaluating language models through goal-oriented games. arXiv preprint arXiv:2304.01213.<br>* Chiang, D., et al. (2024). Chatbot Arena: A preference-based evaluation platform for conversational agents. arXiv preprint arXiv:2403.04528.<br>* Qiao, Y., et al. (2023). Evaluating language models through interactive games. arXiv preprint arXiv:2303.08574.</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一种新的评估多模态模型的方法，即通过目标导向的游戏（自我）玩法来评估模型。论文中定义了三种对话游戏：参考游戏、图像比较游戏和导航游戏。这些游戏旨在挑战模型从视觉信息中构建情境模型并通过对话进行对齐的能力。通过这些游戏，研究人员可以更全面地评估模型在复杂场景下的交互能力。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明这个benchmark需要什么设备条件。但是，由于多模态模型通常需要大量的计算资源，因此运行这个benchmark可能需要高性能的计算机，例如具有多个GPU和大量内存的服务器。至于本文中模型训练和推理使用的设备，论文中没有提供具体信息。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确说明这个benchmark是否具有高质量的结果奖励或过程奖励。但是，由于游戏化评估方法通常需要模型在游戏中取得成功才能获得高分，因此可以认为这个benchmark具有一定的结果奖励。至于过程奖励，论文中没有提及。因此，这个benchmark是否支持RL类模型大放异彩还有待进一步研究。</td>
    </tr>
    <tr>
      <th>12</th>
      <td>AvalonBench: Evaluating LLMs Playing the Game of Avalon</td>
      <td>In this paper, we explore the potential of Large Language Models (LLMs)<br>Agents in playing the strategic social deduction game, Resistance Avalon.<br>Players in Avalon are challenged not only to make informed decisions based on<br>dynamically evolving game phases, but also to engage in discussions where they<br>must deceive, deduce, and negotiate with other players. These characteristics<br>make Avalon a compelling test-bed to study the decision-making and<br>language-processing capabilities of LLM Agents. To facilitate research in this<br>line, we introduce AvalonBench - a comprehensive game environment tailored for<br>evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game<br>environment for Avalon, (2) rule-based bots as baseline opponents, and (3)<br>ReAct-style LLM agents with tailored prompts for each role. Notably, our<br>evaluations based on AvalonBench highlight a clear capability gap. For<br>instance, models like ChatGPT playing good-role got a win rate of 22.2% against<br>rule-based bots playing evil, while good-role bot achieves 38.2% win rate in<br>the same setting. We envision AvalonBench could be a good test-bed for<br>developing more advanced LLMs (with self-playing) and agent frameworks that can<br>effectively model the layered complexities of such game environments.</td>
      <td>## 🌟 论文解读 | AvalonBench：评估大型语言模型在社交推理游戏中的表现<br><br>## 📌 背景痛点/本文动机<br>社交推理游戏如 Resistance Avalon 对玩家的推理、沟通和决策能力提出了挑战。这些游戏要求玩家在动态变化的游戏阶段做出明智的决策，并在讨论中欺骗、推理和与其他玩家协商。这些特点使得 Avalon 成为研究大型语言模型（LLM）代理的决策和语言处理能力的理想测试平台。然而，目前缺乏一个全面的评估 LLM 代理在多代理游戏环境中的性能的基准测试平台。<br><br>## 🚀 核心方法<br>本文提出了 AvalonBench，一个专门用于评估多代理 LLM 代理的游戏环境。AvalonBench 包含以下三个关键组成部分：<br><br>1. **Avalon 游戏环境**：为代理提供游戏平台，记录所有玩家的行动并推动游戏进程。<br>2. **基于规则的机器人**：作为基线对手，为代理提供可比较的基准。<br>3. **ReAct 风格的 LLM 代理**：针对每个角色定制提示，以评估 LLM 代理在不同角色下的表现。<br><br>## 📈 实验结果<br>本文使用 AvalonBench 对 ChatGPT-3.5 和 Llama2 模型进行了评估，并与基于规则的机器人进行了比较。结果显示，即使在有讨论的情况下，LLM 代理的表现也远低于基于规则的机器人。例如，ChatGPT-3.5 在扮演好人角色时，在与扮演坏人的基于规则的机器人对抗中，胜率为 22.2%，而好人角色的机器人胜率为 38.2%。这表明当前 LLM 代理在推理、说服、协商和欺骗能力方面存在明显差距。<br><br>## 💬 可借鉴之处<br>AvalonBench 为研究 LLM 代理在社交推理游戏中的表现提供了一个有价值的测试平台。该平台可以帮助研究人员开发更先进的 LLM 代理，并探索如何将决策技术集成到 LLM 中，以提高其在复杂游戏环境中的表现。此外，AvalonBench 还可以用于评估 LLM 代理在多代理协作、沟通和策略制定方面的能力。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是AvalonBench，它是一个专门用于评估多代理大型语言模型（LLM）代理的游戏环境。AvalonBench包含以下三个关键组成部分：<br><br>1. **Avalon 游戏环境**：为代理提供游戏平台，记录所有玩家的行动并推动游戏进程。<br>2. **基于规则的机器人**：作为基线对手，为代理提供可比较的基准。<br>3. **ReAct 风格的 LLM 代理**：针对每个角色定制提示，以评估 LLM 代理在不同角色下的表现。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并没有明确指出AvalonBench所需的设备条件，例如GPU和内存的具体要求。然而，考虑到LLM代理通常需要大量的计算资源，我们可以推测AvalonBench可能需要高性能的计算设备，例如具有多个GPU和大量内存的服务器。<br><br>至于本文中模型训练和推理所使用的设备，论文中提到使用了GPT-3.5和Llama2模型进行评估。这些模型的训练和推理通常需要高性能的计算设备，例如具有多个GPU和大量内存的服务器。然而，论文中并没有提供具体的设备配置信息。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>AvalonBench的环境设计考虑了奖励机制，以支持RL类模型的表现。在AvalonBench中，玩家的目标是赢得游戏，因此结果奖励是明确的。此外，游戏过程中还设置了过程奖励，例如成功完成团队选择、投票和任务等。这些奖励有助于引导RL模型学习有效的策略。<br><br>然而，AvalonBench的环境也具有一定的复杂性，例如玩家需要推理、说服、协商和欺骗等。这可能导致RL模型出现reward hacking现象，即模型通过不合理的策略来获得奖励。为了防止reward hacking，AvalonBench可能需要进一步优化奖励机制，例如引入惩罚机制或调整奖励权重。</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。<br><br>## 🚀 核心方法<br>为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：<br>1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。<br>2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。<br>3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。<br><br>## 💬 可借鉴之处<br>Steve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了三个开放世界的评估基准来评估智能体在开放世界中的交互能力：<br><br>1. **环境视觉描述（ENV-VC）**：评估智能体感知和描述其周围环境的能力。智能体需要根据给定的快照描述其当前状态和周围环境特征，例如生命值、食物、装备等。<br><br>2. **基础知识问答（FK-QA）**：评估智能体掌握对决策至关重要的基本知识的熟练程度。智能体需要回答关于Minecraft中物品、配方等基本知识的问答。<br><br>3. **技能预测与规划（SPP）**：量化智能体在战略行动和规划方面的能力。智能体需要预测技能的执行状态（成功、失败、进行中）并生成可执行的高层次技能计划。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明benchmark所需的设备条件。但是，根据论文内容，我们可以推测：<br><br>* **训练设备**：由于模型规模较大，训练过程需要高性能的GPU和足够的内存。论文中提到使用了LLaMA-2模型作为LLM主干，CLIP作为视觉编码器，VQ-GAN作为视觉分词器，因此训练设备至少需要支持这些模型的GPU。<br><br>* **推理设备**：推理过程对设备的要求相对较低，但仍然需要一定的计算能力。论文中提到使用了LoRA技术来降低推理成本，因此推理设备可以相对简单。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确说明benchmark是否包含结果奖励或过程奖励。但是，根据论文内容，我们可以推测：<br><br>* **结果奖励**：ENV-VC和FK-QA基准通过评估智能体对环境描述和知识问答的准确性来提供结果奖励。<br><br>* **过程奖励**：SPP基准通过评估智能体对技能执行状态的预测和技能计划的生成来提供过程奖励。<br><br>由于这些奖励都是基于智能体的实际表现，因此不容易出现reward hacking现象。这为RL类模型在benchmark上取得优异成绩提供了良好的基础。</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Creative Agents: Empowering Agents with Imagination for Creative Tasks</td>
      <td>We study building embodied agents for open-ended creative tasks. While<br>existing methods build instruction-following agents that can perform diverse<br>open-ended tasks, none of them demonstrates creativity -- the ability to give<br>novel and diverse task solutions implicit in the language instructions. This<br>limitation comes from their inability to convert abstract language instructions<br>into concrete task goals in the environment and perform long-horizon planning<br>for such complicated goals. Given the observation that humans perform creative<br>tasks with the help of imagination, we propose a class of solutions for<br>creative agents, where the controller is enhanced with an imaginator that<br>generates detailed imaginations of task outcomes conditioned on language<br>instructions. We introduce several approaches to implementing the components of<br>creative agents. We implement the imaginator with either a large language model<br>for textual imagination or a diffusion model for visual imagination. The<br>controller can either be a behavior-cloning policy learned from data or a<br>pre-trained foundation model generating executable codes in the environment. We<br>benchmark creative tasks with the challenging open-world game Minecraft, where<br>the agents are asked to create diverse buildings given free-form language<br>instructions. In addition, we propose novel evaluation metrics for open-ended<br>creative tasks utilizing GPT-4V, which holds many advantages over existing<br>metrics. We perform a detailed experimental analysis of creative agents,<br>showing that creative agents are the first AI agents accomplishing diverse<br>building creation in the survival mode of Minecraft. Our benchmark and models<br>are open-source for future research on creative agents<br>(https://github.com/PKU-RL/Creative-Agents).</td>
      <td>## 🌟 论文解读 | 创意智能体：赋予智能体想象力以完成创意任务<br><br>## 📌 背景痛点/本文动机<br>现有的智能体大多只能执行预定义的任务，缺乏处理开放性任务的能力，尤其是那些需要创造力的任务。例如，在Minecraft游戏中，现有的智能体可以执行简单的指令，如“收集石头”或“建造一个雪人”，但无法完成更复杂的创意任务，如“建造一个砂岩宫殿”。这是因为它们无法将抽象的语言指令转换为具体的任务目标，并执行长期规划。<br><br>## 🚀 核心方法<br>本文提出了“创意智能体”的概念，通过赋予智能体想象力来处理开放性创意任务。创意智能体由两个主要部分组成：想象器和控制器。<br><br>💡 创新点1：想象器<br>想象器负责根据语言指令生成任务结果的详细想象。本文提出了两种实现想象器的方法：<br>- **文本想象**：使用大型语言模型（LLM）如GPT-4，通过Chain-of-Thought（CoT）技术生成文本想象。<br>- **视觉想象**：使用扩散模型如Stable Diffusion，生成与文本描述相符的视觉想象。<br><br>💡 创新点2：控制器<br>控制器负责将想象转换为可执行的计划。本文提出了两种实现控制器的方法：<br>- **行为克隆控制器**：从环境中学习行为克隆策略，将图像想象转换为建筑蓝图，并执行相应的动作。<br>- **基于GPT-4(V)的控制器**：利用GPT-4(V)的视觉语言理解和代码生成能力，直接生成可执行代码来完成任务。<br><br>## 📈 实验结果<br>本文在Minecraft游戏中进行了实验，结果表明，创意智能体能够根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。其中，使用扩散模型进行视觉想象，并结合GPT-4(V)进行控制的智能体表现最佳。<br><br>## 💬 可借鉴之处<br>本文提出的创意智能体框架为开放性学习和创意AI智能体研究提供了新的思路和方法。未来可以进一步探索如何提高行为克隆控制器的性能，以及如何增强智能体的创造力。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是Minecraft游戏中的建筑创建任务。在这个benchmark中，智能体需要根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。论文中使用了20个不同的语言指令来测试智能体的创造力，例如“建造一个砂岩宫殿”或“建造一个冰制的巨大火山”。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明这个benchmark需要什么设备条件。但是，由于Minecraft是一个图形密集型的游戏，因此运行这个benchmark可能需要一台性能较好的计算机。此外，由于论文中使用了大型语言模型和扩散模型，因此训练和推理这些模型可能需要大量的计算资源，例如GPU和内存。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>这个benchmark的环境没有提供结果奖励或过程奖励。因此，RL类模型在这个benchmark上可能难以大放异彩。这是因为RL类模型通常需要明确的奖励信号来学习如何完成任务。</td>
    </tr>
    <tr>
      <th>15</th>
      <td>MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs</td>
      <td>While Vision-Language Models (VLMs) hold promise for tasks requiring<br>extensive collaboration, traditional multi-agent simulators have facilitated<br>rich explorations of an interactive artificial society that reflects collective<br>behavior. However, these existing simulators face significant limitations.<br>Firstly, they struggle with handling large numbers of agents due to high<br>resource demands. Secondly, they often assume agents possess perfect<br>information and limitless capabilities, hindering the ecological validity of<br>simulated social interactions. To bridge this gap, we propose a multi-agent<br>Minecraft simulator, MineLand, that bridges this gap by introducing three key<br>features: large-scale scalability, limited multimodal senses, and physical<br>needs. Our simulator supports 64 or more agents. Agents have limited visual,<br>auditory, and environmental awareness, forcing them to actively communicate and<br>collaborate to fulfill physical needs like food and resources. Additionally, we<br>further introduce an AI agent framework, Alex, inspired by multitasking theory,<br>enabling agents to handle intricate coordination and scheduling. Our<br>experiments demonstrate that the simulator, the corresponding benchmark, and<br>the AI agent framework contribute to more ecological and nuanced collective<br>behavior.The source code of MineLand and Alex is openly available at<br>https://github.com/cocacola-lab/MineLand.</td>
      <td>## 🌟 论文解读 | MineLand：模拟大规模多智能体交互的Minecraft模拟器<br><br>## 📌 背景痛点/本文动机<br>传统的多智能体模拟器在处理大规模场景时面临资源消耗过大的问题，并且通常假设智能体拥有完美信息和无限能力，这与现实世界中的人类交互存在较大差距。为了解决这个问题，本文提出了MineLand，一个基于Minecraft的多智能体模拟器，旨在模拟更接近现实世界的多智能体交互。<br><br>## 🚀 核心方法<br>💡 创新点1：大规模可扩展性<br>MineLand通过将每个Minecraft客户端简化为单个线程，优化了性能开销，从而支持64个或更多智能体在主流消费级桌面PC上运行。<br><br>💡 创新点2：有限的模态感知<br>MineLand模拟了人类的视觉和听觉机制，对智能体的感知能力施加了限制，包括距离衰减、环境遮挡和方向约束，使其更接近现实世界。<br><br>💡 创新点3：物理需求<br>MineLand将真实的物理需求（如食物、氧气和饥饿）集成到智能体中，使其需要管理资源并与其他智能体竞争或合作，以维持生存。<br><br>💡 创新点4：多任务处理框架Alex<br>MineLand引入了基于多任务理论的AI智能体框架Alex，允许智能体同时执行复杂的协调和调度，以处理多个任务。<br><br>## 📈 实验结果<br>实验结果表明，MineLand在支持大规模智能体、有限的模态感知和物理需求方面表现出色。此外，Alex框架能够有效地处理多任务，并在合作模式下提高效率。<br><br>## 💬 可借鉴之处<br>MineLand为研究多智能体交互提供了一个强大的平台，其创新的设计和功能可以应用于人类动力学、社会心理学、机器人技术和游戏设计等领域。此外，Alex框架的多任务处理机制为开发更智能的AI智能体提供了新的思路。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是MineLand Benchmark Suite，它是一个用于评估多智能体在MineLand模拟器中能力的基准测试套件。MineLand Benchmark Suite提供了大量的任务，包括程序性任务、创造性任务和混合任务。程序性任务和创造性任务的数量分别是MineDojo的两倍，而混合任务是一个新的类别，它结合了程序性任务和创造性任务的特点。此外，MineLand Benchmark Suite还支持灵活的玩家数量和探索模式，包括合作模式和竞争模式。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>MineLand Benchmark Suite的设备条件取决于任务的规模和复杂性。对于大多数任务，一个主流的消费级桌面PC就足够了。例如，在实验中，MineLand在配备Intel i5-12400F CPU和64GB内存的PC上运行了32个智能体。然而，对于更复杂的任务，可能需要更强大的硬件，例如更多的CPU核心、更多的内存或GPU。至于模型训练和推理，论文中没有明确说明使用了什么设备，但可以推测，由于MineLand Benchmark Suite支持大规模的多智能体交互，因此可能需要使用高性能的计算集群或云服务来进行模型训练和推理。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>MineLand Benchmark Suite的环境设计考虑了奖励机制的合理性和鲁棒性，以避免reward hacking。例如，在创造性任务中，MineLand使用VLM-based evaluations和human evaluation两种评估方法来计算任务得分，这样可以更全面地评估智能体的表现。此外，在混合任务中，MineLand引入了LCS-based metric和human evaluation两种评估指标，以评估智能体的行为完整性和表演的自然性。这些评估方法可以有效地防止reward hacking，并支持RL类模型在MineLand Benchmark Suite上取得优异的性能。</td>
    </tr>
    <tr>
      <th>16</th>
      <td>MindAgent: Emergent Gaming Interaction</td>
      <td>Large Language Models (LLMs) have the capacity of performing complex<br>scheduling in a multi-agent system and can coordinate these agents into<br>completing sophisticated tasks that require extensive collaboration. However,<br>despite the introduction of numerous gaming frameworks, the community has<br>insufficient benchmarks towards building general multi-agents collaboration<br>infrastructure that encompass both LLM and human-NPCs collaborations. In this<br>work, we propose a novel infrastructure - MindAgent - to evaluate planning and<br>coordination emergent capabilities for gaming interaction. In particular, our<br>infrastructure leverages existing gaming framework, to i) require understanding<br>of the coordinator for a multi-agent system, ii) collaborate with human players<br>via un-finetuned proper instructions, and iii) establish an in-context learning<br>on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new<br>gaming scenario and related benchmark that dispatch a multi-agent collaboration<br>efficiency and supervise multiple agents playing the game simultaneously. We<br>conduct comprehensive evaluations with new auto-metric CoS for calculating the<br>collaboration efficiency. Finally, our infrastructure can be deployed into<br>real-world gaming scenarios in a customized VR version of CUISINEWORLD and<br>adapted in existing broader Minecraft gaming domain. We hope our findings on<br>LLMs and the new infrastructure for general-purpose scheduling and coordination<br>can help shed light on how such skills can be obtained by learning from large<br>language corpora.</td>
      <td>## 🌟 论文解读 | MindAgent：大型语言模型在游戏交互中的涌现式规划与协调能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在各个领域的应用日益广泛，其在多智能体系统中的规划与协调能力也逐渐受到关注。然而，现有的游戏框架和基准测试还不足以评估LLMs在游戏交互中的涌现式规划与协调能力，尤其是在LLMs与人类NPCs协作的场景下。本文旨在解决这个问题，提出了一种名为MindAgent的新型基础设施，用于评估LLMs在游戏交互中的规划与协调能力。<br><br>## 🚀 核心方法<br>💡 创新点1：CUISINEWORLD游戏场景与基准测试<br>本文设计了一个名为CUISINEWORLD的游戏场景，模拟了一个虚拟厨房环境，其中多智能体系统需要协调多个代理，完成尽可能多的菜肴订单。CUISINEWORLD游戏场景具有多种任务结构和难度，是评估LLMs涌现式多智能体规划能力的理想测试平台。<br><br>💡 创新点2：MindAgent基础设施<br>MindAgent是一个用于LLMs交互式多智能体规划的基础设施，它展示了LLMs的涌现式多智能体规划能力，并引入了多种提示技术，以促进LLMs的规划能力，包括提供少量示例、规划理由和环境反馈。<br><br>## 📈 实验结果<br>本文在CUISINEWORLD游戏场景中进行了广泛的实验，结果表明：<br>1. 零样本多智能体规划：强大的预训练LLMs（如GPT-4）能够通过阅读简单的游戏指令和食谱，调度多个代理（2到4个）完成菜肴，甚至与人类玩家协作。<br>2. 基于高级提示的规划：通过利用涌现式上下文学习能力，可以显著提高LLMs的多智能体规划性能。例如，添加少量专家演示、解释某些行动的理由，以及在规划过程中提供实时反馈。<br>3. 通用潜力：LLMs表现出成为通用多智能体规划器的巨大潜力，因为它能够通过少量示例泛化到更多代理，并适应新的游戏领域，如Minecraft。<br><br>## 💬 可借鉴之处<br>本文提出的MindAgent基础设施和CUISINEWORLD游戏场景为评估LLMs在游戏交互中的涌现式规划与协调能力提供了新的思路和方法。此外，本文的研究结果也表明，LLMs在多智能体规划方面具有巨大的潜力，有望在未来推动游戏AI的发展。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是CUISINEWORLD，它是一个基于多智能体虚拟厨房环境的游戏场景。在这个游戏中，多智能体系统需要协调多个代理，完成尽可能多的菜肴订单。CUISINEWORLD游戏场景具有多种任务结构和难度，是评估LLMs涌现式多智能体规划能力的理想测试平台。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明CUISINEWORLD benchmark需要什么设备条件。但是，根据论文内容，我们可以推测，运行这个benchmark需要一台性能较好的计算机，至少需要一块GPU来支持LLMs的推理。至于模型训练和推理使用的设备，论文中提到，所有GPT-4实验都使用gpt-4-0613模型，所有chat-GPT实验都使用gpt-3.5-turbo-0613模型，对于Llama 2实验，使用hugging face inference endpoints Llama-2-70b-chat-hf。所有实验的温度都设置为0.1。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确说明CUISINEWORLD benchmark是否有一个高质量的结果奖励或者过程奖励。但是，根据论文内容，我们可以推测，CUISINEWORLD benchmark的奖励机制可能比较简单，因为它主要关注的是多智能体协作的效率，而不是单个代理的奖励。这可能不利于RL类模型在这个benchmark上大放异彩，因为RL类模型通常需要更复杂的奖励机制来学习。</td>
    </tr>
    <tr>
      <th>17</th>
      <td>BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</td>
      <td>We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in<br>simulation, spanning a range of everyday household chores such as cleaning,<br>maintenance, and food preparation. These activities are designed to be<br>realistic, diverse, and complex, aiming to reproduce the challenges that agents<br>must face in the real world. Building such a benchmark poses three fundamental<br>difficulties for each activity: definition (it can differ by time, place, or<br>person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these<br>with three innovations. First, we propose an object-centric, predicate<br>logic-based description language for expressing an activity's initial and goal<br>conditions, enabling generation of diverse instances for any activity. Second,<br>we identify the simulator-agnostic features required by an underlying<br>environment to support BEHAVIOR, and demonstrate its realization in one such<br>simulator. Third, we introduce a set of metrics to measure task progress and<br>efficiency, absolute and relative to human demonstrators. We include 500 human<br>demonstrations in virtual reality (VR) to serve as the human ground truth. Our<br>experiments demonstrate that even state of the art embodied AI solutions<br>struggle with the level of realism, diversity, and complexity imposed by the<br>activities in our benchmark. We make BEHAVIOR publicly available at<br>behavior.stanford.edu to facilitate and calibrate the development of new<br>embodied AI solutions.</td>
      <td>## 🌟 论文解读 | BEHAVIOR：虚拟交互生态环境中日常家庭活动的基准<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的发展，模拟环境中的基准测试对于评估和推动智能体在现实世界中的表现至关重要。然而，现有的基准测试往往缺乏现实性、多样性和复杂性，无法全面评估智能体在真实世界中的能力。为了解决这个问题，本文提出了BEHAVIOR，一个包含100个日常家庭活动的基准测试，旨在模拟真实世界中的挑战，并推动智能体在现实世界中的发展。<br><br>## 🚀 核心方法<br>💡 创新点1：基于谓词逻辑的描述语言<br>BEHAVIOR引入了一种基于谓词逻辑的描述语言，用于表达活动的初始和目标条件。这种语言允许生成多样化的活动实例，并能够接受任何有意义的解决方案。<br><br>💡 创新点2：模拟器无关的环境特征<br>BEHAVIOR确定了支持其活动的模拟器无关特征，并在iGibson 2.0中实现了这些特征。这使得BEHAVIOR可以在多种环境中实现，并提供了无限多样化的活动实例。<br><br>💡 创新点3：基于人类表现的评估指标<br>BEHAVIOR引入了一系列评估指标，用于衡量智能体在任务进度和效率方面的表现。这些指标包括成功分数、效率指标和基于人类表现的指标，以确保评估的公平性和可比性。<br><br>## 📈 实验结果<br>实验结果表明，即使是当前最先进的智能体，在面对BEHAVIOR的挑战时也难以取得良好的表现。这表明BEHAVIOR的基准测试具有很高的难度，能够有效地评估智能体在现实世界中的能力。<br><br>## 💬 可借鉴之处<br>BEHAVIOR的基准测试为评估和推动智能体在现实世界中的发展提供了重要的工具。其基于谓词逻辑的描述语言、模拟器无关的环境特征和基于人类表现的评估指标，为其他基准测试提供了可借鉴的经验。此外，BEHAVIOR的基准测试还可以用于开发新的智能体解决方案，并推动人工智能技术的发展。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一个名为BEHAVIOR的基准测试，用于评估具身AI在虚拟、交互和生态环境中的表现。BEHAVIOR包含100个日常家庭活动，如清洁、维护和食物准备等。这些活动旨在模拟真实世界中的挑战，并推动具身AI在现实世界中的发展。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确指出BEHAVIOR基准测试所需的设备条件，如GPU和内存等。然而，由于BEHAVIOR是一个基于模拟的基准测试，因此需要一台能够运行模拟环境的计算机。对于模型训练和推理，论文中使用了两种具身AI解决方案：Soft-Actor Critic (SAC) 和 Proximal-Policy Optimization (PPO)。这些算法在训练过程中使用了GPU进行加速，但具体使用的GPU型号和数量并未在论文中提及。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>BEHAVIOR基准测试提供了一个基于成功和效率的评估指标体系。成功指标包括成功分数，用于衡量智能体在任务进度方面的表现。效率指标包括模拟时间、运动学混乱、逻辑混乱、导航距离、手部位移等，用于衡量智能体在完成任务过程中的效率。此外，BEHAVIOR还引入了一个基于人类表现的评估指标，用于衡量智能体相对于人类演示者的表现。这些指标旨在全面评估智能体的性能，并减少reward hacking的可能性。然而，论文中的实验结果表明，即使是当前最先进的RL算法，在面对BEHAVIOR的挑战时也难以取得良好的表现。这表明BEHAVIOR的基准测试具有很高的难度，能够有效地评估智能体在现实世界中的能力。</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</td>
      <td>In this study, we explore the application of Large Language Models (LLMs) in<br>\textit{Jubensha}, a Chinese detective role-playing game and a novel area in<br>Artificial Intelligence (AI) driven gaming. We introduce the first dataset<br>specifically for Jubensha, including character scripts and game rules, to<br>foster AI agent development in this complex narrative environment. Our work<br>also presents a unique multi-agent interaction framework using LLMs, allowing<br>AI agents to autonomously engage in this game. To evaluate the gaming<br>performance of these AI agents, we developed novel methods measuring their<br>mastery of case information and reasoning skills. Furthermore, we incorporated<br>the latest advancements in in-context learning to improve the agents'<br>performance in information gathering, murderer identification, and logical<br>reasoning. The experimental results validate the effectiveness of our proposed<br>methods. This work aims to offer a novel perspective on understanding LLM<br>capabilities and establish a new benchmark for evaluating large language<br>model-based agents.</td>
      <td>## 🌟 论文解读 | 解码数字侦探：理解大型语言模型在多智能体推理游戏中的行为和能力<br><br>## 📌 背景痛点/本文动机<br>随着互动角色扮演游戏（IRPGs）的全球流行，特别是中国侦探角色扮演游戏“剧本杀”（Jubensha）的兴起，人工智能（AI）在游戏领域的应用也日益受到关注。然而，现有的AI研究主要集中在传统的棋类游戏、视频游戏等领域，对于“剧本杀”这类需要多轮语言交互、信息收集和逻辑推理的游戏，AI的应用还处于起步阶段。本文旨在探索大型语言模型（LLMs）在“剧本杀”游戏中的应用，并建立一个新的评估基准，以衡量LLM在复杂叙事环境中的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：构建了首个专门针对“剧本杀”游戏的中文数据集，包括角色剧本和预设游戏规则，为AI代理的开发提供了基础。<br>💡 创新点2：设计了一个独特的多智能体交互框架，使用LLMs使AI代理能够自主参与“剧本杀”游戏。<br>💡 创新点3：为了评估AI代理在“剧本杀”游戏中的表现，设计了两个新颖的任务：一个用于评估他们对案件信息的掌握程度，另一个用于评估他们的推理能力。<br>💡 创新点4：利用最新的上下文学习技术，设计了模块来增强LLM代理在信息收集、凶手识别和逻辑推理方面的性能。<br><br>## 📈 实验结果<br>实验结果表明，本文提出的方法在信息收集、凶手识别和推理能力方面显著提高了LLM代理的性能。具体来说，与没有记忆检索模块的代理相比，具有记忆检索模块的代理在回答关于其他角色的问题时准确率显著提高。此外，自完善和自验证模块的组合进一步提高了代理的准确率，表明这些模块有效地增强了代理在“剧本杀”游戏中的沟通效率。<br><br>## 💬 可借鉴之处<br>本文的研究为LLMs在复杂叙事环境中的应用提供了新的视角，并为评估LLM代理的性能建立了新的基准。此外，本文提出的ThinkThrice框架和上下文学习模块的设计，为开发更智能、更具推理能力的AI代理提供了有价值的参考。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一个名为“剧本杀”的中文侦探角色扮演游戏数据集，该数据集包括角色剧本和预设游戏规则，旨在为AI代理的开发提供基础。此外，论文还设计了一个独特的多智能体交互框架，使用LLMs使AI代理能够自主参与“剧本杀”游戏。为了评估AI代理在“剧本杀”游戏中的表现，论文设计了两个新颖的任务：一个用于评估他们对案件信息的掌握程度，另一个用于评估他们的推理能力。这两个任务构成了论文提出的benchmark，用于衡量LLM在复杂叙事环境中的能力。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确提及benchmark所需的设备条件，但根据论文内容，我们可以推测，由于LLMs的训练和推理需要大量的计算资源，因此该benchmark可能需要高性能的计算机设备，例如具有多个GPU和大量内存的服务器。至于本文的模型训练和推理所使用的设备，论文中提到使用了OpenAI的GPT-3.5和GPT-4模型，但没有具体说明这些模型的训练和推理所使用的设备。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中并未提及benchmark是否具有高质量的结果奖励或过程奖励，以及是否支持RL类模型。然而，根据论文内容，我们可以推测，由于“剧本杀”游戏是一个复杂的叙事环境，需要多轮语言交互、信息收集和逻辑推理，因此该benchmark可能更适合评估LLM在复杂环境中的能力，而不是RL类模型。</td>
    </tr>
    <tr>
      <th>19</th>
      <td>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</td>
      <td>The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by<br>Large Language Models (LLMs) make them promising candidates for developing<br>coordination agents. In this study, we introduce a new LLM-Coordination<br>Benchmark aimed at a detailed analysis of LLMs within the context of Pure<br>Coordination Games, where participating agents need to cooperate for the most<br>gain. This benchmark evaluates LLMs through two distinct tasks: (1)<br>\emph{Agentic Coordination}, where LLMs act as proactive participants for<br>cooperation in 4 pure coordination games; (2) \emph{Coordination Question<br>Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice<br>questions from the 4 games for evaluation of three key reasoning abilities:<br>Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to<br>enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture<br>for Coordination (CAC) framework that can easily integrate different LLMs as<br>plug-and-play modules for pure coordination games. Our findings indicate that<br>LLM agents equipped with GPT-4-turbo achieve comparable performance to<br>state-of-the-art reinforcement learning methods in games that require<br>commonsense actions based on the environment. Besides, zero-shot coordination<br>experiments reveal that, unlike RL methods, LLM agents are robust to new unseen<br>partners. However, results on Coordination QA show a large room for improvement<br>in the Theory of Mind reasoning and joint planning abilities of LLMs. The<br>analysis also sheds light on how the ability of LLMs to understand their<br>environment and their partner's beliefs and intentions plays a part in their<br>ability to plan for coordination. Our code is available at<br>\url{https://github.com/eric-ai-lab/llm_coordination}.</td>
      <td>## 🌟 论文解读 | LLM-Coordination：评估和分析大型语言模型的多智能体协调能力<br><br>## 📌 背景痛点/本文动机<br>在许多日常任务和关键操作中，如烹饪和救援行动，合作是至关重要的。这些场景可以被视为纯协调游戏，其中所有参与方都从选择完全一致的战略中受益，避免任何利益冲突。这些游戏要求代理推理他们的环境并计划，同时考虑他们的伙伴的信念和意图。大型语言模型（LLMs）最近在物理和虚拟环境中的涌现规划能力、令人印象深刻的推理能力和对心智理论的暗示，使它们成为开发协调代理的有希望的候选者。然而，LLMs在协调游戏中的必要条件、优势和局限性仍然不清楚。本文旨在通过进行LLMs的多智能体协调能力的全面评估和分析来弥合这一差距。<br><br>## 🚀 核心方法<br>本文提出了一个新的LLM-Coordination基准，旨在对LLMs在纯协调游戏中的能力进行详细分析。该基准通过两个不同的任务评估LLMs：<br>1. **代理协调**：LLMs作为积极合作参与者参与4个纯协调游戏。<br>2. **协调问答（QA）**：LLMs被提示回答来自4个游戏的198个多项选择题，以评估三个关键推理能力：环境理解、心智理论推理和联合规划。<br><br>此外，为了使LLMs能够进行多智能体协调，本文引入了一个协调认知架构（CAC）框架，该框架可以轻松地将不同的LLMs作为即插即用模块集成到纯协调游戏中。<br><br>## 📈 实验结果<br>实验结果表明，配备GPT-4-turbo的LLM代理在需要基于环境的常识行动的游戏中，其性能与最先进的强化学习方法相当。此外，零样本协调实验表明，与RL方法不同，LLM代理对新未见伙伴具有鲁棒性。然而，协调QA的结果表明，LLMs的心智理论推理和联合规划能力还有很大的改进空间。分析还揭示了LLMs理解其环境和其伙伴的信念和意图的能力如何影响它们协调计划的能力。<br><br>## 💬 可借鉴之处<br>本文提出的LLM-Coordination基准和CAC框架为评估和分析LLMs的多智能体协调能力提供了一个有价值的工具。此外，本文的结果突出了LLMs在协调任务中的优势和局限性，并为未来研究提供了有价值的见解。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一个名为LLM-Coordination的基准，用于评估和分析大型语言模型（LLMs）在纯协调游戏中的多智能体协调能力。该基准包括两个主要任务：<br><br>1. **代理协调**：LLMs作为积极合作参与者参与4个纯协调游戏，包括Hanabi Challenge、Overcooked-AI、Collab Capture和Collab Escape。这些游戏要求代理推理他们的环境并计划，同时考虑他们的伙伴的信念和意图。<br><br>2. **协调问答（QA）**：LLMs被提示回答来自4个游戏的198个多项选择题，以评估三个关键推理能力：环境理解、心智理论推理和联合规划。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明LLM-Coordination基准所需的特定设备条件，例如GPU数量或内存大小。然而，考虑到LLMs（如GPT-4-turbo）的计算需求，可以推测需要高性能的计算资源。论文中提到，GPT-4-turbo的推理延迟为8.36 ± 1.79秒（使用思维链）和1.02 ± 0.09秒（不使用思维链），这表明LLM推理可能需要强大的GPU和足够的内存来支持。<br><br>至于模型训练和推理所使用的设备，论文中没有提供具体信息。然而，考虑到GPT-4-turbo等大型LLMs的训练通常需要大量的计算资源，可以推测训练过程可能使用了多个高性能GPU和大量的内存。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>LLM-Coordination基准的环境设计旨在评估LLMs在纯协调游戏中的协调能力。这些游戏通常具有明确的目标和规则，因此结果奖励通常与游戏的成功完成相关。例如，在Hanabi Challenge中，结果奖励是正确排列的牌的数量，而在Overcooked-AI中，结果奖励是成功交付的洋葱汤的数量。<br><br>虽然论文中没有明确提到过程奖励，但可以推测这些游戏可能包含一些过程奖励，例如在游戏过程中获得提示或完成特定任务。这些过程奖励可以帮助LLMs学习有效的协调策略，并避免reward hacking。<br><br>总的来说，LLM-Coordination基准的环境设计旨在评估LLMs在纯协调游戏中的协调能力，并支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>20</th>
      <td>SimulBench: Evaluating Language Models with Creative Simulation Tasks</td>
      <td>We introduce SimulBench, a benchmark designed to evaluate large language<br>models (LLMs) across a diverse collection of creative simulation scenarios,<br>such as acting as a Linux terminal or playing text games with users. While<br>these simulation tasks serve as effective measures of an LLM's general<br>intelligence, they are seldom incorporated into existing benchmarks. A major<br>challenge is to develop an evaluation framework for testing different LLMs<br>fairly while preserving the multi-round interactive nature of simulation tasks<br>between users and AI. To tackle this issue, we suggest using a fixed LLM as a<br>user agent to engage with an LLM to collect dialogues first under different<br>tasks. Then, challenging dialogue scripts are extracted for evaluating<br>different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4<br>is employed as the evaluator, tasked with reviewing the quality of the final<br>response generated by the target LLMs given multi-turn dialogue scripts. Our<br>comprehensive experiments indicate that these simulation tasks continue to pose<br>a significant challenge with their unique natures and show the gap between<br>proprietary models and the most advanced open LLMs. For example, GPT-4-turbo<br>outperforms LLaMA-3-70b-Chat on 18.55\% more cases.</td>
      <td>## 🌟 论文解读 | SimulBench：评估语言模型在创意模拟任务中的表现<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在模拟复杂任务方面的能力日益增强，评估这些模型在模拟任务中的表现变得至关重要。然而，现有的评估基准主要集中在单轮、静态的用户与LLMs之间的交互，缺乏对多轮交互和复杂模拟能力的评估。此外，现有的基准主要集中在与人类相关的模拟任务上，而忽略了非人类中心的模拟任务，如Linux终端或文本游戏等。<br><br>## 🚀 核心方法<br>💡 创新点1：SimulBench基准<br>本文提出了SimulBench，一个旨在评估LLMs在创意模拟任务中的表现的基准。SimulBench包含109个独特的模拟任务，涵盖了各种接口，如Linux终端、SQL执行器、文本游戏等。<br><br>💡 创新点2：多轮脚本评估框架<br>为了公平地评估不同LLMs，SimulBench采用了一个三阶段的评估框架。首先，使用一个固定的LLM作为用户代理与另一个LLM进行多轮对话，收集对话历史。然后，从这些对话历史中提取具有挑战性的对话脚本，用于评估不同的目标LLMs。最后，使用GPT-4作为评估者，对目标LLMs在给定多轮对话脚本下的最终响应质量进行评估。<br><br>## 📈 实验结果<br>实验结果表明，SimulBench中的模拟任务对LLMs来说仍然是一个巨大的挑战，并且显示了专有模型和最先进的开源LLMs之间的差距。例如，GPT-4-turbo在18.55%的情况下优于LLaMA-3-70b-Chat。<br><br>## 💬 可借鉴之处<br>SimulBench基准为评估LLMs在模拟任务中的表现提供了一个有价值的工具。其多轮脚本评估框架可以确保公平的比较，并有助于研究人员更好地理解LLMs在不同模拟任务中的表现。此外，SimulBench的实验结果也揭示了LLMs在处理复杂模拟任务时的挑战和局限性，为未来的研究提供了方向。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了一个名为 SimulBench 的基准，旨在评估大型语言模型（LLMs）在创意模拟任务中的表现。SimulBench 包含了 109 个独特的模拟任务，涵盖了各种接口，如 Linux 终端、SQL 执行器、文本游戏等。这些任务旨在测试 LLMs 在模拟复杂任务方面的能力，例如模拟 Linux 终端或与用户玩文本游戏。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确提及 SimulBench 基准所需的设备条件。然而，由于 SimulBench 需要使用大型语言模型进行评估，因此需要具备足够的计算资源，例如高性能的 GPU 和足够的内存。此外，由于 SimulBench 需要进行多轮对话，因此还需要具备足够的存储空间来存储对话历史。<br><br>论文中提到，模型训练和推理使用了 OpenAI 的 GPT-3.5-turbo 和 GPT-4。这些模型是通过 API 请求获得的，因此需要具备访问 OpenAI API 的权限。此外，由于 GPT-4 是一个大型模型，因此需要具备足够的计算资源来进行推理。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>SimulBench 基准使用 GPT-4 作为评估者，对目标 LLMs 在给定多轮对话脚本下的最终响应质量进行评估。这种评估方式可以确保公平的比较，并有助于研究人员更好地理解 LLMs 在不同模拟任务中的表现。然而，由于 SimulBench 基准的评估方式是基于对话脚本的质量，因此可能存在 reward hacking 的风险。<br><br>为了降低 reward hacking 的风险，SimulBench 基准采用了多轮脚本评估框架，并使用 GPT-4 作为评估者。这种评估方式可以确保评估的公平性和可靠性。此外，SimulBench 基准还采用了多种策略来识别具有挑战性的对话历史，从而确保评估的准确性。<br><br>尽管如此，SimulBench 基准仍然存在一定的 reward hacking 风险。为了进一步降低这种风险，可以考虑采用更复杂的评估方式，例如结合人工评估和自动评估，或者使用更先进的评估指标。</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</td>
      <td>Video games have served as useful benchmarks for the decision making<br>community, but going beyond Atari games towards training agents in modern games<br>has been prohibitively expensive for the vast majority of the research<br>community. Recent progress in the research, development and open release of<br>large vision models has the potential to amortize some of these costs across<br>the community. However, it is currently unclear which of these models have<br>learnt representations that retain information critical for sequential decision<br>making. Towards enabling wider participation in the research of gameplaying<br>agents in modern games, we present a systematic study of imitation learning<br>with publicly available visual encoders compared to the typical, task-specific,<br>end-to-end training approach in Minecraft, Minecraft Dungeons and<br>Counter-Strike: Global Offensive.</td>
      <td>## 🌟 论文解读 | 视觉编码器在现代视频游戏中的高效模仿学习<br><br>## 📌 背景痛点/本文动机<br>视频游戏一直是决策制定社区的有用基准，但将研究扩展到现代游戏对于大多数研究社区来说成本高昂。近年来，大型视觉模型的研究、开发和公开发布有可能在整个社区中分摊这些成本。然而，目前尚不清楚这些模型中的哪些模型已经学习了保留对顺序决策至关重要的信息的表示。为了使更广泛的社区参与现代游戏中的游戏代理研究，本文对Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive中的模仿学习进行了系统研究，并与典型的、特定任务的端到端训练方法进行了比较。<br><br>## 🚀 核心方法<br>💡 创新点1：本文比较了端到端训练的视觉编码器和公开可用的预训练编码器在模仿学习中的表现。端到端训练的编码器在相对较小的图像上训练，而预训练编码器则是在大型数据集上训练的，可能提供有用且通用的表示，而无需额外的训练。<br><br>💡 创新点2：本文研究了不同数量的训练数据对视觉编码器性能的影响。结果表明，即使在使用少量高质量数据的情况下，预训练编码器也能表现出与特定任务编码器相当或更好的性能。<br><br>## 📈 实验结果<br>本文在三个现代视频游戏（Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive）中进行了实验，结果表明：<br><br>1. 小图像（128×128）足以训练现代视频游戏中的代理，即使在使用少量高质量数据的情况下也能取得良好的性能。<br>2. 预训练编码器，特别是DINOv2，在模仿学习中表现出色，并且在使用少量数据时仍然有效。<br>3. 端到端训练的编码器在处理更真实世界的图像时表现更好，但在使用预训练编码器时需要仔细考虑图像大小和比例。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，预训练编码器在现代视频游戏的模仿学习中具有巨大的潜力。研究人员可以利用这些编码器来训练代理，从而降低成本并提高效率。此外，本文还强调了图像大小和比例对预训练编码器性能的重要性，这为未来的研究提供了有价值的见解。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文的benchmark是针对现代视频游戏中的模仿学习。具体来说，论文在三个现代视频游戏（Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive）中进行了实验，比较了端到端训练的视觉编码器和公开可用的预训练编码器在模仿学习中的表现。实验结果表明，预训练编码器，特别是DINOv2，在模仿学习中表现出色，并且在使用少量数据时仍然有效。此外，论文还发现，即使在使用少量高质量数据的情况下，小图像（128×128）也能训练出现代视频游戏中的代理，并取得良好的性能。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中使用的设备条件如下：<br><br>* **训练设备**： Azure compute，使用了Nvidia 16GB V100s、32GB V100s和A6000 GPUs。<br>* **推理设备**： 对于Minecraft Dungeons，使用了Azure虚拟机运行Windows 10，配备了M60 GPUs、24 CPU cores和224GB of RAM。对于Minecraft，使用了远程Linux机器，配备了A6000 GPUs。对于Counter-Strike: Global Offensive，使用了本地Windows机器，配备了Nvidia Titan X。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中并没有明确说明这个benchmark的环境是否具有高质量的结果奖励或过程奖励。但是，论文中提到，实验结果表明，即使在使用少量高质量数据的情况下，小图像（128×128）也能训练出现代视频游戏中的代理，并取得良好的性能。这表明，这个benchmark的环境可能具有一定的挑战性，能够支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Predicting Outcomes in Video Games with Long Short Term Memory Networks</td>
      <td>Forecasting winners in E-sports with real-time analytics has the potential to<br>further engage audiences watching major tournament events. However, making such<br>real-time predictions is challenging due to unpredictable variables within the<br>game involving diverse player strategies and decision-making. Our work attempts<br>to enhance audience engagement within video game tournaments by introducing a<br>real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)<br>based approach enables efficient predictions of win-lose outcomes by only using<br>the health indicator of each player as a time series. As a proof of concept, we<br>evaluate our model's performance within a classic, two-player arcade game,<br>Super Street Fighter II Turbo. We also benchmark our method against state of<br>the art methods for time series forecasting; i.e. Transformer models found in<br>large language models (LLMs). Finally, we open-source our data set and code in<br>hopes of furthering work in predictive analysis for arcade games.</td>
      <td>## 🌟 论文解读 | 利用长短期记忆网络预测电子竞技比赛结果<br><br>## 📌 背景痛点/本文动机<br>随着电子竞技（Esports）的日益流行，观众对于实时比赛结果的预测产生了浓厚的兴趣。然而，由于游戏中的变量众多，包括玩家策略和决策的不确定性，实时预测比赛结果一直是一个挑战。本文旨在通过引入一种实时预测方法来增强观众在电子游戏锦标赛中的参与度。<br><br>## 🚀 核心方法<br>💡 创新点1：使用长短期记忆网络（LSTM）进行实时预测<br>本文提出了一种基于LSTM的实时预测方法，该方法仅使用每个玩家的健康指示器作为时间序列来预测胜负结果。这种方法能够有效地处理时间序列数据，并捕捉游戏中的动态变化。<br><br>💡 创新点2：在经典的双人街机游戏《超级街头霸王II Turbo》中评估模型性能<br>为了验证模型的有效性，本文在经典的双人街机游戏《超级街头霸王II Turbo》中评估了模型的性能。通过分析玩家健康指示器的变化，模型能够在比赛的不同阶段进行实时预测。<br><br>💡 创新点3：与大型语言模型中的Transformer模型进行基准测试<br>为了进一步验证模型的有效性，本文将LSTM模型与大型语言模型中的Transformer模型进行了基准测试。结果表明，LSTM模型在预测比赛结果方面表现出了更高的准确率。<br><br>## 📈 实验结果<br>实验结果表明，LSTM模型在预测比赛结果方面表现出了较高的准确率。在比赛的不同阶段（25%，75%，95%），LSTM模型的ROC-AUC分数均高于Transformer模型。此外，LSTM模型的训练时间也相对较短，更适合实时预测场景。<br><br>## 💬 可借鉴之处<br>本文提出的基于LSTM的实时预测方法为电子竞技比赛结果的预测提供了一种新的思路。该方法可以应用于其他电子竞技游戏，并有助于提高观众在比赛中的参与度。此外，本文还开源了数据集和代码，为其他研究人员提供了进一步研究的便利。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark主要是针对电子竞技比赛结果的实时预测。论文中使用了长短期记忆网络（LSTM）和Transformer模型进行实验，并将这两种模型在预测比赛结果方面的性能进行了比较。实验数据来自于经典的双人街机游戏《超级街头霸王II Turbo》的比赛视频，通过分析玩家健康指示器的变化，模型能够在比赛的不同阶段进行实时预测。实验结果表明，LSTM模型在预测比赛结果方面表现出了更高的准确率，尤其是在比赛的不同阶段（25%，75%，95%），LSTM模型的ROC-AUC分数均高于Transformer模型。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明benchmark所需的设备条件，但根据论文内容可以推测，由于LSTM和Transformer模型都需要大量的计算资源，因此可能需要高性能的计算机，例如配备多个GPU和足够的内存。至于本文的模型训练和推理所使用的设备，论文中也没有明确说明，但可以推测，由于论文是在Keck Data Science Institute进行的，因此可能使用了该机构的高性能计算资源。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有提到benchmark环境是否具有高质量的结果奖励或过程奖励，也没有提到是否支持RL类模型。因此，无法确定这个benchmark是否容易受到reward hacking的影响，以及是否支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>23</th>
      <td>EmoLLM: Multimodal Emotional Understanding Meets Large Language Models</td>
      <td>Multi-modal large language models (MLLMs) have achieved remarkable<br>performance on objective multimodal perception tasks, but their ability to<br>interpret subjective, emotionally nuanced multimodal content remains largely<br>unexplored. Thus, it impedes their ability to effectively understand and react<br>to the intricate emotions expressed by humans through multimodal media. To<br>bridge this gap, we introduce EmoBench, the first comprehensive benchmark<br>designed specifically to evaluate the emotional capabilities of MLLMs across<br>five popular emotional tasks, using a diverse dataset of 287k images and videos<br>paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a<br>novel model for multimodal emotional understanding, incorporating with two core<br>techniques. 1) Multi-perspective Visual Projection, it captures diverse<br>emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it<br>guides MLLMs to reason about emotions in the correct direction. Experimental<br>results demonstrate that EmoLLM significantly elevates multimodal emotional<br>understanding performance, with an average improvement of 12.1% across multiple<br>foundation models on EmoBench. Our work contributes to the advancement of MLLMs<br>by facilitating a deeper and more nuanced comprehension of intricate human<br>emotions, paving the way for the development of artificial emotional<br>intelligence capabilities with wide-ranging applications in areas such as<br>human-computer interaction, mental health support, and empathetic AI systems.<br>Code, data, and model will be released.</td>
      <td>## 🌟 论文解读 | EmoLLM：多模态情感理解与大型语言模型的结合<br><br>## 📌 背景痛点/本文动机<br>随着多模态大型语言模型（MLLMs）在目标多模态感知任务上取得了显著成果，但它们在解释主观、情感丰富的多模态内容方面的能力仍然没有得到充分探索。这阻碍了它们有效地理解和反应人类通过多模态媒体表达的情感。为了弥合这一差距，本文提出了EmoBench，这是第一个专门设计用于评估MLLMs在五个流行情感任务中的情感能力的全面基准，使用了一个包含287k图像和视频以及相应文本指令的多样化数据集。同时，本文提出了EmoLLM，这是一种用于多模态情感理解的新型模型，结合了两种核心技术。<br><br>## 🚀 核心方法<br>💡 创新点1：多视角视觉投影<br>它从多个视角捕获视觉数据中的多样化情感线索。<br><br>💡 创新点2：EmoPrompt<br>它引导MLLMs在正确的方向上推理情感。<br><br>## 📈 实验结果<br>实验结果表明，EmoLLM显著提高了多模态情感理解性能，在EmoBench上多个基础模型平均提高了12.1%。<br><br>## 💬 可借鉴之处<br>本文提出的EmoBench基准和EmoLLM模型为MLLMs在情感理解方面的研究提供了新的思路和方法，有助于推动MLLMs在情感智能领域的进一步发展。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是EmoBench，它是一个专门设计用于评估多模态大型语言模型（MLLMs）在情感理解方面的能力的全面基准。EmoBench包含五个流行的情感任务，使用了一个包含约287k图像和视频以及相应文本指令的多样化数据集。这些任务包括情感识别、意图理解、仇恨检测、讽刺检测和幽默检测。EmoBench旨在提供一个丰富的训练材料来源，以增强MLLMs的性能，并评估它们在情感理解方面的能力。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中提到，EmoLLM模型的训练在一个拥有4个RTX 4090 GPU的机器上进行，每个epoch的训练时间大约为5小时。对于推理部分，论文没有明确说明所需的设备条件，但通常情况下，推理所需的资源会比训练少。因此，一个具有足够内存和计算能力的GPU或CPU应该足以进行推理。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确提到EmoBench是否具有高质量的结果奖励或过程奖励，以及是否支持RL类模型。然而，由于EmoBench是一个基于任务的基准，它可能更适合于评估模型在特定任务上的性能，而不是支持RL类模型。RL类模型通常需要更复杂的奖励机制和训练过程，而EmoBench可能没有提供这样的环境。</td>
    </tr>
    <tr>
      <th>24</th>
      <td>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</td>
      <td>With ChatGPT-like large language models (LLM) prevailing in the community,<br>how to evaluate the ability of LLMs is an open question. Existing evaluation<br>methods suffer from following shortcomings: (1) constrained evaluation<br>abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that<br>task-based evaluation, where LLM agents complete tasks in a simulated<br>environment, is a one-for-all solution to solve above problems. We present<br>AgentSims, an easy-to-use infrastructure for researchers from all disciplines<br>to test the specific capacities they are interested in. Researchers can build<br>their evaluation tasks by adding agents and buildings on an interactive GUI or<br>deploy and test new support mechanisms, i.e. memory, planning and tool-use<br>systems, by a few lines of codes. Our demo is available at<br>https://agentsims.com .</td>
      <td>## 🌟 论文解读 | AgentSims：大型语言模型评估的开放源代码沙盒<br><br>## 📌 背景痛点/本文动机<br>随着ChatGPT等大型语言模型（LLM）在社区中的普及，如何评估LLM的能力成为一个开放性问题。现有的评估方法存在以下不足：<br>1. 评估能力受限：大多数任务采用单轮问答格式，无法全面评估LLM的各种能力。<br>2. 基准易受攻击：由于LLM具有大量的预训练知识，测试集容易无意中混入训练集。<br>3. 指标不客观：现有的开放式问答指标涉及自动指标和主观指标，无法客观评估LLM的能力。<br><br>## 🚀 核心方法<br>本文提出了基于任务的评估方法，即LLM代理在模拟环境中完成任务来证明其能力。为了解决现有评估方法的不足，本文提出了AgentSims，一个易于使用的评估LLM能力的平台。AgentSims具有以下特点：<br>1. 可扩展性和可组合性：允许用户组合不同的计划、记忆和使用工具系统，研究各种系统设计的影响和有效性。<br>2. 交互式用户界面：为地图设计和代理创建提供交互式UI，降低非专业人士的入门门槛。<br>3. 标准化实现：确保实验结果的再现性。<br><br>## 📈 实验结果<br>本文展示了AgentSims在评估LLM能力方面的应用，包括：<br>1. 评估LLM的社会能力，如心智理论（ToM）。<br>2. 评估LLM的长期规划和组织能力，如担任市长或公司总裁。<br>3. 作为数据生成平台，用于数据标注和增强。<br>4. 为社会科学研究提供可控的初步实验环境。<br><br>## 💬 可借鉴之处<br>AgentSims为LLM评估提供了一个开放源代码的沙盒平台，具有以下可借鉴之处：<br>1. 基于任务的评估方法，可以更全面地评估LLM的能力。<br>2. 交互式用户界面，降低非专业人士的入门门槛。<br>3. 标准化实现，确保实验结果的再现性。<br>4. 可扩展性和可组合性，方便用户研究和开发新的支持系统。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是AgentSims，它是一个开放源代码的沙盒平台，用于评估大型语言模型（LLM）的能力。AgentSims通过模拟一个社会-经济环境，让LLM驱动的代理完成预定义的任务目标来证明其能力。这种基于任务的评估方法可以更全面地评估LLM的能力，包括社会能力、长期规划和组织能力等。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并没有明确说明AgentSims需要什么设备条件。但是，由于AgentSims是一个基于Python的框架，并且使用了Tornado web服务器和Unity前端，因此它应该可以在大多数现代计算机上运行。至于模型训练和推理使用的设备，论文中也没有明确说明。但是，由于LLM通常需要大量的计算资源，因此训练和推理LLM模型可能需要高性能的GPU和足够的内存。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>AgentSims的环境并没有明确提到结果奖励或过程奖励。然而，由于AgentSims是一个基于任务的评估平台，代理完成任务的成功率可以作为评估LLM能力的指标。这种指标是客观的，因为它不依赖于任何黑盒评级过程，如深度神经网络或人脑。因此，AgentSims可以支持RL类模型在这个平台上进行评估，并且不容易受到reward hacking的影响。</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Benchmarking End-to-End Behavioural Cloning on Video Games</td>
      <td>Behavioural cloning, where a computer is taught to perform a task based on<br>demonstrations, has been successfully applied to various video games and<br>robotics tasks, with and without reinforcement learning. This also includes<br>end-to-end approaches, where a computer plays a video game like humans do: by<br>looking at the image displayed on the screen, and sending keystrokes to the<br>game. As a general approach to playing video games, this has many inviting<br>properties: no need for specialized modifications to the game, no lengthy<br>training sessions and the ability to re-use the same tools across different<br>games. However, related work includes game-specific engineering to achieve the<br>results. We take a step towards a general approach and study the general<br>applicability of behavioural cloning on twelve video games, including six<br>modern video games (published after 2010), by using human demonstrations as<br>training data. Our results show that these agents cannot match humans in raw<br>performance but do learn basic dynamics and rules. We also demonstrate how the<br>quality of the data matters, and how recording data from humans is subject to a<br>state-action mismatch, due to human reflexes.</td>
      <td>## 🌟 论文解读 | 行为克隆在视频游戏中的端到端基准测试<br><br>## 📌 背景痛点/本文动机<br>行为克隆是一种基于人类演示来训练计算机执行任务的机器学习方法。它已被成功应用于各种视频游戏和机器人任务，包括端到端方法，其中计算机通过观察屏幕上的图像并发送按键来像人类一样玩游戏。这种方法具有许多吸引人的特性，例如无需对游戏进行特殊修改、无需长时间训练以及能够在不同游戏之间重用相同工具。然而，相关工作通常需要针对特定游戏进行工程化才能取得成果。本文旨在研究行为克隆在视频游戏中的通用性，并使用人类演示作为训练数据，在十二款视频游戏（包括六款现代视频游戏）上进行了研究。<br><br>## 🚀 核心方法<br>本文使用深度神经网络来学习人类玩家的行为，并使用人类演示数据来训练模型。模型通过观察屏幕图像来预测人类玩家的动作，并使用这些预测来控制游戏。本文还研究了数据质量和数量对行为克隆性能的影响，以及人类反应时间延迟对数据质量的影响。<br><br>## 📈 实验结果<br>实验结果表明，行为克隆代理可以学习游戏的基本动态和规则，但通常只能达到人类玩家性能的一小部分，有时甚至不如随机代理。本文还发现，当只有少量数据可用时，数据数量对结果的影响较小，而调整人类反应时间延迟可以提高性能。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，行为克隆在视频游戏中的应用具有潜力，但仍面临一些挑战。本文提出的建议包括使用高质量数据、调整人类反应时间延迟以及探索其他机器学习方法，以提高行为克隆的性能。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是针对行为克隆（BC）在视频游戏中的应用进行的。论文使用了12款视频游戏，包括6款现代视频游戏（2010年后发布），通过使用人类演示作为训练数据，评估了行为克隆代理在游戏中的表现。这些游戏包括经典的Atari 2600游戏、Doom（1993）以及各种现代视频游戏。实验结果表明，行为克隆代理可以学习游戏的基本动态和规则，但通常只能达到人类玩家性能的一小部分，有时甚至不如随机代理。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中提到，该研究部分得到了NVIDIA公司捐赠的Titan Xp GPU的支持。因此，可以推测模型训练和推理可能使用了NVIDIA的GPU。至于具体的机器配置，论文中没有详细说明。但是，由于使用了深度神经网络进行模型训练，因此可以推测需要一定数量的GPU和内存来支持模型的训练和推理。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中提到，行为克隆代理可以学习游戏的基本动态和规则，但通常只能达到人类玩家性能的一小部分，有时甚至不如随机代理。这表明，该benchmark的环境可能没有提供足够高质量的结果奖励或过程奖励，使得行为克隆代理难以达到人类玩家的水平。因此，该benchmark可能不支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Benchmarking Cognitive Abilities of the Brain with Computer Games</td>
      <td>Most of the players have experienced the feeling of temporarily losing their<br>character in a given gameplay situation when they cannot control the character,<br>simply because they temporarily cannot see it. The main reasons for this<br>feeling may be due to the interplay of the following factors: (1) the visual<br>complexity of the game is unexpectedly increased compared with the previous<br>time period as more and more game objects and effects are rendered on the<br>display; (2) and/or the game is lagging; (3) and finally, it is also possible<br>that the players have no sufficient experience with controlling the character.<br>This paper focuses on the first reason. We have developed a benchmark program<br>which allows its user to experience the feeling of losing character. While the<br>user can control the character well the benchmark program will increase the<br>visual complexity of the display. Otherwise, if the user lost the character<br>then the program will decrease the complexity until the user will find the<br>character again, and so on. The complexity is measured based on the number of<br>changed pixels between two consecutive display images. Our measurements show<br>that the average of bit per second values of losing and finding pairs describes<br>the user well. The final goal of this research is to further develop our<br>benchmark to a standard psychological test.</td>
      <td>## 🌟 论文解读 | 游戏中的认知能力评估：BrainB Test Series 6<br><br>## 📌 背景痛点/本文动机<br>在游戏过程中，玩家有时会因视觉复杂性的增加而暂时失去对角色的控制。这种现象可能由多种因素引起，包括游戏视觉复杂性的突然增加、游戏延迟以及玩家对角色控制经验的不足。本文重点关注视觉复杂性增加这一因素，并开发了一个名为 BrainB Test Series 6 的基准程序，用于模拟玩家失去角色的感觉，并评估其认知能力。<br><br>## 🚀 核心方法<br>💡 创新点1：BrainB Test Series 6 基准程序<br>该程序通过控制屏幕上移动的方块的视觉复杂性来模拟玩家失去角色的感觉。当玩家能够很好地控制角色时，程序会增加屏幕的视觉复杂性；如果玩家失去了角色，程序会降低复杂性，直到玩家再次找到角色。程序的复杂性基于连续显示图像之间变化的像素数量来衡量。<br><br>💡 创新点2：认知能力评估<br>通过测量玩家在失去和找到角色时屏幕复杂性的平均值，可以评估玩家的认知能力。研究表明，失去和找到角色时屏幕复杂性的平均值可以很好地描述玩家。<br><br>## 📈 实验结果<br>研究人员对 BrainB Test Series 6 进行了初步测试，并发现玩家在屏幕复杂性较高时更容易失去角色，而在屏幕复杂性较低时更容易找到角色。此外，研究人员还发现，通过调整鼠标设置和颜色方案，可以提高测试的准确性和可用性。<br><br>## 💬 可借鉴之处<br>本文提出的 BrainB Test Series 6 基准程序为评估玩家的认知能力提供了一种新的方法。该方法可以用于电子竞技人才选拔、认知能力研究等领域。此外，本文还强调了测试环境标准化和测试结果分析的重要性，为相关研究提供了参考。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是一个名为BrainB Test Series 6的程序，它通过控制屏幕上移动的方块的视觉复杂性来模拟玩家失去角色的感觉，并评估其认知能力。当玩家能够很好地控制角色时，程序会增加屏幕的视觉复杂性；如果玩家失去了角色，程序会降低复杂性，直到玩家再次找到角色。程序的复杂性基于连续显示图像之间变化的像素数量来衡量。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并没有明确说明BrainB Test Series 6程序对设备的具体要求。但是，从论文中提供的实验结果来看，该程序可以在Windows和Linux系统上运行，并且测试者使用了不同的设备，包括笔记本电脑、台式机以及触摸屏等。因此，可以推测该程序对设备的要求并不高，普通的计算机设备应该都可以满足其运行需求。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>BrainB Test Series 6程序并没有设计奖励机制，因此不存在reward hacking的问题。该程序主要通过测量玩家在失去和找到角色时屏幕复杂性的平均值来评估其认知能力。这种评估方式较为客观，不容易受到玩家主观因素的影响，因此可以支持RL类模型在该benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests</td>
      <td>We examine three evaluation paradigms: large question-answering benchmarks<br>(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and<br>cognitive tests (e.g., for working memory or theory of mind). First, we<br>investigate which of the former two-benchmarks or games-is most effective at<br>discriminating LLMs of varying quality. Then, inspired by human cognitive<br>assessments, we compile a suite of targeted tests that measure cognitive<br>abilities deemed essential for effective language use, and we investigate their<br>correlation with model performance in benchmarks and games. Our analyses reveal<br>that interactive games are superior to standard benchmarks in discriminating<br>models. Causal and logical reasoning correlate with both static and interactive<br>tests, while differences emerge regarding core executive functions and<br>social/emotional skills, which correlate more with games. We advocate the<br>development of new interactive benchmarks and targeted cognitive tasks inspired<br>by assessing human abilities but designed specifically for LLMs.</td>
      <td>## 🌟 论文解读 | 通过基准测试、游戏和认知测试来评估大型语言模型<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，如何有效地评估这些模型的能力变得至关重要。传统的评估方法主要依赖于大规模的问答基准测试，但这些测试往往存在数据污染和模型对提示格式敏感的问题。此外，这些测试忽略了实际的语言使用场景，即在社会和任务导向的多轮交互中使用语言。因此，本文旨在探索更有效的评估方法，以更好地理解LLMs的能力和局限性。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了三种评估范式：大规模问答基准测试（如MMLU和BBH）、交互式游戏（如信号游戏或禁忌游戏）和认知测试（如工作记忆或心智理论测试）。通过比较这些范式，研究团队旨在找出最有效的评估方法。<br><br>💡 创新点2：受人类认知评估的启发，研究团队设计了一系列针对性的测试，以衡量对有效语言使用至关重要的认知能力。这些测试旨在评估LLMs在因果推理、逻辑推理、工作记忆、情绪智力等方面的能力，并分析这些能力与模型在基准测试和游戏中的表现之间的相关性。<br><br>## 📈 实验结果<br>实验结果表明，交互式游戏在区分不同质量的LLMs方面优于传统的问答基准测试。因果推理和逻辑推理与静态和交互式测试都相关，而核心执行功能和社交/情感技能则更多地与游戏相关。特别是，工作记忆和情绪智力仅与游戏中的表现显著相关。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，交互式游戏是一种更有效的评估LLMs的方法，因为它能够更好地捕捉模型在实际语言使用场景中的能力。此外，针对性的认知测试可以帮助我们更好地理解LLMs的内在能力，并为未来的模型设计和评估提供指导。因此，本文的研究结果对于LLMs的评估和开发具有重要意义。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出了三种评估大型语言模型（LLMs）的范式：<br><br>1. **大规模问答基准测试**：例如MMLU（Measuring Massive Multitask Language Understanding）和BBH（BIG-Bench Hard）。这些测试评估LLMs在特定领域应用知识的能力，通常包括多选题，涵盖多个学术科目。<br><br>2. **交互式游戏**：例如信号游戏、禁忌游戏、Wordle及其变体、参考游戏、图像游戏和私人/共享游戏。这些游戏评估LLMs在对话游戏中的表现，模拟实际语言使用场景。<br><br>3. **认知测试**：例如工作记忆、认知灵活性、逻辑推理、因果推理、常识推理、规划、语用学、心智理论、归因和判断、社会常识推理和情绪智力。这些测试旨在衡量LLMs在因果推理、逻辑推理、工作记忆、情绪智力等方面的能力。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确提及进行这些基准测试所需的特定设备条件，如GPU数量或内存大小。然而，考虑到LLMs的规模和复杂性，进行这些测试通常需要高性能的计算资源。对于模型训练，可能需要多个高性能GPU和大量内存。对于模型推理，虽然资源需求较低，但仍然需要足够的计算能力来处理复杂的语言任务。<br><br>论文中提到的模型包括：<br><br>- Olmo-2-1124（7B和13B参数）<br>- Qwen2.5（7B、32B和72B参数）<br>- LLama-3（8B和70B参数）<br>- Falcon3-10B<br><br>这些模型的训练和推理可能需要不同的设备配置，具体取决于模型的规模和复杂度。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中并未明确提及benchmark环境中的奖励机制，特别是针对强化学习（RL）模型的奖励设计。然而，交互式游戏范式可能更适合RL模型，因为它们模拟了实际的语言使用场景，并提供了更丰富的交互机会。与传统的问答基准测试相比，交互式游戏可能更难进行reward hacking，因为它们需要模型在多轮对话中展现出更复杂的语言理解和生成能力。<br><br>为了支持RL模型在benchmark上大放异彩，可能需要设计更精细的奖励机制，以鼓励模型在长期对话中展现出更有效的语言使用能力。此外，还需要考虑如何评估模型在不同游戏中的表现，以及如何平衡不同认知能力的重要性。</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research</td>
      <td>After the great achievement of solving two-player zero-sum games, more and<br>more AI researchers focus on solving multiplayer games. To facilitate the<br>development of designing efficient learning algorithms for solving multiplayer<br>games, we propose a multiplayer game platform for solving Urban Network<br>Security Games (\textbf{UNSG}) that model real-world scenarios. That is,<br>preventing criminal activity is a highly significant responsibility assigned to<br>police officers in cities, and police officers have to allocate their limited<br>security resources to interdict the escaping criminal when a crime takes place<br>in a city. This interaction between multiple police officers and the escaping<br>criminal can be modeled as a UNSG. The variants of UNSGs can model different<br>real-world settings, e.g., whether real-time information is available or not,<br>and whether police officers can communicate or not. The main challenges of<br>solving this game include the large size of the game and the co-existence of<br>cooperation and competition. While previous efforts have been made to tackle<br>UNSGs, they have been hampered by performance and scalability issues.<br>Therefore, we propose an open-source UNSG platform (\textbf{GraphChase}) for<br>designing efficient learning algorithms for solving UNSGs. Specifically,<br>GraphChase offers a unified and flexible game environment for modeling various<br>variants of UNSGs, supporting the development, testing, and benchmarking of<br>algorithms. We believe that GraphChase not only facilitates the development of<br>efficient algorithms for solving real-world problems but also paves the way for<br>significant advancements in algorithmic development for solving general<br>multiplayer games.</td>
      <td>## 🌟 论文解读 | GraphChase：解决城市网络安全游戏的AI平台<br><br>## 📌 背景痛点/本文动机<br>随着人工智能在解决两人零和博弈方面取得巨大成就，越来越多的研究人员开始关注解决多人游戏。城市网络安全游戏（UNSG）作为一种模拟现实世界场景的多玩家游戏，对于研究多人博弈算法具有重要意义。然而，UNSG的解决面临着游戏规模庞大、合作与竞争共存等挑战，现有的算法在性能和可扩展性方面存在不足。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：GraphChase平台<br>本文提出了一个名为GraphChase的开源UNSG平台，旨在为研究人员提供一个统一的、灵活的游戏环境，用于模拟各种UNSG变体，并支持算法的开发、测试和基准测试。GraphChase平台具有以下特点：<br>- **灵活的游戏环境**：用户可以自定义游戏参数，包括图结构、初始位置、时间范围等，以模拟不同的UNSG场景。<br>- **多种算法支持**：GraphChase平台支持多种深度学习算法，例如CFR-MIX、NSG-NFSP、NSGZero、Pretrained PSRO和Grasper，方便研究人员进行算法比较和评估。<br>- **模块化设计**：GraphChase平台采用模块化设计，包括游戏模块、代理模块和求解器模块，方便用户进行自定义和扩展。<br><br>💡 创新点2：基准测试<br>本文在GraphChase平台上进行了实验，评估了现有算法的性能和可扩展性。实验结果表明，虽然现有算法在简单场景下能够取得合理性能，但在现实世界场景下仍然存在性能和可扩展性问题。这表明，开发高效且可扩展的算法来解决现实世界的UNSG仍然是一个重要的研究方向。<br><br>## 📈 实验结果<br>实验结果表明，GraphChase平台能够有效地模拟UNSG场景，并且能够加速算法的训练过程。此外，实验还发现，现有算法在解决复杂场景和大规模游戏时存在性能和可扩展性问题。<br><br>## 💬 可借鉴之处<br>GraphChase平台为研究UNSG和多人博弈算法提供了一个有价值的工具。其灵活性和可扩展性使得研究人员可以轻松地模拟不同的场景和测试不同的算法。此外，GraphChase平台还可以作为其他多人博弈问题的测试平台，例如对抗性团队游戏和追逃游戏。<br><br>## 📚 总结<br>GraphChase平台为解决城市网络安全游戏提供了一个重要的工具，并为研究多人博弈算法提供了新的思路。随着人工智能技术的不断发展，GraphChase平台有望在解决现实世界问题方面发挥更大的作用。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是GraphChase平台，它是一个开源的城市网络安全游戏（UNSG）平台，旨在为研究人员提供一个统一的、灵活的游戏环境，用于模拟各种UNSG变体，并支持算法的开发、测试和基准测试。GraphChase平台具有以下特点：<br><br>* **灵活的游戏环境**：用户可以自定义游戏参数，包括图结构、初始位置、时间范围等，以模拟不同的UNSG场景。<br>* **多种算法支持**：GraphChase平台支持多种深度学习算法，例如CFR-MIX、NSG-NFSP、NSGZero、Pretrained PSRO和Grasper，方便研究人员进行算法比较和评估。<br>* **模块化设计**：GraphChase平台采用模块化设计，包括游戏模块、代理模块和求解器模块，方便用户进行自定义和扩展。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>GraphChase平台的具体设备需求取决于所使用的算法和游戏场景的规模。一般来说，运行GraphChase平台需要一台配备GPU的计算机，以便加速深度学习算法的训练过程。对于大型游戏场景，可能需要更高性能的GPU和更多的内存。<br><br>本文的模型训练和推理使用了以下设备：<br><br>* **服务器**：48-core 3.00GHz Intel(R) Xeon(R) Gold 6248R CPU<br>* **GPU**：8 NVIDIA A30 GPUs<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>GraphChase平台使用的是结果奖励，即根据最终是否抓到逃犯来评估算法的性能。这种奖励机制可以有效地避免reward hacking，因为算法无法通过操纵中间过程来获得更高的奖励。<br><br>此外，GraphChase平台还支持多种算法，包括基于深度学习的强化学习算法，因此可以支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>29</th>
      <td>How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games</td>
      <td>The deployment of large language models (LLMs) in diverse applications<br>requires a thorough understanding of their decision-making strategies and<br>behavioral patterns. As a supplement to a recent study on the behavioral Turing<br>test, this paper presents a comprehensive analysis of five leading LLM-based<br>chatbot families as they navigate a series of behavioral economics games. By<br>benchmarking these AI chatbots, we aim to uncover and document both common and<br>distinct behavioral patterns across a range of scenarios. The findings provide<br>valuable insights into the strategic preferences of each LLM, highlighting<br>potential implications for their deployment in critical decision-making roles.</td>
      <td>## 🌟 论文解读 | 探索大型语言模型在行为经济学游戏中的行为模式<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在各个领域的广泛应用，理解这些模型的决策策略和行为模式变得至关重要。这不仅有助于优化它们在特定应用中的性能，还能更好地评估它们的可靠性和可预测性，尤其是在涉及重大责任的情境中。然而，目前对于不同LLMs的行为模式的研究仍然有限，尤其是在行为经济学游戏中的表现。<br><br>## 🚀 核心方法<br>💡 创新点1：通过行为经济学游戏评估LLMs<br>本文通过一系列经典的行为经济学游戏，如独裁者游戏、最后通牒游戏、信任游戏、公共物品游戏、炸弹风险游戏和囚徒困境游戏，对五种领先的LLM-based聊天机器人进行了全面分析。这些游戏旨在测试LLMs在信任、公平、风险规避、利他主义和合作等方面的行为模式。<br><br>💡 创新点2：引入行为图灵测试和分布相似性测试<br>为了评估LLMs的行为模式，本文引入了行为图灵测试和分布相似性测试。行为图灵测试通过比较LLMs和人类的行为分布，评估LLMs是否能够模仿人类的行为。分布相似性测试则使用Wasserstein距离来衡量LLMs的行为分布与人类行为分布之间的相似度。<br><br>## 📈 实验结果<br>实验结果表明，所有测试的聊天机器人都能成功地捕捉到特定的人类行为模式，导致决策分布高度集中。尽管旗舰聊天机器人表现出显著的通过图灵测试的概率，但它们只能产生与人类相似的行为分布。与人类相比，聊天机器人在收益偏好上更强调公平性。聊天机器人在不同游戏中的收益偏好可能存在不一致性。不同的聊天机器人在游戏中表现出不同的行为模式，这些模式可以通过图灵测试结果、收益偏好和行为的连贯性进一步区分。<br><br>## 💬 可借鉴之处<br>本文的研究结果为理解LLMs的行为模式提供了有价值的见解，并为未来在AI行为科学领域的研究奠定了基础。研究结果表明，LLMs在模仿人类行为方面仍然存在局限性，需要进一步的研究来提高LLMs在行为经济学游戏中的表现。此外，LLMs在不同游戏中的行为不一致性也表明，需要开发更通用的偏好和目标，以便LLMs能够有效地适应各种情境。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark是通过一系列行为经济学游戏来评估大型语言模型（LLMs）的行为模式。这些游戏包括独裁者游戏、最后通牒游戏、信任游戏、公共物品游戏、炸弹风险游戏和囚徒困境游戏。通过这些游戏，研究者可以测试LLMs在信任、公平、风险规避、利他主义和合作等方面的行为模式。此外，论文还引入了行为图灵测试和分布相似性测试来评估LLMs的行为模式。行为图灵测试通过比较LLMs和人类的行为分布，评估LLMs是否能够模仿人类的行为。分布相似性测试则使用Wasserstein距离来衡量LLMs的行为分布与人类行为分布之间的相似度。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明这个benchmark需要什么设备条件。但是，由于LLMs通常需要大量的计算资源，因此进行这个benchmark可能需要高性能的计算设备，例如具有多个GPU和大量内存的服务器。至于本文的模型训练和推理使用了什么设备，论文中也没有明确说明。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中没有明确说明这个benchmark的环境是否有一个高质量的结果奖励或者过程奖励。但是，由于这个benchmark是通过一系列行为经济学游戏来评估LLMs的行为模式，因此可能需要设计合适的奖励机制来引导LLMs在游戏中表现出期望的行为。此外，由于RL类模型通常需要大量的训练数据和时间，因此在这个benchmark上大放异彩可能需要更多的研究和开发工作。</td>
    </tr>
    <tr>
      <th>30</th>
      <td>AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games</td>
      <td>Decision-making in large-scale games is an essential research area in<br>artificial intelligence (AI) with significant real-world impact. However, the<br>limited access to realistic large-scale game environments has hindered research<br>progress in this area. In this paper, we present AuctionNet, a benchmark for<br>bid decision-making in large-scale ad auctions derived from a real-world online<br>advertising platform. AuctionNet is composed of three parts: an ad auction<br>environment, a pre-generated dataset based on the environment, and performance<br>evaluations of several baseline bid decision-making algorithms. More<br>specifically, the environment effectively replicates the integrity and<br>complexity of real-world ad auctions through the interaction of several<br>modules: the ad opportunity generation module employs deep generative networks<br>to bridge the gap between simulated and real-world data while mitigating the<br>risk of sensitive data exposure; the bidding module implements diverse<br>auto-bidding agents trained with different decision-making algorithms; and the<br>auction module is anchored in the classic Generalized Second Price (GSP)<br>auction but also allows for customization of auction mechanisms as needed. To<br>facilitate research and provide insights into the environment, we have also<br>pre-generated a substantial dataset based on the environment. The dataset<br>contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over<br>500 million auction records. Performance evaluations of baseline algorithms<br>such as linear programming, reinforcement learning, and generative models for<br>bid decision-making are also presented as a part of AuctionNet. We believe that<br>AuctionNet is applicable not only to research on bid decision-making in ad<br>auctions but also to the general area of decision-making in large-scale games.</td>
      <td>## 🌟 论文解读 | AuctionNet：大型游戏中决策制定的新基准<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，大型游戏中的决策制定是一个重要的研究方向，它对现实世界有着深远的影响。然而，由于缺乏对真实大型游戏环境的访问，这一领域的研究进展受到了限制。现有的模拟环境往往与真实环境存在较大差距，无法完全反映现实世界中的多智能体动态。<br><br>## 🚀 核心方法<br>💡 创新点1：AuctionNet环境<br>AuctionNet是一个基于真实在线广告平台的大型广告拍卖决策制定基准。它由三个部分组成：广告拍卖环境、基于环境的预生成数据集以及几个基线投标决策制定算法的性能评估。该环境通过几个模块的交互有效地复制了现实世界广告拍卖的完整性和复杂性：<br>- 广告机会生成模块：使用深度生成网络来弥合模拟数据和现实世界数据之间的差距，同时降低敏感数据泄露的风险。<br>- 投标模块：实现了多种自动投标代理，这些代理使用不同的决策制定算法进行训练。<br>- 拍卖模块：以经典的广义第二价格（GSP）拍卖为基础，但也允许根据需要定制拍卖机制。<br><br>💡 创新点2：预生成数据集<br>为了促进研究和提供对环境的洞察，AuctionNet还基于环境预生成了一个庞大的数据集。该数据集包含1000万个广告机会、48种不同的自动投标代理和超过5亿条拍卖记录。这些数据可以用于建模环境，并有效地训练自动投标代理。<br><br>## 📈 实验结果<br>AuctionNet已经为NeurIPS 2024“大型拍卖中的自动投标”竞赛提供了动力，为来自世界各地的1500多支队伍提供了竞赛环境。该竞赛解决了在不确定和竞争环境中进行高频投标决策制定的关键问题，并持续了4个月。AuctionNet提供的广告拍卖环境、数据集和基线投标决策制定算法为参赛者提供了准确和公平的性能评估，并激发了他们的创造力，推动了该领域的技术发展。<br><br>## 💬 可借鉴之处<br>AuctionNet不仅为研究人员提供了研究大型拍卖中自动投标算法的机会，还帮助研究人员和实践者在博弈论、强化学习、生成模型、运营优化等领域解决广泛的决策制定研究问题。此外，AuctionNet还可以用于研究其他大型游戏中的决策制定问题，例如在线广告、推荐系统、资源分配等。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark名为AuctionNet，它是一个用于大型广告拍卖中自动投标决策制定的基准。AuctionNet由三个主要部分组成：<br><br>1. **广告拍卖环境**：这个环境模拟了现实世界中的广告拍卖，包括广告机会生成模块、投标模块和拍卖模块。广告机会生成模块使用深度生成网络来生成与真实数据相似的广告机会，同时降低敏感数据泄露的风险。投标模块实现了多种自动投标代理，这些代理使用不同的决策制定算法进行训练。拍卖模块以经典的广义第二价格（GSP）拍卖为基础，但也允许根据需要定制拍卖机制。<br><br>2. **预生成数据集**：为了促进研究和提供对环境的洞察，AuctionNet还基于环境预生成了一个庞大的数据集。该数据集包含1000万个广告机会、48种不同的自动投标代理和超过5亿条拍卖记录。这些数据可以用于建模环境，并有效地训练自动投标代理。<br><br>3. **性能评估**：AuctionNet提供了对几种基线投标决策制定算法的性能评估，包括线性规划、强化学习和生成模型。这些评估有助于研究人员了解不同算法在AuctionNet环境中的表现，并为未来的研究提供参考。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中没有明确说明AuctionNet所需的设备条件，例如GPU数量和内存大小。然而，考虑到AuctionNet使用了深度生成网络和多种自动投标代理，可以推测它需要一定计算能力的设备。例如，可能需要至少一个高性能GPU（如NVIDIA Tesla V100或更高）和足够的内存（至少32GB）来运行AuctionNet环境、训练模型和进行推理。<br><br>至于本文中模型训练和推理所使用的设备，论文中没有提供具体信息。然而，考虑到论文的作者来自北京大学和阿里巴巴集团，可以推测他们使用了高性能计算集群来训练和推理模型。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>AuctionNet的环境设计考虑了结果奖励和过程奖励，以支持强化学习（RL）类模型。结果奖励基于代理在广告拍卖中获得的印象和转化，而过程奖励则考虑了代理的投标策略和预算约束。这种设计旨在鼓励代理制定有效的投标策略，同时避免过度优化特定指标（如点击率或转化率）。<br><br>此外，AuctionNet还提供了多种拍卖机制和广告机会，以模拟现实世界中的复杂性和不确定性。这有助于RL模型学习更通用的决策制定策略，并提高其在不同环境中的鲁棒性。<br><br>因此，AuctionNet的环境设计支持RL类模型在该benchmark上大放异彩，并推动自动投标决策制定领域的研究进展。</td>
    </tr>
    <tr>
      <th>31</th>
      <td>From Code to Play: Benchmarking Program Search for Games Using Large Language Models</td>
      <td>Large language models (LLMs) have shown impressive capabilities in generating<br>program code, opening exciting opportunities for applying program synthesis to<br>games. In this work, we explore the potential of LLMs to directly synthesize<br>usable code for a wide range of gaming applications, focusing on two<br>programming languages, Python and Java. We use an evolutionary hill-climbing<br>algorithm, where the mutations and seeds of the initial programs are controlled<br>by LLMs. For Python, the framework covers various game-related tasks, including<br>five miniature versions of Atari games, ten levels of Baba is You, an<br>environment inspired by Asteroids, and a maze generation task. For Java, the<br>framework contains 12 games from the TAG tabletop games framework. Across 29<br>tasks, we evaluated 12 language models for Python and 8 for Java. Our findings<br>suggest that the performance of LLMs depends more on the task than on model<br>size. While larger models generate more executable programs, these do not<br>always result in higher-quality solutions but are much more expensive. No model<br>has a clear advantage, although on any specific task, one model may be better.<br>Trying many models on a problem and using the best results across them is more<br>reliable than using just one.</td>
      <td>## 🌟 论文解读 | 从代码到游戏：使用大型语言模型进行游戏程序搜索的基准测试<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在生成程序代码方面的能力日益增强，将程序合成应用于游戏领域展现出巨大的潜力。然而，传统的程序合成方法通常依赖于预定义的领域特定语言（DSL）或JSON转换器，限制了其适用范围和灵活性。本文旨在探索LLMs直接合成可用于各种游戏应用的代码的潜力，并评估其在游戏领域的性能。<br><br>## 🚀 核心方法<br>本文提出了一种基于LLMs的程序搜索框架，用于评估LLMs在游戏领域合成程序代码的能力。该框架使用进化爬山算法，其中初始程序的突变和种子由LLMs控制。研究人员使用Python和Java两种编程语言，在29个不同的游戏任务上评估了12个Python语言模型和8个Java语言模型。<br><br>## 📈 实验结果<br>研究发现，LLMs的性能更多地取决于任务本身，而不是模型的大小。虽然更大的模型可以生成更多可执行的程序，但这些程序并不总是产生更高质量的解决方案，并且成本更高。没有模型在所有任务中都表现出明显的优势，但在特定任务上，某些模型可能表现更好。尝试多种模型并使用最佳结果可以提高可靠性。<br><br>## 💬 可借鉴之处<br>本文提出的框架为评估LLMs在游戏领域合成程序代码的能力提供了一个易于使用且可扩展的平台。研究结果表明，LLMs在游戏领域具有巨大的潜力，但仍存在一些局限性，例如生成的代码质量不高、可执行性差等。未来研究可以探索更复杂的搜索策略和更好的提示工程，以提高LLMs的性能。此外，使用多种模型可以降低成本并提高结果的可靠性。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark旨在评估大型语言模型（LLMs）在游戏领域合成程序代码的能力。该benchmark使用Python和Java两种编程语言，在29个不同的游戏任务上评估了12个Python语言模型和8个Java语言模型。这些任务涵盖了各种游戏相关的任务，包括五个迷你版本的Atari游戏、十个级别的Baba is You、一个受Asteroids启发的环境和一个迷宫生成任务。对于Java，该框架包含12个来自TAG桌面游戏框架的游戏。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>论文中并未明确指出benchmark所需的设备条件，例如GPU数量和内存大小。然而，考虑到LLMs的计算需求，运行这个benchmark可能需要高性能的计算资源。论文中提到，模型训练和推理使用了多种LLMs，包括GPT-4o、Claude 3.5、Llama 3.1、Mistral和Gemini等。这些模型的训练和推理可能需要不同的设备配置，具体取决于模型的规模和复杂性。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>论文中并未明确提到benchmark是否具有高质量的结果奖励或过程奖励，以防止reward hacking。然而，论文中提到，该benchmark评估了LLMs在不同游戏任务上的性能，并发现LLMs的性能更多地取决于任务本身，而不是模型的大小。这意味着，LLMs在特定任务上可能表现出更好的性能，但在其他任务上可能表现不佳。因此，虽然RL类模型可能在某些任务上表现出色，但在其他任务上可能无法取得良好的结果。</td>
    </tr>
    <tr>
      <th>32</th>
      <td>OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research</td>
      <td>Owning to the unremitting efforts by a few institutes, significant progress<br>has recently been made in designing superhuman AIs in No-limit Texas Hold'em<br>(NLTH), the primary testbed for large-scale imperfect-information game<br>research. However, it remains challenging for new researchers to study this<br>problem since there are no standard benchmarks for comparing with existing<br>methods, which seriously hinders further developments in this research area. In<br>this work, we present OpenHoldem, an integrated toolkit for large-scale<br>imperfect-information game research using NLTH. OpenHoldem makes three main<br>contributions to this research direction: 1) a standardized evaluation protocol<br>for thoroughly evaluating different NLTH AIs, 2) four publicly available strong<br>baselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs<br>for public NLTH AI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,<br>hoping it facilitates further studies on the unsolved theoretical and<br>computational issues in this area and cultivate crucial research problems like<br>opponent modeling and human-computer interactive learning.</td>
      <td>## 🌟 论文解读 | OpenHoldem：大规模不完全信息游戏研究的基准<br><br>## 📌 背景痛点/本文动机<br>近年来，在无限制德州扑克（NLTH）领域，设计出超越人类的AI取得了显著进展，NLTH已成为大规模不完全信息游戏研究的主要测试平台。然而，由于缺乏标准基准，新研究人员难以研究此问题，这严重阻碍了该研究领域的进一步发展。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：标准化评估协议<br>OpenHoldem 提出了一个标准化的评估协议，包括四种不同的评估指标，用于全面评估不同的 NLTH AI。这些指标包括：<br>- **对局评估指标**：通过重复对局，评估 AI 的平均效用，并使用方差减少技术（如复制扑克和 AIVAT）来减少随机性影响。<br>- **可利用性评估指标**：通过计算最佳响应策略，评估 AI 的可利用性，并使用局部最佳响应（LBR）和深度强化学习（DRL-BR）来近似计算。<br><br>💡 创新点2：公开可用的强基线 AI<br>OpenHoldem 设计并实现了四种不同类型的 NLTH AI，作为未来研究的良好起点：<br>- **基于规则的 AI**：由领域专家设计的规则集合，用于处理各种场景。<br>- **基于 CFR 的静态 AI**：使用 CFR 算法近似求解纳什均衡策略，并通过信息抽象和动作抽象来降低游戏规模。<br>- **类似 DeepStack 的在线 AI**：使用持续重解和深度神经网络来处理离树动作，并提高决策效率。<br>- **基于深度强化学习的 AI**：使用端到端深度强化学习框架，直接从游戏状态学习到动作，无需手动设计特征或进行迭代推理。<br><br>💡 创新点3：在线测试平台<br>OpenHoldem 开发了一个在线测试平台，内置了四种强基线 AI，并提供了易于使用的 API，方便研究人员测试和比较他们的 AI。<br><br>## 📈 实验结果<br>OpenHoldem 的基线 AI 在与现有公开可用的 NLTH AI 的比较中表现出色，证明了其有效性。此外，消融实验分析了不同设计选择对 AI 性能的影响，并验证了 OpenHoldem 的各个组件的有效性。<br><br>## 💬 可借鉴之处<br>OpenHoldem 为大规模不完全信息游戏研究提供了一个宝贵的工具，其标准化评估协议、强基线 AI 和在线测试平台将促进该领域的进一步发展。OpenHoldem 的设计思路和实现方法也为其他游戏 AI 研究提供了参考。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br><br>这篇论文提出的benchmark名为OpenHoldem，它是一个用于大规模不完全信息游戏研究的集成工具包，主要针对无限制德州扑克（NLTH）。OpenHoldem旨在解决当前NLTH AI研究中缺乏标准基准的问题，为新研究人员提供评估、比较和改进NLTH AI的平台。<br><br>OpenHoldem的主要组成部分包括：<br><br>1. **评估协议**：OpenHoldem提出了四种不同的评估指标，用于全面评估NLTH AI的性能。这些指标包括：<br>    - **对局评估指标**：通过重复对局，评估AI的平均效用，并使用方差减少技术（如复制扑克和AIVAT）来减少随机性影响。<br>    - **可利用性评估指标**：通过计算最佳响应策略，评估AI的可利用性，并使用局部最佳响应（LBR）和深度强化学习（DRL-BR）来近似计算。<br><br>2. **基线AI**：OpenHoldem设计并实现了四种不同类型的NLTH AI，作为未来研究的良好起点：<br>    - **基于规则的AI**：由领域专家设计的规则集合，用于处理各种场景。<br>    - **基于CFR的静态AI**：使用CFR算法近似求解纳什均衡策略，并通过信息抽象和动作抽象来降低游戏规模。<br>    - **类似DeepStack的在线AI**：使用持续重解和深度神经网络来处理离树动作，并提高决策效率。<br>    - **基于深度强化学习的AI**：使用端到端深度强化学习框架，直接从游戏状态学习到动作，无需手动设计特征或进行迭代推理。<br><br>3. **在线测试平台**：OpenHoldem开发了一个在线测试平台，内置了四种强基线AI，并提供了易于使用的API，方便研究人员测试和比较他们的AI。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br><br>OpenHoldem的设备需求取决于所使用的基线AI和评估方法。例如，基于规则的AI对设备要求较低，而基于深度强化学习的AI则需要较强的计算能力。<br><br>根据论文描述，模型训练和推理使用的设备如下：<br><br>- **基于规则的AI**：无需特殊设备，普通计算机即可运行。<br>- **基于CFR的静态AI**：需要一定的计算资源，但不需要GPU。<br>- **类似DeepStack的在线AI**：需要大量的GPU资源进行训练，例如论文中提到使用120个GPU进行训练。<br>- **基于深度强化学习的AI**：需要GPU进行训练，例如论文中提到使用8个GPU进行训练。推理阶段对设备要求较低，例如论文中提到使用单个2.00GHz的CPU即可满足需求。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br><br>OpenHoldem的环境为NLTH游戏，其奖励机制是游戏结果，即玩家赢得的筹码数量。由于NLTH游戏具有高度的不确定性和复杂性，因此不容易出现reward hacking现象。<br><br>此外，OpenHoldem提供了多种评估指标，包括对局评估指标和可利用性评估指标，可以更全面地评估RL类模型在NLTH游戏中的性能。因此，OpenHoldem支持RL类模型在这个benchmark上大放异彩。</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors</td>
      <td>Video game testing requires game-specific knowledge as well as common sense<br>reasoning about the events in the game. While AI-driven agents can satisfy the<br>first requirement, it is not yet possible to meet the second requirement<br>automatically. Therefore, video game testing often still relies on manual<br>testing, and human testers are required to play the game thoroughly to detect<br>bugs. As a result, it is challenging to fully automate game testing. In this<br>study, we explore the possibility of leveraging the zero-shot capabilities of<br>large language models for video game bug detection. By formulating the bug<br>detection problem as a question-answering task, we show that large language<br>models can identify which event is buggy in a sequence of textual descriptions<br>of events from a game. To this end, we introduce the GameBugDescriptions<br>benchmark dataset, which consists of 167 buggy gameplay videos and a total of<br>334 question-answer pairs across 8 games. We extensively evaluate the<br>performance of six models across the OPT and InstructGPT large language model<br>families on our benchmark dataset. Our results show promising results for<br>employing language models to detect video game bugs. With the proper prompting<br>technique, we could achieve an accuracy of 70.66%, and on some video games, up<br>to 78.94%. Our code, evaluation data and the benchmark can be found on<br>https://asgaardlab.github.io/LLMxBugs</td>
      <td>## 🌟 论文解读 | 大型语言模型在零样本视频游戏漏洞检测中的潜力<br><br>## 📌 背景痛点/本文动机<br>视频游戏测试需要游戏特定的知识和对游戏事件的常识推理。虽然 AI 驱动的代理可以满足第一个要求，但自动满足第二个要求仍然不可能。因此，视频游戏测试通常仍然依赖于手动测试，需要人类测试者彻底地玩游戏来检测漏洞。这使得完全自动化游戏测试具有挑战性。<br><br>## 🚀 核心方法<br>💡 创新点1：将漏洞检测问题表述为问答任务，利用大型语言模型的零样本能力来识别游戏事件序列中的漏洞事件。<br>💡 创新点2：引入 GameBugDescriptions 基准数据集，包含 167 个有漏洞的游戏玩法视频和 334 个问答对，涵盖 8 个游戏。<br>💡 创新点3：在基准数据集上评估了 OPT 和 InstructGPT 大型语言模型家族的六个模型的性能。<br>💡 创新点4：分析了语言模型对不同事件描述的鲁棒性。<br><br>## 📈 实验结果<br>实验结果表明，大型语言模型在视频游戏漏洞检测方面具有很大的潜力。通过适当的提示技术，可以实现 70.66% 的准确率，在某些视频游戏中甚至可以达到 78.94%。<br><br>## 💬 可借鉴之处<br>这篇论文展示了大型语言模型在视频游戏漏洞检测方面的潜力，为自动化游戏测试提供了新的思路。此外，论文中提出的 GameBugDescriptions 基准数据集可以用于评估和比较不同语言模型在漏洞检测任务上的性能。</td>
      <td>## 💡 解答问题1 (介绍这篇论文「提出的benchmark」，或者这篇论文「该方法进行实验的benchmark」)<br>这篇论文提出了一个名为 GameBugDescriptions 的基准数据集，用于评估大型语言模型在视频游戏漏洞检测方面的能力。该数据集包含 167 个有漏洞的游戏玩法视频和 334 个问答对，涵盖 8 个游戏。每个视频都有两个由不同作者撰写的文本描述，每个描述都包含对视频事件的描述，但没有对事件是否为漏洞进行推理。此外，每个视频都被标记为 9 种不同的漏洞类型之一。<br><br>## 💡 解答问题2 (这个benchmark需要什么设备条件? 例如：需要什么机器（多少GPU，多少内存）等。本文的模型 训练和推理分别使用了什么设备？)<br>论文中提到，他们使用了 NVIDIA DGX System，配备了 8 个 A100 GPU（每个 80GB）和 2TB 的系统内存来运行 OPT 模型。对于 InstructGPT 模型，他们使用了 OpenAI API。在实验中，他们设置了温度参数为 0，停止序列为 "Q:"，以防止重复，并在第一阶段和答案提取阶段分别设置了 max_tokens 为 256 和 32。<br><br>## 💡 解答问题3 (这个benchmark的环境，是否有一个高质量的结果奖励或者过程奖励，从而不容易reward hacking，从而支持RL类模型在这个benchmark上大放异彩？)<br>论文中没有明确提到 GameBugDescriptions 基准数据集是否包含奖励机制，也没有提到该数据集是否支持强化学习（RL）类模型。然而，由于该数据集旨在评估大型语言模型在视频游戏漏洞检测方面的能力，因此它可能更适合评估语言模型而非强化学习模型。</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        