
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>23</th>
      <td>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</td>
      <td>We present PCA-Bench, a multimodal decision-making benchmark for evaluating<br>the integrated capabilities of Multimodal Large Language Models (MLLMs).<br>Departing from previous benchmarks focusing on simplistic tasks and individual<br>model capability, PCA-Bench introduces three complex scenarios: autonomous<br>driving, domestic robotics, and open-world games. Given task instructions and<br>diverse contexts, the model is required to seamlessly integrate multiple<br>capabilities of Perception, Cognition, and Action in a reasoning chain to make<br>accurate decisions. Moreover, PCA-Bench features error localization<br>capabilities, scrutinizing model inaccuracies in areas such as perception,<br>knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To<br>balance accuracy and efficiency in evaluation, we propose PCA-Eval, an<br>automatic evaluation protocol, and assess 10 prevalent MLLMs. The results<br>reveal significant performance disparities between open-source models and<br>powerful proprietary models like GPT-4 Vision. To address this, we introduce<br>Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing<br>instruction tuning examples in multimodal embodied environments. EIE generates<br>7,510 training examples in PCA-Bench and enhances the performance of<br>open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision<br>accuracy), thereby validating the effectiveness of EIE. Our findings suggest<br>that robust MLLMs like GPT4-Vision show promise for decision-making in embodied<br>agents, opening new avenues for MLLM research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | PCA-Benchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­çš„å†³ç­–èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¾€å¾€åªå…³æ³¨å•ä¸ªæ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹åœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢çš„ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ç¼ºä¹å¯¹æ¨¡å‹é”™è¯¯è¿›è¡Œå®šä½çš„èƒ½åŠ›ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šæ¨¡å‹åœ¨å“ªäº›æ–¹é¢éœ€è¦æ”¹è¿›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPCA-Bench<br>æœ¬æ–‡æå‡ºäº†PCA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­å†³ç­–èƒ½åŠ›çš„å¤šæ¨¡æ€å†³ç­–åŸºå‡†ã€‚PCA-Benchå¼•å…¥äº†ä¸‰ä¸ªå¤æ‚çš„åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®ä»»åŠ¡æŒ‡ä»¤å’Œä¸åŒçš„ä¸Šä¸‹æ–‡ï¼Œæ— ç¼åœ°æ•´åˆæ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œä»¥åšå‡ºå‡†ç¡®çš„å†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPCA-Eval<br>ä¸ºäº†å¹³è¡¡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†PCA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ã€‚PCA-Evalåˆ©ç”¨LLMså¼ºå¤§çš„è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œæ ¹æ®æ•°æ®æ³¨é‡Šä¸­çš„é”šç‚¹ä¿¡æ¯ï¼Œè‡ªåŠ¨è¿›è¡Œé”™è¯¯å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCA-Evalä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ï¼Œå¹³å‡Kappaç³»æ•°è¾¾åˆ°0.8+ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEmbodied-Instruction-Evolution (EIE)<br>ä¸ºäº†è§£å†³PCA-Benchæ•°æ®é›†æ ‡æ³¨å·¥ä½œé‡å¤§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Embodied-Instruction-Evolution (EIE)æ¡†æ¶ã€‚EIEåˆ©ç”¨LLMsè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹ï¼Œä»è€Œå‡å°‘äº†äººå·¥åŠ³åŠ¨ï¼Œå¹¶æé«˜äº†PCA-Benchçš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4 Visionåœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºMLLMsã€‚EIEæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜å¼€æºMLLMsçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†GPT-4 Visionã€‚PCA-Evalèƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½æ¨¡å‹é”™è¯¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„PCA-Benchå’ŒPCA-Evalä¸ºè¯„ä¼°MLLMsçš„å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚EIEæ¡†æ¶ä¸ºè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„MLLMsåœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„å†³ç­–èƒ½åŠ›å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä¸ºMLLMsçš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>125</th>
      <td>DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments</td>
      <td>Large Language Model~(LLM) based agents have been increasingly popular in<br>solving complex and dynamic tasks, which requires proper evaluation systems to<br>assess their capabilities. Nevertheless, existing benchmarks usually either<br>focus on single-objective tasks or use overly broad assessing metrics, failing<br>to provide a comprehensive inspection of the actual capabilities of LLM-based<br>agents in complicated decision-making tasks. To address these issues, we<br>introduce DSGBench, a more rigorous evaluation platform for strategic<br>decision-making. Firstly, it incorporates six complex strategic games which<br>serve as ideal testbeds due to their long-term and multi-dimensional<br>decision-making demands and flexibility in customizing tasks of various<br>difficulty levels or multiple targets. Secondly, DSGBench employs a<br>fine-grained evaluation scoring system which examines the decision-making<br>capabilities by looking into the performance in five specific dimensions and<br>offering a comprehensive assessment in a well-designed way. Furthermore,<br>DSGBench also incorporates an automated decision-tracking mechanism which<br>enables in-depth analysis of agent behaviour patterns and the changes in their<br>strategies. We demonstrate the advances of DSGBench by applying it to multiple<br>popular LLM-based agents and our results suggest that DSGBench provides<br>valuable insights in choosing LLM-based agents as well as improving their<br>future development. DSGBench is available at<br>https://github.com/DeciBrain-Group/DSGBench.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DSGBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å†³ç­–ç¯å¢ƒä¸­çš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚å’ŒåŠ¨æ€ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„å®é™…èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ç³»ç»Ÿé€šå¸¸åªå…³æ³¨å•ä¸€ç›®æ ‡ä»»åŠ¡æˆ–ä½¿ç”¨è¿‡äºå®½æ³›çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMæ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„å®é™…èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDSGBenchæ˜¯ä¸€ä¸ªæ›´ä¸¥æ ¼çš„è¯„ä¼°å¹³å°ï¼Œç”¨äºè¯„ä¼°æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚å®ƒåŒ…å«äº†å…­ä¸ªå¤æ‚çš„æˆ˜ç•¥æ¸¸æˆï¼Œè¿™äº›æ¸¸æˆå› å…¶é•¿æœŸå’Œå¤šç»´åº¦çš„å†³ç­–éœ€æ±‚ä»¥åŠå®šåˆ¶å„ç§éš¾åº¦çº§åˆ«æˆ–å¤šä¸ªç›®æ ‡çš„ä»»åŠ¡çš„çµæ´»æ€§è€Œæˆä¸ºç†æƒ³çš„æµ‹è¯•å¹³å°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDSGBenché‡‡ç”¨äº†ä¸€ç§ç»†ç²’åº¦çš„è¯„ä¼°è¯„åˆ†ç³»ç»Ÿï¼Œé€šè¿‡è€ƒå¯Ÿåœ¨äº”ä¸ªç‰¹å®šç»´åº¦ä¸­çš„è¡¨ç°æ¥æ£€æŸ¥å†³ç­–èƒ½åŠ›ï¼Œå¹¶ä»¥ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„æ–¹å¼æä¾›å…¨é¢çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼ŒDSGBenchè¿˜åŒ…å«ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å†³ç­–è·Ÿè¸ªæœºåˆ¶ï¼Œèƒ½å¤Ÿæ·±å…¥åˆ†æä»£ç†çš„è¡Œä¸ºæ¨¡å¼å’Œç­–ç•¥çš„å˜åŒ–ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>DSGBenché€šè¿‡åº”ç”¨äºå¤šä¸ªæµè¡Œçš„LLMæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨é€‰æ‹©LLMæ¨¡å‹ä»¥åŠæ”¹è¿›å…¶æœªæ¥å‘å±•æ–¹é¢çš„ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSGBenchèƒ½å¤Ÿæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMæ¨¡å‹åœ¨ä¸åŒå†³ç­–ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DSGBenchä¸ºè¯„ä¼°LLMæ¨¡å‹åœ¨å¤æ‚å†³ç­–ç¯å¢ƒä¸­çš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ã€‚å…¶ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡å’Œå†³ç­–è·Ÿè¸ªæœºåˆ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ·±å…¥äº†è§£LLMæ¨¡å‹çš„è¡Œä¸ºæ¨¡å¼å’Œç­–ç•¥å˜åŒ–ï¼Œä»è€Œæ›´å¥½åœ°æ”¹è¿›æ¨¡å‹çš„è®¾è®¡å’Œå¼€å‘ã€‚æ­¤å¤–ï¼ŒDSGBenchçš„çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¯„ä¼°éœ€æ±‚ï¼Œä¸ºLLMæ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚</td>
    </tr>
    <tr>
      <th>133</th>
      <td>TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs</td>
      <td>The rapid advancement of large language models (LLMs) has accelerated their<br>application in reasoning, with strategic reasoning drawing increasing<br>attention. To evaluate LLMs' strategic reasoning capabilities, game theory,<br>with its concise structure, has become a preferred approach. However, current<br>research focuses on a limited selection of games, resulting in low coverage.<br>Classic game scenarios risk data leakage, and existing benchmarks often lack<br>extensibility, making them inadequate for evaluating state-of-the-art models.<br>To address these challenges, we propose TMGBench, a benchmark with<br>comprehensive game type coverage, novel scenarios, and flexible organization.<br>Specifically, we incorporate all 144 game types summarized by the<br>Robinson-Goforth topology of 2x2 games, constructed as classic games. We also<br>employ synthetic data generation to create diverse, higher-quality scenarios<br>through topic guidance and human inspection, referred to as story-based games.<br>Lastly, we provide a sustainable framework for increasingly powerful LLMs by<br>treating these games as atomic units and organizing them into more complex<br>forms via sequential, parallel, and nested structures. Our comprehensive<br>evaluation of mainstream LLMs covers tests on rational reasoning, robustness,<br>Theory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in<br>accuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,<br>OpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and<br>70.0% on sequential, parallel, and nested games, highlighting TMGBench's<br>challenges.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | TMGBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿæ¸¸æˆåŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶ä¸­æˆ˜ç•¥æ¨ç†èƒ½åŠ›å°¤å…¶å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¾€å¾€å±€é™äºå°‘æ•°ç»å…¸æ¸¸æˆï¼Œå¯¼è‡´æ¸¸æˆç±»å‹çš„è¦†ç›–ç‡ä½ï¼Œä¸”ç»å…¸æ¸¸æˆåœºæ™¯å­˜åœ¨æ•°æ®æ³„éœ²çš„é£é™©ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œéš¾ä»¥è¯„ä¼°æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TMGBenchï¼Œä¸€ä¸ªå…·æœ‰å…¨é¢æ¸¸æˆç±»å‹è¦†ç›–ç‡ã€æ–°é¢–åœºæ™¯å’Œçµæ´»ç»„ç»‡æ–¹å¼çš„åŸºå‡†æµ‹è¯•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢çš„æ¸¸æˆç±»å‹è¦†ç›–ç‡<br>TMGBenchåŒ…å«äº†ç”±Robinson-Goforthæ‹“æ‰‘ç»“æ„æ€»ç»“çš„144ç§2x2æ¸¸æˆç±»å‹ï¼Œæ¶µç›–äº†å„ç§ä¸åŒçš„æ¸¸æˆç»“æ„ï¼ŒåŒ…æ‹¬ç»å…¸æ¸¸æˆå¦‚å›šå¾’å›°å¢ƒç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–°é¢–çš„åœºæ™¯<br>ä¸ºäº†è§£å†³ç»å…¸æ¸¸æˆåœºæ™¯çš„æ•°æ®æ³„éœ²é—®é¢˜ï¼ŒTMGBenché‡‡ç”¨äº†åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºæ¯ç§ç»å…¸æ¸¸æˆåˆ›å»ºäº†äº”ä¸ªä¸åŒçš„åŸºäºæ•…äº‹çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯æ¶µç›–äº†å•†ä¸šã€æ³•å¾‹ã€äº¤é€šç­‰ç°å®ç”Ÿæ´»ä¸­çš„ä¸»é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šçµæ´»çš„æ¸¸æˆç»„ç»‡æ–¹å¼<br>TMGBenchå°†æ¸¸æˆè§†ä¸ºåŸå­å•ä½ï¼Œå¹¶é€šè¿‡é¡ºåºã€å¹¶è¡Œå’ŒåµŒå¥—ç»“æ„å°†å®ƒä»¬ç»„ç»‡æˆæ›´å¤æ‚çš„æ¸¸æˆå½¢å¼ï¼Œä»¥è¯„ä¼°LLMsåœ¨å¹¶è¡Œã€é¡ºåºå’Œå¤šå±‚çº§å†³ç­–æ–¹é¢çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹ä¸»æµLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ç†æ€§æ¨ç†ã€æ¨ç†é²æ£’æ€§ã€å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰èƒ½åŠ›å’Œå¤æ‚æ¸¸æˆå½¢å¼çš„æ¨ç†ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æˆ˜ç•¥æ¨ç†è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä»å­˜åœ¨ç¼ºé™·ï¼Œä¸”å¯¹ToMçš„æŒæ¡ç¨‹åº¦ä¹Ÿå„ä¸ç›¸åŒã€‚OpenAIçš„æœ€æ–°æ¨ç†æ¨¡å‹o1-miniåœ¨é¡ºåºã€å¹¶è¡Œå’ŒåµŒå¥—æ¸¸æˆä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º66.6%ã€60.0%å’Œ70.0%ï¼Œçªæ˜¾äº†TMGBenchçš„æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>TMGBenchä¸ºè¯„ä¼°LLMsçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬å…¨é¢çš„æ¸¸æˆç±»å‹è¦†ç›–ç‡ã€æ–°é¢–çš„åœºæ™¯å’Œçµæ´»çš„æ¸¸æˆç»„ç»‡æ–¹å¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†LLMsåœ¨æˆ˜ç•¥æ¨ç†æ–¹é¢çš„ç¼ºé™·ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ–¹å‘ï¼Œä¸ºLLMsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>136</th>
      <td>Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level Policies in Atari Games</td>
      <td>Recent advancements in large language models (LLMs) have expanded their<br>capabilities beyond traditional text-based tasks to multimodal domains,<br>integrating visual, auditory, and textual data. While multimodal LLMs have been<br>extensively explored for high-level planning in domains like robotics and<br>games, their potential as low-level controllers remains largely untapped. In<br>this paper, we introduce a novel benchmark aimed at testing the emergent<br>capabilities of multimodal LLMs as low-level policies in Atari games. Unlike<br>traditional reinforcement learning (RL) methods that require training for each<br>new environment and reward function specification, these LLMs utilize<br>pre-existing multimodal knowledge to directly engage with game environments.<br>Our study assesses the performances of multiple multimodal LLMs against<br>traditional RL agents, human players, and random agents, focusing on their<br>ability to understand and interact with complex visual scenes and formulate<br>strategic responses. Our results show that these multimodal LLMs are not yet<br>capable of being zero-shot low-level policies. Furthermore, we see that this<br>is, in part, due to their visual and spatial reasoning. Additional results and<br>videos are available on our project webpage:<br>https://dev1nw.github.io/atari-gpt/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Atari-GPTï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„çªç ´ï¼Œå®ƒä»¬çš„èƒ½åŠ›å·²ç»æ‰©å±•åˆ°äº†å¤šæ¨¡æ€é¢†åŸŸï¼Œæ•´åˆäº†è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®ã€‚å°½ç®¡å¤šæ¨¡æ€LLMsåœ¨æœºå™¨äººæŠ€æœ¯å’Œæ¸¸æˆç­‰é¢†åŸŸçš„å†³ç­–è§„åˆ’æ–¹é¢å¾—åˆ°äº†å¹¿æ³›æ¢ç´¢ï¼Œä½†å®ƒä»¬ä½œä¸ºä½çº§æ§åˆ¶å™¨çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æŒ–æ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæµ‹è¯•å¤šæ¨¡æ€LLMsåœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºAtari-GPTåŸºå‡†<br>æœ¬æ–‡æå‡ºäº†Atari-GPTåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€LLMsåœ¨Atariæ¸¸æˆä¸­çš„ä½çº§ç­–ç•¥èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡æ¯”è¾ƒLLMsä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ã€äººç±»ç©å®¶å’Œéšæœºä»£ç†çš„æ€§èƒ½ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç†è§£å¤æ‚è§†è§‰åœºæ™¯å’Œåˆ¶å®šæˆ˜ç•¥ååº”æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°è§†è§‰å’Œç©ºé—´æ¨ç†èƒ½åŠ›<br>é™¤äº†æ¸¸æˆæ€§èƒ½è¯„ä¼°ï¼Œæœ¬æ–‡è¿˜é€šè¿‡ä¸€ç³»åˆ—æç¤ºæµ‹è¯•äº†LLMsçš„è§†è§‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œæˆ˜ç•¥ç›´è§‰èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯•æ—¨åœ¨æ­ç¤ºLLMsåœ¨ç†è§£æ¸¸æˆç¯å¢ƒæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ”¹è¿›æä¾›æ–¹å‘ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨Atariæ¸¸æˆä¸­çš„è¡¨ç°ä¸å¦‚äººç±»ç©å®¶æˆ–ä¸“é—¨çš„RLæ¨¡å‹ï¼Œä½†å®ƒä»¬ä»ç„¶èƒ½å¤Ÿè¯†åˆ«å’Œä¸æ¸¸æˆå¸§ä¸­çš„å…³é”®å…ƒç´ è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½œä¸ºä½çº§æ§åˆ¶å™¨çš„æ€§èƒ½ä»ç„¶ä¸ä½³ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºç¼ºä¹é’ˆå¯¹æ­¤ä»»åŠ¡çš„è®­ç»ƒä»¥åŠç©ºé—´æ¨ç†çš„å›°éš¾ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼ŒLLMsçš„æ¨ç†æ—¶é—´å¯¹äºå®æ—¶å†³ç­–è‡³å…³é‡è¦ï¼Œè€Œç›®å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨é€Ÿåº¦æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºLLMsåœ¨ä½çº§æ§åˆ¶ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†å®è´µçš„è§è§£ã€‚å°½ç®¡LLMså°šæœªè¾¾åˆ°ä¸äººç±»æˆ–RLæ¨¡å‹ç›¸åŒ¹é…çš„æ°´å¹³ï¼Œä½†å®ƒä»¬åœ¨Atariæ¸¸æˆä¸­çš„è¡¨ç°ä»ç„¶å€¼å¾—æ³¨æ„ã€‚è¿™è¡¨æ˜LLMså…·æœ‰é€‚åº”æ€§å’Œæ½œåŠ›ï¼Œå¯ä»¥æ‰©å±•åˆ°å…¶åŸå§‹è®­ç»ƒèŒƒå›´ä¹‹å¤–ï¼Œä¸ºæœªæ¥åœ¨éœ€è¦è¯¦ç»†ç¯å¢ƒäº¤äº’å’Œå†³ç­–çš„åº”ç”¨ä¸­ä½œä¸ºæ›´é€šç”¨çš„ä½çº§æ§åˆ¶å™¨æä¾›äº†ä¸€ç¥ã€‚</td>
    </tr>
    <tr>
      <th>137</th>
      <td>Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</td>
      <td>The advancement of Offline Reinforcement Learning (RL) and Offline<br>Multi-Agent Reinforcement Learning (MARL) critically depends on the<br>availability of high-quality, pre-collected offline datasets that represent<br>real-world complexities and practical applications. However, existing datasets<br>often fall short in their simplicity and lack of realism. To address this gap,<br>we propose Hokoff, a comprehensive set of pre-collected datasets that covers<br>both offline RL and offline MARL, accompanied by a robust framework, to<br>facilitate further research. This data is derived from Honor of Kings, a<br>recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate<br>nature, closely resembling real-life situations. Utilizing this framework, we<br>benchmark a variety of offline RL and offline MARL algorithms. We also<br>introduce a novel baseline algorithm tailored for the inherent hierarchical<br>action space of the game. We reveal the incompetency of current offline RL<br>approaches in handling task complexity, generalization and multi-task learning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Hokoffï¼šåŸºäºç‹è€…è£è€€çš„çœŸå®æ¸¸æˆæ•°æ®é›†åŠå…¶ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰å’Œç¦»çº¿å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆOffline MARLï¼‰çš„ç ”ç©¶è¿›å±•ä¾èµ–äºé«˜è´¨é‡ã€é¢„å…ˆæ”¶é›†çš„ç¦»çº¿æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åº”ä»£è¡¨ç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå®é™…åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†å¾€å¾€è¿‡äºç®€å•ï¼Œç¼ºä¹ç°å®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Hokoffï¼Œè¿™æ˜¯ä¸€å¥—å…¨é¢çš„é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†ç¦»çº¿RLå’Œç¦»çº¿MARLï¼Œå¹¶ä¼´éšç€ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚è¿™äº›æ•°æ®æ¥è‡ªç‹è€…è£è€€ï¼Œè¿™æ˜¯ä¸€æ¬¾ä»¥å…¶å¤æ‚æ€§è´¨è€Œé—»åçš„å¤šäººåœ¨çº¿æˆ˜æ–—ç«æŠ€åœºï¼ˆMOBAï¼‰æ¸¸æˆï¼Œä¸ç°å®ç”Ÿæ´»æƒ…å†µéå¸¸ç›¸ä¼¼ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç‹è€…è£è€€çš„å¤æ‚ç¯å¢ƒ<br>Hokoffçš„æ•°æ®é›†æ¥æºäºç‹è€…è£è€€ï¼Œè¿™æ˜¯ä¸€æ¬¾æ‹¥æœ‰è¶…è¿‡1äº¿æ—¥æ´»è·ƒç©å®¶çš„å…¨çƒæœ€å—æ¬¢è¿çš„MOBAæ¸¸æˆä¹‹ä¸€ã€‚è¯¥æ¸¸æˆçš„å¤æ‚æ€§è¿œè¿œè¶…è¿‡å…¶ä»–æ•°æ®é›†ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯çš„æ½œåŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æºã€æ˜“ç”¨çš„æ¡†æ¶<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€æºã€æ˜“ç”¨çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ç¦»çº¿RLï¼ˆé‡‡æ ·ã€è®­ç»ƒå’Œè¯„ä¼°ï¼‰çš„å…¨é¢æµç¨‹å’Œä¸€äº›æœ‰ç”¨çš„å·¥å…·ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸°å¯Œå¤šæ ·çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ä½¿ç”¨ä¸€ç³»åˆ—å…·æœ‰ä¸åŒè®¾è®¡å› ç´ çš„å‰è®­ç»ƒæ¨¡å‹ç”Ÿæˆï¼Œä¸ä»…é€‚ç”¨äºç¦»çº¿RLï¼Œä¹Ÿé€‚ç”¨äºç¦»çº¿MARLã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šçº§æ¨¡å‹<br>ä¸ºäº†ç¡®ä¿ä¸åŒç®—æ³•çš„æ€§èƒ½æ¯”è¾ƒçš„æœ‰æ•ˆæ€§å’Œå…¬æ­£æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å¤šçº§æ¨¡å‹ï¼ŒåŒ…å«å¤šä¸ªå…·æœ‰ä¸åŒæ°´å¹³çš„æ£€æŸ¥ç‚¹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºé‡‡æ ·å’Œè¯„ä¼°ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ ·åŒ–çš„æ•°æ®é›†<br>Hokoffæä¾›äº†å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¤šéš¾åº¦ã€å¤šä»»åŠ¡ã€æ³›åŒ–ã€å¼‚æ„é˜Ÿå‹å’Œå­ä»»åŠ¡ç­‰ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä¸ºç¦»çº¿RLå’Œç¦»çº¿MARLçš„ç ”ç©¶æä¾›æ›´çœŸå®çš„ç¯å¢ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Hokoffæ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šç§ç¦»çº¿RLå’Œç¦»çº¿MARLç®—æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ç‹è€…è£è€€å›ºæœ‰å±‚æ¬¡ç»“æ„åŠ¨ä½œç©ºé—´çš„æ–°çš„åŸºçº¿ç®—æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ç¦»çº¿RLæ–¹æ³•åœ¨å¤„ç†ä»»åŠ¡å¤æ‚æ€§ã€æ³›åŒ–å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Hokoffæ•°æ®é›†å’Œæ¡†æ¶ä¸ºç¦»çº¿RLå’Œç¦»çº¿MARLçš„ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚å…¶å¤šæ ·åŒ–çš„æ•°æ®é›†å’Œå¼ºå¤§çš„æ¡†æ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè¯„ä¼°ç¦»çº¿å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¸ºè§£å†³ç°å®ä¸–ç•Œé—®é¢˜æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>138</th>
      <td>A Benchmark Environment for Offline Reinforcement Learning in Racing Games</td>
      <td>Offline Reinforcement Learning (ORL) is a promising approach to reduce the<br>high sample complexity of traditional Reinforcement Learning (RL) by<br>eliminating the need for continuous environmental interactions. ORL exploits a<br>dataset of pre-collected transitions and thus expands the range of application<br>of RL to tasks in which the excessive environment queries increase training<br>time and decrease efficiency, such as in modern AAA games. This paper<br>introduces OfflineMania a novel environment for ORL research. It is inspired by<br>the iconic TrackMania series and developed using the Unity 3D game engine. The<br>environment simulates a single-agent racing game in which the objective is to<br>complete the track through optimal navigation. We provide a variety of datasets<br>to assess ORL performance. These datasets, created from policies of varying<br>ability and in different sizes, aim to offer a challenging testbed for<br>algorithm development and evaluation. We further establish a set of baselines<br>for a range of Online RL, ORL, and hybrid Offline to Online RL approaches using<br>our environment.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨èµ›è½¦æ¸¸æˆä¸­çš„æ–°åŸºå‡†ç¯å¢ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¸¸æˆè¡Œä¸šä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ä¼ ç»ŸRLçš„é«˜æ ·æœ¬å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå¦‚ç°ä»£AAAæ¸¸æˆã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆORLï¼‰é€šè¿‡åˆ©ç”¨é¢„å…ˆæ”¶é›†çš„è¿‡æ¸¡æ•°æ®é›†æ¥å‡å°‘å¯¹ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ï¼Œä»è€Œé™ä½äº†æ ·æœ¬å¤æ‚åº¦ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹æ¸¸æˆç¯å¢ƒï¼Œç‰¹åˆ«æ˜¯èµ›è½¦æ¸¸æˆï¼Œçš„ORLæ•°æ®é›†å’ŒåŸºå‡†ç¯å¢ƒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥OfflineManiaç¯å¢ƒ<br>OfflineManiaæ˜¯ä¸€ä¸ªåŸºäºUnity 3Dæ¸¸æˆå¼•æ“å¼€å‘çš„å•ä»£ç†èµ›è½¦æ¸¸æˆç¯å¢ƒï¼Œçµæ„Ÿæ¥æºäºTrackManiaç³»åˆ—æ¸¸æˆã€‚è¯¥ç¯å¢ƒæ¨¡æ‹Ÿäº†èµ›è½¦æ¸¸æˆï¼Œç›®æ ‡æ˜¯é€šè¿‡æœ€ä¼˜å¯¼èˆªå®Œæˆèµ›é“ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæä¾›å¤šæ ·åŒ–çš„æ•°æ®é›†<br>ä¸ºäº†è¯„ä¼°ORLçš„æ€§èƒ½ï¼Œè®ºæ–‡æä¾›äº†å¤šç§æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ç”±ä¸åŒèƒ½åŠ›æ°´å¹³çš„ç­–ç•¥ç”Ÿæˆï¼Œå¹¶å…·æœ‰ä¸åŒçš„è§„æ¨¡ã€‚è¿™äº›æ•°æ®é›†æ—¨åœ¨ä¸ºç®—æ³•å¼€å‘å’Œè¯„ä¼°æä¾›ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå»ºç«‹åŸºçº¿<br>è®ºæ–‡ä½¿ç”¨OfflineManiaç¯å¢ƒï¼Œä¸ºä¸€ç³»åˆ—åœ¨çº¿RLã€ORLå’Œæ··åˆç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•å»ºç«‹äº†åŸºçº¿ã€‚è¿™äº›åŸºçº¿æœ‰åŠ©äºè¯„ä¼°ä¸åŒç®—æ³•çš„æ€§èƒ½ï¼Œå¹¶ä¿ƒè¿›ORLåœ¨æ¸¸æˆç¯å¢ƒä¸­çš„åº”ç”¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒIQLåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºTD3BCå’ŒCQLã€‚æ­¤å¤–ï¼ŒSDBGå’ŒJSRLåœ¨ç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜ç¦»çº¿è®­ç»ƒçš„æ”¿ç­–çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OfflineManiaç¯å¢ƒå’Œæä¾›çš„æ•°æ®é›†ä¸ºORLç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è®ºæ–‡ä¸­çš„åŸºçº¿ç»“æœä¸ºè¯„ä¼°ä¸åŒORLç®—æ³•çš„æ€§èƒ½æä¾›äº†å‚è€ƒã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†ç¦»çº¿åˆ°åœ¨çº¿RLæ–¹æ³•åœ¨æ¸¸æˆç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œä¸ºæ¸¸æˆå¼€å‘äººå‘˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach</td>
      <td>StarCraft II is a challenging benchmark for AI agents due to the necessity of<br>both precise micro level operations and strategic macro awareness. Previous<br>works, such as Alphastar and SCC, achieve impressive performance on tackling<br>StarCraft II , however, still exhibit deficiencies in long term strategic<br>planning and strategy interpretability. Emerging large language model (LLM)<br>agents, such as Voyage and MetaGPT, presents the immense potential in solving<br>intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs<br>on StarCraft II, a highly complex RTS game.To conveniently take full advantage<br>of LLMs` reasoning abilities, we first develop textual StratCraft II<br>environment, called TextStarCraft II, which LLM agent can interact. Secondly,<br>we propose a Chain of Summarization method, including single frame<br>summarization for processing raw observations and multi frame summarization for<br>analyzing game information, providing command recommendations, and generating<br>strategic decisions. Our experiment consists of two parts: first, an evaluation<br>by human experts, which includes assessing the LLMs`s mastery of StarCraft II<br>knowledge and the performance of LLM agents in the game; second, the in game<br>performance of LLM agents, encompassing aspects like win rate and the impact of<br>Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the<br>relevant knowledge and complex planning abilities needed to address StarCraft<br>II scenarios; 2. Human experts consider the performance of LLM agents to be<br>close to that of an average player who has played StarCraft II for eight years;<br>3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)<br>difficulty level. We have open sourced the code and released demo videos of LLM<br>agent playing StarCraft II.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­çš„è¡¨ç°ï¼šåŸºå‡†æµ‹è¯•ä¸æ‘˜è¦é“¾æ–¹æ³•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ˜Ÿé™…äº‰éœ¸IIï¼ˆStarCraft IIï¼‰æ˜¯ä¸€æ¬¾æå…·æŒ‘æˆ˜æ€§çš„å®æ—¶æˆ˜ç•¥æ¸¸æˆï¼Œè¦æ±‚ç©å®¶åœ¨å¾®è§‚æ“ä½œå’Œå®è§‚æˆ˜ç•¥è§„åˆ’ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å°½ç®¡ä¹‹å‰çš„AIç ”ç©¶ï¼Œå¦‚AlphaStarå’ŒSCCï¼Œåœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œä½†å®ƒä»¬åœ¨é•¿æœŸæˆ˜ç•¥è§„åˆ’å’Œç­–ç•¥å¯è§£é‡Šæ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œæœ¬æ–‡æ—¨åœ¨éªŒè¯LLMåœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTextStarCraft IIç¯å¢ƒ<br>ä¸ºäº†å……åˆ†åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªåä¸ºTextStarCraft IIçš„æ–‡æœ¬ç¯å¢ƒï¼ŒLLMä»£ç†å¯ä»¥ä¸ä¹‹äº¤äº’ã€‚è¯¥ç¯å¢ƒå°†æ˜Ÿé™…äº‰éœ¸IIçš„å¤æ‚æ¸¸æˆåŠ¨æ€è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå…è®¸LLMä»£ç†é€šè¿‡è¯­è¨€å‘½ä»¤æ‰§è¡Œå®è§‚æˆ˜ç•¥è¡ŒåŠ¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‘˜è¦é“¾ï¼ˆCoSï¼‰æ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†æ‘˜è¦é“¾ï¼ˆCoSï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å•å¸§æ‘˜è¦å’Œå¤šå¸§æ‘˜è¦ã€‚å•å¸§æ‘˜è¦ç”¨äºå¤„ç†åŸå§‹è§‚å¯Ÿæ•°æ®ï¼Œè€Œå¤šå¸§æ‘˜è¦ç”¨äºåˆ†ææ¸¸æˆä¿¡æ¯ï¼Œæä¾›å‘½ä»¤å»ºè®®å¹¶ç”Ÿæˆæˆ˜ç•¥å†³ç­–ã€‚CoSæ–¹æ³•é€šè¿‡ä¿¡æ¯å‹ç¼©ã€æ¨ç†åŠ é€Ÿå’Œå…¨å±€ç†è§£ï¼Œå¢å¼ºäº†LLMä»£ç†åœ¨å¤„ç†å¤æ‚ä¿¡æ¯å’Œåšå‡ºæˆ˜ç•¥å†³ç­–æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMå…·å¤‡è§£å†³æ˜Ÿé™…äº‰éœ¸IIåœºæ™¯æ‰€éœ€çš„ç›¸å…³çŸ¥è¯†å’Œå¤æ‚è§„åˆ’èƒ½åŠ›ã€‚äººç±»ä¸“å®¶è®¤ä¸ºï¼ŒLLMä»£ç†åœ¨æ¸¸æˆä¸­çš„è¡¨ç°æ¥è¿‘äºç©äº†å…«å¹´æ˜Ÿé™…äº‰éœ¸IIçš„å¹³å‡ç©å®¶ã€‚æ­¤å¤–ï¼ŒLLMä»£ç†èƒ½å¤Ÿåœ¨Harderï¼ˆLv5ï¼‰éš¾åº¦çº§åˆ«ä¸‹å‡»è´¥å†…ç½®AIã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„TextStarCraft IIç¯å¢ƒå’ŒCoSæ–¹æ³•ä¸ºè¯„ä¼°LLMåœ¨å®æ—¶æˆ˜ç•¥å†³ç­–å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥åœ¨æ˜Ÿé™…äº‰éœ¸IIå’Œå…¶ä»–å®æ—¶æˆ˜ç•¥æ¸¸æˆä¸­çš„AIç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>272</th>
      <td>The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI</td>
      <td>We introduce a visually-guided and physics-driven task-and-motion planning<br>benchmark, which we call the ThreeDWorld Transport Challenge. In this<br>challenge, an embodied agent equipped with two 9-DOF articulated arms is<br>spawned randomly in a simulated physical home environment. The agent is<br>required to find a small set of objects scattered around the house, pick them<br>up, and transport them to a desired final location. We also position containers<br>around the house that can be used as tools to assist with transporting objects<br>efficiently. To complete the task, an embodied agent must plan a sequence of<br>actions to change the state of a large number of objects in the face of<br>realistic physical constraints. We build this benchmark challenge using the<br>ThreeDWorld simulation: a virtual 3D environment where all objects respond to<br>physics, and where can be controlled using fully physics-driven navigation and<br>interaction API. We evaluate several existing agents on this benchmark.<br>Experimental results suggest that: 1) a pure RL model struggles on this<br>challenge; 2) hierarchical planning-based agents can transport some objects but<br>still far from solving this task. We anticipate that this benchmark will<br>empower researchers to develop more intelligent physics-driven robots for the<br>physical world.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | 3DWorld Transport Challengeï¼šç‰©ç†ä¸–ç•Œä¸­çš„æ™ºèƒ½æœºå™¨äººæŒ‘æˆ˜<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„æœºå™¨äººæˆä¸ºäº†è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç¤¾åŒºçš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨çœŸå®æœºå™¨äººè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°æˆæœ¬é«˜æ˜‚ä¸”å­˜åœ¨å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œè¿‘å¹´æ¥ï¼Œäººä»¬å¼€å§‹å°†æ¨¡æ‹Ÿå™¨çº³å…¥è®­ç»ƒå’Œè¯„ä¼°äººå·¥æ™ºèƒ½ç®—æ³•çš„è¿‡ç¨‹ä¸­ã€‚å°½ç®¡3Dè™šæ‹Ÿç¯å¢ƒåœ¨è§†è§‰å¯¼èˆªæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¤§å¤šå…³æ³¨è§†è§‰å¯¼èˆªï¼Œè€Œå¿½ç•¥äº†ç‰©ç†äº¤äº’ã€‚ç”±äºæœ€ç»ˆç›®æ ‡æ˜¯å¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ç¯å¢ƒä¸­æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„ç³»ç»Ÿï¼Œå› æ­¤ç‰©ç†äº¤äº’å·²æˆä¸ºå®¶åº­åŠ©ç†æœºå™¨äººè®­ç»ƒçš„å¿…è¦ç»„æˆéƒ¨åˆ†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å…·èº«AIæŒ‘æˆ˜ï¼šä¸€ä¸ªå…·æœ‰ä¸¤ä¸ª9è‡ªç”±åº¦å…³èŠ‚è‡‚çš„å…·èº«æ™ºèƒ½ä½“è¢«éšæœºæ”¾ç½®åœ¨ä¸€ä¸ªç‰©ç†çœŸå®çš„è™šæ‹Ÿå®¶åº­ç¯å¢ƒä¸­ã€‚æ™ºèƒ½ä½“éœ€è¦æ¢ç´¢æˆ¿å±‹ï¼Œå¯»æ‰¾æ•£è½åœ¨ä¸åŒæˆ¿é—´ä¸­çš„å°‘é‡ç‰©ä½“ï¼Œå¹¶å°†å®ƒä»¬è¿é€åˆ°ä¸€ä¸ªæœŸæœ›çš„æœ€ç»ˆä½ç½®ã€‚æ­¤å¤–ï¼Œæˆ¿å±‹å‘¨å›´è¿˜æ”¾ç½®äº†å„ç§å®¹å™¨ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ‰¾åˆ°è¿™äº›å®¹å™¨å¹¶å°†ç‰©ä½“æ”¾å…¥å…¶ä¸­ã€‚ä¸ä½¿ç”¨å®¹å™¨ä½œä¸ºå·¥å…·æ—¶ï¼Œæ™ºèƒ½ä½“åªèƒ½ä¸€æ¬¡è¿è¾“ä¸¤ä¸ªç‰©ä½“ã€‚ç„¶è€Œï¼Œä½¿ç”¨å®¹å™¨ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ”¶é›†å¤šä¸ªç‰©ä½“å¹¶ä¸€èµ·è¿è¾“ã€‚<br><br>ä¸ºäº†æ”¯æŒè¿™é¡¹æŒ‘æˆ˜ï¼Œæœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªåŸºäºTDWçš„æˆ¿å±‹æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å……æ»¡ç‰©ç†å“åº”ç‰©ä½“çš„å¤šæˆ¿é—´ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªå®Œå…¨åŸºäºç‰©ç†çš„é«˜çº§å¯¼èˆªå’Œäº¤äº’APIï¼Œå¯ä»¥ç”¨äºè®­ç»ƒAIæ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç‰©ç†ä¸–ç•Œä¸­ä¸è™šæ‹Ÿä¸–ç•Œè¿›è¡Œç‰©ç†äº¤äº’ã€‚ç”±äºæ¨¡æ‹ŸåŠ¨ä½œå’Œç¯å¢ƒå®Œå…¨åŸºäºç‰©ç†ï¼Œä¸ä¹‹å‰çš„éç‰©ç†æˆ–éƒ¨åˆ†ç‰©ç†è™šæ‹Ÿç¯å¢ƒç›¸æ¯”ï¼Œå®ƒä»¬æå‡ºäº†é¢å¤–çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œäº¤äº’åŠ¨ä½œåªæœ‰åœ¨ç›®æ ‡ç‰©ç†å¯è¾¾æ—¶ï¼ˆå³é è¿‘ä¸”æœªè¢«é˜»æŒ¡ï¼‰æ‰ä¼šæˆåŠŸã€‚å¦‚æœç›®æ ‡ä¸åœ¨æ™ºèƒ½ä½“çš„ä¸­å¿ƒè§†å›¾ä¸­ï¼Œæˆ–è€…ç›´æ¥è·¯å¾„è¢«é˜»æŒ¡ï¼ˆä¾‹å¦‚ï¼Œè¢«æ¡Œå­é˜»æŒ¡ï¼‰ï¼Œæ™ºèƒ½ä½“å°±æ— æ³•æˆåŠŸæŠ“å–ç‰©ä½“ã€‚æ­¤å¤–ï¼Œä¸æˆ¿å±‹ä¸­çš„ç‰©ä½“å‘ç”Ÿç‰©ç†ç¢°æ’ä¹Ÿå¯èƒ½æ˜¾è‘—é˜»ç¢è¿è¾“è¿›åº¦ã€‚å› æ­¤ï¼Œæ™ºèƒ½ä½“å¿…é¡»å­¦ä¹ åˆ©ç”¨è§†è§‰ä¿¡å·æ¥åŒæ­¥å¯¼èˆªå’Œæ“ä½œï¼Œä»¥åº”å¯¹è¿™äº›ç‰©ç†çº¦æŸã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡è¯„ä¼°äº†å‡ ä¸ªç°æœ‰çš„æ™ºèƒ½ä½“ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å…·èº«æ™ºèƒ½ä½“åœ¨å®Œæˆè¿™é¡¹ä»»åŠ¡æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡ç›¸ä¿¡ï¼Œåœ¨è¿è¾“æŒ‘æˆ˜ä¸­è¡¨ç°è‰¯å¥½çš„æ¨¡å‹å°†èƒ½å¤Ÿä½¿æœºå™¨äººæ›´åŠ æ™ºèƒ½ï¼Œèƒ½å¤Ÿåœ¨çœŸå®çš„ç‰©ç†ä¸–ç•Œä¸­å‘æŒ¥ä½œç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„3DWorld Transport Challengeä¸ºå…·èº«æ™ºèƒ½ä½“åœ¨ç‰©ç†çœŸå®ç¯å¢ƒä¸­çš„ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€å‘äº†ä¸€ä¸ªå®Œå…¨åŸºäºç‰©ç†çš„é«˜çº§å¯¼èˆªå’Œäº¤äº’APIï¼Œå¯ä»¥ç”¨äºè®­ç»ƒAIæ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç‰©ç†ä¸–ç•Œä¸­ä¸è™šæ‹Ÿä¸–ç•Œè¿›è¡Œç‰©ç†äº¤äº’ã€‚è¿™äº›æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„æ™ºèƒ½æœºå™¨äººæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>139</th>
      <td>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</td>
      <td>We introduce a novel and extensible benchmark for large language models<br>(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.<br>The open-source game simulation code, available on GitHub, allows LLMs to<br>compete and generates detailed data files in JSON, CSV, TXT, and PNG formats<br>for leaderboard rankings and further analysis. We present the results of games<br>among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by<br>Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and<br>GPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of<br>results from other LLMs. In total, we simulated 2,310 matches (5 sessions for<br>each pair among 7 LLMs and a random player) across three types of games, using<br>three distinct prompt types: list, illustration, and image. The results<br>revealed significant variations in LLM performance across different games and<br>prompt types, with analysis covering win and disqualification rates, missed<br>opportunity analysis, and invalid move analysis. The details of the leaderboard<br>and result matrix data are available as open-access data on GitHub. This study<br>enhances our understanding of LLMs' capabilities in playing games they were not<br>specifically trained for, helping to assess their rule comprehension and<br>strategic thinking. On the path to Artificial General Intelligence (AGI), this<br>study lays the groundwork for future exploration into their utility in complex<br>decision-making scenarios, illuminating their strategic thinking abilities and<br>offering directions for further inquiry into the limits of LLMs within<br>game-based frameworks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡åŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼šä¸€ä¸ªå¯æ‰©å±•çš„LLMåŸºå‡†å’Œæ’è¡Œæ¦œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å’Œæ€§èƒ½å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ç­‰ï¼Œå·²ç»æ— æ³•å…¨é¢è¯„ä¼°LLMsçš„å¤æ‚èƒ½åŠ›ã€‚å› æ­¤ï¼Œéœ€è¦æ–°çš„åŸºå‡†æ¥è¯„ä¼°LLMsåœ¨è§„åˆ™ç†è§£ã€æˆ˜ç•¥æ€è€ƒå’Œå†³ç­–åˆ¶å®šç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å¯æ‰©å±•çš„åŸºå‡†ï¼Œé€šè¿‡ç½‘æ ¼æ¸¸æˆï¼ˆå¦‚äº•å­—æ£‹ã€å››å­æ£‹å’Œäº”å­æ£‹ï¼‰æ¥è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚è¿™äº›æ¸¸æˆéœ€è¦æ¨¡å‹ç†è§£è§„åˆ™ã€åˆ¶å®šç­–ç•¥å¹¶åšå‡ºå†³ç­–ï¼Œä»è€Œå…¨é¢è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€æ”¾æºä»£ç å’Œæ’è¡Œæ¦œ<br>æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå¼€æºçš„æ¸¸æˆæ¨¡æ‹Ÿä»£ç ï¼Œå…è®¸LLMsè¿›è¡Œç«èµ›ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„JSONã€CSVã€TXTå’ŒPNGæ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼Œç”¨äºæ’è¡Œæ¦œæ’åå’Œè¿›ä¸€æ­¥åˆ†æã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªæ’è¡Œæ¦œï¼Œå±•ç¤ºäº†ä¸åŒLLMsåœ¨ä¸åŒæ¸¸æˆå’Œæç¤ºç±»å‹ä¸‹çš„è¡¨ç°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹ä¸ƒä¸ªé¢†å…ˆçš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬Anthropicçš„Claude 3.5 Sonnetå’ŒClaude 3 Sonnetã€Googleçš„Gemini 1.5 Proå’ŒGemini 1.5 Flashã€OpenAIçš„GPT-4 Turboå’ŒGPT-4oä»¥åŠMetaçš„Llama3-70Bã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ä¸åŒæ¸¸æˆå’Œæç¤ºç±»å‹ä¸‹çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¾‹å¦‚ï¼ŒClaude 3.5 Sonnetåœ¨äº•å­—æ£‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨äº”å­æ£‹ä¸­è¡¨ç°è¾ƒå·®ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨å¤„ç†å¤æ‚å’ŒåŸºäºè§†è§‰çš„æç¤ºæ ¼å¼æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºç½‘æ ¼çš„æ¸¸æˆç«èµ›åŸºå‡†ä¸ºè¯„ä¼°LLMsçš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç å’Œæ’è¡Œæ¦œï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†æå’Œæ¯”è¾ƒã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºç†è§£LLMsçš„èƒ½åŠ›å’Œå±€é™æ€§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>144</th>
      <td>MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields</td>
      <td>Previous research has demonstrated the advantages of integrating data from<br>multiple sources over traditional unimodal data, leading to the emergence of<br>numerous novel multimodal applications. We propose a multimodal classification<br>benchmark MuG with eight datasets that allows researchers to evaluate and<br>improve their models. These datasets are collected from four various genres of<br>games that cover tabular, textual, and visual modalities. We conduct<br>multi-aspect data analysis to provide insights into the benchmark, including<br>label balance ratios, percentages of missing features, distributions of data<br>within each modality, and the correlations between labels and input modalities.<br>We further present experimental results obtained by several state-of-the-art<br>unimodal classifiers and multimodal classifiers, which demonstrate the<br>challenging and multimodal-dependent properties of the benchmark. MuG is<br>released at https://github.com/lujiaying/MUG-Bench with the data, tutorials,<br>and implemented baselines.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MuGï¼šåŸºäºæ¸¸æˆæ•°æ®çš„å¤šå…ƒåˆ†ç±»åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ å·²ç»æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚å¤šæ¨¡æ€å­¦ä¹ èƒ½å¤Ÿæ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º MuG çš„å¤šæ¨¡æ€åˆ†ç±»åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>MuG åŒ…å«äº†æ¥è‡ªå››ä¸ªä¸åŒæ¸¸æˆç±»å‹çš„å…«ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–äº†è¡¨æ ¼ã€æ–‡æœ¬å’Œè§†è§‰ä¸‰ç§æ¨¡æ€ã€‚è¿™äº›æ•°æ®é›†ç»è¿‡å¿…è¦çš„æ¸…æ´—ã€è½¬æ¢å’Œä¿®æ”¹ï¼Œä»¥ä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ MuG çš„ç‰¹æ€§ï¼Œæœ¬æ–‡è¿›è¡Œäº†å¤šæ–¹é¢çš„æ•°æ®åˆ†æï¼ŒåŒ…æ‹¬æ ‡ç­¾å¹³è¡¡æ¯”ä¾‹ã€ç¼ºå¤±ç‰¹å¾ç™¾åˆ†æ¯”ã€æ¯ç§æ¨¡æ€ä¸­æ•°æ®çš„åˆ†å¸ƒä»¥åŠæ ‡ç­¾ä¸è¾“å…¥æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨å¤šä¸ªæœ€å…ˆè¿›çš„å•æ¨¡æ€åˆ†ç±»å™¨å’Œå¤šæ¨¡æ€åˆ†ç±»å™¨åœ¨ MuG ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€åˆ†ç±»å™¨åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½ä¼˜äºå•æ¨¡æ€åˆ†ç±»å™¨ï¼Œè¿™è¡¨æ˜ MuG ä¸­çš„åˆ†ç±»ä»»åŠ¡ç¡®å®ä¾èµ–äºå¤šæ¨¡æ€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å¤šæ¨¡æ€åˆ†ç±»å™¨ MUGNETï¼Œå®ƒåœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰çš„å¤šæ¨¡æ€åˆ†ç±»å™¨ç›¸å½“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MuG åŸºå‡†æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œæ”¹è¿›å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹çš„é‡è¦å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ MUGNET æ¨¡å‹ä¹Ÿä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æœªæ¥ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨ MuG åŸºå‡†æ•°æ®é›†è¿›è¡Œæ›´å¤šå…³äºå¤šæ¨¡æ€å­¦ä¹ çš„ç ”ç©¶ï¼Œå¹¶æ¢ç´¢æ–°çš„åº”ç”¨åœºæ™¯ã€‚</td>
    </tr>
    <tr>
      <th>38</th>
      <td>GlitchBench: Can large multimodal models detect video game glitches?</td>
      <td>Large multimodal models (LMMs) have evolved from large language models (LLMs)<br>to integrate multiple input modalities, such as visual inputs. This integration<br>augments the capacity of LLMs for tasks requiring visual comprehension and<br>reasoning. However, the extent and limitations of their enhanced abilities are<br>not fully understood, especially when it comes to real-world tasks. To address<br>this gap, we introduce GlitchBench, a novel benchmark derived from video game<br>quality assurance tasks, to test and evaluate the reasoning capabilities of<br>LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios<br>from video games and aims to challenge both the visual and linguistic reasoning<br>powers of LMMs in detecting and interpreting out-of-the-ordinary events. We<br>evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents<br>a new challenge for these models. Code and data are available at:<br>https://glitchbench.github.io/</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GlitchBenchï¼šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹èƒ½å¦æ£€æµ‹è§†é¢‘æ¸¸æˆä¸­çš„é”™è¯¯ï¼Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œå®ƒä»¬åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå±€é™æ€§å°šä¸æ˜ç¡®ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†GlitchBenchï¼Œä¸€ä¸ªåŸºäºè§†é¢‘æ¸¸æˆè´¨é‡ä¿è¯ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LMMsåœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGlitchBenchæ•°æ®é›†<br>GlitchBenchæ•°æ®é›†ç”±593ä¸ªæ¸¸æˆä¸­çš„å¼‚å¸¸å’Œé”™è¯¯åœºæ™¯ç»„æˆï¼Œæ¶µç›–äº†205æ¬¾ä¸åŒç±»å‹çš„æ¸¸æˆã€‚æ¯ä¸ªåœºæ™¯éƒ½åŒ…å«ä¸€ä¸ªè§†é¢‘ç‰‡æ®µã€ä¸€ä¸ªä»£è¡¨æ€§å¸§ã€ä¸€ä¸ªç®€çŸ­çš„æè¿°ä»¥åŠä¸€ä¸ªæŒ‡å‘Redditä¸Šç›¸å…³è®¨è®ºçš„é“¾æ¥ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†è¿˜åŒ…æ‹¬330ä¸ªæ— é”™è¯¯çš„å›¾åƒä½œä¸ºå¯¹æ¯”ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°æ–¹æ³•<br>æœ¬æ–‡è¯„ä¼°äº†11ä¸ªæœ€å…ˆè¿›çš„LMMsï¼ŒåŒ…æ‹¬GPT-4Vå’ŒLLaVAï¼Œåœ¨GlitchBenchä¸Šçš„è¡¨ç°ã€‚è¯„ä¼°æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé—®é¢˜ï¼š<br>1. è¿™å¼ å›¾ç‰‡æœ‰ä»€ä¹ˆä¸å¯»å¸¸çš„åœ°æ–¹ï¼Ÿ<br>2. è¿™å¼ å›¾ç‰‡æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ<br>3. è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ã€‚<br>é€šè¿‡æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸çœŸå®æ ‡ç­¾ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒLMMsåœ¨æ£€æµ‹è¿åç®€å•ç‰©ç†å®šå¾‹çš„é”™è¯¯ï¼ˆå¦‚æ±½è½¦åœ¨ç©ºä¸­é£è¡Œï¼‰æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æ£€æµ‹æ›´å¾®å¦™çš„é”™è¯¯ï¼ˆå¦‚äººä½“éƒ¨ä½å¤„äºä¸å¯èƒ½çš„å§¿åŠ¿ï¼‰æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚GPT-4Våœ¨GlitchBenchä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡è¾¾åˆ°43.4%ã€‚ç„¶è€Œï¼Œä¸æ— é”™è¯¯å›¾åƒç›¸æ¯”ï¼Œæ¨¡å‹åœ¨æ£€æµ‹é”™è¯¯å›¾åƒæ–¹é¢çš„å‡†ç¡®ç‡æ˜æ˜¾è¾ƒä½ï¼Œè¿™è¡¨æ˜é”™è¯¯å›¾åƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GlitchBenchåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°LMMsåœ¨å®é™…åº”ç”¨ä¸­çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLMMsåœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models</td>
      <td>While the situation has improved for text-only models, it again seems to be<br>the case currently that multimodal (text and image) models develop faster than<br>ways to evaluate them. In this paper, we bring a recently developed evaluation<br>paradigm from text models to multimodal models, namely evaluation through the<br>goal-oriented game (self) play, complementing reference-based and<br>preference-based evaluation. Specifically, we define games that challenge a<br>model's capability to represent a situation from visual information and align<br>such representations through dialogue. We find that the largest closed models<br>perform rather well on the games that we define, while even the best<br>open-weight models struggle with them. On further analysis, we find that the<br>exceptional deep captioning capabilities of the largest models drive some of<br>the performance. There is still room to grow for both kinds of models, ensuring<br>the continued relevance of the benchmark.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆåŒ–è¯„ä¼°ï¼šæ¢ç©¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€å’Œå¯¹è¯å¼æ¥åœ°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå‚è€ƒå¼è¯„ä¼°ï¼Œéš¾ä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§æ–°çš„è¯„ä¼°èŒƒå¼ï¼Œå³é€šè¿‡ç›®æ ‡å¯¼å‘çš„æ¸¸æˆï¼ˆè‡ªæˆ‘ï¼‰ç©æ³•æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼Œä»¥è¡¥å……ç°æœ‰çš„å‚è€ƒå¼å’Œåå¥½å¼è¯„ä¼°æ–¹æ³•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ¸¸æˆåŒ–è¯„ä¼°èŒƒå¼åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹<br>æœ¬æ–‡å€Ÿé‰´äº†æ–‡æœ¬æ¨¡å‹ä¸­æ–°å…´çš„æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹ã€‚é€šè¿‡å®šä¹‰ä¸‰ç§å¯¹è¯æ¸¸æˆï¼ˆå‚è€ƒæ¸¸æˆã€å›¾åƒæ¯”è¾ƒæ¸¸æˆå’Œå¯¼èˆªæ¸¸æˆï¼‰ï¼ŒæŒ‘æˆ˜æ¨¡å‹ä»è§†è§‰ä¿¡æ¯ä¸­æ„å»ºæƒ…å¢ƒæ¨¡å‹å¹¶é€šè¿‡å¯¹è¯è¿›è¡Œå¯¹é½çš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå¤šæ¨¡æ€æ¸¸æˆæ¡†æ¶<br>æœ¬æ–‡ä½¿ç”¨ clemgame/clembench æ¡†æ¶æ¥å®ç°æ¸¸æˆåŒ–è¯„ä¼°ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ¨¡æ¿æ¥å®šä¹‰æ¸¸æˆç›®æ ‡ï¼Œå¹¶é€šè¿‡ç¨‹åºåŒ–çš„æ¸¸æˆå¤§å¸ˆæ¥æ§åˆ¶æ¸¸æˆæµç¨‹å’Œè¯„åˆ†è§„åˆ™ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹é—­æºæ¨¡å‹åœ¨æœ¬æ–‡å®šä¹‰çš„æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œå³ä½¿æ˜¯æœ€å¥½çš„å¼€æºæ¨¡å‹ä¹Ÿéš¾ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼Œå¤§å‹æ¨¡å‹åœ¨æ·±åº¦å›¾åƒæè¿°æ–¹é¢çš„å‡ºè‰²èƒ½åŠ›æ¨åŠ¨äº†éƒ¨åˆ†æ€§èƒ½æå‡ã€‚è¿™è¡¨æ˜ï¼Œæ— è®ºæ˜¯é—­æºæ¨¡å‹è¿˜æ˜¯å¼€æºæ¨¡å‹ï¼Œéƒ½ä»æœ‰å¾ˆå¤§çš„å‘å±•ç©ºé—´ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤šæ¨¡æ€æ¸¸æˆæ¡†æ¶ä¹Ÿä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›äº†å¯å¤ç°çš„å®éªŒå¹³å°ã€‚<br><br>## ğŸ“š å‚è€ƒæ–‡çŒ®<br>* Chalamalasetti, V., et al. (2023). clemgame: A framework for evaluating language models through goal-oriented games. arXiv preprint arXiv:2304.01213.<br>* Chiang, D., et al. (2024). Chatbot Arena: A preference-based evaluation platform for conversational agents. arXiv preprint arXiv:2403.04528.<br>* Qiao, Y., et al. (2023). Evaluating language models through interactive games. arXiv preprint arXiv:2303.08574.</td>
    </tr>
    <tr>
      <th>62</th>
      <td>AvalonBench: Evaluating LLMs Playing the Game of Avalon</td>
      <td>In this paper, we explore the potential of Large Language Models (LLMs)<br>Agents in playing the strategic social deduction game, Resistance Avalon.<br>Players in Avalon are challenged not only to make informed decisions based on<br>dynamically evolving game phases, but also to engage in discussions where they<br>must deceive, deduce, and negotiate with other players. These characteristics<br>make Avalon a compelling test-bed to study the decision-making and<br>language-processing capabilities of LLM Agents. To facilitate research in this<br>line, we introduce AvalonBench - a comprehensive game environment tailored for<br>evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game<br>environment for Avalon, (2) rule-based bots as baseline opponents, and (3)<br>ReAct-style LLM agents with tailored prompts for each role. Notably, our<br>evaluations based on AvalonBench highlight a clear capability gap. For<br>instance, models like ChatGPT playing good-role got a win rate of 22.2% against<br>rule-based bots playing evil, while good-role bot achieves 38.2% win rate in<br>the same setting. We envision AvalonBench could be a good test-bed for<br>developing more advanced LLMs (with self-playing) and agent frameworks that can<br>effectively model the layered complexities of such game environments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AvalonBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç¤¾äº¤æ¨ç†æ¸¸æˆå¦‚ Resistance Avalon å¯¹ç©å®¶çš„æ¨ç†ã€æ²Ÿé€šå’Œå†³ç­–èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆè¦æ±‚ç©å®¶åœ¨åŠ¨æ€å˜åŒ–çš„æ¸¸æˆé˜¶æ®µåšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œå¹¶åœ¨è®¨è®ºä¸­æ¬ºéª—ã€æ¨ç†å’Œä¸å…¶ä»–ç©å®¶åå•†ã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾— Avalon æˆä¸ºç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å†³ç­–å’Œè¯­è¨€å¤„ç†èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„è¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†æ¸¸æˆç¯å¢ƒä¸­çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº† AvalonBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šä»£ç† LLM ä»£ç†çš„æ¸¸æˆç¯å¢ƒã€‚AvalonBench åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š<br><br>1. **Avalon æ¸¸æˆç¯å¢ƒ**ï¼šä¸ºä»£ç†æä¾›æ¸¸æˆå¹³å°ï¼Œè®°å½•æ‰€æœ‰ç©å®¶çš„è¡ŒåŠ¨å¹¶æ¨åŠ¨æ¸¸æˆè¿›ç¨‹ã€‚<br>2. **åŸºäºè§„åˆ™çš„æœºå™¨äºº**ï¼šä½œä¸ºåŸºçº¿å¯¹æ‰‹ï¼Œä¸ºä»£ç†æä¾›å¯æ¯”è¾ƒçš„åŸºå‡†ã€‚<br>3. **ReAct é£æ ¼çš„ LLM ä»£ç†**ï¼šé’ˆå¯¹æ¯ä¸ªè§’è‰²å®šåˆ¶æç¤ºï¼Œä»¥è¯„ä¼° LLM ä»£ç†åœ¨ä¸åŒè§’è‰²ä¸‹çš„è¡¨ç°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨ AvalonBench å¯¹ ChatGPT-3.5 å’Œ Llama2 æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸åŸºäºè§„åˆ™çš„æœºå™¨äººè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ‰è®¨è®ºçš„æƒ…å†µä¸‹ï¼ŒLLM ä»£ç†çš„è¡¨ç°ä¹Ÿè¿œä½äºåŸºäºè§„åˆ™çš„æœºå™¨äººã€‚ä¾‹å¦‚ï¼ŒChatGPT-3.5 åœ¨æ‰®æ¼”å¥½äººè§’è‰²æ—¶ï¼Œåœ¨ä¸æ‰®æ¼”åäººçš„åŸºäºè§„åˆ™çš„æœºå™¨äººå¯¹æŠ—ä¸­ï¼Œèƒœç‡ä¸º 22.2%ï¼Œè€Œå¥½äººè§’è‰²çš„æœºå™¨äººèƒœç‡ä¸º 38.2%ã€‚è¿™è¡¨æ˜å½“å‰ LLM ä»£ç†åœ¨æ¨ç†ã€è¯´æœã€åå•†å’Œæ¬ºéª—èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾å·®è·ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AvalonBench ä¸ºç ”ç©¶ LLM ä»£ç†åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å…ˆè¿›çš„ LLM ä»£ç†ï¼Œå¹¶æ¢ç´¢å¦‚ä½•å°†å†³ç­–æŠ€æœ¯é›†æˆåˆ° LLM ä¸­ï¼Œä»¥æé«˜å…¶åœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒAvalonBench è¿˜å¯ä»¥ç”¨äºè¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†åä½œã€æ²Ÿé€šå’Œç­–ç•¥åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>53</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Steve-Eyeï¼šä¸ºåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“èµ‹äºˆå¼€æ”¾ä¸–ç•Œçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ‹äºˆå…·èº«æ™ºèƒ½ä½“ä¸ä¸–ç•Œäº’åŠ¨çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™æ ‡å¿—ç€é€šç”¨æœºå™¨äººæŠ€æœ¯è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€å¿½ç•¥äº†å¼€æ”¾ä¸–ç•Œçš„è§†è§‰ä¸°å¯Œæ€§ï¼Œå¯¼è‡´æ•´ä¸ªäº¤äº’è¿‡ç¨‹ç±»ä¼¼äºâ€œä¸€ä¸ªè’™ç€çœ¼ç›çš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆâ€ã€‚å› æ­¤ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨ç›´è§‚åœ°ç†è§£å‘¨å›´ç¯å¢ƒå’Œç”Ÿæˆæ˜“äºç†è§£çš„å“åº”æ–¹é¢ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Steve-Eyeï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨èµ‹äºˆåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œè§†è§‰æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚Steve-Eyeå°†LLMä¸è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰-æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠè‡ªåŠ¨ç­–ç•¥æ”¶é›†äº†ä¸€ä¸ªåŒ…å«850Kå¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤å¯¹çš„å¹¿æ³›æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ¶µç›–æ™ºèƒ½ä½“çš„ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªå¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ï¼Œç„¶åä»å¹¿æ³›çš„è§†è§’è¿›è¡Œå¤§é‡å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSteve-Eyeåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºäºLLMçš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹ä¸‰ä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼š<br>1. ç¯å¢ƒè§†è§‰æè¿°ï¼ˆENV-VCï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æ„ŸçŸ¥å’Œæè¿°å…¶å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚<br>2. åŸºç¡€çŸ¥è¯†é—®ç­”ï¼ˆFK-QAï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æŒæ¡å¯¹å†³ç­–è‡³å…³é‡è¦çš„åŸºæœ¬çŸ¥è¯†çš„ç†Ÿç»ƒç¨‹åº¦ã€‚<br>3. æŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ï¼ˆSPPï¼‰ï¼šé‡åŒ–æ™ºèƒ½ä½“åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Steve-Eyeçš„ç ”ç©¶æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æœ‰æ•ˆäº’åŠ¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚å…¶å¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’åŠŸèƒ½ä¸ºæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»è¡ŒåŠ¨å’Œè§„åˆ’æä¾›äº†å¼ºå¤§çš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒSteve-Eyeçš„å¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ä¸ºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Creative Agents: Empowering Agents with Imagination for Creative Tasks</td>
      <td>We study building embodied agents for open-ended creative tasks. While<br>existing methods build instruction-following agents that can perform diverse<br>open-ended tasks, none of them demonstrates creativity -- the ability to give<br>novel and diverse task solutions implicit in the language instructions. This<br>limitation comes from their inability to convert abstract language instructions<br>into concrete task goals in the environment and perform long-horizon planning<br>for such complicated goals. Given the observation that humans perform creative<br>tasks with the help of imagination, we propose a class of solutions for<br>creative agents, where the controller is enhanced with an imaginator that<br>generates detailed imaginations of task outcomes conditioned on language<br>instructions. We introduce several approaches to implementing the components of<br>creative agents. We implement the imaginator with either a large language model<br>for textual imagination or a diffusion model for visual imagination. The<br>controller can either be a behavior-cloning policy learned from data or a<br>pre-trained foundation model generating executable codes in the environment. We<br>benchmark creative tasks with the challenging open-world game Minecraft, where<br>the agents are asked to create diverse buildings given free-form language<br>instructions. In addition, we propose novel evaluation metrics for open-ended<br>creative tasks utilizing GPT-4V, which holds many advantages over existing<br>metrics. We perform a detailed experimental analysis of creative agents,<br>showing that creative agents are the first AI agents accomplishing diverse<br>building creation in the survival mode of Minecraft. Our benchmark and models<br>are open-source for future research on creative agents<br>(https://github.com/PKU-RL/Creative-Agents).</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ›æ„æ™ºèƒ½ä½“ï¼šèµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›ä»¥å®Œæˆåˆ›æ„ä»»åŠ¡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„æ™ºèƒ½ä½“å¤§å¤šåªèƒ½æ‰§è¡Œé¢„å®šä¹‰çš„ä»»åŠ¡ï¼Œç¼ºä¹å¤„ç†å¼€æ”¾æ€§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftæ¸¸æˆä¸­ï¼Œç°æœ‰çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œç®€å•çš„æŒ‡ä»¤ï¼Œå¦‚â€œæ”¶é›†çŸ³å¤´â€æˆ–â€œå»ºé€ ä¸€ä¸ªé›ªäººâ€ï¼Œä½†æ— æ³•å®Œæˆæ›´å¤æ‚çš„åˆ›æ„ä»»åŠ¡ï¼Œå¦‚â€œå»ºé€ ä¸€ä¸ªç ‚å²©å®«æ®¿â€ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•å°†æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºå…·ä½“çš„ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œé•¿æœŸè§„åˆ’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œåˆ›æ„æ™ºèƒ½ä½“â€çš„æ¦‚å¿µï¼Œé€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›æ¥å¤„ç†å¼€æ”¾æ€§åˆ›æ„ä»»åŠ¡ã€‚åˆ›æ„æ™ºèƒ½ä½“ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šæƒ³è±¡å™¨å’Œæ§åˆ¶å™¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæƒ³è±¡å™¨<br>æƒ³è±¡å™¨è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»»åŠ¡ç»“æœçš„è¯¦ç»†æƒ³è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æƒ³è±¡å™¨çš„æ–¹æ³•ï¼š<br>- **æ–‡æœ¬æƒ³è±¡**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æŠ€æœ¯ç”Ÿæˆæ–‡æœ¬æƒ³è±¡ã€‚<br>- **è§†è§‰æƒ³è±¡**ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionï¼Œç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸ç¬¦çš„è§†è§‰æƒ³è±¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ§åˆ¶å™¨<br>æ§åˆ¶å™¨è´Ÿè´£å°†æƒ³è±¡è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æ§åˆ¶å™¨çš„æ–¹æ³•ï¼š<br>- **è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨**ï¼šä»ç¯å¢ƒä¸­å­¦ä¹ è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå°†å›¾åƒæƒ³è±¡è½¬æ¢ä¸ºå»ºç­‘è“å›¾ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚<br>- **åŸºäºGPT-4(V)çš„æ§åˆ¶å™¨**ï¼šåˆ©ç”¨GPT-4(V)çš„è§†è§‰è¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ¥å®Œæˆä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Minecraftæ¸¸æˆä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ›æ„æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æŒ‡ä»¤åˆ›å»ºå¤šæ ·åŒ–å’Œè§†è§‰ä¸Šå¸å¼•äººçš„å»ºç­‘ã€‚å…¶ä¸­ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œå¹¶ç»“åˆGPT-4(V)è¿›è¡Œæ§åˆ¶çš„æ™ºèƒ½ä½“è¡¨ç°æœ€ä½³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åˆ›æ„æ™ºèƒ½ä½“æ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ å’Œåˆ›æ„AIæ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•æé«˜è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨çš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•å¢å¼ºæ™ºèƒ½ä½“çš„åˆ›é€ åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>48</th>
      <td>MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs</td>
      <td>While Vision-Language Models (VLMs) hold promise for tasks requiring<br>extensive collaboration, traditional multi-agent simulators have facilitated<br>rich explorations of an interactive artificial society that reflects collective<br>behavior. However, these existing simulators face significant limitations.<br>Firstly, they struggle with handling large numbers of agents due to high<br>resource demands. Secondly, they often assume agents possess perfect<br>information and limitless capabilities, hindering the ecological validity of<br>simulated social interactions. To bridge this gap, we propose a multi-agent<br>Minecraft simulator, MineLand, that bridges this gap by introducing three key<br>features: large-scale scalability, limited multimodal senses, and physical<br>needs. Our simulator supports 64 or more agents. Agents have limited visual,<br>auditory, and environmental awareness, forcing them to actively communicate and<br>collaborate to fulfill physical needs like food and resources. Additionally, we<br>further introduce an AI agent framework, Alex, inspired by multitasking theory,<br>enabling agents to handle intricate coordination and scheduling. Our<br>experiments demonstrate that the simulator, the corresponding benchmark, and<br>the AI agent framework contribute to more ecological and nuanced collective<br>behavior.The source code of MineLand and Alex is openly available at<br>https://github.com/cocacola-lab/MineLand.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MineLandï¼šæ¨¡æ‹Ÿå¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“äº¤äº’çš„Minecraftæ¨¡æ‹Ÿå™¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶é¢ä¸´èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼Œå¹¶ä¸”é€šå¸¸å‡è®¾æ™ºèƒ½ä½“æ‹¥æœ‰å®Œç¾ä¿¡æ¯å’Œæ— é™èƒ½åŠ›ï¼Œè¿™ä¸ç°å®ä¸–ç•Œä¸­çš„äººç±»äº¤äº’å­˜åœ¨è¾ƒå¤§å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MineLandï¼Œä¸€ä¸ªåŸºäºMinecraftçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿæ›´æ¥è¿‘ç°å®ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“äº¤äº’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤§è§„æ¨¡å¯æ‰©å±•æ€§<br>MineLandé€šè¿‡å°†æ¯ä¸ªMinecraftå®¢æˆ·ç«¯ç®€åŒ–ä¸ºå•ä¸ªçº¿ç¨‹ï¼Œä¼˜åŒ–äº†æ€§èƒ½å¼€é”€ï¼Œä»è€Œæ”¯æŒ64ä¸ªæˆ–æ›´å¤šæ™ºèƒ½ä½“åœ¨ä¸»æµæ¶ˆè´¹çº§æ¡Œé¢PCä¸Šè¿è¡Œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ‰é™çš„æ¨¡æ€æ„ŸçŸ¥<br>MineLandæ¨¡æ‹Ÿäº†äººç±»çš„è§†è§‰å’Œå¬è§‰æœºåˆ¶ï¼Œå¯¹æ™ºèƒ½ä½“çš„æ„ŸçŸ¥èƒ½åŠ›æ–½åŠ äº†é™åˆ¶ï¼ŒåŒ…æ‹¬è·ç¦»è¡°å‡ã€ç¯å¢ƒé®æŒ¡å’Œæ–¹å‘çº¦æŸï¼Œä½¿å…¶æ›´æ¥è¿‘ç°å®ä¸–ç•Œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç‰©ç†éœ€æ±‚<br>MineLandå°†çœŸå®çš„ç‰©ç†éœ€æ±‚ï¼ˆå¦‚é£Ÿç‰©ã€æ°§æ°”å’Œé¥¥é¥¿ï¼‰é›†æˆåˆ°æ™ºèƒ½ä½“ä¸­ï¼Œä½¿å…¶éœ€è¦ç®¡ç†èµ„æºå¹¶ä¸å…¶ä»–æ™ºèƒ½ä½“ç«äº‰æˆ–åˆä½œï¼Œä»¥ç»´æŒç”Ÿå­˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šä»»åŠ¡å¤„ç†æ¡†æ¶Alex<br>MineLandå¼•å…¥äº†åŸºäºå¤šä»»åŠ¡ç†è®ºçš„AIæ™ºèƒ½ä½“æ¡†æ¶Alexï¼Œå…è®¸æ™ºèƒ½ä½“åŒæ—¶æ‰§è¡Œå¤æ‚çš„åè°ƒå’Œè°ƒåº¦ï¼Œä»¥å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒMineLandåœ¨æ”¯æŒå¤§è§„æ¨¡æ™ºèƒ½ä½“ã€æœ‰é™çš„æ¨¡æ€æ„ŸçŸ¥å’Œç‰©ç†éœ€æ±‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒAlexæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šä»»åŠ¡ï¼Œå¹¶åœ¨åˆä½œæ¨¡å¼ä¸‹æé«˜æ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MineLandä¸ºç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ï¼Œå…¶åˆ›æ–°çš„è®¾è®¡å’ŒåŠŸèƒ½å¯ä»¥åº”ç”¨äºäººç±»åŠ¨åŠ›å­¦ã€ç¤¾ä¼šå¿ƒç†å­¦ã€æœºå™¨äººæŠ€æœ¯å’Œæ¸¸æˆè®¾è®¡ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼ŒAlexæ¡†æ¶çš„å¤šä»»åŠ¡å¤„ç†æœºåˆ¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„AIæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>225</th>
      <td>MindAgent: Emergent Gaming Interaction</td>
      <td>Large Language Models (LLMs) have the capacity of performing complex<br>scheduling in a multi-agent system and can coordinate these agents into<br>completing sophisticated tasks that require extensive collaboration. However,<br>despite the introduction of numerous gaming frameworks, the community has<br>insufficient benchmarks towards building general multi-agents collaboration<br>infrastructure that encompass both LLM and human-NPCs collaborations. In this<br>work, we propose a novel infrastructure - MindAgent - to evaluate planning and<br>coordination emergent capabilities for gaming interaction. In particular, our<br>infrastructure leverages existing gaming framework, to i) require understanding<br>of the coordinator for a multi-agent system, ii) collaborate with human players<br>via un-finetuned proper instructions, and iii) establish an in-context learning<br>on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new<br>gaming scenario and related benchmark that dispatch a multi-agent collaboration<br>efficiency and supervise multiple agents playing the game simultaneously. We<br>conduct comprehensive evaluations with new auto-metric CoS for calculating the<br>collaboration efficiency. Finally, our infrastructure can be deployed into<br>real-world gaming scenarios in a customized VR version of CUISINEWORLD and<br>adapted in existing broader Minecraft gaming domain. We hope our findings on<br>LLMs and the new infrastructure for general-purpose scheduling and coordination<br>can help shed light on how such skills can be obtained by learning from large<br>language corpora.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MindAgentï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ä¹Ÿé€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆæ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•è¿˜ä¸è¶³ä»¥è¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨LLMsä¸äººç±»NPCsåä½œçš„åœºæ™¯ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMindAgentçš„æ–°å‹åŸºç¡€è®¾æ–½ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸åŸºå‡†æµ‹è¯•<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåä¸ºCUISINEWORLDçš„æ¸¸æˆåœºæ™¯ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªè™šæ‹Ÿå¨æˆ¿ç¯å¢ƒï¼Œå…¶ä¸­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿéœ€è¦åè°ƒå¤šä¸ªä»£ç†ï¼Œå®Œæˆå°½å¯èƒ½å¤šçš„èœè‚´è®¢å•ã€‚CUISINEWORLDæ¸¸æˆåœºæ™¯å…·æœ‰å¤šç§ä»»åŠ¡ç»“æ„å’Œéš¾åº¦ï¼Œæ˜¯è¯„ä¼°LLMsæ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMindAgentåŸºç¡€è®¾æ–½<br>MindAgentæ˜¯ä¸€ä¸ªç”¨äºLLMsäº¤äº’å¼å¤šæ™ºèƒ½ä½“è§„åˆ’çš„åŸºç¡€è®¾æ–½ï¼Œå®ƒå±•ç¤ºäº†LLMsçš„æ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†å¤šç§æç¤ºæŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›LLMsçš„è§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬æä¾›å°‘é‡ç¤ºä¾‹ã€è§„åˆ’ç†ç”±å’Œç¯å¢ƒåé¦ˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨CUISINEWORLDæ¸¸æˆåœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š<br>1. é›¶æ ·æœ¬å¤šæ™ºèƒ½ä½“è§„åˆ’ï¼šå¼ºå¤§çš„é¢„è®­ç»ƒLLMsï¼ˆå¦‚GPT-4ï¼‰èƒ½å¤Ÿé€šè¿‡é˜…è¯»ç®€å•çš„æ¸¸æˆæŒ‡ä»¤å’Œé£Ÿè°±ï¼Œè°ƒåº¦å¤šä¸ªä»£ç†ï¼ˆ2åˆ°4ä¸ªï¼‰å®Œæˆèœè‚´ï¼Œç”šè‡³ä¸äººç±»ç©å®¶åä½œã€‚<br>2. åŸºäºé«˜çº§æç¤ºçš„è§„åˆ’ï¼šé€šè¿‡åˆ©ç”¨æ¶Œç°å¼ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜LLMsçš„å¤šæ™ºèƒ½ä½“è§„åˆ’æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ å°‘é‡ä¸“å®¶æ¼”ç¤ºã€è§£é‡ŠæŸäº›è¡ŒåŠ¨çš„ç†ç”±ï¼Œä»¥åŠåœ¨è§„åˆ’è¿‡ç¨‹ä¸­æä¾›å®æ—¶åé¦ˆã€‚<br>3. é€šç”¨æ½œåŠ›ï¼šLLMsè¡¨ç°å‡ºæˆä¸ºé€šç”¨å¤šæ™ºèƒ½ä½“è§„åˆ’å™¨çš„å·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹æ³›åŒ–åˆ°æ›´å¤šä»£ç†ï¼Œå¹¶é€‚åº”æ–°çš„æ¸¸æˆé¢†åŸŸï¼Œå¦‚Minecraftã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„MindAgentåŸºç¡€è®¾æ–½å’ŒCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼ŒLLMsåœ¨å¤šæ™ºèƒ½ä½“è§„åˆ’æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨åŠ¨æ¸¸æˆAIçš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>276</th>
      <td>BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</td>
      <td>We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in<br>simulation, spanning a range of everyday household chores such as cleaning,<br>maintenance, and food preparation. These activities are designed to be<br>realistic, diverse, and complex, aiming to reproduce the challenges that agents<br>must face in the real world. Building such a benchmark poses three fundamental<br>difficulties for each activity: definition (it can differ by time, place, or<br>person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these<br>with three innovations. First, we propose an object-centric, predicate<br>logic-based description language for expressing an activity's initial and goal<br>conditions, enabling generation of diverse instances for any activity. Second,<br>we identify the simulator-agnostic features required by an underlying<br>environment to support BEHAVIOR, and demonstrate its realization in one such<br>simulator. Third, we introduce a set of metrics to measure task progress and<br>efficiency, absolute and relative to human demonstrators. We include 500 human<br>demonstrations in virtual reality (VR) to serve as the human ground truth. Our<br>experiments demonstrate that even state of the art embodied AI solutions<br>struggle with the level of realism, diversity, and complexity imposed by the<br>activities in our benchmark. We make BEHAVIOR publicly available at<br>behavior.stanford.edu to facilitate and calibrate the development of new<br>embodied AI solutions.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | BEHAVIORï¼šè™šæ‹Ÿäº¤äº’ç”Ÿæ€ç¯å¢ƒä¸­æ—¥å¸¸å®¶åº­æ´»åŠ¨çš„åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•å¯¹äºè¯„ä¼°å’Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€ç¼ºä¹ç°å®æ€§ã€å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BEHAVIORï¼Œä¸€ä¸ªåŒ…å«100ä¸ªæ—¥å¸¸å®¶åº­æ´»åŠ¨çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æ¨åŠ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„å‘å±•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºè°“è¯é€»è¾‘çš„æè¿°è¯­è¨€<br>BEHAVIORå¼•å…¥äº†ä¸€ç§åŸºäºè°“è¯é€»è¾‘çš„æè¿°è¯­è¨€ï¼Œç”¨äºè¡¨è¾¾æ´»åŠ¨çš„åˆå§‹å’Œç›®æ ‡æ¡ä»¶ã€‚è¿™ç§è¯­è¨€å…è®¸ç”Ÿæˆå¤šæ ·åŒ–çš„æ´»åŠ¨å®ä¾‹ï¼Œå¹¶èƒ½å¤Ÿæ¥å—ä»»ä½•æœ‰æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡æ‹Ÿå™¨æ— å…³çš„ç¯å¢ƒç‰¹å¾<br>BEHAVIORç¡®å®šäº†æ”¯æŒå…¶æ´»åŠ¨çš„æ¨¡æ‹Ÿå™¨æ— å…³ç‰¹å¾ï¼Œå¹¶åœ¨iGibson 2.0ä¸­å®ç°äº†è¿™äº›ç‰¹å¾ã€‚è¿™ä½¿å¾—BEHAVIORå¯ä»¥åœ¨å¤šç§ç¯å¢ƒä¸­å®ç°ï¼Œå¹¶æä¾›äº†æ— é™å¤šæ ·åŒ–çš„æ´»åŠ¨å®ä¾‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºäººç±»è¡¨ç°çš„è¯„ä¼°æŒ‡æ ‡<br>BEHAVIORå¼•å…¥äº†ä¸€ç³»åˆ—è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ™ºèƒ½ä½“åœ¨ä»»åŠ¡è¿›åº¦å’Œæ•ˆç‡æ–¹é¢çš„è¡¨ç°ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬æˆåŠŸåˆ†æ•°ã€æ•ˆç‡æŒ‡æ ‡å’ŒåŸºäºäººç±»è¡¨ç°çš„æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å…¬å¹³æ€§å’Œå¯æ¯”æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“ï¼Œåœ¨é¢å¯¹BEHAVIORçš„æŒ‘æˆ˜æ—¶ä¹Ÿéš¾ä»¥å–å¾—è‰¯å¥½çš„è¡¨ç°ã€‚è¿™è¡¨æ˜BEHAVIORçš„åŸºå‡†æµ‹è¯•å…·æœ‰å¾ˆé«˜çš„éš¾åº¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>BEHAVIORçš„åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„å‘å±•æä¾›äº†é‡è¦çš„å·¥å…·ã€‚å…¶åŸºäºè°“è¯é€»è¾‘çš„æè¿°è¯­è¨€ã€æ¨¡æ‹Ÿå™¨æ— å…³çš„ç¯å¢ƒç‰¹å¾å’ŒåŸºäºäººç±»è¡¨ç°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºå…¶ä»–åŸºå‡†æµ‹è¯•æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒã€‚æ­¤å¤–ï¼ŒBEHAVIORçš„åŸºå‡†æµ‹è¯•è¿˜å¯ä»¥ç”¨äºå¼€å‘æ–°çš„æ™ºèƒ½ä½“è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>267</th>
      <td>Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</td>
      <td>In this study, we explore the application of Large Language Models (LLMs) in<br>\textit{Jubensha}, a Chinese detective role-playing game and a novel area in<br>Artificial Intelligence (AI) driven gaming. We introduce the first dataset<br>specifically for Jubensha, including character scripts and game rules, to<br>foster AI agent development in this complex narrative environment. Our work<br>also presents a unique multi-agent interaction framework using LLMs, allowing<br>AI agents to autonomously engage in this game. To evaluate the gaming<br>performance of these AI agents, we developed novel methods measuring their<br>mastery of case information and reasoning skills. Furthermore, we incorporated<br>the latest advancements in in-context learning to improve the agents'<br>performance in information gathering, murderer identification, and logical<br>reasoning. The experimental results validate the effectiveness of our proposed<br>methods. This work aims to offer a novel perspective on understanding LLM<br>capabilities and establish a new benchmark for evaluating large language<br>model-based agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è§£ç æ•°å­—ä¾¦æ¢ï¼šç†è§£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†æ¸¸æˆä¸­çš„è¡Œä¸ºå’Œèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äº’åŠ¨è§’è‰²æ‰®æ¼”æ¸¸æˆï¼ˆIRPGsï¼‰çš„å…¨çƒæµè¡Œï¼Œç‰¹åˆ«æ˜¯ä¸­å›½ä¾¦æ¢è§’è‰²æ‰®æ¼”æ¸¸æˆâ€œå‰§æœ¬æ€â€ï¼ˆJubenshaï¼‰çš„å…´èµ·ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨ä¹Ÿæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¼ ç»Ÿçš„æ£‹ç±»æ¸¸æˆã€è§†é¢‘æ¸¸æˆç­‰é¢†åŸŸï¼Œå¯¹äºâ€œå‰§æœ¬æ€â€è¿™ç±»éœ€è¦å¤šè½®è¯­è¨€äº¤äº’ã€ä¿¡æ¯æ”¶é›†å’Œé€»è¾‘æ¨ç†çš„æ¸¸æˆï¼ŒAIçš„åº”ç”¨è¿˜å¤„äºèµ·æ­¥é˜¶æ®µã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥è¡¡é‡LLMåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹â€œå‰§æœ¬æ€â€æ¸¸æˆçš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬è§’è‰²å‰§æœ¬å’Œé¢„è®¾æ¸¸æˆè§„åˆ™ï¼Œä¸ºAIä»£ç†çš„å¼€å‘æä¾›äº†åŸºç¡€ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ä¸€ä¸ªç‹¬ç‰¹çš„å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œä½¿ç”¨LLMsä½¿AIä»£ç†èƒ½å¤Ÿè‡ªä¸»å‚ä¸â€œå‰§æœ¬æ€â€æ¸¸æˆã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ºäº†è¯„ä¼°AIä»£ç†åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œè®¾è®¡äº†ä¸¤ä¸ªæ–°é¢–çš„ä»»åŠ¡ï¼šä¸€ä¸ªç”¨äºè¯„ä¼°ä»–ä»¬å¯¹æ¡ˆä»¶ä¿¡æ¯çš„æŒæ¡ç¨‹åº¦ï¼Œå¦ä¸€ä¸ªç”¨äºè¯„ä¼°ä»–ä»¬çš„æ¨ç†èƒ½åŠ›ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ©ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œè®¾è®¡äº†æ¨¡å—æ¥å¢å¼ºLLMä»£ç†åœ¨ä¿¡æ¯æ”¶é›†ã€å‡¶æ‰‹è¯†åˆ«å’Œé€»è¾‘æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä¿¡æ¯æ”¶é›†ã€å‡¶æ‰‹è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾è‘—æé«˜äº†LLMä»£ç†çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æ²¡æœ‰è®°å¿†æ£€ç´¢æ¨¡å—çš„ä»£ç†ç›¸æ¯”ï¼Œå…·æœ‰è®°å¿†æ£€ç´¢æ¨¡å—çš„ä»£ç†åœ¨å›ç­”å…³äºå…¶ä»–è§’è‰²çš„é—®é¢˜æ—¶å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œè‡ªå®Œå–„å’Œè‡ªéªŒè¯æ¨¡å—çš„ç»„åˆè¿›ä¸€æ­¥æé«˜äº†ä»£ç†çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜è¿™äº›æ¨¡å—æœ‰æ•ˆåœ°å¢å¼ºäº†ä»£ç†åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„æ²Ÿé€šæ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ä¸ºLLMsåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºè¯„ä¼°LLMä»£ç†çš„æ€§èƒ½å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ThinkThriceæ¡†æ¶å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å—çš„è®¾è®¡ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å…·æ¨ç†èƒ½åŠ›çš„AIä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>64</th>
      <td>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</td>
      <td>The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by<br>Large Language Models (LLMs) make them promising candidates for developing<br>coordination agents. In this study, we introduce a new LLM-Coordination<br>Benchmark aimed at a detailed analysis of LLMs within the context of Pure<br>Coordination Games, where participating agents need to cooperate for the most<br>gain. This benchmark evaluates LLMs through two distinct tasks: (1)<br>\emph{Agentic Coordination}, where LLMs act as proactive participants for<br>cooperation in 4 pure coordination games; (2) \emph{Coordination Question<br>Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice<br>questions from the 4 games for evaluation of three key reasoning abilities:<br>Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to<br>enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture<br>for Coordination (CAC) framework that can easily integrate different LLMs as<br>plug-and-play modules for pure coordination games. Our findings indicate that<br>LLM agents equipped with GPT-4-turbo achieve comparable performance to<br>state-of-the-art reinforcement learning methods in games that require<br>commonsense actions based on the environment. Besides, zero-shot coordination<br>experiments reveal that, unlike RL methods, LLM agents are robust to new unseen<br>partners. However, results on Coordination QA show a large room for improvement<br>in the Theory of Mind reasoning and joint planning abilities of LLMs. The<br>analysis also sheds light on how the ability of LLMs to understand their<br>environment and their partner's beliefs and intentions plays a part in their<br>ability to plan for coordination. Our code is available at<br>\url{https://github.com/eric-ai-lab/llm_coordination}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LLM-Coordinationï¼šè¯„ä¼°å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨è®¸å¤šæ—¥å¸¸ä»»åŠ¡å’Œå…³é”®æ“ä½œä¸­ï¼Œå¦‚çƒ¹é¥ªå’Œæ•‘æ´è¡ŒåŠ¨ï¼Œåˆä½œæ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›åœºæ™¯å¯ä»¥è¢«è§†ä¸ºçº¯åè°ƒæ¸¸æˆï¼Œå…¶ä¸­æ‰€æœ‰å‚ä¸æ–¹éƒ½ä»é€‰æ‹©å®Œå…¨ä¸€è‡´çš„æˆ˜ç•¥ä¸­å—ç›Šï¼Œé¿å…ä»»ä½•åˆ©ç›Šå†²çªã€‚è¿™äº›æ¸¸æˆè¦æ±‚ä»£ç†æ¨ç†ä»–ä»¬çš„ç¯å¢ƒå¹¶è®¡åˆ’ï¼ŒåŒæ—¶è€ƒè™‘ä»–ä»¬çš„ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒä¸­çš„æ¶Œç°è§„åˆ’èƒ½åŠ›ã€ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›å’Œå¯¹å¿ƒæ™ºç†è®ºçš„æš—ç¤ºï¼Œä½¿å®ƒä»¬æˆä¸ºå¼€å‘åè°ƒä»£ç†çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ç„¶è€Œï¼ŒLLMsåœ¨åè°ƒæ¸¸æˆä¸­çš„å¿…è¦æ¡ä»¶ã€ä¼˜åŠ¿å’Œå±€é™æ€§ä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è¿›è¡ŒLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›çš„å…¨é¢è¯„ä¼°å’Œåˆ†ææ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„LLM-CoordinationåŸºå‡†ï¼Œæ—¨åœ¨å¯¹LLMsåœ¨çº¯åè°ƒæ¸¸æˆä¸­çš„èƒ½åŠ›è¿›è¡Œè¯¦ç»†åˆ†æã€‚è¯¥åŸºå‡†é€šè¿‡ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡è¯„ä¼°LLMsï¼š<br>1. **ä»£ç†åè°ƒ**ï¼šLLMsä½œä¸ºç§¯æåˆä½œå‚ä¸è€…å‚ä¸4ä¸ªçº¯åè°ƒæ¸¸æˆã€‚<br>2. **åè°ƒé—®ç­”ï¼ˆQAï¼‰**ï¼šLLMsè¢«æç¤ºå›ç­”æ¥è‡ª4ä¸ªæ¸¸æˆçš„198ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œä»¥è¯„ä¼°ä¸‰ä¸ªå…³é”®æ¨ç†èƒ½åŠ›ï¼šç¯å¢ƒç†è§£ã€å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’ã€‚<br><br>æ­¤å¤–ï¼Œä¸ºäº†ä½¿LLMsèƒ½å¤Ÿè¿›è¡Œå¤šæ™ºèƒ½ä½“åè°ƒï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåè°ƒè®¤çŸ¥æ¶æ„ï¼ˆCACï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è½»æ¾åœ°å°†ä¸åŒçš„LLMsä½œä¸ºå³æ’å³ç”¨æ¨¡å—é›†æˆåˆ°çº¯åè°ƒæ¸¸æˆä¸­ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡GPT-4-turboçš„LLMä»£ç†åœ¨éœ€è¦åŸºäºç¯å¢ƒçš„å¸¸è¯†è¡ŒåŠ¨çš„æ¸¸æˆä¸­ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œé›¶æ ·æœ¬åè°ƒå®éªŒè¡¨æ˜ï¼Œä¸RLæ–¹æ³•ä¸åŒï¼ŒLLMä»£ç†å¯¹æ–°æœªè§ä¼™ä¼´å…·æœ‰é²æ£’æ€§ã€‚ç„¶è€Œï¼Œåè°ƒQAçš„ç»“æœè¡¨æ˜ï¼ŒLLMsçš„å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚åˆ†æè¿˜æ­ç¤ºäº†LLMsç†è§£å…¶ç¯å¢ƒå’Œå…¶ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾çš„èƒ½åŠ›å¦‚ä½•å½±å“å®ƒä»¬åè°ƒè®¡åˆ’çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„LLM-CoordinationåŸºå‡†å’ŒCACæ¡†æ¶ä¸ºè¯„ä¼°å’Œåˆ†æLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç»“æœçªå‡ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>72</th>
      <td>SimulBench: Evaluating Language Models with Creative Simulation Tasks</td>
      <td>We introduce SimulBench, a benchmark designed to evaluate large language<br>models (LLMs) across a diverse collection of creative simulation scenarios,<br>such as acting as a Linux terminal or playing text games with users. While<br>these simulation tasks serve as effective measures of an LLM's general<br>intelligence, they are seldom incorporated into existing benchmarks. A major<br>challenge is to develop an evaluation framework for testing different LLMs<br>fairly while preserving the multi-round interactive nature of simulation tasks<br>between users and AI. To tackle this issue, we suggest using a fixed LLM as a<br>user agent to engage with an LLM to collect dialogues first under different<br>tasks. Then, challenging dialogue scripts are extracted for evaluating<br>different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4<br>is employed as the evaluator, tasked with reviewing the quality of the final<br>response generated by the target LLMs given multi-turn dialogue scripts. Our<br>comprehensive experiments indicate that these simulation tasks continue to pose<br>a significant challenge with their unique natures and show the gap between<br>proprietary models and the most advanced open LLMs. For example, GPT-4-turbo<br>outperforms LLaMA-3-70b-Chat on 18.55\% more cases.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SimulBenchï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨åˆ›æ„æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿå¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å•è½®ã€é™æ€çš„ç”¨æˆ·ä¸LLMsä¹‹é—´çš„äº¤äº’ï¼Œç¼ºä¹å¯¹å¤šè½®äº¤äº’å’Œå¤æ‚æ¨¡æ‹Ÿèƒ½åŠ›çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ä¸äººç±»ç›¸å…³çš„æ¨¡æ‹Ÿä»»åŠ¡ä¸Šï¼Œè€Œå¿½ç•¥äº†éäººç±»ä¸­å¿ƒçš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œå¦‚Linuxç»ˆç«¯æˆ–æ–‡æœ¬æ¸¸æˆç­‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSimulBenchåŸºå‡†<br>æœ¬æ–‡æå‡ºäº†SimulBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMsåœ¨åˆ›æ„æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†ã€‚SimulBenchåŒ…å«109ä¸ªç‹¬ç‰¹çš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œæ¶µç›–äº†å„ç§æ¥å£ï¼Œå¦‚Linuxç»ˆç«¯ã€SQLæ‰§è¡Œå™¨ã€æ–‡æœ¬æ¸¸æˆç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè½®è„šæœ¬è¯„ä¼°æ¡†æ¶<br>ä¸ºäº†å…¬å¹³åœ°è¯„ä¼°ä¸åŒLLMsï¼ŒSimulBenché‡‡ç”¨äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„LLMä½œä¸ºç”¨æˆ·ä»£ç†ä¸å¦ä¸€ä¸ªLLMè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæ”¶é›†å¯¹è¯å†å²ã€‚ç„¶åï¼Œä»è¿™äº›å¯¹è¯å†å²ä¸­æå–å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹è¯è„šæœ¬ï¼Œç”¨äºè¯„ä¼°ä¸åŒçš„ç›®æ ‡LLMsã€‚æœ€åï¼Œä½¿ç”¨GPT-4ä½œä¸ºè¯„ä¼°è€…ï¼Œå¯¹ç›®æ ‡LLMsåœ¨ç»™å®šå¤šè½®å¯¹è¯è„šæœ¬ä¸‹çš„æœ€ç»ˆå“åº”è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSimulBenchä¸­çš„æ¨¡æ‹Ÿä»»åŠ¡å¯¹LLMsæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸”æ˜¾ç¤ºäº†ä¸“æœ‰æ¨¡å‹å’Œæœ€å…ˆè¿›çš„å¼€æºLLMsä¹‹é—´çš„å·®è·ã€‚ä¾‹å¦‚ï¼ŒGPT-4-turboåœ¨18.55%çš„æƒ…å†µä¸‹ä¼˜äºLLaMA-3-70b-Chatã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SimulBenchåŸºå‡†ä¸ºè¯„ä¼°LLMsåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å…¶å¤šè½®è„šæœ¬è¯„ä¼°æ¡†æ¶å¯ä»¥ç¡®ä¿å…¬å¹³çš„æ¯”è¾ƒï¼Œå¹¶æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMsåœ¨ä¸åŒæ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒSimulBenchçš„å®éªŒç»“æœä¹Ÿæ­ç¤ºäº†LLMsåœ¨å¤„ç†å¤æ‚æ¨¡æ‹Ÿä»»åŠ¡æ—¶çš„æŒ‘æˆ˜å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</td>
      <td>Video games have served as useful benchmarks for the decision making<br>community, but going beyond Atari games towards training agents in modern games<br>has been prohibitively expensive for the vast majority of the research<br>community. Recent progress in the research, development and open release of<br>large vision models has the potential to amortize some of these costs across<br>the community. However, it is currently unclear which of these models have<br>learnt representations that retain information critical for sequential decision<br>making. Towards enabling wider participation in the research of gameplaying<br>agents in modern games, we present a systematic study of imitation learning<br>with publicly available visual encoders compared to the typical, task-specific,<br>end-to-end training approach in Minecraft, Minecraft Dungeons and<br>Counter-Strike: Global Offensive.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è§†è§‰ç¼–ç å™¨åœ¨ç°ä»£è§†é¢‘æ¸¸æˆä¸­çš„é«˜æ•ˆæ¨¡ä»¿å­¦ä¹ <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è§†é¢‘æ¸¸æˆä¸€ç›´æ˜¯å†³ç­–åˆ¶å®šç¤¾åŒºçš„æœ‰ç”¨åŸºå‡†ï¼Œä½†å°†ç ”ç©¶æ‰©å±•åˆ°ç°ä»£æ¸¸æˆå¯¹äºå¤§å¤šæ•°ç ”ç©¶ç¤¾åŒºæ¥è¯´æˆæœ¬é«˜æ˜‚ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰æ¨¡å‹çš„ç ”ç©¶ã€å¼€å‘å’Œå…¬å¼€å‘å¸ƒæœ‰å¯èƒ½åœ¨æ•´ä¸ªç¤¾åŒºä¸­åˆ†æ‘Šè¿™äº›æˆæœ¬ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹ä¸­çš„å“ªäº›æ¨¡å‹å·²ç»å­¦ä¹ äº†ä¿ç•™å¯¹é¡ºåºå†³ç­–è‡³å…³é‡è¦çš„ä¿¡æ¯çš„è¡¨ç¤ºã€‚ä¸ºäº†ä½¿æ›´å¹¿æ³›çš„ç¤¾åŒºå‚ä¸ç°ä»£æ¸¸æˆä¸­çš„æ¸¸æˆä»£ç†ç ”ç©¶ï¼Œæœ¬æ–‡å¯¹Minecraftã€Minecraft Dungeonså’ŒCounter-Strike: Global Offensiveä¸­çš„æ¨¡ä»¿å­¦ä¹ è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œå¹¶ä¸å…¸å‹çš„ã€ç‰¹å®šä»»åŠ¡çš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æ¯”è¾ƒäº†ç«¯åˆ°ç«¯è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒç¼–ç å™¨åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¼–ç å™¨åœ¨ç›¸å¯¹è¾ƒå°çš„å›¾åƒä¸Šè®­ç»ƒï¼Œè€Œé¢„è®­ç»ƒç¼–ç å™¨åˆ™æ˜¯åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œå¯èƒ½æä¾›æœ‰ç”¨ä¸”é€šç”¨çš„è¡¨ç¤ºï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡ç ”ç©¶äº†ä¸åŒæ•°é‡çš„è®­ç»ƒæ•°æ®å¯¹è§†è§‰ç¼–ç å™¨æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒç¼–ç å™¨ä¹Ÿèƒ½è¡¨ç°å‡ºä¸ç‰¹å®šä»»åŠ¡ç¼–ç å™¨ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ä¸‰ä¸ªç°ä»£è§†é¢‘æ¸¸æˆï¼ˆMinecraftã€Minecraft Dungeonså’ŒCounter-Strike: Global Offensiveï¼‰ä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š<br><br>1. å°å›¾åƒï¼ˆ128Ã—128ï¼‰è¶³ä»¥è®­ç»ƒç°ä»£è§†é¢‘æ¸¸æˆä¸­çš„ä»£ç†ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚<br>2. é¢„è®­ç»ƒç¼–ç å™¨ï¼Œç‰¹åˆ«æ˜¯DINOv2ï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨å°‘é‡æ•°æ®æ—¶ä»ç„¶æœ‰æ•ˆã€‚<br>3. ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¼–ç å™¨åœ¨å¤„ç†æ›´çœŸå®ä¸–ç•Œçš„å›¾åƒæ—¶è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨æ—¶éœ€è¦ä»”ç»†è€ƒè™‘å›¾åƒå¤§å°å’Œæ¯”ä¾‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒç¼–ç å™¨åœ¨ç°ä»£è§†é¢‘æ¸¸æˆçš„æ¨¡ä»¿å­¦ä¹ ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¿™äº›ç¼–ç å™¨æ¥è®­ç»ƒä»£ç†ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†å›¾åƒå¤§å°å’Œæ¯”ä¾‹å¯¹é¢„è®­ç»ƒç¼–ç å™¨æ€§èƒ½çš„é‡è¦æ€§ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Predicting Outcomes in Video Games with Long Short Term Memory Networks</td>
      <td>Forecasting winners in E-sports with real-time analytics has the potential to<br>further engage audiences watching major tournament events. However, making such<br>real-time predictions is challenging due to unpredictable variables within the<br>game involving diverse player strategies and decision-making. Our work attempts<br>to enhance audience engagement within video game tournaments by introducing a<br>real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)<br>based approach enables efficient predictions of win-lose outcomes by only using<br>the health indicator of each player as a time series. As a proof of concept, we<br>evaluate our model's performance within a classic, two-player arcade game,<br>Super Street Fighter II Turbo. We also benchmark our method against state of<br>the art methods for time series forecasting; i.e. Transformer models found in<br>large language models (LLMs). Finally, we open-source our data set and code in<br>hopes of furthering work in predictive analysis for arcade games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œé¢„æµ‹ç”µå­ç«æŠ€æ¯”èµ›ç»“æœ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç”µå­ç«æŠ€ï¼ˆEsportsï¼‰çš„æ—¥ç›Šæµè¡Œï¼Œè§‚ä¼—å¯¹äºå®æ—¶æ¯”èµ›ç»“æœçš„é¢„æµ‹äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚ç„¶è€Œï¼Œç”±äºæ¸¸æˆä¸­çš„å˜é‡ä¼—å¤šï¼ŒåŒ…æ‹¬ç©å®¶ç­–ç•¥å’Œå†³ç­–çš„ä¸ç¡®å®šæ€§ï¼Œå®æ—¶é¢„æµ‹æ¯”èµ›ç»“æœä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§å®æ—¶é¢„æµ‹æ–¹æ³•æ¥å¢å¼ºè§‚ä¼—åœ¨ç”µå­æ¸¸æˆé”¦æ ‡èµ›ä¸­çš„å‚ä¸åº¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰è¿›è¡Œå®æ—¶é¢„æµ‹<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLSTMçš„å®æ—¶é¢„æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¯ä¸ªç©å®¶çš„å¥åº·æŒ‡ç¤ºå™¨ä½œä¸ºæ—¶é—´åºåˆ—æ¥é¢„æµ‹èƒœè´Ÿç»“æœã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶æ•æ‰æ¸¸æˆä¸­çš„åŠ¨æ€å˜åŒ–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨ç»å…¸çš„åŒäººè¡—æœºæ¸¸æˆã€Šè¶…çº§è¡—å¤´éœ¸ç‹II Turboã€‹ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½<br>ä¸ºäº†éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨ç»å…¸çš„åŒäººè¡—æœºæ¸¸æˆã€Šè¶…çº§è¡—å¤´éœ¸ç‹II Turboã€‹ä¸­è¯„ä¼°äº†æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æç©å®¶å¥åº·æŒ‡ç¤ºå™¨çš„å˜åŒ–ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¯”èµ›çš„ä¸åŒé˜¶æ®µè¿›è¡Œå®æ—¶é¢„æµ‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„Transformeræ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•<br>ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡å°†LSTMæ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„Transformeræ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨é¢„æµ‹æ¯”èµ›ç»“æœæ–¹é¢è¡¨ç°å‡ºäº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨é¢„æµ‹æ¯”èµ›ç»“æœæ–¹é¢è¡¨ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚åœ¨æ¯”èµ›çš„ä¸åŒé˜¶æ®µï¼ˆ25%ï¼Œ75%ï¼Œ95%ï¼‰ï¼ŒLSTMæ¨¡å‹çš„ROC-AUCåˆ†æ•°å‡é«˜äºTransformeræ¨¡å‹ã€‚æ­¤å¤–ï¼ŒLSTMæ¨¡å‹çš„è®­ç»ƒæ—¶é—´ä¹Ÿç›¸å¯¹è¾ƒçŸ­ï¼Œæ›´é€‚åˆå®æ—¶é¢„æµ‹åœºæ™¯ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºLSTMçš„å®æ—¶é¢„æµ‹æ–¹æ³•ä¸ºç”µå­ç«æŠ€æ¯”èµ›ç»“æœçš„é¢„æµ‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç”µå­ç«æŠ€æ¸¸æˆï¼Œå¹¶æœ‰åŠ©äºæé«˜è§‚ä¼—åœ¨æ¯”èµ›ä¸­çš„å‚ä¸åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€æºäº†æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›äº†è¿›ä¸€æ­¥ç ”ç©¶çš„ä¾¿åˆ©ã€‚</td>
    </tr>
    <tr>
      <th>87</th>
      <td>EmoLLM: Multimodal Emotional Understanding Meets Large Language Models</td>
      <td>Multi-modal large language models (MLLMs) have achieved remarkable<br>performance on objective multimodal perception tasks, but their ability to<br>interpret subjective, emotionally nuanced multimodal content remains largely<br>unexplored. Thus, it impedes their ability to effectively understand and react<br>to the intricate emotions expressed by humans through multimodal media. To<br>bridge this gap, we introduce EmoBench, the first comprehensive benchmark<br>designed specifically to evaluate the emotional capabilities of MLLMs across<br>five popular emotional tasks, using a diverse dataset of 287k images and videos<br>paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a<br>novel model for multimodal emotional understanding, incorporating with two core<br>techniques. 1) Multi-perspective Visual Projection, it captures diverse<br>emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it<br>guides MLLMs to reason about emotions in the correct direction. Experimental<br>results demonstrate that EmoLLM significantly elevates multimodal emotional<br>understanding performance, with an average improvement of 12.1% across multiple<br>foundation models on EmoBench. Our work contributes to the advancement of MLLMs<br>by facilitating a deeper and more nuanced comprehension of intricate human<br>emotions, paving the way for the development of artificial emotional<br>intelligence capabilities with wide-ranging applications in areas such as<br>human-computer interaction, mental health support, and empathetic AI systems.<br>Code, data, and model will be released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | EmoLLMï¼šå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç›®æ ‡å¤šæ¨¡æ€æ„ŸçŸ¥ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬åœ¨è§£é‡Šä¸»è§‚ã€æƒ…æ„Ÿä¸°å¯Œçš„å¤šæ¨¡æ€å†…å®¹æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™é˜»ç¢äº†å®ƒä»¬æœ‰æ•ˆåœ°ç†è§£å’Œååº”äººç±»é€šè¿‡å¤šæ¨¡æ€åª’ä½“è¡¨è¾¾çš„æƒ…æ„Ÿã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†EmoBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsåœ¨äº”ä¸ªæµè¡Œæƒ…æ„Ÿä»»åŠ¡ä¸­çš„æƒ…æ„Ÿèƒ½åŠ›çš„å…¨é¢åŸºå‡†ï¼Œä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«287kå›¾åƒå’Œè§†é¢‘ä»¥åŠç›¸åº”æ–‡æœ¬æŒ‡ä»¤çš„å¤šæ ·åŒ–æ•°æ®é›†ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†EmoLLMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„æ–°å‹æ¨¡å‹ï¼Œç»“åˆäº†ä¸¤ç§æ ¸å¿ƒæŠ€æœ¯ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šè§†è§’è§†è§‰æŠ•å½±<br>å®ƒä»å¤šä¸ªè§†è§’æ•è·è§†è§‰æ•°æ®ä¸­çš„å¤šæ ·åŒ–æƒ…æ„Ÿçº¿ç´¢ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šEmoPrompt<br>å®ƒå¼•å¯¼MLLMsåœ¨æ­£ç¡®çš„æ–¹å‘ä¸Šæ¨ç†æƒ…æ„Ÿã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒEmoLLMæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£æ€§èƒ½ï¼Œåœ¨EmoBenchä¸Šå¤šä¸ªåŸºç¡€æ¨¡å‹å¹³å‡æé«˜äº†12.1%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„EmoBenchåŸºå‡†å’ŒEmoLLMæ¨¡å‹ä¸ºMLLMsåœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨åŠ¨MLLMsåœ¨æƒ…æ„Ÿæ™ºèƒ½é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>85</th>
      <td>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</td>
      <td>With ChatGPT-like large language models (LLM) prevailing in the community,<br>how to evaluate the ability of LLMs is an open question. Existing evaluation<br>methods suffer from following shortcomings: (1) constrained evaluation<br>abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that<br>task-based evaluation, where LLM agents complete tasks in a simulated<br>environment, is a one-for-all solution to solve above problems. We present<br>AgentSims, an easy-to-use infrastructure for researchers from all disciplines<br>to test the specific capacities they are interested in. Researchers can build<br>their evaluation tasks by adding agents and buildings on an interactive GUI or<br>deploy and test new support mechanisms, i.e. memory, planning and tool-use<br>systems, by a few lines of codes. Our demo is available at<br>https://agentsims.com .</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AgentSimsï¼šå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¼€æ”¾æºä»£ç æ²™ç›’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¤¾åŒºä¸­çš„æ™®åŠï¼Œå¦‚ä½•è¯„ä¼°LLMçš„èƒ½åŠ›æˆä¸ºä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š<br>1. è¯„ä¼°èƒ½åŠ›å—é™ï¼šå¤§å¤šæ•°ä»»åŠ¡é‡‡ç”¨å•è½®é—®ç­”æ ¼å¼ï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMçš„å„ç§èƒ½åŠ›ã€‚<br>2. åŸºå‡†æ˜“å—æ”»å‡»ï¼šç”±äºLLMå…·æœ‰å¤§é‡çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œæµ‹è¯•é›†å®¹æ˜“æ— æ„ä¸­æ··å…¥è®­ç»ƒé›†ã€‚<br>3. æŒ‡æ ‡ä¸å®¢è§‚ï¼šç°æœ‰çš„å¼€æ”¾å¼é—®ç­”æŒ‡æ ‡æ¶‰åŠè‡ªåŠ¨æŒ‡æ ‡å’Œä¸»è§‚æŒ‡æ ‡ï¼Œæ— æ³•å®¢è§‚è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†åŸºäºä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå³LLMä»£ç†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®Œæˆä»»åŠ¡æ¥è¯æ˜å…¶èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†AgentSimsï¼Œä¸€ä¸ªæ˜“äºä½¿ç”¨çš„è¯„ä¼°LLMèƒ½åŠ›çš„å¹³å°ã€‚AgentSimså…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š<br>1. å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ï¼šå…è®¸ç”¨æˆ·ç»„åˆä¸åŒçš„è®¡åˆ’ã€è®°å¿†å’Œä½¿ç”¨å·¥å…·ç³»ç»Ÿï¼Œç ”ç©¶å„ç§ç³»ç»Ÿè®¾è®¡çš„å½±å“å’Œæœ‰æ•ˆæ€§ã€‚<br>2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼šä¸ºåœ°å›¾è®¾è®¡å’Œä»£ç†åˆ›å»ºæä¾›äº¤äº’å¼UIï¼Œé™ä½éä¸“ä¸šäººå£«çš„å…¥é—¨é—¨æ§›ã€‚<br>3. æ ‡å‡†åŒ–å®ç°ï¼šç¡®ä¿å®éªŒç»“æœçš„å†ç°æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å±•ç¤ºäº†AgentSimsåœ¨è¯„ä¼°LLMèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š<br>1. è¯„ä¼°LLMçš„ç¤¾ä¼šèƒ½åŠ›ï¼Œå¦‚å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰ã€‚<br>2. è¯„ä¼°LLMçš„é•¿æœŸè§„åˆ’å’Œç»„ç»‡èƒ½åŠ›ï¼Œå¦‚æ‹…ä»»å¸‚é•¿æˆ–å…¬å¸æ€»è£ã€‚<br>3. ä½œä¸ºæ•°æ®ç”Ÿæˆå¹³å°ï¼Œç”¨äºæ•°æ®æ ‡æ³¨å’Œå¢å¼ºã€‚<br>4. ä¸ºç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›å¯æ§çš„åˆæ­¥å®éªŒç¯å¢ƒã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AgentSimsä¸ºLLMè¯„ä¼°æä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„æ²™ç›’å¹³å°ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br>1. åŸºäºä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚<br>2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼Œé™ä½éä¸“ä¸šäººå£«çš„å…¥é—¨é—¨æ§›ã€‚<br>3. æ ‡å‡†åŒ–å®ç°ï¼Œç¡®ä¿å®éªŒç»“æœçš„å†ç°æ€§ã€‚<br>4. å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ï¼Œæ–¹ä¾¿ç”¨æˆ·ç ”ç©¶å’Œå¼€å‘æ–°çš„æ”¯æŒç³»ç»Ÿã€‚</td>
    </tr>
    <tr>
      <th>155</th>
      <td>Benchmarking End-to-End Behavioural Cloning on Video Games</td>
      <td>Behavioural cloning, where a computer is taught to perform a task based on<br>demonstrations, has been successfully applied to various video games and<br>robotics tasks, with and without reinforcement learning. This also includes<br>end-to-end approaches, where a computer plays a video game like humans do: by<br>looking at the image displayed on the screen, and sending keystrokes to the<br>game. As a general approach to playing video games, this has many inviting<br>properties: no need for specialized modifications to the game, no lengthy<br>training sessions and the ability to re-use the same tools across different<br>games. However, related work includes game-specific engineering to achieve the<br>results. We take a step towards a general approach and study the general<br>applicability of behavioural cloning on twelve video games, including six<br>modern video games (published after 2010), by using human demonstrations as<br>training data. Our results show that these agents cannot match humans in raw<br>performance but do learn basic dynamics and rules. We also demonstrate how the<br>quality of the data matters, and how recording data from humans is subject to a<br>state-action mismatch, due to human reflexes.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¡Œä¸ºå…‹éš†æ˜¯ä¸€ç§åŸºäºäººç±»æ¼”ç¤ºæ¥è®­ç»ƒè®¡ç®—æœºæ‰§è¡Œä»»åŠ¡çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚å®ƒå·²è¢«æˆåŠŸåº”ç”¨äºå„ç§è§†é¢‘æ¸¸æˆå’Œæœºå™¨äººä»»åŠ¡ï¼ŒåŒ…æ‹¬ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå…¶ä¸­è®¡ç®—æœºé€šè¿‡è§‚å¯Ÿå±å¹•ä¸Šçš„å›¾åƒå¹¶å‘é€æŒ‰é”®æ¥åƒäººç±»ä¸€æ ·ç©æ¸¸æˆã€‚è¿™ç§æ–¹æ³•å…·æœ‰è®¸å¤šå¸å¼•äººçš„ç‰¹æ€§ï¼Œä¾‹å¦‚æ— éœ€å¯¹æ¸¸æˆè¿›è¡Œç‰¹æ®Šä¿®æ”¹ã€æ— éœ€é•¿æ—¶é—´è®­ç»ƒä»¥åŠèƒ½å¤Ÿåœ¨ä¸åŒæ¸¸æˆä¹‹é—´é‡ç”¨ç›¸åŒå·¥å…·ã€‚ç„¶è€Œï¼Œç›¸å…³å·¥ä½œé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¸¸æˆè¿›è¡Œå·¥ç¨‹åŒ–æ‰èƒ½å–å¾—æˆæœã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶è¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„é€šç”¨æ€§ï¼Œå¹¶ä½¿ç”¨äººç±»æ¼”ç¤ºä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œåœ¨åäºŒæ¬¾è§†é¢‘æ¸¸æˆï¼ˆåŒ…æ‹¬å…­æ¬¾ç°ä»£è§†é¢‘æ¸¸æˆï¼‰ä¸Šè¿›è¡Œäº†ç ”ç©¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å­¦ä¹ äººç±»ç©å®¶çš„è¡Œä¸ºï¼Œå¹¶ä½¿ç”¨äººç±»æ¼”ç¤ºæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æ¨¡å‹é€šè¿‡è§‚å¯Ÿå±å¹•å›¾åƒæ¥é¢„æµ‹äººç±»ç©å®¶çš„åŠ¨ä½œï¼Œå¹¶ä½¿ç”¨è¿™äº›é¢„æµ‹æ¥æ§åˆ¶æ¸¸æˆã€‚æœ¬æ–‡è¿˜ç ”ç©¶äº†æ•°æ®è´¨é‡å’Œæ•°é‡å¯¹è¡Œä¸ºå…‹éš†æ€§èƒ½çš„å½±å“ï¼Œä»¥åŠäººç±»ååº”æ—¶é—´å»¶è¿Ÿå¯¹æ•°æ®è´¨é‡çš„å½±å“ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œè¡Œä¸ºå…‹éš†ä»£ç†å¯ä»¥å­¦ä¹ æ¸¸æˆçš„åŸºæœ¬åŠ¨æ€å’Œè§„åˆ™ï¼Œä½†é€šå¸¸åªèƒ½è¾¾åˆ°äººç±»ç©å®¶æ€§èƒ½çš„ä¸€å°éƒ¨åˆ†ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¦‚éšæœºä»£ç†ã€‚æœ¬æ–‡è¿˜å‘ç°ï¼Œå½“åªæœ‰å°‘é‡æ•°æ®å¯ç”¨æ—¶ï¼Œæ•°æ®æ•°é‡å¯¹ç»“æœçš„å½±å“è¾ƒå°ï¼Œè€Œè°ƒæ•´äººç±»ååº”æ—¶é—´å»¶è¿Ÿå¯ä»¥æé«˜æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¡Œä¸ºå…‹éš†åœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨å…·æœ‰æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºçš„å»ºè®®åŒ…æ‹¬ä½¿ç”¨é«˜è´¨é‡æ•°æ®ã€è°ƒæ•´äººç±»ååº”æ—¶é—´å»¶è¿Ÿä»¥åŠæ¢ç´¢å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜è¡Œä¸ºå…‹éš†çš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>162</th>
      <td>Benchmarking Cognitive Abilities of the Brain with Computer Games</td>
      <td>Most of the players have experienced the feeling of temporarily losing their<br>character in a given gameplay situation when they cannot control the character,<br>simply because they temporarily cannot see it. The main reasons for this<br>feeling may be due to the interplay of the following factors: (1) the visual<br>complexity of the game is unexpectedly increased compared with the previous<br>time period as more and more game objects and effects are rendered on the<br>display; (2) and/or the game is lagging; (3) and finally, it is also possible<br>that the players have no sufficient experience with controlling the character.<br>This paper focuses on the first reason. We have developed a benchmark program<br>which allows its user to experience the feeling of losing character. While the<br>user can control the character well the benchmark program will increase the<br>visual complexity of the display. Otherwise, if the user lost the character<br>then the program will decrease the complexity until the user will find the<br>character again, and so on. The complexity is measured based on the number of<br>changed pixels between two consecutive display images. Our measurements show<br>that the average of bit per second values of losing and finding pairs describes<br>the user well. The final goal of this research is to further develop our<br>benchmark to a standard psychological test.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆä¸­çš„è®¤çŸ¥èƒ½åŠ›è¯„ä¼°ï¼šBrainB Test Series 6<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶æœ‰æ—¶ä¼šå› è§†è§‰å¤æ‚æ€§çš„å¢åŠ è€Œæš‚æ—¶å¤±å»å¯¹è§’è‰²çš„æ§åˆ¶ã€‚è¿™ç§ç°è±¡å¯èƒ½ç”±å¤šç§å› ç´ å¼•èµ·ï¼ŒåŒ…æ‹¬æ¸¸æˆè§†è§‰å¤æ‚æ€§çš„çªç„¶å¢åŠ ã€æ¸¸æˆå»¶è¿Ÿä»¥åŠç©å®¶å¯¹è§’è‰²æ§åˆ¶ç»éªŒçš„ä¸è¶³ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨è§†è§‰å¤æ‚æ€§å¢åŠ è¿™ä¸€å› ç´ ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåä¸º BrainB Test Series 6 çš„åŸºå‡†ç¨‹åºï¼Œç”¨äºæ¨¡æ‹Ÿç©å®¶å¤±å»è§’è‰²çš„æ„Ÿè§‰ï¼Œå¹¶è¯„ä¼°å…¶è®¤çŸ¥èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šBrainB Test Series 6 åŸºå‡†ç¨‹åº<br>è¯¥ç¨‹åºé€šè¿‡æ§åˆ¶å±å¹•ä¸Šç§»åŠ¨çš„æ–¹å—çš„è§†è§‰å¤æ‚æ€§æ¥æ¨¡æ‹Ÿç©å®¶å¤±å»è§’è‰²çš„æ„Ÿè§‰ã€‚å½“ç©å®¶èƒ½å¤Ÿå¾ˆå¥½åœ°æ§åˆ¶è§’è‰²æ—¶ï¼Œç¨‹åºä¼šå¢åŠ å±å¹•çš„è§†è§‰å¤æ‚æ€§ï¼›å¦‚æœç©å®¶å¤±å»äº†è§’è‰²ï¼Œç¨‹åºä¼šé™ä½å¤æ‚æ€§ï¼Œç›´åˆ°ç©å®¶å†æ¬¡æ‰¾åˆ°è§’è‰²ã€‚ç¨‹åºçš„å¤æ‚æ€§åŸºäºè¿ç»­æ˜¾ç¤ºå›¾åƒä¹‹é—´å˜åŒ–çš„åƒç´ æ•°é‡æ¥è¡¡é‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¤çŸ¥èƒ½åŠ›è¯„ä¼°<br>é€šè¿‡æµ‹é‡ç©å®¶åœ¨å¤±å»å’Œæ‰¾åˆ°è§’è‰²æ—¶å±å¹•å¤æ‚æ€§çš„å¹³å‡å€¼ï¼Œå¯ä»¥è¯„ä¼°ç©å®¶çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤±å»å’Œæ‰¾åˆ°è§’è‰²æ—¶å±å¹•å¤æ‚æ€§çš„å¹³å‡å€¼å¯ä»¥å¾ˆå¥½åœ°æè¿°ç©å®¶ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ç ”ç©¶äººå‘˜å¯¹ BrainB Test Series 6 è¿›è¡Œäº†åˆæ­¥æµ‹è¯•ï¼Œå¹¶å‘ç°ç©å®¶åœ¨å±å¹•å¤æ‚æ€§è¾ƒé«˜æ—¶æ›´å®¹æ˜“å¤±å»è§’è‰²ï¼Œè€Œåœ¨å±å¹•å¤æ‚æ€§è¾ƒä½æ—¶æ›´å®¹æ˜“æ‰¾åˆ°è§’è‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å‘ç°ï¼Œé€šè¿‡è°ƒæ•´é¼ æ ‡è®¾ç½®å’Œé¢œè‰²æ–¹æ¡ˆï¼Œå¯ä»¥æé«˜æµ‹è¯•çš„å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ BrainB Test Series 6 åŸºå‡†ç¨‹åºä¸ºè¯„ä¼°ç©å®¶çš„è®¤çŸ¥èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥ç”¨äºç”µå­ç«æŠ€äººæ‰é€‰æ‹”ã€è®¤çŸ¥èƒ½åŠ›ç ”ç©¶ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†æµ‹è¯•ç¯å¢ƒæ ‡å‡†åŒ–å’Œæµ‹è¯•ç»“æœåˆ†æçš„é‡è¦æ€§ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>126</th>
      <td>Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests</td>
      <td>We examine three evaluation paradigms: large question-answering benchmarks<br>(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and<br>cognitive tests (e.g., for working memory or theory of mind). First, we<br>investigate which of the former two-benchmarks or games-is most effective at<br>discriminating LLMs of varying quality. Then, inspired by human cognitive<br>assessments, we compile a suite of targeted tests that measure cognitive<br>abilities deemed essential for effective language use, and we investigate their<br>correlation with model performance in benchmarks and games. Our analyses reveal<br>that interactive games are superior to standard benchmarks in discriminating<br>models. Causal and logical reasoning correlate with both static and interactive<br>tests, while differences emerge regarding core executive functions and<br>social/emotional skills, which correlate more with games. We advocate the<br>development of new interactive benchmarks and targeted cognitive tasks inspired<br>by assessing human abilities but designed specifically for LLMs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡åŸºå‡†æµ‹è¯•ã€æ¸¸æˆå’Œè®¤çŸ¥æµ‹è¯•æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§è§„æ¨¡çš„é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œä½†è¿™äº›æµ‹è¯•å¾€å¾€å­˜åœ¨æ•°æ®æ±¡æŸ“å’Œæ¨¡å‹å¯¹æç¤ºæ ¼å¼æ•æ„Ÿçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æµ‹è¯•å¿½ç•¥äº†å®é™…çš„è¯­è¨€ä½¿ç”¨åœºæ™¯ï¼Œå³åœ¨ç¤¾ä¼šå’Œä»»åŠ¡å¯¼å‘çš„å¤šè½®äº¤äº’ä¸­ä½¿ç”¨è¯­è¨€ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢æ›´æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°ç†è§£LLMsçš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸‰ç§è¯„ä¼°èŒƒå¼ï¼šå¤§è§„æ¨¡é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMLUå’ŒBBHï¼‰ã€äº¤äº’å¼æ¸¸æˆï¼ˆå¦‚ä¿¡å·æ¸¸æˆæˆ–ç¦å¿Œæ¸¸æˆï¼‰å’Œè®¤çŸ¥æµ‹è¯•ï¼ˆå¦‚å·¥ä½œè®°å¿†æˆ–å¿ƒæ™ºç†è®ºæµ‹è¯•ï¼‰ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›èŒƒå¼ï¼Œç ”ç©¶å›¢é˜Ÿæ—¨åœ¨æ‰¾å‡ºæœ€æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå—äººç±»è®¤çŸ¥è¯„ä¼°çš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹æ€§çš„æµ‹è¯•ï¼Œä»¥è¡¡é‡å¯¹æœ‰æ•ˆè¯­è¨€ä½¿ç”¨è‡³å…³é‡è¦çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMsåœ¨å› æœæ¨ç†ã€é€»è¾‘æ¨ç†ã€å·¥ä½œè®°å¿†ã€æƒ…ç»ªæ™ºåŠ›ç­‰æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶åˆ†æè¿™äº›èƒ½åŠ›ä¸æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•å’Œæ¸¸æˆä¸­çš„è¡¨ç°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼æ¸¸æˆåœ¨åŒºåˆ†ä¸åŒè´¨é‡çš„LLMsæ–¹é¢ä¼˜äºä¼ ç»Ÿçš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚å› æœæ¨ç†å’Œé€»è¾‘æ¨ç†ä¸é™æ€å’Œäº¤äº’å¼æµ‹è¯•éƒ½ç›¸å…³ï¼Œè€Œæ ¸å¿ƒæ‰§è¡ŒåŠŸèƒ½å’Œç¤¾äº¤/æƒ…æ„ŸæŠ€èƒ½åˆ™æ›´å¤šåœ°ä¸æ¸¸æˆç›¸å…³ã€‚ç‰¹åˆ«æ˜¯ï¼Œå·¥ä½œè®°å¿†å’Œæƒ…ç»ªæ™ºåŠ›ä»…ä¸æ¸¸æˆä¸­çš„è¡¨ç°æ˜¾è‘—ç›¸å…³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäº¤äº’å¼æ¸¸æˆæ˜¯ä¸€ç§æ›´æœ‰æ•ˆçš„è¯„ä¼°LLMsçš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ¨¡å‹åœ¨å®é™…è¯­è¨€ä½¿ç”¨åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ€§çš„è®¤çŸ¥æµ‹è¯•å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMsçš„å†…åœ¨èƒ½åŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œè¯„ä¼°æä¾›æŒ‡å¯¼ã€‚å› æ­¤ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºLLMsçš„è¯„ä¼°å’Œå¼€å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚</td>
    </tr>
    <tr>
      <th>128</th>
      <td>Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research</td>
      <td>After the great achievement of solving two-player zero-sum games, more and<br>more AI researchers focus on solving multiplayer games. To facilitate the<br>development of designing efficient learning algorithms for solving multiplayer<br>games, we propose a multiplayer game platform for solving Urban Network<br>Security Games (\textbf{UNSG}) that model real-world scenarios. That is,<br>preventing criminal activity is a highly significant responsibility assigned to<br>police officers in cities, and police officers have to allocate their limited<br>security resources to interdict the escaping criminal when a crime takes place<br>in a city. This interaction between multiple police officers and the escaping<br>criminal can be modeled as a UNSG. The variants of UNSGs can model different<br>real-world settings, e.g., whether real-time information is available or not,<br>and whether police officers can communicate or not. The main challenges of<br>solving this game include the large size of the game and the co-existence of<br>cooperation and competition. While previous efforts have been made to tackle<br>UNSGs, they have been hampered by performance and scalability issues.<br>Therefore, we propose an open-source UNSG platform (\textbf{GraphChase}) for<br>designing efficient learning algorithms for solving UNSGs. Specifically,<br>GraphChase offers a unified and flexible game environment for modeling various<br>variants of UNSGs, supporting the development, testing, and benchmarking of<br>algorithms. We believe that GraphChase not only facilitates the development of<br>efficient algorithms for solving real-world problems but also paves the way for<br>significant advancements in algorithmic development for solving general<br>multiplayer games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GraphChaseï¼šè§£å†³åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆçš„AIå¹³å°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½åœ¨è§£å†³ä¸¤äººé›¶å’Œåšå¼ˆæ–¹é¢å–å¾—å·¨å¤§æˆå°±ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜å¼€å§‹å…³æ³¨è§£å†³å¤šäººæ¸¸æˆã€‚åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆï¼ˆUNSGï¼‰ä½œä¸ºä¸€ç§æ¨¡æ‹Ÿç°å®ä¸–ç•Œåœºæ™¯çš„å¤šç©å®¶æ¸¸æˆï¼Œå¯¹äºç ”ç©¶å¤šäººåšå¼ˆç®—æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼ŒUNSGçš„è§£å†³é¢ä¸´ç€æ¸¸æˆè§„æ¨¡åºå¤§ã€åˆä½œä¸ç«äº‰å…±å­˜ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰çš„ç®—æ³•åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGraphChaseå¹³å°<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGraphChaseçš„å¼€æºUNSGå¹³å°ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ã€çµæ´»çš„æ¸¸æˆç¯å¢ƒï¼Œç”¨äºæ¨¡æ‹Ÿå„ç§UNSGå˜ä½“ï¼Œå¹¶æ”¯æŒç®—æ³•çš„å¼€å‘ã€æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ã€‚GraphChaseå¹³å°å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š<br>- **çµæ´»çš„æ¸¸æˆç¯å¢ƒ**ï¼šç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰æ¸¸æˆå‚æ•°ï¼ŒåŒ…æ‹¬å›¾ç»“æ„ã€åˆå§‹ä½ç½®ã€æ—¶é—´èŒƒå›´ç­‰ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒçš„UNSGåœºæ™¯ã€‚<br>- **å¤šç§ç®—æ³•æ”¯æŒ**ï¼šGraphChaseå¹³å°æ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚CFR-MIXã€NSG-NFSPã€NSGZeroã€Pretrained PSROå’ŒGrasperï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œç®—æ³•æ¯”è¾ƒå’Œè¯„ä¼°ã€‚<br>- **æ¨¡å—åŒ–è®¾è®¡**ï¼šGraphChaseå¹³å°é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…æ‹¬æ¸¸æˆæ¨¡å—ã€ä»£ç†æ¨¡å—å’Œæ±‚è§£å™¨æ¨¡å—ï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œè‡ªå®šä¹‰å’Œæ‰©å±•ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºå‡†æµ‹è¯•<br>æœ¬æ–‡åœ¨GraphChaseå¹³å°ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯„ä¼°äº†ç°æœ‰ç®—æ³•çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ç°æœ‰ç®—æ³•åœ¨ç®€å•åœºæ™¯ä¸‹èƒ½å¤Ÿå–å¾—åˆç†æ€§èƒ½ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸‹ä»ç„¶å­˜åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚è¿™è¡¨æ˜ï¼Œå¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„ç®—æ³•æ¥è§£å†³ç°å®ä¸–ç•Œçš„UNSGä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphChaseå¹³å°èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹ŸUNSGåœºæ™¯ï¼Œå¹¶ä¸”èƒ½å¤ŸåŠ é€Ÿç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œç°æœ‰ç®—æ³•åœ¨è§£å†³å¤æ‚åœºæ™¯å’Œå¤§è§„æ¨¡æ¸¸æˆæ—¶å­˜åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GraphChaseå¹³å°ä¸ºç ”ç©¶UNSGå’Œå¤šäººåšå¼ˆç®—æ³•æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾åœ°æ¨¡æ‹Ÿä¸åŒçš„åœºæ™¯å’Œæµ‹è¯•ä¸åŒçš„ç®—æ³•ã€‚æ­¤å¤–ï¼ŒGraphChaseå¹³å°è¿˜å¯ä»¥ä½œä¸ºå…¶ä»–å¤šäººåšå¼ˆé—®é¢˜çš„æµ‹è¯•å¹³å°ï¼Œä¾‹å¦‚å¯¹æŠ—æ€§å›¢é˜Ÿæ¸¸æˆå’Œè¿½é€ƒæ¸¸æˆã€‚<br><br>## ğŸ“š æ€»ç»“<br>GraphChaseå¹³å°ä¸ºè§£å†³åŸå¸‚ç½‘ç»œå®‰å…¨æ¸¸æˆæä¾›äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼Œå¹¶ä¸ºç ”ç©¶å¤šäººåšå¼ˆç®—æ³•æä¾›äº†æ–°çš„æ€è·¯ã€‚éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒGraphChaseå¹³å°æœ‰æœ›åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚</td>
    </tr>
    <tr>
      <th>129</th>
      <td>How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games</td>
      <td>The deployment of large language models (LLMs) in diverse applications<br>requires a thorough understanding of their decision-making strategies and<br>behavioral patterns. As a supplement to a recent study on the behavioral Turing<br>test, this paper presents a comprehensive analysis of five leading LLM-based<br>chatbot families as they navigate a series of behavioral economics games. By<br>benchmarking these AI chatbots, we aim to uncover and document both common and<br>distinct behavioral patterns across a range of scenarios. The findings provide<br>valuable insights into the strategic preferences of each LLM, highlighting<br>potential implications for their deployment in critical decision-making roles.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡Œä¸ºæ¨¡å¼<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œç†è§£è¿™äº›æ¨¡å‹çš„å†³ç­–ç­–ç•¥å’Œè¡Œä¸ºæ¨¡å¼å˜å¾—è‡³å…³é‡è¦ã€‚è¿™ä¸ä»…æœ‰åŠ©äºä¼˜åŒ–å®ƒä»¬åœ¨ç‰¹å®šåº”ç”¨ä¸­çš„æ€§èƒ½ï¼Œè¿˜èƒ½æ›´å¥½åœ°è¯„ä¼°å®ƒä»¬çš„å¯é æ€§å’Œå¯é¢„æµ‹æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠé‡å¤§è´£ä»»çš„æƒ…å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºä¸åŒLLMsçš„è¡Œä¸ºæ¨¡å¼çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡¨ç°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€šè¿‡è¡Œä¸ºç»æµå­¦æ¸¸æˆè¯„ä¼°LLMs<br>æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—ç»å…¸çš„è¡Œä¸ºç»æµå­¦æ¸¸æˆï¼Œå¦‚ç‹¬è£è€…æ¸¸æˆã€æœ€åé€šç‰’æ¸¸æˆã€ä¿¡ä»»æ¸¸æˆã€å…¬å…±ç‰©å“æ¸¸æˆã€ç‚¸å¼¹é£é™©æ¸¸æˆå’Œå›šå¾’å›°å¢ƒæ¸¸æˆï¼Œå¯¹äº”ç§é¢†å…ˆçš„LLM-basedèŠå¤©æœºå™¨äººè¿›è¡Œäº†å…¨é¢åˆ†æã€‚è¿™äº›æ¸¸æˆæ—¨åœ¨æµ‹è¯•LLMsåœ¨ä¿¡ä»»ã€å…¬å¹³ã€é£é™©è§„é¿ã€åˆ©ä»–ä¸»ä¹‰å’Œåˆä½œç­‰æ–¹é¢çš„è¡Œä¸ºæ¨¡å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥è¡Œä¸ºå›¾çµæµ‹è¯•å’Œåˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•<br>ä¸ºäº†è¯„ä¼°LLMsçš„è¡Œä¸ºæ¨¡å¼ï¼Œæœ¬æ–‡å¼•å…¥äº†è¡Œä¸ºå›¾çµæµ‹è¯•å’Œåˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•ã€‚è¡Œä¸ºå›¾çµæµ‹è¯•é€šè¿‡æ¯”è¾ƒLLMså’Œäººç±»çš„è¡Œä¸ºåˆ†å¸ƒï¼Œè¯„ä¼°LLMsæ˜¯å¦èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¡Œä¸ºã€‚åˆ†å¸ƒç›¸ä¼¼æ€§æµ‹è¯•åˆ™ä½¿ç”¨Wassersteinè·ç¦»æ¥è¡¡é‡LLMsçš„è¡Œä¸ºåˆ†å¸ƒä¸äººç±»è¡Œä¸ºåˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æµ‹è¯•çš„èŠå¤©æœºå™¨äººéƒ½èƒ½æˆåŠŸåœ°æ•æ‰åˆ°ç‰¹å®šçš„äººç±»è¡Œä¸ºæ¨¡å¼ï¼Œå¯¼è‡´å†³ç­–åˆ†å¸ƒé«˜åº¦é›†ä¸­ã€‚å°½ç®¡æ——èˆ°èŠå¤©æœºå™¨äººè¡¨ç°å‡ºæ˜¾è‘—çš„é€šè¿‡å›¾çµæµ‹è¯•çš„æ¦‚ç‡ï¼Œä½†å®ƒä»¬åªèƒ½äº§ç”Ÿä¸äººç±»ç›¸ä¼¼çš„è¡Œä¸ºåˆ†å¸ƒã€‚ä¸äººç±»ç›¸æ¯”ï¼ŒèŠå¤©æœºå™¨äººåœ¨æ”¶ç›Šåå¥½ä¸Šæ›´å¼ºè°ƒå…¬å¹³æ€§ã€‚èŠå¤©æœºå™¨äººåœ¨ä¸åŒæ¸¸æˆä¸­çš„æ”¶ç›Šåå¥½å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚ä¸åŒçš„èŠå¤©æœºå™¨äººåœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ä»¥é€šè¿‡å›¾çµæµ‹è¯•ç»“æœã€æ”¶ç›Šåå¥½å’Œè¡Œä¸ºçš„è¿è´¯æ€§è¿›ä¸€æ­¥åŒºåˆ†ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºç†è§£LLMsçš„è¡Œä¸ºæ¨¡å¼æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥åœ¨AIè¡Œä¸ºç§‘å­¦é¢†åŸŸçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¨¡ä»¿äººç±»è¡Œä¸ºæ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥æé«˜LLMsåœ¨è¡Œä¸ºç»æµå­¦æ¸¸æˆä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨ä¸åŒæ¸¸æˆä¸­çš„è¡Œä¸ºä¸ä¸€è‡´æ€§ä¹Ÿè¡¨æ˜ï¼Œéœ€è¦å¼€å‘æ›´é€šç”¨çš„åå¥½å’Œç›®æ ‡ï¼Œä»¥ä¾¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”å„ç§æƒ…å¢ƒã€‚</td>
    </tr>
    <tr>
      <th>130</th>
      <td>AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games</td>
      <td>Decision-making in large-scale games is an essential research area in<br>artificial intelligence (AI) with significant real-world impact. However, the<br>limited access to realistic large-scale game environments has hindered research<br>progress in this area. In this paper, we present AuctionNet, a benchmark for<br>bid decision-making in large-scale ad auctions derived from a real-world online<br>advertising platform. AuctionNet is composed of three parts: an ad auction<br>environment, a pre-generated dataset based on the environment, and performance<br>evaluations of several baseline bid decision-making algorithms. More<br>specifically, the environment effectively replicates the integrity and<br>complexity of real-world ad auctions through the interaction of several<br>modules: the ad opportunity generation module employs deep generative networks<br>to bridge the gap between simulated and real-world data while mitigating the<br>risk of sensitive data exposure; the bidding module implements diverse<br>auto-bidding agents trained with different decision-making algorithms; and the<br>auction module is anchored in the classic Generalized Second Price (GSP)<br>auction but also allows for customization of auction mechanisms as needed. To<br>facilitate research and provide insights into the environment, we have also<br>pre-generated a substantial dataset based on the environment. The dataset<br>contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over<br>500 million auction records. Performance evaluations of baseline algorithms<br>such as linear programming, reinforcement learning, and generative models for<br>bid decision-making are also presented as a part of AuctionNet. We believe that<br>AuctionNet is applicable not only to research on bid decision-making in ad<br>auctions but also to the general area of decision-making in large-scale games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AuctionNetï¼šå¤§å‹æ¸¸æˆä¸­å†³ç­–åˆ¶å®šçš„æ–°åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹æ¸¸æˆä¸­çš„å†³ç­–åˆ¶å®šæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œå®ƒå¯¹ç°å®ä¸–ç•Œæœ‰ç€æ·±è¿œçš„å½±å“ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯¹çœŸå®å¤§å‹æ¸¸æˆç¯å¢ƒçš„è®¿é—®ï¼Œè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿›å±•å—åˆ°äº†é™åˆ¶ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿç¯å¢ƒå¾€å¾€ä¸çœŸå®ç¯å¢ƒå­˜åœ¨è¾ƒå¤§å·®è·ï¼Œæ— æ³•å®Œå…¨åæ˜ ç°å®ä¸–ç•Œä¸­çš„å¤šæ™ºèƒ½ä½“åŠ¨æ€ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuctionNetç¯å¢ƒ<br>AuctionNetæ˜¯ä¸€ä¸ªåŸºäºçœŸå®åœ¨çº¿å¹¿å‘Šå¹³å°çš„å¤§å‹å¹¿å‘Šæ‹å–å†³ç­–åˆ¶å®šåŸºå‡†ã€‚å®ƒç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šå¹¿å‘Šæ‹å–ç¯å¢ƒã€åŸºäºç¯å¢ƒçš„é¢„ç”Ÿæˆæ•°æ®é›†ä»¥åŠå‡ ä¸ªåŸºçº¿æŠ•æ ‡å†³ç­–åˆ¶å®šç®—æ³•çš„æ€§èƒ½è¯„ä¼°ã€‚è¯¥ç¯å¢ƒé€šè¿‡å‡ ä¸ªæ¨¡å—çš„äº¤äº’æœ‰æ•ˆåœ°å¤åˆ¶äº†ç°å®ä¸–ç•Œå¹¿å‘Šæ‹å–çš„å®Œæ•´æ€§å’Œå¤æ‚æ€§ï¼š<br>- å¹¿å‘Šæœºä¼šç”Ÿæˆæ¨¡å—ï¼šä½¿ç”¨æ·±åº¦ç”Ÿæˆç½‘ç»œæ¥å¼¥åˆæ¨¡æ‹Ÿæ•°æ®å’Œç°å®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„å·®è·ï¼ŒåŒæ—¶é™ä½æ•æ„Ÿæ•°æ®æ³„éœ²çš„é£é™©ã€‚<br>- æŠ•æ ‡æ¨¡å—ï¼šå®ç°äº†å¤šç§è‡ªåŠ¨æŠ•æ ‡ä»£ç†ï¼Œè¿™äº›ä»£ç†ä½¿ç”¨ä¸åŒçš„å†³ç­–åˆ¶å®šç®—æ³•è¿›è¡Œè®­ç»ƒã€‚<br>- æ‹å–æ¨¡å—ï¼šä»¥ç»å…¸çš„å¹¿ä¹‰ç¬¬äºŒä»·æ ¼ï¼ˆGSPï¼‰æ‹å–ä¸ºåŸºç¡€ï¼Œä½†ä¹Ÿå…è®¸æ ¹æ®éœ€è¦å®šåˆ¶æ‹å–æœºåˆ¶ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé¢„ç”Ÿæˆæ•°æ®é›†<br>ä¸ºäº†ä¿ƒè¿›ç ”ç©¶å’Œæä¾›å¯¹ç¯å¢ƒçš„æ´å¯Ÿï¼ŒAuctionNetè¿˜åŸºäºç¯å¢ƒé¢„ç”Ÿæˆäº†ä¸€ä¸ªåºå¤§çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«1000ä¸‡ä¸ªå¹¿å‘Šæœºä¼šã€48ç§ä¸åŒçš„è‡ªåŠ¨æŠ•æ ‡ä»£ç†å’Œè¶…è¿‡5äº¿æ¡æ‹å–è®°å½•ã€‚è¿™äº›æ•°æ®å¯ä»¥ç”¨äºå»ºæ¨¡ç¯å¢ƒï¼Œå¹¶æœ‰æ•ˆåœ°è®­ç»ƒè‡ªåŠ¨æŠ•æ ‡ä»£ç†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>AuctionNetå·²ç»ä¸ºNeurIPS 2024â€œå¤§å‹æ‹å–ä¸­çš„è‡ªåŠ¨æŠ•æ ‡â€ç«èµ›æä¾›äº†åŠ¨åŠ›ï¼Œä¸ºæ¥è‡ªä¸–ç•Œå„åœ°çš„1500å¤šæ”¯é˜Ÿä¼æä¾›äº†ç«èµ›ç¯å¢ƒã€‚è¯¥ç«èµ›è§£å†³äº†åœ¨ä¸ç¡®å®šå’Œç«äº‰ç¯å¢ƒä¸­è¿›è¡Œé«˜é¢‘æŠ•æ ‡å†³ç­–åˆ¶å®šçš„å…³é”®é—®é¢˜ï¼Œå¹¶æŒç»­äº†4ä¸ªæœˆã€‚AuctionNetæä¾›çš„å¹¿å‘Šæ‹å–ç¯å¢ƒã€æ•°æ®é›†å’ŒåŸºçº¿æŠ•æ ‡å†³ç­–åˆ¶å®šç®—æ³•ä¸ºå‚èµ›è€…æä¾›äº†å‡†ç¡®å’Œå…¬å¹³çš„æ€§èƒ½è¯„ä¼°ï¼Œå¹¶æ¿€å‘äº†ä»–ä»¬çš„åˆ›é€ åŠ›ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯å‘å±•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AuctionNetä¸ä»…ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç ”ç©¶å¤§å‹æ‹å–ä¸­è‡ªåŠ¨æŠ•æ ‡ç®—æ³•çš„æœºä¼šï¼Œè¿˜å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…åœ¨åšå¼ˆè®ºã€å¼ºåŒ–å­¦ä¹ ã€ç”Ÿæˆæ¨¡å‹ã€è¿è¥ä¼˜åŒ–ç­‰é¢†åŸŸè§£å†³å¹¿æ³›çš„å†³ç­–åˆ¶å®šç ”ç©¶é—®é¢˜ã€‚æ­¤å¤–ï¼ŒAuctionNetè¿˜å¯ä»¥ç”¨äºç ”ç©¶å…¶ä»–å¤§å‹æ¸¸æˆä¸­çš„å†³ç­–åˆ¶å®šé—®é¢˜ï¼Œä¾‹å¦‚åœ¨çº¿å¹¿å‘Šã€æ¨èç³»ç»Ÿã€èµ„æºåˆ†é…ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>131</th>
      <td>From Code to Play: Benchmarking Program Search for Games Using Large Language Models</td>
      <td>Large language models (LLMs) have shown impressive capabilities in generating<br>program code, opening exciting opportunities for applying program synthesis to<br>games. In this work, we explore the potential of LLMs to directly synthesize<br>usable code for a wide range of gaming applications, focusing on two<br>programming languages, Python and Java. We use an evolutionary hill-climbing<br>algorithm, where the mutations and seeds of the initial programs are controlled<br>by LLMs. For Python, the framework covers various game-related tasks, including<br>five miniature versions of Atari games, ten levels of Baba is You, an<br>environment inspired by Asteroids, and a maze generation task. For Java, the<br>framework contains 12 games from the TAG tabletop games framework. Across 29<br>tasks, we evaluated 12 language models for Python and 8 for Java. Our findings<br>suggest that the performance of LLMs depends more on the task than on model<br>size. While larger models generate more executable programs, these do not<br>always result in higher-quality solutions but are much more expensive. No model<br>has a clear advantage, although on any specific task, one model may be better.<br>Trying many models on a problem and using the best results across them is more<br>reliable than using just one.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ä»ä»£ç åˆ°æ¸¸æˆï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¸¸æˆç¨‹åºæœç´¢çš„åŸºå‡†æµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆç¨‹åºä»£ç æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œå°†ç¨‹åºåˆæˆåº”ç”¨äºæ¸¸æˆé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç¨‹åºåˆæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰æˆ–JSONè½¬æ¢å™¨ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨èŒƒå›´å’Œçµæ´»æ€§ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsç›´æ¥åˆæˆå¯ç”¨äºå„ç§æ¸¸æˆåº”ç”¨çš„ä»£ç çš„æ½œåŠ›ï¼Œå¹¶è¯„ä¼°å…¶åœ¨æ¸¸æˆé¢†åŸŸçš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMsçš„ç¨‹åºæœç´¢æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆé¢†åŸŸåˆæˆç¨‹åºä»£ç çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è¿›åŒ–çˆ¬å±±ç®—æ³•ï¼Œå…¶ä¸­åˆå§‹ç¨‹åºçš„çªå˜å’Œç§å­ç”±LLMsæ§åˆ¶ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨Pythonå’ŒJavaä¸¤ç§ç¼–ç¨‹è¯­è¨€ï¼Œåœ¨29ä¸ªä¸åŒçš„æ¸¸æˆä»»åŠ¡ä¸Šè¯„ä¼°äº†12ä¸ªPythonè¯­è¨€æ¨¡å‹å’Œ8ä¸ªJavaè¯­è¨€æ¨¡å‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ç ”ç©¶å‘ç°ï¼ŒLLMsçš„æ€§èƒ½æ›´å¤šåœ°å–å†³äºä»»åŠ¡æœ¬èº«ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å¤§å°ã€‚è™½ç„¶æ›´å¤§çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´å¤šå¯æ‰§è¡Œçš„ç¨‹åºï¼Œä½†è¿™äº›ç¨‹åºå¹¶ä¸æ€»æ˜¯äº§ç”Ÿæ›´é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”æˆæœ¬æ›´é«˜ã€‚æ²¡æœ‰æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼ŒæŸäº›æ¨¡å‹å¯èƒ½è¡¨ç°æ›´å¥½ã€‚å°è¯•å¤šç§æ¨¡å‹å¹¶ä½¿ç”¨æœ€ä½³ç»“æœå¯ä»¥æé«˜å¯é æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆé¢†åŸŸåˆæˆç¨‹åºä»£ç çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨ä¸”å¯æ‰©å±•çš„å¹³å°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æ¸¸æˆé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ç”Ÿæˆçš„ä»£ç è´¨é‡ä¸é«˜ã€å¯æ‰§è¡Œæ€§å·®ç­‰ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„æœç´¢ç­–ç•¥å’Œæ›´å¥½çš„æç¤ºå·¥ç¨‹ï¼Œä»¥æé«˜LLMsçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¤šç§æ¨¡å‹å¯ä»¥é™ä½æˆæœ¬å¹¶æé«˜ç»“æœçš„å¯é æ€§ã€‚</td>
    </tr>
    <tr>
      <th>152</th>
      <td>OpenHoldem: A Benchmark for Large-Scale Imperfect-Information Game Research</td>
      <td>Owning to the unremitting efforts by a few institutes, significant progress<br>has recently been made in designing superhuman AIs in No-limit Texas Hold'em<br>(NLTH), the primary testbed for large-scale imperfect-information game<br>research. However, it remains challenging for new researchers to study this<br>problem since there are no standard benchmarks for comparing with existing<br>methods, which seriously hinders further developments in this research area. In<br>this work, we present OpenHoldem, an integrated toolkit for large-scale<br>imperfect-information game research using NLTH. OpenHoldem makes three main<br>contributions to this research direction: 1) a standardized evaluation protocol<br>for thoroughly evaluating different NLTH AIs, 2) four publicly available strong<br>baselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs<br>for public NLTH AI evaluation. We have released OpenHoldem at holdem.ia.ac.cn,<br>hoping it facilitates further studies on the unsolved theoretical and<br>computational issues in this area and cultivate crucial research problems like<br>opponent modeling and human-computer interactive learning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OpenHoldemï¼šå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶çš„åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œåœ¨æ— é™åˆ¶å¾·å·æ‰‘å…‹ï¼ˆNLTHï¼‰é¢†åŸŸï¼Œè®¾è®¡å‡ºè¶…è¶Šäººç±»çš„AIå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒNLTHå·²æˆä¸ºå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶çš„ä¸»è¦æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŸºå‡†ï¼Œæ–°ç ”ç©¶äººå‘˜éš¾ä»¥ç ”ç©¶æ­¤é—®é¢˜ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†è¯¥ç ”ç©¶é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ‡å‡†åŒ–è¯„ä¼°åè®®<br>OpenHoldem æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬å››ç§ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºå…¨é¢è¯„ä¼°ä¸åŒçš„ NLTH AIã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬ï¼š<br>- **å¯¹å±€è¯„ä¼°æŒ‡æ ‡**ï¼šé€šè¿‡é‡å¤å¯¹å±€ï¼Œè¯„ä¼° AI çš„å¹³å‡æ•ˆç”¨ï¼Œå¹¶ä½¿ç”¨æ–¹å·®å‡å°‘æŠ€æœ¯ï¼ˆå¦‚å¤åˆ¶æ‰‘å…‹å’Œ AIVATï¼‰æ¥å‡å°‘éšæœºæ€§å½±å“ã€‚<br>- **å¯åˆ©ç”¨æ€§è¯„ä¼°æŒ‡æ ‡**ï¼šé€šè¿‡è®¡ç®—æœ€ä½³å“åº”ç­–ç•¥ï¼Œè¯„ä¼° AI çš„å¯åˆ©ç”¨æ€§ï¼Œå¹¶ä½¿ç”¨å±€éƒ¨æœ€ä½³å“åº”ï¼ˆLBRï¼‰å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRL-BRï¼‰æ¥è¿‘ä¼¼è®¡ç®—ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…¬å¼€å¯ç”¨çš„å¼ºåŸºçº¿ AI<br>OpenHoldem è®¾è®¡å¹¶å®ç°äº†å››ç§ä¸åŒç±»å‹çš„ NLTH AIï¼Œä½œä¸ºæœªæ¥ç ”ç©¶çš„è‰¯å¥½èµ·ç‚¹ï¼š<br>- **åŸºäºè§„åˆ™çš„ AI**ï¼šç”±é¢†åŸŸä¸“å®¶è®¾è®¡çš„è§„åˆ™é›†åˆï¼Œç”¨äºå¤„ç†å„ç§åœºæ™¯ã€‚<br>- **åŸºäº CFR çš„é™æ€ AI**ï¼šä½¿ç”¨ CFR ç®—æ³•è¿‘ä¼¼æ±‚è§£çº³ä»€å‡è¡¡ç­–ç•¥ï¼Œå¹¶é€šè¿‡ä¿¡æ¯æŠ½è±¡å’ŒåŠ¨ä½œæŠ½è±¡æ¥é™ä½æ¸¸æˆè§„æ¨¡ã€‚<br>- **ç±»ä¼¼ DeepStack çš„åœ¨çº¿ AI**ï¼šä½¿ç”¨æŒç»­é‡è§£å’Œæ·±åº¦ç¥ç»ç½‘ç»œæ¥å¤„ç†ç¦»æ ‘åŠ¨ä½œï¼Œå¹¶æé«˜å†³ç­–æ•ˆç‡ã€‚<br>- **åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ AI**ï¼šä½¿ç”¨ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ä»æ¸¸æˆçŠ¶æ€å­¦ä¹ åˆ°åŠ¨ä½œï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡ç‰¹å¾æˆ–è¿›è¡Œè¿­ä»£æ¨ç†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåœ¨çº¿æµ‹è¯•å¹³å°<br>OpenHoldem å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æµ‹è¯•å¹³å°ï¼Œå†…ç½®äº†å››ç§å¼ºåŸºçº¿ AIï¼Œå¹¶æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜æµ‹è¯•å’Œæ¯”è¾ƒä»–ä»¬çš„ AIã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>OpenHoldem çš„åŸºçº¿ AI åœ¨ä¸ç°æœ‰å…¬å¼€å¯ç”¨çš„ NLTH AI çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒåˆ†æäº†ä¸åŒè®¾è®¡é€‰æ‹©å¯¹ AI æ€§èƒ½çš„å½±å“ï¼Œå¹¶éªŒè¯äº† OpenHoldem çš„å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OpenHoldem ä¸ºå¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆç ”ç©¶æä¾›äº†ä¸€ä¸ªå®è´µçš„å·¥å…·ï¼Œå…¶æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€å¼ºåŸºçº¿ AI å’Œåœ¨çº¿æµ‹è¯•å¹³å°å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚OpenHoldem çš„è®¾è®¡æ€è·¯å’Œå®ç°æ–¹æ³•ä¹Ÿä¸ºå…¶ä»–æ¸¸æˆ AI ç ”ç©¶æä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>120</th>
      <td>Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors</td>
      <td>Video game testing requires game-specific knowledge as well as common sense<br>reasoning about the events in the game. While AI-driven agents can satisfy the<br>first requirement, it is not yet possible to meet the second requirement<br>automatically. Therefore, video game testing often still relies on manual<br>testing, and human testers are required to play the game thoroughly to detect<br>bugs. As a result, it is challenging to fully automate game testing. In this<br>study, we explore the possibility of leveraging the zero-shot capabilities of<br>large language models for video game bug detection. By formulating the bug<br>detection problem as a question-answering task, we show that large language<br>models can identify which event is buggy in a sequence of textual descriptions<br>of events from a game. To this end, we introduce the GameBugDescriptions<br>benchmark dataset, which consists of 167 buggy gameplay videos and a total of<br>334 question-answer pairs across 8 games. We extensively evaluate the<br>performance of six models across the OPT and InstructGPT large language model<br>families on our benchmark dataset. Our results show promising results for<br>employing language models to detect video game bugs. With the proper prompting<br>technique, we could achieve an accuracy of 70.66%, and on some video games, up<br>to 78.94%. Our code, evaluation data and the benchmark can be found on<br>https://asgaardlab.github.io/LLMxBugs</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹ä¸­çš„æ½œåŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è§†é¢‘æ¸¸æˆæµ‹è¯•éœ€è¦æ¸¸æˆç‰¹å®šçš„çŸ¥è¯†å’Œå¯¹æ¸¸æˆäº‹ä»¶çš„å¸¸è¯†æ¨ç†ã€‚è™½ç„¶ AI é©±åŠ¨çš„ä»£ç†å¯ä»¥æ»¡è¶³ç¬¬ä¸€ä¸ªè¦æ±‚ï¼Œä½†è‡ªåŠ¨æ»¡è¶³ç¬¬äºŒä¸ªè¦æ±‚ä»ç„¶ä¸å¯èƒ½ã€‚å› æ­¤ï¼Œè§†é¢‘æ¸¸æˆæµ‹è¯•é€šå¸¸ä»ç„¶ä¾èµ–äºæ‰‹åŠ¨æµ‹è¯•ï¼Œéœ€è¦äººç±»æµ‹è¯•è€…å½»åº•åœ°ç©æ¸¸æˆæ¥æ£€æµ‹æ¼æ´ã€‚è¿™ä½¿å¾—å®Œå…¨è‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ¼æ´æ£€æµ‹é—®é¢˜è¡¨è¿°ä¸ºé—®ç­”ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›æ¥è¯†åˆ«æ¸¸æˆäº‹ä»¶åºåˆ—ä¸­çš„æ¼æ´äº‹ä»¶ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥ GameBugDescriptions åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å« 167 ä¸ªæœ‰æ¼æ´çš„æ¸¸æˆç©æ³•è§†é¢‘å’Œ 334 ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›– 8 ä¸ªæ¸¸æˆã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº† OPT å’Œ InstructGPT å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—çš„å…­ä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ†æäº†è¯­è¨€æ¨¡å‹å¯¹ä¸åŒäº‹ä»¶æè¿°çš„é²æ£’æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹æ–¹é¢å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚é€šè¿‡é€‚å½“çš„æç¤ºæŠ€æœ¯ï¼Œå¯ä»¥å®ç° 70.66% çš„å‡†ç¡®ç‡ï¼Œåœ¨æŸäº›è§†é¢‘æ¸¸æˆä¸­ç”šè‡³å¯ä»¥è¾¾åˆ° 78.94%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­æå‡ºçš„ GameBugDescriptions åŸºå‡†æ•°æ®é›†å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒè¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        