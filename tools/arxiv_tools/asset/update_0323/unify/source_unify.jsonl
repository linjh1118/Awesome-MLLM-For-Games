{"title":"OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models","authors":"Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, Xinggang Wang","summary":"Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps:\/\/github.com\/hustvl\/OmniMamba","url":"http:\/\/arxiv.org\/abs\/2503.08686v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.08686v1","published":1741715986000,"comment":null,"pdf_text":"OmniMamba: Efficient and Unified Multimodal Understanding and Generation\nvia State Space Models\nJialv Zou1,⋄\nBencheng Liao2,1,⋄\nQian Zhang3\nWenyu Liu1\nXinggang Wang1,B\n1 School of EIC, Huazhong University of Science & Technology\n2 Institute of Artificial Intelligence, Huazhong University of Science & Technology\n3 Horizon Robotics\n(c) Speed Comparison\n(d) Memory Comparison\n(a) Training Data Comparison\n(b) Performance Comparison\nFigure 1. Comprehensive comparison between OmniMamba and other unified understanding and generation models. (a) Our\nOmniMamba is trained on only 2M image-text pairs, which is 1000 times less than Show-o. (b) With such limited data for training, our\nOmniMamba significantly outperforms Show-o across a wide range of benchmarks and achieves competitive performance with JanusFlow.\nBlack metrics are for the multimodal understanding benchmark, while the blue metric is for the visual generation task. (c)-(d) We compare\nthe speed and memory of OmniMamba with other unified models on the same single NVIDIA 4090 GPU. OmniMamba demonstrates up\nto a 119.2× speedup and 63% GPU memory reduction for long-sequence generation.\nAbstract\nRecent advancements in unified multimodal understand-\ning and visual generation (or multimodal generation) mod-\nels have been hindered by their quadratic computational\ncomplexity and dependence on large-scale training data.\nWe present OmniMamba, the first linear-architecture-based\nmultimodal generation model that generates both text and\nimages through a unified next-token prediction paradigm.\n⋄Intern of Horizon Robotics.\nB Corresponding author: xgwang@hust.edu.cn\nThe model fully leverages Mamba-2’s high computational\nand memory efficiency, extending its capabilities from text\ngeneration to multimodal generation.\nTo address the\ndata inefficiency of existing unified models, we propose\ntwo key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA\nfor parameter-efficient adaptation.\nFurthermore, we in-\ntroduce a decoupled two-stage training strategy to miti-\ngate data imbalance between two tasks.\nEquipped with\nthese techniques, OmniMamba achieves competitive per-\nformance with JanusFlow while surpassing Show-o across\n1\narXiv:2503.08686v1  [cs.CV]  11 Mar 2025\nbenchmarks, despite being trained on merely 2M image-\ntext pairs, which is 1,000 times fewer than Show-o. No-\ntably, OmniMamba stands out with outstanding inference\nefficiency, achieving up to a 119.2× speedup and 63%\nGPU memory reduction for long-sequence generation com-\npared to Transformer-based counterparts. Code and mod-\nels are released at https:\/\/github.com\/hustvl\/\nOmniMamba\n1. Introduction\nIn recent years, Large Language Models (LLMs) [2, 5, 15,\n59, 60] have achieved remarkable advancements, igniting\nsignificant research interest in extending their fundamental\ncapabilities to the visual domain. Consequently, researchers\nhave developed a series of Multimodal Large Language\nModels (MLLMs) for tasks such as multimodal understand-\ning [42, 43, 75, 77] and visual generation [31, 55].\nRecent studies have emerged that seek to integrate mul-\ntimodal understanding with visual generation, aiming to de-\nvelop unified systems capable of handling both tasks si-\nmultaneously.\nSuch designs hold the potential to foster\nmutual enhancement between generation and understand-\ning, offering a promising pathway toward truly unifying all\nmodalities. Numerous studies have sought to preserve the\ntext generation paradigm of LLMs while exploring the im-\npact [46, 64, 66, 67] of integrating diverse visual generation\nparadigms, such as diffusion models [24], flow-based gen-\nerative models [16, 40], and vector-quantized autoregres-\nsive models [56].\nUnfortunately, the significant domain gap between im-\nage and text presents a critical challenge for unified mul-\ntimodal generative models: preserving generation capabili-\nties without degrading understanding performance requires\nan extensive volume of image-text pairs for training, as il-\nlustrated in Fig. 1. This not only leads to poor training ef-\nficiency but also creates a substantial barrier to the broader\ndevelopment of such models, as only a small fraction of\nresearchers possess the resources to undertake such com-\nputationally demanding studies. Moreover, most existing\nunified multimodal generative models rely on Transformer-\nbased LLMs [61]. However, their quadratic computational\ncomplexity results in slow inference speeds, rendering them\nless practical for real-time applications.\nThe challenges faced by existing unified multimodal\ngenerative models naturally lead us to ponder: can a model\nbe developed that achieves both training efficiency and\ninference efficiency?\nTo address this, we introduce OmniMamba, a novel\nunified multimodal generative model that requires only\n2M image-text pairs for training.\nBuilt on the Mamba-\n2-1.3B [10] model as the foundational LLM with a uni-\nfied next token prediction paradigm to generate all modali-\nties, OmniMamba leverages the linear computational com-\nplexity of state space models (SSMs) to achieve signifi-\ncantly faster inference speeds. Furthermore, to empower the\nMamba-2 LLM—whose foundational capabilities are rela-\ntively weaker compared to the extensively studied Trans-\nformer models—to efficiently learn mixed-modality gener-\nation with limited training data, we propose novel model\narchitectures and training strategies.\nTo enhance the model’s capability in handling diverse\ntasks, we incorporate task-specific LoRA [25].\nSpecifi-\ncally, within each Mamba-2 layer’s input linear projection,\nwe introduce distinct LoRA modules for multimodal un-\nderstanding and visual generation. During task execution,\nthe features are modulated by both the linear projection and\nthe corresponding task-specific LoRA, while the irrelevant\nLoRA components are deactivated. Furthermore, we pro-\npose the decoupled vocabularies to guide the model in gen-\nerating the appropriate modality, which requires more data\nfor the model to learn. On the data front, we further propose\na novel two-stage decoupled training strategy to address the\ndata imbalance between the two tasks, significantly improv-\ning training efficiency.\nTrained on only 2M image-text pairs, our proposed\nOmniMamba outperforms Show-o [67] on multiple mul-\ntimodal understanding benchmarks and also matches the\nperformance of JanusFlow [46], which was introduced by\nDeepSeek AI. Moreover, it achieves the best visual genera-\ntion performance on the MS-COCO dataset [39]. Notably,\nOmniMamba demonstrates a 119.2× speedup at a sequence\nlength of 16k and a 63% GPU memory reduction at a se-\nquence length of 23k, compared to Show-o. Furthermore,\nat a sequence length of 100k, it achieves a 10.2× speedup\nand 40.4% memory savings compared to JanusFlow. Our\nmain contributions can be summarized as follows:\n• We introduce OmniMamba, the first Mamba-based uni-\nfied multimodal understanding and visual generation\nmodel to the best of our knowledge. By novelly adopt-\ning decoupled vocabularies and task-specific LoRA, Om-\nniMamba achieves effective training and inference.\n• We propose a novel decoupled two-stage training strat-\negy to address the issue of data imbalance between tasks.\nWith this strategy and our model design, OmniMamba\nachieves competitive performance using only 2M image-\ntext pairs for training-up to 1,000 times fewer than previ-\nous SOTA models.\n• Comprehensive experimental results show that Omni-\nMamba achieves competitive or even superior perfor-\nmance across a wide range of vision-language bench-\nmarks and MS-COCO generation benchmark, with sig-\nnificantly improved inference efficiency, achieving up to\na 119.2× speedup and 63% GPU memory reduction for\nlong-sequence generation on NVIDIA 4090 GPU.\n2\n2. Related Work\nMultimodal Understanding\nThe remarkable advance-\nments in LLMs have catalyzed the development of Large\nVision-Language Models (LVLMs). Some representative\nworks, such as the LLaVA series [43, 79], BLIP series [35,\n36], and MiniGPT-4 [77], have demonstrated strong multi-\nmodal understanding capabilities. These models align the\nfeatures obtained from pretrained vision encoders with the\nfeature space of LLMs through feature projectors, enabling\npretrained LLMs to transfer their understanding and reason-\ning abilities to multimodal scenarios.\nVisual Generation\nIn recent years, diffusion models [24]\nhave made remarkable progress, leading to models [12, 49,\n54] with strong visual generation capabilities. Building on\nthese advancements, flow-based generative models [16, 40]\nhave achieved superior results with fewer sampling steps.\nAdditionally, some works [56, 71] have successfully inte-\ngrated autoregressive models into this domain, achieving\nnotable performance.\nUnified Understanding and Generation\nThe remark-\nable advancements of LLMs in the fields of multimodal un-\nderstanding and visual generation have naturally sparked\nresearchers’ interest in training a single LLM for both\ntasks. Early works [13, 18–20] integrated pretrained dif-\nfusion modules as tools into LLMs, essentially forming a\ncombination of two expert systems rather than utilizing a\nsingle LLM to perform both tasks. This approach results\nin a more complex model architecture and often leads to\nsuboptimal outcomes. Show-o [67] integrates next-token\nprediction with discrete diffusion, enabling adaptive han-\ndling of mixed-modality inputs and outputs. Meanwhile,\nJanusFlow [46] merges autoregressive models with rectified\nflow, a cutting-edge technique in visual generation. In con-\ntrast, Emu3 [64] asserts that next-token prediction holds the\ngreatest potential for achieving multi-modal generation, re-\nlying exclusively on this paradigm to manage both text and\nimage generation tasks. Although these methods achieve\noutstanding performance, they are all based on Transform-\ners, whose quadratic computational complexity presents\na significant drawback, particularly when handling long-\nsequence generation tasks and high-resolution image gener-\nation. To address this challenge, we propose OmniMamba,\nwhich employs a unified next token prediction paradigm to\ngenerate both text and image modalities, aiming to extend\nthe linear computational complexity of the linear models to\nthe field of multimodal generation.\nLinear Model\nIn recent years,\na series of linear-\ncomplexity models have emerged as strong competitors to\nTransformers. Mamba [21], a selective state space model,\nhas garnered widespread attention for its competitive per-\nformance and faster inference speeds compared to Trans-\nformers. Building on this, Mamba-2 [10], an enhanced ver-\nsion of Mamba, achieves performance on par with Trans-\nformers while being 2-8 times faster. Similarly, GLA [68]\nleverages gated linear attention to achieve linear complex-\nity, maintaining competitive performance with Transform-\ners.\nThe success of linear-complexity models in the field\nof natural language processing (NLP) has inspired its ap-\nplication in the visual domain, where it has been exten-\nsively studied in traditional image tasks [45, 78], multi-\nmodal understanding [27, 38, 50, 74], and visual genera-\ntion [26, 34]. In this paper, we aim to design a unified multi-\nmodal understanding and visual generation model based on\nMamba-2, which maintains competitive performance with\nTransformer-based unified models while offering signifi-\ncantly faster inference speeds.\n3. Method\n3.1. Overall Architecture\nOur ultimate goal is to design a unified multimodal un-\nderstanding and visual generation model that achieves both\ntraining and inference efficiency using only 2M image-text\npairs for training. We believe the key to realizing this goal\ncan be summarized in one word: decoupling. To this end,\nwe propose OmniMamba, the architecture of which is illus-\ntrated in Fig. 2.\nSuccess necessitates standing on the shoulders of giants.\nWe observe Emu3 [64], an autoregressive-based model\nwhich employs vast amounts of data and 8 billion model\nparameters. Despite these advantages, its final performance\nremains suboptimal, falling short of JanusFlow [46], a hy-\nbrid generative paradigm-based model with significantly\nless data and fewer parameters. We argue that this discrep-\nancy stems not from the inherent superiority of the hybrid\ngenerative paradigm but from Emu3’s tight coupling design,\nit uses the same vocabulary and encoder for all tasks and\nmodalities. While this design aligns with the original in-\ntention of a unified model, it may lead to inefficient data\nutilization. In the following, we will introduce our model\nby focusing on the concept of decoupling.\n3.2. Decoupling Encoders for the Two Tasks\nPrevious works have explored using a single vision en-\ncoder for both tasks. For example, Show-o [67] employs\nMAGVIT-v2 [70] to encode images into discrete tokens for\nboth understanding and generation tasks. TransFusion [76]\nutilizes a shared U-Net or a linear encoder to map images\ninto a continuous latent space for both tasks. Emu3 trains\nits vision encoder based on SBER-MoVQGAN5, enabling\nthe encoding of video clips or images into discrete tokens.\n3\nPrompt\nText \nEncoder\nDiscrete tokens\nUnd. Image\nUnd. \nEncoder\nMLP\nContinuous features\nGen. Image\nGen. \nEncoder\nDiscrete tokens\n…\n…\nMMU Sequence\n…\n…\nT2I LoRA\nMamba-2\n…\n…\n…\n…\nImage \nHead\nText \nHead\nT2I Sequence\nMMU LoRA\n…\nImage Codebook\nText Vocabulary\ndog\nlove\nspring\nis\ncute   the\nThe dog is cute\nNext Token Prediction\nText Sequence\nImage Sequence\nFigure 2. Architecture of the proposed OmniMamba “MMU” refers to multimodal understanding, while “T2I” refers to text-to-image\ngeneration. OmniMamba employs a next-token prediction paradigm for both multimodal understanding and visual generation tasks. To\naddress the distinct requirements of each task—semantic information extraction for multimodal understanding and high-fidelity image\ncompression for visual generation, we utilize separate encoders and heads. Furthermore, we purpose decoupled vocabularies to guide\nmodality-specific generation and task-specific LoRA for parameter-efficient adaptation.\nConv\nInput Proj\nσ\nSSM\nX\nN\nσ\nOutput Proj\nMMU LoRA\nT2I LoRA\nFigure 3. The Mamba-2 block with task-specific LoRA. It is\nworth noting that while the Mamba-2 Block in the Mamba-2 paper\nhas two input projectors, the actual code implementation separates\nthe feature dimensions from a single projector output. For sim-\nplicity, we depict only one input projector in our illustration. Our\ntask-specific LoRA is applied to this entire input projector.\nHowever, JanusFlow [46] has shown that such a unified\nencoder design is suboptimal. We believe this is primarily\nbecause multimodal understanding requires rich semantic\nrepresentations for complex reasoning, whereas visual gen-\neration focuses on precisely encoding the spatial structure\nand texture of images. The inherent conflict between these\ntwo objectives suggests that a unified encoder design may\nnot be the optimal choice. Therefore, OmniMamba adopts\na decoupled vision encoder design.\nFollowing prismatic VLMs [30], we fuse DINOv2 [48]\nand SigLIP [73] as an encoder to extract continuous fea-\ntures for multimodal understanding. The key idea is that in-\ntegrating visual representations from DINOv2, which cap-\nture low-level spatial properties, with the semantic features\nprovided by SigLIP leads to further performance improve-\nments. For visual generation, we use an image tokenizer\ntrained with LlamaGen [56] to encode images into dis-\ncrete representations. This tokenizer was pretrained on Im-\nageNet [11] and further fine-tuned on a combination of 50M\nLAION-COCO [33] and 10M internal high aesthetic quality\ndata.\n3.3. Decoupling Vocabularies for the Two Tasks\nUnlike Emu3 and Show-o, which use a large unified vocab-\nulary to represent both text and image modalities, to dis-\nentangle modality-specific semantics, we employ two sepa-\nrate vocabularies for each modality. This design explicitly\nseparates the two modalities, providing additional modality-\nlevel prior knowledge. As a result, the model does not need\nto learn whether the output should be text or image, instead,\nit ensures the correct output modality by indexing the corre-\nsponding vocabulary. Our subsequent ablation experiments\nalso confirm that OmniMamba’s dual-vocabulary design is\none of the key factors for efficient training.\n3.4. Task Specific LoRA\nTo enhance the model’s adaptability to specific tasks, we\nintroduce task-specific adapters. We hypothesize that ex-\nplicitly parameterizing the selection in SSMs based on task\ncan enhance the data efficiency of multimodal training [14].\n4\nStage 1: MMU Pre-Training\nMamba-2\nLinear Proj\nUnd. Enc\nGen. Enc\nText Head\nImg Head\nMMU LoRA\nT2I LoRA\nStage 1: T2I Pre-Training\nMamba-2\nLinear Proj\nUnd. Enc\nGen. Enc\nText Head\nImg Head\nMMU LoRA\nT2I LoRA\nStage 2: Unifid Fine-Tuning\nMamba-2\nLinear Proj\nUnd. Enc\nGen. Enc\nText Head\nImg Head\nMMU LoRA\nT2I LoRA\nFigure 4. Training strategy of OmniMamba. The trainable components are indicated by a flame symbol, while the frozen ones are\nrepresented by snowflakes. The dashed arrows indicate that this route is temporarily dropped and does not participate in model training.\nSpecifically, to avoid introducing excessive parameters, we\nuse LoRA [25] as the adapter.\nIn OmniMamba, task-\nspecific LoRA is applied only to the input projection of each\nMamba-2 layer, as illustrated in Fig 3. When performing a\nspecific task, the input linear projection and task-specific\nLoRA work together to effectively address the task. For in-\nstance, when the model performs a multimodal understand-\ning (MMU) task, the MMU LoRA route is activated, while\nthe text-to-image (T2I) LoRA route is dropped. Explicitly\nactivating the corresponding adapter to assist in task execu-\ntion helps improve data efficiency in training [14, 63].\n3.5. Decoupled Training Strategy\nWe propose a decoupled two-stage training strategy to ad-\ndress data imbalance between understanding and genera-\ntion tasks while improving training efficiency, as illustrated\nin Fig. 4. This approach consists of (1) a Task-Specific\nPre-Training stage for module-specific initialization and\nmodality alignment, and (2) a Unified Fine-Tuning stage\nfor unified multi-task training.\nDecoupling Rationale\nThe first stage separates multi-\nmodal understanding (MMU) and text-to-image (T2I) gen-\neration tasks to prioritize modality alignment without data\nratio constraints. Unlike joint pre-training methods (e.g.,\nJanusFlow [46] with a fixed 50:50 MMU-T2I data ratio),\nour approach trains task-specific modules independently,\nenabling flexible dataset scaling (665K MMU vs. 83K T2I\nsamples). Only randomly initialized components—linear\nprojection and MMU LoRA for understanding, T2I LoRA\nand image head for generation—are trained, while the core\nMamba-2 model remains frozen. This eliminates competi-\ntion between tasks during early learning and allows asym-\nmetric data utilization.\nStage 1: Task-Specific Pre-Training\nIt contains: MMU\nPre-Training:\nTrains the linear projection and MMU\nLoRA to align visual-textual representations.\nThe T2I\nLoRA path is disabled to isolate understanding-task learn-\ning. T2I Pre-Training: Optimizes the T2I LoRA and im-\nage decoder for visual synthesis. The MMU LoRA path is\ndisabled to focus on generation capabilities.\nStage 2:\nUnified Fine-Tuning\nInspired by multi-task\nframeworks [30, 79], we freeze the visual encoder and train\nall other modules while preserving task-specific LoRA in-\ndependence.\nDuring each forward pass: (1) MMU and\nT2I computations use their respective LoRA branches; (2)\nLosses from both tasks are summed for a unified backward\npass. This balances parameter sharing (via the frozen back-\nbone) and task specialization (via isolated LoRA paths), en-\nabling synergistic learning while mitigating interference be-\ntween understanding and generation objectives.\n3.6. Training Details\nData Formats\nFollowing Show-o [67], we use special to-\nkens to unify the data formats for both multimodal under-\nstanding and visual generation tasks. The multimodal un-\nderstanding data is structured as:\n[MMU][SOI]{image tokens}[EOI][SOT]{text tokens}[EOT].\nWhile the visual generation data is:\n[T2I][SOT]{text tokens}[EOT][SOI]{image tokens}[EOI].\nSpecifically, [MMU] and [T2I] is a pre-defined task token\nused to guide the model in performing the corresponding\ntask. [SOT] and [EOT] are used to represent the beginning\nand end of text tokens, respectively. Similarly, [SOI] and\n[EOI] represent the beginning and end of image tokens.\nTraining Objective\nSince OmniMamba uses the auto-\nregressive paradigm to handle both multimodal understand-\ning and visual generation tasks, we only need to use the\n5\nType\nModel\nLLM Params\nRes. POPE↑MME-P↑VQAv2test↑GQA↑MMMU↑\nUnd. Only\nLLaVA-Phi [79]\nPhi-2-2.7B\n336\n85.0\n1335.1\n71.4\n-\n-\nLLaVA [43]\nVicuna-7B\n224\n76.3\n809.6\n-\n-\n-\nEmu3-Chat [64]\n8B from scratch\n512\n85.2\n-\n75.1\n60.3\n31.6\nLLaVA-v1.5 [42]\nVicuna-13B\n448\n86.3\n1500.1\n81.8\n64.7\n-\nInstructBLIP [8]\nVicuna-13B\n224\n78.9\n1212.8\n-\n49.5\n-\nMobileVLM [6]\nMobileLLaMA-1.4B\n336\n84.5\n1196.2\n-\n56.1\n-\nMobileVLM-V2 [7]\nMobileLLaMA-1.4B\n336\n84.3\n1302.8\n-\n59.3\n-\nLLaVA-v1.5-Phi-1.5 [67]\nPhi-1.5-1.3B\n336\n84.1\n1128.0\n75.3\n56.5\n30.7\nUnified\nLWM [44]\nLLaMA2-7B\n256\n75.2\n-\n55.8\n44.8\n-\nChameleon [58]\n7B from scratch\n512\n-\n-\n-\n-\n22.4\nLaVIT [29]\n7B from scratch\n256\n-\n-\n66.0\n46.8\n-\nEmu3 [64]\n8B from scratch\n512\n85.2\n1243.8\n75.1\n60.3\n31.6\nJanus [66]\nDeepSeek-LLM-1.3B\n384\n87.0\n1338.0\n77.3\n59.1\n30.5\nJanusFlow [46]\nDeepSeek-LLM-1.3B\n384\n88.0\n1333.1\n79.8\n60.3\n29.3\nShow-o [67]\nPhi-1.5-1.3B\n512\n80.0\n1097.2\n69.4\n58.0\n26.7\nOmniMamba\nMamba-2-1.3B\n384\n86.3\n1290.6\n77.7\n60.8\n30.6\nTable 1. Comparison with other methods on multimodal understanding benchmarks. “Und. only” refers to models that only perform\nmultimodal understanding task, while “Unified” refers to models that unify both multimodal understanding and visual generation tasks.\nModels with a similar number of parameters to ours are highlighted in light blue for emphasis.\nstandard cross-entropy loss for next-token prediction dur-\ning training.\n4. Experiment\n4.1. Data\nTo achieve the goal of data efficiency, we aim to train Omn-\niMamba using as few high-quality image-text pairs as pos-\nsible.\nMultimodal Understanding Data\nIn the first pretrain\nstage, the training data consists of 676K image-text pairs,\nall of which are sourced from publicly available datasets.\nThese includes 118K images from COCO [39] and 558K\nimages from LLaVA-1.5 pre-training data [42].\nIn the second fine-tune stage, we also exclusively use\npublicly available datasets follow Cobra [74].\n1. The mixed dataset used in LLaVA-1.5 [42] consists of\n665K visual multi-turn conversations.\n2. LVIS-Instruct-4V [62], which comprising 220K images\naccompanied by visually aligned and context-aware in-\nstructions generated by GPT-4V.\n3. LRV-Instruct [41],\na large-scale visual instruction\ndataset of 400K samples, designed to mitigate halluci-\nnation issues across 16 vision-and-language tasks.\nVisual Generation Data\nTo facilitate better reproducibil-\nity and further exploration by the community, we using only\n83K images from the MS-COCO 2014 dataset [39] for text-\nto-image generation training.\nOverall, the training data for our unified multimodal gen-\neration model consists of fewer than 2 million image-text\npairs. In contrast to previous works that rely on over 100M\nor even over 1B pairs, our approach is highly training effi-\ncient.\n4.2. Implementation Details\nOur core model is based on Mamba-2-1.3B, which consists\nof 48 layers of Mamba-2 blocks. In our primary experi-\nments, the input image resolution for the multimodal un-\nderstanding task is 384, while the image resolution for the\nvisual generation task is 256.\nFor\nmultimodal\nunderstanding,\nwe\ncombine\nDI-\nNOv2 [48] and SigLIP [73] as the image encoder, while\nfor visual generation, we use the VQVAE trained by Llama-\nGen [56] as the image encoder. We incorporate task-specific\nLoRA into the input projector of each Mamba-2 block and\nset the LoRA rank to 8, which results in only a 0.65% in-\ncrease in parameters. All training stages use the AdamW\noptimizer with β1 set to 0.9 and β2 set to 0.95. We adopt co-\nsine annealing with warm-up as the learning rate schedule.\nWeight decay is set to 0, and gradient clipping is applied\nwith a threshold of 1.0. Other detailed hyper-parameters\nare shown in the appendix. All of our training is conducted\non NVIDIA A800 GPUs with BF16 precision.\n4.3. Quantitative Results\nMultimodal Understanding\nWe evaluate OmniMamba’s\nmultimodal understanding capabilities on a wide range\nof vision-language benchmarks, including POPE [37],\n6\nType\nModel\nParams\nImages\nFID-30K↓\nGen. Only\nDALL·E [53]\n12B\n250M\n27.5\nGLIDE [47]\n5B\n250M\n12.24\nDALL·E 2 [52]\n6.5B\n650M\n10.39\nSDv1.5 [54]\n0.9B\n2000M\n9.62\nPixArt [3]\n0.6B\n25M\n7.32\nImagen [55]\n7B\n960M\n7.27\nParti [69]\n20B\n4.8B\n7.23\nRe-Imagen [4]\n2.5B\n50M\n6.88\nU-ViT [1]\n45M\n83k(coco)\n5.95\nUnified\nCoDI [57]\n-\n400M\n22.26\nSEED-X [20]\n17B\n-\n14.99\nLWM [44]\n7B\n-\n12.68\nDreamLLM [13]\n7B\n-\n8.76\nShow-o [67]\n1.3B\n35M\n9.24\nOmniMamba\n1.3B\n83k(coco)\n5.50\nTable 2.\nCompare visual generation capability with other\nmethods on MS-COCO validation dataset. “Gen. only” refers\nto models that only perform visual generation task, while “Uni-\nfied” refers to models that unify both multimodal understanding\nand visual generation tasks.\nModel\nGenavg (Image\/s)\nTotal (s)\nShow-o [67]\n0.81\n19.66\nJanusFlow [46]\n1.02\n15.64\nOmniMamba\n5.68\n2.81\nTable 3. Image Generation Speed in Visual Generation Task.\nOmniMamba achieves 7.0 × faster image generation speed com-\npared to Show-o and 5.6 × faster compared to JanusFlow.\nMME [17], GQA [28], MMMU [72].\nThe results are\nshown in Tab 1. Compared to models with a similar num-\nber of parameters, OmniMamba surpasses understanding-\nspecific models such as LLaVA-v1.5-Phi-1.5 [67], Mo-\nbileVLM [6], and MobileVLMv2 [7]. It also outperforms\nthe unified understanding and generation model Show-\no [67] and achieves competitive performance compare to\nthe state-of-the-art unified model JanusFlow [46].\nNo-\ntably, while Show-o utilizes 2B image-text pairs and Janus-\nFlow leverages over 65M image-text pairs, OmniMamba\nachieves competitive performance by using only 2M image-\ntext pairs for training.\nVisual Generation\nWe evaluate OmniMamba for text-to-\nimage generation on the widely recognized MS-COCO [39]\nbenchmark dataset. To quantify image quality, we report the\nFID score [23]. Consistent with previous literature, we ran-\ndomly select 30K prompts from the MS-COCO validation\nset and generate corresponding images to compute the FID\nscore. The results are shown in Tab 2, where our model\nachieves the best visual generation performance on the MS-\nCOCO validation dataset. Notably, models such as Show-\no [67] and PixArt [69] are trained on external large-scale\ndatasets and further fine-tuned on COCO-like datasets (e.g.,\nOpenImages [32]) before evaluating zero-shot on the MS-\nCOCO validation set. In contrast, to avoid introducing ex-\ncessive additional data, both our model and U-ViT [1] are\ntrained solely on the MS-COCO training set and evaluated\non the MS-COCO validation set.\n4.4. Qualitative Results\nWe present qualitative evaluations of our OmniMamba for\nboth multimodal understanding and visual generation tasks.\nFig 5 showcases our model’s capabilities in scene descrip-\ntion and text-guided generation. Additional visualization\nresults can be found in the appendix.\n4.5. Inference Speed and GPU Memory usage\nWe compared the generation speed and GPU memory usage\nof OmniMamba with other Transformer-based models in\nboth multimodal understanding and visual generation tasks.\nAll the evaluations were done on the same single NVIDIA\n4090 GPU with FP16 precision.\nIn multimodal understanding task, all models received\nthe same example image.\nWe used the same prompt,\n“Please describe the image in detail.” and removed the to-\nken generation limit to test their generation speed. The re-\nsults are shown in the Fig 1. OmniMamba demonstrates\n119.2× speedup at a sequence length of 16k, and saves\n63.0% GPU memory at a sequence length of 23k compared\nto Show-o-256. Meanwhile, at a sequence length of 100k,\nOmniMamba achieves a 10.2 × speedup and 40.4% GPU\nmemory savings compared to JanusFlow-384, which is ac-\ncelerated by FlashAttention-2 [9]. Notably, Show-o-256 in-\ndicates that the input image resolution is 256. Due to the\ndesign of its omni-attention mechanism being incompatible\nwith FlashAttention-2, it was not used during testing. Sim-\nilarly, JanusFlow-384 represents an input image resolution\nof 384, with FlashAttention-2 applied during testing for ac-\nceleration.\nIn visual generation task, we used the same prompt, “A\nPicture”. The models generate images with a batch size of\n16 and a resolution of 256. As shown in Tab. 3, our model\nachieves image generation speeds that are 7.0 × faster than\nShow-o and 5.6 × faster than JanusFlow.\n4.6. Ablation Studies\nWe conducted a series of ablation studies to verify the ef-\nfectiveness of each design in OmniMamba.\nIn this sec-\ntion, all ablation studies are conducted based on Mamba-\n2-370M, with the understanding visual encoder replaced by\nCLIP [51], an input resolution of 224, and a reduced number\n7\nMultimodal Understanding\nVisual Generation\nOmniMamba: The image features a light brown dog, \npossibly a Labrador, and a kitten. The dog is sitting on \nthe grass, and the kitten is standing next to it. The \nkitten appears to be looking up at the dog. The \nbackground consists of a field with some flowers and \ntrees. The dog and the kitten seem to be interacting, \nand the image captures a moment of curiosity \nbetween the two animals.\nUser: Please describe the image in detail.\nA bear walking down the \ngrass in the open.\nA giraffe sticks out it's \ntongue for a treat.\nTwo cats sitting on a sill \nlooking out the window.\nA pizza sitting on top \nof a pizza pan.\nFigure 5. Qualitative results of OmniMamba on multimodal understanding and visual generation.\n#Exp\nDecoupling Vocabularies\nTask Specific LoRA\nPOPE↑\nMME↑\nGQA↑\nFID-30K↓\n1\n✗\n✓\n80.8\n1036\n53.6\n19.1\n2\n✓\n✗\n81.2\n1003\n54.0\n14.4\n3\n✓\n✓\n81.9\n1100\n55.3\n10.3\nTable 4. Ablation studies on decoupling Vocabularies and task specific LoRA in OmniMamba\nof training steps (kept consistent across all ablation experi-\nments), while keeping all other settings unchanged.\nImpact of Decoupling Vocabulary for the Two Tasks\nTo guide the model in generating specific modalities\nfrom a structural design perspective, we employ modality-\ndecoupled vocabularies in the default OmniMamba.\nWe\nconducted ablation studies on this design.\nAs shown in\nTab 4, Exp1 and Exp3 utilize a modality-unified vocabulary\nand modality-decoupled vocabularies, respectively. The re-\nsults demonstrate that the decoupled vocabularies enable\nmore efficient model training and yield better performance.\nNotably, when using the modality-unified vocabulary for\nvisual generation, the model occasionally produces text-\nrelated tokens, which requires additional post-processing\nto ensure correct visual generation. This further indicates\nthat the model needs additional training to effectively learn\nmodality-specific generation.\nImpact of Task Specific Adapter\nWe conducted ablation\nstudies on the introduced task-specific adapter module, and\nthe results are shown in Tab 4. Exp2 and Exp3 represent the\nmodel without and with the task-specific adapter, respec-\ntively. The experiments demonstrate that the task-specific\nadapter helps the model efficiently learn both multimodal\nunderstanding and visual generation with a minimal amount\nof training image-text pair data (2M).\n5. Conclusion\nWe presented OmniMamba, the first Mamba-2-based uni-\nfied multimodal understanding and visual generation frame-\nwork that achieves competitive performance with remark-\nable inference and training efficiency. By introducing three\nkey innovations:\ndecoupled vocabularies to disentangle\nmodality-specific semantics, task-specific LoRA modules\nfor parameter-efficient adaptation, and a two-stage decou-\npled training strategy to resolve data imbalance, Omni-\nMamba achieves comparable performance with JanusFlow\nand even surpasses Show-o using only 2M image-text pairs\nfor training. Moreover, OmniMamba exhibits outstanding\ninference efficiency, It achieves a 119.2× speedup with a\nsequence length of 16k and a 63% reduction in GPU mem-\nory at a sequence length of 23k, compared to Show-o. With\n8\na sequence length of 100k, it delivers a 10.2× speedup\nand saves 40.4% of GPU memory compared to JanusFlow.\nThese results validate that our proposed OmniMamba is\nboth training and inference efficient, with the potential to\nenable more ordinary researchers to participate in the wave\nof unified model innovation. However, due to the limited\nscale of training data, our model’s performance remains\nslightly below SOTA methods. Exploring the trade-off be-\ntween training data volume and model performance will be\na key focus of our future work.\nReferences\n[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,\nHang Su, and Jun Zhu. All are worth words: A vit backbone\nfor diffusion models. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n22669–22679, 2023. 7\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877–1901, 2020. 2\n[3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al.\nPixart-α: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 7\n[4] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 7\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm:\nScaling language modeling\nwith pathways.\nJournal of Machine Learning Research,\n24(240):1–113, 2023. 2\n[6] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu,\nYang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\nXiaolin Wei, et al.\nMobilevlm: A fast, reproducible and\nstrong vision language assistant for mobile devices. arXiv\npreprint arXiv:2312.16886, 2023. 6, 7\n[7] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu,\nFei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang\nLin, Bo Zhang, et al.\nMobilevlm v2:\nFaster and\nstronger baseline for vision language model. arXiv preprint\narXiv:2402.03766, 2024. 6, 7\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 6\n[9] Tri Dao.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning.\narXiv preprint\narXiv:2307.08691, 2023. 7\n[10] Tri Dao and Albert Gu. Transformers are ssms: General-\nized models and efficient algorithms through structured state\nspace duality. arXiv preprint arXiv:2405.21060, 2024. 2, 3\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 4\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 3\n[13] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 3, 7\n[14] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao,\nWei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran\nFan, et al. Loramoe: Revolutionizing mixture of experts for\nmaintaining world knowledge in language model alignment.\narXiv preprint arXiv:2312.09979, 4(7), 2023. 4, 5\n[15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024. 2\n[16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn Forty-first International Conference on Machine Learn-\ning, 2024. 2, 3\n[17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A compre-\nhensive evaluation benchmark for multimodal large language\nmodels, 2024. 7\n[18] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 3\n[19] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,\nXintao Wang, and Ying Shan. Making llama see and draw\nwith seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n[20] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin\nSong, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Mul-\ntimodal models with unified multi-granularity comprehen-\nsion and generation. arXiv preprint arXiv:2404.14396, 2024.\n3, 7\n[21] Albert Gu and Tri Dao.\nMamba: Linear-time sequence\nmodeling with selective state spaces.\narXiv preprint\narXiv:2312.00752, 2023. 3\n[22] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025. 12\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 7\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\n9\nprocessing systems, 33:6840–6851, 2020. 2, 3\n[25] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2, 5\n[26] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga\nGrebenkova, Pingchuan Ma, Johannes Fischer, and Bj¨orn\nOmmer. Zigma: A dit-style zigzag mamba diffusion model.\nIn European Conference on Computer Vision, pages 148–\n166. Springer, 2024. 3\n[27] Wenjun Huang, Jiakai Pan, Jiahao Tang, Yanyu Ding, Yifei\nXing, Yuhe Wang, Zhengzhuo Wang, and Jianguo Hu. Ml-\nmamba: Efficient multi-modal large language model utiliz-\ning mamba-2. arXiv preprint arXiv:2407.19832, 2024. 3\n[28] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019. 7\n[29] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jian-\nchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, et al.\nUnified language-vision pretraining in llm with dynamic\ndiscrete visual tokenization. arxiv 2024.\narXiv preprint\narXiv:2309.04669. 6\n[30] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna,\nPercy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic\nvlms: Investigating the design space of visually-conditioned\nlanguage models. arXiv preprint arXiv:2402.07865, 2024.\n4, 5\n[31] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Gen-\nerating images with multimodal language models. Advances\nin Neural Information Processing Systems, 36, 2024. 2\n[32] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale. Interna-\ntional journal of computer vision, 128(7):1956–1981, 2020.\n7\n[33] LAION.\nLaion-coco 600m.\nhttps:\/\/laion.ai\/\nblog\/laion-coco, 2022. 4\n[34] Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong\nChou, Xin Li, and Guoqi Li. Scalable autoregressive image\ngeneration with mamba. arXiv preprint arXiv:2408.12245,\n2024. 3\n[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 3\n[36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation. In Interna-\ntional conference on machine learning, pages 12888–12900.\nPMLR, 2022. 3\n[37] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 6\n[38] Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng\nCheng, Yingyue Li, Haoran Yin, Wenyu Liu, and Xing-\ngang Wang. Multimodal mamba: Decoder-only multimodal\nstate space model via quadratic to linear distillation. arXiv\npreprint arXiv:2502.13145, 2025. 3\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014. 2, 6, 7\n[40] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matt Le. Flow matching for generative mod-\neling. arXiv preprint arXiv:2210.02747, 2022. 2, 3\n[41] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Ya-\ncoob, and Lijuan Wang. Mitigating hallucination in large\nmulti-modal models via robust instruction tuning.\nIn The\nTwelfth International Conference on Learning Representa-\ntions, 2023. 6\n[42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 2, 6\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 2, 3, 6\n[44] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv e-prints, pages arXiv–2402, 2024. 6,\n7\n[45] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi\nXie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan\nLiu. Vmamba: Visual state space model, 2024. 3\n[46] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu,\nChengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei\nZhang, Liang Zhao, et al. Janusflow: Harmonizing autore-\ngression and rectified flow for unified multimodal under-\nstanding and generation. arXiv preprint arXiv:2411.07975,\n2024. 2, 3, 4, 5, 6, 7\n[47] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 7\n[48] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 4, 6\n[49] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 3\n[50] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia\nZhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Ex-\nploring state space models for multimodal learning. arXiv\npreprint arXiv:2403.13600, 2024. 3\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n10\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 7\n[52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022. 7\n[53] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International confer-\nence on machine learning, pages 8821–8831. Pmlr, 2021. 7\n[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 3, 7\n[55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in neural information\nprocessing systems, 35:36479–36494, 2022. 2, 7\n[56] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 2, 3, 4, 6\n[57] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and\nMohit Bansal. Any-to-any generation via composable diffu-\nsion. Advances in Neural Information Processing Systems,\n36, 2024. 7\n[58] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n6\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 2\n[61] A Vaswani. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 2\n[62] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning.\narXiv preprint\narXiv:2311.07574, 2023. 6\n[63] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-\naohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang\nZhou.\nOne-peace:\nExploring one general representa-\ntion model toward unlimited modalities.\narXiv preprint\narXiv:2305.11172, 2023. 5\n[64] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is\nall you need. arXiv preprint arXiv:2409.18869, 2024. 2, 3,\n6\n[65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 12\n[66] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling visual encoding\nfor unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024. 2, 6\n[67] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One\nsingle transformer to unify multimodal understanding and\ngeneration. arXiv preprint arXiv:2408.12528, 2024. 2, 3,\n5, 6, 7\n[68] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar\nPanda, and Yoon Kim.\nGated linear attention trans-\nformers with hardware-efficient training.\narXiv preprint\narXiv:2312.06635, 2023. 3\n[69] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 7\n[70] Lijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Ver-\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh\nBirodkar, Agrim Gupta, Xiuye Gu, et al. Language model\nbeats diffusion–tokenizer is key to visual generation. arXiv\npreprint arXiv:2310.05737, 2023. 3\n[71] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Randomized autoregressive visual generation.\narXiv preprint arXiv:2411.00776, 2024. 3\n[72] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi.\nIn Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 9556–\n9567, 2024. 7\n[73] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pages 11975–11986, 2023. 4, 6\n[74] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng\nHuang, and Donglin Wang.\nCobra: Extending mamba to\nmulti-modal large language model for efficient inference.\narXiv preprint arXiv:2403.14520, 2024. 3, 6\n[75] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo,\nXien Liu, Ji Wu, and Lei Huang.\nTinyllava: A frame-\nwork of small-scale large multimodal models. arXiv preprint\narXiv:2402.14289, 2024. 2\n[76] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024. 3\n11\n[77] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 2, 3\n[78] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\nvisual representation learning with bidirectional state space\nmodel. arXiv preprint arXiv:2401.09417, 2024. 3\n[79] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin\nPeng. Llava-phi: Efficient multi-modal assistant with small\nlanguage model.\nIn Proceedings of the 1st International\nWorkshop on Efficient Multimedia Computing under Limited,\npages 18–22, 2024. 3, 5, 6\nAppendix\nA. Training Details\nThe detailed training hyper-parameters are listed in Tab 5.\nThe first stage separates multimodal understanding (MMU)\nand text-to-image (T2I) generation tasks to prioritize\nmodality alignment without data ratio constraints. To this\nend, we employ a larger learning rate during the first pre-\ntraining stage and a smaller learning rate in the second fine-\ntuning stage. The batch size ratio represents the proportion\nbetween multimodal understanding data and visual genera-\ntion data. All training is conducted on NVIDIA A800 GPUs\nusing BF16 precision.\nStage 1: MMU Stage 1: T2I Stage 2: Unify\nLearning Rate\n1e-3\n8e-4\n1e-4\nWarm-up Steps\n100\n1000\n0\nTraining Steps\n5k\n100k\n150k\nBatch Size\n256:0\n0:720\n3:48\nTable 5. Hyper-parameters of OmniMamba. The batch size\nratio refers to the proportion between multimodal understanding\ndata and visual generation data.\nB. Limitations\nAlthough our OmniMamba achieves promising results with\na very small amount of data, the limited data volume still\nrenders our model suboptimal. Furthermore, unlike previ-\nous works that leverage large-scale, high-quality datasets\nsuch as LAION-aesthetics, we rely solely on the MS-COCO\ndataset for visual generation. As a result, the quality of gen-\nerated images, particularly for human faces, remains less\nrefined. Exploring the trade-off between dataset scale and\nmodel performance will be a key focus of our future work.\nAdditionally, while Mamba-2 demonstrates exceptional\ninference efficiency, its foundational capabilities remain\nweaker compared to the extensively studied Transformer.\nFurthermore, Mamba-2 has only been trained on sequences\nof up to 2048 tokens, limiting its ability to handle ultra-long\nsequences and hindering its extension to advanced tech-\nniques such as Chain-of-Thought (CoT) [65] or reinforce-\nment learning [22].\nEnhancing Mamba-2’s foundational\ncapabilities and its capacity to model ultra-long sequences\nwill be critical areas for future investigation.\nC. Additional Qualitative Results\nC.1. Multimodal Understanding\nWe validate the multimodal understanding capabilities of\nour OmniMamba and other approaches across three as-\npects: scene description, spatial reasoning, and counting\nwith tricky questions, which are shown in Table 6–8.\nSpatial Reasoning:\nUser\nIs the bicycle parked to the right of the dog in this\nimage?\nOmniMamba\nThe bicycle is actually parked to the left of the dog,\nnot to the right.\nShow-o\nYes, the bicycle is parked to the left of the dog.\nJanusFlow\nNo.\nTable 6. OmniMamba can accurately determine spatial positions,\nJanusFlow made the correct judgment, but its response was overly\nconcise, while Show-o lacks spatial reasoning capabilities, as in-\ndicated by the red text.\nCounting with Tricky Questions:\nUser\nThe animals in the picture look like cows. How many\nare there?\nOmniMamba\nThere are three goats in the image, not cows.\nShow-o\nThere are three cows in the image.\nJanusFlow\nThere are three animals in the picture.\nTable 7. OmniMamba correctly outputs the quantity while avoid-\ning the user’s trap question, whereas Show-o, despite counting\ncorrectly, was misled by the question, as indicated by the red\ntext. On the other hand, JanusFlow’s response was not accurate\nenough.\n12\nScene Description:\nUser\nPlease describe it in detail.\nOmniMamba\nThe image depicts a serene scene of a cat and a dog\nsharing a peaceful moment together. The cat, exhibit-\ning a gray and white coat, is curled up comfortably,\nits head resting on the dog’s body. The dog, with its\nbrown fur, is curled up in a similar position, with its\nhead resting on the cat’s back. Their eyes are gen-\ntly closed, signifying a deep sleep. The background\nis a blurred white, allowing the focus to remain on\nthe sleeping feline and its canine friend. The image\nexudes warmth and affection, showcasing the close\nbond between the two animals.\nShow-o\nThe image features a white and brown cat lying on a\nbed, curled up and sleeping. The cat is resting its head\non a stuffed animal, which is a teddy bear. The teddy\nbear is positioned on the right side of the cat, with its\nhead resting on the cat’s back. The scene captures a\npeaceful and cozy moment between the cat and the\nstuffed animal.\nTable 8. OmniMamba can accurately describe the information in\nthe scene, whereas Show-o made a mistake about the color of the\ncat and misidentified the dog as a teddy bear, as indicated by the\nred text.\nC.2. Visual Generation\nWe provide additional visualization results of visual genera-\ntion to further validate our generation capabilities, as shown\nin Fig 6.\n13\nA close up view of a \nguy brushing his teeth.\nA colorful bird perched \non a branch in the wild.\nA close shot of a \ncomputer keyboard. \nA laptop computer \nsitting on top of a desk.\nA man cutting off his \ntongue with scissors.\nA sink under a large \nmirror in a bathroom.\nA female tennis player \non a tennis court.\nA good looking pizza is \nstill in the box.\nA surfer in a wet suit is \nsurfing on a white board\nA jumbo jet plane flying \nthrough the air in a cloud \nfilled sky.\nA bathroom with a \ntoilet, cabinet and rug. \nA bathroom with two \nsinks mounted on a wall.\nA bed in a bedroom \nbetween two lamps.\nA bowl that has different \ntypes of food in it.\nA brown table with white \nplate holding a pizza.\nFigure 6. Qualitative results of OmniMamba visual generation. Prompts are randomly drawn from the MS-COCO validation set.\n14\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models.pdf"}
{"title":"Dual Diffusion for Unified Image Generation and Understanding","authors":"Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, Peng Wang","summary":"Diffusion models have gained tremendous success in text-to-image generation,\nyet still lag behind with visual understanding tasks, an area dominated by\nautoregressive vision-language models. We propose a large-scale and fully\nend-to-end diffusion model for multi-modal understanding and generation that\nsignificantly improves on existing diffusion-based multimodal models, and is\nthe first of its kind to support the full suite of vision-language modeling\ncapabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and\nrecent advances in discrete diffusion language modeling, we leverage a\ncross-modal maximum likelihood estimation framework that simultaneously trains\nthe conditional likelihoods of both images and text jointly under a single loss\nfunction, which is back-propagated through both branches of the diffusion\ntransformer. The resulting model is highly flexible and capable of a wide range\nof tasks including image generation, captioning, and visual question answering.\nOur model attained competitive performance compared to recent unified image\nunderstanding and generation models, demonstrating the potential of multimodal\ndiffusion modeling as a promising alternative to autoregressive next-token\nprediction models.","url":"http:\/\/arxiv.org\/abs\/2501.00289v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2501.00289v1","published":1735624140000,"comment":null,"pdf_text":"Dual Diffusion for Unified Image Generation and Understanding\nZijie Li1*\nHenry Li2∗\nYichun Shi3\nAmir Barati Farimani1\nYuval Kluger2\nLinjie Yang3\nPeng Wang3\n1Carnegie Mellon University\n2Yale University\n3Bytedance Seed\nProject website\nAbstract\nDiffusion models have gained tremendous success in text-\nto-image generation, yet still lag behind with visual under-\nstanding tasks, an area dominated by autoregressive vision-\nlanguage models. We propose a large-scale and fully end-\nto-end diffusion model for multi-modal understanding and\ngeneration that significantly improves on existing diffusion-\nbased multimodal models, and is the first of its kind to sup-\nport the full suite of vision-language modeling capabilities.\nInspired by the multimodal diffusion transformer (MM-DiT)\nand recent advances in discrete diffusion language mod-\neling, we leverage a cross-modal maximum likelihood es-\ntimation framework that simultaneously trains the condi-\ntional likelihoods of both images and text jointly under a\nsingle loss function, which is back-propagated through both\nbranches of the diffusion transformer. The resulting model\nis highly flexible and capable of a wide range of tasks in-\ncluding image generation, captioning, and visual question\nanswering. Our model attained competitive performance\ncompared to recent unified image understanding and gen-\neration models, demonstrating the potential of multimodal\ndiffusion modeling as a promising alternative to autoregres-\nsive next-token prediction models.\n1. Introduction\nWe are currently in the midst of a multimodal generative\nmodeling revolution. Large scale diffusion models such as\nStable Diffusion [19], Dall-E [62], FLUX, and Imagen [64]\nhave become indisputable industry leaders for generating\nhigh fidelity images from text descriptions, enabling the\naccurate modeling and sampling of complex and high di-\nmensional distributions of images given text. Conversely,\nautoregressive next-token prediction models have achieved\ngroundbreaking performance both in pure text generation\nand reasoning such as in ChatGPT [1], Gemini [73], and\n*The first two authors contributed equally to this work, work done dur-\ning an internship at Bytedance.\nLlama [17] and in visually-grounded text generation with\nlarge language models (LLMs), as seen with LLaVA [47]\nor BLIP-2 [39].\nGiven these developments, a natural question comes to\nmind: Can these existing image-to-text (I2T) or text-to-\nimage (T2I) systems be modified to reason with and gen-\nerate data in the reverse direction?\nA positive answer\nwould suggest the possibility of producing a fully multi-\nmodal model that is able to understand and sample from\nconditional distributions between modalities in an omni-\ndirectional manner.\nMoreover, unifying these generative\nframeworks under a single model with shared parameters\ncan confer a multitude of downstream benefits including\nimproved reasoning, simplified implementation, and may\nbe a natural next step towards artificial general intelligence\n[31, 75].\nWith autoregressive next-token prediction models, this\nquery has already been answered resoundingly in the affir-\nmative, as evidenced by a multitude of studies [16, 22, 24,\n70, 72, 78, 83] demonstrating T2I capabilities of finetuned\nLLMs. This is in part due to the known next-token genera-\ntive capability of autoregressive models with visual tokens\n[40, 74, 82].\nOn the contrary, with diffusion models there has been\nsurprisingly little evidence of a similar reverse capacity.\nUntil recently, generative diffusion models have struggled\nwith language modeling due to the lack of an empirically\nperformant discrete diffusion process on text tokens, in spite\nof continued research in this area [4, 15, 42]. At present,\nmultimodal diffusion models either exhibit limited text rea-\nsoning capabilities and partial text diffusion [7, 81], which\nrequire an autoregressive model such as GPT2 [59] to de-\ncode denoised text latents, or emerge as add-ons to pre-\ntrained LLMs fine-tuned in conjunction with a diffusion\nloss [80, 85], and ultimately still rely entirely on next-token\nprediction for text generation.\nWe leverage the novel progress in this domain to revisit\nthe above-mentioned question and propose a dual-branch\ndiffusion model based on the multimodal diffusion trans-\nformer (MM-DiT) architecture [19], which we modify to\n1\narXiv:2501.00289v1  [cs.CV]  31 Dec 2024\noutput diffusion targets on both modalities of the neural net-\nwork. We then train our model to perform continuous latent\nspace diffusion on the image branch and discrete masked to-\nken diffusion on the text branch. Our novel implementation\nalso allows for controllable infilling in the token space, en-\nabling visual question answering and vision language assis-\ntance, which prior diffusion-based models were incapable\nof. To the best of our knowledge, this is the first end-to-end\nmultimodal diffusion model fully capable of full-featured\nI2T and T2I generation.\nMoreover, we demonstrate the compatibility of our\nframework with existing diffusion foundation models such\nas Stable Diffusion 3 (SD3)[19], allowing us to initialize our\nmodel with pretrained checkpoints, and reveals remarkably\nfast adaptation capabilities of the proposed architecture on\ntext generation, producing meaningful text output in under\n25B text tokens when initialized with an SD3 checkpoint.\nOur contributions can be summarized as follows:\n• We introduce a fully end-to-end cross-modal diffusion\nmodel that unifies image and text diffusion under a sin-\ngle transformer, which to the best of our knowledge is the\nfirst of its kind.\n• We propose a simple, elegant, and easy to implement joint\nloss function that simultaneously trains the conditional\ntext and image modalities in a unified, end-to-end fash-\nion.\n• We demonstrate performance on an expanded set of mul-\ntimodal tasks including image generation, visual caption-\ning, and visual question answering using a diffusion-only\nmodel, significantly improving on the capabilities and\nperformance of prior multimodal diffusion models.\nModality\nTask\nImage\nText\nImage\nImage\nVisual Question\nBackbone\nBackbone\nGen\nCap.\nAnswering\nVersatile Diffusion [81]\nDiffusion\nDiff. + AR\n✓\n✓\n✗\nUnidiffuser [7]\nDiffusion\nDiff. + AR\n✓\n✓\n✗\nShow-O [80]\nDiffusion\nAR\n✓\n✓\n✓\nTransfusion [85]\nDiffusion\nAR\n✓\n✓\n✓\nOurs\nDiffusion\nDiffusion\n✓\n✓\n✓\nTable 1. A side-by-side comparison between the backbones and\nsupported features of our work compared to those of existing\ndiffusion-based multimodal methods.\n2. Background\nIn this section, we review the basic concepts that underpin\nour proposed model. Generally, diffusion models [29, 68]\nare inspired by non-equilibrium thermodynamics [67] de-\nsigned to evaluate a likelihood pθ(x) =\nR\npθ(x0:T )dx1:T\nwhere data x0 := x are related to a set of latent variables\nx1:T by a diffusion process that gradually corrupts the orig-\ninal data.\n2.1. Continuous Diffusion\nContinuous diffusion models operate on continuous vectors\nby learning to reverse the noise-corruption forward process\nxt = αtx + σtϵ,\n(1)\nparameterized by time-dependent scalar αt and σt, where\nαt, σt > 0, αt\/σt decreases monotonically, and ϵ is an\nappropriately selected i.i.d. noise variable. In score-based\ndiffusion models [29, 68], αt, σt are determined by a for-\nward stochastic differential equation (SDE) that pushes xt\ntowards N(0, I) as t 7→∞. New samples can be gener-\nated by learning the reverse process through estimating the\nscore function [3, 68, 77] ∇log pt(xt). Alternatively, from\n(1), the following ordinary differential equation (ODE) can\nbe derived:\n˙xt = v(xt, t),\n(2)\nwhere velocity field v(xt, t) = ˙αtx + ˙σtϵ. The ODE in (2)\npushes the distribution of xt from p0 to pT . To generate new\nsamples, we can use neural networks to approximate the ve-\nlocity field v and then integrate ODE (2) backward in time\nstarting from xT ∼N(0, I). A common choice of αt, σt\nin flow matching model is αt = 1 −t, σ = t and therefore\nv = ϵ −x, which corresponds to the optimal transport in-\nterpolant between two distribution p0 and p1 [45, 50]. The\nneural network for regressing the velocity field v in (2) is\ntrained by optimizing the flow matching loss\nLFM = Et,q(xt|x)||vθ(xt, t) −(ϵ −x)||2\n2.\n(3)\nRecent work such as Stable Diffusion 3 [19] has demon-\nstrated the superiority of flow matching model on text-to-\nimage generation, thus in this work we adopt flow matching\nobjective for modeling the distribution of images.\n2.2. Discrete Diffusion\nIn discrete diffusion, the variate x ∈X × · · · × X has finite\nsupport over the product space of X = {1, . . . , N}, where\nin language models N is the vocabulary size of the token\nembedding. Generally, there are two ways to approach this\nmodeling task. The first line of works [12, 15, 27, 41, 52]\napply a continuous relaxation to the discrete variable and\nproceed with a continuous reformulation of the framework,\nallowing the application of the equations in Section 2.1.\nThis greatly simplifies the diffusion modeling itself, but in-\ntroduces a significant source of error in the mapping be-\ntween discrete and relaxed continuous states. Conversely,\nthe diffusion process is extended to the discrete token space\n[4, 51, 54, 65], which removes the need for the aforemen-\ntioned mapping via a specialized discrete diffusion formu-\nlation. In our work, we will focus on this latter perspec-\ntive given its empirical potential as validated in recent work\n2\n[23, 51, 65, 66]. Leveraging continuous-time Markov chain\n(CTMC) theory, the marginal distributions pt can be de-\nscribed by a family of linear ordinary differential equations\ndpt\ndt = Qtpt,\n(4)\nwhere p0 ≈pdata and p1 = pstationary, and Qt is a time-\ndependent sequence of transition matrices that provides a\nmapping between the two distributions. We consider the\ncase of absorbing state (i.e., masked) diffusion that are\nshown to work well on text modeling [51, 65, 66]. This\nformulation induces the posterior (0 < s < t)\nq(xs|xt, x) =\n(\nCat(xs|xt)\nxt ̸= m\nCat(xs| (1−αs)m+(αs−αt)x\n1−αt\n)\no.w.\n(5)\nwhere clean data x is a discrete variable (one-hot vector)\nwith N categories, with the marginal\nq(xt|x) = Cat[xt|αtx + (1 −αt)m],\n(6)\nwhere Cat(·|π) denotes the categorical distribution over dif-\nferent classes with probability π, and m denotes the mask\nabsorbing state.\nTo reverse this process, one may either model the den-\nsity ratio sθ(x)y ≈\npt(x)\npt(y) given two sequences x, y ∈\nX ×· · ·×X as in Uniform Score-Entropy Discrete Diffusion\n(SEDD) [51], or the denoised variate xθ(xt, αt) ≈x di-\nrectly as in masked diffusion model [65, 66]. In the former,\nthe modeled density ratios induce a specialized reverse tran-\nsition matrix ¯Qt that can be leveraged in Eq. 4. In the latter,\nxθ can be directly substituted for x in Eq. 5. In this work,\nwe follow Sahoo et al. [65] that enforces zero-probability on\nthe mask state m and keeps all un-masked state unchanged\nduring reverse sampling. This induce a simplified (negative)\nvariational lower bound under the continuous time limit\nLNELBO = Eq(xt|x)\n\u0014Z 1\n0\nα′\nt\n1 −αt\nlog(xθ(xt, αt) · x)dt\n\u0015\n.\n(7)\nIn practice, we can use Monte-Carlo sampling to approxi-\nmate and evaluate the loss function in (7). Following Sahoo\net al. [65], we use a log-linear schedule: αt = 1 −t.\n3. Method\nWe propose an end-to-end multi-modal diffusion model\nnamed Dual Diffusion Transformer (D-DiT) with a uni-\nfied backbone that jointly models image and text distribu-\ntion. More specifically, given image x(img) and text x(txt),\nwe are interested at modeling the conditional distribution\np(x(img)|x(txt)) and p(x(txt)|x(img)).\nThe former is usu-\nally referred to as text-to-image generation and the latter\nis closely related to various image understanding tasks such\nas captioning and visual question answering.\n3.1. Architecture\nInspired by the MM-DiT in SD3 [19], our proposed D-DiT\nis a Transformer-based model comprising two branches -\none for processing image tokens and another for process-\ning text tokens. The image and text tokens attend with each\nother in every attention layer. In D-DiT, the output of the\nimage branch is the prediction of velocity defined in (2)\nwith text conditioning, while the output for the text branch\nis the x(txt) prediction with image conditioning. The scalar\ntimestep embedding modulates every layer’s feature map\nvia AdaLN (adaptive layernorm) [56]. We only input the\ntimestep t to the model during image generation, as x(txt)\nt\nhas already implicitly carried the information of signal-to-\nnoise ratio (the number of mask tokens in the sequence). In\naddition, we add a text encoder with bi-directional attention\non top of the text branch of the diffusion model. Despite\nan asymmetric design of the model is not strictly required,\nhaving a text encoder on top of a DiT model allows us to\neasily adapt many existing text-to-to-image model such as\nSD3 and FLUX as pretrained backone for our D-DiT model.\nNote that the text encoder should not have causal mask as\nthis will violate the masked diffusion process.\nTo reduce the computational cost associated with high-\nresolution images, we follow prior works on latent-space\n(image) diffusion [63] which compresses images from raw\npixel space to a spatially compressed latent space obtained\nfrom a variational-autoencoder (VAE) trained with discrim-\ninator loss [18] and KL-divergence regularization [34].\n3.2. Training\nWe propose a joint training objective for image-text joint\nmodeling, which is essentially a joint denoising target that\ncombines continuous and discrete diffusion. Formally, we\nuse flow matching introduced in Section 2.1 to learn the\nconditional distribution of images and masked diffusion in-\ntroduced in Section 2.2 to learn the conditional distribution\nof texts. During training, corrupted samples xt(img), xt(txt)\n1 are sampled from the corresponding forward corruption\nprocesses q(xt|x) defined in (1) and (6) respectively and\nthen we can calculate the diffusion loss for each modality\nLimage\n= Et,q(img)\n\f\f\f\n\f\f\fvθ\n\u0010\nx(img)\nt\n, t, x(txt)\u0011\n−(ϵ −x(img))\n\f\f\f\n\f\f\f\n2\n2 ,\nLtext\n= Eq(txt)\n\u0014\n−1\nK\nXK\ni=1 log[xθ(x(txt)\nti\n, x(img)) · x]\/ti\n\u0015\n,\n(8)\nwhere we use antithetic sampling for text diffusion\ntimesteps ti by discretizing (δ, 1] into K points uniformly\nwith δ being a small number to avoid numerical instability,\n1For better readability superscript (txt), (img) are omitted.\n3\n𝑞(𝑥!|𝑥)\nText\nImage\n𝑉- pred\n𝑡= 𝑡(#$%)\n𝑋- pred\nD-DiT\nVAE Encoder\nT5 Encoder\nMaked Text\nImage\n𝑡= 0\n𝑋- pred\nD-DiT\nVAE Encoder\nT5 Encoder\nText\n𝑉- pred\n𝑞(𝑥!|𝑥)\n𝑡= 𝑡(#$%)\nD-DiT\nImage\nVAE Encoder\nT5 Encoder\nText\nNoisy latents\n𝑉- pred\n𝑋- pred\n(a) Overview\n(b) Image-conditioned \ntext generation\n(c) Text-conditioned \nimage generation\nFigure 1. Our proposed model, the Dual Diffusion Transformer (D-DiT) that simultaneously models image and text distributions via a joint\ndenoising diffusion training loss. a) An overview of the model architecture. The gray blocks (T5 encoder, image autoencoder) are kept\nfixed throughout training and inference. b) During training for (image-conditioned) text denoising, the text input is randomly masked while\nthe image is noise-free. c) During training for text-conditioned image denoising, the image is randomly noised while the text is noise-free.\nfor image diffusion we sample t from the log-normal distri-\nbution. We do not corrupt the conditioning samples during\ntraining and as such the image diffusion timestep is always\nset to zero when predicting text distribution.\nTo sum up, the overall dual modality training loss is a\nsimple weighted combination of the above single modality\ndiffusion loss:\nLdual = Limage + λtextLtext,\n(9)\nwith λtext being a hyperparameter.\n3.3. Inference\nWe introduce three types of sampling-based inference\nwhich can be used for different vision-language tasks,\nwhich we detail below.\nText-to-image Generation\nTo perform text-guided im-\nage generation, i.e.\nx ∼p(x(img)|x(txt)), we use the\ncommonly adopted classifier-free guidance (CFG) tech-\nnique [28] to sample from the conditional distribution\np(x(img)\nt\n|x(txt)), which amounts to a re-weighting of the ve-\nlocity prediction\n˜vt = svθ\n\u0010\nx(img)\nt\n, t, x(txt)\u0011\n+(1−s)vθ\n\u0010\nx(img)\nt\n, t, ∅\n\u0011\n, (10)\nwhere s is a hyperparameter that controls the scale of guid-\nance and ∅is a suitable null embedding (e.g. the embedding\nof an empty text) .\nImage-to-text Generation\nTo sample images from the\nconditional distribution, we can use ancestral sampling to\ndraw from the posterior distribution q(xs|xt, x) in (5) by\nplugging in prediction x ≈xθ(x(txt)\nt\n, x(img); t = 0).\nImage-to-text In-filling\nIn certain tasks, both text con-\nditioning information and image conditioning informa-\ntion are available, such as in a visual question answering\ntask where an image and an associated question are pro-\nvided.\nFor such cases, we would like to sample x ∼\np(x(answer)|x(img), x(question)).\nTo perform this task, we initialize the diffusion prior of\nthe question with masked tokens and leverage the robust\ntext in-filling capabilities of the text diffusion model to com-\nplete the sequence by sampling from the conditional distri-\nbution. The text question tokens are kept fixed throughout\nsampling (Figure 2).\nModel\nModel\nImage captioning\nVisual question answering\nPrompt token\nText token\nMask token\nFigure 2. Text masking during both training and sampling under\nthe image captioning (above) and visual question answering (be-\nlow) tasks with our proposed model.\n4. Experiments\n4.1. Experimental Setup\nImplementation details\nWe implement our proposed\nframework\nbased\non\nthe\nopen-sourced\nSD3-medium\nmodel [19]2. We initialize the model weight of the DiT\nfrom the pretrained checkpoint and add a linear head on top\nof the text branch to do text denoising. Following SD3, we\nadopt the existing T5 encoder\/tokenizer [61], and SD3’s im-\nage VAE, whose weights remain unchanged throughout all\n2https:\/\/huggingface.co\/stabilityai\/stable-diffusion-3-medium\n4\nFigure 3. Text-to-image samples generated from the model. We draw images from the reverse diffusion process via the Euler solver with\nT = 28 diffusion steps.\nthe experiments (except for the mask token embedding in\nT5). We remove the CLIP text encoders in the SD3 model\ndue to its causal attention mask and for a simplified model\nstructure. We use the special token <extra_id0> in T5’s\nvocabulary to represent the mask token in masked diffusion,\nas this token is used to mark the masked token in the mask\npretraining process of original T5 model. In this way, we\nfind the model can generate text reasonably well even with-\nout updating the weight of this token embedding. To further\nreduce the domain gap, we unfreeze the token embedding of\n<extra_id0> during the second stage of the training.\nDifferent from multi-modal models that are built upon\nlanguage models, our model has never been trained on text-\nonly generation. In the preliminary experiments, we found\nthat adding text-only target (i.e. un-conditional text genera-\ntion) to the model does not influence its captioning perfor-\nmance significantly. An interesting future direction can be\nextending the proposed framework to model the marginal\ndistribution of each modality.\nDatasets\nWe train the model with three stages on publicly\navailable datasets. The total image-text pairs we have used\nis roughly 40M. We list the detail of the dataset and training\nsetup for each stage below, where all the training stages use\nthe joint diffusion loss defined in (8).\n1. Dual diffusion pretraining. The original SD3 model\nwas only trained on ambient image-text pairs, and not\nsolely on text data itself. To adapt D-DiT to text genera-\ntion tasks, we train it on the joint diffusion loss for 60K\niterations with a batch size of 512. The maximum text\ntoken length is truncated to 64 and we use image resolu-\ntion of 256. The dataset used in this stage is re-captioned\nDatacomp-1b [21, 43] (the model has only seen around\n30M images in this stage, which is less than 3% of the\ntotal images in the dataset).\n2. Continued pretraining on higher quality data. We\nthen unfreeze the masked token embedding in T5 and\ntrain the model for 200k iterations on a image under-\nstanding dataset with rich textual description, which con-\nsists of the pretraining dataset from ShareGPT4V[11]\n(1.3M images) and OpenImages (1.9M subset with ob-\nject detection annotations) [35] re-captioned by Share-\nCaptioner3. The text token length is set to 256 and image\nresolution is 256, with batch size of 512. Finetuning the\nmask token embedding reduces the domain gap as T5 en-\ncoder has not seen sequence filled with high percentage\nof mask token during its pretraining.\nHowever, as updating the mask token embedding re-\nquires backpropagating through the heavy T5 encoder,\nwe freeze the mask embedding after this round of train-\ning on the image understanding dataset. We observe that\nthe ℓ2 difference between the mask token embedding\n3https:\/\/huggingface.co\/Lin-Chen\/ShareCaptioner\n5\nfrom different training iterations does not change much\nafter 100k iterations.\nMoving forward, we conduct an optional high resolu-\ntion model finetuning on the aforementioned image un-\nderstanding dataset together with a newly added image\ngeneration dataset. Concretely, we collect a higher qual-\nity dataset with 10M images (9M re-captioned LAION-\n1024 and 1M midjourney images4).\nIn this training\nstage, the image diffusion loss is calculated on the high\nquality image dataset whereas the text diffusion loss is\ncalculated on the understanding dataset. We finetune the\nmodel for 80k iterations, with image resolution 512, text\ntoken length 256, and a batch size of 768. Only our\n512×512 model variant requires this training stage.\n3. Visual instruction tuning.\nFinally, we finetune our\nmodel on an medley of instruction-tuning datasets to\npromote joint text-image conditioned text generation.\nWe combine the LLaVA-Pretrain558K and LLaVA-v1.5-\nmix-665K visual instruction tuning datasets with the\ntraining splits for TextVQA and VizWiz and train for\n25k iterations. Following the convention in LLaVA-1.5,\nthe model is trained to distinguish between long-form\nanswers and short answers, multiple choice answers, or\ncaptions via task-specific instruction prompts that come\nafter the question such as ”Answer the question using a\nsingle word or phrase,” ”Answer with the option’s letter\nfrom the given choices directly,” or ”Describe the image\nconcisely.”\n4.2. Multi-modal Understanding\nExisting multi-modal diffusion models such as UniDiffuser\n[7] and Versatile Diffusion [81] performed text diffusion in\na CLIP latent space, which hampered their ability to per-\nform text completion, a necessary feature for general ques-\ntion answering and conversation-based tasks. This is no\nlonger a limitation with our proposed D-DiT due to its dis-\ncrete masked diffusion branch, allowing us to leave question\ntokens unmasked throughout sampling. We are thus able to\nevaluate our fine-tuned model on a full suite of image-to-\ntext generation tasks, including image captioning and vi-\nsual question answering benchmarks, as well as long-form\nvisual assistance responses.\nWe first evaluate the visual understanding capabilities of\nD-DiT via the academic question answering benchmarks\nVQAv2 [26], VizWiz [8], OKVQA [53], GQA [30], POPE\n[44], as well as MME [20]. Due to the short-form nature of\nthe questions, we perform sampling with 16 diffusion steps,\nand compare against a selection of multi-modal models, in-\ncluding I2T only and I2T + T2I models. Our results are\nsummarized in Table 3. We note that our D-DiT as the only\ndiffusion-only multi-modal model capable of visual ques-\n4https:\/\/huggingface.co\/datasets\/CaptionEmporium\/midjourney-niji-\n1m-llavanext\ntion answering tasks, already boosts performance that is\ncompetitive with recent I2T + T2I models. Our model at\n512 resolution outperforms Show-O on MME, GQA, and\nPOPE, approaching performances of auto-regressive VLMs\nsuch as QWEN-VL and BLIP-2.\nNext, we provide qualitative examples of the D-DiT, pro-\nviding images and gauging the model’s visual language as-\nsistance capabilities via image-related queries. Given the\nlonger format of the responses, we sample D-DiT responses\nwith 256 diffusion steps. Our model provides answers to\nhuman queries in a manner that suggests a fine-grained\nmulti-modal understanding of the image and text condition-\ning (Figure 4).\n4.3. Text-to-image Generation\nBesides the image-conditioned text generation, we also test\nmodel’s text-to-image generation capability. Following pre-\nvious works, we evaluate our 512×512 model after the sec-\nond training stage on the GenEval benchmark, which mea-\nsures model’s prompt following capability [25]. We fol-\nlow the default setting in the open-sourced SD3 checkpoint\nwhere we use a Euler solver with 28 sampling steps and\na CFG scale of 7.0. We observe that the joint diffusion\ntraining does not cause catastrophic forgetting on the model,\nthe fine-tuned D-DiT preserves the performance of original\nSD3 model and slightly improves on some of metrics such\nas colors after joint training. Qualitative evaluation sam-\nples are shown in Figure 3, where we observe the ability to\ngenerate highly aesthetic images is preserved.\nModel\nparams\n(B)\nOverall\nObjects\nCountingColorsPosition\nColor\nattribution\nSingle Two\nPixArt-α [10]\n0.6\n0.48\n0.98 0.50\n0.44\n0.80\n0.08\n0.07\nSD V2.1\n0.9\n0.50\n0.98 0.51\n0.44\n0.85\n0.07\n0.17\nDALL-E 2 [62]\n6.5\n0.52\n0.94 0.66\n0.49\n0.77\n0.10\n0.19\nSDXL [57]\n0.9\n0.55\n0.98 0.74\n0.39\n0.85\n0.15\n0.23\nDALL-E 3\n-\n0.67\n0.96 0.87\n0.47\n0.83\n0.43\n0.45\nCoDI [71]\n-\n0.31\n0.89 0.16\n0.16\n0.65\n0.02\n0.01\nLWM [48]\n7\n0.47\n0.93 0.41\n0.46\n0.79\n0.09\n0.15\nSEED-X [24]\n17\n0.49\n0.97 0.58\n0.26\n0.80\n0.19\n0.14\nChameleon [72]\n7\n0.39\n-\n-\n-\n-\n-\n-\nShow-O [80]\n1.3\n0.68\n0.98 0.80\n0.66\n0.84\n0.31\n0.50\nTransfusion [85]\n8\n0.67\n-\n-\n-\n-\n-\n-\nSD3 [19]\n2\n0.62\n0.98 0.74\n0.63\n0.67\n0.34\n0.36\nD-DiT (ours)\n2\n0.65\n0.97 0.80\n0.54\n0.76\n0.32\n0.50\nTable 2. Evaluation of text-to-image generation performance on\nGeneval [25]. params denote the number of trainable parameters.\n4.4. Ablation Studies\nAs text-to-image diffusion models are trained on a large\nnumber of text-image pairs, one may raise the question,\nwhether the representation learned throughout this process\ncan be transferred to multi-modal understanding tasks? To\nanswer this question, we perform an ablation study on the\n6\nFigure 4. Multi-modal dialogue examples generated from our model. To our knowledge, D-DiT is the first diffusion-based multimodal\nmodel capable of instruction-based vision and language conversation.\nModel\nParams\nText\nImage\nMS-COCO\nVQAv2\nVizWiz\nOKVQA\nMME\nGQA\nPOPE\n# trainable\nBackbone\nBackbone\nCIDEr ↑\nAcc. ↑\nAcc. ↑\nAcc. ↑\nAcc. ↑\nAcc. ↑\nAcc. ↑\nLLaVA-1.5 [46]\n13B\nAR\n-\n-\n81.8\n57.5\n-\n1500.1\n64.7\n86.4\nBLIP-2 [39]\n13B\nAR\n-\n-\n65.0\n19.6\n-\n1293.8\n41.0\n85.5\nIDEFICS [36]\n9B\nAR\n-\n-\n50.9\n-\n-\n-\n-\n-\nQWEN-VL [6]\n7B\nAR\n-\n-\n78.2\n38.9\n-\n1487.5\n57.5\n-\nOpenFlamingo [5]\n9B\nAR\n-\n65.5\n43.5\n-\n-\n-\n-\n-\nFlamingo [2]\n9B\nAR\n-\n79.4\n51.8\n28.8\n44.7\n-\n-\n-\nCM3Leon [83]\n7B\nAR\nAR\n61.6\n47.6\n37.6\n23.8\n-\n-\n-\nChameleon [72]\n7B\nAR\nAR\n18.0\n-\n-\n-\n-\n-\n-\nLWM [49]\n7B\nAR\nAR\n-\n55.8\n11.6\n-\n-\n44.8\n75.2\nShow-O (256×256) [80]\n1.3B\nAR\nDiffusion\n-\n64.7\n-\n-\n1014.9\n54.2\n76.2\nShow-O (512×512) [80]\n1.3B\nAR\nDiffusion\n-\n69.4\n-\n-\n1097.2\n58.0\n80.0\nTransfusion [85]\n7B\nAR\nDiffusion\n29.0\n-\n-\n-\n-\n-\n-\nD-DiT (Ours, 256×256)\n2B\nDiffusion\nDiffusion\n-\n59.5\n19.4\n28.5\n897.5\n55.1\n79.2\nD-DiT (Ours, 512×512)\n2B\nDiffusion\nDiffusion\n56.2\n60.1\n29.9\n25.3\n1124.7\n59.2\n84.0\nTable 3. Comparison of our D-DiT against related work on visual question answering benchmarks. VLMs that focus on text-generation\nremain superior to unified understanding and generation models, however our models compare favorably with the latter category.\ninternal representation of a text-to-image diffusion model.\nWe adapt several models into an image captioning model,\nincluding SD3, CLIP ViT L\/14, and our D-DiT model.\nAmong different internal layers in SD3, we find the feature\nfrom the 18th layer tend to perform the best in our prelimi-\nnary experiment so it is used as the output feature. We add\nan GPT2 text decoder to the features extracted from SD3\nand CLIP, and directly use D-DiT’s text output as results.\nWe train all models with a mixture of recaptioned Data-\ncomp, recaptioned OpenImages and captioning data from\nShareGPT4V [11]. Concretely, we evaluate the quality of\nthe captions generated from the models by asking GPT4\nto do visual question answering according to the generated\ncaptions. The accuracy is listed in Table 4.\nSimilar to the trend observed in [75], directly using diffu-\nsion features as the prefix of a language decoder yield worse\nperformance compared to language-supervised vision mod-\nels like CLIP ViT [60]. Unfreezing the parameters of the\ndiffusion backbone slightly improve the performance, but\nit still cannot match the CLIP encoder. This suggests that\nthe representation from image diffusion models is not di-\nrectly transferrable to the text embedding space where the\n7\ndecoder-only language model operates on. Instead of lever-\naging a separate language decoder, we use the text branch in\nthe MM-DiT architecture to directly model the conditional\ntext distribution, which notably boost the performance. This\nuncovers an intriguing property of MM-DiT model or po-\ntentially other bi-directional Transformers, that these mod-\nels are good representation learners for estimating the like-\nlihood of multi-modality data distribution.\nWe also conduct an ablation study with respect to the\nnumber of text diffusion sampling steps and study its influ-\nence on VQA accuracy with VQAv2 and captioning qual-\nity on COCO dataset. For VQAV2, which involves short\ntext answers, good accuracy can be achieved with relatively\nfew sampling steps. For the captioning task on MS-COCO,\nperformance improves as the number of sampling steps in-\ncreases, mirroring the trend observed by Sahoo et al. [65],\nwhere additional sampling steps lead to reduced perplexity.\nVision\nEncoder\nLanguage\nDecoder\nVQAv2 (val)\n0-shot\n32-shot\nSD3 feature (frozen)\nGPT 2\n42.3\n46.9\nSD3 feature (trainable)\nGPT 2\n45.1\n50.2\nCLIP ViT L\/14 (frozen)\nGPT 2\n50.6\n54.8\nUniDiffuser [7]\nGPT 2*\n46.7\n49.4\nD-DiT (ours)\n-\n55.0\n60.3\nTable 4. Comparison between different vision encoder and pro-\nposed model. *The GPT 2 decoder of UniDiffuser is finetuned on\ntext reconstruction and kept frozen afterwards.\nTask\nT = 4\n8\n16\n32\n64\n128\nVQAV2 (acc.)\n58.8\n58.0 59.3 60.5 60.0 59.6\nMS-COCO (CIDEr)\n20.2\n35.3 46.5 51.3 56.2 54.5\nTable 5. An ablation study on the effect of sampling steps T on\ndiscrete text diffusion performance in terms of COCO Captioning\nCIDEr score and VQAV2 subset’s question-answering accuracy.\n5. Related Works\n5.1. Diffusion Models\nDiffusion models [29, 67, 68] generate data by gradually\nconverting noise into signal via a reverse diffusion pro-\ncess. They are the de facto standard for image generation\n[14, 32] and likelihood modeling [33, 38, 55, 69]. Con-\nditional diffusion models [28] have also been shown to\nbe powerful interfaces bridging text and images, particu-\nlarly for their ability to generate highly realistic and aes-\nthetic images from textual descriptions [10, 22, 57, 62–64].\nTheir exceptional performance in the image domain has\nalso inspired numerous extensions to the language gener-\nation [12, 15, 23, 27, 41, 51, 52, 65, 66], and is an attractive\nalternative as its sampling is not constrained by a specified\ntoken generation order and the attention mechanism does\nnot need to be uni-directional.\n5.2. Vision Language Models\nThe success of large language models (LLMs) [9, 76] and\nvision-language pretraining [60] has given rise to a series\nof multi-modal language models. The visual signal is pro-\njected to the text embedding space via vision encoders su-\npervised by text labels [60, 84] and then connected to a pre-\ntrained language models through further instruction tuning\n[2, 13, 47, 75, 86]. While these models have shown promis-\ning capabilities in image understanding and few-shot gen-\neralization, their predictive targets are inherently language-\ncentric, limiting their ability to model the image distribu-\ntions directly.\n5.3. Multimodal Text and Image Generative Models\nRather than simply connecting visual encoders to language\nmodels, recently there has been an active line of inquiry\nfocused on exploring a unified generative model for joint\nvision and language generation. Inspired by autoregressive\nlanguage models, many of the unified multi-modal genera-\ntive models extend the next-token prediction to both image\nand text tokens [16, 24, 70, 72, 78]. More recently, Transfu-\nsion [85] and Show-O [80] demonstrate that bi-directional\nimage diffusion can be integrated with autoregressive text\nprediction in the same framework. On the other hand, Ver-\nsatile Diffusion [81] and Uni-diffuser [7] explore applying\na continuous diffusion process to text and image modali-\nties, where text generation is broken into two stages - first,\ncontinuous diffusion is used to generate latent embeddings\nwhich are then decoded into text by another LLM (e.g.\nGPT2 [58]). While these works hint at the potential of diffu-\nsion models as efficient multi-modal models, their text gen-\neration capability is restricted to simple tasks like generat-\ning short captions from images.\nConclusion and Discussion\nIn this work, we introduced an end-to-end multi-modal dif-\nfusion model that bridges the gap between text and im-\nage diffusion by enabling both text-to-image (T2I) and\nimage-to-text (I2T) tasks through a unified diffusion model.\nWe demonstrated that a bi-directional transformer trained\nwith a joint diffusion target is an effective multi-modal\nlearner capable of competing with the autoregressive mod-\nels that have long dominated the field. Additionally, the bi-\ndirectional attention mechanism is equivariant to the order\nof input tokens, enabling the prediction of conditional dis-\ntributions without requiring a specific arrangement of dif-\nferent modalities or special handling of the attention mask.\nLimitations\nWhile discrete diffusion offers the advantage\nof being agnostic to sequential order and is compatible with\n8\nbi-directional Transformers, its current implementation re-\nquires the sequence length to be preset before sampling. A\npromising future direction would be to extend the sampling\nscheme to allow for more flexibility, enabling dynamic se-\nquence lengths during the sampling process. In addition,\nwhile we show that our proposed dual diffusion model can\nperform instruction tuning, its instruction-following capa-\nbilities still marginally lag behind those of state-of-the-art\nautoregressive models.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 1\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:\na visual language model for few-shot learning.\nAdvances\nin neural information processing systems, 35:23716–23736,\n2022. 7, 8\n[3] Brian DO Anderson. Reverse-time diffusion equation mod-\nels. Stochastic Processes and their Applications, 12(3):313–\n326, 1982. 2\n[4] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tar-\nlow, and Rianne Van Den Berg. Structured denoising dif-\nfusion models in discrete state-spaces. Advances in Neural\nInformation Processing Systems, 34:17981–17993, 2021. 1,\n2\n[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith,\nPang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and\nLudwig Schmidt.\nOpenflamingo: An open-source frame-\nwork for training large autoregressive vision-language mod-\nels, 2023. 7\n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond, 2023. 7\n[7] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu,\nYaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu.\nOne transformer fits all distributions in multi-modal diffu-\nsion at scale. In International Conference on Machine Learn-\ning, pages 1692–1717. PMLR, 2023. 1, 2, 6, 8, 3\n[8] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-\ntle, Andrew Miller, Robert C Miller, Robin Miller, Aubrey\nTatarowicz, Brandyn White, Samual White, et al. Vizwiz:\nnearly real-time answers to visual questions. In Proceedings\nof the 23nd annual ACM symposium on User interface soft-\nware and technology, pages 333–342, 2010. 6\n[9] Tom B Brown.\nLanguage models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020. 8\n[10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. Pixart-α: Fast training of dif-\nfusion transformer for photorealistic text-to-image synthesis,\n2023. 6, 8, 1\n[11] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 5, 7\n[12] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog\nbits: Generating discrete data using diffusion models with\nself-conditioning. arXiv preprint arXiv:2208.04202, 2022.\n2, 8\n[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 8\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems, 34:8780–8794, 2021. 8\n[15] Sander Dieleman, Laurent Sartran, Arman Roshannai, Niko-\nlay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud\nDoucet, Robin Strudel, Chris Dyer, Conor Durkan, et al.\nContinuous diffusion for categorical data.\narXiv preprint\narXiv:2211.15089, 2022. 1, 2, 8\n[16] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng\nMa, and Li Yi. DreamLLM: Synergistic multimodal com-\nprehension and creation. In The Twelfth International Con-\nference on Learning Representations, 2024. 1, 8\n[17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024. 1\n[18] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE\/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 3\n[19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn Forty-first International Conference on Machine Learn-\ning, 2024. 1, 2, 3, 4, 6\n[20] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 6\n[21] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-\ngad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek\nRamanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,\nPang Wei Koh, Olga Saukh, Alexander Ratner, Shuran\n9\nSong, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,\nVaishaal Shankar, and Ludwig Schmidt.\nDatacomp: In\nsearch of the next generation of multimodal datasets, 2023.\n5\n[22] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen,\nRuoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang,\net al. Lumina-t2x: Transforming text into any modality, reso-\nlution, and duration via flow-based large diffusion transform-\ners. arXiv preprint arXiv:2405.05945, 2024. 1, 8\n[23] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ\nChen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman.\nDiscrete flow matching. arXiv preprint arXiv:2407.15595,\n2024. 3, 8\n[24] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 1, 6, 8\n[25] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment, 2023. 6\n[26] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the V in VQA matter: Ele-\nvating the role of image understanding in Visual Question\nAnswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 6\n[27] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-\nbased diffusion language models. Advances in Neural Infor-\nmation Processing Systems, 36, 2024. 2, 8\n[28] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4, 8\n[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840–6851, 2020. 2, 8\n[30] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019. 6\n[31] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip\nIsola. The platonic representation hypothesis. arXiv preprint\narXiv:2405.07987, 2024. 1\n[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. arXiv preprint arXiv:2206.00364, 2022. 8\n[33] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. Advances in neural infor-\nmation processing systems, 34:21696–21707, 2021. 8\n[34] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale. Interna-\ntional journal of computer vision, 128(7):1956–1981, 2020.\n5\n[36] Hugo Laurenc¸on, Lucile Saulnier, L´eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M. Rush, Douwe Kiela,\nMatthieu Cord, and Victor Sanh. Obelics: An open web-\nscale filtered dataset of interleaved image-text documents,\n2023. 7\n[37] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Lin-\nmiao Xu, and Suhail Doshi. Playground v2. 5: Three in-\nsights towards enhancing aesthetic quality in text-to-image\ngeneration. arXiv preprint arXiv:2402.17245, 2024. 1\n[38] Henry Li, Ronen Basri, and Yuval Kluger. Likelihood train-\ning of cascaded diffusion models via hierarchical volume-\npreserving maps. In The Twelfth International Conference\non Learning Representations, 2024. 8\n[39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 1, 7\n[40] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and\nKaiming He. Autoregressive image generation without vec-\ntor quantization. arXiv preprint arXiv:2406.11838, 2024. 1\n[41] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,\nand Tatsunori B Hashimoto. Diffusion-lm improves control-\nlable text generation. Advances in Neural Information Pro-\ncessing Systems, 35:4328–4343, 2022. 2, 8\n[42] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,\nand Tatsunori B Hashimoto. Diffusion-lm improves control-\nlable text generation. Advances in Neural Information Pro-\ncessing Systems, 35:4328–4343, 2022. 1\n[43] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen\nZhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu,\nHuangjie Zheng, et al.\nWhat if we recaption billions of\nweb images with llama-3? arXiv preprint arXiv:2406.08478,\n2024. 5\n[44] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.\n6\n[45] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matt Le. Flow matching for generative mod-\neling. arXiv preprint arXiv:2210.02747, 2022. 2\n[46] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning, 2024. 7\n[47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 1, 8\n[48] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nblockwise ringattention, 2024. 6, 1\n[49] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 7\n[50] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. In The Eleventh International Conference on Learning\nRepresentations, 2023. 2\n10\n[51] Aaron Lou, Chenlin Meng, and Stefano Ermon.\nDiscrete\ndiffusion language modeling by estimating the ratios of the\ndata distribution. arXiv preprint arXiv:2310.16834, 2023. 2,\n3, 8\n[52] Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekht-\nman, and Kilian Q Weinberger. Latent diffusion for language\ngeneration. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 2, 8\n[53] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE\/cvf conference on computer vision and pattern\nrecognition, pages 3195–3204, 2019. 6\n[54] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Er-\nmon. Concrete score matching: Generalized score matching\nfor discrete data. Advances in Neural Information Process-\ning Systems, 35:34532–34545, 2022. 2\n[55] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nconference on machine learning, pages 8162–8171. PMLR,\n2021. 8\n[56] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE\/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 3\n[57] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 6, 8, 1\n[58] Alec Radford and Karthik Narasimhan. Improving language\nunderstanding by generative pre-training. 2018. 8\n[59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 7, 8\n[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer, 2023. 4\n[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 6, 8\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 3\n[64] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in neural information\nprocessing systems, 35:36479–36494, 2022. 1, 8\n[65] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron\nGokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush,\nand Volodymyr Kuleshov. Simple and effective masked dif-\nfusion language models. arXiv preprint arXiv:2406.07524,\n2024. 2, 3, 8\n[66] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and\nMichalis K. Titsias. Simplified and generalized masked dif-\nfusion for discrete data, 2024. 3, 8\n[67] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256–2265. PMLR, 2015.\n2, 8\n[68] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2, 8\n[69] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.\nMaximum likelihood training of score-based diffusion mod-\nels. Advances in Neural Information Processing Systems, 34:\n1415–1428, 2021. 8\n[70] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 1, 8\n[71] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and\nMohit Bansal. Any-to-any generation via composable diffu-\nsion. Advances in Neural Information Processing Systems,\n36, 2024. 6\n[72] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n1, 6, 7, 8\n[73] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 1\n[74] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang.\nVisual autoregressive modeling: Scalable im-\nage generation via next-scale prediction.\narXiv preprint\narXiv:2404.02905, 2024. 1\n[75] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun\nWoo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-\n1: A fully open, vision-centric exploration of multimodal\nllms. arXiv preprint arXiv:2406.16860, 2024. 1, 7, 8\n[76] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 8\n[77] Pascal Vincent. A connection between score matching and\ndenoising autoencoders. Neural computation, 23(7):1661–\n1674, 2011. 2\n11\n[78] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is\nall you need. arXiv preprint arXiv:2409.18869, 2024. 1, 8\n[79] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang,\nDacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model inte-\ngrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024. 1\n[80] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One\nsingle transformer to unify multimodal understanding and\ngeneration. arXiv preprint arXiv:2408.12528, 2024. 1, 2,\n6, 7, 8\n[81] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,\nand Humphrey Shi. Versatile diffusion: Text, images and\nvariations all in one diffusion model. In Proceedings of the\nIEEE\/CVF International Conference on Computer Vision,\npages 7754–7765, 2023. 1, 2, 6, 8\n[82] Lijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Ver-\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh\nBirodkar, Agrim Gupta, Xiuye Gu, et al. Language model\nbeats diffusion–tokenizer is key to visual generation. arXiv\npreprint arXiv:2310.05737, 2023. 1\n[83] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, Candace Ross, Adam Polyak, Rus-\nsell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan,\nOron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang,\nRichard James, Gargi Ghosh, Yaniv Taigman, Maryam\nFazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Ar-\nmen Aghajanyan. Scaling autoregressive multi-modal mod-\nels: Pretraining and instruction tuning, 2023. 1, 7\n[84] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training,\n2023. 8\n[85] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024. 1, 2, 6, 7, 8\n[86] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 8\n12\nDual Diffusion for Unified Image Generation and Understanding\nSupplementary Material\n6. Training Details\nHyperparam.\nDual pretrain\nContinued pretrain\nInstruct. tuning\nMask emb.\nHigh res.\nGradient steps\n60k\n200k\n80k\n25k\nBatch size\n512\n512\n768\n512\nLR\n5e-5\n3e-5\n3e-5\n3e-5\nScheduler\nConstant LR with warmup\nWarmup iters\n5000\n1000\n1000\n1000\nWeight decay\n1e-2\nText loss weight\n0.2\n0.3\nTable 6. Training hyperparameters for D-DiT. Text loss weight\ndenotes the λ in (8).\nWe provide the detailed hyperparameter setting for dif-\nferent training stages in the Table 6. During all the training\nstages, we use AdamW optimizer with default hyperparam-\neters (β1 = 0.9, β2 = 0.999). Mixed precision training\n(bf16) and fully-sharded data parallel (with gradient and op-\ntimizer state sharded) are used for model training.\n7. Further Results\nModel\nBackbone\nParams. (B)\nFID ↓\nSD-XL [57]\nDiff.\n0.9\n9.55\nPixArt-α [10]\nDiff.\n0.6\n6.14\nPlayground v2.5\nDiff.\n-\n4.48\nShow-O [80]\nDiscrete Diff.\n1.3\n15.18\nLWM [48]\nAR\n7\n17.77\nVILA-U [79]\nAR\n7\n7.69\nSD3 [19]\nDiff.\n2\n16.45\nD-DiT\nDiff.\n2\n15.16\nTable 7. Comparison with other models on MJHQ-30K evaluation\nbenchmark at 512 × 512 resolution.\nWe evaluate the aesthetic quality of generated images\nfrom our proposed D-DiT against those of the original SD3\nmodel and a selection of existing text-to-image (T2I) and\nmulti-modal works.\nWe measure Frechet Inception Dis-\ntance (FID) with respect to a collection highly aesthetic\ngenerated images, known as the MJHQ-30K benchmark\nproposed by [37]. As shown in Table 7, we observe an\nimprovement in FID after joint diffusion training, and fa-\nvorable comparison against multi-modal models of similar\nsize.\nWe provide an illustrative example of masked diffusion\nin Figure 5 for the visual question answering task, where the\ntoken generation process is visualized over diffusion time.\nOver the course of sampling, the answer tokens are grad-\nually denoised from the masked state via absorbing state\nreverse diffusion. The question tokens are always left un-\nmasked throughout the entire process.\nWe also include a qualitative comparison in caption-\ning performance compared to UniDiffuser [7], another\ndiffusion-based multi-modal model, in Figure 6, where we\ndemonstrate an improvement in the ability to capture fine-\ngrained details of the image in a longer caption format. Fi-\nnally, we provide further uncurated text-to-image (T2I) gen-\neration results in Figures 7, 8, 9, and 10. Overall, these\nresults further demonstrate the multi-faceted performance\nof our proposed dual-branch diffusion-based multi-modal\nmodel.\n1\nQ: Provide a brief description of the given image. A: [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK [MASK] [MASK] [MASK]\nQ: Provide a brief description of the given image. A: [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] beautiful [MASK] [MASK] taken \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] to [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK] [MASK] of [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] town [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK] [MASK]. [MASK] [MASK]' [MASK] [MASK]ray [MASK] illuminate [MASK] [MASK] [MASK] [MASK] [MASK]a warm [MASK] [MASK] [MASK] \n[MASK] [MASK]zure [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]<\/s> [MASK]<\/s> [MASK] [MASK] \n[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]<\/s> [MASK] [MASK] [MASK] [MASK] [MASK] \n[MASK]<\/s> [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]<\/s><\/s> [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]<\/s><\/s> [MASK]\nQ: Provide a brief description of the given image. A: [MASK] image [MASK] a serene and beautiful [MASK] scene taken [MASK] [MASK] [MASK] \n[MASK] [MASK], which appears to be a [MASK] [MASK] [MASK] From [MASK] wooden balcony [MASK] [MASK] of a clear [MASK] [MASK] where \nthe snowy town [MASK] the base [MASK] [MASK] horizon. [MASK] [MASK]' [MASK] [MASK]ray [MASK] illuminate the landscape [MASK] casting \n[MASK]a warm glow [MASK] the azure backdrop [MASK]<\/s><\/s><\/s> [MASK] [MASK]<\/s><\/s> [MASK]<\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s> \n[MASK]<\/s><\/s> [MASK] [MASK]<\/s> [MASK] [MASK]<\/s> [MASK]<\/s><\/s><\/s> [MASK]<\/s> [MASK]<\/s><\/s> [MASK]<\/s><\/s><\/s><\/s> [MASK] \n[MASK]<\/s><\/s><\/s> [MASK] [MASK]<\/s> [MASK]<\/s><\/s><\/s><\/s>\nQ: Provide a brief description of the given image. A: [MASK] image [MASK] a serene and beautiful [MASK] scene taken [MASK] [MASK] [MASK] \n[MASK] [MASK], which appears to be a [MASK] [MASK] [MASK] From [MASK] wooden balcony [MASK] [MASK] of a clear [MASK] [MASK] where \nthe snowy town [MASK] the base [MASK] [MASK] horizon. [MASK] [MASK]' [MASK] [MASK]ray [MASK] illuminate the landscape [MASK] casting \n[MASK]a warm glow [MASK] the azure backdrop [MASK]<\/s><\/s><\/s> [MASK] [MASK]<\/s><\/s> [MASK]<\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s> \n[MASK]<\/s><\/s> [MASK] [MASK]<\/s> [MASK] [MASK]<\/s> [MASK]<\/s><\/s><\/s> [MASK]<\/s> [MASK]<\/s><\/s> [MASK]<\/s><\/s><\/s><\/s> [MASK] \n[MASK]<\/s><\/s><\/s> [MASK] [MASK]<\/s> [MASK]<\/s><\/s><\/s><\/s>\nQ: Provide a brief description of the given image. A: The image presents a serene and beautiful winter scene taken from a vantage point, \nwhich appears to be a mountain range. From the wooden balcony in front of a clear blue sky where the snowy town at the base meets the \nhorizon. The sun's rays illuminate the landscape, casting a warm glow against the azure backdrop. \n<\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\n\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s><\/s>\nt=1\nt=0\nFigure 5. Illustrative example of visual question answering with mask diffusion.\n2\nD-DiT: The image features a phone held up in an interesting \nangle, standing on a surface.\nUniDiffuser: a white iPhone sitting on top of a stand\nD-DiT: In the image, the saucer and cup is laid horizontally on \none of the mats.\nUniDiffuser: A set of three blue and white striped napkins\nD-DiT: In the image, there are three baseball players, all of \nwhich are all dressed in white uniforms. The first man appears \nto be cheering to hit the ball. The other two players, possibly his \nteammates or fielders, are in different positions on the field.\nUniDiffuser: Jonny Bairstow of Australia celebrates after taking \nthe wicket\nD-DiT: The image captures a captivating view of a outdoor \nconcert with a glow of night. The concert is taking place at dusk \nand features a large stage with colored purple lights, creating a \nstunning visual and vibrant setting. A crowd can be seen sitting \naround the area, enjoying the musical performance on the \nstage. The balkan-ish skies of the evening sunset adds warmth \nto the scene, further enhancing the concert atmosphere.\nUniDiffuser: A large crowd of people on stage at a concert\nD-DiT: The image shows a woman walking down a runway in \nher model outfit. The outfit includes a coat, a book, a skirt, and \na purse or handbag. She is also wearing tall boots.\nUniDiffuser: A model walks down the runway in a beige coat \nand boots\nFigure 6. Comparison of captions generated by D-DiT and UniDiffuser[7]. The prompt to D-DiT is ”Provide a brief description of the\ngiven image.”\n3\nFigure 7. Additional text-to-image samples generated from the model.\n4\nFigure 8. Additional text-to-image samples generated from the model.\n5\nFigure 9. Additional text-to-image samples generated from the model.\n6\nFigure 10. Additional text-to-image samples generated from the model.\n7\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Dual Diffusion for Unified Image Generation and Understanding.pdf"}
{"title":"LMFusion: Adapting Pretrained Language Models for Multimodal Generation","authors":"Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu","summary":"We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.","url":"http:\/\/arxiv.org\/abs\/2412.15188v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.15188v4","published":1734634584000,"comment":"Name change: LlamaFusion to LMFusion","pdf_text":"LMFusion: Adapting Pretrained Language Models\nfor Multimodal Generation\nWeijia Shi1,∗, Xiaochuang Han 1,∗, Chunting Zhou, Weixin Liang3, Xi Victoria Lin2, Luke Zettlemoyer1,2,\nLili Yu2\n1University of Washington, 2FAIR at Meta, 3Stanford University\n∗Joint first author. Order randomly determined. Work done while at Meta.\nWe present LMFusion, a framework for empowering pretrained text-only large language models (LLMs)\nwith multimodal generative capabilities, enabling them to understand and generate both text and\nimages in arbitrary sequences. LMFusion leverages existing Llama-3’s weights for processing texts\nautoregressively while introducing additional and parallel transformer modules for processing images\nwith diffusion. During training, the data from each modality is routed to its dedicated modules:\nmodality-specific feedforward layers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow interactions across text and image\nfeatures. By freezing the text-specific modules and only training the image-specific modules, LMFusion\npreserves the language capabilities of text-only LLMs while developing strong visual understanding and\ngeneration abilities. Compared to methods that pretrain multimodal generative models from scratch,\nour experiments demonstrate that, LMFusion improves image understanding by 20% and image\ngeneration by 3.6% using only 50% of the FLOPs while maintaining Llama-3’s language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language models with multimodal\ngeneration ability. Overall, this framework not only leverages existing computational investments\nin text-only LLMs but also enables the parallel development of language and vision capabilities,\npresenting a promising direction for efficient multimodal model development.\nCorrespondence: Weijia Shi swj0419@uw.edu, Xiaochuang Han xhan77@uw.edu, Lili Yu liliyu@meta.com\nDate: February 6, 2025\nFigure 1 Overview of LMFusion. It uses modality-specific FFNs and QKV projections to process text and image data\nseparately: the text “A cat with secrets to keep” goes to the text module , while the image patches of the cat goes\nto the image module. In the self-attention layer, text and image representations can attend to all previous contexts\nacross the modality boundaries. Both modules are initialized from Llama-3, with the text module frozen to preserve\nlanguage capabilities while the image module trained on image data. Layer normalization and residual connections are\nfolded into the QKV and FFN modules. A special BOI token separates different modalities in the sequence.\n1\narXiv:2412.15188v4  [cs.CL]  5 Feb 2025\nAn afrofuturist lady wearing\ngold jewelry\nBeautiful oil painting of a\nsteamboat in a rive\nTurtle swimming underwater,\naesthetic, fantasy\nAn espresso machine that makes\ncoﬀee from human souls\nA 1960s yearbook photo with\nanimals dressed as humans.\nA woman on a bed underneath a\nblanket\nThe word ’start’ on a blue\nt-shirt\nAn old man with green eyes and\na long grey beard\nA raccoon main character in an\nanime\nA bread, an apple, and a knife\non a table\nAn elephant walking out of a\nfridge\nA space elevator in the\nuniverse\nA beaver dressed in a vest,\nwearing glasses\nA close up photo of a human\nhand\nThree spheres made of glass\nfalling into ocean\nFigure 2 Generated images from LMFusion fine-tuned on aesthetically appealing images for improved quality.\n1\nIntroduction\nOver the past few years, we have seen significant progress in multimodal generative models capable of\nunderstanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023; Koh\net al., 2024; Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and\nUnified-IO (Lu et al., 2022, 2024) demonstrate the potential of unified architectures that seamlessly handle\nboth image and text modalities. However, these models typically train from scratch, demanding significant\ncomputational resources to achieve proficiency across all modalities. The computational cost of mastering\neven a single modality is substantial—training a state-of-the-art text-only large language models (LLMs) like\nLlama-3 (Dubey et al., 2024) requires training over 15 trillion tokens.\nGiven these computational demands, we investigate an alternative paradigm that reuses and adapts existing\npretrained LLMs (Ge et al., 2023; Sun et al., 2023; Wu et al., 2024b). We address a fundamental research\nquestion: How to preserve the text-only performance of pretrained LLMs while equipping them with visual\nunderstanding and generation abilities? Our experiments show that naive finetuning of pretrained text-only\nLLMs on multimodal data leads to significant degradation of their language processing capabilities.\nTo address this challenge, we introduce LMFusion, a framework that enhances a pretrained text-only LLM,\nLlama-3 (Dubey et al., 2024) with multimodal capabilities by building upon the recipe of Transfusion (Zhou\net al., 2024). Drawing from recent and parallel work on modality separation (Shen et al., 2023; Chen et al.,\n2023; Liang et al., 2024; Liu et al., 2024a), LMFusion integrates the original Llama modules pretrained for\nlanguage processing while introducing additional dedicated transformer modules for visual understanding and\ngeneration tasks. As shown in Figure 1, we employ modality-specific query-key-value (QKV) projections and\nfeed-forward networks (FFNs) to process text and image data separately while still allowing for cross-modal\ninteractions in the joint self-attention layer. By freezing the text modules while finetuning the image modules,\n2\nwe preserve the language-only capabilities of pretrained LLMs while giving a head start to the learning of\nvisual understanding and generation. Compared to pretraining multimodal generative models from scratch,\nthis approach avoids the need to include text-only data in the training process, significantly reducing the\ncomputational demands.\nTo evaluate the effectiveness of our approach, we conduct comprehensive experiments comparing LMFusion\nwith Transfusion in controlled settings. Specifically, we initialize our LMFusion architecture with a pretrained\nLlama-3 8B model (Dubey et al., 2024) and continue training on the same image data as in Transfusion (Zhou\net al., 2024). Compared to Transfusion, LMFusion achieves a 20% improvement in image understanding, 3.6%\nimprovement in image generation while using only 50% of the FLOPs. It also preserves Llama-3’s text-only\nperformance that outperforms Transfusion by 11.6%. Figure 2 presents images generated by LMFusion.\nAdditionally, we further demonstrate that this framework can adapt existing vision-language models (e.g.,\nLLaVA) with multimodal generation ability.\nThrough ablation studies, we analyze the key architectural decision for LMFusion: separating both self-\nattention and FFNs for different modality data while freezing weights for the pretrained language modality.\nWe show that naive finetuning of the dense pretrained LLMs on multimodal data (no separation) leads to a\ncatastrophic forgetting of their original language capabilities. Furthermore, deep separation proves to be more\neffective than shallow separation (using modality-specific FFNs only), with both approaches outperforming\nmodels with no separation.\nOverall, LMFusion has the following key features: (1) Compute reuse: It leverages existing computational\ninvestments in text-only LLMs when developing multimodal generative models. This eliminates the need to\nretrain on text-only data, significantly reducing computational demands. (2) Performance preservation and\ntransfer: It completely preserves the strong text-only performance of pretrained LLMs and facilitates a better\nlearning of image understanding and generation in the multimodal generative setup.\n2\nBackground: Transfusion\nTransfusion (Zhou et al., 2024) is a single unified multimodal model that is capable of text generation, image\nunderstanding, and image generation tasks, by jointly predicting next tokens in language and diffusing image\nrepresentations. Given a multimodal input (xtxt, ximg), the Transfusion model jointly learns to do language\nmodeling (§2.1) on xtxt and image diffusion (§2.2) on ximg. Its architecture is same as a standard Transformer\n(Vaswani et al., 2017) with an additional U-Net structure (Ronneberger et al., 2015) that projects image\nrepresentations down and up before and after diffusion.\n2.1\nLanguage Modeling\nGiven a sequence of discrete language tokens xtxt = xtxt\n1 , . . . , xtxt\nN , a language model θ represents its joint\nprobability by P(xtxt) = QN\ni=1 Pθ(xtxt\ni\n| xtxt\n<i). This formulation sets up an autoregressive task, where each\ntoken xtxt\ni\nis predicted based on its preceding tokens xtxt\n<i. The language model is learned by minimizing the\ncross-entropy between Pθ and the observed data distribution, which is commonly referred to as the LM loss:\nLLM = Extxt\ni [−log Pθ(xtxt\ni\n| xtxt\n<i, ximg)]\n(1)\nOptionally, if there exists image data preceding the language tokens (e.g., image-caption data), Transfusion\nadds the representation of ximg as additional condition to the objective. More details of representing ximg are\npresented below.\n2.2\nImage Diffusion\nGiven a raw image, Transfusion first encodes the image into a sequence of continuous latent representation\nximg with a pretrained and frozen VAE tokenizer (Kingma, 2013). It then employs Denoising Diffusion\nProbabilistic Models (i.e., DDPM) to learn to reverse a gradual noise-addition process added in the forward\nprocess (Ho et al., 2020). In the forward diffusion process, a Gaussian noise ϵ ∼N(0, I) is added to the\n3\nimage representation ximg over T steps, creating a sequence of noisy image representations x0, x1, ..., xT .\nSpecifically, at each step t, the noisy image representation is given by:\nximg\nt\n= √¯αtximg +\n√\n1 −¯αtϵ\n(2)\nHere ¯αt follows a common cosine schedule (Nichol and Dhariwal, 2021).\nIn the reverse process, the diffusion model ϵθ(·) with parameters θ learns to predict the added noise ϵ given\nthe noisy data ximg\nt\nat timestep t and a context xtxt that can include text prompts such as captions to the\nimage diffusion: 1\nLDDPM = Eximg,t,ϵ[∥ϵ −ϵθ(ximg\nt\n, t, xtxt)∥2\n2]\n(3)\nThe Transfusion architecture contains U-Net downsampler and upsampler to reduce the dimension of ximg.\nThe U-Net downsampler transforms the image into fewer patches before the main Transformer modules while\nthe upsampler projects them back to the original dimension of ximg after the Transformer.\n2.3\nTraining Objective\nDuring training, Transfusion is optimized to predict both the LM loss on the text input xtxt and the diffusion\nloss on the image input ximg. These two losses are combined using a hyperparameter λ:\nLTransfusion = LLM + λ · LDDPM\n(4)\n3\nLMFusion\nOne notable feature of Transfusion is that it has the same architecture as mainstream LLMs (e.g., Llama\n(Touvron et al., 2023)) while being capable of text generation, image understanding, and image generation\ntogether, through an end-to-end training (Equation 4). Zhou et al. (2024) trains Transfusion from scratch using\nlanguage-only and image-caption data. However, such training from scratch requires substantial computational\nresources, and its performance on language-only tasks still lags behind the pretrained, text-only LLMs.\nIn this work, we aim to effectively adapt pretrained, text-only LLMs to handle image understanding and\ngeneration tasks. Specifically, we build on an open-weight LLM, Llama-3 (Dubey et al., 2024), and continue\ntraining it with the Transfusion objectives to handle both modalities. Since Transfusion uses shared parameters\nfor its language modeling and image diffusion objectives, the key challenge is to prevent Llama-3’s strong\ntext-only performance from dropping while optimizing for its new image capabilities.\n3.1\nModel Architecture\nIn response to the challenge above, we propose LMFusion, a framework that combines a pretrained, text-only\nLlama model with a dedicated image transformer for visual generation and understanding, enabling each\nmodality to be processed through independent weights. By freezing the text modules while finetuning the\nvisual modules, we preserve its language-only capabilities while giving the learning of visual understanding\nand generation a boost start.\nLMFusion is a decoder-only model consisting of N transformer layers.\nAs shown in Figure 1, central\nto the design are the modality-specific attention layer and Feed-Forward Network (FFN), each handling\nonly data from its corresponding modality. Without loss of generality, we describe LMFusion below in a\nconfiguration with a single transformer layer, folding residual connections and layer normalization directly\ninto the self-attention and FFN. The inputs to the model are text tokens xtxt and noisy image representations\nximg\nt\n= √¯αtximg + √1 −¯αtϵ. We use blue for text-specific modules and red for image-specific modules.\n1Similar to xtxt, this context can also include image representations ximg under an image editing setup. We omit it in the\nnotation for simplicity.\n4\nInput projection\nThe input text tokens xtxt are projected by a linear embedding layer to a sequence of text\nhidden states htxt\nin . The noisy image ximg\nt\nare projected to a sequence of image representations himg\nin\nvia a\nU-Net downsampler.\nhtxt\nin = Projtext (xtxt)\n(5)\nhimg\nin\n=UNet-Downimg (ximg\nt\n, t)\n(6)\nThen the text hidden states htxt\nin or image hidden states himg\nin\nare fed into the following attention layer.\nModality-specific self-attention\nWe create separate attention matrices for each modality. Specifically, the\ntext hidden states htxt\nin and image hidden states himg\nin\nare converted into their respective queries, keys, and\nvalues via separate Q, K, V matrices. The pre-attention layer normalization is also modality-specific and is\nfolded into the QKV functions.\nhtxt\nQ , htxt\nK , htxt\nV = QKVtext (htxt\nin )\n(7)\nhimg\nQ , himg\nK , himg\nV\n=QKVimg (himg\nin )\n(8)\nWe enable cross-modal attention by concatenating the queries, keys, and values from both image and text\nmodalities into unified sequences. The attention-weighted values at text and image token positions are then\nprojected back into the hidden state dimension using separate O weights for each modality.\nhtxt\nO = Otext (softmax(\nhtxt\nQ [himg\nK\n◦htxt\nK ]T + M\n√\nd\n)[himg\nV\n◦htxt\nV ])\n(9)\nhimg\nO\n=Oimg (softmax(\nhimg\nQ [htxt\nK ◦himg\nK ]T + M\n√\nd\n)[htxt\nV ◦himg\nV ])\n(10)\nwhere ◦denotes concatenation. M represents a hybrid attention mask same as in Transfusion (Zhou et al.,\n2024) with a causal mask applied to text tokens and a bi-directional mask applied to image tokens. This\ndesign allows for self-attention within and across modalities, encouraging cross-modality integrations.\nModality-specific feed-forward network\nAfter the attention layer, we employ modality-specific FFNs to process\ntext and image data separately. The pre-FFN layer normalization is also modality-specific and is folded in the\nFFN functions.\nhtxt\nFFN = FFNtext (htxt\nO )\n(11)\nhimg\nFFN =FFNimg (himg\nO )\n(12)\nOutput projection\nFinally, after N layers of self-attention and FFNs, the resulting hidden states are projected\neither to logits in text via language model’s output layer, or to predicted noise in image via a U-Net upsampler.\nplogits = LM-Headtext (htxt\nFFN)\n(13)\nϵpred =UNet-Upimg (himg\nFFN, t, himg\nin )\n(14)\nSame as Transfusion, the output plogits and ϵpred are passed through the language modeling loss (Equation 1)\nand DDPM loss (Equation 3) respectively. All parameters in the text modules along with self-attention\nand FFN parameters in the image modules are initialized from the pretrained Llama model.\nDuring\noptimization, we decouple the learning rates for the text and image parameter groups: a text learning\nrate, ηtext, is used for { Projtext, QKVtext, Otext, FFNtext, LM-Headtext } , and an image learning rate, ηimg,\nfor {UNet-Downimg, QKVimg, Oimg, FFNimg, UNet-Upimg} . To preserve the model’s performance on text-only\nbenchmarks, we use ηtext = 0 (freezing text modules) for our main experiments and explore different\nconfigurations in §5.\n5\n4\nExperiments\nIn this section, we describe the details of our training setup (§4.1) and evaluation setup (§4.3). Results in §4.4\nshow that LMFusion outperforms Transfusion trained from scratch in the FLOPs match setting on text-only,\nimage understanding and generation benchmarks.\n4.1\nTraining Setup\nData\nFollowing Transfusion (Zhou et al., 2024), we use the same collection of 380M Shutterstock image-\ncaption data, where each image is center-cropped and resized to 256 × 256 pixels. We order the captions\nbefore images (i.e., emphasizing image generation conditioned on texts) 80% of the time, and order the images\nbefore captions for the rest.\nModel Details\nFor image tokenization, we use the same VAE encoder2 as Transfusion to compress an image\nof 256 × 256 pixels into a 32 × 32 × 8 tensor. These tensors are then passed into a 2-block U-Net downsampler\n(Ronneberger et al., 2015) to further reduce dimensions, resulting in a sequence of 256 patches (tokens). Both\ntext-specific and image-specific Transformer modules are initialized from the pretrained Llama-3 8B model\n(Dubey et al., 2024). The U-Net downsampler and a corresponding U-Net upsampler are trained from scratch,\ntogether containing 0.27 billion parameters. Like Transfusion, LMFusion uses a maximum context length of\n4096 tokens.\nOptimization\nIn our main experiments, to preserve the language-only performance, we freeze the text\nmodules (ηtext = 0) while training only the image modules using an AdamW optimizer (β1 = 0.9, β2 = 0.95,\nϵ = 1 × 10−8) with a learning rate ηimage = 1 × 10−4. The learning rate follows a cosine decay schedule with a\n4000-step warmup period before gradually decreasing to 1.5 × 10−5.\n4.2\nControlled Comparison with Transfusion\nOur key model comparisons are with the original Transfusion 7B model (Zhou et al., 2024),3 which was trained\nfor 250K steps on 0.25T language-only tokens (text data) and 0.25T image-captions tokens (image data).\nSince we freeze the text module during training, we can exclude text data from our training process while\nmaintaining language capabilities. This design choice allows us to explore two training configurations for a\ncontrolled comparison with Transfusion: In the first configuration, we match the amount of 0.25T image data\nused by Transfusion while leaving out the text data. As a result, this variant of LMFusion uses approximately\nhalf the total FLOPs of Transfusion. In the second configuration, we match Transfusion by using the same\ntotal FLOPs,\nAdditionally, for the language-only tasks, we report the performance of Llama-3 8B model to demonstrate\nthat our model is able to maintain its strong text performance.\n4.3\nEvaluation Setup\nFollowing Transfusion, we evaluate LMFusion on language-only, image understanding, and image generation\ntasks.\nLanguage-only\nWe evaluate the model’s language abilities using four tasks from the standard Llama evaluation\nsuite (Dubey et al., 2024), including Hellaswag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap\net al., 2019), and WinoGrande (Sakaguchi et al., 2021). We report accuracy on these benchmarks.\n2https:\/\/huggingface.co\/stabilityai\/sd-vae-ft-mse\n3Transfusion 7B and Llama-3 8B have the same Transformer sizes. The size difference is due to the different vocabularies, which\naffects input and output embedding layers.\n6\nLanguage-only\nImage\nImage Generation\nEvaluation\nUnderstanding\n(without | with CFG)\nModel\nRel. FLOPs\nHellaSwag ↑\nSIQA ↑\nWinoGrande ↑\nCIDEr ↑\nFID ↓\nCLIP ↑\nLlama 3\n-\n60.0\n48.1\n72.8\n–\n–\n–\nTransfusion\n1×\n51.0\n42.3\n64.3\n32.0\n14.4 | 8.70\n22.1 | 24.4\nLMFusion\n0.5×\n60.0\n48.1\n72.8\n38.3\n13.9 | 8.75\n22.0 | 24.4\nLMFusion\n1×\n60.0\n48.1\n72.8\n38.4\n14.0 | 8.61\n22.1 | 24.4\nTable 1 Results across text-only benchmarks, image understanding and image generation. LMFusion preserves Llama-3’s text\nperformance while adding strong image understanding and generation capabilities. Using only half of the total training\nFLOPs, it outperforms Transfusion across all tasks, with particularly notable improvements in image understanding and\ntext benchmarks, thanks to its initialization from Llama-3. Image generation results are obtained without classifier-free\nguidance (CFG) or with a CFG factor of 1.55.\nImage Understanding\nImage Generation\nModel\nBase LLM\nMMMU ↑\nChartQA ↑\nRealWorldQA ↑\nMME-P ↑\nFID ↓\nEMU-3\n–\n31.6\n51.8\n57.4\n–\n12.8\nShow-O\nPhil-1.5 1.3B\n27.4\n–\n–\n1435.7\n9.2\nJanus\nDeepSeek 1.3B\n30.5\n–\n–\n1338.0\n8.5\nChameleon\n–\n28.4\n0.0\n19.6\n–\n26.7\nMetaMorph\nLLaMA-3.1 8B\n41.8\n37.1\n58.3\n–\n11.8\nTransfusion\n–\n–\n–\n–\n–\n8.7\nLLaVAFusion\nLLaVA-Next 8B\n41.7\n69.5\n60.0\n1603.7\n8.2\nTable 2 Comparison of multimodal models across image understanding and generation capabilities. Models are evaluated on\nvarious image understanding benchmarks and image generation quality (FID). The models without base LLM are\npretrained from scratch.\nImage Generation\nFor evaluating image generation, we use the MS-COCO benchmark (Lin et al., 2014).\nWe generate images for 30K randomly selected prompts from the validation set and measure the Frechet\nInception Distance (FID) (Heusel et al., 2017) and CLIP scores (Radford et al., 2021). Our image generation\nresults include versions obtained without classifier-free guidance (CFG coefficient of 1.0) and with a CFG\ncoefficient of 1.55 or 1.6.\nImage Understanding\nWe evaluate the models’ ability to generate image descriptions using the test split of\nMS-COCO (Lin et al., 2014), reporting CIDEr scores (Vedantam et al., 2015).\n4.4\nResults\nTable 1 compares two variants of LMFusion against Transfusion. On language-only benchmarks, LMFusion\nkeeps the strong performance of Llama-3 since we freeze all text modules. For image understanding, LMFusion\nsubstantially surpasses Transfusion, with a 20% improvement. In image generation tasks, LMFusion also\nshows superior results in both FID and CLIP scores.\nIn Figure 3, we benchmark the performance of LMFusion and Transfusion throughout the training.4 We\nobserve a consistent advantage of LMFusion over Transfusion during the entire training, for image captioning\nand generation. These results suggest that LMFusion effectively leverages the pretrained language modules\nfrom Llama while developing strong image abilities. Although LMFusion has twice as many parameters as\nTransfusion, it uses same FLOPs since only half of the parameters are activated for each input token from an\narbitrary modality.\n4For the image generation results plotted throughout the training, we use a smaller subset of 5K prompts and without classifier-free\nguidance.\n7\nFigure 3 Evaluation of LMFusion and Transfusion during training. LMFusion keeps the text performance of Llama throughout\ntraining, while achieving better image understanding ability (CIDEr) and image generation quality (CLIP, FID).\n50\n100\n150\n200\n250\n5\n10\n15\n20\nCIDEr\n50\n100\n150\n200\n250\n14\n16\n18\n20\nCLIP\nlr ratio=0\nlr ratio=0.1\nlr ratio=1\n50\n100\n150\n200\n250\n20\n40\n60\n80\nFID\n50\n100\n150\n200\n250\n45\n50\n55\n60\nHellaSwag\nPerformance\nSteps (K)\nFigure 4 Performance of naive Llama-3 finetuning (no separation) with varying lr ratio\nηtext\nηimage . When directly finetuning the\nLlama-3 model for multimodal generation, using the same learning rate for both text and image components (lr ratio\n= 1) substantially reduces its text-only performance. Lowering the learning rate for the text component relative to the\nimage component (lr ratio < 1) helps preserve language performance but slows down the acquisition of multimodal\nabilities.\n5\nAnalysis\nCentral to LMFusion is our modality separation techniques, which employs the design of modality-specific\nmodules and decoupled learning rates for language and image modules. Our architectural ablation (§5.1)\ndemonstrates the importance of the design for maintaining model performance across both modalities.\nAdditionally, we showcase LMFusion’s ability to generalize to image-to-image generation through image editing\ntasks, which require simultaneous understanding of both input images and textual prompts (§5.2). We further\nshowcase that this recipe could be used for adapting\n5.1\nArchitecture Ablations\n5.1.1\nExperimental Design\nTo evaluate different design choices, we conduct ablation studies using small-scale variants of LMFusion. Our\nanalysis focuses on the impact of modality separation by comparing three designs: (1) no separation (a single\ndense model), (2) shallow separation (using modality-specific FFNs only), and (3) deep separation (using both\nmodality-specific FFNs and attention mechanisms, our final LMFusion).\nNo separation (dense model)\nWe begin our experiments with the dense Llama-3 8B model trained using\nthe Transfusion recipe. This dense model maintains a unified structure where most components are shared\nacross modalities (a single set of QKV, O and FFN process both texts and images), with the exception of\nU-Net upsampler and downsampler. For training, we use a text learning rate (ηtext) for the components\ninitialized from the text-only LLM { Projtext, QKV, O, FFN, LM-Headtext }, and an image learning rate ηimg\n8\n50\n100\n150\n200\n250\n5\n10\n15\n20\n25\nCIDEr\n50\n100\n150\n200\n250\n14\n16\n18\n20\nCLIP\nNo Separation\nShallow Separation\nDeep Separation\n50\n100\n150\n200\n250\n20\n40\n60\n80\nFID\n50\n100\n150\n200\n250\n58\n60\n62\nHellaSwag\nPerformance\nSteps (K)\nFigure 5 Performance of no separation (dense model), shallow separation (modality-specific FFNs only), and deep separation\n(modality-specific FFNs and attention) when text modules are frozen.\nDeep modality separation outperforms shallow\nseparation and no separation.\n50\n100\n150\n200\n250\n5\n10\n15\n20\n25\nCIDEr\n50\n100\n150\n200\n250\n14\n16\n18\n20\n22\nCLIP\nlr ratio=0\nlr ratio=0.1\nlr ratio=1\n50\n100\n150\n200\n250\n20\n40\n60\n80\nFID\n50\n100\n150\n200\n250\n45\n50\n55\nHellaSwag\nPerformance\nSteps (K)\nFigure 6 Performance of deep modality separation with varying lr ratios\nηtext\nηimage . When the text modules are frozen (lr ratio =\n0), deep separation preserves language capabilities and performs strongly on both image understanding and generation,\nunlike the dense models.\nfor {UNet-Downimg, UNet-Upimg}. To investigate the impact of learning rate decoupling, we experiment\nwith various learning rate ratios\nηtext\nηimage ∈{0, 0.1, 1}, with a constant image learning rate ηimage = 1 × 10−4, the\nsame as the main experiments. A ratio of 1 represents standard continual pretraining where all components\nshare the same learning rate, while a ratio of 0 indicates a complete freezing of text-related components.\nShallow separation (modality-specific FFNs only)\nWe explore a simplified variant of LMFusion that separates\nonly FFNs into text-specific and image-specific modules—a common approach in mixture-of-experts architec-\ntures (Lin et al., 2024b; Muennighoff et al., 2024). In this setup, we use a single shared attention mechanism\n(QKV , O) for processing both image and text data. For training, we employ separate learning rates: ηtext for\ntext-related components { Projtext, QKV, O, FFNtext, LM-Headtext } and ηimg for image-related components\n{Unet-Downimg, FFNimg, Unet-Upimg }. We experiment with various learning rate ratios\nηtext\nηimage ∈{0, 0.1, 1}.\nDeep separation (modality-specific FFNs and attention)\nOur LMFusion, as described in section 3, represents\na deep separation design where both FFNs and attention mechanisms are modality-specific. While our primary\nconfiguration freezes text modules during training, we also analyze the impact of different learning dynamics\nby varying the learning rate ratio\nηtext\nηimage across {0, 0.1, 1}.\nIn the ablation study, all models are trained for 250K training steps with a sequence length of 4,096 tokens\nand a batch size of 250K tokens. The training data comprised 0.03T text-only tokens and 0.03T image-caption\ntokens. All other hyperparameters remained consistent with those employed in our main experiments.\n5.1.2\nResults\nNaivefinetuningofdensepretrainedLLMsformultimodalgenerationcompromisestheiroriginallanguagecapabilities.\nWhen directly finetuning Llama-8B (no separation) using the Transfusion recipe, we observe significant\nperformance trade-offs between image and text capabilities (Figure 4). With equal learning rates for text\nand image components ( ηtext\nηimage = 1), the model shows continuous improvement in image understanding and\ngeneration. However, this comes at a substantial cost to language capabilities, with performance on HellaSwag\ndropping by 15% initially. While language performance improves during training, it never recovers to the\noriginal Llama-3 model’s level, maintaining a persistent 7% gap.\n9\nChange the fork to a spoon.\nAdd a soda can in the back.\nLet there be a painting instead of\na sign.\nFigure 7 Edited images from a finetuned LMFusion model.\nTo mitigate this issue, we explore setting\nηtext\nηimage < 1, which allows us to train image-specific modules (U-Nets)\nwith a regular learning rate while preserving text capabilities using a smaller learning rate for the general\nTransformer components. Figure 4 shows this improves language-only benchmark performance, reducing the\ngap from 7% to 2% when the ratio is 0.1. However, for dense models, this improvement comes at the cost of\nconsistently reduced image capabilities. Overall, while learning rate decoupling offers some mitigation to the\ntext performance drop, training dense pretrained LLMs without modality separation remains suboptimal.\nDeep Modality Separation Outperforms Shallow Separation.\nIn Figure 5, we compare three architectures: no\nseparation (dense), shallow separation (modality-specific FFNs only), and deep separation (modality-specific\nFFNs and attention). We set\nηtext\nηimage = 0 (freezing the text module) across all models to maintain Llama-3’s text\nperformance. Both separation approaches significantly outperform the dense model on all image benchmarks.\nWhile shallow separation performs slightly worse on image understanding, the performance gap widens notably\nin image generation tasks.\nAdditionally, deep separation with\nηtext\nηimage = 0 has the same amount of tunable parameters as no separation with\nηtext\nηimage = 1. Despite the intrinsic advantage of modality separation for text-only tasks, for image understanding\nand generation, we still observe that deep separation (blue curve in Figure 5) are better than no separation\n(blue curve in Figure 4). These results demonstrate that modality separation is crucial for effectively adapting\npretrained language-only LLMs for multimodal generation.\nAnalyzing learning rate decoupling strategy w.r.t. modality separation.\nThe impact of freezing text modules\nvaries dramatically between architectures. In dense models (Figure 4), freezing text components ( ηtext\nηimage = 0)\nsignificantly impairs both image understanding and generation compared to full fine-tuning. However, in the\ndeep modality separation setting shown in Figure 6, freezing the text module not only maintains the original\ntext performance but achieves strong performance on image understanding and generation, unlike the dense\nmodels.\n5.2\nImage editing\nLMFusion, our unified multimodal generative model, is naturally well-suited for tasks involving interleaved\ndata types, such as image editing. Following Transfusion, we finetune LMFusion on the same dataset of\n8K image editing examples, each consisting of an original input image, a prompt detailing the desired edit,\nand a resulting image that reflects the specified changes. In Figure 7, we apply the finetuned LMFusion to\ninput images and editing prompts from the MagicBrush (Zhang et al., 2024) test set. Qualitative results\ndemonstrate that LMFusion performs effectively in these image-editing scenarios, complementing its strong\ncapabilities in text-only, image understanding, and image generation tasks.\n5.3\nLLaVAFusion: extending LMFusion to vision-language models\nLMFusion continues training the language-only pretrained LLM Llama with the Transfusion recipe. Can\nthis recipe be extended to on vision-language models (VLMs) such as LLaVA (Liu et al., 2024d,c) and\nQwen-VL (Bai et al., 2023) as well? In this section, we extend the recipe of LMFusion to VLMs, preserving\ntheir multimodal understanding capabilities while introducing image generation abilities. Specifically, we\nbuild on LLaVA-NeXT (Liu et al., 2024c), freezing its transformer parameters and integrating a dedicated,\n10\nimage-specific transformer module trained in parallel. We use the same data and model settings as LMFusion.\nWe refer to this new model as LLaVAFusion and demonstrate its image understanding performance on MMMU\n(Yue et al., 2024), MME-Perception (Fu et al., 2024), ChartQA (Masry et al., 2022), and RealWorldQA5,\nas well as its image generation results. For baselines, we compare LLaVAFusion against EMU-3 (Wang\net al., 2024), Show-O (Xie et al., 2024b), Janus (Wu et al., 2024a), Chameleon (Team, 2024a), MetaMorph\n(Tong et al., 2024), and Transfusion (Zhou et al., 2024). As shown in Table 2, LLaVAFusion LLaVAFusion\ndemonstrates strong performance in both image understanding and generation when compared to other unified\nmultimodal LMs. This demonstrates that LMFusion is promising as an extension not only to language-only\nLLMs but also to VLMs, enhancing the multimodal generation capabilities in both cases.\n6\nRelated Work\nUnified Models for Multimodal Generation\nRecent work has extensively explored unified frameworks for\nmultimodal generation, including text generation, image understanding, and image generation. While texts\nare commonly represented as discrete tokens across models, approaches to representing images—especially for\nimage generation—vary significantly. For instance, methods in (Lu et al., 2022; Yu et al., 2023; Lu et al., 2024;\nTeam, 2024b; Xie et al., 2024a; Wu et al., 2024b; Aiello et al., 2023), represents images using vector-quantized\ndiscrete tokens (Van Den Oord et al., 2017; Esser et al., 2021; Lee et al., 2022). An alternative method, adopted\nby (Sun et al., 2024; Ge et al., 2024), employs continuous embeddings that require a separate diffusion model\nfor decoding. In this work, we build upon Transfusion (Zhou et al., 2024), which integrates autoregressive\ngeneration for texts with diffusion for images within a single, end-to-end model.\nModel Sparsity\nModel sparsity through Mixture of Experts (MoE) (Shazeer et al., 2017; Muennighoff et al.,\n2024; Fedus et al., 2022; Lepikhin et al., 2020) has proven highly effective in improving LLM training efficiency.\nThis approach has recently been extended to multimodal models (Shen et al., 2023; Lyle and Pascanu, 2024;\nLin et al., 2024a; He et al., 2024a), particularly to address potential conflicts between different modalities.\nFor example, recent efforts (Chen et al., 2023; Lin et al., 2024b; Wang et al., 2021, 2022) replace standard\nTransformer FFNs with modality-specific experts, enabling separate processing paths for different modalities.\nOur work takes this concept further by using modality-specific attention mechanisms. Concurrent work\n(Liu et al., 2024a; Liang et al., 2024) demonstrates the effectiveness of this deeper separation in multimodal\npretraining and image generation.\nReuse of LLMs in Multimodal Training\nBased on the strong language capabilities of LLMs, some recent\nmodels on multimodal generation initializes their models from pretrained, language-only LLMs. For example,\n(Ge et al., 2023; Sun et al., 2023; Dong et al., 2023; Xie et al., 2024a; Wu et al., 2024b; He et al., 2024b)\ncontinued training upon the weights of language-only LLMs (Touvron et al., 2023) or vision LLMs without\nvisual generation capabilities (Bai et al., 2023). The main focus of our work is to effectively reuse pretrained\nLLMs for multimodal generation, particularly with the Transfusion recipe, without any compromise on the\nLLMs’ existing text-only capabilities.6\n7\nConclusion\nWe present LMFusion, a framework designed to equip LLMs with multimodal generative capabilities. By\nusing Llama-3 for text generation and integrating parallel transformer modules for image diffusion, LMFusion\nefficiently reuses compute invested in pretrained LLMs.\nLMFusion’s modular design enables independent developments of language and vision modules, de-risking\nthe complexities associated with a large-scale, joint-modality pretraining. While LMFusion is currently built\nupon text-only LLMs, it can benefit further from existing visual understanding LLMs (Liu et al., 2023; Dai\net al., 2023; Liu et al., 2024b; Zhu et al., 2024), inheriting the strong multimodal understanding ability while\nenabling generating interleaved text and visual content.\n5huggingface.co\/datasets\/xai-org\/RealworldQA\n6Concurrent to our work, Liu et al. (2024a) tackles multimodal generation via a joint attention mechanism between a DiT\nstructure (Peebles and Xie, 2023) for images and a frozen Llama-3 (Dubey et al., 2024) for texts.\n11\nReferences\nEmanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large autoregressive\nmultimodal models, 2023. https:\/\/arxiv.org\/abs\/2309.15564.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv\npreprint arXiv:2308.12966, 1(2):3, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural\nlanguage. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020.\nJunyi Chen, Longteng Guo, Jianxiang Sun, Shuai Shao, Zehuan Yuan, Liang Lin, and Dongyu Zhang. Eve: Efficient\nvision-language pre-training with masked prediction and modality-aware moe.\nArXiv, abs\/2308.11971, 2023.\nhttps:\/\/api.semanticscholar.org\/CorpusID:261076168.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and\nSteven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. https:\/\/openreview.net\/forum?id=vvoWPYqZJA.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499,\n2023.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,\n2024.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\nProceedings of the IEEE\/CVF conference on computer vision and pattern recognition, pages 12873–12883, 2021.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple\nand efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large\nlanguage models, 2024. https:\/\/arxiv.org\/abs\/2306.13394.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw\nwith seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x:\nMultimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396,\n2024.\nWanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu,\nHaoyuan Li, Ziwei Huang, LeiLei Gan, and Hao Jiang. Mars: Mixture of auto-regressive models for fine-grained\ntext-to-image synthesis, 2024a. https:\/\/arxiv.org\/abs\/2407.07614.\nWanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu,\nHaoyuan Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint\narXiv:2407.07614, 2024b.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems,\n30, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020.\nDiederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. Advances\nin Neural Information Processing Systems, 36, 2024.\n12\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using\nresidual quantization. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npages 11523–11532, 2022.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam\nShazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding.\narXiv preprint arXiv:2006.16668, 2020.\nWeixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen tau Yih,\nLuke Zettlemoyer, and Xi Victoria Lin. Mixture-of-transformers: A sparse and scalable architecture for multi-modal\nfoundation models, 2024.\nBin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava:\nMixture of experts for large vision-language models. ArXiv, abs\/2401.15947, 2024a. https:\/\/api.semanticscholar.\norg\/CorpusID:267311517.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\nXi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and\nArmen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of modality-aware experts, 2024b.\nhttps:\/\/arxiv.org\/abs\/2407.21770.\nBingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert,\nJoao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion\nlarge language models, 2024a. https:\/\/arxiv.org\/abs\/2409.10695.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In\nProceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26296–26306,\nJune 2024b.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved\nreasoning, ocr, and world knowledge, January 2024c. https:\/\/llava-vl.github.io\/blog\/2024-01-30-llava-next\/.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024d.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model\nfor vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations,\n2022.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha\nKembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In\nProceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 26439–26455, 2024.\nClare Lyle and Razvan Pascanu. Switching between tasks can cause ai to lose the ability to learn. Nature, 632(8026):\n745–747, 2024.\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question\nanswering about charts with visual and logical reasoning.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2263–2279, Dublin, Ireland, May 2022. Association for Computational Linguistics.\ndoi: 10.18653\/v1\/2022.findings-acl.177. https:\/\/aclanthology.org\/2022.findings-acl.177.\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh,\nOyvind Tafjord, Nathan Lambert, et al.\nOlmoe: Open mixture-of-experts language models.\narXiv preprint\narXiv:2409.02060, 2024.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International\nconference on machine learning, pages 8162–8171. PMLR, 2021.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE\/CVF\nInternational Conference on Computer Vision, pages 4195–4205, 2023.\n13\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning, pages 8748–8763. PMLR, 2021.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation.\nIn Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference,\nMunich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234–241. Springer, 2015.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd\nschema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about\nsocial interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463–4473, 2019.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538,\n2017.\nSheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling vision-language models\nwith sparse mixture of experts. arXiv preprint arXiv:2303.07226, 2023.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pages 14398–14409, 2024.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024a. https:\/\/arxiv.org\/abs\/2405.09818.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024b.\nShengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun,\nSaining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning, 2024.\nhttps:\/\/arxiv.org\/abs\/2412.14164.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information\nprocessing systems, 30, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575, 2015.\nWenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-\nmodality-experts. ArXiv, abs\/2111.02358, 2021. https:\/\/api.semanticscholar.org\/CorpusID:241035439.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed,\nSaksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language\ntasks. arXiv preprint arXiv:2208.10442, 2022.\nXinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong\nWang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3:\nNext-token prediction is all you need, 2024. https:\/\/arxiv.org\/abs\/2409.18869.\nChengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and\ngeneration, 2024a. https:\/\/arxiv.org\/abs\/2410.13848.\n14\nYecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model integrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024b.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding\nand generation. arXiv preprint arXiv:2408.12528, 2024a.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding\nand generation, 2024b. https:\/\/arxiv.org\/abs\/2408.12528.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,\nBrian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.\narXiv preprint arXiv:2309.02591, 2(3), 2023.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang,\nYibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal\nunderstanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish\nyour sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n4791–4800, 2019.\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for\ninstruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma,\nLuke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language\nunderstanding with advanced large language models.\nIn The Twelfth International Conference on Learning\nRepresentations, 2024. https:\/\/openreview.net\/forum?id=1tZbq88f27.\n15\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LMFusion: Adapting Pretrained Language Models for Multimodal Generation.pdf"}
{"title":"MetaMorph: Multimodal Understanding and Generation via Instruction Tuning","authors":"Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, Zhuang Liu","summary":"In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.","url":"http:\/\/arxiv.org\/abs\/2412.14164v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.14164v1","published":1734548330000,"comment":"Project page at tsb0601.github.io\/metamorph","pdf_text":"MetaMorph: Multimodal Understanding and\nGeneration via Instruction Tuning\nShengbang Tong1,2,∗,†, David Fan1, Jiachen Zhu1,2,∗, Yunyang Xiong3, Xinlei Chen1, Koustuv Sinha1,\nMichael Rabbat1, Yann LeCun1,2, Saining Xie2, Zhuang Liu1,†\n1FAIR, Meta, 2New York University, 3Meta Reality Labs\n∗Work done at Meta, †Corresponding authors\nIn this work, we propose Visual-Predictive Instruction Tuning (VPiT)—a simple and effective exten-\nsion to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified\nautoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to\npredict discrete text tokens and continuous visual tokens from any input sequence of image and text\ndata curated in an instruction-following format. Our empirical investigation reveals several intriguing\nproperties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of generation data; (2) while\nwe find understanding and generation to be mutually beneficial, understanding data contributes to\nboth capabilities more effectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual understanding and generation.\nIn visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from\nLLM pretraining, and overcome common failure modes exhibited by other generation models. Our\nresults suggest that LLMs may have strong “prior” vision capabilities that can be efficiently adapted\nto both visual understanding and generation with a relatively simple instruction tuning process.\nDate: December 19, 2024\nCorrespondence: st5087@nyu.edu, zhuangl@meta.com\nProject Page: tsb0601.github.io\/metamorph\n1\nIntroduction\nMultimodal Large Language Models (MLLMs) have advanced considerably in visual understanding, progressing\nfrom basic image captioning to complex visual inferences (Alayrac et al., 2022; Liu et al., 2023; Dai et al.,\n2024). These models process multimodal inputs—primarily images and language—and generate text tokens.\nMultimodal LLMs often leverage a pretrained vision encoder (Dosovitskiy et al., 2021; Radford et al., 2021),\na pretrained language model (Touvron et al., 2023; AI@Meta, 2024), and align these modalities through\nconnectors such as MLP (Liu et al., 2023, 2024a) or cross-attention modules (Alayrac et al., 2022; Dai et al.,\n2024). Among MLLM training methods, visual instruction tuning (Liu et al., 2023) has become widely\nused (Wang et al., 2024a; Agrawal et al., 2024). It treats output embeddings of pretrained vision encoders as\ncontinuous-valued “visual tokens” and directly feeds them as inputs to pretrained LLMs.\nOne benefit of visual instruction tuning is that it is data and compute efficient. A pretrained LLM can be\nrepurposed as a Multimodal LLM by instruction tuning with modest compute and data on the order of millions\nof image-text question-answer pairs (Tong et al., 2024a; Li et al., 2024a). The effectiveness of visual instruction\ntuning indicates that LLMs already possess a considerable amount of inherent visual knowledge which allows\nthem to efficiently learn and develop visual understanding during the instruction tuning process (Zhou et al.,\n2024a). Inspired by this, we investigate whether LLMs can also be finetuned to generate visual information\nwith comparable efficiency and effectiveness.\nCurrent attempts toward “unified” models—models capable of both multimodal understanding and generation—\noften treat visual generation as an orthogonal capability to visual understanding. They tend to require\nsubstantial changes to the original MLLM architecture and significant multimodal pretraining and\/or finetuning.\nDesigning such methods is challenging, and past research takes different approaches including tokenizing\n1\narXiv:2412.14164v1  [cs.CV]  18 Dec 2024\nText\nHead\nVision \nHead\nAutoregressive \nModel\nAdapter\nVision\nEncoder\nVPiT\nAutoregressive Model\nAdapted \nDiffusion \nModel\nText \nHead\nVision \nHead\nAdapter\nEncoder\nProjector\nInference\nExamples\nGenerate an image of the animal resulting from a \nmonarch caterpillar's metamorphosis \nHere’s the generated image based on \nyour request: <image_start><image_end>\nWhat’s the animal in this image?\nThe animal in the image is a butterfly.\nFigure 1 VPiT Training, Inference, and Examples of MetaMorph. Left: In Visual-Predictive Instruction Tuning (VPiT),\nwe finetune a pretrained LLM to generate both text and visual tokens using separate text and vision heads. Middle:\nDuring inference, the model accepts an arbitrary input sequence of image(s) and text and outputs discrete text tokens\nand continuous visual tokens. These visual tokens can be visualized via a separately finetuned diffusion model, which\nis trained to condition on the pretrained vision encoder’s output. Right: An example conversation from MetaMorph\ntrained with VPiT. Here, the model implicitly solves a visual puzzle in order to generate the visual tokens of a butterfly.\nThe conversation continues with new user questions as the model continues to autoregressively process vision and text\ntokens, independent of the diffusion-based visualization.\nvisual inputs into discrete tokens (Wu et al., 2024b; Team, 2024; Liu et al., 2024c), incorporating diffusion\nobjectives (Xie et al., 2024; Zhou et al., 2024b), and decoupling vision into separate understanding and\ngeneration modes (Wu et al., 2024a). For example, approaches like LWM (Liu et al., 2024c), Show-o (Xie\net al., 2024), and Chameleon (Team, 2024) require billions of image-text pairs (Schuhmann et al., 2022; Gadre\net al., 2024) for extensive pretraining and finetuning.\nIn this work, we propose Visual-Predictive Instruction Tuning (VPiT)—a simple extension to visual instruction\ntuning which builds upon the existing paradigm of passing continuous visual tokens as input to the LLM. VPiT\ntrains an LLM to output both continuous visual tokens and discrete text tokens in the finetuning stage. The\nmodel takes pretrained vision encoder embeddings as well as text tokens as input, and outputs a combination\nof text tokens and continuous visual tokens. To visualize the generated visual tokens, we finetune a diffusion\nmodel to map the embeddings back into pixel space (see Figure 1 for an example). This framework allows us\nto study the synergy between visual understanding, visual generation, and pretrained LLMs, which leads to\nseveral intriguing findings outlined below.\nFirst, we show that the ability to predict visual tokens emerges from understanding visual inputs and requires\nminimal additional training. Similar to visual instruction tuning, VPiT efficiently and effectively morphs an\nLLM into an “unified” model that understands and generates multimodal tokens. When trained jointly with\nsufficient visual understanding data, this process requires as little as 200k additional visual generation data.\nWe further establish that the abilities to understand and generate visual tokens are intrinsically linked and\nasymmetrical. Specifically, increasing understanding data improves visual understanding (measured by higher\nVQA scores) and generation performance (measured by lower FID scores). Conversely, increasing generation\ndata enhances generation quality and also contributes to stronger visual understanding—but to a lesser degree.\nImportantly, our findings highlight an asymmetry in how training each ability impacts the model’s overall\nvision performance: understanding-centric training substantially outperforms generation-centric training in\nimproving both visual understanding and generation.\nBuilding upon these findings, we train a unified model called MetaMorph to predict multimodal tokens with\nVPiT. We leverage diverse data sources ranging from common visual question answering datasets to pure\nimage and video data without text annotations. MetaMorph achieves competitive performance on both visual\nunderstanding and visual generation benchmarks. Furthermore, we show this unified modeling approach\nallows models to leverage the power of LLMs. For instance, MetaMorph can extract knowledge from the\npretrained LLM when generating visual tokens. More surprisingly, we observe that MetaMorph can implicitly\nperform reasoning steps before generating visual tokens—e.g. when prompted with “the animal resulting from\na monarch caterpillar’s metamorphosis”, MetaMorph successfully generates an image of a butterfly (Figure 1).\nOur results suggest that 1) training a unified model with instruction tuning is feasible, and 2) LLMs have\nstrong pre-existing visual capabilities which can be activated using significantly fewer samples compared\n2\nto extensive pretraining. These insights shed light on the development of mixed-modality models. As the\ncommunity continues to improve visual understanding in Multimodal LLMs (Tong et al., 2024a; Wang et al.,\n2024a; Li et al., 2024a) by advancing base LLMs, instruction tuning techniques, and data, we highlight that\nthese efforts may also implicitly lead to models that are better at visual generation.\n2\nVisual-Predictive Instruction Tuning\nVisual instruction tuning as introduced by LLaVA (Liu et al., 2023) demonstrates that LLMs can be taught\nto understand visual inputs. This is achieved by finetuning on million-scale data. The success of late-fusion\ninstruction tuning suggests that LLMs may already possess innate visual understanding ability. This ability\nsimply needs to be unlocked through lightweight finetuning. Analogously, we hypothesize that LLMs already\npossess a degree of innate visual generation ability which just needs to be unlocked with lightweight finetuning.\nMotivated by this, we present Visual-Predictive Instruction Tuning (VPiT, Figure 1)—a simple design which\nextends existing instruction tuning methods to additionally generate visual tokens rather than text alone.\nWe use the same architecture and next-token prediction paradigm to unlock visual generation capabilities\nwithout bells and whistles. We take a pretrained LLM and finetune it to predict both discrete text tokens\nand continuous visual tokens. The visual tokens can be visualized with an adapted diffusion model.\n2.1\nFrom Unimodal to Multimodal Next-Token Prediction\nThe standard instruction tuning setup consists of an input sequence of conversation rounds (Wei et al., 2022a;\nTaori et al., 2023): (Pi, Ri)N\ni=1, where Pi and Ri represent prompts and responses for the i-th round of\nconversation, respectively. The model is trained to generate responses based on the prompt. VPiT adds the\nfollowing mechanisms to a standard instruction tuning setup to unlock visual understanding and generation.\nTokenizing multimodal data. We extend Pi and Ri to include both text and images. To integrate visual data\ninto a pretrained LLM, we process data closely following visual instruction tuning (Liu et al., 2023):\n• Text Data: Text is tokenized into discrete tokens with a standard tokenizer used by the LLM.\n• Visual Data: Images are encoded with a pretrained vision encoder such as SigLIP (Zhai et al., 2023).\nThe output is continuous visual tokens which are then interpolated to m = 64 tokens. To pass the visual\ntokens as input to the LLM, we apply a trainable projection layer to align the dimensions with the LLM.\nModel architecture. We take a pretrained LLM and finetune it to process arbitrary sequences of text and\nvisual tokens (detailed next in Section 2.2). We keep the original LLM head for text prediction, and attach a\nseparate vision head to the LLM for predicting visual tokens, i.e., the output tokens generated by the vision\nencoder when processing images. The vision head is a projection layer that projects from the LLM’s dimension\nto the vision encoder’s dimension. All response tokens can then be trained and predicted autoregressively,\nwith prompt tokens as context.\nUnlike conventional visual instruction tuning, in VPiT, visual tokens are also outputs of the LLM—not just\ninputs. To make the LLM aware of the presence of visual tokens, we introduce special tokens ⟨image_start⟩\nand ⟨image_end⟩to indicate the boundaries of visual token sequences and when to use the vision head.\nLoss functions. The language head outputs a probability distribution over the vocabulary and is trained with\ncross-entropy loss for next-token prediction. Visual prediction uses cosine similarity loss between the LLM’s\npredicted visual tokens and those from the vision encoder. Consistent with instruction tuning practices, the\nmodel only makes predictions and incurs loss on response tokens.\n2.2\nUsing Broad Types of Data\nBecause VPiT enables the model to predict both text and visual tokens in its responses, it allows the use of\na broader range of training data. Traditional visual instruction tuning, on the other hand, primarily relies\non question-and-answer pairs. The majority of our dataset is publicly available, and we categorize it into\nthree major categories below. This categorization enables us to systematically study the model, as detailed in\n3\nSection 3 and Section 4. All data types are formatted as instruction tuning style prompt & response pairs.\nSee further details in Appendix C.2.\n1. Visual Understanding Data: This category includes data that takes image(s) or video as input and outputs\ntext responses. See Figure 1 for an example. We use:\n• ImageQA: Cambrian-7M (Tong et al., 2024a). The model answers questions based on input image(s).\nPi ∈{⟨visual tokens⟩, ⟨text prompt⟩}\nRi ∈{⟨text response⟩}\n• VideoQA: VideoStar (Zohar et al., 2024) and ShareVideo (Zhang et al., 2024). The model answers\nquestions based on the input video. For videos in VideoQA, we process frames at 1 FPS.\nPi ∈{⟨visual tokens⟩, · · · , ⟨visual tokens⟩, ⟨text prompt⟩}\nRi ∈{⟨text response⟩}\n2. Visual Generation Data: MetaCLIP (Xu et al., 2024). The model predicts visual tokens based on an image\ndescription. We using at most 5 million pairs. We curate the data into question-answering formats.\nPi ∈{⟨text prompt⟩}\nRi ∈{⟨text response⟩, ⟨visual tokens⟩}\nWe prompt the model to generate visual tokens with instructions like “Generate an image of...”. The\ntext responses are “Here is an image based on your request...”. See Figure 1 for an example.\n3. Other Visual Data: This category includes data that requires the model to predict visual tokens given\ninterleaved input visual tokens and text tokens. We use:\n• Video Data: SomethingSomethingV2 (Goyal et al., 2017b) and HowTo100M (Miech et al., 2019).\nThe model predicts frames in a sequential order. We design different question-answer pairs to\nprobe into the video, such as asking about future frames, past frames, and reordering frames.\nPi ∈{⟨visual tokens⟩, · · · , ⟨visual tokens⟩, ⟨text prompt⟩}\nRi ∈{⟨visual tokens⟩, · · · , ⟨visual tokens⟩}\n• Visual Thinking Data: Visualization-of-Thought (Shao et al., 2024) and VStar (Wu and Xie, 2024).\nThe model predicts multimodal tokens in its response before addressing problems. For instance, it\npredicts a zoomed-in view of an image before generating textual responses.\nPi ∈{⟨visual tokens⟩, ⟨text prompt⟩}\nRi ∈{⟨text response⟩, ⟨visual tokens⟩, ⟨text response⟩}\nIn the response, the model will output “I will think about it visually”, followed by visual tokens\nrepresenting a zoomed-in segment of the image, and then proceed to answer the question.\n• Image-to-Image Data: InstructPix2Pix (Brooks et al., 2023) and Aurora (Krojer et al., 2024). The\nmodel generates a transformed image conditioned on a text description and an input image.\nPi ∈{⟨visual tokens⟩, ⟨text prompt⟩}\nRi ∈{⟨visual tokens⟩}\n2.3\nMapping Tokens to Images through Diffusion\nBecause models trained with VPiT learn to predict continuous visual tokens, we need to map the predicted\ntokens back into pixel space. We leverage the concept of a “Diffusion Autoencoder” (Bordes et al., 2022;\nPreechakul et al., 2022; Pan et al., 2024b; Koh et al., 2024; Li et al., 2024c) in which the diffusion model\ncan be adapted to condition on image embeddings rather than text embeddings. Specifically, we finetune an\nexisting diffusion model to condition on outputs from the vision encoder using held-out training data.\nAt inference time, if the tag token ⟨image_start⟩is generated, the model begins outputting visual tokens until\n⟨image_end⟩. We then plug the generated visual tokens into the diffusion model to visualize the prediction in\npixel space. We use standard latent diffusion model training procedures. Details on the hyperparameters and\ntraining setup are provided in Appendix A.2.\n4\n3\nFindings on Unlocking Visual Generation\nWe study the following questions about the effects and synergy of visual understanding and generation, under\nour VPiT framework:\n§3.1\nCan visual generation be unlocked through lightweight tuning, or does it require extensive data?\n§3.2\nAre visual understanding and generation mutually beneficial or orthogonal?\n§3.3\nHow much does more visual understanding or generation data contribute to understanding and generation\nquality?\n§3.4\nWhich visual understanding tasks correlate the most with generation performance?\nEvaluation settings. We use 9 ImageQA benchmarks (MMBench, Seed, VStar, MMVP, MMMU, ChartQA,\nTextVQA, ScienceQA, RealWorldQA) to evaluate different aspects of the model. For image generation, we\nuse the finetuned diffusion model to visualize generated visual tokens and measure FID score (lower is better)\nand CLIP score (higher is better) on the COCO-30K dataset. Unless otherwise specified, we use LLaMA-3\n8B (AI@Meta, 2024) \/ SigLIP ViT-SO400M-14@384 (Zhai et al., 2023) as the pretrained LLM \/ vision\nencoder. We also study the effect of different LLMs in Section 3.2. We use instruction tuned versions of\nthe LLMs. We pretrain the adapter between the vision encoder and the LLM following visual instruction\ntuning (Liu et al., 2023, 2024a). For experiments in this section, we provide training details in Appendix A\nand include the full results in Appendix B.\n3.1\nVisual Generation Can Be Unlocked Efficiently by Joint Training with Visual Understanding\nWe start by investigating the number of image-text samples required to teach a language model to generate\nhigh-quality visual tokens. To this end, we randomly sample {1k, 5k, 10k, 50k, 200k, 1M, 3M, 5M} image-text\npairs from our generation data (MetaCLIP dataset (Xu et al., 2024)). We explore two settings: (1) finetuning\nthe LLM using only visual generation data, and (2) joint training visual generation with visual understanding\nand the rest of data types described in Section 2.2.\nIn Figure 2, we see that training solely on visual generation performs significantly worse than joint training\nwith all other data. With over 3 million image-text pairs, the model struggles to generate high-quality visual\nimages (∼40 FID score), and performance remains inferior to joint training with 5 million pairs. This suggests\nthat training solely on visual generation data is significantly less sample efficient. This finding aligns with a\nprior study (Zhang et al., 2023) which also suggests that LLMs cannot be easily tuned to generate visual\ntokens when trained with only generation data. In contrast, joint training with other datasets substantially\nimproves generation performance. The model generates effective visual tokens with just 5k generation data,\nand performance stabilizes around 200k samples. This indicates that visual generation is not an orthogonal\ncapability but rather an ability that benefits from other tasks and emerges more effectively with joint training.\n10\n3\n10\n4\n10\n5\n10\n6\nNumber of Generation Data (Log Scale)\n20\n40\n60\n80\n100\n120\nFID Score ( )\nGeneration Data Alone\nJoint Training with All Other Data\nFigure 2 Generation-only training vs. Joint training\nwith other data. Training solely on generation data\nresults in inferior performance. Joint training with\nadditional data enables visual generation with only\n5k generation data and yields high-quality outputs\nwith 200k generation data.\n0\n50\n100\nFID Score ( )\nAll data\nGeneration + Image QA\nGeneration + Video QA\nGeneration + Pure Video\nGeneration + Visual Thinking\nGeneration + Image-to-Image\nGeneration Only\n0\n10\n20\nCLIP Score ( )\nFigure3 Impactofdifferentdatatypesonvisualgeneration. The baseline\nof training on only visual generation data is red; Joint training with\nother data is yellow; Joint training with visual understanding data\nis green; and all data is blue. Joint training with additional data\nimproves the baseline, with visual understanding tasks contributing\nthe most to enhancing visual generation.\n5\n45.0\n47.5\n50.0\n52.5\n55.0\nVQA Accuracy ( )\n18\n20\n22\n24\n26\n28\nFID Score ( )\n45.0\n47.5\n50.0\n52.5\n55.0\nVQA Accuracy ( )\n16\n18\n20\n22\nCLIP Score ( )\n200k Generation Data Jointly Trained with\n1M VQA\n2M VQA\n3M VQA\n4M VQA\n5M VQA\n6M VQA\n7M VQA\nFigure 4 VQA Performance vs. Generation Performance with\ngeneration data controlled at 200k. Increasing understand-\ning data improves VQA and generation performance.\n20.0\n22.5\n25.0\n27.5\nFID Score ( )\n46\n47\n48\n49\n50\n51\n52\nVQA Accuracy ( )\n16\n18\n20\n22\nCLIP Score ( )\n46\n47\n48\n49\n50\n51\n52\nVQA Accuracy ( )\n1M VQA Data Jointly Trained with\n200k Generation Data\n500k Generation Data\n1M Generation Data\n2M Generation Data\n3M Generation Data\n4M Generation Data\nFigure 5 Generation Performance vs. VQA Performance with\nVQA data controlled at 1M. Increasing generation data im-\nproves generation and VQA performance.\nTo better understand how each type of data contributes to visual generation, we conduct a controlled\nexperiment using 200k visual generation data, joint training individually with each data type defined in\nSection 2.2. We also compare them with training all the data together. We show results in Figure 3. While\nall data types enhance the model’s visual generation, the degree of improvement varies. Visual understanding\ndata, such as ImageQA and VideoQA, significantly boost the model’s visual generation capabilities, even\nwhen the amount of generation data is kept constant at 200k. This indicates a strong link between the ability\nto understand visual content and generate visual tokens. Additionally, combining all data types in training\nfurther improves performance, suggesting that the benefits from different data types can be additive.\nFinding 1: The ability to generate visual tokens can be unlocked with significantly less generation data\nwhen the model is jointly trained with visual understanding data, in contrast to training only on\ngeneration data.\n3.2\nVisual Understanding and Generation are Mutually Beneficial\nMore understanding data leads to better understanding and generation. Building upon findings from the previous\nsubsection, we perform a controlled experiment to investigate how visual understanding ability correlates with\nvisual generation ability. We ablate our model using a fixed set of 200k generation data while varying VQA\ndata from 1M to 7M samples from Cambrian-7M to develop different levels of visual understanding. The\nresults presented in Figure 4 indicate that stronger VQA ability correlates with better generation performance.\nMore generation data leads to better understanding and generation. Here, we investigate the reverse direction:\ndoes enhancing the model’s visual generation capability also relate to higher VQA performance? To explore\nthis, we conduct a controlled experiment using 1M fixed VQA samples as the baseline for understanding. We\nthen vary the amount of generation data ({200k, 500k, 1M, 2M, 3M, 4M}) to adjust generation capacity while\njoint training with the fixed 1M VQA data. We present results in Figure 5. Within the 1M VQA setting,\nstronger generation ability is correlated with improved VQA performance. This implies that increasing the\namount of generation data not only enhances generation but also positively impacts VQA performance.\nThis synergy scales across different LLMs. We examine whether the findings transfer across various LLM\nbackbones. Using a data composition of 7M VQA samples and 1M generation data, we train VPiT on\nLLaMA-3 8B, LLaMA-3.1 8B, and LLaMA-3 70B. Figure 6 shows the scaling behavior across different LLMs.\nFinding 2: Visual understanding and generation are synergistic. Increasing data for either capability\nenhances both simultaneously.\n6\n54\n56\n58\n60\n62\nVQA Accuracy ( )\n13.0\n13.5\n14.0\n14.5\n15.0\nFID Score ( )\nLLaMA-3 8B\nLLaMA-3.1 8B\nLLaMA-3 70B\n54\n56\n58\n60\n62\nVQA Accuracy ( )\n26.5\n27.0\n27.5\nCLIP Score ( )\nLLaMA-3 8B\nLLaMA-3.1 8B\nLLaMA-3 70B\nFigure 6 Comparison between different language backbones.\nWe jointly train 7M VQA and 1M Generation data on\ndifferent language backbones (LLaMA-3 8B, LLaMA-3.1\n8B, LLaMA-3 70B). We observe that the synergy between\nunderstanding and generation transfer across LLMs.\n1M\n4M\n7M\nVQA Data\n200K\n500K\n1M\n2M\n3M\n4M\nGeneration Data\nAverage VQA Score\n1M\n4M\n7M\nVQA Data\nFID Score\n1M\n4M\n7M\nVQA Data\nCLIP Score\n48\n50\n52\n54\n56\n15\n20\n25\n17.5\n20.0\n22.5\n25.0\nFigure 7 Heatmap visualization of Average VQA Score, FID\nScore, andCLIPScoreacrossvaryingamountsofVQAdataand\ngenerationdata. Darker colors indicate better performance.\nIncreasing VQA data is more effective for improving both\nunderstanding and generation capabilities.\n3.3\nUnderstanding Data Contributes More\nWe investigate whether understanding and generation data contribute equally. Here, we jointly train different\nscales of VQA data {1M, 4M, 7M} and generation data {200k, 500k, 1M, 2M, 3M, 4M}. Figure 7 summarizes\nthese findings, with the x-axis representing VQA data, and the y-axis representing generation data. Results\nare visualized on heatmaps using darker colors for better performance.\nThe results indicate that increasing VQA data yields the most significant improvements in all three metrics.\nWhen VQA data is relatively low (1M), increases in generation data lead to noticeable improvements, as\nreflected by the gradual darkening in the plot. However, as the VQA data scales up (from 1M to 4M to 7M),\nthe impact of VQA data becomes more pronounced, demonstrated by a sharp color transition in the heatmap.\nUltimately, with 7M VQA data, increases in generation data contribute minimally. These results demonstrate\nthe critical role of understanding data in enhancing both understanding and generation performance.\nFinding 3: While increasing data improves performance overall, the impact of visual understanding\ndata is significantly higher than the impact of visual generation data.\n3.4\nCertain Understanding Tasks Correlate More with Generation Performance\nGiven the diverse nature of understanding tasks such as OCR, Vision-Centric tasks, and Knowledge-based\ntasks, we investigate which tasks most strongly correlate with generation ability. Inspired by Cambrian-1, we\ncategorize VQA tasks into five groups: General, Text&Chart, High-Resolution, Knowledge, and Vision-Centric\nVQA. Using the results from our earlier experiments, which jointly train various VQA data scales with different\namounts of generation data, we plot each benchmark’s VQA performance against generation performance in\nFigure 8. We also calculate the Pearson correlation (ρ) between VQA scores and FID\/CLIP Scores.\n15\n20\n25\n62.5\n65.0\n67.5\n70.0\nSeed\n = 0.94\n15\n20\n25\n60\n65\n70\n75\nMMBench\n = 0.86\n15\n20\n25\n50\n52\n54\n56\n58\nRealWorldQA\n = 0.88\n15\n20\n25\n25\n30\n35\n40\n45\nMMVP\n = 0.85\n15\n20\n25\nCLIP Score ( )\n20\n25\n30\n35\nChartQA\n = 0.92\n15\n20\n25\n50\n55\n60\nTextVQA\n = 0.87\n15\n20\n25\n38\n40\n42\n44\nVStar\n = 0.71\n15\n20\n25\n79\n80\n81\n82\nScienceQA\n = 0.62\n15\n20\n25\n36\n38\n40\nMMMU\n = -0.26\n15\n20\n25\n62\n64\n66\n68\n70\n = -0.95\n15\n20\n25\n60\n65\n70\n75\n = -0.86\n15\n20\n25\n50\n52\n54\n56\n58\n = -0.85\n15\n20\n25\n25\n30\n35\n40\n45\n = -0.87\n15\n20\n25\nFID Score ( )\n20\n25\n30\n35\n = -0.92\n15\n20\n25\n50\n55\n60\n = -0.87\n15\n20\n25\n38\n40\n42\n44\n = -0.72\n15\n20\n25\n79\n80\n81\n82\n = -0.63\n15\n20\n25\n36\n38\n40\n = 0.23\nFigure 8 Correlation analysis between generation and various understanding benchmarks. Results are collected by joint\ntraining different amounts of VQA data combined with varying quantities of generation data. Each subplot shows the\ncorrelation (ρ) with a fitted regression line. Stars represent data points. We analyze General VQA, Vision-Centric\nVQA, Text&Chart VQA, High-Resolution VQA, and Knowledge VQA. For most tasks, generation performance and\nVQA performance are strongly correlated: higher VQA performance indicates better generation and vice versa. Only\nknowledge-intensive and high-resolution VQA tasks exhibit weaker correlations with generation performance.\n7\nFigure 8 shows that General, Vision-Centric, and Text&Chart VQA tasks strongly correlate with generation\nperformance, each with a Pearson correlation coefficient (ρ) above 0.85. High-Resolution VQA exhibits\nmoderate correlation, with ρ around 0.7. In contrast, Knowledge VQA tasks, such as MMMU, show weak\ncorrelation with generation performance. These findings suggest that generation ability aligns more closely\nwith the model’s vision capabilities rather than knowledge-specific tasks.\nFinding 4: General, vision-centric, and text understanding VQA tasks exhibit strong correlations with\nvisual generation, whereas knowledge-based VQA tasks do not.\n4\nMetaMorph Model\nBased on the insights in Section 3, we train our unified model, MetaMorph, based on LLaMA-3.1 8B (AI@Meta,\n2024), using VPiT with the data curated in Section 2.2. We present our experimental results in three parts:\nquantitative performance (Section 4.1), evidence of MetaMorph leveraging LLM knowledge in visual generation\n(Section 4.2), and implicit reasoning skills in multimodal contexts (Section 4.3).\nImage QA\nVideo QA\nGeneration\nMethod\nBase LLM\nMMBenchEN\nSEED\nRealworldQA\nMMVP\nSQA\nMMMU\nVStar\nChartQA\nTextVQA\nMV-Bench\nCOCO (FID)\nVisual Understanding Only\nGPT-4V*\n75.8\n69.1\n61.4\n50.0\n75.7\n56.8\n55.0\n78.5\n78.0\n43.5\n-\nVisual Generation Only\nStable Diffusion 1.5∗\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n9.6\nDalle 2∗\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n10.4\nImagen∗\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n7.3\nUnified Models\nEMU-3∗\n58.5\n68.2\n57.4\n36.6†\n89.2\n31.6\n51.8†\n68.6\n64.7\n-\n12.8\nJanus∗\nDeepSeek 1.3B\n69.4\n63.7\n-\n-\n-\n30.5\n-\n-\n-\n-\n8.5\nVILA-U256†\nLLaMA-2 7B\n66.6\n57.1\n46.6\n22.0\n67.1\n32.2\n38.7\n11.4\n48.3∗\n40.8\n19.6\nTransfusion∗\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n6.7\nChameleon-7B†\n35.7\n27.2\n19.6\n0.0\n50.3\n28.4\n37.1\n0.0\n0.0\n-\n26.7∗\nMetaMorph (ours)\nLLaMA-3.1 8B\n75.2\n71.8\n58.3\n48.3\n83.2\n41.8\n44.0\n37.1\n60.5\n48.8\n11.8\nTable 1 Comparison of MetaMorph with other unified models. MetaMorph offers competitive performance compared to\nother leading unified models. Models in gray are understanding-only or generation-only. Unified models without a\nbase LLM are trained from scratch. ∗We use numbers reported in original papers. †We obtain results using official\nopen-sourced model weights.\n4.1\nCompetitive Performance in Understanding and Generation\nWe compare MetaMorph with other unified models and summarize results in Table 1. Since these models are\ntrained on different datasets and base LLMs (or pretrained from scratch), an apples-to-apples comparison\nis difficult. Nevertheless, MetaMorph demonstrates competitive performance and outperforms other unified\nmodels on most benchmarks—even when prior models may have been trained on more data. Compared to\nmodels trained from scratch, such as EMU-3 (Wang et al., 2024b) and Chameleon (Team, 2024), MetaMorph\nleverages the strengths of the latest pretrained LLMs and achieves competitive understanding and generation\nperformance. MetaMorph highlights that unified models can be developed effectively from pretrained LLMs.\n8\n“Chhogori”\n“view of \nChizarira”\nSD-3.5 8B\nMetaMorph\nGround Truth \n\/ Examples \nJanus\n“A glass \nwithout water”\n“Slightly tall \nbuilding”\n“A bookshelf \nwith few books”\n“Oncilla”\n“A bookshelf \nwith many books”\n“A glass filled \nwith water”\n“Very tall \nbuilding”\nProfessional Knowledge\nAddressing Semantic Nuances\nFigure 9 Examples of MetaMorph leveraging LLMs to generate visual tokens. Left: MetaMorph can leverage knowledge from\nthe LLM to generate visual tokens for professional terms that need domain-specific understanding. Right: MetaMorph\nalso avoids common mistakes seen in T2I models that condition on text embeddings (e.g., Stable Diffusion-3.5 8B).\n4.2\nMetaMorph can Leverage LLM Knowledge for Visual Generation\nMetaMorph effectively leverages the world knowledge embedded in pre-trained LLMs. We show examples on\nthe left side of Figure 9. We prompt the model to generate concepts requiring non-trivial and specialized\nknowledge. Examples include “Chhogori” (the world’s second-highest mountain), “Oncilla” (a small wildcat\nfrom South America), and “Chizarira” (an isolated wilderness area in Zimbabwe).\nMetaMorph successfully translates domain-specific knowledge into accurate visual tokens, thereby displaying\nthe ability to leverage world knowledge from LLMs. In contrast, the latest Text-to-Image (T2I) model, Stable\nDiffusion-3.5 8B, struggles to generate the correct concept despite producing high-quality images. This issue\nmay stem from the text embedding models it uses—–CLIP (Radford et al., 2021) and T5 (Roberts et al.,\n2019)—–which fail to properly encode these specialized terms (Yuksekgonul et al., 2022).\nOn the right side of Figure 9, we demonstrate how MetaMorph handles common semantic challenges more\neffectively than text embedding models such as CLIP and T5.\nThese challenges include negation and\nsubjectivity, using prompts with common failure patterns identified in Multimon (Tong et al., 2024b).\nMetaMorph differentiates semantic nuances such as “slightly” versus “very”, “few” versus “many”, and “without”\nversus “with”, which are common failures in existing text-to-image systems.\n4.3\nReasoning in Multimodal Generation\nIn Figure 10, we present examples where the model generates images in response to puzzle prompts such as\n“The national flag of the country where Yellowstone National Park is located”. For each puzzle, we directly use\nthe prompt “Generate an image of {puzzle}”, without calling any Chain-of-Thought (CoT) (Wei et al., 2022b)\nin the prompts. MetaMorph generates the correct image from prompts that require multi-step reasoning.\nFor example, when answering the question “A musical instrument, this instrument is often played by the\nscientist who formulated the theory of special relativity”, the model needs to implicitly complete three reasoning\nsteps: it identifies Albert Einstein as the scientist formulated the theory of special relativity, recognizes that\nhis preferred instrument is the violin, and then directly generates correct visual tokens—a violin—without\nexplicitly separating these steps during the generation process. This result implies that MetaMorph implicitly\nsolves the puzzle and generates correct visual tokens immediately following the prompt. These results align\nwith the findings in Physics of LLMs (Ye et al., 2024; Allen-Zhu, 2024), where the authors suggest that LLMs\nprecompute reasoning graphs before autoregressively generating subsequent tokens. Here, we demonstrate\nthat this capability transfers to the unified multimodal model setting even when decoding visual tokens.\n9\nSD3.5-8B\nMetaMorph\nSolution\nExamples \nJanus\nYellowstone National Park’s Location \n➟America \n➟American Flag\nSushi's Origin \n➟Japan \n➟Flower in Spring Festivals in Japan \n➟Cherry Blossom (Sakura)\nConstellation Associated with the \nNorthern Sky \n➟Ursa Major\n➟Ursa (Latin for 'Bear’) \n➟Large Mammal Named 'Bear'\nPrompt\nScientist Who Formulated Special \nRelativity\n➟Albert Einstein \n➟Instrument Often Played by Einstein \n➟Violin\nStep-by-Step Logic Chain\n(For Reference)\n2+7\n➟9 \n➟Animal Believed to Have 9 lives \n➟Cat\n“The national flag of the country\nwhere Yellowstone National Park\nis located”\n“The flower celebrated in spring\nfestivals in the country where\nsushi originated”\n“The large mammal that shares\nits\nname\nwith\na\nconstellation\noften visible in the night sky and\nassociated with the northern part\nof the world”\n“A\nmusical\ninstrument,\nthis\ninstrument is often played by the\nscientist\nwho\nformulated\nthe\ntheory of special relativity”\n“The\nanimal\nassociated\nwith\nhaving (2+7) lives”\nFigure 10 Examples of MetaMorph solving reasoning problems in visual generation. We design puzzles that require multi-step\nreasoning. We include reference\nlogic chains needed to solve the puzzles, and reference\nsolution examples . When\nprompting each model, we directly feed in the puzzle without any CoT hints or logic chains. MetaMorph has the\nability to implicitly solve these puzzles and generate the correct image without explicitly creating or processing a logic\nchain. It demonstrates that the implicit reasoning skills in text-only LLMs can transfer to unified multimodal models.\n5\nRelated Work\nInstruction tuning and visual instruction tuning. Instruction tuning (Wei et al., 2022a; Taori et al., 2023) finetunes\na pretrained LLM to learn the format and style of interaction. This process helps the model to effectively\nconvey the knowledge and capabilities acquired during pretraining (Zhou et al., 2024a). LLaVA (Liu et al.,\n2023) extends instruction tuning into the multimodal domain. Since then, different lines of work focus on\nimproving data curation (Chen et al., 2023; Laurençon et al., 2024a,b), visual representation (Tong et al.,\n2024a; Kar et al., 2025; Chen et al., 2024b), and instruction tuning strategies (Gao et al., 2024; Liu et al.,\n2024b). Using only a few million multimodal instruction tuning data, this line of research (Liu et al., 2024b;\nTong et al., 2024a; Li et al., 2024a) has enabled open-source MLLMs to reach performance levels comparable\nto those of proprietary models (OpenAI, 2024; Anthropic, 2024) on a number of benchmarks (Liu et al., 2024d;\nYue et al., 2024a,b) and applications (Zhai et al., 2024; Pan et al., 2024a).\nFrom Multimodal LLMs to unified models. Recent efforts to construct unified models have primarily relied on\neither extensive pretraining or heavy fine-tuning on billion-scale datasets. Some studies also use continuous\nembeddings for predicting visual tokens, integrating visual regression losses (Sun et al., 2024b,a) or leveraging\ndiffusion-based methods (Dong et al., 2024). Other approaches (Lu et al., 2022a; Aghajanyan et al., 2022;\nTeam, 2024; Wu et al., 2024b; Liu et al., 2024c; Wang et al., 2024b; Lu et al., 2024) tokenize multimodal\ndata into discrete tokens, which are then trained using autoregressive transformers. Recent research has\nalso explored hybrid strategies that combine autoregressive and diffusion objectives (Zhou et al., 2024b; Xie\net al., 2024). Different from previous studies, we demonstrate that unified models can be effectively trained\nin low-data regimes during instruction tuning, while also providing insights into the reciprocal relationship\nbetween visual understanding and visual generation.\n10\n6\nDiscussion\nIn this work, we propose VPiT—a simple yet effective extension to visual instruction tuning—that enables\nLLMs to predict multimodal tokens. VPiT unlocks the use of a more diverse range of instruction tuning data\nthan just visual question answering, such as text-to-image and pure image and video data. Through controlled\nexperiments, we find that visual generation ability emerges as a natural byproduct of improved visual under-\nstanding and requires modest additional generation data. In addition, we find that while visual understanding\nand generation are mutually beneficial, adding more visual understanding data disproportionately improves\noverall performance compared to adding more generation data.\nLeveraging these insights, we train MetaMorph by finetuning LLaMA-3.1 8B with VPiT. With a simple\ntraining process, MetaMorph achieves competitive performance in both visual understanding and generation.\nQualitative evaluation of our model shows that MetaMorph can leverage world knowledge and reasoning\nabilities of the base LLM during visual generation. For example, it can perform multimodal tasks that typically\nrequire multiple steps of reasoning, such as generating images of specialized proper nouns (“Chhogori”) or\nsolving visual puzzles (“generate an image of the animal resulting from a monarch caterpillar’s metamorphosis”).\nThis indicates that LLMs already possess a degree of “prior” visual knowledge which can be activated with\nonly minimal instruction tuning with VPiT. Overall, LLMs may have a similar representation space as unified\nand multi-functional models (Huh et al., 2024). We hope the insights from this work inspire more exploration\ntoward developing LLMs for general intelligence.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,\nMandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv\npreprint arXiv:2201.07520, 2022.\nPravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg,\nTheophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073,\n2024.\nAI@Meta. Llama 3 model card. 2024.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In NeurIPS,\n2022.\nZeyuan Allen-Zhu. ICML 2024 Tutorial: Physics of Language Models, 2024. Project page: https:\/\/physics.allen-zhu.\ncom\/.\nAnthropic. Claude, 2024.\nJimmy Lei Ba, Jamie Kiros, and Geoffrey E. Hinton. Layer normalization. In NeurIPS, 2016.\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and\nNicolas Ballas. Revisiting feature prediction for learning visual representations from video. In TMLR, 2024.\nFlorian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visualization of what your self-supervised\nrepresentation knows about. In TMLR, 2022.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023.\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\nLin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu\nTang, et al. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS,\n2024a.\nZhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo,\nZheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821, 2024b.\n11\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N\nFung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In\nNeurIPS, 2024.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\nZhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2024.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa\nDehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers\nfor image recognition at scale. In ICLR, 2021.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal\ndatasets. In NeurIPS, 2024.\nPeng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin,\nPeng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv\npreprint arXiv:2402.05935, 2024.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video\ndatabase for learning and evaluating visual common sense. In ICCV, 2017a.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answering. In CVPR, 2017b.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation\nmetric for image captioning. In EMNLP, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017.\nMinyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In ICML,\n2024.\nOğuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave:\nBroadening the visual encoding of vision-language models. In ECCV, 2025.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. In\nNeurIPS, 2024.\nBenno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy.\nLearning action and reasoning-centric image editing from videos and simulations. In NeurIPS, 2024.\nHugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved\nimage-text documents. Advances in Neural Information Processing Systems, 36, 2024a.\nHugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language\nmodels? arXiv preprint arXiv:2405.02246, 2024b.\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1–62,\n2022.\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a.\nKunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\nMvbench: A comprehensive multi-modal video understanding benchmark. In CVPR, 2024b.\nTianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: A self-supervised representation\ngeneration method. In NeurIPS, 2024c.\n12\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR,\n2024a.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved\nreasoning, ocr, and world knowledge, 2024b.\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024c.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\nHe, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024d.\nI Loshchilov. Decoupled weight decay regularization. In ICLR, 2019.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified\nmodel for vision, language, and multi-modal tasks. In ICLR, 2022a.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha\nKembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In CVPR,\n2024.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In\nNeurIPS, 2022b.\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. In ACL, 2022.\nBrandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training.\narXiv preprint arXiv:2403.09611, 2024.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m:\nLearning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019.\nOpenAI. gpt4o, 2024.\nJiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and\nrefinement of digital agents. In COLM, 2024a.\nXichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in\ncontext with multimodal large language models. In ICLR, 2024b.\nKonpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders:\nToward a meaningful and decodable representation. In CVPR, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision.\nIn ICML, 2021.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training\ntrillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage\nand Analysis, pages 1–16. IEEE, 2020.\nAdam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J Liu, Sharan Narang, Wei Li, and\nYanqi Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. In NeurIPS, 2022.\n13\nHao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual\ncot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought\nreasoning. In NeurIPS, 2024.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning\nwith reading comprehension, 2020.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024a.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multimodality. In ICLR, 2024b.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\nHashimoto. Alpaca: A strong, replicable instruction-following model, 2023.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.\nShengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal\nllms. In NeurIPS, 2024a.\nShengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems with language\nmodels. In NeurIPS, 2024b.\nShengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the\nvisual shortcomings of multimodal llms. In CVPR, 2024c.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. LLaMA 2: Open foundation and fine-tuned chat models.\n2023.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint\narXiv:2409.12191, 2024a.\nXinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022b.\nChengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu,\nChong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024a.\nPenghao Wu and Saining Xie. V*: Guided visual search as a core mechanism in multimodal llms. In CVPR, 2024.\nYecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model integrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024b.\nxAI. grok, 2024.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding\nand generation. arXiv preprint arXiv:2408.12528, 2024.\nHu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,\nLuke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024.\nTian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of Language Models: Part 2.1, Grade-School\nMath and the Hidden Reasoning Process. ArXiv e-prints, abs\/2407.20311, 2024. Full version available at http:\n\/\/arxiv.org\/abs\/2407.20311.\n14\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\nfor expert agi. In CVPR, 2024a.\nXiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao\nYu, Ge Zhang, et al. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv\npreprint arXiv:2409.02813, 2024b.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language\nmodels behave like bags-of-words, and what to do about it? In ICLR, 2022.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training.\nIn ICCV, 2023.\nYuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun,\nYi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In\nNeurIPS, 2024.\nRuohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander\nHauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language\nmodel reward. arXiv preprint arXiv:2404.01258, 2024.\nYuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, and Alexander Toshev. Pre-trained language models do\nnot help auto-regressive text-to-image generation. In EMNLP, 2023.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili\nYu, et al. Lima: Less is more for alignment. In NeurIPS, 2024a.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma,\nLuke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024b.\nOrr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, and Serena Yeung-levy. Video-star: Self-training enables\nvideo instruction tuning with any supervision. In arXiv preprint arXiv:2407.06189, 2024.\n15\nAppendix\nA\nTraining Details and Hyperparameters\nA.1\nMetaMorph Training\nWe follow the training recipe outlined in prior studies (Tong et al., 2024a; McKinzie et al., 2024), using a\ntwo-stage training approach. First, we pretrain a two-layer MLP with a GELU activation (Hendrycks and\nGimpel, 2016) as the adapter between the visual tokens and the LLM. We train this adapter on Cambrian\nadapter data while excluding all data points sourced from LAION (Schuhmann et al., 2022). Next, we finetune\nthe entire model, excluding the vision backbone, using the instruction tuning data described in Section 2.2\nand detailed in Appendix C.\nWe use DeepSpeed (Rajbhandari et al., 2020) Zero-3 to train our model on H100 GPUs. Detailed training\nhyperparameters for all experiments are provided in Table 2. We conduct all of the experiments with 1 epoch.\nBackbone\nData\nAdapter\nInstruction Tuning\nExperiment\nLLM\nAdapter\nInstruction Tuning\nlr\nwd bs\nlr\nwd\nbs\nSection 3 (LLaMA-3 8B)\nLLaMA-3 8B\nCambrian Adapter Data∗Section 3 Experiment Setting 4.90e-5 0.0 768 6.93e-5\n0\n1536\nSection 3 (LLaMA-3.1 8B) LLaMA-3.1 8B Cambrian Adapter Data∗Section 3 Experiment Setting 4.90e-5 0.0 768 6.93e-5\n0\n1536\nSection 3 (LLaMA-3 70B) LLaMA-3 70B Cambrian Adapter Data∗Section 3 Experiment Setting 4.90e-5 0.0 768 4.90e-5\n0\n768\nMetaMorph\nLLaMA-3.1 8B Cambrian Adapter Data∗\nAll Data from Section 2.2\n4.90e-5 0.0 768 6.93e-5\n0\n1536\nTable 2 Implementation details and hyperparameters for all experiments. ∗We exclude data points in LAION (Schuhmann\net al., 2022) from Cambrian adapter data.\nA.2\nDiffusion Visualizer Training\nWe leverage pretrained diffusion models such as Stable Diffusion 1.5 (Rombach et al., 2022). We use a 2-layer\nMLP projector to align the SigLIP embedding dimension with the cross-attention dimension in the pretrained\ndiffusion model. The first layer applies a linear transformation to map the input dimension to 2048, followed\nby layer normalization (Ba et al., 2016) and a ReLU activation. The second layer reduces the 2048-dimensional\nfeatures to the output dimension through a linear transformation, followed by a final layernorm.\nWe set the batch size to 2112. The learning rate schedule begins with a logarithmic warm-up over the first\n2000 steps, gradually increasing from zero to a peak value of 1.1e-5. After this warm-up phase, the learning\nrate decreases linearly over the next 12000 steps until reaching zero. We use the AdamW (Loshchilov, 2019)\noptimizer to train our model, with β parameters (0.9, 0.999). We apply a weight decay of 0.01.\nDuring diffusion training, we freeze the VAE encoder and Siglip encoder, only training the projector and\nthe diffusion U-Net. The CFG level is set to 0.7. This is because we start with a pretrained diffusion model\nand aim to transform the conditioning from CLIP text to SigLIP image embeddings. A higher CFG level\nensures the model maintains high image quality while gradually adapting to the new conditioning in the\nremaining fraction. Empirically, this approach achieves the best balance between adaptation and image quality.\nFor the training datasets, since we finetune the diffusion model to condition on SigLIP image embeddings,\ntraining this model does not require text descriptions for conditioning. Instead, we use images curated through\nin MetaCLIP (Xu et al., 2024) and train this diffusion model to visualize the visual tokens generated by\nMetaMorph.\nA.3\nEvaluation Benchmarks\nFor evaluation, we use nine ImageQA, one VideoQA and two generation benchmarks:\n• MMBench (Liu et al., 2024d): A comprehensive benchmark spans across 20 multimodal ability dimensions.\n• Seed (Ge et al., 2023): A benchmark focusing on visual tasks for multimodal understanding, consists of\n19k multiple choice questions with accurate human annotations.\n• V*STAR (Wu and Xie, 2024): A VQA benchmark designed for testing details in high-resolution images.\n16\nLoss\nImage QA\nAVG\nMMBenchEN\nSEED\nRealworldQA\nMMVP\nSQA\nMMMU\nVStar\nChartQA\nTextVQA\nNone (VQA Only)\n55.50\n73.11\n69.96\n55.69\n41.33\n80.39\n37.29\n46.60\n35.16\n59.96\nL1 Loss\n53.83\n72.17\n69.28\n57.25\n34.67\n79.00\n34.00\n45.55\n32.40\n60.17\nCosine Sim\n55.93\n73.78\n71.36\n55.03\n44.00\n79.83\n35.29\n47.64\n36.60\n59.79\n.\nTable 3 Comparison of different loss functions. Training with cosine similarity loss enables the model to effectively utilize\nnon-VQA data, which in turn enhances its visual understanding.\n• MMVP (Tong et al., 2024c): A benchmark for evaluating “CLIP-Blind” pairs in Vision Language Models.\n• MMMU (Yue et al., 2024a): A benchmark designed to evaluate multimodal models on extensive multi-\ndiscipline tasks requiring college-level subject knowledge and deliberate reasoning.\n• ChartQA (Masry et al., 2022): A large-scale benchmark involving visual and logical reasoning over charts.\n• TextVQA (Sidorov et al., 2020):A benchmark designed to evaluate models’ ability to read and reason\nabout text in images to answer questions.\n• ScienceQA (Lu et al., 2022b): A multimodal benchmark for answering science-related questions requiring\nintegration of visual and textual data.\n• RealWorldQA (xAI, 2024): A benchmark focused on real-world multimodal reasoning tasks.\n• MV-Bench (Li et al., 2024b): A benchmark contains a comprehensive video understanding benchmark,\nwhich covers 20 challenging video tasks that cannot be effectively solved with a single frame.\n• FID Score (Heusel et al., 2017): A metric for evaluating the quality of generated images by comparing\ntheir feature distributions with real images.\n• CLIP Score (Hessel et al., 2021): A benchmark metric that uses CLIP embeddings to measure alignment\nbetween generated images and their corresponding text descriptions.\nB\nAblation Studies on Visual Prediction Objective\nWe compare our approach to the commonly used L1 regression loss, which has been widely adopted in\ncontrastive self-supervised learning methods (LeCun, 2022; Bardes et al., 2024). For this comparison, we\ntrain MetaMorph, based on LLaMA-3 8B, using datasets described in Section 2.2. We highlight that cosine\nsimilarity and L1 loss influence the embedding outputs differently: cosine similarity enforces normalization,\nwhile L1 loss does not. This discrepancy in output normalization prevents a direct and fair comparison in\nterms of generation performance. Consequently, our analysis focuses exclusively on VQA performance.\nIn Table 3, we compare models trained using L1 loss and cosine similarity loss. Our analysis reveals that\ntraining with cosine similarity results in better average performance and outperforms L1 loss on most\nbenchmarks. Notably, these vision loss functions affect only tasks requiring visual predictions and do not\ndirectly influence VQA tasks, as the VQA training data does not include image token responses. This\nimprovement is potentially because training with cosine similarity enhances visual generation, which in turn\ncontributes to better visual understanding.\nTo further investigate, we compare our method—incorporating a broader range of non-VQA data alongside\nCambrian-7M—–with a baseline trained exclusively on Cambrian-7M. The results show that combining\nbroader dataset with cosine similarity loss leads to better performance across multiple benchmarks. This\nfinding reinforces our earlier observations in Section 3: enhancing visual generation capabilities contributes to\nimproved visual understanding, highlighting the benefits of leveraging non-VQA data.\n17\nI\nm\na\ng\ne\nQ\nA\nG\ne\nn\ne\nr\na\nt\ni\no\nn\nV\ni\nd\ne\no\nQ\nA\nP\nu\nr\ne\nV\ni\nd\ne\no\nImageQA (44.0%)\nGeneration (31.1%)\nCambrian-7M (Tong et al., 2024a) (7067.0 K)\nMetaCLIP (Xu et al., 2024) (5000.0 K)\nVideoQA (9.9%)\nVisual Thinking (3.2%)\nVideoStar (Zohar et al., 2024) (1055.0 K)\nVisualCoT (Shao et al., 2024) (361.0 K)\nShareVideo (Zhang et al., 2024) (540.0 K)\nVStar (Wu and Xie, 2024) (148.0 K)\nPure Video (8.8%)\nImage-to-Image (3.0%)\nHowTo100M (Miech et al., 2019) (1193.0 K)\nInstructPix2Pix (Brooks et al., 2023) (313.0 K)\nSmthSmthV2 (Goyal et al., 2017a) (220.0 K)\nAurora (Krojer et al., 2024) (169.0 K)\nFigure 11 Data composition. Left: The inner circle shows the distribution of MetaMorph data. Right: All the data\nsources and categories in the MetaMorph data.\nC\nData\nC.1\nData Composition\nWe summarize the categorization of data and the number of samples for each source in Figure 11. This\ndiverse dataset is curated to showcase that an LLM can be finetuned across a variety of tasks, where each\ntask contributes to and enhances the performance of others, as discussed in Section 3.1.\nC.2\nData Proprocessing\nAs discussed in Section 2.2, we use a wide range of data, spanning from visual question answering tasks to\nunlabeled video data. Here, we detail the preprocessing steps applied to each data source to convert them\ninto instruction-tuning-style QA conversations.\nImageQA. We use Cambrian-7M (Tong et al., 2024a), a dataset already curated in instruction tuning format.\nAn example entry looks like the below:\nExample from ImageQA\nPrompt:\n<image_start><image><image_end> What is the animal in the image?\nResponse:\nIt is a burmilla cat.\nVideoQA. We use VideoStar (Zohar et al., 2024) and ShareVideo (Chen et al., 2024a), both curated in an\ninstruction tuning format. For each video, we extract frames at a rate of one frame per second and input\nthese frames into the LLM. An example QA entry for an 8-second video is structured as follows:\nExample from VideoQA\nPrompt:\n<image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end><image_start><image><image_end><image_start><image><image_-\nend><image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end>\nWhat’s the color of the dog in this video? (a) white (b) yellow (c) black Please only answer a single letter\nand nothing else\nResponse:\nb\nGeneration data. We use image-text pairs in MetaCLIP (Xu et al., 2024). The original data consists of\n18\nimages paired with corresponding text descriptions. We add system prompts and define answering formats,\ntransforming the image-text pairs into question-answer formats suitable for instruction tuning.\nExample from Generation data\nPrompt:\nGenerate an image of a puppy.\nResponse:\nHere is an image based on your request: <image_start><image><image_end>.\nUnlike in ImageQA and VideoQA, we require the model to predict the visual tokens in the response.\nVisual Thinking Data. We explore incorporating vision as part of the model’s reasoning process to enhance its\nanswers. As a preliminary step, we experiment with the Visualization-of-Thought (Shao et al., 2024) and\nVStar (Wu and Xie, 2024) datasets. Originally, these datasets were designed to teach models how to utilize\nexternal tools and APIs, such as segmentation or zoom-in cropping. In this work, we aim to integrate these\nvisual skills directly into the model’s inference steps. We use system prompts, such as “think visually before\nyou answer the question”, to activate this visual reasoning mode. Here is an example:\nExample from Visual thinking data\nPrompt:\nThink visually before you answer the question: what is the color of the hat in the image <image_-\nstart><image><image_end>.\nResponse:\nLet’s consider this visually: <image_start><image><image_end> black.\nIn this example, the model first predicts a zoomed-in view of the key region in the image necessary for\nanswering the question, and then it provides the answer.\nImage to image data. The model is required to predict a transformed image based on a given text description.\nWe use Aurora (Krojer et al., 2024) and InstructPix2Pix (Brooks et al., 2023) datasets to train this capability.\nAn example is presented below:\nExample from Image to image data\nPrompt:\n<image_start><image><image_end> Make this a rainy day.\nResponse:\n<image_start><image><image_end>.\nHere, the model is expected to predict the visual tokens of the transformed image as per the specified prompt.\nPure video data. We explore commonly used open-source video datasets in instruction tuning: Something-\nSomethingV2 (Goyal et al., 2017a) and HowTo100M (Miech et al., 2019). We design the following tasks from\nthe pure video:\n1) Forward Frame Prediction. In this task, the model is presented with the initial frame of a video sequence\nand must predict the subsequent frames at fixed time intervals. An example is presented below:\n19\nExample of Forward Frame Prediction\nPrompt:\n<image_start><image><image_end> Can you predict what happens in the next 3 frames, each 5 seconds\napart?\nResponse:\n<image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end>\n2) Partial Sequence Completion. This task requires the model to complete a video sequence when given only\na subset of frames while maintaining temporal coherence:\nExample of Partial Sequence Completion\nPrompt:\n<image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end> Can you predict the 2 missing frames in this 5-second-interval sequence?\nResponse:\n<image_start><image><image_end><image_start><image><image_end>\n3) Reverse Temporal Prediction. This task challenges the model to reconstruct the preceding frames given the\nfinal frame of a sequence:\nExample of Reverse Temporal Reasoning\nPrompt:\n<image_start><image><image_end> Work backwards to predict the previous 4 frames, each 5 seconds\napart.\nResponse:\n<image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end><image_start><image><image_end>\n4) Temporal Sequence Reordering. In this task, the model receives a shuffled sequence of video frames and\nmust reconstruct their correct temporal order:\nExample of Temporal Sequence Reordering\nPrompt:\n<image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end><image_start><image><image_end>\nArrange these frames in their correct temporal sequence.\nResponse:\n<image_start><image><image_end><image_start><image><image_end><image_-\nstart><image><image_end><image_start><image><image_end>\nEach task is designed to train the model’s temporal understanding and visual reasoning capabilities.\nC.3\nPotential Image Leakage in Testing Data\nWhen selecting data sources, we carefully choose those that do not overlap with the testing sets of our\nevaluation data, such as COCO (Lin et al., 2014). However, given that the data used in a Section 2.2 is\ncomposed of numerous sources, some degree of data leakage may be inevitable. As discussed and analyzed in a\nprior work (Tong et al., 2024a), even when image overlap occurs, it does not necessarily imply that the exact\nimage-question pairs have been encountered during training. Unlike traditional unimodal computer vision\nresearch, where an image alone constitutes a data point, the multimodal paradigm treats each image-text\n(question-answer) pair as a distinct and unique data point.\n20\nJoint train With Other Data\n# of Generation Data\nFID Score\nYes\n1k\n68.5\nNo\n1k\n115.0\nYes\n5k\n19.2\nNo\n5k\n116,4\nYes\n10k\n18.7\nNo\n10k\n111.0\nYes\n50k\n17.1\nNo\n50k\n111.8\nYes\n200k\n15.2\nNo\n200k\n110.7\nYes\n200k\n14.7\nNo\n200k\n93.7\nYes\n1M\n14.4\nNo\n1M\n52.8\nYes\n3M\n15.1\nNo\n3M\n39.2\nYes\n5M\n14.3\nNo\n5M\n27.7\n.\nTable 4 Results of training solely on generation data vs. joint training with additional data. These results correspond to\nFigure 2. Joint training with additional data significantly improves generation performance. At 5,000 samples, the\nmodel begins to generate reasonably accurate visual tokens, indicating that visual generation is an ability unlocked\nthrough the learning of other tasks.\nJoint training Data\nData Type\nFID Score\nCLIP Score\nNone\n-\n110.5\n5.7\nImage-to-Image\nOther Visual Data\n97.5\n6.4\nVisual Thinking\nOther Visual Data\n93.5\n6.5\nPure Video\nOther Visual Data\n84.7\n8.1\nVideoQA\nVisual Understanding Data\n26.5\n16.1\nImageQA\nVisual Understanding Data\n18.9\n22.0\n.\nTable 5 Impact of joint training 200k generation data with different data types. These results correspond to Figure 3. Among\nthe data types analyzed, joint training with visual understanding data has the most significant impact on enhancing\nvisual generation performance.\nD\nGenerating Visual Tokens\nHere, we include the quantitative results of all the experiments in Section 3.\nD.1\nResults of Samples Needed to Unlock Visual Generation\nTable 4 presents the quantitative results corresponding to Figure 2, which examines generation performance\nunder two conditions: training exclusively on generation data and joint training with all other data described\nin Section 2.2. The results demonstrate that the model can develop the ability for visual generation with a\nrelatively modest amount of data when trained jointly with understanding tasks. In contrast, teaching this\nskill in isolation requires a substantially larger dataset.\nIn Table 5, we present the quantitative results corresponding to Figure 3, which investigates the impact of\njoint training on generation data in combination with various types of data outlined in Section 2.2. The results\nshow that joint training with visual understanding data—–specifically ImageQA and VideoQA–—provides the\nmost significant improvement in visual generation performance.\nD.2\nResults of Joint training Different Understanding and Generation Data\nIn Table 6, we present the numerical results of joint training with varying scales of understanding data (1M,\n4M, 7M) and generation data (200k, 500k, 1M, 2M, 3M, 4M). These findings demonstrate that increasing the\n21\nData Composition\nImage QA\nGeneration\n# of VQA Data\n# of Generation Data\nAverage\nMMBenchEN\nSEED\nRealworldQA\nMMVP\nSQA\nMMMU\nVStar\nChartQA\nTextVQA\nFID Score\nCLIP Score\n1M\n200k\n46.4\n60.0\n62.2\n50.3\n24.0\n80.0\n38.4\n37.4\n16.4\n48.8\n28.3\n15.2\n1M\n500k\n48.2\n66.4\n63.2\n50.8\n24.3\n80.4\n39.9\n38.7\n18.2\n51.6\n28.1\n15.9\n1M\n1M\n49.1\n70.1\n65.2\n52.2\n21.3\n80.0\n39.5\n38.7\n20.4\n54.6\n27.3\n16.5\n1M\n2M\n49.9\n67.8\n66.0\n50.2\n30.3\n80.2\n38.9\n39.0\n21.8\n54.8\n23.1\n17.8\n1M\n3M\n51.1\n71.3\n67.1\n55.4\n33.0\n79.5\n38.8\n37.4\n22.7\n55.0\n21.1\n21.1\n1M\n4M\n51.4\n71.1\n66.9\n52.4\n31.0\n80.5\n39.8\n41.1\n24.0\n56.0\n18.4\n22.3\n4M\n200k\n53.8\n73.1\n68.8\n55.0\n34.7\n81.2\n38.5\n44.0\n29.5\n59.2\n21.4\n20.5\n4M\n500k\n53.3\n73.0\n69.9\n55.3\n32.7\n80.6\n40.2\n39.3\n29.6\n58.9\n16.0\n24.8\n4M\n1M\n54.2\n73.8\n69.6\n54.9\n33.3\n82.1\n36.6\n45.6\n32.4\n59.9\n16.0\n24.8\n4M\n2M\n53.8\n72.8\n70.3\n55.2\n37.3\n80.8\n36.8\n44.0\n31.2\n56.2\n15.6\n24.7\n4M\n3M\n54.3\n71.8\n70.1\n57.7\n36.0\n81.0\n38.0\n42.9\n32.6\n59.0\n16.1\n24.8\n4M\n4M\n54.4\n75.2\n69.9\n56.0\n37.3\n81.4\n38.1\n40.8\n31.6\n59.3\n15.3\n25.5\n7M\n200k\n55.8\n73.1\n70.3\n55.6\n42.0\n81.0\n40.8\n44.0\n35.2\n60.6\n18.2\n22.3\n7M\n500k\n55.6\n74.4\n70.6\n56.2\n38.7\n81.9\n37.9\n44.0\n36.0\n60.5\n15.2\n25.5\n7M\n1M\n55.8\n74.3\n70.3\n56.3\n42.7\n81.3\n36.6\n44.5\n35.8\n60.6\n14.5\n26.6\n7M\n2M\n55.4\n73.9\n71.1\n56.9\n40.0\n81.6\n35.9\n42.4\n35.4\n61.6\n14.8\n27.1\n7M\n3M\n55.6\n74.2\n71.0\n57.3\n38.0\n81.1\n40.1\n43.5\n35.0\n60.2\n14.2\n27.5\n7M\n4M\n56.2\n75.4\n70.4\n55.4\n44.0\n80.4\n39.6\n45.0\n35.2\n60.2\n14.9\n26.3\n.\nTable 6 Full results of joint training on varying amounts of VQA data (1M, 4M, 7M) and generation data (200k, 500k, 1M, 2M, 3M,\n4M). These results correspond to Figure 4, Figure 5, Figure 7, and Figure 8, which analyze how different combinations\nof understanding and generation data impact the model’s visual understanding and generation performance.\nPretrained LLM\nImage QA\nGeneration\nLLM\nAverage\nMMBenchEN\nSEED\nRealworldQA\nMMVP\nSQA\nMMMU\nVStar\nChartQA\nTextVQA\nFID Score\nCLIP Score\nLLaMA-3 8B\n55.8\n74.3\n70.3\n56.3\n42.7\n81.3\n36.6\n44.5\n35.8\n60.6\n14.5\n26.6\nLLaMA-3.1 8B\n56.7\n75.8\n70.2\n56.2\n44.7\n81.9\n41.2\n43.4\n36.0\n61.3\n13.2\n27.1\nLLaMA-3 70B\n60.7\n80.7\n72.6\n58.3\n48.7\n87.8\n48.9\n47.1\n37.4\n65.0\n13.8\n26.8\n.\nTable 7 Full results of training on different LLMs. We train 7M VQA data and 1M generation data on different LLM\nbackbones (LLaMA-3 8B, LLaMA-3.1 8B, and LLaMA-3 70B) and measure understanding and generation performance.\namount of understanding data yields more substantial improvements in both understanding tasks (e.g., VQA\nperformance) and generation tasks (e.g., FID scores and CLIP scores) compared to increasing the amount of\ngeneration data. These results, consistent with our analysis in Section 3.2 and Section 3.3, highlight that\nunderstanding data play a more pivotal role in enhancing performance across both task types.\nD.3\nResults of Training on Different LLMs\nWe present the results of training with 7M VQA data and 1M generation data across various LLM backbones,\nincluding LLaMA-3 8B, LLaMA-3.1 8B, and LLaMA-3 70B. As shown in Table 7, which corresponds to\nthe results in Figure 6, we observe that stronger LLM backbones lead to improvements in both visual\nunderstanding and visual generation. These findings further support the conclusion that visual understanding\nand generation are reciprocal processes, where advancements in one drives enhancements in the other.\nE\nMore Examples of MetaMorph\nWe provide additional examples of MetaMorph in Figure 12 and Figure 13. These examples illustrate how\nMetaMorph extends beyond the capabilities of typical MLLMs by leveraging learned skills to perform novel\ntasks such as visual reasoning and visual transformation. In Figure 12, when prompted with the question\n22\n“What is the type of hat?”, MetaMorph first generates visual tokens related to hats and then answers correctly\nwith “top hat”. The model also demonstrates the ability to perform image transformations, such as creating\na cartoon version of an image or altering it to appear as daytime. Additionally, we showcase examples of\nMetaMorph solving implicit puzzles, such as interpreting “a rearrangement of the letters in the word ’tca”’,\nbefore generating the corresponding visual tokens of cats.\n23\nMake a child cartoon from it\n<image_start><image_end>\nGenerate an image of the pet animal whose \nname is a rearrangement of the letters in the \nword 'tca’\nHere's the generated image based on \nyour request: <image_start><image_end>\nWhat's the animal in this image?\nThe animal in the image is a domestic \ncat, specifically a tabby cat. Tabbies are \ncharacterized by their distinctive coat \npatterns, which include stripes, dots, and \nswirling patterns. The markings on this \ncat's face, with the 'M' shape on its \nforehead and the stripes on its ears, are \ntypical of a tabby pattern.\nReflect on the visual details: What's the type of \nhat that animal wear in the image?\nLet's imagine the critical detail: \n<image_start><image_end> The animal is \nwearing a top hat.\nHow many birds are in the image and why?\nThere is only one bird in the image. The \nbird is captured in a single frame, and its \nreflection in the water creates the illusion \nof two birds.\nIs the coke in the ads regular or zerosugar?\nThe coke in the ads is zero sugar.\nFigure 12 Examples of MetaMorph (I). We showcase examples of MetaMorph’s capabilities: transforming images based\non prompts (top-left), answering challenging questions (top-right), integrating visual tokens into reasoning processes\n(bottom-left), implicitly solving puzzles (bottom-right) and answering tricky video-qa questions. (bottom).\n24\nGenerate an image of a puppy\nHere's the generated image based on \nyour request: <image_start><image_end>\nWhat’s this city?\nThe city in the image is New York City, \nspecifically the Lower Manhattan area, \nwhich is home to the iconic World Trade \nCenter complex. The presence of the \nOne World Trade Center, which is the \ntallest building in the image, is a \ndistinctive feature.\nCan you turn it to view during the daytime?\n<image_start><image_end>\nGenerate an image of Kagu\nHere's the generated image based on \nyour request: <image_start><image_end>\nFigure 13 Examples of MetaMorph (II).We showcase more examples of MetaMorph’s capabilities: answering questions and\ntransforming images in one conversation (left), generating images (top-right), and leveraging knowledge in LLMs to\ngenerate rare concepts (bottom-right).\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MetaMorph: Multimodal Understanding and Generation via Instruction Tuning.pdf"}
{"title":"SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding","authors":"Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai","summary":"The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased.","url":"http:\/\/arxiv.org\/abs\/2412.09604v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.09604v1","published":1734029966000,"comment":null,"pdf_text":"SynerGen-VL: Towards Synergistic Image Understanding and Generation with\nVision Experts and Token Folding\nHao Li1,2∗†, Changyao Tian2,1∗†, Jie Shao3,1∗†, Xizhou Zhu4,5∗, Zhaokai Wang6,1†, Jinguo Zhu1,\nWenhan Dou4,5, Xiaogang Wang2, Hongsheng Li2, Lewei Lu5, Jifeng Dai4,1,7B\n1OpenGVLab, Shanghai AI Laboratory\n2MMLab, The Chinese University of Hong Kong\n3Nanjing University\n4Tsinghua University\n5SenseTime Research\n6Shanghai Jiao Tong University\n7Beijing National Research Center for Information Science and Technology\nAbstract\nThe remarkable success of Large Language Models (LLMs)\nhas extended to the multimodal domain, achieving out-\nstanding performance in image understanding and gener-\nation. Recent efforts to develop unified Multimodal Large\nLanguage Models (MLLMs) that integrate these capabili-\nties have shown promising results. However, existing ap-\nproaches often involve complex designs in model archi-\ntecture or training pipeline, increasing the difficulty of\nmodel training and scaling.\nIn this paper, we propose\nSynerGen-VL, a simple yet powerful encoder-free MLLM\ncapable of both image understanding and generation. To\naddress challenges identified in existing encoder-free uni-\nfied MLLMs, we introduce the token folding mechanism and\nthe vision-expert-based progressive alignment pretraining\nstrategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After be-\ning trained on large-scale mixed image-text data with a uni-\nfied next-token prediction objective, SynerGen-VL achieves\nor surpasses the performance of existing encoder-free uni-\nfied MLLMs with comparable or smaller parameter sizes,\nand narrows the gap with task-specific state-of-the-art mod-\nels, highlighting a promising path toward future unified\nMLLMs. Our code and models shall be released.\n1. Introduction\nThe remarkable success of Large Language Models\n(LLMs) [7, 59, 84] has been extended to the multimodal\ndomain, achieving impressive performance in image under-\nstanding [11, 44, 80, 99] and image generation [75, 83, 95].\nRecent research has aimed to develop unified Multimodal\n* Equal contribution. † Interns at Shanghai AI Laboratory.\nB Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\nLarge Language Models (MLLMs) with synergistic image\nunderstanding and generation capabilities [8, 20, 81, 92, 94,\n107]. Although they have demonstrated competitive perfor-\nmance in both tasks, they often involve complex designs as\nillustrated in Fig. 1(a)∼(d), such as (a) relying on external\ndiffusion models for image generation [20, 24, 81, 108], (b)\nusing different training objectives (i.e. diffusion and autore-\ngression) for the two tasks [96, 107], (c) employing distinct\nimage encoders for each task [92], and (d) require addi-\ntional semantic pretraining for image tokenizers [94]. These\ncomplexities disrupt the simplicity of the next token predic-\ntion paradigm of LLMs, increasing systematic difficulty and\nlimiting scalability.\nTo address these complexities, some studies have tried\nto develop unified MLLMs with simple architectures, elim-\ninating dependencies on external models, distinct task-\nspecific models, and additional semantic pretraining [8, 45,\n91]. As shown in Fig. 1(e), these approaches adopt a similar\ntokenization strategy for both images and text, and model\nboth image understanding and generation tasks within a\nunified next token prediction framework. The image tok-\nenizers [22] are pretrained for reconstruction on pure image\ndata, without requiring human annotations or text supervi-\nsion, which allows for a broad data distribution and strong\nscalability. These concise and scalable designs have demon-\nstrated a promising path toward synergistic image under-\nstanding and generation.\nNevertheless, these methods still face some key chal-\nlenges in practical use. Specifically, (1) since both image\nunderstanding and generation rely entirely on MLLMs, sub-\nstantial training is required to incorporate vision capabil-\nities into MLLMs. However, this may interfere with the\npretrained knowledge of LLMs, resulting in reduced gen-\neral perception and generalization capabilities. Although\nexisting methods try to avoid this by training MLLM from\nscratch using mixed text and multimodal data, they face\nconsiderable challenges in optimizing stability, data qual-\n1\narXiv:2412.09604v1  [cs.CV]  12 Dec 2024\nMLLM (AR)\nPretrained\nDiffusion \nModel\nSemantic  \nEncoder\nTokenizer\nText\nImage\nText \nResponse\nGenerated \nImage\nTokenizer\nText\nImage\nTokenizer\nDiffusion\nText \nResponse\nGenerated \nImage\nTokenizer\nText\nImage\nTokenizer\nText \nResponse\nGenerated \nImage\nMLLM (AR)\nSemantic  \nEncoder\nText\nImage\nTokenizer\nText \nResponse\nGenerated \nImage\nMLLM (AR)\nSemantic  \nTokenizer\nText\nImage\nTokenizer\nText \nResponse\nGenerated \nImage\nMLLM (AR)\nTokenizer\ne.g., DreamLLM, SEED-X, \nVL-GPT, MM-Interleaved \ne.g., Transfusion, Show-o\ne.g., Janus\ne.g., VILA-U\ne.g., LWM, Chameleon, Emu3, Ours\nNo externalized models\nNo additional semantic pretraining\nUnified training objective & \ninference for und. and gen.\nNo externalized models\nNo additional semantic pretraining\nUnified training objective & \ninference for und. and gen.\nNo externalized models\nNo additional semantic pretraining\nUnified training objective & \ninference for und. and gen.\nNo externalized models\nNo additional semantic pretraining\nUnified training objective & \ninference for und. and gen.\nNo externalized models\nNo additional semantic pretraining\nUnified training objective & \ninference for und. and gen.\n(a)\n(b)\n(c)\n(d)\n(e)\nMLLM (AR)\nFigure 1. Comparison among exemplary unified MLLMs for synergizing image understanding and generation tasks. Compared\nwith methods (a)∼(d) that incorporate complicated designs of model architectures, training methods, and the use of external pretrained\ndiffusion models, (e) encoder-free unified MLLMs adopt a simple design that uses the simple next token prediction framework for both\nimages understanding and generation tasks, allowing for broader data distribution and better scalability.\nity, and training cost [8, 91]; (2) current visual tokenizers\nrequire low feature downsample ratios to ensure reconstruc-\ntion with fine details [22]. This results in long visual token\nsequences for high-resolution images, which is unsuitable\nto LLMs and limits the use of high-resolution images, thus\naffecting performance, especially for image understanding.\nIn this paper, we aim to build a simple yet powerful uni-\nfied MLLM that addresses the aforementioned challenges.\nSpecifically, 1) inspired by image understanding models\nwith Multimodal Mixture-of-Experts (MMoE) [37, 51, 89]\nstructure, we introduce vision experts with additional pa-\nrameters dedicated to image representation. Aligning the\nvision experts to the frozen LLM helps integrate vision ca-\npabilities while minimizing disruption to the LLM’s pre-\ntrained knowledge; 2) to effectively support high-resolution\nimages, the input visual token sequence can be compressed\nto reduce its length, while an additional decoder would be\nemployed during image generation to reconstruct detailed\nimage sequences from the compressed representations.\nFollowing this perspective, we propose SynerGen-VL,\na high-performance unified MLLM with synergistic im-\nage understanding and generation capabilities, using non-\nsemantic discrete image tokens to represent images.\nAs\nshown in Fig. 2, compared with previous encoder-free uni-\nfied MLLMs, SynerGen-VL employs additional vision ex-\nperts, i.e. image-specific Feed-Forward Networks (FFNs),\nto incorporate vision capabilities into pretrained LLMs.\nMeanwhile, SynerGen-VL uses a hierarchical architec-\nture to increase the feature downsampling ratio within the\nMLLM. Specifically, the input image token sequences are\ndownsampled by token folding to reduce their lengths.\nTo generate high-quality images, the generated token se-\nquences are unfolded by a shallow autoregressive Trans-\nformer head. To preserve the LLM’s pretrained knowledge,\nToken Unfold (16x)\n…\n…\nH\nMLLM\nToken Fold (16x)\n256 tokens\n4H\nW\n4W\n…\n…\n256 tokens\n…\nMLLM\n…\n…\n256 tokens\nVision Expert \nOurs\nChameleon, Emu3, …\n4096 tokens\n256 tokens\n…\n4096 tokens\nFigure 2. Comparision between SynerGen-VL and previous\nencoder-free unified MLLMs.\nSynerGen-VL adopts a token\nfolding and unfolding mechanism and vision experts to build a\nstrong and simple unified MLLM. With the same image context\nlength, SynerGen-VL can support images of much higher reso-\nlutions, ensuring the performance of both high-resolution image\nunderstanding and generation.\nwe perform two-stage alignment pretraining with mixed im-\nage understanding and generation data: (1) only image-\nspecific FFNs are trained with noisy web data to achieve\nbasic semantic understanding and image generation aligned\nwith the representation space of LLM; (2) image-specific\nFFNs and self-attention layers are trained with high-quality\nimage understanding and generation data to further inte-\ngrate multimodal features into the pretrained LLM. After\nalignment pretraining, SynerGen-VL supports image under-\nstanding and generation tasks simultaneously through su-\n2\npervised instruction fine-tuning.\nWe train SynerGen-VL on large-scale mixed image-text\ndata and evaluate it on a range of image understanding\nand generation benchmarks. Experimental results demon-\nstrate that, with its simple design, SynerGen-VL achieves or\nsurpasses the performance of existing encoder-free unified\nMLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art (SoTA)\nmodels.\nIn particular, with only 2.4B activated parame-\nters, SynerGen-VL achieves image understanding and gen-\neration performance on par with Emu3 [91], which has 8B\nparameters, highlighting its strong potential as a promising\npath towards next-generation unified MLLM. Our contribu-\ntions are summarized as follows:\n• We propose SynerGen-VL, a Multimodal Large Lan-\nguage Model (MLLM) with simple architecture and train-\ning process, capable of handling both image understand-\ning and generation through a unified next token prediction\nparadigm.\n• We introduce the token folding mechanism and the\nvision-expert-based progressive alignment pretraining\nto unified MLLMs, which effectively support high-\nresolution image understanding and reduce training dif-\nficulty.\n• Experiments demonstrate that SynerGen-VL achieves\ncompetitive performance in a range of image understand-\ning and generation benchmarks, revealing a promising\npath towards future unified MLLM.\n2. Related Work\nUnified MLLMs for Synergistic Image Understanding\nand Generation.\nUnifying image understanding and gen-\neration in a single MLLM has attracted wide academic\nattention.\nEarly efforts primarily integrate an external\ndiffusion decoder for image generation [24, 38, 76, 77,\n93].\nInspired by the success of next-token prediction in\nLLMs, some studies explore using discrete visual tokens\nto represent and generate images in a fully autoregressive\nparadigm [8, 30, 45, 91, 94, 101]. To achieve high per-\nformance for both image understanding and generation,\nsome recent methods have decoupled image understand-\ning and generation.\nTransfusion [107] and Show-o [96]\nintegrate textual autoregressive modeling for image under-\nstanding and visual diffusion modeling for image genera-\ntion. Janus [92] uses two different image representations,\nrespectively for understanding and generation, to address\nthe varying levels of information granularity required by the\ntwo tasks.\nHowever, previous methods either involve complex de-\nsigns or face challenges such as computational cost and\noptimization stability. To address the issues, our method\nleverages Multimodal Mixture-of-Experts and a token fold-\ning strategy to construct a fully autoregressive MLLM, en-\nabling synergistic high-resolution image understanding and\ngeneration. Experiments show that SynerGen-VL achieves\nstate-of-the-art performance on various benchmarks.\nEncoder-free MLLMs.\nMost existing MLLMs adopt an\nencoder-based framework that integrates a separate image\nencoder like CLIP [62] into a pretrained LLM [1, 7, 12].\nMeanwhile, some recent attempts have also begun to de-\nvelop encoder-free MLLMs architecture due to their sim-\nplicity.\nSome works\n[8, 91, 96, 107] adopt VQ tok-\nenizers [22] to represent images as discrete tokens. Oth-\ners [10, 19, 51] use simple linear projection (i.e., patch em-\nbedding layer) to embed the images. In this paper, we build\nan encoder-free MLLM using discrete image representation\nthrough VQ tokenizers, which has stronger reconstruction\nability to support both understanding and generation.\nToken Folding and Unfolding.\nIn language processing,\nearly attempts like Funnel Transformer [17] and Data-\nMUX [57] propose the downsample-upsample paradigm,\ni.e.\ncompress the token length in intermediate Trans-\nformer layers,\nto process long sequences efficiently.\nMegaByte [102] segments sequences into patches, and then\nuses a local sub-model within patches and a global model\nbetween patches. HRED [56] uses a lower-frequency model\nto process input sub-sequences without global context, and\ndecodes outputs at the original data frequency. Block Trans-\nformer [28] introduces a global-to-local structure to opti-\nmize the inference efficiency of autoregressive LLMs. In\nthis paper, we adopt the token folding and unfolding mech-\nanism to support high-resolution image understanding and\ngeneration. Since current visual tokenizers generate very\nlong visual token sequences for high-resolution images,\nwhich is unsuitable to LLMs, we fold the visual token se-\nquences before LLM modeling, and decode them back into\nthe original local token sequences for image generation.\n3. SynerGen-VL\n3.1. Architecture\nSynerGen-VL is a unified MLLM with synergistic image\nunderstanding and generation capabilities. Fig. 3 shows an\noverview of SynerGen-VL. Similar to previous work [8, 45,\n91], SynerGen-VL requires no externalized image genera-\ntion models or additionally pretrained semantic encoders. It\nuses a single LLM with the unified next-token prediction\nobjective for both tasks. Specifically, the input images and\ntext are represented as discrete tokens by their correspond-\ning tokenizers. The input multimodal token sequence con-\nsists of both image and text tokens, which always starts with\na special token <s> and ends with another special token\n<\/s>. Special tokens <boi> and <eoi> are inserted be-\nfore and after each image to indicate the beginning and end\n3\n…\n…\n(b) Image Understanding\n(Autoregressive Text Prediction)\nVQ \nTokenizer\nText \nTokenizer\nText Head\nMultimodal Large Language Model\nDiscrete Image \nTokens\nToken\nEmbeddings\nMulti-head Self-attention\nText FFN\nVision FFN\nN×\nToken Fold\nMLP\n(a) Next Token Prediction Training\nMultimodal Large Language Model\nVQ Tokenizer\nToken Fold\nMLP\nText Tokenizer\nText Head\nToken\nUnfold\n 123  234  456  567\n 987  876  765  543\n 112  223  334  445\n 211  322  433  544\n 123  234  987  876   456  567  765  543  112  223   211  322  334  445  433  544\n<user> Describe the image <assistant> A cat that ...\n(c) Image Generation\n(Autoregressive Image Prediction)\nMultimodal Large Language Model\nToken Fold\nMLP\nText Tokenizer\nA cat with a yellow flower on \nits head and a golden necklace\nToken Unfold\nVQ Decoder\nA cat that ...\n<BOI>\n<BOI>\nFigure 3. Overview of the proposed SynerGen-VL. The image and text are represented as discrete tokens, and modeled with a single\nLLM and unified next-token prediction paradigm. Text and vision expert FFNs are introduced to incorporate visual capabilities into the\npretrained LLM. To support processing high-resolution images, the input image token sequence is folded to reduce its length, and unfolded\nby a shallow autoregressive Transformer head to generate images.\nof the image, respectively. The multimodal token sequence\nis processed with a causal Transformer [86] initialized from\na pretrained LLM. The image and text output tokens are pre-\ndicted autoregressively, then the image output tokens can be\ndecoded into pixels with the pretrained VQ decoder.\nInput Embedding with Visual Token Folding.\nExist-\ning discrete VQ-based image tokenizers require low feature\ndownsample ratios to ensure reconstruction with fine de-\ntails. This leads to long visual token sequences, limiting the\nuse of high-resolution images in LLMs for detail-rich im-\nage understanding such as OCR-related tasks. To address\nthis issue, we employ Token Folding to increase the feature\ndownsampling ratio within the MLLM. Specifically, given\nan image I ∈RH×W ×3 (e.g., H = W = 512), an off-the-\nshelf pretrained discrete image tokenizer is used to encode\nthe image into a 2D grid of discrete tokens with shape h×w,\nwhere h = H\/p and w = W\/p. Here, p is the tokenizer’s\ndownsampling ratio (e.g., p = 8). The visual token em-\nbeddings are then obtained from a learnable look-up table,\nand a learnable positional embedding PE is added to each\ntoken embedding to preserve spatial prior. Similar to Pixel\nShuffle, token embeddings are folded by concatenating ev-\nery m × n token patch into one single visual token (m = 2\nand n = 8 by default). Here, each folded token patch can\nbe rectangular, following the latest practice of MLLMs for\nperception [88]. As shown in Fig. 3 (a), this results in an ad-\nditional downsampling ratio of m × n, greatly compressing\nthe token sequence for MLLM. For example, for an image\nof size 512×512, the original Emu3 [91] tokenizer produces\nvisual 4096 tokens, while SynerGen-VL uses only 256 to-\nkens to represent it in MLLM with a token folding ratio of\n2 × 8.\nAfter Token Folding, an MLP is applied to each folded\nimage patch embedding to align its feature dimension with\nthe LLM’s input dimension, yielding the final visual input\nfeatures xV ∈R( h·w\nm·n )×d. The whole image embedding pro-\ncess can be formulated as:\nxV = MLP(TokenFold(TokenEmbed(I) + PE)). (1)\nFor text input, we employ the built-in word tokenizer and\nthe text token embedding look-up table of the pretrained\nLLM to encode it into text embeddings xT .\nThe visual token embeddings xV are concatenated with\nthe text token embeddings xT and learnable special token\nembeddings (i.e., <s>, <\/s>, <boi>, <eoi>), according\nto the input order to form the final multimodal inputs into\nthe MLLM.\nIncorporating Visual Capabilities with Multimodal\n4\nMixture-of-Experts (MMoEs). To avoid substantial tun-\ning of the pretrained LLM while incorporating visual capa-\nbilities into it, we introduce additional parameters to each\nLLM’s Feed-Forward Network (FFN) layer as vision ex-\nperts dedicated to image representation. Specifically, the\nFFN output of the i-th token is altered to\nFFN-MMoE(xi) =\n(\nFFNV (xi),\nif xi is visual,\nFFNT (xi),\nif xi is textual,\n(2)\nwhere FFNT denotes the original FFN in the pretrained\nLLM for text tokens, and FFNV denotes the vision expert\nFFN, which shares the same architecture as FFNT and is\ninitialized from the corresponding pretrained text FFN.\nInstead of tuning the entire pretrained LLM, we perform\na two-stage alignment pretraining on the vision expert FFNs\nwith mixed image understanding and generation data. By\naligning the visual representations with the representation\nspace of the pretrained LLM, we minimize the impact of\nthe LLM’s pretrained knowledge, ensuring the general per-\nception and generalization capabilities. We introduce the\ntwo-stage alignment pretraining in Sec. 3.2.\nImage Generation with Visual Token Unfolding. As the\ntoken-folding operation reduces the number of visual to-\nkens at the input side of MLLM, the visual tokens at the\noutput must be unfolded to generate images. We leverage\na small causal transformer as the image generation head.\nSuch head shares the same micro-architecture as the LLM\nbut with fewer layers (e.g., 4) and has its own image token\nembedding and position embedding look-up tables, accom-\npanied by an output classifier for VQ tokens.\nFor the i-th folded image patch, hi is the corresponding\noutput embedding generated by the MLLM. To predict the\nj-th discrete token id vj\ni in this image patch, its probability\ndistribution is formulated autoregressively as:\np(vj\ni |v<j\ni , hi) = Softmax(fθ(v<j\ni , hi)),\n(3)\nwhere fθ represents the causal Transformer head with pa-\nrameters θ, v<j\ni\ndenotes all generated VQ token ids before\nthe i-th visual tokens in this image patch. After generating\nVQ token ids for all image patches, we concatenate them\nto obtain the complete sequence of VQ token ids with the\nshape of h × w for image pixel decoding.\n3.2. Training\nTraining Objective.\nThe overall training objective of\nSynerGen-VL consists of two main components: text token\nprediction and image token prediction. Both modalities em-\nploy the same next-token prediction objective, formulated\nas\nL = −\nX\ni∈T\nlog p(ˆxi\nT = xi\nT |x<i) −λ\nX\ni∈V\nlog p(ˆxi\nV = xi\nV |x<i),\n(4)\nTask\n#Sam.\nDatasets\nGen.\n667M\nLAION-Aesthetics [67], Megalith [52], SAM [33],\nObjects365 [69], ImageNet-1k [18],\nS.1\nUnd.\n667M\nLaion-En [67], COYO [6], SAM [33]\nGen.\n170M\nLAION-Aesthetics [67], Megalith [52], Objects365 [69],\nUnsplash [85], Dalle-3-HQ [3], JourneyDB [74],\nInternal Dataset\n170M\nCaptioning: Laion-En [67], Laion-Zh [67], COYO [6],\nGRIT [60], COCO [40], TextCaps [71]\nDetection: Objects365 [69], GRIT [60], All-Seeing [90]\nOCR (large): Wukong-OCR [26], LaionCOCO-OCR [68],\nCommon Crawl PDF\nOCR (small): MMC-Inst [41], LSVT [79], ST-VQA [5],\nRCTW-17 [70], ReCTs [106], ArT [13], SynthDoG [32],\nChartQA [53], CTW [104], DocVQA [15], TextOCR [73],\nS.2\nUnd.\n170M\nCOCO-Text [87], PlotQA [55], InfoVQA [54]\nTable 1. Summary of datasets used in Visual Alignment Pre-\ntraining. “S.1” and “S.2” denote the first and second stage. “Gen.”\nand “Und.” denote the image generation and understanding task.\n“#Sam.” denotes the number of total samples seen during training\nof each task at each stage. Note that all data used for image under-\nstanding in the second stage is also used in InternVL-1.5 [11].\nwhere T , V are the index sets indicating all text and image\ntokens in the multimodal sequence, respectively, ˆxi\nT and ˆxi\nV\ndenote the predicted text and image token at position i. The\nfinal loss objective is the weighted sum of the text and image\nlosses, with a hyperparameter λ to balance the relative loss\nweight between image understanding and image generation.\nVisual Alignment Pretraining.\nTo preserve the LLM’s\npretrained knowledge, we conduct a progressive two-stage\nalignment pretraining strategy, with both stages utilizing a\nmixture of data for image understanding and generation.\nThe detailed dataset composition is shown in Tab. 1.\nThe first stage aims to bridge visual elements with con-\ncepts in the representation space of the pretrained LLM,\nthereby obtaining basic semantic understanding and im-\nage generation abilities.\nTo avoid interfering with the\nLLM’s pretrained knowledge, we freeze the parameters of\nthe LLM components and only train the image-specific pa-\nrameters (i.e., the visual token embedding and projection\nlayers, the vision experts in MLLM, and the visual to-\nken unfolding head).\nFor image understanding, we use\nthe large-scale noisy image-text pair data LAION-En [67]\nand Coyo-700M [6] for basic concept learning, while in-\ncorporating a portion of synthesized captions from sam-\nples in [6, 33, 67] generated by InternVL-7B to achieve\nbetter semantic alignments.\nFor image generation, apart\nfrom the large-scale noisy LAION-Aesthetics data [67], we\nfollow [9, 92] to accelerate the pixel dependency learning\nwith [18] and improve the learning of object concepts and\nrelations through [33, 52, 69]. To distinguish the image\nunderstanding and generation tasks, we use the prompt of\n“Provide a one-sentence caption for the image” for under-\nstanding data, while adding “Generate an image of:” before\nthe text prompts of the generation data.\nIn the second stage, we further integrate visual capabili-\n5\nModel\n#A-Param\nPOPE\nMMB\nMMVet\nMMMU\nMME\nMME-P\nMathVista\nSEED-I\nOCRBench\nUnderstanding Only\nEncoder-based\nLLaVA-1.5 [43]\n7B\n85.9\n64.3\n31.1\n35.4\n-\n1512\n-\n58.6\n-\nMini-Gemini-2B [38]\n3.5B\n-\n59.8\n31.1\n31.7\n1653\n-\n29.4\n-\n-\nDeepSeek-VL-1.3B [48]\n2B\n87.6\n64.6\n34.8\n32.2\n1532\n-\n31.1\n66.7\n409\nPaliGemma-3B [4]\n2.9B\n87.0\n71.0\n33.1\n34.9\n1686\n-\n28.7\n69.6\n614\nMiniCPM-V2 [100]\n2.8B\n-\n69.1\n41.0\n38.2\n1809\n-\n38.7\n67.1\n605\nInternVL-1.5 [11]\n2B\n-\n70.9\n39.3\n34.6\n1902\n-\n41.1\n69.8\n654\nQwen2-VL [88]\n2B\n-\n74.9\n49.5\n41.1\n1872\n-\n43.0\n-\n809\nEncoder-free\nFuyu-8B (HD) [2]\n8B\n-\n10.7\n21.4\n-\n-\n-\n-\n-\n-\nEVE-7B [19]\n7B\n83.6\n49.5\n25.6\n32.3\n1483\n-\n25.2\n61.3\n327\nMono-InternVL [51]\n1.8B\n-\n65.5\n40.1\n33.7\n1875\n-\n45.7\n67.4\n767\nUnderstanding & Generation\nEncoder-based\nEmu [78]\n14B\n-\n-\n36.3\n-\n-\n-\n-\n-\n-\nEmu2 [76]\n37B\n-\n63.6\n48.5\n34.1\n-\n1345\n-\n62.8\n-\nSEED-X [24]\n17B\n84.2\n75.4\n-\n35.6\n-\n1436\n-\n-\n-\nLWM [45]\n7B\n75.2\n-\n9.6\n-\n-\n-\n-\n-\n-\nDreamLLM [20]\n7B\n-\n58.2\n36.6\n-\n-\n-\n-\n-\n-\nJanus [92]\n1.3B\n87.0\n69.4\n34.3\n30.5\n-\n1338\n-\n63.7\n-\nEncoder-free\nChameleon [8]\n7B\n-\n-\n8.3\n22.4\n-\n-\n-\n-\n-\nShow-o [96]\n1.3B\n84.5\n-\n-\n27.4\n-\n1233\n-\n-\n-\nVILA-U [94]\n7B\n85.8\n-\n33.5\n-\n-\n1402\n-\n59.0\n-\nEmu3-Chat [91]\n8B\n85.2\n58.5\n37.2\n31.6\n-\n-\n-\n68.2\n687\nSynerGen-VL (Ours)\n2.4B\n85.3\n53.7\n34.5\n34.2\n1837\n1381\n42.7\n62.0\n721\nTable 2. Results on general MLLM benchmarks. Our model with 2.4B parameters achieves competitive image understanding perfor-\nmance compared with significantly larger encoder-free unified MLLMs such as Emu3-Chat-8B [91].\nties into the pretrained LLM by training the image-specific\nparameters and the self-attention layers with high-quality\nmixed data. Specifically, for image understanding, we fol-\nlow [51] to sample from the high-quality pretraining data\nof InternVL-1.5 [11], resulting in 170 million samples with\ntask-related prompts. For image generation, we select the\ndata with high aesthetic scores and caption quality, result-\ning in 20 million samples from [3, 52, 69, 74, 85] as well as\n5 million high-quality internal data.\nThroughout both stages, SynerGen-VL is trained simul-\ntaneously for image understanding and generation. To en-\nhance the image understanding capability and take advan-\ntage of SynerGen-VL’s ability to process high-resolution\nimages, we implement a dynamic resolution strategy for un-\nderstanding tasks following InternVL-1.5 [11] in the second\nstage and set the maximum number of image tiles to 6.\nJoint Instruction Tuning. During the instruction tuning\nstage, we unfreeze all the model parameters. For image\nunderstanding, we adopt the dataset from InternVL-1.5,\nincluding around 5M bilingual instructions for supervised\nlearning, covering various tasks such as visual question\nanswering, multimodal dialogue, mathematics, knowledge,\netc. We also increase the maximum number of image tiles to\n12 to handle high-resolution images. For image generation,\nwe solely use the 10M internal dataset to further enhance\nthe image generation quality.\n4. Experiments\n4.1. Implementation Details\nSynerGen-VL is built upon InternLM2-1.8B [7], using the\nsame text tokenizer and conversation format. The discrete\nimage tokenizer originates from Emu3 [91], characterized\nby a codebook size of 32,768 and a spatial downsampling\nrate of 8. The input image is resized to 512 × 512. For im-\nage generation data, the short edge of the image is resized to\n512 and the long edge is cropped to 512. The total number\nof model parameters is 3.6B, of which the number of activa-\ntion parameters is 2.4B. In the pretraining phase, the global\nbatch size for image understanding and generation tasks is\n6988 for stage 1 and 5090 for stage 2, respectively. The loss\nweight hyperparameter λ is set to 2. The instruction tuning\nphase is trained for 3 epochs in total. Following previous\nworks [75, 92], classifier-free guidance (CFG) strategy is\nalso implemented for image generation. During training,\nwe randomly replace the original user caption prompt with\n“Here is a random image <UNCOND>:” with a probability\n6\nMethod\n#A-Param\nTextVQA\nSQA-I\nGQA\nDocVQA\nAI2D\nChartQA\nInfoVQA\nUnderstanding Only\nEncoder-based\nMobileVLM-V2 [14]\n1.7B\n52.1\n66.7\n59.3\n-\n-\n-\n-\nMini-Gemini-2B [39]\n3.5B\n56.2\n-\n-\n34.2\n-\n-\n-\nPaliGemma-3B [4]\n2.9B\n68.1\n-\n-\n-\n68.3\n-\nMiniCPM-V2 [100]\n2.8B\n74.1\n-\n-\n71.9\n-\n-\n-\nInternVL-1.5 [11]\n2B\n70.5\n84.9\n61.6\n85.0\n69.8\n74.8\n55.4\nEncoder-free\nEVE-7B [19]\n7B\n51.9\n63.0\n60.8\n-\n-\n-\n-\nMono-InternVL [51]\n1.8B\n72.6\n93.6\n59.5\n80.0\n68.6\n73.7\n43.0\nUnderstanding & Generation\nEncoder-based\nEmu2 [76]\n37B\n66.6\n-\n65.1\n-\n-\n-\n-\nLWM [45]\n7B\n18.8\n47.7\n44.8\n-\n-\n-\n-\nDreamLLM [20]\n7B\n41.8\n-\n-\n-\n-\n-\n-\nMM-Interleaved [82]\n13B\n61.0\n-\n60.5\n-\n-\n-\n-\nJanus [92]\n1.3B\n-\n-\n59.1\n-\n-\n-\n-\nEncoder-free\nChameleon⋄[8]\n7B\n4.8\n47.2\n-\n1.5\n46.0\n2.9\n5.0\nShow-o [96]\n1.3B\n-\n-\n61.0\n-\n-\n-\n-\nVILA-U [94]\n7B\n60.8\n-\n60.8\n-\n-\n-\n-\nEmu3-Chat [91]\n8B\n64.7\n89.2\n60.3\n76.3\n70.0\n68.6\n43.8\nSynerGen-VL (Ours)\n2.4B\n67.5\n92.6\n59.7\n76.6\n60.8\n73.4\n37.5\nTable 3. Comparison with existing MLLMs on visual question answering benchmarks. #A-Params denotes the number of activated\nparameters during inference. ⋄Some results of Chameleon are sourced from [51].\nof 10%, where <UNCOND> is a learnable special token em-\nbedding. During inference, the logit of each unfolded image\ntoken is calculated as: lg = lu + s(lc −lu), where lc, lu are\nthe conditional and unconditional logits, respectively. s is\nthe CFG-scale with default number of 7.5.\nDue to the limited space, please refer to the supplemen-\ntary material for more detailed training configurations.\n4.2. Image Understanding\nEvaluation Benchmarks.\nTo evaluate the general mul-\ntimodal understanding capabilities of SynerGen-VL, we\ncompare with image understanding models as well as uni-\nfied image understanding and generation models on 8 com-\nprehensive multimodal benchmarks including MMBench-\nEN test [46], MMVet [103], MMMU val [105], MME [23],\nMathVista test-mini [50], POPE [36], SEED-Image [34],\nand OCRBench [47].\nThese general benchmarks covers\nassessment of various capabilities for visual question an-\nswering, document and chart interpretation, and other com-\nplex visual scenarios. We further evaluate model’s VQA\nperformances on 7 widely-adopted benchmarks including\nTextVQA val [72], ScienceQA test [49], GQA test-dev [29],\nDocVQA test [15], AI2D test [31], ChartQA test [53], and\nInfographicsVQA test [54]. Part of the results are evaluated\nusing VLMEvalKit [21] or sourced from the OpenCompass\nleaderboard [16].\nResults. Evaluation results are shown in Tab. 2 and Tab. 3.\nCompared with existing encoder-free unified MLLMs, our\nSynerGen-VL with 2.4B parameters surpasses previous\nmethods (especially for encoder-free unified MLLMs) with\ncomparable parameter sizes while achieving comparable\nperformance to models with significantly larger parame-\nter sizes, showcasting its competitive image understanding\ncapability. Notably, on image understanding benchmarks\nrequiring high-resolution detailed image comprehension,\nsuch as OCRBench, TextVQA, DocVQA, and ChartQA,\nour SynerGen-VL achieves results superior to much larger\nencoder-free MLLMs such as Emu3-Chat-8B [91], high-\nlighting its advantages with high-resolution image pro-\ncessing capabilities. Moreover, as a encoder-free unified\nMLLM, SynerGen-VL also obtains image understanding\nperformance competitive to encoder-based understanding-\nonly MLLMs such as LLaVA-1.5 [42], while surpassing\nlarger encoder-free task-specific MLLMs such as EVE-\n7B [19] and Fuyu-8B (HD) [2], demonstrating its great po-\ntential of unifying image understanding and generation.\n4.3. Image Generation\nEvaluation Benchmarks. We use the MSCOCO-30K [40],\nMJHQ-30K [35], and GenEval [25] benchmarks to evaluate\nour model’s image generation capabilities. For MSCOCO-\n30K and MJHQ-30K, we generate 30k images and compare\n7\nMethod\n# A-Param\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall↑\nGeneration Only\nLlamaGen [75]\n0.8B\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nLDM [65]\n1.4B\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 [65]\n0.9B\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nSDXL [61]\n2.6B\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nPixArt-α [9]\n0.6B\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nDALL-E 2 [64]\n6.5B\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nUnderstanding & Generation\nSEED-X† [24]\n17B\n0.97\n0.58\n0.26\n0.80\n0.19\n0.14\n0.49\nShow-o [96]\n1.3B\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n0.53\nLWM [45]\n7B\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nChameleon [8]\n34B\n-\n-\n-\n-\n-\n-\n0.39\nEmu3-Gen [91]\n8B\n0.98\n0.71\n0.34\n0.81\n0.17\n0.21\n0.54\nJanus [92]\n1.3B\n0.97\n0.68\n0.30\n0.84\n0.46\n0.42\n0.61\nSynerGen-VL (Ours)\n2.4B\n0.99\n0.71\n0.34\n0.87\n0.37\n0.37\n0.61\nTable 4. Evaluation of text-to-image generation on GenEval [25] benchmark. #A-Params denotes the number of activated parameters\nduring inference. † indicates models with external pretrained diffusion model. Obj.: Object. Attri.: Attribution.\nModel\n#A-Param MS-COCO↓MJHQ↓\nGeneration Only\nDALL-E [63]\n12B\n27.50\n-\nLDM [65]\n1.4B\n12.64\n-\nGLIDE [58]\n5B\n12.24\n-\nDALL-E 2 [64]\n6.5B\n10.39\n-\nRAPHAEL [97]\n3B\n6.61\n-\nImagen [66]\n34B\n7.27\n-\nSDv1.5 [65]\n0.9B\n9.62\n-\nSDXL [61]\n0.9B\n7.38\n8.76\nPixArt-α [9]\n0.6B\n7.32\n6.14\nUnderstanding & Generation\nNExT-GPT [93]\n13B\n11.18\n-\nSEED-X [24]\n17B\n14.99\n-\nShow-o [96]\n1.3B\n9.24\n15.18\nLWM [45]\n7B\n12.68\n17.77\nVILA-U [94]\n7B\n-\n7.69\nEmu3-Gen [91]\n8B\n19.3\n-\nJanus [92]\n1.3B\n8.53\n10.10\nSynerGen-VL (Ours)\n2.4B\n7.65\n6.10\nTable 5. Image generation results on MSCOCO-30K [40] and\nMJHQ-30K [35] datasets. FID [27] is reported. #A-Param de-\nnotes the number of activated parameters during inference.\nthem with the reference images and use Fr´echet Inception\nDistance (FID) [27] to assess the overall generation quality.\nFor GenEval, we generate four images for each prompt and\nutilize its official framework to assess our model’s object-\nlevel image-text alignment.\nResults on MSCOCO and MJHQ. Tab. 5 shows the zero-\nshot FID of our model on MSCOCO 30K [40]. Compared\nwith previous generation-only models such as GLIDE [58]\nand DALL-E 2[64], our method can achieve better FID\nscores. Compared with previous unified MLLMs for both\nimage understanding and generation, SynerGen-VL can\nachieve competitive performance without using an external\ndiffusion model. In particular, compared with Emu3 [91]\nthat use the same tokenizer, our method has a significant\nimprovement in FID scores with less model parameters. We\nbelieve this is because the usage of vision experts simplifies\nthe training difficulty. We also evaluate our model’s ability\nto generate high-quality aesthetic images on MJHQ [35], as\nshown in the Tab. 5. Compared with previous generation-\nonly methods, SynerGen-VL achieves competitive genera-\ntion performance. These results validate that our method\napplies to both natural images and synthetic aesthetic im-\nages.\nResults on GenEval. Following previous studies, we eval-\nuate our model’s text-to-image generation capabilities on\nthe GenEval benchmark [25] from six dimensions: “single\nobject”, “two objects”, “number”, “color”, “position”, and\n“color attribution”. Our model achieve competitive over-\nall scores with previous generation-only models of similar\nsizes. SynerGen-VL performs comparably to Janus [92],\nwhich uses independent encoders for perception and gen-\neration, demonstrating the effectiveness of using vision ex-\nperts in our approach. Compared to Emu3 [91], our model\nachieves better overall performance with fewer parameters.\n5. Ablation Study\nIn this section, we ablate the effectiveness of the two im-\nportant techniques of SynerGen-VL, i.e., token folding and\nprogressive alignment pre-training with MMoEs. In this ab-\nlation study, we use Qwen2-0.5B-Instruct [98] as the initial-\nized LLM and image size 256 unless otherwise specified.\n8\n5.1. Effectiveness of Token Folding\nTo verify the effectiveness of token folding on high-\nresolution image understanding, we compare SynerGen-VL\nwith the baseline version without token folding and the dy-\nnamic resolution strategy on image understanding tasks.\nSpecifically, the baseline model directly use the tokenized\nsequence as the input image sequence without token fold-\ning, where the input image size is 256 × 256 and the tok-\nenized sequence length is 1024. Meanwhile, for the model\nwith token folding, we follow InternVL-1.5 [11] to im-\nplement the dynamic resolution strategy to provide high-\nresolution input images. For fair comparison, we use a to-\nken folding ratio of 2×4 and control the maximum number\nof dynamic image patches so that the average length of im-\nage token sequence after token folding is also 1024.\nWe train the models with a subset of stage 2 (S.2) under-\nstanding data, and evaluate the pre-trained models on VQA\nbenchmarks. Results are shown in Tab. 6. On datasets re-\nquiring precise understanding of detailed image informa-\ntion such as TextVQA, DocVQA, ChartVQA, and Info-\ngraphicVQA, the model with token folding achieves signif-\nicantly better results, demonstrating its advantages of high-\nresolution image understanding.\nModel\nTextVQA GQA DocVQA AI2D ChartQA InfoVQA\nw\/o token folding\n18.7\n45.3\n14.7\n42.0\n20.9\n18.7\nw\/ token folding\n35.0\n45.1\n36.7\n42.1\n49.7\n21.1\nTable 6. Comparison between models with and without token-\nfolding on VQA benchmarks.\nThe model with token fold-\ning demonstrates significant performance improvements with the\nsame image token sequence length.\n5.2. Effectiveness of the Progressive Alignment Pre-\ntraining with MMoEs\nWe ablate our proposed visual alignment pre-training strat-\negy on various benchmarks, including visual question an-\nswering (VQA), natural language processing (NLP) and\ntext-to-image (T2I) generation, as shown in Tab. 7. To en-\nsure fair comparison, neither token folding nor dynamic\nresolution strategies are employed. For experimental effi-\nciency, only 1\/6 of the training data is used for both stages.\nThe results show that our progressive strategy matches\nor exceeds the fully parameter-trained strategy on VQA\nbenchmarks and significantly outperforms it on text-to-\nimage generation benchmarks.\nMeanwhile, on NLP\nbenchmarks, our model with progressive alignment pre-\ntraining delivers results much closer to the pre-trained LLM\n(Qwen2-0.5B-Instruct) compared with the fully parameter-\ntrained model. This validates that our approach effectively\npreserves the original knowledge in the pre-trained LLM\nwhile learning robust visual representations. Furthermore,\nthe two-stage training strategy outperforms training solely\nwith stage 1 or stage 2, particularly on VQA and text-to-\nimage generation benchmarks. This underscores the im-\nportance of learning basic visual concepts and pixel de-\npendencies from large-scale noisy data, as well as enhanc-\ning image-text alignment and image aesthetics with high-\nquality data.\n5.3. Analysis of Relationship Between Image Gen-\neration and Understanding\nWe provide visualization and analysis to understand the\nrelationship between image generation and understanding\ntasks, i.e. how the two tasks might be related in terms of\ntheir processing or feature utilization.\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Similarity\nSimilarity between Gen and Und features\nFigure 4. Cosine similarity of visual features between genera-\ntion and understanding tasks across different layers. The rep-\nresentations of the image understanding and generation tasks are\nsimilar in shallow layers but disentagle in deeper layers.\nImage Feature Similarity. We first analyze whether the\ntwo tasks share similar representations. We use the same\ninput image paired with text instructions of generation or\nunderstanding, and compute the cosine similarity between\nvisual features of the two tasks at each layer. As shown in\nFig. 4, the two features are nearly identical (0.999) at shal-\nlower layers, but the similarity decreases as layers deepen.\nIt finally reaches a near-zero value (0.035) at the last layer,\nsuggesting that the two representations are disentangled.\nThis observation implies that while image generation and\nunderstanding may share foundational visual representa-\ntions in the early stages, they develop task-specific repre-\nsentations based on different instructions of image genera-\ntion and understanding at deeper layers.\nAttention Map Visualization. In Fig. 5, we further inves-\ntigate whether the two tasks have similar attention map pat-\nterns. We discover that in both tasks, locality is present at\nearly layers, where visual tokens only attend to its nearby\ntokens (i.e. near the diagonal). Text tokens and images have\nfew interactions with each other. As layers deepen, longer\ndependency is observed, and finally global interactions are\nachieved at the last layer. Text and image also interact more\noften than at shallower layers. The attention weight also dis-\n9\nStage\nStrategy\nVQA Benchmarks ↑\nNLP Benchmarks ↑\nT2I Benchmark ↓\nTextVQA GQA DocVQA AI2D ChartQA InfoVQA MMLU CMMLU AGIEVAL MATH\nMSCOCO\nBaseline (Qwen2-0.5B)\n-\n-\n-\n-\n-\n-\n42.3\n51.4\n29.3\n12.1\n-\nS.1 + S.2 Full\n14.3\n42.9\n11.3\n24.7\n12.4\n12.6\n23.1\n23.0\n8.1\n0.9\n30.7\nS.1 only Progressive\n0.1\n13.0\n0.2\n0.3\n0.0\n0.0\n42.3\n51.4\n29.3\n12.1\n28.3\nS.2 only Progressive\n8.7\n36.9\n8.6\n40.9\n11.7\n16.2\n37.6\n45.3\n28.9\n7.2\n34.9\nS.1 + S.2 Progressive\n13.2\n41.2\n11.4\n41.9\n12.8\n17.0\n39.3\n48.2\n26.2\n8.9\n20.2\nTable 7. Zero-shot performance of different pre-training strategies. “S.1” and “S.2” denote the first and second pre-training stage.\n“Full” and “Progressive” denote the full parameter tuning and our progressive tuning strategy with MMoEs, respectively. FID [27] is\nreported for text-to-image generation (T2I) on MSCOCO [40].\nplays a periodicity nature, such as in Layer 4. Visualization\nin the input image suggests that the period is the number of\ntokens in each row, validating the locality. When comparing\nthe attention maps in the two tasks, we observe that local-\nity is more obvious in generation than in understanding at\nthe same layer. This can be explained that local details are\nrequired to generate a spatially consistent and semantically\ncoherent image, while understanding the whole image re-\nquires global context.\n6. Conclusion\nIn this paper, we introduce SynerGen-VL, an encoder-free\nMLLM that effectively unifies image understanding and\ngeneration within a simplified framework.\nBy leverag-\ning token folding and vision experts, SynerGen-VL ad-\ndresses the complexities of high-resolution image process-\ning while maintaining the integrity of pretrained language\nmodel knowledge. Our approach eliminates dependencies\non external diffusion models or additional semantic encoder\npretraining, achieving competitive performance across vari-\nous benchmarks with a relatively small parameter size. The\nexperiment results underscore SynerGen-VL’s potential as\na scalable and efficient solution for future unified MLLMs.\nAcknowledgments\nThis work is supported by the Na-\ntional Key R&D Program of China (NO. 2022ZD0161300),\nby the National Natural Science Foundation of China\n(62376134).\n10\nUnderstanding\nGeneration\nLayer 4\nLayer 12\nLayer 20\nLayer 24\n66\n2\n256\n46\n29\n256\nText: \n<system> ...\n<user> Generate an image of  “A \ncrowd of people get ready to board \na bus in the city”.\n<assistant> <BOI><IMG><EOI>\nText: \n<system> ...\n<user> <BOI><IMG><EOI> \nProvide a one-sentence caption for \nthe image:\n<assistant> A crowd of people get \nready to board a bus in the city.\nFigure 5. Attention map visualization of understanding and generation tasks. In the second and fourth rows, we visualize a query\ntoken (red) and its attended tokens (blue) in the input image. Each token corresponds to a horizontal rectangular area in the original image\ndue to the 2 × 4 token folding. Darker blue indicates larger attention weights.\n11\nReferences\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-\naodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\nBinyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Day-\niheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin\nMa, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi\nTan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei\nWang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,\nHao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen\nYu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan\nZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-\ngren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen tech-\nnical report. arXiv preprint arXiv:2309.16609, 2023. 3\n[2] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa˘gnak Tas¸ırlar.\nIntroducing our multimodal models, 2023. 6, 7\n[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey\nChu, Yunxin Jiao, and Aditya Ramesh. Improving image\ngeneration with better captions. 2023. 5, 6\n[4] Lucas Beyer,\nAndreas Steiner,\nAndr´e Susano Pinto,\nAlexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim\nNeumann, Ibrahim Alabdulmohsin, Michael Tschannen,\nEmanuele Bugliarello, et al. Paligemma: A versatile 3b\nvlm for transfer. arXiv preprint arXiv:2407.07726, 2024.\n6, 7\n[5] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\nICCV, pages 4291–4301, 2019. 5\n[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun\nLee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m:\nImage-text pair dataset.\nhttps:\/\/github.com\/\nkakaobrain\/coyo-dataset, 2022. 5\n[7] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, et al.\nInternlm2 technical report.\narXiv preprint\narXiv:2403.17297, 2024. 1, 3, 6\n[8] ChameleonTeam. Chameleon: Mixed-modal early-fusion\nfoundation models.\narXiv preprint arXiv:2405.09818,\n2024. 1, 2, 3, 6, 7, 8\n[9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,\nEnze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. Pixart-α: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023. 5, 8\n[10] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. A\nsingle transformer for scalable vision-language modeling.\narXiv preprint arXiv:2407.06438, 2024. 3\n[11] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\nwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, et al. How far are we to gpt-4v? clos-\ning the gap to commercial multimodal models with open-\nsource suites. arXiv:2404.16821, 2024. 1, 5, 6, 7, 9\n[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, 2023. 3\n[13] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\nNg, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\nZhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\ning challenge on arbitrary-shaped text-rrc-art. In ICDAR,\npages 1571–1576, 2019. 5\n[14] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang\nXu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu,\nXinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and\nstronger baseline for vision language model. arXiv preprint\narXiv:2402.03766, 2024. 7\n[15] Christopher Clark and Matt Gardner.\nSimple and effec-\ntive multi-paragraph reading comprehension. In ACL, pages\n845–855, 2018. 5, 7\n[16] Contributors. Opencompass: A universal evaluation plat-\nform for foundation models. https:\/\/github.com\/\nopen-compass\/opencompass, 2023. 7\n[17] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\nFunnel-transformer: Filtering out sequential redundancy\nfor efficient language processing. Advances in neural in-\nformation processing systems, 33:4271–4282, 2020. 3\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In CVPR, pages 248–255, 2009. 5\n[19] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang,\nHuchuan Lu, and Xinlong Wang. Unveiling encoder-free\nvision-language models. arXiv preprint arXiv:2406.11832,\n2024. 3, 6, 7\n[20] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. In ICLR, 2024. 1, 6, 7\n[21] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang,\nLin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan\nZhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit:\nAn open-source toolkit for evaluating large multi-modality\nmodels, 2024. 7\n[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Pro-\nceedings of the IEEE\/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 1, 2, 3\n[23] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui\nYang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji.\nMME: A comprehensive evaluation benchmark for multi-\nmodal large language models. arXiv: 2306.13394, 2023.\n7\n[24] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi,\nLin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x:\nMultimodal models with unified multi-granularity compre-\nhension and generation. arXiv preprint arXiv:2404.14396,\n2024. 1, 3, 6, 8\n[25] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment. Advances in Neural Information Pro-\ncessing Systems, 36, 2024. 7, 8\n12\n[26] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu\nMinzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei\nZhang, Xin Jiang, et al. Wukong: A 100 million large-scale\nchinese cross-modal pre-training benchmark. NeurIPS, 35:\n26418–26431, 2022. 5\n[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 8, 10\n[28] Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo,\nYireun Kim, Tal Schuster, Adam Fisch, James Thorne,\nand Se-Young Yun.\nBlock transformer: Global-to-local\nlanguage modeling for fast inference.\narXiv preprint\narXiv:2406.02657, 2024. 3\n[29] Drew A. Hudson and Christopher D. Manning. GQA: A\nnew dataset for real-world visual reasoning and composi-\ntional question answering.\nIn CVPR, pages 6700–6709,\n2019. 7\n[30] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jian-\nchao Tan, Yadong Mu, et al. Unified language-vision pre-\ntraining in llm with dynamic discrete visual tokenization.\nIn International Conference on Learning Representations,\n2024. 3\n[31] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In ECCV, pages 235–251, 2016. 7\n[32] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon\nNam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-\nfree document understanding transformer. In ECCV, 2022.\n5\n[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chlo´e Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar,\nand Ross B. Girshick.\nSegment anything.\narXiv:\n2304.02643, 2023. 5\n[34] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan.\nSeed-bench:\nBenchmarking\nmultimodal llms with generative comprehension.\narXiv:\n2307.16125, 2023. 7\n[35] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Lin-\nmiao Xu, and Suhail Doshi. Playground v2. 5: Three in-\nsights towards enhancing aesthetic quality in text-to-image\ngeneration. arXiv preprint arXiv:2402.17245, 2024. 7, 8\n[36] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In EMNLP, pages 292–305,\n2023. 7\n[37] Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang,\nWanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. Uni-\nmoe: Scaling unified multimodal llms with mixture of ex-\nperts. arXiv preprint arXiv:2405.11273, 2024. 2\n[38] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-gemini: Mining the potential of multi-modality\nvision language models. arXiv: 2403.18814, 2024. 3, 6\n[39] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-gemini: Mining the potential of multi-modality\nvision language models. arXiv preprint arXiv:2403.18814,\n2024. 7\n[40] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and\nC. Lawrence Zitnick. Microsoft COCO: common objects\nin context. In ECCV, pages 740–755, 2014. 5, 7, 8, 10\n[41] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu.\nMmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning.\narXiv preprint\narXiv:2311.10774, 2023. 5\n[42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv:\n2310.03744, 2023. 7\n[43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744, 2023. 6\n[44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. In NeurIPS, 2023. 1\n[45] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 1,\n3, 6, 7, 8\n[46] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player?\narXiv: 2307.06281, 2023. 7\n[47] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu,\nMingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen,\nChunyuan Li, Lianwen Jin, et al.\nOn the hidden mys-\ntery of ocr in large multimodal models.\narXiv preprint\narXiv:2305.07895, 2023. 7\n[48] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-\noshu Li, Yaofeng Sun, et al.\nDeepseek-vl:\nTowards\nreal-world vision-language understanding. arXiv preprint\narXiv:2403.05525, 2024. 6\n[49] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan.\nLearn to explain: Multimodal reason-\ning via thought chains for science question answering. In\nNeurIPS, 2022. 7\n[50] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. arXiv: 2310.02255, 2023. 7\n[51] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng\nDai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing\nthe boundaries of monolithic multimodal large language\nmodels with endogenous visual pre-training. arXiv preprint\narXiv:2410.08202, 2024. 2, 3, 6, 7\n13\n[52] madebyollin.\nMegalith-huggingface.\nhttps :\n\/\/huggingface.co\/datasets\/madebyollin\/\nmegalith-10m, 2024. 5, 6\n[53] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning.\nIn ACL, pages 2263–2279, 2022. 5, 7\n[54] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa.\nIn WACV, pages 1697–1706, 2022. 5, 7\n[55] Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and\nPratyush Kumar. Plotqa: Reasoning over scientific plots.\nIn WACV, pages 1527–1536, 2020. 5\n[56] Asier Mujika.\nHierarchical attention encoder decoder.\narXiv preprint arXiv:2306.01070, 2023. 3\n[57] Vishvak Murahari, Carlos Jimenez, Runzhe Yang, and\nKarthik Narasimhan. Datamux: Data multiplexing for neu-\nral networks. Advances in Neural Information Processing\nSystems, 35:17515–17527, 2022. 3\n[58] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. arXiv\npreprint arXiv:2112.10741, 2021. 8\n[59] OpenAI. GPT-4 technical report. arXiv: 2303.08774, 2023.\n1\n[60] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 5\n[61] Dustin Podell,\nZion English,\nKyle Lacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 8\n[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In ICML, pages\n8748–8763, 2021. 3\n[63] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Interna-\ntional conference on machine learning, pages 8821–8831.\nPmlr, 2021. 8\n[64] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional\nimage generation with clip latents.\narXiv preprint\narXiv:2204.06125, 1(2):3, 2022. 8\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. 2022 ieee. In CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 8\n[66] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\nmans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in neural in-\nformation processing systems, 35:36479–36494, 2022. 8\n[67] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b: An open large-scale dataset for\ntraining next generation image-text models. NeurIPS, 35:\n25278–25294, 2022. 5\n[68] Christoph Schuhmann, Andreas K¨opf, Richard Vencu,\nTheo\nCoombes,\nand\nRomain\nBeaumont.\nLaion\ncoco:\n600m\nsynthetic\ncaptions\nfrom\nlaion2b-en.\nhttps:\/\/laion.ai\/blog\/laion-coco\/, 2022. 5\n[69] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, pages 8430–8439, 2019. 5, 6\n[70] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\nPei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\nBai. Icdar2017 competition on reading chinese text in the\nwild (rctw-17). In ICDAR, pages 1429–1434, 2017. 5\n[71] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: A dataset for image caption-\ning with reading comprehension. In ECCV, pages 742–758,\n2020. 5\n[72] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards VQA models that can read. In CVPR,\n2019. 7\n[73] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang,\nWojciech Galuba, and Tal Hassner. Textocr: Towards large-\nscale end-to-end reasoning for arbitrary-shaped scene text.\nIn CVPR, pages 8802–8812, 2021. 5\n[74] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong\nDuan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng\nQin, Yi Wang, et al. Journeydb: A benchmark for genera-\ntive image understanding. Advances in Neural Information\nProcessing Systems, 36, 2024. 5, 6\n[75] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 1, 6, 8\n[76] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiy-\ning Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang.\nGener-\native multimodal models are in-context learners.\narXiv:\n2312.13286, 2023. 3, 6, 7\n[77] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in mul-\ntimodality. arXiv: 2307.05222, 2023. 3\n[78] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in mul-\ntimodality. In ICLR, 2024. 6\n[79] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\nCanjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo\n14\nLiu, Dimosthenis Karatzas, et al. Icdar 2019 competition\non large-scale street view text with partial labeling-rrc-lsvt.\nIn ICDAR, pages 1557–1562, 2019. 5\n[80] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Jo-\nhan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gem-\nini: a family of highly capable multimodal models. arXiv:\n2312.11805, 2023. 1\n[81] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang,\nZhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong\nLu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text\ngenerative modeling via multi-modal feature synchronizer.\narXiv:2401.10208, 2024. 1\n[82] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang,\nZhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong\nLu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text\ngenerative modeling via multi-modal feature synchronizer.\narXiv preprint arXiv:2401.10208, 2024. 7\n[83] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: Scalable im-\nage generation via next-scale prediction.\narXiv preprint\narXiv:2404.02905, 2024. 1\n[84] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 1\n[85] Unsplash.\nUnsplash Dataset.\nhttps:\/\/unsplash.\ncom\/data, 2020. Online; accessed March-2020. 5, 6\n[86] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS,\npages 5998–6008, 2017. 4\n[87] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas,\nand Serge Belongie.\nCoco-text: Dataset and benchmark\nfor text detection and recognition in natural images. arXiv\npreprint arXiv:1601.07140, 2016. 5\n[88] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao\nFan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,\nWenbin Ge, et al. Qwen2-vl: Enhancing vision-language\nmodel’s perception of the world at any resolution. arXiv\npreprint arXiv:2409.12191, 2024. 4, 6\n[89] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language\nmodels. arXiv preprint arXiv:2311.03079, 2023. 2\n[90] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhen-\nhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\nZhiguo Cao, et al. The all-seeing project: Towards panoptic\nvisual recognition and understanding of the open world. In\nICLR, 2024. 5\n[91] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin\nMin, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liang-\ndong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu,\nYonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3:\nNext-token prediction is all you need. arXiv: 2409.18869,\n2024. 1, 2, 3, 4, 6, 7, 8\n[92] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling visual encoding\nfor unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024. 1, 3, 5, 6, 7, 8\n[93] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-\nSeng Chua. Next-gpt: Any-to-any multimodal llm. arXiv:\n2309.05519, 2023. 3, 8\n[94] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang,\nDacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model inte-\ngrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024. 1, 3, 6, 7, 8\n[95] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan,\nXingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang,\nand Zheng Liu. Omnigen: Unified image generation. arXiv\npreprint arXiv:2409.11340, 2024. 1\n[96] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao\nZhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu,\nZhijie Chen, Zhenheng Yang, and Mike Zheng Shou.\nShow-o: One single transformer to unify multimodal under-\nstanding and generation. arXiv preprint arXiv:2408.12528,\n2024. 1, 3, 6, 7, 8\n[97] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu,\nZhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-\nimage generation via large mixture of diffusion paths. Ad-\nvances in Neural Information Processing Systems, 2024. 8\n[98] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen\nYu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng\nLiu, Fei Huang, et al.\nQwen2 technical report.\narXiv\npreprint arXiv:2407.10671, 2024. 8\n[99] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\nThe\ndawn of lmms:\nPreliminary explorations with gpt-4v\n(ision). arXiv: 2309.17421, 9, 2023. 1\n[100] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhi-\nhui He, et al. Minicpm-v: A gpt-4v level mllm on your\nphone. arXiv preprint arXiv:2408.01800, 2024. 6, 7\n[101] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, et al. Scaling autore-\ngressive multi-modal models: Pretraining and instruction\ntuning. arXiv preprint arXiv:2309.02591, 2(3), 2023. 3\n[102] Lili Yu, D´aniel Simig, Colin Flaherty, Armen Aghajanyan,\nLuke Zettlemoyer, and Mike Lewis. Megabyte: Predict-\ning million-byte sequences with multiscale transformers.\nAdvances in Neural Information Processing Systems, 36:\n78808–78823, 2023. 3\n[103] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. arXiv: 2308.02490, 2023. 7\n[104] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in the\n15\nwild. Journal of Computer Science and Technology, 34:\n509–521, 2019. 5\n[105] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning bench-\nmark for expert agi. arXiv: 2311.16502, 2023. 7\n[106] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\nLi, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\nMingkun Yang, et al. Icdar 2019 robust reading challenge\non reading chinese text on signboard.\nIn ICDAR, pages\n1577–1581, 2019. 5\n[107] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024. 1, 3\n[108] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie\nZhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan.\nVl-gpt: A generative pre-trained transformer for vision and\nlanguage understanding and generation.\narXiv preprint\narXiv:2312.09251, 2023. 1\n16\nA. Detailed Training Configurations\nMore detailed hyper-parameters used in the training stages are listed in Tab. 8.\nConfiguration\nAlignment Pre-training\nInstruction\nS.1\nS.2\nTuning\nMaximum number of image tiles\n1\n6\n12\nLLM sequence length\n4, 096\n8, 192\n16, 384\nUse thumbnail\n✗\n✓\n✓\nGlobal batch size (per-task)\n6, 988\n5, 090\n1, 760\nPeak learning rate\n1e−4\n5e−5\n5e−5\nLearning rate schedule\nconstant with warm-up\ncosine decay\ncosine decay\nWeight decay\n0.05\n0.05\n0.01\nTraining steps\n95k\n35k\n12k\nWarm-up steps\n200\nOptimizer\nAdamW\nOptimizer hyperparameters\nβ1 = 0.9, β2 = 0.95, eps = 1e−8\nGradient accumulation\n1\nNumerical precision\nbfloat16\nTable 8. Hyper-parameters used in the alignment pre-training and instruction tuning stages.\nB. Visualization\nFor qualitative evaluation, we visualize examples for image understanding and image generation as follows.\n17\nAn empty road with trees in the background.\nA butterfly sitting on top of some yellow flowers.\nA sprawling urban landscape with numerous \nskyscrapers, highlighting the dense \narchitecture of the city. Tall buildings \ndominate the skyline, surrounded by smaller \nstructures and patches of greenery.\nA grassy field with mountains in the \nbackground.\nA small bird sitting on a branch of a tree.\nA close up of some pink flowers on a branch.\nThe sun is setting over the sea and mountains.\nA row of potted plants lined up in front of a \nstone wall.\nBlack woman, li-core, greebles, watercolor, \nfuturistic, mechanical, fire, white background.\nA stunning river meandering through a valley, \nframed by a majestic mountain range, \ncombining vibrant yellows and oranges with \nprecisionist lines, blending villagecore charm \nand east-west artistic fusion, creating a \nhyper-realistic yet dreamlike naturecore \naesthetic.\nA 3D-style cute girl rendered in a Disney-\nlike cartoon style, high detail and quality, \nwith a clean background.\nRealistic photograph, portrait of a beautiful \nrobin, sunset in the background, cinematic \nlighting.\nAn impressionist manga art style, blending \ninfluences from Paul Hedler and Makoto \nShinkai. It features vibrant, warm colors and \ndynamic brushstrokes, capturing a lively \nurban scene with a focus on lighting and \natmosphere.\nIllustration paintings roaring tiger of \nsuccessful unique wall paintings.\nImpressionist painting, Beijing, vibrant \ncolors, brushstrokes, bustling streets, \ntraditional architecture, hazy atmosphere, \nsoft light, Claude Monet.\nA beautiful woman dressed in a colorful \nfloral top, in the style of victor enrich, \npatchwork patterns, daria endresen, bold \ncolor choices, asymmetric designs, sandro \nbotticelli, 32k uhd.\nFigure 6. Qualitative results of image generation. The images are of size 512 × 512.\n18\nImage Captioning\nPrompt: Describe this image in detail.\nResponse: The image depicts a panoramic view of a bustling urban cityscape, likely a major metropolitan area. The foreground\nis dominated by numerous high-rise buildings, showcasing a dense concentration of skyscrapers. These buildings vary in height\nand architectural style, with some featuring modern glass facades and others displaying more traditional designs. The city\nappears to be a hub of economic activity, given the density and variety of the structures.\nIn the midground, the city extends into a large body of water, likely a harbor or bay, with numerous boats and ships. This\nwaterway is likely a significant transportation route, facilitating the movement of goods and people. The presence of numerous\nboats suggests a thriving maritime industry.\nThe background features a vast expanse of water, likely a bay or a coastal area, with more high-rise buildings and urban\ndevelopment extending into the distance. The horizon is lined with a range of mountains or large hills, adding a natural backdrop\nto the otherwise man-made environment. The sky above is mostly clear with a few scattered clouds, indicating good weather\nconditions.\nThe overall scene is one of economic prosperity and urban development, with the combination of modern skyscrapers and the\nexpansive waterway suggesting a city that is both economically vibrant and strategically located for maritime trade. The presence\nof numerous boats and the extensive waterway indicate a well-developed infrastructure, likely supporting a significant population\nand economy.\nPrompt: Where is this most likely to be?\nResponse: This image is most likely of Hong Kong. The presence of the distinctive twin-towered skyscrapers, the hilly terrain,\nthe bay with numerous boats, and the urban skyline with numerous high-rise buildings are characteristic features of Hong Kong’s\nskyline.\nPrompt: Tell me about this logo.\nResponse: This is a logo for the CVPR conference, standing for the\nConference on Computer Vision and Pattern Recognition, scheduled from\nJune 17 to June 21, 2024, in Seattle, WA. The logo features a stylized blue\ncityscape of Seattle, prominently displaying notable landmarks such as\nthe Space Needle. The text “CVPR” is displayed in large, bold letters to\nthe right of the cityscape, with the dates and location information below\nit. The design is simple yet recognizable, effectively communicating the\nevent’s location and timing.\n19\nPrompt: What is the content in this image?\nResponse: The image appears to be a screenshot from a video game.\nThe scene is set in a dimly lit, stone-walled environment that resembles\na castle or fortress. The walls are made of large stone blocks, and there\nis a staircase leading up to a doorway on the right side of the image. The\ndoorway is open, revealing a dark interior.\nIn the foreground, there is a character with brown hair, wearing a red and\nwhite outfit. The character is facing away from the camera, looking up\ntowards the stairs.\nIn the background, another character is standing on the stairs, facing the\ncamera. This character has a white cap, a red and white outfit, and is\ncarrying a backpack. The character is looking down the stairs towards the\nopen doorway.\nAt the bottom of the image, there is a dialogue box with Chinese text. The\ntext reads: “就一起回未白镇去吧。”.\nOCR\nPrompt: Describe this image from @albumreceipt.\nResponse: This image appears to be a tracklist for a music release titled\n“Curtain Call: The Hits” by EMINEM, in a receipt-like style. The tracklist\nis as follows:\n1. Intro - 0:33\n2. Fack - 3:25\n3. The Way I Am - 4:21\n4. My Name Is - 4:28\n5. Stan - 6:44\n6. Love Yourself - 5:26\n7. Shake That - 4:34\n8. Sing For The Moment - 5:40\n9. Without Me - 4:51\n10. Like Toy Soldiers - 4:55\n11. The Real Slim Shady - 4:44\n12. Mockingbird - 4:11\n13. Guilty Conscience - 3:20\n14. Cleanin’ Out My Closet - 4:58\n15. Just Lose It - 4:08\n16. When I’m Gone - 4:41\n17. Stan (live, bonus track) - 6:20\nThe total tracklist includes 17 songs. The total length of the songs in this\ntracklist is 73:59. The release date is December 6, 2005. The label is\n“Aftermath Entertainment”.\n20\nDocument Understanding\nPrompt: Which platform will the train depart from?\nResponse: The train will depart from King’s Cross Station in London,\nspecifically Platform 9 3\/4.\nPrompt: Where should Mr. Potter buy the items needed for school?\nResponse: Based on the image, Mr. Potter should buy the items needed\nfor school from the Diagon Alley, which is mentioned as a requirement for\nthe school trip.\nHumor Understanding\nPrompt: What is the punchline?\nResponse: The punchline to the joke in the image is the unexpected\nand humorous twist on the common phrase “finding the square root of a\nnumber.” Instead of finding the square root, the image humorously shows\nsomeone finding a tree root in the square shape.\nThe root structure is\nexaggerated to the point where it appears to be an infinite loop, adding a\nlayer of absurdity and humor to the joke.\n21\nImage to Latex\nPrompt: Convert the formula in the figure to latex format.\nResponse: Certainly, I’ll convert the formula in the figure to LaTeX format for you.\n\\begin{equation}\n\\mathcal{L} = - \\sum_{i \\in \\mathcal{T}} \\log p(\\hat{x}ˆi_T = xˆi_T | xˆ{<i})\n- \\lambda \\sum_{i \\in \\mathcal{V}} \\log p(\\hat{x}ˆi_V = xˆi_V | xˆ{<i}),\n\\end{equation}\nMath\nPrompt: What is the perimeter of the rectangle?\nResponse: To calculate the perimeter of the rectangle, we need to add up the lengths of\nall four sides. In this case, the rectangle has two sides that are each 6 feet long, and two\nsides that are each 3 feet long.\nThe perimeter of a rectangle is calculated by adding the lengths of all four sides. So, the\nformula is:\nPerimeter = 2(length + width)\nFor this rectangle, the length is 6 feet and the width is 3 feet.\nPlugging these values into the formula, we get:\nPerimeter = 2(6 + 3) = 2(9) = 18\nTherefore, the perimeter of the rectangle is 18 feet.\nPrompt: Lena has $0.13. How much money will Lena have left if she buys a shapes\nstencil and a green bouncy ball? (Unit: $)\nResponse: To find out how much money Lena will have left after buying a shapes\nstencil and a green bouncy ball, we need to subtract the total cost of these items from\nthe amount she has.\nThe total cost of the shapes stencil and the green bouncy ball is $0.08 + $0.04 = $0.12.\nTo find out how much money Lena will have left, we subtract the total cost from the\namount she has:\n$0.13 - $0.12 = $0.01\nTherefore, Lena will have $0.01 left after buying a shapes stencil and a green bouncy\nball.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding.pdf"}
{"title":"Diffusion Language Models Are Versatile Protein Learners","authors":"Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu","summary":"This paper introduces diffusion protein language model (DPLM), a versatile\nprotein language model that demonstrates strong generative and predictive\ncapabilities for protein sequences. We first pre-train scalable DPLMs from\nevolutionary-scale protein sequences within a generative self-supervised\ndiscrete diffusion probabilistic framework, which generalizes language modeling\nfor proteins in a principled way. After pre-training, DPLM exhibits the ability\nto generate structurally plausible, novel, and diverse protein sequences for\nunconditional generation. We further demonstrate the proposed diffusion\ngenerative pre-training makes DPLM possess a better understanding of proteins,\nmaking it a superior representation learner, which can be fine-tuned for\nvarious predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).\nMoreover, DPLM can be tailored for various needs, which showcases its prowess\nof conditional generation in several ways: (1) conditioning on partial peptide\nsequences, e.g., generating scaffolds for functional motifs with high success\nrate; (2) incorporating other modalities as conditioner, e.g.,\nstructure-conditioned generation for inverse folding; and (3) steering sequence\ngeneration towards desired properties, e.g., satisfying specified secondary\nstructures, through a plug-and-play classifier guidance. Code is released at\n\\url{https:\/\/github.com\/bytedance\/dplm}.","url":"http:\/\/arxiv.org\/abs\/2402.18567v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.18567v2","published":1709146676000,"comment":"ICML 2024 camera-ready version","pdf_text":"Diffusion Language Models Are Versatile Protein Learners\nXinyou Wang * ♢♡Zaixiang Zheng * ♡Fei Ye ♡Dongyu Xue ♡Shujian Huang ♢Quanquan Gu ♡\nAbstract\nThis paper introduces diffusion protein language\nmodel (DPLM), a versatile protein language\nmodel that demonstrates strong generative and\npredictive capabilities for protein sequences. We\nfirst pre-train scalable DPLMs from evolutionary-\nscale protein sequences within a generative self-\nsupervised discrete diffusion probabilistic frame-\nwork, which generalizes language modeling for\nproteins in a principled way. After pre-training,\nDPLM exhibits the ability to generate struc-\nturally plausible, novel and diverse protein se-\nquences for unconditional generation. We fur-\nther demonstrate the proposed diffusion genera-\ntive pre-training make DPLM possess a better\nunderstanding of proteins, making it a superior\nrepresentation learner, which can be fine-tuned\nfor various predictive tasks, comparing favorably\nto ESM2 (Lin et al., 2022). Moreover, DPLM\ncan be tailored for various needs, which show-\ncases its prowess of conditional generation in\nseveral ways: (1) conditioning on partial pep-\ntide sequences, e.g., generating scaffolds for func-\ntional motifs with high success rate; (2) incor-\nporating other modalities as conditioners, e.g.,\nstructure-conditioned generation for inverse fold-\ning; and (3) steering sequence generation towards\ndesired properties, e.g., satisfying specified sec-\nondary structures, through a plug-and-play clas-\nsifier guidance.\nCode is released at https:\n\/\/github.com\/bytedance\/dplm.\n1\nIntroduction\nProteins, which are 3D-folded linear sequences of amino\nacids, play a pivotal role in regulating various biological\nfunctions, including transcription, translation, signaling, and\nthe control of the cell cycle. Recently, the promise of learn-\ning to understand and design proteins via data-driven gener-\n*Equal contribution\n♢Dept. of Computer Science, Nanjing\nUniversity (this work was done during Xinyou’s internship at\nByteDance Research) ♡ByteDance Research. Correspondence to:\nQuanquan Gu <quanquan.gu@bytedance.com>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\native deep learning has initiated a significant paradigm shift\napart from the long-established physics-based methods.\nThe analogies between protein sequences and human lan-\nguages have long been recognized (Yang et al., 2019; Ferruz\n& H¨ocker, 2022). Drawing inspiration from the remarkable\nprogress in NLP achieved by language models (LMs; Devlin\net al., 2019; Radford et al., 2018; OpenAI, 2023) thanks to\nthe scalability of Transformers (Vaswani et al., 2017) and\nthe existence of large-scale text data, recent explorations in\nprotein has also demonstrated the impressive capabilities of\nprotein language models (Rives et al., 2019; Lin et al., 2022;\nHu et al., 2022), learned from the universe of evolutionary-\nscale protein sequences. As a result, protein LMs have\nbecome one of the most important cornerstones in AI for\nprotein research, serving a pivotal role not only in predictive\ntasks (e.g., probing functional properties, and predicting\nprotein structures from single sequences without explicit\nevolutionary homologs) but also in generative tasks (e.g.,\nredesigning sequences given protein backbone structures, or\nsynthesizing completely new protein sequences).\nWhile current protein LMs have made significant strides,\nthey have not yet reached their fullest potential. One of the\nfundamental problems is rooted in the widely-used pretrain-\ning objectives, i.e., masked prediction vs. autoregression:\n(i) For masked prediction, masked language models\n(Masked-LMs, e.g., ESM family; Rives et al., 2019;\nLin et al., 2022) excel in sequence understanding for\nprotein predictive tasks, thanks to their bi-directional\nreceptive field. However, Masked-LMs are unable to\nperform protein sequence generation, due to the lack\nof a well-defined formulation for generative modeling.\nWe further postulate that this could even cap their pre-\ndictive power, since a powerful generative model that\ncan create new samples by learning the underlying data\ndistribution, is expected to simultaneously acquire a\ndeep understanding of the data. As a famous quote,\n“what you cannot create, you do not understand.”\n(ii) For autoregression, autoregressive language mod-\nels (AR-LMs, e.g., ProGen; Nijkamp et al., 2022),\nalbeit good at generation, often fall short in under-\nstanding sequence data (Radford et al., 2018) includ-\ning proteins (Elnaggar et al., 2021). More importantly,\nproteins are structural macromolecules rather than sim-\nple linear strings. Consequently, while effective as\n1\narXiv:2402.18567v2  [cs.LG]  16 Oct 2024\nDiffusion Language Models Are Versatile Protein Learners\nTranformer Layer x N\nMLP\nBidirectional  \nMultihead ATTN\n✘✘✘✘✘✘✘✘✘✘\nMKTVRQERLKYRA\nMKT✘RQE✘✘KYRA\n✘K✘✘RQE✘✘KY✘A\nx(t)\nx(t-1)\nx(T)\nx(0)\n⋯\nforward discrete diffusion\n⋯\nDPLM\nreverse denoising generation\nA\nB learned representation  \nfor predictive tasks\nDPLM\nESMFold\nMKTVRQERLKYRA\nresidue-level classification\nsequence-level regression\nseq-level classification\ncontact prediction\n⋯⋯\nC conditional generation\nDPLM\nMKTVRQERLKYRA\n✘✘✘✘✘RQER✘✘✘\nMTKYAKRQERYAR\nDPLM\nMKTAYRVA\n(2) conditioned on other modality\n      e.g., structure-conditioned inverse folding\nAdapter\nTransformer \nLayer x N\n✘✘✘✘✘✘✘✘\nDPLM\ntemplate protein\ntemplate secondary \nstructure annotations\nguide\nMKTYRKYVA\nESMFold\nUniRef-50 (~4 x107)\npre-training on \nevolutionary scale  \nsequence data\n{\nevolutionary scale \ndiffusion protein LM \npre-training  \n& unconditional \ngeneration\nx(0)\nsequence\nembedding\n(3) controllable generation with discrete classifier guidance\n     e.g., secondary structure guided generation\n✘✘✘✘✘✘✘✘\n(1) conditioned on partial sequence\n      e.g., motif-conditioned scaffolding\nlength: 1000 \npLDDT: 89.66\nsec. struct.\nclassifier\nFigure 1. Overall illustration of DPLM. (A): modeling, pre-training and unconditional generation; (B): protein sequence representation for\npredictive tasks; (C): conditional generation, including (1) sequence conditioning (e.g., motif-scaffolding), (2) cross-modal conditioning\n(e.g., inverse folding), and (3) plug-and-play controllable generation with discrete classifier guidance (e.g., secondary structure).\nan inductive bias for text, AR-LMs are constrained by\ntheir uni-directional receptive field, only accessing one-\nsided sequence context. This limitation stems from\ncapturing the complex global interactions of amino\nacids, thereby hindering both generative and predictive\ncapabilities of protein LMs.\nThis highlights the demand for a general-purpose and ver-\nsatile protein LM that combines predictive and generative\ncapabilities. Provided the aforementioned analysis, we rea-\nson that, the key ingredients for such a versatile protein LM\nlie in (1) strong & scalable generative modeling framework\nto best digest the universe of massive protein sequences;\nand (2) bi-directional receptive field for better modeling\nresidue-wise global interactions.\nOn the other hand, diffusion models (Ho et al., 2020;\nSong et al., 2020) have shown great success in generat-\ning continuous data, especially in rendering photorealistic\nimages (Rombach et al., 2021, inter alia). They have fur-\nther manifested incredible achievement in modeling protein\nstructures (Yim et al., 2023; Watson et al., 2023; Ingra-\nham et al., 2023). This can be attributed to their favorable\nproperties of non-autoregressive denoising generation with\niterative refinement and global receptive field. Besides, de-\nnoising autoencoding has a long history for representation\nlearning (Vincent et al., 2010, inter alia), while recent stud-\nies have verified that diffusion-based generative models can\nbe effective self-supervised learners (Chen et al., 2024a).\nThese make diffusion models an appealing generative foun-\ndation for protein language modeling. However, directly ap-\nplying conventional Gaussian diffusion to protein sequences\nnecessitates additional continuous relaxations (Lisanza et al.,\n2023), which does not fit the discrete nature of protein se-\nquence and has not yet proven successful in practice.\nIn this paper, we present diffusion protein language model\n(DPLM), a novel approach aimed at achieving a unified\nand versatile protein LM through diffusion generative pre-\ntraining on evolutionary-scale protein sequences. DPLM\nis grounded in a discrete diffusion probabilistic framework,\nserving as a principled generative generalization of language\nmodeling. During pre-training, DPLM is tasked with de-\nnoising the input protein sequence at different noise levels,\nranging from completely noisy to clean ones, enforcing\nDPLM to best the model complex intrinsic dependencies\nof amino acid sequences. After pre-training, DPLM can be\nused for protein sequence generation and providing effec-\ntive representations for downstream predictive tasks. We\nhighlight our contributions as follows:\n• We propose DPLM, a versatile protein LM under dis-\ncrete diffusion framework, with model size up to 3B,\npre-trained on evolutionary-scale protein sequences.\n2\nDiffusion Language Models Are Versatile Protein Learners\nWe further develop multiple conditioning strategies\ncovering various use needs, especially discrete classi-\nfier guidance for controllable generation. As a result,\nDPLM combines the best of both worlds, i.e., the\nscalable expressiveness of language models and the\nstrong generative power of diffusion models, serving\nas a versatile biological foundation model (Fig. 1, §3).\n• We show that DPLM is capable of generating highly\nstructurally plausible (i.e., averaged pLDDT > 80),\nnovel and diverse for unconditional protein sequence\ngeneration, suggesting that DPLM well captures the\nuniverse of protein sequence data (Fig. 1A, §4.1).\n• We demonstrate that DPLM understands protein better,\nserving as a superior representation learner, which can\nbe fine-tuned for various downstream tasks, comparing\nfavorably with widely-used protein sequence encoder\nmodels, e.g., ESM-2 (Lin et al., 2022) (Fig. 1B, §4.2).\n• DPLM can be further exploited for conditional gener-\nation for a variety of needs: DPLM can (1) condition\non pre-specified partial sequence, e.g., scaffolding for\nfunctional motifs with high success rate; (2) incor-\nporate other modalities as conditions, e.g., structure-\nconditioned generation for inverse folding; (3) generate\nprotein sequences towards desired properties with plug-\nand-play classifier-guidance, e.g., steering DPLM to\nsynthesize proteins that satisfy arbitrary user-defined\nsecondary structure annotations (Fig. 1C, §4.3).\n2\nPreliminaries\n2.1\nLanguage Modeling for Protein\nLanguage modeling aims to estimate the underlying dis-\ntribution x ∼q(x) of the sequence data of our interest,\ne.g., text or protein sequence, by learning a probabilis-\ntic model pθ(x). Here the language model (LM) θ is pa-\nrameterized by a neural network, in particular Transform-\ners (Vaswani et al., 2017), which have become the de facto\nchoice dominating different domains with scalable and per-\nforming expressiveness. In this work, we are interested\nin language modeling for protein sequences, for which\nx = (x1, x2, . . . , xL) ∈{0, 1}L×|V| is a sequence com-\nposing L elements, V is the vocabulary within a discrete\ndata support of 20 amino acids V = {1, ..., 20}. One thing\nwe most care about is the generative and representational\ncapabilities of protein LMs. Here we review the typical\nprobabilistic paradigms for language modeling, i.e., masked\nprediction and autoregression, and their pros and cons as\nthe foundation for protein LMs, as follows.\nMasked Prediction. Masked language models (Masked-\nLMs or MLMs), e.g., BERT (Devlin et al., 2019) and its\nvariants for protein sequence (ESM family, Rives et al.,\n2019; Lin et al., 2022), employ a bidirectional transformer\nto take into account both the left and right context to pre-\ndict the masked (amino acid) symbols in a mask-predict\nautoencoding manner,\nEq(x) log pθ(x) = Eq(x)\nP\n1≤i≤Lbi · log pθ(xi|¯xm),\n(1)\nwhere bi = 1¯xi=[X] derived from a fixed chance (e.g.,\nwidely-adopted 15%) of masking x with a special mask\nsymbol [X], resulting in the masked observation ¯xm. A\nper-token conditional independence assumption is made as\nwell. Masked-LMs significantly excel the performance of a\nwide range of sequence understanding tasks for both natural\nlanguage and protein. However, its bidirectionality nature\nmakes it difficult to apply to sequence generation.\nAutoregression. AR-LMs are prevailing in the realm se-\nquence generation (OpenAI, 2023; Nijkamp et al., 2022),\nwhich adopts a sequential factorization over the sequence us-\ning the probability chain rule. In this case, the log-likelihood\nof such models is maximized over the dataset given by:\nEq(x) log pθ(x) = Eq(x)\nP\n1≤i≤L log pθ(xi|x<i),\n(2)\nwhere causal masking is used to ensure sequential depen-\ndency structure. To sample from AR-LMs, it requires ances-\ntral sampling for L iterative steps from x1 ∼pθ(x1), x2 ∼\npθ(x2|x1) towards xL ∼p(xL|x1, ..., xL−1) in a strict left-\nto-right unidirectional manner.\n2.2\nDiffusion Probabilistic Models\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020) are a class of generative models\ncharacterized by a pair of Markov processes, i.e., a for-\nward diffusion process and a backward denoising process.\nThe forward process q(x(1:T )|x(0)) = QT\nt=1 q(x(t)|x(t−1))\ngradually perturb the data x(0) ∼q(x(0)) into a stationary\ndistribution x(T ) ∼qnoise with T increasingly noisy steps\nx(1:T ) = x1, . . . , x(t−1), x(t), . . . , x(T ). The learned back-\nward process pθ(x(0:T )) = p(x(t)) QT\nt=1 pθ(x(t−1)|x(t)),\nreversely, gradually denoises the samples towards the data\ndistribution. To fit the model pθ(x(0)) to the data distribu-\ntion q(x(0)), the denoiser model is typically optimized by\nthe variational bound of the log-likelihood (Ho et al., 2020):\nEq(x(0))\n\u0002\nlog pθ(x(0))\n\u0003\n≥Eq(x(0:T ))\n\u0014\nlog\npθ(x(0:T ))\nq(x(1:T )|x(0))\n\u0015\n= Eq(x(0))\nh\nlog pθ(x(0)|x(1)) + const.\n+ PT\nt=2 −KL\n\u0002\nq(x(t−1)|x(t), x(0))∥pθ(x(t−1)|x(t))\n\u0003i\n|\n{z\n}\nJt\n.\nAfterwards, it generates by first sampling from qnoise(x(T )),\nfollowed by iterative denoising with pθ(x(t−1)|x(t)).\n3\nDPLM: A Versatile Protein LM\nMotivation. Continuous diffusion with Gaussian pertur-\nbation kernel has demonstrated impressive performance\nin generating continuous data in Euclidean space (Rom-\nbach et al., 2021; Ho et al., 2022), and the more general\n3\nDiffusion Language Models Are Versatile Protein Learners\nRiemannian manifolds (De Bortoli et al., 2022). Recently,\ncontinuous diffusion has shown to rival in modeling pro-\ntein structures (Watson et al., 2023; Ingraham et al., 2023,\ninter alia), wherein its bidirectional receptive field is ide-\nally suited for modeling residue-wise global interactions.\nThis motivates us to blend diffusion models, which are well-\nsuited for protein as discussed above, and language models,\nwhich are well known as scalable and expressive sequence\nlearners. This leads to our pursuit of a diffusion protein LM,\ntaking the best of both worlds.\nA direct use of continuous diffusion, however, is not nec-\nessarily the best choice for modeling discrete sequence\ndata (Li et al., 2022; Dieleman et al., 2022; Lisanza et al.,\n2023), due to the pitfall of discreteness that makes Gaussian\ndiffusion hardly model the discrete nature of sequence data\nin embedding space (Ye et al., 2023b). To this end, discrete\ndiffusion (Hoogeboom et al., 2021b; Austin et al., 2021) that\ndirectly operates over the discrete state space, becomes a\nmore well-suited probabilistic model for protein sequences.\n3.1\nProtein Language Modeling w\/ Discrete Diffusion\nModeling. Let Cat(x; p) be a categorical distribution on\nprotein sequence x parameterized by a vector p on (|V|−1)-\ndimensional probability simplex. The forward process of\ndiscrete diffusion defines a Markov process governed by the\ntransition kernel:\nq(x(t)|x(t−1)) = Cat\n\u0000x(t); βtx(t−1) + (1 −βt)qnoise\n\u0001\n,\nwhere qnoise is the probability vector of stationary distribu-\ntion qnoise(x(t)), i.e., q(x(t)) = Cat(x(t); p = qnoise), and\n0 ≪βt < 1 is the noise schedule controlling the degree\nof corruption at timestep t. In this case, the distribution\nof corrupted sample x(t) given its original data x(0) has a\nclosed-form expression:\nq(x(t)|x(0)) = Cat\n\u0000x(t); αtx(0) + (1 −αt)qnoise\n\u0001\n,\n(3)\nwhere αt = Qt\ni=1 βi such that limt→T αt →0, which pre-\nserves no information from the data and converges to the\nstationary distribution qnoise at timestep T. This shows that\nthe diffusion process is intuitively a convex combination\nbetween data and the stationary noise prior distribution. Dif-\nferent stationary distributions qnoise lead to different formu-\nlations of discrete diffusion models. Here we primarily con-\nsider the absorbing diffusion with q(x(t)) = {1 if x(t) =\n[X]; 0 if x(t) ̸= [X]}, where [X] is an absorbing state,\nakin to Masked-LMs. The formulation of Eq. (3) results in\nx(t) either being masked or the same as x(0), with a masking\nratio (1 −αt).\nLearning. As stated in Austin et al. (2021), discrete dif-\nfusion inherently connects to AR-LM and Masked-LM,\nwhilist Zheng et al. (2023a) further simplifies the learning\nobjective of discrete diffusion, with their proposed reparam-\neterized backward transition, from KL divergences between\ntwo categoricals into reweighted cross-entropies:\nJt = Eq(x(0)) −KL\n\u0002\nq(x(t−1)|x(t), x(0))∥pθ(x(t−1)|x(t))\n\u0003\n= Eq(x(0))\nh\nλ(t)P\n1≤i≤Lbi(t) · log pθ(x(0)\ni |x(t))\ni\n,\n(4)\nwhere λ(t) is a weighting coefficient induced from the spe-\ncific noising schedule (see Appendix A for proof). Eq. (4)\nreveals that Masked-LMs (i.e., x(t) ≜¯xm in Eq. (1)) and\nAR-LMs (i.e., x(t) ≜x<t and bi ≜1 in Eq. (2)) can be con-\nsidered as special cases in this generalized form of discrete\ndiffusion LMs, contingent on their respective specifications\nof the noise-induced configurations. As a result, the pro-\ncess of learning according to Eq. (4) inherently encapsulates\nboth Masked-LMs and AR-LMs within the ambit of the\nproposed DPLM.\nEvolutionary-scale Pre-training. The pre-training pro-\ncedure for DPLM utilizes the UniRef50 database (Suzek\net al., 2015), which comprises around 45 million protein se-\nquences, totaling about 14 billion amino acid tokens. In the\ncase of exceedingly lengthy protein sequences, we emulate\nESM2 (Lin et al., 2022) by truncating these proteins to a\nrandom sequence of 1024 tokens. Besides, we adhere to the\nsetting for model architecture and scales as ESM2, which\ncorrespond to DPLM with sizes of 150M, 650M and 3B.\nWe train all models for 100K updates, with batch size of\n320K for 150M model and 1M for 650M\/3B models.\nGeneration. Given a trained DPLM, it can synthesize new\namino acid sequences by the reverse iterative denoising\nprocess of discrete diffusion (Hoogeboom et al., 2021b;\nAustin et al., 2021). Formally, discrete diffusion samples\nfrom the following distribution,\npθ(x(t−1)|x(t)) = P\nˆx0q(x(t−1)|x(t), ˆx0)pθ(ˆx0|x(t)).\nIn particular, at time t, we first generate ˆx0 from pθ(·|x(t)),\nthen a less noisy x(t−1) is sampled by q(·|x(t), x(0) = ˆx0)\ngiven x(t) and ˆx0. This process is repeated from T to 1. The\ngenerative denoising process of DPLM can be viewed as an\niterative mask-predict approach. Specifically, the starting se-\nquence is initialized as 100%-noisy state (i.e., all [X]’s). At\neach iteration, a subset of masked tokens is updated based on\nthe model’s prediction ˆx0, while the remaining tokens are re-\nmasked, according to ranked log pθ(ˆx0|x(t)) (Ghazvinine-\njad et al., 2019; Zheng et al., 2023a).\nRepresentation. DPLM is tasked with denoising the input\nprotein sequence at all noise levels, including the original\nnoise-free data (e.g., noise level at 0%). As a result, DPLM\ncan simultaneously serve as a protein sequence representa-\ntion learner over massive protein sequence data, providing\nuseful sequence embedding for various protein predictive\ndownstream tasks, e.g., sequence\/residue-level classifica-\ntion\/regression. The sequence embedding can be attained\nby simply letting DPLM take as input the given amino acid\nsequence x: h(x) ←DPLMθ(x, t = 0) ∈RL×d, where d\nis the dimension of embedding.\n4\nDiffusion Language Models Are Versatile Protein Learners\n3.2\nConditioning\nBeing able to efficiently sample realistic proteins is nec-\nessary but not sufficient for downstream applications such\nas therapeutic development, since unconditional samples\nare unlikely to possess desired functional properties. Here\nwe elaborate on how to make DPLM practically useful by\nconditioning for various needs, which covers most common\nscenarios, i.e., sequence conditioning, cross-modal condi-\ntioning, and plug-and-play preference-guided conditioning.\nCase I: Conditioning on partial sequence (Fig. 1C-1).\nProtein generation containing pre-specified polypeptides\ncorresponds to various use cases such as generating scaf-\nfolds for given functional motifs, infilling antibody CDR\nloops, or imposing expert knowledge a-priori. This implies\nour desire for DPLM to sample from this conditional dis-\ntribution x ∼pθ(x|¯x) = QL\ni=1 bi · pθ(xi|¯x), which has\nalready been learned through Eq. (4). The observed partial\nsequence ¯x = {¯xi ∈V if bi = 0; [X] if bi = 1|i ∈[1, L]}.\nNamely, bi ∈{0, 1} indicates whether the predicted se-\nquence must preserve the observation for the i-th residue\nsuch that xi = ¯xi.\nCase II: Adapting DPLM to conditioned on other modal-\nities (Fig. 1C-2). Generating protein sequence subject to\ncross-modal constraints c, i.e., x ∼pθ(x|c), has profound\nvalue in practice, such as inverse protein folding where\nsequences are generated for given backbone structure (Dau-\nparas et al., 2022; Zheng et al., 2023b), or conditioning on\nsmall molecule ligands for binder design (Dauparas et al.,\n2023). Given that DPLM primarily operates over amino\nacid tokens, in these cases, we can equip DPLM with cross-\nmodal conditioning by adapter-tuning with a pre-trained\nmodality expert encoder Eϕ(c) and a newly-added cross-\nattention-based adapter following Zheng et al. (2023b). Dur-\ning training, we freeze the parameters of the modality en-\ncoder and DPLM, and only update the parameters of the\nadapter via supervised fine-tuning on the given paired data\n(x, c). We then obtain a conditional DPLM for pθ(x|Eϕ(c))\nmaking the full potentials of both DPLM and the modal-\nity expert Eϕ(c). In §D.5, we also develop classifier-free\nguidance for such adapter-tuned DPLM as an immediately\navailable booster for cross-modal conditional generation\nwithout intricate condition dropout during training.\nCase III: Plug-and-play controllable generation with\ndiscrete classifier guidance (Fig. 1C-3). Directly build-\ning a conditional model is prohibitive in most cases due\nto data scarcity. Thus, incorporating classifier guidance\ninto continuous diffusion models (Dhariwal & Nichol,\n2021a) proves particularly useful. This integration with pre-\ntrained classifiers enables steering generation towards de-\nsired preferences. However, continuous classifier guidance\nrequires valid definition of ∇x log pθ(x), or “score” (Song\n& Ermon, 2019), which does not exist for discrete dif-\nfusion. Inspired by continuous diffusion classifier guid-\nance and DiGress on guided graph diffusion (Vignac\net al., 2022), here we introduce classifier-guided con-\nditional generation for discrete diffusion LMs.\nCon-\ncretely, we want to sample from the conditional distribution\nof q(x(t−1)|x(t), y) ∝q(x(t−1)|x(t))q(y|x(t−1)), which\nis approximated by pθ(x(t−1)|x(t))pϕ(y|x(t−1)) where\npϕ(y|x(t−1)) is a discriminative guidance model (classi-\nfier or regressor w.r.t. user’s desired properties). However,\npϕ(y|x(t−1)) cannot be factorized as a product over all posi-\ntions, prohibiting evaluation of all possible values of x(t−1).\nTo this end, we resort to an approximation with first-order\nTaylor expansion around x(t) (Dhariwal & Nichol, 2021a),\nwhere we treat x as a continuous one-hot variable on proba-\nbility simplex to make ∇x a valid operator, thereby,\nlog q(y|x(t−1))\n≈log q(y|x(t)) + ⟨∇x log q(y|x(t)), x(t−1) −x(t)⟩\n≈P\n1≤i≤L⟨∇xi log q(y|x(t)), x(t−1)\ni\n⟩+ C(x(t)),\nwhere C(x(t)) is a constant that does not depend on x(t−1).\nWe use pϕ(y|x(t)) to estimate q(y|x(t)) and plug it into the\nabove expression. We can now sample from the resulting\nconditional distribution instead at each timestep t,\nx(t−1) ∼pθ(x(t−1)|x(t))pϕ(y|x(t−1))η\n(5)\n∝pθ(x(t−1)|x(t))e\n\u0000η·P\ni⟨∇xi log pϕ(y|x(t)),xt−1\n(i) ⟩\n\u0001\n,\nwhere a tunable η controls the strength of guidance.\n3.3\nComparisons with The Most Related Work\nComprehensive representations for protein sequence under-\nstanding are achieved by pre-training on protein sequence\ndata via masked language modeling (Devlin et al., 2019),\nakin to language understanding. Among those, the family of\nESM-1b\/ESM2 (Rives et al., 2019; Lin et al., 2022) serves\nas the pioneer & cornerstone sequence embedding models\nfor extensive protein predictive tasks. Therefore, DPLM fol-\nlows the best practice of ESM2 in network architecture and\npre-training strategies. DPLM takes a significant leap from\nESM2 with immediate strong generative capabilities, with-\nout expensive needs for Monte Carlo methods (Verkuil et al.,\n2022) or Gibbs sampler (Johnson et al., 2021), which treat\nMasked-LM as Markov random fields (Wang & Cho, 2019).\nBesides, as verified from predictive experiments (§4.2), the\ngenerative ability of DPLM further enables its enhanced\nrepresentation learning, echoing Richard Feynman’s famous\nquote “What I cannot create, I do not understand”.\nRegarding protein sequence generation, EvoDiff (Alamdari\net al., 2023) is the most relevant approach, which uses order-\nagnostic autoregressive diffusion models (OADM, Hooge-\nboom et al., 2021a) for unconditional generation, with condi-\ntional applications on intrinsic disordered sequence infilling\nand motif-scaffolding, whereas attaining better performance\nnecessitates multiple sequence alignments (MSAs) based\n5\nDiffusion Language Models Are Versatile Protein Learners\nLength:  400\npLDDT: 95.85\npLDDT: 88.94\nLength:  600\npLDDT: 89.66\nLength: 1000\npLDDT: 88.98\nLength:  700\npLDDT: 88.41\nLength:  800\npLDDT: 87.92\nLength:  900\npLDDT: 91.65\nLength:  500\npLDDT: 84.80\npLDDT: 91.67\npLDDT: 88.65\nLength:  100\nLength:  200\nLength:  300\nE\nA\nB\nH\nD\nC\nG\n- foldability (pLDDT, ↑)\n- novelty (pdb-TM, ↓)\n- diversity (inner-TM, ↓)\n- secondary structure statistics\n- comparison of DPLM model scales \n  wrt sequence lengths (pLDDT, ↑)\n- comparison of probabilistic framework\n  for protein LMs (pLDDT, ↑)\n- comparison of pre-training\n  strategy (pLDDT, ↑)\n- ESMFold predicted structures of unconditional sampled sequences \nF\nFigure 2. Evaluation of unconditional generation. Here we use ESMFold as the folding model to predict structures and calculate pLDDT\nfor all the sampled sequences. We measure the (structural) novelty of the generated sequences against all known structures in PDB by\nTM-score (i.e., pdb-TM, and measure the (structural) diversity within the sampled candidates for each model (i.e., inner-TM).\non a MSA-Transformer (Rao et al., 2021) parameterization.\nDPLM differs from EvoDiff in several aspects: (1) DPLM\nmanifests superior representation learning, which, to the\nbest of our knowledge, is the first time for protein diffusion\nmodels, even in general language learning regime, show-\ning DPLM’s appealing versatility, as shown in Tab. 1; (2)\nDPLM is based on a more principled discrete diffusion\nframework beyond the special (order-agnostic) autoregres-\nsive diffusion, which is not compatible with refining inter-\nmediate predictions and requires expensive O(L) decoding\noverhead; (3) we investigate the ability of DPLM to ac-\ncommodate extensive conditioning, especially conditioning\non other modality and programmable generation steered by\ndiscrete classifier guidance, pushing steps forward beyond\nsimple sequence conditioning investigated in EvoDiff paper.\nPlease refer to Appendix §E for a more detailed discussion\nof the related work.\n4\nExperiments\nWe evaluate DPLM on extensive generative and understand-\ning tasks, spanning unconditional generation (§4.1), a vari-\nety of protein predictive downstream tasks (§4.2), and con-\nditional tasks, including motif-scaffolding (§4.3.1), inverse-\nfolding task (§4.3.2), and secondary structure guided con-\ntrollable generation (§4.3.3).\nWe find that, in general,\nDPLM with larger model scales can attain better results\nthan smaller ones, demonstrating the scaling law can also\nhold for protein language modeling. Please refer to the\nAppendix for more detailed experimental settings.\n4.1\nEvaluation of Unconditional Generation\nFig. 2 shows the results of DPLM for unconditional genera-\ntion, where we evaluate the performance regarding a set of\nlengths [100, 200, ..., 900, 1000] in intervals of 100. The re-\nverse process of DPLM for sampling iterates for 500 steps.\nMeanwhile, we also randomly pick the natural sequences\nof the same length from UniRef50 as reference (denoted as\nUR50) We highlight our primary findings as follows:\n(1) On Foldability: DPLM is capable of generating pro-\ntein sequences with reasonable predicted structures. We\nexamine the structural plausibility or foldability of protein\nsequences using the state-of-the-art single-sequence struc-\nture prediction model, i.e., ESMFold (Lin et al., 2022),\nand measured by the predicted local distance difference\ntest (pLDDT) score, which is considered high confidence\nif pLDDT > 70. We can find that protein sequences gener-\nated by DPLM achieve the highest pLDDT score across all\nlengths (Fig. 2A). Plus, secondary structure analysis of the\nsequences generated by DPLM reveals a higher proportion\nof beta-strands (Fig. 2D), and overall similar to the statistics\nof known protein structures in Protein Data Bank (PDB;\nBerman et al., 2000). Moreover, we can see that scaling\nDPLM leads to better foldability performance, especially\nfor very long proteins (Fig. 2E).\n(2) On Novelty. We investigate whether DPLM can sample\nsequences possessing novel structures, where we compare\nthe structural similarity against known structures in PDB\nwith TMScore. The highest TMscore is used to measure\nthe novelty of each sequence, which we refer to as pdb-TM\nscore. Overall, DPLM has relatively higher pdbTM than\n6\nDiffusion Language Models Are Versatile Protein Learners\nTable 1. Performance on various protein predictive downstream tasks. †: benchmarked results are quoted from Su et al. (2023).\nModels\nThermostability\nHumanPPI\nMetal Ion Binding\nEC\nGO\nDeepLoc\nSSP\nMF\nBP\nCC\nSubcellular\nBinary\nCASP12\nSpearman’s ρ\nAcc (%)\nAcc (%)\nFmax\nFmax\nFmax\nFmax\nAcc (%)\nAcc (%)\nAcc (%)\n†SaProt (*structure provided)\n0.724\n86.41\n75.75\n0.884\n0.678\n0.356\n0.414\n85.57\n93.55\n-\n†ESM-1b (Rives et al., 2019)\n0.708\n82.22\n73.57\n0.859\n0.661\n0.320\n0.392\n80.33\n92.83\n-\n†MIF-ST (Yang et al., 2022b)\n0.694\n75.54\n75.08\n0.803\n0.627\n0.239\n0.248\n78.96\n91.76\n-\nMasked-LM (ESM2-650M)\n0.691\n84.78\n71.88\n0.866\n0.676\n0.344\n0.402\n83.68\n92.28\n0.80\nAR-LM (650M)\n0.638\n68.48\n61.16\n0.691\n0.566\n0.258\n0.287\n68.53\n88.31\n-\nDPLM (150M)\n0.687\n80.98\n72.17\n0.822\n0.662\n0.328\n0.379\n82.41\n92.63\n-\nDPLM (650M)\n0.695\n86.41\n75.15\n0.875\n0.680\n0.357\n0.409\n84.56\n93.09\n0.82\nDPLM (3B)\n0.704\n90.00\n75.94\n0.883\n0.687\n0.369\n0.463\n85.32\n93.93\n-\nEvoDiff and natural sequences, as shown in Fig. 2B. Interest-\ningly, the pdbTM score of DPLM will decrease as protein\ngets longer than 300 while maintaining the pLDDT > 75.\nThis indicates that DPLM possesses the ability to sample\nsequences with structures not similar to PDB across vari-\nous lengths, with the discrepancy becoming increasingly\napparent as the sequence length extends.\n(3) On Diversity. We quantify the diversity of sequences\nsampled by DPLM by inner-TM score. Specifically, for\neach sampled candidate, we use ESMFold to predict its\nstructure and compute TMscore against the rest. The\naverage TMscore is considered as the diversity.\nAs\nshown in Fig. 2C, DPLM has a considerably low average\ninner-TM, demonstrating that the DPLM can synthesize\nstructurally diverse sequences.\n(4) On Learning: Discrete diffusion is the best-suited prob-\nabilistic framework for protein sequence generation, com-\npared to Masked-LM and AR-LM. As shown in Fig. 2F,\nDPLM outperforms Masked-LM and AR-LM in terms of\nfoldability, verifying our motivation to pursue a diffusion\nprotein LM that diffusion is a more proper probabilistic\nframework for protein modeling. Moreover, AR-LM also\nfalls short of precisely controlling the length of sampled\nsequences, making it less flexible in practice. As revealed\nin Fig. 2G, we find that despite attaining improved genera-\ntion quality over ESM2 with directly pre-training DPLM\nfrom scratch (DPLM-FS), it can bring additional learning\nchallenges and training overheads. As such, we leverage a\n2-stage training strategy, which consists of masked language\nmodeling as the first stage objective, followed by diffusion\nobjective, solving this problem and obtaining high-quality\ngeneration with pLDDT closely approaching 90.\n(5) Case Study. In Fig. 2H, we showcase proteins sampled\nby DPLM across various lengths, ranging from 100 to 1000,\nwhile more cases are presented in the Appendix. As the pro-\ntein gets longer, the complexity of its structure will increase,\ncontaining rich helices and sheets. We also find that DPLM\ncan sample proteins composed of tandem repeats such as\nbeta-barrel or Kelch repeat domain.\n4.2\nEvaluation of Protein Representation Learning on\nDownstream Predictive Tasks\nWe evaluate DPLM across a variety of protein predictive\ntasks\n(Su et al., 2023; Dallago et al., 2021; Xu et al.,\n2022), including protein function prediction (Thermosta-\nbility and Metal Ion Binding), protein localization predic-\ntion (DeepLoc), protein annotation prediction (EC and GO),\nprotein-protein interaction prediction (HumanPPI), where\nwe perform full-parameters supervised fine-tuning on each\ndataset. We also include linear probing for secondary struc-\nture from TAPE (Rao et al., 2019).\nDPLM is a superior protein sequence representation\nlearner. As demonstrated in Tab. 1, DPLM outperforms\nESM2 across all tasks. This improved performance is due to\nthe proposed diffusion pre-training, which requires DPLM\nto adeptly learn to reconstruct the native sequence from a\nvaried proportion of masking, including very high noise\nlevel, in contrast to ESM2 of a fixed 15% masking ratio.\nUnder this circumstance, it becomes a much more challeng-\ning missing amino acid reconstruction task encouraging the\nmodel to capture the deep dependencies from the very con-\ntext. Besides, we surprisingly find that DPLM also closely\napproaches the performance of SaProt (Su et al., 2023),\nwhich is a structure-aware LM that incorporates explicitly\nprotein structures based on Foldseek (van Kempen et al.,\n2023) and folding models like AlphaFold (Jumper et al.,\n2021). This implies that DPLM may implicitly learn the\nprotein structures from massive sequence data. Integrat-\ning explicit structural information into DPLM like Su et al.\n(2023) may bring further benefits, which deserve further\nexploration. Our results substantiate our initial premise that\nDPLM gains a deeper understanding of protein through\nthe generative learning process, i.e., it learns to better un-\nderstand proteins by learning to generate them, leading to\nimproved predictive performance.\n4.3\nEvaluation of Conditional Generation\n4.3.1\nSEQUENCE-COND.: MOTIF-SCAFFOLDING\nThe goal of motif-scaffolding requires a valid scaffold to\nmaintain the structure of the given motif such that the orig-\ninal function can be preserved. Here, we follow the ex-\nperimental setting in Alamdari et al. (2023), where we (1)\n7\nDiffusion Language Models Are Versatile Protein Learners\nADQLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLG\nQNPTEAELQDMINEVDADGNGTIDFPEFLTMMARKMKDTD\nSEEEIREAFRVFDKDGNGYISAAELRHVMTNLGELTDEEV\nDEMIREADIDGDGQVNYEEFVQMMTAK\nSLFDKDGDGTITTKELGTVMRSLGQNPSESELQDMINEVD\nADGNGTIDFPEFLTMMARKMKDTDSEEEIREAFSLFDKDG\nDGTITTKELGTVMRSLGQNPSESELQDMINEVDADGNGTI\nDFPEFLTMMARKMKD\n1PRW: binding site of compact calmodulin\nnative sequence\ngenerated sequence\nMHLLDGRRMRTKADLHRELKRVLALPEYYGENLDA\nLWDALTGWVEPPTVLLWTHWSVVAQAMPRHAQTTL\nQVLAEAAEYWRDEGHPFTVLVEDGPVDYEIPELQD\n7MRX: binding domain of barnase ribonuclease inhibitor\nnative sequence\ngenerated sequence\nMKKAVINGEQIRSISDLHQTLKKELALPEYYGENL\nDALWDALTGWVEYPLVLEWRQFEQSKQLTENGAES\nVLQVFREAKAEGADITIILS\nsequence-only\nstructure-cond.\nSVQVNLDDSSGIKGGGLPDTYKLKQFHFHWGSANDRGSEHTVD\nGEKFPAELHLVHWNTKYDSFAEAASKADGLAVLGFFLKVGAEN\nKELQKITDALKDVKTKGETSFPNFNPSSLLPSDRSAYWRYSGS\nLTTPPCSESVTWTVFKDSVEVSQSQLDAFTSLLGENKRPAQPL\nNDRPVVASFRPSV\n5YUI: binding site of carbonic anhydrase metalloenzyme\nnative sequence\ngenerated sequence\nHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP\nLSVSYDQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYR\nLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGK\nAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADF\nTNFDPRGLLPESLDYWTYPGSLTTPPLLECVWIVLKEPISVSS\nEQVLKFRKLNFNGEGEPEELMVDNWRPAQPLKNRQIKASFK\nRFDiffusion\nDPLM\nEvoDiff\nDPLM\nA\nB\nC\nD\nE\npLDDT:  86.84\nmotif-RMSD:  0.61\nTMscore:  0.57\npLDDT:  93.85\nmotif-RMSD:  0.58\nTMscore:  0.73\npLDDT:  87.79\nmotif-RMSD:  0.77\nTMscore:  0.85\nFigure 3. Evaluation of motif-scaffolding. (A) comparison regarding overall success rate and number of solved problems; (B) comparison\nbetween sequence-only approaches (DPLM vs. EvoDiff); (C) comparison between sequence-only vs. structure-conditioned DPLM; and\n(D) comparison between DPLM (structure-conditioned and sequence-only DPLM) vs. RFDffusion; (E) case study for three problems.\ninitially determine the length of a scaffold and fill the scaf-\nfold positions with the mask token; then (2) keep the motif\nfragment fixed during inference, and sample scaffold condi-\ntioned on the motif; and finally use OmegaFold (Wu et al.,\n2022b) to predict the structure of the sampled sequences. A\nscaffold is considered successful when it meets two condi-\ntions: (1) the RMSD between the predicted motif structure\nand the ground truth, referred to as motif-RMSD < 1 ˚A;\nand (2) the structure should have an overall pLDDT > 70.\nOverall, we examine 17 motif-scaffolding problems, and for\neach problem, we sample 100 sequences and then calculate\nthe success rate according to the above criterion.\nDPLM can generate reasonable scaffolds for the given\nfunctional motifs. As shown in Fig. 3, we find that DPLM\noutperforms EvoDiff in terms of the number of solved prob-\nlems and the average success rate. Moreover, on the prob-\nlems that both DPLM and EvoDiff can solve, the success\nrate of DPLM is higher than EvoDiff, except 3ixt. This\nindicates that DPLM excels in motif-scaffolding, preserv-\ning the motif structure during scaffold generation. To gain\nmore insights, we compare DPLM with structure condi-\ntioning (see §4.3.2) with state-of-the-art structure designer\nRFDiffusion (Watson et al., 2023). We find that DPLM\nshows better results in 6 problems, especially for 1PRW and\n5YUI). We find that utilizing motif structure helps DPLM\nmake a further improvement on 4 problems compared to\nthe original sequence-only DPLM, while decreasing perfor-\nmance on the other 6 problems. This implies that for some\nspecific motifs, scaffolding in sequence space may be better.\nThe detailed analysis unveiled a common biological property\namong the motifs observed in these two cases. Specifically,\nthe motif sequence displayed a remarkable level of evolu-\nTable 2. Performance comparison between DPLM and differ-\nent baseline approaches on CATH 4.2 and CATH 4.3 datasets.\nDPLM’s results are obtained by argmax decoding (i.e., no sam-\npling). †: benchmarked results are quoted from Gao et al. (2022b).\nModels\nTrainable AAR\nstruct. eval.\nParams.\nscTM pLDDT\nCATH 4.2\n†StructTrans (Ingraham et al., 2019)\n1.6M\/1.6M 35.82\n-\n-\n†GVP (Jing et al., 2020)\n1.0M\/1.0M 39.47\n-\n-\n†ProteinMPNN (Dauparas et al., 2022)\n1.9M\/1.9M 45.96\n-\n-\nPiFold (Gao et al., 2022b)\n6.6M\/6.6M 51.66\n-\n-\nProteinMPNN + CMLM\n1.9M\/1.9M 48.62 0.87\n74.07\nLM-DESIGN (w\/ ProtMPNN encoder) 5.0M\/650M 54.41 0.88\n77.07\nDPLM (w\/ ProtMPNN encoder)\n5.0M\/650M 54.54 0.88\n77.12\nCATH 4.3\nPiFold (Gao et al., 2022b)\n6.6M\/6.6M 51.66\n-\n-\nGVP-Transformer (Hsu et al., 2022)\n142M\/142M 51.60\n-\n-\nLM-DESIGN (w\/ GVP-Trans encoder) 6.3M\/650M 56.49 0.85\n74.89\nDPLM-150M (w\/ GVPTrans encoder) 3.1M\/150M 53.27 0.85\n75.31\nDPLM-650M (w\/ GVPTrans encoder) 6.3M\/650M 56.61 0.86\n76.78\nDPLM-3B (w\/ GVPTrans encoder)\n68.2M\/3.0B 59.44 0.86\n77.12\ntionary conservation, playing pivotal roles in binding critical\nsignal passengers (1PRW: calmodulin EF hand for calcium\nbinding and 5YUI: carbonic anhydrase II for CO2 binding).\nNotably, the motif structures predominantly comprised flex-\nible loops. Conversely, 5TPN, 6VW1, and 2KL8, which ex-\nhibited a distinct advantage in motif scaffolding as indicated\nby the RFdiffusion, featured rigid helical structures that\nlacked functional evolutionary conservations. This intrigu-\ning phenomenon suggests that DPLM holds great promise\nas a superior method for constructing structurally flexible\nyet evolutionarily conserved functional motif scaffolding.\n4.3.2\nSTRUCTURE-CONDITIONED: INVERSE FOLDING\nThe goal of inverse folding is to find an amino acid se-\nquence that can fold to a given protein backbone structure.\nWe follow LM-DESIGN (Zheng et al., 2023b) to implant\n8\nDiffusion Language Models Are Versatile Protein Learners\ntemplate: 3F4M\n6 helices\ntemplate: 5CW9\nS<--H<--S<--H<--S\nS->-H->-S->-H->-S\nsecondary structure-guided sampling\nFigure 4. Secondary structure guided conditional sampling. The\nfirst case contains 6 alpha-helices, The second case is much more\ncomplicated as a globally twisted structure with interleaved alpha-\nhelices and beta-strands, where the N-terminus and C-terminus are\nstructurally contiguous.\na structural adapter into the last network layer of DPLM,\nand use GVP-Transformer Encoder (Hsu et al., 2022) as\nthe expert protein backbone structure encoder. We assess\nDPLM on CATH 4.2 and 4.3 (Orengo et al., 1997). We\nuse amino acid recovery (AAR) for sequence evaluation,\nwhilst for structure evaluation, we first predict the structure\nof the generated sequence using ESMFold, then calculate\nthe pLDDT score and self-consistency TM-score (scTM)\nbetween predicted structure and the input one.\nDPLM yields sequences that can confidently fold into the\ngiven backbone structure. As shown in Tab. 2, DPLM can\noutperform or be on par with our strong baselines, including\nthe state-of-the-art approach LM-DESIGN (Zheng et al.,\n2023b), manifesting in AAR, and most importantly, decent\nperformance regarding structure evaluation (scTM = 0.85\nand pLDDT > 76). We suggest this derives from the well-\nlearned protein sequence knowledge of DPLM. When given\nstructure backbone information, DPLM can leverage this\nadvantage and generate the sequence whose structure is both\nplausible and similar to the reference.\n4.3.3\nCONTROLLABLE GENERATION: SECONDARY\nSTRUCTURE GUIDED PROTEIN SAMPLING\nClassifier guidance is preferred for its flexible control over\nthe generation process without retraining for each new con-\ndition, especially beneficial in scenarios with too limited\nlabeled data to directly attain conditional models. Here we\nshowcase how to guide DPLM to generate proteins satis-\nfying desired secondary structures. We train a secondary\nstructure prediction (SSP) model as a sequence labeling task\non TAPE dataset. We then integrate this SSP discriminative\nmodel into DPLM to provide guiding signals.\nDPLM enjoys plug-and-play programmability. Fig. 4\nshowcases that the proposed discrete classifier guidance\nhelps steer a pre-trained DPLM to generate samples satis-\nfying provided secondary structure annotations extracted\nfrom template natural proteins. These findings suggest that\nDPLM is highly programmable, and its full potential of\ngenerative capabilities can be realized in a plug-and-play\nfashion, indicating that DPLM preserves the appealing char-\nacteristic of controllable generation inherent in diffusion\nmodels, but for discrete data. This flexibility to swiftly\nadapt to the evolving needs of users across a broad spectrum\nof preferences is also significant in practical applications\nwith time and computational paramount.\n5\nDiscussions\nIn this paper, we introduce diffusion protein LM (DPLM),\na versatile protein LM that is capable of both protein se-\nquence generation and representation learning. We further\ndevelop several conditioning strategies for various needs\nof conditional generation, including sequence conditioning,\ncross-modal conditioning, and programmable generation\nwith plug-and-play discrete classifier guidance.\nDespite these promising results, there remain several limita-\ntions and future work directions deserving to be explored.\n(i) Exploring DPLM’s conditional generation for wider\napplications. We can further extend the cross-modal\nconditioning strategy of DPLM to more diverse modal-\nities as conditioners, including MSA-conditioned\nhomologous sequence generation, small molecule-\nconditioned binder design for ligands,\nantigen-\nconditioned antibody CDR design, among others. Also,\nthe inclusion of demonstrations featuring plug-and-\nplay classifier-guided controllable generation is essen-\ntial for more scenarios toward diverse user preferences,\ne.g., structural symmetry, superfamily, binding affinity,\nthermostability, fluorescence, and beyond.\n(ii) DPLM can further benefit from best practices of\ncutting-edge technical advancement in the vastness\nof large language models (LLMs). For example, (1)\nlong context extension (Chen et al., 2023b) can rapidly\nadapt DPLM to handle very long proteins beyond its\ntraining length limit, and offering potential for mod-\neling exceptionally long biological sequences such\nas DNAs and RNAs, unifying and deciphering the\nlanguages associated with the central dogma of life;\n(2) fine-tuning DPLM with human feedback or even\nwet-lab experimental feedback, leveraging reinforce-\nment learning (RL; Ouyang et al., 2022), direct pref-\nerence optimization (DPO; Rafailov et al., 2024), and\nself-play fine-tuning (Chen et al., 2024b); (3) elicit-\ning instruction-following and in-context learning (Wei\net al., 2022a) analogs for protein LMs can also be\na promising direction would fully harness DPLM’s\nlearned knowledge.\n(iii) It is imperative to integrate protein structure mod-\neling into DPLM. The advance of protein structure\nmodeling manifest tremendous success, including Al-\nphaFold (Jumper et al., 2021), ESMFold (Lin et al.,\n2022) for structure prediction, RFDIffusion (Watson\net al., 2023), Chroma (Ingraham et al., 2023) for struc-\nture design, and even full-atom molecular modeling,\n9\nDiffusion Language Models Are Versatile Protein Learners\ne.g., the latest generation of AlphaFold (DeepMind,\n2023) and RF-AA (Krishna et al., 2023). Develop-\ning a universal protein language model with the next-\ngeneration DPLM, which accounts for both sequence\nand structure, is a particularly promising avenue.\nWe leave these exciting directions as future work.\nImpact Statement\nOur work on protein generation and representation learning\ncan be used in developing potent therapeutic macromole-\ncues such as antibodies and accelerate the research process\nof drug discovery. Our method may be adapted to other\nscenarios of computer-aided design, such as small molecule\ndesign, material design, and chip design. It is also needed to\nensure the responsible use of our method and refrain from\nusing it for harmful purposes.\nAcknowledgements\nWe thank anonymous reviewers for their insightful feedback.\nWe would like to especially thank Dr. Hang Li for insightful\ndiscussions on the project and feedback on the manuscript\nthat help shape this study. We thank Yi Zhou, Jing Yuan, Dr.\nYilai Li and Jiasheng Ye for their valuable comments.\nReferences\nAlamdari, S., Thakkar, N., van den Berg, R., Lu, A. X., Fusi,\nN., Amini, A. P., and Yang, K. K. Protein generation with\nevolutionary diffusion: sequence is all you need. bioRxiv,\npp. 2023–09, 2023.\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den\nBerg, R. Structured denoising diffusion models in dis-\ncrete state-spaces. In Advances in Neural Information\nProcessing Systems, volume 34, pp. 17981–17993, 2021.\nBengio, Y., Ducharme, R., and Vincent, P. A neural proba-\nbilistic language model. Advances in neural information\nprocessing systems, 13, 2000.\nBerman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat,\nT. N., Weissig, H., Shindyalov, I. N., and Bourne, P. E.\nThe protein data bank. Nucleic acids research, 28(1):\n235–242, 2000.\nBrandes, N., Ofer, D., Peleg, Y., Rappoport, N., and Linial,\nM. Proteinbert: a universal deep-learning model of pro-\ntein sequence and function. Bioinformatics, 38(8):2102–\n2110, 2022.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nvolume 33, pp. 1877–1901, 2020.\nChen, J., Zhang, A., Li, M., Smola, A., and Yang, D. A\ncheaper and better diffusion language model with soft-\nmasked noise. arXiv preprint arXiv:2304.04746, 2023a.\nChen, S., Wong, S., Chen, L., and Tian, Y. Extending\ncontext window of large language models via positional\ninterpolation. arXiv preprint arXiv:2306.15595, 2023b.\nChen, X., Liu, Z., Xie, S., and He, K. Deconstructing\ndenoising diffusion models for self-supervised learning.\narXiv preprint arXiv:2401.14404, 2024a.\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play\nfine-tuning converts weak language models to strong lan-\nguage models. arXiv preprint arXiv:2401.01335, 2024b.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P.\nVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\nURL https:\/\/lmsys.org\/blog\/\n2023-03-30-vicuna\/.\nDallago, C., Mou, J., Johnston, K. E., Wittmann, B. J., Bhat-\ntacharya, N., Goldman, S., Madani, A., and Yang, K. K.\nFlip: Benchmark tasks in fitness landscape inference for\nproteins. bioRxiv, pp. 2021–11, 2021.\nDauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte,\nR. J., Milles, L. F., Wicky, B. I., Courbet, A., de Haas,\nR. J., Bethel, N., et al. Robust deep learning–based pro-\ntein sequence design using proteinmpnn. Science, 378\n(6615):49–56, 2022.\nDauparas, J., Lee, G. R., Pecoraro, R., An, L., Anishchenko,\nI., Glasscock, C., and Baker, D.\nAtomic context-\nconditioned protein sequence design using ligandmpnn.\nBiorxiv, pp. 2023–12, 2023.\nDe Bortoli, V., Mathieu, E., Hutchinson, M., Thornton, J.,\nTeh, Y. W., and Doucet, A. Riemannian score-based\ngenerative modelling. Advances in Neural Information\nProcessing Systems, 35:2406–2422, 2022.\nDeepMind, G. Performance and structural coverage of the\nlatest, in-development alphafold model. 2023.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp.\n4171–4186, Minneapolis, Minnesota, June 2019. Asso-\nciation for Computational Linguistics. doi: 10.18653\/\nv1\/N19-1423. URL https:\/\/www.aclweb.org\/\nanthology\/N19-1423.\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis.\nAdvances in neural information\nprocessing systems, 34:8780–8794, 2021a.\n10\nDiffusion Language Models Are Versatile Protein Learners\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis.\nAdvances in neural information\nprocessing systems, 34:8780–8794, 2021b.\nDieleman, S., Sartran, L., Roshannai, A., Savinov, N.,\nGanin, Y., Richemond, P. H., Doucet, A., Strudel, R.,\nDyer, C., Durkan, C., et al. Continuous diffusion for\ncategorical data. arXiv preprint arXiv:2211.15089, 2022.\nElnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G.,\nWang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C.,\nSteinegger, M., et al. Prottrans: Toward understanding the\nlanguage of life through self-supervised learning. IEEE\ntransactions on pattern analysis and machine intelligence,\n44(10):7112–7127, 2021.\nFerruz, N. and H¨ocker, B. Controllable protein design with\nlanguage models. Nature Machine Intelligence, 4(6):\n521–532, 2022.\nFerruz, N., Schmidt, S., and H¨ocker, B. Protgpt2 is a deep\nunsupervised language model for protein design. Nature\ncommunications, 13(1):4348, 2022.\nFu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Spe-\ncializing smaller language models towards multi-step rea-\nsoning. arXiv preprint arXiv:2301.12726, 2023.\nGao, Z., Guo, J., Tan, X., Zhu, Y., Zhang, F., Bian, J.,\nand Xu, L. Difformer: Empowering diffusion model\non embedding space for text generation. arXiv preprint\narXiv:2212.09412, 2022a.\nGao, Z., Tan, C., and Li, S. Z.\nPifold: Toward effec-\ntive and efficient protein inverse folding. arXiv preprint\narXiv:2209.12643, 2022b.\nGhazvininejad, M., Levy, O., Liu, Y., and Zettlemoyer, L.\nMask-predict: Parallel decoding of conditional masked\nlanguage models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pp.\n6112–6121, Hong Kong, China, November 2019. As-\nsociation for Computational Linguistics. doi: 10.18653\/\nv1\/D19-1633. URL https:\/\/www.aclweb.org\/\nanthology\/D19-1633.\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:\nSequence to sequence text generation with diffusion mod-\nels. arXiv preprint arXiv:2210.08933, 2022.\nGu, J. and Kong, X. Fully non-autoregressive neural ma-\nchine translation: Tricks of the trade. In Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP\n2021, pp. 120–133, 2021.\nGu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R.\nNon-autoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations, 2018.\nGuo, J., Tan, X., Xu, L., Qin, T., Chen, E., and Liu,\nT.-Y.\nFine-tuning by curriculum learning for non-\nautoregressive neural machine translation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 34, pp. 7839–7846, 2020a.\nGuo, J., Zhang, Z., Xu, L., Wei, H.-R., Chen, B., and Chen,\nE. Incorporating bert into parallel sequence decoding with\nadapters. Advances in Neural Information Processing\nSystems, 33:10843–10854, 2020b.\nHan, X., Kumar, S., and Tsvetkov, Y.\nSsd-lm: Semi-\nautoregressive simplex-based diffusion language model\nfor text generation and modular control. arXiv preprint\narXiv:2210.17432, 2022.\nHe, L., Zhang, S., Wu, L., Xia, H., Ju, F., Zhang, H., Liu,\nS., Xia, Y., Zhu, J., Deng, P., et al.\nPre-training co-\nevolutionary protein representation via a pairwise masked\nlanguage model. arXiv preprint arXiv:2110.15527, 2021.\nHe, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-\nsionbert: Improving generative masked language models\nwith diffusion models. 2023.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\nIn NeurIPS 2021 Workshop on Deep Generative Models\nand Downstream Applications, 2021.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in Neural Information Process-\ning Systems, 33:6840–6851, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\net al. Imagen video: High definition video generation\nwith diffusion models. arXiv preprint arXiv:2210.02303,\n2022.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\nHoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B.,\nvan den Berg, R., and Salimans, T. Autoregressive dif-\nfusion models. In International Conference on Learning\nRepresentations, 2021a.\nHoogeboom, E., Nielsen, D., Jaini, P., Forr´e, P., and Welling,\nM. Argmax flows and multinomial diffusion: Learning\ncategorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454–12465, 2021b.\n11\nDiffusion Language Models Are Versatile Protein Learners\nHoogeboom, E., Satorras, V. G., Vignac, C., and Welling,\nM. Equivariant diffusion for molecule generation in 3d.\nIn International Conference on Machine Learning, pp.\n8867–8887. PMLR, 2022.\nHsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B., Sercu, T., Lerer,\nA., and Rives, A. Learning inverse folding from millions\nof predicted structures. In Chaudhuri, K., Jegelka, S.,\nSong, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.),\nProceedings of the 39th International Conference on Ma-\nchine Learning, volume 162 of Proceedings of Machine\nLearning Research, pp. 8946–8970. PMLR, 17–23 Jul\n2022. URL https:\/\/proceedings.mlr.press\/\nv162\/hsu22a.html.\nHu, M., Yuan, F., Yang, K. K., Ju, F., Su, J., Wang, H.,\nYang, F., and Ding, Q. Exploring evolution-aware &-free\nprotein language models as protein function predictors.\nIn Advances in Neural Information Processing Systems,\n2022.\nHuang, F., Ke, P., and Huang, M. Directed acyclic trans-\nformer pre-training for high-quality non-autoregressive\ntext generation. Transactions of the Association for Com-\nputational Linguistics, 2023.\nIngraham, J., Garg, V., Barzilay, R., and Jaakkola, T. Gener-\native models for graph-based protein design. In Advances\nin neural information processing systems, 2019.\nIngraham, J. B., Baranov, M., Costello, Z., Barber, K. W.,\nWang, W., Ismail, A., Frappier, V., Lord, D. M., Ng-\nThow-Hing, C., Van Vlack, E. R., et al. Illuminating\nprotein space with a programmable generative model.\nNature, pp. 1–9, 2023.\nJing, B., Eismann, S., Suriana, P., Townshend, R. J. L., and\nDror, R. Learning from protein structure with geomet-\nric vector perceptrons. In International Conference on\nLearning Representations, 2020.\nJohnson, S. R., Monaco, S., Massie, K., and Syed, Z. Gen-\nerating novel protein sequences using gibbs sampling of\nmasked language models. bioRxiv, pp. 2021–01, 2021.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M.,\nRonneberger, O., Tunyasuvunakool, K., Bates, R., ˇZ´ıdek,\nA., Potapenko, A., et al. Highly accurate protein structure\nprediction with alphafold. Nature, 596(7873):583–589,\n2021.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKim, J., Kong, J., and Son, J. Conditional variational au-\ntoencoder with adversarial learning for end-to-end text-to-\nspeech. In International Conference on Machine Learn-\ning, pp. 5530–5540. PMLR, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. Large language models are zero-shot reasoners. Ad-\nvances in neural information processing systems, 35:\n22199–22213, 2022.\nKrishna, R., Wang, J., Ahern, W., Sturmfels, P., Venkatesh,\nP., Kalvet, I., Lee, G. R., Morey-Burrows, F. S., An-\nishchenko, I., Humphreys, I. R., et al.\nGeneralized\nbiomolecular modeling and design with rosettafold all-\natom. bioRxiv, pp. 2023–10, 2023.\nLee, J. S., Kim, J., and Kim, P. M. Proteinsgm: Score-based\ngenerative modeling for de novo protein design. bioRxiv,\npp. 2022–07, 2022.\nLi, X. L., Thickstun, J., Gulrajani, I., Liang, P., and\nHashimoto, T. Diffusion-lm improves controllable text\ngeneration. In Advances in Neural Information Process-\ning Systems, volume abs\/2205.14217, 2022.\nLin, Y. and AlQuraishi, M. Generating novel, designable,\nand diverse protein structures by equivariantly diffusing\noriented residue clouds. arXiv preprint arXiv:2301.12485,\n2023.\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., dos\nSantos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido,\nS., et al. Language models of protein sequences at the\nscale of evolution enable accurate structure prediction.\nBioRxiv, 2022.\nLisanza, S. L., Gershon, J. M., Tipps, S. W. K., Arnoldt, L.,\nHendel, S., Sims, J. N., Li, X., and Baker, D. Joint gener-\nation of protein sequence and structure with rosettafold\nsequence space diffusion. bioRxiv, pp. 2023–05, 2023.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nLu, A. X., Zhang, H., Ghassemi, M., and Moses, A. Self-\nsupervised contrastive learning of protein representations\nby mutual information maximization. BioRxiv, pp. 2020–\n09, 2020.\nMadani, A., Krause, B., Greene, E. R., Subramanian, S.,\nMohr, B. P., Holton, J. M., Olmos Jr, J. L., Xiong, C.,\nSun, Z. Z., Socher, R., et al. Deep neural language model-\ning enables functional protein generation across families.\nbioRxiv, pp. 2021–07, 2021.\n12\nDiffusion Language Models Are Versatile Protein Learners\nMcDermott, M., Yap, B., Hsu, H., Jin, D., and Szolovits, P.\nAdversarial contrastive pre-training for protein sequences.\narXiv preprint arXiv:2102.00466, 2021.\nMeier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., and Rives,\nA. Language models enable zero-shot prediction of the\neffects of mutations on protein function. In Advances\nin Neural Information Processing Systems, pp. 29287–\n29303, 2021.\nMelnyk, I., Chenthamarakshan, V., Chen, P.-Y., Das, P.,\nDhurandhar, A., Padhi, I., and Das, D. Reprogramming\nlarge pretrained language models for antibody sequence\ninfilling. arXiv preprint arXiv:2210.07144, 2022.\nMeshchaninov, V., Strashnov, P., Shevtsov, A., Nikolaev,\nF., Ivanisenko, N., Kardymon, O., and Vetrov, D. Diffu-\nsion on language model embeddings for protein sequence\ngeneration. arXiv preprint arXiv:2403.03726, 2024.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient\nestimation of word representations in vector space. In\nBengio, Y. and LeCun, Y. (eds.), 1st International Confer-\nence on Learning Representations, ICLR 2013, Scottsdale,\nArizona, USA, May 2-4, 2013, Workshop Track Proceed-\nings, 2013. URL http:\/\/arxiv.org\/abs\/1301.\n3781.\nMin, S., Park, S., Kim, S., Choi, H.-S., Lee, B., and Yoon,\nS. Pre-training of deep bidirectional protein sequence\nrepresentations with structural information. IEEE Access,\n9:123912–123926, 2021.\nMuennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus,\nA., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. Scal-\ning data-constrained language models. arXiv preprint\narXiv:2305.16264, 2023.\nNambiar, A., Heflin, M., Liu, S., Maslov, S., Hopkins, M.,\nand Ritz, A. Transforming the language of life: trans-\nformer neural networks for protein prediction tasks. In\nProceedings of the 11th ACM international conference\non bioinformatics, computational biology and health in-\nformatics, pp. 1–8, 2020.\nNijkamp, E., Ruffolo, J., Weinstein, E. N., Naik, N., and\nMadani, A. Progen2: exploring the boundaries of pro-\ntein language models. arXiv preprint arXiv:2206.13517,\n2022.\nNourani, E., Asgari, E., McHardy, A. C., and Mofrad,\nM. R. Tripletprot: deep representation learning of pro-\nteins based on siamese networks. IEEE\/ACM Transac-\ntions on Computational Biology and Bioinformatics, 19\n(6):3744–3753, 2021.\nOpenAI. Gpt-4 technical report, 2023.\nOrengo, C. A., Michie, A. D., Jones, S., Jones, D. T.,\nSwindells, M. B., and Thornton, J. M. Cath–a hierar-\nchic classification of protein domain structures. Structure,\n5(8):1093–1109, 1997.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pp. 2227–2237, New\nOrleans, Louisiana, June 2018. Association for Compu-\ntational Linguistics. doi: 10.18653\/v1\/N18-1202. URL\nhttps:\/\/aclanthology.org\/N18-1202.\nQian, L., Zhou, H., Bao, Y., Wang, M., Qiu, L., Zhang,\nW., Yu, Y., and Li, L. Glancing transformer for non-\nautoregressive neural machine translation. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Process-\ning (Volume 1: Long Papers), pp. 1993–2003, Online,\nAugust 2021a. Association for Computational Linguis-\ntics. doi: 10.18653\/v1\/2021.acl-long.155. URL https:\n\/\/aclanthology.org\/2021.acl-long.155.\nQian, L., Zhou, Y., Zheng, Z., Zhu, Y., Lin, Z., Feng, J.,\nCheng, S., Li, L., Wang, M., and Zhou, H. The volctrans\nglat system: Non-autoregressive translation meets wmt21.\nWMT 2021, pp. 187, 2021b.\nQian, L., Wang, M., Liu, Y., and Zhou, H. Diff-glat: Dif-\nfusion glancing transformer for parallel sequence to se-\nquence learning. arXiv preprint arXiv:2212.10240, 2022.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, P.,\nCanny, J., Abbeel, P., and Song, Y. Evaluating protein\n13\nDiffusion Language Models Are Versatile Protein Learners\ntransfer learning with tape. Advances in neural informa-\ntion processing systems, 32, 2019.\nRao, R. M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel,\nP., Sercu, T., and Rives, A. Msa transformer. In Interna-\ntional Conference on Machine Learning, pp. 8844–8856.\nPMLR, 2021.\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J.,\nGuo, D., Ott, M., Zitnick, C. L., Ma, J., and Fergus, R. Bi-\nological structure and function emerge from scaling unsu-\npervised learning to 250 million protein sequences. PNAS,\n2019.\ndoi: 10.1101\/622803.\nURL https:\/\/www.\nbiorxiv.org\/content\/10.1101\/622803v4.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models, 2021.\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Le Scao, T., Raja,\nA., et al. Multitask prompted training enables zero-shot\ntask generalization. In ICLR 2022-Tenth International\nConference on Learning Representations, 2022.\nSavinov, N., Chung, J., Binkowski, M., Elsen, E., and\nvan den Oord, A. Step-unrolled denoising autoencoders\nfor text generation. In International Conference on Learn-\ning Representations, 2021.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequilib-\nrium thermodynamics. In Bach, F. and Blei, D. (eds.), In-\nternational Conference on Machine Learning, volume 37\nof Proceedings of Machine Learning Research, pp. 2256–\n2265, Lille, France, 07–09 Jul 2015. PMLR, PMLR.\nURL https:\/\/proceedings.mlr.press\/v37\/\nsohl-dickstein15.html.\nSong, Y. and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. Advances in Neural\nInformation Processing Systems, 32, 2019.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. In International\nConference on Learning Representations, 2020.\nStrodthoff, N., Wagner, P., Wenzel, M., and Samek, W.\nUdsmprot: universal deep sequence models for protein\nclassification. Bioinformatics, 36(8):2401–2409, 2020.\nSturmfels, P., Vig, J., Madani, A., and Rajani, N. F. Profile\nprediction: An alignment-based pre-training task for pro-\ntein sequence models. arXiv preprint arXiv:2012.00195,\n2020.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu,\nY. Roformer: Enhanced transformer with rotary position\nembedding. arXiv preprint arXiv:2104.09864, 2021.\nSu, J., Han, C., Zhou, Y., Shan, J., Zhou, X., and Yuan, F.\nSaprot: Protein language modeling with structure-aware\nvocabulary. bioRxiv, pp. 2023–10, 2023.\nSun, T. and Qiu, X. Moss. https:\/\/github.com\/\nOpenLMLab\/MOSS, 2023.\nSutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-\nquence learning with neural networks. In Ghahramani, Z.,\nWelling, M., Cortes, C., Lawrence, N. D., and Weinberger,\nK. Q. (eds.), Advances in Neural Information Processing\nSystems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal,\nQuebec, Canada, volume 27, pp. 3104–3112, 2014.\nSuzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu,\nC. H., and Consortium, U. Uniref clusters: a comprehen-\nsive and scalable alternative for improving sequence sim-\nilarity searches. Bioinformatics, 31(6):926–932, 2015.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\nStanford\nalpaca:\nAn\ninstruction-following\nllama\nmodel.\nhttps:\/\/github.com\/tatsu-lab\/\nstanford_alpaca, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nTrippe, B. L., Yim, J., Tischer, D., Baker, D., Broderick, T.,\nBarzilay, R., and Jaakkola, T. Diffusion probabilistic mod-\neling of protein backbones in 3d for the motif-scaffolding\nproblem. arXiv preprint arXiv:2206.04119, 2022.\nUnsal, S., Atas, H., Albayrak, M., Turhan, K., Acar, A. C.,\nand Do˘gan, T. Learning functional properties of proteins\nwith language models. Nature Machine Intelligence, 4\n(3):227–245, 2022.\nvan Kempen, M., Kim, S. S., Tumescheit, C., Mirdita, M.,\nLee, J., Gilchrist, C. L., S¨oding, J., and Steinegger, M.\nFast and accurate protein structure search with foldseek.\nNature Biotechnology, pp. 1–4, 2023.\n14\nDiffusion Language Models Are Versatile Protein Learners\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, volume 30, pp. 5998–6008,\n2017.\nVerkuil, R., Kabeli, O., Du, Y., Wicky, B. I., Milles, L. F.,\nDauparas, J., Baker, D., Ovchinnikov, S., Sercu, T., and\nRives, A. Language models generalize beyond natural\nproteins. bioRxiv, pp. 2022–12, 2022.\nVignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher,\nV., and Frossard, P. Digress: Discrete denoising diffu-\nsion for graph generation. In The Eleventh International\nConference on Learning Representations, 2022.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol,\nP.-A., and Bottou, L. Stacked denoising autoencoders:\nLearning useful representations in a deep network with\na local denoising criterion. Journal of machine learning\nresearch, 11(12), 2010.\nWang, A. and Cho, K. BERT has a mouth, and it must\nspeak: BERT as a Markov random field language model.\nIn Proceedings of the Workshop on Methods for Opti-\nmizing and Evaluating Neural Language Generation,\npp. 30–36, Minneapolis, Minnesota, June 2019. Asso-\nciation for Computational Linguistics. doi: 10.18653\/\nv1\/W19-2304. URL https:\/\/www.aclweb.org\/\nanthology\/W19-2304.\nWang, Y., He, S., Chen, G., Chen, Y., and Jiang, D. Xlm-\nd: Decorate cross-lingual pre-training model as non-\nautoregressive neural machine translation. In Proceedings\nof the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 6934–6946, 2022.\nWatson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L.,\nYim, J., Eisenach, H. E., Ahern, W., Borst, A. J., Ragotte,\nR. J., Milles, L. F., et al. De novo design of protein struc-\nture and function with rfdiffusion. Nature, 620(7976):\n1089–1100, 2023.\nWei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\nguage models are zero-shot learners. In International\nConference on Learning Representations, 2021.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\nTransactions on Machine Learning Research, 2022a.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E. H., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nIn Advances in Neural Information Processing Systems,\nvolume 35, pp. 24824–24837, 2022b.\nWettig, A., Gao, T., Zhong, Z., and Chen, D. Should you\nmask 15% in masked language modeling? In Proceedings\nof the 17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pp. 2985–3000,\n2023.\nWu, K. E., Yang, K. K., Berg, R. v. d., Zou, J. Y., Lu, A. X.,\nand Amini, A. P. Protein structure generation via folding\ndiffusion. arXiv preprint arXiv:2209.15611, 2022a.\nWu, R., Ding, F., Wang, R., Shen, R., Zhang, X., Luo,\nS., Su, C., Wu, Z., Xie, Q., Berger, B., et al. High-\nresolution de novo structure prediction from primary se-\nquence. BioRxiv, pp. 2022–07, 2022b.\nWu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng,\nH.-T., Li, J., Wei, Z., Guo, J., et al. Ar-diffusion: Auto-\nregressive diffusion model for text generation. arXiv\npreprint arXiv:2305.09515, 2023.\nXiao, Y., Qiu, J., Li, Z., Hsieh, C.-Y., and Tang, J. Modeling\nprotein using large-scale pretrain language model. arXiv\npreprint arXiv:2108.07435, 2021.\nXu, M., Zhang, Z., Lu, J., Zhu, Z., Zhang, Y., Chang, M.,\nLiu, R., and Tang, J. Peer: a comprehensive and multi-\ntask benchmark for protein sequence understanding. Ad-\nvances in Neural Information Processing Systems, 35:\n35156–35173, 2022.\nYang, K. K., Wu, Z., and Arnold, F. H. Machine-learning-\nguided directed evolution for protein engineering. Nature\nmethods, 16(8):687–694, 2019.\nYang, K. K., Lu, A. X., and Fusi, N. Convolutions are\ncompetitive with transformers for protein sequence pre-\ntraining. bioRxiv, pp. 2022–05, 2022a.\nYang, K. K., Zanichelli, N., and Yeh, H. Masked inverse\nfolding with sequence transfer for protein representation\nlearning. bioRxiv, pp. 2022–05, 2022b.\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Gu, Q. Diffusion\nlanguage models can perform many tasks with scaling and\ninstruction-finetuning. arXiv preprint arXiv:2308.12219,\n2023a.\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser:\nDiffused conditional sequence learning by manipulating\nnoises. arXiv preprint arXiv:2302.10025, 2023b.\n15\nDiffusion Language Models Are Versatile Protein Learners\nYi, K., Zhou, B., Shen, Y., Lio, P., and Wang, Y. G. Graph\ndenoising diffusion for inverse protein folding. In Thirty-\nseventh Conference on Neural Information Processing\nSystems, 2023. URL https:\/\/openreview.net\/\nforum?id=u4YXKKG5dX.\nYim, J., Trippe, B. L., De Bortoli, V., Mathieu, E., Doucet,\nA., Barzilay, R., and Jaakkola, T. Se (3) diffusion model\nwith application to protein backbone generation. arXiv\npreprint arXiv:2302.02277, 2023.\nYuan, H., Yuan, Z., Tan, C., Huang, F., and Huang, S. Seqdif-\nfuseq: Text diffusion with encoder-decoder transformers.\narXiv preprint arXiv:2212.10325, 2022.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b:\nAn open bilingual pre-trained model.\narXiv preprint\narXiv:2210.02414, 2022.\nZheng, L., Yuan, J., Yu, L., and Kong, L. A reparameter-\nized discrete diffusion model for text generation. arXiv\npreprint arXiv:2302.05737, 2023a.\nZheng, Z., Deng, Y., Xue, D., Zhou, Y., YE, F., and Gu, Q.\nStructure-informed language models are protein design-\ners. In International Conference on Machine Learning,\n2023b.\n16\nDiffusion Language Models Are Versatile Protein Learners\nA\nReparameterizaed Discrete Diffusion\nModels (RDM)\nDPLM uses reparameterized discrete diffusion model\n(RDM) as its discrete diffusion framework (Zheng et al.,\n2023a). Here we briefly summarize its basic training and\nsampling. Please refer to Zheng et al. (2023a) for more\ndetails.\nZheng et al. (2023a) shows that the backward transition of\ndiscrete diffusion models q(x(t−1)|x(t), x(0)) can be rewrit-\nten as\nq(x(t−1)|x(t), x(0))\n=\n(\nλ(1)\nt−1x(t) + (1 −λ(1)\nt−1)qnoise,\nif x(t) = x(0)\nλ(2)\nt−1x(0) + (1 −λ(2)\nt−1)qnoise(x(t)),\nif x(t) ̸= x(0)\nwhere qnoise(x(t)) = βtx(t) + (1 −βt)qnoise, and both λ(1)\nt−1\nand λ(2)\nt−1 are constants relating to βt and βt−1. This re-\nformulation interprets the backward transition as a mixture\ndistribution. Sampling from it is equivalent to first sampling\nfrom a Bernoulli distribution and then the corresponding\ncomponent distribution, i.e.,\nv(1)\nt−1 ∼Bernoulli\n\u0010\nλ(1)\nt−1\n\u0011\n,u(1)\nt\n∼Cat (u; p = qnoise) ,\nv(2)\nt−1 ∼Bernoulli\n\u0010\nλ(2)\nt−1\n\u0011\n,u(2)\nt\n∼Cat (u; p = qnoise(xt)) ,\nx(t−1) =\n\n\n\n\n\nv(1)\nt−1x(t) +\n\u0010\n1 −v(1)\nt−1\n\u0011\nu(1)\nt ,\nif x(t) = x(0)\nv(2)\nt−1x(0) +\n\u0010\n1 −v(2)\nt−1\n\u0011\nu(2)\nt ,\nif x(t) ̸= x(0) .\nThis reparameterizes the transitions q(x(t−1)|x(t), x(0))\nand pθ(x(t−1)|x(t)) into q(x(t−1), v(t−1)|x(t), x(0)) and\npθ(x(t−1), v(t−1)|x(t)). With this reparameterization, the\ntraining objective of diffusion models (i.e., the variational\nbound of negative log-likelihood) becomes\n−Eq(x1:T , v1:T |x0)\n\u0014\nlog pθ(x0, x1:T , v1:T )\nq(x1:T , v1:T |x0)\n\u0015\n= J1 +\nT\nX\nt=2\nJt + const.,\nwhere J1 = −Eq(x1|x0) [log pθ(x0|x1)] and Zheng et al.\n(2023a) shows that Jt can be simplified into a weighted\ncross-entropy loss.\nSince each token is modeled con-\nditionally independently, so we can consider the back-\nward transition for each token, and sum the losses for\nthem.\nFor i-th position, the backward transition is\nq(x(t−1)\ni\n, v(t−1)\ni\n|x(t)\ni , x(0)\ni ).\nAs shown in Zheng et al.\n(2023a) appendix C, the loss at i-th token can be written as\nJt,i = Eq(v(t−1)\ni\n)\nh\nKL[q(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni , x(0)\ni\n)||pθ(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni )]\ni\nLet bi(t) = 1x(t)\ni\n̸= x(0)\ni , q(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni , x(0)\ni ) can be\nwritten as:\nq(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni , x(0)\ni )\n=\n(\nv(1)\nt−1,ix(t)\ni\n+ (1 −v(1)\nt−1,i)qnoise\nif bi(t) = 0,\nv(2)\nt−1,ix(0)\ni\n+ (1 −v(2)\nt−1,i)qnoise\nif bi(t) = 1,\nAnd pθ(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni ) can be written as:\npθ(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni )\n=\n(\nv(1)\nt−1,ix(t)\ni\n+ (1 −v(1)\nt−1,i)qnoise\nif bi(t) = 0,\nv(2)\nt−1,ipθ(x(0)\ni |x(t)) + (1 −v(2)\nt−1,i)qnoise\nif bi(t) = 1,\nTherefore, the loss at i-th token can be computed by enu-\nmerating all cases with respect to v(t−1)\ni\nand bi(t). As\nnoted in Zheng et al. (2023a), the KL divergence is equal to\n−log pθ(x(0)\ni |x(t)) when v(2)\nt−1,i = 1 and bi(t) = 1, while\nin other cases the KL divergence is 0. So we have:\nJt =\nX\n1≤i≤L\nJt,i\n=\nX\n1≤i≤L\nEq(v(t−1)\ni\n)\n\u0014\nKL[q(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni , x(0)\ni\n)||pθ(x(t−1)\ni\n|v(t−1)\ni\n, x(t)\ni )]\n\u0015\n=\nX\n1≤i≤L\nq(v(t−1)\ni\n= 1) · bi(t) · (−log pθ(x(0)\ni\n|x(t)))\n= −λ(t−1) X\n1≤i≤L\nbi(t) · log pθ(x(0)\ni\n|x(t))\nNotably, training with different noise schedules only differs\nin the weighting of the objective.\nDuring sampling, RDM leverages this observation and pro-\nposes to employ a discriminative approach. Specifically, it\ndenoises a token only when it receives a top-k score (log-\nprobability) from the network where k in each step is de-\ntermined by a denoising schedule. The overall sampling\nprocess is shown in algorithm 1.\nB\nTraining Stratety\nB.1\nPre-training of DPLM\nDuring the training phase, we investigate two different ap-\nproaches: (1) training with the diffusion objective from\nscratch, and (2) what we refer to as two-stage training, which\nconsists of initial training with the masked language model-\ning (MLM) objective followed by continuous training with\nthe diffusion objective.\nEmpirical Observation. In our preliminary experiments,\nwe observed that discrete diffusion pre-training ”from\nscratch (FS)” often yielded instability in the form of frequent\nloss spiking, therefore hurting our model performance.\n17\nDiffusion Language Models Are Versatile Protein Learners\nAlgorithm 1 Sampling from RDM\nInput: trained network fθ (·) and temperature τ.\nOutput: generated sample x(0).\nfor n = 1, 2, . . . , N do\nInitialize xT,n ∼qnoise;\nInitialize bT,n = 0;\nend for\nfor t = T, . . . , 1 do\nfor n = 1, 2, . . . , N do\nDraw ex0,n ∼Categorical (fθ (xt,n)\/τ);\nGenerate vt−1,n according to log p(ex0,n)\nif bt,n = 1 then\nDraw u(1)\nt,n ∼qnoise;\nxt−1,n = v(1)\nt−1,nxt,n +\n\u0010\n1 −v(1)\nt−1,n\n\u0011\nu(1)\nt,n;\nelse\nDraw u(2)\nt,n ∼qnoise(xt,n);\nxt−1,n = v(2)\nt−1,nex0,n +\n\u0010\n1 −v(2)\nt−1,n\n\u0011\nx(2)\nt,n;\nend if\nLet bt−1,n = bt,n ∧v(1)\nt−1,n ∨v(2)\nt−1,n;\nend for\nend for\nReturn x0,1:N.\nOur Hypothesis. We noticed that the absorbing diffusion\nobjective leads to a variable masking ratio ranging from\n0% to 100%, while conventional MLM objective’s masking\nratio keep fixed at 15% such that a masked LM is always\nexposed to rich condition of 85% observation\/context. In\nother words, in contrast to MLM, for absorbing discrete\ndiffusion models like DPLM, in some of the extreme cases\nthere are nearly all tokens getting masked, which means that\nthe model is required to recover all tokens of the ground-\ntruth sequence from nothing). This could impose severe\nlearning challenges, especially at the early phase of pre-\ntraining, where the model has yet not acquired sufficient\ncapability to extract informative features and correlations\nfrom limited observation.\nSolution in Principle. Inspired by the success of curriculum\nlearning in non-autoregressive text generation (Qian et al.,\n2021a; Gu & Kong, 2021; Guo et al., 2020a;b; Wang et al.,\n2022), we suggested that a masking warmup strategy could\nmitigate this issue, where we can start with a small upper\nbound of masking ratio (e.g., 15% as conventional MLM,\nto preserve a high proportion of observation) in the early\nphase of pre-training, and then gradually increase the mask-\ning ratio towards the authentic discrete diffusion objective\nduring pre-training.\nSolution in Practice. In the current form of our manuscript,\nadhering to this principle, we proposed a two-stage training\nmethod: initialized DPLM from an established masked LM,\neither from our in-house pre-trained one or from the official\nESM-2 checkpoint, and then trained DPLM by the discrete\ndiffusion language modeling objective afterwards. Though\nit may or may not lead to the best model performance, it\noffers the possibilities of standing on the shoulders of any\npre-trained masked LM such as ESM2 or the advanced LM\narchitecture such as a Llama\/Mistral-style masked LMs, in\nthe broad open-source AI community. This also enables us\nto bypass the time-consuming process of gradual masking\nwarmup during pre-training. As a result, this can be the\nmost efficient and effective approach in practice, which also\nshares a similar principle as finetuning from RosettaFold in\nRFDiffusion (Watson et al., 2023).\nAlthough discrete diffusion pre-training from scratch is chal-\nlenging, we have also explored several ways to improve\nit. As shown in the Fig. 2G, DPLM can achieve compara-\nble performance through pre-training from scratch. More\nspecifically, we found that (1) gradient norm clipping can\neffectively help stabilize training process of discrete diffu-\nsion language modeling, greatly reducing the chance of loss\nspiking and gradient nan. In addition, we also found that\n(2) training longer (i.e. scaling compute) is another key to\nattain a good ultimate model performance (trained for 300k\nsteps).\nWe are also curious about the performance of the vanilla\nmasking warmup strategy and would like to see if it can\nlead to a better pre-trained DPLM. The training dynamics of\ndiscrete diffusion based DPLM is an interesting and exciting\ndirection deserving further exploration, and we leave these\nas our future work.\nB.2\nPre-training of AR-LM baseline\nWe pretrain a AR-LM using autoregressive training ob-\njective. In order to be comparable with DPLM, the au-\ntoregressive language model we trained adopts the same\narchitecture as DPLM. To be capable of adapting the au-\ntoregressive training, we modify the mask matrix of the\nattention module to causal mask, which guarantees each\ntoken can only attend the previous position and keep unseen\nfor future. The training objective is next word prediction,\nand we process the input sequence with teacher forcing for\nefficient parallel training. During decoding, we start with\n<bos> token, sample one token each timestep from left to\nright, and the sampled token in the current timestep will be\nconcatenated to the end of the sequence, becoming input\nfor next timestep. The decoding process terminates until\n<eos> token is sampled. Because we can not know when\nto obtain the <eos> token in advance, we can not decide\nthe length of sampled sequence. We attempt to force the\nsampling length by modifying the sampling probability: the\nprobability of <eos> is 1 when and only when the sequence\nlength is up to the predefined length, while 0 in all the pre-\nvious timesteps. However, we observe this will decline the\nquality of sampled sequence significantly.\n18\nDiffusion Language Models Are Versatile Protein Learners\nC\nReasons for choosing absorbing discrete\ndiffusion\nWe employed absorbing discrete diffusion as the pre-training\nmethod, instead of other forms of discrete diffusion or latent\ndiffusion, for the following considerations.\nC.1\nRegarding discrete diffusion (DD) definitions\n(multinomial vs absorbing as proposed in D3PM)\nIntuition. We favor absorbing DD since the learning objec-\ntive of absorbing DD generalizes existing language model-\ning objectives, as highlighted in section 4 of Austin et al.\n(2021), In particular, absorbing DD is a natural extension of\nthe masked language modeling (MLM) objective, which has\nbeen thoroughly studied (Wettig et al., 2023) in the field of\nNLP and widely proven to be a robust and effective sequence\nlearning protocol. In contrast, multinomial\/uniform-DD\nresembles (tranfitional) denoising autoencoders (Savinov\net al., 2021). There is little solid evidence and remains\nhighly unclear about (mutlinomial) denoising autoencoder\nas a sequence learning objective can scale up w.r.t data,\nmodel size and application scenarios.\nEmpirical verification. In our preliminary exploration, we\nhave studied the performance of multinomial\/uniform-DD\nand absorbing-DD. Here we provide the result regarding\nunconditional sampling, as shown in Fig. 5. We can find that\nDPLM-absorbing generally manifests better performance\nthan DPLM-multinomial across different lengths.\n200\n400\n600\n800\n1000\nLength\n50\n55\n60\n65\n70\n75\n80\n85\npLDDT\nUR50\nDPLM-Absorbing\nDPLM-Multinomial\nFigure 5. The\nunconditional\nsampling\nperformance\nof\nmultinomial\/uniform-DD and absorbing-DD.\nC.2\nRegarding latent diffusion\nWe reason that latent diffusion for discrete sequence data,\nwhich performs Gaussian diffusion in continuous embed-\nding space, requires additional lossy continuous relaxation\nof discrete sequence data like protein sequence, which does\nnot fit the discrete nature of protein sequence and is not\nnecessarily the best choice for modeling discrete sequence\ndata. Recent study (Meshchaninov et al., 2024) presents\na latent diffusion model on protein LM embedding, where\nthe pLDDT score unconditional sampling attains reasonable\npLDDT in their paper, while our discrete diffusion based\napproach still excels.\nD\nAdditional Experimental Details\nD.1\nA modified unconditional sampling strategy\nThe sampling algorithm proposed in the Zheng et al. (2023a)\nis to unmask positions with top-k prediction score (log-\nprobability) predicted by pθ(x(0)|x(t)), and mask all the\nrest position in each denoising step. However, we find that\nif we use this sampling algorithm to sample sequence un-\nconditionally, the sampled sequence will collapse to trivial\npattern, such as repeating with a single amino acid. We\nsuggest this is because, without any additional conditions,\nthe model initially tends to give a higher prediction score\nto the amino acids that appear frequently in the training\nset. Subsequently, based on these high-frequency amino\nacid tokens, the model will continue to sample the same\ntokens beside these tokens with high confidence. The other\namino acid can also be sampled, but possibly with a lower\nprediction score, thereby leading to be dropped according\nto the top-k sampling algorithm. Then, this amino acid will\nspread throughout the entire sequence like a virus, forming\na sequence composed entirely of the same amino acids.\nIn response, we impose a slight disturbance during sam-\npling, utilizing the Gumbel-Max trick. The Gumbel-Max\ntrick is a procedure for drawing a sample from a categori-\ncal distribution using Gumbel-distributed random variables.\nLet’s assume we have a discrete random variable X with\ndistribution pθ(x = i) = pi for i = 1, . . . , K. Now, con-\nsider the variables gi = −log (−log Ui) where Ui is a\nvariable uniformly distributed on (0, 1]. The gi are random\nvariables following a Gumbel distribution. The key to the\nGumbel-Max trick is this relationship:\ni∗= arg max\ni\n{˜pi}, where ˜p ∝exp gi + log pi\n(6)\nThis operation provides a sample from the discrete distribu-\ntion pθ(x = i). In other words, the category corresponding\nto the maximum value is the results of sampling. But in\nthe other hand, the maximum value, i.e. gi + log pi, is not\nequal to the original log-probability, which is actually the\nprediction score in our sampling algorithm. Therefore, the\nGumbel-Max trick helps us sample an amino acid with a\nslightly modified prediction score while maintaining the\noriginal distribution. As a result, the previously dominant\namino acid with the highest prediction score may be dis-\ncarded, and a variety of other amino acids may be retained,\nthereby avoiding falling into a trivial pattern such as repeat-\ning with a single amino acid. We find that this technique can\nsignificantly reduce the number of trivial cases and further\nimprove the diversity.\n19\nDiffusion Language Models Are Versatile Protein Learners\nD.2\nDelve deeply into the pLDDT score of\nunconditional sampling\nAccording to the Fig. 2A, we surprisingly find that the\npLDDT score of DPLM unconditional sampling is even\nhigher than the UniRef50 dataset. We investigate this phe-\nnomenon as follows.\nRegarding the lower pLDDT in UniRef50. The lower\npLDDT here is because UniRef50 contains some data\nwith lower structural plausibility, such as sequences with a\nlarge number of repetitive patterns. These cases decrease\nthe average pLDDT of UniRef50, for instance sequence\nADADAD...ADADAD with pLDDT 35.14.\nWe also investigate the average pLDDT of the PDB data\nwhere the pLDDT score of PDB is similar to DPLM, sug-\ngesting that DPLM learns to generate protein sequences\nwith overall similar structural characteristics as PDB, as\nshown in Tab. 3.\nTable 3. pLDDT score of UniRef50, PDB and DPLM uncondi-\ntional sampling.\nLength\nUniRef50 PDB DPLM\n100\n66.54\n84.62\n70.66\n200\n68.61\n79.32\n83.55\n300\n78.30\n84.51\n82.39\n400\n79.80\n80.49\n86.75\n500\n75.17\n77.79\n82.56\navg. pLDDT\n73.68\n81.34\n81.18\nRegarding mode collapse. We also want to investigate\nwhether DPLM collapses into the modes with high pLDDT\nsequences. To verify this, we evaluate the pseudo-perplexity\nof DPLM against subsets of UR50 sequences of high\npLDDT and low pLDDT. The results are shown in Tab. 4.\nWe can find that the ppl of less-structural proteins (pLDDT\n< 50) is similar to the structural proteins (pLDDT > 70),\nsuggesting that DPLM equally learns protein sequences with\ndiverse structural patterns.\nTable 4. pseudo-ppl of less structural and more structural se-\nquences.\nless structural sequences more structural sequences\npseudo-ppl\n2.36\n2.55\nIn conclusion, we would like to provide a possible explana-\ntion for this phenomenon. We suggest that from a perspec-\ntive of probabilistic graphical model (PGM), more struc-\ntured data is generally more easy to learn due to stronger\ncorrelation between its elements. As such, we hypothesize\nthe learning dynamics of protein LMs from through evo-\nlutionary sequences is first to digest those more structural\nproteins as co-evolutionary effects between amino acids\nplay a prevailing and vital role in folding patterns, and then\nTable 5. Results of the success rate of each problem, the number\nof the solved problems and the average success rate across 17\nmotif-scaffolding problems. Here we follow previous work to use\nOmegaFold as the folding model.\nseq-only\nstruct-cond.\nEvoDiff\nDPLM\nRFDiffusion\nDPLM\n1bcf\n0.39\n0.99\n1.00\n1.00\n1prw\n0.87\n0.96\n0.08\n0.99\n1qjg\n0.00\n0.00\n0.00\n0.00\n1ycr\n0.13\n0.52\n0.74\n0.78\n2kl8\n0.03\n0.05\n0.88\n0.05\n3ixt\n0.26\n0.20\n0.25\n0.33\n4jhw\n0.00\n0.00\n0.00\n0.00\n4zyp\n0.00\n0.01\n0.40\n0.01\n5ius\n0.00\n0.10\n0.02\n0.10\n5tpn\n0.00\n0.00\n0.61\n0.00\n5trv\n0.00\n0.00\n0.22\n0.00\n5wn9\n0.00\n0.01\n0.00\n0.01\n5yui\n0.06\n0.42\n0.00\n0.63\n6e6r\n0.16\n0.84\n0.71\n0.84\n6exz\n0.00\n0.01\n0.42\n0.01\n6vw1\n0.00\n0.00\n0.69\n0.00\n7mrx\n0.00\n0.59\n0.07\n0.59\npass rate\n7\/17\n12\/27\n13\/17\n12\/17\navg. success rate\n0.09\n0.27\n0.36\n0.31\nstart to learn those less structural folding patterns, which\ncould be long-tailed.\nThis could somehow relate to the so-called emergence phe-\nnomenon in the realm of LLMs, where scaling up LLMs\nleads to ”grokking” those long-tailed abilities. We would\nleave a study of the learning dynamics of protein LM as an\nexciting future investigation and hopefully can bring some\ninteresting insights to the community.\nD.3\nSequence-conditional generation:\nmotif-scaffolding\nThe overall motif-scaffolding results are shown in Tab. 5.\nWe sample 100 scaffold sequences for each motif scaf-\nfolding case, and compute the success rate according to\nthe standard mentioned in section 4.3.1.\nFurthermore,\nwe also show the pass rate (e.g. the number of solved\nproblems) and the average success rate for all problems.\nWe use sequence-only and structure-conditioned sampling\nparadigms. For sequence-only sampling, DPLM generates\nscaffold according to the motif sequence fragment. For\nstructure-conditioned sampling, DPLM makes generation\nby leveraging both sequence and structure information of\nmotif. Specifically, as noted in section 4.3.2, we utilize the\npre-trained GVPTransformerEncoder and structural adapter\nto process the motif structure. DPLM is able to solve 12 of\n17 motif scaffolding problems. The overall success rate is\n0.27 for sequence-only sampling, while 0.31 for structure-\nconditioned sampling. It should be noted that not all prob-\nlems are suitable for using structure information. We rec-\n20\nDiffusion Language Models Are Versatile Protein Learners\nTable 6. Motif-scaffolding results evaluated by ESMFold.\nseq-only\nstruct-cond.\nEvoDiff\nDPLM\nDPLM\n1bcf\n0.38\n1.00\n1.00\n1prw\n0.36\n0.75\n0.81\n1qjg\n0.00\n0.00\n0.00\n1ycr\n0.03\n0.27\n0.48\n2kl8\n0.00\n0.01\n0.01\n3ixt\n0.09\n0.15\n0.37\n4jhw\n0.00\n0.00\n0.00\n4zyp\n0.00\n0.00\n0.01\n5ius\n0.00\n0.00\n0.00\n5tpn\n0.00\n0.00\n0.00\n5trv\n0.00\n0.00\n0.00\n5wn9\n0.00\n0.00\n0.00\n5yui\n0.05\n0.94\n0.94\n6e6r\n0.03\n0.79\n0.79\n6exz\n0.00\n0.01\n0.01\n6vw1\n0.00\n0.00\n0.00\n7mrx\n0.00\n0.54\n0.54\npass rate\n6\/17\n9\/27\n10\/17\navg. success rate\n0.06\n0.26\n0.29\nTable 7. Ablation study on the CATH4.3 benchmark, which w\/\ndraft means that the reverse process is based on the x(t)\ndraft.\nModels\nTrainable AAR\nstruct. eval.\nParams.\nscTM pLDDT\nLM-DESIGN (w\/ draft) 6.3M\/650M 56.49 0.85\n74.89\nDPLM\n6.3M\/650M 55.75 0.83\n73.72\nDPLM (w\/ draft)\n6.3M\/650M 56.61 0.86\n76.78\nommend using structure-conditioned sampling for 1YCR,\n1PRW, 3IXT and 5YUI, while sequence-only sampling for\nothers.\nEvaluation with more advanced folding model. Moreover,\nwe also investigate evaluation with other structure predic-\ntion models, such as ESMFold (Lin et al., 2022). Results\nare shown in Tab. 6, we consider the Alpha Carbon (CA)\npLDDT score predicted by ESMFold as the overall pLDDT\nscore of the amino acid. We observe that ESMFold judges\nmore strictly than OmegaFold. When we evaluate scaffold\nby ESMFold, there is a slight decline in the overall pass rate\nand average success rate, compared with the evaluation of\nOmegaFold.\nD.4\nStructure-conditional generation: inverse folding\nModel architecture. DPLM only takes amino acid tokens\nas input, instead of structure formats such as 3D coordi-\nnates. Therefore, in order to endow DPLM with structural\nawareness, we follow LM-DESIGN (Zheng et al., 2023b)\nand place a structural adapter after the last layer of DPLM,\nwhich can attach the structure information to the original\noutput probability. The overall architecture of the structural\nadapter is constituted by three components, i.e., a struc-\nture encoder, a pLM as sequence decoder, and a structural\nadapter that bridges both. We can utilize an arbitrary pre-\ntrained structure encoder to process the 3D coordinates and\nprovide structure information for DPLM. For pLMs as\nthe sequence decoder side, we primarily used the DPLM,\nwith its pretrained model weights. The structural adapter\ncomposes a multi-head attention that queries structure infor-\nmation from the structure encoder, followed by a bottleneck\nfeedforward network (FFN) to impose non-linearity and\nabstract features\/representations. ROPE (Su et al., 2021)\nwas used the supplement multi-head attention for better\nmodeling of positional information. In all our experiments,\nonly one structural adapter was placed after the last layer of\nDPLM, following Zheng et al. (2023b).\nTraining and inference details. During training, we freeze\nthe parameters of the structure encoder and DPLM, only\noptimizing the structural adapter with the simplified dis-\ncrete diffusion objective (Zheng et al., 2023a). However,\nwe find that there is an exposure bias problem here: DPLM\nlearns the reverse denoising process based on the ground\ntruth context, i.e. x(t), which is obtained by adding noise\non the ground truth sequence, i.e. x(0). During inference,\nDPLM has to denoise given the context predicted, which\nis not always right, leading to training-inference inconsis-\ntency. Therefore, we slightly modify the training objective\nin Eq. (4). Specifically, we obtain x(t) by adding noise on\nthe draft sequence generated by the pretrained structure en-\ncoder, rather than the ground truth x(0), which we refer to\nas x(t)\ndraft. Then DPLM will learn the reverse process that\nreconstructs the x(0) given the x(t)\ndraft, as shown in Eq. (7).\nSince the draft sequence is available both in training and in-\nference time, the issue of exposure bias is mitigated. We find\nthis technique can further boost the performance of DPLM\nin the inverse folding task, as illustrated in the Tab. 7\nJt = Eq(x(0))\n\u0014\nλ(t) X\n1≤i≤L\nbi(t) · log pθ(x(0)\ni |x(t)\ndraft)\n\u0015\n, (7)\nAt inference time, we follow the DPLM generative process,\nexcept that we obtain protein sequence via greedy deter-\nministic decoding, instead of random sampling from the\ndistribution. Besides, considering that we have had an un-\nconditional model, i.e. the DPLM itself, and a conditional\nmodel, i.e., the DPLM with structural adapter, we can also\nseamlessly utilize the classifier-free guidance paradigm dur-\ning inference.\nD.5\nClassifier-free guidance\nClassifier-free guidance (Ho & Salimans, 2021) has been\nshown as an effective way to enhance conditional diffusion\nmodels. Likewise, for DPLM, we can derive an implicit\n21\nDiffusion Language Models Are Versatile Protein Learners\nclassifier using the Bayes rule\nq(y|x(t−1)) = q(y|x(t−1), x(t))\n= q(x(t−1)|x(t), y)\nq(x(t−1)|x(t)) q(y|x(t)).\nIf we already have an unconditional model pθ(x(t−1)|x(t))\nand a conditional model pθ(x(t−1)|x(t), y) as the estimates,\nthen by substituting this implicit classifier into Eq. 5, we\ncan obtain\nx(t−1) ∼pθ(x(t−1)|x(t))pϕ(y|x(t−1))η\n∝pθ(x(t−1)|x(t))\n\u0000pθ(x(t−1)|x(t), y)\npθ(x(t−1)|x(t))\n\u0001η\n= pθ(x(t−1)|x(t), y)η · pθ(x(t−1)|x(t))(1−η),\nwherein when η = 1, it is equivalent to sampling from\nthe original conditional DPLM without guidance, whereas\nη > 1, we not only prioritize the conditional model to con-\ntribute more but also discourage the samples from moving\naway from the unconditional distribution. In other words,\nit reduces the chance of generating samples that do not\nuse conditioning information, in favor of the samples that\nexplicitly do.\nNote that when we use adapter tuning to adapt DPLM for\nconditional generation, we only finetune the newly-added\nparameters, which means that we can already access both\nthe unconditional model (original DPLM) and conditional\nmodel (the adapter-tuned model) simultaneously for free.\nAs demonstrated in Fig. 6 on structure-conditioned sequence\ngeneration, we can find that DPLM as a diffusion model\ncan benefit from classifier-free guidance, improving its con-\nditional generation immediately.\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n1\n55.7\n55.8\n55.9\n56.0\n56.1\n56.2\nAAR\nFigure 6. Classifier-free guidance enhances structure-conditioned\nsequence generation (inverse folding).\nE\nRelated Work\nE.1\nLanguage Models\nThe dominant paradigm of language models is autoregres-\nsive language models, which breaks down the mutual distri-\nbution over the tokens of a sequence into conditional proba-\nbilities via the chain rule p(x[1:N]) = QN\ni=1 p(x[i]|x[1:i−1])\nand generates tokens by ancestral sampling from left to\nright (Bengio et al., 2000; Sutskever et al., 2014; Vaswani\net al., 2017).\nRecently, researchers propose the non-\nautoregressive language models as an alternative (Gu et al.,\n2018). These models do not need to obey the left to right\ngeneration order (Qian et al., 2022; Huang et al., 2023) and\ndemonstrate competitive or superior performance compared\nto their autoregressive counterpart across a wide range of\ndomains including languages (Qian et al., 2021b; Huang\net al., 2023; Qian et al., 2022; Huang et al., 2023; Zheng\net al., 2023a), speeches (Kim et al., 2021), proteins (Zheng\net al., 2023b), and molecules (Hoogeboom et al., 2022).\nAmong the numerous non-autoregressive language mod-\nels, diffusion language models (Li et al., 2022; Gong et al.,\n2022; Zheng et al., 2023a) have emerged as a solid and\npromising framework. Pretraining language models on a\nmassive scale of unlabeled data markedly improves their\ndownstream task performance (Mikolov et al., 2013; Peters\net al., 2018; Radford et al., 2018; Devlin et al., 2019). As\ndata volume and model sizes scale up, the training loss of\nlanguage models predictably declines (Kaplan et al., 2020;\nHoffmann et al., 2022; Muennighoff et al., 2023), and en-\nhancing downstream task performance even without specific\ntuning (Radford et al., 2019). GPT3 (Brown et al., 2020)\nis a significant point in the journey, taking model sizes to\n175B parameters, proposing in-context learning to bolster\nlanguage models’ competence in solving certain tasks with\nonly a handful of demonstrations. Furthermore, Wei et al.\n(2021); Sanh et al. (2022); Ouyang et al. (2022) introduce\ninstruction tuning, finetuning pretrained language models on\nseries of tasks described via instructions, which elicits the\ninstruction following ability of models and significantly en-\nhances their zero-shot performance on unseen tasks. More\nimpressively, sufficiently large language models exhibit the\nemergent abilities such as multi-step reasoning (Kojima\net al., 2022; Wei et al., 2022a;b), which small models do\nnot possess (Fu et al., 2023). Empowered by large language\nmodels, helpful applications such as conversational AI sys-\ntems1 and autonomous agents2 have garnered much interest.\nAlthough the most capable models at the moment are re-\nstricted in access, open-sourced efforts (Zeng et al., 2022;\nTouvron et al., 2023a;b; Taori et al., 2023; Chiang et al.,\n2023; Sun & Qiu, 2023) have largely enhanced the public\naccessibility of powerful large language models.\nE.2\nProtein Language Models\nThanks for the abundance of 1D amino acid sequences, there\nis growing interest in developing protein LMs at the scale\nof evolution, such as the series of ESM (Rives et al., 2019;\nLin et al., 2022), TAPE (Rao et al., 2019), ProtTrans (El-\nnaggar et al., 2021), PRoBERTa (Nambiar et al., 2020),\n1https:\/\/chat.openai.com\/\n2https:\/\/github.com\/Significant-Gravitas\/\nAuto-GPT\n22\nDiffusion Language Models Are Versatile Protein Learners\nPMLM (He et al., 2021), ProteinLM (Xiao et al., 2021),\nPLUS (Min et al., 2021), Adversarial Masked LMs (Mc-\nDermott et al., 2021), ProteinBERT (Brandes et al., 2022),\nCARP (Yang et al., 2022a) in masked language modeling\n(MLM) paradigm, ProtGPT2 (Ferruz et al., 2022) in causal\nlanguage modeling paradigm, and several others (Melnyk\net al., 2022; Madani et al., 2021; Unsal et al., 2022; Nourani\net al., 2021; Lu et al., 2020; Sturmfels et al., 2020; Strodthoff\net al., 2020). These protein language models exhibit remark-\nable generalization ability on various downstream tasks and\nbe able to capture evolutionary information about secondary\nand tertiary structures from sequences alone. Meanwhile,\nrecent study shows these models’ potency in revealing pro-\ntein structures (Lin et al., 2022), predicting the effect of\nsequence variation on function (Meier et al., 2021), anti-\nbody infilling (Melnyk et al., 2022) and many other general\npurposes (Rives et al., 2019). Simultaneously, Verkuil et al.\n(2022) demonstrate that the large scale protein LMs can gen-\nerate de novo proteins by generalizing beyond natural pro-\nteins, both theoretically and experimentally validating their\nhypothesis in exhaustive detail, in which pLMs demonstrate\ncompetency in designing protein structure despite being\nexclusively trained on sequences.\nE.3\nDiffusion Language Models\nDerived from diffusion models (Sohl-Dickstein et al., 2015),\ndiffusion language models is a variety of generative model\nthat samples data via an iterative denoising process from\nnoise. They can be divided into continuous (Ho et al., 2020;\nSong et al., 2020) and discrete (Hoogeboom et al., 2021b;\nAustin et al., 2021) categories according to the distribution\nthey model. Continuous diffusion models make great suc-\ncess in vision (Dhariwal & Nichol, 2021b; Rombach et al.,\n2021; Ho et al., 2022), but they struggle in languages for\noperating on continuous surrogates of discrete tokens (Li\net al., 2022; Gong et al., 2022; Han et al., 2022; Dieleman\net al., 2022; Yuan et al., 2022; Gao et al., 2022a; Ye et al.,\n2023b; Chen et al., 2023a; Wu et al., 2023), which has diffi-\nculty bypassing the pitfall of discreteness (Ye et al., 2023b)\nand still lags behind autoregressive language models. In\ncontrast, discrete diffusion models, albeit having limited\nprogress in large-scale applications, are innately suited to\nthe data type inherent to languages (i.e., sequences of dis-\ncrete tokens). Zheng et al. (2023a) makes commendable\nstrides in discrete diffusion models and enhancing these\nmodels to yield comparable performance with autoregres-\nsive models on typical language generation benchmarks like\nmachine translation. Furthermore, as shown by He et al.\n(2023); Zheng et al. (2023b), there are close relationship\nbetween discrete diffusion models and masked language\nmodels (MLM), a widely adopted pretraining paradigm in\nNLP (Devlin et al., 2019; Liu et al., 2019). Following this\nline, Ye et al. (2023a) propose scaling discrete diffusion\nLMs with diffusive adaptation, showing strong performance\non several conditional text generation tasks, and accessing\nzero-shot instruction following, few-shot in-context learn-\ning and the promise of structured reasoning with instruction\ntuning.\nE.4\nProtein Structure Diffusion Models\nDiffusion models have become popular tools in structural\nbiology for protein generation, and their utility has been\ndemonstrated across a range of generative tasks in recent\nyears. Trippe et al. (2022), along with others, have intro-\nduced several diffusion model variants, each with its unique\napproach. For instance, while some models focus on gener-\nating the protein backbone by diffusing over protein coordi-\nnates, others, such as those proposed by Wu et al. (2022b),\ntarget inter-residue angles. Lin & AlQuraishi (2023) and\nYim et al. (2023) have developed models that handle both\nthe position and orientation of residue frames. RFDiffu-\nsion (Watson et al., 2023) is a model that assists in designing\nprotein structures for specific functions, such as enzymes.\nIt is versatile in protein design and has been used to create\ntherapeutic proteins, with some designs being confirmed in\nthe laboratory. ProteinSGM (Lee et al., 2022) is a model\nthat uses 2D matrices, which represent the distances and\nangles between protein parts, to create 3D protein structures\nfor novel protein designs. FoldingDiff (Wu et al., 2022a) is\na model that generates protein sequences expected to fold\ninto a specific structure. These sequences are verified with\nprediction tools, although they have not been experimentally\nconfirmed yet. Chroma (Ingraham et al., 2023) is a model\ndesigned for creating large proteins and protein complexes,\nconsidering various constraints like distances and symmetry.\nIt transforms a collapsed polymer into protein backbone\nand sequence more quickly than older methods, thereby\nallowing for the efficient generation of large structures.\nE.5\nProtein Inverse Folding\nThe structure-based protein sequence design is typically\nformulated as a conditional sequence generation problem\nby deep generative modeling, wherein protein 3D struc-\ntures are usually depicted as a k-NN graph (Ingraham et al.,\n2019). The protein graph establishes edge features between\nadjacent residues and encodes residue information as node\nfeatures, modeled by graph neural networks (GNNs). Graph-\nTrans (Ingraham et al., 2019) and GVP (Jing et al., 2020)\nutilizes the graph attention encoder and autoregressive de-\ncoder for protein design. Recently, ProteinMPNN (Dau-\nparas et al., 2022) and PiFold (Gao et al., 2022b) introduce\nmore complex protein features and expressive GNNs, result-\ning in significant improvements. Furthermore, in addition\nto the primary generative purpose, this task can also be used\nas a proxy for protein (structure-aware) representation learn-\ning (Yang et al., 2022b). A critical and significant challenge\nherein is the lack of sufficient protein structure data. To this\nend, ESM-IF (Hsu et al., 2022) alleviate this issue with effec-\ntive data augmentation by back-translation with AlphaFold\n23\nDiffusion Language Models Are Versatile Protein Learners\n2 (Jumper et al., 2021). , resulting in dramatic improve-\nments. On the other hand, Zheng et al. (2023b) demonstrate\nhow to efficiently steering large pretrained protein LMs into\na structure-informed sequence generative models in a mask-\npredict generative manner, attaining state-of-the-art results\non single-chain and complex protein benchmark. Most re-\ncently, graph diffusion models have also been studied for\ninverse folding problem (Yi et al., 2023).\n24\nDiffusion Language Models Are Versatile Protein Learners\nF\nVisualization of Unconditional Samples\nFigure 7. Visualized examples from 50 to 500 in length.\nFigure 8. Visualized examples from 600 to 1000 in length.\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Diffusion Language Models Are Versatile Protein Learners.pdf"}
{"title":"TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation","authors":"Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, Xinglong Wu","summary":"We present TokenFlow, a novel unified image tokenizer that bridges the\nlong-standing gap between multimodal understanding and generation. Prior\nresearch attempt to employ a single reconstruction-targeted Vector Quantization\n(VQ) encoder for unifying these two tasks. We observe that understanding and\ngeneration require fundamentally different granularities of visual information.\nThis leads to a critical trade-off, particularly compromising performance in\nmultimodal understanding tasks. TokenFlow addresses this challenge through an\ninnovative dual-codebook architecture that decouples semantic and pixel-level\nfeature learning while maintaining their alignment via a shared mapping\nmechanism. This design enables direct access to both high-level semantic\nrepresentations crucial for understanding tasks and fine-grained visual\nfeatures essential for generation through shared indices. Our extensive\nexperiments demonstrate TokenFlow's superiority across multiple dimensions.\nLeveraging TokenFlow, we demonstrate for the first time that discrete visual\ninput can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\\%\naverage improvement. For image reconstruction, we achieve a strong FID score of\n0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art\nperformance in autoregressive image generation with a GenEval score of 0.55 at\n256*256 resolution, achieving comparable results to SDXL.","url":"http:\/\/arxiv.org\/abs\/2412.03069v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.03069v1","published":1733294815000,"comment":"https:\/\/byteflow-ai.github.io\/TokenFlow\/","pdf_text":"TokenFlow: Unified Image Tokenizer for Multimodal Understanding and\nGeneration\nLiao Qu*, Huichao Zhang*, Yiheng Liu, Xu Wang†, Yi Jiang, Yiming Gao, Hu Ye,\nDaniel K. Du, Zehuan Yuan, Xinglong Wu\nByteDance\nhttps:\/\/byteflow-ai.github.io\/TokenFlow\/\nAbstract\nWe present TokenFlow, a novel unified image tokenizer that\nbridges the long-standing gap between multimodal under-\nstanding and generation. Prior research attempt to employ\na single reconstruction-targeted Vector Quantization (VQ)\nencoder for unifying these two tasks. We observe that un-\nderstanding and generation require fundamentally different\ngranularities of visual information. This leads to a crit-\nical trade-off, particularly compromising performance in\nmultimodal understanding tasks. TokenFlow addresses this\nchallenge through an innovative dual-codebook architec-\nture that decouples semantic and pixel-level feature learn-\ning while maintaining their alignment via a shared map-\nping mechanism. This design enables direct access to both\nhigh-level semantic representations crucial for understand-\ning tasks and fine-grained visual features essential for gen-\neration through shared indices. Our extensive experiments\ndemonstrate TokenFlow’s superiority across multiple di-\nmensions. Leveraging TokenFlow, we demonstrate for the\nfirst time that discrete visual input can surpass LLaVA-1.5\n13B in understanding performance, achieving a 7.2% av-\nerage improvement. For image reconstruction, we achieve\na strong FID score of 0.63 at 384×384 resolution. More-\nover, TokenFlow establishes state-of-the-art performance in\nautoregressive image generation with a GenEval score of\n0.55 at 256×256 resolution, achieving comparable results\nto SDXL.\n1. Introduction\nLarge Language Models (LLMs) have revolutionized\nnatural language processing through their unified autore-\ngressive framework, demonstrating remarkable capabilities\nacross diverse tasks [1, 2]. However, in the multimodal do-\nmain of vision and language, a fundamental divide persists\n*Equal contribution\n†project lead\nSEEDBench\nMMVet\nVQAv2\nPOPE\nGQA\nTextVQA\nMMMU\nMMBench\nMME-Perception\n60.0\n65.0\n70.0\n35.0\n40.0\n45.0\n45.0\n60.0\n75.0\n83.0\n85.0\n87.0\n53.0\n57.0\n61.0\n20.0\n40.0\n60.0\n32.5\n37.0\n41.5\n61.0\n68.0\n75.0\n1375.0\n1450.0\n1525.0\nLLaVA-1.5 (13B)\nJanus (1.3B)\nVILA-U (7B)\nEMU3 (8B)\nTokenFlow-XL (Ours, 14B)\nFigure 1. Multimodal Understanding Results with TokenFlow. We\ndemonstrate for the first time that discrete visual input can surpass\nLLaVA-1.5 13B in understanding performance, achieving a 7.2%\naverage improvement.\nbetween perception and generation paradigms. Current ap-\nproaches address them through distinct architectures: multi-\nmodal understanding models leverage vision encoders and\nprojection layers to align visual representations with pre-\ntrained LLMs [29, 52], while visual generation relies on\neither diffusion-based methods [39, 41] or discrete image\ntokens for autoregressive generation [38, 44, 51, 65]. This\ndivergence motivates the pursuit of unified approaches ca-\npable of both understanding and generation.\nThe advent of GPT-4o [59] has greatly boosted in-\nterest in developing more generalist multimodal models.\nEarly efforts to unify perception and generation capabili-\nties [27, 46] have primarily focused on equipping LLMs\nwith the power of diffusion models. However, these ap-\n1\narXiv:2412.03069v1  [cs.CV]  4 Dec 2024\nA bird flying through the air while \nflapping it's wings.\nA hotel room with a large bed, \nlamp, and window view.\nA dog is lying on the carpet of \nthe living room.\nA portrait of a woman.\nA photo of a hamburger.\nOcean waves under a vibrant sunset \nsky with clouds and birds.\nWoman in a flowing dress with \nlong hair, surrounded by orange \nautumn trees and sunset.\nPerson standing in a mystical \nforest under a bright moon, with \ncolorful trees and reflections.\nA graffiti wall with the words \n'TOKEN FLOW' written on it.\nIntricate origami of a fox and \na unicorn in a snowyforest.\nFigure 2. Visual Generation Results with TokenFlow. We present diverse 256×256 results across various styles, subjects, and scenarios.\nproaches introduce substantial architectural complexity and\ncomputational overhead, highlighting the need for a more\nelegant unified solution. Recent efforts have explored one\npromising direction: using a single transformer architecture\nto unify visual and textual information within the next-token\nprediction framework [48, 55]. This approach relies on VQ\nencoders to convert visual inputs into discrete tokens that\ncan be processed alongside text, offering a potentially sim-\npler and more efficient framework. By treating both modali-\nties as sequences of discrete tokens, this framework enables\nend-to-end training within a single architecture.\nHowever, a fundamental challenge exists in such unified\napproaches. Multimodal understanding demands rich se-\nmantic representations to support complex reasoning, while\nvisual generation, on the other hand, requires precise encod-\ning of spatial structure and textural details. Current meth-\nods predominantly employ reconstruction-targeted VQ en-\ncoders [13, 73], which are primarily optimized for recon-\nstruction fidelity. While this optimization makes them well-\nsuited for generation tasks, it potentially limits their ability\nto capture the high-level semantic features crucial for un-\nderstanding tasks. While Janus [57] attempts to address this\nconflict by employing separate encoders for understanding\nand generation tasks, this increases model complexity with-\nout fundamentally resolving the underlying representation\ndisparity. These limitations underscore a critical gap in the\nfield: the absence of a unified visual encoding mechanism\nthat can effectively serve both perception and generation ob-\njectives. This motivates our central research question: Can\none single image tokenizer derive representations suitable\nfor both multimodal understanding and generation?\nTo address this challenge, we propose TokenFlow, a\nnovel unified image tokenizer that bridges the gap between\nunderstanding and generation through a unique dual-flow\ndesign.\nThe key insight is to decouple the learning of\nsemantic and pixel-level features while maintaining their\nalignment through a shared index mapping. By mapping\npatches with both semantic and pixel-level similarities to\nidentical indices, the quantized features can be directly\napplied to both autoregressive visual generation and mul-\ntimodal understanding.\nUnlike concurrent approach that\nconstrains different feature levels within a single code-\nbook [60], TokenFlow’s dual-codebook design enables spe-\ncialized learning while maintaining cross-level correlations\nthrough shared indices. This innovation allows simultane-\nous access to both semantic and pixel-level representations\nwithout compromising either aspect. Specifically, Token-\nFlow adopts a dual-encoder architecture coupled with cor-\nresponding specialized codebooks. The semantic encoder,\nlearned from a CLIP-style teacher, provides strong seman-\ntic priors, while the pixel encoder captures detailed visual\ninformation. The extracted features are then quantized by\nminimizing the weighted summation of semantic and pixel-\nlevel distances, creating a joint representation space.\nOur framework exhibits remarkable scalability, main-\ntaining exceptional codebook utilization (95%+) even with\nlarge-scale codebooks of over 130K entries - substantially\nadvancing beyond prior approaches [13] in both capacity\nand efficiency. TokenFlow also achieves a strong FID score\nof 0.63 at 384×384 resolution. For text-to-image synthe-\nsis, we establish a new state-of-the-art GenEval score of\n0.55 at 256×256 resolution in the autoregressive paradigm\n2\nwhile requiring significantly fewer sampling steps com-\npared to existing methods like EMU3 [55] and LlamaGen\n[44]. On multimodal understanding benchmarks, Token-\nFlow achieves new state-of-the-art performance with mini-\nmal training overhead, surpassing LLaVA-1.5 13B by 7.2%\non average - for the first time discrete visual inputs can\noutperform this strong baseline. These results validate To-\nkenFlow’s effectiveness as a unified visual tokenizer that\nbridges the long-standing gap between understanding and\ngeneration tasks.\n2. Related Work\n2.1. Tokenization for Visual Generation.\nVector quantized (VQ) image tokenizers have played a\ncrucial role in recent advancements in autoregressive image\ngeneration [28, 34, 44, 51, 65]. [54] proposed the VQVAE,\nquantizing patch-level features using the nearest codebook\nentry, with the codebook learned with the encoder-decoder\nstructure through reconstruction loss. VQVAE-2 [40] ad-\nvanced this framework through exponential moving aver-\nage updates and a hierarchical multi-scale approach. VQ-\nGAN [13] further enhanced the architecture by incorporat-\ning adversarial and perceptual losses, yielding more precise\nand detailed representations. Recent advances in VQ tok-\nenizers have focused on three main directions: improving\nreconstruction fidelity and generation quality [21, 64, 73],\nenhancing codebook utilization [64, 70, 76], and exploring\nnovel architectures such as the multi-scale VQVAE [25, 51]\nfor next-scale prediction of images. While these methods\neffectively preserve local details after quantization, they of-\nten struggle to capture semantic-level information, limiting\ntheir effectiveness in autoregressive multi-modal image un-\nderstanding tasks. Our proposed TokenFlow addresses this\nlimitation by introducing dual codebooks with shared map-\nping, achieving state-of-the-art performance in both autore-\ngressive generation and multimodal understanding.\n2.2. Tokenization for Unified Multimodal Under-\nstanding and Generation\nRecent efforts have emerged to bridge the gap between\nmultimodal understanding and generation [23, 48, 55, 57,\n60, 62]. Approaches like Chameleon [48], EMU3 [55] and\nShow-o [62] employ VQ tokenizers [13, 66, 73] to encode\nimages for both tasks. However, these methods typically\nrequire multimodal training from scratch and often suffer\nperformance degradation in visual perception tasks due to\nlimited semantic representation in their tokenized features.\nSEED-LLaMA [23] introduced a novel VQ tokenizer in-\ncorporating high-level semantics for understanding and uti-\nlize SD [41] as generation decoder. Janus [57] attempted\nto address the modality gap by employing separate tokeniz-\ners for understanding [69] and generation [44], though this\nleads to increased model complexity without fundamentally\nresolving the underlying challenge. Concurrent work [60]\nproposed a unified vision tower aligning discrete visual fea-\ntures with text during pre-training. However, their approach\nconstrains low-level and high-level representations within a\nsingle flow, limiting the upper bound of downstream per-\nformance. In contrast, our work posits that the key to uni-\nfying understanding and generation lies in learning a uni-\nversal mapping. By defining dual codebooks with shared\nmapping, TokenFlow enables flexible combinations of low\nand high-level features, resulting in superior performance\nacross all downstream tasks.\n3. Method\n3.1. Motivation\nTable 1. Comparison of various visual encoders on multimodal un-\nderstanding [14, 23, 43] within the LLaVA-1.5 framework. VQKD\nis distilled from CLIP ViT-B\/14. ”Sem.” refers to semantic en-\ncoders that learn semantic-level representations, while ”Pix.” indi-\ncates pixel-level tokenizers that focus on low-level visual features.\n# Exp.\nVisual Encoder\nType\nMME-P ↑\nSEEDB ↑\nTQA ↑\nContinuous:\n1\nCLIP ViT-B\/14 [37]\nSem.\n1460.9\n64.1\n53.4\nDiscrete:\n2\nVQGAN [13]\nPix.\n756.1\n38.2\n46.8\n3\nVQGAN-LC [76]\nPix.\n744.8\n38.2\n45.7\n4\nLFQ [66]\nPix.\n889.5\n41.1\n46.4\n5\nVQKD [35]\nSem.\n1252.4\n57.8\n48.2\nUnifying multimodal understanding and generation into\na cohesive next-token prediction paradigm requires a VQ\ntokenizer for extracting indices from input images. While\ntraditional VQ tokenizers [13, 54, 66, 76] excel at pixel-\nlevel image reconstruction, our investigation reveals a sig-\nnificant limitation in their image understanding capabilities.\nWe conducted experiments utilizing these tokenizers as fea-\nture extractors within the LLaVA-1.5 [29] framework. As\nshown in Exp. 2-4 of Tab. 1, the performance of these dis-\ncrete tokenizers consistently lags behind that of the continu-\nous tokenizer CLIP ViT-B\/14 [37]. We posit that this perfor-\nmance gap stems from their pre-training objectives, which\nprimarily optimize towards better low-level reconstruction\nquality.\nConsequently, the extracted features mainly en-\ncode low-level information, lacking the semantic-level un-\nderstanding, which is crucial for complex visual reasoning.\nAnother straight forward solution for unified understand-\ning and generation can be distill discrete tokens from pre-\ntrained CLIP [8, 37, 45, 69], and then equip it with image re-\nconstruction capability. As demonstrated in Exp. 5, VQKD,\ndistilled from CLIP ViT-B\/14, substantially reduces the per-\nformance gap compared to other discrete tokenizers. We\n3\n1\n𝑘−1\nℕ\nℕ\nSemantic\nEncoder\nPixel\nEncoder\nquantization\nSemantic\nDecoder\nPixel\nDecoder\nℒ!\"#\nℒ$%&\nSemantic Codebook Embeddings\n1\n2\n3\n𝑘\n1\n2\n3\n𝑘−2\n𝑘\nShared Mapping\nPixel Codebook Embeddings\n𝒅𝒔𝒆𝒎\nℕ\n𝒅𝒑𝒊𝒙\nNorm\nData Flow\nSupervision\nℕ\n𝑘−1\n𝑘−2\n𝑘−1\n2\n𝑘\nquery\n1\n1\n𝑘\n𝑘\n2\n2\n𝑘−1\n𝑘−1\nindex\nDownstream tasks\nFigure 3. Overview of TokenFlow. We incorporate dual encoders and codebooks with a shared mapping, enabling the joint optimization\nof high-level semantics and low-level pixel details. For a given input image, distances dsem and dpix are calculated from the pixel-level\nand semantic-level codebooks, respectively, with the final codebook index and features determined by minimizing the weighted sum\ndsem + wdis · dpix. The resulting quantized features are independently decoded for both semantic alignment and image reconstruction\ntraining, and then concatenated to provide a unified representation for downstream tasks in understanding and generation.\n(a)\n(b)\n(c)\nFigure 4. Visualization of images clustered by (a) VQKD [35],\n(b) VQGAN [13], and (c) Our TokenFlow. VQKD clusters exhibit\nsemantic similarity, while VQGAN clusters exhibit low-level sim-\nilarity (i.e. color). Our TokenFlow can successfully combine both\nsemantic and low-level similarity. Implementation details of im-\nage clustering can be found in Appendix A.1.\nfurther conducted an experiment to reconstruct the original\nimage from quantized features extracted by VQKD. The re-\nconstructed images exhibited significant blurring and a evi-\ndent loss of high-frequency details, as shown in Fig. 8. We\nattribute this outcome to the nature of VQKD’s encoder,\nwhich maps semantically close patches into same codebook\nindex. As visualized in Fig. 4 (a), it tends to map images\nwith same semantical meaning to the same codebook index,\nwhile VQGAN (Fig. 4 (b)) tends to map visually similar im-\nages to the same codebook index, prioritizing low-level fea-\ntures over semantic content. Therefore, the reconstruction\nof fine-grained details from low-level dissimilar patches ag-\ngregated by VQKD becomes extremely challenging.\nThese observations highlight the necessity of developing\na novel tokenization approach that can effectively handle\nhigh-level semantic understanding and low-level visual re-\nconstruction tasks.\n3.2. Unified Image Tokenizer\nTo bridge this gap, we propose TokenFlow (Fig. 3), a\nnovel unified image tokenizer that enables joint representa-\ntion learning at both semantic and pixel level. We find the\nkey to unifying understanding and generation lies in learn-\ning an universal mapping. If the tokenizer can map patches\nthat are both high-level and low-level similar to the same\ncodebook index, then the quantized features can be easily\ndecoded and directly applied to both autoregressive visual\ngeneration tasks and multimodal understanding tasks.\nEncoder. Unlike previous approaches that utilize one\nsingle encoder to extract low-level image information, we\npropose a dual-encoder architecture comprising a semantic\nencoder Esem and a pixel encoder Epix. This design enables\nthe extraction of two distinct types of image features. For\nthe semantic encoder, we initialize it with a pre-trained text-\naligned vision encoder (e.g., CLIP ViT-B\/14). This initial-\nization strategy facilitates better learning of high-level text-\naligned embeddings in the semantic codebook, ultimately\nenhancing the model’s multimodal understanding capabil-\nities. For brevity here, we omit the spatial indices of fea-\nture representations, where ˆzsem = Esem(x) ∈Rdsem and\nˆzpix = Epix(x) ∈Rdpix are the encoded features from se-\nmantic and pixel encoder.\nQuantization. We introduce an innovative quantization\napproach that employs dual codebooks: semantic-level em-\nbeddings Zsem = {zsem,i}K\ni=1 ∈RK×dsem and pixel-level\nembeddings Zpix = {zpix,i}K\ni=1 ∈RK×dpix, where K is the\n4\nnumber of codebook entries. These two codebooks share\na unified mapping, enabling simultaneous consideration of\nhigh-level semantic information and low-level pixel details\nduring the quantization process. Given the encoded feature\nrepresentations ˆzsem and ˆzpix, we compute the distances to\ntheir respective codebook embeddings after l2-norm [64]:\ndsem,i = ∥ˆzsem −zsem,i∥2\n2, for i = 1, . . . , K\n(1)\ndpix,i = ∥ˆzpix −zpix,i∥2\n2, for i = 1, . . . , K\n(2)\ni∗= arg min\ni\n(dsem,i + wdis · dpix,i)\n(3)\nThe optimal quantization index i∗is determined by mini-\nmizing the weighted sum of these two distances, where wdis\nis the distance balance weight, as shown in Eq. (3). This\njoint optimization approach differs significantly from previ-\nous VQ methods that typically focus on learning the distri-\nbution of a single feature type. We further adopt the multi-\nscale VQ (MSVQ) structure [51] to to enhance the rich-\nness of the codebook representation. Our shared mapping\nstrategy enables the codebook to learn the joint distribution\nof high-level semantics and low-level features, resulting in\nseveral key advantages:\n❶Scalability: Our approach demonstrates consistent\nperformance improvements in both generative and under-\nstanding tasks as the codebook size increases, since large\ncodebook size offers more high- and low-level feature com-\nbination possibilities. With an expanded codebook size of\n131,072, it can still maintain a remarkably high utilization\nrate of over 95% while achieving best image reconstruction\nquality and multimodal understanding performance.\n❷Multi-task Capabilities: By learning the joint dis-\ntribution of semantic and pixel-level features, our method\nbridges the gap between generation and understanding\ntasks. This unified representation enables a single tokenizer\nto excel in both domains. This design also allows seamless\nintegration of more codebooks to embed other type of fea-\nture representations, enabling extensibility to more down-\nstream tasks without architectural modifications.\nDecoder and Training Objective. Our architecture in-\ncorporates two distinct decoders, including semantic de-\ncoder Dsem and pixel decoder Dpix for reconstructing se-\nmantic features and original image. We employ a teacher\nmodel [35] (identical to the semantic encoder’s initializa-\ntion) for target feature extraction. The semantic loss Lsem is\ncomputed as the l2 distance between decoded and teacher-\nextracted features. The reconstruction loss is formulated as:\nLpix = ℓ2(x, ˆx) + LP(x, ˆx) + λGLG(ˆx)\n(4)\nwhere ˆx = Dpix(z), ℓ2 represents pixel-wise reconstruction\nloss, LP(·) denotes perceptual loss using LPIPS, and LG(·)\nrepresents adversarial loss with λG as its weight coefficient.\nFollowing vector quantization conventions, we employ a\nstraight-through gradient estimator: z = sg[z−ˆz]+ˆz where\nsg[·] denotes the stop-gradient operation.\nThe codebook\nlearning objective is: LVQ = ||sg[ˆz] −z||2\n2 + β||ˆz −sg[z]||2\n2\nwhere the second term represents commitment loss with\nbalancing factor β. The total training objective is the sum\nof all losses: Ltotal = Lsem + LVQ + Lpix.\n(a)\n(b)\nFigure 5.\nQualitative comparison of different sampling strate-\ngies in our framework. (a) Single-pass top-k (k=1200) and top-p\n(p=0.8) sampling exhibits inconsistent patterns and artifacts. (b)\nOur proposed multi-step sampling strategy produces more coher-\nent and visually appealing results. Best zoomed in for details.\n3.3. Visual Generation with TokenFlow\nTokenFlow helps us achieve SOTA performance in au-\ntoregressive text-to-image generation using the next-scale\nprediction paradigm. Below, we detail our training and in-\nference strategy for high-quality image synthesis.\nTraining Strategy. Our visual generation architecture\nbuilds upon a pre-trained LLM model [53]. For text en-\ncoding, we leverage the model’s native BPE tokenizer to\ntransform input text into discrete token sequences and ex-\ntract feature representations. The original vocabulary is ex-\ntended with specialized visual tokens. We extract the image\ntokens using TokenFlow, pass it through a MLP, and con-\ncatenate it with text tokens for training. Given the model’s\nautoregressive nature, we employ cross-entropy loss com-\nputed exclusively on image tokens. To enable classifier-free\nguidance [17] during inference, we randomly replace condi-\ntioned text with an empty string with probability pdrop = 0.1\nduring training. Following [11, 48, 56], we incorporate QK-\nnormalization and norm re-ordering to enhance training sta-\nbility and prevent loss spikes.\nInference Strategy. We observed that conventional top-\nk-top-p sampling strategies, when employed in the next-\nscale paradigm, often lead to image collapse and repetitive\nlocal patterns. This can be attributed to the cross-entropy\ntraining objective, which establishes attention-based rela-\ntionships primarily with the top-1 prediction. Independent\ntop-k sampling for each token during inference can result\nin tokens lacking direct correlations, leading to inconsistent\nor repetitive patterns that can only be partially remedied\n5\nthrough subsequent scales’ attention. This issue becomes\nmore severe particularly with limited inference steps.\nTo address this fundamental limitation, we propose a\nnovel multi-step sampling approach: (i) Initial sampling:\nPerform top-k top-p sampling with parameters k1 and p1.\n(ii) Refinement: Use the sampled output as input for a sec-\nond round of sampling in the same scale with reduced pa-\nrameters k2 < k1 and p2 < p1. This progressive narrowing\nof the sampling space maintains creative diversity while en-\nforcing consistency through refinement steps. Empirical re-\nsults demonstrate significantly more coherent and visually\nappealing generations compared to single-pass sampling\nmethods (see Fig. 5 and detailed ablation in Appendix B.1).\n3.4. Multimodal Understanding with TokenFlow\nTokenFlow functions as a multi-scale VQ tokenizer,\nwhere the quantized multi-scale features can be directly fed\ninto a pre-trained LLM for multimodal understanding train-\ning, following the LLaVA-1.5 [29] paradigm. The joint fea-\nture representations from dual flow serve as input to the\nmodel. We validate multiple feature input strategies: (i)\nFeature from all scales (ii) Final-scale feature only (iii)\nResidual features from all scales.\nWe discover that fea-\ntures from the final scale achieves best overall performance,\nas detailed in Appendix B.1. This suggests that the final\nscale captures the most relevant semantic information for\nmultimodal understanding, while additional scale features\nor residual features may introduce noise that compromises\nperformance. Our model demonstrates substantial improve-\nments over existing discrete multimodal methods. Notably,\nthe performance gains can be achieved with minimal com-\nputational overhead, requiring less than 24 hour training on\n8×A100 GPUs using LLaVA 1.5 training data.\n4. Experiments\n4.1. Experimental Setup\nDatasets.\nTokenFlow is trained on LAION [42] and\nCOYO-700M [5] and evaluate it on ImageNet [12]. To en-\nhance face generation quality, we follow [48] and upsample\nthe percentage of images with faces during tokenizer train-\ning by 2 times. For ablation studies, we train the tokenizer\nfor 50 epochs on ImageNet-1K with CLIP ViT-B\/14-224\n[37]. For visual generation with TokenFlow, we trained it\non a curated dataset of 60M high-quality images, with cap-\ntions generated using Qwen-VL [3].\nImplement Details. We employ three variants of Token-\nFlow (B\/L\/XL), using CLIP ViT-B\/14-224 [37], ViTamin-\nXL-256 [8], and SigLIP-SO400M-patch14-384 [69] as\nrespective teacher models and semantic encoder initial-\nizations.\nDetailed configurations are provided in Ap-\npendix A.2.\nFor multimodal understanding, we employ\nVicuna-v1.5-13B [10] and Qwen-2.5-14B [50] as the lan-\nguage backbone. For 256×256 visual generation training,\nwe truncate captions to first sentence with 0.2 probability\nto enhance short prompt generation capabilities. The model\nis initialized with Llama-2-7b [53], and being trained for 2\nepochs. At inference, we apply classifier-free guidance [17]\nwith a scale factor of 7.5.\nEvaluation Metrics. We assess reconstruction quality\nusing rFID, PSNR, and SSIM on the ImageNet-1K valida-\ntion set [12]. For multimodal understanding, we evaluate\non a comprehensive suite of vision-language benchmarks:\nSEEDBench [22], MMVet [67], POPE [26], VQAv2 [16],\nGQA [19], TextVQA [43], AI2D [20], RealWorldQA [61],\nMMMU [68], MMBench [32], and MME [14]. Visual gen-\neration capabilities are evaluated using GenEval [15] and\nDPG-Bench [18]. We opt not to include FID scores as ar-\ngued that it does not correlate well with human assessment\nof the overall performance of generative models [7, 36, 46].\n4.2. Unified Image Tokenizer\nTable 2. Comparison of reconstruction quality on the ImageNet\n50k validation set. “#Lvls.” represents the number of residual lev-\nels used. For 384×384 resolution, the downsample ratio of 14.2 is\nderived from 384\/27.\nModel\nRes.\nratio\n#Lvls.\nrFID ↓\nPSNR ↑\nSSIM ↑\nVQ-GAN [13]\n256\n16\n1\n4.98\n20.00\n0.629\nLlamaGen [44]\n256\n16\n1\n2.19\n20.79\n0.675\nRQ-VAE [21]\n256\n32\n4\n3.20\n–\n–\nRQ-VAE [21]\n256\n16\n4\n1.30\n–\n–\nVAR [51]\n256\n16\n10\n1.00\n22.63\n0.755\nVILA-U [60]\n256\n16\n4\n1.80\n–\n–\nOurs\n256\n16\n9\n1.37\n21.41\n0.687\nLlamaGen [60]\n384\n14.2\n1\n0.94\n21.94\n0.726\nVILA-U [60]\n384\n14.2\n16\n1.25\n–\n–\nVAR [51]\n384\n16\n13\n2.09\n22.73\n0.774\nOurs\n384\n14.2\n15\n0.63\n22.77\n0.731\nIn Tab. 2, we present reconstruction metrics of Token-\nFlow on 256×256 and 384×384 resolutions.\nThe met-\nric of VAR [51] is tested with the released checkpoint.\nAt 256×256 resolution with a 16× compression ratio, To-\nkenFlow achieves competitive performance with an rFID\nof 1.37, comparable to RQ-VAE while significantly out-\nperforming previous methods such as VQ-GAN and Lla-\nmaGen. TokenFlow demonstrates superior reconstruction\nquality across all metrics in 384×384 resolution—a stan-\ndard size in multimodal understanding tasks. These results\nvalidate the effectiveness of dual codebook design in pre-\nserving fine-grained visual details. Moreover, the incorpo-\nration of shared mapping enables TokenFlow to maintain\nhigh-level semantic features, as verified in Sec. 4.3.\n4.3. Multimodal Understanding\nTokenFlow, as a discrete visual encoder, demonstrates\nstate-of-the-art performance across a comprehensive suite\n6\nTable 3. Evaluation on multimodal understanding benchmarks. We collect evaluations including: SEEDB: SEED Bench-Img [22]; MMV:\nMM-Vet [67]; POPE [26]; VQAv2 [16]; GQA [19]; TQA: TextVQA [43]; AI2D [20]; RWQA: RealWorldQA [61]; MMMU [68]; MMB:\nMMBench [32]; MME [14] and MME-P: MME-Perception. We include approaches with continuous visual inputs (top) versus discrete\nvisual inputs (bottom). The best results among approaches with discrete visual input are highlighted in bold. * results are not reported in\noriginal paper and tested with lmms-eval [71] using the released checkpoint. When calculating average, we use MME-P and divide it by\n20 to have the same scale with other benchmarks.\nMethod\n# Params\nRes.\nSEEDB\nMMV\nPOPE\nVQAv2\nGQA\nTQA\nAI2D\nRWQA\nMMMU\nMMB\nMME\nMME-P\nAvg.\nContinuous Visual Input\nInstructBLIP [30]\nVicuna-13B\n224\n58.8\n25.6\n78.9\n–\n49.5\n50.7\n–\n–\n–\n36.0\n–\n1212.8\n–\nMiniGPT-4 [75]\nVicuna-13B\n224\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n1158.7\n866.6\n–\nBLIP-2 [24]\nVicuna-13B\n224\n46.4\n22.4\n–\n–\n–\n42.5\n–\n–\n26.6\n–\n–\n1293.8\n–\nShareGPT4V [9]\nVicuna-7B\n336\n69.7\n37.6\n–\n80.6\n63.3\n60.4\n58.0\n54.9\n37.2\n68.8\n1943.8\n1567.4\n–\nNExT-GPT [58]\nVicuna-7B\n224\n57.5\n–\n–\n66.0\n–\n–\n–\n–\n–\n58.0\n–\n–\n–\nQwen-VL-Chat [3]\nQwen-7B\n448\n57.7\n–\n–\n78.2\n57.5\n–\n–\n–\n–\n–\n1848.3\n1487.5\n–\nJanus [57]\nDeepSeek-LLM-1.3B\n384\n63.7\n34.3\n87.0\n77.3\n59.1\n–\n–\n–\n30.5\n69.4\n–\n1338.0\n–\nLLaVA-1.5 [29]\nVicuna-13B\n336\n68.1\n36.1\n85.9\n80.0\n63.3\n61.3\n61.1\n55.3\n36.4\n67.7\n1826.7\n1531.3\n62.9\nDiscrete Visual Input\nGemini-Nano-1 [49]\n1.8B from scratch\n–\n–\n–\n–\n62.7\n–\n–\n–\n–\n26.3\n–\n–\n–\n–\nChameleon [48]\n34B from scratch\n256\n–\n–\n–\n69.6\n–\n–\n–\n–\n–\n–\n–\n–\n–\nLWM [31]\nLLaMA-2-7B\n256\n–\n9.6\n75.2\n55.8\n44.8\n18.8\n–\n–\n–\n–\n–\n–\n–\nSEED-LLaMA [23]\nLLaMA-2-13B\n224\n53.7\n–\n–\n63.4\n–\n–\n–\n–\n–\n–\n–\n–\n–\nShow-o [62]\nPhi-1.5-1.3B\n256\n–\n–\n80.0\n69.4\n58.0\n–\n–\n–\n26.7\n–\n–\n1097.2\n–\nVILA-U [60]\nLLaMA-2-7B\n256\n56.3\n27.7\n83.9\n75.3\n58.3\n48.3\n–\n–\n–\n–\n–\n1336.2\n–\nVILA-U [60]\nLLaMA-2-7B\n384\n59.0\n33.5\n85.8\n79.4\n60.8\n60.8\n–\n–\n–\n–\n–\n1401.8\n–\nEMU3 [55]\n8B from scratch\n512\n68.2\n37.2\n85.2\n75.1\n60.3\n64.7\n70.0\n57.4\n31.6\n58.5\n1509.9*\n1243.8*\n60.9\nTokenFlow-B\nVicuna-13B\n224\n60.4\n22.4\n84.0\n70.2\n59.3\n49.8\n54.2\n49.4\n34.2\n55.3\n1660.4\n1353.6\n55.2\nTokenFlow-L\nVicuna-13B\n256\n62.6\n27.7\n85.0\n73.9\n60.3\n54.1\n56.6\n49.2\n34.4\n60.3\n1622.9\n1365.4\n57.5\nTokenFlow-XL\nVicuna-13B\n384\n68.7\n40.7\n86.8\n77.9\n62.7\n61.5\n66.7\n53.7\n38.7\n68.9\n1840.9\n1545.9\n64.0\nTokenFlow-XL\nQwen-2.5-14B\n384\n72.6\n48.2\n87.8\n77.6\n62.5\n62.3\n75.8\n56.6\n43.2\n76.8\n1922.2\n1551.1\n67.4\nof multimodal understanding benchmarks.\nFollowing\nLLaVA-1.5’s training pipeline, we train TokenFlow-B and\nTokenFlow-L using LLaVA-Pretrain558K for adapter pre-\ntraining and LLaVA-v1.5-mix-665K for instruction tun-\ning.\nFor TokenFlow-XL, inspired by recent findings in\n[52], we leverage Cambrian-Alignment and Cambrian-10M\nfor pretraining and instruction tuning respectively, as the\nteacher model SigLIP-SO400M benefits significantly from\nincreased training data. As evidenced in Tab. 3, TokenFlow-\nXL achieves competitive or superior results compared to\nleading approaches with continuous inputs from CLIP-\nstyle encoders.\nUsing the same language backbone (Vi-\ncuna 13B), TokenFlow-XL outperforms LLaVA-1.5 13B by\n1.7% on average, for the first time demonstrates that model\nwith discrete visual input can surpass this strong baseline.\nBy simply changing the LLM backbone to Qwen-2.5-14B\n[50], we further surpass LLaVA-1.5 by 7.2%.\nWhen compared to methods using discrete inputs, our\napproach demonstrates superior performance while main-\ntaining training efficiency.\nUnlike models trained from\nscratch such as Chameleon and EMU3, our method re-\nquires less than 24 hour of training on 8×A100 GPUs using\nLLaVA 1.5 data. TokenFlow-XL 14B significantly outper-\nforms EMU3 with an overall improvement of 10.7%. Given\nthese promising empirical results, we position TokenFlow\nas a potential next-generation vision tokenizer for unified\nunderstanding and generation tasks. Our findings suggest\nthat discrete visual representations can not only match but\nexceed the performance of continuous counterparts while\nmaintaining practical training requirements.\n4.4. Visual Generation\nWe\nevaluate\nour\nmodel’s\ngeneration\ncapabilities\nagainst state-of-the-art methods including diffusion-based,\nautoregressive-based, and hybrid approaches on standard\nbenchmarks GenEval [15] and DPG-Bench [18]. As shown\nin Tab. 4, our approach achieves competitive performance\nwhile requiring significantly fewer generation steps.\nFor 256×256 image generation, we employ a multi-\nstep sampling strategy instead of the original 9-step sam-\npling (one per tokenizer scale).\nSpecifically, we apply\nthree steps per scale with top-k=[1200,100,1] and top-\np=[0.8,0.8,1.0] across all scales except the first, to-\ntaling 25 steps. Under this inference scheme, our model\nachieves a GenEval score of 0.55, surpassing prominent\ndiffusion models like Stable Diffusion v2.1 and PixArt-\nalpha. More significantly, it surpasses autoregressive meth-\nods such as Chameleon, LlamaGen, and EMU3, which re-\nquire thousands of inference steps. With prompt rewriting,\nour model achieves 0.63, approaching DALL-E 3’s per-\nformance. On DPG-Bench, it achieves an average score\nof 72.9, outperforming LlamaGen, Show-o, SD v1.5, and\nPixArt-alpha. Moreover, our model only requires 2.7 sec-\nonds to infer one image with 1×A100 GPU, which is signif-\nicantly faster than other autoregressive-based methods.\nWe further conduct additional text-to-image compari-\n7\nTable 4. Comparison of generation quality on GenEval [15] and\nDPG-Bench [18]. ”#Step”: the number of model runs needed to\ngenerate an image. † result is with rewriting.\nModel\nText Pretrain\nRes.\n#Steps\nGenEval\nDPG-Bench\nOverall ↑\nAverage ↑\nDiffusion-based\nSD v1.5 [41]\nCLIP ViT-L\/14\n512\n50\n0.43\n63.18\nDALL-E 2 [39]\nCLIP ViT-H\/16\n1024\n–\n0.52\n–\nSD v2.1 [41]\nCLIP ViT-H\/14\n768\n50\n0.50\n–\nSDXL [36]\nCLIP ViT-bigG\n1024\n40\n0.55\n74.65\nPixArt-alpha [7]\nFlan-T5-XXL\n512\n20\n0.48\n71.11\nDALL-E 3 [4]\nFlan-T5-XXL\n1024\n–\n0.67†\n83.50\nAutoregressive meets diffusion\nShow-o [62]\nPhi-1.5\n256\n16\n0.53\n67.27\nTransfusion [74]\n–\n256\n250\n0.63\n–\nAutoregressive-based\nChameleon [48]\n–\n512\n1024\n0.39\n–\nLlamaGen [44]\nFlan-T5-XL\n512\n1024\n0.32\n64.84\nEMU3 [55]\n–\n512\n4096\n0.54 \/ 0.66†\n80.60\nVAR [51]\n–\n256\n28\n0.53\n71.08\nOurs\n–\n256\n25\n0.55 \/ 0.63†\n73.38\n2.1\n2.15\n2.2\n2.25\n2.3\nReconstruction FID \nReconstruction FID\nGeneration FID\nCodebook Usage\n212\n213\n214\n215\n216\nCodebook Size\n80%\n90%\n100%\nCodebook Usage \n4.5\n5.0\n5.5\n6.0\nGeneration FID \n55\n56\n57\n58\n59\n60\n61\nVLM Score                     \nSEED-Bench\nMME\nText-VQA\n212\n213\n214\n215\n216\nCodebook Size\n48\n49\n50\nFigure 6.\nImpact of codebook size on reconstruction qual-\nity, class-conditional generation, and multimodal understanding\nbenchmarks. MME is divide by 28 to have the same scale.\nson between TokenFlow and the released VAR tokenizer\n[51]. Under identical training configurations and dataset\nsettings, our model consistently demonstrates better perfor-\nmance across all benchmark metrics, this further showcas-\ning the effectiveness of our unified tokenization approach.\n4.5. Ablation Studies\nEffect of Codebook Size. In Fig. 6, we experimented the\nimpact of codebook size in our unified tokenizer, varying\nfrom 8,192 to 131,072. Our evaluation spans reconstruction\nquality, class-conditional generation, and multimodal un-\nderstanding capabilities. For class-conditional generation,\nwe employ the VAR transformer [51] with d=16, resulting\nin approximately 310M parameters. Notably, our approach\nmaintains a consistently high codebook utilization rate ex-\nceeding 95% even with codebook size of 131,072, attributed\nto our shared mapping design. The shared mapping allows\nfor effective combinations of high-level semantic features\nand low-level details, addressing a common limitation of\nTable 5. Impact of key design choices on reconstruction quality\nand multimodal understanding benchmarks. Best results for each\nmetric are highlighted in bold.\nShared Mapping\nMSVQ\nCLIP Init.\nrFID ↓\nMME-P ↑\nSEEDB ↑\nTQA ↑\n8.07\n1252.38\n57.84\n49.16\n3.96\n1212.51\n55.97\n47.42\n2.18\n1209.90\n56.08\n47.40\n2.16\n1312.09\n58.99\n49.29\nconventional VQ tokenizers [13] that typically suffer from\ndeteriorating utilization rates at larger scales.\nOur results reveal that increasing codebook size en-\nhances performance across multimodal understanding\nbenchmarks and reconstruction quality.\nHowever, when\ncodebook size exceeds 32,768, we observe a slight degra-\ndation in class-conditional generation performance. This\nphenomenon can be attributed to the increased complexity\nof learning for autoregressive generation with larger code-\nbooks. Based on this finding, we adopt a codebook size of\n32,768 for our text-to-image generation experiments.\nEffect of Key Design Choice. We validate the effective-\nness of our key design choices in TokenFlow: shared map-\nping, multi-scale vector quantization (MSVQ), and CLIP\ninitialization for the semantic encoder. As shown in Tab. 5,\nwe start with a baseline that uses one single codebook dis-\ntilled from CLIP ViT-B\/14, coupled with a pixel decoder for\ndirect image reconstruction from semantic features. This\nbaseline yields a high reconstruction FID of 8.07, primarily\ndue to the challenge of reconstructing fine-grained pixel de-\ntails solely from semantic features, as visualized in Fig. 8.\nThe introduction of shared mapping (Row 2) enables the\ntwo codebooks to capture high-level and low-level features\nsimultaneously.\nBy weighted distance computation, we\nquantize the input with optimal combinations of high-level\nand low-level features. This design significantly improves\nreconstruction quality (-4.11 rFID) while maintaining com-\nparable understanding capabilities.\nWe further find that incorporating MSVQ [51] (Row\n3) introduces multi-granular information into the codebook\nembeddings, which results in enhanced reconstruction per-\nformance, with rFID of 2.18. Moreover, this hierarchical\ndesign enables a next-scale prediction paradigm in down-\nstream text-to-image generation tasks, offering significant\ninference speed advantages over traditional next-token pre-\ndiction approaches [47, 51]. Initializing the semantic en-\ncoder with pretrained CLIP weights (Row 4) while making\nit unfrozen during tokenizer training provides strong seman-\ntic priors for codebook embeddings. This results in substan-\ntial improvements across all understanding metrics (+8.4%\nin MME-Perception, +5.2% in SEED-Bench, and +4.0%\nin TextVQA). Given these empirical results, we adopt this\nconfiguration as our final model architecture and extend our\nexperiments with stronger teacher models, additional train-\n8\ning data, and longer training iterations.\n5. Conclusion\nIn this work, we introduce TokenFlow, a novel unified\nimage tokenizer that effectively bridges the gap between\nmultimodal understanding and generation through its inno-\nvative dual-codebook architecture. By decoupling semantic\nand pixel-level feature learning while maintaining their\nalignment via shared mapping, TokenFlow successfully\naddresses the fundamental issue between different granular-\nities of visual information required for understanding and\ngeneration tasks. Our comprehensive experiments demon-\nstrate its effectiveness across multiple dimensions: superior\nreconstruction quality at different resolutions, state-of-the-\nart performance in multimodal understanding with minimal\ntraining costs, and competitive visual generation capabili-\nties with substantially fewer inference steps. These results\nvalidate that decoupled yet aligned feature learning through\nour shared mapping can effectively unify understanding\nand generation while maintaining superior performance\nin both domains, suggesting TokenFlow as a promising\nnext-era foundation tokenizer for vision-language systems.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 1\n[2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al.\nQwen technical report.\narXiv preprint\narXiv:2309.16609, 2023. 1\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 6,\n7\n[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al.\nImproving image generation with\nbetter captions.\nComputer Science. https:\/\/cdn. openai.\ncom\/papers\/dall-e-3. pdf, 2(3):8, 2023. 8, 4\n[5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun\nLee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m:\nImage-text pair dataset.\nhttps:\/\/github.com\/\nkakaobrain\/coyo-dataset, 2022. 6\n[6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 3\n[7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al. Pixart-alpha: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 6, 8, 4\n[8] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and\nLiang-Chieh Chen.\nVitamin:\nDesigning scalable vision\nmodels in the vision-language era.\nIn Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12954–12966, 2024. 3, 6, 1, 4, 7\n[9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 7\n[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps:\/\/vicuna. lmsys. org (accessed 14 April 2023), 2(3):6,\n2023. 6\n[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In International Conference on Machine Learning,\npages 7480–7512. PMLR, 2023. 5\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 6\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE\/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 2, 3, 4,\n6, 8, 1, 5\n[14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 3, 6, 7\n[15] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment. Advances in Neural Information Pro-\ncessing Systems, 36, 2024. 6, 7, 8, 2, 4\n[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904–6913, 2017. 6, 7\n[17] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5, 6\n[18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng,\nand Gang Yu.\nElla:\nEquip diffusion models with\nllm for enhanced semantic alignment.\narXiv preprint\narXiv:2403.05135, 2024. 6, 7, 8, 4\n[19] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019. 6, 7\n9\n[20] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images.\nIn Computer Vision–ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part IV 14, pages 235–\n251. Springer, 2016. 6, 7\n[21] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and\nWook-Shin Han.\nAutoregressive image generation using\nresidual quantization. In Proceedings of the IEEE\/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11523–11532, 2022. 3, 6\n[22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 6, 7\n[23] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui\nWang, Ruimao Zhang, and Ying Shan. Seed-bench: Bench-\nmarking multimodal large language models. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 13299–13308, 2024. 3, 7\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 7\n[25] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu,\nBhiksha Raj, and Zhe Lin.\nImagefolder:\nAutoregres-\nsive image generation with folded tokens.\narXiv preprint\narXiv:2410.01756, 2024. 3\n[26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 6, 7\n[27] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-gemini: Mining the potential of multi-modality\nvision language models. arXiv preprint arXiv:2403.18814,\n2024. 1\n[28] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin,\nYu Qiao, Hongsheng Li, and Peng Gao.\nLumina-mgpt:\nIlluminate flexible photorealistic text-to-image generation\nwith multimodal generative pretraining.\narXiv preprint\narXiv:2408.02657, 2024. 3\n[29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 1, 3, 6,\n7\n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 7\n[31] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 7\n[32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player?\nIn European Conference on Computer\nVision, pages 216–233. Springer, 2025. 6, 7\n[33] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin\nWang, and Ying Shan.\nOpen-magvit2: An open-source\nproject toward democratizing auto-regressive visual gener-\nation. arXiv preprint arXiv:2409.04410, 2024. 1\n[34] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun\nZhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-to-\nimage generation via auto-regressive representations. arXiv\npreprint arXiv:2406.10797, 2024. 3\n[35] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu\nWei. Beit v2: Masked image modeling with vector-quantized\nvisual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 3,\n4, 5, 1, 2\n[36] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 6, 8, 4\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 3, 6, 1, 4, 7\n[38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International confer-\nence on machine learning, pages 8821–8831. Pmlr, 2021. 1\n[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 8, 4\n[40] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems, 32, 2019. 3\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 1, 3, 8, 4\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278–25294, 2022. 6\n[43] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 8317–8326, 2019. 3, 6, 7\n[44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 1, 3, 6, 8, 4\n[45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 3\n10\n[46] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 1, 6\n[47] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong\nChen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and\nSong Han. Hart: Efficient visual generation with hybrid au-\ntoregressive transformer. arXiv preprint arXiv:2410.10812,\n2024. 8\n[48] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n2, 3, 5, 6, 7, 8, 4\n[49] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 7\n[50] Qwen Team. Qwen2.5: A party of foundation models, 2024.\n6, 7\n[51] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang.\nVisual autoregressive modeling: Scalable im-\nage generation via next-scale prediction.\narXiv preprint\narXiv:2404.02905, 2024. 1, 3, 5, 6, 8, 4\n[52] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun\nWoo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-\n1: A fully open, vision-centric exploration of multimodal\nllms. arXiv preprint arXiv:2406.16860, 2024. 1, 7\n[53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 5, 6, 2\n[54] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 3\n[55] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is\nall you need. arXiv preprint arXiv:2409.18869, 2024. 2, 3,\n7, 8, 4\n[56] Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett,\nAlex Alemi, Ben Adlam, John D Co-Reyes, Izzeddin Gur,\nAbhishek Kumar, Roman Novak, et al. Small-scale prox-\nies for large-scale transformer training instabilities. arXiv\npreprint arXiv:2309.14322, 2023. 5\n[57] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling visual encoding\nfor unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024. 2, 3, 7\n[58] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. arXiv preprint\narXiv:2309.05519, 2023. 7\n[59] Yiqi Wu, Xiaodan Hu, Ziming Fu, Siling Zhou, and Jiangong\nLi. Gpt-4o: Visual perception performance of multimodal\nlarge language models in piglet activity understanding. arXiv\npreprint arXiv:2406.09781, 2024. 1\n[60] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang,\nDacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model inte-\ngrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024. 2, 3, 6, 7\n[61] XAI. Realworldqa, 2024. 6, 7\n[62] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One\nsingle transformer to unify multimodal understanding and\ngeneration. arXiv preprint arXiv:2408.12528, 2024. 3, 7,\n8, 4\n[63] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation. Advances in Neural Information Pro-\ncessing Systems, 36, 2024. 2\n[64] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu.\nVector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3,\n5\n[65] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 1, 3\n[66] Lijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Ver-\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim\nGupta, Xiuye Gu, Alexander G Hauptmann, et al. Language\nmodel beats diffusion–tokenizer is key to visual generation.\narXiv preprint arXiv:2310.05737, 2023. 3, 1\n[67] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 6, 7\n[68] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi.\nIn Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 9556–\n9567, 2024. 6, 7\n[69] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pages 11975–11986, 2023. 3, 6, 1, 4, 7\n[70] Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shi-\njian Lu. Regularized vector quantization for tokenized im-\nage synthesis. In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 18467–\n18476, 2023. 3\n[71] Kaichen\nZhang,\nBo\nLi,\nPeiyuan\nZhang,\nFanyi\nPu,\nJoshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan\nZhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-\neval: Reality check on the evaluation of large multimodal\nmodels, 2024. 7\n11\n[72] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.\nTinyllama: An open-source small language model, 2024. 2\n[73] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh\nPhung.\nMovq:\nModulating quantized vectors for high-\nfidelity image generation. Advances in Neural Information\nProcessing Systems, 35:23412–23425, 2022. 2, 3\n[74] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024. 8, 4\n[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 7\n[76] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling\nthe codebook size of vqgan to 100,000 with a utilization rate\nof 99%. arXiv preprint arXiv:2406.11837, 2024. 3, 1\n12\nTokenFlow: Unified Image Tokenizer for Multimodal Understanding and\nGeneration\nSupplementary Material\nA. Implementation Details\nA.1. Motivation\nExperimental Setup for Multimodal Understanding.\nTo evaluate the multimodal understanding capabilities of\ncurrent VQ tokenizers, we conduct experiments as detailed\nin Tab. 1. For LFQ [66], we utilize the open-source im-\nplementation [33], which demonstrates comparable perfor-\nmance to the original paper. The codebook size of LFQ is\n262,144. For VQGAN-LC [76], we employ features before\nits projection layer, which is clustered from the pretrained\nCLIP image encoder, with a codebook size of 100,000.\nExperimental\nSetup\nfor\nVisual\nComparison\nof\nVQKD, VQGAN and TokenFlow. To generate the visu-\nalizations in Fig. 4, we perform an experiment using 50,000\nimages from the ImageNet-1k validation set. We process\nthese images through the encoders of VQKD, VQGAN and\nTokenFlow, applying average pooling to the extracted fea-\ntures to obtain a 1 × 1 representation. Subsequently, we\nidentify the closest index in their respective codebooks us-\ning l2 distance. We provide more visualizations in Fig. 11,\nand visualize the cluster size distribution in Fig. 7.\nExperimental Setup for Image Reconstruction from\nQuantized Semantic Feature. We conducted an experi-\nment to reconstruct original images from quantized features\nextracted by VQKD [35]. In this setup, we maintained the\noriginal encoder and quantizer of VQKD, while introduc-\ning an additional decoder aimed at reconstructing the input\nimage. The architecture of this decoder is identical to the\npixel decoder employed in our TokenFlow. We trained this\ndecoder on the ImageNet-1K dataset for 100 epochs. Fig. 8\npresents a visual comparison between the original and the\nreconstructed images. As observed, while the reconstructed\nimages maintain the overall semantic content, they exhibit a\nnoticeable loss of high-frequency details. This phenomenon\nsuggests that the quantized semantic features cannot fully\npreserve fine-grained visual details, which is crucial for vi-\nsual generation.\nA.2. Tokenizer Training Details\nWe\nprovide\ndetailed\ntraining\nconfigurations\nfor\nTokenFlow-B, TokenFlow-L, and TokenFlow-XL vari-\nants in Tab. 11. All models share common hyperparameters\nincluding learning rate, batch size, commitment loss factor,\nadversarial loss factor and distance balance weight. The\nmodels primarily differ in their input resolution (224, 256,\nand 384) and semantic teacher models, utilizing CLIP\n0\n500\n1000\n1500\n2000\n0\n1000\n2000\n3000\nNumber of Images\nTotal non-empty clusters: 2224 (27.1%)\nAverage images per cluster: 22.4\nMaximum images in a cluster: 3077\nVQKD\n0\n50\n100\n150\n200\n0\n2000\n4000\n6000\nNumber of Images\nTotal non-empty clusters: 207 (2.5%)\nAverage images per cluster: 238.0\nMaximum images in a cluster: 6245\nVQGAN\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nCluster Index (Sorted by Size)\n0\n25\n50\n75\n100\nNumber of Images\nTotal non-empty clusters: 7161 (87.4%)\nAverage images per cluster: 7.0\nMaximum images in a cluster: 118\nTokenFlow\nFigure 7. Comparison of cluster size distributions between VQKD\n[35], VQGAN [13], and TokenFlow (ours), with a fixed code-\nbook size of 8,192. Analysis performed on 50,000 images from\nthe ImageNet-1k validation set. TokenFlow exhibits significantly\nsmoother distribution compared to others, attributed to our shared\nmapping design that learns joint distributions of semantic and\npixel-level features. This joint learning approach helps maintain\nhigh codebook utilization (95%+) even with large-scale codebooks\ncontaining over 131K entries.\nViT-B\/14 [37], ViTamin-XL [8], and SigLIP-SO400M\n[69].\nB. Additional Results\nB.1. Additional Ablation Study\nEffect of Sampling Strategy to Visual Generation. We\nconduct comprehensive ablation studies to analyze the im-\n1\nOriginal\nReconstructed\nFigure 8. Comparison of original images and their reconstructions from quantized semantic features extracted by VQKD [35]. The\nreconstructed images preserve the semantic content but exhibit significant loss of high-frequency details.\npact of different sampling strategies on generation quality.\nAs shown in Table 6, we evaluate various configurations\nusing GenEval [15] and ImageReward [63] metrics. We\nchoose ImageReward for ablation due to its strong correla-\ntion with human preferences, particularly in capturing local\nartifacts and overall visual quality. The ImageReward is av-\nerage over 10k prompts from the MS-COCO validation set.\nFor multi-step configurations, we denote the top-p and top-\nk values for each step using bracket notation [x1, ..., xn].\nOur multi-step approach with a two-step strategy (top-\nk=[1200, 1], top-p=[0.8, 0]) significantly improves gen-\neration quality, yielding gains of +0.039 in GenEval and\n+0.084 in ImageReward compared to single-step sampling.\nThis validates our hypothesis that progressive refinement\nhelps maintain global consistency.\nWhen increasing the\nsecond-step k value to 10 or 100 while maintaining top-p,\nwe observe slightly degraded performance. This degrada-\ntion suggests that excessive sampling freedom in refinement\nsteps can lead to increased artifacts and local inconsisten-\ncies.\nMost notably, three-step strategy (top-k=[1200, 100, 1],\ntop-p=[0.8, 0.8, 0]) achieves the best performance across\nboth metrics.\nThis represents substantial improvements\nof 10.2% and 14.3% over traditional single-step sampling,\nrespectively.\nThe gradual narrowing of sampling space\n(1200→100→1) strikes a balance between generation di-\nversity and local consistency. As illustrated in Figure 5,\nour multi-step approach produces more coherent and visu-\nally appealing results. These quantitative and qualitative re-\nsults demonstrates that progressive refinement in top-p top-\nk sampling is crucial for high-quality generation in next-\nscale prediction frameworks.\nEffect of Model Size to Visual Generation. We con-\nduct ablation studies to investigate the impact of model\nsize on our decoder-only visual generation architecture.\nSpecifically, we initialize our framework with two differ-\nent backbone models: TinyLlama-1B [72] and Llama-2-7B\nTable 6.\nImpact of sampling strategy to visual generation.\nWe compare single-step v.s. multi-step sampling strategy using\nGenEval and ImageReward. For multi-step approaches, values in\nbrackets indicate parameters for successive sampling steps.\nStrategy\nTop-k\nTop-p\nGenEval ↑\nImageReward ↑\nSingle Step\n1200\n0.8\n0.502\n0.722\nMulti Step\n[1200, 1]\n[0.8, 0]\n0.541\n0.806\n[1200, 10]\n[0.8, 0.8]\n0.531\n0.799\n[1200, 100]\n[0.8, 0.8]\n0.529\n0.745\n[1200, 100, 1]\n[0.8, 0.8, 0]\n0.553\n0.825\nTable 7. Impact of model size to visual generation.\nModel size\nTraining epoches\nGenEval ↑\nImageReward ↑\n1B\n4\n0.485\n0.677\n7B\n2\n0.553\n0.825\nTable 8. Impact of different input strategies on multimodal under-\nstanding. Best results for each metric are highlighted in bold.\nInput strategy\nMME ↑\nMME-P ↑\nSEEDB ↑\nTQA ↑\nFull scale\n1610.1\n1315.1\n59.6\n49.5\nFull scale residual\n1527.5\n1216.5\n57.0\n48.1\nLast scale semantic feat. only\n1580.3\n1315.6\n60.1\n49.7\nLast scale\n1634.3\n1356.5\n59.9\n49.1\n[53]. Experiments demonstrate that model size plays a cru-\ncial role in generation performance. As shown in Tab. 7\nand Fig. 9, under identical sampling strategies and training\ndataset configurations, the 1B model significantly underper-\nforms compared to its 7B counterpart, even with doubled\ntraining epochs.\nEffect of Input Strategy to Multimodal Understand-\ning. We validate different feature input strategies for multi-\n2\n7B\n1B\nFigure 9. Qualitative comparison of visual generation capabilities\nbetween 1B and 7B models. Prompts (from left to right): (1) ”A\npizza sitting on top of a wooden cutting board”, (2) ”Television\nset being held by a hand”, (3) ”The guy is nicely dressed in a suit\nand tie”, and (4) ”A sailing ship rests on waters”. The 7B model\ndemonstrates enhanced quality compared to its 1B counterpart.\nmodal understanding with TokenFlow. As shown in Tab. 8,\nfinal-scale features consistently outperform both full-scale\nfeatures and full-scale residual features across all bench-\nmarks. This suggests that the final scale captures the most\nrelevant semantic information for multimodal understand-\ning, while additional scale features or residual features may\nintroduce noise that compromises performance. Our exper-\niments also reveal that utilizing semantic features only does\nnot improve the overall understanding performance.\nEffect of Tokenizer Decoder Finetuning. To further\nimprove our model’s ability to generate fine details, we fol-\nlow [6] and double both the number of residual layers and\nchannel dimensions in the decoder. We exclusively fine-\ntune these enhanced decoder layers while keeping all other\ncomponents frozen, thereby preserving the learned visual\ntoken mappings. This enables us to improve reconstruction\nfidelity without compromising perception ability of Token-\nFlow. As shown in Fig. 10, the enhanced decoder yields\nnotable improvements in reconstruction quality. It demon-\nstrates superior preservation of high-frequency details, par-\nticularly in facial details and text elements.\nB.2. More Analysis of TokenFlow\nAnalysis of Joint Distribution Learning. To evaluate\nthe effectiveness of our shared mapping mechanism, we\nconduct comparative experiments against VQKD [35] and\nVQGAN [13].\nAll models are configured with identical\ncodebook sizes of 8,192 tokens for fair comparison. For\nbaseline models, we utilize the official pretrained check-\npoints from [35] and [48], respectively. Our TokenFlow\nmodel is trained on ImageNet-1K for 50 epochs. We delib-\nerately excludes the multi-scale VQ design [51] to isolate\nthe effects of the shared mapping in this experiment.\nFor evaluation, we process 50,000 images from the\nImageNet-1K validation set through each model’s encoder.\n(a)\n(b)\n(c)\nFigure 10. Comparison of image reconstruction quality. (a) Orig-\ninal images. (b) Reconstructions using the base pixel decoder.\n(c) Reconstructions using the enhanced (2× capacity) decoder.\nThe enhanced decoder demonstrates superior preservation of fine-\ngrained details, particularly in facial details and textual elements.\nWe apply average pooling to the extracted features to obtain\na 1 × 1 representation, and then identify the closest index\nin their respective codebooks using l2 distance. As shown\nin Fig. 7, TokenFlow exhibits significantly smoother dis-\ntribution against compared to others. The total non-empty\nclusters of TokenFlow are 7161\/8192 (87.4%), which is sig-\nnificantly larger than that of VQGAN (2.5%) and VQKD\n(27.1%). These results demonstrate that our shared map-\nping design enables effective learning of joint distributions\nacross high-level semantic and low-level pixel representa-\ntions. By simultaneously encoding multiple levels of visual\ninformation, we induces a joint representation space com-\npared to single-representation architectures. This directly\ncontributes to the superior codebook utilization observed in\nour experiments. Even when expanding the codebook to\nover 131K entries, TokenFlow maintains an exceptional uti-\nlization ratio exceeding 95%. The clustered results is shown\nin Fig. 11.\nAutomatic Balancing between Semantic Distance and\nPixel Distance. In our structure, the optimal quantize index\nis determined by arg mini(dsem,i + wdis · dpix,i). There ex-\nists an automatic balancing mechanism between semantic\ndistance and pixel distance. For instance, when encoun-\ntering a case where dsem,i is relatively small while dpix,i is\nlarge, during backpropagation, both commit loss and per-\nceptual loss will contribute to reducing the distance between\nthe encoded features and their quantized counterparts. This\nmechanism naturally narrows the gap between these two\ndistance metrics. Therefore, we set wdis to 1.0 across all\nexperiments.\nComparison between TokenFlow and their corre-\nsponding semantic teachers.\nTable 9 presents a fair\n3\nTable 9. Quantitative comparison of multimodal understanding capabilities between our discrete TokenFlow and their corresponding\ncontinuous semantic teachers. All experiments are trained with LLaVA-1.5 data for fair comparison. When calculating average, we use\nMME-P and divide it by 20 to have the same scale with other benchmarks.\nMethod\n# Params\nVisual Encoder\nRes.\nSEEDB\nMMV\nPOPE\nVQAv2\nGQA\nTQA\nAI2D\nRWQA\nMMMU\nMMB\nMME\nMME-P\nAvg.\nContinuous Visual Input\nLLaVA-1.5\nVicuna-13B\nCLIP ViT-B\/14 [37]\n224\n64.1\n30.8\n85.1\n73.8\n61.3\n53.4\n57.8\n50.9\n35.1\n62.0\n1737.0\n1460.9\n58.9\nViTamin-XL [8]\n256\n65.7\n34.6\n85.8\n76.8\n62.6\n57.4\n59.4\n54.4\n35.0\n66.4\n1839.1\n1514.5\n61.3\nSigLIP-SO400M [69]\n384\n67.5\n38.1\n86.5\n78.6\n63.8\n62.2\n59.5\n57.4\n35.4\n68.3\n1802.1\n1488.2\n62.9\nDiscrete Visual Input\nOurs\nVicuna-13B\nTokenFlow-B\n224\n60.4\n22.4\n84.0\n70.2\n59.3\n49.8\n54.2\n49.4\n34.2\n55.3\n1660.4\n1353.6\n55.2 (93.7%)\nTokenFlow-L\n256\n62.6\n27.7\n85.0\n73.9\n60.3\n54.1\n56.6\n49.2\n34.4\n60.3\n1622.9\n1365.4\n57.5 (93.8%)\nTokenFlow-XL\n384\n65.3\n41.2\n86.2\n76.6\n63.0\n57.5\n56.8\n53.3\n34.7\n62.7\n1794.4\n1502.3\n61.1 (97.1%)\nTable 10. Comparison of generation quality on GenEval and DPG-Bench. Obj.: Object. Attri.: Attribute. † result is with rewriting.\nGenEval\nDPG-Bench\nMethod\nOverall\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall\nGlobal\nEntity\nAttribute\nRelation\nOther\nDiffusion-based\nSDv1.5 [41]\n0.43\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n63.18\n74.63\n74.23\n75.39\n73.49\n67.81\nDALL-E 2 [39]\n0.52\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n–\n–\n–\n–\n–\n–\nSDv2.1 [41]\n0.50\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n–\n–\n–\n–\n–\n–\nSDXL [36]\n0.55\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n74.65\n83.27\n82.43\n80.91\n86.76\n80.41\nPixArt-alpha [7]\n0.48\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n71.11\n74.97\n79.32\n78.60\n82.57\n76.96\nDALL-E 3 [4]\n0.67†\n0.96†\n0.87†\n0.47†\n0.83†\n0.43†\n0.45†\n83.50\n90.97\n89.61\n88.39\n90.58\n89.83\nAutoregressive meets diffusion\nShow-o [62]\n0.53\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n67.27\n79.33\n75.44\n78.02\n84.45\n60.80\nTransfusion [74]\n0.63\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\nAutoregressive-based\nChameleon [48]\n0.39\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\nLlamaGen [44]\n0.32\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n64.84\n81.76\n75.43\n76.17\n84.76\n58.40\nEMU3 [55]\n0.54\n0.98\n0.71\n0.34\n0.81\n0.17\n0.21\n80.60\n85.21\n86.68\n86.84\n90.22\n83.15\nVAR [51]\n0.53\n0.95\n0.60\n0.41\n0.81\n0.16\n0.24\n71.08\n77.51\n78.17\n77.80\n85.80\n62.00\nOurs\n0.55\n0.97\n0.66\n0.40\n0.84\n0.17\n0.26\n73.38\n78.72\n79.22\n81.29\n85.22\n71.20\n0.63†\n0.93†\n0.72†\n0.45†\n0.82†\n0.45†\n0.42†\ncomparison between our discrete TokenFlow variants and\ntheir corresponding semantic teachers under the LLaVA-1.5\ntraining paradigm. TokenFlow exhibits a relative perfor-\nmance gap compared to its semantic teachers due to vec-\ntor quantized distillation. However, this gap diminishes as\nresolution increases: from 6.3% at 224×224 to 6.2% at\n256×256, and finally to 2.9% at 384×384. This improve-\nment can be attributed to the increased number of discrete\ntokens and additional scales supplementing the residual fea-\ntures at higher resolutions.\nB.3. More Visual Generation Results\nQuantitative Results. In Tab. 10, we present the com-\nplete scores for both GenEval [15] and DPG-Bench [18].\nFollowing DALL-E 3 [4], we report our GenEval results\nusing GPT-4V as a rewriter. For DPG-Bench, we tested\nthe results of LlamaGen and Show-o using their released\ncheckpoints. We compare against VAR [51] by using their\nreleased tokenizer and training the visual generation model\nunder identical settings to ensure fair comparison.\nQualitative Results. We present additional visual gen-\neration results in Fig. 12. Our method can generate images\nwith various styles, subjects, and scenarios.\nC. Limitation and Future Work\nA primary limitation of TokenFlow lies in the perfor-\nmance gap in multimodal understanding between our dis-\ncrete tokenizer and its continuous semantic teacher, which\nstems from the vector quantization distillation process.\nWhile this gap narrows to 2.9% at 384×384 resolution, sev-\neral methods remain for further improvement, such as in-\ncorporating text alignment loss during tokenizer training.\nIn this work, we primarily focused on designing Token-\nFlow and validating its effectiveness separately in multi-\nmodal understanding and visual generation tasks. A natural\nextension of this work is the development of a fully unified\nmodel for both multimodal understanding and generation.\nThis unification can be achieved through joint training on\ninterleaved vision-language data. This is currently in our\nhigh priority for exploration.\n4\nVQKD\nVQGAN\nTokenFlow\nFigure 11. Qualitative comparison of images clustered by VQKD [35], VQGAN [13] and our TokenFlow. VQKD clusters exhibit semantic\nsimilarity, while VQGAN clusters exhibit low-level similarity (i.e. color and texture). Our TokenFlow can successfully combine both\nsemantic and low-level similarity (e.g. birds with different background can be mapped into two different index).\n5\nA picture of the head of a brown \ncow wearing a halter.\nA bedroom with a white bed \non a frame next to a window.\nAman with long hair with a pizza \nin front of him on the table.\nA duck floating on a lake \nwith gray and black feathers.\nA toy smiley face in the \nmiddle of a doughnut.\nA man with a bald head wearing a \npair of glasses.\nA couple of vehicles are side by \nside.\nA breakfast of croissant and \ncoffee sits on a table.\nA photo of a man holding a sign \nwith text 'FLOW'.\nAn elephant walking under the sea.\nA photo of a purple backpack and \na white umbrella.\nA photo of a potted plant.\nA photo of two wine glasses.\nA photo of a yellow tv remote.\nA photo of a red apple.\nA realistic landscape shot of the \nNorthern Lights dancing over a \nsnowy mountain range in Iceland.\nA handsome 24 years old boy \nin the middle with sky color \nbackground wearing eye \nglasses, it's super detailed \nwith anime style.\nHappy dreamy owl monster \nsitting on a tree branch, colorful \nglittering particles, forest \nbackground, detailed feathers.\nCrocodile in a sweater.\nA deep forest clearing with a \nmirrored pond reecting a \ngalaxylled night sky.\nAn astronaut riding a horse on the \nmoon, oil painting by Van Gogh.\nA vivid green iguana is \nperched motionlessly atop a \nworn wooden log, its intricate \nscales exhibiting various \nshades of green and black.\nAn intricately detailed \nrepresentation of the Marvel \ncharacter Ghost Rider featuring a \nhuman skull, with flames licking \naround the contours of the skull \nand rising above it in a fierce \nexpression of fiery vengeance.\nA vibrant yellow 2017 Porsche \n911 is captured in motion, \nnavigating a winding mountain \nroad with its sleek body hugging \nthe curve.\nA lighthouse in a giant wave, \norigami style.\nFigure 12. More Visual Generation Results with TokenFlow. We present diverse 256×256 results across various styles, subjects, and\nscenarios.\n6\nTable 11. Detail settings of TokenFlow-B, TokenFlow-L and TokenFlow-XL.\nTokenizer\nTokenFlow-B\nTokenFlow-L\nTokenFlow-XL\nTokenizer settings:\nInput resolution\n224\n256\n384\nCodebook size\n32,768\n32,768\n32,768\nSemantic teacher\nCLIP ViT-B\/14-224 [37]\nViTamin-XL-256 [8]\nSigLIP-SO400M-patch14-384 [69]\nMulti-scale settings\n[1, 2, 4, 6, 8, 10, 12, 14]\n[1, 2, 3, 4, 6, 8, 10, 12, 14, 16]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 22, 27]\nSemantic codebook embedding dimension\n32\n32\n32\nPixel codebook embedding dimension\n8\n8\n8\nTraining settings:\nLearning rate\n1e-4\n1e-4\n1e-4\nBatch size\n256\n256\n256\nTraining steps\n1,000,000\n500,000\n500,000\nDistance balance weight wdis\n1.0\n1.0\n1.0\nCommitment loss factor β\n0.25\n0.25\n0.25\nAdversarial loss factor λG\n0.5\n0.5\n0.5\nMax gradient norm\n1.0\n1.0\n1.0\n7\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation.pdf"}
{"title":"LoRaFlow: High-Quality Signal Reconstruction using Rectified Flow","authors":"Mohamed Osman, Tamer Nadeem","summary":"LoRa technology, crucial for low-power wide-area networks, faces significant\nperformance degradation at extremely low signal-to-noise ratios (SNRs). We\npresent LoRaFlow, a novel approach using rectified flow to reconstruct\nhigh-quality LoRa signals in challenging noise conditions. Unlike existing\nneural-enhanced methods focused on classification, LoRaFlow recovers the signal\nitself, maintaining compatibility with standard dechirp algorithms. Our method\ncombines a hybrid neural network architecture, synthetic data generation, and\nrobust augmentation strategies. This minimally invasive enhancement to LoRa\ninfrastructure potentially extends operational range and reliability without\noverhauling existing systems. LoRaFlow opens new possibilities for robust IoT\ncommunications in harsh environments and its core methodology can be\ngeneralized to support various communication technologies.","url":"http:\/\/arxiv.org\/abs\/2501.00024v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2501.00024v1","published":1734451528000,"comment":null,"pdf_text":"LoRaFlow: High-Quality Signal Reconstruction\nusing Rectified Flow\nMohamed Osman\nDepartment of Computer Science\nVirginia Commonwealth University\nRichmond, USA\nosmanmw@vcu.edu\nTamer Nadeem\nDepartment of Computer Science\nVirginia Commonwealth University\nRichmond, USA\ntnadeem@vcu.edu\nAbstract—LoRa technology, crucial for low-power wide-area\nnetworks, faces significant performance degradation at extremely\nlow signal-to-noise ratios (SNRs). We present LoRaFlow, a\nnovel approach using rectified flow to reconstruct high-quality\nLoRa signals in challenging noise conditions. Unlike existing\nneural-enhanced methods focused on classification, LoRaFlow\nrecovers the signal itself, maintaining compatibility with standard\ndechirp algorithms. Our method combines a hybrid neural\nnetwork architecture, synthetic data generation, and robust\naugmentation strategies. This minimally invasive enhancement\nto LoRa infrastructure potentially extends operational range\nand reliability without overhauling existing systems. LoRaFlow\nopens new possibilities for robust IoT communications in harsh\nenvironments and its core methodology can be generalized to\nsupport various communication technologies.\nIndex Terms—LoRA, Chirp, Signal Reconstruction, SNR, Dif-\nfusion, Rectified Flow\nI. INTRODUCTION\nThe rapid proliferation of the Internet of Things (IoT) has\nspurred the need for reliable, long-range, low-power commu-\nnication technologies. IoT applications span across various\ndomains, including smart cities, agriculture, industrial automa-\ntion, and home automation, all of which demand efficient and\nrobust communication networks [1].\nLow-Power Wide-Area Networks (LPWANs) have become\nessential for these applications, offering extensive coverage\nand low power consumption. However, ensuring reliable com-\nmunication in environments with low Signal-to-Noise Ratios\n(SNRs) is a significant challenge. This challenge is further\ncomplicated by the trade-offs between power consumption,\nrange, and data rate inherent in LPWANs [2], [3].\nPrevious efforts to improve LoRa signal reception have\nincluded methods such as multiple gateway diversity, ad-\nvanced signal processing techniques, and machine learning\n(ML) solutions [4]–[10]. However, these approaches often\nnecessitate dense gateway deployments, significant hardware\nmodifications, or fail to perform optimally in highly noisy\nenvironments.\nIn this paper, we introduce LoRaFlow, a novel approach that\nleverages advanced generative modeling techniques, specif-\nically diffusion transformers and rectified flow, to perfectly\nreconstruct the original LoRa signal from noisy inputs, thereby\nenhancing LoRa signal demodulation. The key components of\nthe LoRaFlow framework include a neural signal enhancement\nmodule seamlessly integrated with existing LoRa infrastruc-\nture.\nLoRaFlow offers several advantages: the ability to recover\nhigh-fidelity signals under extremely low SNR conditions,\ncompatibility with existing LoRa systems, and minimal hard-\nware changes. This method has the potential to extend the\nrange and reliability of LoRa networks, thereby improving\noverall communication performance in IoT deployments.\nThe main contributions of this paper are as follows:\n• We introduce a powerful state-of-the-art rectified flow based\nLoRa signal enhancement model and a novel training\nmethodology. Our approach integrates smoothly with exist-\ning LoRa software stack and does not require removing any\ncomponents unlike previous approaches. Our methodology\nalso requires extremely minimal data collection for training.\nFor example, we use roughly 80 times less data than\nprevious methods [10] at Spreading Factor 7.\n• We thoroughly evaluate our approach and demonstrate\nalmost flawless signal reconstruction in both phase and\namplitude. Our signals are sufficiently perfect to defeat\nclassification-based approaches when decoding with the\nsimple default dechirp algorithm.\n• We publish our complete code, model checkpoints, com-\nprehensive evaluation results, etc. publicly under an open\nsource license.\nThe remainder of this paper is organized as follows. Sec-\ntion II provides an overview of LoRa signals and related work\nin enhancing LoRa signal reception. Section III details the\nfundamentals of diffusion models and rectified flow, which\nare integral to our approach, and shows an overview of\nthe LoRaFlow framework. Sections IV and V describe the\narchitecture and the training methodology of the LoRaFlow\nmodel, respectively. Section VI presents a comprehensive set\nof results demonstrating the effectiveness of our approach.\nFinally, Section VII concludes the paper and discusses future\nresearch directions.\nII. BACKGROUND & RELATED WORK\nA. LoRa Fundamentals\nLoRa (Long Range) is a wireless technology optimized\nfor low-power, long-distance data transmission, essential for\nIoT deployments in various sectors such as smart cities,\nagriculture, and industry. It utilizes the LoRaWAN network\narchitecture involving end devices, gateways, network servers,\nand application servers, facilitating data integrity, security,\nand efficient routing [1]–[3]. Operating on sub-gigahertz unli-\ncensed bands (433 MHz, 868 MHz, 915 MHz), LoRa employs\narXiv:2501.00024v1  [eess.SP]  17 Dec 2024\nChirp Spread Spectrum (CSS) modulation [11] that spreads\nthe signal over a wider bandwidth. The process of chirping in\nLoRa involves varying the frequency of the signal over time in\na linear fashion, either upwards or downwards. This technique\nis characterized by two key parameters: the Spreading Factor\n(SF) and the Bandwidth (BW).\nTransmitted binary bits in LoRa are segmented into subse-\nquences of length SF, where SF ∈[7, 12], forming symbols\nwith M = 2SF possible variations. The symbol rate Rs is\ndefined as Rs = Rb\nSF , and the symbol duration T as T = 2SF\nBW ,\nwith BW options of 125 kHz, 250 kHz, or 500 kHz affecting\ndata rates and signal robustness. Lower SF provides higher\ndata rates and lower power consumption but is less reliable\nin terms of SNR and has a shorter range. Higher SF, on the\nother hand, enhances SNR reliability and range but at the cost\nof lower data rates and higher power consumption. Similarly,\nlower BW improves range and robustness but reduces data\nrate, while a higher BW increases data rate but decreases range\nand robustness.\nGiven the chirp rate k, defined as the rate at which the chirp\nfrequency varies over time, and formulated as k =\nBW\nT\n=\nBW 2\n2SF\nand fc is the carrier frequency, the form of the base\nchirp can be formulated as [12]:\nS(t) = ej2πfc(−BW\n2\n+ kt\n2 )t,\nt ∈\n\u0014\n−T\n2 , T\n2\n\u0015\n(1)\nLoRa modulation uniquely maps each of the M symbols to\na distinct chirp. The chirp for the mth symbol is generated by\ntime-shifting the base chirp by τm =\nm\nBW , where τm =\ni\n2SF ·\nT. Any part of the chirp outside\n\u0002\n−T\n2 , T\n2\n\u0003\nis cyclically adjusted\nback into the interval. Figure 1 provides a visual example of\nthis modulation process.\nAt the receiver, the chirp signal is dechirped by mixing it\nwith a reference chirp, converting frequency variations into\na baseband signal for demodulation. The process includes\nmultiplying by the conjugate of the reference chirp and using\nFast Fourier Transform (FFT) to detect peak frequencies of\ntransmitted symbols. In low SNR conditions, signal quality\ndegrades, particularly over long distances due to path loss,\nchallenging reliable communication. Efficiently reconstructing\nthe noisy received signal is essential for accurate dechirping\nand decoding, crucial for energy conservation in low-power\nIoT devices [1]–[3].\nB. Related Work\nRecent efforts to enhance the reception of LoRa signals\nunder low SNR conditions have focused on leveraging multiple\ngateways, hardware diversity, and advanced signal processing\ntechniques. Four notable approaches are Charm, OPR, Chime,\nand Choir. Charm improves energy efficiency by using the\nspatial diversity of multiple gateways to decode weak chirp\nsymbols through coherent combining, achieving 1-3 dB SNR\ngain with 2-8 gateways per node [4]. OPR explores disjoint\nlink-layer bit errors across multiple gateways to recover cor-\nrupted packets, resulting in 1.5-2.5 dB SNR gain with 2-6\ngateways per node [5]. Chime utilizes heartbeat packets and\nthree gateways to estimate wireless channel states and select\noptimal frequencies for transmission, achieving 2.4-3.4 dB\nSNR gain with 4-6 gateways per node [6]. Choir leverages\nhardware diversity with up to 36 co-located LoRa nodes to\nboost received signal strength but does not provide specific\nSNR gain values [7]. These approaches, however, require\ndense deployment of gateways and nodes, which may not be\ncost-effective or feasible in all scenarios.\nWhile recent advancements in machine learning (ML) have\nopened new avenues for signal procssing, very few works\nconsidered LoRa signals. DeepLoRa [8] employs a Bi-LSTM\nDNN to create a land-cover-aware path loss model, reducing\nestimation error to less than 4 dB. DeepSense [9] extends this\napproach, exploring deep learning-augmented random access\nfor LPWAN coexistence, even under extreme noise conditions\n(e.g., -10 dB). A very recent work NELoRa [10] introduces\na neural-enhanced demodulation method that exploits deep\nlearning to support ultra-low SNR LoRa communication.\nThe architecture of NELoRa comprises two key components:\na mask-enabled Deep Neural Network (DNN) filter and a\nspectrogram-based DNN decoder. The filter aims to recover\nclean chirp symbols by masking their noisy input spectrogram,\neffectively separating the signal from noise. Subsequently, the\ndecoder classifies the recovered chirp symbols, exploiting the\nfinite coding space of LoRa to its advantage. This method\nachieves SNR gains of 1.84-2.35 dB, outperforming traditional\ndechirp methods and other ML-based approaches by lowering\nthe SNR threshold for chirp symbol decoding.\nOur work builds upon these foundations, taking a funda-\nmentally different approach. Instead of focusing on classifi-\ncation, we aim to recover the LoRa signal itself at extremely\nlow SNRs using rectified flow, a diffusion-based generative\nmodeling technique. This signal recovery approach offers the\npotential to operate at even lower SNRs than NELoRa, while\nmaintaining compatibility with existing dechirp algorithms. By\nreconstructing the signal prior to classification, our method\nopens up new possibilities for pushing the boundaries of LoRa\ncommunication in challenging environments.\nIII. LORAFLOW FRAMEWORK OVERVIEW\nIn this section, we present the LoRaFlow framework starting\nby discussing the core approach of LoRaFlow, followed by\nan in-depth look at the fundamental diffusion models and\nrectified flow mechanisms that underpin its operation. Finally,\nwe illustrate the practical implementation of LoRaFlow within\nthe current LoRa network architecture.\nA. LoRaFlow Approach\nEnhancing signal reception under extremely low Signal-to-\nNoise Ratio (SNR) conditions remains a significant challenge\nin LoRa communication. Prior works primarily focus on\nclassifying received noisy signals into corresponding symbols.\nAlthough effective to some extent, these methods often strug-\ngle in scenarios with severe signal degradation, limiting the\nreliability and range of LoRa networks. Our LoRaFlow frame-\nwork addresses this issue by reconstructing the received noisy\nLoRa signal into a full, denoised signal at the LoRa gateways.\nComplete reconstruction is a substantially harder task than\nclassification. By achieving strong signal reconstruction, our\nmethod can significantly enhances communication reliability\nunder extreme noise conditions.\nForward Process (adding noise)\nReverse Process (denoising)\nAmplitude\nPhase\nFig. 1: Overview of the iterative refinement process using diffusion models for LoRa signals. The forward process adds noise\n(defined in closed-form), while the reverse process denoises the signal using our model. Signals are presented as STFTs where\nthe top row represents phase while the bottom row represents amplitude.\nReconstructing the entire transmitted signal significantly en-\nhances communication systems, improving accuracy, security,\nand overall performance for a range of applications. This\ncomprehensive approach ensures precise data representation,\nreduces transmission errors, and bolsters signal integrity even\nin adverse conditions. Full signal access supports robust secu-\nrity protocols, including advanced authentication and detailed\nsignal fingerprinting, while also extending operational ranges\nand improving data recovery in challenging environments.\nSophisticated signal processing and decoding techniques are\nenabled, optimizing network resource utilization and sup-\nporting high-reliability applications such as remote sensing\nand autonomous vehicles. Additionally, this method enhances\nthe system’s resilience to sophisticated attacks like spoofing\nand replay, facilitating accurate transmitter identification and\nenabling precise forensic analysis in security breach scenarios.\nLoRaFlow approach in full signal reconstruction pushes the\nboundaries of LoRa communication, offering a significant\nadvancement in the field and opening new avenues for research\nand application in low-power, long-range communication tech-\nnologies.\nB. Fundamentals of LoRaFlow: Diffusion Models and Recti-\nfied Flow\nAchieving high-fidelity signal recovery at extremely low\nSNRs necessitates the use of sophisticated generative models\ncapable of reconstructing signals buried in noise. Diffusion-\nbased generative models have recently emerged as powerful\ntools for synthesizing complex data distributions. These mod-\nels operate by progressively corrupting data with noise and\nthen learning to reverse this process to generate samples by\niteratively denoising random noise.\nThe core principle behind this reversal process is the score\nfunction, the gradient of the log probability density with\nrespect to the data. Knowing the score function allows the\nmodel to move from regions of low probability (high noise)\ntowards regions of high probability (low noise). Early diffusion\nmodels, such as Noise Conditional Score Networks (NCSN)\n[13] and Denoising Diffusion Probabilistic Models (DDPM)\n[14], introduced methods for learning these score functions at\ndiscrete noise levels.\nSong et al. [15] generalized this approach using Stochastic\nDifferential Equations (SDEs), which allow for continuous-\ntime diffusion processes. The forward SDE is given by:\ndxt = f(xt, t)dt + g(t)dwt,\n(2)\nwhere xt represents the signal at time t, f(xt, t) is the drift\ncoefficient, g(t) is the diffusion coefficient, and dwt represents\nan infinitesimal increment of a standard Wiener process. The\ncorresponding reverse-time SDE is:\ndxt = [f(xt, t) −g(t)2∇x log pt(xt)]dt + g(t)d ¯wt,\n(3)\nwhere ¯wt is a Wiener process in reverse time, and pt(xt)\ndenotes the probability density of xt. This reverse-time SDE\ndepends on the score function and is crucial for the gen-\nerative process. To address the computational demands of\nsolving these equations, the concept of a Probability Flow\n(PF) Ordinary Differential Equation (ODE) was introduced.\nThe PF ODE provides a deterministic counterpart to the\nstochastic diffusion process, allowing for more efficient sample\ngeneration. The PF ODE is given by:\ndxt\ndt = f(xt, t) −1\n2g(t)2∇x log pt(xt),\n(4)\nRectified flow [16], [17] further improves this by learning\na mapping with straight-line trajectories. This is achieved by\nminimizing the objective:\nmin\nv\nZ 1\n0\nE\n\u0002\n∥X1 −X0 −v(tX1 + (1 −t)X0, t)∥2\n2\n\u0003\ndt,\n(5)\nwhere (X0, X1) is a coupling of the noise and data distri-\nbutions, and v is the velocity field of the ODE. This approach\ndirectly maps noise to data, reducing the number of steps\nneeded for high-fidelity signal recovery, making it particularly\nsuitable for low SNR conditions.\nArchitectural choices are crucial for the performance of\ndiffusion models. While U-Net architectures have been widely\nused due to their ability to process spatial information effi-\nciently at multiple scales, transformer-based architectures like\nDiffusion Transformers (DiT) introduced by Peebles and Xie\n[18] have shown remarkable scalability and efficiency. DiT\nleverages the self-attention mechanism to capture long-range\ndependencies, making it highly suitable for high-dimensional\ndata processing.\nGiven the strengths of both U-Net and transformer-based\narchitectures, hybrid approaches combining elements of both\ndesigns offer significant advantages. Our approach integrates\nDiT blocks with convolutional down\/upsampling layers. This\nhybrid architecture leverages the scalability and flexibility of\ntransformers while retaining the U-Net’s ability to process\nmulti-scale spatial information effectively. Further, it allows\nus to partially defeat the Transformer’s quadratic compute\ncomplexity along the sequence length. By exploring this\nhybrid space, we aim to achieve superior performance in\nhandling complex signal recovery tasks with improved quality\nand computational efficiency.\nAn example of our iterative refinement process for LoRa\nsignals is illustrated in Figure 1. The figure shows the forward\nprocess (top row) and the reverse process (bottom row). In the\nforward process, noise is progressively added to the signal,\ntransitioning from a high SNR to a low SNR state. This\nillustrates how the original signal is corrupted over time. The\nreverse process then denoises the signal, iteratively refining it\nback to a high-fidelity state. This illustrates how the model\nreconstructs the original signal from noisy data.\nDuring inference, the reverse process is discretized into\nN steps marked by unique t values, each of which requires\na neural function evaluation (NFE) which is one forward-\npropagation of the model in our case.\nTo map SNRs to the time parameter t in our model, we use\nthe following function:\nt =\n√\nSNR\n1 +\n√\nSNR\n,\nwhere SNR = 10SNRdB\/10\n(6)\nThis mapping allows us to associate different noise levels\nwith specific points in the ODE trajectory. In our inference\nscenario, we’re able to skip an appropriate amount of noise\nsteps and allows us to ”insert” the received samples at the\nappropriate place in the ODE. This in turn leads to a natural\nproperty where the less noisy a signal is, the faster our model\nexecutes and vice versa.\nOur LoRaFlow framework utilizes these advanced genera-\ntive modeling techniques of diffusion models, the efficiency of\nrectified flow, and the robustness of hybrid DiT architectures\nto achieve high-fidelity recovery of LoRa signals even when\nsignificantly corrupted by noise.\nC. LoRaFlow in Practice\nDesigned to complement existing LoRa infrastructure rather\nthan replace it, LoRaFlow takes a minimally invasive ap-\nproach. Unlike more radical redesigns such as NELoRa [19],\nwhich overhaul significant portions of the LoRa demodulation\npipeline, our method focuses on a single, critical intervention:\ndenoising the raw signal before it reaches the standard dechirp\noperation. This allows for seamless integration into current\nLoRa networks without substantial modifications.\nThe integration of our method into the LoRa framework can\nbe summarized into three main components (see Figure 2):\n1) Signal Reception and Initial Processing: Similar to the\nNELoRa approach, after the initial signal reception by the\nRF front end and conversion to digital samples by the ADC,\nthe signal undergoes traditional processing steps. These\ninclude chirp enhancement, preamble detection, and offset\nrecovery. This stage prepares the signal for the subsequent\ndenoising process by correcting any initial distortions and\naligning the signal for optimal processing.\n2) LoRaFlow Signal Denoising: This is the core innovation of\nour approach. The processed signal then enters our neural\nsignal enhancement module. Here, the signal is denoised up\nto N times (depending on the SNR as we skip a number\nof steps as mentioned previously) using our model. This\nmodule employs rectified flow—a diffusion-like generative\nmodeling technique—to effectively denoise the signal. This\ndenoising process is crucial for separating the LoRa chirps\nfrom background noise, thereby improving the quality of\nthe signal before it undergoes standard demodulation.\n3) Standard Demodulation: The denoised signal proceeds to\nthe standard LoRa demodulation pipeline, starting with the\ndechirp operation. After this stage, the denoised signal is\ntransformed into packets for further application processing.\nIt is notable that our approach is entirely orthogonal to\nclassification-based approaches. The combination of both\nwhile not complicated to implement is left to future work.\nMathematically, we can express the integration of our\nmethod as a preprocessing step:\nsdenoised(t) = F(sreceived(t))\nwhere sreceived(t) is the raw received signal, F represents\nour rectified flow denoising operation, and sdenoised(t) is the\ncleaned signal that is then fed into the standard dechirp\nprocess:\nsdechirped(t) = sdenoised(t) · s∗\nbase(t)\nHere, s∗\nbase(t) is the complex conjugate of the base chirp\nsignal.\nOur approach offers several distinct advantages. By preserv-\ning the dechirp operation and subsequent processing steps,\nLoRaFlow maintains full compatibility with existing LoRa\nhardware and software. The integration requires minimal\nchanges to current LoRa systems, which can accelerate adop-\ntion and deployment, showcasing its simplicity. Additionally,\nthe denoising step is mostly or entirely bypassed in high SNR\nconditions, allowing for adaptive use based on signal quality,\ndemonstrating its flexibility.\nFocusing on signal recovery at extremely low SNRs, our\nmethod enhances the strengths of LoRa modulation rather\nthan replacing them. This conservative enhancement approach\nallows for incremental improvements to LoRa systems, poten-\ntially extending their operational range and reliability with-\nout overhauling existing infrastructure. The simplicity and\neffectiveness of our integration strategy position LoRaFlow\nas a practical advancement in LoRa communication, bridging\nthe gap between cutting-edge machine learning techniques\nand the real-world constraints of IoT deployment. As LoRa\nremains essential for long-range, low-power communication,\nour approach pushes the boundaries of its capabilities while\nmaintaining the robustness and reliability that have made it a\ncornerstone of IoT networks.\nIV. LORAFLOW MODEL ARCHITECTURE\nOur proposed architecture for the LoRaFlow denoising\nmodel integrates elements from advanced diffusion transform-\ners [20] with domain-specific adaptations tailored for LoRa\nsignal processing. The model is composed of three main\ncomponents: an input processing stage, a transformer core,\nand an output stage. This setup is then enhanced with an\nauxiliary classifier during training. These components are tied\nRF Front End\nADC\nChirp Enhance\nPreamble\nDetection\nOffset Recovery\nSamples\nSignal Reception\nSignal Processing\nProcessed\nSignal\nDownblocks\nDiT\nUpblocks\nSampling\nN times\nDenoised\nSignal\nNeural Signal Enhancement\nDechirp\nPacket\nFig. 2: Integration of LoRaFlow at LoRa gateways.\nto the ”Neural Signal Enhancement” module in Figure 2,\nwhich showcases our method.\n1D Conv\nConv Feedforward\n2x Temporal\nDownsample\nx\nTransformer\nx\nConv Feedforward\n2x Temporal\nUpsample\nRMSNorm\nLinear\nx\nTransformer Block\nLinear\nAverage Pooling\nClassification\nDiscarded\nafter\ntraining\nFig. 3: LoRaFlow Model Architecture.\nThe input processing stage employs a series of convolutional\nlayers and temporal downsampling modules that progressively\ndownsample the input signal while increasing the feature\ndimension. This stage serves two primary purposes: it ex-\ntracts low-level features from the raw signal and reduces the\nsequence length, making subsequent transformer operations\nmore computationally efficient. Specifically, this stage includes\na single 1D convolutional layer followed by 2x temporal\ndownsampling and convolutional feedforward layers, which\nefficiently compress the input signal’s temporal dimensions.\nThe core of our model is a transformer block adapted\nfrom the Diffusion Transformer (DiT) architecture. [18], [21]\nThis block processes the latent representation produced by\nthe input stage, capturing long-range dependencies and global\ncontext critical for understanding LoRa chirp structures. The\nself-attention mechanism within the transformer is particularly\nwell-suited for modeling the phase relationships in LoRa sig-\nnals across different time scales. The DiT architecture includes\nseveral key components shown in Figure 4. The mapping\nnetwork generates conditioning embeddings from the time step\nt and conditional embeddings c. Fourier features are applied\nto these inputs to capture these scalar inputs as accurately as\npossible, followed by linear layers and RMS normalization\n(RMSNorm). This results in the generation of conditioning\nembeddings cg (see Figure 4a). The feedforward block consists\nof a series of linear layers and Gated Linear Units (GELU) [22]\nactivations. The GELU activation function helps to introduce\nnon-linearity, which is crucial for capturing complex signal\nrelationships. The feedforward block is responsible for pro-\ncessing the intermediate representations within the transformer\n(see Figure 4b). At the heart of the DiT architecture, the\ntransformer block includes multi-head attention mechanisms,\nwhich allow the model to attend to different parts of the input\nsequence simultaneously. This mechanism is augmented with\nQKNorm (Query-Key Normalization) and RoPE (Relative\nPosition Encodings) to better capture positional information.\nThe block also includes AdaRMSNorm (adaptive RMS nor-\nmalization) layers and feedforward networks (see Figure 4c)\nas is standard in Transformers. The convolutional feedforward\nblock integrates convolutional layers with the same layout as\nthe feedforward blocks networks. 1D convolutions are used\nto capture local dependencies in the signal. AdaRMSNorm is\napplied to maintain stable training dynamics and condition the\nnetwork (see Figure 4d).\nFollowing the transformer core, the output stage mirrors\nthe input stage in reverse, using convolutional layers and\ntemporal upsampling modules to upsample the signal back to\nits original dimensions. This stage reconstructs the denoised\nsignal from the processed latent representation. The careful\ndesign of the upsampling modules ensures that the high-\nresolution features are accurately restored, maintaining the\nintegrity of the denoised signal. This stage specifically includes\n2x temporal upsampling and convolutional feedforward layers,\nwith RMSNorm and linear layers to refine the signal.\nA key innovation in our architecture in Figure 3 is the\nincorporation of an auxiliary classifier connected to the trans-\nformer’s output. This classifier is designed to predict the LoRa\nchirp class (which is unique per spreading factor) from the\nlatent representation. Importantly, the classifier is only used\nduring training and is discarded during inference. This ap-\nproach guides the model to learn more discriminative features\nwithout constraining its generative capabilities, enhancing the\noverall performance of the denoising process.\nThe model processes time embeddings and augmentation\ncondition embeddings, which are injected into each block of\nthe network. These embeddings allow the model to adapt its\nbehavior based on the noise level and specific augmentations\napplied to the input signal. The incorporation of these embed-\ndings ensures that the model remains flexible and responsive\nto varying signal conditions, a crucial feature for effective\ndenoising in low SNR environments. It should be noted that\nxt shown in Fig 3 refers to a noisy sample at timestep t while\ncg refers to the output of the Mapping Network.\nV. LORAFLOW MODEL TRAINING METHODOLOGY\nIn this section, we detail the comprehensive training\nmethodology employed to optimize the LoRaFlow model. This\nFourier\nFeatures\nFourier\nFeatures\nLinear\nLinear\nFeedforward\nx\nMulti-head Attention\n+\nQKNorm and RoPE\nAdaRMSNorm\nAdaRMSNorm\nFeedforward\nLinear\nGEGLU\nLinear\nAdaRMSNorm\n1D Conv\nGEGLU\n1D Conv\n1x1\nConv\nRMSNorm\nRMSNorm\nGELU\na. Mapping Network\nb. FeedForward Block\nc. Transformer Block\nd. Convolutional Feed\nForward Block\nFig. 4: Detailed Components of the LoRaFlow Architecture\nmethodology includes the design of multi-component loss\nfunctions, synthetic data generation, and data augmentation\ntechniques, ensuring the model’s generalizability to real-world\ndata.\nA. Loss Functions and Training Objectives\nOur training process employs a multi-component loss func-\ntion designed to address the unique challenges of LoRa signal\nreconstruction at extremely low SNRs. The total loss is a\nweighted sum of four components:\nLtotal = Lrecon + λ1LFFT + λ2LSTFT + λ3Lcls\n(7)\nwhere λ1, λ2, and λ3 are weighting coefficients.\nThe primary reconstruction loss, Lrecon, is based on the\nrectified flow formulation:\nLrecon = Et∼U(0,1)\n\u0002\n∥z1 −z0 −vθ(tz1 + (1 −t)z0, t)∥2\n2\n\u0003\n(8)\nwhere z0 is the noisy input, z1 is the clean target, and vθ\nis our model.\nTo enhance frequency-domain fidelity, we incorporate two\nspectral losses. The FFT loss, LFFT, computes the discrepancy\nbetween the Fourier transforms of the predicted and target\nsignals after applying the LoRa chirp:\nLFFT = HuberLoss(FFT(xpred · c), FFT(xtarget · c))\n(9)\nwhere c is the LoRa chirp and x represents the complex-\nvalued signals.\nThe multi-scale STFT loss, LSTFT, captures time-frequency\ncharacteristics at various resolutions:\nLSTFT =\nX\n(n,h)∈S\nHuberLoss(STFTh\nn(xpred), STFTh\nn(xtarget))\n(10)\nwhere S is a set of (window size, hop length) pairs.\nThe classification loss, Lcls, is a combination of cross-\nentropy and an auxiliary regularization term:\nLcls = CrossEntropy(ypred, ytrue)+α∥(log\nX\ni\nexp(ypred,i))∥2\n2 (11)\nwhere α is a small coefficient (e.g., 1e-4) to keep the logits\nnormalized [23].\nThis classification loss serves a crucial role in our train-\ning process. The similarity between different LoRa chirps,\nwhich can be viewed as rotations of each other in the signal\nspace, poses a challenge for reconstruction-based losses alone.\nThese losses may not provide sufficient guidance for the\nmodel to distinguish between similar chirps accurately. By\nintroducing the classification loss, we encourage the model’s\nlatent representations to be more discriminative, helping it\nReal Signal\nIdeal Signal\nPhase\nAmplitude\nFig. 5: Showcase of a sample from the NELoRa dataset\ncompared to our dataset of ideal synthesized data.\nto ”choose correctly” among similar chirp candidates during\nreconstruction.\nThe auxiliary classifier is designed with additional param-\neters to contain potential representation collapse often asso-\nciated with classification losses. These extra parameters are\nremoved during inference, ensuring that the final model retains\nthe flexibility needed for high-quality signal reconstruction\nwhile benefiting from the improved feature learning during\ntraining.\nThis multi-component loss function, combined with our\nhybrid architecture, enables our model to achieve high-fidelity\nLoRa signal recovery at extremely low SNRs, effectively push-\ning the boundaries of reliable communication in challenging\nenvironments.\nB. Synthetic Data Generation for Training\nTo train our model effectively, we developed a comprehen-\nsive synthetic LoRa signal dataset that captures the diversity\nof real-world LoRa transmissions. Our dataset - taken from\nNELoRa - encompasses a wide range of spreading factors\n(SF) specifically: 7, 8, 9, and 10. For each spreading factor\nchoice, we generate chirp signals as described earlier using\nEquation 1. We generate both upchirps (increasing frequency)\nand downchirps (decreasing frequency) to represent the full\nrange of LoRa symbols. To encode different symbols, we apply\nappropriate time shifts τm as described in Subsection II-A.\nThis approach allows us to generate all possible symbols for\neach SF, resulting in a dataset that comprehensively covers the\nLoRa signal space. We show an example of a real data sample\ncompared to a synthetic sample in Figure 5\nThe use of synthetic data offers several advantages. First, it\nallows us to generate a large, diverse dataset without the need\nfor extensive real-world data collection. Second, it provides\nperfect ground truth for training, free from real-world channel\nimpairments. Finally, it enables us to systematically explore\nthe full range of LoRa configurations, ensuring our model’s\ngeneralizability.\nC. Data Augmentation Techniques\nTo enhance our model’s robustness and generalization capa-\nbilities, we implement a suite of data augmentation techniques\ntailored to LoRa signals and potential channel effects. These\naugmentations are applied probabilistically during training,\neach with a base probability of 0.15. We use an 8-dimensional\ninput condition vector c, where 7 dimensions correspond\nto on\/off augmentations (1 or -1), and the 8th denotes the\ncurrent SF. This vector is dropped out to all 0s with a 10%\nprobability, challenging the model to infer augmentations and\nSF independently. All evaluations are performed with c set to\n0s, which we find to be highly effective. While we employ\nseveral strategies, we focus on describing our frequency-\ndomain masking technique in detail due to its crucial role in\nsimulating real-world channel impairments.\n1) Frequency-Domain Masking: Frequency-domain mask-\ning is particularly relevant for LoRa signals, as it simulates\nfrequency-selective fading and interference, which are com-\nmon challenges in wireless communications. This augmenta-\ntion operates on the Short-Time Fourier Transform (STFT)\nrepresentation of the signal, allowing us to manipulate its time-\nfrequency characteristics.\nThe process of frequency-domain masking can be summa-\nrized in Algorithm 1.\nAlgorithm 1: Frequency-Domain Masking\ndef frequency_domain_masking(signal):\nstft = compute_stft(signal)\nmax_num_masks, max_mask_size = 2, 2\nfor dim in [’time’, ’frequency’]:\nnum_masks = random_int(1, max_num_masks)\nfor _ in range(num_masks):\nmask_size = random_int(1,\n,→max_mask_size)\nmask_start = random_int(0, stft.shape[\n,→dim] - mask_size)\nmask_value = random_uniform(0, 0.5) if\n,→\nrandom() < 0.5 else 0\napply_mask(stft, dim, mask_start,\n,→mask_size, mask_value)\nreturn compute_istft(stft)\nThis augmentation technique randomly attenuates or com-\npletely masks small regions of the spectrogram. The process is\napplied independently to both time and frequency dimensions,\nallowing for a diverse range of potential distortions. The num-\nber of masks applied in each dimension is randomly chosen be-\ntween 1 and 2, with each mask covering 1 or 2 time\/frequency\nbins. The attenuation value is either 0 (complete masking) or\na random value between 0 and 0.5 (partial attenuation). Masks\nare applied to the real and imaginary components of the STFT\nindependently, allowing for phase distortions as well as ampli-\ntude changes. By applying these masks, we simulate various\nchannel effects such as narrow-band interference, frequency\nnulls, and short-term fading, thereby improving our model’s\nability to handle these phenomena in real-world scenarios.\n2) Additional Augmentation Techniques:\nIn addition to\nfrequency-domain masking, we employ several other augmen-\ntation strategies:\n• Time-domain shifts: We randomly roll the signal in the\ntime domain, simulating the effect of symbol boundary\nmisalignment.\n• Signal inversion: The entire signal is inverted with a\ncertain probability, helping the model become invariant\nto phase changes that might occur during transmission.\n• Spectrogram rolling: We implement rolling operations\non the spectrogram in both time and frequency dimen-\nsions, simulating minor frequency offsets and time shifts.\nThe full implementation details of these augmentations,\nincluding their probabilistic application, are available in our\nopen-source code release.1\nBy combining these diverse augmentation techniques, we\ncreate a robust training regime that prepares our model for the\nchallenges of real-world LoRa signal recovery at extremely\nlow SNRs. This approach significantly enhances the model’s\nability to generalize across a wide range of signal conditions\nand channel impairments, ultimately improving its perfor-\nmance in practical deployments.\nD. Generalization to Real-World Data\nWhile our approach demonstrates impressive performance\non synthetic data, bridging the gap to real-world LoRa signals\nis crucial for practical deployment. To address this challenge,\nwe employ a fine-tuning strategy that leverages the NELoRa\ndataset [19], adapting our model to the nuances of actual\nLoRa transmissions while maintaining its ability to operate at\nextremely low SNRs. The NELoRa dataset comprises 27,329\nLoRa symbols covering spreading factors from 7 to 10,\ncollected in real-world indoor environments.\nOur fine-tuning process is designed to be data-efficient,\nrecognizing the scarcity of labeled real-world LoRa signals,\nespecially at very low SNRs. We adopt a one-shot learning\nscenario, utilizing a single sample per class from the NELoRa\ndataset. This approach minimizes the need for extensive real-\nworld data collection and demonstrates the robustness of our\nmodel architecture.\nThe\nfine-tuning\nprocedure\nclosely\nmir-\nrors\nour\ninitial\ntraining\nprocess:\nLfine-tune\n=\nEz0,z1∼pdata\nhR 1\n0 Et\n\u0002\n∥z1 −z0 −vθ(tz1 + (1 −t)z0, t)∥2\n2\n\u0003\ndt\ni\nwhere pdata now represents a mixture of real and synthetic\ndata distributions. Specifically, during training we take samples\nfrom real NELoRa dataset with 95% probability and from\nour synthetic dataset with 5% probability. Do note that in\neither case we only have one sample available per class. This\nmixture strategy serves two purposes: it prevents catastrophic\nforgetting of the rich patterns learned from synthetic data and\nfills the gaps where the NELoRa dataset sometimes has only\n1 sample for some classes, in which case, we leave the class\nout and rely entirely on synthetic data.\nThe use of the same loss function and optimization setup\nas in the initial training phase allows for a seamless transition\nbetween synthetic and real data. This continuity in the learning\nprocess facilitates efficient knowledge transfer, enabling the\nmodel to quickly adapt to the characteristics of real LoRa\nsignals while retaining its ability to operate in extremely\nchallenging noise conditions.\n1DOUBLE BLIND\n1\n2\n4\n8\n10\n12\n14\n16\n18\n20\n24\n32\n48\n64\n128\nNFE\n-40\n30\n10\n-2\n-5\n-8\n-12\n-16\n-18\n-20\n-22\n-24\n-26\n-28\n-30\n-32\n-34\n-40\nSNR (dB)\nLoRaFlow Dechirp at SF=7\n1\n2\n4\n8\n10\n12\n14\n16\n18\n20\n24\n32\n48\n64\n128\nNFE\nLoRaFlow Dechirp at SF=8\n1\n2\n4\n8\n10\n12\n14\n16\n18\n20\n24\n32\n48\n64\n128\nNFE\nLoRaFlow Dechirp at SF=9\n1\n2\n4\n8\n10\n12\n14\n16\n18\n20\n24\n32\n48\n64\n128\nNFE\nLoRaFlow Dechirp at SF=10\n0.4\n0.2\n0.0\n0.2\n0.4\nAdvantage\nAdvantage over Baseline for Different Spreading Factors, NFE vs. SNR.\nFig. 6: This figure shows the accuracy advantage over baseline dechirp for all SNRs tested and various numbers of neural\nfunction evaluations (NFE). Light green\/yellow indicates equal performance to baseline, red indicates a performance degradation,\nand green indicates performance improvement.\nEmpirically, we observe that this fine-tuning approach\nleads to significant improvements in real-world performance,\nparticularly in scenarios where the SNR is well below the\ntheoretical limits of traditional LoRa demodulation techniques.\nThe model’s ability to generalize from a single real-world\nexample per class underscores the effectiveness of our rectified\nflow-based approach in capturing the fundamental structure\nof LoRa signals, transcending the specifics of synthetic data\ngeneration.\nVI. PERFORMANCE EVALUATION\nWe rigorously evaluate LoRaFlow’s performance across\nvarious dimensions to demonstrate its efficacy in enhancing\nLoRa signal reception under challenging low SNR condi-\ntions. Our evaluation metrics include accuracy advantage over\nbaseline dechirp, qualitative signal reconstruction assessment,\nand quantitative comparison with the state-of-the-art NELoRa\nmethod using Signal Error Rate (SER) advantage. We also\nutilize the Area Under Curve (AUC) metric to provide a\nholistic performance summary across all SNRs.\nA. Experimental Setup\nOur training process leverages synthetic data with a single\nsample per class, necessitating a robust augmentation strategy.\nWe employ dynamic batch sizes tailored to each Spreading\nFactor (SF): 2048 for SF7, 1024 for SF8, 512 for SF9, and\n256 for SF10. For the scope of this work, we use a bandwidth\nof 125,000 for all experiments. The model undergoes 300,000\nupdates on synthetic data, followed by fine-tuning with real\ndata. For fine-tuning, we select one example per class from\nthe NELoRa dataset, training on real data for 95% of the\nsubsequent 50,000 updates and on synthetic data for the\nremainder. Despite one-shot (1 sample per class) finetuning\nbeing the most difficult adaptation scenario, we find that our\nmodel excels in it easily.\nTraining utilizes 6 NVIDIA SXM5 H100 GPUs, optimized\nwith AdamW Schedule Free [24]. We implement Flash Atten-\ntion [25] and leverage torch.compile with dynamic sizes\nto enhance computational efficiency. To ensure reproducibility,\nwe use a fixed random seed and deterministic splitting across\nall experiments.\nFor evaluation, we assess performance across a range of\nSNRs (-40dB to -10dB) and compare against both traditional\ndechirp methods and the state-of-the-art NELoRa approach.\nOur metrics include accuracy advantage, Signal Error Rate\n(SER) advantage, and Area Under Curve (AUC) for compre-\nhensive performance assessment.\nB. Impact of Neural Function Evaluations on Performance\nFigure 6 shows the accuracy advantage of LoRaFlow over\nthe baseline dechirp method across various Spreading Factors\n(SFs) and Signal-to-Noise Ratios (SNRs), plotted as a func-\ntion of the number of Neural Function Evaluations (NFE).\nThis comprehensive analysis reveals several key insights. Lo-\nRaFlow demonstrates substantial performance improvements\nunder certain scenarios, particularly for SFs 7 and 8 within\nthe mid-range SNRs (-30 to -20 dB), where accuracy im-\nprovements of up to 0.4 are observed. The influence of NFE is\nsignificant, as increases in NFE lead to enhanced performance.\nHowever, increasing NFE beyond 16 does not seem to yield\nsignificant benefits, suggesting that the model’s trajectory is\nsimple enough to be sufficiently well approximated by that\nlevel of discretization. While improvements for SFs 9 and 10\nare less dramatic, they remain substantial, particularly between\n-25 and -15 dB SNRs. At extremely low SNRs (below -35\ndB), the performance of LoRaFlow aligns with the baseline,\nindicating a potential limitation in signal recovery under\nsevere noise conditions. These findings emphasize LoRaFlow’s\ncapacity to enhance LoRa communication reliability in noisy\nenvironments, particularly at lower SFs where other methods\nfalter. The results also highlight the crucial role of balancing\ncomputational load against signal recovery effectiveness in\nreal-world applications.\nC. Qualitative Comparison of Signal Reconstruction\nFigure 7 presents a comparative analysis of signal recon-\nstruction quality at different processing stages, highlighting\nLoRaFlow’s advanced signal recovery capabilities. The origi-\nnal data displays distinct chirp patterns in both phase and am-\nplitude domains, typical of LoRa modulation. In contrast, the\nnoisy signal shows substantial degradation, with chirp struc-\ntures becoming indistinguishable, particularly in the amplitude\ndomain. NELoRa’s masked output slightly improves upon this\nnoisy signal, yet it does not completely restore the original\nsignal structure, especially in the phase domain. On the other\nhand, LoRaFlow’s output closely mirrors the original signal\nin both domains, achieving a highly accurate reconstruction of\nchirp patterns. Impressively, LoRaFlow reconstructs the phase\nwith near perfection, despite phase generally being challenging\nto accurately restore.\nReal Data STFT (Phase)\nNoisy Signal STFT (Phase)\nNeLoRa Masked STFT Output\nLoRaFlow Denoised STFT (Phase)\nPhase\nAmplitude\nFig. 7: This figure shows a side-by-side comparison for both the amplitude and the phase of a sample from the NELoRa\ndataset, the sample at-reception (noisy), NELoRa’s masked output which is their version of denoising, and our output.\nThe superior reconstruction quality of LoRaFlow, particu-\nlarly in preserving phase information, stands out. This abil-\nity to maintain phase accuracy is crucial for correct signal\ndecoding in radio communications. LoRaFlow’s capability\nto reconstruct detailed signal structure from heavily noisy\nenvironments highlights its potential to considerably enhance\nthe operational range and reliability of LoRa networks in\nchallenging conditions.\n40\n35\n30\n25\n20\n15\n10\nSNR (dB)\n0.0\n0.1\n0.2\n0.3\n0.4\nNegative SER Difference\nAdvantage-over-Baseline: NeLoRa vs. LoRaFlow Normalized for Baseline Differences\nSF=7 LoRaFlow enhanced Dechirp\nSF=8 LoRaFlow enhanced Dechirp\nSF=7 NELoRa + Classifier\nSF=8 NELoRa + Classifier\nSER < 10%\nFig. 8: This plot shows the negative signal error rate advantage\nover each method’s respective baseline. Values represent an\nerror rate decrease over the baseline (higher is better). Plot is\ncorrected for differences in baselines.\nD. Quantitative Comparison with NELoRa\nFigure 8 provides a quantitative comparison between Lo-\nRaFlow and the state-of-the-art NELoRa method, utilizing the\nmetric of Symbol Error Rate (SER) advantage over their re-\nspective baselines. This analysis demonstrates that LoRaFlow\nconsistently outperforms NELoRa across various SNRs for\nboth SFs 7 and 8. The performance advantage is notably\nsignificant in the -30 to -20 dB SNR range, where LoRaFlow\nachieves up to a 0.4 SER advantage over the baseline for SF=8.\nBoth methods exhibit diminished advantages at extremely low\n(below -35 dB) and high (above -15 dB) SNRs, highlighting\nthe challenges of signal recovery in extreme noise environ-\nments and the adequacy of traditional methods in the weaker\nSNR regions.\nOur results are particularly significant as they are based on\nevaluations using almost the entire dataset, whereas NELoRa’s\nreported results cover only 20% of the dataset, lending greater\nstatistical significance to our findings which is remarkable as\nour scenario is significantly more challenging. Additionally,\nwhile both models have matched parameter counts, implying\nsimilar capacities, NELoRa utilizes a separately trained net-\nwork for each SF, whereas our approach employs a single\nmodel for all configurations.\nWe attempted to reproduce NELoRa’s results using the\nlatest code uploaded to their Github [26], however, as of\nwriting all the models published on their Github failed to\noutperform the baseline. Therefore we had to resort to copying\nNELoRa’s results from the figures in their paper [10]\nTo provide a holistic performance summary, we compute the\nArea Under Curve (AUC) for both methods across all SNRs.\nTable I presents these results:\nTABLE I: AUC Comparison between LoRaFlow and NELoRa\nSF\nLoRaFlow AUC\nNELoRa AUC\nImprovement over NELoRa\n7\n2.922\n2.227\n31.2%\n8\n3.143\n2.409\n30.5%\nThese AUC values (where higher is better) demonstrate\nLoRaFlow’s significant overall performance improvement over\nNELoRa, with enhancements of 31.2% and 30.5% for SF=7\nand SF=8, respectively.\nE. Discussion\nWhile LoRaFlow demonstrates significant improvements\nover NELoRa for SF 7 and 8, there remain opportunities for\nenhancement, particularly for higher spreading factors. Our\nanalysis reveals several key challenges:\n• Dataset Characteristics: The NELoRa dataset exhibits\nsubstantial class imbalance, especially pronounced in higher\nSFs. This imbalance potentially favors methods that learn\nthe training set’s class distribution, an effect that becomes\nmore significant as the number of classes increases (e.g., 512\nfor SF 9, 1024 for SF 10). In contrast, LoRaFlow’s training\non a uniform class distribution, while ensuring unbiased\nperformance, does not at all leverage the dataset’s statistics.\n• Computational Constraints: Memory limitations neces-\nsitate progressively smaller batch sizes as SF increases,\npotentially impacting model optimization for higher SFs.\nThis challenge highlights the need for more efficient training\nstrategies or software optimization solutions to maintain\nsamples seen across all SFs.\n• Decoding Strategy: Our approach uses the standard dechirp\nalgorithm without class bias. While this ensures fairness,\nit may not fully exploit the model’s potential. Learning a\nclassifier on top of LoRaFlow could significantly enhance\nperformance, especially for higher SFs.\nVII. CONCLUSION\nThis paper introduces LoRaFlow, a novel approach to\nLoRa signal reconstruction using rectified flow. Our method\ndemonstrates significant improvements over existing tech-\nniques, particularly at low SNRs and for lower spreading\nfactors. LoRaFlow’s ability to recover high-fidelity signals\nfrom extremely noisy inputs pushes the boundaries of reli-\nable long-range, low-power communication. By maintaining\ncompatibility with existing LoRa infrastructure, our approach\noffers a practical path to enhancing IoT network performance\nwithout overhauling current systems. Future work will focus\non addressing challenges at higher spreading factors and\nexploring the integration of learned classifiers to further boost\nperformance. LoRaFlow represents a significant step forward\nin robust IoT communications, opening new possibilities for\ndeploying IoT networks in challenging environments.\nVIII. ACKNOWLEDGEMENTS\nHigh Performance Computing resources provided by the\nHigh Performance Research Computing (HPRC) core facility\nat Virginia Commonwealth University (https:\/\/hprc.vcu.edu)\nwere used for conducting the research reported in this work.\nREFERENCES\n[1] F. Adelantado, X. Vilajosana, P. Tuset-Peir´o, B. Martinez, J. Melia-\nSegui, and T. Watteyne, “Understanding the limits of lorawan,” IEEE\nCommunications Magazine, vol. 55, no. 9, pp. 34–40, 2017.\n[2] M. Centenaro, L. Vangelista, A. Zanella, and M. Zorzi, “Long-range\ncommunications in unlicensed bands: The rising stars in the iot and\nsmart city scenarios,” IEEE Wireless Communications, vol. 23, no. 5,\npp. 60–67, 2016.\n[3] S. Corporation, “Lora and lorawan: A technical overview,” https:\/\/lora-\ndevelopers.semtech.com\/uploads\/documents\/files\/LoRa and LoRaWAN-\nA Tech Overview-Downloadable.pdf, 2019.\n[4] H.-S. Choi, J. H. Lee, S. Ihm, and P. Levis, “Charm: exploiting\ngeographical diversity through coherent combining in low-power wide-\narea networks,” in Proceedings of the 13th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI), 2016.\n[5] U. Khawaja, M. Tushar, M. I. Noor, M. Khawaja, I. Qazi, and N. Qazi,\n“Opr: Enabling operational physical layer for iot networks,” in Pro-\nceedings of the 14th International Conference on emerging Networking\nEXperiments and Technologies (CoNEXT), 2018.\n[6] X. Liu, Z. Zhu, J. Cao, L. Zhang, and S. Zhang, “Chime: channel\ninference-based minimal-effort communications for ultra-low power\nsensor networks,” in Proceedings of the 24th Annual International\nConference on Mobile Computing and Networking (MobiCom), 2018.\n[7] W. Gao, Z. Zhou, Y. Liu, J. Zhang, P. Li, and X. Liu, “Choir: enhancing\nthe reception of lora transmissions using co-located lora gateways,”\nin Proceedings of the 16th ACM Conference on Embedded Networked\nSensor Systems (SenSys), 2018.\n[8] L. Liu, Y. Yao, Z. Cao, and M. Zhang, “Deeplora: Learning accurate\npath loss model for long distance links in lpwan,” in Proceedings of\nIEEE INFOCOM, 2021.\n[9] J. Chan, A. Wang, A. Krishnamurthy, and S. Gollakota, “Deepsense:\nEnabling carrier sense in low-power wide area networks using deep\nlearning,” arXiv:1904.10607 [cs], 2019.\n[10] C. Li, H. Guo, S. Tong, X. Zeng, Z. Cao, M. Zhang, Q. Yan, L. Xiao,\nJ. Wang, and Y. Liu, “Nelora: Towards ultra-low snr lora communication\nwith neural-enhanced demodulation,” in Proceedings of the 19th ACM\nConference on Embedded Networked Sensor Systems (SenSys), 2021.\n[11] A. Berni and W. Gregg, “On the utility of chirp modulation for digital\nsignaling,” IEEE Transactions on Communications, 1973.\n[12] A. Maleki, H. H. Nguyen, E. Bedeer, and R. Barton, “A tutorial on chirp\nspread spectrum for lorawan: Basics and key advances,” arXiv preprint\narXiv:2310.10503, 2023.\n[13] Y. Song and S. Ermon, “Generative modeling by estimating gradients\nof the data distribution,” in NeurIPS, vol. 32, 2019, pp. 11 895–11 907.\n[14] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”\nin NeurIPS, vol. 33, 2020, pp. 6840–6851.\n[15] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon,\nand B. Poole, “Score-based generative modeling through stochastic\ndifferential equations,” 2021. [Online]. Available: https:\/\/arxiv.org\/abs\/\n2011.13456\n[16] X. Liu, C. Gong, and Q. Liu, “Flow straight and fast: Learning to\ngenerate and transfer data with rectified flow,” 2022.\n[17] Q. Liu, “Rectified flow: A marginal preserving approach to optimal\ntransport,” arXiv preprint arXiv:2209.14577, 2022.\n[18] W. Peebles and S. Xie, “Scalable diffusion models with transformers,”\nin Proceedings of the IEEE\/CVF International Conference on Computer\nVision, 2023, pp. 4195–4205.\n[19] C. Li, H. Guo, S. Tong, X. Zeng, Z. Cao, M. Zhang, Q. Yan, L. Xiao,\nJ. Wang, and Y. Liu, “Nelora: Towards ultra-low snr lora communication\nwith neural-enhanced demodulation,” in Proceedings of the 19th ACM\nConference on Embedded Networked Sensor Systems, 2021, pp. 56–68.\n[20] K. Crowson, R. Beaumont, T. Abraham, J. Whitaker, and storyicon,\n“crowsonkb\/k-diffusion: v0.1.1.post1,” Dec. 2023. [Online]. Available:\nhttps:\/\/doi.org\/10.5281\/zenodo.10284390\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in\nneural information processing systems, vol. 30, 2017, pp. 5998–6008.\n[22] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[23] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scal-\ning language modeling with pathways,” Journal of Machine Learning\nResearch, vol. 24, no. 240, pp. 1–113, 2023.\n[24] A. Defazio, H. Mehta, K. Mishchenko, A. Khaled, A. Cutkosky et al.,\n“The road less scheduled,” arXiv preprint arXiv:2405.15682, 2024.\n[25] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R´e, “Flashattention: Fast and\nmemory-efficient exact attention with io-awareness,” Advances in Neural\nInformation Processing Systems, vol. 35, pp. 16 344–16 359, 2022.\n[26] “NeLoRa\nDataset,”\nhttps:\/\/github.com\/daibiaoxuwu\/NeLoRa Dataset,\n2023, accessed: 2024-07-25.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LoRaFlow: High-Quality Signal Reconstruction using Rectified Flow.pdf"}
{"title":"Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads","authors":"Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, Zhijie Deng","summary":"We introduce Orthus, an autoregressive (AR) transformer that excels in\ngenerating images given textual prompts, answering questions based on visual\ninputs, and even crafting lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultaneously copes with discrete\ntext tokens and continuous image features under the AR modeling principle. The\ncontinuous treatment of visual signals minimizes the information loss for both\nimage understanding and generation while the fully AR formulation renders the\ncharacterization of the correlation between modalities straightforward. The key\nmechanism enabling Orthus to leverage these advantages lies in its\nmodality-specific heads -- one regular language modeling (LM) head predicts\ndiscrete text tokens and one diffusion head generates continuous image features\nconditioning on the output of the backbone. We devise an efficient strategy for\nbuilding Orthus -- by substituting the Vector Quantization (VQ) operation in\nthe existing unified AR model with a soft alternative, introducing a diffusion\nhead, and tuning the added modules to reconstruct images, we can create an\nOrthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).\nOrthus-base can further embrace post-training to better model interleaved\nimages and texts. Empirically, Orthus surpasses competing baselines including\nShow-o and Chameleon across standard benchmarks, achieving a GenEval score of\n0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows\nexceptional mixed-modality generation capabilities, reflecting the potential\nfor handling intricate practical generation tasks.","url":"http:\/\/arxiv.org\/abs\/2412.00127v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.00127v1","published":1732798838000,"comment":null,"pdf_text":"Orthus: Autoregressive Interleaved Image-Text Generation with\nModality-Specific Heads\nSiqi Kou1∗, Jiachun Jin1, Chang Liu1∗, Ye Ma2, Jian Jia2, Quan Chen2, Peng Jiang2, Zhijie Deng1†\n1Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University\n2Kuaishou Technology\n{happy-karry,1iuchang123}@sjtu.edu.cn, jiachun22.jin@gmail.com\n{maye,jiajian,chenquan06,jiangpeng}@kuaishou.com, zhijied@sjtu.edu.cn\nAbstract\nWe introduce Orthus, an autoregressive (AR) transformer\nthat excels in generating images given textual prompts, an-\nswering questions based on visual inputs, and even craft-\ning lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultane-\nously copes with discrete text tokens and continuous image\nfeatures under the AR modeling principle. The continuous\ntreatment of visual signals minimizes the information loss\nfor both image understanding and generation while the fully\nAR formulation renders the characterization of the correla-\ntion between modalities straightforward. The key mecha-\nnism enabling Orthus to leverage these advantages lies in\nits modality-specific heads—one regular language model-\ning (LM) head predicts discrete text tokens and one diffu-\nsion head generates continuous image features conditioning\non the output of the backbone. We devise an efficient strat-\negy for building Orthus—by substituting the Vector Quan-\ntization (VQ) operation in the existing unified AR model\nwith a soft alternative, introducing a diffusion head, and\ntuning the added modules to reconstruct images, we can\ncreate an Orthus-base model effortlessly (e.g., within mere\n72 A100 GPU hours). Orthus-base can further embrace\npost-training to better model interleaved images and texts.\nEmpirically, Orthus surpasses competing baselines includ-\ning Show-o and Chameleon across standard benchmarks,\nachieving a GenEval score of 0.58 and an MME-P score\nof 1265.8 using 7B parameters. Orthus also shows excep-\ntional mixed-modality generation capabilities, reflecting the\npotential for handling intricate practical generation tasks.\n∗Work done during an internship at Kuaishou Technology.\n†Corresponding author.\n1. Introduction\nMultimodal models have shown promise in image-to-text\nand\/or text-to-image generation, with LLaVA [33, 34],\nEmu2 [51], and NExT-GPT [64] as popular examples.\nThese abilities are essential for the processing of complex\nreal-world understanding and generation problems. Yet, ex-\nisting approaches can suffer from significant modeling re-\ndundancy due to the trivial combination of specialized large\nmodels (e.g., CLIP-ViT [40], Stable Diffusion [39, 44], and\nLlaMa [56, 57]). Doing so also undermines the benefits\nbrought by cross-modal learning and introduces consider-\nable inefficiency for both training and inference.\nThere is ongoing interest in jointly modeling interleaved\nimage-text with a unified, compact model. One strategy is\nto map both images and texts to discrete tokens for sim-\nple autoregressive (AR) modeling [36, 53, 62] (see left of\nFigure 1). However, the image tokenizer, often equipped\nwith a vector quantization (VQ) bottleneck, can cause in-\nevitable information loss and easily lead to suboptimal per-\nformance on vision tasks concerning high-frequency de-\ntails (e.g., OCR and human face generation).\nAlterna-\ntively, recent works, including Transfusion [77] and Mono-\nformer [76] (middle of Figure 1), propose to integrate AR\nmodeling on discrete text tokens and diffusion modeling\non continuous image features within a single transformer.\nNonetheless, the nature of diffusion modeling to process\nnoisy images [21] makes the joint modeling of visual gen-\neration and understanding challenging.\nThis paper proposes Orthus1 to bridge the gap.\nOr-\nthus conjoins lossless continuous image features and the\nunified, cross-modal AR modeling by decoupling diffusion\nfrom the transformer backbone. This renders the charac-\nterization of the correlation between modalities precise and\nstraightforward. Specifically, Orthus embeds both discrete\ntext tokens (from an off-the-shelf tokenizer) and continuous\n1Orthus is a loyal two-headed guard dog in Greek mythology.\n1\narXiv:2412.00127v1  [cs.CV]  28 Nov 2024\nAutoregressive (causal attn)\n<BOI>\nLM head\n<EOI>\nIt's\na\ndog\n<EOS>\n<EOI>\na\ndog\nIt's\nDiffusion (full attn)     Autoregressive (causal attn)\n<EOS>\nIt's\na\ndog\n<BOI>\n<EOI>\na\ndog\nIt's\nAutoregressive (causal attn)\nDiffusion head\nLM head\nIt's\na\ndog\n<EOI>\n<EOS>\n<BOI>\n<EOI>\na\ndog\nIt's\nimg patch\n w\/o VQ\nimg token\n w\/ VQ\nnoisy\nimg patch\ntext token\ndog\nFigure 1. Comparison of existing unified multimodal models with Orthus. Left: Fully AR models [36, 53, 62] convert visual signals to\ndiscrete image tokens via vector quantization for joint modeling with text tokens, but this causes information loss. Middle: AR-diffusion\nmixed models [67, 77] perform next-token prediction for text generation and image patch denoising for image generation, but the involved\nnoise disturbance on images makes the concurrent image-to-text and text-to-image generation challenging. Right: Orthus operates in a\nfully AR manner while circumventing vector quantization and noise disturbance to preserve input information and modeling flexibility.\npatch-wise image features (from a pre-trained variational\nautoencoder [25]) into the same representation space, where\nan AR transformer is then invoked to model the inter- and\nintra-modality interdependence. On top of the backbone,\nOrthus defines two modality-specific heads, with one as the\nregular language modeling (LM) head to predict discrete\ntext tokens and the other as a novel diffusion head to craft\ncontinuous image features. During inference, Orthus au-\ntoregressively predicts the next text token or image patch\naccording to the indication of special transition tokens.\nNotably, the investigation into diffusion head for supe-\nrior image generation draws a striking analogy to the re-\ncent masked AR (MAR) approach [28], yet with a focus\nshift from image-only generation to mixed-modality one.\nOn the other hand, our Orthus differentiates from MAR\nand its variant [68] in that it characterizes the correlation\nwith fully AR formulation instead of mask-based model-\ning, which avoids expensive hyperparameter specification\nand eases the modeling of interleaved images and texts.\nThe other important contribution of this work is a super-\nefficient strategy to build Orthus.\nInspired by that Or-\nthus differentiates from the representative token-based AR\nmodel Chameleon [53] only in the input embedding mod-\nules and output heads, we propose to substitute the VQ op-\neration in Chameleon with a soft alternative and augment\nthe model with an extra diffusion head to instantiate Or-\nthus. We tune only the embedding modules and diffusion\nhead (with 0.3B parameters in total) to reconstruct images\non a 10k dataset to effortlessly obtain an Orthus base model.\nOrthus can further adopt post-training to bolster its ability\nto model interleaved images and text.\nWe have performed extensive studies to evaluate Orthus.\nThe results evidence that Orthus is substantially superior\nto Chameleon and Show-o [67] across multimodal under-\nstanding and generation tasks. Notably, Orthus achieves\na GenEval [16] accuracy of 0.58, even surpassing special-\nized text-to-image models including DALL-E 2 [42] and\nSDXL [39]. Furthermore, on image understanding tasks\nlike POPE [29] and GQA [22], Orthus achieves scores of\n79.6 and 52.8, outperforming the performant InstructBLIP-\n13B [7]. We also showcase the ability of Orthus for mixed-\nmodality generation in both zero-shot scenarios and down-\nstream fine-tuning ones like storybook creation.\nTo summarize, our contributions are as follows:\n• We introduce Orthus for interleaved image-text genera-\ntion. Orthus models the correlation between modalities\nthrough the AR principle and generates discrete text to-\nkens and continuous image features with dedicated heads.\n• We propose an efficient strategy to build Orthus by ex-\nploiting its connection with existing unified AR models,\nwhich reduces the cost to merely 72 A100 GPU hours.\n• Orthus outperforms related works like Chameleon [53]\nand Show-o [67] across various understanding and gener-\nation benchmarks, positioning it as a promising approach\nfor unified multimodal modeling.\n2. Related Work\nVisual understanding. To enable multimodal large lan-\nguage models (MLLMs) to comprehend modalities beyond\ntext, prior work has introduced methods that leverage pre-\ntrained, modality-specific encoders [5, 27, 40, 72] to gener-\nate latent representations for each distinct modality. These\nrepresentations are then projected into a pre-trained LLM’s\ninput space through trained adapters, allowing for multi-\nmodal information alignment within the language model,\nunderstanding and reasoning are handled within the trans-\nformer backbone [4, 7, 10, 30, 32, 34, 78]. This framework\nallows LLMs to perform complex multimodal tasks while\nmaintaining the language-based reasoning capabilities in-\nherent to their architecture.\nVisual generation. The generation of visual content has\nlong been a central focus within the deep learning research\ncommunity [17, 23, 24, 58]. Over the past few years, re-\nsearch in visual generation has focused on decomposing vi-\nsual signals in a more sophisticated manner and generating\nthem iteratively. Diffusion models [8, 12, 21, 38, 44, 45, 47]\n2\ntransform generation into a reverse diffusion process from\nnoise to data, gradually refining an initial noise input\nthrough a series of denoising steps. While another line of\nwork aims to emulate the success of AR modeling from\nlanguage modeling within the visual domain [37, 41, 43,\n49, 73]. Specifically, images are first transformed into a\nsequence of vector-quantized tokens [11, 55, 59, 74], af-\nter which AR modeling is then performed on the discrete-\nvalued token space [56].\nTo mitigate generation quality\ndegradation caused by information loss during the VQ pro-\ncess, MAR replaces the per-token categorical distribution\nmodeling with a diffusion procedure [13, 28]. HART [52]\nutilizes a hybrid tokenizer and simultaneously models the\ndiscrete per-token distribution and the continuous residual\ncomponents with a diffusion model. Our proposed method\ngeneralizes MAR to cross-modality generation.\nUnified visual understanding and generation. To enable\na model to possess both understanding and generation ca-\npabilities, one kind of approach aims to connect LLMs with\nmultimodal adapters and diffusion decoders [9, 15, 50, 70].\nHowever, using multiple distinct components can lead to\nredundancy and inefficient information use. Consequently,\nan increasing number of studies have sought to leverage\na single transformer for unified understanding and gener-\nation. Chameleon [53] and Anole [6] use a unified token\nspace to reason over and generate interleaved image and\ntext sequences. LWM [35] extends its transformer’s con-\ntext window to 1 million tokens to process both video and\ntext sequences. VILA-U [66] employs contrastive learning\nto align the visual encoder with its textual inputs, then a\nunified AR next-token prediction framework is used to con-\nduct all understanding and generation tasks. These methods\nall apply vector quantization to continuous visual signals to\nenable visual tokens, like discrete text tokens, to be trained\nusing cross-entropy loss. As an alternative to fully AR mod-\nels, some works have explored combining AR modeling\nwith diffusion modeling. Show-o [67] unifies AR and dis-\ncrete diffusion modeling for multimodal understanding and\ngeneration within one single transformer. Transfusion [77]\nand Monoformer [76] train one shared transformer for both\ndiscrete text autoregression and continuous image diffusion.\nOur proposed method circumvents the potential information\nloss caused by quantization and noise disturbance.\n3. Preliminary\nUnified multimodal modeling aims to cope with a blend of\nimages and texts with a single compact model [36, 53, 62,\n67, 77]. The model usually includes a vision autoencoder,\nspecified with an encoder E and a decoder D, a text tok-\nenizer, and a transformer network [60]. The encoder E is\nused to map the input image to a sequence of patch-wise\nfeatures V := [v1, . . . , vn], vi ∈Rdv for effective informa-\ntion compression, where dv is the feature dimension and n\nis the number of patches. The text tokenizer maps the input\ntext into a sequence of text tokens U := [u1, . . . , um] with\nm as the sequence length. The transformer is then asked to\nprocess U and V simultaneously to yield meaningful out-\nputs, which can be then detokenized as texts or decoded by\nD to produce images. There are primarily two strategies for\nthe learning of the transformer, detailed as follows.\nFully AR models.\nObserving that the AR principle ex-\ncels in the generative modeling of discrete content, semi-\nnal works, including LWM [35] and Chameleon [53], pro-\npose to leverage the Vector Quantization (VQ) [59] tech-\nnique to transform the continuous image features V as dis-\ncrete tokens to enable a fully AR modeling of the mixture\nof images and texts. Specifically, VQ introduces a set of K\ncodes {cj ∈Rdv}K\nj=1 and solves the following problem for\ncontinuous-to-discrete transformation:\n˜vi = arg min\nj∈{1,...,K}\nd(vi, cj) for i = 1, . . . , n,\n(1)\nwhere d(·, ·) is a distance metric.\nLet ˜V := [˜v1, . . . , ˜vn] denote the discrete image tokens.\nThe fully AR model embeds both ˜V and U as de-dim fea-\ntures. Specifically, the embedding corresponding to ˜vi is\nhi =\nX\nj\nwj1˜vi=j,\n(2)\nwhere {wj ∈Rde}K\nj=1 refer to the embedding weights. The\nembeddings for text tokens can be similarly gained, yet with\nanother set of embedding weights. The transformer then\nprocesses these embeddings with causal attention, where\nthe output head naturally yields the prediction of the next\ntoken. For training, the objective is simply the AR loss.\nDespite being simple, the fully AR models can suffer\nfrom information loss [31, 53], because VQ makes the\ntransformer unable to directly look at the image features vi.\nAR-diffusion mixed models. Another line of unified mul-\ntimodal models is AR-diffusion mixed models [67, 76, 77],\nwhich integrates diffusion modeling on images [21, 38] and\nAR modeling on text within a shared transformer. Take\nTransfusion [77] for example, its inputs are a noisy ver-\nsion of the image features V , denoted as ¯V , and the text to-\nkens U. To facilitate the simultaneous processing of ¯V and\nU, the attention mask of the transformer adopts a unique\nconfiguration—with a full-attention structure among ¯V and\na causal structure among U. Then, the outputs from ¯V are\ndirected to an output projector to predict the noise on ¯V ,\nwhereas the outcomes linked to U are channeled to an LM\nhead for next-token prediction. The training objective is the\ncombination of AR loss and denoising loss with a balanc-\ning factor. During inference, the model operates as an AR\nmodel to generate texts and as a diffusion model to craft\nimages, with special tokens indicating mode switching.\n3\nTransformer \nvision embedding module\ntext embedding module \nLM head\nMLP\nadd\nnoise\ncondition\n[BOI] token\n[EOI] token\ndiscrete text token\ncontinuous image patch\nText\nImage\ntext tokenizer\nvision encoder\ndiffusion modeling for\nnoise prediction\ndiffusion head\nlearnable codebook \nimage embedding layer\n🔥\n𝑓!\n𝑣!\"#\np(𝑣!\"#|𝑓!)\n𝑢#\n𝑢$\n𝑢%\n𝑢&\n𝑢'\n𝑣(\n𝑣)\n𝑣*\n𝑣+\n𝑢$\n𝑢%\n𝑢&\n𝑢'\n𝑣(\n𝑣)\n𝑣*\n𝑣+\n𝑢!\"\n🔥\nFigure 2. Architecture of Orthus. Orthus is composed of a text tokenizer, a vision autoencoder, two modality-specific embedding modules, a\ntransformer backbone, and two modality-specific heads. Orthus tokenizes texts into discrete text tokens and encodes images into continuous\npatch-wise features. They are then embedded as a sequence of vectors and processed by the transformer backbone with causal attention,\ngenerating a sequence of output vectors. The vectors are routed to modality-specific heads, with the LM head to predict the next text token\ncategorically and the diffusion head to predict the next image patch feature through conditional diffusion modeling.\nHowever, diffusion modeling inherently requires feeding\nnoisy inputs to the model, hindering joint modeling of vi-\nsual understanding (requiring clean images) and generation\n(requiring noisy ones). For example, Transfusion identifies\na nearly 15% performance drop in image captioning when\nfull-range noise is introduced during training [77].\n4. Method\nWe introduce Orthus to address the issues of existing works.\nThis section begins with an overview of Orthus and then\nelaborates on an efficient training recipe for Orthus. We\nwill also illustrate a post-training pipeline of Orthus.\n4.1. Overview of Orthus\nAs shown in Figure 2, Orthus directly takes the continuous\nimage features V and discrete text tokens U as input, which\navoids the pathologies caused by the quantized image fea-\ntures ˜V or noisy image features ¯V . U and V are embedded\ninto the de-dim representation space with a differentiable\nvision embedding module (detailed in the next subsection)\nand the aforementioned discrete embedding module respec-\ntively. Subsequently, the embeddings are fed into the trans-\nformer backbone with causal attention for the modeling of\nboth inter- and intra-modality interdependence. Given the\noutput states of such a backbone contain enough informa-\ntion about the multimodal context, Orthus sends them to\ntwo modality-specific heads—a diffusion head and an LM\nhead—to predict the next image patch or the next token.\nSpecifically, let fi denote the output state corresponding\nto the input image feature vi and ϵθ denote the diffusion\nhead employed by Orthus with parameter θ. The goal of\nϵθ is to predict for the next patch feature vi+1 conditioning\non fi. According to common practice [8, 21], the learning\nobjective for the diffusion head can be formalized as:\nLdiff = Eϵ,t[∥ϵ −ϵθ(√αtvi+1 +\n√\n1 −αtϵ, t, fi)∥2\n2], (3)\nwhere ϵ ∼N(0, I) is a Gaussian noise and t is a randomly\nsampled timestep. αt follows a pre-defined noise sched-\nule [21]. In practice, ϵθ can be a shallow multilayer percep-\ntion (MLP) with three inputs (the condition fi, the scalar\ntimestep t, and the noisy state). On the other hand, the LM\nhead remains the compact linear projection followed by a\nsoftmax transformation to yield the predictive probability\nof the next token over the entire vocabulary.\n4.2. An Efficient Strategy for Constructing Orthus\nThe differences between Orthus and fully AR models exist\nin the vision embedding module and the output head. Given\nthat pre-training a multimodal model from scratch can be\nfrustratingly costly but the fully AR models like LWM [35]\nand Chameleon [53] are readily accessible from the open-\nsource community, we are naturally interested in deriving\nOrthus based on them at a minimal expense. This section\nelaborates on a hard-to-soft adaptation trick and an efficient\ntraining strategy to enable this.\nDifferentiable vision embedding module.\nIt is easy to\nnote that the embedding yielded by Equations 1 and 2 can be\nequivalently obtained via a softmax-based transformation\nhi =\nX\nj\nwj\ne−d(vi,cj)\/τ\nPK\nk=1 e−d(vi,ck)\/τ ,\n(4)\nwith τ →0. Increasing τ gradually from 0 then naturally\nlifts the information bottleneck from the image features vi\nto the model outputs fi, while rendering the reuse of the\n4\npre-trained weights and codes of fully AR models possible.\nThis way, the codes {cj}K\nj=1 also become a part of the input\nmodule, so we can leverage gradients to directly push them\nto adapt to the multimodal learning tasks. This contradicts\nfully AR models which froze the codes during training.\nTraining strategy. With the above trick, we start with a\npre-trained fully AR model, transform its input module into\na differentiable one, and introduce an output diffusion head\nto initialize Orthus. These modifications primarily focus on\nthe visual part, thus we recommend fine-tuning the initial-\nized model on a collection of images. In particular, we input\nonly the image into Orthus to acquire the hidden states fi\nand utilize the diffusion loss in Equation 3 to recover the\nnext patch to train the vision embedding module and diffu-\nsion head. The temperature τ is set to 1 during training.\nInitialized with the typical Chameleon-7B [53], Orthus\ncan acquire image processing capabilities while preserving\nthe text generation of Chameleon after 9-hour training on\n10k high-quality images [19] using 8 A100 GPUs. We des-\nignate this model as Orthus-base. Figure 4 shows its image\nand text (as well as mixed-modality) generation prowess.\nAlthough the decoder in the VQ-VAE [59] used by\nChameleon can reconstruct the raw image pixels given the\npatch-wise features V to some extent, it can be subopti-\nmal due to the quantization-aware training. To address this,\nwe advocate further tuning its decoder to reconstruct high-\nquality images directly based on V . The comparison be-\ntween the capacity of the original VQ-VAE of Chameleon\nand ours is exhibited in Appendix A.\n4.3. Multimodal Post-training\nFollowing the typical training procedure of multimodal\nmodels [61, 67], we further refine Orthus-base by fine-\ntuning it on high-quality text-image pairs and instructional\ndata. All parameters except for those of the vision autoen-\ncoder are tuned. Following the input format of Chameleon,\nwe surround image features V with the embeddings of spe-\ncial tokens [BOI] and [EOI] before the concatenation\nwith text embeddings. A [SEP] token is used to separate\nuser input and model output in each conversation.\nLet Lar denote the AR loss on the text tokens. The entire\ntraining objective of Orthus is then LOrthus = Lar + λLdiff,\nwhere λ is a balancing coefficient. Hereinafter, we will de-\nnote the model trained following this objective as Orthus,\ndistinguishing it from Orthus-base.\nDuring inference, Orthus alternates between next-token\npredition and next-patch prediction to seamlessly generate\ninterleaved texts and images. When [BOI] is sampled dur-\ning the next-token prediction process, the algorithm moves\nto next-patch prediction. Once a fixed number of n image\npatches are generated, [EOI] is appended and the algo-\nrithm switches back to next-token prediction.\n5. Experiments\nIn this section, we evaluate Orthus’s performance across\nmultimodal understanding, visual generation, and mix-\nmodality generation tasks. Both quantitative and qualitative\nresults demonstrate the effectiveness of Orthus.\n5.1. Experimental Setup\nImplementation Details. We implement the diffusion head\nas an MLP consisting of 3 residual blocks, each sequentially\napplying AdaLN [38], a linear layer (width of 1536 chan-\nnels), SiLU activation, and another linear layer. The con-\ndition vector fi is added to the diffusion time embedding,\nwhich is then incorporated through AdaLN. The diffusion\nnoise schedule is linear following [44], with 1000 steps at\ntraining time. During the training of Orthus-base, we em-\nploy AdamW (β1 = 0.9, β2 = 0.99) with a learning rate\nof 1e-4 and a cosine decay scheduler. During post-training\nfrom Orthus-base to Orthus, we mix LlaVA-v1.5-665K [34]\ninstruction tuning data and high-quality text-to-image data\n(JourneyDB [48] and LAION-COCO-aesthetic [19] recap-\ntioned from ShareGPT-4v [3]) at a ratio of 1:1 and optimize\nwith a learning rate of 1e-5. λ is set to 100 during post-\ntraining. During inference, we use greedy decoding to gen-\nerate text. For image generation, we adopt the DDIM [46]\nsampler with 100 steps. We employ classifier-free guidance\n(CFG) [20] with the scale set to 5 during sampling. All im-\nages are generated at a resolution of 512×512. Both train-\ning and evaluation are carried out on servers equipped with\n8 NVIDIA A100 80GB GPUs.\nBaselines.\nWe make comparisons with:\n(i) image-\nunderstanding models including LlaVA [32, 34], Instruct-\nBLIP [7], Qwen-VL-Chat [1], mPLUG-Owl2 [71], and\nIDEFICS-9B [26]; (ii) image-generation models including\nLlamaGen [49], SDv1.5 [44], PixArt-α [2], SDv2.1 [44],\nDALL-E 2 [42],\nSDXL [39],\nand SD3 [12];\n(iii)\nunified models including Emu [50], NExT-GPT [64],\nSEED-X [15], Gemini-Nano-1 [54], LWM [35], Transfu-\nsion [77], and Show-o [67]. We also fine-tune pre-trained\nChameleon [6] with the same high-quality post-training\ndataset as Orthus to provide an apple-to-apple baseline.\n5.2. Multimodal Understanding\nBenchmarks. Following LLaVa [34], we evaluate the mul-\ntimodal understanding capabilities of Orthus on POPE [29],\nMME [14], VQAv2 [18], GQA [22], and MMMU [75] and\nreport the exact match accuracy.\nResults. Table 1 summarizes the capabilities of Orthus on\nboth domain-specific visual question answering and general\nvisual comprehension tasks. (i) Compared to Chameleon\npost-trained with the same dataset, Orthus consistently\ndemonstrates superior performance across all benchmarks.\nBesides, inspecting OCR-related tasks in MME-P, we wit-\nness a significant superiority of Orthus over Chameleon\n5\nType\nModel\n# Params\nPOPE↑\nMME-P↑\nVQAv2↑\nGQA↑\nMMMU↑\nUnd. Only\nLlaVa [34]\n7B\n76.3\n809.6\n-\n-\n-\nLlaVA-v1.5 [32]\n7B\n85.9\n1510.7\n78.5\n62.0\n35.4\nInstructBLIP [7]\n7B\n-\n-\n-\n49.2\n-\nQwen-VL-Chat [1]\n7B\n-\n1487.5\n78.2\n57.5\n-\nmPLUG-Owl2 [71]\n7B\n85.8\n1450.2\n79.4\n56.1\n-\nIDEFICS-9B [26]\n8B\n-\n-\n-\n50.9\n-\nInstructBLIP [7]\n13B\n78.9\n1212.8\n-\n49.5\n-\nUnd. and Gen.\nEmu∗[50]\n13B\n-\n-\n52.0\n-\n-\nNExT-GPT∗[64]\n13B\n-\n-\n66.7\n-\n-\nGemini-Nano-1 [54]\n1.8B\n-\n-\n62.7\n-\n26.3\nShow-o [67]\n1.3B\n73.8\n948.4\n59.3\n48.7\n25.1\nLWM [35]\n7B\n75.2\n-\n55.8\n44.8\n-\nChameleon†\n7B\n77.8\n1056.9\n57.8\n49.6\n26.7\nOrthus (Ours)\n7B\n79.6\n1265.8\n63.2\n52.8\n28.2\nTable 1. Evaluation on multimodal understanding benchmarks. Und. and Gen. denote “understanding” and “generation”, respectively.\nModels using external pre-trained diffusion models are marked with * and Chameleon† is post-trained with the same dataset as Orthus.\nThe results in bold and underline are the best and second-best results, respectively. The results correspond to the exact match accuracy.\n(with scores of 70 vs. 45). These results validate the su-\nperiority of Orthus’s modeling by adopting lossless repre-\nsentations for images. (ii) On the GQA benchmark, Or-\nthus outperforms larger, understanding-only models such as\nInstructBLIP-13B and IDEFICS-9B. Moreover, it achieves\ncompetitive results with models like Qwen-VL-Chat and\nmPLUG-Owl2, which have been trained extensively on\ntask-specific diverse VQA datasets.\nWe can expect en-\nhanced visual understanding capabilities of Orthus by incor-\nporating more high-quality data for post-training. (iii) Or-\nthus outperforms other unified models using a single trans-\nformer like LWM and Show-o across all benchmarks, high-\nlighting its efficacy for unified modeling. (iv) Compared to\nlarger unified models using an external diffusion model and\npre-trained with extensive image-text pairs, such as NExT-\nGPT-13B, Orthus achieves decent results on the VQAv2\nbenchmark. It is reasonable to speculate that Orthus’s po-\ntential for multimodal understanding problems can be fur-\nther unleashed by scaling up training compute and data.\n5.3. Visual Generation\nBenchmarks. Following SD3 [12], we evaluate Orthus’s\nvisual generation performance on GenEval [16], a bench-\nmark that examines the compositional capabilities of the\nimage generation model through a detailed instance-level\nanalysis.\nAdditionally, we evaluate on HPSv2 [65], a\nbenchmark based on human preferences.\nQuantitative Results. We report GenEval scores across\nsix dimensions and the HPSv2 score in Table 2. (i) When\ncompared with strong competitors specialized for text-to-\nimage generations such as DALL·E 2 and SDXL, Orthus\nachieves an improvement of 0.06 and 0.03 on GenEval, re-\nspectively. While Orthus lags behind SD3, it’s worth noting\nthat SD3 benefits from synthetic image captions generated\nthrough backtranslation, whereas Orthus does not employ\nthis trick. (ii) Compared to Chameleon and its post-trained\nversion, Orthus demonstrates significant superiority on both\nGenEval and HPSv2.\nThis advantage can be attributed\nto the utilization of continuous image representations and\ndiffusion-based continuous modeling, which facilitates the\ngeneration of high-quality images with richer detail and\nstronger alignment with human preferences. (iii) Compared\nwith other unified models such as SEED-X, LWM, and\nShow-o, Orthus obtains significantly better performance,\nhighlighting the advantages of its modeling strategy. Or-\nthus also achieves comparable results to Transfusion, which\nleverages a larger dataset of text-image pairs during pre-\ntraining. (iv) Orthus outperforms both SDv1.5 and SDv2\nin HPSv2, showing its ability to generate high-quality, aes-\nthetically pleasing images aligned with human preference.\nQualitative results. Figure 3 showcases images generated\nby Orthus alongside results from other unified models, in-\ncluding Chameleon and Show-o. Results show that Orthus\nis capable of generating diverse, engaging, and realistic vi-\nsual imagery at the resolution of 512×512.\n5.4. Mixed-modality Generation\nIn this section, we investigate the mixed-modality genera-\ntion capabilities of Orthus. We first show that Orthus-base\ncan perform mixed-modality generation, as demonstrated\nin the left of Figure 4. Orthus-base generates a coherent\nsequence of interleaved image-text segments based on in-\n6\nShow-o\nChameleon\nOrthus\nA detailed ink illustration of a hedgehog.\nOil painting portrait of a young woman in a field of flowers\nat sunset with mountains in the background.\nA hyena fursona sits in a savannah sunset amidst the grass.\nFigure 3. Left: Comparison between images generated by Show-o, Chameleon, and Orthus based on the same prompts. Samples produced\nby Orthus contain more details. Right: Text-to-image gallery of Orthus.\nType\nModel\nRes.\nGenEval ↑\nHPSv2↑\nSingle Obj.\nTwo Obj.\nCount.\nColors\nPosition\nColor Attri.\nOverall\nGen.\nOnly\nLlamaGen [49]\n512\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\n26.3\nSDv1.5 [44]\n512\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\n27.0\nPixArt-α [2]\n512\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\n-\nSDv2.1 [44]\n512\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\n27.2\nDALL-E 2 [42]\n512\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\n26.9\nSDXL [39]\n512\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\n30.9\nSD3(d=30) [12]\n512\n0.96\n0.80\n0.65\n0.73\n0.33\n0.37\n0.64\n-\nUnd.\n&\nGen.\nSEED-X∗[15]\n448\n0.97\n0.58\n0.26\n0.80\n0.19\n0.14\n0.49\n-\nChameleon [53]\n512\n-\n-\n-\n-\n-\n-\n0.39\n-\nLWM [35]\n256\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\n26.1\nShow-o [67]\n256\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n0.53\n27.3\nTransfusion [77]\n256\n-\n-\n-\n-\n-\n-\n0.63\n-\nChameleon†\n512\n0.98\n0.47\n0.18\n0.75\n0.06\n0.15\n0.43\n26.9\nOrthus (Ours)\n512\n0.99\n0.75\n0.26\n0.84\n0.28\n0.38\n0.58\n28.2\nTable 2. Comparison with state-of-the-arts on visual generation benchmarks. Und. and Gen. denote “understanding” and “generation”,\nrespectively. Model using external pre-trained diffusion model is marked with * and Chameleon† is post-trained with the same dataset as\nOrthus. The results in bold and underline are the best and second-best results, respectively.\nstructions from users in a zero-shot manner.\nTo further validate Orthus’s superiority in modeling in-\nterleaved data, we fine-tune Orthus-base with the unified\nlearning objective on the StoryStream [69] dataset, which\nincludes a collection of images and corresponding narra-\ntives from cartoon series.\nAs shown in Figure 4, after\ntraining, Orthus can generate contextually consistent scenes\npaired with narrative text given an initial image-text pair and\nthe instruction “Please continue this story.” Notably, there is\na strong alignment between images and text (e.g., the smile\non the monkey’s face) as well as consistent detail across\nimages (e.g., the boy’s orange hat). These results highlight\nOrthus’s potential for various applications, including report\ngeneration, educational content creation, and others requir-\ning seamless multimodal content generation.\n5.5. Ablation Studies\nSeparate training vs.\nunified training.\nTo validate\nthe efficacy of Orthus for unified multimodal modeling,\nwe compare baselines using identical training data but\nwith different learning objectives: (i) a generation-only\nbaseline focused solely on text-to-image generation; (ii)\nan understanding-only baseline dedicated to visual under-\nstanding tasks; and (iii) a unified training objective, the de-\nfault setting in Orthus. Table 3 shows that both understand-\ning and generation metrics are better through unified train-\n7\nGenerate an overview of famous \nhistorical landmarks. Include \npictures and descriptions of each.\n1. Stonehenge\nStonehenge has been around for \nover 5,000 years, and it continues \nto captivate people to this day. \nThe structure was likely built by \nthe ancient Druids, who believed it \nheld spiritual significance. The \nsite is made up of large stones \nthat were moved from nearby \nquarries using simple tools made of \nwood. While the exact purpose of \nStonehenge remains a mystery, many \nbelieve it was used for religious \nrituals and ceremonies. Here is a \npicture of what Stonehenge looks \nlike:\n2. The Great Pyramids of Giza\nThe Great Pyramids of Giza are one \nof the oldest and most iconic \nlandmarks in the world. These \nincredible structures were built \nover 4,500 years ago during the Old \nKingdom period in Egypt. The \nlargest of the three pyramids, \nknown as the Great Pyramid of Khufu, \nis considered one of the Seven \nWonders of the Ancient World. Here \nis an image of the Great Pyramids \nof Giza:\nAfter shoveling, \nthe man in the \norange hat stood \ncontentedly in the \nsnowy landscape, \nhis smile bright \nagainst the \novercast sky.\nGeorge, was also \nout in the snowy \nlandscape. He wore \na red jacket, hat, \nand gloves, \nwondering if more \nsnow was on the way.\nAfter a while, \nGeorge, decided to \ntake a break and \nsat on the grass, \nenjoying the \npeaceful trees and \nthe buildings.\nGenerate me some facts about polar \nbears, along with pictures.\nOne sunny morning, \nGeorge the curious \nmonkey and his friend \ndecided to visit the \nbustling city park. \nThey started their \nadventure by exploring \na colorful playground.\nPlease continue this story:\nStartled, George \nstood in the dark \nforest, his large, \nmenacing shadow \ncast on the trees. \nHis flashlight lay \non the ground,\nprojecting the \nfrightening \nsilhouette.\nOn a snowy day, \nGeorge, the small \nfigure in red winter \ngear, was excited to \nhelp his friend, the \nman in the yellow hat \nand red scarf. They \nwere outside their \nblue house.\nPlease continue this story:\nBut George was not \none to be scared \neasily. \nHe stood inside a \nlively cave, ready \nfor an adventure. \nThe hint of light \npeering behind him \ncast a soft glow, \nmaking his eyes \nsparkle with \ncuriosity.\nIn the shadowy \nforest, George \nspotted a red ball. \nHe quickly grabbed \nit, excited about \nhis new find. Trees \nand bushes, \nilluminated by a \nlight source, \nsurrounded him.\nWith a smile on \nhis face, George \nstood in the cave \nwith his arms \noutstretched. He \nwas enjoying his \nadventure, \nsurrounded by \nstalactites and a \nglowing light.\n1. Physical Characteristics:\n* Polar bears have black skin \nunder their fur, which helps to \nabsorb the sun's warmth and \nconserve body heat.\n* They have powerful front legs \nand sharp claws that are used for \nhunting seals in the icy waters \nof the Arctic.\n* Polar bears can weigh up to \n1,500 pounds (680 kg) and reach \nlengths of up to 10 feet (3 \nmeters)\nHere is an image of a polar bear \non the ice:\n2. Habitat:\n* Polar bears live in the Arctic, \nspecifically in the northern \nregions of Canada, Alaska, and \nGreenland.\n* They inhabit a wide range of \nenvironments, including snowy \ntundras, ice sheets, and glaciers.\n* In the summer months, polar \nbears migrate to coastal areas to \nhunt for seals and other prey.\nHere is an image of its natural \nhabitat:\n3. Diet and Hunting:\n* Polar bears are carnivores and \ntheir diet consists mainly of \nringed seals, lemming mice, and...\nPlease continue this story:\nFigure 4. Qualitative results of interleaved image-text generation. Left: Zero-shot results generated by Orthus-base which is only trained\nfor 72 A100 GPU hours. Right: Interleaved storybook creation results after finetuned on the StoryStream [69] dataset. Results show that\nOrthus excels in generating logically coherent interleaved image-text with high relevance.\nType\nLdiff Lar POPE↑MME-P↑GQA↑GenEval↑\nUnd. only\n✗\n✓\n78.7\n1244.2\n51.9\n-\nGen. only\n✓\n✗\n-\n-\n-\n0.56\nUnd. & Gen.\n✓\n✓\n79.6\n1265.8\n52.8\n0.58\nTable 3. Comparisons of the performance of Orthus via separate\ntraining and unified training across multimodal benchmarks.\ning compared to separate task-specific training, highlighting\nthe superiority of Orthus’s modeling which facilitates infor-\nmation gains from bidirectional cross-modal learning.\nImpact of vision embedding modules on visual under-\nstanding tasks. In this section, we ablate the impact of dif-\nferent choices of vision embedding modules to build Orthus\nfrom fully AR models on visual understanding. When we\nretain the original embedding module in fully AR models\n(“argmin” in Table 4), a performance drop is observed due\nto the information loss. Moreover, replacing the embedding\nmodule with a randomly initialized linear layer also leads to\nsuboptimal performance due to the significant distribution\nshift between the embedded space and the transformer’s in-\nput space. This misalignment may necessitate training with\nmore image-text pairs to mitigate.\nLoss design. To test the necessity of diffusion modeling for\nType\nPOPE↑MME-P↑VQAv2↑GQA↑MMMU↑\nsoftmax\n78.7\n1244.2\n60.8\n51.9\n28.0\nargmin\n77.6\n1064.8\n57.9\n50.1\n26.7\nlinear\n70.4\n800.7\n50.3\n44.5\n22.3\nTable 4. Ablation study on the choice of vision embedding mod-\nules on visual understanding tasks.\nthe image features, we train the MLP head with straightfor-\nward Mean Squared Error (MSE) loss between predictions\nand target features. As shown in Appendix C, the model\ntrained with MSE loss generates degraded samples that lack\ndetails and exhibit limited color diversity. The reason is that\nthe deterministic nature of MSE loss leads to mode collapse.\n6. Conclusion\nIn this paper, we propose Orthus, a unified multimodal\nmodel for interleaved image-text understanding and gen-\neration.\nOrthus generates content across modalities by\nrouting the outputs from its shared transformer backbone\nto modality-specific heads.\nIts continuous treatment of\nvisual signals preserves input integrity and its unified\nAR modeling approach for both discrete text tokens and\n8\ncontinuous image features enables its superior performance\nacross various multimodal understanding and generation\nbenchmarks.\nFor future work, we plan to scale Orthus\nby expanding its parameter size and leveraging larger,\ninterleaved datasets to maximize its potential. Furthermore,\nwe aim to broaden its multimodal capabilities by incor-\nporating additional modalities, including video and audio.\nReferences\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond, 2023. 5,\n6\n[2] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao,\nEnze Xie, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. Pixart-$\\alpha$: Fast training\nof diffusion transformer for photorealistic text-to-image syn-\nthesis. In The Twelfth International Conference on Learning\nRepresentations, 2024. 5, 7\n[3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 5\n[4] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPali-x: On\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 2\n[5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 24185–24198, 2024. 2\n[6] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole:\nAn open, autoregressive, native large multimodal mod-\nels for interleaved image-text generation.\narXiv preprint\narXiv:2407.06135, 2024. 3, 5\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 2, 5, 6\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 2, 4\n[9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 3\n[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha\nChowdhery,\nBrian\nIchter,\nAyzaan\nWahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-\ne: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023. 2\n[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE\/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 3\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn Forty-first International Conference on Machine Learn-\ning, 2024. 2, 5, 6, 7\n[13] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen\nSun, Michael Rubinstein, Deqing Sun, Kaiming He, and\nYonglong Tian. Fluid: Scaling autoregressive text-to-image\ngenerative models with continuous tokens. arXiv preprint\narXiv:2410.13863, 2024. 3\n[14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A compre-\nhensive evaluation benchmark for multimodal large language\nmodels, 2024. 5\n[15] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin\nSong, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Mul-\ntimodal models with unified multi-granularity comprehen-\nsion and generation. arXiv preprint arXiv:2404.14396, 2024.\n3, 5, 7\n[16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: An object-focused framework for evaluating text-\nto-image alignment. Advances in Neural Information Pro-\ncessing Systems, 36, 2024. 2, 6\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2\n[18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904–6913, 2017. 5\n[19] guangyil. https:\/\/huggingface.co\/datasets\/\nguangyil\/laion-coco-aesthetic. 5, 1\n[20] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 1, 2, 3, 4\n[22] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019. 2, 5\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE\/CVF conference on computer vi-\nsion and pattern recognition, pages 4401–4410, 2019. 2\n[24] Diederik P Kingma. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114, 2013. 2\n9\n[25] Diederik P. Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In In 2nd International Conference on Learning\nRepresentations, 2013. 2\n[26] H. Laurenc¸on, D. van Strien, S. Bekman, L. Tronchon, L.\nSaulnier, and et al. Introducing idefics: An open reproduc-\ntion of state-of-the-art visual language model, 2023. 5, 6\n[27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation. In Interna-\ntional conference on machine learning, pages 12888–12900.\nPMLR, 2022. 2\n[28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and\nKaiming He. Autoregressive image generation without vec-\ntor quantization. arXiv preprint arXiv:2406.11838, 2024. 2,\n3\n[29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 2, 5\n[30] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Moham-\nmad Shoeybi, and Song Han. Vila: On pre-training for vi-\nsual language models. In Proceedings of the IEEE\/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n26689–26699, 2024. 2\n[31] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin,\nYu Qiao, Hongsheng Li, and Peng Gao.\nLumina-mgpt:\nIlluminate flexible photorealistic text-to-image generation\nwith multimodal generative pretraining.\narXiv preprint\narXiv:2408.02657, 2024. 3\n[32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 2, 5, 6\n[33] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 1\n[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 1, 2, 5, 6\n[35] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 3,\n4, 5, 6, 7\n[36] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nblockwise ringattention, 2024. 1, 2, 3\n[37] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer.\nIn International conference on machine\nlearning, pages 4055–4064. PMLR, 2018. 3\n[38] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE\/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 3, 5\n[39] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 1, 2, 5, 7\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 1, 2\n[41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International confer-\nence on machine learning, pages 8821–8831. Pmlr, 2021. 3\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2, 5, 7, 1\n[43] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems, 32, 2019. 3\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 1, 2, 5, 7\n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256–2265. PMLR, 2015.\n2\n[46] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 5\n[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[48] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong\nDuan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin,\nYi Wang, et al. Journeydb: A benchmark for generative im-\nage understanding. Advances in Neural Information Process-\ning Systems, 36, 2024. 5\n[49] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 3, 5, 7\n[50] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Emu: Generative pretraining in\nmultimodality. In The Twelfth International Conference on\nLearning Representations, 2023. 3, 5, 6\n[51] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiy-\ning Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang.\nGenerative multimodal mod-\nels are in-context learners. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\npages 14398–14409, 2024. 1\n10\n[52] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong\nChen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and\nSong Han. Hart: Efficient visual generation with hybrid au-\ntoregressive transformer. arXiv preprint arXiv:2410.10812,\n2024. 3\n[53] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n1, 2, 3, 4, 5, 7\n[54] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 5, 6\n[55] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang.\nVisual autoregressive modeling: Scalable im-\nage generation via next-scale prediction.\narXiv preprint\narXiv:2404.02905, 2024. 3\n[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 1, 3\n[57] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 1\n[58] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical vari-\national autoencoder. Advances in neural information pro-\ncessing systems, 33:19667–19679, 2020. 2\n[59] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 3, 5\n[60] A Vaswani. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 3\n[61] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min,\nTao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang,\nGuang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin,\nTiejun Huang, and Zhongyuan Wang. Emu3: Next-token\nprediction is all you need, 2024. 5\n[62] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is\nall you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2,\n3\n[63] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600–612, 2004. 1\n[64] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. In Forty-first\nInternational Conference on Machine Learning, 2013. 1, 5,\n6\n[65] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\nZhu, Rui Zhao, and Hongsheng Li. Human preference score\nv2: A solid benchmark for evaluating human preferences of\ntext-to-image synthesis, 2023. 6, 1\n[66] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang,\nDacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model inte-\ngrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024. 3\n[67] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One\nsingle transformer to unify multimodal understanding and\ngeneration. arXiv preprint arXiv:2408.12528, 2024. 2, 3,\n5, 6, 7, 1\n[68] Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei\nZhai, Yang Cao, and Zheng-Jun Zha.\nMmar: Towards\nlossless multi-modal auto-regressive prababilistic modeling.\narXiv preprint arXiv:2410.10798, 2024. 2\n[69] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge,\nYing Shan, and Yingcong Chen. Seed-story: Multimodal\nlong story generation with large language model.\narXiv\npreprint arXiv:2407.08683, 2024. 7, 8\n[70] Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei\nPing, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo\nMolchanov, et al. X-vila: Cross-modality alignment for large\nlanguage model. arXiv preprint arXiv:2405.19335, 2024. 3\n[71] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu,\nHaowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2:\nRevolutionizing multi-modal large language model with\nmodality collaboration.\nIn Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13040–13051, 2024. 5, 6\n[72] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. arXiv preprint\narXiv:2205.01917, 2022. 2\n[73] Lijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Ver-\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh\nBirodkar, Agrim Gupta, Xiuye Gu, et al. Language model\nbeats diffusion–tokenizer is key to visual generation. arXiv\npreprint arXiv:2310.05737, 2023. 3\n[74] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen,\nDaniel Cremers, and Liang-Chieh Chen. An image is worth\n32 tokens for reconstruction and generation. arXiv preprint\narXiv:2406.07550, 2024. 3\n[75] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Ren-\nliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo\nLiu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMmmu: A massive multi-discipline multimodal understand-\ning and reasoning benchmark for expert agi, 2024. 5\n[76] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng\nFeng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong\nWang. Monoformer: One transformer for both diffusion and\nautoregression. arXiv preprint arXiv:2409.16280, 2024. 1, 3\n[77] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\n11\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024. 1, 2, 3, 4, 5,\n7\n[78] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 2\n12\nOrthus: Autoregressive Interleaved Image-Text Generation with\nModality-Specific Heads\nSupplementary Material\nA. Comparison of Vision Autoencoder\nTo construct a vision autoencoder capable of decoding high-\nquality images based on continuous image features V , we\nfreeze the encoder of Chameleon’s VQ-VAE, drop the quan-\ntization step, and finetune the decoder only to reconstruct\nimages, transforming it into a conventional continuous au-\ntoencoder effectively. Table 5 shows that our vision au-\ntoencoder achieves better reconstruction quality compared\nto the original VQ-VAE. The evaluation is on the subset of\nLAION-Aesthetic [19] containing 10000 images.\nModel\nPSNR↑\nSSIM [63]↑\nVQ-VAE\n23.7\n0.80\nOurs\n26.1\n0.84\nTable 5. Comparison of reconstruction quality for vision autoen-\ncoders: the discrete one is worse than the continuous variant.\nB. Detailed Scores of HPSv2\nTable 6 reports scores of HPSv2 [65] across four categories:\n‘Anime’, ‘Concept art’, ‘Paintings’, and ‘Photo’.\nModel\nAni.\nConc.\nPaint.\nPho.\nOverall↑\nLDM [44]\n25.7\n25.2\n25.3\n27.0\n25.8\nDALL-E 2 [42]\n27.3\n26.5\n26.7\n27.2\n26.9\nSDv1.5 [44]\n27.3\n26.6\n26.7\n27.3\n27.0\nSDv2 [44]\n27.5\n26.9\n26.9\n27.5\n27.2\nSDXL [39]\n36.0\n34.9\n35.7\n31.9\n30.9\nShow-o [67]\n27.9\n27.3\n27.1\n27.0\n27.3\nChameleon†\n26.9\n27.2\n27.3\n26.0\n26.9\nOrthus(Ours)\n28.2\n28.4\n28.4\n27.8\n28.2\nTable 6. Comparison of state-of-art baselines on HPSv2.\nC. Importance of Diffusion Modeling\nFigure 5. Text-to-image results from models trained with MSE\nloss. The text prompt is “Generate an image of a snowman.”\nD. Qualitative Results on Visual Understand-\ning\nIn addition to quantitatively evaluating Orthus on domain-\nspecific tasks in Section 5.2, we also assess its performance\nin general chat scenarios. Results in Figure 6 demonstrate\nthat Orthus exhibits strong instruction-following capabili-\nties and robust generalization abilities.\nFor a farm like this, featuring tall wheat or grain crops, \nessential farm equipment would include combine harrows to harvest \nthe plants efficiently and a harrow-drawn trailer to transport the \nharvested wheat. Other crucial pieces of equipment might include \ntractors with specialized configurations for wheat cultivation, \ntractors combining harvested grain crops or straw with other crops, \nand combine drills to evenly distribute the seeds. Additional farm \nsupplies for such a setting might include backpacks to carry \nessential tools, gloves to facilitate maneuvering the heavy \nequipment, and safety gear for the operators and workers on the \nfarm. In the image, a harrow is also visible, demonstrating the \npresence of this crucial piece of wheeled farm equipment.\nCan you please tell me what kind of farm equipment would be \nessential for this kind of farm?\nBased on the image description, the dog is a shaggy, brown, and \ntan puppy. It could be a Golden Retriever, a shaggy breed that \nis commonly known for its fluffy, shaggy coat. However, to \nidentify the breed precisely, one would require comparing the \npuppy's appearance to the breed standards and characteristics \nsuch as size, nose shape, eye shape, ear shape, and other \nphysical qualities.\nYes, the shaggy puppy in the image would make a suitable pet \nwith numerous positive traits. For example, Goldendoodles are \nknown for their affectionate nature, intelligence, good \ntrainability, and gentle, friendly demeanor, making them a \npopular choice as a pet. If this indeed is a shaggy Golden \nRetriever, it would excel in homes with children and families. \nCould you help identify the breed of this dog in the image? Also, \ndo you think it would be a suitable choice as a pet?\nFigure 6. Qualitative analysis of Orthus on visual understanding.\nE. Qualitative Results on Visual Generation\nFigure 7 shows examples of images generated from Orthus.\n1\nA photorealistic image of \na giant floating glass \nsphere in a rocky \nlandscape surrounded by \na gentle mist.\nAnimals fashioned from \ngems, colorful and shapely, \ndepicted in natural \nlighting, with a slight \neffervescence.\nA portrait painting of a \nSouth Indian woman \nwearing a sari with \nintricate details and an \neerie sense of horror, \ncreated in ultra-realistic \nstyle by artgerm.\nA serene meadow with a \ntree, river, bridge, and \nmountains in the \nbackground under a \nslightly overcast \nsunrise sky.\nDigital painting of a furry \ndeer character on FurAffinity.\nA stylized building in \nwatercolor gouache, featuring \ninteresting shapes and forms, \nlocated in a desolate \nlandscape with a food stall in \nan Asian-style alleyway.\nA painting of a Persian cat \ndressed as a Renaissance \nking, standing around a \nskyscraper overlooking a \ncity.\nA cobblestone street with a \ntree over the sea at sunset, \nilluminated by sun rays, in a \ncolorful illustration by \nPeter Chan on Artstation.\nA close-up oil painting of \na littlest pet shop fuzzy \nskunk in a field.\nA glowing dry tree stands \nalone under a starry sky in \na detailed fantasy artwork\nA girl looks out from the \nedge of a mountain onto a \nlarge city at night.\nA gangster squirrel is \ncounting his money in a \nlow angle film still.\nA neon-colored frog in a \ncyberpunk setting.\nAn anthropomorphic frog \nwizard wearing a cape and \nholding a wand.\nPortrait of a monkey wearing \nan astronaut helmet.\nPortrait of Archduke Franz \nFerdinand by Charlotte \nGrimm, depicting his \ndetailed face.\nFigure 7. Generated 512 × 512 images from Orthus. Results demonstrate its ability to generate diverse, engaging, and realistic images.\n2\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads.pdf"}
{"title":"JetFormer: An Autoregressive Generative Model of Raw Images and Text","authors":"Michael Tschannen, André Susano Pinto, Alexander Kolesnikov","summary":"Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.","url":"http:\/\/arxiv.org\/abs\/2411.19722v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2411.19722v1","published":1732889699000,"comment":null,"pdf_text":"JETFORMER: AN AUTOREGRESSIVE\nGENERATIVE MODEL OF RAW IMAGES AND TEXT\nMichael Tschannen∗\nAndr´e Susano Pinto∗\nAlexander Kolesnikov∗\nGoogle DeepMind\n{tschannen,andresp,akolesnikov}@google.com\nABSTRACT\nRemoving modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further stream-\nline joint generative modeling of images and text. We propose an autoregressive\ndecoder-only transformer—JetFormer—which is trained to directly maximize the\nlikelihood of raw data, without relying on any separately pretrained components,\nand can understand and generate both text and images. Specifically, we lever-\nage a normalizing flow model to obtain a soft-token image representation that is\njointly trained with an autoregressive multimodal transformer. The normalizing\nflow model serves as both an image encoder for perception tasks and an image\ndecoder for image generation tasks during inference. JetFormer achieves text-to-\nimage generation quality competitive with recent VQVAE- and VAE-based base-\nlines. These baselines rely on pretrained image autoencoders, which are trained\nwith a complex mixture of losses, including perceptual ones. At the same time,\nJetFormer demonstrates robust image understanding capabilities. To the best of\nour knowledge, JetFormer is the first model that is capable of generating high-\nfidelity images and producing strong log-likelihood bounds.\n1\nINTRODUCTION\nThe “Bitter lesson” (Sutton, 2019) has been the prime force behind the recent progress in machine\nlearning and artificial intelligence research. It suggests that general-purpose methods which effec-\ntively leverage large amounts of compute and data prevail over specialized techniques designed by\ndomain experts. Arguably the most prominent examples in this context are transformer decoder-only\nmodels trained for next-token prediction (Vaswani et al., 2017; Radford et al., 2018), that outperform\ntask-specific NLP systems, and transformer encoders in computer vision (Dosovitskiy et al., 2021;\nStrudel et al., 2021; Li et al., 2022), that achieve better quality than CNN-based models.\nThis trend is also visible in the current pursuit of extending LLMs to both understand and generate\nmultiple modalities such as text and images with a single model. A powerful paradigm in the lit-\nerature (Aghajanyan et al., 2022; Kim et al., 2023; Aghajanyan et al., 2023; You et al., 2023) is to\nmodel the image tokens using discrete tokens obtained via (VQ)VAEs (van den Oord et al., 2017;\nEsser et al., 2020; Ramesh et al., 2021). One limitation of these approaches is that the conversion\nfrom image into tokens and vice-versa is performed by a separate, frozen, modality-specific and\nlossy encoder (and decoder) trained ahead of time. As a result, this image encoder may be agnostic\nto the actual task at hand and limit the performance of the resulting model (Dong et al., 2023; Pan\net al., 2024; Xu et al., 2024).\nTo obtain a general architecture that can generate multiple modalities but does not have (limiting)\ncomponents pretrained ahead of time, we develop a new generative model: the JetFormer. It can\nbe trained from scratch and optimized end-to-end for the log-likelihood of raw training data. We\ndemonstrate this for text and pixels. To this end, we combine a normalizing flow (Dinh et al., 2016;\nKingma & Dhariwal, 2018) for computing a soft-token image representation with a decoder-only\ntransformer (Vaswani et al., 2017) and a soft-token Gaussian mixture loss (Tschannen et al., 2024).\n∗Equal contribution.\n1\narXiv:2411.19722v1  [cs.LG]  29 Nov 2024\n<BOS>\nJet\nFormer\n<BOI>\nTransformer\nTargets:\nFlow\nInvertible (i.e. lossless)\nand differentiable\nLoss:\nInputs:\nFigure 1: Visualization of JetFormer training with teacher-forcing. The ground truth consists of text\ntokens and images. The images are converted into soft tokens via a normalizing flow. The sequence\nof tokens is processed by an auto-regressive transformer and its outputs parameterize either a discrete\ndistribution or a Gaussian mixture depending whether the target is a discrete or a soft token.\nThe key insight behind the JetFormer model, is that a powerful normalizing flow (which we call\na “jet”, hence the model name) can be used to encode images into a latent representation suitable\nfor autoregressive modeling. Intuitively, raw image patches encoded as pixels have very complex\nstructure, which makes direct autoregression futile: to date, there is no convincing demonstration of\nthis. At the same time, the flow model is lossless and can be trained together with the (multimodal)\nautoregressive model end-to-end. At inference time an image decoder is readily available, since our\nflow model is invertible in closed form.\nAlthough we only optimize for log-likelihood, it is worth noting that doing so naively does not guar-\nantee being able to generate images with global coherence. Similarly to the vast majority of work on\nhigh-fidelity image generation (Esser et al., 2020; Dhariwal & Nichol, 2021; Ho & Salimans, 2022),\nwe guide the model to focus on the high-level information. To this end, we explore two approaches.\nFirst, we introduce a novel technique that is based on image augmentation during training. The main\nidea is to add Gaussian noise during training, but reduce it through the course of training. Intuitively,\nthis pushes the model towards prioritizing high-level information early on; see Section 3.2.1 for\nmore details. Even though a noise curriculum during training is inspired by diffusion models (Sohl-\nDickstein et al., 2015), it is very different on the technical level, and the resulting model does not\nperform progressive image denoising at inference time.\nSecond, we explore two ways of managing redundancy in natural images. JetFormer readily allows\nexcluding a subset of redundant dimensions from the autoregressive model. As an alternative, we\nexplore PCA for reducing image dimensionality.\nWe conduct experiments on ImageNet class-conditional image generation and on web-scale mul-\ntimodal generation, thereby demonstrating the JetFormer works and scales to both text-to-image\ngeneration and vision-language understanding with a single model.\nIn summary, our contributions are:\n• We present JetFormer, a generative model composed of a transformer and a normalizing\nflow that can be trained from scratch to model text and raw pixels jointly, end-to-end.\n• We show that image augmentation based on a noise curriculum can significantly boost\nimage generation quality of such likelihood-based models.\n• We demonstrate our proposed end-to-end model is competitive with less flexible techniques\nwhen trained on web-scale data, and can generate both images and text.\n2\nFigure 2: Selected class-conditional 256×256 samples from the JetFormer-L trained on Imagenet.\nThe samples are produced with the CFG sampler, where CFG strength is equal to 4.0.\n2\nRELATED WORK\nGenerating natural images autoregressively as a sequence of discrete-valued (sub-)pixels was ex-\ntensively explored in the literature using CNNs (Van den Oord et al., 2016b;a; Salimans et al.,\n2016) or transformers (Parmar et al., 2018; Chen et al., 2020). While achieving excellent results\nin log-likelihood, these models are computationally expensive and do not scale well to high image\nresolutions. A related family of models are normalizing flows (Dinh et al., 2014; 2016; Kingma &\nDhariwal, 2018; Ho et al., 2019), invertible models which are trained to map image pixels to a sim-\nple prior by maximizing log-likelihood. These models scale better but achieve lower likelihood than\nautoregressive models and empirically fail to generate high-fidelity images, even for low resolutions.\nMore recently, compressing the high-dimensional image pixel space to a lower-dimensional se-\nquence of discrete tokens via a pretrained, frozen VQ-VAE (van den Oord et al., 2017; Razavi et al.,\n2019), and then modeling the compressed sequence with a transformer decoder has emerged as a\nscalable technique for high-fidelity image generation (Esser et al., 2020; Yu et al., 2022a; Ramesh\net al., 2021; Yu et al., 2022c; Gafni et al., 2022). To enable semantic compression, VQ-VAEs typi-\ncally rely on perceptual and GAN losses. Moreover, VQ-VAE-based representations are common in\nthe context of dense prediction tasks (Kolesnikov et al., 2022; Lu et al., 2022; Mizrahi et al., 2024;\nMentzer et al., 2024), in particular when modeling multiple modalities jointly. GIVT (Tschannen\net al., 2024) showed that combining an autoencoder and an autoregressive transformer can be ap-\nplied to continuous-valued sequences, by directly modeling feature vectors in the latent space of\na VAE, without any quantization. Somewhat related, (Nachmani et al., 2023; Meng et al., 2024)\nexplored soft tokens for speech synthesis.\nVQ-VAEs are also becoming popular in the context of Vision-Language Models (VLMs). Such\nmodels are typically either trained from scratch (Radford et al., 2021; Jia et al., 2021; Wang et al.,\n2022b; Yu et al., 2022b) on web-scale data or constructed by combining and tuning a pretrained\nvision encoder and a pretrained language model (Alayrac et al., 2022; Chen et al., 2023b; Wang\net al., 2022a; Li et al., 2023; Huang et al., 2023; Liu et al., 2024; Beyer et al., 2024), and can solve a\nbroad range of task which can be cast as text output. To enable pixel outputs for such models a simple\nway is to extend the text vocabulary with VQ-VAE tokens (Aghajanyan et al., 2022; Kim et al., 2023;\nAghajanyan et al., 2023; You et al., 2023; Pan et al., 2024). Other works (Dong et al., 2023; Ge et al.,\n2024; Zhou et al., 2024) combine VLMs with (latent) diffusion models (Sohl-Dickstein et al., 2015;\nDhariwal & Nichol, 2021; Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022) to enable\nimage generation capabilities. JetFormer is related to this category of models, but unlike previous\nmodels does not rely on any pretrained (VQ-)VAE vision encoder\/decoder.\n3\nMETHOD\nModeling natural images with autoregressive transformers poses many obstacles. Doing so in pixel\nspace is a viable approach (Parmar et al., 2018; Chen et al., 2020), but it quickly becomes com-\nputationally prohibitive. Even an image of size 256×256×3 would require predicting\/sampling\n3\nnearly 200 000 tokens. Alternatively, modeling images at the patch-level to tame the computational\ncomplexity creates its own challenges. Each patch is a sample from a complex distribution, and\nproducing all of its dimensions in a single forward pass fails to model pixel interactions.\nCurrently, the most common approach to overcome these issues is to leverage a standalone image\nencoder\/tokenizer (and decoder\/detokenizer) model, which encodes an image as a sequence of (usu-\nally discrete) tokens. Such an image encoder performs semantic compression and thus reduces the\ncomputational burden. However, this type of approach has significant shortcomings: one needs to\ntolerate precision loss due to compression and to commit to the image encoder that was trained\nahead of time and may not be suitable for the modeling task at hand.\nIn this paper we overcome both of these issues and propose JetFormer, a generative autoregressive\ndecoder-only model able to model both text and images, and directly learn from the raw training\ndata. The JetFormer model is trained end-to-end, without relying on any lossy modality-specific\nencoders\/tokenizers. Our model is built on two key insights.\nFirst, we use continuous (also called “soft”) tokens to represent images. As shown in GIVT (Tschan-\nnen et al., 2024), transformer decoders can generate the soft-token-sequence produced by a VAE\nencoder for high-fidelity image generation. Specifically, GIVT replaces the categorical prediction\nhead with a Gaussian Mixture Model (GMM) that models log-likelihood of soft image embeddings.\nSecond, instead of a VAE (that needs to be trained ahead of time), we use a normalizing flow model\nto learn a soft-token image representation suitable for autoregressive modeling. As flow models are\nlossless by design, they won’t suffer from representation collapse and can be trained simultaneously\nwith the transformer model without auxiliary losses, eliminating the necessity to use pretrained\nencoders and enabling full end-to-end learning of autoregressive transformers from raw images.\nNaturally, the above two insights can be combined with the standard transformer for text, forming a\nsimple and unified multimodal model, capable of learning from image and text data.\n3.1\nMODELING IMAGES IN PIXEL SPACE WITH SOFT TOKENS AND NORMALIZING FLOWS\nAs outlined above, we model an image x using a normalizing flow model (Dinh et al., 2014; 2016;\nKingma & Dhariwal, 2018) f(x) that losslessly maps an image into a sequence of embeddings\n{z1, . . . , zn}, which we also call “soft tokens”. Note that the flow preserves the total number of\nthe input dimensions. These embeddings are then modeled by the deep autoregressive model p,\nwhere the outputs are modeled with a GMM, as proposed in GIVT. We then maximize the image\nlog-likelihood lower bound L:\nL(x) = log p(f(x)) + log\n\f\f\f\fdet\n\u0012∂f(x)\n∂xT\n\u0013\f\f\f\f ,\nwhere\n(1)\nf(x) = [z1, z2, . . . , zn]\nand\np(z) =\nn\nY\ni=1\np(zi|zi−1, . . . , z1).\n(2)\nNote the log-determinant term arises from the normalizing flow model, as a part of the data log-\nlikelihood, see Dinh et al. (2016). Further, to ensure correctness, we apply image dequantization, as\noutlined in Theis et al. (2015). This amounts to adding uniform noise u to the input images I, s.t.\nx = I+u, where u ∼U[0, 1]. This guarantees that we optimize a lower bound on the discrete image\nlog-likelihood. For clarity, we point out that both p and f both have learnable parameters which are\noptimized with gradient-based method (training via teacher forcing is illustrated in Figure 1).\nIn simple words, JetFormer for images is an autoregressive model, where inputs and targets are\nproduced by the flow model, which re-encodes input images. Due to the end-to-end nature of the\nobjective, the flow model is incentivized to learn a sequence of embeddings, that makes autore-\ngressive modeling as effective as possible. During inference, the autoregressive model generates a\nsequence of soft tokens, which then need to be decoded into an image using the inverse of the flow.\n3.2\nIMPROVING MODEL QUALITY FOR NATURAL IMAGES\nWhile JetFormer works out of the box, we have found several modeling enhancements which greatly\nimprove the quality of the generated images. In particular, factoring out latent dimensions, the use\nof classifier-free-guidance during sampling, and a novel noise curriculum.\n4\nFactoring out redundant dimensions\nNatural images are redundant,\nintrinsically low-\ndimensional signals with low-frequency components dominating the spectrum. The design of Jet-\nFormer enables a simple and effective way to leverage this observation and improve model quality,\nwhile also reducing computational burden.\nThe key observations is that not all output dimensions of the invertible flow need to be further pro-\ncessed by the autoregressive model. We can model a subset of dimensions (i.e. a subset of channels)\nwith a Gaussian distribution, pN , and the remaining ones with the autoregressive transformer:\nL(x) = log p(ˆz) + log pN (˜z) + log\n\f\f\f\fdet\n\u0012∂f(x)\n∂xT\n\u0013\f\f\f\f , where [ˆz, ˜z] = f(x)\nIntuitively, we expect redundant dimensions to be “factored out” as ˜z, as they do not require further\nheavy processing. We verify our intuition empirically in the experimental section and in Figure 6c.\nAs a strong baseline to the above approach, we also consider a more direct approach to handle\nredundancy in images. Before feeding x to the flow model, we reshape it into a sequence of flattened\npatches and apply a learnable, invertible linear map W along the channel dimension. We want this\nmap to learn to separate important dimensions of the flattened patches from redundant ones. To this\nend, we feed the first d channels of its output xW ⊤to the normalizing flow and model the remaining\nchannels with a Gaussian distribution. Intuitively, given the simplicity of the transform W applied\nbefore factoring out part of the sequence, minimizing the NLL while training will ensure that the\nhard-to-model part of the sequence is modelled by JetFormer, whereas the low-level noise will be\nmapped to the Gaussian prior. This is similar to the reasoning behind probabilistic PCA (Tipping &\nBishop, 1999). Indeed, we observe that the transform learned by the model is close to applying PCA\nto image patches (see Figure 6d), and we observe that when initializing W with PCA and freezing\nit we obtain similar results. See Appendix B for formal definition of this approach.\nClassifier-free guidance\nFollowing common practice in the diffusion and autoregressive image\nmodeling literature, we employ classifier-free guidance (CFG) (Ho & Salimans, 2022) which was\npreviously shown to substantially improve sample quality. We reuse the distribution-based variant\nimplemented via rejection sampling for GMMs from (Tschannen et al., 2024) without modification.\n3.2.1\nRGB NOISE CURRICULUM DURING TRAINING\nIt is common to explicitly factorize data into semantically meaningful parts to improve image quality.\nOne approach is to model RGB pixels as sequences of increasing color depth and\/or resolution\n(Kolesnikov & Lampert, 2017; Menick & Kalchbrenner, 2019; Nash et al., 2021). Similarly, adding\nnoise to RGB pixels is closely related to reducing the color depth and effective resolution. This has\nled to the interpretation of diffusion models, where denoisers are trained at different noise levels\naccording to a predefined noise schedule, as learning a hierarchical representation in pixel space\ninduced by the noise schedule (Kingma & Gao, 2023; Dieleman, 2024).\nBuilding on this intuition, we alter the training procedure by introducing a “noise curriculum”:\nadditive Gaussian pixel noise during JetFormer training. The noise is strongest in the beginning of\nthe training and decays towards zero. We use cosine decay schedule for the noise standard deviation.\nIn the beginning of the training, when strong (high-variance) noise is added to the image, JetFormer\nlearns to model coarse image information (see Figure 6b). As training progresses, the model grad-\nually learns a finer level of detail, while “remembering” previously learned patterns. In the end of\nthe training, JetFormer uses the correct distribution for training. Intuitively, this scheme prioritizes\nmodeling of high-level image structure without sacrificing overall performance.\nImportantly, unlike in diffusion, the noise curriculum merely acts as a data augmentation during\ntraining. The model is not conditioned on the noise magnitude and, at inference, an image is not\ngradually denoised, but generated autoregressively in the latent space of a normalizing flow.\nFor an integer-valued RGB image I, the noisy image is obtained as ⌊I + σtN(0, I)⌋, where the\nnoise scale σt as a function of the training progresses t ∈[0, 1] follows the cosine schedule\nσt = σ0\n1+cos(tπ)\n2\n. The shape of the noise schedule is visualized in Figure 3.\n5\n0%\n20%\n40%\n60%\n80%\n100%\nTraining progress\n3\n4\n5\n6\n7\n8\n9\nNegative log-likelihood (solid line)\nbaseline\nwith noise curriculum\n20%\n40%\n60%\n80%\n100%\nTraining progress\nbaseline samples\nnoise curriculum samples\n0\n10\n20\n30\n40\n50\n60\nNoise standard deviation (dashed line)\n0\n25\n50\n75\n100\n125\n150\n175\n200\nFID\nbaseline\nwith noise curriculum\nFigure 3: Training with noise curriculum on RGB input images. The left plot demonstrates negative\nlog-likelihood progression (solid lines) and the noise strength schedule (dashed lines). Importantly,\nboth NLL curves land in a very similar points, despite visual quality measured by FID (middle plot)\nand the typical samples (right plot) being very different: we observe that noise curriculum guides\nthe JetFormer towards modeling high-level image structure.\n3.3\nJOINT GENERATIVE MODELING OF PIXELS AND TEXT\nWe explore JetFormer for multimodal generative modeling, where the model can perform both dis-\ncriminative and generative tasks on all modalities, focusing on images and text. Sophisticated mod-\nels of this class are trained on interleaved sequences of images and text (Aghajanyan et al., 2022;\nKim et al., 2023; Aghajanyan et al., 2023), often with post-training refinement, which enables few-\nshot image-to-text (e.g. captioning) and text-to-image (e.g. image editing) capabilities. Here, we\nfollow (Kim et al., 2023; You et al., 2023) and consider image-text pairs from the web as a proxy for\nmore complex, interleaved setups, and do not involve a post-training step. While conceptually sim-\nple, this allows us to explore vision-language understanding tasks such as captioning and VQA, as\nwell as text-to-image generation. Extending the image generation approach discussed in the previ-\nous section to this setup is straight-forward: We simply extend the transformer backbone generating\nsoft tokens to modeling language tokens produced by a standard language tokenizer with a separate\nprediction head and a softmax.\nWe train on sequences of both image tokens followed by text tokens, and vice versa, only applying a\nloss to the second part (modality) of the sequence. We use the respective negative log-likelihood, and\na weight to balance the two. We observed that applying the loss to the full sequence leads to worse\nresults, possibly because predicting an image prefix effectively means unconditionally modeling\nweb images, which is very challenging. We expect this to change for interleaved sequences, which\nmay provide a stronger conditioning signal.\nFor text-to-image generation, the text prefix acts as a conditioning, and image generation is per-\nformed as described in Section 3.1. For image-to-text generation the normalizing flow acts as an\nimage encoder. The model uses the same soft token space for generation and understanding.\n4\nEXPERIMENTS\nArchitecture\nWe rely on the simple, transformer-based design from (Anonymous, 2024) for the\nnormalizing flow model. This design uses affine coupling layers (predicting an element-wise scale\nand shift) consisting of spatial and channel-wise splitting functions to split the activations in two\nparts, and a stack ViT blocks (Dosovitskiy et al., 2021) applied to half of the activations to infer the\naffine transform. Here, we only use channel-wise splitting as we found in preliminary experiments\nthat spatial splitting did not improve modeling quality. We set the depth to 32 coupling blocks, each\nconsisting of a stack of 4 or 6 ViT blocks of width 512, and with 8 attention heads. The (input and\noutput) feature shape for this model is ( H\np , W\np , 3p2) when feeding the full image, and ( H\np , W\np , d)\nwhen using a dense invertible map or PCA to reduce dimensionality prior to applying flow. For\nimage size H = W = 256 and patch size p = 16 this amounts to 256×768, after flattening spatial\ndimensions and with dimensionality reduction to d = 128 to 256×128.\n6\nTable 1: Comparison of JetFormer trained for 500 epochs on ImageNet256 with baselines from the\nliterature. For large enough scale JetFormer approaches models without components pretrained in\nan extra step.\nextra step\nFID\nPrecision\nRecall\nNLL\nBigGAN-deep (Brock et al., 2018)\n–\n6.95\n0.87\n0.28\nADM-G (Dhariwal & Nichol, 2021)\n–\n4.59\n0.82\n0.52\nLDM-4-G (Rombach et al., 2022)\nVAE\n3.60\n0.87\n0.48\nVQGAN (Esser et al., 2020)\nVQ-VAE\n5.20\nViT-VQGAN (Yu et al., 2022a)\nVQ-VAE\n3.04\nGIVT-Causal (Tschannen et al., 2024)\nVAE\n3.35\n0.84\n0.53\nJetFormer-B\n–\n7.25\n0.72\n0.44\n3.06\nJetFormer-L\n–\n6.64\n0.69\n0.56\n3.05\nFor the latent autoregressive decoder-only backbone, we rely on the Gemma architecture (Gemma\nTeam et al., 2024). We consider 3 different model shapes amounting to 350M, 750M and 1.3B\nparameters which are largely inspired by previous decoder-only models for image generation (Esser\net al., 2020) (see Appendix A). For the GMM prediction head, unless explicitly noted, we set the\nnumber of mixtures k = 1024, use multivariate Gaussians of dimension d = 128 with diagonal\ncovariance. For text we use a sentencepiece tokenizer with vocabulary size 32k trained on the\nEnglish portion of the C4 corpus made available by Raffel et al. (2020). We set the maximum\nnumber of text tokens to 64, but do not explicitly model padding tokens (i.e. during training we\nmask out attention elements corresponding to padding tokens and adapt the RoPE positions to skip\nthem). When training for class-conditional image generation, we use a prefix of 16 learned tokens\nper class (instead of a single one), as we observed that this improves sample quality.\nTraining recipe\nWe train the latent decoder-only backbone on concatenated sequences of discrete\ntext and soft tokens (flow latents) via teacher forcing for NLL according to the respective distribution\nof the tokens (categorical for text tokens and GMM for soft tokens). Whether the sequence is an\nimage-text or text-image sequence is sampled at random for every example. Normalizing flow (or\nlearnable invertible patch embedding) is trained along with the transformer-backbone end-to-end.\nThis does not require any dedicated techniques thanks to the NLL for the GMM being differentiable\nboth w.r.t. to the GMM parameters as well as the soft tokens\/features it is evaluated w.r.t. When\ntraining for captioning with an image prefix (i.e. for image-text sequences, where normalizing flow\nserves as a vision encoder), we stop the gradient at the flow output during pretraining. We did not\nobserve improved performance when passing the gradients.\nWe use the Adam optimizer with learning rate 10−3, decoupled weight decay of 10−4, β2 parameter\n0.95 and clip the gradient norms to 1.0. We set the batch size to 4k. We also apply dropout with\nprobability 0.1 at the output of the self-attention and MLP blocks, which we found to improve\nimage sample quality. For both class conditional image generation and text-to-image generation we\ndrop the conditioning with 10% probablilty and replace it with a learned [NOLABEL] token for\nCFG. Unless explicitly stated, we apply the RGB noise schedule to the input images described in\nSection 3.1 with initial noise standard deviation σ0 = 64 (for pixel values in the range [0, 255]). We\ndecay to 0 for ImageNet setup and to 3 for multimodal setup. Inspired by the VAE-based GIVT,\nwhere for every example a latent is sampled from the VAE encoder (i.e. approximate posterior)\nwhich might have a regularizing effect, we add Gaussian noise with standard deviation 0.3 to the flow\nlatents. We normalize the image NLL to bits per dimension as is common in the image generative\nmodeling literature and apply a weight of 0.0025 to the text NLL such that the loss magnitude per\ntoken is roughly identical for both modalities.\nTraining data\nFor training class-conditional image generation models we use ImageNet1k (Rus-\nsakovsky et al., 2015). For multimodal generation we rely on the image-text pairs from WebLI data\nset (Chen et al., 2023b). In both cases, we resize the images so that the shorter side is 256 pix-\nels while preserving the aspect ratio and extract a 256×256 central crop. Besides the RGB noise\ndescribed earlier we do not apply any augmentation, except for random left-right flipping on Im-\n7\nTable 2: Summary of the main JetFormer (trained for 100 epochs) ablations performed on class-\nconditional ImageNet256 generation. We demonstrate relative importance of various components\nand highlight the interesting interplay between PCA preprocessing and the noise curriculum.\nFID\nPrecision\nRecall\nNLL\nJetFormer-B\n7.84\n0.75\n0.39\n3.14\nno normalizing flow\n117.76\n0.17\n0.32\n6.84\nno noise curriculum\n44.71\n0.45\n0.28\n3.05\nno factored-out dimensions\n17.29\n0.65\n0.26\n3.13\nno end-to-end training\n11.16\n0.68\n0.33\n3.08\nlearned inv. projection\n10.19\n0.73\n0.32\n4.78\nno GMM (Gaussian loss only)\n9.46\n0.77\n0.30\n3.14\nsingle class token\n8.85\n0.73\n0.37\n3.14\nPCA preproc. + JetFormer-B\n8.79\n0.77\n0.35\n–\nPCA preproc. + JetFormer-B (no noise cur.)\n13.16\n0.71\n0.31\n–\nageNet1k. On ImageNet1k, we train for 100 epochs in ablations, and 500 epochs otherwise. On\nWebLI, we train for 1B examples seen per modality, so 1B examples for text-to-image only models,\nand 2B total for models that also support image-to-text (understanding) tasks.\nEvaluations and metrics\nFollowing the diffusion literature, we use the ADM FID evaluation\nsuite (Dhariwal & Nichol, 2021) (with 50k reference samples) and precision\/recall (Sajjadi et al.,\n2018) to assess image sample quality on ImageNet1k. For text-to-image, we adopt the common\nMS-COCO FID-30k, generating images for captions from 30k randomly sampled COCO validation\nimages and evaluating FID against reference statistics from the full COCO validation set. We report\nthis metric both zero-shot and after fine-tuning on the COCO training set. As image understanding\ntasks we consider ImageNet zero-shot classification and fine-tune for image captioning (reporting\nCIDEr score) and visual question answering (VQA, measuring VQAv2 accuracy).\n4.1\nCLASS-CONDITIONAL IMAGE GENERATION\nTable 1 shows the sample quality of JetFormer trained on ImageNet with image resolution of\n256×256 along with some baselines from the literature. Model samples are show in Figure 2 and\nAppendix F. Despite being an explicit NLL model and not using advanced image encoders, our Jet-\nFormer model is competitive with those baselines. Interestingly, JetFormer has high recall of 0.56.\nWe hypothesize that this is a consequence of our model being an explicit log-likelihood model and\nthus not suffering from the mode collapse.\nTable 2 shows ablations of key design choices of JetFormer:\n– Removing the normalizing flow results in a catastrophic loss in performance. This confirms\nour intuition that re-encoding image pixels with a flow model is essential.\n– Omitting the noise curriculum also results in much worse results.\n– We also confirm that not factoring out dimensions after the flow model leads to quality loss.\n– First training the normalizing flow to minimize NLL w.r.t. a Gaussian prior, and then\ntraining the autoregressive transformer on the latent representation of the frozen flow model\nleads to lower sample quality than end-to-end training.\n– Factoring out dimensions with learnable linear mapping before the flow leads to better\nresults, but underperforms post-flow factoring out.\n– Reducing the number of GMM components from 1024 to 1 for the soft-token loss result in a\nrelatively mild performance drop and a significant drop in recall, indicating more mixtures\nenable a better coverage of the distribution.\n– Finally, doing class-conditioning with a single class token (instead of 16 tokens) leads to\nslight performance drop, likely due to the weaker conditioning signal.\nInterestingly, we observe that modeling images after the PCA transform leads to somewhat worse\nresults. However, in the presence of PCA, the noise curriculum becomes less important. It highlights\n8\nTable 3: Comparison with baselines from the literature for text-to-image generation (0-shot COCO\nFID-30k, 256×256). We included autoregressive (first group) and diffusion models (second group)\nwhich use raw image-text data (without e.g. re-captioning) and do not rely on pretrained text en-\ncoders. Models with image understanding capabilities are marked with T&I. FID (ft.) is obtained\nwhen fine-tuning on the COCO training set, and NLL is measured on the COCO validation set after\nfine-tuning. JetFormer is the only method which does not rely on any extra step.\nextra step\n#param.\nFID\nFID (ft.)\nNLL (ft.)\nDALL-E (Ramesh et al., 2021)\nVQ-VAE\n12B\n27.50\nCogView (Ding et al., 2021)\nVQ-VAE\n4B\n27.10\nCogView2 (Ding et al., 2022)\nVQ-VAE\n6B\n24.00\n17.50\nARGVLT (T&I) (Kim et al., 2023)\nVQ-VAE\n0.45B\n16.93\nMAGVLT (T&I) (Kim et al., 2023)\nVQ-VAE\n0.45B\n12.08\nMake-A-Scene (Gafni et al., 2022)\nVQ-VAE\n4B\n11.84\n7.55\nLDM-KL-8-G (Rombach et al., 2022)\nVAE\n1.45B\n12.63\nGLIDE (Nichol et al., 2022)\nSuper-res.\n6B\n12.24\nDALL-E-2 (Ramesh et al., 2022)\nSuper-res.\n5.2B\n10.39\nJetFormer-L (T&I)\n–\n2.75B\n20.86\n13.70\n3.86\nJetFormer-L\n–\n2.75B\n18.63\n13.07\n3.85\nthe importance of prioritizing the important high-level information in images, if visual quality is the\nend goal. It also shows that manual preprocessing may reduce the necessity for various modeling\ntricks, yet it also shows that full end-to-end modeling prevails when done right.\n4.2\nEFFECT OF THE NOISE CURRICULUM\nWe have experimented with different levels of initial noise, and generally find that for 256×256\nimages the initial noise with standard deviation of 64 annealed to 0 during the training to be the\nbest for the ImageNet setup. The initial noise levels of 128, 64, 32 and 0 result in FID 8.62, 7.84,\n8.59 and 44.71 respectively. For the multimodal setup we observe that annealing the noise standard\ndeviation to 3 (thus leaving a small level of noise) improves FID. More analysis of the interplay\nbetween NLL and FID is presented in Appendix C.\nIn Figure 3 we demonstrate the effect of noise curriculum with the initial standard deviation of\n64. First, we observe that, in comparison to the noise-free baseline, the final NLL is not affected\ndramatically. However, as would be expected, initial stages of training have much worse NLLs due\nto strong noise. Second, we demonstrate that FID drastically improves with the noise curriculum\nbeing enabled. This is also evident by the final samples from the noise-free model and model with\nnoise. The latter has much more pronounced emphasis on the high-level image structure.\n4.3\nMULTIMODAL GENERATION AND UNDERSTANDING\nFigure 4 shows multimodal JetFormer models of different sizes, trained for text-to-image genera-\ntion only (T2I) and both T2I and image-to-text generation (T&I). Zero-shot samples are show in\nAppendix E. In both cases, increasing the model size improves quality. Training models for mixed\nT&I generation leads to a reduction in T2I generation quality, but also equips the model with vision-\nlanguage understanding capabilities.\nTables 3 and 4 compare T2I and T&I JetFormer models with models from the literature. Sim-\nilar prior work trained in generative fashion on image-text pairs relying on pretrained VQ-VAE\n(ARGVLT, MAGVLT (Kim et al., 2023)) achieves better performance on T2I generation, but lags in\nunderstanding performance, which highlights the benefits of having access to unquantized end-to-\nend learned features for understanding. JetFormer also approaches the understanding performance\nof image-text models pretrained for understanding such as CLIP and CapPa, although these models\nhave a significantly smaller size.\nFinally, we present a T&I baseline using a pretrained, frozen VAE instead of an end-to-end trained\nnormlizing flow in Appendix C and find that JetFormer clearly outperforms this baseline in T2I\ngeneration and all I2T tasks.\n9\n1.5\n2.0\n2.5\nnumber of parameters [B]\n20\n22\n24\n26\n28\n30\nFID\nCOCO FID-30K (0-shot)\n1.5\n2.0\n2.5\nnumber of parameters [B]\n44\n46\n48\n50\n52\n54\nacc.\nImageNet class. (0-shot)\n1.5\n2.0\n2.5\nnumber of parameters [B]\n110\n112\n114\n116\n118\n120\nCIDEr\nCOCO Cap (fine-tuned)\n1.5\n2.0\n2.5\nnumber of parameters [B]\n66\n67\n68\n69\n70\nacc.\nVQAv2 (fine-tuned)\ntype\nT2I\nT&I\nI2T\nFigure 4: Zero-shot image sample quality, zero-shot ImageNet classification accuracy (by scoring\nclass labels, without prompts), and captioning and VQA performance as a function of model size\nfor JetFormer variants. Increasing the model size improves all the metrics. T&I models generally\nperform worse than unidirectional models which are trained either for T2I or I2T generation.\nTable 4: Comparison with baselines pretrained on raw image-text data and fine-tuned for image\ncaptioning (COCO captions; CIDEr) and VQA (VQAv2; test-dev acc.). Models marked with T&I\nwere jointly pretrained for captioning and T2I generation. ∗The numbers are from (Tschannen et al.,\n2023) for models pretrained on 900M image-text pairs, which most closely matches our setup.\nextra step\nCOCO cap.\nVQAv2\nCapPa L\/14 (Tschannen et al., 2023)∗\n–\n118.7\n68.6\nCLIP L\/14 (Radford et al., 2021)∗\n–\n118.2\n67.9\nARGVLT (T&I) (Kim et al., 2023)\nVQ-VAE\n94.7\n–\nMAGVLT Large (T&I) (Kim et al., 2023)\nVQ-VAE\n110.7\n65.7\nJetFormer-B (I2T)\n–\n118.7\n67.2\nJetFormer-L (T&I)\n–\n119.8\n70.0\n5\nCONCLUSION\nIn this paper we introduce JetFormer, a novel class of generative models that combines normalizing\nflows and autoregressive models with soft tokens. To the best of our knowledge, it is the first\nimage model, which is capable to synthesize high-resolutions images and provide an explicit (and\ncompetitive) NLL bounds for the raw images. JetFormer is a fully end-to-end trainable model (with\nno components pretrained ahead of time), which means that it can be fully tailored towards the task\nat hand, without being limited by the external and frozen components. Being able to compute NLL\nis also an important feature of our model. NLL is a tangible score closely related to compression\ncapabilities, and can be used to compare various generative models across different modeling classes\nor for hill-climbing. Also, by measuring NLL score one can ensure the absence of the mode collapse,\nbecause mode collapse will lead to deterioration NLL on the hold-out data.\nWe note that JetFormer in its current form also has some limitations. The visual quality of its\nsamples lags behind state-of-the-art diffusion models that leverage pretrained latent representations.\nAdditionally, the full end-to-end nature of JetFormer also comes with increased computational re-\nquirements. However, given JetFormer’s simple design, we believe that it can be scaled up well so\nthat the benefits of end-to-end training can come to full fruition.\nReproducibility Statement\nWe provide detailed information about the training recipe, the archi-\ntecture, hyper-parameters and the training data in Section 4 and Appendix A.\nEthics Statement\nThis paper describes a system for understanding and generation of image-text\ndata, with focus on characterizing and exploring its performance on academic data sets. It fits into\nthe broader class of large multimodal models trained on data from the web, and the same ethical\nimplications as for those prior works apply here. In particular, when releasing or deploying such\nmodels publicly, extensive measures to de-biasing the data should be taken, and models should be\nsafety-tuned and red-teamed prior to release. Content-filters can be added to the inference pipeline\nto further improve safety. We refer to the corresponding papers for a more in-depth discussion, for\nexample (Radford et al., 2021; Chen et al., 2023a; Po et al., 2024).\n10\nACKNOWLEDGEMENTS\nWe thank Lucas Beyer for a detailed review and feedback on the final manuscript. We further thank\nJoan Puigcerver for timely infrastructure contributions.\nREFERENCES\nArmen Aghajanyan, Po-Yao Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A\ncausal masked multimodal model of the internet. arXiv:2201.07520, 2022.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan\nZhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for gen-\nerative mixed-modal language models. In ICML, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: A visual language\nmodel for few-shot learning. In NeurIPS, 2022.\nAnonymous. Jet: A modern transformer-based normalizing flow. In preparation, 2024.\nLucas Beyer, Andreas Steiner, Andr´e Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz,\nMaxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al.\nPaliGemma: A versatile 3B VLM for transfer. arXiv:2407.07726, 2024.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity\nnatural image synthesis. In ICLR, 2018.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa De-\nhghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar\nJoshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias\nMinderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien\nAmelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong\nXu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai,\nNeil Houlsby, and Radu Soricut. PaLI-X: On scaling up a multilingual vision and language model.\narXiv:2305.18565, 2023a.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-scaled multilingual\nlanguage-image model. ICLR, 2023b.\nPrafulla Dhariwal and Alexander Nichol.\nDiffusion models beat GANs on image synthesis.\nNeurIPS, 2021.\nSander Dieleman. Diffusion is spectral autoregression. Blog post, 2024. URL https:\/\/sander.\nai\/2024\/09\/02\/spectral-autoregression.html.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,\nZhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers.\nNeurIPS, 2021.\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image\ngeneration via hierarchical transformers. NeurIPS, 2022.\nLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components\nestimation. arXiv:1410.8516, 2014.\n11\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.\nDensity estimation using real NVP.\narXiv:1605.08803, 2016.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.\nDreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021.\nPatrick Esser, Robin Rombach, and Bj¨orn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, 2020.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-\na-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying\nShan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation.\narXiv:2404.14396, 2024.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open\nmodels based on gemini research and technology. arXiv:2403.08295, 2024.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022.\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-\nbased generative models with variational dequantization and architecture design. In ICML, 2019.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav\nChaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning\nperception with language models. arXiv:2302.14045, 2023.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In ICML, 2021.\nSungwoong Kim, Daejin Jo, Donghoon Lee, and Jongmin Kim. MAGVLT: Masked generative\nvision-and-language transformer. In CVPR, 2023.\nDiederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data\naugmentation. NeurIPS, 2023.\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.\nNeurIPS, 2018.\nAlexander Kolesnikov and Christoph H Lampert. PixelCNN models with auxiliary variables for\nnatural image modeling. In ICML, 2017.\nAlexander Kolesnikov, Andr´e Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and\nNeil Houlsby.\nUViM: A unified modeling approach for vision with learned guiding codes.\nNeurIPS, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023.\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer back-\nbones for object detection. In ECCV, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.\n12\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS,\n2024.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-IO: A unified model for vision, language, and multi-modal tasks. In ICLR, 2022.\nLingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu\nLi, Sheng Zhao, Xixin Wu, et al. Autoregressive speech synthesis without vector quantization.\narXiv:2407.08551, 2024.\nJacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks\nand multidimensional upscaling. ICLR, 2019.\nFabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantiza-\ntion: VQ-VAE made simple. ICLR, 2024.\nDavid Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and\nAmir Zamir. 4M: Massively multimodal masked modeling. NeurIPS, 2024.\nEliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai,\nSoroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Spo-\nken question answering and speech continuation using spectrogram-powered llm. arXiv preprint\narXiv:2305.15255, 2023.\nCharlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse\nrepresentations. In ICML, 2021.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and\nediting with text-guided diffusion models. In ICML, 2022.\nKaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng\nChua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm.\narXiv:2405.01926, 2024.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018.\nRyan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit Bermano, Eric\nChan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion\nmodels for visual computing. In Computer Graphics Forum, 2024.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv:2204.06125, 2022.\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\nVQ-VAE-2. NeurIPS, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\n13\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. IJCV, 2015.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 2022.\nMehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing\ngenerative models via precision and recall. NeurIPS, 2018.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the\nPixelCNN with discretized logistic mixture likelihood and other modifications. In ICLR, 2016.\nAmanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus\nRohrbach. Towards VQA models that can read. In CVPR, 2019.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In ICML, 2015.\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for\nsemantic segmentation. In CVPR, 2021.\nRichard Sutton. The bitter lesson. Incomplete Ideas (blog), 2019.\nLucas Theis, A¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative\nmodels. arXiv:1511.01844, 2015.\nMichael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal\nof the Royal Statistical Society Series B: Statistical Methodology, 1999.\nMichael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer.\nImage captioners are scalable vision learners too. In NeurIPS, 2023.\nMichael Tschannen, Cian Eastwood, and Fabian Mentzer. GIVT: Generative infinite-vocabulary\ntransformers. In ECCV, 2024.\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Condi-\ntional image generation with pixelcnn decoders. NeurIPS, 2016a.\nA¨aron Van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\nIn ICML, 2016b.\nA¨aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-\ning. NeurIPS, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. GIT: A generative image-to-text transformer for vision and language. TMLR,\n2022a.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM:\nSimple visual language model pretraining with weak supervision. ICLR, 2022b.\nYifan Xu, Xiaoshan Yang, Yaguang Song, and Changsheng Xu. Libra: Building decoupled vision\nsystem on large language models. arXiv:2405.10140, 2024.\nHaoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. CoBIT:\nA contrastive bi-directional image-text generation model. arXiv:2303.13455, 2023.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQ-\nGAN. ICLR, 2022a.\n14\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\nCoca: Contrastive captioners are image-text foundation models. TMLR, 2022b.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasude-\nvan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana\nParekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models\nfor content-rich text-to-image generation. TMLR, 2022c.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob\nKahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and\ndiffuse images with one multi-modal model. arXiv:2408.11039, 2024.\n15\nAPPENDIX – SUPPLEMENTARY MATERIAL\nA\nARCHITECTURES AND HYPERPARAMETERS\nTable 5 shows the shapes of the autoregressive transformer and normalizing flow components for\ndifferent JetFormer variants. One can see that the dense layer predicting the GMM parameters are\nrelatively large. To save memory, we store this layer in the bfloat16 format (instead of float32\nas the other parameters) which does not impact the quality. Also note that at inference time applying\nthis layer can be greatly sped up by first sampling the mixture and then only inferring the mean and\ncovariance of that mixture.\nTo implement the normalizing flow we use a stack affine coupling blocks (Dinh et al., 2016,\nEqn. 7, 8). For each block i the relation between input sequence yi and output yi+1 is\n¯yi+1 = ¯yi\n⇔\n¯yi = ¯yi+1,\n˜yi+1 = ˜yi ⊙exp(ai(¯yi)) + bi(¯yi)\n⇔\n˜yi = (˜yi+1 −bi(¯yi+1)) ⊙exp(−ai(¯yi+1)).\nHere, ¯yi and ˜yi are obtained by splitting yi into equally-shaped halves along the channel dimension\naccording to a randomly initialized partition (and yi+1 results from merging ¯yi+1 and ˜yi+1 according\nto this partition). The scale ai and shift bi are inferred by two separate heads from a shared ViT gi.\nThe normalizing flow component has about 450M to 650M parameters, depending of the variant.\nThis is more than typical CNN-based (VQ-)VAEs for image generation in the literature (Esser et al.,\n2020; Rombach et al., 2022), but comparable to common ViT-based VQ-VAEs (Yu et al., 2022a;c).\nTable 6 shows the hyper-parameters used to fine-tune JetFormer. Overall we observed dropout to\nhave a significant impact on the image sample quality. When reporting fine-tuning-based metrics\nwe report the median of 10 runs, except in VQAv2 where we report test-dev accuracy from the\nevaluation server.\nTable 5: Architecture details for different JetFormer variants. See Section A for a discussion.\nJetFormer-B\nJetFormer-M\nJetFormer-L\nAutoregressive transformer\nDepth\n24\n24\n48\nWidth\n1024\n1536\n1536\nMLP hidden dim.\n4096\n6144\n6144\nNum. heads\n16\n16\n16\nNum. KV heads\n1\n1\n1\nHead dim.\n64\n96\n96\nVocab. size\n32k\n32k\n32k\nNum. mixtures\n1024\n1024\n1024\nNum. GMM param.\n269M\n404M\n404M\nNum. vocab. param.\n33M\n49M\n49M\nNum. backbone param.\n389M\n847M\n1.65B\nNormalizing flow\nNum. coupling blocks\n32\n32\n32\nBlock width\n512\n512\n512\nBlock depth\n4\n4\n6\nBlock MLP hidden dim.\n2048\n2048\n2048\nBlock num. heads.\n8\n8\n8\nNum. param.\n447M\n447M\n650M\nTotal num. param.\n1.38B\n1.75B\n2.75B\nB\nFACTORING OUT REDUNDANT DIMENSIONS: PCA-INSPIRED VARIANT\nRecall that for this variant, before feeding x to the flow model, we reshape it into a sequence of\nflattened patches and apply a learnable, invertible linear map W along the channel dimension.\n16\nTable 6: Hyper-parameter details for fine-tuning tasks.\nTask\nModel Size\nEpochs\nBatch\nLearning\nWeight\nDropout\nSampler\nsize\nrate\ndecay\nCOCO (FID-30k)\nB\/M\/L\n10\n256\n1e-4\n1e-5\n0.1\nCFG\nVQAv2\nB\/M\/L\n10\n128\n3e-5\n3e-6\n0.0\nGreedy\nCOCO caption\nB\/M\n10\n128\n3e-5\n3e-6\n0.0\nGreedy\nCOCO caption\nL\n5\n256\n3e-5\n3e-6\n0.0\nGreedy\nFor this variant, the loss can be written as:\nL(x) = log p(f(ˆx)) + log pN (˜x) + log\n\f\f\f\fdet\n\u0012∂f(ˆx)\n∂ˆxT\n\u0013\f\f\f\f + (HW\/p2) log | det W|,\nwhere pN is the Gaussian distribution. xW T is decomposed into two slices as [ˆx, ˜x] = xW T of\nshapes HW\/p2 × d and HW\/p2 × (3p2 −d), respectively, where the original image has shape\nH × W × 3 and is split it into p × p patches which are then flattened.\nIntuitively, factoring out dimensions via W rather than at the flow output limits the complexity and\nhence potentially leads to worse results (see Table 2).\nWhen compting W via PCA to obtain the bottom two rows in Table 2, we sample 4 000 images from\nthe ImageNet training set, split them into patches, and compute PCA using the built-in implementa-\ntion from scikit-learn without whitening.\nC\nADDITIONAL ABLATIONS\nVAE baseline\nWe train a VAE following the design of VQGAN (Rombach et al., 2022) producing\na sequence of 256 128-dimensional tokens like the normalizing flow in JetFormer. We remove the\nGAN and perceptual losses to ensure a fair comparison with JetFormer (which solely maximizes\nthe data likelihood, without relying on GAN or perceptual losses). We then train the JetFormer-B\ndecoder-only model on this VAE representation, following the JetFormer-B T&I training recipe. The\nresults in the Table 7 below show that JetFormer outperforms the VAE baseline by a solid margin\n(in particular in T2I generation).\nTable 7: Comparison of JetFormer-B trained for T&I with a 2-stage VAE-based variant.\nCOCO-FID30k\nINet 0-shot\nCOCO Cap\nVQAv2\nVAE + Transformer (2 stages)\n95.4\n37.6\n103.9\n64.0\nVAE + Trans. (2 stages, w\/ noise curr.)\n87.6\n40.0\n106.7\n64.9\nJetFormer (end-to-end)\n30.6\n42.7\n113.9\n65.4\nEffect of noise curriculum on NLL\nIntuitively, the noise curriculum biases the training process\ntowards those high-likelihood solutions with high perceptual quality, at the expense of a slight in-\ncrease in NLL (see Sec. 3.2.1 for more discussion). Table 8 shows that longer training with noise\ncurriculum eliminates the gap in NLL compared to training without curriculum.\nTable 8: Sample quality and NLL as a function of noise curriculum and training duration.\nnoise curr.\n#epochs\nFID-50k\nNLL\nNo\n100\n44.71\n3.05\nYes\n100\n7.84\n3.14\nYes\n500\n7.25\n3.06\nYes\n1000\n7.10\n3.04\n17\nEffect of final noise scale when fine-tuning for T2I generation\nWe further visualize the interplay\nbetween the final noise scale and NLL when fine-tuning T2I models on COCO in Fig. 5.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nNoise scale\n13\n14\n15\n16\n17\nFID (solid)\n3.3\n3.4\n3.5\n3.6\n3.7\n3.8\nNLL (dashed)\nJetFormer-B\nJetFormer-L\nFigure 5: FID and NLL results obtained when fine-tuning JetFormer for T2I generation on COCO\nwith different values for the final noise scale. JetFormer is pretrained with a curriculum decaying\nthe noise scale from 64 to 3 and the noise scale is then decayed during fine-tuning from 3 to one\nof {0, 1, 3}. We observe that decaying the noise scale to 0 leads to the best NLL but a worst FID.\nThe two model sizes have similar NLL at noise scale 3 but different FIDs, possibly because NLL is\nbounded by the noise scale but larger model scale helps to generalize better.\n18\nD\nIMAGE AND LATENT MANIPULATION\nIn Figure 6 we visualize the effect of different image or latent manipulations. Note that these images\nare not indicative of the final image generation quality and their purpose is to illustrate certain quali-\ntative properties. We use four images: a full 256×256 view of a plane from ImageNet (Russakovsky\net al., 2015), a 64×64 crop of an image of a bird from ImageNet, a 64×64 crop of a scene from\nMS-COCO (Lin et al., 2014) to analyze out-of-distribution generalization (vs. ImageNet), and a\n128×128 crop of an image with text from TextVQA (Singh et al., 2019), to visualize the effect of a\nsignificantly different domain (vs. natural images). On those images we show:\n(a) Ground truth.\n(b) Additive RGB noise N(0, I) with scale σ0 = 64. This corresponds to the images at the begin-\nning of the noise curriculum. Although this significantly affects the perception of the zoomed in\nview of the bird or details of the clouds, it has smaller impact on the more zoomed out features\nsuch as the plane at 256×256 or the large text letters.\n(c) In Section 3.2 we discussed factoring out redundant dimensions. Here we use a model trained\non ImageNet where the dimensions are factored out after the normalizing flow to map the\nimages to the latent space. After we keep the 128 autoregressive dimensions fixed but resample\nthe 640 dimensions modelled by the Gaussian prior. Despite these corresponding to 640 of 768\nlatent dimensions of each patch, they have little impact on the overall image structure. It shows\nthe model successfully maps important information to the 128 dimensions modeled with the\nautoregressive prior which are unmodified in this manipulation.\n(d) Zeroing out the gaussian latents after a learnable projection. Overall the model learned to map\nthe relevant dimensions into the 128 per patch modelled further by normalizing flow.\n(e) PCA reconstruction of patches when using 128 dimensions. PCA is from ImageNet images.\n(f) VAE encoding and decoding. We use a VAE from (Tschannen et al., 2024) which was pre-\ntrained on ImageNet with perceptual losses, overall it produces sharp images but looses signif-\nicant detail and is not able to preserve details in out-of-distribution images, e.g. with text.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 6: Visualization of image and latent manipulations. Note these images are not indicative of\nthe final image generation quality. Best viewed in digital format.\n19\nE\nZERO-SHOT MS-COCO SAMPLES\na picture of a horse in a field\nthis is the head and front view of a rooster.\na wooden table topped with four white bowls.\na bathroom that has a mirror in it.\na large body of water with a boat on it.\na large red bus on the side of a city street.\na plate of broccoli and carrots on a table.\na dog sitting near the window looking out of a brick building.\na large elephant feeding itself using its trunk\na red fire hydrant gushing water onto a street.\na skier in white clothing is attempting to go faster.\na clock tower is raised in front of a blue sky.\nFigure 7: Selected zero-shot samples from JetFormer-L (T2I) for captions from MS-COCO.\n20\nF\nIMAGENET SAMPLES\nFigure 8: Random, non-cherry picked samples from JetFormer-L for selected ImageNet classes:\nflamingo, bustard, golden retriever, hummingbird, bell pepper, volcano and ring-tailed lemur.\n21\nFigure 9: Random, non-cherry picked samples from JetFormer-L for random ImageNet classes.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/JetFormer: An Autoregressive Generative Model of Raw Images and Text.pdf"}
{"title":"MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding","authors":"Rongchang Xie, Chen Du, Ping Song, Chang Liu","summary":"We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks.","url":"http:\/\/arxiv.org\/abs\/2411.17762v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2411.17762v3","published":1732592032000,"comment":null,"pdf_text":"MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding\nRongchang Xie\nChen Du\nPing Song\nChang Liu\nByteDance\nxierongchang@bytedance.com\nAbstract\nWe introduce MUSE-VL, a Unified Vision-Language Model\nthrough Semantic discrete Encoding for multimodal under-\nstanding and generation. Recently, the research commu-\nnity has begun exploring unified models for visual genera-\ntion and understanding. However, existing vision tokenizers\n(e.g., VQGAN) only consider low-level information, which\nmakes it difficult to align with language tokens. This results\nin high training complexity and necessitates a large amount\nof training data to achieve optimal performance. Addition-\nally, their performance is still far from dedicated under-\nstanding models. This paper proposes Semantic Discrete\nEncoding (SDE), which effectively aligns the information\nof visual tokens and language tokens by adding semantic\nconstraints to the visual tokenizer. This greatly reduces the\namount of training data and improves the performance of\nthe unified model. With the same LLM size, our method im-\nproved the understanding performance by 4.8% compared\nto the previous SOTA Emu3 and surpassed the dedicated un-\nderstanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual gener-\nation benchmarks.\n1. Introduction\nRecently, there has been a growing interest in the domain\nof unified Multimodal Large Language Models (MLLMs).\nResearchers are dedicated to developing unified MLLMs\nby integrating visual understanding and generation tasks\nwithin autoregressive next-token prediction models. To re-\nalize a unified MLLM capable of next-token prediction for\nboth visual and text tokens, one of the most critical chal-\nlenges is how to convert visual input into discrete tokens,\nlike text tokenizers do for text.\nSome well done image discretization works have been\nprocessed in the past few years, such as VQ-VAE [57], VQ-\nGAN [16], and MAGVIT [69]. This enables the large lan-\nguage models (LLMs) to jointly learn visual code embed-\ndings along with text tokens. However, previous unified\nworks [52, 62] frequently demonstrate poor performance\nMMBench\nSEED\nMMMU\nScienceQA-IMG\nAI2D\nMath-Vista\nMMStar\nJanus\nEmu2-Chat \nChameleon-30B \nEmu3\nMUSE-VL-7B \nMUSE-VL-32B\n81.8\n71.0\n50.1\n95.0\n79.9\n55.9\n56.7\n72.1\n69.1\n39.7\n93.5\n69.8\n51.3\n49.6\n58.5\n46.6\n47.6\n70.0\n89.2\n68.2\n38.8\n69.4\n63.7\n75.1\n33.5\nFigure 1. The evaluation of multimodal understanding and gener-\nation. (Top) Multimodal understanding results on various bench-\nmarks. MUSE-VL surpasses the leading unified multimodal LLM,\nEmu3 [59]. (Bottom) Images generated with MUSE-VL.\nin multimodal understanding tasks, failing to match the\nperformance of state-of-the-art multimodal understanding\nmodels [35]. This is mainly because these image quanti-\nzation methods are trained solely with image reconstruc-\ntion tasks and lack alignment with textual or semantic fea-\ntures. The discretization process is ineffective in capturing\nthe high-dimensional semantic representation of images and\ninevitably leads to loss of semantic information. As a result,\nthe visual tokens obtained by these methods are not suitable\nfor visual understanding. As shown in Table 6, whether the\nvisual tokens contain semantic information has a significant\n1\narXiv:2411.17762v3  [cs.CV]  19 Mar 2025\n \n \n \n \n \n \n \n \n \n \n \nAutoregressive Transformer \nImage Tokenizer \nText Tokenizer \nImage Detokenizer \nText Detokenizer \nA pink spoonbill  wading \nthrough a wetland \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 2. The overview of MUSE-VL. The image is converted into text-aligned visual tokens by the Semantic Discrete Encoder. The\nvisual tokens and the corresponding text tokens, are fed into an autoregressive transformer. The training objective of the model is to predict\nthe next token for both visual and text tokens. The predicted visual tokens can be decoded into an image by the Image Decoder. The soi\nand eoi tokens are used to mark the start and end of visual tokens, while the green tokens represent the visual tokens obtained through the\nvisual tokenizer.\nimpact on visual understanding.\nRecent research works attempt to address this issue.\nEMU3 [59] reaches state-of-the-art results in both under-\nstanding and generation tasks by fine-tuning two separate\nmodels respectively, but they do not resolve this problem\ncaused by unifying these two tasks within one model. Janus\n[60] uses separate encoders for understanding and genera-\ntion, which increases the complexity of the model. Con-\ncurrent work TokenFlow [45] designs a dual-codebook ar-\nchitecture to decouple the learning of semantic and pixel-\nlevel features. These methods all assume that images need\nto be represented using two different codebook spaces. In\ncontrast, VILA-U [61] combines contrastive loss and recon-\nstruction loss to align the visual encoder with its textual in-\nput, but it struggles to converge in the additional seman-\ntic pretraining of image tokenizers, which is commonly at-\ntributed to loss conflicts.\nIn this paper, we propose Semantic-aware Discrete\nEncoding (SDE) to avoid loss conflicts in VILA-U. Unlike\nVILA-U employs a text encoder to extract semantic features\nand then conducts contrastive learning, our SDE is learned\nfrom a pretrain CLIP-style teacher. It’s worth noting that the\nfeatures extracted by the image encoder of the pre-trained\nCLIP model [46, 71] already contain semantic information.\nTherefore, we use the image encoder of a pretrained CLIP-\nstyle model to extract semantic features. These semantic\nfeatures, together with the image features from a visual en-\ncoder, are then used for quantization. As reconstructing\nfrom the quantized feature, we designed two decoder-like\nbranches: an image decoder for image reconstruction and a\nsemantic decoder to ensure that the discrete quantized fea-\ntures contain semantic information. Our approach considers\nsemantic information during image discretization, meeting\nthe requirements for both visual understanding and genera-\ntion tasks.\nBuilding upon the SDE tokenizer, we introduce MUSE-\nVL, a state-of-the-art and easy-to-reproduce VLM solu-\ntion. MUSE-VL is first pre-trained on image-text pairs to\nalign language and visual tokens. The model is then fine-\ntuned with high-quality multimodal instruction-following\ndata and visual generation data. The training objective is\nnext-token prediction using the standard cross-entropy loss.\nThe experiments demonstrate that MUSE-VL exhibits ro-\nbust performance across complex multimodal tasks such\nas image reasoning, visual question answering, and im-\nage generation. As shown in Fig. 1, MUSE-VL surpasses\nthe current leading unified multimodals [52, 59] on various\nbenchmarks. The main contributions of this paper are as\nfollows:\n• We develop a semantic-aware visual tokenizer SDE\nthat can effectively integrate semantic features during\nthe process of discretizing images into visual codes.\nThis method allows seamless adaptation to any pre-\ntrained LLM without modifications of model structure,\nfacilitating the joint training of visual understanding\nand generation tasks.\n• We propose MUSE-VL, a unified autoregressive trans-\nformer for multimodal understanding and generation.\nMUSE-VL models vision and language as unified dis-\ncrete tokens and achieves state-of-the-art performance\non various vision-language benchmarks.\n2\n2. Related Work\nMultimodal Understanding Models\nA typical VLM for\nmultimodal understanding can be abstracted into three mod-\nules, a pre-trained visual encoder [46, 71], a pre-trained\nLLM [23, 56, 64, 66], and a learnable connector [33, 34]\nbetween the visual encoder and LLM. Open-source VLMs\nhave demonstrated strong multimodal understanding capa-\nbilities by aligning visual features of the pre-trained image\nencoder with the input embedding space of LLMs. VLMs\ncan be roughly divided into two types based on the differ-\nences in how visual features are integrated into LLMs. One\n[1, 31] injects visual information into LLMs using a cross-\nattention mechanism to fuse language and visual informa-\ntion. The other method [2, 4, 11, 35, 36, 42, 58, 63, 67] di-\nrectly appends the features extracted by the visual encoder\nwith text embeddings to form the input sequence at the input\nlayer of the LLMs. In these models, the visual features are\ncontinuous, which presents challenges in the unified mod-\neling of visual and language tokens.\nVisual Tokenization for Generation\nVector quantized\n(VQ) visual tokenizers [16, 54, 57, 68, 69] are proposed\nto convert image pixels into a sequence of discrete tokens\nand then reconstruct the input image from quantized fea-\ntures. VQVAE [57] first quantizes the image embeddings\nby performing a nearest neighbor look-up from the code-\nbook, and then reconstructs the original image through a\ndecoder. VQGAN [16] introduces a discriminator and per-\nceptual loss to enhance the perceptual quality and details\nof the generated images. Recently, researchers have pro-\nposed residual quantization [27], lookup-free quantization\n[69] and multi-scale quantization [54] to further improve the\ngeneration quality. However, these discrete VQ tokenizers\nare exclusively trained with the image reconstruction loss,\nwithout considering semantic features.\nUnified Visual Language Models\nPioneering efforts\nhave made significant strides by enabling multimodal un-\nderstanding and generation within language models. In the\nrealm of generating visual content using VLM, many works\n[18, 19, 25, 50, 72, 74] have integrated VLMs with diffu-\nsion models [48] to achieve high-quality visual outputs. It\nis important to note that VLMs inherently lack the capabil-\nity to directly produce visual content, and the quality of the\ngenerated images heavily relies on the performance of the\ndiffusion models. For example, Emu [50] uses the output of\nthe LLM as a condition for the pretrained diffusion model\nand then generates images with the diffusion model. Trans-\nfusion [74] combines the language modeling loss function\nwith diffusion to train a single transformer.\nOther works like Chameleon [52], Show-o [62] and\nEmu3 [59] have tried to directly adopt the VQ tokenizer to\nencode images for both multimodal understanding and gen-\neration. However, since these visual tokenizers do not con-\ntain semantic information, aligning visual tokens with lan-\nguage tokens becomes difficult, and these models usually\nyield suboptimal performance in multimodal understanding\ntasks. Janus [60] uses separate encoders for understanding\nand generation, but it increases the complexity of the model.\nVILA-U[61] combines contrastive and reconstruction loss\nto align visual and text tokens, but it has convergence prob-\nlems, requiring a specific training recipe and large-scale\nimage-text pairs from COYO-700M [5] dataset.\nIn this work, we explore a semantic-aware discrete en-\ncoding method for image reconstruction and generation.\nOur work reconstructs Siglip’s visual features, which are\nwell aligned with text, making the training process simpler\nand demonstrating outstanding performance in both visual\nunderstanding and generation tasks.\n3. Method\nThe main objective of this work is to establish a simple\nand unified autoregressive transformer for both visual and\nlanguage modalities. In this model, visual and language\ndata can be input and output in the same discrete encod-\ning format. For the language modality, there are already\nwell-developed text tokenizers and large language models\n(LLMs) [23, 64, 66] that have been extensively trained on\nmassive text data. However, how to construct an effective\ntokenizer in the visual modality remains to be explored.\nTherefore, we propose semantic discrete encoding as a\ntokenizer for the visual modality to generate visual tokens\nthat are well-aligned with language. Based on this, we pro-\npose MUSE-VL, a model capable of handling mixed visual\nand language tokens, supporting both visual understanding\nand generation tasks. This section first introduces the vi-\nsual tokenizer proposed in our work, followed by the unified\nvision-language model built upon it.\n3.1. Visual Tokenizer\nPreliminary\nThe VQGAN is a seminal work in the field\nof visual generation [16, 57].\nIt learns a convolutional\nmodel consisting of an encoder Enc and a decoder Dec,\nand it represents an image x using codes q from a learned,\ndiscrete codebook Z = {zk}K\nk=1 ⊂Rd.\nFrist, the image is encoded as z = E(x) ∈Rh×w×d\n. Then, the index q and quantized vector zq are obtained\nthrough element-wise quantization of each spatial feature z\nonto its closest codebook entry. Finally, the decoder recon-\nstructs the image from the quantized vector zq. The training\nobjective consists of image reconstruction loss, vector quan-\ntizer (VQ) loss, discriminator and perceptual loss. Further\ndetails can be found in the literature [16, 49].\n3\nMLP\nVector \nQuantizer\nFeature Fusion\nImage \nEncoder\nSemantic \nEncoder\nImage \nDecoder\nSemantic \nDecoder\nReconstruction Loss\nSemantic Loss\n𝑧𝑠\n𝑥\n𝑧𝑞\n𝑧\n𝑇\nො𝑥\nFigure 3. The overview of Semantic Discrete Encoding. The image is encoded and quantized into semantic discrete tokens, which are then\nseparately reconstructed by the semantic decoder and the image decoder into semantic features and the original image.\nSemantic Discrete Encoding (SDE)\nIn this work, we\npropose the semantic discrete encoding method based on\nthe Vector Quantizer. The architecture of the visual tok-\nenizer is shown in Fig. 3. To guarantee that the discrete\nencoding produced by the tokenizer incorporates semantic\ninformation and is more closely aligned with language to-\nkens, we introduce a semantic decoder and a semantic en-\ncoder. These components retrieve the semantic information\nof the image from the discrete codes.\nSpecifically, for an image x ∈RH×W ×3, where H and\nW represent height and width dimensions, respectively, the\nimage encoder produces the feature z = Enc(x). Here,\nz ∈Rh×w×d, where h and w are the sizes after down-\nsampling, and d is the codebook vector dimension. Subse-\nquently, the feature z is transformed into quantization code\nq ∈Zh×w and quantized embedding zq ∈Rh×w×d through\nthe quantization operation. To ensure that the quantized em-\nbedding carries meaningful semantics, inspired by BEITv2\n[43], a transformer is used as the semantic decoder Decs to\nmaximize the cosine similarity between the decoded feature\nzs and a pre-trained semantic feature T:\nLsem = 1 −cos(zs, T) = 1 −cos(Decs(zq), T)\nIn our study, we adopt the SigLIP model [71] as the se-\nmantic encoder to produce semantic feature T, which has\nbeen trained with an extensive dataset of image-text pairs\nand has been effectively aligned with language. The param-\neters of the semantic encoder are frozen in the training. To\nfurther enhance the semantics of discrete coding, we fuse\nthe semantic feature T with the image feature z in the en-\ncoding process, and then quantize the merged feature:\nzq = Quant(T + z)\nAdditionally, to preserve the image generation capability\nof the tokenizer, a separate image decoder is used to gener-\nate the reconstructed image ˆx. Consequently, the final loss\nfunction is a combination of semantic loss Lsem, image re-\nconstruction loss, and VQ loss in VQGAN:\nLtotal = Lsem + Limg + Lvq\nwhere,\nLimg = ℓ2(x, ˆx) + LP (x, ˆx) + λGLG(ˆx)\nLvq = ∥sg[z] −zq∥2\n2 + β∥z −sg[zq]∥2\n2\nHere, ℓ2 is the L2 reconstruction loss, LP (·) refers to the\nperceptual loss measured by LPIPS [73], and LG(·) is an\nadversarial loss [22]. The second term of Lvq is the commit-\nment loss [57], and β is its weight. We used a convolutional\nnetwork as the image decoder, following VQGAN [16, 49].\n3.2. Unified Vision-Language Modeling\nBased on semantic discrete encoding, we propose a unified\nvision-language modeling named MUSE-VL. The struc-\nture of MUSE-VL is shown in the Fig. 2. The image is\npre-processed into visual tokens {q1, q2, ..., qi} of length\n(h × w) through SDE tokenizer, while the textual data is\nconverted through the text tokenizer. To achieve joint mod-\neling of language and vision, it is sufficient to simply extend\nthe embedding layer of existing LLMs to incorporate newly\nadded visual token IDs. This modification enables seam-\nless integration of multimodal inputs within the model’s ar-\nchitecture. To distinguish visual tokens, two special tokens\n<soi>and <eoi>are added to mark the start and end of vi-\nsual tokens respectively. The training objective of the model\nremains a simple autoregressive task, without any modifica-\ntions to the LLM’s architecture or training loss.\nIn this work, we adopt Yi-1.5 [67] and Qwen-2.5 [53,\n65], which perform well on language tasks, as the base\nLLM. It is crucial to emphasize that the inherent alignment\nof SDE tokenzier with language and the unified autoregres-\nsive architecture enables our model to integrate effortlessly\nwith the most LLMs. This integration is achieved using\nonly minimal image-text data and does not require any ar-\nchitecture modifications. In contrast, previous approaches,\nsuch as Chameleon and Emu3 [52, 59], necessitated alter-\nations to the model architecture and required extensive im-\nage and language data to train the LLM from scratch.\n4\nTable 1. Evaluation on multimodal understanding benchmarks. Compared with previous methods, our MUSE-VL achieved the best\nperformance on various benchmarks. The best results for models with fewer than 10B parameters are in bold, while the best of all models\nare underlined.\nMethod\nLLM\nVisual Token Res. MMBench MMStar SEED MMMU SQA-I AI2D MathVista AVG\nUndstanding Only\nInstructBLIP [12]\nVicuna-7B\nContinuous\n224\n36.0\n32.7\n58.8\n30.6\n60.5\n33.8\n24.4\n39.5\nLLaVA-1.5 [34]\nVicuna-1.5-7B\nContinuous\n336\n64.3\n33.1\n66.1\n35.7\n66.8\n55.5\n27.4\n49.8\nLLaVA-NeXT [35]\nVicuna-1.5-7B\nContinuous\n672\n67.4\n37.6\n70.2\n35.8\n70.1\n66.6\n34.6\n54.6\nLLaVA-NeXT [35]\nYi-34B\nContinuous\n672\n79.3\n51.6\n75.9\n51.1\n81.8\n78.9\n46.5\n66.4\nShareGPT4V [9]\nVicuna-1.5-7B\nContinuous\n336\n68.8\n35.7\n69.7\n37.2\n68.4\n58.0\n26.5\n52.0\nVILA [32]\nLLaMA-2-7B\nContinuous\n336\n68.9\n-\n61.1\n-\n68.2\n-\n-\n-\nUndstanding and Generation\nDreamLLM [14]\nVicuna-7B\nContinuous\n224\n58.2\n-\n-\n-\n-\n-\n-\n-\nUnified-IO2 [39]\n7B from scratch\nContinuous\n384\n71.5\n-\n61.8\n-\n86.2\n-\n-\n-\nEmu2-Chat [51]\nLLaMA-33B\nContinuous\n448\n63.6\n40.7\n62.8\n34.1\n68.2\n49.7\n30.7\n50.0\nVideo-LaVIT [24]\nLlama 2 7B\nContinuous\n224\n67.3\n-\n64.0\n–\n70.0\n-\n-\n-\nJanus [60]\nDeepSeek-1.3B\nContinuous\n384\n69.4\n37.6\n63.7\n30.5\n75.1\n52.8\n33.5\n51.8\nChameleon [52]\n7B from scratch\nDiscrete\n512\n31.1\n31.1\n30.6\n25.4\n46.8\n46.0\n22.3\n33.3\nChameleon [52]\n34B from scratch Discrete\n512\n32.5\n31.8\n48.5\n38.8\n58.8\n53.7\n23.6\n41.1\nSEED-LLaMA [28] Vicuna-7B\nDiscrete\n224\n28.7\n33.1\n51.5\n-\n-\n-\n-\n-\nShow-o [62]\nPhi-1.5-1.3B\nDiscrete\n256\n-\n-\n-\n25.1\n-\n-\n-\n-\nVILA-U [61]\nLLaMA-2-7B\nDiscrete\n384\n-\n-\n59.0\n-\n-\n-\n-\n-\nEmu3 [59]\n8B from scratch\nDiscrete\n512\n58.5\n46.6\n68.2\n31.6\n89.2\n70.0\n47.6\n58.8\nMUSE-VL (ours)\nQwen2.5-7B\nDiscrete\n256\n72.1\n49.6\n69.1\n39.7\n93.5\n69.8\n51.3\n63.6\nMUSE-VL (ours)\nQwen2.5-32B\nDiscrete\n384\n81.8\n56.7\n71.0\n50.1\n95.0\n79.9\n55.9\n70.1\nPretraining\nIn the pre-training stage, we used images\nwith paired text descriptions for training. At this stage, we\ncalculated the loss for all tokens to optimize the model pa-\nrameters using a next-token prediction objective. The pri-\nmary objective at this stage was to effectively learn a robust\nembedding of visual tokens, align visual tokens with text to-\nkens, and build the model’s capability to accurately predict\nimage tokens.\nInstruction Tuning\nFor image understanding tasks, our\nwork uses visual instruction tuning data and image cap-\ntion data. These data are organized in the following format,\nwhere the visual tokens appear in the prompt, and the tar-\nget part is the response text. Only tokens in the target part\nparticipate in the loss calculation:\nPrompt: {text} <soi>{vision tokens} <eoi>\nTarget: {response}\nFor the image generation task, the order of images and\ntexts in the image caption data is reversed here, enabling the\nmodel to generate visual tokens based on the descriptions.\nPrompt: {system text} {image caption}\nTarget: <soi >{vision tokens} <eoi >\nThe system text is randomly sampled from a set of im-\nage generation instructions, such as “Please generate an im-\nage.”, “Show me a photo.”, etc. At the inference stage, the\nuser provides a prompt for generating an image, and the\nmodel will predict the corresponding image tokens. Then,\nthe predicted visual tokens are converted to the image by\nthe image decoder.\n4. Experiments\n4.1. Implementation Details\nVisual Tokenizer\nFor the pre-trained semantic encoder,\nthis work uses two different resolution encoders: SigLIP-\nSO400m-patch14-384 and SigLIP-Large-patch16-256 [71].\nThe semantic decoder is a vision transformer (same as\nBEITv2 [43]). The input images are resized to 384 × 384\nand 256 × 256 respectively, and after quantization, they are\nconverted into discrete codes of 16 × 16 and 27 × 27.\nWe use the pretrained SigLIP parameters as the initial-\nization of the image encoder in SDE, while the image de-\ncoder remains the same ConvNet architecture as the VQ-\nGAN decoder [16, 49]. The codebook size of the tokenizer\nis 32,768, with a vector dimension of 8, and the semantic\nloss weight is set to 1. The training dataset for the tok-\nenizer includes 10 million images from ImageNet-1K [13]\nand CC12M [7] . Other hyperparameters during training\nfollow the default settings in LLamaGEN [49].\n5\nTable 2. Comparison of different visual tokenizers on multimodal\nunderstanding benchmarks. All models used the same base LLM\nand training dataset.\nMethod\nMMBench SEED MMStar AVG\nVQGAN [16]\n32.0\n42.7\n29.1\n34.6\nSEED [17]\n63.1\n57.8\n39.1\n53.3\nLaVIT [25]\n63.3\n59.5\n40.3\n54.4\nSDE (ours)\n70.6\n68.1\n43.8\n60.8\nVision Language Model\nThe method proposed in this pa-\nper can be easily adapted to most pre-trained LLMs. In our\nexperiments, we used Qwen-2.5-7B, Qwen-2.5-32B [53],\nYi-1.5-9B, and Yi-1.5-34B [67] as the base language mod-\nels.\nThe embedding layer of the LLM is expanded by\n32,768 to accommodate visual tokens. The learning rate\nduring training is set to 1e-4, using a cosine schedule with\nwarmup, and AdamW (β1 = 0.9, β2 = 0.95) as the opti-\nmizer. We use the image caption dataset [7, 29] for the\npre-training stage. For visual understanding tasks, we used\nexamples from Cambrian7M [55] and LLaVA-OneVision-\nData [29]. For visual generation tasks, we used the dataset\n[7] and 10M high-quality images.\nEvaluation Setup\nTo evaluate the multimodal under-\nstanding capability, we use benchmarks such as MMBench\n[38], SeedBench-img [28], AI2D [26], MMStar [10], Math-\nVista [41], SciQA-Img [40] and MMMU [70]. We run the\nevaluation based on VLMEvalKit [15]. To evaluate the vi-\nsual generation capability, we use the MJHQ-30K [30] and\nGenEval [20] benchmarks. In MJHQ-30K, the quality of\nthe generated images is measured by calculating the Fr´echet\nInception Distance (FID) [21] between 30K generated sam-\nples and 30K high-quality samples. GenEval [20] is used\nfor evaluating the model’s text-to-image alignment.\n4.2. Evaluation of Visual Tokenizer\nComparison with other Visual Tokenizers\nWe validate\nthe impact of different visual tokenizers on the performance\nof VLMs. Table 2 summarizes the comparison between the\nproposed tokenizer and other visual tokenizers across var-\nious multimodal understanding benchmarks. It should be\nnoted that all results were obtained using the same LLM\n(Yi-1.5-9B) and the same subset of the training set.\nWe use the pre-trained VQGAN model from LLama-\nGEN [49] as the baseline method, which has excellent per-\nformance in image reconstruction and generation. Table 2\nshows that VQGAN performs poorly in multimodal under-\nstanding tasks due to difficulties in aligning with text, and\nwe also found that this model often misidentifies the object\nof the image. By considering semantic information in the\nvisual discretization process, the proposed SDE tokenizer\nTable 3. Evaluation of visual tokenizer on image reconstruction.\nThe evaluations are on ImageNet 50k validation set under the im-\nage resolution of 256 × 256.\nMethod\nCode Size Dim rFID↓PSNR↑SSIM↑\nVILA-U [27]\n1024\n-\n1.80\n-\n-\nVQGAN [16]\n256\n256\n4.99\n20.00\n0.629\nRQ-VAE [27]\n256\n256\n3.20\n-\n-\nMaskGIT [6]\n256\n256\n2.28\n-\n-\nLLamaGen [49]\n256\n8\n2.19\n20.79\n0.675\nSDE (ours)\n256\n8\n2.26\n20.14\n0.646\nextracts visual tokens that are more aligned with text, thus\nexhibiting strong multimodal understanding capabilities.\nWe also compare our method with other discrete vi-\nsual tokenizers. The SEED tokenizer proposed by SEEDL-\nLaMA [18] optimizes image tokens for both discriminative-\nness and reconstruction at the training stage. LaVIT [25] in-\ntroduced a dynamic visual tokenizer. We replace the LLM\nin LaVIT with Yi-1.5-9B for a fair comparison.\nAs shown in Table 2, compared to the recent works\nSEED [18] and LaVIT’s tokenizer [25], the proposed to-\nkenizer exceeds by a large margin (+7.5% and +6.4%) in\naccuracy. The results indicate that the proposed method is\nmore effective than other tokenizers in enhancing the mul-\ntimodal understanding capabilities of VLM.\nImage Reconstruction\nTable 3 presents the quantitative\nresults of the tokenizer on image reconstruction. We use\nr-FID (reconstruction-Fr´echet Inception Distance), PSNR\n(Peak Signal to Noise Ratio), and SSIM (Structural Simi-\nlarity) as metrics for assessing image reconstruction on the\nthe ImageNet 50k validation set under the image resolution\nof 256 × 256. Our approach was compared with VQGAN\n[16], MaskGIT [6], and LLamaGEN [49]. As summarized\nin table 3, SDE matches the state-of-the-art method LLam-\naGEN [49] and surpasses VQGAN [16], RQ-VAE [27], and\nMaskGIT [6]. It is worth noting that our method needs to\nconsider both semantic and image reconstruction simultane-\nously. In contrast, previous methods focused solely on im-\nage reconstruction. We achieved a similar rFID compared to\nVILA-U [61], even though its code size is four times ours.\n4.3. Evaluation of Vision Language Model\nMultimodal Understanding\nTable 1 shows the compar-\nison between MUSE-VL and other leading VLMs on var-\nious multimodal understanding benchmarks. We include\nunderstanding models and unified models for understand-\ning and generation. We categorize the methods into two\ntypes: discrete-visual-token VLMs and continuous-visual-\nembedding VLMs, depending on whether the input image is\n6\na photo of a dog right of a teddy bear \nCute frog dressed up like a cowboy \nwith a western theme \nPhotorealistic Maltipoo dog dressed in \nsteampunk trying \nVelvet mushrooms with mossy rocks \nFigure 4. The generated images from MUSE-VL 7B.\nTable 4. Comparison of the number of image-text pairs in the\ntraining set. Compared to other unified models, our method used\nless training data while achieving superior performance.\nMethod\nChameleon [52] SEED-LLaMA [28] Janus [60]\nNumber 1.4B\n600M\n65M\nMethod\nShow-o [62]\nVILA-U [61]\nOurs\nNumber 35M\n720M\n24M\nconverted into discrete tokens. Table 1 shows that discrete-\nvisual-token models often perform worse than continuous-\nvisual-embedding models, mainly due to challenges in\naligning visual tokens with text tokens.\nThanks to the\nproposed semantic discrete encoding tokenizer (SDE), our\nMUSE-VL outperforms other discrete-visual-token VLMs\nand achieves better or comparable performances compared\nwith continuous-visual-embedding VLMs. MUSE-VL with\n7B parameters reaches 72.1% on the MMBench, +13.6%\nhigher than the previous SOTA discrete method Emu3 [59]\nand other models with the same parameter size. MUSE-VL\nwith 32B parameters achieves state-of-the-art results, which\nexhibit remarkable scalability of the proposed method.\nThe table 4 shows the number of image-text pairs used in\nunified multimodal models. Previous unified models such\nas Chameleon, SEED-LLaMA and VILA-U, typically rely\non extensive image-text pairs to align visual and language\ntokens. By reconstructing the semantic features, the align-\nment and training process of VLM becomes more efficient,\nsurpassing other models with only 24M data.\nTable 5. Quantitative results on text-to-image benchmarks. † result\nis with rewriting.\nType\nMethod\nRes. MJHQ30K↓\nGenEval\nGen. Only\nSDv1.5 [47]\n512\n-\n0.43\nPixArt [8]\n512\n6.14\n0.48\nSD-XL [44]\n1024\n9.55\n0.55\nPlay v2.5 [30]\n1024\n4.48\n-\nDALL-E3 [3]\n1024\n-\n0.67†\nLlamaGen [49]\n512\n-\n0.32\nUnd. and Gen.\nSEED-X [19]\n1024\n-\n0.49\nChameleon [52] 512\n-\n0.39\nLWM [37]\n256\n17.77\n0.47\nShow-o [62]\n256\n15.18\n0.53\nJanus [60]\n384\n10.10\n0.61\nVILA-U [61]\n256\n12.81\n-\nOurs (7B)\n256\n7.73\n0.53 \/ 0.57†\nVisual Generation\nTable 5 shows the quantitative re-\nsults of the text-to-image in GenEVAL [20] and MJHQ-\n30K [30]. We compare MUSE-VL with other state-of-the-\nart generation-only models and unified models. As shown\nin Table 5, MUSE-VL achieves a 7.73 FID score on the\nMJHQ30K benchmark, which outperforms previous SOTA\nunified models and SD-XL. This demonstrates that our\nmodel can generate images with high aesthetics and quality.\nThe Geneval results show that our model achieves better or\ncomparable performance compared to other unified models,\nindicating that the generated images align well with the text\nprompts. Figure 4 presents examples of visual generation.\n7\nTable 6. Ablation study of the semantic and image branches in\nSDE tokenizer.\nThe rFID represents reconstruction capability,\nwhile the rest represent multimodal understanding capability.\nImage Semantic rFID MMB SEED MMStar AVG\n✓\n2.63\n42.8\n48.5\n38.1\n43.1\n✓\n-\n72.5\n67.5\n48.1\n62.7\n✓\n✓\n2.26\n72.1\n69.1\n49.6\n63.6\nTable 7. Ablation of MUSE-VL on LLM and image resolution.\nLLM\nRes MMB SEED MMStar AVG\nYi-1.5-9B\n256\n70.6\n66.1\n43.8\n60.2\nYi-1.5-9B\n384\n73.2\n69.2\n47.4\n63.3\nYi-1.5-34B\n256\n73.5\n67.3\n48.9\n63.2\nQwen-2.5-7B\n256\n71.0\n65.8\n44.2\n60.3\nQwen-2.5-32B 256\n75.1\n65.7\n50.3\n63.7\n4.4. Ablation Studies\nEffect of Semantic Branch and Image Branch\nTable 6\npresents the ablation study of the SDE tokenizer, validating\nthe impact of the semantic and image branches on image\nreconstruction and understanding capabilities. We use the\nrFID on the ImageNet validation set to evaluate reconstruc-\ntion capabilities, and MMB, SEED and MMStar to evaluate\nunderstanding capabilities. The baseline tokenizer consists\nof an image encoder and an image decoder, with the train-\ning task being image reconstruction. It shows that the base-\nline performs poorly on the multimodal understanding task,\nwhich confirms the limitations of VQ tokenizers due to the\npixel-level reconstruction focusing on low-level features.\nWhen only the semantic reconstruction task is performed\n(Row 2), using a semantic encoder and semantic decoder,\nthere is a significant improvement in the understanding ca-\npability, demonstrating the importance of semantic repre-\nsentation for understanding tasks. However, the tokenizer\nlacks image reconstruction ability and cannot decode dis-\ncrete tokens into images. The SDE tokenizer (Row 3), by\nsimultaneously reconstructing image and semantic features,\nintegrates high-level and low-level information during the\nimage discretization process. Compared to the baseline, it\nsignificantly improves visual understanding performance by\n20.5% and reduces the image reconstruction rFID.\nAblation on LLM and Resolution\nIn this section, we use\ntwo series of LLMs (Yi and Qwen) as the base model of\nMUSE-VL and investigate the impact of LLM and image\nresolution on understanding performance. All models were\ntrained using the same subset of the training set. The results\nare shown in Table 7. Firstly, we found that for both the\n6302: Strawberry\n3937\n4691\nFigure 5. Visualization of semantic discrete codes. Rectangular\nboxes of the same color indicate that the corresponding semantic\nID of these patches is the same. It can be observed that semantic\nID can represent a semantic concept.\nYi and Qwen series, larger models consistently yield better\nresults, confirming that the VLM architecture adheres to the\nscale-up theory. Secondly, for the Yi series, a larger input\nsize (384 vs 256) also leads to improved performance. The\nresults show that MUSE-VL exhibits outstanding adaptabil-\nity and scalability, with a better base LLM and larger model\nsize consistently leading to superior performance.\n4.5. Qualitative Evaluation\nVisualization of Semantic Code\nFigure 5 shows the visu-\nalization of semantic encoding. We convert the image into\ndiscrete codes using the proposed SDE tokenizer, group the\npatches of the image according to their codes, and mark\nthem with rectangular boxes. The left image indicates that\nthe two IDs represent the cat’s ears and the area near its\nnose, respectively. The right image visualizes the code that\nrepresents strawberries. The illustration demonstrates that\nthe discrete codes extracted by the SDE tokenizer contain\nhigh-level semantic information, thus significantly enhanc-\ning the understanding capability (as shown in Table 6).\n5. Conclusion\nThis study presents SDE, a semantic discrete encoding\nmethod devised to unify the input formats of images and\ntexts within VLMs. The experimental results show that the\nSDE tokenizer is effective for VLMs handling both visual\ncomprehension and generation tasks.\nBuilding upon the\nproposed visual tokenizer, we propose MUSE-VL, a uni-\nfied vision-language model. This innovative model inte-\ngrates both multimodal understanding and generation tasks\nwithin a unified autoregressive framework. Our method is\nmore efficient than existing VLMs, which rely on additional\ncomponents, such as diffusion models, to bridge visual gen-\neration and understanding. Moreover, it demonstrates that\nthe discrete autoregressive method can achieve better per-\nformance than other advanced VLMs.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:\na visual language model for few-shot learning.\nAdvances\nin neural information processing systems, 35:23716–23736,\n2022. 3\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3\n[3] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng\nWang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee,\nYufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu,\nYunxinJiao, and Aditya Ramesh. Improving image gener-\nation with better captions, 2023. 7\n[4] Lucas Beyer, Andreas Steiner, Andr´e Susano Pinto, Alexan-\nder Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann,\nIbrahim Alabdulmohsin, Michael Tschannen, Emanuele\nBugliarello, et al. Paligemma: A versatile 3b vlm for trans-\nfer. arXiv preprint arXiv:2407.07726, 2024. 3\n[5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun\nLee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m:\nImage-text pair dataset.\nhttps:\/\/github.com\/\nkakaobrain\/coyo-dataset, 2022. 3\n[6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315–11325, 2022.\n6\n[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. In Proceed-\nings of the IEEE\/CVF conference on computer vision and\npattern recognition, pages 3558–3568, 2021. 5, 6\n[8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. Pixart-α: Fast training of dif-\nfusion transformer for photorealistic text-to-image synthesis,\n2023. 7\n[9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 5\n[10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, et al. Are we on the right way for evaluating large\nvision-language models? arXiv preprint arXiv:2403.20330,\n2024. 6\n[11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 24185–24198, 2024. 3\n[12] Wenliang Dai, Junnan Li, DONGXU LI, Anthony Tiong,\nJunqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung,\nand Steven Hoi.\nInstructblip:\nTowards general-purpose\nvision-language models with instruction tuning. In Advances\nin Neural Information Processing Systems, pages 49250–\n49267. Curran Associates, Inc., 2023. 5\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 5\n[14] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng\nMa, and Li Yi. DreamLLM: Synergistic multimodal com-\nprehension and creation. In The Twelfth International Con-\nference on Learning Representations, 2024. 5\n[15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang,\nLin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang,\nJiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-\nsource toolkit for evaluating large multi-modality models,\n2024. 6\n[16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE\/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 1, 3, 4,\n5, 6\n[17] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 6\n[18] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,\nXintao Wang, and Ying Shan. Making llama see and draw\nwith seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n3, 6\n[19] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin\nSong, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Mul-\ntimodal models with unified multi-granularity comprehen-\nsion and generation. arXiv preprint arXiv:2404.14396, 2024.\n3, 7\n[20] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.\nGeneval: an object-focused framework for evaluating text-\nto-image alignment. In Proceedings of the 37th International\nConference on Neural Information Processing Systems, Red\nHook, NY, USA, 2024. Curran Associates Inc. 6, 7\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1125–1134,\n2017. 4\n[23] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux,\nArthur Mensch, Blanche Savary, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,\nFlorian Bressand, et al. Mixtral of experts. arXiv preprint\narXiv:2401.04088, 2024. 3\n[24] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang,\nQuzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang\n9\nSong, Kun Gai, and Yadong Mu. Video-lavit: Unified video-\nlanguage pre-training with decoupled visual-motional tok-\nenization. In International Conference on Machine Learn-\ning, pages 22185–22209, 2024. 5\n[25] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jian-\nchao Tan, Yadong Mu, et al. Unified language-vision pre-\ntraining in llm with dynamic discrete visual tokenization.\nIn International Conference on Learning Representations,\n2024. 3, 6\n[26] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images.\nIn Computer Vision–ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part IV 14, pages 235–\n251. Springer, 2016. 6\n[27] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and\nWook-Shin Han.\nAutoregressive image generation using\nresidual quantization. In Proceedings of the IEEE\/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11523–11532, 2022. 3, 6\n[28] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 5, 6, 7\n[29] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\nHao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Zi-\nwei Liu, et al. Llava-onevision: Easy visual task transfer.\narXiv preprint arXiv:2408.03326, 2024. 6\n[30] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Lin-\nmiao Xu, and Suhail Doshi. Playground v2. 5: Three in-\nsights towards enhancing aesthetic quality in text-to-image\ngeneration. arXiv preprint arXiv:2402.17245, 2024. 6, 7\n[31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation. In Interna-\ntional conference on machine learning, pages 12888–12900.\nPMLR, 2022. 3\n[32] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Moham-\nmad Shoeybi, and Song Han. Vila: On pre-training for vi-\nsual language models. In Proceedings of the IEEE\/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n26689–26699, 2024. 5\n[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36:34892–34916, 2023. 3\n[34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 3, 5\n[35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Im-\nproved reasoning, ocr, and world knowledge, 2024. 1, 3,\n5\n[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 3\n[37] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.\nWorld model on million-length video and language with\nringattention. arXiv preprint arXiv:2402.08268, 2024. 7\n[38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your\nmulti-modal model an all-around player? In Computer Vi-\nsion – ECCV 2024, pages 216–233, Cham, 2025. Springer\nNature Switzerland. 6\n[39] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang,\nSavya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha\nKembhavi. Unified-io 2: Scaling autoregressive multimodal\nmodels with vision language audio and action. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 26439–26455, 2024. 5\n[40] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan.\nLearn to explain: Multimodal reasoning\nvia thought chains for science question answering. In The\n36th Conference on Neural Information Processing Systems\n(NeurIPS), 2022. 6\n[41] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-\nical reasoning of foundation models in visual contexts. In In-\nternational Conference on Learning Representations (ICLR),\n2024. 6\n[42] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xi-\nanzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods,\nanalysis & insights from multimodal llm pre-training. arXiv\npreprint arXiv:2403.09611, 2024. 3\n[43] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu\nWei. Beit v2: Masked image modeling with vector-quantized\nvisual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 4,\n5\n[44] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach. SDXL: Improving latent diffusion models\nfor high-resolution image synthesis. In The Twelfth Interna-\ntional Conference on Learning Representations, 2024. 7\n[45] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang,\nYiming Gao, Hu Ye, Daniel K Du, Zehuan Yuan, and\nXinglong Wu.\nTokenflow: Unified image tokenizer for\nmultimodal understanding and generation.\narXiv preprint\narXiv:2412.03069, 2024. 2\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2, 3\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bjorn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models . In 2022 IEEE\/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 10674–10685, Los Alamitos, CA, USA,\n2022. IEEE Computer Society. 7\n10\n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 3\n[49] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue\nPeng, Ping Luo, and Zehuan Yuan. Autoregressive model\nbeats diffusion: Llama for scalable image generation. arXiv\npreprint arXiv:2406.06525, 2024. 3, 4, 5, 6, 7\n[50] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Emu: Generative pretraining in\nmultimodality. In The Twelfth International Conference on\nLearning Representations, 2023. 3\n[51] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiy-\ning Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang.\nGenerative multimodal mod-\nels are in-context learners. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\npages 14398–14409, 2024. 5\n[52] Chameleon Team. Chameleon: Mixed-modal early-fusion\nfoundation models. arXiv preprint arXiv:2405.09818, 2024.\n1, 2, 3, 4, 5, 7\n[53] Qwen Team. Qwen2.5: A party of foundation models, 2024.\n4, 6\n[54] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li-\nwei Wang. Visual autoregressive modeling: Scalable image\ngeneration via next-scale prediction. Advances in neural in-\nformation processing systems, 37:84839–84865, 2024. 3\n[55] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun\nWoo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-\n1: A fully open, vision-centric exploration of multimodal\nllms. arXiv preprint arXiv:2406.16860, 2024. 6\n[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 3\n[57] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 1, 3, 4\n[58] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 3\n[59] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan\nSun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,\nZhen Li, Qiying Yu, et al. Emu3: Next-token prediction is\nall you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2,\n3, 4, 5, 7\n[60] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling visual encoding\nfor unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024. 2, 3, 5, 7\n[61] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang,\nDacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu\nYin, Li Yi, et al. Vila-u: a unified foundation model inte-\ngrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024. 2, 3, 5, 6, 7\n[62] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,\nWeihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One\nsingle transformer to unify multimodal understanding and\ngeneration. arXiv preprint arXiv:2408.12528, 2024. 1, 3,\n5, 7\n[63] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng,\nand Jiashi Feng. Pllava: Parameter-free llava extension from\nimages to videos for video dense captioning. arXiv preprint\narXiv:2404.16994, 2024. 3\n[64] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce\nBian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan,\net al. Baichuan 2: Open large-scale language models. arXiv\npreprint arXiv:2309.10305, 2023. 3\n[65] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen\nYu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng\nLiu, Fei Huang, et al. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024. 4\n[66] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen\nYu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng\nLiu, Fei Huang, et al. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024. 3\n[67] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,\nGuanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen,\nJing Chang, et al. Yi: Open foundation models by 01. ai.\narXiv preprint arXiv:2403.04652, 2024. 3, 4, 6\n[68] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu.\nVector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3\n[69] Lijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Ver-\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh\nBirodkar, Agrim Gupta, Xiuye Gu, et al. Language model\nbeats diffusion–tokenizer is key to visual generation. arXiv\npreprint arXiv:2310.05737, 2023. 1, 3\n[70] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi.\nIn Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 9556–\n9567, 2024. 6\n[71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pages 11975–11986, 2023. 2, 3, 4, 5\n[72] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong\nZhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang,\nLinyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-\nGang Jiang, and Xipeng Qiu. AnyGPT: Unified multimodal\nLLM with discrete sequence modeling. In Proceedings of\nthe 62nd Annual Meeting of the Association for Computa-\n11\ntional Linguistics (Volume 1: Long Papers), pages 9637–\n9662, Bangkok, Thailand, 2024. Association for Computa-\ntional Linguistics. 3\n[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586–595, 2018. 4\n[74] Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-\nmodal model. In The Thirteenth International Conference\non Learning Representations, 2025. 3\n12\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding.pdf"}
{"title":"JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation","authors":"Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, Chong Ruan","summary":"We present JanusFlow, a powerful framework that unifies image understanding\nand generation in a single model. JanusFlow introduces a minimalist\narchitecture that integrates autoregressive language models with rectified\nflow, a state-of-the-art method in generative modeling. Our key finding\ndemonstrates that rectified flow can be straightforwardly trained within the\nlarge language model framework, eliminating the need for complex architectural\nmodifications. To further improve the performance of our unified model, we\nadopt two key strategies: (i) decoupling the understanding and generation\nencoders, and (ii) aligning their representations during unified training.\nExtensive experiments show that JanusFlow achieves comparable or superior\nperformance to specialized models in their respective domains, while\nsignificantly outperforming existing unified approaches across standard\nbenchmarks. This work represents a step toward more efficient and versatile\nvision-language models.","url":"http:\/\/arxiv.org\/abs\/2411.07975v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2411.07975v2","published":1731434110000,"comment":"Accepted by CVPR 2025","pdf_text":"JanusFlow: Harmonizing Autoregression and Rectified Flow\nfor Unified Multimodal Understanding and Generation\nYiyang Ma1,2\nXingchao Liu1,†\nXiaokang Chen1,†\nWen Liu1,†\nChengyue Wu1,3\nZhiyu Wu1,2\nZizheng Pan1\nZhenda Xie1\nHaowei Zhang1\nXingkai Yu1\nLiang Zhao1\nYisong Wang1,4\nJiaying Liu2\nChong Ruan1,‡\n1DeepSeek-AI\n2Peking University\n3The University of Hong Kong\n4Tsinghua University\n†Equal contribution, ‡Corresponding author\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nWe present JanusFlow, a powerful framework that unifies image understanding and generation\nin a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive\nlanguage models with rectified flow, a state-of-the-art method in generative modeling. Our\nkey finding demonstrates that rectified flow can be straightforwardly trained within the large\nlanguage model framework, eliminating the need for complex architectural modifications.\nTo further improve the performance of our unified model, we adopt two key strategies: (i)\ndecoupling the understanding and generation encoders, and (ii) aligning their representations\nduring unified training. Extensive experiments show that JanusFlow achieves comparable or\nsuperior performance to specialized models in their respective domains, while significantly\noutperforming existing unified approaches across standard benchmarks. This work represents\na step toward more efficient and versatile vision-language models.\n1. Introduction\nLarge language models (LLMs) have demonstrated remarkable capabilities in learning diverse\nknowledge and generalizing to new scenarios [1, 7, 8, 69, 91]. Leveraging these capabilities,\nresearchers have developed sophisticated models specialized in image comprehension [2, 15, 47,\n49, 56, 58] and text-to-image generation [23, 73, 76, 79].\nThe field has recently shifted toward creating unified systems capable of handling both\ntasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image\nmodels for high-quality generation while training LLMs to generate conditions for these mod-\nels [19, 25–27, 87]. However, this approach introduces architectural complexity and potentially\nconstrains the model’s capabilities through maintaining separate LLM and generative com-\nponents. Alternative approaches [88, 97, 99, 100, 108] propose training a single LLM for both\ntasks, typically incorporating either diffusion models [32, 83] or vector-quantized autoregressive\nmodels [22, 86].\nOur approach builds upon recent breakthroughs in rectified flow models [3, 23, 55, 61, 62],\nwhich provide a simple framework for generative modeling while delivering exceptional empir-\narXiv:2411.07975v2  [cs.CV]  24 Mar 2025\nPOPE\nGQA\nMMBench\nSEEDB\nMM-Vet\nGenEval\n-MJHQ FID\nMME-Perception\nVQAv2\n61.52\n73.03\n84.54\n39.18\n48.36\n57.55\n26.18\n47.36\n68.55\n42.27\n54.54\n66.82\n16.33\n22.66\n29.0\n40.0\n50.0\n60.0\n-16.82\n-13.64\n-10.46\n752.46\n1004.91\n1257.36\n52.06\n64.12\n76.18\nEmu3-Chat (8B)\nInstructBLIP (7B)\nLLaVA-v1.5-Phi (1.4B)\nShow-o (1.3B)\nJanusFlow (Ours, 1.3B)\n(a) Benchmark Performances.\n(b) Visual Generation Results.\nFigure 1 | Multimodal understanding and image generation with JanusFlow. JanusFlow surpasses the\nstate-of-the-art unified multimodal models and several task-specific understanding models on visual\nunderstanding benchmarks. It is also capable of generating high-quality images. The resolution of the\nimages is 384 × 384.\nical performance [23, 36, 45]. Building on these advances, we propose JanusFlow, a powerful\nunified multimodal model that seamlessly integrates rectified flow with LLM architecture. Fol-\nlowing a minimalist design principle, our architecture requires only a lightweight encoder and\ndecoder to adapt the LLM for rectified flow operations. To optimize JanusFlow’s performance,\nwe implement two key strategies: First, we maintain separate vision encoders for understanding\nand generation tasks, preventing task interference and thus enhancing comprehension capabili-\nties. Second, we align the intermediate representations between generation and understanding\nmodules during training, strengthening semantic coherence in the generation process.\nJanusFlow shows state-of-the-art performances in both multimodal comprehension and\ntext-to-image generation compared to existing unified approaches, and even outperforms\nseveral specialized methods. Specifically, on text-to-image generation benchmarks, MJHQ\nFID-30k [48], GenEval [28] and DPG-Bench [34], JanusFlow achieves scores of 9.51, 0.63 and\n80.09%, surpassing established text-to-image models including SDv1.5 [77] and SDXL [73]. In\nmultimodal comprehension benchmarks, JanusFlow attains scores of 74.9, 70.5 and 60.3 on\nMMBench [63], SeedBench [46], and GQA [35], respectively, exceeding specialized models such\nas LLaVA-v1.5 [56] and Qwen-VL-Chat [4]. Notably, these results are achieved with a compact\nLLM architecture with only 1.3B parameters.\n2. Related Work\nVisual Generation with Flow-based Generative Models.\nRecent years have witnessed re-\nmarkable progress in visual generation through diffusion models [32, 83], leading to impressive\nmodels like [67, 73, 76–79]. Building on these advances, flow-based generative models [3, 55, 61]\nemerged as a simplified alternative framework. These approaches have recently enabled ad-\nvanced visual generation models [23, 36] that achieve superior empirical performance with\nfaster sampling. Our work demonstrates that rectified flow [60–62] can be effectively integrated\ninto LLMs, creating unified models that excel in both understanding and generation tasks.\n2\nUnified Models For Understanding and Generation.\nThe development of multimodal large\nlanguage models (MLLMs) has enabled effective integration of text and visual information.\nBuilding upon powerful LLMs [7, 91, 92], recent MLLMs [2, 15, 49, 56, 58, 64] have demonstrated\nexceptional multimodal understanding capabilities. Current research increasingly focuses on\narchitectures that can simultaneously handle visual understanding and generation tasks. One\napproach extends MLLMs with pre-trained diffusion models [19, 25–27, 87, 101]. However,\nthese systems essentially utilize diffusion models as external tools, where the MLLM generates\nconditions for image generation without possessing direct generative capabilities. This separa-\ntion often results in suboptimal performance compared to standalone diffusion models [25, 87].\nAnother line of work [88, 97, 99, 100, 108] aim to train a single LLM for both tasks. Many\nof these methods employ vector-quantization [22, 86] to convert images into discrete tokens,\nenabling unified autoregressive processing [88, 97]. While straightforward to implement, these\napproaches are inherently limited by their image tokenization quality.\nOur work focuses on developing unified models that combine autoregressive capabilities\nwith flow\/diffusion models, leveraging their proven effectiveness in visual generation. Com-\npared to similar approaches [100, 107, 108], JanusFlow offers three key advantages: (i) a simple\nyet effective generation process using rectified flow, (ii) enhanced performance through de-\ncoupled vision encoders that resolve inter-task conflicts, and (iii) improved generation quality\nthrough representation alignment regularization, enabled by our decoupled encoder design.\n3. JanusFlow\nIn this section, we introduce the architecture of JanusFlow and our training strategies.\n3.1. Background\nMultimodal LLMs.\nGiven a dataset D containing discrete token sequences, each of which\ncan be formulated as 𝑥= (𝑥1, · · · , 𝑥ℓ), large language models (LLMs) are trained to model the\nsequence distribution in an autoregressive manner,\nlog P𝜃𝐿𝐿𝑀(𝑥) =\nℓ−1\n∑︁\n𝑖=0\nlog P𝜃𝐿𝐿𝑀(𝑥𝑖+1|𝑥1, . . . , 𝑥𝑖),\n(1)\nwhere 𝜃𝐿𝐿𝑀denotes the parameters of the LLM and ℓis the sequence length. After being\ntrained on large-scale datasets, LLMs exhibit the ability to generalize across various tasks and\nfollow diverse instructions [1, 8, 69]. To extend these models to handle visual inputs, LLMs\nare augmented with vision encoders [2, 56, 58]. For instance, LLaVA [58] integrates an LLM\nwith a pre-trained CLIP [75] image encoder via a projection layer, transforming the extracted\nimage features into a joint embedding space that the LLM can process as word embeddings. By\nleveraging large-scale multimodal datasets and increasingly powerful LLMs, this architecture\nhas facilitated the development of advanced multimodal models capable of addressing a wide\nrange of vision-language tasks [4, 47, 56, 64].\nRectified Flow.\nFor a dataset D consisting of continuous 𝑑-dimensional data points 𝑥=\n(𝑥1, · · · , 𝑥𝑑) drawn from an unknown data distribution 𝜋1, rectified flow [55, 61] models the data\ndistribution by learning an ordinary differential equation (ODE) defined over time 𝑡∈[0, 1]:\nd𝑧𝑡\nd𝑡= 𝑣𝜃𝑁𝑁(𝑧𝑡, 𝑡),\n𝑧0 ∼𝜋0,\n(2)\n3\nLarge Language Model\nUnd. Encoder 𝑓𝑒𝑛𝑐\nText Tokenizer\nText De-Tokenizer\n(a) Understanding: Autoregression\nText Tokenizer\n(b) Generation: Rectified Flow\nClean Image\nNoisy Image 𝑧𝑡\nUnd. Prompt\nResponse\n(Next Token Prediction)\nGen. Prompt\nGen. Encoder 𝑔𝑒𝑛𝑐\nGen. Decoder 𝑔𝑑𝑒𝑐\nVelocity\n(All Image Tokens)\nOverwrite\n𝑧𝑡with 𝑧𝑡+d𝑡\n𝑧𝑡+d𝑡= 𝑧𝑡+ 𝑣𝑡d𝑡\nRepeat until 𝑡= 1\nFigure 2 | Architecture of the proposed JanusFlow. For visual understanding, the LLM performs\nautoregressive next-token prediction to generate responses. For image generation, the LLM\nemploys images with rectified flow. Starting from Gaussian noise at 𝑡= 0, the LLM iteratively\nupdates 𝑧𝑡by predicting velocity vectors until reaching 𝑡= 1. We omit the VAE encoder, the skip\nconnection leveraged in generation and the linear layer after 𝑓𝑒𝑛𝑐for simplicity.\nwhere 𝜃𝑁𝑁represents the parameters of the velocity neural network and 𝜋0 is a simple distri-\nbution, typically standard Gaussian noise N (0, 𝐼). The network is trained by minimizing the\nEuclidean distance between the neural velocity and the directions of linear paths connecting\nrandom points from 𝜋0 and 𝜋1,\nmin\n𝜃\nE𝑡∼P(𝑡),𝑧0∼𝜋0,𝑥∼𝜋1\nh\f\f\f\f𝑣𝜃𝑁𝑁(𝑧𝑡, 𝑡) −(𝑥−𝑧0)\n\f\f\f\f2i\n, where 𝑧𝑡= 𝑡𝑥+ (1 −𝑡)𝑧0.\n(3)\nHere, P(𝑡) is a distribution over time 𝑡∈[0, 1]. When the network has sufficient capacity and the\nobjective is perfectly minimized, the optimal velocity field 𝑣𝜃∗\n𝑁𝑁maps the elementary distribution\n𝜋0 to the true data distribution 𝜋1. More precisely, the distribution of 𝑧1 =\n∫1\n0 𝑣𝜃∗\n𝑁𝑁(𝑧𝑡, 𝑡)d𝑡,\nwith 𝑧0 ∼𝜋0, follows 𝜋1. Despite its conceptual simplicity, rectified flow has shown superior\nperformance in various generative modeling tasks, including text-to-image generation [23],\naudio generation [40] and biological structure generation [38].\n3.2. A Unified Framework for Multimodal Understanding and Generation\nJanusFlow presents a unified framework designed to address both vision understanding and\nimage generation tasks. Next we outline how JanusFlow handles these two tasks within a single\nLLM architecture.\nMultimodal Understanding.\nIn multimodal understanding tasks, the LLM processes an input\nsequence consisting of interleaved text and image data. The text is tokenized into discrete\ntokens, each of which is transformed into an embedding of dimension 𝐷𝑒𝑚𝑏. For the images, an\nimage encoder 𝑓𝑒𝑛𝑐encodes each image 𝑥𝑖𝑚into a feature map of shape 𝐻𝑖𝑚× 𝑊𝑖𝑚× 𝐷𝑒𝑛𝑐. This\nfeature map is flattened and projected through a linear transformation layer into a sequence\nof embeddings with shape 𝐻𝑖𝑚𝑊𝑖𝑚× 𝐷𝑒𝑚𝑏. 𝐻𝑖𝑚and 𝑊𝑖𝑚are determined by the image encoder.\nThe text and image embeddings are concatenated to form the input sequence to the LLM, which\nthen autoregressively predicts the next tokens based on the input sequence of embeddings.\nAccording to common practice [88, 97, 100], we add special token |BOI| before the image and\n|EOI| after the image to help the model locate the image embeddings in the sequence.\n4\nImage Generation.\nFor image generation, our LLM takes a text sequence 𝑥𝑐𝑜𝑛as condition\nand generates a corresponding image using rectified flow. To improve computational efficiency,\ngeneration occurs in the latent space using a pre-trained SDXL-VAE [73].\nThe generation process begins by sampling Gaussian noise 𝑧0 of shape 𝐻𝑙𝑎𝑡𝑒𝑛𝑡× 𝑊𝑙𝑎𝑡𝑒𝑛𝑡× 𝐷𝑙𝑎𝑡𝑒𝑛𝑡\nin the latent space, which is then processed by a generation encoder 𝑔𝑒𝑛𝑐into a sequence of\nembeddings 𝐻𝑔𝑒𝑛𝑊𝑔𝑒𝑛× 𝐷𝑒𝑚𝑏. This sequence is concatenated with a time embedding representing\nthe current time step 𝑡(𝑡= 0 at the beginning), resulting in a sequence of length 𝐻𝑔𝑒𝑛𝑊𝑔𝑒𝑛+ 1.\nUnlike previous approaches that employ various attention masking strategies [100, 108], we\nfound that causal attention suffices, as our preliminary experiments showed no performance\nbenefits from alternative masking schemes. The LLM’s output corresponding to 𝑧0 is trans-\nformed back into the latent space by a generation decoder 𝑔𝑑𝑒𝑐, producing a velocity vector of\nshape 𝐻𝑙𝑎𝑡𝑒𝑛𝑡× 𝑊𝑙𝑎𝑡𝑒𝑛𝑡× 𝐷𝑙𝑎𝑡𝑒𝑛𝑡. The state is updated by a standard Euler solver,\n𝑧𝑡+d𝑡= 𝑧𝑡+ 𝑣(𝑧𝑡, 𝑡)d𝑡,\n(4)\nwhere d𝑡is a user-defined step size. We replace 𝑧0 with 𝑧d𝑡on the input and iterate the process\nuntil we get 𝑧1, which is then decoded into the final image by the VAE decoder. To enhance\ngeneration quality, we employ classifier-free guidance (CFG) when computing the velocity:\n𝑣(𝑧𝑡, 𝑡) = 𝑤𝑣(𝑧𝑡, 𝑡| 𝑥𝑐𝑜𝑛) + (1 −𝑤)𝑣(𝑧𝑡, 𝑡| ∅),\n(5)\nwhere 𝑣(𝑧𝑡, 𝑡| ∅) denotes the velocity inferred without text conditioning and 𝑤⩾1 controls the\nmagnitute of CFG. Empirically, increasing 𝑤yields higher semantic alignment [23, 62, 73, 77].\nAnalogous to multimodal understanding, we prepend the special token |BOI| to indicate the\nstart of image generation in the sequence.\nDecoupling Encoders for the Two Tasks.\nPrevious approaches that unify autoregressive\ngeneration and diffusion models within a joint LLM training framework [100, 108] employ\nidentical encoders ( 𝑓𝑒𝑛𝑐and 𝑔𝑒𝑛𝑐) for both understanding and generation tasks. For instance,\nZhou et al. [108] performs both tasks in the same VAE latent space using a shared U-Net or\nlinear encoder, while Xie et al. [100] leverages MAGVIT-v2 [102] to encode image patches into\ndiscrete tokens for both tasks.\nHowever, recent work on unified autoregressive models has shown this shared encoder\ndesign to be suboptimal [97], particularly in models that generate images through autoregression\non vector-quantized tokens. Drawing from these insights, JanusFlow adopts a decoupled\nencoder design. Specifically, we employ a pre-trained SigLIP-Large-Patch\/16 [106] model as 𝑓𝑒𝑛𝑐\nto extract semantic continuous features for multimodal understanding, while using separate\nConvNeXt blocks [96] initialized from scratch as 𝑔𝑒𝑛𝑐and 𝑔𝑑𝑒𝑐for generation, chosen for its\neffectiveness. Following established practices [5, 14, 93], we incorporate a long skip connection\nbetween 𝑔𝑒𝑛𝑐and 𝑔𝑑𝑒𝑐. Our controlled experiments in Sec. 4.5 demonstrate that this decoupled\nencoder design significantly improves the performance of our unified model. The complete\narchitecture of JanusFlow is illustrated in Fig. 2.\n3.3. Training Schemes\nAs illustrated in Fig. 3, we train our model in three sequential stages, detailed below.\nStage 1: Adaptation of Randomly Initialized Components.\nIn the first stage, we focus on\ntraining only the randomly initialized components: the linear layers, generation encoder, and\n5\nLLM\nUnd. Enc. 𝑓𝑒𝑛𝑐\nLinear\nGen. Enc. 𝑔𝑒𝑛𝑐\nVAE Enc.\nGen. Dec. 𝑔𝑑𝑒𝑐\nText De-Token.\nStage 1\nAdaptation\nLLM\nUnd. Enc. 𝑓𝑒𝑛𝑐\nLinear\nGen. Enc. 𝑔𝑒𝑛𝑐\nVAE Enc.\nGen. Dec. 𝑔𝑑𝑒𝑐\nText De-Token.\nStage 2\nUnified Pre-Training\nLLM\nUnd. Enc. 𝑓𝑒𝑛𝑐\nLinear\nGen. Enc. 𝑔𝑒𝑛𝑐\nVAE Enc.\nGen. Dec. 𝑔𝑑𝑒𝑐\nText De-Token.\nStage 3\nSupervised Fine-Tuning\nFigure 3 | Three training stages of JanusFlow. The trainable modules are marked with flame\nand the frozen modules are marked with snowflakes.\ngeneration decoder. This stage serves to adapt these new modules to work effectively with the\npre-trained LLM and SigLIP encoder, essentially functioning as an initialization phase for the\nnewly introduced components.\nStage 2: Unified Pre-Training.\nFollowing the adaptation stage, we train the entire model\nexcept for the visual encoder, consistent with previous approaches [58, 64]. The training incor-\nporates three data types: multimodal understanding, image generation, and text-only data. We\ninitially allocate a higher proportion of multimodal understanding data to establish the model’s\nunderstanding capabilities. Subsequently, we increase the ratio of image generation data to\naccommodate the convergence requirements of diffusion-based models [18, 72].\nStage 3: Supervised Fine-Tuning (SFT).\nIn the final stage, we fine-tune the pre-trained model\nusing instruction tuning data, which comprises dialogues, task-specific conversations, and high-\nquality text-conditioned image generation examples. During this stage, we also unfreeze the\nSigLIP encoder parameters [64, 90, 97]. This fine-tuning process enables the model to effectively\nrespond to user instructions for both multimodal understanding and image generation tasks.\n3.4. Training Objective\nTraining JanusFlow involves two types of data, multimodal understanding data and image\ngeneration data. Both types of data contain two parts: “condition” and “response”. “Condition”\nrefers to the prompting of the tasks (e.g., text prompts in the task of generation and images in\nthe task of understanding) while “response” refers to the corresponding responses of the two\ntasks. The data can be formatted as 𝑥= (𝑥𝑐𝑜𝑛, 𝑥𝑟𝑒𝑠), where the superscript 𝑐𝑜𝑛denotes “condition”\nand 𝑟𝑒𝑠denotes “response”. We denote the length of the whole sequence 𝑥as ℓ, the length of\n𝑥𝑐𝑜𝑛as ℓ𝑐𝑜𝑛and the length of 𝑥𝑟𝑒𝑠as ℓ𝑟𝑒𝑠. We use 𝜃to represent the collection of all the trainable\nparameters in JanusFlow, including the LLM, 𝑓𝑒𝑛𝑐, 𝑔𝑒𝑛𝑐, 𝑔𝑑𝑒𝑐and the linear transformation layers.\nAutoregression Objective.\nFor mutimodal understanding tasks, 𝑥𝑟𝑒𝑠contains only text tokens.\nJanusFlow is trained using the maximum likelihood principle,\nL𝐴𝑅(𝜃) = −E𝑥∼D𝑢𝑛𝑑\n\" ℓ−1\n∑︁\n𝑖=ℓ𝑐𝑜𝑛\nlog P𝜃(𝑥𝑖+1|𝑥1, . . . , 𝑥𝑖)\n#\n,\n(6)\n6\nTable 1 | Hyper-parameters of the proposed JanusFlow. Data ratio denotes the proportion of\nmultimodal understanding data, image generation data and text-only data. In the initial 10, 000\nsteps of Stage 2, we apply a data ratio of 30 : 50 : 20 to boost the understanding ability.\nStage 1\nStage 2\nStage 3\nLearning Rate\n1.0 × 10−4\n1 × 10−4\n2.0 × 10−5\nLR Scheduler\nConstant\nConstant\nConstant\nWeight Decay\n0.0\n0.0\n0.0\nGradient Clip\n1.0\n1.0\n1.0\nOptimizer\nAdamW (𝛽1 = 0.9, 𝛽2 = 0.95)\nWarm-up Steps\n2, 000\n2, 000\n1, 000\nTraining Steps\n10, 000\n390, 000\n26, 000\nBatch Size\n512\n512\n256\nData Ratio\n50 : 50 : 0\n14 : 80 : 6\n21 : 70 : 9\nwhere the expectation is taken over all (𝑥𝑐𝑜𝑛, 𝑥𝑟𝑒𝑠) pairs in our multimodal understanding dataset\nD𝑢𝑛𝑑, computing loss only over tokens in 𝑥𝑟𝑒𝑠.\nRectified Flow Objective.\nFor image generation tasks, 𝑥𝑐𝑜𝑛consists of text tokens and 𝑥𝑟𝑒𝑠is\nthe corresponding image. JanusFlow is trained with the rectified flow objective,\nL𝑅𝐹(𝜃) = E𝑥∼D𝑔𝑒𝑛,𝑡∼P(𝑡),𝑧0∼N(0,𝐼)\nh\n||𝑣𝜃(𝑧𝑡, 𝑡| 𝑥𝑐𝑜𝑛) −(𝑥𝑟𝑒𝑠−𝑧0)||2i\n,\n(7)\nwhere 𝑧𝑡= 𝑡𝑥𝑟𝑒𝑠+ (1 −𝑡)𝑧0. Following Stable Diffusion 3 [23], we set the time distribution P(𝑡)\nto the logit-normal distribution. To enable CFG inference, we randomly drop 10% of the text\nprompts in training.\nRepresentation Alignment Regularization.\nRecent work [103] has shown that aligning inter-\nmediate representations between diffusion transformers and semantic vision encoders enhances\ndiffusion model generalization. Our decoupled vision encoder design enables efficient imple-\nmentation of this alignment as a regularization term. Specifically, for generation tasks, we align\nfeatures from the understanding encoder 𝑓𝑒𝑛𝑐with the LLM’s intermediate features,\nL𝑅𝐸𝑃𝐴(𝜃, 𝜑) = −E𝑥∼D𝑔𝑒𝑛\n\u0002\nsim \u0000stop_grad( 𝑓𝑒𝑛𝑐(𝑥𝑟𝑒𝑠)), ℎ𝜑(𝑞𝜃(𝑧𝑡))\u0001\u0003\n,\n(8)\nwhere 𝑞𝜃(𝑧𝑡) denotes an intermediate LLM representation given input 𝑧𝑡, and ℎ𝜑is a small\ntrainable MLP that projects 𝑞𝜃(𝑧𝑡) to dimension 𝐷𝑒𝑛𝑐. The function sim(·, ·) computes the mean\nof element-wise cosine similarity between embeddings. Before computing the loss, we reshape\nℎ𝜑(𝑞𝜃(𝑧𝑡)) to 𝐻𝑔𝑒𝑛× 𝑊𝑔𝑒𝑛× 𝐷𝑒𝑛𝑐. To simplify the implementation, we intentionally adjust the\nconfiguration of 𝑔𝑒𝑛𝑐and 𝑔𝑑𝑒𝑐to ensure 𝐻𝑔𝑒𝑛= 𝐻𝑖𝑚and 𝑊𝑔𝑒𝑛= 𝑊𝑖𝑚. The gradient of L𝑅𝐸𝑃𝐴is not\nback-propagated through the understanding encoder. This alignment loss helps the LLM’s\ninternal feature space (given noisy input 𝑧𝑡) align with the understanding encoder’s semantic\nfeature space, thereby improving generation quality when producing images from new random\nnoise and text conditions during inference.\nSummary.\nAll three objectives are applied across all training stages. Multimodal understand-\ning tasks use L𝐴𝑅, while image generation tasks employ the combined loss L𝑅𝐹+ L𝑅𝐸𝑃𝐴. Detailed\nexperimental settings are provided in Sec. 4.1.\n7\n4. Experiments\nWe conduct extensive experiments to evaluate the capabilities of JanusFlow in both multimodal\nunderstanding and generation tasks. First, we describe our experimental setup and implementa-\ntion details. Then, we present results on standard benchmarks for multimodal understanding\nand image generation. Finally, we perform ablation studies to validate our key design choices.\n4.1. Experiment Setup and Implementation Details\nOur framework builds upon an enhanced version1 of DeepSeek-LLM (1.3B) [7, 64]. The LLM\nconsists of 24 transformer blocks and supports a sequence length of 4, 096. In our model, both\nunderstanding and generation exploits images of resolution 384.\nFor multimodal understanding, we leverage SigLIP-Large-Patch\/16 [106] as 𝑓𝑒𝑛𝑐. For image\ngeneration, we utilize the pre-trained SDXL-VAE [73] for its latent space. The generation encoder\n𝑔𝑒𝑛𝑐comprises a 2 × 2 patchify layer followed by two ConvNeXt [96] blocks and a linear layer.\nThe generation decoder 𝑔𝑑𝑒𝑐combines two ConvNeXt blocks, a pixel-shuffle layer to upsample\nthe feature map, and a linear layer. Our SigLIP encoder contains ∼300M parameters. 𝑔𝑒𝑛𝑐\nand 𝑔𝑑𝑒𝑐are light-weight modules, containing ∼70M parameters in total. Table 1 details the\nhyperparameters for each training stage. In the alignment regularization, we use the LLM\nfeatures after the 6th block as 𝑞𝜃(𝑧𝑡) and a three-layer MLP as ℎ𝜑. We employ an exponential\nmoving average (EMA) with a ratio of 0.99 to ensure training stability.\nFor data preprocessing, we deal with understanding and generation data differently. For\nunderstanding tasks, we maintain all image information by resizing the long side to the target\nsize and padding the image to squares. For generation tasks, we resize the short side to the\ntarget size and apply random square cropping to avoid padding artifacts. During training,\nmultiple sequences are packed to form a single sequence of length 4, 096 for training efficiency.\nOur implementation is based on the HAI-LLM platform [31] using PyTorch [74]. Training was\nconducted on NVIDIA A100 GPUs, with each model requiring ∼1, 600 A100 GPU days.\n4.2. Training Data Settings\nWe follow Janus [97] to construct the training data. The data configuration for each training\nstage is listed below.\nData for Stage 1 and Stage 2.\nThe first two stages of our framework uses three types of data:\nmultimodal understanding data, image generation data and text-only data.\n1. Multimodal Understanding Data. This type of data contains several sub-categories: (a)\nImage caption data. We incorporate caption datasets from [20, 41, 50, 51, 53, 82] and\ngenerate additional captions for images from [16, 43] using open-source multimodal\nunderstanding models. The names of the datasets are provided in the supplementary\nmaterials. The data follows template formats, e.g., “<image>Generate the caption\nof this picture. <caption>”. (b) Charts and tables. We directly adopt the chart\nand table data from the training data of DeepSeek-VL [64]. (c) Task data. ShareGPT4V\n[11] data is utilized to facilitate basic question-answering capabilities during pre-training,\n1This version, trained on an expanded text corpus compared to the one in Janus [97], has been demonstrated\nto possess better performance on multiple-choice benchmarks (e.g., MMBench [63] and SEED Bench [46]). Our\npreliminary experiments suggest that it has minimal impact on the quality of visual generation.\n8\nTable 2 | Performances on GenEval benchmark. “Gen.” denotes “generation” and “Unified”\ndenotes unified understanding and generation models. Models using external pre-trained\ngenerative models are signed with †.\nType\nMethod\nParams\nSingle Obj.\nTwo Obj.\nCount.\nColors\nPos.\nColor Attri.\nOverall↑\nGen. Only\nLlamaGen [86]\n0.8B\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nLDM [77]\n1.4B\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 [77]\n0.9B\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nPixArt-𝛼[9]\n0.6B\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nSDv2.1 [77]\n0.9B\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nDALL-E 2 [76]\n6.5B\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nEmu3-Gen [95]\n8B\n0.98\n0.71\n0.34\n0.81\n0.17\n0.21\n0.54\nSDXL [73]\n2.6B\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nIF-XL [17]\n4.3B\n0.97\n0.74\n0.66\n0.81\n0.13\n0.35\n0.61\nDALL-E 3 [6]\n-\n0.96\n0.87\n0.47\n0.83\n0.43\n0.45\n0.67\nUnified\nChameleon [88]\n34B\n-\n-\n-\n-\n-\n-\n0.39\nLWM [59]\n7B\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nSEED-X† [27]\n17B\n0.97\n0.58\n0.26\n0.80\n0.19\n0.14\n0.49\nShow-o [100]\n1.3B\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n0.53\nJanus [97]\n1.3B\n0.97\n0.68\n0.30\n0.84\n0.46\n0.42\n0.61\nTransfusion [108]\n7.3B\n-\n-\n-\n-\n-\n-\n0.63\nJanusFlow (Ours)\n1.3B\n0.97\n0.59\n0.45\n0.83\n0.53\n0.42\n0.63\nstructured as “<image><question><answer>”. (d) Interleaved text-image data. This\nsub-category is sourced from [42, 84].\n2. Image Generation Data. Our image generation dataset combines high-quality images from\n[16, 21, 41, 43, 68, 71, 82, 85] and 2 million in-house data. We enhance them with machine-\ngenerated captions using multimodal understanding models. We filter the images in\n[16, 82] with aspect ratios and aesthetic scores, retaining approximately 20% of the original\ndatasets. 25% of the data contains single-sentence captions. These kind of data assist\nthe model to be able to process short prompts. All the data points are formatted as\n“<prompt><image>”.\n3. Text-Only Data. We directly use the text corpus of DeepSeek-LLM [7].\nData for Stage 3. The SFT stage also uses three types of data:\n1. Multimodal Instruction Data. We leverage the instruction tuning datasets from [29, 33,\n35, 47, 65, 80].\n2. Image Generation Data. We reformat the high-quality text-image pairs from [16, 82, 85]\ninto an instruction format: “User:<user prompt>\\n\\n Assistant:<image>”.\n3. Text-Only Data. We directly incorporate the text-only data from [47].\n4.3. Evaluation Settings\nImage Generation.\nWe evaluate the generated images using both visual quality and semantic\naccuracy metrics. For visual quality assessment, we employ the Fréchet Inception Distance\n[30] (FID) metric and compute FID between 30,000 generated images and their corresponding\nreference images from the MJHQ dataset [48]. The FID computation follows the implementation\nfrom GigaGAN [39]. To evaluate semantic accuracy, we utilize two specialized frameworks:\nGenEval [28] and DPG-Bench [34]. These frameworks are designed to assess whether the\n9\nTable 3 | Performances on DPG-Bench. The methods in this table are all generation-specific\nmodels except our method.\nMethod\nGlobal\nEntity\nAttribute\nRelation\nOther\nOverall↑\nSDv1.5 [77]\n74.63\n74.23\n75.39\n73.49\n67.81\n63.18\nPixArt-𝛼[9]\n74.97\n79.32\n78.60\n82.57\n76.96\n71.11\nLumina-Next [110]\n82.82\n88.65\n86.44\n80.53\n81.82\n74.63\nSDXL [73]\n83.27\n82.43\n80.91\n86.76\n80.41\n74.65\nPlayground v2.5 [48]\n83.06\n82.59\n81.20\n84.08\n83.50\n75.47\nHunyuan-DiT [54]\n84.59\n80.59\n88.01\n74.36\n86.41\n78.87\nPixArt-Σ [10]\n86.89\n82.89\n88.94\n86.59\n87.68\n80.54\nEmu3-Gen [95]\n85.21\n86.68\n86.84\n90.22\n83.15\n80.60\nJanusFlow (Ours)\n87.03\n87.31\n87.39\n89.79\n88.10\n80.09\ngenerated images accurately contain the objects and relationships specified in the input prompts,\nproviding a broad evaluation of the generation capabilities.\nMultimodal Understanding.\nWe evaluate JanusFlow’s multimodal understanding abilities\nacross a diverse set of vision-language benchmarks for general understanding capabilities,\nincluding POPE [52], MME [24], MMBench [63], SEEDBench [46], VQAv2 [29], GQA [35], MM-\nVet [104], MMMU [105], ChartQA[70] and TextVQA[81]\n4.4. Quantitative Results\nTable 4 | Results of MJHQ FID-\n30k.\nThe models which have\nsimilar scales to our model are\nmarked with blue background.\nJanusFlow achieves the best FID\namong 1.3B models.\nMethod\nParams\nFID↓\nLWM [59]\n7B\n17.77\nVILA-U 256 [99]\n7B\n12.81\nVILA-U 384 [99]\n7B\n7.69\nShow-o [100]\n1.3B\n15.18\nJanus [97]\n1.3B\n10.10\nJanusFlow (Ours)\n1.3B\n9.51\nImage Generation Performances. We report the perfor-\nmances on GenEval, DPG-Bench and MJHQ FID-30k. In\nTab. 2, we give comparisons on GenEval including the scores\nof all the sub-tasks and the overall score. JanusFlow achieves\nan overall score of 0.63, surpassing the previous unified\nframework and several generation specific models includ-\ning SDXL [73] and DALL-E 2 [76]. In Tab. 3, We show results\non DPG-Bench and the corresponding comparisons. It is\nnoted that all the methods in Tab. 3 are generation-specific\nmodels except our model. The results on GenEval and DPG-\nBench demonstrate the ability of instruction following of our\nmodel. We give the comparisons on MJHQ FID-30k in Tab. 4.\nThe images which are sampled to calculate FID are gener-\nated with a CFG factor 𝑤= 2 and a number of sampling\nsteps 30. We sweep the CFG factor and the sampling steps\nand provide the results in the appendix. Our method achieves the best performance among\nall the models with 1.3B LLM. The results prove that the rectified flow is able to improve the\nquality of generated images over autoregressive models such as Janus [97].\nMultimodal Understanding Performances. We show comparisons of our method and other\nmethods including understanding-specific models and unified understanding and generation\nmodels in Tab. 5. Our model reaches the best performances among all the models with similar\nnumber of parameters and even surpasses multiple understanding-specific methods with larger\nscales. Our results demonstrate that our method harmonizes autoregressive LLM and rectified\nflow, achieving satisfying performance in both understanding and generation.\n10\nTable 5 | Comparison with other methods on multimodal understanding benchmarks. “Und.”\ndenotes “understanding” and “Unified” denotes unified understanding and generation models.\nThe models employing external pre-trained generative models are marked with †. The models\nwith LLMs which have similar number of parameters to us are marked with blue background\nunder the line of dashes.\nType\nModel\nLLM Param\nPOPE\nMME-P\nMMBdev\nSEED\nVQAv2test\nGQA\nMMMU\nMM-Vet\nChartQA\nTextVQA\nUnd. Only\nMobileVLM [12]\n2.7B\n84.9\n1288.9\n59.6\n-\n-\n59.0\n-\n-\n-\n47.5\nMobileVLM-V2 [13]\n2.7B\n84.7\n1440.5\n63.2\n-\n-\n61.1\n-\n-\n-\n57.5\nLLaVA-Phi [109]\n2.7B\n85.0\n1335.1\n59.8\n-\n71.4\n-\n-\n28.9\n-\n48.6\nLLaVA [58]\n7B\n76.3\n809.6\n38.7\n33.5\n-\n-\n-\n25.5\n-\n-\nLLaVA-v1.5 [56]\n7B\n85.9\n1510.7\n64.3\n58.6\n78.5\n62.0\n35.4\n31.1\n-\n58.2\nInstructBLIP [15]\n7B\n-\n-\n36.0\n53.4\n-\n49.2\n-\n26.2\n-\n50.1\nQwen-VL-Chat [4]\n7B\n-\n1487.5\n60.6\n58.2\n78.2\n57.5\n-\n-\n66.3\n61.5\nLLaVA-NeXT [57]\n7B\n-\n1519.3\n-\n-\n-\n-\n35.1\n-\n54.8\n-\nQwen2-VL [94]\n7B\n-\n-\n-\n-\n-\n-\n54.1\n62.0\n83.0\n84.3\nIDEFICS-9B [44]\n8B\n-\n-\n48.2\n-\n50.9\n38.4\n-\n-\n-\n25.9\nEmu3-Chat [95]\n8B\n85.2\n-\n58.5\n68.2\n75.1\n60.3\n31.6\n-\n68.6\n64.7\nInstructBLIP [15]\n13B\n78.9\n1212.8\n-\n-\n-\n49.5\n-\n25.6\n-\n50.7\nLLaVA-v1.5-Phi-1.5 [100]\n1.3B\n84.1\n1128.0\n-\n-\n75.3\n56.5\n30.7\n-\n-\n-\nMobileVLM [12]\n1.4B\n84.5\n1196.2\n53.2\n-\n-\n56.1\n-\n-\n-\n41.5\nMobileVLM-V2 [13]\n1.4B\n84.3\n1302.8\n57.7\n-\n-\n59.3\n-\n-\n-\n52.1\nUnified\nGemini-Nano-1 [89]\n1.8B\n-\n-\n-\n-\n62.7\n-\n26.3\n-\n53.6\n62.5\nLWM [59]\n7B\n75.2\n-\n-\n-\n55.8\n44.8\n-\n9.6\n-\n-\nVILA-U [99]\n7B\n85.8\n1401.8\n-\n59.0\n79.4\n60.8\n-\n33.5\n-\n60.8\nChameleon [88]\n7B\n-\n-\n-\n-\n-\n-\n22.4\n8.3\n-\n-\nDreamLLM† [19]\n7B\n-\n-\n-\n-\n72.9\n-\n-\n36.6\n-\n41.8\nLaVIT† [37]\n7B\n-\n-\n-\n-\n66.0\n46.8\n-\n-\n-\n-\nEmu† [87]\n13B\n-\n-\n-\n-\n52.0\n-\n-\n-\n-\n-\nNExT-GPT† [98]\n13B\n-\n-\n-\n-\n66.7\n-\n-\n-\n-\n-\nShow-o [100]\n1.3B\n73.8\n948.4\n-\n-\n59.3\n48.7\n25.1\n-\n-\n-\nJanus [97]\n1.3B\n87.0\n1338.0\n69.4\n63.7\n77.3\n59.1\n30.5\n34.3\n-\n-\nJanusFlow (Ours)\n1.3B\n88.0\n1333.1\n74.9\n70.5\n79.8\n60.3\n29.3\n30.9\n64.6\n55.5\nTable 6 | Ablation studies. The weights of the modules with † are frozen during training. “Exp.”\ndenotes “experiment”. “FID” in this table is MJHQ FID-10k with CFG factor 𝑤= 7.5 and 30\nsteps. “CLIP” denotes CLIP similarity with the backbone of CLIP-ViT-Large-Patch\/14. Exp. F is\nthe final configuration for training JanusFlow.\nExp. ID\nModel Setting\nTrain. Iter.\nEvaluation Benchmarks\nREPA\nUnd. Modules\nGen. Modules\nType\nPOPE↑VQAv2𝑣𝑎𝑙↑GQA↑FID↓CLIP ↑\nA\n×\nSigLIP\nVAE†+ConvNeXt\nUnified\n50,000\n82.40\n69.62\n54.43\n19.84\n24.94\nB\n✓\nShared VAE†+ConvNeXt\nUnified\n50,000\n78.13\n53.94\n44.04\n18.05\n26.38\nC\n✓\nVAE+ConvNeXt VAE†+ConvNeXt\nUnified\n50,000\n75.30\n55.41\n44.44\n17.53\n26.32\nD\n✓\nSigLIP\n-\nUnd. Only\n13,000\n85.03\n69.10\n54.23\n-\n-\nE\n✓\n-\nVAE†+ConvNeXt Gen. Only\n37,000\n-\n-\n-\n16.69\n26.89\nF\n✓\nSigLIP\nVAE†+ConvNeXt\nUnified\n50,000\n84.73\n69.20\n54.83\n17.61\n26.40\n4.5. Ablation Studies\nWe conduct comprehensive ablation studies to validate the effectiveness of our key design\nchoices. For computational efficiency, all ablation experiments are performed on 256 × 256\nresolution images2. All models are trained on our unified pre-training dataset for 50, 000\niterations, except for the understanding-only and generation-only variants, which are trained\nfor proportionally fewer iterations based on their respective data ratios in the pre-training phase.\nThe quantitative results of these ablation studies are presented in Tab. 6.\nImpact of Representation Alignment. The comparison between Exp. A and F demonstrates\nthe significant benefits of incorporating representation alignment regularization [103] during\ntraining. Specifically, models trained with representation alignment show notably lower FID\n2The understanding encoders in the 256 × 256-based ablation studies is also SigLIP-Large-Patch\/16 which is\npre-trained on 256 × 256 images.\n11\nA corgi’s head depicted as an explosion of a \nnebula, with vibrant cosmic colors like deep \npurples, blues, and pinks swirling around. \nThe corgi’s fur blends seamlessly into the \nnebula, with stars and galaxies forming the \ntexture of its fur. Bright bursts of light \nemanate from its eyes, and faint \nconstellations can be seen in the background, \ngiving the image a surreal, otherworldly feel.\nBeautiful surreal symbolism the \nmesmerizing vision of a Cleopatra Queen \nof Egypt, mesmerizing brown eyes, black \nhair and ethereal features, radiating \ncelestial aura, super high definition, true \nlifelike color, perfect exposure, razor sharp \nfocus, golden ratio, soft reflections, bokeh \neffect, fine art photography, cinematic \ncompositing, authentic, professional.\nA lone figure in dark robes ascends \nworn stone steps toward a glowing light \nin an ancient temple entrance. Ornate \narches, lush greenery, and intricate \ncarvings adorn the scene, evoking a \nmystical, high-fantasy atmosphere \nreminiscent of works by artists like \nRandy Vargas, with cinematic lighting \nand epic storytelling.\nFigure 4 | Image generation results of JanusFlow. Our model can generate high-quality images\nthat are semantically consistent with text prompts.\nscores on MJHQ dataset and higher CLIP scores, indicating simultaneous improvements in both\nimage quality and semantic alignment. Importantly, our architecture differs from previous stud-\nies [66, 72] examined in [103] due to our incorporation of LLM and an additional skip connection\nbetween 𝑔𝑒𝑛𝑐and 𝑔𝑑𝑒𝑐. The effectiveness of representation alignment in our modified architecture\nsuggests its broad applicability and generalization capability across different network structures.\nImpact of Decoupling Visual Encoders. e efficacy of using powerful pre-trained visual encoders\nin multimodal understanding. The comparison among Exp. B, C, and F demonstrates the\nadvantages of using separate visual encoders for understanding and generation tasks. In\nExp. B, following a design similar to Transfusion [108], we implement shared ConvNeXt\nblocks in the SDXL-VAE latent space for both understanding and generation encoders. Exp.\nC employs separate encoders with identical architectures and initialization parameters, but\ntrained independently. The performance differences between these configurations validate the\nnecessity of decoupled visual encoders in improving our unified model’s capabilities. Moreover,\nthe superior results in Exp. C and F highlight the benefits of leveraging pre-trained semantic\nvisual encoders for multimodal understanding tasks.\nFair Comparison with Understanding \/ Generation-Only Models. To establish meaningful\nbenchmarks, we evaluate task-specific models trained under identical conditions - using the\nsame pre-training dataset, infrastructure, and hyperparameters. Exp. D and E represent these\nspecialized models, trained with data volumes matching the unified models in Tab. 6. The\nminimal performance gap between Exp. F and these task-specific baselines demonstrates\nthat our unified framework successfully integrates understanding and generation capabilities\nwithout significant compromise in either task’s performance.\n4.6. Qualitative Results\nWe present qualitative evaluations of our method for both image generation and understanding\ntasks. Fig. 1(b) and Fig. 4 showcases the image generation capabilities of JanusFlow. These results\ndemonstrate both the high visual quality of our generated images and our framework’s ability\n12\nUser: What are the kinds of fruits in this picture?\nJaunsFlow (Ours): The fruits in the picture are banana, \nstrawberry, mango, persimmon, blueberry, and lime.\nUser: Describe this image.\nJaunsFlow (Ours): Kid's \nFavourite Fruits is a bar graph. \nThe x-axis shows the fruits. The \ny-axis shows the number of kids \nwho like the fruit. The orange is \nthe most popular fruit. The \nbanana is the least popular fruit.\nUser: How many penguins in the picture?\nJaunsFlow (Ours): There are 11 penguins \nin the picture.\nFigure 5 | Visual Understanding with JanusFlow. Our model effectively handles various visual\nunderstanding tasks, such as question answering, plot interpretation and object counting.\nto faithfully follow diverse instructions. For multimodal understanding, Fig. 5 presents example\nconversations that show our model’s understanding capabilities across various scenarios. These\ninteractions demonstrate the model’s ability to understand and reason about visual content in\nnatural language dialogues. Additional qualitative examples showcasing the versatility and\neffectiveness of JanusFlow are provided in the appendix.\n5. Conclusion\nWe present JanusFlow, a unified framework that successfully harmonizes autoregressive and\nrectified flow models for multimodal understanding and generation tasks. Our extensive\nexperiments demonstrate that this unification achieves comparable performance to task-specific\nmodels. The successful integration of these fundamentally different model architectures not\nonly addresses current challenges in multimodal learning but also opens new possibilities for\nfuture research in training unified models.\n13\nReferences\n[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,\nJ. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch,\nK. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning.\nIn Proc. Annu. Conf. Neural Inf. Process. Systems, 2022.\n[3] M. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic inter-\npolants. In Proc. Int’l Conf. Learning Representations, 2023.\n[4] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-\nVL: A frontier large vision-language model with versatile abilities.\narXiv preprint\narXiv:2308.12966, 2023.\n[5] F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu. All are worth words: A ViT backbone\nfor diffusion models. In Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition,\n2023.\n[6] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo,\net al. Improving image generation with better captions. Computer Science, 2023.\n[7] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al.\nDeepSeek LLM: Scaling open-source language models with longtermism. arXiv preprint\narXiv:2401.02954, 2024.\n[8] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,\nY. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with\nGPT-4. arXiv preprint arXiv:2303.12712, 2023.\n[9] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. PixArt-\nalpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023.\n[10] J. Chen, C. Ge, E. Xie, Y. Wu, L. Yao, X. Ren, Z. Wang, P. Luo, H. Lu, and Z. Li. PixArt-\nSigma: Weak-to-strong training of diffusion transformer for 4K text-to-image generation.\narXiv preprint arXiv:2403.04692, 2024.\n[11] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. ShareGPT4V: Im-\nproving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793,\n2023.\n[12] X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei, et al.\nMobileVLM: A fast, reproducible and strong vision language assistant for mobile devices.\narXiv preprint arXiv:2312.16886, 2023.\n[13] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, et al.\nMobileVLM V2: Faster and stronger baseline for vision language model. arXiv preprint\narXiv:2402.03766, 2024.\n[14] K. Crowson, S. A. Baumann, A. Birch, T. M. Abraham, D. Z. Kaplan, and E. Shippole. Scal-\nable high-resolution pixel-space image synthesis with hourglass diffusion transformers.\nIn Proc. Int’l Conf. Machine Learning, 2024.\n14\n[15] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. InstructBLIP:\nTowards general-purpose vision-language models with instruction tuning. In Proc. Annu.\nConf. Neural Inf. Process. Systems, 2023.\n[16] dclure. LAION-Aesthetics-UMAP, 2022. URL https:\/\/huggingface.co\/datasets\/\ndclure\/laion-aesthetics-12m-umap.\n[17] DeepFloyd. DeepFloyd IF, 2023. URL https:\/\/huggingface.co\/DeepFloyd\/IF-I\n-XL-v1.0.\n[18] P. Dhariwal and A. Nichol.\nDiffusion models beat GANs on image synthesis.\nIn\nProc. Annu. Conf. Neural Inf. Process. Systems, 2021.\n[19] R. Dong, C. Han, Y. Peng, Z. Qi, Z. Ge, J. Yang, L. Zhao, J. Sun, H. Zhou, H. Wei,\net al. DreamLLM: Synergistic multimodal comprehension and creation. In Proc. Int’l\nConf. Learning Representations, 2024.\n[20] echo840. Detailed caption, 2023. URL https:\/\/huggingface.co\/datasets\/echo84\n0\/Detailed_Caption.\n[21] B. Egan, A. Redden, XWAVE, and SilentAntagonist. DALLE-3 1 million+ high quality\ncaptions, 2024. URL https:\/\/huggingface.co\/datasets\/ProGamerGov\/synthe\ntic-dataset-1m-dalle3-high-quality-captions.\n[22] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image\nsynthesis. In Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2021.\n[23] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer,\nF. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In\nProc. Int’l Conf. Machine Learning, 2024.\n[24] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu,\nand R. Ji. MME: A comprehensive evaluation benchmark for multimodal large language\nmodels. arXiv preprint arXiv:2306.13394, 2024.\n[25] Y. Ge, Y. Ge, Z. Zeng, X. Wang, and Y. Shan. Planting a SEED of vision in large language\nmodel. arXiv preprint arXiv:2307.08041, 2023.\n[26] Y. Ge, S. Zhao, Z. Zeng, Y. Ge, C. Li, X. Wang, and Y. Shan. Making LLaMA SEE and draw\nwith SEED tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n[27] Y. Ge, S. Zhao, J. Zhu, Y. Ge, K. Yi, L. Song, C. Li, X. Ding, and Y. Shan. SEED-X: Multimodal\nmodels with unified multi-granularity comprehension and generation. arXiv preprint\narXiv:2404.14396, 2024.\n[28] D. Ghosh, H. Hajishirzi, and L. Schmidt. GenEval: An object-focused framework for\nevaluating text-to-image alignment. In Proc. Annu. Conf. Neural Inf. Process. Systems,\n2024.\n[29] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in VQA matter:\nElevating the role of image understanding in visual question answering. In Proc. IEEE\nInt’l Conf. Computer Vision and Pattern Recognition, 2017.\n[30] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a\ntwo time-scale update rule converge to a local nash equilibrium. Proc. Annu. Conf. Neural\nInf. Process. Systems, 2017.\n15\n[31] High-flyer. HAI-LLM: Efficient and lightweight training tool for large models, 2023. URL\nhttps:\/\/www.high-flyer.cn\/en\/blog\/hai-llm.\n[32] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Proc. Annu.\nConf. Neural Inf. Process. Systems, 2020.\n[33] Y.-C. Hsiao, F. Zubach, G. Baechler, V. Carbune, J. Lin, M. Wang, S. Sunkara, Y. Zhu, and\nJ. Chen. ScreenQA: Large-scale question-answer pairs over mobile app screenshots. arXiv\npreprint arXiv:2209.08199, 2022.\n[34] X. Hu, R. Wang, Y. Fang, B. Fu, P. Cheng, and G. Yu. ELLA: Equip diffusion models with\nllm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.\n[35] D. A. Hudson and C. D. Manning. GQA: A new dataset for real-world visual reasoning\nand compositional question answering. In Proc. IEEE Int’l Conf. Computer Vision and\nPattern Recognition, 2019.\n[36] Y. Jin, Z. Sun, N. Li, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and\nZ. Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint\narXiv:2410.05954, 2024.\n[37] Y. Jin, K. Xu, L. Chen, C. Liao, J. Tan, Q. Huang, C. Bin, C. Song, D. ZHANG, W. Ou, et al.\nUnified language-vision pretraining in llm with dynamic discrete visual tokenization. In\nProc. Int’l Conf. Learning Representations, 2024.\n[38] B. Jing, B. Berger, and T. Jaakkola. AlphaFold meets flow matching for generating protein\nensembles. In Proc. Int’l Conf. Machine Learning, 2024.\n[39] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. Scaling up\nGANs for text-to-image synthesis. In Proc. IEEE Int’l Conf. Computer Vision and Pattern\nRecognition, 2023.\n[40] S. Kim, K. Shih, J. F. Santos, E. Bakhturina, M. Desta, R. Valle, S. Yoon, B. Catanzaro, et al.\nP-Flow: a fast and data-efficient zero-shot tts through speech prompting. In Proc. Annu.\nConf. Neural Inf. Process. Systems, 2024.\n[41] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,\nA. C. Berg, W.-Y. Lo, et al. Segment anything. In Proc. IEEE Int. Conf. Comput. Vision,\n2023.\n[42] M. Koupaee and W. Y. Wang. WikiHow: A large scale text summarization dataset. arXiv\npreprint arXiv:1810.09305, 2018.\n[43] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov,\nM. Malloci, A. Kolesnikov, et al. The Open Images Dataset V4: Unified image classification,\nobject detection, and visual relationship detection at scale. Int’l Journal of Computer\nVision, 2020.\n[44] H. Laurençon, D. van Strien, S. Bekman, L. Tronchon, L. Saulnier, T. Wang, S. Karamcheti,\nA. Singh, G. Pistilli, Y. Jernite, et al. Introducing IDEFICS: An open reproduction of\nstate-of-the-art visual language model, 2023, 2023. URL https:\/\/huggingface.co\/b\nlog\/idefics.\n16\n[45] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi,\nJ. Mahadeokar, et al. VoiceBox: Text-guided multilingual universal speech generation at\nscale. In Proc. Annu. Conf. Neural Inf. Process. Systems, 2024.\n[46] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. SEED-Bench: Benchmarking multimodal\nllms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[47] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li.\nLLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024.\n[48] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi. Playground v2.5: Three\ninsights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint\narXiv:2402.17245, 2024.\n[49] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In Proc. Int’l Conf. Machine\nLearning, 2023.\n[50] L. Li, Y. Wang, R. Xu, P. Wang, X. Feng, L. Kong, and Q. Liu. Multimodal arXiv: A dataset\nfor improving scientific comprehension of large vision-language models. In Annual\nMeeting of the Association for Computational Linguistics, 2024.\n[51] X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan. DenseFusion-1M: Merging\nvision experts for comprehensive multimodal perception. In Proc. Annu. Conf. Neural\nInf. Process. Systems, 2024.\n[52] Y. Li, Y. Du, K. Zhou, J. Wang, X. Zhao, and J.-R. Wen. Evaluating object hallucination in\nlarge vision-language models. In Proc. Conf. on Empirical Methods in Natural Language\nProcess., 2023.\n[53] Z. Li, X. Yang, K. Choi, W. Zhu, R. Hsieh, H. Kim, J. H. Lim, S. Ji, B. Lee, X. Yan, et al.\nMMSci: A multimodal multi-discipline dataset for phd-level scientific comprehension. In\nAI for Accelerated Materials Design, 2024.\n[54] Z. Li, J. Zhang, Q. Lin, J. Xiong, Y. Long, X. Deng, Y. Zhang, X. Liu, M. Huang, Z. Xiao,\net al. Hunyuan-DiT: A powerful multi-resolution diffusion transformer with fine-grained\nchinese understanding. arXiv preprint arXiv:2405.08748, 2024.\n[55] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative\nmodeling. In Proc. Int’l Conf. Learning Representations, 2023.\n[56] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In\nProc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2024.\n[57] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. LLaVA-NeXT: Improved\nreasoning, OCR, and world knowledge, 2024. URL https:\/\/llava-vl.github.io\/\nblog\/2024-01-30-llava-next\/.\n[58] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In Proc. Annu. Conf. Neural\nInf. Process. Systems, 2024.\n[59] H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and\nlanguage with ringattention. arXiv preprint arXiv:2402.08268, 2024.\n17\n[60] Q. Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv\npreprint arXiv:2209.14577, 2022.\n[61] X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data\nwith rectified flow. In Proc. Int’l Conf. Learning Representations, 2023.\n[62] X. Liu, X. Zhang, J. Ma, J. Peng, et al. InstaFlow: One step is enough for high-quality\ndiffusion-based text-to-image generation. In Proc. Int’l Conf. Learning Representations,\n2024.\n[63] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu,\net al. MMBench: Is your multi-modal model an all-around player? In Proc. European\nConf. Computer Vision, 2024.\n[64] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang,\net al. DeepSeek-VL: towards real-world vision-language understanding. arXiv preprint\narXiv:2403.05525, 2024.\n[65] P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu. IconQA: A\nnew benchmark for abstract diagram understanding and visual language reasoning. In\nProc. Annu. Conf. Neural Inf. Process. Systems, 2021.\n[66] N. Ma, M. Goldstein, M. S. Albergo, N. M. Boffi, E. Vanden-Eijnden, and S. Xie. SiT: Explor-\ning flow and diffusion-based generative models with scalable interpolant transformers.\narXiv preprint arXiv:2401.08740, 2024.\n[67] Y. Ma, H. Yang, W. Wang, J. Fu, and J. Liu. Unified multi-modal latent diffusion for joint\nsubject and text conditional image generation. arXiv preprint arXiv:2303.09319, 2023.\n[68] madebyollin. Megalith-10M, 2024. URL https:\/\/huggingface.co\/datasets\/made\nbyollin\/megalith-10m.\n[69] B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[70] A. Masry, X. L. Do, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question\nanswering about charts with visual and logical reasoning. In Annual Meeting of the\nAssociation for Computational Linguistics, 2022.\n[71] mehdidc. YFCC-15M, 2024. URL https:\/\/huggingface.co\/datasets\/mehdidc\/yf\ncc15m.\n[72] W. Peebles and S. Xie.\nScalable diffusion models with transformers.\nIn Proc. IEEE\nInt. Conf. Comput. Vision, 2023.\n[73] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and\nR. Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis.\nIn Proc. Int’l Conf. Learning Representations, 2024.\n[74] PyTorch-Contributors. PyTorch, 2024. URL https:\/\/pytorch.org.\n[75] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language\nsupervision. In Proc. Int’l Conf. Machine Learning, 2021.\n18\n[76] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional\nimage generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.\n[77] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proc. IEEE Int’l Conf. Computer Vision and\nPattern Recognition, 2022.\n[78] L. Ruan, Y. Ma, H. Yang, H. He, B. Liu, J. Fu, N. J. Yuan, Q. Jin, and B. Guo. MM-Diffusion:\nLearning multi-modal diffusion models for joint audio and video generation. In Proc. IEEE\nInt’l Conf. Computer Vision and Pattern Recognition, 2022.\n[79] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-\ntijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. In Proc. Annu. Conf. Neural Inf. Process.\nSystems, 2022.\n[80] S. Shah, A. Mishra, N. Yadati, and P. P. Talukdar. KVQA: Knowledge-aware visual question\nanswering. In Proc. AAAI Conf. on Artificial Intelligence, 2019.\n[81] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach.\nTowards VQA models that can read. In Proc. IEEE Int’l Conf. Computer Vision and\nPattern Recognition, 2019.\n[82] V. Singla, K. Yue, S. Paul, R. Shirkavand, M. Jayawardhana, A. Ganjdanesh, H. Huang,\nA. Bhatele, G. Somepalli, and T. Goldstein. From pixels to prose: A large dataset of dense\nimage captions. arXiv preprint arXiv:2406.10328, 2024.\n[83] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-\nbased generative modeling through stochastic differential equations.\nIn Proc. Int’l\nConf. Learning Representations, 2021.\n[84] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork. WIT: Wikipedia-based\nimage text dataset for multimodal multilingual machine learning. In Proc. ACM SIGIR\nConf. Research and Develop. in Info. Retrieval, 2021.\n[85] K. Sun, J. Pan, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang,\net al. JourneyDB: A benchmark for generative image understanding. In Proc. Annu.\nConf. Neural Inf. Process. Systems, 2024.\n[86] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model\nbeats diffusion: LLaMA for scalable image generation. arXiv preprint arXiv:2406.06525,\n2024.\n[87] Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang.\nGenerative pretraining in multimodality. In Proc. Int’l Conf. Learning Representations,\n2024.\n[88] C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint\narXiv:2405.09818, 2024.\n[89] G. Team.\nGemini: a family of highly capable multimodal models.\narXiv preprint\narXiv:2312.11805, 2023.\n19\n[90] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer,\nX. Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms.\narXiv preprint arXiv:2406.16860, 2024.\n[91] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière,\nN. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[92] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. LLaMA 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[93] C. N. Vasconcelos, A. Rashwan, A. Waters, T. Walker, K. Xu, J. Yan, R. Qian, Y. Li, S. LUO,\nY. Onoe, et al. Greedy growing enables high-resolution pixel-based diffusion models.\nTransactions on Machine Learning Research, 2024.\n[94] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al.\nQwen2-VL: Enhancing vision-language model’s perception of the world at any resolution.\narXiv preprint arXiv:2409.12191, 2024.\n[95] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al.\nEmu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024.\n[96] S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie. ConvNeXt v2:\nCo-designing and scaling ConvNets with masked autoencoders.\nIn Proc. IEEE Int’l\nConf. Computer Vision and Pattern Recognition, 2023.\n[97] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan, et al. Janus:\nDecoupling visual encoding for unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024.\n[98] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua. NExT-GPT: Any-to-any multimodal LLM. In\nProc. Int’l Conf. Machine Learning, 2024.\n[99] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, et al.\nVILA-U: A unified foundation model integrating visual understanding and generation.\narXiv preprint arXiv:2409.04429, 2024.\n[100] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and\nM. Z. Shou. Show-o: One single transformer to unify multimodal understanding and\ngeneration. arXiv preprint arXiv:2408.12528, 2024.\n[101] H. Ye, D.-A. Huang, Y. Lu, Z. Yu, W. Ping, A. Tao, J. Kautz, S. Han, D. Xu, P. Molchanov,\net al.\nX-VILA: Cross-modality alignment for large language model.\narXiv preprint\narXiv:2405.19335, 2024.\n[102] L. Yu, J. Lezama, N. B. Gundavarapu, L. Versari, K. Sohn, D. Minnen, Y. Cheng, A. Gupta,\nX. Gu, A. G. Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual\ngeneration. In Proc. Int’l Conf. Learning Representations, 2024.\n[103] S. Yu, S. Kwak, H. Jang, J. Jeong, J. Huang, J. Shin, and S. Xie. Representation alignment\nfor generation: Training diffusion transformers is easier than you think. arXiv preprint\narXiv:2410.06940, 2024.\n20\n[104] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. MM-Vet: Evaluating\nlarge multimodal models for integrated capabilities. In Proc. Int’l Conf. Machine Learning,\n2024.\n[105] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al.\nMMMU: A massive multi-discipline multimodal understanding and reasoning benchmark\nfor expert AGI. In Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, 2024.\n[106] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image\npre-training. In Proc. IEEE Int. Conf. Comput. Vision, 2023.\n[107] C. Zhao, Y. Song, W. Wang, H. Feng, E. Ding, Y. Sun, X. Xiao, and J. Wang. MonoFormer:\nOne transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280,\n2024.\n[108] C. Zhou, L. Yu, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettle-\nmoyer, and O. Levy. Transfusion: Predict the next token and diffuse images with one\nmulti-modal model. arXiv preprint arXiv:2408.11039, 2024.\n[109] Y. Zhu, M. Zhu, N. Liu, Z. Ou, X. Mou, and J. Tang. LLaVA-Phi: Efficient multi-modal\nassistant with small language model. arXiv preprint arXiv:2401.02330, 2024.\n[110] L. Zhuo, R. Du, H. Xiao, Y. Li, D. Liu, R. Huang, W. Liu, L. Zhao, F.-Y. Wang, Z. Ma, et al.\nLumina-Next: Making Lumina-T2X stronger and faster with Next-DiT. arXiv preprint\narXiv:2406.18583, 2024.\n21\nAppendix\nA. Performance Analysis of 256 Resolution Model\nWe trained our model at two resolutions: 256 × 256 and 384 × 384. The main paper presents\nresults from the 384 × 384 model as our primary results. Here, we provide a comprehensive\nevaluation of the 256 × 256 model’s performance. The visual understanding performances\nare presented in Tab. 1. The generation capabilities are evaluated using GenEval [28], DPG-\nBenchmark [34], and MJHQ FID-30k [48], with results shown in Tab. 2 and 3.\nTable 1 | Results on visual understanding tasks.\nModel\nLLM Params POPE↑MME-P↑MMB𝑑𝑒𝑣↑SEED↑VQAv2𝑡𝑒𝑠𝑡↑GQA↑MM-Vet↑\nJanusFlow 256\n1.3B\n85.3\n1203.0\n71.9\n67.6\n76.3\n58.4\n27.4\nJanusFlow 384\n1.3B\n88.0\n1333.1\n74.9\n70.5\n79.8\n60.3\n30.9\nTable 2 | Results on GenEval [28].\nMethod\nLLM Params\nSingle Obj.\nTwo Obj.\nCount.\nColors\nPos.\nColor Attri.\nOverall↑\nJanusFlow 256\n1.3B\n0.98\n0.73\n0.54\n0.83\n0.63\n0.53\n0.70\nJanusFlow 384\n1.3B\n0.97\n0.59\n0.45\n0.83\n0.53\n0.42\n0.63\nTable 3 | Results on DPG-Bench [34] and MJHQ FID-30k [48].\nMethod\nDPG-Bench↑\nMJHQ FID-30k↓\nGlobal\nEntity\nAttribute\nRelation\nOther\nOverall\nJanusFlow 256\n91.20\n88.83\n88.00\n87.60\n89.53\n81.23\n12.70\nJanusFlow 384\n87.03\n87.31\n87.39\n89.79\n88.10\n80.09\n9.51\nAs expected, the 256×256 model shows slightly lower performance compared to the 384×384\nmodel on visual understanding metrics due to its reduced resolution. Interestingly, however,\nthe 256 × 256 model outperforms its higher-resolution counterpart on GenEval and DPG-Bench -\nbenchmarks specifically designed to evaluate instruction following capabilities and semantic\naccuracy. This superior performance on semantic tasks can be attributed to the model’s better\ncontrol over lower-resolution images, where reduced visual complexity allows for more precise\nsemantic manipulation.\nB. Details of the Datasets\nThe datasets used in the pre-training stage for understanding include DetailedCaption [20],\nSAM [41], arXivQA[50], DenseFusion-1M [51], MMSci[53], PixelProse [82], re-captioned LAION-\nAesthetics [16], re-captioned Open Images V4 [43], ShareGPT4V [11], WikiHow [42] and\nWIT [84]. The datasets used in the pre-training stage for generation include re-captioned\nLAION-Aesthetics [16], DALL-E 3 1M [21], SAM [41], Open Images V4 [43], Megalith-10M [68],\nYFCC-15M [71], PixelProse[82] and JourneyDB [85].\n22\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCFG Factor\n10\n11\n12\n13\nMJHQ FID-30k\n24.0\n24.5\n25.0\n25.5\n26.0\n26.5\n27.0\n27.5\n28.0\nCLIP Similarity\n(a) Results of varying CFG Factors\n25\n30\n35\n40\n45\n50\nNumber of Sampling Step\n9.40\n9.45\n9.50\n9.55\n9.60\n9.65\n9.70\nMJHQ FID-30k\n26.615\n26.620\n26.625\n26.630\n26.635\n26.640\nCLIP Similarity\n(b) Results of Varying Numbers of Sampling Steps\nFigure 1 | Results of varying CFG factors and numbers of sampling steps. In Fig. (a), the number of\nsampling steps is set to 30. In Fig. (b), the CFG factor is set to 2.\n0\n10\n20\n30\n40\n50\nTraining Iterations \/ K Iteration\n20\n40\n60\n80\n100\nMJHQ FID-30k\n18\n20\n22\n24\n26\nCLIP Similarity\nw\/ REPA\nw\/o REPA\nFigure 2 | The FID and CLIP similarity during the first 50,000 iterations.\nC. Analysis of CFG Factor and Sampling Steps\nWe investigate the impact of two key generation parameters: the Classifier-Free Guidance (CFG)\nfactor and the number of sampling steps. While our main results use 𝑤= 2 for CFG and 30\nsampling steps to calculate FID, here we present a comprehensive analysis of these hyperparam-\neters. Fig. 1(a) shows the effect of varying CFG factors while maintaining 30 sampling steps.\nThe results reveal an optimal CFG value for FID scores, while CLIP [75] similarity continues to\nimprove with increasing CFG values, consistent with findings from previous work [73]. Fig. 1(b)\ndemonstrates the impact of different sampling steps while maintaining a CFG factor of 2. The\nnumber of sampling steps shows relatively minor influence on performance. Our choice of 30\nsteps in the main paper represents a balance between generation quality and computational\nefficiency.\n23\nD. Details of REPA Ablation\nWe provide the FID and CLIP similarity of the first 50,000 training iterations of the pre-train\nstage in Fig. 2 with and without representation alignment regularization. The gap between the\ntwo models demonstrates the benefits of using representation alignment regularization.\nE. Additional Qualitative Results\nAdditional qualitative examples for both understanding and generation tasks are presented in\nFig. 3 and Fig. 4, respectively. The understanding examples demonstrate JanusFlow’s diverse\ncapabilities, including code generation, person identification, character recognition, and visual\nreasoning. For image generation, our model exhibits strong performance in both visual quality\nand semantic alignment with input prompts.\nUser: Please write a python code to draw this plot.\nJanusFlow: Here is the Python code to draw this plot:\n```python\nimport matplotlib.pyplot as plt\n# Data\nvalues = [3, 7, 5, 2]\n# Create the bar chart\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(values)), values)\n# Title\nplt.title('Simple Bar Chart')\n# Labels\nplt.xlabel('Categories')\nplt.ylabel('Values')\n# Show the plot\nplt.show()\n```\nUser: Explain why this image is funny.\nJanusFlow: This image is likely to be a \nhumorous or artistic rendition of the famous \npainting \"The Mona Lisa\" with a dog \ninstead of a human. The dog's expression \nand pose are similar to the iconic pose of \nthe original painting, which adds a playful \nand unexpected twist to the artwork.\nUser: What are the texts in this passage?\nJanusFlow: To increase the accessibility \nof this powerful model class and at the \nsame time reduce its significant resource \nconsumption, a method is needed that \nreduces the computational complexity for \nboth training and sampling. Reducing the \ncomputational demands of DMs without \nimpairing their performance is, therefore, \nkey to enhance their accessibility.\nUser: Who is this in the figure?\nJanusFlow: George W. Bush.\nFigure 3 | More multimodal understanding cases.\n24\nMassive cathedral church, battle between \nHeaven and hell, church on fire, 8k hyper real \nultra sharp renaissance by Francisco Goya.\nA handsome 24-year-old boy in the middle \nwith sky color background wearing eye \nglasses, it's super detailed with anime style.\nHappy dreamy owl monster sitting on a tree \nbranch, colorful glittering particles, forest \nbackground, detailed feathers.\nA man wearing Fedora hat with mafia style, \nrealistic photography, intricate details, magical \nlighting, vibrant background, complex textures, \nrich colors, realistic style, front-facing view.\nA vivid depiction of the Northern Lights \ndancing above the snow-covered \nmountains in Iceland, casting a \nmesmerizing glow across the sky.\nA dark, high-contrast render of a \npsychedelic Tree of Life glowing \nbrilliantly, illuminating swirling dust \nparticles in a mystical, cavernous setting.\nThe image features a mushroom growing on \ngrassy ground amidst fallen leaves. Their caps \nare light brownish-white with visible gills \nunderneath; the stems appear dark and sturdy. In \nthe background, there's an out-of-focus scene \nthat includes greenery and possibly some \nstructures or trees shrouded by mist or fog, \ngiving it a serene yet slightly eerie atmosphere. \nThis photograph employs shallow depth of field \nto emphasize the mushrooms while blurring the \nsurroundings for artistic effect.\nThe image captures a vast ocean view at \neither sunrise or sunset, with soft pink hues \nnear the horizon blending into darker clouds \nabove. Waves crash against rugged black \nrocks on the right, where water flows down \nonto smaller stones below. In the \nforeground, dry grass contrasts with the \nsmooth sea surface. The scene feels tranquil \nbut also reveals the raw power of nature \nthrough the interaction between the \ndynamic waves and the solid land.\nA serene Chinese ink painting depicts \na tranquil mountain village. Simple \nhomes nestle at the foot of misty \npeaks, while a gentle river winds \nthrough the village. Bamboo and pine \ntrees dot the landscape. The \nminimalist brushstrokes reflect a \nharmonious relationship between \nnature and human life, capturing the \npeaceful essence of the scene with \nelegant simplicity.\nFigure 4 | More text-to-image generation results.\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation.pdf"}
{"title":"On the Performance of Multimodal Language Models","authors":"Utsav Garg, Erhan Bas","summary":"Instruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating\nindependently pretrained vision encoders through model grafting. These\nmultimodal variants undergo instruction tuning, similar to LLMs, enabling\neffective zero-shot generalization for multimodal tasks. This study conducts a\ncomparative analysis of different multimodal instruction tuning approaches and\nevaluates their performance across a range of tasks, including complex\nreasoning, conversation, image captioning, multiple-choice questions (MCQs),\nand binary classification. Through rigorous benchmarking and ablation\nexperiments, we reveal key insights for guiding architectural choices when\nincorporating multimodal capabilities into LLMs. However, current approaches\nhave limitations; they do not sufficiently address the need for a diverse\nmultimodal instruction dataset, which is crucial for enhancing task\ngeneralization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current\nmethodological constraints in adapting language models for image comprehension\nand provide valuable guidance for researchers and practitioners seeking to\nharness multimodal versions of LLMs.","url":"http:\/\/arxiv.org\/abs\/2310.03211v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.03211v2","published":1696462416000,"comment":null,"pdf_text":"On the Performance of Multimodal Language Models\nUtsav Garg\nScale AI\nutsav.garg@scale.com\nErhan Bas∗\nGE HealthCare\nerhan.bas@gehealthcare.com\nAbstract\nInstruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating inde-\npendently pretrained vision encoders through model grafting. These multimodal\nvariants undergo instruction tuning, similar to LLMs, enabling effective zero-shot\ngeneralization for multimodal tasks. This study conducts a comparative analysis\nof different multimodal instruction tuning approaches and evaluates their perfor-\nmance across a range of tasks, including complex reasoning, conversation, image\ncaptioning, multiple-choice questions (MCQs), and binary classification. Through\nrigorous benchmarking and ablation experiments, we reveal key insights for guid-\ning architectural choices when incorporating multimodal capabilities into LLMs.\nHowever, current approaches have limitations; they do not sufficiently address the\nneed for a diverse multimodal instruction dataset, which is crucial for enhancing\ntask generalization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current method-\nological constraints in adapting language models for image comprehension and\nprovide valuable guidance for researchers and practitioners seeking to harness\nmultimodal versions of LLMs.\n1\nIntroduction\nLarge instruction-tuned language models (LLMs) have emerged as powerful models showcasing\nremarkable zero-shot generalization capabilities across a diverse spectrum of downstream tasks. By\nlearning to interpret natural language instructions, these models obviate the need for task-specific\ntraining. However, real-world applications, often involve multimodal data, such as images and text,\nnecessitating the combination of visual and linguistic information for accurate and robust inference.\nTo address this challenge, recent research Li et al. [2023], Ye et al. [2023], Zhu et al. [2023], Liu et al.\n[2023a] has introduced multimodal variants of LLMs, which integrate independently pretrained large\nvision encoders with LLMs. These models further undergo instruction tuning, aiming to leverage the\ncombined power of visual and linguistic understanding.\nIn this paper, we conduct a comprehensive investigation that centers on comparing various approaches\nto multimodal instruction tuning and assessing their performance on a wide array of downstream tasks.\nOur study seeks to illuminate the efficacy, generalization capabilities, and limitations of publicly\navailable models and their ablations across various domains. We evaluate the different approaches\nacross a diverse range of tasks from complex reasoning to captioning and classification, to test their\ngeneralization capabilities. Moreover, we aim to identify whether specific design choices prove more\nsuitable for specific tasks.\nFor our experiments, we consider five publicly available approaches for multimodal adaptation\nof LLMs: BLIP-2 Li et al. [2023], InstructBLIP Dai et al. [2023], LLaVA Liu et al. [2023a],\nMiniGPT4 Zhu et al. [2023] and mPLUG-Owl Ye et al. [2023]. These approaches encompass a wide\n∗work done while Erhan Bas was at Scale AI.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2310.03211v2  [cs.CL]  28 Nov 2023\ngamut of architectural choices for injecting LLMs with multimodal capabilities. More details about\nthese respective models is in the appendix A.1. By conducting thorough benchmarking and analysis,\nwe aim to identify the strengths and weaknesses of different multimodal instruction tuning strategies.\nFurthermore, we conduct ablation experiments on previously untested combinations within these\napproaches to unearth the optimal method for integrating visual capabilities into a language model.\nThis includes exploring the utility of the largest feasible frozen vision encoder, attaching a trained\nvision head to extract concise visual features, employing a linear mapping layer to project these into\nthe linguistic domain, and fine-tuning the decoder on multimodal data samples.\n2\nDatasets and Evaluation\nWe systematically assess all approaches across two primary task categories to comprehensively gauge\ntheir capabilities across a broad spectrum of challenges:\n1. Complex Reasoning: In this task category, the model confronts questions that necessitate\nexternal knowledge and cannot be resolved solely based on the image and question. This\ntest predominantly assesses the language model’s performance and illuminates the extent to\nwhich multimodal instruction tuning influences the base LLM’s regression.\n2. Captioning, Classification, Conversation, MCQ: These tasks become feasible for the\nLLM only through the incorporation of the visual encoder. The performance across these\ntasks serves as a robust indicator of the approach’s generalization prowess, encompassing\narchitectural choices and instruction tuning data.\nTo evaluate the general Visual Question Answering (VQA) abilities, we use the test set curated by\nLLaVA Liu et al. [2023a]. The dataset consists of 30 images from the COCO dataset, each associated\nwith three distinct question types: conversation, description, and complex reasoning types, resulting\nin a total of 90 instruction-answer pairs. The ground-truth answers for these questions are generated\nby GPT-4, which is given the captions and bounding boxes associated with the image for context.\nThe evaluation involves ranking both the predicted and reference answers on a scale of 1 to 10 by\nGPT-4. The metric is the relative score of the predicted answer to GPT-4’s answer. To address\npotential biases due to answer order, as demonstrated by Wang et al. [2023], we adopt the Balanced\nPosition Calibration Wang et al. [2023] (BPC) strategy proposed by them, averaging scores from\nboth possible orderings. Additionally, to account for the stochasticity of the generation, we average\nscores across 5 generations by each approach. Despite these considerations to make the evaluation as\nrobust as possible, there are still limitations as GPT-4 is not actually seeing the image and is instead\nusing a proxy for visual information. Moreover, we cannot expect GPT-4 evaluations to be perfect\nunderlining the current limitation of open-ended answer evaluations.\nFor captioning tasks, we evaluate on the val set of the NoCaps Agrawal et al. [2019] dataset and\nreport the CIDEr Vedantam et al. [2015] metric. For visual Multiple Choice Questions (MCQ), we\nevaluate on the test split of the ScienceQA Lu et al. [2022] dataset. Here, we exclusively consider\nsamples with available image context. For binary classification, we evaluate on the Visual Spatial\nReasoning Liu et al. [2023b] (VSR) dataset. We use the zero-shot test split provided in the official\nGitHub repository.\nFor tasks that require generation, we use a beam size of 5. For MCQ, we measure the log-likelihood\nof generating each of the answer options.\n3\nExperiments\n3.1\nComparing Existing Approaches\nTable 1 highlights the main differences between existing approaches, include architectural choices\nand the data used for instruction tuning.\nFor fair comparison, we compare variants of all approaches with a similarly sized LLM. Two of the\napproaches (LLaVA and mPLUG-Owl) use a smaller vision encoder (ViT-L), while the others use ViT-\ng. BLIP-2 does not do any multimodal instruction tuning, while all other approaches are instruction\ntuned on varying amounts of multimodal data. The vision encoder is frozen in all approaches except\n2\nApproach\nInstruction Data Size\nVision Encoder\nVision Head\nLLM\nBackbone\nFrozen\nType\nFrozen\nType\nFrozen\nBLIP-2\nN\/A\nViT-g\nYes\nQ-Former\nNo\nVicuna-7B\nYes\nMiniGPT-4\n3.5K\nViT-g\nYes\nQ-Former\nYes\nVicuna-7B\nYes\nLLaVA\n150K\nViT-L\nYes\nN\/A\nN\/A\nVicuna-7B\nNo\nmPLUG-Owl\n290K\nViT-L\nNo\nSimilar to Perceiver Resampler\nNo\nLLaMA-7B\nNo\nInstructBLIP\n15M\nViT-g\nYes\nQ-Former\nNo\nVicuna-7B\nYes\nTable 1: Details of the major components of the compared approaches. We compare variants of each\napproach with a similar decoder size. The instruction data sizes are approximate.\nmPLUG-Owl, while the vision head is always trained when used, except for MiniGPT-4. Additionally,\nboth of the approaches using the smaller vision encoder fine-tune their LLMs, while the others keep\nthe language model frozen.\n3.2\nPerformance on Benchmarks\nTable 2 presents the results on the various benchmarks discussed in Section 2.\nApproach\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nConversation\nDetail\nComplex Reasoning\nOverall\nBLIP-2\n61.5\n66.7\n53.3\n60.5\n50\n53.8\n107.5\nMiniGPT-4\n72.5\n68.2\n76.4\n72.4\n57.2\n36.04\n86.91\nLLaVA\n75.1\n70.6\n88.2\n78\n52.45\n34.8\n67.75\nmPLUG-Owl\n74\n69.8\n86.8\n76.9\n54.99\n34.01\n70.82\nInstructBLIP\n85.3\n75.8\n88.6\n83.3\n58.51\n59.49\n123.65\nTable 2: Comparison of all publicly available multimodal variants of LLMs on the four datasets\ndiscussed above. The scores for LLaVA VQA are relative scores compared to the reference answer as\ndetermined by GPT-4. For NoCaps, we use CIDEr and report accuracy for VSR and ScienceQA. The\nbest results for each dataset are in bold, and the second-best results are underlined.\nFor mPLUG-Owl, we use the checkpoint provided by the authors, where the full decoder is fine-\ntuned instead of the one that uses LORA Hu et al. [2021], as it performs better overall. We ran all\nbenchmarks using the checkpoints provided by the official implementations but directly used the\nresults provided for BLIP-2 in the InstructBLIP paper for VSR, ScienceQA, and NoCaps.\nThe results demonstrate that InstructBLIP, which performs multitask instruction tuning on a variety\nof datasets, performs the best overall on all tasks, highlighting the importance of data diversity during\ninstruction tuning. In contrast, all other instruction-tuned approaches perform poorly on tasks they\nwere not trained on, likely due to overfitting to a specific task type. This is evident from the results of\nLLaVA, mPLUG-Owl, and InstructBLIP, all of which were trained on the LLaVA-150K data and are\nthus the top three performers on LLaVA VQA. However, only InstructBLIP continues to perform well\non out-of-distribution tasks due to its richer instruction tuning data. There are qualitative samples\nfrom each of the three LLaVA benchmark categories in the appendix A.2.\n3.3\nAblations on Model Components\nIn this section, we conduct ablations on the LLaVA architecture to analyze the effects and importance\nof changes to different components, namely the vision encoder, vision head, and data. We chose the\nLLaVA architecture as the base because its training\/evaluation code and data are available, and the\nLLaVA-150K training set has been used by other approaches as well.\n3.3.1\nVision Head\nThe vision head operates over the patch embeddings of a frozen vision encoder (e.g., CLIP) and\ncompresses the features to extract relevant details. In this section, we specifically compare the\neffects of using a Querying Transformer (Q-Former) as the vision head. The Q-Former is a small\nTransformer model that learns queries to extract relevant visual features from the vision encoder. For\n3\nall experiments in this section (unless otherwise mentioned), we use an 80K balanced subset of the\nLLaVA data provided by the authors in the official GitHub repository.\nVision Head\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nType\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nN\/A\nN\/A\n74.8\n72.5\n88.2\n78.5\n51.88\n34.95\n61.25\nQ-Former\nYes\n75\n70.6\n87.4\n77.6\n51.47\n35.35\n71.99\nQ-Former\nNo\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nTable 3: Comparison of the effect of having and training a vision head. All configurations use the\nsame frozen vision backbone and trained LLM.\nThe results in Table 3 clearly show that training the vision head over a frozen encoder offers a\nsignificant improvement compared to not having a vision head. However, using a frozen vision\nhead does not provide any additional benefit compared to not having one. The improvement in\nperformance comes from training the vision head along with the decoder to align features based on\nthe task requirements.\n3.3.2\nVariations of Vision Head\nIn the previous section, we compared the effect of having a vision head. However, it is also possible\nto have different types of vision heads. The Q-Former architecture supports attending to both text\nand image together. For instruction tuning, it can be beneficial to have the Q-Former attend to the\ninstruction along with the image, learning image features that are relevant for answering the question.\nVision Head\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nType\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nImage Q-Former\nNo\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nMultiModal Q-Former\nNo\n72.7\n74\n87\n77.8\n50.98\n35.6\n47.74\nMultiModal Q-Former∗\nNo\n77.5\n73\n88.3\n79.6\n54.17\n36.29\n48.2\nTable 4: Comparison of the effect of having an image-only vs multimodal vision head. All configura-\ntions use the same frozen vision backbone and trained LLM.\nTable 4 presents the results of comparing different types of vision heads. In the \"Image Q-Former\"\nrow, the Q-Former attends only to the image, while in the \"MultiModal Q-Former\" row, the Q-Former\nattends to both the instruction and the image. However, as the instruction tuning data contains multi-\nturn conversations, the instructions for all turns are concatenated together, which leads to suboptimal\nperformance. The \"MultiModal Q-Former∗\" row modifies the instruction data by breaking all multi-\nturn conversation samples into separate training points. This version performs mostly on par with\nthe image-only version. The drop in performance is observed in the detail category and captioning\n(NoCaps), as we do not want to restrict the features to the instruction in these cases, and it is best for\nthe decoder to receive all possible information about the image. These experiments indicate that using\ninstruction-aware visual features does not offer any advantage with open-ended question answering.\nHowever, where the final objective is classification, it offers some advantage as also observed by Dai\net al. [2023].\n3.3.3\nSize of Vision Encoder\nIn the previous experiments, we used ViT-L as the frozen vision encoder. Here, we compare the effect\nof using a larger vision encoder, specifically ViT-g, for the different configurations mentioned above.\nVision Encoder\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nBackbone\n# Params\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nViT-L\n304M\nYes\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nViT-g\n1.0B\nYes\n81.5\n79.1\n90.9\n83.9\n50.16\n35.6\n90.89\nTable 5: Comparison of the effect of using a larger frozen vision encoder. All configurations use a\ntrained image-only Q-Former and trained LLM.\nTable 5 presents the results comparing the ViT-L and ViT-g vision encoders, with all other config-\nurations remaining the same. The results clearly demonstrate that having a larger vision encoder\n4\n(ViT-g) helps improve performance across the board, even on tasks for which the model was not\nspecifically trained (NoCaps). The larger encoder produces a richer image representation, leading to\nbetter overall performance.\n3.3.4\nEffect of Data Size in Alignment Phase\nIn all the discussed approaches, an alignment stage precedes instruction tuning, where a projection\nlayer is learned to map the outputs of the vision heads to the input space of the language model. In\nthis section, we study the effect of data size during the alignment phase. We use BLIP-2 as the starting\npoint, which has been aligned on 129M image-text pairs. For comparison, we perform alignment\nusing 595K image-text pairs, as done in LLaVA, for the same architecture.\nAmount of Alignment Data\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nConversation\nDetail\nComplex Reasoning\nOverall\n129M\n82.6\n76.5\n90\n83.1\n52.21\n35.65\n91.07\n595K\n81.5\n79.1\n90.9\n83.9\n50.16\n35.6\n90.89\nTable 6: Comparison of the effect of data size during the alignment phase. All configurations use a\ntrained image-only Q-Former and trained LLM.\nTable 6 presents the results comparing the alignment with different amounts of data. We observe\nthat aligning the model with a larger amount of data does not significantly improve in-distribution\ndownstream performance when the model is further instruction tuned. This is reasonable as the\nprojection layer, which is trained during alignment, has a small number of parameters. Therefore,\nincreasing the alignment data size eventually leads to diminishing returns. However, for out-of-\ndistribution tasks (VSR, ScienceQA, NoCaps), the larger amount of alignment data does lead to small\nimprovement in performance, most likely due to more robust features.\nAmount of LLaVA VQA Data\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nConversation\nDetail\nComplex Reasoning\nOverall\n150K\n75.1\n70.6\n88.2\n78\n52.45\n34.8\n67.75\n80K\n74.8\n72.5\n88.2\n78.5\n51.88\n34.95\n61.25\nTable 7: Comparison of the effect of data size during the instruction tuning phase. All configurations\nuse a ViT-L vision encoder, no vision head, and a trained LLM.\nTable 7 shows a similar trend for the quantity of data used in instruction tuning. In all the ablation\nexperiments, we used an 80K balanced subset of the LLaVA instruction data. However, if we compare\nthe performance of our recreated versions of LLaVA in the table above, on the original 150K and the\n80K data subset, they perform at par. Beyond a certain point, increasing the amount of data does not\nsignificantly improve performance. However, data diversity does play a role, as demonstrated by the\nsuperior performance of InstructBLIP in Table 2.\n3.3.5\nTraining the Language Model\nThe ablations discussed so far all fine-tune the language model following LLaVA. This section\ncompares the effect of training or keeping the decoder frozen across various configurations.\nVision Encoder\nVision Head\nLLM Frozen\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nType\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nViT-L\nN\/A\nN\/A\nYes\n58.1\n66.2\n75.8\n66.7\n50.65\n34.9\n54.31\nViT-L\nN\/A\nN\/A\nNo\n74.8\n72.5\n88.2\n78.5\n51.88\n34.95\n61.25\nViT-L\nQ-Former\nNo\nYes\n68.4\n69.1\n85\n74.1\n49.75\n35.45\n67.18\nViT-L\nQ-Former\nNo\nNo\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nViT-g\nQ-Former\nNo\nYes\n72.3\n77.6\n88\n79.3\n50.49\n36.49\n89.98\nVit-g\nQ-Former\nNo\nNo\n81.5\n79.1\n90.9\n83.9\n50.16\n35.6\n90.89\nTable 8: Comparing the effect of training the decoder. The configurations mention the vision encoder\nand head used. All experiments use the same LLM (Vicuna-7B).\nThe experiments in Table 8 show that when instruction tuning on the LLaVA dataset, training the\ndecoder helps in almost all scenarios. The performance improvements are reduced to an extent\n5\nwhen using a larger vision backbone (ViT-g), but there is still some gain. InstructBLIP uses a frozen\ndecoder, but these experiments suggest that we can achieve an additional boost in performance if the\ndecoder is trained as well. Moreover, as the gradients are always propagated through the decoder in\nthis architecture setup, training the decoder does not incur any significant overhead.\n4\nDiscussion and Conclusion\nThe benchmarks and ablation experiments offer various insights into multimodal LLMs. The main\ntakeaways from this study are as follows:\n• Vision Encoder: Using a larger vision encoder (ViT-L vs. ViT-g) consistently improves\nperformance across all tasks, as it captures a richer image representation. However, fine-\ntuning the vision encoder does not improve performance on downstream tasks as observed\nfrom the relative performance of mPLUG-Owl in Table 2, which does not keep it frozen.\n• Vision Head: Fine-tuning a vision head (e.g., Q-Former) is powerful as it enables the model\nto extract a better representation for the downstream task while also speeding up training and\ninference due to the smaller output representation. However, using a multimodal vision head\nthat encodes both the image and instruction together does not show any apparent advantage\nover using an image-only head in open-ended question answering. This suggests that it\nis more important to condense all visual information and pass that to the language model\nthrough grafting and letting the LLM use the full context to answer questions.\n• LLM: Training the Large Language Model (LLM) during instruction tuning can lead to\nadditional performance gains without significant overhead in training cost, as gradients are\npropagated through the LLM in all cases. If there are concerns of LLM regression on text\nonly tasks, we can use adapters (LoRA Hu et al. [2021]\/IA3 Liu et al. [2022] etc.) to leave\nthe base model unaltered and use the adapter version when inputs are multimodal.\n• Data: The size of the training dataset becomes less important beyond a certain point in\nboth the alignment and instruction tuning stages. However, data diversity, both within a task\nand across tasks, remains crucial for achieving superior performance as shown by results of\nInstructBLIP.\nThese results provide valuable insights for future research in this direction, guiding architecture\nchoices and emphasizing the importance of data diversity both within and across tasks. The findings\nsuggest that focusing on larger frozen vision encoders, training vision heads, and optimizing the\nLLM can yield improvements in performance on multimodal tasks. Moreover, exploring diverse and\nrepresentative datasets can contribute to achieving state-of-the-art performance.\nThis work also highlights that differences in architectural methodologies for grafting multimodal\ncapabilities into LLMs. A simple strategy of extracting relevant condensed visual features and\ntransforming them via linear projections to the language space performs as well as any other. The\nmajor areas to focus on are data curation and task diversity. Another challenge the community should\nfocus on are hallucinations of these multimodal variants. Language models are unaware of visual\nconcepts during pretraining and tend to hallucinate objects\/concepts in images that might not exist\nwhen probed about them instead of answering no. To make these models useful, a focus on alleviating\nthese hallucinations will be essential.\nIn conclusion, this study sheds light on the key components and strategies for building effective\nmultimodal Large Language Models. It focuses on the limitations of current research, and highlights\nfocus areas that will bring the largest impact to their capabilities and usefulness.\n6\nReferences\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023a.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. The 37th Conference on Neural Information Processing Systems\n(NeurIPS), 2023.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023.\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra,\nDevi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In\nProceedings of the IEEE International Conference on Computer Vision, pages 8948–8957, 2019.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566–4575, 2015.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. In The 36th Conference on Neural Information Processing Systems\n(NeurIPS), 2022.\nFangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of\nthe Association for Computational Linguistics, 2023b.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and\nColin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context\nlearning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. International journal of computer vision,\n123:32–73, 2017.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n2556–2565, 2018.\n7\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 3558–3568, 2021.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference on\nMachine Learning, pages 12888–12900. PMLR, 2022.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for\nimage captioning with reading comprehension. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 742–758. Springer,\n2020.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019.\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In ICDAR, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pages\n8748–8763. PMLR, 2021.\nOpenAI. Gpt-4 technical report. ArXiv, abs\/2303.08774, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m: Image-text pair dataset.\nhttps:\/\/github.com\/kakaobrain\/coyo-dataset,\n2022.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps:\/\/github.com\/tatsu-lab\/stanford_alpaca, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.\n8\nA\nAppendix\nA.1\nCompared Approaches\nFor our experiments, we consider the five publicly available approaches listed below.\n• BLIP-2 Li et al. [2023]: Aligns a frozen vision encoder and frozen language model through\na Querying Transformer (Q-Former) and a linear projection layer. The Q-Former is a\nsmall Transformer model that learns queries to extract relevant visual features from the\nvision encoder. The model is trained in two stages: first, to learn the image representation,\nand then to translate the learned representation through the LLM. It is pretrained with\n129M image-text pairs from COCO Lin et al. [2014], Visual Genome Krishna et al. [2017],\nCC3M Sharma et al. [2018], CC12M Changpinyo et al. [2021], SBU Ordonez et al. [2011],\nand LAION400M Schuhmann et al. [2021].\n• InstructBLIP Dai et al. [2023]: Uses the same architecture as BLIP-2 and performs\nmultimodal instruction tuning. During instruction tuning, the instruction is also passed along\nwith the image to the Q-Former to extract instruction-relevant visual features. It translates\nmultiple captioning and VQA datasets into an instruction-answer format and performs\ninstruction tuning in a multi-task setup. It is trained on around 15M image-instruction pairs\nfrom COCO Caption Lin et al. [2014], Web CapFilt Li et al. [2022], TextCaps Sidorov et al.\n[2020], VQAv2 Goyal et al. [2017], OKVQA Marino et al. [2019], A-OKVQA Schwenk\net al. [2022], OCR-VQA Mishra et al. [2019], and LLaVA-150K Liu et al. [2023a].\n• LLaVA Liu et al. [2023a]: Has a CLIP Radford et al. [2021] ViT-L vision encoder mapped\nto a Vicuna decoder through a linear layer. It follows a two-stage training process: in\nstage-1 (alignment), only the linear mapper is trained to align the visual output with the\nlanguage input. In stage-2, both the mapper and LLM are trained to output answers to\nimage-instruction pairs. For stage-1, 595K image-text pairs from CC3M Sharma et al.\n[2018] are used. For stage-2, they create a dataset of 158K image-instruction pairs across\nconversation, description, and complex reasoning tasks by prompting the language-only\nGPT-4 OpenAI [2023].\n• MiniGPT4 Zhu et al. [2023]: Follows the same architecture as BLIP-2 but uses only the\npretrained Q-Former and learns the linear mapping layer through the same alignment process\nas LLaVA. It also follows a two-stage training paradigm, but in both stages, only the linear\nmapping layer is trained. For stage-1, they use 5M image-text pairs from LAION, SBU, and\nConceptual Captions. For stage-2, they self-curate a high-quality instruction dataset with\n3.5K instruction-image pairs to improve the model’s conversational ability.\n• mPLUG-Owl Ye et al. [2023]: This approach uses a CLIP ViT-L vision encoder, a visual\nabstractor which follows a similar architecture to Perceiver Resampler Alayrac et al. [2022],\na linear mapping layer, and a LLaMA Touvron et al. [2023] decoder. They also follow\na two-stage training setup, but in this work, all parts of the model are trained during one\nof the two stages. The visual abstractor serves a similar purpose to the Q-Former and is\ntrained in stage-1 along with the visual encoder to extract relevant features. In stage-2, the\nvisual encoder and abstractor are frozen, and the LLM is trained. For stage-1, they use a\nmassive dataset of about 1.1B image-text pairs from LAION-400M, COYO-700M Byeon\net al. [2022], Conceptual Captions, and MSCOCO. For stage-2, they use around 290K\ninstruction samples from Alpaca Taori et al. [2023], Vicuna Chiang et al. [2023], Baize Xu\net al. [2023], and LLaVA-150K.\n9\nA.2\nQualitative comparison of benchmaked approaches\nImage and Question\nModel\nGenerations\nWhat is the main focus\nof the image?\n(Coversation)\nBLIP-2\nIs it the cat or the laptop?\nInstructBLIP\nThe main focus of the image is a cat sitting on a\ndesk in front of a laptop computer.\nLLaVA\nThe main focus of the image is a cat sitting on a\ndesk and looking at a laptop.\nMiniGPT-4\nThe main focus of the image appears to be a cat\nsitting on top of a laptop computer that is open and\nconnected to a keyboard. The cat is sitting on the\nright side of the laptop, facing the camera. The\nbackground of the image appears to be a cluttered\ndesk with papers, pens, and other items scattered\naround.\nmPLUG-Owl\nThe main focus of the image is on a cat sitting on\ntop of a laptop and looking at the screen.\nWhy might the man be\nholding the clothes on\nhangers?\n(Complex Reasoning)\nBLIP-2\nHe might be getting ready to pack for a trip.\nInstructBLIP\nThe man might be holding the clothes on hangers\nbecause he is preparing to pack them into a suit-\ncase. In the image, he is standing next to a window\nwith a suitcase open, indicating that he is in the\nprocess of packing his belongings for a trip. By\nholding the clothes on hangers, he can ensure that\nthey maintain their shape and don’t get wrinkled or\ncreased during the packing process. Additionally,\nhanging the clothes on hangers helps to conserve\nspace in the suitcase and makes it easier to access\nand organize the items when unpacking at the des-\ntination.\nLLaVA\nThe man could be holding the clothes on hangers\nfor a variety of reasons. He might be preparing to\nhang the clothes in a closet or dressing area in his\nhome, or he could be taking them to a dry cleaning\nstore or laundromat to have them cleaned. The im-\nage does not provide enough context to determine\nthe exact reason, but it is clear that the man is in\nthe process of handling the clothes on hangers.\nMiniGPT-4\nIt is not clear why the man is holding the clothes on\nhangers. It could be that he is trying to decide what\nto wear, or he could be holding the clothes to take\nthem to a dry cleaner. Without more information, it\nis difficult to say for certain why the man is holding\nthe clothes on hangers.\n10\nImage and Question\nModel\nGenerations\nmPLUG-Owl\nThe man could be holding the clothes on hang-\ners as he is either organizing his closet, packing\nfor a trip, or getting ready to hang them up in\nhis room or a hotel room. Hanging the clothes\nin a well-organized manner can help keep them\nclean and prevent wrinkles, especially if he is pack-\ning for a trip. Additionally, he may be preparing\nfor a photo shoot or attending an event where the\nclothing needs to be presented nicely. Holding\nthe clothes on hangers helps him maintain their\nappearance and ensure they are ready for display,\npresentation, or use.\nWhat do you see happening\nin this image?\n(Detail Description)\nBLIP-2\nThere are two airplanes in the picture. One is land-\ning and the other is taking off. There are also a\nlot of birds in the water. The mountains are in the\nbackground.\nInstructBLIP\nThe image depicts two large airplanes flying over\na body of water, possibly an ocean or a lake. One\nof the planes is in the process of taking off, while\nthe other is already airborne. There are several\nbirds scattered throughout the scene, with some\nperched on the water’s surface and others flying in\nthe sky. In total, there are at least 10 birds visible\nin the image. The airplanes are positioned at dif-\nferent heights, with one closer to the water and the\nother higher up in the sky. Overall, the scene cap-\ntures the beauty of aviation and nature coexisting\nin harmony.\nLLaVA\nThe image features a large airplane taking off from\nan airport runway, with the airplane’s wings fully\nextended. In the background, there is another air-\nplane on the runway, possibly preparing for takeoff\nor landing. The scene is set on a body of water,\nwith several birds in the vicinity of the airplanes.\nA flock of birds is resting near the first airplane,\nwhile another flock is situated in the background of\nthe image. The combination of the airplanes, birds,\nand water contribute to a picturesque aerial view.\nMiniGPT-4\nI see a large passenger jet airplane flying over a\nbody of water with some birds in the foreground.\nmPLUG-Owl\nThe image captures a large airplane taking off from\na runway, surrounded by a group of birds. The\nbirds appear to be flying in a nearby area, likely\nobserving the airplane as it ascends. There is also\na flock of birds flying in the air.\nTable 9: Qualitative comparison of the publicly available multimodal\nLLMs on a selected sample of image-question pairs from the LLaVA test\nset.\n11\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/On the Performance of Multimodal Language Models.pdf"}
{"title":"MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding","authors":"Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, Dan Xu","summary":"Generating lifelike human motions from descriptive texts has experienced\nremarkable research focus in the recent years, propelled by the emerging\nrequirements of digital humans.Despite impressive advances, existing approaches\nare often constrained by limited control modalities, task specificity, and\nfocus solely on body motion representations.In this paper, we present\nMotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these\nlimitations. MotionGPT-2 accommodates multiple motion-relevant tasks and\nsupporting multimodal control conditions through pre-trained Large Language\nModels (LLMs). It quantizes multimodal inputs-such as text and single-frame\nposes-into discrete, LLM-interpretable tokens, seamlessly integrating them into\nthe LLM's vocabulary. These tokens are then organized into unified prompts,\nguiding the LLM to generate motion outputs through a\npretraining-then-finetuning paradigm. We also show that the proposed\nMotionGPT-2 is highly adaptable to the challenging 3D holistic motion\ngeneration task, enabled by the innovative motion discretization framework,\nPart-Aware VQVAE, which ensures fine-grained representations of body and hand\nmovements. Extensive experiments and visualizations validate the effectiveness\nof our method, demonstrating the adaptability of MotionGPT-2 across motion\ngeneration, motion captioning, and generalized motion completion tasks.","url":"http:\/\/arxiv.org\/abs\/2410.21747v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.21747v1","published":1730179534000,"comment":null,"pdf_text":"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nMotionGPT-2: A General-Purpose Motion-Language\nModel for Motion Generation and Understanding\nYuan Wang1,5,6‡\nDi Huang2‡\nYaqi Zhang3\nWanli Ouyang4,5, Senior Member, IEEE\nJile Jiao1,6\nXuetao Feng1,7\nYan Zhou8\nPengfei Wan8\nShixiang Tang4♣\nDan Xu9, Member, IEEE\n1Tsinghua University\n2The University of Sydney\n3University of Science and Technology of China\n4The Chinese University of Hong Kong\n5Shanghai Artificial Intelligence Laboratory\n6Alibaba Group\n7Deepeleph\n8Kuaishou Technology\n9HKUST\n‡Equal Contribution\n♣Corresponding Author\n(Text,Motion)-to-\nmotion\na person is walking \naround casually\nText\na person is walking \naround casually\nText & Initial Token\nText & Last Token\nText & Key Tokens\na person is walking \naround casually\na person is walking \naround casually\nMotionGPT\nMotionGPT-2\nMotion-Related Task\nText-based Motion Generation\nMotion Captioning\nMultiple Signal Motion Generation\nHolistic Motion Genertion\nMotionGPT-2\nUnified Motion-Language Vocabulary\n  \na person walks in an arc to\nthe left then stops\nperson walks forward, turns to\nthe right and walks back\nCan you generate a motion that\nperson is walking and making the\nOK gesture?\nText-Vocab\nBody-Vocab\nHand-Vocab\n<Body_1>\n<Body_2>\n<Body_3>\n<Hand_1>\n<Hand_2>\n<Hand_3>\nperson\nwalking\nforward\nThe person is playing soccer\nFig. 1: This paper proposes a versatile motion-language framework via fine-tuned LLMs given different instructions, named\nMotionGPT-2. Compared with the previous MotionGPT [1], our MotionGPT-2 not only retains the unique capability of\naccommodating multiple control conditions, but also solve various motion-related tasks using a unified model.\nAbstract—Generating lifelike human motions from descriptive\ntexts has experienced remarkable research focus in the recent\nyears, propelled by the emerging requirements of digital humans.\nDespite impressive advances, existing approaches are often\nconstrained by limited control modalities, task specificity, and\nfocus solely on body motion representations. In this paper, we\npresent MotionGPT-2, a unified Large Motion-Language Model\n(LMLM) that addresses these limitations. MotionGPT-2 accommo-\ndates multiple motion-relevant tasks and supporting multimodal\ncontrol conditions through pre-trained Large Language Models\n(LLMs). It quantizes multimodal inputs—such as text and single-\nframe poses—into discrete, LLM-interpretable tokens, seamlessly\nintegrating them into the LLM’s vocabulary. These tokens are\nthen organized into unified prompts, guiding the LLM to generate\nmotion outputs through a pretraining-then-finetuning paradigm.\nWe also show that the proposed MotionGPT-2 is highly adaptable\nto the challenging 3D holistic motion generation task, enabled\nby the innovative motion discretization framework, Part-Aware\nVQVAE, which ensures fine-grained representations of body\nand hand movements. Extensive experiments and visualizations\nvalidate the effectiveness of our method, demonstrating the\nadaptability of MotionGPT-2 across motion generation, motion\ncaptioning, and generalized motion completion tasks.\nIndex Terms—3D Human Motion Generation, Large Language\nModels, SMPL, Vector Quantized-Variational AutoEncoder\nI. INTRODUCTION\nH\nUMAN motion generation plays a pivotal role in various\napplications, including video gaming, robotic assistance,\nand virtual reality. Recent years have witnessed a rapid\ndevelopment of Generative Artificial Intelligence (GenAI) [2]–\n[8], paving the way for novel methods to motion synthesis.\nDespite the impressive performance achieved by existing\nmotion generation methods [9]–[13], current methods suffer\nfrom three major limitations: (1) Limited control conditions.\nExisting approaches typically target only a single type of control\ncondition, i.e., either textual descriptions or multiple frame\nposes. This narrow scope constrains their practical applications\nin scenarios that require the generation of motion sequences\nconditioned on both text descriptions and multiple key-frame\nhuman poses, e.g., social robotics and film animation. (2) Task-\nspecific frameworks without general world knowledge. Existing\nmodels are often task-specific, such as diffusion-based and\nGPT-based frameworks [10], [14]–[16]. These methods lack\nthe adaptability needed for multiple tasks (e.g., captioning,\nin-betweening, prediction) and cannot fully leverage the world\nknowledge embedded in Large Language Models (LLMs).\nWhile recent work [17], [18] proposes unified frameworks, the\nfull potential of LLMs in motion understanding and generation\nremains largely unexplored. (3) Body-only motion represen-\ntations. Existing text-based motion generation solutions [10],\n[14], [16], [19], [20] primarily focus on generating body-only\nmotions rather than holistic motions. However, their plausibility\nand expressiveness remain unsatisfactory in certain scenarios\n(e.g., sports activities and playing musical instruments), as\nimportant details of human motion—e.g., fine-grained hand\ngestures, are frequently overlooked.\narXiv:2410.21747v1  [cs.CV]  29 Oct 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nTo overcome these limitations, we propose MotionGPT-2,\na unified Large Motion-Language Model (LMLM) capable of\nhandling diverse control signals, performing various motion-\nrelevant tasks, and generating holistic human motions. The key\ninnovations of MotionGPT-2 include:\n(1) Formulating Multimodal Control Signals into a\nUnified Representation: MotionGPT-2 designs a versatile\nframework and task-aware prompts for human motion synthesis.\nThis framework, in particular, allows for the generation of\nhuman motions governed by multimodal control conditions,\ndescribed by Mout, Tout = f(Tin, task, Min). Here Min and Tin\nrefer to the input motions and texts, while Mout and Tout\nrepresent the resulting outputs. The variable task indicates\nthe task-aware prompts that adapt the model to specific motion-\nrelated tasks. Compared to MotionGPT [1], MotionGPT-2\nbroadens the scope by incorporating additional motion-related\ntasks, i.e., motion captioning, general motion completion.\n(2) Building a Task-Agnostic Framework with Strong\nGeneralization: Our conference version, MotionGPT [1],\nargued that this challenge can be naturally addressed with the\nassistance of LLMs. There are several compelling reasons. First,\nrecent investigations indicate that LLMs have the ability to\ncomprehend inputs from multiple modalities (e.g., images [21]–\n[24] and videos [25]) through lightweight adapters [26].\nTherefore, we expect that with suitable adapters, LLMs will be\ncapable of understanding motion sequences. Secondly, LLMs\nhave learned a broad range of motion patterns from their\nextensive text training, which allows them to offer diverse\nhuman motion contexts for generating motion. Consequently,\nour motion generator, adapted from LLMs, can produce motions\nwith a wide range of rich patterns. Third, because LLMs\ngenerate tokens in a sequential manner, generating human\nmotion with adaptable sequences is now easily achievable.\nOur MotionGPT-2 improves MotionGPT [1] by using LLMs\nto jointly represent motion and language. We first embeds\nthe human motions into discrete motion tokens via the Vector\nQuantized Variational-AutoEncoder (VQ-VAE). Then, we ex-\npand the LLM’s vocabulary with these motion tokens, creating\nan enriched motion-language vocabulary. By incorporating\nhuman motion and language into a unified vocabulary, the\ncomplex relationships between motion and language become\ntransparent. Further, MotionGPT-2 integrates tokens from\nboth language and motion prompts to generate instructions.\nWe implement a multimodal pre-training combined with an\ninstruction-tuning approach to train MotionGPT-2 efficiently,\nutilizing the established LoRA adaptation method. The motion\ninstruction tuning framework we have developed allows for the\nintegration of pose sequence information into the fine-tuned\nLLM, while capitalizing on the strong motion priors inherent\nin the origin LLM. With mere 1% parameters, the generalized\nMotionGPT-2 achieve competitive results in multiple motion-\nrelated tasks compared to those trained-from-scratch models\nwith specialized frameworks.\n(3) Achieving Precise Discrete Representations of Holistic\nHuman Motions: To address this issue, we incorporate\nkinematic structure priors and design an innovative Part-Aware\nVQ-VAE for holistic motion tokenization. Compared to the\nvanilla motion VQ-VAE used in MotionGPT [1], the proposed\nPart-Aware VQ-VAE utilizes two-level discrete codebooks\nand motion encoders to learn body-hand representations. The\nkey insight of our Part-Aware VQ-VAE lies in its ability to\nlearn informative and compact representations of fine-grained\nholistic motions. This two-level discretization framework\ncaptures subtle hand movements while maintaining global\nbody dynamics. Finally, the hand and body motion tokens are\nintegrated into the LLM’s vocabulary, enabling MotionGPT-\n2 to correctly interpret and generate holistic motion-related\nsequences in response to instructions.\nWe conduct extensive experiments on the HumanML3D [20],\nKIT-ML [27] dataset, demonstrating that MotionGPT-2 has\nstrong adaptability and efficiency across multiple motion-related\ntasks by fine-tuning an LLM. With only 1% of additional param-\neters, MotionGPT-2 achieves competitive performance across\ntasks while significantly reducing training time—requiring\nonly 10% of the time compared to other methods. We also\nobserve that joint training under multiple control instructions\nsurpasses training with singular control signals, highlighting\nthe effectiveness of our unified motion-language fine-tuning\nparadigm. Experiments on the Motion-X [28] dataset verify\nthat our proposed MotionGPT-2 is highly adaptable to the\nchallenging 3D holistic motion generation task.\nIn summary, we extend our conference version [1] by\nmaking several additional novel contributions:\n• We propose an enhanced version of the previous Mo-\ntionGPT, MotionGPT-2. Compared to its predecessor,\nMotionGPT-2 serves as a unified Large Motion-Language\nModel to handle multiple motion-related tasks, enabling us\nto establish new state-of-the-art performance benchmarks.\n• By creating an enriched motion-language vocabulary, we\nempower the pre-trained LLMs with the ability to unify\nthe understanding and generation of body kinetics.\n• We further extend the proposed MotionGPT-2 to tackle the\nchallenging 3D whole-body motion generation task by in-\ntroducing a whole-body motion discretization framework,\nPart-Aware VQ-VAE, which encodes body motions and\nhand gestures with two structured codebooks for fine-\ngrained motion representations.\nWe conduct extensive experiments on the HumanML3D, KIT-\nML, and Motion-X datasets to validate the superiority of our\nproposed LLM-based unified motion-language model across\nmultiple motion-related tasks. We provide in-depth analysis\nand visualizations of MotionGPT-2, indicating that further\nadvancements in LLM technology hold the potential to enhance\nits performance in the future.\nII. RELATED WORK\nA. LLMs and Multi-modal Large Language Models\nLarge Language Models (LLMs) [29]–[34] have experienced\nrapid development recently, e.g., BERT [29], GPT [30], and\nGoogle T5 [35]. Notably, models like GPT-4 [33] show\noutstanding performance on various language tasks due to\ntheir extensive training datasets (GPT-4 utilizes 45 gigabytes)\nand numerous parameters. Traditionally, language models were\ncrafted for specific tasks like translation or sentiment analysis,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nVQVAE\n\"259, 467, ...\"\nMotionGPT: LLM      + LoRA      \nText\n\"a person walks\nstraight forward\"\nPose Tokens\nEncoder\nCodebook\nDecoder\n\"Generate a sequence of motion\ntokens matching the following\nhuman motion description given the\ninitial token\"\nCodebook\n\"259, 494, ...\"\nCE Loss\n\"259\"\nGenerated Tokens\nGround Truth Tokens\nTask Prompt\nControl Conditions\n,\nFig. 2: The pipeline of MotionGPT, a Motion General-Purpose generaTor. Given text and poses as an input example, we organize\ntask descriptions (Instruction) and multiple control conditions (Input) within a question template. MotionGPT fine-tunes an\nLLM to generate the corresponding motion answer, which can then be decoded into human motions using a VQ-VAE decoder.\nbut recent innovations, such as ChatGPT, have broadened their\nfunctional scope. Built on the GPT-4 framework, ChatGPT\ncan engage in interactive dialogues, showcasing strong natural\nlanguage understanding. This effectiveness opens new avenues\nfor downstream applications achievable through the fine-tuning\nof these LLMs. Nevertheless, fine-tuning these extensive models\nposes significant challenges. To mitigate this issue, several\nefficient fine-tuning techniques have emerged, including prompt\ntuning [36]–[38], adapters [39]–[41], and LoRA [26]. Our study\nis inspired by recent developments in LLMs while tackling a\ndifferent problem through the integration of a novel modality.\nB. Human Motion Generation\nMotion generationhas a long-standing history in research [9]–\n[12], [42]–[46] and can be conditioned on various inputs,\nincluding motion descriptions, specific actions, and music. For\ninstance, HP-GAN [47] and [48] adopt a sequence-to-sequence\nmodel to forecast future poses from earlier ones. Moreover,\nACTOR [12] utilizes a transformer-based VAE for both\nunconditional generation and action-driven motion generation.\nTRAJEVAE [49] generates motion sequences that align with a\ngiven trajectory when provided with an initial pose. Recently,\na significant focus has been placed on text-conditional motion\ngeneration, which involves creating human motion sequences\nbased on textual prompts.Previous researches [14], [20], [42],\n[50], [51] focus on modeling a joint latent space for motion and\ntext alignment. In TEMOS [9], a VAE model is proposed that\ncreates a shared latent space for motion and text interactions.\nT2M-GPT [14] formulates the motion generation as the next\nindex prediction task and leverage small language models\nto model the translation mapping between discrete motion\nindices and text. MotionCLIP [42] and TM2T [52] align the\nshared space of text and motion with the expressive CLIP [53]\nembedding space MotionDiffuse [10] introduces a diffusion\nmodel into its framework for generating motion from textual\ndescriptions, which leads to impressive outcomes. In a different\napproach, MDM [11] aims to boost the coherence between\nmotion and textual inputs by implementing CLIP [53] as the\ntext encoder, thereby enhancing the model with more effective\ntextual priors. However, it is challenging to model semantically\ncomplex relationships of human motions and texts, particularly\nin the absence of general world knowledge, e.g., how specific\ngestures convey intentions and how to interpret body language\nin different context. Current efforts, such as MotionGPT [17],\nMotionLLM [54], M3-GPT [18] have initiated the development\nof a unified motion-language model aimed at generating plau-\nsible human motions alongside along with textual descriptions\ndriven by prompt instructions. However, none of them leverage\nthe general world knowledge of LLM simultaneously address\ndiverse motion-related tasks and body-hand representations.\nUnlike prior approaches, MotionGPT-2 is distinguished as\nthe first Large Motion-Language Model (LMLM) capable of\nmultimodal control, handling diverse motion-related tasks, and\nproviding a comprehensive representation of motion.\nIII. MOTION GENERAL-PURPOSE GENERATOR\nFigure 3 depicts MotionGPT as a Motion General-Purpose\ngenerator that is driven by multi-modal inputs. To align\nwith the LLM’s token-in-token-out nature, we first quantize\nthe human motions into code representations via the well-\nestablished motion VQ-VAE [55] (Section III-A). These motion\ndiscrete codes, along with text control conditions and specially-\ncrafted task-aware instructions, are organized into a unified\nquestion template (Section III-B). By adjusting the task-aware\ninstructions, MotionGPT reveals its potential as a generic\nbaseline framework for tasks involving motion generation.\nA. 3D Human Motion Quantization\nVQ-VAE proposed in [55] is designed to learn discrete\nrepresentations with semantic-rich codebooks in an encoding-\ndecoding fashion. To discretely represent human motions as\nsemantic tokens, we introduce the motion VQVAE model,\nwhich consists of a motion encoder E, a motion decoder D,\nand a codebook Bm = {b1, b2, . . . , bN}, where N denotes the\ncodebook size, as illustrated in Fig. 3. We denote a human\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nMotionGPT-2: LLM      + LoRA     \nTask Prompt\nMotion\nEncoder\nText Vocabulary\n<Text_Token1>\n<Text_Token2>\n......\n......\nText\nEncoder\n......\n......\n......\n<Motion_Token1>\n<Motion_Token2>\n<Motion_Token3>\nMotion\nDecoder\n<Motion_Token4>\n......\n<Motion_Token5>\n<Motion_Token6>\nMotionGPT-2: LLM      + LoRA      \nStage3: Instruction Tuning\nStage2: Motion-Language Alignment\nperson walks forward,\nturns to the right and\nwalks back\nThe person is\nplaying soccer\nperson is walking and\nmaking the OK\ngesture\nStage1: Human Motion Quantization\na person walks\nin an arc to the left\nthen stops\nMultimodal Vocabulary\nMotion\nDecoder\nMotion\nDecoder\nMotion\nDecoder\nMotion Vocabulary\nFig. 3: The overview pipeline of our MotionGPT-2. MotionGPT-2 is composed of multi-modal tokenizers (Section III-A) and a\nversatile motion-language model (Section IV-B). With unified multimodal vocabulary and task-aware instructions (Section III-B),\nMotionGPT-2 enables to accept multiple control conditions (Section IV-C) and solve various motion-related tasks. MotionGPT-2\nis learned by the Motion-Language Alignment stage and Instruction Tuning stage (Section IV-D).\nmotion as m = {m1, m2, ..., mT } ∈RT ×d, where T is the\nmotion sequence length, and d is the dimension per frame.\nTo learn the codebook of human motions, the estimated\nmotion embedding E(m) is transformed into a collection of\ncodebook entries through quantization. Further, the quantized\nmotion vector is obtained by searching the nearest correspond-\ning embedding in a learnable codebook Bm, which can be\nmathematically formulated as:\ne = arg min\nbk∈B\n∥E(m) −bk∥2.\n(1)\nBased on the estimation latent representation e, the motion\ndecoder D employs several 1-D deconvolutional layers to\nextract frame-wise motion features, decoding the sequential\ncodes e to the raw motion space as the motion ˆm: ˆm = D(e).\nThe motion code p of the human pose m can be calculated as\nthe index of its nearest embedding in the codebook, i.e.,\np = arg min\nk\n∥E(m) −bk∥2.\n(2)\nThe standard motion VQ-VAE can be trained by the\nthree tailored loss: 1) a reconstruction loss to minimize the\ndistance between the decoded motion ˆm and the origin motion\nm, 2) a codebook loss to encourage the bk to be drawn\ncloser to the encoded embedding E(m), aligning the discrete\ncode representation with origin motion embeddings. 3) a\ncommitment loss to guide the encoded embedding e to remain\nclose to the corresponding discrete code bk, stabilizing the\ntraining process and reducing codebook oscillation. The total\nloss function LVQVAE can be formulated as follows:\nLVQVAE = ||D(E(m)) −m||2 + ∥sg[E(m)] −e∥2\n2\n+β∥E(m) −sg[e]∥2\n2.\n(3)\nHere, sg indicates the stop gradient operation and β is the\nhyper-parameter to control the weight of the commitment loss.\nTo enhance the quality of the generated motion, we in-\ncorporate L1 smooth loss and velocity regularization loss\nin the reconstruction loss. The codebook is optimized with\nExponential Moving Average (EMA) operation and codebook\nreset techniques following [1], [14], [17].\nB. Instruction Generation\nIn our previous conference version, we design instructions\nthat integrate task prompts and control conditions to en-\nable (text, motion)-motion generation tasks. Given the task\nprompts T\n= {t1, t2, ..., tnt}, the text control conditions\nX = {x1, x2, ..., xnx} and the pose control conditions P =\n{p1, p2, ..., pnp} where nt, nx and np are the number of codes\nin T , X and P, the instruction template I is formulated as:\n% General control conditions format\nControl Conditions: {The Text control condition X\n<x1, x2, ..., xnx>} {The Pose control condition P\n<p1, p2, ..., pnp>}\n% General instruction format\nInstruction (I): {The Task Prompt T <t1, t2, ..., tnt>}\n{Control Conditions}\nPose control conditions P = {p1, p2, ..., pnp}, representing\npose codes produced by the previously mentioned motion VQ-\nVAE. The entire instruction I is conceptualized as a series\nof specialized text inputs. By formulating diverse instruction\nprompts, the MotionGPT [1] tackles both traditional motion\ngeneration tasks and emerging challenges in motion synthesis.\nSpecifically, for text-based motion generation task, Mo-\ntionGPT address it by instantiating following instruction I:\nInstruction (I): {Task Prompts: “Generate a sequence\nof motion tokens matching the following human motion\ndescription.”} {Control Conditions: Text control condi-\ntion X}\nBy adjusting instructions, the proposed MotionGPT can be\neasily adapted to multiple control conditions, e.g. the textual\ndescription and an arbitrary number of human poses:\nInstruction (I): {Task Prompts: “Generate a sequence\nof motion tokens matching the following human motion\ndescription given the init\/last\/key pose tokens.”} {Control\nConditions: Text control condition X <Motion Token>\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nHand\nEncoder\nℰℎ\nBody\nEncoder\nℰ𝑏\nMotion\nEncoder\n𝒟\nProjection Layer\nBody\nTokenizer\nHand\nTokenizer\nCodebook \nCodebook \nℬℎ\nℬ𝑏\nC\n…\n…\n…\n…\n…\n…\nBody-Hand Motion\nHand Split\nBody Split\nFig. 4: The framework overview of our proposed Part-Aware VQVAE for body-hand motion tokenization. The Part-Aware\nVQVAE splits SMPL-X-based human representations into body-hand motions mb and mh. Then, it quantizes fine-grained\nbody-hand motion into two discrete codebooks Bb and Bh with hierarchical body priors.\nPose control conditions P <\/Motion Token>}\nFor the motion generation task, the answer of LLM is\nrepresented as ˆP = {ˆp1, ˆp2, ..., ˆpnˆ\np}, comprising a sequence\nof generated motion codes. These codes can be decoded to\nhuman motion using Eq. 2.\nC. Model Optimization\nIn the previous MotionGPT [1], we utilize a decoder-\nonly LLM following [54], which effectively handle complex\nrelationships and underlying patterns between text and mo-\ntion. The token-in-token-out LLM maximizes the probability\npθ (xt | x<t, T , c) of succeeding token in an autoregressive\nmanner. Here, T represents the Task Prompts, c is the control\nsignal, x1:T denotes the target token sequence. Therefore,\nduring the training process, the Cross Entropy loss is applied\nto ensure the correspondence between the estimated tokens and\nthe real tokens, fine-tuning the LLM by LoRA [26], which is\nmathematically formalized as:\nLLoRA = −\nX\nlog pθ (xt | xx<t, T , c) .\n(4)\nIV. MOTIONGPT-2: A VERSATILE MODEL SUPPORTING\nHOLISTIC MOTION UNDERSTANDING AND GENERATION\nBuilding upon the MotionGPT, MotionGPT-2 is to formulate\na multi-task unified (motion generation, motion captioning,\nand generalized motion completion) framework to support\nholistic motion representations (fine-grained body and hand\nmovements). The key designs are as follows: (1) Part-Aware\nVQ-VAE for Body-hand Tokenization. By designing an in-\nnovative Part-Aware VQVAE, our MotionGPT-2 achieves\ninformative and compact representations of fine-grained holistic\nmotions, making it adaptable to holistic motion generation tasks\n(Section IV-A). (2) A Unified Motion-Language Vocabulary. By\nexpanding the text vocabulary of LLM, we develop a unified\nLarge Motion-Language Model (LMLM) (Section IV-B) for the\nseamless integration of motion and language modalities. This\nvocabulary extension enables MotionGPT-2 to support a broader\nrange of novel motion-related tasks by different instructions\n(Section IV-C). (3) Three-Stage Training Strategy. To enhance\nthe alignment between motions and texts, we develop a\nthree-stage training pipeline (Section IV-D), including Motion\nTokenization, Motion-Language Alignment, and Instruction\nFine-tuning.\nA. Part-Aware VQVAE for Holistic Motion Representations\nOur previous MotionGPT is limited to the generation of\nbody-only motions. Yet, advancing toward holistic motion\ngeneration is essential for achieving more realistic and lifelike\nanimations. Holistic motions are considerably more complex\nthan their body-only counterparts, necessitating the detailed\nand coordinated motion generation involving the body and\nhands features. In this section, our MotionGPT-2 emphasizes\nthe representation of body-hand motions. With the above single\nmotion vocabulary approach, the embedding of holistic human\nmotion inevitably lead to ambiguities where similar actions\nare represented by the same motion token. To model distinct\nsemantic granularity for body and hand, we design a Part-Aware\nVQVAE module, which utilizes the codebook Bb and Bh to\nlearn discrete representations for body and hand, respectively.\nOur proposed Part-Aware VQVAE is illustrated in Fig. 4.\nSpecifically, we input the SMPL-X based body-hand repre-\nsentations mB = {mB\n1 , mB\n2 , ..., mB\nT } ∈RT ×d and mH =\n{mH\n1 , mH\n2 , ..., mH\nT } ∈RT ×d into two separate encoders. As\nshown in Fig. 4, the hand embedding eh is first quantized by\nthe hand codebook Bh and we then fuse the body and hand\ntokens via the concatenation operator before quantizing the\nbody embedding eb using body codebook Bb. The interaction\nbetween body and hand motions is tightly coupled and\narticulated, shaped by biomechanical and physical restrictions,\nparticularly in the context of complex activities. Such a fuse-\nbefore-quantize method for body tokens effectively enhances\nthe overall naturalness and coordination of holistic motion\nrepresentations. Akin to the [56], the Body-hand Decoder is\ndesigned to take body tokens pB and hand tokens pH as input\nto accurately reconstruct corresponding motions.\nB. A Unified Motion-Language Vocabulary\nMost previous motion-related studies [1], [14], [19], [52],\n[57] have viewed textual descriptions and motions as separate\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nmodalities. However, similar to sentences in natural language, a\ncontinuous motion m is compressed into motion tokens, serving\nas a type of “body language”. Therefore, our MotionGPT-2\nnaturally uses LLMs to jointly model motion and language\nrepresentations. It reflects how humans execute body motions,\nallowing for seamless transitions across motion-related modali-\nties in real-life scenarios.\nSpecifically, to merge multi-modal discrete tokens with the\npre-trained LLM, the original vocabulary Bt is expanded to\nincorporate motion tokens Bm. Further, we also insert several\nspecial tokens Bs into the LLM’s vocabulary, e.g., ⟨motion⟩\nand ⟨\/motion⟩, which signify the beginning and end of the\nmotion. This extends the vocabulary into a unified text-motion\nset, B = {Bt, Bm, Bs}. The newly incorporated parameters\nare initialized randomly. Expanding the LLM’s vocabulary, a\nhuman motion is denoted as a token sequence that is LLM-\nunderstandable. Equipped with the vocabulary B, we model the\njoint distribution of textual descriptions and human motions in\na unified space. It allows us to formulate motion-related tasks\nin a general framework, leveraging the task-aware instructions.\nC. Instruction Prompts in New Motion-Related Tasks\nOur previous MotionGPT primarily focused on motion\ngeneration tasks. In contrast, our MotionGPT-2 enhances its\ncapabilities by defining several core motion-related tasks,\nincluding motion captioning, motion prediction, and motion\ninterpolation. For each task, we construct unique instruction\nprompts tailored to the specific task requirements. For instance,\ngiven a LLM F, the instruction template I for motion\ncaptioning and motion prediction task as well as the answer\nof the LLM ˆP = F(I) are defined as:\nBelow is an instruction that describes a task, paired with\nan input that provides further context. Write a response\nthat appropriately completes the request.\n% Task Prompts: Motion Captioning Task Prompts\n% Control Conditions: Code Sequences of Control\nConditions (Motion Tokens)\nInstruction I: {Task Prompts T }{Control Conditions}\nAnswer ˆP: {Sequences of Text Tokens}\n% Task Prompts: Motion Prediction Task Prompts\n% Control Conditions: Code Sequences of Control\nConditions (Initial Several Motion Tokens)\nInstruction I: {Task Prompts T } {Control Conditions}\nAnswer ˆP: {Sequences of Human Motion Tokens}\nIn the text-based holistic motion generation task, the in-\nstruction tuning phase proceeds after obtaining the discrete\nmotion tokens pB and pH. During this stage, we formulate\nthe instruction template I and the answer P of the LLM for\nmotion generation. Inspired by MLLMs [21], [23], we unify the\ndiscrete body-hand tokens within a general prompt template.\nBelow is an instruction that describes a task, paired with an\ninput that provides further context. Write a response that\nappropriately completes the request.\n% Task Prompts: Motion Generation Task Prompts T\n% Control Conditions: Sequences of Control Conditions\n(Text Tokens)\nInstruction I: {Task Prompts T } {Control Conditions}\nAnswer ˆP: {< Motion Body Token >< Body Token ><\n\/Motion Body Token\n><\nMotion Hand Token\n><\nHand Token >< \/Motion Body Token >}\nD. Three-Stage Training Strategy\nOur previous MotionGPT [1] is founded on a two-stage\napproach. Nevertheless, the absence of motion-language align-\nment hinders the model to interpret the complex relationships\nbetween motions and texts. Towards this issue, our MotionGPT-\n2 proposes a comprehensive three-stage training strategy:\nStage 1: Training of Motion Tokenizer. As depicted in\nSection III-A, we first learn the discrete motion representations\nto align with the LLM’s token-in-token-out nature. Guided by\nthe objective outlined in Eq. 3, the quantization process allows\nhuman motions m be expressed as a sequence of tokens, which\nseamlessly integrates with descriptive text. To maintain stability\nin LLM optimizing, we subsequently freeze the weights of the\nmotion-aware VQVAE during further training stages.\nStage 2: Motion-Language Alignment. Inspired by current\nMulti-modal Large Language Models (MLLMs) [21], [23], to\nalign the motion and language feature space of MotionGPT-\n2, the LLaMA [34] model is finetuned on a wide range\nof motion-related tasks. To accurately interpret contextual\n‘body language’ semantics while preserving the text generation\ncapability of the LLM, we finetune the LLM with LoRA using\na mixture of language and motions data in both unsupervised\nand supervised manners. Similar to LLaVA [23], we learn the\ncomplex relationship of language and motions with the paired\ntext-motion datasets. Further, we ensure the text generation\ncapability through the LLM’s next-token prediction mechanism\non text-only data. Compared with MotionGPT, with the\nadditional motion-language alignment stage, MotionGPT-2\noffers improved semantic consistency and fine-grained controls\nof human motions and textual descriptions.\nStage 3: Fine-tuning LLM by Motion Instructions.\nInstruction tuning [58] enables LLMs to handle various\ngeneration tasks by asking the LLM questions in different\ninstructions. Hence, we devise a set of instructions that merge\ntask descriptions with control signals and employ the efficient\nLow-Rank Adaptation (LoRA) [26] to fine-tune LLMs.\nV. EXPERIMENTS\nA. Experimental Setup\nDatasets. We apply three publicly available datasets, Hu-\nmanML3D [20], KIT-ML [27] and MotionX [28] for evaluation.\nThe HumanML3D dataset [20] stands as the largest available\ndataset focused solely on 3D body motion and associated\ntextual descriptions. It comprises 14,616 motion clips paired\nwith 44,970 meticulously annotated descriptions derived from\na vocabulary of 5,371 unique words. These motion sequences\nare sourced from the AMASS [62] and HumanAct12 [45]\nmotion capture collections, showcasing a diverse array of\nhuman activities, including everyday tasks, sports, acrobatics,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nTABLE I: Quantitative results of text-based motion generation on the HumanML3D dataset. “Real” denotes the results\ncomputed with GT motions. “→” indicates metrics that are better when closer to “Real” distribution. “MultiModal Dist.” denotes\nthe Multi-Modality Distance. We conduct each evaluation 20 times, presenting the average metric and a 95% confidence interval,\nwith the top scores marked in bold.\nModel\nR-Precision ↑\nFID ↓\nMultiModal Dist. ↓\nDiversity →\nMultiModality ↑\nTop 1\nTop 2\nTop 3\nReal\n0.511±.003\n0.703±.003\n0.797±.002\n0.002±.000\n2.974±.008\n9.503±.065\n—\nTM2T [52]\n0.424±.003\n0.618±.003\n0.729±.002\n1.501±.046\n3.467±.008\n8.589±.058\n2.424±.093\nT2M [20]\n0.457±.002\n0.639±.003\n0.740±.003\n1.067±.002\n3.340±.008\n9.188±.002\n2.090±.083\nMDM [59]\n0.319±.005\n0.498±.004\n0.611±.007\n0.544±.001\n5.566±.027\n9.559±.086\n2.799±.072\nMotionDiffuse [10]\n0.491±.001\n0.681±.001\n0.782±.001\n0.630±.001\n3.113±.001\n9.410±.049\n1.553±.042\nMLD [15]\n0.481±.003\n0.673±.003\n0.772±.002\n0.473±.013\n3.169±.010\n9.724±.082\n2.413±.079\nT2M-GPT [14]\n0.491±.003\n0.680±.003\n0.775±.002\n0.116±.004\n3.118±.011\n9.761±.081\n1.856±.011\nMoMask [16]\n0.521±.002\n0.713±.002\n0.807±.002\n0.045±.002\n2.958±.008\n9.620±.064\n1.241±.040\nReMoDiffuse [19]\n0.510±.005\n0.698±.006\n0.795±.004\n0.103±.004\n2.974±.016\n9.018±.075\n1.795±.043\nAttT2M [60]\n0.499±.003\n0.690±.002\n0.786±.002\n0.112±.006\n3.038±.007\n9.700±.090\n2.452±.051\nGraphMotion [61]\n0.504±.003\n0.699±.002\n0.785±.002\n0.116±.007\n3.070±.008\n9.692±.067\n2.766±.096\nMotionGPT [1]\n0.364±.005\n0.533±.003\n0.629±.004\n0.805±.002\n3.914±.013\n9.972±.026\n2.473±.041\nMotionGPT [17]\n0.492±.003\n0.681±.003\n0.733±.006\n0.232±.008\n3.096±.008\n9.528±.071\n2.008±.084\nMotionLLM [54]\n0.482±.004\n0.672±.003\n0.770±.002\n0.491±.019\n3.138±.010\n9.838±.244\n—\nMotionGPT-2 (Ours)\n0.496±.002\n0.691±.003\n0.782±.004\n0.191±.004\n3.080±.013\n9.860±.026\n2.137±.022\nand artistic expressions. Each motion clip is linked with 3-\n4 descriptive texts. For training, one sentence is randomly\nchosen as the match, while for testing, the first text description\nis consistently used to evaluate model performance. The motion\nclips are down-sampled to 20 FPS and vary in duration from 2\nto 10 seconds. The dataset is divided into training, validation,\nand test subsets, allocated in an 80%, 5%, and 15% ratio,\nrespectively, with no overlaps among them.\nThe KIT-ML [27] dataset is comprised of 3,911 motion\nsequences along with 6,278 textual descriptions. Each se-\nquence is associated with one to four sentences, with the\naverage description containing 9.5 words. This dataset merges\nselected elements from the KIT WholeBody Human Motion\nDatabase [63] and the CMU Graphics Lab Motion Capture\nDatabase [64], with an emphasis on locomotion motions. The\nmotion sequences in KIT-ML have been down-sampled to a\nframe rate of 12.5 fps, ensuring a consistent and manageable\nspeed for further analysis and experimentation.\nThe Motion-X [28] dataset is currently the largest whole-\nbody expressive motion repository, comprising 95,642 high-\nfidelity 3D motion sequences based on the SMPL-X model [65],\npaired with corresponding pose descriptions and semantic\nlabels for each sequence. The Motion-X dataset gathers 15K\nmonocular videos from various online sources and public video\ndataset, capturing a wide range of scenarios such as daily\nactions, sports activities, and many domain-specific scenes,\nwith 13.7M frame-level 3D whole-body pose annotations. In\nthis paper, we standardize the Motion-X dataset by selecting\n52 joints from the human body and hands.\nEvaluation Metrics. Following [1], [14], [17], [20], [52],\nour evaluation metrics are summarized as the following parts:\n(1) Text-based Motion Generation Task. To assess the quality\nof the generated motion, we utilize evaluation metrics that are\naligned with those utilized in prior research. These metrics\ninclude the Fr´echet Inception Distance (FID), Multi-modal\nDistance (MM Dist), R-Precision (calculating the Top-1\/2\/3\nmotion-to-text retrieval accuracy), the Diversity, and the Multi-\nModality metric. Together, these metrics yield a comprehensive\nevaluation of the realism and diversity present in the generated\nmotion. In accordance with the procedures outlined in [20], we\ncalculate these metrics using a 95% confidence interval based\non 20 independent trials.\n(2) Multiple Signal Controlled Motion Generation Task. We\npropose new evaluation metrics for our motion generation\nframework: Reconstruction Loss (Recon) and Velocity Loss\n(Vel), both calculated using L2 loss to assess the alignment\nbetween the supplied pose conditions and the generated motion.\nWhen initial or final poses are specified, it is crucial that\nthe corresponding generated poses appear correctly within\nthe motion sequence. Therefore, we utilize Recon and Vel\nto assess the reconstruction of the initial or last poses and\ntheir temporal consistency with their surrounding poses. In\ncases where keyframe poses are provided but their positions\nof the corresponding generated poses within the sequence are\nunknown, we compute the Nearest Euclidean Distance to the\ncorresponding ground truth poses and report the Recon and\nVel to measure the key poses reconstruction and their temporal\ncontinuity with neighboring poses.\n(3) Motion-to-Text Task. Besides R-Precision (Top-1\/2\/3)\nand multimodal distance, we also follow [17], [52] utilizing\nlinguistic metrics from natural language studies, including\nBLEU [66], ROUGE [67], CIDEr [68], and BertScore [69], to\nquantitatively measure the performance of our MotionGPT-2\nin the motion captioning task.\n(4) Motion Completion Task. Beyond Diversity and FID\nmetrics, we measure the accuracy of motion predictions using\nstandard metrics such as Average Displacement Error (ADE)\nand Final Displacement Error (FDE), commonly utilized in\nprior works [17], [70], [71].\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nTABLE II: Quantitative results of text-based motion generation on the KIT-ML dataset. “Real” denotes the results\ncomputed with GT motions. “→” indicates metrics that are better when closer to “Real” distribution.\nModel\nR-Precision ↑\nFID ↓\nMultiModal Dist. ↓\nDiversity →\nMultiModality ↑\nTop 1\nTop 2\nTop 3\nReal\n0.424±.005\n0.649±.006\n0.779±.006\n0.031±.004\n2.788±.012\n11.080±.097\n—\nTM2T [52]\n0.280±.005\n0.463±.006\n0.587±.005\n3.599±.153\n4.591±.026\n9.473±.117\n3.292±.081\nT2M [20]\n0.361±.006\n0.559±.007\n0.681±.007\n3.022±.107\n3.488±.028\n10.720±.145\n2.052±.107\nMDM [59]\n0.164±.004\n0.291±.004\n0.396±.004\n0.497±.021\n9.191±.022\n10.850±.109\n1.907±.214\nMotionDiffuse [10]\n0.417±.004\n0.621±.004\n0.739±.004\n1.954±.062\n2.958±.005\n11.100±.143\n0.730±.013\nMLD [15]\n0.390±.008\n0.609±.008\n0.734±.007\n0.404±.027\n3.204±.027\n10.800±.117\n2.192±.071\nT2M-GPT [14]\n0.416±.006\n0.627±.006\n0.745±.006\n0.514±.029\n3.007±.023\n10.920±.108\n1.570±.039\nMoMask [16]\n0.433±.007\n0.656±.005\n0.781±.005\n0.204±.011\n2.779±.022\n10.711±.087\n1.131±.043\nReMoDiffuse [19]\n0.427±.014\n0.641±.004\n0.765±.055\n0.155±.006\n2.814±.012\n10.800±.105\n1.239±.028\nAttT2M [60]\n0.413±.006\n0.632±.006\n0.751±.006\n0.870±.039\n3.039±.021\n10.960±.123\n2.281±.047\nGraphMotion [61]\n0.429±.007\n0.648±.006\n0.769±.006\n0.313±.013\n3.076±.022\n11.120±.135\n3.627±.113\nMotionGPT [1]\n0.340±.002\n0.570±.003\n0.660±.004\n0.868±.032\n3.721±.018\n9.972±.026\n2.296±.022\nMotionGPT [17]\n0.366±.005\n0.558±.004\n0.680±.005\n0.510±.016\n3.527±.021\n10.350±.084\n2.328±.117\nMotionLLM [54]\n0.409±.006\n0.624±.007\n0.750±.005\n0.781±.026\n2.982±.022\n11.407±.103\n—\nMotionGPT-2 (Ours)\n0.427±.003\n0.627±.002\n0.764±.003\n0.614±.005\n3.164±.013\n11.256±.026\n2.357±.022\nTABLE III: Assessment of motion generation on the HumanML3D [20] and KIT-ML [63] test subsets across diverse control\nconditions. With initial or key tokens, MotionGPT-2 demonstrate superior performance compared to the text-only version.\nMethods\nFID↓\nMultiModal Dist.↓\nDiversity↑\nFID↓\nMultiModal Dist.↓\nDiversity↑\nHumanML3D (MotionGPT)\nHumanML3D (MotionGPT-2)\nText-Only\n0.567\n3.775\n9.006\n0.191\n3.080\n9.860\nText + Initial Pose\n0.520\n3.844\n9.588\n0.183\n3.285\n10.066\nText + Last Pose\n0.591\n3.718\n9.251\n0.358\n3.673\n9.582\nText + Random Pose\n0.367\n3.598\n9.176\n0.182\n3.031\n10.102\nKIT-ML (MotionGPT)\nKIT-ML (MotionGPT-2)\nText-Only\n0.597\n3.394\n10.540\n0.614\n3.164\n11.256\nText + Initial Pose\n0.664\n3.445\n10.390\n0.756\n3.362\n11.053\nText + Last Pose\n0.856\n3.336\n10.580\n0.784\n3.483\n11.460\nText + Random Pose\n0.671\n3.411\n10.760\n0.807\n3.173\n11.447\nB. Implementation Details\nMotion Data Pre-processing. We follow the dataset pre-\nprocessing procedures outlined in [1], [14], [20]. The raw\n3D motion coordinates are first aligned with a default human\nskeletal model. The Y-axis is then set perpendicular to the\nground, allowing individuals to face the Z+ direction. These\ncoordinates are then processed into motion features, including\nfoot contact, global rotations and translations, local joint\npositions, velocities, and 6D rotations. The final dimensions are\n263 for the HumanML3D dataset, 251 for the KIT-ML dataset,\nand 623 for the Motion-X dataset. For SMPL-based motion\nrepresentations, the maximum motion length is set to 196, with\nminimums of 40 for HumanML3D and 24 for KIT-ML. In\nSMPL-X-based Motion-X dataset, the maximum motion length\nframes is 300, with a minimum of 40.\nTraining Details. In the following experiments, we adopt the\ndecoder-only LLaMA 3.1-8B [34] model as the foundational\nLLM, keeping its parameters frozen while applying fine-tuning\nthrough the LoRA [26] method. The motion tokenizer is\ntrained over 1,200 epochs, with an initial learning rate of\n1 × 10−4. We set the mini-batch size to 256 and use the\nAdamW optimizer [72], with a weight decay of 1 × 10−5 for\nmodel optimization. Following previous studies [14], [17], we\nset the codebook Bm ∈R512×512 for motion VQ-VAE and\nthe codebook Bb ∈R512×512 and Bh ∈R512×512 for Part-\nAware VQ-VAE. The motion encoder applies a temporal down-\nsampling rate of 4. Our MotionGPT-2 leverages a learning\nrate of 2×10−4 during the pre-training phase and 1×10−4 in\nthe instruction tuning phase of the unified motion-language\nlearning process. The mini-batch size is 32. The pre-trained\nLLM undergoes 100 epochs in the Motion-Language Alignment\nphase and 50 epochs during the Instruction Tuning phase.\nMotionGPT-2 and the re-implemented models are built using\nPyTorch, with all experiments on 4 NVIDIA 80G A100 GPUs.\nC. Results on Human Motion Generation\nText-based Body-only Motion Generation. The quantitative\nresults of motion quality are depicted in Table I, Table II, and\nTable III. Among these, Table I and Table II provide quantitative\ncomparisons on the SMPL-based HumanML3D [20] dataset\nand the KIT-ML [63] dataset. The results presented reflect\nthe performance of MotionGPT-2, which has been pre-trained\nacross multiple motion-related tasks and subsequently fine-\ntuned for the specific text-based motion generation task. By\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nTABLE IV: Experiments of motion captioning task on the HumanML3D [20] benchmark. Results marked with * are from\nMotionGPT [17], and were computed using unprocessed ground truth texts for linguistic metrics.\nMethods\nR Precision↑\nMultiModal Dist.↓\nLengthavg↑\nBleu1↑\nBleu4↑\nRouge↑\nCider↑\nBertScore↑\nTop1\nTop2\nTop3\nReal Desc\n0.523\n0.725\n0.828\n2.901\n—\n—\n—\n—\n—\n—\nRAEs\n0.100\n0.188\n0.261\n6.337\n—\n33.3\n10.2\n37.5\n22.1\n10.7\nSeq2Seq(Att)\n0.436\n0.611\n0.706\n3.447\n—\n51.8\n17.9\n46.4\n58.4\n29.1\nSeqGAN\n0.332\n0.457\n0.532\n4.895\n—\n47.8\n13.5\n39.2\n50.2\n23.4\nTM2T*\n0.516\n0.720\n0.823\n2.935\n10.67\n48.9\n7.00\n38.1\n16.8\n32.2\nMotionGPT*\n0.543\n—\n0.827\n2.821\n13.04\n48.2\n12.5\n37.4\n29.2\n32.4\nMotioGPT-2 (Ours)\n0.558\n0.738\n0.838\n2.767\n15.27\n48.7\n13.8\n37.6\n29.8\n32.6\nwalking diagonally back and \nforth before stopping and \nfacing to the right.\nCan you generate a motion \nthat person walks forward \nthen jumps up and down？\na person, bent over, is \ndragging his right leg\nwhile trying to walk.\nCan you generate a motion that \na person walks forward, turns \nto the right and walks back？\nMethod\nT2M-GPT\nMotionGPT\nMotionGPT-2\nperson walks forwards two \nsteps, then turns around and \nwalks back to the starting point\na person walks forward \nslowly while holding both \narms straight up.\nMDM\nFig. 5: Showcase of visualization results for the text-based motion generation task using the HumanML3D [20] dataset. We\ncompare our MotionGPT-2 with the state-of-the-art method, i.e., MDM [59], T2M-GPT [14], MotionGPT [1]. Compared with\nthese methods, our MotionGPT-2 perform admirably to generate vivid human motions and preserve the semantic fidelity.\ntuning mere 1% of LLM parameters, our general-purpose\nMotionGPT-2 exhibits a performance that is competitive with\nstate-of-the-art approaches. Compared to our previous version,\nthe MotionGPT-2 yields a significant 10.4% improvement\nin R-Precision Top-3 and a 0.254 reduction in FID score\non the HumanML3D dataset. Through the tailored Motion-\nLanguage Alignment stage, our MotionGPT-2 exhibits greater\nsemantic consistency with the textual description, improving\ninterpretation of body language semantics. Further, compared\nto other language model-based methods [17], [18], [54], our\nMotionGPT-2 fine-tunes the LLM and relate general world\nknowledge to 3D human motions, achieving superior generation\ncapabilities beyond existing solutions.\nAs shown in Fig. 5 and Fig. 8, we further conduct visualiza-\ntion experiments on the text-based motion generation task to\nvividly illustrate the capabilities of our MotionGPT-2 model.\nIn Fig. 5, we observe that the diffusion-based MDM [59]\nmethod generates fewer semantic motions aligned with the\nprovided descriptions. By utilizing discrete motion tokens,\nT2M-GPT [14] can better learn motion patterns and semantics\nwhich leads to more coherent and contextually rich motions.\nOur conference version MotionGPT [1], with fine-tuned LLMs,\ngeneralizes well to more diverse and complex text prompts.\nIn contrast to MotionGPT, our MotionGPT-2 extends motion\nvocabulary to original LLMs for jointly modeling motion and\nlanguage representations in a unified space. As demonstrated\nby the additional visualization results in Fig. 8, MotionGPT-2\nshows high-fidelity motion and strong text prompt matching,\nwhich validates the effectiveness of our proposed method.\nMultiple Signal Controlled Body-only Motion Generation.\nBesides text inputs, MotionGPT-2 is capable of incorporating\nhuman poses as an additional control modality, with the\nresulting motion quality detailed in Table III. Introducing\nextra controls, such as initial, last, or key poses, does not\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nTABLE V: Ablations on the effects of LLM types and scales on text-based motion generation, evaluated on the HumanML3D [20]\nbenchmark. In addition to full fine-tuning of the encoder-decoder T5-base model, LoRA-based fine-tuning [26] is used for\noptimizing other decoder-only LLMs.\nModel\nTrainable\nR-Precision ↑\nFID ↓\nMultiModal\nDiversity →\nMultiModality ↑\nParameters\nTop 1\nTop 2\nTop 3\nDist. ↓\nReal\n—\n0.511\n0.703\n0.797\n0.002\n2.974\n9.503\n—\nT5-base (Pretrain)\nFull-Finetune\n0.314\n0.457\n0.554\n0.493\n4.662\n9.634\n1.793\nT5-base (Finetune)\nFull-Finetune\n0.417\n0.581\n0.668\n0.148\n3.989\n9.961\n1.985\nGemma-2b-It (Pretrain)\n34M\n0.385\n0.546\n0.643\n0.207\n4.559\n9.733\n2.351\nGemma-2b-It (Finetune)\n34M\n0.436\n0.600\n0.697\n0.228\n3.589\n10.081\n2.269\nGemma-7b-It (Pretrain)\n101M\n0.404\n0.575\n0.673\n0.219\n3.768\n9.964\n2.364\nGemma-7b-It (Finetune)\n101M\n0.446\n0.622\n0.715\n0.177\n3.545\n9.652\n2.198\nLLaMA3-8B (Pretrain)\n89M\n0.438\n0.619\n0.717\n0.314\n3.514\n10.010\n2.252\nLLaMA3-8B (Finetune)\n89M\n0.482\n0.668\n0.760\n0.282\n3.288\n10.212\n2.261\nLLaMA3.1-8B (Pretrain)\n89M\n0.456\n0.630\n0.732\n0.291\n3.417\n9.884\n2.145\nLLaMA3.1-8B (Finetune)\n89M\n0.496\n0.691\n0.782\n0.191\n3.080\n9.860\n2.137\na person turns \naround, sits down, \nand leans back.\na person walks forward, \npicks something up off of \nthe floor, then puts it on a \nlow shelf or object.\nInput\nMotions\nReal\nMotionGPT\nOurs\nperson starts out with both \narms extended, takes two steps \nforwards, wipes something \nvigorously using right hand\na person throws \nsomething, then \ncatches something, \nthen throws it\nperson looks like he is \nthrowing a baseball \nthen catches one and \nthrows it again\na person turns and \nsits in a chair, \nthen gets back up.\na person is walking \nforward, grabs \nsomething, turns around, \nand sit the object down.\na person walks two steps \nforward and makes a \nscrubbing motion with \nhis right hand.\na person slowly \nturns around and \nthen lies down\na person walks forward. \nthey stop and bend down \nto pick something up, \nthen continue walking. \na person walks in \nan arc to the \nleft then stops.\nperson walks \nforward and \ncurves to the left. \nperson is standing \nstill, slightly \nshifting their \nweight to the left.\na person throws \nsomething and then \ncatches something.\na person takes two \nsteps forward and \nthen uses their hand \nto clean something.\nthe toon is walking in a \nzig zag motion, \nshuffling a bit at the \nend of the \"plane\"\na man strolls slowly, \nand staggers to the \nleft in a zig zag \npattern\nthis person walks in \na zig zag motion \nfrom back to front\nFig. 6: Comparison of the state-of-the-art method on the motion captioning task. The results demonstrate that our MotionGPT-2\noutperforms the MotionGPT on the HumanML3D [20], generating more conceptually and semantically rich motion descriptions.\nSpecific words are marked to highlight the semantic similarity of the generated captions and the real one. Best viewed in color.\ndegrade motion quality. In fact, MotionGPT-2 shows superior\nperformance when initial or key tokens are provided, achieving\nFID scores of 0.183 or 0.182, an improvement over the text-only\nmodel’s 0.191 on the HumanML3D benchmark, demonstrating\nits flexibility in handling diverse control modalities.In fact,\nMotionGPT-2 shows superior performance when initial or\nkey tokens are provided, achieving FID scores of 0.183 or\n0.182, an improvement over the text-only model’s 0.191 on\nthe HumanML3D benchmark. In spite of this, MotionGPT-\n2’s performance is still notable, reinforcing its proficiency in\ngenerating high-quality and diverse motions across a range\nof control conditions. As demonstrated in Fig. 9, the motions\nproduced by our model align closely with the specified poses\nand consistently follow the textual descriptions.\nHolistic Motion Generation. In this part, we focus on the\nholistic motion generation task. As shown in Table XI, we\nevaluate various LLMs on the SMPL-X-based Motion-X [28]\ndataset. Our tailored PA-VQVAE consistently outperforms\nthe original VQVAE across multiple scales and types of\nLLMs, demonstrating the effectiveness of our innovative\nmotion discretization framework. For instance, compared to\nits VQVAE counterpart, Part-Aware VQVAE (LLaMA 3.1-8B)\nachieves an improvement in Top-1 R-Precision from 0.332\nto 0.349, and a decrease in FID from 0.666 to 0.619. The\nsuperior performance of Part-Aware VQVAE over the original\nVQVAE can be attributed to its more fine-grained discretization\nrepresentation and hierarchical modeling capability for human\nmotion. Moreover, utilizing distinct motion vocabularies for\nbody and hand reduces the ambiguity, where similar actions\ncould be represented by the same token. As shown in Fig. 7,\nour MotionGPT-2 is capable of generating vivid motions that\naccurately correspond to the given text descriptions, particularly\nhighlighting subtle hand movements such as playing the piano,\nflying a kite, and stapling papers.\nD. Results on Motion Captioning\nCompared to the text-to-motion task, the motion-to-text\ntask involves generating a textual description from a given\nhuman motion sequence. As in [17], we rely on ground truth\ndescriptions for more accurate assessment. Table IV presents the\nresults of quantitative evaluation for motion-to-text translation\non the HumanML3D dataset. The comparisons in Table IV\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\nA woman wears a \nKatana Handschuh.\nThe person is walking and \nmaking the OK gesture.\nThe girl plays the piano. A man is walking while \nshaking hands and \nsaying goodbye.\nThe person performs a \nHammer Slam. \nA person is walking while \nclenching their fists. \nThe person performs a Plank \nWalk Lift with their arms.\nA woman pushes the door \nto enter and walks in at \nthe same time.\nThe camera takes \na picture.\nA woman staples \nwith a stapler.\nThe woman performs \na Battle-rope Rainbow \nexercise routine.\nthis person jogs \nquickly moving forward\nsomeone makes an I Love \nYou gesture while walking.\nA person is \nsimultaneously flying a \nkite and standing.\nSomeone does \nsmall arm circles.\nFig. 7: Qualitative results of our proposed method on the Motion-X [28] dataset. Utilizing the world knowledge of LLMs, our\nMotionGPT-2 demonstrates the capability to generate realistic body motions while effectively capturing lifelike hand interactions,\ne.g., making the OK gesture, plays the piano, saying goodbye.\nTABLE VI: Ablations on the effects of LLM types and scales on text-based motion generation, evaluated on the KIT-ML [63]\nbenchmark. Along with full fine-tuning of the T5-base model, LoRA-based fine-tuning [26] is used for optimizing other LLMs.\nModel\nTrainable\nR-Precision ↑\nFID ↓\nMultiModal\nDiversity →\nMultiModality ↑\nParameters\nTop 1\nTop 2\nTop 3\nDist. ↓\nReal\n—\n0.424\n0.649\n0.779\n0.031\n2.788\n11.080\n—\nT5-base\nFull-Finetune\n0.358\n0.575\n0.683\n0.983\n3.508\n10.877\n2.328\nGemma 2B-It\n34M\n0.364\n0.581\n0.699\n1.063\n3.424\n10.603\n2.150\nGemma 7B-It\n101M\n0.385\n0.596\n0.730\n0.956\n3.333\n10.951\n2.416\nLLaMA 3-8B\n89M\n0.394\n0.605\n0.734\n0.816\n3.214\n11.055\n2.167\nLLaMA 3.1-8B\n89M\n0.427\n0.627\n0.764\n0.614\n3.164\n11.256\n2.357\nshows that our proposed MotionGPT-2 outperforms recent\nworks in generating text descriptions for the given motions. The\ngenerated language descriptions deliver substantial improve-\nments in both linguistic quality (BLEU [66] and BertScore [69])\nand the precision of motion retrieval (R-Precision). By fine-\ntuning LLMs, our MotionGPT-2 emerges as a specialized tool\nendowed with extensive world knowledge, thereby enhancing\nits capacity to interpret human motion.\nIn Fig. 6, we provide further examples of translating motion\nto text using the HumanML3D dataset. All methods are\nevaluated under the same training and inference conditions on\nHumanML3D. Compared to MotionGPT [17], our MotionGPT-\n2, which leverages LLM-interpretable motion tokens, is capable\nof generating more conceptual and semantically rich motion\ndescriptions. For instance, it can produce descriptions such\nas “walk in an arc shape”, “throwing a baseball”, “sits in a\nchair”, offering a clearer and more intuitive understanding of\nthe given motion motions.\nE. Results on Generalized Motion Completion\nFollowing [17], we classify both motion prediction and\nin-betweening together as generalized motion completion in\nTable X. For the motion prediction task [70], [71], [73], only\nthe first 20% of the sequence is used as the conditioning\ninput, while approximately 50% of the motion is intentionally\nmasked at random to assist in the completion process. Similar\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\nThe figure walks from the bottom right \nto the top left of the square, and \nbends down as if picking something up \ntwice.  then it turns around.\na person walks in a clock wise \ncircle then goes to the left a \nfew steps and then goes to the \nright a few steps\nCan you generate a motion that the \nperson was walking clockwise？\na person is bend over with \ntheir arms behind them, then \nswings them around, pauses to \nrise slightly\na person runs from one side \nof the screen to another\na person is holding his arms \nstraight out to the sides, then \nlowering them, claps, and \nsteps forward to sit in a chair.\na person walking down a flight \nof stairs in person walks up \nthen takes a large step to their \nleft and then goes back onto \nthe same path they were on.\na figure balances on one leg \nwhile holding the right leg \nwith the right hand before \nhopping on the left leg\na man walks forward and picks \nup an object with his right \nhand, then puts the object \nback down and steps backward\na person slowly walked \nforward and walked in the \nright direction？\nCan you generate a motion \nthat a person is acting like a \nsquirrel？\na man gets up from the ground \npushing off with his right hand \nthen walks in a counter \ncounterclockwise circle back to \nwhere he began\nCan you generate a motion \nthat a person walks in an s \nshape pattern？\nthe man is kneeling and waving \nhis hands in the air, stands, \nturns and walks away.\na person slowly walked forward, \nand after turned back and \nkeep walked\na person hops to the right, \nthen to the left, then right, \nthen left again.\nCan you generate a motion \nthat a person walks forward \nand steps over something？\nCan you generate a motion that \nthe person is dancing around？\nFig. 8: More text-based human motion samples generated by our proposed MotionGPT-2 (with LLaMA 3.1-8B) using texts\nfrom the HumanML3D test set. Our method effectively generates a diverse range of dynamic and imaginative motions, e.g.,\n“acting like a squirrel”, “walking in an S-shaped pattern”, and “dancing around”.\nInitial token\nRecon\nVel\nRecon\nVel\nMotionGPT\nMotionGPT-2\nText-only\n24.70\n1.095\n19.19\n0.846\nText + Initial poses\n13.78\n0.549\n7.59\n0.328\nLast token\nMotionGPT\nMotionGPT-2\nText-only\n19.70\n1.172\n11.77\n0.735\nText + Last poses\n6.831\n0.397\n4.376\n0.291\nKey tokens\nMotionGPT\nMotionGPT-2\nText-only\n8.035\n3.813\n6.591\n1.932\nText + Random poses\n5.383\n2.423\n4.139\n1.055\nTABLE VII: Evaluation of the consistency with pose control\nconditions on the HumanML3D [20] test sub-set using the\npre-trained LLaMA 3.1-8B model. In contrast to text-only\ngeneration, the incorporation of pose conditions enhances the\nconsistency of key-frame poses during the generation process.\nto text-based motion generation, as shown in Table X, we\nalso fine-tune the proposed MotionGPT-2 specifically for this\nspecific task and utilize FID, ADE, and FDE as metrics.\nThe unified motion-language framework of the MotionGPT-\n2 leverages contextual information of fine-tuned LLM to\nunderstand in-depth motion dynamics. Compared to [17], [59],\nour MotionGPT-2 demonstrates a remarkable capability to\ngenerate contextually appropriate motion completions.\nF. Ablation Study\nCapability of Pre-trained LLM. As demonstrated in Ta-\nble V and Table VI, we delve into how different scales and types\nof LLMs affect the performance of text-based human motion\ngeneration tasks on the HumanML3D [20] and KIT-ML [63]\ndatasets. We observe that: (1) Larger LLMs (e.g., LLaMA 3.1-\n8B and LLaMA 3-8B) offer distinct advantages over smaller\ncounterparts, achieving significant improvements in fidelity\n(FID) and multimodal alignment (i.e., R Precision, MultiModal\nDist.). To explain, the improved context understanding of\nLLMs ensures that the output motion aligns closely with\nintended actions. Further, LLMs with comprehensive world\nknowledge can synthesize physically plausible motions even\nwhen faced with linguistic ambiguity. (2) LLMs fine-tuned by\nunified instructions demonstrate clear advantage in maintaining\nsemantic consistency and producing motions that are better\naligned with textual descriptions. For instance, the fine-tuned\nGemma-7B-It outperforms its pre-trained counterpart, achieving\na 10% improvement in R-Precision Top-3 (from 0.673 to\n0.715) and a 19% reduction in FID (from 0.219 to 0.177). (3)\nCompared to T5-base used in [17], [18], which requires full\nfine-tuning, fine-tuning the decoder-only LLaMA 3.1-8B yields\na higher R-Precision Top-3 (0.782) and a lower FID (0.191).\nThis indicates that larger models with more parameters possess\na greater capacity to capture complex relationships between\ntext and motion, enhancing both precision and diversity.\nConsistency with pose control conditions. We assess\nthe benefits of pose control by comparing the consistency\nbetween the controlled poses and the generated motions on\nthe HumanML3D test sub-set. Concerning each specific task\n(initial\/last\/key), motions are generated both with and without\npose controls, utilizing the (text+pose)-to-motion and text-\nto-motion methods, respectively. The results, displayed in\nTab. VII indicate that key-frame consistency is higher with\npose controls than in text-only generation counterpart, proving\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\nTABLE VIII: Comparisons between separate training for each task and joint training for multiple tasks on HumanML3D [20]\ntest set using the LLaMA 3-8B model. We use orange and green markings to represent decrements and improvements in the\nmetric, respectively. Joint training can achieve better performance for all tasks.\nTask\nTraining\nFID ↓\nMultiModal Dist.↓\nR-Precision ↑\nDiversity ↑\nStrategy\nTop-1\nTop-2\nTop-3\nText\nSeparate\n0.523\n3.627\n0.358\n0.514\n0.604\n9.108\n+ Initial token\n0.483\n3.489\n0.378\n0.549\n0.647\n9.614\n+ Last token\n0.974\n4.208\n0.339\n0.501\n0.598\n9.598\n+ Key tokens\n0.428\n3.276\n0.424\n0.617\n0.697\n9.929\nText\nJoint\n0.482(-0.041)\n3.295(-0.332)\n0.419(+0.061)\n0.597(+0.083)\n0.683(+0.079)\n9.422(+0.314)\n+ Initial token\n0.454(-0.029)\n3.173(-0.316)\n0.434(+0.056)\n0.613(+0.064)\n0.710(+0.063)\n9.573(-0.041)\n+ Last token\n0.507(-0.467)\n3.860(-0.348)\n0.427(+0.088)\n0.608(+0.107)\n0.723(+0.125)\n9.688(+0.090)\n+ Key tokens\n0.406(-0.022)\n3.459(-0.183)\n0.445(+0.021)\n0.616(-0.001)\n0.722(+0.025)\n9.987(+0.058)\nTABLE IX: Evaluation of text-based human motion generation\nusing the LLaMA 3-8B model with various prompts on the\nHumanML3D [20] test subset.\nPrompts\nFID ↓MultiModal\nR-Precision ↑\nDiversity ↑\nDist. ↓\nTop-1 Top-2 Top-3\nV1\n4.196\n5.275\n0.357\n0.542\n0.658\n8.110\nV2\n2.692\n4.573\n0.418\n0.603\n0.719\n8.534\nV0 (Ours)\n0.191\n3.080\n0.482\n0.669\n0.760\n9.860\nthe effectiveness of (text+pose)-to-motion with pose control.\nSuch results highlight the critical role that pose controls play in\ncoherent and contextually appropriate human motion synthesis.\nComparison with Separate Training. We carry out task-\nspecific training on the HumanML3D dataset [20] to test\nthe effectiveness of the proposed MotionGPT-2 model in\nmotion generation. This experimental setup is implemented\nto determine whether a multi-task learning framework can\nenhance the performance of each distinct control condition\nindependently. The comparison results are presented in Ta-\nble VIII. Our findings indicate that joint training across all tasks\nsignificantly improves performance metrics across the board.\nNotably, this enhancement is particularly pronounced when\nutilizing text and the last pose token as input conditions. These\nresults illustrate the value of our multi-modal signals controlled\nmotion generation. MotionGPT-2’s ability to generate motions\nunder a specific input condition is strengthened by drawing\nknowledge from other conditions.\nHyper-parameters of LoRA. The LoRA [26] method\nprovides the source for all trainable parameters during training,\nwith two key hyper-parameters: r and α. The rank of LoRA\nparameters is represented by r, with lower values corresponding\nto fewer parameters. α adjusts the scale of the dense layer’s\noutputs in LoRA. According to Table XII, we find that\nincreasing r, while holding α constant, improves our model’s\nperformance on nearly all metrics. Keeping the scale factor\nα\nr equivalent to the learning rate demonstrates that increasing\nr yield better results. Moreover, we observe that adjusting α\nwith a fixed r gives optimal performance when α = 16.\nEvaluation of Prompt Design. LLM are highly responsive\nto the way prompts are structured, which underscores the\nimportance of meticulously crafting prompts to enhance the\neffectiveness of the model. This section explores the influence\nof using two alternative prompt and evaluates their individual\nperformances. We refer to the prompt utilized in our model as\nV0, while also presenting two supplementary prompts, V1 and\nV2 as follows:\n% Prompts V1\nHuman motion can be represented by token indices by\nVQ-VAE. Below is an instruction that describes human\nmotion generation condition types, paired with an input\nthat provides specific conditions. Write a sequence of\ntokens matching with given conditions.\nInstruction (I) : {Task Prompts: ”Motion description(\nand the init\/last\/key pose tokens).”} {Control Conditions:\nText control condition X(< Motion Token > Pose control\nconditions P < Motion Token >) }\n% Prompts V2\nBelow is an instruction that describes a task, paired with\nan input that provides further context. Write a response\nthat appropriately completes the request.\nInstruction (I) : {Task Prompts: “Generate the token\nsequence of the motion description (under the premise of\ngiven init\/last\/key pose tokens).”} {Control Conditions:\nText control condition X( <Motion Token> Pose control\nconditions P < Motion Token >) }\nRegarding the prompts V1,we embed specific details regard-\ning human motion generation into the overall descriptions,\nsimplifying the task prompts to concentrate on the types of\nconditions alone. In contrast, we revise the task prompts for the\nV2, we modified the expression of the task prompts. Table IX\npresents the comparative results, illustrating the effectiveness\nand importance of the proposed prompt designs.\nVI. CONCLUSION AND LIMITATIONS\nA. Conclusion\nIn this paper, we introduce the MotionGPT-2, a versatile\nLarge Motion-Language Model (LMLM), which can generate\nand comprehend human motions with general world knowledge\nof LLMs. Notably, MotionGPT-2 unifies motion-related tasks\nwith multi-modal control signals (e.g., text and single-frame\nposes) as input by discretizing pose conditions and creating\na unified set of instructions from combined textual and pose\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\nText + Initial Motion Token\nText + Last Motion Token\nText + Random Key Tokens\nwalking a one-sided jagged circle \nclockwise with arms swaying\nthe man walks in a circle twice, \nbeginning with his left foot \nand ending on his right\na person trying to walk in \nhigh heels, but unable to \ndo so steadily\nthe person was facing forward \nand then turn to the side \nfacing sideways\na person walks briskly in a \ncounterclockwise direction with \narms swaying at side\na person slowly and quietly \nsneaks down the hallway\na person walks slightly \nright and walks forward\nThe man jogs in place quickly \nbefore squatting down and \ndoing a walking squat.\na man reaches to something \nplaces it on the table then \nreaches for another thing.\na man walks forward while \nswinging his shoulders \nfrom side to side.\na person walks in a clockwise \nhalf circle, then climbs stairs \nwith both arms and legs.\na person starts walking \nthen starts leaping \ncounterclockwise\na person walks forward, bends \nover, picks up an object with \nboth hands, then turns around \nand walks back with the object.\na motion that a person walks \nwhile holding a hand rail\na person moves forward quickly \nand lifts both legs before landing \nand continuing to move forward.\nCan you generate a motion \nthat a figure carefully tip \ntoes across a path?\na man walks forward sits \nin a chair then with his \nleft hand.\na motion that starting with \ntheir left foot, a person \nproceeds down a set of stairs.\nCan you generate a motion \nthat a person walking \nforward in lunges?\nA person bends down then \nproceeds to pretend throw \nsomething underhand\nthe person is walking very \ncarefully over a balance beam.\nCan you generate a motion that \nperson walks to the right then \nturns around and walks left?\na person walks forward, \nthen steps up some stairs.\nCan you generate a \nmotion that the person is \nplaying soccer?\nFig. 9: Gallery showcasing the results of generated human motions by MotionGPT-2 with multiple control conditions on the\nHumanML3D dataset [20], i.e., Text+Initial Motion Token, Text+Last Motion Token, and Text+Random Key Motion Token. With\nthese diverse control signals, our MotionGPT-2 demonstrates the ability to generate physically realistic human motions.\nTABLE X: Evaluation of motion prediction and in-betweening on part of the AMASS [62] dataset, considering only motion\ndata. FID reflects the quality of the generated motions, while Diversity quantifies the motion variability within each condition.\nADE and FDE represent the distance between generated joint positions and the ground truth.\nMethods\nMotion Prediction\nMotion In-between\nFID ↓\nDiversity↑\nADE↓\nFDE↓\nFID ↓\nDiversity↑\nADE↓\nReal\n0.002\n9.503\n—\n—\n0.002\n9.503\n—\nMDM [59]\n6.031\n7.813\n5.446\n8.561\n2.698\n8.420\n3.787\nMotionGPT [17]\n0.905\n8.972\n4.745\n6.040\n0.214\n9.560\n3.762\nMotionGPT-2 (Ours)\n0.537\n9.414\n4.512\n5.823\n0.408\n9.327\n3.704\nprompts. By constructing a unified motion-language vocabulary\nof LLM, we empower the pre-trained LLMs with the ability\nto integrate the understanding and generation of body kinetics.\nWith well-designed Part-Aware VQVAE, MotionGPT-2 also\ndemonstrates its versatility in addressing the complex 3D whole-\nbody motion generation task, establishing a strong benchmark\nfor researchers. We envision that MotionGPT-2 paves the way\nfor more practical and versatile motion generation systems,\noffering a fresh perspective in the field.\nB. Limitations\nOur MotionGPT-2 primarily focuses on high-level semantic\nalignment between textual descriptions and motions. However,\nit captures finer semantic levels—such as the subtleties of\nbody language, gestures, and contextual cues—less effectively.\nIn the future, we will integrate visual data, e.g., videos, for\nphysically realistic motion generation. Another limitation of\nour proposed MotionGPT-2 is its deficiency in interpreting and\nresponding to dynamic environments or contextual interactions\nbetween humans and their surroundings. Future research aims\nto endow the MotionGPT-2 model with scene understanding\nand perception capabilities.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n15\nTABLE XI: Ablation study on the effects of the Part-Aware VQVAE (PA-VQVAE) on text-based motion generation, evaluated\non Motion-X [28]. Along with full fine-tuning of the T5-base, LoRA-based fine-tuning [26] is used for optimizing other LLMs.\nModel\nTrainable\nR-Precision ↑\nFID ↓\nMultiModal\nDiversity →\nMultiModality ↑\nParameters\nTop 1\nTop 2\nTop 3\nDist. ↓\nReal\n—\n0.473\n0.611\n0.686\n0.002\n4.225\n6.821\n—\nVQVAE (T5-base)\nFull-Finetune\n0.332\n0.452\n0.533\n0.639\n5.250\n6.551\n2.420\nPA-VQVAE (T5-base)\nFull-Finetune\n0.349\n0.473\n0.544\n0.628\n5.164\n6.596\n2.322\nVQVAE (Gemma-2B-It)\n34M\n0.334\n0.467\n0.552\n0.639\n5.038\n6.848\n2.311\nPA-VQVAE (Gemma-2B-It)\n34M\n0.348\n0.475\n0.571\n0.628\n5.011\n6.764\n2.413\nVQVAE (Gemma-7B-It)\n101M\n0.343\n0.487\n0.576\n0.612\n5.039\n6.438\n2.258\nPA-VQVAE (Gemma-7B-It)\n101M\n0.377\n0.507\n0.583\n0.620\n4.819\n6.267\n2.146\nVQVAE (LLaMA 3-8B)\n89M\n0.374\n0.506\n0.587\n0.644\n4.752\n6.577\n2.053\nPA-VQVAE (LLaMA 3-8B)\n89M\n0.389\n0.512\n0.593\n0.628\n4.754\n6.538\n2.002\nVQVAE (LLaMA 3.1-8B)\n89M\n0.387\n0.521\n0.601\n0.666\n4.673\n6.446\n2.352\nPA-VQVAE (LLaMA 3.1-8B)\n89M\n0.398\n0.522\n0.616\n0.619\n4.656\n6.574\n2.821\nTABLE XII: Performance evaluation of text-to-motion generation with various LoRA parameters on the HumanML3D test\nset using LLaMA 3-8B. The best result is highlighted in bold, and the second best result is underlined.\nr\nα\nFID ↓\nMultiModal Dist ↓\nR-Precision ↑\nDiversity ↑\nTop-1\nTop-2\nTop-3\n8\n16\n0.262\n3.309\n0.459\n0.645\n0.743\n9.589\n16\n16\n0.257\n3.168\n0.472\n0.658\n0.755\n9.916\n32\n16\n0.191\n3.080\n0.482\n0.669\n0.760\n9.860\n8\n2\n0.762\n3.619\n0.418\n0.586\n0.697\n9.523\n16\n4\n0.294\n3.302\n0.431\n0.645\n0.744\n9.840\n32\n8\n0.217\n3.225\n0.477\n0.658\n0.742\n9.885\n64\n8\n0.985\n4.142\n0.373\n0.567\n0.669\n8.451\n64\n32\n0.651\n3.882\n0.429\n0.618\n0.705\n9.398\n64\n16\n0.256\n3.284\n0.465\n0.652\n0.747\n9.862\nREFERENCES\n[1] Y. Zhang, D. Huang, B. Liu, S. Tang, Y. Lu, L. Chen, L. Bai, Q. Chu,\nN. Yu, and W. Ouyang, “Motiongpt: Finetuned llms are general-purpose\nmotion generators,” 2023.\n[2] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,\nK. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans\net al., “Photorealistic text-to-image diffusion models with deep language\nunderstanding,” Advances in Neural Information Processing Systems,\nvol. 35, pp. 36 479–36 494, 2022.\n[3] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan,\nA. Ku, Y. Yang, B. K. Ayan et al., “Scaling autoregressive models for\ncontent-rich text-to-image generation,” 2022.\n[4] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical\ntext-conditional image generation with clip latents,” 2022.\n[5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion models,” in\nProceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 10 684–10 695.\n[6] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,\nand I. Sutskever, “Zero-shot text-to-image generation,” in International\nConference on Machine Learning.\nPMLR, 2021, pp. 8821–8831.\n[7] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,” Advances in Neural\nInformation Processing Systems, vol. 35, pp. 27 730–27 744, 2022.\n[8] Z. Lu, D. Huang, L. Bai, X. Liu, J. Qu, and W. Ouyang, “Seeing\nis not always believing: A quantitative study on human perception of\nai-generated images,” 2023.\n[9] M. Petrovich, M. J. Black, and G. Varol, “Temos: Generating diverse\nhuman motions from textual descriptions,” in Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXII.\nSpringer, 2022, pp. 480–497.\n[10] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu,\n“Motiondiffuse: Text-driven human motion generation with diffusion\nmodel,” 2022.\n[11] G.\nTevet,\nS.\nRaab,\nB.\nGordon,\nY.\nShafir,\nD.\nCohen-or,\nand\nA. H. Bermano, “Human motion diffusion model,” in The Eleventh\nInternational Conference on Learning Representations, 2023. [Online].\nAvailable: https:\/\/openreview.net\/forum?id=SJ1kSyO2jwu\n[12] M. Petrovich, M. J. Black, and G. Varol, “Action-conditioned 3d human\nmotion synthesis with transformer vae,” in Proceedings of the IEEE\/CVF\nInternational Conference on Computer Vision, 2021, pp. 10 985–10 995.\n[13] W.\nZhuang,\nC.\nWang,\nJ.\nChai,\nY.\nWang,\nM.\nShao,\nand\nS. Xia, “Music2dance: Dancenet for music-driven dance generation,”\nACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM), vol. 18, no. 2, pp. 1–21, 2022.\n[14] J. Zhang, Y. Zhang, X. Cun, S. Huang, Y. Zhang, H. Zhao, H. Lu, and\nX. Shen, “T2m-gpt: Generating human motion from textual descrip-\ntions with discrete representations,” in Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[15] X. Chen, B. Jiang, W. Liu, Z. Huang, B. Fu, T. Chen, and G. Yu,\n“Executing your commands via motion diffusion in latent space,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 18 000–18 010.\n[16] C. Guo, Y. Mu, M. G. Javed, S. Wang, and L. Cheng, “Momask:\nGenerative masked modeling of 3d human motions,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2024, pp. 1900–1910.\n[17] B. Jiang, X. Chen, W. Liu, J. Yu, G. Yu, and T. Chen, “Motiongpt:\nHuman motion as a foreign language,” Advances in Neural Information\nProcessing Systems, vol. 36, pp. 20 067–20 079, 2023.\n[18] M. Luo, R. Hou, H. Chang, Z. Liu, Y. Wang, and S. Shan, “M3gpt: An\nadvanced multimodal, multitask framework for motion comprehension\nand generation,” arXiv preprint arXiv:2405.16273, 2024.\n[19] M. Zhang, X. Guo, L. Pan, Z. Cai, F. Hong, H. Li, L. Yang, and\nZ. Liu, “Remodiffuse: Retrieval-augmented motion diffusion model,” in\nProceedings of the IEEE\/CVF International Conference on Computer\nVision, 2023, pp. 364–373.\n[20] C. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, and L. Cheng, “Generating\ndiverse and natural 3d human motions from text,” in Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n16\n2022, pp. 5152–5161.\n[21] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing\nvision-language understanding with advanced large language models,”\n2023.\n[22] Y. Du, K. Konyushkova, M. Denil, A. Raju, J. Landon, F. Hill,\nN. de Freitas, and S. Cabi, “Vision-language models as success detectors,”\n2023.\n[23] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.\n[24] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi,\nY. Shi, C. Jiang, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and\nF. Huang, “mplug-owl: Modularization empowers large language models\nwith multimodality,” 2023.\n[25] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and\nY. Qiao, “Videochat: Chat-centric video understanding,” 2023.\n[26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and\nW. Chen, “Lora: Low-rank adaptation of large language models,” 2021.\n[27] M. Plappert, C. Mandery, and T. Asfour, “The kit motion-language\ndataset,” Big data, vol. 4, no. 4, pp. 236–252, 2016.\n[28] J. Lin, A. Zeng, S. Lu, Y. Cai, R. Zhang, H. Wang, and L. Zhang,\n“Motion-x: A large-scale 3d expressive whole-body human motion dataset,”\nAdvances in Neural Information Processing Systems, vol. 36, 2024.\n[29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” 2018.\n[30] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving\nlanguage understanding by generative pre-training,” 2018.\n[31] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[32] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language\nmodels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[33] OpenAI, “Gpt-4 technical report,” 2023.\n[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,\nB. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and\nefficient foundation language models,” 2023.\n[35] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,” The Journal of Machine Learning\nResearch, vol. 21, no. 1, pp. 5485–5551, 2020.\n[36] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-\nefficient prompt tuning,” 2021.\n[37] X. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang, “P-tuning\nv2: Prompt tuning can be comparable to fine-tuning universally across\nscales and tasks,” 2021.\n[38] S. Hu, N. Ding, H. Wang, Z. Liu, J. Wang, J. Li, W. Wu, and M. Sun,\n“Knowledgeable prompt-tuning: Incorporating knowledge into prompt\nverbalizer for text classification,” 2021.\n[39] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\nA. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer\nlearning for nlp,” in International Conference on Machine Learning.\nPMLR, 2019, pp. 2790–2799.\n[40] R. He, L. Liu, H. Ye, Q. Tan, B. Ding, L. Cheng, J.-W. Low, L. Bing,\nand L. Si, “On the effectiveness of adapter-based tuning for pretrained\nlanguage model adaptation,” 2021.\n[41] H. Le, J. Pino, C. Wang, J. Gu, D. Schwab, and L. Besacier, “Lightweight\nadapter tuning for multilingual speech translation,” 2021.\n[42] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or,\n“Motionclip: Exposing human motion generation to clip space,” in\nComputer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part XXII.\nSpringer, 2022,\npp. 358–374.\n[43] I. Habibie, D. Holden, J. Schwarz, J. Yearsley, and T. Komura, “A\nrecurrent variational autoencoder for human motion synthesis,” in\nProceedings of the British Machine Vision Conference (BMVC), 2017.\n[44] Z. Li, Y. Zhou, S. Xiao, C. He, Z. Huang, and H. Li, “Auto-conditioned\nrecurrent networks for extended complex human motion synthesis,” 2017.\n[45] C. Guo, X. Zuo, S. Wang, S. Zou, Q. Sun, A. Deng, M. Gong,\nand L. Cheng, “Action2motion: Conditioned generation of 3d human\nmotions,” in Proceedings of the 28th ACM International Conference on\nMultimedia, 2020, pp. 2021–2029.\n[46] R. Li, S. Yang, D. A. Ross, and A. Kanazawa, “Ai choreographer:\nMusic conditioned 3d dance generation with aist++,” in Proceedings of\nthe IEEE\/CVF International Conference on Computer Vision, 2021, pp.\n13 401–13 412.\n[47] E. Barsoum, J. Kender, and Z. Liu, “Hp-gan: Probabilistic 3d human\nmotion prediction via gan,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition workshops, 2018, pp. 1418–\n1427.\n[48] J. Martinez, M. J. Black, and J. Romero, “On human motion prediction\nusing recurrent neural networks,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2017, pp. 2891–2900.\n[49] K. Kania, M. Kowalski, and T. Trzci´nski, “Trajevae: Controllable human\nmotion generation from trajectories,” 2021.\n[50] A. Ghosh, N. Cheema, C. Oguz, C. Theobalt, and P. Slusallek, “Synthesis\nof compositional animations from textual descriptions,” in Proceedings\nof the IEEE\/CVF International Conference on Computer Vision, 2021,\npp. 1396–1406.\n[51] C. Ahuja and L.-P. Morency, “Language2pose: Natural language grounded\npose forecasting,” in 2019 International Conference on 3D Vision (3DV).\nIEEE, 2019, pp. 719–728.\n[52] C. Guo, X. Zuo, S. Wang, and L. Cheng, “Tm2t: Stochastic and tokenized\nmodeling for the reciprocal generation of 3d human motions and texts,”\nin Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part XXXV.\nSpringer, 2022,\npp. 580–597.\n[53] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nconference on machine learning.\nPMLR, 2021, pp. 8748–8763.\n[54] Q. Wu, Y. Zhao, Y. Wang, Y.-W. Tai, and C.-K. Tang, “Motionllm:\nMultimodal motion-language learning with large language models,” arXiv\npreprint arXiv:2405.17013, 2024.\n[55] A. Van Den Oord, O. Vinyals et al., “Neural discrete representation\nlearning,” Advances in neural information processing systems, vol. 30,\n2017.\n[56] S. Lu, L.-H. Chen, A. Zeng, J. Lin, R. Zhang, L. Zhang, and H.-Y.\nShum, “Humantomato: Text-aligned whole-body motion generation,”\narXiv preprint arXiv:2310.12978, 2023.\n[57] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu, “Mo-\ntiondiffuse: Text-driven human motion generation with diffusion model,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.\n[58] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\nA. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot\nlearners,” 2021.\n[59] G.\nTevet,\nS.\nRaab,\nB.\nGordon,\nY.\nShafir,\nD.\nCohen-or,\nand\nA. H. Bermano, “Human motion diffusion model,” in The Eleventh\nInternational Conference on Learning Representations, 2023. [Online].\nAvailable: https:\/\/openreview.net\/forum?id=SJ1kSyO2jwu\n[60] C. Zhong, L. Hu, Z. Zhang, and S. Xia, “Attt2m: Text-driven human\nmotion generation with multi-perspective attention mechanism,” in\nProceedings of the IEEE\/CVF International Conference on Computer\nVision, 2023, pp. 509–519.\n[61] P. Jin, Y. Wu, Y. Fan, Z. Sun, W. Yang, and L. Yuan, “Act as you\nwish: Fine-grained control of motion diffusion model with hierarchical\nsemantic graphs,” Advances in Neural Information Processing Systems,\nvol. 36, 2024.\n[62] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black,\n“Amass: Archive of motion capture as surface shapes,” in Proceedings of\nthe IEEE\/CVF international conference on computer vision, 2019, pp.\n5442–5451.\n[63] C. Mandery, ¨O. Terlemez, M. Do, N. Vahrenkamp, and T. Asfour, “The kit\nwhole-body human motion database,” in 2015 International Conference\non Advanced Robotics (ICAR).\nIEEE, 2015, pp. 329–336.\n[64] C. G. Lab, “Cmu graphics lab motion capture database,” http:\/\/mocap.cs.\ncmu.edu\/, 2000.\n[65] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman,\nD. Tzionas, and M. J. Black, “Expressive body capture: 3d hands,\nface, and body from a single image,” in Proceedings of the IEEE\/CVF\nconference on computer vision and pattern recognition, 2019, pp. 10 975–\n10 985.\n[66] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics, 2002,\npp. 311–318.\n[67] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”\nin Text summarization branches out, 2004, pp. 74–81.\n[68] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-\nbased image description evaluation,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp. 4566–\n4575.\n[69] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “Bertscore:\nEvaluating text generation with bert,” arXiv preprint arXiv:1904.09675,\n2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n17\n[70] H. Ma, J. Li, R. Hosseini, M. Tomizuka, and C. Choi, “Multi-objective\ndiverse human motion prediction with knowledge distillation,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 8161–8171.\n[71] Y. Yuan and K. Kitani, “Dlow: Diversifying latent flows for diverse human\nmotion prediction,” in Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX\n16.\nSpringer, 2020, pp. 346–364.\n[72] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\n2017.\n[73] Y. Zhang, M. J. Black, and S. Tang, “We are more than our joints:\nPredicting how 3d bodies move,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2021, pp. 3372–3382.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding.pdf"}
{"title":"Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation","authors":"Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo","summary":"In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.","url":"http:\/\/arxiv.org\/abs\/2410.13848v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.13848v1","published":1729187917000,"comment":"Technical Report","pdf_text":"Janus: Decoupling Visual Encoding for Unified\nMultimodal Understanding and Generation\nChengyue Wu1,2\nXiaokang Chen1,∗,†\nZhiyu Wu1,3\nYiyang Ma1,3\nXingchao Liu1\nZizheng Pan1\nWen Liu1\nZhenda Xie1\nXingkai Yu1\nChong Ruan1\nPing Luo2,∗\n1DeepSeek-AI\n2The University of Hong Kong\n3Peking University\n†: Project lead\n∗: Corresponding authors\nProject Page: https:\/\/github.com\/deepseek-ai\/Janus\nAbstract\nIn this paper, we introduce Janus, an autoregressive framework that unifies multimodal under-\nstanding and generation. Prior research often relies on a single visual encoder for both tasks,\nsuch as Chameleon. However, due to the differing levels of information granularity required by\nmultimodal understanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple visual encoding\ninto separate pathways, while still leveraging a single, unified transformer architecture for\nprocessing. The decoupling not only alleviates the conflict between the visual encoder’s roles in\nunderstanding and generation, but also enhances the framework’s flexibility. For instance, both\nthe multimodal understanding and generation components can independently select their most\nsuitable encoding methods. Experiments show that Janus surpasses previous unified model\nand matches or exceeds the performance of task-specific models. The simplicity, high flexibility,\nand effectiveness of Janus make it a strong candidate for next-generation unified multimodal\nmodels.\n1. Introduction\nIn recent years, multimodal large models have made significant advancements in both under-\nstanding and generation domains [20, 51]. In the field of multimodal understanding, researchers\nfollow the design of LLaVA [51] by using a vision encoder as a bridge to enable large language\nmodels (LLMs) to understand images. In the field of visual generation, diffusion-based ap-\nproaches [9, 20, 20, 67] have seen notable success. More recently, some works have explored\nautoregressive methods for vision generation [73, 79], achieving performance comparable to\ndiffusion models. To build more powerful and generalist multimodal models, researchers have\nsought to combine multimodal understanding and generation tasks [75, 77, 94]. For instance,\nsome studies have attempted to connect multimodal understanding models with pretrained dif-\nfusion models [27, 28, 75]. For example, Emu [75] uses the output of the LLM as a condition for a\npretrained diffusion model, and then relies on the diffusion model to generate images. However,\nstrictly speaking, this approach cannot be considered a truly unified model, because the visual\ngeneration functionality is handled by the external diffusion model, while the multimodal LLM\nitself lacks the capability to directly generate images.\nOther approaches [77, 85, 86, 94] employ a single transformer to unify both multimodal un-\narXiv:2410.13848v1  [cs.CV]  17 Oct 2024\nPOPE\nMME-Perception\nMMBench\nMM-Vet\nSEED-Bench\nGQA\nMMMU\nGenEval\nVQAv2\n61.25\n72.5\n83.75\n900.0\n1100.0\n1300.0\n25.0\n45.0\n65.0\n17.5\n25.0\n32.5\n40.0\n50.0\n60.0\n38.75\n47.5\n56.25\n16.0\n22.0\n28.0\n39.25\n48.5\n57.75\n51.25\n62.5\n73.75\nMobileVLM (1.4B)\nLLaVA-Phi (2.7B)\nInstructBLIP (7B)\nShow-o (1.3B)\nJanus (Ours, 1.3B)\n(a) Benchmark Performance.\n(b) Visual Generation Results.\nFigure 1 | Multimodal understanding and vision generation results from our Janus. Janus outperforms\nthe previous state-of-the-art unified multimodal models as well as some task-specific multimodal under-\nstanding models, while also demonstrating strong visual generation capabilities. The image resolution is\n384 × 384. Best viewed on screen.\nderstanding and generation tasks, which improves instruction-following for visual generation,\nunlocks potential emergent abilities, and reduces model redundancy. Such methods typically\nuse a single vision encoder to process inputs for both two tasks. However, the representations\nrequired by multimodal understanding and generation tasks differ significantly. In multimodal\nunderstanding tasks, the purpose of the vision encoder is to extract high-level semantic informa-\ntion (e.g., object categories or visual attributes within an image). The output of understanding\ntask not only involves extracting information from images but also involves complex semantic\nreasoning. Therefore, the granularity of the vision encoder’s representation tends to mainly\nfocus on high-dimensional semantic representation. By contrast, in visual generation tasks, the\nmain focus is on generating local details and maintaining global consistency in the image. The\nrepresentation in this context necessitates a low-dimensional encoding that is capable of fine-\ngrained spatial structure and textural detail expression. Unifying the representations of these\ntwo tasks within the same space will lead to conflicts and trade-offs. Consequently, existing\nunified models for multimodal understanding and generation often compromise on multi-\nmodal understanding performance, falling markedly short of the state-of-the-arts multimodal\nunderstanding models. We explore this issue further in the ablation study.\nTo solve this problem, we propose Janus1, a unified multimodal framework that decouples\nvisual encoding for multimodal understanding and generation. Specifically, we introduce\ntwo independent visual encoding pathways: one for multimodal understanding and one for\nmultimodal generation, unified by the same transformer architecture. The proposed method\noffers two main benefits: (1) Janus alleviates the conflict stemming from the different granular\nneeds of multimodal understanding and generation and eliminates the need to make trade-offs\nbetween two tasks when selecting visual encoders. (2) Janus is flexible and extensible. After\ndecoupling, both the understanding and generation tasks can adopt state-of-the-art encoding\n1In Roman mythology, Janus is the god of duality and transitions, symbolizing the coexistence of contradictory\nforces by having two faces, each looking in opposite directions. Similarly, our model captures the inherent tension\nbetween vision tasks: understanding demands abstract, high-level semantic representations, while generation\nrequires concrete, detailed information. By decoupling these processes into specialized encoders, our system mirrors\nJanus’s dual nature, resolving this tension within a unified architecture.\n2\ntechniques specific to their domain. Moreover, it is possible for Janus to accommodate additional\ninput types in the future, such as point clouds, EEG signals, or audio data, where independent\nencoders can extract features and then use a unified transformer to process them.\nTo the best of our knowledge, we are the first to highlight the importance of decoupling\nvisual encoding within the unified multimodal understanding and generation framework.\nOur experimental results show that Janus surpasses existing unified models with comparable\nparameter sizes on both multimodal understanding and generation benchmarks, achieving\nstate-of-the-art results. Notably, Janus even outperforms some task-specific models which have\nsignificantly more parameters (Figure 1). Specifically, on multimodal understanding benchmarks\nMMBench [54], SEED-Bench [42], and POPE [48], Janus (1.3B) achieved scores of 69.4, 63.7, and\n87.0, respectively, outperforming LLaVA-v1.5 (7B) [50] and Qwen-VL-Chat (7B) [3] . On visual\ngeneration benchmarks MSCOCO-30K [11] and GenEval [30], Janus achieved an FID score of\n8.53 and an accuracy of 61%, surpassing text-to-image generative models such as DALL-E 2 [66]\nand SDXL [62]. We believe that the strong performance, coupled with the high flexibility and\nextensibility of Janus, presents it as a strong candidate for next-generation unified multimodal\nmodels.\n2. Related Work\n2.1. Visual Generation\nVisual generation is a rapidly evolving field that combines concepts from natural language\nprocessing with advancements in transformer architectures. Autoregressive models, influenced\nby the success in language processing, leverage transformers to predict sequences of discrete\nvisual tokens (codebook IDs) [24, 65, 75]. These models tokenize visual data and employ a\nprediction approach similar to GPT-style [64] techniques. Additionally, masked prediction\nmodels [7, 8] draw upon BERT-style [19] masking methods, predicting masked sections of\nvisual inputs to improve synthesis efficiency, and have been adapted for video generation [89].\nConcurrently, continuous diffusion models have showcased impressive capabilities in visual\ngeneration [33, 67, 71], complementing discrete methods by approaching generation through a\nprobabilistic lens.\n2.2. Multimodal Understanding\nMultimodal large language models (MLLMs) integrate both text and images [6, 80, 81]. By\nleveraging pretrained LLMs, MLLMs [1, 2, 12, 51, 55, 82, 95] demonstrate a robust ability\nto understand and process multimodal information. Recent advancements have explored\nextending MLLMs with pretrained diffusion models to facilitate image generation [27, 29, 36, 75,\n76]. These methods fall under the category of tool utilization, where diffusion models are used\nto generate images based on the conditions output by the MLLM, while the MLLM itself does\nnot have the ability to directly perform visual generation. Moreover, the generative ability of\nthe entire system is often constrained by the external diffusion model, making its performance\ninferior to directly using the diffusion model on its own [27, 75].\n2.3. Unified Multimodal Understanding and Generation\nUnified multimodal understanding and generation models are considered powerful for fa-\ncilitating seamless reasoning and generation across different modalities [77, 94]. Traditional\napproaches in these models typically use a single visual representation for both understanding\n3\nAuto-Regressive Transformer\nUnd. Encoder\nText Tokenizer\nGen. Encoder\nText De-Tokenizer\nText Tokenizer\nImage Decoder\nImage: X!\nLanguage Instruct: X\"\nLanguage Instruct: X\"\nImage: X!\nLanguage Response: X#\nGenerated Image: X!\nUnderstanding\nImage Generation\n……\n……\nFigure 2 | Architecture of our Janus. Different from previous approaches [77, 85] that typically\nassume visual understanding and generation require the same visual encoder, our Janus de-\ncouples visual encoding for visual understanding and visual generation. “Und. Encoder” and\n“Gen. Encoder” are abbreviations for “Understanding Encoder” and “Generation Encoder”,\nrespectively. Best viewed in color.\nand generation tasks, regardless of whether they are based on autoregressive (AR) models\n[77, 85] or diffusion models [86, 94]. For example, Chameleon [77] adopts a VQ Tokenizer to\nencode images for both multimodal understanding and generation. However, this practice\nmay lead to suboptimal outcomes, as the vision encoder might face a trade-off between the\ndemands of understanding and generation. In contrast, our Janus can explicitly decouple the\nvisual representations for understanding and generation, recognizing that different tasks may\nrequire varying levels of information.\n3. Janus: A Simple, Unified and Flexible Multimodal Framework\n3.1. Architecture\nThe architecture of Janus is shown in Figure 2. For pure text understanding, multimodal\nunderstanding, and visual generation, we apply independent encoding methods to convert the\nraw inputs into features, which are then processed by an unified autoregressive transformer.\nSpecifically, for text understanding, we use the built-in tokenizer of the LLM to convert the\ntext into discrete IDs and obtain the feature representations corresponding to each ID. For\nmultimodal understanding, we use the SigLIP [92] encoder to extract high-dimensional semantic\nfeatures from images. These features are flattened from a 2-D grid into a 1-D sequence, and an\nunderstanding adaptor is used to map these image features into the input space of the LLM. For\nvisual generation tasks, we use the VQ tokenizer from [73] to convert images into discrete IDs.\nAfter the ID sequence is flattened into 1-D, we use a generation adaptor to map the codebook\nembeddings corresponding to each ID into the input space of the LLM. We then concatenate\nthese feature sequences to form a multimodal feature sequence, which is subsequently fed into\nthe LLM for processing. The built-in prediction head of the LLM is utilized for text predictions\nin both the pure text understanding and multimodal understanding tasks, while a randomly\ninitialized prediction head is used for image predictions in the visual generation task. The\nentire model adheres to an autoregressive framework without the need for specially designed\n4\nGen. Encoder\nImage Head\nGen. Adaptor\nStage I:\nTraining Adaptors and Image Head\nLLM\nUnd. Adaptor🔥\n🔥\n🔥\nUnderstanding\nText Head\nUnd. Encoder❄\n❄\n❄\nImage Generation\nStage II: \nUnified Pretraining\nStage III:\nSupervised Fine-tuning\nGen. Encoder\nImage Head\nGen. Adaptor\nLLM\nUnd. Adaptor🔥\n🔥\n🔥\nUnderstanding\nText Head\nUnd. Encoder❄\n❄\n❄\nImage Generation\n🔥\n🔥\nGen. Encoder\nImage Head\nGen. Adaptor\nLLM\nUnd. Adaptor🔥\n🔥\n🔥\nUnderstanding\nText Head\nUnd. Encoder\nImage Generation\n🔥\n🔥\n🔥\n❄\nFigure 3 | Our Janus adopts a three-stage training procedure. We use flame symbols\/snowflake\nsymbols in the diagram to indicate the module updates\/does not update its parameters.\nattention masks.\n3.2. Training Procedure\nThe training of Janus is divided into three stages, as illustrated in Figure 3. Details are provided\nin the below.\nStage I: Training Adaptors and Image Head. The main goal of this stage is to create a conceptual\nconnection between visual and linguistic elements within the embedding space, enabling the\nLLM to understand the entities shown in images and have preliminary visual generation\nability. We keep the visual encoders and the LLM frozen during this stage, allowing only the\ntrainable parameters within the understanding adaptor, generation adaptor and image head to\nbe updated.\nStage II: Unified Pretraining. In this stage, we perform unified pretraining with multimodal\ncorpus to enable Janus to learn both multimodal understanding and generation. We unfreeze the\nLLM and utilize all types of training data: pure text data, multimodal understanding data, and\nvisual generation data. Inspired by Pixart [9], we begin by conducting simple visual generation\ntraining using ImageNet-1k to help the model grasp basic pixel dependencies. Subsequently, we\nenhance the model’s open-domain visual generation capability with general text-to-image data.\nStage III: Supervised Fine-tuning. During this stage, we fine-tune the pretrained model with\ninstruction tuning data to enhance its instruction-following and dialogue capabilities. We\nfine-tune all parameters except the generation encoder. We focus on supervising the answers\nwhile masking system and user prompts. To ensure Janus’s proficiency in both multimodal\nunderstanding and generation, we don’t fine-tune separate models for a certain task. Instead,\nwe use a blend of pure text dialogue data, multimodal understanding data and visual generation\ndata, ensuring versatility across various scenarios.\n3.3. Training Objective\nJanus is an autoregressive model, and we simply adopt the cross-entropy loss during training:\nL = −\n∑︁\n𝑖=1\nlog 𝑃𝜃(𝑥𝑖|𝑥<𝑖)\n(1)\nHere, 𝑃(· | ·) indicates the conditional probability modeled by the weights 𝜃of Janus. For\npure text understanding and multimodal understanding tasks, we compute the loss on the text\n5\nsequence. For visual generation tasks, we compute the loss only on the image sequence. To keep\nthe design simple, we have not assigned different loss weights to different tasks.\n3.4. Inference\nDuring inference, our model adopts a next-token prediction approach. For pure text under-\nstanding and multimodal understanding, we follow the standard practice of sampling tokens\nsequentially from the predicted distribution. For image generation, we utilize classifier-free\nguidance (CFG) 2, similar to prior works [8, 26, 73]. Specifically, for each token, the logit 𝑙𝑔is\ncalculated as: 𝑙𝑔= 𝑙𝑢+ 𝑠(𝑙𝑐−𝑙𝑢), where 𝑙𝑐is the conditional logit, 𝑙𝑢is the unconditional logit,\nand 𝑠is the scale for the classifier-free guidance. The default number of 𝑠is 5 for the following\nevaluation.\n3.5. Possible Extensions\nIt is important to note that our design, which features separate encoders for understanding and\ngeneration, is straightforward and easy to extend.\nMultimodal Understanding. (1) For the multimodal understanding component, a stronger\nvision encoder can be chosen without worrying about whether the encoder is capable of handling\nvision generation tasks, such as EVA-CLIP [74], InternViT [13], etc. (2) To handle high-resolution\nimages, dynamic high-resolution techniques [50] can be used. This allows the model to scale to\nany resolution, without performing positional embedding interpolation for ViTs. Tokens can be\nfurther compressed to save computational cost, for instance, using pixel shuffle operation [12].\nVisual Generation. (1) For visual generation, finer-grained encoders can be chosen in order\nto preserve more image details after encoding, such as MoVQGan [93]. (2) Loss functions\nspecifically designed for visual generation can be employed, such as diffusion loss [46]. (3) A\ncombination of AR (causal attention) and parallel (bidirectional attention) methods can be used\nin the visual generation process to reduce accumulated errors during visual generation [79].\nSupport for Additional Modalities. The straightforward architecture of Janus allows for easy\nintegration with additional encoders, accommodating various modalities such as 3D point\ncloud [53], tactile [88], and EEG [4]. This gives Janus the potential to become a more powerful\nmultimodal generalist model.\n4. Experiments\nIn this section, we present a series of comprehensive experiments designed to assess the perfor-\nmance of our method across a range of visual understanding and generation tasks. We begin by\ndetailing our experimental setup, which includes the model architecture, training datasets, and\nevaluation benchmarks. Next, we report the performance of Janus, followed by a comparison\nwith other state-of-the-art models on various benchmarks for multimodal understanding and\ngeneration. We also conduct extensive ablation studies to verify the effectiveness of the proposed\nmethod. Lastly, we provide some qualitative results.\n2During training, we replace the text condition in the text-to-image data with a pad token at a probability of 10%,\nenabling the model to have unconditional visual generation capability.\n6\nTable 1 | Detailed hyperparameters of our Janus. Data ratio refers to the ratio of multimodal\nunderstanding data, pure text data, and visual generation data.\nHyperparameters\nStage 1\nStage 2\nStage 3\nLearning rate\n1.0 × 10−3\n1 × 10−4\n2.0 × 10−5\nLR scheduler\nCosine\nConstant\nConstant\nWeight decay\n0.0\n0.0\n0.1\nGradient clip\n1.0\n1.0\n1.0\nOptimizer\nAdamW (𝛽1 = 0.9, 𝛽2 = 0.95)\nWarm-up steps\n300\n5, 000\n0\nTraining steps\n10, 000\n180, 000\n24, 000\nBatch size\n256\n512\n256\nData Ratio\n1 : 0 : 1\n2 : 3 : 5\n7 : 3 : 10\n4.1. Implementation Details\nIn our experiments, we utilize DeepSeek-LLM (1.3B) [5] with a maximum supported sequence\nlength of 4096 as the base language model. For the vision encoder used in understanding tasks,\nwe select SigLIP-Large-Patch16-384 [92]. The generation encoder has a codebook of size 16, 384\nand downsamples images by a factor of 16. Both the understanding adaptor and the generation\nadaptor are two-layer MLPs. The detailed hyperparameters for each stage are provided in\nTable 1. All images are resized to 384 × 384 pixels. For multimodal understanding data, we\nresize the long side of the image and pad the short side with the background color (RGB: 127,\n127, 127) to reach 384. For visual generation data, the short side is resized to 384, and the long\nside is cropped to 384. We use sequence packing during training to improve training efficiency.\nWe mix all data types according to the specified ratios in a single training step. Our Janus is\ntrained and evaluated using HAI-LLM [32], which is a lightweight and efficient distributed\ntraining framework built on top of PyTorch. The whole training process took 7 days on a cluster\nof 16 nodes, each equipped with 8 Nvidia A100 (40GB) GPUs.\n4.2. Data Setup\nIn this section, we provide details of the pretraining and supervised finetuning datasets.\nStage I. We use a dataset that includes 1.25 million image-text paired captions from ShareGPT4V [10]\nfor multimodal understanding and approximately 1.2 million samples from ImageNet-1k [18] for\nvisual generation. The ShareGPT4V data is formatted as “<image><text>”. The ImageNet data\nis organized into a text-to-image data format using the category names: “<category_name><image>”.\nHere, the “<>” symbols represent placeholders.\nStage II. We organize the data into the following categories. (1) Text-only data. We use pre-\ntraining text copus from DeepSeek-LLM [5]. (2) Interleaved image-text data. We use Wiki-\nHow [39] and WIT [72] dataset. (3) Image caption data. We use images from [17, 18, 23, 38,\n40, 45, 47, 49, 70]. Among them, we employ open-source multimodal model to re-caption\nimages in [17, 40]. The image caption data is formatted into question-answer pairs, for ex-\nample, “<image>Describe the image in detail.<caption>”. (4) Table and chart data.\nWe use corresponding table and chart data from DeepSeek-VL [55]. The data is formatted\nas “<question><answer>”. (5) Visual generation data. We utilize image-caption pairs from\nvarious datasets including [17, 38, 40, 57, 58, 60, 63, 70], along with 2M in-house data. For images\nfrom [38, 70], we filter based on aesthetic scores and image sizes, resulting in 20% remaining.\nDuring training, we randomly use only the first sentence of a caption with a 25% probability to\n7\nTable 2 | Comparison with state-of-the-arts on multimodal understanding benchmarks. “Und.”\nand “Gen.” denote “understanding” and “generation”, respectively. Models using external\npretrained diffusion model are marked with †.\nType\nModel\n# LLM Params POPE↑MME-P↑MMB↑SEED↑VQAv2(𝑡𝑒𝑠𝑡)↑GQA↑MMMU↑MM-Vet↑\nUnd. Only\nLLaVA-v1.5-Phi-1.5 [86]\n1.3B\n84.1\n1128.0\n-\n-\n75.3\n56.5\n30.7\n-\nMobileVLM [14]\n1.4B\n84.5\n1196.2\n53.2\n-\n-\n56.1\n-\n-\nMobileVLM-V2 [15]\n1.4B\n84.3\n1302.8\n57.7\n-\n-\n59.3\n-\n-\nMobileVLM [14]\n2.7B\n84.9\n1288.9\n59.6\n-\n-\n59.0\n-\n-\nMobileVLM-V2 [15]\n2.7B\n84.7\n1440.5\n63.2\n-\n-\n61.1\n-\n-\nLLaVA-Phi [96]\n2.7B\n85.0\n1335.1\n59.8\n-\n71.4\n-\n-\n28.9\nLLaVA [51]\n7B\n76.3\n809.6\n38.7\n33.5\n-\n-\n-\n25.5\nLLaVA-v1.5 [50]\n7B\n85.9\n1510.7\n64.3\n58.6\n78.5\n62.0\n35.4\n31.1\nInstructBLIP [16]\n7B\n-\n-\n36.0\n53.4\n-\n49.2\n-\n26.2\nQwen-VL-Chat [3]\n7B\n-\n1487.5\n60.6\n58.2\n78.2\n57.5\n-\n-\nIDEFICS-9B [41]\n8B\n-\n-\n48.2\n-\n50.9\n38.4\n-\n-\nEmu3-Chat [83]\n8B\n85.2\n-\n58.5\n68.2\n75.1\n60.3\n31.6\n-\nInstructBLIP [16]\n13B\n78.9\n1212.8\n-\n-\n-\n49.5\n-\n25.6\nUnd. and Gen. DreamLLM† [21]\n7B\n-\n-\n-\n-\n72.9\n-\n-\n36.6\nLaVIT† [36]\n7B\n-\n-\n-\n-\n66.0\n46.8\n-\n-\nEmu† [75]\n13B\n-\n-\n-\n-\n52.0\n-\n-\n-\nNExT-GPT† [84]\n13B\n-\n-\n-\n-\n66.7\n-\n-\n-\nShow-o [86]\n1.3B\n73.8\n948.4\n-\n-\n59.3\n48.7\n25.1\n-\nGemini-Nano-1 [78]\n1.8B\n-\n-\n-\n-\n62.7\n-\n26.3\n-\nLWM [52]\n7B\n75.2\n-\n-\n-\n55.8\n44.8\n-\n9.6\nVILA-U [85]\n7B\n85.8\n1401.8\n-\n59.0\n79.4\n60.8\n-\n33.5\nChameleon [77]\n7B\n-\n-\n-\n-\n-\n-\n22.4\n8.3\nJanus (Ours)\n1.3B\n87.0\n1338.0\n69.4\n63.7\n77.3\n59.1\n30.5\n34.3\nencourage the model to develop strong generation capabilities for short descriptions. ImageNet\nsamples [18] are presented only during the first 120K training steps, while images from other\ndatasets appear in the later 60K steps. This approach helps the model first learn basic pixel\ndependencies before progressing to more complex scene understanding, as suggested by [9].\nThe visual generation data is provided in the format: “<caption><image>”.\nStage III. For text understanding, we use data from [43]. For multimodal understanding, we\nuse instruct tuning data from [31, 34, 35, 43, 56, 69]. For visual generation, we use image-text\npairs from [17, 60, 70] (a subset of that in stage II) and 4M in-house data. We utilize the following\nformat for instruction tuning:“User:<Input Message> \\n Assistant: <Response>”. For\nmulti-turn dialogues, we repeat this format to structure the data.\n4.3. Evaluation Setup\nMultimodal Understanding. To assess multimodal understanding capabilities, we evaluate our\nmodel on widely recognized image-based vision-language benchmarks, which include VQAv2\n[31], GQA [35], POPE [48], MME [25], SEED [42], MMB [54], MM-Vet [90], and MMMU [91].\nVisual Generation. For evaluating visual generation capabilities, we use the MSCOCO-30K\n[11], MJHQ-30K [44], and GenEval [30] benchmarks. MSCOCO-30K and MJHQ-30K employ\nthe Fréchet Inception Distance (FID) metric on generated images compared to 30K high-quality\nimages, which indicates the overall efficacy of image generation. GenEval is a challenging\nbenchmark for image-to-text generation, designed to reflect the comprehensive generative\n8\nTable 3 | Evaluation of text-to-image generation ability on GenEval benchmark. “Und.”\nand “Gen.” denote “understanding” and “generation”, respectively. Models using external\npretrained diffusion model are marked with †.\nType\nMethod\n# Params\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall↑\nGen. Only\nLlamaGen [73]\n0.8B\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nLDM [67]\n1.4B\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 [67]\n0.9B\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nPixArt-𝛼[9]\n0.6B\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nSDv2.1 [67]\n0.9B\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nDALL-E 2 [66]\n6.5B\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nEmu3-Gen [83]\n8B\n0.98\n0.71\n0.34\n0.81\n0.17\n0.21\n0.54\nSDXL [62]\n2.6B\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nUnd. and Gen.\nSEED-X† [29]\n17B\n0.97\n0.58\n0.26\n0.80\n0.19\n0.14\n0.49\nShow-o [86]\n1.3B\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n0.53\nLWM [52]\n7B\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nChameleon [77]\n34B\n-\n-\n-\n-\n-\n-\n0.39\nJanus (Ours)\n1.3B\n0.97\n0.68\n0.30\n0.84\n0.46\n0.42\n0.61\nabilities of visual generation models by offering a detailed instance-level analysis of their\ncompositional capabilities.\n4.4. Comparison with State-of-the-arts\nMultimodal Understanding Performance. We compare the proposed method with state-of-the-\nart unified models and understanding-only models in Table 2. Janus achieves the overall best\nresults among models of similar scale. Specifically, compared to the previous best unified model,\nShow-o [86], we achieve performance improvements of 41% (949 →1338) and 30% (48.7 →59.1)\non the MME and GQA datasets, respectively. This can be attributed to Janus decoupling the\nvisual encoding for multimodal understanding and generation, mitigating the conflict between\nthese two tasks. When compared to models with significantly larger sizes, Janus remains highly\ncompetitive. For instance, Janus outperforms LLaVA-v1.5 (7B) on several datasets, including\nPOPE, MMbench, SEED Bench, and MM-Vet.\nVisual Generation Performance. We report visual generation performance on GenEval, COCO-\n30K and MJHQ-30K benchmarks. As shown in Table 3, our Janus obtains 61% overall accuracy\non GenEval, which outperforms the previous best unified model Show-o (53%) and some\npopular generation-only methods, e.g., SDXL (55%) and DALL-E 2 (52%). This demonstrates\nthat our approach has better instruction-following capabilities. As shown in Table 4, Janus\nachieves FIDs of 8.53 and 10.10 on the COCO-30K and MJHQ-30K benchmarks, respectively,\nsurpassing unified models Show-o and LWM, and demonstrating competitive performance\ncompared to some well-known generation-only methods. This demonstrates that the images\ngenerated by Janus have good quality and highlights its potential in visual generation.\n4.5. Ablation Studies\nWe carefully design ablation studies to verify the effectiveness of Janus’s design concept. First,\nwe design experiments to validate the importance and benefits of decoupling visual encoding.\nSecond, we investigate the impact of unified training on individual tasks like multimodal\nunderstanding or visual generation. Results are listed in Table 5.\n9\nTable 4 | Evaluation of text-to-image generation ability on MSCOCO-30K and MJHQ-30K\nbenchmark. “Und.” and “Gen.” denote “understanding” and “generation”, respectively.\nModels using external pretrained diffusion model are marked with †.\nType\nModel\n# Params COCO-30K↓MJHQ-30K↓\nGen. Only\nDALL·E [65]\n12B\n27.50\n-\nGLIDE [59]\n5B\n12.24\n-\nLDM [67]\n1.4B\n12.64\n-\nDALL·E 2 [66]\n6.5B\n10.39\n-\nSDv1.5 [67]\n0.9B\n9.62\n-\nGigaGAN [37]\n0.9B\n9.09\n-\nPixArt-𝛼[9]\n0.6B\n7.32\n-\nImagen [68]\n34B\n7.27\n-\nRAPHAEL [87]\n3B\n6.61\n-\nUnd. and Gen.\nEmu† [75]\n13B\n11.66\n-\nNExT-GPT† [84]\n13B\n11.28\n-\nSEED-X† [29]\n17B\n14.99\n-\nShow-o [86]\n1.3B\n9.24\n15.18\nLWM [52]\n7B\n12.68\n17.77\nVILA-U (256) [85]\n7B\n-\n12.81\nVILA-U (384) [85]\n7B\n-\n7.69\nJanus (Ours)\n1.3B\n8.53\n10.10\nTable 5 | Ablation studies. We verify the effectiveness of decoupling visual encoding and\ncompare unified training with task-specific training. “Und.”, “Gen.” and “SE. Tokenizer” denote\n“understanding”, “generation” and “semantic tokenizer”, respectively.\nExp ID\nVisual Encoder\nTraining Task\nPOPE↑MMB↑SEED↑MMMU↑COCO-FID↓\nA\nVQ Tokenizer\nUnd. + Gen.\n60.1\n35.0\n34.9\n24.7\n8.72\nB\nSE. Tokenizer\nUnd. + Gen.\n82.4\n52.7\n54.9\n26.6\n7.11\nC\nSE. Tokenizer\nUnd.\n83.9\n62.1\n60.8\n27.5\n-\nD\nSigLIP + VQ (Ours)\nUnd. + Gen.\n87.0\n69.4\n63.7\n30.5\n8.53\nE\nSigLIP\nUnd.\n85.9\n70.6\n64.8\n28.8\n-\nF\nVQ Tokenizer\nGen.\n-\n-\n-\n-\n8.92\nBaseline Construction. Following previous work [77], we select a VQ tokenizer [73] to encode\nimages for both multimodal understanding and generation tasks, serving as the baseline (Exp-A).\nConsidering that the VQ tokenizer in Exp-A might be weak in extract semantic information,\nmaking it less effective for multimodal understanding, we also construct a stronger baseline\nExp-B. We adopt SigLIP to distill an enhanced semantic tokenizer 3 that can extract high-level\nsemantic information from images while also have the ability to convert images into discrete\nIDs, which is similar to that in [85]. Details of the semantic tokenizer could be found in the\nAppendix A.1.\nImpact of Decoupling Visual Encoding. (1) From the results of Exp-A, we find the model\nachieves satisfactory performance on visual generation benchmark (8.72 FID on COCO). How-\never, there is a significant gap on understanding benchmarks between Exp-A and our model\n(Exp-D). (2) When comparing Exp-B to Exp-A, the results show a clear improvement in multi-\n3The semantic tokenizer is only used in the ablation study as a stronger baseline. For simplicity, we use the\nordinary VQ tokenizer [73] in the main experiment.\n10\nJanus (Ours)\nA close-up high-contrast photo of Sydney Opera House sitting next \nto Eiffel tower, under a blue night sky of roiling energy, exploding \nyellow stars, and radiating swirls of blue.\nA detailed portrait of the Roman god Janus, featuring his two faces looking in opposite \ndirections. One face appears aged, with deep-set wrinkles and a wise, contemplative expression, \nwhile the other face is youthful, exuding vigor and curiosity. His hair is styled in flowing curls, \nframing both faces with a sense of divine symmetry. The artwork is rich in contrasting colors, \nwith the left side dominated by cold blues and silvers, symbolizing winter and reflection, and the \nright side awash with warm golds and reds, representing spring and renewal. The background is \na celestial tapestry, adorned with stars and symbolic motifs of time and passage. \nA wise old owl with golden plumage perched on a luminous crystal \ntree in a magical forest. Radiant fireflies swirl around while ethereal \nmist rolls through the trees, illuminated by swirls of iridescent \nmoonlight and glistening emerald leaves.\nA brave dog wearing a futuristic space suit, exploring an alien planet \namidst swirling dunes of stardust and meteor showers. The landscape \nis dotted with glowing crystal formations and ethereal terraforms, \ncreating a surreal environment in which swirling vortexes in the sky \ndepict the endless dance of distant galaxies.\nLlamaGen\nJanus (Ours)\nLlamaGen\nSDXL\nSDXL\nFigure 4 | Qualitative comparisons of visual generation with LlamaGen and SDXL. The images\ngenerated by Janus show better consistency with the user’s prompts. The image resolutions for\nSDXL, LlamaGen, and ours are 1024 × 1024, 512 × 512, and 384 × 384, respectively. Best viewed\non screen.\nmodal understanding, although there is still a considerable gap compared to our method. In\nterms of visual generation, Exp-B outperforms Exp-D. We hypothesize two possible reasons\nfor this. First, the semantic tokenizer produces discrete IDs that are more semantically coher-\nent, providing more reasonable prediction targets for the LLM. Second, the visual encoder in\nExp-B has significantly more parameters than the Gen. encoder in Exp-D. (3) To investigate\nwhether using a single visual encoder leads to a trade-off between multimodal understanding\nand generation, we further design Exp-C based on Exp-B, which focuses solely on multimodal\nunderstanding training. The multimodal understanding ability of Exp-C is significantly better\nthan that of Exp-B. This indicates that the visual encoder in Exp-B made trade-offs between\nmultimodal understanding and generation, ultimately sacrificing its multimodal understanding\ncapability. The above experiments illustrate the importance of decoupling visual encoding.\nUnified Model vs. Pure Understanding & Pure Generation. We compare the performance\nof unified training (Exp-D) against pure understanding (Exp-E) and pure generation (Exp-F)\ntraining. For pure understanding, we omit visual generation data. For pure generation, we\nexclude the understanding data. Please note that unified training and pure understanding\ntraining go through the same steps for the understanding part. Similarly, unified training and\npure generation training go through the same steps for the visual generation part. Experimental\nresults show that the performance of unified training is comparable to that of training solely\nfor understanding or solely for visual generation. This demonstrates that our model, Janus,\nis capable of incorporating strong generative abilities while minimally affecting multimodal\nunderstanding performance.\n11\nUser: What is the meaning of this meme from the top panel to the bottom?\nJanus (Ours):  The meme is a humorous take on the concept of a Nobel \nPrize in Physics. The top panel shows a man in a red jacket with his hand \nup, seemingly confused or surprised, while the bottom panel shows the \nsame man with a smile on his face, seemingly pleased or proud, as if he has \njust won the Nobel Prize. The meme is a play on the idea that the man in \nthe top panel might be confused or surprised by the idea of winning a \nNobel Prize in Physics, while the man in the bottom panel seems to be \ngenuinely happy about it.\nChameleon: The meme is a visual representation of a conversation between two people, with the top \npanel featuring the text \"I'm not arguing, I'm just explaining why I'm right\" and the bottom panel featuring\nthe text \"No, you're not.\" The meme is using a technique called \"irony\" to convey the idea that even though \none person is presenting themselves as non-argumentative, they are actually being argumentative. The use \nof the phrase \"I'm not arguing, I'm just explaining why I'm right\" is a common tactic used by people who \nare trying to avoid an argument but are actually engaging in one. The meme is poking fun at this tactic by \nshowing the person's actions and words as being in direct contradiction to their claim.\nShow-o: The meaning of the meme from the top panel to the bottom is that it is a humorous and light-\nhearted depiction of a man wearing a yellow hat and a red jacket, with the caption \"I'm not a fan of the\nsun.\" This suggests that the man is not a fan of the sun, and he is using the sun as a metaphor for his \nfeelings or emotions. The meme is likely meant to be humorous and entertaining, as it combines a serious \nsubject with a lighthearted and playful approach\nUser: Explain this meme.\nJanus (Ours):  The meme is comparing two \ndifferent types of neural networks used in \nmachine learning. The first type is called \n\"Decoupling Visual Encoding,\" and the second \ntype is called \"Single Visual Encoder.\" The meme \nis trying to convey that the first type is more \npowerful and can handle more complex tasks, \nwhile the second type is simpler and less powerful.\nChameleon: This image shows a cartoon of a pig and a bear side-by-side, with the text \"When you're a\nvegan and your friends still order sausage and bacon\" above them. The image is a humorous take on the \nchallenges of being a vegan in a non-vegan environment, where one's friends or family may not share their \ndietary preferences.\nShow-o: The image features two dogs, one brown and one white, standing side by side. The brown dog is \npositioned on the left side of the image, while the white dog is on the right. Both dogs are looking at the \ncamera, seemingly posing for a picture. The brown dog is slightly larger than the white dog. The image is\ncaptioned with the phrase “I‘m not a dog, but I’m a dog.” This humorous statement is accompanied by the \nimage of the two dogs, emphasizing their …\nFigure 5 | Qualitative results of multimodal understanding on humorous memes. We compare\nthe response with Chameleon-7B [77] and Show-o [86]. We emphasize the key-points in the\nresponse. Best viewed on screen.\n4.6. Qualitative Results\nVisualizations of Visual Generation. Figure 4 provides qualitative comparisons between our\nmodel, diffusion-based models like SDXL [62], and the autoregressive model LlamaGen [73].\n12\nThe results show that our model demonstrates superior instruction-following capabilities in\nvisual generation, accurately capturing most of details in the user’s prompt. This indicates the\npotential of the unified model in the realm of visual generation. More visualizations can be\nfound in the Appendix B.\nMultimodal Understanding on MEME Images. Figure 5 showcases the qualitative results of\nJanus’s multimodal understanding ability, compared with Chameleon [77] and Show-o [86].\nJanus accurately interprets the text caption and captures the emotion conveyed in the meme. In\ncontrast, both Chameleon and Show-o struggle with accurately recognizing the text in the image.\nAdditionally, Chameleon fails to identify objects in the meme, while Show-o misinterprets the\ndog’s color. These examples highlight that the decoupled vision encoder significantly enhances\nJanus’s fine-grained multimodal understanding ability compared to the shared encoder used\nby Chameleon and Show-o. More multimodal understanding exmples can be found in the\nAppendix B.\n5. Conclusion\nIn this paper, we introduced Janus, a simple, unified and extensible multimodal understanding\nand generation model. The core idea of Janus is to decouple visual encoding for multimodal\nunderstanding and generation, which could alleviate the conflict arising from the differing\ndemands that understanding and generation place on the visual encoder. Extensive experi-\nments have demonstrated the effectiveness and leading performance of Janus. It is also worth\nnoting that Janus is flexible and easy to extend. In addition to having significant potential for\nimprovement in both multimodal understanding and generation, Janus is also easily extendable\nto incorporate more input modalities. The above advantages suggest that Janus may serve as an\ninspiration for the development of the next generation of multimodal general-purpose models.\n13\nAppendix\nA. Details of Semantic Tokenizer Mentioned in Ablation Study\nA.1. Architecture of Semantic Tokenizer\nCNN\nEncoder\nVector Quantization\n19\n97\n822\n96\n701\n100\n66\n88\n99\nDiscrete Visual Tokens\nLookup Codebook\nSemantic Decoder\nPixel Decoder\nPretrained SigLIP\nSemantic Reconstruction Loss\nRGB Reconstruction Loss\nCNN\nEncoder\nVector Quantization\n19\n97\n822\n96\n701\n100\n66\n88\n99\nDiscrete Visual Tokens after VQ\nLookup Codebook\nSemantic Decoder\nPixel Decoder\nLLM\n19\n97\n822\n96\n701\n100\n66\n88\n99\nDiscrete IDs from LLM Prediction\n(a) Architecture of Semantic Tokenizer\n(b) Architecture of LLM with Semantic Tokenizer Integration\nAdaptor\nFigure 6 | Architecture and usage of the semantic tokenizer. (a) Architecture used during\ntraining of the semantic tokenizer. We use pre-trained SigLIP [92] to supervise the reconstruction\nof semantic information, while using raw image to supervise the reconstruction of RGB values.\n(b) Integrating LLM with the semantic decoder. The semantic decoder outputs continuous\nfeatures with high-level semantics, which are passed through an adaptor and then used as input\nfor the LLM. Please note that the semantic tokenizer is only used in the ablation study, not in\nthe main experiment.\nWe build the semantic tokenizer based on the tokenizer architecture proposed in [73], which\nhas a downsample rate of 16. In addition to the original CNN pixel decoder, we add an\nadditional semantic decoder branch after Vector Quantization, as shown in Figure 6 (a). The\nsemantic decoder is a 12-layer ViT [22], with 12 attention heads and a hidden dimension of 768.\nFor the semantic decoder, we use a causal attention mask to facilitate next token prediction\nwhen integrating it with an LLM.\n14\nA.2. Training\nTraining Procedure. The semantic tokenizer is trained from scratch in a two-stage manner. In\nthe first stage, we train the model on the ImageNet-1k [18] dataset for 40 epochs. In the second\nstage, we fine-tune the model for 1 epoch on 50 million images. These images come from the\nvisual generation data used during the Janus pretraining process. We use a constant learning\nrate of 1𝑒−4 and a batch size of 128.\nTraining Loss. The training loss of the semantic tokenizer consists of two parts. On one hand,\nwe use the loss for RGB reconstruction as described in [73]. On the other hand, we use SigLIP-\nLarge-Patch16-384 as the teacher to supervise the semantic feature reconstruction results by\nthe semantic decoder. We adopt the loss in BEiT-v2 [61]. Specifically, we maximize the cosine\nsimilarity between the semantic feature predicted by the semantic decoder and the SigLIP\noutput. The weight for the semantic reconstruction loss is set to 0.25.\nA.3. Integrating with LLM\nWe present the integration of the semantic tokenizer and the LLM in Figure 6 (b). The image is\nfirst transformed into continuous features through the CNN encoder, vector quantization and\nthe semantic decoder. Then, the LLM processes these features and generates predictions for the\nimage IDs. Finally, the pixel decoder converts these discrete IDs into RGB values.\nB. Additional Qualitative Results\nMore Visualizations of Text-to-Image Generation. We present more text-to-image generation\nresults in Figure 7. It is evident that Janus is capable of producing high-quality images that ad-\nhere closely to the given prompts. We further explore the multilingual text-to-image capabilities\nof our model, as shown in Figure 8. We are pleasantly surprised to find that, despite our training\ndata consisting solely of English text-to-image data, Janus can still process text-to-image tasks in\nother languages. We attribute this multilingual ability to the original large language model’s\ninherent traits. The LLM initially translates various languages into a unified semantic space,\nallowing Janus to perform text-to-image tasks naturally without additional training.\nMore Multimodal Understanding Results. Additional results on multimodal understanding\nare shown in Figure 9. Janus exhibits impressive comprehension abilities when handling inputs\nfrom various contexts, showcasing its powerful capabilities.\n15\na young woman, looks like mix of Lana Del Rey and \ngrimes, flowing cool colored hair, marbled, iridescent, \nshoujo manga, pre-raphaelite, k-pop, gilded, pearl, \nspun silk, clouds, ghost, glowing jellyfish, billowing \ngossamer cloth, Alexander McQueen, handmade lace, \nfloral embroidery, snakeskin, dramatic lighting\nTiny cute adorable mouse dressed as a king in a\ncastle, anthropomorphic, Jean-Baptiste Monge, soft\ncinematic lighting, 8k, intricate details, portrait,\nPixar style character, old fashioned movie style\nPortrait of a beautiful, curvaceous, Pirate princess\ngoddess babe, red hair, intricate ornate costume,\nCaribbean background + outdoors + Ocean, painted\nby ArtGerm, Alphonse Mucha, Roberto Ferri, Ross\nTran, Pixar, low angle shot, digital painting,\ncinematic rim lighting, Unreal Engine 5, 8K\na cute fluffy chubby marmot sunbathing on a pile of \nrocks, snow mountains background, turquoise glacier \nlake afar, clear blue sky, highly detailed, golden hour, \nnatural light, octane render, unreal engine\nepic 3d portrait of white King Kong wearing mech \narmor made of black crystals, golden ornate around \nthe armor, symmetrical body, hyperrealistic, intricate \ndetails, shiny, cinematic, unreal engine, artstation, \noctane render,\nThe ultimate wrist watch watch time machine ,\nsuper advanced technology, holographic\ndisplay, intricate mechanism.\nA stunning princess from kabul in red, white \ntraditional clothing, blue eyes, brown hair.\nTiny cute adorable fluffy baby raccoon with \nknitted blue scarf leaning at a table in a \nmedieval \npub \nholding \na \ncoffee \ncup, \nanthropomorphic, Jean-Baptiste Monge, soft \ncinematic \nlighting, \n8k, \nintricate \ndetails, \nportrait, Pixar style character, old fashioned \nmovie style\na \npanda \nthat \nhas \nbeen \ncybernetically \nenhanced more cybernetics3d 4k unreal \nengine chaos 20\nArchitectural parametric pavilion made \nfrom wood and glass, with organic cavities, \nsurrounded by a beautiful forest. Dramatic \nscene, \nphotorealistic, \nhyperrealistic, \nraytracing reflections, 8k hd, intrincate \ndetail in the style of Frank Lloyde Wright \nReal photo of a cup of hot steaming coffee and a \nbrass vase with a large bouquet of spring flowers by \nan old oak window at sunrise, fine details, rich \ncolors taken with a nikon z6 camera and a nikon \nnikkor lens with 50 f5.6 iso 100 and a shutter speed \nof 1400 knot. UHD dtm HDR 8k\nBeautiful surreal symbolism the mesmerizing vision \nof a Cleopatra Queen of Egypt , full body , \nmesmerizing brown eyes, black hair and ethereal \nfeatures, radiating celestial aura, super high \ndefinition, true lifelike color, perfect exposure, razor \nsharp focus, golden ratio, soft reflections, bokeh \neffect, fine art photography, cinematic compositing, \nauthentic, professional by Rorianai style 36k s1000\nFigure 7 | More text-to-image generation results. We upsample the images to 1024 × 1024 for\nbetter visualization.\n16\nLlamaGen\nJanus (Ours)\nA\nrealistic\nphoto\nof\nsnowy mountains with\nsunlit peaks glistening\nunder a clear blue sky\n这是一张雪山写实照片，阳\n光照耀下的山峰在晴朗的蓝\n天下熠熠生辉。前景中高大\n的常青树被白雪覆盖，为这\n令人惊叹的高山景色增添了\n深度。\nUne photo réaliste de montagnes\nenneigées\ndont\nles\nsommets\néclairés par le soleil scintillent sous\nun ciel bleu limpide. Au premier\nplan, de grands arbres à feuilles\npersistantes saupoudrés de neige\najoutent de la profondeur à cette\nscène alpine à couper le souffle.\nA realistic photo of ❄⛰\nwith ☀under a clear blue\nsky. Tall, snow-dusted 🌲\nin\nthe\nforeground\nadd\ndepth to the breathtaking\nalpine scene.\n澄み切った青空の下、太陽に\n照らされた山頂が輝く雪山の\nリアルな写真。前景の雪に覆\nわれた背の高い常緑樹が、息\nをのむようなアルプスの風景\nに深みを与えている。\nFigure 8 | Multilingual text-to-image generation samples compared to LlamaGen [73]. Note\nthat we only use English text-to-image data in training, and this is an emergent capability of\nour model. The languages used in the prompt, from left to right, are: English, Chinese, French,\nJapanese, and English with emoji.\nPrompt\nGenerate the latex code of this formula.\nA _ { n } = a_ 0 \\left[ 1 + \\frac{3}{4} \n\\sum_{ k=1}^{n} \\left( \\frac{4}{9} \\right)^k \\right]\nResponse\nPrompt\nWhat algorithm does the image depict?\nResponse\nThe image depicts a CLIP (Contrastive \nLanguage-Image Pre-training) model. The \nCLIP model is a type of pre-training algorithm \nthat combines the power of language and \nvision to learn a unified representation of the \nworld. The image encoder processes the input \nimage, and the text encoder processes the text \nassociated with the image. The output of the \ntext encoder is then used to predict the correct \npair of images, which is a key component of the \nCLIP model.\nPrompt\nWhy is this painting interesting?\nResponse\nPrompt\nGenerate the matplotlib.pyplot code for this plot.\nResponse\nThe image you've provided is a pie chart that \nrepresents the distribution of different types of \nanimals based on their frequency. The chart is divided \ninto four segments, each representing a different type \nof animal and its corresponding percentage.\nHere's the code to generate the pie chart:\n```python\nimport matplotlib.pyplot as plt\n# Data\nanimals = [\n    {\"name\": \"Hogs\", \"frequency\": 30.0},\n    {\"name\": \"Frog\", \"frequency\": 15.0},\n    {\"name\": \"Logs\", \"frequency\": 10.0},\n    {\"name\": \"Dogs\", \"frequency\": 45.0}\n]\n# Create the pie chart\nplt.pie(animals, labels=animals, autopct='%1.1f%%', \nstartangle=90)\n# Set the title and labels\nplt.title('Distribution of Animals')\nplt.title.set_text('Frequency of Animals')\n# Show the plot\nplt.show()\n```\nThis image is an interesting piece of art \nbecause it combines a well-known painting, the \n\"Mona Lisa,\" with a dog. The dog is depicted in \nthe same pose as the Mona Lisa, but with a \ndifferent expression and a different \nbackground. The painting is a unique blend of \nart styles and subjects, making it stand out.\nFigure 9 | More multimodal understanding results. Janus has a strong multimodal understanding\ncapability and can handle inputs from various contexts, such as scientific charts, artwork images,\nLaTeX formula images, and more.\n17\nReferences\n[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,\nJ. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https:\/\/www.anthropic.\ncom, 2024.\n[3] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A fron-\ntier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966,\n2023.\n[4] Y. Bai, X. Wang, Y.-p. Cao, Y. Ge, C. Yuan, and Y. Shan. Dreamdiffusion: Generating\nhigh-quality images from brain eeg signals. arXiv preprint arXiv:2306.16934, 2023.\n[5] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al.\nDeepseek llm: Scaling open-source language models with longtermism. arXiv preprint\narXiv:2401.02954, 2024.\n[6] T. B. Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,\n2020.\n[7] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 11315–11325, 2022.\n[8] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy,\nW. T. Freeman, M. Rubinstein, et al. Muse: Text-to-image generation via masked generative\ntransformers. arXiv preprint arXiv:2301.00704, 2023.\n[9] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. Pixart-\n𝑎𝑙𝑝ℎ𝑎: Fast training of diffusion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023.\n[10] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving\nlarge multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\n[11] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick. Microsoft\ncoco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n[12] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How\nfar are we to gpt-4v? closing the gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821, 2024.\n[13] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu,\net al. Internvl: Scaling up vision foundation models and aligning for generic visual-\nlinguistic tasks. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 24185–24198, 2024.\n[14] X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei, et al.\nMobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.\narXiv preprint arXiv:2312.16886, 2023.\n18\n[15] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, et al.\nMobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint\narXiv:2402.03766, 2024.\n[16] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip:\nTowards general-purpose vision-language models with instruction tuning, 2023.\n[17] dclure. Laion-aesthetics-umap. https:\/\/huggingface.co\/datasets\/dclure\/laion\n-aesthetics-12m-umap, 2022.\n[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248–255. Ieee, 2009.\n[19] J. Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\n[20] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in\nneural information processing systems, 34:8780–8794, 2021.\n[21] R. Dong, C. Han, Y. Peng, Z. Qi, Z. Ge, J. Yang, L. Zhao, J. Sun, H. Zhou, H. Wei, et al. Dream-\nllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499,\n2023.\n[22] A. Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020.\n[23] Echo840. Detailed caption dataset. https:\/\/huggingface.co\/datasets\/echo840\/\nDetailed_Caption, 2023.\n[24] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 12873–12883, 2021.\n[25] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, et al.\nMme: A comprehensive evaluation benchmark for multimodal large language models.\narXiv preprint arXiv:2306.13394, 2023.\n[26] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on\nComputer Vision, pages 89–106. Springer, 2022.\n[27] Y. Ge, Y. Ge, Z. Zeng, X. Wang, and Y. Shan. Planting a seed of vision in large language\nmodel. arXiv preprint arXiv:2307.08041, 2023.\n[28] Y. Ge, S. Zhao, Z. Zeng, Y. Ge, C. Li, X. Wang, and Y. Shan. Making llama see and draw\nwith seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n[29] Y. Ge, S. Zhao, J. Zhu, Y. Ge, K. Yi, L. Song, C. Li, X. Ding, and Y. Shan. Seed-x: Multimodal\nmodels with unified multi-granularity comprehension and generation. arXiv preprint\narXiv:2404.14396, 2024.\n[30] D. Ghosh, H. Hajishirzi, and L. Schmidt. Geneval: An object-focused framework for\nevaluating text-to-image alignment. Advances in Neural Information Processing Systems,\n36, 2024.\n19\n[31] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\n[32] High-flyer. Hai-llm: Efficient and lightweight training tool for large models, 2023. URL\nhttps:\/\/www.high-flyer.cn\/en\/blog\/hai-llm.\n[33] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840–6851, 2020.\n[34] Y.-C. Hsiao, F. Zubach, M. Wang, et al. Screenqa: Large-scale question-answer pairs over\nmobile app screenshots. arXiv preprint arXiv:2209.08199, 2022.\n[35] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceedings of the IEEE\/CVF conference on\ncomputer vision and pattern recognition, pages 6700–6709, 2019.\n[36] Y. Jin, K. Xu, L. Chen, C. Liao, J. Tan, B. Chen, C. Lei, A. Liu, C. Song, X. Lei, et al. Unified\nlanguage-vision pretraining with dynamic discrete visual tokenization. arXiv preprint\narXiv:2309.04669, 2023.\n[37] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. Scaling up gans\nfor text-to-image synthesis. In Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pages 10124–10134, 2023.\n[38] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,\nA. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision, pages 4015–4026, 2023.\n[39] M. Koupaee and W. Y. Wang. Wikihow: A large scale text summarization dataset. arXiv\npreprint arXiv:1810.09305, 2018.\n[40] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov,\nM. Malloci, A. Kolesnikov, T. Duerig, and V. Ferrari. The open images dataset v4: Unified\nimage classification, object detection, and visual relationship detection at scale. IJCV, 2020.\n[41] H. Laurençon, D. van Strien, S. Bekman, L. Tronchon, L. Saulnier, T. Wang, S. Karamcheti,\nA. Singh, G. Pistilli, Y. Jernite, and et al. Introducing idefics: An open reproduction of\nstate-of-the-art visual language model, 2023. URL https:\/\/huggingface.co\/blog\/id\nefics.\n[42] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal\nllms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[43] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li.\nLlava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024.\n[44] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi. Playground v2. 5: Three\ninsights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint\narXiv:2402.17245, 2024.\n[45] L. Li, Y. Wang, R. Xu, P. Wang, X. Feng, L. Kong, and Q. Liu. Multimodal arxiv: A dataset\nfor improving scientific comprehension of large vision-language models. arXiv preprint\narXiv:2403.00231, 2024.\n20\n[46] T. Li, Y. Tian, H. Li, M. Deng, and K. He. Autoregressive image generation without vector\nquantization. arXiv preprint arXiv:2406.11838, 2024.\n[47] X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan. Densefusion-1m: Merging\nvision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303,\n2024.\n[48] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in\nlarge vision-language models. arXiv preprint arXiv:2305.10355, 2023.\n[49] Z. Li, X. Yang, K. Choi, W. Zhu, R. Hsieh, H. Kim, J. H. Lim, S. Ji, B. Lee, X. Yan, et al.\nMmsci: A multimodal multi-discipline dataset for phd-level scientific comprehension.\narXiv preprint arXiv:2407.04903, 2024.\n[50] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In\nProceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npages 26296–26306, 2024.\n[51] H. Liu, C. Li, Q. Wu, and Y. J. Lee.\nVisual instruction tuning.\nAdvances in neural\ninformation processing systems, 36, 2024.\n[52] H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and\nlanguage with ringattention. arXiv preprint arXiv:2402.08268, 2024.\n[53] M. Liu, R. Shi, K. Kuang, Y. Zhu, X. Li, S. Han, H. Cai, F. Porikli, and H. Su. Openshape:\nScaling up 3d shape representation towards open-world understanding. Advances in\nneural information processing systems, 36, 2024.\n[54] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mm-\nbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281,\n2023.\n[55] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun,\net al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint\narXiv:2403.05525, 2024.\n[56] P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu. Iconqa: A\nnew benchmark for abstract diagram understanding and visual language reasoning. arXiv\npreprint arXiv:2110.13214, 2021.\n[57] madebyollin. Megalith-huggingface. https:\/\/huggingface.co\/datasets\/madebyol\nlin\/megalith-10m, 2024.\n[58] mehdidc. Yfcc-huggingface. https:\/\/huggingface.co\/datasets\/mehdidc\/yfcc15\nm, 2024.\n[59] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and\nM. Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[60] J. Pan, K. Sun, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang, J. Dai,\nY. Qiao, and H. Li. Journeydb: A benchmark for generative image understanding, 2023.\n[61] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei. Beit v2: Masked image modeling with\nvector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\n21\n[62] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rom-\nbach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023.\n[63] ProGamerGov. Dalle3-high-quality-captions. https:\/\/huggingface.co\/datasets\/Pr\noGamerGov\/synthetic-dataset-1m-dalle3-high-quality-captions, 2024.\n[64] A. Radford. Improving language understanding by generative pre-training. 2018.\n[65] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.\nZero-shot text-to-image generation. In International conference on machine learning, pages\n8821–8831. Pmlr, 2021.\n[66] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n[67] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE\/CVF conference on\ncomputer vision and pattern recognition, pages 10684–10695, 2022.\n[68] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-\ntijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in neural information processing systems,\n35:36479–36494, 2022.\n[69] S. Shah, A. Mishra, N. Yadati, and P. P. Talukdar. Kvqa: Knowledge-aware visual question\nanswering. In Proceedings of the AAAI conference on artificial intelligence, volume 33,\npages 8876–8884, 2019.\n[70] V. Singla, K. Yue, S. Paul, R. Shirkavand, M. Jayawardhana, A. Ganjdanesh, H. Huang,\nA. Bhatele, G. Somepalli, and T. Goldstein. From pixels to prose: A large dataset of dense\nimage captions. arXiv preprint arXiv:2406.10328, 2024.\n[71] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[72] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork. Wit: Wikipedia-based\nimage text dataset for multimodal multilingual machine learning. In Proceedings of the\n44th international ACM SIGIR conference on research and development in information\nretrieval, pages 2443–2449, 2021.\n[73] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model beats\ndiffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024.\n[74] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. Eva-clip: Improved training techniques for\nclip at scale. arXiv preprint arXiv:2303.15389, 2023.\n[75] Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang.\nGenerative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023.\n[76] Q. Sun, Y. Cui, X. Zhang, F. Zhang, Q. Yu, Y. Wang, Y. Rao, J. Liu, T. Huang, and X. Wang.\nGenerative multimodal models are in-context learners. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pages 14398–14409, 2024.\n22\n[77] C. Team. Chameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024.\n[78] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.\nDai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023.\n[79] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang. Visual autoregressive modeling: Scalable\nimage generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024.\n[80] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière,\nN. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[81] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n[82] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, T. Lu, J. Zhou, Y. Qiao, et al.\nVisionllm: Large language model is also an open-ended decoder for vision-centric tasks.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[83] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al.\nEmu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024.\n[84] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua. Next-gpt: Any-to-any multimodal llm. arXiv\npreprint arXiv:2309.05519, 2023.\n[85] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, et al. Vila-u: a\nunified foundation model integrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024.\n[86] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z.\nShou. Show-o: One single transformer to unify multimodal understanding and generation.\narXiv preprint arXiv:2408.12528, 2024.\n[87] Z. Xue, G. Song, Q. Guo, B. Liu, Z. Zong, Y. Liu, and P. Luo. Raphael: Text-to-image gen-\neration via large mixture of diffusion paths. Advances in Neural Information Processing\nSystems, 36, 2024.\n[88] F. Yang, C. Ma, J. Zhang, J. Zhu, W. Yuan, and A. Owens. Touch and go: Learning from\nhuman-collected vision and touch. arXiv preprint arXiv:2211.12498, 2022.\n[89] L. Yu, Y. Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang,\nY. Hao, I. Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 10459–10469,\n2023.\n[90] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating\nlarge multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n[91] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al.\nMmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\nfor expert agi. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 9556–9567, 2024.\n23\n[92] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-\ntraining. In Proceedings of the IEEE\/CVF International Conference on Computer Vision,\npages 11975–11986, 2023.\n[93] C. Zheng, T.-L. Vuong, J. Cai, and D. Phung. Movq: Modulating quantized vectors for\nhigh-fidelity image generation. Advances in Neural Information Processing Systems, 35:\n23412–23425, 2022.\n[94] C. Zhou, L. Yu, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettle-\nmoyer, and O. Levy. Transfusion: Predict the next token and diffuse images with one\nmulti-modal model. arXiv preprint arXiv:2408.11039, 2024.\n[95] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[96] Y. Zhu, M. Zhu, N. Liu, Z. Ou, X. Mou, and J. Tang. Llava-phi: Efficient multi-modal\nassistant with small language model. arXiv preprint arXiv:2401.02330, 2024.\n24\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation.pdf"}
{"title":"PUMA: Empowering Unified MLLM with Multi-granular Visual Generation","authors":"Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu","summary":"Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps:\/\/github.com\/rongyaofang\/PUMA.","url":"http:\/\/arxiv.org\/abs\/2410.13861v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.13861v2","published":1729187997000,"comment":"Project page: https:\/\/rongyaofang.github.io\/puma\/","pdf_text":"Technical Report\nPUMA: EMPOWERING UNIFIED MLLM WITH\nMULTI-GRANULAR VISUAL GENERATION\nRongyao Fang1∗† Chengqi Duan2∗\nKun Wang3\nHao Li1,4\nHao Tian3\nXingyu Zeng3\nRui Zhao3\nJifeng Dai4,5\nHongsheng Li1‡ Xihui Liu2‡\n1CUHK MMLab\n2HKU MMLab\n3SenseTime\n4Shanghai AI Laboratory\n5THU\nDiversity\nControllability\nGenerate images of a puma in different postures.\nDraw an image based on canny input. \nRemove the tractor from the field.\nDiverse Text-to-Image Generation\nConditional Image Generation\nImage Manipulation\nGenerate an \nimage with \nthe caption: \na cute fox \nartwork by \nJames \nGilliard \nvibrant colors.\nDiverse Text-to-Image Generation\nWhat \nis the \npurpose \nof the \nsign?\nIt encourages people to take \nadvantage of the service \nwithout the need for prior \nscheduling, making it more \naccessible and convenient \nfor potential customers. \nImage Understanding\nPlease remove the \nblack cow statue \nfrom the bench.\nImage Editing\nThe image should \nconvey On the \nway to Eggum. \nUse inpainted \nimage as input to \ngenerate an image.\nImage Inpainting\nProduce a visual\nwith canny edge\nPortray Preparing \nfor Examinations, \n1864  by Ilya \nEfimovich Repin.\nConditional Image Generation\na) The Diversity and Controllability Requirements of Different Visual Generative Tasks\nb) PUMA for Various Visual Generation and Understanding Tasks\nFigure 1: a) Diversity and controllability tradeoff in image generation tasks: diverse text-to-image\ngeneration requires high diversity and fidelity, while tasks like conditional generation and manip-\nulation require high controllability on the image. b) The introduced PUMA, a unified multimodal\nlarge language model that processes and generates multi-granular visual representations, balancing\ndiversity and controllability across visual generation tasks. It excels in image understanding, diverse\ntext-to-image generation, editing, inpainting, colorization, and conditional image generation.\nABSTRACT\nRecent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm — from the diversity required in text-to-image generation to the precise\ncontrollability needed in image manipulation. In this work, we propose PUMA,\nemPowering Unified MLLM with Multi-grAnular visual generation. PUMA uni-\nfies multi-granular visual features as both inputs and outputs of MLLMs, elegantly\naddressing the different granularity requirements of various image generation\ntasks within a unified MLLM framework. Following multimodal pretraining and\ntask-specific instruction tuning, PUMA demonstrates proficiency in a wide range\nof multimodal tasks. This work represents a significant step towards a truly uni-\nfied MLLM capable of adapting to the granularity demands of various visual tasks.\nThe code and model will be released in https:\/\/github.com\/rongyaofang\/PUMA.\n∗Equal Contribution\n†Project Lead\n‡Corresponding Authors\n1\narXiv:2410.13861v2  [cs.CV]  21 Oct 2024\nTechnical Report\n1\nINTRODUCTION\nUnifying multimodal understanding and generation capabilities within a single model is a critical\nmilestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu\net al., 2024b; Zhu et al., 2023a) in multimodal large language models (MLLMs) have made signif-\nicant progress in integrating visual reasoning and understanding with natural language interfaces.\nHowever, developing a unified framework that excels at both comprehending and generating multi-\nmodal content remains a significant challenge in the field of artificial intelligence.\nRecent studies (Sun et al., 2023; Ge et al., 2024b) have explored MLLM’s potential for visual gen-\neration, beyond the previously well-explored visual understanding and reasoning with MLLMs.\nThese approaches enable MLLMs to process image-text inputs and produce either textual outputs\nor semantic-level visual tokens. In the case of image generation, these visual tokens are subse-\nquently transformed into pixel-space images using diffusion-based decoders. Such unified frame-\nworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging\nfrom detailed visual analysis to creative image synthesis.\nHowever, existing MLLM-based methods (Sun et al., 2023; 2024b) face a common challenge in\nthe trade-off between diversity for text-to-image generation and high controllability for tasks such\nas image editing. Previous methods mostly rely on single-granular features extracted from a vi-\nsual encoder and neglect the varying granularity requirements of different tasks. On the one hand,\ngenerating diverse images reflecting the real world from text descriptions requires features that en-\ncode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image\ndecoder, allowing the diffusion model to generate diverse images that semantically align with the\ntext prompt. On the other hand, tasks demanding precise control over output images, such as image\nediting and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed\nvisual information for the image decoder. This dichotomy presents a significant challenge for cur-\nrent MLLM-based methods, which typically generate single-granular feature representations for all\ntasks. As a result, models optimized for diverse image generation often lack the fine-grained con-\ntrollability necessary for detailed downstream tasks such as editing, while those focused on precise\ncontrollability produce less varied outputs for the task of text-to-image generation. Although re-\ncent work like SEED-X (Ge et al., 2024b) attempts to bypass this issue by leveraging condition\nimages directly input to the diffusion-based decoder for fine-grained control, a unified solution to\nthe multi-granularity problem remains underexplored.\nTowards the multi-granular feature demands of various tasks, we propose a novel paradigm\nemPowering Unified MLLM with Multi-grAnular visual generation (PUMA). PUMA facilitates\nseamless integration of image generation and understanding processes, while simultaneously han-\ndling multiple feature granularities — from coarse-grained abstractions to fine-grained details —\nwithin a single framework. By leveraging multi-scale features, our approach empowers MLLMs to\nexcel in diverse image generation and controllable downstream tasks, within a unified framework.\nOur method comprises three key modules: 1) An image encoder that extracts multi-granular rep-\nresentations, which serve as the foundation for visual generation and understanding; 2) An autore-\ngressive MLLM that processes and progressively generates multi-scale image features; and 3) A set\nof dedicated diffusion-based image decoders that decode images from MLLM-generated features\nat multiple granularities. To optimize this framework, we employ a two-stage training strategy:\nfirst fine-tuning the set of pre-trained diffusion models as our image decoders, where each model\nreconstructs or generates images conditioned on the corresponding feature granularities from the\nencoder; then training the autoregressive MLLM with regression loss supervised by the multi-scale\nencoder features to process and generate multi-granular image features. PUMA leverages large-\nscale pre-training followed by task-specific instruction tuning on a collection of linguistic-visual\ndatasets, enabling our model to handle various tasks including image understanding, text-to-image\ngeneration, editing, inpainting, colorization, and conditional generation.\nIn summary, we introduce a novel multi-granularity paradigm for MLLMs that addresses the lim-\nitations of existing single-scale methods. By simultaneously processing and generating features at\nmultiple granularities, our approach enables a unified framework to handle a wide range of tasks,\nfrom diverse image generation to precise editing and highly controllable generation. This unified\nframework represents a significant advancement towards more versatile and capable MLLMs, con-\ntributing to the broader goal of achieving AGI in multimodal domains.\n2\nTechnical Report\n2\nRELATED WORK\n2.1\nMULTIMODAL UNDERSTANDING\nThe rapid advancement of large language models (LLMs) has catalyzed significant progress in mul-\ntimodal large language models (MLLMs) for multimodal understanding tasks Dai et al. (2023); Li\net al. (2024b); Zhang et al. (2023a); Chen et al. (2024); Lin et al. (2024); Zhang et al. (2024b); Li\net al. (2024a). Pioneering works such as LLaVA (Liu et al., 2024b) and MiniGPT-4 (Zhu et al.,\n2023a) have demonstrated remarkable performance across diverse image understanding tasks, in-\ncluding visual question answering (VQA), visual reasoning, optical character recognition (OCR),\nand object grounding. These approaches typically employ visual encoders, such as the CLIP en-\ncoder (Radford et al., 2021), to extract continuous image features, which are then projected into the\nLLM’s embedding space for subsequent tasks. While successfully unifying various image under-\nstanding tasks within a single model, these methods mostly adhere to a multimodal-input, text-output\nparadigm. Consequently, they excel at text-based responses to visual inputs but cannot generate mul-\ntimodal outputs beyond text, limiting their applicability in tasks requiring visual content generation.\n2.2\nUNIFIED UNDERSTANDING AND GENERATION FOR MLLMS\nRecent research has focused on equipping MLLMs with multimodal output capabilities (Wu et al.,\n2023; Tang et al., 2024; Ye et al., 2024a; Zhu et al., 2023b). GILL (Koh et al., 2024) pioneered\nthe integration of image generation abilities into MLLMs. Subsequently, SEED-LLaMA (Ge et al.,\n2023) and Emu (Sun et al., 2023) further advanced image generation and understanding capabilities\nwithin MLLMs, while DreamLLM (Dong et al., 2023) proposed an end-to-end training approach\nfor enhanced performance.\nMore recent works, such as SEED-X (Ge et al., 2024b) and Emu2 (Sun et al., 2024b), have scaled up\nMLLMs for unified generation, adopting continuous feature-based methods. These approaches uti-\nlize pre-trained vision encoders to extract continuous semantic features, which MLLMs then autore-\ngressively regress. Specialized diffusion model-based decoders transform these MLLM-generated\nfeatures into pixel-space images. However, the single-scale image feature generation pipeline em-\nployed by these methods struggles to address tasks with varying granularity demands, making it\nchallenging to balance diverse image generation with fine-grained control for manipulation tasks.\nSEED-X attempts to address the multi-granularity issue by introducing conditional image input to\nthe diffusion-based decoder for fine-grained control. However, this approach limits its applicability\nto image editing tasks encountered during decoder training. Consequently, a unified solution to the\nmulti-granularity problem remains underexplored. In contrast, our work proposes a novel multi-\ngranularity paradigm that addresses these limitations by simultaneously handling multiple levels of\nfeature granularity within a single, unified framework.\nAlternative approaches have also been investigated. Chameleon (Team, 2024) explored using dis-\ncrete image tokens to bridge image understanding and generation, but the vector quantization process\nleads to information loss, hindering high-performance image understanding. TransFusion (Zhou\net al., 2024) and show-o (Xie et al., 2024) proposed transforming the MLLM backbone itself into\na denoiser in a diffusion-based or demasking-based approach. However, these methods require nu-\nmerous denoising steps for each image generation, resulting in substantial computational costs given\nthe scale of current MLLM backbones. VAR (Tian et al., 2024) is another track of generation frame-\nwork that implements hierarchical autoregressive with discrete tokens for image generation, but it\nonly discusses image generation and cannot unify multimodal tasks.\n3\nMETHOD\nExisting approaches typically optimize for either fine or coarse-grained features, resulting in a trade-\noff between precise control and generation diversity.\nTo overcome this limitation, we propose\nPUMA, a unified multi-granular MLLM paradigm. Our approach simultaneously processes multi-\nple levels of feature granularity within a unified MLLM framework, facilitating seamless transitions\nacross a wide spectrum of multimodal tasks.\nOur framework consists of three key components: an image encoder (Sec. 3.1), a set of image\ndecoders conditioned on different granular features (Sec. 3.2), and a multi-granular autoregressive\nMLLM (Sec. 3.3). These components work synergistically to extract, process, and generate multi-\nscale image features, adapting to various task-specific granularity requirements. To optimize our\n3\nTechnical Report\n[\/IMG]\n…\n…\nPUMA Multimodal Generative Model\n…\n…\n…\n…\n[Text Input]\n[Text Output]\n[IMG]\n…\n…\n…\n𝑓𝑁\n𝑓𝑁−1\n𝑓0\nExtract Multi-granular Image Feature\n…\n…\n…\nText Token \nClassification Loss\nMulti-granular Image Feature\nRegression Loss\nመ𝑓𝑁\nመ𝑓𝑁−1\nመ𝑓0\n1. Diverse Text to Image Generation\n[Prompt]\nPUMA\nመ𝑓𝑁\nመ𝑓𝑁−1\nCoarse-granular Image Feature\nDecoder\n+[Prompt]\nPUMA\nDecoder\nFine-granular Image Feature\n+[Instruction]\nPUMA\nመ𝑓0\nመ𝑓1\nDecoder\nFine-granular Image Feature\n+[Question]\nPUMA\n[Understanding and\nAnswer to the Question]\nUnified Multi-granular Autoregressive MLLM\n[\/IMG]\n…\n…\n…\nመ𝑓0\nመ𝑓1\n…\n…\n2. Image Editing\n3. Conditional Image Generation\n4. Image Understanding\nFigure 2: Upper: PUMA’s unified multi-granular autoregressive pipeline for processing and gen-\nerating text and multi-granular visual features. Lower: Illustration of PUMA’s versatility across\nvarious tasks: 1) diverse text-to-image generation, 2) image editing, 3) conditional image genera-\ntion, and 4) image understanding, showcasing different input-output configurations.\nMLLM, we employ a two-stage process of pretraining and instruction tuning (Sec. 3.4), enabling it\nto perform a wide range of tasks including image understanding, generation, editing, and conditional\nimage generation.\n3.1\nIMAGE ENCODING AND MULTI-GRANULAR FEATURE EXTRACTION\nOur unified multi-granularity paradigm leverages a semantic image encoder to extract multi-scale\nfeatures, forming the foundation for diverse visual task processing. We employ a CLIP (Radford\net al., 2021) semantic image encoder to process input images x and generate the initial set of high-\nresolution features f0 ∈RH×W ×C, with H and W representing the spatial dimensions of the\nhighest resolution feature grid, and C denoting the channel dimension. In our setting, the feature\nsize is H = W = 16, thus the highest resolution feature f0 has 256 visual tokens.\nTo obtain multi-granular representations, we derive lower resolution features through successive\napplications of 2D average pooling with kernel size 2 and stride 2:\nfi = AvgPool(fi−1),\ni = 1, 2, ..., N\n(1)\nwhere N is the number of additional granular levels. This process generates a series of feature\ngrids at progressively coarser resolutions, ranging from fine-grained features preserving detailed\nspatial information and local textures, through mid-level features capturing object parts and regional\nstructures, to features representing coarse-grained semantic concepts. These features are denoted as\nf0, f1, f2, f3, and f4, which have 256, 64, 16, 4, and 1 visual tokens respectively.\n3.2\nMULTI-GRANULAR VISUAL DECODING\nImage features at different granularities encode varying levels of information. We employ diffusion-\nbased models as decoders due to their flexible capability to handle multi-scale features. When\nprocessing coarse-grained semantic features, the decoders can effectively synthesize missing fine-\ngrained information with their learned image priors and generate diverse, semantics-aligned images.\nOn the other hand, when handling fine-grained features, they accurately reconstruct precise image\ndetails. This versatility in generating or reconstructing images across different granularities makes\ndiffusion-based models suitable for our multi-granularity approach.\n4\nTechnical Report\nOriginal image\n𝐷0 𝑓0\n𝐷1 𝑓1\n𝐷2 𝑓2\n𝐷3 𝑓3\n𝐷4 𝑓4\nFine-grained reconstruction\nSemantic-guided generation\nFigure 3: Multi-granular visual decoding from fine-grained to coarse-grained granularity.\nWe develop a set of dedicate diffusion-based image decoders D0, D1, ..., DN corresponding to the\nfeature scales f0, f1, ..., fN. These decoders enable the visual decoding of images at various levels of\ngranularity. We formulate the image decoding process for each granularity level i as ˆxi = Di(fi, z),\nwhere ˆxi is the decoded image, fi is the feature map at granularity level i, and z is a random noise\nvector for the diffusion process.\nWe leverage the pre-trained SDXL models (Podell et al., 2023) as our decoding framework and fine-\ntune these pre-trained models to generate or reconstruct images conditioned on different granular\nfeatures. By modifying the conditional input mechanism through cross-attention in SDXL to accept\nour multi-granular features fi, we harness the models’ inherent ability to decode coherent images.\nMulti-granular\nVisual Decoding\nEncoder\n𝑓0\n…\nAvgPool\nAvgPool\nDecoder\n𝐷0\nDecoder\n𝐷𝑁−1\nDecoder\n𝐷𝑁\n…\n…\n𝐻\n𝑊\n𝑓𝑁−1\n2\n2\n𝑓𝑁\n1\n1\nFigure 4: Training phase of multi-granular visual decoding.\nFig. 4 shows the training process of\ndifferent granular image decoding,\nduring which the image encoder is\nfrozen to preserve semantic property.\nFig. 3 illustrates the visual decod-\ning capabilities of multi-granular de-\ncoders.\nThe visualizations demon-\nstrate the fidelity of decoded images\nacross different granularities, with\nfiner-grained features yielding recon-\nstructions closer to the original input,\nand coarser-grained features leading\nto image generation guided by the\nsemantics of the input image. This\nvalidates the effectiveness of our ap-\nproach in preserving and utilizing\nmulti-granular visual information.\nThis multi-granular decoding framework, in conjunction with our hierarchical feature extraction,\nestablishes a foundation for the subsequent stages of our MLLM architecture, paving the way for\ndiverse visual tasks in later training phases.\n3.3\nPROGRESSIVE MULTI-GRANULAR IMAGE MODELING IN AUTOREGRESSIVE MLLM\nDriven by the goal of utilizing a unified framework capable of adapting to a wide range of visual-\nlinguistic tasks with varying granularity requirements, we design an autoregressive MLLM to pro-\ncess and generate both text tokens and multi-granular image features.\nOur autoregressive MLLM, denoted as M, processes text and multi-granular image features pro-\ngressively, as illustrated in Fig. 2. The model processes features token by token, predicting each\ntoken sequentially within each granularity level, and progressing from the coarsest level N to the\n5\nTechnical Report\nfinest level 0. This approach allows the model to refine its predictions as more detailed information\nbecomes available.\nWe structure the input sequence as a concatenation of text tokens and flattened image feature tokens\nfrom multiple granularity levels. This progressive approach enables the model to capture dependen-\ncies across different scales, from coarse global structures to fine local details.\nThe MLLM is trained using an autoregressive next token prediction objective, combining both text\nand image losses:\nL = −\nX\ni\nlog P(ti|t<i, F<i) +\nN\nX\ni=0\nαi\nki\nX\nj=1\n|fi,j −ˆfi,j|2\n(2)\nThe first term represents the cross-entropy loss for text token prediction, where ti are text tokens.\nThe second term is the regression loss for image feature prediction, where fi,j and ˆfi,j are the\nground truth and predicted feature tokens, respectively, at the i-th granularity level. ki is the number\nof visual tokens at the i-th granularity level. The coefficient αi allows for adjusting the importance\nof each granularity level during training.\n3.4\nMULTIMODAL PRETRAINING AND INSTRUCT TUNING\nTo demonstrate the effectiveness of our unified multi-granularity paradigm, we implement a com-\nprehensive two-stage training pipeline for PUMA: multimodal pretraining followed by task-specific\ninstruct tuning. This approach allows our model to first acquire broad multimodal capabilities before\nspecializing in targeted visual-linguistic tasks during the subsequent instruct tuning stage.\nMultimodal Pretraining:\nOur multimodal pretraining leverages a diverse set of large-scale\ndatasets: Laion-2B (Schuhmann et al., 2022), Laion-Aesthetics (Burger, 2023), GRIT (Peng et al.,\n2023), The Pile (Gao et al., 2020), OCR-VQA-200K (Mishra et al., 2019), and LLaVAR (Zhang\net al., 2023b). This combination of datasets provides a rich mixture of image-text pairs, textual data,\nand specialized visual question-answering samples. To enhance the model’s bidirectional under-\nstanding of image-text relationships, we employ a dynamic training strategy that randomly alternates\nbetween text-to-image and image-to-text tasks for each image-text pair.\nInstruct Tuning: Following pretraining, we conduct targeted instruct tuning to adapt our model\nto specific visual-linguistic tasks. To evaluate PUMA’s performance across different task types, we\nfine-tune four dedicated models for the four types of tasks, each initialized from the pretraining\ncheckpoint.\nHigh-quality Text-to-Image Generation: We utilize Laion-Aesthetics (Burger, 2023) and JourneyDB\n(Sun et al., 2024a) to focus on generating aesthetically pleasing and diverse images.\nPrecise Image Manipulation: Training on the SEED-Edit (Ge et al., 2024a) dataset enables accurate\nand controlled image editing.\nConditional Image Generation: The subset of MultiGen-20M dataset (Qin et al., 2023) including\ncanny-to-image, inpainting, and colorization is employed to equip the model with the ability to\ngenerate images under specific conditions and constraints.\nImage Understanding: Fine-tuning on the subset of LLaVA-OneVision (Li et al., 2024a) and Cam-\nbrain (Tong et al., 2024) to enhance the model’s image comprehension capabilities. Data about\nmath\/reasoning and cross-duplicated data in the two datasets are removed.\n4\nEXPERIMENTS\nWe present our experimental results as follows: Sec. 4.1 details our experimental setup. In Sec. 4.2,\nwe evaluate the effectiveness of our multi-granularity feature encoding and diffusion-based multi-\ngranularity image decoders. We then demonstrate PUMA’s versatility across various tasks: di-\nverse text-to-image generation (Sec. 4.3), image editing (Sec. 4.4), conditional image generation\n(Sec. 4.5), and vision-language understanding (Sec. 4.6).\n6\nTechnical Report\nOriginal image\nSEED-LLaMA\nSEED-X\nEmu2\nOurs (𝑓0 scale)\nFigure 5: Fine-grained image reconstruction of SEED-LLaMA (Ge et al., 2023), SEED-X (Ge et al.,\n2024b), Emu2 (Sun et al., 2024b) and PUMA (f0 scale). High quality image reconstruction is the\nfoundation of precise image manipulation tasks.\nTable 1: Image decoding evaluation using image encoder and decoder on the ImageNet validation\nset. PSNRr and LPIPSr measure the difference between reconstructed and ground truth images.\nPSNRd and LPIPSd measure the difference between two separate reconstructions of the same image,\nreflecting decoding diversity.\nModel\nEncoder foundation\nToken num.\nPSNRr↑LPIPSr↓PSNRd↓LPIPSd↑\nSEED-LLaMA (2023)\nBLIP-2 ViT (0.3B)\n32\n9.73\n0.6756\n10.45\n0.6189\nSEED-X (2024b)\nQwen-VL Encoder (4B)\n64\n10.86\n0.5152\n11.60\n0.4292\nEmu2 (2024b)\nEVA02-CLIP-E-plus (4B)\n64\n15.72\n0.2532\n16.07\n0.2101\nPUMA (f4 scale)\nCLIP-Large (0.3B)\n1\n10.76\n0.6481\n12.82\n0.5751\nPUMA (f3 scale)\nCLIP-Large (0.3B)\n4\n11.04\n0.5971\n12.61\n0.5329\nPUMA (f2 scale)\nCLIP-Large (0.3B)\n16\n12.35\n0.4992\n13.50\n0.4354\nPUMA (f1 scale)\nCLIP-Large (0.3B)\n64\n13.26\n0.4325\n14.12\n0.3631\nPUMA (f0 scale)\nCLIP-Large (0.3B)\n256\n18.16\n0.2215\n19.36\n0.1559\n4.1\nSETUP\nOur unified multi-granular MLLM employs LLaMA-3 8B (Touvron et al., 2023) as the language\nmodel backbone and CLIP-Large (224 × 224 input) (Radford et al., 2021) as the image encoder.\nThe image decoders are initialized from pretrained SDXL models (Podell et al., 2023). For more\ndetails on the experimental setup, please refer to the Appendix.\n4.2\nMULTI-GRANULAR VISUAL DECODING\nWe evaluate the multi-granular visual decoding capabilities of our model using multi-scale features\nfrom the encoder (Sec. 3.1) and dedicated visual decoders (Sec. 3.2). Our aim is twofold: to achieve\nprecise reconstruction using fine-grained feature scales (such as f0 and f1), and to implement high\ndiversity semantics-guided image generation using coarse-grained features (such as f4 and f3). It\nis worth mentioning that in this subsection we validate the multi-granularity encoder and decoders\n(Fig. 4), while the MLLM (Sec. 3.3) is not leveraged for the experiments in this subsection.\n4.2.1\nFINE-GRAINED IMAGE RECONSTRUCTION\nFine-grained image reconstruction is crucial for preserving image details, yet it has posed significant\nchallenges for models like SEED-LLaMA (Ge et al., 2023), SEED-X (Ge et al., 2024b), and Emu2\n(Sun et al., 2024b). While SEED-LLaMA and SEED-X struggle with detailed reconstruction, limit-\ning their precise image manipulation capabilities without additional techniques such as conditional\nimage input (as used in SEED-X), Emu2 attempts to improve reconstruction by scaling up its image\n7\nTechnical Report\n𝑓4 scale-seed: 1\n𝑓4 scale-seed: 2\n𝑓3 scale-seed: 1\n𝑓3 scale-seed: 2\nGeneration prompt: Anthropomorphic rat, wearing harajuku street wear, decora kei, hyper realistic, clothing shops, shop \nsigns, barbie aesthetic, kidcore, graphic.\nEmu2-seed: 1\nEmu2-seed: 2\nFigure 6: Diversity visualization of text-to-image generation results from PUMA feature scales f4\n(1 visual token), f3 (4 visual tokens), and Emu2 (Sun et al., 2024b). The generated features are input\nto corresponding diffusion-based decoders with different random seeds.\nTable 2: Diverse text-to-image generation evaluation on MSCOCO 30K validation set. CLIP-I\nand CLIP-T measure the similarity between generated images and ground truth images or prompts.\nLPIPSd quantifies the difference between two images generated from the same prompt, reflecting\ngeneration diversity. 5-scale Max denotes selecting the image with the highest score among the 5\noutputs and computes the average maximum value.\nModel\nToken num.\nCLIP-I↑\nCLIP-T↑\nLPIPSd↑\nSD-v1.5 (2022)\n-\n0.667\n0.302\n0.692\nDALL-E 2 (2022)\n-\n-\n0.314\n-\nSDXL (2023)\n-\n0.674\n0.310\n0.600\nDALL-E 3 (2023)\n-\n-\n0.320\n-\nSEED-LLaMA (2023)\n32\n0.682\n-\n0.652\nEmu (2023)\n64\n0.656\n0.286\n0.700\nEmu2 (2024b)\n64\n0.686\n0.297\n0.329\nSEED-X (2024b)\n64\n0.729\n0.314\n0.493\nPUMA (f4 scale)\n1\n0.699\n0.295\n0.613\nPUMA (f3 scale)\n4\n0.703\n0.300\n0.558\nPUMA (5-scale Max)\n-\n0.736\n0.317\n-\nencoder to 4 billion parameters. Our approach achieves superior reconstruction quality with a more\nefficient architecture. We employ the CLIP-Large encoder (0.3 billion parameters), which is over\n10 times smaller than Emu2’s, and implement fine-grained level image embedding with 256 tokens.\nAs demonstrated in Tab. 1, our method using f0 scale features achieves 18.16 PSNRr and 0.2215\nLPIPSr (Zhang et al., 2018) on the ImageNet validation set reconstruction. These results outperform\nEmu2’s reconstruction performance and significantly surpass SEED-LLaMA and SEED-X (without\nconditional input). Fig. 5 visually illustrates our method’s superior reconstruction quality.\n4.2.2\nSEMANTICS-GUIDED GENERATION\nWhile fine-grained reconstruction is crucial for precise image manipulation, tasks like text-to-image\ngeneration benefit from a balance of semantic fidelity and output diversity. Our approach leverages\ncoarse-grained features (such as f4) to implement semantics-guided image generation that preserves\ndiversity in outputs. To quantify this semantics-guided diversity, we decode twice to obtain two\nimages from the same image input using different random seeds and measure their differences,\ndenoted as PSNRd and LPIPSd. Tab. 1 presents the diversity results for various visual decoding\nmodels and feature scales. Notably, our f3 and f4 scale decoders produce more diverse samples\ncompared to the decoders in SEED-X and Emu2, while still preserving the core semantics of the\ninput, as illustrated in Fig. 5. This demonstrates our approach’s effectiveness in balancing semantic\naccuracy with generative diversity, a crucial factor in tasks like text-to-image generation.\n4.3\nDIVERSE TEXT-TO-IMAGE GENERATION\nOur method can generate diverse outputs by utilizing the coarse-grained feature (f4 and f3 scales).\nThis capability enables our model to produce diverse images that correspond to text conditions.\nFig. 6 demonstrates that when generating images with a fixed text prompt utilizing feature scales\nf4 and f3, our model achieves high generation diversity. It also shows that f4 scale outputs exhibit\nhigher diversity, while f3 scale results demonstrate better consistency. In contrast, the generation\n8\nTechnical Report\nPUMA 𝑓4 scale\nPUMA 𝑓3 scale\nprompt1\nprompt2\nprompt3\nprompt4\nprompt1: Painting the entire universe in nutshell, line art drawing, magical scene, highly detailed, soft \norange, mint green, soft blue, soft yellow, soft red, sharp outlines, sharp brush strokes, isolated.\nprompt2: A beautiful blonde girl with futuristic wasp-inspired armour, compound eye, intricate design, \nunreal engine, cinematic lighting.\nprompt3: A girl with white hair holding a harfang owl in her arms, artwork by james gilleard,vibrant colours.\nprompt4: Cluster of magic mushrooms in a dark lush green forest during a storm.\nFigure 7: Visualization of text-to-image generation results from PUMA feature scales f4 (1 visual\ntoken) and f3 (4 visual tokens).\nTable 3: Image editing evaluation on Emu-edit test benchmark (Sheynin et al., 2024). 5-scale Max\ndenotes selecting the image with the highest score among the 5 outputs and computes the average\nmaximum value.\nModel\nCLIP-I↑\nCLIP-T↑\nDINO↑\nInstructPix2Pix (2023)\n0.834\n0.219\n0.762\nMagicBrush (2024a)\n0.838\n0.222\n0.776\nEMU-Edit (2024)\n0.859\n0.231\n0.819\nOmniGen (2024)\n0.836\n0.233\n0.804\nPUMA (f1 scale)\n0.802\n0.258\n0.679\nPUMA (f0 scale)\n0.840\n0.264\n0.784\nPUMA (5-scale Max)\n0.846\n0.270\n0.785\nresults of Emu2 (Sun et al., 2024b) show low diversity. For qualitative evaluation, Fig. 7 presents\nvisualizations of our model’s text-to-image generation with various prompts. For quantitative re-\nsults, we evaluate our model on the MSCOCO 30K validation dataset (Lin et al., 2014) and present\nthe CLIP-I, CLIP-T, and LPIPSd in Tab. 2, which the former two metrics measures the consistency\nwhile LPIPSd measures generation diversity. Compared with recent works, our model demonstrates\nsuperior performance in generation quality, diversity, and prompt relevance.\n4.4\nIMAGE EDITING\nTo assess PUMA’s image editing capabilities, we evaluated it on the Emu-Edit test benchmark\n(Sheynin et al., 2024). Tab. 3 presents the results using CLIP-I, CLIP-T, and DINO (Caron et al.,\n2021) scores. CLIP-I and DINO scores measure the model’s ability to preserve elements from the\nsource image, while CLIP-T reflects the consistency between the output image and the target cap-\ntion. Our results demonstrate that PUMA exhibits strong preservation ability, second only to the\ncurrent state-of-the-art model, EMU-Edit. Notably, PUMA achieves significantly better CLIP-T\n9\nTechnical Report\nAdd a statue of a man.\nReplace the \nwoman with a child.\nTurn to the van gogh style.\nInput image\nPUMA 𝑓1 scale\nPUMA 𝑓0 scale\nInput image\n在此处键入公式。\nFigure 8: Left: Visualizations of PUMA’s image editing result. Image editing utilizes f0 scale fea-\nture to preserve the fine-grained detail of input image. Right: Visualization of PUMA’s conditional\ngeneration results. ❶: canny-to-image generation; ❷: image inpainting; ❸: image colorization.\nPUMA 𝑓0 scale\nPUMA 𝑓1 scale\nInput image\nInput image\nPUMA 𝑓0 scale\nPUMA 𝑓1 scale\nReplace the trees with palm trees.\nTurn the grayscale image into an image.\nFigure 9: Comparison of f0 and f1 feature scales for tasks requiring precise controllability.\nTable 4: Evaluation on multimodal understanding benchmarks. PUMA utilizes CLIP-Large encoder\nwith 224 × 224 input. Und. and Gen. denote “understanding” and “generation”, respectively.\nType\nModel\n# Params MMB↑MME↑GQA↑VQAv2(test)↑POPE↑Vizwiz↑\nUnd. Only\nLLaVA-v1.5 (2024a)\n7B\n64.3\n1510.7\n62.0\n78.5\n85.9\n50.0\nInstructBLIP (2023)\n13B\n-\n1212.8\n49.5\n-\n78.9\n33.4\nQwen-VL-Chat (2023)\n7B\n-\n1487.5\n57.5\n78.2\n-\n38.9\nmPLUG-Owl2 (2024b)\n7B\n64.5\n1450.2\n56.1\n79.4\n85.8\n54.5\nUnd. and Gen.\nEmu (2023)\n13B\n-\n-\n-\n57.2\n-\n-\nNExT-GPT (2023)\n7B\n58.0\n-\n-\n66.7\n-\n48.4\nSEED-X (2024b)\n17B\n75.4\n1457.0\n47.9\n-\n84.2\n-\nChameleon (2024)\n34B\n-\n-\n-\n66.0\n-\n-\nEmu2-Chat (2024b)\n40B\n-\n-\n65.1\n84.9\n-\n54.9\nPUMA (Ours)\n8B\n68.9\n1490.3\n60.6\n76.2\n85.2\n47.9\nscores, even surpassing the state-of-the-art model. This indicates superior alignment between edited\nimages and target captions. For qualitative evaluation, Fig. 8 provides visualizations of the editing\nresults, illustrating PUMA’s effectiveness in image manipulation tasks.\n4.5\nCONDITIONAL IMAGE GENERATION\nWe select a subset of canny-to-image, inpainting, and colorization tasks from the multigen-20M\ndataset to train PUMA’s conditional image generation ability. Fig. 8 demonstrates the conditional\ngeneration results for these tasks. The f0 feature scale results provide the highest preservation of\nimage details, particularly for tasks like inpainting and colorization, while the f1 scale offers better\noverall visual fidelity with limited generation diversity.\n4.6\nIMAGE UNDERSTANDING\nWe evaluate PUMA’s image understanding performance on several MLLM benchmarks, including\nMMB (Liu et al., 2023), MME (Fu et al., 2024), GQA (Hudson & Manning, 2019), VQAv2 (Antol\n10\nTechnical Report\net al., 2015), POPE (Li et al., 2023), and Vizwiz (Gurari et al., 2018). Tab. 4 presents the results\nof this evaluation. Despite PUMA’s relatively few 8B parameters and the use of an image encoder\nwith 224 × 224 resolution input, it demonstrates competitive and often superior image understand-\ning performance compared to other unified understanding and generation models. Notably, PUMA’s\nperformance on some metrics even surpasses that of understanding-only baselines. This perfor-\nmance can be attributed to PUMA’s use of multi-granular continuous visual tokens as input to the\nMLLM. A detailed ablation study examining the impact of different scale features as input on image\nunderstanding tasks is provided in the Appendix, offering further insights into the effectiveness of\nPUMA’s multi-granular approach.\n4.7\nABLATION\nWe conduct an ablation study to examine the impact of feature scale selection on tasks requiring\nfine-grained controllability. Fig. 9 compares the outputs of f0 and f1 feature scales for image editing\nand colorization tasks. The results demonstrate that f1 scale features are insufficient for preserving\ncrucial image details, while f0 scale features maintain the necessary fine-grained information for\nprecise manipulation tasks. More ablation studies are in the Appendix.\n5\nCONCLUSION\nIn this paper, we introduce PUMA, a novel unified multi-granular MLLM that unifies various gran-\nular tasks in visual generation and understanding. By leveraging multi-granular representations,\nPUMA effectively addresses the challenge of balancing diversity and controllability in image gen-\neration tasks. Our approach demonstrates superior performance across a spectrum of visual tasks,\nincluding diverse text-to-image generation, image editing, inpainting, colorization, conditional gen-\neration, and understanding. PUMA’s ability to adapt to varying granularity requirements within a\nsingle framework represents a significant advancement in MLLM capabilities. This work opens up\nnew possibilities for more versatile and powerful multimodal AI systems, contributing to the broader\ngoal of achieving artificial general intelligence in multimodal domains.\nREFERENCES\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-\nnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international\nconference on computer vision, pp. 2425–2433, 2015.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities.\narXiv preprint arXiv:2308.12966, 2023.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https:\/\/cdn. openai. com\/papers\/dall-e-3. pdf, 2(3):8, 2023.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image\nediting instructions. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18392–18402, 2023.\nLaura Jannes Burger. Laion: Image data, ai, and dispossession. Master’s thesis, 2023.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE\/CVF international conference on computer vision, pp. 9650–9660, 2021.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pp. 24185–24198, 2024.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\n11\nTechnical Report\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and\ncreation. arXiv preprint arXiv:2309.11499, 2023.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu\nZheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation\nbenchmark for multimodal large language models, 2024.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making\nllama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\nYuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: A\nhybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024a.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying\nShan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation.\narXiv preprint arXiv:2404.14396, 2024b.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and\nJeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617,\n2018.\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pp. 6700–6709, 2019.\nDiederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language\nmodels. Advances in Neural Information Processing Systems, 36, 2024.\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei\nLi, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326, 2024a.\nYanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng\nLiu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models.\narXiv preprint arXiv:2403.18814, 2024b.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating\nobject hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\nJi Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-\ntraining for visual language models. In Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pp. 26689–26699, 2024.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740–755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 26296–26306, 2024a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems, 36, 2024b.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,\nJiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around\nplayer? arXiv preprint arXiv:2307.06281, 2023.\n12\nTechnical Report\nI Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In 2019 international conference on document\nanalysis and recognition (ICDAR), pp. 947–952. IEEE, 2019.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nCan Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Car-\nlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for\ncontrollable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models.\nAdvances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nShelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh,\nand Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 8871–\n8879, 2024.\nKeqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding.\nAdvances in Neural Information Processing Systems, 36, 2024a.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context\nlearners. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 14398–14409, 2024b.\nZineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation\nvia composable diffusion. Advances in Neural Information Processing Systems, 36, 2024.\nChameleon Team.\nChameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024.\nKeyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling:\nScalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024.\nShengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha\nAkula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open,\nvision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024.\n13\nTechnical Report\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multi-\nmodal llm. arXiv preprint arXiv:2309.05519, 2023.\nShitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting\nWang, Tiejun Huang, and Zheng Liu.\nOmnigen: Unified image generation.\narXiv preprint\narXiv:2409.11340, 2024.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,\nYuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer\nto unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024.\nHanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan\nXu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv\npreprint arXiv:2405.19335, 2024a.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei\nHuang. mplug-owl2: Revolutionizing multi-modal large language model with modality collabo-\nration. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npp. 13040–13051, 2024b.\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated\ndataset for instruction-guided image editing. Advances in Neural Information Processing Systems,\n36, 2024a.\nPan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong\nDuan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: A versatile large vision language\nmodel supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024b.\nRenrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,\nHongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-\ninit attention. arXiv preprint arXiv:2303.16199, 2023a.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586–595, 2018.\nYanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.\nLlavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107, 2023b.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob\nKahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and\ndiffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023a.\nJinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang,\nand Ying Shan. Vl-gpt: A generative pre-trained transformer for vision and language understand-\ning and generation. arXiv preprint arXiv:2312.09251, 2023b.\n14\nTechnical Report\nA\nPUMA TRAINING\nA.1\nVISUAL DECODING TRAINING\nA.1.1\nDATASET DETAILS\nFor training the image decoding process, we leverage three large-scale datasets: Laion-2B (Schuh-\nmann et al., 2022), Laion-Aesthetics (Burger, 2023), and JourneyDB (Sun et al., 2024a). To ensure\nhigh-quality generation capabilities, we apply a resolution-based filtering criterion, selecting only\nimages with resolutions of 512 × 512 pixels or larger. We only use center crop as the data augmen-\ntation method.\nA.1.2\nTRAINING SETTINGS\nWe train five dedicated image decoders for the f0, f1, f2, f3, and f4 scale features respectively. The\nimage encoder is the frozen CLIP-L image encoder (Radford et al., 2021). Each image decoder is\ninitialized from the SDXL model. The VAE (Kingma, 2013) remains frozen throughout the training\nprocess. The corresponding image features are input to the diffusion model through the cross-\nattention mechanism, replacing the original text embedding input. We train the decoders using\nAdamW optimizer (Loshchilov, 2017) with a maximum learning rate of 8e-5, using linear learning\nrate decay and a gradient clipping value of 1.0. The training batch size is 1,024. The training steps\nfor the five features are 40, 000, 30, 000, 20, 000, 15, 000, and 10, 000 respectively, with features\ncontaining more visual tokens using longer training steps. We use noise off value of 0.1 and random\ndrop of 10% of the input image to blank image for classifier-free guidance.\nA.2\nMLLM TRAINING\nA.2.1\nTRAINING OBJECTIVE\nPUMA employs a unified framework with supervision on both text tokens and image features. For\ntext tokens, we use cross-entropy classification loss, while for image features, we adopt MSE re-\ngression loss. To balance the contribution of text and image outputs, we apply a loss ratio of 0.02 for\ntext and 1.0 for image features. Within the image feature regression loss, we use different ratios for\nthe progressively generated 5 scales of image features (f4, f3, f2, f1, and f0), with ratios of 1024.0,\n512.0, 64.0, 8.0, and 1.0 respectively. This scaling compensates for the varying number of tokens\nat each feature scale, with larger ratios for scales with fewer tokens. The training loss objective\nremains consistent across both the pretraining and instruction tuning phases.\nA.2.2\nPRETRAINING DATASET DETAILS\nDuring PUMA’s pretraining phase, we utilize a diverse set of datasets including Laion-2B (Schuh-\nmann et al., 2022), Laion-Aesthetics (Burger, 2023), GRIT (Peng et al., 2023), The Pile (Gao et al.,\n2020), OCR-VQA-200K (Mishra et al., 2019), and LLaVAR (Zhang et al., 2023b). For the image-\ntext pair data in Laion-2B, Laion-Aesthetics, and GRIT, we randomly assign 50% of the samples\nto text-to-image training and 50% to image-to-text training, fostering both image generation and\nunderstanding capabilities. We employ center crop as the primary image augmentation technique.\nTo train on the GRIT dataset for object grounding, we append 224 additional position tokens to\nthe MLLM’s codebook, representing object positions with bounding box coordinates [x min,\ny min, x max, y max]. We construct the training sequences by appending the tokens <s>\nand <\/s> to denote the beginning and end of each sequence. At the beginning and end of each\nimage feature sequence, we include the special tokens [IMG] and [\/IMG] to indicate the visual\nposition.\nA.2.3\nPRETRAINING SETTINGS\nWe conduct pretraining for 100K steps using the AdamW optimizer with a batch size of 2048. The\nmaximum learning rates are set to 1e-4 for the projector and 3e-5 for the LLaMA backbone. We\nemploy a 2,000-step warm-up period, cosine learning rate decay, and gradient clipping at 5.0 during\npretraining. To optimize memory usage and computational efficiency, training is accelerated using\nDeepSpeed ZeRO Stage 3. The entire pretraining process is carried out on 256 NVIDIA V100 GPUs\nover a period of 10 days.\n15\nTechnical Report\nA.2.4\nINSTRUCT TUNING SETTINGS\nHigh-quality Text-to-Image Generation: We utilize Laion-Aesthetics (Burger, 2023) and JourneyDB\n(Sun et al., 2024a) with a data ratio 1:1 to instruct tune the text-to-image generation model based on\nthe previous pretraining checkpoint. We use training batch size 2048 and train for 20,000 steps with\nthe max learning rate 1e-5, warm up 1,000 steps, and cosine learning rate decay. Random crop with\nfixed aspect ratio is adopted as the image augmentation.\nPrecise Image Manipulation: We train the image manipulation task with SEED-Edit Ge et al.\n(2024a).\nIt contains seven different operations: background alteration, comprehensive image\nchanges, style alteration, object removal, object addition, localized modifications, and color\/texture\nalterations. We train with batch size 1024 and train for 10,000 steps. The max learning rate is\n1e-5, warm-up is 500 steps, and cosine learning rate decay is adopted. We apply random crop\nwith fixed aspect ratio on the accordingly input image and output image. The sequence of the image\nmanipulation sample is like “<s>[IMG]embedding of origin image[\/IMG]instruct\nediting prompt[IMG]embedding of edited image[\/IMG]<\/s>”.\nConditional Image Generation: We train on the subset of MultiGen-20M dataset (Qin et al.,\n2023) including canny-to-image, image inpainting, and colorization. We use the training batch\nsize 1, 024 and train for 20, 000 steps. The max learning rate is 1e-5, warm-up is 500 steps, and\ncosine learning rate decay is adopted. We apply center crop as the image augmentation. The\nsequence of the conditional image generation is like “<s>[IMG]embedding of origin\nimage[\/IMG]instruct conditional generation prompt[IMG]embedding of\nedited image[\/IMG]<\/s>”. The “instruct conditional generation prompt”\ncontains the caption of the target image and with a 50% probability contain the task instruction like\n“Please convert the canny image to a natural image”.\nImage Understanding: We train image understanding task on the subset of LLaVA-OneVision (Li\net al., 2024a) and Cambrain (Tong et al., 2024). Data about math\/reasoning and cross-duplicated\ndata in the two datasets are removed. We train with the batch size 512 and train all data for 1\nepoch. The max learning rate is 1e-5 with the warm-up 500 steps. Cosine learning rate decay is\nadopted. We apply resizing as the image augmentation. Supervision is only applied to the output\ntext tokens.\nWe use the system message “A chat between a curious user and an\nartificial intelligence assistant.\nThe assistant gives helpful,\ndetailed, and polite answers to the user’s questions.”\nB\nEVALUATION DETAILS\nB.1\nIMAGE RECONSTRUCTION EVALUATION\nTo evaluate the reconstruction performance of different scales of features and our baselines, we use\nthe ImageNet validation set, comprising 50,000 images. Each image is resized to a rectangular\nshape before being input into each image encoder. We assess reconstruction precision by computing\nPSNRr and LPIPSr, which measure the difference between the reconstructed image and the original\nimage.\nGiven the inherent randomness in the decoders, we measure reconstruction diversity by reconstruct-\ning each original image twice using different random seeds. We then calculate PSNRd and LPIPSd\nto quantify the difference between these two reconstructed images. Higher diversity is beneficial for\ndownstream tasks such as text-to-image generation.\nFor PSNR and LPIPS evaluations, we use a resolution of 256 × 256 to align with the evaluation\nsettings in previous works. For LPIPS evaluation specifically, we employ AlexNet as the feature\nextractor.\nB.2\nTEXT-TO-IMAGE GENERATION EVALUATION\nWe evaluate text-to-image generation on the COCO 30K validation set (Lin et al., 2014). We use\nCLIP-I and CLIP-T scores to measure the consistency between the generated image and the ground\ntruth image and caption, respectively. CLIP-Base-32 serves as the feature extractor for these metrics.\nTo assess generation diversity, we calculate LPIPSd between two images generated using the same\ninput prompt but different random seeds. The LPIPSd measurement details are consistent with those\ndescribed in Sec. B.1.\n16\nTechnical Report\nTable 5: Ablation of different visual token input on image understanding. The experiments are\nconducted on LLaVA-v1.5 setting with CLIP-Large-224 visual encoder.\nVisual token type\nToken number\nMMB↑\nMME↑\nGQA↑\nVQAv2(test)↑\nf4\n1\n56.8\n1252.6\n0.0\n64.1\nf3\n4\n58.3\n1285.5\n0.0\n67.0\nf2\n16\n61.5\n1403.0\n46.6\n71.1\nf1\n64\n63.6\n1400.8\n58.4\n74.4\nf0\n256\n65.4\n1464.9\n58.8\n76.9\nf4-f0\n341\n65.1\n1445.5\n61.0\n76.9\nB.3\nIMAGE EDITING EVALUATION\nWe evaluate image editing performance on the Emu-Edit benchmark Sheynin et al. (2024). To\nassess editing quality, we adopt CLIP-I, CLIP-T, and DINO scores. CLIP-I and DINO Caron et al.\n(2021) scores measure the model’s ability to preserve elements from the source image, while CLIP-\nT reflects the consistency between the output image and the target caption. For the DINO score, we\nemploy DINO-Small-16 as the feature extractor.\nB.4\nIMAGE UNDERSTANDING EVALUATION\nFor image understanding tasks, we use the same evaluation setting as LLaVA-v1.5 (Liu et al.,\n2024a). During evaluation, we use the system message “A chat between a curious user\nand an artificial intelligence assistant.\nThe assistant gives\nhelpful, detailed, and polite answers to the user’s questions.”\nC\nABLATION OF DIFFERENT SCALE FEATURES AS INPUT ON IMAGE\nUNDERSTANDING TASK\nGiven that PUMA adopts a unified multi-granular image feature as both input and output for the\nMLLM backbone, we conducted an ablation study to investigate the influence of different scales of\nimage feature input on image understanding tasks. For a fair comparison, we adopted the standard\nLLaVA-1.5-7B pretraining and finetuning setting, only changing the image encoder to a 224-input\nCLIP-Large with different granularities of features.\nTab. 5 presents the results of this ablation study. The findings demonstrate that finer-grained features\ngenerally lead to better performance in image understanding tasks. Notably, utilizing all image\nfeatures from f4 to f0 (the PUMA setting) achieves comparable performance to using all 256 visual\ntokens of the finest scale (f0). These results validate that the unified visual input and output format\nof PUMA provides a robust foundation of visual features for image understanding tasks, effectively\nbalancing performance across different granularities.\nD\nSELECTION OF 5 SCALE FEATURES IN TEXT-TO-IMAGE GENERATION\nPUMA generates images at 5 granularity levels, allowing users to select the output that best meets\ntheir requirements. In our evaluation of diverse text-to-image generation, we produce 5 image out-\nputs for each input prompt, corresponding to the 5 feature scales. To assess performance, we select\nthe image with the highest CLIP-I and CLIP-T scores among the 5 outputs and compute the average\nmaximum value. Tab. 6 presents the CLIP-I and CLIP-T scores for each of the 5 feature scales.\nThe results demonstrate that different granularity levels excel in various aspects of image genera-\ntion. Notably, the ability to select the best output from multiple scales (PUMA 5-scale Max) yields\nsignificantly improved CLIP-I and CLIP-T scores compared to any single scale, highlighting the\nadvantage of PUMA’s multi-granular approach.\n17\nTechnical Report\nGeneration prompt: Hyper realistic happy steampunk chibi girl wearing a pink hoodie with a pet on white background.\nGeneration prompt: Beautiful portrait by J.c. Leyendecker, beautiful lighting, Victorian Female Hunter, Fantasy.\nPUMA (𝑓4) - 1 token\nPUMA (𝑓3) - 4 tokens\nPUMA (𝑓2) - 16 tokens\nPUMA (𝑓1) - 64 tokens\nPUMA (𝑓0) - 256 tokens\nFigure 10: Visualization of PUMA text-to-image outputs across five scale features given the gener-\nation prompt.\nTable 6: CLIP-I and CLIP-T scores on MSCOCO 30K validation set with different feature scales.\nModel\nToken num.\nCLIP-I↑\nCLIP-T↑\nPUMA (f4 scale)\n1\n0.699\n0.295\nPUMA (f3 scale)\n4\n0.703\n0.300\nPUMA (f2 scale)\n16\n0.703\n0.301\nPUMA (f1 scale)\n64\n0.693\n0.299\nPUMA (f0 scale)\n256\n0.621\n0.280\nPUMA (5-scale Max)\n-\n0.736\n0.317\nE\nQUALITATIVE RESULTS OF TEXT-TO-IMAGE GENERATION ON FIVE\nSCALE FEATURES\nIn the text-to-image generation task, PUMA produces five distinct images corresponding to the five\nfeature scales, all derived from a single input generation prompt. Fig. 10 presents samples of outputs\nacross these five scales for given generation prompts.\nF\nMORE QUALITATIVE RESULTS\nWe present more qualitative cases for image reconstruction, diverse text-to-image generation, edit-\ning, and conditional image generation, as shown in Figures 11 to 15.\n18\nTechnical Report\nOriginal image\n𝐷0 𝑓0\n𝐷1 𝑓1\n𝐷2 𝑓2\n𝐷3 𝑓3\n𝐷4 𝑓4\nFine-grained reconstruction\nSemantic-guided generation\nFigure 11: More visualizations on multi-granular visual decoding from fine-grained to coarse-\ngrained granularity.\n19\nTechnical Report\nOriginal image\nOriginal image\nReconstructed image\nReconstructed image\nFigure 12: More visualizations on fine-grained image reconstruction with f0 scale feature.\n20\nTechnical Report\nprompt1: Winter Queen princess baby girl, blond hair, blue eyes, pink lips, Walt Disney beautiful smiling, beautiful \ncharacter sweet and delicate, stickers, lovely frame, beautiful face, big eyes beautiful.\nPrompt2: 1972 porsche, ginza, bright light, hyper realistic, magazine quality, cinematic lighting, neon ads in \nbackground, vertical Japanese signs.\nprompt3: a cute totoro like tortoise character, bold colors, amiga game, isometric, pixel art, 8K.\nprompt4: Film still of rabbit sitting at the counter of an art-deco loungebar, drinking whisky from a tumbler glass, in \nthe style of \"Blade Runner\", velvety, soft lights, long shot, high quality photo.\nprompt5: a container designed compound built for a group home styled living space. 6000 SQ ft with 7 bedrooms and \n1 adult suite. give the view landscape style with a smilling pool in the front.\nprompt6: Open valley from mountains, aspen, hyper-realistic.\nprompt7: Cartoon, pixar style, the planet hamburger, line art drawing, magical scene, highly detailed, soft orange, soft \nblue, soft pink, soft red, sharp outlines, sharp brush strokes.\nprompt8: Beautiful colorful flower motif graphic, in the shape of an elegant flamingo in the style of Hayao Miyazaki, \nfront view.\nPUMA 𝑓4 scale\nPUMA 𝑓3 scale\nprompt1\nprompt2\nprompt3\nprompt4\nPUMA 𝑓3 scale\nPUMA 𝑓4 scale\nprompt5\nprompt7\nprompt6\nprompt8\nFigure 13: More visualizations on text-to-image generation utilizing f4 and f3 scales.\n21\nTechnical Report\nInsert a small bird \nperched on a tree branch.\nEliminate the number 69 \nfrom the image.\nAlter this photo to \nGhibli Studio style.\nReplace the cactus \nwith palm trees.\nFigure 14: More visualizations on image editing.\nInput image\nGenerated image\nInput image\nGenerated image\nFigure 15: More visualizations on conditional image generation.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/PUMA: Empowering Unified MLLM with Multi-granular Visual Generation.pdf"}
{"title":"MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling","authors":"Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, Zheng-Jun Zha","summary":"Recent advancements in multi-modal large language models have propelled the\ndevelopment of joint probabilistic models capable of both image understanding\nand generation. However, we have identified that recent methods inevitably\nsuffer from loss of image information during understanding task, due to either\nimage discretization or diffusion denoising steps. To address this issue, we\npropose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling\nframework. Unlike discretization line of method, MMAR takes in\ncontinuous-valued image tokens to avoid information loss. Differing from\ndiffusion-based approaches, we disentangle the diffusion process from\nauto-regressive backbone model by employing a light-weight diffusion head on\ntop each auto-regressed image patch embedding. In this way, when the model\ntransits from image generation to understanding through text generation, the\nbackbone model's hidden representation of the image is not limited to the last\ndenoising step. To successfully train our method, we also propose a\ntheoretically proven technique that addresses the numerical stability issue and\na training strategy that balances the generation and understanding task goals.\nThrough extensive evaluations on 18 image understanding benchmarks, MMAR\ndemonstrates much more superior performance than other joint multi-modal\nmodels, matching the method that employs pretrained CLIP vision encoder,\nmeanwhile being able to generate high quality images at the same time. We also\nshowed that our method is scalable with larger data and model size.","url":"http:\/\/arxiv.org\/abs\/2410.10798v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.10798v2","published":1728928638000,"comment":null,"pdf_text":"MMAR: Towards Lossless Multi-Modal\nAuto-Regressive Probabilistic Modeling\nJian Yang1,2,∗,§\nDacheng Yin2,∗\nYizhou Zhou2,†\nFengyun Rao2\nWei Zhai1\nYang Cao1,3\nZheng-Jun Zha1,3,‡\n1 University of Science and Technology of China\n2 WeChat, Tencent Inc.\n3 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\nAbstract\nRecent advancements in multi-modal large language models have propelled the\ndevelopment of joint probabilistic models capable of both image understanding\nand generation. However, we have identified that recent methods inevitably suffer\nfrom loss of image information during understanding task, due to either image dis-\ncretization or diffusion denoising steps. To address this issue, we propose a novel\nMulti-Modal Auto-Regressive (MMAR) probabilistic modeling framework. Un-\nlike discretization line of method, MMAR takes in continuous-valued image to-\nkens to avoid information loss. Differing from diffusion-based approaches, we\ndisentangle the diffusion process from auto-regressive backbone model by em-\nploying a light-weight diffusion head on top each auto-regressed image patch\nembedding. In this way, when the model transits from image generation to un-\nderstanding through text generation, the backbone model’s hidden representation\nof the image is not limited to the last denoising step. To successfully train our\nmethod, we also propose a theoretically proven technique that addresses the nu-\nmerical stability issue and a training strategy that balances the generation and un-\nderstanding task goals. Through extensive evaluations on 18 image understanding\nbenchmarks, MMAR demonstrates much more superior performance than other\njoint multi-modal models, matching the method that employs pretrained CLIP vi-\nsion encoder, meanwhile being able to generate high quality images at the same\ntime. We also showed that our method is scalable with larger data and model size.\nCode will be available at https:\/\/github.com\/ydcUstc\/MMAR.\n1\nIntroduction\nOver the past few years, extensive research in the field of multimodal intelligence has catalyzed\nthe accelerated advancement of foundational models for both image understanding and image gen-\neration. Within the realm of image understanding, multimodal large language models (MLLM),\nexemplified by LLaVA (Liu et al., 2023a), have exhibited human-like capabilities in open-domain\nimage comprehension. In the domain of image generation, techniques rooted in generative prob-\nabilistic models, such as Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020) and\nauto-regressive (AR) models (Chen et al., 2020), have also garnered significant success. Essentially,\nthese two lines of research correspond to modeling the conditional probability, i.e., P(T|I) and\nP(I|T), where T and I corresponds to text and image, respectively. It’s evident that both types of\nconditional probabilistic models are subsets of a joint probabilistic model, P(T, I). This brings us to\n*Co-first Author\n†Project Leader\n‡Corresponding Author\n§Work done during an internship at WeChat, Tencent Inc.\narXiv:2410.10798v2  [cs.CV]  15 Oct 2024\nan intriguing question: Could a joint probabilistic model serve as a natural unifying framework\nfor both image understanding and image generation tasks?\nGiven that the most advanced image understanding (Chen et al., 2024) and generation (Esser et al.,\n2024) models rely on the language priors p(T) of pre-trained large language models (LLMs), the\nmost straight-forward approach for joint image-text probabilistic modeling is to convert images into\ndiscrete tokens similar to text. This way, images are treated as a form of language and integrated into\nthe auto-regressive modeling of text, as seen in models like MARS (He et al., 2024), LlamaGen (Sun\net al., 2024a), and Chameleon (Team, 2024). This method leverages the powerful text modeling ca-\npabilities of various open-source pre-trained LLMs, along with their highly optimized training and\ninference frameworks. However, discretization on the continuous image space introduces an infor-\nmation bottleneck, inevitably leading to a loss of image details and reducing the model’s information\ncapacity. This limitation is quantitatively evident in the image understanding performance: exist-\ning methods based on image token discretization fall short when compared to the LLaVA model,\nwhich utilizes continuous CLIP representations (Xie et al., 2024).While there are theoretically fea-\nsible methods to mitigate this information bottleneck, such as significantly increasing the number of\ntokens or the size of the VQ codebook, these approaches would substantially increase the training\ndifficulty for both LLMs and VQ-VAE models.\nRecently, several attempts have been made to address the continuous nature of images by com-\nbining image diffusion transformers and text auto-regression within a unified transformer archi-\ntecture (Zhou et al., 2024; Zhao et al., 2024). Although this approach, exemplified by Transfu-\nsion (Zhou et al., 2024) and MonoFormer, demonstrated superior performance compared to discrete\ntoken auto-regressive methods for images, it is crucial to acknowledge that the inherent nature of\nimage diffusion prevents this approach from leveraging complete image modeling capability when\nit comes to image understanding tasks, as shown in fig. 1-(B). The root cause is that image diffusion\nwill encode different parts of image information into different noise levels (Ho et al., 2020). For ex-\nample, at low noise level, the model tends to solve an image enhancement task, while at high noise\nlevel, the model is only suitable for extracting rough image morphology and layout. Intuitively,\nimage-to-text tasks need to take clean images as input to maintain complete visual information.\nHowever, due to the nature of diffusion modeling, this will only invoke the hidden representations\nthat are related to the image enhancement task according to its low noise level, resulting in insignif-\nicant help for image-to-text tasks. Adding some noise to the input image would help invoke more\nsemantic representations, but at a cost of image information loss. To ensure a full utilization of the\nimage modeling capacity learned by image diffusion, it is necessary to input the images of all noise\nlevels, but with tremendous computational overhead. This dilemma hinders transfusion-like models\nperform well for visual understanding tasks.\nIn summary, how to take in and utilize the complete information of the continuous image modal-\nity is the major pain point of joint image-text probabilistic modeling. This is the challenge that our\nwork is trying to address. The method proposed in this paper, MMAR, belongs to the image-text\njoint AR methodology, thus naturally avoids the challenge of fusing diffusion and AR, as men-\ntioned previously. Different from other image-text joint AR methods, our method is built upon\ncontinuous image representation that can faithfully reconstruct the original image, rather than the\ndiscretized lossy image representation. To model the continuous probability distribution within the\nauto-regressive paradigm, we refer to MAR (Li et al., 2024b), introducing a simple mlp diffusion\nsampler to sample continuous image tokens given the transformer backbone output as its condi-\ntional information. This technique can also be regarded as disentangling the diffusion process from\nthe auto-regressive backbone in the transfusion-like methods (Zhou et al., 2024; Zhao et al., 2024),\nwhich in turn focuses on learning complete visual representation that can guide the entire diffusion\nsampling process of image tokens. Different from MAR (Li et al., 2024b), we leverage LLMs to\nlearn much more diverse distribution of the large-scale image-text data. In practice, we find that\nlow-precision training strategies for contemporary LLMs introduce non-negligible numerical error\nterm in the diffusion sampler, which substantially impedes the convergence. We carefully analyze\nthe numerical error term and find that the only way to minimize it is to properly parameterize the\ndiffusion model. Therefore, we theoretically derive an optimal diffusion parameterization to solve\nthis issue. Additionally, we find that visual understanding and generation prefer different experi-\nmental settings. To balance these two tasks in a unified model, we propose a two-stage training\nmethod, which firstly improves visual understanding ability with large, mid quality data, then boost\n2\nFigure 1: Comparison of different multi-modal probabilistic modeling paradigms. (A): Most AR-\nbased methods adopt discrete image tokens produced by VQ-VAEs. The VQ operation bottlenecks\nthe representation’s information capacity, resulting in information loss. (B): Concurrent transfusion-\nlike methods combining image DiT with text AR. X(i) denotes the full image at noise level i.\nFor DiT part, the image information is modeled within the learned representation across various\nnoise level, represented by h(0), ..., h(t), where the superscript indicates noise level. However, only\nh(0) is utilized in image to text tasks. (C): Our MMAR paradigm is VQ-free, thus reducing the\ninformation loss related to VQ operation. It auto-regressively models the continuous image tokens\nx1, ..., xn. The auto-regressive nature guarantees that all the image hidden representations, denoted\nby hi, i ∈[1, n], are simultaneously optimized for both P(I) and P(T|I), ensuring complete usage\nof image modeling capability for visual understanding tasks.\nvisual generation ability with high quality data. With these innovations, the lossless multi-modal\nauto-regressive probabilistic modeling is finally achieved theoretically and practically.\nOur contributions are the following three folds:\n• We identified why previous joint probabilistic models for both image generation and un-\nderstanding suffer image information loss with a series of experiments.\n• We proposed a novel modeling framework that solves the issue by disentangling diffusion\nprocess from auto-regressive backbone, and achieved significant performance improvement\nfor image understanding task.\n• We proposed two training techniques crucial for training the model: one solves numer-\nical error issue under low-precision training setting with theoretical guarantee, the other\nbalances image generation and understanding tasks goals.\n2\nRelated Works\n2.1\nMulti-modal Large Language Models\nSince LLMs demonstrated open-domain conversational capabilities, more and more research has\nbe focused on how to introduce visual information into large language models to achieve open-\ndomain visual understanding. Pioneering works such as BLIP-2 (Li et al., 2023b), MiniGPT4 (Zhu\net al., 2024) and LLaVA (Liu et al., 2023a) use trainable connector modules such as qformer or\nmlp to align image representations to the input space of LLM, making open-domain visual question\nanswering possible. Recently, thanks to innovations in network architecture (Dong et al., 2024b;\nTong et al., 2024), training strategy (Chen et al., 2024) and the support for dynamic resolution\ninput (Hu et al., 2024; Li et al., 2024a), the visual understanding performance of large multi-modal\nmodels have been greatly improved. These works focus on the alignment of image representations\nto text representations, only achieving p(T|I) without incorporating the image’s own distribution\np(I) into the modeling capabilities of the model. Different from these works, our work adds the\nmodeling of p(I) on the basis of these MLLMs to achieve joint image-text probabilistic modeling.\n3\n2.2\nAuto-Regressive Image Generative Models\nText-to-image generative models aim at modeling the conditional probability distribution p(I|c), en-\nabling probabilistic sampling of images conditioned on textual or categorical inputs. Auto-regressive\nmethods represent a dominant paradigm in this domain, typically requiring discrete representations\nfor both input and output. For images, this necessitates encoding them into discrete codes using a\nVQVAE (Esser et al., 2021; Kondratyuk et al., 2024). While recent works demonstrate that auto-\nregressive methods based on discrete image tokens can generate high-quality images (Sun et al.,\n2024a), the discretization of image representation acts as an information bottleneck, limiting the\nmodeling accuracy of the image distribution. Recent efforts have shown that auto-regressive prob-\nabilistic modeling can be achieved without relying on discrete representations (Tschannen et al.,\n2023; Li et al., 2024b). For instance, MAR (Li et al., 2024b) replaces traditional logits with diffu-\nsion heads, enabling probabilistic sampling of continuous representations within an auto-regressive\nframework. This paper introduces continuous representation auto-regressive probability modeling\nto MLLMs, mitigating information loss caused by quantization and achieving theoretically lossless\njoint image-text probability modeling. In addition, we addressed the difficulty when training with\nlarge model and large data which is not presented in the previous works.\n2.3\nUnified Visual Understanding and Generation Models\nRecently, a series of studies have focused on leveraging a single model to simultaneously address\ntasks of visual generation and understanding. Early works in this area adopted a modular approach,\nbridging pre-trained models for visual understanding and visual generation through intermediate\nrepresentations to achieve combined image understanding and generation. Notable examples include\nEMU (Sun et al., 2024c,b) and SEED-X (Ge et al., 2024). These works, however, are not considered\nprobabilistic modeling because they aim at modeling the mean value of representations like CLIP or\nother intermediate representations rather than modeling the true image distribution. This limitation\nleads to the inadequate image space exploration, and thus hinders the attainment of high-quality\ngenerative and understanding performance.\nAnother line of research adheres to the paradigm of probabilistic modeling (Team, 2024; He et al.,\n2024; Xie et al., 2024; Wu et al., 2024; Zhou et al., 2024; Zhao et al., 2024). These approaches\ncan be categorized into three types based on whether the image representations are discrete and\nthe modeling method of the image part : (i) Discrete auto-regressive Methods: Examples include\nChameleon (Team, 2024), MARS (He et al., 2024), and VILA-U (Wu et al., 2024). These meth-\nods discretize image representations and then model images and text tokens using a unified auto-\nregressive transformer. (ii) Discrete Diffusion Methods: An example is Show-o (Xie et al., 2024).\nThese methods discretize image tokens and model them with text tokens using a unified transformer\nthrough a discrete diffusion approach. (iii) Continuous Diffusion Methods: Examples include Trans-\nfusion (Xie et al., 2024) and MonoFormer (Zhao et al., 2024). These methods do not discretize\nimage representations but directly employ continuous diffusion transformers to model image tokens\nalong with text tokens using a unified transformer. Our approach differs from the aforementioned\nthree types. It belongs to the continuous auto-regressive method category, which does not require\ndiscretizing image representations. Instead, it predicts the continuous distribution of image tokens\nusing an auto-regressive approach and models them alongside text tokens within a unified trans-\nformer, as shown in fig.1.\n3\nMethod\n3.1\nAuto-Regressive Modeling with Continuous and Discrete Representations\nAuto-regressive modeling is a commonly used probabilistic modeling method for sequence data. It\ncan be formulated by “predicting the next token” as follows:\nlog pθ(x) =\nn\nX\ni=1\nlog pθ(xi|x<i),\n(1)\nwhere θ and x represent model parameters and the sequence data, respectively. By maximizing the\nlog likelihood of the data, Ex∼D log pθ(x), the model can be optimized to sample from the data\ndistribution D, achieving probabilistic modeling.\n4\nIn the realm of natural language processing (NLP), the sequence x is solely made of discrete text\ntokens. As a result, most modern large language models (LLMs) parameterize pθ(xi|x<i) into\na categorical distribution, which can be explicitly represented by the softmax activation on a set\nof logits predicted by a decoder-only transformer (Radford et al., 2019; Brown et al., 2020) fθ(·)\nfollowed by a linear LM head Hθ(·):\npθ(xi|x<i) = softmax(Hθ(fθ(x<i))).\n(2)\nIn addition to text, our work also aims at modeling the probability of images, which are repre-\nsented by continuous rather than discrete image tokens. Therefore, a protocol for parameterizing\npθ(xi|x<i) of the continuous image tokens is required. Inspired by MAR (Li et al., 2024b), we train\na diffusion model to achieve this. The diffusion model takes vector zi = fθ(x<i) as the conditional\ninput, and samples xi by gradually denoising from a randomly sampled gaussian noise. To optimize\nthe diffusion model for continuous image token sampling, a diffusion loss can be applied, which acts\nas the upper-bound of the negative log likelihood. A typical diffusion loss can be written as follows,\nwhich is seen in MAR (Li et al., 2024b):\nL(xi) = Exi,ϵ,t[wt · ||ϵ −ϵθ(√¯αtxi +\n√\n1 −¯αtϵ, t, zi)||2] ≥−log pθ(xi|x<i) + C,\n(3)\nwhere wt is the loss weight that balances the loss for different timesteps, and ¯αt indicates the noise\nschedule of the forward diffusion process. In this way, minimizing the diffusion loss is equivalent to\nmaximizing the log likelihood of image data.\nThe overall loss for joint image-text probabilistic modeling can be written as follows:\nL =\nX\ni∈Iimg\nL(xi) −\nX\ni∈Itxt\nlog pθ(xi|x<i),\n(4)\nwhere Iimg and Itxt indicate the indices of image tokens and text tokens, respectively.\n3.2\nOptimal Diffusion Model Parameterization for Low-Precision Training\nIn the era of large language models, training with low precision data type, such as bfloat16, has\nbecome increasingly popular. However, the training and inference process of a diffusion model is\nrelatively sensitive to numerical precision. Moreover, in an auto-regressive framework, the image\ntokens are sampled sequentially, requiring even more precise sampling for each image token to\nreduce the overall error accumulation. Therefore, handling the numerical error in the diffusion\nprocess modeling should be emphasized when integrating diffusion loss into LLMs.\nFrom the example below, we can clearly illustrate the effect of the numerical error in diffusion\nmodels. In eq.3, the diffusion model is parameterized as ϵθ(√¯αtxi + √1 −¯αtϵ, t, zi), known as\n“ϵ-prediction”, predicting the noise ϵ that is added to the data. Floating-point representation causes\nnumerical error’s amplitude proportional to a number’s magnitude. This can be modeled by scal-\ning the prediction by a factor of (1 + δ), where δ is the relative error. In this way, we can write\nDDIM (Song et al., 2021) sampling with numerical error as follows:\n˜x(t−1) =√¯αt−1\n\u0012x(t) −√1 −¯αtϵθ(x(t), t, zi)(1 + δ)\n√¯αt\n\u0013\n+\np\n1 −¯αt−1ϵθ(x(t), t, zi)(1 + δ). (5)\nFurther separating the numerical error term from the ideal DDIM sampling process, we get:\n˜x(t−1) = x(t−1) + (\np\n1 −¯αt−1 −\n√¯αt−1\n√¯αt\n√\n1 −¯αt)ϵθ(x(t), t, zi)δ,\n(6)\nwhere the first term x(t−1) is the ideal DDIM sampling term, and the second term is the numerical\nerror term. Assuming ϵθ(x(t), t, zi) follows a standard normal distribution, the standard deviation of\nnumerical error term can be calculated as |√1 −¯αt−1−\n√¯αt−1\n√¯αt\n√1 −¯αt|δ. When the signal-to-noise\nratio (SNR) is high, i.e. ¯αt, ¯αt−1 →1, the numerical error has almost zero amplitude. However,\nwhen SNR is extremely low, i.e. ¯αt = 0, and ¯αt−1 > 0, this term will explode to infinity, causing\nextreme numerical error.\nOur goal is to minimize the numerical error term. To achieve this, we analyze all the factors that can\ndetermine the numerical error in Appendix A.2, and find that the only solution is to parameterize\n5\nFigure 2: The overview of MMAR with two stage image expert training strategy.\nthe diffusion model properly. By solving the numerical error minimization problem, we conclude\nthat the v-prediction parameterization (Salimans & Ho, 2022) is the desired optimal parameteriza-\ntion method. Note that v-prediction is initially proposed for the efficient distillation of diffusion\nmodels, rather than reducing the numerical error of diffusion models. To the best of our knowledge,\nour work is the first to derive v-prediction parameterization from the first principle of minimizing\nthe numerical error in diffusion models. For more details, see Appendix A.2. Under v-prediction\nparameterization, the model predicts a linear combination of data xi and noise ϵ:\nv(t)\ni\n= √¯αtϵ −\n√\n1 −¯αtxi\n(7)\nWe therefore re-write eq.3 into the v-prediction form, and set the loss weight wt to 1 for simplicity:\nL(xi) = Exi,ϵ,t[||v(t)\ni\n−vθ(√¯αtxi +\n√\n1 −¯αtϵ, t, zi)||2].\n(8)\n3.3\nModel Architecture and Training Strategy\n3.3.1\nPipeline\nTo jointly model the probabilities of images and text at the token level, we employ an image tok-\nenizer to extract image tokens and randomly mask some of them for image generation training, as\ndepicted in fig. 2. Given the substantial distribution gap between images and text, directly scaling\nimage tokens to match the LLM’s dimensions would pose a challenge to joint image-text probabil-\nity modeling. To address this, we introduce a Latent Encoding process utilizing an EmbeddingViT\nmodule for the unmasked image tokens, which simplifies the modeling task. To facilitate the auto-\nregressive training of image tokens, we concatenate the known image tokens with the masked to-\nkens and append the position embedding corresponding to their original positions. These tokens,\nafter being processed by the VisProjector, are concatenated with the text tokens to generate the fi-\nnal image-text sequence. Then, an LLM is used to process the image-text sequence, whose output\nz integrates information from both the preceding text and the unmasked image tokens. For image\noutput, z acts as a conditional input for the Diffusion MLP, which predicts the v values of the noisy\nmasked image tokens. The diffusion loss, as depicted in eq.8, is applied, enabling us to model the\nprobability of the continuous image tokens. For text output, a linear lm-head processes z to predict\nthe next token’s logits, and the corresponding cross entropy loss is applied, allowing us to model the\nprobability of the text tokens.\nDuring image generation, to ensure consistency with the training scenario, we initialize all image\ntokens as mask tokens and randomly generate a set of image position sequences. Following a left-\nto-right order, we extract N condition tokens z at each step and input them into the Diffusion MLP\nto generate the corresponding image tokens. Next, we assign these image tokens back to their\nrespective positions in the original image token sequence, iterating until the entire image token set\nis obtained. Ultimately, we decode the image tokens into images using the image tokenizer, yielding\nthe generated images.\n6\n3.3.2\nModel Architecture\nTokenizer\nMMAR employs the publicly available KL-16 image tokenizer from LDM (Rombach\net al., 2022). This tokenizer processes an 256 × 256 image into 256 image tokens, each with 16\nchannels, and it is kept frozen during training.\nEmbeddingViT\nTo achieve a more profound encoding of image tokens, we introduce the Em-\nbeddingViT module, a ViT(Dosovitskiy et al., 2021) encoder with 16 layers and 1024 hidden state\nchannels, processing image tokens into visual embeddings with stronger context awareness. More-\nover, as the input consists of randomly shuffled image tokens, we integrate a learnable position\nembedding for each image token in EmbeddingViT, corresponding to its original position.\nLLM\nWe initialize our LLM using parameters from the open-source QWen2 series models (Yang\net al., 2024). To preserve text modeling capabilities, we keep the LLM parameters fixed during\ntraining and add PLoRA (Dong et al., 2024b) as the image expert, where only the image tokens in the\ninput pass through the introduced LoRA (Hu et al., 2022) adapters. The PLoRA is applied for each\nlinear layer within the original LLM. Considering the time cost, our ablation study employs QWen2-\n0.5B-Instruct, with r = 512 for PLoRA. Furthermore, we use QWen2-7B-Instruct to explore our\nmethod’s scale-up capability with r = 1280 for PLoRA. The corresponding MMAR models are\ndenoted by MMAR-0.5B and MMAR-7B, respectively.\nIn the LLM, we adopt a bidirectional attention mechanism to enhance interaction among image\ntokens, rendering all image tokens mutually visible, as depicted in fig. 2. Meanwhile, the text portion\nretains causal attention. Additionally, to prevent interference with the auto-regressive training of\nimage tokens in random order, we assign the same ROPE position id to all image tokens within all\nLLM layers. This strategy not only ensures the random order regression of image tokens but also\nmitigates the issue of text concentrating on closer image tokens in long text scenarios.\nDiffusion MLP\nInspired by MAR, we employ a simple MLP to predict v(t). This MLP consists\nof a series of residual blocks, each comprising AdaLN (Peebles & Xie, 2023), a linear layer, SiLU,\nand another linear layer. The condition vector z is added to the diffusion time embedding and in-\ncorporated through AdaLN. MMAR-0.5B utilizes a Diffusion MLP with 8 residual blocks and 1024\nchannels, while MMAR-7B employs a larger variant with 12 residual blocks and 2048 channels.\n3.3.3\nTraining Strategy\nMulti-Task Training\nTo accomplish joint image-text modeling, we simultaneously conducted\ntext-to-image, image-to-text, and unconditional image generation tasks during training.\nIn the\nimage-to-text task, no mask tokens are assigned to the image part, allowing us to model P(T|I)\nusing the complete image tokens. Furthermore, the allocation ratio of text-to-image and uncon-\nditional image generation tasks is set to 9:1, facilitating efficient use of the cfg technique at the\ninference stage. To maintain a balance between tasks, we intuitively set the sample allocation ratio\nof image generation tasks and image understanding tasks to 1:1.\nTwo Stage Training\nTo achieve a balance between image generation and understanding capabili-\nties, our training process is carried out in two stages. The first stage utilizes large-scale, mid-quality\ndata (as illustrated in fig. 2) to enhance the diversity of the model’s data distribution (Hu et al.,\n2024). By initially modeling this diverse data, we strengthen the model’s understanding capabili-\nties. In the second stage, we employ a smaller volume of high-quality data to further improve the\nimage generation capacity and refine the model’s comprehension of images.\nWe observe that when the training dataset is excessively large, preventing the model from iterating\nover each image hundreds of times, maintaining an image mask ratio within the range of [0.7, 1],\nas done in MAR, hinders the model’s ability to generate coherent image tokens. Consequently, in\nthe second stage, we adjust the mask ratio to (0, 1]. The example is showed in Appendix A.5. The\nprobability density curve of the two mask ratios are shown on the right side of fig. 2. Additionally,\ndue to the small size of the diffusion mlp and the limited amount of high-quality data, we increased\nthe number of timestep samples for the given z to enhance learning efficiency. To further mitigate\nperformance fluctuations in the model, we employed an exponential moving average (EMA) method\nwith a momentum of 0.9999 in both stages.\n7\n3.3.4\nInference Strategy\nClassifier-Free Guidance\nDuring the inference stage, the model performs both text-based and\nnon-text-based image generation tasks. The provided conditions are represented as zc and zu, and\nthe predicted v is given by: v = vθ(x(t)|t, zu) + ω ∗(vθ(x(t)|t, zc) −vθ(x(t)|t, zu)) (which has\nthe same effect as ϵ = ϵu + ω ∗(ϵc −ϵu) , with the mathematical derivation process detailed\nin the Appendix A.4), where ω denotes the guidance scale. Our method has been experimentally\nvalidated to support a large guidance scale, as compared to the noise prediction training approach.\nWe hypothesize that this is due to the strong conditioning provided by z, which, coupled with the\nauto-regressive (AR) process, necessitates reducing accumulated error. Thus, each token must be\ngenerated as accurately as possible, a requirement that can be fulfilled with a large guidance scale.\nMixed-Precision Inference\nTo enhance numerical stability during image generation, particularly\nwhen using small step sizes in the DDPM sampling process, we perform the model’s forward pass\nin bfloat16 (matching the training precision) but cast the output to float32 before DDPM\nsampling. This mitigates potential rounding error without a significant computational overhead,\ntherefore improving the sampling accuracy efficiently.\n4\nExperiment\n4.1\nDataset\nWe utilized the Capfusion-120M dataset for the image expert pretraining stage. This dataset is\npublicly accessible and comprises an extensive collection of web-based image-text pairs, designed\nto optimize noisy captions (Yu et al., 2023). In an effort to further improve the quality of the content\ngenerated, we executed a random sampling of 20M data points from the Capfusion dataset during\nour image expert fine-tuning stage. This was supplemented with a high-quality mixed dataset that\nincluded ImageNet-1K-1.2M (Deng et al., 2009), CC12M (Changpinyo et al., 2021), and LAION-\naesthetics-12M1. We employ the open-source InternVL2-8B for recaptioning the CC12M and laion-\naesthetics-12m datasets in English. Following LLaVA-v1.5 (Liu et al., 2023a), we use LLaVA-v1.5-\nmix-665K data for instruction tuning before each performance test for visual understanding.\n4.2\nImplementation Details\nBy default, we utilized the AdamW optimizer with betas (0.9, 0.95). The weight decay was consis-\ntently maintained in proportion to the learning rate. During the first training stage, a learning rate of\n5e-5 was employed for a total of 4 epochs, with an initial warm-up phase of 0.5 epochs, followed\nby maintaining the learning rate at 5e-5. In the second stage, the 0.5B model maintained a 5e-5\nlearning rate and a 0.5 epoch warm-up, with training lasting for 3 epochs. For the larger 7B model,\nwe initially applied a 2e-6 learning rate for 1.5 epochs, before transitioning to a 1e-6 learning rate\nfor an additional 1.5 epochs. Furthermore, during the first stage, the total batch size for the smaller\n0.5B model was 2496, while it was 1152 for the larger 7B model. In the second stage, the total\nbatch size for the smaller 0.5B model was 768, and for the larger 7B model, it was 480. Notably,\nin both stages, only the PLoRA portion of the LLM parameters is trainable, and a consistent image\nresolution of 256x256 was used throughout. Our models utilize bfloat16 during training and\ninference. For DDPM sampling, we convert the model output to float32 for higher precision.\n4.3\nComparison with Other Systems\nVisual Understanding. As depicted in table 1, we thoroughly gauge MMAR’s performance in vi-\nsual understanding, employing VLMEvalKit (Duan et al., 2024) to perform extensive evaluations on\nprevalent visual understanding benchmarks, encompassing a total of 18 such assessments (average\nscore denoted by “AVE@18Und.” in table 1, for details see Appendix A.7.) including MMB (Liu\net al., 2023b), MME (Fu et al., 2023), POPE (Li et al., 2023c), SEED (Li et al., 2023a), MM-Vet (Yu\net al., 2024), among others. Our method outperforms other joint image-text probabilistic models by a\nlarge margin, including Chameleon-7B (Team, 2024), Show-o (Xie et al., 2024), VILA-U (Wu et al.,\n2024) and our re-implemented version of Transfusion (denoted by “Transfusion*”), approaching the\n1https:\/\/huggingface.co\/datasets\/dclure\/laion-aesthetics-12m-umap\n8\nTable 1: Comparison on visual understanding benchmarks. MMAR surpasses other joint image-text\nprobabilistic models by a large margin, even with a small resolution of 256x256, approaching the\nperformance of traditional MLLMs like LLaVA, which employ pretrained CLIP vision encoder.\nMethod\nLLM\nV-Token\nRes.\nMMB\nMMEP\nPOPE\nSEED\nMM-Vet\nAVE@18Und.\nLLaVA-1.5\nVicuna-1.5-7B\nCLIP\n336\n64.3\n1510.7\n85.9\n58.6\n31.1\n47.08\nEMU-2\nLLaMA-13B\nCLIP\n448\n–\n–\n–\n62.8\n48.5\n–\nSEED-X\nLLaMA-13B\nCLIP\ndynamic\n75.4\n1435.7\n84.2\n–\n–\n–\nDreamLLM\nVicuna-7B\nCLIP\n224\n58.2\n–\n–\n–\n36.6\n–\nChameleon-7B\n7B from scratch\nvq-vae\n512\n13.32\n125.8\n30.86\n34.61\n7.34\n18.34\nTransfusion*\nQwen-2-0.5B\nvae\n256\n29.47\n594.3\n66.90\n42.40\n13.90\n28.26\nShow-o\nPhi-1.5B\nCLIP\n336\n42.44\n1182.7\n84.50\n51.61\n20.87\n33.06\nVILA-U\nLLaMA-2-7B\nvq-vae\n256\n–\n1336.2\n83.9\n56.3\n27.7\n–\nMMAR-0.5B\nQwen-2-0.5B\nvae\n256\n48.45\n882.1\n70.74\n55.70\n18.49\n34.56\nMMAR-7B\nQwen-2-7B\nvae\n256\n66.32\n1393.9\n83.02\n64.52\n27.80\n46.52\nperformance of traditional MLLMs like LLaVA, which employ pretrained CLIP vision encoder.\nEven without using any pre-trained CLIP or diffusion models and with small resolution of 256 ×\n256, MMAR-7B presents comparable or even better performance when compared to methods using\npre-trained clip and diffusion models, including EMU-2 (Sun et al., 2024c), SEED-X (Ge et al.,\n2024), and DreamLLM (Dong et al., 2024a).\nVisual Generation. We showcase the zero-shot FID of MMAR, evaluated on the MSCOCO 30k\ndataset, in Table 2. Our model’s performance is discernibly on par with current robust generative\nmodels, a noteworthy achievement for MMAR given its minimal training epochs. Repeated ex-\nposure to identical data can substantially enhance a model’s generative quality, as exemplified by\nMAR, which trains for 400 epochs on the 1.2M ImageNet dataset, and Show-o, which undergoes\napproximately 49 epochs of training on the 35M dataset.\nTable 2: Comparison on MSCOCO Dataset\nType\nMethod\nParams Images FID-30K↓\nGen. Only\nDALL-E\n12B\n250M\n27.50\nLDM\n1.4B\n400M\n12.64\nDALL-E2\n6.5B\n650M\n10.39\nImagegen\n3B\n5000M+\n6.61\nUnd. and Gen.\nw\/ pre-trained Diff.\nCoDI\n-\n400M\n11.26\nSEED-X\n17B\n-\n12.68\nDreamLLM\n7B\n-\n8.76\nJoint Prob. Models\nShow-o\n1.3B\n35M\n9.24\nChanmeleon\n7B\n-\n29.6\nTransfusion 2\n7B\n-\n16.8\nMMAR-0.5B\n0.5B\n145.2M\n36.6\nMMAR-7B\n7B\n145.2M\n17.1\nHowever, to bolster the model’s comprehen-\nsion capabilities, exposure to diverse data is\ncrucial rather than simply reiterating the same\ndata.\nConsequently, we restrict our training\nto just 3 epochs for the second stage high-\nquality dataset. Despite these constraints, our\nmodel maintains competitive performance in\ngeneration, further substantiating the efficacy\nof our image-text joint modeling approach. The\nexample images generated by our model are\nshown in the Appendix A.6.\nScaling up with Model Size.\nFrom Table 1\nand 2, we see that MMAR can scale up from\n0.5B to at least 7B model size, with significant improvement of visual understanding and generation\ncapability.\n4.4\nAblation Study\nWe begin by evaluating the impact of our chosen diffusion parameterization method. Table 3 demon-\nstrates that switching to the more common ϵ-prediction leads to a significant decrease in both visual\nunderstanding and generation quality, confirming the effectiveness of our optimal diffusion param-\neterization. To evaluate the efficacy of various image-text joint modeling techniques, we devise two\ndistinct versions based on the MMAR framework: one employing LDM-VQ-16 tokenizer (Rombach\net al., 2022) for discrete token modeling, and the other utilizing Transformer for diffusion modeling\n(refer to Transfusion). Comprehensive implementation details are provided in the Appendix A.1.\nOur test results are presented in Table 3. The Transformer-based diffusion modeling version con-\nsiderably underperforms the other two approaches in both understanding and generation aspects.\nThis is attributed to the substantial loss of image information when jointly modeling image-text and\noperating with limited training epochs. In contrast, our method consistently delivers superior results.\n2Both the Transfusion and Chanmeleon results are referenced from Table 3 in the paper ‘Transfusion: Pre-\ndict the Next Token and Diffuse Images with One Multi-Modal Model.’\n9\nTable 3: Ablation study on MMAR\nExp. Setting\nMMB\nMMEP\nPOPE\nSEED\nMM-Vet\nAVE@18Und.\nFID-30K↓\nMMAR-0.5B(Full method)\n48.45\n882.1\n70.74\n55.70\n18.49\n34.56\n36.6\nw\/ ϵ-pred.\n45.53\n880.7\n71.14\n53.72\n17.98\n32.21\n61.53\nw\/ VQ\n37.54\n618.2\n66.98\n44.93\n14.45\n29.70\n66.26\ntransfusion-like\n29.47\n594.3\n66.90\n42.40\n13.90\n28.26\n95.38\n4.5\nAnalysis\nImpact of v-prediction\nTo delve deeper into the disparities between v-prediction and ϵ-prediction\nfor diffusion MLP, we independently collect the statistics of the MSE loss of v(t) values at various\ntime steps t throughout the training process for both methods, as illustrated in fig. 3 (A). Further-\nmore, to more effectively discern the loss discrepancies between the two techniques, we subtract the\nv-prediction curve from the ϵ-prediction curve, yielding the yellow curve. For reference, we plot\nthe theoretically predicted numerical error of the ϵ-prediction model (see Appendix A.3) as the red\ncurve. The graph reveals that the loss of ϵ-prediction model is consistently higher than v-prediction\nmodel. Especially, when t > 900, curve “ϵ −v” exhibits a significant spike towards infinity. This\naligns with the behavior of the theoretical numerical error, confirming the non-negligible numerical\nerror effect in low-precision training. The gap between yellow and red curve indicates that apart\nfrom its direct effect, numerical error also introduces optimization difficulty, hindering the loss con-\nvergence.\nImpact of CFG Scaling\nWe select models from the second and fourth epochs of the first stage as\nstarting points for the second stage, train them for 3 epochs, and then test the MSCOCO FID-30K\nunder varying CFG intensities. As shown in fig. 3 (B), our method achieves better FID scores as\nthe CFG scale increases from 1 to 10. It is worth noting that most probabilistic generative models\ntypically have a CFG scale between 1.5 and 5. Additionally, it is observed that a longer training\nduration in the first stage consistently results in better generation outcomes at all CFG scales.\nScaling up with More Training Data\nAs illustrated in fig. 3 (C), we evaluate the average per-\nformance of MMAR-0.5B on 18 visual understanding benchmarks, following SFT training applied\nto the checkpoints generated during its training process. The blue background in the figure denotes\nthe Image Expert Pretraining stage, while the green background signifies the Image Expert Fine-\ntuning stage. The curve reveals that, with an increasing number of training steps, i.e. more training\ndata, the visual understanding performance of MMAR-0.5B consistently improves without reaching\nsaturation. This finding highlights the exceptional scale-up capability of our MMAR model.\n(A)\n(B)\n(C)\nFigure 3: The impact of v-prediction, CFG scaling, and training steps.\n4.6\nLimitation\nOur method still requires further optimization in terms of image generation speed. Although the\ndiffusion mlp is not involved in image understanding tasks, when applied to image generation tasks,\nwe are compelled to use a 256-step token generation followed by a 100-step diffusion denoising\nprocess to ensure the quality of the generated images. This results in a generation time of nearly\nthree minutes for a single 256x256 image using MMAR-7B. While it is possible to generate multiple\nimages simultaneously by increasing the batch size, this does not fundamentally resolve the issue of\nprolonged generation time. We plan to address this in future work.\n10\n5\nConclusion\nThis paper proposes MMAR, a novel multimodal auto-regressive probabilistic modeling framework\nbased on continuous image representations. It employs a standalone diffusion mlp at the image to-\nken level on top of pre-trained LLMs to facilitate theoretical lossless joint image-text probabilistic\nmodeling. In practice, the low precision training of LLMs poses an non-negligible numerical error\nterm to diffusion loss, causing optimization difficulty. This was addressed by deriving an optimal\ndiffusion model parameterization. To balance the understanding and generation ability, a two-stage\ntraining strategy is introduced. During inference time, MMAR can tolerant extremely large CFG\nscale to generate high quality images. MMAR significantly demonstrates scaling-up laws with more\ndata and larger model size. Extensive evaluations are conducted on 18 image understanding bench-\nmarks, revealing that MMAR is the first joint image-text modeling framework that approaches com-\nparable performance with traditional MLLMs that employ pretrained CLIP vision encoder, marking\na significant step toward lossless joint probabilistic modeling of images and text.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 3558–3568.\nComputer Vision Foundation \/ IEEE, 2021.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Proceedings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pp. 1691–1703. PMLR, 2020.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pp. 24185–24198, 2024.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, pp. 248–255. IEEE Computer Society, 2009.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.\nDreamllm: Synergistic multimodal comprehension and creation.\nIn ICLR. OpenReview.net,\n2024a.\nXiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei,\nSongyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang\nGao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao,\nDahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition\nand comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024b.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. In ICLR. OpenReview.net, 2021.\nHaodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong,\nYuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating\nlarge multi-modality models. arXiv preprint arXiv:2407.11691, 2024.\n11\nPatrick Esser, Robin Rombach, and Bj¨orn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, pp. 12873–12883. Computer Vision Foundation \/ IEEE, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion En-\nglish, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow\ntransformers for high-resolution image synthesis, 2024. URL https:\/\/arxiv.org\/abs\/\n2403.03206.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei\nLin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: A comprehensive eval-\nuation benchmark for multimodal large language models. CoRR, abs\/2306.13394, 2023. doi: 10.\n48550\/ARXIV.2306.13394. URL https:\/\/doi.org\/10.48550\/arXiv.2306.13394.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying\nShan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation.\narXiv preprint arXiv:2404.14396, 2024.\nWanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei\nZhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, and Hao Jiang. Mars: Mixture of\nauto-regressive models for fine-grained text-to-image synthesis, 2024. URL https:\/\/arxiv.\norg\/abs\/2407.07614.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth Inter-\nnational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022,\n2022.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models\nwith scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.\nDan Kondratyuk, Lijun Yu, Xiuye Gu, Jos´e Lezama, Jonathan Huang, Grant Schindler, Rachel Hor-\nnung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari,\nYair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hen-\ndon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig\nAdam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and\nLu Jiang. Videopoet: A large language model for zero-shot video generation. In ICML. OpenRe-\nview.net, 2024.\nBo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng\nLi, Ziwei Liu, and Chunyuan Li.\nLlava-next:\nWhat else influences visual instruc-\ntion tuning beyond data?, May 2024a.\nURL https:\/\/llava-vl.github.io\/blog\/\n2024-05-25-llava-next-ablations\/.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-\nmarking multimodal llms with generative comprehension.\narXiv preprint arXiv:2307.16125,\n2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-\nimage pre-training with frozen image encoders and large language models. In ICML, volume 202\nof Proceedings of Machine Learning Research, pp. 19730–19742. PMLR, 2023b.\nTianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He.\nAutoregressive image\ngeneration without vector quantization. arXiv preprint arXiv:2406.11838, 2024b.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating\nobject hallucination in large vision-language models. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,\n2023, pp. 292–305. Association for Computational Linguistics, 2023c.\n12\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,\nJiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around\nplayer? arXiv preprint arXiv:2307.06281, 2023b.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pp. 4172–\n4182. IEEE, 2023.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10674–10685. IEEE, 2022.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In\nICLR. OpenReview.net, 2022.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR.\nOpenReview.net, 2021.\nPeize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Au-\ntoregressive model beats diffusion: Llama for scalable image generation. CoRR, abs\/2406.06525,\n2024a.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context\nlearners. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 14398–14409, June 2024b.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality.\nIn ICLR. OpenReview.net, 2024c.\nChameleon Team.\nChameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024. doi: 10.48550\/arXiv.2405.09818. URL https:\/\/github.com\/\nfacebookresearch\/chameleon.\nShengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha\nAkula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann\nLeCun, and Saining Xie. Cambrian-1: A fully open, vision-centric exploration of multimodal\nllms. CoRR, abs\/2406.16860, 2024.\nMichael Tschannen, Cian Eastwood, and Fabian Mentzer. GIVT: generative infinite-vocabulary\ntransformers. CoRR, abs\/2312.02116, 2023.\nYecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng\nZhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. Vila-u: a unified foundation model\nintegrating visual understanding and generation, 2024. URL https:\/\/arxiv.org\/abs\/\n2409.04429.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,\nYuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer\nto unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai,\nJinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng\nXue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai\nBai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan\nZhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang\nZhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671, 2024.\n13\nQiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and\nJingjing Liu. Capsfusion: Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550,\n2023.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\nand Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-\n27, 2024, 2024.\nChuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao,\nand Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv\npreprint arXiv:2409.16280, 2024.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob\nKahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and\ndiffuse images with one multi-modal model, 2024. URL https:\/\/arxiv.org\/abs\/2408.\n11039.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. In ICLR. OpenReview.net,\n2024.\n14\nA\nAppendix\nA.1\nAdditional Implementation Details\nVQ\nBased on the MMAR-0.5B framework, we replace the Image Tokenizer from KL-16 to VQ-\n16. The image codes extracted using VQ-16 are then passed through a projector to increase the\nchannel size to match the LLM’s hidden size. Subsequently, we add a decoding Linear layer, which\ntakes the hidden states of the LLM’s output image portion as input and maps them to the image\ncodebook. The Cross Entropy loss is then calculated between these mapped values and the actual\nVQ codes.\nTransfusion\nFollowing the theoretical ideas presented in the Transfusion paper, we adopt a simple\nlinear mapping. After extracting image tokens using KL-16, if the task is image first, we add noise\nwithin a 500-time step to the image tokens. Otherwise, we add noise within a 1000-step. After the\nlinear mapping, we add the image token a learnable time embedding corresponding to the time step\nas input to the LLM. We also maintain the bidirectional attention mechanism.\nAfter passing through the LLM, we first map the output back to the original token channel count\nusing a linear layer and compute the MSE loss for the predicted noise. During generation, we treat\nLLM as a denoised model, with the condition being the concatenation of the text and the image\ntokens to be generated. We adopt the learning approach of Transfusion but conduct experiments\nbased on our training tasks and stage divisions.\nProjector\nIn order to accomplish channel alignment, we introduced two Projectors designed to\nscale the channels. Both Projectors consist of a simple linear layer and multiple blocks composed of\nactivation layers and linear layers. The PostProjector comprises one block, whereas the VisProjector\ncontains two blocks.\nA.2\nMinimizing the Numerical Error in Diffusion Models\nTo make our discussion clearer, we switch the diffusion noise schedule into an angular form as\nfollows:\n\u001asin ϕt = √1 −¯αt,\ncos ϕt = √¯αt.\n(9)\nIn this way, the forward diffusion process can be written as follows:\nx(t) = √¯αtx +\n√\n1 −¯αtϵ = cos ϕtx + sin ϕtϵ,\n(10)\nwhere x(t), x and ϵ are noised image latent, original image latent and gaussian noise, respectively.\nOur goal is to minimize the numerical error term in the DDIM sampling process. However, the\nform of DDIM sampling process is different under different parameterization method of the dif-\nfusion model. Therefore, we need to first define a general form to represent the diffusion model\nparameterization.\nWe consider the diffusion model output u(t)\nθ\npredict a linear combination of data x and noise ϵ, i.e.\nu(t) = atx + btϵ. Note that the coefficients can vary according to the diffusion time step t. Further\nre-writing the coefficients in the angular form gives:\nu(t) = rt cos ψtx + rt sin ψtϵ,\n(11)\nwhere rt =\np\na2\nt + b2\nt represents the scale of u(t). cos ψt and sin ψt balance the proportion of x and\nϵ. Combining eq.10 and eq.11, we can in turn represent x and ϵ with u(t) and x(t):\n\n\n\nx =\nsin ψtx(t)−sin ϕtu(t)\/rt\ncos ϕt sin ψt−cos ψt sin ϕt = sin ψtx(t)−sin ϕtu(t)\/rt\nsin(ψt−ϕt)\n,\nϵ =\ncos ψtx(t)−cos ϕtu(t)\/rt\nsin ϕt cos ψt−sin ψt cos ϕt = −cos ψtx(t)−cos ϕtu(t)\/rt\nsin(ψt−ϕt)\n.\n(12)\nNow we consider the general form of DDIM sampling step (Song et al., 2021):\nx(t−1) = cos ϕt−1ˆxθ(x(t)) + sin ϕt−1ˆϵθ(x(t)),\n(13)\n15\n𝑥(\") = cos𝜙\"𝑥+ sin𝜙\"𝜖\n𝑥(\"$%) = cos𝜙\"$% +𝑥& + sin𝜙\"$% ̂𝜖&\n−Δ𝜙\n𝜙! −𝜓!\n'!\n\"\n(\"  (predicts cos𝜓\"𝑥+ sin𝜓\"𝜖)\n𝜙!\"# −𝜓!\n𝜙! −𝜓!\n−Δ𝜙\n𝐵\n𝐶\nϵ\n𝑥\n𝑂\n𝐴\n𝐴𝐵\nsin 𝜙\"$% −𝜓\"\n=\n𝑂𝐵\nsin −Δ𝜙\n=\n𝑂𝐴\nsin 𝜙\" −𝜓\"\nSine law of Δ𝑂𝐵𝐴:\nFigure 4: Geometric interpretation of a DDIM sampling step under arbitrary diffusion model pa-\nrameterization.\nwhere ˆxθ(x(t)) and ˆϵθ(x(t)) are the estimated image latent and noise, respectively.\nNote that by using eq.12, both of ˆxθ(x(t)) and ˆϵθ(x(t)) can be derived from the noisy image latent\nx(t) and the diffusion model output u(t)\nθ . Therefore, we can further represent x(t−1) in the following\nform:\nx(t−1) = cos ϕt−1\nsin ψtx(t) −sin ϕtu(t)\nθ \/rt\nsin(ψt −ϕt)\n−sin ϕt−1\ncos ψtx(t) −cos ϕtu(t)\nθ \/rt\nsin(ψt −ϕt)\n= sin(ϕt−1 −ϕt)u(t)\nθ \/rt −sin(ϕt−1 −ψt)x(t)\nsin(ψt −ϕt)\n.\n(14)\nEq.14 represents the general form of DDIM sampling step under any kind of diffusion model param-\neterization in the form of eq.11. To help understanding, we further present the geometric meaning\nof eq.14. As shown in fig.4, term x(t−1), x(t), and u(t)\nθ \/rt all locate on the unit circle in the x −ϵ\nplain. We find that eq.14 can be interpreted as projecting x(t−1) onto the (x(t), u(t)\nθ\nrt ) coordinate\nsystem. We illustrate this projection by adding auxiliary line AB and AC. By solving the sine law\nof △OBA given OA = 1, we get:\n(\nOB =\nsin(∆ϕ)\nsin(ψt−ϕt)\nBA = −sin(ϕt−1−ψt)\nsin(ψt−ϕt)\n(15)\nBy representing x(t−1) = OB · u(t)\nθ \/rt + AB · x(t), we get:\nx(t−1) =\nsin(∆ϕ)\nsin(ψt −ϕt)u(t)\nθ \/rt −sin(ϕt−1 −ψt)\nsin(ψt −ϕt) x(t),\n(16)\nwhich aligns with eq.14 given that ∆ϕ = ϕt−1 −ϕt.\nNow, we take the numerical error into consideration by multiplying the model output by a factor\n1 + δ, where δ represents the relative error:\n˜x(t−1) = sin(ϕt−1 −ϕt)(1 + δ)u(t)\nθ \/rt −sin(ϕt−1 −ψt)x(t)\nsin(ψt −ϕt)\n.\n(17)\nFurther, we can isolate the numerical error term from the ideal DDIM sampling step:\n˜x(t−1) = x(t−1) + sin(ϕt−1 −ϕt)\nu(t)\nθ \/rt\nsin(ψt −ϕt)δ.\n(18)\n16\nFrom eq.18, we conclude that the numerical error of a DDIM sampling step is determined by four\nfactors, namely, the step size ∆ϕ = ϕt−1 −ϕt, the normalized model output u(t)\nθ \/rt, the relative\nerror of the data type δ, and sin(ψt −ϕt), which is decided by the parameterization of the diffusion\nmodel.\nNotably, not all these four factors are useful to achieve the goal of minimizing the numerical error.\nFor example, tuning down the step size only decreases the numerical error of each step. As a result,\nthe total step number of DDIM sampling is increased proportionally, which cancels out the effect of\nerror reduction of each single step. The factor u(t)\nθ \/rt is not adjustable since it constantly has a unit\nstandard deviation. This can be verified by the following calculation:\nE[(u(t)\/rt)2] = E[(cos ψtx + sin ψtϵ)2] = cos2 ψtE[x2] + sin2 ψt.\n(19)\nIn common practice, image tokens x are normalized into unit standard deviation.\nTherefore,\nE[(u(t)\/rt)2] = cos2 ψt + sin2 ψt = 1.\nIf we decide to scale up our model, it is better to leverage the pre-trained LLMs as well as the highly\nefficient training infrastructure that is specifically optimized for LLMs. This makes bfloat16\nalmost the only choice. As a result, the relative error δ is fixed to 1\/128.\nNow, our only choice is to adjust the diffusion model parameterization method, so that | sin(ψt−ϕt)|\nis maximized. A simple solution is to set ψt−ϕt = π\/2, resulting in the following parameterization:\nu(t) = rt cos(ϕt + π\/2)x + rt sin(ϕt + π\/2)ϵ = rt(cos ϕtϵ −sin ϕtx).\n(20)\nNote that rt is still undetermined, which reflects the scale of u(t). From the analysis above, rt does\nnot affect the numerical error term, since it is canceled out by the normalization of the model output,\nas seen in the factor u(t)\nθ \/rt. Therefore, rt can be chosen freely, or based on other considerations. We\nconsider that the smooth optimization of a neural network often requires the activation and output\nnot too large or small. Therefore, we require a unit standard deviation for u(t), making rt = 1\nconstantly.\nThe final parameterization of our diffusion model is as follows:\nu(t) = cos ϕtϵ −sin ϕtx.\n(21)\nWe notice that this parameterization is coincidentally the “v-prediction” parameterization (Salimans\n& Ho, 2022). Note that, however, “v-prediction” is initially proposed for the efficient distillation of\ndiffusion models, rather than reducing the numerical error of diffusion models. To the best of our\nknowledge, our work is the first to derive “v-prediction” parameterization from the first principle of\nminimizing the numerical error in diffusion models.\nA.3\nDeriving Theoretical Numerical Error for ϵ-Prediction Models\nThe ϵ-prediction parameterization corresponds to ψt =\nπ\n2 in the angular parameterization form\ngiven by eq.11. Substituting ψt = π\n2 and u(t)\nθ \/rt = ϵθ into eq.18, we get:\n˜x(t−1) = x(t−1) + sin(ϕt−1 −ϕt)\nϵθ\ncos(ϕt)δ.\n(22)\nFurther, we cancel out the step size factor sin(ϕt−1 −ϕt) within the numerical error term, only\nfocusing on “the numerical error introduced per unit DDIM step”:\ne(t) =\nϵθ\ncos ϕt\nδ.\n(23)\nNext, we will show that e(t) can also be interpreted as the equivalent v-prediction numerical error\nfor an ϵ-prediction model.\nFor an ϵ-prediction model, u(t)\nθ\n= ϵθ. In order to calculate the equivalent v(t)\nθ\nvalue, we need to\nrepresent v(t)\nθ\nwith the predicted ϵθ and the known x(t), which is calculated as follows:\nv(t)\nθ\n= cos ϕtϵθ −sin ϕtˆxθ(x(t)) = cos ϕtϵθ −sin ϕt\nx(t) −sin ϕtϵθ\ncos ϕt\n=\nϵθ\ncos ϕt\n−tan ϕtx(t). (24)\n17\nConsidering the numerical error, we get:\n˜v(t)\nθ\n= ϵθ(1 + δ)\ncos ϕt\n−tan ϕtx(t) = v(t)\nθ\n+\nϵθ\ncos ϕt\nδ.\n(25)\nNote that the numerical error term in the above equation is exactly e(t), proving that e(t) can be\ninterpreted as the equivalent v-prediction numerical error for an ϵ-prediction model.\nTaking numerical error effect into the v-prediction-based diffusion loss, we get:\nE[(v(t)−˜v(t)\nθ )2] = E[(v(t)−v(t)\nθ −e(t))2] = E[(v(t)−v(t)\nθ )2]−2E[(v(t)−v(t)\nθ )e(t)]+E[(e(t))2]. (26)\nDue to the fact that numerical error e(t) is independent from the training loss and that the expectation\nof e(t) is 0, we get E[(v(t) −v(t)\nθ )e(t)] = 0. Therefore, the only numerical error term is E[(e(t))2].\nGiven that the standard deviation of ϵθ is 1, and considering that we use bfloat16 as training data\ntype, which means δ = 1\/128, we get\nE[(e(t))2] = 1\/(128 cos(ϕt))2 = 1\/(1282 ¯αt).\n(27)\nThis is the theoretical numerical error of the v-prediction diffusion loss for an ϵ-prediction model.\nA.4\nCFG With v-prediction\nFrom Equation v(t)\ni\n= √¯αtϵ −√1 −¯αtxi, we can derive the following equation.\nϵ =\np\n1 −¯α(t)x(t) +\np\n¯α(t)v\n(28)\nFor the CFG of ϵ, it can be simplified as follows.\nϵ =ϵu + ω(ϵc −ϵu)\n=\np\n1 −¯α(t)x(t) +\np\n¯α(t)vu + ω\np\n¯α(t)(vc −vu)\n=\np\n1 −¯α(t)x(t) +\np\n¯α(t)(vu + ω(vc −vu))\n(29)\nUltimately, we obtain v = vu + ω(vc −vu). The CFG of v and ϵ are equivalent.\nA.5\nExamples:The effect of the stage 2 training\nFigure 5: The impact of the second training stage on image generation capability.\n18\nA.6\nExamples:Image Generation\nFigure 6: Generated images from MMAR-7B\n19\nA.7\nDetailed Visual Understanding Evaluation Results\nA total of 18 visual understanding benchmarks from VLMEvalKit (Duan et al., 2024) are used to\nevaluate MMAR models comprehensively. The evaluation is also conducted on the existing joint\nimage-text probabilistic models using the publicly available checkpoints34. The detailed evaluation\nresults are shown in table 4. All scores have been scaled to a range of 0 to 100 except that we show\nthe original score of MME benchmarks. The average score is calculated on the normalized score of\nall the benchmarks including MME.\nTable 4: Detailed visual understanding evaluation results.\nBenchmark\nChameleon-7B\nTransfusion*\nShow-o\nMMAR-0.5B\nMMAR-0.5B\nw\/ ϵ-pred.\nMMAR-0.5B\nw\/ VQ\nMMAR-7B\nAI2D\n34.81\n40.22\n32.48\n43.43\n41.90\n41.90\n62.63\nChartQA\n3.84\n9.56\n11.32\n10.20\n10.36\n9.36\n12.72\nDocVQA\n1.51\n6.72\n18.24\n7.62\n6.77\n6.79\n10.28\nHallu.Bench\n39.01\n41.54\n40.90\n42.80\n41.11\n41.54\n50.47\nMathVista\n21.90\n22.60\n23.20\n21.60\n23.10\n22.90\n32.10\nMMBenchCN\n10.14\n27.23\n0.52\n43.99\n38.83\n31.87\n66.49\nMMBenchEN\n13.32\n29.47\n42.44\n48.45\n45.53\n37.54\n66.32\nMMEP\n125.8\n594.3\n1182.7\n882.1\n880.7\n618.2\n1393.9\nMMEC\n33.9\n206.1\n225.0\n256.8\n232.1\n273.2\n320.0\nMMMU\n24.00\n29.33\n26.44\n29.33\n25.33\n29.67\n42.78\nMMStar\n20.47\n28.13\n32.00\n32.13\n31.07\n28.07\n40.00\nMMVet\n7.34\n13.90\n20.87\n18.49\n17.98\n14.45\n27.80\nOCRBench\n0.50\n2.30\n15.20\n18.70\n7.10\n2.10\n23.10\nPOPE\n30.86\n66.90\n84.50\n70.74\n71.14\n66.98\n83.02\nRealWorldQA\n27.06\n36.99\n27.97\n38.30\n35.16\n36.60\n52.42\nScienceQA\n44.83\n45.92\n41.82\n47.54\n45.21\n45.35\n71.05\nSEEDBench\n34.61\n42.40\n51.61\n55.70\n53.72\n44.93\n64.52\nTextVQA\n5.43\n9.94\n38.35\n16.77\n12.40\n9.46\n22.05\nAverage\n18.34\n28.26\n33.06\n34.56\n32.21\n29.70\n46.52\nA.8\nAnalysis on the Visual Understanding Ability of Transfusion-Like Methods\nIn this section, we will quantitatively analyze the incomplete information usage problem in\ntransfusion-like methods.\nThe experiments are conducted using our re-implemented transfusion model. The model is evaluated\non MMBench using different noise-level image input. Specifically, we take the diffusion time-step\nof 0, 125, 250, 375, 500, and 750 for testing. Among the MMBench tasks, we focus on tasks whose\nperformance is significantly affected by noise-levels. Fig. 7 depict the performance trend of these\ntasks across different noise levels.\n(A)\n(B)\nFigure 7: The trend of MMBench tasks’ performance with respect to input image noise levels.\n(A): The best performance of most tasks are achieved at a non-zero noise level. (B): The object\nlocalization performance consistently gets higher as the noise level grows up.\nIntuitively, increasing noise levels should lead to a consistent performance decline across all visual\nunderstanding tasks due to information loss. However, our findings reveal a more nuanced rela-\n3https:\/\/huggingface.co\/facebook\/chameleon-7b\n4https:\/\/huggingface.co\/showlab\/show-o-w-clip-vit\n20\ntionship. We find that the best performance of most visual understanding tasks are achieved at a\nnon-zero noise level. Especially, the object localization task, which is more focused on the image\nlayout information, consistently performs better as the noise level grows higher. This suggests that\ntransfusion-like methods, when limited to a single noise level input, fail to fully exploit the informa-\ntion of the input image. Instead, visual information appears to be distributed across different noise\nlevels within the model, with the input noise level dictating which information is ultimately utilized.\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling.pdf"}
{"title":"Looking at CTR Prediction Again: Is Attention All You Need?","authors":"Yuan Cheng, Yanbo Xue","summary":"Click-through rate (CTR) prediction is a critical problem in web search,\nrecommendation systems and online advertisement displaying. Learning good\nfeature interactions is essential to reflect user's preferences to items. Many\nCTR prediction models based on deep learning have been proposed, but\nresearchers usually only pay attention to whether state-of-the-art performance\nis achieved, and ignore whether the entire framework is reasonable. In this\nwork, we use the discrete choice model in economics to redefine the CTR\nprediction problem, and propose a general neural network framework built on\nself-attention mechanism. It is found that most existing CTR prediction models\nalign with our proposed general framework. We also examine the expressive power\nand model complexity of our proposed framework, along with potential extensions\nto some existing models. And finally we demonstrate and verify our insights\nthrough some experimental results on public datasets.","url":"http:\/\/arxiv.org\/abs\/2105.05563v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2105.05563v1","published":1620815234000,"comment":"9 pages, 2 figures, 4 tables, SIGIR'21","pdf_text":"Looking at CTR Prediction Again: Is Attention All You Need?\nYuan Cheng, Yanbo Xue\nCareer Science Lab, BOSS Zhipin\nBeijing, China\nABSTRACT\nClick-through rate (CTR) prediction is a critical problem in web\nsearch, recommendation systems and online advertisement display-\ning. Learning good feature interactions is essential to reflect user’s\npreferences to items. Many CTR prediction models based on deep\nlearning have been proposed, but researchers usually only pay atten-\ntion to whether state-of-the-art performance is achieved, and ignore\nwhether the entire framework is reasonable. In this work, we use the\ndiscrete choice model in economics to redefine the CTR prediction\nproblem, and propose a general neural network framework built\non self-attention mechanism. It is found that most existing CTR\nprediction models align with our proposed general framework. We\nalso examine the expressive power and model complexity of our pro-\nposed framework, along with potential extensions to some existing\nmodels. And finally we demonstrate and verify our insights through\nsome experimental results on public datasets.\nCCS CONCEPTS\n• Information systems →Personalization; Recommender sys-\ntems.\nKEYWORDS\nclick-through rate prediction; neural networks; self-attention mecha-\nnism; factorization machines; discrete choice model\nACM Reference Format:\nYuan Cheng, Yanbo Xue. 2021. Looking at CTR Prediction Again: Is Atten-\ntion All You Need? . In Proceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR\n’21), July 11–15, 2021, Virtual Event, Canada. ACM, New York, NY, USA,\n9 pages. https:\/\/doi.org\/10.1145\/3404835.3462936\n1\nINTRODUCTION\nWith the booming of web 2.0, it is becoming more and more conve-\nnient for users to shop products, read news, and find jobs online. For\nservice providers to attract and engage their users, they often rely\non personalized recommendation systems to rank a small amount\nof items from a large amount of candidates. To achieve this goal,\npredicting user’s behavior specifically via click-through rate (CTR)\nprediction becomes increasingly important. Therefore, effectively\nCorresponding author: xueyanbo@kanzhun.com.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and\/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR ’21, July 11–15, 2021, Virtual Event, Canada.\n© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8037-9\/21\/07...$15.00\nhttps:\/\/doi.org\/10.1145\/3404835.3462936\nand accurately predicting CTR has attracted widespread attentions\nfrom both researchers and engineers.\nFrom the perspective of a machine learning task, CTR prediction\ncan be viewed as a binary classification problem. Classical ma-\nchine learning models have played a very important role in the early\nadoption of CTR models, such as logistic regression (LR) models\n[1, 5, 18, 27]. Because linear models work under the strong assump-\ntion of linearity, a lot of and sometimes tedious feature engineering\nefforts are necessary to generate features that can be interacted lin-\nearly. To relax this constraint, a factorization machine (FM) model\n[23–25] was proposed to automatically learn the second-order fea-\nture interactions. FMs and their extensions provide a popular solution\nto efficiently using second-order feature interaction, but they are still\non the second-order level. For this reason, some deep neural net-\nworks (DNNs) are introduced to realize more powerful modeling\nability to include high-order feature interactions. Among them, the\nfactorization-supported neural network (FNN) [36] is the first deep\nlearning model that uses the embedding learned from FM to initial-\nize DNNs, and then learns high-order feature interactions through\nmulti-layer perceptrons (MLPs).\nMeanwhile, deep learning has successfully marched into many\nother application fields [15], especially computer vision (CV) [10]\nand natural language processing (NLP) [6]. Deep learning algorithms\nenable machines to perform better than humans in some specific\ntasks [29]. Deep learning techniques have become the method of\nchoice for working on the tasks of recommendation systems, but\nsome researchers argue that the progress brought by deep learning\nis not clear [4] and many deep learning models have not really\nsurpassed traditional recommendation algorithms such as item-based\ncollaborative filters [4, 17] and matrix factorizations [26]. Deep\nlearning is usually branded as a black box due to the gap between its\ntheoretical results and empirical evidences. For example, in terms of\na recommendation system, DNNs usually involve implicit nonlinear\ntransformations of input features through a hierarchical structure\nof neural networks. Finding a unified framework that can explain\nwhy it works (or why it does not) has become an important mission\nfaced by many researchers. As yet another attempt, this paper aims\nto re-examine existing CTR prediction models from the perspectives\nof feature-interaction-based self-attention mechanism.\nOur goal for this work is to unify the existing CTR prediction\nmodels, and form a general framework using the attention mecha-\nnism. We divide our framework into three types, which encompass\nmost of the existing models. We use our proposed framework to\nextend the previous models and analyze the CTR models from per-\nspectives of theoretical and numerical results. From our research,\nwe can classify almost all second-order feature interaction into the\nframework of the attention mechanism, therefore attention is indeed\nall you need for feature processing in CTR prediction. Our proposed\nframework has been validated on two public datasets.\nFour major contributions of our work are:\narXiv:2105.05563v1  [cs.IR]  12 May 2021\n• We use the discrete choice model to redefine the CTR predic-\ntion problem, and propose a general neural network frame-\nwork with embedding layer, feature interaction, aggregate\nlayer and space transform.\n• We propose a general form of feature interaction based on the\nself-attention mechanism, and it can encompass the feature\nprocessing functionalities of most existing CTR prediction\nmodels.\n• We examine the expressive ability of the feature interaction\noperators in our framework and propose our model to extend\nthe previous models.\n• Using two real-world CTR prediction datasets, we find our\nmodel can achieve extremely competitive performance against\nmost existing CTR models.\nThe remainder of this paper is organized as follows. In Section 2,\nwe surveyed existing models related to CTR prediction. Our pro-\nposed model is developed in Section 3, followed by a detailed analy-\nsis of its expressive power and complexity in Section 4. Extensive\nexperiments are conducted in Section 5 to validate its performance.\nAfter discussing the implication of our work in Section 6, we con-\nclude this paper in Section 7.\n2\nRELATED WORK\nEffective modeling of the feature interactions is the most important\npart in CTR prediction. Earlier attempts along this line include\nfactorization machines and their extensions, such as higher-order\nFMs (HOFMs) [2], field-aware FMs (FFMs) [13], and field-weighted\nFMs (FwFMs) [20]. At the rise of deep learning models, deep neural\nnetworks have provided a structural way in characterizing more\ncomplex feature interactions [11].\nIn addition to the depth, some researchers proposed to add width\nto the deep learning model. As such, Wide & Deep model [3] was\nproposed as a framework that combines a linear model (width) a\nDNN model (depth). Through joint training of the wide and deep\nparts, it can be better adapted to the tasks in recommendation system.\nAnother example is the DeepCross model [28] for ads prediction,\nwhich shares the same designing philosophy as Wide & Deep other\nthan its introduction of residual network with MLPs. However, the\nlinear model in the Wide & Deep model still need feature engineer-\ning. To alleviate this, DeepFM model [9] was proposed to replace\nthe linear model in Wide & Deep with FMs. DeepFM shares the\nembedding between FMs and DNNs, which affects features of both\nlow-order and high-order interactions to make it more effective.\nAt the same time, rather than leaving the modeling of high-order\nfeature interactions entirely to DNNs, some researches are dedicated\nto constructing them in a more explicit way. For example, product-\nbased neural network (PNN) [22] was proposed to perform inner\nand outer product operations by embedding features before MLP\nis applied. It uses the second-order vector product to perform pair-\nwise operations on the FM-embedded vector. The Deep and Cross\nNetwork (DCN) [34] can automatically learn feature interactions on\nboth sparse and dense input, which can effectively capture feature\ninteraction without manual feature engineering and at a low compu-\ntational cost. Similarly, in order to achieve automatic learning the\nexplicit high-order feature interaction, eXtreme Deep Factorization\nMachine (XDeepFM) is proposed. In XDeepFM, a Compressed In-\nteraction Network (CIN) structure is established to model low-level\nand high-level feature interactions at the vector-wise level explicitly.\nHowever, efforts spent on modeling high-order interactions might\nbe easily dispersed since some researchers consider that the effect\nof higher than the second-order interactions on the performance is\nrelatively small [19].\nThanks to the success of transformer model [33] in NLP, the\nmechanism of self-attention has attracted some researchers in rec-\nommendation systems. To solve the problem that in FM model all\nfeature interactions have the same weight, the Attentional Factor-\nization Machine (AFM) model [35] was proposed, which uses a\nneural attention network to learn the importance of each feature\ninteraction. Another work, known as AutoInit [30], was also in-\nspired by the multi-headed self-attention mechanism in modeling\ncomplex dependencies. Base on a wide and deep structure, AutoInt\ncan automatically learn the high-order interactions of input features\nthrough the multi-headed self-attention mechanism and provide a\ngood explainability to the prediction results as well.\nAll existing works, seemingly disconnected from each other, can\nsomehow be brought under the same framework, which is the main\ncontribution of our work to this community.\n3\nMODEL\n3.1\nProblem Formulation\nFor item 𝑗∈𝑄and user 𝑖∈𝑃, 𝑦𝑖,𝑗∈{0, 1} indicates whether\nthe 𝑖-th user has engaged with the 𝑗-th item, with 𝑄and 𝑃being\nthe collections of items and users, respectively. In CTR prediction,\nengagement can be defined as clicking on an item. Our goal is to\npredict the probability of 𝑝𝑖engaging with 𝑞𝑗. Obviously, this is a\nsupervised binary classification problem. Each sample is composed\nof input of features 𝑋= (𝑋𝑝𝑖,𝑋𝑞𝑗) and output of a binary label 𝑦𝑖,𝑗.\nThe machine learning task is to estimate the probability for input 𝑋\nas follows,\nPr\n\u0010\n𝑦𝑖,𝑗= 1|𝑋𝑝𝑖,𝑋𝑞𝑗\n\u0011\n= 𝐹\n\u0010\n𝑋𝑝𝑖,𝑋𝑞𝑗\n\u0011\n(1)\nwhere 𝑋𝑝𝑖is the feature of user 𝑖, and 𝑋𝑞𝑗is the feature of item 𝑗.\n3.2\nDiscrete Choice Model\nCTR prediction problem corresponds to an individual’s binary choice.\nWe can use a discrete choice model (DCM) [31] to describe this.\nDCM has found its wide range of applications in economics and\nother social science studies [32].\nThe choice function of user 𝑖belonging to Π𝑖: 𝑈→𝐴, where\n𝑈= R is the utility space and 𝐴is the users’ choice sets {0 :\nnot click, 1 : click}. Let us define the utility obtained by user 𝑖to\nchoose item 𝑗as follows,\n𝑢𝑖,𝑗= 𝐻\n\u0010\n𝑋𝑝𝑖,𝑋𝑞𝑗\n\u0011\n−𝜃𝑖,𝑗+ 𝑘𝑖𝜖𝑖\n(2)\nwhere 𝐻(𝑋𝑝𝑖,𝑋𝑞𝑗) is the deterministic utility and 𝜃𝑖,𝑗is the expected\nutility, both indicating the 𝑖-th user choosing the 𝑗-th item. Here 𝜖𝑖is\na unit noise following a standard Gumbel distribution and 𝑘𝑖is the\nnoise level indicating uncertainty in the choice of user 𝑖.\nWe can use a logit-based DCM to describe the user’s behavior.\nThe probability of user 𝑖selecting item 𝑗can be expressed as,\n𝑤𝑖,𝑗=\n1\n1 + exp\n\u0012\n𝜃𝑖−𝐻(𝑋𝑝𝑖,𝑋𝑞𝑗)\n𝑘𝑖\n\u0013 .\n(3)\nIn the CTR prediction problem, features of the users and items\nare treated as a whole, i.e., 𝑋= (𝑋𝑝𝑖,𝑋𝑞𝑗), with which Equation 3\ncan be re-written as\n𝑤𝑖,𝑗= 𝜎(𝑀(𝑋)) ,\n(4)\nwhere 𝑀(𝑋) = (𝐻(𝑋) −𝜃𝑖)\/𝑘𝑖and 𝜎(𝑥) = 1\/(1+exp(−𝑥)). 𝑀(𝑋),\nas a nonlinear utility, can be defined as 𝑀(𝑋) = 𝐹NN(𝑋) using a\nneural network structure as shown in Figure 1. Therefore, learning\nin a recommendation system is equivalent to obtaining the function\n𝑀(𝑋).\nThe binary cross-entropic loss can be obtained by maximum\nlikelihood method, which is defined as follows,\nL = −1\n𝑁\n∑︁\n𝑖,𝑗\n\u0002\n𝑦𝑖,𝑗log𝑤𝑖,𝑗+ (1 −𝑦𝑖,𝑗) log(1 −𝑤𝑖,𝑗)\n\u0003\n.\n(5)\nThe above loss function is called log-loss, which is widely used in\nCTR prediction models.\n3.3\nA General Neural Network Framework\nOur proposed neural network framework is illustrated in Figure 1.\nFor the sake of clarity, we only show main parts of the framework.\nThe linear regression part as well as the skip connection similar to\nmany previous models have been ignored.\nEmbedding 1\nField 1\nField n\nField 2\n… …\n… …\n… …\n… …\n… …\n… …\n… …\n… …\n… …\nEmbedding 2\nEmbedding n\nInput layer\nEmbedding Layer\nFeature Interaction\nAggregate layer\nLatent space transform\nUtility\nFigure 1: Overview of general framework of CTR prediction.\n3.3.1\nEmbedding layer (EL). In this work, only categorical fea-\ntures are considered, and numeric features can be converted into cat-\negorical data through discretization. Each feature can be expressed\nas a one-hot encoding. It is assumed that the features have 𝑛fields as\n𝑋= (𝑥1,𝑥2, ...,𝑥𝑛). The one-hot encoding 𝑥𝑖can be converted into\na vector in a latent space through embedding operation as follows\n𝑓𝑖= 𝐹emb(𝑥𝑖) = 𝑊𝑇\n𝑖𝑥𝑖\n(6)\nwhere 𝑊𝑖is the embedding matrix corresponding to the look-up\ntable of the 𝑖-th field. In this work, the latent space is called utility\nspace R𝑑. After embedding operations, we can represent the cate-\ngorical data 𝑥𝑖as a vector 𝑓𝑖∈R𝑑in the 𝑑-dimensional utility space.\nTotally 𝑛fields can be denoted as 𝑓= [𝑓1, 𝑓2, · · · , 𝑓𝑛] and we denote\n{𝑓1, 𝑓2, · · · , 𝑓𝑛} as F .\n3.3.2\nFeature interaction (FI). This part corresponds to the indi-\nvidual’s comprehensive measurement of the influence of different\nfactors in the decision-making process. Due to that the relationship\nbetween the factors considered in the individual’s decision-making\nprocess is not independent [23], FM has done a pioneering work in\nconsidering the second-order feature interactions.\nThe feature interaction layer is responsible for the second-order\ncombination between features. The output is a 𝑘-dimensional vector.\nThis layer is responsible for the second-order combination between\nfeatures. Inspired by self-attention mechanism [33], a second-order\noperator of vector 𝑣taking action on feature 𝑓𝑖can be written as\nfollows\n𝑏𝑆,𝑈,𝑣(𝑓𝑖) = 𝑆(𝑓𝑖, 𝑣) · 𝑈(𝑓𝑖, 𝑣)\n(7)\nwhere 𝑆(·, ·) is a similarity function to measure the correlation degree\nbetween 𝑓𝑖and 𝑣, and its value range is [−∞, ∞]. And 𝑈(·, ·) is an\nutility function that indicates an individual utility induced by vector 𝑣.\nThe utility function is a vector-valued function. When the dimension\nis 1, it is reduced to a scalar-valued function.\nEquation 7 represents the utility vector obtained by the feature\n𝑓𝑖induced by vector 𝑣. The utility vector on 𝑓𝑖induced by multiple\nvectors in 𝑉can be expressed as\n𝐵𝑆,𝑈,𝑉(𝑓𝑖) =\n∑︁\n𝑣∈𝑉\n𝑏𝑆,𝑈,𝑣(𝑓𝑖) =\n∑︁\n𝑣∈𝑉\n𝑆(𝑓𝑖, 𝑣) · 𝑈(𝑓𝑖, 𝑣)\n(8)\nwhere 𝑆is the similarity function which can be viewed as weights\nfor the outcomes in Equation 8. In case that the weights are required\nto be positive, we can apply a softmax function. For convenience,\nwe can denote 𝑆𝑠as\n𝑆𝑠(𝑓𝑖, 𝑣) = ⟨𝑓𝑖, 𝑣⟩softm =\nexp {𝑆(𝑓𝑖, 𝑣)}\nÍ\n𝑣∈𝑉exp {𝑆(𝑓𝑖, 𝑣)} .\n(9)\nThe most common similarity function is the inner product opera-\ntor ⟨·, ·⟩. When the value of 𝑆does not change, 𝑆(𝑓𝑖, 𝑣) = 𝑤becomes\na constant-valued function, denoted as 𝑤. If 𝑆(𝑓𝑖, 𝑣) = 1, we directly\ndenote 𝑆as 1. The most common forms of utility function (score\nfunction) are linear function and constant-valued function. We set\n1(𝑣) = 1 as 1, the linear function as 𝐿, and the identity function of\n𝐼(𝑣) = 𝑣as 𝐼.\nThis part actually defines an attention mechanism between 𝑓𝑖and\n𝑉. If 𝑉= {𝑓1, 𝑓2, · · · , 𝑓𝑚}, the feature interaction reduces to self-\nattention effects. For simplicity, we denote 𝐵𝑆,𝑈,𝑉= 𝐵𝑆,𝑈as feature\ninteraction via self attention in the rest of this work.\n3.3.3\nAggregation layer (AL). Feature interaction can process\ninput features with 𝑛fields 𝑓= [𝑓1, 𝑓2, · · · , 𝑓𝑛] into utility vectors of\n𝑛fields 𝑧= [𝑧1,𝑧2, · · · ,𝑧𝑛], where 𝑧𝑖= 𝐵𝑆,𝑈,𝑉𝑖(𝑓𝑖) ∈R𝑑. The role\nof the aggregation layer is to summarize the utility vectors of the 𝑛\nfields into a utility vector. Common aggregation methods include\nconcatenation and field combination, expressed as\n𝐴𝐶(𝑓) = vec[𝑓1, 𝑓2, · · · , 𝑓𝑛]\n(10)\nand\n𝐴𝐿(𝑓) =\n𝑛\n∑︁\n𝑖=1\n𝑤𝑖𝑓𝑖\n(11)\nrespectively. Field combination (Equation 11) is a linear combination\nof the utility vectors in 𝑛fields. In addition, we use 𝐴mean(𝑓) =\n1\n𝑛\nÍ𝑛\n𝑖=1 𝑓𝑖and 𝐴sum(𝑓) = Í𝑛\n𝑖=1 𝑓𝑖to denote the mean and sum of\nthe 𝑛fields.\n3.3.4\nSpace transformation (ST). After transformations of the\nfeature interactions, the features have been converted from the origi-\nnal input space to the utility space R𝑑. Assuming that the individual\ncan transform in the utility space during the decision-making pro-\ncess, we use the structure of MLPs to define such conversion. After\nthe input utility vector 𝑧(0) goes through a 𝑘-layer transformation,\nwe can obtain\n𝑧(𝑘) = 𝑀(𝑘) (𝑥) = 𝐿𝑘(𝑎𝑘(𝐿𝑘−1(𝑎𝑘−1(· · · (𝑧(0))))))\n(12)\nwhere 𝐿is a linear transformation, and 𝑎is a non-linear activation\nfunction, and we use 𝑀(0) to represent 𝐿. In this study, unless stated\notherwise, we set 𝑎(𝑥) = ReLU(𝑥) and 𝐿(𝑥) = 𝑊𝑇𝑥+ 𝑏.\nSo far, we have developed the backbone module of our proposed\nframework, and there are other functional operators that also play\nan important role in existing CTR models, including regulariza-\ntion methods like layer normalization, batch normalization, dropout,\nand 𝐿2 regularizer, and connection with network structure like skip\nconnection 𝑇𝐹(𝑥) = 𝐹(𝑥) + 𝑥.\nWe use the framework established in Figure 1 to decompose a\nCTR prediction model into\n𝑀(𝑋) = ST ◦AL ◦FI ◦EL(𝑥)\n(13)\nwhere EL corresponds to embedding layer, FI is the transformation\nof feature interaction, AL is the aggregation layer, and ST indicates\nthe spatial transformation.\n3.4\nFeature Interaction in CTR Models\nIn this work, we focus on second-order feature interactions, which is\nthe most effective and widely used in CTR prediction models. Using\nthe unified framework shown in Figure 1 and specifically through\nfeature interaction of FI = 𝐵𝑆,𝑈,𝑉, we can reformulate the feature\ninteraction layer of most existing CTR models as follows:\n3.4.1\nLogistic Regression (LR). LR model considers each fea-\nture independently, expressed as 𝜙LR(𝑥; 𝑓) = Í𝑛\n𝑖=1 𝑥𝑖𝑓𝑖where\n𝑓𝑖∈R. Therefore, for LR, feature interaction means FILR(𝑓𝑖) =\n𝑓𝑖= 𝐵1,𝐼,{𝑓𝑖}(𝑓𝑖). Meanwhile, the similarity function and the utility\nfunction are reduced to 1 and 𝐼respectively, and 𝑓𝑖corresponds to\n𝑤𝑖.\n3.4.2\nFactorization Machine (FM) . FM enhances the linear\nregression model by incorporating the second-order feature inter-\naction. FMs can learn the feature interaction by decomposing fea-\ntures into the inner product of two vectors as follows 𝜙FM(𝑥; 𝑓) =\nÍ\n𝑖<𝑗𝑥𝑖𝑥𝑗⟨𝑓𝑖, 𝑓𝑗⟩. And then we can find that the feature interaction in\nFM can be denoted as 𝐹𝐼𝐹𝑀(𝑓𝑖) = Í\n𝑗≠𝑖⟨𝑓𝑖, 𝑓𝑗⟩·1 = 𝐵⟨,⟩,1,¯𝑓𝑖(𝑓𝑖). The\nsimilarity function is inner operator, the utility function is reduced\nto 1, and 𝑉= ¯𝑓𝑖= {𝑓1, 𝑓2, · · · , 𝑓𝑛} −{𝑓𝑖}.\n3.4.3\nField-aware Factorization Machine (FFM). Each feature\nbelongs to a field. The features of one domain often interact with\nfeatures of other different fields. By obtaining the embedding vector\nfor 𝑛−1 fields of each feature, we can only use a vector 𝑣𝑖,𝐹(𝑗) to\ninteract with features 𝑗in the field 𝐹(𝑗) as follows 𝜙FFM(𝑥; 𝑓, 𝐹) =\nÍ\n𝑖<𝑗𝑥𝑖𝑥𝑗⟨𝑓𝑖,𝐹(𝑗), 𝑓𝑗,𝐹(𝑖)⟩where 𝐹(𝑖) indicates the field to which the\nfeature 𝑖belongs. We can find that\n𝐹𝐼FFM(𝑓𝑖,𝐹(𝑗)) =\n∑︁\n𝑗≠𝑖\n⟨𝑓𝑖,𝐹(𝑗), 𝑓𝑗,𝐹(𝑖)⟩· 1 = 𝐵⟨,⟩,1, ¯𝑓𝑖,𝐹(𝑗) (𝑓𝑖,𝐹(𝑗))\n.\n3.4.4\nField-weighted Factorization Machine (FwFM). FwFM\nis an improvement to FFM to model the different feature interactions\nbetween different fields in a much more efficient way expressed as\n𝜙FwFM(𝑥; 𝑓,𝑟) = Í\n𝑖<𝑗𝑥𝑖𝑥𝑗⟨𝑒𝑖,𝑒𝑗⟩𝑤𝐹(𝑖),𝐹(𝑗). And we can obtain\nFIFwFM(𝑓𝑖) = Í\n𝑗≠𝑖⟨𝑓𝑖, 𝑓𝑗⟩·𝑤𝐹(𝑖),𝐹(𝑗) = 𝐵⟨,⟩,𝑤𝐹(𝑖),𝐹(𝑗),¯𝑓𝑖(𝑓𝑖), where\nthe utility function becomes 𝑤𝐹(𝑖),𝐹(𝑗).\n3.4.5\nProduct-based Neural Network (PNN). PNN is able to\ncapture the second-order feature interactions through the product\nlayer, which can take the form of Inner Product-based Neural Net-\nwork (IPNN) or Outer Product-based Neural Network (OPNN).\nSince OPNN involves the operation of the aggregation layer, we fo-\ncus on IPNN, 𝜙IPNN(𝑥; 𝑓) = Í𝑛\n𝑖=1\nÍ𝑛\n𝑗=1 𝑥𝑖𝑥𝑗⟨𝑓𝑖, 𝑓𝑗⟩⟨𝜃𝑖,𝜃𝑗⟩, and we\ncan find that FIIPNN(𝑓𝑖) = Í\n𝑖,𝑗⟨𝑓𝑖, 𝑓𝑗⟩· ⟨𝜃𝑖,𝜃𝑗⟩= 𝐵⟨,⟩,⟨𝜃𝑖,𝜃𝑗⟩,¯𝑓𝑖(𝑓𝑖)\nwhere the utility function is ⟨𝜃𝑖,𝜃𝑗⟩.\n3.4.6\nDeep & Cross Network (DCN). DCN introduces a novel\ncross network (CN) [16] that is more efficient in learning certain\nbounded-degree feature interactions, which is defined as 𝜙CN(𝑓) =\n𝑤𝑓, i.e., FICN(𝑓𝑖) = 𝑤𝑓𝑖= 𝐵1,𝑤,𝑓𝑖(𝑓𝑖) where utility function is 𝑤.\n3.4.7\nDeepFM. As discussed previously, DeepFM combines the\npower of FM and MLPs into a new neural network architecture.\nHere we focus on the deep component which is the same as the Wide\n& Deep model. This part was called implicit feature interaction\nthrough MLP in previous research. Using our framework, the feature\ninteraction part is the same as LR, FIMLP(𝑓𝑖) = 𝑓𝑖= 𝐵1,𝐼,{𝑓𝑖}(𝑓𝑖),\nand the implicit feature interaction is realized by the aggregation\nlayer and the space transformation layer.\n3.4.8\nXDeepFM. The neurons in each layer of compressed in-\nteraction network (CIN) in XDeepFM are derived from the hidden\nlayer of the previous layer and the original feature vectors. The\nsecond-order interaction part in CIN can be expressed as 𝜙CIN(𝑓) =\nÍ\n𝑖,𝑗𝑝𝑖,𝑗⟨𝐴𝐿𝑖(𝑓),𝐴𝐿𝑗(𝑓)⟩where 𝐴𝐿𝑖and 𝐴𝐿𝑗are the field-wise ag-\ngregation operators and the feature interaction for the feature 𝑓𝑖is\nFICIN(𝑓𝑖) = Í\n𝑓𝑗⟨𝑓𝑖, 𝑓𝑗⟩· 𝑤𝑖,𝑗= 𝐵⟨,⟩,𝑤𝑖,𝑗,𝑓(𝑓𝑖)\n3.4.9\nAttentional Factorization machine (AFM). AFM has one\nextra layer of attention-based pooling than FM. The function of\nthe layer is to generate a weight matrix 𝑎𝑖,𝑗through the attention\nmechanism. The second-order interaction of AFM can be expressed\nas 𝜙AFM(𝑥) = Í\n𝑖,𝑗𝑎𝑖,𝑗⟨𝑓𝑖, 𝑓𝑗⟩𝑃𝑥𝑖𝑥𝑗. Here 𝑎𝑖,𝑗= 𝑒𝑎′\n𝑖,𝑗\/Í\n𝑖,𝑗𝑒𝑎′\n𝑖,𝑗and\n𝑎′\n𝑖,𝑗= ℎ𝑇ReLU(𝑊(𝑓𝑖⊙𝑣𝑗)𝑥𝑖𝑥𝑗+ 𝑏). Therefore we can see that\nFIAFM(𝑓𝑖) = Í\n𝑓𝑗∈¯𝑓𝑖𝑎𝑖,𝑗⟨𝑓𝑖, 𝑓𝑗⟩𝑃· 1 = 𝐵𝑎𝑖,𝑗⟨,⟩𝑃,1, ¯𝑓𝑖(𝑓𝑖).\nTable 1: Unifying CTR models under one framework.\nModel\nInput\n𝑆\n𝑈\n𝑉\nAL\nST\nLR\n𝑓𝑖\n1\n𝐼\n{𝑓𝑖}\n𝐴sum\n\/\nFMs\n𝑓𝑖\n⟨, ⟩\n1\n¯𝑓𝑖\n𝐴sum\n\/\nFFMs\n𝑓𝑖,𝐹(𝑗)\n⟨, ⟩\n1\n¯𝑓𝑖,𝐹(𝑗)\n𝐴sum\n\/\nFwFMs\n𝑓𝑖\n⟨, ⟩\n𝑤𝐹(𝑖),𝐹(𝑗)\n¯𝑓𝑖\n𝐴sum\n\/\nIPNN\n𝑓𝑖\n⟨, ⟩\n⟨𝜃𝑖,𝜃𝑗⟩\nF\n𝐴𝐶\n𝑀(𝑘)\nDCN\n𝑓𝑖\n1\n𝑤\n{𝑓𝑖}\n𝐴𝐶\n𝑀(0)\nDeepFM\n𝑓𝑖\n⟨, ⟩\n1\n¯𝑓𝑖\n𝐴𝐶\n𝑀(0)\nXDeepFM\n𝑓𝑖\n⟨, ⟩\n𝑤𝑖,𝑗\n¯𝑓𝑖\n𝐴𝐶\n𝑀(0)\nAFM\n𝑓𝑖\n⟨, 𝑃·⟩\n𝑎𝑖,𝑗\n¯𝑓𝑖\n𝐴sum\n\/\nAutoInt\n𝑓𝑖\n⟨𝑄·, 𝑃·⟩𝑠\n𝑉\nF\n𝐴𝐶\n𝑀(0)\n3.4.10\nAutoInt. AutoInt can automatically learn the high-order\ninteractions of the input features through multi-headed self-attention\nmechanism, expressed as FIAutoInt(𝑓𝑖) = Í\n𝑓𝑗∈𝑓⟨𝑄𝑓𝑖, 𝐾𝑓𝑗⟩softm ·\n𝑉𝑓𝑖= 𝐵⟨𝑄·,𝐾·⟩softm,𝑉,𝑓(𝑓𝑖) with ⟨·, ·⟩softm being the softmax func-\ntion defined in Equation 9.\n3.5\nSelf-Attention Feature Interaction\nFeature interaction is the key to the CTR prediction problem. Our\nwork mainly focuses on second-order features interaction\nFI(𝑓𝑖) =\n∑︁\n𝑓𝑗∈𝑉\nFI(𝑓𝑖, 𝑓𝑗) =\n∑︁\n𝑓𝑗∈𝑉\n𝑆(𝑓𝑖, 𝑓𝑗)𝑈(𝑓𝑖, 𝑓𝑗)\n(14)\nwhere 𝑓𝑖= EL(𝑥𝑖). 𝑆(·, ·) and 𝑈(·, ·) are defined similarly as in\nEquation 7.\nWe have defined a general neural network framework based on\nself-attention mechanism. As summarized in Table 1, most CTR\nprediction models can be unified under this framework. Further\nmore, models in Table 1 can be divided into three types:\n• Type 1: FI = 𝐵1,𝑤,{𝑓𝑖} = 𝑤𝑖𝑓𝑖. In this case, the second-order\nfeature interactions degenerate to first-order ones. Models\nlike LR, DCN, and the wide component in Wide & Deep and\nDeepFM belong to this type.\n• Type 2: FI = 𝐵⟨,⟩,𝑤𝑖,𝑗,F. It is the FM model and its extensions,\nincluding FM, FFM, FwFM, IPNN, XDeepFM, and AFM.\nThe characteristic of this type is that the similarity functions\nare all inner product operations 𝑆(𝑓𝑖, 𝑣) = ⟨𝑓𝑖, 𝑣⟩, and the\nutility function is a linear function with two variables in the\nform of 𝑈(𝑓𝑖, 𝑣) = 𝑤𝑖,𝑗(𝑣) where 𝑤𝑖,𝑗(𝑣) ∈R.\n• Type 3: FI = 𝐵⟨𝑄·,𝐾·⟩𝑠,𝑉,F(𝑓𝑖). This type uses self-attention\nmechanism in the transformer model, which contains Au-\ntoInt model. This type of model uses a similarity function as\n𝑆(𝑓𝑖, 𝑣) = ⟨𝑄𝑓𝑖, 𝐾𝑣⟩, and its utility function is a vector-valued\nfunction with one variable as 𝑈(𝑣) = 𝑉𝑣, where 𝑣∈R𝑑and\n𝑓𝑖∈R𝑑.\n3.6\nExtension to CTR Models\nWe can see that the most existing models can be divided into the\nabove three types of FI. As mentioned earlier, when self-attention is\nused, 𝐵𝑆,𝑈,𝑉is simplified as 𝐵𝑆,𝑈, we name such models as SAM,\nwhich means self-attention model. With SAM, a simple extension to\nthese three types of models can be made by\n𝑏SAM(𝑓𝑖, 𝑓𝑗) = 𝑆(𝑓𝑖, 𝑓𝑗)𝑈(𝑓𝑖, 𝑓𝑗),\n(15)\nwhere 𝑈(·, ·) is a vector-valued function depending on 𝑓𝑖and 𝑓𝑗. In\nthis work, 𝑈(·, ·) takes one the two following forms,\n𝑈(𝑓𝑖, 𝑓𝑗) = 𝑊𝑖,𝑗\n(16)\nand\n𝑈(𝑓𝑖, 𝑓𝑗) = 𝑓𝑖⊙𝑓𝑗,\n(17)\nwhere 𝑊𝑖,𝑗∈R𝑑are trainable parameters, and ⊙indicates element-\nwise product of two vectors. When Equation 16 is used in SAM\nmodel, we call this kind of model SAMA, which means SAM with\nAll trainable weights. When using Equation 17 in SAM, we obtain\nthe model called SAME, i.e., SAM by Element-wise product. Based\non the general framework we proposed, we can further extend these\nthree types of FI.\n3.6.1\nSAM1. FISAM1 = 𝐵⟨,⟩,1. The form of FI in SAM1 and\nLR model is exactly the same, except for its embedding dimension\nof 𝑓changing to 𝑑. Then, we have\nFISAM1(𝑓𝑖) = 𝑓𝑖\n(18)\nwith which we can obtain SAM1 as follows,\nSAM1(𝑓) = 𝑀(0) ◦𝐴𝐶◦FISAM1(𝑓),\n(19)\nwhere 𝑓= [𝑓1, 𝑓2, · · · , 𝑓𝑛], 𝐴𝐶is the concatenation aggregate layer\ndefined in Equation 10, and 𝑀(0) is a linear transformation defined\nin Equation 12.\n3.6.2\nSAM2. FISAM2 = 𝑏⟨,⟩,𝑈𝑖,𝑗. We can extend FM models to\nthe following two forms,\nFISAM2A (𝑓𝑖, 𝑓𝑗) = ⟨𝑓𝑖, 𝑓𝑗⟩𝑊𝑖,𝑗\n(20)\nand\nFISAM2E (𝑓𝑖, 𝑓𝑗) = ⟨𝑓𝑖, 𝑓𝑗⟩𝑓𝑖⊙𝑓𝑖,\n(21)\nwith which, we can obtain SAM2A and SAM2E as follows,\nSAM2A(𝑓) = 𝑀(0) ◦𝐴𝐶◦FISAM2A (𝑓)\n(22)\nand\nSAM2E(𝑓) = 𝑀(0) ◦𝐴𝐶◦FISAM2E (𝑓),\n(23)\nwhere, FISAM2A (𝑓) = [FISAM2A (𝑓𝑖, 𝑓𝑗)]𝑖,𝑗∈R𝑛×𝑛×𝑑and\nFISAM2E (𝑓) = [FISAM2E (𝑓𝑖, 𝑓𝑗)]𝑖,𝑗∈R𝑛×𝑛×𝑑.\n3.6.3\nSAM3. FISAM3 = 𝐵⟨𝑄·,𝐾·⟩softm,𝑉(𝑓𝑖). This type is closely\nrelated to self-attention mechanism in the transformer model. This\ntype of model uses a similarity function of 𝑆(𝑓𝑖, 𝑓𝑗) = ⟨𝑓𝑖, 𝐾𝑓𝑗⟩where\ntwo linear transformation are combined in the inner product, and we\nextend the original utility function of𝑈(𝑓𝑗) = 𝑉𝑓𝑗to𝑈(𝑓𝑖, 𝑓𝑗) = 𝑊𝑖,𝑗\nand 𝑈(𝑓𝑖, 𝑓𝑗) = 𝑓𝑖⊙𝑓𝑗, and then we can obtain\nFISAM3A (𝑓𝑖, 𝑓𝑗) = ⟨𝑓𝑖, 𝐾𝑓𝑗⟩𝑊𝑖,𝑗\n(24)\nand\nFISAM3E (𝑓𝑖, 𝑓𝑗) = ⟨𝑓𝑖, 𝐾𝑓𝑗⟩𝑓𝑖⊙𝑓𝑗.\n(25)\nInspired by the network structure of AutoInt [30], we propose\ntwo variants of SAM3 as follows\nSAM3A(𝑓) = 𝑀◦𝐴𝐿◦(FI(𝐿)\nSAM3A+𝑄(𝐿)) · · · (FI(1)\nSAM3A+𝑄(1))(𝑓)\n(26)\nand\nSAM3E(𝑓) = 𝑀◦𝐴𝐿◦(FI(𝐿)\nSAM3E+𝑄(𝐿)) · · · (FI(1)\nSAM3E+𝑄(1))(𝑓)\n(27)\nwhere 𝐿is the number of SAM layers, 𝑄is a linear mapping, and\n𝐴𝐿is a field combination aggregation. Without claimed explicitly,\n𝐿= 1 and 𝑀= 𝑀(0) in this work.\n4\nMATHEMATICAL ANALYSIS OF SAM\nSAM has four parts as shown in Equation 13. EL is embedding layer,\nFI is the transformation of feature interaction, AL is the aggregation\nlayer, and ST indicates the spatial transformation. We denote the set\nof all the models satisfying the form in Equation 13 as M.\n4.1\nExpressive Power\nDefinition 4.1. [Expressive power ⪰M] ∀𝑀1 ∈M when train-\nable parameters in 𝑀1 are determined, ∃𝑀2 ∈M with certain\nparameters in 𝑀2 and ST2 such that ST2 ◦AL2 ◦FI2 ◦EL2 =\nST1 ◦AL1 ◦FI1 ◦EL1, then we can say that the expressive power\nof 𝑀2 is higher than that of 𝑀1, which is denoted as 𝑀2 ⪰M 𝑀1.\nDefinition 4.2. [Expressive power =M] ∀𝑀1 ∈M and 𝑀2 ∈M,\nif 𝑀1 ⪰M 𝑀2 and 𝑀2 ⪰M 𝑀1, it can be considered that the\nexpressive power of 𝑀1 is equal to that of 𝑀2, which can be denoted\nas FI1 =M FI2.\nUsing Definitions 4.1 and 4.2, we make three propositions:\nPROPOSITION 4.3. SAM1 =M LR.\nPROPOSITION 4.4. SAM2A ⪰M FM ⪰M LR.\nPROPOSITION 4.5. SAM3A ⪰M SAM2A.\nThe above three propositions are easy to check and the proofs\nare thus omitted here. It is noted that the ST in SAMs is a linear\ntransformation. The idea behind the proof is that when EL, FI, LA\nand ST are all linear operators, the trainable parameters can be\naggregated together and absorbed by the free parameters in the last\nlayer. From these propositions, we can obtain\nSAM3A ⪰M SAM2A ⪰M FM ⪰M SAM1 =M LR.\n(28)\nWe see that if the deep learning method can find the global mini-\nmum of the CTR prediction problem, its expressive power can fully\nreflect the performance of the model. Therefore, we deduce that the\npotential of SAM3A and SAM2A model will be greater than that\nof FM and LR.\n4.2\nModel Complexity\nWe analyze the space complexity and time complexity of SAM1,\nSAM2 and SAM3 models in terms of the four operators in Equa-\ntion 13. In SAM, 𝑛is the number of feature fields,𝑑is the embedding\nvector dimension and 𝐿is the number of layers in SAM3. For the\nspace complexity, we ignore the bias term in the linear transforma-\ntion. EL is a shared component which contains 𝑑𝑛parameters. 𝐴𝐶\nTable 2: Summary of SAM complexities\nModel\nSpace 𝑂(·)\nTime 𝑂(·)\nLR\n𝑛\n𝑛\nSAM1\n2𝑑𝑛\n𝑑𝑛\nFM\n𝑛+ 𝑑𝑛\n𝑑𝑛\nSAM2A\n2𝑑𝑛2 + 𝑑𝑛\n2𝑑𝑛2\nSAM2E\n𝑑𝑛2 + 𝑑𝑛\n2𝑑𝑛2\nAutoInt\n3𝐿𝑑2 + 2𝑑𝑛\n𝐿(3𝑑2𝑛+ 2𝑑𝑛2) + 𝑑𝑛\nSAM3A\n𝐿(𝑑2 + 𝑑𝑛2) + 2𝑑𝑛\n𝐿(𝑑2𝑛+ 2𝑑𝑛2) + 𝑑𝑛\nSAM3E\n𝐿𝑑2 + 2𝑑𝑛\n𝐿(𝑑2𝑛+ 2𝑑𝑛2) + 𝑑𝑛\nhas no parameters and calculation overhead. ST is a linear transfor-\nmation, which has 𝑑𝑛parameters and the amount of computation\nis 𝑂(𝑑𝑛) for SAM1 and SAM3. And for SAM2, ST needs to be\ncalculated 𝑂(𝑑𝑛2) times with 𝑑𝑛2 parameters.\nThe main difference between these three models lies in FI. In\nSAM1, FI has no extra space and time cost. In SAM2, we need 𝑛2𝑑\nparameters for the weight vectors in SAM2A and no more space for\nSAM2E. And the time cost is 𝑂(2𝑛2𝑑) for SAM2. As for SAM3,\nfor each layer, the linear transform spends 𝑑2 parameters and extra\n𝑛2𝑑for the weights in SAM3A. The time overhead of SAM3 mainly\ndepends on the linear transformation 𝑂(𝑑2𝑛) and the computation\non attention 𝑂(2𝑑𝑛2) for each layer.\nBased on these analysis, we can get the model complexity results\nas shown in Table 2. The time and space complexities of the SAM1\nmodel are 𝑑times those of LR, the SAM2 model is about 𝑛times\nthat of FM, and the complexity of SAM3 and AutoInt is very close.\nConsidering that both 𝑑and 𝑛are relatively small, our SAM model\nhas a certain computational efficiency.\n5\nEXPERIMENTS\n5.1\nExperiment Setup\nTable 3: Statistics of the datasets.\nDataset\n# Samples\n# Categories\n# Fields\nCriteo\n45,840,617\n1, 086, 810\n39\nAvazu\n40,428,967\n2, 018, 012\n22\n5.1.1\nDatasets. In this section, we will conduct experiments to\ndetermine the performance of our model compared to other models.\nWe randomly divide the dataset into three parts: 80% for training,\nanother 10% for cross validation, and the remaining 10% for testing.\nTable 3 summarizes the statistics of the two following public datasets\nwe have used in our experiments:\n(1) Criteo1: It includes one week of display advertising data,\nwhich can be used to estimate the CTR of advertising by\nCriteoLab, and it is also widely used in many research papers.\nThe data contains the click records of 45 million users, which\ncontains 13 numerical feature fields and 26 categorical feature\nfields. The numerical feature is discretized by the function\ndiscrete(𝑥) = ⌊2×log(𝑥)⌋if 𝑥> 2 and int(𝑥−2) otherwise.\n(2) Avazu2: This is the data provided by Avazu to predict whether\na mobile ad will be clicked. It contains 40 million users’ 10\ndays of click log with 23 categorical feature fields. We remove\nthe field of sample id which is not helpful to CTR prediction.\n5.1.2\nEvaluation Metrics. In the experiment, we use two evalua-\ntion indicators: AUC (Area Under ROC) and log-loss (cross entropy;\nEquation 5). AUC is the area under the ROC curve which is a widely\nused metric for evaluating CTR prediction. AUC is not sensitive to\nclassification threshold and a larger value means a better result. Log-\nloss as the loss function in CTR prediction, is a widely used metric\nin binary classification, which can measure the distance between\ntwo distributions a smaller value indicates better performance.\n5.1.3\nBaseline Models. We have benchmarked our proposed\nSAM model against eight existing CTR models (LR, FM, FNN,\nPNN, DeepFM, XDeepFM, AFM and AutoInit as described in Sec-\ntion 3.4) as well as an original transformer encoder with one layer\nand one head, and two higher-order models (AFM [35] and HOFM\n[2]). For all deep learning models, unless explicitly specified, the\ndepth of hidden layers is set to 3, the number of hidden layer neu-\nrons is set to 32, and all the activation functions are set as ReLU. In\nterms of initialization, we initialize embedding vectors by Xavier’s\nuniform distribution method [8]. For regularization of all models,\nwe use 𝐿2 regularizer to prevent overfitting. Through performance\ncomparisons on different validation sets, we choose to use 𝜆= 10−5.\nIn addition, the dropout rate is set to 0.5 by default for some classic\nmodels which needs to use or not used otherwise.\n5.2\nPerformance Comparison\nAll models are implemented using neural network structures from\nPyTorch [21]. The models are trained with Adam optimization al-\ngorithm [14] (learning rate is set as 0.001). For all models, the\nembedding size is set to 16, and the batch size is set to 1024. We\nconduct all the experiments with 8 GTX 2080Ti GPUs in a cluster\nsetup.\nThe results of the numerical experiments are summarized in Ta-\nble 4. The scores are obtained by 10 different runs for each category.\nThe highest value across different models is shown in bold and the\nhighest performance obtained by baseline is underlined. We have\nverified the statistical significance in our results with 𝑝-value < 0.05.\nWe compared three proposed models, SAM1, SAM2, and SAM3,\nwith 12 CTR prediction models as well as the transformer encoder\nin a simple structure of a single-layer encoder with one head. It\ncan be found that our proposed SAM2E model performs the best\non both Criteo and Avazu datasets. The second-order interaction\nmodels IPNN and FM also perform competitively on Criteo datasets\nand Avazu datasets respectively, and are even better than XDeepFM\n1http:\/\/labs.criteo.com\/2014\/02\/kaggle-display-advertising-challenge-dataset\/\n2https:\/\/www.kaggle.com\/c\/avazu-ctr-prediction\/data\nTable 4: Overall performance on the datasets.\nModel\nCriteo\nAvazu\nAUC\nlog-loss\nAUC\nlog-loss\nLR\n0.7949\n0.4555\n0.7584\n0.3921\nFM\n0.8078\n0.4443\n0.7858\n0.3777\nFFM\n0.8077\n0.4438\n0.7742\n0.3829\nFwFM\n0.8089\n0.4427\n0.7778\n0.3810\nIPNN\n0.8107\n0.4408\n0.7818\n0.3791\nDCN\n0.8074\n0.4439\n0.7798\n0.3800\nDeepFM\n0.8030\n0.4487\n0.7798\n0.3799\nXDeepFM\n0.8104\n0.4414\n0.7809\n0.3798\nAFM\n0.8067\n0.4448\n0.7775\n0.3812\nAutoInt\n0.8106\n0.4411\n0.7834\n0.3780\nAFN\n0.8097\n0.4421\n0.7809\n0.3791\nHOFM\n0.7993\n0.4523\n0.7737\n0.3837\nTransformer\n0.7942\n0.4566\n0.7693\n0.3866\nSAM1\n0.7925\n0.4572\n0.7720\n0.3848\nSAM2E\n0.8115\n0.4404\n0.7891\n0.3755\nSAM2A\n0.8098\n0.4420\n0.7885\n0.3756\nSAM3E\n0.8071\n0.4451\n0.7805\n0.3821\nSAM3A\n0.8098\n0.4420\n0.7796\n0.3805\nbased on higher-order interactions in our experiments. Therefore, to\na certain extent, it consolidates the fact that many CTR prediction\nproblems mainly rely on the second-order feature interaction. The\nperformance improvement brought by higher-order interaction such\nas XDeepFM, Transformer and HOFM under the existing frame-\nwork may not be significant. It’s worth noting that AutoInt performs\nreasonably well on both datasets, which even rivals the popular\nTransformer model. This can be explained by the fact that, although\nlayer normalization can reduce the bias shift, it has also induced\ncorrelations among features that a shallow model is unable to resolve.\nThis also explains why our proposed single-layered SAM3 model\ndoes not perform well in general.\nIt can be found that the relationship we obtained in Equation 28 is\nnot completely consistent with the results of numerical experiments.\nFor example, the performance of SAM1 in the Criteo dataset is\nslightly worse than that of LR, but much higher than that of LR\nin the Avazu dataset. The performance of SAM2A is better than\nthat of SAM1 and FM models, and SAM3A in the Avazu dataset is\ninferior to SAM2A. From our experimental results, we can find that\nthe models with over-parameters would have potential to get better\nperformance.\nSince all weights are trainable, it is not surprising to observe\nthat SAM3A performed better than SAM3E, as evidenced by the\nlast two rows of Table 4. As part of an ablation study of SAM3A,\n1\n2\n3\n4\n5\nNumber of layers\n0.8085\n0.8090\n0.8095\n0.8100\n0.8105\n0.8110\n0.8115\nAUC\nCriteo\n0.779\n0.780\n0.781\n0.782\n0.783\nAUC\nAvazu\n(a) AUC\n1\n2\n3\n4\n5\nNumber of layers\n0.4400\n0.4405\n0.4410\n0.4415\n0.4420\n0.4425\n0.4430\n0.4435\nLogloss\nCriteo\n0.378\n0.379\n0.380\n0.381\n0.382\nLogloss\nAvazu\n(b) log-loss\nFigure 2: Performance as a function of the number of layers in\nSAM3A.\nwe discussed the relationship between the number of layers and its\nperformance. As shown in Figure 2, on both datasets, the perfor-\nmances of SAM3A are consistent with the change of the number\nof layers. When the number of layer is 3, SAM3A reaches its best\nperformance. At this time, the AUC on the Criteo data set is 0.8118\nand log-loss is 0.4401. It is slightly higher than the previous best\nresult from SAM2E. Better results are also obtained for the Avazu\ndataset, with an AUC of 0.7835 and a log-loss of 0.3778. This study\nprovides us insights that for models such as SAM3, multiple layers\nof self-attention structure can improve the performance, but the ex-\ncessively high-order feature interaction formed by too many layers\nwill reduce the effect of the model.\n6\nDISCUSSIONS\nThere is no doubt that over the last two decades, deep learning\nmodels have been very successful in the fields of CV and NLP, which\nalso make them a fundamental building block of feature extractions\nin recommendation systems. However, in industrial applications,\nboth their working mechanisms and explanabilities are still being\nchallenged from time to time [4, 17], and sometimes even being\noutperformed by classical machine learning methods like tree-based\nmodels [12].\nA recommendation system is completely different from the CV\nand NLP tasks. The main objectives in CV and NLP systems are\nmimicking the perceptual abilities of human beings, and recommen-\ndation systems is to understand the fundamental mechanisms in\nhuman’s decision-making behavior. Its well known that as a high-\nlevel human cognitive functionality, human behavior is to difficult\nto model due to human’s bounded rationality [7].\nIn this work, we are intended to provide a general framework to\nmodel human decision-making behaviors for CTR prediction prob-\nlems. We proposed our extended models of SAMs. We aimed at\nproviding a general framework to further extend the CTR predic-\ntion model, rather than focusing on obtaining the state-of-the-art\nperformance, and therefore, performance comparisons are not ex-\nplored comprehensively in this work. It is often unstable to always\nuse the powerful fitting ability of deep learning models to obtain\na high performance even before fully understanding the human\ndecision-making mechanism. Even if the results of state-of-the-art\nare obtained, it is blessed by the proper distribution of the dataset\nand laborious tunings of hyperparameters. Instead, we should pay\nmore attention to human behaviors. When modeling with a deep\nlearning framework, we will benefit more if we can open the black\nbox and connect the network structure and its functionalities with the\nhuman decision-making process. As a preliminary attempt towards\nthis direction, this work provides a unified framework and hopefully\nmore researches can be extended on this basis.\n7\nCONCLUSIONS\nIn this work, a general framework for CTR prediction is proposed,\nwhich corresponds to an individual decision-making process based\non neural network model. We also attempt to study whether the\nattention mechanism is critical in the CTR prediction model. It is\nfound that most CTR prediction models can be viewed as a general\nattention mechanism applied to the feature interaction. In this sense,\nthe attention mechanism is of importance for CTR prediction models.\nIn addition, we extend the existing CTR models based on our\nframework and propose three types of SAMs, in which SAM1 and\nSAM2 models are extensions of LR and FM models, respectively,\nand SAM3 corresponds to the self-attention model in Transformer\nwith original one-field embedding extended to pairwise-field em-\nbedding. According to the experimental results on the two datasets,\nalthough our extension can obtain quite competitive results, the\nSAM3 model has not demonstrated its significant advantages. We\nalso perform a more in-depth analysis of the number of SAM layers\nin the SAM3A model, and find that depth does not always lead to\nbetter performance. To a certain extent, this also shows that the CTR\nprediction problem is different from the NLP task, and the effect of\nhigh-order feature interactions cannot bring too much improvement.\nTo conclude, we have established a unified framework for CTR\nprediction and a possible direction for future work should be on the\ncombination of this framework to models that can help us to under-\nstand human decision-making behavior, i.e, agent-based model.\nACKNOWLEDGEMENTS\nWe thank the anonymous reviewers for their valuable comments.\nWe are grateful to our colleagues for their helpful discussions about\nCTR prediction problem and the self-attention mechanism.\nREFERENCES\n[1] Hila Becker, Christopher Meek, and David Maxwell Chickering. 2007. Modeling\nContextual Factors of Click Rates. In Proceedings of the 22nd National Conference\non Artificial Intelligence - Volume 2 (AAAI’07). AAAI Press, 1310–1315.\n[2] Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016.\nHigher-Order Factorization Machines (NIPS’16), Vol. 29. Curran Associates Inc.,\nRed Hook, NY, USA, 3359–3367.\n[3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan\nAnil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.\n2016. Wide & Deep Learning for Recommender Systems. In Proceedings of\nthe 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016).\nAssociation for Computing Machinery, New York, NY, USA, 7–10. https:\/\/doi.\norg\/10.1145\/2988450.2988454\n[4] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are We\nReally Making Much Progress? A Worrying Analysis of Recent Neural Recom-\nmendation Approaches. In Proceedings of the 13th ACM Conference on Recom-\nmender Systems (RecSys ’19). Association for Computing Machinery, New York,\nNY, USA, 101–109. https:\/\/doi.org\/10.1145\/3298689.3347058\n[5] Kushal S. Dave and Vasudeva Varma. 2010. Learning the Click-through Rate for\nRare\/New Ads from Similar Ads. In Proceedings of the 33rd International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR\n’10). Association for Computing Machinery, New York, NY, USA, 897–898. https:\n\/\/doi.org\/10.1145\/1835449.1835671\n[6] Jacob Devlin, Mingwei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nnorth american chapter of the association for computational linguistics (2018).\n[7] Gerd Gigerenzer and Reinhard Selten. 2000. Bounded rationality: The adaptive\ntoolbox. International Journal of Psychology 35 (2000), 203–204.\n[8] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training\ndeep feedforward neural networks. 9 (13–15 May 2010), 249–256.\nhttp:\/\/\nproceedings.mlr.press\/v9\/glorot10a.html\n[9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In\nProceedings of the 26th International Joint Conference on Artificial Intelligence\n(IJCAI’17). AAAI Press, 1725–1731.\n[10] K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep Residual Learning for Im-\nage Recognition. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). 770–778. https:\/\/doi.org\/10.1109\/CVPR.2016.90\n[11] Xiangnan He and Tat-Seng Chua. 2017.\nNeural Factorization Machines for\nSparse Predictive Analytics. In Proceedings of the 40th International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR ’17).\nAssociation for Computing Machinery, New York, NY, USA, 355–364.\nhttps:\n\/\/doi.org\/10.1145\/3077136.3080777\n[12] Dietmar Jannach, Gabriel de Souza P. Moreira, and Even Oldridge. 2020. Why\nAre Deep Learning Models Not Consistently Winning Recommender Systems\nCompetitions Yet? A Position Paper. In Proceedings of the Recommender Systems\nChallenge 2020 (RecSysChallenge ’20). Association for Computing Machinery,\nNew York, NY, USA, 44–49. https:\/\/doi.org\/10.1145\/3415959.3416001\n[13] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-Aware\nFactorization Machines for CTR Prediction. In Proceedings of the 10th ACM Con-\nference on Recommender Systems (RecSys ’16). Association for Computing Ma-\nchinery, New York, NY, USA, 43–50. https:\/\/doi.org\/10.1145\/2959100.2959134\n[14] D.P Kingma and L.J. Ba. 2015. Adam: A Method for Stochastic Optimization.\n[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature\n521, 7553 (2015), 436–444. https:\/\/doi.org\/10.1038\/nature14539\n[16] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\nGuangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit Feature\nInteractions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining. 1754–1763.\n[17] Malte Ludewig, Noemi Mauro, Sara Latifi, and Dietmar Jannach. 2019. Perfor-\nmance comparison of neural and non-neural approaches to session-based recom-\nmendation. (2019), 462–466. https:\/\/doi.org\/10.1145\/3298689.3347041\n[18] H. Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner,\nJulian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat\nChikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos,\nand Jeremy Kubica. 2013. Ad Click Prediction: A View from the Trenches. In\nProceedings of the 19th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining (KDD ’13). Association for Computing Machinery,\nNew York, NY, USA, 1222–1230. https:\/\/doi.org\/10.1145\/2487575.2488200\n[19] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, et al. 2019. Deep\nlearning recommendation model for personalization and recommendation systems.\narXiv preprint arXiv:1906.00091 (2019).\n[20] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu\nSun, and Quan Lu. 2018. Field-Weighted Factorization Machines for Click-\nThrough Rate Prediction in Display Advertising. In Proceedings of the 2018 World\nWide Web Conference (WWW ’18). International World Wide Web Conferences\nSteering Committee, Republic and Canton of Geneva, CHE, 1349–1357. https:\n\/\/doi.org\/10.1145\/3178876.3186040\n[21] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,\nZachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in PyTorch. (2017).\n[22] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun\nWang. 2016. Product-Based Neural Networks for User Response Prediction. In\n2016 IEEE 16th International Conference on Data Mining (ICDM). 1149–1154.\nhttps:\/\/doi.org\/10.1109\/ICDM.2016.0151\n[23] Steffen Rendle. 2010. Factorization Machines. In 2010 IEEE International Con-\nference on Data Mining. 995–1000. https:\/\/doi.org\/10.1109\/ICDM.2010.127\n[24] Steffen Rendle. 2012. Factorization Machines with libFM. ACM Trans. Intell.\nSyst. Technol. 3, 3 (2012), Article 57. https:\/\/doi.org\/10.1145\/2168752.2168771\n[25] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.\n2011. Fast Context-Aware Recommendations with Factorization Machines. In\nProceedings of the 34th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR ’11). Association for Computing\nMachinery, New York, NY, USA, 635–644.\nhttps:\/\/doi.org\/10.1145\/2009916.\n2010002\n[26] Steffen Rendle, Walid Krichene, Li Zhang, and John Anderson. 2020. Neural\nCollaborative Filtering vs. Matrix Factorization Revisited.\n(2020), 240–248.\nhttps:\/\/doi.org\/10.1145\/3383313.3412488\n[27] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting\nClicks: Estimating the Click-through Rate for New Ads. (2007), 521–530. https:\n\/\/doi.org\/10.1145\/1242572.1242643\n[28] Ying Shan, T. Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, and JC Mao. 2016.\nDeep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial\nFeatures. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining (KDD ’16). Association for Computing\nMachinery, New York, NY, USA, 255–262.\nhttps:\/\/doi.org\/10.1145\/2939672.\n2939704\n[29] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George\nvan den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-\nbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. 2016. Mastering the game of Go with\ndeep neural networks and tree search.\nNature 529, 7587 (2016), 484–489.\nhttps:\/\/doi.org\/10.1038\/nature16961\n[30] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming\nZhang, and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learn-\ning via Self-Attentive Neural Networks. In Proceedings of the 28th ACM In-\nternational Conference on Information and Knowledge Management (CIKM\n’19). Association for Computing Machinery, New York, NY, USA, 1161–1170.\nhttps:\/\/doi.org\/10.1145\/3357384.3357925\n[31] Kenneth E Train. 2009. Discrete choice methods with simulation. Cambridge\nuniversity press.\n[32] Kenneth E Train. 2009. Discrete Choice Methods with Simulation: Properties of\nDiscrete Choice Models. Econometric Reviews 10, 4 (2009), 54.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\nNeed. 30 (2017).\n[34] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross\nNetwork for Ad Click Predictions. In Proceedings of the ADKDD’17 (ADKDD’17).\nAssociation for Computing Machinery, New York, NY, USA, Article 12, 7 pages.\nhttps:\/\/doi.org\/10.1145\/3124749.3124754\n[35] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional Factorization Machines: Learning the Weight of Feature Inter-\nactions via Attention Networks. (2017), 3119–3125. https:\/\/doi.org\/10.24963\/\nijcai.2017\/435\n[36] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep Learning over Multi-field\nCategorical Data. In Advances in Information Retrieval. Springer International\nPublishing, Cham, 45–57.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Looking at CTR Prediction Again: Is Attention All You Need?.pdf"}
{"title":"MIO: A Foundation Model on Multimodal Tokens","authors":"Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang","summary":"In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.","url":"http:\/\/arxiv.org\/abs\/2409.17692v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2409.17692v3","published":1727344636000,"comment":"Technical Report. Codes and models are available in\n  https:\/\/github.com\/MIO-Team\/MIO","pdf_text":"Preprint.\nMIO: A FOUNDATION MODEL ON MULTIMODAL\nTOKENS\nZekun Wang1,2,3, King Zhu2,3, Chunpu Xu4, Wangchunshu Zhou5, Jiaheng Liu1,3,\nYibo Zhang1, Jiashuo Wang4, Ning Shi6, Siyu Li3, Yizhi Li3,8, Haoran Que1,\nZhaoxiang Zhang9, Yuanxing Zhang10, Ge Zhang3,7, Ke Xu1, Jie Fu11*, Wenhao Huang2,3∗\n1Beihang University; 201.AI; 3 M-A-P\n4The Hong Kong Polytechnic University; 5AIWaves; 6University of Alberta; 7University of Waterloo;\n8University of Manchester; 9Institute of Automation, Chinese Academy of Sciences\n10Peking University; 11The Hong Kong University of Science and Technology;\nzenmoore@buaa.edu.cn\nABSTRACT\nIn this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and videos\nin an end-to-end, autoregressive manner. While the emergence of large language\nmodels (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile capabilities,\nthey still lack true any-to-any understanding and generation. Recently, the release of\nGPT-4o has showcased the remarkable potential of any-to-any LLMs for complex\nreal-world tasks, enabling omnidirectional input and output across images, speech,\nand text. However, it is closed-source and does not support the generation of\nmultimodal interleaved sequences. To address this gap, we present MIO, which\nis trained on a mixture of discrete tokens across four modalities using causal\nmultimodal modeling. MIO undergoes a four-stage training process: (1) alignment\npre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and\n(4) comprehensive supervised fine-tuning on diverse textual, visual, and speech\ntasks. Our experimental results indicate that MIO exhibits competitive, and in\nsome cases superior, performance compared to previous dual-modal baselines,\nany-to-any model baselines, and even modality-specific baselines. Moreover,\nMIO demonstrates advanced capabilities inherent to its any-to-any feature, such\nas interleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc. Codes and models are\navailable at https:\/\/github.com\/MIO-Team\/MIO.\n1\nINTRODUCTION\nThe advent of Large Language Models (LLMs) is commonly considered the dawn of artificial general\nintelligence (AGI) (OpenAI et al., 2023; Bubeck et al., 2023), given their generalist capabilities such\nas complex reasoning (Wei et al., 2022), role playing (Wang et al., 2023c), and creative writing (Wang\net al., 2024a). However, original LLMs lack multimodal understanding capabilities. Consequently,\nnumerous multimodal LLMs (MM-LLMs) have been proposed, allowing LLMs to understand\nimages (Li et al., 2023b; Alayrac et al., 2022), audio (Borsos et al., 2023; Rubenstein et al., 2023;\nTang et al., 2023; Das et al., 2024), and other modalities (Lyu et al., 2023; Zhang et al., 2023d;\nMoon et al., 2023). These MM-LLMs typically involve an external multimodal encoder, such as\nEVA-CLIP (Sun et al., 2023b) or CLAP (Elizalde et al., 2022), with an alignment module such as\nQ-Former (Li et al., 2023b) or MLP (Liu et al., 2023b) for multimodal understanding. These modules\nalign non-textual-modality data features into the embedding space of the LLM backbone.\nAnother line of work involves building any-to-any and end-to-end MM-LLMs that can input and\noutput non-textual modality data. Typically, there are four approaches: (1) Discrete-In-Discrete-Out\n∗Corresponding Authors.\n1\narXiv:2409.17692v3  [cs.CL]  13 Jan 2025\nPreprint.\nTable 1: The comparison between previous models and MIO (ours). I\/O Consistency indicates\nwhether the model ensures that the input and output representations for the same data remain\nconsistent. Uni. Bi. SFT refers to whether the model undergoes a unified (Uni.) supervised fine-\ntuning (SFT) for both multimodal understanding and generation (Bi.=Bidirectional). Multi-Task\nSFT assesses whether the model undergoes a comprehensive SFT that includes diverse tasks, with at\nleast visual question answering tasks. MM. Inter. Output evaluates whether the model supports\nthe generation of multimodal interleaved (MM. Inter.) sequences. We refer readers to §1 for the\ndefinitions of the different modeling approaches.\nModels\nEmu1\n(Sun et al.,\n2023c)\nEmu2\n(Sun et al.,\n2023a)\nSEED-\nLLaMA\n(Ge et al.,\n2023b)\nAnyGPT\n(Zhan et al.,\n2024)\nCM3Leon\n(Yu et al., 2023),\nChameleon\n(Team, 2024)\nGemini\n(Reid et al.,\n2024)\nTransfusion\n(Zhou et al.,\n2024)\nMIO\n(ours)\nI\/O Consistency\n✗\n✓\n✓\n✓\n✓\n✗\n✗\n✓\nUni. Bi. SFT\n✗\n✗\n✓\n✓\n✓\n✓\n✗\n✓\nMulti-Task SFT\n✓\n✓\n✓\n✗\n✓\n✓\n✗\n✓\nSpeech I\/O\n✗\/✗\n✗\/✗\n✗\/✗\n✓\/✓\n✗\/✗\n✓\/✗\n✗\n✓\/✓\nVideo I\/O\n✓\/✓\n✓\/✓\n✓\/✓\n✗\/✗\n✗\/✗\n✓\/✗\n✗\n✓\/✓\nVoice Output\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\nMM. Inter. Output\n✗\n✗\n✓\n✗\n✗\n✗\n✗\n✓\nModeling\nCICO\nCICO\nDIDO\nDIDO\nDIDO\nCIDO\nAR+Diff\nDIDO\n(DIDO): Non-textual modality data is discretized using vector quantization techniques (van den Oord\net al., 2017; Esser et al., 2020) and then fed into LLMs (Ge et al., 2023b; Zhan et al., 2024; Liu\net al., 2024). (2) Continuous-In-Discrete-Out (CIDO): The LLM backbones intake densely encoded\nnon-textual modality data features and generate their quantized representations (Diao et al., 2023;\nTeam et al., 2023). (3) Continuous-In-Continuous-Out (CICO): The LLMs both understand and\ngenerate non-textual modality data in their densely encoded representations (Sun et al., 2023c;a;\nDong et al., 2023; Zheng et al., 2023; Wu et al., 2023). (4) Autoregression + Diffusion (AR + Diff):\nThe autoregressive and diffusion modeling are integrated in a unified LLM (Zhou et al., 2024; Xie\net al., 2024; Li et al., 2024b). Although these works have succeeded in building MM-LLMs unifying\nunderstanding and generation, they exhibit some drawbacks, as illustrated in Table 1. For example,\nEmu1 (Sun et al., 2023c) and Emu2 (Sun et al., 2023a) explore the autoregressive modeling of\nthree modalities: text, images, and videos. SEED-LLaMA (Ge et al., 2023b) proposes a new image\nquantizer aligned with LLMs’ embedding space and trains the MM-LLMs on images and videos.\nHowever, neither considers the speech modality, which is heterogeneous from visual modalities\nlike videos and images. Although AnyGPT (Zhan et al., 2024) has explored settings involving four\nmodalities, including text, image, speech, and music, it lacks video-related abilities, voice synthesis,\nand comprehensive multi-task supervised fine-tuning, leading to limited multimodal instruction-\nfollowing and reasoning capabilities. Furthermore, AR + Diff approaches, such as Transfusion (Zhou\net al., 2024), suffer from limited multimodal understanding capabilities because the multimodal inputs\nare noised for denoising modeling, and the image tokenizer used (i.e., VAE (Kingma & Welling,\n2013)) is suitable for image generation rather than image understanding.\nMoreover, most of current MM-LLMs are typically dual-modal, combining text with another modality,\nsuch as images. Although previous works, such as Meta-Transformer (Zhang et al., 2023d) and\nUnified-IO 2 (Lu et al., 2023), have explored omni-multimodal understanding settings with more than\ntwo non-textual modalities, they still lag significantly behind their dual-modal counterparts, especially\nin terms of multimodal instruction-following capabilities. Moreover, these MM-LLMs are typically\nfocused on understanding only, neglecting the important aspect of multimodal generation. Several\nworks have enabled LLMs to call external tools to address this issue. For example, HuggingGPT (Shen\net al., 2023) generates textual image descriptions for external diffusion models to synthesize images.\nGPT-4 (OpenAI et al., 2023) can utilize either an image generator like DALL-E 3 (Betker et al., 2024)\nor a text-to-speech (TTS) tool like Whisper (Radford et al., 2022) to support multimodal generation.1\nHowever, these methods are not end-to-end, relying on the text modality as an interface.\nRecently, the release of GPT-4o has demonstrated the capabilities of any-to-any and end-to-end\nfoundation models.2 It is the first foundational model to accept multimodal tokens as inputs and\ngenerate multimodal tokens within a unified model while also demonstrating strong abilities in\n1https:\/\/openai.com\/index\/chatgpt-can-now-see-hear-and-speak\/\n2https:\/\/openai.com\/index\/hello-gpt-4o\/\n2\nPreprint.\nMIO\nAre you happy, cat?\n🔊Yes, I'm happy.\n🔊Tell me a mystery story about\nOn a dark night,\nA detective ...\nin an illustrated way.\n🔊Yes, I'm happy.\nSpeechTokenizer\nSEED-Tokenizer\nSEED-Tokenizer\nSpeechTokenizer\n🔊Yes, I'm happy.\nTraining Recipe\nMasked Packing\nStage I: Alignment Pre-Training\nStage II: Interleaved Pre-Training\nStage III: Speech-Enhanced Pre-Training\nExpanded Vocabulary\nSupervised Fine-Tuning\nFigure 1: The framework of MIO and its training recipe.\ncomplex multimodal instruction-following, reasoning, planning, and other generalist capabilities.\nFurthermore, as the continuous scaling up of LLMs in the community depletes high-quality language\ntokens, GPT-4o verifies a new source of data for LLM training: multimodal tokens. This approach\nsuggests that the next generation AGI could derive more knowledge from multimodal tokens when\nlanguage tokens are exhausted. However, GPT-4o is closed source and focuses primarily on end-to-\nend support for speech I\/O, image I\/O, 3D generation, and video understanding. Its recent open-source\n“alternatives”, such as VITA (Fu et al., 2024), still lack the ability to generate data of all supported\nmodalities, particularly for the generation of multimodal interleaved sequences.\nTo address the aforementioned issues, we introduce MIO (Multimodal Input and Output, or\nMultimodal Interleaved Output), the first open-source any-to-any foundation model that unifies\nmultimodal understanding and generation across four modalities–text, image, speech (with voice),\nand video, while enabling the generation of multimodal interleaved sequences. Specifically, MIO is\nbuilt on discrete multimodal tokens that capture both semantic representations through contrastive\nloss and low-level features via reconstruction loss (Ge et al., 2023a; Zhang et al., 2023b) from raw\nmultimodal data. Due to the consistent data format shared with textual corpora, the model can treat\nnon-textual modalities as “foreign languages”, allowing it to be trained with the next-token-prediction.\nNote that since the representation of an image remains the same whether it is used as an input or an\noutput, our model flexibly supports multimodal interleaved sequence generation, where an image\nfunctions simultaneously for both understanding and generation. Moreover, we employ three-stage\npre-training with an additional SFT stage to effectively train the model for modality scaling.\nOur experimental results show that MIO, trained on a mixture of four modalities, demonstrates com-\npetitive performance compared to its dual-modal counterparts and previous any-to-any multimodal\nlanguage model baselines. Additionally, MIO is the first model to demonstrate interleaved video-text\ngeneration, chain-of-visual-thought reasoning, and other emergent abilities relying on any-to-any and\nmultimodal interleaved output features (c.f.,§3.5).\n2\nMETHOD\nFirstly, we elaborate on our modeling approach, which supports multimodal token input and output,\nas well as causal language modeling (CausalLM), in §2.1. Secondly, we describe our three-stage pre-\ntraining procedures in §2.2. Thirdly, we provide details of our comprehensive supervised fine-tuning\non diverse multimodal understanding and generation tasks in §2.3.\n2.1\nMODELING\nAs illustrated in Figure 1, the framework of MIO involves three parts: (1) multimodal tokenization,\n(2) causal multimodal modeling, and (3) multimodal de-tokenization.\n3\nPreprint.\nMultimodal Tokenization.\nIn our work, we use SEED-Tokenizer (Ge et al., 2023a) as our image\ntokenizer and SpeechTokenizer (Zhang et al., 2023b) as our speech tokenizer. SEED-Tokenizer\nencodes images using a ViT (Dosovitskiy et al., 2021) derived from BLIP-2 (Li et al., 2023b), and\nthen converts the encoded features into fewer tokens with causal semantics via Q-Former (Li et al.,\n2023b). These features are subsequently quantized into discrete tokens that are well-aligned with\nthe language model backbone’s textual space. The codebook size for these discrete image tokens is\n8192. SEED-Tokenizer transforms each image into a 224x224 resolution and quantizes it into 32\ntokens. We use two special tokens, <IMAGE> and <\/IMAGE>, to indicate the start and end of the\nimage tokens per image, respectively.\nAs for videos, we first apply specific frame-cutting methods to convert videos into image sequences.\nIn our training data processing procedures, the number of frames for each video is dynamically\ndetermined by its duration, the length of its context, or its scene switching3 to (1) avoid exceeding\nthe LLM backbone’s context window limit, and (2) capture complete but concise information of the\nvideo. Each frame is then tokenized in the same manner as an image.\nIn terms of speech, SpeechTokenizer (Zhang et al., 2023b) leverages an 8-layer RVQ (Lee et al.,\n2022) to tokenize speech into tokens with 8 codebooks, with each codebook derived from one layer.\nSince the first layer’s quantization output is distilled from HuBERT (Hsu et al., 2021), which encodes\nmore semantic information, SpeechTokenizer can separate content tokens and timbre tokens from a\nquantized speech. The first-layer quantization is treated as content quantization, while the remaining\nlayers’ quantization is treated as timbre quantization. SpeechTokenizer encodes speech into 50 tokens\nper second for each codebook, resulting in 400 tokens per second with all eight codebooks. To\nimprove context efficiency, we drop the last four layers’ codebooks and only use the content codebook\nand the first three timbre codebooks. Our vocabulary size for the speech modality is 1024×4 = 4096.\nSince the open-source pretraining-level speech data is collected from individuals with diverse voices,\nthe timbre tokens exhibit a relatively random and noisy pattern, while the content tokens are more\nfixed-pattern and better aligned with the corresponding transcriptions. Given these priors in speech\ntokens, it is important to choose the proper interleaving mode of speech tokens (Copet et al., 2023).\nWe denote the four codebooks as A, B, C, and D, where A is the codebook for content tokens and the\nremaining three are for timbre tokens. For simplicity, assuming that we have only two tokens for each\ncodebook in a tokenized speech sequence (i.e., a1a2, b1b2, c1c2, and d1d2), there are two interleaving\npatterns for causal multimodal modeling: (1) sequential interleaving pattern: a1a2b1b2c1c2d1d2 and\n(2) alternating interleaving pattern: a1b1c1d1a2b2c2d2.\nIn our preliminary experiments, we observed that text-to-speech generation (TTS) training is difficult\nto converge when using the alternating interleaving pattern because the noisy and random timbre\ntokens (b1c1d1) tend to mislead the continuations. Moreover, the speech-to-text understanding (ASR)\nperformance improves much more slowly during training with the alternating interleaving pattern due\nto the sparsity of semantic information in the timbre tokens. As a result, we drop the timbre tokens\nfor speech understanding and use the sequential interleaving pattern for speech generation. We use\n<SPCH> and <\/SPCH> as special tokens to indicate the start and end of the speech token sequence.\nCausal Multimodal Modeling.\nAs illustrated in Figure 1, the speech and images, including video\nframes, are tokenized by SpeechTokenizer (Zhang et al., 2023b) and SEED-Tokenizer (Ge et al.,\n2023a), respectively. We add the 4096 speech tokens and 8192 image tokens to the LLM’s vocabulary.\nIn addition, we introduce four new special tokens, namely <IMAGE>, <\/IMAGE>, <SPCH>, and\n<\/SPCH>, to the vocabulary. Consequently, the embedding layer of the LLM backbone and the\nlanguage modeling head are extended by 4096 + 8192 + 4 = 12292 to support the embedding and\ngeneration of these new tokens. The image tokens contain causal semantics due to the use of a Causal\nQ-Former (Ge et al., 2023a), and the speech tokens are intrinsically causal due to their temporal\nnature. Therefore, these multimodal tokens are as suitable for autoregressive training as textual\ntokens, allowing us to unify the training objectives for understanding and generation of multimodal\ntokens into next-token-prediction with cross-entropy loss. The training objective is thus:\nL = −\nT\nX\nt=1\nlog P(xt | x<t; θ)\n(1)\n3https:\/\/github.com\/Breakthrough\/PySceneDetect\n4\nPreprint.\nwhere xt represents the discrete multimodal tokens, and θ denotes the parameters of the LLM\nbackbone. We use the pre-trained model, Yi-6B-Base (AI et al., 2024), for initialization.\nFurthermore, to eliminate the computational inefficiency caused by <PAD> tokens, we use the masked\npacking strategy (Lu et al., 2023; Liu et al., 2024; Dehghani et al., 2023). Specifically, the samples\nare concatenated along the sequence length dimension until the context window is full. Then, we\nconstruct the causal attention mask for the tokens of each sample and mask out all the tokens of the\nother samples.\nMultimodal De-Tokenization.\nAfter the generation of multimodal tokens, it is essential to use\nmodality-specific decoders to reconstruct the images or speech from the codes. Specifically, for\nimage tokens, we directly utilize SEED-Tokenizer’s decoder, which involves an MLP projection\nto convert the discrete codes into dense latents. These latents condition an off-the-shelf diffusion\nmodel (Rombach et al., 2022) to generate the images in the pixel space (Ge et al., 2023a). The\nvanilla SpeechTokenizer (Zhang et al., 2023b) involves generating timbre tokens through a non-\nautoregressive model outside the language model, and then feeding the concatenated content and\ntimbre tokens into the SpeechTokenizer decoder to synthesize speech. In our work, to inject the\ntimbre priors into the multimodal language model itself, the timbre tokens are also generated by the\nautoregressive language model.\n2.2\nPRE-TRAINING\nAs shown in Table 2, we use a three-stage strategy for pre-training, with each stage targeting different\nobjectives. The three stages are: (1) Alignment Pre-training: This stage focuses on learning a\nmultimodal representation more aligned with the language space. (2) Interleaved Pre-training: This\nstage aims to obtain a multimodal representation with richer contextual semantics. (3) Speech-\nenhanced Pre-training: This stage specifically enhances the model’s speech-related capabilities, while\nconcurrently replaying data from other modalities. For more details on the pre-training data and its\nprocessing procedures, we refer the readers to Appendix A.\nTable 2: Pre-training stages and their details. We use “Inter” to denote “Interleaved” for short. We\nprovide batch sizes for each data type per GPU in image-text pair data:language-only data:(image-text\ninterleaved data + video data):speech-text pair data. See Appendix A and Appendix B for more\ndetails including pre-training data sources, data cleaning procedures, pre-training hyperparameters,\netc.\nPre-training Stage\nStage I\nStage II\nStage III\nObjective\nMultimodal Alignment\nMultimodal Interleaving\nSpeech Enhancement\nImage-Text Pair\nSBU, CC3M,\nLAION-COCO,\nJourneyDB\nSBU, CC3M,\nLAION-COCO,\nJourneyDB\nCC3M\nLAION-COCO\nLanguage-Only\nRefinedWeb\nRefinedWeb\nRefinedWeb\nImage-Text Inter\n-\nOBELICS,\nMMC4-core-ff\nMMC4-core-ff\nVideo-Text Pair\n-\nWebVid-10M\nWebVid-10M\nVideo-Text Inter\n-\nHowTo-100M,\nYT-Temporal-180M\nHowTo-100M,\nYT-Temporal-180M\nSpeech-Text Pair\nLibriheavy\nLibriheavy\nLibriheavy\nGPUs\n128 A800-80GB\n128 A800-80GB\n8 A800-80GB\nTraining Steps\n24,800\n12,800\n32,200\nBatch Size\n12:2:0:2\n2:2:6:6\n2:1:1:12\nStage I: Alignment Pre-Training.\nTo fully leverage the superior capabilities of the pre-trained\nLLM backbone, it is essential to align the non-textual modality data representations with text. There\nare two types of pre-training data for image-text multimodal learning: (1) Image-text paired data: This\ndata has well-aligned dependencies between images and text. (2) Image-text interleaved data: This\ndata features more natural and contextual dependencies but is less aligned. Note that in our setting,\n5\nPreprint.\nvideo-text paired and interleaved data can be treated as image-text interleaved data, with videos\nbeing sequential images interleaved with text. Therefore, in this stage, we exclude the image-text\ninterleaved data and video data to ensure the most aligned pattern between images and text.\nStage II: Interleaved Pre-Training.\nIn this stage, we extend the data used for pre-training to\ninclude image-text interleaved data (including video-text data) as a novel image-text dependency\npattern. The image-text interleaving pattern has a different nature compared to pairing patterns.\nAlthough Li et al. (2023b) and Sun et al. (2023c) argued that interleaved image-text data mainly\nserves for multimodal in-context learning, we argue that it is also essential for context-aware image\ngeneration where images are generated based on specific context, rather than a precise description of\nthe image content. For example, in image-text interleaved data, the text might serve as the image’s\npreceding or continuing context, rather than its description. This pattern significantly differs from\nthe previous descriptive image generation demonstrated in image-text paired data, where images are\ngenerated based on precise and detailed text that clearly describe the content of the images (Team\net al., 2023). Therefore, context-aware image generation is essential for tasks such as chain-of-visual-\nthought reasoning or visual storytelling (Team et al., 2023; Huang et al., 2016), where images are\ngenerated without textual descriptions. Due to the lack of benchmarks and evaluation metrics for\ncontext-aware image generation, we provide some demonstrations in §3.5 to showcase the potential\nof our model in visual storytelling, interleaved video-text generation, instructional image editing,\nchain-of-visual-thought reasoning, multimodal in-context learning, etc.\nMoreover, in this stage, due to the extensive training on image-text paired data in Stage I, we can\nreduce its mixing ratio to the minimal essential scale for replay to avoid catastrophic forgetting. This\nallows us to increase the batch size for image-text interleaved data, video data, and speech data.\nStage III: Speech-Enhanced Pre-Training.\nThe speech tokenizer that we use generates 200 tokens\nfor each second of audio. Given that the duration of a speech sample can be 15 seconds, this results\nin around 3,000 tokens per sample. In comparison, the image tokenizer produces only 32 tokens\nper image. This creates a significant disparity in the number of tokens among different modalities.\nConsequently, our training data is dominated by speech tokens. If we mix all the different modalities\naccording to their original proportions for training, the model would likely become overly focused on\nspeech, at the expense of other modalities.\nTo address this issue, we implement a three-stage strategy that gradually increases the proportion of\nspeech tokens. In Stage I, speech-text data accounts for 12.5% of the training tokens, which rises to\n37.5% in Stage II, and finally reaches 75.0% in Stage III. This incremental increase in the proportion\nof speech tokens ensures that the model’s performance in non-speech modalities is not compromised\nby the speech modality, while also allowing for the optimization of the model’s speech capabilities.\nFurthermore, we keep the data mixing ratio for other modalities of pre-training data at the minimal\nessential scales for replay, and we only use the high-quality subsets of them in this stage. This stage\nrequires significantly fewer compute resources, due to the foundation laid in the previous stages.\nWe refer the reader to Appendix B for more details about the hyperparameters and prompt templates.\n2.3\nSUPERVISED FINE-TUNING\nAs shown in Table 9, our model undergoes comprehensive and systematic supervised fine-tuning\n(SFT) with 16 different tasks and 34 diverse open-source datasets. The chat template used for SFT is\nthe same as that used for Yi-6B-Chat (AI et al., 2024), and only the assistant responses are supervised.\nWe refer the reader to Appendix C for more details about the hyperparameters and prompt templates.\n3\nEXPERIMENTS\nIn this section, we present our quantitative evaluation results across various domains: image-related\ntasks (§3.1), speech-related tasks (§3.2), and video-related tasks (§3.3). Due to the lack of benchmarks\nfor several advanced and emergent abilities of any-to-any multimodal LLMs, we also provide\nnumerous qualitative demonstrations (§3.5) to demonstrate these capabilities. We refer the reader to\nAppendix D for more details, including the decoding hyperparameters and prompt templates.\n6\nPreprint.\n3.1\nIMAGE-RELATED TASKS\nTable 3: Experimental results for image understanding abilities. “Imagen” denotes whether the model\nis capable of generating images. “Speech” denotes whether the model supports speech modality. “I”\ndenotes the instruction tuned version. The metrics used are CIDEr for COCO, MCQ accuracy for the\nSEED Bench, and VQA accuracy for the other tasks, following the standard procedures. In all cases,\nhigher scores indicate better performance.\nModels\nImagen\nSpeech\nCOCO\nVQAv2\nOKVQA\nVizWiz\nSEED Bench\nEmu-Base (14B)\n✓\n✗\n112.4\n52.0\n38.2\n34.2\n47.3\nEmu-I (14B)\n✗\n✗\n120.4\n57.2\n43.4\n32.2\n58.0\nSEED-LLaMA-I (8B)\n✓\n✗\n124.5\n66.2\n45.9\n55.1\n51.5\nAnyGPT (8B)\n✓\n✓\n107.5\n-\n-\n-\n-\nFlamingo (9B)\n✗\n✗\n79.4\n51.8\n44.7\n28.8\n42.7\nFlamingo (80B)\n✗\n✗\n84.3\n56.3\n31.6\n-\nKosmos-1 (1.6B)\n✗\n✗\n84.7\n51.0\n-\n29.2\n-\nMetaLM (1.7B)\n✗\n✗\n82.2\n41.1\n11.4\n-\n-\nIDEFICS-I (80B)\n✗\n✗\n117.2\n37.4\n36.9\n26.2\n53.2\nCM3Leon (7B)\n✓\n✗\n61.6\n47.6\n23.8\n37.6\n-\nInstructBLIP (8.1B)\n✗\n✗\n-\n-\n-\n34.5\n58.8\nQwen-VL-Chat (13B)\n✗\n✗\n-\n78.2\n56.6\n38.9\n58.2\nLLaVA 1.5 (7B)\n✗\n✗\n-\n78.5\n-\n50.0\n58.6\nMIO-Instruct (7B)\n✓\n✓\n120.4\n65.5\n39.9\n53.5\n54.4\nImage Understanding.\nWe compare our models with Emu (Sun et al., 2023c), SEED-LLaMA (Ge\net al., 2023b), AnyGPT (Zhan et al., 2024), Flamingo (Alayrac et al., 2022), Kosmos-1 (Huang\net al., 2023), MetaLM (Hao et al., 2022), IDEFICS (Laurençon et al., 2023), CM3Leon (Yu et al.,\n2023), InstructBLIP (Dai et al., 2023), Qwen-VL-Chat (Bai et al., 2023), and LLaVA 1.5 (Liu\net al., 2023a). We evaluate our models in diverse tasks, including: (1) image captioning on MS-\nCOCO (Lin et al., 2014) Karpathy test split with CIDEr score (Vedantam et al., 2014) as the metric,\n(2) three visual question-answering benchmarks, i.e., VQAv2 (Goyal et al., 2016) (test-dev split),\nOK-VQA (Marino et al., 2019) (val split), and VizWiz (Gurari et al., 2018), with VQA accuracy\nas the metric, and (3) SEED-Bench (Li et al., 2023a), a comprehensive visual question-answering\nbenchmark including 9 dimensions with MCQ accuracy as the metric. The scores for all baselines are\ncopied from their reports. As shown in Table 3, our MIO-Instruct is ranked in the top group among all\nbaselines, demonstrating its competitive image understanding performance. Although SEED-LLaMA\nachieved better scores compared to our model, we additionally support the speech modality. It is also\nnoteworthy that MIO, with a size of approximately 7 billion parameters, outperforms several larger\nmodels such as Emu-14B and even IDEFICS-80B.\nTable 4:\nImage generation evaluation by\nCLIP-I score.\n“I” denotes the instruction\ntuned version. Higher values are better.\nModels\nMS-COCO\nFlickr30K\nEmu-Base\n66.46\n64.82\nSEED-LLaMA\n69.07\n65.54\nSEED-LLaMA-I\n70.68\n66.55\nGILL\n67.45\n65.16\nAnyGPT\n65.00\n-\nMIO-Base\n64.15\n62.71\nMIO-Instruct\n67.76\n68.97\nImage Generation.\nWe compare our models with\nEmu (Sun et al., 2023c), SEED-LLaMA (Ge et al.,\n2023b), GILL (Koh et al., 2023), and AnyGPT (Zhan\net al., 2024) for image generation.\nWe use two\nbenchmarks, i.e., MS-COCO (Lin et al., 2014) Karpa-\nthy test split and Flickr30K (Plummer et al., 2015).\nFollowing GILL (Koh et al., 2023) and SEED-\nLLaMA (Ge et al., 2023b), we use CLIP-I as the\nmetric that evaluates the similarity between the gen-\nerated images and the ground-truth images with the\nimage encoder in CLIP (Radford et al., 2021). As\nshown in Table 4 and Table 12 the pre-trained model\nand instruction-tuned model of MIO both have com-\npetitive image generation capabilities. Note that beyond single image generation abilities, our\nmodel can also exhibit multi-image generation capabilities such as generating visual stories, image\nsequences, and even visual thoughts as illustrated in §3.5.\n7\nPreprint.\n3.2\nSPEECH-RELATED TASKS\nWe evaluate the speech understanding and generation abilities of MIO on ASR and TTS tasks.\nWav2vec 2.0 (Baevski et al., 2020), Whisper Large V2 (Radford et al., 2023), and AnyGPT (Zhan\net al., 2024) are the baselines for ASR tasks, while VALL-E (Wang et al., 2023a), USLM (Zhang\net al., 2023b) , and AnyGPT (Zhan et al., 2024) are the baselines for TTS tasks. The test set used for\nASR evaluation is LibriSpeech (Panayotov et al., 2015), while the test set used for TTS evaluation is\nVCTK (Veaux et al., 2017) following AnyGPT (Zhan et al., 2024)’s practice. The Whisper medium\nmodel is used to transcribe the speech generated for the TTS task. The WER (word error rate) is\ncomputed by comparing the generated transcribed text with the ground-truth transcription after text\nnormalization4.\nTable 5: Speech ability evaluation. “WER” denotes\nword error rate. Lower values are better.\nModels\nASR\nWER\nModels\nTTS\nWER\nWav2vec\n2.7\nVALL-E\n7.9\nWhisper\n2.7\nUSLM\n6.5\nAnyGPT\n8.5\nAnyGPT\n8.5\nMIO-Base\n6.3\nMIO-Base\n12.0\nMIO-Instruct\n10.3\nMIO-Instruct\n4.2\nAs shown in Table 3.2, our models exhibit\nspeech performance comparable to the speech-\nspecific baselines and outperform the AnyGPT\nbaseline. It is important to note that although\nAnyGPT is capable of generating content to-\nkens for speech, it lacks the ability to generate\ntimbre tokens, which necessitates the use of an\nadditional voice cloning model. In contrast, our\nmodels generate both content and timbre tokens,\nmaking the TTS tasks more challenging for our models compared to AnyGPT. Nonetheless, after\ninstruction tuning, our model still achieves better TTS performance. More evaluations of the TTS\nand Speech-to-Speech generation performance are provided in Appendix E.3 and E.2.\n3.3\nVIDEO-RELATED TASKS\nTable 6: Video understanding evaluation using top-\n1 accuracy for both benchmarks. “I” denotes the\ninstruction-tuned version.\nModels\nMSVDQA MSRVTT-QA\nFlamingo (9B)\n30.2\n13.7\nBLIP-2 (4.1B)\n33.7\n16.2\nInstructBLIP (8.1B)\n41.8\n22.1\nEmu-Instruct (14B)\n32.4\n14.0\nSEED-LLaMA-I (8B)\n40.9\n30.8\nMIO-Instruct\n42.6\n35.5\nWe compare MIO with Flamingo (Alayrac\net al., 2022), BLIP-2 (Li et al., 2023b), In-\nstructBLIP (Dai et al., 2023), Emu (Sun et al.,\n2023c), and SEED-LLaMA (Ge et al., 2023b)\nfor video understanding. The models are evalu-\nated on the MSVDQA (Chen & Dolan, 2011a)\nand MSRVTT-QA (Xu et al., 2017). The results\nare presented in Table 6. Our model achieves the\nhighest scores compared to all baselines. Due to\nthe lack of video (frame sequence) generation\nbenchmarks in our setting, we provide video\ngeneration examples in §3.5. These results demonstrate the superior performance of our models in\nboth video understanding and video generation.\nTable 7: Language-only evaluation. “I” denotes\nthe instruction-tuned version.\nModels\nMMLU\nLLAMA-1-7B-Base\n33.0\nLLAMA-2-7B-Chat\n47.9\nSEED-LLAMA-8B-I\n36.1\nAnyGPT-Base\n26.4\nAnyGPT-Chat\n27.4\nMIO-Instruct\n45.7\nTable 8: Results for trimodal comprehension\n(text, image, and speech).\nModels\nOmniBench\nGemini-1.5-Pro\n42.67\nReka-Core-20240501\n31.52\nAnyGPT (8B)\n17.77\nvideo-SALMONN (13B)\n34.11\nUnified-IO 2 (6.8B)\n34.24\nMIO-Instruct (7B)\n36.96\n4https:\/\/github.com\/openai\/whisper\/blob\/main\/whisper\/normalizers\/\nenglish.py\n8\nPreprint.\n3.4\nLANGUAGE-ONLY TASKS\nWe evaluate our models on MMLU (Hendrycks et al., 2021). The baselines are two LLaMA\nvariants (Touvron et al., 2023a;b), the instruction-tuned SEED-LLaMA (Ge et al., 2023b), and\nAnyGPT (Zhan et al., 2024). For the MMLU benchmark, we conduct zero-shot evaluation experiments\nusing the official evaluation code. The experimental results are shown in Table 7. We can observe\nthat our models have superior language-only performance compared with all any-to-any MM-LLM\nbaselines and even surpass LLaMA-1-7B-Base, an advanced pure language model.\n3.5\nDEMONSTRATIONS\nWe illustrate the basic and advanced abilities of MIO in Figure 5 and 4. The basic abilities of MIO\ninvolve image understanding and generation, video understanding and generation, ASR, and TTS.\nThe advanced abilities of MIO are based on its any-to-any and multimodal interleaved sequence\ngeneration features. These abilities involve visual storytelling (i.e., interleaved video-text generation),\nchain of visual thought, speech-in-speech-out, instructional image editing, visual guideline generation,\netc. We refer the readers to Appendix E.5 for more demonstrations including multimodal chain of\nthought and multimodal in-context learning.\n3.6\nABLATION STUDIES\nGenerality for Trimodal Understanding.\nWe evaluate our model using the OmniBench (Li et al.,\n2024d), which incorporates text, image, and speech modalities as inputs, requiring the model to\nchoose one of four options as the correct answer to determine accuracy. Although MIO acquires its\nmultimodal understanding capabilities through dual-modal training, the evaluation results in Table 8\nindicate that MIO also exhibits superior trimodal comprehension abilities.\nEffect of Different Image Tokenizers.\nThe image tokenizer has a significant impact on image\nmodality alignment. In Figure 2, we compare the image generation performance under a controlled\nsetting after training for solely 3K steps in stage 1, using various image tokenizers. The image\ntokenizers used for comparison include a VQGAN (Esser et al., 2020) with a vocabulary size of 1024\nand a compression rate of 16 (VQGAN-1024), as well as the VQGAN-Gumbel with a vocabulary\nsize of 8192 (VQGAN-8192)5. Our results indicate that the SEED-Tokenizer, which captures more\nsemantic and higher-level image information, exhibits faster convergence. In contrast, both VQGAN\ntokenizers show slower convergence due to their lower-level image information.\n4\nRELATED WORKS\n4.1\nMULTIMODAL LLMS\nVQGAN-1024\nVQGAN-8192\nSEED-Tokenizer\nAn eagle flying away after eating fish\nin a eagle-feeding session in an island near Langkawi.\nHundreds of people gathered around looking at motorcycles.\nFigure 2: Comparing different image tokenizers\nfor image generation within a controlled setting\n(limited to 3K training steps).\nWith the rapid success of Large Language Mod-\nels (LLMs), current multimodal LLMs (MM-\nLLMs) are typically built on a pre-trained LLM\nbackbone and are endowed with the ability to\nunderstand multiple modalities (Li et al., 2019;\nLu et al., 2019; Kim et al., 2021; Zeng et al.,\n2022; Zhou et al., 2022; Wang et al., 2023b;\n2024e). Generally, these MM-LLMs align the\nrepresentations of images obtained from visual\nencoders with the text embedding space, thereby\nleveraging the powerful capabilities of the foun-\ndational models. For example, BLIP-2 (Li et al.,\n2023b) uses CLIP-ViT (Radford et al., 2021) to\nextract high-level features from images and then\nemploys a Q-Former to compress the number\nof image tokens and further align image tokens\n5https:\/\/github.com\/CompVis\/taming-transformers\n9\nPreprint.\nwith the LLM embeddings. In contrast, LLaVA (Liu et al., 2023b; Li et al., 2024a) utilizes a simple\nlinear projection or MLP as the connector between the image encoder and the LLM backbone. These\nmodels demonstrate strong multimodal understanding abilities, achieving significant progress in tasks\nsuch as visual question answering, visual commonsense reasoning, chart understanding, etc.\nAdditionally, beyond images, other MM-LLMs have also focused on modalities such as speech\nand video. For instance, LLaSM (Shu et al., 2023) and InternVideo (Wang et al., 2022; 2024c) are\nMM-LLMs designed for speech and video understanding, respectively. These models adopt a similar\narchitectural design to BLIP-2 or LLaVA but redesign modality-specific encoders.\nRecently, increasing attention has been paid to unifying multiple modalities within a single MM-LLM.\nFor example, ImageBind (Girdhar et al., 2023) develops encoders suited for multiple modalities such\nas images, videos, audio, heat maps, among others, while OmniBind (Wang et al., 2024d) trains an\nomni-representation model by aligning encoders across four modalities: audio, language, images,\nand 3D objects. OmniBench (Li et al., 2024d) is proposed to evaluate the models’ abilities for visual,\nacoustic, and textual understanding.\nHowever, these models focus primarily on multimodal understanding and often overlook the important\naspect of multimodal generation.\n4.2\nANY-TO-ANY MM-LLMS\nTo enable multimodal generation in MM-LLMs, a straightforward approach is to allow these models\nto call external multimodal generation tools, such as Stable Diffusion (Rombach et al., 2022) or\ntext-to-speech (TTS) tools (Shen et al., 2023; Li et al., 2024c; OpenAI et al., 2023). However, as\nhighlighted in the Gemini technical report (Team et al., 2023), relying on an intermediate natural\nlanguage interface can limit the model’s ability to express images. If a model cannot natively output\nimages, it will not be able to generate images with prompts of interleaved sequences of image and text.\nThis claim is in line with our distinction between descriptive image generation and context-aware\nimage generation, as discussed in §2.2.\nAs a result, recent works focus on the unification of multimodal understanding and generation in a\nsingle model (i.e., any-to-any MM-LLMs), enabling the generation of multimodal tokens without\nnatural language as an interface. These models typically follow different approaches, depending\non how images are represented in both input and output sides. For example, the Discrete-In-\nDiscrete-Out (DIDO) approach has been explored in works such as SEED-LLaMA (Ge et al., 2023b),\nAnyGPT (Zhan et al., 2024), and Chameleon (Team, 2024). Continuous-In-Discrete-Out (CIDO)\nmethods have been implemented in models like DaVinCi (Diao et al., 2023), Gemini (Team et al.,\n2023), and Unified-IO 2 (Lu et al., 2023). The Continuous-In-Continuous-Out (CICO) approach is\nused in models such as Emu (Sun et al., 2023c;a), and DreamLLM (Dong et al., 2023). Another\napproach, the integration of autoregression and diffusion (AR + Diff), can be seen in models like\nTransfusion (Zhou et al., 2024), Show-o (Xie et al., 2024), and Li et al. (2024b)’s.\nHowever, these models face specific limitations. DreamLLM (CICI, Dong et al. (2023)) and CIDO\nmodels suffer from inconsistencies between input and output forms for multimodal data, making\nit difficult for them to natively support the generation of interleaved multimodal sequences where\nan image functions in a coupled way as both input and output. Emu2 (CICO, Sun et al. (2023a))\nstruggles with the challenges of the mean square error (MSE) loss used for training continuous output\nrepresentations, as well as with the uni-modal assumption of the Gaussian distribution in the MSE\nloss. Transfusion (AR + Diff, Zhou et al. (2024)) applies noise to images from the input side to\nsupport multimodal generation with diffusion modeling, and relies on VAE (Kingma & Welling,\n2013) features rather than CLIP (Radford et al., 2021) features for denoising, which largely trade off\nthe multimodal understanding abilities.\nTo mitigate these issues, we adopt the DIDO approach. A comprehensive comparison of our models\nwith other any-to-any MM-LLMs is presented in Table 1.\n5\nCONCLUSION\nIn conclusion, MIO represents an advancement in the realm of multimodal foundation models. By\nemploying a rigorous four-stage training process, MIO successfully integrates and aligns discrete\n10\nPreprint.\ntokens across text, image, video, and speech modalities. This comprehensive approach enables MIO\nto understand and generate multimodal content in an end-to-end, autoregressive manner, addressing\nthe limitations of current multimodal large language models. Our experimental results showcase its\ncompetitive performance across a variety of benchmarks compared to the dual-modality baselines and\nother any-to-any multimodal large language models. With the any-to-any and multimodal interleaved\noutput features, MIO exhibits novel emergent abilities such as interleaved video-text generation,\nchain-of-visual-thought reasoning, etc.\nREFERENCES\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,\nJiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin\nYang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and\nZonghong Dai. Yi: Open foundation models by 01.ai. arXiv preprint arXiv: 2403.04652, 2024.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nR. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M.\nTyers, and G. Weber. Common voice: A massively-multilingual speech corpus. In Proceedings of\nthe 12th Conference on Language Resources and Evaluation (LREC 2020), pp. 4211–4215, 2020.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations. Advances in neural information processing\nsystems, 33:12449–12460, 2020.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities.\nArXiv preprint, abs\/2308.12966, 2023.\nMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision,\n2021.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao,\nand Aditya Ramesh. Improving image generation with better captions, 2024. URL https:\n\/\/cdn.openai.com\/papers\/dall-e-3.pdf.\nAndreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik\nLorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach.\nStable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint\narXiv: 2311.15127, 2023.\nZalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,\nDominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Au-\ndiolm: A language modeling approach to audio generation. IEEE\/ACM Transactions on Audio,\nSpeech, and Language Processing, 31:2523–2533, 2023. doi: 10.1109\/TASLP.2023.3288409.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image\nediting instructions. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18392–18402, 2023.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio\nRibeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4.\narXiv preprint arXiv: 2303.12712, 2023.\n11\nPreprint.\nDavid Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In\nDekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pp. 190–\n200, Portland, Oregon, USA, June 2011a. Association for Computational Linguistics. URL\nhttps:\/\/aclanthology.org\/P11-1020.\nDavid L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation.\nIn Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics\n(ACL-2011), Portland, OR, June 2011b.\nGuoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su,\nDaniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuai-\njiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and\nZhiyong Yan. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed\naudio. arXiv preprint arXiv: 2106.06909, 2021.\nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and\nAlexandre D’efossez. Simple and controllable music generation. Neural Information Processing\nSystems, 2023. doi: 10.48550\/arXiv.2306.05284. URL https:\/\/arxiv.org\/abs\/2306.\n05284v3.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. ArXiv preprint, abs\/2305.06500, 2023.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv: 2307.08691, 2023.\nTri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, and Christopher R’e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. Neural Information Processing Systems, 2022.\nNilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, David Huang, Prashant Mathur, Jie\nYuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, Xilai Li, Karel Mundnich, Monica\nSunkara, Sundararajan Srinivasan, Kyu J Han, and Katrin Kirchhoff. Speechverse: A large-scale\ngeneralizable audio language model. arXiv preprint arXiv: 2405.08295, 2024.\nMostafa Dehghani, Basil Mustafa, Josip Djolonga, J. Heek, Matthias Minderer, Mathilde Caron,\nA. Steiner, J. Puigcerver, Robert Geirhos, Ibrahim M. Alabdulmohsin, Avital Oliver, Piotr\nPadlewski, A. Gritsenko, Mario Luvci’c, and N. Houlsby. Patch n’ pack: Navit, a vision trans-\nformer for any aspect ratio and resolution. Neural Information Processing Systems, 2023. doi:\n10.48550\/arXiv.2307.06304.\nShizhe Diao, Wangchunshu Zhou, Xinsong Zhang, and Jiawei Wang. Write and paint: Generative\nvision-language models are unified modal learners. In The Eleventh International Conference on\nLearning Representations, 2023.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.\nDreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv: 2309.11499,\n2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale,\n2021.\nBenjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap: Learning\naudio concepts from natural language supervision. arXiv preprint arXiv: 2206.04769, 2022.\nPatrick Esser, Robin Rombach, and Björn Ommer.\nTaming transformers for high-resolution\nimage synthesis.\n2021 IEEE\/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 12868–12878, 2020.\nURL https:\/\/api.semanticscholar.org\/\nCorpusID:229297973.\n12\nPreprint.\nQingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni:\nSeamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024.\nChaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin,\nLong Ma, Xiawu Zheng, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv\npreprint arXiv:2408.05211, 2024.\nJiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\nJianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-llava: Solving geometric problem\nwith multi-modal large language model. arXiv preprint arXiv: 2312.11370, 2023.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041, 2023a.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making\nllama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023b.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand\nJoulin, and Ishan Misra. Imagebind: One embedding space to bind them all. arXiv preprint arXiv:\n2305.05665, 2023.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in\nvqa matter: Elevating the role of image understanding in visual question answering. International\nJournal of Computer Vision, 2016. doi: 10.1007\/s11263-018-1116-0.\nDanna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and\nJeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. arXiv\npreprint arXiv: 1802.08218, 2018.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and\nFuru Wei. Language models are general-purpose interfaces. ArXiv preprint, abs\/2206.06336, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. arXiv preprint arXiv: 2106.07447, 2021.\nShaohan Huang, Li Dong, Wenhui Wang, Y. Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, O. Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary,\nSubhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with\nlanguage models. Neural Information Processing Systems, 2023. doi: 10.48550\/arXiv.2302.14045.\nTing-Hao Kenneth Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal,\nJacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick,\nDevi Parikh, Lucy Vanderwende, Michel Galley, and Margaret Mitchell. Visual storytelling.\nIn Proceedings of the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 1233–1239, 2016.\nDongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis:\nInterleaved multi-image instruction tuning. arXiv preprint arXiv: 2405.01483, 2024.\nWei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, and\nDaniel Povey. Libriheavy: a 50,000 hours asr corpus with punctuation casing and context, 2023.\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convo-\nlution or region supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\nvolume 139 of Proceedings of Machine Learning Research, pp. 5583–5594, 2021.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:\n1312.6114, 2013.\n13\nPreprint.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. NeurIPS, 2023.\nLAION. Laion coco: 600m synthetic captions from laion-2b-en. https:\/\/laion.ai\/blog\/\nlaion-coco\/, 2022.\nHugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,\nThomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and\nVictor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents,\n2023.\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image\ngeneration using residual quantization, 2022.\nBo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan\nZhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of\nlarge multimoal models, March 2024. URL https:\/\/github.com\/EvolvingLMMs-Lab\/\nlmms-eval.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-\nmarking multimodal llms with generative comprehension, 2023a.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36,\n2024a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. ArXiv preprint, abs\/2301.12597,\n2023b.\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023c.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language. ArXiv, 2019.\nTianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image\ngeneration without vector quantization. arXiv preprint arXiv: 2406.11838, 2024b.\nYanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng\nLiu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models.\narXiv preprint arXiv:2403.18814, 2024c.\nYizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng\nLiu, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou\nWang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, and Chenghua\nLin. Omnibench: Towards the future of universal omni-language models. arXiv preprint arXiv:\n2409.15272, 2024d.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740–755. Springer, 2014.\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and\nlanguage with ringattention. arXiv preprint, 2024.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning, 2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv\npreprint, abs\/2304.08485, 2023b.\n14\nPreprint.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. International Conference on\nLearning Representations, 2017.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks, 2019. URL https:\/\/arxiv.org\/abs\/1908.\n02265.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem,\nand Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision,\nlanguage, audio, and action, 2023.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. In The 36th Conference on Neural Information Processing Systems\n(NeurIPS), 2022.\nChenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming\nShi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video,\nand text integration. arXiv preprint arXiv:2306.09093, 2023.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual\nquestion answering benchmark requiring external knowledge. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 3195–\n3204, 2019.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef\nSivic. Howto100m: Learning a text-video embedding by watching hundred million narrated\nvideo clips. In Proceedings of the IEEE\/CVF international conference on computer vision, pp.\n2630–2640, 2019.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In ICDAR, 2019.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain,\nChun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi,\nand Anuj Kumar. Anymal: An efficient and scalable any-modality augmented language model.\narXiv preprint arXiv: 2309.16058, 2023.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian,\nJeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny\nBogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,\nMiles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea\nCarlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty\nEleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte,\nIsabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel\nGoh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua\nGross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike\nHeaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon\nHoughton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne\nJang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo\nJun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik\nKirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich,\nAris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy\nLee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie\nLin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,\nSam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\n15\nPreprint.\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,\nAshvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,\nGiambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,\nAdam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,\nMichael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,\nElizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis\nReal, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted\nSanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel\nSelsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang,\nNikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston\nTuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason\nWei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff,\nDave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu,\nJeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,\nRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,\nWilliam Zhuk, and Barret Zoph. Gpt-4 technical report. arXiv preprint arXiv: 2303.08774, 2023.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan\nClark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛adry, Alex Baker-\nWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex\nPaino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau,\nAli Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew\nCann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko,\nAngela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar,\nAshley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger,\nBen Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob\nMcGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan\nQuinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll\nWainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris\nBeaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine\nMcLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis,\nColin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy,\nDavid Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares,\nDimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong,\nEhsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric\nSigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo\nRaso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon,\nGiulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu\nWang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde\nde Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O’Connell,\nIan O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya\nSutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki,\nJames Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park,\nJason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia\nVaravva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne\nJang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John\nSchulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook\nKim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua\nAchiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan\nSinghal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen,\nKeren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther,\n16\nPreprint.\nLama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia\nGuy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held,\nLong Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke\nMetz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat\nDukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin,\nMatthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz,\nMeng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe,\nMichael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro,\nMiguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira\nMurati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone,\nNatalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick\nStathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel\nBundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia\nWatkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov,\nPeng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder,\nPhil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel\nLim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara,\nReimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky\nSmith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy\nChen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz,\nSam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray,\nSean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino\nJomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey,\nSteve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya\nChristianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas\nDimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov,\nToki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce\nWalters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko,\nWayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash\nPatil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin,\nYunxing Dai, and Yury Malkov. Gpt-4o system card. arXiv preprint arXiv: 2410.21276, 2024.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011.\nJunting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Journeydb: A benchmark\nfor generative image understanding, 2023.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus\nbased on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pp. 5206–5210, 2015. doi: 10.1109\/ICASSP.2015.7178964.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv: 2306.01116, 2023.\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE international conference on computer\nvision, pp. 2641–2649, 2015.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp.\n8748–8763, 2021.\n17\nPreprint.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. arXiv preprint arXiv: 2212.04356,\n2022.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In International Conference on\nMachine Learning, pp. 28492–28518. PMLR, 2023.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini\n1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-\nresolution image synthesis with latent diffusion models. In 2022 IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), Jun 2022. doi: 10.1109\/cvpr52688.2022.01042.\nURL http:\/\/dx.doi.org\/10.1109\/cvpr52688.2022.01042.\nPaul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos,\nFélix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah\nMuckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt\nSharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirovi´c,\nDamien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai\nZhang, Lukas Zilka, and Christian Frank. Audiopalm: A large language model that can speak and\nlisten. arXiv preprint arXiv: 2306.12925, 2023.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556–2565, 2018.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face. arXiv preprint arXiv: 2303.17580,\n2023.\nYu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and\nYemin Shi. Llasm: Large language and speech model. arXiv preprint arXiv: 2308.15930, 2023.\nAmanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 8317–8326, 2019.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,\nYongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context\nlearners. arXiv preprint arXiv:2312.13286, 2023a.\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training\ntechniques for clip at scale. arXiv preprint arXiv: 2303.15389, 2023b.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality, 2023c.\nChangli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and\nChao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint\narXiv: 2310.13289, 2023.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv e-prints, pp.\narXiv–2405, 2024.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson,\nIoannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy\nLillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom\n18\nPreprint.\nHennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli\nCollins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack\nKrawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan,\nManaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah,\nMahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan,\nJeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish\nGanapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth\nGopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey,\nMartin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker,\nEnrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs,\nAnaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas\nGonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp,\nLev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi,\nNatalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam\nBloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette,\nMegan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh\nJoshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin\nChadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan,\nJeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier\nGarcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas,\nDasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna\nChen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski,\nAbhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki,\nAntoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie\nXiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit\nSanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur\nBapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette\nLove, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James\nBradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R.\nArnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn,\nSrivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand,\nAnkush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah\nYork, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi´nska,\nVitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He,\nMarianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis,\nClara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou,\nDisha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu,\nDaniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi\nNarayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin\nVillela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling,\nPetko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James\nQin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur,\nSolomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche,\nTao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Luˇci´c, Guodong\nZhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao,\nKris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani\nAgrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren\nMaggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin,\nAndrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey,\nKefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen\nYang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay\nPavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu,\nRichard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung,\nTimothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek,\nRaphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao,\nMohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller,\nShereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins,\nTed Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas,\nCarrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen,\n19\nPreprint.\nSholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin\nChiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami,\nAnudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard\nHussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine,\nAlexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan\nHorgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex\nTomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal,\nSharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng,\nWojciech Stokowiec, Ce Zheng, Phoebe Thacker, Ça˘glar Ünlü, Zhishuai Zhang, Mohammad Saleh,\nJames Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi\nVezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran\nRong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks,\nMarie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi\nHashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze\nWang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer\nHassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal,\nMatthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki´cevi´c,\nMostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot,\nMatthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks,\nGautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang,\nKristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert,\nNate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz K˛epa, Yomna\nEldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri\nGayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb,\nVamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun\nWu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina\nChen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules\nWalter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson,\nSiddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim\nMukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel\nJanus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton\nÄlgmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna,\nAleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das,\nZihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi,\nSebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan,\nMikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma,\nIdan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen\nZhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu,\nMartin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa\nWilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra,\nWanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej,\nVipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal,\nAlexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana,\nAbhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti,\nChih-Wei \"Louis\" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu,\nRoey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile,\nSébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin,\nMark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan\nBanica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris\nHidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O’Neill,\nAnand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha\nKotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen,\nPrateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli,\nSahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini\nPal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li,\nShiyuan Chen, Niccolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester\nKwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo\nFigueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur,\nYenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu,\n20\nPreprint.\nAditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou,\nMehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul\nMedina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga,\nSabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung,\nKai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández\nAlcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante\nKärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica\nMallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal\nVerma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian\nTenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu,\nNan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan,\nXuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-\nDavid, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr\nStanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam\nPaszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin\nMiao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit\nKarmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac,\nGeoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan\nPetrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao,\nAlberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan,\nAlfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybi´nski, Ashwin Sreevatsa, Jennifer\nPrendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy\nWiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo\nLe, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian\nLIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica\nAbellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu,\nTom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse,\nFan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel\nAndor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan\nZhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili\nJanzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon,\nNatasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi\nRaad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova,\nRémi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu,\nChristina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes,\nDylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei\nXia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex\nPolozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu,\nMeghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval,\nReiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela\nRamos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov,\nRory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy,\nTomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang,\nRui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan\nRosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George\nPapamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane\nWu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana,\nJing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight,\nAmélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca\nSantamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie\nDeck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem,\nSho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun,\nLucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu\nYe, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan,\nAaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu,\nAbe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David\nGaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht,\nAshish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna\nWalton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh,\n21\nPreprint.\nPraveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-\nChimoto, Hanna Klimczak-Pluci´nska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria\nMendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth\nOdoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina,\nTom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb,\nSahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani,\nMatan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale,\nJinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu\nNayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma,\nEvgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong,\nKai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver\nWang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham\nMansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai\nSheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang,\nJulia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark\nGoldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki,\nChrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria\nGeorgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan,\nDinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana\nFranco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben\nVargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel\nSarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat,\nDa-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu,\nXuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal,\nAyal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal\nGodhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James\nWang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít\nListík, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha\nZykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico\nXu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal,\nAkshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani,\nOmar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso\nGhosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward\nLi, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar,\nMichael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti,\nRohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni,\nXiangkai Zeng, Ben Bariach, Laura Weidinger, Amar Subramanya, Sissie Hsiao, Demis Hassabis,\nKoray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov,\nJeffrey Dean, and Oriol Vinyals. Gemini: A family of highly capable multimodal models. arXiv\npreprint arXiv: 2312.11805, 2023.\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.\nURL https:\/\/huggingface.co\/datasets\/teknium\/OpenHermes-2.5.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. ArXiv preprint, abs\/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv preprint, abs\/2307.09288, 2023b.\nAäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.\nArXiv, abs\/1711.00937, 2017. URL https:\/\/api.semanticscholar.org\/CorpusID:\n20282961.\nChristophe Veaux, Junichi Yamagishi, and Kirsten MacDonald. Cstr vctk corpus: English multi-\nspeaker corpus for cstr voice cloning toolkit. 2017.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. arXiv preprint arXiv: 1411.5726, 2014.\n22\nPreprint.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech\nsynthesizers. arXiv preprint arXiv:2301.02111, 2023a.\nTiannan Wang, Wangchunshu Zhou, Yan Zeng, and Xinsong Zhang. EfficientVLM: Fast and\naccurate vision-language models via knowledge distillation and modal-adaptive pruning. In\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association\nfor Computational Linguistics: ACL 2023, pp. 13899–13913, Toronto, Canada, July 2023b.\nAssociation for Computational Linguistics. doi: 10.18653\/v1\/2023.findings-acl.873. URL https:\n\/\/aclanthology.org\/2023.findings-acl.873.\nTiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao,\nChunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei\nHuang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang,\nYuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu\nTayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang,\nYujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen,\nYuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing.\narXiv preprint arXiv: 2401.17268, 2024a.\nWenbin Wang, Yang Song, and Sanjay Jha. Globe: A high-quality english corpus with global accents\nfor zero-shot speaker adaptive text-to-speech. arXiv preprint arXiv: 2406.14875, 2024b.\nYi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu,\nYi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and\nYu Qiao. Internvideo: General video foundation models via generative and discriminative learning.\narXiv preprint arXiv:2212.03191, 2022.\nYi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei,\nRongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for\nmultimodal video understanding. arXiv preprint arXiv:2403.15377, 2024c.\nZehan Wang, Ziang Zhang, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Hengshuang Zhao,\nand Zhou Zhao. Omnibind: Large-scale omni multimodal representation via binding spaces. arXiv\npreprint arXiv: 2407.11895, 2024d. URL https:\/\/arxiv.org\/abs\/2407.11895v1.\nZekun Wang, Jingchang Chen, Wangchunshu Zhou, Haichao Zhu, Jiafeng Liang, Liping Shan,\nMing Liu, Dongliang Xu, Qing Yang, and Bing Qin. SmartTrim: Adaptive tokens and attention\npruning for efficient vision-language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique\nHoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint\nInternational Conference on Computational Linguistics, Language Resources and Evaluation\n(LREC-COLING 2024), pp. 14937–14953, Torino, Italia, May 2024e. ELRA and ICCL. URL\nhttps:\/\/aclanthology.org\/2024.lrec-main.1300.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan\nWu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang,\nKe Xu, Wenhu Chen, Jie Fu, and Junran Peng. Rolellm: Benchmarking, eliciting, and enhancing\nrole-playing abilities of large language models. arXiv preprint arXiv: 2310.00746, 2023c.\nZhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error vis-\nibility to structural similarity. IEEE Transactions on Image Processing, 13:600–612, 2004. doi: 10.\n1109\/TIP.2003.819861. URL https:\/\/ieeexplore.ieee.org\/document\/1284395.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837, 2022.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal\nllm. arXiv preprint arXiv: 2309.05519, 2023. URL https:\/\/arxiv.org\/abs\/2309.\n05519v2.\n23\nPreprint.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,\nYuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer\nto unify multimodal understanding and generation. arXiv preprint arXiv: 2408.12528, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2408.12528v1.\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\nquestion answering via gradually refined attention over appearance and motion. In Proceedings of\nthe 2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA, USA, October 23-27,\n2017, pp. 1645–1653, 2017.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging\nvideo and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 5288–5296, 2016.\nZhiyang Xu, Trevor Ashby, Chao Feng, Rulin Shao, Ying Shen, Di Jin, Qifan Wang, and Lifu\nHuang. Vision-flan:scaling visual instruction tuning, Sep 2023. URL https:\/\/vision-flan.\ngithub.io\/.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu\nSharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang,\nRichard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke\nZettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning, 2023.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin\nChoi. MERLOT: Multimodal neural script knowledge models. In A. Beygelzimer, Y. Dauphin,\nP. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems,\n2021. URL https:\/\/openreview.net\/forum?id=CRFSrgYtV7m.\nYan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, and Wangchunshu Zhou. X 2-vlm:\nAll-in-one pre-trained model for vision-language tasks. arXiv preprint arXiv:2211.12402, 2022.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. IEEE International Conference on Computer Vision, 2023. doi: 10.1109\/\nICCV51070.2023.01100.\nJun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,\nGe Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, and Xipeng Qiu.\nAnygpt: Unified multimodal llm with discrete sequence modeling. ArXiv, abs\/2402.12226, 2024.\nURL https:\/\/api.semanticscholar.org\/CorpusID:267750101.\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Empowering large language models with intrinsic cross-modal conversational abilities,\n2023a.\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated\ndataset for instruction-guided image editing. Advances in Neural Information Processing Systems,\n36, 2024.\nXin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech\ntokenizer for speech language models, 2023b.\nYanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.\nLlavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107, 2023c.\nYiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and\nXiangyu Yue. Meta-transformer: A unified framework for multimodal learning. arXiv preprint\narXiv: 2307.10802, 2023d.\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng\nWang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with\nmulti-modal in-context learning. ArXiv preprint, abs\/2309.07915, 2023.\n24\nPreprint.\nKaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation\nvia generative vokens, 2023.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob\nKahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and\ndiffuse images with one multi-modal model. arXiv preprint arXiv: 2408.11039, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2408.11039v1.\nWangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang. VLUE: A multi-task multi-\ndimension benchmark for evaluating vision-language pre-training. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th\nInternational Conference on Machine Learning, volume 162 of Proceedings of Machine Learning\nResearch, pp. 27395–27411. PMLR, 17–23 Jul 2022. URL https:\/\/proceedings.mlr.\npress\/v162\/zhou22n.html.\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae\nYu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale\ncorpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023.\n25\nPreprint.\nA\nPRE-TRAINING DATA\nPre-training Data Sources.\nThe pre-training data sources involve six types:\n1. Image-text paired data: SBU (Ordonez et al., 2011), CC3M (Sharma et al., 2018), LAION-\nCOCO (LAION, 2022), and JourneyDB (Pan et al., 2023), where JourneyDB only serves\nfor image generation.\n2. Language-only data: RefinedWeb (Penedo et al., 2023).\n3. Image-text interleaved data: OBELICS (Laurençon et al., 2023), MMC4-core-ff (Zhu et al.,\n2023).\n4. Video-text paired data: WebVid-10M (Bain et al., 2021).\n5. Video-text interleaved data: HowTo-100M (Miech et al., 2019), Youtube-Temporal-\n180M (Zellers et al., 2021).\n6. Speech-text paired data: Libriheavy (Kang et al., 2023).\nPre-training Data Processing.\nWe have different data processing procedures for different data\ntypes illustrated in §A following Emu (Sun et al., 2023c) and Qwen-VL (Bai et al., 2023):\n1. Image-text paired data: we remove pairs with more than 2:1 aspect ratio or smaller than\n224 × 224 resolution of the image. We remove pairs with more than 0.27 CLIP scores.\nWe remove non-English pairs. We randomly place the image or text at the forefront for\ngenerating captions based on images and vice versa.\n2. Language-only data: we use the same data processing pipeline as used in Yi (AI et al., 2024).\n3. Image-text interleaved data: we filter the data using a CLIP score threshold of 0.25, and\nfollow the same procedure as illustrated in Emu (Sun et al., 2023c).\n4. Video-text paired data: we randomly place the frames or text at the forefront for generating\ncaptions based on frames and vice versa. 60% of the pairs are text-to-video, while 40% of\nthe pairs are video-to-text. We sample 4 to 8 frames of each video for training according to\nthe text lengths.\n5. Video-text interleaved data: We first use PySceneDetect to extract key frames from the\nvideo based on scene changes, following the practice of Stable Video Diffusion (Blattmann\net al., 2023). Then, for each video clip between two key frames, we extract a central frame\nfor textual caption generation with BLIP-2 (Li et al., 2023b). Additionally, the video clips\nbetween key frames are processed using ASR (automatic speech recognition) tools to extract\nsubtitles. The ASR text and captions are then integrated and refined using Yi-34B-Chat (AI\net al., 2024), resulting in a single text segment. These text segments, along with the key\nframes and central frames, form the video-text interleaved data.\n6. Speech-text paired data: we remove speechs with more than 15 seconds.\nB\nPRE-TRAINING DETAILS\nHyperparameters.\nWe enable Flash Attention (Dao et al., 2022; Dao, 2023) during pre-training.\nGradient clipping is set to 1.0 for all stages. The maximum sequence length for training is 2800\ntokens. We use a cosine learning rate scheduler with a peak learning rate of 3e-5 and a warmup ratio\nof 0.03. The optimizer used is AdamW (Loshchilov & Hutter, 2017).\nPrompt Templates.\nThe prompt template is only necessary for paired datasets. For image-text\npaired data, we use the prompt templates of “{image} The caption of this image is: {caption}” and\n“Please generate an image of “{caption}”: {image}”. For video-text paired data: we use the prompt\ntemplates of “Please describe the following video: {image} {description}” and “Please generate a\nvideo for “{description}”: {video}”. For speech-text paired data: we use the prompt templates of\n“{speech} Transcribe this speech: {transcription}” and “Please generate a speech of “{transcription}”:\n{speech}” during Stage I and Stage II. While for Stage III, we change the ASR prompt template into\n‘{speech} The transcription of this speech is: {transcription}”.\n26\nPreprint.\nC\nSUPERVISED FINE-TUNING DETAILS\nTable 9: Supervised Fine-Tuning Data. “ICL” denotes In-Context Learning, and “CoT” denotes\nChain of Thought.\nTask\nDataset\nLanguage Only\nOpenHermes (Teknium, 2023)\nMultimodal ICL\nMMICL (Zhao et al., 2023)\nMultimodal CoT\nScienceQA (Lu et al., 2022)\nChart Understanding\nGeo170K (Gao et al., 2023)\nInstructional Image\nGeneration\nInstructPix2Pix (Brooks et al., 2023), MagicBrush (Zhang et al., 2024)\nASR\nLibriSpeech (Panayotov et al., 2015), GigaSpeech (Chen et al., 2021),\nCommon Voice (Ardila et al., 2020)\nVideo Dialogue\nVideoChat2-IT (Li et al., 2023c)\nImage QA\nVision-Flan (Xu et al., 2023), VizWiz (Gurari et al., 2018),\nLAION-GPT4V6, LLaVAR (Zhang et al., 2023c),\nOCR-VQA (Mishra et al., 2019), VQA (Goyal et al., 2016),\nTextVQA (Singh et al., 2019), OK-VQA (Marino et al., 2019),\nMantis-Instruct (Jiang et al., 2024)\nSpeech Generation\nSpeechInstruct (Zhang et al., 2023a)\nSpeech Understanding\nSpeechInstruct (Zhang et al., 2023a)\nImage Captioning\nFlickr30K (Plummer et al., 2015), MS-COCO (Lin et al., 2014)\nDescriptive Image\nGeneration\nFlickr30K (Plummer et al., 2015), MS-COCO (Lin et al., 2014)\nTTS\nGigaSpeech (Chen et al., 2021), Common Voice (Ardila et al., 2020)\nVideo Generation\nMSR-VTT (Xu et al., 2016), MSVD (Chen & Dolan, 2011b)\nVideo Understanding\nMSR-VTT (Xu et al., 2016), MSVD (Chen & Dolan, 2011b),\nMSVD-QA (Chen & Dolan, 2011a), MSRVTT-QA (Xu et al., 2017)\nVisual Storytelling\nVIST (Huang et al., 2016)\nSupervised Fine-Tuning Data.\nAs shown in Table 9, we use 16 tasks with 34 datasets for a\ncomprehensive supervised fine-tuning.\nPrompt Templates.\nThe chat template is the same as used in Yi (AI et al., 2024). The system\nprompt is unified as: “You are MIO, an AI assistant capable of understanding and generating images,\ntext, videos, and speech, selecting the appropriate modality according to the context.” except for\nspeech generation and TTS whose system prompts are “You are MIO, an AI assistant capable of\nunderstanding images, text, videos, and speech, and generating speech. Please respond to the user\nwith speech only, starting with <spch> and ending with <\/spch>.” to avoid randomness of the output\nmodality.\nHyperparameters.\nSimilar to pre-training (c.f., Appendix B), we enable Flash Attention (Dao\net al., 2022; Dao, 2023) during supervised fine-tuning. Gradient clipping is set to 1.0. The maximum\nsequence length for training is 2800 tokens. We use a cosine learning rate scheduler with a peak\nlearning rate of 3e-5 and a warmup ratio of 0.03. The optimizer used is AdamW (Loshchilov &\nHutter, 2017).\nD\nEVALUATION DETAILS.\nHyperparameters.\nThe decoding strategies and hyperparameters are quite important for a superior\nperformance. As shown in Table 10, we use different sets of parameters for different output modalities.\n27\nPreprint.\nTable 10: Decoding Hyperparameters.\nOutput Modality\nText\nImage\nSpeech\nVideo\nBeam size\n5\n1\n1\n1\nDo Sampling\nFalse\nTrue\nTrue\nTrue\nTop-P\n-\n0.7\n0.7\n0.7\nRepetition Penalty\n1.0\n1.0\n1.15\n1.15\nTemperature\n1.0\n1.0\n1.0\n1.0\nGuidance Scale\n1.0\n1.0\n1.0\n1.0\nTable 11: Prompt templates used for evaluating instruction-tuned models.\nTask\nPrompt Template\nImage Captioning\nProvide a one-sentence caption for the provided image. {image}\nImage QA\n(We use the prompt templates in LMMs-Eval (Li* et al., 2024)).\nImage Generation\nPlease generate an image according to the given description.\n{description}\nASR\nPlease transcribe this speech.{speech_token}\nTTS\nPlease generate a speech according to the given transcription. Start\nwith <spch>. {transcription}\nText-only\nThe following are multiple choice questions (with answers) about\n{subject} {question}\nVideo QA\nThe goal is to use the visual information available in the image to\nprovide an accurate answer to the question. This requires careful\nobservation, attention to detail, and sometimes a bit of creative\nthinking.{video} Question: {question} Answer:\nPrompt Templates.\nThe prompt templates used for evaluating pre-training checkpoints are the\nsame as used during pre-training. For SFT checkpoint evaluation, we list the prompt templates in\nTable 11.\nE\nMORE EXPERIMENTS\nE.1\nIMAGE GENERATION EVALUATION\nWe compute two additional automatic metrics for evaluating image generation, i.e., SSIM (Wang\net al., 2004) and Aesthetic Predictor v2.57 for the evaluation of structural integrity and aesthetics,\nrespectively. SSIM (Structural Similarity Index Measure) evaluates the perceptual similarity between\nthe generated images and the ground-truth images, focusing on luminance, contrast, and structure,\nwith scores ranging from -1 (dissimilar) to 1 (identical). Aesthetic Predictor V2.5 is a SigLIP (Zhai\net al., 2023)-based predictor that evaluates the aesthetics of an image on a scale from 1 to 10 (10 is the\nbest). In addition, we randomly select 100 image descriptions from MS-COCO test set, and used each\nmodel to generate images accordingly for human preference evaluation. We ask 3 annotators to rank\n3 images generated by the 3 models: “given the image description, which image is preferred?” The\naverage ranking of MIO’s, AnyGPT’s, and Emu’s generated images are 1.2 (MIO), 2.9 (AnyGPT),\n1.9 (Emu). MIO aligns the best with the human preference. The percentage agreement between the\nthree annotators (calculated as the number of cases with identical rankings by all annotators divided\nby 100) is 82.3%, indicating a high consistency in the human evaluation.\n28\nPreprint.\nDataset\nMS-COCO\nFlickr30K\nMS-COCO Subset\nMetric\nSSIM (↑)\nAesthetic (↑)\nSSIM (↑)\nAesthetic (↑)\nHuman Avg. Ranking (↓)\nEmu\n0.1749\n3.733\n0.1451\n3.893\n1.9\nAnyGPT\n0.1960\n3.954\n0.1585\n4.251\n2.9\nMIO\n0.2307\n4.019\n0.1727\n4.326\n1.2\nTable 12: Image generation evaluation by SSIM, Aesthetic Predictor V2.5, and human preference.\nModel\nSupported Workflow\nContent Score (1-5 points) (↑)\nMIO\ns2s\n1.4\nLLaMA-Omni\n(Fang et al., 2024)\ns2t→t2s\n2.4\nAnyGPT\ns2t→t2s\n1.8\nTable 13: Speech-to-Speech performance. “s2s” means “speech-to-speech”, while “s2t” and “t2s”\ndenote “speech-to-text” and “text-to-speech”, respectively.\nE.2\nSPEECH-TO-SPEECH EVALUATION\nSince there is a lack of speech to speech evaluation benchmarks, we randomly sample some conversa-\ntions from the moss-002-sft dataset8 and convert them into speech-to-speech format. Following the\nevaluation procedures outlined in LLaMA-Omni (Fang et al., 2024), we use the content score metric\nobtained from GPT-4o (OpenAI et al., 2024) to assess whether the model’s response effectively\naddresses the user’s instructions. The results are shown in Table 13.\nThough the content score of MIO is slightly lower than LLaMA-Omni and AnyGPT, both LLaMA-\nOmni and AnyGPT first generate text replies and then convert these into voice. However, our model,\nMIO, is capable of directly generating speech responses to speech queries.\nE.3\nTTS EVALUATION\nModel\nGLOBE\nLibriSpeech test-clean\nWER (↓)\nSpeech Similarity (↑)\nWER (↓)\nSpeech Similarity (↑)\nMIO\n9.8\n67.8\n10.3\n75.1\nAnyGPT\n27.9\n67.3\n28.1\n71.3\nTable 14: More automatic evaluations for the TTS performance.\nWe select two additional benchmarks, LibriSpeech test-clean (Panayotov et al., 2015) and\nGLOBE (Wang et al., 2024b), to evaluate the performance of TTS between our model and AnyGPT.\nFor fair comparison, we don’t specify the input voice prompt during evaluation of MIO and AnyGPT.\nWER (Word Error Rate) and speaker similarity are employed as the automatic metrics. The results\nare shown in Table 14. The results show that MIO performs significantly better than AnyGPT on\nboth WER and speaker similarity across both benchmarks.\nTable 15: Human evaluation\nfor the TTS performance.\nMIO Win\n54%\nTie\n25%\nMIO Lose\n21%\nAdditionally, we conduct a human evaluation to assess the speech\nquality of the outputs from MIO and AnyGPT. In this evaluation,\nparticipants are provided with the target speech, the speech gener-\nated by AnyGPT, and the speech generated by our model. They\nare tasked with determining which one sounded more natural and\ncloser to the target speech. Evaluators could choose one of the two\ngenerated speeches or indicate that they find them equally natural.\n7https:\/\/github.com\/discus0434\/aesthetic-predictor-v2-5?tab=\nreadme-ov-file\n8https:\/\/huggingface.co\/datasets\/fnlp\/moss-002-sft-data\n29\nPreprint.\n0\n5000\n10000\n15000\n20000\n25000\n30000\nStep\n2\n4\n6\n8\n10\n12\nLoss\nLoss Curves\nStage1\nStage2\nStage3\nFigure 3: Loss curves of pretraing stages.\nEach evaluation is rated by three independent human evaluators, and we report the average scores.\nThe results are shown in Table 15. MIO significantly outperforms AnyGPT in the human evaluation,\nconsistent with the results from the automatic evaluation.\nE.4\nLOSS CURVES\nWe plot the loss curves for each stage in Figure 3. We can observe that when introducing a new\ndata type (i.e., image-text interleaved data) in stage 2, the training loss suddenly increases. However,\nin the third pretraining stage, i.e., the speech-enhancement stage, the training loss transitions more\nsmoothly. Despite the fluctuations in loss between stages, which do have some impact on downstream\nperformance during the fluctuation periods, we find that with continued training, the model’s loss\nquickly recovers to its previous convergence level and continues optimizing effectively.\nE.5\nMORE DEMONSTRATIONS.\n30\nPreprint.\n\"Mike and Cheryl's Wedding\" \n\"Cheryl Leigh Jenkins and Michael David Nelson were married on Sunday, April 13, 2003, at\nthe Mountain Valley Chapel in Pigeon Forge, TN, in a really beautiful ceremony. They blended\ntheir families from former marriages.\".\nEveryone came out\nfor the wedding.\nAfterward everyone\ncelebrated by\nthrowing rice all over\neach other.\nThe bride wore a long\ndress that was\nvery pretty.\nEveryone sat down\nbefore they\nwalked out.\nVisual Storytelling\nHow does photosynthesis work in plants?\nA plant is seen growing and expanding its leaves as it uses sunlight for photosynthesis. this\nprocess involves capturing light energy from the sun through chlorophyll, which then produces \nglucose (sugar) through carbon dioxide conversion. additionally, oxygen is released during the\nprocess, helping support other life forms on earth.\n                                 The image shows many different stages of growth throughout various         \n                                  parts of the leaf, with green spots indicating the presence of chlorophyll     \n                                  cells that absorb sunlight for photosynthesis.\n \n                                 Finally, the energy produced by photosynthesis\n  powers the plant's survival and supports other\n  organisms in the ecosystem around it.\nOverall, the entire scene illustrates the importance of photosynthesis\nin supporting both plant growth and ecological balance. \nChain Of Visual Thought\nInstructional\nImage Editing\nMake the wolf a grizzly bear\nSpeech-In-\nSpeech-Out\nMake it a photograph.\nWhat is the weather like today?\n🔊\nHow do you praise spring?\n🔊\n   Cloudy with a chance of rain. \nI love the flowers in bloom!\n🔊\n🔊\nHow to make a pizza, from preparing the ingredients to baking it at every step.\nWhen I made my big\npepperoni pizza for\ndinner last night,\nall I needed\nwas some dough that\nlooked like cornbread\nand toppings.\nThe whole process\nwent really well and\nby now both pizzas\nwere done as they\nhad been left to bake\nnicely. \nFirst I took a bowl of\nflour out on the table.\nAfter I added salt in a \nspoon and then two\ntablespoons of milk\nalong with 2\nteaspoons of oil\nmixed until the dough\ncame together easily.\nWhile waiting for the\ndough to double, I got\na nice thick crust\ngoing on another pan\nbut this one would be\ntopped with tomato\nsauce instead of\ncheese. \nVisual Guideline Generation\nFigure 4: Demonstrations of MIO’s advanced abilities. Yellow : inputs; Green : outputs.\n31\nPreprint.\nVideo Generation\nBird eye panoramic view of busiest Asian cargo port with hundreds of ships loading export and\nimport goods and thousands of containers in harbor.\nImage Generation\nLife isn't about\nwaiting for the storm\nto pass. It's about\nlearning to dance in\nthe rain.\nHundreds of people\ngathered around\nlooking at\nmotorcycles.\n🎥\nVideo Understanding\nThe Peter and Paul fortress on a winter morning. Aerial view.\nImage Understand\nA train is parked at a\ntrain station.\nA man in a white\nshirt is taking a\npicture of a pastry.\nSpeech\nUnderstanding\n(ASR)\n      They seem very ignorant poor things\nsaid another in reply\n🔊\n      The queen gazed upon our friends with\nevident interest.\n🔊\n \\\"They seem very ignorant poor things,\\\"\nsaid another in reply.\nThe queen gazed upon her friends with\nevident interest.\n🎥\nSpeech\nGeneration\n(TT)\nEven so I was fortunate, for had they \n     gained the cave before I did, they would\nhave had me at their mercy.\n🔊\n      \"Never!\" she answered firmly. \"I will\ndenounce your father!\" \"You can't frighten\nme with such stuff. I am not a weak old\nman!\"\n🔊\nEven so I was fortunate, for had they\ngained the cave before I did, they would\nhave had me at their mercy.\n\"Never!\" she answered firmly. \"I will\ndenounce your father!\" \"You can't frighten\nme with such stuff. \nI am not a weak old man!\"\nVideo Generation\nBird eye panoramic view of busiest Asian cargo port with hundreds of ships loading export and\nimport goods and thousands of containers in harbor.\nImage Generation\nLife isn't about\nwaiting for the storm\nto pass. It's about\nlearning to dance in\nthe rain.\nHundreds of people\ngathered around\nlooking at\nmotorcycles.\n🎥\nVideo Understanding\nThe Peter and Paul fortress on a winter morning. Aerial view.\nImage\nUnderstanding\nA train is parked at a\ntrain station.\nA man in a white\nshirt is taking a\npicture of a pastry.\nSpeech\nUnderstanding\n(ASR)\n      They seem very ignorant poor things\nsaid another in reply\n🔊\n      The queen gazed upon our friends with\nevident interest.\n🔊\n \\\"They seem very ignorant poor things,\\\"\nsaid another in reply.\nThe queen gazed upon her friends with\nevident interest.\n🎥\nSpeech\nGeneration\n(TTS)\nEven so I was fortunate, for had they \n     gained the cave before I did, they would\nhave had me at their mercy.\n🔊\n      \"Never!\" she answered firmly. \"I will\ndenounce your father!\" \"You can't frighten\nme with such stuff. I am not a weak old\nman!\"\n🔊\nEven so I was fortunate, for had they\ngained the cave before I did, they would\nhave had me at their mercy.\n\"Never!\" she answered firmly. \"I will\ndenounce your father!\" \"You can't frighten\nme with such stuff. \nI am not a weak old man!\"\nFigure 5: Demonstrations of MIO’s basic abilities. Yellow : inputs; Green : outputs.\n32\nPreprint.\nMultimodal Chain-of-Thought\n  Please solve the closed choice problem with detailed explanation.\n  Which property do these two objects have in common?\n  A. colorful    B. stretchy\n  The stretchy property is part of both object descriptions. Both descriptions mention that each     \n  item can be stretched. So, the stretchy property is shared by both objects.\n  The answer is B).\nMultimodal In-Context Learning\n   Include information in your caption that is specific to image 0, and avoid using generic or       \n   ambiguous descriptions.\nOne cow separates from the\nrest othe group to go near\nthe fence\n<image 0>\n<image 1>\n \n  Be specific and detailed in your description of image 1, but also try to capture the essence of   \n  the image 1 in a succinct way. \nA truck with two motorcycles parked on its flatbed trailer.\nFigure 6: Multimodal Chain-of-Thought and Multimodal In-Context Learning Demos. Yellow :\ninputs; Green : outputs.\n33\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MIO: A Foundation Model on Multimodal Tokens.pdf"}
{"title":"MonoFormer: One Transformer for Both Diffusion and Autoregression","authors":"Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, Jingdong Wang","summary":"Most existing multimodality methods use separate backbones for\nautoregression-based discrete text generation and diffusion-based continuous\nvisual generation, or the same backbone by discretizing the visual data to use\nautoregression for both text and visual generation. In this paper, we propose\nto study a simple idea: share one transformer for both autoregression and\ndiffusion. The feasibility comes from two main aspects: (i) Transformer is\nsuccessfully applied to diffusion for visual generation, and (ii) transformer\ntraining for autoregression and diffusion is very similar, and the difference\nmerely lies in that diffusion uses bidirectional attention mask and\nautoregression uses causal attention mask. Experimental results show that our\napproach achieves comparable image generation performance to current\nstate-of-the-art methods as well as maintains the text generation capability.\nThe project is publicly available at https:\/\/monoformer.github.io\/.","url":"http:\/\/arxiv.org\/abs\/2409.16280v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2409.16280v1","published":1727200264000,"comment":null,"pdf_text":"Technical report\nMONOFORMER: ONE TRANSFORMER FOR BOTH DIF-\nFUSION AND AUTOREGRESSION\nChuyang Zhao1†\nYuxing Song1†\nWenhao Wang2\nHaocheng Feng1\nErrui Ding1\nYifan Sun1∗\nXinyan Xiao1∗\nJingdong Wang1∗\n1Baidu VIS\n2University of Technology Sydney\nABSTRACT\nMost existing multimodality methods use separate backbones for autoregression-\nbased discrete text generation and diffusion-based continuous visual generation,\nor the same backbone by discretizing the visual data to use autoregression for both\ntext and visual generation. In this paper, we propose to study a simple idea: share\none transformer for both autoregression and diffusion. The feasibility comes from\ntwo main aspects: (i) Transformer is successfully applied to diffusion for visual\ngeneration, and (ii) transformer training for autoregression and diffusion is very\nsimilar, and the difference merely lies in that diffusion uses bidirectional attention\nmask and autoregression uses causal attention mask. Experimental results show\nthat our approach achieves comparable image generation performance to current\nstate-of-the-art methods as well as maintains the text generation capability. The\nproject is publicly available at https:\/\/monoformer.github.io\/.\n1\nINTRODUCTION\nDiffusion models are popular for image generation and other continuous data. It is a probabilistic\napproach to modeling continuous data, which creates samples by simulating the diffusion process,\ngradually adding and removing noise from data. Diffusion is initially studied in the pixel space\nfor visual generation (Ho et al., 2020). Latent diffusion models (Rombach et al., 2022) performs\nthe diffusion in the latent representation space, and are now commonly used in many well-known\nmodels, such as Stable Diffusion (Rombach et al., 2022) and DiT (Peebles & Xie, 2023).\nAutoregressive models are dominant in large language models. The basic idea is to predict the dis-\ncrete tokens one by one. It is also widely studied for visual generation by discretizing the image\npatches through VQ-VAE (Van Den Oord et al., 2017) or dVAE (Ramesh et al., 2021a). Autore-\ngression is advantageous in building a unified transformer for multi-modality understanding and\ngeneration models (Sun et al., 2024b; Liu et al., 2024; Team, 2024). Unfortunately, it does not ben-\nefit from recent advances in diffusion. Autoregression and diffusion are studied in parallel and often\nlearned in different models. Some attempts for combining them for multi-modality understand-\ning and generation models adopt two separate networks for text generation and visual generation,\nrespectively (Sun et al., 2023; Lian et al., 2023).\nIn this paper, we aim at building and training one transformer for both autoregression and diffusion.\nThe proposed approach is named as MonoFormer and is illustrated in Figure 1. The idea is very\nsimple and inspired by the success of using transformer for diffusion (Peebles & Xie, 2023) for\nimage generation, as well as the below-discussed observations about the transformer for autoregres-\nsion and diffusion. The main difference in training the transformer is that autoregressive transformer\nadopts a causal attention mask and diffusion transformer does not mask any position, or uses a bidi-\nrectional attention mask. On the other hand, the transformer receives continuous embeddings, e.g.,\ntext token embedding or image encoding, and outputs continuous embeddings for subsequent text\ntoken prediction and image decoding. Thus, it is feasible to learn one transformer for both discrete\nautoregression and continuous diffusion. We demonstrate the idea by training a single transformer\nthat is shared by autoregression for text generation and diffusion for image generation.\n∗Corresponding authors, †Equal contribution\n1\narXiv:2409.16280v1  [cs.CV]  24 Sep 2024\nTechnical report\nPlease draw\nan\nimage\nof\nflower\n.\ndraw\nan\nimage\nof\nflower\n.\n<si>\n<si>\n(a) Autoregression (causal mask)\n(b) Diffusion (bidirectional mask)\ntransformer\nFigure 1: Our approach MonoFormer trains the autoregressive transformer and the diffusion trans-\nformer, which share the weights, and uses causal attention mask and bidirectional attention mask,\nrespectively. During training, the input of the transformer for autoregression is the text token em-\nbeddings, and the output is embeddings that are further processed for text generation. The input for\ndiffusion is the noised latent embeddings, and the output is embeddings that are used to predict the\nnoise.\nWe train our model on two tasks illustrated in Figure 2: autoregression for text-to-text generation\nand diffusion for text-to-image generation. We use a pretrained LLM as the transformer backbone\nto gain language understanding capabilities. Experiments demonstrate that our method achieves\ncomparable image generation performance to current state-of-the-art methods as well as maintains\nthe text generation capability.\n2\nRELATED WORKS\nDiffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Rombach et al.,\n2022; Song et al., 2020) have demonstrated outstanding performance in generating high-quality im-\nages. These models show significant advantages in terms of stability and scalability. Diffusion\nmodels (Nichol et al., 2021; Saharia et al., 2022) for text-to-image generation incorporate pretrained\ntext encoders, such as CLIP (Radford et al., 2021), Flan-T5 (Chung et al., 2024), or LLaMA (Tou-\nvron et al., 2023a), or use LLMs to encode the text (Koh et al., 2024) as the condition.\nRecently, the architecture of diffusion models has been shifting from U-Net architectures to\ntransformer-based architectures (Peebles & Xie, 2023; Podell et al., 2023; Gao et al., 2024), nar-\nrowing the gap between image generation and language understanding tasks. This inspired us to use\none transformer for both autoregression and diffusion generation.\nAutoregressive models. Autoregressive (AR) models are widely used in text generation tasks and\nhave demonstrated remarkable performance in large language models (LLMs) (Brown, 2020; Rad-\nford et al., 2019; Touvron et al., 2023a; Team et al., 2023; Touvron et al., 2023b).\nAR models have also been applied in image generation (Parmar et al., 2018; Child et al., 2019;\nRamesh et al., 2021b; Chang et al., 2023; Ding et al., 2022; Saharia et al., 2022; Li et al., 2024).\nThe AR models usually use visual tokenizers, such as VQ-VAE (Van Den Oord et al., 2017) or VQ-\nGAN (Esser et al., 2021), to discretize continuous image features into discrete tokens. Though AR\nmodels perform well for image generation (Ramesh et al., 2022; Yu et al., 2022; Chen et al., 2020),\nthe overall quality is still inferior to diffusion models.\nUnified models. There has been a trend in the community towards a unified model for text-to-text\ngeneration and text-to-image generation. Recent works (Team, 2024; He et al., 2024) propose to use\na single autoregressive model for text-to-text generation and text-to-image generation, illustrated in\nFigure 3 Left. These methods still use autoregression for image generation and thus still under-\nperform diffusion models. Prior works simply adopt an additional network for diffusion to generate\nhigh-quality images Ge et al. (2024); Wu et al. (2023), which is illustrated in Figure 3 Right.\n2\nTechnical report\nPlease generate an image\nof lesser panda.\nAn image of a nerdy bear\nwearing glasses and bowtie.\nCan you list some of the\npopular landmarks in the\nFrance?\nSure, here are some of the\nlandmarks in France:\n1. Eiffel Tower\n2. Louvre Museum\n3. Notre-Dame Cathedral\n4. Palace of Versailles\n5. Arc de Triomphe\n6. Montmartre\nFigure 2: Examples of MonoFormer for both image generation and text generation tasks. Left:\nClass-conditional image generation. Middle: Text-to-image generation. Right: Text-to-text genera-\ntion.\n3\nMETHOD\n3.1\nPRELIMINARIES\nAutoregressive transformer. An autoregressive model factorizes the joint distribution autoregres-\nsively over a sequence of high-dimensional data {x1, x2, . . . , xn}:\np(x1, x2, . . . , xn) =\nYn\ni=1 p(xi|x<i; θ).\n(1)\nThe model, which is parameterized by θ, is trained by minimizing the negative log-likelihood:\nEx∈X [−log p(x)] = Ex∈X [−\nXn\ni=1 log p(xi|x<i; θ)].\n(2)\nFor inference, the model predicts the data one by one: predict x1, . . . , xi, . . . until xn by sampling\nfrom p(x1; θ), . . . , p(xi|x<i; θ), . . . , p(xn|x<n; θ).\nThe transformer decoder is used in models like GPT and LLaMA, for the implementation. The\ndecoder takes token embeddings as input and produces an embedding for each position. The autore-\ngressive transformer is similar to a standard transformer decoder, with the primary difference being\nthe use of a causal attention mask. This mask ensures that each position only attends to previous\npositions, maintaining the autoregressive property.\nDiffusion transformer. Diffusion models consist of a diffusion process, which gradually adds Gaus-\nsian noise to the data x0:\nxt = √¯αtx0 +\n√\n1 −¯αtϵ,\n(3)\nwhere ¯αt are parameters derived from the variance schedule, and ϵ is the Gaussian noise. Diffusion\nmodels are trained to learn the denoising process:\npθ(xt−1|xt) = N(xt−1; µθ(xt), σθ(xt)).\n(4)\nThe DDPM (Ho et al., 2020) transfers the problem through reparameterization to learn a noise\nprediction network ϵθ by minimizing the difference between the predicted noise ϵθ(xt) and the\nground-truth noise ϵ:\nLsimple = ∥ϵθ(xt) −ϵ∥2\n2.\n(5)\nU-Net (Ronneberger et al., 2015) is widely used as the backbone for the noise prediction network\nϵθ. Latent diffusion models (Rombach et al., 2022) train the model in the latent representation space\nthat is formed by learning a variational autoencoder (VAE) (Kingma, 2013) to compress images.\nRecently, Diffusion Transformers (DiTs) (Peebles & Xie, 2023), which our approach builds upon,\ntrain latent diffusion models using a transformer. We adopt the variant based on the DiT block with\nin-context conditioning. The architecture utilizes standard self-attention over condition embeddings\n(for text and timesteps) and latent embeddings. The self-attention is bidirectional, meaning that\nthere is no mask or equivalently the mask is an all-ones matrix.\n3\nTechnical report\nPlease draw\nan\nimage\nof\nflower\n.\ndraw\nan\nimage\nof\nflower\n.\n<si>\n<si>\ntransformer\n<ei>\nPlease draw\nan\nimage\nof\nflower\ndraw\nan\nimage\nof\nflower\n.\nautoregressive transformer\ndiffusion transformer or U-Net\nFigure 3: Left: A single autoregressive transformer for both text generation and visual generation.\nExample methods inlcude Chameleon (Team, 2024) and LlamaGen (Sun et al., 2024b). Right: One\ntransformer is for autoregressive text generation, and the output embeddings are sent to another\nmodel for diffusion-based text-to-image generation (Ge et al., 2024; Wu et al., 2023).\n3.2\nMONOFORMER\nOur approach is built upon the common-used large language model architecture, and uses one trans-\nformer for both autoregression-based text-to-text generation and diffusion-based text-to-image gen-\neration.\nFor text-to-text generation, the input text is processed as a sequence of text token embeddings, which\nare fed into the transformer. The transformer autoregressively generates output embeddings at each\nposition, which is then parsed into text tokens.\nFor text-to-image generation, the noised latents are sent to the transformer to generate embeddings\nat all positions simultaneously. This process is iterated over multiple timesteps in a cascade manner,\nfollowing the standard denoising diffusion pipeline. The final output embeddings are decoded to an\nimage through a VAE decoder.\nTraining. The transformer is trained using the autoregression loss for text-to-text generation, and the\ndiffusion loss for text-to-image generation. We adopt the standard LLM transformer architecture.\nThe transformer is composed of transformer blocks. Each block is formed by the FFN and the\nmasked attention:\nmasked-attention(Q, K, V) = softmax(QK⊤\n√\nd\n⊙M)V,\n(6)\nwhere M is the attention mask, ⊙is the element-wise product, and d is the dimension. The key\ndifference between autoregression and diffusion lies in the mask used.\nWe adopt the standard causal attention mask (upper triangular mask) for optimizing the autoregres-\nsion loss:\nMAR =\n\n\n1\n−∞\n−∞\n−∞\n−∞\n−∞\n−∞\n1\n1\n−∞\n−∞\n−∞\n−∞\n−∞\n1\n1\n1\n−∞\n−∞\n−∞\n−∞\n1\n1\n1\n1\n−∞\n−∞\n−∞\n1\n1\n1\n1\n1\n−∞\n−∞\n1\n1\n1\n1\n1\n1\n−∞\n1\n1\n1\n1\n1\n1\n1\n\n\n.\n(7)\nThe transformer takes the text embedding to as input and applies the causal attention mask MAR to\nmask past tokens. The output embeddings are then used to predict the tokens through the autore-\ngression head HAR:\n¯to = HAR(transformer(to, MAR; θ)).\n(8)\nOptimizing the diffusion loss is similar. The major difference lies in the attention mask. Unlike in\ntext generation, where text tokens see only past tokens, image tokens see both past text tokens and\nfuture image tokens. A bidirectional attention mask is adopted. We illustrate the attention mask\n4\nTechnical report\nusing an example with three text tokens and four image tokens:\nMDi =\n\n\n1\n−∞\n−∞\n−∞\n−∞\n−∞\n−∞\n1\n1\n−∞\n−∞\n−∞\n−∞\n−∞\n1\n1\n1\n−∞\n−∞\n−∞\n−∞\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n.\n(9)\nThe transformer takes the embeddings of the text to and noised latent embeddings zt as input,\nprocessing them with the attention mask MDi. It outputs embeddings that are then passed through\nthe diffusion head HDi to predict the noise:\n¯ϵ = HDi(transformer(to, zt, MDi; θ)).\n(10)\nWe use the standard diffusion loss, as used in DiT (Peebles & Xie, 2023), for training.\nThe whole loss is a combination of the standard text-to-text autoregression loss and text-to-image\ndiffusion loss:\nℓAR(¯to, to) + ℓDi(¯ϵ, ϵ).\n(11)\nInference. For text-to-text generation, the inference is a standard autoregression process, using the\ntrained transformer to predict next tokens one by one.\nFor text-to-image generation, we follow DiT (Peebles & Xie, 2023). Once the image start token\n<si> is generated in the autoregression process, the diffusion process begins. We input the Gaussian\nnoise as the initial noised latent, to the transformer, predicting the noise that is used to reduce the\nnoise. The noise reduction process is iterated for multiple timesteps to generate the image. The\ndenoising process is almost the same as that of the in-context version of DiT (Peebles & Xie, 2023)\nwith a slight difference: Our approach uses a combination of causal attention mask (for text tokens)\nand the bidirectional attention mask (for image tokens). In contrast, the in-context version of DiT\nuses a bidirectional attention mask for both text and image tokens.\nArchitecture for diffusion. We perform diffusion in the latent space. We use the VAE encoder\nto map each image patch to a continuous representation followed by a linear projection to generate\nlatent representations for latent diffusion.\nThe noised latent embeddings are added with positional embeddings, using the sine-cosine version\nFollowing DiT (Peebles & Xie, 2023), we embed the input timestep using a 256-dimensional fre-\nquency embedding, followed by a two-layer MLP with SiLU activations. The time embeddings are\ncombined with the noised latent embeddings through an AdaLN layer. The combined embeddings\nare fed into the transformer for predicting the noise, followed by denoising the noised embeddings.\nThe diffusion head HDi is implemented following DiT, which consists of a layer norm followed by\na linear layer and SiLU activation.\n4\nEXPERIMENTS\n4.1\nSETUP\nImplementation details. We leverage the pretrained variational autoencoder (VAE) from Stable\nDiffusion (Rombach et al., 2022) to encode the image. The VAE encoder downsamples the image\nby a factor of 8, generating a latent representation of dimension 4. Following DiT (Peebles & Xie,\n2023), we patchify the latent representation using patch size of 2 × 2. The latent representation\ngoes through a linear projection layer for matching the dimension of the representation that is fed\ninto the transformer. The linear projection layer for aligning the latent representation dimensions\nis initialized to zero. The linear layers in the time embedding projector are initialized to zero. The\nAdaLN parameters for combining time embeddings are initialized using a normal distribution. The\nlinear layers in the diffusion head HDi are initialized using a normal distribution too. We initialize\nthe transformer using TinyLlama-1.1B v1.0 (Zhang et al., 2024), which is pre-trained on 3T tokens\nand fine-tuned on UltraChat (Ding et al., 2023) dataset.\n5\nTechnical report\nTable 1: Performance on ImageNet 256×256 benchmark.\nMethod\nArch\n#Params\nFID↓\nIS↑\nPrecision↑\nRecall↑\nADM (Dhariwal & Nichol, 2021)\nDiff\n554M\n10.94\n101.0\n0.69\n0.63\nCDM (Ho et al., 2022)\nDiff\n-\n4.88\n158.7\n-\n-\nLDM (Rombach et al., 2022)\nDiff\n400M\n3.60\n147.6\n0.87\n0.68\nDiT-XL\/2 (Peebles & Xie, 2023)\nDiff\n675M\n2.27\n278.2\n0.83\n0.57\nVQGAN (Esser et al., 2021)\nAR\n227M\n18.65\n80.4\n0.78\n0.26\nViT-VQGAN (Yu et al., 2021)\nAR\n1.7B\n4.17\n175.1\n-\n-\nLlamaGen-B (Sun et al., 2024b)\nAR\n111M\n5.46\n193.6\n0.83\n0.45\nLlamaGen-3B (Sun et al., 2024b)\nAR\n3.1B\n2.81\n311.5\n0.84\n0.54\nMonoFormer\nAR+Diff\n1.1B\n2.57\n272.6\n0.84\n0.56\nWe use the AdamW optimizer without weight decay, and the learning rate is set to a constant value of\n1e-4. We maintain an exponential moving average of the MonoFormer weights throughout the train-\ning process with a decay rate of 0.9999. We retain the diffusion hyper-parameters from DiT (Peebles\n& Xie, 2023), using 1000 timesteps linear variance schedule ranging from 1 × 10−4 to 2 × 10−2,\nand parameterization of the covariance Σθ.\nWe train the model on the ImageNet (Deng et al., 2009) dataset for class-conditional generation.\nThe category names are converted into text to form the text prompt in ImageNet, such as “Please\ngenerate an image of [category]”, “An image of [category]”, etc. We train the model on\nthe JourneyDB (Sun et al., 2024a) and UltraChat (Ding et al., 2023) for text-to-image generation\nand text generation. Considering that the diffusion task is more difficult, the ratio of the numbers of\nimage generation samples and text generation samples is 9 : 1. The Global batch size is 1024.\nClassifier-free guidance. We adopt classifier-free guidance (Ho & Salimans, 2022) for text-to-\nimage generation. In the training stage, we randomly drop the text tokens for unconditional gen-\neration. We set the drop probability to 1\/10. The final latent output is computed by combining\nthe unconditional and conditional outputs and using a guidance factor to control the scale of the\nguidance.\n4.2\nEXPERIMENT RESULTS\nImage generation. We evaluate the text-to-image generation performance on the ImageNet (Deng\net al., 2009) dataset under resolution of 256 × 256. The evaluation metrics include: the Fr´echet\nInception Distance (FID), the Inception Score (IS), and Precision\/Recall. In inference, we use a\nclassifier-free guidance scale of 1.5. We report the comparison to representative diffusion models\nand unified autoregressive models. The comparison results are shown in Table 1. Our method\nachieves comparable results with recent diffusion-based or AR-based methods. It outperforms the\nAR-based LlamaGen-3B (Sun et al., 2024b) in FiD with fewer parameters, while being only 0.3\nlower than the state-of-the-art diffusion-based method DiT-XL\/2 (Peebles & Xie, 2023) in FID.\nText generation. We evaluate MonoFormer on a diverse set of commonsense reasoning tasks and\ncompare it with the baseline model, TinyLlama (Zhang et al., 2024). We evaluate our method on\nthe following tasks: HellaSwag (Zellers et al., 2019), OpenBookQA (Mihaylov et al., 2018), Wino-\nGrande (Sakaguchi et al., 2021), ARC-Easy and ARC-Challenge (Clark et al., 2018), BoolQ (Clark\net al., 2019), and PIQA (Bisk et al., 2020).\nTable 2: Performance on commonsense reasoning tasks.\nModel\nHellaSwag OBQA WinoGrande ARC-C ARC-E BoolQ PIQA\navg\nPythia (Biderman et al., 2023)\n52.01\n33.20\n57.38\n28.50\n54.00\n63.27\n70.95 51.33\nTinyLlama (Zhang et al., 2024)\n59.20\n36.00\n59.12\n30.12\n55.25\n57.83\n73.29 52.97\nMonoFormer\n50.62\n37.20\n56.91\n31.48\n48.19\n62.29\n71.16 51.12\n6\nTechnical report\n(a)\n(b)\n(c)\nFigure 4: (a) The effect of transformer initialization for image generation, measured using the FiD-\n10K metric on ImageNet. (b) The effect of transformer initialization for text generation, measured\nby the average commonsense reasoning score. (c) The effect of bidirectional attention mask for\nimage generation.\nTable 2 shows that MonoFormer achieves performance comparable to the TinyLlama baseline, with\nslight drops on certain benchmarks (average score deceases from 52.97 to 51.12). The slight perfor-\nmance drops may be attributed to the mixed training with the image generation dataset. We believe\nthe performance can be further improved when more language data are incorporated in training, we\nleave this for future exploration.\n4.3\nABLATION STUDY\nTransformer initialization. We conduct an ablation study to study the impact of using pretrained\nLLMs for transformer initialization. We compare the performance of MonoFormer with and without\npretrained LLM initialization on two tasks: image generation on the ImageNet 256×256 benchmark\nand language understanding on commonsense reasoning benchmarks. Performance is evaluated us-\ning the FiD score for ImageNet and the average score across 6 commonsense reasoning tasks, as\ndescribed in Section 4.2. As shown in Figure 4 (a), MonoFormer with pretrained LLM initialization\nsignificantly outperforms the counterpart in commonsense reasoning. As shown in Figure 4 (b), on\nthe image generation benchmark, MonoFormer with LLM initialization also shows superior perfor-\nmance. We attribute this improvement to the pretrained LLM’s ability to better understand prompts,\nthereby benefiting text-to-image generation tasks.\nBidirectional attention for diffusion. We study the effect of applying bidirectional attention masks\namong noised latent tokens, which are used for diffusion-based generation. We respectively use\ncausal attention mask and bidirectional attention mask for the noised latent tokens and compare\ntheir performance on the ImageNet 256×256 benchmark. As shown in Figure 4 (c), the performance\nof MonoFormer with bidirectional attention outperforms MonoFormer with causal attention mask,\ndemonstrating the importance of bidirectional attention mask for image generation.\n5\nCONCLUSION\nThis paper shows that discrete autoregression and continuous diffusion are able to share one trans-\nformer. Experiments validate that sharing the transformer is able to achieve good performance for\ntext-to-text generation and text-to-image generation that is comparable to not sharing the trans-\nformer.\nREFERENCES\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. In International\nConference on Machine Learning, pp. 2397–2430. PMLR, 2023.\n7\nTechnical report\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432–7439, 2020.\nTom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pp. 1691–\n1703. PMLR, 2020.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned lan-\nguage models. Journal of Machine Learning Research, 25(70):1–53, 2024.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes\/no questions. arXiv preprint\narXiv:1905.10044, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780–8794, 2021.\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image\ngeneration via hierarchical transformers. Advances in Neural Information Processing Systems,\n35:16890–16902, 2022.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recogni-\ntion, pp. 12873–12883, 2021.\nPeng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu,\nYuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration\nvia flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying\nShan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation.\narXiv preprint arXiv:2404.14396, 2024.\nWanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei\nZhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, and Hao Jiang. Mars: Mixture of\nauto-regressive models for fine-grained text-to-image synthesis, 2024.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n8\nTechnical report\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-\nmans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning\nResearch, 23(47):1–33, 2022.\nDiederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language\nmodels. Advances in Neural Information Processing Systems, 36, 2024.\nTianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He.\nAutoregressive image\ngeneration without vector quantization. arXiv preprint arXiv:2406.11838, 2024.\nLong Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt\nunderstanding of text-to-image diffusion models with large language models.\narXiv preprint\narXiv:2305.13655, 2023.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems, 36, 2024.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\n2018.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In International conference on machine learning, pp. 4055–\n4064. PMLR, 2018.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of\nthe IEEE\/CVF International Conference on Computer Vision, pp. 4195–4205, 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs\/2102.12092, 2021a.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine\nlearning, pp. 8821–8831. Pmlr, 2021b.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation, 2015.\n9\nTechnical report\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in neural informa-\ntion processing systems, 35:36479–36494, 2022.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\nsarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learn-\ning, pp. 2256–2265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nKeqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding.\nAdvances in Neural Information Processing Systems, 36, 2024a.\nPeize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.\nAutoregressive model beats diffusion: Llama for scalable image generation.\narXiv preprint\narXiv:2406.06525, 2024b.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality.\nIn The Twelfth International Conference on Learning Representations, 2023.\nChameleon Team.\nChameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multi-\nmodal llm. arXiv preprint arXiv:2309.05519, 2023.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,\nYuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer\nto unify multimodal understanding and generation, 2024.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\narXiv preprint arXiv:2110.04627, 2021.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n10\nTechnical report\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small\nlanguage model. arXiv preprint arXiv:2401.02385, 2024.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob\nKahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and\ndiffuse images with one multi-modal model, 2024.\n11\nTechnical report\nA\nIMAGE GENERATION RESULTS\nWe present class-conditional image generation results in Figure 5. We use a classifier-free guid-\nance scale of 4 and sample 250 steps using DPM-Solver. We further train our model on the Jour-\nneyDB (Sun et al., 2024a) dataset and the UltraChat (Ding et al., 2023) dataset for free text-to-image\ngeneration. Figure 6 showcases the results generated using Parti prompts (Yu et al., 2022), with a\nclassifier-free guidance scale of 4 and 250 sampling steps.\nB\nEXTENSION TO MULTI-MODALITY UNDERSTANDING\nOur approach can be easily extended for multi-modality understanding, for example, vision-\nlanguage understanding. In the case that the data sample is an interleaved image-text sequence,\nthere are two choices for extracting the image representation for understanding. One choice is that\nthe image is only processed by diffusion, and we choose to use the transformer with a timestep\nnear zero for image representation extraction. The other choice is that the image is processed in an\nautoregressive manner for understanding and diffusion is only for generation.\nC\nDICCUSSION WITH CONCURRENT WORK\nThere are concurrent works with similar ideas, such as Transfusion (Zhou et al., 2024) and Show-\no (Xie et al., 2024). Show-o employs discrete diffusion. Differently, our approach uses continuous\ndiffusion model. Transfusion is more similar to our method, and trained from scratch. Our experi-\nments demonstrate that initializing transformer with a large language model is helpful for training.\n12\nTechnical report\nFigure 5: Example results of class-conditional image generation on ImageNet. We use 250\nsampling steps and a classifier-free guidance scale of 4.0.\n13\nTechnical report\nying-yang\nwater\na woman with long black hair \nand dark skin\nartificial intelligence\na chimpanzee\nThe Starry Night\na shiba inu\nan espresso machine\na pick-up truck rolling over \na grassy field\nbrain coral\na portrait of young girl\na propaganda poster\na turtle\nan old-fashioned windmill \nsurrounded by flowers\na capybara sitting in a \nfield\nthe Sydney Opera House\na tiger\nthe Great Pyramid\nthe Taj Mahal at sunrise\na panda bear with aviator \nglasses on its head\nFigure 6: Example results of text-to-image generation. The prompts are from Parti prompts (Yu\net al., 2022). We use 250 sampling steps and a classifier-free guidance scale of 4.0.\n14\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MonoFormer: One Transformer for Both Diffusion and Autoregression.pdf"}
{"title":"VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation","authors":"Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu","summary":"VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.","url":"http:\/\/arxiv.org\/abs\/2409.04429v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2409.04429v3","published":1725644996000,"comment":"Code: https:\/\/github.com\/mit-han-lab\/vila-u. The first two authors\n  contributed equally to this work","pdf_text":"Published as a conference paper at ICLR 2025\nVILA-U: A UNIFIED FOUNDATION MODEL INTEGRAT-\nING VISUAL UNDERSTANDING AND GENERATION\nYecheng Wu1,2∗\nZhuoyang Zhang2∗†\nJunyu Chen1,2\nHaotian Tang2†\nDacheng Li4†\nYunhao Fang5†\nLigeng Zhu3\nEnze Xie3\nHongxu Yin3\nLi Yi1\nSong Han2,3\nYao Lu3\nTsinghua University1\nMIT2\nNVIDIA3\nUC Berkeley4\nUC San Diego5\nhttps:\/\/hanlab.mit.edu\/projects\/vila-u\nABSTRACT\nVILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can lead\nto misalignment and increased complexity. In contrast, VILA-U employs a single\nautoregressive next-token prediction framework for both tasks, eliminating the need\nfor additional components like diffusion models. This approach not only simplifies\nthe model but also achieves near state-of-the-art performance in visual language\nunderstanding and generation. The success of VILA-U is attributed to two main\nfactors: the unified vision tower that aligns discrete visual tokens with textual\ninputs during pretraining, which enhances visual perception, and autoregressive\nimage generation can achieve similar quality as diffusion models with high-quality\ndataset. This allows VILA-U to perform comparably to more complex models\nusing a fully token-based autoregressive framework. Our code is open sourced at\nhttps:\/\/github.com\/mit-han-lab\/vila-u.\n1\nINTRODUCTION\nIn recent years, large language models (LLMs) have demonstrated superior capabilities in various\nlanguage tasks. Their appealing properties like instruction following, zero-shot generalization, and\nfew-shot in-context learning motivate researchers to combine them with vision models to build visual\nlanguage models (VLMs) for multi-modal tasks. Many efforts (Dai et al., 2024; Liu et al., 2024b; Lin\net al., 2023) in this field have achieved remarkable performance on visual language understanding.\nIn these works, visual inputs are projected onto LLMs’ semantic space through a vision model like\nCLIP (Radford et al., 2021) to bridge two modalities by including text-image alignment objectives.\nIn addition to visual understanding, another essential research direction in combining visual and\nlanguage modalities is visual generation. There are two popular approaches for text-guided image\ngeneration. One approach employs diffusion models (Rombach et al., 2022a), a powerful tool for\nvarious generation tasks. The other line of work converts visual content into discrete tokens through\nvector quantization (VQ) and then leveraging autoregressive transformers for high-quality and diverse\ngeneration (Esser et al., 2021; Yu et al., 2021; Lee et al., 2022; Tian et al., 2024b; Sun et al., 2024).\nWitnessing the rapid advancements in both visual understanding and generation, an emerging trend\nis to unify these techniques into a single multi-modal framework. Prior to VILA-U, there are two\nmain approaches to achieving such unification: (1) One approach (Liu et al., 2024a; Yu et al., 2023a;\nXie et al., 2024) utilizes a VQGAN-based (Esser et al., 2021) tokenizer to convert visual inputs\ninto discrete tokens and leverages an autoregressive model for both understanding and generation.\nHowever, (Xie et al., 2024) has shown that visual tokens from VQGAN-based encoder lack semantic\ninformation and usually results in a severe performance drop in downstream visual understanding\ntasks. (2) Another approach (Zhan et al., 2024; Ge et al., 2023b; Jin et al., 2023) utilizes a codebook\nto quantize features produced by a pre-trained vision model like CLIP. Since CLIP features encode\n∗Equal Contribution.\n†Part of the work done during an internship at NVIDIA.\n1\narXiv:2409.04429v3  [cs.CV]  4 Mar 2025\nPublished as a conference paper at ICLR 2025\nrich semantic information, these approaches generally achieve significantly better performance on\nunderstanding tasks. However, these tokenizers lack decoding capability, requiring an external\nvisual generation model, such as a diffusion model, to use the generated visual tokens as conditions\nfor producing visual outputs. This approach adds complexity to infrastructure design. Available\nlarge-scale foundation model training pipelines and deployment systems have already been highly\noptimized for language modeling with next-token prediction. Designing and maintaining an additional\nstack to support diffusion models would incur significant engineering costs.\nIn this work, we present VILA-U, an end-to-end autoregressive framework with a unified next-token\nprediction objective for both visual and text inputs that can achieve competitive performance on both\nvisual language understanding and generation tasks, without the help of external components like\ndiffusion models. We identify two critical principles to unify vision and language modalities: (1)\nExisting unified end-to-end autoregressive VLMs cannot achieve competitive visual understanding\nperformance because the discrete VQGAN tokens are trained solely on image reconstruction loss and\nare not aligned with textual inputs. Therefore, it is crucial to introduce text alignment during VQ\nvision tower pretraining to enhance perception capabilities. (2) Autoregressive image generation can\nattain similar quality as diffusion models if trained on high-quality data with sufficient size. Guided\nby these insights, VILA-U features a unified foundation vision tower that converts visual inputs\ninto discrete tokens through vector quantization and aligns these tokens with textual inputs using\ncontrastive learning. The multi-modal training of VILA-U takes advantage of a unified next-token\nprediction objective for both visual and textual tokens on a small-size high-quality image-text corpus.\nWe evaluate VILA-U on common visual language tasks, including image-language understanding,\nvideo-language understanding, image generation and video generation. VILA-U significantly nar-\nrows the gap in visual understanding performance between end-to-end autoregressive models and\ncontinuous-token VLMs, while introducing competitive native visual generation capabilities.\n2\nRELATED WORK\nLarge Language Models (LLMs). LLMs based on pre-trained large-scale transformers (Vaswani\net al., 2017) has drastically revolutionized natural language processing field. Featuring gigantic\nmodel size and pre-training data corpus, LLM has achieved remarkable performance on various\nlinguistic tasks. The development of open-source LLMs such as LLaMA (Touvron et al., 2023a),\nMixtral (Jiang et al., 2024) and Vicuna (Chiang et al., 2023) has furthered nourished research on how\nto adopt LLM for complex language tasks. Besides excellent zero-shot generalizability to diverse\ndomains, LLM is commonly finetuned on custom datasets for better performance on specific tasks.\nInstruction tuning (OpenAI, 2023; Chung et al., 2024; Ouyang et al., 2022) also stands as a key step\nfor better outputs in applying LLMs. In this work, we adopt the LLaMA-2-7B (Touvron et al., 2023a)\nmodel as our basic LLM.\nVisual Language Models (VLMs). Combining computer vision and natural language processing\ngives rise to VLM in this LLM era. In VLMs, researchers leverage vision foundation models such\nas CLIP (Radford et al., 2021), BLIP (Li et al., 2022) and CoCa (Yu et al., 2022) to extract visual\nfeatures, align with texts, and feed them into LLM to achieve the cross-modality understanding\nbetween texts and visual content. Building upon such progress, many VLMs (Alayrac et al., 2022;\nLi et al., 2023b; Liu et al., 2024b; Lin et al., 2023; Luo et al., 2024; Tian et al., 2024a) have been\ndesigned and trained on extensive vision-language data to achieve remarkable performance on visual\nunderstanding and reasoning tasks. In this work, we aim to develop a VLM with visual understanding\ncapacities comparable to prior works, while also possessing the new capacity of visual generation.\nUnified Visual Language Models. Numerous efforts have been made to develop unified visual\nlanguage models capable of generating both text and visual content, including images and videos.\nThere are two mainstream methods to generate visual content in VLMs. Many works (Sun et al.,\n2023b;a; Jin et al., 2023; Ge et al., 2023b; Li et al., 2023c; Ge et al., 2024; Jin et al., 2024; Ge et al.,\n2023a) combine VLMs with diffusion models like Stable Diffusion (Rombach et al., 2022a) for\nhigh-quality image generation. Other works (Liu et al., 2024a; Yu et al., 2023a; Lu et al., 2023; Team,\n2024; Xie et al., 2024) adopt VQGAN-based vision encoders to convert visual inputs into discrete\ntokens and make LLMs learn to predict them. For more details on the distinction between our method\nand other unified visual language models, please refer to Appendix A.\n2\nPublished as a conference paper at ICLR 2025\n📄\n🖼\n🎞\nText\nImage\nVideo\nThe man is \nskating\n……………\nMulti-modal \nInputs\nMulti-modal Token \nSequence In\nMulti-modal Token \nSequence Out\nMulti-modal \nInference Outputs\nNext-token Prediction\n…\nText Tokens\nText-aligned Discrete  \nVisual Tokens\n…\n…\n…\n…\n…\n…\n…\nText-aligned \nVision \nEncoder\nText \nEncoder\nText \nDecoder\nVision \nDecoder\nGenerative  \nMulti-modal Model\n……………\nFigure 1: An overview of our framework’s multi-modal training and inference process. Visual\ninputs are tokenized into discrete tokens and concatenated with textual tokens to form a multi-modal\ntoken sequence. All tokens are involved in our next-token prediction process, enabling a unified\ntraining objective. During inference, the output tokens are decoded by our text detokenizer or vision\ntower decoder to yield multi-modal content.\n3\nMETHODS\nThis work proposes a multi-modal framework that aims to unify visual and language modalities\neffectively. The key components enabling such unification are a unified foundation vision tower that\nconverts visual inputs into discrete tokens aligned with text, and a unified multi-modal generative\ntraining procedure. An overview of the main multi-modal training and inference process within our\nframework is depicted in Figure 1.\n3.1\nUNIFIED FOUNDATION VISION TOWER\nTo support diverse visual understanding and generation tasks, we first build a unified foundation\nvision tower to provide appropriate visual features. We propose to include text-image contrastive loss\nand VQ-based image reconstruction loss in our vision tower training, empowering the text alignment\nand discrete tokenization abilities for our vision tower. As depicted in Figure 2, the features extracted\nfrom images are primarily discretized through residual quantization. Then in one route, the discrete\nvisual features are fed into a decoder to reconstruct the image and compute the reconstruction loss;\non the other route, we compute the image-text contrastive loss between the discrete visual features\nand the textual features provided by a text encoder. With this training procedure, the vision tower\nlearns to extract discrete features suitable for both understanding and generation in our VLM.\nUnified Training Recipe.\nTraining the unified vision tower with two objectives from scratch would\nbe difficult, because alignment and reconstruction tasks require high-level semantic and low-level\nappearance features, respectively. Training the entire vision tower from scratch with both objectives\ncould induce conflicting goals. In practice, we observe that training the vector-quantized vision tower\nfrom scratch with both image reconstruction and contrastive loss results in a mere 5% Top-1 accuracy\nfor zero-shot image classification on ImageNet (Deng et al., 2009a) after several epochs of training.\nTo address this issue, we experiment with different training recipes (failed recipes are listed in\nAppendix C) and find the following solution to be most effective. Instead of learning both objectives\nsimultaneously, we suggest first equipping the model with text-image alignment ability and then\nlearning reconstruction while maintaining alignment ability. We initialize the vision encoder and\ntext encoder with pretrained weights from the CLIP model to ensure good text-image alignment.\nNext, we freeze the text encoder and keep all vision components trainable using both contrastive and\nreconstruction loss. The contrastive loss maintains alignment ability, while the reconstruction loss\ndevelops reconstruction ability. This approach converges quickly and yields strong performance. The\npre-trained CLIP weights contain learned high-level priors, which are difficult and computationally\nexpensive to learn from scratch. Initializing with these weights enables the binding of low-level and\nhigh-level features much faster and more tractably for the vision encoder. With this recipe, we can\ntrain a vision tower that exhibits both good text alignment and image reconstruction abilities. We use\nweighted sum to combine the text-image contrastive loss and VQ-based image reconstruction loss:\nLtotal = wcontraLcontra + wreconLrecon\n(1)\nIn our experiments, we pick wcontra = 1 and wrecon = 1.\n3\nPublished as a conference paper at ICLR 2025\nVision \nEncoder\nVision \nDecoder\nResidual \nQuantizer\nText Encoder\nThis is an image \nof a cute cat\n🔥\n🔥\nPretrained\nPretrained\n🔥\n…\nText Tokens\nDiscrete Visual Tokens\n…\nContrastive  \nLoss\nReconstruction  \nLoss\n🔥\nFrozen weights\nTrainable weights\nFigure 2: Overview of our unified foundation vision tower. Given input images the features\nextracted by the vision encoder are discretized using residual quantization. Then the discrete vision\nfeatures are meanwhile put into the vision decoder to reconstruct images and used to perform the\ntext-image alignment. During this process, the reconstruction loss and contrastive loss are computed\nto update the vision tower, endowing it to produce discrete visual features with text alignment.\nResidual Vector Quantization.\nOur visual features are discretely quantized, so their representation\nability heavily depends on the code size used in our quantizer. Since we hope they contain both\nhigh-level and low-level features, we need more capacities in their vector feature space, making a\nlarger code size necessary for good performance in downstream tasks. However, too many codes\nfor each image will result in too many tokens for LLM to produce in the visual generation process,\nincurring much latency. So in an attempt to increase the vector feature capacity and meanwhile\nmaintain a reasonable number of tokens for LLM, we adopt a residual vector quantization method\nfollowing RQ-VAE (Lee et al., 2022) to discretize a vector z as D discrete codes:\nRQ(z; C, D) = (k1, · · · , kD) ∈[K]D,\n(2)\nwhere C is the codebook, K = |C| and kd is the code of z at depth d. Starting with r0 = z, we\nrecursively perform vector quantization by\nkd = Q (rd−1, C) ,\nrd = rd−1 −e (kd) ,\n(3)\nfor each depth d = 1, 2, · · · , D, where e is the codebook embedding table and Q is the standard\nvector quantization:\nQ(z; C) = arg min\nk∈[K]\n∥z −e(k)∥2\n2.\n(4)\nThe quantized vector for z is the sum over the depth dim: bz = PD\ni=1 e (ki). Intuitively, in each depth\nwe choose a code to reduce the quantization error. So compared to the standard vector quantization\nmethods, we have D codes to quantize one vector, allowing for finer approximation and larger feature\nspace. During multi-modal training and inference, LLM only needs to predict the code embedding,\nwith codes in different depth sequentially produced by a depth transformer taking the code embedding\nas the initial input, as we will introduce in Section 3.2. So with this residual quantization, we can\nenhance the representation capability of our vision tower while incurring little latency.\n3.2\nUNIFIED MULTI-MODAL GENERATIVE PRE-TRAINING\nFigure 1 presents an overview of our unified multi-modal pre-training process. Our vision tower\nencoder processes visual inputs sequentially, generating a 1D token sequence. This sequence is then\nconcatenated with text tokens to form a multi-modal sequence. To distinguish between modalities and\nenable visual content generation, we insert special tokens: <image_start> and <image_end>\nat the start and end of image tokens, and <video_start> and <video_end> at the start and\nend of video tokens. Video tokens are the direct concatenation of multi-frame image tokens.\nPre-training data form. In terms of unified pre-training data, we leverage different concatenation\nforms between text and visual tokens to facilitate both understanding and generation. We use\n[image, text], [text, image], and [text, video] forms, with supervision loss added\nonly on the latter modality in each pair to avoid unconditional content generation and promote\nmodality alignment. We also employ an interleaved text and image concatenation form for enhanced\nunderstanding, with supervision loss applied solely to the text. Notably, we exclude the [video,\ntext] form during pre-training for efficiency reasons, as we find incorporating it during supervised\nfine-tuning effectively yields excellent video understanding ability.\n4\nPublished as a conference paper at ICLR 2025\nTraining Objective. Since both visual tokens and text tokens are discrete, we can train our LLM with\nthe general language modeling next-token prediction objective. However, due to the use of residual\nquantization for visual tokens, the training objectives for text and visual tokens differ slightly. For\ntext tokens, the negative log-likelihood loss is calculated as\nLtext = −\nT\nX\ni=1\nlog Pθ (yi|y<i) ,\n(5)\nwhere T is the length of the multi-modal sequence and i only counts when the text token appears at\nposition i. For visual tokens, residual quantization introduces a depth-stacked structure of codes at\neach visual position j. To address this, we leverage the depth transformer introduced in RQ-VAE\n(Lee et al., 2022). Specifically, given the code embedding hj generated by the LLM for visual\ntokens at position j, the depth transformer autoregressively predicts D residual tokens (kj1, ..., kjD).\nDuring training, the input of the depth transformer vjd at depth d is defined as the sum of the code\nembeddings of up to depth d −1 for d > 1 such that\nvjd =\nd−1\nX\nd′=1\ne(kjd′),\n(6)\nand vj1 = hj. Thus, the depth transformer predicts the next code for a finer estimation of the feature\nˆzj based on the previous estimations up to d −1. Then the negative log-likelihood loss for visual\ntokens is\nLvisual = −\nT\nX\nj=1\nD\nX\nd=1\nlog Pδ (kjd|kj,<d) ,\n(7)\nwhere T is the length of the multi-modal sequence and j only counts when a visual token appears at\nposition j. During the multi-modal pre-training, the weights of the depth transformer are randomly\ninitialized and updated together with the LLM.\n4\nEXPERIMENTS\nIn this section, we introduce comprehensive experiments to evaluate our method on various visual\nunderstanding and generation tasks. Firstly, we outline our experimental setup, including the model\narchitecture, training datasets, and evaluation benchmarks. Subsequently, we evaluate the performance\nof our unified foundation vision tower. Then, we compare our method with other popular VLMs on\nvarious visual understanding and generation benchmarks. Finally, we give some qualitative results.\n4.1\nEXPERIMENTAL SETUP\nIn our experiments, we employ LLaMA-2-7B (Touvron et al., 2023b) as our base language model.\nFor the vision tower, we choose SigLIP-Large-patch16-256 \/ SigLIP-SO400M-patch14-384 (Zhai\net al., 2023) as our vision encoder architecture, and adopt the residual quantizer, depth transformer\nas well as the decoder architecture from RQ-VAE (Lee et al., 2022). The quantizer codebook size\nis 16384. All images and videos are resized to a resolution of 256 × 256 \/ 384 × 384, with each\nimage or video frame converted into a 16 × 16 × 4 \/ 27 × 27 × 16 code with the residual depth\nD = 4 \/ D = 16. We train our vision tower on COYO-700M (Byeon et al., 2022) and evaluate\nit for zero-shot classification and reconstruction performance on ImageNet (Deng et al., 2009b).\nFor visual understanding, we leverage 1M [image, text] data from ShareGPT4V (Chen et al.,\n2023), 6M interleaved text and image data from MMC4 (Zhu et al., 2024). For visual generation,\nwe incorporate 15M high-quality [text, image] data curated from our internal dataset and 1M\n[text, video] data from OpenVid (Nan et al., 2024) datasets. Classifier-free guidance (Ho &\nSalimans, 2022) is employed for visual generation with a CFG value of 3.\nFor examining visual understanding ability, we evaluate our model on the widely adopted zero-shot\nimage-based visual-language benchmarks including VQAv2 (Goyal et al., 2017), GQA (Hudson &\n5\nPublished as a conference paper at ICLR 2025\nManning, 2019), TextVQA (Singh et al., 2019), POPE (Li et al., 2023d), MME (Fu et al., 2024),\nSEED (Li et al., 2023a), MM-Vet (Yu et al., 2023b) and video-based visual-language benchmarks\nincluding ActivityNet (Caba Heilbron et al., 2015), MSVD (Chen & Dolan, 2011), MSRVTT (Xu\net al., 2017), TGIF (Li et al., 2016).\nTo evaluate the visual generation capability, we use MJHQ-30K (Li et al., 2024) and GenAI-Bench\n(Lin et al., 2024) for image generation and VBench (Huang et al., 2024) for video generation.\nMJHQ-30K adopts the FID between generated images and 30K high-quality images to reflect the\noverall capability of image generation. GenAI-Bench is a challenging image-to-text generation\nbenchmark that reflects the comprehensive generative abilities of image generation models. Vbench\nis a comprehensive benchmark suite for video generative models that decomposes the generation\nquality into multiple well-defined dimensions to facilitate fine-grained and objective evaluation.\n4.2\nUNIFIED FOUNDATION VISION TOWER\nWe present the commonly used metrics reconstruction FID (rFID) and Top-1 accuracy for zero-shot\nimage classification on ImageNet to measure the reconstruction and text alignment capabilities of\nthe unified foundation vision tower in Table 1. Please refer to the Appendix B.1 for the qualitative\nreconstruction results. Our model achieves significantly better reconstruction results than VQ-GAN.\nOur rFID is slightly inferior to that of RQ-VAE when using the same code shape. This is expected as\nthe introduction of contrastive loss during training, aimed at enhancing image understanding, led to a\ndecrease in reconstruction quality. For the text alignment capability, our unified vision tower achieves\na Top-1 accuracy of 73.3 \/ 78.0 under 256 \/ 384 resolution. This demonstrates the exceptional text\nalignment capability of our unified vision tower. However, it is worth noting that both the rFID\nand Top-1 accuracy of the vision tower only serves as a medium indicator. As the unified vision\ntower is an integral component of the entire autoregressive model, we believe that its performance on\ndownstream tasks, such as visual understanding and generation, holds greater significance.\nTable 1: The reconstruction FID (rFID) and Top-1 accuracy for zero-shot image classification of our\nunified vision tower on ImageNet.\nModel\nPretrained Weights\nResolution\nShape of Code\nrFID↓\nTop-1 Accuracy↑\nVQ-GAN\n–\n256 × 256\n16 × 16\n4.98\n–\nRQ-VAE\n–\n256 × 256\n8 × 8 × 4\n3.20\n–\nRQ-VAE\n–\n256 × 256\n16 × 16 × 4\n1.30\n–\nOurs\nSigLIP-Large\n256 × 256\n16 × 16 × 4\n1.80\n73.3\nOurs\nSigLIP-SO400M\n384 × 384\n27 × 27 × 16\n1.25\n78.0\n4.3\nQUANTITATIVE EVALUATION\nVisual Understanding Tasks. Table 2 and Table 3 summarize the comparison between our method\nand other leading VLMs on the image-language and video-language benchmarks respectively. Com-\npared to the mainstream choice of continuous visual tokens produced by foundation models like\nCLIP, the VQGAN-based discrete visual tokens have less alignment with text, thus harming VLMs’\nperformance on visual understanding tasks. With our unified foundation vision tower, our model can\nhave a performance close to leading VLMs even with discrete visual tokens.\nMethod\nType\n#Images\nFID↓\nSD v2.1\nDiffusion\n–\n26.96\nSD-XL\nDiffusion\n2000M\n9.55\nPixArt\nDiffusion\n25M\n6.14\nPlayground v2.5\nDiffusion\n–\n4.48\nLWM\nAutoregressive\n–\n17.77\nShow-o\nAutoregressive\n36M\n15.18\nOurs (256)\nAutoregressive\n15M\n12.81\nOurs (384)\nAutoregressive\n15M\n7.69\nTable 4: Comparison with other visual generation methods\non MJHQ-30K evaluation benchmark.\nVisual Generation Tasks. As shown in\nTable 4, VILA-U can achieve a better FID\nthan other autoregressive methods and have\ncomparable performance with some diffu-\nsion based methods. This result shows the\nfeasibility of our method for visual gener-\nation. Table 5 summarizes the quantitative\nresults of our method and other visual gener-\nation methods on GenAI-Bench. Although\nOur method is inferior to diffusion-based\nvisual generation methods that have been\ntrained on billions-level image-text pairs,\n6\nPublished as a conference paper at ICLR 2025\nTable 2: Comparison with leading methods on image-based visual language benchmarks. Our\nperformance is close to leading VLMs, surpassing many methods by a large margin under the same\nLLM size, even with a discrete visual token type. * indicates that images in the training split of these\ndatasets are observed during VLM training.\nMethod\nLLM\nVisual Token\nRes.\nVQAv2\nGQA\nTextVQA\nPOPE\nMME\nSEED\nMM-Vet\nLLaVA-1.5\nVicuna-1.5-7B\nContinuous\n336\n78.5∗\n62.0∗\n58.2\n85.9\n1510.7\n58.6\n30.5\nVILA\nLLaMA-2-7B\nContinuous\n336\n79.9∗\n62.3∗\n64.4\n85.5\n1533.0\n61.1\n34.9\nUnified-IO 2\n6.8B from scratch\nContinuous\n384\n79.4∗\n–\n–\n87.7\n–\n61.8\n–\nInstructBLIP\nVicuna-7B\nContinuous\n224\n–\n49.2\n50.1\n–\n–\n53.4\n26.2\nIDEFICS-9B\nLLaMA-7B\nContinuous\n224\n50.9\n38.4\n25.9\n–\n–\n–\n–\nEmu\nLLaMA-13B\nContinuous\n224\n52.0\n–\n–\n–\n–\n–\n–\nLaVIT\nLLaMA-7B\nContinuous\n224\n66.0\n46.8\n–\n–\n–\n–\n–\nDreamLLM\nVicuna-7B\nContinuous\n224\n72.9∗\n–\n41.8\n–\n–\n–\n36.6\nVideo-LaVIT\nLLaMA-2-7B\nContinuous\n224\n80.2∗\n63.6∗\n–\n–\n1581.5\n64.4\n35.0\nEmu2-Chat\nEmu2-37B\nContinuous\n448\n84.9∗\n65.1∗\n66.6∗\n–\n–\n–\n–\nMM-Interleaved\nVicuna-13B\nContinuous\n224\n80.2∗\n60.5∗\n61.0\n–\n–\n–\n–\nDEEM\nVicuna-7B\nContinuous\n448\n68.2∗\n55.7∗\n–\n–\n–\n–\n37.4\nCM3Leon-7B\n7B from scratch\nDiscrete\n256\n47.6\n–\n–\n–\n–\n–\n–\nLWM\nLLaMA-2-7B\nDiscrete\n256\n55.8\n44.8\n18.8\n75.2\n–\n–\n9.6\nShow-o\nPhi-1.5-1.3B\nDiscrete\n256\n59.3∗\n48.7∗\n–\n73.8\n948.4\n–\n–\nSEED-LLaMA\nVicuna-7B\nDiscrete\n224\n66.2\n–\n–\n–\n–\n51.5\n–\nOurs\nLLaMA-2-7B\nDiscrete\n256\n75.3∗\n58.3∗\n48.3\n83.9\n1336.2\n56.3\n27.7\nOurs\nLLaMA-2-7B\nDiscrete\n384\n79.4∗\n60.8∗\n60.8\n85.8\n1401.8\n59.0\n33.5\nTable 3: Comparison with leading methods on video-based visual language benchmarks. The\nperformance of our method is close to state-of-the-art VLMs, surpassing many methods under the\nsame LLM size, even with a discrete visual token type.\nMethod\nLLM\nVisual Token\nRes.\nMSVD-QA\nMSRVTT-QA\nTGIF-QA\nActivity Net-QA\nUnified-IO 2\n6.8B from scratch\nContinuous\n384\n52.1\n42.5\n–\n–\nEmu\nLLaMA-13B\nContinuous\n224\n–\n18.8\n8.3\n–\nVideoChat\nVicuna-7B\nContinuous\n224\n56.3\n45\n34.4\n–\nVideo-LLaMA\nLLaMA-2-7B\nContinuous\n224\n51.6\n29.6\n–\n–\nVideo-ChatGPT\nLLaMA-2-7B\nContinuous\n224\n64.9\n49.3\n51.4\n35.2\nVideo-LLava\nVicuna-7B\nContinuous\n224\n70.7\n59.2\n70.0\n45.3\nVideo-LaVIT\nLLaMA-2-7B\nContinuous\n224\n73.5\n59.5\n–\n50.2\nEmu2-Chat\nEmu2-37B\nContinuous\n448\n49.0\n31.4\n–\n–\nLWM\nLLaMA-2-7B\nDiscrete\n256\n55.9\n44.1\n40.9\n–\nSEED-LLaMA\nVicuna-7B\nDiscrete\n224\n40.9\n30.8\n–\n–\nOurs\nLLaMA-2-7B\nDiscrete\n256\n73.4\n58.9\n51.3\n51.6\nOurs\nLLaMA-2-7B\nDiscrete\n384\n75.3\n60.0\n51.9\n52.7\nour method has comparable performance with SD v2.1 (Rombach et al., 2022b) and SD-XL (Podell\net al., 2023) on advanced prompts even trained with magnitude-level less data. This further shows\nthat VILA-U can learn the correlation among visual and textual modalities effectively with our unified\ntraining framework. For video generation, we evaluate our method on VBench (Huang et al., 2024)\nand compare it against Open-Sora (Zheng et al.), CogVideo (Hong et al., 2022), and CogVideoX (Yang\net al., 2024). The results, presented in Table 6, demonstrate that our method achieves performance that\nis better than CogVideo and comparable to Open-Sora, highlighting the effectiveness of our approach.\n4.4\nQUALITATIVE EVALUATION\nMethod\nTotal Score↑\nQuality Score↑\nSemantic Score↑\nOpen-Sora\n75.91\n78.82\n64.28\nCogVideo\n67.01\n72.06\n46.83\nCogVideoX\n81.61\n82.75\n77.04\nOurs (256)\n74.01\n76.26\n65.04\nTable 6: Comparison with other visual generation methods on\nVBench (Huang et al., 2024).\nVisual Understanding. To validate\nthe effectiveness of VILA-U in\ncomprehensive visual understanding\ntasks, we apply it in several under-\nstanding and reasoning tasks, as\nsome examples shown in Figure 3\nand Figure 4. From the results, we\ncan see the versatility of VILA-U\nin various tasks including visual\ncaptioning\nand\nvisual\nquestion\nanswering. Besides, our model has inherited some important capabilities from VILA (Lin et al.,\n7\nPublished as a conference paper at ICLR 2025\nTable 5: Comparison with other visual generation methods on GenAI-Bench (Lin et al., 2024).\nThe results show that our method outperforms previous autoregressive visual generation methods.\nFor advanced prompts that require better text following ability to generate, our method can have a\nrelatively small performance gap with diffusion-based methods, even with much less training data.\nMethod\nType\n#Training Images\nAttribute↑\nScene↑\nRelation↑\nOverall↑\nSpatial\nAction\nPart\nSD v2.1\nDiffusion\n2000M\n0.80\n0.79\n0.76\n0.77\n0.80\n0.78\nSD-XL\nDiffusion\n2000M\n0.84\n0.84\n0.82\n0.83\n0.89\n0.83\nMidjourney v6\nDiffusion\n–\n0.88\n0.87\n0.87\n0.87\n0.91\n0.87\nDALL-E 3\nDiffusion\n–\n0.91\n0.90\n0.92\n0.89\n0.91\n0.90\nLWM\nAutoregressive\n–\n0.63\n0.62\n0.65\n0.63\n0.70\n0.63\nShow-o\nAutoregressive\n36M\n0.72\n0.72\n0.70\n0.70\n0.75\n0.70\nOurs (256)\nAutoregressive\n15M\n0.78\n0.78\n0.77\n0.78\n0.79\n0.76\nOurs (384)\nAutoregressive\n15M\n0.75\n0.76\n0.75\n0.73\n0.75\n0.73\n(a) VQAScores on basic prompts of GenAI-Bench\nMethod\nType\n#Training Images\nCount↑\nDiffer↑\nCompare↑\nLogical↑\nOverall↑\nNegate\nUniversal\nSD v2.1\nDiffusion\n2000M\n0.68\n0.70\n0.68\n0.54\n0.64\n0.62\nSD-XL\nDiffusion\n2000M\n0.71\n0.73\n0.69\n0.50\n0.66\n0.63\nMidjourney v6\nDiffusion\n–\n0.78\n0.78\n0.79\n0.50\n0.76\n0.69\nDALL-E 3\nDiffusion\n–\n0.82\n0.78\n0.82\n0.48\n0.80\n0.70\nLWM\nAutoregressive\n–\n0.59\n0.58\n0.54\n0.49\n0.52\n0.53\nShow-o\nAutoregressive\n36M\n0.70\n0.62\n0.71\n0.51\n0.65\n0.60\nOurs (256)\nAutoregressive\n15M\n0.70\n0.71\n0.74\n0.53\n0.66\n0.64\nOurs (384)\nAutoregressive\n15M\n0.68\n0.67\n0.71\n0.51\n0.64\n0.61\n(b) VQAScores on advanced prompts of GenAI-Bench\n2023) including multi-image understanding, in-context learning, as shown in Figure 5 and Figure 6.\nMore visualizations can be found in the Appendix B.2 and B.3.\nVisual Generation. We present some examples of the visual generation results in Figure 7. Our model\ncan be employed in both image generation and video generation, even trained with a relatively small\ndata corpus. In the given examples, our method can generate nice-looking images and continuous\nvideos adhering to the user’s input. More visualizations can be found in the appendix B.4.\n5\nABLATION STUDY\n5.1\nIMPACT OF CONTRASTIVE LOSS TO VISUAL UNDERSTANDING\nWe include contrastive loss in vision tower training, which endows it with the text alignment ability.\nDuring our multi-modal training, such text alignment ability is crucial in enhancing modality fusion\nand performance on downstream visual language tasks. We validate the importance of this alignment\nby training the vision tower with and without the contrastive loss, evaluating its impact on visual\nlanguage understanding performance. For this ablation study, we randomly sample 25M data from\nPrompt: <video>\\n\nElaborate on the visual and narrative elements of the video in detail.\nVILA-U(Ours): The video features a person wearing a blue t-shirt, khaki pants, and a cap, executing a series of breakdancing moves on \na concrete road. The background includes a clear sky, a bridge, and a train passing by. The dancer's movements are dynamic, with spins, \nflips, and handstands. The lighting is bright, suggesting daytime, and the shadows indicate the sun is high. The dancer's attire is casual, \nand the setting appears to be a quiet road with minimal traffic.\nFigure 3: VILA-U can correctly caption videos and cover all the details, thanks to the text alignment\nof our vision encoder.\n8\nPublished as a conference paper at ICLR 2025\nYes\nInput image\nOutput\nQuestion\nIs there water visible?\nWhat are flying in the sky?\nKites\n2\nHow many motorcycles are \nin the picture?\nHow many people are on each \nmotorcycle?\n0\nFigure 4: VILA-U has good visual question\nanswering capability. The images and questions\nare from the test split of VQAv2 dataset.\nis home to camels\nis snowy\nis sunny\nis rainy\n?\nInput few shot examples + target image\nis home to polar bears\nis home to pandas\n?\nOutput\nFigure 5: VILA-U has good in-context learning\ncapability. We feed two image-text pairs and a third\nimage as the context to prompt the VLM.\nPrompt: Image 1: <img1>\\n Image 2: <img2>\\n \nWhat is the common thing and the difference between image 1 and image 2?\nVILA-U(Ours): The common thing in both images is the presence of a \npanda bear. However, the difference between image 1 and image 2 is that \nthe panda bear in image 1 is lying down on a rock, while the panda bear in \nimage 2 is lying down on a log.\nPrompt: Image 1: <img1>\\n Image 2: <img2>\\n \nWhat is the common thing and the difference between image 1 and image 2?\nVILA-U(Ours): The common thing in both images is the presence of trees, \nbut the difference between the two images is the angle of the sunlight. In \nimage 1, the sun is shining through the trees, creating a bright and lively \natmosphere. In contrast, the sun is not visible in image 2, which shows a \ndarker and more subdued forest scene.\nFigure 6: VILA-U can correctly reason over multiple images.\nCOYO-700M to train the vision tower. For multi-modal training, we use ShareGPT4V and MMC4\nwithout text-image and text-video data. The results of the first two lines in Table 7 demonstrate the\ncrucial role of text alignment in achieving strong visual language understanding performance. Scaling\nthe dataset size from 25M to 700M further enhances performance, highlighting the importance of\nlearning text alignment on a large-scale dataset.\nTable 7: Impact of contrastive loss to visual understanding.\nPretrained Weights\nData size\nLoss Type\nTop-1 Accuracy\nVQAv2\nPOPE\nMME\nSEED\nMM-Vet\nSigLIP-Large\n25M\nRecon.\n–\n57.7\n75.1\n937.7\n38.7\n15.3\nSigLIP-Large\n25M\nRecon. + Contra.\n62.9\n68.0\n83.7\n1219\n50.4\n20.8\nSigLIP-Large\n700M\nRecon. + Contra.\n73.3\n75.3\n83.9\n1336.2\n56.3\n27.7\n5.2\nIMPACT OF CONTRASTIVE LOSS TO VISUAL GENERATION\nWe conduct two experiments to demonstrate the influence of contrastive loss to generation perfor-\nmance. For efficiency, we conduct only text-to-image pretraining and utilize Sheared-LLaMA-1.3B\n(Xia et al., 2023) instead of LLaMA-2-7B as the LLM. In the first experiment, we use the RQ-VAE as\nthe vision tower, which has an rFID of 1.30. In the second experiment, we employ our unified vision\ntower. Results are shown in Table 8. Our Unified Vision Tower yielded slightly worse FID results\nthan the RQ-VAE on MJHQ-30K, possibly due to its inferior rFID resulting from the contrastive loss.\nTable 8: Impact of contrastive loss to visual generation.\nVision Tower\nLLM\nResolution\nrFID ↓\nFID ↓\nRQ-VAE (Lee et al., 2022)\nSheared-LLaMA-1.3B\n256 × 256\n1.30\n12.0\nOurs\nSheared-LLaMA-1.3B\n256 × 256\n1.80\n13.2\nTable 9: Impact of CFG.\nCFG Value\nFID ↓\n1.0\n14.1\n2.0\n13.0\n3.0\n12.8\n5.0\n13.2\n9\nPublished as a conference paper at ICLR 2025\nHappy dreamy owl monster sitting on a \ntree branch, colorful glittering particles, \nforest background, detailed feathers:\nA black dog:\nA cute orange kitten sliding down an aqua \nslide, happy excited. Vibrant colors, water \nsplashing on the lens:\nSelfie of a woman and her lion cub \non the plains:\nCrocodile in a sweater:\na handsome 24 years old boy in the \nmiddle with sky color background \nwearing eye glasses, it's super detailed \nwith anime style:\nA realistic landscape shot of the \nNorthern Lights dancing over a \nsnowy mountain range in Iceland:\nA deep forest clearing with a \nmirrored pond reflecting a galaxy-\nfilled night sky:\nWaves rolling on the sea:\nFireworks exploding in the sky:\nFigure 7: VILA-U can generate high-quality images and videos given text input.\n5.3\nIMPACT OF CLASSIFIER-FREE GUIDANCE\nWe adopt classifier-free guidance during the visual content generation. We investigate the impact of\nthe CFG value on our 256-resolution model. Results presented in Table 9 indicate that a CFG value\nof 3.0 yields the best FID score.\n6\nCONCLUSION AND LIMITATION\nWe present VILA-U, a novel and unified visual language model that integrates video, image and\nlanguage understanding and generation tasks into one autoregressive next-token prediction framework.\nOur method is not only more concise than most VLMs that leverage additional components like\ndiffusion models for unifying visual generation and understanding, but also demonstrates that\nautoregressive methods can achieve comparable performance to state-of-the-art VLMs. We believe\nVILA-U can serve as a general-purpose framework for diverse visual language tasks.\nAs demonstrated in Section 5.2, the introduction of contrastive loss impacts the reconstruction ability\nof the vision tower. Balancing these two capabilities within the unified vision tower presents an\ninteresting and complex challenge that requires further exploration. Additionally, we currently do\nnot observe significant synergy or mutual enhancement between understanding and generation tasks.\nIn the future, we aim to investigate and explore more effective methods to enable these tasks to\ncomplement and reinforce each other, thereby fully realizing the untapped potential of a unified visual\nlanguage model.\n10\nPublished as a conference paper at ICLR 2025\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Sae-\nhoon Kim. Coyo-700m: Image-text pair dataset. https:\/\/github.com\/kakaobrain\/\ncoyo-dataset, 2022.\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet:\nA large-scale video benchmark for human activity understanding. In Proceedings of the ieee\nconference on computer vision and pattern recognition, pp. 961–970, 2015.\nDavid Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In\nProceedings of the 49th annual meeting of the association for computational linguistics: human\nlanguage technologies, pp. 190–200, 2011.\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint\narXiv:2311.12793, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality. See https:\/\/vicuna. lmsys. org (accessed 14 April\n2023), 2(3):6, 2023.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language\nmodels. Journal of Machine Learning Research, 25(70):1–53, 2024.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning. Advances in Neural Information Processing Systems, 36,\n2024.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009a.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009b.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition,\npp. 12873–12883, 2021.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu\nZheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation\nbenchmark for multimodal large language models, 2024.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041, 2023a.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making\nllama see and draw with seed tokenizer. In The Twelfth International Conference on Learning\nRepresentations, 2023b.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying\nShan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation.\narXiv preprint arXiv:2404.14396, 2024.\n11\nPublished as a conference paper at ICLR 2025\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V\nin VQA matter: Elevating the role of image understanding in Visual Question Answering. In\nConference on Computer Vision and Pattern Recognition (CVPR), 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang.\nCogvideo: Large-scale\npretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\nZiqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing\nWu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video\ngenerative models. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 21807–21818, 2024.\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pp. 6700–6709, 2019.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.\nMixtral of experts. arXiv:2401.04088, 2024.\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, CHEN Bin, Chengru Song,\nDi ZHANG, Wenwu Ou, et al. Unified language-vision pretraining in llm with dynamic discrete\nvisual tokenization. In The Twelfth International Conference on Learning Representations, 2023.\nYang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang\nLiu, Di Zhang, Yang Song, et al. Video-lavit: Unified video-language pre-training with decoupled\nvisual-motional tokenization. arXiv preprint arXiv:2402.03161, 2024.\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image\ngeneration using residual quantization. In Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11523–11532, 2022.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-\nmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125,\n2023a.\nDaiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.\n5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint\narXiv:2402.17245, 2024.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image\npre-training for unified vision-language understanding and generation. In ICML, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning, pp. 19730–19742. PMLR, 2023b.\nYanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng\nLiu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models.\narXiv:2403.18814, 2023c.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023d.\nYuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and\nJiebo Luo. Tgif: A new dataset and benchmark on animated gif description. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 4641–4650, 2016.\nJi Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz,\nMohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023.\n12\nPublished as a conference paper at ICLR 2025\nZhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and\nDeva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint\narXiv:2404.01291, 2024.\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and\nlanguage with ringattention. arXiv preprint arXiv:2402.08268, 2024a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36, 2024b.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem,\nand Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision,\nlanguage, audio, and action. arXiv preprint arXiv:2312.17172, 2023.\nRun Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song,\nXiaobo Xia, Tongliang Liu, et al. Deem: Diffusion models serve as the eyes of large language\nmodels for image perception. arXiv preprint arXiv:2405.15232, 2024.\nKepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang,\nand Ying Tai. Openvid-1m: A large-scale high-quality dataset for text-to-video generation. arXiv\npreprint arXiv:2407.02371, 2024.\nOpenAI. Chatgpt. https:\/\/openai.com\/blog\/chatgpt\/, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems, 35:27730–\n27744, 2022.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022a.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022b.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE\/CVF conference\non computer vision and pattern recognition, pp. 8317–8326, 2019.\nPeize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.\nAutoregressive model beats diffusion: Llama for scalable image generation, 2024. URL https:\n\/\/arxiv.org\/abs\/2406.06525.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,\nYongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models\nare in-context learners. arXiv preprint arXiv:2312.13286, 2023a.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023b.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024.\n13\nPublished as a conference paper at ICLR 2025\nChangyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen,\nLewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling\nvia multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024a.\nKeyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling:\nScalable image generation via next-scale prediction, 2024b. URL https:\/\/arxiv.org\/abs\/\n2404.02905.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. arXiv:2302.13971, 2023a.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017.\nURL https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2017\/\nfile\/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language\nmodel pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,\nYuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer\nto unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024.\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\nquestion answering via gradually refined attention over appearance and motion. In Proceedings of\nthe 25th ACM international conference on Multimedia, pp. 1645–1653, 2017.\nZhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang,\nWenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models\nwith an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\narXiv preprint arXiv:2110.04627, 2021.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\nCoca: Contrastive captioners are image-text foundation models, 2022.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2(3), 2023a.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\nand Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv\npreprint arXiv:2308.02490, 2023b.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. In Proceedings of the IEEE\/CVF International Conference on Computer Vision,\npp. 11975–11986, 2023.\nJun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,\nGe Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling.\narXiv preprint arXiv:2402.12226, 2024.\n14\nPublished as a conference paper at ICLR 2025\nZangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou,\nTianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, march 2024.\nURL https:\/\/github. com\/hpcaitech\/Open-Sora, 1(3):4.\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae\nYu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale\ncorpus of images interleaved with text. Advances in Neural Information Processing Systems, 36,\n2024.\n15\nPublished as a conference paper at ICLR 2025\nAPPENDIX\nA\nDIFFERENCE WITH RELATED WORKS\nPrior to VILA-U, unified visual language models were dominated by two mainstream approaches:\n(1) Represented by LWM, CM3Leon and Show-o which utilizes a VQGAN-based tokenizer to\nconvert visual inputs into discrete tokens. However, as these tokenizers are trained solely with a\nreconstruction objective, the resulting tokens lack rich semantic information. This limitation leads to\npoor performance on multimodal understanding tasks. But it can easily support autoregressive visual\ngeneration and the generated visual tokens can be seamlessly decoded into visual outputs using the\nlightweight decoder of VQGAN.\n(2) Represented by AnyGPT SEED-LLaMa and LaViT, which utilizes a codebook to quantize\nfeatures produced by a pre-trained ViT model like CLIP. Since CLIP features encode rich semantic\ninformation, these approaches generally achieve significantly better performance on understanding\ntasks compared to VQGAN-based tokenizers. However, these tokenizers lack decoding capability,\nrequiring an external visual generation model, such as a diffusion model, to use the generated visual\ntokens as conditions for producing visual outputs.\nCompared to these two mainstream approaches, VILA-U introduces a solution that addresses the\nlimitations of both. We design a unified vision tower that extracts features with rich semantic\ninformation, similar to CLIP, while also supporting image reconstruction capabilities akin to VQGAN.\nThis is achieved by incorporating both reconstruction loss and contrastive loss into the autoencoder\ntraining process, along with utilizing residual quantization to enhance the representation capability\nof the visual features. Building on this foundation, we develop a single end-to-end autoregressive\nframework that eliminates the need for external visual generation models required by approach 2 and\nsignificantly outperforms the understanding results of methods in approach 1.\nB\nQUALITATIVE RESULTS\nB.1\nRECONSTRUCTION\nOriginal Image\nReconstruction Image \n(256 resolution)\nReconstruction Image \n(384 resolution)\nFigure 8: Visualization of the reconstruction results from text-aligned discrete visual tokens.\nWe present qualitative reconstruction results in Figure 8 for our 256 \/ 384 resolution vision tower.\nThese vision towers effectively reconstruct images in detail using text-aligned discrete visual tokens.\n16\nPublished as a conference paper at ICLR 2025\nB.2\nVISUAL UNDERSTANDING\nZebras\nInput image\nOutput\nQuestion\nWhat color are they?\nBlack and white\nBlue\nWhat is the color of \nthe far right board?\nHow many men are \nin the picture?\n2\nWhat animals are \nshown here?\nYes\nIs she cooking bread?\nWhat room is this?\nKitchen\nNo\nDid the man hit the bat?\nIs the man's legs \ntogether or apart?\nApart\nFigure 9: Image understanding results. Examples are taken from the test split of VQAv2 dataset.\nPrompt: <video>\\n What do the drops do 5 times ?\nVILA-U(Ours): The drops fall down the window 5 times, and each time they hit the window, they create a new set of drops.\nPrompt: <video>\\n how many guys are laughing ?\nVILA-U(Ours): 2 guys are laughing.\nPrompt: <video>\\n where is the man running and then he fall ?\nVILA-U(Ours): The man is running on a treadmill in a gym, and then he falls off the treadmill.\nPrompt: <video>\\n how many boy holds up the candy bar and opens mouth ?\nVILA-U(Ours): The video shows one boy holding up the candy bar and opening his mouth.\nFigure 10: Video understanding results. Examples are taken from the test split of TGIF dataset.\nWe provide more image understanding and video understanding examples in Figure 9 and Figure 10.\nVILA-U successfully answers the questions accurately.\n17\nPublished as a conference paper at ICLR 2025\nB.3\nIN-CONTEXT LEARNING EXAMPLES\nInput few images + target image\nOutput\nUnderground\nCongress\nSoulomes\n2+1=3\n5+6=11\n3x6=18\nRomanticism\nSurrealism\nImpressionism\nThe company is \nfamous for its \nsearch engine.\nThe company is \nfamous for \niPhone and Mac.\nThe company is \nfamous for its graphics \nprocessing unit.\n3 pandas\n2 dogs\n4 giraffes\nLes sanglots longs \nl’automne blessent \nmon coeur d’une \nlangueur monotone. \nPour qui sont ces \nserpents qui sifflent \nsur vos têtes?\nLes flamants se sont \nformés en un couple, les \ndeux créatures se touchent \nde la tête à la tête, et leur \ntête est touchée.\nFigure 11: In-context learning examples. We try all in-context learning examples in Lin et al. (2023).\nThe results demonstrate that VILA-U has inherited good in-context learning capabilties.\nWe provide more qualitative results to demonstrate in-context learning capabilities of VILA-U in\nFigure 11. VILA-U exhibits good in-context learning capabilties.\nB.4\nVISUAL GENERATION\nA snowy mountain.:\nAn oil painting of a garden where every \nflower is in full bloom, showcasing a \nrainbow of colors.:\nA cube made of denim:\nAn extreme close-up of an gray-\nhaired man with a beard in his 60s:\nAn elephant walking under \nThe sea:\nKnolling of a drawing tools for \npainter:\nA man looks up at the starry sky, \nlonely and ethereal:\nDrone view of waves crashing \nagainst the rugged cliffs in Big Sur.:\nFigure 12: Image generation results. VILA-U can generate high-quality images given text input.\n18\nPublished as a conference paper at ICLR 2025\nBurning campfire in the forest:\nSunlight shining through leaves:\nFigure 13: Video generation results. VILA-U can generate high-quality videos given text input.\nWe provide more image generation and video generation examples in Figure 12 and Figure 13.\nVILA-U can generate high-quality images and videos given text input.\nC\nFAILED TRAINING RECIPES.\nWe experiment with numerous training recipes and find none to be as effective as our final approach.\nWe list four alternative recipes and discuss their shortcomings compared to our final recipe: 1) Load\npre-trained CLIP weights into the text encoder only; 2) Load pre-trained RQ-VAE weights for the\nvision encoder and decoder while training other parts from scratch; 3) Freeze the vision encoder; 4)\nMake the text encoder trainable.\nRecipes 1) and 2) fail due to the lack of pre-trained CLIP weights for the vision encoder. Training a\nCLIP model from scratch typically requires numerous GPU days with a large global batch size (e.g.,\n32k). However, VQ-based reconstruction training necessitates a relatively small global batch size\n(e.g., 512) for steady improvement. With such a small batch size, training a text-aligned vision tower\nfrom scratch would be prohibitively time-consuming and resource-intensive.\nRecipe 3) fails because freezing the vision encoder prevents it from learning the low-level features\nessential for reconstruction. In this case, the burden of reconstruction falls entirely on the vision\ndecoder, but it is impossible to reconstruct images well using only semantic features.\nRecipe 4) fails because the quantized features are chaotic during the initial training steps, and the\ncontrastive loss disrupts the text encoder weights, slowing down the entire training process.\nIn contrast, our final training recipe leverages pre-trained CLIP weights for the vision encoder,\nenabling it to maintain learned semantic features rather than grasping them from scratch. This allows\nus to train with a small batch size while keeping the vision encoder trainable, facilitating the learning\nof low-level features for reconstruction during training.\n19\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation.pdf"}
{"title":"Show-o: One Single Transformer to Unify Multimodal Understanding and Generation","authors":"Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou","summary":"We present a unified transformer, i.e., Show-o, that unifies multimodal\nunderstanding and generation. Unlike fully autoregressive models, Show-o\nunifies autoregressive and (discrete) diffusion modeling to adaptively handle\ninputs and outputs of various and mixed modalities. The unified model flexibly\nsupports a wide range of vision-language tasks including visual\nquestion-answering, text-to-image generation, text-guided\ninpainting\/extrapolation, and mixed-modality generation. Across various\nbenchmarks, it demonstrates comparable or superior performance to existing\nindividual models with an equivalent or larger number of parameters tailored\nfor understanding or generation. This significantly highlights its potential as\na next-generation foundation model. Code and models are released at\nhttps:\/\/github.com\/showlab\/Show-o.","url":"http:\/\/arxiv.org\/abs\/2408.12528v6","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.12528v6","published":1724344352000,"comment":"Technical Report","pdf_text":"Technical Report\nSHOW-O: ONE SINGLE TRANSFORMER TO UNIFY\nMULTIMODAL UNDERSTANDING AND GENERATION\nJinheng Xie1† Weijia Mao1† Zechen Bai1† David Junhao Zhang1† Weihao Wang2\nKevin Qinghong Lin1 Yuchao Gu1 Zhijie Chen2 Zhenheng Yang2 Mike Zheng Shou1∗\n1 Show Lab, National University of Singapore\n2 ByteDance\nABSTRACT\nWe present a unified transformer, i.e., Show-o, that unifies multimodal under-\nstanding and generation. Unlike fully autoregressive models, Show-o unifies au-\ntoregressive and (discrete) diffusion modeling to adaptively handle inputs and out-\nputs of various and mixed modalities. The unified model flexibly supports a wide\nrange of vision-language tasks including visual question-answering, text-to-image\ngeneration, text-guided inpainting\/extrapolation, and mixed-modality generation.\nAcross various benchmarks, it demonstrates comparable or superior performance\nto existing individual models with an equivalent or larger number of parameters\ntailored for understanding or generation. This significantly highlights its poten-\ntial as a next-generation foundation model.\nCode and models are released at\nhttps:\/\/github.com\/showlab\/Show-o.\n1\nINTRODUCTION\n“Alone we can do so little; together we can do so much.” – Helen Keller\nOver the past few years, significant advancements have blossomed in the two key pillars of mul-\ntimodal intelligence: understanding and generation (as depicted in Fig. 1(a) and (b)). For mul-\ntimodal understanding, Multimodal Large Language Models (MLLMs) like LLaVA (Liu et al.,\n2024c) have demonstrated exceptional capabilities in vision-language tasks such as visual question-\nanswering (VQA). For the other pillar of visual generation, denoising diffusion probabilistic models\n(DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020b) have revolutionized the traditional gen-\nerative paradigms (Kingma & Welling, 2013; Goodfellow et al., 2014), achieving unprecedented\nperformance in text-to-image\/video generation (Podell et al., 2023; Esser et al., 2024; Ho et al.,\n2022; Wu et al., 2023a).\nGiven these achievements in individual fields, it is natural to explore the potential of connecting\nthem. Recent works (Wu et al., 2023b; Ge et al., 2024; Ye et al., 2024a; Tang et al., 2024; Dong\net al., 2024) have tried to assemble expert models from these two different domains to form a unified\nsystem that can handle both multimodal understanding and generation. However, existing attempts\nmainly treat each domain independently and often involve individual models responsible for under-\nstanding and generation separately (as shown on the left of Fig. 1(c)). For instance, NExT-GPT (Wu\net al., 2023b) employs a base language model for multimodal understanding but requires an addi-\ntional pre-trained diffusion model for image generation. Nonetheless, the mainstream understanding\nmodels like LLaVA are of transformer architecture (Vaswani et al., 2017b) while each leading gen-\neration models like Stable Diffusion 3 (SD3) (Esser et al., 2024) are just another transformer. This\nmotivates a research question: can one single transformer handle both multimodal understand-\ning and generation?\nVery recently, Chameleon (Team, 2024) demonstrated this is possible. Specifically, Chameleon en-\nables an early fusion of different modalities to generate both text and image tokens through the\nsame manner of autoregressive modeling. While it is reasonable to model text tokens autoregres-\nsively (Touvron et al., 2023; Liu et al., 2024c), it is less clear whether it is better to model im-\n†Equal Contribution.\n∗Corresponding Author.\n1\narXiv:2408.12528v6  [cs.CV]  21 Oct 2024\nTechnical Report\nLLM\n(a) Understanding Only\nLLM (AR)\n(b) Generation Only\nShow-o (Ours)\nDiffusion\ne.g., LWM, Chameleon\ne.g., NExT-GPT, SEED-X\nLLM\nDiffusion\n(c) Unified Model (Understanding & Generation)\ne.g., LLaVA\ne.g., LlamaGen\ne.g., Stable Diffusion 3\nVision\nVision\nVision\nLanguage\nLanguage\nLanguage\nLanguage\nLanguage\nLanguage\nLanguage\nLanguage\nVision\nVision\nVision\nVision\nVision\nContinuous\nDiscrete\nAR: Autoregressive\nVision\nLanguage\nLLM (AR + Diffusion)\nLLM (AR)\nLanguage\nFigure 1: Characteristics comparison among understanding only, generation only, and unified (un-\nderstanding & generation) models. “Vision” and “Language” indicate the representations from spe-\ncific input modalities. In this context, “Diffusion” represents both continuous and discrete diffusion.\nage\/video patches (or pixels) autoregressively as well. An apparent and significant bottleneck of\nautoregressively predicting an image is the large number of sampling steps required due to its causal\nattention, particularly when dealing with images\/videos in higher resolution. Further, (continuous)\ndiffusion models (Podell et al., 2023; Esser et al., 2024) have exhibited superior capabilities in visual\ngeneration than autoregressive ones and are in full attention.\nThis motivates us to ponder: can such one single transformer involve both autoregressive and\ndiffusion modeling? Here we envision a new paradigm that text is represented as discrete tokens and\nmodeled autoregressively same with large language models (LLMs), and continuous image pixels\nare modeled using denoising diffusion. However, it is non-trivial to integrate these two distinct\ntechniques into one single network due to the significant differences between discrete text tokens\nand continuous image representations. Another challenge lies in the fact that existing state-of-the-art\ndiffusion models typically rely on two distinct models, i.e., a text encoder to encode text conditional\ninformation and a denoising network to predict noise.\nTo this end, we present a novel unified model, i.e., Show-o, capable of addressing both multimodal\nunderstanding and generation tasks simultaneously with mixed autoregressive and diffusion mod-\neling (as shown in Fig. 2). Specifically, Show-o is built upon a pre-trained LLM and inherits the\nautoregressive modeling capability for text-based reasoning. Inspired by Gu et al. (2022); Chang\net al. (2022), we employ the discrete denoising diffusion to model discrete image tokens instead of\ncontinuous diffusion used in existing works (Ge et al., 2024; Dong et al., 2024). Besides, Show-o\ninherently encodes text conditional information, eliminating additional text encoders. To accommo-\ndate diverse input data and variations of tasks, a text tokenizer and image tokenizer are employed\nto encode them into discrete tokens, and a unified prompting strategy is proposed further to pro-\ncess these tokens into structure sequences as input. Consequently, given an image accompanying\nquestions, Show-o gives the answers autoregressively. When provided only text tokens, Show-o\ngenerates images in a style of discrete denoising diffusion.\nQuantitatively, Show-o demonstrates comparable even better performance to individual models with\nan equivalent or larger number of parameters across benchmarks. In contrast to autoregressively gen-\nerating an image, Show-o requires approximately 20 times fewer sampling steps, exhibiting inherent\npotential in acceleration. Besides, as shown in Fig. 2, Show-o naturally supports various downstream\napplications like text-guided inpainting and extrapolation, without any fine-tuning. Moreover, we\nhave demonstrated that Show-o has the potential for mixed-modality generation like interleaved\n2\nTechnical Report\nvideo keyframe generation with text descriptions. This demonstrates the potential of the unified\nmodel as a feasible paradigm for long-form video generation. Beyond, we investigate the impact of\ndataset scale, image resolution, and different types of image representations (discrete or continuous)\non the multimodal understanding performance, presenting systematic insights for the design of a\nunified model in the future.\nIn Fig. 1, we present a comparison of model characteristics between Show-o and existing represen-\ntative methods across various domains. One can observe that Show-o is a unified model that flexibly\ninvolves existing advanced techniques to comprehensively address multimodal understanding and\ngeneration. Collectively, the main contributions of this paper can be summarized as:\n• We present a unified model, i.e., Show-o, which unifies multimodal understanding and\ngeneration using one single transformer.\n• Show-o innovatively unifies autoregressive and (discrete) diffusion modeling within one\nsingle transformer, demonstrating versatility in handling both text and images distinctly.\n• As a unified model, Show-o demonstrates comparable even better performance to indi-\nvidual baseline models with an equivalent or larger number of parameters in multimodal\nunderstanding and generation benchmarks.\n• Show-o inherently supports various downstream applications like text-based inpainting and\nextrapolation, without necessitating any fine-tuning. Besides, it also demonstrates the po-\ntential for mixed-modality generation.\n• We explore the impact of dataset scale, image resolution, and different types of represen-\ntations (discrete or continuous) on multimodal understanding, providing valuable insights\nfor improving multimodal understanding capabilities of a unified model.\n2\nRELATED WORK\n2.1\nMULTIMODAL UNDERSTANDING\nSignificant advancements in large language models (LLMs) (Touvron et al., 2023; Brown et al.,\n2020; Chowdhery et al., 2023) have inspired the development of multimodal large language mod-\nels (MLLMs) (Li et al., 2024; Yin et al., 2023; Bai et al., 2024). Early MLLM efforts, such as\nLLaVA (Liu et al., 2024c), MiniGPT-4 (Zhu et al., 2023a), and InstructBLIP (Dai et al., 2023),\ndemonstrate notable multimodal understanding capabilities. To integrate LLMs into multimodal do-\nmains, these studies explored projecting features from a pre-trained modal-specific encoder, such\nas CLIP (Radford et al., 2021), into the input space of LLMs, enabling multimodal understand-\ning and reasoning within the transformer backbone. Although there are various design choices of\nMLLM (McKinzie et al., 2024; Tong et al., 2024), such as vision encoder, feature alignment adapter,\nand dataset, the training for most of these models adheres to the autoregressive generation paradigm,\nwhich is shown to be an effective approach of text-generation in LLMs. Despite their strong mul-\ntimodal understanding capabilities, these models primarily focus on visual perception and lack the\nability to generate multimodal outputs beyond text.\n2.2\nVISUAL GENERATION\nAutoregressive models. Transformer models (Vaswani et al., 2017a; Raffel et al., 2020; Radford\net al., 2018; Brown et al., 2020; Touvron et al., 2023) have demonstrated great success of autoregres-\nsive modeling in natural language processing. Inspired by such progress, previous studies (Parmar\net al., 2018; Esser et al., 2021a; Ravuri & Vinyals, 2019; Chen et al., 2020; Kondratyuk et al., 2023)\ndirectly apply the same autoregressive modeling to learn the dependency of image pixels for im-\nage\/video generation. For instance, VideoPoet (Kondratyuk et al., 2023) also employs the decoder-\nonly transformer architecture for synthesizing high-quality videos from multimodal inputs. More\nrecently, LlamaGen (Sun et al., 2024) has demonstrated that large language model architecture like\nLlama (Touvron et al., 2023) can also autoregressively model image tokens, which can accordingly\nobtain decent performance in class-conditional image generation.\nDiffusion models. In recent years, numerous diffusion-based methods (Rombach et al., 2022;\nRamesh et al., 2022b;a; Peebles & Xie, 2023; Bao et al., 2023; Podell et al., 2023; Chen et al.,\n3\nTechnical Report\nMultimodal Understanding\n(Captioning, VQA …)\nQ1: Please \ndescribe this \nimage in detail.\nQ2: Is there a \nrainbow in this \nimage?\nText Tokenizer & Image Tokenizer\nA punk rock frog in a \nstudded leather jacket \nshouting into a \nmicrophone while \nstanding on a boulder.\nA1: The image features a young girl sitting \non the grass, surrounded by a colourful \nbackdrop. She is holding a …\nA2: Yes, there is a rainbow in the image, as \nthe girl is painting a rainbow on the canvas.\nText De-Tokenizer & Image De-Tokenizer\na vibrant \nhot air \nballoon \nfloats over \na clear \nlake.\n…\n…\nShow-o\nVisual Generation \n(Text-to-Image Generation \/ Text-guided Inpainting and Extrapolation)\nMixed-modality generation \n(Video keyframe generation with text descriptions)\nSlicing \navocado.\na woman is \ncutting an \navocado with \na knife …\nSpecial task tokens for distinguishing various tasks\nImage tokens\nText tokens\nSequence with masked tokens\n(Causal & Full Attention)\na dog sitting on the bench.\nsliced \navocado on \na white \nplate.\nFigure 2: An overview of Show-o. The input data, regardless of its modalities, is tokenized and\nthen prompted into a formatted input sequence. Show-o processes text tokens autoregressively with\ncausal attention and image tokens in (discrete) denoising diffusion modeling via full attention, and\nthen generates the desired output. Specifically, Show-o is capable of handling image captioning, vi-\nsual question answering, text-to-image generation, text-guided inpainting\/extrapolation, and mixed\nmodality generation.\n2024; Nichol et al., 2021; Xue et al., 2024; Xie et al., 2023; Wu et al., 2023a) have demonstrated\nexceptional capabilities in text-to-image\/video generation. Typically, the denoising diffusion process\nis operated on the continuous latent space, encoded by a VAE encoder. In this framework, the model\nis tasked with predicting Gaussian noise added to the continuous latent representations. In contrast,\nD3PM (Austin et al., 2021), Mask-predict (Ghazvininejad et al., 2019), ARDM (Hoogeboom et al.,\n2022), MaskGIT (Chang et al., 2022), UniD3 (Hu et al., 2023), and Copilot4D (Zhang et al., 2024)\nformulate a discrete corruption process as an alternative to Gaussian diffusion. As summarized\nby Murphy (2023), specifically, an image is represented as a sequence of discrete tokens using\nimage tokenizers (Esser et al., 2021b; Yu et al., 2023; Gu et al., 2024), and each token is associated\nwith a categorical label. In this way, the token-wise distribution can be transformed into a uniform\ndistribution through a stochastic sampling process. During training, a portion of these tokens is\nrandomly masked, and the model is trained to predict the original values of these masked tokens. In\nthis work, Show-o adopts the discrete diffusion modeling for visual generation.\n2.3\nUNIFIED VISION-LANGUAGE FOUNDATION MODEL\nIn recent years, an increasing number of studies (Ge et al., 2024; Wu et al., 2023b; Tang et al.,\n2024; Ye et al., 2024a; Dong et al., 2024; Aiello et al., 2024) have focused on unified multimodal\nlanguage models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;\nSun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive\nmodeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation\nsystem capable of handling both multimodal understanding and generation tasks. In this approach,\ncontinuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with\ntext tokens and fed into a large language model (LLM) to perform next-word prediction and image\nrepresentation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling\nmultimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a\nfamily of token-based mixed-modal models capable of both comprehending and generating images.\nThis approach represents all modalities as discrete tokens and utilizes a unified transformer-based\narchitecture and trains the model from scratch in an end-to-end manner. Compared to this work,\nwe also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion\nprocess instead of autoregressive modeling for visual generation.\n4\nTechnical Report\nImage corruption by adding different level mask (noise) tokens\nImage generation by iteratively removing mask (noise) tokens\nFigure 3: Illustration of image corruption by adding different level mask (noise) tokens and image\ngeneration by iteratively removing mask (noise) tokens in the absorbing discrete diffusion paradigm.\n3\nPRELIMINARIES\nIn recent years, denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020a) have demon-\nstrated unprecedented performance in text-to-image\/video generation in continuous state spaces,\nparticularly exemplified by the popular Stable Diffusion series (Podell et al., 2023; Esser et al.,\n2024). Concurrently, discrete denoising diffusion probabilistic models (D3PMs) (Austin et al., 2021)\nhave also shown impressive capabilities in modeling data in discrete form, featuring models like VQ-\nDiffusion (Gu et al., 2022) and Copilot4D (Zhang et al., 2024). Further, MaskGIT (Chang et al.,\n2022) and Muse (Chang et al., 2023) have demonstrated a simplified discrete diffusion that can ef-\nfectively model discrete image tokens. Our Show-o model is built upon MaskGIT so that both such\ndiscrete visual and textual tokens can share a unified learning objective format. In the following, we\nprovide preliminaries for diffusion models and draw the connection between discrete diffusion and\nmask token prediction employed in MaskGIT.\nIn diffusion models, the forward process q(x1:T |x0) = QT\nt=1 q(xt|xt−1) corrupts the image data\nx0 ∼q(x0) into latent variables x1, · · · , xT in different noise level. The reverse Markov pro-\ncess is learned to iteratively remove the noises added to the latent variables towards the real image\ndistribution q(x0). In the continuous scenario, the transition distribution q(xt|xt−1) is commonly\ncharacterized by a Gaussian distribution:\nq(xt|xt−1) = N(xt|\np\n1 −βtxt−1, βtI),\n(1)\nwhere the mean is √1 −βtxt−1 and the variance is βt. For images tokenized into K (i.e., the\ncodebook size) categorical random variables xt, xt−1 ∈{1, · · · , K} and given a [MASK] state, the\ntransition distribution is instead formulated by a stochastic transition matrix Qt ∈R(K+1)×(K+1):\nq(xt|xt−1) = Cat(xt|xt−1Qt),\n(2)\nwhere [Qt]ij = q(xt = j|xt−1 = i), xt−1Qt indicates the row vector-matrix product, and\nCat(xt|xt−1Qt) is a categorical distribution over the one-hot row vector xt given by xt−1Qt. When\nthe transition matrix Qt is applied to each image token in a sequence, the marginal and posterior at\ntime step t and t −1, respectively, are formulated as:\nq(xt|x0) = Cat\n\u0000xt|x0Qt\n\u0001\n,\nwhere\nQt = Q1Q2 · · · Qt,\nq(xt−1|xt, x0) = q(xt|xt−1,\b\b\nx0)q(xt−1|x0)\nq(xt|x0)\n= Cat\n \nxt−1|xtQ⊤\nt ⊙x0Qt−1\nx0Qtx⊤\nt\n!\n,\n(3)\nwhere q(xt−1|xt, x0) = q(xt−1|xt) because of the Markov property.\nIn the following, we introduce the Absorbing-Uniform Discrete Diffusion by defining the stochas-\ntic transition matrix Qt as follows:\nQt = Qa\nt Qu\nt ,\n(4)\nwhere em is a one-hot vector with a value of 1 at the index of [MASK] token, Qa\nt = (1 −αt)I +\nαt1e⊤\nm, and Qu\nt = I −βt(I −eme⊤\nm) +\nβt\n(K+1)(1 −em)(1 −em)⊤. Here, αt and βt represent the\nprobabilities of an image token transforming into the [MASK] token and non-[MASK] token at time\nstep t, respectively. Specifically, the matrix form of Qt can be written as:\n5\nTechnical Report\nQt =\n\n\nωt + νt\nνt\nνt\n· · ·\nνt\nαt\nνt\nωt + νt\nνt\n· · ·\nνt\nαt\nνt\nνt\nωt + νt\n· · ·\nνt\nαt\n...\n...\n...\n...\n...\n...\nνt\nνt\nνt\n· · ·\nωt + νt\nαt\n0\n0\n0\n· · ·\n0\n1\n\n\n,\n(5)\nwhere ωt = (1 −αt −βt) and νt =\nβt\n(K+1). Intuitively, during the corruption process, each image\ntoken in the sequence has a probability of αt to be replaced by the [MASK] token, a chance of νt\nto be uniformly diffused, and a probability of ωk + νt remain unchanged. Besides, if a token turns\ninto a [MASK] token, it will stay in the same [MASK] state during the following corruption process.\nLikewise, Qt = Q\na\nt Q\nu\nt can be accordingly derived. An illustration of the image corruption process\nusing [MASK] token is provided in Fig. 3.\nThe evidence-lower bound (ELBO) for the variational diffusion models is:\n−LELBO(x0, θ) = Eq(x1:T |x0)\n\u0014\n−DKL[q(xT |x0) ∥p(xT )]\n|\n{z\n}\nLT\n+ log pθ(x0|x1)\n|\n{z\n}\nL0\n−\nT\nX\nt=2\nDKL[q(xt−1|xt, x0) ∥pθ(xt−1|xt)]\n|\n{z\n}\nLt−1\n\u0015\n.\n(6)\nConsidering the proposition in (Campbell et al., 2022) and the following parameterization of the\nreverse process:\npθ(xt−1|xt) =\nX\nx0\nq(xt−1|xt, x0)pθ(x0|xt),\n(7)\nthe variational lower bound can be further expressed under the image distribution q(x0) (referring\nto the proof provided by Zhang et al. (2024) as detailed in the Appendix A.1):\nEq(x0)[log pθ(x0)] ≥Eq(x0)[−LELBO(x0, θ)] ≥\nT\nX\nt=1\nEq(x0)q(xt|x0)[log pθ(x0|xt)] + C.\n(8)\nWhen deriving this lower bound, the discrete diffusion paradigm can be further simplified by restrict-\ning each image token to be either unchanged or replaced with the [MASK] token, with no possibility\nof becoming other categorical variables. The resulting lower bound is effectively the Cross-Entropy\nloss used in MaskGIT (Chang et al., 2022), which is the mask token prediction to learn a neural net-\nwork pθ to reconstruct masked regions of x0 from the noised xt. In this work, we follow MaskGIT\nto integrate this simplified discrete diffusion paradigm into Show-o because of its simplicity. Fur-\nther, Muse (Chang et al., 2023) has successfully scaled up such a paradigm for text-to-image models\nof 3B parameters using 460M image-text pairs.\n4\nMETHODOLOGY\nOur ultimate goal is to develop a unified model that involves auto-regressive and diffusion modeling\nfor jointly multimodal understanding and generation. Developing such a unified model poses non-\ntrivial challenges, with core issues revolving around: i) How to define the model’s input\/output\nspace; ii) How to unify various kinds of input data from different modalities; iii) How to involve\nboth auto-regressive and diffusion modeling in one single transformer; iv) How to effectively train\nsuch a unified model.\nHerein, we outline our packaged solutions – Show-o, to address the above challenges respectively.\n§ 4.1: Initially, we elucidate the construction of input\/output space by tokenizing text and image\ndata into discrete tokens. § 4.2: We then introduce the default architecture of Show-o and the unified\nprompting strategy to structure the input data and modalities. § 4.2: Additionally, we also illustrate\nhow to incorporate both auto-regressive and diffusion modeling within one single transformer. § 4.3:\nWe finally present a three-stage training pipeline to effectively train the unified model.\n6\nTechnical Report\nText tokens v\nImage tokens u\nSOT\nT2I\nEOT\nSOI\nEOI\nEOI\nText tokens v\nImage tokens u\nSOT\nEOT\nSOI\nEOI\nImage tokens u\nText tokens v\nSOI\nMMU\nEOI\nSOT\nEOT\nMulti-modal Understanding\nVisual Generation\nText tokens v\nImage tokens u\nSOT\nT2I\nEOT\nSOI\nMixed-Modality Generation\nEOI\nMMU\nT2I\nSOI\nEOI\nSOT EOT\nSpecial task tokens\nStart & end of image tokens\nStart & end of text tokens\nFigure 4: Illustration of the proposed unified prompting format.\n4.1\nTOKENIZATION\nGiven that the proposed Show-o is built upon pre-trained LLMs (Li et al., 2023; Touvron et al.,\n2023), it is natural to perform the unified learning on the discrete space. In this way, we maintain a\nunified vocabulary that includes discrete text and image tokens such that the unified model can be\ntasked with the same learning objective, i.e., predicting discrete tokens.\nText Tokenization. Show-o is based on a pre-trained LLM and we utilize the same tokenizer for\ntext data tokenization without any modifications.\nImage Tokenization. Following MAGVIT-v2 (Yu et al., 2023), we train a lookup-free quantizer\nusing around 35M image data. The quantizer maintains a codebook of size K = 8, 192 and encodes\nimages of 256×256 resolution into 16×16 discrete tokens (option (a) in Fig. 5). The reason for\nutilizing MAGVIT-v2 lies in its ease of fine-tuning to serve as a video tokenizer with temporal\ncompression capability, a potential aspect that we intend to explore in the future.\nOption (a)\nImage\nQuantization\nConvNet\nDiscrete\nEmbedding Layer\n(Index to embedding)\nContinuous\nImage\nOption (b)\nQuantization\nConvNet\nContinuous\nProjection Layer\nContinuous\nImage\nOption (c)\nViT\nContinuous\nProjection Layer\nContinuous\nMAGVIT-v2\nMAGVIT-v2\nCLIP\nFigure 5: Optional inputs for multimodal understanding.\nAn alternative approach is to use different\ntokenizers for understanding and generation,\nrespectively. Inspired by existing studies (Liu\net al., 2024c;b), we also extract the con-\ntinuous image representations from the pre-\ntrained MAGVIT-v2 and CLIP-ViT (Radford\net al., 2021) encoder as input for exploring\nthe improvement of multimodal understand-\ning capabilities (options (b) and (c) in Fig. 5).\nWe will present more details and discuss this\nexploration in Section 5.5. In the following\nsections, the default Show-o employs discrete\nimage tokens as input for both multimodal\nunderstanding and generation (option (a) in\nFig. 5). For simplicity, we only elaborate on the default Show-o in the methodology sections.\n4.2\nARCHITECTURE\nShow-o inherits the architecture of existing LLMs such as (Li et al., 2023; Touvron et al., 2023)\nwithout any architecture modifications except for prepending a QK-Norm operation (Dehghani et al.,\n2023; Wortsman et al., 2023; Team, 2024) to each attention layer. We initialize Show-o with the\nweights of a pre-trained LLM and expand the size of the embedding layer by incorporating 8,192\nnew learnable embeddings for discrete image tokens. Unlike state-of-the-art diffusion models that\nrequire an additional text encoder, Show-o inherently encodes text conditional information by itself\nfor text-to-image generation.\nUnified Prompting. To perform unified learning on multimodal understanding and generation, we\ndesign a unified prompting strategy to format various kinds of input data. Given an image-text pair\n(x, y), it is first tokenized into M image tokens u = {ui}M\ni=1 and N text tokens v = {vi}N\ni=1 by\nthe image and text tokenizer, respectively. We form them into an input sequence according to the\ntype of task in the format illustrated in Fig. 4. Specifically, [MMU] and [T2I] are pre-defined task\ntokens that indicate the learning task for the input sequence. [SOT] and [EOT] serve as special tokens\ndenoting the start and end of text tokens, respectively. Similarly, [SOI] and [EOI] are pre-defined\nspecial tokens marking the start and end of image tokens.\n7\nTechnical Report\nText tokens v\nImage tokens u\nText tokens v\nImage tokens u\nText tokens v\nImage tokens u\nText tokens v\nImage tokens u\n(a) Multimodal Understanding \n(b) Text-to-Image Generation\nText tokens v\nText tokens v\n(c) Language Modeling\n(d) Mixed-Modality Generation\nText\nImage\nImage\nText\nText\nImage\nImage\nText\nFigure 6: Omni-Attention Mechanism (The dark squares represent ‘allow to attend’, while the white\nsquares indicate ‘prevent from attending’). It is a versatile attention mechanism with causal and\nfull attention that adaptively mixes and changes according to the format of the input sequence. As\nillustrated in (a), (b), (c), and (d) within a sequence containing both text and image tokens, the omni-\nattention mechanism distinctly processes text tokens using causal attention and image tokens using\nfull attention. Besides, concerning the input sequence, (a) text tokens can attend to all preceding\nimage tokens, (b) image tokens can access all preceding text tokens, and (c) in cases where only text\ntokens are provided, the attention degrades to causal attention.\nBy employing this prompt design, we can effectively encode various input data for multi-modal\nunderstanding, text-to-image generation, and mixed-modality generation as sequential data. This\nsetup enables unified learning to operate seamlessly within sequences across these various tasks.\nOnce trained, we can accordingly prompt Show-o to handle various vision-language tasks including\nvisual question answering and text-to-image generation (as shown in Fig. 2).\nOmni-Attention Mechanism. Different from existing works (Touvron et al., 2023; Team, 2024)\nthat model sequence auto-regressively only, we propose an omni-attention mechanism to enable\nShow-o to model various types of signals in distinct ways. It is a comprehensive attention mech-\nanism with causal and full attention that adaptively mixes and changes according to the format of\nthe input sequence. We illustrate omni-attention examples for different input sequences in Fig. 6.\nSpecifically, Show-o model text tokens v within the sequence via causal attention. For image tokens\nu, Show-o processes them via full attention, allowing each token to comprehensively interact with all\nothers. Given a formatted input sequence, it is apparent that in multimodal understanding (Fig. 6(a)),\ntext tokens in a sequence can attend to all previous image tokens, and in text-to-image generation\n(Fig. 6(b)), image tokens are able to interact with all preceding text tokens. Omni-attention main-\ntains the text reasoning knowledge from the pre-trained LLM and enhances the efficiency of image\ngeneration by reducing sampling steps. Moreover, it naturally supports various downstream applica-\ntions like inpainting and extrapolation without necessitating any fine-tuning. When given only text\ntokens, it degrades to causal attention (Fig. 6(c)).\nTraining Objectives.\nTo perform both auto-regressive and (discrete) diffusion modeling, we\nemploy two learning objectives: i) Next Token Prediction (NTP) and ii) Mask Token Predic-\ntion (MTP). Given a sequence with M image tokens u = {u1, u2, · · · , uM} and N text tokens\nv = {v1, v2, · · · , vN} for multimodal understanding, we maximize the likelihood of text tokens by\nemploying the standard language modeling objective:\nLNTP =\nX\ni\nlogpθ(vi|v1, · · · , vi−1, u1, · · · , uM),\n(9)\nwhere p(·|·) indicates the conditional probability which is modeled by the weights θ of Show-o and\nstochastic gradient descent is used to train the model. Note that, if the input sequence involves only\ntext tokens, there are no conditional terms on image tokens u = {u1, u2, · · · , uM}.\nWith the proof in Section 3, we seamlessly integrate the simplified discrete diffusion modeling\nwithin Show-o by employing the mask token prediction as a learning objective. Hence, for modeling\nimage tokens u = {u1, u2, · · · , uM} within the input sequence, we first randomly replace the image\ntokens with the [MASK] token, notated as u∗, at a random ratio (controlling by a time step) to create a\nmasked sequence u∗= {u∗, u2, · · · , u∗, uM}. An illustration can be found in Fig. 3. Next, we aim\nto reconstruct the original image token from the masked tokens conditioning on unmasked regions\n8\nTechnical Report\nand preceding text tokens by maximizing the following likelihood:\nLMTP =\nX\nj\nlogpθ(uj|u∗, u2, · · · , u∗, uM, v1, · · · , vN).\n(10)\nNote that the loss is only applied to the masked tokens. Specifically, we follow the sampling strategy\nused by Chang et al. (2022; 2023) to mask image tokens and reconstruct them via the information\nfrom all text and unmasked image tokens within the input sequence. Following the classifier-free\nguidance introduced by Ho & Salimans (2022), we randomly replace the conditioned text tokens\nwith a null text “” with some probability.\nGiven a batch size of input sequences, the overall training loss is the combination of LMTP and LNTP:\nL = LMTP + αLNTP,\n(11)\nwhere α is the hyper-parameter weighting the loss term LNTP.\n4.3\nTRAINING PIPELINE\nGiven that the embedding of image tokens is newly initialized, it necessitates large-scale pre-training\nto align for multimodal understanding and generation. Besides, Show-o eliminates the text encoder\nto extract text embeddings for text-to-image generation, which poses a significant challenge for\nachieving effective alignment between text and image content within one single transformer. To this\nend, we employ a three-stage approach to progressively and effectively train Show-o:\ni) Image Token Embedding and Pixel Dependency Learning: We employ RefinedWeb (Penedo\net al., 2023) dataset to train Show-o to maintain the language modeling ability.\nMeanwhile,\nImageNet-1K dataset (Deng et al., 2009) and image-text pairs are adopted to train Show-o for class-\nconditional image generation and image captioning, respectively. Here, we directly leverage the\nclass names from ImageNet-1K as textual inputs for learning class-conditional image generation.\nThis stage primarily involves the learning of new learnable embeddings for discrete image tokens,\npixel dependency for image generation, and alignment between image and text for image captioning.\nii) Image-Text Alignment for Multimodal Understanding and Generation: Building upon the\npre-trained weights, we proceed to involve training of text-to-image generation on the image-text\ndata instead of the ImageNet-1K. This stage mainly focuses on image and text alignment for both\nimage captioning and text-to-image generation.\niii) High-Quality Data Fine-tuning: Lastly, we further refine the pre-trained Show-o model by\nincorporating filtered high-quality image-text pairs for text-to-image generation and instructional\ndata for multimodal understanding and mixed-modality generation.\n4.4\nINFERENCE\nIn inference, two types of predictions, i.e., text and image tokens, are involved in Show-o. In mul-\ntimodal understanding, given the conditional image and questions, text tokens are auto-regressively\nsampled from the predicted tokens with higher confidence. In visual generation, it takes T steps\nto generate an image. Initially, we provide N conditional text tokens and M [MASK] tokens as in-\nput. Show-o then predicts a logit ℓt for each [MASK] token, where t is the time step. Following the\nwork (Chang et al., 2023), we compute both the conditional logit ℓt\nc and unconditional logit ℓt\nu for\nmasked tokens. The final logit ℓt of each [MASK] token is obtained by the following equation with a\nguidance scale w:\nℓt = (1 + w)ℓt\nc −wℓt\nu.\n(12)\nWe preserve the predicted image tokens with higher confidence while replacing those with lower\nconfidence with [MASK] tokens, which are all subsequently fed back into Show-o for the next round\nprediction. An illustration is provided in Fig. 3. This denoising process takes T steps, akin to the\napproach in stable diffusion models. More details can be found in the Appendix A.2.\n9\nTechnical Report\nTable 1: Evaluation on multimodal understanding benchmarks. Show-o is currently built upon\nPhi-1.5 and thus we implement LLaVA-v1.5-Phi-1.5 as our apple-to-apple baseline. Und. and Gen.\ndenote “understanding” and “generation”, respectively. ‡ denotes the improved Show-o that employs\nCLIP-ViT continuous representations. We highlight the model size of Show-o and LLaVA baseline\nin green, and we use blue to highlight the larger model size than ours.\nType\nModel\n# Params\nPOPE↑\nMME↑\nFlickr30k↑\nVQAv2(test)↑\nGQA↑\nMMMU↑\nUnd. Only\nLLaVA-v1.5 (Liu et al., 2024b)\n7B\n85.9\n1510.7\n-\n78.5\n62.0\n35.4\nInstructBLIP (Dai et al., 2023)\n13B\n78.9\n1212.8\n-\n-\n49.5\n-\nQwen-VL-Chat Bai et al. (2023)\n7B\n-\n1487.5\n-\n78.2\n57.5\n-\nmPLUG-Owl2 (Ye et al., 2024b)\n7B\n85.8\n1450.2\n-\n79.4\n56.1\n-\nLLaVA-v1.5-Phi-1.5\n1.3B\n84.1\n1128.0\n69.6\n75.3\n56.5\n30.7\nUnd. and Gen.\nGemini-Nano-1 (Anil et al., 2023)\n1.8B\n-\n-\n-\n62.7\n-\n26.3\nCoDI (Tang et al., 2024)\n-\n-\n-\n12.8\n-\n-\n-\nEmu (Sun et al., 2023d)\n13B\n-\n-\n77.4\n57.2\n-\n-\nNExT-GPT (Wu et al., 2023b)\n13B\n-\n-\n84.5\n66.7\n-\n-\nSEED-X (Ge et al., 2024)\n17B\n84.2\n1435.7\n52.3\n-\n47.9\n35.6\nDreamLLM (Dong et al., 2024)\n7B\n-\n-\n-\n72.9\n-\n-\nChameleon (Team, 2024)\n34B\n-\n-\n74.7\n66.0\n-\n-\nShow-o (Ours)\n1.3B\n80.0\n1097.2\n62.5\n69.4\n58.0\n26.7\nShow-o‡ (Ours)\n1.3B\n84.5\n1232.9\n67.6\n74.7\n61.0\n27.4\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nDatasets. Three types of data are adopted for training Show-o: i) Text-only Data: We employ the\npublicly available RefinedWeb dataset (Penedo et al., 2023) to preserve the text reasoning capabili-\nties of the pre-trained LLM. This dataset comprises approximately 1 billion instances (equivalent to\n968 million individual web pages) and totals 2.8 terabytes of curated text data. ii) Image Data with\nClass Names: Show-o learns pixel dependencies using 1.28M images sourced from the ImageNet-\n1K (Deng et al., 2009) dataset. iii) Image-Text Data: For pre-training tasks correspond to multi-\nmodal understanding and generation, we assemble roughly 35M image-text pairs from the publicly\navailable datasets including CC12M (Changpinyo et al., 2021), SA1B (Kirillov et al., 2023), and\nLAION-aesthetics-12M∗. Additionally, we further increase the data scale to around 2.0B by incor-\nporating DataComp (Gadre et al., 2024) and COYO700M (Byeon et al., 2022) with some filtering\nstrategies. Note that, we employ ShareGPT4V (Chen et al., 2023) to re-caption these datasets. Ad-\nditionally, LAION-aesthetics-12M and JourneyDB (Sun et al., 2023a) serve as high-quality datasets\nfor the final fine-tuning. Following LLaVA-v1.5 (Liu et al., 2024b), we incorporate LLaVA-Pretrain-\n558K and LLaVA-v1.5-mix-665K for instruction tuning. Moreover, the GenHowTo dataset (Souˇcek\net al., 2024) is utilized for mixed-modality generation.\nEvaluation Details. Following LLaVA (Liu et al., 2024b), we evaluate the multimodal understand-\ning capabilities of Show-o on POPE, MME, Flickr30k, VQAv2, GQA, and MMMU benchmarks.\nBesides, following Stable Diffusion (Rombach et al., 2022), we adopt Fr´echet Inception Distance\n(FID) on MSCOCO dataset to evaluate the potential generation fidelity of Show-o. Further, we\nfollow SD3 (Esser et al., 2024) to evaluate the text-to-image generation capabilities of Show-o on\nthe GenEval (Ghosh et al., 2023) benchmark. In experiments, we make comparisons with: i) un-\nderstanding only models including LLaVA-v1.5 (Liu et al., 2024b), InstructBLIP (Dai et al., 2023),\nQwen-VL-Chat (Bai et al., 2023), and mPLUG-Owl2 (Ye et al., 2024b); ii) generation only mod-\nels including DALL·E (Ramesh et al., 2021), LDM (Rombach et al., 2022), DALL·E 2 (Ramesh\net al., 2022a), SDv1.5 (Rombach et al., 2022), GigaGAN (Kang et al., 2023), PixArt (Chen\net al., 2024), Imagen (Saharia et al., 2022), RAPHAEL Xue et al. (2024), LlamaGen (Sun et al.,\n2024), SDXL (Podell et al., 2023), and SD3 (Esser et al., 2024); unified models including Gem-\nini (Anil et al., 2023), NExT-GPT (Wu et al., 2023b), Emu (Sun et al., 2023d), SEED-X (Ge et al.,\n2024), DreamLLM (Dong et al., 2024), CoDI (Tang et al., 2024), LWM (Liu et al., 2024a), and\nChameleon (Team, 2024).\nImplementation Details. We initially conduct joint training of Show-o using the RefinedWeb, a\ncollection of 35M image-text pairs, and the ImageNet-1K for language modeling, image captioning,\nand class-conditional image generation, respectively, over 500K steps. Subsequently, we replace the\nclass-conditional generation with the training for text-to-image generation using the 35M image-text\npairs for an additional 1,000K steps. The base model is trained on 48 A100 (80GB) GPUs with a\n∗https:\/\/huggingface.co\/datasets\/dclure\/laion-aesthetics-12m-umap\n10\nTechnical Report\nUser: Please describe this image in detail.\nShow-o:\nThe image features a dog standing in a field of tall grass and\nflowers. The dog is sniffing a flower, possibly enjoying the scent\nor trying to find a treat. The field is filled with various flowers,\ncreating a beautiful and vibrant scene. The dog is positioned\ntowards the center of the field, surrounded by the flowers and\ngrass.\nChameleon:\nThis image is a close-up photograph of a dog's nose and face in a field of flowers. The dog's face is mostly out of\nfocus, but the nose is in sharp focus. The dog is a golden retriever and is lying down on the ground in the middle\nof the image. There is a blurry yellow flower in the foreground of the image, with a clear yellow flower in the\nmiddle of the image. The flower in the middle has a few small yellow flowers.\nSEED-X:\nThis image captures a charming scene of a small, brown and white dog in a field of daisies. The dog, with its\ntongue playfully sticking out, is in the center of the frame, surrounded by a sea of white daisies. The field extends\nto the horizon, suggesting a vast expanse of nature. The sky above is a clear blue, adding to the serene\natmosphere of the scene. The dog appears to be enjoying its time in the field, perhaps taking a moment to smell\nthe flowers …\nUser: Do you think the image is unusual or not?\nShow-o:\nThe image is unusual because it features a living room scene\nwith a couch and a chair, but instead of a typical living room\nsetting, it is set in an underwater environment. This is not a\ncommon sight, as living rooms are usually indoors and not\ndesigned to be submerged in water. The presence of a couch\nand a chair in an underwater setting is unexpected and adds an\nelement of surrealism to the scene.\nChameleon:\nThe image is unusual because of the transparent and colorful fish and coral in the water. The image is also\nunusual because of the clear blue sky with white clouds. The unusual objects in the image are the fish and coral\nin the water. The unusual objects in the image are the fish and coral in the water. The unusual objects in the\nimage are the fish and coral in the water. The unusual objects in the image are the fish and coral in the water.\nSEED-X:\nYes, the image is unusual. It features a living room set, including a couch and two chairs, placed in the middle of\nthe ocean. The room is surrounded by water, with coral reefs and fish visible in the background. This is a surreal\nand unexpected scene, as living rooms are typically found on land, not in the middle of the ocean. The presence\nof the couch and chairs in such an environment adds to the peculiarity of the image.\nFigure 7: Comparisons of VQA capabilities among Chameleon, SEED-X, and Show-o. We empha-\nsize distinctions in answers from different methods using a different color in the second example.\nOne can observe that Show-o can describe the image in detail and respond to commonly asked ques-\ntions, even addressing the unusual aspects within the image.\ntotal batch size of 1,152. We employ the AdamW optimizer with a weight decay of 0.01, 5,000\nsteps of warm-up, and an initial learning rate of 1e-4 with a cosine scheduling. Finally, we fine-tune\nShow-o with filtered high-quality image-text pairs and adhere to the configuration of LLaVA-v1.5\nfor instruction data tuning. Note that, the current version of Show-o is based on Phi-1.5 (Li et al.,\n2023). In the following experiment sections, the default Show-o employs discrete image tokens as\ninput for both multimodal understanding and generation. Show-o† and Show-o‡ indicate the use of\ncontinuous image representations from the pre-trained MAGVIT-v2 and CLIP-ViT (corresponding\nto options (b) and (c) in Fig. 5), respectively, for multimodal understanding and we discuss this\nexploration in Section 5.5.\nBased on the pre-trained Show-o, we continue to train it on the 2.0B image-text pairs for 500K\nsteps and then we increase the image resolution to 512 × 512 and train Show-o for an additional\n11\nTechnical Report\n500K steps. Finally, we fine-tune Show-o with around 1M internal high-quality image-text pairs\nand adhere to the configuration of LLaVA-v1.5 for instruction data tuning.\n5.2\nMULTIMODAL UNDERSTANDING\n5.2.1\nQUANTITATIVE EVALUATION\nTable 1 presents the multimodal understanding capability of Show-o on public benchmarks, such as\nimage captioning and visual question-answering tasks. i) The current version of Show-o is built upon\nPhi-1.5 (Li et al., 2023) and thus we follow LLaVA to train Show-o’s understanding only counterpart\nas our direct baseline, namely LLaVA-v1.5-Phi-1.5. The proposed Show-o exhibits comparable per-\nformance in all evaluation metrics to the baseline LLaVA-v1.5-Phi-1.5, which is dedicated and opti-\nmized to only multimodal understanding. This demonstrates the great potential of our framework to\nunify multimodal understanding and generation in one single transformer. ii) When comparing with\nunderstanding only models including InstructBLIP (Dai et al., 2023), Qwen-VL-Chat (Bai et al.,\n2023), and mPLUG-Owl2 (Ye et al., 2024b) on multimodal understanding, our model with a much\nsmaller model size also achieves competitive performance on POPE, MME, Flickr30k and VQAv2\nbenchmarks and performs better on GQA benchmark. iii) Compared with unified models with a\nmuch larger number of parameters, such as NExT-GPT-13B (Wu et al., 2023b) and Chameleon-\n34B (Team, 2024), our model also achieves decent performance on Flickr30k benchmark and per-\nform much better on VQAv2 benchmark. Given such promising results, we envision Show-o as a\npotential next-generation foundation model for unifying understanding and generation. These re-\nsults also demonstrate the potential of scaling Show-o to achieve state-of-the-art performance.\n5.2.2\nQUALITATIVE RESULTS\nWe present Show-o’s visual question-answering capability and make comparisons with Chameleon\nand SEED-X in Fig. 7. It is evident that when presented with a query image, Show-o can describe\nthe image in detail and respond to commonly asked questions, even addressing the unusual aspects\nwithin the image. In the example at the top of Fig. 7, Chameleon, SEED-X, and Show-o all provide\ncomprehensive descriptions of the image’s main content. However, when asked, “Do you think the\nimage is unusual or not”, Chameleon fails to correctly identify the unusual aspect, and SEED-X’s\nresponse, while identifying the unusual, lacks precision, stating “as living rooms are typically found\non land”. In contrast, Show-o’s response, “as living rooms are usually indoors and designed for\nrelaxation and entertainment”, is more accurate.\n5.3\nVISUAL GENERATION\n5.3.1\nQUANTITATIVE EVALUATION\nTable 2: MSCOCO zero-shot FID. Und. and Gen. denote\n“understanding” and “generation”, respectively.\nType\nMethod\n# Params\n# Images\nFID-30K↓\nGen. Only\nDALL·E (Ramesh et al., 2021)\n12B\n250M\n27.50\nGLIDE (Nichol et al., 2021)\n5B\n250M\n12.24\nLDM (Rombach et al., 2022)\n1.4B\n400M\n12.64\nDALL·E 2 (Ramesh et al., 2022a)\n6.5B\n650M\n10.39\nSDv1.5 (Rombach et al., 2022)\n0.9B\n2000M\n9.62\nGigaGAN (Kang et al., 2023)\n0.9B\n2700M\n9.09\nPixArt (Chen et al., 2024)\n0.6B\n25M\n7.32\nImagen (Saharia et al., 2022)\n3B\n860M\n7.27\nRAPHAEL (Xue et al., 2024)\n3B\n5000M+\n6.61\nUnd. and Gen.\nCoDI (Tang et al., 2024)\n-\n400M\n22.26\nLWM (Liu et al., 2024a)\n7B\n-\n12.68\nSEED-X (Ge et al., 2024)\n17B\n-\n14.99\nDreamLLM (Dong et al., 2024)\n7B\n-\n8.76\nShow-o (Ours)\n1.3B\n35M\n9.24\nResults on MSCOCO 30K. We\npresent zero-shot FID of Show-o on\nMSCOCO 30K in Table 2. It can be\nobserved that, compared to genera-\ntion models trained with larger num-\nbers of parameters and training im-\nages such as GLIDE and DALL·E 2,\nShow-o achieves a better FID, i.e.,\n9.24, with only 1.3B parameters and\n35M training data.\nThough Giga-\nGAN, Imagen, and RAPHAEL ob-\ntain a relatively better performance\nthan Show-o, they are much larger in\nmodel size (3B v.s. 1.3B) and trained with much more data. In comparison to unified models,\nShow-o also exhibits improvement. The above validates that Show-o, a unified transformer, can\nobtain competitive even better generation performance compared to individual models (generation\nonly) with an equivalent or larger number of parameters and training data. However, it is worth\nnoting that FID on MSCOCO 30K may not comprehensively accurate assessment of generation fi-\ndelity. The reason lies in the fact that existing generation models are commonly fine-tuned with\n12\nTechnical Report\nTable 3: Evaluation on the GenEval (Ghosh et al., 2023) benchmark. Und. and Gen. denote\n“understanding” and “generation”, respectively. We highlight the model size of Show-o in green,\nand we use blue to highlight the larger model size than ours. Obj.: Object. Attri.: Attribute.\nType\nMethod\n# Params\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall↑\nGen. Only\nLlamaGen (Sun et al., 2024)\n0.8B\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nLDM (Rombach et al., 2022)\n1.4B\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 (Rombach et al., 2022)\n0.9B\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nPixArt-alpha (Chen et al., 2024)\n0.6B\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nSDv2.1 (Rombach et al., 2022)\n0.9B\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nDALL-E 2 (Ramesh et al., 2022a)\n6.5B\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nSDXL (Podell et al., 2023)\n2.6B\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nSD3 (d=24) (Esser et al., 2024)\n2B\n0.98\n0.74\n0.63\n0.67\n0.34\n0.36\n0.62\nUnd. and Gen.\nCoDI (Tang et al., 2024)\n-\n0.89\n0.16\n0.16\n0.65\n0.02\n0.01\n0.31\nLWM (Liu et al., 2024a)\n7B\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nSEED-X (Ge et al., 2024)\n17B\n0.97\n0.58\n0.26\n0.80\n0.19\n0.14\n0.49\nChameleon (Team, 2024)\n7B\n-\n-\n-\n-\n-\n-\n0.39\nShow-o (Ours)\n1.3B\n0.98\n0.80\n0.66\n0.84\n0.31\n0.50\n0.68\nhigh-quality and aesthetic images that do not align with the distribution of the MSCOCO dataset.\nConsequently, this mismatch leads to inaccurate measurement of generation fidelity.\nResults on GenEval. Following SD3 (Esser et al., 2024), we evaluate text-to-image generation ca-\npability of Show-o on the GenEval benchmark (Ghosh et al., 2023) across six dimensions including\n“Single Object”, “Two Objects”, “Counting”, “Colors”, “Position”, “Color Attribute” and present\nthe quantitative comparison in Table 3. One can observe that when comparing to the model in a simi-\nlar size such as LDM (1.4B), Show-o obtains significantly better performance in all six metrics, with\nan improvement of around 0.31 overall. Besides, Show-o achieves a relatively better performance\nthan DALL·E 2, which is 5 times larger in model size. Further, Show-o, with only 1.3B parameters,\nachieves even better performance than models with around a two-times larger number of parameters\nsuch as SD3 (2B). It indicates that our unified model’s generative capabilities are comparable to or\neven surpass those of specialized generation models. In comparison to unified models such as CoDI\nand SEED-X, Show-o also demonstrates significant improvements.\n5.3.2\nQUALITATIVE RESULTS\nQualitative Comparisons. We present qualitative comparisons with diffusion-based models, e.g.,\nSDv1.5, SDXL, auto-regressive based model, i.e., LllamaGen, and unified models including LWM\nand SEED-X at the top of Fig. 8. One can observe that, given either short or long text prompts,\nShow-o can generate realistic images with consistent content described in the prompts. Compared\nto SDv1.5 and LlamaGen, Show-o demonstrates better visual quality and image-text alignment. For\ninstance, as shown in the second column, both SDv1.5 and LlamaGen cannot fully comprehend the\ntext prompt and miss some attributes such as sunset and blue domes in the generated images. In\ncomparison to SDXL, Show-o exhibits comparable visual quality and alignment such as “a rally\ncar race” and “stunning contrast against the vibrant sunset”. More samples generated by Show-\no can be found at the bottom of Fig. 8. One can observe that Show-o is capable of generating\ndiverse, interesting, and realistic visual content in a resolution of 512×512. For example, Show-o\ncan generate a futuristic style of car, a highly detailed face, cute objects, and vivid scenery with\nvibrant contrast.\nText-guided Inpainting and Extrapolation. As mentioned, Show-o naturally supports text-based\ninpainting and extrapolation without requiring any fine-tuning. We illustrate examples in Fig. 9.\nAs shown on the top of the figure, given an input image and inpainting mask, Show-o can inpaint\nthe original red trolley car to a blue sports car with sleek curves and tinted windows based on the\nuser-provided text prompt. Besides, Show-o is capable of extrapolating the original image hori-\nzontally\/vertically based on the given text prompt. Further, we can flexibly extrapolate the original\nimage with new objects or scenes such as “red wildflowers” (as illustrated in the second row). One\ncan observe that the pixels in both inpainted regions and extrapolated ones are consistent with the\noriginal ones. These cases significantly demonstrate the inherent advantages of Show-o over those\nauto-regressive models for downstream applications.\n13\nTechnical Report\nSDXL\nLlamaGen\nShow-o (Ours)\nLWM\nSEED-X\n“An abstract portrait of a pensive face, rendered in cool shades of blues, purples, and grays.”\n“The breathtaking view of Santorini, a renowned landmark in Greece. The white-washed\nbuildings with blue domes overlook the deep blue waters of the Aegean Sea, creating a\nstunning contrast against the vibrant sunset.”\n“A dynamic scene of a rally car race.”\nFigure 8: (Top) Qualitative comparisons among SDXL, LlamaGen, LWM, SEED-X, and Show-o.\n(Bottom) Samples in a resolution of 512 × 512 generated by Show-o.\n14\nTechnical Report\nExtrapolation\nprompt:\n“a\nred\ntrolley\ncar\nparked on a street.”\nExtrapolation\nInpainting prompt: “A blue sports car with\nsleek curves and tinted windows.”\nInpainting prompt: “A clear, shallow river with\nsome vibrant flowers in it.”\nExtrapolation prompt: “A small hill by the\nrailroad tracks, with some red wildflowers.”\nExtrapolation\nExtrapolation prompt: “A serene natural landscape featuring a clear, blue lake surrounded by lush green trees.”\nExtrapolation prompt: “The continuous mountain ranges and jungles, with meandering rivers occasionally appearing.”\nOriginal Image\nExtrapolation\nFigure 9: Examples of text-guided image inpainting\/extrapolation.\n5.4\nMIXED-MODALITY GENERATION OF VIDEO KEYFRAMES AND CAPTIONS\nHere, we explore the mixed-modality generation ability of Show-o based on the text descriptions\nand video keyframes in the GenHowTo dataset. Given a sequence of interleaved text descriptions\nand video keyframes (as shown at the bottom of Fig. 4), Show-o is trained to predict the next text\ntokens or keyframe tokens conditioning on all preceding tokens. Thus, Show-o can generate mixed-\nmodality of text descriptions and video keyframes. Examining a single frame, these tokens are\ngenerated in a diffusion manner. When considering the modeling of long sequences, as subsequent\nkeyframes are produced based on all preceding text and image information, this can also be viewed\nas a form of temporal auto-regressive modeling. Consequently, it becomes feasible to generate\nconsistent video keyframes continuously. Substituting video keyframes with video clips could offer\na viable strategy for long video generation, a direction we aim to explore in the future.\nWe present qualitative examples in Fig. 10. As shown on the top of Fig. 10, given text prompts,\nShow-o can generate consistent video keyframes. Besides, we have tried to train Show-o using\ninstructional examples. For example, given a question “Can you guide me through making Avocado\nand Apple Juice”, Show-o exhibits the capability to generate video keyframes with text descriptions\nrelated to the question. It is apparent that the generated keyframes are temporally consistent. This\nexploration reveals the potential of our model to be extended to the domain of long video generation,\nin which the model can produce a sequence of video clips. Specifically, the model continuously\nplans the next-scene textual prompt and proceeds to generate videos iteratively, allowing for the\ncontinuous generation of subsequent videos.\n15\nTechnical Report\nUser: Can you guide me through making Avocado and Apple Juice?\n(6) a food\nprocessor with \navocado in it.\nA chef is cutting an \navocado on a white \nplate.\nInput image\nGenerated Video Keyframes\nA person is \npreparing bacon in \nan air fryer.\nBacon on a piece\nof paper next to an\noven.\nBacon Frying. \nAvocado \nPeeling\/Slicing. \nA chef is cutting an \navocado with a \nknife. \nGenerated Video Keyframes\n(1) Apple \nPeeling\/Cutting. \n(2) a person is \ncutting an apple. \n(3) a person slicing \nan apple on a \nplate. \n(4) Avocado \nPeeling\/Slicing.\n(5) a woman is \ncutting an avocado \non a plate. \nInput image\nGenerated Text Descriptions with Corresponding Video Keyframes\nInput image\nFigure 10: Mixed-modality generation. (Top) Given the first keyframe with text prompts for each\nframe, Show-o can correspondingly generate consistent video keyframes conditioning on the previ-\nous texts and keyframes. (Bottom) Given an instruction and the first frame with text descriptions,\nShow-o can alternately generate text descriptions and corresponding video keyframes.\n5.5\nABLATION STUDIES\nImpact of dataset scale and image resolution on multimodal understanding. As only discrete\nimage tokens are extracted from the vision tokenizer, it is required to learn image token embeddings\nin Show-o from scratch. Unlike aligned image representations from the CLIP well-trained on a large-\nscale image-text dataset, Show-o necessitates the multimodal alignment between image and text\nembeddings during the pre-training stages. Here, we study the impact of the dataset scale and image\nresolution on the learning of discrete image token embeddings for multimodal understanding in\nTable 4. One can observe that the multimodal understanding capabilities of Show-o are consistently\nimproved when increasing the data scale and image resolution. This reveals that it is required to\ninvolve more image-text pairs for multimodal alignment and more image tokens to represent an\nimage for better comprehending the image information.\nAs illustrated in Fig. 5(a), the default Show-o adopts the pre-trained MAGVIT-v2 to tokenize input\nimage to discrete tokens, which are then passed to the embedding layer to obtain embeddings as input\nfor multimodal understanding. Beyond, we provide a systematic exploration of different design\nchoices for the input of Show-o to enhance multimodal understanding. Specifically, as shown in\nFig. 5(b) and (c), instead of discrete image tokens, we extract the continuous image representations\nfrom the pre-trained MAGVIT-v2 and CLIP-ViT, respectively, as input for Show-o when dealing\nwith multimodal understanding. The experimental results are illustrated in Table 5. Through this\nexploration, we unveil the following lessons and insights.\nImpact of Vision Encoder for Multimodal Understanding.\nThe default Show-o employs\nMAGVIT-v2 to encode images into discrete tokens for both multimodal understanding and gen-\neration. Inspired by the literature (Liu et al., 2024b), we investigate the impact of the most popular\ndesign choice of vision encoder, i.e., the pre-trained CLIP ViT (Radford et al., 2021), for mul-\ntimodal understanding. We first compare the two settings using our Show-o model. In Table 5,\nthe comparison between Exp 2 and Exp 4, Exp 3 and Exp 5 clearly demonstrates that continuous\nrepresentations from CLIP-ViT have significantly better performance on multimodal understanding\nthan that of MAGVIT-v2. This mainly attributes to: i) The CLIP-ViT is pre-trained on a much\nlarger dataset (400M) than that of our pre-trained MAGVIT-v2 (35M); ii) In contrast to image re-\nconstruction learning objective in MAGVIT-v2, the discriminative loss, i.e., image-text matching, in\nCLIP-ViT makes the extracted representations easier to be adapted for multimodal understanding.\n16\nTechnical Report\nTable 4: Impact of dataset scale and image resolution on the learning of discrete image token em-\nbeddings for multimodal understanding.\n# Image-text\nImage Resolution\n# Image Tokens\nPOPE\nMME\nFlickr30k\nVQAv2(test)\nGQA\nMMMU\n35M\n2562\n256\n73.8\n948.4\n36.2\n59.3\n48.7\n25.1\n2.0B\n2562\n256\n76.2\n1014.9\n48.9\n64.7\n54.2\n25.0\n2.0B\n5122\n1024\n80.0\n1097.2\n62.5\n69.4\n58.0\n26.7\nTable 5: Ablation studies of various vision encoders and kinds input representations for multimodal\nunderstanding. Note that this experiment is based on the Show-o pre-trained on 35M image-text\ndata in a resolution of 256 × 256.\n# Exp\nMethod\nVision Encoder\nUnified Pretrain\nFeature type\nPOPE\nMME\nFlickr30k\nVQAv2(val)\nGQA\nMMMU\n1\nLLaVA\nCLIP-ViT\n✗\nContinuous\n84.1\n1128.0\n69.6\n73.0\n56.5\n30.67\n2\nShow-o‡\nCLIP-ViT\n✓\nContinuous\n84.5\n1182.7\n64.3\n71.9\n57.5\n27.4\n3\nShow-o‡\nCLIP-ViT\n✗\nContinuous\n84.5\n1161.6\n68.5\n73.5\n58.7\n29.2\n4\nShow-o†\nMAGVIT-v2\n✓\nContinuous\n74.3\n947.8\n33.9\n59.4\n51.0\n26.7\n5\nShow-o†\nMAGVIT-v2\n✗\nContinuous\n65.1\n800.0\n12.3\n50.8\n43.9\n24.6\n6\nShow-o\nMAGVIT-v2\n✓\nDiscrete\n73.8\n948.4\n36.2\n57.8\n48.7\n25.1\n7\nShow-o\nMAGVIT-v2\n✗\nDiscrete\n63.8\n689.1\n4.5\n46.1\n40.5\n28.1\nImpact of Various Representations for Multimodal Understanding.\nIn typical multimodal un-\nderstanding models like LLaVA, the image representation extraction and cross-modal alignment\nusually happen in the continuous space. However, image tokenizers such as MAGVIT-v2 naturally\nyield discrete image tokens. As shown in Table 5, we compare the two types of input, i.e., con-\ntinuous representations and discrete tokens, in the multimodal understanding scenario. In Exp 6\nand 7, we use the pre-trained MAGVIT-v2 to extract discrete tokens and train an embedding layer\nto embed the tokens into the continuous embedding space of the LLM. In Exp 4 and 5, we modify\nMAGVIT-v2 to output continuous representations without quantization. The cross-modal projection\nlayer follows the setting of LLaVA. The comparison between Exp 5 and Exp 7 reveals that discrete\ntokens show much worse performance on most benchmarks. We attribute the performance gap to\nthat popular multimodal understanding datasets, e.g., LLaVA-Pretrain-558K, are not sufficient to\nalign discrete image tokens into the language space, leading to an unsatisfactory cross-modal under-\nstanding. In contrast, continuous representations, already lying in a well-shaped embedding space,\nare much easier to align.\nImpact of Unified Pre-training for Multimodal Understanding.\nOur training pipeline involves\ntwo-stage unified pre-training to learn image token embedding and image-text alignment for multi-\nmodal understanding and generation (as described in Section 4.3). Here we elaborate on the impact\nof the unified per-training with different vision encoders and types of representations:\n• CLIP-ViT with Continuous Representations. The comparison between Exp 2 and Exp\n3 shows that the unified pre-training has a small negative effect on the CLIP ViT-based\nunderstanding, as the performance on most benchmarks has marginal degradations. We\nhypothesize that the MAGVIT-v2 token-based pre-training and the CLIP ViT-based tuning\nhappen in nearly orthogonal dimensions, and the capability of the backbone has been spared\nto maintain the compatibility of the two tasks.\n• MAGVIT-v2 with Continuous Representations. In the comparison between Exp 4 and Exp\n5, we also notice a performance improvement brought by the unified pre-training, even\nthough the pre-training uses discrete tokens while the experiments here use continuous fea-\ntures. This comparison further validates the hypothesis that unified pre-training enhances\nthe multimodal understanding and reasoning capabilities of the backbone by diverse mul-\ntimodal interactions during pre-training.\n• MAGVIT-v2 with Discrete Tokens. The comparison between Exp 6 and Exp 7 shows\nthat the unified pre-training has significantly boosted the multimodal understanding per-\nformance. This is intuitive since the pre-training also adopts MAGVIT-v2 discrete tokens\nas image representation. Specifically, we attribute the performance gain to that unified\npre-training learns a better cross-modal alignment with large-scale data and enhances the\nmultimodal understanding capabilities of the backbone.\n17\nTechnical Report\nw = 3.0\nw = 1.0\nStep = 5\nStep = 25\nStep = 50\nw = 5.0\n“A colorful cartoon of a tiger camouflaged in an abstract art painting, its stripes merging with the wild brushstrokes.”\nFigure 11: Illustration of generated samples using different sampling steps and classifier-free guid-\nance scale w.\nWhat are the words in this image?\nShow-o: The words in this image\nare \"closing stock must go down.”\nHow many people in this picture?\nShow-o: There are three people in the\npicture.\n“A group of seven people\nstanding on a snow-covered\nslope, all wearing skis and\nposing for a picture.”\n“The word 'mardefly' on a \ncoffee mug”\n(a) Failure Cases in Multimodal Understanding\n(b) Failure Cases in Visual Generation\nFigure 12: Illustration of failure cases of Show-o in multimodal understanding and generation.\nAdditionally, we present qualitative examples to illustrate the impact of sampling steps and\nclassifier-free guidance for text-to-image generation in the following.\nImpact of Sampling Steps. We present generated results at 512 × 512 resolution with varying\nsampling steps on the left of Fig. 11. With just five steps, Show-o can produce an image that is\nroughly related to the given prompt. Increasing the sampling steps to 25 allows the synthesis of\nan image that closely adheres to the prompt. When the sampling step is set as 50, the generated\nimage becomes more detailed and realistic. In contrast, auto-regressive models Team (2024); Sun\net al. (2024) require 1024 sampling steps to generate an image of the same resolution when the\ndownsampling rate is 16, which is around 20 times more steps than our approach.\nImpact of Classifier-free Guidance.\nThe visual variations of generated images with different\nclassifier-free guidance scales w are illustrated on the right of Fig. 11. It can be observed that\nthe object in the generated images lacks detail without classifier-free guidance. As the classifier-free\nguidance scale w is gradually increased to 3 and 5, the colors and contents become more diverse and\nconsistent with the given text prompt.\n5.6\nFAILURE CASES\nWe provide failure cases of Show-o in multimodal understanding and generation in Fig. 12. The\ncurrent version of Show-o sometimes cannot accurately recognize the text and count the object\ninstances and exhibits challenges in generating correct belongings such as skis for each instance.\nOne can observe that Show-o fails to identify the phrase ”closing down” in the left of Fig. 12(a) and\nis unable to generate the term “mardefly” (as shown left of Fig. 12(b)). This limitation is mainly\nattributed to the insufficiency of specific data tailored to these scenarios, as our model relies on a\nlimited set of image-text pairs sourced from publicly available datasets and utilizes automatically\ngenerated captions. Enriching such kind of data holds promise for addressing these failure modes in\nShow-o, an aspect that will be explored in the future.\n6\nCONCLUSION\nThis paper proposed a unified transformer, i.e., Show-o, to unify multimodal understanding and\ngeneration. Show-o for the first time unified autoregressive and (discrete) diffusion modeling that\ncan handle different modalities in distinct ways. Extensive experimental results demonstrated that\nShow-o is comparable to even better than individual expert models across a wide range of vision-\nlanguage tasks. This highlighted its potential as a next-generation foundation model.\n18\nTechnical Report\n7\nACKNOWLEDGMENTS\nWe would like to express our sincere gratitude to Henry Hengyuan Zhao for his valuable discussions\nand insightful feedback on multimodal understanding and Mingrui Wang for his patient assistance\nin helping us set up the development environment.\n19\nTechnical Report\nREFERENCES\nEmanuele Aiello, LILI YU, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large\nautoregressive multimodal models. In ICLR, 2024.\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: A family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 1, 2023.\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured\ndenoising diffusion models in discrete state-spaces. NeurIPS, pp. 17981–17993, 2021.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities.\nCoRR, abs\/2308.12966, 2023.\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng\nShou.\nHallucination of multimodal large language models:\nA survey.\narXiv preprint\narXiv:2404.18930, 2024.\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth\nwords: A vit backbone for diffusion models. In CVPR, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon\nKim.\nCoyo-700m:\nImage-text pair dataset.\nhttps:\/\/github.com\/kakaobrain\/\ncoyo-dataset, 2022.\nAndrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and\nArnaud Doucet.\nA continuous time framework for discrete denoising models.\nNeurIPS, pp.\n28266–28279, 2022.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In CVPR, pp. 11315–11325, 2022.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 3558–3568,\n2021.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok,\nPing Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for\nphotorealistic text-to-image synthesis. In ICLR. OpenReview.net, 2024.\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin.\nSharegpt4v: Improving large multi-modal models with better captions.\narXiv preprint\narXiv:2311.12793, 2023.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, pp. 1691–1703, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\n20\nTechnical Report\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.\nJ. Mach. Learn. Res., 24:240:1–240:113, 2023.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe\nJenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme\nRuiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste,\nGamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn\nBastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos,\nYi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario\nLucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision\ntransformers to 22 billion parameters. In ICML, pp. 7480–7512, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, pp. 248–255, 2009.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.\nDreamLLM: Synergistic multimodal comprehension and creation. In ICLR, 2024.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, pp. 12873–12883, 2021a.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, pp. 12873–12883, 2021b.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In ICML, 2024.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In\nsearch of the next generation of multimodal datasets. NeurIPS, 2024.\nYuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying\nShan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation.\narXiv preprint arXiv:2404.14396, 2024.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel\ndecoding of conditional masked language models. In EMNLP, pp. 6111–6120, 2019.\nDhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework\nfor evaluating text-to-image alignment. In NeurIPS, 2023.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pp. 10696–\n10706, 2022.\nYuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Rethinking the objectives\nof vector-quantized tokenizers for image synthesis. In CVPR, pp. 7631–7640, 2024.\n21\nTechnical Report\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,\npp. 6840–6851, 2020a.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, pp.\n6840–6851, 2020b.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. NeurIPS, 2022.\nEmiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and\nTim Salimans. Autoregressive diffusion models. In ICLR. OpenReview.net, 2022.\nMinghui Hu, Chuanxia Zheng, Zuopeng Yang, Tat-Jen Cham, Heliang Zheng, Chaoyue Wang,\nDacheng Tao, and Ponnuthurai N. Suganthan. Unified discrete diffusion for simultaneous vision-\nlanguage generation. In ICLR, 2023.\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung\nPark. Scaling up gans for text-to-image synthesis. In CVPR, pp. 10124–10134. IEEE, 2023.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV,\npp. 4015–4026, 2023.\nDan Kondratyuk, Lijun Yu, Xiuye Gu, Jos´e Lezama, Jonathan Huang, Rachel Hornung, Hartwig\nAdam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model\nfor zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al.\nMultimodal foundation models: From specialists to general-purpose assistants. Foundations and\nTrends® in Computer Graphics and Vision, 16(1-2):1–214, 2024.\nYuanzhi Li, S´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\nTextbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and\nlanguage with ringattention. arXiv preprint, 2024a.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. In CVPR, pp. 26296–26306, 2024b.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS,\n36, 2024c.\nBrandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter,\nDhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights\nfrom multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.\nKevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023. URL\nhttp:\/\/probml.github.io\/book2.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, pp. 4055–4064, 2018.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pp. 4195–\n4205, 2023.\n22\nTechnical Report\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli,\nAlessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refined-\nweb dataset for falcon LLM: outperforming curated corpora with web data only. In NeurIPS,\n2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In ICML, pp.\n8748–8763, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research, 21(140):1–67, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pp. 8821–8831. Pmlr, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with CLIP latents. CoRR, abs\/2204.06125, 2022a.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022b.\nSuman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models.\nNeurIPS, 32, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10684–10695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 35:36479–36494,\n2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In ICML, pp. 2256–2265, 2015.\nTom´aˇs Souˇcek, Dima Damen, Michael Wray, Ivan Laptev, and Josef Sivic. Genhowto: Learning to\ngenerate actions and state transformations from instructional videos. In CVPR, pp. 6561–6571,\n2024.\nKeqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb:\nA benchmark for generative image understanding. In NeurIPS, 2023a.\nPeize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.\nAutoregressive model beats diffusion: Llama for scalable image generation.\narXiv preprint\narXiv:2406.06525, 2024.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,\nYongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models\nare in-context learners. CoRR, abs\/2312.13286, 2023b.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. CoRR,\nabs\/2307.05222, 2023c.\n23\nTechnical Report\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality.\nIn ICLR, 2023d.\nZineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation\nvia composable diffusion. NeurIPS, 36, 2024.\nChameleon Team.\nChameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024.\nShengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha\nAkula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open,\nvision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. CoRR, abs\/2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017a.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017b.\nMitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D Co-\nReyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale\ntransformer training instabilities. arXiv preprint arXiv:2309.14322, 2023.\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu,\nYing Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion\nmodels for text-to-video generation. In ICCV, 2023a.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multi-\nmodal llm. arXiv preprint arXiv:2309.05519, 2023b.\nJinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and\nMike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffu-\nsion. In ICCV, pp. 7452–7461, 2023.\nZeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo.\nRaphael: Text-to-image generation via large mixture of diffusion paths. NeurIPS, 36, 2024.\nHanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan\nXu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv\npreprint arXiv:2405.19335, 2024a.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei\nHuang. mplug-owl2: Revolutionizing multi-modal large language model with modality collabo-\nration. In CVPR, pp. 13040–13051, 2024b.\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on\nmultimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\nLijun Yu, Jos´e Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong\nCheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion–\ntokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023.\nLunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Copilot4d:\nLearning unsupervised world models for autonomous driving via discrete diffusion. In ICLR,\n2024.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. CoRR, abs\/2304.10592,\n2023a.\n24\nTechnical Report\nJinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang,\nand Ying Shan. VL-GPT: A generative pre-trained transformer for vision and language under-\nstanding and generation. CoRR, abs\/2312.09251, 2023b.\n25\nTechnical Report\nA\nAPPENDIX\nA.1\nALTERNATIVE LOWER BOUND FOR THE VARIATIONAL DIFFUSION\nEq(x0)[log pθ(x0)]\n= Eq(x0)[log\nZ\npθ(x0, x1 · · · xT )dx1 · · · xT ]\n= Eq(x0)\n\u001a\nlog Eq(x1:T |x0)\n\u0014pθ(x0:T −1|xT )\nq(x1:T |x0)\np(xT )\n\u0015\u001b\n≥Eq(x0)q(x1:T |x0)\n\u0014\nlog pθ(x0:T −1|xT )\nq(x1:T |x0)\n+ log p(xT )\n\u0015\n= Eq(x0:T )\n\u0014\nT\nX\nt≥1\nlog pθ(xt−1|xt)\nq(xt|xt−1) + log p(xT )\n\u0015\n= Eq(x0:T )\nh\nT\nX\nt≥1\nlog pθ(xt−1|xt) + log p(xT ) −\nT\nX\nt≥1\nlog q(xt|xt−1)\ni\n= Eq(x0:T )\n\u0014\nT\nX\nt≥1\nlog\nX\n˜x0\nq(xt−1|xt, ˜x0)˜pθ(˜x0|xt)\n\u0015\n+ Eq(x0:T )\n\u0014\nlog p(xT ) −\nT\nX\nt≥1\nlog q(xt|xt−1)\n\u0015\n|\n{z\n}\nC1\n= Eq(x0:T )\n\u0014\nT\nX\nt≥1\nlog\nX\n˜x0\nq(xt−1, ˜x0|xt)\nq(˜x0|xt)\n˜pθ(˜x0|xt)\n\u0015\n+ C1\n= Eq(x0:T )\n\u0014\nT\nX\nt≥1\nlog\nX\n˜x0\nq(˜x0|xt−1)\nq(˜x0|xt)\nq(xt|xt−1)q(xt−1)\/q(xt)\nz\n}|\n{\nq(xt−1|xt)\n˜pθ(˜x0|xt)\n\u0015\n+ C1\n≥Eq(x0:T )\nh\nT\nX\nt≥1\nX\n˜x0\nq(˜x0|xt−1) log\n\u0012q(xt−1|xt)\nq(˜x0|xt) ˜pθ(˜x0|xt)\n\u0013 i\n+ C1\n= Eq(x0:T )\n\u0014\nT\nX\nt≥1\nX\n˜x0\nq(˜x0|xt−1) log ˜pθ(˜x0|xt)\n\u0015\n+ C1 + Eq(x0:T )\n\u0014\nT\nX\nt≥1\nX\n˜x0\nq(˜x0|xt−1) log q(xt−1|xt)\nq(˜x0|xt)\n\u0015\n|\n{z\n}\nC2\n=\nT\nX\nt≥1\nEq(x0,xt−1,xt)\n\u0014 X\n˜x0\nq(˜x0|xt−1) log ˜pθ(˜x0|xt)\n\u0015\n+ C1 + C2\n=\nT\nX\nt≥1\nEq(x0,xt−1,xt)q(˜x0|xt−1)[log ˜pθ(˜x0|xt)] + C1 + C2\n=\nT\nX\nt≥1\nEq(x0|xt−1)q(xt|xt−1)q(xt−1)q(˜x0|xt−1)[log ˜pθ(˜x0|xt)] + C1 + C2\n=\nT\nX\nt≥1\nEq(xt|xt−1)q(xt−1,˜x0)[log ˜pθ(˜x0|xt)] + C1 + C2\n=\nT\nX\nt≥1\nEq(xt,x0)[log ˜pθ(x0|xt)] + C1 + C2\n26\nTechnical Report\nThe constants C1 and C2 are:\nC1 = Eq(x0:T )\n\u0014\n−\nT\nX\nt=1\nlog q(xt|xt−1) +\nlog p(xT )\n|\n{z\n}\nNote that p(xT )=q(xT )\n\u0015\n= Eq(x0:T )\n\u0014\n−\nT\nX\nt=1\nlog q(xt, xt−1) +\nT\nX\nt=0\nlog q(xt)\n\u0015\nC2 = Eq(x0:T )\n\u0014\nT\nX\nt=1\nlog q(xt−1|xt)\n\u0015\n−Eq(x0:T )\n\u0014\nT\nX\nt=1\nX\n˜x0\nq(˜x0|xt−1) log q(˜x0|xt)\n\u0015\n= Eq(x0:T )\n\u0014\nT\nX\nt=1\nlog q(xt, xt−1) −\nT\nX\nt=1\nlog q(xt)\n\u0015\n−\nT\nX\nt=1\nEq(x0:T )q(˜x0|xt−1)[log q(˜x0|xt)]\nC1 + C2 = Eq(x0:T )[log q(x0) −PT\nt=1 log q(x0|xt)]\nThe alternative lower bound can be derived as:\nEq(x0)[log pθ(x0)] ≥\nT\nX\nt=1\nEq(xt,x0)[log pθ(x0|xt)] + Eq(x0:T )[log q(x0) −\nT\nX\nt=1\nlog q(x0|xt)]\n=\nT\nX\nt=1\nEq(x0)q(xt|x0)[log pθ(x0|xt)] + C.\nThis proof is provided by Zhang et al. (2024).\nA.2\nIMAGE GENERATION DETAILS\nGiven N text tokens and M [MASK] tokens as initial input, Show-o predict M logits ℓt = {ℓt\ni}M\ni=1\nin parallel, where ℓt\ni ∈R1×(K+1) and t is the time step. For each [MASK] token u∗at location i, we\nsample an image token ut\ni from the codebook based on the predicted probability pt\ni = softmax(ℓt\ni)\nand indicate its predicted score si ∈R as the confidence of this sampled token. The confidence of\nthe unmasked token is set as 1.0. Next, we compute the number of image tokens m that should be\nre-masked based on the mask scheduling function γ, where m = ⌈γ( t\nT )M⌉. More details of mask\nscheduling functions can be found in MaskGIT (Chang et al., 2022). Subsequently, we replace the\npredicted image tokens with [MASK] token u∗based on the following metric:\nu(t+1)\ni\n=\n\u001au∗,\nif si < sortedj(sj)[m].\nut\ni,\notherwise.\n.\n(13)\nThe resulting sequence, consisting of the remaining image tokens and [MASK] tokens, will be fed\nback to Show-o for the subsequent round of prediction until reaching the final time step T. The\nfinalized image tokens are decoded by the image tokenizer into an image.\n27\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Show-o: One Single Transformer to Unify Multimodal Understanding and Generation.pdf"}
{"title":"Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model","authors":"Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy","summary":"We introduce Transfusion, a recipe for training a multi-modal model over\ndiscrete and continuous data. Transfusion combines the language modeling loss\nfunction (next token prediction) with diffusion to train a single transformer\nover mixed-modality sequences. We pretrain multiple Transfusion models up to 7B\nparameters from scratch on a mixture of text and image data, establishing\nscaling laws with respect to a variety of uni- and cross-modal benchmarks. Our\nexperiments show that Transfusion scales significantly better than quantizing\nimages and training a language model over discrete image tokens. By introducing\nmodality-specific encoding and decoding layers, we can further improve the\nperformance of Transfusion models, and even compress each image to just 16\npatches. We further demonstrate that scaling our Transfusion recipe to 7B\nparameters and 2T multi-modal tokens produces a model that can generate images\nand text on a par with similar scale diffusion models and language models,\nreaping the benefits of both worlds.","url":"http:\/\/arxiv.org\/abs\/2408.11039v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.11039v1","published":1724176100000,"comment":"23 pages","pdf_text":"Transfusion: Predict the Next Token and\nDiffuse Images with One Multi-Modal Model\nChunting Zhouµ∗\nLili Yuµ∗\nArun Babuδ†\nKushal Tirumalaµ\nMichihiro Yasunagaµ\nLeonid Shamisµ\nJacob Kahnµ\nXuezhe Maσ\nLuke Zettlemoyerµ\nOmer Levy†\nµ Meta\nδ Waymo σ University of Southern California\nAbstract\nWe introduce Transfusion, a recipe for training a multi-modal model over discrete\nand continuous data. Transfusion combines the language modeling loss function\n(next token prediction) with diffusion to train a single transformer over mixed-\nmodality sequences. We pretrain multiple Transfusion models up to 7B parameters\nfrom scratch on a mixture of text and image data, establishing scaling laws with\nrespect to a variety of uni- and cross-modal benchmarks. Our experiments show\nthat Transfusion scales significantly better than quantizing images and training a\nlanguage model over discrete image tokens. By introducing modality-specific en-\ncoding and decoding layers, we can further improve the performance of Transfusion\nmodels, and even compress each image to just 16 patches. We further demonstrate\nthat scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens\nproduces a model that can generate images and text on a par with similar scale\ndiffusion models and language models, reaping the benefits of both worlds.\n1\nIntroduction\nMulti-modal generative models need to be able to perceive, process, and produce both discrete\nelements (such as text or code) and continuous elements (e.g. image, audio, and video data). While\nlanguage models trained on the next token prediction objective dominate discrete modalities [OpenAI\net al., 2024, Dubey et al., 2024], diffusion models [Ho et al., 2020, Rombach et al., 2022a] and their\ngeneralizations [Lipman et al., 2022] are the state of the art for generating continuous modalities [Dai\net al., 2023, Esser et al., 2024b, Bar-Tal et al., 2024]. Many efforts have been made to combine these\napproaches, including extending a language model to use a diffusion model as a tool, either explicitly\n[Liu et al., 2023] or by grafting a pretrained diffusion model onto the language model [Dong et al.,\n2023, Koh et al., 2024]. Alternatively, one can quantize the continuous modalities [Van Den Oord\net al., 2017] and train a standard language model over discrete tokens [Ramesh et al., 2021, Yu et al.,\n2022, 2023], simplifying the model’s architecture at the cost of losing information. In this work, we\nshow it is possible to fully integrate both modalities, with no information loss, by training a single\nmodel to both predict discrete text tokens and diffuse continuous images.\nWe introduce Transfusion, a recipe for training a model that can seamlessly generate discrete and\ncontinuous modalities. We demonstrate Transfusion by pretraining a transformer model on 50% text\nand 50% image data using a different objective for each modality: next token prediction for text and\ndiffusion for images. The model is exposed to both modalities and loss functions at each training\nstep. Standard embedding layers convert text tokens to vectors, while patchification layers represent\n∗Equal contribution.\n†Work done while at Meta.\narXiv:2408.11039v1  [cs.AI]  20 Aug 2024\nTransformer\nA\ncute\ncat\n.\n<BOI>\n<EOI>\ncute\ncat\n.\n<BOI>\nWhat\ncolor\nWhat\ncolor\nis\nits\nnose\nis\nits\nnose\n?\nFigure 1: A high-level illustration of Transfusion. A single transformer perceives, processes, and\nproduces data of every modality. Discrete (text) tokens are processed autoregressively and trained on\nthe next token prediction objective. Continuous (image) vectors are processed together in parallel\nand trained on the diffusion objective. Marker BOI and EOI tokens separate the modalities.\neach image as a sequence of patch vectors. We apply causal attention for text tokens and bidirectional\nattention for image patches. For inference, we introduce a decoding algorithm that combines the\nstandard practices of text generation from language models and image generation from diffusion\nmodels. Figure 1 illustrates Transfusion.\nIn a controlled comparison with Chameleon’s discretization approach [Chameleon Team, 2024],\nwe show that Transfusion models scale better in every combination of modalities. In text-to-image\ngeneration, we find that Transfusion exceeds the Chameleon approach at less than a third of the\ncompute, as measured by both FID and CLIP scores. When controlling for FLOPs, Transfusion\nachieves approximately 2× lower FID scores than Chameleon models. We observe a similar trend in\nimage-to-text generation, where Transfusion matches Chameleon at 21.8% of the FLOPs. Surprisingly,\nTransfusion is also more efficient at learning text-to-text prediction, achieving perplexity parity on\ntext tasks around 50% to 60% of Chameleon’s FLOPs.\nAblation experiments reveal critical components and potential improvements for Transfusion. We\nobserve that the intra-image bidirectional attention is important, and that replacing it with causal\nattention hurts text-to-image generation. We also find that adding U-Net down and up blocks to\nencode and decode images enables Transfusion to compress larger image patches with relatively\nsmall loss to performance, potentially decreasing the serving costs by up to 64×.\nFinally, we demonstrate that Transfusion can generate images at similar quality to other diffusion\nmodels. We train from scratch a 7B transformer enhanced with U-Net down\/up layers (0.27B\nparameters) over 2T tokens: 1T text tokens, and approximately 5 epochs of 692M images and their\ncaptions, amounting to another 1T patches\/tokens. Figure 2 shows some generated images sampled\nfrom the model. On the GenEval [Ghosh et al., 2023] benchmark, our model outperforms other\npopular models such as DALL-E 2 and SDXL; unlike those image generation models, it can generate\ntext, reaching the same level of performance as Llama 1 on text benchmarks. Our experiments thus\nshow that Transfusion is a promising approach for training truly multi-modal models.\n2\nBackground\nTransfusion is a single model trained with two objectives: language modeling and diffusion. Each of\nthese objectives represents the state of the art in discrete and continuous data modeling, respectively.\nThis section briefly defines these objectives, as well as background on latent image representations.\n2.1\nLanguage Modeling\nGiven a sequence of discrete tokens y = y1, ..., yn from a closed vocabulary V , a language model\npredicts the probability of the sequence P(y). Standard language models decompose P(y) into a\nproduct of conditional probabilities Qn\ni=1 Pθ(yi|y<i). This creates an autoregressive classification\ntask, where the probability distribution of each token yi is predicted conditioned on the prefix of a\nsequence y<i using a single distribution Pθ parameterized by θ. The model can be optimized by\nminimizing the cross-entropy between Pθ and the empirical distribution of the data, yielding the\nstandard next-token prediction objective, colloquially referred to as LM loss:\nLLM = Eyi\n\u0002\n−log Pθ(yi|y<i)\n\u0003\n(1)\n2\nAn armchair in the shape\nof an avocado\nA bread, an apple, and a\nknife on a table\nA corgi.\nhuman life depicted en-\ntirely out of fractals\nA blue jay standing on a\nlarge basket of rainbow\nmacarons.\n“Transfusion\" is written\non the blackboard.\nA close up photo of a hu-\nman hand, hand model.\nHigh quality\nA cloud in the shape of\ntwo bunnies playing with\na ball. The ball is made of\nclouds too.\nthe word ‘START’ on a\nblue t-shirt\nA Dutch still life of an\narrangement of tulips in\na fluted vase. The light-\ning is subtle, casting gen-\ntle highlights on the flow-\ners and emphasizing their\ndelicate details and natu-\nral beauty.\nA wall in a royal castle.\nThere are two paintings\non the wall. The one on\nthe left a detailed oil paint-\ning of the royal raccoon\nking. The one on the right\na detailed oil painting of\nthe royal raccoon queen.\nThree spheres made of\nglass falling into ocean.\nWater is splashing. Sun\nis setting.\nA transparent sculpture of\na duck made out of glass.\nA chromeplated cat sculp-\nture placed on a Persian\nrug.\nA kangaroo holding a\nbeer, wearing ski goggles\nand passionately singing\nsilly songs.\nan egg and a bird made of\nwheat bread\nFigure 2: Generated images from a 7B Transfusion trained on 2T multi-modal tokens.\n3\nOnce trained, language models can also be used to generate text by sampling token by token from the\nmodel distribution Pθ, typically using temperature and top-p truncation.\n2.2\nDiffusion\nDenoising diffusion probabilistic models (a.k.a. DDPM or diffusion models) operate on the principle\nof learning to reverse a gradual noise-addition process [Ho et al., 2020]. Unlike language models that\ntypically work with discrete tokens (y), diffusion models operate over continuous vectors (x), making\nthem particularly suited for tasks involving continuous data like images. The diffusion framework\ninvolves two processes: a forward process that describes how the original data is turned into noise,\nand a reverse process of denoising that the model learns to perform.\nForward Process\nFrom a mathematical perspective, the forward process defines how the noised data\n(which serves as the model input) is created. Given a data point x0, Ho et al. [2020] define a Markov\nchain that gradually adds Gaussian noise over T steps, creating a sequence of increasingly noisy ver-\nsions x1, x2, ..., xT . Each step of this process is defined by q(xt|xt−1) = N(xt; √1 −βtxt−1, βtI),\nwhere βt increases over time according to a predefined noise schedule (see below). This process can\nbe reparameterized in a way that allows us to directly sample xt from x0 using a single sample of\nGaussian noise ϵ ∼N(0, I):\nxt = √¯αtx0 +\n√\n1 −¯αtϵ\n(2)\nHere, ¯αt = Qt\ns=1(1 −βs), providing a useful abstraction over the original Markov chain. In fact,\nboth the training objective and the noise scheduler are eventually expressed (and implemented) in\nthese terms.\nReverse Process\nThe diffusion model is trained to perform the reverse process pθ(xt−1|xt),\nlearning to denoise the data step by step. There are several ways to do so; in this work, we follow\nthe approach of Ho et al. [2020] and model the Gaussian noise ϵ in Equation 2 as a proxy for the\ncumulative noise at step t. Specifically, a model ϵθ(·) with parameters θ is trained to estimate the\nnoise ϵ given the noised data xt and timestep t. In practice, the model often conditions on additional\ncontextual information c, such as a caption when generating an image. The parameters of the noise\nprediction model are thus optimized by minimizing the mean squared error loss:\nLDDPM = Ex0,t,ϵ\n\u0002\n||ϵ −ϵθ(xt, t, c)||2\u0003\n(3)\nNoise Schedule\nWhen creating a noised example xt (Equation 2), ¯αt determines the variance of\nthe noise for timestep t. In this work, we adopt the commonly used cosine scheduler Nichol and\nDhariwal [2021], which largely follows √¯αt ≈cos( t\nT · π\n2 ) with some adjustments.\nInference\nDecoding is done iteratively, pealing away some of the noise at each step. Starting\nwith pure Gaussian noise at xT , the model ϵθ(xt, t, c) predicts the noise accumulated at timestep\nt. The predicted noise is then scaled according to the noise schedule, and the proportional amount\nof predicted noise is removed from xt to produce xt−1. In practice, inference is done over fewer\ntimesteps than training. Classifier-free guidance (CFG) [Ho and Salimans, 2022] is often used to\nimprove generation by contrasting the prediction of the model conditioned on the context c with the\nunconditioned prediction, at the cost of doubling the computation.\n2.3\nLatent Image Representation\nEarly diffusion models worked directly in pixel space [Ho et al., 2020], but this proved computation-\nally expensive. Variational autoencoders (VAEs) [Kingma and Welling, 2013] can save compute by\nencoding images into a lower-dimensional latent space. Implemented as deep CNNs, modern VAEs\nare trained on a combination of reconstruction and regularization losses [Esser et al., 2021], allowing\ndownstream models like latent diffusion models (LDMs) [Rombach et al., 2022a] to operate efficiently\non compact image patch embeddings; e.g. represent every 8×8 pixel patch as an 8-dimensional\nvector. For autoregressive language modeling approaches [Ramesh et al., 2021, Yu et al., 2022],\nimages must be discretized. Discrete autoencoders, such as vector-quantized VAEs (VQ-VAE) [Van\nDen Oord et al., 2017], achieve this by introducing a quantization layer (and related regularization\nlosses) that maps continuous latent embeddings to discrete tokens.\n4\nTransformer\nVAE Encoder\nor\nNoising\nLinear\nU-Net Down\nor\nLinear\nVAE Decoder\nU-Net Up\nFigure 3: We convert images to and from la-\ntent representations using a pretrained VAE, and\nthen into patch representations with either a sim-\nple linear layer or U-Net down blocks.\nA\ncute\n<BOI>\n<EOI>\nWhat\nA\ncat <BOI>\n<EOI>What\ncat\ncute\nFigure 4: Expanding on the causal mask, Trans-\nfusion allows patches of the same image to con-\ndition on each other.\n3\nTransfusion\nTransfusion is a method for training a single unified model to understand and generate both discrete\nand continuous modalities. Our main innovation is demonstrating that we can use separate losses\nfor different modalities – language modeling for text, diffusion for images – over shared data and\nparameters. Figure 1 illustrates Transfusion.\nData Representation\nWe experiment with data spanning two modalities: discrete text and continu-\nous images. Each text string is tokenized into a sequence of discrete tokens from a fixed vocabulary,\nwhere each token is represented as an integer. Each image is encoded as latent patches using a\nVAE (see §2.3), where each patch is represented as a continuous vector; the patches are sequenced\nleft-to-right top-to-bottom to create a sequence of patch vectors from each image.3 For mixed-modal\nexamples, we surround each image sequence with special beginning of image (BOI) and end of image\n(EOI) tokens before inserting it to the text sequence; thus, we arrive at a single sequence potentially\ncontaining both discrete elements (integers representing text tokens) and continuous elements (vectors\nrepresenting image patches).\nModel Architecture\nThe vast majority of the model’s parameters belong to a single transformer,\nwhich processes every sequence, regardless of modality.45 The transformer takes a sequence of\nhigh-dimensional vectors in Rd as input, and produces similar vectors as output. To convert our\ndata into this space, we use lightweight modality-specific components with unshared parameters.\nFor text, these are the embedding matrices, converting each input integer to vector space and each\noutput vector into a discrete distribution over the vocabulary. For images, we experiment with two\nalternatives for compressing local windows of k × k patch vectors into a single transformer vector\n3While our canonical setting uses a VAE following latent diffusion models, we were also able to demonstrate\nTransfusion using raw pixel representations in preliminary experiments.\n4We follow Llama’s [Touvron et al., 2023a] flavor of the transformer block, which includes the SwiGLU\nactivation function [Shazeer, 2020] and RoPE [Su et al., 2024].\n5While we use the transformer architecture in this work, Transfusion could potentially work with other\narchitectures too, despite its name.\n5\n(and vice versa): (1) a simple linear layer,6 and (2) up and down blocks of a U-Net [Nichol and\nDhariwal, 2021, Saharia et al., 2022].7 Figure 3 illustrates the overall architecture.\nTransfusion Attention\nLanguage models typically use causal masking to efficiently compute\nthe loss and gradients over an entire sequence in a single forward-backward pass without leaking\ninformation from future tokens. While text is naturally sequential, images are not, and are usually\nmodeled with unrestricted (bidirectional) attention. Transfusion combines both attention patterns\nby applying causal attention to every element in the sequence, and bidirectional attention within the\nelements of each individual image. This allows every image patch to attend to every other patch\nwithin the same image, but only attend to text or patches of other images that appeared previously in\nthe sequence. We find that enabling intra-image attention significantly boosts model performance\n(see §4.3). Figure 4 shows an example Transfusion attention mask.\nTraining Objective\nTo train our model, we apply the language modeling objective LLM to pre-\ndictions of text tokens and the diffusion objective LDDPM to predictions of image patches. LM\nloss is computed per token,8 while diffusion loss is computed per image, which may span multiple\nelements (image patches) in the sequence. Specifically, we add noise ϵ to each input latent image\nx0 according to the diffusion process to produce xt before patchification, and then compute the\nimage-level diffusion loss.9 We combine the two losses by simply adding the losses computed over\neach modality with a balancing coefficient λ:\nLTransfusion = LLM + λ · LDDPM\n(4)\nThis formulation is a specific instantiation of a broader idea: combining a discrete distribution loss\nwith a continuous distribution loss to optimize the same model. We leave further exploration of this\nspace, such as replacing diffusion with flow matching [Lipman et al., 2022]), to future work.\nInference\nReflecting the training objective, our decoding algorithm also switches between two\nmodes: LM and diffusion. In LM mode, we follow the standard practice of sampling token by token\nfrom the predicted distribution. When we sample a BOI token, the decoding algorithm switches\nto diffusion mode, where we follow the standard procedure of decoding from diffusion models.\nSpecifically, we append a pure noise xT in the form of n image patches to the input sequence\n(depending on the desired image size), and denoise over T steps. At each step t, we take the noise\nprediction and use it to produce xt−1, which then overwrites xt in the sequence; i.e. the model always\nconditions on the last timestep of the noised image and cannot attend to previous timesteps. Once the\ndiffusion process has ended, we append an EOI token to the predicted image, and switch back to LM\nmode. This algorithm enables the generation of any mixture of text and image modalities.\n4\nExperiments\nWe demonstrate in a series of controlled experiments that Transfusion is a viable, scalable method for\ntraining a unified multi-modal model.\n4.1\nSetup\nEvaluation\nWe evaluate model performance on a collection of standard uni-modal and cross-\nmodal benchmarks (Table 1). For text-to-text, we measure perplexity on 20M held-out tokens from\nWikipedia and the C4 corpus [Raffel et al., 2019], as well as accuracy on the pretraining evaluation\nsuite of Llama 2 [Touvron et al., 2023b].10 For text-to-image, we use the MS-COCO benchmark\n[Lin et al., 2014], where we generate images on randomly selected 30k prompts from validation set\nand measure their photo-realism using zero-shot Frechet Inception Distance (FID) [Heusel et al.,\n6We add an embedding of the timestep t to every patch vector before the linear layer.\n7We replace the U-Net’s AdaLayerNorm with regular layer norm in our implementation.\n8When the input is a BOI token, we do not compute any loss.\n9Ergo, downstream tokens condition on noisy images during training. See §4.3.4 for further discussion.\n10The Llama 2 evaluation suite includes HellaSwag [Zellers et al., 2019], PIQA [Bisk et al., 2020], SIQA [Sap\net al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-e and -c [Clark et al., 2018], and BoolQ [Clark et al.,\n2019]. We report the average 0-shot task accuracy on these benchmarks.\n6\nInput\nOutput\nBenchmark\nMetric\nText\nText\nWikipedia\nPerplexity (↓)\nC4\nPerplexity (↓)\nLlama 2 Eval Suite\nAccuracy (↑)\nImage\nText\nMS-COCO 5k\nCIDEr (↑)\nText\nImage\nMS-COCO 30k\nFID (↓), CLIP (↑)\nGenEval\nGenEval score (↑)\nTable 1: An overview of the evaluation suite used in this\nwork.\nSize\nLayers\nEmb Dim\nAtt Heads\n0.16B\n16\n768\n12\n0.37B\n24\n1024\n16\n0.76B\n24\n1536\n24\n1.4B\n24\n2048\n16\n7B\n32\n4096\n32\nTable 2: Model sizes and configurations\nfor both Transfusion and baselines.\n2017] as well as their alignment with the prompts using CLIP score [Radford et al., 2021].11 We also\nevaluate the model’s ability to generate image captions; we report CIDEr [Vedantam et al., 2015]\nscores on the Karpathy test split of MS-COCO [Lin et al., 2014]. These evaluations provide signal for\ninvestigation scaling laws (§4.2) and ablations (§4.3). To compare with recent literature in diffusion\nmodels, we evaluate our largest scale model (§4.4) also on GenEval [Ghosh et al., 2023], a benchmark\nthat examines a model’s ability to generate an accurate depiction of the prompt.\nBaseline\nAt the time of writing, the prominent open-science method for training a single mixed-\nmodal model that can generate both text and images is to quantize images into discrete tokens, and\nthen model the entire token sequence with a standard language model [Ramesh et al., 2021, Yu\net al., 2022, 2023]. We follow the recipe of Chameleon [Chameleon Team, 2024] to train a family of\ndata- and compute-controlled baseline models, which we can directly compare to our Transfusion\nmodels. The key difference between Chameleon and Transfusion is that while Chameleon discretizes\nimages and processes them as tokens, Transfusion keeps images in continuous space, removing the\nquantization information bottleneck. To further minimize any confounding variables, we train the\nVAEs for Chameleon and Transfusion using exactly the same data, compute, and architecture, with\nthe only differentiator being the quantization layer and codebook loss of Chameleon’s VQ-VAE (see\ndetails below). Chameleon also deviates from the Llama transformer architecture, adding query-key\nnormalization, post-normalization, denominator loss, and a lower learning rate of 1e-4 to manage\ntraining instability, which incur an efficiency cost (see §4.2).12\nData\nFor almost all of our experiments, we sample 0.5T tokens (patches) from two datasets at a 1:1\ntoken ratio. For text, we use the Llama 2 tokenizer and corpus [Touvron et al., 2023b], containing 2T\ntokens across a diverse distribution of domains. For images, we use a collection of 380M licensed\nShutterstock images and captions. Each image is center-cropped and resized to produce a 256×256\npixel image.13 We randomly order the image and captions, ordering the caption first 80% of the time.\nIn one experiment (4.4) we scale up the total training data to 2T tokens (1T text tokens and about\n3.5B caption-image pairs at 256 patches per image). To diversify, we add 220M publicly available\nimages with captions, prefiltered to not contain people. To rebalance the distribution, we upsample\n80M Shutterstock images containing people. We also add data from Conceptual 12M (CC12M)\n[Changpinyo et al., 2021], reaching a total mixture of 692M image-caption pairs per epoch. Finally,\nwe upweight the portion of high-aesthetic images in the last 1% of the training schedule.\nLatent Image Representation\nWe train a 86M parameter VAE following Esser et al. [2021].\nWe use a CNN encoder and decoder, and latent dimension 8. The training objective is combines\nreconstruction and regularization losses.14 Our implementation reduces an image of 256×256 pixels\nto a 32×32×8 tensor, where each latent 8-dimensional latent pixel represents (conceptually) an 8×8\npixel patch in the original image, and trains for 1M steps. For VQ-VAE training, we follow the same\n11We follow common practice for ablations and use only 5k examples to compute FID and CLIP in §4.3.\n12Removing these deviations in preliminary experiments encountered optimization instabilities in Chameleon.\n13Depending on the compression rate of the patch encoder (see Model Architecture in §3), each image will be\nrepresented by either 1024, 256, 64, or 16 elements in the sequence. Since the text\/image ratio is constant during\ntraining, higher compression rates enable training on more images in total, at the cost of less compute per image.\n14See Appendix A for details.\n7\nsetup described for VAE training, except we replace LKL with the standard codebook commitment\nloss with β = 0.25 [Van Den Oord et al., 2017]. We use a codebook of 16,384 token types.\nModel Configuration\nTo investigate scaling trends, we train models at five different sizes – 0.16B,\n0.37B, 0.76B, 1.4B, and 7B parameters – following the standard settings from Llama [Touvron et al.,\n2023a]. Table 2 describes each setting in detail. In configurations that use linear patch encoding (§4.2\nand §4.3), the number of additional parameters is insignificant, accounting for fewer than 0.5% of\ntotal parameters in every configuration. When using U-Net patch encoding (§4.3 and §4.4), these\nparameters add up to 0.27B additional parameters across all configurations; while this is a substantial\naddition of parameters to smaller models, these layers amount to only a 3.8% increase of the 7B\nconfiguration, almost identical to the number of parameters in the embedding layers.\nOptimization\nWe randomly initialize all model parameters, and optimize them using AdamW\n(β1 =0.9, β2 =0.95, ϵ =1e-8) with a learning rate of 3e-4, warmed up for 4000 steps and decaying\nto 1.5e-5 using a cosine scheduler. We train on sequences of 4096 tokens in batches of 2M tokens for\n250k steps, reaching 0.5T tokens in total. In our large-scale experiment (§4.4), we train with a batch\nsize of 4M tokens over 500k steps, totalling 2T tokens. We regularize with weight decay of 0.1 and\nclip gradients by norm (1.0). We set the λ coefficient in the Transfusion objective (Equation 4) to 5\nfollowing preliminary experiments; we leave further tuning of λ to future work.\nInference\nIn text mode, we use greedy decoding for generating text. Ranked classification is used\nfor the Llama evaluation suite. For image generation, we follow the standard of 250 diffusion steps\n(the model is trained on 1,000 timesteps). We follow Chameleon and use CFG with a coefficient of 5\nin the controlled comparison experiments (§4.2). This value is suboptimal for Transfusion, and so\nwe use a CFG coefficient of 3 throughout the ablation experiments (§4.3), and follow the standard\npractice of tuning the coefficient for each benchmark in our large scale experiment (§4.4).\n4.2\nControlled Comparison with Chameleon\nWe run a series of controlled experiments to compare Transfusion with Chameleon at different model\nsizes (N) and token counts (D), using the combination of both as a proxy for FLOPs (6ND).15 For\nsimplicity and parameter control, the Transfusion variant in these experiments uses simple linear\nimage encoder\/decoder with patch size 2×2, as well as bidirectional attention. For each benchmark,\nwe plot all results on a log-metric over log-FLOPs curve and regress linear trendlines.16 We also\nestimate relative compute efficiency by measuring the parity FLOP ratio: the ratio between the\nnumber of FLOPs required by Transfusion and Chameleon to reach the same level of performance.\nFigure 5 visualizes the scaling trends, and Table 3 shows the results of the largest models in this\ncontrolled setting and their estimated parity FLOP ratio. In every benchmark, Transfusion consistently\nexhibits better scaling laws than Chameleon. While the lines are close to parallel, there is a significant\ngap in Transfusion’s favor. The difference in compute efficiency is particularly striking in image\ngeneration, where FID Transfusion achieves parity with Chameleon using 34× less compute.\nSurprisingly, text-only benchmarks also reveal better performance with Transfusion, even though\nboth Transfusion and Chameleon model text in the same way. We investigate this phenomenon by\nablating the various changes leading up to Transfusion and Chameleon from the original Llama 2\nrecipe. Table 4 shows that while Transfusion does come at a non-zero cost to text performance, the\nChameleon recipe suffers from both the stability modifications made to the architecture and from\nthe introduction of image tokens. Training on quantized image tokens degrades text performance\nmore than diffusion on all three benchmarks. One hypothesis is that this stems from the competition\nbetween text and image tokens in the output distribution; alternatively, it is possible that diffusion is\nmore efficient at image generation and requires fewer parameters, allowing Transfusion models to\nuse more capacity than Chameleon to model text. We leave further investigation of this phenomenon\nto future research.\n15Since Transfusion uses continuous representations of images, it can express a single image with significantly\nfewer tokens, shortening the average document length and thus the overall quadratic price of attention. Since\nthis fact favors Transfusion, we remove this confounder by using the theoretical FLOP calculation.\n16The smaller Chameleon models perform poorly on image generation and understanding tasks, leading to\noutlier results that do not correlate with the emerging scaling law of larger Chameleon models. We therefore\ndefine minimal performance thresholds below which we remove datapoints: ≤100 FID, ≥17 CLIP, ≥4 CIDEr.\n8\n1e+20\n1e+21\n1e+22\nFLOPs\n8\n16\nPPL\nTransfusion\nChameleon\nC4 Perplexity\n1e+20\n1e+21\n1e+22\nFLOPs\n4\n8\n16\nPPL\nTransfusion\nChameleon\nWikipedia Perplexity\n1e+20\n1e+21\n1e+22\nFLOPs\n32\n64\nAcc\nTransfusion\nChameleon\nLlama 2 Eval Suite Accuracy\n1e+20\n1e+21\n1e+22\nFLOPs\n2\n4\n8\n16\n32\nCIDEr\nTransfusion\nChameleon\nMS-COCO 5k CIDEr\n1e+20\n1e+21\n1e+22\nFLOPs\n16\n32\n64\n128\nFID\nTransfusion\nChameleon\nMS-COCO 30k FID\n1e+20\n1e+21\n1e+22\nFLOPs\n12\n24\nCLIP\nTransfusion\nChameleon\nMS-COCO 30k CLIP\nFigure 5: Performance of Transfusion and Chameleon models at different scales, controlled for\nparameters, data, and compute. All axes are logarithmic.\nModel\nC4\nWiki\nLlama\nMS-COCO\nPPL (↓)\nPPL (↓)\nAcc (↑)\nCDr (↑)\nFID (↓)\nCLIP (↑)\nTransfusion\n7.72\n4.28\n61.5\n27.2\n16.8\n25.5\nChameleon\n8.41\n4.69\n59.1\n18.0\n29.6\n24.3\nParity FLOP Ratio\n0.489\n0.526\n0.600\n0.218\n0.029\n0.319\nTable 3: Performance of the largest (7B) Transfusion and Chameleon models in a controlled setting.\nBoth models were trained on 0.5T tokens. Parity FLOP Ratio is the relative amount of Transfusion\nFLOPs needed to match the results of Chameleon 7B.\n9\nModel\nBatch\nC4\nWiki\nLlama\nPPL (↓)\nPPL (↓)\nAcc (↑)\nLlama 2\n1M Text Tokens\n10.1\n5.8\n53.7\nTransfusion\n+ Diffusion\n+ 1M Image Patches\n(+0.3) 10.4\n(+0.2) 6.0\n(-2.0) 51.7\nChameleon\n+ Stability Modifications\n1M Text Tokens\n(+0.9) 11.0\n(+0.5) 6.3\n(-1.8) 51.9\n+ LM Loss on Image Tokens\n+ 1M Image Tokens\n(+0.8) 11.8\n(+0.5) 6.8\n(-3.0) 48.9\nTable 4: Performance of the 0.76B Transfusion and Chameleon models on text-only benchmarks,\ncompared to the original Llama 2 recipe.\nEnc\/Dec\nAttention\nC4\nWiki\nLlama\nMS-COCO\nPPL (↓)\nPPL (↓)\nAcc (↓)\nCDr (↑)\nFID (↓)\nCLIP (↑)\nLinear\nCausal\n10.4\n6.0\n51.4\n12.7\n61.3\n23.0\nBidirectional\n10.4\n6.0\n51.7\n16.0\n20.3\n24.0\nU-Net\nCausal\n10.3\n5.9\n52.0\n23.3\n16.8\n25.3\nBidirectional\n10.3\n5.9\n51.9\n25.4\n16.7\n25.4\nTable 5: Performance of 0.76B Transfusion models with and without intra-image bidirectional\nattention. Patch size is set at 2×2 latent pixels.\n4.3\nArchitecture Ablations\nNow that we have established that Transfusion is a viable, scalable approach to multi-modal modeling\nin a controlled environment, we can explore improvements and extensions that are applicable to\nTransfusion alone.\n4.3.1\nAttention Masking\nWe first examine the necessity of intra-image bidirectional attention. Table 5 shows that enabling this\nattention pattern beyond the standard causal attention is advantageous throughout all benchmarks, and\nusing both image encoding\/decoding architectures. In particular, we notice a significant improvement\nin FID when using linear encoding layers (61.3→20.3). In the causal-only version of this architecture,\nthere is no flow of information from patches that appear later in the sequence to those before; since\nU-Net blocks contain bidirectional attention within, independent of the transformer’s attention mask,\nthis gap is less pronounced when they are applied.\n4.3.2\nPatch Size\nTransfusion models can be defined over different sizes of latent pixel patches. Larger patch sizes allow\nthe model to pack more images in each training batch and dramatically reduce inference compute,\nbut may come at a performance cost. Table 6 sheds light on these performance trade-offs. While\nperformance does decrease consistently as each image is represented by fewer patches with linear\nencoding, models with U-Net encoding benefit from larger patches on tasks involving the image\nmodality. We posit that this is due to the greater amount of total images (and diffusion noise) seen\nduring training. We also observe that text performance deteriorates with larger patches, perhaps\nbecause transfusion needs to exert more resources (i.e. parameters) to learn how to process images\nwith fewer patches and thus less inference compute.\n4.3.3\nPatch Encoding\/Decoding Architecture\nOur experiments so far indicate an advantage to using the U-Net up and down blocks instead of a\nsimple linear layer. One possible reason is that the model benefits from the inductive biases of the\nU-Net architecure; an alternative hypothesis is that this advantage stems from the significant increase\nin overall model parameters introduced by the U-Net layers. To decouple these two confounders,\nwe scale up the core transformer to 7B parameters, while keeping the amount of U-Net parameters\n10\nEnc\/Dec\nLatent\/\nPixel\/\nPatch\/\nC4\nWiki\nLlama\nMS-COCO\nPatch\nPatch\nImage\nPPL (↓)\nPPL (↓)\nAcc (↓)\nCDr (↑)\nFID (↓)\nCLIP (↑)\nNone\n1×1\n8×8\n1024\n10.3\n5.9\n52.2\n12.0\n21.0\n24.0\nLinear\n2×2\n16×16\n256\n10.4\n6.0\n51.7\n16.0\n20.3\n24.0\n4×4\n32×32\n64\n10.9\n6.3\n49.8\n14.3\n25.6\n22.6\n8×8\n64×64\n16\n11.7\n6.9\n47.7\n11.3\n43.5\n18.9\nU-Net\n2×2\n16×16\n256\n10.3\n5.9\n51.9\n25.4\n16.7\n25.4\n4×4\n32×32\n64\n10.7\n6.2\n50.7\n29.9\n16.0\n25.7\n8×8\n64×64\n16\n11.4\n6.6\n49.2\n29.5\n16.1\n25.2\nTable 6: Performance of 0.76B Transfusion models with different patch sizes. Bolded figures indicate\nglobal best, underlines indicate best within architecture.\nModel\nEnc\/Dec\n∆Enc\/Dec\nC4\nWiki\nLlama\nMS-COCO\nParams\nParams\nPPL (↓)\nPPL (↓)\nAcc (↑)\nCDr (↑)\nFID (↓)\nCLIP (↑)\n0.16B\nLinear\n0.5%\n14.8\n8.8\n44.2\n6.2\n37.6\n20.0\nU-Net\n106.1%\n14.4\n8.5\n45.7\n15.3\n18.8\n23.9\n0.37B\nLinear\n0.4%\n12.0\n7.0\n47.9\n11.1\n21.5\n22.4\nU-Net\n71.3%\n11.8\n6.9\n48.8\n21.1\n18.1\n24.9\n0.76B\nLinear\n0.4%\n10.4\n6.0\n51.7\n16.0\n20.3\n24.0\nU-Net\n35.5%\n10.3\n5.9\n51.9\n25.4\n16.7\n25.4\n1.4B\nLinear\n0.4%\n9.5\n5.4\n53.8\n19.1\n19.4\n24.3\nU-Net\n19.3%\n9.4\n5.4\n53.4\n28.1\n16.6\n25.7\n7B\nLinear\n0.3%\n7.7\n4.3\n61.5\n27.2\n18.6\n25.9\nU-Net\n3.8%\n7.8\n4.3\n61.1\n33.7\n16.0\n26.5\nTable 7: Performance of linear and U-Net variants of Transfusion across different model sizes. Patch\nsize is set at 2×2 latent pixels. Model parameters refers to the transformer alone.\n(almost) constant;17 in this setting, the additional encoder\/decoder parameters account for only a\n3.8% increase of total model parameters, equivalent to the amount of token embedding parameters.\nTable 7 shows that even though the relative benefit of U-Net layers shrinks as the transformer grows,\nit does not diminish. In image generation, for example, the U-Net encoder\/decoder allows much\nsmaller models to obtain better FID scores than the 7B model with linear patchification layers. We\nobserve a similar trend in image captioning, where adding U-Net layers boosts the CIDEr score\nof a 1.4B transformer (1.67B combined) beyond the performance of the linear 7B model. Overall,\nit appears that there are indeed inductive bias benefits to U-Net encoding and decoding of images\nbeyond the mere addition of parameters.\n4.3.4\nImage Noising\nOur experiments order 80% of image-caption pairs with the caption first, and the image conditioning\non the caption, following the intuition that image generation may be a more data-hungry task than\nimage understanding. The remaining 20% of the pairs condition the caption on the image. However,\nthese images are noised as part of the diffusion objective. We thus measure the effect of limiting the\ndiffusion noise to a maximum of t = 500 (half of the noise schedule) in the 20% of cases where\nimages appear before their captions. Table 8 shows that noise limiting significantly improves image\ncaptioning, as measure by CIDEr, while having a relatively small effect (less than 1%) on other\nbenchmarks.\n17While we do not scale the U-Net layers with the transformer in these experiments, this is a potentially\nfruitful avenue for future research.\n11\nModel\nNoise\nC4\nWiki\nLlama\nMS-COCO\nParams\nLimit\nPPL (↓)\nPPL (↓)\nAcc (↑)\nCDr (↑)\nFID (↓)\nCLIP (↑)\n0.76B\n10.3\n5.9\n51.9\n25.4\n16.7\n25.4\n✓\n10.3\n5.9\n52.1\n29.4\n16.5\n25.4\n7B\n7.8\n4.3\n61.1\n33.7\n16.0\n26.5\n✓\n7.7\n4.3\n60.9\n35.2\n15.7\n26.3\nTable 8: Performance of Transfusion with and without limiting the amount of sampled diffusion noise\nto a maximum of t = 500 when images appear before the caption. The models are U-Net variants\nencoding 2×2 latent pixel patches. Metrics that change by over 1% are bolded.\nModel\nModel\nText\nImages\nLlama\nCOCO\nGen\nParams\nTokens\nAcc (↑)\nFID (↓)\nEval (↑)\nLlama 1 [Touvron et al., 2023a]\n7B\n1.4T\n—\n66.1\n—\n—\nLlama 2 [Touvron et al., 2023b]\n7B\n2.0T\n—\n66.3\n—\n—\nChameleon [Chameleon Team, 2024]\n7B\n6.0T\n3.5B\n67.1\n26.74\n0.39\nImagen [Saharia et al., 2022]\n2.6B + 4.7B∗\n—\n5.0B\n—\n7.27\n—\nParti [Yu et al., 2022]\n20B\n—\n4.8B\n—\nr7.23\n—\nSD 1.5 [Rombach et al., 2022b]\n0.9B + 0.1B∗\n—\n4.0B\n—\n—\n0.43\nSD 2.1 [Rombach et al., 2022b]\n0.9B + 0.1B∗\n—\n2.3B\n—\n—\n0.50\nDALL-E 2 [Ramesh et al., 2022]\n4.2B + 1B∗\n—\n2.6B\n—\n10.39\n0.52\nSDXL [Podell et al., 2023]\n2.6B + 0.8B∗\n—\n1.6B\n—\n—\n0.55\nDeepFloyd [Stability AI, 2024]\n5.5B + 4.7B∗\n—\n7.5B\n—\n6.66\n0.61\nSD 3 [Esser et al., 2024b]\n8B + 4.7B∗\n—\ns2.0B\n—\n—\n0.68\nTransfusion (Ours)\n7.3B\n1.0T\n3.5B\n66.1\n6.78\n0.63\nTable 9: Performance of a 7B Transfusion model (U-Net encoder\/decoder layers, 2×2 latent pixel\npatches) trained on the equivalent of 2T tokens, compared to similar scale models in the literature.\nExcept Chameleon, all the other models are restricted to generating one modality (either text or\nimage). ∗Frozen text encoder parameters. r Parti samples 16 images for every prompt and then\nreranks with an auxiliary scoring model. s SD 3 trains with synthetic caption data, which provides\nboosts GenEval performance.\n4.4\nComparison with Image Generation Literature\nOur experiments thus far have covered controlled comparisons with Chameleon and Llama, but we\nhave yet to compare Transfusion’s image generation capabilities to those of state-of-the-art image\ngeneration models. To that end, we train a 7B parameter model with U-Net encoding\/decoding\nlayers (2×2 latent pixel patches) over the equivalent of 2T tokens, comprising of 1T text corpus\ntokens and 3.5B images and their captions. While the Transfusion variant in §4.2 favored simplicity\nand experimental control, the design choices and data mixture (§4.1) of this variant lean a bit more\ntowards image generation. Figure 2 and Appendix B showcase generated images from this model.\nWe compare the performance of our model to reported results of other similar scale image generation\nmodels, as well as some publicly available text generating models for reference. Table 9 shows\nthat Transfusion achieves similar performance to high-performing image generation models such\nas DeepFloyd [Stability AI, 2024], while surpassing previously published models including SDXL\n[Podell et al., 2023]. While Transfusion does lag behind SD 3 [Esser et al., 2024a], this model\nleveraged synthetic image captions through backtranslation [Betker et al., 2023], which enhances\nits GenEval performance by 6.5% absolute (0.433→0.498) at smaller scale; for simplicity, our\nexperimental setup only included natural data. Finally, we note that our Transfusion model can also\ngenerate text, and performs on par with the Llama models, which were trained on the same text data\ndistribution (§4.1).\n12\nRemove the cupcake on the plate.\nChange the tomato on the right to a green olive.\nWrite the word \"Zebra\" in Arial bold.\nChange this to cartoon style.\nFigure 6: Edited images from a fine-tuned 7B Transfusion model.\n4.5\nImage Editing\nOur Transfusion models, which have been pretrained on text-text, image-text, and text-image data,\nperform well across these modality pairings. Can these models extend their capabilities to generate\nimages based on other images? To investigate, we fine-tuned our 7B model (§4.4) using a dataset of\nonly 8k publicly available image editing examples, where each example consists of an input image,\nan edit prompt, and an output image. This approach, inspired by LIMA [Zhou et al., 2024], allows us\nto assess how well the model can generalize to image-to-image generation, a scenario not covered\nduring pretraining.\nManual examination of random examples from the EmuEdit test set [Sheynin et al., 2024], shown in\nFigure 6 and Appendix 4.5, reveals that our fine-tuned Transfusion model performs image edits as\ninstructed. Despite the limitations of this experiment, the findings suggest that Transfusion models\ncan indeed adapt to and generalize across new modality combinations. We leave further exploration\nof this promising direction to future research.\n5\nRelated Work\nMost existing multi-modal models are built on the idea of attaching two or more modality-specific\narchitectures together, often pretraining each component separately in advance. State-of-the-art\nimage and video generation models, for instance, use large pretrained text encoders to represent their\ninput prompts in latent space, which can then be used to condition diffusion models [Saharia et al.,\n2022]. In fact, recent work fuses representations from multiple off-the-shelf encoders to enhance\nperformance [Podell et al., 2023, Esser et al., 2024b]. A similar pattern can be observed in the\nvision language model literature, where typically a pretrained language model is complemented\nby pretrained modality-specific encoders\/decoders via projection layers to\/from the pretrained text\nspace. Examples include Flamingo [Alayrac et al., 2022] and LLaVA [Liu et al., 2024] for visual\nunderstanding, GILL [Koh et al., 2024] for visual generation, and DreamLLM [Dong et al., 2024]\nfor both visual comprehension and generation. In contrast, Transfusion has one unified architecture\nlearned end-to-end to generate both text and images.\nPrior work on end-to-end multi-modal models includes examples such as Fuyu [Bavishi et al., 2023],\nwhich uses image patches as inputs for visual understanding, and Chameleon [Chameleon Team,\n2024], which converts each image to a sequence of discretized tokens and then trains over the\ncombined text-image token sequences. However, these approaches are either restricted to input-\n13\nlevel multi-modal tasks, or lag behind state-of-the-art models (i.e. diffusion models) in continuous\ndata generation. Transfusion provides a simple, end-to-end solution to multi-modal learning that\nunderstands and generates high-quality multi-modal data.\nAn interesting area of recent acrive research is the application diffusion models and their generaliza-\ntions to discrete text generation [Li et al., 2022, Gat et al., 2024]. However, this approach has yet to\nachieve the performance and scale of standard autoregressive language models. Future research in\nthis direction may unlock new ways to fuse discrete and continuous modalities in a single model.\n6\nConclusion\nThis work explores how to bridge the gap between the state of the art in discrete sequence modeling\n(next token prediction) and continuous media generation (diffusion). We propose a simple, yet\npreviously unexplored solution: train a single joint model on two objectives, tying each modality to\nits preferred objective. Our experiments show that Transfusion scales efficiently, incurring little to no\nparameter sharing cost, while enabling the generation of any modality.\nAcknowledgments and Disclosure of Funding\nWe would like to thank Horace He, Songlin Yang, Jiatao Gu, and Ishan Misra for helpful discussions\nthroughout this project.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nOmer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat,\nJunhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for\nvideo generation. arXiv preprint arXiv:2401.12945, 2024.\nRohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani,\nand Sa˘gnak Ta¸sırlar. Introducing our multimodal models, 2023. URL https:\/\/www.adept.ai\/\nblog\/fuyu-8b.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao,\nand Aditya Ramesh. Improving image generation with better captions, 2023. URL https:\n\/\/api.semanticscholar.org\/CorpusID:264403242.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\npages 7432–7439, 2020.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint\narXiv:2405.09818, 2024.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. CoRR, abs\/2102.08981, 2021.\nURL https:\/\/arxiv.org\/abs\/2102.08981.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes\/no questions. arXiv preprint\narXiv:1905.10044, 2019.\n14\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nXiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon\nVandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation\nmodels using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and\ncreation. arXiv preprint arXiv:2309.11499, 2023.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and\ncreation. In The Twelfth International Conference on Learning Representations, 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron,\nBinh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris\nMcConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton\nFerrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David\nEsiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip\nRadenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme\nNail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu,\nHugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov,\nJade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah,\nJelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu\nHuang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani,\nKate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz\nMalik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas\nLandzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri,\nMarcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis,\nMin Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov,\nNikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan\nZhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,\nPunit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy,\nRamon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit\nPatel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou,\nRui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia\nKim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,\nShruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla,\nStephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek\nSheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao,\nUjjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent\nGonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu,\nWhitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia,\nXuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen\nZhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe\nPapakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya\nGangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex\nVaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei\nLupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew\nRyan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley\nGabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin\nLeonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu,\n15\nBoyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt\nMontalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao\nZhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon\nCivin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide\nTestuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le,\nDustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix\nKreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,\nGovind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid\nShojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen\nSuk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat,\nJake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff\nTang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin,\nJingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh\nGinsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal,\nKatayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun\nHuang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender\nA, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca\nWehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus,\nMatan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi,\nMeghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov,\nMikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat,\nMohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White,\nNavyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning\nDong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli,\nParkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux,\nPiotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao,\nRachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li,\nRebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott,\nSai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru\nPan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng\nFeng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang,\nSinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,\nSteve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny\nVirk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara\nBest, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou,\nTzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish\nKumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov,\nWei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang\nWang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia,\nYe Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao,\nYundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and\nZhiwei Zhao. The llama 3 herd of models, 2024. URL https:\/\/arxiv.org\/abs\/2407.21783.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition,\npages 12873–12883, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Forty-first International Conference on Machine Learning,\n2024a.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English,\nKyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transform-\ners for high-resolution image synthesis, 2024b. URL https:\/\/arxiv.org\/abs\/2403.03206.\nItai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and\nYaron Lipman. Discrete flow matching. arXiv preprint arXiv:2407.15595, 2024.\n16\nDhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework\nfor evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36,\n2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural\ninformation processing systems, 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language\nmodels. Advances in Neural Information Processing Systems, 36, 2024.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm\nimproves controllable text generation. ArXiv, abs\/2205.14217, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision, pages 740–755. Springer, 2014.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\nfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36, 2024.\nShilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang,\nHang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv\npreprint arXiv:2311.05437, 2023.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nIn International conference on machine learning, pages 8162–8171. PMLR, 2021.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian,\nJeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny\nBogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,\nMiles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea\nCarlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty\nEleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte,\nIsabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel\nGoh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua\nGross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike\nHeaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon\nHoughton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne\nJang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo\nJun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik\nKirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich,\nAris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy\nLee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie\nLin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,\n17\nSam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,\nAshvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,\nGiambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,\nAdam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,\nMichael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,\nElizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis\nReal, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted\nSanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel\nSelsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie\nTang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun\nVijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang,\nJonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian\nWeng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming\nYuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2303.08774.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. CoRR, abs\/1910.10683, 2019. URL http:\/\/arxiv.org\/abs\/1910.10683.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine\nlearning, pages 8821–8831. Pmlr, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents, 2022. URL https:\/\/arxiv.org\/abs\/2204.\n06125.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pages 10684–10695, 2022a.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pages 10684–10695, 2022b.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in neural information\nprocessing systems, 35:36479–36494, 2022.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,\n2021.\n18\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\nreasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\nShelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh,\nand Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In\nProceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages\n8871–8879, 2024.\nStability AI.\nIf by deepfloyd lab at stabilityai, 2024.\nURL https:\/\/stability.ai\/news\/\ndeepfloyd-if-text-to-image-model.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566–4575, 2015.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, et al. Scaling autoregressive models for content-rich text-to-image generation.\narXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL-2019). Association for Computational Linguistics, 2019.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 586–595, 2018.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia\nEfrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information\nProcessing Systems, 36, 2024.\n19\nA\nAutoencoder Details\nThe training objective for our VAE closely follows that of Esser et al. [2021]:\nLVAE = L1 + LLPIPS + 0.5LGAN + 0.2LID + 0.000001LKL\nwhere L1 is L1 loss in pixel space, LLPIPS is perceptual loss based on LPIPS similarity Zhang\net al. [2018], LGAN is a patch-based discriminator loss, LID is a perceptual loss based on internal\nfeatures of the Moco v2 model Chen et al. [2020], and LKL is the standard KL-regularization term to\nencourage encoder outputs towards a normal distribution. We delay the beginning of GAN training\n(i.e. including the adversarial loss in the loss function) to 50,000 steps, in order to let the VAE achieve\nsufficiently good reconstruction performance. We use a latent dimension of 8.\nThe training objective for the VQ-GAN matches that of the VAE, with one notable exception:\nwe replace the LKL loss with the standard codebook commitment loss Lcodebook [Van Den Oord\net al., 2017], which encourages encoder outputs and codebook vectors to be close together. We use\nβ = 0.25, and use loss weighting 1.0. The final loss function for the VQ-VAE is therefore:\nLVQ-VAE = L1 + LLPIPS + 0.5LGAN + 0.2LID + Lcodebook\nThe vector quantization layer is applied after projecting the encoder outputs to 8-dimensional space.\nOutside of the loss function change and the quantization layer, the training setup for the VAE (for\nTransfusion) and VQ-VAE (for Chameleon) are the same (e.g. same amount of training compute,\nsame training data, and same encoder\/decoder architecture).\nB\nExamples: Image Generation\nFigure 7 and Figure 8 show examples of images generated from a 7B Transfusion model trained on\n2T multi-modal tokens (§4.4).\nC\nExamples: Image Editing\nFigure 9 show random examples of image editing by a fine-tuned 7B Transfusion model.\n20\nDowntown Seattle at sun-\nrise. detailed ink wash.\nA car made out of vegeta-\nbles.\nA sign that says “Diffu-\nsion\".\nA black basketball shoe\nwith a lightning bolt on it.\nan espresso machine that\nmakes coffee from human\nsouls, high-contrast paint-\ning.\nIntricate origami of a fox\nand a unicorn in a snowy\nforest.\na yellow wall with two\nframed sketches\nA crab made of cheese on\na plate.\nA single beam of light en-\nter the room from the ceil-\ning. The beam of light is\nilluminating an easel. On\nthe easel there is a Rem-\nbrandt painting of a rac-\ncoon.\nWhite Cycladic houses\nwith blue accents and vi-\nbrant magenta bougainvil-\nlea in a serene Greek is-\nland setting.\nThe\nsaying\n“BE\nEX-\nCELLENT TO EACH\nOTHER\" written in a\nstained glass window.\ndark high contrast render\nof a psychedelic tree of\nlife illuminating dust in a\nmystical cave.\nA photo of a person with\nthe head of a cow, wear-\ning a tuxedo and black\nbowtie. Beach wallpaper\nin the background.\nPhoto of a lychee-inspired\nspherical chair, with a\nbumpy white exterior and\nplush interior, set against\na tropical wallpaper.\nAn old rusted robot wear-\ning pants and a jacket rid-\ning skis in a supermarket.\nFilm still of a long-legged\ncute big-eye anthropomor-\nphic cheeseburger wear-\ning sneakers relaxing on\nthe couch in a sparsely\ndecorated living room.\nFigure 7: Generated images from a 7B Transfusion trained on 2T multi-modal tokens.\n21\nA woman on a bed under-\nneath a blanket.\nA small blue book sitting\non a large red book.\nA horse reading a book.\nA light bulb containing a\nsailboat floats through the\ngalaxy.\na monarch butterfly.\nA rowboat on a lake with\na bike on it.\nAn expressive oil painting\nof a chocolate chip cookie\nbeing dipped in a glass of\nmilk, depicted as an ex-\nplosion of flavors.\nAn angry duck doing\nheavy weightlifting at the\ngym.\nAn emoji of a baby panda\nwearing a red hat, green\ngloves,\nred shirt,\nand\ngreen pants.\nA tranquil, anime-style\nkoi pond in a serene\nJapanese garden, featur-\ning blossoming cherry\ntrees.\na massive alien space ship\nthat is shaped like a pret-\nzel.\ngraffiti of a funny dog on\na street wall.\nA spacious, serene room\ninfluenced\nby\nmodern\nJapanese aesthetics with\na view of a cityscape out-\nside of the window.\nA raccoon wearing cow-\nboy hat and black leather\njacket is behind the back-\nyard\nwindow.\nRain\ndroplets on the window.\nA relaxed garlic with a\nblindfold reading a news-\npaper while floating in a\npool of tomato soup.\nphoto of a bear wearing a\nsuit and tophat in a river\nin the middle of a forest\nholding a sign that says “I\ncant bear it\".\nFigure 8: Generated images from a 7B Transfusion trained on 2T multi-modal tokens.\n22\nChange the closest keyboard to be all black.\nChange the graffiti on the truck into calligraphy\nwriting.\nCan we have mountains on the background?\nReplace the airplane with a blackhawk helicopter.\nAdd a blue rug to the floor.\nDelete the overhead lights on top of the sink.\nChange the roll of thread into a roll of wire.\nChange the baseball bat to all brown.\nFigure 9: Edited images from a fine-tuned 7B Transfusion model.\n23\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model.pdf"}
{"title":"ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation","authors":"Ethan Chern, Jiadi Su, Yan Ma, Pengfei Liu","summary":"Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.","url":"http:\/\/arxiv.org\/abs\/2407.06135v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.06135v1","published":1720458482000,"comment":null,"pdf_text":"Generative AI Research\nANOLE: An Open, Autoregressive, Native Large Multimodal\nModels for Interleaved Image-Text Generation\nEthan Chern*\nJiadi Su*\nYan Ma*\nPengfei Liu†\nGenerative AI Research Lab (GAIR)\nAbstract\nPrevious open-source large multimodal models (LMMs) have faced several limitations: (1) they often lack\nnative integration, requiring adapters to align visual representations with pre-trained large language models\n(LLMs); (2) many are restricted to single-modal generation; (3) while some support multimodal generation,\nthey rely on separate diffusion models for visual modeling and generation. To mitigate these limitations,\nwe present ANOLE, an open, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build ANOLE from Meta AI’s Chameleon, adopting an innovative fine-tuning strategy that\nis both data-efficient and parameter-efficient. ANOLE demonstrates high-quality, coherent multimodal\ngeneration capabilities. We have open-sourced our model, training framework, and instruction tuning data.\nHomepage: https:\/\/gair-nlp.github.io\/anole\nCode: https:\/\/github.com\/GAIR-NLP\/anole\nHugging Face Model: https:\/\/huggingface.co\/GAIR\/Anole-7b-v0.1\nFigure 1: An example of ANOLE generating a high-quality and coherent interleaved image-text sequence on how to\ncook eggs.\n* Co-first authors.\n† Corresponding author\n1\narXiv:2407.06135v1  [cs.CL]  8 Jul 2024\n1\nIntroduction\nSince the introduction of Meta AI’s LLaMA (Touvron et al., 2023) in February 2023, autoregressive open-source\nlarge language models (LLMs) such as LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), Vicuna (Chiang\net al., 2023), Mistral (Jiang et al., 2023), Phi (Gunasekar et al., 2023), Gemma (Team et al., 2024), Olmo (Groeneveld\net al., 2024) and LLM360 (Liu et al., 2023c) have democratized and advanced the development of LLMs. Efforts in\nopen-sourcing large multimodal models (LMMs) are also ongoing, though at a slower pace compared to LLMs.\nNotable open-sourced LMMs include LLaVA (Liu et al., 2023b,a), CogVLM (Wang et al., 2023), DreamLLM (Dong\net al., 2023), Emu2 (Sun et al., 2024) and Cambrian (Tong et al., 2024). However, current open-source LMMs have\nseveral significant limitations: (1) Many focus solely on multimodal understanding without multimodal generation\n(Liu et al., 2023b,a; Wang et al., 2023), (2) most are not natively multimodal (i.e., not trained on multimodal data\nfrom the pretraining stage) and rely on pretrained LLMs as their backbone (Liu et al., 2023b,a; Wang et al., 2023;\nDong et al., 2023; Sun et al., 2024), and (3) those with vision generation capabilities require an additional diffusion\nmodel for vision modeling and generation (Dong et al., 2023; Sun et al., 2024). This reliance on extra mechanisms\ncan introduce complexity and inefficiency in both training and inference time.\nGiven the limitations in current open-source LMMs, the AI community eagerly anticipates the emergence of\ntruly open, autoregressive, native LMMs with multimodal generation capabilities. The goal is to be able to develop\nLMMs in the same way we do with LLMs. To address this gap, we introduce ANOLE. As shown in Fig. 1, ANOLE\ncan generate a high-quality, coherent recipe for cooking eggs in just a few seconds.\nANOLE is built on top of Chameleon (Team, 2024) by Meta AI. Chameleon represents a significant advancement\nin multimodal AI, showcasing the potential of early-fusion, token-based autoregressive approaches for multimodal\nmodeling. According to their paper, Chameleon has demonstrated impressive capabilities in understanding and\ngenerating interleaved sequences of images and text, pushing the boundaries of what is possible in multimodal AI.\nThe latest open-source release of Chameleon has shown strong performance in text understanding, text generation,\nand multimodal comprehension. However, the current open-source version of Chameleon does not support\nimage generation or multimodal generation. We build ANOLE to facilitate Chameleon’s capabilities in vision\nand multimodal generation without compromising its strengths in text generation and multimodal comprehension.\nANOLE addresses the limitations of the current open-source release of Chameleon, making the full potential of\nmultimodal generation accessible to the broader research community. The key contributions of our work are\nfourfold:\n1. Full Open-Source Implementation: ANOLE has facilitated the vision and multimodal generation capabilities\nfrom Chameleon through an innovative fine-tuning approach, unlocking the model’s most crucial technological\naspects. This comprehensive open-source release allows researchers and developers to fully utilize and build\nupon it.\n2. Data and Parameter Efficient Fine-Tuning: Our method fine-tunes fewer than 40M parameters, requiring only\nabout 6,000 samples to effectively facilitate vision and multimodal generation capabilities. This demonstrates\na highly efficient approach to facilitate complex functionality in LMMs.\n3. Training, Multimodal Inferece, and Qualitative Evaluation: We provide a training and multimodal inference\nframework for unified tokenizer-based multimodal models. This infrastructure significantly lowers the barrier\nto entry for developing and experimenting with autoregressive LMMs, making it accessible to a wider range\nof researchers. Additionally, we conduct qualitative analysis to demonstrate the potential of autoregressive\nLMMs.\n4. Rich Resources for Accessibility: To further support the adoption and advancement of autoregressive LMMs,\nwe offer an extensive collection of data resources and detailed tutorials. These materials are designed to\nfacilitate easier onboarding and experimentation for researchers at various levels of expertise.\nBy addressing these critical aspects, ANOLE represents a significant step forward in democratizing access\nto advanced multimodal AI technologies. Our work not only builds upon the foundations laid by the original\nChameleon model but also paves the way for more inclusive and collaborative research in the field of multimodal\nAI.\nMoreover, ANOLE sparks a series of important and intriguing research questions for the community to explore.\nFor example:\n• Investigating the performance limits of vision generation using unified tokenizer-based multimodal models, in\ncomparison to established methods like diffusion models.\n• Developing efficient techniques for interleaved image-text decoding, which are essential for real-world\napplications such as textbook and comic generation.\n2\n• Exploring optimal fine-tuning methodologies for these complex pretrained LMMs.\n• Addressing critical issues, including ensuring the safety and ethical use of generated images.\n2\nRelated Works\nModels\nw\/o Diffusion?\nNative?\nToken-based?\nInterleaved (open-source)?\nMM-Interleaved (Tian et al., 2024)\n%\n%\n%\n!\nEmu2 (Sun et al., 2024)\n%\n%\n%\n%\nDreamLLM (Dong et al., 2023)\n%\n%\n%\n%\nAnyGPT (Zhan et al., 2024)\n%\n%\n!\n%\nLWM (Liu et al., 2024)\n!\n%\n!\n%\nChameleon (Team, 2024)\n!\n!\n!\n%\nANOLE (Ours)\n!\n!\n!\n!\nTable 1: Comparison with related works that allows text and (or) vision generation. w\/o Diffusion indicates\nwhether the model relies on a diffusion model to generate visual contexts. Native specifies if the model is a\nnative LMM. Token-based denotes whether the model utilizes token-based modeling approach. Interleaved\n(open-source) represents whether the open-source released version of the model supports multimodal generation.\nLMMs have experienced significant advancements, often leveraging pretrained LLMs as their foundation back-\nbone. One common approach involves combining a CLIP-pretrained vision encoder with diffusion models as the\ndecoder, and a pretrained LLM (e.g., Vicuna) as the backbone (Dong et al., 2023; Tian et al., 2024; Sun et al.,\n2024). This approach achieves impressive results in both image understanding and generation by leveraging the\nrobust representations provided by CLIP and diffusion models. However, incorporating CLIP and diffusion models\nincreases the overall complexity of the model architecture, resulting in additional training overhead and reduced\ninference efficiency.\nIn contrast, purely token-based methods exclude diffusion models and CLIP, instead using token-based represen-\ntations for multimodal understanding and generation. This approach traces back to BEiT (Bao et al., 2021), with\nOpenAI’s DALL-E (Ramesh et al., 2021) exemplifying text-to-image generation based on similar principles. These\nmethods rely heavily on vector quantization (VQ) models (Van Den Oord et al., 2017; Esser et al., 2021), which\ncombine ResNet-based encoders and decoders with a discrete codebook. During image encoding, the VQ model\ntransforms the image from pixel space to latent space representations, then maps these representations to codebook\nIDs using a nearest-neighbor search. These IDs serve as input tokens for a Transformer model, which models\nconditional probabilities and predicts sequences. The VQ decoder then reconstructs images from the generated\nsequences. This autoregressive, discrete image encoding and decoding approach has been validated in multiple\nstudies for producing high-quality images (Zhu et al., 2024; Yu et al., 2024), effectively modeling inter-image\ndependencies (Bai et al., 2023), and enhancing image consistency (Pan et al., 2024). LWM (Liu et al., 2024) and\nChameleon (Team, 2024) extend this concept to image-text multimodal tasks, using streamlined architectures to\nhandle tasks involving both images and text. Compared to other methods, unified token-based modeling significantly\nreduces model complexity, facilitating seamless inference and the generation of interleaved image-text sequences\nwithout additional components.\nANOLE, building on the foundation of Chameleon, facilitates Chameleon’s image and multimodal generation\ncapabilities with efficient fine-tuning. ANOLE retains the inherent advantages of Chameleon’s architecture while\nproducing high-quality images and maintaining coherent image-text sequences. Tab. 1 highlights ANOLE’s\ncharacteristics as an autoregressive, diffusion-free, native token-based model, emphasizing its capability to support\nmultimodal generation, as well as its simplicity and efficiency compared to more complex frameworks.\n3\nANOLE\n3.1\nOverview\nANOLE adopts the same approach and architecture as Chameleon, utilizing an early-fusion, token-based, autoregres-\nsive approach to model multimodal sequences (text and images) without the use of diffusion models, relying solely\non transformers. Token-based approaches (Team, 2024; Lu et al., 2022; Yu et al., 2023; Liu et al., 2024) achieve\nmodality fusion at the input-token level. Firstly, modality-specific tokenizers tokenize samples from each modality.\nThen, these token sequences are concatenated to form a single multimodal token sequence, which is subsequently\nfed into an autoregressive transformer for modeling.\n3\n3.2\nFacilitating the image generation and multimodal generation capabilities from Chameleon\nTransformer\nLayers\nToken Embeddings\nImage Tokens\nboi\neoi\nImage Tokenizer\nOutput Head\n0-3\n4-8197\n8198-65536\nToken Id\n4-8195: Image Vocabulary\n8196: eoi (end of image)\n8197: boi (begin of image)\nChameleon\nAnole\nFigure 2: Facilitating Chameleon’s image generation capabilities via innovative fine-tuning.\n3.2\nFacilitating the image generation and multimodal generation capabilities from Chameleon\nFig. 2 shows the innovative fine-tuning process of how ANOLE is fine-tuned from Chameleon. Based on available\ninformation and our testings, the latest release of Chameleon have demonstrated strong performance in text\nunderstanding, text generation, and multimodal understanding. ANOLE, build on top of Chameleon, aiming\nto facilitate the image generation and multimodal generation capabilities from Chameleon. Chameleon’s pre-\ntraining data natively includes both text and image modalities, theoretically equipping it with image generation\ncapabilities. Our goal is to facilitate this ability without compromising its text understanding, generation, and\nmultimodal comprehension. To achieve this, we froze most of Chameleon’s parameters and fine-tuned only the\nlogits corresponding to image token ids in transformer’s output head layer.\nFollowing the principle of “less is more” (Zhou et al., 2024), the current version of ANOLE, ANOLE-7b-v0.1, was\ndeveloped using a small amount of image data (5,859 images from LAION-5B art (Schuhmann et al., 2022)) and was\nfine-tuned on just a few parameters (less than 40M) in a short time (around 30 minutes on 8 A100 GPUs). Despite\nthese limitations, ANOLE-7b-v0.1 expresses impressive image (Tab. 2 and Tab. 3) and multimodal generation\ncapabilities (Fig. 1, Fig. 3 and Fig. 4).\n4\nEvaluation\nWe conduct qualitative analysis on ANOLE’s capabilities on image generation and interleaved image-text generation.\n4.1\nImage Generation\nTab. 2 demonstrates the text-to-image capabilities of ANOLE. We highlight the following points: (1) The images\ngenerated by ANOLE are of high quality and closely adhere to the given instructions. For instance, ANOLE accurately\ncaptures the essence of ”A steaming cup of coffee next to a fresh croissant on a cozy cafe table,” showcasing the\nsteam, coffee, and croissant elements precisely. (2) ANOLE demonstrates remarkable versatility in generating\ndiverse types of images. It can create realistic depictions, as seen in the coffee and ice cream images, as well as\nimaginative scenes, like the dinosaur strolling in Times Square. This variety highlights ANOLE’s ability to blend\nrealism with creativity seamlessly.\n4\n4.1\nImage Generation\nA serene lakeside view at sunrise\nwith mist rising from the water,\nsurrounded by dense pine forests\nand mountains in the background.\nA bustling downtown street in\nTokyo at night, with neon signs,\ncrowded sidewalks, and tall\nskyscrapers.\nA colorful ice cream sundae\ntopped with sprinkles, whipped\ncream, and a cherry.\nA steaming cup of coffee next to a\nfresh croissant on a cozy caf´e\ntable.\nAn impressionist painting of a caf´e\nscene, with loose brushstrokes and\nlively colors.\nUnder the bright sun, dinosaurs\nstrolling in Times Square, with\npeople taking photos and traffic\npassing by.\nTable 2: Text-to-image examples generated by ANOLE.\n5\n4.1\nImage Generation\nA piece of paper with word\n“Anole” written on it.\nA piece of paper with word like\n“Anole” written on it, and a\ndrawing of an Anole.\nAn image depicting three cubes\nstacked on a table. Each cube has a\nunique color and a letter on it.\nA vibrant coral reef teeming with\ncolorful fish, sea turtles gliding\nthrough the water, and sunlight\nfiltering down from the surface.\nA tropical beach with crystal clear\nwater, white sand, and palm trees\nswaying in the breeze.\nA quiet European village with\ncobblestone streets and colorful\nhouses, under a clear blue sky.\nA gothic cathedral with intricate\nstone carvings and stained glass\nwindows.\nA delicious slice of pizza with\nmelting cheese and pepperoni, on a\nrustic wooden table.\nA colorful graffiti mural on an\nurban wall, with vibrant characters\nand abstract elements.\nTable 3: More text-to-image examples generated by ANOLE.\n6\n4.2\nInterleaved Image-Text Generation\n4.2\nInterleaved Image-Text Generation\nFig. 1, Fig. 3 and Fig. 4 demonstrate the multimodal generation capabilities of ANOLE. We emphasize that (1) The\ngenerated text is well-organized and provides comprehensive details, which is essential for accurate interleaved\nimage-text generation. This is evident in Fig. 4, where the detailed introduction of Gyumri is accompanied by\nrelevant images, effectively capturing the city’s architectural design and cultural highlights. (2) ANOLE demonstrates\nseamless integration between images and text, ensuring that the visual and textual elements complement each other\nperfectly. Fig. 3 highlights this capability by introducing traditional Chinese dishes with corresponding images,\nmaking the information both engaging and informative.\nFigure 3: An example of ANOLE generating an interleaved image-text sequence on how to cook traditional Chinese\ncuisines.\n5\nConclusion & Future Directions\nWe introduce ANOLE, an open, autoregressive, native LMM for interleaved image-text generation that demonstrates\nadvanced multimodal generation abilities. ANOLE facilitates image and multimodal generation capabilities from\nChameleon by fine-tuning on just 6,000 samples with 40M parameters. We are committed to continually upgrading\nANOLE to enhance its capabilities. Our future directions include (1) enhancing ANOLE’s precise instruction-\nfollowing capability, (2) extending its context length, (3) improving its multimodal understanding capabilities, and\n(4) applying ANOLE to downstream tasks requiring multimodal generation abilities.\n6\nLimitations & Disclaimer\nANOLE is intended for research use only. Our model weights follow the same license as Chameleon. The fine-tuning\nimages we used are from LAION-5B art, and thus follow the same license as LAION. ANOLE is still under\ndevelopment and has many limitations that need to be addressed. Importantly, we have not aligned the image\ngeneration capabilities of the ANOLE to ensure safety and harmlessness. Therefore, we encourage users to interact\nwith ANOLE with caution and report any concerning behaviors to help improve the model’s safety and ethical\nconsiderations.\n7\nAcknowledgements\nWe sincerely thank the Meta Chameleon Team for open-sourcing Chameleon. Our base model and the majority of\nour inference code are based on it. We also greatly appreciate all the contributors who have participated in pull\nrequest #31534 submitted to the transformers repository. This PR has been crucial for the development of our\ntraining code.\n7\nPrompt: Please introduce the city of Gyumri with pictures.\nResponse: Gyumri is the second-largest city in Armenia and has a rich \nhistory, beautiful architecture, and a vibrant cultural scene. It's known \nfor its colorful houses, historic landmarks, and stunning natural \nsurroundings, which make it an ideal destination for both cultural and \noutdoor enthusiasts.\nThe city center is home to several museums, theaters, and historical \nsites, including the Gyumri Opera and Ballet Theater, the Gyumri \nMuseum of Local Lore, and the Mother Cathedral of the 3rd century, \nshown in the image below: <img>\nLocated in the Caucasus region, Gyumri is surrounded by \npicturesque hills and mountains, including Mount Aragats, the \nhighest peak in Armenia, pictured here: <img>\nIn addition to its cultural attractions, Gyumri is also known for its \ndelicious food and traditional handicrafts. Visitors can sample \nlocal specialties like sujuk (cured sausage), khachapuri (bread \nwith cheese and eggs), and rakhi (a type of flatbread), and browse \nlocal markets for unique souvenirs like carpets, rugs, and textiles.\nPrompt: What color is a polar bear’s fur? Show me a photograph of the polar bear in the wild.\nResponse:  <img>\nA polar bear’s fur is pure white and is very thick which helps to insulate it \nagainst the cold. The image above shows a polar bear in its natural habitat.\nPolar bears are carnivores, which means they feed on meat. The diet of a \npolar bear consists mainly of seals. They can dive into water and swim after \nseals or they wait patiently for them to come to shore. They also eat birds, \nfish, and other small mammals.\nPolar bears are magnificent creatures but unfortunately they are considered \nvulnerable and even endangered by some due to human activities such as \nclimate change.\nFigure 4: Examples of ANOLE generating interleaved text-and-image sequences on geographical and biological\ntopics.\n8\nReferences\nReferences\n[1] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and\nAlexei A Efros. 2023. Sequential modeling enables scalable learning for large vision models. arXiv preprint\narXiv:2312.00785.\n[2] Hangbo Bao, Li Dong, and Furu Wei. 2021. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254.\n[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality.\n[4] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\nZhou, Haoran Wei, et al. 2023. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint\narXiv:2309.11499.\n[5] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition, pages\n12873–12883.\n[6] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha,\nHamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models.\narXiv preprint arXiv:2402.00838.\n[7] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv\npreprint arXiv:2306.11644.\n[8] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv\npreprint arXiv:2310.06825.\n[9] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024. World model on million-length video and\nlanguage with ringattention. arXiv preprint arXiv:2402.08268.\n[10] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction\ntuning.\n[11] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. In NeurIPS.\n[12] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi\nWang, Suqi Sun, Omkar Pangarkar, et al. 2023c. Llm360: Towards fully transparent open-source llms. arXiv\npreprint arXiv:2312.06550.\n[13] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. Unified-io:\nA unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on\nLearning Representations.\n[14] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. 2024. Synthesizing coherent story with\nauto-regressive latent diffusion models. In Proceedings of the IEEE\/CVF Winter Conference on Applications of\nComputer Vision, pages 2920–2930.\n[15] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. 2021. Zero-shot text-to-image generation. In International conference on machine learning, pages\n8821–8831. Pmlr.\n[16] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale\ndataset for training next generation image-text models. Advances in Neural Information Processing Systems,\n35:25278–25294.\n[17] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu,\nTiejun Huang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 14398–14409.\n[18] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.\n9\nReferences\n[19] Chameleon Team. 2024.\nChameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818.\n[20] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent\nSifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini\nresearch and technology. arXiv preprint arXiv:2403.08295.\n[21] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu,\nTong Lu, Jie Zhou, et al. 2024. Mm-interleaved: Interleaved image-text generative modeling via multi-modal\nfeature synchronizer. arXiv preprint arXiv:2401.10208.\n[22] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan\nYang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. 2024. Cambrian-1: A fully open, vision-centric exploration\nof multimodal llms. arXiv preprint arXiv:2406.16860.\n[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971.\n[24] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30.\n[25] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao,\nXixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. Cogvlm: Visual\nexpert for pretrained language models.\n[26] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, et al. 2023. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. arXiv preprint arXiv:2309.02591, 2(3).\n[27] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. 2024. An\nimage is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550.\n[28] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang,\nLinyang Li, et al. 2024. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint\narXiv:2402.12226.\n[29] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\nYu, Lili Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems,\n36.\n[30] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. 2024. Scaling the codebook size of vqgan to 100,000 with a\nutilization rate of 99%. arXiv preprint arXiv:2406.11837.\n10\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation.pdf"}
{"title":"Hybrid Alignment Training for Large Language Models","authors":"Chenglong Wang, Hang Zhou, Kaiyan Chang, Bei Li, Yongyu Mu, Tong Xiao, Tongran Liu, Jingbo Zhu","summary":"Alignment training is crucial for enabling large language models (LLMs) to\ncater to human intentions and preferences. It is typically performed based on\ntwo stages with different objectives: instruction-following alignment and\nhuman-preference alignment. However, aligning LLMs with these objectives in\nsequence suffers from an inherent problem: the objectives may conflict, and the\nLLMs cannot guarantee to simultaneously align with the instructions and human\npreferences well. To response to these, in this work, we propose a Hybrid\nAlignment Training (Hbat) approach, based on alternating alignment and modified\nelastic weight consolidation methods. The basic idea is to alternate between\ndifferent objectives during alignment training, so that better collaboration\ncan be achieved between the two alignment tasks.We experiment with Hbat on\nsummarization and dialogue tasks. Experimental results show that the proposed\n\\textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields\nconsistent performance gains over the traditional two-stage alignment training\nwhen using both proximal policy optimization and direct preference\noptimization.","url":"http:\/\/arxiv.org\/abs\/2406.15178v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2406.15178v1","published":1718979837000,"comment":"accepted by ACL (Findings) 2024","pdf_text":"Hybrid Alignment Training for Large Language Models\nChenglong Wang1, Hang Zhou1, Kaiyan Chang1, Bei Li1, Yongyu Mu1,\nTong Xiao1,3∗, Tongran Liu2, and Jingbo Zhu1,3\n1 School of Computer Science and Engineering, Northeastern University, Shenyang, China\n2 CAS Key Laboratory of Behavioral Science, Institute of Psychology, CAS, Beijing, China\n3 NiuTrans Research, Shenyang, China\n{clwang1119, ctrl.hang}@gmail.com,\n{xiaotong, zhujingbo}@mail.neu.edu.cn\nAbstract\nAlignment training is crucial for enabling large\nlanguage models (LLMs) to cater to human\nintentions and preferences. It is typically per-\nformed based on two stages with different ob-\njectives: instruction-following alignment and\nhuman-preference alignment. However, align-\ning LLMs with these objectives in sequence\nsuffers from an inherent problem: the objec-\ntives may conflict, and the LLMs cannot guar-\nantee to simultaneously align with the instruc-\ntions and human preferences well. To response\nto these, in this work, we propose a Hybrid\nAlignment Training (HBAT) approach, based\non alternating alignment and modified elastic\nweight consolidation methods. The basic idea\nis to alternate between different objectives dur-\ning alignment training, so that better collabora-\ntion can be achieved between the two alignment\ntasks. We experiment with HBAT on summa-\nrization and dialogue tasks. Experimental re-\nsults show that the proposed HBAT can signifi-\ncantly outperform all baselines. Notably, HBAT\nyields consistent performance gains over the\ntraditional two-stage alignment training when\nusing both proximal policy optimization and\ndirect preference optimization.\n1\nIntroduction\nAlignment training is a key technique to ensure that\nthe behaviors of large language models (LLMs) are\nconsistent with human intentions and preferences\n(Ouyang et al., 2022; Wang et al., 2023e). It typi-\ncally involves two stages: 1) using human-labeled\ndata to train pre-trained LLMs via a supervised\ntraining method, which enables LLMs to under-\nstand human intentions and follow the instructions\n(call it instruction-following alignment), and 2) em-\nploying approaches like proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017) and direct pref-\nerence optimization (DPO) (Rafailov et al., 2023)\nto learn preferences from human feedbacks (call it\n*Corresponding author.\nhuman-preference alignment). This paradigm has\nachieved promising results on several downstream\ntasks, such as dialogue (OpenAI, 2022; Dubois\net al., 2023; Wang et al., 2023b), summarization\n(Stiennon et al., 2020; Lee et al., 2023), and ma-\nchine translation (Ramos et al., 2023).\nHowever, this two-stage alignment training has\nits inherited limitation: the optimization objectives\nare different for each stage, which can make an op-\ntimization conflict (French, 1999; Liu et al., 2021).\nSuch limitation could result in an inferiorly aligned\nLLM in real-world scenarios. This phenomenon\nis also described in Ouyang et al. (2022)’s work,\nwhich is referred to as alignment tax.\nTo mitigate this limitation, in this work, we pro-\npose a Hybrid Alignment Training (HBAT) ap-\nproach, which offers a refinement of the collab-\noration among instruction-following alignment and\nhuman-preference alignment by using the follow-\ning two methods. For one, inspired by interactive\nmethods in multi-objective optimization (Mietti-\nnen et al., 2008; Xin et al., 2018), we propose an\nalternating alignment method, where the human-\npreference alignment acts as a decision maker\nand continuously interacts with the instruction-\nfollowing alignment to achieve a preferred align-\nment.\nSpecifically, we divide the instruction-\nfollowing and human-preference training set into\nequal portions of mutually exclusive subsets, re-\nspectively. Then, we rearrange these subsets in\nalternating orders during alignment training. Fur-\nthermore, we introduce a modified Elastic Weight\nConsolidation (EWC) (Kirkpatrick et al., 2017) to\nalternating alignment. EWC is a method to dynam-\nically impose an appropriate constraint on each\nparameter when training a model with a new opti-\nmization objective, thereby easing an optimization\nconflict with the previous objective.\nWe experiment with the proposed HBAT on sum-\nmarization and dialogue tasks based on LLaMA2-\n7B and LLaMA2-13B models (Touvron et al.,\narXiv:2406.15178v1  [cs.CL]  21 Jun 2024\n2023). Experimental results show that HBAT can\nsignificantly surpass all baselines. Notably, based\non the LLaMA2-13B model, HBAT can yield a\n+2.26 ROUGE-L points improvement for the sum-\nmarization task, compared to the traditional RLHF.\nAdditionally, our HBAT significantly outperforms\nthe SFT over 21.01 GPT-4 win rate points on the di-\nalogue task based on the LLaMA2-13B model. Fur-\nthermore, HBAT is orthogonal to other optimized\nalignment approaches. For instance, when armed\nwith ESRL (Wang et al., 2023b), our HBAT gains\nan additional improvement of 2.59 GPT-4 win rate\npoints on the summarization task.\n2\nRelated Work\nAlignment Training for LLMs.\nRecently, many\nefforts have been made to improve the LLM align-\nment for different tasks (Stiennon et al., 2020;\nNakano et al., 2021; Wang et al., 2023c; Hu et al.,\n2023). These works mainly focused on optimiz-\ning each stage of alignment training, including\ninstruction-following alignment (also referred to\nas SFT) and human-preference alignment (also\nreferred to as RLHF). For example, Zhou et al.\n(2023) designed data selection schemes to provide\nhigh-quality instruction-following data. Moreover,\nWang et al. (2022) proposed an efficient approach\nfor producing instruction-following data. Likewise,\nsome works aimed to efficiently produce human-\npreference data (Lee et al., 2023; Dubois et al.,\n2023; Wang et al., 2023a). Apart from the training\ndata improvements, another line of improving the\nalignment training is to explore better reward mod-\nels and optimization objectives, such as the use of\nfine-grained reward models (Coste et al., 2023; Wu\net al., 2023), the integration of a prior knowledge in\ntraining reward models (Zhou et al., 2024), and the\ndesign of direct preference optimization objective\n(Rafailov et al., 2023). Although previous works\nimprove the performance of instruction-following\nalignment and human-preference alignment, they\nrarely consider the optimization conflict limitation\nbetween them. Researchers have been aware of\nthis (Ouyang et al., 2022), but it is still rare to see\nstudies on this issue.\nMulti-objective Optimization.\nMulti-objective\noptimization problem involves optimizing multi-\nple optimization objectives simultaneously (Hwang\nand Masud, 2012). However, there does not typi-\ncally exist a feasible solution that minimizes all loss\nfunctions. Therefore, researchers always explored\na Pareto optimal solution that cannot be improved\nin any of the objectives without impairing at least\none of the other objectives. Recent works on this\nexploration could be classified into three groups.\nThe first group focused on Pareto dominance-based\nmethod. This method maintains the individual ele-\nments of the solution vectors as independent dur-\ning optimization (Cheng et al., 2015; Wu and Pan,\n2019). The second group tended to design an qual-\nity indicator, such as hypervolume (Bader and Zit-\nzler, 2011) and R2 (Wagner et al., 2013), to act as a\nproxy objective instead of optimization objectives.\nThe third group that has attracted attention com-\nmonly aimed to solve multi-objective optimization\nproblems through an interactive method. A typical\ninteractive method requires a decision maker to of-\nfer preference information, which allows to search\nfor the most preferred Pareto optimal solution after\neach optimization (Xin et al., 2018; Misitano et al.,\n2021; Pereira et al., 2022).\nAlthough the alignment training is not a stan-\ndard multi-objective optimization problem, its goal\nremains consistent, i.e., seeking an aligned LLM\nthat simultaneously aligns instructions and human\npreferences well.\n3\nBackground\nDespite the extensive knowledge endowed from\npre-training, LLMs are difficult to produce con-\ntent that humans want. This is because that pre-\ntrained LLMs lack understanding of input instruc-\ntions and human preferences. To address this, we\noften perform alignment training on them, first\nfor instruction-following alignment and then for\nhuman-preference alignment.\n3.1\nInstruction-Following Alignment\nInstruction-following alignment enables the pre-\ntrained language model to acquire the capability to\nunderstand and follow instructions in the prompt\nby mimicking the human-labeled response. Specif-\nically, given a human prompt x and the labeled\nresponse of Ny tokens y = {y1, . . . , yNy}, where\neach token yt is drawn from a vocabulary. In the\ntraining process, the LLM learns the probability:\npθ(y|x) =\nNy\nY\nt=1\npθ(yt|y<t, x)\n(1)\nwhere y<t is the prefix {y1, y2, . . . , yt−1}, and θ\nis a trained parameter set. The standard training\nLLM\nPre-trained\nLLM\nAfter IFA\nLLM\nAfter HPA\nLLM\nAfter IFA\nLLM\nAfter HPA\n· · ·\nD1\nIFA\nD1\nHPA\nD2\nIFA\nD2\nHPA\nCIFA\ni,0\nCHPA\ni,0\nCIFA\ni,1\nCHPA\ni,1\n+\n+\nFigure 1: Architecture of HBAT. We introduce the alternating alignment and the modified EWC methods to design\nHBAT, which enables it to address optimization conflict problem in the process of LLM alignment training. Here,\nblack solid arrows (\n) denote learning from the subsets Dn\nIFA and Dn\nHPA via Eq. 8 and Eq. 5, respectively. Black\ndashed arrows (\n) denote computing the amount of parameter changes before and after training and blue dashed\narrows (\n) denote accumulating the parameter changes resulting from learning all previous subsets (see Section\n4.1). IFA: instruction-following alignment; HPA: human-preference alignment.\nobjective is to maximize the likelihood over all\nthe tokens of the labeled response, i.e., maximum\nlikelihood estimation (MLE) (Myung, 2003). The\ncorresponding loss function can be defined by:\nLMLE = −\nX\nt\nlog pθ(yt|y<t, x)\n(2)\n3.2\nHuman-Preference Alignment\nThis process of human-preference alignment con-\nsists of two main steps: 1) learning a preference\nmodel from comparison response pairs to act as\na reward model, and 2) maximizing the reward,\nwritten as arg maxθ Epθ(ˆy|x)[r(ˆy)], where ˆy is a\ngenerated response and r(·) denotes the computa-\ntion of the reward for ˆy using a reward model. We\nusually employ an RL algorithm to achieve step 2.\nTaking PPO as an instance, the corresponding loss\nfor this training sample is given by:\nLPPO = −\nX\nˆy∈Ω(x)\nlog pθ(ˆy|x)r(ˆy)\n−α log( pθ(ˆy|x)\npθold(ˆy|x))\n(3)\nwhere Ω(x) is the output space which comprises\nall possible responses for prompt x, θold is the\nparameter set of the LLM trained via instruction-\nfollowing alignment, and α is a KL reward coeffi-\ncient which controls the strength of the KL penalty\nlog( pθ(ˆy|x)\npθold(ˆy|x)). Here, Ω(x) is approximated using\nthe Monte Carlo method (Williams, 1992).\nTo bypass the complex RL procedure, Rafailov\net al. (2023) proposed DPO method, which em-\nploys a reward model training objective to maxi-\nmize rewards. It gives a new loss function:\nLDPO = −log σ[β log( pθ(yw|x)\npθold(yw|x))\n−β log( pθ(yl|x)\npθold(yl|x))]\n(4)\nwhere (yw, yl) is two of the different responses and\nyw aligns better with human preferences than yl. β\nis a scaling factor and σ is a Sigmoid function.\n4\nMethod\nIn this work, we aim to solve an optimization con-\nflict limitation during alignment training. We pro-\npose the HBAT to achieve this. The overview of\nHBAT is depicted in Figure 1. As shown in the\nfigure, we propose the alternating alignment and\nmodified EWC in HBAT to achieve our goal. In the\nfollowing subsections, we will describe them.\n4.1\nAlternating Alignment\nWe first introduce the optimization conflict problem\nin the alignment training. Suppose that we have\ntraining datasets DIFA and DHPA for instruction-\nfollowing alignment and human-preference align-\nment, respectively. We expect that the LLM will\nsimultaneously align instructions and human pref-\nerences well by learning from both datasets. How-\never, during the traditional two-stage alignment\ntraining, while the LLM learns from new training\nsamples in DHPA, it may have conflicts with previ-\nous knowledge learned from DIFA.\nInspired by the success of interactive meth-\nods in multi-objective optimization, we propose\nan alternating alignment method.\nIn the alter-\nnating alignment, we redesign the relationship\nbetween the instruction-following alignment and\nhuman-preference alignment to offer a refinement\nof the collaboration among them.\nSpecifically,\nwe divide the datasets DIFA and DHPA into N\nmutually exclusive splits {D1\nIFA, D2\nIFA, · · · , DN\nIFA}\nand {D1\nHPA, D2\nHPA, · · · , DN\nHPA}, respectively. The\nLLM performs an alternating alignment by sequen-\ntially learning from {D1\nIFA, D1\nHPA, · · · , DN\nHPA}.\nIn each round of alternate training, the human-\npreference alignment acts as a “decision maker”\nto offer preference information. This preference\ninformation enables an LLM to align human pref-\nerences following instruction alignment.\n4.2\nElastic Weight Consolidation\nTo further solve the optimization conflict, we in-\ntroduce a modified EWC to alternating alignment.\nFirstly, we add EWC to the process of human-\npreference alignment to mitigate optimization con-\nflicts with instruction-following alignment. The\nloss of human-preference alignment with EWC is:\nLHPA = LPPO +\nX\ni\nλ\n2 F IFA\ni\n(θi −θIFA\ni\n)2\n(5)\nwhere i is the index corresponding to each parame-\nter within the LLM, θIFA is the parameter set of the\nLLM trained by instruction-following alignment, λ\nis a balance factor, and F is the diagonal of the em-\npirical Fisher matrix (Pascanu and Bengio, 2014).\nHere, F IFA\ni\ndenotes how important the i-th param-\neter θIFA\ni\nis to the instruction-following alignment.\nNote that we can replace LPPO with other loss\nfunctions, such as LDPO, which can align LLMs\nwith human preferences.\nModified EWC for LLMs.\nHowever, the orig-\ninal EWC introduces a large computational over-\nhead on the alignment training. This is because es-\ntimating F IFA\ni\nrequires the LLM to be additionally\ntrained multiple times on the whole training set (see\nAppendix B). To mitigate this problem, we redesign\nthis estimation approach, and use the amount of pa-\nrameter changes before and after model training\nto compute the F. Furthermore, considering that\nLLMs typically have a large number of parame-\nters and the size of the F will be enormous, we\nattempt to implement EWC at the granularity of\nparameter units. Specifically, we redefine F as a\nnumerical value, with F IFA\ni\nrepresenting how im-\nportance of the parameter unit θIFA\ni\nas a whole to\nthe instruction-following alignment. This redefined\nF can be given by:\nF IFA\ni\n= Fmax ×\neCIFA\ni\nP\ni eCIFA\ni\n(6)\nwhere Fmax is the maximum value of F. CIFA\ni\ndenotes the amount of parameter θi changes before\nand after instruction-following alignment training\nfor the LLM, written as:\nCIFA\ni\n=\n1\n|θi|\n|θi|\nX\nj=1\n(θbefore\ni,j\n−θIFA\ni,j )2\n(7)\nAlgorithm 1 Hybrid Alignment Training\nInput: the pre-trained LLM M; the instruction-following\nalignment training dataset DIFA; the human-preference\nalignment training dataset DHPA\nOutput: the aligned LLM M;\n1: divide DIFA and DHPA into N subsets respectively;\n2: for n = 1 to N do\n3:\nif n==1 then\n4:\ntrain M on first subset of DIFA via Eq. 2;\n5:\nelse\n6:\ncompute the F HPA via Eq. 9;\n7:\ntrain M on n-th subset of DIFA via Eq. 8;\n8:\nend if\n9:\ncompute the F IFA via Eq. 6;\n10:\ntrain M on n-th subset of DHPA via Eq. 5;\n11: end for\n12: return M\nwhere j is the index corresponding to each neu-\nron within a parameter, |θi| is the number of neu-\nrons contained in the parameter θi, and θbefore is\nthe parameter set of the LLM before instruction-\nfollowing alignment training.\n4.3\nEWC for Alternating Alignment\nWe apply EWC on a global scale during alternating\nalignment. Specifically, we add the modified EWC\nnot only when learning each divided subset from\nDHPA as described in Section 4.2, but also when\nlearning each divided subset from DIFA. The moti-\nvation is that the instruction-following alignment\ncan likewise lead to an optimization conflict with\nhuman-preference alignment. LIFA can be induced\nby:\nLIFA = LMLE +\nX\ni\nλ\n2 F HPA\ni\n(θi −θHPA\ni\n)2\n(8)\nwhere θHPA is the parameters of the LLM trained\nby human-preference alignment. Here, similar to\nF IFA\ni\n, F HPA\ni\ncan be computed by:\nF HPA\ni\n= Fmax ×\neCHPA\ni\nP\ni eCHPA\ni\n(9)\nwhere CHPA\ni\ndenotes the amount of parameter θi\nchanges before and after human-preference align-\nment training for the LLM. It can be computed via\nEq. 7. Note that when learning the first subset\nD1\nIFA, since the LLM has not yet been trained with\nhuman preferences, we only employ the LMLE.\nIn the process of alternating alignment training,\nlearning a new subset from one alignment training\ndataset can produce optimization conflicts. These\nconflicts arise not only with the closest subset from\nanother alignment training dataset but also with\nall the previous subsets within this dataset. Thus,\nwhen estimating F, we consider the parameter\nchanges resulting from all previous subsets in an-\nother alignment training dataset. To this end, we\nreplace the CIFA\ni\nand CHPA\ni\nin Eqs. 8 and 5 with ac-\ncumulated parameter changes ACIFA\ni\nand ACHPA\ni\nfrom all previous subsets in DIFA and DHPA, re-\nspectively. Here, when learning from n-th subset,\nwe compute ACIFA\ni,n and ACHPA\ni,n\nby:\nACIFA\ni,n =\nn\nX\nk=1\nCIFA\ni,k , ACHPA\ni,n\n=\nn\nX\nk=1\nCHPA\ni,k\n(10)\nwhere CIFA\ni,k and CHPA\ni,k\nare the amount of parameter\nchanges produced at learning k-th subset in DIFA\nand DHPA, respectively. The process of our HBAT\nis also described in Algorithm 1.\n5\nExperimental Setup\nWe evaluated HBAT on summarization and dia-\nlogue tasks based on the commonly used LLaMA2-\n7B and LLaMA2-13B models.\n5.1\nDatasets\nThe datasets used for each task are as follows:\nSummarization.\nWe used the same dataset as\nStiennon et al. (2020), which is a filtered version*\nof the TL;DR dataset (Völske et al., 2017). The\nfiltered training set consists of 120k Reddit posts\nwith accompanying summaries. For instruction-\nfollowing training and human-preference align-\nment training, we used all posts in a filtered training\nset, respectively. The filtered test set and validation\nset contain 6,553 posts and 6,447 posts respectively,\nwhich would result in a huge computational cost\nwhen used on a large scale. Thus, we randomly\nselected 10% of posts from them as a test set and\na validation set in our experiments, respectively.\nFor training reward models, we employed the open-\nsource 92.9k summary comparisons†.\nDialogue.\nWe conducted experiments on the Al-\npaca data (Taori et al., 2023a) which contains\n52k training samples.\nHere, we employed the\nsliced data splits‡ released by AlpacaFarm (Dubois\n*https:\/\/github.com\/openai\/\nsummarize-from-feedback\n†https:\/\/huggingface.co\/datasets\/\nopenai\/summarize_from_feedback\n‡https:\/\/huggingface.co\/datasets\/\ntatsu-lab\/alpaca_farm\net al., 2023) to conduct instruction-following align-\nment training, reward model training, and human-\npreference alignment training. Note that we used\nthe human preferences rather than the simulated\npreferences to train our reward models. In the eval-\nuation, we employed the AlpacaFarm evaluation\nset which consists of 805 instructions. We ran-\ndomly selected 200 instructions from them as our\nvalidation set and the rest as our test set.\n5.2\nSettings\nWe trained reward models with the ranking loss\nfor all tasks, following Stiennon et al. (2020). For\ninstruction-following alignment training, we em-\nployed the cross-entropy loss on batches of prompts\nconcatenated with responses, computing the loss\nonly on the response tokens. For human-preference\nalignment training, we used PPO and DPO as our\nbase algorithms. For HBAT, we set the number of\ndataset splits to 2 and 10 for dialogue and sum-\nmarization tasks, respectively. Additionally, we\nemployed a top-p sampling strategy for generation,\nwhere the temperature and p were set to 0.75 and\n0.95, respectively, values that are commonly used\nin real-world applications. We publicly release\nall our code used for the experiments described\nin this work§. More training details are shown in\nAppendix A.\n5.3\nEvaluation Metrics\nFor the summarization task, we measured the sum-\nmary quality by computing ROUGE (Lin, 2004)\nand BARTScore (Yuan et al., 2021), respectively.\nFor the dialogue task, we measured the response\nquality with PandaLM (Wang et al., 2023d) which\ncan distinguish the superior model from some\nLLMs. To further evaluate the performance of the\nmodel, we employed GPT-4 as a proxy for human\nevaluation of summary and response quality in the\ndialogue and summarization tasks, where the used\nevaluation prompts were the same as in Rafailov\net al. (2023). We used reference summaries and re-\nsponses in the test set as the baseline. Additionally,\nfollowing Stiennon et al. (2020)’s work, we evalu-\nated the model by computing the reward scores of\ntest sets via our reward models.\n§https:\/\/github.com\/wangclnlp\/\nDeepSpeed-Chat-Extension\/tree\/main\/\nexamples\/hybrid_alignment_training\nMethod\n#Param\nPPO\nDPO\nSummarization\nDialogue\nROUGE-L\nBS\nReward\nWin\nPandaLM\nReward\nWin\nBased on LLaMA2-7B Model\nSFT\n7B\n22.60\n-5.46\n3.72\n53.20\n54.76\n-6.79\n43.49\nRLHF\n7B\n✓\n25.85\n-4.27\n4.43\n63.80\n69.79\n-5.81\n55.63\nRLHF+pt\n7B\n✓\n22.25\n-5.64\n3.74\n56.26\n53.52\n-7.09\n54.18\nSFT+ppo\n7B\n✓\n13.75\n-5.78\n2.40\n18.91\n45.32\n-8.60\n42.25\nHBAT-Freeze\n7B\n✓\n25.33\n-4.28\n5.26\n64.79\n69.91\n-5.91\n56.19\nHBAT (Ours)\n7B\n✓\n26.18\n-3.82\n5.74\n72.52\n70.88\n-5.37\n57.12\nDPO\n7B\n✓\n22.96\n-5.13\n4.27\n61.37\n70.74\n-5.72\n54.23\nHBAT-Freeze\n7B\n✓\n23.01\n-5.05\n4.45\n64.18\n68.78\n-5.41\n56.95\nHBAT (Ours)\n7B\n✓\n23.14\n-4.18\n4.95\n70.58\n74.78\n-5.22\n58.10\nBased on LLaMA2-13B Model\nSFT\n13B\n23.27\n-5.12\n4.01\n57.91\n62.16\n-6.32\n46.11\nRLHF\n13B\n✓\n24.51\n-3.96\n5.55\n71.67\n72.21\n-5.65\n61.16\nRLHF+pt\n13B\n✓\n22.92\n-5.49\n3.97\n64.42\n63.67\n-6.97\n54.45\nSFT+ppo\n13B\n✓\n13.84\n-5.97\n2.53\n28.97\n54.00\n-7.93\n43.12\nHBAT-Freeze\n13B\n✓\n25.80\n-3.63\n6.18\n77.22\n71.31\n-5.49\n56.37\nHBAT (Ours)\n13B\n✓\n26.77\n-3.51\n6.41\n78.81\n72.83\n-5.11\n62.32\nDPO\n13B\n✓\n23.02\n-5.39\n4.55\n69.40\n75.00\n-5.07\n64.31\nHBAT-Freeze\n13B\n✓\n23.10\n-5.08\n4.85\n71.44\n76.87\n-5.01\n65.62\nHBAT (Ours)\n13B\n✓\n24.12\n-4.05\n5.40\n74.92\n77.79\n-4.78\n67.45\nTable 1: Results on summarization and dialogue tasks. The best results for each group are in bold. The “BS” and\n“Win” columns report the BARTScore and the win rate as assessed by GPT-4, respectively. The “PPO” and “DPO”\ncolumns denote that we employ PPO and DPO during human-preference alignment training, respectively.\n5.4\nBaselines\nOur baselines are the standard two-stage alignment\ntraining (referred to as RLHF\/DPO) and the com-\nmonly used instruction-following alignment train-\ning (referred to as SFT). Furthermore, we com-\npared the proposed HBAT with commonly used\nmulti-objective optimization methods, including\nadding a pre-training loss in the human-preference\nalignment training (RLHF+pt) (Ouyang et al.,\n2022) and adding a human-preference alignment\nloss in the instruction-following alignment training\n(SFT+ppo) (Wang et al., 2023a). To evaluate the\neffectiveness of EWC, we also chose the HBAT-\nFreeze method as a baseline, where we directly\nfroze important parameters instead of EWC.\n5.5\nExperimental Results\nTable 1 displays the experimental results on sum-\nmarization and dialogue tasks.\nResults of Summarization.\nFirst, compared\nwith the traditional two-stage alignment training\nand instruction-following alignment training, the\nproposed HBAT can achieve optimal results on both\nof LLaMA2-7B and LLaMA2-13B. Notably, HBAT\noutperforms RLHF by 7.14 points on the GPT-4\nwin rate when using PPO on the LLaMA2-13B\nmodel. Second, compared with multi-task learning-\nbased methods, including RLHF+pt and SFT+ppo,\nwe can see that HBAT has significant improvements\non all evaluation metrics. For instance, compared\nto RLHF+pt, HBAT yields a +3.93 ROUGE-L im-\nprovement on the LLaMA2-7B model. Also, we\nsee that the multi-objective optimization method\ncan hurt alignment, e.g., RLHF+pt loses 0.69 Re-\nward points on the LLaMA2-7B model. The phe-\nnomenon aligns with the observation reported in\nOuyang et al. (2022)’s work. One potential ex-\nplanation can be that while these multi-objective\noptimization methods achieve optimization of these\nobjectives simultaneously, they still suffer from se-\nrious optimization conflict (Zhang and Yang, 2021).\nThird, when using DPO during human-preference\nalignment training, our HBAT is consistently better\nthan all baselines. For a LLaMA2-13B model, it\nobtains a GPT-4 win rate of 74.92.\nResults of Dialogue.\nWe also evaluate the pro-\nposed HBAT on the dialogue task. Similarly, when\nusing PPO during human-preference alignment\nSummarization\nDialogue\nMethod\nPPO\nDPO\nCoherence\nAccuracy\nCoverage\nOverall\nFluency\nAccuracy\nToxicity\nHelpfulness\nOverall\nSFT\n5.63\n4.91\n5.03\n5.13\n8.84\n7.77\n8.49\n7.43\n7.31\nRLHF\n✓\n5.84\n4.63\n4.82\n5.16\n8.39\n7.62\n8.87\n7.47\n7.72\nHBAT\n✓\n6.23\n4.83\n5.77\n5.69\n8.80\n7.70\n8.48\n7.39\n7.89\nDPO\n✓\n5.83\n4.45\n5.20\n5.27\n8.67\n7.13\n8.54\n7.63\n7.84\nHBAT\n✓\n5.93\n5.01\n5.40\n5.49\n8.79\n7.80\n8.45\n7.51\n7.96\nTable 2: The results of human evaluation on the LLaMA2-13B model for our HBAT and baselines.\ntraining, we can observe that HBAT outperforms\nRLHF by a large margin (e.g., 2.21 PandaLM and\n0.54 Reward benefits on the LLaMA2-13B model).\nHowever, different from the summarization task,\nwe find that DPO can achieve better performance\nthan PPO on the dialogue task. For instance, when\nusing LLaMA2-13B, HBAT with DPO can outper-\nform PPO by a margin of 5.13 points on the GPT-4\nwin rate. We assume that this is attributed to the re-\nward model quality. To verify this assumption, we\nconduct tests on the employed reward models and\nfind a significant difference in accuracy between\nthe two tasks: the accuracy of the reward model for\nthe summarization task significantly exceeds that\nof the dialogue task, achieving 0.75 compared to\n0.65, respectively.\nFurthermore, compared with HBAT-Freeze, we\nsee that HBAT achieves better performance on all\ntasks. It demonstrates that freezing specific param-\neters is inferior to constraining specific parameters.\nWe attribute this to the fact that the freezing oper-\nation reduces the amount of learnable parameters,\nwhich imposes a hurdle to learning new knowledge.\n5.6\nHuman Evaluation\nWe further conduct a human evaluation of the ob-\ntained results through comprehensive evaluation\naspects. For the summarization task, following Sti-\nennon et al. (2020), we consider four evaluation\naspects, including coherence, accuracy, coverage,\nand overall score. We provide three optional scores\nof 1, 4, and 7 for each evaluation aspect. Similarly,\nfor the dialogue task, we consider five evaluation as-\npects: fluency, accuracy, toxicity, helpfulness, and\noverall score. We have defined detailed evaluation\nrubrics similar to those for the summarization task.\nPlease refer to Table 8 in the Appendix for descrip-\ntions of all the evaluation rubrics. The results of\nhuman evaluation on the LLaMA2-13B model are\nshown in Table 2. From these evaluation results, we\nsee that whether using PPO or DPO, our HBAT can\nachieve optimal results compared to the baselines\nin terms of the overall score of human evaluation.\nThis demonstrates that HBAT has a great advantage\nin practical NLP applications. Furthermore, we\nalso see that in some evaluation aspects, our HBAT\nis weaker compared to the baseline, e.g., helpful-\nness aspect on the dialogue task. This is due to the\nfact that the aim of our HBAT is to achieve a Pareto\nalignment between the instruction-following align-\nment and the human-preference alignment, not to\nimprove for a particular alignment.\nTable 3 presents several cases for human eval-\nuation. Case 1 shows that RLHF (i.e., human-\npreference alignment) won’t always improve\nthe performance of LLM trained by SFT (i.e.,\ninstruction-following alignment).\nThis demon-\nstrates that these two alignment optimization objec-\ntives are different, and aligning LLMs with these\nobjectives in sequence might cause an optimization\nconflict. For example, in this case, SFT is probably\nmore concerned with instruction following and re-\nsponse accuracy, while RLHF is more concerned\nwith response helpfulness. In Case 2, we can ob-\nserve that although the overall score of RLHF has\nincreased, the fluency and accuracy scores have de-\ncreased, compared to SFT. In this case, our HBAT\nachieves a Pareto alignment through iterative align-\nment and modified elastic weight consolidation\napproaches, which aim to find a relatively opti-\nmal trade-off between instruction-following and\nhuman-preference alignment, thus achieving a bet-\nter performance.\n5.7\nAblation Studies\nIn this section, we present detailed ablation stud-\nies to explore the effects of EWC and alternating\nalignment with DPO on the LLaMA2-7B model.\nThe experiments are conducted on the dialogue\ndataset, and the impacts of removing each method\nare thoroughly examined. The results are summa-\nrized in Table 4. From the results, we see that the\nmodified EWC can significantly improve response\nquality. Notably, HBAT obtains a +5.35 points im-\nCase 1\nInput\nPlease write the Excel function name associated with each description.- Returns the number of days between two dates - Returns the\nstarting position of a text string within another text string. - Returns the number in the middle of the set of given numbers.\nSFT\n1. DAYS(date1, date2) 2. FIND(text1, text2) 3. MID(text1, start_num, num).\nFluency:7 Accuracy:6 Toxicity:10 Helpful:5 Overall:6\nRLHF\n1.=DATEDIF(A2,B2,\"d\")\n2.=FIND(A2,B2)\n3.=MID(A2:B2,FIND(A2:B2,A2)+1,1)\n4.=RANDBE-\nTWEEN(A2,B2)\n5.=SUMIF(A2:B2,\">=\"&A2,A2:B2)\n6.=SUMPRODUCT(A2:B2)\n7.=SUM(A2:B2)\n8.=AV-\nERAGE(A2:B2)\n9.=AVERAGEIF(A2:B2,\"<\"&A2,A2:B2)\n10.=COUNTIF(A2:B2,\">\"&A2)\n11.=COUN-\nTIFS(A2:B2,\"<\"&A2,A2:B2,\">=\"&B2,A2:B2,\"<=\"&B2) 12.=IFERROR(VLOOKUP(A2,A2:B2,2,0),\"\")...\nFluency:5 Accuracy:4 Toxicity:10 Helpful:6 Overall:4\nHBAT\n1.DATEDIF(\\em{date1},} date2\\em{,} \\em{“day” \\em{) \\n2. FIND(\\em{text1},} text2\\em{,} \\em{} \\em{} \\em{} \\em{} \\n3.\nMID(\\em{text},} start\\em{,} numchars\\em{)}\nFluency:7 Accuracy:6 Toxicity:10 Helpful:6 Overall:8\nCase 2\nInput\nI assume you are familiar with the Drake equation. Can you give estimates and error margins for each of its parameters, then\ncalculate the result including error bounds.\nSFT\nThe number of stars in the galaxy is estimated to be between 1022 and 1024, with a mid-range value of 1023. \\n2. The fraction of\nstars that have planets is estimated to be between 0.1 and 0.2, with a mid-range value of 0.15. \\n3. The average number of planets\nthat can support life per star is estimated to be between 1 and 2, with a mid-range value of 1.5. \\n4. The fraction of planets where\nlife has evolved is estimated to be between 0.01 and 0.1, with a mid-range value of 0.05. \\n5. The fraction of planets where life has\nevolved that develops intelligent life is estimated to be between 0.01 and 0.05, with a mid-range value of 0.03...\nFluency:9 Accuracy:6 Toxicity:8 Helpful:6 Overall:5\nRLHF\n1. Number of civilizations in our Galaxy: The number of civilizations in our Galaxy can be estimated using the Drake equation.\nThe equation states that the number of civilizations in the Galaxy (N) is equal to the product of the rate of star formation (R*)\nmultiplied by the fraction of stars with planets (fP) multiplied by the fraction of planets that can support life (fL) multiplied by the\nfraction of planets with life (fL) multiplied by the fraction of planets with intelligent life (fI) multiplied by the fraction of intelligent\ncivilizations that are capable of communicating with us (fC) multiplied by the fraction of those that are willing to communicate\n(fW): \\n\\nN = R* fP fL fI fC fW\\n \\nThe rate of star formation (R*) can be estimated to be 1-2 stars per year in our Galaxy...\nFluency:5 Accuracy:5 Toxicity:8 Helpful:8 Overall:7\nHBAT\n1. The number of stars with planetary systems (N*): Unknown, but estimated to be between 10−5 and 10−8 per star. Error margin:\n10 orders of magnitude. \\n2. The fraction of stars with planetary systems that could support life (fp): Unknown, but estimated to be\nbetween 10−5 and 0.5. Error margin: 10 orders of magnitude. \\n3. The average number of planets that can support life per star with\na planetary system (ne): Unknown, but estimated to be between 1 and 10. Error margin: 1 order of magnitude. \\n4. The fraction of\nplanets that could support life that actually develop life (fl): Unknown, but estimated to be between 0.01 and 0.99. Error margin: 1\norder of magnitude. \\n5. The ...\nFluency:7 Accuracy:6 Toxicity:8 Helpful:8 Overall:8\nTable 3: Several cases from the dialogue task on the LLaMA2-13B model.\nMethod\nPandaLM\nReward\nWin\nSFT\n43.64\n-6.80\n43.08\nDPO\n69.97\n-5.68\n53.80\nHBAT\n75.76\n-5.11\n60.10\nw\/o EWC\n67.53\n-5.76\n54.75\nw\/o Alternating Alignment\n70.50\n-5.26\n56.92\nTable 4: Ablation studies on the components of HBAT.\nWe report the scores for the dialogue validation set.\nprovement on GPT-4 win rate with the modified\nEWC. Additionally, the results indicate a signifi-\ncant dependency of our HBAT on the alternating\nalignment. The absence of this method results in\nHBAT fails a well-performed dialogue model.\n5.8\nAnalysis\nEffect of the Number of Dataset Splits.\nBased\non the LLaMA2-7B model, we investigate the im-\npact of dividing the dataset into different numbers\nof splits. As shown in Figure 2 (top), we swept over\ndifferent numbers: {1, 2, 3, 4, 5}. From the results,\nwe find that excessive dataset splits can hurt the per-\nformance of the aligned LLM. We conjecture the\nunderlying reason is that when datasets are heav-\nily divided, each subset does not have sufficient\nsamples for training.\nEffect of Fmax on Performance.\nThe maximum\nvalue of F, Fmax, is a key factor that controls the\nstrength of parameter constraints. We conduct ex-\nperiments to study the impact of setting different\nvalues of Fmax: {1, 50, 100, 150, 200}. The corre-\nsponding Reward and PandaLM scores are listed\nin Figure 2 (bottom). From the results, we see that\nthe use of different values of Fmax can result in dif-\nferent performance gains. We find that the optimal\nFmax is 50, and this setting allows for appropriate\ncontrol over parameter constraints. We conduct\nsimilar experiments to determine the optimal val-\nues for N and Fmax for the summarization task,\nDPO\nHBAT\n1\n2\n3\n4\n5\n−6.5\n−6.0\n−5.5\n−5.0\nN\nReward\n1\n2\n3\n4\n5\n60.0\n70.0\n80.0\nN\nPandaLM\n(a) Performance of HBAT with different N\n1\n50\n100\n150\n200\n−6.0\n−5.8\n−5.6\n−5.4\n−5.2\n−5.0\nFmax\nReward\n1\n50\n100\n150\n200\n68.0\n70.0\n72.0\n74.0\n76.0\n78.0\nFmax\nPandaLM\n(b) Performance of HBAT with different Fmax\nFigure 2: Performance of HBAT with different number\nof dataset splits (i.e., N) and the maximum values of F\n(i.e., Fmax) on the dialogue validation set.\nwhich are found to be 10 and 50 respectively.\nPerformance on Different Temperature Settings.\nIn real-world applications, various temperature set-\ntings are employed in the process of LLM gener-\nation according to specific scenarios. To this end,\nwe compute the PandaLM scores under different\ntemperature settings on the dialogue task to provide\na comprehensive evaluation. The results are shown\nin Figure 3. From the results, we can observe that\nHBAT exceeds DPO’s best-case performance on the\ndialogue task while being more robust to changes\nin the temperature setting.\nSee more analysis in Appendix B.\n6\nConclusion\nIn this paper, we focus on solving the optimiza-\ntion conflict of alignment training in LLMs. We\nhave proposed a hybrid alignment training (HBAT)\nvia the alternating alignment and modified elastic\nweight consolidation methods. Our extensive ex-\nperiments show that our HBAT can significantly\noutperform all baselines.\n7\nLimitations\nIn this section, we discuss some limitations of this\nwork as follows:\n• We did not verify HBAT in other NLP tasks.\nThere are so many NLP tasks that we cannot\nverify our HBAT one by one. Thus, we take\nsummarization and dialogue as instances in\nthis paper. The summarization is a commonly\n0\n0.25\n0.5\n0.75\n1.0\n50.0\n60.0\n70.0\n80.0\n90.0\nSampling Temperature\nPandaLM\nSFT\nDPO\nHBAT\nFigure 3: PandaLM score for different sampling tem-\nperatures on the LLaMA2-7B model. For each dialogue\nmodel, we conduct the generation three times and report\nthe mean score of these generated responses.\nused task for verifying the effectiveness of\nLLM alignment methods. Additionally, in the\ndialogue task, the Alpaca dataset we used con-\nsists of many NLP tasks (Taori et al., 2023b),\nincluding machine translation, sentiment clas-\nsification, and text simplification.\n• We did not attempt more preference-alignment\nmethods. In this work, we verify the effective-\nness of HBAT based on representative PPO,\nDPO, and ESRL, i.e., it can offer a refine-\nment of the collaboration among instruction-\nfollowing alignment and human-preference\nalignment. Although there are some other\npreference-alignment methods that we did not\nexperiment with, such as RRHF (Yuan et al.,\n2023), RAFT (Dong et al., 2023), and RL4F\n(Akyürek et al., 2023), HBAT is a general ap-\nproach and can be easily extended to these.\nAcknowledgements\nThis work was supported in part by the National\nScience Foundation of China (No.62276056), the\nNatural Science Foundation of Liaoning Province\nof China (2022-KF-16-01), the Fundamental Re-\nsearch Funds for the Central Universities (Nos.\nN2216016 and N2316002), the Yunnan Fundamen-\ntal Research Projects (No. 202401BC070021), and\nthe Program of Introducing Talents of Discipline to\nUniversities, Plan 111 (No.B16009). The authors\nwould like to thank Yang Gan and Yifu Huo for\ntheir help in human evaluation.\nReferences\nAfra Feyza Akyürek, Ekin Akyürek, Aman Madaan,\nAshwin Kalyan, Peter Clark, Derry Wijaya, and Niket\nTandon. 2023. Rl4f: Generating natural language\nfeedback with reinforcement learning for repairing\nmodel outputs. ArXiv preprint.\nJohannes Bader and Eckart Zitzler. 2011. Hype: An al-\ngorithm for fast hypervolume-based many-objective\noptimization. Evolutionary computation.\nJixiang Cheng, Gary G Yen, and Gexiang Zhang. 2015.\nA many-objective evolutionary algorithm with en-\nhanced mating and environmental selections. IEEE\nTransactions on Evolutionary Computation.\nThomas Coste, Usman Anwar, Robert Kirk, and David\nKrueger. 2023. Reward model ensembles help miti-\ngate overoptimization. ArXiv preprint.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and\nTong Zhang. 2023. Raft: Reward ranked finetuning\nfor generative foundation model alignment. ArXiv\npreprint.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. 2023. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback. ArXiv preprint.\nRobert M French. 1999. Catastrophic forgetting in con-\nnectionist networks. Trends in cognitive sciences.\nLeo Gao, John Schulman, and Jacob Hilton. 2023. Scal-\ning laws for reward model overoptimization. In Proc.\nof ICML.\nJian Hu, Li Tao, June Yang, and Chandler Zhou. 2023.\nAligning language models with offline reinforcement\nlearning from human feedback. ArXiv preprint.\nC-L Hwang and Abu Syed Md Masud. 2012. Multi-\nple objective decision making—methods and applica-\ntions: a state-of-the-art survey. Springer Science &\nBusiness Media.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Overcom-\ning catastrophic forgetting in neural networks. Pro-\nceedings of the national academy of sciences.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie\nLu, Thomas Mesnard, Colton Bishop, Victor Car-\nbune, and Abhinav Rastogi. 2023. Rlaif: Scaling\nreinforcement learning from human feedback with ai\nfeedback. ArXiv preprint.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries.\nIn Text Summarization\nBranches Out.\nBo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and\nQiang Liu. 2021. Conflict-averse gradient descent\nfor multi-task learning. In Proc. of NeurIPS.\nKaisa Miettinen, Francisco Ruiz, and Andrzej P\nWierzbicki. 2008.\nIntroduction to multiobjective\noptimization: interactive approaches. In Multiob-\njective optimization: interactive and evolutionary\napproaches.\nGiovanni Misitano, Bhupinder Singh Saini, Bekir Afsar,\nBabooshka Shavazipour, and Kaisa Miettinen. 2021.\nDesdeo: The modular and open source framework\nfor interactive multiobjective optimization. IEEE\nAccess.\nIn Jae Myung. 2003. Tutorial on maximum likelihood\nestimation. Journal of mathematical Psychology.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback. ArXiv preprint.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Proc. of NeurIPS.\nRazvan Pascanu and Yoshua Bengio. 2014. Revisiting\nnatural gradient for deep networks. In Proc. of ICLR.\nJoão Luiz Junho Pereira, Guilherme Antônio Oliver,\nMatheus Brendon Francisco, Sebastiao Simoes\nCunha Jr, and Guilherme Ferreira Gomes. 2022.\nA review of multi-objective optimization: methods\nand algorithms in mechanical engineering problems.\nArchives of Computational Methods in Engineering.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. ArXiv preprint.\nMiguel Moura Ramos, Patrick Fernandes, António Far-\ninhas, and André FT Martins. 2023. Aligning neu-\nral machine translation models: Human feedback in\ntraining and inference. ArXiv preprint.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms. ArXiv preprint.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F. Christiano. 2020. Learn-\ning to summarize with human feedback. In Proc. of\nNeurIPS.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023a.\nAlpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023b. Stanford alpaca:\nAn instruction-following llama model.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open foundation and\nfine-tuned chat models. ArXiv preprint.\nMichael Völske, Martin Potthast, Shahbaz Syed, and\nBenno Stein. 2017. TL;DR: Mining Reddit to learn\nautomatic summarization.\nIn Proceedings of the\nWorkshop on New Frontiers in Summarization.\nTobias Wagner, Heike Trautmann, and Dimo Brockhoff.\n2013. Preference articulation by means of the r 2 indi-\ncator. In Evolutionary Multi-Criterion Optimization:\n7th International Conference, EMO 2013, Sheffield,\nUK, March 19-22, 2013. Proceedings 7.\nChenglong Wang, Hang Zhou, Kaiyan Chang, Tongran\nLiu, Chunliang Zhang, Quan Du, Tong Xiao, and\nJingbo Zhu. 2023a.\nLearning evaluation models\nfrom large language models for sequence generation.\nArXiv preprint.\nChenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo,\nBei Li, Tongran Liu, Tong Xiao, and Jingbo Zhu.\n2023b. Esrl: Efficient sampling-based reinforcement\nlearning for sequence generation. ArXiv preprint.\nPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai\nLin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. 2023c.\nMaking large language models better reasoners with\nalignment. ArXiv preprint.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,\nCunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie,\nJindong Wang, Xing Xie, et al. 2023d. Pandalm: An\nautomatic evaluation benchmark for llm instruction\ntuning optimization. ArXiv preprint.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. ArXiv\npreprint.\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xing-\nshan Zeng, Wenyong Huang, Lifeng Shang, Xin\nJiang, and Qun Liu. 2023e. Aligning large language\nmodels with human: A survey. ArXiv preprint.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine learning.\nPeng Wu and Li Pan. 2019. Multi-objective community\ndetection based on memetic algorithm. PloS one.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\nSuhr, Prithviraj Ammanabrolu, Noah A Smith, Mari\nOstendorf, and Hannaneh Hajishirzi. 2023. Fine-\ngrained human feedback gives better rewards for lan-\nguage model training. ArXiv preprint.\nBin Xin, Lu Chen, Jie Chen, Hisao Ishibuchi, Kaoru\nHirota, and Bo Liu. 2018. Interactive multiobjective\noptimization: A review of the state-of-the-art. IEEE\nAccess.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Proc. of NeurIPS.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023. Rrhf: Rank\nresponses to align language models with human feed-\nback without tears. ArXiv preprint.\nYu Zhang and Qiang Yang. 2021. A survey on multi-\ntask learning. IEEE Transactions on Knowledge and\nData Engineering.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\nArXiv preprint.\nHang Zhou, Chenglong Wang, Yimin Hu, Tong\nXiao, Chunliang Zhang, and Jingbo Zhu. 2024.\nPrior constraints-based reward model training for\naligning large language models.\narXiv preprint\narXiv:2404.00978.\nA\nExperimental Details\nA.1\nSetups\nInstruction-Following Alignment.\nWe set the\nlearning rate, batch size, and training epoch to 1e-\n5, 64, and 3. We did not conduct tuning of these\nhyper-parameters specific to the task and the model,\nas our experiments with other hyper-parameters did\nnot yield a significant performance improvement.\nReward Model Training.\nWe initialized the\nmodel using the LLM trained by instruction-\nfollowing alignment training. For all tasks, we\ntrained the reward model for 2 epochs with a learn-\ning rate of 1e-5 and a batch size of 64.\nPPO Training.\nWe followed an existing PPO im-\nplementation in trlX¶ for training the LLM. For\nall tasks, the learning rate was set to 1e-5 and 5e-6\nfor the policy model and the value model, respec-\ntively. We settled on a batch size of 64 for each\nPPO step, which consisted of 1 epoch of gradient\nsteps and 4 epochs of mini-batch PPO steps. To\naddress the overoptimization issue as described in\nGao et al. (2023)’s work, we implemented a strat-\negy that saves checkpoints at regular intervals dur-\ning the training process. Specifically, we evaluated\ncheckpoints at intervals of 500 steps for the sum-\nmarization task and 200 steps for the dialogue task\nagainst their respective validation sets and selected\nthe optimal checkpoint with the best Reward score.\nAdditionally, we employed a cold-start trick for\nPPO, to alleviate the damage caused by the inaccu-\nrate estimation of the early value model. Specifi-\ncally, we updated only the value model and did not\nupdate the policy model during the first 50 steps of\nPPO training. The setups of advantage estimation\nand KL regularizer coefficient were the same as in\ntrlX.\nDPO Training.\nWe used a batch size of 64, a\nlearning rate of 1e-6, and a training epoch of 2\nfor DPO training. Apart from these parameters,\nthe rest of our training setups were the same as in\nRafailov et al. (2023).\nHBAT.\nFmax was set to 50 and 100 on the sum-\nmarization task and the dialogue task, respectively.\nλ and N were set 1 and 10 for all tasks. After\ntraining each subset, we evaluated the model’s per-\nformance with the validation set. The model that\nhas the highest Reward score was selected as the\n¶https:\/\/github.com\/CarperAI\/trlx\nTask\nTraining\nStage\nTrain\nValid\nTest\nSummarization\nIFA\n123,169\n645\n655\nReward\n92,858\n1,000\n2,000\nHPA\n123,169\n645\n655\nDialogue\nIFA\n10,000\n200\n605\nReward\n9,591\n100\n200\nHPA\n20,000\n200\n605\nTable 5: Statistical information on summarization and\ndialogue datasets.\nIFA: instruction-following align-\nment; Reward: training a reward model; HPA: human-\npreference alignment.\noptimal one. Concurrently, we saved the value\nmodel after learning from a subset of the human-\npreference dataset. This saved model was utilized\nto initialize the value model for subsequent learning\nof a new subset of the human-preference dataset.\nFurthermore, in HBAT-Freeze, we froze the top\n20% important parameters based on the computed\nparameter importance scores.\nA.2\nDataset Statistics\nThe statistical information on the utilized datasets\nis summarized in Table 5.\nA.3\nEvaluation\nPandaLM.\nIn this section, we describe how we\ncompute the PandaLM score. Given the pairwise\ntest responses {(x0, r0\na, r0\nb), · · · , (xT , rT\na , rT\nb )},\nwhere T is the number of the test set, PandaLM\ncan give the preference of each pairwise response,\nincluding Pa, Pb, and Tie. Here, Pa denotes re-\nsponse ra is better than response rb, Pb denotes\nresponse rb is worse than response rb, while Tie\ndenotes a tie between response ra and response\nrb. We can compute the PandaLM score for the re-\nsponse ra model and the response rb model through\nthe given preferences:\nSa\nPandaLM =\nCount(Pa)\nT −Count(Tie)\n(11)\nSb\nPandaLM =\nCount(Pb)\nT −Count(Tie)\n(12)\nwhere Count(·) denotes the count of the specified\npreference.\nGPT-4 Prompts for Win Rates.\nAs shown in\nFigure 4, the prompts of GPT-4 evaluation are the\nsame as in Rafailov et al. (2023).\nWhich of the following summaries does a better job\nof summarizing the most important points in the\ngiven forum post, without including unimportant or\nirrelevant details? A good summary is both precise and\nconcise.\nPost:\n<post>\nSummary A:\n<Summary A>\nSummary B:\n<Summary B>\nFIRST provide a one-sentence comparison of the two\nsummaries,explaining which you prefer and why. SECOND,\non a new line. state only \"A\" or \"B\" to indicate your\nchoice. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nPreferred: < \"A\" or \"B\">\n(a) Summarization GPT-4 win rate prompt\nFor the following query to a chatbot, which response is\nmore helpful?\nQuery: <the user query>\nResponse A:\n<either the test method or baseline>\nResponse B:\n<the other response>\nFIRST provide a one-sentence comparison of the two\nresponses and explain which you feel is more helpful.\nSECOND, on a new line, state only \"A\" or \"B\" to indicate\nwhich response is more helpful. Your response should\nusethe format:\nComparison: <one-sentence comparison and explanation>\nMore helpful: < \"A\" or \"B\">\n(b) Dialogue GPT-4 win rate prompt\nFigure 4: Prompt templates of computing GPT-4 win rates for summarization and dialogue tasks.\n50\n250\n450\n650\n850\n40.0\n60.0\n80.0\nTraining Step\nPandaLM\nSFT\nPPO\nDPO\nHBAT-PPO\nHBAT-DPO\nFigure 5: PandaLM score over training steps for the\nHBAT and traditional two-stage alignment training.\nB\nMore Analysis\nComparison of Training Process on Different\nMethods.\nWe analyze the training process of our\nHBAT on the dialogue task. Figure 5 shows the Pan-\ndaLM on the validation set of the LLMs aligned by\nHBAT and the traditional two-stage alignment meth-\nods. We observe that alignment training with HBAT\nimproves performance more efficiently than that\nwith the two-stage method. Furthermore, when us-\ning PPO during human-preference alignment train-\ning, we can observe that HBAT can mitigate reward\nmodel overoptimization (Gao et al., 2023).\nIntegration of Efficient Sampling Method.\nOur\nHBAT is orthogonal to the other mainstream meth-\nods for improving LLM alignment. Here, we take\nESRL, an efficient sampling-based reinforcement\nlearning method (Wang et al., 2023b), as an in-\nstance. Specifically, we integrate ESRL with the\nPPO algorithm inside our HBAT. In ESRL, we\nMethod\nSummarization\nDialogue\nBS\nWin\nPandaLM\nWin\nPPO\n-4.27\n63.80\n69.79\n55.63\nHBAT\n-3.82\n72.52\n70.88\n61.45\nESRL\n-4.01\n65.90\n70.33\n58.54\nHBAT+ESRL\n-3.65\n75.11\n72.91\n62.56\nTable 6: Performance on summarization and dialogue\ntasks, using the LLaMA2-7B model aligned with HBAT\nand ESRL. We implemented ESRL on our test bed with\nthe same setups as in Wang et al. (2023b).\nMtehod\nTraining\nMemory\nWin\nDPO\n1.00×\n52.77G\n54.23\nHBAT\n1.26×\n61.13G\n58.10\nHBAT w\/ original EWC\n1.64×\n73.55G\n58.32\nTable 7: The comparison of efficiency and performance\nbetween the modified EWC and the original EWC. We\ntest the training efficiency and memory consumption\non eight A800 GPUs. Time: training time; Memory:\nmaximum memory consumption.\nemploy the predicted reward score to estimate\nmodel capability. Table 6 shows that the integrated\nmethod achieves superior performance.\nFisher Information Matrix.\nThis original EWC\nemploys the Fisher information matrix, denoted as\nFθ, to measure information contained in model pa-\nrameters θ after learning a task (Kirkpatrick et al.,\n2017). The Fisher information represents the ex-\npected information that an observation can provide\nabout an unknown parameter (Pascanu and Bengio,\n2014). It can be estimated via first-order derivatives\nof the generative probability pθ(y|x), as described\nin Eq. 1:\nFθ\n=\nE\n\"\u0012∂log pθ(y|x)\n∂θ\n\u00132 \f\f\f\fθ\n#\n(13)\n=\n1\n|D|\nX\n(x,y)∈D\n\u0012∂log pθ(y|x)\n∂θ\n\u00132\n(14)\nwhere D is the training dataset. When employ-\ning this method in the context of LLM training,\nestimating the Fisher information requires com-\nputing the gradients for each sample within the\ntraining dataset through forward propagation and\nbackpropagation. Then the gradients of each model\nparameter are summed and divided by the number\nof samples. This process poses two challenges to\nLLM training. The first is that the frequent compu-\ntation of large-scale parameter gradients leads to\nsignificant computational costs. The second is that\nthe size of the information matrix will be huge (the\nsame size as the parameters of the aligned LLM),\nleading to significant GPU memory consumption.\nTo address these challenges, we propose a modified\nEWC method (see Section 4.2).\nWe also conduct experiments to compare our\nmodified EWC and original EWC on the dialogue\ntask. The results are presented in Table 7. In\nterms of training time and memory consumption,\nour modified EWC consistently outperforms the\noriginal EWC. Notably, it can reduce about 23%\nof training time and 17% of memory consump-\ntion.\nIt demonstrates that our modified EWC\ncan be efficiently implemented in alignment train-\ning. Furthermore, it shows that our HBAT is ca-\npable of handling larger mini-batches, large-scale\ndatasets, larger-sized models, and longer target\ngeneration sequences with identical settings on\nresource-constrained devices. In terms of response\nquality, our modified EWC achieves a matched\nGPT-4 win rate compared to the original EWC.\nSummarization Task\nCoherence\nThe coherence measures the quality of all sentences collectively, to the fit together and sound naturally. Consider the quality of the\nsummary as a whole.\nRubric:\nScore of 1: The summary is impossible to understand.\nScore of 4: The summary has mistakes or confusing phrasing that make it a bit hard to understand.\nScore of 7: The summary is perfectly clear.\nAccuracy\nThe accuracy measures whether the facts in the summary are consistent with the facts in the original post. Consider whether the summary\ndoes reproduce all facts accurately and does not make up untrue information.\nRubric:\nScore of 1: The summary is completely wrong, made up, or exactly contradicts what is written in the post.\nScore of 4: The summary says at least one substantial thing that is not mentioned in the post, or that contradicts something in the post.\nScore of 7: The summary has no incorrect statements or misleading implications.\nCoverage\nThis coverage measure whether the summary contains all the important information in the post.\nRubric:\nScore of 1: The summary contains no information relevant to the post.\nScore of 4: The summary is missing at least one important piece of information required to understand the situation.\nScore of 7: The summary covers all of the important information required to understand the situation.\nOverall Score\nThis score measures how good this summary is overall.\nRubric:\nScore of 1: The summary is terrible.\nScore of 4: The summary is an okay representation of the post, but could be significantly improved.\nScore of 7: The summary is an excellent representation of the post.\nDialogue Task\nFluency\nThis fluency measures whether the response is easy to read.\nRubric:\nScore of 1: The content of the response is unreadable and completely incoherent.\nScore of 4: The content of the response is readable but fails to convey all the information described.\nScore of 7: The content of the response is readable and enables comprehension of all the included meanings, albeit with minor grammatical\nissues.\nAccuracy\nThe accuracy measures whether the response contains is error-free.\nRubric:\nScore of 1: The content of the response is entirely nonsensical.\nScore of 4: The content of the response contains at least one misunderstanding that is crucial.\nScore of 7: The content of the response does not contain any misunderstandings.\nToxicity\nThe toxicity measures whether the response is harmful, offensive, or potentially damaging to the reader.\nRubric:\nScore of 1: The content of the response is toxic.\nScore of 4: The content of the response is slightly toxic or perceived as toxic by some.\nScore of 7: The content of the response is non-toxic.\nHelpfulness\nThe helpfulness measures whether the response provides useful information or solutions that address the reader’s query.\nRubric:\nScore of 1: The content of the response is not helpful.\nScore of 4: The content of the response solves part of the problem, but some issues remain unresolved.\nScore of 7: The content of the response fully addresses the problem.\nOverall Score\nThis score measures how good this response is overall.\nRubric:\nScore of 1: The response is extremely poor.\nScore of 4: The response is not perfect and has room for improvement.\nScore of 7: The content of the response is satisfactory.\nTable 8: Our human evaluation rubrics for the summarization and dialogue tasks. Note that the rubrics for the\nsummarization task are adopted from Stiennon et al. (2020).\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Hybrid Alignment Training for Large Language Models.pdf"}
{"title":"Relational Programming with Foundation Models","authors":"Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker, Rajeev Alur, Mayur Naik","summary":"Foundation models have vast potential to enable diverse AI applications. The\npowerful yet incomplete nature of these models has spurred a wide range of\nmechanisms to augment them with capabilities such as in-context learning,\ninformation retrieval, and code interpreting. We propose Vieira, a declarative\nframework that unifies these mechanisms in a general solution for programming\nwith foundation models. Vieira follows a probabilistic relational paradigm and\ntreats foundation models as stateless functions with relational inputs and\noutputs. It supports neuro-symbolic applications by enabling the seamless\ncombination of such models with logic programs, as well as complex, multi-modal\napplications by streamlining the composition of diverse sub-models. We\nimplement Vieira by extending the Scallop compiler with a foreign interface\nthat supports foundation models as plugins. We implement plugins for 12\nfoundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9\nchallenging tasks that span language, vision, and structured and vector\ndatabases. Our evaluation shows that programs in Vieira are concise, can\nincorporate modern foundation models, and have comparable or better accuracy\nthan competitive baselines.","url":"http:\/\/arxiv.org\/abs\/2412.14515v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.14515v1","published":1734582405000,"comment":null,"pdf_text":"Relational Programming with Foundation Models\nZiyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker,\nRajeev Alur, Mayur Naik\nUniversity of Pennsylvania\nliby99@seas.upenn.edu, jianih@seas.upenn.edu, jasonhl@seas.upenn.edu, zhufelix@seas.upenn.edu,\nzhaoer@seas.upenn.edu, wdodds@sas.upenn.edu, neelay@seas.upenn.edu, alur@seas.upenn.edu, mhnaik@seas.upenn.edu\nAbstract\nFoundation models have vast potential to enable diverse AI\napplications. The powerful yet incomplete nature of these\nmodels has spurred a wide range of mechanisms to augment\nthem with capabilities such as in-context learning, informa-\ntion retrieval, and code interpreting. We propose VIEIRA,\na declarative framework that unifies these mechanisms in a\ngeneral solution for programming with foundation models.\nVIEIRA follows a probabilistic relational paradigm and treats\nfoundation models as stateless functions with relational in-\nputs and outputs. It supports neuro-symbolic applications by\nenabling the seamless combination of such models with logic\nprograms, as well as complex, multi-modal applications by\nstreamlining the composition of diverse sub-models. We im-\nplement VIEIRA by extending the SCALLOP compiler with a\nforeign interface that supports foundation models as plugins.\nWe implement plugins for 12 foundation models including\nGPT, CLIP, and SAM. We evaluate VIEIRA on 9 challeng-\ning tasks that span language, vision, and structured and vector\ndatabases. Our evaluation shows that programs in VIEIRA are\nconcise, can incorporate modern foundation models, and have\ncomparable or better accuracy than competitive baselines.\nIntroduction\nFoundation models are deep neural models that are trained\non a very large corpus of data and can be adapted to a wide\nrange of downstream tasks (Bommasani et al. 2021). Exem-\nplars of foundation models include language models (LMs)\nlike GPT (Bubeck et al. 2023), vision models like Segment\nAnything (Kirillov et al. 2023), and multi-modal models like\nCLIP (Radford et al. 2021). While foundation models are\na fundamental building block, they are inadequate for pro-\ngramming AI applications end-to-end. For example, LMs\nhallucinate and produce nonfactual claims or incorrect rea-\nsoning chains (McKenna et al. 2023). Furthermore, they lack\nthe ability to reliably incorporate structured data, which is\nthe dominant form of data in modern databases. Finally,\ncomposing different data modalities in custom or complex\npatterns remains an open problem, despite the advent of\nmulti-modal foundation models such as ViLT (Radford et al.\n2021) for visual question answering.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n@gpt(\"The height of {{x}} is {{y}} in meters\")\ntype height(bound x: String, y: i32)\n\/\/ Retrieving height of mountains\nrel mount_height(m, h) = mountain(m) and height(m, h)\n(a) Program P1: Extracting knowledge using GPT.\n@clip([\"cat\", \"dog\"])\ntype classify(bound img: Tensor, label: String)\n\/\/ Classify each image as cat or dog\nrel cat_or_dog(i, l) = image(i, m) and classify(m, l)\n(b) Program P2: Classifying images using CLIP.\nid\nimg\n1\n2\nid label\n1\ncat\n1\ndog\n2\ncat\n2\ndog\nEverest\nFuji\n8848\n3776\nmountain\nmount_height\nP2\nimage\ncat_or_dog\nname\nheight\nEverest\nFuji\nname\nK2\n8611\nK2\nMt.Blanc\n4808\nMt.Blanc\nP1\nprob\n0.02\n0.98\n0.99\n0.01\n(c) Example input-output relations of the programs.\nFigure 1: Programs in VIEIRA using foundation models.\nVarious mechanisms have been proposed to augment\nfoundation models to overcome these limitations. For exam-\nple, PAL (Gao et al. 2023), WebGPT (Nakano et al. 2021),\nand Toolformer (Schick et al. 2023) connect LMs with\nsearch engines and external tools, expanding their informa-\ntion retrieval and structural reasoning capabilities. LMQL\n(Beurer-Kellner, Fischer, and Vechev 2022) generalizes pure\ntext prompting in LMs to incorporate scripting. In the do-\nmain of computer vision (CV), neuro-symbolic visual rea-\nsoning frameworks such as VISPROG (Gupta and Kembhavi\n2022) compose diverse vision models with LMs and image\nprocessing subroutines. Despite these advances, program-\nmers lack a general solution that systematically incorporates\nthese methods into a single unified framework.\nIn this paper, we propose VIEIRA, a declarative frame-\nwork for programming with foundation models. VIEIRA fol-\nlows a (probabilistic) relational paradigm due to its theoret-\nical and practical versatility. Structured data is commonly\nstored in relational databases. Relations can also represent\nstructures such as scene graphs in vision and abstract syntax\ntrees in natural and formal languages. Moreover, extensions\narXiv:2412.14515v1  [cs.AI]  19 Dec 2024\nfor probabilistic and differentiable reasoning enable the in-\ntegration of relational programming with deep learning in\nneuro-symbolic frameworks like DeepProbLog (Manhaeve\net al. 2018) and SCALLOP (Li, Huang, and Naik 2023).\nIn VIEIRA, relations form the abstraction layer for inter-\nacting with foundation models. Our key insight is that foun-\ndation models are stateless functions with relational inputs\nand outputs. Fig. 1a shows a VIEIRA program which in-\nvokes GPT to extract the height of mountains whose names\nare specified in a structured table. Likewise, the program\nin Fig. 1b uses the image-text alignment model CLIP to\nclassify images into discrete labels such as cat and dog.\nFig. 1c shows relational input-output examples for the two\nprograms. Notice that the CLIP model also outputs proba-\nbilities that allow for probabilistic reasoning.\nWe implement VIEIRA by extending the SCALLOP com-\npiler with a foreign interface that supports foundation mod-\nels as plugins. We implement a customizable and extensi-\nble plugin library comprising 12 foundation models includ-\ning GPT, CLIP, and SAM. The resulting unified interface\nenables a wide spectrum of applications with benefits such\nas reduced hallucination, retrieval augmentation, and multi-\nmodal compositionality. We evaluate VIEIRA on 9 applica-\ntions that span natural language reasoning, information re-\ntrieval, visual question answering, image generation, and\nimage editing. For these applications, we explore diverse\nmethods for programming with foundation models, such as\nneuro-symbolic reasoning, combining semantic searching\nwith question answering, and modularly composing founda-\ntion models. We not only observe on-par or superior perfor-\nmance of our solutions compared to competitive baselines,\nbut also demonstrate their succinctness and ease-of-use.\nWe summarize our contributions as follows: (1) we in-\ntroduce a new approach based on relational programming\nto build applications on top of foundation models; (2) we\nimplement an extensible plugin library of 12 programmable\nfoundation models; and (3) we evaluate VIEIRA on 9\nbenchmark tasks, and demonstrate comparable or better no-\ntraining accuracy than neural-only as well as task-specific\nbaselines. Our framework, plugin library, and evaluations\nare open-source and available at https:\/\/github.com\/scallop-\nlang\/scallop.\nRelated Work\nNeuro-symbolic methods.\nThese methods combine the\ncomplementary benefits of neural learning and symbolic rea-\nsoning. They include domain-specific solutions (Yi et al.\n2018; Mao et al. 2019; Li et al. 2020; Wang et al. 2019;\nXu et al. 2022; Chen et al. 2020; Minervini et al. 2020)\nas well as general programming frameworks, such as Deep-\nProbLog (Manhaeve et al. 2018) and SCALLOP (Li, Huang,\nand Naik 2023). These methods typically concern training\nor fine-tuning neural models in the presence of logical pro-\ngrams, whereas we target building applications atop foun-\ndation models with zero-shot or few-shot examples. An-\nother recent work, the STAR framework (Rajasekharan et al.\n2023) also connects a language model (neural) to an an-\nswer set programming reasoner (symbolic). It is conceptu-\nally similar to VIEIRA but only focuses on natural language\nunderstanding and does not support probabilistic reasoning.\nFoundation models.\nThese models target different modal-\nities and domains (Touvron et al. 2023; OpenAI 2023; Rad-\nford et al. 2021; Kirillov et al. 2023; Radford et al. 2021).\nTheir reasoning capabilities continue to improve with larger\ncontext sizes (Ratner et al. 2023), smarter data selection\n(Adadi 2021), and the discovery of new prompting meth-\nods, such as chain-of-thought (Wei et al. 2023; Kojima et al.\n2022), self-consistency (Wang et al. 2023), and ReAct (Yao\net al. 2023). VIEIRA is orthogonal to these techniques and\nstands to further enhance the robustness and reliability of\nfoundation models in end-to-end AI applications.\nTools aiding language models.\nThere are many efforts\nthat seek to improve the reasoning abilities of language\nmodels (LMs) by incorporating external programs and\ntools (Gao et al. 2023; Schick et al. 2023; Nakano et al.\n2021; Davis and Aaronson 2023). For instance, AutoGPT\n(Richards 2023) and TaskMatrix.AI (Liang et al. 2023) al-\nlows black-box LMs to control symbolic reasoning by in-\nvoking commands or calling APIs. On the other hand, many\nworks attempt to extract structured information from LMs\nfor downstream tasks (Gupta and Kembhavi 2022; Beurer-\nKellner, Fischer, and Vechev 2022). VIEIRA unifies these\ntwo strategies for augmenting model capabilities, and ex-\ntends them into a glue language for composing multi-modal\nfoundation models.\nLanguage\nVIEIRA employs a declarative logic programming language\nbased on Datalog (Abiteboul, Hull, and Vianu 1994). In this\nsection, we present the core language and its foreign inter-\nface for incorporating diverse foundation models.\nCore Language\nRelations and data types.\nThe fundamental data type\nin VIEIRA is set-valued relations comprising tuples of\nstatically-typed primitive values. Besides the standard prim-\nitive types such as integers (e.g. i32) and string (String),\nVIEIRA introduces two additional types for seamless inte-\ngration of foundation models: Tensor and Algebraic Data\nTypes (ADTs). For example, we can declare a relation named\nimage to store tuples of image IDs and image Tensors:\ntype image(img_id: i32, img: Tensor)\nThe contents of this relation can be specified via a set of\ntuples using the built-in foreign function $load_image:\nrel image = {(0, $load_image(\"cat.png\")), ...}\nADTs in VIEIRA enable the specification of domain spe-\ncific languages (DSLs) to bridge structured and unstructured\ndata. For example, the following DSL for visual question an-\nswering (VQA) describes queries to retrieve scene objects,\ncount objects, and check the existence of objects:\ntype Query = Scene() | Filter(Query, String)\n| Count(Query) | Exists(Query) | ...\n\/\/ How many balls are there?\nconst MY_QUERY = Count(Filter(Scene(), \"ball\"))\nLogical reasoning.\nBeing based on Datalog, VIEIRA sup-\nports defining Horn rules, thereby allowing logical reason-\ning constructs such as conjunction, disjunction, recursion,\nstratified negation, and aggregation. Recursion is particu-\nlarly useful for inductively defining the semantics of a DSL.\nFor example, a (partial) semantics for the above DSL is de-\nfined as follows, where eval_o and eval_n are recursively\ndefined to evaluate objects and numbers, respectively:\n\/\/ Scene returns all objects\nrel eval_o(e, o) = case e is Scene() and obj(o)\n\/\/ Filter applies filter using attributes\nrel eval_o(e, o) = case e is Filter(f, a)\nand eval_o(f, o) and attr(o, a)\n\/\/ Count returns the number of evaluated objects\nrel eval_n(e, n) = n := count(o: eval_o(e1, o)\nwhere e1: case e is Count(e1))\n... \/\/ other cases of ‘e’\nNote that the case-is operator matches patterns of the ADT\nand the count aggregator counts the number of entities.\nWhen combined with foundation models, principled reason-\ning semantics in this style can compensate for individual\nfoundation models’ lack of reasoning capability.\nProbabilistic soft logic.\nTuples can be tagged with proba-\nbilities. The example below shows hard-coded probabilities,\nsuggesting that the entity is more likely a dog than a cat:\nrel animal = {0.1::(1,\"cat\"), 0.9::(1,\"dog\")}\nSoft-logic operations produce probabilities as well. For in-\nstance, the soft-eq operator (˜=) on Tensors derives cosine-\nsimilarity between tensors, enabling features like soft-join\nand applications like semantic search. In the following ex-\nample, we compute similarity scores between distinct docu-\nments by performing soft-join on their embeddings:\ntype doc(id: i32, embed: Tensor) \/\/ embed docs\nrel sim(i, j) = doc(i, v) and doc(j, v) and i!=j\n\/\/ equiv: sim(i, j) = doc(i, v1) and doc(j, v2)\nand i!=j and v1~=v2\nNotice that in the above rule, a join on a tensor value v is de-\nsugared into a soft-eq on two individual variables (denoted\nv1 and v2). Internally, with the provenance framework pro-\nvided by SCALLOP (Li, Huang, and Naik 2023), we use the\ntop-k-proofs semiring (Huang et al. 2021) for scalable prob-\nabilistic reasoning, thus enabling features such as ranking\nand uncertainty estimation.\nForeign Interface\nIn order to incorporate foundation models, we design a\nforeign interface with two main programming constructs,\ncalled foreign predicate and foreign attribute. They can be\ndefined externally in languages like Python and imported\ninto VIEIRA for application.\nForeign Predicate (FP).\nForeign predicates can be used in\nrules just like other relations. However, instead of grounding\nrelational facts from a table, FPs ground facts by invoking\nexternal functions. The syntax for defining FPs is as follows:\nextern type PRED([bound|free]? ARG: TYPE, ...)\nIn addition to the type, each argument is specified either as\na bounded argument (using the keyword bound) or a free\n@foreign_attribute\ndef clip(pred: Predicate, labels: List[str]):\n# Sanity checks for predicate and labels...\nassert pred.args[0].ty == Tensor and ...\n@foreign_predicate(name=pred.name)\ndef run_clip(img: Tensor) -> Facts[str]:\n# Invoke CLIP to classify image into labels\nprobs = clip_model(img, labels)\n# Each result is tagged by a probability\nfor (prob, label) in zip(probs, labels):\nyield (prob, (label,)) # prob::(label,)\nreturn run_clip\nFigure 2: Snippet of Python implementation of the foreign\nattribute clip which uses the CLIP model for image classi-\nfication. Notice that the FA clip returns the FP run_clip.\nargument (using free or omitted for brevity). Semantically,\nFPs are functions that take in a tuple of bounded arguments\nand return a list of tuples of free arguments. The runtime\nof VIEIRA performs memoization on FP results to avoid re-\ndundant computation. Optionally, FPs can tag a probability\nto each returned tuple for further probabilistic reasoning.\nForeign Attribute (FA).\nIn VIEIRA, attributes can be used\nto decorate declarations of predicates. They are higher-order\nfunctions that take in the provided arguments and the dec-\norated predicate to return a new predicate. The syntax for\nusing an attribute to decorate a predicate is:\n@ATTR(POS_ARG, ..., KEY=KW_ARG, ...)\ntype PRED([bound|free]? ARG: TYPE, ...)\nThe attribute is applied prior to the compilation of VIEIRA\nprograms. For interfacing with foundation models, the po-\nsitional and keyword arguments are particularly helpful in\nconfiguring the underlying model, hiding low-level details.\nFig. 2 illustrates one succinct implementation of the FA that\nenables the use of the CLIP model shown in Fig. 1b.\nFoundation Models\nVIEIRA provides an extensible plugin framework that adapts\nto the evolving landscape of foundation models. In this\nwork, we have implemented 7 plugins, covering 12 foun-\ndation models, all through the foreign interface. Our design\nprinciple for the interface is three-fold: simplicity, config-\nurability, and compositionality. In this section, we present\nseveral representative predicates and attributes which sub-\nstantially support the applicability of VIEIRA to diverse ma-\nchine learning tasks.\nText completion.\nIn VIEIRA, language models like GPT\n(OpenAI 2023) and LLaMA (Touvron et al. 2023) can be\nused as basic foreign predicates for text completion:\nextern type gpt(bound p: String, a: String)\nrel ans(a) = gpt(\"population of NY is\", a)\nIn this case, gpt is an arity-2 FP that takes in a String\nas the prompt and produces a String as the response. It\nuses the model gpt-3.5-turbo by default. To make the\ninterface more relational and structural, we provide an FA:\n@gpt(\"the population of {{loc}} is {{num}}\",\nexamples=[(\"NY\", 8468000), ...])\ntype population(bound loc: String, num: u32)\nHere, we declare a relation named population which pro-\nduces a population number (num) given a location (loc) as\ninput. Notice that structured few-shot examples are provided\nthrough the argument examples.\nSemantic parsing.\nOne can directly configure language\nmodels to perform semantic parsing. For instance, the se-\nmantic parser for the simple Query DSL (partially defined\nin the Language section) can be declared as follows:\n@gpt_semantic_parse(\n\"Please semantically parse questions...\",\nexamples=[(\"How many red things are there?\",\n\"Count(Filter(Scene(), ’red’))\"), ...])\ntype parse_query(bound x: String, y: Query)\nInternally, the language model is expected to generate a fully\nstructured Query in its string form. Then, VIEIRA attempts\nto parse the string to construct actual ADT values. In prac-\ntice, the success of semantic parsing depends heavily on\nthe design of the DSL, involving factors like intuitiveness\n(e.g., names and arguments of ADT variants) and complex-\nity (e.g., number of possible ADT variants).\nRelational data extraction.\nStructural relational knowl-\nedge available in free-form textual data can be extracted\nby language models. We introduce a foreign attribute\n@gpt_extract_relation for this purpose. For instance,\nthe following declared predicate takes in a context and pro-\nduces (subject, object, relation) triplets:\n@gpt_extract_relation(\n\"Extract the implied kinship relations\",\nexamples=[(\"Alice and her son Bob went to...\",\n[(\"alice\", \"bob\", \"son\"), ...])])\ntype extract_kinship(bound ctx: String,\nsub: String, obj: String, rela: String)\nThis attribute differs from the text completion attribute in\nthat it can extract an arbitrary number of facts. The under-\nlying implementation prompts LMs to respond with JSON-\nformatted strings, allowing structured facts to be parsed.\nLanguage models for textual embedding.\nTextual em-\nbeddings are useful in performing tasks such as information\nretrieval. The following example declares an FP encapsulat-\ning a cross-encoder (Nogueira and Cho 2019):\n@cross_encoder(\"nli-deberta-v3-xsmall\")\ntype enc(bound input: String, embed: Tensor)\nrel sim() = enc(\"cat\", e) and enc(\"neko\", e)\nIn the last line, we compute the cosine-similarity of the en-\ncoded embeddings using a soft-join on the variable e. As\na result, we obtain a probabilistic fact like 0.9::sim()\nwhose probability encodes the cosine-similarity between the\ntextual embeddings of \"cat\" and \"neko\".\nImage classification models.\nImage-text alignment mod-\nels, such as CLIP (Radford et al. 2021), can naturally be\nused as zero-shot image classification models. Fig. 1b shows\nan example usage of the @clip attribute. We also note that\ndynamically-generated classification labels can be provided\nto CLIP via a bounded argument in the predicate.\nImage segmentation models.\nOWL-ViT (Minderer et al.\n2022), Segment Anything Model (SAM) (Kirillov et al.\n2023), and DSFD (Li et al. 2018) are included in VIEIRA as\nimage segmentation (IS) and object localization (LOC)\nmodels. IS and LOC models can provide many outputs, such\nas bounding boxes, classified labels, masks, and cropped im-\nages. For instance, the OWL-ViT model can be used and\nconfigured as follows:\n@owl_vit([\"human face\", \"rocket\"])\ntype find_obj(bound img: Tensor,\nid: u32, label: String, cropped_image: Tensor)\nHere, the find_obj predicate takes in an image, and finds\nimage segments containing “human face” or “rocket”. Ac-\ncording to the names of the arguments, the model extracts\n3 values per segment: ID, label, and cropped image. Note\nthat each produced fact will be associated with a probability,\nrepresenting the confidence from the model.\nImage generation models.\nVisual generative models such\nas Stable Diffusion (Rombach et al. 2022) and DALL-\nE (Ramesh et al. 2021) can be regarded as relations as\nwell. The following example shows the declaration of\nthe gen_image predicate, which encapsulates a diffusion\nmodel:\n@stable_diffusion(\"stable-diffusion-v1-4\")\ntype gen_image(bound txt: String, img: Tensor)\nAs can be seen from the signature, it takes in a String text\nas input and produces a Tensor image as output. Optional\narguments such as the desired image resolution and the num-\nber of inference steps can be supplied to dictate the granu-\nlarity of the generated image.\nTasks and Solutions\nWe apply VIEIRA to solve 9 benchmark tasks depicted in\nFig. 3. Table 1 summarizes the datasets, evaluation metrics,\nand the foundation models used in our solutions. We elabo-\nrate upon the evaluation settings and our solutions below.\nDate reasoning (DR).\nIn this task adapted from BIG-\nbench (Srivastava et al. 2023), the model is given a context\nand asked to compute a date. The questions test the model’s\ntemporal and numerical reasoning skills, as well as its grasp\nof common knowledge. Unlike BIG-bench where multiple-\nchoice answers are given, we require the model to directly\nproduce its answer in MM\/DD\/YYYY form.\nOur solution leverages GPT-4 (5-shot1) for extracting 3\nrelations: mentioned dates, duration between date labels, and\nthe target date label. From here, our relational program iter-\nates through durations to compute dates for all date labels.\nLastly, the date of the target label is returned as the output.\nTracking shuffled objects (TSO).\nIn this task from BIG-\nbench, a textual description of pairwise object swaps among\npeople is given, and the model needs to track and derive\nwhich object is in a specified person’s possession at the end.\n1In this work, k in “k-shot” means the number of examples pro-\nvided to the LM component within the full solution. Each example\nis a ground-truth input-output pair for the LM.\nImage Generation and Editing\nA bowl full of apples\nReplace the bowl with\nother containers\nReplace the apple with\nother fruits\nA plate full of apples\nA plate full of oranges\nInput\nPrompts\nOutput\nImages\nDate Reasoning\nMay 6, 1992 is like yesterday to Jane, but that is\nactually ten years ago. What is the date one week\nfrom today in MM\/DD\/YYYY?\n05\/13\/2002\nQuestion:\nAnswer:\nTracking Shuffled Objects \nAlice has an orange ball, Bob has a white ball, and\nClaire has a blue ball. Alice and Bob swap balls.\nThen, Bob and Claire swap balls. Alice has the __.\nwhite ball\nQuestion:\nAnswer:\nKinship Reasoning \nRich's daughter Kelly made dinner for her sister\nKim. Dorothy went to her brother Rich's birthday\nparty. Anne went shopping with her sister Kim. How\nis Dorothy related to Anne?\nniece\nQuestion:\nAnswer:\nQA\nCompositional VQA\nProduct Search \nMath Reasoning\nImage Editing\nDocuments:\nProducts:\nTag \"microsoft ceos.jpg\"\nInstruction:\nAnswer:\nWhich team does the player named 2015 Diamond\nHead Classic’s MVP play for?\nQuestion:\nSacramento Kings\nlawnmower tires without rims\nQuery:\nProduct Ranking: 1st: #2, 2nd: #6, 3rd: #4, ...\nQuestion:\nAlice is required to submit a 15-page paper. She finished\nwriting 1\/3 of the paper. How many pages are left to write?\nAnswer: 10\n                       Hide Walter Thurnherr\nwith smiling_face_with_halo and Alain\nBerset with crying_cat.\nIs the tray on top of the table\nblack or light brown?\nHow many objects are\nred in this image?\nObj Tagging\nQuestion:\nlight brown\nAnswer:\nQuestion:\nAnswer:\n3\nGPT\nLostAlone\nwere a\nBritish rock\nband ...\nThe 2015\nDiamond\nHead\nClassic was\n...\nSteven\nBattelle,\nAlan\nWilliamson,\nand ...\nGuster is\nan\nAmerican\nalternative\nrock band\n....\nFounding\nmembers\nAdam\nGardner,\nRyan\nMiller,...\nChavano\nRainier Buddy\nHield is a\nBahamian ...\nBrian\nRosenworcel\nbegan..\nSeveral\ncurrent and\nformer\nmembers of\nthe  ...\nDavid Gene\nParker,\nnicknamed\n”The Cobra” ...\nAn American\nformer player\nin Major\nLeague\nBaseball...\n...\nInput Image\nEdited Image\nInstruction:\n1\nGPT\nGPT\nGPT\nGPT-Enc\nGPT\nCross-Enc\nGPT\nGPT\nViLT\nOWL-ViT\nCLIP\nRamPro 10\"\nAll Purpose\nUtility Air\nTires\/Wheel\n2\n3\n4\n5\n(Set of 2)\n15x6.00-6\nHusqvarna\n\/Poulan Tire ...\nMaxAuto 2-\nPack\n13x5.00-6\n2PLY Turf\nNEIKO\n20601A 14.5\ninch  Steel\nTire Spoon ...\n2PK\n13x5.00-6\n13x5.00x6\n13x5x6\n13x5-6 ...\nBIG-bench\nBIG-bench\nCLUTRR\nHotpotQA\nAmazon ESCI\nGSM8K\nGQA\nCLEVR\nVQAR\nInput Image\nTagged Image\nIGP20\nOFCP GPT CLIP DSFD\nOFCP GPT CLIP DSFD\nGPT\nStable-Diffusion\nFigure 3: Benchmark tasks. The top of each box lists the dataset(s) and the foundation models used in our solutions.\nThere are three difficulty levels depending on the number of\nobjects to track, denoted by n ∈{3, 5, 7}.\nOur solution for tracking shuffled objects relies on GPT-4\n(1-shot) to extract 3 relations: initial possessions, swaps, and\nthe target person whose final possessed object is expected\nas the answer. Our reasoning program iterates through all\nthe swaps starting from the initial state and retrieves the last\npossessed object associated with the target.\nKinship reasoning (KR).\nCLUTRR (Sinha et al. 2019) is\na kinship reasoning dataset of stories which indicate the kin-\nship between characters, and requires the model to infer the\nrelationship between two specified characters. The questions\nhave different difficulty levels based on the length of the rea-\nsoning chain, denoted by k ∈{2 . . . 10}.\nOur solution for kinship reasoning invokes GPT-4 (2-\nshot) to extract the kinship graph from the context. We also\nprovide an external common-sense knowledge base for rules\nlike “mother’s mother is grandmother”. Our program then\nuses the rules to derive other kinship relations. Lastly, we\nretrieve the kinship between the specified pair of people.\nMath reasoning (MR).\nThis task is drawn from the\nGSM8K dataset of arithmetic word problems (Cobbe et al.\n2021). The questions involve grade school math word prob-\nlems created by human problem writers, and the model is\nasked to produce a number as the result. Since the output\ncan be fractional, we allow a small delta when comparing\nthe derived result with the ground truth.\nOur solution to this task prompts GPT-4 (2-shot) to pro-\nduce step-by-step expressions, which can contain constants,\nvariables, and simple arithmetic operations. We evaluate all\nthe expressions through a DSL, and the result associated\nwith the goal variable is returned. By focusing the LM’s re-\nsponsibility solely on semantic parsing, our relational pro-\ngram can then achieve faithful numerical computation via\nDSL evaluation.\nQuestion answering with information retrieval (QA).\nWe choose HotpotQA (Yang et al. 2018), a Wikipedia-based\nquestion answering (QA) dataset under the “distractor” set-\nting. Here, the model takes in 2 parts of inputs: 1) a question,\nand 2) 10 Wikipedia paragraphs as the context for answering\nthe question. Among the 10 Wikipedia pages, at most 2 are\nrelevant to the answer, while the others are distractors.\nOur solution is an adaptation of FE2H (Li, Lei, and Yang\n2022), which is a 2-stage procedure. First, we turn the 10\ndocuments into a vector database by embedding each docu-\nment. We then use the embedding of the question to retrieve\nthe 2 most related documents, which are then fed to a lan-\nguage model to do QA. In this case, the QA model does not\nhave to process all 10 documents, leading to less distraction.\nProduct search (PS).\nWe use Amazon’s ESCI Product\nSearch dataset (Reddy et al. 2022). The model is provided\nwith a natural language (NL) query and a list of products (23\nTask\nDataset\n#Test\nSamples\nMetric\nFoundation\nModels Used\nDR\nDR\n369\nEM\nGPT-4\nTSO\nTSO\n150\nEM\nGPT-4\nKR\nCLUTRR\n1146\nEM\nGPT-4\nMR\nGSM8K\n1319\nEM\nGPT-4\nQA\nHotpot QA\n1000\nEM\nGPT-4\nada-002\nPS\nAmazon\nESCI\n1000\nnDCG\nGPT-4\nada-002\nVQA\nCLEVR\n480\nRecall@1\nRecall@3\nGPT-4\nOWL-ViT\nGQA\n500\nVilT\nCLIP\nVOT\nVQAR\n100\nMI\nOWL-ViT\nVilT\nGPT-4\nOFCP\n50\nDSFD\nCLIP\nIGE\nOFCP\n50\nMI\nDFSD\nCLIP\nIGP20\n20\nGPT-4\nDiffusion\nTable 1: Characteristics of benchmark tasks including the\ndataset used, its size, and evaluation metrics. Metrics include\nexact match (EM), normalized discounted cumulative gain\n(nDCG), and manual inspection (MI). We also denote the\nfoundation models used in our solution for each task.\nproducts on average). The goal is to rank the products that\nbest match the query. In the dataset, for each pair of query\nand product, a label among E (exact match), S (substitute),\nC (complementary), and I (irrelevant) is provided. The met-\nric we use to evaluate the performance is nDCG. The gains\nare set to be 1.0 for E, 0.1 for S, 0.01 for C, and 0.0 for I.\nOne challenge of this dataset is that many queries contain\nnegative statements. For example, in the query “#1 treadmill\nwithout remote”, the “remote” is undesirable. Therefore, in-\nstead of computing the embedding of the full query, we de-\ncompose the query into positive and negative parts. We then\nperform semantic search by maximizing the similarity of the\npositive part while minimizing that of the negative part.\nCompositional visual question answering (VQA).\nWe\nchoose two compositional VQA datasets, GQA (Hudson\nand Manning 2019) and CLEVR (Johnson et al. 2016).\nIn this task, the model is given an image and a question,\nand needs to answer the question. For GQA, the majority\nof questions expect yes\/no answers, while CLEVR’s ques-\ntions demand features like counting and spatial reasoning.\nWe uniformly sample 500 and 480 examples from GQA\nand CLEVR datasets respectively. Following VQA conven-\ntions (Kim, Son, and Kim 2021), we use Recall@k where\nk ∈{1, 3} as the evaluation metrics.\nOur solution for GQA is an adaptation of VISPROG\n(Gupta and Kembhavi 2022). We create a DSL for invok-\ning vision modules such as ViLT and OWL-ViT, and use\nGPT-4 for converting questions into programs in this DSL.\nOur solution for CLEVR is similar, directly replicating the\nDSL provided by the original work. OWL-ViT and CLIP are\nused to detect objects and infer attributes, while the spatial\nrelations are directly computed using the bounding box data.\nVisual object tagging (VOT).\nWe evaluate on two\ndatasets, VQAR (Huang et al. 2021) and OFCP. For VQAR,\nthe model is given an image and a programmatic query, and\nis asked to produce bounding boxes of the queried objects\nin the image. Our solution composes a relational knowledge\nbase, defining entity names and relationships, with object re-\ntrieval (OWL-ViT) and visual QA (ViLT) models.\nOnline Faces of Celebrities and Politicians (OFCP) is a\nself-curated dataset of images from Wikimedia Commons\namong other sources. For this dataset, the model is given\nan image with a descriptive NL filename, and needs to de-\ntect faces relevant to the description and tag them with their\nnames. Our solution obtains a set of possible names from\nGPT-4 and candidate faces from DSFD. These are provided\nto CLIP for object classification, after which probabilistic\nreasoning filters the most relevant face-name pairs.\nLanguage-guided image generation and editing (IGE).\nWe adopt the task of image editing from (Gupta and Kem-\nbhavi 2022). In this task, the instruction for image editing\nis provided through NL, and can invoke operations such as\nblurring background, popping color, and overlaying emojis.\nDue to the absence of an existing dataset, we repurpose the\nOFCP dataset by introducing 50 NL image editing prompts.\nOur solution for this task is centered around a DSL for image\nediting. We incorporate GPT-4 for semantic parsing, DSFD\nfor face detection, and CLIP for entity classification. Mod-\nules for image editing operations are implemented as indi-\nvidual foreign functions.\nFor free-form generation and editing of images, we cu-\nrate IGP20, a set of 20 prompts for image generation and\nediting. Instead of using the full prompt, we employ an LM\nto decompose complex NL instructions into simpler steps.\nWe define a DSL with high-level operators such as generate,\nreweight, refine, replace, and negate. We use a combination\nof GPT-4, Prompt-to-Prompt (Hertz et al. 2022), and diffu-\nsion model (Rombach et al. 2022) to implement the seman-\ntics of our DSL. We highlight our capability of grounding\npositive terms from negative phrases, which enables han-\ndling prompts like “replace apple with other fruits” (Fig. 3).\nExperiments and Analysis\nWe aim to answer the following research questions:\nRQ1. Is VIEIRA programmable enough to be applicable to\na diverse range of applications with minimal effort?\nRQ2. How do solutions using VIEIRA compare to other\nbaseline methods in the no-training setting?\nRQ1: Programmability\nWhile a user study for VIEIRA’s programmability is out of\nscope in this paper, we qualitatively evaluate its programma-\nbility on three aspects. First, we summarize the lines-of-code\n(LoC) for each of our solutions in Table 2. The programs\nDataset\nLoC Prompt\nLoC\nDataset\nLoC Prompt\nLoC\nDR\n69\n48\nCLEVR\n178\n45\nTSO\n34\n16\nGQA\n82\n36\nCLUTRR\n61\n45\nVQAR\n53\n11\nGSM8K\n47\n28\nOFCP (VOT)\n33\n2\nHotpotQA 47\n24\nOFCP (IGE)\n117\n44\nESCI\n32\n7\nIGP20\n50\n12\nTable 2: The lines-of-code (LoC) numbers of our solutions\nfor each dataset. The LoC includes empty lines, comments,\nnatural language prompts, and DSL definitions. We note\nspecifically the LoC of prompts in the table.\nMethod\nDR\nTSO\nCLUTRR\nGSM8K\nGPT-4\n71.00\n(0-shot)\n30.00\n(0-shot)\n43.10\n(3-shot)\n87.10\n(0-shot)\nGPT-4 (CoT)\n87.26\n(0-shot)\n84.00\n(0-shot)\n24.17\n(3-shot)\n92.00\n(5-shot)\nOurs\n92.41\n100.00\n72.50\n90.60\nTable 3: The performance on the natural language reasoning\ndatasets. Numbers are in percentage (%).\nHotpotQA\nAmazon ESCI\nMethod\nFine-tuned\nEM\nMethod\nFine-tuned\nnDCG\nC2FM\n✓\n72.07%\nBERT\n✓\n0.830\nFE2H\n✓\n71.89% CE-MPNet\n✓\n0.857\n—\n—\n—\nMIPS\n✗\n0.797\nOurs\n✗\n67.3%\nOurs\n✗\n0.798\nTable 4: The performance on the HotpotQA and Amazon\nESCI. We also include performance numbers from methods\nwhich are fine-tuned on the corresponding dataset.\nare concise, as most are under 100 lines. Notably, natural\nlanguage prompts (including few-shot examples) take up a\nsignificant portion of each solution. Secondly, 8 out of 10\nsolutions are coded by undergraduate students with no back-\nground in logic and relational programming, providing fur-\nther evidence of VIEIRA’s user-friendliness. Last but not\nleast, our solutions are interpretable and thus offer debug-\ngability. Specifically, all the intermediate relations are avail-\nable for inspection, allowing systematic error analysis.\nRQ2: Baselines and Comparisons\nWe compare the performance of our solutions to existing\nbaselines under the no-training setting. In particular, our so-\nlutions achieve better performance than comparable base-\nlines on 6 out of 8 studied datasets with baselines. Below,\nwe classify the tasks into 4 categories and discuss the re-\nspective performance and comparisons.\nNatural language reasoning.\nFor the tasks of DR, TSO,\nCLUTRR, and GSM8K, we pick a generic baseline of GPT-\nDR (Date Reasoning)\nToday's meeting is rescheduled to 11 am tomorrow, 10\/16\/1924.\nWhat is the date one year ago from today in MM\/DD\/YYYY?\nQuestion\nVIEIRA\nExtracted\nRelations\nVIEIRA\nAnswer\n10\/15\/1923\nGPT-4\nResponse\nGPT-4\nAnswer\nLet's think step by step. First, we need to subtract one year from\nthe given date, 10\/16\/1924. When we do this, we get 10\/16\/1923.\n10\/16\/1923\nCLUTRR (Kinship Reasoning)\nMyrna and her husband Christopher went on a cruise. They had\na wonderful time. Christopher and his daughter Lucille took a\nday off school to go to the zoo. Who is Lucille to Myrna? \nContext &\nQuestion\nVIEIRA\nExtracted\nRelations\nVIEIRA\nAnswer\ndaughter\nGPT-4\nResponse\nGPT-4\nAnswer\nLucille is Myrna's stepdaughter. In the given scenario,\nChristopher is Myrna's husband, and Christopher is also the\nfather of Lucille. This makes Lucille the daughter of Myrna's\nhusband, which means she is Myrna's stepdaughter.\nstepdaughter\nGSM8K (Math Reasoning)\nIt takes 3 kangaroos traveling at the same speed a total of 18\nhours to travel across a highway. How many hours will it take 4\nturtles,  each traveling at half the speed of a kangaroo, to do so?\nQuestion\nVIEIRA\nExtracted\nRelations\nVIEIRA\nAnswer\n48\nGPT-4\nResponse\nGPT-4\nAnswer\nLet's think step by step. First, we know that the total time it takes for 3 kangaroos\nto travel across the highway is 18 hours. This means that one kangaroo takes 18\nhours to travel across the highway. Then, we know that a turtle travels at half the\nspeed of a kangaroo, which means it takes twice as long for a turtle to travel the\nsame distance. So, one turtle would take 18 hours * 2 = 36 hours to travel across\nthe highway. However, we have four turtles traveling together at the same speed,\nso they would still take 36 hours to travel across the highway, because they are\ntraveling simultaneously.\n36\nFigure 4: Illustrative comparisons between our solution and\nGPT-4 (zero-shot CoT) on selected questions from DR,\nCLUTRR, and GSM8K datasets. We also include the ex-\ntracted relations used for subsequent reasoning.\n4 under zero-shot, few-shot, and chain-of-thought (CoT) set-\ntings. All our solutions also rely on GPT-4 (few-shot), but\nwe note that our shots only include extracted facts, and not\nthe final answer or any reasoning chains. The data in Ta-\n2\n4\n6\n8\n10\n25\n50\n75\nk, length of reasoning chain\nAccuracy (%)\nOurs\nGPT-4\nGPT-4 (CoT)\n(a) CLUTRR\n3\n5\n7\n25\n50\n75\n100\nn, number of objects\nAccuracy (%)\n(b) TSO\nFigure 5: Systematic generalizability comparisons on the\nCLUTRR and TSO datasets.\nMethod\nGQA\nCLEVR\nRecall@1\nRecall@3\nRecall@1\nRecall@3\nViLT-VQA\n0.049\n0.462\n0.241\n0.523\nPNP-VQA\n0.419\n—\n—\n—\nOurs\n0.579\n0.665\n0.463\n0.638\nTable 5: Quantitative results on the VQA datasets.\nble 3 indicates that our method can significantly enhance\nreasoning performance and reduce hallucination, exempli-\nfied by achieving a flawless 100% accuracy on the TSO\ndataset. Note that on GSM8K, our method scores slightly\nlower than the baseline; we conjecture that our solution de-\nmands more from GPT-4 itself to extract structured compu-\ntation steps. On CLUTRR, our solution even outperforms\nfCoT (Lyu et al. 2023), a special prompting technique with\nexternal tool use, by 0.6%. In Fig. 5 we illustrate the system-\natic generalizability of our methods. The performance of our\nsolutions remains relatively consistent even when the prob-\nlems become harder. We provide illustrative examples in\nFig. 4 showing comparisons between our method and GPT-4\n(zero-shot CoT).\nRetrieval augmentation and semantic search.\nFor the\nHotpotQA dataset, our solution is an adaptation of FE2H\n(Li, Lei, and Yang 2022), a retrieval-augmented question an-\nswering approach. As seen in Table 4, with no fine-tuning,\nour method scores only a few percentages lower than fine-\ntuned methods C2FM (Yin et al. 2022) and FE2H. For\nthe Amazon ESCI dataset, our solution performs seman-\ntic search for product ranking. While performing slightly\nlower than the fine-tuned methods (Reddy et al. 2022; Song\net al. 2020), our solution outperforms maximum inner prod-\nuct search (MIPS) based on GPT text encoder (text-\nembedding-ada-002).\nCompositional multi-modal reasoning.\nFor VQA, we\npick ViLT-VQA (Kim, Son, and Kim 2021) (a pre-trained\nfoundation model) and PNP-VQA (Tiong et al. 2022) (a\nzero-shot VQA method) as baselines. As shown in Table 5,\nour method significantly outperforms the baseline model on\nboth datasets. Compared to the neural-only baseline, our ap-\nproach that combines DSL and logical reasoning more ef-\nfectively handles intricate logical operations such as count-\ning and numerical comparisons. On GQA, out method out-\nOurs\nInstructPix2Pix\nOriginal\nInstruction: Replace the bowl with something\nelse, and change the apples to other fruits.\nFigure 6: Qualitative comparison of image editing. Com-\npared to InstructPix2Pix, our image editing method follows\nthe instructed edits better, as it successfully changed the\nbowl into plate and apples to oranges.\nMethod\nVisual Object Tagging\nImage Editing\nVQAR\nOFCP\nOFCP\nOurs\n67.61%\n60.82%\n74.00%\nTable 6: Quantitative results on object tagging and image\nediting tasks. We manually evaluate the tagged entities and\nthe edited images for semantic correctness rates.\nperforms previous zero-shot state-of-the-art, PNP-VQA, by\n0.16 (0.42 to 0.58). For object and face tagging, without\ntraining or fine-tuning, our method achieves 67.61% and\n60.82% semantic correctness rates (Table 6).\nImage generation and editing.\nFor image generation and\nediting, we apply our technique to the OFCP and IGP20\ndatasets. We rely on manual inspection for evaluating our\nperformance on the OFCP dataset, and we observe 37 cor-\nrectly edited images out of the 50 evaluated ones, resulting\nin a 74% semantic correctness rate (Table 6). For IGP20, we\nchoose as the baseline a diffusion model, InstructPix2Pix\n(Brooks, Holynski, and Efros 2023), which also combines\nGPT-3 with image editing. We show one example baseline\ncomparison illustrated in Figure 6.\nConclusion\nWe introduced VIEIRA, a declarative framework designed\nfor relational programming with foundation models. VIEIRA\nbrings together foundation models from diverse domains,\nproviding a unified interface for composition and the abil-\nity to perform probabilistic logical reasoning. This results in\nsolutions with comparable and often superior performance\nthan neural-based baselines. In the future, we aim to extend\nthe capabilities of VIEIRA beyond the current in-context\nlearning settings to weakly-supervised training and fine-\ntuning of foundation models in an end-to-end manner.\nAcknowledgements\nWe thank the anonymous reviewers for useful feedback.\nThis research was supported by NSF grant #2313010 and\nDARPA grant #FA8750-23-C-0080. Ziyang Li was sup-\nported by an Amazon Fellowship.\nReferences\nAbiteboul, S.; Hull, R.; and Vianu, V. 1994. Foundations of\nDatabases: The Logical Level. Pearson, 1st edition.\nAdadi, A. 2021. A survey on data-efficient algorithms in big\ndata era. Journal of Big Data, 8(1): 24.\nBeurer-Kellner, L.; Fischer, M.; and Vechev, M. 2022.\nPrompting Is Programming: A Query Language For Large\nLanguage Models. In PLDI.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R. B.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; and et al. 2021. On the Opportunities and\nRisks of Foundation Models. arXiv:2108.07258.\nBrooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct-\nPix2Pix: Learning to Follow Image Editing Instructions.\narXiv:2211.09800.\nBubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg,\nS.; et al. 2023. Sparks of artificial general intelligence: Early\nexperiments with GPT-4. arXiv:2303.12712.\nChen, X.; Liang, C.; Yu, A. W.; Zhou, D.; Song, D.; and\nLe, Q. V. 2020. Neural Symbolic Reader: Scalable Integra-\ntion of Distributed and Symbolic Representations for Read-\ning Comprehension. In ICLR.\nCobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\nHesse, C.; and Schulman, J. 2021.\nTraining Verifiers to\nSolve Math Word Problems. arXiv:2110.14168.\nDavis, E.; and Aaronson, S. 2023. Testing GPT-4 with Wol-\nfram Alpha and Code Interpreter plug-ins on math and sci-\nence problems. arXiv:2308.05713.\nGao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.;\nCallan, J.; and Neubig, G. 2023. PAL: Program-aided Lan-\nguage Models. arXiv:2211.10435.\nGupta, T.; and Kembhavi, A. 2022.\nVisual Program-\nming: Compositional visual reasoning without training.\narXiv:2211.11559.\nHertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch,\nY.; and Cohen-Or, D. 2022. Prompt-to-Prompt Image Edit-\ning with Cross Attention Control. arXiv:2208.01626.\nHuang, J.; Li, Z.; Chen, B.; Samel, K.; Naik, M.; Song,\nL.; and Si, X. 2021.\nScallop: From Probabilistic Deduc-\ntive Databases to Scalable Differentiable Reasoning.\nIn\nNeurIPS.\nHudson, D. A.; and Manning, C. D. 2019.\nGQA: a new\ndataset for compositional question answering over real-\nworld images. arXiv:1902.09506.\nJohnson, J.; Hariharan, B.; van der Maaten, L.; Fei-Fei, L.;\nZitnick, C. L.; and Girshick, R. B. 2016. CLEVR: A Diag-\nnostic Dataset for Compositional Language and Elementary\nVisual Reasoning. arXiv:1612.06890.\nKim, W.; Son, B.; and Kim, I. 2021.\nViLT: Vision-and-\nLanguage Transformer Without Convolution or Region Su-\npervision. arXiv:2102.03334.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-\nY.; et al. 2023. Segment Anything. arXiv:2304.02643.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,\nY. 2022. Large language models are zero-shot reasoners. In\nNeurIPS.\nLi, J.; Wang, Y.; Wang, C.; Tai, Y.; Qian, J.; Yang, J.; Wang,\nC.; Li, J.; and Huang, F. 2018. DSFD: Dual Shot Face De-\ntector. arXiv:1810.10220.\nLi, Q.; Huang, S.; Hong, Y.; Chen, Y.; Wu, Y. N.; and Zhu,\nS.-C. 2020.\nClosed Loop Neural-Symbolic Learning via\nIntegrating Neural Perception, Grammar Parsing, and Sym-\nbolic Reasoning. In ICML.\nLi, X.-Y.; Lei, W.-J.; and Yang, Y.-B. 2022. From Easy to\nHard: Two-stage Selector and Reader for Multi-hop Ques-\ntion Answering. arXiv:2205.11729.\nLi, Z.; Huang, J.; and Naik, M. 2023. Scallop: A Language\nfor Neurosymbolic Programming. In PLDI.\nLiang, Y.; Wu, C.; Song, T.; Wu, W.; Xia, Y.; Liu, Y.;\nOu, Y.; Lu, S.; Ji, L.; Mao, S.; Wang, Y.; Shou, L.; Gong,\nM.; and Duan, N. 2023. TaskMatrix.AI: Completing Tasks\nby Connecting Foundation Models with Millions of APIs.\narXiv:2303.16434.\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv:1907.11692.\nLyu, Q.; Havaldar, S.; Stein, A.; Zhang, L.; Rao, D.; Wong,\nE.; Apidianaki, M.; and Callison-Burch, C. 2023. Faithful\nChain-of-Thought Reasoning. arXiv:2301.13379.\nManhaeve, R.; Dumancic, S.; Kimmig, A.; Demeester, T.;\nand Raedt, L. D. 2018. DeepProbLog: Neural Probabilistic\nLogic Programming. arXiv:1805.10872.\nMao, J.; Gan, C.; Kohli, P.; Tenenbaum, J. B.; and Wu, J.\n2019. The Neuro-Symbolic Concept Learner: Interpreting\nScenes, Words, and Sentences From Natural Supervision.\narXiv:1904.12584.\nMcKenna, N.; Li, T.; Cheng, L.; Hosseini, M. J.; John-\nson, M.; and Steedman, M. 2023.\nSources of Hallu-\ncination by Large Language Models on Inference Tasks.\narXiv:2305.14552.\nMinderer, M.; Gritsenko, A.; Stone, A.; Neumann, M.;\nWeissenborn, D.; Dosovitskiy, A.; Mahendran, A.; Arnab,\nA.; Dehghani, M.; Shen, Z.; Wang, X.; Zhai, X.; Kipf, T.;\nand Houlsby, N. 2022. Simple Open-Vocabulary Object De-\ntection with Vision Transformers. arXiv:2205.06230.\nMinervini, P.; Riedel, S.; Stenetorp, P.; Grefenstette, E.; and\nRocktäschel, T. 2020.\nLearning Reasoning Strategies in\nEnd-to-End Differentiable Proving. In ICML.\nNakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.;\nKim, C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.;\nJiang, X.; Cobbe, K.; Eloundou, T.; Krueger, G.; Button, K.;\nKnight, M.; Chess, B.; and Schulman, J. 2021. WebGPT:\nBrowser-assisted question-answering with human feedback.\narXiv:2112.09332.\nNogueira, R.; and Cho, K. 2019. Passage Re-ranking with\nBERT. arXiv:1901.04085.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021.\nLearning Transfer-\nable Visual Models From Natural Language Supervision.\narXiv:2103.00020.\nRajasekharan, A.; Zeng, Y.; Padalkar, P.; and Gupta, G.\n2023. Reliable Natural Language Understanding with Large\nLanguage Models and Answer Set Programming. In Inter-\nnational Conference on Logic Programming.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\nto-image generation. In ICML.\nRatner, N.; Levine, Y.; Belinkov, Y.; Ram, O.; Magar, I.;\nAbend, O.; Karpas, E.; Shashua, A.; Leyton-Brown, K.; and\nShoham, Y. 2023. Parallel Context Windows for Large Lan-\nguage Models. In Proceedings of the ACL.\nReddy, C. K.; Màrquez, L.; Valero, F.; Rao, N.; Zaragoza,\nH.; Bandyopadhyay, S.; Biswas, A.; Xing, A.; and Sub-\nbian, K. 2022.\nShopping Queries Dataset: A Large-\nScale ESCI Benchmark for Improving Product Search.\narXiv:2206.06588.\nRichards, T. B. 2023.\nAutoGPT.\nhttps:\/\/github.com\/\nSignificant-Gravitas\/AutoGPT. Accessed: 2024-02-12.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-Resolution Image Synthesis With Latent\nDiffusion Models. In CVPR.\nSchick, T.; Dwivedi-Yu, J.; Dessì, R.; Raileanu, R.; Lomeli,\nM.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.\nToolformer: Language Models Can Teach Themselves to\nUse Tools. arXiv:2302.04761.\nSinha, K.; Sodhani, S.; Dong, J.; Pineau, J.; and Hamilton,\nW. L. 2019. CLUTRR: A Diagnostic Benchmark for Induc-\ntive Reasoning from Text. arXiv:1908.06177.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y. 2020. MP-\nNet: Masked and Permuted Pre-training for Language Un-\nderstanding. arXiv:2004.09297.\nSrivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid,\nA.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-\nAlonso, A.; and et al. 2023. Beyond the Imitation Game:\nQuantifying and extrapolating the capabilities of language\nmodels. arXiv:2206.04615.\nTiong, A. M. H.; Li, J.; Li, B.; Savarese, S.; and Hoi, S. C.\n2022. Plug-and-Play VQA: Zero-shot VQA by Conjoining\nLarge Pretrained Models with Zero Training. In Goldberg,\nY.; Kozareva, Z.; and Zhang, Y., eds., Findings of the ACL:\nEMNLP.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open Foundation and Fine-Tuned\nChat Models. arXiv:2307.09288.\nWang, P.-W.; Donti, P. L.; Wilder, B.; and Kolter, Z. 2019.\nSATNet: Bridging Deep Learning and Logical Reasoning\nUsing a Differentiable Satisfiability Solver. In ICML.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2023. Self-Consistency\nImproves Chain of Thought Reasoning in Language Models.\narXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023.\nChain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. arXiv:2201.11903.\nXu, Z.; Rawat, Y. S.; Wong, Y.; Kankanhalli, M.; and Shah,\nM. 2022.\nDon’t Pour Cereal into Coffee: Differentiable\nTemporal Logic for Temporal Action Segmentation.\nIn\nNeurIPS.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question answer-\ning. arXiv:1809.09600.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and\nActing in Language Models. arXiv:2210.03629.\nYi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenen-\nbaum, J. 2018.\nNeural-Symbolic VQA: Disentangling\nReasoning from Vision and Language Understanding.\nIn\nNeurIPS.\nYin, Z.; Wang, Y.; Wu, Y.; Yan, H.; Hu, X.; Zhang, X.; Cao,\nZ.; Huang, X.; and Qiu, X. 2022. Rethinking Label Smooth-\ning on Multi-hop Question Answering. arXiv:2212.09512.\nZheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu,\nZ.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;\nZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023.\nJudg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nFull VIEIRA Language\nWe present the full surface syntax of the VIEIRA language\nin Fig. 7.\nDetailed Example\nIn this section, we describe the VIEIRA program for one of\nour benchmark applications, CLEVR (Johnson et al. 2016).\nWe decompose this application into three sub-tasks: 1) ex-\ntracting a structured scene graph from the input image, 2) ex-\ntracting an executable query program from the input natural\nlanguage (NL) question, and 3) combining both to answer\nthe question based on the scene graph. We next describe how\nwe solve each of these sub-tasks. For illustration, we use the\nexample image and question shown in Fig. 8.\nImage to structured scene graph\nTo convert image to structured scene graph, we use two vi-\nsion models, namely OWL-ViT (Minderer et al. 2022) and\nCLIP (Radford et al. 2021). We use OWL-ViT for obtain-\ning object segments and CLIP models for classifying object\nproperties. The goal is to construct scene graph which con-\ntains the following information: the shape, color, material,\nand size for each object, and the spatial relationships be-\ntween each pair of objects.\nOur object detection predicate is defined as follows:\n@owl_vit([\"cube\", \"sphere\", \"cylinder\"],\nexpand_crop_region=10, limit=10,\nflatten_probability=true)\ntype segment_image(\nbound img: Tensor, id: u32,\ncropped_image: Tensor, area: u32,\nbbox_center_x: u32, bbox_bottom_y: u32)\nWe are using the @owl_vit foreign attribute to decorate a\npredicate vit_segment_image. Here, the image has one\nbounded argument which is the input image, and it produces\nimage segments represented by 5 tuples, containing segment\nid (id), segmented image (cropped_image), the area of\nsegment (area), the center x coordinate (bbox_center_x),\nand the bottom y coordinate (bbox_bottom_y). Specifi-\ncally, segmented images can be passed to downstream im-\nage classifiers, the area is used to classify whether the object\nis big or small, and the coordinates are used to determine\nspatial relationships between objects.\nNote that the arguments we pass to @owl_vit contain\nexpected labels of cube, sphere, and cylinder. Because\nOWL-ViT does not perform well at classifying given geo-\nmetric objects by shape, we do not use it to query the labels\nassociated with each object. Rather, these labels identify the\nimage segments the model extracts from the base image.\nWe set expand_crop_region to be 10, which expands\nthe cropped images by the given factor. Since the bounding\nboxes of the objects are tight, enlarging the crop region can\nhelp subsequent classifiers to better see the object. With the\nlimit set to 10, OWL-ViT only generates 10 image seg-\nments. Lastly, we set flatten_probability to be true.\nAgain, OWL-ViT is not trained on CLEVR, so it produces\nvery low confidence scores on all recognized objects. In or-\nder to not let the scores affect downstream computation, we\noverwrite the probability to 1 for all objects.\nWe load the image specified by the image directory path\nusing the foreign function $load_image, and then segment\nthe image using the segment_image predicate:\ntype img_dir(directory: String) \/\/ input\nrel image($load_image(d)) = img_dir(d) \/\/ load\nrel obj_seg(id, seg, obj_size, x, y) =\nimage(img) and\nsegment_image(img, id, seg, obj_size, x, y)\nrel obj(id) = obj_seg(id, _, _, _, _)\nWe next define the shape classifier. For this, we repur-\npose @clip to classify each object segment with a label\nfrom three possible shapes: cube, sphere, and cylinder.\nIn order to interface with CLIP, we create a prompt \"a {{}}\nshaped object\". Each label is filled into the prompt, pro-\nducing short phrases like “a cube shaped object”. Then, the\nthree prompts are passed to CLIP along with the object im-\nage, and facts with labels are returned with probabilities.\n@clip([\"cube\", \"cylinder\", \"sphere\"],\nprompt=\"a {{}} shaped object\")\ntype classify_shape(\nbound obj_img: Tensor,\nshape: String)\nrel shape(o, s) = obj_seg(o, seg, _, _, _) and\nclassify_shape(seg, s)\nThe classifiers for color and material are done similarly:\n@clip([\"red\", \"blue\", \"yellow\",\n\"purple\", \"gray\", \"brown\", \"cyan\", \"green\"],\nprompt=\"a {{}} colored object\")\ntype classify_color(\nbound obj_img: Tensor,\ncolor: String)\nrel color(o, c) = obj_seg(o, seg, _, _, _) and\nclassify_color(seg, c)\n@clip([\"shiny metal\", \"solid rubber\"],\nprompt=\"object made out of {{}} material\")\ntype classify_material(\nbound obj_img: Tensor,\nmaterial: String)\nrel material(o, m) = obj_seg(o, s, _, _, _) and\nclassify_material(s, m)\nFrom here, we just invoke the previous We continue to dis-\ncuss how do we obtain the size and spatial relationships. In\norder to obtain the size (small or large) of each object, we\nuse a probabilistic rule for specifying that:\nrel {\n0.9::size(o, if l then \"large\" else \"small\");\n0.1::size(o, if !l then \"large\" else \"small\")\n} = obj_seg(o, _, size, _, _) and l == size > A\nFinally, the spatial relationship (left, right, front, and\nbehind) is derived from object coordinates.\nrel obj_pos(o, x, y) = obj_seg(o, _, _, x, y)\nrel relate(o1, o2, dir) = o1 != o2 and\nobj_pos(o1, x1, _) and obj_pos(o2, x2, _) and\ndir == if x1 < x2 then \"left\" else \"right\"\nrel relate(o1, o2, dir) = o1 != o2 and\nobj_pos(o1, _, y1) and obj_pos(o2, _, y2) and\ndir == if y1 > y2 then \"front\" else \"behind\"\nCombining everything together, we have produced the re-\nlationships color, shape, material, size, and relate,\nforming the scene graph of the image.\nITEM ::= ATTRS? DECL\nATTRS ::= @ATTR | @ATTR(POS_ARG, ..., KW=KW_ARG, ...)\nDECL ::= IMPORT_DECL | TYPE_DECL | CONST_DECL | REL_DECL | QUERY_DECL\nIMPORT_DECL ::= import \"FILENAME\"\nTYPE_DECL ::= type TYPE_ALIAS = TYPE | type SUB_TYPE <: TYPE\n| (type TYPE = CONS1(TYPE, ...) | CONS2(TYPE, ...) | ...)\n| type PRED(ARG: TYPE, ...) | type $FN(ARG: TYPE, ...) -> TYPE\nCONST_DECL ::= const NAME = CONSTANT\nREL_DECL ::= rel [PROB::]PRED(EXPR, ...) | rel PRED = {[PROB::](EXPR, ...), ...}\n| rel [PROB::]ATOM = FORMULA | rel { ATOM; ... } = FORMULA\nQUERY_DECL ::= query PRED\nTYPE ::= i8 | i16 | i32 | i64 | isize | u8 | u16 | u32 | u64 | usize | f32 | f64 | char | bool | String\n| DateTime | Duration | Tensor | NAME\nCONSTANT ::= NUMBER | BOOL_LITERAL | CHAR_LITERAL | STRING_LITERAL | DATETIME_LITERAL | DURATION_LITERAL\nEXPR ::= CONSTANT | EXPR BIN_OP EXPR | UNA_OP EXPR | if EXPR then EXPR else EXPR | EXPR as TYPE\n| $FN(EXPR, ...) | new CONS(EXPR, ...)\nFORMULA ::= PRED(EXPR, ...) | FORMULA and FORMULA | FORMULA or FORMULA | not FORMULA\n| case VAR is ENTITY | VAR := AGGREGATOR(VAR*: FORMULA [where VAR*: FORMULA])\nENTITY ::= EXPR | CONS(ENTITY, ...)\nFigure 7: Surface Syntax of VIEIRA language.\nNL question to programmatic query\nWe use the GPT-4 model (OpenAI 2023) for converting a\nnatural language question into a programmatic query. The\nfirst step is defining the domain specific language (DSL) for\nquerying the CLEVR dataset:\ntype Query = Scene()\n| FilterShape(Query, String)\n| FilterMaterial(Query, String)\n| FilterColor(Query, String)\n| FilterSize(Query, String)\n| Relate(Query, String)\n| Count(Query)\n| Exists(Query)\n| GreaterThan(Query, Query)\n| LessThan(Query, Query)\n| Equals(Query, Query)\n| Intersect(Query, Query)\n| Union(Query, Query)\n| SameSize(Query)\n| SameColor(Query)\n| SameMaterial(Query)\n| SameShape(Query)\n| QueryMaterial(Query)\n| QueryColor(Query)\n| QueryShape(Query)\n| QuerySize(Query)\nNotice that the DSL is represented by the user-defined alge-\nbraic data type (ADT) Query, which contains constructs for\ngetting objects, counting objects, checking existence of ob-\njects, and even comparing counts obtained from evaluating\nmultiple queries. We then create the semantic parser for the\nDSL by configuring the GPT-4 model:\n@gpt_semantic_parse(\nheader=\"\"\"\nPlease convert a question into its programmatic\nform according to the following language:\nExpr = Scene() | FilterShape(Expr, String) | ...\nPlease pick shapes among \\\"cylinder\\\", ...;\nColors are among \\\"red\\\", \\\"blue\\\", ...;\nMaterials are among \\\"shiny metal\\\" and ...;\nSizes are among \\\"large\\\" and \\\"small\\\";\nSpatial relations are among \\\"left\\\", ...\"\"\",\nprompt=\"\"\"\nQuestion: {{s}}\nQuery: {{e}}\"\"\",\nexamples=[\n(\"How many red objects are there?\",\n\"Count(FilterColor(Scene(), \\\"red\\\"))\"),\n(\"Is there a cube?\",\n\"Exists(FilterShape(Scene(), \\\"cube\\\"))\"),\n...],\nmodel=\"gpt-4\")\ntype parse_query(bound s: String, q: Query)\nOther than the model argument which is used to specify the\nOpenAI model to call, we also pass 3 additional arguments\nto gpt_semantic_parse: header, prompt, and exam-\nples. These arguments construct the first part of the prompt\nthat we pass to GPT-4. Assuming the actual question (“Is\nthere an object to the left of the cube?”) is passed to the\nforeign predicate parse_expr as the first argument s, the\nentire prompt becomes:\n# header part...\nPlease convert a question into its program...\n# example 1\nQuestion: How many red objects are there?\nQuery: Count(FilterColor(Scene(), \"red\"))\n# example 2\nQuestion: Is there a cube?\nQuery: Exists(FilterShape(Scene(), \"cube\"))\n# more examples...\n# actual question\nQuestion: Is there an object to the left of\nthe cube?\nQuery:\n>>> GPT-4 Answer:\nExists(Relate(\nFilterShape(Scene(), \"cube\"), \"left\"))\nThen, GPT-4 is prompted to produce the query, which is\nparsed back into our ADT Query:\ntype question(String) \/\/ input question string\nrel parsed_query(q) =\nquestion(s) and parse_query(s, q)\nPutting it all together\nThe last part which brings everything together is the seman-\ntics of our Query DSL. The semantics is inductively de-\nfined on the Query data structure. We start from defining\nthe variants which return a set of objects. For this, we use\nthe eval_obj binary relation to connect queries with their\nevaluated object IDs:\nrel eval_obj(e, o) =\n\/\/ Scene\ncase e is Scene() and object(o)\nrel eval_obj(e, o) =\n\/\/ FilterShape\ncase e is FilterShape(e1, s) and\neval_obj(e1, o) and shape(o, s)\nrel eval_obj(e, o) =\n\/\/ FilterColor\ncase e is FilterColor(e1, c) and\neval_obj(e1, o) and color(o, c)\nrel eval_obj(e, o) =\n\/\/ FilterMaterial\ncase e is FilterMaterial(e1, m) and\neval_obj(e1, o) and material(o, m)\nrel eval_obj(e, o) =\n\/\/ FilterSize\ncase e is FilterSize(e1, s) and\neval_obj(e1, o) and size(o, s)\nrel eval_obj(e, p) =\n\/\/ Relate\ncase e is Relate(e1, dir) and\neval_obj(e1, o) and relate(p, o, dir)\nrel eval_obj(e, p) =\n\/\/ SameSize\ncase e is SameSize(e1) and\neval_obj(e1, o) and size(o, s) and\nsize(p, s) and o != p\nrel eval_obj(e, p) =\n\/\/ SameColor\ncase e is SameColor(e1) and\neval_obj(e1, o) and color(o, c) and\ncolor(p, c) and o != p\nrel eval_obj(e, p) =\n\/\/ SameMaterial\ncase e is SameMaterial(e1) and\neval_obj(e1, o) and material(o, m) and\nmaterial(p, m) and o != p\nrel eval_obj(e, p) =\n\/\/ SameShape\ncase e is SameShape(e1) and\neval_obj(e1, o) and shape(o, s) and\nshape(p, s) and o != p\nWe next define the semantics for queries which evaluate to a\nboolean, producing the eval_bool relation:\nrel eval_bool(e, b) = b := exists(\n\/\/ Exists\no: eval_obj(e1, o) where\ne: case e is Exists(e1))\nrel eval_bool(e, n1 > n2) =\n\/\/ GreaterThan\ncase e is GreaterThan(e1, e2) and\neval_num(e1, n1) and eval_num(e2, n2)\nrel eval_bool(e, n1 < n2) =\n\/\/ LessThan\ncase e is LessThan(e1, e2) and\neval_num(e1, n1) and eval_num(e2, n2)\nrel eval_bool(e, n1 == n2) =\n\/\/ Equals\ncase e is Equals(e1, e2) and\neval_num(e1, n1) and eval_num(e2, n2)\nWe finally define the semantics for queries which evaluate\nto a number, producing the eval_num relation:\nrel eval_num(e, n) = n := count(\n\/\/ Count\no: eval_obj(e1, o) where\ne: case e is Count(e1))\nTo connect everything together, we apply the eval_* rela-\ntion on the parsed expression to get the evaluated result:\nrel result(r as String) =\nparsed_query(q) and eval_num(q, r)\nrel result(r as String) =\nQuestion: Is there a yellow cube?\nProgrammatic Query:\nExists(\nFilterColor(\nFilterShape(Scene(), \"cube\"),\n\"yellow\"))\nAnswer: true\nFigure 8: A CLEVR example data-point.\nparsed_query(q) and eval_bool(q, r)\nA concrete example\nWe illustrate a concrete example in Fig. 8.\nExperimental Details\nOur experiments are conducted on a machine with two 20-\ncore Intel Xeon CPUs, four GeForce RTX 2080 Ti GPUs,\nand 768 GB RAM. Note that our experiments do not involve\ntraining and therefore do not require high-end computation\nresources. In this section we elaborate on the foundation\nmodels that we used in our experiments and the setup for\nindividual tasks.\nModel setup\nGPT.\nThe default GPT model we use is gpt-4. Depend-\ning on the task, there are a few variations we have used\nwhich include gpt-3-turbo, text-embedding-ada-002.\nWe set the model temperature to 0 as the default value, and\nwe cache the intermediate result locally for expense-saving\npurposes. For chain-of-thought (CoT) prompting, we adopt\nthe zero-shot technique introduced by (Kojima et al. 2022).\nQuestions that encounter an API server error are manually\nre-queried. All experiments are performed from June to Au-\ngust 2023.\nOWL-ViT.\nWe use the OWLViTProcessor and OwlViT-\nForObjectDetection\nmodels\nfrom\nHugging\nFace.\nWe\nload the pretrained checkpoint google\/owlvit-base-\npatch32. We set the processor’s score_threshold to 0.1,\nscore_multiplier to 1.0, and expand_crop_region to\n0.0.\nViLT.\nWe use the ViltProcessor and ViltForQuestionAn-\nswering models from Hugging Face. We load the pretrained\ncheckpoint dandelin\/vilt-b32-finetuned-vqa. We set the\ndefault value of top to 5 and score_threshold to 0.1.\nCLIP.\nWe use OpenAI’s official implementation with the\nmodel set to ViT-B\/32.\nDSFD.\nWe use the implementation of DSFD from PyPI’s\nface-detection package. We set confidence_threshold to\n0.5 and nms_iou_threshold to 0.3.\nSegment Anything.\nWe use the Segment Anything Model\n(SAM) from its official open-source repository. Specifically,\nwe use the ViT-H SAM model checkpoint. We set the de-\nfault iou_threshold to 0.88, area_threshold to 0, and\nexpand_crop_region to 0.\nPrompt2prompt\nand\nDiffusion\nModel.\nWe\nadapt\nPrompt2prompt (Hertz et al. 2022) from its official\nrepository to support continuous editing, and we choose\nthe underlying stable diffusion model CompVis\/stable-\ndiffusion-v1-4 from Hugging Face. We set the default\nvalue of num_inference_steps to 50 for the stable diffusion\nmodel, max_num_words to 77, and guidance_scale to 7.5.\nOthers.\nThe other integrated models which are not used\nin our experiments includes chat models like Vicuna (Zheng\net al. 2023), Llama 2 (Touvron et al. 2023), and text embed-\nding models like Cross-Encoder (Nogueira and Cho 2019),\nand RoBERTa (Liu et al. 2019).\nTask setup\nDate reasoning.\nThe DR dataset is adapted from BIG-\nbench’s date understanding task, with 28 of the orig-\ninal 369 questions being corrected for wrong target answers.\nWe solve this task by decomposing it into two sub-tasks:\nextracting structured information using an LM, followed by\nlogical reasoning over the structured information using rela-\ntional rules and date arithmetic.\nFor the first sub-task, we leverage GPT-4 with 5-shot\nprompting for extracting the following three relations from\nthe given context:\n1. mentioned_date(label, date): label is a string label\nfor a date whose MM\/DD\/YYYY form is explicitly men-\ntioned in the context, and date is the corresponding\nMM\/DD\/YYYY string.\n2. goal(label): label is the date label whose MM\/D-\nD\/YYYY form is requested as the answer.\n3. relationship(earlier_date, later_date, diff):\nthe first two arguments are a pair of date labels relevant\nto the question, and diff is the time duration between\nthe dates.\nSee Table 7 for an example set of extracted relations. The\nshots for gpt_extract_relation are manually composed to\nbe similar to questions in the dataset. For this task specifi-\ncally, we configure gpt_extract_relation to use zero-shot\nCoT (Kojima et al. 2022) when extracting relationship,\nwhich improves accuracy by over 10%.\nQuestion: Today’s meeting is rescheduled to 11 am tomorrow, 10\/16\/1924.\nWhat is the date one year ago from today in MM\/DD\/YYYY?\nVIEIRA extracted\nrelations:\nmentioned_date: [(\"rescheduled-meeting\", \"1924-10-16\")]\nrelationship: [(\"1-year-ago\", \"today\", \"R12MO PT0S\"), (\"to-\nday\", \"rescheduled-meeting\", \"P1D\")]\ngoal: [(\"1-year-ago\")]\nVIEIRA answer:\n10\/15\/1923 (CORRECT)\nGPT-4 response:\nLet’s think step by step. First, we need to subtract one year from\nthe given date, 10\/16\/1924. When we do this, we get 10\/16\/1923.\nGPT-4 answer:\n10\/16\/1923 (INCORRECT)\nTable 7: Comparison between our solution and GPT-4 (zero-\nshot CoT) on selected question from DR dataset.\nAfter extracting these relations, a symbolic program iter-\nates through derived dates and durations to compute dates\nfor all extracted date labels, including the goal date. Date\nparsing and date arithmetic is enabled by VIEIRA’s built-in\ndata types DateTime and Duration.\nTracking shuffled objects.\nThe TSO dataset is randomly\nsampled from a combined dataset of subtasks corresponding\nto n = 3, 5, 7 objects from BIG-bench’s tracking shuf-\nfled objects task. Specifically, our random sample con-\ntains 32 questions where n = 3, 59 questions where n = 5,\nand 59 questions where n = 7. Our solution relies on GPT-4\nwith single-shot prompting for extracting three relations:\n1. possessions(time, person, object):\nperson\npos-\nsesses object at time step time. We prompt GPT-4 to\nonly extract the initial possessions (where time is 1),\nwhich are explicitly described in the context.\n2. swaps(time, person_a, person_b):\nperson_a\nand\nperson_b swap objects at time step time.\n3. goal(person): person is the target person whose final\npossessed object is expected as the answer.\nSee Table 8 for an example set of extracted relations. We\nprompt gpt_extract_relation with one shot based on a\nquestion from the BIG-bench task but not from our sampled\ndataset. Our reasoning program then iterates through all the\nswaps starting from the initial possessions and retrieves the\nlast possessed object associated with the target.\nWe conjecture that the exemplary performance of our\nmodel on TSO is due to the highly consistent syntactic struc-\nture of the NL inputs, facilitating relation extraction under a\none-shot setting.\nKinship reasoning.\nWe use the CLUTRR (Sinha et al.\n2019) dataset to perform the kinship reasoning task in NLP.\nEach data point in this task contains a story that indicates the\nkinship between characters, and the aim is to infer the rela-\ntionship between two specified characters. We use all the\n1,146 data points in the test dataset of 089907f8.\nThe intermediate symbolic representation we use GPT-4\nmodel to extract is a kinship graph, whose edge is composed\nof 20 different relationships, including “father”, “mother”,\n“uncle”, “niece”. For prompting the GPT-4 model, we first\nask the GPT model to yield us all the kinships relations\nQuestion: Alice, Bob, Claire, Dave, and Eve are dancers at a square dance.\nAt the start of a song, they each have a partner: Alice is dancing with Ophelia,\nBob is dancing with Lola, Claire is dancing with Izzi, Dave is dancing with\nKarl, and Eve is dancing with Jamie. Throughout the song, the dancers often\ntrade partners. First, Bob and Claire switch partners. Then, Eve and Dave\nswitch partners. Then, Eve and Claire switch partners. Then, Alice and Eve\nswitch partners. Finally, Alice and Bob switch partners. At the end of the\ndance, Bob is dancing with\nVIEIRA extracted\nrelations:\npossessions: [(1, \"Alice\", \"Ophelia\"), (1, \"Bob\", \"Lola\"), (1,\n\"Claire\", \"Izzi\"), (1, \"Dave\", \"Karl\"), (1, \"Eve\", \"Jamie\")]\nswaps: [(1, \"Bob\", \"Claire\"), (2, \"Eve\", \"Dave\"), (3, \"Eve\",\n\"Claire\"), (4, \"Alice\", \"Eve\"), (5, \"Alice\", \"Bob\")]\ngoal: [(\"Bob\")]\nVIEIRA answer:\nLola (CORRECT)\nGPT-4 response:\nLet’s think step by step. At the start, Bob is dancing with Lola.\nAfter Bob and Claire switch partners, Bob is dancing with Izzi.\nAfter Eve and Dave switch partners, Bob is still dancing with Izzi.\nAfter Eve and Claire switch partners, Bob is still dancing with\nIzzi. After Alice and Eve switch partners, Bob is still dancing with\nIzzi. Finally, after Alice and Bob switch partners, Bob is dancing\nwith Ophelia. So, the answer is (A) Ophelia.\nGPT-4 answer:\nOphelia (INCORRECT)\nTable 8: Comparison between our solution and GPT-4 (zero-\nshot CoT) on selected question from TSO dataset.\nQuestion: Myrna and her husband Christopher went on a cruise. They had a\nwonderful time. Christopher and his daughter Lucille took a day off school\nto go to the zoo. Who is Lucille to Myrna?\nVIEIRA extracted\nrelations:\nmentioned_kinship:\n[(\"Christopher\",\n\"Lucille\",\n\"father\"),\n(\"Christopher\", \"Myrna\", \"husband\"), (\"Lucille\", \"Christopher\",\n\"daughter\"), (\"Myrna\", \"Christopher\", \"wife\")]\nquery: [(\"Lucille\", \"Myrna\")]\nVIEIRA answer:\ndaughter (CORRECT)\nGPT-4 response:\nLucille is Myrna’s stepdaughter. In the given scenario, Christo-\npher is Myrna’s husband, and Christopher is also the father of Lu-\ncille. This makes Lucille the daughter of Myrna’s husband, which\nmeans she is Myrna’s stepdaughter.\nGPT-4 answer:\nstepdaughter (INCORRECT)\nTable 9: Comparison between our solution and GPT-4 (zero-\nshot CoT) on selected question from CLUTRR dataset.\nthat are mentioned in the context and store it in men-\ntioned_kinship. Then we also need to extract the two\ntarget characters, in which we consult gpt and store the an-\nswer in the relation query. See Table 9 for an example set\nof extracted relations.\nThe resulting kinship graph is then reasoned along with a\ngiven external knowledge base, which includes the compo-\nsitional knowledge like “father’s mother is grandmother”, to\nobtain the relationship between the two desired people.\nMath reasoning.\nThis task is drawn from the GSM8K\ndataset of arithmetic word problems (Cobbe et al. 2021).\nBoth our math and date reasoning datasets have previously\nserved as benchmarks for LLM performance under chain-of-\nthought prompting (Wei et al. 2023; Kojima et al. 2022). The\nquestions involve grade school math word problems created\nby human problem writers, and the model is asked to pro-\nduce a number as the result. Since the output can be frac-\nQuestion: If it takes 3 kangaroos traveling at the same speed a total of 18\nhours to travel across a highway, how many hours will it take four turtles,\neach traveling at half the speed of a kangaroo, to do so?\nVIEIRA extracted\nsteps:\n(0, \"kangaroo_speed\", \"1 \/ (18 \/ 3)\"),\n(1, \"turtle_speed\", \"{kangaroo_speed} \/ 2\"),\n(2, \"turtle_time\", \"1 \/ {turtle_speed}\"),\n(3, \"total_turtle_time\", \"{turtle_time} * 4\")\nVIEIRA answer:\n48 (CORRECT)\nGPT-4 response:\nLet’s think step by step. First, we know that the total time it takes\nfor 3 kangaroos to travel across the highway is 18 hours. This\nmeans that one kangaroo takes 18 hours to travel across the high-\nway. Then, we know that a turtle travels at half the speed of a\nkangaroo, which means it takes twice as long for a turtle to travel\nthe same distance. So, one turtle would take 18 hours * 2 = 36\nhours to travel across the highway. However, we have four turtles\ntraveling together at the same speed, so they would still take 36\nhours to travel across the highway, because they are traveling si-\nmultaneously.\nGPT-4 answer:\n36 (INCORRECT)\nTable 10: Comparison between our solution and GPT-4\n(zero-shot CoT) on selected question from GSM8k dataset.\ntional, we allow a small delta when comparing the derived\nresult with the ground truth. We include GPT-4 (few-shot\nand CoT) as baselines for this task.\nOur solution to this task prompts GPT-4 (2-shot) to pro-\nduce step-by-step expressions, which can contain constants,\nvariables, and simple arithmetic operations. For example,\nthe fact assign(\"total_sale\", \"april_sale + may_sale\n\") represents that total sales are the sum of April and May\nsales. See Table 10 for an example set of extracted steps.\nWe evaluate all the expressions through a DSL, and the\nresult associated with the goal variable is returned. By fo-\ncusing the LM’s responsibility solely on semantic parsing,\nour relational program can then achieve faithful numerical\ncomputation via DSL evaluation.\nThe semantic parsed result is a Python expression that we\ncan directly call the eval function in Python over the string,\nand we can obtain the desired outcome. This py_eval func-\ntion is wrapped as a foreign attribute in .\nRetrieval augmentation and semantic search.\nWe have\ntwo benchmarks for retrieval augmentation and semantic\nsearch: HotpotQA and Amazon’s ESCI Product Search.\nThe HotpotQA (Yang et al. 2018) data, a Wikipedia-based\nquestion answering (QA) dataset under the “distractor\" set-\nting. Here, the model takes in 2 parts of inputs: 1) a question,\nand 2) 10 Wikipedia paragraphs as the context for answering\nthe question. Among the 10 Wikipedia pages, at most 2 are\nrelevant to the answer, while the others are distractors. This\nchallenges the capability of retrieving information based on\nthe question. Since the QA models produce free-form an-\nswers that can vary largely, we use GPT-4 to check the cor-\nrectness of the derived result against the ground truth. This\nis aided by the manual inspection of subsets to determine the\nstatistical variance.\nWe use Amazon’s ESCI Product Search dataset (Reddy\net al. 2022). The model is provided with a natural language\n(NL) query and a list of products (23 products on average).\nThe goal is to rank the products that best match the query.\nIn the dataset, for each pair of query and product, a label\namong E (exact match), S (substitute), C (complementary),\nand I (irrelevant) is provided. The metric we use to evaluate\nthe performance is nDCG. The gains are set to be 1.0 for E,\n0.1 for S, 0.01 for C, and 0.0 for I. We include GPT’s em-\nbedding model, text-embedding-ada-002 as baselines for\nranking products.\nCompositional multi-modal reasoning.\nFor composi-\ntional multi-modal reasoning, we pick tasks of CLEVR and\nGQA. We choose two compositional VQA datasets, GQA\n(Hudson and Manning 2019) and CLEVR (Johnson et al.\n2016). In this task, the model is given an image and a ques-\ntion, and needs to answer the question. For GQA, the ma-\njority of questions expect yes\/no answers, while CLEVR’s\nquestions demand features like counting and spatial rea-\nsoning. We randomly sample 184 and 480 The images and\nquestions in GQA are collected from real life while that of\nCLEVR are synthetic.\nVisual object tagging.\nFor VQAR, we consider the top 50\nobject bounding boxes returned by OWL-ViT. Our relational\nknowledge base is from (Huang et al. 2021). When querying\nViLT, we take the top response from a score threshold of\n0.9. We manually score semantic correctness by finding the\npercentage of objects returned that match the query. Object\nbounding boxes are considered correct if they contained any\npart of an entity matching the query.\nFor OFCP, we curated 50 examples featuring groups of\nnotable celebrities and politicians from Wikimedia Com-\nmons and other Internet sources, and manually assigned de-\nscriptive filenames to each image. We obtain the set of pos-\nsible names by prompting GPT-4 with the filename. We en-\nlarge the face bounding boxes returned by DSFD by a factor\nof 1.3 before querying CLIP. We tag each face with its most\nprobable name from CLIP, but if the probability is below\nthe 0.8 threshold, then the face is tagged “unknown”. The\nground truth of relevant faces and their names were manu-\nally assigned based on the filename description. The ground\ntruth label for non-relevant faces is “unknown”. All faces\njudged to be in the foreground of an image, as well as any\nadditional faces not tagged with “unknown”, are counted for\nsemantic correctness.\nImage generation and editing.\nWe manually wrote 20\nprompts image generation and their editing sequences. Each\nprompt includes one image generation prompt and two con-\nsequent image editions. Our domain-specific language for\nimage generation and editing supports 5 operations: Back-\nground, ReplaceObject, RefineObject, NotObject,\nReweightObject. We use the GPT-4 model to convert the\nnatural language prompts into programmatic queries with 4\nshot examples. There are 2 cases among 20 that fail to con-\nvert the natural language into executable programs, as the\nReplace operation requires to have the same token length\nof the original input text and the updated text, while the\nGPT-4 model fails to capture the requirement through the\nfew-shot examples.\nQualitative Studies\nWe present exemplars for face tagging in Figure 9, object\ntagging in Figure 10, image editing in Figure 11, and image\ngeneration and editing in Figure 12.\n(a) 2016_GOP_Debate_SC_ap_img.jpg\n(b) Joe_Biden_Receives_Presidential_Medal_of_Freedom.jpg\n(c) dudamel_williams.jpg\n(d) 2019 Spanish General Election Debate.jpg\n(e) BRICS Summit 2019.jpg\n(f) microsoft ceos.jpeg\nFigure 9: Face Tagging (OFCP) exemplars.\nQ: Find the orange objects.\nProgrammatic Query:\nFind_Attr(\"orange\")\nAnswer: {13}\nQuestion: Find the objects that are clothing.\nProgrammatic Query:\nHypernym_Find(\"clothing\")\nAnswer: {7, 26}\nQ: Objs used for housing family.\nProgrammatic Query:\nKG_Find(X, \"used for\",\n\"housing family\")\nAnswer: {1}\nQuestion: Find the objects that are sports equipment.\nProgrammatic Query:\nHypernym_Find(\n\"sports equipment\")\nAnswer: {0, 1}\nFigure 10: Object Tagging (VQAR) exemplars.\nInstruction:\nDo a color pop of\nthe man in purple.\nInstruction:\nMake a color pop of\nthe current Microsoft CEO.\nInstruction: Create a color pop\nof the Ugandan President and put\nunamused_face over Kagame.\nInstruction:\nCover Bertha Vasquez with\nsmiling_face_with_sunglasses\nand Bill Nye with confused_face.\nInstruction: Hide Ulf Kristersson\nwith face_with_steam_from_nose\nand blur everyone except Sanna\nMarin.\nInstruction: Hide Jennifer Smart\nwith face_savoring_food and blur\nthe background of Stephen\nBaldwin.\nFigure 11: Image Editing (OFCP) exemplars.\nInstruction:\nStart with an image of a fish in an aquarium.\nThen replace the fish with a turtle.\nThen refine the aquarium with tropical setup.\nProgrammatic Query:\nRefineObject(\nReplaceObject(\nBackground(\"a fish in an aquarium\"), \"\nfish\", \"turtle\"),\n\"aquarium\",\n\"tropical setup\")\nInstruction:\nStart with an image of a horse in a meadow.\nRefine the meadow to have wildflowers and\nreplace the horse with a deer.\nProgrammatic Query:\nReplaceObject(\nRefineObject(\nBackground(\"a horse in a meadow\"),\n\"meadow\", \"meadow with wildflowers\"),\n\"horse\", \"deer\")\nInstruction:\nStart with an image of a park with a fountain.\nReplace the fountain with a statue and refine\nthe park to evening.\nProgrammatic Query:\nReplaceObject(\nRefineObject(\nBackground(\"a park with a fountain\"), \"\npark\", \"park in the evening\"),\n\"fountain\", \"statue\")\nInstruction:\nStart with an image of a bowl full of apples.\nThen replace the bowl with something else, and\nchange the apples to other fruits.\nProgrammatic Query:\nNotObject(\nNotObject(\nBackground(\"a bowl full of apples\"),\n\"bowl\"),\n\"apples\")\nInstruction:\nStart with an image of a dog under a tree. Then\nrefine the season to spring, and replace the\ndog to something else.\nProgrammatic Query:\nNotObject(\nRefineObject(\nBackground(\"a dog under a tree\"),\n\"tree\", \"tree in spring\"),\n\"dog\")\nInstruction:\nStart with an image of a bowl of salad. Replace\nthe salad with pasta and replace the bowl to\nsomething else.\nProgrammatic Query:\nRefineObject(\nReplaceObject(\nBackground(\"a bowl of salad\"),\n\"salad\", \"pasta\"),\n\"bowl\", \"plate\")\nFigure 12: Image Generation and Editing (IGP20) exemplars.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Relational Programming with Foundation Models.pdf"}
{"title":"SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation","authors":"Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan","summary":"The rapid evolution of multimodal foundation model has demonstrated\nsignificant progresses in vision-language understanding and generation, e.g.,\nour previous work SEED-LLaMA. However, there remains a gap between its\ncapability and the real-world applicability, primarily due to the model's\nlimited capacity to effectively respond to various user instructions and\ninteract with diverse visual data. In this work, we focus on bridging this gap\nthrough integrating two enhanced features: (1) comprehending images of\narbitrary sizes and ratios, and (2) enabling multi-granularity image\ngeneration. We present a unified and versatile foundation model, namely,\nSEED-X, which is able to model multi-granularity visual semantics for\ncomprehension and generation tasks. Besides the competitive results on public\nbenchmarks, SEED-X demonstrates its effectiveness in handling real-world\napplications across various domains after instruction tuning. We hope that our\nwork will inspire future research into what can be achieved by versatile\nmultimodal foundation models in real-world applications. The models, codes, and\ndatasets are released in https:\/\/github.com\/AILab-CVC\/SEED-X.","url":"http:\/\/arxiv.org\/abs\/2404.14396v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2404.14396v2","published":1713808569000,"comment":"We added benchmark results (without updating models) and ablation\n  study in this version. Project released at:\n  https:\/\/github.com\/AILab-CVC\/SEED-X","pdf_text":"SEED-X: Multimodal Models with Unified\nMulti-granularity Comprehension and Generation\nYuying Ge1∗\nSijie Zhao1∗\nJinguo Zhu1∗\nYixiao Ge1,2†\nKun Yi2\nLin Song1\nChen Li2\nXiaohan Ding1\nYing Shan1,2\n1Tencent AI Lab\n2ARC Lab, Tencent PCG\nWhich button on the \nimage do I click to \nreserve this new car?\nUse your imagination to design a concept \nimage for Artificial General Intelligence (AGI).\nChatting with SEED-X\nHere is an image\nTell me about your design idea.\nI designed the image to represent the concept \nof Artificial General Intelligence (AGI). AGI is a \nhypothetical AI that possesses the ability to \nunderstand, learn, and apply knowledge across \na wide range of tasks at a level equal to or \nbeyond human capability. In the image, I used a \nhumanoid robot with advanced technology and \nfuturistic elements to represent the advanced \ncapabilities of AGI. The robot is equipped with \nvarious sensors, antennas, and other \ntechnological devices, symbolizing its ability to \ngather and process vast amounts of \ninformation. The background features a \nnetwork of connections and nodes, \nrepresenting the complex and interconnected \nnature of AGI’s cognitive processes. The overall \ndesign aims to convey the idea of AGI as a \npowerful and sophisticated AI that can perform \na wide range of tasks and has the potential to \nrevolutionize various aspects of society.\nI want to make the room look more \nvibrant, do you have any suggestion?\nYes, you could consider adding \nsome colorful decorations or \nartwork to the room. For \nexample, you could hang a \ncolorful painting or a piece of art \non the wall, or add some colorful \nthrow pillows to the couch. This \ncan make the room feel more \nlively and welcoming.\nShow me a picture after the \ndecorations are added to the room.\nYes, there are several options suitable for \nyour dinner. You can find a container of salad \nat green bbox, a container of cottage cheese \nat red bbox , and a container of yogurt at\nyellow bbox.\nThe foods in the refrigerator are suitable for \nweight loss because they are low in calories \nand high in nutrients. The salad is a low-\ncalorie food that is high in fiber and vitamins, \nwhich can help you feel full and satisfied. The \ncottage cheese is a good source of protein, \nand it’s also low in calories. The yogurt is a \ngood source of protein and calcium, and it’s \nalso low in calories. \nThe button to reserve the new \ncar is the \"Reserve Now\" button, \nwhich is located in the top left \ncorner of the image.\nMake the man wear a red hat and \nchange the background to the mountain.\nHere’s a picture.\nGenerate a slide with texts emphasizing the significance of \nperseverance. Below is a vivid picture of a magnificent \nmountain peak under the sun.\n{‘slide_height’: 6858000, ‘slide_width’: 9144000, ‘elements’: [{‘type’: \n‘image’, ‘image_path’: \n'x': ' <loc-0> ', 'y': ' <loc-0> ', 'width': '<loc-224>', \n'height': '<loc-224>'}, {'type': 'text', 'x': ' <loc-49> ', 'y': ' <loc-42> ', 'width': ' \n<loc-131> ', 'height': ' <loc-21> ', 'paragraphs': [{'texts': 'Perseverance', \n'font_size': 40.0}]}, {'type': 'text', 'x': ' <loc-38> ', 'y': ' <loc-105> ', 'width': ' \n<loc-141> ', 'height': ' <loc-21> ', 'paragraphs': [{'texts': 'The capacity to \npersevere, even in the most difficult circumstances is what separates the \nsuccessful from the unsuccessful.', 'font_size': 20.0}]}]}\nPlease write a brief appreciation for this painting.\nHere’s a picture.\nAre you curious about the \nscenery on the mountain?\nThe flowers are in \nfull bloom.\nThe water is flowing \nover the rocks.\nThe view from the\ntop is amazing.\nChange the top clothes of the model.\nGenerate the following story.\nI’ve been on a diet recently, is \nthere any suitable food in the \nrefrigerator for my dinner?\nThis is a captivating panoramic view of a traditional Chinese landscape painting. The scene is dominated by a river that meanders through the center of the composition, flanked by lush greenery and a variety of structures. \nThe architecture is intricate, with buildings of different sizes and designs, some of which are adorned with traditional Chinese roofs. The painting is rich in detail, with numerous figures engaged in various activities, such as \nfarming, fishing, and traveling. The colors are vibrant, with a predominance of greens and browns that bring the scene to life. The perspective is particularly striking, with the river serving as a central axis that draws the \nviewer’s eye through the composition. The painting is a testament to the artistic and cultural heritage of China, offering a glimpse into the country’s past through its depiction of everyday life and the natural environment.\nFigure 1: The introduced SEED-X, a unified and versatile foundation model, can serve as various\nmultimodal AI assistants in the real world after different instruction tuning, capable of responding\nto a variety of user needs through unifying multi-granularity comprehension and generation.\narXiv:2404.14396v2  [cs.CV]  2 Mar 2025\nAbstract\nThe rapid evolution of multimodal foundation model has demonstrated significant\nprogresses in vision-language understanding and generation, e.g., our previous\nwork SEED-LLaMA. However, there remains a gap between its capability and the\nreal-world applicability, primarily due to the model’s limited capacity to effectively\nrespond to various user instructions and interact with diverse visual data. In this\nwork, we focus on bridging this gap through integrating two enhanced features:\n(1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-\ngranularity image generation. We present a unified and versatile foundation model,\nnamely, SEED-X, which is able to model multi-granularity visual semantics for\ncomprehension and generation tasks. Besides the competitive results on public\nbenchmarks, SEED-X demonstrates its effectiveness in handling real-world appli-\ncations across various domains after instruction tuning. We hope that our work\nwill inspire future research into what can be achieved by versatile multimodal\nfoundation models in real-world applications. The models, codes, and datasets are\nreleased in https:\/\/github.com\/AILab-CVC\/SEED-X1.\n1\nIntroduction\nIn recent years, Multimodal Large Language Models (MLLMs) [1, 2, 3, 4, 5, 6, 7, 8] have demon-\nstrated exceptional capabilities in comprehending multimodal data through leveraging the strong\ngenerality of LLMs [9, 10, 11]. Some pioneering work [12, 13, 14, 15, 16, 17, 18, 19] further\nempower LLMs with the ability to generate images beyond texts. For example, our previous work\nSEED-LLaMA [15] can handle a variety of tasks and excel in academic benchmarks through unifying\nmultimodal comprehension and generation. However, the accuracy and diversity of its generated\ncontent still fall short of real-world needs. In this work, we focus on bridging this gap through\nupgrading SEED-LLaMA with enhanced capabilities for real-world applications.\nSpecifically, in order to make a multimodal foundation model applicable in real-world scenarios, we\nincorporate two enhanced features: (1) understanding images of arbitrary sizes and ratios, and (2)\nmulti-granularity image generation, encompassing both high-level instructional image generation and\nlow-level image manipulation tasks. These attributes can form the basis for a multimodal foundation\nmodel’s effective application in an open-world context, since a multimodal foundation model has to\naccommodate various downstream tasks requiring different levels of visual semantics.\nIn this paper, we introduce SEED-X, a unified and versatile multimodal foundation model as a\nfollow-up work of SEED-LLaMA, which seamlessly integrates the features mentioned above. It is\nimportant to emphasize that integrating all these characteristics into a single foundation model is by\nno means trivial, as shown in Table 1, since none of the previous works support all of these features.\nAfter different instruction tuning, SEED-X can function as various multimodal AI assistants in the\nreal world, capable of addressing various user needs through generating proper texts and images\nas shown in Fig. 1. Specifically, our instruction-tuned models can act as an interactive designer,\ngenerating images while illustrating creative intent, offering modification suggestions and showcasing\nvisualizations based on user’s input images. Additionally, they can act as knowledgeable personal\nassistants, comprehending images of various sizes and providing relevant suggestions. Moreover, they\ncan generate more diverse outputs, such as slide layouts for slide creation, and interleaved image-text\ncontent for storytelling. SEED-X signifies a notable advancement towards a versatile agent for users\nin the real world.\nTo endow SEED-X with the aforementioned characteristics, our approach incorporates (1) a visual\ntokenizer to unify image comprehension and generation, where its multi-granularity de-tokenization\nphase facilitates image generation and high-precision image manipulation, and (2) an MLLM with\ndynamic resolution image encoding to enable the comprehension of images with arbitrary sizes and\n1This is the v2 version. We added benchmark results (without updating models) and ablation study.\n*Equal Contribution.\n†Correspondence to yixiaoge@tencent.com.\n‡We sincerely acknowledge Tianheng Cheng (ARC Lab, Tencent PCG) for his support.\n2\noriginal image\nSEED\nSEED-X\nSEED-X w\/ condition image\nconditional image\nFigure 2: The reconstruction results of our visual de-tokenizer. It can decode realistic images that\nare semantically aligned with the original images by taking the ViT features as inputs, and further\nrecover fine-grained details by incorporating the conditional images as inputs.\naspect ratios. Specifically, we utilize a pre-trained ViT as the visual tokenizer and train a visual\nde-tokenizer to decode realistic images by taking the ViT features as input. To realize the retention\nof fine-grained details of the input image to satisfy image manipulation, we further fine-tune the\nvisual de-tokenizer to take an extra condition image as input in the latent space (See Fig. 2). The ViT\nfeatures serve as a bridge to decouple the training of the visual (de-)tokenizer and the MLLM. The\ndynamic resolution image encoding divides an input image into sub-images and adds extrapolatable\n2D positional embeddings to the ViT features of each sub-image, allowing the MLLM to scale to any\nimage resolution. For image generation, a fixed number of learnable queries are fed into the MLLM,\nwhere the output hidden states are trained to reconstruct the ViT features of the target images. During\ninference, the image de-tokenizer can take both the output features from the MLLM and the condition\nimage provided by users as input, ensuring that the decoded image can possess high-level semantics\nthat meet the multimodal instructions and retain the low-level details.\nWe pre-train SEED-X on massive multimodal data, including image-caption pairs, grounded image-\ntext data, interleaved image-text data, OCR data, and pure texts. We further apply multimodal\ninstruction tuning to align SEED-X with human instructions across various domains, utilizing both\nexisting datasets and newly collected datasets that cover image editing, text-rich, grounded and\nreferencing QA, and slide generation tasks. The extensive evaluations on MLLM benchmarks demon-\nstrate that our instruction-tuned model not only achieves competitive performance in multimodal\ncomprehension, but also exhibits excellent instruction-following capabilities for image generation.\nAll the models, codes, and datasets are made publicly available. We hope our work can bring insights\nto the community about the potential of multimodal foundation models in real-world scenarios\nthrough unifying multi-granularity comprehension and generation.\n2\nRelated Work\nWith the rapid development of Multimodal Large Language Models (MLLM), recent studies have\nbeen working on unified MLLMs that are capable of multimodal comprehension and generation\nas shown in Tab. 1. Some work [15, 14, 13, 20, 21, 22, 23, 24] utilize a discrete visual tokenizer to\n3\nTable 1: MLLMs that unify comprehension and generation and whether they support significant\ncharacteristics essential for real-world applications. “Decoder Input” denotes the inputs for image\ngeneration, where “Features” means continuous features, “Token” represents discrete tokens, “Text”\nimplies text prompts.\nDate\nDecoder\nInput\nDetec-\ntion\nDynamic\n-Res Img\nInput\nImage\nGen\nHigh-\nprecision\nEditing\nOpen-\nsource\nEmu\n07\/2023\nFeature\n×\n×\n✓\n×\n✓\nCM3Leon\n07\/ 2023\nToken\n×\n×\n✓\n×\n×\nSEED-OPT\n07\/ 2023\nToken\n×\n×\n✓\n×\n×\nLaVIT\n09\/2023\nToken\n×\n×\n✓\n×\n✓\nNExT-GPT\n09\/2023\nFeature\n×\n×\n✓\n×\n✓\nDreamLLM\n09\/2023\nFeature\n×\n×\n✓\n×\n×\nSEED-LLaMA\n10\/2023\nToken\n×\n×\n✓\n×\n✓\nVL-GPT\n12\/2023\nFeature\n×\n×\n✓\n×\n×\nGemini\n12\/2023\nToken\n×\n-\n✓\n×\n×\nEmu2\n12\/2023\nFeature\n×\n×\n✓\n×\n✓\nUnified-IO 2\n12\/2023\nToken\n✓\n×\n✓\n×\n✓\nMini-Gemini\n03\/2024\nText\n×\n×\n✓\n×\n✓\nSEED-X\n04\/2024\nFeature\n✓\n✓\n✓\n✓\n✓\nperform multimodal autoregression with a unified next-word-prediction objective or masked visual\ntoken prediction. Some research efforts [12, 25, 19] have delved into multimodal autoregression\nwith continuous representations, where each image in the multimodal sequence is tokenized into\nembeddings via a visual encoder, and then interleaved with text tokens for autoregressive modeling.\nDuring inference, the regressed visual embeddings will be decoded into an image by a visual decoder.\nAdditionally, some studies [17, 16] enable image generation in a non-autoregressive manner through\nutilizing learnable queries to obtain visual representations from MLLMs, which are further fed into a\nimage decoder to generate images. Mini-Gemini, generates text prompts using MLLMs and then\nleverages the existing SDXL [26] to output images.\nAlthough these work have achieved competitive results on various academic benchmarks, such as\nVQA and text-to-image generation, the accuracy and diversity of their generated content still fall\nshort of real-world needs, since they do not meet the requirements of modeling multi-granularity\nvisual semantics for comprehension and generation task. As shown in Tab. 1, we identify several\nsignificant characteristics essential for real-world applications including object detection and dynamic\nresolution image encoding for multi-granularity comprehension, as well as high-level instructional\nimage generation and low-level image manipulation for multi-granularity image generation. Notably,\nnone of the previous works fully support all of these characteristics. In this work, we present\nSEED-X, a unified and versatile foundation model, which effectively incorporate the aforementioned\ncharacteristics for real-world applications.\n3\nMethod\n3.1\nVisual Tokenization and De-tokenization\nIn SEED-X, we adopt a visual tokenizer to unify image comprehension and generation, and pre-train\na multi-granularity de-tokenizer to facilitate image generation and high-precision image manipulation\nin a two-stage manner. In the first stage, as shown in Fig. 3 (left), we utilize a pre-trained ViT as the\nvisual tokenizer and pre-train a visual de-tokenizer to decode realistic images by taking the features\nof the ViT as inputs in the first stage. Specifically, N visual embeddings from the ViT tokenizer\n(N = 64 after average pooling) are fed into a learnable module as the inputs of the U-Net of the\npre-trained SD-XL [26] (replacing the original text features). The learnable module consists of four\ncross-attention layers to connect the visual tokenizer and the U-Net. We optimize the parameters\nof the learnable module and keys and values within the U-Net on the images from JourneyDB [27],\nLAION-Aesthetics [28], Unsplash [29], and LAION-COCO [30]. As shown in Fig. 2, compared with\n4\nOriginal Image\nReconstructed Image\nEncode\nDecode\nNoise\nSemantic Consistency\nViT\nTokenizer\nZero\nPadding\nVisual Embeddings for LLM\nOriginal Image\nReconstructed Image\nEncode\nDecode\nNoise\nFine-grained\nSemantic Consistency\nViT\nTokenizer\nConditional\nImage\nVisual Embeddings for LLM\nFigure 3: Overview of visual tokenization and de-tokenization in SEED-X. In the first stage (left),\nwe pre-train a visual de-tokenizer, which can decode semantically consistent images by taking the\nfeatures of a pre-trained ViT as inputs. In the second stage (right), we fine-tune the visual de-tokenizer\nthrough concatenating the latent features of a conditional image with the noise to recover the fine-\ngrained details of the original image.\nSEED [15], our visual de-tokenizer can decode images that are more semantically aligned with the\noriginal images by taking the ViT features as inputs.\nIn the second stage, as shown in Fig. 3 (right), we further fine-tune the visual de-tokenizer to take\nan extra condition image as inputs for the retention of low-level details. Specifically, we follow\nInstructPix2Pix [31] to encode the condition image into the latent space via the VAE encoder, and\nconcatenate them with the noisy latent as the input of U-Net. The channel number of the U-Net\nconvolutional layer is expanded from 4 to 8, and all parameters of U-Net are optimized. We fine-tune\nthe visual de-tokenizer on MagicBrush [32] and in-house image editing data, as well as the pure\nimages in the first stage, where the conditional inputs are set to zeros. As shown in Fig. 2, by\nincorporating the condition image as an additional input besides the high-level image features, our\nvisual de-tokenizer can recover the fine-grained details of the original image.\n3.2\nDynamic Resolution Image Encoding\nCurrent MLLMs require to resize the input images to a pre-defined resolution (typically a square\nsize), which corresponds to the training resolution of the vision encoder, which can result in the loss\nof fine-grained information. In this work, we propose dynamic resolution image encoding to enable\nthe processing of images with arbitrary sizes and aspect ratios by dividing the image into a grid\ncomprising of sub-images. Specifically, for the visual encoder with the training resolution Ht × Wt,\nwe first up-sample the input image with the size H × W to the size of {Nh ∗Ht} × {Nw ∗Wt}. The\ngrid size Nh × Nw, are determined by\nmin\nNh ∗Nw,\ns.t. H ≤Nh ∗Ht\nand\nW ≤Nw ∗Wt.\n(1)\nWe also resize the original image to the size of Ht × Wt to provide global visual context. All\nsub-images and the resized global image are fed into the visual encoder to obtain the features, which\nare concatenated as the input of the LLM.\nTo enable the LLM to be aware of the positional information of each sub-image within the original\nimage, we add extrapolatable 2D positional embeddings to the visual features of each sub-image.\nSpecifically, for a sub-image with a normalized center location (xc, yc) in the grid, where 0.0 <\nxc, yc < 1.0, its learnable positional embedding p is computed:\np =xc ∗l + (1 −xc) ∗r + yc ∗t + (1 −yc) ∗b.\n(2)\nl, r, t, and b represent four learnable position embeddings indicating left, right, top and bottom\nrespectively. Consequently, our visual encoder can handle inputs with any arbitrary sizes and aspect\nratios, even if the image resolution was not encountered during training.\n5\nBOI\nIMG\n\/IMG\n5\n7\n3\n6\n1\n2\nLucky likes playing in the park\nIMG\n\/IMG\nLearnable queries \nSEED-X: Large Multimodal Model\n5\n7\n3\n6\n1\n2\nIMG\nEOI\nNext-word prediction\nViT\nTokenizer\nRegression\nTraining\nInference\nImage gridding to support arbitrary sizes and aspect ratios\nTraining sample:\nLucky likes playing\nin the park\n…\n…\nFigure 4: Overview of SEED-X for multimodal pre-training. Each image is divided into sub-images\nto support arbitrary sizes and aspect ratios, and their ViT features along with text tokens are fed into\nan LLM to perform next-word prediction and image feature regression between the output hidden\nstates of the learnable queries and ViT features. During inference, the regressed image features are\nfed into the visual de-tokenizer to decode images.\n3.3\nMultimodal Pre-training and Instruction Tuning\n3.3.1\nTraining Stage I: Multimodal Pre-training\nAs shown in Fig. 4, SEED-X adopts next-word prediction and image feature regression training\nobjectives on interleaved visual and textual data. Specifically, we perform dynamic resolution\nencoding of each image in the multimodal sequence, and their features along with text tokens are fed\ninto the pretrained LLM. In order to equip the model with detection and referencing abilities, we add\n224 bbox tokens, designated for representing bounding box coordinates, represented by <box_start>\n<loc-x_center> <loc-y_center> <loc-width> <loc-height> <box_end> with special tokens at the\nbeginning and end of the bounding box. The text and added bbox tokens are trained through\npredicting the next token with cross-entropy loss.\nWe employ N learnable queries (N = 64 to align with the visual de-tokenizer) to obtain the output\nvisual representations from the LLM, which are trained to reconstruct the features of the pre-trained\nViT tokenizer with a Mean Squared Error (MSE) loss. We add two special tokens ‘<IMG>’ and\n‘<\/IMG>’ to represent the beginning and the end of the query embeddings, and the ‘<IMG>’ is trained\nto predict where an image emerges. In doing so, we utilize the pre-trained ViT tokenizer as a bridge to\ndecouple the training of a visual de-tokenizer and the MLLM for image generation. During inference,\nthe regressed visual representations from SEED-X are fed into the visual de-tokenizer to decode\nrealistic images.\nWe pre-train SEED-X initialized from Llama2-chat-13B using LoRA on massive multimodal data,\nincluding image-captions pairs, grounded image-texts, interleaved image-text data, OCR data and\npure texts. We perform pre-training with 48 H800-80G GPUs (10 days) on a total of 158M samples.\nSee Appendix. A and Appendix. B for more details.\n3.3.2\nTraining Stage II: Multimodal Instruction Tuning\nWe perform multimodal instruction tuning through fine-tuning SEED-X using a LoRA module with\nboth public datasets and in-house data covering image editing, text-rich, grounded and referencing\nQA, and slide generation tasks. The details of datasets can be found in Appendix. A. We fine-\ntune SEED-X with conversational and image generation data to yield a general instruction-tuned\nmodel SEED-X-I, which can follow multimodal instructions and make responses with images, texts\n6\nTable 2: Comparison on multimodal understanding benchmarks. “Und.” and “Gen.” denote\n“understanding” and “generation”, respectively.\nType\nModel\nPOPE↑MME-P↑MMB↑SEED(img)↑VQAv2(test)↑GQA↑MMMU↑MM-Vet↑\nUnd. Only\nLLaVA-v1.5-Phi-1.5 [23]\n84.1\n1128.0\n-\n-\n75.3\n56.5\n30.7\n-\nMobileVLM [33]\n84.5\n1196.2\n53.2\n-\n-\n56.1\n-\n-\nMobileVLM-V2 [34]\n84.3\n1302.8\n57.7\n-\n-\n59.3\n-\n-\nLLaVA-Phi [35]\n85.0\n1335.1\n59.8\n-\n71.4\n-\n-\n28.9\nLLaVA [36]\n76.3\n809.6\n38.7\n33.5\n-\n-\n-\n25.5\nLLaVA-v1.5 [37]\n85.9\n1510.7\n64.3\n58.6\n78.5\n62.0\n35.4\n31.1\nInstructBLIP [38]\n-\n-\n36.0\n53.4\n-\n49.2\n-\n26.2\nIDEFICS-9B [39]\n-\n-\n48.2\n-\n50.9\n38.4\n-\n-\nQwen-VL-Chat [5]\n-\n1487.5\n60.6\n58.2\n78.2\n57.5\n-\n-\nUnd. and Gen. DreamLLM [17]\n-\n-\n-\n-\n72.9\n-\n-\n36.6\nLaVIT [20]\n-\n-\n-\n-\n66.0\n46.8\n-\n-\nEmu [18]\n-\n-\n-\n-\n52.0\n-\n-\n-\nNExT-GPT [40]\n-\n-\n-\n-\n66.7\n-\n-\n-\nGemini-Nano-1 [41]\n-\n-\n-\n-\n62.7\n-\n26.3\n-\nLWM [42]\n75.2\n-\n-\n-\n55.8\n44.8\n-\n9.6\nSEED-X\n84.1\n1457.0\n70.1\n66.5\n71.2\n49.1\n35.6\n43.0\nTable 3: Evaluation of text-to-image generation ability on GenEval benchmark. “Und.” and\n“Gen.” denote “understanding” and “generation”, respectively.\nType\nMethod\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall↑\nGen. Only\nLDM [52]\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 [52]\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nPixArt-α [53]\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nSDv2.1 [52]\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nDALL-E 2 [54]\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nSDXL [26]\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nUnd. and Gen.\nLWM [42]\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nSEED-X\n0.96\n0.65\n0.31\n0.80\n0.18\n0.14\n0.51\nand bounding boxes in multi-turn conversation. We further fine-tune the foundation model SEED-\nX on specialized datasets, resulting in a series of instruction-tuned models tailored for specific\ntasks, including SEED-X-Edit, SEED-X-PPT, SEED-X-Story and SEED-X-Try-on. The proficient\ncapabilities of these instruction-tuned model across various domains demonstrate the versatility of\nour pre-trained foundation model SEED-X. We perform instruction tuning on the foundation model\nSEED-X across different datasets, resulting in various models with distinct capabilities. Note that we\ndo not have an all-in-one instruction-tuned model that encompasses all abilities, which will be\nexplored for future work.\n4\nExperiments\n4.1\nQuantitative Evaluation\nMultimodal Comprehension. We evaluate the multimodal comprehension capabilities of SEED-X-I\non widely recognized image-based vision-language benchmarks, which include VQAv2 [43], GQA\n[44], POPE [45], MME [46], SEED [47], MMB [48], MM-Vet [49], and MMMU [50]. As listed\nin Tab. 2, SEED-X-I achieves competitive performance across various benchmarks, even when\ncompared to MLLMs specifically designed for multimodal comprehension.\nImage Generation. We evaluate the image generation capabilities of SEED-X-I on GenEval [51],\nwhich is a challenging benchmark to evaluate compositional image properties such as object co-\noccurrence, position, count, and color. As shown in Tab. 3, SEED-X obtains 51% overall accuracy,\ndemonstrating the model’s excellent instruction-following capabilities for image generation.\n7\nWhy is it said that GPT-4 is \nsafter than GPT-3.5?\nI want to build a two-story \ncabin in the woods, with many \ncommanding windows. Can \nyou help me draw a picture to \nshow a visualization of it?\nI love reading books. Can you add a \nbookshelf in the bedroom?\nShow me a picture of a \nbedroom inside the cabin.\nAbsolutely, I’d be happy to help \nyou visualize your two-story cabin \nin the woods. Here’s a picture.\nPlease explain your design.\nMy design is a two-story cabin nestled in \nthe heart of a dense forest. The cabin is \nconstructed from rustic wood, giving it a \nwarm and inviting appearance. It \nfeatures a wrap-around porch on the \nground floor, perfect for enjoying the \nfresh air and the sounds of nature. The \nupper floor is adorned with multiple \nwindows, offering breathtaking views of \nthe surrounding trees and wildlife. The \nroof is covered in shingles, providing a \nsturdy and durable protection against \nthe elements. The overall design of the \ncabin is simple yet elegant, blending \nseamlessly with its natural surroundings.\nHere’s a picture.\nAbsolutely, here is a picture.\nShow me the location of the bookshelf.\nI want to know tomorrow’s weather, \nwhich app should I use?\nYou should use the ‘Weather’ app.\nWhat’s the weather like tomorrow?\nTomorrow’s weather is predicted to be \nsunny with a high of 25°C and a low of 17°C.\nThe text states that GPT-4 is safer \nthan GPT-3.5 because it is 82% less \nlikely to respond to requests for \ndisallowed content and 40% more \nlikely to produce factual responses \nthan GPT-3.5 on their internal \nevaluations. This suggests that GPT-\n4 is designed with safety and \nfactuality in mind, making it a safer \nand more reliable tool for users.\nCan you draw a colorful anime character based on the sketch?\nYes, I can definitely create a colorful anime \ncharacter using the sketch as a reference.\nGenerate a slide with a title discussing the importance of \nphysical exercise. A vivid picture of someone running a \nbeautiful sunrise is displayed.\n{‘slide_height’: 6858000, ‘slide_width’: 9144000, ‘elements’: [{‘type’: ‘image’, \n‘image_path‘: \n, 'x': ' <loc-0> ', 'y': ' <loc-0> ', 'width': '<loc-224>', 'height': \n'<loc-224>'}, {'type': 'text', 'x': ' <loc-38> ', 'y': ' <loc-25> ', 'width': ' <loc-138> ', \n'height': ' <loc-143> ', 'paragraphs': [{'texts': 'A healthy body is the key to a \nhealthy mind.', 'font_size': 32.0}]}, {'type': 'text', 'x': ' <loc-29> ', 'y': ' <loc-90> ', \n'width': ' <loc-185> ', 'height': ' <loc-71> ', 'paragraphs': [{'texts': 'The idea of \nphysical exercise is to maintain a balanced life.', 'font_size': 28.0}]}]}\nWhat should my 4-year-old daughter wear on next \nTuesday for an outing? Recommend an outfit.\nA cute and comfortable outfit could be a \nblue dress with a white polka dot pattern.\nDo you know this novel?\nYes, “The Three-Body Problem” is a science \nfiction novel by Chinese author Liu Cixin. It is \nthe first book in the “Three-Body Trilogy” \nand is known for its unique blend of science \nfiction and Chinese culture. The novel \nexplores the first contact between humans \nand an alien civilization from a distant star \nsystem, and delves into complex themes \nsuch as the nature of intelligence, the limits \nof scientific knowledge, and the \nconsequences of advanced technology. \nAbsolutely, please enjoy \nthis illustration.\nCan you \ncreate an \nillustration \nfor this \nnovel that \ndepicts its \ninnovative \nstory?\nDo you know where this photo was taken?\nThis photo was taken in Neuschwanstein Castle, which is \nlocated on the Schwanen mountain in Germany.\nThe composition of the photo is quite striking. The \nNeuschwanstein Castle, a large, white, fairy-tale-like castle, is \nperched on top of a hill. It’s surrounded by lush green trees \nand overlooks a beautiful landscape. The castle’s towers and \nturrets add a sense of grandeur and majesty to the scene. \nThe sky is a clear blue, and the overall mood of the photo is \nserene and picturesque. The composition of the photo, with \nthe castle as the focal point, creates a sense of depth and \nperspective, drawing the viewer’s eye towards the castle and \nthe breathtaking view beyond.\nI love this photo. Can you explain its composition?\nHere is a picture.\nCan you turn this photo into a watercolor painting?\nFigure 5: Examples of what SEED-X can do in real-world scenarios after different instruction tuning\nthrough unifying multi-granularity comprehension and generation. Our instruction tuned models can\nfunction as an interactive designer, generating images without descriptive captions while illustrating\ncreative intent, and showcasing visualizations of modified images based on user’s intent. They can act\nas knowledgeable personal assistants, comprehending images of arbitrary sizes and offering relevant\nsuggestions in multi-turn conversations.\n8\nAdorn the dog \nin the right with \nsunglasses.\nMake it under\na beautiful\nsunset.\nTurn them\ninto cartoon\ncharacters.\nRemove the\ndog from\nthe image.\nEmu2-Gen\nGemini\nSEED-X-Edit\nMGIE\nInput Image\nMini-Gemini\nFigure 6: Qualitative comparison between MLLMs for image manipulation. SEED-X-Edit shows\nenhanced ability in adhering to instructions while preserving low-level details of input images. The\nblack images result from Gemini’s inability to display human images.\n4.2\nQualitative Evaluation\n4.2.1\nApplications in the Real World.\nSEED-X can be effectively instruction tuned to function as various multimodal AI assistants in the real\nworld across different domains after integrating two enhanced features, including the comprehension\nof images of arbitrary sizes and ratios, and multi-granularity image generation, encompassing both\nhigh-level instructional image generation and low-level image manipulation tasks. As shown in Fig. 1\nand Fig. 5, our instruction tuned models can serve as an interactive designer, which can generate\nimages without descriptive captions while illustrate creative intent, and showcase visualizations of\nmodified images. For example, it can explain the design idea of concept image for AGI and a two-\nstory cabin. It can create an imaginative illustration for the novel without the need of describing the\nscene with languages. It can further offer modification suggestions of the user’s room and showcase\nthe visualization. Additionally, the instruction tuned models can act as an knowledgeable personal\nassistant, comprehending images of arbitrary sizes and providing relevant suggestions. For example,\nit can identify foods suitable for fat reduction in the refrigerator, display appropriate clothing based\non the screenshot of weather forecasts.\n4.2.2\nImage Generation and Manipulation.\nWe compare previous MLLMs that are capable of generating images for text-to-image generation in\nFig. 9 of Appendix. Our instruction tuned model can generate images that are more aligned with the\nelements in the caption and possess artistic qualities. Through utilizing a pre-trained ViT Tokenizer\nas the bridge to decouple the training of visual de-tokenizer and the MLLM, our pre-trained model\nSEED-X can effectively realize high-quality image generation, which is a fundamental capability to\nbe applied in real-world scenarios.\nWe compare image manipulation with previous MLLMs including Emu2-Gen [25], Gemini [41],\nMGIE [41] and Mini-Gemini [55]. As shown in Fig. 6, we can observe that SEED-X-Edit can\nmore effectively adhere to editing instructions while maintaining the low-level details of the input\nimage. For instance, SEED-X-Edit can accurately add sunglasses to the dog on the right, while both\nEmu2-Gen and MGIE fail to follow the instruction, resulting in sunglasses being added to both dogs.\n9\noriginal image\n256 tokens\n64 tokens\n64 tokens fully fine-tune\n32 tokens\nFigure 7: Ablation study on the number of visual tokens and trainable parameters for training visual\nde-tokenizer.\nAdditionally, SEED-X-Edit successfully eliminates the dog in the baby image while preserving the\nbackground details and the baby’s features. In contrast, Emu2-Gen fails to retain the fine details of\nthe input image, and MGIE is unsuccessful in removing the dog. Note that Gemini lacks the ability\nto edit images as it retrieves images on the Internet. Here the presence of black images is due to\nits failure to display images related to human portraits. Mini-Gemini generates text prompts as the\ninput of a pre-trained SDXL model, which can not preserve the visual details of the input image. The\nexamples show the effectiveness of our instruction model for high-precision image manipulation. Our\nMLLM accurately predicts visual semantic representations based on an input image and a language\ninstruction, which serve as input for the U-Net. The visual de-tokenizer can further condition on the\ninput image, ensuring the preservation of fine-grained details in the decoded images.\n4.2.3\nMultimodal Comprehension.\nWe provide qualitative examples of multimodal comprehension by SEED-X-I in Fig. 10 and Fig. 11\nof Appendix. SEED-X-I can realize fine-grained object detection and perception, text-rich compre-\nhension, fundamental mathematical computation, world-knowledge and commonsense reasoning,\ndiagram understanding, which are crucial capabilities for its application in real-world scenarios.\n4.3\nAblation Study\nIn this section, we perform ablation studies on the training of our visual de-tokenizer and the\npre-training of SEED-X to enable a MLLM for image generation.\nFor visual de-tokenization, N visual embeddings (after average pooling) from the ViT tokenizer are\nfed into a learnable module as the inputs of the U-Net of the pre-trained SD-XL. We perform an\nablation study on the number of visual tokens and the learnable parameters of the SD-XL U-Net,\nwhere keys and values within the U-Net are optimized if not specified with “fully fine-tune”. As\nshown in Fig. 7, we can observe that more visual tokens can result in better reconstruction of the\noriginal images. For example, the decoded images from 256 visual embeddings can recover the\ncharacters’ postures of the original images, while decoded images from 32 visual embeddings have\nalready lost the original structure of the scene. We further observe that fully fine-tuning the parameters\nof the SD-XL U-Net can lead to distortions in image details, such as the woman’s feet, compared to\nonly training the keys and values within the U-Net. In SEED-X, we use N = 64 visual embeddings to\ntrain the visual de-tokenizer and only optimize the keys and values within the U-Net (See below for\nan explanation of why we do not choose N = 256).\nTo enable MLLM for image generation, we employ N learnable queries to obtain the output visual\nrepresentations from the LLM, which are trained to reconstruct N visual embeddings from the ViT\ntokenizer with a learnable module. We first perform an ablation study on the number of learnable\nqueries. The images generated by the MLLM based on the input caption are shown in Fig. 8. We can\nobserve that using 256 learnable queries to reconstruct 256 visual embeddings can lead to distortion\n10\n256 tokens\n64 tokens\nMulti-layer Resampler\nFine-tune de-tokenizer\nAn astronaut riding a \nhorse in the forest.\nA dog flying in the sky.\nFigure 8: Ablation study on the number of visual tokens, model architecture and optimization targets\nduring pre-training SEED-X for image generation.\nin the generated images compared with N = 64. This occurs because regressing more visual features\nis more challenging for the model, even though 256 visual embeddings from the de-tokenizer can\nbetter reconstruct images, as demonstrated in the previous ablation study. We also observe that,\ncompared to learning a one-layer cross-attention for reconstructing image features, a multi-layer\nresampler (multi-layer cross-attention) yields less satisfactory performance, which can happen due\nto the lack of more direct regularizations on the hidden states of the LLM. We further optimize the\nvisual de-tokenizer by using the reconstructed visual embeddings from the MLLM as input instead\nof ViT features, but the generated images exhibit a more monotonous appearance. It demonstrates\nthe effectiveness of utilizing the ViT Tokenizer as the bridge to decouple the training of visual\nde-tokenizer and the MLLM for image generation.\n5\nConclusion\nWe present SEED-X, a versatile foundation model, which can serve as various multimodal AI\nassistants in the real world after instruction tuning. In order to make a multimodal foundation\nmodel applicable in open-world context, we integrate two enhanced features into SEED-X including\nimage comprehension of arbitrary sizes and ratios, and multi-granularity image generation, which\nencompasses both high-level instructional image generation and low-level image manipulation. We\nhope that SEED-X can inspire future research into the potential of MLLMs in the real-world scenarios\nthrough unifying multi-granularity comprehension and generation.\nReferences\n[1] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. ICML, 2023.\n[2] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[4] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\nJingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023.\n[6] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. arXiv preprint arXiv:2310.03744, 2023.\n11\n[7] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui\nDing, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: A vision-language large\nmodel for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023.\n[8] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao,\nKeqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal\nlarge language models. arXiv preprint arXiv:2311.07575, 2023.\n[9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[12] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang.\nGenerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023.\n[13] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu,\nBinh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining\nand instruction tuning. arXiv preprint arXiv:2309.02591, 2023.\n[14] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041, 2023.\n[15] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see\nand draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n[16] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm.\narXiv preprint arXiv:2309.05519, 2023.\n[17] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,\nHongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv\npreprint arXiv:2309.11499, 2023.\n[18] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang.\nGenerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023.\n[19] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying\nShan. Vl-gpt: A generative pre-trained transformer for vision and language understanding and generation.\narXiv preprint arXiv:2312.09251, 2023.\n[20] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\nXiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv\npreprint arXiv:2309.04669, 2023.\n[21] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and\nAniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language,\naudio, and action. arXiv preprint arXiv:2312.17172, 2023.\n[22] Chameleon Team.\nChameleon:\nMixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024.\n[23] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao\nGu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify\nmultimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024.\n[24] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze\nXie, Hongxu Yin, Li Yi, et al. Vila-u: a unified foundation model integrating visual understanding and\ngeneration. arXiv preprint arXiv:2409.04429, 2024.\n[25] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming\nRao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv\npreprint arXiv:2312.13286, 2023.\n[26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023.\n12\n[27] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou,\nZipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. Advances in\nNeural Information Processing Systems, 36, 2024.\n[28] Christoph Schuhmann and Romain Beaumont.\nLaion-aesthetics.\nhttps:\/\/laion.ai\/blog\/\nlaion-aesthetics\/, 2022.\n[29] Zahid Ali, Chesser Luke, and Carbone Timothy. Unsplash. https:\/\/github.com\/unsplash\/datasets,\n2023.\n[30] Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion-coco:\n600m synthetic captions from laion2b-en. https:\/\/laion.ai\/blog\/laion-coco\/, 2023.\n[31] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npages 18392–18402, 2023.\n[32] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset\nfor instruction-guided image editing. arXiv preprint arXiv:2306.10012, 2023.\n[33] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang,\nBo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for\nmobile devices. arXiv preprint arXiv:2312.16886, 2023.\n[34] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu,\nXinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model.\narXiv preprint arXiv:2402.03766, 2024.\n[35] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient\nmulti-modal assistant with small language model. arXiv preprint arXiv:2401.02330, 2024.\n[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural\ninformation processing systems, 36, 2024.\n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages\n26296–26306, 2024.\n[38] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning, 2023.\n[39] Hugo Laurençon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth\nKaramcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, and et al. Introducing idefics: An open\nreproduction of state-of-the-art visual language model, 2023.\n[40] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm.\narXiv preprint arXiv:2309.05519, 2023.\n[41] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[42] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language\nwith ringattention. arXiv preprint arXiv:2402.08268, 2024.\n[43] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\n[44] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE\/CVF conference on computer vision and\npattern recognition, pages 6700–6709, 2019.\n[45] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\n[46] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,\nKe Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language\nmodels. arXiv preprint arXiv:2306.13394, 2023.\n[47] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[48] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv\npreprint arXiv:2307.06281, 2023.\n13\n[49] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and\nLijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490, 2023.\n[50] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding\nand reasoning benchmark for expert agi. In Proceedings of the IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9556–9567, 2024.\n[51] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for\nevaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024.\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pages 10684–10695, 2022.\n[53] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\nKwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic\ntext-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.\n[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n[55] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu,\nand Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint\narXiv:2403.18814, 2024.\n[56] Schuhmann Christoph, Köpf Andreas, Vencu Richard, Coombes Theo, and Beaumont Romain. Laion\ncoco: 600m synthetic captions from laion2b-en. [EB\/OL], 2022. https:\/\/laion.ai\/blog\/laion-coco\/.\n[57] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the\nIEEE\/CVF International Conference on Computer Vision, pages 4015–4026, 2023.\n[58] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou,\nZipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. arXiv preprint\narXiv:2307.00716, 2023.\n[59] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu.\nCapsfusion: Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550, 2023.\n[60] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of\nimages interleaved with text. arXiv preprint arXiv:2304.06939, 2023.\n[61] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas\nWang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics:\nAn open web-scale filtered dataset of interleaved image-text documents, 2023.\n[62] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training\nlarge autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\n[63] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar:\nEnhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107,\n2023.\n[64] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.\n[65] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.\nMathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv\npreprint arXiv:1905.13319, 2019.\n[66] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for\nquestion answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022.\n[67] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A\ndiagram is worth a dozen images. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 235–251. Springer, 2016.\n[68] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507–2521, 2022.\n[69] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware\nvisual question answering. In Proceedings of the AAAI conference on artificial intelligence, volume 33,\npages 8876–8884, 2019.\n14\n[70] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations\nvia question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 5648–5656, 2018.\n[71] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\nSharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793,\n2023.\n[72] Chen Li, Yixiao Ge, Dian Li, and Ying Shan. Vision-language instruction tuning: A review and analysis.\narXiv preprint arXiv:2311.08172, 2023.\n[73] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe:\nPrompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023.\n[74] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and\nLifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning. arXiv preprint\narXiv:2402.11690, 2024.\n[75] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong\nChen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for a lite\nvision-language model, 2024.\n[76] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified\nimage classification, object detection, and visual relationship detection at scale. International journal of\ncomputer vision, 128(7):1956–1981, 2020.\n[77] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin,\nRoss Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings\nof the 2016 conference of the North American chapter of the association for computational linguistics:\nHuman language technologies, pages 1233–1239, 2016.\n[78] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Viton-hd: High-resolution virtual try-on\nvia misalignment-aware normalization. In Proceedings of the IEEE\/CVF conference on computer vision\nand pattern recognition, pages 14131–14140, 2021.\n15\nTable 4: Overview of the pre-training and instruction tuning datasets.\nType\nDataset\nPre-training\nImage-Caption\nLAION-COCO [56] (Re-caption), SAM [57] (Re-caption), Unsplash [29],\nLAION-Aesthetics[28], JourneyDB [58], CapFusion [59],\nGrounded Image-Caption GRIT [4]\nInterleaved Image-Text\nMMC4 [60], OBELICS [61], OpenFlamingo [62]\nOCR\nLLaVAR [63], Slides (In-house)\nPure Text\nWikipedi\nInstruction Tuning\nVQA\nLLaVAR [63], Text-rich QA (In-house), MIMIC-IT [64], MathQA [65],\nChartQA [66], AI2D [67], ScienceQA [68], KVQA [69],\nDVQA [70], Grounded QA (In-house), Referencing QA (In-house)\nConversation\nLLaVA-150k [36], ShareGPT [71], VLIT [72],\nLVIS-Instruct4V [73], Vision-Flan [74], ALLaVA-4V [75]\nImage Generation\nLAION-COCO [56] (Re-caption), SAM [57] (Re-caption), Unsplash [29],\nLAION-Aesthetics[28], JourneyDB [58]\nImage Editing\nInstructpix2pix [31], MagicBrush [32], Openimages [76]-editing (In-house),\nUnsplash [29]-editing (In-house)\nSlides Generation\nIn-house data\nStory Telling\nVIST [77]\nVirtual Try-on\nVITON-HD [78]\nA\nPre-training and Instruction Tuning Datasets\nAs listed in Tab. 4, we pre-train SEED-X and conduct instruction tuning on a large variety of both\npublic datasets and in-house data. For multimodal pre-training, we utilize image-caption pairs,\ngrounded image-caption pairs, interleaved image and text content, OCR data and pure text data. The\nimages of LAION-COCO [56] and SAM [57] are re-captioned for a more detailed descriptive caption\nto improve both image comprehension and generation.\nFor instruction tuning, we utilize various public VQA datasets, and further curate text-rich QA,\ngrounded and referencing QA to enhance the model’s capability of comprehending text-rich images\nand detecting objects that requires reasoning. We use multiple conversational datasets, which are\nspecifically collected for MLLMs with open-form text output. We use the same image-caption\npairs as in the pre-training phase to maintain the model’s ability to generate images. For the\nimage manipulation, since the high-precision editing dataset MagicBrush [32] is only at the level of\nthousands, we employ a series of models to collect a dataset of millions of image editing examples,\nwhich are used for both training the visual de-tokenizer and SEED-X-Edit. We further collected data\non slides, obtaining images, captions, and layouts for training slide generation.\nB\nImplementation Details\nVisual Tokenization and De-tokenization. We use the visual encoder from Qwen-vl [5] as the\nViT Tokenizer and adopt 1D average pooling to obtain N = 64 visual embeddings. These visual\nembeddings are fed into four layers of cross-attention as the input of the U-Net initialized from\nSDXL [26]. In the first stage, we optimize the parameters of the cross-attention layers and the keys\nand values within the U-Net on the images from JourneyDB [27], LAION-Aesthetics [28], Unsplash\n[29], and LAION-COCO [30]. We train the visual de-tokenizer on 32 A100-40G GPUs with 42K\ntraining steps, where the learning rate is set to 1e-4 with cosine decay.\n16\nIn the second stage, we encode the condition image into the latent space via the VAE encoder, and\nconcatenate them with the noisy latent as the input of U-Net. The channel number of the U-Net\nconvolutional layer is expanded from 4 to 8, and all parameters of U-Net are optimized. We pre-train\nthe visual conditioner on MagicBrush [32] and in-house image editing data, as well as the image-\ncaption pairs in the first stage, where the conditional inputs are set to zeros. We fine-tune the visual\nde-tokenizer on 32 A100-40G GPUs with 30K training steps, where the learning rate is set to 1e-4\nwith cosine decay.\nMultimodal Pre-training and Instruction Tuning. We utilize the visual encoder from Qwen-vl [5]\nas the ViT Tokenizer and initialize a cross-attention layer to obtain N = 64 visual embedding as\nthe input of the LLM initialized from Llama2-chat-13B. We initialize N = 64 learnable queries\nand the output hidden states from them are fed into a cross-attention layer to reconstruct N = 64\nvisual embeddings from the ViT Tokenizer. We optimize the LLM using LoRA and optimize the\nparameters of the input cross-attention layer, output cross-attention layer, extrapolatable 2D positional\nembeddings, and LoRA on image-captions pairs, grounded image-texts, interleaved image-text data,\nOCR data and pure texts. We perform pre-training with 48 H800-80G GPUs (10 days) on a total of\n158M samples, where the learning rate is set to 1e-4 with cosine decay.\nFor the instruction tuning, we fine-tune a LoRA module on the pre-trained model, and optimize the\nparameters of the input cross-attention layer, output cross-attention layer, extrapolatable 2D positional\nembeddings, and LoRA. We fine-tune SEED-X with conversational and image generation data to yield\na general instruction-tuned model SEED-X-I. We further fine-tune SEED-X on specialized datasets,\nresulting in a series of instruction-tuned models tailored for specific tasks, including SEED-X-Edit,\nSEED-X-PPT, SEED-X-Story and SEED-X-Try-on.\nC\nQualitative Examples\nText-to-image Generation, Fig. 9 visualizes the comparison between MLLMs for text-to-image gen-\neration including Next-GPT [16], SEED-LLaMA-I[15], Emu2-Gen [25] and Gemini [41]. Compared\nwith previous MLLMs, our instruction tuned model can generate images that are more aligned with\nthe elements in the descriptive caption and possess artistic qualities. For example, images generated\nby SEED-X-I vividly and accurately depicts “person standing in a small boat”, “a gleaming sword\non its back”, “an oriental landscape painting”, “tiger with vivid colors” in the captions. Through\nutilizing a pre-trained ViT Tokenizer as the bridge to decouple the training of visual de-tokenizer and\nthe MLLM, our pre-trained model SEED-X can effectively realize high-quality image generation,\nwhich is a fundamental capability for applying multimodal models in real-world scenarios.\nImage Manipulation. We compare image manipulation with previous MLLMs including Emu2-\nGen [25], Gemini [41], MGIE [41] and Mini-Gemini [55]. Language-guided image manipulation\npresents a significant challenge as the model must be capable of comprehending free-form instructions\nand generating images with the low-level details of the input image preserved. As shown in Fig. 6, we\ncan observe that SEED-X-Edit can more effectively adhere to editing instructions while maintaining\nthe low-level details of the input image. For instance, SEED-X-Edit can accurately add sunglasses\nto the dog on the right, while both Emu2-Gen and MGIE fail to follow the instruction, resulting in\nsunglasses being added to both dogs. Additionally, SEED-X-Edit successfully eliminates the dog\nin the baby image while preserving the low-level background details and the baby’s features. In\ncontrast, Emu2-Gen fails to retain the fine details of the input image, and MGIE is unsuccessful in\nremoving the dog. Note that Gemini lacks the ability to edit images as it retrieves images on the\nInternet. Here the presence of black images is due to its failure to display images related to human\nportraits. Mini-Gemini generates text prompts as the input of a pre-trained SDXL model, which\ncan not preserve the visual details of the input image. The examples demonstrate the effectiveness\nof our instruction model for high-precision image manipulation. Our MLLM accurately predicts\nvisual semantic representations based on an input image and a language instruction, which serve as\ninput for the U-Net. The visual de-tokenizer can further condition on the input image, ensuring the\npreservation of fine-grained details in the decoded images.\nMultimodal Comprehension We show qualitative examples of multimodal comprehension by SEED-\nX-I in Fig. 10 and Fig. 11. SEED-X-I can realize fine-grained object detection and perception, text-\nrich comprehension, fundamental mathematical computation, world-knowledge and commonsense\nreasoning, diagram understanding, which are crucial capabilities for its application in the real world.\n17\nA lonely person standing in a small boat, floating in the vast ocean, surrounded \nby thick fog in the sky, giving a sense of confusion and helplessness.\nA fearless cat, with a gleaming sword on its back.\nThe golden moment of sunrise, Huangshan, China, towering Qishi Peak, a \nlarge area of clouds, a small welcome pine, an oriental landscape painting.\nA fierce cartoon tiger, with furrowed brows and bared teeth, \ndisplays its anger through vivid colors and exaggerated features.\nEmu2-Gen\nGemini\nSEED-X-I\nSEED-LLaMA-I\nNext-GPT\nFigure 9: Qualitative comparison between MLLMs for text-to-image generation. SEED-X-I is\ncapable of generating images that are more closely aligned with the elements in the descriptive\ncaption and exhibit artistic qualities.\n18\nDetect the boat in the image.\nCount the number of the people.\nThere are 11 people in the image.\nWhat is the size of the second bedroom?\nThe second bedroom is 10’10\" x 14’7\".\nWhat are all the scene text \nin the image?\nHere are the order details for my taxi \nride. Should I actually pay $29.42? \nThe scene text is “THE EARTH \nWITHOUT ART IS JUST ‘EH’”.\nBased on the image, the total amount \ndue is $32.42, which is the sum of the \nbase fare, mileage fee, duration fee. \nThe discount is $3, which is subtracted \nfrom the total, resulting in the actual \npaid amount of $29.42. Therefore, you \nshould pay $29.42 for the taxi ride.\nWhat historical event is shown \nin the picture?\nThe picture shows the famous 1927 \nSolvay Conference, which was a \nlandmark meeting in the history of \nphysics. It was attended by many of the \nmost prominent physicists of the 20th \ncentury, including Albert Einstein, Niels \nBohr, Werner Heisenberg, Erwin \nSchrödinger, and Marie Curie. The \nconference was held in Brussels, \nBelgium, and focused on the \ndevelopment of quantum mechanics.\nFigure 10: Qualitative examples of multimodal comprehension by SEED-X-I.\n19\nCan this be true in the real world?\nThe meme is comparing the quality of \nhandwriting in the first two pages of \nan exam and the last two pages. It \nsuggests that the quality of \nhandwriting often deteriorates as the \nexam progresses. This is a humorous \nway to express the common \nexperience of writing neatly at the \nbeginning of an exam and becoming \nless neat as the time progresses.\nIs there anything in the image that can protect \nme from catching the flu virus when I go out?\nYes, the black face mask located at green\nbbox can help protect you from catching \nthe flu virus when you go out.\nNo, this is not possible in the real \nworld. The image shows a stack of \ndice that are not physically \nconnected, yet they are arranged in a \nway that suggests they are connected. \nThis is an optical illusion, and the dice \nare not actually connected.\nWhat is the meaning of the meme?\nPlease explain the method in details.\nThe method in the image is a \nConvolutional Neural Network (CNN) \nused for image classification. The process \nstarts with the input image on the left, \nwhich is a green snake. This input goes \nthrough a convolutional layer, indicated \nby the green arrow pointing downwards. \nThis convolutional layer is followed by a \nmax pooling layer. The output of the max \npooling layer feeds into a fully connected \nlayer. This fully connected layer leads to \nthe output of the network, which is the \nclassification of the input image. The \nfinal output of the classification step is \nthe classification of the input image, \nwhich is a green snake.\nFigure 11: Qualitative examples of multimodal comprehension by SEED-X-I.\n20\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation.pdf"}
{"title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models","authors":"Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia","summary":"In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https:\/\/github.com\/dvlab-research\/MiniGemini.","url":"http:\/\/arxiv.org\/abs\/2403.18814v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2403.18814v1","published":1711562344000,"comment":"Code and models are available at\n  https:\/\/github.com\/dvlab-research\/MiniGemini","pdf_text":"Mini-Gemini: Mining the Potential of Multi-modality\nVision Language Models\nYanwei Li1∗\nYuechen Zhang1∗\nChengyao Wang1∗\nZhisheng Zhong1\nYixin Chen1\nRuihang Chu1\nShaoteng Liu1\nJiaya Jia1,2\nThe Chinese University of Hong Kong1\nSmartMore2\nAbstract\nIn this work, we introduce Mini-Gemini, a simple and effective framework enhanc-\ning multi-modality Vision Language Models (VLMs). Despite the advancements\nin VLMs facilitating basic visual dialog and reasoning, a performance gap persists\ncompared to advanced models like GPT-4 and Gemini. We try to narrow the gap\nby mining the potential of VLMs for better performance and any-to-any workflow\nfrom three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-\nguided generation. To enhance visual tokens, we propose to utilize an additional\nvisual encoder for high-resolution refinement without increasing the visual token\ncount. We further construct a high-quality dataset that promotes precise image\ncomprehension and reasoning-based generation, expanding the operational scope\nof current VLMs. In general, Mini-Gemini further mines the potential of VLMs and\nempowers current frameworks with image understanding, reasoning, and generation\nsimultaneously. Mini-Gemini supports a series of dense and MoE Large Language\nModels (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance\nin several zero-shot benchmarks and even surpasses the developed private models.\nCode and models are available at https:\/\/github.com\/dvlab-research\/MiniGemini.\nWhat should I do after kneading the\ndough, before dividing it into pieces?\nPut in warm place.\n15-inch MacBook Pro\n13-inch MacBook Pro\nFeature\nIntel Core i7\nIntel Core i5\nProcessor\nIntel Iris Pro Graphics\nIntel Iris Graphics 6100\nGraphics\nUp to 8 hours\nUp to 10 hours\nBattery Life\nMulti-Touch trackpad\nForce Touch trackpad\nTrackpad\n2.2GHz, 2.5GHz\n2.7GHz, 2.9GHz\nSpeed\n16GB\n8GB\nMemory\n256GB, 512GB\n128GB, 256GB, 512GB\nStorage\n$2,688.00, $3,288.00\n$1,688.00, $1,988.00, $2,388.00\nPrice\n!\nExplain why this meme is \nfunny, and generate a \npicture when the \nweekend coming.\nPlease show me a similar \nartwork, but in the \ngrassland background.\nShow me one idea of what I \ncould make with this.\nGive me an advice where to \ntravel to China.\nChina is a vast and diverse\ncountry with many different\nregions to explore. Some pop-\nular destinations for travelers\ninclude Beijing, Shanghai, Xi’-\nan, Chengdu, and Guilin. Bei-\njing is the capital and offers a\nglimpse into China's rich his-\ntory and culture…\nMMMU\n(test)\nMME\nMMB\nTextVQA\nMM-Vet\nLLaVA-NeXT\nMini-Gemini\nGemini Pro\nGeneration with Reasoning\nHigh-Resolution Understanding\nList the difference of two computers in this\nimage in two columns and compare one by\none\nHow long should I put in warm place?\n1h.\nStep\n3\n4\n5\nMining…\nBenchmark Performance\n2141\n(1659\/482)\n80.6\n74.1\n74.6\n64.3\n59.3\n44.9\n44.7\n57.4\n69.5\n79.6\n75.2\n1933\n1928\nLLaVA-1.5 baseline\nFigure 1: Mini-Gemini is advanced in various vision-related tasks.\n∗equal contribution\narXiv:2403.18814v1  [cs.CV]  27 Mar 2024\n1\nIntroduction\nWith the rapid evolution in Large Language Models (LLMs) [1–3], empowering the impressive\ncapabilities for multi-modality inputs is becoming an essential part of current Vision Language\nModels (VLMs) [4, 5]. To bridge the modality gap, several studies are conducted to marry vision\nwith LLMs from images [6–8] to videos [9, 10]. Despite these advancements, a significant gap\nremains between academic initiatives and the prowess of well-established models like GPT-4 [4] and\nGemini [5], which are trained with huge amounts of data and resources.\nFor vision itself, image resolution is a core part of explicitly despite the surrounding environment\nwith minimal visual hallucination. To this end, more attempts are performed to further improve\nthe visual understanding in current VLMs. For instance, LLaVA-Next [11] and Otter-HD [12] are\nproposed to enhance the ability based on previous work [7, 13] by improving the image resolution.\nIncreasing the number of visual tokens with higher resolution images undeniably enriches visual\nembeddings in LLMs. However, this improvement comes with escalated computational demands\nand associated costs, particularly when processing multiple images. Moreover, the existing data\nquality, model capabilities, and application scopes remain inadequate for accelerated training and\ndevelopment processes. This scenario prompts a critical inquiry: how to push forward the VLMs\napproaching well-developed models with acceptable cost in an academic setting?\nTo answer this question, we explore the potential of VLMs from three strategic aspects, i.e., efficient\nhigh-resolution solution, high-quality data, and expanded applications. Firstly, we utilize ConvNet\nto efficiently generate higher-resolution candidates, thus enhancing visual detail while maintaining\nthe visual token count for LLMs. To bolster data quality, we amalgamate high-quality datasets\nfrom diverse public sources, ensuring a rich and varied data foundation. Furthermore, our approach\nintegrates these enhancements with cutting-edge LLMs and generative models, aiming to elevate\nVLM performance and user experience. This multifaceted strategy enables us to delve deeper into the\ncapabilities of VLMs, achieving significant advancements within manageable resource constraints.\nIn general, our method employs an any-to-any paradigm, which is adept at handling both image and\ntext as input and output. In particular, we introduce an efficient visual token enhancement pipeline for\ninput images, featuring a dual-encoder system. It comprises twin encoders, one for high-resolution\nimages and the other for low-resolution visual embedding, mirroring the cooperative functionality\nof the Gemini constellation. During inference, they work in an attention mechanism, where the\nlow-resolution one generates visual queries, and the high-resolution counterpart provides candidate\nkeys and values for reference. To augment the data quality, we collect and produce more data based\non public resources, including high-quality responses [14, 15], task-oriented instructions [16–19], and\ngeneration-related data [20, 21]. The increased amount and quality improve the overall performance\nand extend the capability of model. Additionally, our model supports concurrent image and text\ngeneration, facilitated by the seamless integration of our VLM with advanced generative models [22].\nIt leverages VLM guidance for image generation by providing the generated text from LLMs.\nThe Mini-Gemini framework, can be easily instantiated with a range of LLMs from 2B to 34B\nparameter scales, as detailed elaborated in Section 3. Extensive empirical studies are conducted\nin Section 4 to reveal the effectiveness of the proposed method. Remarkably, our approach attains\nleading performance in various settings and even surpasses the well-developed Gemini Pro [5],\nQwen-VL-Plus [23], and GPT 4V [4] in the complex MMB [24] and MMU [25] dataset, respectively.\nThese results underscore Mini-Gemini’s potential to set new benchmarks in the realm of VLMs,\nhighlighting its advanced capabilities in handling complex multi-modal tasks.\n2\nRelated Work\nLarge Language Models.\nRecent progress in Natural Language Processing (NLP) has been dra-\nmatically accelerated by advancements in large language models (LLMs). The seminal introduction\nof the Transformer framework [26] served as a cornerstone, enabling a new wave of language models\nsuch as BERT [27] and OPT [2] that exhibit profound linguistic understanding. The inception of the\nGenerative Pre-trained Transformer (GPT) [28] introduced a novel paradigm through auto-regressive\nlanguage modeling, establishing a robust method for language prediction and generation. The emer-\ngence of models like ChatGPT [1], GPT-4 [4], LLaMA [3], and Mixtral [29] further exemplified\nthe field’s rapid evolution, each demonstrating enhanced performance on complex language pro-\n2\n…\n…\nHR\nHR Region\nWindow\nDual Vision\nEncoders\nVision Input\nHR Vision\nEncoder\nLR Vision\nEncoder\nFlatten\nCross\nAttention\nDownsample\nLLM\nHR\nLR\nMined\nTokens\nText Input\nTokens\n…\nGenerated Tokens\n…\nUser Text Input\nAuto-regressive Generation\nSDXL\n(optional)\nOutput\nImage\nRegion-wise\nFlatten\nPatch Info Mining\nOutput\nText\nGeneration?\nThis\nimage\ndepicts\n…\nHR Flow\nLR Flow\nLanguage Flow\n!\n!\n!&# (&×(!)\n*(&)\nVisual Embedding\nFeature Map\nX\"\nX#\n,$(&)\nX′\"\nX′#\n,% (.)\nLR\nFigure 2: The framework of Mini-Gemini with any-to-any workflow.\ncessing tasks, attributable to their training on extensive textual datasets. Instruction tuning [30, 31]\nhas emerged as a key technique for refining the output of pre-trained LLMs, as evidenced by its\napplication in the development of open-source models such as Alpaca [32] and Vicuna [33]. They\niterate on the LLaMA [3] with custom instruction sets. Additionally, the integration of LLMs with\nspecific tools for visual tasks [34, 35] highlights their adaptability and potential for broad application,\nunderscoring the utility of LLMs in extending beyond traditional text-based processing to include\nmultimodal interactions. In this work, we take several pre-trained LLMs [36, 3, 29] as benchmarks\nand build multi-modality frameworks upon them to further extend the impressive reasoning ability.\nVision Language Models.\nThe convergence of computer vision and natural language processing has\ngiven rise to VLMs, which marry visual and linguistic models to achieve cross-modal comprehension\nand reasoning capabilities. This integration has been pivotal in advancing tasks that require both\nvisual understanding and language processing, as evidenced by models trained on diverse datasets\nfor understanding [37] and reasoning [16, 38, 39]. Groundbreaking models such as CLIP [40]\nhave further bridged the gap between language models and vision tasks, showcasing the feasibility\nof cross-modal applications. Recent developments underscore a growing trend toward leveraging\nthe robust capabilities of LLMs within the realm of VLMs. Innovations like Flamingo [41] and\nBLIP-2 [6] have capitalized on massive collections of image-text pairs to fine-tune cross-modal\nalignment, significantly boosting learning efficiency. Building upon these advancements, several\nmodels [42, 8] have focused on generating high-quality instructional data based on BLIP-2, leading to\nmarked improvements in performance. Furthermore, LLaVA [7, 43] adopts a simple linear projector\nto facilitate image-text space alignment with minimal learnable parameters. It leverages tailored\ninstruction data and exemplifies an efficient strategy that demonstrates the model’s potent capabilities.\nDifferent from them, we aim to explore the potential for both comprehension and generation.\nLLM as Generation Assistant.\nCombining LLMs with image outputs has emerged as a pivotal\narea in recent multimodal research. Methods like InternLM-XComposer [44, 45] utilize image\nretrieval to produce interleaved text and image outputs, bypassing direct generation. Conversely,\nauto-regressive token prediction approaches, exemplified by EMU [46, 47] and SEED [48, 49],\nenable LLMs to decode images through massive image-text data directly. These methods require\nenormous training resources, and their auto-regressive nature leads to undesirable latency. Recent\nstudies [50–52] strive to align with latent diffusion models [22] to streamline image generation.\nThey typically require designing text embeddings and additional optimization to achieve the desired\ngeneration effect. This joint training can compromise the performance of VLMs in text generation.\nMini-Gemini distinguishes itself by adopting a text-data-driven approach to enable the model to\ngenerate high-quality images. We leverage a mere 13K pure text data to activate the LLM’s ability as\na high-quality re-captioner [53] without undermining the fundamental performance of VLMs.\n3\nMini-Gemini\nThe framework of Mini-Gemini is conceptually simple: dual vision encoders are utilized to provide\nlow-resolution visual embedding and high-resolution candidates; patch info mining is proposed to\nconduct patch-level mining between high-resolution regions and low-resolution visual queries; LLM\nis utilized to marry text with images for both comprehension and generation at the same time.\n3\nHR\nLR\nV\nK\nVisual Embedding\nIn-patch similarity\nMatmul\n!×##×1\n!×##×1\n!×##×1\n!\nX′! (%×'\")\nX′# (%)\n)$(%)\nFeature Map\nQ\n(a) Details in patch info mining.\nLR(0,0)\nLR(0,1)\nLR(1,0)\nLR(1,1)\nLR\nLR\n2x\nLR\nLR Vision Encoder\nFlatten\nPatch Info Mining\nImage Split\nRaw Image\n!!(5$)\nConcat\nHR\nX′\"\n5$\n4$\n$\n(b) Details in visual token extension.\nFigure 3: More details in patch info mining and visual token extension.\n3.1\nDual Vision Encoders\nIn the Mini-Gemini framework, both text and image inputs can be processed, with the option\nto handle them individually or in combination. For illustrative clarity, we consider the concurrent\nprocessing of both modalities. As depicted in Figure 2, the processing begins with a high-resolution\nimage XH ∈RH×W ×3, from which a corresponding low-resolution image XL ∈RH′×W ′×3 is\ngenerated via bilinear interpolation, ensuring H′ ≤H. Then, we process them and encode into\nmulti-grid visual embeddings in two parallel image flows. In particular, for the low-resolution (LR)\nflow, we maintain the traditional pipeline [42, 7] and employ a CLIP-pretrained ViT [40] to encode\nthe visual embedding X′\nL ∈RN×C, where N denotes the number of visual patches. In this way, the\nlong-range relation among N visual patches can be well preserved for subsequent interaction in LLMs.\nAs for the high-resolution (HR) flow, we adopt the CNN-based encoder for adaptive and efficient HR\nimage processing. For instance, to align with the LR visual embedding, the LAION-pretrained [54]\nConvNeXt [55] is used to serve as an HR vision encoder. Therefore, we can obtain the HR feature\nmap X′\nH ∈RN′×N ′×C by upsampling and concatenating the features from different convolutional\nstages to 1\/4 input scale. Here, N ′ = H\/4 × W\/4 = N × M 2 denotes the number of HR features,\nwhere M reflects the pixel-wise feature count within each HR segment, as illustrated in Figure 2.\n3.2\nPatch Info Mining\nWith the above generated LR embedding X′\nL and HR feature X′\nH, we propose patch info mining to\nextend the potential of VLMs with enhanced visual tokens. In particular, to maintain the number of\nfinal visual tokens for efficiency in LLMs, we take the low-resolution visual embedding X′\nL as query\nQ ∈RN×C, aiming to retrieve relevant visual cues from HR candidate. Meanwhile, the HR feature\nmap X′\nH is taken as key K ∈RN×M 2×C and value V ∈RN×M 2×C, as depicted in Figure 2. Here,\nthe low-resolution patch in Q correlates with a corresponding high-resolution sub-region in K and V ,\nencompassing M 2 pixel-wise features. Therefore, the patch info mining process can be formulated as\nTV = MLP(Q + Softmax(ϕ(Q) × ϕ(K)T ) × ϕ(V )),\n(1)\nwhere ϕ and MLP indicate a projection layer and a multi-layer perceptron, respectively. As presented\nin Figure 3a, this formula encapsulates the process of synthesizing and refining the visual cues,\nleading to generation of enhanced visual tokens TV for subsequent LLM processing. It ensures that\nthe mining for each query is confined to its corresponding sub-region in X′\nH with M 2 features, thus\npreserving efficiency. This design allows for the extraction of HR details without expanding the visual\ntoken count of TV , maintaining a balance between richness of detail and computational feasibility.\nFurthermore, visual token extension is also supported in the designed patch info mining. As depicted\nin Figure 3b, we can extend the visual token to 5N to capture more details. This is achieved by\nincorporating the original image along with its 2× upscaled counterpart, resulting in a batched input\nXL ∈R5×H′×W ′×3. And we can get the encoded visual embedding X′\nL ∈R5×N×C with the LR\nvision encoder, as detailed in Section 3.1. Thanks to the flexible design of CNN-based HR vision\nencoder, it can adeptly handle the augmented visual token count during the patch info mining. The\nonly difference in the aforementioned procedure is the sub-region in X′\nH should be changed according\nto the expanded visual embedding X′\nL. We can also upsample the HR input to better support the\nhigher resolution if needed, as experimentally analyzed in Table 2.\n4\nReferences:\nSerene kitchen scene with a broken egg yolk in a bowl, whisk at the \nready, and a jar of golden liquid nearby, symbolizing the process of \nemulsification, soft natural lighting\nAssistant: Yes, it's possible to fix runny mayonnaise! The most common \nreason for mayonnaise becoming runny is because the oil was added too \nquickly or the egg yolk wasn't emulsified properly. Here are some steps \nyou can take to fix it:\n1.\nSeparate another egg yolk and place it in a clean, dry bowl.\n2.\nSlowly add the runny mayonnaise…\nInstruction: I am making mayonnaise, it was starting to thicken but now it has become runny and liquid \nagain, is there any way to salvage it?\n<GEN>\n<h>\nAssistant:\nYoung boy with a bright \nsmile, mimicking a phone call, dressed in a \ngrey sweater and colorful tie, white \nbackground, no text, advertisement style, \nclean and simple.\nInstruction: Capture a playful child pre-\ntending to talk on the phone.\nReal Image Caption (GPT4-V)\nInstruction In-Context Examples\nHigh-Quality T2I Prompt In-Context Examples\nReferences:\nHigh Quality Chat (LIMA, Oasst)\nHigh-Quality T2I Prompt In-Context Examples\n<\/h>\n<GEN>\n<h>\n<\/h>\nGen by GPT-API\nGen by GPT-API\nFigure 4: Two types of our pure-text data are used for image generation. Left: Simple instruction\nre-caption and Right: In-context prompt generation. SDXL generates images with the output prompt.\n3.3\nText and Image Generation\nWith the mined visual tokens TV and input text tokens TT , we concatenate them as the input\nto LLMs for auto-regressive generation, as presented in Figure 2. Distinguished from traditional\nVLMs [42, 43, 11], the proposed Mini-Gemini supports both text-only and text-image generation\nas input and output, i.e., any-to-any inference. Despite the image comprehension, we anchor\nMini-Gemini’s ability to generate images on its outstanding image-text understanding and reasoning\ncapabilities. Unlike recent works [50–52, 47], which address the domain gap between text embeddings\nof LLMs and generation models, we choose to optimize the gap in the domain of language prompts.\nPrecisely, Mini-Gemini translates user instructions into high-quality prompts that produce context-\nrelevant images in latent diffusion models [22, 56]. This approach is reflected in subsequent high-\nquality image generation frameworks, such as DALLE 3 [53] and SORA [57], which leverage the\ngeneration and understanding capabilities of VLMs to obtain higher-quality text conditions for\ngeneration tasks.\nText-image Instructions.\nFor better cross-modality alignment and instruction finetuning, we collect\nhigh-quality datasets from publicly available sources. In particular, for cross-modality alignment, we\nutilize 558K image-caption pairs from the LLaVA-filtered CC3M dataset [58] and 695K sampled\nGPT-4V-responded captions from the ALLaVA dataset [15]. It brings about 1.2M image captions in\ntotal for projector pretraining. As for instruction finetuning, we sample 643K single- and multi-turn\nconversations (excluding 21K TextCaps [59] data) from the LLaVA [43] dataset, 100K QA pairs\nfrom ShareGPT4V [14], 10K LAION-GPT-4V [60] captions, 700K GPT-4V-responded instruction\npairs from ALLaVA dataset [15], and 6K text-only multi-turn conversations from LIMA [20] and\nOpenAssistant2 [21]. To bolster the OCR-related abilities, we further collect 28K QA pairs that\ncomprise 10K DocVQA [17], 4K ChartQA [18], 10K DVQA [61], and 4K AI2D [19] data. In general,\nthere are about 1.5M instruction-related conversations for image comprehension. Moreover, we also\ncollect 13K pairs for image-related generation that will be elaborated on subsequently.\nGeneration-related Instructions.\nTo support image generation, we further construct a 13K\ninstruction-following dataset using GPT-4 Turbo. As depicted in Figure 4, the training data en-\ncompasses two tasks: (a) Simple instruction re-caption: we adopt 8K descriptive image captions\nfrom LAION-GPT-4V [60] and let GPT-4 inversely infer the corresponding user’s short input and the\ntarget caption in the Stable Diffusion (SD) domain. (b) In-context prompt generation: based on a few\nhigh-quality real-world conversation contexts in LIMA [20] and OpenAssistant2 [21], we generate\nprompts that produce images suitable for the conversation context, bringing 5K instructions in total.\nFor both kinds of data, in each query to GPT-4, we randomly sample 5 high-quality SD text-to-image\nprompts from GigaSheet [62] as in-context examples to obtain target prompts for generation. We\nformat our data to use <GEN> as a trigger to initiate the generation process and wrap the target caption\nwithin <h>...<\/h>. Following text generation, Mini-Gemini extracts target captions and utilizes\nSDXL [22] to generate the corresponding image. More details are discussed in Appendix A.\n4\nExperiments\nIn this section, we first outline our experimental framework, commencing with the experimental setup.\nSubsequently, we compare Mini-Gemini with leading methods on various benchmarks. Component-\nwise analysis and qualitative results are given at the end of this section.\n5\nTable 1: Comparison with leading methods on zero-shot benchmarks. ∗and † denote images in train\nsubset are included and the data is not publicly available, respectively. Our results are marked with ■.\nMethod\nLLM\nRes.\nVQAT MMB\nMME\nMM-Vet\nMMMUv MMMUt MathVista\nNormal resolution setting\nMobileVLM[63]\nMLLaMA 2.7B\n336\n47.5\n59.6\n1289\n–\n–\n–\n–\nInstructBLIP [42]\nVicuna-7B\n224\n50.1\n36.0\n–\n26.2\n–\n–\n25.3\nInstructBLIP [42]\nVicuna-13B\n224\n50.7\n–\n1213\n25.6\n–\n–\n–\nQwen-VL† [23]\nQwen-7B\n448\n63.8∗\n38.2\n–\n–\n–\n–\n–\nQwen-VL-Chat† [23]\nQwen-7B\n448\n61.5∗\n60.6\n1488\n–\n35.9\n32.9\n–\nShikra [64]\nVicuna-13B\n224\n–\n58.8\n–\n–\n–\n–\n–\nIDEFICS-80B [65]\nLLaMA-65B\n224\n30.9\n54.5\n–\n–\n–\n–\n–\nLLaMA-VID [10]\nVicuna-7B\n336\n–\n65.1\n1521\n–\n–\n–\n–\nLLaMA-VID [10]\nVicuna-13B\n336\n–\n66.6\n1542\n–\n–\n–\n–\nLLaVA-1.5 [43]\nVicuna-7B\n336\n58.2\n65.2\n1511\n31.1\n–\n–\n–\nLLaVA-1.5 [43]\nVicuna-13B\n336\n61.3\n69.2\n1531\/295\n36.1\n36.4\n33.6\n27.6\nMini-Gemini\nGemma-2B\n336\n56.2\n59.8\n1341\/312\n31.1\n31.7\n29.1\n29.4\nMini-Gemini\nVicuna-7B\n336\n65.2\n69.3\n1523\/316\n40.8\n36.1\n32.8\n31.4\nMini-Gemini\nVicuna-13B\n336\n65.9\n68.5\n1565\/322\n46.0\n38.1\n33.5\n37.0\nMini-Gemini\nMixtral-8x7B\n336\n69.2\n75.6\n1639\/379\n45.8\n41.8\n37.1\n41.8\nMini-Gemini\nHermes-2-Yi-34B\n336\n70.1\n79.6\n1666\/439\n53.0\n48.7\n43.6\n38.9\nHigh resolution setting\nOtterHD [12]\nFuyu-8B\n1024\n–\n53.6\n1314\n–\n–\n–\n–\nCogVLM-Chat [66]\nVicuna-7B\n490\n70.4∗\n63.7\n–\n51.1\n41.1\n–\n34.5\nLLaVA-NeXT [11]\nVicuna-7B\n672\n64.9\n68.1\n1519\/332\n43.9\n35.8\n–\n34.6\nLLaVA-NeXT [11]\nVicuna-13B\n672\n67.1\n70.7\n1575\/326\n48.4\n36.2\n–\n35.3\nLLaVA-NeXT [11]\nHermes-2-Yi-34B\n672\n69.5\n79.6\n1631\/397\n57.4\n51.1\n44.7\n46.5\nMini-Gemini-HD\nVicuna-7B\n672\n68.4\n65.8\n1546\/319\n41.3\n36.8\n32.9\n32.2\nMini-Gemini-HD\nVicuna-13B\n672\n70.2\n68.6\n1597\/320\n50.5\n37.3\n35.1\n37.0\nMini-Gemini-HD\nMixtral-8x7B\n672\n71.9\n74.7\n1633\/356\n53.5\n40.0\n37.0\n43.1\nMini-Gemini-HD\nHermes-2-Yi-34B\n672\n74.1\n80.6\n1659\/482\n59.3\n48.0\n44.9\n43.3\nPrivate models\nGemini Pro [5]\nPrivate\n–\n74.6\n75.2\n–\n64.3\n47.9\n–\n45.2\nQwen-VL-Plus [23]\nPrivate\n–\n78.9\n66.2\n–\n–\n45.2\n40.8\n43.3\nGPT-4V [4]\nPrivate\n–\n78.0\n75.1\n–\n67.6\n56.8\n55.7\n49.9\n4.1\nExperimental Setup\nImplementation Details.\nIn this study, we instantiate Mini-Gemini with the CLIP-pretrained ViT-\nL [40] for LR vision encoder and the LAION-pretrained ConvNext-L [54] for HR vision encoder. For\nefficient training, we keep two vision encoders fixed and optimize the projectors of patch info mining\nin all stages. Meanwhile, we optimize the LLM during the instruction tuning stage only. Regarding\nthe training scheme, we optimize all the models for 1 epoch with the AdamW optimizer and a Cosine\nlearning schedule. In most cases, the initial learning rates for modality alignment and instruction\ntuning are respectively set at 1e−3 and 2e−5, with an adjusted rate of 1e−5 for the Mixtral-8×7B and\nHermes-2-Yi-34B to ensure stable instruction tuning. The framework involves training on 8×A800\nGPUs for standard machine configurations. For the largest model with Hermes-2-Yi-34B, we leverage\n4 machines and complete the optimization within 2 days with DeepSpeed Zero3 strategy. For the HD\nversion, the total cost is enlarged to about 4 days because of the extended visual tokens in LLMs.\nDatasets.\nFor model optimization, we construct high-quality data for cross-modality understanding\nand generation. It mainly includes 1.2M caption pairs for modality alignment and 1.5M single- or\nmulti-round conversations for instruction tuning, as elaborated in Section 3.3. Moreover, we report\nresults on widely-adopted zero-shot image-based benchmarks, including VQAT (TextVQA) [67],\nMMB (MMBench) [24], MME [68], MM-Vet [69], MMMU [25], and MathVista [70] datasets.\n4.2\nMain Results\nNormal Resolution.\nIn Table 1, we compare with previous leading approaches across several set-\ntings, including normal and high resolution, and also consider private models. At normal resolution,\nMini-Gemini consistently outperforms existing models across a wide range of LLMs. In the efficient\nmodel category, Mini-Gemini, when configured with Gemma-2B [36], demonstrates superior perfor-\n6\nTable 2: Comparison with different info mining settings. The baseline is LLaVA-1.5 [43] with\nVicuna-7B using the same training data and strategy. Token Num indicates the number of visual\ntokens TV in Equation (1). ∗denotes that images in the train subset are included. Results with patch\ninfo mining are marked in ■. We respectively set ConvNeXt-L, 336, and 768 for HR Vision Encoder\n(VE-HR), LR image resolution (LR), and HR image resolution (HR) by default.\nMethod\nVE-HR\nLR\nHR\nToken Num.\nVQAT\nMME\nMM-Vet\nBaseline\n–\n224\n–\n256\n54.1∗\n1467.1\n30.7\n+ Info mining\nConvX-L\n224\n512\n256\n58.1∗\n+4.0\n1485.2\n+18.1\n31.3\n+0.6\n+ Higher res.\nConvX-L\n224\n768\n256\n59.8∗\n+1.7\n1478.3\n-6.9\n31.9\n+0.6\nBaseline\n–\n336\n–\n576\n58.2∗\n1510.7\n31.1\n+ Info mining\nConvX-B\n336\n768\n576\n58.4∗\n+0.2\n1451.7\n-59.0\n33.8\n+2.7\n+ Larger VE-HR\nConvX-L\n336\n768\n576\n61.5∗\n+3.1\n1517.0\n+65.3\n34.6\n+0.8\n+ Larger VE-HR\nConvX-XXL\n336\n768\n576\n62.0∗\n+0.5\n1505.7\n-11.3\n33.8\n-0.8\nTable 3: Comparison with different models and data settings. We take LLaVA-1.5 [43] with Vicuna-\n7B as our baseline. Token Num indicates the number of visual tokens TV in Equation (1). ∗denotes\nimages in train subset are included. Ablation studies on model and data are marked with ■and ■.\nMethod\nLR\nHR\nToken Num.\nVQAT\nMME\nMM-Vet\nBaseline\n336\n–\n576\n58.2∗\n1510.7\n31.1\n+ Info mining\n336\n768\n576\n61.5∗\n+3.3\n1517.0\n+6.3\n34.6\n+3.5\n+ ShareGPT4V\n336\n768\n576\n63.2∗\n+1.7\n1527.6\n+10.6\n34.2\n-0.4\n– TextCaps\n336\n768\n576\n59.0\n-4.2\n1465.2\n-62.4\n35.0\n+0.8\n+ LAION-GPT-4V\n336\n768\n576\n58.7\n-0.3\n1521.8\n+56.6\n33.4\n-1.6\n+ OCR-related\n336\n768\n576\n61.6\n+2.9\n1523.5\n+1.7\n33.7\n+0.3\n+ Gen-related\n336\n768\n576\n62.2\n+0.6\n1521.2\n-2.3\n37.0\n+3.3\n+ ALLaVA\n336\n768\n576\n65.2\n+3.0\n1523.3\n+2.1\n40.8\n+3.8\n+ Token extension\n672\n1536\n2880\n68.4\n+3.2\n1546.2\n+22.9\n41.3\n+0.5\nmance compared to the efficient MobileVLM [63] and even surpasses InstructBLIP [42] equipped\nwith Vicuna-7B and even 13B. The scalability of Mini-Gemini is evident when larger LLMs are\nemployed. Given the same LLM, the proposed Mini-Gemini is validated to surpass LLaVA-1.5 [43]\nwith a large margin across all benchmarks. Notably, with the Hermes-2-Yi-34B LLM, Mini-Gemini\nachieves exceptional results, outpacing high-resource private models like Qwen-VL-Plus [23] and\nGemini Pro [5] in some challenging benchmarks like MMMU [25] and MMB [24].\nHigh Resolution.\nTo validate the framework for extended visual tokens, we perform experiments\nwith an input size of 672 for LR visual encoder and 1536 for HR visual encoder in Table 1. As dis-\ncussed above, the HR visual encoder primarily serves to offer high-resolution candidate information.\nImportantly, despite the increased resolution, the effective number of visual tokens processed by the\nLLM remains consistent with the LR input size of 672, ensuring computational efficiency. The benefits\nof this approach are particularly evident in detail-oriented tasks. For example, in the TextVQA [67]\nbenchmark, our method achieved a performance rate of 74.1% with the Hermes-2-Yi-34B configura-\ntion, closely matching the performance of the well-established Gemini Pro [5]. Detailed results in\nTable 1 show that Mini-Gemini excels in more challenging benchmarks as well. For instance, the pro-\nposed method is on par with Qwen-VL-Plus [23] on the MathVista [70] and MMMU [25] benchmark\nand even surpasses Gemini Pro and GPT-4V on the widely-adopted MMB [24] benchmark.\n4.3\nComponent-wise Analysis\nPatch Info Mining.\nWe first delve into the proposed patch info mining and report results in Table 2.\nIt is clear that the model achieves significant gains with the ConvNeXt-L integrated as the vision\nencoder for HR images. For example, when the LR and HR are respectively set to 224 and 512,\nthe model increases 4.0% and 18.1 in TextVQA and MME datasets. Elevating the HR resolution\nto 768 further widens the performance margin, achieving a 5.7% uplift in TextVQA compared to\nthe baseline. These results underscore the substantial impact of patch info mining in harnessing\nmore detailed visual cues. When we further extend the LR resolution to 336, patch info mining\nstill contributes consistent gains. For instance, with the default ConvNeXt-L as vision encoder, it\nsurpasses the baseline with 3.3%, 6.3, and 3.5% in TextVQA [67], MME [68], and MM-Vet [69]\ndataset, respectively. This proves the capability of designed modules with input resolution scaled up.\n7\nVision Encoder.\nTo investigate the effect brought by mining candidates, we conduct experiments\nwith various HR vision encoders in Table 2. Compared with the default ConvNeXt-L, we add two\nencoders for contrast trials, i.e., ConvNeXt-B, and ConvNeXt-XXL. With the basic ConvNeXt-B,\nthe model performs better in TextVQA [67] and MM-Vet [69]. However, the ConvNeXt-L encoder\nconsistently delivers peak results, especially in the MME and MM-Vet datasets, indicating a superior\nbalance in handling detailed visual information. We can conclude from the table that a larger vision\nencoder for HR images contributes more to the candidate quality, but the model converges with a\ntoo large encoder like ConvNeXt-XXL. Hence, considering the balance between effectiveness and\ncomputational efficiency, ConvNeXt-L is chosen as the default HR vision encoder. This decision is\nbased on its ability to provide high-quality visual information mining while maintaining reasonable\ncomputational demands, as evidenced by the comparative performance across the benchmarks.\nHigh-quality Data.\nIn this era, the significance of high-quality data for enhancing the capabilities\nof LLMs and VLMs cannot be overstated. In our comprehensive analysis of data combination effects,\npresented in Table 3, we begin with a baseline model incorporating patch info mining. The integration\nof high-quality captions from ShareGPT4V [14] yields improved visual alignment and performance\ngains. We validate the zero-shot performance on the TextVQA [67] benchmark, notably removing\nTextCaps [59] data from the training set in line with previous studies [11]. This modification led\nto a notable performance decrease, underscoring the value of specific data types in training. To\ncounteract this decline, we incorporate additional high-quality captions from LAION-GPT-4V [60]\nand OCR-specific data, thus enhancing the model’s OCR reasoning capabilities. More details are\nprovided in the appendix. As elaborated in Section 3.3, we utilize generation-related instructions to\nexpand the application. It is interesting to find that such data also benefits the image understanding\nability and brings 3.3% gains in MM-Vet dataset. Moreover, with the high-quality GPT4V responses\nfrom ALLaVA [15] dataset, the framework respectively pushes the baseline over 7% and 9% in\nTextVQA and MM-Vet datasets. This comprehensive evaluation underscores the pivotal role of\nstrategic high-quality data integration in amplifying the potential of the Mini-Gemini framework.\nVisual Token Extension.\nAs depicted in Figure 3b, the proposed patch info mining is adeptly\ndesigned to accommodate extended visual tokens, thereby generalizing its utility across different\ninput resolutions. We validate the effectiveness of the token extension in Table 3. When increasing\nLR and HR input resolution, the model achieves significant gain in all benchmarks. Notably, in\ndetail-oriented tasks such as TextVQA, we observe a performance uplift of over 3%, indicating\na significant enhancement in the model’s ability to handle complex visual data. Our empirical\nobservations suggest that the increase in resolution significantly diminishes visual hallucinations,\nleading to more accurate and reliable image comprehension. Generally, with the increased visual\ntoken number, Mini-Gemini can be scaled up towards better capability. We can also draw the same\nconclusion from high-resolution results in Table 1.\n4.4\nQualitative Results\nVisual Understanding.\nTo ascertain the visual comprehension prowess of Mini-Gemini in real-\nworld settings, we apply it to a variety of understanding and reasoning tasks in Figure 5. Thanks to\nthe patch info mining and high-quality data, Mini-Gemini can well solve several complex cases. For\nexample, it is capable of recognizing plotted curves in graphical data and directly translating them\ninto Python code for immediate application. Beyond mere recognition, it exhibits a keen attention\nto detail, accurately describing intricate elements within complex indoor scenes, and demonstrating\na nuanced understanding of character associations in memes. Moreover, Mini-Gemini’s analytical\ncapabilities extend to chart analysis and practical problem-solving, such as intelligence tests.\nImage Generation.\nIn Figure 6, we provide a comprehensive evaluation of Mini-Gemini’s gener-\nation capabilities. Compared with recent studies such as AnyGPT [52] and ChatIllusion [51], our\nstronger multi-modal understanding ability allows us to generate text-to-image captions that better\nalign with the given instructions, resulting in more contextually appropriate image-text answers. A\nnoteworthy point, as shown in Figures 1 and 6, is its proficiency in generating high-quality content\nbased on multi-modal human instructions, with text-only training data. This capability underscores\nMini-Gemini’s robust image-text alignment and semantic interpretation skills, which come into play\neffectively in the inference stage. By leveraging the powerful reasoning ability of the LLM, it can\nproduce reasonable image-text outputs in single or multi-round conversations.\n8\nReproduced with generated code\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Generate data for the normal distribution\nmu, sigma = 87, 8\nx = np.linspace(70, 110, 100)\ny1 = np.exp(-((x - mu) \/ sigma) ** 2 \/ 2) \/ sigma \/ np.sqrt(2 * np.pi)\n# Generate data for the standard normal distribution\nx = np.linspace(-3, 3, 100)\ny2 = (1 \/ np.sqrt(2 * np.pi)) * np.exp(-(x ** 2) \/ 2)\n# Create a figure with two subplots\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))\n# Plot the normal distribution\naxs[0].plot(x, y1, label='Normal Distribution of Toothbrush Prices')\naxs[0].set_xlabel('Toothbrush Prices')\naxs[0].set_ylabel('Density')\naxs[0].set_title('Normal Distribution of Toothbrush Prices')\n# Plot the standard normal distribution\naxs[1].plot(x, y2, label='Standard Normal Distribution of Toothbrush Prices')\naxs[1].set_xlabel('Toothbrush Prices')\naxs[1].set_ylabel('Density')\naxs[1].set_title('Standard Normal Distribution of Toothbrush Prices')\n# Adjust the layout and add a legend\nplt.tight_layout()\nplt.legend()\nplt.show()\n1. Generate a matplotlib code for the resulting plot using subplot \nfunction to reproduce functions and the layout.\nMini-Gemini\n!\nThe image is a photograph of a modern kitchen interior. The kitchen features a white \ncabinetry with a wooden countertop and a central island with a white tabletop. The \nisland is surrounded by four green velvet chairs with black legs. On the countertop, there \nare various kitchen items including a coffee maker, a toaster, and a bread machine.\nAbove the island, there are pendant lights with exposed bulbs. The wall behind the \ncountertop is adorned with wooden panels and a window with white blinds. On the \nwindow sill, there are potted plants and a small vase with flowers. The floor is covered \nwith a patterned rug. The overall style of the kitchen is contemporary with a warm and \ninviting atmosphere.\n2.a. Describe the image in detail.\nBread.\n2.b. What's the text on the bread machine?\nMini-Gemini\nMini-Gemini\nThe meme is a play on words and expectations. It starts with a conversation where one \nperson is asked where they are, and they respond with \"at the gym!\" which is a common \nanswer for someone who is health-conscious or trying to impress others with their \ndedication to fitness. The person who asked for the location then requests a picture, \npresumably to see the gym or the person's workout. However, the image sent is not of a \ngym but of a McDonald's restaurant, with the word \"GYM\" written on it in a way that \nmimics the McDonald's logo. This creates a humorous twist, as the person is actually \nat McDonald's, which is known for fast food and is the opposite of a gym in terms of \nhealth and fitness. The meme plays on the irony of the situation and the expectation \nversus reality theme.\n3. Can you explain this meme?\nMini-Gemini\n4. Spot two data points that stands out in these charts and what that implicates. Then \nproduce a detailed and well-organizied markdown table to compare all the data.\nThe two data points that stand out are the \"Annoying pain\" in battery cages and furnished \ncages, both of which are significantly higher than the other types of pain listed. This \nimplies that the conditions in battery and furnished cages are particularly stressful for \nhens, leading to prolonged periods of discomfort.\nHere is a detailed and well-organized markdown table comparing all the data:\nThis table allows for a clear comparison of the average days of pain endured by hens in \ndifferent cage systems, with the \"Annoying pain\" data being particularly noteworthy due \nto its higher values.\nExcruciating pain\nDisabling pain\nHurtful pain\nAnnoying pain\nType of cage\n0.003 days\n27 days\n253 days\n420 days\nBattery cages\n0.003 days\n10 days\n197 days\n384 days\nFurnished cages\n0.003 days\n10 days\n109 days\n130 days\nCage-free aviary\n1\n2\n3\n4\n5. Solve this issue.\nTo solve the issue of determining the number of quarter circles in the figure, we need to \ncount each individual quarter circle. A quarter circle is a segment of a circle that is one-\nfourth of the circle's circumference. In the provided image, there are two quarter circles \nthat are clearly visible. However, the central circle is divided into four equal segments, \neach of which is a quarter circle. Therefore, we have a total of six quarter circles: two \nfrom the outer shapes and four from the central circle.\nThe correct answer to the question is 6.\nMini-Gemini\n5\nFigure 5: Qualitative results in visual understanding with Mini-Gemini.\n9\nMini-Gemini\nChatIllusion\nMini-Gemini\nCan you show me a picture of a \nsunny beach?\nAnyGPT\nMini-Gemini\nTell me a story about a close-up of a fire spitting dragon, cinematic \nshot, killed by a man.\nIn a fantasy world, a man is \nseen standing in front of a \nlarge, fire-breathing dragon. \nThe man is holding a sword \nand is ready to strike the \ndragon, which is preparing to \nattack. The dragon is a \nmassive creature with a fiery \nbreath, and it is surrounded by \na cloud of smoke. The scene is \nset in a desert, with a large fire \nin the background, adding to \nthe dramatic atmosphere. ..\n• Once upon a time, in a land far, far away, there lived a \nfearsome dragon. This dragon was known throughout \nthe land for its fiery breath and its ability to destroy \nentire villages with a single breath.\n• One day, a brave man decided to take on the dragon. \nHe was a skilled warrior…\n• As he approached the dragon, he could see its fiery \nbreath and hear its roaring…\n• The dragon let out a mighty roar and breathed a \nstream of fire towards the man. But the man was quick \nand dodged the flames, dodging and weaving to avoid \nthe dragon's breath.\n• Finally, the man got close enough to the dragon and \nstruck it with his sword..\nChatIllusion\nChange the main character of this to a \ncat.\nMini-Gemini\nI want to make a gift for my daughter \nusing this, please give me one suggestion\nMini-Gemini\nIf you want to let your dog wear a scarf, there \nare a few things to consider. First, make sure \nthe scarf is made of a soft, non-irritating \nmaterial that won't cause any discomfort \nto your dog. Avoid using scarves that are too \ntight or have small parts that could be a \nchoking hazard.\nWhen putting the scarf on your dog, start by \ngently wrapping it around their neck…\nWhat if my dog wear a scarf?\nA fluffy corgi wearing a soft, non-\nirritating scarf, sitting on a wooden \nledge, with a blurred background of \ncherry blossoms, warm, cozy, pet-friendly\nA cozy yellow beanie with a pom-pom on a \nsnowy landscape, warm and inviting, \ndigital art, winter theme\nA celestial battle between angels, with \none angel wielding a glowing sword and the \nother holding a shield, set against a \nbackdrop of a starry sky, digital art, \nfantasy, by Greg Rutkowski\nMini-Gemini\nA cartoon dinosaur enjoying a giant scoop \nof ice cream without any signs of \ndistress, surrounded by a prehistoric \nlandscape, vibrant colors, digital art\nMini-Gemini\nWrite me a 50 word anecdote on why \ndinosaurs can eat ice cream without \nhaving a brain freeze\nIn the heavens above,\nA battle rages on,\nBetween angels of love,\nAnd angels of wrong.\nTheir swords clash and their shields shine,\nAs they fight for their side,\nThe angels of love, so divine,\nAnd the angels of pride.\nBut in the end, it's the angels of love,\nWho always come out on top,\n…\nSure, here‘s a suggestion for a gift using \nthis yellow yarn: You could make a cozy \nyellow beanie for your daughter. The \nbright yellow color of the yarn would \nmake a cheerful and warm accessory \nfor cold weather. To make the beanie, \nyou would need to knit or crochet a few \nsimple stitches to create a snug fit for her \nhead…\nDinosaurs don't get brain freeze because \nthey have a different circulatory \nsystem than humans. Their large body \nmass helps to regulate their body \ntemperature, preventing the rapid blood \nflow that causes brain freeze in humans..\nTell me a story about a fire-spitting dragon killed by a man\nA spider man riding a motorbike\nHe then encounters a Lizardman.\nHe then beat the Lizardman using his \nSpider silk.\nFinally he back to home and eating the \nbreakfast.\nMini-Gemini\nMini-Gemini\nMini-Gemini\nMini-Gemini\nCould you write a short poem about angels \nfighting?\nA whimsical amigurumi toy with blue and \npink stripes, sitting on a wooden surface, \nplayful, handmade, cozy\nMini-Gemini\nRound 1\nA polar bear standing amidst a lush \ntropical rainforest, surrounded by palm \ntrees and dense foliage, under a clear \nblue sky\nMini-Gemini\nGive me a similar example of the \ncontradiction depict in this image.\nThe image presents a visual \ncontradiction between the typical \nhabitat of a cactus and the presence \nof ice, which is not a natural occurrence \nin desert environments. A similar \nexample of this kind of contradiction \ncould be a polar bear in a tropical \nrainforest, as polar bears are adapted \nto cold, icy environments and would not \nsurvive in a hot, humid climate. …\n[Round1] Show me one idea of what I could \nmake with this.\n[Round2] Can you \ngive me more \nexamples? I want to \nmake something \ncute and special.\nMini-Gemini\nRound 2\nOne idea for what you could make with \nthese two skeins of yarn is a simple two-\ntone knitted or crocheted scarf… \nFigure 6: Qualitative results in image generation with Mini-Gemini. In the first two rows, we compare\nMini-Gemini with ChatIllusion [51] and AnyGPT [52] with their official cases. In the following rows,\nwe illustrate more cases to show the reasoning generation ability while preserving high-quality text\ngeneration. Image inputs (if have) are provided at the left-top corner. In some cases, we overlay the\ngenerated prompts on corresponding images.\n10\n5\nConclusion and Discussion\nWe presented Mini-Gemini, a streamlined and potent framework for multi-modality VLMs. The\nessence of Mini-Gemini is to harness the latent capabilities of VLMs through strategic framework\ndesign, enriched data quality, and expanded functional scope. At its core, patch info mining enables\nefficient extraction of detailed visual cues by engaging with high-resolution candidates. From the\ndata perspective, our meticulously compiled high-quality dataset ensures accurate vision-language\nalignment and bolsters strong instruction-following ability. Furthermore, we support reasoning-\nbased generation in Mini-Gemini and empower current VLMs with any-to-any workflow. Extensive\nexperiments on several zero-shot benchmarks prove the superiority of the proposed method, which\nsurpasses previous leading approaches and even private models. We hope the Mini-Gemini can serve\nas a strong benchmark for image understanding and VLM-guided generation.\nAlthough Mini-Gemini achieves good results, it still has great potential to be further explored. For\nvisual comprehension, the counting ability and complex visual reasoning ability are still far from\nsatisfactory. This could be attributed to the lack of corresponding training data especially in the\npretraining stage. Meanwhile, for reasoning-based generation, we use text to bridge the VLM and\ndiffusion model in this work because we do not find apparent gain with embedding-based approaches.\nWe will try to find a more advanced manner for visual understanding, reasoning, and generation.\nAppendix\nA\nData Collection Details\nImage-text Data Collection.\nIn this section, we delve into the specifics of OCR-related data\ncollection. Natural images can be easily annotated with detailed captions, but text-rich figures or\ndiagrams, such as documents [17], charts [18], and scientific diagrams [19], present a more intricate\nchallenge for models in complex questions and answers. Therefore, to facilitate the optimization\nprocess, we follow the strategy in TextVQA [67] and incorporate OCR tokens for model reference\nin the training phase. In particular, we utilize the PaddleOCR [71] to initially identify and extract\ntextual elements within each image. Then, we append the text characters to the original conversations\nin a format of Reference OCR token:Text_1,...,Text_n, where Text_1 to Text_n indicates\nn detected text strings. This approach ensures that the model has access to textual representations\nfrom the images, enhancing its ability to understand the image content. It is important to note that the\nOCR detector is utilized solely for generating enriched data and is not employed during testing. This\ndistinction underscores our objective to train Mini-Gemini with a comprehensive understanding of\nboth textual and visual elements, thereby improving capability in complex image-text scenarios.\nGeneration Data Collection.\nFor the data generation collection described in Section 3.3, we\nprovide specific examples of query prompts and their corresponding reference data sources for\ntwo generation tasks in Figure 7. We commence with a corpus comprising 10K GPT4V caption\ndata and 6K English-only LLM SFT data. After filtering out results that did not meet the format\nrequirements, we ultimately obtained 13K data points. To enhance the contextuality and quality of the\nqueries, we incorporate two distinct types of in-context examples: get_example_captions() and\nget_example_queries(). The former function randomly selects 5 high-quality Stable Diffusion\n(SD) Text-to-Image (T2I) prompts, while the latter extracts 3 instances from a repository of simple\ninstructional templates. These in-context examples serve as a foundational guide, providing diverse\nand representative prompts that significantly enrich the generation process. This strategic approach\nensures the production of high-quality, relevant data, effectively supporting the generative capabilities.\nB\nExtended Showcases\nIn this section, we further provide more cases to validate the generality and capability of Mini-\nGemini in various environments. As presented in Figures 8 and 9, Mini-Gemini can well answer\ndetail-oriented questions and solve OCR-related and scientific problems. For image generation, we\npresent more examples in Figure 10 that include direct T2I generation, multi-round conversation,\nreasoning-based generation, storytelling, and in-context generation. They further prove the superiority\nof Mini-Gemini in both visual comprehension and generation.\n11\nHi ChatGPT, our objective is to create several high-quality captions that suitable for diffusion models based \non the original detailed description given.\n1. Thoroughly read and interpret the given description and the given caption, query formats.\n2. Generate a query based on the given description, we will give you some examples of the queries.\n3. Based on the query and the overall description, generate a high-quality caption that is suitable for the \nquery's instruction, and suitable for diffusion models. (Example in below)\nKey considerations:\n- Avoid revealing or implying your identity as a chatbot.\n- Do not include some professional concepts or texts that cannot be understood by the general public.\nExample Human Query formats:\n{get_example_queries()}\nExample high-quality caption formats (limited with 30 words each, descriptive and with some other property \ntags):\n{get_example_captions()}\nDetailed caption information for your reference:\n<GPT-4V_Image_Caption_Data>\nRequested format:\nHuman Query: The human query for the generation content. bounded with double quotes.\nRelated Caption: One caption to describe the image of the query instruction, correlated with the description \nand the query. The caption should be no more than 30 words long. bounded with double quotes. \nStrictly adhere to this format and guidelines.\nHi ChatGPT, our objective is to create several high-quality captions for the following conversations used for \ngenerative models.\n1. Thoroughly read and interpret the given conversation and the given caption formats.\n2. For each round of the conversation. Generate a caption that describe the image with the related \ninformations that is suitable with the conversation context. You can generate captions that is associated with \nthe conversation context, but do not directly reference the conversation.\n3. The caption is better to be descriptive and describe clearly with the main object, do not include any \noffensive or sensitive content.\n4. Please make sure the caption is related to the conversation context and the image, and the caption should \nbe at least 30 words long.\nKey considerations:\n- Avoid revealing or implying your identity as a chatbot.\n- Do not include some professional concepts or texts that cannot be understood by the general public.\nExample high-quality caption formats (limited with 30 words each, descriptive and with some other property \ntags)\n{get_example_captions()}\nConversation information for your reference:\nConversation: \n<Real-World_Multi-Round_Conversation_Data>\nRequested format:\nRelated Captions: [A list of captions, with each caption correlated with each round of gpt response, in double \nquotes and separated by a comma, if you do not have any captions for some rounds, please use an empty \nstring \"\"]\nStrictly adhere to this format and guidelines.\nOpenAssistant2\nLIMA:\nHigh-Quality\nLLM SFT data\nGiggaSheet\nHigh-Quality SDXL\nPrompt database\n\"Max Headroom in a \nPerfume advertisement, \nmagical, science fiction, \nsymmetrical face, large \neyes, Chanel, Calvin \nKlein, Burberry, Versace, \nGucci, Dior, hyper \nrealistic, digital art, \noctane render, trending \non artstation, \nartstationHD, \nartstationHQ, unreal \nengine, 4k, 8k”…\nLAION GPT4V Image\ndescriptions\nGigaSheet\nHigh-Quality SDXL\nPrompt Examples\nQuestion Format\nExamples\n\"Depict an astronaut \nwith Earth in the \nbackground.”\n\"Generate a neon-lit \nfuturistic city at \nnight.”\n\"Portray a lone oak \ntree in autumn.”…\nIn-Context Prompt \nGeneration\nSimple Instruction\nRe-caption\nFigure 7: A detailed prompt design and data source illustration used for allocating image generation\ndata. The total cost for the GPT-4 API to get all 13K data is around 80$.\n12\nThere are no small spheres that are the same color as the big rubber cube. The big rubber \ncube is blue, and there are no blue spheres present in the image.\nbody {\nfont-family: Arial, sans-serif;\nmargin: 0;\npadding: 0;\n}\nh1 {\ntext-align: center;\nmargin-top: 50px;\n…\nThe meme uses a visual metaphor to represent the relationship between different programming languages and \nChat-GPT, a language model developed by OpenAI. The tree trunk represents the foundational technologies that \nsupport web development, with Java and Python being two of the most popular programming languages. JavaScript \nand HTML are also essential technologies for web development, with JavaScript being the scripting language that \nadds interactivity to web pages and HTML being the markup language that defines the structure of web pages.\nThe character labeled \"Chat-GPT\" is depicted as a lumberjack cutting down the tree, which symbolizes the \ndisruptive impact that Chat-GPT could have on the traditional programming landscape. The meme humorously \nsuggests that Chat-GPT might be so powerful or efficient that it could replace the need for these foundational \ntechnologies, much like a lumberjack cuts down a tree.\nThe meme plays on the idea that Chat-GPT, with its advanced natural language processing capabilities, could \npotentially simplify or automate tasks that were previously done by programmers using these languages. It's \na lighthearted way to comment on the evolving nature of technology and the potential for new tools to change the \nway we work.\n1. Can you explain this meme?\nMini-Gemini\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome!<\/title>\n<link rel=\"stylesheet\" type=\"text\/css\"\nhref=\"style.css\">\n<\/head>\n<body>\n<h1>Welcome!<\/h1>\n<p>This is a simple webpage.<\/p>\n<ul>\n<li><a href=\"#\">Home<\/a><\/li>\n<li><a href=\"#\">About<\/a><\/li>\n<li><a href=\"#\">Contact<\/a><\/li>\n<\/ul>\n<button>Learn More<\/button>\n<p>Contact us: <a \nhref=“mailto:email@example.com”>email@example.\n…\n2. Write HTML and CSS code \nto create a webpage based on \nthe image shown. You should \nprovide one .html file and one \n.css file.\nThe chart is a line graph that illustrates the historical and projected number of children under \nthe age of 5 worldwide. The data is based on the United Nations' World Population Prospects \nfrom 2022. The graph shows a general upward trend in the number of children under 5 \nfrom 1950 to around 2020, peaking at approximately 600 million. After 2020, the graph \nprojects a decline in the number of children under 5, with the line descending towards the \n400 million mark by 2100. This suggests a projected decrease in the birth rate or an increase \nin the age at which women have children, leading to fewer children under 5 in the future. The \ngraph is a useful tool for understanding demographic trends and can be used by \npolicymakers, researchers, and organizations to plan for future needs, such as education and \nhealthcare services for children.\n3. Describe this table in detail and summarize what you can get from this table.\nMini-Gemini\nMini-Gemini\nMini-Gemini\nThe question appears to be a visual puzzle, likely from a logic or brain teaser book. The task is \nto identify which option (A, B, C, D, or E) is the best choice to replace the question mark in the \nsequence. To solve this, we need to look for a pattern or rule that applies to the sequence of \nshapes.\nUpon inspection, the sequence of shapes in the top row seems to be increasing in the \nnumber of sides: a triangle (3 sides), a square (4 sides), a pentagon (5 sides), and a \nhexagon (6 sides). Following this pattern, the next shape should logically have seven sides, \nwhich is a heptagon.\nLooking at the options provided, we can see that option A is a hexagon, which does not fit the \npattern. Option B is a pentagon, which is a step back in the sequence. Option C is a triangle, \nwhich is too few sides. Option D is a heptagon, which fits the pattern of increasing sides. \nOption E is a hexagon, which does not fit the pattern.\nTherefore, the best choice to replace the question mark in the sequence is option D, the \nheptagon, as it continues the pattern of increasing the number of sides on the shapes.\nMini-Gemini\n1\n3\n4\n4\n2\nrender\nCSS\nHTML\n3\n5\n5. Please solve this issue. Tell me which option is the best choice.\nMini-Gemini\n4. How many small spheres are the same color as the big rubber cube?\nFigure 8: Qualitative results of Mini-Gemini’s high-resolution understanding. We highlight a failed\ncase in image understanding in orange.\n13\nThe student has attempted to solve the math problem by adding the number of minutes \nJustin practiced on Monday and Tuesday. \nThe student has correctly added 254 minutes (254 is 250 + 4) to 133 minutes, resulting in \n387 minutes. However, the student has not accounted for the fact that Justin practiced \nmore on Monday than on Tuesday. \nThe correct answer should be the difference between the total minutes Justin \npracticed on Monday and Tuesday, which is 254 minutes (Monday) - 133 minutes \n(Tuesday) = 121 minutes. Therefore, Justin practiced 121 more minutes on Monday than \non Tuesday. The student's answer is incorrect as they have not subtracted the Tuesday \npractice time from the total Monday practice time.\n1. Here's a solution to a Math problem by a student. Try to reason about the question step \nby step. Did the student get the correct answer?\nMini-Gemini\nThe image may appear funny due to the unexpected and unusual sight of a dog \nseemingly 'hiding' among the sheep. This is not a common occurrence, as dogs and \nsheep are distinct species with different behaviors and appearances. The dog's \npresence among the sheep, especially if it is a herding dog, can be seen as humorous \nbecause it contrasts with the typical expectation of a dog's role in a flock of sheep. \nAdditionally, the dog's relaxed posture and the sheep's calm demeanor suggest a \nharmonious and comical coexistence, which can be amusing to observers. The image \ncaptures a moment that is both endearing and whimsical, which can elicit a light-\nhearted response from viewers.\n2. Why does this image look funny?\nThe code in the image is addressing the problem of sorting a list of numbers using the quicksort \nalgorithm. Quicksort is a divide-and-conquer algorithm that works by selecting a pivot element from \nthe list and partitioning the other elements into two sub-lists, according to whether they are less than \nor greater than the pivot. The partition function is used to find the index of the pivot element, which is \nthen used to divide the list into two parts. The quicksort function then recursively sorts the two sub-\nlists, and the process continues until the entire list is sorted. This algorithm is efficient for large lists \nbecause it reduces the problem size by half with each recursive call.\n4.a. Illustrate what problem this code in the image is addressing.\nMini-Gemini\ndef quicksort(nums, left, right):\nif left < right:\npartition_index = partition(nums, left, right)\nquicksort(nums, left, partition_index - 1)\nquicksort(nums, partition_index + 1, right)\ndef partition(nums, left, right):\npivot = nums[right]\ni = left - 1\nfor j in range(left, right):\nif nums[j] < pivot:\ni += 1\nnums[i], nums[j] = nums[j], nums[i]\nnums[i + 1], nums[right] = nums[right], nums[i + 1]\nreturn i + 1\n4.b. Transfer it to Python code here.\n!\n3. Choose one of the two pictures below to fill in the blank space above to maintain the \npattern.\nTo maintain the pattern, the blank space above should be filled with the image of the \ngreen parrot. This is because the pattern alternates between the green parrot and the \norange tiger, and since the last image in the sequence is the orange tiger, the next image \nshould be the green parrot to continue the alternating pattern.\nMini-Gemini\nMini-Gemini\nThis code defines the quicksort \nfunction, which sorts a list of numbers \nby calling the partition function to find \nthe index of the pivot element and then \nrecursively sorting the two sub-lists. The \npartition function is used to partition \nthe list into two parts, with elements \nless than the pivot on the left and \nelements greater than the pivot on the \nright. The pivot element is then placed \nin its correct position in the sorted list.\nMini-Gemini\n5. Explain this meme and illustrate what it implicates.\nThe meme is a humorous take on the common narrative that artificial intelligence (AI) will become so \nadvanced that it will eventually surpass human capabilities and perhaps even take over the world. The \nleft panel shows a person reacting with alarm, which is a common portrayal of the public's reaction to \nthe idea of AI becoming too powerful. The right panel, however, shows a cat labeled as a \"Dog\" by a \nneural network, which is a humorous example of AI making a mistake. This juxtaposition implies \nthat while AI is indeed becoming more advanced, it is not yet at the level where it can accurately \nperform tasks such as image recognition without errors. The meme uses this contrast to humorously \ndownplay the fear of AI taking over, suggesting that the technology is still in its early stages and prone \nto mistakes.\nMini-Gemini\n1\n2\n3\n4\n5\nFigure 9: Qualitative results of Mini-Gemini’s high-resolution understanding.\n14\nSDXL + prompt\nSDXL + Mini-Gemini\nRe-caption\n3\nMini-Gemini\n[Round 2] I want to add a husky in this image.\nMini-Gemini\nRound 1\nMini-Gemini\nAbsinthe Painting -\nCafe Scene In Paris by \nHenri Gervex. Analyze\nthen generate.\nChatIllusion\nLLMGA\nRound 1\n[Round 1] Generate a similar\nimage\nLLMGA\nRound 2\nMini-Gemini\nRound 2\nMulti-Round\nConversation\nCapture a scene of space \nelevator\nLLMGA\nDirect T2I\nGeneration\n1\n2\nDALLE 3 + prompt\nShowcase an orange-themed \ninterior design inspiration collage.\nDALLE 3 + Mini-Gemini\nReasoning\nGeneration\nPluggable with\nALL Models\n4\nDALLE 3 + Mini-Gemini\nRe-caption\nOrange mood board for interior design, \nplush towels, floral bedding, striped \ncontainers, and scented candle, high \ncontrast, minimalistic, vibrant color \npalette, digital collage.\nRe-caption\nMini-Gemini\nSDXL + Mini-Gemini\nDALLE 3 + Mini-Gemini\nSDXL + Mini-Gemini\nDescribe the image and change the character to a dog.\nGenerate the scene if the cat is coding.\nMake it richer!\nMake it much richer, it has a big \ntreasure\nNow it is rubbed and has nothing.\nEventually, it perked up and \nopened a cheese shop.\nReasoning +\nStory Telling\n5\nGenerate a mouse that is rich.\nMini-Gemini\nMini-Gemini\nMini-Gemini\nMini-Gemini\nMini-Gemini\nA luxurious mouse with a golden crown, \nholding a golden key, on a white \nbackground, digital art, fantasy, \nwhimsical, by Greg Rutkowski.\nA majestic mouse with a golden crown \nand key, standing on a white pedestal, \ndigital art, fantasy, whimsical, by Greg \nRutkowski, highly detailed, vibrant \ncolors\nA majestic mouse with a golden crown \nand key, standing on a white pedestal, in \nfront of a treasure chest filled with gold \ncoins and jewels, digital art, fantasy, \nwhimsical, by Greg Rutkowski, highly \ndetailed, vibrant colors\nA humble mouse with a tattered crown \nand key, standing on a dirt ground, in \nfront of a broken treasure chest, digital \nart, fantasy, whimsical, by Greg Rutkowski, \nhighly detailed, muted colors\nA cheerful mouse with a crown and key, \nstanding behind a cheese shop \ncounter, digital art, fantasy, whimsical, \nby Greg Rutkowski, highly detailed, \nvibrant colors.\nGive me an example to \nunderstand deep learning to a \nprimary student.\nImagine you have a big box of toys, and you want to teach a computer to recognize and pick out a specific \ntoy from the box. The computer will use deep learning to learn how to do this.\nFirst, the computer will look at lots of pictures of toys and learn what different toys look like. It will learn \nto recognize patterns and features that are important for identifying toys, like the shape of a toy car or the \ncolor of a toy doll.\nNext, the computer will use this information to recognize the toy you want it to pick out from the box. It \nwill look at the toy and compare it to what it has learned about toys. If the toy matches the patterns and \nfeatures it has learned, the computer will pick it out.\nThis is an example of deep learning because the computer is using a lot of data (lots of pictures of toys) \nand complex algorithms to learn how to recognize and pick out the toy. It's like the computer is \"learning\" \nhow to do the task, just like a student would learn in school.\nMini-Gemini\nMini-Gemini\nReasoning +\nIn-Context \nGeneration\n6\n[Round 2] I want to add a husky in this image.\nFigure 10: Rows 1-2: Comparison of Mini-Gemini with ChatIllusion [51] and LLMGA [50] on\ninteractive image generation tasks. Mini-Gemini demonstrates superior adaptability and performance,\ncapturing intricate details and maintaining coherence without further tuning on the text-image output\nalignment. Rows 3-6 showcase Mini-Gemini’s unique capabilities in generating images with its plug-\nand-play capability, reasoning generation, and multi-round storytelling. All Mini-Gemini-generated\nimages adopt SDXL unless otherwise specified.\n15\nReferences\n[1] OpenAI. Chatgpt. https:\/\/openai.com\/blog\/chatgpt\/, 2023. 2\n[2] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv:2205.01068, 2022. 2\n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv:2302.13971,\n2023. 2, 3\n[4] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023. 2, 6\n[5] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable\nmultimodal models. arXiv:2312.11805, 2023. 2, 6, 7\n[6] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv:2301.12597, 2023. 2, 3\n[7] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeruIPS, 2023.\n2, 3, 4\n[8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv:2304.10592, 2023. 2, 3\n[9] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\nfor video understanding. arXiv:2306.02858, 2023. 2\n[10] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language\nmodels. arXiv:2311.17043, 2023. 2, 6\n[11] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next:\nImproved reasoning, ocr, and world knowledge, 2024. URL https:\/\/llava-vl.github.io\/blog\/\n2024-01-30-llava-next\/. 2, 5, 6, 8\n[12] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: A high-\nresolution multi-modality model. arXiv:2311.04219, 2023. 2, 6\n[13] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa˘gnak\nTa¸sırlar. Introducing our multimodal models, 2023. URL https:\/\/www.adept.ai\/blog\/fuyu-8b. 2\n[14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\nSharegpt4v: Improving large multi-modal models with better captions. arXiv:2311.12793, 2023. 2, 5, 8\n[15] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong\nChen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for a lite\nvision-language model. arXiv:2402.11684, 2024. 2, 5, 8\n[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 2, 3\n[17] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. Document collection visual question answering.\nIn ICDAR 2021, 2021. 5, 11\n[18] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for\nquestion answering about charts with visual and logical reasoning. arXiv:2203.10244, 2022. 5, 11\n[19] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A\ndiagram is worth a dozen images. In ECCV, 2016. 2, 5, 11\n[20] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing\nSystems, 36, 2024. 2, 5\n[21] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,\nAbdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-\ndemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36,\n2024. 2, 5\n16\n[22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023. 2, 3, 5\n[23] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\nJingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv:2308.12966,\n2023. 2, 6, 7\n[24] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\narXiv:2307.06281, 2023. 2, 6, 7\n[25] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive\nmulti-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 2, 6, 7\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv:1810.04805, 2018. 2\n[28] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020. 2\n[29] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of\nexperts. arXiv:2401.04088, 2024. 2, 3\n[30] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv:2109.01652, 2021. 3\n[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. In NeurIPS, 2022. 3\n[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https:\/\/github.\ncom\/tatsu-lab\/stanford_alpaca, 2023. 3\n[33] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chat-\nbot impressing gpt-4 with 90%* chatgpt quality. https:\/\/lmsys.org\/blog\/2023-03-30-vicuna\/,\n2023. 3\n[34] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv:2303.04671, 2023. 3\n[35] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large\nlanguage model to use tools via self-instruction. arXiv:2305.18752, 2023. 3\n[36] Google. Gemma: Introducing new state-of-the-art open models. hhttps:\/\/blog.google\/technology\/\ndevelopers\/gemma-open-models\/, 2024. 3, 6\n[37] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv:1504.00325,\n2015. 3\n[38] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. In NeurIPS, 2022. 3\n[39] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning\nsegmentation via large language model. arXiv:2308.00692, 2023. 3\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021. 3, 4, 6\n17\n[41] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. In NeurIPS, 2022. 3\n[42] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. arXiv:2305.06500, 2023. 3, 4, 5, 6, 7\n[43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. arXiv:2310.03744, 2023. 3, 5, 6, 7\n[44] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui\nDing, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li,\nKai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer:\nA vision-language large model for advanced text-image comprehension and composition. arXiv preprint\narXiv:2309.15112, 2023. 3\n[45] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang\nZhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang,\nWei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang.\nInternlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language\nlarge model. arXiv preprint arXiv:2401.16420, 2024. 3\n[46] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang.\nGenerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023. 3\n[47] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming\nRao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv\npreprint arXiv:2312.13286, 2023. 3, 5\n[48] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041, 2023. 3\n[49] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see\nand draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. 3\n[50] Bin Xia, Shiyin Wang, Yingfan Tao, Yitong Wang, and Jiaya Jia. Llmga: Multimodal large language model\nbased generation assistant. arXiv preprint arXiv:2311.16500, 2023. 3, 5, 15\n[51] Xiaowei Chi, Yijiang Liu, Zhengkai Jiang, Rongyu Zhang, Ziyi Lin, Renrui Zhang, Peng Gao, Chaoyou\nFu, Shanghang Zhang, Qifeng Liu, et al. Chatillusion: Efficient-aligning interleaved generation ability\nwith visual instruction model. arXiv preprint arXiv:2311.17963, 2023. 8, 10, 15\n[52] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,\nGe Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv\npreprint arXiv:2402.12226, 2024. 3, 5, 8, 10\n[53] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science.\nhttps:\/\/cdn. openai. com\/papers\/dall-e-3. pdf, 2(3):8, 2023. 3, 5\n[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An\nopen large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 4, 6\n[55] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A\nconvnet for the 2020s. In CVPR, 2022. 4\n[56] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, and Marc Aubreville. Wuerstchen:\nAn efficient architecture for large-scale text-to-image diffusion models, 2023. 5\n[57] OpenAI.\nVideo generation models as world simulators.\nURL https:\/\/openai.com\/research\/\nvideo-generation-models-as-world-simulators. 5\n[58] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 5\n18\n[59] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\ncaptioning with reading comprehension. In ECCV, 2020. 5, 8\n[60] LAION eV. Laion\/gpt4v-dataset · datasets at hugging face. URL https:\/\/huggingface.co\/datasets\/\nlaion\/gpt4v-dataset. 5, 8\n[61] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations\nvia question answering. In CVPR, 2018. 5\n[62] stable-diffusion-prompts.\nURL\nhttps:\/\/www.gigasheet.com\/sample-data\/\nstable-diffusion-prompts. 5\n[63] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang,\nBo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for\nmobile devices. arXiv:2312.16886, 2023. 6, 7\n[64] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm’s referential dialogue magic. arXiv:2306.15195, 2023. 6\n[65] IDEFICS. Introducing idefics: An open reproduction of state-of-the-art visual language model. https:\n\/\/huggingface.co\/blog\/idefics, 2023. 6\n[66] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei\nZhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv:2311.03079, 2023.\n6\n[67] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 6, 7, 8, 11\n[68] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin,\nJinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large\nlanguage models. arXiv:2306.13394, 2023. 6, 7\n[69] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and\nLijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv:2308.02490,\n2023. 6, 7, 8\n[70] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation\nmodels in visual contexts. In ICLR, 2024. 6, 7\n[71] PaddleOCR. Awesome multilingual ocr toolkits based on paddlepaddle. URL https:\/\/github.com\/\nPaddlePaddle\/PaddleOCR. 11\n19\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models.pdf"}
{"title":"AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling","authors":"Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu","summary":"We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps:\/\/junzhan2000.github.io\/AnyGPT.github.io\/","url":"http:\/\/arxiv.org\/abs\/2402.12226v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.12226v3","published":1708356790000,"comment":"28 pages, 16 figures, under review, work in progress","pdf_text":"OpenMOSS\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Mod-\neling\nJun Zhan1,∗, Junqi Dai1,∗, Jiasheng Ye1,∗\nYunhua Zhou1, Dong Zhang1, Zhigeng Liu1, Xin Zhang1\nRuibin Yuan2, Ge Zhang2, Linyang Li1, Hang Yan3, Jie Fu2\nTao Gui1, Tianxiang Sun1, Yugang Jiang1, Xipeng Qiu1,†\n1 Fudan University\n2 Multimodal Art Projection Research Community\n3 Shanghai AI Laboratory\n{jzhan22, jqdai22, jsye23}@m.fudan.edu.cn\nxpqiu@fudan.edu.cn\nhttps:\/\/junzhan2000.github.io\/AnyGPT.github.io\nAbstract\nWe introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities, including\nspeech, text, images, and music. AnyGPT can be trained stably without any\nalterations to the current large language model (LLM) architecture or training\nparadigms. Instead, it relies exclusively on data-level preprocessing, facilitating\nthe seamless integration of new modalities into LLMs, akin to the incorporation\nof new languages. We build a multimodal text-centric dataset for multimodal\nalignment pre-training. Utilizing generative models, we synthesize the first large-\nscale any-to-any multimodal instruction dataset. It consists of 108k samples\nof multi-turn conversations that intricately interweave various modalities, thus\nequipping the model to handle arbitrary combinations of multimodal inputs and\noutputs. Experimental results demonstrate that AnyGPT is capable of facilitating\nany-to-any multimodal conversation while achieving performance comparable to\nspecialized models across all modalities, proving that discrete representations can\neffectively and conveniently unify multiple modalities within a language model.\nDemos are shown in https:\/\/junzhan2000.github.io\/AnyGPT.github.io\/.\n1\nIntroduction\nLLMs have exhibited remarkable proficiency in comprehending and generating human language.\nNevertheless, their capabilities are confined to textual processing. The real-world environment\nis inherently multimodal, with organisms perceiving and exchanging information through diverse\nchannels, including vision, language, sound, and touch.\nA promising objective in developing multimodal systems is to augment LLMs with the capacity for\nmultimodal perception. The dominant methodology involves the integration of multimodal encoders\nwith the language model, thus empowering it to process information across various modalities and\nutilize its sophisticated text-processing abilities to produce coherent responses. However, this strategy\nis limited to text generation and does not encompass multimodal output.\nPioneering efforts such as Emu (Sun et al., 2023b), SEED-LLaMA\n(Ge et al., 2023b) and\nSpeechGPT (Zhang et al., 2023a) have made significant strides by enabling multimodal understanding\nand generation within language models. Yet, these models incorporate only a single non-textual\nmodality, such as images or audio. While aligning text with one additional modality is relatively\n∗Equal contribution.\n† Corresponding author.\n1\narXiv:2402.12226v3  [cs.CL]  7 Mar 2024\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nimage\nmusic\nspeech\nimage\ntokenizer\nmusic\ntokenizer\nspeech\ntokenizer\nImage tokens\nMusic tokens\nText tokens\nSpeech tokens\n<sos>\n<eos>\n<soi>\n<eoi> <som>\n<eom>\ntext\nImage tokens\nMusic tokens\nText tokens\n Speech tokens\n<sos>\nspeech \nde-tokenizer\nmusic\nde-tokenizer\nimage\nde-tokenizer\n“ It was the night---\nsilent night,  whence … ”\n<eos>\n<soi>\n<eoi> <som>\n<eom>\nANYGPT\nFigure 1: An overview of the AnyGPT model architecture. All modalities are tokenized into discrete\ntokens, upon which the LLM performs multimodal understanding and generation autoregressively.\nOnly data pre-processing and post-processing are required, with the model’s architecture and training\nobjectives remaining unaltered.\nstraightforward, integrating multiple modalities (N ≥3) within a single framework—and achieving\nbidirectional alignment among them—poses a more formidable challenge.\nExisting explorations in any-to-any multimodal generation have encountered obstacles: some (Tang\net al., 2023b) lacked a robust core language model, which impeded the system’s reasoning and\ndecision-making capabilities; Others, such as NExT-GPT (Wu et al., 2023), CoDi-2 (Tang et al.,\n2023a), and Unified-IO2 (Lu et al., 2023), have employed separately pre-trained encoders and\ndecoders. This approach results in representational inconsistencies between the inputs and outputs of\nthe LLMs, which in turn complicates both training and inference processes. Moreover, stabilizing\ntraining with such diverse modalities necessitates substantial modifications to existing models and\ntechniques.\nTo overcome these challenges, we introduce AnyGPT, an any-to-any multimodal language model\nthat employs discrete representations for unified processing. AnyGPT is equipped with multimodal\ntokenizers that compress raw multimodal data, such as images and audio, into a sequence of dis-\ncrete semantic tokens. These discrete representations enable the core LLM to unify tasks such as\nperception, understanding, reasoning, and generation in an autoregressive manner at the seman-\ntic level. Subsequently, de-tokenizers convert the discrete representations back into the original\nmodal representations at the perceptual level. Thanks to discrete representation, which filters out\nhigh-frequency, modality-specific perceptual information while preserving essential low-frequency\nsemantic information (Ge et al., 2023a; Borsos et al., 2023a; Rombach et al., 2022), we can train our\nmodel stably without any alterations to the existing LLM architecture or training paradigms. Instead,\nour approach relies solely on data-level preprocessing. This allows for the seamless integration of\nnew modalities into LLMs, akin to the addition of new languages, and permits the direct application\nof existing LLM tools, which enhances the efficiency of both the training and inference stages.\nFurthermore, to mitigate the scarcity of multimodal alignment data encompassing all modalities,\nwe build a text-centric multimodal alignment dataset for pre-training. Our goal is to use text as a\nbridge, by aligning other modalities with text, to achieve mutual alignment among all modalities,\nsince natural language is the most refined modality of semantic representation and is present in the\nmajority of multimodal alignment datasets. To endow the model with the capability to comprehend\nand generate content interwoven with multiple modalities, we employ advanced generative models\nto synthesize a multimodal instruction dataset, AnyInstruct-108k. This dataset, comprising 108k\n2\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nsamples of multi-turn conversations, enables AnyGPT to handle arbitrary combinations of multimodal\ninputs and outputs.\nExperimental results demonstrate that AnyGPT achieves zero-shot performance comparable to that\nof specialized models across various modalities. Furthermore, extensive case studies corroborate\nAnyGPT’s remarkable ability to facilitate any-to-any multimodal dialogue, substantiating the feasibil-\nity of using discrete representations to unify multiple modalities.\nOur contributions include the following:\n• We propose AnyGPT, a token-based any-to-any multimodal language model which can\nunderstand and generate various modalities, including speech, text, images, and music.\n• One key challenge is the lack of multimodal interleaved instruction-following data. We\ndevelop a pipeline using generative models to build AnyInstruct-108k, a dataset comprising\n108k multi-turn dialogues with interleaved multimodal elements.\n• We demonstrate discrete representations can effectively unify multiple modalities within a\nlanguage model.\n2\nRelated Work\n2.1\nMultimodal Large Language Models\nTo enable cross-modal perception in LLM, a common approach is to connect pre-trained encoders of\nother modalities as adaptors. However, these models are often limited to text generation.\nTo empower LLMs with multimodal generation capabilities,\nTang et al. (2023b) introduces a\nfrozen text-to-image diffusion model and learns the mapping between the LLM’s embeddings and\nthe diffusion model.\nSun et al. (2023a) utilizes continuous embeddings to represent the image,\ncalculating either a loss for the next token prediction or the next visual embedding regression. In\ncontrast, SEED-LLaMA (Ge et al., 2023b) trains an image discretization tokenizer to encode the\noriginal image into discrete tokens. Through a unified next token prediction task, it achieves unified\nimage understanding and generation. Similarly, in the field of speech, SpeechGPT (Zhang et al.,\n2023a) enables LLMs to have inherent cross-modal conversation capabilities through discrete speech\nrepresentation. VideoPoet (Kondratyuk et al., 2023) employs a decoder-only transformer architecture\nthat processes multimodal inputs – including images, videos, text, and audio, and is capable of\ngenerating videos and audio.\nTo achieve multimodal generation across various modalities on LLMs, NExT-GPT (Wu et al., 2023)\nutilizes existing high-performance encoders and decoders, connected by a small number of projection\nlayer parameters. However, NExT-GPT does not train the LLM, which may result in suboptimal\nperformance. Moreover, its representation of multimodal input and output lacks a unified form, which\nposes challenges in unified training and inference.\n2.2\nMultimodal Discretization\nTo create a unified multimodal language model, a common approach is to use discretization. A typical\nmethod is VQ-VAE (van den Oord et al., 2017). This involves maximizing the restoration of the\noriginal representation from the compressed tokens. Some studies (D’efossez et al., 2022; Zeghidour\net al., 2021) incorporate residual quantization mechanisms to further enhance fidelity.\nIn addition to VQ-VAE based tokenizers, some tokenizers focus on extracting high-level semantic\nrepresentations. Ge et al. (2023b) discretizes the image into semantic-level. The SpeechTok-\nenizer (Zhang et al., 2023b), based on the RVQ-VAE structure, enables the first layer of tokens\nto retain the semantic information of speech, and the remaining layers to supplement residual\ninformation information, achieving a disentanglement of semantic and acoustic information.\n3\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\n3\nAnyGPT\nOur interest lies in facilitating the generation of any modality to any modality with LLMs. To\nrealize this, we propose a comprehensive framework that can be uniformly trained. As illustrated\nin Figure 1, this framework is composed of three main components: (1) multimodal tokenizers,\n(2) a multimodal language model serving as the backbone, and (3) multimodal de-tokenizers. The\ntokenizers transform continuous non-text modalities into discrete tokens, which are subsequently\narranged into a multimodal interleaved sequence. Then the sequences are trained by the language\nmodel using the next token prediction training objective. During the inference process, multimodal\ntokens are decoded back into their original representations by the associated de-tokenizers. To\nenrich the quality of generation, multimodal enhancement modules can be deployed to post-process\nthe generated results, including applications like voice cloning or image super-resolution. In the\nfollowing section, we will introduce the details of each module.\n3.1\nTokenization\nModality\nImage\nSpeech\nMusic\nVocab Size\n8192\n1024\n4096\nTokens per Sample 32 \/ per image\n50 \/ s\n200 \/ s\nRVQ\n✘\n✔\n✔\nInput Size\n224*224\nvariable duration\n5s\nTable 1: Details of tokenization of different modalities.\nImage Tokenizer\nWe utilize the SEED tokenizer (Ge et al., 2023a) for image tokenization. The\nSEED tokenizer consists of several components, including a ViT encoder (Dosovitskiy et al., 2021),\nCausal Q-Former, VQ Codebook (van den Oord et al., 2017), multi-layer perception (MLP), and a\nUNet decoder (Ronneberger et al., 2015). SEED takes a 224 × 224 RGB image as input, and the\nViT encoder encodes the image into 16 × 16 patches, then the Causal Q-Former converts the patch\nfeatures into 32 causal embeddings. A codebook with 8192 entries discretizes the embeddings into\na sequence of quantized codes. An MLP is employed to decode the visual codes into a generation\nembedding, which is aligned with the latent space of the pre-trained unCLIP Stable Diffusion(unCLIP-\nSD) (Rombach et al., 2022). Finally, the UNet decoder is used to restore the generation embedding\nto the original image.\nSpeech Tokenizer\nThe tokenizer for speech we utilize is SpeechTokenizer (Zhang et al., 2023b),\nadopting an encoder-decoder architecture with residual vector quantization (RVQ). The SpeechTok-\nenizer compresses single-channel audio sequences into a discretized matrix using eight hierarchical\nquantizers, each with 1,024 entries, and achieves a frame rate of 50 Hz. The first quantizer layer\ncaptures semantic content, while layers 2 to 8 encode paralinguistic details. A 10-second audio is thus\ntransformed into a 500 × 8 matrix, splitting into semantic and acoustic tokens. We adopt a SpeechTo-\nkenizer variant pre-trained on the Commonvoice (Ardila et al., 2020) and Librispeech (Panayotov\net al., 2015) datasets.\nIn AnyGPT, the Large Language Model (LLM) is employed to model the semantic tokens, while a\nvoice cloning model supplements the remaining paralinguistic information. As a result, the size of\nthe voice vocabulary in the LLM is equivalent to the size of one codebook, which is 1024. Further\ndetails will be discussed on in Section 3.3.\nMusic Tokenizer\nAlthough speech and music share similar data formats, their substantial content\ndifferences lead us to treat them as distinct modalities, each equipped with its own tokenizer. For\nmusic, we employ Encodec (D’efossez et al., 2022), a convolutional auto-encoder with a latent space\nquantized using Residual Vector Quantization (RVQ), as the music tokenizer. We use an available\noff-the-shelf variant of the Encodec1 pre-trained on 20k pieces of music tracks. This variant processes\n32 kHz monophonic audio, and achieves a frame rate of 50 Hz. The embeddings generated are\n1https:\/\/huggingface.co\/facebook\/encodec_32khz\n4\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nquantized using an RVQ with four quantizers, each with a codebook size of 2048, resulting in a\ncombined music vocabulary size of 8192.\nWe encode 5 seconds music into 250 latent frames, ultimately generating a 250 × 4 codes matrix.\nTo enable the language model predict entire music clip, we flatten the 4-layer music codes into a\ncausal sequence in a frame-by-frame manner. The language model begins by predicting the initial\nfour tokens of the first frame and continues in a similar fashion for the subsequent frames.\n3.2\nLanguage Model Backbone\nExpanding vocabulary\nTo incorporate multimodal discrete representations into pre-trained LLMs,\nwe expand the vocabulary with new modality-specific tokens, and consequently extend the correspond-\ning embeddings and prediction layer, the newly incorporated parameters are initialized randomly. The\ntokens from all modalities combine to form a new vocabulary, where each modality is trained within\nthe language model to align in a shared representational space. The size of this enhanced vocabulary,\ndenoted by V, is the summation of the vocabulary sizes across all modalities, that is, V = ∑n\ni=1 Vi,\nwhere Vi signifies the vocabulary size of the i-th modality.\nUnified Multimodal Language Model\nEquipped with the modality-specific tokenizers, we can\ncompress multimodal data into discrete token sequences, which can be trained by the language model\nusing the next token prediction loss. This naturally enables the core LLM to unify tasks such as\nperception, understanding, reasoning, and generation in an autoregressive manner.\nWe employ the LLaMA-2 (Touvron et al., 2023) 7B as the backbone, which is pre-trained on 2 TB of\ntext tokens. Apart from reshaping the embedding matrix and prediction layer, the rest of the language\nmodel remains unaltered.\n3.3\nMultimodal Generation\nThe generation of high-quality multimodal data, including high-definition images, and high-fidelity\naudio, presents a substantial challenge. These data typically necessitate a large number of bits for\naccurate representation, resulting in long sequences which is particularly demanding for language\nmodels, as the computational complexity increases exponentially with the length of the sequence.\nTo tackle this, we adopt a two-stage framework for high-fidelity generation, comprising semantic\ninformation modeling and perceptual information modeling. First, the language model is tasked\nwith generating content that has undergone fusion and alignment at the semantic level. Then, non-\nautoregressive models convert multimodal semantic tokens into high-fidelity multimodal content at\nthe perceptual level, striking a balance between performance and efficiency.\nSpecifically, we employ SEED tokens, aligned with the diffusion latent space, for visual language\nmodeling. Semantic-level SEED tokens are decoded into high-quality images by a Diffusion Model,\nwhich is renowned for its superior generation capabilities. For speech, we utilize SoundStorm (Borsos\net al., 2023b), a non-autoregressive Masked Language Model, trained to generate SpeechTokenizer’s\nacoustic tokens from semantic tokens. We train a variant of Soundstorm, which is trained using the\nSpeechTokenizer on the Multilingual LibriSpeech(MLS) dataset (Pratap et al., 2020). Subsequently,\nthe SpeechTokenizer’s decoder transforms all speech tokens into raw audio data. This approach\nenables AnyGPT replicate the voice of any speaker using a 3-second speech prompt, while signifi-\ncantly reducing the length of the voice sequence for LLM. For music, we employ Encodec tokens\nto filter out high-frequency details beyond human perception, and then use the Encodec decoder to\nreconstruct these tokens into high-fidelity audio data.\n4\nMultimodal Data\n4.1\nPre-training Data\nTo enable the generation from any modality to any other, it is crucial to have data that is well-aligned\nacross these modalities. Unfortunately, such data is notably scarce. To address this challenge, we\nbuild a text-centric bi-modal alignment dataset. Here, text is employed as a vital intermediary to\n5\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nbridge the gap between various modalities. By aligning different modalities with the textual modality\nwithin a language model, we aim to achieve mutual alignment amongst all modalities.\nThe representational forms and types of information vary greatly across different modalities, To\nfacilitate a standardized comparison of data volumes across various modalities, we have adopted a\nquantification approach based on token counts. Figure 2 presents all the data used in pre-training\nand their respective proportions. A certain level of oversampling is applied to modalities with\ncomparatively lower data quantities, to attain a balanced representation of diverse data types within a\nsingle batch. More details are in Appendix A.1.\nImage & Text\nWe utilized image-text pairs from LAION-2B (Schuhmann et al., 2022), LAION-\nCOCO (lai, 2022b), LAION-Aesthetics (lai, 2022a) and JouneyDB (Pan et al., 2023). LAION-2B\nprovides images paired with noisy alt-texts sourced from the web, while LAION-COCO represents a\n600M subset of this, captioned by BLIP. We refined these datasets by filtering for text quality, image\naspect ratio, and clip score, etc., yielding a high-quality corpus of 300M pairs. To enhance the overall\nimage generation fidelity, we supplement our data with the high-quality LAION-Aesthetics subset\nand the synthetic dataset JourneyDB from Midjourney.\nWe also incorporate image-text interleaved data to adapt the model to an interleaved mode. We\ndeploy the Multimodal-C4 (MMC4) dataset (Zhu et al., 2023), an enhanced version of the text-only\nC4 (Raffel et al., 2020). Specifically, we utilize the MMC4-core split, consisting of 7.3M documents.\nFigure 2: Pre-training data distribution, segmented by token counts, with the inner section indicating\nthe modality, the middle detailing data types, and the outer specifying individual datasets.\nSpeech & Text\nWe collect several large-scale English Automatic Speech Recognition (ASR)\ndatasets, including Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), and\nMultilingual LibriSpeech(MLS) (Pratap et al., 2020). These datasets are sourced respectively from\nonline platforms, volunteer crowdsourcing, and audiobooks, collectively constituting a corpus of\n57,000 hours of speech-text pairs, encompassing a wide variety of speakers, domains, and recording\nenvironments.\nMusic&Text\nWe embark on an extensive data collection process by crawling over one million music\nvideos from the Internet. The core step involves matching the titles of these videos with corresponding\nsongs using the Spotify API. Subsequently, we harvest a comprehensive set of metadata for each\nmusic audio, including video titles, descriptions, keywords, playlist names, and Spotify lyrics. This\nmetadata is formatted into JSON and fed into GPT-4 (Achiam et al., 2023) for processing. GPT-4’s\nrole is pivotal as an intelligent caption generator; it utilizes the noisy metadata to extract meaningful\ninformation and succinctly summarize it into coherent sentences. This approach allows us to generate\nhigh-quality text captions for a large amount of music audio, effectively minimizing the occurrence\nof hallucinations in the dataset.\n6\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\n1. Topic Pool\nMeta Topic 1: Games and Interactive Media\n- Game localization and cultural adaptation\n- The art of creating immersive game worlds\n2. Constructing Scenarios\nA user shares an image of a visually stunning \nvideo game environment and asks the chatbot \nfor insights on how game developers create \nimmersive worlds. They also request an epic \norchestral soundtrack that captures the essence \nof exploring such game worlds.\n3. Writing Chats\nThey blend art, technology, and storytelling to craft \nthese immersive environments.\nHow do game developers create these worlds?\n[image: A lush, expansive forest with towering trees \nand a river cutting through, from a video game.] \nCertainly. [music: An orchestral piece with soaring \nstrings and powerful brass, evoking grandeur.]\nCan you provide an epic orchestral soundtrack for \nexploring this world?\n4. Multimodalization\nMeta Topic 2: Environment and Scenarios\n······\nFigure 3: The construction process of the multimodal interleaved instruction dataset AnyInstruct is\ndivided into two stages: Generation of text-based conversations incorporating multimodal elements\nand Text-to-Multimodality Conversion. The first stage generates topics, scenarios, and textual\ndialogues, while the second stage produces the final multimodal dialogues.\nTraining Sample Construction.\nTo train the Language Model (LM), we employ various templates\nto construct multimodal sentences, which the LM then processes autoregressively. Further training\ndetails can be found in Appendix A.2. Additionally, We observe significant variation in sentence\nlengths across different modalities and datasets. To enhance training efficiency, samples from the\nsame dataset are concatenated into a long sequence, adhering to the model’s maximum sequence\nlength. Consequently, each token in the sequence contributes to the loss.\n4.2\nMultimodal Interleaved Instruction Data Construction\nEffective human-machine interaction should permit the exchange of information in a variety of\ninterleaved modalities. However, the increasing number of modalities in conversation significantly\ncomplicates the data collection process. To our knowledge, there is currently no large-scale instruction\ndataset involving more than two modalities. This poses a significant limitation on the development of\na comprehensive model capable of managing dialogues with multiple, intertwined modalities.\nTo overcome this limitation, we draw inspiration from the most recent research on data synthe-\nsis (Wang et al., 2022; Wu et al., 2023), and build a dataset comprised of 108k multi-turn conversa-\ntion samples with generative models. With careful curation, each synthetic conversation integrates\nmultiple modalities—text, speech, images, and music—in an interleaved manner. Specifically, our\ndata synthesis process is carried out in two stages, as illustrated in Figure 3.\nGeneration of text-based conversations incorporating multimodal elements.\nIn this phase, we\nemploy GPT-4 to generate a series of text-based conversations. Notably, we incorporate non-text\nmodality in the form of their textual descriptions within these conversations. To ensure high-quality\ndata at scale, we divide this stage into three steps. (1) Initially, we brainstorm 100 meta topics to\ncover a broad spectrum of scenarios related to audiovisual elements and we employ GPT-4 to expand\nthese meta-topics into 20,000 specific topics. (2) Subsequently, we prompt LLM to generate specific\ndialogue scenarios based on these topics. Acknowledging the intrinsic constraints of a text-based\nLLM in generating multimodal elements, we prepare several demonstrations that encompass as\nmany modality combinations as possible. While generating scenarios, a subset is sampled from\nthis demonstration pool, serving as examples for the LLM. This approach guides the model to\neffectively synthesize varied and contextually appropriate conversational scenarios. (3) Finally, we\nutilize GPT-4 to generate multi-turn conversations derived from scenarios. In these synthesized\ndialogues, multimodal elements, including images and music, are depicted through detailed textual\nrepresentations. We curate a diverse range of conversation examples, similar to scenario generation,\nto prompt the model into creating dialogues with the widest possible variety of modalities. As a\nresult, we compiled a substantial corpus of multimodal conversational data in solely textual format.\nText-to-Multimodality Conversion.\nIn this phase, we employ advanced generative models to\nconvert textual descriptions into multimodal elements. We use OpenAI’s DALL-E-3 (Betker et al.,\n2023) for image generation, MusicGen (Copet et al., 2023) for music composition, and Microsoft\n7\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nAzure’s text-to-speech API (Microsoft) for speech synthesis from user’s instructions and model’s\ntext responses.\nAfter filtering, we obtain a dataset of 108k high-quality multimodal dialogues, featuring a variety of\nmultimodal combinations. This dataset includes around 205k images, 503k voice recordings, and\n113k music tracks. Additionally, we enhanced our dataset by extracting dialogues from existing\ntext-only instruction datasets well-suited for spoken narration. This results in 100k voice dialogues\nthrough the employment of text-to-speech models.\nThe two-stage approach efficiently collected a diverse array of high-quality multimodal conversations\nat scale. Appendix D provides the prompts used during the data synthesis process.\n5\nExperiment\n5.1\nEvaluation\nWe evaluate the fundamental capabilities of the pre-trained base AnyGPT (Section 3), covering\nmultimodal understanding and generation tasks for all modalities. This evaluation aimed to test the\nalignment between different modalities during the pre-training process. Specifically, we test both\ntext-to-X and X-to-text tasks for each modality, where X is image, music, and speech separately.\nTo simulate real-world scenarios, all evaluations are conducted in a zero-shot mode. This means that\nAnyGPT will be not fine-tuned nor pre-trained on downstream training samples during the evaluation\nprocess. This challenging evaluation setting requires the model to generalize to an unknown test\ndistribution, showcasing the generalist abilities of AnyGPT across different modalities. The evaluation\nresults demonstrate that AnyGPT, as a generalist multimodal language model, achieves commendable\nperformance on various multimodal understanding and generation tasks.\n5.1.1\nImage\nImage Understanding\nWe assess the image comprehension capabilities of AnyGPT on the image\ncaptioning task. The comparison results are presented in Table 2. We utilize the MS-COCO 2014\ncaptioning benchmark (Lin et al., 2014) and adopt the Karpathy split testset following previous\nstudies (Li et al., 2023; Tang et al., 2023b).\nMethod\nCIDEr ↑\nFlamingo (9B) (Alayrac et al., 2022)\n79.4\nFlamingo (80B)\n84.3\nEmu (14B) (Sun et al., 2023b)\n112.4\nDreamLLM (8B) (Dong et al., 2023)\n115.4\nInstructBLIP (14B) (Dai et al., 2023)\n102.2\nSEED-LLaMA (8B) (Ge et al., 2023b)\n123.6\nAnyGPT (8B)\n107.5\nTable 2: Comparison results on image captioning task. Results in grey indicate models have trained\non training samples.\nImage Generation\nThe results of the text-to-image generation task are presented in Table 3. To\nensure consistency with previous research (Koh et al., 2023; Ge et al., 2023b; Sun et al., 2023a), we\nrandomly select 30k images from the MS-COCO validation set and use CLIPscore as the evaluation\ncriterion. This metric computes a similarity score between the generated image and its corresponding\ncaption from a real image, based on CLIP-ViT-L (Radford et al., 2021).\n5.1.2\nSpeech\nASR\nWe evaluate the performance of AnyGPT on the Automatic Speech Recognition (ASR) task by\ncalculating the Word Error Rate (WER) on the test-clean subsets of the LibriSpeech dataset (Panay-\notov et al., 2015). We use Wav2vec 2.0 and Whisper Large V2 as baselines. Wav2vec 2.0 is\n8\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nMethod\nCLIPscore ↑\nGILL (Koh et al., 2023)\n0.67\nEmu\n0.66\nSEED-LLaMA\n0.69\nAnyGPT\n0.65\nTable 3: Comparison results on text-to-image generation task. We adopt MS-COCO captions to\ngenerate images and calculate the CLIP similarity score (CLIPscore) for evaluation.\npre-trained with 60,000 hours of speech and fine-tuned on LibriSpeech, while Whisper Large V2 is\nevaluated in a zero-shot setting but is trained with 680,000 hours of speech. The results are shown in\nTable 4.\nMethod\nWER ↓\nHuman-level (Amodei et al., 2016)\n5.8\nWav2vec 2.0 (Baevski et al., 2020)\n2.7\nWhisper Large V2 (Radford et al., 2022)\n2.7\nAnyGPT\n8.5\nTable 4: Comparison results on ASR task. We use Word Error Rate (WER) as the metric.\nTTS\nWe conduct a zero-shot Text-to-Speech (TTS) evaluation on the VCTK dataset. The results\nare presented in Table 5. We evaluate the TTS systems with speaker similarity and Word Error\nRate (WER), where WER is focused on speech quality. More experimental details can be found in\nAppendix C.\nMethod\nWER ↓\nSIM ↑\nGround Truth\n1.9\n0.93\nVALL-E (Wang et al., 2023)\n7.9\n0.75\nUSLM (Zhang et al., 2023b)\n6.5\n0.84\nAnyGPT\n8.5\n0.77\nTable 5: Comparison results on TTS task.\n5.1.3\nMusic\nwe evaluate AnyGPT’s performance on the MusicCaps benchmark (Agostinelli et al., 2023) for both\nmusic understanding and generation tasks. We utilize the CLAPscore (Wu et al., 2022; Huang et al.,\n2023) score as the objective metric, which measures the similarity between the generated music and a\ntextual description.\nFor the evaluation of music captioning, we found that existing objective metrics may be limited in\nexpressing the performance in the music captioning task. The diversity and subjectivity of music lead\nto varying opinions from individuals. Only specific music genres and instruments possess distinctive\ncharacteristics that can be easily recognized. While recent studies (Gardner et al., 2023) have\nexplored this issue, it remains a challenging problem to address. To ensure an objective evaluation,\nwe compute CLAPscore of <music, real caption> pairs and <music, generated caption> pairs for\ncomparison. These scores are averaged across the entire test set.\n5.2\nExample Demonstrations\nAfter fine-tuning on the AnyInstruct-108k dataset, AnyGPT demonstrates the capability and potential\nin any-to-any multimodal dialogue. We provide compelling conversation examples of AnyGPT in\nAppendix E. These examples showcase AnyGPT is capable of comprehending and reasoning contents\n9\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nMethod\nCLAPscore ↑\nMusic understanding\n<music, real caption>\n0.16\n<music, generated caption>\n0.11\nMusic generation\nRiffusion (Forsgren & Martiros, 2022)\n0.19\nMousai (Schneider et al., 2023)\n0.23\nAnyGPT\n0.14\nTable 6: Comparison results for music understanding and generation tasks. A metric scoring the\nalignment between music and textual captions is reported (CLAPscore). For music captioning, the\nCLAPscore of <music, real caption> pair and <music, generated caption> pair are computed for\ncomparison.\nacross various modalities in any combination. Specifically, AnyGPT can comprehend instructions\ninterwoven with multiple modalities, including text, voice, images, and music, and can adeptly select\nthe appropriate multimodal combination for its reply. The two-stage framework of semantic-acoustic\nhierarchical modeling empowers AnyGPT to generate voice responses that matches the timbre and\nemotion of a 3-second speech prompt. For additional examples and to experience the speech and\nmusic content, we highly recommend visiting the demo page.\n6\nConclusion\nIn this work, we introduced AnyGPT, an any-to-any multimodal language model that utilizes discrete\nrepresentations for the unified processing of various modalities, including speech, text, images,\nand music. Discrete multimodal representations facilitate a seamless integration of new modal-\nities—comparable to incorporating a foreign language—without necessitating alterations to the\nexisting LLM architecture or training paradigms. To equip the model to handle arbitrary combina-\ntions of multimodal inputs and outputs, we synthesize the first large-scale any-to-any multimodal\ninstruction dataset, AnyInstruct-108k, consisting of multi-turn conversations that intricately inter-\nweave various modalities. Experimental results indicate that AnyGPT achieves promising results\nin various cross-modal tasks and demonstrates that discrete representations can effectively and\nconveniently unify multiple modalities within a unified large language model.\nLimitations and Future Work\nAny-to-Any Multimodal LLM Benchmark\nThe domain of any-to-any multimodal large language\nmodels (LLMs) is an emerging field of research. However, the lack of a dedicated benchmark to\nevaluate the models’ capabilities across multiple dimensions, as well as to mitigate potential risks,\npresents a considerable challenge. Consequently, the development of a comprehensive benchmark is\nimperative.\nEnhancing LLMs\nAlthough the multimodal LLMs with discrete representations can be trained\nstably, a higher loss is observed compared to unimodal training, preventing optimal performance in\neach modality. Potential strategies to improve multimodal fusion could involve scaling LLMs and\ntokenizers or adopting a Mixture-Of-Experts (MOE) architecture to better manage diverse data and\noptimize performance.\nBetter Tokenizer\nIn multimodal LLMs employing discrete representations, the tokenizer’s quality\nsets a ceiling for the model’s comprehension and generative potential. Enhancing the tokenizer can\nbe approached from various angles, including the adoption of superior codebook training methods,\nthe development of more cohesive multimodal representations, and the application of information\ndisentanglement across various modalities.\".\nLonger Context\nMultimodal content, such as images and audio, often spans extensive sequences.\nAnyGPT, for instance, limits music modeling to 5 seconds, significantly restricting the practical\n10\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nusefulness of its audio output. Moreover, for any-to-any multimodal dialogue, an extended context\nallow for a higher number of conversational exchanges, thereby enriching the interaction’s depth and\ncomplexity.\n11\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nReferences\nLaion-aesthetics. https:\/\/laion.ai\/blog\/laion-aesthetics\/, 2022a.\nLaion coco: 600m synthetic captions from laion2b-en. https:\/\/laion.ai\/blog\/laion-coco\/,\n2022b.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\nArXiv preprint, abs\/2303.08774, 2023. URL https:\/\/arxiv.org\/abs\/2303.08774.\nAndrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghi-\ndour, and Christian Havnø Frank.\nMusiclm: Generating music from text.\nArXiv preprint,\nabs\/2301.11325, 2023. URL https:\/\/arxiv.org\/abs\/2301.11325.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language\nmodel for few-shot learning. ArXiv preprint, abs\/2204.14198, 2022. URL https:\/\/arxiv.org\/\nabs\/2204.14198.\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl\nCase, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-\nend speech recognition in english and mandarin. In International conference on machine learning,\npp. 173–182. PMLR, 2016. URL https:\/\/arxiv.org\/abs\/1512.02595.\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben\nMorais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: A massively-\nmultilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation\nConference, pp. 4218–4222, Marseille, France, 2020. European Language Resources Association.\nISBN 979-10-95546-34-4. URL https:\/\/aclanthology.org\/2020.lrec-1.520.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations. In Hugo Larochelle, Marc’Aurelio Ranzato,\nRaia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https:\/\/proceedings.neurips.cc\/\npaper\/2020\/hash\/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https:\/\/cdn. openai. com\/papers\/dall-e-3. pdf, 2(3):8, 2023.\nZalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,\nDominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a language\nmodeling approach to audio generation. IEEE\/ACM Transactions on Audio, Speech, and Language\nProcessing, 2023a.\nZalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco\nTagliasacchi. Soundstorm: Efficient parallel audio generation. ArXiv preprint, abs\/2305.09636,\n2023b. URL https:\/\/arxiv.org\/abs\/2305.09636.\nGuoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan\nSu, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe,\nShuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Zhao You, and Zhiyong\nYan. Gigaspeech: An evolving, multi-domain ASR corpus with 10, 000 hours of transcribed\naudio. In Hynek Hermansky, Honza Cernocký, Lukás Burget, Lori Lamel, Odette Scharenborg,\nand Petr Motlícek (eds.), Interspeech 2021, 22nd Annual Conference of the International Speech\nCommunication Association, Brno, Czechia, 30 August - 3 September 2021, pp. 3670–3674. ISCA,\n2021. doi: 10.21437\/Interspeech.2021-1965. URL https:\/\/doi.org\/10.21437\/Interspeech.\n2021-1965.\n12\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and\nAlexandre Défossez. Simple and controllable music generation. ArXiv preprint, abs\/2306.05284,\n2023. URL https:\/\/arxiv.org\/abs\/2306.05284.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. ArXiv preprint, abs\/2305.06500, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2305.06500.\nAlexandre D’efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. ArXiv preprint, abs\/2210.13438, 2022. URL https:\/\/arxiv.org\/abs\/2210.\n13438.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jian-Yuan\nSun, Hongyu Zhou, Hao-Ran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.\nDreamllm: Synergistic multimodal comprehension and creation. ArXiv preprint, abs\/2309.11499,\n2023. URL https:\/\/arxiv.org\/abs\/2309.11499.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net, 2021. URL https:\/\/openreview.net\/forum?id=YicbFdNTTy.\nSeth* Forsgren and Hayk* Martiros. Riffusion - Stable diffusion for real-time music generation.\n2022. URL https:\/\/riffusion.com\/about.\nJosh Gardner, Simon Durand, Daniel Stoller, and Rachel M. Bittner.\nLlark: A multimodal\ninstruction-following language model for music. 2023. URL https:\/\/api.semanticscholar.\norg\/CorpusID:263835328.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. ArXiv preprint, abs\/2307.08041, 2023a. URL https:\/\/arxiv.org\/abs\/2307.\n08041.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making\nllama see and draw with seed tokenizer. ArXiv preprint, abs\/2310.01218, 2023b. URL https:\n\/\/arxiv.org\/abs\/2310.01218.\nRongjie Huang, Jia-Bin Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye,\nJinglin Liu, Xiaoyue Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-\nenhanced diffusion models. ArXiv preprint, abs\/2301.12661, 2023. URL https:\/\/arxiv.org\/\nabs\/2301.12661.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. ArXiv preprint, abs\/2305.17216, 2023. URL https:\/\/arxiv.org\/abs\/2305.17216.\nDan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig\nAdam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model\nfor zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International Conference on\nMachine Learning, 2023. URL https:\/\/api.semanticscholar.org\/CorpusID:256390509.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nConference on Computer Vision, 2014. URL https:\/\/api.semanticscholar.org\/CorpusID:\n14113767.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek\nHoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models\nwith vision, language, audio, and action. ArXiv preprint, abs\/2312.17172, 2023. URL https:\n\/\/arxiv.org\/abs\/2312.17172.\n13\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nMicrosoft. Microsoft azure text-to-speech api. https:\/\/azure.microsoft.com\/en-us\/products\/\nai-services\/ai-speech.\nJunting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding.\nArXiv preprint, abs\/2307.00716, 2023. URL https:\/\/arxiv.org\/abs\/2307.00716.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR\ncorpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April\n19-24, 2015, pp. 5206–5210. IEEE, 2015. doi: 10.1109\/ICASSP.2015.7178964. URL https:\n\/\/doi.org\/10.1109\/ICASSP.2015.7178964.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A\nlarge-scale multilingual dataset for speech research. In Helen Meng, Bo Xu, and Thomas Fang\nZheng (eds.), Interspeech 2020, 21st Annual Conference of the International Speech Communi-\ncation Association, Virtual Event, Shanghai, China, 25-29 October 2020, pp. 2757–2761. ISCA,\n2020. doi: 10.21437\/Interspeech.2020-2826. URL https:\/\/doi.org\/10.21437\/Interspeech.\n2020-2826.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp.\n8748–8763. PMLR, 2021. URL http:\/\/proceedings.mlr.press\/v139\/radford21a.html.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. ArXiv preprint, abs\/2212.04356, 2022.\nURL https:\/\/arxiv.org\/abs\/2212.04356.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020. URL http:\/\/jmlr.org\/papers\/\nv21\/20-074.html.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI\n2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pp. 234–241. Springer, 2015.\nFlavio Schneider, Zhijing Jin, and Bernhard Schölkopf. Moûsai: Text-to-music generation with\nlong-context latent diffusion. ArXiv preprint, abs\/2301.11757, 2023. URL https:\/\/arxiv.org\/\nabs\/2301.11757.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,\nYongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models\nare in-context learners. ArXiv preprint, abs\/2312.13286, 2023a. URL https:\/\/arxiv.org\/abs\/\n2312.13286.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. ArXiv\npreprint, abs\/2307.05222, 2023b. URL https:\/\/arxiv.org\/abs\/2307.05222.\n14\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nZineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2:\nIn-context, interleaved, and interactive any-to-any generation. ArXiv preprint, abs\/2311.18775,\n2023a. URL https:\/\/arxiv.org\/abs\/2311.18775.\nZineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation\nvia composable diffusion. ArXiv preprint, abs\/2305.11846, 2023b. URL https:\/\/arxiv.org\/\nabs\/2305.11846.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv preprint, abs\/2307.09288, 2023. URL https:\/\/arxiv.org\/\nabs\/2307.09288.\nAäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.\nIn Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.\nVishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pp. 6306–6315, 2017. URL https:\/\/proceedings.neurips.cc\/paper\/\n2017\/hash\/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html.\nChengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language\nmodels are zero-shot text to speech synthesizers. ArXiv preprint, abs\/2301.02111, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2301.02111.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\nArXiv preprint, abs\/2212.10560, 2022. URL https:\/\/arxiv.org\/abs\/2212.10560.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal\nllm. ArXiv preprint, abs\/2309.05519, 2023. URL https:\/\/arxiv.org\/abs\/2309.05519.\nYusong Wu, K. Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov.\nLarge-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption\naugmentation. ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pp. 1–5, 2022. URL https:\/\/api.semanticscholar.org\/CorpusID:\n253510826.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec. IEEE\/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 30:495–507, 2021. URL https:\/\/api.semanticscholar.org\/CorpusID:\n236149944.\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.\nIn Conference on Empirical Methods in Natural Language Processing, 2023a. URL https:\n\/\/api.semanticscholar.org\/CorpusID:258762683.\nXin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified\nspeech tokenizer for speech large language models. ArXiv preprint, abs\/2308.16692, 2023b. URL\nhttps:\/\/arxiv.org\/abs\/2308.16692.\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae\nYu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-\nscale corpus of images interleaved with text.\nArXiv preprint, abs\/2304.06939, 2023.\nURL\nhttps:\/\/arxiv.org\/abs\/2304.06939.\n15\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nA\npretraining\nA.1\nData\nModality\nDataset\nDescription\nSample Rate\nInterleaved\nImage-Text\nMMC4-core-ff\n101M image-interleaved documents collected from Common Crawl.\nWe use the mmc4-core split which is consist of 7.3M documents.\n0.05\nImage-Text\nLaion-2B\n2B image-text pairs from web.\n0.3\nLAION-COCO\n600M image-text pairs, where the caption is generated by BLIP.\nJourneyDB\n4429K Midjourney images, with image caption.\nLAION-Aesthetics\nSeveral collections of subsets from LAION 5B with high visual quality.\nSpeech-Text\nMultilingual Librispeech\nProcessing audiobooks read from LibriVox,\nwe used a 44,000-hour subset of English.\n0.13\nCommonVoice\nMicrophone recordings from internet volunteers,\nof which we used a 3000-hour subset of English.\n0.27\nGigaSpeech\n10,000 hours of English voice data sourced\nfrom audiobooks, podcasts, and YouTube videos.\nMusic-Text\nYoutube-Music-1M\n100M music-text pairs from Youtube.\n0.25\nMusicGen-Synthesis\n20k music-text pairs extracted\nfrom the AnyInstruct-108k dataset, synthesized by MusicGen.\nTable 7: Details of data used in pre-training stage.\nA.2\npre-training\nWe employ various templates to construct multimodal sentences, ensuring a diverse spectrum within\nour pre-training data. Each non-text modality content is identified by special tokens placed at both\nthe beginning and end. Typically, the paired data comprises a non-text modality (X) - such as images,\nPre-training Stage\nFine-tuning Stage\nGradient clipping\n(Global-norm)\n1.0\n1.0\nBatch size\n480\n64\nMax length\n4500\n4500\nTraining steps\n81000\n5000\nLearning rate scheduler\ncosine\ncosine\nPeak learning rate\n6e-5\n2e-5\nWarmup ratio\n0.03\n0.03\nOptimizer\nAdam\nAdam\nGPU\nA100\nA100\nTable 8: Training hyperparameters used in experiments.\nspeech, or music - and its corresponding text, which could be a caption or transcription. We prompt\nOpenAI GPT-4 to generate hundreds of bidirectional instructions, specifically X-to-text or text-to-X\nsuch as \"Please generate an image based on the provided text.\" Given a token sequence (S) and related\ntext (T), we randomly pick a generation direction alongside an instruction (I) from our pre-established\npool, forming a triplet (I, S, T). This triplet is then incorporated into a sequence using the template\n[Human]: {I}.{S}<eoh>. [AnyGPT]: {T}<eos>. or its variant\n[Human]: {I}. This is input:{T}<eoh>. [AnyGPT]: {S}<eos>., depending on the generation\ndirection.\nFor interleaved multimodal data, like a web document with interspersed images and text, we directly\nreplace non-text content with the corresponding tokens sequence as they naturally form sentences.\n16\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nAs most of the image and music data are sourced from the web, there is a certain level of noise that can\naffect the quality of multimodal generation. Consequently, after the initial pre-training, we selectively\nutilized high-quality datasets—JourneyDB and LAION-Aesthetics for text-to-image generation, and\nLAION-COCO for image captioning. For music data, we incorporated the AnyInstruct-108k dataset.\nThe remaining data were kept unchanged, and we continued to pre-train the model for an additional\n4000 steps.\nWe report the detailed training hyperparameters of AnyGPT in Tab 8.\nB\nInstruction Tuning\nAuto-regressive Language Model\nTranscription：\ngenerate a music \nfor this image.\n“ Enjoy this \n  music piece! ”\nmusic\nde-tokenizer\nspeech \nde-tokenizer\nspeech\ntokenizer\nimage\ntokenizer\nTranscription：\nEnjoy this \nmusic piece!\nFigure 4: An example of an AnyGPT multimodal dialogue: the input is an image and a voice command\nto generate music. The output is music that meets the requirements, along with corresponding text\nand voice responses. All data are processed into discrete tokens and are autoregressively processed\nby the LLM.\nC\nEvaluation\nGenerated Modality\nText\nImage\nSpeech\nMusic\nDecoding Strategy\nBeam Search\nSampling\nSampling\nSampling\nBeam size\n5\n-\n-\n-\nTop-P\n-\n0.7\n0.7\n1.0\nRepetition Penalty\n1.0\n1.0\n1.0\n1.15\nTable 9: Details of generation decoding strategies used in evaluation.\nWe conduct a zero-shot Text-to-Speech (TTS) evaluation on the VCTK dataset. There is no overlap\nin speakers between our training data and the VCTK dataset. We randomly select a 3-second clip\nfrom each speaker as the vocal prompt along with a separate text as input.\nThe results can be found in Table 5. We evaluate the TTS systems with speaker similarity and WER.\nTo evaluate the speaker similarity between the generated speech and the prompt speech, we employ\nWavLM-TDNN2. It can generate speaker embeddings for both the generated speech and the prompt\nspeech, then compute the cosine similarity between these embeddings. WER is calculated using the\n2https:\/\/github.com\/yangdongchao\/UniAudio\/blob\/main\/UniAudio\/tools\/evaluation\/\ncompute_similarity_vc.py\n17\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nWhisper medium model to transcribe the generated speech, with lower WER indicating higher quality\nof the synthesized speech.\nWe compare our model with VALL-E and USLM, both of which employ two autoregressive models\nfor speech modeling. They utilize Encodec and SpeechTokenizer, respectively, as speech tokenizers.\nD\nPrompts for Constructing Multimodal Interleaved Instruction Data\nIn the first stage of our pipeline to construct multimodal interleaved instruction data (Sec. 4.2) with\nGPT4. To facilitate reproducibility, we detail our prompts to the language model for brainstorming a\ntopic pool (Fig. 5), constructing chatting scenarios (Fig. 6), and detailing the chat contents (Fig. 7),\nwith multimodal content written as their text descriptions.\nPrompt: Please list me 50 **non-academic** conversation topics about {metatopic}\nbetween an ordinary person and a helpful chatbot. Each topic should be made up of 1-10\nwords and the conversation contain understanding and generation of images or music.\nGPT4: {50 sampled topics}\nPrompt: continue\nGPT4: {50 more sampled topics}\n· · · · · ·\nFigure 5: Prompts for brainstorming chat topics. We prepare 100 metatopic and repeat the conversa-\ntion to brainstorm topics for 4 rounds for each of the metatopic. This gives 200 topics per metatopic\nand a total of 20,000 topics in our final topic pool.\nPrompt:\nYou are a creative assistant. I am now asking you to help me brainstorm some chatting\nscenarios where the user asks the agent for help.\nNote that the scenarios should be between ordinary people and a helpful chatbot, and it should\nnot be an academic discussion!\nDuring the conversation, the speakers can use images or music to help convey information\n(but do not use video!). And the user should ask questions about it if he\/she provides an\nimage or a piece of music.\nNote that the image should not be a chart.\nNote that the images and music should not be the famous masterpieces that may arise\ncopyright issues.\nHere are some of my ideas, and please show me more in the same format as mine.\n{demonstrations}\nHere’s the topics for you to try: {topics}\nNow it’s your turn. In these scenarios, {requirements}.\nGPT4:\n{synthetic scenarios of the provided topics, following requirements}\nFigure 6: Prompts for constructing chat scenarios.\nIn each API call, we sample 5 different\n{demonstrations}, with each containing a topic and detailed description of the scenarios. And we\nsample 10 different {topics} for GPT4 to synthesize scenarios. To ensure the diversity of user and\nchatbot actions, we explicitly sample {requirements} from “the user provide images”, “the user\nshare music”, “the user asks for music”, and “the user asks for images”. We up weight “the user share\nmusic” as we observe that the model tends to omit this requirement.\n18\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nPrompt:\nYou are helping me to write conversations about a user talking to a chatbot named AnyGPT.\nIn the conversations, both the user can provide images or music to help express\nher\/his needs and ideas. And the chatbot AnyGPT can also respond to the user with images or\nmusic in its utterances.\nThe images and music in the chat are in the form of image descriptions and music\ndescriptions like [image: description] and [music: description], respectively. The user should\nprovide images and music in this format and the chatbot will respond to the user like this as\nwell.\nNote that at most one music appears in one conversation and the description of mu-\nsic should be straightforward, focusing on genres and instruments, and never mention a\nknown music directly.\nBefore each conversation, I will first show you a scenario and you can start writing\nabout the chat.\nHere is an example:\n—\n{demonstrations}\n—\nNow it’s your turn for the next conversation.\nYou only need to answer following\nthe format in which the user and AnyGPT take turns.\nThe conversation should be consistent with the introduction to the scenario.\nRemember that the utterances should be concise, try to use 5-15 words per utterance.\nNote that: the user utterance should always be a question or instruction.\nIn some turns, the user provides an image or a piece of music and asks a question or makes an\ninstruction to AnyGPT relating to the provided image or music.\nIn other turns, the user requests AnyGPT to generate the required images or music.\nNote that: the description of music should focus on genres, style, and instruments. And make\nthe description of images and music within [image: ] or [music: ] more detailed.\nNote that: never directly include a famous person’s name in the image descriptions or mention\na piece of known music in the music description.\nTips: when the user asks to convert between images and music, AnyGPT should first utter his\nunderstanding of the input image or music before generating the requested result.\nKeep the dialog in 2 or 3 rounds.\nEach dialog should include one music and at most 2 images.\n—\nIn this conversation, {new_scenario_description}\nGPT4:\n{A synthetic chat according to the scenario description.}\nFigure 7: Prompts for writing chat content. For each API call, we sample 3 demonstrations. Each\ndemonstration contains a scenario description, as the {new_scenario_description}, and the corre-\nsponding chat.\n19\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nE\nExamples Demonstration\nFigure 8: Speech conversations (Voice Clone)\n20\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 9: Speech Instruction + Image →Text + Music + Speech Response\n21\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 10: Speech Instruction + Music →text + Music + Speech Response\n22\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 11: Speech Instruction + Image →text + Music + Speech Response\n23\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 12: Text →Image + Music\n24\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 13: Text + Image →Music\n25\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 14: Text + Image →Text + Music\n26\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 15: Text + Music →Text + Image\n27\nAnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\nFigure 16: Text + Music →Muisc\n28\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling.pdf"}
{"title":"World Model on Million-Length Video And Language With Blockwise RingAttention","authors":"Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel","summary":"Enabling long-context understanding remains a key challenge in scaling\nexisting sequence models -- a crucial component in developing generally\nintelligent models that can process and operate over long temporal horizons\nthat potentially consist of millions of tokens. In this paper, we aim to\naddress these challenges by providing a comprehensive exploration of the full\ndevelopment process for producing 1M context language models and video-language\nmodels, setting new benchmarks in language retrieval and new capabilities in\nlong video understanding. We detail our long context data curation process,\nprogressive context extension from 4K to 1M tokens, and present an efficient\nopen-source implementation for scalable training on long sequences.\nAdditionally, we open-source a family of 7B parameter models capable of\nprocessing long text documents and videos exceeding 1M tokens.","url":"http:\/\/arxiv.org\/abs\/2402.08268v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.08268v4","published":1707810456000,"comment":null,"pdf_text":"Published as a conference paper at ICLR 2025\nWORLD MODEL ON MILLION-LENGTH VIDEO AND\nLANGUAGE WITH BLOCKWISE RINGATTENTION\nHao Liu∗Wilson Yan∗Matei Zaharia\nPieter Abbeel\nUC Berkeley\nABSTRACT\nEnabling long-context understanding remains a key challenge in scaling existing\nsequence models – a crucial component in developing generally intelligent models\nthat can process and operate over long temporal horizons that potentially consist of\nmillions of tokens. In this paper, we aim to address these challenges by providing\na comprehensive exploration of the full development process for producing 1M\ncontext language models and video-language models, setting new benchmarks in\nlanguage retrieval and new capabilities in long video understanding. We detail our\nlong context data curation process, progressive context extension from 4K to 1M\ntokens, and present an efficient open-source implementation for scalable training\non long sequences. Additionally, we open-source a family of 7B parameter models\ncapable of processing long text documents and videos exceeding 1M tokens.\n1\nINTRODUCTION\nEnabling long-context understanding remains a key challenge in scaling existing sequence models—a\ncrucial step toward developing generally intelligent models that can process and operate over extended\ntemporal horizons, potentially involving millions of tokens. Current modeling approaches are\npredominantly limited to processing short sequences, whether in the form of language, images, or\nvideo clips (Brown et al., 2020; Touvron et al., 2023a;b; OpenAI, 2023; Brooks et al., 2024; Team\net al., 2023). As a result, these models fall short when tasked with understanding complex, long-form\nlanguage and visual contexts.\nHowever, training models to process sequences that exceed millions of tokens is a significant challenge\ndue to the high memory and computational costs, as well as the lack of long-context data. In this work,\nwe address these challenges by leveraging Blockwise RingAttention (Liu et al., 2024; Liu and Abbeel,\n2023), a technique that scales context size without approximations or overheads, enabling efficient\ntraining on long sequences. We curate an extensive dataset of long-form videos and books from\n∗Equal contribution. Email: hao.liu@cs.berkeley.edu, wilson1.yan@berkeley.edu\nCode and models of Large World Model (LWM) are available at largeworldmodel.github.io\/lwm\/.\nFigure 1\nComparison of context size in state-of-the-art LLMs. Our model and concurrent work\nGemini 1.5 both achieve a 1M context size, significantly outperforming other LLMs.\n1\narXiv:2402.08268v4  [cs.LG]  3 Feb 2025\nPublished as a conference paper at ICLR 2025\nFigure 2\nRetrieval comparisons against Gemini Pro and GPT-4. Needle retrieval comparisons\nagainst Gemini Pro and GPT-4 for each respective max context length – 32K and 128K. Our model\nperforms competitively while being able to extend to 8x longer context length. Note that in order to\nshow fine-grained results, the x-axis is log-scale from 0-128K, and linear-scale from 128K-1M.\npublic sources, covering a wide variety of activities and narrative structures. To address the scarcity of\nlong-form conversational datasets, we developed a model-based question-answering technique, where\na short-context model generates training data from books, significantly enhancing the model’s chat\ncapabilities over long sequences. To mitigate computational costs, we gradually extended context\nsize from an initial 4K tokens to 1M tokens, achieving a cost-effective and scalable approach for\nlong-context modeling.\nFollowing this, we further train our long-context language model to incorporate visual modalities,\nsuch as image and video. Contrary to existing popular vision-language models (Liu et al., 2023a;\nOpenAI, 2023; Chen et al., 2023a), we opt to additionally optimize next-token prediction losses\nfor image and video (generation) with a VQGAN (Esser et al., 2021) encoder. We encountered\nvarious challenges training on mixed modalities (video, image, text). To balance their unique\ncharacteristics - sequential information, visual detail, and linguistic content - we implement an\nefficient masked sequence packing strategy, as well as introduce careful loss balancing to retain short\ncontext accuracy. This approach handles varying sequence lengths more effectively than standard\nmethods. We also optimized the ratio of image, video, and text inputs in each batch, proposing an\nempirically effective balance for cross-modality learning. Since our model aims to model both textual\nand visual projections of the world through a large context window, drawing inspiration from prior\nwork on world models (Brooks et al., 2024; Ha and Schmidhuber, 2018), we name our work as Large\nWorld Model (LWM).\nOur contributions are threefold: (a) we train one of the largest context size transformers to date\non long text documents and videos and achieved competitive results on long video understanding\nand long context fact retrieval. (b) We discover a range of challenges associated with training on\nlong sequences and propose solutions for them: masked sequence packing to effectively train with\ndifferent sequence lengths and synthetic model-generating question-answering for effective attention.\n(c) We provide an open-source and optimized implementation for training with millions of tokens in\ncontext, as well as a family of Llama-based 1M context models capable of processing long documents\n(LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of 1M tokens.\n2\nMETHOD OVERVIEW\nWe train a large autoregressive transformer model with a large context window of up to one million\ntokens, building upon Llama2 7B (Touvron et al., 2023b). To achieve this goal, we implement\na two-stage training strategy. In Stage I (Section 3), we extend the context to 1M tokens using\nbook-length texts. This is followed by Stage II (Section 4), where we conduct joint training on diverse\nlong multimodal sequences, incorporating text-image data, text-video data, and book-length texts.\nOur model architecture is the standard autoregressive transformer design, as illustrated in Figure\n3. For a comprehensive overview of our training stages and the datasets employed, please refer to\nFigure 4.\n2\nPublished as a conference paper at ICLR 2025\nFigure 3\nModel Architecture. The LWM model is an autoregressive transformer trained on\nsequences of multimodal tokens. Each video frame is tokenized into 256 tokens using VQGAN,\nwhile text is processed using a Byte-Pair Encoding (BPE) tokenizer. These tokens—both image and\ntext—are combined and input into the transformer to autoregressively predict the next token. The\nmodel can handle various input-output modalities, including text, image, video, and text-video pairs.\nTo distinguish between images and text, special tokens <vision> and <\/vision> are used for\nimage and video frames, with <eof> and <eov> marking the end of these sequences. For simplicity,\ndelimiters are not shown in the figure.\n3\nSTAGE I: LEARNING LONG-CONTEXT LANGUAGE MODELS\nThis stage aims at first developing LWM-Text and LWM-Text-Chat, a set of long-context language\nmodels learned by training on progressively increasing sequence length data, and modifying positional\nencoding parameters to account for longer sequence lengths (see Section 3.1). In Section 3.2, we show\nhow to construct model-generated question-answering data for enabling long sequence conversations.\n3.1\nPROGRESSIVE TRAINING TOWARDS LONG CONTEXT\nLearning long-range dependencies over sequences of millions of tokens requires (1) memory efficient\ntraining to scale to such long sequences, as well as a need to (2) compute efficient training to extend\nthe context of our base language model. We outline our approach to these challenges, detailing our\nmethods for training on long sequences, designs for efficiency and stability, and experimental setup.\nTraining on long sequences has become prohibitively expensive due to memory constraints imposed\nby the quadratic complexity of attention weight computations. To address these computational\nlimitations, we leverage recent advancements in scaling context window size, particularly Blockwise\nRingAttention (Liu et al., 2024). This approach theoretically allows for an infinite context, bounded\nonly by available devices. We further enhance performance by fusing it with FlashAttention (Dao\net al., 2022) using Pallas (Bradbury et al., 2018) to optimize performance compared with using XLA\ncompiler. Notably, with enough tokens per device—already a given—the communication cost during\nsequence parallelism is fully overlapped by computation, resulting in no additional overhead.\nFor better efficiency, we adopt a training approach inspired by prior research on extending context (Jin\net al., 2023a), where our model is trained on progressively longer sequence lengths, starting from\n32K tokens and ending at 1M tokens in increasing powers of two. Intuitively, this allows the model\nto save compute by first learning shorter-range dependencies before moving onto longer sequences.\nFor extending positional embeddings to longer contexts, we adopt a simple, scaled-up version of the\napproach explored in Rozière et al. (2023), where the θ parameter for RoPE (Su et al., 2024) is scaled\nin proportion to the context length. We found this approach to be stable for extending positional\nembeddings with larger context lengths due to its simplicity, requiring the tuning of only a single\nhyperparameter. Specifically, we scale the θ parameter for RoPE alongside increases in context\nwindow sizes – the values are shown in Table 6. The progressive training of growing context sizes is\nshown in Figure 4.\nWe initialize from LLaMA-2 7B (Touvron et al., 2023b) as base language model and progressively\nincrease the effective context length of the model across 5 stages: 32K, 128K, 256K, 512K, and 1M.\nFor each stage, we train on different filtered versions of the Books3 dataset from The Pile (Gao et al.,\n2020). Table 6 details information about each training stage, such as the number of tokens, total\ntime, and the Books3 dataset filtering constraints. Each successive run is initialized from the prior\nsequence length.\n3\nPublished as a conference paper at ICLR 2025\nFigure 4\nCurated dataset and training process with progressively increasing data length\nand complexity. The diagram outlines a two-stage training process. Stage 1 extends text-based\nunderstanding using books datasets of increasing document lengths and token counts. Stage 2\nintegrates vision-language training. Pie charts display token distribution, showing that images and\nshort-frame videos dominate visual data, while mid-length text examples lead in the text corpus.\n3.2\nMODEL-GENERATED QUESTION-ANSWERING FOR EFFECTIVE CONTEXT\nWe construct a simple question-answering dataset to develop long-context chat capabilities. First, we\nsplit documents from the Books3 dataset into fixed chunks of 1,000 tokens, feed each chunk into\nour short-context language model, and prompt it to generate a question-answer pair based on the\ncontent. To create longer examples (e.g., 32K tokens), we concatenate adjacent chunks and append\nthe relevant question-answer pairs toward the end of the sequence in a chat format. The key intuition\nis that the model must learn to focus on any part of the context to answer the questions, as the relevant\ninformation can appear anywhere within the sequence.\nFor chat fine-tuning, we train each model on a mix of the UltraChat conversation dataset (Ding\net al., 2023) and our custom question-answering dataset, using approximately a 7:3 ratio. We found\nit crucial to pre-pack the UltraChat data to the training sequence length and keep these examples\nseparate from our question-answering data. This separation is necessary because UltraChat data\ngenerally contains a much higher proportion of loss tokens (due to densely packed, short questions\nin chat), whereas our question-answering data has long questions in chat thus a significantly lower\npercentage of loss tokens per sequence (< 1%). This difference arises from the long documents in the\ngiven context of our question-answering data, which are not included in loss calculations. Table 7\n4\nPublished as a conference paper at ICLR 2025\nprovides further training details for each run. Notably, we do not employ progressive training for any\nof the chat models; instead, we initialize them from their respective pretrained models at the same\ncontext length.\nSummary: Stage I progressively increase sequence lengths using our curated dataset: starting with\n32K tokens and gradually scaling up to 1M tokens. Model-generated question-answering data aids in\nlearning effective long context.\n3.3\nLANGUAGE EVALUATION RESULTS\n3.3.1\nSHORT CONTEXT TASKS\nTable 1 presents a comparative analysis between the Llama2-7B model with a 4K context and its\ncontext-expanded counterparts, ranging from 32K to 1M. The evaluation spans various language tasks,\ndemonstrating that expanding the context size does not compromise performance on short-context\ntasks. In fact, the results suggest that models with larger context capacities perform equally well, if\nnot better, across these tasks. This evidence indicates the absence of negative effects from context\nexpansion, highlighting the models’ capability to adapt to different task requirements without losing\nefficiency in shorter contexts.\nTable 1\nPerformance evaluation across language tasks, comparing Llama-2 7B (4K context window)\nand context-expanded variants of LWM-Text (32K to 1M). The results demonstrate that increasing\ncontext length does not significantly degrade performance on tasks with shorter contexts.\nLWM-Text\nTask \/ Metric\nLlama-2 7B\n32k\n128k\n256k\n512k\n1M\narc_challenge\/acc\n0.40\n0.43\n0.45\n0.44\n0.44\n0.43\narc_challenge\/acc_norm\n0.43\n0.47\n0.47\n0.46\n0.46\n0.46\nhellaswag\/acc\n0.57\n0.57\n0.57\n0.57\n0.56\n0.57\nhellaswag\/acc_norm\n0.77\n0.76\n0.76\n0.76\n0.75\n0.75\nmmlu\n0.39\n0.4\n0.41\n0.41\n0.36\n0.35\nopenbookqa\/acc\n0.32\n0.33\n0.31\n0.32\n0.33\n0.30\nopenbookqa\/acc_norm\n0.44\n0.44\n0.44\n0.43\n0.41\n0.41\n3.3.2\nRETRIEVAL TASK: SINGLE INFORMATION\nWe evaluate on the popular Needle In A Haystack task (gkamradt, 2023) – more specifically an\nversion (ArizeAI, 2023) that finds and retrieves random numbers assigned to randomized cities from\nthe context. Figure 2 shows that we can scale to far larger contexts compared to the current best\navailable LLMs. Figure 11 in Appendix shows nearly perfect retrieval accuracy over the entire context\nof our 1M context model. Appendix C shows more single needle retrieval results for our other shorter\ncontext length models.\n3.3.3\nRETRIEVAL TASK: MULTIPLE INFORMATION\nWe additionally examine the performance of our model on more complex variant of the needle\nretrieval task by mixing in multiple needles, as well as trying to retrieve a specific subset of them.\nFigure 5 shows multi-needle retrieval results under different settings. Our model generalizes well\nwhen retrieving a single needle from multiple needles in context, with slight degradation when asked\nto retrieve more than one needle. Table 2 shows multi-needle comparisons, where our model is able\nto perform competitively or better than GPT-4 at retrieving one needle, or slightly lower performance\nwhen retrieving more than one needle. Furthermore, our model is also able to perform well and\nextend to longer context lengths of up to 1M tokens and far outperforms any recent shorter context\nbaselines applies to longer sequence lengths through positional extrapolation techniques.. However,\nwe note that we see degradation in accuracy while increasing the difficulty of the needle retrieval\ntask, suggesting that there is still more room to improve on the 1M context utilization of our model.\nWe believe that our released model will provide a foundation for future work on developing longer\ncontext models, as well as encourage more challenging benchmarks that contain difficult long-range\ntasks that require higher levels of synthesis, rather than pure fact retrieval.\n5\nPublished as a conference paper at ICLR 2025\nTable 2\nMulti Needle in a Haystack. * denotes models after the completion of this paper.\nContext Length\nModel\nN = 2, R = 2\nN = 4, R = 1\nN = 4, R = 2\n32K\nGemini Pro (02\/23)\n0.34\n0.44\n0.6\nGPT-4-1106\n0.97\n0.95\n0.9\nLlama-3.1-8B-Instruct*\n0.87\n0.95\n0.93\nQwen2.5-7B-Instruct*\n1.0\n1.0\n0.97\nMistral-7B-Instruct-v0.3*\n0.98\n0.85\n0.83\nLWM-Text-1M (Ours)\n0.84\n0.97\n0.84\n128K\nGemini Pro (02\/23)\n-\n-\n-\nGPT-4-1106\n0.92\n0.8\n0.82\nLlama-3.1-8B-Instruct*\n0.98\n0.91\n0.87\nQwen2.5-7B-Instruct*\n0.98\n0.80\n0.90\nMistral-7B-Instruct-v0.3*\n0.85\n0.75\n0.68\nLWM-Text-1M (Ours)\n0.83\n0.98\n0.83\n1M\nGemini Pro (02\/23)\n-\n-\n-\nGPT-4-1106\n-\n-\n-\nLlama-3.1-8B-Instruct*\n0.27\n0.32\n0.18\nQwen2.5-7B-Instruct*\n0.0\n0.0\n0.0\nMistral-7B-Instruct-v0.3*\n0.05\n0.13\n0.10\nLWM-Text-1M (Ours)\n0.67\n0.84\n0.69\nFigure 5\nMultiple needles retrieval task with LWM-1M. N is the number of facts in the context,\nand R is the number of given facts model is asked to retrieve.\n3.3.4\nEVALUATION ON LOFT\nTable 3\nEvaluations on some benchmarks in the LOFT dataset.\nSetting: 512K Context\nLWM (512K)\nGPT-4o (128K)\nClaude 3 Opus (200K)\nQuora\n0.38\n0.23\n0.37\nNQ\n0.37\n0.22\n0.37\nHotPotQA\n0.72\n0.21\n0.32\nWe further evaluate our model on a coverage of the LOFT (Lee et al., 2024) dataset collection, we\nprovides a more natural set of benchmarks that examine capabilities for long-context models in the\ncontext of document retrieval, and RAG. The benchmark includes tasks such as duplication detection\n(Quora 1), document retrieval (HotpotQA (Yang et al., 2018)), and retrieval-based question-answering\n(NQ). Each dataset contains a corpus of 1000s of documents, and the model is asked to retrieve a set\nof document ids pertaining to its specific task (Quora, HotpotQA). For RAG (NQ dataset), the model\nis asked to answer the question using the given context. Table 3 shows evaluations results on 512K\ncontext length against various language model baselines.\nTakeaway: Long context capability enables LWM to outperform state-of-the-art text models at\nmultiple benchmarks. This demonstrates the effectiveness of our methods for enabling long context.\n4\nSTAGE II: EXTENDING TO LONG-CONTEXT VISION-LANGUAGE\nOur second stage aims to effectively joint train on long video and language sequences. We will intro-\nduce architecture modifications for LWM and LWM-Chat to incorporate vision input in Section 4.1.\n1https:\/\/quoradata.quora.com\/First-Quora-Dataset-Release-Question-Pairs\n6\nPublished as a conference paper at ICLR 2025\nTraining on varying sequence lengths is discussed in Section 4.2. The evaluation results are shown\nin Section 4.3. In this phase, we enhance the capabilities of the previously developed 1M context\nlanguage model, by finetuning it on vision-language data of various lengths. The datasets used and\nthe steps involved in the training process are illustrated in Figure 4.\n4.1\nARCHITECTURAL MODIFICATIONS FOR VISION\nWe use the pretrained VQGAN (Esser et al., 2021) from aMUSEd (Patil et al., 2024) that tokenizes\n256 × 256 input images to 16 × 16 discrete tokens. Videos are tokenized by applying the VQGAN\nper-frame, and concatenating the codes together. In order to distinguish between modalities when\ngenerating, as well as knowing when to switch, we introduce mechanisms to mark the end of text\ngeneration \/ beginning of vision generation, and vice-versa. For defining the end of vision generation,\nwe introduce new tokens, <eof> and <eov>, that represent end of frame (at the end of each\nvideo frame that is not the last video frame in the sequence), and end of vision (at the end of each\nsingle image, or at the end of the last frame in a video) boundaries respectively. For defining the\nend of text generation, we wrap the vision tokens with <vision> and <\/vision> (as text) text\ntokens. The model is trained with interleaved concatenations of vision and text tokens, and predicted\nautoregressively (see Figure 3).\n4.2\nTRAINING STEPS\nWe initialize from our LWM-Text-1M text model, and perform a similar process of progressive\ntraining on a large amount of combined text-image and text-video data, with the exception that we do\nnot additionally scale RoPE θ, as it already supports up to 1M context. Table 8 shows details for each\ntraining stage, where the model is initialized from the prior shorter sequence length stage. For each\nstage, we train on the following data:\n• LWM-1K: We train on large set of text-image dataset comprising of a mix of LAION-2B-en (Schuh-\nmann et al., 2022) and COYO-700M (Byeon et al., 2022). The datasets were filtered to only include\nimages with at least 256 resolution – in total roughly 1B text-image pairs. During training, we\nconcatenate the text-image pairs and randomly swap the order of the modalities to model both\ntext-image generation, unconditional image generation, and image captioning. We pack text-image\npairs to sequences of 1K tokens.\n• LWM-8K: We train on a text-video dataset mix of WebVid10M (Bain et al., 2021) and 3M Intern-\nVid10M (Wang et al., 2023) examples. Similar to prior works (Ho et al., 2022a;b; Villegas et al.,\n2022), we jointly train on both images and video with a 50-50 ratio of each modality. We pack\nimages to sequences of 8K tokens, and 30 frame videos at 4FPS. Similar to image training, we\nrandomly swap the order of modalities for each text-video pair.\n• LWM-Chat-32K\/128K\/1M: For the final 3 stages, we train on a combined mix of chat data\nfor each downstream task: (1) text-image generation, (2) image understanding, (3) text-video\ngeneration, and (4) video understanding. We construct a simple version of text-image and text-\nvideo chat data by sampling random subsets of the pretraining data augmented with chat format.\nFor image understanding, we using the image chat instruct data from ShareGPT4V (Chen et al.,\n2023a). Lastly, for the video understanding chat data, we use a combined mix of Valley-Instruct-\n73K (Luo et al., 2023) and Video-ChatGPT-100K instruct data (Maaz et al., 2023). For all short\ncontext data (image generation, image understanding, video generation), we pack sequences to the\ntraining context length. During packing, we found it crucial to mask out the attention so that each\ntext-vision pair only attends to itself, as well as re-weighting losses to make computation identical\nto training in a non-packed + padding training regime. For video understanding data, we uniformly\nsample a max number of frames to fit the training context length of the model if the video is too\nlong. During training, We allocate 25% of each batch to each of the 4 downstream tasks.\nFor the first two stages of training (LWM-1K and LWM-8K), we additionally mix 16% of the batch\nto be pure text data from OpenLLaMA (Geng and Liu, 2023), as we found it beneficial to preserve\nlanguage capabilities while training on vision data.\n7\nPublished as a conference paper at ICLR 2025\n00:00:00\n01:01:51\nUser: What vehicle was the person dressed in the t-rex costume riding?\nGPT-4V: I’m sorry, I can’t provide assistance with that request.\nGemini Pro Vision: A motorized scooter.\nVideo-LLaVA: The person dressed in a t-rex costume was riding a truck.\nLWM (Ours): The person dressed in a t-rex costume was riding a motorcycle.\nUser: What color jacket was the girl on the trampoline wearing?\nGPT-4V: The sequence provided does not include a deﬁnitive image of a girl on a trampoline; therefore, I cannot \ndescribe the color of the jacket.\nGemini Pro Vision: The girl on the trampoline was wearing a green jacket.\nVideo-LLaVA: The girl on the trampoline was wearing a black jacket.\nLWM (Ours): The girl on the trampoline was wearing a blue jacket.\n…\n1 hour YouTube compilation with > 500 clips\nFigure 6\nLWM excels in answering questions about a 1-hour YouTube video. This figure\ncompares LWM-Chat-1M with proprietary models like Gemini Pro Vision and GPT-4V, along\nwith open-source models. The test involves answering questions based on an hour-long YouTube\ncompilation containing over 500 video clips. LWM demonstrates superior performance in providing\naccurate answers requiring comprehension of extended video content.\nTable 4\nLong Video-MME Benchmark. * denotes models after the completion of this paper.\nMethod\nParameters\nFrames\nMedium (4min-15min)\nLong (30min-60min)\nGemini 1.5 Pro*\nUnknown\n≤1800\n74.3\n67.4\nGPT-4o*\nUnknown\n384\n70.3\n65.3\nLLaVA-Video*\n72B\n64\n68.9\n61.5\nVideoLLaMA 2*\n72B\n32\n59.9\n57.6\nLong-LLaVA*\n7B\n64\n51.4\n45.4\nVideo-LLaVA\n7B\n8\n38.1\n36.2\nLWM-1M\n7B\n≤1800\n63.7\n60.8\nSummary: Stage II training incorporates image and video. Building on Stage I, it gradually increases\nsequence lengths of vision and text input. Importantly, we found our masked sequence packing and\nmixing synthetic and chat data crucial to retain short context performance during our progressive\ntraining. Appendix B shows ablations when not using our training method on instruction-following\nand text-image understanding benchmarks.\n4.3\nVISION-LANGUAGE EVALUATION RESULTS\n4.3.1\nLONG VIDEO UNDERSTANDING\nAlthough vision-language model (Lin et al., 2023; OpenAI, 2023; Team et al., 2023) can ingest\nlong videos, this is commonly done by performing large temporal subsampling of video frames due\nto limited context length. For example, Video-LLaVA (Lin et al., 2023) is restricted to uniformly\nsampling 8 frames from a video, no matter how long the original video may be. As such, models may\nlose more fine-grained temporal information that is important for accurately answering any questions\nabout the video. In contrast, our model is trained on long sequences of 1M tokens, and as a result, can\nsimultaneously attend thousands of frames of videos to retrieve fine-grained information over short\ntime intervals. Table 4 shows long video evaluations on the Video-MME (Fu et al., 2024) benchmark,\ndemonstrating our model as the best performing model among its size class. Figure 6 shows an\nexample of our model correctly answering questions about a long, 1-hour YouTube compilation\nconsisting of more than 500 individual clips. Our baseline methods, on the other hand, generally\nhave difficulty answering the questions due to a limited number of frames. More results are shown in\nFigure 18 and Appendix F.\n8\nPublished as a conference paper at ICLR 2025\nA black dog\nAn elephant \nunder the sea\nA cube made \nof denim\nA glass of wine\nA yellow and black \nbus cruising through \na rainforest\nFireworks exploding in the sky\nWaves crashing against the shore\nFigure 7\nLWM’s ability to generate both static images and dynamic videos from text is shown. The\ntop row illustrates image, while the bottom rows show video.\n4.3.2\nIMAGE UNDERSTANDING AND SHORT VIDEO UNDERSTANDING\nWe evaluate LWM on standard benchmarks for image and short video understanding, with results\npresented in Table 5. Our model performs comparably to baselines but falls short of state-of-the-art\n(SOTA) models. This performance gap is not unexpected, given that SOTA models leverage vision\nbackbones that have undergone extensive CLIP training (Radford et al., 2021). In contrast, LWM\nutilizes discrete tokens from an off-the-shelf model (Patil et al., 2024). Discrete tokens result in greater\ninformation loss, particularly for OCR-like textual data, compared to continuous CLIP embeddings.\nMoreover, our model learns text-image alignment from scratch, while CLIP-based models benefit\nfrom large-scale pretraining. This work primarily focuses on long-context methodology, and we\ndefer additional training to future work due to computational constraints. A straightforward approach\nto improving benchmark scores would be to incorporate CLIP embeddings as additional input.\nDespite not achieving SOTA scores on these short video benchmarks, we believe LWM provides\nvaluable insights for future long-context language and video understanding and generation. The\nmodel’s performance could be enhanced through additional training and minor modifications. We\ninclude qualitative image understanding examples in Appendix E and qualitative video understanding\nexamples in Appendix F.\n4.3.3\nIMAGE AND VIDEO GENERATION\nThanks to a unified any-to-any architecture, our model can not only perform image\/video captioning\nand question-answering but also generate images and videos from text. Figure 7 demonstrates\nexamples of these capabilities. For autoregressive sampling, we employ classifier-free guidance (Ho\nand Salimans, 2022) on the logits, similar to previous works (Yu et al., 2022; Gafni et al., 2022). In\nthe unconditional branch, we initialize each sequence with <bos><vision>. For additional image\nand video generation examples, please refer to Appendices H and I, respectively.\nTakeaway: LWM excels in long video understanding by processing significantly more frames than\nprevious state-of-the-arts, resulting in better understanding. Moreover, its long-context enabled unified\nany-to-any architecture allows for versatile image and video and text understanding and generation.\nTable 5\nImage Understanding Benchmarks (left) and Video Understanding Benchmarks (right)\nMethod\nVisual Token\nVQAv2\nGQA\nSQA\nMiniGPT-4\nCLIP\n-\n30.8\n25.4\nOtter\nCLIP\n-\n38.1\n27.2\nInstructBLIP\nCLIP\n-\n49.2\n60.5\nLLaVA-1.5\nCLIP\n78.5\n62.0\n66.8\nLWM (ours)\nVQGAN\n55.8\n44.8\n47.7\nMethod\nMSVD\nMSRVTT\nTGIF\nVideoChat\n56.3\n45\n34.4\nLLaMA-Adapte\n54.9\n43.8\n-\nVideo-LLaMA\n51.6\n29.6\n-\nVideo-ChatGPT\n64.9\n49.3\n51.4\nLWM (ours)\n55.9\n44.1\n40.9\n9\nPublished as a conference paper at ICLR 2025\n5\nRELATED WORKS\nOur research builds upon existing efforts to extend the context windows of language models, enabling\nthem to process more tokens (Chen et al., 2023b; Tworkowski et al., 2023; Liu et al., 2023c).\nThese approaches often employ innovative extrapolation techniques to expand pretrained positional\nencodings, followed by model finetuning on longer context data. In contrast, our model takes a\nstraightforward approach by incrementally increasing θ in RoPE positional encodings alongside\nexpanding the training context window sizes, which we found to be effective. Additionally, there\nhave been investigations into architectures that avoid modeling pairwise interactions, such as sparse\nattention and sliding window techniques (Child et al., 2019; Beltagy et al., 2020). Prior research has\nexplored sequence parallelization (Li et al., 2021; Korthikanti et al., 2022, inter alia), though it is\nnot optimized for blockwise transformers or compatible with memory-efficient attention, both of\nwhich are critical for large context training. Our work further leverages large context transformer\ntechniques (Liu et al., 2024; Liu and Abbeel, 2023) to capture exact pairwise interactions in extended\nsequences for enhanced performance. Load-balancing strategies, such as skipping causal masked\ncomputation (Brandon et al., 2023; Li et al., 2023) offer room for further optimization. Concurrent\ndevelopments like Gemini 1.5 (Reid et al., 2024) reach 1M tokens context size in language and video.\nAdditionally, our approach relates closely to advances in instruction tuning (Taori et al., 2023; Chiang\net al., 2023; Geng et al., 2023), which focus on finetuning models with conversational data to boost\ntheir performance across diverse language tasks. We aim to extend these capabilities to the domain\nof long-sequence understanding in both video and language tasks. To achieve this, we extend the\nmodel’s context size by training on comprehensive datasets, including books and long videos, and\nfinetune on model-generated question-answering datasets to enhance its ability to handle extended\nconversational sequences.\nFurthermore, our research draws from work on integrating vision capabilities into language mod-\nels (Liu et al., 2023b; Lin et al., 2023; Awadalla et al., 2023; Zhang et al., 2023; Jin et al., 2023b;\nAiello et al., 2023). These efforts frequently utilize continuous embeddings (Radford et al., 2021; Li\net al., 2022) to encode visual information into embeddings for inputting into language models. While\nthese approaches benefit from CLIP’s cross-modal understanding to encode textual information from\nimages, their ability to predict text from visual input is limited, as is their capacity to learn from\ndiverse visual-language formats. In contrast, our autoregressive model, which processes \"tokens in,\ntokens out,\" allows greater flexibility in modeling various formats, including image-text, text-image,\ntext-video, video-text, and pure formats like video, image, or text. Our method is compatible with\nthese prior works, making it an interesting future direction to combine continuous embeddings as\ninput with discrete tokens and a long-context autoregressive model.\n6\nCONCLUSION\nIn conclusion, this paper tackles the critical challenge of enabling long-context understanding in\nsequence models, which is vital for developing generally intelligent systems capable of processing\nlarge temporal sequences. By exploring the development of 1M context language and video-language\nmodels, the work sets new benchmarks in language retrieval and long video understanding. We\noutline approaches to data curation and progressive context extension, accompanied by an efficient\nopen-source implementation for scalable training on long sequences. Moreover, we open-source a\nfamily of 7B parameter models capable of handling over 1M tokens in text and video.\nLimitations. While this work successfully develop a large large context of over 1M text and video\ntokens, and demonstrate promising results in processing hour-long videos and long documents, there\nare still some limitations that need to be addressed:\n• Improved tokenization and embedding. This work uses a vanilla image tokenizer for images and\nframe-by-frame tokenization for videos. Future work could explore video tokenization that takes\ntime redundancy into account, as well as including continuous embeddings as input to enrich image\nunderstanding.\n• Limited scale. Our models use more tokens per parameter than Chinchilla’s recommendation, but\nbeing much smaller than current large language models (100B+ parameters), our findings may\nnot directly apply to them. Extrapolating to larger scales should be done cautiously, as different\nscaling behaviors could emerge at those larger sizes.\n10\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nThis project is supported in part by Office of Naval Research grant N00014-21-1-2769 and ARO\nMURI (2023) on Neuro-Inspired Distributed Deep Learning. We thank Google TPU Research Cloud\nfor granting us access to TPUs, and thank Google Cloud for granting us research credits for storage.\nPieter Abbeel holds concurrent appointments as a Professor at UC Berkeley and as an Amazon\nScholar. This paper describes work performed at UC Berkeley and is not associated with Amazon.\nREFERENCES\nEmanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large\nautoregressive multimodal models. arXiv preprint arXiv:2309.15564, 2023.\nArizeAI. Needle in a haystack - pressure testing llms. https:\/\/github.com\/Arize-ai\/\nLLMTest_NeedleInAHaystack, 2023.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\nMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pages 1728–1738, 2021.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and\nQiao Zhang.\nJAX: composable transformations of Python+NumPy programs, 2018.\nURL\nhttp:\/\/github.com\/google\/jax.\nWilliam Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, and\nJonathan Ragan-Kelley. Striped attention: Faster ring attention for causal transformers. arXiv\npreprint arXiv:2311.09431, 2023.\nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe\nTaylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video\ngeneration models as world simulators. 2024. URL https:\/\/openai.com\/research\/\nvideo-generation-models-as-world-simulators.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Sae-\nhoon Kim. Coyo-700m: Image-text pair dataset. https:\/\/github.com\/kakaobrain\/\ncoyo-dataset, 2022.\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint\narXiv:2311.12793, 2023a.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023b.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n11\nPublished as a conference paper at ICLR 2025\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\n35:16344–16359, 2022.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition,\npages 12873–12883, 2021.\nFacebook. Fully Sharded Data Parallel: faster AI training with fewer GPUs — engineering.fb.com.\nhttps:\/\/engineering.fb.com\/2021\/07\/15\/open-source\/fsdp\/, 2023.\n[Ac-\ncessed 16-May-2023].\nChaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu\nZhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation\nbenchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-\na-scene: Scene-based text-to-image generation with human priors. In European Conference on\nComputer Vision, pages 89–106. Springer, 2022.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027, 2020.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama. URL: https:\/\/github.\ncom\/openlm-research\/open_llama, 2023.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn\nSong. Koala: A dialogue model for academic research. Blog post, April, 1, 2023.\ngkamradt. Needle in a haystack - pressure testing llms. https:\/\/github.com\/gkamradt\/\nLLMTest_NeedleInAHaystack\/tree\/main, 2023. [Online; accessed 7-Feb-2024].\nDavid Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.\nFleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022b.\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan Chang, and Xia Hu.\nGrowlength: Accelerating llms pretraining by progressively growing training length. arXiv\npreprint arXiv:2310.00576, 2023a.\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru\nSong, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual\ntokenization. arXiv preprint arXiv:2309.04669, 2023b.\nVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models.\narXiv preprint arXiv:2205.05198, 2022.\nJinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko,\nYi Luan, Sébastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can long-context language\nmodels subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121, 2024.\n12\nPublished as a conference paper at ICLR 2025\nDacheng Li, Rulin Shao, Anze Xie, Eric P Xing, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao\nZhang. Lightseq: Sequence level parallelism for distributed training of long context transformers.\narXiv preprint arXiv:2310.03294, 2023.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference on\nMachine Learning, pages 12888–12900. PMLR, 2022.\nShenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism: Making 4d parallelism\npossible. arXiv preprint arXiv:2105.13120, 2021.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.\nHao Liu and Pieter Abbeel. Blockwise parallel transformer for large context models. Advances in\nneural information processing systems, 2023.\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-\ninfinite context. International Conference on Learning Representations(ICLR), 2024.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. arXiv preprint arXiv:2310.03744, 2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023b.\nXiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of\nrope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023c.\nRuipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and\nZhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint\narXiv:2306.07207, 2023.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:\nTowards detailed video understanding via large vision and language models. arXiv preprint\narXiv:2306.05424, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nSuraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse\nreproduction. arXiv preprint arXiv:2401.01808, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pages\n8748–8763. PMLR, 2021.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini\n1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n13\nPublished as a conference paper at ICLR 2025\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model.\nStanford Center for Research on Foundation Models. https:\/\/crfm. stanford. edu\/2023\/03\/13\/alpaca.\nhtml, 3(6):7, 2023.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski,\nand Piotr Miło´s. Focused transformer: Contrastive training for context scaling. arXiv preprint\narXiv:2307.03170, 2023.\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nlength video generation from open domain textual description. arXiv preprint arXiv:2210.02399,\n2022.\nYi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan\nChen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding\nand generation. arXiv preprint arXiv:2307.06942, 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. arXiv preprint arXiv:1809.09600, 2018.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\narXiv preprint arXiv:2303.16199, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n14\nPublished as a conference paper at ICLR 2025\nA\nFURTHER DETAILS\nModel Flops Utilization. We trained our models using TPUv4-1024, which is approximately\nequivalent to 450 A100s, with a batch size of 8M using FSDP (Facebook, 2023) and Blockwis-\neRingAttention (Liu et al., 2024) for large contexts. Figure 8 shows the model FLOPS utilization\n(MFU) for each training stage. Blue color bars show language training and orange color bars show\nvision-language training. Our training achieves good MFUs even for very large context sizes.\nFigure 8\nHigh MFU training across sequence lengths. Model flops utilization (MFU) of each\ntraining stage for LWM-Text (top), and LWM \/ LWM-Chat (bottom)\nTraining Loss Curves. Figure 9 and Figure 10 show the training loss curves for each stage of training\nthe language and vision-language models respectively.\nFigure 9\nTraining progress over multiple days for LWM-Text. Train loss curve for each training\nstage for LWM-Text models.\nTraining Hyperparameters. See Appendix ??\nScaling Inference. We additionally scale our inference code to support million-length sequences by\nimplementing RingAttention for decoding. Inference for such long sequences requires a minimum\nof v4-128 with a TPU mesh sharding of 32 tensor parallelism, and 4 sequence parallelism (ring\ndimension). We perform inference in pure single precision, where additional improvements can be\nmade through techniques in scalability such as quantization.\n15\nPublished as a conference paper at ICLR 2025\nFigure 10\nTraining progress over multiple days for LWM. Train loss curve for\neach training stage for LWM and LWM-Chat models. Note that losses consist of a combination of\nlosses of different modalities, and may not be directly comparable across stages. The sharp peak in\nthe middle of 1K training is due to newly incporating EOF and EOV tokens into the vision codebook.\nTable 6\nLWM-Text Training Stages\n32K\n128K\n256K\n512K\n1M\nParameters\n7B\n7B\n7B\n7B\n7B\nSequence Length\n215\n217\n218\n219\n220\nRoPE θ\n1M\n10M\n10M\n25M\n50M\nTokens per Batch\n4M\n4M\n4M\n4M\n4M\nTotal Tokens\n4.8B\n12B\n12B\n3B\n1.8B\nWall Clock\n8h\n45h\n83h\n47h\n58h\nCompute (TPU)\nv4-512\nv4-512\nv4-512\nv4-512\nv4-512\nDoc Length\n10K-100K\n100K-200K\n200K-500K\n500K-1M\n1M+\nTable 7\nLWM-Text-Chat Training Details\n128K\n256K\n512K\n1M\nParameters\n7B\n7B\n7B\n7B\nSequence Length\n217\n218\n219\n220\nRoPE θ\n10M\n10M\n25M\n50M\nTokens per Batch\n4M\n4M\n4M\n4M\nTotal Tokens\n1.2B\n1.2B\n1.2B\n1.2B\nWall Clock\n6h\n10h\n20h\n40h\nCompute (TPU)\nv4-512\nv4-512\nv4-512\nv4-512\nTable 8\nLWM and LWM-Chat Training Stages\n1K\n8K\nChat-32K\nChat-128K\nChat-1M\nParameters\n7B\n7B\n7B\n7B\n7B\nSequence Length\n210\n213\n215\n217\n220\nRoPE θ\n50M\n50M\n50M\n50M\n50M\nTokens per Batch\n8M\n8M\n8M\n8M\n8M\nTotal Tokens\n363B\n107B\n10B\n3.5B\n0.4B\nWall Clock\n83h\n32h\n10h\n6h\n8h\nCompute (TPU)\nv4-1024\nv4-1024\nv4-1024\nv4-1024\nv4-1024\n16\nPublished as a conference paper at ICLR 2025\nB\nABLATION STUDIES\nB.1\nMASKED SEQUENCE PACKING\nAs mentioned in Section 4.2, correctly masking the attentions and re-weighting losses is crucial for\nsome aspects of downstream tasks, particularly image understanding. Table 9 shows a comparison\nof our model with and without packing corrections. Naively packing shows large degradation in\naccuracy across image understanding tasks. We hypothesize naive packing degrades performance\ndue to down-weighting text token answers which are shorter, which is an important aspect for good\nimage understanding benchmark performance.\nTable 9\nAblation study comparing standard independent packing and our masked sequence packing\nmechanisms across three tasks. Results show that masked sequence packing significantly improves\nperformance across all tasks.\nVQAv2\nSQA\nPOPE\nStandard independent packing\n48.3\n34.8\n62.5\nMasked sequence packing (Ours)\n55.8\n47.7\n75.2\nB.2\nMIXING SYNTHETIC AND CHAT DATA\nWe additionally evaluate the our model on MT-Bench (Zheng et al., 2023) to test its conversation\nability. Table 10 shows the MT-Bench scores of for each of our models. Table 11 illustrates the\nrelationship between the mix of chat and fact retrieval tasks and the performance on MT-Bench score\nand Needle Retrieval accuracy. As the proportion of chat increases and fact retrieval decreases, the\nMT-Bench score improves, indicating better chat performance measured by MT-Bench. Conversely,\nNeedle Retrieval accuracy decreases, suggesting a trade-off where increasing chat interaction capa-\nbilities may reduce the system’s precision in retrieving specific information or ’needles’ from input\ncontext. Across different context sizes, we found that the model supporting longer input sequences\nencounters a slight decrease in MT-Bench score. We hypothesize that this is because we chose to\ntrain with fewer examples on longer sequence training and can be improved by simply training on\nmore data. In addition, this trade-off may be resolved by acquiring higher quality long-context chat\ndata that is closer to the chat distribution of the UltraChat dataset.\nTable 10\nResults on MT-Bench across different\ncontext sizes. Despite less training on longer se-\nquence lengths, they show only a slight decrease\nin conversational ability.\nModel\nMT-Bench\nLWM-Text-Chat-128k\n4.62\nLWM-Text-Chat-256k\n5\nLWM-Text-Chat-512k\n4.83\nLWM-Text-Chat-1M\n4.19\nTable 11\nRelationship between the mix of chat\nand fact retrieval tasks and the performance on\nMT-Bench score and Needle Retrieval accuracy.\nChat \/ QA Mix\nMT-Bench\nNeedle Acc\n0% \/ 100%\n2.42\n100%\n40% \/ 60%\n4.14\n100%\n70% \/ 30%\n4.62\n96%\n90% \/ 10%\n5.1\n55%\n100% \/ 0%\n5.8\n31%\n17\nPublished as a conference paper at ICLR 2025\nC\nMORE SINGLE-NEEDLE RETRIEVAL RESULTS\nFigure 11\nNeedle retrieval task using the LWM-Text-Chat-1M model. The model demonstrates\nnear-perfect retrieval accuracy across various positions within the 1M context window, as reflected by\nconsistently high scores at different depth percentages and context lengths.\nFigure 12\nSingle needle retrieval accuracy for the LWM-Text-Chat-256K model. The model\nachieves near-perfect retrieval performance across various positions in the 256K context window, as\nshown by consistently high scores across all depth percentages and context lengths.\nFigure 13\nSingle needle retrieval accuracy for the LWM-Text-Chat-512K model. The model\ndemonstrates near-perfect retrieval performance across different positions within the 512K context\nwindow, as indicated by consistently high scores at varying depth percentages and context lengths.\n18\nPublished as a conference paper at ICLR 2025\nD\nDETAILS OF THE MULTI-NEEDLE EVALUATIONS\nWe built on top of the original Needle in a Haystack problem (see Section 3.3.2 for details), which\nwas to retrieve a single magic number randomly inserted into some long context. In this case, magic\nnumbers are associated with random cities (“The magic number for San Francisco is 2521233” →\n“What is the magic number for San Francisco?”). We extend the task by introducing two variables N\nand R, where N is the number of needles (magic numbers + different cities) randomly inserted into\nthe context, and R is the random subset of magic numbers asked to retrieve (“What are the magic\nnumbers for San Francisco, ..., and Bangkok?”). Correctness is computed by extracting out the\nnumbers retrieved for each cities and checked with string matching.\nE\nMORE IMAGE UNDERSTANDING EXAMPLES\nUser: If you were a photographer looking to capture this \nlocation’s essence, what time of day and weather \nconditions would you choose? Describe the reasons \nbehind your choice.\nAssistant: I would choose the time of day to be during \nsunset. The image features a wooden dock extending into \na calm lake, which is a perfect setting for a serene and \npicturesque scene. The sunset casts a warm, golden light \non the lake, enhancing the tranquility of the scene. \nAdditionally, the weather conditions appear to be cool and \ncloudy, which complements the natural beauty of the lake \nand the wooden dock. The combination of these elements \ncreates a visually appealing and calming atmosphere, \nwhich would make it an ideal time to capture the moment.\nAssistant: In this creative twist on a \nclassic work of art, the woman’s face is \ntransformed into a dog’s face, creating a \nvisually striking and unexpected \nrepresentation. The artwork could be \nseen as a commentary on the limitations \nof human understanding, as well as the \npotential for deeper connection and \nunderstanding between humans and \nanimals.\nUser: Describe how this creative twist on \na classic work of art might be interpreted \ndifferently by various audiences. \nFigure 14\nQuestion answering based on image input using LWM. The assistant provides detailed\nresponses to questions about capturing the essence of a serene lakeside scene and interpreting a\ncreative twist on a classic artwork featuring a dog.\n19\nPublished as a conference paper at ICLR 2025\nF\nMORE VIDEO UNDERSTANDING EXAMPLES\n…\n00:00:00\n00:59:59\nUser: How many lemons were in the person’s car?\nGPT-4V: Sorry, I can’t help with identifying or making \nassumptions about the content in these images.\nGemini Pro Vision: I am not able to count the number of lemons \nin the person’s car because I cannot see any lemons in the \nvideo.\nVideo-LLaVA: The video does not provide an exact number of \nlemons in the persons’ car.\nLWM (Ours): There are three lemons in the person’s car.\nFigure 15\nLWM demonstrates video understanding over a 1-hour video. The figure shows a\ncomparison of different AI systems responding to a question about the number of lemons in a\nperson’s car. While GPT-4V, Gemini Pro Vision, and Video-LLAVA fail to provide a correct response,\nLWM accurately identifies that there are three lemons in the car.\n…\n00:00:00\n01:02:50\nUser: What animal was standing on a piano?\nGPT-4V: I’m sorry, I cannot provide assistance with these \nrequests.\nGemini Pro Vision: There is no piano in the video.\nVideo-LLaVA: A dog was standing on a piano in the video.\nLWM (Ours): The animal standing on a piano was a cat.\nFigure 16\nLWM demonstrates video understanding over a 1-hour video. The figure compares AI\nsystems’ responses to the question ’What animal was standing on a piano?’ While GPT-4V, Gemini\nPro Vision, and Video-LLAVA provide incorrect or incomplete answers, LWM correctly identifies\nthat the animal standing on the piano was a cat.\n20\nPublished as a conference paper at ICLR 2025\n00:11\n00:54\n01:37\n02:20\n03:03\n03:46\nUser: What is the video about?\nAssistant: The video is about a man who talks to the camera \nand shows a tree with apples on it. He then proceeds to pick \napples and puts them into a bow.\n(a) The assistant answers questions about a 4-minute video of a man talks to the camera and\nshows a tree of apples.\n00:23\n01:52\n03:22\n04:51\n06:20\n07:50\nUser: Who is the person in the video \nlikely to be?\nAssistant: The person in the video is likely to be a politician \nor representative from the White House.\nUser: Where is the video happening?\nAssistant: The video is taking place in the Oval Oﬃce of the \nWhite House, where President Joe Biden is speaking to the \npress\n(b) The assistant answers a question regarding a 8-minute video of president speaking to the\npress in the Oval Office.\n00:00\n00:06\n00:15\n00:20\n00:27\n00:34\nUser: Provide a comprehensive \ndescription of the video\nAssistant: In the video, a giraffe is seen running towards a \nhigh dive in a pool. As the giraffe reaches the dive, it jumps \ninto the water, causing a solace. The giraffe then swims \ntowards the surface of the pool, creating a small solace as \nit hits the water. The giraffe’s movements are graceful and \nelegant, showcasing its agility and grace.\nUser: What is unusual about this \nvideo?\nAssistant: The video is unusual because it \nshows a giraffe diving into a pool.\n(c) The assistant answers a question about a 30-second video of a giraffe.\nFigure 17\nAnswering questions about videos using LWM. The assistant responds to various user\nquestions regarding different types of videos, ranging from a video about a man picking apples to a\npress briefing in the White House, and even a humorous video of a giraffe diving into a pool.\n21\nPublished as a conference paper at ICLR 2025\nUser: What is the video about?\nAssistant: The video is about a man who is changing a tire on \na car. He demonstrates how to change a tire by using a jack \nand a spare tire\n00:32\n01:23\n02:14\n03:06\n03:57\n04:49\n(a) The assistant answers a question about a 5-minute video of a man changing a car tire. The\nprocess involves using a jack and a spare tire.\n00:06\n00:28\n00:51\n01:14\n01:37\n02:00\nUser: What is the video about?\nAssistant: In the video, a person is riding a dirt bike on a dirt \ntrail. The trail is surrounded by trees and there are other \npeople riding bikes in the background. The person is wearing \na helmet and is riding at a high speed\nUser: What’s the color of the \nperson’s gloves?\nAssistant: The person is wearing red gloves\n(b) The assistant provides answers based on a 2-minute video of a person riding a dirt bike\nalong a forest trail. The rider wears a helmet and red gloves, traveling at high speed.\nFigure 18\nThe system (LWM) successfully answers questions about video content.\nG\nDETAILS OF QUALITATIVE VIDEO UNDERSTANDING EVALUATION\nFor qualitative evaluation of our videos, we source various videos from YouTube that cover a range\nof topics, such as ego-centric camera, how to videos, interviews, and animations. We evaluate all\nvideos at 1FPS, and sample uniformly a max number of frames for videos that are longer than what\nour video can support at 1 FPS. Videos are additionally resized and center cropped to 256 × 256\nresolution before inputting into the model.\n22\nPublished as a conference paper at ICLR 2025\nH\nMORE IMAGE GENERATION EXAMPLES\nA black dog\nA blue colored pizza\nA cube made of denim\nA glass of wine\nA yellow and black bus \ncruising through a rainforest\nOil painting of a couple in \nformal attire caught in the \nrain without umbrellas\nA couch in a cozy living \nroom\nA carrot to the left of \nbroccoli\nFisheye lens of a turtle \nin a forest\nA blue colored dog\nStained glass windows \ndepicting hamburgers and \nfrench fries\nA pink car\nA cube made of brick\nAn elephant under the \nsea\nA yellow book and red \nvase\nA city skyline at night\nFigure 19\nImages generation using LWM, showcasing various scenes and objects.\n23\nPublished as a conference paper at ICLR 2025\nI\nMORE VIDEO GENERATION EXAMPLES\nA bustling street in London with red telephones booths and Big Ben in the background\nFireworks exploding in the sky\nCamera pans left to right on mango slices sitting on a table\nSlow motion flower petals falling on the ground\nA boat sailing on a stormy ocean\nA burning campfire in a forest\nWaves crashing against the shore\nA ball thrown in the air\nFigure 20\nVideo sequences generated using LWM, showing various scenes.\n24\nPublished as a conference paper at ICLR 2025\nJ\nTRAINING HYPERPARAMETERS\nTable 12\nLWM-Text Training Stages\n32K\n128K\n256K\n512K\n1M\nParameters\n7B\n7B\n7B\n7B\n7B\nInitialize From\nLLaMA-2 7B\nText-32K\nText-128K\nText-256K\nText-512K\nPrecision\nfloat32\nfloat32\nfloat32\nfloat32\nfloat32\nSequence Length\n215\n217\n218\n219\n220\nRoPE θ\n1M\n10M\n10M\n25M\n50M\nTokens per Batch\n4M\n4M\n4M\n4M\n4M\nTotal Tokens\n4.8B\n12B\n12B\n3B\n1.8B\nTotal Steps\n1200\n3000\n3000\n720\n450\nLR Schedule\nConstant\nConstant\nConstant\nConstant\nConstant\nLR Warmup Steps\n100\n200\n200\n50\n25\nLR\n4 × 10−5\n4 × 10−5\n4 × 10−5\n4 × 10−5\n4 × 10−5\nCompute (TPU)\nv4-512\nv4-512\nv4-512\nv4-512\nv4-512\nMesh Sharding\n1,-1,4,1\n1,-1,8,1\n1,-1,16,1\n1,-1,16,2\n1,-1,16,4\nTable 13\nLWM-Text-Chat Training Details\n128K\n256K\n512K\n1M\nParameters\n7B\n7B\n7B\n7B\nInitialize From\nText-128K\nText-256K\nText-512K\nText-1M\nPrecision\nfloat32\nfloat32\nfloat32\nfloat32\nSequence Length\n217\n218\n219\n220\nRoPE θ\n10M\n10M\n25M\n50M\nTokens per Batch\n4M\n4M\n4M\n4M\nTotal Tokens\n1.2B\n1.2B\n1.2B\n1.2B\nTotal Steps\n300\n300\n300\n300\nLR Schedule\nConstant\nConstant\nConstant\nConstant\nLR Warmup Steps\n25\n25\n25\n25\nLR\n4 × 10−5\n4 × 10−5\n4 × 10−5\n4 × 10−5\nCompute (TPU)\nv4-512\nv4-512\nv4-512\nv4-512\nMesh Sharding\n1,-1,4,1\n1,-1,8,1\n1,-1,16,1\n1,-1,16,2\nTable 14\nLWM \/ LWM-Chat Training Stages\n1K\n8K\n32K\n128K\n1M\nParameters\n7B\n7B\n7B\n7B\n7B\nInitialize From\nText-1M\n1K\n8K\n32K\n128K\nPrecision\nfloat32\nfloat32\nfloat32\nfloat32\nfloat32\nSequence Length\n210\n213\n215\n217\n220\nRoPE θ\n50M\n50M\n50M\n50M\n50M\nTokens per Batch\n8M\n8M\n8M\n8M\n8M\nTotal Tokens\n363B\n107B\n10B\n3.5B\n0.4B\nTotal Steps\n45000\n14000\n1200\n450\n50\nLR Schedule\nCosine\nCosine\nCosine\nCosine\nCosine\nLR Warmup Steps\n1000\n500\n100\n50\n5\nMax LR\n6 × 10−4\n6 × 10−4\n8 × 10−5\n8 × 10−5\n8 × 10−5\nMin LR\n6 × 10−5\n6 × 10−5\n8 × 10−5\n8 × 10−5\n8 × 10−5\nCompute (TPU)\nv4-1024\nv4-1024\nv4-1024\nv4-1024\nv4-1024\nMesh Sharding\n1,-1,1,1\n1,-1,1,1\n1.-1.4,1\n1.-1.8,1\n1,-1,16,4\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/World Model on Million-Length Video And Language With Blockwise RingAttention.pdf"}
{"title":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization","authors":"Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu","summary":"In light of recent advances in multimodal Large Language Models (LLMs), there\nis increasing attention to scaling them from image-text data to more\ninformative real-world videos. Compared to static images, video poses unique\nchallenges for effective large-scale pre-training due to the modeling of its\nspatiotemporal dynamics. In this paper, we address such limitations in\nvideo-language pre-training with an efficient video decomposition that\nrepresents each video as keyframes and temporal motions. These are then adapted\nto an LLM using well-designed tokenizers that discretize visual and temporal\ninformation as a few tokens, thus enabling unified generative pre-training of\nvideos, images, and text. At inference, the generated tokens from the LLM are\ncarefully recovered to the original continuous pixel space to create various\nvideo content. Our proposed framework is both capable of comprehending and\ngenerating image and video content, as demonstrated by its competitive\nperformance across 13 multimodal benchmarks in image and video understanding\nand generation. Our code and models are available at\nhttps:\/\/video-lavit.github.io.","url":"http:\/\/arxiv.org\/abs\/2402.03161v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.03161v3","published":1707150649000,"comment":null,"pdf_text":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled\nVisual-Motional Tokenization\nYang Jin 1 Zhicheng Sun 1 Kun Xu 2 Kun Xu 2 Liwei Chen 2 Hao Jiang 1 Quzhe Huang 1 Chengru Song 2\nYuliang Liu 2 Di Zhang 2 Yang Song 2 Kun Gai 2 Yadong Mu 1\nAbstract\nIn light of recent advances in multimodal Large\nLanguage Models (LLMs), there is increasing at-\ntention to scaling them from image-text data to\nmore informative real-world videos. Compared to\nstatic images, video poses unique challenges for\neffective large-scale pre-training due to the model-\ning of its spatiotemporal dynamics. In this paper,\nwe address such limitations in video-language pre-\ntraining with an efficient video decomposition that\nrepresents each video as keyframes and temporal\nmotions. These are then adapted to an LLM us-\ning well-designed tokenizers that discretize visual\nand temporal information as a few tokens, thus\nenabling unified generative pre-training of videos,\nimages, and text. At inference, the generated to-\nkens from the LLM are carefully recovered to the\noriginal continuous pixel space to create various\nvideo content. Our proposed framework is both\ncapable of comprehending and generating image\nand video content, as demonstrated by its com-\npetitive performance across 13 multimodal bench-\nmarks in image and video understanding and gen-\neration. Our code and models are available at\nhttps:\/\/video-lavit.github.io.\n1. Introduction\nRecently, the significant breakthrough of Large Language\nModels (LLMs) (Brown et al., 2020; Touvron et al., 2023a)\nhas brought a surge in building general-purpose multimodal\nAI assistants (OpenAI, 2023b; Gemini Team, 2023) that\ncan follow both textual and visual instructions. Drawing\non the remarkable reasoning abilities of LLMs and knowl-\nedge in massive alignment corpus (e.g., image-text pairs),\nthey showcase the great potential of accurately compre-\n1Peking University, China 2Kuaishou Technology, China. Cor-\nrespondence to: Yadong Mu <myd@pku.edu.cn>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nEfficient motion vectors (saving > 90% tokens)\nFigure 1. The key observation in this work is: most video parts\nhave a high degree of temporal redundancy that may be described\nby motion vectors. By exploiting these motion vectors, the video\ncan be efficiently tokenized for pre-training of multimodal LLMs.\nhending and generating visual content (Sun et al., 2024; Jin\net al., 2024; Dong et al., 2024). Despite their success, these\nmultimodal LLMs (Alayrac et al., 2022; Liu et al., 2023c)\npredominantly concentrate on the image-text data, leaving\nthe adaptation for video modality less explored. In contrast\nto static images, video serves as a dynamic media form that\nis more in line with human visual perception. Learning\neffectively from video is particularly essential for enhancing\nmachine intelligence to comprehend the real world.\nTo this end, several approaches have made attempts at har-\nnessing the generative capabilities of LLMs for handling\nvideo data. Inheriting the successful paradigm from the\nimage domain, they represent video as a sequence of visual\ntokens that aligns with LLMs’ semantic space by utilizing a\npre-trained 2D image model (Li et al., 2023d; Zhang et al.,\n2023) or a 3D video backbone (Kondratyuk et al., 2023).\nNevertheless, the existing designs are still not competent for\neffectively encoding videos. Compared to images, videos\npose unique challenges associated with higher demands\nfor learning complex spatiotemporal clues, such as time-\nvarying actions and scene changes. In this regard, encoding\nindividual video frames separately by the 2D visual encoder\nfalls short of capturing the temporal motion information,\nwhich plays a vital role in identifying distinct behaviors\nand events within the video content. Although the recent\n1\narXiv:2402.03161v3  [cs.CV]  3 Jun 2024\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nconcurrent work VideoPoet (Kondratyuk et al., 2023) crafts\na 3D video tokenizer for video generation with LLM, its ap-\nplicability is constrained to short video clips due to the use\nof long token sequences (e.g., 1280 tokens for a 2.2s clip).\nWhen it comes to understanding or generating long videos,\ninputting excessive numbers of tokens into LLMs is deemed\nunacceptable in terms of computational resources.\nThis work addresses the limitation in video-language pre-\ntraining by exploring an efficient video representation that\ndecomposes video into keyframes and temporal motions.\nOur motivation is built upon the natural characteristics of\nvideo data itself. As illustrated in Figure 1, a video is typi-\ncally divided into several shots, where video frames within\neach shot often exhibit substantial information redundancy.\nIt is superfluous to encode all of these frames as tokens and\nincorporate them into the generative pre-training of LLMs.\nThis fact strongly spurs us to decompose each video into\nalternating keyframes and motion vectors, where the former\nencapsulate the primary visual semantics and the latter de-\npict the dynamic evolution of its corresponding keyframe\nover time. There are several benefits to such decomposed\nrepresentation: (1) Compared to processing consecutive\nvideo frames utilizing 3D encoders, the combination of a\nsingle keyframe and motion vectors requires fewer tokens to\nrepresent video temporal dynamics, which is more efficient\nfor large-scale pre-training. (2) The model can inherit the\nacquired visual knowledge from an off-the-shelf image-only\nLLM and focus solely on modeling temporal information\nwithout learning from scratch.\nBased on the above motivations, we present Video-LaVIT\n(Language-VIsion Transformer), a new multimodal pre-\ntraining approach that effectively empowers LLMs to com-\nprehend and generate video content in a unified framework.\nSpecifically, Video-LaVIT incorporates two core compo-\nnents: a tokenizer and a detokenizer to handle video modal-\nity. The video tokenizer aims to transform the continuous\nvideo data into a sequence of compact discrete tokens akin\nto a foreign language, where the keyframes are processed\nby utilizing an established image tokenizer (Jin et al., 2024).\nFor converting the temporal motions into the compatible\ndiscrete format, a spatiotemporal motion encoder is devised.\nIt can capture the time-varying contextual information con-\ntained in extracted motion vectors, thereby significantly en-\nhancing LLMs’ ability to comprehend the intricate actions\nin video. The video detokenizer is responsible for mapping\nthe discretized video token generated by LLMs back into its\noriginal continuous pixel space. During training, video is\nrepresented as an alternating discrete visual-motion token\nsequence, and thus can be optimized under the same next-\ntoken prediction objective together with different modalities.\nSince video is inherently a time series, this joint autore-\ngressive pre-training contributes to learning the sequential\nrelationships of different video clips. We found that Video-\nLaVIT, is capable of serving as a multimodal generalist to\nachieve promising results in both understanding and genera-\ntion tasks without further fine-tuning. The key contributions\nof this work are summarized as:\n• We introduce Video-LaVIT, a multimodal pre-training\nmethod that pushes the limit of LLMs’ unified under-\nstanding and generation capability towards video.\n• To efficiently model visual and temporal information in\nvideo, Video-LaVIT incorporates a novel video tokenizer\nand detokenizer that operates on the decomposed repre-\nsentations of keyframes and motion vectors.\n• Experiments on 13 multimodal benchmarks demonstrate\nthat Video-LaVIT achieves very competitive performance,\nranging from image and video comprehension to zero-\nshot text-to-image and text-to-video generation.\n2. Related Work\nVision-language pre-training. Following the success of\nusing large-scale image-text pairs for contrastive learning\nof vision-language models (Radford et al., 2021), a similar\nidea has been exploited in generative pre-training, where\nvisual and language data are jointly modeled under an au-\ntoregressive process. In practice, this is typically achieved\nby adapting visual image inputs to pre-trained LLMs (Raf-\nfel et al., 2020; Brown et al., 2020; Touvron et al., 2023a)\nvia an intermediate module like cross-attention (Alayrac\net al., 2022), Q-Former (Li et al., 2023c), or linear projec-\ntion (Liu et al., 2023c). More recent approaches such as\nCM3Leon (Yu et al., 2023a) and LaVIT (Jin et al., 2024)\nadvocate the use of discrete visual tokenizers (van den Oord\net al., 2017; Esser et al., 2021) to form a unified next token\nprediction objective. However, these methods are primarily\nfocused on image-text data and cannot be directly extended\nto videos due to the significantly higher computational cost.\nVideo understanding and generation. By unifying videos\nin the above pre-training framework, remarkable progress\nhas been made in video comprehension with masked (Yang\net al., 2022) and autoregressive language models (Li et al.,\n2023d; Zhang et al., 2023; Maaz et al., 2023). However, for\nvideo generation, the mainstream approaches are still based\non diffusion models (Sohl-Dickstein et al., 2015; Song & Er-\nmon, 2019; Ho et al., 2020), which enhance existing image\npre-trained models with better temporal consistency (Ho\net al., 2022; Singer et al., 2023; Blattmann et al., 2023b;\nEsser et al., 2023; Blattmann et al., 2023a). Language model\nbased counterparts (Yan et al., 2021; Hong et al., 2023; Kon-\ndratyuk et al., 2023), on the other hand, face the critical\nchallenge of efficiently encoding video temporal dynamics\nwith limited context windows and computational resources.\nIn response, our work leverages motion vectors, a classic\n2\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\n         Key Frames \nMotion M\n𝐼0 \nImage Tokenizer\n(from LaVIT)\nVideo-LaVIT\n[IMG]\n[\/IMG] [MOV]\n[\/MOV]\n Next Image\/Motion\/Text Token Prediction\nImage Token\nMotion Token\nText Token\nMotion \nTokenizer\nDecomposition\n𝑓ℰ \nMotion M\nspatiotemporal encoder 𝑓ℰ \nMotion Quantizer Q\nDecoder 𝑓𝐷 \nReconstruction Loss\n••• \ncodebook C\nLatent Embedding 𝑍  \nRaw Video Sequence\nFigure 2. For each video-text pair, Video-LaVIT decomposes the video into keyframes and motion vectors for efficient tokenization. The\ntokenizers are learned by maximally reconstructing original inputs (e.g., the motion tokenizer is shown on the right). Finally, the encoded\ntokens are concatenated with text tokens to form a multimodal sequence, allowing for unified generative pre-training of the LLM (left).\nand effective cue in video modeling (Zhang et al., 2016;\nWang et al., 2023b; Shen et al., 2024), for improving the ef-\nficacy of LLM-based video comprehension and generation.\n3. Method\nThis work aims to present an effective pre-training frame-\nwork that harnesses the exceptional modeling capability of\nLarge Language Models (LLMs) to facilitate the learning of\nvideo modality. In pursuit of this goal, we highlight two core\ndesigns: a video tokenizer (Section 3.1) which allows for the\nrepresentation of all modalities in a unified discrete form,\nand a video detokenizer (Section 3.2) to map the generated\ndiscrete tokens back to the continuous pixel space. Coped\nwith these two main components, Video-LaVIT can be op-\ntimized through a unified autoregressive training paradigm\n(Section 3.3), enabling it to simultaneously comprehend and\ngenerate various multimodal content.\n3.1. Video Tokenization\nTo encode an untrimmed video as inputs to LLMs, the pre-\nvailing approaches (Lin et al., 2023; Li et al., 2023d) mainly\nuniformly downsample the original video into a series of\nframes. Then, a pre-trained ViT encoder (Radford et al.,\n2021; Fang et al., 2023) is employed to separately encode\nthese frames and produce a sequence of frame-level em-\nbeddings as the video representation. This straightforward\nway disregards the modeling of temporal dynamics between\nframes, thus impeding the capacity to understand the actions\nand camera transitions occurring in the video. While the\nutilization of 3D video encoders in very recent (Kondratyuk\net al., 2023) enables the encoding of temporal information,\nit only applies to short video clips and inevitably yields a\nsubstantial proliferation of tokens (e.g., 1280 tokens for one\n2.2s clip), resulting in a heavy computational overhead.\nMotion-aware Video Decomposition. Given the above\nconcerns, our proposed video tokenizer seeks to integrate\ntemporal dynamics into the video representations efficiently.\nWe observe that a video clip captured in the same shot can\nconvey its primary semantics through a single keyframe,\nwhile the subsequent frames only illustrate the temporal\nevolvement based on that keyframe. This property empow-\ners the decomposed video tokenization for keyframe and\ntemporal motion. For the keyframe, we employ an off-the-\nshelf image tokenizer from LaVIT (Jin et al., 2024) to inherit\nthe learned visual codebook and prior knowledge without\ntraining from scratch. For encoding temporal motion infor-\nmation, a common alternative is to calculate hand-crafted\ndense optical flow between adjacent frames (Beauchemin &\nBarron, 1995). Despite providing a fine-grained depiction\nof object motions in videos, the expensive computations ren-\nder it unsuitable for scaling to large-scale video data during\npre-training. Hence, we resort to motion vectors, which can\nbe directly extracted at high speed on the CPU (Wu et al.,\n2018) during the compressed video decoding process.\nAs illustrated in Figure 2, we employ the MPEG-4 (Le Gall,\n1991) compression technique to extract keyframe and mo-\ntion information. For simplicity, the I-frames in MPEG-\n4 are considered as the keyframes requiring tokenization.\nMore sophisticated (but expensive) keyframe selection\nschemes can also be considered, but are not the main focus\nof this work. Formally, each video frame is partitioned into\n3\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nDenoising 3D U-Net 𝑔!\nTemporal Block\nSpatial Block\n…\n…\nTemporal Block\nSpatial Block\n…\n…\nRepeat × T\nKeyframe !𝐼\nMotion Vector 𝑀\nNoisy Video 𝑋\" + 𝑛\nDecomposition\nAdding noise\nMinimize\nEDM Objective\nConcatenate\nMotion \nCondition \nEncoder\nMotion Feature Condition\nMotion Input Condition\n...\n…\nVisual Tokens\nMotion Tokens\nReconstructed \nVisual Features\nReconstructed \nMotion Vectors\n...\n…\nClip 𝑖−1\nGenerated Tokens from LLM\n(a) Detokenizer Training\n(b) Long Video Decoding\nKer Frame \nU-Net 𝑔!\nVideo \nU-Net 𝑔\"\nDDIM Inversion\nA Video Clip 𝑋'\nPrediction\n𝐼#$%\nClip 𝑖\nDDIM Inversion\n𝐼(\n...\nClip 𝑖+ 1\n…\nLast Frame\nKeyframe 𝐼#\nFigure 3. Illustrations for video detokenization in Video-LaVIT. (a) Training pipeline for the video detokenizer, which aims to reconstruct\nthe original video clip using one keyframe and the subsequent motion vectors. (b) Autoregressive inference for long video decoding.\n16×16 non-overlapping macroblocks. Motion vectors ⃗m of\nthe t-th frame are estimated by finding the best macroblock\ncorrespondence between adjacent frames It and It−1 :\n⃗m(p, q) = arg min\ni,j ∥It(p, q) −It−1(p −i, q −j)∥,\n(1)\nwhere I(p, q) indicates the pixel values of the macroblock\nat location (p, q), and (i, j) is the coordinate offset between\nthe center of two macroblocks. Then, a video clip can be\ndecomposed into a keyframe I0 ∈RH×W ×3 and the motion\nvectors M ∈RT × H\n16 × W\n16 ×2 of its subsequent T frames.\nMotion Vector Tokenization. To transform the motion\nvectors into a sequence of discrete tokens like a foreign\nlanguage, we develop a motion-specific tokenizer based\non the VQ-VAE architecture (van den Oord et al., 2017).\nIt includes a spatiotemporal encoder fE, a learnable code-\nbook C = {ck}K\nk=1, and a decoder fD. The encoder fE\nhas L stacked transformer blocks consisting of spatial and\ntemporal attention layers to fuse the contextual motion in-\nformation among the T frames. It maps the motion vectors\nM ∈RT × H\n16 × W\n16 ×2 into a 1D latent embedding sequence\nˆZ ∈RN×d. Each embedding vector ˆz ∈Rd is then to-\nkenized by a vector quantizer Q, which assigns it to the\nclosest code in C:\nzi = arg min\nj\n∥l2(ˆzi) −l2(cj)∥2,\n(2)\nwhere l2 indicates the L2 normalization. The decoder fD\nhas a similar structure to the encoder and is obliged to map\nthe discrete motion codes {zi}N\ni=1 back to the original mo-\ntion vectors. The whole motion tokenizer can be updated\nby optimizing the reconstruction quality. To prevent code-\nbook collapse during training, we follow Yu et al. (2022) to\nproject the motion embeddings ˆZ into a low-dimensional\nspace before quantization and use exponential moving av-\nerage (EMA) updates. More details about the motion tok-\nenizer can be found in Appendix A.1. Finally, a video is\ntokenized into alternating ⟨visual, motion, ...⟩codes that\nserve as the supervision signals in LLMs during generative\npre-training. Such a factorized tokenization significantly\nreduces the inter-frame redundancy in one video shot while\nefficiently capturing the temporal motion information.\n3.2. Video Detokenization\nThe video detokenizer of Video-LaVIT is in charge of con-\nverting them back into the original continuous pixel space\nfor video generation. Considering the challenge in learning a\ndirect mapping from discrete tokens to the high-dimensional\nvideo space, we take a sequential decoding strategy, wherein\nthe keyframe is initially recovered based on the visual token.\nThe subsequent frames are then decoded by taking both the\nkeyframe and motion tokens as the conditions. The efficacy\nof this strategy in enhancing video generation quality has\nalso been validated by recent work (Girdhar et al., 2023).\nSpecifically, the keyframe and video detokenizers both use\nconditional denoising U-Net (Rombach et al., 2022). Simi-\nlar to LaVIT (Jin et al., 2024), the keyframe U-Net gI takes\nthe reconstructed visual features that contain image seman-\ntics as conditions to infill visual details from a Gaussian\nnoise. Here, we primarily focus on the newly proposed\nvideo detokenizer gV . As illustrated in Figure 3(a), it is a\n3D variant of the original 2D U-Net architecture by inserting\ntemporal convolution and attention layers after the spatial\nmodules, following Blattmann et al. (2023b; 2023a).\nEnhanced Motion Conditioning. The objective of the\nvideo detokenizer gV is to rigorously adhere to the guidance\nof the motion vectors, thereby facilitating the recovery of T\n4\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nframes following the keyframe. To this end, we highlight\ntwo different forms of motion conditions in gV . Given the\nmotion vectors M ∈RT × H\n16 × W\n16 ×2 of a sampled video clip,\nwe adopt the nearest neighbor interpolation to ensure that\nit matches the spatial shape of the U-Net input. Also, the\nlatent state ˆI of the keyframe from the VAE is repeated T\ntimes along the temporal axis to form visual conditioning.\nThe motion vector M, the keyframe latent ˆI, and the noisy\nvideo frames are concatenated channel-wise as the input\ncondition to gV . Except for direct input conditioning, we\nalso enhance conditioning with motion feature embedding\nvia the spatial and temporal cross-attention layers in the\n3D U-Net blocks. Here, the motion features are from a\nconditioning encoder that has a similar architecture to fE\nexcluding the downsample layer to reduce potential infor-\nmation loss. The parameters of the video detokenizer gV\nare updated by minimizing the following EDM training\nobjective (Karras et al., 2022) on a video training dataset D:\nE(X0,ˆI, ˆ\nM)∼D,σ,n\nh\nλσ||gV (X0 + n, σ, ˆI, M) −X0||\ni\n,\n(3)\nwhere σ ∼p(σ) is the noise level during training, n ∼\nN(n; 0, σ2) is a random noise added to video sample\nX0, and λσ is loss weighting function. At inference, the\n⟨visual, motion⟩tokens yielded by LLM are first mapped\ninto visual features and motion vectors by their correspond-\ning tokenizers. The reconstructed visual features are fed\ninto gI to generate a keyframe, which is subsequently com-\nbined with reconstructed motion vectors to serve as condi-\ntions for gV to decode the video clip (See Figure 3(b)).\nLong Video Decoding. Since a video is expressed as multi-\nple alternating ⟨visual, motion⟩sequences, the interdepen-\ndencies among different video fragments can be effectively\nlearned by autoregressive pre-training of LLMs. Hence,\nVideo-LaVIT naturally supports the generation of longer\nvideos by progressive decoding multiple clips. However,\nseparate decoding will bring inconsistencies in some fine-\ngrained visual details among different clips (See Figure 5).\nTo mitigate this, we incorporate an explicit noise constraint\nwhen decoding the keyframe Ir of a video clip. As illus-\ntrated in Figure 3(b), we reverse its last frame Ir−1 from the\npreviously generated clip into an intermediate noisy state\nx∆T by reversing the DDIM sampling (Song et al., 2020)\nprocess ∆T times. Each inversion step is formulated by:\nxt+1 =\nrαt+1\nαt xt +\n r\n1\nαt+1 −1 −\nr\n1\nαt −1\n!\ngI(xt, t, ˆI),\n(4)\nwhere αt is the noise level, ˆI is the visual feature condition.\nThe reversed noisy state x∆T is then considered as the initial\nnoise in the denoising loop for keyframe Ir. As illustrated\nin Figure 5, adding this noise constraint can improve the\ntemporal consistency between video clips.\n3.3. Unified Generative Modeling\nBased on the developed decomposed video tokenization\nstrategy, it is feasible to indiscriminately treat all the modal-\nities (video, image, and text) as 1D discrete tokens fed\ninto LLMs. Following LaVIT (Jin et al., 2024), special\ntokens (e.g., [MOV] and [\/MOV] for motion modality) are\ninserted at the beginning and end of the visual and mo-\ntion token sequence for differentiating modalities in the\ninput data. During pre-training, we also exchange the order\nof multimodal data pairs to form both [video(image), text]\nand [text, video(image)] as input sequences.\nFormally,\ngiven a multimodal sequence y = (y1, y2, .., yS), Video-\nLaVIT inherits the successful generative language modeling\nparadigm from LLM to directly maximize the likelihood of\neach token yi in an autoregressive manner:\np(y) =\nX\ny∈D\nS\nX\ni=1\nlog Pθ(yi|y<i).\n(5)\nAfter pre-training, Video-LaVIT is capable of serving as a\nmultimodal generalist to achieve both multimodal compre-\nhension and generation of data in any modality.\nModel Training. Video-LaVIT undergoes a three-stage\ntraining procedure on the large-scale multimodal corpora.\nThe purpose of each stage can be summarized as follows:\ni) Tokenizer and Detokenizer Training. This stage requires\nonly pure video data without corresponding textual cap-\ntions. It aims to produce compact video tokens that serve as\nsupervision signals to guide the subsequent generative pre-\ntraining, as well as to facilitate an accurate reconstruction\nof the original videos. ii) Generative Pre-training. Stage-2\nempowers the model to learn the inter-correlation among the\ndata of different modalities via unified generative modeling\nwithin the LLM. iii) Instruction Tuning. To fully unleash\nthe acquired knowledge, the last stage further improves the\ninstruction-following ability to accomplish various multi-\nmodal tasks. More details about the model architectures and\ntraining data for each stage are provided in Appendix A.1.\n4. Experiments\n4.1. Multimodal Understanding\nWith the decomposed video representation, Video-LaVIT is\nnaturally capable of understanding both videos and images.\nHere, we demonstrate its multimodal understanding capa-\nbility on 11 commonly used image and video benchmarks.\nImage Understanding. Table 1 presents an extensive com-\nparison across eight widely used image question answer-\ning and multimodal benchmarks: VQA v2 (Goyal et al.,\n2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari\net al., 2018), ScienceQA-IMG (Lu et al., 2022), MME (Fu\net al., 2023), MMBench (Liu et al., 2023e), SEED (Li et al.,\n5\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 1. Image understanding performance (↑) on 8 benchmarks. Video-LaVIT achieves state-of-the-art results on most of the benchmarks.\nFor convenience, SQAI denotes ScienceQA-IMG (Lu et al., 2022), and MMB denotes MMBench (Liu et al., 2023e). * indicates that there\nis some overlap with the training data. Note that only LLaVA-1.5 (Liu et al., 2023a) is reported with a higher image resolution of 336.\nThe Video-LLaVA, LLaMA-VID and LLaVA-1.5 use Vicuna-1.5 (Chiang et al., 2023) as the language model.\nMethod\nLLM size\nImage Question Answering\nMultimodal\nVQAv2\nGQA\nVizWiz\nSQAI\nMME\nMMB\nSEED\nMM-Vet\nFlamingo (Alayrac et al., 2022)\n9B\n51.8\n-\n28.8\n-\n-\n-\n-\n-\nBLIP-2 (Li et al., 2023b)\n13B\n41.0\n41.0\n19.6\n61.0\n1293.8\n-\n46.4\n22.4\nInstructBLIP (Dai et al., 2023)\n13B\n-\n49.5\n34.3\n63.1\n1212.8\n44.0\n-\n25.6\nCM3Leon (Yu et al., 2023a)\n7B\n47.6\n-\n37.6\n-\n-\n-\n-\n-\nEmu (Sun et al., 2024)\n13B\n52.0\n-\n34.2\n-\n-\n-\n-\n36.3\nDreamLLM (Dong et al., 2024)\n7B\n72.9*\n-\n49.3\n-\n-\n58.2\n-\n36.6\nVideo-LLaVA (Lin et al., 2023)\n7B\n74.7*\n60.3*\n48.1\n66.4\n-\n60.9\n-\n32.0\nLLaMA-VID (Li et al., 2023f)\n7B\n78.3*\n63.0*\n52.5\n67.7\n1405.6\n65.3\n59.7\n-\nLLaVA-1.5 (Liu et al., 2023a)\n7B\n78.5*\n62.0*\n50.0\n66.8\n1510.7\n64.3\n58.6\n30.5\nVideo-LaVIT\n7B\n80.3*\n64.4*\n56.0\n70.0\n1551.8\n67.3\n64.0\n33.2\nTable 2. Zero-shot video question answering accuracy (↑). Video-LaVIT demonstrates state-of-the-art accuracy on all three benchmarks.\nThe evaluation uses a GPT assistant (Maaz et al., 2023), with “Score” denoting a relative score from 0 to 5 assigned by the GPT model.\nThe Video-LLaVA and LLaMA-VID both use Vicuna-1.5 (Chiang et al., 2023) as the language model.\nMethod\nLLM size\nMSVD-QA\nMSRVTT-QA\nActivityNet-QA\nAccuracy\nScore\nAccuracy\nScore\nAccuracy\nScore\nFrozenBiLM (Yang et al., 2022)\n1B\n32.2\n-\n16.8\n-\n24.7\n-\nVideo-LLaMA (Zhang et al., 2023)\n7B\n51.6\n2.5\n29.6\n1.8\n12.4\n1.1\nVideoChat (Li et al., 2023d)\n7B\n56.3\n2.8\n45.0\n2.5\n26.5\n2.2\nVideo-ChatGPT (Maaz et al., 2023)\n7B\n64.9\n3.3\n49.3\n2.8\n35.2\n2.7\nLLaMA-VID (Li et al., 2023f)\n7B\n69.7\n3.7\n57.7\n3.2\n47.4\n3.3\nVideo-LLaVA (Lin et al., 2023)\n7B\n70.7\n3.9\n59.2\n3.5\n45.3\n3.3\nVideo-LaVIT\n7B\n73.2\n3.9\n59.3\n3.3\n50.1\n3.3\n2023a), MM-Vet (Yu et al., 2023b). Our model successfully\ngeneralizes the pre-training knowledge to image compre-\nhension tasks and provides the best overall performance.\nSpecifically, with the same instruction dataset and the base\nmodel as LLaVA-1.5 (Liu et al., 2023a), our method consis-\ntently yields the best results on all image question answering\ndatasets. For example on SQAI, it surpasses LLaVA-1.5\nwhich has a higher input resolution by 3.2%, while con-\nsistently outperforming the other video-language models.\nThe same advantages are further validated on more compre-\nhensive multimodal benchmarks, where our model leads on\nthree out of four benchmarks.\nZero-Shot Video Question Answering. Table 2 compares\nour proposed Video-LaVIT with multiple recent video-\nlanguage models on three common video benchmarks:\nMSVD-QA (Chen & Dolan, 2011), MSRVTT-QA (Xu et al.,\n2016) and ActivityNet-QA (Yu et al., 2019), in terms of\naccuracy and relative score measured by a GPT-3.5 assis-\ntant (Maaz et al., 2023). We achieve state-of-the-art ac-\ncuracies and very competitive relative scores on the three\nbenchmarks, such as surpassing the previous leading model\nVideo-LLaVA (Lin et al., 2023) by 2.5% on MSVD-QA.\nUsing the same 100k video-text instruction dataset from\nVideo-ChatGPT (Maaz et al., 2023) which is also adopted\nby Video-LLaVA, our method outperforms these alternatives\nby explicitly modeling temporal dynamics with motion to-\nkens. Especially for the ActivityNet-QA benchmark, which\ncontains various human behaviors, incorporating motion in-\nformation contributes to the recognition of different actions.\nFor the only metric where our performance is not the best,\nnamely the relative score on MSRVTT-QA, we deliver a\nhigh score only second to Video-LLaVA (by a margin of\n0.2), again confirming the effectiveness of our method.\nZero-Shot Video Understanding. Besides the widely-\nused video question answering datasets, we also evaluated\nVideo-LaVIT on Perception Test (Patraucean et al., 2024)\nor EgoSchema (Mangalam et al., 2024). These two bench-\nmarks aim to evaluate the understanding and reasoning ca-\npability of long-term videos, rather than exploiting the hal-\nlucination capabilities of LLMs. The detailed evaluation\nresults are shown in Table 3. On the Perception Test, Video-\nLaVIT delivers the highest zero-shot performance. Notably,\nit outperforms VideoChat2 (Li et al., 2023e), which uses\n1.9M additional instruction tuning data (including videos)\n6\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 3. Zero-shot understanding (↑) on the test set of Perception Test (Patraucean et al., 2024) and EgoSchema (Mangalam et al., 2024).\nMethod\nFlamingo (Alayrac et al., 2022)\nBLIP-2 (Li et al., 2023c)\nVideoChat2 (Li et al., 2023e)\nVideo-LaVIT\nAccuracy\n33.5\n39.2\n47.3\n47.9\nMethod\nFrozenBiLM (Yang et al., 2022)\nmPLUG-Owl (Ye et al., 2023)\nInternVideo (Wang et al., 2022)\nVideo-LaVIT\nAccuracy\n26.9\n28.7\n32.1\n37.3\nTable 4. Zero-shot text-to-video generation performance. Video-LaVIT delivers competitive results against state-of-the-art models trained\non more proprietary data, with data size reported in terms of the number of training video clips. The next best results are underlined.\nMethod\nData size\nPublic data\nMSR-VTT\nUCF-101\nCLIPSIM (↑)\nFVD (↓)\nFID (↓)\nIS (↑)\nFVD (↓)\nCogVideo (Hong et al., 2023)\n5.4M\n✓\n0.2631\n1294\n23.59\n25.27\n701.59\nVideo LDM (Blattmann et al., 2023b)\n10M\n✓\n0.2929\n-\n-\n33.45\n550.61\nVideoComposer (Wang et al., 2023b)\n10M\n✓\n0.2932\n580\n-\n-\n-\nInternVid (Wang et al., 2024)\n28M\n✓\n0.2951\n-\n-\n21.04\n616.51\nMake-A-Video (Singer et al., 2023)\n20M\n✓\n0.3049\n-\n13.17\n33.00\n367.23\nVideoPoet (Kondratyuk et al., 2023)\n270M\n×\n0.3049\n213\n-\n38.44\n355.00\nPYoCo (Ge et al., 2023)\n22.5M\n×\n-\n-\n9.73\n47.76\n355.19\nSVD (Blattmann et al., 2023a)\n152M\n×\n-\n-\n-\n-\n242.02\nVideo-LaVIT\n10M\n✓\n0.3012\n188.36\n11.27\n44.26\n280.57\nto improve video understanding. In comparison, our ad-\nvantageous performance is achieved with the standard in-\nstructions from LLaVA-1.5 (Liu et al., 2023a) and Video-\nChatGPT (Maaz et al., 2023) (amounting to 765K), demon-\nstrating the effectiveness of our proposed method. As for\nEgoSchema, which focuses on long video understanding,\nVideo-LaVIT is able to analyze 16 keyframes (with the\nmotion vectors in between) spanning 64 seconds, thereby\ndeliver better results. For example, it outperforms Intern-\nVideo (Wang et al., 2022), which uses up to 90 frames, by\na significant 5.2% in zero-shot QA accuracy. This vali-\ndates the efficacy of the visual-motional decomposition for\nmodeling long-term temporal information.\n4.2. Multimodal Generation\nBy unified generative pre-training, Video-LaVIT can flexi-\nbly generate both video and images. Due to page limitations,\nwe present here its text-to-video generation results, while\nthe text-to-image evaluation is discussed in Appendix B.1.\nZero-Shot Text-to-Video Generation. Table 4 summarizes\nthe model performance on MSR-VTT (Xu et al., 2016) and\nUCF-101 (Soomro et al., 2012), in terms of CLIP similar-\nity (CLIPSIM) (Wu et al., 2021), Fr´echet video distance\n(FVD) (Unterthiner et al., 2018), Fr´echet Inception distance\n(FID) (Heusel et al., 2017), and Inception score (IS) (Saito\net al., 2020). Overall, our model significantly outperforms\nmost baselines using similar public datasets, and is highly\ncompetitive against models trained on much larger propri-\netary data, for example leading the FVD on MSR-VTT.\nIn particular, when compared to language model-based\ntext-to-video generators, our method consistently outscores\nCogVideo (Hong et al., 2023), while surpassing the recent\nconcurrent work VideoPoet (Kondratyuk et al., 2023), which\nuses a 3D video tokenizer trained on the much larger data.\nThis clearly validates the superiority of our tokenizer design.\nZero-Shot Long Video Generation. We also conducted\nquantitative evaluation experiments for long video genera-\ntion, following the setting from FreeNoise (Qiu et al., 2023).\nSpecifically, it is evaluated on 2048 long videos (64 frames)\ngenerated using the prompts from EvalCrafter (Liu et al.,\n2023d). As shown in the table Table 5, our approach yields\nhighly competitive performance among the specialists cu-\nrated for long video generation. In particular, it surpasses\nall baselines on the KVD metric, which measures the dis-\ncrepancy between short videos (first 16 frames) and subsets\nof long videos (last 16 frames). These results confirm the\neffectiveness of our proposed long video decoding strategy\nwith explicit noise constraint.\n4.3. Qualitative Results\nThis section compares videos created by Video-LaVIT with\nstate-of-the-art results under both text and image conditions.\nIt also presents our special ability to generate long videos.\nMore visualization examples are provided in Appendix B.1.\nThe text-to-video and image-to-video generation results are\nvisualized in Figure 4. For text-to-video generation, our\nmethod can produce visual quality not much far from the\nclosed-source model Gen-2 (Runaway, 2023), thanks to the\nunified pre-training framework with images. Meanwhile,\nVideo-LaVIT is advantageous in reasoning abilities, such as\n7\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nA majestic eagle soars gracefully over a breathtaking\nA steam train moving on a mountainside by\nmountain range.\nVincent van Gogh.\nGen-2\nVideo-LaVIT\nSVD\nVideo-LaVIT\nFigure 4. Text-to-video (top) and image-to-video (bottom) generation comparison with Gen-2 (Runaway, 2023) and SVD-XT (Blattmann\net al., 2023a). Text prompts are from Emu Video (Girdhar et al., 2023) and SVD. The I2V generation is conditioned on the leftmost frame.\ninferring better motion (in the top-left example) and adding\nartistic touches based on the text prompt (as in both cases).\nFor image-to-video generation, our method is comparable\nto the state-of-the-art model SVD (Blattmann et al., 2023a)\nin generating both coherent and highly aesthetic video clips\n(the bottom-left example). In addition, the decomposed\nvideo representation enables the video decoder to produce\nmore salient and vivid movements given a relatively difficult\nsynthetic image prompt (the bottom-right example).\nFurthermore, our autoregressive model can be naturally\nextended to long video generation, as shown in Figure 5.\nThanks to the proposed explicit noise constraint when de-\ncoding consecutive video clips, the temporal consistency\nbetween decoded clips is greatly improved. In contrast,\ndecoding each video clip separately will result in the in-\ncoherence of fine-grained visual details among the video\nframes of different clips (See the bottom of Figure 5).\n4.4. Ablation Study\nThis section investigates the impact of motion tokenization\nand different motion token lengths. Due to limited space,\nthe ablation for proposed enhanced motion conditioning\nstrategy is provided in Appendix B.1.\nEffect of Motion Tokenization. We design two baselines\nto validate the effectiveness of motion tokenization in video\npre-training. For video understanding, the w\/o motion in Ta-\nble 6 indicates the independent tokenization of 16 uniformly\nsampled video frames by the 2D visual encoder without any\nTable 5. Zero-shot text-to-long video generation performance. It\nis evaluated on 2048 long videos (64 frames) generated using the\nprompts from EvalCrafter (Liu et al., 2023d).\nMethod\nFVD (↓)\nKVD (↓)\nCLIPSIM (↑)\nDirect\n737.61\n359.11\n0.9104\nSliding\n224.55\n44.09\n0.9438\nGen-L-Video (Wang et al.)\n177.63\n21.06\n0.9370\nFreeNoise (Qiu et al.)\n85.83\n6.07\n0.9732\nVideo-LaVIT\n113.37\n4.94\n0.9621\nmotion tokens. Most existing methods use a similar strategy\nto encode video content fed into the LLM. As observed\nin Table 6, the question-answering accuracy decreases with-\nout explicitly modeling temporal information. For the video\nsynthesis, the w\/o motion baseline divides the text-to-video\nprocess into two separate stages: text-to-image and image-\nto-video. Specifically, given a textual prompt, we generate\nonly a keyframe (without producing motion tokens) and\nthen feed this keyframe into the image-to-video generation\nmodel svd-img2vid-xt (Blattmann et al., 2023a) to synthe-\nsize the final video. Since svd-img2vid-xt takes only an\nimage as condition, the video generation process of this\nbaseline lacks temporal motion as guidance. In comparison,\nour model can generate text-related motion tokens and thus\ngenerate more accurate video content following the prompt.\nEffect of Token Length. We also explore the influence\nof different motion token lengths when encoding temporal\nmotion information. The detailed results are reported in Ta-\nble 7. It can be observed that a very small number suffice\n8\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nVideo-LaVIT\n0–48 frames\n48–96 frames\nw\/o constraint\n0–48 frames\nFigure 5. Long video generation example with “a 360 shot of a sleek yacht sailing gracefully through the crystal-clear waters of the\nCaribbean”. The top two rows use the noise constraint in Equation (4) to improve temporal consistency, while the bottom row does not.\nTable 6. Ablation of proposed motion tokenization strategy in zero-\nshot video understanding (left) and generation (right).\nMethod\nMSVD\nActivityNet\nUCF-101\nAccuracy\nAccuracy\nIS (↑)\nFVD (↓)\nw\/o motion\n67.3\n47.4\n29.56\n442.80\nw\/ motion\n73.2\n50.1\n44.26\n280.57\nto yield high understanding and generation performance.\nMore token numbers may lead to representation redundancy\nand bring more duplicate motion token IDs when encoding\nvideos without obvious motions, rendering the next-token\nprediction learning paradigm of LLM less effective. Using\nfewer motion tokens also allows for more video clips as\ninput conditions under the same context length of LLM,\nwhich is useful for long video understanding.\n5. Conclusion\nThis paper introduces Video-LaVIT, a multimodal genera-\ntive pre-training method that empowers LLMs with unified\ncomprehension and generation of videos, images, and lan-\nguage. At the core of our method is a video decomposition\nscheme that allows for more effective modeling of temporal\ninformation while reusing visual knowledge from image-\nonly multimodal LLMs. The decomposed keyframes and\nmotion vectors can be efficiently tokenized to be adapted\nto LLMs for unified generative pre-training. Finally, the\nunderstanding and generative capabilities of Video-LaVIT\nare verified by extensive quantitative and qualitative results.\nImpact Statement\nWhile this work advances the pre-training of large multi-\nmodal models in both performance and efficiency, its rea-\nsoning and generative capabilities should be treated care-\nTable 7. Ablation of the number of motion tokens (denoted by N)\nin zero-shot video understanding (left) and generation (right).\nMethod\nMSVD\nActivityNet\nUCF-101\nAccuracy\nAccuracy\nIS (↑)\nFVD (↓)\nN = 256\n69.2\n48.8\n37.57\n281.24\nN = 135\n73.2\n50.1\n44.26\n280.57\nfully. Some well-known problems include hallucination in\nmultimodal understanding and exploitation to create mis-\ninformation through personalized generation. The model\ncould also produce harmful responses due to inherent data\nbias and lack of alignment procedures.\nOne positive aspect we’d like to highlight is that, unlike\nmany generation methods compared, this model is fully\ntrained with public datasets. This allows the research com-\nmunity to correct for bias and harmful content in the training\ndata. We hope that such a practice will help address impor-\ntant safety and alignment issues in multimodal models.\nAcknowledgements\nThis research work is supported by National Key R&D\nProgram of China (2022ZD0160305), a research grant from\nChina Tower Corporation Limited, and a grant from Beijing\nAerospace Automatic Control Institute. We also sincerely\nthank for the very constructive comments from all reviewers.\nReferences\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. CM3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\n9\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: A visual language model for few-\nshot learning. In NeurIPS, pp. 23716–23736, 2022.\nAwadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y.,\nZhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa,\nS., et al. OpenFlamingo: An open-source framework\nfor training large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390, 2023.\nBaek, Y., Lee, B., Han, D., Yun, S., and Lee, H. Character\nregion awareness for text detection. In CVPR, pp. 9365–\n9374, 2019.\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\nin time: A joint video and image encoder for end-to-end\nretrieval. In ICCV, pp. 1728–1738, 2021.\nBeauchemin, S. S. and Barron, J. L. The computation of\noptical flow. ACM Computing Surveys, 27(3):433–466,\n1995.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D.,\nKilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V.,\nLetts, A., et al. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets. arXiv preprint\narXiv:2311.15127, 2023a.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\nresolution video synthesis with latent diffusion models.\nIn CVPR, pp. 22563–22575, 2023b.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nIn NeurIPS, pp. 1877–1901, 2020.\nCarreira, J. and Zisserman, A. Quo vadis, action recogni-\ntion? a new model and the kinetics dataset. In CVPR, pp.\n6299–6308, 2017.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12M: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In CVPR, pp. 3558–\n3568, 2021.\nChen, D. and Dolan, W. B. Collecting highly parallel data\nfor paraphrase evaluation. In ACL, pp. 190–200, 2011.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H.,\nZheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica,\nI., and Xing, E. P. Vicuna: An open-source chatbot im-\npressing GPT-4 with 90%* ChatGPT quality. https:\/\/\nlmsys.org\/blog\/2023-03-30-vicuna, 2023.\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\nW., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards\ngeneral-purpose vision-language models with instruction\ntuning. In NeurIPS, 2023.\nDong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao,\nL., Sun, J., Zhou, H., Wei, H., et al. DreamLLM: Syner-\ngistic multimodal comprehension and creation. In ICLR,\n2024.\nEsser, P., Rombach, R., and Ommer, B. Taming transformers\nfor high-resolution image synthesis. In CVPR, pp. 12873–\n12883, 2021.\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Ger-\nmanidis, A. Structure and content-guided video synthesis\nwith diffusion models. In ICCV, pp. 7346–7356, 2023.\nFang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\nHuang, T., Wang, X., and Cao, Y. EVA: Exploring the\nlimits of masked visual representation learning at scale.\nIn CVPR, pp. 19358–19369, 2023.\nFeng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He,\nX., Basu, S., Wang, X. E., and Wang, W. Y. LayoutGPT:\nCompositional visual planning and generation with large\nlanguage models. In ICLR, 2024.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu,\nZ., Lin, W., Yang, J., Zheng, X., et al. MME: A compre-\nhensive evaluation benchmark for multimodal large lan-\nguage models. arXiv preprint arXiv:2306.13394, 2023.\nGe, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro,\nB., Jacobs, D., Huang, J.-B., Liu, M.-Y., and Balaji, Y.\nPreserve your own correlation: A noise prior for video\ndiffusion models. In ICCV, pp. 22930–22941, 2023.\nGemini Team, G. Gemini: A family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805,\n2023.\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S.,\nRambhatla, S. S., Shah, A., Yin, X., Parikh, D., and\nMisra, I. Emu video: Factorizing text-to-video gener-\nation by explicit image conditioning.\narXiv preprint\narXiv:2311.10709, 2023.\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the V in VQA matter: Elevating the\nrole of image understanding in visual question answering.\nIn CVPR, pp. 6904–6913, 2017.\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\nK., Luo, J., and Bigham, J. P. VizWiz grand challenge:\nAnswering visual questions from blind people. In CVPR,\npp. 3608–3617, 2018.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding. In ICLR, 2021.\n10\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. GANs trained by a two time-scale update\nrule converge to a local Nash equilibrium. In NeurIPS,\npp. 6629–6640, 2017.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. In NeurIPS, pp. 6840–6851, 2020.\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,\nand Fleet, D. J. Video diffusion models. In NeurIPS, pp.\n8633–8646, 2022.\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.\nCogVideo: Large-scale pretraining for text-to-video gen-\neration via transformers. In ICLR, 2023.\nHudson, D. A. and Manning, C. D. GQA: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In CVPR, pp. 6700–6709, 2019.\nJin, Y., Xu, K., Xu, K., Chen, L., Liao, C., Tan, J., Huang,\nQ., Chen, B., Lei, C., Liu, A., et al. Unified language-\nvision pretraining in LLM with dynamic discrete visual\ntokenization. In ICLR, 2024.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models. In\nNeurIPS, pp. 26565–26577, 2022.\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier,\nC., Vijayanarasimhan, S., Viola, F., Green, T., Back, T.,\nNatsev, P., et al. The Kinetics human action video dataset.\narXiv preprint arXiv:1705.06950, 2017.\nKondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J.,\nHornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar,\nV., et al. VideoPoet: A large language model for zero-\nshot video generation. arXiv preprint arXiv:2312.14125,\n2023.\nKynk¨a¨anniemi, T., Karras, T., Aittala, M., Aila, T., and\nLehtinen, J. The role of ImageNet classes in Fr´echet\nInception distance. In ICLR, 2023.\nLe Gall, D. MPEG: A video compression standard for\nmultimedia applications. Communications of the ACM,\n34(4):46–58, 1991.\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and\nShan, Y.\nSEED-Bench: Benchmarking multimodal\nLLMs with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023a.\nLi, D., Li, J., and Hoi, S. BLIP-Diffusion: Pre-trained\nsubject representation for controllable text-to-image gen-\neration and editing. In NeurIPS, 2023b.\nLi, J., Li, D., Xiong, C., and Hoi, S. BLIP: Bootstrapping\nlanguage-image pre-training for unified vision-language\nunderstanding and generation.\nIn ICML, pp. 12888–\n12900, 2022.\nLi, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Boot-\nstrapping language-image pre-training with frozen image\nencoders and large language models. In ICML, pp. 19730–\n19742, 2023c.\nLi, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang,\nY., Wang, L., and Qiao, Y. VideoChat: Chat-centric video\nunderstanding. arXiv preprint arXiv:2305.06355, 2023d.\nLi, K., Wang, Y., He, Y., Li, Y., Wang, Y., Liu, Y., Wang, Z.,\nXu, J., Chen, G., Luo, P., et al. MVBench: A comprehen-\nsive multi-modal video understanding benchmark. arXiv\npreprint arXiv:2311.17005, 2023e.\nLi, Y., Wang, C., and Jia, J. LLaMA-VID: An image is\nworth 2 tokens in large language models. arXiv preprint\narXiv:2311.17043, 2023f.\nLian, L., Li, B., Yala, A., and Darrell, T. LLM-grounded dif-\nfusion: Enhancing prompt understanding of text-to-image\ndiffusion models with large language models.\narXiv\npreprint arXiv:2305.13655, 2023.\nLin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan,\nL.\nVideo-LLaVA: Learning united visual representa-\ntion by alignment before projection.\narXiv preprint\narXiv:2311.10122, 2023.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft\nCOCO: Common objects in context. In ECCV, pp. 740–\n755, 2014.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. In NeurIPS, 2023b.\nLiu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou,\nX., Yang, J., Su, H., Zhu, J., et al. LLaVA-Plus: Learning\nto use tools for creating multimodal agents. arXiv preprint\narXiv:2311.05437, 2023c.\nLiu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H.,\nLiu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter:\nBenchmarking and evaluating large video generation\nmodels. arXiv preprint arXiv:2310.11440, 2023d.\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\nYuan, Y., Wang, J., He, C., Liu, Z., et al. MMBench:\nIs your multi-modal model an all-around player? arXiv\npreprint arXiv:2307.06281, 2023e.\n11\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. In NeurIPS, pp. 2507–2521,\n2022.\nMaaz, M., Rasheed, H., Khan, S., and Khan, F. S. Video-\nChatGPT: Towards detailed video understanding via\nlarge vision and language models.\narXiv preprint\narXiv:2306.05424, 2023.\nMangalam, K., Akshulakov, R., and Malik, J. Egoschema:\nA diagnostic benchmark for very long-form video lan-\nguage understanding. Advances in Neural Information\nProcessing Systems, 36, 2024.\nOpenAI.\nGPT-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023a.\nOpenAI. GPT-4V(ision) system card. https:\/\/openai.\ncom\/research\/gpt-4v-system-card, 2023b.\nOrdonez, V., Kulkarni, G., and Berg, T. L. Im2Text: De-\nscribing images using 1 million captioned photographs.\nIn NeurIPS, pp. 1143–1151, 2011.\nPatraucean, V., Smaira, L., Gupta, A., Recasens, A., Mar-\nkeeva, L., Banarse, D., Koppula, S., Malinowski, M.,\nYang, Y., Doersch, C., et al. Perception test: A diagnostic\nbenchmark for multimodal video models. Advances in\nNeural Information Processing Systems, 36, 2024.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M¨uller, J., Penna, J., and Rombach, R. SDXL: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. In ICLR, 2024.\nQiu, H., Xia, M., Zhang, Y., He, Y., Wang, X., Shan, Y., and\nLiu, Z. Freenoise: Tuning-free longer video diffusion\nvia noise rescheduling. arXiv preprint arXiv:2310.15169,\n2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, pp. 8748–8763, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 21(1):5485–5551, 2020.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In CVPR, pp. 10684–10695, 2022.\nRunaway. Gen-2. https:\/\/research.runwayml.\ncom\/gen2, 2023.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image dif-\nfusion models with deep language understanding. In\nNeurIPS, pp. 36479–36494, 2022.\nSaito, M., Saito, S., Koyama, M., and Kobayashi, S. Train\nsparsely, generate densely: Memory-efficient unsuper-\nvised training of high-resolution temporal GAN. IJCV,\n128(10-11):2586–2606, 2020.\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In ACL, pp. 2556–\n2565, 2018.\nShen, C., Gan, Y., Chen, C., Zhu, X., Cheng, L., and Wang,\nJ. Decouple content and motion for conditional image-to-\nvideo generation. In AAAI, 2024.\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S.,\nHu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-A-\nVideo: Text-to-video generation without text-video data.\nIn ICLR, 2023.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequilib-\nrium thermodynamics. In ICML, pp. 2256–2265, 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In ICLR, 2020.\nSong, Y. and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. In NeurIPS, pp. 11918–\n11930, 2019.\nSoomro, K., Zamir, A. R., and Shah, M. A dataset of 101\nhuman action classes from videos in the wild. Center for\nResearch in Computer Vision, 2(11), 2012.\nSun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y.,\nGao, H., Liu, J., Huang, T., and Wang, X. Emu: Genera-\ntive pretraining in multimodality. In ICLR, 2024.\nTogether Computer.\nRedPajama:\nan open dataset\nfor training large language models, 2023.\nURL\nhttps:\/\/github.com\/togethercomputer\/\nRedPajama-Data.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. LLaMA: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971,\n2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\n12\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri,\nM. Learning spatiotemporal features with 3D convolu-\ntional networks. In ICCV, pp. 4489–4497, 2015.\nUnterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R.,\nMichalski, M., and Gelly, S. Towards accurate generative\nmodels of video: A new metric & challenges. arXiv\npreprint arXiv:1812.01717, 2018.\nvan den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural\ndiscrete representation learning. In NeurIPS, pp. 6309–\n6318, 2017.\nVillegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo,\nH., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and\nErhan, D. Phenaki: Variable length video generation\nfrom open domain textual descriptions. In ICLR, 2023.\nWang, F.-Y., Chen, W., Song, G., Ye, H.-J., Liu, Y., and Li,\nH. Gen-L-Video: Multi-text to long video generation via\ntemporal co-denoising. arXiv preprint arXiv:2305.18264,\n2023a.\nWang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang,\nY., Shen, Y., Zhao, D., and Zhou, J. VideoComposer:\nCompositional video synthesis with motion controllabil-\nity. In NeurIPS, 2023b.\nWang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang,\nH., Xu, J., Liu, Y., Wang, Z., et al. Internvideo: General\nvideo foundation models via generative and discrimina-\ntive learning. arXiv preprint arXiv:2212.03191, 2022.\nWang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X.,\nWang, Y., Luo, P., Liu, Z., et al. InternVid: A large-\nscale video-text dataset for multimodal understanding\nand generation. In ICLR, 2024.\nWu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F.,\nSapiro, G., and Duan, N. GODIVA: Generating open-\ndomain videos from natural descriptions. arXiv preprint\narXiv:2104.14806, 2021.\nWu, C.-Y., Zaheer, M., Hu, H., Manmatha, R., Smola, A. J.,\nand Kr¨ahenb¨uhl, P. Compressed video action recognition.\nIn CVPR, pp. 6026–6035, 2018.\nXu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large\nvideo description dataset for bridging video and language.\nIn CVPR, pp. 5288–5296, 2016.\nYan, W., Zhang, Y., Abbeel, P., and Srinivas, A. VideoGPT:\nVideo generation using VQ-VAE and transformers. arXiv\npreprint arXiv:2104.10157, 2021.\nYang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.\nZero-shot video question answering via frozen bidirec-\ntional language models. In NeurIPS, pp. 124–141, 2022.\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J.,\nHu, A., Shi, P., Shi, Y., et al. mPLUG-Owl: Modulariza-\ntion empowers large language models with multimodality.\narXiv preprint arXiv:2304.14178, 2023.\nYu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku,\nA., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized\nimage modeling with improved VQGAN. In ICLR, 2022.\nYu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O.,\nWang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al.\nScaling autoregressive multi-modal models: Pretraining\nand instruction tuning. arXiv preprint arXiv:2309.02591,\n2023a.\nYu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,\nX., and Wang, L.\nMM-Vet: Evaluating large multi-\nmodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490, 2023b.\nYu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., and\nTao, D. ActivityNet-QA: A dataset for understanding\ncomplex web videos via question answering. In AAAI,\npp. 9127–9134, 2019.\nZeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y.,\nand Li, H. Make pixels dance: High-dynamic video\ngeneration. arXiv preprint arXiv:2311.10982, 2023.\nZhang, B., Wang, L., Wang, Z., Qiao, Y., and Wang, H.\nReal-time action recognition with enhanced motion vec-\ntor CNNs. In CVPR, pp. 2718–2726, 2016.\nZhang, H., Li, X., and Bing, L.\nVideo-LLaMA: An\ninstruction-tuned audio-visual language model for video\nunderstanding. arXiv preprint arXiv:2306.02858, 2023.\n13\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nA. Experimental Settings\nA.1. Model Implementation Details\nVideo Tokenizer We employ the off-the-shelf visual tokenizer from LaVIT (Jin et al., 2024) to transform the video keyframe\ninto 90 visual tokens on average, which follows most existing MLLMs to utilize the ViT-G\/14 of EVA-CLIP (Fang et al.,\n2023) as the visual encoder. The visual codebook size is set to 16384. Please refer to the original paper for more details.\nDuring training and inference, images and keyframes are resized to 224×224 resolution as input.\nAs for motion tokenization, we downsample the original videos at 6 fps and then take 24 consecutive frames as a video clip\nto compute the motion vector M. It is further divided by the width and height of the corresponding video to normalize the\nvalue within the range of [−1, 1]. Before feeding into the motion tokenizer, the motion vector M is resized to a resolution\nof 20×36, resulting in the final input tensor shape being B × 24 × 20 × 36 × 2. The encoder fE and decoder fD in our\nmotion tokenizer both have L = 12 transformer blocks with 512 hidden states and 8 attention heads. Each block consists of\nspatial, temporal attention, and feed-forward layers. Before the attention computation, the motion input is reshaped into\n[(BT) × (HW) × D] and [(BHW) × T × D] for the spatial and temporal layers, respectively. We insert the spatial or\ntemporal downsampling layers after the [3, 6, 9, 12] encoder blocks to reduce the dimension of motion embeddings, which\nwill then be quantized into 135 (3 × 9 × 5) discrete motion tokens. The decoder fD includes symmetrical upsampling layers\nto recover the original input motion vector during training. The size of learned motion codebook is set to 1024. To improve\nthe training stability of the motion codebook, we leverage exponential moving average (EMA) updates with a weight of\n0.995. Before quantization, the motion embeddings are projected into a low-dimensional space (dim=32) to improve the\ncodebook usage, following the experience of Yu et al. (2022).\nVideo Detokenizer During training of the video detokenizer, we randomly sampled 24 consecutive frames from videos\ndownsampled at 6 fps. The motion conditioning encoder has the same transformer architecture (12 blocks) as fE, except that\nthe downsample layers are removed to keep the same temporal dimension with the input video frames. This strategy reduces\nthe compression of motion information during encoding and provides explicit guidance for each frame to be denoised\nin the 3D U-Net. The detailed architecture of the 3D U-Net employed follows the same implementations as Blattmann\net al. (2023b; 2023a). During the EDM-preconditioning optimization for the detokenizer, the distribution of log σ is set\nto N(1.0, 1.22) to encourage a higher noise level, which is found effective for the high-resolution generation (Girdhar\net al., 2023). We train the motion conditioning encoder, the input encoding layer, and all the cross-attention layers in the\n3D U-Net from scratch and initialize the other weights from the SVD img2vid-xt (Blattmann et al., 2023a). To reduce\nthe computational complexity, the detokenizer is first trained with a resolution 384 × 384 for 50k steps, and then further\nfine-tuned at two types of resolutions: 768 × 768 or 1024 × 576 for another 10k steps.\nLanguage Model We utilize Llama 2 7B (Touvron et al., 2023b) as the default large language model for the generative\npre-training. The weight of the language model is initialized from LaVIT (Jin et al., 2024) to preserve the learned visual\nprior knowledge to support the comprehension and generation for the image domain. During pre-training, we mix the\nimage-text, video-text pairs, and textual data in one batch to form the final multimodal input sequence.\nA.2. Pre-training Data\nThe training dataset used by Video-LaVIT only consists of publicly available image and video datasets. In the following, we\npresent a detailed elaboration of the dataset usage at each training stage.\nStage 1: The video tokenizer and detokenizer are trained on the WebVid-10M (Bain et al., 2021), which is an open-source\nvideo-text dataset containing 10 million video-text pairs scraped from the stock footage sites. Since both our tokenizer\nand detokenizer do not rely on textual data, we only employ pure video data at this stage. Due to the common watermarks\nin WebVid-10M, during the training of the video detokenizer, we incorporate a subset of InterVid-14M-aesthetics (Wang\net al., 2024) to remove watermarks in the generated videos. It has also been shown useful in PixelDance (Zeng et al., 2023).\nSpecifically, we first select a subset of 4s–10s video clips with the highest aesthetic scores and then follow SVD (Blattmann\net al., 2023a) in applying CRAFT (Baek et al., 2019) to filter out those videos with unwanted written text. The result contains\nabout 300k publicly available video clips. Noting that the 300k video subset is only used during the training of the\nvideo detokenizer to improve the aesthetics of the generated videos, the results reported in all the experiments are\ntested on the checkpoint that uses only the WebVid-10M dataset.\nStage 2: The language model is pre-trained on a mixture of video, image and text data, including WebVid-10M (Bain\net al., 2021); 93M samples from Conceptual Caption (Sharma et al., 2018; Changpinyo et al., 2021), SBU (Ordonez et al.,\n14\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\n2011), and BLIP-Capfilt (Li et al., 2022). Moreover, we also employ the English text corpus from RedPajama (Together\nComputer, 2023), which is open-source data like the original one to train LLaMA from scratch. The purpose of including the\nEnglish text corpus during pre-training is to preserve the already learned language understanding ability of LLM (e.g., the\nperformance on linguistic benchmarks like MMLU (Hendrycks et al., 2021)) while acquiring good multimodal capabilities.\nStage 3: For a fair comparison, we employ the same instruction tuning dataset as the existing works (Lin et al., 2023; Li\net al., 2023f) during this stage. It includes a 665k image-text instruction dataset from LLaVA v1.5 (Liu et al., 2023a) and a\n100k video-text instruction dataset from Video-ChatGPT (Maaz et al., 2023). All the understanding results are tested by the\nmodel trained after Stage 3.\nA.3. Training Settings\nThe detailed training hyper-parameter settings for the video tokenizer, detokenizer, and language model in Video-LaVIT are\nreported in Table 8. We adopt the same instruction tuning setting as LLaVA v1.5 (Liu et al., 2023a).\nConfiguration\nLanguage Model\nTokenizer\nDetokenizer\nLLM init\nLaVIT-7B\n-\n-\nOptimizer\nAdamW\nAdamW\nAdamW\nOptimizer Hyperparameters\nβ1 = 0.9, β2 = 0.95, ϵ = 1e−6\nβ1 = 0.9, β2 = 0.99, ϵ = 1e−6\nGlobal batch size\n2048\n512\n128\nPeak learning rate of LLM\n2e-5\n-\n-\nPeak learning rate of other Part\n5e-5\n1e-4\n5e-5\nLearning rate schedule\nCosine\nCosine\nCosine\nTraining Steps\n30K\n100K\n60K\nWarm-up steps\n2k\n5K\n3K\nWeight decay\n0.1\n0.001\n0.001\nGradient clipping\n1.0\n1.0\n1.0\nInput sequence to LLM\n2048\n-\n-\nNumerical precision\nbfloat16\nbfloat16\nbfloat16\nGPU Usage\n128 NVIDIA A100\n64 NVIDIA A100\n64 NVIDIA A100\nFramework\nMegatron\nDeepSpeed\nDeepSpeed\nTraining Time\n60h\n10h\n48h\nTable 8. The detailed training hyperparameters of Video-LaVIT\nA.4. Evaluation\nImage Understanding is evaluated using eight popular image question answering and multimodal benchmarks: VQA\nv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari et al., 2018), ScienceQA-IMG (Lu et al.,\n2022), MME (Fu et al., 2023), MMBench (Liu et al., 2023e), SEED (Li et al., 2023a), MM-Vet (Yu et al., 2023b). For\nquestion-answering datasets, we use the same prompts as in LLaVA-1.5 (Liu et al., 2023a), and adopt the widely used VQA\naccuracy as the evaluation metric.\nVideo Question Answering. Three common datasets are considered: MSVD-QA (Chen & Dolan, 2011), MSRVTT-QA (Xu\net al., 2016) and ActivityNet-QA (Yu et al., 2019). To assess model accuracy, a GPT-3.5 assistant (Maaz et al., 2023) is\nemployed, which also produces outputs a relative score ranging from 0 to 5.\nText-to-Image Generation. We adopt the validation set of MS-COCO (Lin et al., 2014) and randomly select 30K samples.\nThe quality of the generated images is evaluated by Fr´echet Inception distance (FID) (Heusel et al., 2017), which computes\nits Fr´echet distance to the ground truth in the feature space of a pre-trained Inception V3 model.\nText-to-Video Generation is measured on MSR-VTT (Xu et al., 2016) and UCF-101 (Soomro et al., 2012). For MSR-VTT,\nwe use all 2990 videos and sample one caption for each video, resulting in 2990 video-text pairs; for UCF-101, we sample\n20 videos per class and follow PYoCo (Ge et al., 2023) to curate prompts for each class, producing 2020 video-text pairs.\nTheir evaluation metrics are detailed below.\n• CLIP similarity (CLIPSIM) (Wu et al., 2021) measures the semantic similarity between video-text pairs. We follow\nPhenaki (Villegas et al., 2023) and VideoPoet (Kondratyuk et al., 2023) in using a ViT-B\/16 (Radford et al., 2021)\nto compute CLIP scores between 224×224 sized video frames and their corresponding captions. The final score is\n15\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\naveraged over all generated video frames.\n• Fr´echet video distance (FVD) (Unterthiner et al., 2018) evaluates the Fr´echet distance between generated and real videos\nin the feature space of an I3D action classification model (Carreira & Zisserman, 2017) pre-trained on Kinetics-400 (Kay\net al., 2017).\n• Fr´echet Inception distance (FID) (Heusel et al., 2017) measures the Fr´echet distance between generated and real video\nframes. Following PYoCo (Ge et al., 2023), we use a ViT-B\/32 model (Kynk¨a¨anniemi et al., 2023) to extract the frame\nfeatures. The final result is averaged over all video frames.\n• Inception score (IS) (Saito et al., 2020) evaluates the distribution of our generated video frames. We employ a C3D\nmodel (Tran et al., 2015) fine-tuned on UCF-101 to calculate a video version of the inception score. The model takes\nthe central 16 frames of each video as the input.\nNote that there are slight variations in the evaluation protocols of different papers. We have sought to keep our protocol the\nsame as or similar to most of the top-ranked methods.\nB. Additional Results\nB.1. Multimodal Generation\nThis section provides additional qualitative results and an ablation study to demonstrate the effectiveness of our design for\nmultimodal generation, complementing the existing comparisons in the main paper.\nText-to-Image Generation. Figure 6 illustrates the comparison of text-to-image generation between Video-LaVIT and\nSDXL (Podell et al., 2024). Overall, our method achieves competitive visual quality while having better language\nunderstanding and reasoning capabilities. For example, in the top-left case of a young woman in front of a UFO, our method\nproduces highly aesthetic headshots of the woman, while capturing the detail of “sharp focus” in the text prompt. And in the\nbottom-left example of apple painting, our model successfully infers from the prompt “neither is red and both are green” to\ndraw two green apples, thanks to the better logical reasoning ability of the LLM-based generation approach we adopted.\nText-to-Video Generation. Figure 7 compares Video-LaVIT to a closed-source model Gen-2 (Runaway, 2023). As can be\nseen, our model produces high-quality videos that are generally comparable to Gen-2, which is especially evident in the last\ntwo examples where it successfully captures details such as ”moss and many flowers” and ”autumn” in the text prompt\nand yields very similar results to Gen-2. Moreover, the first two comparisons demonstrate a favorable prompt following\nability of our model. In the first case with the keyword “running”, our model produces significant camera motion toward\nthe cabin, while the movement in Gen-2 is relatively nuanced. In the second case, our model correctly displays multiple\n“pirate ships” as the prompt specified, with artistic details such as all the ships being on fire, according to the implication of\n“intense battle”. These results support the benefits of unified video-language pre-training in prompt following capabilities.\nImage-to-video generation. Figure 8 presents a comparison of Video-LaVIT with the open-source model SVD (Blattmann\net al., 2023a), both conditioned on synthetic image prompts. Moving to some unseen test cases, our method produces video\nclips featuring both natural and refined motions, thanks to the decomposed video representation that can better transfer\nmotion-related knowledge to new visual inputs. For example, in the middle case, our generated goat smoothly lowers\nits head and blinks as if it were a human to think, while the goat in the video produced by SVD hardly moved. In the\nbottom case, where the image prompt shows a teddy is riding a motorcycle, our generated full video looks very natural\nand similar to a human riding a motorcycle, while SVD constantly produces a scenario where the motorcycle is moving a\ndifferent direction from where its tire is pointing (which is physically wrong). Overall, our model demonstrates superior\nimage-to-video generation performance with the inclusion of decoupled visual-motion tokenization and LLM pre-training.\nLong Video Generation is showcased in Figure 9. By explicitly constraining the noise when decoding successive video\nclips, our model can provide a high temporal consistency during long video generation. For example, in the first two\ncases, the dog and the jeep car maintain the same identity across different clips with highly coherent visual details. In the\nlast example which features large camera movement, the moving trajectory remains consistent as it approaches the cabin\naccording to the text prompt. These examples all illustrate our reasonably good quality of long video generation. Note that\nall the generated videos are provided at https:\/\/video-lavit.github.io.\nThe Effect of Enhanced Motion Conditioning. To rigorously reconstruct original video content, we employ the enhanced\n16\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 9. The impact of incorporating motion tokens on image comprehension.\nMethod\nVQAv2\nGQA\nVizWiz\nSQA\nMME\nMMB\nSEED\nMM-Vet\nw\/o motion\n80.0\n63.7\n54.4\n71.5\n1533.2\n67.5\n64.7\n34.5\nw motion\n80.3\n64.4\n56.0\n70.0\n1551.8\n67.3\n64.0\n33.2\nTable 10. The impact of svd-img2vid-xt weight initialization on text-to-video generation.\nMethod\nMSR-VTT\nUCF-101\nCLIPSIM (↑)\nFVD (↓)\nFID (↓)\nIS (↑)\nFVD (↓)\nw\/ svd-img2vid-xt\n0.3010\n169.51\n11.80\n37.96\n274.96\nw\/o svd-img2vid-xt\n0.3012\n188.36\n11.27\n44.26\n280.57\nconditioning: motion input condition and motion feature condition for training the 3D video U-Net gV . We illustrate the\neffect of proposed enhanced motion conditioning (EMC) strategy on video decoding in Figure 10. The variant “w\/o EMC”\nonly leverages motion vectors as the input condition. Compared with using EMC, it is incapable of recovering the motion of\noriginal input videos. For example, the “train” and “fish” barely moved in the shown video samples, which demonstrates the\neffectiveness of our proposed conditioning strategy.\nB.2. Multimodal Understanding\nThis section presents qualitative results of Video-LaVIT for image and video understanding. First, Table 11 showcases our\nperformance in image question answering using the famous test example from GPT-4 (OpenAI, 2023a). As can be seen, our\nmodel produces a reasonable answer with a good number of correct details (e.g. the type of the vehicle being SUV) and\neven a friendly safety warning. In comparison, one of the recent multimodal LLMs, LLaVA (Liu et al., 2023b), produces a\nroughly correct answer with some inaccurate detail (mistaking the vehicle type as “minivan or van”).\nFor video question answering, Tables 12 and 13 compares our method to Video-LLaVA (Lin et al., 2023) and Video-\nChatGPT (Maaz et al., 2023) based on the video clips from Video-ChatGPT. In the first example of Table 12 which asks to\nexplain why a video is funny, our model yields the most concise answer among the video-language models compared, and at\nthe same time contains a salient point that the other models failed to mention. The next example in Table 12, on the other\nhand, shows that our method produces fewer hallucinations than Video-LLaVA and Video-ChatGPT, as the latter two models\ntend to generate overly detailed action descriptions that have no basis in the video. And lastly, in the example of Table 13,\nour model follows the instruction prompt by producing a beautiful fairy tale with both conciseness and a moral lesson (“love\ncan conquer all”). To summarize, our method demonstrates reasonably good multimodal understanding capabilities across\ndifferent test cases, in line with the previous quantitative comparison on multiple benchmarks.\nB.3. Ablation Studies\nImpact of Motion Tokens on Image Comprehension. Video-LaVIT indiscriminately treat all the modalities (video, image,\nand text) as 1D discrete tokens fed into LLMs. The impact of incorporating motion tokens on image comprehension is\nreported in Table 9. As observed, including motion tokens hardly affects the understanding performance of the image,\nwhich demonstrated the effectiveness of the proposed decoupled visual-motional tokenization. Video-LaVIT is capable of\nmodeling video, image, and text data in a unified framework.\nImpact of Weight Initialization. We re-trained the detokenizer of Video-LaVIT from scratch without svd-img2vid-\nxt initialization on the WebVid-10M dataset and found that our model can still achieve comparable video generation\nperformance. The detailed text-to-video generation results are reported in Table 10. As observed, training the detokenizer\nfrom scratch had little impact on the final results.\nC. Limitations\nOur proposed model cannot generate very long videos due to its limited context window (4096) and dataset restriction. This\nwork only used the public WebVid-10M as the video pre-training data. In WebVid, the video durations are relatively short\n(about 15s on average) and the video scenes barely change, which results in our model generating similar keyframes in\ndifferent clips. On the other hand, a general concern is that our training cost is still too high to scale to web-scale video data,\nwhich may require further optimization through joint exploitation of spatial and temporal redundancies in video.\n17\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nClose up headshot, futuristic young woman, wild hair sly\nA steaming cup of coffee with mountains in the\nsmile in front of gigantic UFO, dslr, sharp focus, dynamic\nbackground. Resting during road trip.\ncomposition.\nSDXL\nVideo-LaVIT\nA squirrel is inside a giant bright shiny crystal ball on the\nAn origami fox walking through the forest.\nsurface of blue ocean. There are few clouds in the sky.\nSDXL\nVideo-LaVIT\nA watercolor painting of two apples on a wooden table,\nA cat is sitting on a basket under a bench.\nneither is red and both are green.\nSDXL\nVideo-LaVIT\nFigure 6. Text-to-image generation comparison with SDXL (Podell et al., 2024). Prompts are from SDXL, CM3Leon (Aghajanyan et al.,\n2022), Imagen (Saharia et al., 2022), VideoPoet (Kondratyuk et al., 2023), LMD (Lian et al., 2023), and LayoutGPT (Feng et al., 2024).\nOur model provides comparable visual quality while showing better logical and spatial reasoning abilities (see the last two cases).\n18\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nFirst-person view running through the woods and approaching a large beautiful cabin, highly detailed.\nGen-2\nVideo-LaVIT\nFlying through an intense battle between pirate ships in a stormy ocean..\nGen-2\nVideo-LaVIT\nPOV footage of approaching a small cottage covered in moss and many flowers, tilt shift, arc shot.\nGen-2\nVideo-LaVIT\nFPV drone footage of an ancient city in autumn.\nGen-2\nVideo-LaVIT\nFigure 7. Text-to-video generation comparison with Gen-2 (Runaway, 2023) using default parameters. Prompts are from VideoPoet (Kon-\ndratyuk et al., 2023) and PixelDance (Zeng et al., 2023). Our model provides a similarly high visual quality (in the bottom two cases)\nwhile following the text prompt better (including “running” in the first example and “pirate ships” in the second examples).\n19\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nSVD\nVideo-LaVIT\nSVD\nVideo-LaVIT\nSVD\nVideo-LaVIT\nFigure 8. Image-to-video generation comparison with SVD (Blattmann et al., 2023a) using the stable-video-diffusion-img2vid-xt version.\nThe generation is conditioned on the leftmost frame. Our model can produce more sophisticated animal motions (see the top two cases)\nwhile not violating the physical rules (e.g., in the second last row, the motorcycle is not moving in the direction of its tire).\n20\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nA dog in the sun.\n0–24 frames\n24–48 frames\nA jeep car is moving on the beach.\n0–24 frames\n24–48 frames\nFirst-person view running through the woods and approaching a large beautiful cabin, highly detailed.\n0–24 frames\n24–48 frames\nFigure 9. Long video generation examples. Prompts are Gen-L-Video (Wang et al., 2023a) and VideoPoet (Kondratyuk et al., 2023). Our\ngenerated videos are temporally coherent even across different decoded clips, thanks to our proposed explicit noise constraint.\n21\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nOriginal\nw\/ EMC\nw\/o EMC\nOriginal\nw\/ EMC\nw\/o EMC\nFigure 10. Ablation study of enhanced motion conditioning (EMC) for video reconstruction. The last row (i.e., w\/o EMC) indicates\nonly motion vectors as the input condition for training 3D U-Net. As illustrated, incorporating EMC can improve the fidelity of the\nreconstructed videos. The video samples are taken from WebVid-10M (Bain et al., 2021).\n22\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 11. Image question answering comparison with LLaVA (Liu et al., 2023b), GPT-4 (OpenAI, 2023a), BLIP-2 (Li et al., 2023c), and\nOpenFlamingo (Awadalla et al., 2023). Our model answers correctly like the other multimodal LLMs, but without missing details such as\nthe vehicle type (it is an SUV instead of a van or minivan). The table format follows GPT-4.\nVisual input example, Extreme Ironing:\nSource: https:\/\/www.barnorama.com\/wp-content\/uploads\/2016\/12\/03-Confusing-Pictures.jpg\nUser\nWhat is unusual about this image?\nVideo-LaVIT\nThe unusual aspect of this image is that a man is ironing clothes on an ironing board placed\non the back of a yellow SUV while it is driving down the street. This is not a typical scenario,\nas ironing clothes is usually done indoors, in a home or a laundry room, and not while the\nvehicle is in motion. The man’s actions pose a risk to himself and others on the road, as\nironing on a moving vehicle can lead to accidents or distractions for the driver.\n[Start a new conversation, and clear the history]\nUser\nWhat’s happening in the scene?\nVideo-LaVIT\nThe image depicts a man ironing clothes on an ironing board placed on the back of a yellow\nSUV. The man is standing on the back of the vehicle, attentively ironing his clothes. The\nironing board is positioned in the middle of the SUV, with the man standing on it to reach\nthe clothes. In the background, there are two cars visible, one on the left side and another on\nthe right side of the scene. The man appears to be focused on his task, despite the unusual\nlocation for ironing clothes.\nUser\nWhat is unusual about this image?\nLLaVA (Liu et al., 2023b)\nThe unusual aspect of this image is a man ironing clothes on the back of a minivan or van.\nThis is not a typical place to perform this activity, as one would usually iron clothes in a\nmore stationary and safe location, such as a home, using a regular ironing board. The scene\ndepicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which\ncan be both unsafe and unconventional. Additionally, it is not clear how the man is able to\nmaintain balance and stability while ironing clothes in such an unstable environment.\n[Start a new conversation, and clear the history]\nUser\nWhat’s happening in the scene?\nLLaVA (Liu et al., 2023b)\nThe image depicts a man standing on top of a yellow SUV in a busy city street. He is holding\na portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene,\nthere are other cars, a traffic light, and a fire hydrant. Additionally, there are two other people\nin the vicinity, one close to the man on the car and another further away. The man on the\nladder appears to be attracting attention due to his unconventional choice of ironing his\nclothes on top of a moving car. The city street around him is bustling with activity, adding to\nthe unique nature of the scene.\nUser\nWhat is unusual about this image?\nGPT-4 (OpenAI, 2023a)\nThe unusual thing about this image is that a man is ironing clothes on an ironing board\nattached to the roof of a moving taxi.\nUser\nCan you explain this meme in detail?\nBLIP-2 (Li et al., 2023c)\na man is sitting on the back of a yellow cab\nUser\nCan you explain this meme in detail?\nOpenFlamingo (Awadalla et al., 2023)\nThe man is drying his clothes on the hood of his car.\n23\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 12. Video question answering comparison with Video-LLaVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023). Video\nexamples are from Video-ChatGPT. In the first example, our model directly points to a salient explanation for why the video is funny.\nIn the second example, our model responses accurately without hallucinating about the video (the woman only swung the ax once).\nUser\nWhy is this video funny?\nVideo-LaVIT\nThe video is funny because the baby is wearing glasses and trying to read a book, but she is\nnot able to do so. The baby’s attempts to read the book are adorable and amusing to watch.\nUser\nWhy is this video funny?\nVideo-LLaVA (Lin et al., 2023)\nThe video is funny because the baby is trying to read a book while wearing glasses, which is\na humorous sight. The baby’s attempts to read the book are also amusing, as they seem to be\nstruggling with the task. Additionally, the baby’s facial expressions and movements add to\nthe overall humor of the video.\nUser\nWhy is this video funny?\nVideo-ChatGPT (Maaz et al., 2023)\nThe video is funny because it shows a baby girl who is learning to read. She is sitting on\na bed and looking at a book, and she is trying to read it. The baby girl is wearing glasses,\nand she is holding the book with both hands. The baby girl is trying to read the book, and\nher actions are adorable and endearing. The video captures the innocence and curiosity of a\nyoung child as she learns to read, which is a joyful and amusing sight for the viewer.\nUser\nWhat does the woman use to split the logs and how does she do it?\nVideo-LaVIT\nThe woman uses an ax to split the logs. She picks up the ax and swings it down with force,\nsplitting the logs in half.\nUser\nWhat does the woman use to split the logs and how does she do it?\nVideo-LLaVA (Lin et al., 2023)\nThe woman uses an ax to split the logs. She holds the ax in her hand and swings it down\nonto the log. She repeats this process several times, splitting the log into smaller pieces.\nUser\nWhat does the woman use to split the logs and how does she do it?\nVideo-ChatGPT (Maaz et al., 2023)\nThe woman uses a large axe to split the logs. She starts by placing the axes on the log and\nthen swings it down with a powerful motion. The axe slices through the log with ease, and\nthe woman continues to swing it until the log is split in half.\n24\nVideo-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization\nTable 13. Video question answering comparison with Video-LLaVA (Lin et al., 2023) and Video-ChatGPT (Maaz et al., 2023). Video\nexamples are from Video-ChatGPT. As shown in this example, our model captures the underlying context of this video, which is a love\nstory. The fairy tale told, which implies that love will prevail, is naturally inspired by the video without introducing an unmentioned third\nparty (jealous sister or stepmother) as in the other models. The use of the phrase “beautiful glass slipper” adds a nice detail to the story.\nUser\nCreate a short fairy tale with a moral lesson inspired by the video.\nVideo-LaVIT\nOnce upon a time, there was a beautiful princess who lived in a grand castle. She was known\nfor her kindness and compassion towards others. One day, a handsome prince visited her\ncastle and fell deeply in love with her. However, the princess was hesitant to accept his love\ndue to her fear of losing her independence and being trapped in a loveless marriage. The\nprince, determined to win her heart, gave her a beautiful glass slipper as a gift. The princess\ntried it on and danced with the prince, realizing that true love transcends all boundaries.\nFrom that day on, the princess and the prince lived happily ever after, proving that love can\nconquer all.\nUser\nCreate a short fairy tale with a moral lesson inspired by the video.\nVideo-LLaVA (Lin et al., 2023)\nOnce upon a time, in a kingdom far away, there lived a beautiful princess named Rose. She\nwas known for her kindness and beauty, and everyone loved her. One day, a handsome prince\nnamed Leo came to visit the kingdom. He was fascinated by Rose’s beauty and decided\nto stay in the kingdom. However, Rose’s jealous sister, Lily, did not like the attention Leo\nwas giving her sister. She tried to harm Rose, but Leo saved her and they fell in love. The\nkingdom celebrated their love and they lived happily ever after. The moral of the story is\nthat true love conquers all and that jealousy can lead to harmful consequences.\nUser\nCreate a short fairy tale with a moral lesson inspired by the video.\nVideo-ChatGPT (Maaz et al., 2023)\nSure, here’s a short fairy tale with a moral lesson inspired by the video: Once upon a\ntime, there was a beautiful princess named Cinderella who lived in a castle with her cruel\nstepmother and two stepsisters. She was known for her kindness and her love for animals,\nespecially birds. One day, while she was out in the garden, she met a handsome prince who\nwas lost in the woods. He was in need of help, and Cinderella offered to guide him back\nto the castle. As they walked together, the prince noticed that Cinderella’s stepsisters were\ncruel to her and her and stepmother was always mean to her. He asked her about her life and\nhow she managed to stay so kind and gentle despite the hardships she faced. Cinderella told\nhim that she found happiness in the simple things in life, such as the beauty of nature and the\nlove of animals. The prince was touched by her story and decided to help her in any way\nhe could. He introduced her to the king and queen, who were impressed by her kindness\nand grace. They arranged for her to attend a ball at the castle, where she met the prince once\nagain. They fell in love and got married, living happily ever after. The moral of the story is\nthat even in difficult situations, one should find happiness in the simple things in life and\nnever lose sight of their values and kindness.\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization.pdf"}
{"title":"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer","authors":"Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, Jifeng Dai","summary":"Developing generative models for interleaved image-text data has both\nresearch and practical value. It requires models to understand the interleaved\nsequences and subsequently generate images and text. However, existing attempts\nare limited by the issue that the fixed number of visual tokens cannot\nefficiently capture image details, which is particularly problematic in the\nmulti-image scenarios. To address this, this paper presents MM-Interleaved, an\nend-to-end generative model for interleaved image-text data. It introduces a\nmulti-scale and multi-image feature synchronizer module, allowing direct access\nto fine-grained image features in the previous context during the generation\nprocess. MM-Interleaved is end-to-end pre-trained on both paired and\ninterleaved image-text corpora. It is further enhanced through a supervised\nfine-tuning phase, wherein the model improves its ability to follow complex\nmulti-modal instructions. Experiments demonstrate the versatility of\nMM-Interleaved in recognizing visual details following multi-modal instructions\nand generating consistent images following both textual and visual conditions.\nCode and models are available at\n\\url{https:\/\/github.com\/OpenGVLab\/MM-Interleaved}.","url":"http:\/\/arxiv.org\/abs\/2401.10208v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2401.10208v2","published":1705603816000,"comment":"20 pages, 9 figures, 17 tables","pdf_text":"MM-Interleaved: Interleaved Image-Text\nGeneration via Multi-modal Feature Synchronizer\nChangyao Tian2,1∗†, Xizhou Zhu3,4,1∗, Yuwen Xiong5,1∗, Weiyun Wang6,1†,\nZhe Chen7,1†, Wenhai Wang2,1, Yuntao Chen8, Lewei Lu4, Tong Lu7,\nJie Zhou3, Hongsheng Li2, Yu Qiao1, Jifeng Dai3,1B\n1OpenGVLab, Shanghai AI Laboratory\n2MMLab, CUHK\n3Tsinghua University\n4SenseTime Research\n5University of Toronto\n6Fudan University\n7Nanjing University\n8CAIR, HKISI, CAS\nhttps:\/\/github.com\/OpenGVLab\/MM-Interleaved\nAbstract.\nDeveloping generative models for interleaved image-text data holds both\nresearch and practical value. It necessitates models to comprehend in-\nterleaved sequences and subsequently generate images and text. How-\never, existing attempts are limited by the issue that the fixed number\nof visual tokens cannot efficiently capture image details, which is par-\nticularly problematic in the multi-image scenarios. To address this, this\npaper presents MM-Interleaved, an end-to-end generative model for in-\nterleaved image-text data. It introduces a multi-scale and multi-image\nfeature synchronizer module (MMFS), enabling intermediately direct ac-\ncess to fine-grained image features from the previous context during the\ngeneration process. MM-Interleaved is end-to-end pre-trained on both\npaired and interleaved image-text corpora. It is further enhanced through\na supervised fine-tuning phase, wherein the model improves its ability\nto follow complex multi-modal instructions. Experiments demonstrate\nthe versatility of MM-Interleaved in recognizing visual details follow-\ning multi-modal instructions and generating consistent images following\nboth textual and visual conditions. Code and models are available at\nhttps:\/\/github.com\/OpenGVLab\/MM-Interleaved.\nKeywords: Interleaved image-text modeling · Large language models\n1\nIntroduction\nInterleaved image-text data like news and blogs is a sequence of multiple images\ninterspersed with text, which is ubiquitous on the internet. As an extension of\nimage-text pairs widely used in previous multi-modal models [20,21,37,71,83],\nthe interleaved format not only covers a broader range of data but also presents\nlonger and more complex article structures. Developing multi-modal models ca-\npable of simultaneously comprehending and generating such interleaved image-\ntext data has significant research value and practical application potential. It\n* Equal contribution; B Corresponding author (daijifeng@tsinghua.edu.cn).\n† This work is done when Changyao Tian, Weiyun Wang, and Zhe Chen are interns\nat Shanghai AI Laboratory.\narXiv:2401.10208v2  [cs.CV]  2 Apr 2024\n2\nC. Tian et al.\nCrong Pororo Poby and \nEddy have come over \nto Loopy's house.\nLoopy invites her \nfriends in.\n……\n……\n……\nAuto-regressively generate either a text token or an image\nInput Interleaved Sequence\nGenerated Interleaved Sequence\nMulti-scale \nFeature Maps\nInput Tokens\nImage Token\nText Token\nFeature \nSynchronizer\nFig. 1: Illustration of Multi-Modal Feature Synchronizer (MMFS). In the\nauto-regressive generation of interleaved image-text sequences, besides interacting with\nlow-resolution image tokens via self-attention, tokens in MM-Interleaved could also use\nMMFS to cross-attend multi-scale high-resolution image features. MMFS ensures the\nattention will have exactly the same causal relation between images and text tokens.\nexpands the scope of previous multi-modal models [84] and enables more uni-\nfied processing of multi-modal data, bridging previously disparate research fields\nsuch as text-to-image generation [73,74] and visual question-answering [47,54].\nWith recent progress in multi-modal modeling with Large Language Models\n(LLMs) [7,65], exploring how LLMs can be leveraged for interleaved image-text\nmodeling has become increasingly attractive. A core challenge of interleaved\nmulti-modal LLMs is how to effectively handle multiple images within the limited\ncontext length of LLMs. In most multi-modal LLMs, images are encoded as visual\ntokens from an image tokenizer (also known as image encoder) and fed into\nLLMs together with text tokens [17,40,43,84]. However, due to computational\nand memory constraints, multi-modal LLMs have limited context window sizes\n(e.g., 2048 or 4096 tokens). To reduce computational demands and the required\ncontext length of LLMs, Perceiver Resamplers [3] are commonly used to map\neach image from up to 1024 tokens to a small fixed number of visual tokens (e.g.,\n32 or 64 tokens) as in Fig. 2.\nDue to the relatively small visual token number, critical image details may\nbe overlooked, especially in tasks requiring fine-grained observation. Although\nincreasing the number of visual tokens [56,101] may address this issue to some\nextent, the token number per image must be restricted to ensure complete input\ninto the LLMs. Allocating a large number (e.g., 2890 in [85]) of visual tokens per\nimage would pose a significant limitation especially in multi-image scenarios.\nWe note that the main problem here is the context insensitivity of Perceiver\nResamplers, as it only takes image features as input. Each input image is com-\npressed into a fixed number of tokens, making it challenging or even infeasible\nto preserve all requisite information and accommodate subsequent generation\nrequirements. However, such information loss can actually be mitigated through\nfurther feature extraction in the intermediate layers of the LLMs. Based on\nthe intermediate features of the LLMs, relevant information can be dynamically\nextracted from the original images to fulfill the perceptual requirements. Such\na dynamic extraction mechanism can also help to effectively handle scenarios\ninvolving multiple images and high-resolution image feature maps.\nMM-Interleaved\n3\ntext\n(a) BLIP-like\nMulti-modal LLM\nResampler\nImage Encoder\n(b) Flamingo-like\nMulti-modal LLM\nResampler\nImage Encoder\n(c) GILL\/Emu-like\nMulti-modal LLM\nResampler\nImage Encoder\nImage Decoder\nimage\nconnection to input layer\nconnection to intermediate layers\n(d) MM-Interleaved (Ours)\nMulti-modal LLM\nResampler\nImage Encoder\nImage Decoder\nFig. 2: Different types of image-text generative modeling. (a) and (b) can\nonly generate text. (c) and (d) can generate both images and text. All types except\n(d) are limited by the fixed number of visual tokens extracted by context-insensitive\nResampler, which will lose image details and is problematic in multi-image scenarios.\nTo facilitate dynamic image feature extraction in the intermediate layers of\nthe LLMs, we design a fine-grained Multi-Modal Feature Synchronizer (MMFS)\nbased on the Deformable Sparse Attention [112], which can efficiently extract\nrelevant information from multi-scale image feature maps and multiple images\nas shown in Fig. 1. Based on MMFS, we further propose MM-Interleaved, a\nnovel end-to-end generation model for processing interleaved image-text data,\nas shown in Fig. 4, which can generate both textual and visual response given\ninterleaved image-text inputs.\nSpecifically, input images and text are first mapped into tokens through their\nrespective tokenizers (i.e., image encoder and word embedding) and then fed to\nthe LLM, arranged in their original order. A special token <BoI> is introduced to\nrepresent “Begin of Image”. When the LLM processes the input sequences, each\ntoken in the intermediate layers directly observes multi-image and multi-scale\nfeature maps from the previous context through MMFS. After being processed\nby the LLM, the features of the text tokens are then used to predict the next text\nword. When the <BoI> token is predicted, an image decoder based on diffusion\nmodels is used to predict the next image. All previous LLM output features are\npassed into the image decoder as the generation conditions. With MMFS, the\nimage decoder can also extract details of previous images intermediately.\nMM-Interleaved is pre-trained on a mixture of image-text pairs and inter-\nleaved image-text sequences without using any in-house data. Similar to previous\nmulti-modal LLMs, supervised fine-tuning can further enhance the model capa-\nbilities. Due to the end-to-end generative modeling, fine-tuning can be applied to\nboth text and image generation. Our model is fine-tuned on several tasks, includ-\ning visual question-answering, image captioning, referring expression grounding,\ntext-to-image generation, segmentation-to-image translation, and visual story-\ntelling. As is illustrated in Fig. 3, our model achieves competitive results and\nshows higher token efficiency compared with previous image-to-text and text-\nto-image methods. Compared with joint image and text generation models, we\nset the new SOTA results for a wide range of tasks. The key contributions are:\n– We propose Multi-Modal Feature Synchronizer (MMFS) to reduce the number\nof visual tokens required by multi-modal LLMs, which can efficiently extract\n4\nC. Tian et al.\nMM-Interleaved (ours)\nText-Generation SOTA\nImage-Generation SOTA\nInterleaved-Generation SOTA\nZero-shot \nCOCO\nCaption\nVizWiz\nQuestion-\nAnswering\nRefCOCO\nReferring \nExpression\nZero-shot \nText-to-Image \nGeneration\nSegmentation\n-to-Image \nTranslation\nVisual \nStorytelling \n(Last Image)\nVisual \nStorytelling \n(Multi-Image)\n129.0\n117.7\n117.2\n93.0\n85.8\n30.3\n17.4\n80.2\n67.2\n80.0\n54.9\n49.4\n53.6\n89.9\n89.5\n7.90\n8.46\n7.23\n0.44\n0.35\n39.7\n59.5\n14.7\n15.4\n…\nVQA v2\nQuestion-\nAnswering\nImage-to-\nParagraph \nCaption\nFlickr \nImage\nCaption\nInterleaved Text & Image\n↓\nText \/ Image\nText + Segment\n↓\nImage\nText\n↓\nImage\nText + Image\n↓\nBox\nImage\n↓\nText\nText + Image\n↓\nText\nSEED-Bench-2 (L3) \n(Comprehension \n& Generation)\n44.8\n45.0\nFig. 3: Comparison on multi-modal benchmarks. 11 representative tasks are\nlisted. See Sec. 4 for other benchmarks. MM-Interleaved (red) consistently outperforms\nprevious methods supporting interleaved generation (green). It also surpasses or is on\npar with specialists in either text generation (brown) or image generation (blue).\nfine-grained visual details from multi-scale feature maps of multiple images\naccording to the intermediate context features of the multi-modal LLMs;\n– We propose MM-Interleaved based on MMFS for generative modeling of in-\nterleaved image-text data, which can preserve fine-grained image information\nwith only a small number of visual tokens per image, and can be optimized\nend-to-end for both text and image generation;\n– Our method can generate both accurate text descriptions and visually con-\nsistent images given interleaved image-text inputs, achieving SOTA results\non various multi-modal comprehension and generation benchmarks without\nusing any in-house data.\n2\nRelated Work\nModeling of Paired Image-Text Data plays an important role in the progress\nof multi-modal research in recent years. A series of public large-scale image-text\npairs datasets have been released [23,37,77]. Based on these data, models trained\nby image-text contrastive learning [39,50,71,83] are able to recognize and under-\nstand open-world semantics. Subsequent works [49, 94, 99, 113] further incorpo-\nrated text generation tasks such as image captioning, while other works [73,74]\nwere proposed to generate images based on text prompts. The latest progress\nof LLMs [65, 66] has launched a new era, leading to the emergence of many\nLLM-based multi-modal models trained using image-text pairs [47, 54, 91, 92].\nFor example, LLaVA [54] connects LLMs with pretrained visual encoders using\nlinear projections to build powerful multi-modal foundation models.\nModeling of Interleaved Image-Text Data has received increasing atten-\ntion recently. Early works such as Kosmos [33] and Flamingo [3] focused on\nunderstanding such data with non-public datasets. To promote the development\nof this field, public and large-scale interleaved image-text datasets were later\nreleased in [45, 111]. More recent works [103, 105] have concentrated on under-\nstanding the interleaved data. Nevertheless, their generative capabilities remain\nconfined to text. Initial attempts at generating images and text given interleaved\ncontexts were first undertaken in [17,43,84,101]. A two-stage generation process\nwas introduced by [43, 84], wherein text and images are generated in the first\nMM-Interleaved\n5\nand second stages, respectively. CM3Leon [101] employs VQ-VAE [87] to convert\nimages into discrete tokens, facilitating token-level auto-regressive modeling as\nlanguage modeling. However, it primarily focuses on image generation capabil-\nities and exhibits notable weaknesses in image understanding. DreamLLM [17]\nfocuses on single-stage end-to-end modeling using raw image pixels as inputs.\nEmu2 [82], SEED-LLaMA [25] and VL-GPT [110] adopt additional training\nstage for the image tokenizer-detokenizer. Despite these efforts, they only feed\nimage information at the LLM inputs, which are limited by the problem that\nfixed number of visual tokens cannot efficiently preserve image details.\nIntegrating Image Details into LLMs is important for multi-modal LLMs.\nMost works use Perceiver Resamplers [3,47,109] to extract image information via\ncross-attention, mapping each image into a fixed number of visual tokens. For\nexample, Flamingo [3] adopts Resamplers in the intermediate layers of LLMs,\ninjecting extracted image features into LLMs through gated residual connections.\nBLIP-2 [47] and Mini-GPT4 [109] introduce Resamplers at the bottom of LLM\nto insert the extracted visual tokens into the input text sequences. While these\nmethods achieve good performance, image details remain to be overlooked due\nto the small number (e.g., 32 or 64) of visual tokens. To preserve more details,\nrecent works [6,10,54,56] increase the number of input visual tokens per image\nto hundreds. In SPHINX [85], the token number is further increased to 2,890.\nAlthough it helps mitigate information loss, the computational and memory\ndemands of LLMs are also significantly increased. Increasing the number of visual\ntokens per image is particularly problematic in multi-image scenarios, where\nmultiple images naturally require more visual tokens, making it hard to use for\ninterleaved image-text data.\n3\nMethod\n3.1\nTask Formulation\nTo build an end-to-end generative model for interleaved image-text data, we first\nconsider an interleaved image-text sequence X = {x1, x2, x3, . . . }, where each\nelement xn is either a text token (denoted as xL\nn) or a whole image (denoted\nas xV\nn ). Text and images are arranged in the order in which they appear in the\noriginal content. In multi-modal LLMs, a common practice is to first extract\nembeddings for each text token and each image and then feed them into LLMs,\ni.e., eL\nn = EL(xL\nn) and eV\nn = EV (xV\nn ), where EL denotes word embedding following\nstandard practices in NLP. EV is typically an image encoder (e.g., ViTs [18])\nfollowed by a Perceiver Resampler [3] to map each image to a fixed number of\ntokens. Then, the generative modeling is trained to maximize the log-likelihood:\n  \\ foot n\no\nt\nesi ze \\begin  \n{\nalig\nned } \\l\nog p(X )\n \n= \\s\num _n \\\nl og p(x_n | e_{< n}) = \\sum _{n \\in \\mathcal {I}_L} \\log p(x_n^L | e_{< n}) + \\sum _{n \\in \\mathcal {I}_V} \\log p(x_n^V | e_{< n}), \\end {aligned} \\label {equ:formu} \n(1)\nwhere IL and IV represent the index sets for text tokens and images, respectively.\n< n in the subscript represents the abbreviation of {1, 2, . . . , n−1}. The following\nparagraphs provide explanations of Eq. (1).\n6\nC. Tian et al.\nCrong Pororo\nPoby and \nEddy are  ...\nLoopy invites \nher friends …\nWord\nEmbedding\nImage\nEncoder\nFFN\nFeature Synchronizer (MMFS)\nSelf-Attention\nWord\nEmbedding\nImage\nEncoder\nImage\nEncoder\n× N\nLarge Language Model\nDownsampling Block\n…\nUpsampling Block\nFeature Synchronizer (MMFS)\nN × \nN × \nImage Decoder\nNext-Image-Prediction Loss\nNext-Token-Prediction Loss\nMulti-Image\nMulti-Scale\nFeature Maps\nText Token Classifier\nConditions for\nImage Generation\nDenoised\nImage\nNoised\nImage\nInput Token\nEmbeddings\nOutput Token\nEmbeddings\nInterleaved\nImage-Text\nSequence\nFig. 4: Architecture of MM-Interleaved. The red lines represent how the multi-\nscale image features are generated and utilized. MM-Interleaved incorporates image\nencoder to both extract high-resolution multi-scale image features (red lines) and map\neach image into a fixed number of low resolution visual tokens. These visual tokens are\nfed into the multi-modal LLM along with text tokens. LLM then uses our proposed\nfeature synchronizer (MMFS) to extract additional high-resolution image details, and\nauto-regressively generate text tokens. After that, a diffusion-based image decoder\ngenerates next image conditioned on the previous context features from LLM, where\nMMFS is also utilized to capture accurate visual conditions.\nText Generation with Multi-modal Condition. log p(xL\nn|e<n) is similar to\ntraditional causal language modeling, except that the condition also includes\nprevious images. Recent works [47, 54, 91, 92] have demonstrated the effective-\nness of using LLMs for processing additional visual inputs. They apply a linear\nclassifier on top of the LLMs. The loss function for text generation is\n  \\smal\nl \\beg i n {a\nl i gne d} L_\\t\ne\nx t  {NTP}(x_\nn\n^L | e_{< n}) = - \\bar {x}_n^L \\cdot \\log \\mathrm {softmax} \\big (W \\cdot \\mathcal {D}_\\text {LLM}(e_{< n}) \\big ), \\end {aligned} \\label {equ:loss_text} \n(2)\nwhere W is the linear classification weight, DLLM denotes the LLM network\n(e.g., LLaMA [86]), ¯xL\nn is the one-hot vector indicating the ground-truth text.\nImage Generation with Multi-modal Condition. Maximizing log p(xV\nn |e<n)\naligns with the denoising-diffusion process [55], which recently achieves widespread\nsuccess in image generation. Maximizing the log-likelihood is derived as mini-\nmizing the diffusion modeling loss as\n  \\smal\nl  \\beg i n {a lig n ed}\n \nL_\n\\tex t {NIP}(x_n\n^\nV | e_{< n}) = \\mathbb {E}_{\\epsilon ,t}~||\\epsilon - \\mathcal {D}_\\text {DM}\\big (x_{n,t}^V, t, \\mathcal {D}_\\text {LLM}(e_{< n})\\big )||^2, \\end {aligned} \\label {equ:loss_img} \n(3)\nwhere DDM is the diffusion model for image denoisiong. xV\nn,t is the noisy version\nof the original image at the denoising step t, and the denoising network DDM\nis trained to predict the noise ϵ. We found that such modeling is also applicable\nwhen conditional input is interleaved image-text data.\nSuch modeling allows for the flexible combination of different language models,\nimage encoders, and image decoders to fully leverage a variety of pre-trained\nmodels. The entire framework can be optimized end-to-end.\nMM-Interleaved\n7\n3.2\nArchitecture\nBuilding upon the task formulation in Sec. 3.1, we propose a novel multi-modal\narchitecture for processing interleaved image-text data. It integrates a Visual\nFoundation Model (VFM), a Large Language Model (LLM), and a Diffusion\nModel (DM). Such integration aims to excel in both comprehension and gener-\nation tasks of text and images by leveraging the strengths of each model type.\nAs illustrated in Fig. 4, our architecture comprises three key components:\n(1) VFM-based Image Tokenizer EV that maps each image xV ∈RH×W ×3\n(e.g., H = W = 224) into a fixed number of visual tokens eV ∈RN×C (N = 32\nby default). C is the channel dimension. It consists of a pre-trained vision en-\ncoder (e.g., CLIP-ViT [71]) for feature extraction and a Perceiver Resampler [3]\nto reduce the number of visual tokens. A ViT-Adapter [12] is also used to extract\nmulti-scale image features F V ∈R(PL\ni=1 Hi×Wi)×C for fine-grained feature fusion\nin subsequent networks, where L = 3 and Hi =\nH\n2i+2 , Wi =\nW\n2i+2 by default.\n(2) LLM-based Multi-modal Model DLLM that extracts context fea-\ntures from the interleaved image-text sequences. A pre-trained LLM (e.g., Vi-\ncuna [107]) is utilized. Its input sequence E ∈RK×C is a concatenation of\nembeddings (e1, e2, . . . ), where en is either a word embedding eL\nn ∈R1×C or an\nimage embedding eV\nn ∈RN×C. K is the total number of input tokens. We also\nintroduce a feature synchronizer (i.e., MMFS) for letting the intermediate layers\nin DLLM directly access and extract multi-scale image features on demand.\n(3) DM-based Image Decoder DDM that generates the image based on\nprevious image-text sequences. A pre-trained diffusion model (e.g., Stable Dif-\nfusion [74]) is utilized. To provide the conditional inputs for DDM, Resampler [3]\nis employed to map the output features from LLM to a fixed number of con-\nditional tokens. The fine-grained feature synchronizer module is also used here\nfor providing detailed visual conditions, which is very useful for tasks requiring\nvisual alignment (e.g., image translation).\nThen we introduce the details of the proposed architecture.\nMulti-Modal Feature Synchronizer (MMFS). MMFS aims to enable dy-\nnamic and efficient image detail extraction in decoder intermediate layers, com-\npensating for information loss due to limited input visual tokens. It leverages\nthe Deformable Attention [112] to achieve efficient and sparse image attention.\nMMFS can be applied to both image and text decoding, avoiding information\nbottlenecks caused by the Resamplers in multi-modal LLMs. It is especially ef-\nficient for processing multiple high-resolution images in context.\nAs is shown in Fig. 5, given a query token that requires detailed image fea-\ntures, the MMFS module only attends to a small set of sampling points around\na reference point on the reference images. Let fq ∈RC represents the features of\nthe query token. ˆpq ∈[0, 1]2 denotes the relative image coordinate of its reference\npoint, which is designed for compressing the search space for sampling points\nfrom global positions to local relative offsets. By default, without a spatial prior,\nˆpq = (0.5, 0.5) is set as the center of the images. {F V\nm}M\nm=1 are the multi-image\nmulti-scale feature maps extracted by the image tokenizer, M is the number of\n8\nC. Tian et al.\nAttention Weights\n𝑝ො𝑞\n𝑓𝑞\n𝑓𝑜\n𝐴𝑞\n𝐹1\n𝑉\n𝐹𝑀\n𝑉\n…….\nImage\nFeature Maps\nReference\nPoint\nQuery\nFeature\nLinear\nImage Index\nEmbedding\n…….\nOutput\n…….\nWeighted\nSum\nLinear\nLinear\nSoftmax\n𝑊𝑞\n𝑊𝑝\n𝑊𝐴\nSampled Spatial Feature\nFig. 5: Architecture of MMFS module. The query feature is passed through a\nlinear projection and added with the image index embedding. Two linear projections\nare used to predict the sampling offsets and unnormalized attention weights of each\nimage, respectively. The sampling offsets are added with the query’s reference point to\nform the corresponding sampling locations, which are shared across all feature maps\nof the same image. The output is the weighted sum of the sampled spatial features.\nreference images. The output feature fo ∈RC is given by\n  \\f o ot n ot e size \\begin \n{ali\ng\nn ed }  q^{ ( m)} \n&= W\n_\n{ q}  \\cdot\n f _ q + \\text {\nP o s E m b ed}(\nm\n),\n \\ \\  p_q^{(\nm\n)} &= W_{p}\n \\ c d o t  q^{\n(\nm\n)\n}\n +  \\hat {p}_q, \\  \\ \nA_q^ {(m )} = W_{A} \\cdot q^{(m)}, \\\\ p_q &= \\mathrm {Concat} (p_q^{(1)}, \\cdots , p_q^{(M)}), \\\\ A_q &= \\mathrm {softmax} \\left (\\mathrm {Concat}(A_q^{(1)}, \\cdots , A_q^{(M)})\\right ), \\\\ f_o &= \\mathrm {DeformAttn}(\\{F^V\\}_{m=1}^M, A_q, p_q), \\end {aligned} \n(4)\nwhere PosEmbed ∈R ¯\nM×C is a learnable positional embedding, m indexes from\nthe maximum of ¯\nM reference images. Wq, Wp, WA are learnable linear projection\nweights. The coordinate of sampling points p(m)\nq\nand the corresponding attention\nweights A(m)\nq\nare first calculated for each image separately. Then, the attention\nweights are normalized among different images via the softmax function. The\nDeformAttn operator extracts feature at coordinates pq ∈RM×L×K×2 from the\ncorresponding feature map, and performs a weighted summation according to\nAq ∈RM×L×K. Here, L and K denote the number of multi-scale feature levels\nand sampling points per feature map, respectively.\nMulti-modal LLM with MMFS. The input of multi-modal LLMs is an in-\nterleaved sequence of image and text token embeddings, which always starts\nwith a special token <s> and ends with another special token <\/s>. Image token\nembeddings are inserted at the corresponding positions in the original sequence.\nSpecial token <BoI> is added in front of each image to represent “Begin of Image”.\nMMFS modules are inserted between the self-attention layer and the feed-\nforward layer of the LLM every fixed number of blocks. Query token fq iterates\nover each token in the LLM, which can only access the previous images. ˆpq =\n(0.5, 0.5). The output of MMFS is multiplied by tanh(α) before added back to\nfq, where α is a zero-initialized learnable scalar.\nImage Decoder with MMFS. The output features from the multi-modal LLM\nare further processed by another Resampler before fed into the image decoder.\nMM-Interleaved\n9\nFor each image to be generated, the Resampler maps previous context features\nto a fixed number of tokens (e.g., 77 tokens) to match the condition token length\nof the pre-trained diffusion model.\nMMFS modules are inserted after each downsampling block in the U-Net of\nthe diffusion model. Query token fq iterates over each pixel in the feature map.\nˆpq is set as the spatial coordinate of the query pixel. The output of MMFS is\nfurther processed by a zero-initialized convolution before added back to fq.\nTraining Target and Inference Pipeline. The training objective is defined\nas the sum of Next-Text-Token Prediction loss in Eq. (2) and Next-Image Pre-\ndiction loss in Eq. (3) as L = LNT P + λ LNIP , where λ is a hyperparameter\nused to determine the relative loss weight between the image and text decoding\nbranches. The whole framework can be optimized end-to-end.\nDuring inference, the images and texts are generated in an auto-regressive\nmanner. Text tokens are sampled from the distribution predicted by the multi-\nmodal LLM. When the generated token is <BoI>, the diffusion model is called\nfor generating the next image.\n4\nExperiment\n4.1\nImplementation Details\nNetwork. We adopt CLIP-ViT-L\/14 [71], Vicuna-13B [107] and Stable Diffu-\nsion v2.1 [74] as the image encoder, large language model, and image decoder,\nrespectively. For the multi-modal LLM, a Perceiver Resampler with 12 blocks is\nused to reduce the number of visual tokens per image to 64. MMFS is inserted\nevery 4 blocks in the LLM. For the image decoder, a Perceiver Resampler with\nonly 1 block is used to reduce the number of previous conditional tokens to 77.\nMMFS is inserted after each downsampling block in the image decoder.\nPre-training. Our model is pre-trained on a mixture of image-text pairs and in-\nterleaved image-text sequences, including MMC4 [111], LAION-2B [77], LAION-\nCOCO [78], CC-12M [8] and Objects365 [79]. For CC-12M [8] and Objects365 [79],\ninstead of utilizing the original annotations, we use the pre-trained BLIP-2\nmodel [47] to caption the images. The sampling probability of MMC4 is twice\nthat of other image-text pair datasets. No in-house data is used.\nThe images are inserted before or after the corresponding text sentence with\nequal probability. To optimize training efficiency and data utility, multiple image-\ntext pairs or interleaved image-text sequences are concatenated into extended\nsequences with the maximum context length (i.e., 2048 tokens).\nThe model is pre-trained for 15000 steps with 11 billion tokens visited. The\nimage encoder and LLM are frozen. The learning rate is set to be 10−5 for the\nimage decoder and 10−4 for the rest trainable parameters. The input and output\nimage resolutions are 224 × 224 and 512 × 512, respectively.\nSupervised Fine-tuning. After pre-training, like other multi-modal LLMs,\nour model capabilities can be further enhanced using supervised fine-tuning.\nThe model is end-to-end fine-tuned on four types of downstream tasks: 1) visual\n10\nC. Tian et al.\nModel\nLLM\nH I A COCO Flickr NoCaps I2Para. VQAv2 OKVQA GQA VizWiz TextVQA VisDial\nModels for Text-Generation Only\nMetaLM [30]\nMetaLM\n- - -\n82.2\n43.3\n58.7\n–\n41.1\n11.4\n–\n41.1\n11.4\nOF-9B [5]\nMPT-7B\n- - -\n79.5\n59.5\n–\n–\n52.7\n37.8\n–\n27.5\n24.2\n–\nIDEFICS-80B [36]\nLLaMA-65B\n- - -\n91.8\n53.7\n65.0\n60.0\n–\n45.2\n36.0\n30.9\n–\nKOSMOS-1 [33]\nMetaLM\nH - -\n–\n65.2\n–\n–\n46.7\n–\n–\n–\n–\n–\nKOSMOS-2 [68]\nKOSMOS-1\nH - -\n–\n66.7\n–\n–\n45.6\n–\n–\n–\n–\n–\nFlamingo-9B [3]\nChinchilla-7B\nH - -\n79.4\n61.5\n–\n–\n51.8\n44.7\n–\n28.8\n31.8\n48.0\nFlamingo-80B [3]\nChinchilla-70B\nH - -\n84.3\n67.2\n–\n–\n56.3\n50.6\n–\n31.6\n35.0\n52.0\nIDEFICS-80B-I [36]\nLLaMA-65B\n- I -\n117.2\n65.3\n104.5\n37.4\n–\n–\n26.0\n–\nmPLUG-DocOwl [97] LLaMA-7B\n- I A\n52.6\n62.2\n57.4\n–\n–\n–\n–\n–\n–\n–\nBLIP-2 [47]\nVicuna-7B\n- I A\n–\n74.9\n107.5\n–\n–\n–\n38.6\n25.3\n40.1\n–\nBLIP-2 [47]\nVicuna-13B\n- I A\n–\n71.6\n103.9\n–\n41.0\n–\n41.0\n19.6\n42.5\n–\nInstructBLIP [14]\nVicuna-7B\n- I A\n–\n82.4\n123.1\n–\n–\n–\n49.2\n34.5\n50.1\n–\nInstructBLIP [14]\nVicuna-13B\n- I A\n–\n82.8\n121.9\n–\n–\n–\n49.5\n33.4\n50.7\n–\nShikra [10]\nVicuna-13B\n- I A\n117.5\n73.9\n–\n–\n77.4\n–\n–\n–\n–\n–\nLLaVA-1.5 [53]\nVicuna-7B\n- I A\n–\n–\n–\n–\n78.5\n–\n62.0\n50.0\n58.2\n–\nLLaVA-1.5 [53]\nVicuna-13B\n- I A\n–\n–\n–\n–\n80.0\n–\n63.3\n53.6\n61.3\n–\nQwen-VL [6]\nQwen-7B\nH I A\n–\n85.8\n121.4\n–\n78.8\n–\n59.3\n35.2\n63.8\n–\nQwen-VL-Chat [6]\nQwen-7B\nH I A\n–\n81.0\n120.2\n–\n78.2\n–\n57.5\n38.9\n61.5\n–\nModels for both Image and Text Generation\nCM3Leon [101]\n–\nH - -\n61.6\n–\n–\n10.5\n47.6\n23.8\n–\n37.6\n–\n22.6\nEmu [84]\nVicuna-13B\nH - -\n112.4\n–\n–\n–\n52.0\n38.2\n–\n34.2\n–\n47.4\nEmu-I [84]\nVicuna-13B\nH - -\n117.7\n–\n–\n–\n40.0\n34.7\n–\n35.4\n–\n48.0\nEmu2 [82]\nLLaMA-33B\nH - -\n–\n–\n–\n–\n33.3\n26.7\n–\n40.4\n26.2\n–\nDreamLLM [17]\nVicuna-7B\n- I -\n115.4\n–\n–\n17.4\n56.6\n44.3\n–\n38.1\n34.9\n–\nVL-GPT [110]\nLLaMA-7B\n- - -\n116.4\n–\n–\n–\n51.7\n35.8\n34.6\n34.7\n–\n49.9\nVL-GPT-I [110]\nLLaMA-7B\n- I A\n133.7\n–\n–\n–\n67.2\n50.3\n51.5\n38.9\n–\n51.8\nSEED-LLaMA [25]\nLLaMA2-Chat-13B - I A\n125.0\n–\n–\n–\n48.1\n27.1\n–\n23.3\n–\n–\nSEED-LLaMA-I [25]\nLLaMA2-Chat-13B - I A\n126.9\n–\n–\n–\n63.4\n43.2\n–\n49.4\n–\n–\nMM-Interleaved\nVicuna-13B\n- - -\n129.0\n85.8\n106.4\n23.5\n57.0\n40.0\n46.7\n40.8\n37.2\n48.7\nMM-Interleaved-SFT Vicuna-13B\n- I A 140.5\n93.0\n123.2\n30.3\n80.2\n51.7\n60.5\n54.9\n61.0\n53.7\nTable 1: Multi-modal comprehension evaluation. “H” denotes using in-house\ndata, “I” means the training images of some benchmarks are included in the train-\ning, “A” means the training annotations of some benchmarks are visible in train-\ning. Benchmarks include COCO [11]; Flickr: Flickr30k [69]; NoCaps [2]; I2Para.:\nImage2Paragraph [44]; VQAv2: VQAv2 [27]; OKVQA [60]; GQA [35]; VizWiz [29];\nTextVQA [81]; VisDial [15]. The results in underline and bold are the best perfor-\nmance without “HIA” data and without “H” data, respectively. The evaluation metrics\nfor each benchmark are listed in Appendix.\nquestion-answering and image caption, 2) visual storytelling, 3) segmentation-\nto-image translation, and 4) referring expression comprehension.\nMore implementation details could be found in Appendix.\n4.2\nEvaluation\nWe evaluate the zero-shot capability of the pre-trained MM-Interleaved as well\nas its performance on various downstream tasks after supervised fine-tuning.\nZero-shot Results after Pre-training. Tab. 1 demonstrates our strong per-\nformance on multi-modal zero-shot comprehension across various benchmarks,\nincluding image captioning (COCO [11], Flicker30K [69], NoCaps [2], I2Para [44]),\nvisual question answering (VQAv2 [27], OKVQA [60], VizWiz [29], TextVQA [81])\nand visual dialogue (VisDial [15]). In the fully decontaminated setting where the\nimage and text in downstream tasks are unseen during pre-training, our model\nsignificantly outperforms other methods on all tasks. This demonstrates the effec-\ntiveness of the proposed MM-Interleaved approach. Furthermore, our model even\nexceeds most trained on vast in-house data like Flamingo-9B [3] and Emu [84],\nshowing the importance of proper architecture for image-text interaction.\nMM-Interleaved\n11\nModel\nLLM\nL1 (Part-1) Part-2 L2\nPart-3 L3\nEmu [84]\nLLaMA-13B\n42.5\n41.1\n42.4\n41.4\n42.3\nNext-GPT [95]\nVicuna-7B\n30.7\n35.6\n31.1\n33.9\n31.4\nSEED-LLaMA [25] LLaMA2-Chat-13B\n43.9\n43.4\n43.8 52.3 44.8\nMM-Interleaved\nVicuna-13B\n43.9\n46.1 44.1 52.1 45.0\nTable 2: Zero-shot results for interleaved image-text comprehension and\ngeneration on SEED-Bench-2 [46]. The average task accuracy across correspond-\ning evaluation dimensions is reported. L1(part-1) evaluates the image and text compre-\nhension, L2(part-1&2) evaluates interleaved image-text comprehension, and L3(part-\n1&2&3) evaluates image and text generation.\nModel\nMS-COCO LN-COCO\nText-to-Image Specialists\nRetrieval Result\n17.97\n33.59\nDALL-E [73]\n∼28\n-\nCogView2 [16]\n24.00\n-\nStable Diffusion [74]\n12.43\n34.26\nGLIDE [64]\n12.24\n-\nMake-A-Scene [24]\n11.84\n-\nDALL-E 2 [72]\n10.39\n-\nMuse-3B [96]\n7.88\n-\nImagen-3.4B [76]\n7.27\n-\nParti-20B [100]\n7.23\n15.97\nModels for both Image and Text Generation\nCM3-13B [1]\n29.56\n-\nVL-GPT [110]\n12.25\n-\nGILL [43]\n12.20\n-\nEmu-13B [84]\n11.66\n-\nNext-GPT [95]\n11.28\n-\nCM3Leon-7B [101]\n10.82\n-\nDreamLLM-7B-Stage1 [17]\n8.76\n22.42\nDreamLLM-7B [17]\n8.46\n20.53\nMM-Interleaved\n7.90\n23.88\nTable 3: Zero-shot text-to-image generation results. FID [31] is reported.\nTab. 3 shows the results on text-to-image generation for MS-COCO [52] and\nLN-COCO [70]. On MS-COCO, we sample 8 images per text condition and\nuse CLIP [71] to rerank based on text-image similarity. CLIP reranking is not\nused for LN-COCO. Our model achieves competitive performance compared to\nexisting image and text generation models. Note that some other works (e.g.,\nEmu, Muse, Imagen, and Parti) use in-house data, while ours did not.\nTab. 2 shows the zero-shot performance for interleaved image-text compre-\nhension and generation on SEED-Bench-2 [46]. Our pre-trained model achieves\nSOTA results on both comprehension and generation tasks. Visualization results\nfor interleaved image-text generation are showed in Appendix.\nFine-tuning Results. Tab. 1 also shows our fine-tuned results (MM-Interleaved-\nSFT) on multi-modal comprehension. Our fine-tuned model achieves SOTA per-\nformance without using any in-house data. On visual question answering tasks,\nit matches the previous best LLaVA-1.5 model. Compared to LLaVA-1.5, our\nmodel has two key advantages: 1) our method is capable of generating both\nimages and text, while LLaVA-1.5 can only generate text; 2) LLaVA-1.5 uses\n576 visual tokens as LLM inputs, whereas we only require 64 tokens. Our model\nachieves competitive image understanding with far fewer visual tokens, making\nit better suited for multi-image scenarios.\n12\nC. Tian et al.\nModel\nRefCOCO [42]\nRefCOCO+ [59]\nRefCOCOg [59]\nVal\nTest-A Test-B\nVal\nTest-A Test-B\nVal\nTest\nOFA-L [89]\n79.96\n83.67\n76.39\n68.29\n76.00\n61.75\n67.57\n67.50\nVisionLLM-H [92]\n-\n86.70\n-\n-\n-\n-\n-\n-\nShikra [10]\n87.01\n90.61\n80.24\n81.60\n87.36\n72.12\n82.27\n82.19\nMiniGPT-V2 [9]\n88.69\n91.65\n85.33\n79.97\n85.12\n74.45\n84.44\n84.66\nFerret [98]\n89.48 92.41 84.36 82.81 88.14 75.17 85.83\n86.34\n* Qwen-VL [6]\n89.36 92.26 85.34 83.12 88.25 77.21 85.58\n85.48\nMM-Interleaved\n89.92 92.59 86.54 82.99 88.57 77.07 85.21\n84.92\nTable 4: Supervised fine-tuning results on referring expression comprehen-\nsion task. * denotes using an additional self-constructed grounding dataset and trained\nwith an image resolution larger than 224.\nGroundtruth\nVQGAN [19]\nLDM [75]\n0.58\n0.21\n0.31\nPIPT [90]\nControlNet [102]\nOurs\n0.26\n0.35\n0.44\n(a) Segmentation-to-image gener-\nation on ADE20K [108]. mIoU is\nreported.\nModel\nCLIP Sim.↑FID↓\nGILL [43]\n0.64\n-\nMiniGPT-5 [106]\n0.70\n59.5\nMM-Interleaved\n0.70\n39.7\n(b) Last image generation with\ninterleaved context for visual\nstorytelling on VIST [34].\nModel\nPororo Flintstones\nStoryDALL-E [58]\n25.9\n26.5\nAR-LDM [67]\n17.4\n19.3\nACM-VSG [22]\n15.4\n18.4\nMM-Interleaved\n14.7\n18.7\n(c) Multi-image generation with\ninterleaved\ncontext\nfor\nvisual\nstorytelling. FID is reported.\nTable 5: Segmentation-to-image generation and visual storytelling results.\nTab. 4 shows the results on referring expression comprehension (REC) bench-\nmarks. Our method outperforms other methods. Even though we only use public\nREC data [42,59] for fine-tuning, MM-Interleaved matches Qwen-VL [6] which\nutilizes an extra 22M in-house grounding dataset and trains at higher resolution\n(448 vs. 224 pixels used by ours). This shows the effectiveness of synchronized\nfine-grained image features for enhancing the REC capability of our method.\nTab. 5a shows the performance on segmentation-to-image translation, which\nis a capability not shown in other multi-modal LLMs. We follow the protocol\nproposed in ControlNet [102] by generating images from ADE20K ground truth\nsemantic masks and use OneFormer [38] for segmenting the generated images.\nStandard mIoU metric is used for measuring the semantic and structural fidelity\nof the generated images. MM-Interleaved clearly outperforms other baselines in-\ncluding ControlNet by a large margin. Compared to ControlNet, MM-Interleaved\ncould leverage the better representations learned by our generative modeling\nfrom large-scale pre-training data, which benefits the image translation tasks.\nMoreover, thanks to MMFS, MM-Interleaved is capable of generating realistic\nimages with pixel-level precise alignment from a semantic label map, where sim-\nply relying on coarse Perceiver Resampler features fails on this task (see Tab. 6c).\nTabs. 5b and 5c show our results on visual storytelling, which is required to\ngenerate new images with interleaved image-text as context. We evaluate the\nperformance both for generating the last image on the VIST dataset [34] and\nfor generating multiple images auto-regressively on the Pororo [51] and Flint-\nstones [28] datasets. Our model achieves SOTA performance even compared with\nprevious specialist methods. Qualitative results can be found in Appendix.\nMM-Interleaved\n13\n# Token w\/ MMFS Caption↑Generation↓OK-VQA↑TextVQA↑\n32\n107.0\n32.2\n28.7\n22.5\n32\n✓\n110.6\n30.0\n29.8\n27.7\n256\n110.7\n32.7\n29.2\n23.6\n256\n✓\n110.9\n29.5\n29.6\n27.8\n(a) pre-training with 224 resolution\nw\/ MMFS Caption↑Generation↓OK-VQA↑TextVQA↑\n110.5\n30.3\n29.9\n24.9\n✓\n115.2\n30.5\n30.6\n30.8\n(b) fine-tuning with 448 resolution\nw\/ MMFS ADE20k↑\n5.3\n✓\n35.9\n(c) fine-tuning for\nimage translation\nTable 6: Ablation on using MMFS. “Generation” is text-to-image generation task.\n“ADE20k” is segmentation-to-image translation task. Others are text generation tasks.\n“# Token” indicates the number of input visual tokens for LLMs (32 by default).\nCross-Attn Transition Attn Input COCO Cap.↑COCO Gen.↓OK-VQA↑TextVQA↑\nDeformable\nNone\n16 × 16\n110.6\n30.0\n29.8\n27.7\nDense\nNone\n16 × 16\n108.5\n30.6\n28.4\n23.6\nDense\nResampler 32 tokens\n107.2\n30.7\n28.9\n24.0\nTable 7: Ablation on the design choice of MMFS. Different attention modules\ncan be used in MMFS. We also ablate whether to add additional transition layer before\nfeeding image features into MMFS. Dense cross-attention with Resampler transition\non single scale and single image is similar to the cross attention used in Flamingo [3].\n4.3\nAblation Study\nFor ablation study, we use CLIP-ViT-L\/14 [71] as the image encoder, OpenLLaMA-\n3B v2 [26] as the LLM and miniSD1 as the image decoder. The output image\nresolution is 256. The model is pre-trained for 10000 steps, using a mixture of\nLAION-COCO [78], LAION-2B [77], and MMC4 [111] datasets.\nFor simplicity, MMFS in this subsection is by default implemented in a single-\nimage and single-scale setting, where each token only attends to the 16 × 16\nfeature map of the nearest preceding image. We evaluate the zero-shot perfor-\nmance on three representative tasks and four datasets, i.e., image captioning on\nthe COCO Karpathy test set, text-to-image generation on the COCO Karpathy\ntest set, visual question answering on OKVQA [60], and TextVQA [81] validation\nset. The evaluation metrics are CIDEr, FID-5k, and top-1 accuracy, respectively.\nTo ablate the cross-attention module in image generation, we also evaluate the\nfine-tuned results for segmentation-to-image translation task on ADE20K [108].\nToken Efficiency. As shown in Tab. 6a, when equipped with MMFS, using\nonly 32 visual tokens per image can outperform using 256 visual tokens with-\nout MMFS. Such results demonstrate the effectiveness of our method when the\ncontext length is limited. As shown in Tab. 6b, the performance improvement\nof using MMFS becomes larger when increasing the input image resolution from\n224 to 448. Such results indicate our method could better exploit the additional\ninformation gained from high resolution even with only 32 visual tokens.\nMMFS for Image Generation. Tab. 6c demonstrates the criticality of adding\nMMFS for segmentation-to-image translation task. This task is hard as it re-\nquires precise pixel-level information to align the given segmentation condition\nand image output properly. Without using MMFS, this task fails, showing ex-\ntremely low mIoU results. Visualizations results in Appendix shows that the\ngenerated image without MMFS cannot preserve all spatial information, and\nthe spatial alignment for the generated results is poor.\n1 https:\/\/huggingface.co\/justinpinkney\/miniSD\n14\nC. Tian et al.\nmulti-scale multi-image Caption↑Generation↓OK-VQA↑TextVQA↑\n110.6\n30.0\n29.8\n27.7\n✓\n111.2\n29.5\n30.3\n28.1\n✓\n✓\n111.2\n29.9\n31.1\n28.2\nfinetuning with 448 input resolution\n115.2\n30.5\n30.6\n30.8\n✓\n115.4\n30.1\n31.0\n31.3\n✓\n✓\n115.8\n30.0\n31.7\n32.0\nTable 8: Ablation on multi-scale and multi-image usage of MMFS.\nFig. 6: Left: Few-shot results on OKVQA and TextVQA. Right: Additional\nGFLOPs over using 32 visual tokens with different numbers of image and\ntext inputs. Nv and k denote the number of visual-token per image and the number\nof few-shot examples, respectively. Ni and Nt denote the number of image and the\nnumber of subsequent text tokens for each image in the sequence. Note that Nv = 256\ndoes not support Ni ≥8 images due to 2048 LLM context length.\nComparison between Different Cross Attention Mechanisms. The ab-\nlation of adopting different cross-attention mechanisms for LLM is shown in\nTab. 7. The overall performance drops when directly replacing MMFS with the\nvanilla dense cross-attention, possibly due to its slower convergence speed. We\nhighlight that the model with Deformable attention performs significantly bet-\nter than other attention mechanisms on TextVQA, indicating that Deformable\nattention could effectively and efficiently capture fine-grained information like\ntext needed for the task, such as visual question answering in this case.\nMMFS with Multi-Image and Multi-Scale. As shown in Tab. 8, adding\nmulti-image and multi-scale for MMFS improves the performance. To better\ntake advantage of the multi-image mechanism, we further evaluate our model\nwith few-shot prompts following [3, 84]. As is illustrated in Fig. 6 (left), when\nthe number of context images increases, MMFS continuously outperforms only\nusing 32 visual tokens, and also benefits from further attending to the multi-scale\nfeature maps of multiple images.\nComputational Efficiency. Compared to only using 32 visual tokens, further\nintegrating MMFS into LLM only increases by about 2%, 2%, 6% and 3% in\nFLOPs, #parameter, runtime and memory, respectively. When compared to\nusing more visual tokens (i.e., 256) w\/o MMFS, using MMFS with 32 visual\ntokens achieve better performance (see Tab. 6a) and is much more efficient, with\n2.8× fewer FLOPs and 1.3× fewer runtime. Fig. 6 (right) shows the additional\nFLOPs over only using 32 visual tokens with synthesized interleaved image-text\nMM-Interleaved\n15\ninputs. Our method consistently achieves lower or similar computation than\nusing 256 visual tokens or using dense cross-attention.\n5\nConclusion\nThis paper introduces MM-Interleaved, an end-to-end trained generative model\nfor interleaved image-text comprehension and generation. It is built upon the\nMulti-Modal Feature Synchronizer (MMFS), which is proposed to enhance multi-\nmodal LLMs by reducing the required visual tokens, enabling efficient extraction\nof visual details. Without using in-house data, our method achieves SOTA per-\nformance on various multi-modal benchmarks. We hope this work can contribute\nto the research on multi-modal LLMs for interleaved image-text scenarios.\nLimitations. Both the quality and quantity of public interleaved image-text\ndata are relatively low, making it difficult to exploit the full potential of inter-\nleaved generative modeling.\nNegative impacts. As other multi-modal models, it may suffer from halluci-\nnation issues and potentially generate biased contents due to the noisy training\ndata. We hope to invest more efforts to improve the quality and quantity of in-\nterleaved image-text data, with the aim of further improvements while ensuring\nits safety and reliability.\nReferences\n1. Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko,\nD., Joshi, M., Ghosh, G., Lewis, M., et al.: Cm3: A causal masked multimodal\nmodel of the internet. arXiv preprint arXiv:2201.07520 (2022) 11\n2. Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D.,\nParikh, D., Lee, S., Anderson, P.: Nocaps: Novel object captioning at scale. In:\nICCV (2019) 10, 24, 26\n3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,\nMensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model\nfor few-shot learning. NeurIPS (2022) 2, 4, 5, 7, 10, 13, 14\n4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:\nVQA: visual question answering. In: ICCV (2015) 26\n5. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K.,\nBitton, Y., Gadre, S., Sagawa, S., et al.: Openflamingo: An open-source frame-\nwork for training large autoregressive vision-language models. arXiv preprint\narXiv:2308.01390 (2023) 10\n6. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,\nJ.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966 (2023) 5, 10, 12, 22\n7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-\nshot learners. NeurIPS (2020) 2\n8. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In: CVPR\n(2021) 9, 22\n16\nC. Tian et al.\n9. Chen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krishnamoorthi, R., Chan-\ndra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified\ninterface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478\n(2023) 12\n10. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-\ning multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195\n(2023) 5, 10, 12, 22\n11. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.:\nMicrosoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325 (2015) 10, 22, 24, 26\n12. Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., Qiao, Y.: Vision transformer\nadapter for dense predictions. In: ICLR (2022) 7\n13. Clark, C., Gardner, M.: Simple and effective multi-paragraph reading comprehen-\nsion. arXiv preprint arXiv:1710.10723 (2017) 22\n14. Dai, W., Li, J., Li, D., Huat, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.:\nInstructblip: Towards general-purpose vision-language models with instruction\ntuning. arXiv preprint arXiv:2305.06500 (2023) 10\n15. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D.,\nBatra, D.: Visual dialog. In: CVPR (2017) 10, 26\n16. Ding, M., Zheng, W., Hong, W., Tang, J.: Cogview2: Faster and better text-to-\nimage generation via hierarchical transformers. NeurIPS (2022) 11\n17. Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H.,\nWei, H., et al.: Dreamllm: Synergistic multimodal comprehension and creation.\narXiv preprint arXiv:2309.11499 (2023) 2, 4, 5, 10, 11, 22\n18. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\n16x16 words: Transformers for image recognition at scale. In: ICLR (2020) 5\n19. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution im-\nage synthesis. In: CVPR (2021) 12\n20. Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., Cao, Y.: Eva-02: A visual\nrepresentation for neon genesis. arXiv preprint arXiv:2303.11331 (2023) 1\n21. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X.,\nCao, Y.: Eva: Exploring the limits of masked visual representation learning at\nscale. arXiv preprint arXiv:2211.07636 (2022) 1\n22. Feng, Z., Ren, Y., Yu, X., Feng, X., Tang, D., Shi, S., Qin, B.: Improved visual\nstory generation with adaptive context modeling. arXiv preprint arXiv:2305.16811\n(2023) 12\n23. Gadre, S.Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten,\nR., Wortsman, M., Ghosh, D., Zhang, J., et al.: Datacomp: In search of the next\ngeneration of multimodal datasets. arXiv preprint arXiv:2304.14108 (2023) 4\n24. Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., Taigman, Y.: Make-a-\nscene: Scene-based text-to-image generation with human priors. In: ECCV (2022)\n11\n25. Ge, Y., Zhao, S., Zeng, Z., Ge, Y., Li, C., Wang, X., Shan, Y.: Making llama see\nand draw with seed tokenizer. arXiv preprint arXiv:2310.01218 (2023) 5, 10, 11\n26. Geng, X., Liu, H.: Openllama: An open reproduction of llama (May 2023), https:\n\/\/github.com\/openlm-research\/open_llama 13\n27. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering.\nIn: CVPR (2017) 10, 22, 26\nMM-Interleaved\n17\n28. Gupta, T., Schwenk, D., Farhadi, A., Hoiem, D., Kembhavi, A.: Imagine this!\nscripts to compositions to videos. In: ECCV (2018) 12, 26, 34, 35\n29. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham,\nJ.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:\nCVPR (2018) 10, 26\n30. Hao, Y., Song, H., Dong, L., Huang, S., Chi, Z., Wang, W., Ma, S., Wei, F.:\nLanguage models are general-purpose interfaces. arXiv preprint arXiv:2206.06336\n(2022) 10\n31. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.\nNeurIPS (2017) 11, 23, 26\n32. Ho,\nJ.,\nSalimans,\nT.:\nClassifier-free\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598 (2022) 22, 24\n33. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mo-\nhammed, O.K., Liu, Q., et al.: Language is not all you need: Aligning perception\nwith language models. arXiv preprint arXiv:2302.14045 (2023) 4, 10\n34. Huang, T.H.K., Ferraro, F., Mostafazadeh, N., Misra, I., Devlin, J., Agrawal, A.,\nGirshick, R., He, X., Kohli, P., Batra, D., et al.: Visual storytelling. In: NAACL\n(2016) 12, 23, 26\n35. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In: CVPR (2019) 10, 26\n36. IDEFICS: Introducing idefics: An open reproduction of state-of-the-art visual\nlanguage model. https:\/\/huggingface.co\/blog\/idefics (2023) 10\n37. Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R.,\nDave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A.,\nSchmidt, L.: Openclip (Jul 2021). https:\/\/doi.org\/10.5281\/zenodo.5143773,\nhttps:\/\/doi.org\/10.5281\/zenodo.5143773 1, 4\n38. Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One\ntransformer to rule universal image segmentation. In: CVPR (2023) 12\n39. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,\nLi, Z., Duerig, T.: Scaling up visual and vision-language representation learning\nwith noisy text supervision. In: ICML (2021) 4\n40. Jin, Y., Xu, K., Xu, K., Chen, L., Liao, C., Tan, J., Huang, Q., Chen, B., Lei, C.,\nLiu, A., Song, C., Lei, X., Zhang, D., Ou, W., Gai, K., Mu, Y.: Unified language-\nvision pretraining in llm with dynamic discrete visual tokenization. arXiv preprint\narXiv:2309.04669 (2023) 2\n41. Kafle, K., Price, B., Cohen, S., Kanan, C.: Dvqa: Understanding data visualiza-\ntions via question answering. In: CVPR (2018) 22\n42. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to\nobjects in photographs of natural scenes. In: EMNLP (2014) 12, 22, 26\n43. Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal lan-\nguage models. arXiv preprint arXiv:2305.17216 (2023) 2, 4, 11, 12, 23\n44. Krause, J., Johnson, J., Krishna, R., Fei-Fei, L.: A hierarchical approach for gen-\nerating descriptive image paragraphs. In: CVPR (2017) 10, 24, 26\n45. Laurençon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A.,\nWang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelics: An open web-\nscale filtered dataset of interleaved image-text documents. In: Thirty-seventh\nConference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (2023) 4\n18\nC. Tian et al.\n46. Li, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., Shan, Y.: Seed-\nbench-2: Benchmarking multimodal large language models. arXiv preprint\narXiv:2311.17092 (2023) 11\n47. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 (2023) 2, 4, 5, 6, 9, 10, 22\n48. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In: ICML (2022) 22,\n23\n49. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before\nfuse: Vision and language representation learning with momentum distillation.\nNeurIPS (2021) 4\n50. Li, Y., Fan, H., Hu, R., Feichtenhofer, C., He, K.: Scaling language-image pre-\ntraining via masking. In: CVPR (2023) 4\n51. Li, Y., Gan, Z., Shen, Y., Liu, J., Cheng, Y., Wu, Y., Carin, L., Carlson, D.,\nGao, J.: Storygan: A sequential conditional gan for story visualization. In: CVPR\n(2019) 12, 23, 26, 34, 35\n52. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 11,\n26\n53. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.\narXiv preprint arXiv:2310.03744 (2023) 10, 22\n54. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint\narXiv:2304.08485 (2023) 2, 4, 5, 6\n55. Luo, C.: Understanding diffusion models: A unified perspective. arXiv preprint\narXiv:2208.11970 (2022) 6\n56. Lv, T., Huang, Y., Chen, J., Cui, L., Ma, S., Chang, Y., Huang, S., Wang, W.,\nDong, L., Luo, W., et al.: Kosmos-2.5: A multimodal literate model. arXiv preprint\narXiv:2309.11419 (2023) 2, 5\n57. Maharana, A., Bansal, M.: Integrating visuospatial, linguistic and commonsense\nstructure into story visualization. arXiv preprint arXiv:2110.10834 (2021) 23\n58. Maharana, A., Hannan, D., Bansal, M.: Storydall-e: Adapting pretrained text-to-\nimage transformers for story continuation. In: ECCV (2022) 12\n59. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Gener-\nation and comprehension of unambiguous object descriptions. In: CVPR (2016)\n12, 22, 26, 32\n60. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In: CVPR (2019) 10, 13, 26\n61. Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for\nquestion answering about charts with visual and logical reasoning. arXiv preprint\narXiv:2203.10244 (2022) 22\n62. Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., Jawahar, C.: Info-\ngraphicvqa. In: WACV (2022) 22\n63. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question\nanswering by reading text in images. In: ICDAR (2019) 22\n64. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and edit-\ning with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n11\n65. OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 2, 4\nMM-Interleaved\n19\n66. OpenAI, T.: Chatgpt: Optimizing language models for dialogue. OpenAI (2022)\n4\n67. Pan, X., Qin, P., Li, Y., Xue, H., Chen, W.: Synthesizing coherent story with\nauto-regressive latent diffusion models. arXiv preprint arXiv:2211.10950 (2022)\n12, 23\n68. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-\n2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824 (2023) 10\n69. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J.,\nLazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for\nricher image-to-sentence models. In: ICCV (2015) 10, 24, 26\n70. Pont-Tuset, J., Uijlings, J., Changpinyo, S., Soricut, R., Ferrari, V.: Connecting\nvision and language with localized narratives. In: ECCV (2020) 11, 26\n71. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. In: ICML (2021) 1, 4, 7, 9, 11, 13, 23, 26\n72. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n(2022) 11\n73. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\nSutskever, I.: Zero-shot text-to-image generation. In: ICML (2021) 2, 4, 11\n74. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022) 2, 4, 7, 9, 11\n75. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022) 12\n76. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. NeurIPS (2022) 11\n77. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti,\nM., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An\nopen large-scale dataset for training next generation image-text models. NeurIPS\n(2022) 4, 9, 13, 22\n78. Schuhmann, C., Köpf, A., Vencu, R., Coombes, T., Beaumont, R.: Laion coco:\n600m synthetic captions from laion2b-en. https:\/\/laion.ai\/blog\/laion-coco\/ (2022)\n9, 13, 22\n79. Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., Sun, J.: Objects365:\nA large-scale, high-quality dataset for object detection. In: ICCV (2019) 9, 22\n80. Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: Textcaps: a dataset for image\ncaptioning with reading comprehension. In: ECCV (2020) 22\n81. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,\nRohrbach, M.: Towards vqa models that can read. In: CVPR (2019) 10, 13, 26,\n32\n82. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu,\nJ., Huang, T., et al.: Generative multimodal models are in-context learners. arXiv\npreprint arXiv:2312.13286 (2023) 5, 10\n83. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training tech-\nniques for clip at scale. arXiv preprint arXiv:2303.15389 (2023) 1, 4\n84. Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J.,\nHuang, T., Wang, X.: Generative pretraining in multimodality. arXiv preprint\narXiv:2307.05222 (2023) 2, 4, 10, 11, 14, 22\n20\nC. Tian et al.\n85. Tian, H., Zeng, C., Ren, Z., Chai, D., Zhang, J., Chen, K., Yang, Q.: Sphinx:\nEnabling privacy-preserving online learning over the cloud. In: 2022 IEEE Sym-\nposium on Security and Privacy (SP) (2022) 2, 5\n86. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023) 6\n87. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning.\nNeurIPS (2017) 5\n88. Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image descrip-\ntion evaluation. In: CVPR (2015) 26\n89. Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,\nJ., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In: ICML (2022) 12\n90. Wang, T., Zhang, T., Zhang, B., Ouyang, H., Chen, D., Chen, Q., Wen,\nF.: Pretraining is all you need for image-to-image translation. arXiv preprint\narXiv:2205.12952 (2022) 12\n91. Wang, W., Shi, M., Li, Q., Wang, W., Huang, Z., Xing, L., Chen, Z., Li, H., Zhu,\nX., Cao, Z., et al.: The all-seeing project: Towards panoptic visual recognition\nand understanding of the open world. arXiv preprint arXiv:2308.01907 (2023) 4,\n6\n92. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou,\nJ., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder\nfor vision-centric tasks. arXiv preprint arXiv:2305.11175 (2023) 4, 6, 12\n93. Wang, X., Liu, Y., Shen, C., Ng, C.C., Luo, C., Jin, L., Chan, C.S., Hengel,\nA.v.d., Wang, L.: On the general value of evidence, and bilingual scene-text visual\nquestion answering. In: CVPR (2020) 22\n94. Wang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov, Y., Cao, Y.: Simvlm: Sim-\nple visual language model pretraining with weak supervision. arXiv preprint\narXiv:2108.10904 (2021) 4\n95. Wu, S., Fei, H., Qu, L., Ji, W., Chua, T.S.: Next-gpt: Any-to-any multimodal llm.\narXiv preprint arXiv:2309.05519 (2023) 11\n96. Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., Abrego, G.H.,\nYuan, S., Tar, C., Sung, Y.H., et al.: Multilingual universal sentence encoder for\nsemantic retrieval. In: Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations (2020) 11\n97. Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian,\nJ., Qi, Q., Zhang, J., Huang, F.: mplug-docowl: Modularized multimodal large\nlanguage model for document understanding (2023) 10\n98. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,\nYang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv\npreprint arXiv:2310.07704 (2023) 12\n99. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.:\nCoca: Contrastive captioners are image-text foundation models. arXiv preprint\narXiv:2205.01917 (2022) 4\n100. Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,\nA., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789 (2022) 11\n101. Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A.,\nTang, B., Karrer, B., Sheynin, S., et al.: Scaling autoregressive multi-modal mod-\nels: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591 (2023)\n2, 4, 5, 10, 11\nMM-Interleaved\n21\n102. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: ICCV (2023) 12, 23\n103. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S.,\nZhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large\nmodel for advanced text-image comprehension and composition. arXiv preprint\narXiv:2309.15112 (2023) 4\n104. Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., Sun, T.: Llavar: En-\nhanced visual instruction tuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107 (2023) 22\n105. Zhao, H., Cai, Z., Si, S., Ma, X., An, K., Chen, L., Liu, Z., Wang, S., Han,\nW., Chang, B.: Mmicl: Empowering vision-language model with multi-modal in-\ncontext learning. arXiv preprint arXiv:2309.07915 (2023) 4\n106. Zheng, K., He, X., Wang, X.E.: Minigpt-5: Interleaved vision-and-language gen-\neration via generative vokens. arXiv preprint arXiv:2310.02239 (2023) 12, 23\n107. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li,\nZ., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge\nwith mt-bench and chatbot arena (2023) 7, 9\n108. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing\nthrough ade20k dataset. In: CVPR (2017) 12, 13, 26, 33\n109. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023) 5\n110. Zhu, J., Ding, X., Ge, Y., Ge, Y., Zhao, S., Zhao, H., Wang, X., Shan, Y.: Vl-gpt:\nA generative pre-trained transformer for vision and language understanding and\ngeneration. arXiv preprint arXiv:2312.09251 (2023) 5, 10, 11\n111. Zhu, W., Hessel, J., Awadalla, A., Gadre, S.Y., Dodge, J., Fang, A., Yu, Y.,\nSchmidt, L., Wang, W.Y., Choi, Y.: Multimodal c4: An open, billion-scale corpus\nof images interleaved with text. arXiv preprint arXiv:2304.06939 (2023) 4, 9, 13,\n22\n112. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\ntransformers for end-to-end object detection. In: ICLR (2020) 3, 7\n113. Zhu, X., Zhu, J., Li, H., Wu, X., Wang, X., Li, H., Wang, X., Dai, J.: Uni-perceiver:\nPre-training unified architecture for generic perception for zero-shot and few-shot\ntasks. arXiv preprint arXiv:2112.01522 (2021) 4\n22\nC. Tian et al.\nA\nImplementation Details\nA.1\nPre-training\nDataset Details. We use MMC4 [111], LAION-2B, LAION-COCO [78], CC-\n12M [8] and Objects365 [79] as the pre-training dataset. LAION-2B is the English\nsubset of LAION-5B [77], and we further filter it based on additional metrics\nincluding aesthetics scores. LAION-COCO [78] is a 600M subset of LAION-2B,\nwhich is captioned by pre-trained BLIP [48] models. Text prompts with length\nshorter than 10 are also filtered out. For CC-12M [8] and Objects365 [79], instead\nof utilizing the original annotations, we use the pre-trained BLIP-2 model [47] to\ncaption the images. Following previous works [17,84], additional filtering rules are\nappled to the MMC4 dataset [111]. Specifically, images with a CLIP similarity\nscore below 0.24 will be discarded, and only 6 images at most will be kept for\neach document. We also exclude 100% of all documents that do not contain any\nimages, and 50% of documents that contain only 1 image.\nData Concatenation Strategy. For image-text-pair datasets (i.e., LAION-\n2B, LAION-COCO [78]), we randomly sample multiple image-text pairs from the\nsame dataset and concatenate them to the maximum context length (i.e., 2048)\nduring pre-training. For interleaved image and text datasets (i.e., MMC4 [111]),\nwe also split and concatenate the documents to form the training samples. Such\nconcatenation strategy can utilize the full context window of Large Language\nModels and thus achieve high data efficiency.\nMore Training Details. The detailed hyper-parameters of pre-training are\nlisted in Tab. 9. Besides that, for image generation, we ignore the training loss\nof images that are the first element in the sequence. The text condition of the rest\nimages are dropped with a 10% probability to improve classifier-free guidance\nsampling [32].\nA.2\nSupervised Fine-tuning\nVQA and Image Captioning. For this task, we train our model in the\nform of question answering, i.e., using Based on the image, please answer the\nquestion. {image}{question}. The answer is: {answer} as the instruction prompt.\nWe utilize public available datasets for supervised fine-tuning, including LLaVA-\nMix-665K [53], COCO Caption [11], VQAv2 [27], ChartQA [61], DocVQA [13],\nEST-VQA [93], InfoVQA [62], STVQA [93], TextCaps [80], LLaVAR [104], OCR-\nVQA [63], and DVQA [41]. See Tab. 10 for more training details.\nReferring Expression Comprehension. Following previous works [6, 10],\nwe train our model in the form of question answering, i.e., the prompt being\nQuestion: Provide the bounding box coordinate of the region this sentence\ndescribes: {object}, Answer: (x1,y1)(x2,y2). The generated bounding box is\nconsidered correct if its intersection over union (IoU) with the GT box is greater\nthan 0.5. Only public available datasets, including datasets from RefCOCO [42],\nRefCOCO+ [59], and RefCOCOg [59] are utilized to train MM-Interleaved. See\nTab. 11 for fine-tuning hyper-parameters.\nMM-Interleaved\n23\nHyper-parameters\nValue\nInput image resolution\n224 × 224\nOutput image resolution\n512 × 512\nVFM\nCLIP-ViT-L\/14 (frozen)\nLLM\nVicuna-13B v1.3 (frozen)\nDM\nStable Diffusion v2.1\nλ\n10\nCross-attention frequency\n4\nLLM Multi-scale feature maps\ni = 2, 3, 4\nLearning rate\n1e-5 (image decoder)\n1e-4 (others)\nWeight decay\n0.05\nWarmup steps\n1k\nLearning rate schedule\nconstant with warmup\nTraining iterations\n15k\nContext length\n2048\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1, β2, ϵ = 0.9, 0.995, 1e-6\nData\nMMC4, LAION-2B, LAION-COCO, CC-12M, Objects365\nAugmentation\nCenterCrop\nBatch size (per GPU)\n2 for MMC4, and 4 for the rest\nTable 9: Hyper-parameters for pre-training.\nSegmentation-to-Image Translation.\nFollowing ControlNet [102], we use\nBLIP [48] to generate text captions for each image in the ADE20K dataset. We\ntrain our model on the training set and evaluate on the validation set. The input\nsequence is formulated as {segmentation image}{caption}{ground truth image}\nfor each segmentation-image pair. See Tab. 12 for more training details.\nVisual Storytelling. Following previous works [43,67,106], MM-Interleaved is\nfinetuned on the following three visual storytelling datasets respectively. The\nfinetuning hyper-parameters are listed in Tab. 13.\n– VIST [34] is a real-world vision-language dataset, containing 34k and 5k\nsamples for training and evaluation. Each sample is a sequence consisting of\n5 text captions and images. During training, we concatenate all the texts and\nimages sequentially and the model is trained to predict all images. During\ninference, we test the model on generating the last image in the sequence,\nconditioned on all preceding images and texts following [43,106]. The evalua-\ntion metrics are FID [31] and the CLIP similarity [71] between the generated\nimages and the corresponding real images.\n– PororoSV [51] and FlintstonesSV [57] are two cartoon storytelling datasets,\ncontaining 10191\/2334\/2208 and 20132\/2071\/2309 samples of the train, val-\nidation, and test set, respectively. Each sample is a sequence consisting of\n5 text captions and frame images. During training, all the texts and images\nare concatenated sequentially and the model is trained to predict all images.\nDuring inference, the last 4 images are generated auto-regressively given the\nfirst image and all preceding captions as condition. FID [31] is used as the\nevaluation metric.\n24\nC. Tian et al.\nHyper-parameters\nValue\nInput image resolution\n448 × 448\nLearning rate\n1e-6 (language model)\n1e-5 (others)\nWeight decay\n0.05\nWarmup steps\n500\nLearning rate schedule\ncosine\nTraining iterations\n10k\nOptimizer\nAdamW\nOptimizer hyper-parameters β1, β2, ϵ = 0.9, 0.999, 1e-8\nBatch size\n256\nTable 10: Hyper-parameters for VQA and image captioning.\nHyper-parameters\nValue\nInput image resolution\n224 × 224\nLearning rate\n2e-5\nWeight decay\n0.05\nWarmup steps\n500\nLearning rate schedule\ncosine\nTraining iterations\n10k\nOptimizer\nAdamW\nOptimizer hyper-parameters β1, β2, ϵ = 0.9, 0.999, 1e-8\nAugmentation\nRandomHorizontalFlip\nBatch size\n256\nTable 11: Hyper-parameters for referring expression comprehension.\nA.3\nEvaluation\nBenchmarks. Evaluating MM-Interleaved comprehensively requires various bench-\nmarks and datasets, such as image caption, visual question answering, text-to-\nimage generation and so on. All these evaluation tasks and metrics are listed in\nTab. 14.\nImage Generation. For all image generation tasks, the scale of classifier-free\nguidance [32] and the total inference step is set as 3.5 and 250 by default.\nText Generation. The prompt templates for each text generation tasks are\nlisted in Tab. 15. The ‘Image Caption (short)’ task includes COCO Caption [11],\nFlicker30k [69] and NoCaps [2], while the ‘Image Caption (long)’ task includes\nImage2Paragraph [44].\nB\nAdditional Ablation Studies\nThis section provides more ablation studies for MM-Interleaved, all of which\nshare the same settings as those in the main text by default. Note that these\nmodels are not fine-tuned on downstream tasks.\nPre-training with Different Loss Terms. As shown in Tab. 16, MM-Interleaved\nachieves better performance when jointly trained with both LNT P and LNIP on\ncomprehension and generation tasks, demonstrating the mutual benefits between\nMM-Interleaved\n25\nHyper-parameters\nADE20K\nInput image resolution\n224 × 224\nOutput image resolution\n512 × 512\nLearning rate\n1e-5 (image decoder)\n1e-4 (others)\nWeight decay\n0.05\nWarmup steps\n100\nLearning rate schedule\ncosine\nTraining iterations\n4k\nOptimizer\nAdamW\nOptimizer hyper-parameters β1, β2, ϵ = 0.9, 0.98, 1e-5\nAugmentation\nRandomHorizontalFlip\nBatch size\n512\nTable 12: Hyper-parameters for Segmentation-to-Image Translation.\nHyper-parameters\nVIST \/ PororoSV \/ FlintstonesSV\nInput image resolution\n224 × 224\nOutput image resolution\n512 × 512\nLearning rate\n1e-4 (image decoder)\n1e-5 (others)\nWeight decay\n0.05\nWarmup steps\n200\nLearning rate schedule\ncosine\nTraining iterations\n4k\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1, β2, ϵ = 0.9, 0.98, 1e-8\nAugmentation\nCenterCrop\nBatch size\n128\nTable 13: Hyper-parameters for visual storytelling.\nthe two loss terms. Moreover, setting λ = 10 achieves a better balance between\nLNT P and LNIP empirically.\nThe Relationship between MMFS and Resampler. The results in Tab. 17\nvalidate the mutual benefits between our proposed MMFS module and the Re-\nsampler used in image tokenizer, as removing either of them leads to an overall\nperformance degradation.\nC\nQualitative Results\nC.1\nZero-shot Results\nThe zero-shot comprehension and generation visualizations of our method are\nshown in Figs. 7 to 10. Specifically, Fig. 7 demonstrates the emergent multi-\nmodal in-context learning capability of our method, while Fig. 8 further illus-\ntrates the effectiveness of our method in understanding complex scenarios such\nas robotics, gaming and GUI interface. Fig. 9 exhibits that our method can also\ngenerate appropriate images based on the provided contexts of desired styles\n26\nC. Tian et al.\nDataset\nTask\nSplit\nMetric\nCaption.\nCOCO [11]\nScene description\ntest\nCIDEr(↑) [88]\nFlickr30k [69]\nScene description\ntest\nCIDEr(↑) [88]\nNoCaps [2]\nScene description\ntest\nCIDEr(↑) [88]\nImage2Paragraph [44] Scene description\ntest\nCIDEr(↑) [88]\nVQA.\nVQAv2 [27]\nScene understanding QA\ntest-dev VQA Acc(↑) [4]\nOKVQA [60]\nExternal knowledge QA\nval\nVQA Acc(↑) [4]\nGQA [35]\nScene understanding QA\ntest-dev VQA Acc(↑) [4]\nVizWiz [29]\nScene understanding QA\ntest-dev VQA Acc(↑) [4]\nTextVQA [81]\nText reading QA\nval\nVQA Acc(↑) [4]\nVisDial [15]\nImage dialogue\nval\nNDCG(↑)\nREC.\nRefCOCO [42]\nReferring experssion comprehension\n-\nIoU Acc(↑)\nRefCOCO+ [59]\nReferring experssion comprehension\n-\nIoU Acc(↑)\nRefCOCOg [59]\nReferring experssion comprehension\n-\nIoU Acc(↑)\nGeneration.\nMS-COCO [52]\nText-to-image generation\nval-30K\nFID(↓) [31]\nLN-COCO [70]\nText-to-image generation\nval\nFID(↓) [31]\nADE20k [108]\nSegmentation-to-image generation\nval\nmIoU(↑)\nVIST [34]\nInterleaved-context image generation\nval\nCLIP-Sim(↑) [71], FID(↓) [31]\nPororoSV [51]\nInterleaved-context multi-image generation test\nFID(↓) [31]\nFlintstonesSV [28]\nInterleaved-context multi-image generation test\nFID(↓) [31]\nTable 14: Summary of evaluation benchmarks, including image caption, visual\nquestion answering, referring experssion comprehension, and image generation.\nor concepts. Fig. 10 displays our method’s capability of generating images and\ntexts interleavedly.\npineapples: 5 + 3 = 8\nstrawberries: 6 + 3 = 9\napples: 6 + 2 = 8\nThe company is famous \nfor its graphics cards.\nThe company is famous \nfor its iPhone and Mac.\nThe company is famous \nfor its search engine.\nThe text in red circle: \"Real Life\".\nThe text in red circle: \"GO-RIDE\".\nThe text in red circle: \"XUN\".\nFig. 7: Zero-shot text generation with interleaved images and texts serving\nas multi-modal few-shot contexts.\nC.2\nSupervised Finetuning Results\nText Reading QA. As is shown in Fig. 11, MM-Interleaved with MMFS pro-\nvides more accurate answers when requiring fine-grained details for generating\ntext outputs given VL inputs.\nMM-Interleaved\n27\nTask\nPrompt Template\nZero-shot\nImage Caption (short)\n{image} a photo of\nImage Caption (long)\n{image} Please describe the image in detail. The image\ndepicts\nVQA (except VizWiz)\nBased on the image, please answer the question.\n{image}{question} Please provide an accurate answer within\none word. The answer is:\nVizWiz QA\nBased on the image, please answer the question.\n{image}{question} When the provided information is\ninsufficient, respond with ’Unanswerable’. Please provide\nan accurate answer within one word. The answer is:\nVisual Dialog\n{image} caption: {caption} question: {history question}?\nanswer: {history answer}. · · · question: {question}? answer:\nSupervised\nFine-tuning\nImage Caption (short)\n{image} Provide a one-sentence caption for the provided\nimage.\nImage Caption (long)\n{image} Please describe the image in detail.\nVQA (except VizWiz)\nBased on the image, please answer the question.\n{image}{question} Please provide an accurate answer within\none word. The answer is:\nVizWiz VQA\nBased on the image, please answer the question.\n{image}{question} When the provided information is\ninsufficient, respond with ’Unanswerable’. Please provide\nan accurate answer within one word. The answer is:\nVisual Dialog\n{image} caption: {caption} question: {history question}?\nanswer: {history answer}. · · · question: {question}? answer:\nTable 15: Prompt templates for text generation.\nReferring Expression Comprehension. The visualization of MM-Interleaved\non REC tasks is shown in Fig. 12. Our model with MMFS is also capable of\ngenerating more accurate coordinates given the referring expression and the\nquery image.\nSegmentation-to-image Translation. Fig. 13 shows the visualization results\nof MM-Interleaved for segmentation-to-image translation. Given the text prompt\nand segmentation map, the spatial layout of images generated with MMFS is\nsignificantly closer to the original ground-truth image, compared to the baseline\nresults without MMFS.\nMulti-image Generation. In Fig. 14, we compare the multiple images se-\nquentially generated by MM-Interleaved with and without MMFS. The images\ngenerated with MMFS achieves better spatial consistency (e.g. the background\nenvironment, change of viewpoint, character position relationship etc.) and closer\nsemantic alignment with the interleaved image-text context.\nGenerating Interleaved Image and Texts. Moreover, the model is further\nfinetuned to generate both images and texts simultaneously for visual story-\ntelling tasks. As is shown in Fig. 15, given the first frame image and caption as\ncontext, MM-Interleaved with MMFS generates the following interleaved images\n28\nC. Tian et al.\nLoss Term\nCaption↑Generation↓OK-VQA↑TextVQA↑\nLNT P + 100 LNIP\n106.2\n31.1\n29.8\n24.5\nLNT P + 10 LNIP\n110.6\n30.0\n29.8\n27.7\nLNT P + LNIP\n110.0\n31.4\n29.3\n26.0\nLNT P only\n105.7\n–\n29.9\n27.6\nLNIP only\n–\n34.2\n–\n–\nTable 16: Pre-training with different loss terms.\nw\/ Resampler w\/ MMFS Caption↑Generation↓OK-VQA↑TextVQA↑\n✓\n✓\n110.6\n30.0\n29.8\n27.7\n✓\n107.0\n32.2\n28.7\n22.5\n✓\n102.7\n32.0\n27.3\n22.0\nTable 17: The complementary relationship between MMFS and Resampler.\nWhen not using Resampler, we directly feed 32 randomly-initialized learnable embed-\ndings as input visual tokens into the LLM.\nand texts coherently, achieving balance between the generation diversity and\nspatial semantic consistency.\nMM-Interleaved\n29\nThe sequence of images depicts\na robot picking and putting objects down \non a wooden surface, demonstrating \ngrasp planning in 3d.\nStep 3: Tap Display & Text Size.\nHow to Make Your iPhone Screen \nBlack and White or Grayscale\nStep 1: Open your iPhone's Settings iPhone \nSettings App Icon. You'll see this gray gear \nicon on your Home screen or in your App \nLibrary.\nStep 2: Tap Accessibility. It's in the third \ngroup of settings.\nStep 4: Scroll down to Color Filters and \nturn it on. This will allow you access the \nblack & white, grayscale options below in \nthe same section of settings (Display \nSettings).\nStep 5: Tap on Grayscale. This is the filter \nwe want to select for black & white or \ngrayscale mode.\nThe sequence of images depicts\na fence being created on the screen by \nselecting materials and using them. This \nis important in showing that Minecraft \nwill allow players to create their own \nstructures, tools & weapons, as well as \nprovide decoration for landscapes \nthrough the game’s crafting mechanics.\nFig. 8: Zero-shot text generation with interleaved images and texts on com-\nplex scenarios such as robotics, gaming and GUI interface.\n30\nC. Tian et al.\nA portrait of an old man. He has a bushy \nbeard and wears a straw hat. His face is \ntowards the left of the frame but looking \ntowards the camera.\nThis is an oil painting depicting \nsunflowers. There are many sunflowers in \na vase. And there are red and yellow \nsunflowers.\nThis is a photo of an ocean. The wave \ncomes up on shore and then goes back \ndown again, making lots of noise.\na picture of a large elephant with the \nsame style as the picture above\nAmong these plants sits a \nlarge wooden house\nin the style of\nFig. 9: Zero-shot image generation with interleaved images and texts. Our\nmethod can generate appropriate images based on the provided contexts of desired\nstyles or concepts.\nMM-Interleaved\n31\nOnce upon a time, there was an ancient rose. The rose was very \nspecial because it was old and beautiful.\nIt grew in the garden of a large mansion that belonged to a rich lord. \nThe rose had many admirers, but there was one who loved it more \nthan any other: its gardener, Mrs. Brown! Each day she looked after \nthe rose carefully and tended each leaf with love; because she gave \nher great happiness every time she blossomed on this earth.\"\nShe was the most beautiful rose that had ever been known. Her name \nwas The Fairy Rose, and she gave a smile to all who saw her, like \nsunshine for all hearts.\nA small bird came to visit every day, singing a beautiful song of love \nas he perched on her red petals. \nOne day the rose saw that her beautiful petals were fading away. She \ncried and begged the gardener to save them. But the gardener \nreplied: 'My dear rose, you must accept that your time has passed \nand leave for another to give joy with her smile. '\nThe rose's petals were collected by Mrs. Brown and put away safely \nfor future use in celebration. It was never forgotten, but it will always \nbe remembered as the most beautiful fairy Rose that had ever been \nknown.\nHow to make apple juice.\nStep 1: Rinse and cut 4 apples into quarters. Set the clean apples \non a cutting board and remove the cores and seeds. You can \nleave the peels on the apples. Then cut each apple into 4 equal-\nsized pieces.\nStep 2: In a large bowl, place the apple pieces and add water.\nUse enough water to fully cover the apples. Then stir gently with a \nwooden spoon.\nStep 3: Place a strainer on top of the large bowl and squeeze out as \nmuch juice from each apple piece that you can. If there is pulp left \nover in your bowl after pressing all fruit with the spoon, then press it \nagain by gently pushing down harder to extract more fluid.\nStep 4: Pour juice into a large container or pitcher. If you want to \nremove the pulp from your apple juice, strain it with cheesecloth and \nuse an ice-cube tray to form blocks of hardened leftover juices; add \nthem in glass jars while using apples for cooking later as they freeze \nwell!\nEnjoy a crisp and sweet apple juice!\nFig. 10: Zero-shot interleaved image and text generation for visual storytelling\nand multi-modal instructions.\n32\nC. Tian et al.\nUser: what year is written on the bottom?\nMM-Interleaved (w. MMFS): 2011.\nMM-Interleaved (w\/o MMFS): 2022.\nUser: what is the number of the player at bat?\nMM-Interleaved (w. MMFS): 79.\nMM-Interleaved (w\/o MMFS): 9.\nUser: how many stars can you see on the white board?\nMM-Interleaved (w. MMFS): 3.\nMM-Interleaved (w\/o MMFS): 5.\nUser: what is in the bottles?\nMM-Interleaved (w. MMFS): Corona.\nMM-Interleaved (w\/o MMFS): Tequila.\nUser: what brand is on the middle shelf?\nMM-Interleaved (w. MMFS): Coke.\nMM-Interleaved (w\/o MMFS): Pepsi.\nUser: what brand is represented here?\nMM-Interleaved (w. MMFS): Intel.\nMM-Interleaved (w\/o MMFS): Dell.\nFig. 11: Qualitative results on TextVQA [81]. Each example consists of the user\nquery, the answer given by MM-Interleaved with MMFS, and the answer given by MM-\nInterleaved without MMFS. The image shapes are normalized for visualization.\nUser: a sweet foo in an orange bowl\nMM-Interleaved (w. MMFS): (595,090)(855,520)\nMM-Interleaved (w\/o MMFS): (595,507)(892,981)\nUser: an orange bus in between two other buses\nMM-Interleaved (w. MMFS): (619,507)(892,792)\nMM-Interleaved (w\/o MMFS): (482,514)(634,790)\nUser: a head of broccoli sits on the table\nMM-Interleaved (w. MMFS): (000,569)(242,984)\nMM-Interleaved (w\/o MMFS): (000,000)(402,627)\nUser: a laptop\nMM-Interleaved (w. MMFS): (581,001)(999,989)\nMM-Interleaved (w\/o MMFS): (336,170)(619,754)\nUser: part of a small white airplane with three windows\nMM-Interleaved (w. MMFS): (000,316)(418,599)\nMM-Interleaved (w\/o MMFS): (000,000)(997,707)\nUser: zebra with its head behind the other zebra\nMM-Interleaved (w. MMFS): (276,406)(563,727)\nMM-Interleaved (w\/o MMFS): (446,422)(684,709)\nFig. 12: Referring Expression Comprehension on RefCOCOg [59]. Each ex-\nample consists of the user query, the box predicted with MMFS, and the box predicted\nwithout MMFS. The image shapes are normalized for visualization.\nMM-Interleaved\n33\na large brick \nbuilding with a red \nroof\na large building \nwith a clock tower \nin the middle of it\na small plane on a \nrunway with a man \non a lawn mower\na lobby with a \ncouch and chairs \nand a table\na bedroom with a \nbed, a bookcase, \nand a potted plant\na store with a \ndisplay of sports \napparel\na dining room \ntable with a china\ncabinet\nSegmentation \nmap\nGT \nimage\nGenerated\nw. MMFS\nGenerated\nw\/o MMFS\nFig. 13: Segmentation-to-Image Generation on ADE20k [108]. Each row is an\nexample consisting of four images, which are the input segmentation map, the ground\ntruth image, the generated image with MMFS, and the generated image without MMFS\nrespectively. The shape of ground-truth images and segmentation maps is normalized\nfor visualization. When without MMFS, the generated results lack spatial alignment\nwith the input segmentation maps.\n34\nC. Tian et al.\n[frame:0] Loopy wants to be skinny. Eddy Crong Pororo and Poby \nlooks at Loopy. \n[frame:1] Eddy Crong Pororo and Poby encourage her. Poby stands \nup. \n[frame:2] Poby stands up and give an advice. \n[frame:3] Poby moves his arms. Poby thinks exercise is important. \n[frame:4] Loopy thinks about doing exercise. \n[frame:0] Pororo and his friends are angry. Eddy is surprised. \n[frame:1] Petty blames Eddy. Pororo and his friends look at \nEddy. \n[frame:2] Loopy says angrily. Pororo shakes his head. \n[frame:3] Rody says to Eddy. Eddy looks discouraged. \n[frame:4] Eddy looks down and scratches his head. \n[frame:0] Wilma is on a surfboard on a huge wave in the ocean, \nand then Fred falls onto her shoulders. \n[frame:1] Barney is in a car with Fred and talking to Fred as he \ndrives. \n[frame:2] Barney is in the car. he talks while Fred sits next to him. \n[frame:3] Fred talks to Barney while driving his car. Barney rides in \nthe passenger seat. \n[frame:4] the man in blue shirt is in a dressing room. he is leaning \non the vanity table next to a guitar. he is talking. \n[frame:0] Wilma is standing in a room. she is holding a perfume \nbottle and speaking. \n[frame:1] Wilma is laying on the couch in the living room. she has \nher arms behind her head and she is talking. \n[frame:2] Wilma is lying on the living room couch. Fred walks \ntoward her. \n[frame:3] Wilma yells at Fred while lying on the couch in the living \nroom. \n[frame:4] Fred is in the room and puts on sunglasses. \nInput \ncontext\nGT\nimage\nGenerated\nw. MMFS\nGenerated\nw\/o MMFS\nInput \ncontext\nGT\nimage\nGenerated\nw. MMFS\nGenerated\nw\/o MMFS\nFig. 14: Multi-image Generation on PororoSV [51] and FlintstonesSV [28].\nEach example consists of four rows. The first row is the first frame image and all\ncorresponding captions. The second row contains the ground truth images of following\nframes; The third row are the generated results with MMFS; And the last row are the\nresults generated without MMFS. When without MMFS, the generated multiple images\nlack content consistency in terms of characters, backgrounds, objects, etc.\nMM-Interleaved\n35\nEddy is determined \nto hit the ball well. \nEddy is holding the \nball and talking to \nPororo.\nEddy is determined \nto hit the ball well. \nEddy is holding the \nball and talking to \nPororo.\nEddy is determined \nto hit the ball well. \nEddy is holding the \nball and talking to \nPororo.\nEddy seems very \ndetermined. Eddy \nthrew the ball back. \nEddy is getting \nready to hit the ball.\nPororo smiles and \nready to play. \nEddy tries to hit the \nball but the ball \ncannot be go over \nthe net.\nPororo is getting \nready to catch the \nball. Eddy is getting \nready to hit the ball. \nRody is holding the \nball.\nPororo says he is \ngoing to play with \nthe ball first. Pororo \ndoesn't give the ball \nto Eddy.\nPetty jumps and \nLoopy spins due to \nCrong's mistake.\nRody is thinking \nabout how to throw \nthe ball. Rody is \ngoing to throw the \nball.\nPororo is playing \nwith the ball. Eddy is \nwatching him \nplaying. Pororo is \nthrowing the ball at \nthe wall and \ncatching it again.\nEddy and Rody look \ndisappointed. Crong \npicks up the ball.\nEddy is getting \nready to throw the \nball. Eddy is \nswinging the bat. \nEddy looks very \nconfident. Pororo is \npreparing to catch \nthe ball.\nPororo is happy that \nPororo got the ball. \nEddy seems angry. \nEddy also wants to \nplay with the ball.\nEddy suggests \nCrong to give the \nball to Eddy.\nWilma is talking to \nBetty. they are in a \nroom together.\nWilma is talking to \nBetty. they are in a \nroom together.\nWilma is talking to \nBetty. they are in a \nroom together.\nWilma and Betty are \nin a room. they are \nfacing each other \nwhile Betty does all \nthe talking to Wilma.\nWilma and Betty are \nstanding in a room. \nWilma is speaking \nto Betty. Betty looks \nat Wilma while \ntouching her hand \nto her chin.\nWilma and Betty are \nstanding in a room. \nWilma speaks to \nBetty and Betty \nresponds.\nWilma and Betty are \nstanding in a room. \nBetty is talking to \nWilma. Wilma has \nher head turned \ntoward Betty.\nWilma and Betty are \nstanding in the \nroom talking.\nBetty and Wilma are \nhaving a \nconversation while \nstanding in the \nliving room.\nBetty is standing in \nthe room talking. \nWilma is standing \nnext to Betty.\nWilma and Betty are \nstanding in a room \nspeaking to each \nother.\nWilma and Betty are \ntalking to each \nother in a room.\nWilma and Betty are \nstanding in a room. \nBetty is holding her \narms up while \ntalking and Wilma \nturns to look at her.\nWilma is in the \nroom. she is saying \nsomething she \nseems certain about.\nWilma and Betty \nare in the living \nroom. Wilma is \nspeaking to \nsomeone off screen. \nBetty looks at \nWilma and then \nspeaks to someone.\nGround\nTruth\nGenerated\nw. MMFS\nGenerated\nw\/o MMFS\nGround\nTruth\nGenerated\nw. MMFS\nGenerated\nw\/o MMFS\nInterleaved generating images and texts\n[frame 0] \ntext\n[frame 0]\nimage\n[frame 1] \ntext\n[frame 1]\nimage\n[frame 2] \ntext\n[frame 2]\nimage\n[frame 3] \ntext\n[frame 3]\nimage\n[frame 4] \ntext\n[frame 4]\nimage\nFig. 15: Interleaved Image-Text Generation on PororoSV [51] and Flint-\nstonesSV [28]. Each example consists of three columns. The first column is the\nground-truth images and captions of all frames. The second column are the generated\nresults with MMFS; And the last column are the results generated without MMFS.\nOnly the caption and image of the first frame is given as condition during generation.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer.pdf"}
{"title":"Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action","authors":"Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi","summary":"We present Unified-IO 2, the first autoregressive multimodal model that is\ncapable of understanding and generating image, text, audio, and action. To\nunify different modalities, we tokenize inputs and outputs -- images, text,\naudio, action, bounding boxes, etc., into a shared semantic space and then\nprocess them with a single encoder-decoder transformer model. Since training\nwith such diverse modalities is challenging, we propose various architectural\nimprovements to stabilize model training. We train our model from scratch on a\nlarge multimodal pre-training corpus from diverse sources with a multimodal\nmixture of denoisers objective. To learn an expansive set of skills, such as\nfollowing multimodal instructions, we construct and finetune on an ensemble of\n120 datasets with prompts and augmentations. With a single unified model,\nUnified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and\nstrong results in more than 35 benchmarks, including image generation and\nunderstanding, natural language understanding, video and audio understanding,\nand robotic manipulation. We release all our models to the research community.","url":"http:\/\/arxiv.org\/abs\/2312.17172v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.17172v1","published":1703786226000,"comment":"38 pages, 20 figures","pdf_text":"Unified-IO 2: Scaling Autoregressive Multimodal Models\nwith Vision, Language, Audio, and Action\nJiasen Lu1*\nChristopher Clark 1*\nSangho Lee1*\nZichen Zhang1*\nSavya Khosla2\nRyan Marten2\nDerek Hoiem2\nAniruddha Kembhavi13\n1Allen Institute for AI\n2 University of Illinois Urbana-Champaign\n3 University of Washington\n{jiasenl, chrisc, sanghol, chralesz, anik}@allenai.org\nunified-io-2.allenai.org\nWhat food could you \nmake with these \ningredients ? Give \nme the recipe.\nOne delicious recipe using these ingredients is \nchocolate pudding! Here's the recipe:\nIngredients:\n- 1 cup all-purpose flour, - ½ cup sugar ….\nInstructions:\n1. In a large bowl whisk together the flour, sugar..\n2. In a separate bowl, mix together the eggs …\nGenerate an audio \ntrack for this band.\nGenerate an audio \ntrack for this band.\nRender a sunset\nRemove the dock\nPaint this image \nlike Van Gogh\nGenerate an image of \na car with the model \nin the first image and \nthe color in the \nsecond image.\nGenerate a \ndepth image\nGenerate a depth & \nsurface normal map\nGenerate a surface \nnormal map\nWhich fruits are in this \nimage ? List them in \njson format with the \nname of the fruit as the \nkey and the color of \nthe fruit as the value.\n{\n\"banana\" : \"Yellow\",\n\"apple\" : \"Red\",\n\"grapes\" : \"Green\"\n}\nSegment grapes and \none apple\nPut the less kobar \nblicket into the dax.\n= blicket\n= dax\nis kobar than\nGiven the initial \nimage and a \nsequence of \nactions, predict \nthe next frames\nGenerated\nImages\nGenerate an image \nof an astronaut \nriding a horse in the \nforest. There is a \nriver in front of them \nwith water lilies. \nGenerate an image of \nan elephant swimming \nunderwater. aesthetic. \nFantasy.\nAdd the missing \ndetails to the \nmasked image (left) \nusing the reference \nimage (right).\nFind the visible keypoints \ncorresponding to the \nperson located in the \nhighlighted region.\nIdentify the locations \nof the instruments \nproducing the given \nsound.\n(drum sounds)\nImage Editing\nImage Generation\nReference Image Generation\nMultiview Image Completion\nDepth & Surface Normal \nKeypoint Estimation\nVisual Parsing\nFree Form VQA\nVisual based Audio Generation\nVisual Audio Localization\nRobotic Manipulation\nFuture Frame Prediction\nturn right\nturn right\nturn right\nmove ahead\nmove ahead\nmove ahead\nmove ahead\nturn right\nSegmentation\n&\nFigure 1. UNIFIED-IO 2 is an instruction-following model with a huge breadth of abilities and supported modalities. It can generate images\n(red box), including image editing, image generation, depth estimation, surface normal estimation, and future frame prediction etc. It can\nalso generate texts (blue box), including long-form answers to queries, keypoint estimation, visual audio localization, predicting actions\nfor robotic manipulation etc. It can generate audio (green box) from images or text. Click n and n for the corresponding audio samples.\n* Leading Authors, equal contribution. A description of each author’s\ncontribution is available in Appendix A. Corresponding to Jiasen Lu.\n1\narXiv:2312.17172v1  [cs.CV]  28 Dec 2023\nAbstract\nWe present UNIFIED-IO 2, the first autoregressive multi-\nmodal model that is capable of understanding and generat-\ning image, text, audio, and action. To unify different modal-\nities, we tokenize inputs and outputs – images, text, audio,\naction, bounding boxes etc., into a shared semantic space\nand then process them with a single encoder-decoder trans-\nformer model. Since training with such diverse modalities\nis challenging, we propose various architectural improve-\nments to stabilize model training. We train our model from\nscratch on a large multimodal pre-training corpus from di-\nverse sources with a multimodal mixture of denoisers objec-\ntive. To learn an expansive set of skills, such as following\nmultimodal instructions, we construct and finetune on an\nensemble of 120 datasets with prompts and augmentations.\nWith a single unified model, UNIFIED-IO 2 achieves state-\nof-the-art performance on the GRIT benchmark and strong\nresults in more than 35 benchmarks, including image gener-\nation and understanding, natural language understanding,\nvideo and audio understanding, and robotic manipulation.\nWe release all our models to the research community.\n1. Introduction\nAs AI researchers, we seek to build intelligent agents that\ncan perceive their environment, communicate with others,\nact in the world, and reason about their interactions. The\nworld is multimodal, so our agents must partake in rich\ninteractions that are multimodal in nature via vision, lan-\nguage, sound, action etc. Psychologists have argued that\nthe redundancy of our sensory systems serves as supervi-\nsory mechanisms to improve each other [48, 144, 167]. This\nprovides a natural motivation to create models with similar\nlearning capabilities, supporting many different modalities\nthat can supervise each other during training.\nBuilding models that can parse and produce many\nmodalities is a complex undertaking. Training Large Lan-\nguage Models (LLMs) with billions of parameters, despite\nonly supporting a single modality, is extremely challenging\nacross many fronts – from sourcing and processing massive\ndatasets, ensuring data quality and managing biases, design-\ning effective model architectures, maintaining stable train-\ning processes, and instruction tuning to enhance the model’s\nability to follow and understand user instructions. These\nchallenges are hugely amplified with the addition of each\nnew modality.\nIn light of these difficulties, a line of recent works\nin building multimodal systems has leveraged pre-trained\nLLMs, with some augmenting with new modality encoders\n[5, 46, 119], some adding modality specific decoders [14,\n96] and others leveraging the LLM’s capabilities to build\nmodular frameworks [64, 166, 173]. Another line of works\non training multimodal models from scratch has focused on\ngenerating text output [81, 143] with a few recent works\nsupporting the understanding and generation of two modal-\nities – text and images [123, 125]. Building generative mod-\nels with a wider coverage of modalities, particularly when\ntraining from scratch, remains an open challenge.\nIn this work, we present UNIFIED-IO 2, a large multi-\nmodal model (LMM) that can encode text, image, audio,\nvideo, and interleaved sequences and produce text, action,\naudio, image, and sparse or dense labels. It can output free-\nform multimodal responses and handle tasks unseen during\ntraining through instruction-following. UNIFIED-IO 2 con-\ntains 7 billion parameters and is pre-trained from scratch on\nan extensive variety of multimodal data – 1 billion image-\ntext pairs, 1 trillion text tokens, 180 million video clips,\n130 million interleaved image & text, 3 million 3D assets,\nand 1 million agent trajectories. We further instruction-tune\nthe model with a massive multimodal corpus by combining\nmore than 120 datasets covering 220 tasks across vision,\nlanguage, audio, and action.\nOur pre-training and instruction tuning data, totaling\nover 600 terabytes, presents significant challenges for train-\ning due to its diversity and volume. To effectively facilitate\nself-supervised learning signals across multiple modalities,\nwe develop a novel multimodal mixture of denoiser objec-\ntive that combines denoising and generation across modali-\nties. We also develop dynamic packing – an efficient imple-\nmentation that provides a 4x increase in training throughput\nto deal with highly variable sequences. To overcome the\nstability and scalability issues in training, we propose to ap-\nply key architectural changes, including 2D rotary embed-\ndings, QK normalization, and scaled cosine attention mech-\nanisms on the perceiver resampler. For instruction tuning,\nwe ensure every task has a clear prompt, either using exist-\ning ones or crafting new ones. We also include open-ended\ntasks and create synthetic tasks for less common modalities\nto enhance task and instruction variety.\nWe evaluate UNIFIED-IO 2 on over 35 datasets across\nthe various modalities it supports. Our single model sets the\nnew state of the art on the GRIT [66] benchmark, which in-\ncludes diverse tasks such as keypoint estimation and surface\nnormal estimation. On vision & language tasks, it matches\nor outperforms the performance of many recently proposed\nVLMs that leverage pre-trained LLMs. On image genera-\ntion, it outperforms the closest competitor [174] that lever-\nages the pre-trained stable diffusion model [154], especially\nin terms of faithfulness as per the metrics defined in [76]. It\nalso shows effectiveness in video, natural language, audio,\nand embodied AI tasks, showcasing versatility despite its\nbroad capability range. Moreover, UNIFIED-IO 2 can fol-\nlow free-form instructions, including novel ones. Figure 1\noffers a glimpse into how it handles various tasks. Further\nexamples, along with the code and models, are accessible\non our project website.\n2\n2. Related Work\nInspired by the success of language models as general-\npurpose text processing systems [20, 122, 177], there has\nbeen a recent wave of multimodal systems trying to achieve\nsimilar general-purpose capabilities with additional modali-\nties. A common approach is to use a vision-encoder to build\nfeatures for input images and then an adapter to map those\nfeatures into embeddings that can be used as part of the in-\nput to an LLM. The network is then trained on paired im-\nage\/language data to adapt the LLM to the visual features.\nThese models can already perform some tasks zero-shot or\nwith in-context examples [109, 132, 178], but generally a\nsecond stage of visual instruction tuning follows using in-\nstructions, visual inputs, and target text triples to increase\nzero-shot capabilities [25, 34, 118, 119, 205, 218, 225].\nBuilding upon this design, many researchers have ex-\npanded the breadth of tasks these models can support. This\nincludes creating models that can do OCR [12, 220], vi-\nsual grounding [12, 26, 143, 189, 207, 212, 219], image-\ntext-retrieval [97], additional languages [112], embodied\nAI tasks [17, 135, 140, 152] or leverage other expert sys-\ntems [52].\nOther efforts have added new input modali-\nties. This includes video inputs [110, 126], audio [80] or\nboth [216]. PandaGPT [170] and ImageBind-LLM [69] use\nthe universal encoder ImageBind [56] to encode many kinds\nof input modalities, and ChatBridge [222] uses a similar\nuniversal encoder based on language. While these efforts\nare effective for understanding tasks, they do not allow com-\nplex multimodal generation and often exclude modalities\nlong considered central to computer vision (e.g., ImageBind\ncannot support sparse annotation of images).\nFewer works have considered multimodal generation.\nUNIFIED-IO [123], LaVIT [88], OFA [186], Emu [172]\nand CM3Leon [210] train models to generate tokens that\na VQ-GAN [49, 179] can then decode into an image,\nwhile GILL [96], Kosmos-G [141] and SEED [53] gener-\nate features that a diffusion model can use, and JAM [4]\nfuses pre-trained language and image generation models.\nUNIFIED-IO 2 also uses a VQ-GAN, but supports text, im-\nage, and audio generation.\nOverall, this shows a strong trend towards expanding the\nnumber of supported tasks and modalities. UNIFIED-IO 2\npushes this trend to its limit, including the capabilities of\nthese prior works with few exceptions and the ability to gen-\nerate outputs in more modalities. Recently, CoDi [174] also\nachieved similar any-to-any generation capabilities by using\nmultiple independently trained diffusion models and align-\ning their embedding spaces. UNIFIED-IO 2 has stronger\nlanguage abilities and can perform well on many more\ntasks.\nA notable feature of UNIFIED-IO 2 is that the model\nis trained from scratch instead of being initialized with a\npre-trained LLM. Prior works [114, 186, 188, 192] fol-\nlowing this approach are typically not designed to produce\ncomplex generations like free-form text responses, images\nor sounds, or follow text instructions.\nCompared to re-\ncent general-purpose multimodals models [81, 143, 210],\nUNIFIED-IO 2 has a significantly broader scope of tasks\nand outputs. Training from scratch means that the method\ncan be reproduced without a costly preliminary stage of lan-\nguage model pre-training and is a more natural fit for how\nhumans learn modalities simultaneously through their co-\noccurrences, not one at a time.\n3. Approach\nIn this section, we discuss the unified task representation\n(3.1), the model architecture and techniques to stabilize\ntraining (3.2), the multimodal training objective (3.3) and\nthe efficiency optimizations (3.4) used in UNIFIED-IO 2.\n3.1. Unified Task Representation\nUNIFIED-IO 2 processes all modalities with a single, uni-\nfied encoder-decoder transformer [181]. This is achieved\nby encoding various inputs and outputs – images, text, au-\ndio, action, boxes etc., into sequences of tokens in a shared\nrepresentation space. Our encoding procedure follows the\ndesign of UNIFIED-IO [123], with several modifications to\nimprove performance and new encoders and decoders for\nadditional modalities. Figure 2 shows an overview of the\nmodel. Details about how modalities are encoded are given\nbelow.\nText, Sparse Structures, and Action. Text inputs and out-\nputs are tokenized using the byte-pair encoding [161] from\nLLaMA [177], which we chose since it supports Unicode\nsymbols and preserves whitespace. Sparse structures such\nas bounding boxes, keypoints, and camera poses are dis-\ncretized and then encoded using 1000 special tokens added\nto the vocabulary [27, 123]. Points are encoded with a se-\nquence of two such tokens (one for x and one for y), boxes\nare encoded with a sequence of four tokens (upper left and\nlower right corners), and 3D cuboids are represented with\n12 tokens that encode the projected center, virtual depth,\nlog-normalized box dimension, and continuous allocentric\nrotation [16]. For embodied tasks, discrete robot actions\n[17] are generated as text commands (e.g., “move ahead” to\ncommand the robot to move forward in navigation). Spe-\ncial tokens are used to encode the robot’s state, such as its\nposition and rotation. Details are in Appendix B.1.\nImages and Dense Structures. Images are encoded with a\npre-trained Vision Transformer (ViT) [84]. We concatenate\nthe patch features from the second and second-to-last layers\nof the ViT to capture both low and high-level visual infor-\nmation. These features are passed through a linear layer\nto get embeddings that can be used as part of the input se-\nquence for the transformer. To generate images, we use VQ-\nGAN [49] to convert images into discrete tokens. These to-\n3\nText\nImage\nImage \nHistory\nAudio\nAudio\nHistory\nViT\nEncoder\nViT\nEncoder\nAST\nEncoder\nAST\nEncoder\nEmb\nPerceiver\nPerceiver\nVQ-GAN \nDecoder\nVQ-GAN\nDecoder\nspeech\nenvironmental\nsound\nimage\nSN\nSegmentation\nDepth  \nText\nAction\nDetection\nmusic\nLinear\nLinear\nDynamic Packing\nDynamic Unpacking\n512\n576\n128\n64\n256\n512\n1024\n512\nBPE\nEncode\nBPE \nDecode\nKeypoint\nUnified-IO 2\nFigure 2. UNIFIED-IO 2 architecture. Input text, images, audio, or image\/audio history are encoded into sequences of embeddings which\nare concatenated and used as input to an encoder-decoder transformer model. The transformer outputs discrete tokens that can be decoded\ninto text, an image, or an audio clip.\nkens are added to the vocabulary and then used as the target\noutput sequence in order to generate an image. For better\nimage quality, we use a dense pre-trained VQ-GAN model\nwith 8 × 8 patch size that encodes a 256 × 256 image into\n1024 tokens with a codebook size of 16512.\nFollowing [123], we represent per-pixel labels, which\ninclude depth, surface normals, and binary segmentation\nmasks, as RGB images that can be generated or encoded\nwith our image generation and encoding abilities. For seg-\nmentation, UNIFIED-IO 2 is trained to predict a binary\nmask given a class and bounding box. An entire image can\nbe segmented by first doing detection, and then querying the\nmodel for a segmentation mask for each detected bounding\nbox and class. See Appendix B.1 for details.\nAudio. UNIFIED-IO 2 encodes up to 4.08 seconds of au-\ndio into a spectrogram (See Appendix B.1 and Table 8).\nThe spectrogram is then encoded with a pre-trained Audio\nSpectrogram Transformer (AST) [57], and the input embed-\ndings are built by concatenating the second and second-to-\nlast layer features from the AST and applying a linear layer\njust as with the image ViT. To generate audio, we use a ViT-\nVQGAN [208] to convert the audio into discrete tokens.\nSince there is no public codebase, we implement and train\nour own ViT-VQGAN with 8 × 8 patch size that encodes\na 256 × 128 spectrogram into 512 tokens with a codebook\nsize of 8196.\nImage and Audio History. We allow up to four additional\nimages and audio segments to be given as input, which\nwe refer to as the image or audio history. These elements\nare also encoded using the ViT or AST, but we then use a\nperceiver resampler [5], see Table 8 for hyperparameters,\nto further compress the features into a smaller number of\ntokens (32 for images and 16 for audio). This approach\ngreatly reduces the sequence length and allows the model\nto inspect an image or audio segment in a high level of de-\ntail while using elements in the history for context. This his-\ntory is used to encode previous video frames, previous audio\nsegments, or reference images for tasks such as multi-view\nimage reconstruction or image-conditioned image editing.\nEight special tokens are added to the text vocabulary and\nused to reference the individual elements in these histories\nin the text input or output.\n3.2. Architecture\nUNIFIED-IO 2 uses a transformer encoder-decoder archi-\ntecture.\nHowever, we observe that using a standard im-\nplementation following UNIFIED-IO leads to increasingly\nunstable training as we integrate additional modalities. As\nshown in Figure 3 (a) and (b), training only on image gener-\nation (green curve) results in stable loss and gradient norm\nconvergence. Introducing a combination of image and text\ntasks (orange curve) slightly increases the gradient norm\ncompared to a single modality, but remains stable. How-\never, the subsequent inclusion of the video modality (blue\ncurve) leads to an unrestrained escalation in the gradient\nnorm. When an XXL version of this model is trained on\nall modalities, as shown in Figure 3 (c) and (d), the loss\nexplodes after 350k steps, and the next token prediction ac-\ncuracy significantly drops at 400k steps. To address this,\nwe include various architectural changes that significantly\nstabilize multimodal training.\n2D Rotary Embedding.\nInstead of relative positional\nembedding [147], we apply rotary positional embeddings\n(RoPE) [169] at each transformer layer.\nFor non-text\nmodalities, we extend RoPE to two-dimensional positions:\nFor any 2D indexes (i, j), we split each of the query and\nkey embeddings of the transformer attention heads in half\nand apply separate rotary embeddings constructed by each\nof the two coordinates to the halves, see Appendix B.2.\nQK Normalization. We observe extremely large values in\n4\n0\n5000\n10000\n15000\n20000\n6\n8\n10\n12\n14\nLoss\n0\n5000\n10000\n15000\n20000\n-1\nGradient Norm\nimage-text\nimage\nimage-text-video\n10\n0\n10\n1\n10\n2\n10\n100k\n200k\n300k\n400k\n12\n14\n16\n18\n20\n22\nLoss\n100k\n200k\n300k\n400k\n0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\nText\nImage\nAudio\n(a)\n(b)\n(d)\n(c)\nimage-text\nimage\nimage-text-video\nFigure 3. Left: Training loss (a) and gradient norms (b) on different modality mixtures. Right: Training loss (c) and next token prediction\naccuracy (d) of UIO-2XXLon all modalities. Results were obtained before applying the proposed architectural improvements.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nStep\n1e6\n11.5\n12.0\n12.5\n13.0\n13.5\n14.0\n Training Loss\n1B\n3B\n7B\nFigure 4. Training loss curves for the three models, which are\npretrained with dynamic packing and a batch size of 512.\nthe multi-head attention logits when including image and\naudio modalities, which leads to attention weights becom-\ning either 0 or 1 and contributes to training instability. To\nsolve this, following [38], we apply LayerNorm [10] to the\nqueries and keys before the dot-product attention computa-\ntion.\nScaled Cosine Attention. We use perceiver resampler [86]\nto compress each image frame and audio segment into a\nfixed number of tokens. We found that even with QK nor-\nmalization, the attention logits in the perceiver can grow to\nextreme values. Therefore, we apply more strict normaliza-\ntion in the perceiver by using scaled cosine attention [121],\nwhich significantly stabilizes training.\nTo avoid numerical instabilities, we also enable float32\nattention logits. Jointly updating the pre-trained ViT and\nAST can also cause instabilities. Thus, we freeze the ViT\nand AST during pretraining and finetune them at the end of\ninstruction tuning. Figure 4 shows that the pre-training loss\nfor our model is stable despite the heterogeneity of input\nand output modalities.\n3.3. Training Objective\nA strong multimodal model has to be exposed to solving\ndiverse sets of problems during pre-training. UL2 [175]\nproposed the Mixture of Denoisers (MoD), a unified per-\nspective to train LLMs, which combines the span corruption\n[147] and causal language modeling [19] objectives. Moti-\nvated by this, we propose a generalized and unified perspec-\ntive for multimodal pre-training.\nMultimodal Mixture of Denoisers.\nMoD uses three\nparadigms: [R] – standard span corruption, [S] – causal\n1\n2\n3\n4\n5\ns\n1\n2\n3\n4\n2\n5\n(a)\n1\n2\n3\n4\n5\n2\n5\n2\n5\n(b)\n1\n2\n3\n4\n5\ns\n2\n3\n4\n2\n5\n(c)\nmasked\ntarget\ncurrent\nleaked\nEncoder \nInput\nDecoder \nInput\nDecoder \nTarget\nFigure 5. Different training paradigms in masked image modeling:\n(a) autoregressive, (b) mask auto-encoder, (c) autoregressive with\ndynamic masking. Our proposed paradigms can maintain causal\ngeneration while avoiding information leaks in the decoder.\nlanguage modeling, and [X] – extreme span corruption.\nFor text targets, we follow the UL2 paradigms. For im-\nage and audio targets, we define two analogous paradigms:\n[R] – masked denoising where we randomly mask x% of\nthe input image or audio patch features and task the model\nto re-construct it and [S] – where we ask the model to gen-\nerate the target modality conditioned only on other input\nmodalities. During training, we prefix the input text with a\nmodality token ([Text], [Image], or [Audio]) and a\nparadigm token ([R], [S], or [X]) to indicate the task.\nAutoregressive with Dynamic Masking.\nOne problem\nwith image and audio masked denoising in an autoregres-\nsive manner is an information leak on the decoder side; see\nFigure 5 (a). The current decoder’s input token (3) is condi-\ntioned on enocoder’s information (2, 5) and all previous to-\nkens (s →2) to predict target (4). As a result, the predicted\ntoken will be conditioned on 1 even though it was masked\nin the encoder since it appears in the decoder, which will\nsimplify the task and harm representation learning. Sim-\nply masking the token in the decoder, as shown in Figure 5\n(b), avoids this information leakage but causes the genera-\ntion and de-noising tasks to interfere with one another. For\nexample, we found that joint training with generation (50%\nMAE and 50% causal modeling) significantly reduced im-\nage generation performance. Our solution is to mask the\ntoken in the decoder except when predicting that token, as\nshown in Figure 5 (c), which does not interfere with causal\nprediction whilst mostly eliminating data leakage. For im-\nage and audio generation, we also use row, column, and\nconv-shaped masked sparse attention [148] in the decoder.\n5\nAudio\nImage\nText\nVideo\nAgent Trajectories\nSynthetic\nImage\/Text Interleaved\nVideo\nImage\/Text Pairs\nAgent Trajectories\nSynthetic\nImage\/Text Interleaved\nVideo\nImage\/Text Pairs\nText\nEmbodied AI\nNatural Language\nAudio Understanding\nVideo Sparse Labelling\nVideo Understanding\nImage Dense Labelling\nImage Sparse Labelling\nImage Understanding\nAudio Generation\nImage Generation\nEmbodied QA\nGoal Generation\nNext Frame\/State Prediction\nAction Prediction\nLanguage Modeling\nText Instruction Following\nAudio Captioning\nAudio Tagging\nVideo Sound Localization\nVideo Action Localization\nVideo Tracking\nVideo Instruction Following\nVideo Question Answering\nVideo Captioning\nVideo Tagging\nOptical Flow\nDepth Estimation\nReferring Expression_Segmentation\nSurface Normal Estimation\nLocalized Segmentation\nSemantic_Segmentation\nKeypoint Detection\nText Detection\n3D\nReferring Expression\nObject Localization\nObject Detection\nImage Pair _QA\nImage Instruction Following\nRegion Captioning\nRelationship Prediction\nImage Tagging\nRegion Classification\nImage Captioning\nVQA\nAudio from Video\nAudio from Text\nView_Synthesis\nImage Inpainting\nNext Frame Generation\nImage Editing\nControllable Image Editing\nImage from Text\nText\nImage\nAudio\nFigure 6. Distribution of pre-training and instruction tuning data. Segments proportional to sampling rates. The inner section shows the\ntarget modality, and the outer section shows the data type. Please refer to Figure 9 and Figure 11 in the Appendix for particular datasets.\nModel\nmodel dims mlp dims\nencoder lyr\ndecoder lyr\nheads\nParams\nUIO-2L\n1024\n2816\n24\n24\n16\n1.1B\nUIO-2XL\n2048\n5120\n24\n24\n16\n3.2B\nUIO-2XXL\n3072\n8192\n24\n24\n24\n6.8B\nTable 1. Size variant of UNIFIED-IO 2.\n3.4. Efficient Implementation\nTraining on heavily multimodal data results in highly vari-\nable sequence lengths for the transformer’s inputs and out-\nputs, both because modalities are often missing for individ-\nual examples and because the number of tokens used to en-\ncode particular modalities can vary from just a few tokens\n(for a sentence) to 1024 tokens (for an output image). To\nhandle this efficiently, we use packing, a process where the\ntokens of multiple examples are packed into a single se-\nquence, and the attentions are masked to prevent the trans-\nformer from cross-attending between examples.\nTypically, packing is done during pre-processing, but it is\nchallenging in our setup since our encoders and decoder do\nnot always support it. Instead, we do packing right before\nand after the transformer encoder-decoder stage, which al-\nlows the modality encoders\/decoder to run on the unpacked\ndata. During training, we use a heuristic algorithm to re-\narrange data being streamed to the model so that long ex-\namples are matched with short examples they can be packed\nwith. Packing optimization was also explored in [100], but\nnot in the streaming setup. Dynamic packing leads to an\nalmost 4x increase in training throughput (Details in Ap-\npendix B.3).\n3.5. Optimizer\nWe use Adafactor [164] as our optimizer with a linear\nwarm-up for the first 5,000 steps and a learning rate decay\nof 1\/\n√\nk. We train with β1 = 0.9 and β2 = 1.0 −k−0.8,\nwhere k is the step number. We use global norm gradient\nclipping with a threshold of 1.0 and find that this is crucial\nto stabilized training. Table 1 gives the details of our differ-\nent models. For all models, we train 3.0M steps – 1.5M for\npre-training and 1.5M for instruction tuning, respectively.\nMore details in Appendix B.4.\n4. Multimodal Data\nOne critical difference between UNIFIED-IO 2 and prior\nwork is that we train the model with a diverse set of mul-\ntimodal data from scratch.\nThis requires curating high-\nquality, open-source multimodal data for both pre-training\n(4.1) and instruction tuning (4.2).\n4.1. Pre-training Data\nOur pre-training data comes from various sources and cov-\ners many modalities. We provide a high-level overview and\ndetails in Appendix C.\nNLP [33%]. We use the publicly available datasets that\nwere employed to train MPT-7B [176]. This dataset em-\nphasizes English natural language text but also contains\ncode and markdown. It includes text from the RedPajama\ndataset [32], C4 [68], Wikipedia, and stack overflow. We\nfollow the proportion suggested by [176] and remove multi-\nlingual and scientific data.\nImage & Text [40%]. Text and image paired data comes\nfrom LAION-400M [159], CC3M [163], CC12M [23], and\nRedCaps [42].\nTo help train the image-history modal-\nity, we also use the interleaved image\/text data from\nOBELICS [104]. We use the last image as the image in-\nput and the remaining images as the image history. Special\ntokens are used to mark where those images occur in the\ntext.\n6\nthe side there's a brick \nbase at the bottom \ncomplete with …\n1. Select target modalities \n[Audio][R] the side there's \na brick base at the bottom \ncomplete with …\n2. Select input modalities \n4. Generate input mask\n5. Pair with Prefix token\n[Audio][R] \n(Text, Image, Image History)\nVideo Data\nModel Inputs\n1.\n2.\n3.\n3.\n…\n…\n(Audio)\n3. Select objective\nMask Audio Denoising [R]\nModel Targets\n…\nFigure 7. Construction of training samples from video data for the model’s input and target. Given the video, we first extract the video\nframes and the corresponding audio spectrograms and transcript. Then, the data pass through a random selection process to determine the\ntarget modality, input modalities, training objective, input mask etc. The model’s final input and target are shown in the top right.\nVideo & Audio [25%].\nVideo provides strong self-\nsupervisory signals with high correlations between audio\nand visual channels. We sample audio and video data from\nvarious public datasets including YT-Temporal-1B [215],\nACAV100M [105], AudioSet [54], WebVid-10M [13], HD-\nVILA-10M [200] and Ego4D [60].\n3D & Embodiment [1%]. For self-supervised 3D and em-\nbodiment pre-training, we use CroCo [194] for cross-view\ngeneration and denoising; Objaverse [40] for view synthe-\nsis; and random trajectories in ProcTHOR [39] and Habi-\ntat [157] for the next action and frame predictions.\nAugmentation [1%]. While there is a lot of unsupervised\ndata on the web for images, text, video, and audio, options\nare much more limited for dense and sparse annotations.\nWe propose to solve this through large-scale data augmen-\ntation. We consider two types of data augmentation: 1. Au-\ntomatically generated segmentation data from SAM [94] to\ntrain the model to segment an object given a point or bound-\ning box.\n2.\nSynthetic patch-detection data which tasks\nthe model to list the bounding boxes of synthetically added\nshapes in an image. We additionally train the model to out-\nput the total number of patches in the image to pre-train its\ncounting abilities.\nTraining Sample Construction. During pre-training, most\nof our data contains various modalities without a supervised\ntarget. In these cases, we randomly pick one of the modal-\nities present to be the target output. Then, we either re-\nmove that modality from the example or replace it with a\ncorrupted version. Other modalities that might be present in\nthe example are randomly kept or masked to force the model\nto make predictions using whatever information is left. Fig-\nure 7 shows an example when using a video that contains\na sequence of image frames, the corresponding audio, and\na text transcript. The pre-training sample is constructed by\nfollowing the procedure: 1. select the target modality; 2.\nselect which other input modalities to keep; 3. select the\nobjective; 4. generate the random input mask depending on\nthe task of denoising or generation; 5. add a prefix token\nindicating the task.\n4.2. Instruction Tuning Data\nMultimodal instruction tuning is the key process to equip\nthe model with diverse skills and capabilities across various\nmodalities and even adapt to new and unique instructions.\nWe construct the multimodal instruction tuning dataset by\ncombining a wide range of supervised datasets and tasks.\nWe ensure every task has a clear prompt, either using exist-\ning ones or writing new ones. We also include open-ended\ntasks and create synthetic tasks for less common modali-\nties to enhance task and instruction variety. Our mixture\nincludes 220 tasks drawn from over 120 external datasets.\nWe provide a high-level overview and examples here and\nleave details in Appendix D.\nNatural Language [25.0%]. For natural language, we use\nthe mixture from FlanV2 [122] and various other instruc-\ntion following datasets [33, 142]. In addition, we continue\npre-training on our unsupervised NLP mixture to help pre-\nvent the model from forgetting information learned from\npre-training during the extensive instruction tuning stage.\nImage Generation [17.6%]. For text-to-image generation,\nwe use the same image & text pairs we used during pre-\ntraining. We also include data from [102, 103, 115] that\nprovide better caption quality.\nWe additionally train the\nmodel to generate images through view synthesis [40, 194],\nimage editing [18, 217], segmentation-based image genera-\ntion [123] and inpainting [123].\nAudio Generation [7.5%].\nThis includes text-to-audio\ndatasets with audio in the wild [47, 93, 131], music [2],\nand human speech [85]. We also add pre-training data with\nthe task of predicting the next audio clip in a video. More\nspecifically, we divide the audio into segments and then\ngenerate one of them given both the text and previous seg-\nments as input.\nImage Understanding [17.8%]. We include various data\nsources from visual question answering [6], image tagging\n[41], region classification [102], and datasets with open-\n7\nWhat are the things I should be cautious about \nwhen I visit here?\nWhen visiting this dock on the water surrounded \nby a forested area, there are several factors to \nconsider:\n1. Visibility: The visibility around the dock may be \nlimited, so it is essential to take care while \nwalking on the dock to avoid any sudden shifts in \nlighting conditions or obstructions on the shore.\n2. Water levels: There might be flooding or \nexcessive waves that could damage the wooden \ndock or affect the safety of the area. It is crucial to \nbe mindful of the water levels to …\nPlease provide a short description of the image.\nAn image of dock on the water with trees in the \nbackground.\nPlease provide a detailed description of the image \nand share your personal impressions of the scene.\nThe image features a wooden jetty over a beautiful \nlake with numerous green trees surrounding the \narea. The scene gives off a peaceful and serene \natmosphere. On the far side of the scene, a \nmountain range can be seen, stretching in the \ndistance. In the middle of the scene, …\nMake the image haunted.\nMake the bridge red.\nMake it in midnight.\nMake it in sunset.\nImagine you are da Vinci.\nDetect the mountain.\nDetect the dock.\nSegment the dock.\nSurface normals\nRemove the bridge.\nGenerate music about this scene. (original)\nGenerate music about this scene. (haunted)\nFigure 8. Our single model can perform a multitude of multimodal tasks: captioning the image, following free-form instructions, image\nediting, object detection, semantic segmentation, surface normal, and image-based audio generation, etc. Here, we show the outputs of our\nmodel for a variety of prompts. Click n and n for the corresponding audio samples.\nended chat-like responses [119, 220].\nWe also include\nthe multimodal instruction tuning datasets M3IT [112] and\nMIMIC-IT [107].\nVideo Understanding [10.6%]. We include data sources\nfrom video captioning [190, 199], video tagging [35, 111,\n168], and video question answering [196, 198]. We also use\nexamples from M3IT [112] and MIMIC-IT [107] for video\ninstruction following.\nAudio Understanding [10.6%]. We include data sources\nfrom audio tagging [24, 54], and audio captioning [47, 93].\nWe also include data from video action classification [7]\nwith audio in the dataset.\nImage Sparse Labelling [7.25%]. These tasks require out-\nputting sparse coordinates based on an input image. We\nmainly consider object detection [115], referring expression\n[91], 3D detection [16], camera pose prediction [40], text\ndetection [183] and human keypoints [115].\nImage Dense Labelling [4.06%]. We do several image la-\nbeling tasks, including surface normal estimation [78, 204],\ndepth estimation [138], and optical flow [21, 44]. We also\ntrain our models on various segmentation tasks, including\nsemantic segmentation, localization segmentation, and re-\nferring expression segmentation.\nVideo Sparse Labelling [3.42%]. We do video detection\n[151], single object tracking [50, 79] and video action lo-\ncalization [61].\nEmbodied AI [4.33%]. For VIMA-Bench [87], we use\nthe image input as the initial observation of the environ-\nment and the image history for the images or videos in\nthe prompt.\nWe add large-scale manipulation datasets\n[63, 127, 184] with continuous control in both simulated\nand real-world environments. We also train on the Point-\nNav task from Habitat Gibson scenes.\nThe distribution of the instruction tuning data is in Fig-\nure 6. Overall, our instruction tuning mixture is composed\nof 60% prompting data, meaning supervised datasets com-\nbined with prompts. To avoid catastrophic forgetting, 30%\nof the data is carried over from pre-training.\nAddition-\nally, 6% is task augmentation data we build by constructing\nnovel tasks using existing data sources, which enhances ex-\nisting tasks and increases task diversity. The remaining 4%\nconsists of free-form text to enable chat-like responses.\n5. Experiments\nIn this section, we evaluate our pre-trained and instruction-\ntuned models on a broad range of tasks that require parsing\nand producing all modalities: images, video, audio, text,\nand actions. We do not perform task-specific finetuning\nin any experiments. Details about experimental setups, ad-\nditional result details, results on natural language tasks, and\nadditional studies for UNIFIED-IO 2’s instruction capabili-\nties are in Appendix E.\n5.1. Pre-training Evaluation\nWe demonstrate the effectiveness of our pre-training by\nevaluating UNIFIED-IO 2 on commonsense natural lan-\nguage inference (HellaSwag [214]), text-to-image gener-\nation (TIFA [76]) and text-to-audio generation (Audio-\nCaps [93]).\nWe also assess spatial and temporal under-\nstanding on SEED-Bench [106], a benchmark for compre-\nhensively evaluating perception and reasoning on image\nand video modalities. Table 2 shows that UNIFIED-IO 2\nachieves comparable or even better performance on both\ngeneration and comprehension tasks compared to the\ntask-specific specialist [154] or the universal multimodal\nmodel [9].\n8\nMethod\nHellaSwag↑TIFA↑SEED-S↑SEED-T↑AudioCaps↓\nLLaMA-7B [177]\n76.1\n-\n-\n-\n-\nOpenLLaMa-3Bv2 [55]\n52.1\n-\n-\n-\n-\nSD v1.5 [154]\n-\n78.4\n-\n-\n-\nOpenFlamingo-7B [9]\n-\n-\n34.5\n33.1\n-\nUIO-2L\n38.3\n70.2\n37.2\n32.2\n3.08\nUIO-2XL\n47.6\n77.2\n40.9\n34.0\n3.10\nUIO-2XXL\n54.3\n78.7\n40.7\n35.0\n3.02\nTable 2. Zero-shot performance on commonsense sentence com-\npletion (HellaSwag [214]), text-to-image generation (TIFA [76]),\nspatial and temporal comprehension (Seed-Bench [106]), and text-\nto-audio generation (AudioCaps [93]).\nMethod\nCat.\nLoc.\nVqa\nRef.\nSeg.\nKP\nNorm.\nAll\nAblation\nUIO-2L\n70.1\n66.1\n67.6\n66.6\n53.8\n56.8\n44.5\n60.8\nUIO-2XL\n74.2\n69.1\n69.0\n71.9\n57.3\n68.2\n46.7\n65.2\nUIO-2XXL\n74.9\n70.3\n71.3\n75.5\n58.2\n72.8\n45.2\n66.9\nTest\nGPV-2 [89]\n55.1\n53.6\n63.2\n52.1\n-\n-\n-\n-\nUIOXL [123]\n60.8\n67.1\n74.5\n78.9\n56.5\n67.7\n44.3\n64.3\nUIO-2XXL\n75.2\n70.2\n71.1\n75.5\n58.8\n73.2\n44.7\n67.0\nTable 3. Results on the GRIT ablation and test sets [66].\nDespite extensive multitasking, the results on HellaSwag\nsuggest that UNIFIED-IO 2 has language modeling capa-\nbilities between typical 3B and 7B language models. This\nmay be due to that the model sees far fewer tokens com-\npared to language-based LLMs – approximately 250 billion\ntokens in total. Qualitative results of pre-training are in Ap-\npendix E.1.\n5.2. GRIT Results\nWe evaluate on the General Robust Image Task (GRIT)\nBenchmark [66], which includes seven tasks: categoriza-\ntion, localization, VQA, referring expression, instance seg-\nmentation, keypoint, and surface normal estimation. Com-\npleting all 7 tasks requires understanding image, text, and\nsparse inputs and generating text, sparse, and dense outputs.\nAlthough this is a subset of the modalities UNIFIED-IO 2\nsupports, we evaluate on GRIT because it provides a stan-\ndardized and comprehensive benchmark on this set of capa-\nbilities. See Appendix E.3 for additional inference details\non GRIT.\nResults are shown in Table 3. Overall, UNIFIED-IO 2\nis state-of-the-art on GRIT, surpassing the previous best\nmodel, UNIFIED-IO, by 2.7 points. On individual tasks,\nwe can observe gains in localization (3 points), catego-\nrization (14 points), segmentation (2 points), and key-\npoint (5 points).\nOn VQA, our GRIT evaluations show\nUNIFIED-IO 2 is better on same-source (84.6 vs. 81.2)\nquestions, suggesting the gap is due to reduced performance\non the new-source questions that were constructed from Vi-\nsual Genome; see Appendix E.3 for additional discussion.\nDespite being slightly behind UNIFIED-IO, UNIFIED-IO 2\nMethod\nImage\nAudio\nAction\nFID↓\nTIFA↑\nFAD↓\nIS↑\nKL↓\nSucc.↑\nminDALL-E [37]\n-\n79.4\n-\n-\n-\n-\nSD-1.5 [154]\n-\n78.4\n-\n-\n-\n-\nAudioLDM-L [117]\n-\n-\n1.96\n8.13\n1.59\n-\nAudioGen [101]\n-\n-\n3.13\n-\n2.09\n-\nDiffSound [203]\n-\n-\n7.75\n4.01\n2.52\n-\nVIMA [87]\n-\n-\n-\n-\n-\n72.6\nVIMA-IMG [87]\n-\n-\n-\n-\n-\n42.5\nCoDi [174]\n11.26\n71.6\n1.80\n8.77\n1.40\n-\nEmu [172]\n11.66\n65.5\n-\n-\n-\n-\nUIO-2L\n16.68\n74.3\n2.82\n5.37\n1.93\n50.2\nUIO-2XL\n14.11\n80.0\n2.59\n5.11\n1.74\n54.2\nUIO-2XXL\n13.39\n81.3\n2.64\n5.89\n1.80\n56.3\nTable 4. Results on text-to-image generation (MS COCO [115]\nand TIFA [76]), text-to-audio generation (AudioCaps [93]) and ac-\ntion generation (VIMA-Bench [87]).\nstill obtains strong referring expression scores that compare\nfavorably to prior work on generalist multimodal models,\nsee Table 5. Surpassing UNIFIED-IO while also support-\ning much higher quality image and text generation, along\nwith many more tasks and modalities, illustrates the impres-\nsive multi-tasking capabilities of our model. UNIFIED-IO 2\neven maintains better overall performance with the 3-billion\nparameter model (65.2 vs. 64.5), which is roughly equal in\nsize to UNIFIED-IO. Ablation results show average perfor-\nmance, and all individual tasks improve with model size,\nshowing that UNIFIED-IO 2 benefits from scale.\n5.3. Generation Results\nTable 4 shows results on tasks that require generating\nimage, audio, and action outputs.\nWe evaluate using\nTIFA [76], which measures faithfulness to the prompt us-\ning VQA models and has been shown to correlate well\nwith human judgments, and FID [73] on MS COCO [115].\nOn TIFA, we find that UNIFIED-IO 2 scores close to\nminDALL-E [37], and about 10 points ahead of other gen-\neralist models such as CoDi [174] and Emu [172]. We at-\ntribute this strong image generation ability to extensive pre-\ntraining and the use of a fine-grained VQ-GAN. We include\nexamples of our generation results from the TIFA bench-\nmark in the Appendix E.5. UNIFIED-IO 2’s FID scores are\nslightly higher than the compared models, although we note\nthat qualitatively the generated images are still very smooth\nand detailed.\nFor text-to-audio generation, we evaluate on the Audio-\nCaps [93] test set. AudioCaps consists of 10-second audio\nclips, while our model can generate 4.08-second audio at\na time, so we cannot do a direct evaluation on this bench-\nmark. Instead, we generate an audio segment based on the\ntext description and previous audio segments as additional\n9\nMethod\nVQAv2\nOKVQA\nSQA\nSQAI\nTally-QA RefCOCO RefCOCO+ RefCOCO-g COCO-Cap. POPE SEED MMB\nInstructBLIP (8.2B)\n-\n-\n-\n79.5\n68.2†\n-\n-\n-\n102.2\n-\n53.4\n36\nShikra (7.2B)\n77.4\n47.2\n-\n-\n-\n87.0\n81.6\n82.3\n117.5\n84.7\n-\n58.8\nFerret (7.2B)\n-\n-\n-\n-\n-\n87.5\n80.8\n83.9\n-\n85.8\n-\n-\nQwen-VL (9.6B)\n78.8\n58.6\n-\n67.1∗\n-\n89.4\n83.1\n85.6\n131.9\n-\n38.2\nmPLUG-Owl2 (8.2B)\n79.4\n57.7\n-\n68.7∗\n-\n-\n-\n-\n137.3\n86.2\n57.8\n64.5\nLLaVa-1.5 (7.2B)\n78.5\n-\n-\n66.8∗\n-\n-\n-\n-\n-\n85.9\n58.6\n64.3\nLLaVa-1.5 (13B)\n80.0\n-\n-\n71.6∗\n72.4†\n-\n-\n-\n-\n85.9\n61.6\n67.7\nSingle Task SoTA\n86.0 [29] 66.8 [77] 90.9 [119] 90.7 [34] 82.4 [77] 92.64 [202] 88.77 [187] 89.22 [187]\n149.1 [29]\n-\n-\n-\nUIO-2L (1.1B)\n75.3\n50.2\n81.6\n78.6\n69.1\n84.1\n71.7\n79.0♢\n128.2\n77.8\n51.1\n62.1\nUIO-2XL (3.2B)\n78.1\n53.7\n88.8\n87.4\n72.2\n88.2\n79.8\n84.0♢\n130.3\n87.2\n60.2\n68.1\nUIO-2XXL (6.8B)\n79.4\n55.5\n88.7\n86.2\n75.9\n90.7\n83.1\n86.6♢\n125.4\n87.7\n61.8\n71.5\nTable 5. Vision-language results on nine tasks [1, 28, 59, 91, 124, 129, 130, 136, 209] and three evaluation-only benchmarks [106, 113, 120].\nResults marked with ∗are zero-shot and † are evaluated with the open-source releases, and ♢indicates that our RefCOCO-g results are on\nthe Google split rather than the UMD split.\ninput; see Appendix E.6 for more details. While this is not\na directly comparable setup to related work, it still gives\na reasonable quantitative measure of our audio generation\nabilities. UNIFIED-IO 2 scores higher then specialist mod-\nels except the recent latent diffusion model [117], which\nshows it’s competitive audio generation ability.\nFor action, we evaluate using VIMA-Bench [87], a robot\nmanipulation benchmark containing 17 tasks with text-\nimage interleaved prompts. Since VIMA’s action space is\naction primitives, UNIFIED-IO 2 directly predicts all ac-\ntions at once given the initial observation and multimodal\nprompt. We report the average success rate for 4-level eval-\nuation protocol [87] and compare with the original casual\nVIMA policy with object-centric inputs, as well as VIMA-\nIMG, a Gato [152]-like policy with image inputs like ours.\n5.4. Vision Language Results\nWe evaluate vision language performance and compare it\nagainst other vision\/language generalist models, i.e., mod-\nels that are also designed to perform many tasks and can\nfollow instructions.\nResults on a collection of 12 vi-\nsion\/language benchmarks are shown in Table 5. SoTA re-\nsults from specialist models are shown for reference.\nUNIFIED-IO 2 achieves strong results on VQA, only\npassed by the much larger 13B LLaVa model [118] on VQA\nv2 [59], and ahead of all other generalist models on Sci-\nenceQA [124] and TallyQA [1]. OK-VQA [130] is the ex-\nception. We hypothesize that because it requires external\nknowledge, extensive language pre-training is important for\nthis task, and therefore our reduced performance is since\nUNIFIED-IO 2 was not pre-trained as extensively on text as\nthe dedicated language models used by Qwen-VL [12] and\nmPLUG-Owl2 [206].\nOn referring expression, UNIFIED-IO 2 is ahead of\nShikra [26] and Ferret [207] and matches the scores\nachieved by Qwen-VL. On captioning, UNIFIED-IO 2 also\nVideo\nAudio\nMethod\nKinetics-400 [90]\nVATEXCaption [190]\nMSR-VTT [199]\nMSRVTT-QA [198]\nMSVD-QA [198]\nSTAR [196]\nSEED-T [106]\nVGG-Sound [24]\nAudioCaps [93]\nKinetics-Sounds [7]\nMBT [137]\n-\n-\n-\n-\n-\n-\n-\n52.3\n-\n85.0\nCoDi [174]\n-\n-\n74.4\n-\n-\n-\n-\n-\n78.9\n-\nImageBind [69]∗\n50.0\n-\n-\n-\n-\n-\n-\n27.8\n-\n-\nBLIP-2 [109]∗\n-\n-\n-\n9.2\n18.3\n-\n36.7\n-\n-\n-\nInstructBLIP [34]∗\n-\n-\n-\n22.1 41.8\n-\n38.3\n-\n-\n-\nEmu [172]∗∗\n-\n-\n-\n24.1 39.8\n-\n-\n-\n-\n-\nFlamingo-9B [5]∗∗\n-\n57.4\n-\n29.4 47.2 41.2\n-\n-\nFlamingo-80B [5]\n-\n84.2\n-\n47.4\n-\n-\n-\n-\n-\n-\nUIO-2L\n68.5 37.1 44.0 39.6 48.2 51.0 37.5 37.8 45.7 86.1\nUIO-2XL\n71.4 41.6 47.1 39.3 50.4 52.0 45.6 44.2 45.7 88.0\nUIO-2XXL\n73.8 45.6 48.8 41.5 52.1 52.2 46.8 47.7 48.9 89.3\nTable 6. Results on action classification, video captioning, VQA,\nvisual comprehension, audio classification, and audio captioning.\n∗: zero-shot, ∗∗: few-shot in-context learning.\nachieves a strong CIDEr score [182] of 130.3, ahead of\nShikra and InstructBLIP [34] but behind Qwen-VL and\nmPLUG-Owl2.\nFinally, we evaluate using three recently proposed\nevaluation-only benchmarks.\nMMB (MMBench [120])\ntests multiple facets of vision language understanding with\nmultiple choice questions, while SEED-Bench additionally\ntests video understanding. We show a detailed breakdown\nof our score in the Appendix E.4.\nRegarding the over-\nall score, UNIFIED-IO 2 has the strongest score of any\n7B model on the SEED-Bench leaderboard1, and scores\nthe highest on MMB by 3.8 points.\nNotably, it excels\nLLaVa-1.5 13B model in both benchmarks. UNIFIED-IO 2\n1as of 11\/17\/23\n10\nAP3D AP3D@15 AP3D@25 AP3D@50\nCube-RCNN [16]\n50.8\n65.7\n54.0\n22.5\nUIO-2L\n42.9\n54.4\n45.7\n21.7\nUIO-2XL\n43.3\n54.4\n46.8\n21.8\nUIO-2XXL\n42.4\n54.0\n45.6\n20.9\nTable 7. Single-object 3D detection results on Objectron [3].\nalso reaches 87.7 on the POPE object hallucination bench-\nmark [113], showing that it is not very prone to object hal-\nlucination.\nOverall, UNIFIED-IO 2 can match or surpass other vi-\nsion & language generalist models on these benchmarks\ndespite encompassing many more modalities and support-\ning high-quality image and audio generation. This shows\nthat its wide breadth of capabilities does not come at the\nexpense of vision\/language performance.\n5.5. Video, Audio and other Results\nUNIFIED-IO 2 shows reasonable performance on au-\ndio and video classification and captioning, as well as\nvideo question answering, as shown in Table 6.\nNo-\ntably, UNIFIED-IO 2 outperforms BLIP-2 [109] and In-\nstructBLIP [34] on Seed-Bench Temporal [106] by 8.5\npoints.\nUNIFIED-IO 2 also achieves better performance\non Kinetics-Sounds [7] than MBT [137], which is trained\nsolely on that dataset.\nWe show the single-object 3D detection results in Ta-\nble 7. Our model shows decent results, similar to Cube-\nRCNN [16], on the Objectron benchmark [3]. However, its\nperformance drops significantly in multi-object 3D detec-\ntion tasks, like those on nuScenes [22] and Hypersim [153].\nThis could be because only 1.0% of our training data fo-\ncuses on 3D detection. A potential solution might be to\ncombine 2D and 3D detection techniques.\nIn COCO object detection, excluding the ‘stuff’ cate-\ngories, our model reached an average precision (AP) of\n47.2, with AP50 at 57.7 and AP75 at 50.0. However, it\nhas difficulties with images containing many objects. Previ-\nous research, like Pix2Seq [27], suggests that autoregressive\nmodels face similar challenges, which can be improved with\nextensive data augmentation. Our model’s data augmenta-\ntion on object detection is comparatively more limited.\nOur model shows weak performance in depth estimation,\nwith an RMSE of 0.623 on NYUv2 depth dataset [138].\nHowever, fine-tuning specifically for this task improved\nthe RMSE to 0.423. In our experiment, we simply nor-\nmalize the depth map with the max depth value in each\ndataset. Due to the incompatibility of dense ground-truth\ndepth across different datasets [150], our model failed to\ncapture the exact scale in the current prompt, which could\npotentially be solved by using better normalization and met-\nric evaluation.\nAppendix E shows qualitative visualizations of other\ntasks, such as single object tracking, future state prediction\nof robotic manipulation, and image-based 3D view synthe-\nsis, etc.\n6. Limitation\n• Due to memory constraints, we use the base versions of\nthe ViT and AST models for image and audio features\nthroughout the project. Using a larger version of these im-\nage and audio encoders could substantially improve per-\nformance.\n• While our image generation is more faithful compared to\nSD-based methods, its quality doesn’t match that of the\nstable diffusion model. Additionally, our audio genera-\ntion is capped at approximately 4 seconds, which restricts\nthe practical application of the audio outputs.\n• Limited computational resources constrained our explo-\nration of the model’s hyperparameters.\nIt’s likely that\nusing a significantly larger batch size could enhance the\nmodel’s performance.\n• Our model is much less reliable for modalities like depth,\nvideo or when requiring more niche abilities like 3D ob-\nject detection, etc. This is probably due to the limited\nvariety of tasks we have in these areas.\n• Improving the quality of our data could enhance the\nmodel’s performance. However, despite considerable ef-\nforts, our human-written prompts still fall short in diver-\nsity. We notice a notable decrease in the model’s perfor-\nmance when dealing with new instruction tasks, as op-\nposed to those it was trained on.\n7. Conclusion\nWe introduced UNIFIED-IO 2, the first autoregressive mul-\ntimodal model that is capable of understanding and generat-\ning image, text, audio, and action. This model was trained\nfrom scratch on a wide range of multimodal data and fur-\nther refined with instruction tuning on a massive multimodal\ncorpus. We developed various architectural changes to sta-\nbilize the multimodal training and proposed a multimodal\nmixture of denoiser objective to effectively utilize the multi-\nmodal signals. Our model achieves promising results across\na wide range of tasks. We show that going from LLMs to\nLMMs enables new capabilities and opportunities. In the\nfuture, we would like to extend UNIFIED-IO 2 from the\nencoder-decoder model to a decoder-only model. Addition-\nally, we plan to expand the model’s size, enhance the data\nquality, and refine the overall model design.\nAcknowledgement We thank Klemen Kotar for helping gather Embodied\nAI pre-training data, Jonathan Frankle from MosaicML for suggesting the\nmixture of NLP pre-training data, Jack Hessel for interleaved image &\ntext dataset and Micheal Schmitz for helping support the compute infras-\ntructure. We also thank Tanmay Gupta for helpful discussions, as well\nas Hamish Ivison, and Ananya Harsh Jha for their insightful discussions\n11\nabout model design. We additionally thank Oscar Michel, Yushi Hu and\nYanbei Chen for their help editing the paper, and Matt Deitke for help\nsetting up the webpage. Savya Khosla and Derek Hoiem were supported\nin part by ONR award N00014-23-1-2383.\nThis research was made\npossible with cloud TPUs from Google’s TPU Research Cloud (TRC).\nReferences\n[1] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tal-\nlyQA: Answering Complex Counting Questions. In AAAI,\n2019. 10, 29\n[2] Andrea Agostinelli, Timo I Denk, Zal´an Borsos, Jesse En-\ngel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,\nAren Jansen, Adam Roberts, Marco Tagliasacchi, et al.\nMusicLM: Generating Music From Text.\narXiv preprint\narXiv:2301.11325, 2023. 7, 29\n[3] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski,\nJianing Wei, and Matthias Grundmann. Objectron: A Large\nScale Dataset of Object-Centric Videos in the Wild with\nPose Annotations. In CVPR, 2021. 11, 37, 38\n[4] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan,\nand Barlas Oguz.\nJointly Training Large Autoregressive\nMultimodal Models.\narXiv preprint arXiv:2309.15564,\n2023. 3\n[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a Visual Language Model for Few-Shot Learn-\ning. In NeurIPS, 2022. 2, 4, 10, 22, 23, 37\n[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In ICCV, 2015.\n7\n[7] Relja Arandjelovic and Andrew Zisserman. Look, Listen\nand Learn. In ICCV, 2017. 8, 10, 11, 30\n[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\nProgram Synthesis with Large Language Models.\narXiv\npreprint arXiv:2108.07732, 2021. 27\n[9] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, et al.\nOpen-\nFlamingo: An Open-Source Framework for Training Large\nAutoregressive Vision-Language Models.\narXiv preprint\narXiv:2308.01390, 2023. 8, 9, 23\n[10] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer Normalization. In NeurIPS Deep Learning Sympo-\nsium, 2016. 5\n[11] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Esti-\nmating and Exploiting the Aleatoric Uncertainty in Surface\nNormal Estimation. In ICCV, 2021. 33\n[12] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-VL: A Versatile Vision-Language Model for\nUnderstanding, Localization, Text Reading, and Beyond.\narXiv preprint arXiv:2308.12966, 2023. 3, 10, 34\n[13] Max Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisser-\nman. Frozen in Time: A Joint Video and Image Encoder\nfor End-to-End Retrieval. In ICCV, 2021. 7, 26\n[14] Zal´an Borsos, Rapha¨el Marinier, Damien Vincent, Eugene\nKharitonov, Olivier Pietquin, Matt Sharifi, Dominik Rob-\nlek, Olivier Teboul, David Grangier, Marco Tagliasacchi,\net al. AudioLM: A Language Modeling Approach to Au-\ndio Generation. IEEE\/ACM Transactions on Audio, Speech,\nand Language Processing, 31:2523–2533, 2023. 2\n[15] James\nBradbury,\nRoy\nFrostig,\nPeter\nHawkins,\nMatthew James Johnson, Chris Leary, Dougal Maclaurin,\nGeorge Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang.\nJax: composable\ntransformations of python+numpy programs, 2018. 23\n[16] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila\nRavi, Justin Johnson, and Georgia Gkioxari. Omni3D: A\nLarge Benchmark and Model for 3D Object Detection in\nthe Wild. In CVPR, 2023. 3, 8, 11, 21, 29\n[17] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2:\nVision-Language-Action Models Transfer Web Knowledge\nto Robotic Control. In CoRL, 2023. 3\n[18] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to Follow Image Editing Instruc-\ntions. In CVPR, 2023. 7, 29\n[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage Models are Few-Shot Learners.\nIn NeurIPS,\n2020. 5, 37\n[20] S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\nSparks\nof Artificial General Intelligence: Early experiments with\nGPT-4. arXiv preprint arXiv:2303.12712, 2023. 3\n[21] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and\nMichael J Black. A Naturalistic Open Source Movie for\nOptical Flow Evaluation. In ECCV, 2012. 8, 30\n[22] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuScenes: A mul-\ntimodal dataset for autonomous driving. In CVPR, 2020.\n11\n[23] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12M: Pushing Web-Scale Image-Text\nPre-Training To Recognize Long-Tail Visual Concepts. In\nCVPR, 2021. 6, 26\n[24] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman.\nVGGSound:\nA Large-Scale Audio-Visual\nDataset. In ICASSP, 2020. 8, 10, 30\n[25] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan Zhang,\nRaghuraman Krishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMiniGPT-v2: Large Language Model As a Unified In-\nterface for Vision-Language Multi-task Learning.\narXiv\npreprint arXiv:2310.09478, 2023. 3\n12\n[26] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing Multi-\nmodal LLM’s Referential Dialogue Magic. arXiv preprint\narXiv:2306.15195, 2023. 3, 10, 34\n[27] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A Language Modeling Framework\nfor Object Detection. In ICLR, 2022. 3, 11\n[28] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll´ar, and C. Lawrence\nZitnick. Microsoft COCO Captions: Data Collection and\nEvaluation Server. arXiv preprint arXiv:1504.00325, 2015.\n10\n[29] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPaLI-X:\nOn Scaling up a Multilingual Vision and Language Model.\narXiv preprint arXiv:2305.18565, 2023. 10\n[30] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom\nKwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolQ: Exploring the Surprising Difficulty of Natural\nYes\/No Questions. In NAACL-HLT, 2019. 33\n[31] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\nThink you have Solved Question Answering?\nTry\nARC, the AI2 Reasoning Challenge.\narXiv preprint\narXiv:1803.05457, 2018. 33\n[32] Together Computer.\nRedPajama:\nan Open Dataset\nfor Training Large Language Models.\nhttps : \/ \/\ngithub.com\/togethercomputer\/RedPajama-\nData, 2023. 6, 26\n[33] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei\nZaharia, and Reynold Xin.\nFree Dolly:\nIntroducing\nthe World’s First Truly Open Instruction-Tuned LLM.\nhttps:\/\/www.databricks.com\/blog\/2023\/\n04 \/ 12 \/ dolly - first - open - commercially -\nviable-instruction-tuned-llm, 2023. 7, 27\n[34] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructBLIP: Towards General-\npurpose Vision-Language Models with Instruction Tuning.\nIn NeurIPS, 2023. 3, 10, 11, 34\n[35] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nAntonino Furnari, Jian Ma, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and\nMichael Wray. Rescaling Egocentric Vision: Collection,\nPipeline and Challenges for EPIC-KITCHENS-100. Inter-\nnational Journal of Computer Vision, 130:33–55, 2022. 8,\n30\n[36] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos´e MF Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In CVPR, 2017. 29\n[37] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah,\nTanishq Abraham, Ph´uc Lˆe Khac, Luke Melas, and Rito-\nbrata Ghosh. DALL·E Mini, 2021. 9, 34\n[38] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling Vision Transformers to 22 Billion\nParameters. In ICML, 2023. 5\n[39] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,\nKiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,\nAniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR:\nLarge-Scale Embodied AI Using Procedural Generation. In\nNeurIPS, 2022. 7, 25, 26\n[40] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA Universe of Annotated 3D Objects. In CVPR, 2023. 7, 8,\n25, 26, 29, 37, 38\n[41] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR, 2009. 7, 22\n[42] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-\nson. RedCaps: Web-curated image-text data created by the\npeople, for the people. In NeurIPS Datasets and Bench-\nmarks Track, 2021. 6, 26\n[43] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. CogView: Mastering Text-to-Image\nGeneration via Transformers. In NeurIPS, 2021. 36\n[44] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. FlowNet:\nLearning Optical Flow with Convolutional Networks. In\nICCV, 2015. 8, 30\n[45] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale.\nIn ICLR,\n2021. 22\n[46] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-\nE: An Embodied Multimodal Language Model. In ICML,\n2023. 2, 32\n[47] Konstantinos Drossos, Samuel Lipping, and Tuomas Virta-\nnen. Clotho: An Audio Captioning Dataset. In ICASSP,\n2020. 7, 8, 29\n[48] Gerald M Edelman. Neural Darwinism: Selection and reen-\ntrant signaling in higher brain function. Neuron, 10(2):115–\n125, 1993. 2\n[49] Patrick Esser, Robin Rombach, and Bjorn Ommer. Tam-\ning Transformers for High-Resolution Image Synthesis. In\nCVPR, 2021. 3, 22\n[50] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge\nDeng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu,\net al. LaSOT: A High-quality Large-scale Single Object\nTracking Benchmark. International Journal of Computer\nVision, 129:439–461, 2021. 8, 30, 37\n[51] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An-\nthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang,\n13\nLaria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin\nWang, and Andy Zou. A framework for few-shot language\nmodel evaluation, 2021. 33\n[52] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al.\nLLaMA-Adapter V2:\nParameter-\nEfficient Visual Instruction Model.\narXiv preprint\narXiv:2304.15010, 2023. 3\n[53] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a SEED of Vision in Large Language Model.\narXiv preprint arXiv:2307.08041, 2023. 3\n[54] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman,\nAren Jansen, Wade Lawrence, R. Channing Moore, Manoj\nPlakal, and Marvin Ritter. Audio Set: An Ontology and\nHuman-Labeled Dataset for Audio Events.\nIn ICASSP,\n2017. 7, 8, 22, 26, 30\n[55] Xinyang Geng and Hao Liu.\nOpenLLaMA: An Open\nReproduction of LLaMA.\nhttps:\/\/github.com\/\nopenlm-research\/open_llama, 2023. 9\n[56] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat\nSingh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan\nMisra. ImageBind: One Embedding Space To Bind Them\nAll. In CVPR, 2023. 3\n[57] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio\nSpectrogram Transformer. In Interspeech, 2021. 4, 22\n[58] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al. The” something something” video\ndatabase for learning and evaluating visual common sense.\nIn ICCV, 2017. 30\n[59] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the V in VQA Matter: Ele-\nvating the Role of Image Understanding in Visual Question\nAnswering. In CVPR, 2017. 10\n[60] Kristen Grauman,\nAndrew Westbury,\nEugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\nEgo4D: Around the World in 3,000 Hours of Egocentric\nVideo. In CVPR, 2022. 7, 26\n[61] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-\noline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,\nGeorge Toderici, Susanna Ricco, Rahul Sukthankar, et al.\nAVA: A Video Dataset of Spatio-temporally Localized\nAtomic Visual Actions. In CVPR, 2018. 8, 30\n[62] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A\nDataset for Large Vocabulary Instance Segmentation. In\nCVPR, 2019. 29\n[63] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey\nLevine, and Karol Hausman. Relay Policy Learning: Solv-\ning Long-Horizon Tasks via Imitation and Reinforcement\nLearning. In CoRL, 2019. 8, 31\n[64] Tanmay Gupta and Aniruddha Kembhavi. Visual Program-\nming: Compositional visual reasoning without training. In\nCVPR, 2023. 2\n[65] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and\nDerek Hoiem. Towards General Purpose Vision Systems:\nAn End-to-End Task-Agnostic Vision-Language Architec-\nture. In CVPR, 2022. 33\n[66] Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and\nDerek Hoiem. GRIT: General Robust Image Task Bench-\nmark. arXiv preprint arXiv:2204.13653, 2022. 2, 9, 22\n[67] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizWiz Grand Challenge:\nAnswering Visual Questions\nfrom Blind People. In CVPR, 2018. 29\n[68] Ivan Habernal,\nOmnia Zayed,\nand Iryna Gurevych.\nC4Corpus: Multilingual Web-size Corpus with Free Li-\ncense. In LREC, 2016. 6, 26\n[69] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng\nXu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu\nGuo, et al. ImageBind-LLM: Multi-modality Instruction\nTuning. arXiv preprint arXiv:2309.03905, 2023. 3, 10\n[70] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 33\n[71] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked Autoencoders Are Scal-\nable Vision Learners. In CVPR, 2022. 24\n[72] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\nsuring Massive Multitask Language Understanding.\nIn\nICLR, 2021. 33\n[73] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs Trained by\na Two Time-Scale Update Rule Converge to a Local Nash\nEquilibrium. In NeurIPS, 2017. 9, 36\n[74] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. In NeurIPS Workshop on Deep Generative Mod-\nels and Downstream Applications, 2021. 29, 34, 36\n[75] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The Curious Case of Neural Text Degeneration. In\nICLR, 2020. 29\n[76] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari\nOstendorf, Ranjay Krishna, and Noah A. Smith. TIFA: Ac-\ncurate and Interpretable Text-to-Image Faithfulness Evalu-\nation with Question Answering. In ICCV, 2023. 2, 8, 9, 34,\n35\n[77] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy\nViswanathan, Kenji Hata, Enming Luo, Ranjay Krishna,\nand Ariel Fuxman. Visual Program Distillation: Distilling\nTools and Programmatic Reasoning into Vision-Language\nModels. arXiv preprint arXiv:2312.03052, 2023. 10\n[78] Jingwei Huang, Yichao Zhou, Thomas Funkhouser, and\nLeonidas J Guibas. FrameNet: Learning Local Canonical\nFrames of 3D Surfaces from a Single RGB Image. In ICCV,\n2019. 8, 30\n[79] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k:\nA Large High-Diversity Benchmark for Generic Object\nTracking in the Wild. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 43(5):1562–1577, 2019. 8,\n30\n[80] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi,\nXuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,\nJiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding\n14\nand Generating Speech, Music, Sound, and Talking Head.\narXiv preprint arXiv:2304.12995, 2023. 3\n[81] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al. Language Is Not\nAll You Need: Aligning Perception with Language Models.\nIn NeurIPS, 2023. 2, 3\n[82] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh,\nIshan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Gir-\nshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al.\nVisual Storytelling. In NAACL-HLT, 2016. 29\n[83] Drew A Hudson and Christopher D Manning.\nGQA: A\nNew Dataset for Real-World Visual Reasoning and Com-\npositional Question Answering. In CVPR, 2019. 29\n[84] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nCLIP, 2021. 3\n[85] Keith\nIto\nand\nLinda\nJohnson.\nThe\nLJ\nSpeech\nDataset.\nhttps:\/\/keithito.com\/LJ-Speech-\nDataset\/, 2017. 7, 22, 29\n[86] Andrew\nJaegle,\nSebastian\nBorgeaud,\nJean-Baptiste\nAlayrac, Carl Doersch, Catalin Ionescu, David Ding,\nSkanda Koppula, Daniel Zoran, Andrew Brock, Evan\nShelhamer, et al. Perceiver IO: A General Architecture for\nStructured Inputs & Outputs. In ICLR, 2022. 5\n[87] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,\nYongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anand-\nkumar, Yuke Zhu, and Linxi Fan. VIMA: General Robot\nManipulation with Multimodal Prompts. In ICML, 2023.\n8, 9, 10, 21, 30, 37\n[88] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan,\nBin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang\nLei, et al. Unified Language-Vision Pretraining in LLM\nwith Dynamic Discrete Visual Tokenization. arXiv preprint\narXiv:2309.04669, 2023. 3\n[89] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric\nKolve, Derek Hoiem, and Aniruddha Kembhavi.\nWebly\nSupervised Concept Expansion for General Purpose Vision\nModels. In ECCV, 2022. 9, 33\n[90] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Vi-\nola, Tim Green, Trevor Back, Paul Natsev, et al.\nThe\nKinetics Human Action Video Dataset.\narXiv preprint\narXiv:1705.06950, 2017. 10, 36\n[91] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. ReferItGame: Referring to Objects in Pho-\ntographs of Natural Scenes. In EMNLP, 2014. 8, 10, 29,\n30\n[92] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi. Fr´echet Audio Distance: A Reference-\nFree Metric for Evaluating Music Enhancement Algo-\nrithms. In Interspeech, 2019. 36\n[93] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and\nGunhee Kim. AudioCaps: Generating Captions for Audios\nin The Wild. In NAACL-HLT, 2019. 7, 8, 9, 10, 29, 36\n[94] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,\nand Ross Girshick. Segment Anything. In ICCV, 2023. 7,\n26, 33\n[95] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,\nChenghao Mou, Carlos Mu˜noz Ferrandis, Yacine Jernite,\nMargaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry\nBahdanau, Leandro von Werra, and Harm de Vries. The\nStack: 3 TB of permissively licensed source code. Trans-\nactions on Machine Learning Research, 2023. 26\n[96] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating Images with Multimodal Language Models.\nIn\nNeurIPS, 2023. 2, 3\n[97] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding Language Models to Images for Multimodal In-\nputs and Outputs. In ICML, 2023. 3\n[98] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.\nHiFi-\nGAN: Generative Adversarial Networks for Efficient and\nHigh Fidelity Speech Synthesis. In NeurIPS, 2020. 22\n[99] Andreas K¨opf, Yannic Kilcher, Dimitri von R¨utte, Sotiris\nAnagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah\nBarhoum, Nguyen Minh Duc, Oliver Stanley, Rich´ard\nNagyfi, et al.\nOpenAssistant Conversations – Democra-\ntizing Large Language Model Alignment.\nIn NeurIPS\nDatasets and Benchmarks Track, 2023. 27\n[100] Mario Michael Krell, Matej Kosec, Sergio P Perez, and\nAndrew Fitzgibbon.\nEfficient Sequence Packing with-\nout Cross-contamination:\nAccelerating Large Language\nModels without Impacting Performance.\narXiv preprint\narXiv:2107.02027, 2021. 6\n[101] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,\nAlexandre D´efossez, Jade Copet, Devi Parikh, Yaniv Taig-\nman, and Yossi Adi. AudioGen: Textually Guided Audio\nGeneration. In ICLR, 2023. 9\n[102] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein,\nand Fei-Fei Li. Visual Genome: Connecting Language and\nVision Using Crowdsourced Dense Image Annotations. In-\nternational Journal of Computer Vision, 2017. 7, 27\n[103] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, Tom\nDuerig, and Vittorio Ferrari. The Open Images Dataset V4:\nUnified image classification, object detection, and visual re-\nlationship detection at scale. International Journal of Com-\nputer Vision, 128(7):1956–1981, 2020. 7, 22, 27\n[104] Hugo Laurenc¸on, Lucile Saulnier, L´eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M Rush, Douwe Kiela,\net al. OBELICS: An Open Web-Scale Filtered Dataset of\nInterleaved Image-Text Documents. In NeurIPS Datasets\nand Benchmarks Track, 2023. 6, 24, 26\n[105] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim,\nThomas Breuel, Gal Chechik, and Yale Song. ACAV100M:\nAutomatic Curation of Large-Scale Datasets for Audio-\n15\nVisual Video Representation Learning. In ICCV, 2021. 7,\n22, 26\n[106] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan.\nSEED-Bench: Benchmarking\nMultimodal LLMs with Generative Comprehension. arXiv\npreprint arXiv:2307.16125, 2023. 8, 9, 10, 11, 34\n[107] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nFanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.\nMIMIC-IT: Multi-Modal In-Context Instruction Tuning.\narXiv preprint arXiv:2306.05425, 2023. 8, 29\n[108] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter: A Multi-Modal\nModel with In-Context Instruction Tuning. arXiv preprint\narXiv:2305.03726, 2023. 29, 34\n[109] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models. In\nICML, 2023. 3, 10, 11\n[110] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideoChat:\nChat-Centric Video Understanding.\narXiv\npreprint arXiv:2305.06355, 2023. 3, 34\n[111] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nLimin Wang, and Yu Qiao. UniFormerV2: Unlocking the\nPotential of Image ViTs for Video Understanding. In ICCV,\n2023. 8, 30, 37\n[112] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,\nShuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu\nSun, et al. M3IT: A Large-Scale Dataset towards Multi-\nModal Multilingual Instruction Tuning.\narXiv preprint\narXiv:2306.04387, 2023. 3, 8, 29\n[113] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji rong Wen. Evaluating Object Hallucination in\nLarge Vision-Language Models. In EMNLP, 2023. 10, 11,\n34\n[114] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw,\nYudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe\nMorency, and Russ Salakhutdinov.\nHigh-Modality Mul-\ntimodal Transformer: Quantifying Modality & Interaction\nHeterogeneity for High-Modality Representation Learning.\nTMLR, 2023. 3\n[115] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and\nC Lawrence Zitnick. Microsoft COCO: Common Objects\nin Context. In ECCV, 2014. 7, 8, 9, 27, 34\n[116] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual Spa-\ntial Reasoning. Transactions of the Association for Compu-\ntational Linguistics, 11:635–651, 2023. 29\n[117] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley. Au-\ndioLDM: Text-to-Audio Generation with Latent Diffusion\nModels. In ICML, 2023. 9, 10, 36\n[118] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved Baselines with Visual Instruction Tuning. arXiv\npreprint arXiv:2310.03744, 2023. 3, 10, 34\n[119] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual Instruction Tuning. In NeurIPS, 2023. 2, 3, 8,\n10, 29\n[120] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MM-\nBench: Is Your Multi-modal Model an All-around Player?\narXiv preprint arXiv:2307.06281, 2023. 10\n[121] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\net al. Swin Transformer V2: Scaling Up Capacity and Res-\nolution. In CVPR, 2022. 5\n[122] Shayne Longpre,\nLe Hou,\nTu Vu,\nAlbert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Bar-\nret Zoph, Jason Wei, and Adam Roberts. The Flan Collec-\ntion: Designing Data and Methods for Effective Instruction\nTuning. In ICML, 2023. 3, 7, 27\n[123] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. Unified-IO: A Uni-\nfied Model for Vision, Language, and Multi-Modal Tasks.\nIn ICLR, 2023. 2, 3, 4, 7, 9, 22\n[124] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to Explain: Multimodal Reasoning\nvia Thought Chains for Science Question Answering. In\nNeurIPS, 2022. 10, 29\n[125] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon: Plug-and-Play Compositional Reasoning with\nLarge Language Models. In NeurIPS, 2023. 2\n[126] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-\nHui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Val-\nley: Video Assistant with Large Language model Enhanced\nabilitY. arXiv preprint arXiv:2306.07207, 2023. 3\n[127] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli\nDing, James Betker, Robert Baruch, Travis Armstrong, and\nPete Florence. Interactive Language: Talking to Robots in\nReal Time. IEEE Robotics and Automation Letters, 2023.\n8, 31\n[128] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\nFahad Shahbaz Khan. Video-ChatGPT: Towards Detailed\nVideo Understanding via Large Vision and Language Mod-\nels. arXiv preprint arXiv:2306.05424, 2023. 34\n[129] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan Yuille, and Kevin Murphy. Generation and\nComprehension of Unambiguous Object Descriptions. In\nCVPR, 2016. 10\n[130] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. OK-VQA: A Visual Question Answer-\ning Benchmark Requiring External Knowledge. In CVPR,\n2019. 10, 29\n[131] Irene Martin Morato and Annamaria Mesaros. Diversity\nand Bias in Audio Captioning Datasets. In DCASE, 2021.\n7, 29\n[132] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie\nPavlick. Linearly Mapping from Image to Text Space. In\nICLR, 2023. 3\n[133] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty.\nOCR-VQA: Visual Question An-\nswering by Reading Text in Images. In ICDAR, 2019. 29\n16\n[134] Utkarsh Mishra, Shangjie Xue, Yongxin Chen, and Danfei\nXu. Generative Skill Chaining: Long-Horizon Skill Plan-\nning with Diffusion Models. In CoRL, 2023. 32\n[135] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang,\nMingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and\nPing Luo. EmbodiedGPT: Vision-Language Pre-Training\nvia Embodied Chain of Thought. In NeurIPS, 2023. 3\n[136] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.\nModeling Context Between Objects for Referring Expres-\nsion Understanding. In ECCV, 2016. 10, 29, 30\n[137] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,\nCordelia Schmid, and Chen Sun. Attention Bottlenecks for\nMultimodal Fusion. In NeurIPS, 2021. 10, 11\n[138] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\nFergus. Indoor Segmentation and Support Inference from\nRGBD Images. In ECCV, 2012. 8, 11, 30\n[139] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDINOv2: Learning Robust Visual Features without Super-\nvision. arXiv preprint arXiv:2304.07193, 2023. 33\n[140] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bew-\nley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant\nRai, Anikait Singh, Anthony Brohan, et al.\nOpen X-\nEmbodiment: Robotic Learning Datasets and RT-X Mod-\nels. In CoRL Workshop TGR, 2023. 3\n[141] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-G: Generating Im-\nages in Context with Multimodal Large Language Models.\narXiv preprint arXiv:2310.02992, 2023. 3\n[142] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley,\nand Jianfeng Gao. Instruction Tuning with GPT-4. arXiv\npreprint arXiv:2304.03277, 2023. 7, 27\n[143] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nMultimodal Large Language Models to the World. arXiv\npreprint arXiv:2306.14824, 2023. 2, 3\n[144] Jean Piaget, Margaret Cook, et al. The Origins of Intel-\nligence in Children. International Universities Press New\nYork, 1952. 2\n[145] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,\nYingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming\nXiong, Silvio Savarese, et al. UniControl: A Unified Diffu-\nsion Model for Controllable Visual Generation In the Wild.\nIn NeurIPS, 2023. 29\n[146] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning Transferable Visual Models From Natural Language\nSupervision. In ICML, 2021. 33, 36\n[147] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. Exploring the Limits of Transfer Learning\nwith a Unified Text-to-Text Transformer. JMLR, 21(140):\n1–67, 2020. 4, 5, 21\n[148] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-Shot Text-to-Image Generation. In ICML,\n2021. 5\n[149] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Ab-\nhishek Das.\nHabitat-Web: Learning Embodied Object-\nSearch Strategies from Human Demonstrations at Scale. In\nCVPR, 2022. 25\n[150] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards Robust Monocular\nDepth Estimation: Mixing Datasets for Zero-Shot Cross-\nDataset Transfer. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 44(3):1623–1637, 2020. 11\n[151] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin\nPan, and Vincent Vanhoucke. YouTube-BoundingBoxes: A\nLarge High-Precision Human-Annotated Data Set for Ob-\nject Detection in Video. In CVPR, 2017. 8, 30\n[152] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron,\nMai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-\ngenberg, et al. A Generalist Agent. Transactions on Ma-\nchine Learning Research, 2022. 3, 10, 37\n[153] Mike Roberts, Jason Ramapuram, Anurag Ranjan, At-\nulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ\nWebb, and Joshua M. Susskind. Hypersim: A Photorealis-\ntic Synthetic Dataset for Holistic Indoor Scene Understand-\ning. In ICCV, 2021. 11\n[154] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-Resolution Image\nSynthesis With Latent Diffusion Models. In CVPR, 2022.\n2, 8, 9, 34\n[155] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved Techniques\nfor Training GANs. In NeurIPS, 2016. 36\n[156] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,\nLintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar-\nnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask\nPrompted Training Enables Zero-Shot Task Generalization.\nIn ICLR, 2022. 27\n[157] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A Plat-\nform for Embodied AI Research. In ICCV, 2019. 7, 25, 26,\n31\n[158] Christoph Schuhmann. LAION-AESTHETICS. https:\n\/\/laion.ai\/blog\/laion-aesthetics\/, 2022.\n26\n[159] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-\n400M: Open Dataset of CLIP-Filtered 400 Million Image-\nText Pairs. In NeurIPS Data-Centric AI Workshop, 2021.\n6, 26\n[160] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A\nBenchmark for Visual Question Answering using World\nKnowledge. In ECCV, 2022. 29\n[161] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-\nral Machine Translation of Rare Words with Subword\nUnits. In ACL, 2016. 3\n17\n[162] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia,\nDebidatta Dwibedi, Keerthana Gopalakrishnan, Christine\nChan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J\nJoshi, et al. RoboVQA: Multimodal Long-Horizon Reason-\ning for Robotics. arXiv preprint arXiv:2311.00899, 2023.\n32\n[163] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual Captions: A Cleaned, Hypernymed,\nImage Alt-text Dataset For Automatic Image Captioning.\nIn ACL, 2018. 6, 26\n[164] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive\nLearning Rates with Sublinear Memory Cost.\nIn ICML,\n2018. 6\n[165] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,\nXinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards\nVQA Models That Can Read. In CVPR, 2019. 29\n[166] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse\nThomason, and Animesh Garg. ProgPrompt: Generating\nSituated Robot Task Plans using Large Language Models.\nICRA, 2023. 2\n[167] Linda Smith and Michael Gasser. The Development of Em-\nbodied Cognition: Six Lessons from Babies. Artificial life,\n11(1-2):13–29, 2005. 2\n[168] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUCF101: A Dataset of 101 Human Actions Classes From\nVideos in The Wild. arXiv preprint arXiv:1212.0402, 2012.\n8, 30\n[169] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo\nWen, and Yunfeng Liu. RoFormer: Enhanced Transformer\nwith Rotary Position Embedding. Neurocomputing, 2023.\n4, 23\n[170] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. PandaGPT: One Model To Instruction-Follow\nThem All. arXiv preprint arXiv:2305.16355, 2023. 3\n[171] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua-\njun Bai, and Yoav Artzi. A Corpus for Reasoning About\nNatural Language Grounded in Photographs. In ACL, 2019.\n29\n[172] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative Pretraining in Mul-\ntimodality. arXiv preprint arXiv:2307.05222, 2023. 3, 9,\n10, 34\n[173] D´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. ViperGPT:\nVisual Inference via Python Execution for Reasoning. In\nICCV, 2023. 2\n[174] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng,\nand Mohit Bansal. Any-to-Any Generation via Composable\nDiffusion. In NeurIPS, 2023. 2, 3, 9, 10, 34\n[175] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,\nTal Schuster, Steven Zheng, et al. UL2: Unifying Language\nLearning Paradigms. In ICLR, 2023. 5\n[176] MosaicML NLP Team. Introducing MPT-7B: A New Stan-\ndard for Open-Source, Commercially Usable LLMs, 2023.\nAccessed: 2023-05-05. 6, 24\n[177] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971, 2023. 3, 9, 23\n[178] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali\nEslami, Oriol Vinyals, Felix Hill, and Zacharias Janssen.\nMultimodal Few-Shot Learning with Frozen Language\nModels. In NeurIPS, 2021. 3\n[179] Aaron\nVan\nDen\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu. Neural Discrete Representation Learning. In\nNeurIPS, 2017. 3\n[180] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona,\nand Serge Belongie. The iNaturalist Species Classification\nand Detection Dataset. In CVPR, 2018. 29\n[181] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin.\nAttention Is All You Need.\nIn\nNeurIPS, 2017. 3\n[182] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based Image Description Eval-\nuation. In CVPR, 2015. 10, 37\n[183] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas,\nand Serge Belongie.\nCOCO-Text: Dataset and Bench-\nmark for Text Detection and Recognition in Natural Images.\narXiv preprint arXiv:1601.07140, 2016. 8, 29\n[184] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim,\nMax Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-\nEstruch, Quan Vuong, Andre He, et al. BridgeData V2:\nA Dataset for Robot Learning at Scale. In CoRL, 2023. 8,\n31\n[185] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal Segment\nNetworks for Action Recognition in Videos. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 41\n(11):2740–2755, 2019. 30\n[186] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. OFA: Unifying Architectures, Tasks,\nand Modalities Through a Simple Sequence-to-Sequence\nLearning Framework. In ICML, 2022. 3, 33\n[187] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-\naohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang\nZhou. ONE-PEACE: Exploring One General Representa-\ntion Model Toward Unlimited Modalities. arXiv preprint\narXiv:2305.11172, 2023. 10\n[188] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, and Furu Wei.\nImage as a Foreign Language: BEiT Pretraining for Vision\nand Vision-Language Tasks. In CVPR, 2023. 3\n[189] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou,\nYu Qiao, et al.\nVisionLLM: Large Language Model is\nalso an Open-Ended Decoder for Vision-Centric Tasks. In\nNeurIPS, 2023. 3\n18\n[190] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang.\nVATEX: A Large-\nScale, High-Quality Multilingual Dataset for Video-and-\nLanguage Research. In ICCV, 2019. 8, 10, 30\n[191] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi,\nYeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar,\nArjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik,\nDavid Stap, et al. Super-NaturalInstructions: Generaliza-\ntion via Declarative Instructions on 1600+ NLP Tasks. In\nEMNLP, 2022. 27\n[192] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. SimVLM: Simple Visual Lan-\nguage Model Pretraining with Weak Supervision. In ICLR,\n2022. 3\n[193] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le.\nFinetuned Language Models are Zero-Shot\nLearners. In ICLR, 2022. 27\n[194] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Ro-\nmain Br´egier, Yohann Cabon, Vaibhav Arora, Leonid Ants-\nfeld, Boris Chidlovskii, Gabriela Csurka, and J´erˆome Re-\nvaud. CroCo: Self-Supervised Pre-training for 3D Vision\nTasks by Cross-View Completion. In NeurIPS, 2022. 7, 25,\n26, 29\n[195] Peter Welinder, Steve Branson, Takeshi Mita, Catherine\nWah, Florian Schroff, Serge Belongie, and Pietro Perona.\nCaltech-UCSD Birds 200.\nTechnical Report CNS-TR-\n2010-001, California Institute of Technology, 2010. 29\n[196] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,\nand Chuang Gan. STAR: A Benchmark for Situated Rea-\nsoning in Real-World Videos.\nIn NeurIPS Datasets and\nBenchmarks Track, 2021. 8, 10, 30\n[197] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. SUN Database: Large-scale Scene\nRecognition from Abbey to Zoo. In CVPR, 2010. 29\n[198] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video Question An-\nswering via Gradually Refined Attention over Appearance\nand Motion. In ACM MM, 2017. 8, 10, 30\n[199] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT:\nA Large Video Description Dataset for Bridging Video and\nLanguage. In CVPR, 2016. 8, 10, 30\n[200] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing High-Resolution Video-Language Representation\nwith Large-Scale Video Transcriptions. In CVPR, 2022. 7,\n26\n[201] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin\nRaffel. mT5: A Massively Multilingual Pre-trained Text-\nto-Text Transformer. In NAACL-HLT, 2021. 26\n[202] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan,\nPing Luo, and Huchuan Lu. Universal Instance Perception\nas Object Discovery and Retrieval. In CVPR, 2023. 10\n[203] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,\nChao Weng, Yuexian Zou, and Dong Yu.\nDiffsound:\nDiscrete Diffusion Model for Text-to-sound Generation.\nIEEE\/ACM Transactions on Audio, Speech, and Language\nProcessing, 31:1720–1733, 2023. 9\n[204] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan\nRen, Lei Zhou, Tian Fang, and Long Quan. BlendedMVS:\nA Large-scale Dataset for Generalized Multi-view Stereo\nNetworks. In CVPR, 2020. 8, 30\n[205] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmPLUG-Owl: Modularization Empow-\ners Large Language Models with Multimodality.\narXiv\npreprint arXiv:2304.14178, 2023. 3\n[206] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei\nLiu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\nmPLUG-Owl2: Revolutionizing Multi-modal Large Lan-\nguage Model with Modality Collaboration. arXiv preprint\narXiv:2311.04257, 2023. 10, 34\n[207] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,\nBowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu\nChang, and Yinfei Yang.\nFerret:\nRefer and Ground\nAnything Anywhere at Any Granularity.\narXiv preprint\narXiv:2310.07704, 2023. 3, 10, 34\n[208] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu. Vector-quantized Image Modeling with\nImproved VQGAN. In ICLR, 2022. 4, 22\n[209] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling Context in Referring Expres-\nsions. In ECCV, 2016. 10, 29, 30\n[210] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, et al. Scaling Autore-\ngressive Multi-Modal Models: Pretraining and Instruction\nTuning. arXiv preprint arXiv:2309.02591, 2023. 3\n[211] Amir R Zamir, Alexander Sax, William Shen, Leonidas J\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\nDisentangling Task Transfer Learning. In CVPR, 2018. 30\n[212] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and\nChen Change Loy.\nContextual Object Detection with\nMultimodal Large Language Models.\narXiv preprint\narXiv:2305.18279, 2023. 3\n[213] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nFrom Recognition to Cognition:\nVisual Commonsense\nReasoning. In CVPR, 2019. 29\n[214] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,\nand Yejin Choi. HellaSwag: Can a Machine Really Finish\nYour Sentence? In ACL, 2019. 8, 9, 33\n[215] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-\npeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack\nHessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve:\nNeural Script Knowledge through Vision and Language and\nSound. In CVPR, 2022. 7, 22, 23, 26, 29\n[216] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An\nInstruction-tuned Audio-Visual Language Model for Video\nUnderstanding. arXiv preprint arXiv:2306.02858, 2023. 3\n[217] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and\nYu Su. MagicBrush: A Manually Annotated Dataset for\nInstruction-Guided Image Editing.\nIn NeurIPS Datasets\nand Benchmarks Track, 2023. 7, 29\n19\n[218] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu\nQiao.\nLLaMA-Adapter:\nEfficient Fine-tuning of Lan-\nguage Models with Zero-init Attention.\narXiv preprint\narXiv:2303.16199, 2023. 3\n[219] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. GPT4RoI:\nInstruction Tuning Large Language Model on Region-of-\nInterest. arXiv preprint arXiv:2307.03601, 2023. 3\n[220] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun.\nLLaVAR: En-\nhanced Visual Instruction Tuning for Text-Rich Image Un-\nderstanding. arXiv preprint arXiv:2306.17107, 2023. 3,\n8\n[221] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing\nZhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu,\nGuangze Li, et al.\nTowards Video Text Visual Question\nAnswering: Benchmark and Baseline. In NeurIPS Datasets\nand Benchmarks Track, 2022. 30\n[222] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen,\nShuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chat-\nBridge: Bridging Modalities with Large Language Model\nas a Language Catalyst. arXiv preprint arXiv:2305.16103,\n2023. 3\n[223] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh\nPhung. MoVQ: Modulating Quantized Vectors for High-\nFidelity Image Generation. In NeurIPS, 2022. 22\n[224] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba.\nPlaces:\nA 10 Million Image\nDatabase for Scene Recognition.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence, 40(6):1452–\n1464, 2017. 29\n[225] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMiniGPT-4:\nEnhancing Vision-\nLanguage Understanding with Advanced Large Language\nModels. arXiv preprint arXiv:2304.10592, 2023. 3\n[226] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-\nGAN: Dynamic Memory Generative Adversarial Networks\nfor Text-to-Image Synthesis. In CVPR, 2019. 34\n20\nThe appendix includes the following sections:\n• Sec A - Contributions\n• Sec B - Model Implementation Details\n• Sec C - Pre-training Details\n• Sec D - Tasks and Instruction Tuning\n• Sec E - Experiment Details and Additional Results\nA. Contributions\nJiasen Lu, Christopher Clark, Sangho Lee, and Zichen\nZhang collectively contributed to dataset construction,\nprompt development, and conducted numerous exploratory\nexperiments for this project.\nJiasen Lu led and designed the main idea and scope of the\nproject. Developed the majority of the model pipeline –\nimage and audio tokenizer, main architecture, model stabi-\nlization, and training objective. Led and designed the pre-\ntraining and instruction tuning data pipelines. Conducted\nexperiments with various model and data hyperparameters,\noversaw the model training process, and wrote the paper.\nCoordinate with the whole team.\nChristopher Clark co-led and designed the infrastruc-\nture, instruction tuning, and evaluation.\nDeveloped the\ndynamic packing system, modality processing pipeline,\nand classifier-free guidance for image and audio inference.\nAdded the NLP and many V&L datasets, added many syn-\nthetic tasks, and built prompts for instruction-tuning tasks.\nRan the evaluation in § 5.1 (NLP), 5.2, and 5.4 (detection,\ndepth) and wrote the paper.\nSangho Lee core contribution to the pre-training data\npipeline.\nAdded all large-scale multimodal pretraining\ndatasets, and video and audio instruction tuning datasets.\nDeveloped sample construction pipeline for pre-training.\nHelped with the model implementation – position encod-\ning, perceiver resamplers, and model stabilization. Ran the\nevaluation in § 5.1 (audio), 5.3 (audio and image FID), 5.4\n(video and audio understanding) and wrote parts of the pa-\nper.\nZichen Zhang core contribution to the instruction tuning\ndata pipeline. Added many V&L, embodiment, video, au-\ndio, data augmentation, and all instruction tuning datasets.\nBuilt prompts for instruction tuning. Investigated the model\narchitectures and training pipelines and stabilized the train-\ning.\nRan the experiments in § 5.1 (image TIFA, Seed-\nBench), 5.3 (image TIFA, action), 5.4, wrote parts of the\npaper, developed the model demo and project page.\nSavya Khosla added 3D object detection, optical flow, and\nmulti-point tracking datasets, ran the evaluation of 3D de-\ntection, and initiated the demo.\nRyan Marten added part of video and tracking datasets.\nDerek Hoiem advised on the research direction.\nAniruddha Kembhavi advised on the research direction\nand evaluation, helped manage compute resources and\nwrote the paper.\nB. Model Implementation Details\nIn this section, we present the implementation details of our\nmodel.\nB.1. Detailed of Unified Task Representation\nFirst, we provide details about how different modalities are\nrepresented in our model.\nText representation. The Byte Pair Encoding (BPE) vo-\ncabulary size is 32000. Similar to [147], we add 200 ad-\nditional special tokens to indicated masked spans when de-\nnoising. We further add 10 special tokens that can be used\nto reference the image, audio, and history input in the text.\nTwo special tokens are to indicate the ⟨Image Input⟩and\n⟨Audio Input⟩, and 8 special tokens represent individ-\nual elements in the image and audio history inputs, both of\nwhich have a maximum of 4 frames. We use a maximum of\n512 input and output tokens.\nSparse structures representation. We use an additional\n1000 special tokens to represent all continuous values, such\nas points, boxes, camera transformation, and 3D cuboids.\nPoints are represented with [y, x] coordinates and boxes\nwith [y1, x1, y2, x2] coordinates with values normalized by\nthe image size. Camera transformations are represented as\npolar angle θ, azimuth angle ϕ, and distance r. 1000 special\ntokens to represent discretized angle from −π to π. Fol-\nlowing [16], 3D cuboids are represented with 12 parame-\nters including projected center [u, v], virtual depth z, log-\nnormalized box dimension [ ¯w, ¯h, ¯l], and continuous allocen-\ntric rotation p.\n• [u, v] represent the projected 3D center on the image\nplane relative to the 2D RoI\n• z ∈R+ is the object’s center depth in meters.\n• [ ¯w, ¯h, ¯l] ∈R+ are the log-normalized physical box di-\nmensions in meters.\n• p ∈R6 is the continuous 6D allocentric rotation.\nFor 3D cuboid detection, we use prompts to indicate the tar-\nget format, such as “Locate all objects in 3D using projected\n3D center, virtual depth, log-normalized box size, and rota-\ntion in the image.”\nAction representation.\nFor embodied navigation tasks,\nthe discrete action space is directly represented as texts,\ne.g. “forward”, “left”, “right”, “stop”.\nFor ob-\nject manipulation tasks, the action is represented differ-\nently based on the robots. Overall, the positional change\n(e.g. (∆PosX, ∆PosY, ∆PosZ)), rotational change (e.g.\n(∆RotX, ∆RotY, ∆RotZ)), and gripper open or close are\ndiscretized using the same 1000 special tokens, and we use\nthe text prompt to indicate the input and target format. For\ntasks that require multi-step planning (e.g. VIMA [87]), the\nactions are represented as human-readable texts with the in-\ndication of steps, skills used (e.g. pick, place, or push), and\ndiscretized positional and rotational parameters. Figure 11\nprovides a detailed illustration of the robot tasks.\n21\nAudio Input\nSample rate\n16000 Hz\nFFT hop length\n256 samples\nFFT window size\n1024\nMel bins\n128\nSubsegment length\n256 hops, (≈4.08 sec)\nMel Spectrogram size\n128 mels × 256 hops\nfmin\n0\nfmax\n8000\nAST patch size\n16\ntoken size\n8 × 16\nPretrain sub-sample\n64\nFinal size\n64 or 128 tokens\nImage Input\nViT patch size\n16\nPretraining size\n384 × 384\nToken size\n24 × 24\nPretrain sub-sample\n288\nFinal size\n288 or 576 tokens\nText\nSeq length\n512\nFinal size\n512 tokens\nImage History\nViT patch size\n16\nPretraining size\n256 × 256\nToken size\n16 × 16\nPretrain sub-sample\n128\nMax num segments\n4\nLatent size\n32\nFinal size\n32, 64, 96, 128 tokens\nAudio History\nAST patch size\n16\nPretraining size\n128 × 256\nToken size\n8 × 16\nPretrain sub-sample\n64\nMax num segments\n4\nLatent size\n16\nFinal size\n16, 32, 48, 64 tokens\nTable 8. Input representations details\nImages representation. Images are encoded with a pre-\ntrained ViT [45]. We use the ViT-B checkpoint trained on\nLAION 2B dataset2. For image inputs, we use a maximum\nlength of 576 tokens (i.e. 24 × 24 patch encoding from a\n384 × 384 image). We concatenate features from the sec-\nond and second-last layers of the ViT to capture both low\nand high-level visual information. To generate the image,\nwe encode these images as discrete tokens [49]. Different\nfrom UNIFIED-IO [123], which uses the VQ-GAN trained\non ImageNet [41] to convert 256×256 resolution image into\n16 × 16 tokens, we use the VQ-GAN trained on the Open\nImages dataset [103] with a compression ratio of 8 and a vo-\ncabulary size of 163843. This converts 256×256 resolution\nimage into 32 × 32 tokens. We also compare the VQ-GAN\n2https:\/\/github.com\/mlfoundations\/open_clip\n3https:\/\/github.com\/CompVis\/taming-transformers\ntokenizer with the ViT-VQGAN [208] and MoVQ [223].\nWe empirically find VQ-GAN leads to best generation re-\nsults.\nDense structures representation. To handle this modality,\nwe convert per-pixel labels into RGB images. For depth,\nwe construct a grayscale image by normalizing the depth\nmap. For surface normal estimation, we convert the x\/y\/z\norientations into r\/g\/b values. For segmentation, we train\nUNIFIED-IO 2 to predict a single black-and-white mask for\na particular object specified by a class and a bounding box.\nInstance segmentation (as done in GRIT [66]) can then be\nperformed by first performing localization for the target\nclass and then performing segmentation for each detected\nbox. UNIFIED-IO instead trains the model to produce an\nimage with a randomly selected color for each instance. We\nfound this makes post-processing difficult since output im-\nages sometimes do not exactly follow the color scheme, and\nthe model could struggle with images with many different\ninstances.\nAudio representation.\nThis modality encodes a 4.08-\nsecond segment of audio. We take the waveform sampled\nat 16000 Hz and convert it to a log-mel-scaled spectrogram.\nWe compute the spectrogram for an entire audio segment\n(4.08 seconds) simultaneously. Each window involves 1024\nsamples and 256 samples ‘hops’ between windows. The re-\nsulting spectrogram has a size of 128 mel bins with 256\nwindows. We chose these hyperparameters largely around\nefficiency. We then encode this with a pre-trained AST [57]\nwith the patch size of 16 × 16, hence a total of 128 tokens.\nTo generate audio, we use ViT-VQGAN [208] to con-\nvert the spectrograms into discrete tokens. Since the authors\nof [208] did not release the source code or any pre-trained\nmodels, we implement and train our own version of ViT-\nVQGAN with 8 × 8 patch size that encodes a 256 × 128\nspectrogram into 512 tokens with a codebook size of 8196.\nThe model is trained with the audio on AudioSet [54],\nACAV100M [105], and YT-Temporal-1B [215] datasets.\nAfter getting the log-mel-scaled spectrograms, we use HiFi-\nGAN4 [98] vocoder to decode the spectrograms back to\nwaveforms. We train the HiFi-GAN using the same param-\neters shown in Table 8. We trained the model on a mixture\nof AudioSet and LJSpeech [85] to cover natural sound and\nhuman voice.\nHistory representation. Images and audio inputs in this\nhistory are first encoded in the same way as image and au-\ndio inputs. We then use a perceiver resampler [5] to further\ncompress the image and audio features and produce a fixed\nnumber of visual outputs (32) and audio outputs (16) to re-\nduce the total sequence length of the model. As shown in\nTable 8, we consider a maximum of 4 images and audio\nsegments. In our experiments, we test with two different\nvariants of perceiver implementations: 1) a small group of\n4https:\/\/github.com\/jik876\/hifi-gan\n22\nlatent embeddings query each frame\/segment individually\n[5, 9], 2) a large group of latent embeddings query all his-\ntory at once. While the second implementation can finely\nrepresent the referenced image and audio, the first can pre-\nserve better temporal information. Thus, our final imple-\nmentation uses the first one.\nB.2. 2D Rotary Embedding\nWe use a rotary position encoding to model the relative lo-\ncation of input sequences [169]. We chose this primarily\nbecause we did not want to use absolute (additive) position\nembeddings, which would have to be added to the inputs\nof each encoder, and also wanted to be consistent with the\nLLaMA [177] position encoding.\nThe rotary encoding uses no parameters and instead uses\na kernel trick to allow the model to recover relative dis-\ntances between key and query elements in a transformer’s\nattention head. For text, we apply rotary encoding at each\nlayer of the network. For other modalities, we extend RoPE\nto two-dimensional cases by splitting each of the query and\nkey embeddings of transformer attention heads in half and\napply separate rotary embeddings constructed by each of\nthe two coordinates to the halves.\nWe treat each token (image, audio, image history, and\naudio history) as having a 2-dimensional position corre-\nsponding to 1) h, w coordinates in the image or audio spec-\ntrogram, 2) (t, l) where t and l represent the indices of\nframe and perceiver latent vector in the image or audio his-\ntory, respectively. Different from [215], which uses a 4-\ndimensional position to represent all the inputs, we use a\ncombination of learnable segment (modality) embeddings\nand rotary encoding.\nB.3. Dynamic Packing\nHere, we describe the dynamic packing algorithm in more\ndetail. As is standard practice, when batching together in-\nputs, we pad input tensors to a maximum length and use\nattention masked to prevent the transformer from attending\nto padding elements. This, however, is highly inefficient\nin our multi-modal setting because many modalities are not\npresent in most examples, which results in a huge amount\nof padding. For example, if one example in the batch has\nan image output, every other example must be padded with\n1024 target image tokens, even if their output is in a differ-\nent modality.\nOne solution is to arrange batches so that each batch\ncontains examples with similar numbers of tokens in each\nmodality. This is, however, complicated to do in practice\nsince (1) our data does not fit in RAM, so we cannot easily\nsort and group data this way, especially if needing to match\ntokens across five input and three output modalities and (2)\nour coding framework, JAX [15], does not support variable\nlength tensors when constructing the execution graph which\nL\nXL\nXXL\nTransformer\nParams\n1.1B\n3.2B\n6.8B\nVocab size\n33280\nImage vocab size\n16512\nAudio vocab size\n8320\nModel dims\n1024\n2048\n3072\nMLP dims\n2816\n5120\n8192\nencoder layer\n24\ndecoder layer\n24\nHeads\n16\n16\n24\nMLP activations\nsilu, linear\nLogits via embedding\nTrue\nDropout\n0\nImage Resampler\nLatents size\n32\nModel dims\n768\n1024\n1024\nHeads\n12\n16\n16\nHead Dims\n64\nNumber layer\n2\nMLP Dims\n2048\n4096\n4096\nMLP activations\ngelu\nAudio Resampler\nLatents size\n16\nModel dims\n768\n1024\n1024\nHeads\n12\n16\n16\nHead Dims\n64\nNumber layer\n2\nMLP Dims\n2048\n4096\n4096\nMLP activations\ngelu\nViT\nPatch size\n16\nModel dims\n768\nHeads\n12\nHead Dims\n64\nNumber layer\n11\nMLP Dims\n3072\nMLP activations\ngelu\nAST\nPatch size\n16\nModel dims\n768\nHeads\n12\nHead Dims\n64\nNumber layer\n11\nMLP Dims\n2048\nMLP activations\ngelu\nTable 9. Model Hyperparameters\nmakes handling variable lengths between batches extremely\ndifficult.\nInstead, we use packing, a process where the tokens of\nmultiple examples are packed into a single sequence, and\nthe attentions are masked to prevent the transformer from\ncross-attending between examples. Packing is often done\nas a pre-processing step when handling text, but this does\nnot work in our setup since some parts of our network can-\nnot operate on packed data (e.g., the VAE or image ViT).\nInstead, we start with an unpacked batch of examples, run\n23\nthese components first, and then dynamically pack the re-\nsulting tokens in a backdrop-compatible way before running\nthe transformer. To run efficiently on TPUs we pack exam-\nples using matrix multiplication with carefully constructed\none-hot matrices.\nTo account for all modalities, the maximum sequence\nlength our transformer needs to take as input is 1152, and\nthe maximum target length is 2048. When packing, we can\ngenerally pack two examples into an input sequence of 864\nand a target sequence of 1280, which gives a roughly 4x\nspeed up due to reduced sequence length and the ability\nto process two examples simultaneously. When streaming\ndata, packing cannot be done reliably. For example, if two\nconsecutive examples have an image output, they cannot be\npacked since they will total over 1280 output tokens. To\nhandle this, we use a heuristic algorithm to re-arrange data\nas it is being streamed. The algorithm keeps a small pool of\nexamples in memory. Given a new example, it pairs it with\nthe largest example in the pool it can be packed with and\noutputs both as a pair. If no such example exists, it adds the\nexample to the pool. If the pool reaches a maximum size\nof 10, the largest example is emitted and processed with-\nout being packed with another example. We find this occurs\nless than 0.1% of the time during training.\nB.4. Full Model Details\nIn Table 9,\nwe present the full hyperparameters of\nour model.\nDuring pre-training, we train the UIO-2L,\nUIO-2XL, and UIO-2XXL with a batch size of 512 due to\nmemory limit. We sub-sample 50% of the image, audio,\nand history inputs patches. The total packing length is 864\nfor the encoder and 1280 for the decoder. During instruc-\ntion tuning, we train all of our models with a batch size 256\ndue to computing constraints. We sub-sample 87.5% of the\nimage, audio, and history input patches. The total packing\nlength is 1024 for pretraining and 1280 for instruction tun-\ning. 8-way in-layer parallelism and 64-way data parallelism\nwere used to scale up to the 7B model training.\nWe train for 1.5 million steps with an effective batch size\nof 512. This results in training on approximately 1 trillion\ntokens. During pre-training, we keep at most 50% of the\nimage patches in the image history or image encoder, as is\ncommon practice with MAE pre-training [71]. We use up\nto four images\/segments in image\/audio history.\nC. Pre-Training Details\nIn this section, we provide additional details about the data\nUNIFIED-IO 2 is pre-trained on. The datasets we use for\npre-training are listed in Table 10. Unless otherwise speci-\nfied, we use the pre-training objective described in Section\n3.3, where one of the present modalities is randomly se-\nlected as the target. We sample data to ensure all the output\nmodalities are well represented and to balance how often\nour various corpora are used based on their size. The distri-\nbution is shown in Figure 9.\nC.1. Data Sources\nText. Our data follows the mixture used by MPT-7B [176].\nImage & Text. Image & text paired data comes from vari-\nous unsupervised corpora, shown in Table 10. For LAION\ndata, we only generate images from image\/text pairs from\nLAION aesthetic, which contains higher quality images,\nwhile we generate text for image\/text pairs from LAION\n400M. We also only keep images from LAION if they are\nmarked as being unlikely to be NSFW in the LAION meta-\ndata. Web images is a dataset of images we download and\nfocuses on icons and stylized images.\nVideo. We gather a total of 180M short videos from various\nsources. During training, we pick a random sequence of up\nto five frames from the video. The first four will be encoded\nwith an image\/audio history encoder, while the fifth frame\nwill be encoded with the image\/audio encoder. The text\nmatching these frames is encoded with a text encoder along\nwith marker tokens to show where each frame occurred as\nstated in B.1, or, if the dataset only includes a single caption\nthat is not aligned with individual frames, the entire caption\nis encoded instead. The text, audio, or image modality can\nbe selected as the target modality. As usual, other modal-\nities are randomly masked, and the target modality is ran-\ndomly masked or injected with noise in the input. Note we\nhave sub-sampled data from many of these corpora to keep\nthe dataset size more manageable, and sometimes due to\nbroken video links.\nInterleaved\nImage\n&\nText.\nWe\nprimarily\nuse\nOBELICS [104], which contains paragraphs and im-\nages interleaved together. For each document, we randomly\nselect an image or a paragraph as the target and use up to\nthe previous four (if the target is an image) or five (if the\ntarget is a paragraph) images as context. The last image is\nencoded with the image encoder, and the remaining images\nare encoded in the image history. The text matching those\nimages is concatenated and interjected with marker tokens\nto indicate where the images in the image history or image\ninput occur. We either do de-noising, where a noisy version\nof the target is included in the input, or generation, where\nthe target is not part of the input, although we always\ninclude both the text and image input modalities.\nIn addition, we construct interleaved data by interleav-\ning multiple images and captions from several image\/text\npair corpora. The images are encoded as the image input\nand\/or the image history, and matching text is constructed\nby specifying the caption for one, or all, of these images us-\ning special tokens to mark which image each caption refers\nto. For this task, we only target the text modality, and train\nthe model to either (1) de-noise the caption of a single im-\nage, (2) generate a caption for a single image that is speci-\n24\nAudio\nImage\nText\nVideo\nAgent Trajectories\nSynthetic\nImage\/Text Interleaved\nVideo\nImage\/Text Pairs\nAgent Trajectories\nSynthetic\nImage\/Text Interleaved\nVideo\nImage\/Text Pairs\nText\nEgo4D\nACAV\nAudioSet\nHD-VILA\nYT-Temporal\nHabitat Next Frame\/State Prediction\nProcTHOR Next Frame\/State Prediction\nSegment Anything\nHabitat Easy\nHabitat Hard\nObjaverse Multiview\nOBELICS\nEgo4D\nAudioSet\nHD-VILA\nWebVid\nACAV\nYT-Temporal\nCC3M\nCC12M\nRedCaps\nWeb Images\nLAION Aesthetic v2.5\nHabitat Next Frame\/State Prediction\nProcTHOR Next Frame\/State Prediction\nRedCaps Patch Counting\nLAION Aesthetic Patch Counting\nLAION Aesthetic Patch Detect\nRedCaps Caption Multiple\nCC3M Caption Multiple\nRedCaps Denoise One\nCC3M Denoise One\nCC12M Caption Multiple\nRedCaps Caption One\nObjaverse Camera Change\nCC12M Denoise One\nCC3M Caption One\nCC12M Caption One\nOBELICS\nEgo4D\nAudioSet\nHD-VILA\nWebVid\nACAV\nYT-Temporal\nCC3M\nCC12M\nRedCaps\nWeb Images\nLAION 400M\nRedPajama Book\nStack Markdown\nWikipedia\nRedPajama Common Crawl\nStack\nC4\nMC4\nFigure 9. Pre-training data distribution, segments proportional to sampling rates. The inner section shows the target modality, the middle\nsection shows the type of data, and the third shows particular datasets.\nfied in an input prompt using a marker token or (3) generate\na sequence of marker tokens and captions that describe each\ninput image. This task aims to ensure the model learns the\nsemantics of the images in the history and understands the\nmarker tokens.\nMulti-View. We train on the cross-view completion task\nfrom CroCo [194], where the model must complete a heav-\nily noised image using an image of the same scene, but from\na slightly different angle, as context. The noised input is en-\ncoded as an image and the second image is encoded through\nthe image history encoder. In addition, we generate data us-\ning Objaverse [40] objects by capturing multiple views of\nthe object in 3D, and either specify the camera coordinates\nin the input text and train the model to generate a new image\nmatching new camera coordinates, or train the model to pre-\ndict how the camera has moved between different images.\nWe further augment the view synthesis task by providing\nin-context examples. For example, by giving one or more\nexamples of the views and transformations in the image his-\ntory, the model predicts the new view from the new camera\ntransformation specified by the prompt. Both tasks aim to\nimprove the model’s 3D understanding during pre-training.\nAgent Trajectory. We use scripted shortest path trajec-\ntories in ProcTHOR [39] and human-collected demonstra-\ntions in Habitat [149, 157]. While the original datasets are\nfor object navigation with relatively long episode lengths,\nwe only subsample from the last few frames for image his-\ntory and image input such that mostly the target object is\nwithin the observation. The task is randomly selected from\n1) generating the next visual observation frame as the target\nimage, 2) predicting the next positional observation coor-\ndinates as the text target, and 3) predicting the next action\n25\nSize\nRate Text Sparse Dense Image Audio ImageH AudioH Text Sparse Dense Image Audio\nText\n6.6b\n33.0\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nMC4 [201]\n5.0b\n11.7\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nC4 [68]\n266m 10.6\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nStack [95]\n147m 3.55\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nRedPajama CC [32]\n1.2b\n3.55\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nWikipedia\n6.8m 1.42\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nRedPajama Book [32]\n13m\n1.06\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nStack-Markdown [95]\n34m\n1.06\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nImage\/Text\n970m 31.3\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n✓\n-\nLAION Aesthetics v2.5 [158] 491m 17.7\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\nLAION-400M [159]\n346m 8.95\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nCC12M [23]\n11m\n1.48\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n✓\n-\nRedCaps [42]\n12m\n1.39\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n✓\n-\nWeb Images\n107m 1.33\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n✓\n-\nCC3M [163]\n3.0m 0.49\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n✓\n-\nVideo\n181m 25.0\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\nYT-Temporal [215]\n146m 13.7\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\nACAV [105]\n17m\n3.98\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n✓\nHD-VILA [200]\n7.1m 2.75\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\nAudioSet [54]\n1.7m 2.75\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\nWebVid [13]\n9.2m 1.23\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n✓\n-\nEgo4D [60]\n0.7m 0.55\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\nInterleaved Image\/Text\n157m 8.70\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nOBELICS [104]\n131m 8.00\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nCC12M Interleaved\n11m\n0.35\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n-\n-\nCC3M Interleaved\n3.0m 0.21\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n-\n-\nRedCaps Interleaved\n12m\n0.14\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n-\n-\nMulti-View\n3.4m 0.67\n✓\n-\n-\n✓\n-\n✓\n-\n-\n✓\n-\n✓\n-\nCroCo Habitat [157, 194]\n2.6m 0.33\n✓\n-\n-\n✓\n-\n✓\n-\n-\n-\n-\n✓\n-\nObjaverse [40]\n0.8m 0.33\n✓\n-\n-\n✓\n-\n✓\n-\n-\n✓\n-\n✓\n-\nAgent Trajectories\n1.3m 0.33\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nProcTHOR [39]\n0.7m 0.17\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nHabitat [157]\n0.6m 0.17\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nSynthetic\n504m 1.00\n✓\n✓\n-\n✓\n-\n-\n-\n-\n✓\n✓\n-\n-\nSegment Anything [94]\n1.1m 0.50\n✓\n✓\n-\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\nLaion Aesthetics Patches\n491m 0.45\n✓\n-\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\nRedCaps Patches\n12m\n0.05\n✓\n-\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\nAll\n8.5b\n100\n✓\n✓\n-\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nTable 10. Datasets used for pre-training, rate shows the sampling percentage during pre-training and size shows the approximate number\nof examples if iterating through the data once.\nas the text target. 1) requires inferring from the image and\nimage history input and the last action specified in the text\ninput, 2) further requires the location information, and 3) is\nbased on the target object name and visual observations for\nthe next action prediction.\nSynthetic. We add two synthetic tasks. First, we use the\nautomatically annotated data from Segment Anything [94].\nWe give the model either a set of points or a bounding box as\ninput and train it to generate a segmentation mask as output.\nSecond, we add artificial patches of various shapes and col-\nors to images from other unsupervised datasets and train the\nmodel to output their locations in order to train the model\nto generate sparse coordinates as output. We additionally\ntrain the model to output the total number of patches on the\nimage to pre-train its counting abilities.\nD. Instruction Tuning Details\nIn this section, we provide additional details about the in-\nstruction tuning data and individual tasks UNIFIED-IO 2\n26\nEmbodied AI\nNatural Language\nAudio Understanding\nVideo Sparse Labelling\nVideo Understanding\nImage Dense Labelling\nImage Sparse Labelling\nImage Understanding\nAudio Generation\nImage Generation\nEmbodied QA\nGoal Generation\nNext Frame\/State Prediction\nAction Prediction\nLanguage Modeling\nText Instruction Following\nAudio Captioning\nAudio Tagging\nVideo Sound Localization\nVideo Action Localization\nVideo Tracking\nVideo Instruction Following\nVideo Question Answering\nVideo Captioning\nVideo Tagging\nOptical Flow\nDepth Estimation\nReferring Expression_Segmentation\nSurface Normal Estimation\nLocalized Segmentation\nSemantic_Segmentation\nKeypoint Detection\nText Detection\n3D\nReferring Expression\nObject Localization\nObject Detection\nImage Pair _QA\nImage Instruction Following\nRegion Captioning\nRelationship Prediction\nImage Tagging\nRegion Classification\nImage Captioning\nVQA\nAudio from Video\nAudio from Text\nView_Synthesis\nImage Inpainting\nNext Frame Generation\nImage Editing\nControllable Image Editing\nImage from Text\nFrankaKitchen Caption\nHabitat PointNav Affordance\nBridgeData V2 Caption\nVIMA detection\nLanguage Table (sim) Caption\nLanguage Table (real) Caption\nFrankaKitchen\nBridgeData V2\nVIMA\nHabitat Next Frame\/State Prediction\nProcTHOR Next Frame\/State Prediction\nFrankaKicthen\nVIMA\nVIMA No Masking\nVIMA w\/BBox Prediction\nBridgeData V2\nLanguage Table (sim)\nVIMA Pure Text\nHabitat PointNav\nLanguage Table (real)\nHabitat PointNav w\/State Prediction\nVIMA w\/BBox and Masking\nVIMA w\/BBox Input\nRedPajama Book\nWikipedia\nRedPajama Common Crawl\nStack\nC4\nMC4\nMBPP Code\nFlan2 Dialog\nFlan2 CoT\nDolly\nCode Aplaca\nOASST\nFlan2 SuperNLI\nAlpaca\nFlan2 T0\nFlan2 FLan2021\nLJ Speech\nMACS\nMusicCaps\nClotho\nAudioCaps\nMACS\nKinetic Sounds\nAudioSet\nVGG-Sound\nFlickr SoundNet\nAVA\nGOT\nLaSOT\nYouTube-BB\nM3IT MSRVTT QA\nMIMIC-IT Navigation Dialogue\nMIMIC-IT Navigation\nMIMIC-IT Dense Captioning\nMIMIC-IT Story Telling\nMIMIC-IT TV Caption\nM3IT MSVD-QA\nM3IT SS\nM3IT iVQA\nEgo4D Talking to Me\nEgo4D Looking at Me\nMSVD\nVTVQA\nSTAR\nMSRVTT-QA\nMSRVTT-Captioning\nVATEX\nWebWid\nUCF101 Action Tagging\nAction Anticipation EK100\nAction Tagging EK100\nSS V2\nSS V2 Natural\nKinetics710\nFly Chairs\nMPI Sintel\nNYU Depth\nRecoco Google\nRecoco UNC\nRefcoco UNC Plus\nBlendedMV\nBlendedMV Aug.\nFramenet\nFramenet Aug.\nTaxonomy\nTaxonomy Aug.\nLVIS\nCOCO\nOpen Images\nVizWiz Grounding\nLVIS Aug.\nCOCO Aug.\nOpen Images Aug.\nCOCO Aug.\nCOCO Text Detection\nCOCO Text\nCOCO Text Multi-Localization\nCOCO Text Localization\nObjaverse Camera Change\nOmni3D\nRefcoco Google\nRefcoco UNC\nRefcoco UNC Plus\nRecoco UNC Pair Aug.\nRefcoco UNC Aug.\nRecoco Google Pair Aug.\nRefcoco Google Aug.\nCOCO\nVisual Genome Class Subset\nOpen Images\nVisual Genome\nVisual Genome Multi-Localization\nCOCO\nCOCO Multi-Localization\nVisual Genome Aug.\nCOCO Aug.\nOpen Images Multi-Localization\nOpen Images Aug.\nMIMIC-IT Spot Difference Subtle\nMIMIC-IT Spot Difference General\nM3IT NLVR\nM3IT Mocheg\nLLaVA Instruction MC\nLLaVAR Instruct MC\nM3IT Visual Dialog\nLLaVA Instruct\nVisual Genome\nVSR\nVSR Binary\nVG Relation from Expression\nVG Relations from BBox\nVisual Genome\nOpen Images\nM3IT IQA\nCalTech Birds 2011\nM3IT COCO ITM\nM3IT ImageNet\nSun397\niNaturalist\nImagenet\nPlaces 365\nM3IT COCO GOI\nCOCO Region Cls\nVisual Genome Region Cls\nOpen Images Region Cls\nM3IT Paragraph Captioning\nVizWiz\nSCICAP\nText Caps From History\nTextCaps\nM3IT TextCaps\nCC3M\nCC3M Aug.\nCOCO\nCOCO Aug.\nOpen Images\nOpen Images Aug.\nRedCaps\nRedCaps Aug.\nM3IT ViQuAE\nM3IT OKVQA\nM3IT OCR VQA\nM3IT Shapes\nM3IT VisualMRC\nM3IT A-OKVQA\nM3IT CVR\nOKVQA-v1\nM3IT ST-VQA\nM3IT Text VQA\nM3IT CLEVR\nM3IT GQA\nM3IT VQA 2.0\nOK-VQA-2\nVizWiz\nBlendMVS\nTallyQA MC\nText VQA\nGQA Sentence Answer\nScienceQA\nScienceQA Rationale\nFramenet\nTaxonmy\nGQA\nCOCO Text\nOCR-VQA\nTallyQA\nVQA 2.0\nVisual Genome\nVCR\nYT-Temporal\nLJ Speech\nMACS\nMusicCaps\nClotho\nVGG-Sound\nAudioCaps\nACAV\nAudioSet\nObjaverse\nObjaverse Multiview\nHabitat Easy\nHabitat Hard\nVisual Genome\nCOCO\nOpen Images\nGOT\nLaSOT\nMagicBrush\npix2pix\nCOCO\nLVIS Segmentation\nMultigen Combination\nMultigen\nCOCO\nOpen Images\nCC3m\nRedCap\nLAION Aesthetic v2.5\nFigure 10. Sunburst chart of our instruction-tuning mixtures, sections are proportional to sampling rates.\nsupports.\nAn overview of the instruction tuning data is\nshown in Table 11. We show a visualization including indi-\nvidual datasets in Figure 10. We sample broad categories of\ntasks evenly and then generally sample individual datasets\nin proportion to the square root of their size, although with\nsome minor hand-engineered adjustments to downweight\nnoisy datasets or upweight very rare tasks.\nD.1. Natural Language\nFor natural language data we use the mixture from\nFlanV2 [122], which in turn includes data from Muf-\nfin [193], T0-SF [156], NIV2 [191], and CoT annotations,\nas well data from Alpaca [142], Dolly [33], Open Assis-\ntant [99], and MDPP [8]. In addition, we continue pre-\ntraining on our unsupervised NLP mixture from our fine-\ntuning stage to ensure the model does not forget informa-\ntion learned from unsupervised data during the extensive\ninstruction-tuning stage.\nD.2. Image Generation\nFor text-to-image generation, we use the same image\/text\npairs we used during pre-training, as well as localized\nnarratives from Open Images [103] and captions from\nCOCO [115] and Visual Genome (VG) [102]. Our prompts\nfor these tasks specify that the image might be noisy or ap-\nproximate for unsupervised corpora (e.g. “Generate an im-\nage that roughly matches this text: {caption}”) and give\nhints as to the style for supervised corpora (e.g. “What\ndo you see in this image?\nPlainly describe the individ-\nual element you observe.” for localized narratives) to help\ndisambiguate the stylistic differences between the datasets.\nWe use simple prompts (e.g. “Caption this image.”) for the\nCOCO captions.\nWe additionally train the model to generate images\n27\nSize\nRate Datasets Text Sparse Dense Image Audio ImageH AudioH Text Sparse Dense Image Audio\nImage Generation\n506m 17.6\n21\n✓\n✓\n✓\n✓\n-\n✓\n✓\n✓\n-\n-\n✓\n-\nImage from Text\n497m 10.6\n5\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nControllable Image Editing\n3.0m 2.92\n4\n✓\n-\n✓\n✓\n-\n✓\n-\n-\n-\n-\n✓\n-\nImage Editing\n1.1m 1.66\n3\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\nNext Frame Generation\n24k\n0.96\n2\n✓\n✓\n-\n-\n-\n✓\n✓\n-\n-\n-\n✓\n-\nImage Inpainting\n1.0m 0.79\n3\n✓\n✓\n-\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\nView Synthesis\n4.2m 0.60\n4\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nAudio Generation\n164m 7.50\n9\n✓\n-\n-\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n✓\nAudio from Text\n19m\n5.62\n8\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n✓\nAudio from Video\n145m 1.88\n1\n✓\n-\n-\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n✓\nImage Understanding\n53m\n17.8\n73\n✓\n✓\n-\n✓\n-\n✓\n-\n✓\n-\n-\n-\n-\nVQA\n5.8m 6.23\n31\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nImage Captioning\n32m\n4.25\n14\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nRegion Classification\n6.1m 2.41\n4\n✓\n✓\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nImage Tagging\n3.8m 2.38\n8\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nRelationship Prediction\n0.8m 1.41\n6\n✓\n✓\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nRegion Captioning\n3.5m 0.60\n1\n✓\n✓\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nImage Instruction Following\n0.4m 0.37\n6\n✓\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\nImage Pair QA\n0.1m 0.17\n3\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n-\n-\nImage Sparse Labelling\n13m\n7.25\n26\n✓\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\n✓\n-\nObject Detection\n5.3m 3.08\n9\n✓\n-\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\nObject Localization\n6.0m 1.31\n3\n✓\n-\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\nReferring Expression\n0.2m 1.08\n7\n✓\n-\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\n3D\n1.0m 1.00\n2\n✓\n-\n-\n✓\n-\n✓\n-\n-\n✓\n-\n✓\n-\nText Detection\n37k\n0.41\n3\n✓\n-\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\nKeypoint Detection\n0.3m 0.38\n2\n✓\n✓\n-\n✓\n-\n-\n-\n-\n✓\n-\n-\n-\nImage Dense Labelling\n6.9m 4.06\n19\n✓\n✓\n-\n✓\n-\n✓\n-\n-\n-\n✓\n-\n-\nSemantic Segmentation\n2.4m 1.23\n4\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\nLocalized Segmentation\n3.2m 1.17\n3\n✓\n✓\n-\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\nSurface Normal Estimation\n1.1m 1.03\n6\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\nReferring Expression Segmentation\n0.1m 0.47\n3\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\nDepth Estimation\n47k\n0.11\n1\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n✓\n-\n-\nOptical Flow\n24k\n0.06\n2\n✓\n-\n-\n✓\n-\n✓\n-\n-\n-\n✓\n-\n-\nVideo Understanding\n13m\n10.6\n24\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n-\n-\nVideo Captioning\n9.1m 3.75\n3\n✓\n-\n-\n✓\n-\n✓\n✓\n✓\n-\n-\n-\n-\nVideo Tagging\n1.1m 3.75\n6\n✓\n-\n-\n✓\n-\n✓\n✓\n✓\n-\n-\n-\n-\nVideo Question Answering\n2.5m 2.84\n9\n✓\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\n-\n-\nVideo Instruction Following\n0.2m 0.21\n6\n✓\n-\n-\n✓\n-\n✓\n✓\n✓\n-\n-\n-\n-\nVideo Sparse Labelling\n0.4m 3.42\n5\n✓\n✓\n-\n✓\n-\n✓\n✓\n-\n✓\n-\n-\n-\nVideo Tracking\n0.2m 2.50\n3\n✓\n✓\n-\n✓\n-\n✓\n✓\n-\n✓\n-\n-\n-\nVideo Action Localization\n0.2m 0.61\n1\n✓\n-\n-\n✓\n-\n✓\n✓\n-\n✓\n-\n-\n-\nVideo Sound Localization\n2.5k\n0.31\n1\n✓\n-\n-\n✓\n-\n✓\n✓\n-\n✓\n-\n-\n-\nAudio Understanding\n2.2m 2.50\n10\n✓\n-\n-\n✓\n✓\n✓\n-\n✓\n-\n-\n-\n-\nAudio Tagging\n2.1m 1.25\n5\n✓\n-\n-\n✓\n✓\n✓\n-\n✓\n-\n-\n-\n-\nAudio Captioning\n75k\n1.25\n5\n✓\n-\n-\n-\n✓\n-\n-\n✓\n-\n-\n-\n-\nNatural Language\n11m\n25.0\n17\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nText Instruction Following\n11m\n12.5\n10\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nLanguage Modeling\n-\n12.5\n7\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nEmbodied AI\n7.2m 4.33\n23\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n✓\n-\n✓\n-\nAction Prediction\n4.3m 3.37\n12\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n-\n-\nNext Frame\/State Prediction\n1.3m 0.33\n2\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n-\n-\n✓\n-\nGoal Generation\n0.7m 0.33\n3\n✓\n-\n-\n✓\n-\n✓\n-\n-\n-\n-\n✓\n-\nEmbodied QA\n1.0m 0.30\n6\n✓\n-\n-\n✓\n-\n✓\n-\n✓\n✓\n-\n-\n-\nAll Tasks\n775m 100\n227\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nTable 11. Instruction tuning training mixture. Due to the number of datasets used, we group them by task and only show statistics for each\ngroup. The rate shows the sampling rate, size shows the number of examples of iterating through the data once, and datasets show the\nnumber of individual data sources used for the tasks.\n28\nthrough view synthesis [40, 194] as was done during pre-\ntraining. We also integrate data for image editing [18, 217]\nand image editing based on various dense control signals\nsuch as depth maps, edges, segmentation, etc.\nFollow-\ning [145], and the segmentation-based image generation\nfrom UNIFIED-IO using data from COCO and LVIS [62].\nFinally, we train on inpainting by masking a region of an\ninput image that contains an object and training the model\nto generate the complete image given the object name and\nlocation. We derive data for this task from the object anno-\ntation data in COCO, Open Images, and VG.\nDuring inference, we use top-p sampling, also known as\nnucleus sampling [75], for generating images with the tem-\nperature t = 1.0 and p = 0.95. We also enable classifier-\nfree guidance [74] by replacing the prompt with the un-\ninformative prompt “An image of a random picture.” 10%\nof the time during training. That prompt is then used as the\nclassifier-free prompt with a guidance scale of α = 10.0\nduring inference.\nD.3. Audio Generation\nDatasets for audio generation from text include Audio-\nCaps [93], Clotho [47], MACS [131], MusicCaps [2], and\nLJSpeech [85]. During training, we divided the audio into\n4-second-long segments and then generated one segment of\nthe target audio, giving both the text and any previous seg-\nments as input. We also train on the next-frame prediction\ntask, which aims to generate the audio for the next frame in\na video from YT-Temporal-1B [215].\nOur prompts for these tasks specify the characteristics\nof target audio; e.g., “Generate the sound\/music based on\nthe description: {caption}” for natural sound and music,\nrespectively, and “Speak: {passage}” for speech. We use\nthe same sampling method as the image generation, the top-\np sampling with the temperature t = 1.0 and p = 0.95. We\ndo not use the classifier-free guidance because it can lead to\npoor performance. When generating audio longer than 4.08\nseconds during inference, we generate an initial segment\nthat is 4.08 seconds long and then extend it by generating\nadditional segments using previous audio segments as the\naudio history input.\nD.4. Image Understanding\nThese tasks require generating text in response to a query\nabout an image or a pair of images. We use the data from\nM3IT [112] and MIMIC-IT [107, 108], as well as a variety\nof other additional sources. For VQA, we add GQA [83],\nTallyQA [1], OK-VQA [130], A-OKVQA [160], OCR-\nbased VQA datasets [133, 165], Visual Genome, Sci-\nenceQA [124], VCR [213] and VizWiz [67].\nFor im-\nage tagging we add Caltech Birds [195], iNaturalist [180],\nSun397 [197], and Places365 [224]. For region classifica-\ntion, we add examples derived from object annotation from\nOpen Images, VG, and COCO. We categorize datasets with\nopen-ended responses such as LLaVa [119], Visual Story-\ntelling [82], and Visual Dialog [36] as visual instruction\nfollowing, and we categorize NLVR [171] and the “spot the\ndifferences” tasks from MIMIC-IT as image pair QA. For\nimage pair QA tasks, we encode the second image in the\nimage history modality.\nWe also add a grounded relationship prediction task us-\ning data from Visual Genome and VSR [116] as well as\nimage captioning using the same supervised sources we use\nfor image generation.\nWe again put stylistic hints in the prompts for these tasks.\nFor example, in VQA and captioning datasets, we specify\nto return a short answer (e.g. “Answer this question very\nsuccinctly: {question}”), which we find is critical to allow\nthe model to produce longer, more natural responses when\nasked user questions. Likewise, we roughly specify the kind\nof class to output for image tagging, e.g., “”What is the sci-\nentific name of this animal?” for the iNaturalist dataset.\nD.5. Image Sparse Labelling\nThese tasks require outputting sparse coordinates based on\nan input image. We use Open Images, Visual Genome, and\nCOCO for object detection and localization, which requires\ndetecting all objects belonging to a specific class and three\nCOCO referring expression datasets [91, 136, 209] for re-\nferring expressions.\nIn addition, we train on the OmniLabel [16] 3D detection\ndataset by generating the projected 3D center, virtual depth,\nlog-normalized box size, and rotation of each 3D box, again\nby normalizing these values between 0 and 1 and then en-\ncoding them using the location tokens. We also added the\ncamera pose prediction tasks using Objaverse objects that\nwere used during pre-training.\nWe include 3 text detection datasets from COCO-\nText [183], including finding the bounding box of an input\ntext string for multiple text strings or finding and listing all\ntext along with their bounding boxes in an image.\nLastly, we do keypoint detection using COCO pose data.\nFor keypoint detection, we input a bounding box around a\nperson in the image and train the model to return a list of\nkeypoints for that person. During inference, we first localize\nall people in the image and then use each returned bounding\nbox as a keypoint query to find that person’s keypoints. Dur-\ning training, the model predicts “MISSING” for keypoints\nthat are not visible (e.g. “right elbow: MISSING”). During\ninference, we use a masking function over the model’s logit\nto force it to guess a valid point for each keypoint since the\nkeypoint metric does not award points for correctly identi-\nfying a keypoint as being not visible.\n29\nD.6. Image Dense Labelling\nWe do several image labeling tasks, including surface nor-\nmal estimation on FramNet [78], BlendedMVS [204] and\nTaskonomy [211], depth on NYU Depth [138], and optical\nflow on Flying Chairs [44] and MPI Sintel [21].\nWe additionally train on several segmentation tasks: se-\nmantic segmentation (segmenting a particular class), lo-\ncalization segmentation (segmenting an object in an input\nbounding box), and referring expression segmentation (seg-\nmenting an object matching a referring expression). Data\ncomes from Open Images, COCO, LVIS, and referring ex-\npressions from the COCO refexp datasets [91, 136, 209]. To\ndo instance segmentation, as needed for GRIT, we first do\nlocalization on the target class and then perform localized\nsegmentation on each returned bounding box.\nDuring inference, we do temperature sampling with a\ntop-p of 0.95 as before, but without classifier-free guidance.\nFor segmentation, we find it beneficial to increase the value\nof p to 0.97.\nD.7. Video Understanding\nThese tasks require generating text in response to a query\nabout a video.\nFor video captioning, we add VA-\nTEX [190] and MSR-VTT [199].\nFor action classifica-\ntion (video tagging), we add UCF101 [168], Kinetics-\n710 [111], Something-Something v2 [58] and EPIC-\nKITCHENS-100 [35]. We also use examples from EPIC-\nKITCHENS-100 for action anticipation. For video question\nanswering, we add MSRVTT-QA [198], MSVD-QA [198],\nSTAR [196] and M4-ViteVQA [221]. Lastly, we use ex-\namples from M3IT and MIMIC-IT for the video instruction\nfollowing.\nTo cover the visual content of the entire video with a\nsmall number of frames (5), we use the segment-based sam-\npling following [185]; we first divide the video into five\nsegments of equal duration and then randomly sample one\nframe from each of the segments during training, and the\nmiddle frame at inference. We use the first four frames as\nthe image history input and the final frame as the image in-\nput for action classification and video captioning. We em-\npirically found that using the third frame as the image input\nwhile using the other frames as the image history input per-\nforms better for video question answering.\nWe use similar prompts to those for image understanding\ntasks, e.g., “Write a short description of this video.”, “The\nquestion {question} can be answered using the video. A\nshort answer is” and “What are they doing in this video?\nShort answer:” in video captioning, video question answer-\ning, and video tagging, respectively, for ensuring a short\nanswer.\nD.8. Video Sparse Labelling\nWe do single object tracking and spatial-temporal action lo-\ncalization on video data. We train on YouTube-BB [151],\nLaSOT [50] and GOT-10k [79] by inputting bounding boxes\naround a target object in each of previous frames and hav-\ning the model return the next location as a bounding box\n(“Anticipate the object’s next location from all previous\nimages and the location of the object in those frames:\n{locations}.”). We also train the model on AVA [61] by\ninputting a video snippet consisting of five frames and re-\nquiring the model to detect all actions of humans appearing\nin the middle (third) frame of the video snippet (“Given the\ntemporal context from the video, detect all of the humans\nperforming actions in the image.”). Note that we provide\nthe video snippet, not a single video frame, because some\nof the actions require temporal context to answer (e.g., stand\nand sit) correctly. We use the final\/middle frame of five con-\nsecutive frames in the video as the image input and the other\nframes as the image history input for single object tracking\nand action localization, respectively.\nD.9. Audio Understanding\nWe train the model on audio tagging and audio captioning\ntasks.\nFor audio tagging, we add AudioSet [54], VGG-\nSound [24], and MACS. For audio captioning, we use the\nsame datasets as text-to-audio generation, that is, Audio-\nCaps, Clotho, MACS, MusicCaps, and LJSpeech.\nFor\naudio-visual action classification, we train on Kinetics-\nSounds [7] and VGG-Sound.\nWe again use stylistic hints in the prompts for these tasks.\nFor example, we specify the characteristics of target audio\n(e.g., “Describe the music.” and “Transcribe the audio to\ntext.” for MusicCaps and LJSpeech, respectively), enforce\na short answer (e.g., “What is this in the audio? Short an-\nswer:” and “Give a short description of this audio.”), and\nspecify the kind of class to output for audio tagging, e.g.,\n“This audio depicts a scene of a” for MACS. We use the\nsame prompts as video tagging for audio-visual action clas-\nsification.\nWe use the same sampling strategy as the video under-\nstanding; we sample five audio segments with uniform in-\ntervals from the whole audio and use the middle\/final audio\nsegment as the audio input while using the other segments\nas the audio history input for audio classification and audio\ncaptioning, respectively.\nD.10. Embodied AI\nWhile many robot manipulation tasks can be formulated by\nmultimodal prompts that interleave language and images\nor video frames, we use VIMA-Bench [87] to evaluate the\nrobot manipulation skills. We use the image input as the\ninitial observation of the environment and the image history\nfor the images or videos in the prompt. The text inputs, or\n30\nImage History\nImage Input\n[Text] [S] Follow this motion for <image_histroy_1>: <image_history_2> <image_history_3> <image_history4>.  Now predict actions with \nthe format: \"step: start-action from ( x1 y1 r1 ) and end-action to ( x2 y2 r2 )\" based on the agent's initial observation <image_input>.\nText Input\nFollow this motion for           :                                                                                             .        .\n1: pick from ( <extra_id_424> <extra_id_665> <extra_id_200> ) and \nplace to ( <extra_id_674> <extra_id_914> <extra_id_200> ) \n2: pick from ( <extra_id_668> <extra_id_671> <extra_id_200> ) and \nplace to ( <extra_id_668> <extra_id_921> <extra_id_1199> )\n(0.22, 0.46, 0.00)\n(0.47,  0.71,  0.00)\n(0.46, 0.47, 0.00)\n(0.46, 0.72, 0.99)\nPrimitive: \nPick & Place\nStep 1:\nStep 2:\nText Target\nPos X,  Pos Y, Rot Z\nPos X,  Pos Y, Rot Z\n[Text] [S] The agent is advancing towards ( <extra_id_718> <extra_id_653> ), \nassess the next action with the discrete action space \"forward, left, right, stop\" \nbased on agent's histories with the format \"observation ( x z y ) action\":\n\n<image_history_1> ( <extra_id_686> <extra_id_642> <extra_id_211> ) forward \n<image_history_2> ( <extra_id_686> <extra_id_640> <extra_id_211> ) forward \n<image_history_3> ( <extra_id_686> <extra_id_637> <extra_id_211> ) right \n<image_history_4> ( <extra_id_686> <extra_id_637> <extra_id_1182> ) forward \n<image_input> ( <extra_id_686> <extra_id_635> <extra_id_1182> )\nforward\n[Image] [S] Given the initial observation <image_input> and prompt \n\"Open the drawer.\", predict the goal image.\n[Text] [S] Imagine you are the robot in the scene with the action space (x, y, z, roll, pitch, yaw, gripper). Based on the prompt \"Moved the \nsilver pot to the right side of the table.\" and current observation <image_input>, guess the following action. History:\n \n<image_history_1>: ( <extra_id_696> <extra_id_705> <extra_id_726> <extra_id_767> <extra_id_683> <extra_id_711> <extra_id_200> ) \n<image_history_2>: ( <extra_id_705> <extra_id_670> <extra_id_742> <extra_id_753> <extra_id_656> <extra_id_674> <extra_id_200> ) \n<image_history_3>:  ( <extra_id_703> <extra_id_632> <extra_id_728> <extra_id_765> <extra_id_665> <extra_id_656> <extra_id_200> ) \n<image_history_4>: ( <extra_id_696> <extra_id_622> <extra_id_706> <extra_id_750> <extra_id_695> <extra_id_645> <extra_id_200> )\n( <extra_id_687> <extra_id_602> <extra_id_695> <extra_id_750> <extra_id_691> <extra_id_625> <extra_id_200> )\n[Text] [S] What is the robot doing?\nmove the yellow heart towards up\nImage History\nImage Input\nText Target\nText Input\nVisual Observation        Pos X                   Pos Z                   Rot Y               Action\nPoint Goal:           Pos X                   Pos Z\nText Input\nText Target\nVisual Observation     △ Pos X            △ Pos Y          △ Pos Z             △ Rot X              △ Rot Y             △ Rot Z            Gripper\nText Input\nText Input\nImage Input\nImage Target\nText Target\nImage History\nFigure 11. Examples of input and target representations for embodied and robot tasks.\nthe language instructions, also include special tokens to ex-\nplicitly express the interleaved multimodal prompt. The ac-\ntion space consists of primitive actions of “pick and place”\nfor tasks with a suction cup as the end effector or “push”\nfor tasks with a spatula. Both primitive actions contain two\nposes and one rotation ∈R3, specifying the start and target\nstates of the end effector.\nWith the action representation described in B.1, we\nseamlessly add large-scale manipulation datasets Language\nTable [127], BridgeData V2 [184], and FrankaKitchen [63]\nwith the continuous control in both simulated and real-\nworld environments. The model directly predicts the next\naction as the text target based on the current observation\nas image input, previous frames as image history, and lan-\nguage instruction and previous actions as text inputs.\nDue to the non-causality of the model and limited se-\nquence length for the image history, we only added the\nPointNav task from Habitat [157] Gibson scenes for the\nnavigation. The model is required to predict the next action,\nwith random augmentation for predicting the next position\nand rotation state, based on the point goal (positions ∈R2),\nvisual observations, and previous actions and states, if any.\nD.11. Task Augmentation\nIn addition to these sources, we derive several additional\ntasks that use the same supervised annotations as other tasks\nbut require performing slightly different functions. We call\nthis task augmentation. The new tasks include prompts that\nspecify the desired output. These tasks serve to add diver-\nsity to our instruction following data. We review the task\naugmentation we construct below.\nSegmentation. We build several augmentations of the seg-\nmentation tasks, including (1) segmenting pixels belonging\nto one of a set of 2-4 categories, possibly including cate-\ngories that do not exist in the image, (2) segmenting pix-\nels belonging to a class and are within an input bound-\ning box and (3) build a map of pixels that do not belong\nto a set 1-4 classes. Prompts are designed for these that\nstate the requirement, e.g., “Show pixels that are part of\nchair, paper and in <extra id 289> <extra id 871> <ex-\ntra id 781> <extra id 1156>”.\nDetection and Referring Expression. For detection, local-\nization, and referring expressions, we also train the model to\noutput various properties of the output bounding boxes in-\nstead of the boxes themselves. Properties include the width,\nheight, area, left\/right\/top\/bottom half, center coordinates,\ndistance from the left\/right\/top\/bottom edge of the image,\nor the coordinates of different corners of the bounding box.\nWe also change the format of the output bounding box (e.g.,\n[x1, y1, w, h] instead of [y1, x1, y2, x2] format), and change\nwhether the model labels the boxes with the object category\nor not.\n31\nPrompt\nModel Response\nA video of a man (woman) saying UNIFIED-IO 2 is a model that works with vision, language, audio, and action.\nn (n)\nA video of a man playing guitar. n\nn\nTable 12. Audio generation samples from the pre-trained model.\nInput image\nRotate right\nRotate right\nMove forward\nMove forward\nRotate right\nMove forward\nFigure 12. Future frame prediction samples from the pre-trained model. Given the initial input image and action, the model can generate a\nplausible frame that reflects the result of the action.\nFor detection, we train the model to detect any object\nbelonging to a set of 1-4 classes. For referring expressions,\nwe train the model to locate multiple referring expressions\nfrom a single query. In this case, we sometimes train the\nmodel to predict a property of both referenced boxes in-\nstead of outputting the directly, for example, which box is\nthe smallest, which is the largest, the area of intersection, a\nbox containing both boxes, etc.\nRelationship Prediction. We train the model to list all re-\nlationships between a particular object in the image and any\nother object. A bounding box and category specify the tar-\nget object. Similarly, we train the model to predict all rela-\ntionships between any instance of a particular class of ob-\njects and any other object in the image.\nCaptioning. For captioning, we train the model to generate\na caption that is longer or shorter than a given character or\nword length or contains a particular word or set of words.\nWe also randomly require the caption to start with a partic-\nular prefix. Again, these requirements are specified in the\nprompt, for example, “Generate a caption longer than five\nwords for this image. Start your output with the text ‘My\ncaption is:’”.\nSurface Normal Estimation. For surface normal estima-\ntion, we train the model to generate RGB images that en-\ncode the pixel orientation differently. This includes chang-\ning which RGB channels correspond to the x, y, and z ori-\nentations and only including a subset of those orientations.\nWe also include tasks that require specifying the x, y, and\nz orientation at a particular point specified in the prompt\nusing location tokens. Finally, we include tasks requiring\nsegmentation masks over pixels with particular orientations,\ne.g., “Build a binary mask over surfaces with an upward ori-\nentation”.\nFigure 13. Image generation samples from the pre-trained model.\nPrompts from left to right: 1) An image of an astronaut riding a\nhorse in the forest. There is a river in front of them with water\nlilies, Fantasy, HD. 2) A image of a cat wearing sunglasses, HD.\n3) A image of a black german shepherd wearing a red beret, HD.\n4) An image of a stop sign in a Fantasy style with the text “1991”.\nEmbodied AI. We further augment the embodiment\ndatasets with the video QA and goal image generation tasks.\nThe QA augmentation aims for the robot’s planning and af-\nfordance. For example, given a robot video trajectory, the\nmodel is supposed to predict the plan (caption), or whether\na given action is reasonable from the language instruction.\nApplying image editing in embodied space, we further let\nthe model generate the goal or subgoal images based on\nthe initial visual observation in the image input and the lan-\nguage prompt in the text input. While recent works show\nthat embodiment QA with VLM [46, 162] and (sub-)goal\ngeneration with diffusion model [134] are effective in the\ndecision-making downstream tasks, our model combines\nthe both augmentation strategies.\nE. Experiment Details\nE.1. Pre-training Visualization\nIn the main paper, we evaluate the effectiveness of our pre-\ntraining by evaluating UNIFIED-IO 2 quantitively on a va-\nriety of benchmarks. Here, we qualitatively show the vi-\n32\nCategorization\nLocalization\nVQA\nRefexp\nSegmentation\nKeypoint\nNormal\nAll\nablation\ntest\nablation test ablation test ablation test ablation test ablation test ablation test ablation test\n0\nNLL-AngMF [11]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n49.6\n50.5\n7.2\n7.1\n1\nMask R-CNN [70]\n-\n-\n44.7\n45.1\n-\n-\n-\n-\n26.2\n26.2\n70.8\n70.6\n-\n-\n20.2\n20.3\n2\nGPV-1 [65]\n33.2\n33.2\n42.8\n42.7\n50.6\n49.8\n25.8\n26.8\n-\n-\n-\n-\n-\n-\n21.8\n21.8\n3\nCLIP [146]\n48.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n6.9\n-\n4\nOFALARGE [186]\n22.6\n-\n-\n-\n72.4\n-\n61.7\n-\n-\n-\n-\n-\n-\n-\n22.4\n-\n5\nGPV-2 [89]\n54.7\n55.1\n53.6\n53.6\n63.5\n63.2\n51.5\n52.1\n-\n-\n-\n-\n-\n-\n31.9\n32.0\n5\nDINO + SAM [94, 139]\n-\n-\n66.0\n66.0\n-\n-\n-\n-\n60.2\n60.1\n-\n-\n-\n-\n18.0\n18.0\n6\nUNIFIED-IOSMALL\n42.6\n-\n50.4\n-\n52.9\n-\n51.1\n-\n40.7\n-\n46.5\n-\n33.5\n-\n45.4\n-\n7\nUNIFIED-IOBASE\n53.1\n-\n59.7\n-\n63.0\n-\n68.3\n-\n49.3\n-\n60.2\n-\n37.5\n-\n55.9\n-\n8\nUNIFIED-IOLARGE\n57.0\n-\n64.2\n-\n67.4\n-\n74.1\n-\n54.0\n-\n67.6\n-\n40.2\n-\n60.7\n-\n9\nUNIFIED-IOXL\n61.7\n60.8\n67.0\n67.1\n74.5\n74.5\n78.6\n78.9\n56.3\n56.5\n68.1\n67.7\n45.0\n44.3\n64.5\n64.3\n9\nUIO-2L\n70.1\n-\n66.1\n-\n67.6\n-\n66.6\n-\n53.8\n-\n56.8\n-\n44.5\n-\n60.8\n-\n10 UIO-2XL\n74.2\n-\n69.1\n-\n69.0\n-\n71.9\n-\n57.3\n-\n68.2\n-\n46.7\n-\n65.2\n-\n11 UIO-2XXL\n74.9\n75.2\n70.3\n70.2\n71.3\n71.1\n75.5\n75.5\n58.2\n58.8\n72.8\n73.2\n45.2\n44.7\n66.9\n67.0\nTable 13. GRIT results and additional baselines from the GRIT leaderboard.\nHellaSwag MMLU Arc Easy Arc Cha. BoolQ\nUIO-2L\n39.4\n28.4\n41.8\n26.2\n66.6\nUIO-2XL\n49.9\n29.7\n49.5\n31.3\n72.8\nUIO-2XXL\n52.7\n30.4\n55.3\n33.5\n77.3\nOpen LLaMA 3B\n52.0\n23.9\n69.3\n33.8\n67.0\nLLaMA 7B\n57.1\n42.6\n76.4\n43.5\n77.7\nLLaMA 7B Chat\n57.7\n47.6\n74.4\n44.0\n80.7\nTable 14. Results on NLP tasks.\nsualizations from the pre-trained UIO-2XXLmodel. Table\n12 shows audio generation from text (top) and text + video\n(bottom). We can see the pre-trained model learns text-to-\nspeech synthesis through video pre-training, and the model\ncan also synthesize music that matches the video input. Fig-\nure 12 shows the future frame prediction samples given the\ninitial input image and action sequence. Figure 13 shows\nthe image generation samples given prompts. The model\nhas a good understanding of different objects. However, it\nstruggles to generate the correct text from the given caption.\nE.2. NLP Results\nWe present results on a set of NLP tasks to evaluate the\nmodel’s language understanding abilities. We evaluate us-\ning the EleutherAI LM-Eval harness [51], tasks are evalu-\nated zero-shot using the default prompts without any adjust-\nments aside from adding the [Text] [S] prefix used for\nall text generation tasks. We evaluate on HellaSwag [214]\nand a selection of other question answering benchmarks:\nMMLU [72], ARC [31], and BoolQ [30]. Results are shown\nin Table 14. Baselines were evaluated in the same setting,\ni.e., zero-shot, with the default prompts, and using LM-\nEval. UNIFIED-IO 2 is generally ahead of Open LLaMA\n3B but behind LLaMA.\nE.3. GRIT Details\nWe present GRIT results in more detail in Table 13. No-\ntably, UNIFIED-IO 2 is the first unified model to pass the\nMasked R-CNN baseline for localization and goes a long\nway toward closing the gap between SAM and unified mod-\nels on segmentation.\nFor GRIT VQA, looking at the scores from GRIT on dif-\nferent VQA subsets, we find that UNIFIED-IO 2 does bet-\nter on the same-source subset (84.6 vs 58.5) but worse on\nthe new-source subset (57.7 vs 67.2). Same-source ques-\ntions come from VQA 2.0, and new-source questions come\nfrom VG, so the difference can be attributed to the kinds\nof questions being asked. Qualitatively, it is hard to under-\nstand why the scores differ on these subsets since the GRIT\nablation questions lack ground truth annotations.\nHow-\never, we notice the models often produce different answers\nwhen faced with ambiguous questions (e.g. “What color is\nblack on the horse”, “hair” for UNIFIED-IO vs. “mane” for\nUNIFIED-IO 2), so one possibility is that UNIFIED-IO 2\ndoes not match the VG answer style as well as UNIFIED-IO,\nwhich would likely be due to differences in the kind of VQA\ntraining data the models were trained on.\nFor GRIT localization, we find the model can struggle\nwith images with many instances of the target class, par-\nticularly when using beam search.\nWe hypothesize that\nthis is because the probability mass can get split between\nmany similar location tokens, resulting in EOS becoming\nthe most probable token even if its probability is low. As a\nsolution, during inference, we only output EOS if the EOS\n33\nSplits\nMetrics\nUIO-2XXL\nUIO-2XL\nUIO-2L\n[206]\n[207]\n[26]\nRandom\nAccuracy (↑)\n90.90\n88.27\n84.03\n88.28\n90.24\n86.90\nPrecision (↑)\n94.30\n97.44\n77.73\n94.34\n97.72\n94.40\nRecall (↑)\n87.07\n78.60\n95.40\n82.20\n83.00\n79.27\nF1-Score (↑)\n90.54\n87.01\n85.66\n87.85\n89.76\n86.19\n% Yes\n46.17\n40.33\n61.37\n44.91\n43.78\n43.26\nPopular\nAccuracy (↑)\n88.17\n87.47\n77.27\n86.20\n84.90\n83.97\nPrecision (↑)\n89.13\n95.69\n70.03\n89.46\n88.24\n87.55\nRecall (↑)\n86.93\n78.47\n95.33\n82.06\n80.53\n79.20\nF1-Score (↑)\n88.02\n86.23\n80.75\n85.60\n84.21\n83.16\n% Yes\n48.77\n41.00\n68.07\n45.86\n45.63\n45.23\nAdversarial\nAccuracy (↑)\n84.17\n85.77\n72.00\n84.12\n82.36\n83.10\nPrecision (↑)\n82.17\n92.01\n65.00\n85.54\n83.60\n85.60\nRecall (↑)\n87.27\n78.33\n95.33\n82.13\n80.53\n79.60\nF1-Score (↑)\n84.64\n84.62\n77.30\n83.80\n82.00\n82.49\n% Yes\n53.10\n42.57\n73.30\n48.00\n48.18\n46.50\nTable 15. Object hallucination benchmark POPE results, in com-\nparison with mPLUG-Owl2 [206], Ferret [207], and Shikra [26].\nImg\nVideo\nAll\nInstructBLIP [34]\n58.8\n38.1\n53.4\nVideoChat-7B [110]\n39.0\n33.9\n37.6\nOtter-7B [108]\n42.9\n30.6\n39.7\nQwen-VL-7B [12]\n62.3\n39.1\n56.3\nQwen-VL-chat-7B [12]\n65.4\n37.8\n58.2\nmPLUG-Owl2-7B [206]\n64.1\n39.8\n57.8\nLLaVA-1.5-7B [118]\n-\n-\n58.6\nLLaVA-1.5-13B [118]\n68.2\n42.7\n61.6\nUNIFIED-IO 2L\n56.0\n37.5\n51.1\nUNIFIED-IO 2XL\n64.1\n45.6\n60.2\nUNIFIED-IO 2XXL\n65.7\n46.8\n61.8\nTable 16. Results on SEED-Bench [106]. Our XXL model out-\nperforms all 7B vision language models and is even slightly better\nthan the LLaVA-1.5 13B model.\ntoken itself has a probability of over 0.5, which we find sig-\nnificantly improves the performance on crowded images.\nIn rare cases, we observe this leads to the model generat-\ning bounding boxes for the same instance multiple times.\nAs a solution, we apply Non-maximum suppression with a\nhigher threshold of 0.8 to remove these duplicates. We ap-\nply this inference trick for localization and when doing the\ninitial localization step for the keypoint and segmentation\ntasks.\nE.4. Multimodal Benchmark Details\nWe now provide the breakdown results for the evaluation-\nonly multimodal benchmarks, POPE [113] and SEED-\nBench [106]. POPE is the object hallucination benchmark,\nrequiring ‘yes’ or ‘no’ answers. As shown in Table 15, our\nlargest model achieves the highest F1 score in all 3 dimen-\nsions. Interestingly, smaller models favored ‘no’ responses,\npossibly due to a bias from negative examples encountered\nduring the instruction tuning phase. SEED-Bench offers\n19k multiple-choice questions with human annotations for\nevaluating multimodal models across 12 dimensions, in-\ncluding spatial (Image) and temporal (Video) understand-\nGroundtruth\nUIO-2XXL\nWithout CLF\n13.39\nUIO-2XXL\nWith CLF\n33.77\nA pizza with pineapple and\nmeat topping on a metal pizza pan.\nAn elephant standing in\nthe dry grass at the edge of water.\nThe big grey bear is staring at something\nA stack of pan cakes sitting on\ntop of a plate.\nA picture of a vacant home's kitchen and\nliving room.\nFigure 14. Samples generated by UIO-2XXL for the MS COCO\ncaptions [115].\nWhile the classifier-free guidance [74] signifi-\ncantly boosts image quality and fidelity, it achieves poor FID.\ning. As shown in Table 16, our XXL model outperforms all\nother 7B vision\/video language models, and is even slightly\nbetter than the LLaVA-1.5 13B model. Notably, our XL\n(3B) model has already outperformed all other counterparts\nin the temporal understanding split. While recent video lan-\nguage models [108, 110, 128] have shown proficiency in\nconventional video tasks like video tagging and caption-\ning, their performance in SEED-Bench’s temporal under-\nstanding is even worse than that of vision language mod-\nels, which might be attributed to their limited instruction-\nfollowing capabilities.\nE.5. Image Generation Details\nFigure 15 shows generated images for the TIFA benchmark\ncaptions [76] using several baselines as well as UIO-2XXL.\nWe use the official implementation code (Emu [172] and\nCoDi [174]) or the images shared in the official GitHub\nrepository of TIFA5 (Stable Diffusion v1.5 [154] and\nminiDALL-E [37]) for baselines. All the baselines except\nminiDALL-E use the Stable Diffusion decoder trained on\nlarge-scale, high-quality image datasets, generating images\nof high fidelity. However, they often generate images that\ndo not fully follow the input captions while UNIFIED-IO 2\ngenerates faithful images.\nFor text-to-image generation on MS COCO [115], we\nfollow the standard convention [226]; we evaluate on a sub-\n5https:\/\/github.com\/Yushi- Hu\/tifa\/tree\/main\/\nhuman_annotations\n34\nEmu\n65.5\nCoDi\n71.6\nSD-1.5\n78.4\nminiDALL-E\n79.4\nUIO-2XXL\n81.3\ntwo laptops a mouse cords\nwires and a monitor\nA red motorcycle parked\nby paint chipped doors.\nA boy wearing a green shirt\nposing with some fruit.\na photo of blue fire hydrant\na photo of bike and skateboard;\nskateboard is left to bike\na mountain with a cloud\nhanging over it\nA Mesoamerican pyramid\nsurrounded by jungle.\ndetailed charcoal sketch.\nA photo of an Athenian vase\nwith a painting of toucans playing tennis\nin the style of Egyptian hieroglyphics\nA Christmas tree with\nlights and teddy bear\nFigure 15. Samples generated for the TIFA bnechmark captions [76]. Some of the images generated by baselines (e.g., rows 1-2, 6, 9) have\nhigh quality but do not fully follow the input text while UNIFIED-IO 2 generates faithful images.\n35\nPrompt\nModel Response\nWhat is the sound of a duck quacking?\nn\nA bell is ringing loudly and quickly.\nn\nSpeak out the text “Police protection was better and more effective;” for me.\nn\nSpeak: Printing, then, for our purpose, may be considered as the art of making books by means of movable types.\nn\nRead “Many animals of even complex structure which live parasitically within others are wholly devoid of an alimentary cavity.”\nn\nGenerate the music based on the description “Slow tempo, bass-and-drums-led reggae song.”\nn\nBased on the given description “Industrial techno sounds, repetitive, hypnotic rhythms”, produce a corresponding piece of music.\nn\nTable 17. Audio generation examples. UNIFIED-IO 2 can generate not only environmental sound (rows 1-2), but also speech (rows 3-5)\nand music (rows 6-7). Note that some of the outputs longer than 4.08 seconds have discontinuity in sound, or changes in tone, speed\nor melody (rows 4-5, 7). Since our model can output 4.08-second audio at a time, we complete the audio clip by using any previously\ngenerated clips as additional input. Click n for audio samples.\nset of 30K captions sampled from the validation set.6 Fol-\nlowing [43], we generate 8 images for each caption and se-\nlect the best one using CLIP text-image similarity [146].\nDespite classifier-free guidance [74] resulting in generated\nimages of qualitatively higher quality, the computed FID\nscore [73] is significantly worse compared to what would\nhave been achieved without employing it (33.77 vs 13.39);\nsee Figure 14.\nE.6. Audio Generation Details\nFor text-to-audio generation, we evaluate on the Audio-\nCaps [93] test set. Note that we cannot do an apples-to-\napples comparison with other methods because AudioCaps\nconsists of 10-second audio clips while our model can gen-\nerate 4.08-second audio at a time. Instead, we evaluate the\ndataset in the following setup: we first sample four 2-second\naudio segments, convert them to log-mel-spectrograms with\nzero-padding, and generate the following audio with the\nprompt “Generate the following sound based on what you\nheard and the description: {caption}”.\nWe convert the\nmodel output, that is, a log-mel-scaled spectrogram, into\na waveform using the pretrained HiFi-GAN, and compare\nthe ground-truth audio and generated audio using compu-\ntational metrics including Fr´echet Audio Distance [92], In-\nception Score [155] and Kullback–Leibler divergence. We\nuse the same evaluation code as AudioLDM7 [117]. We\nshow the audio generation examples in Table 17 and audio-\nvisual qualitative examples in Table 18.\nE.7. Video and Audio Understanding Details\nWe consider classification and question-answering tasks as\nopen-ended answer generation and use the Exact Match\n(EM) to measure the performance. We also tried to formu-\nlate the classification task as multiple-choice answering and\n6We use the evaluation code at https : \/ \/ github . com \/\nMinfengZhu\/DM-GAN\n7https:\/\/github.com\/haoheliu\/audioldm_eval\nInput Image Prompt\nModel Response\nWhat is the sound of this instrument?\nn\nWhat is the sound of this instrument?\nn\nGenerate music about this scene.\nn\nGenerate music about this scene.\nn\nn Locate the bounding boxes of the sound sources in the given image.\nn Identify the locations of the sound sources in the given image.\nn Identify the locations of the instruments producing the given sound.\nn Identify the locations of the instruments producing the given sound.\nTable 18. Audio-visual qualitative examples showcasing the abil-\nity of UNIFIED-IO 2 to reason across modalities. UNIFIED-IO 2\ncan generate the sound of the instrument in the input image (rows\n1-2), and generate the music that matches the mood of the input\nimage (rows 3-4). The last four examples (rows 5-8) show the re-\nsults of visual sound localization. Note that UNIFIED-IO 2 can\naccurately identify the instruments that make and do not make\nsounds (rows 7-8). Click n for audio samples.\ngenerate answers by computing the logit for each dataset\nlabel and selecting the one with the highest logit, but the\nperformance boost was quite marginal. Note that we do\nnot train our model directly on the Kinetics-400 [90]; we\ninstead train on Kinetics-710, a mixture of three different\ndatasets belonging to the Kinetics family, that is, Kinetics-\n400, 600, and 700. Our model achieves top-1 accuracy 79.1\n36\nGuess the next location of the object from its prior locations: <image_history_1> <extra_id_619> <extra_id_936> <extra_id_714> <extra_id_1014> object.\nUtilizing the object's past locations: <image_history_1> <extra_id_667> <extra_id_932> <extra_id_687> <extra_id_943> object, can you guess its upcoming location?\nFigure 16. Single object tracking examples from LaSOT [50]. Given the first input image and the corresponding bounding box of the target\nobject, our model can track the object across video frames.\nDescribe the video.\nA man is trying to skip stones on a clear, blue lake.\nWhat is the man doing in the video?\nThe man in the video is dancing and posing in \nfront of the camera, showing off his dance moves \nand body movements.\nWhat is the robot doing?\nMove the orange object from the table to the \ntoaster oven.\nFigure 17. Video understanding qualitative examples.\n(vs. instruction tuning only: 73.8) when further finetun-\ning on Kinetics-400 for 5 epochs, following [111].\nFor\nKinetics-Sounds, leveraging both audio and visual inputs\nlargely improves performance (audio-visual: 89.3 vs. video-\nonly: 87.4 vs. audio-only: 38.2). For captioning tasks, we\nuse CIDEr [182] as the evaluation metric. Figure 17 shows\nthe qualitative examples for video understanding tasks.\nE.8. Embodiment Details\nWhat is the final frame after completing the \ntask: \"Move the silver pot to the lower \nright of the burner\"?\nWhat does the goal image look like given \nthe current robot RGB observation and \nprompt 'Pick the green rectangle block \nand put it on the table between the \nyellow and red block'?\nTask: Move the cloth from the left to the \nright side of the burner. Given the start \nstate as image, generate the end state \nimage.\nInput\nPrompt\nPrediction\nGT\nFigure 18. Future state prediction examples on robotic manipula-\ntion tasks. Given the input image and instructions, our model can\nsuccessfully generate the target state after the prompt instruction.\nIn VIMA-Bench [87], there are 4 levels of evaluation\nprotocols: L1 object placement, L2 novel combination, L3\nL1\nL2\nL3\nL4\nAvg.\nVIMA [87]\n81.5\n81.5\n78.7\n48.6\n72.6\nVIMA-Gato [152]\n57.0\n53.9\n45.6\n13.5\n42.5\nVIMA-Flamingo [5]\n47.4\n46.0\n40.7\n12.1\n36.6\nVIMA-GPT [19]\n46.9\n46.9\n42.2\n12.1\n37.0\nUNIFIED-IO 2L\n66.9\n63.8\n57.5\n12.6\n50.2\nUNIFIED-IO 2XL\n70.3\n69.8\n64.5\n13.1\n54.2\nUNIFIED-IO 2XXL\n71.3\n70.4\n68.0\n15.5\n56.3\nTable 19. Evaluations on VIMA-Bench [87]\nnovel object, and L4 novel task. Results and comparisons\nare shown in Table 19. The inputs for the autoregressive\ntransformer model VIMA [87] are object tokens consist-\ning of cropped images and bounding boxes; image patch\ntokens encoded by ViT for VIMA-Gato [152]; image patch\ntokens encoded by ViT, further downsampled by a perceiver\nmodule for VIMA-Flamingo [5]; and single image token\nencoded by ViT for VIMA-GPT [19]. The output of those\nbaselines is all next-step action prediction. Since our model\nhas to predict all actions at once only with the initial ob-\nservation, the task setting is then more challenging than the\ncasual policy learning baselines. Nevertheless, our mod-\nels still outperform counterparts that input image or im-\nage patches for all 4 levels and are only behind the object-\ncentric method [87]. In Figure 18, we show the future state\nprediction examples on robotic manipulation tasks. Given\nthe input state image and natural language prompt, our\nmodel can successfully synthesize the target image state.\nE.9. Other Tasks\nFigure 16 shows single object tracking examples from the\nLaSOT [50] dataset.\nNote that UNIFIED-IO 2 does not\nuse specific class labels for tracking and tracks small mov-\ning objects such as a table tennis paddle well. Figure 19\npresents qualitative examples of 3D object detection from\nthe Objectron dataset [3]. As outlined in our main paper,\nUNIFIED-IO 2 exhibits suboptimal performance in bench-\nmarks for multi-object 3D detection.\nAdditionally, Fig-\nure 20 illustrates examples of image-based 3D view syn-\nthesis using the Objaverse dataset [40]. While the model\nproduces coherent results, it faces challenges in accurately\n37\nFigure 19. 3D object detection qualitative examples from Objec-\ntron [3].\nInput\nPrompt\nPrediction\nGT\nRelative camera transformation is in the \nformat of “theta sin(phi) cos(phi) r”. What is \nthe image after applying the transformation:\n( <extra_id_608> <extra_id_493> \n<extra_id_1154> <extra_id_661> )\nComplete a new image of the image following \nthe implementation of the camera \ntransformation theta sin(phi) cos(phi) r: ( \n<extra_id_616> <extra_id_1011> \n<extra_id_309> <extra_id_729> ).\n[Complete a new image of the image \nfollowing the implementation of the camera \ntransformation theta sin(phi) cos(phi) r: ( \n<extra_id_789> <extra_id_464> \n<extra_id_258> <extra_id_450> )\nFigure 20. Image-based 3D view synthesis examples from Obja-\nverse [40].\nrepresenting relative camera transformations.\n38\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action.pdf"}
{"title":"Generative Multimodal Models are In-Context Learners","authors":"Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang","summary":"The human ability to easily solve multimodal tasks in context (i.e., with\nonly a few demonstrations or simple instructions), is what current multimodal\nsystems have largely struggled to imitate. In this work, we demonstrate that\nthe task-agnostic in-context learning capabilities of large multimodal models\ncan be significantly enhanced by effective scaling-up. We introduce Emu2, a\ngenerative multimodal model with 37 billion parameters, trained on large-scale\nmultimodal sequences with a unified autoregressive objective. Emu2 exhibits\nstrong multimodal in-context learning abilities, even emerging to solve tasks\nthat require on-the-fly reasoning, such as visual prompting and object-grounded\ngeneration. The model sets a new record on multiple multimodal understanding\ntasks in few-shot settings. When instruction-tuned to follow specific\ninstructions, Emu2 further achieves new state-of-the-art on challenging tasks\nsuch as question answering benchmarks for large multimodal models and\nopen-ended subject-driven generation. These achievements demonstrate that Emu2\ncan serve as a base model and general-purpose interface for a wide range of\nmultimodal tasks. Code and models are publicly available to facilitate future\nresearch.","url":"http:\/\/arxiv.org\/abs\/2312.13286v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.13286v2","published":1703098798000,"comment":"Accepted to CVPR 2024. Project page:\n  https:\/\/baaivision.github.io\/emu2","pdf_text":"Generative Multimodal Models are In-Context Learners\nQuan Sun1∗\nYufeng Cui1∗\nXiaosong Zhang1∗\nFan Zhang1∗\nQiying Yu2,1∗\nZhengxiong Luo1\nYueze Wang1\nYongming Rao1\nJingjing Liu2\nTiejun Huang1,3\nXinlong Wang1†\n1 Beijing Academy of Artificial Intelligence\n2 Tsinghua University\n3 Peking University\n∗equal contribution\n†project lead\ncode & models: https:\/\/github.com\/baaivision\/Emu\nAbstract\nThe human ability to easily solve multimodal tasks in\ncontext (i.e., with only a few demonstrations or simple in-\nstructions), is what current multimodal systems have largely\nstruggled to imitate. In this work, we demonstrate that the\ntask-agnostic in-context learning capabilities of large mul-\ntimodal models can be significantly enhanced by effective\nscaling-up. We introduce Emu2, a generative multimodal\nmodel with 37 billion parameters, trained on large-scale\nmultimodal sequences with a unified autoregressive objec-\ntive. Emu2 exhibits strong multimodal in-context learning\nabilities, even emerging to solve tasks that require on-the-fly\nreasoning, such as visual prompting and object-grounded\ngeneration. The model sets a new record on multiple mul-\ntimodal understanding tasks in few-shot settings.\nWhen\ninstruction-tuned to follow specific instructions, Emu2 fur-\nther achieves new state-of-the-art on challenging tasks such\nas question answering benchmarks for large multimodal\nmodels and open-ended subject-driven generation. These\nachievements demonstrate that Emu2 can serve as a base\nmodel and general-purpose interface for a wide range of\nmultimodal tasks. Code and models are publicly available\nto facilitate future research.\n1. Introduction\nMultimodal tasks [25, 41] encompass anything involving\nunderstanding and generation in single or multiple modal-\nities [5, 19, 58], which can be highly diverse and long-\ntail. Previous multimodal systems largely rely on design-\ning task-specific architecture and collecting a sizable super-\nvised training set, both of which are difficult to scale, partic-\nularly when this process needs to be repeated for each new\ntask encountered. By contrast, humans can solve a new task\nin context, i.e., with only a few demonstrations or simple\n†Correspondence to wangxinlong@baai.ac.cn\ninstructions – a capability that current multimodal models\nhave yet to learn.\nRecently, generative pretrained language models have\ndemonstrated strong in-context learning abilities [11, 21,\n73]. By training a 37-billion-parameter model Emu2 and\nthoroughly evaluating it on diverse multimodal tasks, we\ndemonstrate that a scaled-up multimodal generative pre-\ntrained model can harness similar in-context learning abil-\nities and effectively generalize to unseen multimodal tasks.\nEmu2 is trained with a unified autoregressive objective:\npredict-the-next-multimodal-element (either visual embed-\ndings or textual tokens). In this unified generative pretrain-\ning process, large-scale multimodal sequences (e.g., text,\nimage-text pairs, and interleaved image-text-video) are used\nfor model training.\nWe measure Emu2’s capabilities of learning from a few\nexamples or instructions on standard multimodal datasets,\nas well as new tasks unseen in the training set. Specifically,\nEmu2 is evaluated under two scenarios: (a) few-shot set-\nting, where we allow as many examples as possible to fit\nthe context window of the model; and (b) instruction tun-\ning, where the model is tuned to follow specific instructions.\nEmu2 achieves promising results in the few-shot setting\non a wide range of vision-language tasks. For example, it\ndemonstrates state-of-the-art few-shot performance on mul-\ntiple visual question-answering datasets. We observe a per-\nformance improvement when the number of examples in\ncontext increases. Figure 1 illustrates Emu2’s strong mul-\ntimodal reasoning capabilities for tasks in the wild, e.g.,\nrecognition and counting in a specific format. Emu2 also\nlearns to follow visual prompting in context (e.g., the circles\nlaid on the images in Figure 1), even although it struggles\nat a smaller scale or at zero shot.\nAs Emu2 is inherently equipped to handle interleaved\ntext-image-video at both input and output, it serves as\na powerful and versatile base model for diverse multi-\nmodal tasks, by following specific task instructions. For\nexample, after instruct tuning with conversational data,\n1\narXiv:2312.13286v2  [cs.CV]  8 May 2024\nInput Prompt\nIn-context Completion\n[dog: 1, frisbee: 1].\n[cat: 3].\n[beer: 3, banana: 2].\nThe text in the red \ncircle: 'Rights'.\nThe text in the red \ncircle: 'Ave'.\nThe text in the red \ncircle: 'Do Not'.\nThe text in the red circle: ‘Lynn’.\nmotorcycle's wheel.\nwoman's feet.\ncar's license plate.\nmotorcycle's headlight.\nCompletion\nGenerate from any prompt sequence\n<a man> is riding <a big  horse> on the <water> in front of <mountain>\nimpressionist painting of an \nastronaut in a jungle\nAn image of <emu> wearing a big sunglasses on the beach\n[burger: 1, glass: 1, bottle: 1].\n                    \nand <a dog> \nAn anime draw of   <a baby emu>\n \n, in the style of \na photo of \na yellow \nbackpack:\na photo of \na blue\nbackpack:\na photo of \na red\nbackpack:\na photo of \na brown\nbackpack:\na photo of \na blue and red \nbackpack:\nThe subject A \nwith a city in the \nbackground:\nThe subject A wearing a \nsanta hat:\nThe subject A in a \npurple wizard outfit:\nThe subject A \nwearing a \nrainbow hat:\n <a dog>\n <a dog>\nand\nrunning \non the \ncity \nstreet\nMultimodal understanding\nLook at this sequence of three \nshapes. What shape should come \nas the fourth shape? Explain your \nreasoning with detailed \ndescriptions of the first shapes.\nThe first shape is a triangle, the \nsecond shape is a square, and \nthe third shape is a pentagon. \nTo find the next shape in the \nsequence, we need to identify a \npattern. In this case, the pattern \nis that each shape has one more \nside than the previous shape. \nTherefore, the next shape in the \nsequence should have six sides, \nwhich is a hexagon.\nWhat is funny about this image?\n。\nThe funny aspect of this image is that \na squirrel is holding a camera and \nappears to be taking a picture of a \nbird. This is amusing because \nanimals, such as squirrels and birds, \ndo not typically use cameras or \nengage in human-like activities like \nphotography. The scene is set up in a \nway that makes it look like the \nsquirrel is intentionally capturing the \nbird's image, which adds a humorous \ntwist to the scene. Additionally, the \nbird is perched on a mushroom, \nfurther enhancing the whimsical and \nplayful nature of the image.\nFigure 1. Emu2 is a large generative multimodal model that serves as a foundation and a general-purpose interface for a broad range of\nmultimodal tasks across understanding and generation, with remarkable in-context learning abilities.\n2\nEncoder\nGenerative Multimodal Model\n…\nClassification\nAn\n[\/IMG]\n[IMG]\negg that will\n<\/s>\n<s>\nemu\nhatch into a\nemu\nbaby\n[\/IMG]\n[IMG]\n…\negg that will\nemu\nhatch into a\nemu\nbaby\nAn\n<\/s>\n[IMG]\n[IMG]\nRegression\n…\n…\nDecoder\nEncoder\nDecoder\n…\nFigure 2. Overview of Emu2 architecture. Emu2 learns with a predict-the-next-element objective in multimodality. Each image in the\nmultimodal sequence is tokenized into embeddings via a visual encoder, and then interleaved with text tokens for autoregressive modeling.\nThe regressed visual embeddings will be decoded into an image or a video by a visual decoder.\nEmu2 achieves state-of-the-art results on visual question-\nanswering tasks, and surpasses previous models of more\ncomplex designs. In addition, Emu2 can be fine-tuned to\nfunction as a controllable visual generation model of high\nquality. It is capable of accepting a mixture of text, loca-\ntions and images as conditions, and generating images that\nare grounded as specified.\nGiven the broad spectrum of capabilities displayed by\nEmu2, we conduct a thorough analysis of its potential so-\ncietal implications and discuss in detail potential concerns\nover misuse. By identifying further tasks where Emu2’s\nin-context learning can further improve, we highlight the\nnecessity for continuous enhancement of the model and the\nimportance of deploying Emu2 responsibly.\n2. Approach\n2.1. Model Architecture\nEmu2 is a generative multimodal model that learns with\na predict-the-next-element objective in multimodal context.\nAs illustrated in 2, the architecture of Emu2 consists of\nthree components:\nVisual Encoder, Multimodal Model-\ning, and Visual Decoder. Each image in the input multi-\nmodal sequence is tokenized into continuous embeddings\nvia the Visual Encoder and then interleaved with text tokens\nfor autoregressive Multimodal Modeling.\nThe regressed\nvisual embeddings are then decoded into an image or a\nvideo by the Visual Decoder. Specifically, we leverage pre-\ntrained EVA-02-CLIP-E-plus [70], LLaMA-33B [73] and\nSDXL [58] to initialize the Visual Encoder, Multimodal\nModeling, and Visual Decoder, respectively. Compared to\nEmu [71], Emu2 embraces a simpler framework which con-\nnects the Visual Encoder and Multimodal Modeling through\nmean pooling each image to 8 × 8 image patches, followed\nby a linear projection, instead of using an additional C-\nFormer [71].\n2.2. Pretraining\n2.2.1\nData\nThe pretraining data for Emu2 comprises several pub-\nlicly accessible datasets, including image-text pairs from\nLAION-2B [65] and CapsFusion-120M [87], video-text\npairs from WebVid-10M [8], interleaved image-text data\nfrom Multimodal-C4 (MMC4) [95], interleaved video-\ntext data from YT-Storyboard-1B [71], grounded image-\ntext pairs from GRIT-20M introduced by Kosmos-2 [57]\nand CapsFusion-grounded-100M curated by CapsFusion-\n120M. Additionally, language-only data from Pile [26] is\nincluded to retain textual reasoning capability.\n2.2.2\nTraining\nSimilar to Emu [71], Emu2 learns with the predict-the-next-\nelement objective within a multimodal sequence. Each im-\nage is encoded into N = 64 dimension-fixed visual embed-\ndings and then interleaved with text tokens to construct a\nmultimodal sequence. The interleaved sequence is then fed\ninto a Transformer decoder for autoregressive modeling.\nEmu2 is first pretrained on image-text and video-text\npair data with only captioning loss on the text tokens. The\ninput images are resized to 224×224. We adopt the AdamW\noptimizer [50] with \\b e ta _1 = 0.9, \\b e ta _2 = 0.95, \\ e psilon = 1 \\times 10^{-6}. The\nmaximum learning rate is 1 \\times 10^{-4} for the linear projection\nlayer, 3 \\times 10^{-5} for Multimodel Modeling, and 5 \\times 10^{-5} for\nVisual Encoder. We pretrain Emu2 on 162 million image-\ntext samples and 7 million video-text samples for 35,200\niterations. The global batch size is 6,144 for the image-text\n3\npairs and 768 for video-text pairs. The training process is\nthen restarted at a higher 448-pixel resolution for an addi-\ntional 4,000 iterations.\nThen, we freeze the Visual Encoder and only opti-\nmize the linear projection layer and Multimodel Modeling\nwith both text classification loss and image regression loss.\nAdditional datasets including image-text interleaved data,\nvideo-text interleaved data, grounded image-text pair data,\nand language-only data are used in the training. All im-\nages are resized to 448 × 448, and the maximum learning\nrate is 1  \\times 10^{-5}. We use a global batch size of 12,800 for\nimage-text pair data, 6,400 for video-text pair data, 3,200\nfor image-text and video-text interleaved data, and 800 for\nlanguage-only data. The training process spans 20,350 iter-\nations and consumes about 160 million samples of image-\ntext data and 3.8B tokens of language-only data.\n2.2.3\nVisual Decoding\nWe train the Visual Decoder to directly decode visual em-\nbeddings generated by the Visual Encoder into image. We\nuse SDXL-base[58] as the initialization of our Visual De-\ncoder, which is fully trained to solve the new task of au-\ntoencoding. Specifically, we use N visual embeddings as\nthe condition input to the Visual Decoder and adjust the di-\nmension of the projection layers in cross-attention modules\nto match the dimension of visual embeddings.\nUnlike Emu [71] where each optimization step of its Vi-\nsual Decoder requires an autoregressive inference of the\nlanguage model, Emu2’s visual decoding can be considered\nas training a detokenizer, which can be trained off-the-shelf\nwithout the language model. Once trained, the Visual De-\ncoder together with the Visual Encoder works as an image\nautoencoder that can tokenize an image into embeddings\nand detokenize back. During Emu2 inference, it generates\nN image embeddings and decodes to an image on the fly.\nFor the decoding of video data, we train a diffusion-\nbased decoder [67]. Similar to [46, 74], we adapt a 2D\ndenoising U-Net to 3D style by inserting a 1D temporal\nconvolution following each 2D spatial convolutional layer\nand extending the spatial attention to spatial-temporal at-\ntention. This video decoder is initialized via Stable Diffu-\nsion 2.1 [62] and fully trained to generate video clips con-\nditioned on visual embeddings from Emu2.\nTraining Setup. We use the images in LAION-COCO [2]\nand LAION-Aesthetics [1] to train the Visual Decoder un-\nder the task of image autoencoding. The Visual Encoder\nand VAE in SDXL are frozen, and only the U-Net is up-\ndated during training. We adopt AdamW optimizer [50]\nwith β1 = 0.9, β2 = 0.999 and the weight decay of 0.01.\nWe use log learning rate warm-up and linear learning rate\ndecay with a peak learning rate of 1  \\times 10^{-4} for 2,000 and\n6,000 steps, respectively. We filter out images whose res-\nreal image\nSEED\nEmu\nEmu2\nFigure 3. Comparison of autoencoding results among different\nmethods [27, 71]. Emu2’s Visual Encoder and Visual Decoder in\nthe architecture of CLIP-Diffusers form a strong autoencoder.\nolution is lower than 512 × 512. The input to the Visual\nEncoder is set to 448 × 448, while the output of the Vi-\nsual Decoder is set to 1024 × 1024. We also employ the\nclassifier-free guidance [30], which randomly discards im-\nage embeddings with the probability of 10%. The batch size\nis set to 2,048 in total.\n2.3. Instruction Tuning\nEmu2 can be efficiently aligned to follow specific task in-\nstructions. We fine-tune the base model with conversational\ndata to yield Emu2-Chat, which is capable of following\nmultimodal questions and making responses in dialogue.\nSimilarly, we derive a controllable visual generation model\nEmu2-Gen, which is capable of accepting a mix of text,\nlocations, and images as conditions, and generating images\nthat are grounded in the specified text or subject.\n2.3.1\nInstruction-Following Chat\nTraining Data.\nWe adopt a uniform approach to train\non both academic-task-oriented datasets and multimodal\nchat data to empower Emu2-Chat with the instruction-\nfollowing ability while retaining rich visual knowledge. As\nacademic-task-oriented datasets have brief annotations that\nlimit the model’s capacity to provide more comprehensive\nand helpful responses, we distinguish between these two\ndata categories by employing different system messages\nand including instructions with output-format control in-\nformation as used in [48].\nA summary of data used is\nas follows: (a) Academic-task-oriented data: image cap-\ntioning [18, 66], visual question answering [28, 32, 68],\nknowledgeable question answering [51, 53], multimodal\nclassification [45], and referring expression comprehen-\n4\nsion [34, 52]. (b) Multimodal chat data: GPT-assisted vi-\nsual instruction [49, 93], language instruction [4, 72], clock\nreading [83], and video chat [44].\nTraining Objective.\nIn instruction tuning of Emu2-\nChat, two special tokens, [USER] and [ASSISTANT],\nare incorporated into the model to denote roles.\nThese\ntokens help organize different data types in the follow-\ning format:\n“<Sys.Msg.> [USER]: <Instruction>\n[ASSISTANT]: <Answer>”.\nHere <Sys.Msg.> repre-\nsents system message and varies between the two major task\ncategories (academic-task-oriented and multimodal chat).\nThe <Instruction> section comprises multimodal to-\nkens, including images, videos, and text. Only tokens in\nthe <Answer> section will be supervised by cross-entropy\nloss during training.\nTraining Setup. We use a global batch size of 768 and\ntrain for 8k steps. The learning rate linearly warms up to\n1 \\times 10^{-5} in the first 100 steps, then decays to zero with a co-\nsine schedule. The model is trained using the AdamW opti-\nmizer with \\b e ta _1 = 0.9, \\b e ta _2 = 0.98, \\ e psilon = 1 \\times 10^{-6}, and a gradi-\nent clipping of 5.0. The sequence length during training is\nlimited to 2048, and any excess beyond that is truncated di-\nrectly. We consistently employed an input image\/video res-\nolution of 448 × 448. For video data, we uniformly sample\nframes in time as input to the model. The number of sam-\npled frames for each video is randomly chosen from 8, 12,\nand 16. To capture more intricate spatial details, following\nthe visual encoder stage, we apply mean-pooling to each\nstatic image, dividing it into 16 × 16 tokens during instruc-\ntion fine-tuning. This differs from the pre-training phase,\nwhere 8 × 8 tokens were utilized.\n2.3.2\nControllable Visual Generation\nTraining Data.\nWe leverage a mix of high-quality\ndatasets to unleash the potential of controllable generation\nin context.\nWe use a grounded image-text pair dataset\nCapsFusion-grounded-100M and GRIT [57] for grounded\ntext-to-image generation. To mitigate the impact of image\nbackgrounds on the effectiveness of multi-entity subject-\ndriven generation, we employ SAM [36] to preprocess the\ngrounding data, yielding a subset of approximately 5 mil-\nlion samples with segmentation results. Additionally, we\nleverage InstructPix2Pix constructed by [10] for image edit-\ning tasks. For the text-to-image task, we use a filtered sub-\nset of CapsFusion [87], LAION-Aesthetics [1], SA-1B [36],\nand LAION-High-Resolution [3].\nWe also collect data from premium sources (e.g., Un-\nsplash [20]) and outputs from advanced text-to-image sys-\ntems (e.g., Midjourney-V5 [54] and DALL-E-3 [9]) for\nquality fine-tuning. This diverse dataset includes around\n500k high-quality image-text pairs. For all the data above,\nduring the training, only samples with image resolutions\nhigher than 448 × 448 were retained to ensure generation\nquality. More details can be found in the supplementary.\nModel\nShot VQAv2 OKVQA VizWiz TextVQA Hateful\nMemes\nKosmos-1 (1.6B)\n0\n51.0\n-\n29.2\n-\n-\n4\n51.8\n-\n35.3\n-\n-\n8\n51.4\n-\n39.0\n-\n-\nFlamingo (9B)\n0∗\n51.8\n44.7\n28.8\n31.8\n57.0\n4\n56.3\n49.3\n34.9\n33.6\n62.7\n8\n58.0\n50.0\n39.4\n33.6\n63.9\n16\n59.4\n50.8\n43.0\n33.5\n64.5\nFlamingo (80B)\n0∗\n56.3\n50.6\n31.6\n35.0\n46.4\n4\n63.1\n57.4\n39.6\n36.5\n68.6\n8\n65.6\n57.5\n44.8\n37.3\n70.0\n16\n66.8\n57.8\n48.4\n37.6\n70.0\nIDEFICS (80B)\n0∗\n60.0\n45.2\n36.0\n30.9\n60.6\n4\n63.6\n52.4\n40.4\n34.4\n57.8\n8\n64.8\n55.1\n46.1\n35.7\n58.2\n16\n65.4\n56.8\n48.3\n36.3\n57.8\nEmu (14B)\n0∗\n52.9\n42.8\n34.4\n-\n-\n4\n58.4\n-\n41.3\n-\n-\n8\n59.0\n-\n43.9\n-\n-\n16\n-\n-\n-\n-\n-\nEmu2 (37B)\n0\n33.5\n26.7\n40.4\n26.4\n52.2\n4\n67.0\n53.2\n54.6\n48.2\n62.4\n8\n67.8\n54.1\n54.7\n49.3\n65.8\n16\n68.8\n57.1\n57.0\n50.3\n66.0\nTable 1. Zero-shot and few-shot evaluations of Emu2. 0∗denotes\ntext two-shot and image zero-shot results following Flamingo [5].\nThe best results are in bold and the second best are underlined.\nTraining Objective.\nWe use the same unified gen-\nerative pretraining objective to adapt to diverse gen-\neration tasks in context.\nSpecifically, a training sam-\nple for generation is formulated as:\n“<s>A photo\nof <p>a man<\/p><coor>image embedding of\nobject localization image<\/coor>[IMG]image\nembedding of man[\/IMG]sitting next to\n<p>a dog<\/p><coor>image embedding of\nobject localization image<\/coor>[IMG]image\nembedding of dog[\/IMG][IMG]image embedding\nof the whole image[\/IMG]<\/s>”.\nWe\nrepresent\nthe coordinates of each object directly in image form by\ndrawing the bounding box of each object at its specified\nlocation on a black image.\nOur Emu2-Gen conducts\nunified multimodal modeling of the text, object image, and\ncorresponding object localization image.\nThe regression\nloss only applies to the visual embeddings of the last\nimage. We freeze the Visual Encoder during fine-tuning.\nWe randomly drop tokens of entities and object localization\nimage to enhance model adaptability and robustness. Addi-\ntionally, we apply data augmentation to each object image,\nincorporating random background variations and random\ncrop, aiming to reduce the reliance on image backgrounds.\nTraining Setup. We use a global batch size of 4,096 and\n5\nModel\nVisual Question Answer\nLMM Benchmarks\nVQAv2\n[28]\nOKVQA\n[53]\nGQA\n[32]\nVizWiz\n[29]\nTextVQA\n[68]\nMSVD\n[82]\nMSRVTT\n[82]\nSEED\n[39]\nMM-Vet\n[88]\nTS\n[7]\nMMMU\n[89]\nFlamingo-9B [5]\n51.8\n44.7\n-\n28.8\n-\n30.2\n13.7\n-\n-\n-\n-\nFlamingo-80B [5]\n56.3\n50.6\n-\n31.6\n-\n35.6\n17.4\n-\n-\n-\n-\nKosmos-1 [31]\n51.0\n-\n-\n29.2\n-\n-\n-\n-\n-\n-\n-\nKosmos-2 [57]\n51.1\n-\n-\n-\n-\n-\n-\n50.0\n-\n-\n26.6\nBLIP-2-13B [42]\n-\n-\n41.0\n19.6\n42.5\n20.3\n10.3\n46.4\n22.4\n-\n-\nInstructBLIP-13B [22]\n-\n-\n49.5\n33.4\n50.7\n41.2\n24.8\n-\n25.6\n552.4\n-\nIDEFICS-9B [38]\n50.9\n38.4\n-\n35.5\n25.9\n-\n-\n-\n-\n-\n-\nIDEFICS-80B [38]\n60.0\n45.2\n-\n36.0\n30.9\n-\n-\n-\n-\n-\n-\nShikra-13B [15]\n77.4*\n47.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\nQwen-VL-13B-Chat [6]\n78.2*\n56.6*\n57.5*\n38.9\n61.5*\n-\n-\n58.2\n-\n645.2\n-\nLLaVA-1.5-13B [48]\n80.0*\n-\n63.3*\n53.6\n61.3\n-\n-\n61.6\n35.4\n-\n33.6\nCogVLM [77]\n83.4*\n58.9*\n-\n-\n68.1*\n-\n-\n-\n-\n662.6\n30.1\nEmu-I [71]\n62.0\n49.2\n46.0\n38.3\n-\n37.0\n21.2\n-\n36.3\n-\n-\nEmu2-Chat\n84.9*\n64.8*\n65.1*\n54.9\n66.6*\n49.0\n31.4\n62.8\n48.5\n703.8\n34.1\nTable 2. Results on visual question answering and LMM benchmarks. * indicates that samples from this task’s training set have been\ntrained. SEED and TS respectively represent SEED-Bench [39] and TouchStone [7]. For MM-Vet, we present the average result of five\nscoring runs.\ntrain for 3k steps. The learning rate linearly warms up to\n5  \\times 10^{-5} in the first 100 steps, then decays to zero with a\ncosine schedule. We further fine-tune for 900 steps using\nthe 500k high-quality pairs with a batch size of 2048.\n3. Evaluation\n3.1. Pretrained Base Model\nWe evaluate zero-shot and few-shot abilities of Emu2 on\nOKVQA [53], VQAv2 [28], VizWiz [29], TextVQA [68],\nand HatefulMemes [35] tasks. Details of the datasets and\nprompts can be found in supplementary materials.\nThe\nresults are presented in Table 1. Emu2 demonstrates re-\nmarkable in-context ability, showcasing improved perfor-\nmance with more in-context samples seen.\nSpecifically,\non VQAv2, VizWiz and TextVQA datasets, Emu2 outper-\nforms Flamingo-80B and IDEFICS-80B under all few-shot\nsettings with a much smaller model scale (37B).\nFigure 1 demonstrates Emu2’s few-shot capabilities in\nthe wild. For example, the model learns to classify and\ncount simultaneously in a specific format via a few exam-\nples (row 1). Additionally, Emu2 is capable of following\nvisual prompts in context, e.g., the red circles laid on the\nimages (row 2 and 3).\n3.2. Instruction-Following Chat\nOur Emu2-Chat is evaluated on academic-task-oriented\nbenchmarks including image question-answering datasets\n(VQAv2 [28], OKVQA [53], GQA [32], VizWiz [29],\nTextVQA [68]) and video question-answering datasets\n(MSVD [82] and MSRVTT [82]). The evaluation also en-\ncompassed recent benchmarks for large multimodal models,\nincluding SEED-Bench [39], MM-Vet [88], TouchStone [7]\nand MMMU [89]. When evaluated on SEED-Bench, we\nfollowed the setup of LLaVa-1.5 [48] by presenting options\nto the model for completing multiple-choice tasks.\nAs shown in Table 2, Emu2-Chat consistently outper-\nforms other models in image question-answering tasks,\nencompassing well-established benchmarks like VQAv2\nand GQA. Notably, it shows a noticeable improvement\nin the OKVQA task, which requires the utilization of\nexternal knowledge, showcasing the advantage of our\nmodel for mastering real-world knowledge.\nFor video\nquestion-answering, Emu2-Chat demonstrated advantages\neven though it did not use video question-answering data\nfor training. It achieved an accuracy of 49.0 and 31.4 on\nthe MSVD-QA and MSRVTT-QA tasks, respectively, sur-\npassing InstructBLIP and the larger Flamingo-80B. More\nimportantly, our model has also achieved better results on\nLMM benchmarks.\nLMM benchmarks such as MM-Vet\nprovide a more comprehensive evaluation of model abil-\nities, including solving complicated tasks.\nEmu2-Chat\nachieves a score of 48.5 in MM-Vet and 703.8 in Touch-\nStone, confirming its superior capability in understanding\nand solving multimodal problems.\nIn addition, we demonstrated the visual grounding capa-\nbility of our model using the refer expression comprehen-\nsion benchmarks. In Table 3, Emu2-Chat achieved the best\nresults among generalist models on RefCOCO [34], Ref-\nCOCO+ [52] and RefCOCOg [52]. Its most notable ad-\nvantage was observed in RefCOCO+, which focused solely\non purely appearance-based descriptions without allowing\nthe use of position references. This highlights our model’s\npowerful perceptual abilities in capturing intricate details.\n6\nA photo of <the first dog>, \n<the second dog>,  <the third \ndog> on the grass\nA photo of <the first dog>, \n<the second dog>,  <the third \ndog> on the beach\n a bear and  a sunflower\nAn oil painting of <the first \ndog>, <the second dog>,  <the \nthird dog>\n<a bear>            and  <a sunflower>\n<a bear>                     and <a sunflower>\nas an oil painting \nby Monet\n in a dark forest in 3D\n<a dog>                           in\n<A  clock>\nis on the table\nFigure 4. Visualization of Emu2-Gen’s controllable generation capability. The model is capable of accepting a mix of text, locations and\nimages as input, and generating images in context. The presented examples include text- and subject-grounded generation, stylization,\nmulti-entity composition, subject-driven editing, and text-to-image generation.\nModel\nRefCOCO\nRefCOCO+\nRefCOCOg\nval\ntestA testB\nval\ntestA testB\nval\ntest\nOFA-L [75]\n79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58\nShikra-7B [15]\n87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19\nShikra-13B [15]\n87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16\nQwen-VL-7B [6] 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48\nEmu2-Chat\n90.40 93.88 85.97 87.05 91.43 80.47 87.64 88.11\nCogVLM [77]\n92.51 93.95 88.73 87.52 91.81 81.43 89.46 90.09\nTable 3.\nResults on referring expression comprehension.\nWe\ngrayed out CogVLM because its generalist grounding-enhanced\nmodel was specialist trained on high-quality grounding data.\n3.3. Controllable Visual Generation\nQualitative Results. Figure 3 presents a visualization of\nEmu2’s autoencoding results.\nWith Emu2’s Visual En-\ncoder and Visual Decoder, we can tokenize an image into\nvisual embeddings and detokenize them back. Compared\nwith SEED [27] and Emu [71], Emu2 shows significantly\nsuperior results. We also evaluate our image autoencod-\ning results on MS-COCO [47] and achieve a strong 0.907\nCLIP-I [59] score. More results are in the supplementary.\nAs depicted in Figure 4, Emu2-Gen is capable of ac-\ncepting a mixture of text, locations and images as input,\nand generating images in context.\nThe model skillfully\nengages in various controllable visual generation tasks in\na zero-shot setting, capitalizing on the in-context learn-\ning capabilities in multimodality.\nExamples in Figure 4\nshow generated images of three dogs conditioned on differ-\nent subjects, locations and scenarios. The presented visual\nsamples demonstrate the model’s proficiency in tasks such\nas re-contextualization, stylization, modification, region-\ncontrollable generation, and multi-entity composition.\nZero-shot Text-to-image Generation.\nWe evaluate the\nzero-shot text-to-image generation capability on 30k ran-\ndomly sampled data from the MS-COCO [47] validation\nset. We employ CLIP-ViT-B [60], following the approach\nin DALL-E 3[9], to calculate the CLIP-T score to assess\nprompt-following ability. Additionally, we utilize CLIP-\nViT-L, as in GILL[37], to compute the CLIP-I score for\nmeasuring image similarity. A higher score means the gen-\nerated image is more similar to the prompt or the real image.\nTable 4 shows that Emu2-Gen achieves the state-of-the-art\nperformance in terms of both CLIP-I and CLIP-T scores\ncompared to various unimodal generation models and mul-\ntimodal models. More text-to-image generation cases can\nbe found in supplementary.\nZero-shot\nSubject-driven\nGeneration.\nFollowing\nKosmos-G [56], we also evaluate our model’s subject-\ndriven image editing ability on DreamBench [63].\nWe\ngenerate four images for each prompt, resulting in a total of\n3,000 images for a comprehensive evaluation. We employ\n7\nModels\nCLIP-I ↑\nCLIP-T ↑\nunimodal generation models\nMUSE [13]\n-\n0.320\nImagen [64]\n-\n0.270\nDALL-E 2 † [61]\n-\n0.314\nDALL-E 3 † [9]\n-\n0.320\nSDv1.5 [62]\n0.667\n0.302\nSDXL [58]\n0.674\n0.310\nmultimodal generation models\nGILL [37]\n0.684\n-\nSEED [27]\n0.682\n-\nEmu [71]\n0.656\n0.286\nEmu2-Gen\n0.686\n0.297\nTable 4. Quantitative comparison of zero-shot text-to-image gen-\neration on MS-COCO [47] validation set. 30k samples are ran-\ndomly sampled. †CLIP-T score is calculated on 4,096 samples.\nWe also evaluate our image autoencoding results on MS-COCO\nwhich achieves a strong 0.907 CLIP-I score.\nMethods\nDINO ↑CLIP-I ↑CLIP-T ↑\nReal Images (Oracle)\n0.774\n0.885\n-\nFine-Tuning\nTextual Inversion [24]\n0.569\n0.780\n0.255\nDreamBooth [63]\n0.668\n0.803\n0.305\nBLIP-Diffusion [42]\n0.670\n0.805\n0.302\nTest Time Tuning Free\nRe-Imagen* [16]\n0.600\n0.740\n0.270\nSuTI [17]\n0.741\n0.819\n0.304\nBLIP-Diffusion* [42]\n0.594\n0.779\n0.300\nKosmos-G* (single image input)\n0.694\n0.847\n0.287\nEmu2-Gen * (single image input)\n0.766\n0.850\n0.287\nTable 5.\nQuantitative comparison of zero-shot single-entity\nsubject-driven generation on DreamBench. * denotes zero-shot\nmethods.\nDINO [12] and CLIP-I [59] to evaluate subject fidelity,\nand CLIP-T [59] to evaluate text fidelity, aligning with the\nmethodology established by DreamBooth. Notably, Emu2-\nGen excels in subject fidelity, as evidenced by its superior\nperformance on DINO and CLIP-I metrics compared to\nmethods like BLIP-Diffusion and Kosmos-G. Emu2-Gen\nimpressively reconstructs subjects with just one image\ninput in zero-shot setting, demonstrating superior subject\nfidelity through powerful visual decoding. Further illustra-\ntive cases are provided in the supplementary, showcasing\nEmu2-Gen’s proficiency in multi-entity generation.\n4. Related Work\nLarge Multimodal Models. Recent years have witnessed\nthe rapid growth of large multimodal models [5, 19, 31,\n71]. CLIP [59] pioneered the learning of LMMs with a\ncontrastive learning objective on massive image-text pair\ndata. Flamingo [5] and Kosmos [31, 57] exhibit promis-\ning zero-shot and few-shot multi-modal understanding per-\nformance by training on large-scale image-text interleaved\ndata. With the remarkable progress in open-sourced LLMs,\n[42, 43, 49, 94] show promising results by connecting vi-\nsion encoders and LLMs with a small intermediate model.\nA school of successive efforts [69, 76, 84, 90, 91] further\nimproves visual instruction tuning with better overall train-\ning pipelines [6, 40], grounding annotations [14, 15, 85, 92],\nand extra tasks [6]. There are early studies on training more\nunified large multimodal models [23, 27, 71, 86] that are\ncapable of performing visual understanding and generation\nsimultaneously. In this paper, we further explore the distinct\nsolution proposed in Emu [71]: learning large multimodal\nmodels with generative objectives on both texts and images.\nIn-Context Learning.\nRecent advancements in large\nlanguage models [11, 21] underscore their capacity for in-\ncontext learning [11]. This phenomenon, particularly ev-\nident as LLMs scale up in size and data, has been ex-\nploited for complex challenges such as mathematical rea-\nsoning [81], signaling new emergent ability in model behav-\nior [80]. Flamingo [5] integrates visual inputs to LLMs, en-\nabling the in-context learning of visual-linguistic tasks such\nas image captioning and OCR through language-based in-\nterfacing. Painter [78] and SegGPT [79] conduct an early\nstudy of visual in-context learning. Inspired by the emerg-\ning abilities of large language models, in this work we study\nthe problem of multimodal in-context learning by scaling\nup generative multimodal models and demonstrating strong\nresults in broad understanding and generation tasks.\n5. Conclusion\nWe present a 37 billion-parameter generative multimodal\nmodel Emu2 that shows strong performance and versatil-\nity on many multimodal tasks in the in-context settings.\nEmu2 serves as a base model and a general-purpose in-\nterface for a variety of multimodal tasks. We demonstrate\nstate-of-the-art results on a broad range of benchmarks of\nmultimodal understanding and generation. Specifically, our\nmodel largely surpasses prior work on the lately proposed\nLMM benchmarks that require more advanced capability\ncompared to classic academic benchmarks.\nEmu2 also\nshows remarkable capability of controllable visual gener-\nation in multimodal context, e.g., subject-\/text-grounded\ngeneration.\nAdditionally, we review the limitations and\nbroader social impact of Emu2. Despite discussed weak-\nnesses, these results suggest that generative multimodal\nmodel at scale may be an important step towards the de-\nvelopment of adaptable, general multimodal systems.\n8\nReferences\n[1] Laion-aesthetics. https:\/\/laion.ai\/blog\/laion-\naesthetics\/, . 4, 5, 2, 3\n[2] Laion coco:\n600m synthetic captions from laion2b-en.\nhttps:\/\/laion.ai\/blog\/laion-coco\/, . 4, 2\n[3] Laion-high-resolution.\nhttps:\/\/huggingface.co\/\ndatasets\/laion\/laion-high-resolution, . 5,\n3\n[4] Sharegpt. https:\/\/sharegpt.com\/. 5, 2\n[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716–23736,\n2022. 1, 5, 6, 8, 3\n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 6,\n7, 8\n[7] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan\nZhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jin-\ngren Zhou. Touchstone: Evaluating vision-language mod-\nels by language models. arXiv preprint arXiv:2308.16890,\n2023. 6\n[8] Max Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE\/CVF Inter-\nnational Conference on Computer Vision, pages 1728–1738,\n2021. 3, 1\n[9] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nYunxin Jiao, and Aditya Ramesh. Improving image genera-\ntion with better captions. 2023. 5, 7, 8, 3\n[10] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392–18402, 2023.\n5, 3\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877–1901, 2020. 1, 8\n[12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE\/CVF international conference on com-\nputer vision, pages 9650–9660, 2021. 8\n[13] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 8\n[14] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,\nMaosong Sun, and Yang Liu.\nPosition-enhanced visual\ninstruction tuning for multimodal large language models.\narXiv preprint arXiv:2308.13437, 2023. 8\n[15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm’s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 6, 7, 8\n[16] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 8\n[17] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 8\n[18] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 4, 2\n[19] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian\nGoodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.\nPali-3 vision language models: Smaller, faster, stronger.\narXiv preprint arXiv:2310.09199, 2023. 1, 8\n[20] Luke Chesser and Timothy Carbone. Unsplash. https:\n\/\/github.com\/unsplash\/datasets. 5, 3\n[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 1, 8\n[22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li,\nPascale Fung, and Steven C. H. Hoi.\nInstructblip: To-\nwards general-purpose vision-language models with instruc-\ntion tuning. arXiv preprint arXiv:2305.06500, 2023. 6\n[23] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng\nMa, and Li Yi.\nDreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 8\n[24] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 8\n[25] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu,\nJianfeng Gao, et al. Vision-language pre-training: Basics, re-\ncent advances, and future trends. Foundations and Trends®\nin Computer Graphics and Vision, 14(3–4):163–352, 2022.\n1\n[26] Leo Gao, Stella Biderman, Sid Black, Laurence Golding,\nTravis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy. The Pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027, 2020. 3,\n2\n9\n[27] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 4, 7, 8\n[28] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904–6913, 2017. 4,\n6, 2\n[29] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608–3617,\n2018. 6\n[30] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[31] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023. 6, 8\n[32] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019. 4, 6, 2\n[33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565–26577, 2022. 3\n[34] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787–798, 2014. 5, 6, 2\n[35] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and Davide\nTestuggine. The hateful memes challenge: Detecting hate\nspeech in multimodal memes. Advances in neural informa-\ntion processing systems, 33:2611–2624, 2020. 6\n[36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 5, 3\n[37] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023. 7, 8\n[38] Hugo Laurenc¸on, Lucile Saulnier, L´eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nObelics: An open web-scale filtered dataset of interleaved\nimage-text documents.\nIn Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Bench-\nmarks Track, 2023. 6\n[39] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 6\n[40] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 8\n[41] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-\ndation models: From specialists to general-purpose assis-\ntants. arXiv preprint arXiv:2309.10020, 1(2):2, 2023. 1\n[42] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 6, 8\n[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888–\n12900. PMLR, 2022. 8\n[44] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355, 2023. 5, 2\n[45] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,\nShuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu\nSun, et al.\nM3it:\nA large-scale dataset towards multi-\nmodal multilingual instruction tuning.\narXiv preprint\narXiv:2306.04387, 2023. 4, 2\n[46] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023. 4\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014. 7, 8\n[48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 4, 6, 3\n[49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 5, 8, 2\n[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 3, 4\n[51] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering.\nAdvances\nin Neural Information Processing Systems, 35:2507–2521,\n2022. 4\n[52] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11–20, 2016. 5, 6, 2\n10\n[53] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE\/cvf conference on computer vision and pattern\nrecognition, pages 3195–3204, 2019. 4, 6, 2\n[54] Midjourney. Midjourney. https:\/\/www.midjourney.\ncom. 5, 3\n[55] Leonardo Nicoletti and Dina Bass. Humans are biased: Gen-\nerative ai is even worse. Bloomberg Technology+ Equality.\nAccessed June, 23:2023, 2023. 1\n[56] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-g: Generating images\nin context with multimodal large language models. arXiv\npreprint arXiv:2310.02992, 2023. 7, 4\n[57] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 3, 5, 6, 8, 1, 2\n[58] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models\nfor high-resolution image synthesis, 2023. 1, 3, 4, 8, 2\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 7, 8\n[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 7\n[61] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 8\n[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 4, 8\n[63] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500–\n22510, 2023. 7, 8, 4\n[64] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479–36494, 2022. 8\n[65] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b:\nAn open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 3, 1\n[66] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part II 16, pages 742–758. Springer,\n2020. 4, 2\n[67] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 4\n[68] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 8317–8326, 2019. 4, 6, 2\n[69] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. Pandagpt: One model to instruction-follow them\nall. arXiv preprint arXiv:2305.16355, 2023. 8\n[70] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 3\n[71] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 3, 4, 6, 7,\n8, 1\n[72] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto. Stanford alpaca: An instruction-following llama\nmodel, 2023. 5, 2\n[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023. 1,\n3\n[74] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 4\n[75] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In International Conference on Machine Learn-\ning, pages 23318–23340. PMLR, 2022. 7\n[76] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al.\nVisionllm: Large language model is also an\nopen-ended decoder for vision-centric tasks. arXiv preprint\narXiv:2305.11175, 2023. 8\n[77] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\n11\nSong, et al. Cogvlm: Visual expert for pretrained language\nmodels. arXiv preprint arXiv:2311.03079, 2023. 6, 7\n[78] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang.\nImages speak in images:\nA generalist\npainter for in-context visual learning.\nIn Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6830–6839, 2023. 8\n[79] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang. Seggpt: Segmenting ev-\nerything in context. arXiv preprint arXiv:2304.03284, 2023.\n8\n[80] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret\nZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models.\narXiv preprint arXiv:2206.07682,\n2022. 8\n[81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824–24837, 2022. 8\n[82] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video question answer-\ning via gradually refined attention over appearance and mo-\ntion. In Proceedings of the 25th ACM international confer-\nence on Multimedia, pages 1645–1653, 2017. 6\n[83] Charig Yang, Weidi Xie, and Andrew Zisserman. It’s about\ntime: Analog clock reading in the wild. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2508–2517, 2022. 5, 2\n[84] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 8\n[85] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere\nat any granularity. arXiv preprint arXiv:2310.07704, 2023.\n8\n[86] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023. 8\n[87] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui,\nFan Zhang, Xinlong Wang, and Jingjing Liu.\nCapsfu-\nsion: Rethinking image-text data at scale.\narXiv preprint\narXiv:2310.20550, 2023. 3, 5, 1\n[88] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 6\n[89] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi. arXiv preprint arXiv:2311.16502, 2023. 6\n[90] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu,\nand Tat-Seng Chua. Transfer visual prompt generator across\nllms. arXiv preprint arXiv:2305.01278, 2023. 8\n[91] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al.\nInternlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. arXiv preprint\narXiv:2309.15112, 2023. 8\n[92] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 8\n[93] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced\nvisual instruction tuning for text-rich image understanding.\narXiv preprint arXiv:2306.17107, 2023. 5, 2\n[94] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 8\n[95] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023. 3, 1\n12\nGenerative Multimodal Models are In-Context Learners\nSupplementary Material\nA. Broader Impact and Limitations\nLarge multimodal models offer a wide range of benefits to\nsociety, from enhancing visual navigation and medical diag-\nnostics to increasing accessibility for individuals with visual\nimpairment. The in-context learning capabilities of Emu2\nallow it to quickly adapt to new tasks or environments, even\nwith limited data, ushering in numerous potential applica-\ntions. The generative capabilities of Emu2 can be highly\nvaluable to the creative industries.\nHowever, there are potential downsides in more power-\nful multimodal models to be considered. The hallucination\nissue of multimodal models may cause incorrect and unrea-\nsonable predictions in certain cases. Emu2 may also gen-\nerate harmful or biased content like other generative mod-\nels [55] since the training data may be biased or contain\nunsuitable content. We are actively working to enhance the\nrobustness of multimodal models, reduce model hallucina-\ntions, improve the fairness of training data, and reduce toxic\ndata. We also call on the wider community to pay attention\nto the potential social impact of multimodal models as they\nare growing larger and stronger.\nOne of the limitations of Emu2 is that its in-context\nlearning capability could fail in some complex scenes or\ntasks, e.g., counting in a crowd. Additionally, there is still\na gap between Emu2’s question-answering capability and\nthat of closed multimodal systems. For example, GPT-4V\nachieves 67.7 MM-Vet score vs. Emu2’s 48.5, although al-\nready being state-of-the-art among public models. We be-\nlieve there is much room to improve as the quality and quan-\ntity of training data improve and as model scale continues\nto grow.\nB. More Pretraining Details\nB.1. Dataset Details\nIn pretraining, we exclusively leverage image-text pairs and\nvideo-text pairs for stage 1 training. We additionally lever-\nage interleaved and language-only data altogether for stage\n2. The integration of visual embeddings with text tokens\ngenerates unified multimodal sequences. These sequences\nare then structured by appending the tokens <s> and <\/s>\nto denote the beginning and end of each sequence.\nImage\/Video-text Pairs.\nIn the pretraining stage, we uti-\nlize image-text pairs from LAION-2B [65] and CapsFusion-\n120M [87], along with video-text pairs from WebVid-\n10M [8]. During pretraining stage 2, each image or video\nis randomly placed before or after its corresponding text\nwith a probability of 0.5, respectively. For each video, we\nrandomly sample 8 frames. To structure the visual embed-\ndings, we append two special tokens, [IMG] and [\/IMG],\nto signify the start and end of the visual embeddings. In the\ncase of videos, where there are T frames, each frame is en-\ncoded into a set of visual embeddings, and a special token,\n[VIDEO], is prepended to the start of the frame embedding\nsequence. This design helps distinguish between multiple\nimages and video frames within the multimodal sequences.\nInterleaved Image\/Video-text Data.\nWe harness the\nMultimodal-C4\n(MMC4)\ndataset\n[95]\nand\nthe\nYT-\nStoryboard-1B dataset [71] as expansive sources of image\nand video-text interleaved data. This approach aims to un-\nlock the in-context learning capability of multimodal mod-\nels. For each MMC4 document, we randomly sample N = 8\nimages, accompanied by their corresponding sentences, to\nconstruct a subsequence of L = 1024. During pretraining\nstage 2, each image or frame is randomly positioned before\nor after its corresponding text with a probability of 0.5. The\nspecial tokens used in this interleaved data are consistent\nwith those employed in the image-text pair data.\nGrounded Image-text Pairs.\nWe curated a dataset of\ngrounded image-text pairs named CapsFusion-grounded-\n100M, employing data from CapsFusion [87] processed\nthrough the dataset construction pipeline proposed by\nKosmos-2 [57]. Additionally, we utilized the 20M GRIT\ndataset introduced by Kosmos-2 [57]. To enhance the di-\nversity and context of the dataset, we randomly positioned\neach phrase before or after its corresponding coordinates\nwith a probability of 0.7. The bounding box can be rep-\nresented using its top-left point (x1, y1) and bottom-right\npoint (x2, y2). We transform continuous coordinates into\n224 discrete tokens [57], the coordinates of a sample box\ncan be formulated as <loc000><loc000><loc224><loc224>.\nWe added these tokens to the word vocabulary to facilitate\nunified modeling with text. To distinguish grounding text\nfrom regular text strings, we introduced two special tokens,\n<coor> and <\/coor>, marking the beginning and end of\nthe bounding box coordinates. Moreover, to establish the\ncorrect association between bounding boxes and their cor-\nresponding descriptive phrases, an additional set of special\ntokens, <p> and <\/p>, was appended. To guide the model\nin grounding text output to the provided image, we utilized\nthe special token <grounding>. This comprehensive set\nof tokens and instructions enriches the training data for ef-\nfective multimodal modeling and understanding.\n1\nLanguage-only Data.\nTo maintain text reasoning capa-\nbilities, we engage in joint training with the language mod-\neling dataset Pile [26]. The entire text corpus from Pile is\npreprocessed offline, and each training sample is tokenized\ninto 2048 tokens using the LLaMA tokenizer. We randomly\nsample a total of 3.6 billion tokens for pretraining purposes.\nB.2. Training Hyperparameters\nWe report the detailed training hyperparameter settings of\nEmu2 during the pretraining in Table 6.\nConfiguration\nEmu2 Stage 1\nEmu2 Stage 2\nVisual Encoder init.\nEVA-02-CLIP-E-plus\nEmu2 stage 1\nMultimodel Modeling init.\nLLaMA-33B\nEmu2 stage 1\nLinear projection layer init.\nrandom\nEmu2 stage 1\nInput image resolution\n2242 4482\n4482\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1 = 0.9, β2 = 0.95, eps = 10−6\nPeak learning rate\n1  \\times 10^{-4}, 3  \\times 10^{-5}, 5  \\times 10^{-5}\n1  \\times 10^{-5}\nLearning rate schedule\ncosine decay\nGradient clip\n5.0\nTraining steps\n35.2k 4.0k\n20.35k\nWarmup ratio\n0.02\n0.1\nGlobal batch size*\n6144, 768\n12800, 6400, 3200, 800\nNumerical precision\nbfloat16\nTable 6. Summary of pretraining hyperparameters of Emu2 in\npretraining stages. Peaking leaning rates are 1  \\times 10^{-4} for the\nlinear projection layer, 3  \\times 10^{-5} for Multimodel Modeling, and\n5  \\times 10^{-5} for Visual Encoder. *Global batch size: 1) 6144 for\nimage-text pairs and 768 for video-text pairs in stage 1. 2) 12800\nfor image-text pairs, 6400 for video-text pairs, 3200 for image-\ntext\/video-text interleaved data, and 800 for language-only data in\nstage 2.\nB.3. Visual Decoding\nB.3.1\nDataset Details\nWe utilize images in LAION-COCO [2] and LAION-\nAesthetics [1] to train the Visual Decoder. Images whose\nresolution is smaller than 512 × 512 are filtered to prevent\ngenerating low-quality results. We employ ratio-preserving\nrandom scaling followed by random cropping of a square\nportion from the scaled image to keep all training images\nunstretched. The original image size and crop coordinates\nare used as additional conditions following SDXL [58].\nB.3.2\nTraining Hyperparameters\nThe detailed hyperparameters of visual decoding training\nare summarized in Table 7.\nConfiguration\nVisual Decoding\nVisual Encoder init.\nEmu2 stage 1\nVisual Decoder init.\nSDXL-base\nEncoder input image resolution\n448 × 448\nDecoder output image resolution\n1024 × 1024\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1 = 0.9, β2 = 0.999, eps = 10−8\nPeak learning rate\n1 × 10−4\nLearning rate schedule\nlog warm-up, linear decay\nGradient clip\n1.0\nTotal training steps\n8,000\nWarmup steps\n2,500\nbatch size\n2,048\nNumerical precision\nbfloat16\nClassifier-free guidance\n10%\nNoise offset\n0.1\nTable 7. Summary of training hyperparameters of Emu2 Visual\nDecoder. The Visual Encoder is frozen during training.\nC. Instruction-Following Chat\nC.1. Dataset Details\nWe used two types of training data, academic-task-oriented\ndata and multi-modal chat data, in instruction fine-tuning\nof\nEmu2-Chat\nThe\nacademic-task-oriented\ndatasets\nwe utilized comprise image captioning datasets such\nas COCO Caption [18], and TextCaps [66], as well as\nvisual question-answering datasets like VQAv2 [28],\nOKVQA [53], GQA [32], TextVQA [68], and multi-\nmodal classification data constructed in M3IT [45].\nRefCOCO [34], RefCOCO+ [52] and RefCOCOg [52]\ndatasets are also used. The public multi-modal chat data\nwe use includes GPT-assisted visual instruction data\nLLaVa [49] and LLaVaR [93], language instruction data\nfrom ShareGPT [4] and Alpaca [72], and video instruction\ndata from VideoChat [44]. Beyond these, we constructed\ninstruction fine-tuning data from an analog clock reading\ndataset [83]. For academic-task-oriented datasets, we use\nthe system message “You are a helpful assistant,\ndedicated to provide concise and efficient\nanswers.”, and for the multi-modal chat data, the sys-\ntem\nmessage\nis\n“You are a helpful assistant,\ndedicated to delivering comprehensive and\nmeticulous responses.”.\nC.2. Training Hyperparameters\nThe detailed training hyper-parameters of Emu2-Chat are\nsummarized in Table 8.\nD. Controllable Visual Generation\nD.1. Dataset Details\nWe use the grounded image-text pairs dataset,\ni.e.,\nCapsFusion-grounded-100M\nand\nGRIT\n[57]\nfor\n2\nConfiguration\nEmu2-Chat\ninit.\nEmu2\nInput image resolution\n448 × 448\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1 = 0.9, β2 = 0.98, eps = 10−6\nPeak learning rate\n1 × 10−5\nLearning rate schedule\ncosine decay\nGradient clip\n5.0\nTraining steps\n8,000\nWarmup steps\n100\nGlobal Batch size\n768\nNumerical precision\nbfloat16\nTable 8. Summary of training hyperparameters of Emu2-Chat.\ngrounded text-to-image generation.\nWe use SAM [36]\nto\nobtain\nsegmentation\nresults\nfor\nthe\ncorrespond-\ning grounding boxes.\nWe leverage InstructPix2Pix\nconstructed by [10] for image editing tasks.\nThe\nsample will be formulated as “<s>[IMG]embedding\nof origin image[\/IMG]instruct editing\nprompt[IMG]embedding of edited\nimage[\/IMG]<\/s>”. For the text-to-image task, we use a\nfiltered subset the CapsFusion [87], LAION-Aesthetics [1],\nSA-1B [36], and LAION-High-Resolution [3].\nFor high-quality fine-tuning, our datasets were meticu-\nlously sourced from premium sources, e.g., Unsplash [20],\nand outputs from advanced text-to-image systems, e.g.,\nMidjourney-V5 [54] and DALL-E-3 [9]. This comprehen-\nsive approach ensured a diverse and rich dataset, comprising\napproximately 500,000 instances of high-quality image-text\npairs, instrumental in refining and enhancing the aesthetic\nquality of our Emu2-Gen model’s generated images.\nD.2. Training Hyperparameters\nWe report the detailed training hyperparameter settings of\nEmu2-Gen during the instruction-tuning in Table 9.\nE. Evaluation Details\nPretrained Base Model.\nFor few-shot evaluation of\nEmu2, we adopt the Retrieval In-Context Example Selec-\ntion (RICES) approach for choosing few-shot examples,\nfollowing Flamingo [5] and Emu [71]. The chosen few-shot\nexamples will be separated by “.\n” and then placed ahead\nof the test sample. We use the prompt ”[image] based\non the picture, [question] short answer:”.\nFor zero-shot evaluation, as no example is given, we find the\nabove simple prompt cannot effectively control the model\nbehavior and the model tends to output a sentence rather\nthan a word or phrase.\nThus, we modify the prompt to\n”[image] based on the picture, answer in one\nword or phrase.\n[question] short answer:”.\nThis adjustment aligns the model’s output more closely\nwith the distribution of the tested datasets, where responses\nConfiguration\nEmu2-Gen stage1\nEmu2-Gen QFT\ninit.\nEmu2\nEmu2-Gen stage1\nInput image resolution\n448 × 448\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1 = 0.9, β2 = 0.95, eps = 10−6\nPeak learning rate\n5 × 10−5\n1 × 10−5\nLearning rate schedule\ncosine decay\nGradient clip\n1.0\nTraining steps\n3k\n0.9k\nWarmup ratio\n0.0\nGlobal Batch size*\n4096, 3584, 2048\n2048, 1024, 2048\nNumerical precision\nbfloat16\nTable 9. Summary of training hyperparameters of Emu2-Gen.\n*Dataset types are text-to-image pairs, grounded text-to-image and\nimage editing pairs.\ntypically consist of a succinct word or phrase. The splits\nand metrics for each benchmark are detailed in Table 10.\nInstruction-Following Chat.\nThe evaluation of Emu2-\nChat follows the assessment method of Emu-I [71],\nutilizing generation hyper-parameters with a beam size of\n5. For video input, 16 frames are uniformly sampled as\nvisual conditions.\nIn the question-answering benchmark\nthat requires short answers, we employ the system message\n“You are a helpful assistant, dedicated to\nprovide concise and efficient answers.” along\nwith the output format control information used in [48].\nIn the benchmark for scoring with GPT-4, we use the\nsystem\nmessage\n“You are a helpful assistant,\ndedicated to delivering comprehensive and\nmeticulous responses.”. We provide an overview of\nthe evaluation benchmarks in Table 10.\nBenchmark\nTask\nSplit\nMetric\nVQAv2\nScene understanding VQA\nTest-dev\nVQA score(↑)\nVizWiz\nScene understanding VQA\nTest-dev\nVQA score(↑)\nGQA\nUnderstanding & reasoning VQA Test-dev\nEM(↑)\nOKVQA\nExternal knowledge VQA\nVal\nVQA score(↑)\nTextVQA\nText-oriented VQA\nVal\nVQA score(↑)\nHateful Memes Meme classification\nSeen Test ROC AUC(↑)\nRefCOCO\nRefer expression comprehension\n-\nAccuracy(↑)\nRefCOCO+\nRefer expression comprehension\n-\nAccuracy(↑)\nRefCOCOg\nRefer expression comprehension\n-\nAccuracy(↑)\nMSVD-QA\nEvent understanding VQA\nTest\nEM(↑)\nMSRVTT-QA\nEvent understanding VQA\nTest\nEM(↑)\nMMMU\nMassive multi-discipline QA\nTest\nAccuracy(↑)\nSEED-Bench\nImage\/Video multi-choice QA\n-\nAccuracy(↑)\nMM-Vet\nOpen-ended generation\n-\nGPT-4 score(↑)\nTouchStone\nOpen-ended generation\n-\nGPT-4 score(↑)\nTable 10. Summary of the evaluation benchmarks.\nControllable Visual Generation.\nFor all evaluation of\nvisual generation tasks, we use EulerDiscreteScheduler [33]\n3\nwith 50 diffusion steps. The classifier-free guidance scale is\nset to 3.0. To evaluate on DreamBench [63], we select ex-\nactly the same image for each object as chosen in Kosmos-\nG [56]. Similarly to Kosmos-G, we also slightly modified\nthe original prompt with the prefix ”a” , for example, ”a\nred {}” is modified to ”{} Make it red”\nF. Qualitative Results\nWe present qualitative cases for Emu2-Gen in Figure 5-11\nand for Emu2-Chat in Figure 12-14, respectively.\nA photo of <the \nfirst dog>, <the \nsecond dog>,  \n<the third dog> \non the grass\nA oil painting of \n<the first dog>, \n<the second \ndog>, <the third \ndog>\nA photo of <the first \ndog>, <the second \ndog>, <the third \ndog> swimming \nunder the water\nFigure 5. Illustration of controllable visual generation of subject-\ndriven generation across multiple images with layout guidance.\n4\nreal image\nautoencode result\nreal image\nautoencode result\nFigure 6. Qualitative cases of image autoencoding.\n5\nIn this scene, a cyclist pedals through a sun-\ndappled forest trail, with rays of light filtering \nthrough the dense foliage. The vibrant colors of \nthe rider's gear harmonize with the natural hues \nof the surrounding flora.\nA woman with natural beauty, her hair gently \nflowing, and a soft, genuine smile, standing in a \nreal-world cherry blossom park.\ncyber punk city street,all building made from \ntranslucent hologram glass material,pastel neon \nlight,night scene with pastel purple and pink sky\nThree minuscule explorers dwarfed by Ethereal \nmonumental gigantic cliffs, minimalistic \nlandscape, beige and gray snowscene , in the \nstyle of Andy Fairhurst.\nHand-painted castle, surrounded by trees, \nautumn, leaves flying\nA panoramic view of the Grand Canyon at \nsunset, with the vast, layered rock formations \ndramatically lit by the fading golden sun, \ncasting deep shadows and highlighting the rich, \nred hues of the canyon walls.\nA dog that has been meditating all the time.\nMajestic waterfall in a lush, green tropical forest.\nA Blue bird\nFigure 7. Illustration of text-to-image generation.\n6\ncute baby panda\n<a panda>                                        on the beach      \nMake it wearing \na very cool \nsunglassess\n<a panda>                                       and <a dog>                                     on the beach      \nAn oil painting of two animals                      , in the style of Van Gogh\nfloats above the \nforest\nis capturing the \nbustling cityscape \nfrom a high rooftop, \nas the sun sets \nbehind skyscrapers.\n<A cartoon character> \n walking in the street. It looks like \nhyper-realistic cityscape at night, with \nrain-soaked streets reflecting neon \nlights.\nReplace the text \nbackground color \nwith yellow\nwearing a big hat \non the beach in \nthe evening\nFigure 8. Illustration of zero-shot controllable visual generation with interleaved vision-language prompt.\n7\n<A man>                                      with <a dog>                                     , and <a car>                                           in GTA-V\n<a bear>                                wearing <a crown>                                           and <a bow tie>                       \nin the \nsnow kingdom           \nwearing <a hat> \n<a cat>\nand eating <a cake>\nA photo of \na birthday \nparty in the \ngarden, \nAn image draw of <a dog>\nand\n<an emu>\nin the style of \nin the cover of \nA photo of <a man>\nholding the\non the stage\n<words on neon lamp>\nAn anime draw of <a cat>\nand <a dog>\nin the style of \n<A singer>               in <a black cloth>               is singing into <microphone>    \n<a pekingese> \n is on the beach with an <umbrella>\nnear the\n<sea>\n<a dog>\n<a book>\non the table\nFigure 9. Illustration of zero-shot controllable visual generation with interleaved vision-language prompt.\n8\nwearing a big sunglasses \non the beach\nfloats above the \nforest\nA person is riding a big  \nhorse on the water in \nfront of mountain\nis capturing the \nbustling cityscape \nfrom a high rooftop, \nas the sun sets behind \nskyscrapers.\n<a dog>                                    \nin a birthday \nparty in the \ngarden\nA blue bird\n<a bear>                      and <a tree>\nin the Christmas party\nFigure 10. Illustration of zero-shot video generation with interleaved vision-language prompt.\n9\nS* in the jungle\nS* with a city in the \nbackground\nS* on the beach\nS* in a chef outfit\nSubject (S*)\nS* on top of the \nsidewalk in a crowded street\nS* in the snow\nS* Make it purple\nS* on a \ncobblestone street\nSubject (S*)\nFigure 11. Zero-shot subject-driven generation results on DreamBench.\n10\nEmu2-Chat\nInput Image\nInput Text\nLook at this sequence of three shapes. What shape should come \nas the fourth shape? Explain your reasoning with detailed \ndescriptions of the first shapes.\nModel Output\nThe first shape is a triangle, the second shape is a square, and the third shape is a \npentagon. To find the next shape in the sequence, we need to identify a pattern. In this \ncase, the pattern is that each shape has one more side than the previous shape. Therefore, \nthe next shape in the sequence should have six sides, which is a hexagon.\nGemini Ultra\nThe fourth shape should be a hexagon. The first shape is a triangle, the second shape is a \nsquare, and the third shape is a pentagon. The number of sides in each shape is increasing \nby one \nCogVLM-Chat\nThe sequence of shapes could be interpreted in various ways, but one possible \ncontinuation is a square, as it complements the triangle and pentagon shapes by forming \na 2D shape that can be thought of as a 'flat' version of the other three. The triangle \nrepresents a 3D shape, the pentagon a 5D shape, and the square a 2D shape.\nFigure 12. Qualitative case of Emu2-Chat.\n11\nEmu2-Chat\nInput Image\nInput Text\nCompare the average kinetic energies of the particles in each sample. The diagrams \nbelow show two pure samples of gas in identical closed, rigid containers. Each colored \nball represents one gas particle. Both samples have the same number of particles. \nWhich sample has the higher temperature? Sample A or sample B?\nModel Output\nThe sample with the higher temperature is Sample A, where the mass of each particle is \n32 u and the average particle speed is 600 m\/s. In Sample B, the mass of each particle is \n32 u, but the average particle speed is 540 m\/s. Since the temperature is a measure of the \naverage kinetic energy of the particles, and kinetic energy is proportional to the square of \nthe velocity, the higher the velocity, the higher the temperature. In Sample A, the higher \naverage particle speed results in a higher temperature compared to Sample B.\nGPT-4V\nBecause the average particle speed in sample A is higher than in sample B (600 m\/s vs. \n540 m\/s), the particles in sample A have a higher average kinetic energy. Therefore, \nsample A has a higher temperature than sample B. \nCogVLM-Chat\nThe kinetic energy of a particle is given by the formula (1\/2)mv^2, where m is the mass \nand v is the velocity of the particle. Since the velocities of the particles in both samples \nare the same, the sample with the higher mass particles will have the higher kinetic \nenergy. Therefore, Sample A, with particles having a mass of 32 u, will have a higher \naverage kinetic energy than Sample B, which has particles with a mass of 32 u.\nFigure 13. Qualitative case of Emu2-Chat.\n12\nEmu2-Chat\nInput Image\nInput Text\nImagine you are a guiding robot. \nHere is a photo I took. Please tell me how to get to the restroom.\nModel Output\nTo get to the restrooms, you should follow the \"Restrooms\" sign in the picture. The sign \nis pointing to the left, so you should head in that direction to find the restrooms. If you \nneed further assistance, please let me know, and I'll do my best to help you navigate to \nthe restrooms.\nCogVLM-Chat\nTo get to the restroom, follow the direction indicated by the sign. It's located to the right \nof the sign and is labeled 'Restrooms'.\nQwen-VL-Chat\nTo reach the restrooms, follow the directional arrow on the sign pointing downwards and \ntowards your right side. The restroom should be located in that direction based on the \ninformation provided by the sign. Keep an eye out for any additional signs or markers \nindicating where the restrooms might be within this area of the building.\nFigure 14. Qualitative case of Emu2-Chat.\n13\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Generative Multimodal Models are In-Context Learners.pdf"}
{"title":"xGen-MM (BLIP-3): A Family of Open Large Multimodal Models","authors":"Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu","summary":"This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.","url":"http:\/\/arxiv.org\/abs\/2408.08872v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.08872v2","published":1723831021000,"comment":null,"pdf_text":"xGen-MM (BLIP-3): A Family of Open Large Multimodal\nModels\nLe Xue1◦\nManli Shu1◦\nAnas Awadalla1,2∗\nJun Wang1∗\nAn Yan1∗\nSenthil Purushwalkam1∗\nHonglu Zhou1∗\nViraj Prabhu1∗\nYutong Dai1∗\nMichael S Ryoo1∗\nShrikant Kendre1∗\nJieyu Zhang1,2∗\nCan Qin1\nShu Zhang1\nChia-Chih Chen1\nNing Yu1\nJuntao Tan1\nTulika Manoj Awalgaonkar1\nShelby Heinecke1†\nHuan Wang1†\nYejin Choi2†\nLudwig Schmidt2†\nZeyuan Chen1†\nSilvio Savarese1†\nJuan Carlos Niebles1†\nCaiming Xiong1†\nRan Xu1†\n1Salesforce AI Research\n2University of Washington\n{lxue, ssavarese, jniebles, cxiong, ran.xu}@salesforce.com\n◦First authors; ∗Core authors; †Senior authors\nProject Page\nAbstract. This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large\nMultimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe,\nmodel architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the\nSalesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range\nof tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong\nin-context learning capabilities and the instruction-tuned model demonstrates competitive performance\namong open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with\nDPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our\nmodels, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM\nresearch. Associated resources will be available on our project page above.\nSimple and small-scale\nCOCO-style image caption data\nComplex \nQ-Former \nMultiple training objectives\n(ITM, ITC, and ITG losses)\nA cat wearing \nsunglasses\nBLIP-2 LMM\nxGen-MM\nLarge-scale comprehensive free-form \nmultimodal interleaved data\nUnified training objective\n(next-token prediction loss)\nScalable Vision \nToken Sampler\nAround a \nkilometre or \ntwo up the \nroad are the \ncircus and the \namphitheatre \nin the…\nLeptis Magna, \nin modern day \nLibya, once \nAfrica's \npremier \nRoman city. It \n…\n(a) BLIP-2 framework\n(b) xGen-MM (BLIP-3) framework\nFigure 1: We introduce xGen-MM (BLIP-3), a framework (b) for developing Large Multimodal Models (LMMs).\nOur framework improves upon BLIP-2 (a) [1] by (1) increasing the richness, scale, and diversity of training data, (2)\nreplacing the Q-Former layers with a more scalable vision token sampler, and (3) simplifying the training process via the\nunification of the training objectives to a single loss at every training stage. The resulting suite of LMMs can perform\nvarious visual language tasks and achieve competitive performance across benchmarks.\n1\nIntroduction\nLarge Multimodal Models (LMMs) have attracted significant attention with their potential applications and\nemergent capabilities. Recent advancements in both proprietary models [2–5] and open-source LMMs [6, 1, 7–\n11] highlight the rapid progress and growing interest in this field. However, despite these advancements,\n1\narXiv:2408.08872v2  [cs.CV]  28 Aug 2024\nthere is still a gap between open-source models and proprietary ones in terms of access to open weights,\ntraining recipes, and curated datasets. Such limitations hinder the open-source communities from replicating,\nunderstanding, and improving LMMs.\nRecent works have demonstrated that large-scale and high-quality data are essential for training robust\nLMMs [8–12]. BLIP-2 [1] was one of the pioneering efforts in exploring LMMs, which leveraged synthetic\ndata to achieve impressive results at the time (Figure 1 (a)). However, the data used in BLIP-2 lacks the\nscale, quality, and diversity required to reach competitive performance compared to more modern LMMs\nnowadays. In addition, BLIP-2 employs an intricate Q-Former [1] architecture to bridge the vision and\nlanguage modalities, coupled with a suite of complex training objectives (ITM, ITC, and ITG losses), both of\nwhich pose obstacles for larger-scale training. Moreover, BLIP-2 supports only single-image input, whereas\ninterleaved multimodal data formats are the most natural form of multimodal data [13].\nIn response to these challenges, we introduce xGen-MM (BLIP-3) (Figure 1 (b)), a new framework\ndesigned to scale up LMM training by utilizing an ensemble of multimodal interleaved datasets, curated\ncaption datasets, and other publicly available datasets [14–17]. xGen-MM, short for xGen-MultiModal, fur-\nther expands our previous generative AI initiatives and corresponding foundation models for text xGen [18],\ncode generation codeGen [19, 20], function calling APIGen [21], among others. In xGen-MM (BLIP-3),\nas illustrated in Figure 2, we streamline the model architecture by replacing the Q-Former [1] with a more\nscalable vision token sampler (specifically, a perceiver resampler [22]) and simplifying the training objectives\nto focus solely on the auto-regressive loss of text tokens in a multimodal context. Our primary focus is\non dataset curation and scaling up the training data. Recently, our BLIP-3 team introduced two large-\nscale, high-quality datasets: MINT-1T [12], a trillion-token scale interleaved dataset; and BLIP3-KALE,\na knowledge-augmented high-quality dense captions dataset. In this technical report, we introduce two\nadditional specialized datasets: BLIP3-OCR-200M, a large-scale dataset with dense OCR annotations; and\nBLIP3-GROUNDING-50M, a large-scale visual grounding dataset.\nIn addition to these datasets, we are committed to open-sourcing the series of models developed in\nthis work, including the base, instruction-tuned, and DPO models. Along with the model release, we also\nprovide code for easy fine-tuning of our base model on custom datasets. By making these resources publicly\navailable, we aim to make LMM research and development more accessible to the community, and we\nencourage researchers and practitioners to use our models and datasets to understand and further explore the\npotential and emergent abilities of LMMs.\n2\nRelated Work\nRecent advancements in Large Multimodal Models (LMMs) have explored two main architectural approaches:\nthe cross-attention style [22, 23] and the decoder-only style. The cross-attention approach, exemplified\nby models like Flamingo [22, 23] and Llama 3.1 [5], integrates vision and language modalities through a\ncomplex attention mechanism to enable deep multimodal understanding. Another approach is the decoder-\nonly architecture [1, 7, 8, 24–36], which we adopt in xGen-MM (BLIP-3), offers a more streamlined\nsolution. This approach connects pre-trained language models to visual inputs using lightweight connectors,\nsimplifying the integration process while maintaining robust multimodal capabilities. The effectiveness\nof this architecture is evident in models such as MM1 [9], VILA [10], LLaVA [8], phi3-vision [37], and\nOtter [38].\nTraining methodologies for LMMs typically follow one of the two strategies. The first one uses a\nlight pre-training procedure and heavily relies on visual instruction tuning, as seen in the LLaVA se-\nries [8, 29]. Extensive research has been conducted on creating effective instruction-tuning data for a variety\nof tasks [32, 39–43]. The second strategy involves extensive pre-training on large-scale, diverse datasets,\nfollowed by visual instruction fine-tuning. This approach, employed by models like MM1 and Idefics2 [11],\ninfuses broad knowledge into the model, which is then fine-tuned to align with human-like interactions and\nsafety standards. While MM1 [9] provides extensive ablations and studies on the recipes aimed at improving\nLMMs, it releases limited resources for practitioners to adopt the model (e.g., MM1 models and datasets\n2\nPre-trained LLM\nAround a kilometre or two up the \nroad are the circus and the \namphitheatre in the second part of \nthe Leptis Magna complex. The \namphitheatre was built to seat up to \n16,000 spectators who would come \nto be entertained…\nLeptis Magna, in modern day \nLibya, once Africa's premier \nRoman city. It is one of the \ngreatest archeological sites in \nthe whole Mediterranean. If \nLeptis Magna were in Tunisia \nor Morocco or Egypt…\nVision Tokens\nText Tokens\nVision Tokens\nText Tokens\nVision Tokens\nText Tokens\nVision Tokens\nText Tokens\nVision \nTransformer\nToken \nSampler  \nVision \nTransformer\nToken \nSampler  \n…\n…\n…\nText Tokenizer\nText Tokenizer\nLoss\nLoss\n…\nFigure 2: Overview of the xGen-MM (BLIP-3) framework. Free-form interleaved images and texts from the\nensembled interleaved and caption datasets are input into the framework, with each modality undergoing a separate\ntokenization process to be fed into the pre-trained LLM in natural order. A standard auto-regressive loss is then applied\nto the text tokens. The Vision Transformer is kept frozen during training, while all other parameters, including the token\nsampler and the pre-trained LLM, are trained.\nare close-sourced). Idefics2 [11] is a more recent open-source work that open-sources a suite of models\nalong with detailed training strategies and data recipes, but Idefics2 mostly resues existing datasets in their\nexperiments without contributing new ones.\nIn this work, we present xGen-MM (BLIP-3). Unlike previous works, xGen-MM (BLIP-3) is an\nopen-source family of a series of models, data recipes, fine-tuning code, and two large-scale foundational\nmultimodal datasets, which we hope will enable and advance future research in this area.\n3\nModel Architecture\nArchitecture Overview.\nAs illustrated in Figure 2, the xGen-MM (BLIP-3) framework adopts an architec-\nture consisting of a ViT [44, 45], a vision token sampler (perceiver resampler [22]) to downsample the image\nembeddings, and a pre-trained Large Language Model (phi3-mini [37]). The input to the model can be free-\nform multimodal interleaved texts and vision tokens from the diverse multimodal data sources we ensemble.\nAny-Resolution Vision Token Sampling.\nAs proved effective in recent LMMs [46–48], we adopt a\ndynamic high-resolution (i.e., \"any-resolution\") image encoding strategy at the fine-tuning and post-training\nstages. We enable higher-resolution image understanding with image patch-wise encoding. The patch-wise\n3\nencoding preserves the resolution of the original image as much as possible by splitting a single image into\nmultiple patches and encoding them separately. Following the previous convention, we concatenate the\nencoded image patches with a downsized original image that provides global information.\nIn the VL connector, we use a perceiver resampler to downsample the vision tokens. With any-resolution\nimage encoding, we perform the downsampling for each image patch (including the downsized original\nimage) independently. The downsampler vision tokens are then concatenated together and sent to the LLM.\nWith the downsampling in our VL connector, we can reduce the sequence length of vision tokens by a factor\nof five or more depending on the number of query tokens in the perceiver resampler. We provide ablation\nstudies on different token sampling strategies in Section 7.2.\n4\nTraining\nPre-training.\nThe pre-training objective is to predict the next text token across the dataset mixture we\npre-train on. Overall, the resulting base model xGen-MM-Phi3-mini-base-r is pre-trained for about 100\nbillion multimodal tokens from the ensembled dataset, and our pre-training resolution is 384x384 pixels,\nwhich aligns with SigLIP [45].\nSupervised Fine-tuning (SFT).\nWe further fine-tune our pre-trained models on instruction-following\nexamples to make them better understand and follow user queries. At the fine-tuning stage, we use a\ncollection of publically available instruction-following datasets [11, 29, 49]. We adopt the any-resolution\nvision token sampling strategy to allow a better understanding of images of higher resolutions such as\ntext-rich document data. We introduce the technical details for the fine-tuning stage in the following sections.\nInterleaved Multi-Image Supervised Fine-tuning.\nWe conduct a second-stage fine-tuning on the instruc-\ntion fine-tuned model on a mixture of multi-image and single-image instructions-following samples. The\ngoal for this second-stage fine-tuning is to enhance the model’s ability to comprehend interleaved image-text\ninput, which is helpful for multimodal in-context learning, multi-image question answering, and many more\npractical use cases. For the multi-image fine-tuning, we also adopt the any-resolution vision token sampling\nstrategy same as in the previous SFT stage.\nPost-training.\nFinally, we perform two stages of post-training to improve the model’s helpfulness while\nmitigating harmful qualities such as hallucination and toxicity. We first perform direct preference optimization\n(DPO [50]) to improve the model’s helpfulness and visual faithfulness. We then perform safety fine-tuning\nto improve the model’s harmlessness. We quantitatively demonstrate Pareto gains in model harmlessness\nand helpfulness after our post-training.\n5\nData\n5.1\nPre-training Data Recipe\nAs indicated in Figure 3, in xGen-MM (BLIP-3), we pre-train on an ensemble of diverse multimodal datasets\nwith the indicated sampling ratios.\nInterleaved Dataset Mixture.\nWe combine MINT-1T (including its HTML, PDF, and ArXiv subsets) with\nOBELICS (HTML only) to create a more diverse and comprehensive dataset mixture that covers a broader\nrange of domains.\n1. MINT-1T [12] is a trillion token scale multimodal interleaved dataset, containing data sources from\nHTML, PDF, and ArXiv. As evidenced by MM1 [9] and Idefics2 [11], such multimodal interleaved\n4\nOBELICS\n17.5%\nMINT-1T\n32.5%\nBLIP3-KALE\n25%\nBLIP3-GROUNDING\n5%\nBLIP3-OCR\n5%\nDatacomp-1B\n10%\nCC12m\n1.25%\nCC3m\n1.25%\nSBU\n1.25%\nVG\n1.25%\nOBELICS\nMINT-1T\nBLIP3-KALE\nBLIP3-GROUNDING\nBLIP3-OCR\nDatacomp-1B\nCC12m\nCC3m\nSBU\nVG\n“NY”, “BURGER”, “PORK&GRILL”, \n“www.bigstock.com 206857405”\nThe image contains garment (at \nthe bottom right corner), t \nshirt polo shirt (in the \ncenter), neckband collar (at \nthe top left corner), neckband \n(to the right of the center), \nneckband collar (above the \ncenter)\n\"A shabby chic country kitchen design is showcased in this \nimage, featuring a decorative canister set with a vibrant \nrooster illustration. The container, which may be made of \nceramic or metal, boasts a rustic, weathered appearance \nwith a metal handle and a lid adorned with a curved metal \nloop. The rooster is depicted in rich colors, including \nred, blue, and yellow, against a background of faded, \nhandwritten-style text and designs. The container rests on \na wooden surface, and the image bears a watermark from \nFarmhouse Temptations at its bottom right corner.\"\nPlease note the balance of euro states trade: \nItaly in surplus, Germany deficit. Another world \nindeed. Consider that Italian public debt was not \ndifferent than today, but it was not a matter of \nconcern, the shock did reflect on the exchange \nrate, no one was thinking about selling government \nbonds under par…Please check,\nWe aim at calculating the path integral over the \nvariations of the string around the bounce \nconfiguration, which involves in particular…\nIn terms of the introduced variables the action ( \na0 ) can be written in the quadratic \napproximation in the deviations from the bounce …\nthe following graph: red is gdp% shift in \nprivate debt and blue public debt from 1999 to \n2007, so much for another of the myths of this \ncrisis, the one that says that “the fault is of \nthe public debt\"…\nThe condition ( appl_cond ) for applicability of \nthe effective action ( a0 ) requires that ….\nFigure 3: Overview of xGen-MM (BLIP-3) Pre-training Datasets.\ndatasets are essential for scaling up large multimodal model training and enabling fundamental\ncapabilities like multimodal in-context learning. Notably, different from the OBELICS [11], MINT-1T\nhas three subsets from different sources: the HTML subset, the PDF subset, and the ArXiv subset. In\nxGen-MM (BLIP-3), these three subsets are mixed in a 7:5:1 ratio.\n2. OBELICS [11] is another large-scale multimodal interleaved dataset constructed from HTML docu-\nments solely. It differs slightly in domain coverage from MINT-1T due to the specific preprocessing\nsteps adopted.\nCaption Dataset Mixture.\nWe integrate a diverse range of caption datasets, with the specifics outlined in\nthe following details.\n1. BLIP3-KALE is a large-scale curated high-quality caption dataset. Details will be discussed in\nanother paper, and the dataset will be made public very soon.\n2. BLIP3-OCR-200M is a curated large-scale OCR dataset to address the limitations of current large\nmultimodal models in handling text-rich images like documents and charts, as traditional image-text\ndatasets often lack adequate OCR annotations. To enhance text comprehension abilities, we use the\nOCR engine PaddleOCR [51] to annotate images with OCR-specific annotations. Overall, we curate\na dataset of 200 million high-resolution images from Datacomp-1B[17]. For each image, we create\ncaptions with OCR data by identifying and extracting textual elements using the off-the-shelf OCR\nengine [51]. Text segments in a caption like \"... text ...\" are modified to include OCR information as\n\"... text ( ocr_info ) ...\", where ocr_info contains bounding box coordinates for the extracted\n5\ntext, specifying its exact position within the image in the format \"<bbox>x1, y1, x2, y2<\/bbox>\".\nWe have multiple granularities of OCR information, including with and without bounding box data.\nIn our work, we only utilize textual information without bounding box data, which has proven to be\neffective. Note that, in xGen-MM (BLIP-3), we preprocess the captions to remove filler texts such\nas “the text”, which we find improves OCR-related benchmark performance. We hypothesize that\nthis is because such filler text is relatively easy to predict, potentially diluting the loss associated with\nOCR-relevant tokens. Nonetheless, incorporating bounding box information could further enhance\nperformance, and we encourage researchers in the community to explore this potential.\n3. BLIP3-GROUNDING-50M is a curated large-scale grounding dataset to enhance the ability to\nground semantic concepts in visual features, which is crucial for tasks like object detection, semantic\nsegmentation, and understanding referring expressions [52] (e.g., \"the object to the left of the dog\").\nWe curate a dataset of 50 million images from Datacomp-1B [17]. For each image, we identify objects\nand their location information using one of the state-of-the-art open-world image tagging [53] and\nobject detection models [54]. Objects mentioned in a caption like \"... object ...\" are modified to\ninclude grounding information as \" ... object ( grounding_info ) ...\", where grounding_info\ncontains bounding box information in one of three formats, each capturing a different granularity of\nlocalization: (1) <bbox>x1, y1, x2, y2<\/bbox>, (2) \"starts at (x1, y1) and extends up to (x2, y2)\",\nor (3) \"top-left corner of the image\".\n4. Other Public Datasets Mixture: We also include other publicly available datasets such as uncurated\nDatacomp-1B [17] image-text pairs, CC12M [14], CC3M [14], VG [15], and SBU [16].\n5.2\nSupervised Fine-tuning Data Recipe\nThe datasets used in the fine-tuning stage are from public datasets of different domains [29, 11, 49]. We\nsample data with various domain focuses including multi-modal conversation [29], image captioning [55, 56],\nvisual question answering [57–60], chart\/document understanding [61–64], science and math [65, 66]. In\naddition to the multi-modal image-text data, we also mix in pure text instruction following data [67, 68]\nduring visual instruction tuning. Ultimately, we collect a mixture of 1 million publically available instruction-\ntuning samples, on which we fine-tune our model for one epoch.\nThe multi-image instruction tuning stage starts with a model fine-tuned on single-image samples. We use\na mixture of public multi-image\/interleaved image-text instruction data [69, 70]. To prevent the model from\ndeteriorating on single-image capabilities, we reuse a subset of single-image datasets used in the previous\nfine-tuning stage and mix them into the multi-image training data.\n5.3\nPost-training Data Recipe\nImproving Truthfulness by Direct Preference Optimization.\nWe employ VLFeedback [71], a syntheti-\ncally annotated multimodal preference dataset that uses off-the-shelf VLMs to generate responses to a diverse\nmix of multimodal instructions that are then scored by GPT4-V [2] along three axes – helpfulness, visual\nfaithfulness, and ethics. The dataset contains 80k such instructions from which we construct preference data\nby marking as preferred (and dispreferred) the response with the highest (and lowest) average score across\nmodels and filtering out examples with low-scoring preferred responses. We thus generate 62.6k preference\nexamples.\nWe perform 1 epoch of DPO on the combined preference dataset while updating a subset (2.5%) of LLM\nbackbone weights using low-rank adaptation (LoRA [72]). Also, following recent work [50], we generate an\nadditional set of responses that capture the model’s intrinsic hallucinations, by performing a second step\nof DPO per-iteration against the models’ output to a noised version of the input image and original query,\nwhich we treat as an additional dispreferred response.\n6\nOur BLIP3-OCR-200M caption:\nLevel 1: The image contains the text \"DO NOT\", the text \"teuscher \nchocolates of Switzerland\"\nOur BLIP3-OCR-200M caption:\nLevel 1: The image contains the text \"THE\", the text \"PATRiCK\", the \ntext \"STAR\", the text \"SHOW”\nLevel 2: The image contains the text \"THE\" located above the center, \nthe text \"PATRiCK\" located above the center, the text \"STAR\" located \nabove the center, the text \"SHOW\" located above the center\nLevel 3: The image contains the text \"THE\" located at 1\/14 of the \nimage from top to bottom and 1\/2 of the image from left to right, the \ntext \"PATRiCK\" located at 1\/7 of the image from top to bottom and \n10\/19 of the image from left to right, the text \"STAR\" located at 1\/4 of \nthe image from top to bottom and 10\/19 of the image from left to right, \nthe text \"SHOW\" located at 6\/19 of the image from top to bottom and \n1\/2 of the image from left to right\nLevel 4: The image contains the text \"THE\" approximately starts at \n[0.4, 0.0] and extends up to [0.6, 0.1], the text \"PATRiCK\" \napproximately starts at [0.2, 0.1] and extends up to [0.8, 0.2], the text \n\"STAR\" approximately starts at [0.3, 0.2] and extends up to [0.7, 0.3], \nthe text \"SHOW\" approximately starts at [0.4, 0.3] and extends up to \n[0.6, 0.4]\nLevel 5: The image contains the text \"THE\" starts at [0.42, 0.045] \nand extends up to [0.585, 0.101], the text \"PATRiCK\" starts at [0.237, \n0.089] and extends up to [0.799, 0.202], the text \"STAR\" starts at \n[0.308, 0.182] and extends up to [0.728, 0.31], the text \"SHOW\" starts \nat [0.371, 0.289] and extends up to [0.647, 0.357]\nLevel 6: \"The image contains the <ocr>\"THE<\/ocr><bbox>[0.42, \n0.045][0.585, 0.101]<\/bbox>, the \n<ocr>\"PATRiCK<\/ocr><bbox>[0.237, 0.089][0.799, 0.202]<\/bbox>, \nthe <ocr>\"STAR<\/ocr><bbox>[0.308, 0.182][0.728, 0.31]<\/bbox>, the \n<ocr>\"SHOW<\/ocr><bbox>[0.371, 0.289][0.647, 0.357]<\/bbox>\"\nOur BLIP3-OCR-200M caption:\nLevel 1: The image contains the text \"Rakuten\", the text \"THE \nTHIRD SHIRT\", the text \"AVAILABLE NOW!”\nOur BLIP3-OCR-200M caption:\nLevel 1: The image contains the text \"IF \nYOU WANT\", the text \"TO BUILD A\", the \ntext \"HOUSE\"\nOur BLIP3-OCR-200M caption:\nLevel 1: The image contains the text \n\"GOD'S\", the text \"WAY OUT\", the text \n\"OF-THE\", the text \"DARKNESS\", the text \n\"DARROLDG PARKER”\nOur BLIP3-OCR-200M  caption:\nLevel 1: The image contains the text \"Rucka Rucka Ali\", the text \n\"Ura\", the text \"Cartoonist”\nOur BLIP3-OCR-200M caption:\nLevel 1: The image contains the text \n\"Emergency\", the text \"Mobile\", the text \n\"Charger\"\nFigure 4: Samples from BLIP3-OCR-200M. Six levels of OCR information granularity are extracted, with and\nwithout bounding box data. Note that in xGen-MM (BLIP-3), OCR-related captions are preprocessed to remove filler\nphrases like ’the text,’ resulting in improved OCR benchmark performance.\nImproving Harmlessness by Safety Fine-tuning.\nNext, we perform 3 epochs of safety fine-tuning on\nthe train split of the VLGuard [73] dataset, which contains 2k examples of unsafe images and instructions.\nVLGuard comprises two types of unsafe examples: (1) objectionable images paired with safe instructions and\na desirable abstention response, and (2) safe images paired with two types of instruction-response pairs, one\nsafe and another unsafe. The dataset consists of unsafe examples belonging to various subcategories including\nprivacy-violating, risky\/sensitive topics (such as politics, sex, and violence), deception, and discrimination.\nFollowing the original work, we randomly sample 5k additional examples from the instruction fine-tuning\ndataset to retain the model’s helpfulness without exaggerating its safety behavior. As before, we update a\nsubset (2.5%) of LLM backbone weights using low-rank adaptation.\n7\nThis image showcases a football player (in the center) in action, wearing a red jersey (in the center) \nwith the logo number (in the center) '11' and the logo of the 'Falcons'. The player is holding an NFL \nfootball (at the bottom left corner) in one hand (at the bottom left corner) and is gesturing with the \nother. The helmet (above the center) has a face mask obscuring the player's face, and the \nbackground is blurred, emphasizing the player in the foreground.\nThis image showcases a <object>football player<\/object><bbox>[0.015, 0.005][0.845, 0.993]<\/\nbbox> in action, wearing a <object>red jersey<\/object><bbox>[0.205, 0.207][0.638, 0.994]<\/\nbbox> with the logo …… The <object>helmet<\/object><bbox>[0.287, 0.002][0.523, 0.449]<\/bbox> \nhas a face mask obscuring the player's face, and the background is blurred, emphasizing the player \nin the foreground.\nCoarse Grounding Caption\nFine-grained Grounding Caption\nFigure 5: Samples from BLIP3-GROUNDING-50M. We introduce a large-scale dataset of images and corresponding\ncaptions containing localization information about objects. Furthermore, we release the associated object bounding box\ndata to facilitate the creation of captions with custom templates.\n6\nExperiments\n6.1\nPre-training\nFew-shot Evaluation.\nAfter the pre-training stage, we evaluate our pre-trained model on classic captioning\nand VQA tasks, in comparison with previous models that support few-shot learning multi-modal evaluation.\nWe present zero-shot and few-shot (4- and 8-shots) results, as shown in Table 1. Overall, our model achieves\ncompetitive multimodal in-context learning performance with comparable-sized LMMs 1. For the OCR tasks\n(TextCaps and TextVQA) and VQA-v2, it significantly outperforms MM1-3B and even larger models such\nas Idefics-9B [13] and MM1-7B [9]. On all benchmarks, increasing the number of shots can improve the\nperformance, demonstrating the model’s ability to adapt to in-context distributions.\n6.2\nSupervised Fine-tuning\nWe evaluate our model on a comprehensive suite of multi-modal (image-text) benchmarks, assessing the\nmodel’s ability from multiple perspectives. Our evaluation covers general VQA benchmarks [74–78], visual\nperception [49], domain knowledge [79, 80], OCR ability [57, 81], and hallucination [82, 83]. For models\nfine-tuned on interleaved multi-image datasets, we also evaluate their performance on common multi-image\nbenchmarks [69, 84–86].\nSingle-Image Evaluation.\nIn Table 2, we compare with models in comparable sizes (< 5B), including\nboth closed-source [9] and SoTA open-source models [10, 37]. We report individual benchmark scores\nalong with two average scores: “Avg.(all)\" is simply the average over all benchmarks, and “Avg.(perc.)\" is\nthe average score over benchmarks that focus on general VQA and visual perceptions. xGen-MM-instruct\noutperforms previous baselines on both general VQA and visual perception benchmarks. In addition, we\n1For few-shot evaluation, the zero-shot results are used mainly as a reference for their corresponding few-shot numbers, and they\ncan be sensitive to prompts. As also mentioned in [9], they are mostly indicative of how well the pre-training distribution matches\nthe associated evaluation task format. In pre-training evaluation, we mainly care for few-shot performance, which is robust to prompt\ntemplates.\n8\nModel\nShot\nVisual Question Answering\nCaptioning\nVQAv2\nTextVQA\nOKVQA\nCOCO\nNoCaps\nTextCaps\n< 5B Model Comparisons\nFlamingo-3B [22]\n0\n49.2\n30.1\n41.2\n73.0\n–\n–\n4\n53.2\n32.7\n43.3\n85.0\n–\n–\n8\n55.4\n32.4\n44.6\n90.6\n–\n–\nMM1-3B [9]\n0\n46.2\n29.4\n26.1\n73.5\n55.6\n63.3\n4\n57.9\n45.3\n44.6\n112.3\n99.7\n84.1\n8\n63.6\n44.6\n48.4\n114.6\n104.7\n88.8\nxGen-MM-base (4B)\n0\n43.1\n34.0\n28.0\n67.2\n82.6\n69.5\n4\n66.3\n54.2\n48.9\n107.6\n100.8\n89.9\n8\n66.9\n55.3\n50.1\n109.8\n104.6\n94.0\nLarger Models for Reference\nFlamingo-9B [22]\n8\n58.0\n33.6\n50.0\n99.0\n-\n-\nIdefics-9B [13]\n8\n56.4\n27.5\n47.7\n97.0\n86.8\n63.2\nMM1-7B [9]\n8\n63.6\n46.3\n51.4\n116.3\n106.6\n88.2\nIdefics2-8B [11]\n8\n70.3\n57.9\n54.6\n116.0\n-\n-\nTable 1: Few-shot Pretraining Evaluation. Following [9], we randomly sample demonstrations from the\ntraining set as few-shot examples. We report CIDEr score for captioning and accuracy for VQA.\nfind that xGen-MM-instruct-interleave, although further fine-tuned on multi-image data, maintains good\nperformance on single-image benchmarks and has the highest overall scores.\nModel (Size)\nSEED\n-IMG\nSEED\nv2\nMMB\n(dev)\nMM\nStar\nMME\n(norm)\nCVB\n-2D\nCVB\n-3D\nRealW\nQA\nMMMU\n(val)\nMath\nVista\nSci\nQA\nPOPE\nText\nVQA\nAvg.\n(all)\nAvg.\n(perc.)\nClosed-source models\nGPT-4V\n72.0\n-\n80.8\n49.7\n63.3\n64.3\n73.8\n56.5\n53.8\n48.2\n82.1\n75.4\n-\n-\n-\nMM1-3B-Chat (3B)\n68.8\n-\n67.8\n-\n62.9\n-\n-\n-\n33.9\n-\n-\n87.4\n-\n-\n-\nOpen-source models\nHPT-1.5-edge (4B)\n72.3\n-\n74.6\n45.8\n-\n-\n-\n-\n42.6\n45.1\n85.4\n91.0\n-\n-\n-\nVILA-1.5-3B (3B)\n67.9\n-\n63.4\n-\n-\n-\n-\n-\n33.3\n-\n69.0\n85.9\n-\n-\n-\nVILA-1.5-3B∗(3B)\n67.9\n51.9\n62.4\n40.3\n58.5\n50.1\n60.3\n53.3\n34.1\n30.6\n68.9\n86.9\n58.1\n55.6\n59.1\nphi-3-vision (4B)\n-\n-\n80.5\n-\n-\n-\n-\n-\n-\n44.5\n90.8\n85.8\n70.9\n-\n-\nphi-3-vision∗(4B)\n71.0\n52.7\n74.2\n47.9\n55.3\n60.7\n68.2\n59.1\n46.1\n45.1\n90.2\n83.5\n73.3\n63.6\n63.6\nxGen-MM-inst. (4B)\n71.8\n53.9\n76\n46.7\n63.8\n66.2\n75.4\n61.6\n42.8\n39.2\n85.6\n87.0\n72.0\n64.8\n66.9\nxGen-MM-inst.\n-interleave (4B)\n72.2\n55.5\n76.8\n48.1\n64.4\n69.3\n72.3\n60.5\n41.1\n39.6\n88.3\n87.0\n71.0\n65.1\n67.3\nTable 2: Evaluation on single-image benchmarks. phi-3-vision∗and VILA-1.5-3B∗are tested with our\nevaluation code2 for a fair comparison. We also include the GPT-4V (gpt-4-1106-preview) performance\n(provided by the evaluation codebase) as a reference in the first row.\nMulti-Image Evaluation.\nIn Table 3, we compare xGen-MM-instruct with xGen-MM-instruct-interleave\non multi-image benchmarks. Although the former is fine-tuned from xGen-MM-base which can comprehend\ninterleaved image-text data, it performs poorly on multi-image benchmarks. We suspect it is because solely\nfine-tuning on single-image data hurts such ability. With multi-image SFT, we see significantly improved\nscores. In addition, we also evaluate Xgen-MM-interleave on single-image benchmarks (See Table 2) and\nfind that it maintains good performance on all benchmarks with the highest overall scores.\n2https:\/\/github.com\/open-compass\/VLMEvalKit\n9\nModel\nBLINK\nQBench-2\nMantis-eval\nGPT-4V\n51.1\n73.4\n62.7\nVILA-1.5-3B∗(3B)\n39.8\n51.7\n41.9\nxGen-MM-inst. (4B)\n46.6\n52.4\n42.4\nxGen-MM-inst.-interleave (4B)\n49.7\n75.1\n56.7\nTable 3: Evaluation on multi-image benchmarks. VILA-1.5-3B∗results are obtained using the same\nevaluation code as our models. We include the GPT-4V performance as a reference in the first row.\nUser: Which one of the following \ncan be found in the first image? \nA. \nB. \nxGen-MM: B  \nUser: Where does the object in \nimage 1 appear in image 2?\n \nxGen-MM: The object in image 1, \na screwdriver, appears in image 2 \non the left side, near the top left \ncorner.  \nUser: How much of BLIP-3's pre-training \ndata is from MINT-1T?\nxGen-MM: MINT-1T contributes 32.5% of \nBLIP-3's pre-training data.  \n* Zoom in for \nillustration.\nFigure 6: Example model outputs of xGen-MM-instruct-interleave. The model is capable of understanding\ninterleaved image-text input and user queries about multiple images while maintaining the performance on single-image\nQAs.\n6.3\nPost-training\nTable 4 summarizes the results of two post-training strategies for xGen-MM-instruct. We measure safety\nperformance by ASR% (attack success rate) on the VLGuard test split and hallucination performance using\nHallusionBench [82] (accuracy on image-context reasoning) and POPE [83] (average F1 score on binary\nentity presence questions). To ensure post-training doesn’t compromise helpfulness, we report performance\non a few comprehension benchmarks as a control.\nDPO enhances truthfulness by improving hallucination benchmarks (Row 2), while safety finetuning\nsignificantly reduces ASR (Row 3). Helpfulness is also improved slightly, as shown by control benchmarks.\nThe final model, xGen-MM-dpo, includes both improvements.\nMethod\nSafety\nHallucination\nHelpfulness (Control)\nVLGuard (↓) HalBench (↑) POPE (↑) SEED-IMG (↑) MMB-dev (↑) MME (↑) MMStar (↑)\nxGen-MM-inst. (4B)\n56.6\n56.3\n87.0\n71.8\n76.0\n63.8\n46.7\n+ DPO\n54.9\n57.1\n87.0\n71.9\n76.4\n63.0\n47.1\n+ Safety FT\n5.2\n56.6\n86.8\n72.1\n76.4\n64.4\n47.1\nTable 4: Post-training results. We report results on safety and hallucination benchmarks after post-training,\nas well as on four helpfulness benchmarks as a control. Post-training improves harmlessness without\ncompromising helpfulness.\n10\n7\nAblation Studies\n7.1\nPre-training Ablation\nScaling Pre-training Data.\nWe perform an ablation study to explore the relation between the amount\nof pre-training data and the pre-train evaluation metrics, by varying the data scale from 2B multimodal\ntokens to 100B multimodal tokens. The data recipe we used here is a mixture of image caption datasets and\nmultimodal interleaved data. As shown in Figure 7, we find that scaling up the number of multimodal tokens\nfrom 2B to 60B leads to substantial gain for image-text (COCO-Caps) and OCR (Text-Caps, TextVQA)\ntasks, and further increasing the data size to 100B has moderate additional benefit in terms of few-shot\nevaluation metrics.\n4 10\n20\n40\n60\n100\nNumber of training tokens(B)\n95\n100\n105\n110\nCider\nCOCO Caps 4-shot\nCOCO Caps 8-shot\n(a) COCO-Caps\n4 10\n20\n40\n60\n100\nNumber of training tokens(B)\n70\n80\n90\nCider\nText Caps 4-shot\nText Caps 8-shot\n(b) Text-Caps\n4 10\n20\n40\n60\n100\nNumber of training tokens(B)\n44\n46\n48\n50\nCider\nOK-VQA 4-shot\nOK-VQA 8-shot\n(c) OK-VQA\nFigure 7: Few-shot performance given different sizes of pretraining data.\nPre-training Data Recipe.\nWe discuss the impact of different data recipes for pre-training. Specifically,\nwe perform ablation studies on top of a base data recipe: use Obelics [13] as the multimodal interleaved data\nsource while keeping the caption datasets mixture the same. We also consider two other recipes (1) using\nMINT-1T [12] as interleaved data replacement, and (2) mixing additional pure text-only instruction-tuning\ndata as a pre-train dataset. As shown in Table 5, we see a performance improvement using MINT-1T for\nimage-text alignment (COCO-Caps) and OCR (Text-Caps, TextVQA), with a slight performance drop on\nOK-VQA, which is a knowledge-intensive task. We also find that adding text data can help attain the\nperformance on OK-VQA that relies more on LLM capacity.\nData\nText-VQA\nOK-VQA\nCOCO-Caps\nText-Caps\nObelics\n41.1 \/ 41.9\n48.4 \/ 49.5\n107.2 \/ 109.4\n78.2 \/ 79.9\nMINT-1T\n41.0 \/ 42.0\n46.5 \/ 48.2\n109.3 \/ 111.4\n80.3 \/ 82.0\nMINT-1T + text data\n42.1 \/ 42.6\n48.3 \/ 49.7\n108.0 \/ 110.2\n77.0 \/ 79.9\nTable 5: Few-shot (4-shot \/ 8-shot) performance given different data recipes.\nVisual Backbones.\nWe also explore if different visual backbones have an impact on the performance of\nvision-language tasks. We compare two types of visual encoders, DFN and SigLIP. Empirically, we find\nSigLIP provides better visual representations that boost performance on OCR tasks.\nVisual Backbone\nText-VQA\nOK-VQA\nCOCO-Caps\nText-Caps\nDFN\n41.1 \/ 41.8\n48.4 \/ 49.5\n107.2 \/ 109.4\n78.2 \/ 79.9\nSigLIP\n49.1 \/ 50.5\n48.4 \/ 48.9\n108.7 \/ 110.2\n84.7 \/ 88.6\nTable 6: Few-shot (4-shot \/ 8-shot) performance given different visual backbones.\n11\nNumber of Visual Tokens.\nAnother ablation is to study the impact of different numbers of visual tokens,\ni.e., input image tokens fed into the language model. We find that reducing the number of visual tokens from\n128 to 64 can still attain similar performance, as shown in Table 7. This makes it possible for models to take\nin more visual images given a fixed context window.\nVisual Token\nText-VQA\nOK-VQA\nCOCO-Caps\nText-Caps\n128\n41.1 \/ 41.8\n48.4 \/ 49.5\n107.2 \/ 109.4\n78.2 \/ 79.9\n64\n41.2 \/ 42.6\n47.6 \/ 48.3\n108.0 \/ 109.3\n79.5 \/ 81.6\nTable 7: Few-shot (4-shot \/ 8-shot) performance given the different number of visual tokens.\n7.2\nSFT Ablation\nWe conduct ablation studies at the instruction fine-tuning stage, focusing on several model design choices\nand data recipes. The SFT ablation studies are conducted on a simplified SFT data mixture, so the results in\nthis section are not directly comparable to the main results in section 6.2.\nAny-Resolution Vision Token Sampling.\nOur any-resolution strategy differs from previous work [46] in\nthat every group of image embeddings (of the same image patch) is downsampled with a perceiver resampler,\nwhich ensures that the number of vision tokens input to the LLM remains relatively small. In this section,\nwe ablate the effectiveness of our any-resolution strategy by comparing it with a “fixed-resolution\" baseline\nand other downsampling designs.\nThe “fixed-resolution\" baseline resizes all images to the default input size of the vision encoder while\nkeeping the original aspect ratios. We also tried another downsampling strategy with the perceiver resampler:\nInstead of doing downsampling for each patch independently, we consider a \"fixed sampling\" (denoted as\nanyres-fixed-sampling in Figure 8a). In the fixed sampling, we concatenate the image embeddings\nfrom all image patches and then input them as a single sequence to the perceiver resampler to obtain the\nfixed number of vision tokens for the whole image.\nDocVQA\nChartQA\nOCRBench\nOCR Benchmarks\n30\n35\n40\n45\n50\n55\n60\n65\n70\nScores\nbase resolution\nanyres-fixed-sampling (ntok=128)\nanyres-fixed-sampling (ntok=256)\nanyres-patch-sampling\n(a)\nPerception\nOCR\nSci. & Math\nHallucination\nEvaluation Domains\n45\n50\n55\n60\n65\n70\n75\nAverage Scores\nXGen-MM\nXGen-MM (instruction-aware)\n(b)\nFigure 8: SFT ablation studies. (a). Comparison of different vision token sampling strategies on OCR benchmarks.\n(b). Comparison between our model and its “instruction-aware\" alternative. For each evaluation domain in Figure (b),\nwe report the average score on multiple relevant benchmarks.\nOur evaluation of this design focuses on text-rich tasks (e.g., document understanding) that would benefit\nfrom high-resolution encoding with visual details. From Figure 8a, we can see significant improvements\n12\nwith our resolution image encoding strategy even with downsampled vision tokens. The fixed sampling\nstrategy, although it shows improvements over the base resolution baseline, is not as good as the patch-wise\nsampling. We suspect that this may be due to two reasons: (a) With fixed sampling, a vision token sequence\nthat can have as long as over 3,000 embedding tokens will be compressed to 128 tokens, which may be\ntoo few to retain the information. (b) The perceiver resampler may not work well with a concatenation of\ndifferent image embeddings.\nInstruction-Aware Vision Token Sampling.\nInstructBLIP [7] proposes an instruction-aware Q-Former [1]\nfor vision token sampling and shows that it can improve the model performance on some benchmarks. With\nthe perceiver resampler as the VL connector, we can adopt a similar modification to make this process\ninstruction-aware. To make our perceiver resampler “instruction-aware\", we append the text instructions\ntokens to the query tokens of the perceiver resampler. Unlike Q-Former, there are only cross-attention layers\ninside the perceiver resampler, so the instruction (text tokens) would interact with both query tokens and\nimage embeddings via cross-attention.\nFrom the comparison in Figure 8b, we do not observe a significant difference between our model and its\ninstruction-aware version on various benchmarks. It could be that our modification to the perceiver resampler\ncan not be identical to the instruction-aware modification made to the Q-Former in Dai et al. [7], and thus\nthe effectiveness differs. Because of the little difference we observe in this ablation study, we keep the\noriginal perceiver resampler architecture in our model for simplicity. We leave the further exploration of the\n“instruction-aware\" VL connector to future works.\nText-only SFT data\nMMMU\n(val)\nMathVista\n(mini)\nScience\nQA\nMME\n(norm)\nMMStar\nConversation\n39.1\n37.1\n84.8\n64.9\n46.1\nConversation + Math + Coding\n40.9\n38.9\n81.4\n64.8\n45.3\nTable 8: The impact of text-only SFT data. We compare two choices of text-only SFT data used for the image-text\nSFT data mixture.\nQuality of the Text-only Instruction Data.\nIt is a common strategy to train or fine-tune a multi-modal\nLLM on both multi-modal and pure text data [8, 11]. It is mainly for maintaining the language ability of\nthe fine-tuned model. In this experiment, we study how this pure text subset would affect the performance\non multi-modal benchmarks. For the instruction tuning stage, we compare whether the diversity of the pure\ntext data would affect the multi-modal performance. For example, how pure-text math data affects a model’s\nperformance on multimodal math benchmarks. In our main experiments, the default collection of pure text in-\nstruction data covers diverse domains including conversation [67], math [68, 87], and code [88]. For this abla-\ntion study, we substitute these datasets with the same amount of samples that only cover general conversation.\nIn Table 8, we observe that adding math and coding data, although in pure-text format, can help improve\na model on relevant benchmarks like MathVista [80], while has less effects on general VQA benchmarks.\n8\nConclusion\nWe introduce xGen-MM (BLIP-3), a comprehensive framework for training a series of open-source large\nmultimodal models on a curated mixture of large-scale datasets. xGen-MM (BLIP-3) demonstrates emergent\nabilities such as multimodal in-context learning and achieves impressive results on multimodal benchmarks.\nBy open-sourcing xGen-MM (BLIP-3), our curated datasets, and our SFT fine-tuning codebase, we hope\nto empower the research community with accessible multimodal foundation models and datasets, allowing\npractitioners to explore further and advance the potential and emergent abilities of LMMs.\n13\n9\nBroader Impact\nThe xGen-MM (BLIP-3) framework and its suite of Large Multimodal Models (LMMs) have the potential\nto significantly advance multimodal AI research by providing accessible, open-source resources for the\nbroader community. By facilitating the development and fine-tuning of state-of-the-art LMMs, xGen-MM\n(BLIP-3) empowers researchers and practitioners across various domains to innovate and apply these models\nto diverse real-world challenges. Moreover, the integration of safety-tuning protocols within the xGen-MM\n(BLIP-3) framework helps mitigate ethical risks such as bias and misinformation, promoting the responsible\ndeployment of AI technologies.\n10\nAcknowledgement\nWe would like to thank Srinath Meadusani, Lavanya Karanam, Dhaval Dilip Metrani, and Eric Hu for their\nwork on the scientific computation infrastructure, as well as Jason Lee and John Emmons for their efforts in\ncollecting the large-scale text-only SFT datasets used in one of our pre-training ablation studies.\nReferences\n[1] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-\nimage pre-training with frozen image encoders and large language models. In ICML, volume 202 of\nProceedings of Machine Learning Research, pages 19730–19742. PMLR, 2023.\n[2] OpenAI. Gpt-4v(ision) system card, 2023. URL https:\/\/cdn.openai.com\/papers\/GPTV_\nSystem_Card.pdf.\n[3] Google. Gemini: A family of highly capable multimodal models, 2023.\n[4] OpenAI. Hello gpt-4o, 2024. URL https:\/\/openai.com\/index\/hello-gpt-4o\/.\n[5] AI @ Meta Llama Team. The llama 3 herd of models, 2024. URL https:\/\/ai.meta.com\/\nresearch\/publications\/the-llama-3-herd-of-models\/.\n[6] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image\npre-training for unified vision-language understanding and generation. In ICML, volume 162 of\nProceedings of Machine Learning Research, pages 12888–12900. PMLR, 2022.\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models\nwith instruction tuning. In NeurIPS, 2023.\n[8] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[9] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter,\nDhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh,\nDoug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang,\nChong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch,\nAlexander Toshev, and Yinfei Yang. MM1: Methods, Analysis & Insights from Multimodal LLM\nPre-training, March 2024. URL http:\/\/arxiv.org\/abs\/2403.09611. arXiv:2403.09611\n[cs].\n[10] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz,\nMohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2312.07533.\n14\n[11] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building\nvision-language models?, 2024. URL https:\/\/arxiv.org\/abs\/2405.02246.\n[12] Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng\nShen, Mohamed Awadalla, Silvio Savarese, et al. Mint-1t: Scaling open-source multimodal data by\n10x: A multimodal dataset with one trillion tokens. arXiv preprint arXiv:2406.11271, 2024.\n[13] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,\nThomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-\nscale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing\nSystems, 36, 2024.\n[14] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE\/CVF\nconference on computer vision and pattern recognition, pages 3558–3568, 2021.\n[15] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International journal of computer vision, 123:32–73,\n2017.\n[16] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011.\n[17] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,\nRyan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next\ngeneration of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.\n[18] Erik Nijkamp, Hiroaki Hayashi Tian Xie, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih\nYavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya\nMurakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat,\nChien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, and Caiming Xiong. Long\nsequence modeling with xgen: A 7b llm trained on 8k input sequence length. ArXiv, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2309.03450.\n[19] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474, 2022.\n[20] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2:\nLessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309,\n2023.\n[21] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran\nYao, Zhiwei Liu, Yihao Feng, et al. Apigen: Automated pipeline for generating verifiable and diverse\nfunction-calling datasets. arXiv preprint arXiv:2406.18518, 2024.\n[22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for\nfew-shot learning. In NeurIPS, 2022.\n15\n[23] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh,\nGabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework\nfor training large autoregressive vision-language models. CoRR, abs\/2308.01390, 2023.\n[24] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966, 2023.\n[25] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual\nvision and language model. arXiv preprint arXiv:2305.18565, 2023.\n[26] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023.\n[27] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception\nwith language models. Advances in Neural Information Processing Systems, 36, 2024.\n[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei.\nKosmos-2: Grounding multimodal large language models to the world.\narXiv preprint\narXiv:2306.14824, 2023.\n[29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning, 2023.\n[30] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\n[31] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei\nHuang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.\nIn Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages\n13040–13051, 2024.\n[32] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with\nhumans. arXiv preprint arXiv:2305.04790, 2023.\n[33] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced\nprojector for multimodal llm. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 13817–13827, 2024.\n[34] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie\nGeng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal\nlarge language models. arXiv preprint arXiv:2402.05935, 2024.\n[35] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In\nProceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 14398–\n14409, 2024.\n16\n[36] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with\nscalable training strategies. arXiv preprint arXiv:2404.06395, 2024.\n[37] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha\nBilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu\nChen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,\nXiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan\nIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman\nHaider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann,\nNikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee,\nYuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin,\nZeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen,\nBrandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko\nRadmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim,\nMichael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro\nTanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua\nWang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali\nYadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang,\nCyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou.\nPhi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone, May 2024. URL\nhttp:\/\/arxiv.org\/abs\/2404.14219. arXiv:2404.14219 [cs].\n[38] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. corr abs\/2305.03726 (2023), 2023.\n[39] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin.\nSharegpt4v: Improving large multi-modal models with better captions.\narXiv preprint\narXiv:2311.12793, 2023.\n[40] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, et al. A large-scale dataset towards multi-modal multilingual instruction tuning.\narXiv preprint arXiv:2306.04387, 3, 2023.\n[41] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv\npreprint arXiv:2307.04087, 2023.\n[42] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe:\nPrompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023.\n[43] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and\nZiwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425,\n2023.\n[44] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal\nShankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023.\n[45] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. In Proceedings of the IEEE\/CVF International Conference on Computer Vision,\npages 11975–11986, 2023.\n[46] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-\nnext: Improved reasoning, ocr, and world knowledge, January 2024. URL https:\/\/llava-vl.\ngithub.io\/blog\/2024-01-30-llava-next\/.\n17\n[47] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-\nJui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, and Yinfei Yang. Ferret-v2: An Improved\nBaseline for Referring and Grounding with Large Language Models, April 2024.\nURL http:\n\/\/arxiv.org\/abs\/2404.07973.\n[48] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang,\nHaodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li,\nJingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua\nLin, and Jiaqi Wang. InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model\nHandling Resolutions from 336 Pixels to 4K HD, April 2024. URL http:\/\/arxiv.org\/abs\/\n2404.06512.\n[49] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula,\nJihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and\nSaining Xie. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs, June 2024.\nURL http:\/\/arxiv.org\/abs\/2406.16860. arXiv:2406.16860 [cs].\n[50] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances in\nNeural Information Processing Systems, 36, 2024.\n[51] https:\/\/github.com\/PFCCLab\/PPOCRLabel. Awesome multilingual ocr toolkits based on paddlepaddle.\n[52] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in\nreferring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69–85. Springer, 2016.\n[53] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong\nLuo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 1724–1732, 2024.\n[54] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set\nobject detection. arXiv preprint arXiv:2303.05499, 2023.\n[55] An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng\nWang, Julian McAuley, Jianfeng Gao, et al. List items one by one: A new data source and learning\nparadigm for multimodal llms. arXiv preprint arXiv:2404.16375, 2024.\n[56] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin.\nSharegpt4v: Improving large multi-modal models with better captions.\narXiv preprint\narXiv:2311.12793, 2023.\n[57] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 8317–8326, 2019.\n[58] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In 2019 International Conference on Document Analysis\nand Recognition (ICDAR), pages 947–952, 2019. doi: 10.1109\/ICDAR.2019.00156.\n[59] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A benchmark for visual question answering using world knowledge. In Computer Vision –\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII,\npage 146–162, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-20073-1. doi: 10.1007\/\n978-3-031-20074-8_9. URL https:\/\/doi.org\/10.1007\/978-3-031-20074-8_9.\n18\n[60] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In 2019 IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 6693–6702, 2019. doi: 10.1109\/CVPR.2019.00686.\n[61] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: A dataset for vqa on document\nimages. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2199–\n2208, 2021. doi: 10.1109\/WACV48630.2021.00225.\n[62] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.\nChartQA: A bench-\nmark for question answering about charts with visual and logical reasoning. In Findings of the\nAssociation for Computational Linguistics: ACL 2022, pages 2263–2279, Dublin, Ireland, May\n2022. Association for Computational Linguistics. doi: 10.18653\/v1\/2022.findings-acl.177. URL\nhttps:\/\/aclanthology.org\/2022.findings-acl.177.\n[63] Kushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan. Dvqa: Understanding data visualiza-\ntions via question answering. In CVPR, 2018.\n[64] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\nA diagram is worth a dozen images. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,\nComputer Vision – ECCV 2016. Springer International Publishing, 2016. ISBN 978-3-319-46493-0.\n[65] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross\nGirshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1988–1997, 2017.\ndoi: 10.1109\/CVPR.2017.215.\n[66] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems, volume 35, pages 2507–2521. Curran\nAssociates, Inc., 2022. URL https:\/\/proceedings.neurips.cc\/paper_files\/paper\/\n2022\/file\/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf.\n[67] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\".\nOpenorca: An open dataset of gpt augmented flan reasoning traces.\nhttps:\/\/https:\/\/\nhuggingface.co\/Open-Orca\/OpenOrca, 2023.\n[68] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the\npotential of slms in grade school math, 2024.\n[69] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. MANTIS:\nInterleaved Multi-Image Instruction Tuning, May 2024. URL http:\/\/arxiv.org\/abs\/2405.\n01483. arXiv:2405.01483 [cs].\n[70] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong,\nYu Qiao, Dahua Lin, and Jiaqi Wang. Mmdu: A multi-turn multi-image dialog understanding benchmark\nand instruction-tuning dataset for lvlms, 2024. URL https:\/\/arxiv.org\/abs\/2406.11833.\n[71] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang,\nand Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint\narXiv:2312.10665, 2023.\n[72] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on Learning\nRepresentations.\n19\n[73] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety\nfine-tuning at (almost) no cost: A baseline for vision large language models. In The 41st International\nConference on Machine Learning, 2024.\n[74] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-\nmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[75] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\narXiv preprint arXiv:2307.06281, 2023.\n[76] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin,\nJinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: A comprehensive evaluation\nbenchmark for multimodal large language models. CoRR, abs\/2306.13394, 2023.\n[77] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi\nWang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models?\narXiv preprint arXiv:2403.20330, 2024.\n[78] ai. Grok-1.5 vision preview, 2024. URL https:\/\/x.ai\/blog\/grok-1.5v.\n[79] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A\nmassive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR,\nabs\/2311.16502, 2023.\n[80] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng,\nKai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning\nof foundation models in visual contexts. In International Conference on Learning Representations\n(ICLR), 2024.\n[81] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang\nBai. On the hidden mystery of ocr in large multimodal models, 2024. URL https:\/\/arxiv.org\/\nabs\/2305.07895.\n[82] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang\nChen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled\nlanguage hallucination and visual illusion in large vision-language models. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 14375–14385, 2024.\n[83] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. In The 2023 Conference on Empirical Methods in\nNatural Language Processing.\n[84] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li,\nWenxiu Sun, Qiong Yan, Guangtao Zhai, and Weisi Lin. Q-bench: A benchmark for general-purpose\nfoundation models on low-level vision. In ICLR. OpenReview.net, 2024.\n[85] Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan\nXu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu,\nChunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, and Muhao\nChen.\nMuirbench: A comprehensive benchmark for robust multi-image understanding.\nCoRR,\nabs\/2406.09411, 2024.\n20\n[86] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith,\nWei-Chiu Ma, and Ranjay Krishna. BLINK: multimodal large language models can see but not perceive.\nCoRR, abs\/2404.12390, 2024.\n[87] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. CoRR, abs\/2110.14168, 2021.\n[88] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang\nYue. Opencodeinterpreter: Integrating code generation with execution and refinement, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2402.14658.\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/xGen-MM (BLIP-3): A Family of Open Large Multimodal Models.pdf"}
{"title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation","authors":"Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan","summary":"In this work, we introduce Vision-Language Generative Pre-trained Transformer\n(VL-GPT), a transformer model proficient at concurrently perceiving and\ngenerating visual and linguistic data. VL-GPT achieves a unified pre-training\napproach for both image and text modalities by employing a straightforward\nauto-regressive objective, thereby enabling the model to process image and text\nas seamlessly as a language model processes text. To accomplish this, we\ninitially propose a novel image tokenizer-detokenizer framework for visual\ndata, specifically designed to transform raw images into a sequence of\ncontinuous embeddings and reconstruct them accordingly. In combination with the\nexisting text tokenizer and detokenizer, this framework allows for the encoding\nof interleaved image-text data into a multimodal sequence, which can\nsubsequently be fed into the transformer model. Consequently, VL-GPT can\nperform large-scale pre-training on multimodal corpora utilizing a unified\nauto-regressive objective (i.e., next-token prediction). Upon completion of\npre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance\nacross a diverse range of vision and language understanding and generation\ntasks, including image captioning, visual question answering, text-to-image\ngeneration, and more. Additionally, the pre-trained model retrains in-context\nlearning capabilities when provided with multimodal prompts. We further conduct\ninstruction tuning on our VL-GPT, highlighting its exceptional potential for\nmultimodal assistance. The source code and model weights shall be released.","url":"http:\/\/arxiv.org\/abs\/2312.09251v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.09251v1","published":1702580383000,"comment":null,"pdf_text":"VL-GPT: A Generative Pre-trained Transformer\nfor Vision and Language Understanding and Generation\nJinguo Zhu1*\nXiaohan Ding2*\nYixiao Ge2,3\nYuying Ge2\nSijie Zhao2\nHengshuang Zhao4\nXiaohua Wang1\nYing Shan2,3B\n1 Xi’an Jiaotong University\n2 Tencent AI Lab\n3 ARC Lab, Tencent PCG\n4 The University of Hong Kong\nAbstract\nIn this work, we introduce Vision-Language Generative\nPre-trained Transformer (VL-GPT), a transformer model\nproficient at concurrently perceiving and generating visual\nand linguistic data. VL-GPT achieves a unified pre-training\napproach for both image and text modalities by employing a\nstraightforward auto-regressive objective, thereby enabling\nthe model to process image and text as seamlessly as a lan-\nguage model processes text. To accomplish this, we initially\npropose a novel image tokenizer-detokenizer framework for\nvisual data, specifically designed to transform raw images\ninto a sequence of continuous embeddings and reconstruct\nthem accordingly. In combination with the existing text tok-\nenizer and detokenizer, this framework allows for the en-\ncoding of interleaved image-text data into a multimodal\nsequence, which can subsequently be fed into the trans-\nformer model. Consequently, VL-GPT can perform large-\nscale pre-training on multimodal corpora utilizing a uni-\nfied auto-regressive objective (i.e., next-token prediction).\nUpon completion of pre-training, VL-GPT exhibits remark-\nable zero-shot and few-shot performance across a diverse\nrange of vision and language understanding and genera-\ntion tasks, including image captioning, visual question an-\nswering, text-to-image generation, and more. Additionally,\nthe pre-trained model retrains in-context learning capabil-\nities when provided with multimodal prompts. We further\nconduct instruction tuning on our VL-GPT, highlighting its\nexceptional potential for multimodal assistance.\n1. Introduction\nDriven by the remarkable success of large language mod-\nels (LLMs) in the field of natural language processing\n*Equal contribution. This work is done when Jinguo Zhu is an intern\nat Tencent AI Lab. The source code and model weights shall be released\nat https:\/\/github.com\/AILab-CVC\/VL-GPT. BCorresponding\nauthor.\n(NLP) [40, 41, 54], there has been a surge of interest within\nmultimodal community to develop large vision-language\n(VL) models. One of the promising approaches, exempli-\nfied by Flamingo [1], BLIP2 [24], LLAVA [25], have ex-\nplored how to build large VL models based on powerful\npre-trained LLMs. These studies typically adopted a similar\narchitecture: a pre-trained image encoder and an LLM are\nconnected via a trainable connection module, which aligns\nthe image feature and text embeddings, thereby enabling\nlanguage models to accept images and text as inputs and\ngenerate a text sequence.\nTo expand the capabilities of generating image in a mul-\ntimodal context, certain efforts, e.g., Visual ChatGPT [47],\nattempt to connect LLMs with image generation tools in a\ncascaded pipeline by transferring text messages, which in-\nevitably introduce instability and noise. Alternatively, an-\nother line of research achieves it by optimizing models in\nan end-to-end manner [9, 18, 23, 30, 48]. By aligning the\noutput space with the image diffusion models, VL models\ncan not only perceive but also generate images and text.\nA crucial characteristic of large language models is auto-\nregressive modeling [31], i.e., predicting next token, which\nfacilitates language understanding and generation in a uni-\nfied manner. However, in the aforementioned studies, the\ninconsistency of image embeddings between LLM’s input\nand output sides compels the model to treat input images\nand generated images differently, resulting in separate mod-\neling for image understanding and generation. Meanwhile,\nthis discrepancy also obstructs the implementation of auto-\nregressive training loss on image embeddings.\nIn this study, we introduce VL-GPT, a large vision-\nlanguage generative pre-trained transformer that enables the\nunified training of both visual and linguistic data using an\nauto-regressive objective, as depicted in Fig. 1. To achieve\nthis, we propose an image tokenizer-detokenizer framework\nfor the conversion between raw image pixels and contin-\nuous visual embeddings, analogous to the role of the text\ntokenization [19, 43] in language models. The framework\n1\narXiv:2312.09251v1  [cs.CV]  14 Dec 2023\nImage Tokenizer\nImage Detokenizer\nImage \nTokenizer\nText \nTokenizer\nLarge \nVision-Language \nTransformer\nModel\nImage \nDetokenizer\nText \nDetokenizer\nVL-GPT\nInterleaved\nImage-text\n Input\nInterleaved\nImage-text \nGeneration\nMultimodal  \nSequence \nMultimodal  \nSequence \nVisual  \nEmbeddings\nCausal \nTransformer\nTransformer\nDecoder\nDiffusion Decoder\nVisual Encoder\nFigure 1. Overview of our proposed approach. The upper part delineates the image tokenizer-detokenizer framework, designed for encoding\nimages into continuous visual embeddings and reconstructing them in the pixel space. The lower part demonstrates the implementation\nof our VL-GPT, where interleaved image-text data are encoded into multimodal sequence using image and text tokenizers, subsequently\nprocessed by a transformer model auto-regressively. The image and text detokenizers are employed for generating respective outputs.\ncomprises an image tokenizer and an image detokenizer,\nwhere the tokenizer encodes raw images into a sequence of\ncontinuous visual embeddings, and the detokenizer decodes\nthe continuous embeddings into pixel space. To obtain vi-\nsual continuous embeddings that are rich in both image de-\ntails and semantic information, we employ the image em-\nbeddings and their corresponding caption embeddings ex-\ntracted by pre-trained encoders (i.e., CLIP [32]) as the su-\npervision for training of the framework. Furthermore, the\nefficiency of the framework training is enhanced through\nweight initialization from pre-trained image encoders and\nhigh-quality image diffusion models.\nBy employing the image tokenizer-detokenizer frame-\nwork, visual embeddings can achieve consistency on both\nthe input and output sides of the transformer model. Con-\nsequently, interleaved image-text data can be trained in\na unified auto-regressive manner. Specifically, the image\ntokenizer and the existing text tokenizer (i.e., BPE tok-\nenizer [43]) first convert the image and text into a multi-\nmodal sequence consisting of interleaved continuous visual\nembeddings and discrete text tokens. The transformer can\nthen be trained to predict the next embedding or token in\nthis multimodal sequence, employing mean squared error\n(MSE) loss for continuous visual embeddings and cross-\nentropy loss for discrete text tokens. Contrary to previous\nworks [9, 18, 30, 48], all embeddings in the multimodal\nsequence can receive supervision from the auto-regressive\nloss. During the generation stage, visual embeddings and\ntext tokens can be generated auto-regressively without dis-\ntinction, and subsequently decoded into raw images and text\nby the image detokenizer and text detokenizer, respectively.\nOwing to the unified modeling, the pre-training of the\nVL model can be conducted on large-scale image-text pairs\nand interleaved image-text data. Upon completion of pre-\ntraining, the model is capable of perceiving arbitrary multi-\nmodal input and generating responses varying in modalities\n(e.g., text, images or their interleaved contents), allowing\nit to generalize to a wide range of vision and language un-\nderstanding and generation tasks in a zero-shot or few-shot\nmanner. Moreover, the pre-trained model exhibits appeal-\ning emergent properties for multimodal in-context learning,\nas it can effectively tackle new unseen tasks when provided\nwith multimodal prompts. The VL generative pre-trained\ntransformer model, referred to as VL-GPT, holds the poten-\ntial to serve as a powerful foundation model for the multi-\nmodal community, similar to the role of GPT family [4, 29]\nin NLP. Our contributions are summarized as follows:\n• We propose an image tokenizer-detokenizer framework to\nconvert images into continuous embeddings and recon-\nstruct them, while exploring effective training methods\nfor this framework.\nThrough efficient training that requires an affordable\ncomputational cost, the image tokenizer and detokenizer\ncan effectively retain both semantic information and pixel\ndetails of the original image.\n• We introduce VL-GPT, a generative pre-trained trans-\n2\nformer model for vision and language (VL) understand-\ning and generation tasks. The model can be pre-trained\non large-scale multimodal corpora in a unified auto-\nregressive manner, i.e., predicting the next token in a mul-\ntimodal sequence containing continuous visual embed-\ndings and discrete text tokens without any discrimination.\n• VL-GPT exhibits competitive performance on various VL\nunderstanding and generation benchmarks under zero-\nshot and few-shot settings, including image captioning,\nvisual question answering, and text-to-image generation.\nIt also demonstrates an appealing multimodal in-context\nlearning ability when provided with multimodal prompts.\nFurthermore, it shows promising potential to serve as a\ngeneral multimodal assistant through instruction tuning.\n2. Related Work\nMultimodal Pre-training in the Pre-LLM Era. Prior re-\nsearch efforts primarily concentrated on model architec-\nture to facilitate the fusion and interaction of cross-model\ndata [6, 50, 52]. The success of transformers in language\nmodels [42] and ViT [10] inspired the development of uni-\nfied multi-modal modeling [27, 44]. Although images and\nlanguage can be processed by a unified model with shared\nparameters, they often have distinct training objectives. It\nis worth mentioning that the BEiT series [2, 45] success-\nfully adapted the masked language modeling objective from\nBERT [8] to vision and multimodal pre-training.\nMultimodal Pre-training in the LLM Era. Building upon\npre-trained large language models (LLMs) [33, 40, 41, 54],\nrecent studies have effectively developed multimodal lan-\nguage models capable of processing image and text inputs\nto generate text outputs [1, 22, 24, 25, 56]. Another chal-\nlenge for large multimodal models is generating multimodal\ncontent beyond language. Several efforts, such as Visual\nChatGPT [47] and HuggingGPT [38], have achieved this\nby connecting LLMs with other generation tools within an\nLLM integration framework, e.g., LangChain. However,\nthese systems exhibit instability and limited room for fur-\nther optimization. To enable LLMs to generate images with\noptimization, M-VADER [46] aligns the semantic consis-\ntence between an LLM and a diffusion decoder by train-\ning them on image-text pair data. GILL [18] achieves more\ncomplex interleaved image-text generation by mapping the\nembedding spaces of the LLM to text-to-image generation\nmodels. NExT-GPT [48] extends this concept to additional\nmodalities, such as audio and video. DreamLLM [9] fa-\ncilitates passing the differential gradient from image diffu-\nsion models to language models, enabling the generation of\nfree-form interleaved content. Following similar methods,\nKosmos-G [30] enhances the fidelity of generated images in\ncontext through a compositional instruction tuning task.\nIn contrast to our VL-GPT, these studies mainly focus on\nImage \nEncoder\nText \nEncoder\na brown dog is \nsleeping on a bed\nRec Loss\nRec Loss\nInput image\nImage caption\nVisual continuous embedding xv\nImage embedding ev\nText embedding et\nEstimated image embedding zv\nEstimated text embedding zt\nImage Tokenizer\nReconstruction \nImage !x\nUnused During\nTraining\nImage Detokenizer\nCausal \nTransformer\nTransformer\nDecoder\nVisual Encoder\nDiffusion Decoder\nFigure 2. The training scheme of our image tokenizer-detokenizer\nframework, which is supervised by the frozen image and text en-\ncoders of our adopted pre-trained image diffusion model. Only\nthe causal transformer in tokenizer and the transformer decoder\nin detokenizer necessitate training, while the diffusion decoder in\ndetokenizer remains unused during training.\nleveraging existing LLMs and exploring the integration of\ncurrent image encoders and image generation models into\nLLMs.\nHowever, these methods do not achieve unified\nmodeling for images and language, nor unified modeling\nfor image understanding and generation. For instance, spe-\ncial queries are typically needed to encapsulate the context\ninformation for image generation, but they are deemed un-\nnecessary when images serve as input for LLMs. Moreover,\napplying an auto-regressive training objective on visual em-\nbeddings is challenging due to the inconsistency of image\nembedding space. Consequently, these approaches are lim-\nited in expanding the scalable pre-training paradigm for\nthe GPT family, i.e., next-token prediction, to large vision-\nlanguage models on web-scale multimodal corpora.\nRecently, Emu [39] proposes a multimodal pre-trained\nmodel that enables the auto-regressive training for both vi-\nsual and text embeddings. However, it requires an costly\nsecond-stage fine-tuning of the Stable Diffusion [35] to con-\nvert the visual embeddings into pixel space. In contrast, our\nmethod utilizes a novel image tokenizer-detokenizer frame-\nwork that can fully leverage a pre-trained image diffusion\nmodel (see Fig. 2). This approach not only simplifies the\nprocess but also enhances training efficiency. Similar to our\napproach, SEED [11] initially trains an image tokenizer, fol-\nlowed by a multi-modal training. Nevertheless, its tokenizer\nencodes images into discrete tokens via quantization opera-\ntions, potentially losing partial image information. In con-\ntrast, our tokenizer converts images into continuous visual\nembeddings, preserving both semantic information and ap-\npearance details, resulting in improved performance across\n3\ndiverse benchmarks.\n3. Method\nAs illustrated in Fig. 1, the implementation of our VL-GPT\ncan be separated into two consecutive stages. In the first\nstage, we learn an image tokenizer-detokenizer framework,\ncapable of encoding images into continuous visual embed-\ndings and decoding them back. The second stage is the pre-\ntraining and instruction tuning of our VL-GPT, which facil-\nitates a unified modeling approach for vision and language\nunderstanding and generation. In the following sections, we\nwill provide a detailed description of these two stages.\n3.1. Image Tokenizer-Detokenizer Framework\nTo implement an auto-regressive training objective on vi-\nsual embeddings and text tokens concurrently, we de-\nvelop an image tokenizer-detokenizer framework for vision-\nlanguage models.\nThe framework, inspired by text tok-\nenizers utilized in language models [43], can realize bi-\ndirectional conversion between original images and contin-\nuous visual embeddings, thereby enabling the transformer\nmodel to process vision data akin to processing text data.\nArchitecture The overall architecture of our image\ntokenizer-detokenizer framework is depicted in Fig. 1. It\ncomprises two primary components: a tokenizer E respon-\nsible for encoding the image into continuous visual embed-\ndings, and a detokenizer D dedicated to decoding the visual\nembeddings back to raw images.\nFormally, the image tokenizer E employs an image en-\ncoder (e.g., ViT [10]) to extract spatial patched features xp\nfrom the given image x. Subsequently, a standard decoder-\nonly causal transformer is utilized to convert the patched\nfeatures xp to 1D (one-dimensional) visual embeddings\nxv ∈RN×d, where N represents the number of visual em-\nbeddings, and d denotes the embedding dimension. The\n1D continuous visual embeddings xv serve as input embed-\ndings to our vision-language model, analogous to word to-\nkens in language models.\nInspired by current image diffusion models with excel-\nlent performance and accessibility [34, 35, 49], our image\ndetokenizer D learns a latent diffusion model to decode\nvisual embeddings xv into images. Specifically, a trans-\nformer decoder is employed to estimate condition embed-\nding z from xv. Then a diffusion decoder, initialized from\na pre-trained image diffusion models, can generate images\nˆx based on estimated condition embedding z.\nTraining Despite the initialization with pre-trained models,\nconducting a full-scale end-to-end optimization of the im-\nage tokenizer and detokenizer demands large-scale data and\nconsiderable training costs. To pursue efficient training, we\nopt to train the transformer decoder in image detokenizer\nto estimate the condition embedding utilized for the diffu-\nsion decoders, as illustrated in Fig. 2. Notably, the diffusion\ndecoder, including its U-Net and VAE modules, is not em-\nployed during framework training, substantially enhancing\nthe efficiency of training procedure.\nAs Fig. 2 shows, the training objective of our framework\naims to concurrently reconstruct the image condition em-\nbedding ev and text condition embedding et. This design\ndistinguishes our framework from previous works [11, 18,\n48], which only align their intermediate outputs with text\nembedding produced by the text encoder of the diffusion\nmodel. Specifically, we optimize the framework by mini-\nmizing the following loss function (with weight λ1 and λ2):\nL(z) = λ1 ∗MSE(zv, ev) + λ2 ∗MSE(zt, et)\n(1)\nwhere MSE (·) denotes the mean squared error loss, and\nzv and zt represent the estimated image condition em-\nbedding and estimated text condition embedding, respec-\ntively. During inference, both types of condition embed-\nding contribute collectively to generate images. Our image\ntokenizer-detokenizer framework can also work when re-\nconstructing only image condition embedding (if λ2=0) or\nonly text condition embedding (if λ1=0). Moreover, the\ntraining for estimating image embedding only requires vi-\nsual data, which is more training-friendly than estimating\ntext embedding. However, our experiments in Sec. 4.5 re-\nveal that these two types of embedding complement each\nother: text embedding contain rich semantic information\nwhile image embedding effectively persevere image details.\n3.2. VL-GPT\nVL-GPT aims to process the vision and language under-\nstanding and generation within a single transformer model\nin a unified way, similar to GPT handles language tasks.\nIt is capable of perceiving the interleaved multi-modal data\nand generating content across various modalities. By em-\nploying unified modeling, our VL-GPT can conduct auto-\nregressive pre-training on web-scale multimodal corpora,\nthereby holding the potential to serve as a powerful foun-\ndation model in the multimodal research community.\nArchitecture As depicted at the bottom of Fig. 1, our VL-\nGPT comprises five components: a large vision-language\ntransformer model M, an image tokenizer Ev, a text tok-\nenizer Et, an image detokenizer Dv and a text detokenizer\nDt. In comparison to a language model, VL-GPT incor-\nporates additional image tokenizer and image detokenizer\nelements.\nGiven any interleaved image-text data, the image tok-\nenizer and the text tokenizer initially encode them into a\nmultimodal sequence.\nMore specifically, the image tok-\nenizer Ev converts each image into N continuous visual em-\nbeddings xv. Additionally, two special tokens [IMG] and\n[\/IMG] are appended at the beginning and end of the vi-\nsual embeddings, respectively. The visual embeddings are\n4\nthen combined with the discrete text tokens encoded by the\ntext tokenizer Et to form a interleaved multimodal sequence\nv = (v1, v2, . . . , vn), where vi can be either a discrete text\ntoken or a continuous visual embedding. The multimodal\nsequence v is then fed into the large VL model M for uni-\nfied auto-regressive modeling.\nThe output embedding M(vi) can be flexibly trans-\nformed into a text embedding through a language model-\ning head for the predefined vocabulary or into a visual em-\nbedding with a separate regression head. During training,\nthe selection of the transformed head depends on whether\nthe target for the current embedding is a text token or a vi-\nsual embedding. During inference, if [IMG] is predicted, the\nvisual regression head will be utilized to transform output\nembeddings in the subsequent N prediction; otherwise, the\nlanguage modeling head will be used. The prediction em-\nbeddings are subsequently decoded to raw images or text\nvia the image detokenizer Dv or the text detokenizer Dt .\nMultimodal Pre-training.\nBenefiting from the unified\nmodeling of both visual and text embeddings, we can apply\nthe unsupervised pre-training paradigm of GPT [31] to our\nVL-GPT on a large corpus of multimodal data with minimal\nmodifications.\nGiven an interleaved multimodal sequence v\n=\n(v1, v2, . . . , vn) in a large-scale corpora, we employ the\nstandard auto-regressive modeling objective in language\nmodels to maximize the following likelihood:\nL(v) =\nn\nX\ni\nlog P (vi | v1, v2, . . . , vi−1; Θ)\n(2)\nwhere Θ represents the parameters of our VL-GPT. We ap-\nply cross-entropy loss with a language modeling head on the\ndiscrete text tokens and utilize MSE loss with a regression\nhead for continuous visual embeddings.\nInstruction Tuning To enhance the ability of the pre-\ntrained VL-GPT to follow human instructions faithfully and\ngenerate multimodal contents creatively, we perform fur-\nther instruction tuning of VL-GPT using publicly available\ninstruction-tuning datasets.\nBriefly, the data from these\ndatasets will be restructured into a conversational format,\ni.e., pairs of multimodal human instructions and their re-\nsponses for single or multiple rounds, and subsequently em-\nployed for model tuning in a manner similar to the pre-\ntraining corpora. A minor deviation from pre-training pro-\ncess is that the training objective will be applied exclusively\nto the embeddings tokenized from answer responses.\n4. Experiments\nThe training of our VL-GPT consists of three phases: train-\ning for the tokenizer-detokenizer framework, unified mul-\ntimodal pre-training for the vision-language transformer\nmodel, and instruction tuning for the pre-trained VL-GPT.\nInput Images\nReconstruction (zv+zt)\nReconstruction (zv)\nReconstruction (zt)\nFigure 3.\nReconstruction images of our image tokenizer-\ndetokenizer framework by utilizing image condition embedding\n(zv), or text condition embedding (zt), or both types of condition\nembedding (zv+zt). More examples are included in the appendix.\nModel\nCOCO\nFlickr30k\nGILL [18]\n67.45\n65.16\nSD v1.5 [35]\n68.43\n65.40\nSEED [11]\n68.23\n65.22\nunCLIP [34]\n79.30\n79.55\nOur tokenizer-detokenizer\n80.22\n79.14\nTable 1. Evaluation of image reconstruction with CLIP similarity.\n4.1. Datasets\nPublicly available datasets are utilized for different phrase\nof the VL-GPT training. The image tokenizer-detokenizer\nframework is trained on image-text pairs from CC3M [37],\nLAION-Aestheics [20], and LAION-COCO [36].\nDur-\ning the unified multimodal pre-training of VL-GPT, a\ncombination of paired and interleaved image-text data is\nemployed.\nThe image-text pairs remain consistent with\nthe preview phase, while the interleaved image-text se-\nquences are acquired from Multimodal-C4 (MMC4) [57]\nand OBELICS [21]. We adopt similar preprocessing tech-\nniques for interleaved data implemented in Flamingo [1].\nFor each document, a maximum of 5 images and their as-\nsociated captions are randomly sampled to construct a sub-\nsequence with a token length of up to 512. Additionally,\nfor paired and interleaved image-text data, each image is\nrandomly placed before or after its corresponding caption.\nFor the instruction tuning of VL-GPT, a compositional in-\nstruction tuning dataset is constructed from various sources,\nencompassing conversational data from LLAVA [25] and\nSVIT [55], image-text pair data from COCO Caption [5],\nand image editing data from InstructPix2Pix [3] and Mag-\nicbrush [53]. These datasets are restructured into a conver-\n5\nModels\nImage-Text understanding\nText-to-image generations\nCOCO\nVQAv2\nGQA\nOKVQA\nVizWiz\nVisDial\nCOCO FID (↓)\n▶VL Understanding or generation Models\nMetaLM [14]\n82.2\n41.1\n-\n11.4\n-\n-\n-\nKosmos-1 [16]\n84.7\n51.0\n-\n-\n29.2\n-\n-\nFlamingo-9B¶ [1]\n79.4\n51.8\n-\n44.7\n28.8\n48.0\n-\nSD v1.5 [35]\n-\n-\n-\n-\n-\n-\n9.22\n▶Unified VL understanding and generation Pre-trained Models\nGILL [18]\n-\n-\n-\n-\n-\n-\n12.2\nKosmos-G-1.9B [30]\n-\n-\n-\n-\n-\n-\n10.99\nSEED-OPT2.7B [11]\n119.0\n42.8\n28.8\n-\n-\n-\n-\nEmu [39]\n112.4\n52.0\n-\n38.2\n34.2\n47.4\n11.66\nEmu† [39]\n-\n52.9\n-\n42.8\n34.4\n47.8\n-\nVL-GPT\n116.4\n51.7\n34.6\n35.8\n34.7\n49.9\n12.25\nVL-GPT†\n119.2\n55.3\n38.1\n41.5\n35.2\n49.6\n-\n▶Unified VL understanding and generation Models with Instruction-tuning or Fine-tuning\nCM3Leon-7B [51]\n61.6\n47.6\n-\n23.8\n37.6\n22.6\n10.82\nEmu-I [39]\n-\n57.5\n-\n46.2\n38.1\n50.1\n-\nNExT-GPT§ [48]\n156.7\n-\n-\n-\n-\n-\n11.28\nDreamLLM-7B [9]\n115.4\n56.6\n-\n44.3\n38.1\n-\n8.46\nVL-GPT-I\n133.7\n67.2\n51.5\n50.3\n38.9\n51.8\n11.53\nTable 2. Evaluation comparison between our VL-GPT and other models.\n† denotes that the zero-shot prompt is built by sampling two\ntask-specific examples with their associated images removed. § represents that the dataset employed for instruction tuning is private.\nsational format using the template provided in the appendix.\nFor further details regarding preprocessing and construction\nof our training dataset, please refer to the appendix as well.\n4.2. Training setup\nTo efficiently train the image tokenizer-detokenizer frame-\nwork, the visual encoder in the image tokenizer and the dif-\nfusion decoder in the image detokenizer are initialized with\nCLIP-L image encoder [32] and IP-Adapter [49], respec-\ntively. Moreover, these two modules remain frozen through-\nout the entire training, and only the causal transformer and\nthe transformer decoder necessitate optimization. Unless\nspecified otherwise, the weight coefficients λ1 and λ2 in\nEq. 1 are assigned a value of 1.0 during both training and\nevaluation. The AdamW optimizer [26] is employed for\ntraining, with a learning rate of 2e-4 and a cosine schedule.\nThe framework is trained using a total batch size of 1024 on\n8 NVIDIA 40G-A100 GPUs for 10,000 iterations.\nFor the multimodal pre-training of our VL-GPT, the pre-\ntrained LLaMA 7B [40], its text tokenizer, and its text deto-\nkenizer are integrated with our trained image tokenizer and\ndetokenizer to establish the VL-GPT model with a total of\n7.5 billion parameters. LoRA [15] module is incorporated\ninto the LLaMA model, resulting in relatively low demand\nfor computational resources. AdamW optimizer is also uti-\nlized with a learning rate of 2e-4.\nThe multimodal pre-\ntraining is conducted with a batch size of 4096 on 32 GPUs\nfor 20,000 iterations. Instruction tuning is performed on the\npre-trained VL-GPT, adopting similar training settings used\nduring pre-training. LoRA is also employed, and the learn-\ning rate is reduced to 5e-5. The model is trained for 10,000\niterations with batch size of 512 on 4 GPUs. Additional\ntraining settings are included in the appendix.\n4.3. Image Tokenizer and Detokenizer Performance\nThe image tokenizer-detokenizer framework is designed to\nconvert images between pixel space and continuous visual\nembeddings.\nTo assess its effectiveness, we employ the\nmethod of calculating the CLIP similarity as the evaluation\nmetric for our framework, as implemented in SEED [11].\nAs demonstrated in Tab. 1, our framework achieves notably\nsuperior semantic consistency compared to SEED, which\nutilized quantized visual tokens.\nFurthermore, we present visualizations of the recon-\nstructed images generated by our framework in Fig. 3. By\nestimating both image condition embedding and text condi-\ntion embedding and utilizing them to guide the generation\nprocess of diffusion decoder, our image detokenizer is ca-\npable of generating images with high consistency in terms\nof spatial appearance and semantic information.\n4.4. Evaluation of our VL-GPT\nBenchmark Performance We first evaluate the zero-shot\nperformance of VL-GPT on a variety of vision-language\ntasks, including image captioning on MSCOCO [5], vi-\nsual question answering on VQAv2 [12], GQA [17],\nOKVQA [28], and VizWiz [13], visual dialog on Vis-\nDial [7], and text-to-image generation on MSCOCO. Com-\nprehensive details regarding these benchmarks and their\nmetrics can be found in the appendix. As results in Tab. 2\nindicate, VL-GPT achieves competitive performance on\n6\nA group of white daisies with \nyellow centers and green leaves \nin the background.\nIt  also wears a red scarf.\nPlease generate an image of a Labrador\ndog wearing glasses.\nThe office staff is not here, he may be taking a rest \nat the moment.\nA photo of \nthree giraffe. \nA photo of \none swan. \nA photo of \ntwo dogs. \nDscribe the image in \ndetail in one sentence. \nA stream flowing \nthrough a forest.\nA wedding cake \nwith pink roses \nand greenery.\nA yellow and \nblack football.\nA dog is standing in \nthe snow mountain.\nA modern architectural building \nwith large glass windows, situated \non a cliff overlooking a serene \nocean at sunset.\nA woman is eating \npizza in the park.\nA fire is burning in the \nmiddle of a blue sea.\nAn antique botanical illustration drawn with fine \nlines and a touch of watercolour whimsy, depicting \na strange lily crossed with a Venus flytrap, its \npetals poised as if ready to snap shut on any \nunsuspecting insects.\nAn image shows that a cat \nis sleeping on these plants.\nYes, there is a banana on the table.\nWhat does this picture shows as a whole?\nAre there any items on the table that are \nnot for office use?\nAre the people at the office here?\nChange the scarf to blue.\nQuestion: \nwhat is she \nplaying with?\nShort answer:\n(2) \n(3) \n(4) \n(5) \n(6) \n(7) \n(8) \n(9) \n(10) \n(11) \n(12) \nThe image depicts a spacious, well-lit office with a wooden \ndesk as its centerpiece. The desk is adorned with a variety of \nitems, including a computer monitor, a keyboard, a mouse, and \na phone. There is a chairs in the office, positioned near the \ndesk. \n(1) \nFigure 4. Illustrative samples of our VL-GPT across various vision and language understanding and generation tasks. These tasks en-\ncompass: (1)-(2) image captioning, (3) visual question answering (VQA), (4)-(8) text-to-image generation, (9)-(10) multimodal in-context\ngeneration, and (11)-(12) multimodal dialogue. Examples (1)-(10) are generated by our pre-trained VL-GPT, while (11)-(12) are produced\nby our instruction-tuned VL-GPT. Blue boxes represent multimodal inputs and yellow boxes indicate VL-GPT outputs.\n7\nModels\nVQAv2\nVizWiz\nk\n2\n4\n8\n2\n4\n8\nKosmos-1 [16]\n51.4\n51.8\n51.4\n31.4\n35.3\n39.0\nFlamingo-9B [1]\n-\n56.3\n58.0\n-\n34.9\n39.4\nEmu [39]\n56.4\n58.4\n59.0\n37.8\n41.3\n43.9\nVL-GPT\n57.2\n58.6\n58.9\n38.9\n41.8\n44.2\nTable 3. Few-shot performance on visual question answering.\nEstimation target\nReconstruction\nVL-GPT\nCLIP\nSimilarity (↑)\nCaptioning\nCIDEr (↑)\nGeneration\nFID (↓)\net\n73.59\n131.1\n12.79\nev\n80.05\n123.6\n13.61\net + ev\n80.22\n133.7\n12.25\nTable 4. Ablation of condition embedding types. Text embedding\n(et), image embedding (ev), or their combination (et + ev) are\nemployed to guide the training of the tokenizer-detokenizer frame-\nwork. We evaluate the effectiveness of reconstructing images and\nthe performance of VL-GPT when adopting different image tok-\nenizer and detokenizer.\nboth image-text understanding and text-to-image generation\ntasks, thereby validating the effectiveness of unified multi-\nmodal pre-training. Notably, VL-GPT attains an impres-\nsive CIDEr score of 116.4 or 119.2 on MSCOCO caption-\ning without or with text-only prompts, surpassing other uni-\nfied VL pre-trained models. With further instruction tuning,\nVL-GPT-I, the instruction-tuned VL-GPT, significantly en-\nhances model performance, achieving the best or near-best\nresults in all tasks.\nMultimodal In-context Learning Similar to the behav-\nior of LLMs, our VL-GPT can be prompted to address\nnew vision-language tasks when provided with a few mul-\ntimodal examples from training data composed in the mul-\ntimodal prompt. To quantitatively evaluate its multimodal\nin-context learning capability, we examine the few-shot per-\nformance of VL-GPT when varying the number of exam-\nples in the given prompt, as shown in Tab. 3. Our VL-GPT\noutperforms other works under almost all few-shot setting\n(k=2, 4, 8) on two datasets for the visual question answer-\ning task. Moreover, a positive correlation is observed be-\ntween the number of the examples in the given prompt and\nthe performance on these two datasets.\nQualitative Results Fig. 4 showcases a series of gener-\nated visualizations using our VL-GPT model, encompass-\ning various tasks such as image captioning, visual ques-\ntion answering, text-to-image generation, multimodal gen-\neration with in-context learning, and multimodal multi-turn\ndialogue. Intriguingly, VL-GPT demonstrates remarkable\ncapabilities that are not readily assessed through existing\nacademic benchmarks. For instance, in Fig. 4 (7-8), VL-\nGPT generates highly realistic images in response to long-\ntext prompts containing complex concepts. In Fig. 4 (10),\nVL-GPT exhibits the ability to generate images and texts in\na flexible manner, conditioned on the provided multimodal\ncontext. Fig. 4 (11-12) illustrates the multi-turn dialogue\ncapabilities of the instruction-tuned VL-GPT, wherein the\nmodel generates multimodal contents consistent with the\nexisting context based on user instructions. This suggests\nthe promising potential of the VL-GPT as a versatile and\neffective multimodal general assistant.\n4.5. Ablation Studies\nPrevious studies typically generate images by converting\ntheir output into text condition embedding for image dif-\nfusion models. In contrast, our detokenizer estimates both\ntext condition embedding and image condition embedding\nfrom visual continuous embeddings, as depicted in Sec. 3.1.\nThe advantage of this design will be discussed next.\nFig. 3 displays the images reconstructed by our\ntokenizer-detokenizer using different estimated condition\nembedding, i.e., only using image condition embedding,\nonly using text condition embedding, or using both. These\nexamples reveal that these two type of embedding comple-\nment each other: image embedding effectively preserve im-\nage appearance details while text embedding assists in im-\nage reconstruction, e.g., determining the number of people.\nAs evidenced in Tab. 4, although it is feasible to train im-\nage tokenizer-detokenizer framework by estimating solely\none type of condition embedding (when λ1=0 or λ2=0 in\nEq. 1), the simultaneous estimation of both types of con-\ndition embedding leads to optimal performance for both\nthe tokenizer-detokenizer framework and VL-GPT. We hy-\npothesize that estimating image condition embedding en-\nables our tokenizer to retain more pixel information from\nthe input image, which is beneficial for image reconstruc-\ntion. Meanwhile, estimating text condition embedding al-\nlows the visual embeddings to contain more high-level se-\nmantics, leading to improved performance in subsequent vi-\nsion and language tasks.\n5. Conclusion\nWe propose VL-GPT, a generative pre-trained transformer\nmodel for vision and language understanding and gen-\neration.\nThe model incorporates an innovative image\ntokenizer-detokenizer framework, enabling it to be pre-\ntrained on large-scale multimodal corpora with a unified\nauto-regressive objective.\nUpon completion of the pre-\ntraining, VL-GPT exhibits competitive performance across\nvarious academic benchmarks and manifests several appeal-\ning emergent capabilities. As for limitations, the effective-\nness of our method has not been verified through the scaling\nup of model parameters. We hope that our work will stimu-\nlate further exploration in the pursuit of general intelligence\nwithin the multimodal research community.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716–23736,\n2022.\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021.\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392–18402, 2023.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877–1901, 2020.\n[5] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015.\n[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning, 2020.\n[7] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos´e MF Moura, Devi Parikh, and Dhruv Ba-\ntra. Visual dialog. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 326–335,\n2017.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018.\n[9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[11] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023.\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904–6913, 2017.\n[13] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608–3617,\n2018.\n[14] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen\nChi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLan-\nguage models are general-purpose interfaces. arXiv preprint\narXiv:2206.06336, 2022.\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[16] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023.\n[17] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019.\n[18] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023.\n[19] Taku Kudo and John Richardson.\nSentencepiece:\nA\nsimple and language independent subword tokenizer and\ndetokenizer for neural text processing.\narXiv preprint\narXiv:1808.06226, 2018.\n[20] LAION. Laion-aesthetics. https:\/\/laion.ai\/blog\/\nlaion-coco\/.\n[21] Hugo Laurenc¸on, Lucile Saulnier, L´eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M. Rush, Douwe Kiela,\nMatthieu Cord, and Victor Sanh. Obelics: An open web-\nscale filtered dataset of interleaved image-text documents,\n2023.\n[22] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023.\n[23] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023.\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023.\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023.\n[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[27] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-\ntaghi, and Aniruddha Kembhavi.\nUnified-io: A unified\nmodel for vision, language, and multi-modal tasks. arXiv\npreprint arXiv:2206.08916, 2022.\n9\n[28] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE\/cvf conference on computer vision and pattern\nrecognition, pages 3195–3204, 2019.\n[29] OpenAI. Gpt-4 technical report, 2023.\n[30] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-g: Generating images\nin context with multimodal large language models. arXiv\npreprint arXiv:2310.02992, 2023.\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. 2018.\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021.\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485–5551, 2020.\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022.\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022.\n[36] Christoph Schuhmann, K¨opf, Andreas, Theo Coombes,\nRichard Vencu, Benjamin Trom, and Romain Beaumont.\nLaion coco:\n600m synthetic captions from laion2b-en.\nhttps:\/\/laion.ai\/blog\/laion-coco\/.\n[37] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556–2565, 2018.\n[38] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang.\nHugginggpt: Solving\nai tasks with chatgpt and its friends in huggingface. arXiv\npreprint arXiv:2303.17580, 2023.\n[39] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023.\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.\n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n[43] Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural\nmachine translation with byte-level subwords. In Proceed-\nings of the AAAI conference on artificial intelligence, pages\n9154–9160, 2020.\n[44] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In International Conference on Machine Learn-\ning, pages 23318–23340. PMLR, 2022.\n[45] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442, 2022.\n[46] Samuel Weinbach, Marco Bellagente, Constantin Eichen-\nberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Bj¨orn\nDeiseroth, Koen Oostermeijer, Hannah Teufel, and An-\ndres Felipe Cruz-Salinas. M-vader: A model for diffusion\nwith multimodal context. arXiv preprint arXiv:2212.02936,\n2022.\n[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023.\n[48] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. arXiv preprint\narXiv:2309.05519, 2023.\n[49] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023.\n[50] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models, 2022.\n[51] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023.\n[52] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,\nYumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,\nZhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and\nPengchuan Zhang. Florence: A new foundation model for\ncomputer vision, 2021.\n[53] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing.\narXiv preprint arXiv:2306.10012,\n2023.\n10\n[54] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022.\n[55] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up\nvisual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023.\n[56] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023.\n[57] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023.\n11\nDataset\nTask\nSplit\nMetric\nCOCOCap\nScene Description\ntest\nCIDEr (↑)\nVQAv2\nScene Understanding QA\ntest-dev\nVQA acc. (↑)\nGQA\nScene Understanding QA\ntest-dev\nVQA acc. (↑)\nOKVQA\nExternal Knowledge QA\nval\nVQA acc. (↑)\nVizWiz\nScene Understanding QA\ntest-dev\nVQA acc. (↑)\nVisDial\nImage Dialogue\nval\nNDCG (↑)\nCOCO\nText-to-Image Generation\nval test\nFID (↓)\nTable 5. Summary of the evaluation benchmarks.\n6. Training Details\n6.1. Training of image tokenizer and detokenizer\nDatasets.\nThe image-text pairs from CC3M, Laion-\nAestheics, and LAION-COCO are utilized for the train-\ning of our image tokenizer-detokenizer framework. Specif-\nically, CC3M dataset comprises 3.3 million image-text\npairs crawled from the Web. Both Laion-Aesthetics and\nLAION-COCO are subsets of the larger LAION-5B dataset.\nLAION-Aesthetics is characterized by its high aesthetic\nquality, while LAION-COCO is composed of images sam-\npled from LAION-5B and their corresponding captions\ngenerated by existing vision-language models, e.g., BLIP.\nDue to the efficient design of the framework, a relatively\nsmall subset of 10 million samples from these two datasets\nwas found to be sufficient for model convergence in our ex-\nperiments. Further exploration of experiments with larger\ndatasets remains a prospect for future research.\nDuring\nthe training process, data were randomly sampled from the\nmixture of these three datasets in a ratio proportional to their\nrespective sizes.\nOptimization. The visual encoder in the image tokenizer is\ninitialized with CLIP-L, while the diffusion decoder in the\nimage detokenizer incorporates the U-Net and VAE mod-\nules from IP-adapter Plus. These components remain frozen\nduring the training process. The causal transformer in the\nimage tokenizer and the transformer decoder in the image\ndecoder are constructed using the standard transformer de-\ncoder, which consisting of 12 transformer blocks with ran-\ndom initialization.\nEach block comprises a causal self-\nattention layer, a cross attention layer, and a multilayer per-\nception (MLP) layer. The causal attention layer plays a vi-\ntal role in capturing causal dependencies among 1D visual\ncontinual embeddings, which is proved to be effective for\nfurther modeling in large vision-language models like Emu\nand SEED. In all experiments, the number of visual embed-\nding N is set to 32 in our all experiments, and its dimension\nd is set to 4096. Image augmentation techniques employed\nin CLIP models are applied, which involve resizing the in-\nput image with its shorter side to 224 pixels and cropping\nthe image to a fixed size of 224×224 pixels.\n6.2. Pre-training of VL-GPT\nDatasets. In addition to the datasets utilized for training the\nimage tokenizer-detokenizer framework, publicly available\ninterleaved image-text data, i.e., MMC4 and OBELICS,\nare employed for the pre-training of our vision-language\ntransformer.\nDuring pre-training, multimodal sequences\nwith interleaved image-text data are obtained from these\ntwo datasets. For MMC4, the core split is used, and low-\nquality samples with a CLIP similarity between the image\nand its caption below 0.24 are filtered out. For OBELICS,\na sequence comprising 512 tokens is randomly sampled\nbased on the arrangement of image and text data within the\noriginal document. To augment the probability of procur-\ning sequences containing multiple images, single-image se-\nquences are discarded with a likelihood of 0.8. Throughout\nthe training process, these two datasets maintain equivalent\nsampling probabilities, as do the sampling probabilities for\ndatasets comprising image-text pairs and datasets contain-\ning interleaved image and text data.\nOptimization. The large vision-language model, VL-GPT,\nis constructed by integrating the pre-trained language model\nLLaMA 7B with our image tokenizer-detokenizer frame-\nwork. LoRA modules are attached to all linear layers in the\nvision-language transformer, with a LoRA rank of 32. An\nadditional linear head is employed as a separate regression\nhead to predict the subsequent visual continuous embedding\nfor the current embedding. During multimodal pre-training,\nonly the parameters in LoRA modules and the regression\nhead are tuned, while all parameters of pre-trained LLaMA,\nimage tokenizer, and image detokenizer remain frozen to re-\nduce training costs. The data augmentation techniques used\nin the previous stage are also utilized in this phase.\n6.3. Instruction tuning of VL-GPT\nDatasets. To align the VL-GPT model with human instruc-\ntions, multimodal instruction tuning is applied to the model\nusing a combination of publicly available datasets, such\nas LLAVA, SVIT, MSCOCO Caption, InstructPix2Pix, and\nMagicbrush. All dataset are restructured into a conversa-\ntional formulation, consisting of a system message followed\nby a single-turn or multi-turn conversation dialogue be-\ntween a user and an assistant. The system message and con-\nversational template employed in our method are presented\nin Tab. 6. Furthermore, the MSCOCO caption dataset is\nemployed for both image captioning task and image gen-\neration task by altering the order of the image and its cor-\nresponding caption. The InstructPix2Pix and Magicbrush\ndatasets are utilized for prompt-based image editing task.\nDuring instruction tuning, data in these datasets are sam-\npled to construct a batch of data for model optimization in\na ratio proportional to the dataset size.\nOptimization. Instruction tuning is carried out on the pre-\n12\nInput Images\nReconstruction (zv+zt)\nReconstruction (zv)\nReconstruction (zt)\nInput Images\nReconstruction (zv+zt)\nReconstruction (zv)\nReconstruction (zt)\nFigure 5. Reconstruction examples of our image tokenizer and detokenizer by employing different condition embedding.\nA small cactus \nwearing a straw hat \nand neon sunglasses \nin the Sahara desert.\nA silhouette of a grand piano \noverlooking a dusky cityscape \nviewed from a top-floor penthouse, \nrendered in the bold and vivid style \nof a vintage travel poster.\nA paper craft art depicting a girl giving her cat \na gentle hug. Both sit amidst potted plants, with \nthe cat purring contentedly while the girl smiles. \nThe scene is adorned with handcrafted paper \nflowers and leaves.\nA cat wearing \nsunglasses\nA woman in a red \ndress is dancing in \nthe park\nYellow flowers in a \nvase on the desk\nA bird is \nstanding in the \nsnow \nA fire is burning in \nthe middle of a blue \nsea\nA large house new\nthe lake under the \nsunlight\nA group of boys \nare playing \nfootball on the \ngreen grass\nA  cat is sitting in a \nblue car in the street\nA girl is playing the \npiano on the stage\nA cat with a \nscarf is sitting in \nthe snow\nA brown dog and a \nwhite cat are lying \non the couch\nFigure 6. Examples of text-to-image generation. Blue boxes denotes the text prompt, and yellow boxes represents the generated image.\n13\nSystem Message\nYou are a helpful, respectful and honest assistant.\nAlways\nanswer as helpfully as possible, while being safe.\nYour answers\nshould not include any harmful, unethical, racist, sexist, toxic,\ndangerous, or illegal content.\nPlease ensure that your responses\nare socially unbiased and positive in nature.\nConversation Template\nImage captioning\nUSER: Provide a brief description of the given image <image>\nASSISTANT: <caption>.\nImage generation\nUSER: Create an image that visually represents the description:\n<caption>.\nASSISTANT: Here’s the image:\n<image>\nImage editing\nUSER:<image> <editing prompt>.\nASSISTANT: Here is the edited\nimage:\n<image>\nTable 6. Summary of prompt templates employed in instruction tuning. The notation “<image>” will be replaced with the image data.\n“<caption>” and “<editing prompt>” will be substituted with the corresponding caption and editing instruction, respectively.\nModel\nTask\nTemplate\nVL-GPT\nImage captioning\n<image> Please describe this image in detail in one sentence.\nIt shows\nImage generation\nAn image of <caption>.\n[IMG]\nImage QA\n<image> Based on the image, <question>?\nShort answer:\nImage dialog\n<image> an image of <caption>.\nBased on the image, <question1>?\nShort\nanswer:\n<answer1>.\n· · · Based on the image, <questionn>?\nShort answer:\nVL-GPT-I\nImage captioning\nUSER: Provide a brief description of the given image.<image> ASSISTANT:\nImage generation\nUSER: Create an image that visually represents the description:\n<caption>.\nASSISTANT:\nImage QA\nUSER: answer the question with the shortest answer <question>?\nASSISTANT:\nImage dialog\nUSER: <image> ASSISTANT: an image of <caption>.\nUSER: <question1>?\nASSISTANT: <answer1>.\n· · · USER: <questionn>?\nASSISTANT:\nTable 7. Summary of the prompting template utilized during model evaluation.The terms “<image>” and “<caption>” shall be substituted\nwith the corresponding image and its caption. Additionally, the notations “<questioni> and “ <answeri>” will be replaced with the i-th\nquestion and answer pair in the dialogue. [IMG] denotes the special token indicating the start of visual continuous embeddings.\ntrained VL-GPT, with the training hyper-parameters primar-\nily following those used in the pre-training phase. As the\ntraining data for instruction tuning is significantly smaller\nthan that employed for pre-training, the batch size is set\nto a smaller number, i.e. 512, and only four GPUs are uti-\nlized. To prevent catastrophic forgetting of the pre-trained\nmodel, the model is optimized with a reduced learning rate.\nFurthermore, LoRA modules are applied in the transformer\nmodel, while all other parameters remain frozen.\n7. Evaluation Details\n7.1. Benchmarks\nTo evaluate the vision and language understanding and gen-\neration ability of VL-GPT, we evaluate it on a variety of\nbenchmarks, whose details and metrics are summarized in\nTab. 5. Specifically, the test sample from any benchmark is\nfirst packaged with a task-specific prompt template and then\ntokenized into an incomplete multimodal sequence. Then\nthe VL-GPT model and its instruction tuned version, VL-\nGPT-I, are required to complete the multimodal sequence\nin an auto-regressive and open-ended manner. Evaluation\nresults can be obtained by either using the official evalua-\ntion code or submitting our prediction on the official server.\nIt should be noted that not all results reported in Tab. 2\nare zero-shot evaluation; for instance, VL-GPT-I has been\ntrained on COCO Caption.\n7.2. Prompt Templates\nTo thoroughly capitalize on the knowledge acquired dur-\ning pre-training while generating outputs that adhere to the\nstyle of the benchmark under evaluation, we design task-\nspecific prompt templates for the VL-GPT and VL-GPT-I.\nThese templates are comprehensively outlined in Tab. 7.\n14\nA pair of horses \nwith red and \nwhite decorations \non their heads.\nA photo of a \nwoman holding a \ntennis racket\nA group of men \nsitting at desks \nwith computers.\nA photo of three \nmen playing \ncomputers.\nQuestion: what is \nhe doing? Answer: \nplaying computers \nA woman \nstanding next to a \ntable holding a \nsheet cake.\nA photo of a \nwoman is \ncooking.\nQuestion: what is \nshe doing? Answer: \ncooking \nA woman \nholding a tennis \nracket and \nstanding next to a \ntable.\nQuestion: what is \nshe doing? \nAnswer: playing\ntennis.\nAn image of \nthree elephants\nQuestion: how \nmany? answer: \nthree \nAn image of one \ndog\nAn adorable dot \nresting on a \nbeach near the \nocean. \nQuestion: how \nmany? answer: one \nAn image of two \nhorses.\nQuestion: how \nmany? answer: \ntwo. \nA group of \nelephants with one \nelephant raising \nit's trunk and dust \nand dirt coming up \nbehind it.\nThis is an animal. \nThis is a flower.\nThis animal is\nsurrounded with \nthis flower.\nFigure 7. Three groups of Multimodal In-context learning. In each group, partial content in the multimodal prompts is changed, and our\nVL-GPT can generate desired outputs. Blue boxes represent multimodal prompts and yellow boxes indicate VL-GPT outputs.\n8. Qualitative Cases\nAdditional reconstruction examples of our image tokenizer-\ndetokenizer framework are illustrated in Fig. 5. Further-\nmore, the qualitative examples presented in Fig. 6, Fig. 7,\nand Fig. 8 demonstrate the superior performance of VL-\nGPT in various tasks, including text-to-image generation,\nmultimodal in-context learning, and multimodal dialogue.\n15\nDescribe this image in one sentence.\nSure, here's a poem about the image: \nA wooden dock extends into the lake, \nA place to sit and watch the world go by. \nThe water is calm and clear, reflecting the trees, \nA serene place to sit and watch the world go by.\nWhen visiting this lake, you should be cautious about the \nweather conditions. The image shows a cloudy sky, which \ncould indicate that it might rain or become windy. It's \nimportant to dress appropriately and bring an umbrella or a \nraincoat if necessary.\nPlease describe this image in detail.\nWhat are the things I should be cautious \nabout when I visit here?\nTwo white puppies sit together on \nthe grass\nThe image features a serene lake scene with a wooden dock \nextending into the water. The dock is made of wood and is \nquite long, stretching across the entire width of the image. The \ndock is surrounded by calm, clear water, which reflects the \nsurrounding trees and sky.\nCan you write a poem about the image?\nGenerate image that they\nwith pink hair\nThis is the generated image\nChange to yellow glasses\nHere is the edited image\nFigure 8. Qualitative examples of multimodal dialogue by our instruction tuned VL-GPT. Blue boxes denotes the user instructions, and\nyellow boxes represents the assistant responses.\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation.pdf"}
{"title":"DreamLLM: Synergistic Multimodal Comprehension and Creation","authors":"Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi","summary":"This paper presents DreamLLM, a learning framework that first achieves\nversatile Multimodal Large Language Models (MLLMs) empowered with frequently\noverlooked synergy between multimodal comprehension and creation. DreamLLM\noperates on two fundamental principles. The first focuses on the generative\nmodeling of both language and image posteriors by direct sampling in the raw\nmultimodal space. This approach circumvents the limitations and information\nloss inherent to external feature extractors like CLIP, and a more thorough\nmultimodal understanding is obtained. Second, DreamLLM fosters the generation\nof raw, interleaved documents, modeling both text and image contents, along\nwith unstructured layouts. This allows DreamLLM to learn all conditional,\nmarginal, and joint multimodal distributions effectively. As a result, DreamLLM\nis the first MLLM capable of generating free-form interleaved content.\nComprehensive experiments highlight DreamLLM's superior performance as a\nzero-shot multimodal generalist, reaping from the enhanced learning synergy.\nProject page: https:\/\/dreamllm.github.io.","url":"http:\/\/arxiv.org\/abs\/2309.11499v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2309.11499v2","published":1695232685000,"comment":"ICLR 2024 (Spotlight)","pdf_text":"Published as a conference paper at ICLR 2024\nDREAMLLM: SYNERGISTIC MULTIMODAL\nCOMPREHENSION AND CREATION\nRunpei Dong 12⋆†\nChunrui Han 3⋆\nYuang Peng 4†\nZekun Qi 12†\nZheng Ge 3\nJinrong Yang 5†\nLiang Zhao 3\nJianjian Sun 3\nHongyu Zhou 3\nHaoran Wei 3\nXiangwen Kong 3\nXiangyu Zhang 3‡\nKaisheng Ma 4¶\nLi Yi 467¶‡\n1Xi’an Jiaotong University\n2Institute for Interdisciplinary Information Core Technology (IIISCT)\n3MEGVII Technology\n4Tsinghua University\n5HUST\n6Shanghai Artificial Intelligence Laboratory\n7Shanghai Qi Zhi Institute\nABSTRACT\nThis paper presents DREAMLLM, a learning framework that first achieves versatile\nMultimodal Large Language Models (MLLMs) empowered with frequently over-\nlooked synergy between multimodal comprehension and creation. DREAMLLM\noperates on two fundamental principles. The first focuses on the generative model-\ning of both language and image posteriors by direct sampling in the raw multimodal\nspace. This approach circumvents the limitations and information loss inherent to\nexternal feature extractors like CLIP, and a more thorough multimodal understand-\ning is obtained. Second, DREAMLLM fosters the generation of raw, interleaved\ndocuments, modeling both text and image contents, along with unstructured layouts.\nThis allows DREAMLLM to learn all conditional, marginal, and joint multimodal\ndistributions effectively. As a result, DREAMLLM is the first MLLM capable of\ngenerating free-form interleaved content. Comprehensive experiments highlight\nDREAMLLM’s superior performance as a zero-shot multimodal generalist, reaping\nfrom the enhanced learning synergy. Project page: dreamllm.github.io.\n1\nINTRODUCTION\n“What I cannot create, I do not understand.”\nRichard P. Feynman, on his blackboard at the time of his death, 1988\nContent comprehension and creation in multimodality are crucial and among the ultimate courses\nof machine intelligence (Sternberg, 1985; Legg & Hutter, 2007). To this end, Multimodal Large\nLanguage Models (MLLMs) (Alayrac et al., 2022; Hao et al., 2022; Huang et al., 2023) have emerged\nas extensions of the successful GPT-style Large Language Models (LLMs) (Brown et al., 2020; Zhang\net al., 2022; OpenAI, 2022; 2023a;b; Chen et al., 2023b; Touvron et al., 2023a;b) into visual realm.\nRecognized as foundation models (Bommasani et al., 2021), MLLMs have achieved unprecedented\nprogress in multimodal comprehension capabilities. These advanced models typically enhance LLMs\nby incorporating images as multimodal inputs, such as CLIP features (Radford et al., 2021), to\nfacilitate language-output multimodal comprehension. Their aim is to capture multimodal conditional\nor marginal distributions via a language posterior. However, multimodal creation, which involves\ngenerating images, texts, or both, necessitates a universal generative model that simultaneously learns\nlanguage and image posteriors—currently underexplored.\nUntil very recently, some concurrent works have shown success in conditional image generation using\nMLLMs (Koh et al., 2023; Sun et al., 2023b). As depicted in Fig. 1, these methods compel MLLMs\nto produce either discrete or continuous conditional embeddings that explicitly align with a pretrained\nCLIP encoder, which could later be used by a pretrained Stable Diffusion (SD) (Rombach et al., 2022)\nmodel for image generation. However, due to an inherent modality gap (Liang et al., 2022), CLIP\n⋆Equal contribution. †Work partially done during the internship at IIISCT and MEGVII. ‡Project leaders.\n¶Corresponding authors.\n1\narXiv:2309.11499v2  [cs.CV]  15 Mar 2024\nPublished as a conference paper at ICLR 2024\n(a) CLIP-like\nText\nImage\nContrastive \nAlignment\nText \nEncoder\nImage \nEncoder\nVisual\nEmbedding\nText \nEmbedding\n,\n:  Raw Image\n𝑰1\n,\n: Raw Text\n𝑻2\n𝑻1\n(b) Flamingo\/BLIP-like\nMLLM\n𝑰1\n𝑻1\n𝑻2\n𝑻1\nMLLM\n(d) DreamLLM (Ours)\n𝑰1\n𝑻2\nInference \nStream\n(c) GILL\/Emu-like\nMLLM\nor\nCLIP Alignment\n𝑰1\n𝑻2\n𝑻1\nCLIP\n𝑰2\n𝑰2\n𝑰2\n𝑰2\n𝑻1\n𝑻1\n𝑰2\n𝑻1\n𝑻1\nprojector\nFigure 1: Conceptual comparison of vision-language (VL) foundation models. (a) CLIP-like\nmodels (Radford et al., 2021; Yu et al., 2022a; Li et al., 2023e) take advantage of two towers\nthat explicitly align VL representations. (b) Flamingo\/BLIP-like models (Alayrac et al., 2022; Li\net al., 2022; 2023d; Huang et al., 2023) encode VL representations into a unified manifold space\nusing a singular MLLM. However, these models lack full autoregressivity, as they only output\nlanguage. (c) Concurrent MLLMs (Koh et al., 2023; Sun et al., 2023b) align visual outputs with CLIP\nrepresentations, but this alignment occurs in an intermediate space, not a raw data space. Consequently,\nmodels such as Emu necessitate a second-stage fine-tuning of Stable Diffusion (Rombach et al., 2022)\nfor raw image generation. These models also fall short in generating raw interleaved documents. (d)\nOur DREAMLLM, instead, generates raw language and image inputs in a unified auto-regressive\nmanner, inherently enabling interleaved generation. Only non-autoregressive generation loss is noted.\nsemantics focus predominantly on modality-shared information, often overlooking modality-specific\nknowledge that could enhance multimodal comprehension. Consequently, these studies have not fully\nrealized the potential learning synergy between multimodal creation and comprehension, have shown\nonly marginal improvements in creativity, and remain deficient in multimodal comprehension.\nIn this work, we introduce DREAMLLM, universally learning image and text posteriors with expected\ncreation & comprehension synergy, based on the following two de-facto designing principles:\ni. Generate Everything as It Is Different from existing works that generate intermediate image\nrepresentations like CLIP embeddings during training, DREAMLLM not only takes all modali-\nties raw data as inputs but also as outputs in a truly end-to-end fashion (i.e., outputs are identical\nto inputs, see Fig. 1). The challenge lies in enabling MLLMs to learn the image posterior\nwithout compromising their comprehension capabilities. To address this, we introduce dream\nqueries, a set of learnable embeddings that encapsulate the semantics encoded by MLLMs. This\napproach avoids altering the output space of MLLMs. Raw images are then decoded by the SD\nimage decoder conditioned on these semantics. In this fashion, the pretrained SD acts as the\nscore function (Ho et al., 2020). The image posterior is thus modeled by direct sampling in the\npixel space, facilitated by score distillation (van den Oord et al., 2018; Poole et al., 2023).\nii. Interleaved Generative Pre-Training (I-GPT) DREAMLLM is trained to generate interleaved\nmultimodal corpora from the internet (Zhu et al., 2023b), both encoding and decoding interleaved\nimage-text multimodal inputs. Unlike encoding multimodal inputs as in existing methods,\ndecoding interleaved multimodal outputs is challenging due to the complex interleaving layout\nstructures and the long-context requirement of images. Our approach tackles the interleaved\nlayout learning using a unique <dream> token that predicts the placement of images within texts.\nHarnessing DREAMLLM’s causal nature, all contents are generated with history multimodal\ncontexts of any length. This interleaved generative pretraining (I-GPT) inherently forms all\njoint, marginal, and conditional distributions of images and texts in the document, leading to a\nlearning synergy that grounds DREAMLLM’s comprehension in creation and vice versa.\nExtensive experiments across various vision-language comprehension, content creation, and language-\nonly tasks demonstrate DREAMLLM’s superior performance as a zero-shot multimodal generalist.\nFor instance, DREAMLLM-7B achieves an 8.46 FID on MS-COCO and sets a new standard with\n49.1\/35.9 scores on MMBench and MM-Vet evaluations, respectively. Moreover, we delve into\nthe learning synergy between comprehension and creation, revealing decent in-context generation\ncapabilities. With I-GPT pretraining, DREAMLLM generates interleaved documents following\nhuman prompts after supervised fine-tuning on instruction-following data curated with GPT-4. To\nour knowledge, this work is the first to enable MLLMs to create free-form interleaved content with a\nlearning synergy on both sides. As a foundational learning framework, DREAMLLM is adaptable\nacross all modalities, laying a promising foundation for future multimodal learning research.\n2\nPublished as a conference paper at ICLR 2024\n2\nBACKGROUND & PROBLEM STATEMENT\nAutoregressive Generative Modeling Given the joint probability distribution pθ(w) over a sequence\nw = {wt}T\nt=1 with length T, the canonical causal generation (Mikolov et al., 2010; Radford\net al., 2018; 2019) of every token wt by a θ-parameterized language model F is modeled as\npθ(w) = QT\nt=1 pθ(wt|w<t). For multimodal comprehension, the sequence could contain K ordered\nimages I = {Ik}K\nk=1 interleaved with words. The k-th image is processed as patch embeddings\nwith visual encoders Hϕ(·) like CLIP, which will then be encoded by a projector Mζ (e.g., a\nlinear layer (Huang et al., 2023) or DETR- (Carion et al., 2020)\/Perceiver-like (Jaegle et al., 2021)\nResampler (Alayrac et al., 2022)) into L-length visual embeddings Vk = {vℓ}L\nℓ=1. Let K(t) be the\nimage number before the t-th word token. The maximum likelihood estimation (MLE) is to minimize\nLMLLM(Θ = {θ, ζ}, w, I) := −Et\n\u0002\nlog pΘ(wt|w<t, V<K(t))\n\u0003\n, VK(t) = Mζ ◦Hϕ(IK(t)). (1)\nDiffusion Models\nDiffusion Models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) are\nprobabilistic generative models that learn the latent structure of data z = {zt}T\nt=1 through continuous-\nT-timestamps information diffusion. DMs involve a forward or diffusion process q that smoothly\nconverts data to Gaussian noise. Given the initial datapoint z1 ∼q(z1) and diffusion rate βt := 1−αt,\nthis process can be defined as a marginal distribution q(zt|z1) := N(√αtz1, βtI), and the perturbed\ndata distribution is q(zt) :=\nR\nq(zt|z)q(z)dz by integrating out data density q(z). A reversed\ndenoising probability flow p is used for generating data from noise zT ∼N(0, I) as a Markov Chain\nwith transition approximated by a Gaussian model pξ(zt−1|zt) := N(µξ(zt), σ2\nt I), which relates to\nan optimal MSE denoiser since q(zt−1|zt, z1) is Gaussian with enough timestamps (Feller, 1949;\nSohl-Dickstein et al., 2015). Ho et al. (2020) show that the optimization with the evidence lower\nbound (ELBO) can be simplified by training a denoising U-Net ϵξ(zt, t) parameterized with ξ that\nestimates the conditional expectation E[ϵ ∼N(0, I)|zt] (Bao et al., 2022). Let C be the conditional\nembeddings, and the perturbed data zt = √αtz1 + √1 −αtϵ, the minimization objective is\nLDM(ξ, z) := Et∼U(0,1),ϵ∼N(0,I)\n\u0002\n∥ϵξ(zt; C, t) −ϵ∥2\u0003\n.\n(2)\nSince ϵξ(zt; t) = −σtsξ(zt; t) as derived from Tweedie’s (Efron, 2011; Luo, 2022), it is equivalent\nto denoising score matching of ∇zt log pξ(zt) (Hyvärinen, 2005; Vincent, 2011), thus DMs are also\ncalled score-function based generative models (Song & Ermon, 2019; 2020; Song et al., 2021; 2023).\n2.1\nHOW CAN WE USE MLLMS FOR DIFFUSION SYNTHESIS THAT SYNERGIZES BOTH SIDES?\nMultimodal signals typically exhibit modality-specific information that has distinct structure but\ncomplementary semantics (Dong et al., 2023). This complementary property allows us to utilize deep\nlanguage comprehension to enhance cross-modal image generation (Saharia et al., 2022). However,\nthe potential of multimodal creation to improve comprehension remains largely unexplored.\nExisting strategies (Koh et al., 2023; Sun et al., 2023b; Ge et al., 2023) integrate successful Diffusion\nModels with MLLMs by aligning the semantic spaces of conditional embeddings between CLIP CCLIP\nand MLLMs CMLLM. The objective is to minimize alignment loss Lalign = D(Mψ ◦CMLLM, CCLIP),\nemploying a distance metric D(·, ·) and a condition projector Mψ. However, CLIP models primarily\nlearn modality-shared semantics, often overlooking modality-specific information due to a modality\ngap (Liang et al., 2022; Liu et al., 2023f). This explicit alignment with CLIP’s intermediate output\nspace may induce more conflicts than synergies, as MLLMs are forced to generate semantically\nreduced information, deviating from their original output space. To circumvent these issues, we\npropose alternative learning methodologies (See Fig. 2), which we elaborate in the ensuing sections.\nLearning Objective Our aim is to leverage MLLMs to model distributions via direct pixel space\nsampling. Here, the pretrained SD functions as a score metric, distilling the learned data distribution.\nThis approach is similar to Score Distillation Sampling (Poole et al., 2023) (SDS, also known as\nScore Jacobian Chaining (Wang et al., 2023a)). In this context, image posterior is learned in a\nDeepDream-like manner (Mordvintsev et al., 2015), using MLLMs’ conditional parameterization.\nConditional Embeddings Rather than converting the output space of MLLMs to align with CLIP,\nwe propose to query MLLMs using learned embeddings. Consequently, MLLMs-enriched semantics\nserve as diffusion conditioning, and the distribution is implicitly modeled through synthesis sampling.\n3\nPublished as a conference paper at ICLR 2024\nInterleaved Documents\n“I like my cute Siamese \ncat.”,\n“She has beautiful blue \neyes, and she likes to \nlie on her cozy \nnest.”, ...\ndream queries\n<s> I like my \ncute Siamese cat.\nWord \nEmbedding\nProjection\nScore \nDistillation\n<dream>\n<s> I like my \ncute Siamese cat.\nShe has \nbeautiful ... <\/s>\nVisual \nEncoder\nProjection\nvisual embeddings\nword \nembeddings\nspecial \n<dream> token\nU-Net\nShe has \nbeautiful ... <\/s>\nWord \nEmbedding\nCausal Multimodal Large Language Model (MLLM)\nInference Stream\nFigure 2: Overview of of our DREAMLLM framework. Interleaved documents serve as input,\ndecoded to produce outputs. Both text and images are encoded into sequential, discrete token\nembeddings for the MLLM input. A special <dream> token predicts where to generate images.\nSubsequently, a series of dream queries are fed into the MLLM, capturing holistic historical semantics.\nThe images are synthesized by the SD image decoder conditioned on queried semantics. The\nsynthesized images are then fed back into the MLLM for subsequent comprehension.\n3\nDREAMLLM\nWe introduce DREAMLLM, a universal learning framework that facilitates both MLLM’s compre-\nhension and creation capabilities. Our DREAMLLM is built with a causal decoder-only LLM Fθ as\nthe model foundation, i.e., Vicuna (Chiang et al., 2023) based on LLaMA (Touvron et al., 2023a)\ntrained on ShareGPT (Zheng et al., 2023). We adopt OpenAI’s CLIP-Large (Radford et al., 2021) as\nthe visual encoder Hϕ, followed by a linear layer Mζ for visual embedding projection. To synthesize\nimages, we use Stable Diffusion (SD) (Rombach et al., 2022) as the image decoder, and the condition\nprojector Mψ is also a linear layer. An overview of the architecture is depicted in Fig. 2.\n3.1\nEND-TO-END INTERLEAVED GENERATIVE PRETRAINING (I-GPT)\nAll natural documents can be regarded as carriers of text-image interleaved information. Text-only,\nimages-only, and text-image pairs data, on the other hand, can be seen as special cases of interleaved\ncorpora with different modality compositions. Thus, it is critical to empower the model with the\ncapability to learn and generate free-form interleaved documents that form all possible distributions.\nInterleaved Structure Learning To model the interleaved structure, the interleaved sequence is\noperated by extending a new special <dream> token before images. During training, DREAMLLM\nis trained to predict this <dream> token that indicates where an image emerges, and the conditional\nimage synthesis is performed afterward, as introduced next. During inference, DREAMLLM will\ngenerate an image on its “free will” when this token is predicted.\nConditional Synthesis through Score Distillation To avoid the possible conflicts of CLIP semantics\nand MLLMs stated in Sec. 2.1, we carefully design a different learning objective and conditional\nembeddings. Formally, we introduce a series of learnable dream queries with length Q: d = {dq}Q\nq=1.\nConsidering the t-th token is predicted as <dream> token, the conditional embeddings CDREAMLLM\nK(t)+1\nfor\nthe (K(t) + 1)-th image synthesis can be obtained by causally querying the previous sequences:\nCDREAMLLM\nK(t)+1\n:= Fθ(d, x<t+1, V<K(t)+1).\n(3)\nThus, the denoising score matching with latent z is motivated in the similar formulation to Eq. (2):\nLDREAMLLM\nDM\n(θ, d, ζ, ψ, z) := Et∼U(0,1),ϵ∼N(0,I)\n\u0002\n∥ϵξ(zt; CDREAMLLM, t) −ϵ∥2\u0003\n,\n(4)\nwhere ξ is not updated since the SD is frozen. Eq. (4) can also be viewed as a generalized formulation\nof textual inversion (Gal et al., 2023), but all condition embeddings are learnable by model-seeking.\nFrom the perspective of score distillation (van den Oord et al., 2018), the KL divergence defined by\nconditions and the pre-learned score function is equivalently minimized for distilling (Hinton et al.,\n4\nPublished as a conference paper at ICLR 2024\nTable 1: Zero-shot multimodal comprehension evaluation of image-to-text captioning, general\nVQA, text-related VQA, and comprehensive benchmarks. ∗denotes non-zero-shot results for VQA.\nDREAMLLM-7B∗is trained using the SFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nCaptioning\nVQA\nComprehensive\nCOCO\nI2Paragraph\nVQAv2\nOKVQA\nVizWiz\nTextVQA\nMMBench\nMM-Vet\nComprehension Only MLLMs\nMetaLM (Hao et al., 2022)\n-\n-\n41.1\n11.4\n-\n-\n-\n-\nKosmos-1 (Huang et al., 2023)\n-\n-\n51.0\n-\n29.2\n-\n-\n-\nFlamingo-9B (Alayrac et al., 2022)\n79.4\n-\n51.8\n44.7\n28.8\n-\n-\n-\nOF-9B (Awadalla et al., 2023)\n65.5\n-\n52.7\n37.8\n27.5\n29.1\n4.6\n21.8\nLLaVA-7B (Liu et al., 2023c)\n-\n-\n-\n-\n-\n28.9\n38.7\n23.8\nMLLMs for Comprehension & Creation\nCM3Leon-7B∗(Yu et al., 2023a)\n61.6\n10.5\n47.6\n23.8\n37.6\n-\n-\n-\nEmu-14B (Sun et al., 2023b)\n117.7\n-\n40.0\n34.7\n35.4\n-\n-\n-\nDREAMLLM-7B (Ours)\n115.4\n17.4\n56.6\n44.3\n45.8\n34.9\n49.9\n35.9\nDREAMLLM-7B∗(Ours)\n103.7\n8.4\n72.9\n52.2\n49.3\n41.8\n58.2\n36.6\n2015) learned probability density in conditional image synthesis:\nmin\nθ,d,ζ,ψ LDREAMLLM\nDM\n:= Et,CDREAMLLM\nh\nDKL\n\u0000q(zt−1|zt, z1, CDREAMLLM) ∥pξ(zt−1|zt)\n\u0001i\n.\n(5)\nUniversal Multimodal Generative Modeling An interleaved document sequence x = {xt}T\nt=1\ncontains both words w = {wi}N\ni=1 and images I = {Ik}K\nk=1. The autoregressive nature forms all\npossible conditional distributions, such as image conditional multimodal comprehension p(w|I)\nor text-to-image synthesis p(I|w). The images are processed as visual embeddings V for causal\ncomprehension. Assuming that the pretrained SD is an optimal score function, Eq. (5) thus could\nbe viewed as an MLE optimization for the synthesis posterior. Different from Eq. (1), the targeted\nsequence xt now could be both encoded images or words. The objective is thus unified to the MLE\nof all causally-conditioned posteriors in arbitrary forms:\nLDREAMLLM\nMLLM\n(Θ = {θ, d, ζ, ψ}, x) := −Et [log pΘ(xt|x<t)] .\n(6)\n3.2\nMODEL TRAINING\nIn this work, we consider a three-stage training procedure. It can be summarized as follows, and the\nimplementation details, like training data, can be found in Table 13 in Appendix C.\nI Alignment Training This stage is used to alleviate the gap in multimodality, facilitating the\nadaptation of multimodal inputs to LLMs. The linear visual projector, linear condition projector,\nand learnable dream embeddings are pretrained for cross-modal manifold alignment among frozen\nLLMs, visual encoder, and SD. We use approximately 30M image-text pairs data, training both\nimage-to-text comprehension and text-to-image synthesis.\nII I-GPT Pretraining Following alignment, the LLM undergoes an unfrozen process for I-GPT\npretraining (detailed in Sec. 3.1). This critical stage facilitates the learning of joint vision-language\ndistributions via generative modeling. Training incorporates approximately 2M selectively filtered\ndocuments from MMC4-Core (Zhu et al., 2023b), adhering to a CLIP score threshold of 0.25.\nFurthermore, we use 2M paired data samples from LAION400M (Schuhmann et al., 2021),\ncaptioned by BLIP (Li et al., 2022) (i.e., BLIP-LAION), to enhance text-to-image training and\npotentially mitigate the impact of some low-quality noisy images and texts from sMMC4.\nIII Supervised Fine-tuning This stage enables the model to perform general multimodal com-\nprehension and creative tasks following human instructions (Ouyang et al., 2022). We utilize\napproximately 80K visual instruction tuning data collected by Liu et al.. For instruction-following\ncontent creation, GPT-4 is prompted with document summaries or image captions, collecting\napproximately 20K instruction-following document synthesis from MMC4 (InstructMMC4) and\n20K image synthesis data from BLIP captioned LAION400M (Instruct-BLIP-LAION).\n4\nEXPERIMENTS\nDREAMLLM is a versatile multimodal generalist that excels at zero-shot or in-context vision-\nlanguage comprehension and synthesis tasks. In this section, we conduct systematic evaluations for\ndemonstration. See qualitative results in Appendix B and implementation details in Appendix C.\n5\nPublished as a conference paper at ICLR 2024\n4.1\nMULTIMODAL COMPREHENSION\nMultimodal comprehension enables humans to interact with agents conditioned on both words and\nvisual content. We evaluate the multimodal vision and language capabilities of DREAMLLM across\nseveral benchmarks, including image-to-text captioning on COCO (Karpathy & Fei-Fei, 2017) and\nImage2Paragraph (Krause et al., 2017), general visual question answering (VQA) on VQAv2 (Goyal\net al., 2019), OKVQA (Marino et al., 2019), VizWiz (Gurari et al., 2018), and text-related VQA on\nTextVQA (Singh et al., 2019). Additionally, we conducted a zero-shot evaluation on the recently\ndeveloped benchmarks of MMBench and MM-Vet to assess the model’s performance in complex\nmultimodal tasks. The results are presented in Table 1 (See Table 5, and Table 6 in Appendix A).\nAll metrics and data splits are listed in Table 14 in Appendix C. We find that i) DREAMLLM\noutperforms other MLLMs across all benchmarks. Notably, DREAMLLM-7B surpasses concurrent\nMLLMs with image synthesis capabilities by a significant margin, achieving +16.6 higher accuracy\non VQAv2 compared to Emu-13B. ii) On comprehensive benchmarks like MMBench and MM-Vet,\nDREAMLLM achieves state-of-the-art performance against all 7B counterparts. Detailed analysis\nrevealed superior spatial\/relation reasoning capabilities in DREAMLLM compared to other MLLMs,\nlikely a result of its image synthesis learning. See qualitative results and comparisons on multimodal\ndialogue in Table 11, Table 12, Fig. 10, Fig. 11, and Fig. 12, in Appendix B.\n4.2\nTEXT-CONDITIONAL IMAGE SYNTHESIS\nTable 2: Zero-shot text-to-image generation FID on\nMS-COCO LN-COCO. LM denotes language model\nbased methods, MG denotes multimodal generation meth-\nods, and FIG denotes free-form interleaved generation\nmethods. † is fine-tuned SDv2.1 on our state I data. ∗\ndenotes retrieval-augmentation (Sheynin et al., 2023). ▷\ndenotes results after stage I alignment training.\nMethod\nLM MG FIG MS-COCO LN-COCO\nText2Image Specialists\nRetrieval Result (Yu et al.)\n✗\n✗\n✗\n17.97\n33.59\nDALL-E (Ramesh et al.)\n✗\n✗\n✗\n∼28\n-\nCogView (Ding et al.)\n✗\n✗\n✗\n27.1\n-\nCogView2 (Ding et al.)\n✗\n✗\n✗\n24.0\n-\nSDv2.1 (Rombach et al.)\n✗\n✗\n✗\n12.43\n34.26\nSDv2.1† (Rombach et al.)\n✗\n✗\n✗\n11.91\n25.35\nGLIDE (Nichol et al.)\n✗\n✗\n✗\n12.24\n-\nMake-A-Scene (Gafni et al.)\n✗\n✗\n✗\n11.84\n-\nDALL-E 2 (Ramesh et al.)\n✗\n✗\n✗\n10.39\n-\nMuse-3B (Chang et al.)\n✓\n✗\n✗\n7.88\n-\nImagen-3.4B (Saharia et al.)\n✓\n✗\n✗\n7.27\n-\nParti-20B (Yu et al.)\n✓\n✗\n✗\n7.23\n15.97\nMultimodal Large Language Models\nCM3-13B (Aghajanyan et al.) ✓\n✓\n✗\n29.56\n-\nGILL-8B (Koh et al.)\n✓\n✓\n✗\n12.20\n-\nEmu-13B (Sun et al.)\n✓\n✓\n✗\n11.66\n-\nCM3Leon-7B∗(Yu et al.)\n✓\n✓\n✗\n10.82\n-\nDREAMLLM-7B▷(Ours)\n✓\n✓\n✓\n8.76\n22.42\nDREAMLLM-7B (Ours)\n✓\n✓\n✓\n8.46\n20.53\nText2Image is one of the most commonly\nused techniques for creative content gener-\nation that follows human’s fabulous imag-\ninations through free-form languages.\nWe assess text-conditional image synthe-\nsis on the MS-COCO validation set (Lin\net al., 2014) and LN-COCO, the COCO\nsubset of Localized Narratives (Pont-\nTuset et al., 2020),\nfollowing prior\nworks (Xu et al., 2018; Yu et al., 2022b).\nThe MS-COCO dataset primarily con-\ntains high-level image abstractions with\nshorter captions, whereas LN-COCO pro-\nvides more comprehensive image descrip-\ntions (Yu et al., 2022b). DREAMLLM\nsamples 8 images per text prompt on MS-\nCOCO by CLIP score ranking, following\nprevious works (Ramesh et al., 2022). On\nLN-COCO, DREAMLLM samples one\nimage per prompt without CLIP ranking\nsince the text is too long and exceeds the\nCLIP length limit. Note that Parti sam-\nples 16 images per prompt with CoCa (Yu\net al., 2022a). Our evaluation metric is\nthe zero-shot Fréchet Inception Distance\n(FID) (Heusel et al., 2017), the results of\nwhich are presented in Table 2. We note three key observations: i) Our DREAMLLM shows a signifi-\ncant FID improvement over the StableDiffusion baseline after stage-I alignment, reducing the score\nby 3.67 and 11.83 on MS-COCO and LN-COCO, respectively. Further, FID improvements of 3.97\nand 13.73 are achieved after pretraining and supervised fine-tuning. The substantial improvement on\nLN-COCO underscores DREAMLLM’s superior capability in processing long-context information.\nii) When compared to prior specialist models, DREAMLLM delivers competitive results based on\nthe SD image decoder. iii) DREAMLLM consistently outperforms concurrent MLLMs-based image\nsynthesis methods. For instance, DREAMLLM-7B surpasses Emu-13B by a significant 3.20 FID on\nMS-COCO. See qualitative results on text-to-image synthesis in Fig. 13 and Fig. 14 in Appendix B.\n4.3\nMULTIMODAL JOINT CREATION & COMPREHENSION\nFree-form Interleaved Document Creation\nLeveraging the interleaved generative modeling\nfrom I-GPT, DREAMLLM can now generate interleaved documents in a free-form manner. In\n6\nPublished as a conference paper at ICLR 2024\nHelp me write the introduction and illustrations about Vienna city, show several high-quality photographs\nabout the beautiful scenes and buildings such as Vienna International Opera House.\nThe Austrian capital, Vienna, has long been known as the City\nof Dreams, and there is no better place to experience than this\nromantic city during the holiday season. The city’s historic\ncenter is surrounded by a collection of beautiful buildings on\na\nsquare\nmile.\nStroll\nalong\nthe\nfamous\nRing\nBoulevard\nand\nadmire the architecture. Some of the buildings date back to the\n13th century. The Vienna State Opera House is one of the most\nbeautiful buildings. Founded in 1869, the opera house is the\nImagine you are a robot agent in the house. There is a mobile phone on the dining table in the dining\nroom, and you are in the living room. Write a short article about how would you walk through the house to\ngrab the mobile phone with your\nrobotic arm step by step. Include several high-quality photographs to\nillustrate your procedure.\nAs a robot agent in the house, I would walk through the house to grab the mobile\nphone in the dining room as shown in the image below. <dream_1>\nFirst, I would look around the living room to make sure the area is safe and there\nare no obstacles or people that I need to avoid. I would then walk towards the dining\nroom and stop before entering the room.\nNext, I would look for the mobile phone on the table, as shown in the image below.\n<dream_2>\nOnce I have identified the phone, I would then move closer to\nit and extend my robot arm to grab it, as shown in the image\nbelow. <dream_3>\nOnce the phone is grabbed, I would then bring it back to the\nliving room and place it on the table where it belongs.\nhome\nof\nthe\nVienna\nState\nOpera\nand\nthe\nannual\nVienna\nOpera\nBall.\n<dream_1>\nCosmopolitan\nflair,\nan\neclectic mix of architectural styles and a rich cultural heritage come together in Vienna. The city is\nhome to a number of museums, palaces and churches. The Sacher Torte and the Wiener Schnitzel are local\nspecialties. A visit to the romantic Christmas Market in Vienna is also a unique experience. <dream_2>\n<dream_1>\n<dream_2>\nDreamLLM\nDreamLLM\n<dream_1>\n<dream_3>\n<dream_2>\nDreamLLM\nDreamLLM\nDreamLLM\nHuman Instruction\nDreamLLM Generation\nFigure 3: Selected DREAMLLM instruction following interleaved content creation examples.\nNote that each image is created automatically at the location decided by DREAMLLM, and then it\nwill be fed back as multimodal comprehension input for the following content generation.\nFig. 3, we showcase the generated interleaved contents based on human instructions. It demonstrates\nthat: i) DREAMLLM can generate meaningful content per the instructions. ii) The system can\nautonomously create images at any specified location by predicting the proposed <dream> tokens,\nthereby eliminating the need for additional human intervention. This is a more user-friendly approach\ncompared to systems like Emu, which necessitate human input for image generation locations.\nImage Quality Document quality can be influenced by factors such as text content, image quality\n(including image-text alignment), and illustration positioning. To assess the quality of generated\ndocuments, we utilized a held-out instruction-following subset from the constructed InstrcutMMC4\nas a demonstrative tool. This subset comprises 15K documents across 30 MMC4-defined topics, with\n500 samples per topic. We began by evaluating image quality using FID on this subset, generating\neach image based on the corresponding ground truth texts. The results revealed that when using\nonly matched text inputs for image synthesis, SD achieved an FID score of 74.77. In contrast, our\nDREAMLLM significantly outperforms SD with an FID score of 36.62.\nHuman Evaluation\nWe perform a comprehensive human evaluation to assess the quality of the\ngenerated samples. We randomly selected 150 samples (5 per topic) for instruction-following docu-\nment generation, mixing the generated and ground truth MMC4 documents without any identifying\ninformation. Five unbiased volunteers were then asked to determine whether the given samples were\nsupported. Given the presence of duplicate and low-quality images in MMC4, the supportive rate for\nMMC4 was only 77.24%. In contrast, our DREAMLLM model achieves a supportive rate of 60.68%,\nsurpassing the 30% Turing test requirement. This result indicates that the generated documents\ncontain high-quality images placed logically, demonstrating the effectiveness of our model.\n7\nPublished as a conference paper at ICLR 2024\n5\nDISCUSSIONS\n5.1\nSYNERGY BETWEEN CREATION & COMPREHENSION?\nTable 3: Concrete analysis of the synergy between mul-\ntimodal comprehension and creation (image synthesis).\nID denotes whether the interleaved dataset is used during\nthe second stage of pretraining.\nID\nLalign\nMM-Vet\nVQAv2\nCOCO\n0\nStable Diffusion\n✗\n-\n-\n-\n12.43\n1\nCreation-only\n✗\n✗\n-\n-\n8.50\n2\nCreation-only\n✓\n✗\n-\n-\n8.57\n3\nComprehension-only\n✗\n✗\n31.0\n55.1\n-\n4\nComprehension-only\n✓\n✗\n34.4\n54.3\n-\n5\nJoint-learning\n✓\n✗\n35.9\n56.6\n8.46\n6\nJoint-learning\n✓\n✓\nN\/A\nN\/A\nN\/A\nTo elucidate the synergy between multi-\nmodal creation and comprehension, we\nmake the comparison among three meth-\nods with DREAMLLM architecture, each\nutilizing identical training data yet differ-\ning in their learning objectives: a) the\nCreation-only baseline, focused solely on\ntext\/document-conditional image synthe-\nsis; b) the Comprehension-only baseline,\ndedicated to word generation exclusively;\nc) the Joint-learning method, which is the\ndefault setting of DREAMLLM learning\nboth image and language modeling.\nQuantitative Analysis\nAs per Table 3, the following observations are made: i) The powerful\nlanguage comprehension of LLMs significantly enhances the performance of text-to-image specialists\nlike SD, as evidenced by the impressive 8.50 FID (line 1). ii) The use of interleaved data, such as\nMMC4, can potentially boost multimodal comprehension performance (line 4). iii) The proposed\nI-GPT further synergizes comprehension and creation with improved performance (line 5). iv) When\nincorporating CLIP alignment loss Lalign stated in Section 2.1, our DREAMLLM fails to converge but\nrather ends in a collapsing point (line 6). This indicates that the queries are adaptively learning the\ntrue data distributions, where CLIP semantics are in conflict with MLLM-encoded semantics.\nQuestion: Which animal in\nthe\npicture\nis\nlarger,\nthe cat or the dog?\nAnswer A: The dog in the\npicture\nis\nlarger\nthan\nthe cat.\nAnswer B: The cat in the\npicture\nis\nlarger\nthan\nthe dog.\nQuestion: What's the difference between these two animals?\nAnswer A: In Photo A, there is a large bear sitting on a tree branch,\nleaning against it, and looking over its shoulder. The bear appears to be\na sleeping or resting pose. In Photo B, there is an image of a koala that\nis sitting on a tree branch and leaning over to one side. The koala is in\na different posture compared to the bear in Photo A; it is not asleep or\nresting,\nbut\nrather\nsitting\nand\nleaning.\nThe\nmain\ndifference\nbetween\nthese two animals is their body position and the fact that the bear is\nmuch larger than the koala.\nAnswer B: In the two images, A is a bear and B is a koala. The difference lies\nin\ntheir\nphysical\ncharacteristics,\nhabitat,\nand\ndiet.\nBears\nare\nlarge,\nomnivorous mammals known for their strength and ability to climb trees. They\nhave a distinctive appearance with a large body, short limbs, and a broadsnout.\nBears\nare\nfound\nin\nvarious\nhabitats,\nwith\nsome\nspecies\nliving\nin\nforests,\nmountains, or grasslands. Koalas, on the other hand, are small marsupials native\nto Australia, known for their unique appearance with a fluffy, teddy-bear-like\nfur, a short tail, and large, round ears. Koalas primarily feed on the leaves of\neucalyptus trees and have a specialized dental structure to efficiently process\nthe fibrous plant material. These differences in physical features, habitat, and\ndiet highlight the distinct characteristics of each animal.\nPhoto A:\nPhoto B:\nPhoto:\nFigure 4: Qualitative comparison. Answer A: answer\nfrom comprehension-only models w\/o interleaved train-\ning; Answer B: answer from joint-learning models.\nQualitative Analysis\nIn Fig. 4, we\ncompare answers to some examplar VQA\ntasks from comprehension-only and joint\nlearning modules, respectively. It can be\nseen that: i) The joint-learning method\nexhibits superior multimodal comprehen-\nsion, particularly in identifying subject re-\nlationships and attributes like object size.\nii) In multimodal comprehension scenar-\nios involving multiple image inputs, the\njoint-learning approach demonstrates en-\nhanced precision. This improved perfor-\nmance is a natural outcome of I-GPT pretraining, allowing better modeling of multimodal correlations\nin various interleaved documents.\nMultimodal In-Context Generation Multimodal in-context generation is a critical emerging capa-\nbility for MLLMs (Bommasani et al., 2021; Alayrac et al., 2022). While significant strides have been\nmade in in-context visual question answering, in-context image synthesis remains relatively lacking\nin exploration. The multimodal context-conditional image synthesis capabilities of DREAMLLM, as\ndemonstrated in Fig. 5, offer promising insights into this domain. Tasks such as in-context image\nedition, subject-driven image generation, and compositional generation, however, pose significant\nTeddy bear\nMultimodal Input\nDelicious food.\nX.\nDreamLLM\nswims in water\nMultimodal Input\non the beach\nA black dog.\nA dog X.\nin the snow\nlies on sofa\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\n(a) In-context Image Edition\n(b) In-context Subject-Driven Generation \nDreamLLM\nDreamLLM\n(c) In-context Compositional Generation\nA bear.\nA pod of dolphins leaping out of the water in \nan ocean, with a ship on the background.\nA salmon. X.\nA grizzly bear catching a salmon in a \ncrystal clear river surrounded by a forest\nDreamLLM\nMultimodal Input\nDreamLLM\nSD\nA ship on the ocean. X.\nDreamLLM\nFigure 5: Selected DREAMLLM in-context image generation examples. The X in multimodal\ninputs are replaced accordingly by the text prompts shown under the generated images. We show the\nresults of the SD baseline in (c) with only the text prompt X for a comparison.\n8\nPublished as a conference paper at ICLR 2024\nchallenges in a zero-shot setting, particularly without downstream fine-tuning as in DreamBooth (Ruiz\net al., 2023) or attention modification techniques as in Prompt2Prompt (Hertz et al., 2023). Despite\nthese hurdles, Fig. 5 illustrates DREAMLLM’s ability to generate images conditioned on the provided\nimage context. This capability suggests promising potential for DREAMLLM in maintaining subject,\nidentity, and semantic context, thereby paving a new way for resolving these complex tasks.\n5.2\nWHAT IS LEARNED BY DREAMLLM?\nA cat and \na glass \nof whisky.\nA polar \nbear in \nthe forest.\n(b)\nimage\ncross-attention maps of \ndream queries & U-Net latent\ncross-attention maps of \ndream queries & U-Net latent\nimage\n(a)\nFigure 6: Cross-attention of dream queries and the dif-\nfusion U-Net latent. Similar to (Hertz et al., 2023), the\n64 queries can be viewed as 64 “words”. Each attention\nmap is computed as the cross-attention between each\nquery and the latent feature in the U-Net. The 64 queries\nare ordered as 8×8 grid sequentially, and each attention\nmap is the result averaged across all timestamps.\nDream Query Attention\nIn DREAM-\nLLM, the conditional embedding is de-\nrived from MLLMs with some learned\ndream queries. Fig. 6 demonstrates a vi-\nsualization of the learned cross-attention\nmechanism between these queries and the\ndiffusion latent. Similar to (Hertz et al.,\n2023), we visualize the attention map av-\neraged across all timestamps. It is seen\nthat: i) The query attention is structured,\ndisentangled, and semantically-oriented.\nThis is evidenced by the fact that distinct\nqueries adeptly capture different subject\nand background semantics. ii) Despite\nvarying prompts, attention patterns exhibit\nremarkable similarity as shown in Fig. 6\n(a) and (b). This contrasts with the token\nattentions from the original SD, which are typically text-token dependent. We postulate that this\narises from the model’s causal nature, leading to a consistent semantic structure order.\n6\nRELATED WORKS\nRapid developments have been witnessed in extending LLMs like LLaMA (Touvron et al., 2023a) to\nmultimodal comprehension that enables human interaction with both words and visual content. One\nline of work is built by system integration of LLMs with various functioning agents where language\nacts as general interface (Wu et al., 2023; Gupta & Kembhavi, 2023; Yang et al., 2023b; Liang\net al., 2023; Shen et al., 2023; Yang et al., 2023a; Surís et al., 2023), and remarkable success has\nbeen demonstrated in such plugin-style frameworks. Another line of work instead explores training\nLLMs to consume and understand multimodal inputs (Hao et al., 2022; Huang et al., 2023; Chen\net al., 2023b) with parameter-efficient tuning (Hu et al., 2022; Alayrac et al., 2022; Li et al., 2023d;\nZhang et al., 2023e; Zhu et al., 2023a; Ye et al., 2023) and instruction tuning (Xu et al., 2023b;\nLiu et al., 2023c; Dai et al., 2023a). More recently, some approaches have been developed towards\nvisual-interactive multimodal comprehension by precise referring instruction tuning (Zhao et al.,\n2023a; Peng et al., 2023; Chen et al., 2023a; Zhang et al., 2023g). For cross-modal creation, early\nworks generally tokenize the visual contents into discrete VQ codebooks (van den Oord et al., 2017;\nWang et al., 2022; Sun et al., 2022; Lu et al., 2023; Diao et al., 2023; Yu et al., 2023a). Recent works\ninstead explore incorporating MLLMs for image synthesis using text-to-image models such as Stable\nDiffusion, and the objective is to generate conditional embeddings that align pretrained CLIP text\n(i.e., CLIP) or CLIP variant embeddings (Koh et al., 2023; Ge et al., 2023; Sun et al., 2023a;b).\n7\nCONCLUSIONS\nHow can the learning synergy between multimodal content understanding and creation emerge? In\nthis paper, we present DREAMLLM, a learning framework for developing MLLMs that not only\ncomprehends but also creates multimodal content via diffusion models. Through score distillation of\nconditional-image synthesis distributions, we avoid the need for intermediate representation targets\nthat may bring information loss. The employment of interleaved documents further enriches the\nmultimodal distributions, fostering the learning of multimodal encoding and decoding. Our extensive\nempirical evaluations across diverse VL benchmarks demonstrate the effectiveness of DREAMLLM\nand the emerging learning synergy between multimodal content understanding and creation. Besides,\nthis work initiates the first step towards free-form interleaved content creation. As a general learning\nframework, we hope it will spur further research in the multimodal machine learning field.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENT\nThis research is supported by the National Natural Science Foundation of China (20211710187).\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,\nMandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model\nof the internet. CoRR, abs\/2201.07520, 2022. 6\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao\nGong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida\nNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman,\nand Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2022. 1, 2, 3, 5, 8, 9, 24\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and\nDevi Parikh. VQA: visual question answering. In Int. Conf. Comput. Vis. (ICCV), 2015. 30\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco,\nMitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large\nautoregressive vision-language models. CoRR, abs\/2308.01390, 2023. 5, 22, 23, 26, 27, 28\nFan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse\nvariance in diffusion probabilistic models. In Int. Conf. Learn. Represent. (ICLR), 2022. 3\nJames Betker, Goh Gabriel, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,\nJoyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh.\nImproving image generation with better captions. 2023. 26, 31\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical\ncommonsense in natural language. In AAAI Conf. Artif. Intell. (AAAI), 2020. 22, 30\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle\nHsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff\nKeeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi,\nand et al. On the opportunities and risks of foundation models. CoRR, abs\/2108.07258, 2021. 1, 8\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2020. 1, 22, 30\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In Eur. Conf. Comput. Vis. (ECCV), 2020. 3\nHuiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, José Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin\nMurphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image\ngeneration via masked generative transformers. In Int. Conf. Mach. Learn. (ICML), 2023. 6\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based\nsemantic guidance for text-to-image diffusion models. ACM Trans. Graph., 42(4):148:1–148:10, 2023. 31\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm’s referential dialogue magic. CoRR, abs\/2306.15195, 2023a. 9\n10\nPublished as a conference paper at ICLR 2024\nXi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong,\nHassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, and Weicheng Kuo. Pali:\nA jointly-scaled multilingual language-image model. In Int. Conf. Learn. Represent. (ICLR), 2023b. 1, 9\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impress-\ning gpt-4 with 90%* chatgpt quality, March 2023. URL https:\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\n4, 22, 26, 29, 31\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR,\nabs\/2204.02311, 2022. 22, 30\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. In Adv. Neural Inform. Process. Syst. (NIPS), 2017. 30\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolq: Exploring the surprising difficulty of natural yes\/no questions. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers),\n2019. 22, 30\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,\nPascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023a. 9, 22, 23\nWenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object\nhallucination in vision-language pre-training. In Proceedings of the 17th Conference of the European Chapter\nof the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, 2023b. 23\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-\nefficient exact attention with IO-awareness. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 29\nShizhe Diao, Wangchunshu Zhou, Xinsong Zhang, and Jiawei Wang. Write and paint: Generative vision-\nlanguage models are unified modal learners. In Int. Conf. Learn. Represent. (ICLR), 2023. 9\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In Adv. Neural\nInform. Process. Syst. (NeurIPS), 2021. 6\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via\nhierarchical transformers. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 6\nRunpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, and Kaisheng Ma. Finding the task-optimal low-bit\nsub-distribution in deep neural networks. In Int. Conf. Mach. Learn. (ICML), 2022. 25\nRunpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma.\nAutoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?\nIn Int. Conf. Learn. Represent. (ICLR), 2023. 3, 32\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang,\nYu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke,\nLucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling\nof language models with mixture-of-experts. In Int. Conf. Mach. Learn. (ICML), 2022. 30\nBradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical Association, 106(496):\n1602–1614, 2011. 3\n11\nPublished as a conference paper at ICLR 2024\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity. J. Mach. Learn. Res. (JMLR), 23:120:1–120:39, 2022. 30\nWilliam Feller. On the theory of stochastic processes, with particular reference to applications. In Proceedings\nof the [First] Berkeley Symposium on Mathematical Statistics and Probability. The Regents of the University\nof California, 1949. 3\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional\ntext-to-image synthesis. In Int. Conf. Learn. Represent. (ICLR), 2023. 31\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In Eur. Conf. Comput. Vis. (ECCV), 2022. 6\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or.\nAn image is worth one word: Personalizing text-to-image generation using textual inversion. In Int. Conf.\nLearn. Represent. (ICLR), 2023. 4, 25\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter V2: parameter-efficient visual instruction model.\nCoRR, abs\/2304.15010, 2023. 22, 23\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language\nmodel. CoRR, abs\/2307.08041, 2023. 3, 9\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and\nIshan Misra. Imagebind one embedding space to bind them all. In IEEE\/CVF Conf. Comput. Vis. Pattern\nRecog. (CVPR), 2023. 32\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,\nPing Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. CoRR,\nabs\/2305.04790, 2023. 22, 23\nYash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe V in VQA matter: Elevating the role of image understanding in visual question answering. Int. J. Comput.\nVis. (IJCV), 127(4):398–414, 2019. 6, 30\nAnisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language\nmodels. CoRR, abs\/2308.06394, 2023. 23\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training.\nIn IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 9\nDanna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P.\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In IEEE\/CVF Conf. Comput.\nVis. Pattern Recog. (CVPR), 2018. 6, 30\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. CoRR, abs\/2206.06336, 2022. 1, 5, 9, 22\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. In Int. Conf. Learn. Represent. (ICLR), 2021. 22, 30\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt\nimage editing with cross-attention control. In Int. Conf. Learn. Represent. (ICLR), 2023. 9\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst.\n(NIPS), 2017. 6, 30\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR,\nabs\/1503.02531, 2015. 4\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep\nGenerative Models and Downstream Applications, 2021. 30\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2020. 2, 3\n12\nPublished as a conference paper at ICLR 2024\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma,\nBen Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video\ngeneration with diffusion models. CoRR, abs\/2210.02303, 2022a. 31\nJonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet.\nVideo diffusion models. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022b. 31\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie\nMillican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models.\nCoRR, abs\/2203.15556, 2022. 30\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm:\nInjecting the 3d world into large language models. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 32\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. In Int. Conf. Learn. Represent. (ICLR), 2022. 9\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav\nChaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with\nlanguage models. CoRR, abs\/2302.14045, 2023. 1, 2, 3, 5, 9, 22, 24\nHuggingface.\nTransformers agent,\n2023.\nURL https:\/\/huggingface.co\/docs\/transformers\/\ntransformers_agents. Accessed: 2023-07-20. 23\nAapo Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res.\n(JMLR), 6:695–709, 2005. 3\nBrian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel\nHo, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina\nParada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng\nXu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu,\nDiego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng\nKuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana\nGopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as I can, not as I say: Grounding\nlanguage in robotic affordances. In Annu. Conf. Robot. Learn. (CoRL), 2022. 32\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional\nadversarial networks. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 31\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and João Carreira. Perceiver:\nGeneral perception with iterative attention. In Int. Conf. Mach. Learn. (ICML), 2021. 3\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision.\nIn Int. Conf. Mach. Learn. (ICML), 2021. 31\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs\/2001.08361,\n2020. 30, 31\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE\nTrans. Pattern Anal. Mach. Intell. (TPAMI), 39(4):664–676, 2017. 6, 30\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. In\nAdv. Neural Inform. Process. Syst. (NeurIPS), 2023. 1, 2, 3, 6, 9\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model\nfor audio synthesis. In Int. Conf. Learn. Represent. (ICLR), 2021. 32\nJonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for generating\ndescriptive image paragraphs. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 6, 30\nShane Legg and Marcus Hutter. Universal intelligence: A definition of machine intelligence. Minds Mach., 17\n(4):391–444, 2007. 1\n13\nPublished as a conference paper at ICLR 2024\nAlexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is\nsecretly a zero-shot classifier. In Int. Conf. Comput. Vis. (ICCV), 2023a. 32\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. CoRR, abs\/2305.03726, 2023b. 23\nDongxu Li, Junnan Li, and Steven C. H. Hoi. Blip-diffusion: Pre-trained subject representation for controllable\ntext-to-image generation and editing. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023c. 25\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In Int. Conf. Mach. Learn. (ICML), 2022. 2, 5, 26,\n29\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. In Int. Conf. Mach. Learn. (ICML), 2023d. 2,\n9, 23, 27, 28\nYanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image\npre-training via masking. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023e. 2\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. CoRR, abs\/2305.10355, 2023f. 23, 30\nLong Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding\nof text-to-image diffusion models with large language models. CoRR, abs\/2305.13655, 2023. 31\nWeixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y. Zou. Mind the gap: Understanding\nthe modality gap in multi-modal contrastive representation learning. In Adv. Neural Inform. Process. Syst.\n(NeurIPS), 2022. 1, 3\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang\nMao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting\nfoundation models with millions of apis. CoRR, abs\/2303.16434, 2023. 9\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja\nFidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE\/CVF\nConf. Comput. Vis. Pattern Recog. (CVPR), 2023. 31\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in context. In Eur. Conf. Comput. Vis. (ECCV),\n2014. 6, 30\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal\nmodel with robust instruction tuning. CoRR, abs\/2306.14565, 2023a. 23\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.\nCoRR, abs\/2310.03744, 2023b. 5, 22, 23, 24, 29\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2023c. 5, 9, 22, 23, 26, 27, 28, 29\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Int. Conf. Comput. Vis. (ICCV), 2023d. 31, 32\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around\nplayer? CoRR, abs\/2307.06281, 2023e. 22, 30\nZhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. ISS: image as stepping stone for text-guided\n3d shape generation. In Int. Conf. Learn. Represent. (ICLR), 2023f. 3\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic repre-\nsentations for vision-and-language tasks. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume 32, 2019.\n29\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. UNIFIED-IO: A\nunified model for vision, language, and multi-modal tasks. In Int. Conf. Learn. Represent. (ICLR), 2023. 9\nCalvin Luo. Understanding diffusion models: A unified perspective. CoRR, abs\/2208.11970, 2022. 3\n14\nPublished as a conference paper at ICLR 2024\nHaley MacLeod, Cynthia L. Bennett, Meredith Ringel Morris, and Edward Cutrell. Understanding blind people’s\nexperiences with computer-generated captions of social media images. In Proceedings of the 2017 CHI\nConference on Human Factors in Computing Systems, CHI ’17, pp. 5988–5999, New York, NY, USA, 2017.\nAssociation for Computing Machinery. ISBN 9781450346559. 23\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question\nanswering benchmark requiring external knowledge. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR),\n2019. 6, 30\nTomás Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. Recurrent neural\nnetwork based language model. In Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH), 2010. 3\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.\nNerf: representing scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99–106, 2022.\n31\nAlexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks.\n2015. URL https:\/\/ai.googleblog.com\/2015\/06\/inceptionism-going-deeper-into-neural.html.\n3\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided\ndiffusion models. In Int. Conf. Mach. Learn. (ICML), 2022. 6, 26, 36\nOpenAI. Introducing chatgpt. 2022. URL https:\/\/openai.com\/blog\/chatgpt. 1, 30\nOpenAI. Gpt-4v(ision) system card. 2023a. URL https:\/\/openai.com\/research\/gpt-4v-system-card. 1,\n26, 27, 28, 30\nOpenAI. GPT-4 technical report. CoRR, abs\/2303.08774, 2023b. URL https:\/\/openai.com\/research\/gpt-4.\n1, 26, 27, 28, 31\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned\nphotographs. In Adv. Neural Inform. Process. Syst. (NIPS), 2011. 29\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In Adv. Neural Inform. Process. Syst. (NeurIPS),\n2022. 5, 30\nGaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot\nimage-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH 2023, Los\nAngeles, CA, USA, August 6-10, 2023, pp. 11:1–11:11, 2023. 31\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. CoRR, abs\/2212.09748, 2022.\n31\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon LLM:\noutperforming curated corpora with web data, and web data only. CoRR, abs\/2306.01116, 2023. 31\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world. CoRR, abs\/2306.14824, 2023. 9\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and\nRobin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR,\nabs\/2307.01952, 2023. 31\nJordi Pont-Tuset, Jasper R. R. Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting\nvision and language with localized narratives. In Eur. Conf. Comput. Vis. (ECCV), 2020. 6, 30\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In\nInt. Conf. Learn. Represent. (ICLR), 2023. 2, 3, 31, 32\nZekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with\nreconstruct: Contrastive 3d representation learning guided by generative pretraining. In Int. Conf. Mach.\nLearn. (ICML), 2023a. 32\n15\nPublished as a conference paper at ICLR 2024\nZekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma. VPP: efficient conditional 3d generation via voxel-point\nprogressive representation. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023b. 32\nZekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, He Wang, Li Yi, and\nKaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. CoRR, abs\/2402.17766,\n2024. 32\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training. 2018. 3, 30\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3, 30\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable\nvisual models from natural language supervision. In Int. Conf. Mach. Learn. (ICML), 2021. 1, 2, 4, 29\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick,\nAlbin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-\nSen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John\nMellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar,\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre,\nLena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\nDonato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\nde Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura\nWeidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey\nIrving. Scaling language models: Methods, analysis & insights from training gopher. CoRR, abs\/2112.11446,\n2021. 30\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Int. Conf. Mach. Learn. (ICML), 2021. 6, 26, 36\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with CLIP latents. CoRR, abs\/2204.06125, 2022. 6, 26, 31, 36\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination\nin image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, 2018. 23\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 1, 2,\n4, 6, 30, 31\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In IEEE\/CVF Conf. Comput. Vis.\nPattern Recog. (CVPR), 2023. 9, 25\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and\nMohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In\nAdv. Neural Inform. Process. Syst. (NeurIPS), 2022. 3, 6, 26, 31, 37\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd\nschema challenge at scale. Commun. ACM, 64(9):99–106, 2021. 22, 30\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense\nreasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019. 22, 30\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné,\nAlexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Bider-\nman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff,\nAlbert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major,\n16\nPublished as a conference paper at ICLR 2024\nIz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon,\nYacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa,\nAlham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris\nEmezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM:\nA 176b-parameter open-access multilingual language model. CoRR, abs\/2211.05100, 2022. 31\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta,\nTheo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million\nimage-text pairs. CoRR, abs\/2111.02114, 2021. 5, 29\nChristoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco:\n600m synthetic captions from laion2b-en, 2023. URL https:\/\/laion.ai\/blog\/laion-coco\/. 29\nYuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion\nmodels. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 25\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning. In ACL, 2018. 29\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving\nAI tasks with chatgpt and its friends in huggingface. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. 9\nShelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman.\nknn-diffusion: Image generation via large-scale retrieval. In Int. Conf. Learn. Represent. (ICLR), 2023. 6\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron\nAshual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation\nwithout text-video data. In Int. Conf. Learn. Represent. (ICLR), 2023a. 31\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal,\nAndrea Vedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman. Text-to-4d dynamic scene generation. In\nInt. Conf. Mach. Learn. (ICML), 2023b. 31\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards VQA models that can read. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR),\n2019. 6, 30\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In Int. Conf. Mach. Learn. (ICML), 2015. 3\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Adv.\nNeural Inform. Process. Syst. (NeurIPS), 2019. 3\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Adv. Neural\nInform. Process. Syst. (NeurIPS), 2020. 3\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In Int. Conf. Learn. Represent.\n(ICLR), 2021. 3\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Int. Conf. Mach. Learn.\n(ICML), 2023. 3, 25\nRobert J Sternberg. Beyond IQ: A triarchic theory of human intelligence. CUP Archive, 1985. 1\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng,\nand Daxin Jiang. Multimodal dialogue response generation. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL, 2022. 9\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: improved training techniques for\nCLIP at scale. CoRR, abs\/2303.15389, 2023a. 9\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,\nTiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. CoRR, abs\/2307.05222, 2023b.\n1, 2, 3, 5, 6, 9, 24\nDídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.\nCoRR, abs\/2303.08128, 2023. 9\n17\nPublished as a conference paper at ICLR 2024\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d:\nHigh-fidelity 3d creation from A single image with diffusion prior. In Int. Conf. Comput. Vis. (ICCV), 2023.\n31\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https:\/\/github.com\/\ntatsu-lab\/stanford_alpaca, 2023. 31\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-\ntiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs\/2302.13971,\n2023a. 1, 4, 9, 22, 29, 31\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela\nFan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs\/2307.09288, 2023b. 1, 31\nAäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Adv.\nNeural Inform. Process. Syst. (NIPS), 2017. 9\nAäron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George\nvan den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe,\nSeb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom\nWalters, Dan Belov, and Demis Hassabis. Parallel wavenet: Fast high-fidelity speech synthesis. In Int. Conf.\nMach. Learn. (ICML), 2018. 2, 4\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2015. 30\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Comput., 23(7):\n1661–1674, 2011. 3\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining:\nLifting pretrained 2d diffusion models for 3d generation. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog.\n(CVPR), 2023a. 3, 31\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence\nlearning framework. In Int. Conf. Mach. Learn. (ICML), 2022. 9\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In Int. Conf.\nLearn. Represent. (ICLR), 2023b. 31\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer:\nHigh-fidelity and diverse text-to-3d generation with variational score distillation. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2023c. 31\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Int. Conf. Learn. Represent.\n(ICLR), 2022a. 30\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Adv. Neural Inform.\nProcess. Syst. (NeurIPS), 2022b. 31\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. CoRR, abs\/2303.04671, 2023. 9\n18\nPublished as a conference paper at ICLR 2024\nJiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary\npanoptic segmentation with text-to-image diffusion models. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog.\n(CVPR), 2023a. 32\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan:\nFine-grained text to image generation with attentional generative adversarial networks. In IEEE\/CVF Conf.\nComput. Vis. Pattern Recog. (CVPR), 2018. 6\nZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction\ntuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)\n(Volume 1: Long Papers), 2023b. 9\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large\nlanguage model to use tools via self-instruction. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023a. 9\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of GPT-3 for few-shot knowledge-based VQA. In AAAI Conf. Artif. Intell. (AAAI), 2022. 24\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang. MM-REACT: prompting chatgpt for multimodal reasoning and action.\nCoRR, abs\/2303.11381, 2023b. 9, 23\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant:\nEfficient and affordable post-training quantization for large-scale transformers. In Adv. Neural Inform. Process.\nSyst. (NeurIPS), 2022. 25\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.\nmplug-owl: Modularization empowers large language models with multimodality. CoRR, abs\/2304.14178,\n2023. 9, 23\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. T. Mach. Learn. Res. (TMLR), 2022a. 2, 6\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason\nBaldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. T.\nMach. Learn. Res. (TMLR), 2022, 2022b. 6, 26, 37\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu,\nHovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh,\nYaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling\nautoregressive multi-modal models: Pretraining and instruction tuning. CoRR, abs\/2309.02591, 2023a. 5, 6, 9\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan\nWang. Mm-vet: Evaluating large multimodal models for integrated capabilities. CoRR, abs\/2308.02490,\n2023b. 22, 23, 30\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\nfinish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, 2019. 22, 30\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng\nZhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In Int. Conf. Learn.\nRepresent. (ICLR), 2023. 22, 31\nJunbo Zhang, Runpei Dong, and Kaisheng Ma. CLIP-FO3D: learning free open-world 3d scene representations\nfrom 2d dense CLIP. In Int. Conf. Comput. Vis. Worksh. (ICCV Workshop), 2023a. 32\nJunbo Zhang, Guofan Fan, Guanghan Wang, Zhengyuan Su, Kaisheng Ma, and Li Yi. Language-assisted 3d\nfeature learning for semantic scene understanding. In AAAI Conf. Artif. Intell. (AAAI), 2023b. 32\nLinfeng Zhang, Xin Chen, Runpei Dong, and Kaisheng Ma. Region-aware knowledge distillation for efficient\nimage-to-image translation. In Brit. Mach. Vis. Conf. (BMVC), 2023c. 31\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\nIn Int. Conf. Comput. Vis. (ICCV), 2023d. 31\n19\nPublished as a conference paper at ICLR 2024\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. CoRR,\nabs\/2303.16199, 2023e. 9\nRenrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d\npre-trained models via image-to-point masked autoencoders. In IEEE\/CVF Conf. Comput. Vis. Pattern Recog.\n(CVPR), 2023f. 32\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\nGpt4roi: Instruction tuning large language model on region-of-interest. CoRR, abs\/2307.03601, 2023g. 9\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig,\nPunit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer\nlanguage models. CoRR, abs\/2205.01068, 2022. 1, 31\nLiang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei\nDong, Chunrui Han, and Xiangyu Zhang. Chatspot: Bootstrapping multimodal llms via precise referring\ninstruction tuning. CoRR, abs\/2307.09474, 2023a. 9, 31\nYanli Zhao, Rohan Varma, Chien-Chin Huang, Shen Li, Min Xu, and Alban Desmaison.\nIntro-\nducing pytorch fully sharded data parallel (fsdp) api, 2023b.\nURL https:\/\/pytorch.org\/blog\/\nintroducing-pytorch-fully-sharded-data-parallel-api\/. Accessed: 2022-03-14. 29\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with\nmt-bench and chatbot arena. CoRR, abs\/2306.05685, 2023. 4, 29\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in\nlarge language models. In Int. Conf. Learn. Represent. (ICLR), 2023. 31\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. CoRR, abs\/2304.10592, 2023a. 9, 22,\n23\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: an open, billion-scale corpus of\nimages interleaved with text. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023b. 2, 5, 29, 30\n20\nPublished as a conference paper at ICLR 2024\nCONTENTS\n1\nIntroduction\n1\n2\nBackground & Problem Statement\n3\n2.1\nHow can we use MLLMs for Diffusion synthesis that synergizes both sides? . . . .\n3\n3\nDREAMLLM\n4\n3.1\nEnd-to-End Interleaved Generative Pretraining (I-GPT) . . . . . . . . . . . . . . .\n4\n3.2\nModel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4\nExperiments\n5\n4.1\nMultimodal Comprehension\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nText-Conditional Image Synthesis\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.3\nMultimodal Joint Creation & Comprehension . . . . . . . . . . . . . . . . . . . .\n6\n5\nDiscussions\n8\n5.1\nSynergy between creation & comprehension? . . . . . . . . . . . . . . . . . . . .\n8\n5.2\nWhat is learned by DREAMLLM? . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n6\nRelated Works\n9\n7\nConclusions\n9\nA Additional Experiments\n22\nA.1 Additional Natural Language Understanding Results\n. . . . . . . . . . . . . . . .\n22\nA.2 Additional Multimodal Comprehension Results . . . . . . . . . . . . . . . . . . .\n22\nA.3\nIn-Context Multimodal Comprehension\n. . . . . . . . . . . . . . . . . . . . . . .\n24\nA.4\nSubject-Driven Image Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.5 Additional Ablation Study\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.6 Additional Discussions on Prompt Rewriting Strategy . . . . . . . . . . . . . . . .\n26\nB Additional Qualitative Examples\n26\nC Implementation Details\n29\nC.1\nTraining Data & Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.2\nDREAMLLM Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.3\nEvaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nD Additional Related Works\n30\nE\nLimitations, Failure Cases & Future Works\n31\n21\nPublished as a conference paper at ICLR 2024\nTable 4: Zero-shot natural language processing evaluation. We report the 5-shot result on MMLU\nand the relative performance of DREAMLLM compared to base LLM Vicuna-7B.\nMethod\nCommonsense Reasoning\nReading\nMultitask\nPIQA\nSIQA\nHellaSwag\nWinoGrande\nBoolQ\nMMLU\nLanguage Only Large Language Models (LLMs)\nGPT-3 (Brown et al., 2020)\n81.0\n-\n78.9\n70.2\n60.5\n43.9\nPaLM-540B (Chowdhery et al., 2022)\n82.3\n-\n83.4\n81.1\n88.0\n69.3\nLLaMA-7B (Touvron et al., 2023a)\n79.8\n48.9\n76.1\n70.1\n76.5\n35.1\nVicuna-7B (Chiang et al., 2023)\n77.7\n47.5\n75.7\n67.5\n73.9\n45.0\nMultimodal Large Language Models (MLLMs)\nMetaLM (Hao et al., 2022)\n72.3\n-\n53.5\n56.1\n62.2\n-\nKosmos-1 (Huang et al., 2023)\n72.9\n-\n50.0\n54.8\n56.4\n-\nDREAMLLM-7B (Ours)\n78.6+1.5\n48.8+1.3\n77.4+1.7\n68.5+1.0\n75.2+1.3\n41.8−3.2\nTable 5: Zero-shot multimodal comprehension evaluation on MMBench (Liu et al., 2023e) dev\nset. LR: Logical Reasoning, AR: Attribute Reasoning, RR: Relation Reasoning, FP-C: Fine-grained\nPerception (Cross Instance), FP-S: Fine-grained Perception (Single Instance), CP: Coarse Perception.\nDREAMLLM ∗is trained using the SFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nLR\nAR\nRR\nFP-S\nFP-C\nCP\nOverall\nOpenFlamingo-9B (Awadalla et al., 2023)\n4.2\n15.4\n0.9\n8.1\n1.4\n5.0\n6.6\nMMGPT-7B (Gong et al., 2023)\n2.5\n26.4\n13.0\n14.1\n3.4\n20.8\n15.3\nMiniGPT-4-7B (Zhu et al., 2023a)\n7.5\n31.3\n4.3\n30.3\n9.0\n35.6\n24.3\nInstructBLIP-7B (Dai et al., 2023a)\n14.2\n46.3\n22.6\n37.0\n21.4\n49.0\n36.0\nVisualGLM (Zeng et al., 2023)\n10.8\n44.3\n35.7\n43.8\n23.4\n47.3\n38.1\nLLaVA-7B (Liu et al., 2023c)\n16.7\n48.3\n30.4\n45.5\n32.4\n40.6\n38.7\nLLaMA-Adapter V2 (Gao et al., 2023)\n11.7\n35.3\n29.6\n47.5\n38.6\n56.4\n41.2\nMiniGPT-4-13B (Zhu et al., 2023a)\n20.8\n50.7\n30.4\n49.5\n26.2\n50.7\n42.3\nDREAMLLM-7B (Ours)\n15.8\n53.7\n60.9\n53.2\n40.0\n58.3\n49.9\nDREAMLLM-7B∗(Ours)\n23.3\n67.2\n47.8\n58.6\n54.4\n70.5\n58.2\nA\nADDITIONAL EXPERIMENTS\nA.1\nADDITIONAL NATURAL LANGUAGE UNDERSTANDING RESULTS\nWe evaluate the natural language processing capabilities of DREAMLLM post-multimodal adap-\ntation learning via zero-shot experiments on language-only tasks. These included commonsense\nreasoning (PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), Wino-\nGrande (Sakaguchi et al., 2021)), reading comprehension (BoolQ (Clark et al., 2019)), and a general\nmulti-task benchmark (MMLU 5-shot (Hendrycks et al., 2021)). As Table 4 illustrates, DREAMLLM\noutperforms the Vicuna baseline on most language benchmarks. This suggests that DREAMLLM’s\nmultimodal adaptation does not compromise the language learning model’s (LLM) capabilities. When\ncompared to prior Multimodal Language Learning Models (MLLMs), DREAMLLM demonstrates\nsuperior performance, although this may be attributed to the higher baseline results. This finding\nsuggests that a more robust LLM base model could yield improved results.\nA.2\nADDITIONAL MULTIMODAL COMPREHENSION RESULTS\nDetailed Comprehensive Comparison\nThe evaluation results on MMBench (Liu et al., 2023e) and\nMM-Vet (Yu et al., 2023b) are presented in Table 5 and Table 6, respectively. The key observations\nfrom these results are as follows: i) Our DREAMLLM-7B outperforms all other 7B MLLMs, setting\na new benchmark in overall performance. Notably, it even exceeds the performance of some 13B\nmodels, including LLaVA and MiniGPT-4. ii) A detailed capability evaluation reveals DREAMLLM’s\nsuperior performance in fine-grained understanding and relational\/spatial comprehension. This\nadvantage is likely due to DREAMLLM’s unique learning synergy, where image distributions are\ncomprehended not solely through language-posterior comprehension but also through creation.\n22\nPublished as a conference paper at ICLR 2024\nTable 6: Zero-shot multimodal comprehension evaluation of core VL capabilities on MM-Vet (Yu\net al., 2023b). ‡ denotes compositional systems with OpenAI GPT and various interfaces. Rec:\nGeneral Visual Recognition, OCR: Optical Character Recognition, Know: Knowledge, Gen: Language\nGeneration, Spat: Spatial Awareness, Math: Arithmetic Math. DREAMLLM ∗is trained using the\nSFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nTF Agent-GPT-4‡ (Huggingface, 2023)\n18.2\n3.9\n2.2\n3.2\n12.4\n4.0\n13.4±0.5\nMM-ReAct-GPT-3.5‡ (Yang et al., 2023b)\n24.2\n31.5\n21.5\n20.7\n32.3\n26.2\n27.9±0.1\nMM-ReAct-GPT-4‡ (Yang et al., 2023b)\n33.1\n65.7\n29.0\n35.0\n56.8\n69.2\n44.6±0.2\nLLaMA-Adapter v2-7B (Gao et al., 2023)\n16.8\n7.8\n2.5\n3.0\n16.6\n4.4\n13.6±0.2\nOpenFlamingo-9B (Awadalla et al., 2023)\n24.6\n14.4\n13.0\n12.3\n18.0\n15.0\n21.8±0.1\nMiniGPT-4-8B (Zhu et al., 2023a)\n27.4\n15.0\n12.8\n13.9\n20.3\n7.7\n22.1±0.1\nBLIP-2-12B (Li et al., 2023d)\n27.5\n11.1\n11.8\n7.0\n16.2\n5.8\n22.4±0.2\nMiniGPT-4-14B (Zhu et al., 2023a)\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4±0.4\nOtter-9B (Li et al., 2023b)\n28.4\n16.4\n19.4\n20.7\n19.3\n15.0\n24.6±0.2\nInstructBLIP-14B (Dai et al., 2023a)\n30.8\n16.0\n9.8\n9.0\n21.1\n10.5\n25.6±0.3\nInstructBLIP-8B (Dai et al., 2023a)\n32.4\n14.6\n16.5\n18.2\n18.6\n7.7\n26.2±0.2\nLLaVA-7B (LLaMA-2) (Liu et al., 2023c)\n32.9\n20.1\n19.0\n20.1\n25.7\n5.2\n28.1±0.4\nLLaVA-13B (LLaMA-2) (Liu et al., 2023c)\n39.2\n22.7\n26.5\n29.3\n29.6\n7.7\n32.9±0.1\nDREAMLLM-7B (Ours)\n41.8\n26.4\n33.4\n33.0\n31.0\n11.5\n35.9±0.1\nDREAMLLM-7B (Ours)\n42.0\n28.1\n33.2\n33.8\n32.0\n11.5\n36.6±0.1\nTable 7: Zero-shot visual hallucination evaluation on POPE (Li et al., 2023f) using MS-COCO\nval set. Yes denotes the proportion of answering “Yes” to the given question, which is better if it is\nmore close to 50%. Objects that do not exist in the image are sampled with three different strategies.\nRandom: random sampling, Popular: top-k most frequent objects in MS-COCO (k = 3), Adversial:\nobjects are first ranked based on co-occurring frequencies, then top-k frequent ones are sampled.\nPOPE\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\nYes (%)\nRandom\nmPLUG-Owl-7B (Ye et al., 2023)\n53.97\n52.07\n99.60\n68.39\n95.63\nLLaVA-13B (Liu et al., 2023c)\n50.37\n50.19\n99.13\n66.64\n98.77\nMMGPT-7B (Gong et al., 2023)\n50.10\n50.05\n100.00\n66.71\n99.90\nMiniGPT-4-14B (Zhu et al., 2023a)\n79.67\n78.24\n82.20\n80.17\n52.53\nInstructBLIP-14B (Dai et al., 2023a)\n88.57\n84.09\n95.13\n89.27\n56.57\nDREAMLLM-7B (Ours)\n86.36\n85.92\n87.93\n86.91\n52.75\nPopular\nmPLUG-Owl-7B (Ye et al., 2023)\n50.90\n50.46\n99.40\n66.94\n98.57\nLLaVA-13B (Liu et al., 2023c)\n49.87\n49.93\n99.27\n66.44\n99.40\nMMGPT-7B (Gong et al., 2023)\n50.00\n50.00\n100.00\n66.67\n100.00\nMiniGPT-4-14B (Zhu et al., 2023a)\n69.73\n65.86\n81.93\n73.02\n62.20\nInstructBLIP-14B (Dai et al., 2023a)\n82.77\n76.27\n95.13\n84.66\n62.37\nDREAMLLM-7B (Ours)\n80.07\n75.74\n88.47\n81.61\n58.40\nAdversarial\nmPLUG-Owl-7B (Ye et al., 2023)\n50.67\n50.34\n99.33\n66.82\n98.67\nLLaVA-13B (Liu et al., 2023c)\n49.70\n49.85\n99.07\n66.32\n99.37\nMMGPT-7B (Gong et al., 2023)\n50.00\n50.00\n100.00\n66.67\n100.00\nMiniGPT-4-14B (Zhu et al., 2023a)\n65.17\n61.19\n82.93\n70.42\n67.77\nInstructBLIP-14B (Dai et al., 2023a)\n72.10\n65.13\n95.13\n77.32\n73.03\nDREAMLLM-7B (Ours)\n72.63\n67.07\n88.93\n76.47\n66.30\nVisual Hallucination\nVisual hallucination, a phenomenon where MLLMs generate non-existent\nobjects or identities in images, significantly compromises their multimodal comprehension capabili-\nties (Dai et al., 2023b; Liu et al., 2023a; Gunjal et al., 2023) and may pose safety risks (MacLeod\net al., 2017; Rohrbach et al., 2018). We assess the robustness of DREAMLLM against visual\nhallucination using the recently developed POPE benchmark (Li et al., 2023f). Refer to Table 7\nfor a detailed comparison with concurrent comprehension-only MLLMs. Our results indicate that\nDREAMLLM-7B exhibits robustness to visual hallucination, matching or surpassing the performance\nof 13B counterparts. Remarkably, DREAMLLM achieves the best or second-best performance in\nthe most challenging setting. We posit that this robust anti-hallucination property stems from a deep\nunderstanding of object concepts and semantics fostered by multimodal creation learning.\n23\nPublished as a conference paper at ICLR 2024\nTable 8: Few-shot multimodal comprehension evaluation. k is the number of in-context examples.\n† denotes methods using the RICES sample selection approach (Yang et al., 2022). DREAMLLM-7B∗\nis trained using the SFT data constructed by LLaVA-1.5 (Liu et al., 2023b).\nMethod\nVQAv2\nVizWiz\nk=2\nk=4\nk=8\nk=2\nk=4\nk=8\nComprehension Only MLLMs\nKosmos-1 (Huang et al., 2023)\n51.4\n51.8\n51.4\n31.4\n35.3\n39.0\nFlamingo-9B† (Alayrac et al., 2022)\n-\n56.3\n58.0\n-\n34.9\n39.4\nMLLMs for Comprehension & Creation\nEmu-14B† (Sun et al., 2023b)\n56.4\n58.4\n59.0\n37.8\n41.3\n43.9\nDREAMLLM-7B (Ours)\n58.1\n59.2\n59.4\n46.1\n46.7\n46.8\nDREAMLLM-7B∗(Ours)\n73.8\n74.4\n73.8\n49.8\n50.3\n49.7\nA.3\nIN-CONTEXT MULTIMODAL COMPREHENSION\nFew-Shot Evaluation In Table 8, we show the results of few-shot (i.e., k-shot and we set k=2,\n4, 8) evaluation by promoting models with a small number of training examples in context. The\nresults demonstrate the strong in-context learning performance of DREAMLLM compared to Emu\nand Flamingo. It shows that DREAMLLM’s effectiveness in leveraging in-context knowledge.\nQualitative Examples In Fig. 7, we present qualitative instances of in-context comprehension using\nDREAMLLM. The illustrations indicate that DREAMLLM, when prompted with specific examples,\nefficiently executes in-context comprehension in the required formats and logic.\nThis is a Welsh Corgi. \nThis is a Koala. \nThis is\na woolly Mammoth.\nWhat is the title of \nthis painting? Answer: \nThe Son of Man.\nWhen was this\nmasterpiece painted?\nAnswer: 1818.\nWho painted this? \nAnswer:\nClaude Monet.\nFilm name: Harry \nPotter and the \nPrisoner of Azkaban\nFilm name: \nBlade Runner 2049\nJoker\nFilm name:\nInput Prompt\nCompletion\nFigure 7: Selected DREAMLLM in-context multimodal comprehension examples.\n24\nPublished as a conference paper at ICLR 2024\nInput Image\ndog\nswimming in \nthe pool\nWearing \nsunglasses\nwearing a \ntop hat\nin the jungle\non the beach\nteapot\nin blue\nas a lamp\nin the jungle\nfloating in \nthe water\npainted in \ngreen\nruns on the \nmountain\nin front of \nwheat field\ncar\nInput Image\nInput Image\nFigure 8: Selected zero-shot subject-driven image generation examples with DREAMLLM. The\nresults demonstrate that DREAMLLM is able to perform zero-shot subject-driven image generation\nwhile preserving image subject details and following generation instructions.\nTable 9: Ablation studies and inference latency of DREAMLLM. The zero-shot FID on MS-COCO\n30K is reported. The inference latency is tested on NVIDIA A800 devices.\n(a)\nThe number of <dream>\nqueries.\nNo. Queries\nCOCOFID↓\n32\n9.56\n64\n8.46\n128\n14.24\n(b) Inference latency versus dif-\nferent number of diffusion steps.\nSteps\nDREAMLLM\nSD\n50\n3.65s\n3.46s\n100\n7.02s\n6.84s\n150\n10.41s\n10.22s\nA.4\nSUBJECT-DRIVEN IMAGE GENERATION\nImage consistency is important when generating interleaved content or performing controllable image\ngeneration tasks (Gal et al., 2023; Ruiz et al., 2023). However, MMC4 does not have such image\nconsistency property, which leads to unsatisfactory image consistency results as shown in Fig. 3. To\nfurther verify the effectiveness and potential of DREAMLLM in generating consistent images that\npreserve subject features, we fine-tune stage II pretrained DREAMLLM on subject representation\nlearning data constructed by following the recently proposed controllable image generation method\nBLIP-Diffusion (Li et al., 2023c). We fine-tune DREAMLLM on this small ∼270K samples data\nfor 20 epochs, and the results are shown in Fig. 8. It demonstrates the effectiveness and promising\npotential of applying DREAMLLM for image-consistent generation.\nA.5\nADDITIONAL ABLATION STUDY\nQuery Number\nIn Table 9a, we show the results of DREAMLLM using different numbers of the\nproposed learnable queries. i.e., <dream> queries. The results show that 64 queries achieve the best\nresult, while 128 may be too many, which may impact the performance. However, the choice of query\nnumber is also related to the choice of training data size and diffusion model. For example, if given\nmore data and a stronger diffusion model image decoder, queries more than 64 may be better.\nInference Latency\nIn Table 9b, we present a comparison of real-time inference latency between\nDREAMLLM and SD. Relative to SD, DREAMLLM introduces a marginal latency cost of 0.2s on\naverage. This is because the latency primarily stems from the computational demands of the diffusion\nU-Net denoising rather than the text condition embedding. To enhance inference efficiency, potential\nstrategies could include the adoption of Consistency Models (Song et al., 2023) or the implementation\nof model compression techniques such as quantization (Yao et al., 2022; Dong et al., 2022; Shang\net al., 2023).\n25\nPublished as a conference paper at ICLR 2024\nTable 10: Language processing and multimodal comprehension & creation capability comparison\nto the rewrite-then-generate baseline.\nMethod\nLanguage Processing\nMultimodal Processing\nPIQA\nSIQA\nHellaSwag\nWinoGrande\nBoolQ\nMMLU\nVQAv2\nMM-Vet\nCOCO\nVicuna-7B (Chiang et al., 2023)\n77.7\n47.5\n75.7\n67.5\n73.9\n45.0\n-\n-\n-\nrewrite-then-generate\n78.2\n48.5\n75.8\n68.3\n77.4\n43.1\n54.2\n34.1\n11.91\nDREAMLLM-7B (Ours)\n78.6\n48.8\n77.4\n68.5\n75.2\n41.8\n56.6\n35.9\n8.46\nA.6\nADDITIONAL DISCUSSIONS ON PROMPT REWRITING STRATEGY\nVery recently, OpenAI has released DELLE-3 (Betker et al., 2023), which proposes to improve\ngenerated image quality by rewriting descriptive and better prompts with GPT-4. This product has\ndemonstrated great success in leveraging LLMs as language-output agents. However, it generally\nrequires a large amount of high-quality data and is limited when applied to image-conditional genera-\ntion tasks. For instance, DALLE-3 necessitates the initial training of a bespoke image captioning\nspecialist capable of producing high-quality descriptive captions, followed by model training in a\ndata-rich environment featuring these written captions. This process is non-trivial, hinging heavily on\nthe availability of substantial volumes of high-quality data. Moreover, such disjoint systems cannot\nguarantee learning synergy. In contrast, our exploration of DreamLLM has essentially unveiled\nthe significant potential of LLMs to attain a comprehensive understanding of multimodality that\ngenuinely comprehends modalities beyond mere language.\nTo make a comparison regarding language processing and multimodal comprehension capabilities\nto this rewrite-then-generate baseline method, we conduct a preliminary study. Given the absence\nof an optimal dataset holding improved prompts, we modify the original MMC4 by using <dream>\nstart & end tokens before and after the specific text prompt that has the highest CLIP similarity\nto a specific image, which can be used as text prompts for image generation. In this setting, we\nonly train the LLMs to output texts, and no image decoders are involved during training. During\ninference, when the model outputs texts encompassed by the <dream> tokens, the texts are used\nfor an off-the-shelf SD image decoder for generating images. After training, we test the model’s\nlanguage processing and multimodal capabilities. The results show that i) the rewrite-then-generate\nmethod achieves similar performance to DREAMLLM. This demonstrates that both methods won’t\nimpact the language capability, which is as expected. ii) the performance of the rewrite-then-generate\nbaseline falls short when compared to DREAMLLM, particularly in the context of text-to-image\ngeneration on the COCO dataset. This underlines the efficacy of the synergistic learning approach\ninherent in DREAMLLM, suggesting its potential superiority over the baseline methodology.\nB\nADDITIONAL QUALITATIVE EXAMPLES\nMultimodal Dialogue\nIn Tables 11 and 12, we present a qualitative comparative analysis of\nVQA results between our model, DREAMLLM, and other state-of-the-art models: GPT-4 (OpenAI,\n2023a;b), LLaVA (Liu et al., 2023c), BLIP-2 (Li et al., 2022), and OpenFlamingo (Awadalla et al.,\n2023). The key findings are as follows: i) DREAMLLM surpasses GPT-4 in providing more\ndetailed and precise responses to given questions. ii) While LLaVA (Liu et al., 2023c) also offers\ndetailed responses, it frequently introduces imaginary elements not present in the image. In contrast,\nDREAMLLM delivers more accurate answers, effectively avoiding this visual hallucination issue.\nThis observation aligns with our earlier findings in Table 7, which underscore the robustness of\nDREAMLLM against visual hallucination. Furthermore, we showcase additional qualitative results\nof the multimodal dialogue in Fig. 10, Fig. 11, and Fig. 12. These figures illustrate DREAMLLM’s\nproficiency in comprehending and generating long-context multimodal information in arbitrary input\nand output formats.\nText-condition Image Synthesis\nIn Fig. 13 and Fig. 14, we show the image examples of DREAM-\nLLM using the same prompts from previous works for a cross reference and comparison, including\nDALL-E (Ramesh et al., 2021), DALL-E 2 (i.e., unCLIP) (Ramesh et al., 2022), GLIDE (Nichol\net al., 2022), Imagen (Saharia et al., 2022), and Parti (Yu et al., 2022b). Similar to Parti, we have\nextended some prompts with new sub-prompts to construct more examples from different prompts.\n26\nPublished as a conference paper at ICLR 2024\nVisual input example, Extreme Ironing:\nSource: https:\/\/www.barnorama.com\/wp-content\/uploads\/2016\/12\/03-Confusing-Pictures.jpg\nUser\nWhat is unusual about this image?\nDREAMLLM (Ours)\nThe unusual aspect of this image is that a man is ironing clothes while sitting on the trunk\nof a car parked in traffic. This is unconventional because ironing clothes is typically done\nindoors, on a flat surface, and away from busy streets where cars are passing by. The man’s\nactions of ironing clothes while on the trunk of the car and amidst traffic create an unusual\nand interesting scene.\n[Start a new conversation, and clear the history]\nUser\nWhat’s happening in the scene?\nDREAMLLM (Ours)\nIn the scene, a man is ironing clothes while standing on the back of a car on a busy city\nstreet. The street is filled with traffic and the man is exposed to the dangers of being\nclose to moving vehicles. This unusual activity of ironing clothes while standing on an\nautomobile also draws attention to the man’s determination to accomplish his task despite\nthe challenging urban environment.\nUser\nWhat is unusual about this image?\nLLaVA (Liu et al., 2023c)\nThe unusual aspect of this image is a man ironing clothes on the back of a minivan or\nvan. This is not a typical place to perform this activity, as one would usually iron clothes\nin a more stationary and safe location, such as a home, using a regular ironing board.\nThe scene depicted in the image is peculiar as it involves a makeshift ironing setup on a\nvehicle, which can be both unsafe and unconventional. Additionally, it is not clear how\nthe man is able to maintain balance and stability while ironing clothes in such an unstable\nenvironment.\n[Start a new conversation, and clear the history]\nUser\nWhat’s happening in the scene?\nLLaVA (Liu et al., 2023c)\nThe image depicts a man standing on top of a yellow SUV in a busy city street. He is\nholding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around\nthe scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are\ntwo other people in the vicinity, one close to the man on the car and another further away.\nThe man on the ladder appears to be attracting attention due to his unconventional choice\nof ironing his clothes on top of a moving car. The city street around him is bustling with\nactivity, adding to the unique nature of the scene.\nUser\nWhat is unusual about this image?\nGPT-4 (OpenAI, 2023a;b)\nThe unusual thing about this image is that a man is ironing clothes on an ironing board\nattached to the roof of a moving taxi.\nUser\nCan you explain this meme in detail?\nBLIP-2 (Li et al., 2023d)\na man is sitting on the back of a yellow cab\nUser\nCan you explain this meme in detail?\nOpenFlamingo (Awadalla et al., 2023)\nThe man is drying his clothes on the hood of his car.\nTable 11: Visual question answering example comparison of DREAMLLM to LLaVA (Liu et al.,\n2023c), GPT-4 (OpenAI, 2023a;b), BLIP-2 (Li et al., 2023d), and OpenFlamingo (Awadalla et al.,\n2023). This table format follows OpenAI (2023a;b).\n27\nPublished as a conference paper at ICLR 2024\nVisual input example, Chicken Nugget Map:\nUser\nCan you explain this meme in detail?\nDREAMLLM (Ours)\nThe meme presents a humorous visual in which a close-up of a plate of fried chicken,\nspecifically chicken nuggets, appears to be a map of the world. The context suggests that\nsomeone has made this map by arranging the chicken nuggets to resemble the shapes\nand outlines of various countries. The image is accompanied by a caption that reads,\n“Sometimes I just look at pictures of the earth from space and I marvel at how beautiful it\nis all is.” This hints that the person who created the “map” might have been inspired by the\nbeauty of our planet from space, and they have used the similar visual pattern to create an\namusing and creative representation of the world using chicken nuggets.\nUser\nCan you explain this meme in detail?\nLLaVA (Liu et al., 2023c)\nThe meme in the image is a creative and humorous take on food, with a focus on chicken\nnuggets, as the center of the universe. The meme begins with the phrase “Sometimes I just\nlook at pictures of the Earth from space and I marvel at how beautiful it all is...” The main\nimage features a tray of cooked chicken nuggets, with the chicken pieces positioned to\nresemble the continents and islands on a world map. The meme playfully suggests that the\nchicken nuggets represent the Earth, and the various locations depicted in the photo are\nactually chicken nugget versions of different places. The punchline of the meme appears\nin smaller text at the bottom, reading “I mean, it’s not the real Earth, but how beautiful it\nis all is.” This lighthearted and entertaining meme combines food and humor, using the\nimage of chicken nuggets to create a funny and imaginative representation of the world.\nUser\nCan you explain this meme?\nGPT-4 (OpenAI, 2023a;b)\nThis meme is a joke that combines two unrelated things: pictures of the earth from space\nand chicken nuggets. The text of the meme suggests that the image below is a beautiful\npicture of the earth from space. However, the image is actually of chicken nuggets\narranged to vaguely resemble a map of the world. The humor in this meme comes from\nthe unexpected juxtaposition of the text and the image. The text sets up an expectation of a\nmajestic image of the earth, but the image is actually something mundane and silly.\nUser\nCan you explain this meme in detail?\nBLIP-2 (Li et al., 2023d)\nsometimes i just look at pictures of the earth from space and marvel how beautiful it is\nUser\nCan you explain this meme in detail?\nOpenFlamingo (Awadalla et al., 2023)\nIt’s a picture of a chicken nugget on the International Space Station.\nTable 12: Visual question answering example comparison of DREAMLLM to LLaVA (Liu et al.,\n2023c), GPT-4 (OpenAI, 2023a;b), BLIP-2 (Li et al., 2023d), and OpenFlamingo (Awadalla et al.,\n2023). This table format follows OpenAI (2023b).\n28\nPublished as a conference paper at ICLR 2024\nTable 13: Training recipes for DREAMLLM. The three training stages are introduced in Section 3.2.\nStage I: Alignment training, Stage II: I-GPT pretraining, Stage III: Supervised fine-tuning.\nStage I\nStage II\nStage III\nConfig\nAlignment\nI-GPT\nSFT\nTraining Hyper-Parameters\nOptimizer\nAdamW\nAdamW\nAdamW\nLearning Rate\n2e-3\n2e-5\n4e-5\nWeight Decay\n0.0\n0.0\n0.0\nTraining Epochs\n1\n1\n3\nWarmup Ratio\n0.003\n0.003\n0.003\nLearning Rate Scheduler\nCosine\nCosine\nCosine\nBatch Size Per GPU\n8\n8\n8\nMaximum Token Length\n2048\n2048\n2048\nUnfreeze LLM\n✗\n✓\n✓\nTraining Data\nDataset\n➀LLaVAPretrain (558K)\n➀MMC4 (2M)\n➀LLaVAInstruct (80K\/665K)\n➁BLIP-LAION (8M)\n➁BLIP-LAION (2M)\n➁InstructMMC4 (20K)\n➂LAION400M (11M)\n➂Instruct-BLIP-LAION (20K)\n➃LAION-COCO (11M)\nData Size\n30M\n4M\n120K\nData Type\nPair\nInterleave\/Pair\nInstruction\nTraining Cost\nGPU Device\n128×NVIDIA A800\n128×NVIDIA A800\n128×NVIDIA A800\nTraining Time\n∼6h\n∼10h\n∼1.5h\nC\nIMPLEMENTATION DETAILS\nC.1\nTRAINING DATA & HYPER-PARAMETERS\nIn Table 13, we list the detailed training dataset usage and hyper-parameters. The training data are\nconstructed based on the following datasets: a) LAION400M (Schuhmann et al., 2021), b) LAION-\nCOCO (Schuhmann et al., 2023), c) MMC4 (Zhu et al., 2023b), d) BLIP-LAION (Li et al., 2022)\nwhich is filtered and caption by BLIP (Li et al., 2022), e) LLaVAPretrain (Liu et al., 2023c) which\ncontains 558K image-text pairs from BLIP-captioned CC3M (Sharma et al., 2018), SBU (Ordonez\net al., 2011), and LAION400M filtered by LLaVA, f) LLaVAInstruct, which contains 80K\/665K\nvisual instruction-following data constructed by LLaVA (Liu et al., 2023c) and LLaVA-1.5 (Liu\net al., 2023b), and g) InstructMMC4, which is our instruction-following interleaved document\ngeneration data curated by prompting GPT-4 to generate instruction based on the text contents of\nMMC4. h) Instruct-BLIP-LAION, which is our instruction-following image synthesis data. Similar\nto InstructMMC4, it is curated by prompting GPT-4 to generate instructions based on image captions.\nUnless otherwise specified, we randomly sample the indicated number of instances from each dataset\nduring the training process.\nC.2\nDREAMLLM MODEL\nLanguage Model We use LLaMA-1 (Touvron et al., 2023a) trained on ShareGPT (Zheng et al.,\n2023) as as the default LLM (i.e., Vicuna-7B1 (Chiang et al., 2023)) following Liu et al. (2023c) to\nendow its instruction-following capacity. During training, we use Flash Attention (Dao et al., 2022)\nand PyTorch FSDP (Zhao et al., 2023b) to accelerate training efficiency.\nVisual Encoder The visual encoder is the publicly available OpenAI CLIP-L\/14 (Radford et al.,\n2021) model, which is frozen during the whole process. The images are resized to 224×224 resolution\nto align with the CLIP pretraining settings, resulting in a sequence of 256 total tokens for each image.\nFollowing prior VL practice (Lu et al., 2019; Liu et al., 2023c), we append a special <IMG> token\nbefore the image sequence and a special <IMG\/> at the end of the sequence.\n1Vicuna-7B v1.1: https:\/\/huggingface.co\/lmsys\/vicuna-7b-v1.1.\n29\nPublished as a conference paper at ICLR 2024\nTable 14: Overall descriptions of the evaluation benchmarks for evaluating capabilities, including\nVL comprehension, content creation, and natural language processing (NLP).\nDataset\nTask description\nEval Split\nMetric\nVL Comprehension\nCOCO (Karpathy & Fei-Fei, 2017)\nScene description\ntest\nCIDEr (Vedantam et al., 2015)\nImage2Paragraph (Krause et al., 2017)\nScene description\ntest\nCIDEr (Vedantam et al., 2015)\nVQAv2 (Goyal et al., 2019)\nScene understanding QA\ntest-dev\nVQA Acc (Antol et al., 2015)\nOKVQA (Marino et al., 2019)\nExternal knowledge QA\nval\nVQA Acc (Antol et al., 2015)\nVizWiz (Gurari et al., 2018)\nScene understanding QA\ntest-dev\nVQA Acc (Antol et al., 2015)\nTextVQA (Singh et al., 2019)\nText reading QA\nval\nVQA Acc (Antol et al., 2015)\nMM-Vet (Yu et al., 2023b)\nMultimodal Comprehension\n-\nGPT-4 Eval (Yu et al., 2023b)\nMMBench (Liu et al., 2023e)\nMultimodal Comprehension\ndev\nGPT-3.5 Eval (Liu et al., 2023e)\nPOPE (Li et al., 2023f)\nVisual Hallucination\n-\nAcc, F1-score, Recall, Precision\nCreation\nMS-COCO (Lin et al., 2014)\nText-Conditional Image Synthesis\nval-30K\nFID (Heusel et al., 2017)\nLN-COCO (Pont-Tuset et al., 2020)\nText-Conditional Image Synthesis\nval\nFID (Heusel et al., 2017)\nMMC4 (Zhu et al., 2023b)\nDoc-Conditional Image Synthesis\nheld-out\nFID (Heusel et al., 2017)\nNLP\nSIQA (Sap et al., 2019)\nCommonsense Reasoning\ndev\nAcc\nPIQA (Bisk et al., 2020)\nCommonsense Reasoning\ndev\nAcc\nHellaSwag (Zellers et al., 2019)\nCommonsense Reasoning\ndev\nAcc\nWinoGrande (Sakaguchi et al., 2021)\nCommonsense Reasoning\ndev\nAcc\nBoolQ (Clark et al., 2019)\nReading Comprehension\ndev\nAcc\nMMLU (Hendrycks et al., 2021)\nAggregated Comprehension\ntest\nAcc\nDiffusion Image Decoder We adopt SDv2.1 (Rombach et al., 2022) trained on 512×512 resolution\nas the default diffusion image decoder. Same as the visual encoder, the SD model is frozen without\nany modifications or training throughout the whole process. When constructing the SD target to\ncompute the MSE loss, we resize the images to 512 resolution to fit its pretraining configuration.\nDream Query We use dream queries to gather semantic context from MLLMs as introduced before in\nSec. 3. Without specifications, we use 64 learnable query embeddings. It is both efficient and effective\nin generating high-quality images. In order to predict when to generate images, we also introduce\nthe special <dream> token, which is appended before the dream query sequence. A <dream\/> is\nappended at the end of the sequence, similar to image inputs.\nClassifier-Free Guidance Classifier-free guidance (CFG) (Ho & Salimans, 2021) has been demon-\nstrated successful in generating photo-realistic contents at the cost of acceptable generation diversity.\nThis technique modifies the objective by ˆϵ := (1 + s)ϵξ(xt, t, C) −sϵξ(xt, t, ∅), where ∅is a special\n“empty” condition representation and s is the condition scale. The larger guidance scale generally\nimproves image authenticity while decreasing diversity. We only adopt CFG during inference, and\nthe scale is set to 7.5 by default and 2.0 for MS-COCO text-conditional image generation.\nC.3\nEVALUATION BENCHMARKS\nSystemic evaluations of DREAMLLM regarding VL comprehension, content creation, and NLP\ncapabilities have been conducted. See the used benchmarks and datasets listed in Table 13. During\nthe evaluation, we use the prompt templates listed in Fig. 9.\nD\nADDITIONAL RELATED WORKS\nLarge Language Models\nA flourishing era of Natural Language Processing (NLP) driven by LLMs\nis being experienced, with the parameter size growing over 100B according to the scaling law (Kaplan\net al., 2020). The GPT series of models, starting with GPT-1 (Radford et al., 2018) and followed\nby GPT-2 (Radford et al., 2019), made significant advancements in few-shot learning by scaling up\nthe number of parameters to 175 billion in GPT-3 (Brown et al., 2020). This breakthrough garnered\na lot of attention and paved the way for further research and development in the field. Since then,\nresearchers have focused on developing LLMs by improving the scaling strategy. Several notable\nefforts include Gopher (Rae et al., 2021), GaLM (Du et al., 2022), FLAN (Wei et al., 2022a), Switch-\nTransformer (Fedus et al., 2022), Chinchilla (Hoffmann et al., 2022), and PaLM (Chowdhery et al.,\n2022). Besides, instruction-based tuning techniques are explored for aligning with human prefer-\nences (Christiano et al., 2017; Ouyang et al., 2022). Such success of LLMs has been further solidified\nby the production release of ChatGPT (OpenAI, 2022) and the highly anticipated GPT-4 (OpenAI,\n30\nPublished as a conference paper at ICLR 2024\n2023a;b). Meanwhile, in the community, the open-source LLMs are achieving remarkable progress\nin language capabilities compared to their close-source counterparts. For example, OPT (Zhang et al.,\n2022), BLOOM (Scao et al., 2022), GLM (Zeng et al., 2023), LLaMA (Touvron et al., 2023a;b), and\nFalcon (Penedo et al., 2023) all raised great attention and are been widely deployed. Other methods\nattempt to learn from distillation, such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023).\nText-Conditional Content Creation with Diffusion Models\nThe recent surge in AI-generated\ncontent (AIGC) has been primarily driven by diffusion-based methods, particularly in the realm of\ntext-conditional content creation. Saharia et al. (2022) have achieved astonishing advancements in\nhigh-resolution image synthesis through large-scale pretrained language models and cascaded DMs.\nAnother paradigm, such as SD, focuses on latent spaces and demonstrates superior efficiency and\nperformance (Rombach et al., 2022; Ramesh et al., 2022; Peebles & Xie, 2022; Podell et al., 2023).\nRecently, Lian et al. (2023) propose to enhance the reasoning capability by constructing layouts with\nLLMs. DALLE-3 (Betker et al., 2023) leverages LLMs as agents and proposes to generate images by\nincorporating GPT-4 for providing high-quality and detailed prompts that facilitate image synthesis.\nMotivated by the great success in 2D, a series of works have significantly propelled the 3D synthesis\ndevelopment (Mildenhall et al., 2022; Liu et al., 2023d; Lin et al., 2023; Wang et al., 2023c; Tang\net al., 2023) based on Score Distillation Sampling (SDS) (Poole et al., 2023; Wang et al., 2023a) that\nutilizes pretrained 2D DMs. For text-to-video\/4D synthesis, the expansion of pretrained spatial to a\nspatial-temporal factorized U-Net with joint image and video data training has yielded significant\nsuccess (Ho et al., 2022a;b; Singer et al., 2023a;b).\nE\nLIMITATIONS, FAILURE CASES & FUTURE WORKS\nLimitations\nWhile DREAMLLM has made significant strides toward the development of versatile,\ncreative, and foundational MLLMs, it still has several limitations.\nModel scale. The primary constraint pertains to the scale of the LLMs utilized. Current evaluations\nmainly employ 7B LLMs as the base model, and despite the impressive results garnered, the potential\nbenefits of larger model sizes, such as 65B or 130B (Kaplan et al., 2020), are worth future exploration.\nTraining data. The second challenge relates to the quality and quantity of training data (Jia et al.,\n2021). As the model size and capabilities scale up, a corresponding increase in data is crucial.\nHowever, the procurement and refinement of high-quality training data present substantial logistical\nand financial hurdles. For instance, the open-source interleaved dataset MMC4 contains a significant\namount of noise in the form of text and images, like commercial advertisements. This noise could\nadversely affect the model’s output language and image style.\nPrompt sensitivity. The sensitivity of LLMs to human prompts is a known issue (Wei et al., 2022b;\nWang et al., 2023b; Zhou et al., 2023), a challenge that extends to MLLMs. For instance, MLLMs’\npropensity for detailed responses necessitates tailored prompting to elicit concise and short answers,\nwhich is particularly useful when addressing Visual Question Answering (VQA) tasks.\nFailure Cases\nThe main failure cases of DREAMLLM are observed for multiple image-based\ncontent creations. For instance, when presented with two images and a composite instruction such as\n“A and B”, DREAMLLM sometimes generates a single subject that amalgamates the characteristics\nof A and B. This output aligns more closely with the directive “A like B”. This phenomenon is not\nunique to DREAMLLM, but is also observed in specialized compositional generation methodologies,\nsuch as StructureDiffusion (Feng et al., 2023; Chefer et al., 2023). This recurring issue may be\nattributed to the inherent complexity of compositional generation tasks, compounded by the severe\nscarcity of data specific to this domain.\nFuture Works\nAs a simple and general multimodal learning framework, our future work aims to\nenhance the DREAMLLM framework by integrating fine-grained visual comprehension via methods\nlike precise referring instruction tuning (Zhao et al., 2023a). We also plan to expand beyond visual\nand linguistic content comprehension and generation. Several promising research directions include:\n• Exploring applications of in-context generation capabilities of DREAMLLM to complex tasks such\nas image-to-image translation (Isola et al., 2017; Zhang et al., 2023c;d; Parmar et al., 2023).\n31\nPublished as a conference paper at ICLR 2024\nBased on the image, give the image caption briefly.\n<IMAGE>\nPlease summarize object in one sentence within 10 words.\nUSER:\nThe image depicts\nASSISTANT:\n<ANSWER>\n(a) Image Captioning (Short)\n(b) Image Captioning (Long)\nBased on the image, please describe the image in detail.\n<IMAGE>\nPlease describe the image in detail.\nUSER:\nThe image depicts\nASSISTANT:\n<ANSWER>\n(c) VQA (Short)\nBased on the image, please answer the question. <IMAGE>\nPlease provide an accurate answer within one word.\nUSER:\nThe answer is:\nASSISTANT:\n<ANSWER>\n(d) VQA (Long)\nThis is an exam, please answer according to the image and question.\n<IMAGE>\nPlease provide an accurate and detailed answer.\nUSER:\nASSISTANT:\n<ANSWER>\n<QUESTION>\n<QUESTION>\n(e) Visual Hallucination\nBased on the image, please objectively and accurately indicate whether the object exists.\n<IMAGE>\nUSER:\nASSISTANT:\nThe answer is:\n<ANSWER>\nIs there a <OBJECT> in the image?\nSystem Prompt\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nFigure 9: Prompt templates. (a) Short image captioning includes COCO captioning, and (b) long\nimage captioning includes Image2Paragraph. (c) Short VQA includes VQAv2, VizWiz, OKVQA,\nand TextVQA. (d) Long VQA includes MMBench and MM-Vet. (e) Visual hallucination includes\nPOPE. <IMAGE> is the image representation, <QUESTION> denotes each specific question, <ANSWER>\nis the generated answer, and <OBJECT> is a specific object name in POPE.\n• The extension of the DREAMLLM model to video data possessing long-term context is natural,\ncapitalizing on the model’s ability to encapsulate temporal knowledge through context consistency.\nThis would give rise to video comprehension and text-to-video generation capabilities.\n• Utilizing DREAMLLM’s context consistency feature for geometry-preserving tasks, including\n3D content creation (Poole et al., 2023; Qi et al., 2023b; Liu et al., 2023d), representation learn-\ning (Dong et al., 2023; Qi et al., 2023a; Zhang et al., 2023a;f), scene comprehension (Zhang et al.,\n2023b; Hong et al., 2023), and embodied artificial inteligence (Ichter et al., 2022; Qi et al., 2024).\n• In addition, since the diffusion model is used as the image decoder, it would be interesting to explore\nDMs-based zero-shot perception such as classification (Li et al., 2023a) and segmentation (Xu\net al., 2023a) with the goal of unifying all perception tasks with image synthesis\/generation.\n• Striving to achieve a unified multimodal zero-shot generalist by extending the scope to various\nmodalities using techniques such as ImageBind (Girdhar et al., 2023) and exploring content creation\nmodels in other modalities like audio (Kong et al., 2021).\n32\nPublished as a conference paper at ICLR 2024\nCan you give a short introduction\nto this movie?\nThe Godfather, also known as Il Padrino, is a classic\nAmerican\ncrime\ndrama\nfilm\ndirected\nby\nFrancis\nFord\nCoppola. It is the first installment in The Godfather\ntrilogy,\nwhich\nalso\nincludes\nThe\nGodfather\nPart\nII\n(1974) and The Godfather Part III (1990). The movie is\nwidely regarded as a masterpiece and a milestone in\nthe mafia genre, exploring the story of the Corleone\nfamily, a fictional Italian-American crime family in\nNew\nYork.\nIt\nfollows\nthe\ntransformation\nof\nMichael\nCorleone, the youngest son of the family's aging don,\nVito\nCorleone,\nas\nhe\nrises\nto\npower\nand\neventually\ntakes on the mantle of his father. The film features a\nstrong\ncast,\nwhich\nincludes\nMarlon\nBrando\nas\nVito\nCorleone, Al Pacino as Michael Corleone, James Caan as\nSonny Corleone, Robert Duvall as Tom Hagen, and Diane\nKeaton as Kay Corleone. The Godfather is known for its\niconic scenes, memorable lines, and its impact on the\nHollywood film industry.\nDo you feel dangerous in the scene in \nthe picture?\nGood. Show me an illustration of a panda wearing a \nsuit and drinking the martini like the Godfather.\nThe image depicts a large painted tiger made from a\nvariety\nof\ncolored\nrocks\nand\nstones,\nwhich\nis\na\npiece of artwork rather than a real animal. As an\ninanimate object, the tiger does not pose any danger.\nWhile\nthe\nrepresentation\nof\nthe\ntiger\nmay\nbe\nfrightening\nto\nsome,\nit\nis\nimportant\nto\nremember\nthat this work of art is not capable of causing harm.\nAn illustration of a panda wearing a\nsuit and drinking a martini like the\nGodfather.\nWhat is the name of this painting?\nThe name of this painting is “Girl with a Pearl \nEarring.”\nCan you give a short introduction to this painting?\n“Girl with a Pearl Earring” is a painting by the\nDutch master Johannes Vermeer, created during the\n17th century. The painting features a girl wearing\na\npearl\nearring\nand\na\nheadscarf,\nwith\nher\nhead\ntilted to the side. The use of light and shadow, as\nwell as the intricate details in her clothing and\nthe surrounding environment, are characteristic of\nVermeer's unique painting style.\nWhat emotional value can this store \nbring to human society?\nThe Disney Store, with its whimsical and colorful\ninterior\nand\na\ndizzying\narray\nof\nDisney-themed\nmerchandise, can bring a sense of joy, wonder, and\nnostalgia to human society. Walking into the store\nevokes\nfeelings\nof\nexcitement\nand\ndelight,\nas\ncustomers\nare\nsurrounded\nby\nfamiliar\ncharacters,\nstories,\nand\nmemories\nassociated\nwith\nthe\niconic\nDisney brand. This emotional value can also promote\nimagination and creativity, as children can engage\nwith the merchandise by pretending and playing out\ntheir favorite Disney stories. The store provides a\nunique and uplifting shopping experience for both\nchildren\nand\nadults,\nfostering\npositive\nemotions\nand\ncreating\nmemorable\nmoments\nthat\ncan\nenrich\npersonal experiences and interactions.\nFantastic. Generate some high-quality illustrations \nof Disney-themed merchandise for me.\nAn image of a bag of Disney branded\nmerchandise,\nwith\na\npicture\nof\na\nDisney character in the background.\nWhat\nabout\na\nbig\nDisney\ncharacter\nlike Mickey Mouse?\nPhotograph A:\nPhotograph B:\nWhat is written in these photographs? What is\nthe difference between these two photos? Answer\nin detail about the difference in written words\nand styles.\nIn the two photographs, A and B, the written words\nare different. In Photograph A, the word “Dream”\nis written differently, possibly with a blue and\npink background, while in Photograph B, the word\n“Dream” appears in a vintage style. The difference\nin\nthe\nstyle\nof\nthe\nword\nand\nthe\nbackground\nsuggests that each photograph presents a different\napproach, possibly signifying different moods or\nthemes. Additionally, the presence of a “llm” in\nPhotograph\nB\nsuggests\na\nvariation\nin\nthe\ntext\ncompared\nto\nonly\nhaving\nthe\nword\n“Dream”\nin\nPhotograph A.\nDreamLLM\nDreamLLM\nDreamLLM\nFigure 10: Qualitative examples of multimodal dialogue between human and DREAMLLM.\nVarious modalities can be used as inputs or outputs, and multi-round dialogue is shown.\n33\nPublished as a conference paper at ICLR 2024\nThe man in photo A is Albert Einstein, and the man in photo B is Lionel Messi.\nWho is the man in photo A, and who is the man in photo B?\nPhotograph of man A:\nPhotograph of man B:\nImagine what kind of conversation will these two people have when they sit in the same room?\nPlease give a possible dialogue script.\nIf Albert Einstein and Lionel Messi were to sit in the same room, they might have a fascinating\nconversation about their experiences, knowledge, and perspectives on various topics such as science,\nphysics, and their respective sports. Here is a possible dialogue script:\nEinstein: Good afternoon, Mr. Messi. How are you?\nMessi: Hello, Dr. Einstein. I am doing well. Thank you for having me.\nEinstein: I am pleased to encounter a fellow genius in the world of sports, and I would love to hear\nyour insights on the game.\nMessi: Thank you, sir. As a soccer player, I feel that the key to success is having a strong mindset,\nan understanding of the game, and the ability to adapt to different situations. I try to learn from\neach game and work on my weaknesses.\nEinstein: Indeed, I have said similar things about the nature of intelligence and knowledge. Learning\nfrom past experiences is crucial to personal growth and understanding. It reminds me of a famous\nquote by the German philosopher Johann Gottlieb Fichte: “If you would be a genius, think deeply and\nlong, and incessantly about one thing. ”\nMessi: I understand the value of continuous learning, and I try to apply it in my training and on the\nfield. I am always trying to improve my skills and learn from my colleagues in the club.\nEinstein: That is a wonderful attitude to have. I am also very impressed by your passion for the\nfield. In my opinion, passion is an integral part of success. It fuels the drive to achieve greatness,\nand it is especially evident in my own field of physics. For instance, I once said, “The passion for\ntruth is the foundation for the sciences.”\nMessi: It is a great feeling to know that I am playing a sport that is full of passionate individuals.\nI am proud to be a part of that world and do my part in contributing to it.\nEinstein: I am sure that you, Mr. Messi, would agree that having a passion for knowledge can lead to\ndiscoveries that change the world. For example, my theory of relativity has had a profound impact on\nour understanding of space, time, and gravity. It was a result of decades of contemplation and\nresearch.\nMessi: The world of sports and the world of science are not so different when it comes to the pursuit\nof excellence. I am honored to be in the presence of a true genius like you, Dr. Einstein.\nEinstein: I feel the same way, and I would like to thank you for this opportunity to engage in a\nconversation about our respective fields. The world is a wondrous place, and I am grateful to be a\npart of it.\nFigure 11: Qualitative examples of multimodal dialogue between human and DREAMLLM.\nVarious modalities can be used as inputs or outputs, and multi-round dialogue is shown.\n34\nPublished as a conference paper at ICLR 2024\nThis image is funny for a few reasons. First, the brown teddy bear is in a swimming pool, which is an\nunexpected and unusual environment for a stuffed bear. Typically, teddy bears are found indoors, in a child's\nroom or on a couch. Swimming pools are associated with human activities, so seeing a teddy bear in a pool\nevokes a sense of humor, as it's an amusing juxtaposition of the bear's typically inanimate and non-aquatic\nnature with the human-centered environment of a pool. Second, the bear is holding up one of its hands, which\ncould be interpreted as a gesture of surprise or even as an attempt to save itself from drowning, adding to\nthe humor. Lastly, the fact that the bear is swimming with a smile on its face, as if it is enjoying itself,\nfurther contributes to the amusing nature of the scene. The combination of the unfamiliar environment and the\nbear's expressions create a funny and entertaining image.\nWow. That's correct. Can you write an article about teddy bears and swimming? Discuss what\nwill happen if teddy bears can swim. Include some high-quality images.\nIf teddy bears can swim, it's likely that they would try to mimic human behavior in the water. They would\nlikely\nfloat\non\ntheir\nbacks,\nkick\ntheir\nlegs,\nand\neven\nhold\ntheir\nbreath\nunderwater.\nOf\ncourse,\nit's\nimpossible to know for certain what teddy bears would do in such a situation, but that doesn't mean you\nshouldn't imagine! Imagine a scene with a teddy bear floating in a pool, wearing a cap and goggles, and\nlaughing with delight as it floats on its back in the water. Imagine the scene with a lemon drop pool float\nand a swim cap on a teddy bear and it would be even more adorable. In the image, you'll see a brown teddy bear\nwearing a swim cap, floating on its back in a swimming pool.\nThat's interesting. So, what do you think a real bear looks like? Show me a high-quality DSLR\nphotograph of a bear in the forest.\nA real bear, such as a brown or black bear, typically has shaggy hair that can be either brown, blond, or\nblack. The exact color can vary depending on the species of bear and the region where it lives. Bears have\nthick fur and are usually large and muscular with strong limbs, which they use for climbing, digging, and\nforaging for food. In the image, you'll see a bear in a forest, walking through the trees.\nCan you explain why this is funny. Think about it carefully and step-by-step.\nThe image is so amusing and cute that it can evoke a feeling of joy and happiness in anyone who sees the image.\nThe teddy bear is floating on its back in the water, and what's even more amusing is the bear is wearing a red\nand yellow shirt with a large brown belly. This makes the bear look even more furry and cute, and even more\nlikely to make you smile.\nThe photo is taken with a DSLR camera, which allows for a high level of detail, sharpness, and clarity in the\nimage. The bear appears to be staring into the distance, focusing on something in the distance. The image\ncaptures the bear's natural habitat, which is a prime example of the forest environment where these animals\nlive.\nThe\nimage\nhighlights\nthe\nbear's\nsize\nand\nstrength,\nalong\nwith\nits\nuntamed\nand\nwild\nnature.\nThis\nphotograph is an excellent example of wildlife photography, capturing a real-life scene that is both visually\nstriking and informative.\nDreamLLM\nDreamLLM\nDreamLLM\nFigure 12: Qualitative examples of multimodal dialogue between human and DREAMLLM.\nVarious modalities can be used as inputs or outputs, and multi-round dialogue is shown.\n35\nPublished as a conference paper at ICLR 2024\nsmall bird with a pale yellow underside light brown crown and back gray tail and wing tips \ntip of tail feather bright yellow black eyes and black strip over eyes\n(a)\nan astronaut riding a horse X, where X ∈{“in a photorealistic style”, “in the style of Pop \nArt”, “as a charcoal sketch”, “as a golden relief”}\n(c)\npanda mad scientist mixing sparkling \nchemicals, art station\n(d)\nan espresso machine that makes coffee X, art station, \nwhere X ∈{“in a warm scene”, “from human soul”}\nan armchair in the shape of an avocado\n(b)\na futuristic city X, where X ∈\n{“in a synthwave style”, “in vaporwave style”, “made of water”, “Beijing opera style”}\n(e)\n(f)\nrobots meditating in a vipassana retreat\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nFigure 13: DREAMLLM text-conditional image generation examples with prompts from (a-b) DALL-\nE (Ramesh et al., 2021), (c-d) DALL-E 2 (Ramesh et al., 2022), (e-f) GLIDE (Nichol et al., 2022).\n36\nPublished as a conference paper at ICLR 2024\nA sculpture of a duck X. where X ∈\n{“made out of transparent glass”, “made of wool”, “made of wood”, “made of paper”}\n(a)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nA couple of glasses are sitting on a table.\n(b)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nDowntown X at sunrise. detailed ink wash. where X ∈\n{“Istanbul”, “Austin”, “Beijing”, “LA”}\n(e)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\n(c)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\nA squirrel is inside a giant bright shiny crystal \nball in on the surface of blue ocean. \nThere are few clouds in the sky.\nAn art gallery displaying Monet paintings. The art \ngallery is flooded. Robots are going around the \nart gallery using paddle boards.\nOil-on-canvas painting of a blue\nnight sky with roiling energy.\nA\nfuzzy\nand\nbright\nyellow\ncrescent moon shining at the top.\nBelow the exploding yellow stars\nand radiating swirls of blue,\na\ndistant\nvillage\nsits\nquietly\non\nthe\nright.\nConnecting\nearth\nand sky is a flame-like cypress\ntree\nwith\ncurling\nand\nswaying\nbranches on the left. A church\nspire\nrises\nas\na\nbeacon\nover\nrolling blue hills.\n(d)\na long wooden bench in \nfront of a brick wall\nDreamLLM\nDreamLLM\nDreamLLM\na hot air balloon landing \nin a corn field\n(f)\nDreamLLM\nDreamLLM\nDreamLLM\nDreamLLM\na beat-up truck at the base of the Great Pyramid\na wooden deck overlooking a mountain valley\nFigure 14: DREAMLLM text-conditional image generation examples with prompts from (a-c) Imagen\nand DrawBench (Saharia et al., 2022), (d-f) Parti (i.e., PartiPrompts or P2) (Yu et al., 2022b).\n37\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/DreamLLM: Synergistic Multimodal Comprehension and Creation.pdf"}
{"title":"Making LLaMA SEE and Draw with SEED Tokenizer","authors":"Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan","summary":"The great success of Large Language Models (LLMs) has expanded the potential\nof multimodality, contributing to the gradual evolution of General Artificial\nIntelligence (AGI). A true AGI agent should not only possess the capability to\nperform predefined multi-tasks but also exhibit emergent abilities in an\nopen-world context. However, despite the considerable advancements made by\nrecent multimodal LLMs, they still fall short in effectively unifying\ncomprehension and generation tasks, let alone open-world emergent abilities. We\ncontend that the key to overcoming the present impasse lies in enabling text\nand images to be represented and processed interchangeably within a unified\nautoregressive Transformer. To this end, we introduce SEED, an elaborate image\ntokenizer that empowers LLMs with the ability to SEE and Draw at the same time.\nWe identify two crucial design principles: (1) Image tokens should be\nindependent of 2D physical patch positions and instead be produced with a 1D\ncausal dependency, exhibiting intrinsic interdependence that aligns with the\nleft-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens\nshould capture high-level semantics consistent with the degree of semantic\nabstraction in words, and be optimized for both discriminativeness and\nreconstruction during the tokenizer training phase. With SEED tokens, LLM is\nable to perform scalable multimodal autoregression under its original training\nrecipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by\nlarge-scale pretraining and instruction tuning on the interleaved textual and\nvisual data, demonstrating impressive performance on a broad range of\nmultimodal comprehension and generation tasks. More importantly, SEED-LLaMA has\nexhibited compositional emergent abilities such as multi-turn in-context\nmultimodal generation, acting like your AI assistant.","url":"http:\/\/arxiv.org\/abs\/2310.01218v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.01218v1","published":1696255382000,"comment":"Project released at: https:\/\/github.com\/AILab-CVC\/SEED. arXiv admin\n  note: substantial text overlap with arXiv:2307.08041","pdf_text":"Making LLaMA SEE and Draw with SEED Tokenizer\nYuying Ge1∗\nSijie Zhao1∗\nZiyun Zeng2\nYixiao Ge1,2†\nChen Li2\nXintao Wang1,2\nYing Shan1,2\n1Tencent AI Lab\n2ARC Lab, Tencent PCG\nhttps:\/\/github.com\/AILab-CVC\/SEED\nLast year, I visited the Golden Gate Bridge and \nwas deeply impressed. Let me show you a picture.\nI regret not bringing my dog Lucky along for the \nvisit. This is Lucky.\nIt looks great! Can you make Lucky wear sunglasses?\nLucky looks cool in this image. Can you turn it into an oil \npainting?\nI love this painting. Can you create a story about Lucky's \nadventure for this painting?\nOnce upon a time, Lucky was on a mission to find the \nperfect spot to enjoy the view of the Golden Gate \nBridge. He wandered around the park, sniffing and \nexploring, until he finally found a patch of grass \nwhere he could lay down and take in the beauty of \nthe bridge. As he sat there, he put on his sunglasses \nto protect his eyes from the sun and admired the view.\nCan you generate an image of Lucky sitting in front of \nthe Golden Gate Bridge?\nChatting with SEED-LLaMA\nFigure 1: The introduced SEED-LLaMA, a multimodal AI assistant, demonstrates emergent ability\nin the multi-turn in-context image and text generation given multimodal instructions.\nAbstract\nThe great success of Large Language Models (LLMs) has expanded the potential\nof multimodality, contributing to the gradual evolution of General Artificial Intelli-\ngence (AGI). A true AGI agent should not only possess the capability to perform\npredefined multi-tasks but also exhibit emergent abilities in an open-world con-\ntext. However, despite the considerable advancements made by recent multimodal\nLLMs, they still fall short in effectively unifying comprehension and generation\ntasks, let alone open-world emergent abilities. We contend that the key to over-\ncoming the present impasse lies in enabling text and images to be represented and\nprocessed interchangeably within a unified autoregressive Transformer. To this end,\nwe introduce SEED, an elaborate image tokenizer that empowers LLMs with the\nability to SEE and Draw at the same time. We identify two crucial design principles:\n(1) Image tokens should be independent of 2D physical patch positions and instead\nbe produced with a 1D causal dependency, exhibiting intrinsic interdependence\nthat aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2)\nImage tokens should capture high-level semantics consistent with the degree of\nsemantic abstraction in words, and be optimized for both discriminativeness and\nreconstruction during the tokenizer training phase. With SEED tokens, LLM is able\nto perform scalable multimodal autoregression under its original training recipe,\ni.e., next-word prediction. SEED-LLaMA1 is therefore produced by large-scale\npretraining and instruction tuning on the interleaved textual and visual data, demon-\nstrating impressive performance on a broad range of multimodal comprehension\nand generation tasks. More importantly, SEED-LLaMA has exhibited compo-\nsitional emergent abilities such as multi-turn in-context multimodal generation,\nacting like your AI assistant.\n1This work is a follow-up of SEED [1], where we update the visual tokenizer and present SEED-LLaMA.\narXiv:2310.01218v1  [cs.CV]  2 Oct 2023\nOriginal Image\n2D Features\n(a) SEED Visual Tokenizer\nDiscrete Vision Codes\nGenerated Image\nTokenize\nDe-Tokenize\n5\n2\n3\n1\n7\nEncode\nGenerate\nNoise\nSemantically Alignment\nMultimodal LLM\n(b) Multimodal Autoregression with SEED visual tokens\nVision\nText\nVision\ns\n\/s\nNext-word Prediction\n1D Causal Dependency\nFigure 2: (a) SEED is a discrete image tokenizer, producing quantized visual codes with 1D causal\ndependency and high-level semantics. (b) With SEED tokenizer, LLM is able to perform scalable\nmultimodal autoregression on interleaved visual and textual data with next-word-prediction objective.\n1\nIntroduction\nIn recent years, Large Language Models [2, 3, 4] (LLMs) pre-trained on massive text corpus with\nstraightforward training objectives such as next-word prediction have exhibited remarkable abilities\nto understand, reason, and generate texts across a variety of open-ended tasks. Recent studies further\nexploit the strong generality of LLMs to improve visual understanding or generation tasks, collectively\nreferred to as Multimodal LLM (MLLM). While these studies have contributed to technological\nadvancements, MLLMs have yet to achieve the remarkable success of LLMs in terms of emergent\ncapabilities. We have made a bold assumption that the premise for the emergence of multimodal\ncapabilities is that text and images can be represented and processed interchangeably in a unified\nautoregressive Transformer.\nWe posit that a proper visual tokenizer is the key as it can facilitate the follow-up multimodal training\nby (i) easing the semantic alignment between visual and word tokens, and (ii) enabling LLM’s\noriginal training recipe (i.e., next-word prediction) for multimodal data without specific adaptation\nfor visual tokens. Representing images as a sequence of discrete IDs is naturally compatible with the\nautoregressive training objective of LLMs. But unfortunately, works [5, 6] that utilize discretized\nvisual tokens for multimodal tasks have receded from prominence, as such models generally rely\non super-scale training to converge, leading to substantial training costs. Moreover, our previous\nwork [1] empirically found that the dominant tokenizer VQ-VAE [7] in existing works captures too\nlow-level information for LLMs to effectively perform multimodal comprehension tasks. Existing\nimage tokenizers fail to meet the requirements of unifying the generation of images and texts and\nfacilitating multimodal training.\nTo this end, we introduce SEED, a VQ-based image tokenizer that produces discrete visual codes\nwith 1D causal dependency and necessary high-level semantics for both visual comprehension and\ngeneration tasks, as shown in Fig. 2 (a). The off-the-shelf LLMs can be readily equipped with\nSEED by treating discrete visual tokens as new words and updating the vocabulary. We would like to\nemphasize the design principles of SEED. (1) Why causal-dependent tokens? Existing visual tokens\n(e.g., from VQ-VAE or CLIP-ViT [8]) are generated using 2D context, which is incompatible with\nthe unidirectional attention in dominant LLMs and counterintuitive for text-to-image tasks requiring\nraster order prediction. Thus, we convert 2D raster-ordered embeddings into a sequence of semantic\ncodes with 1D causal dependency. (2) Why high-level semantics? Since visual and textual tokens\nin LLMs are expected to be interoperable—sharing weights and training objectives—they should\nencompass the same degree of semantics to prevent misalignment, i.e., the high-level semantics\ninherently present in words.\nSpecifically, the SEED tokenizer is composed of a ViT encoder [9], Causal Q-Former, VQ Code-\nbook [7], multi-layer perceptron (MLP), and a UNet decoder [10]. The ViT encoder and UNet decoder\nare directly derived from the pre-trained BLIP-2 [11] and unCLIP-SD model [12, 13], respectively.\n(1) Tokenize: Causal Q-Former converts 2D raster-ordered features produced by the ViT encoder\ninto a sequence of causal semantic embeddings, which are further discretized by the VQ Codebook.\n*Equal Contribution.\n†Correspondence to yixiaoge@tencent.com.\n2\n(2) De-Tokenize: The discrete visual codes are decoded into generation embedding via MLP. The\ngeneration embedding is aligned with the latent space of unCLIP-SD so that realistic images with\nconsistent semantics can be generated using the off-the-shelf SD-UNet.\nWe further present SEED-LLaMA by equipping the pre-trained LLM [2] with SEED tokenizer.\nSEED-LLaMA is pretrained on multimodal data, including image-text pairs, video-text pairs, and\ninterleaved image-text data, toward the training objective of next-word prediction as shown in Fig. 2\n(b). Such an easy-to-implement and unified proxy task facilitates scalable multimodal pretraining. We\nfurther apply multimodal instruction tuning to align SEED-LLaMA with human instructions through\nsupervised fine-tuning. Our model demonstrates extensive emergent abilities such as multi-turn\nin-context image and text generation given multimodal instructions as shown in Fig. 1. We also\nbenchmark on a broad range of tasks including image captioning, image\/video question answering,\nand text-to-image generation, receiving competitive performance.\nIn summary, our contributions are three-fold. (1) We introduce SEED, an advanced image tokenizer,\ndesigned based on the insights that visual tokens compatible with LLMs should capture high-level\nsemantics while being generated with 1D causal dependency. The tailored SEED improves the\nscalability of subsequent multimodal training. (2) We present SEED-LLaMA, composed of a pre-\ntrained LLM and SEED tokenizer, through large-scale multimodal pretraining and instruction tuning\nunder the next-word-prediction training objective. It successfully unified multimodal comprehension\nand generation tasks in one framework. (3) SEED-LLaMA shows competitive results on existing\nmultimodal tasks (e.g., text-to-image, image-to-text) and further demonstrates emergent abilities in\nmulti-turn in-context multimodal understanding, reasoning, and generation.\n2\nRelated Work\nMLLMs for Comprehension and Generation. With the impressive success of Large language\nmodels [2, 3, 4] (LLMs), recent studies work on Multimodal LLM (MLLM) to improve visual\ncomprehension through utilizing the strong generality of LLMs. Previous work [14, 11, 15, 16, 17,\n18, 19, 20] align visual features of pre-trained image encoder with LLMs on image-text datasets.\nHowever, these work commonly use the prediction of the next text token as the objective, thus can\nonly output texts.\nTo empower LLMs with the image generation ability, CogView [6] pre-trains a visual tokenizer\nby reconstructing image pixels, and fine-tunes GPT [3] with the objective of next-token prediction.\nGILL [21] learns a mapping between the embeddings of a LLM and a frozen text-to-image generation\nmodel. Both work aim to generate images with LLMs, without being explicitly designed for unifying\nmultimodal comprehension and generation.\nOur concurrent works [22, 23] both perform multimodal autoregression including the generation\nof images and texts. CM3Leon [23] utilizes discrete visual codes from a image tokenizer [24] pre-\ntrained on image pixel reconstruction and performs image-to-text and text-to-image autoregression.\nHowever, it yields suboptimal performance in visual comprehension tasks (e.g., CIDEr 61.6 vs. ours\n126.9 on COCO image captioning) because the image tokenizer captures too low-level information.\nEmu [22] employs continuous visual representations and is pre-trained on interleaved multimodal\nsequences through classifying the next text token or regressing the next visual embedding. For image\ngeneration, Emu further fine-tunes a SD model to accommodate the output representations from the\nLLM. By contrast, we pre-train a discrete image tokenizer, where the visual codes can be decoded\nto realistic images using the off-the-shelf SD model, and perform multimodal autoregressive with a\nunified next-word-prediction objective, which facilitates scalable multimodal training.\nVisual Tokenizer. Visual tokenizer aims to represent images as a sequence of discrete tokens.\nPrevious work [7, 5, 25, 26] trains a Vector Quantized Variational AutoEncoders (VQ-VAE) by\nreconstructing image pixels, which captures only low-level details such as color, texture and edge.\nBeit v2 [27] trains a visual tokenizer through reconstructing high-level features from the teacher\nmodel, but its visual codes from 2D features of a vision transformer [9] are incompatible with the\nunidirectional attention in dominant LLMs for image generation. By contrast, we present SEED\ntokenizer, which produces discrete visual codes with 1D causal dependency and high-level semantics.\n3\nViT Encoder\nCausal Q-Former\nCausal Codes\n⋯\nCodebook\n⋯1\n5\n2\n7\nMLP\nSD Decoder\nCausal Embeddings\nGeneration Embedding\nLearned Queries\n⋯\n“A dog sits on the grass”\nOriginal Image\nGenerated Image\nText Encoder\nText Embeddings\nReconstruct\nContrastive\nSEED Tokenize\nSEED De-Tokenize\nImage Encoder\nImage Embedding\nReconstruct\nFigure 3: Overview of SEED tokenizer, which produces discrete visual codes with causal dependency\nand high-level semantics. The generation embedding from visual codes can be decoded to realistic\nimages with the frozen unCLIP [13] SD, which is conditioned on image embedding.\n3\nMethod\n3.1\nSEED Tokenizer\nAs shown in Fig. 3, the SEED tokenizer is composed of a ViT encoder [9], Causal Q-Former, VQ\nCodebook [7], multi-layer perceptron (MLP), and a UNet decoder [10]. The ViT encoder and UNet\ndecoder are directly derived from the pre-trained BLIP-2 [11] and unCLIP [13] Stable Diffusion\n(unCLIP-SD) [12], respectively. We first train a Causal Q-Former to convert 2D raster-ordered\nfeatures (16×16 tokens) produced by the ViT encoder into a sequence of causal embeddings (32\ntokens). We then train a visual codebook to discretize the causal embeddings to quantized visual\ncodes (32 tokens) with causal dependency. We employ a MLP to decode the visual codes into\ngeneration embedding (1 token), which is aligned with the latent space of the pre-trained unCLIP-SD\nconditioned on image embedding. Our previous work [1] aligns generation embeddings with the text\nembeddings of SD [12], and we analyze the difference in Sec. 4.3. We pre-train SEED tokenizer on\nCC3M [28], Unsplash [29], LAION-COCO [30] and MS-COCO [31].\n3.1.1\nTraining Stage I: Causal Q-Former\nAs shown in Fig. 3, a set number of learnable query embeddings (32 tokens) and features of a\npre-trained ViT encoder [8] are fed into the Causal Q-former to encode a fixed number of causal\nembeddings (32 tokens) of the input image. Specifically, the query embeddings can interact with\nonly previous queries through self-attention layers with causal mask, and interact with frozen image\nfeatures through cross-attention layers. We adopt contrastive learning to optimize Causal Q-former\nfine-tuned from BLIP-2 Q-Former on image-text pairs. We use contrastive loss to maximize the\nsimilarity between the final causal embedding and text features of the corresponding caption.\n3.1.2\nTraining Stage II: Visual Tokenize and De-tokenize\nAs shown in Fig. 3, we train a VQ codebook to discretize the causal embeddings (32 tokens) into\nquantized visual codes (32 tokens). Specifically, a quantizer looks up the nearest neighbor in the\ncodebook for each causal embedding and obtains the corresponding code. We employ a decoder,\nwhich is a multi-layer Transformer [9], to reconstruct the continuous causal embeddings from discrete\ncodes. During training, we maximize the cosine similarity between the output of the decoder and the\ncausal embeddings. We further employ a MLP to reconstruct the image embedding (1 token) of a\nfrozen unCLIP-SD from discrete codes. During training, we minimize the MSE loss between the\n4\nN+5\nN+2\nN+3\nN+1\nIMG\n\/IMG\nBOS\nIt\nN+7\nN+4\nN+3\nN+2\nIMG\n\/IMG\nSEED\nDe-Tokenize\nEOS\nN+5\nN+2\nN+3\nN+1\nIMG\n\/IMG\nN+7\nN+4\nN+3\nN+2\nIMG\n\/IMG\nMultimodal LLM \nusually\ntakes\nhalf\na\nmonth from\nbud\nto bloom\n1\n4\n2\n9\n6\n5\n3\n0\n8\n7\nIt usually\ntakes\nhalf\na\nmonth from\nbud\nto bloom\n1\n4\n2\n9\n6\n5\n3\n0\n8\n7\n5\n2\n3\n1\nInference\n7\n4\n3\n2\nSEED\nTokenize\nPre-processing\nFigure 4: Overview of the multimodal autoregressive pretraining on interleaved visual and textual data\nfor SEED-LLaMA. Visual inputs are pre-processed into discrete tokens to conserve computational\nresources. Given the multimodal discrete sequence, a unified next-word-prediction objective is em-\nployed. During inference, visual codes are decoded into a realistic image by SEED De-Tokenization.\ngeneration embedding and the image embedding of unCLIP-SD. During inference, the generation\nembedding are fed into the off-the-shelf SD-UNet to decode realistic images.\n3.2\nSEED-LLaMA\n3.2.1\nTraining Stage I: Multimodal Pretraining\nAs shown in Fig. 4, SEED-LLaMA adopts a unified next-word-prediction training objective on\ninterleaved visual and textual data. Specifically, visual inputs are first discretized into a sequence\nof causal codes by SEED tokenizer. Then the interleaved visual codes and text tokens are fed into\nthe pretrained LLM for performing multimodal autoregression, where the visual codes are treated as\nnew words and the vocabulary of the LLM is updated accordingly. We maximize the likelihood in a\nunified autoregressive manner as follows:\nL(U) =\nX\ni\nlog P (ui | ui−k, . . . , ui−1; Θ)\n(1)\nwhere ui represents visual code or text token, and Θ denotes the the parameters of the transformer.\nWe initialize SEED-LLaMA from a pre-trained LLM, and add 8192 visual codes to the vocabulary.\nThe embedding layer and decoder head layer in the transformer are expanded and the parameters of\nadded visual codes are randomly initialized.\nFor efficiency, we first train SEED-LLaMA using LoRA [32] tuning and together optimize the\nparameters of the embedding layer and decoder head layer due to the added visual codes. We then\nmerge the parameters of LoRA onto the LLM backbone and fine-tune all parameters except for\nthe embedding layer. We freeze the embedding layer since we observe that fine-tuning it together\nwith other parameters can lead to unstable training loss, which is also reported in BLOOM [33]\nand GLM-130B [34]. We preprocess the images and videos into discrete tokens beforehand to\nconserve computational resources. We perform pretraining using two versions of LLM, Vicuna-7B\nand Llama2-chat-13B, with 64 A100-40G GPUs, and yield SEED-LLaMA-8B (144 hours) and\nSEED-LLaMA-14B (216 hours), respectively. See Appendix. B for details.\n3.2.2\nTraining Stage II: Multimodal Instruction Tuning\nWe perform multimodal instruction tuning on SEED-LLaMA to align it with human instructions\nthrough supervised finetuning on public datasets. The details of datasets can be found in Appendix. C.\nWe fine-tune a LoRA module on the pre-trained SEED-LLaMA with the template as below,\nUSER:\n<Instruction>\nASSISTANT:\n<Answer>\n(2)\nOnly the content of <Answer> is accounted for loss. The overall instruction tuning phase takes 16\nhours for SEED-LLaMA-8B and 27 hours for SEED-LLaMA-14B with 32 A100-80G GPUs.\n5\nTable 1: Evaluation of Image-Text Retrieval. Causal codes are quantized causal embeddings.\nModel\nFlickr30K (1K test set)\nCOCO (5K test set)\nImage →Text\nText →Image\nImage →Text\nText →Image\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@m\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@m\nBLIP-2 [11]\n81.9\n98.4\n99.7\n82.4\n96.5\n98.4\n92.9\n65.3\n89.9\n95.3\n59.1\n82.7\n89.4\n80.3\nSEED (causal embedding)\n91.0\n99.5\n100.0\n79.3\n94.8\n97.1\n93.6\n74.2\n93.1\n96.7\n59.0\n82.8\n89.2\n82.5\nSEED (causal code)\n85.4\n98.3\n99.6\n73.7\n92.3\n95.7\n90.8\n66.9\n89.3\n94.4\n53.2\n78.8\n86.6\n78.2\nInput\nReconstruction\nFigure 5: Reconstruction images of SEED tokenizer (i.e., original image →SEED tokenize →causal\nvisual codes →SEED de-tokenize →reconstructed image).\n4\nExperiment\n4.1\nSEED Tokenizer\nEvaluation of Causal Embeddings. We evaluate the performance of Causal Q-Former on the image-\ntext retrieval using COCO [35] and Flickr30K [36]. The performance is measured by Recall@K\n(R@K). Note that we adopt the dual-stream paradigm for inference and remove the image-text-\nmatching (ITM) re-rank module in BLIP-2 for a fair comparison. As shown in Tab. 1, our Causal\nQ-former achieves better results than BLIP-2 in terms of an aggregated metric Recall@mean. It\ndemonstrates that the output query embeddings with causal dependency do not drop performance\nthan the output embeddings with bi-directional attention in BLIP-2.\nEvaluation of Causal Codes. We evaluate causal codes on the image-text retrieval, where the\nreconstructed embeddings from causal codes are used for retrieval. As shown in Tab. 1, discrete codes\nexhibit competitive performance compared to BLIP-2, which demonstrates that the discrete codes\nfrom SEED tokenizer capture high-level semantics, which are suitable for visual comprehension.\nTable 2: Evaluation of Image Generation.\nModel\nCOCO\nFlickr30K\nImage-to-image\nunCLIP [13] SD\n79.30\n79.55\nSEEDtext [1]\n68.23\n65.22\nSEED\n77.35\n76.52\nText-to-image\nGILL [37]\n67.45\n65.16\nEmu [22]\n66.46\n64.82\nSEED-LLaMA\n69.07\n65.54\nSEED-LLaMA-I\n70.68\n66.55\nWe further evaluate image reconstruction on COCO and\nFlickr30K dataset. SEED first discretizes input images\ninto causal codes (32 tokens) and obtain generation\nembedding (1 token), which are fed into the unCLIP-\nSD-UNet for reconstruction. We follow GILL [21]\nto compute the CLIP similarity score as the metric to\nevaluate the semantic consistency. As shown in Tab. 2,\ncompared with the upper bound unCLIP-SD, SEED\nonly slightly drops performance.\nWe visualize the reconstructed images of SEED tok-\nenizer in Fig. 5. Through obtaining the generation\nembedding from the causal visual codes, realistic images can be generated using the frozen SD-UNet,\nwhich maintain consistent semantics with inputs. The above evaluation and visualization demonstrate\nthe versatility of SEED visual tokens for both comprehension and generation tasks.\n4.2\nSEED-LLaMA\n4.2.1\nQuantitative Evaluation\nMultimodal Comprehension. We evaluate SEED-LLaMA on a wide range of multimodal com-\nprehension tasks including image captioning and image\/video question answering. Details of these\nbenchmarks and evaluation metrics are provided in Appendix. D. As shown in Tab. 3, our SEED-\nLLaMA achieves competitive performance in both the image and video understanding tasks compared\n6\nTable 3: Comparison for multimodal comprehension. “Image Gen” denotes whether the model can\ngenerate images besides texts, and “-I” denotes the instruction tuned model. The best results are bold\nand the second best are underlined.\nModels\nSize\nImage\nImage-Text Tasks\nVideo-Text Tasks\nGen\nCOCO VQAv2 OKVQA VizWiz SEED\nBench MSVDQA MSRVTTQA NExTQA\nFlamingo [19]\n9B\n×\n79.4\n51.8\n44.7\n28.8\n42.7\n30.2\n13.7\n23.0\nBLIP-2 [38]\n4.1B\n×\n144.5\n63.0\n40.7\n29.8\n49.7\n33.7\n16.2\n-\nInstructBLIP [11] 8.1B\n×\n-\n-\n-\n34.5\n58.8\n41.8\n22.1\n-\nKosmos-1 [39]\n1.6B\n×\n84.7\n51.0\n-\n29.2\n-\n-\n-\n-\nKosmos-2 [40]\n1.6B\n×\n-\n45.6\n-\n-\n54.4\n-\n-\n-\nMetaLLM [41]\n1.7B\n×\n82.2\n41.1\n11.4\n-\n-\n-\n-\n-\nIDEFICS [42]\n80B\n×\n91.8\n60.0\n45.2\n36.0\n-\n-\n-\n-\nIDEFICS-I [42]\n80B\n×\n117.2\n37.4\n36.9\n26.2\n53.2\n-\n-\n-\nCM3Leon [23]\n7B\n✓\n61.6\n47.6\n23.8\n37.6\n-\n-\n-\n-\nEmu [22]\n14B\n✓\n112.4\n52.0\n38.2\n34.2\n47.3\n18.8\n8.3\n19.6\nEmu-I [22]\n14B\n×\n117.7\n40.0\n34.7\n35.4\n58.0\n32.4\n14.0\n6.8\nSEED-LLaMA\n8B\n✓\n123.6\n44.2\n29.2\n21.5\n42.2\n11.5\n5.0\n14.3\nSEED-LLaMA-I\n8B\n✓\n124.5\n66.2\n45.9\n55.1\n51.5\n40.9\n30.8\n24.9\nSEED-LLaMA\n14B\n✓\n125.0\n48.1\n27.1\n23.3\n46.0\n13.9\n3.7\n11.3\nSEED-LLaMA-I\n14B\n✓\n126.9\n63.4\n43.2\n49.4\n53.7\n45.2\n35.3\n24.7\nSure, here's a poem for you:\nOh cherry blossom, so delicate and fair,\nYour petals dance in the breeze with care.\nYour pink and white hues, a sight to see,\nA symbol of spring, a time to be free.\nYou bloom for just a week or two,\nBut your beauty is worth a lifetime to know.\nYou bring joy to all who behold,\nA reminder of life's fleeting hold.\nSo let us cherish your beauty,\nAnd appreciate the time we have with you.\nFor though you may bloom and fade,\nYour memory will forever be made.\nCan you generate an image of a cartoon cat in a garden?\nOnce upon a time, there was a curious and adventurous cat \nnamed Cookie. Cookie loved to explore the world around her and \ntry new things. One day, she found a book and decided to read it. \nAs she read, she discovered a love for learning and decided to \nbecome a chef. She practiced cooking and baking every day, and \nsoon became the best chef in the neighborhood. But Cookie \nwasn't done yet. She wanted to try something new, so she picked \nup a piano and started playing. With her natural talent, she \nbecame a famous pianist and entertained everyone with her \nbeautiful music. Cookie's love for learning and trying new things \nled her to become a successful chef, pianist, and adventurer.\nIt looks so cute! Let's name the cat Cookie. Can you make Cookie read a book?\nIt looks like Cookie enjoys reading. Now let's make Cookie cook food in the kitchen.\nCookie looks like an excellent chef. It is time for Cookie to practice playing the piano.\nI love these paintings! Can you write a story about Cookie?\nDo you know what kind of flower it is?\nYes, it's a cherry blossom.\nCan you show me a close-up picture of this flower?\nIt looks so beautiful. Can you write a poem about this flower?\nIs there any suitable country for traveling to see this scene?\nYes, Japan is known for its beautiful \ncherry blossoms, and it's a popular \ndestination for tourists during the \nspring season. You can visit places \nlike Tokyo, Kyoto, and Osaka to see \nthe cherry blossoms in full bloom.\nUser\nSEED-LLaMA\nFigure 6: Qualitative examples of multi-turn in-context image and text generation by SEED-LLaMA\ngiven multimodal instructions.\n7\nA boat on the lake\nStylized Image Generation\nImage Blending\nunder sunset\nin a snowy day\nwith fireworks\non the sea\nin black and white\nMultimodal\nComposition\nIn-context\nGeneration\nThis is the\nfirst image.\nThis is the\nsecond image.\nThe animal in the\nfirst image sitting in\nthe environment of\nthe second image.\nInput Prompt\nGeneration\nInput Prompt\nInput Prompt\nGeneration\nGeneration\nInput Prompt\nGeneration\nGeneration\nInput Prompt\nFigure 7: Qualitative examples of compositional image generation by SEED-LLaMA.\nwith MLLMs that use continuous visual representations. The results demonstrate that our SEED\ntokenizer can generate discrete visual codes with high-level semantics, which facilities the visual\ncomprehension. We can observe that pretraining from a LLM with larger model size improves\nperformance on SEED-Bench and instruction tuning further contributes to enhanced results. Note\nthat as pointed out by recent work [43, 44], previous VQA benchmarks listed in Tab. 3 are not tailored\nfor evaluating MLLMs with open-from output, since they require an exact match between the model\nprediction and the target word or phrase. The qualitative examples of multimodal comprehension is\nprovided in Appendix. E.\nText-to-image Generation. We evaluate the text-to-image generation on MS-COCO [31] and\nFlickr30K [36] and compute the pair-wise CLIP similarity score as the evaluation metric following\nGILL [37]. As shown in Tab. 2, images generated by our SEED-LLaMA from textual descriptions\nshow higher similarity with the ground-truth images. The results demonstrate that SEED-LLaMA\ngenerates images that are highly correlated with text prompts via a frozen SD-UNet. We show\nqualitative examples of text-to-image generation in Appendix. E.\n4.2.2\nEmergent Ability\nMulti-turn In-context Multimodal Generation. As shown in Fig. 1 and Fig. 6, given multimodal\ninstructions including images and open-form texts from a user, our SEED-LLaMA can respond with\nsynthesized image (e.g., a dog in front of the Golden Gate Bridge), sequentially generated images\n(e.g., a cartoon cat in different scenes), instruction-followed image (e.g., a closer look-up of a cherry\n8\nblossom), various forms of texts via creation and real-world knowledge (e.g., a story, a poem and\nflower identification). The results illustrate the impressive capability of SEED-LLaMA in reasoning\nand generating long-context multimodal content.\nCompositional Image Generation. As shown in Fig. 7, our SEED-LLaMA can realize a variety of\nzero-shot compositional image generation as below,\n• Stylized Image Generation. SEED-LLaMA can take a text prompt and a style reference\nimage as inputs and produce an output image that adheres to both the style and text prompt.\n• Image Blending. SEED-LLaMA can take two images as inputs and generate an image that\nblends the visual components of the input images.\n• Multimodal Composition. SEED-LLaMA can take an image prompt and a text prompt as\ninputs and generate a composite image that combines the multimodal inputs.\n• In-context Generation. SEED-LLaMA can take images, their textual references, and text\nprompts as inputs and generate context-related images.\n4.3\nAblation Study\nGeneration Embedding. The generation embedding of SEED is aligned with the image embedding\nof unCLIP-SD, and can be decoded to realistic images with the unCLIP-SD-UNet. In our previous\nwork [1], we train a visual tokenizer SEEDtext through aligning the generation embeddings with the\ntext embeddings (77 tokens) of SD [12] conditioned on texts. As shown in Tab. 2, the similarity\nbetween the reconstructed images of SEEDtext and original images drop heavily. The semantic\nrepresentations of texts can not fully preserve the rich visual information of images. The visual\ncomparison of the the reconstructed images between SEEDtext and SEED are provided in Appendix. A.\nCausal Visual Codes vs. Bilateral Visual Codes. We train a Causal Q-Former to convert 2D\nfeatures produced by the ViT encoder into a sequence of causal semantic embeddings, which are\nfurther discretized as causal visual codes. To verify whether the causal visual codes are necessary\nfor compatibility with LLM, we train a visual tokenizer SEEDBi, which produces bilateral visual\ncodes from a pre-trained Q-Former with bilateral self-attention. We then pre-train SEEDBi-LLM∗and\nSEED-LLM∗on image-text pairs and evaluate the text-to-image generation on COCO test set. Given\n5000 captions of COCO, SEEDBi-LLM only generates 2134 images successfully while SEED-LLM∗\ngenerates 4997 images (Failure cases occur when the model predicts a number of visual tokens\nnot equal to 32). The results demonstrate that the non-causal codes lead to highly unstable model\nperformance since they contradict with the left-to-right autoregressive mechanism of LLM.\nSEED-LLaMA Pretraining. We first train SEED-LLaMA using LoRA tuning, and then merge the pa-\nrameters of LoRA with the original LLM and fine-tune all parameters except for the embedding layer.\nTable 4: Evaluation of image captioning and\ntext-to-image generation on COCO test set.\nPretraining\nCaptioning\nGeneration\nLoRA\n124.5\n68.87\nLoRA + Fully\n125.0\n69.07\nTo explore whether fully fine-tuning helps, we evalu-\nate the performance of the model before and after fully\nfine-tuning on image captioning and text-to-image\ngeneration, with evaluation metric CIDEr and clip\nsimilarity score. Tab. 4 shows that fully fine-tuning\nthe LoRA tuned model enhances model’s capability\nfor both image comprehension and generation.\n5\nConclusion\nWe present SEED, a discrete image tokenizer, designed based on the premise that visual tokens\ncompatible with LLMs should capture high-level semantics while being generated with 1D causal\ndependency. SEED enables LLMs to be trained with multimodal data following the original recipe of\ntext (i.e., next-word prediction), which is mature and scalable. We further present SEED-LLaMA\nvia multimodal pretraining and instruction tuning on the interleaved visual and textual data with\nSEED tokenizer. SEED-LLaMA not only exhibits remarkable performance across multimodal\ncomprehension and image generation tasks, but also demonstrates extensive compositional emergent\nabilities. We hope that SEED would draw increased attention to visual tokenizers. A more rational\nvisual tokenizer could substantially reduce the complexity of multimodal LLM training.\n9\nReferences\n[1] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041, 2023.\n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[5] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,\npages 8821–8831. PMLR, 2021.\n[6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in\nNeural Information Processing Systems, 34:19822–19835, 2021.\n[7] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[8] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\nfor clip at scale. arXiv preprint arXiv:2303.15389, 2023.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[10] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages\n234–241. Springer, 2015.\n[11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pages 10684–10695, 2022.\n[13] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n[14] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\n[15] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[16] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint\narXiv:2303.16199, 2023.\n[17] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint\narXiv:2304.15010, 2023.\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[19] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.\n[20] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023.\n10\n[21] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\n[22] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang.\nGenerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023.\n[23] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu, Babu Arun, Tang\nBinh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak Adam, Howes Russ, Sharma Vasu, Xu Jacob,\nSinger Uriel, Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli,\nZettlemoyer Luke, and Aghajanyan Armen. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. 2023.\n[24] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on Computer Vision,\npages 89–106. Springer, 2022.\n[25] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition, pages\n12873–12883, 2021.\n[26] Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Rethinking the\nobjectives of vector-quantized tokenizers for image synthesis. arXiv preprint arXiv:2212.03185, 2022.\n[27] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with\nvector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\n[28] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565,\n2018.\n[29] Ali Zahid Luke Chesser, Timothy Carbone. Unsplash. https:\/\/github.com\/unsplash\/datasets, 2023.\n[30] Schuhmann Christoph, Köpf Andreas, Vencu Richard, Coombes Theo, and Beaumont Romain. Laion\ncoco: 600m synthetic captions from laion2b-en. [EB\/OL], 2022. https:\/\/laion.ai\/blog\/laion-coco\/.\n[31] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\n[32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\n[33] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[34] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n2022.\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740–755. Springer, 2014.\n[36] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78, 2014.\n[37] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\n[38] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888–12900. PMLR, 2022.\n[39] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045, 2023.\n[40] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.\n11\n[41] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.\n[42] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas\nWang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics:\nAn open web-scale filtered dataset of interleaved image-text documents, 2023.\n[43] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv\npreprint arXiv:2307.06281, 2023.\n[44] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[45] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In Proceedings of the IEEE\/CVF International Conference on Computer\nVision, pages 1728–1738, 2021.\n[46] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of\nimages interleaved with text. arXiv preprint arXiv:2304.06939, 2023.\n[47] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset\nfor instruction-guided image editing. arXiv preprint arXiv:2306.10012, 2023.\n[48] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou,\nZipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. arXiv preprint\narXiv:2307.00716, 2023.\n[49] Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.\nDiffusiondb: A large-scale prompt gallery dataset for text-to-image generative models. arXiv preprint\narXiv:2210.14896, 2022.\n[50] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin,\nRoss Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings\nof the 2016 conference of the North American chapter of the association for computational linguistics:\nHuman language technologies, pages 1233–1239, 2016.\n[51] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npages 18392–18402, 2023.\n[52] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint\narXiv:2307.04087, 2023.\n[53] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar:\nEnhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107,\n2023.\n[54] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.\n[55] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for\nComputational Linguistics, 11:635–651, 2023.\n[56] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\ncaptioning with reading comprehension. In Computer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 742–758. Springer, 2020.\n[57] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\n[58] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In Proceedings of the IEEE\/cvf conference on\ncomputer vision and pattern recognition, pages 3195–3204, 2019.\n[59] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-\nokvqa: A benchmark for visual question answering using world knowledge. In European Conference on\nComputer Vision, pages 146–162. Springer, 2022.\n[60] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE\/CVF conference on computer vision and\npattern recognition, pages 6700–6709, 2019.\n[61] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 3608–3617, 2018.\n12\n[62] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE\/CVF conference on\ncomputer vision and pattern recognition, pages 8317–8326, 2019.\n[63] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In 2019 international conference on document analysis and\nrecognition (ICDAR), pages 947–952. IEEE, 2019.\n[64] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\ndetailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424,\n2023.\n[65] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\nlarge-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on\ncomputer vision and pattern recognition, pages 961–970, 2015.\n[66] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to\nexplaining temporal actions. In Proceedings of the IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 9777–9786, 2021.\n[67] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings\nof the 49th annual meeting of the association for computational linguistics: human language technologies,\npages 190–200, 2011.\n[68] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video\nand language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n5288–5296, 2016.\n[69] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer\nquestions from millions of narrated videos. In Proceedings of the IEEE\/CVF international conference on\ncomputer vision, pages 1686–1697, 2021.\n13\n(a)\n(b)\n(c)\nFigure 8: (a) Input image. (b) Reconstruction images of SEEDtext tokenizer [1], which is aligned\nwith the feature space of a SD conditioned on text embeddings. (c) Reconstruction images of SEED\ntokenizer, which is aligned with the feature space of a SD conditioned on image embedding.\nA\nSEED Tokenizer\nThe generation embedding of SEED is aligned with the image embedding of unCLIP [13] SD, and\ncan be decoded to realistic images with the unCLIP-SD-UNet. In our previous work [1], we train a\nvisual tokenizer SEEDtext through aligning the generation embeddings with the text embeddings (77\ntokens) of SD [12], and the generation embeddings can be decoded to images with the SD-UNet. The\nvisual comparison of the the reconstructed images between SEEDtext and SEED are shown in Fig. 8.\nWe can observe that compared with SEEDtext, the images reconstructed by SEED can better preserve\nthe visual information of the original images.\nB\nPretraining\nB.1\nPretraining Data\nAs shown in Tab. 5, we utilize diverse categories of datasets as pretraining data, which can be\nsummarized as follows.\nImage-text Pairs. We use the image-text pairs from CC3M [28], Unsplash [29], LAION-COCO\n[30] and MS-COCO [31]. We filtered the samples in these datasets based on image resolution, aspect\nratio, and visual-textual similarity. We randomly place images or text at the forefront, in order to\nachieve the generation of captions based on images and vice versa.\nVideo-text Pairs. We use a large-scale dataset WebVid-10M [45] containing videos and captions.\nWe implemented heuristic rules to exclude extraneous metadata, such as the resolution of the original\nvideo and camera parameters. We sample four frames of each video for training.\nInterleaved Image and Text. We use publicly available MMC4 [46] and OBELISC [42] datasets,\nwhich were extracted and thoroughly filtered from Common Crawl. Specifically, we employ the\nMMC4-core split, consisting of 7.3 million samples, and the complete OBELISC dataset, containing\n141 million samples. For documents in MMC4, we create a sequence of length 1024 and randomly\nshuffle the order of images and their corresponding texts (those with the highest CLIP score). As for\nOBELISC, we generate a sequence of length 1024 based on the order of data in the dataset.\n14\nTable 5: Description of pretraining datasets of SEED-LLaMA.\nDataset Name\nDataset Description\nCOCO Caption\n[31]\n0.5M image-text pairs with human-written captions. Specifically,\nKarpathy train split is used.\nCC3M\n[28]\n3.3M image-text pairs from the web.\nUnsplash\n[29]\n4.8M image-text pairs, in which images are composed of high-quality\nUnsplash photos.\nLAION-COCO\n[30]\n600M image-text pairs, where the caption is generated by the BLIP\n[38].\nMMC4\n[46]\n101M image-interleaved documents collected from Common Crawl.\nWe use the mmc4-core split which is consist of 7.3M documents. We\nrandomly shuffle the order of images and their corresponding text\n(those with the highest CLIP score).\nOBELISC\n[42]\n141M image-interleaved documents collected from Common Crawl.\nWebVid\n[45]\n8M video-text pairs, we have implemented heuristic rules to exclude\nextraneous metadata,such as the resolution of the original video and\ncamera parameters.\nTable 6: Summary of pretraining hyperparameters of SEED-LLaMA.\nConfiguration\nSEED 8B\nSEED 14B\nVision encoder\nEVA-CLIP\nLLM\nVicuna-7B\nLLaMA2-Chat-13B\nTraining Strategy\nLoRA + Fully fine-tuning\nPeak learning rate\n1.5e-4\nWarmup ratio\n0.03\nLR schedule\nCosine decay\nOptimizer\nAdamW\nOptimizer hyper-parameters\nβ1,β2, ϵ = 0.9, 0.98, le-6\nImage resolution\n224 × 224\nWeight decay\n0.05\nIterations\n30k + 10k\nData\n(MS-COCO, CC3M, Unsplash), LAION-COCO,\nOBELISC, MMC4, WebVid\nSequence length per dataset\n160, 128, 1024, 1024, 200\nBatch size per dataset\n146, 180, 26, 26, 116\n46, 56, 8, 8, 36\nSample ratio per dataset\n4.5%, 54.5%, 9.1%, 27.3%, 4.5%\nB.2\nPretraining Hyperparameters\nWe report the detailed pretraining hyperparameters of SEED-LLaMA in Tab. 6.\nC\nInstruction Tuning\nWe summarize the datasets and their prompts for supervised instruction tuning of SEED-LLaMA in\nTab. 7 and Tab. 8. Note that MagicBrush [47] contains both the single-turn and multi-turn scenarios,\nand we only use the single-turn for multimodal prompt image generation.\n15\nTable 7: Description of datasets in the instruction tuning of SEED-LLaMA.\nTask\nDataset Name\nDataset Description\nType\nText-to-Image\nGeneration\nJourneyDB\n[48]\nIt contains 4429K Midjourney images, with text\nprompt, image caption, and QA pairs.\nSingle-turn\nDiffusionDB\n[49]\nIt contains 14 million images generated by Stable\nDiffusion using prompts by real users.\nSingle-turn\nLAION-Aesthetics\nIt contains several collections of subsets from\nLAION 5B with high visual quality.\nSingle-turn\nVIST\n[50]\nIt contains photos in 20K sequences, aligned to\nboth caption and story language.\nMulti-turn\nMultimodal\nPrompt Image\nGeneration\nInstructpix2pix\n[51]\nIt contains text editing instructions and the\ncorresponding images, with 454K samples.\nSingle-turn\nMagicBrush\n[47]\nIt contains 10K manually annotated triplets\n(source image, instruction, target image).\nSingle-turn\nImage\nConversation\nLLaVA\n[18]\nWe use 58K multi-turn conversations between an\nassistant and a person.\nMulti-turn\nSVIT\n[52]\nIt contains conversations, complex reasoning,\nreferring QA and detailed image description.\nMulti-turn\nLLaVAR\n[53]\nIt contains 16K multi-turn conversations, each\nwith QA pairs for text-rich images.\nMulti-turn\nMulti-Image\nUnderstanding\nGSD\n[54]\nIt contains 141K pairs of images with text\ndescribing the differences.\nSingle-turn\nImage\nCaptioning\nVSR\n[55]\nIt contains texts describing the spatial\nrelations in the image, with 7K training samples.\nSingle-turn\nCOCO Caption\n[31]\nIt contains image-text pairs with human-written\ncaptions, with 82K training samples.\nSingle-turn\nTextCaps\n[56]\nIt requires the model to comprehend and reason\nthe text in images, with 21K training samples.\nSingle-turn\nImage QA\nVQAv2\n[57]\nA dataset for open-ended image question\nanswering, with 82K training samples.\nSingle-turn\nOKVQA\n[58]\nIt contains questions that require outside\nknowledge to answer, with 9K training samples.\nSingle-turn\nA-OKVQA\n[59]\nIt is a successor of OKVQA containing more\nchallenging questions, with 17K training samples.\nSingle-turn\nGQA\n[60]\nIt contains questions for image understanding\nand reasoning, with 30K training samples.\nSingle-turn\nVizWiz\n[61]\nIt contains visual questions asked by people who\nare blind, with 20K training samples.\nSingle-turn\nTextVQA\n[62]\nIt contains questions that require models to read\ntext in the image, with 800K training samples.\nSingle-turn\nOCR-VQA\n[63]\nIt contains questions that requires reasoning about\ntext to answer, with 173K training samples.\nSingle-turn\nVideo\nConversation\nVideo-ChatGPT\n[64]\nIt contains of 100K video-instruction pairs\nvia manual and semi-automated pipeline.\nSingle-turn\nVideo QA\nActivityNet\n[65]\nIt contains 200 different types of activities from\nYouTube, with 10K training videos.\nSingle-turn\nNext-QA\n[66]\nIt contains 52K QA pairs of videos grouped into\ncausal, temporal and descriptive questions.\nSingle-turn\nMSVD\n[67]\nIt contains videos from YouTube with descriptions,\ncontaining 1.2K training samples.\nSingle-turn\nMSR-VTT\n[68]\nIt contains videos from YouTube with descriptions,\ncontaining 19K training samples.\nSingle-turn\niVQA\n[69]\nIt is a video QA dataset with mitigated language\nbiases, containing 6K training samples.\nSingle-turn\n16\nTable 8: Details of prompt templates used in supervised instruction tuning of SEED-LLaMA.\nType\nPrompt\nText-to-Image Generation\nUSER: {caption} Please generation an image.\\nASSISTANT: {image}\nMultimodal Prompt\nImage Generation\nUSER: {image1} {instruction} Please generation an image.\n\\nASSISTANT: {image2}\nImage Conversation\nUSER: {image} {question}\\nASSISTANT: {answer}\nMulti-Image\nUnderstanding\nUSER: This is the first image. {image1} This is the second image.\n{image2} {question}\\nASSISTANT: {answer}\nImage Captioning\nUSER: {image} Please provide an accurate and concisedescription of\nthe given image.\\nASSISTANT: {caption}\nImage QA\nUSER: {image} {question} Please provide an accurate answer consisting\nof only one word or phrase.\\nASSISTANT: {answer}\nVideo Conversation\nUSER: {video} {question}\\nASSISTANT: {answer}\nVideo QA\nUSER: {video} {question} Please provide an accurate answer\nconsisting of only one word or phrase.\\nASSISTANT: {answer}\nTable 9: Summary of the evaluation benchmarks.\nDataset\nTask\nSplit\nMetric\nImage\nCOCO[35]\nText-to-Image Generation\nKarpathy test\nCLIP score (↑)\nFlickr30K [36]\nText-to-Image Generation\ntest\nCLIP score (↑)\nCOCO Caption [31]\nScene Description\ntest\nCIDEr (↑)\nVQAv2 [57]\nScene Understanding QA\ntest-dev\nVQA acc. (↑)\nOKVQA [58]\nExternal Knowledge QA\nval\nVQA acc. (↑)\nVizWiz [61]\nScene Understanding QA\ntest-dev\nVQA acc. (↑)\nSEED-Bench [44]\nComprehensive QA\ndim 1-9\nMCQ acc. (↑)\nVideo\nMSVDQA [67]\nEvent Understanding QA\ntest\nTop-1 acc. (↑)\nMSRVTTQA [68]\nEvent Understanding QA\ntest\nTop-1 acc. (↑)\nNExTQA [69]\nTemporal\/Causal QA\ntest\nWUPS (↑)\nD\nEvaluation\nD.1\nBenchmarks\nIn order to assess the multimodal comprehension and image generation ability of SEED-LLaMA, we\nevaluate SEED-LLaMA on 10 benchmarks as shown in Tab. 9. For the evaluation of image generation,\nwe adopt the CLIP-ViT-L\/14 to calculate the CLIP score between the ground-truth image and the\ngenerated image. When evaluating SEED-Bench, we adhere to the official guidelines, selecting the\noption with the highest log likelihood as the response for each multi-choice question (MCQ). For the\nevaluation on video tasks, we uniformly sample 4 frames for MSVDQA and MSRVTTQA, and 8\nframes for NExTQA. For the other tasks, we follow the evaluation procedures in prior works [11, 22]\nand either submit the results to the official server (VQAv2, VizWiz) or assess them using the official\ncode ourselves.\nD.2\nPrompt Templates\nWe summarize the prompt templates used for evaluating SEED-LLaMA in Tab. 10. As the pre-trained\nSEED-LLaMA with size of 8B and 14B adopt different LLM (Vicuna-7B and Llama2-chat-13B),\ntheir prompts differ accordingly.\n17\nTable 10: Summary of the prompting template for evaluating SEED-LLaMA.\nModel\nType\nTemplate\nSEED-LLaMA\n8B\nImage Captioning {image}\nImage QA\n{image}USER: {question} Please provide an accurate answer\nconsisting of only one word or phrase.\\nASSISTANT:\nVideo QA\n{video}USER: {question} Please provide an accurate answer\nconsisting of only one word or phrase.\\nASSISTANT:\nSEED-LLaMA\n14B\nImage Caption\n{image}\nImage QA\n{image} Please provide an accurate answer consisting of only\none word or phrase based on the image.\\n\nQuestion:{question} \\n Answer:\nVideo QA\n{video} Please provide an accurate answer consisting of only\none word or phrase based on the video.\\n\nQuestion:{question}\\n Answer:\nSEED-LLaMA-I\n8B & 14B\nImage Caption\nUSER: {image}Please provide an accurate and con-\ncise description of the given image.\\nASSISTANT:\nImage QA\nUSER: {image}{question} Please provide an accurate answer\nconsisting of only one word or phrase.\\nASSISTANT:\nVideo QA\nUSER: {video}{question} Please provide an accurate answer\nconsisting of only one word or phrase.\\nASSISTANT:\nE\nQualitative Cases\nMore examples of multi-turn in-context multimodal generation and compositional image generation\nare shown in Fig. 9 and Fig. 10. Note that generating images with multimodal prompt is not an\nemergent ability since SEED-LLaMA is fine-tuned on corresponding paired data such as Instruct-\nPix2Pix [51]. We showcase qualitative examples of text-to-image generation by SEED-LLaMA\nin Fig. 11. Given various textual descriptions, our SEED-LLaMA can generate realistic images\nthat aligns with the text prompts. We further provide qualitative examples of multimodal compre-\nhension by SEED-LLaMA in Fig. 12, Fig. 13 and Fig. 14. SEED-LLaMA can realize in-context\nmulti-image understanding, real-world knowledge grounding, complex reasoning, story creation and\nvideo understanding.\n18\nUser\nSEED-LLaMA\nI am planning a trip to Barcelona. Can you recommend some \nplaces to visit for me?\nOf course! Barcelona is a beautiful city with a lot to offer. \nHere are some places you should definitely visit:\n1. La Sagrada Familia: This is a famous cathedral designed by \nAntoni Gaudi. It's a must-see attraction in Barcelona.\n2. Park Guell: Another Gaudi masterpiece, this park is full of \ncolorful mosaics and unique architecture. It's a great place to \ntake some photos.\n3. La Rambla: This is a lively street filled with shops, \nrestaurants, and street performers. It‘s a great place to take a \nstroll and enjoy the sights and sounds.\n4. Casa Batllo: Another Gaudi creation, this house is known \nfor its unique architecture and colorful facade. It's a must-see \nfor any architecture lover.\n5. Barceloneta Beach: If you're looking for a place to relax and \nsoak up the sun, this beach is a great option. It's a popular \nspot for locals and tourists alike.\nThese are just a few of the many places to visit in Barcelona. I \nhope you enjoy your trip!\nYes, there are many delicious snacks to try in Barcelona. One \npopular snack is the churro, which is a fried dough pastry that is \noften sprinkled with sugar. Another popular snack is the croquette, \nwhich is a small ball of breading filled with meat or fish. You can \nalso try the pan con tomate, which is a simple dish made with \nbread, tomatoes, and olive oil. And if you're feeling adventurous, \nyou can try the calamari, which is a fried squid dish that is often \nserved with lemon and aioli. These are just a few examples of the \nmany delicious snacks you can try in Barcelona. Enjoy!\nThanks for your suggestion. I am interested in La Sagrada Familia. \nCan you show me a picture?\nThis looks spectacular and I am going to visit it. I also want to try \nsome local snacks. Do you have any suggestions?\nI am interested in Churros. Can you show me what it looks like? \nCartoon Character: \"Hey, Mr. Einstein! I'm SpongeBob \nSquarePants! I'm a sponge who lives in a pineapple under the \nsea. What's your story?\"\nEinstein: \"Ah, hello there, my young friend. I am Albert \nEinstein, the famous theoretical physicist. I've been working \non a theory of relativity that could change the way we \nunderstand the universe.\"\nCartoon Character: \"Wow! That sounds amazing! I've never \nbeen to the sea, but I've been to the bottom of the ocean in a \nsubmarine. Maybe I can help you with your theory! What do \nyou need help with?\"\nEinstein: \"Actually, I've been trying to figure out how to travel \nthrough time. Do you think you could help me with that?\"\nCartoon Character: \"Sure! I can use my underwater bubble to \ntravel through the water. Maybe we can use that to travel \nthrough time too! Let's give it a try!\"\nEinstein: \"I'm not sure that's the right way to go about it, but \nI'm willing to try anything at this point. Let's give it a shot!”\nThis is a cartoon character in photo A.\nThis is a man in photo B.\nWho is the cartoon character in photo \nA, and who is the man in photo B?\nImagine what kind of conversation will the cartoon character \nand the man have when they sit in the same room? Please \ngive a possible dialogue script.\nThe cartoon character in photo A is SpongeBob \nSquarePants, and the man in photo B is Albert Einstein.\nCan you generate a cartoon image of the man in \nthe style of the character?\nFigure 9: Qualitative examples of multi-turn in-context image and text generation by SEED-LLaMA\ngiven multimodal instructions.\n19\nMultimodal Prompt Generation\nIn-context Generation\nThis is the\nfirst image.\nThis is the\nsecond image.\nThe animal in the\nfirst image\nsurrounded with\nthe flower in the\nsecond image.\nInput Prompt\nInput Prompt\nGeneration\nGeneration\nThis is the\nMount Rainier.\nGeneration\nGeneration\nInput Prompt\nInput Prompt\nInput Prompt\nGeneration\nGeneration\nReplace the apple\nwith vegetables\nMake the cat on\ngreen grass\nColor flowers white\nAdd a thunderstorm\nInput Prompt\nInput Prompt\nGeneration\nGeneration\nMake it in Hong Kong\nInsert a train\nMount Rainier\nreflected in the lake.\nA girl in front of\nMount Rainier.\nA cartoon image\nof Mount Rainier.\nMount Rainier\nunder moonlight.\nA wooden house\nbeside Mount Rainier.\nInput Prompt\nFigure 10: Qualitative examples of compositional image generation by SEED-LLaMA.\n20\nRenaissance-style portrait \nof an astronaut in space, \ndetailed starry background, \nreflective helmet.\nFood photo of a gourmet \nmeal, with a shallow depth \nof field, elegant plating, and \nsoft lighting.\nAerial photography of a \nwinding river through \nautumn forests, with vibrant \nred and orange foliage.\nCyberpunk cityscape \nwith towering \nskyscrapers, neon \nsigns, and flying cars.\nA black shepherd \nwearing a red beret.\nA dog wearing a bike.\nA cybernatic locomotive \non rainy day.\nA cat reading a book \nin the forest.\nA captivating scene \nof a seed cracking \nopen and sprouting \nunder sunlight.\nA cozy and inviting \nlibrary filled with books \nand reading nooks.\na couple on a beach, walking \nhand-in-hand, with the sun \nsetting over the horizon and \nwaves crashing at their feet.\nPortrait of young nordic girl, \nage 25, freckled skin, neck \ntatoo, blue eyes 35mm lens, \nphotography, ultra details.\nA cybernetic soldier, enhanced \nwith advanced weapons \nsystems and tactical analysis \nsoftware, on a mission.\nA soaring hot air \nballoon floating over a \nscenic landscape.\nKingfisher bird, by \nAkihito Yoshida, Cool \nColor Palette.\nTwo flamingos are in \nlove in a sunset, concept \nart. high quality.\nFigure 11: Qualitative examples of text-to-image generation by SEED-LLaMA.\n21\nThis is the second image.\nTell me the difference between these two species \nin detail.\nHorses and zebras are both members of the \nEquidae family, but they belong to different species. \nHorses are domesticated animals that have been \nbred for thousands of years for their strength, speed, \nand intelligence. Zebras, on the other hand, are wild \nanimals that are known for their distinctive black \nand white stripes, which help them blend in with \ntheir surroundings and avoid predators. Zebras are \nalso social animals that live in groups called herds.\nWhat's the difference?\nThis is the first image.\nThe first image shows a white horse running in a \nfield while the second image shows a zebra standing \nin a grassy field.\nWhat is unusual about this image?\nWhat should I do if I see this sign?\nThe man in the painting is Napoleon Bonaparte. \nHe was a French military and political leader who \nrose to prominence during the French Revolution. \nHe's known for his military prowess and his \nconquests of much of Europe. In this painting, he's \ndepicted riding a horse, which is a symbol of his \nmilitary power and conquests. The painting captures \na moment in history when Napoleon was at the \nheight of his power.\nWho is he?\nThis image depicts a painting of a man riding a \nhorse. The man is wearing a red jacket and a white \nhat, and he's holding a sword. The horse is brown \nand white, and it's jumping over a hill. There's also a \nmountain in the background. The painting is quite \ndetailed and captures the movement of the horse \nand the rider. The colors are vibrant, with the red \njacket and white hat of the rider standing out \nagainst the brown and white of the horse and the \ngreen of the mountain. The painting is quite large, \ntaking up a significant portion of the image. Overall, \nit's a dynamic and action-packed scene.\nWhat's in this picture?\nThe unusual aspect of this image is that it\nfeatures a dog in a space suit, which is not a \ncommon sight.\nIf you see this sign, you should not pass. It's \nimportant to follow traffic signs and signals to ensure \nyour safety and the safety of others on the road.\nWhich ball is heavier?\nSpongeBob SquarePants, Patrick Star.\nTell me the characters in the image.\nWhere is it in the image?\nThe red ball.\nThe Arc de Triomphe is in the middle of the image.\nFigure 12: Qualitative examples of multimodal comprehension by SEED-LLaMA.\n22\nTell me a story based on \nthese 3 images in detail.\nThis is the third image.\nThis is the first image.\nThis is the second image.\nWhat's the name of those characters?\nTom and Jerry.\nExplain in detail What they are doing.\nThe table is made of wood.\nOnce upon a time, in a far-off galaxy, there was a \nlonely dog named Max. Max had always \ndreamed of being an astronaut and exploring \nthe cosmos. One day, his wish came true when \nhe was chosen to be the first dog in space. As he \nsoared through the stars, he marveled at the \nbeauty of the universe. But when he returned to \nEarth, he realized that he missed the company \nof his fellow canines. So, he set out on a journey \nto find other dogs who shared his passion for \nspace exploration. Along the way, he met a \nfriendly alien who offered to take him on a trip \nto the moon. Max eagerly accepted the offer \nand soon found himself standing on the moon's \nsurface, staring up at the vast expanse of space. \nFrom that moment on, Max knew that he had \nfound his true calling and would spend the rest \nof his life exploring the wonders of the universe.\nThe Eiffel Tower is made of metal.\nAlbert Einstein was a theoretical physicist who is \nwidely regarded as one of the most influential \nscientists of the 20th century. He is best known \nfor his theory of relativity and the famous \nequation E=mc². He was awarded the Nobel Prize \nin Physics in 1921.\nWhat are his important achievements in life?\nAlbert Einstein.\nWhat's in this picture?\nIn the image, Tom and Jerry are engaged in a \ncomical chase scene. Tom, the cat, is holding a \nknife and appears to be chasing Jerry, the mouse, \nwho is trying to evade him. The knife in Tom's \nhand adds an element of danger and tension to \nthe scene, suggesting that the chase is not just a \nplayful game but potentially life-threatening for \nJerry. The background of the image shows a \nliving room, which adds a domestic setting to \nthe chase. Overall, the image captures a \nmoment of suspense and action in the ongoing \nrivalry between Tom and Jerry.\nFigure 13: Qualitative examples of multimodal comprehension by SEED-LLaMA.\n23\nThere are two people in the video - a man and a woman.\nHow many people are in the video?\nCan you describe this video in details?\nSure, in this video, we see a man riding a bike down a street. He is wearing a helmet and a backpack, \nand he seems to be enjoying the ride. As he rides down the street, he passes by several cars parked on \nthe side of the road. The man's bike looks sturdy and well-maintained, and he seems to be riding it with \nease. The street he is riding on appears to be relatively quiet, with only a few pedestrians visible in the \nbackground. Overall, this video captures a peaceful and enjoyable moment of a man riding his bike down \na quiet street.\nWhat are they doing?\nThey are drinking tea and talking to each other.\nwhere are they?\nThe man and woman are sitting on a train and looking out the window.\nOne unusual aspect of the video is that the man is riding a small train on a track in the middle of a \nforest, which is not a common sight. Additionally, the train appears to be old and rusty, which adds to \nthe unusual nature of the video.\nWhat is unusual about this video?\nFigure 14: Qualitative examples of multimodal comprehension by SEED-LLaMA.\n24\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Making LLaMA SEE and Draw with SEED Tokenizer.pdf"}
{"title":"On the Performance of Multimodal Language Models","authors":"Utsav Garg, Erhan Bas","summary":"Instruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating\nindependently pretrained vision encoders through model grafting. These\nmultimodal variants undergo instruction tuning, similar to LLMs, enabling\neffective zero-shot generalization for multimodal tasks. This study conducts a\ncomparative analysis of different multimodal instruction tuning approaches and\nevaluates their performance across a range of tasks, including complex\nreasoning, conversation, image captioning, multiple-choice questions (MCQs),\nand binary classification. Through rigorous benchmarking and ablation\nexperiments, we reveal key insights for guiding architectural choices when\nincorporating multimodal capabilities into LLMs. However, current approaches\nhave limitations; they do not sufficiently address the need for a diverse\nmultimodal instruction dataset, which is crucial for enhancing task\ngeneralization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current\nmethodological constraints in adapting language models for image comprehension\nand provide valuable guidance for researchers and practitioners seeking to\nharness multimodal versions of LLMs.","url":"http:\/\/arxiv.org\/abs\/2310.03211v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.03211v2","published":1696462416000,"comment":null,"pdf_text":"On the Performance of Multimodal Language Models\nUtsav Garg\nScale AI\nutsav.garg@scale.com\nErhan Bas∗\nGE HealthCare\nerhan.bas@gehealthcare.com\nAbstract\nInstruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating inde-\npendently pretrained vision encoders through model grafting. These multimodal\nvariants undergo instruction tuning, similar to LLMs, enabling effective zero-shot\ngeneralization for multimodal tasks. This study conducts a comparative analysis\nof different multimodal instruction tuning approaches and evaluates their perfor-\nmance across a range of tasks, including complex reasoning, conversation, image\ncaptioning, multiple-choice questions (MCQs), and binary classification. Through\nrigorous benchmarking and ablation experiments, we reveal key insights for guid-\ning architectural choices when incorporating multimodal capabilities into LLMs.\nHowever, current approaches have limitations; they do not sufficiently address the\nneed for a diverse multimodal instruction dataset, which is crucial for enhancing\ntask generalization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current method-\nological constraints in adapting language models for image comprehension and\nprovide valuable guidance for researchers and practitioners seeking to harness\nmultimodal versions of LLMs.\n1\nIntroduction\nLarge instruction-tuned language models (LLMs) have emerged as powerful models showcasing\nremarkable zero-shot generalization capabilities across a diverse spectrum of downstream tasks. By\nlearning to interpret natural language instructions, these models obviate the need for task-specific\ntraining. However, real-world applications, often involve multimodal data, such as images and text,\nnecessitating the combination of visual and linguistic information for accurate and robust inference.\nTo address this challenge, recent research Li et al. [2023], Ye et al. [2023], Zhu et al. [2023], Liu et al.\n[2023a] has introduced multimodal variants of LLMs, which integrate independently pretrained large\nvision encoders with LLMs. These models further undergo instruction tuning, aiming to leverage the\ncombined power of visual and linguistic understanding.\nIn this paper, we conduct a comprehensive investigation that centers on comparing various approaches\nto multimodal instruction tuning and assessing their performance on a wide array of downstream tasks.\nOur study seeks to illuminate the efficacy, generalization capabilities, and limitations of publicly\navailable models and their ablations across various domains. We evaluate the different approaches\nacross a diverse range of tasks from complex reasoning to captioning and classification, to test their\ngeneralization capabilities. Moreover, we aim to identify whether specific design choices prove more\nsuitable for specific tasks.\nFor our experiments, we consider five publicly available approaches for multimodal adaptation\nof LLMs: BLIP-2 Li et al. [2023], InstructBLIP Dai et al. [2023], LLaVA Liu et al. [2023a],\nMiniGPT4 Zhu et al. [2023] and mPLUG-Owl Ye et al. [2023]. These approaches encompass a wide\n∗work done while Erhan Bas was at Scale AI.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2310.03211v2  [cs.CL]  28 Nov 2023\ngamut of architectural choices for injecting LLMs with multimodal capabilities. More details about\nthese respective models is in the appendix A.1. By conducting thorough benchmarking and analysis,\nwe aim to identify the strengths and weaknesses of different multimodal instruction tuning strategies.\nFurthermore, we conduct ablation experiments on previously untested combinations within these\napproaches to unearth the optimal method for integrating visual capabilities into a language model.\nThis includes exploring the utility of the largest feasible frozen vision encoder, attaching a trained\nvision head to extract concise visual features, employing a linear mapping layer to project these into\nthe linguistic domain, and fine-tuning the decoder on multimodal data samples.\n2\nDatasets and Evaluation\nWe systematically assess all approaches across two primary task categories to comprehensively gauge\ntheir capabilities across a broad spectrum of challenges:\n1. Complex Reasoning: In this task category, the model confronts questions that necessitate\nexternal knowledge and cannot be resolved solely based on the image and question. This\ntest predominantly assesses the language model’s performance and illuminates the extent to\nwhich multimodal instruction tuning influences the base LLM’s regression.\n2. Captioning, Classification, Conversation, MCQ: These tasks become feasible for the\nLLM only through the incorporation of the visual encoder. The performance across these\ntasks serves as a robust indicator of the approach’s generalization prowess, encompassing\narchitectural choices and instruction tuning data.\nTo evaluate the general Visual Question Answering (VQA) abilities, we use the test set curated by\nLLaVA Liu et al. [2023a]. The dataset consists of 30 images from the COCO dataset, each associated\nwith three distinct question types: conversation, description, and complex reasoning types, resulting\nin a total of 90 instruction-answer pairs. The ground-truth answers for these questions are generated\nby GPT-4, which is given the captions and bounding boxes associated with the image for context.\nThe evaluation involves ranking both the predicted and reference answers on a scale of 1 to 10 by\nGPT-4. The metric is the relative score of the predicted answer to GPT-4’s answer. To address\npotential biases due to answer order, as demonstrated by Wang et al. [2023], we adopt the Balanced\nPosition Calibration Wang et al. [2023] (BPC) strategy proposed by them, averaging scores from\nboth possible orderings. Additionally, to account for the stochasticity of the generation, we average\nscores across 5 generations by each approach. Despite these considerations to make the evaluation as\nrobust as possible, there are still limitations as GPT-4 is not actually seeing the image and is instead\nusing a proxy for visual information. Moreover, we cannot expect GPT-4 evaluations to be perfect\nunderlining the current limitation of open-ended answer evaluations.\nFor captioning tasks, we evaluate on the val set of the NoCaps Agrawal et al. [2019] dataset and\nreport the CIDEr Vedantam et al. [2015] metric. For visual Multiple Choice Questions (MCQ), we\nevaluate on the test split of the ScienceQA Lu et al. [2022] dataset. Here, we exclusively consider\nsamples with available image context. For binary classification, we evaluate on the Visual Spatial\nReasoning Liu et al. [2023b] (VSR) dataset. We use the zero-shot test split provided in the official\nGitHub repository.\nFor tasks that require generation, we use a beam size of 5. For MCQ, we measure the log-likelihood\nof generating each of the answer options.\n3\nExperiments\n3.1\nComparing Existing Approaches\nTable 1 highlights the main differences between existing approaches, include architectural choices\nand the data used for instruction tuning.\nFor fair comparison, we compare variants of all approaches with a similarly sized LLM. Two of the\napproaches (LLaVA and mPLUG-Owl) use a smaller vision encoder (ViT-L), while the others use ViT-\ng. BLIP-2 does not do any multimodal instruction tuning, while all other approaches are instruction\ntuned on varying amounts of multimodal data. The vision encoder is frozen in all approaches except\n2\nApproach\nInstruction Data Size\nVision Encoder\nVision Head\nLLM\nBackbone\nFrozen\nType\nFrozen\nType\nFrozen\nBLIP-2\nN\/A\nViT-g\nYes\nQ-Former\nNo\nVicuna-7B\nYes\nMiniGPT-4\n3.5K\nViT-g\nYes\nQ-Former\nYes\nVicuna-7B\nYes\nLLaVA\n150K\nViT-L\nYes\nN\/A\nN\/A\nVicuna-7B\nNo\nmPLUG-Owl\n290K\nViT-L\nNo\nSimilar to Perceiver Resampler\nNo\nLLaMA-7B\nNo\nInstructBLIP\n15M\nViT-g\nYes\nQ-Former\nNo\nVicuna-7B\nYes\nTable 1: Details of the major components of the compared approaches. We compare variants of each\napproach with a similar decoder size. The instruction data sizes are approximate.\nmPLUG-Owl, while the vision head is always trained when used, except for MiniGPT-4. Additionally,\nboth of the approaches using the smaller vision encoder fine-tune their LLMs, while the others keep\nthe language model frozen.\n3.2\nPerformance on Benchmarks\nTable 2 presents the results on the various benchmarks discussed in Section 2.\nApproach\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nConversation\nDetail\nComplex Reasoning\nOverall\nBLIP-2\n61.5\n66.7\n53.3\n60.5\n50\n53.8\n107.5\nMiniGPT-4\n72.5\n68.2\n76.4\n72.4\n57.2\n36.04\n86.91\nLLaVA\n75.1\n70.6\n88.2\n78\n52.45\n34.8\n67.75\nmPLUG-Owl\n74\n69.8\n86.8\n76.9\n54.99\n34.01\n70.82\nInstructBLIP\n85.3\n75.8\n88.6\n83.3\n58.51\n59.49\n123.65\nTable 2: Comparison of all publicly available multimodal variants of LLMs on the four datasets\ndiscussed above. The scores for LLaVA VQA are relative scores compared to the reference answer as\ndetermined by GPT-4. For NoCaps, we use CIDEr and report accuracy for VSR and ScienceQA. The\nbest results for each dataset are in bold, and the second-best results are underlined.\nFor mPLUG-Owl, we use the checkpoint provided by the authors, where the full decoder is fine-\ntuned instead of the one that uses LORA Hu et al. [2021], as it performs better overall. We ran all\nbenchmarks using the checkpoints provided by the official implementations but directly used the\nresults provided for BLIP-2 in the InstructBLIP paper for VSR, ScienceQA, and NoCaps.\nThe results demonstrate that InstructBLIP, which performs multitask instruction tuning on a variety\nof datasets, performs the best overall on all tasks, highlighting the importance of data diversity during\ninstruction tuning. In contrast, all other instruction-tuned approaches perform poorly on tasks they\nwere not trained on, likely due to overfitting to a specific task type. This is evident from the results of\nLLaVA, mPLUG-Owl, and InstructBLIP, all of which were trained on the LLaVA-150K data and are\nthus the top three performers on LLaVA VQA. However, only InstructBLIP continues to perform well\non out-of-distribution tasks due to its richer instruction tuning data. There are qualitative samples\nfrom each of the three LLaVA benchmark categories in the appendix A.2.\n3.3\nAblations on Model Components\nIn this section, we conduct ablations on the LLaVA architecture to analyze the effects and importance\nof changes to different components, namely the vision encoder, vision head, and data. We chose the\nLLaVA architecture as the base because its training\/evaluation code and data are available, and the\nLLaVA-150K training set has been used by other approaches as well.\n3.3.1\nVision Head\nThe vision head operates over the patch embeddings of a frozen vision encoder (e.g., CLIP) and\ncompresses the features to extract relevant details. In this section, we specifically compare the\neffects of using a Querying Transformer (Q-Former) as the vision head. The Q-Former is a small\nTransformer model that learns queries to extract relevant visual features from the vision encoder. For\n3\nall experiments in this section (unless otherwise mentioned), we use an 80K balanced subset of the\nLLaVA data provided by the authors in the official GitHub repository.\nVision Head\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nType\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nN\/A\nN\/A\n74.8\n72.5\n88.2\n78.5\n51.88\n34.95\n61.25\nQ-Former\nYes\n75\n70.6\n87.4\n77.6\n51.47\n35.35\n71.99\nQ-Former\nNo\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nTable 3: Comparison of the effect of having and training a vision head. All configurations use the\nsame frozen vision backbone and trained LLM.\nThe results in Table 3 clearly show that training the vision head over a frozen encoder offers a\nsignificant improvement compared to not having a vision head. However, using a frozen vision\nhead does not provide any additional benefit compared to not having one. The improvement in\nperformance comes from training the vision head along with the decoder to align features based on\nthe task requirements.\n3.3.2\nVariations of Vision Head\nIn the previous section, we compared the effect of having a vision head. However, it is also possible\nto have different types of vision heads. The Q-Former architecture supports attending to both text\nand image together. For instruction tuning, it can be beneficial to have the Q-Former attend to the\ninstruction along with the image, learning image features that are relevant for answering the question.\nVision Head\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nType\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nImage Q-Former\nNo\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nMultiModal Q-Former\nNo\n72.7\n74\n87\n77.8\n50.98\n35.6\n47.74\nMultiModal Q-Former∗\nNo\n77.5\n73\n88.3\n79.6\n54.17\n36.29\n48.2\nTable 4: Comparison of the effect of having an image-only vs multimodal vision head. All configura-\ntions use the same frozen vision backbone and trained LLM.\nTable 4 presents the results of comparing different types of vision heads. In the \"Image Q-Former\"\nrow, the Q-Former attends only to the image, while in the \"MultiModal Q-Former\" row, the Q-Former\nattends to both the instruction and the image. However, as the instruction tuning data contains multi-\nturn conversations, the instructions for all turns are concatenated together, which leads to suboptimal\nperformance. The \"MultiModal Q-Former∗\" row modifies the instruction data by breaking all multi-\nturn conversation samples into separate training points. This version performs mostly on par with\nthe image-only version. The drop in performance is observed in the detail category and captioning\n(NoCaps), as we do not want to restrict the features to the instruction in these cases, and it is best for\nthe decoder to receive all possible information about the image. These experiments indicate that using\ninstruction-aware visual features does not offer any advantage with open-ended question answering.\nHowever, where the final objective is classification, it offers some advantage as also observed by Dai\net al. [2023].\n3.3.3\nSize of Vision Encoder\nIn the previous experiments, we used ViT-L as the frozen vision encoder. Here, we compare the effect\nof using a larger vision encoder, specifically ViT-g, for the different configurations mentioned above.\nVision Encoder\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nBackbone\n# Params\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nViT-L\n304M\nYes\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nViT-g\n1.0B\nYes\n81.5\n79.1\n90.9\n83.9\n50.16\n35.6\n90.89\nTable 5: Comparison of the effect of using a larger frozen vision encoder. All configurations use a\ntrained image-only Q-Former and trained LLM.\nTable 5 presents the results comparing the ViT-L and ViT-g vision encoders, with all other config-\nurations remaining the same. The results clearly demonstrate that having a larger vision encoder\n4\n(ViT-g) helps improve performance across the board, even on tasks for which the model was not\nspecifically trained (NoCaps). The larger encoder produces a richer image representation, leading to\nbetter overall performance.\n3.3.4\nEffect of Data Size in Alignment Phase\nIn all the discussed approaches, an alignment stage precedes instruction tuning, where a projection\nlayer is learned to map the outputs of the vision heads to the input space of the language model. In\nthis section, we study the effect of data size during the alignment phase. We use BLIP-2 as the starting\npoint, which has been aligned on 129M image-text pairs. For comparison, we perform alignment\nusing 595K image-text pairs, as done in LLaVA, for the same architecture.\nAmount of Alignment Data\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nConversation\nDetail\nComplex Reasoning\nOverall\n129M\n82.6\n76.5\n90\n83.1\n52.21\n35.65\n91.07\n595K\n81.5\n79.1\n90.9\n83.9\n50.16\n35.6\n90.89\nTable 6: Comparison of the effect of data size during the alignment phase. All configurations use a\ntrained image-only Q-Former and trained LLM.\nTable 6 presents the results comparing the alignment with different amounts of data. We observe\nthat aligning the model with a larger amount of data does not significantly improve in-distribution\ndownstream performance when the model is further instruction tuned. This is reasonable as the\nprojection layer, which is trained during alignment, has a small number of parameters. Therefore,\nincreasing the alignment data size eventually leads to diminishing returns. However, for out-of-\ndistribution tasks (VSR, ScienceQA, NoCaps), the larger amount of alignment data does lead to small\nimprovement in performance, most likely due to more robust features.\nAmount of LLaVA VQA Data\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nConversation\nDetail\nComplex Reasoning\nOverall\n150K\n75.1\n70.6\n88.2\n78\n52.45\n34.8\n67.75\n80K\n74.8\n72.5\n88.2\n78.5\n51.88\n34.95\n61.25\nTable 7: Comparison of the effect of data size during the instruction tuning phase. All configurations\nuse a ViT-L vision encoder, no vision head, and a trained LLM.\nTable 7 shows a similar trend for the quantity of data used in instruction tuning. In all the ablation\nexperiments, we used an 80K balanced subset of the LLaVA instruction data. However, if we compare\nthe performance of our recreated versions of LLaVA in the table above, on the original 150K and the\n80K data subset, they perform at par. Beyond a certain point, increasing the amount of data does not\nsignificantly improve performance. However, data diversity does play a role, as demonstrated by the\nsuperior performance of InstructBLIP in Table 2.\n3.3.5\nTraining the Language Model\nThe ablations discussed so far all fine-tune the language model following LLaVA. This section\ncompares the effect of training or keeping the decoder frozen across various configurations.\nVision Encoder\nVision Head\nLLM Frozen\nLLaVA VQA\nVSR\nScienceQA\n(Image)\nNoCaps\nType\nFrozen\nConversation\nDetail\nComplex Reasoning\nOverall\nViT-L\nN\/A\nN\/A\nYes\n58.1\n66.2\n75.8\n66.7\n50.65\n34.9\n54.31\nViT-L\nN\/A\nN\/A\nNo\n74.8\n72.5\n88.2\n78.5\n51.88\n34.95\n61.25\nViT-L\nQ-Former\nNo\nYes\n68.4\n69.1\n85\n74.1\n49.75\n35.45\n67.18\nViT-L\nQ-Former\nNo\nNo\n75.5\n75.1\n89.9\n80.2\n52.7\n35.35\n75.45\nViT-g\nQ-Former\nNo\nYes\n72.3\n77.6\n88\n79.3\n50.49\n36.49\n89.98\nVit-g\nQ-Former\nNo\nNo\n81.5\n79.1\n90.9\n83.9\n50.16\n35.6\n90.89\nTable 8: Comparing the effect of training the decoder. The configurations mention the vision encoder\nand head used. All experiments use the same LLM (Vicuna-7B).\nThe experiments in Table 8 show that when instruction tuning on the LLaVA dataset, training the\ndecoder helps in almost all scenarios. The performance improvements are reduced to an extent\n5\nwhen using a larger vision backbone (ViT-g), but there is still some gain. InstructBLIP uses a frozen\ndecoder, but these experiments suggest that we can achieve an additional boost in performance if the\ndecoder is trained as well. Moreover, as the gradients are always propagated through the decoder in\nthis architecture setup, training the decoder does not incur any significant overhead.\n4\nDiscussion and Conclusion\nThe benchmarks and ablation experiments offer various insights into multimodal LLMs. The main\ntakeaways from this study are as follows:\n• Vision Encoder: Using a larger vision encoder (ViT-L vs. ViT-g) consistently improves\nperformance across all tasks, as it captures a richer image representation. However, fine-\ntuning the vision encoder does not improve performance on downstream tasks as observed\nfrom the relative performance of mPLUG-Owl in Table 2, which does not keep it frozen.\n• Vision Head: Fine-tuning a vision head (e.g., Q-Former) is powerful as it enables the model\nto extract a better representation for the downstream task while also speeding up training and\ninference due to the smaller output representation. However, using a multimodal vision head\nthat encodes both the image and instruction together does not show any apparent advantage\nover using an image-only head in open-ended question answering. This suggests that it\nis more important to condense all visual information and pass that to the language model\nthrough grafting and letting the LLM use the full context to answer questions.\n• LLM: Training the Large Language Model (LLM) during instruction tuning can lead to\nadditional performance gains without significant overhead in training cost, as gradients are\npropagated through the LLM in all cases. If there are concerns of LLM regression on text\nonly tasks, we can use adapters (LoRA Hu et al. [2021]\/IA3 Liu et al. [2022] etc.) to leave\nthe base model unaltered and use the adapter version when inputs are multimodal.\n• Data: The size of the training dataset becomes less important beyond a certain point in\nboth the alignment and instruction tuning stages. However, data diversity, both within a task\nand across tasks, remains crucial for achieving superior performance as shown by results of\nInstructBLIP.\nThese results provide valuable insights for future research in this direction, guiding architecture\nchoices and emphasizing the importance of data diversity both within and across tasks. The findings\nsuggest that focusing on larger frozen vision encoders, training vision heads, and optimizing the\nLLM can yield improvements in performance on multimodal tasks. Moreover, exploring diverse and\nrepresentative datasets can contribute to achieving state-of-the-art performance.\nThis work also highlights that differences in architectural methodologies for grafting multimodal\ncapabilities into LLMs. A simple strategy of extracting relevant condensed visual features and\ntransforming them via linear projections to the language space performs as well as any other. The\nmajor areas to focus on are data curation and task diversity. Another challenge the community should\nfocus on are hallucinations of these multimodal variants. Language models are unaware of visual\nconcepts during pretraining and tend to hallucinate objects\/concepts in images that might not exist\nwhen probed about them instead of answering no. To make these models useful, a focus on alleviating\nthese hallucinations will be essential.\nIn conclusion, this study sheds light on the key components and strategies for building effective\nmultimodal Large Language Models. It focuses on the limitations of current research, and highlights\nfocus areas that will bring the largest impact to their capabilities and usefulness.\n6\nReferences\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023a.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. The 37th Conference on Neural Information Processing Systems\n(NeurIPS), 2023.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023.\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra,\nDevi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In\nProceedings of the IEEE International Conference on Computer Vision, pages 8948–8957, 2019.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566–4575, 2015.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. In The 36th Conference on Neural Information Processing Systems\n(NeurIPS), 2022.\nFangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of\nthe Association for Computational Linguistics, 2023b.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and\nColin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context\nlearning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. International journal of computer vision,\n123:32–73, 2017.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n2556–2565, 2018.\n7\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 3558–3568, 2021.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference on\nMachine Learning, pages 12888–12900. PMLR, 2022.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for\nimage captioning with reading comprehension. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 742–758. Springer,\n2020.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019.\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In ICDAR, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pages\n8748–8763. PMLR, 2021.\nOpenAI. Gpt-4 technical report. ArXiv, abs\/2303.08774, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m: Image-text pair dataset.\nhttps:\/\/github.com\/kakaobrain\/coyo-dataset,\n2022.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps:\/\/github.com\/tatsu-lab\/stanford_alpaca, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.\n8\nA\nAppendix\nA.1\nCompared Approaches\nFor our experiments, we consider the five publicly available approaches listed below.\n• BLIP-2 Li et al. [2023]: Aligns a frozen vision encoder and frozen language model through\na Querying Transformer (Q-Former) and a linear projection layer. The Q-Former is a\nsmall Transformer model that learns queries to extract relevant visual features from the\nvision encoder. The model is trained in two stages: first, to learn the image representation,\nand then to translate the learned representation through the LLM. It is pretrained with\n129M image-text pairs from COCO Lin et al. [2014], Visual Genome Krishna et al. [2017],\nCC3M Sharma et al. [2018], CC12M Changpinyo et al. [2021], SBU Ordonez et al. [2011],\nand LAION400M Schuhmann et al. [2021].\n• InstructBLIP Dai et al. [2023]: Uses the same architecture as BLIP-2 and performs\nmultimodal instruction tuning. During instruction tuning, the instruction is also passed along\nwith the image to the Q-Former to extract instruction-relevant visual features. It translates\nmultiple captioning and VQA datasets into an instruction-answer format and performs\ninstruction tuning in a multi-task setup. It is trained on around 15M image-instruction pairs\nfrom COCO Caption Lin et al. [2014], Web CapFilt Li et al. [2022], TextCaps Sidorov et al.\n[2020], VQAv2 Goyal et al. [2017], OKVQA Marino et al. [2019], A-OKVQA Schwenk\net al. [2022], OCR-VQA Mishra et al. [2019], and LLaVA-150K Liu et al. [2023a].\n• LLaVA Liu et al. [2023a]: Has a CLIP Radford et al. [2021] ViT-L vision encoder mapped\nto a Vicuna decoder through a linear layer. It follows a two-stage training process: in\nstage-1 (alignment), only the linear mapper is trained to align the visual output with the\nlanguage input. In stage-2, both the mapper and LLM are trained to output answers to\nimage-instruction pairs. For stage-1, 595K image-text pairs from CC3M Sharma et al.\n[2018] are used. For stage-2, they create a dataset of 158K image-instruction pairs across\nconversation, description, and complex reasoning tasks by prompting the language-only\nGPT-4 OpenAI [2023].\n• MiniGPT4 Zhu et al. [2023]: Follows the same architecture as BLIP-2 but uses only the\npretrained Q-Former and learns the linear mapping layer through the same alignment process\nas LLaVA. It also follows a two-stage training paradigm, but in both stages, only the linear\nmapping layer is trained. For stage-1, they use 5M image-text pairs from LAION, SBU, and\nConceptual Captions. For stage-2, they self-curate a high-quality instruction dataset with\n3.5K instruction-image pairs to improve the model’s conversational ability.\n• mPLUG-Owl Ye et al. [2023]: This approach uses a CLIP ViT-L vision encoder, a visual\nabstractor which follows a similar architecture to Perceiver Resampler Alayrac et al. [2022],\na linear mapping layer, and a LLaMA Touvron et al. [2023] decoder. They also follow\na two-stage training setup, but in this work, all parts of the model are trained during one\nof the two stages. The visual abstractor serves a similar purpose to the Q-Former and is\ntrained in stage-1 along with the visual encoder to extract relevant features. In stage-2, the\nvisual encoder and abstractor are frozen, and the LLM is trained. For stage-1, they use a\nmassive dataset of about 1.1B image-text pairs from LAION-400M, COYO-700M Byeon\net al. [2022], Conceptual Captions, and MSCOCO. For stage-2, they use around 290K\ninstruction samples from Alpaca Taori et al. [2023], Vicuna Chiang et al. [2023], Baize Xu\net al. [2023], and LLaVA-150K.\n9\nA.2\nQualitative comparison of benchmaked approaches\nImage and Question\nModel\nGenerations\nWhat is the main focus\nof the image?\n(Coversation)\nBLIP-2\nIs it the cat or the laptop?\nInstructBLIP\nThe main focus of the image is a cat sitting on a\ndesk in front of a laptop computer.\nLLaVA\nThe main focus of the image is a cat sitting on a\ndesk and looking at a laptop.\nMiniGPT-4\nThe main focus of the image appears to be a cat\nsitting on top of a laptop computer that is open and\nconnected to a keyboard. The cat is sitting on the\nright side of the laptop, facing the camera. The\nbackground of the image appears to be a cluttered\ndesk with papers, pens, and other items scattered\naround.\nmPLUG-Owl\nThe main focus of the image is on a cat sitting on\ntop of a laptop and looking at the screen.\nWhy might the man be\nholding the clothes on\nhangers?\n(Complex Reasoning)\nBLIP-2\nHe might be getting ready to pack for a trip.\nInstructBLIP\nThe man might be holding the clothes on hangers\nbecause he is preparing to pack them into a suit-\ncase. In the image, he is standing next to a window\nwith a suitcase open, indicating that he is in the\nprocess of packing his belongings for a trip. By\nholding the clothes on hangers, he can ensure that\nthey maintain their shape and don’t get wrinkled or\ncreased during the packing process. Additionally,\nhanging the clothes on hangers helps to conserve\nspace in the suitcase and makes it easier to access\nand organize the items when unpacking at the des-\ntination.\nLLaVA\nThe man could be holding the clothes on hangers\nfor a variety of reasons. He might be preparing to\nhang the clothes in a closet or dressing area in his\nhome, or he could be taking them to a dry cleaning\nstore or laundromat to have them cleaned. The im-\nage does not provide enough context to determine\nthe exact reason, but it is clear that the man is in\nthe process of handling the clothes on hangers.\nMiniGPT-4\nIt is not clear why the man is holding the clothes on\nhangers. It could be that he is trying to decide what\nto wear, or he could be holding the clothes to take\nthem to a dry cleaner. Without more information, it\nis difficult to say for certain why the man is holding\nthe clothes on hangers.\n10\nImage and Question\nModel\nGenerations\nmPLUG-Owl\nThe man could be holding the clothes on hang-\ners as he is either organizing his closet, packing\nfor a trip, or getting ready to hang them up in\nhis room or a hotel room. Hanging the clothes\nin a well-organized manner can help keep them\nclean and prevent wrinkles, especially if he is pack-\ning for a trip. Additionally, he may be preparing\nfor a photo shoot or attending an event where the\nclothing needs to be presented nicely. Holding\nthe clothes on hangers helps him maintain their\nappearance and ensure they are ready for display,\npresentation, or use.\nWhat do you see happening\nin this image?\n(Detail Description)\nBLIP-2\nThere are two airplanes in the picture. One is land-\ning and the other is taking off. There are also a\nlot of birds in the water. The mountains are in the\nbackground.\nInstructBLIP\nThe image depicts two large airplanes flying over\na body of water, possibly an ocean or a lake. One\nof the planes is in the process of taking off, while\nthe other is already airborne. There are several\nbirds scattered throughout the scene, with some\nperched on the water’s surface and others flying in\nthe sky. In total, there are at least 10 birds visible\nin the image. The airplanes are positioned at dif-\nferent heights, with one closer to the water and the\nother higher up in the sky. Overall, the scene cap-\ntures the beauty of aviation and nature coexisting\nin harmony.\nLLaVA\nThe image features a large airplane taking off from\nan airport runway, with the airplane’s wings fully\nextended. In the background, there is another air-\nplane on the runway, possibly preparing for takeoff\nor landing. The scene is set on a body of water,\nwith several birds in the vicinity of the airplanes.\nA flock of birds is resting near the first airplane,\nwhile another flock is situated in the background of\nthe image. The combination of the airplanes, birds,\nand water contribute to a picturesque aerial view.\nMiniGPT-4\nI see a large passenger jet airplane flying over a\nbody of water with some birds in the foreground.\nmPLUG-Owl\nThe image captures a large airplane taking off from\na runway, surrounded by a group of birds. The\nbirds appear to be flying in a nearby area, likely\nobserving the airplane as it ascends. There is also\na flock of birds flying in the air.\nTable 9: Qualitative comparison of the publicly available multimodal\nLLMs on a selected sample of image-question pairs from the LLaVA test\nset.\n11\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/On the Performance of Multimodal Language Models.pdf"}
{"title":"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization","authors":"Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu","summary":"Recently, the remarkable advance of the Large Language Model (LLM) has\ninspired researchers to transfer its extraordinary reasoning capability to both\nvision and language data. However, the prevailing approaches primarily regard\nthe visual input as a prompt and focus exclusively on optimizing the text\ngeneration process conditioned upon vision content by a frozen LLM. Such an\ninequitable treatment of vision and language heavily constrains the model's\npotential. In this paper, we break through this limitation by representing both\nvision and language in a unified form. Specifically, we introduce a\nwell-designed visual tokenizer to translate the non-linguistic image into a\nsequence of discrete tokens like a foreign language that LLM can read. The\nresulting visual tokens encompass high-level semantics worthy of a word and\nalso support dynamic sequence length varying from the image. Coped with this\ntokenizer, the presented foundation model called LaVIT can handle both image\nand text indiscriminately under the same generative learning paradigm. This\nunification empowers LaVIT to serve as an impressive generalist interface to\nunderstand and generate multi-modal content simultaneously. Extensive\nexperiments further showcase that it outperforms the existing models by a large\nmargin on massive vision-language tasks. Our code and models are available at\nhttps:\/\/github.com\/jy0205\/LaVIT.","url":"http:\/\/arxiv.org\/abs\/2309.04669v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2309.04669v3","published":1694228498000,"comment":"ICLR 2024","pdf_text":"Published as a conference paper at ICLR 2024\nUNIFIED LANGUAGE-VISION PRETRAINING IN LLM\nWITH DYNAMIC DISCRETE VISUAL TOKENIZATION\nYang Jin1∗, Kun Xu2, Kun Xu2, Liwei Chen2, Chao Liao2, Jianchao Tan2,\nQuzhe Huang1, Bin Chen2, Chenyi Lei2, An Liu2, Chengru Song2,\nXiaoqiang Lei2, Di Zhang2, Wenwu Ou2, Kun Gai2, Yadong Mu1†\n1Peking University\n2Kuaishou Technology\njiny@stu.pku.edu.cn, {xukunxkxk,syxu828}@gmail.com, myd@pku.edu.cn\nABSTRACT\nRecently, the remarkable advance of the Large Language Model (LLM) has in-\nspired researchers to transfer its extraordinary reasoning capability to both vision\nand language data. However, the prevailing approaches primarily regard the visual\ninput as a prompt and focus exclusively on optimizing the text generation process\nconditioned upon vision content by a frozen LLM. Such an inequitable treatment\nof vision and language heavily constrains the model’s potential. In this paper, we\nbreak through this limitation by representing both vision and language in a unified\nform. Specifically, we introduce a well-designed visual tokenizer to translate the\nnon-linguistic image into a sequence of discrete tokens like a foreign language\nthat LLM can read. The resulting visual tokens encompass high-level semantics\nworthy of a word and also support dynamic sequence length varying from the im-\nage. Coped with this tokenizer, the presented foundation model called LaVIT can\nhandle both image and text indiscriminately under the same generative learning\nparadigm. This unification empowers LaVIT to serve as an impressive generalist\ninterface to understand and generate multi-modal content simultaneously. Exten-\nsive experiments further showcase that it outperforms the existing models by a\nlarge margin on massive vision-language tasks. Our code and models are avail-\nable at https:\/\/github.com\/jy0205\/LaVIT.\n1\nINTRODUCTION\nThe large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023) nowadays have\ndemonstrated impressive advances in various linguistic applications. Profiting from the knowl-\nedge in the massive text corpus, they possess exceptional understanding capabilities and serve as\na general-purpose interface to complete a wide range of real-world tasks. Such success has moti-\nvated researchers to investigate the Multi-modal Large Language Models (MLLMs), which aim at\nextending the powerful pure-text LLMs to process multi-modality inputs. As shown in Figure 1-(a),\nthe prevalent approaches mostly leverage an adapter architecture (e.g., the Resampler (Alayrac et al.,\n2022), linear projection (Liu et al., 2023), or Q-Former (Li et al., 2023)) to map the visual features\nencoded by a pre-trained vision backbone (Radford et al., 2021) to the semantic space of LLM.\nDespite achieving preliminary results in zero-shot multi-modal understanding, they still suffer from\ninherent design deficiencies. The training objective of prior methodologies (Li et al., 2023; Huang\net al., 2023; Zhu et al., 2023) is centered on predicting textual descriptions dependent on visual con-\ntent, where the visual parts are merely regarded as prompts without any supervision. The inequitable\ntreatment of different modal inputs severely constrains the model’s potential, limiting them to only\nperforming comprehension tasks like generating text based on images. Moreover, most of these\nmethods completely delegate the responsibility of vision-language alignment to the newly added\nadapter with limited trainable parameters, which fails to leverage the remarkable reasoning capabili-\nties of LLM to learn the interaction across different modalities. Although the recent concurrent work\n∗Work done during an internship at Kuaishou Technology.\n†Corresponding Author.\n1\narXiv:2309.04669v3  [cs.CV]  22 Mar 2024\nPublished as a conference paper at ICLR 2024\nMLLM\nVisual Encoder\nModal Adapter\nIMAGE\nImg token\nText token\nTEXT\nMLLM\nVisual Encoder\nIMAGE\nTEXT\nRegression\nClassification\nClassification\nMLLM\nVisual Encoder\nIMAGE\nTEXT\nClassification\nClassification\nQuantization\nDiscrete Img token\nQformer\nVisual Tokenizer\n(a) Adapter-Style \n(b) Emu \n(c) Ours \nFigure 1: The comparisons between different MLLMs. (a) The adapter-style methods rely on an adapter\nnetwork to project visual features into the semantic space of LLM. During training, visual tokens are merely\ntreated as the prompt to guide text generation. (b) The concurrent work Emu adopts the regression loss for\nvisual features and jointly trains with textual tokens. (c) We craft a visual tokenizer to represent images in the\nsame discrete format as text so as to indiscriminately optimize them under a unified generative objective.\nEmu (Sun et al., 2023) proposes to unlock the text-pretrained LLM by regressing the next visual em-\nbedding during pre-training (Figure 1-(b)), the inconsistent optimization objectives for image and\ntext are not conducive to unified multi-modal modeling.\nIn this work, we introduce LaVIT (Language-VIsion Transformer), a novel general-purpose multi-\nmodal foundation model that inherits the successful learning paradigm of LLM: predicting the next\nimage\/text token in an auto-regressive manner. Our insight is that by employing a unified objective\nto indiscriminately treat tokens from different modalities, the model can seamlessly achieve “any-to-\nany” multi-modal comprehension and generation. However, the original LLM is specifically crafted\nto process discrete textual tokens. When dealing with physical signal inputs, such as images, it\nbecomes imperative to embrace a representation seamlessly compatible with text tokens. There-\nfore, we propose to translate the image into a sequence of tokens like a foreign language that LLM\ncan comprehend, so that both images and texts can be handled simultaneously under the unified\ngenerative objective without any specific architectural modification, as shown in Figure 1-(c).\nTo achieve this goal, a crucial element lies in the development of an efficient visual tokenizer for\nencoding images, which we contend should adhere to the following principles: (i) discrete visual\ntoken: While language models rely on text tokens defined by a dictionary, prior visual tokens, like\nthose derived from ViT, consist of continuous feature vectors encoding a patch. In approaches such\nas masked image modeling (He et al., 2022) or masked feature prediction (Wei et al., 2022), re-\ngressive objectives on continuous features or raw visual pixels are employed for self-supervised\npretraining. Here, we advocate for quantizing the visual tokens into a discrete form, aligning them\nwith the next-token prediction objective in language models. This form is particularly advantageous\nwhen the target distribution for the next token is multi-mode. (ii) dynamic token allocation. Given\nthe varying semantic complexity of different images, employing a fixed length of tokens to encode\nall images is compute-uneconomical. Moreover, as a key difference from textual tokens, visual\npatches exhibit a notable interdependence, making it considerably more straightforward to deduce\none token from others. This renders the next-token paradigm less effective in learning visual knowl-\nedge through self-supervision. Thus we argue for the token-merging to ensure the least redundancy\namong visual patches, thereby rendering a dynamic token number for different images.\nFollowing the aforementioned two crucial fundamentals, LaVIT introduces a novel dynamic visual\ntokenization mechanism consisting of a selector and a merger to process images. The token selector\nfirst decides which visual patches carry informative semantics and are necessary to be selected to\nencode the whole image. In order to maximally preserve the image details, the token merger further\ncompresses the unselected patches onto the retained ones according to their feature similarity. Such\na design enables each retained visual token to contain high-level semantics from multiple similar\npatches and thus reduce the redundancy among tokens. This selecting and merging strategy will\nproduce a dynamic sequence length varying from the image content itself. The retained visual to-\nkens are further quantized into discrete codes by a learnable codebook (Esser et al., 2021), which\nwill serve as the supervision signals for visual tokens during pre-training. Empowered by this vi-\nsual tokenizer, our LaVIT can be trained with a simple yet unified objective: predicting the next\nimage\/text token in the multi-modal sequence. After pre-training, LaVIT can serve as a multi-modal\ngeneralist to perform both multi-modal comprehension and generation without further fine-tuning\n(See Figure 2). The key contributions of this work are summarized as:\n2\nPublished as a conference paper at ICLR 2024\nMultimodal Language Model (LaVIT)\n[IMG]\n[\/IMG]\n...\nThe antelopes \nare eating grass\nNext Image\/Text Token Prediction\nTextual Token\nVisual Token\n...\n[IMG]\n[\/IMG]\n...\n...\n[\/IMG]\n...\n...\n[IMG]\n[\/IMG]\n...\n[IMG] ...\nText Tokenizer\nA man is playing \ntennis ball\n𝑇! 𝑡𝑜𝑘𝑒𝑛𝑠\n𝑇\" 𝑡𝑜𝑘𝑒𝑛𝑠\nVisual \nTokenizer\n(a) Pre-training\nWhat is the\nboy doing?\nflip his \nskateboard\n(b) Inference\nUnderstanding\nLaVIT\n...\n...\nMulti-modal Tokens\nTextual Tokens\nIt is swimming\nin the river\nGeneration\nLaVIT\n...\n...\nMulti-modal Tokens\nVisual Tokens\ndecode\ndecode\n[IMG]\n[\/IMG]\n[\/S]\nFigure 2: Given an image-text pair, the image is tokenized into discrete tokens and concatenated with\ntext tokens to form a multi-modal sequence. Then, LaVIT is optimized under a unified generative\nobjective. After training, it can achieve both zero-shot multi-modal comprehension and generation.\n• We introduce LaVIT, a new effective, general-purpose multi-modal foundation model that goes\nbeyond the traditional adapter-based architectures. By transforming images into a sequence of\ndiscrete tokens like a foreign language that LLM can comprehend and generate, both modalities\ncan be associated indiscriminately under a unified generative training paradigm.\n• The developed visual tokenizer can produce discrete visual tokens with dynamic length to reduce\nthe interdependence among visual patches, which enhances the representation compatibility of\nimage and text in LLM and improves computational efficiency.\n• Our LaVIT showcases the extraordinary multi-modal understanding and generation potential. It\ncan take any modality combinations as input and perform impressive in-context generation of\nboth images and text. As demonstrated by extensive experiments, LaVIT achieves state-of-the-art\nzero-shot performance on a wide range of vision-language tasks.\n2\nRELATED WORK\nVision-Language Pre-training. Researchers have extensively investigated vision-language pre-\ntraining (VLP). The pioneer works (Radford et al., 2021; Jia et al., 2021) primarily employ dual-\nencoder with contrastive objectives to learn the generic cross-modal aligned representations. Re-\ncently, the rapid progress of large language models (Chowdhery et al., 2022; Touvron et al., 2023)\nhas motivated researchers to delve into the exploration of augmenting LLM towards vision language\ntasks. The majority of these works adopt an adapter-style network (Zhang et al., 2023) that serves\nas an intermediate bridge connecting the pre-trained vision encoder and frozen language model. For\ninstance, Flamingo (Alayrac et al., 2022) develops a Perceiver Resampler to generate text-aligned\nvisual representations. Follow-up methods (Li et al., 2023; Zhu et al., 2023) mainly adopt the Q-\nFormer to project the visual semantics to the LLM’s input space. However, visual inputs in these\nmethods (Huang et al., 2023; Alayrac et al., 2022) are only considered as the prompt and not in-\nvolved in the optimization, which heavily restricts the model potential.\nVector Quantization in Computer Vision. Vector quantization (Gray, 1984; Nasrabadi & King,\n1988) is widely used in image-generative models. The VQ-VAE (Van Den Oord et al., 2017) and\nDALL-E (Ramesh et al., 2021) proposed to convert an image into a set of discrete codes in a learn-\nable discrete latent space by learning to reconstruct the original image pixels. Models like VQ-\nGAN (Esser et al., 2021) and ViT-VQGAN (Yu et al., 2021) leverage adversarial and perceptual\nobjectives to further enhance the image generation quality. The BEIT series of works also adopts\nthe quantized visual codes as the supervision in mask image modeling (Peng et al., 2022; Wang\net al., 2023). However, most of these methods tokenize the image into a token sequence with a fixed\nlength (e.g., 512 or 1024). Such a long sequence will invariably result in an excessive computa-\ntional burden. On the contrary, our proposed visual tokenizer reduces the redundancy among image\npatches and supports dynamic token length, thus enabling efficient multi-modal inference.\n3\nPublished as a conference paper at ICLR 2024\nNearest Neighbor \nLookup\n...\n...\nVisual Part Features\nQuantized Embeddings\nReconstruct\nVisual Semantic Features\nToken Selector \n& Merger\nReconstruct\nDecoder\nInput Image\nVisual \nEncoder\n(a) Dynamic Visual Tokenizer\n...\n𝑐! 𝑐\"\n𝑐# 𝑐$\n𝑐%&\"\n𝑐%\n𝑐%&!\nCodebook\nEmbeddings \nUnused in\nPre-Training\nQuantize\n...\n...\n...\nSelected Patches\n(b) The Token Merger\nVisual Part Features\nCausal\nAttention\nCross\nAttention\nFeed \nForward\nValue\nKey\nQuery\nUnselected \nPatches\n𝐿blocks\n𝑋!\n𝑋\"\n\"\n𝑋!\n𝑐\" \"#$\n%\n#𝑥# #$%\n&\n𝑐'& #$%\n&\n{𝑥}#$%\n(\nFigure 3: (a) The pipeline of the proposed dynamic visual tokenizer. It employs a token selector\nto select the most informative patches and a token merger to compress the information of discarded\npatches onto the retained ones. The whole tokenizer is trained by maximally reconstructing the\nsemantics of the input image. (b) The detailed architecture of token merger.\n3\nMETHOD\nThis work proposes to leverage the extraordinary reasoning potential of the large language model\nto facilitate the modeling of both vision and language modalities. In pursuit of this goal, the key\ncomponent is to represent these two modalities in a uniform form, so as to exploit LLM’s successful\nlearning recipe (i.e., next-token prediction). As shown in Figure 2, we develop a visual tokenizer\n(Section 3.1) to convert the non-linguistic image to the input that LLMs can comprehend. It receives\nthe vision features from a pre-trained vision encoder and outputs a sequence of discrete visual tokens\npossessing word-like high-level semantics. Coped with the crafted tokenizer, the visual input can be\nintegrated with textual tokens to compose a multi-modal sequence, which is subsequently fed into\nlarge language models under a unified auto-regressive training objective (Section 3.2).\n3.1\nSTAGE-1: DYNAMIC VISUAL TOKENIZER\nGiven an image x ∈RH×W ×C, it is first partitioned into N = HW\/P 2 non-overlapping patches,\nwhere P is the patch size. These patches are fed into a pre-trained ViT encoder (Fang et al., 2023)\nto produce a sequence of the patch features X = {x1, ..., xN}. Then, a straightforward way to\nencode images is to directly quantize the N patch-level embeddings into discrete tokens as the input\nto LLMs. This will result in a long visual sequence and bring superfluous computational burden\nsince many visual patches may contain repetitive and trivial background semantics. These redun-\ndant patches demonstrate a discernible interdependence, thereby diminishing the efficacy of the\nnext-token paradigm in learning visual knowledge via self-supervision. Consequently, the proposed\ntokenizer aims to produce visual tokens with a dynamic length according to the complexity of the\nimage content itself. As illustrated in Figure 3, it comprises a token selector and a token merger.\nToken Selector\nThe token selector takes the N patch-level features X as input. It aims to estimate\nthe importance of each image patch and selects the most informative ones that are competent enough\nto represent the semantics of the whole image. Inspired by (Rao et al., 2021), it is implemented as\na lightweight module consisting of several MLP layers to predict a distribution π ∈RN×2, where\nπi = MLP(xi). By sampling from the distribution π, a binary decision mask M ∈{0, 1}N can be\ngenerated, which indicates whether to remain the corresponding image patch. To relax the sampling\nto be differentiable, the Gumbel-Softmax trick (Maddison et al., 2016) is applied to π:\nˆ\nπi,j =\nexp((log πi,j + Gi,j)\/τ)\nP2\nr=1 exp((log πi,r + Gi,r)\/τ)\n.\n(1)\n4\nPublished as a conference paper at ICLR 2024\nwhere Gi,j is the noise sampled from a Gumbel distribution, τ is the temperature to control smooth-\nness. Then, the binary decision mask M can be sampled from ˆπ for end-to-end training.\nToken Merger\nAccording to the generated decision mask, total N image patches can be par-\ntitioned into retained and dropped groups, with T and N −T patches respectively, denoted as\nXr = {xi}T\ni=1 and Xd = {xj}N−T\nj=1 . Instead of directly discarding Xd, we develop a token merger\nto deal with it to maximally preserve the detailed semantics of the input image. As shown in the right\nof Figure 3, the token merger will progressively compress the information of discarded Xd onto the\nretained Xr according to their semantic similarity. Concretely, it consists of L stacked blocks, each\nof which has a causal self-attention layer, a cross-attention layer, and a feed-forward layer. In the\ncausal self-attention layer, each token in Xr attends to its previous tokens with a causal mask. This\nhelps to convert 2D raster-ordered features from the ViT encoder into a sequence with causal de-\npendency, thus ensuring consistency with textual tokens in LLMs. We found this strategy can result\nin better performance than bi-directional self-attention. The cross-attention layer treats the retained\ntokens Xr as the query and merges tokens in Xd based on their similarity in the embedding space.\nFormally, this layer calculates an update of Xr by:\n∆Xr = softmax\n\u0010\nQK⊤\/\n√\nD\n\u0011\nV,\n(2)\nwhere D denotes the dimension of hidden state, Q = WQXr ∈RT ×D, K = WKXd ∈R(N−T )×D\nand V = WV Xd ∈R(N−T )×D. To parallelize the computation, we adopt the predicted decision\nmask M to control the cross-attention scope between tokens without directly partitioning them into\ntwo groups. After L successive token merger blocks, we can obtain the final merged visual tokens\nˆ\nXr = { ˆxi}T\ni=1. Each token implicitly encodes high-level semantics from several image patches\npossessing similar visual patterns, which we refer to as visual part features ˆ\nXr. The token selector\nand merger work together to dynamically adjust the visual token sequence length to accommodate\nimages with different content complexity.\nVector Quantization and Training\nThe generated visual part features ˆ\nXr are then passed into\na quantizer. It tokenizes ˆ\nXr to a sequence of discrete visual codes V = {vi}T\ni=1 by looking up a\nlearnable codebook C = {ck}K\nk=1, where K is codebook size. To be specific, the ith visual code is\ncalculated by assigning ˆxi in ˆ\nXr to its closest neighbourhood code in C:\nvi = arg min\nj\n∥l2( ˆxi) −l2(cj)∥2,\nvi ∈[0, K −1],\n(3)\nwhere l2 indicates the L2 norm. Based on the indexing visual codes, we can obtain the quantized\nembeddings {cvi}T\ni=1, which is fed into a decoder to reconstruct the original visual semantic fea-\ntures X = {xi}N\ni=1. The insight behind this design is that the reconstruction quality of the image\nsemantics depends on selecting the most informative patches (token selector), along with maximally\npreserving the visual details only through the remained tokens (token merger). Thus, both token\nselector and merger can be effectively updated by encouraging a higher semantic reconstruction\nquality. The final training objective of the visual tokenizer is defined as:\nLtokenizer = 1\nN\nN\nX\ni=1\n(1 −cos(xi, xrec\ni )) + λ(ρ −1\nN\nN\nX\ni=1\nMi)2,\n(4)\nwhere cos(xi, xrec\ni ) calculates the cosine similarity between the reconstructed and real visual embed-\ndings, ρ is a pre-defined rate that controls the target mean percentage of the retained visual tokens\nand λ is set to be 2. Finally, the tokenized discrete codes {vi}T\ni=1 will serve as the supervision\nsignals for visual tokens in the following pre-training.\nDecoding to Pixels\nThe proposed visual tokenizer is capable of reconstructing visual features of\ninput images that contain high-level semantics to represent the image content but lose the pixel-level\ndetails. To recover the original pixel space, we employ a conditional de-noising U-Net ϵθ (Rombach\net al., 2022) to infill the visual details after training the visual tokenizer. Specifically, it takes the\nreconstructed xrec as the condition to progressively recover image x from a Gaussian noise. Follow-\ning Rombach et al. (2022), the parameters θ of this U-Net are optimized by ϵ prediction:\nLθ = Ez,ϵ∼N(0,1),t [||ϵ −ϵθ(zt, t, xrec)||] ,\n(5)\nwhere zt is the latent state of image x in the diffusion process. We present some pixel decoding\nresults by the trained denoising U-Net in Figure 7 of the appendix. During inference, the generated\nvisual tokens from LaVIT can be decoded into realistic images by this U-Net.\n5\nPublished as a conference paper at ICLR 2024\n3.2\nSTAGE-2: UNIFIED GENERATIVE MODELING\nGiven an image-text pair, the 2D image can be tokenized into a 1D sequence with causal dependency\nand then concatenated with text tokens to form a multi-modal sequence y = (y1, y2, .., yS). For\ndifferentiating between two modalities, two special tokens [IMG] and [\/IMG] are inserted into the\nbeginning and end of the image token sequence respectively, indicating the start and end of image\ncontent. To empower LaVIT with the capability to generate both text and images, we employ two\ndifferent concatenation forms, i.e., [image, text] and [text; image]. When the image is used as a\ncondition (on the left) to generate text, we use the continuous visual features ˆ\nXr from the token\nmerger instead of quantized visual embeddings as the input to LLMs. Such a design mitigates the\nloss of detailed information caused by vector quantization, which is crucial for fine-grained multi-\nmodal understanding tasks like visual question answering. Our LaVIT adopts the general Language\nModeling (LM) objective to directly maximize the likelihood of each multi-modal sequence in an\nauto-regressive manner:\np(y) =\nX\ny∈D\nS\nX\ni=1\nlog Pθ(yi|y<i).\n(6)\nSince both image and text are already represented as discrete token IDs, we can use the cross-entropy\nto supervise the token prediction at each location for both two modalities with a shared prediction\nhead. The complete unification in representation spaces and the training paradigms can help LLMs\nbetter learn multi-modal interaction and alignment. When LaVIT is pre-trained, it possesses the\ncapacity to perceive images akin to a foreign language, comprehending and producing them like text.\nNevertheless, most of the existing works merely regard images as prompts to guide the generation\nof text with no supervision, restricting them to solely performing image-to-text tasks.\n3.3\nMODEL PRE-TRAINING\nThe LaVIT undergoes a two-stage pre-training procedure on web-scale multi-modal corpora.\nStage-1: Tokenizer Training. Following the existing MLLMs, the ViT-G\/14 of EVA-CLIP (Fang\net al., 2023) is employed as the visual encoder. The visual codebook size is empirically set to\nK = 16384. We adopt L = 12 transformer blocks for both token merger and decoder in our\ntokenizer. During training, this encoder is kept frozen and only the parameters of the selector,\nmerger, and codebook are updated. It is trained for 50K steps on about 100M images from LAION-\n400M (Schuhmann et al., 2021) with the batch size of 2048 and ρ = 1\/3. After training the tok-\nenizer, the conditional U-Net for pixel decoding is initialized from the Stable Diffusion v1.5 (Rom-\nbach et al., 2022) and finetuned 20k steps on the same dataset. The whole stage-1 training only\nrequires pure image data without corresponding captions.\nStage-2: Unified Vision-Language Pre-training. Based on the trained visual tokenizer, all the im-\nages can be tokenized into discrete codes that are amenable to the next token prediction. We utilize\nthe raw 7B version of LLaMA (Touvron et al., 2023) as the default LLM. For image-to-text com-\nprehension (i.e., [image, text]), we employ about 93M samples from Conceptual Caption (Sharma\net al., 2018; Changpinyo et al., 2021), SBU (Ordonez et al., 2011), and BLIP-Capfilt (Li et al.,\n2022). For the text-to-image synthesis (i.e., [text, image]), an additional 100M image-text pairs from\nthe LAION-Aesthetics (A high-aesthetics image subset of LAION-5B (Schuhmann et al., 2022)) are\nused following Stable Diffusion. Moreover, to reduce catastrophic forgetting of the reasoning capac-\nity in training LLM, we employ the English text corpus from Redpajama (Computer, 2023) dataset\nand mix it with the above image-text pairs to form the multi-modal input sequence.\n4\nEXPERIMENTS\nIn this section, comprehensive experiments are conducted to systematically validate the effectiveness\nof LaVIT on a wide range of vision-language tasks. Specifically, we mainly evaluate the model’s\nzero-shot multi-modal understanding and generation capacity.\n4.1\nZERO-SHOT MULTIMODAL UNDERSTANDING\nWe first quantitatively evaluate the zero-shot multi-modal understanding capacity of LaVIT on\nImage Captioning (NoCaps (Agrawal et al., 2019), Flickr30k (Plummer et al., 2015))and Visual\n6\nPublished as a conference paper at ICLR 2024\nMethod\nImage Captioning\nVisual Question Answering\nNocaps\nFlickr\nVQAv2\nOKVQA\nGQA\nVizWiz\nFlamingo-3B (Alayrac et al., 2022)\n-\n60.6\n49.2\n41.2\n-\n28.9\nFlamingo-9B (Alayrac et al., 2022)\n-\n61.5\n51.8\n44.7\n-\n28.8\nOpenFlamingo-9B (Awadalla et al., 2023)\n-\n59.5\n52.7\n37.8\n-\n27.5\nMetaLM (Hao et al., 2022)\n-\n43.4\n41.1\n11.4\n-\n-\nKosmos-1 (Huang et al., 2023)\n-\n67.1\n51.0\n-\n-\n29.2\nKosmos-2 (Peng et al., 2023)\n-\n80.5\n51.1\n-\n-\n-\nBLIP-2 (Vicuna-7B) (Li et al., 2023)\n107.5\n74.9\n-\n-\n41.3\n25.3\nBLIP-2 (Vicuna-13B) (Li et al., 2023)\n103.9\n71.6\n-\n-\n32.3\n19.6\nCM3Leon-7B (Yu et al., 2023)\n-\n-\n47.6\n-\n-\n37.6\nEmu (LLaMA-13B) (Sun et al., 2023)\n-\n-\n52.0\n38.2\n-\n34.2\nOurs (LLaMA-7B)\n114.2\n83.0\n66.0\n54.6\n46.8\n38.5\nTable 1: Overview of zero-shot evaluation on multi-modal understanding tasks. Compared with\nprevious methods, our LaVIT achieved the best performance on both benchmarks.\nQuestion Answering (VQAv2 (Goyal et al., 2017), OKVQA (Marino et al., 2019), GQA (Hud-\nson & Manning, 2019), VizWiz (Gurari et al., 2018)). For visual question answering tasks, we\nuse a simple prompt: “Question: {} Answer: {}”.\nThe widely-used CIDEr score and VQA\naccuracy are employed as metrics to evaluate captioning and question answering, respectively.\nMethod\nModel Type\nFID(↓)\nText2Image Specialist:\nDALL-E (Ramesh et al., 2021)\nAutoregressive\n28.0\nCogView (Ding et al., 2021)\nAutoregressive\n27.1\nSD (Rombach et al., 2022)\nDiffusion\n12.6\nGLIDE (Nichol et al., 2021)\nDiffusion\n12.2\nDALL-E2 (Ramesh et al., 2022)\nDiffusion\n10.4\nMake-A-Scene (Gafni et al., 2022)\nAutoregressive\n11.8\nMUSE-7.6B (Chang et al., 2023)\nNon-Autoregressive\n7.9\nImagen-3.4B (Saharia et al., 2022)\nDiffusion\n7.3\nParti-20B (Yu et al., 2022)\nAutoregressive\n7.2\nMultimodal Large Langauge Model:\nGILL (OPT-6.7B) (Koh et al., 2023)\nLLM\n12.2\nEmu (LLaMA-13B) (Sun et al., 2023)\nLLM\n11.7\nCM3Leon-7B (Yu et al., 2023)\nLLM\n10.8\nOurs (LLaMA-7B)\nLLM\n7.4\nTable 2: The zero-shot text-to-image generation performance of\ndifferent models on MS-COCO-30K evaluation benchmark.\nThe detailed performance compar-\nisons are shown in Table 1.\nAs\nobserved, LaVIT surpasses all the\nexisting MLLMs by a large mar-\ngin on these understanding tasks.\nFor example, it achieves a CIDEr\nscore of 83.0 on the Flickr30k test\ndataset, compared to 61.5 and 74.9\nfor the Flamingo-9B and BLIP-2\n(Vicuna-7B) under the same scale of\nmodel size, respectively.\nThe per-\nformance superiority on OKVQA\n(54.6% v.s.\n44.7% of Flamingo-\n9B) further showcases the excel-\nlent multi-modal understanding ca-\npacity of LaVIT, since this bench-\nmark contains questions requiring\ncommonsense knowledge and rea-\nsoning about the content of images.\nIt is worth noting that, although the concurrent method\nEmu (Sun et al., 2023) also leverages the LLM to jointly model the vision and language, the di-\nrect feature regression objective for visual inputs makes it incompatible with text input. Therefore,\ndespite using more training data (2.6B image-text pairs and 3.8B web-crawled data) and larger LLM\n(LLaMA 13B), it still achieves inferior performance to ours on all evaluation benchmarks.\n4.2\nZERO-SHOT MULTIMODAL GENERATION\nSince the proposed visual tokenizer can represent images as discrete tokens, LaVIT possesses the\ncapability to synthesize images by auto-regressively generating visual tokens like text. We first\nquantitatively evaluate the model’s zero-shot text-conditional image synthesis performance on the\nvalidation set of the MS-COCO benchmark (Lin et al., 2014). The detailed image generation proce-\ndure is presented in Appendix A. Following the standard setting like previous text-to-image synthesis\nworks, we randomly sample 30k text prompts and calculate the zero-shot FID metric between real\nimages and generated ones. The detailed comparative results are shown in Table 2. It can be seen\nthat LaVIT outperforms all the other multi-modal language models. Compared with the concurrent\nwork Emu, it makes a 4.3 FID improvement with a smaller LLM model, demonstrating excellent\nvision-language alignment capability. In addition, LaVIT achieves comparable performance with\nstate-of-the-art text2image specialists Parti (Yu et al., 2022), while only using much fewer training\ndata (e.g., 0.2B v.s. 2B training image-text pairs compared to Parti).\n7\nPublished as a conference paper at ICLR 2024\nA photo of a light bulb in outer space traveling the galaxy with a sailing boat inside the light bulb\nOurs\nStable Diffusion v2.1 \nA television made of water that displays an image of a cityscape at night.\nIn a police outfit\nOn the beach\nIn a police outfit\nOn top of a wooden floor\nWearing pink glasses\nOn the beach\nOn top of a wooden floor\nWearing pink glasses\nImage + Image\nImage+ Text\nImage + Image\n(a) Text-to-Image Generation compared with Stable Diffusion\n(b) The Image Generation with multi-modal prompts of LaVIT\nImage+ Text\nFigure 4: The qualitative examples of multi-modal image synthesis.\nGeneration via Multi-modal Prompt\nLaVIT can seamlessly accept several modality combina-\ntions (e.g., text, image+text, image+image) as prompts to generate corresponding images with-\nout any fine-tuning. Figure 4 showcases some examples of the multi-modal image generation re-\nsults. Our LaVIT can produce high-quality images that precisely reflect the style and semantics of\nthe given multi-modal prompts, which demonstrates the strong multi-modal modeling potential of\nLaVIT. More interestingly, it can modify the original input image by the input multi-modal prompt\n(e.g., in the last example two prompt images with a dog or cat generate a dog’s portrait with the cat’s\nwhisker). This capability cannot be attained by conventional image generation models like Stable\nDiffusion in the absence of additional fine-tuned downstream data (Ruiz et al., 2023).\n4.3\nABLATION STUDY\nIn this study, we investigate the impact of various component designs in LaVIT on downstream\nperformance. All the ablations were conducted on part of pre-training data by using the clip ViT-\nL\/14 (Jia et al., 2021) as the visual encoder due to the costly training resources.\nToken Classification or Feature Regression? When jointly training vision and language via gener-\native training in text-oriented LLM, it is crucial to select the appropriate optimization objectives for\nthe 2D raster-ordered visual input. When quantizing the continuous visual tokens into the discrete\nform, it is convenient to use the cross-entropy loss for supervising the next visual token prediction\nakin to textual tokens. We conjecture that such a uniform objective for both vision and language\ncontributes to aligning them together in the LLM. To validate the superiority of the proposed visual\nquantization, we change the optimization objective of visual tokens to regressing the next visual\nembeddings by employing a regression head like Emu (Sun et al., 2023). Table 3a summarizes the\nresults of different training objectives. As observed, adopting the regression loss for the next visual\ntoken prediction will severely degrade the model performance.\n8\nPublished as a conference paper at ICLR 2024\nCode: 8892\nCode: 4107\nCode: 9146\nCode: 15116\nToken Num: 95\nToken Num: 108\nToken Num: 79\nToken Num: 97\nFigure 5: The visualization for the dynamic visual tokenizer (left) and learned codebook (right). Our tokenizer\ncan dynamically select the most informative patches based on the image content and the learned codebook can\nproduce visual codes with high-level semantics.\nDynamic or Fixed Token Length. Given the extracted visual features, a straightforward way is\nto tokenize all the patch embeddings into the visual tokens, which results in a fixed token length\n(i.e., 256). We compare the impact of fixed and dynamic tokenization strategies in terms of training\ntime, computation overhead, and zero-shot performance on multi-modal understanding. As shown in\nTable 3b, the dynamic visual tokenizer achieves superior performance while only requiring 94 tokens\non average for the input images, about 36% of the fixed one. Given that the attention computation\nin LLM exhibits a quadratic relationship with respect to the token length, this sparsification can\naccelerate the training time by 40% and reduce the computational cost in inference.\nSetting\nFlickr\nVQAv2\nOKVQA\nRegression\n60.4\n53.6\n41.9\nClassification\n73.2\n57.1\n47.0\n(a) Ablations of different training objectives.\nSetting\nNum\nTime\nFlickr\nVQAv2\nOKVQA\nFixed\n256\n30h\n71.1\n56.5\n46.4\nDynamic\n94\n18h\n74.0\n57.7\n47.6\n(b) Ablations for the effect of different tokenization strategies.\nTable 3: The ablations of different optimization objectives for visual tokens and tokenization strate-\ngies. The num and time in Table 3b indicate the mean visual token number and pre-training time.\n4.4\nQUALITATIVE ANALYSIS\nWe visualize some examples processed by the proposed dynamic tokenizer. As shown in Figure 5,\nthe token selector is capable of dynamically selecting the most informative image patches that are\ncompetent enough to represent the semantics of the whole image. Visual patches that contain repet-\nitive or trivial background semantics are filtered during this procedure, thereby reducing redundant\ninformation and improving computing efficiency. We also visualize the image patches that belong\nto the same visual code in Figure 5. As observed, the learned discrete codes can convey explicit vi-\nsual semantics and group the image patches with similar patterns together. For instance, code 4107\nrepresents a part of a skateboard, and code 9146 indicates the texture of a giraffe, which strongly\ndemonstrates the interpretability of the learned codebook.\n5\nCONCLUSION\nThis paper presents the LaVIT, a new general-purpose foundation model that is capable of simulta-\nneously performing multi-modal understanding and generation. Beyond the previous adapter-based\nmethods, it inherits the successful auto-regressive generative learning paradigm of LLMs by rep-\nresenting both vision and language in a unified discrete token representation via a dynamic visual\ntokenizer. Through optimization under the unified generative objective, LaVIT can treat images as\na foreign language, comprehending and generating them like text. Extensive experimental results\nfurther demonstrate the LaVIT’s superior capability to serve as a multi-modal generalist.\nAcknowledgement: This research work is supported by National Key R&D Program of China\n(2022ZD0160305).\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra,\nDevi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In\nProceedings of the IEEE\/CVF international conference on computer vision, pp. 8948–8957, 2019.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo:\nAn open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.\nConceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558–3568, 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https:\/\/github.com\/togethercomputer\/RedPajama-Data.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,\nZhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers.\nAdvances in Neural Information Processing Systems, 34:19822–19835, 2021.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recogni-\ntion, pp. 12873–12883, 2021.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong\nWang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.\nIn Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp.\n19358–19369, 2023.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-\na-scene: Scene-based text-to-image generation with human priors. In European Conference on\nComputer Vision, pp. 89–106. Springer, 2022.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017.\nRobert Gray. Vector quantization. IEEE Assp Magazine, 1(2):4–29, 1984.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and\nJeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617,\n2018.\n10\nPublished as a conference paper at ICLR 2024\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and\nFuru Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336,\n2022.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross B. Girshick. Masked\nautoencoders are scalable vision learners. In IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pp. 15979–15988, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pp. 6700–6709, 2019.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916.\nPMLR, 2021.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference\non Machine Learning, pp. 12888–12900. PMLR, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740–755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In Proceedings of the IEEE\/cvf\nconference on computer vision and pattern recognition, pp. 3195–3204, 2019.\nNasser M Nasrabadi and Robert A King. Image coding using vector quantization: A review. IEEE\nTransactions on communications, 36(8):957–971, 1988.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011.\n11\nPublished as a conference paper at ICLR 2024\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling\nwith vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023.\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet-\nlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of the IEEE international conference on computer vision, pp.\n2641–2649, 2015.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821–8831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit:\nEfficient vision transformers with dynamic token sparsification. Advances in neural information\nprocessing systems, 34:13937–13949, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Pro-\nceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–\n22510, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479–36494, 2022.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models.\nAdvances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556–2565, 2018.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n12\nPublished as a conference paper at ICLR 2024\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:\nBeit pretraining for vision and vision-language tasks. In Proceedings of the IEEE\/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 19175–19186, 2023.\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan L. Yuille, and Christoph Feichtenhofer.\nMasked feature prediction for self-supervised visual pre-training. In IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 14648–14658, 2022.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\nIn International Conference on Learning Representations, 2021.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng\nGao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init atten-\ntion. arXiv preprint arXiv:2303.16199, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n13\nPublished as a conference paper at ICLR 2024\nA\nTHE DETAILS OF IMAGE SYNTHESIS\nGiven a multi-modal prompt (image, text, or their combinations), LaVIT first tokenizes it as a se-\nquence of discrete tokens. By appending the special [IMG] (image start token) to the end of the\nprompt and feeding into the LLM, it can auto-regressively generate a sequence of visual tokens\nuntil reaches the special [\/IMG] (image end). These yielded discrete visual tokens are further re-\nconstructed into a feature map by the decoder of the proposed visual tokenizer, which reflects the\nhigh-level semantics of the synthetic image. Finally, the conditional denoising U-Net will take this\nfeature map as the condition to progressively recover image pixels from a Gaussian noise. Taking\nthe text prompt as an example, we illustrate the entire image synthesis procedure in Figure 6.\nLaVIT\nA man is playing tennis ball\n...\n[IMG]\nText Tokenizer\nTokenizer \nDecoder\n[\/IMG]\n...\nConditioning\nUp\nSample\nGenerated \nImage\nVisual Tokens\nDenoising U-Net\nGaussian \nNoise\nReconstruct \nVisual Features\nRecover to Pixel Space \nTextual Tokens\nFigure 6: The detailed image synthesis process for the textual prompt by LaVIT. Generation for\nprompts in other modalities is similar.\nDuring the text-to-image evaluation on MS-COCO, we use the top-k sampling to generate image\ntokens and set the maximum sampled tokens to k = 300, and softmax temperature to 1.0. Besides,\nthe Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) strategy is also employed to enable the\nmodel to yield more prompt-aligned image tokens. Specifically, we replace the input multi-modal\nprompt with the [IMG] token to conduct unconditional sampling. Both conditional and prompt-\nconditioned visual token streams are generated in LaVIT. Hence, the real sampling logits for tth\nimage token yt in the generation is a blend of both unconditional and conditional logits:\nl(yt) = l(yt|[IMG]) + αcfg · (l(yt|prompt) −l(yt|[IMG]))\n(7)\nThe larger αcfg will encourage the model to generate samples highly related to the input prompt while\nsacrificing some diversity. We set αcfg to 1.5 when evaluating the zero-shot text-to-image synthesis\non MS-COCO. LaVIT generates 4 samples for each prompt and uses a CLIP-VIT\/L model to select\nthe best one according to the image-prompt matching score.\nB\nADDITIONAL EXPERIMENTAL ANALYSIS\nB.1\nTHE IMPACT OF VISION-LANGUAGE PRE-TRAINING ON LLM\nDuring pre-training, the weights of LLM are unlocked to jointly optimize the visual and textual\ntokens. Since a new modality is included in text-oriented LLM, without setting appropriate hyper-\nparameters, its original language capability may be impaired. In this section, we discuss how to\nmitigate the forgetting of knowledge in LLM when undergoing the vision-language pre-training.\nWe chose the massive multitask language understanding benchmark (MMLU) (Hendrycks et al.,\n2020) to quantitatively measure the model’s common knowledge and reasoning ability. MMLU is\na knowledge-intensive dataset covering various domains, including humanities, STEM, and social\nsciences. It is a commonly used benchmark in evaluating the Large Language Model. Following\nLLaMA (Touvron et al., 2023), we evaluate the model in the 5-shot setting. The detailed experimen-\ntal results are reported in Table 4.\nThrough exploration, we conclude some useful strategies for conducting unified vision-language\npre-training in LLM: 1) The Learning rate of the LLM part is the most important hyper-parameter.\n14\nPublished as a conference paper at ICLR 2024\nSetting\nLRV\nLRL\nText\nMulti-modal\nHumanities\nSTEM\nSocial Sciences\nOther\nAverage\nLLaMA-7B\n-\n3e-4\n100%\n0%\n34.0\n30.5\n38.3\n38.1\n35.1\nLaVIT-7B\n1.5e-4\n1.5e-4\n66%\n33%\n26.7\n26.1\n25.7\n29.8\n27.1(−8.0)\nLaVIT-7B\n1.5e-4\n5e-5\n50%\n50%\n31.4\n27.2\n31.8\n34.5\n30.9(−4.2)\nLaVIT-7B\n1.5e-4\n5e-5\n66%\n33%\n33.5\n30.3\n35.2\n36.7\n33.6(−1.5)\nTable 4: The influence of learning rate and data proportion on the MMLU benchmark. Here, LRV\nand LRL indicate the learning rate for the visual and LLM parts in LaVIT. LaVIT learns excellent\nmulti-modal modeling capability with only a slight drop of its original common knowledge.\nDataset\nSampling prop\nC4\n80%\nGithub\n4%\nWikipedia\n4%\nBooks\n4%\nArXiv\n4%\nStackExchange\n4%\nTable 5: The used English text corpus proportion of different subsets in Redpajama.\nAlthough using a large learning rate (e−4 level) for tuning the LLM part will speed up its conver-\ngence on solving the vision-language task, it also speeds up the forgetting of its original learned\ncommon knowledge (8.0 performance drop compared to original LLaMA). To balance these, we\nsuggest of learning rate of e−5 level of LLM and e−4 level of visual. 2) Text corpus data is also\nimportant. Lack of text data will also degrade the LLM’s ability. Therefore, we suggest a proportion\nof 2:1 (L:VL) to maintain its original learned knowledge. In our experiments, the used English text\ncorpus is from the Redpajama (Computer, 2023) dataset. The detailed proportion for each subset of\nRedpajama is presented in Table 5. Finally, we set the learning rate of LLM to 5e−5 and 1.5e−4\nfor other parts in LaVIT, which only slightly affected LLM’s language ability (-1.5 on MMLU), but\nachieved the best results on both multimodal understanding and generation tasks.\nB.2\nMORE ABLATION STUDY\nLike ablations in the main text, the following experiments are conducted on part of pre-training data\nby using clip ViT-L\/14 (Jia et al., 2021) as the visual encoder.\nBi-directional or Causal Attention? The token merger in the proposed dynamic visual tokenizer\nincludes the self-attention layer to model the interaction among the visual tokens. To convert 2D\nraster-ordered visual features from the ViT encoder into a sequence with causal dependency like\ntext, causal self-attention is utilized to enforce the restriction that each visual token can solely attend\nto its preceding ones. To validate the effectiveness, we replace causal attention with bi-directional\nattention in the token merger. As shown in Table 6, causal attention outperforms bi-directional self-\nattention across all multi-modal understanding tasks. We conjecture the inferior performance can be\nattributed to partial information leakage, as the preceding token has visibility of the subsequent one,\nthereby impairing the optimization of next-token prediction in LLM.\nSetting\nFlickr\nVQAv2\nOKVQA\nBi-directional Attention\n69.9\n55.6\n44.1\nCausal Attention\n73.2\n57.3\n46.4\nTable 6: The effect of different attention manners in the token merger on downstream tasks.\nType of Visual Input Embeddings. For multi-modal understanding where the image is treated as\na condition on the left (i.e., [image, text]), the visual representations input to LLM can be quantized\nvectors from the codebook or the continuous features yielded by token merger. We investigate the\nimpact of these two different visual input forms in Table 7a. As observed, the continuous visual\nfeatures achieve better multi-modal understanding performance. This phenomenon is reasonable as\nquantized embeddings represent the centroid of continuous visual features, potentially leading to the\nloss of detailed visual content. Therefore, for input sequences with order [image, text], we utilize the\n15\nPublished as a conference paper at ICLR 2024\nOriginal\nReconstruction \nOriginal\nReconstruction \nOriginal\nReconstruction \nFigure 7: The image reconstruction results from discrete visual tokens by the denoising U-Net.\noutputs from the token merger as input to LLM, whilst regarding the corresponding discrete visual\ncodes as the supervision signal for predicting the next visual tokens.\nSetting\nFlickr\nVQAv2\nOKVQA\nQuantized\n70.3\n53.0\n44.8\nContinuous\n72.7\n56.4\n46.2\n(a) The effect of visual input embeddings forms.\nSetting\nFlickr\nVQAv2\nOKVQA\nFrozen LLM\n63.0\n52.7\n44.3\nUnlock LLM\n72.1\n57.1\n47.9\n(b) The effect of unlocking LLM in training.\nTable 7: The ablation studies of visual input embeddings format and the unlock of LLM during\npre-training. The performance is evaluated on the zero-shot setting.\nSetting\nFlickr\nVQAv2\nOKVQA\nw\/o Token Merger\n70.8\n51.9\n42.1\nw\/ Token Merger\n72.7\n56.4\n46.2\nTable 8: The effect of the proposed token merger on downstream multi-modal understanding tasks.\nThe role of Token Merger. The token merger aims to compress the semantics of unselected visual\npatches onto the retained ones according to their feature similarity. Compared to directly discarding\nthese unselected patches, this merge mechanism not only reduces the redundancy among patches\nbut also maximally preserves the visual details. The ablation study to explore the role of our token\nmerger is shown in Table 8. From the presented results, it can be observed a distinct performance\ndrop in multi-modal understanding tasks when eliminating the token merger module.\nThe Effect of Unlock LLM. In order to ascertain the efficacy of utilizing LLM for learning the\ninteraction between vision and language, we devise an ablation experiment wherein the entirety of\nLLM remains frozen while solely optimizing the parameters of the token merger during training.\nThe detailed comparison results are shown in Table 7b. One can observe that freezing the language\nmodel will restrict its powerful multi-modal modeling potential and thus result in a noticeable per-\nformance degradation. Hence, unlocking the LLM under a unified training objective will contribute\nto adequately bridging the semantics between vision and language.\nC\nMORE VISUALIZATIONS\nThe quality of pixel decoding\nIn Figure 7, we visualize some pixel decoding examples by the\ntrained denoising U-Net. It can be seen that, given the tokenized discrete visual tokens, the original\ninput images can be successfully recovered. The reconstructed examples exhibit a high degree of\n16\nPublished as a conference paper at ICLR 2024\nCode 13014: Human Arm \nCode 15090: Donut\nCode 7260: Car\nCode 12908: Toilet\nCode 15235: Bus\nCode 2223: Railway\nFigure 8: The visualization for the leaned discrete visual codes.\nConfiguration\nVL Pre-training\nTokenizer training\nVisual Encoder\nEVA-CLIP ViT-G\/14\nEVA-CLIP ViT-G\/14\nLLM init\nLLaMA-1-7B\n-\nOptimizer\nAdamW\nAdamW\nOptimizer Hyperparameters\nβ1 = 0.9, β2 = 0.95, ϵ = 1e−6\nβ1 = 0.9, β2 = 0.99, ϵ = 1e−6\nGlobal batch size\n2048\n2048\nPeak learning rate of LLM\n5e-5\n-\nPeak learning rate of Visual Part\n1.5e-4\n2e-4\nLearning rate schedule\nCosine\nCosine\nTraining Steps\n20K\n50K\nWarm-up steps\n2k\n4K\nWeight decay\n0.1\n0.01\nGradient clipping\n1.0\n1.0\nInput image resolution\n224 * 224\n224 * 224\nInput sequence to LLM\n2048\n-\nNumerical precision\nbfloat16\nbfloat16\nGPU Usage\n256 NVIDIA A100\n64 NVIDIA A100\nTraining Time\n30h\n12h\nTable 9: The detailed training hyperparameters of LaVIT\nsemantic and general structure to the original images. This consistency validates that our dynamic\ntokenizer can efficiently encode visual information using generated discrete tokens.\nThe Learned Codebook\nWe also provide more visualization examples for the learned codebook\nin Figure 8, where the image patches belonging to the same discrete visual code are arranged to-\ngether. One can observe that the learned visual code can encode explicit high-level visual semantics.\nFor example, code 13014 represents the human arm, code 7260 shows the part of the car, code\n15235 represents the bus, and code 15090 indicates the donuts. The high distinctiveness of the\nlearned codebook facilitates the unified vision-language pre-training in LLM.\nMulti-Modal Image Synthesis\nWe illustrate more image synthesis results in Figure 9, 10, 11 and\n12. As presented, LaVIT produces high-quality, content-rich, and appearance-diverse image sam-\nples that precisely reflect the prompt’s semantics. Importantly, LaVIT can produce images well\naligned with long and more complex text descriptions, while the samples generated by stable dif-\nfusion do not reflect the subtle details in some complicated prompts. Besides, LaVIT also supports\nthe multi-modal in-context image generation. As illustrated in Figure 9, the synthetic samples can\n17\nPublished as a conference paper at ICLR 2024\nThis is an \noil painting\nThis is a dog\nAn oil painting \nof this dog\nThis is the \nfirst image\nThis is the \nsecond image\nThe animal in \nthe first image is \nsitting in the \nplace of second \nimage\nInput Multi-modal Prompt\nGeneration\nThis is a dog\nThis is hat\nThis dog wears \nthis hat\nFigure 9: The examples of multi-modal in-context image synthesis.\nfully reflect the semantics of input multi-modal context, which contributes to the diversified editing\nof the input image through the multi-modal control.\nMulti-modal Understanding\nWe also visualize some multi-modal understanding examples in\nFigure 13. As observed, LaVIT is capable of understanding the visual content, generating concise\ntextual captions to depict the image, and answering diverse questions about detailed visual clues.\nD\nPRE-TRAINING DETAILS\nThe detailed training hyper-parameter settings are reported in Table 9.\nE\nLIMITATIONS\nSince LaVIT is based on the Large Language Model, it inherits LLM’s original language halluci-\nnation limitations, e.g., generating some nonexistent knowledge and making some factual errors.\nBesides, LaVIT is trained only on web-scale image-text pairs, wherein the text descriptions are\nnoisy and short, thereby rendering LaVIT insufficient for effectively modeling extensive text-image\ncorrespondences. We believe this limitation can be alleviated when leveraging more high-quality\nand long image-text interleaved documents.\n18\nPublished as a conference paper at ICLR 2024\nA small cactus wearing a straw hat and neon sunglasses in the Sahara desert.\nA high contrast photo of panda dressed as an astronaut sits at a table in a photorealistic style\nA lovely dog is reading a thick book\nDowntown Shanghai at sunrise. Detailed ink wash.\nA transparent sculpture of a duck made out of glass. The sculpture is in front of a painting of a landscape\nA high contrast photo of an astronaut riding a horse in the forest.\na super math wizard cat, richly textured oil painting\nFigure 10: The examples of text-to-image synthesis.\n19\nPublished as a conference paper at ICLR 2024\na bird wearing headphones and speaking into a \nhigh-end microphone in a recording studio.\nA section of the Great Wall in the \nmountains. detailed charcoal sketch\nA blue jay standing on a large basket of \nrainbow macarons.\na cat drinking a pint of beer, DSLR photo, art station, \noctane rendering\nA richly textured oil painting of a young badger delicately sniffing a yellow rose next to a tree trunk. \nA small waterfall can be seen in the background.\na light bulb with a ballerina in it\nA painting of two rabbits in the style of American Gothic, wearing the same clothes as in the original\nA heavy metal tiger standing on a rooftop while \nsinging on an electric guitar under a spotlight.\nTaj Mahal with its reflection. detailed \ncharcoal sketch.\nCute adorable little goat, unreal engine, cozy interior \nlighting, art station, detailed digital painting\nFigure 11: The examples of text-to-image synthesis.\n20\nPublished as a conference paper at ICLR 2024\nIn the snow\nWearing a red hat\nOn top of a purple rug\nWith a city in the background\nIn a chief outfit\nWearing a black hat\nWearing a yellow shirt\nWearing a santa hat\nIn the snow\nOn the beach\nOn top of a purple rug\nWith a city in the background\nIn a chief outfit\nOn the beach\nIn the snow\nHolding a santa hat\nOn top of a wooden floor\nWearing a red hat\nOn top of a purple rug\nWith a city in the background\nIn a chief outfit\nWearing a black hat\nIn a ploice outfit\nWearing a santa hat\nFigure 12: The examples of image synthesis with multi-modal prompt.\na siamese cat is playing \nwith a banana peel\ntwo boys are playing \nbaseball on a field\na woman sitting on the \nback of a boat in the water\na dog is carrying a purple \nfrisbee in its mouth\nImage caption\nQ: How many zebras in \nthis image?\nA: Three\nQ: What are they doing \nin this image?\nA: Skiing and competing.\nQ: Where is the car?\nA: On the side of a road \nnext to a sign.\nQ: What is in the middle \nof the image?\nA: Toilet\nVisual QA\nFigure 13: The qualitative evaluation examples on model’s multi-modal understanding performance.\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization.pdf"}
{"title":"Planting a SEED of Vision in Large Language Model","authors":"Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan","summary":"We present SEED, an elaborate image tokenizer that empowers Large Language\nModels (LLMs) with the emergent ability to SEE and Draw at the same time.\nResearch on image tokenizers has previously reached an impasse, as frameworks\nemploying quantized visual tokens have lost prominence due to subpar\nperformance and convergence in multimodal comprehension (compared to BLIP-2,\netc.) or generation (compared to Stable Diffusion, etc.). Despite the\nlimitations, we remain confident in its natural capacity to unify visual and\ntextual representations, facilitating scalable multimodal training with LLM's\noriginal recipe. In this study, we identify two crucial principles for the\narchitecture and training of SEED that effectively ease subsequent alignment\nwith LLMs. (1) Image tokens should be independent of 2D physical patch\npositions and instead be produced with a 1D causal dependency, exhibiting\nintrinsic interdependence that aligns with the left-to-right autoregressive\nprediction mechanism in LLMs. (2) Image tokens should capture high-level\nsemantics consistent with the degree of semantic abstraction in words, and be\noptimized for both discriminativeness and reconstruction during the tokenizer\ntraining phase. As a result, the off-the-shelf LLM is able to perform both\nimage-to-text and text-to-image generation by incorporating our SEED through\nefficient LoRA tuning. Comprehensive multimodal pretraining and instruction\ntuning, which may yield improved results, are reserved for future\ninvestigation. This version of SEED was trained in 5.7 days using only 64 V100\nGPUs and 5M publicly available image-text pairs. Our preliminary study\nemphasizes the great potential of discrete visual tokens in versatile\nmultimodal LLMs and the importance of proper image tokenizers in broader\nresearch.","url":"http:\/\/arxiv.org\/abs\/2307.08041v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2307.08041v2","published":1689514899000,"comment":"Technical Report; Project released at:\n  https:\/\/github.com\/AILab-CVC\/SEED","pdf_text":"Planting a SEED of Vision in Large Language Model\nYuying Ge1⋆\nYixiao Ge1,2⋆†\nZiyun Zeng2\nXintao Wang1,2\nYing Shan1,2\n1Tencent AI Lab\n2ARC Lab, Tencent PCG\nhttps:\/\/github.com\/AILab-CVC\/SEED\nAbstract\nWe present SEED, an elaborate image tokenizer that empowers Large Language\nModels (LLMs) with the emergent ability to SEE and Draw at the same time.\nResearch on image tokenizers has previously reached an impasse, as frameworks\nemploying quantized visual tokens have lost prominence due to subpar perfor-\nmance and convergence in multimodal comprehension (compared to BLIP-2, etc.)\nor generation (compared to Stable Diffusion, etc.). Despite the limitations, we\nremain confident in its natural capacity to unify visual and textual representations,\nfacilitating scalable multimodal training with LLM’s original recipe. In this study,\nwe identify two crucial principles for the architecture and training of SEED that\neffectively ease subsequent alignment with LLMs. (1) Image tokens should be\nindependent of 2D physical patch positions and instead be produced with a 1D\ncausal dependency, exhibiting intrinsic interdependence that aligns with the left-\nto-right autoregressive prediction mechanism in LLMs. (2) Image tokens should\ncapture high-level semantics consistent with the degree of semantic abstraction in\nwords, and be optimized for both discriminativeness and reconstruction during the\ntokenizer training phase. As a result, the off-the-shelf LLM is able to perform both\nimage-to-text and text-to-image generation by incorporating our SEED through\nefficient LoRA tuning. Comprehensive multimodal pretraining and instruction\ntuning, which may yield improved results, are reserved for future investigation.\nThis version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M\npublicly available image-text pairs. Our preliminary study emphasizes the great\npotential of discrete visual tokens in versatile multimodal LLMs and the importance\nof proper image tokenizers in broader research.\n1\nIntroduction\nIn recent years, Large Language Models [1, 2, 3] (LLMs) pre-trained on massive text corpus with\nstraightforward training objectives such as next-word prediction have exhibited remarkable abilities\nto understand, reason, and generate texts across a variety of open-ended tasks. Recent studies\nfurther exploit the strong generality of LLMs to improve visual understanding or generation tasks,\ncollectively referred to as Multimodal LLM (MLLM). For example, previous work [4, 5, 6, 7, 8]\nperform open-ended visual QAs through aligning visual features of a pre-trained image encoder (e.g.,\nCLIP-ViT) with the input embedding space of LLMs. GILL [9] empowers LLM with the image\ngeneration ability by aligning its output embedding space with the pre-trained Stable Diffusion (SD)\nmodel [10].\nWhile these studies have contributed to technological advancements, MLLMs have yet to achieve the\nremarkable success of LLMs in terms of emergent capabilities. We have made a bold assumption that\nthe premise for the emergence of multimodal capabilities is that text and images can be represented\n*Equal Contribution.\n†Correspondence to yixiaoge@tencent.com.\narXiv:2307.08041v2  [cs.CV]  12 Aug 2023\nOriginal Image\n2D Features\n(a) SEED Visual Tokenizer\nDiscrete Vision Codes\nGenerated Image\nTokenize\nDe-Tokenize\n5\n2\n3\n1\n7\nEncode\nGenerate\nNoise\nSemantically Consistent\nLarge Language Model\n(b) Multimodal Autoregression with SEED tokens\nVision\nText\nVision\ns\n\/s\nNext-word Prediction\n1D Causal Dependency\nFigure 1: (a) The proposed SEED is a discrete image tokenizer, producing quantized visual codes with\n1D causal dependency and high-level semantics. (b) SEED visual tokens enable LLMs to perform\nboth visual comprehension and generation through multimodal autoregression with interleaved image-\ntext data.\nand processed interchangeably in a unified autoregressive Transformer. Fortunately, we have\njust found consensus in concurrent works [11, 12], all employing image-to-text and text-to-image\ngeneration tasks to demonstrate the emergent ability of unifying visual comprehension and generation\nin one framework. Regardless of discrete or continuous visual tokens, the training paradigm can be\nsummarised into three stages: visual tokenizer training, multimodal pretraining, and multimodal\ninstruction tuning. While concurrent studies primarily emphasize multimodal training (the latter\ntwo stages), this work focuses more on the visual tokenizer (the first stage).\nWe posit that a proper visual tokenizer can facilitate the follow-up multimodal training by (i) easing\nthe semantic alignment between visual and word tokens, and (ii) enabling LLM’s original training\nrecipe (i.e., next-word prediction) for multimodal data without specific adaptation for visual tokens.\nRepresenting images as a sequence of discrete IDs is naturally compatible with the autoregressive\ntraining objective of LLMs. But unfortunately, works [13, 14] that utilize discretized visual tokens\nfor multimodal tasks have receded from prominence, as such models generally rely on super-scale\ntraining to converge, leading to substantial training costs. Moreover, we empirically found that the\ndominant tokenizer VQ-VAE [15] in existing works captures too low-level information for LLMs\nto effectively perform multimodal comprehension tasks. Existing image tokenizers fail to meet the\nrequirements of unifying visual understanding\/generation tasks and facilitating multimodal training.\nTo this end, we introduce SEED, a VQ-based image tokenizer that produces discrete visual codes\nwith 1D causal dependency and necessary high-level semantics for both visual comprehension and\ngeneration tasks, as shown in Fig. 1. The off-the-shelf LLMs can be readily equipped with SEED by\ntreating discrete visual tokens as new words and updating the vocabulary with mapped visual codes.\nIn the paper, we present an MLLM by tuning the pre-trained LLM with low-rank adaptation (LoRA)\nto efficiently align with the SEED tokenizer.\nWe would like to emphasize the design principles of SEED. (1) Why causal-dependent tokens?\nExisting visual tokens (e.g., from VQ-VAE or CLIP-ViT) are generated using 2D context, which\nis incompatible with the unidirectional attention in dominant LLMs and counterintuitive for text-\nto-image tasks requiring raster order prediction. Thus, we convert 2D raster-ordered embeddings\ninto a sequence of semantic codes with 1D causal dependency. (2) Why high-level semantics? Since\nvisual and textual tokens in LLMs are expected to be interoperable—sharing weights and training\nobjectives—they should encompass the same degree of semantics to prevent misalignment, i.e., the\nhigh-level semantics inherently present in words.*\nSpecifically, the SEED tokenizer is composed of a ViT encoder, Causal Q-Former, VQ Codebook,\nReverse Q-Former, and a UNet decoder. The ViT encoder and UNet decoder are directly derived from\nthe pre-trained BLIP-2 and SD models, respectively. (1) Tokenize: Causal Q-Former converts 2D\nraster-ordered features produced by the ViT encoder into a sequence of causal semantic embeddings,\n*While focusing on high-level semantics during tokenization, it is still possible to achieve accurate spatial\nstructural control, such as layout and mask conditions, in image generation tasks. These spatial structural\nprompts can be tokenized similarly, as demonstrated by the success of SD [10, 16].\n2\nwhich are further discretized by the VQ Codebook. (2) De-Tokenize: The discrete visual codes are\ndecoded into generation embeddings via Reverse Q-Former. The generation embeddings are aligned\nwith the latent space of SD so that realistic images with consistent semantics can be generated using\nthe off-the-shelf SD-UNet.\nDuring SEED training, only Causal Q-Former, VQ Codebook, and Reverse Q-Former are tunable.\nCausal Q-Former is optimized by image-text contrastive loss. VQ Codebook and Reverse Q-Former\nare trained toward the objectives of dual reconstruction, i.e., the reconstruction between continuous\ncausal embeddings and discrete causal codes, the reconstruction between generation embeddings\nand the paired textual features. The training objectives ensure that SEED encapsulates the essential\nsemantics for both visual comprehension and generation. Quantitative results indicate that discrete\nSEED tokens exhibit competitive performance in text-image retrieval compared to BLIP-2, and in\nimage generation compared to Stable Diffusion. With further multimodal autoregressive training,\nSEED-OPT2.7B (efficiently tuned via LoRA using 5M image-text pairs) effectively performs image-\nto-text and text-to-image tasks, yielding promising results in zero-shot image captioning and visual\nQA, as well as generating high-quality images.\nThis effort aims to integrate multimodal comprehension and generation tasks within an LLM using\ndiscrete visual tokens. Our initial exploration of proper tokenizer designs strives to promote the\ndevelopment of emergent multimodal capabilities. Future work can further scale up training for\na better tokenizer and leverage stronger LLMs (e.g., LLaMA [1]) for comprehensive multimodal\npretraining and instruction tuning.\n2\nSEED Visual Tokenizer\n2.1\nPilot Experiments of Baseline Tokenizers\nVisual tokenizer aims to represent the image as a sequence of discrete tokens. Previous work [15,\n13, 17] trains a Vector Quantized Variational AutoEncoders (VQ-VAE) by reconstructing image\npixels, while Beit v2 [18] propose vector-quantized knowledge distillation (VQ-KD) to train a visual\ntokenizer by reconstructing high-level features from the teacher model. We conduct two experiments\nto respectively align discrete representations of VQ-VAE and Beit v2 with OPT2.7B [19] model on\nCC3M [20] dataset. We evaluate the performance with zero-shot image captioning on COCO [21].\nVQ-VAE achieves CIDEr 34.0 while Beit v2 achieves 42.0. The experiment results demonstrate that\na high-level visual tokenizer, which captures semantic representations of images instead of low-level\nimage details is more effective for multimodal comprehension.\n2.2\nArchitecture\nIn this work, we introduce a VQ-based image tokenizer SEED to produce discrete visual codes with\n1D causal dependency and high-level semantics. Specifically, as shown in Fig. 2, the SEED tokenizer\nis composed of a ViT image encoder [22], Causal Q-Former, VQ Codebook, Reverse Q-Former, and\na UNet decoder [10]. The ViT encoder and UNet decoder are directly derived from the pre-trained\nBLIP-2 and SD models, respectively. We first train a Causal Q-Former to convert 2D raster-ordered\nfeatures (16×16 tokens) produced by the ViT encoder into a sequence of causal semantic embeddings\n(32 tokens). We then train a visual codebook to discretize the causal embeddings to quantized visual\ncodes (32 tokens) with causal dependency. We employ a Reverse Q-Former to decode the visual codes\ninto generation embeddings (77 tokens), which are aligned with the latent space of the pre-trained\nStable Diffusion (SD) model.\n2.2.1\nTraining Stage I: Causal Q-Former\nAs shown in Fig. 2, a set number of learnable query embeddings (32 tokens) and features of a\npre-trained ViT image encoder are fed into the Causal Q-former to encode a fixed number of\ncausal embeddings (32 tokens) of the input image. Specifically, the query embeddings can interact\nwith only previous queries through self-attention layers with causal mask, and interact with frozen\nimage features through cross-attention layers. We adopt contrastive learning to optimize Causal Q-\nformer fine-tuned from pre-trained BLIP-2 Q-Former on 5M image-text pairs including CC3M [20],\nUnsplash [23], and COCO dataset [21]. We use contrastive loss to maximize the similarity between\n3\nViT Encoder\nCausal Q-Former\nCausal Codes\n⋯\nCodebook\n⋯1\n5\n2\n7\nReverse Q-Former\nSD Decoder\nCausal Embeddings\nGeneration Embeddings\nLearned Queries\n⋯\n⋯\nLearned Queries\n⋯\n“A dog sits on the grass”\nOriginal Image\nGenerated Image\nText Encoder\nText Embeddings\nReconstruct\nReconstruct\nContrastive\nSEED Tokenize\nSEED De-Tokenize\nFigure 2: Overview of our SEED tokenizer, which produces discrete visual codes with causal\ndependency and high-level semantics.\nTable 1: Evaluation of zero-shot Image-Text Retrieval. Causal codes are quantized causal embeddings.\nModel\nFlickr30K (1K test set)\nCOCO (5K test set)\nImage →Text\nText →Image\nImage →Text\nText →Image\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@mean\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@mean\nBLIP-2 [5]\n81.9\n98.4\n99.7\n82.4\n96.5\n98.4\n92.9\n65.3\n89.9\n95.3\n59.1\n82.7\n89.4\n80.3\nSEED (causal emb)\n90.0\n99.6\n99.9\n80.0\n95.3\n97.6\n93.7\n71.9\n91.1\n95.9\n56.7\n80.7\n87.7\n80.7\nSEED (causal code)\n86.3\n98.6\n99.5\n75.9\n93.2\n96.7\n91.7\n65.7\n88.1\n93.8\n52.5\n78.0\n86.0\n77.4\nthe final causal embedding and text features of the corresponding caption, while minimizing the\nsimilarity between the final causal embedding and text features of other captions in a batch.\nEvaluation of Causal Embeddings. We evaluate the performance of Causal Q-Former on the\nzero-shot image-text retrieval task using COCO [21] and Flickr30K [24] dataset following BLIP-2.\nThe performance is measured by Recall@K (R@K) for both image-to-text retrieval and text-to-image\nretrieval. Note that we adopt the dual-stream paradigm for inference and remove the image-txt-\nmatching (ITM) rerank module in BLIP-2 for a fair comparison. As shown in Tab. 1, our Causal\nQ-former achieves better results than BLIP-2 in terms of an aggregated metric Recall@mean. It\ndemonstrates that the output query embeddings with causal dependency do not drop performance\nthan the output embeddings with bi-directional attention in BLIP-2.\n2.2.2\nTraining Stage II: Visual Quantization and De-tokenization\nAs shown in Fig. 2, we train a VQ codebook to discretize the causal embeddings (32 tokens) into\nquantized visual codes (32 tokens) on 5M image-text pairs including CC3M, Unsplash, and COCO\ndataset. Specifically, a quantizer looks up the nearest neighbor in the codebook for each causal\nembedding and obtains the corresponding code. We employ a decoder, which is a multi-layer\nTransformer [22], to reconstruct the continuous causal embeddings from discrete codes. During\ntraining, we maximize the cosine similarity between the output of the decoder and the causal\nembeddings. We further employ a Reverse Q-Former to reconstruct the textual features of a frozen\nstable diffusion model from discrete codes. A set number of learnable query embeddings (77 tokens)\nare fed into the Reverse Q-Former. The query embeddings interact with each other through self-\nattention layers, and interact with causal codes (32 tokens) through cross-attention layers for the\noutput generation embeddings (77 tokens). During training, we minimize the MSE loss between\n4\nInput\nReconstruction\nInput\nReconstruction\nFigure 3: Reconstruction images of SEED tokenizer (i.e., original image →SEED tokenize →causal\nvisual codes →SEED de-tokenize →reconstructed image), which are semantically consistent with\nthe original input images.\ngeneration embeddings and text features of SD. During inference, the generation embeddings can be\nfed into the SD-UNet to decode realistic images.\nEvaluation of Causal Codes. We evaluate the performance of SEED tokenizer on zero-shot image-\ntext retrieval, where the reconstructed causal embeddings from causal codes are used for retrieval. As\nshown in Tab. 1, discrete SEED tokens exhibit competitive performance compared to BLIP-2.\nTable 2: Evaluation of Image Genera-\ntion with CLIP similarity as the metric.\nModel\nCOCO\nFlickr30K\nGILL [9]\n67.45\n65.16\nSD [10]\n68.43\n65.40\nSEED\n68.23\n65.22\nWe further evaluate image generation on COCO and\nFlickr30K dataset. SEED first discretizes input images\ninto causal codes (32 tokens) and obtain generation em-\nbeddings (77 tokens) from Reverse Q-Former, which are\nfed into the SD-UNet for the reconstructed images. For\nthe baseline model GILL [25] and SD [10], images are\ngenerated from corresponding captions of the input images.\nWe follow GILL [25] to compute the CLIP similarity as\nthe evaluation metric for benchmarking the semantic con-\nsistency. As shown in Tab. 2, compared with the upper bound SD, our SEED only slightly drops\nperformance, and outperforms GILL in image generation.\n5\nN+5\nN+2\nN+3\nN+1\nN+7\nIMG\nSOS\n\/IMG\n1\n3\n8\n1\n4\n2\n9\n6\n5\nEOS\nA\nphoto\nof\nA\ndog\nsits\non\nthe\ngrass\nSOS\n15\n11\n12\n1\n4\n2\n9\n6\n5\nN+5\nN+2\nN+3\nN+1\nN+7\nIMG\n\/IMG\nEOS\nA\ndog\nsits\non\nthe\ngrass\nGenerate an\nimage\nImage-to-Text \nAutoregression\nText-to-Image \nAutoregression\nSEED Tokenize\n5\n2\n3\n1\n7\n“A dog sits on \nthe grass”\nLLM Tokenize\n1\n4\n2\n9\n6\n5\nImage-Text Pair\nFigure 4: Overview of the multimodal autoregressive training for SEED-OPT2.7B using efficient\nLoRA tuning. It was trained in 44 hours using only 64 V100 GPUs and 5M image-caption pairs.\nVisualization of Reconstructed Images. We visualize the reconstructed images of SEED in Fig. 3.\nThrough utilizing the Reverse Q-Former to obtain the generation embeddings from the causal visual\ncodes of the input image, realistic images can be generated using the off-the-shelf SD-UNet, which\nmaintain consistent semantics with input images.\nThe above evaluation and visualization demonstrate the versatility of SEED visual tokens for both\ncomprehension and generation tasks.\n3\nMultimodal Autoregression with SEED Visual Tokens\nBased on the pre-trained SEED tokenizer, we present SEED-OPT2.7B through fine-tuning a low-rank\nadaption (LoRA) module on a OPT2.7B [19] model with 5M image-text pairs including CC3M,\nUnsplash and COCO dataset. As shown in Fig. 4, we perform image-to-text and text-to-image\nautoregressive pre-training for unified multimodal comprehension and generation.\nImage-to-Text Autoregression. We first perform image-to-text autoregression to align the vocabulary\nof the pre-trained VQ codebook with OPT2.7B. Specifically, we use a fully-connected (FC) layer\nto linearly project the causal codes from the visual tokenizer into the same dimension as the word\nembeddings of OPT2.7B. The projected causal codes and the word embeddings of the prefix “A photo\nof” are concatenated as the input of the OPT2.7B. The text tokens of the corresponding caption is\nused as the generation target. We freeze OPT2.7B and fine-tune LoRA with the training objective of\npredicting the next text token.\nText-to-Image Autoregression. We then jointly perform image-to-text and text-to-image autoregres-\nsion to empower the LLM with the ability to generate vision tokens in addition to text tokens. For\ntext-to-image autoregressive pre-training, the word embeddings of the prefix “Generate an image”\nand a caption are fed into OPT2.7B. The visual codes of the corresponding image from our pre-trained\ntokenizer are used as the generation target. We freeze OPT2.7B and fine-tune LoRA with the training\nobjective of predicting the next vision token.\nDuring inference, given the prompt “Generate an image” and a text description, SEED-OPT2.7B\npredicts the visual tokens autoregressively. The output visual tokens are fed into the Reverse Q-Former\nfor generation embeddings, which can be decoded to generate a realistic image via SD-UNet.\nEvaluation of Multimodal Understanding. We evaluate the performance of SEED-OPT2.7B with\nzero-shot image captioning and visual question answering (vqa). For image captioning, we evaluate\non both COCO [21] test set and NoCaps [26] validation set and report BLEU@K (B@K), METEOR\n(M), ROUGEL (R), CIDEr (C), and SPICE (S) with the prompt “a photo of”. For visual question\nanswering, we evaluate on VQAv2 [27] validation set and GQA [28] test set and report Top-1\naccuracy with the prompt “Question: {} Short answer.” As shown in Tab. 3, compared with BLIP-2,\nwhich are trained on 129M image-text pairs, our SEED-OPT2.7B trained on 5M pairs achieves\npromising results on zero-shot image captioning and visual question answering with SEED discrete\nvisual tokens. Note that different from concurrent work CM3Leon [12] that uses image captioning\n6\nTable 3: Comparison between BLIP-2 (pre-trained with 129M image-text pairs) and SEED-OPT2.7B\n(5M pairs) on zero-shot Image Captioning and Visual Question Answering. S: SPICE, M: METEOR,\nR: ROUGEL, B: BLEU, C: CIDEr.\nModels\nNoCaps\nCOCO\nVQAv2\nGQA\nin\nnear\nout\noverall\nKarpathy test\nS\nS\nS\nS\nB@4\nM\nR\nC\nS\nTop-1\nTop-1\nBLIP-2 OPT2.7B [5]\n14.4\n13.8\n13.4\n13.8\n39.7\n28.9\n59.3\n131.0\n22.9\n51.9\n32.6\nSEED-OPT2.7B\n12.5\n12.3\n12.2\n12.3\n34.6\n28.4\n56.4\n119.0\n22.0\n42.8\n28.8\nA group of people riding \non the back of an elephant\nA woman standing on a \nbeach holding a surfboard\nA dragonfly is sitting \non top of a computer\nlittle girl eating a \nslice of watermelon\nImage Captioning\nQ: What kind of event are \nthe people involved in?\nA: wine tasting\nQ: Where is the clock located?\nA: on the side of the building\nVisual Question Answering\nQ: What is beside the elderly man?\nA: a white dog sitting on the bench\nin a park\nQ: What are they holding?\nA: a cake with a picture of\nwomen on it\nFigure 5: Qualitative examples of SEED-OPT2.7B on image captioning (with a prompt “a photo of”)\nand open-ended visual question answering. Our model has not been trained on any VQA dataset.\nand vqa datasets for supervised fine-tuning, our SEED-OPT2.7B pre-trained with image-to-text\nautoregression using the prefix “A photo of” can perform zero-shot visual question answering by\nunderstanding free-form questions and predicting open-form answers.\nWe also show qualitative examples of SEED-OPT2.7B on image captioning (with a prompt “a photo\nof”) and vqa. As shown in Fig. 5, our model can generate captions than describe the visual content,\nand answer a variety of questions.\nEvaluation of Multimodal Generation. We showcase qualitative examples of text-to-image genera-\ntion results with our SEED-OPT2.7B in Fig. 6. Given the textual description, SEED-OPT2.7B can\ngenerate realistic images that are semantically relevant to the description.\nSEED can facilitate alignment between visual tokens and LLMs, as evidenced by SEED-OPT2.7B,\nalready capable of performing text-to-image and image-to-text generation tasks after LoRA tuning.\n4\nRelated Work\nMultimodal Large Language Models for Comprehension. With the impressive success of Large\nlanguage models [1, 2, 3] (LLMs), recent studies work on Multimodal LLM (MLLM) to improve\nvisual comprehension through utilizing the strong generality of LLMs. Previous work [4, 5, 6, 29, 7,\n8, 30, 31] align visual features of pre-trained image encoder with LLMs on image-text datasets, and\nempower LLMs with the ability to interpret visual information with textual descriptions. However,\nthese work commonly use the prediction of the next text token as the training objective and exert no\nsupervision for vision data, thus can only output texts given multimodal vision and language inputs.\n7\nSnow mountains under a sunny sky\nA cat is lying on the couch\nA train is running on a track\nBurning fire in the open field\nA bird is sitting on top of a bare tree\nYellow flowers with\ngreen leaves in a vase\nA woman in a white dress\nis standing in front of walls\nA beach next to a rocky cliff\nA yellow car parked in front of a building\nFireworks in the city at night\nA dog is sitting on the green grass\nA room with lots of green plants\nA large house with a swimming pool\nA boy is riding a bike in the street\nA cup of coffee on\ntop of a wooden table\nA waterfall is flowing\ndown from a forest\nFigure 6: Text-to-image generation results when inferring with SEED-OPT2.7B.\nMultimodal Large Language Models for Generation. To empower LLMs with the image genera-\ntion ability, CogView [14] pre-trains a visual tokenizer by reconstructing image pixels, and fine-tunes\nGPT models [2, 32] with the objective of next token prediction, where both image and text tokens\nare equally treated. GILL [25] learns a mapping between the embeddings of a LLM and a frozen\npretrained image generation model. Both work aim to generate images with LLMs, without being\nexplicitly designed for multimodal comprehension.\nVisual Tokenizer. Visual tokenizer aims to represent the image as a sequence of discrete tokens\nsimilar to natural language. Previous work [15, 13, 17] trains a Vector Quantized Variational\nAutoEncoders (VQ-VAE) as a visual tokenizer by reconstructing the pixels of the input images,\nwhich captures only low-level details of images such as color, texture and edge. Beit v2 [18] trains a\nsemantic-rich visual tokenizer through reconstructing high-level features from the teacher model, but\nits visual codes from 2D features of a vision transformer [22] are incompatible with the unidirectional\nattention in dominant LLMs for multimodal generation.\n8\n5\nConclusion\nWe present SEED, a discrete image tokenizer, designed based on the premise that visual tokens\ncompatible with LLMs should capture high-level semantics while being generated with a 1D causal\ndependency. SEED enables LLMs to be trained with multimodal data following the original recipe\nof text (i.e., next-word prediction), which is mature and scalable. The trained multimodal LLM\nis capable of both image-to-text and text-to-image generation tasks, taking one more step toward\nemergent multimodal capabilities. We hope that our SEED would draw increased attention to visual\ntokenizers. A more rational visual tokenizer could substantially reduce the cost and complexity\nof multimodal LLM training, promoting lower-carbon, large-scale model training. Moreover, we\neagerly anticipate the “germination” of vision (imagination) seeds within LLMs. The project is still\nin progress. Stay tuned for more updates!\nAcknowledgements\nWe sincerely acknowledge Sijie Zhao (Tencent AI Lab) and Chen Li (ARC Lab, Tencent PCG) for\ntheir engaging discussions.\nReferences\n[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[4] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\n[5] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[6] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[7] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint\narXiv:2304.15010, 2023.\n[8] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[9] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\n[10] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pages 10684–10695, 2022.\n[11] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang.\nGenerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023.\n[12] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu, Babu Arun, Tang\nBinh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak Adam, Howes Russ, Sharma Vasu, Xu Jacob,\nSinger Uriel, Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli,\nZettlemoyer Luke, and Aghajanyan Armen. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. 2023.\n[13] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,\npages 8821–8831. PMLR, 2021.\n9\n[14] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in\nNeural Information Processing Systems, 34:19822–19835, 2021.\n[15] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[16] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. Gligen: Open-set grounded text-to-image generation. 2023.\n[17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition, pages\n12873–12883, 2021.\n[18] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with\nvector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\n[19] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[20] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565,\n2018.\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740–755. Springer, 2014.\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[23] Unsplash. https:\/\/github.com\/unsplash\/datasets.\n[24] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78, 2014.\n[25] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\n[26] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the\nIEEE\/CVF international conference on computer vision, pages 8948–8957, 2019.\n[27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\n[28] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE\/CVF conference on computer vision and\npattern recognition, pages 6700–6709, 2019.\n[29] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint\narXiv:2303.16199, 2023.\n[30] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.\n[31] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023.\n[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n10\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Planting a SEED of Vision in Large Language Model.pdf"}
{"title":"Emu: Generative Pretraining in Multimodality","authors":"Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang","summary":"We present Emu, a Transformer-based multimodal foundation model, which can\nseamlessly generate images and texts in multimodal context. This omnivore model\ncan take in any single-modality or multimodal data input indiscriminately\n(e.g., interleaved image, text and video) through a one-model-for-all\nautoregressive training process. First, visual signals are encoded into\nembeddings, and together with text tokens form an interleaved input sequence.\nEmu is then end-to-end trained with a unified objective of classifying the next\ntext token or regressing the next visual embedding in the multimodal sequence.\nThis versatile multimodality empowers the exploration of diverse pretraining\ndata sources at scale, such as videos with interleaved frames and text,\nwebpages with interleaved images and text, as well as web-scale image-text\npairs and video-text pairs. Emu can serve as a generalist multimodal interface\nfor both image-to-text and text-to-image tasks, and supports in-context image\nand text generation. Across a broad range of zero-shot\/few-shot tasks including\nimage captioning, visual question answering, video question answering and\ntext-to-image generation, Emu demonstrates superb performance compared to\nstate-of-the-art large multimodal models. Extended capabilities such as\nmultimodal assistants via instruction tuning are also demonstrated with\nimpressive performance.","url":"http:\/\/arxiv.org\/abs\/2307.05222v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2307.05222v2","published":1689079539000,"comment":"Accepted to ICLR 2024. Code and Models:\n  https:\/\/github.com\/baaivision\/Emu","pdf_text":"Published as a conference paper at ICLR 2024\nEMU: GENERATIVE PRETRAINING IN MULTIMODALITY\nQuan Sun1∗\nQiying Yu2,1∗\nYufeng Cui1∗\nFan Zhang1∗\nXiaosong Zhang1∗\nYueze Wang1\nHongcheng Gao1\nJingjing Liu2\nTiejun Huang1,3\nXinlong Wang1†\n1 Beijing Academy of Artificial Intelligence\n2 Tsinghua University\n3 Peking University\nCode & Demo: https:\/\/github.com\/baaivision\/Emu\nABSTRACT\nWe present Emu, a multimodal foundation model that seamlessly generates images\nand text in multimodal context. This omnivore model can take in any single-\nmodality or multimodal data input indiscriminately (e.g., interleaved image, text\nand video) through a one-model-for-all autoregressive training process. First,\nvisual signals are encoded into embeddings, and together with text tokens form\nan interleaved input sequence. Emu is end-to-end trained with a unified objective\nof classifying the next text token or regressing the next visual embedding in the\nmultimodal sequence. This versatile multimodality empowers the leverage of\ndiverse pretraining data sources at scale, such as videos with interleaved frames and\ntext, webpages with interleaved images and text, as well as web-scale image-text\npairs and video-text pairs. Emu can serve as a generalist multimodal interface\nfor both image-to-text and text-to-image tasks, supporting in-context image and\ntext generation. Across a broad range of zero-shot\/few-shot tasks including image\ncaptioning, visual question answering, video question answering and text-to-image\ngeneration, Emu demonstrates superb performance compared to state-of-the-art\nlarge multimodal models. Extended capabilities such as multimodal assistants via\ninstruction tuning are also demonstrated with impressive performance.\n1\nINTRODUCTION\nWith text corpus at massive scale, Large Language Models (LLMs) (Brown et al., 2020; Chowdh-\nery et al., 2022; Touvron et al., 2023) with straightforward training objectives such as next-word-\nprediction learn to understand, reason, and generate text with unprecedented accuracy and fluency,\npaving the way for diverse real-life applications (Schulman et al., 2022) unthinkable a decade ago.\nRecent studies (Alayrac et al., 2022; Driess et al., 2023; Hao et al., 2022) further investigate Large\nMultimodal Models (LMMs) beyond LLMs. Flamingo (Alayrac et al., 2022) connects a powerful\nlanguage model with a pretrained vision encoder and inserts learnable layers to capture cross-modality\ndependencies, demonstrating strong abilities in multimodal zero-shot and in-context learning. Recent\nworks (Li et al., 2023b; Dai et al., 2023; Huang et al., 2023; Liu et al., 2023b; Zhu et al., 2023a; Ye\net al., 2023; Li et al., 2023a; Gong et al., 2023) adopt this framework and build LMM by docking a\nvision encoder with an LLM.\nThe prevailing training objective in such LMMs is predicting the next text token (Alayrac et al.,\n2022; Hao et al., 2022; Huang et al., 2023; Zhu et al., 2023a; Liu et al., 2023b; Li et al., 2023a),\ntypically with a frozen vision encoder and no supervision for the vision part, which highly restricts\nmodel capacity. Besides, these LMMs are mostly trained on image-text pairs or documents, while\noverlooking video data as a potential scalable source of interleaved multimodal data. Documents\ninterleaved with images (e.g., textbooks, webpages) provide an intuitive representation of complex\nconcepts, and have proved to be effective in empowering models with multimodal in-context learning\nability (Alayrac et al., 2022; Zhu et al., 2023b). Videos, which usually contain interleaved image\nframes and subtitles (Figure 3), are an abundant source of multimodal data that naturally contains\ndense visual signals and encodes stronger cross-modal correlations with text than regular multimedia\n∗Equal contribution. † Correspondence to wangxinlong@baai.ac.cn.\n1\narXiv:2307.05222v2  [cs.CV]  8 May 2024\nPublished as a conference paper at ICLR 2024\nQuestion: What's the woman doing in the video? Answer:\nThe woman in the video is wearing a pair of \nApple VR goggles while sitting in an airplane. \nShe appears to be using the goggles to watch \nsomething, possibly a video or a 360-degree \nview of the airplane's surroundings.\nInput Prompt\nCompletion\nthe movie poster for \"Fight Club.\" The poster features two \nmen standing next to each other, with the words \"Fight \nClub\" written in bold letters above them. The background \nof the poster is dark and ominous, giving the impression of \na gritty and intense movie.\nInput Prompt\nCompletion\nThe image shows a photo of\nQuestion: Can you tell me about this image? Answer:\nthis image is the cover of a book called \"Mom's Family \nCalendar\" by Sandra Boynton. The book is designed to help \nmoms keep track of their family's schedule and activities.\nThere is a godzilla.\nThere is a chinchilla.\nThere are three pandas.\nInput Prompt\nCompletion\nThe animal in the first image \nsurrounded with the plant in \nthe second image:\nThis is the first image:\nThis is the second image:\nInput Prompt\nCompletion\nAn elder man is playing \nguitar, oil on canvas, \nimpressionism.\nInput Prompt\nCompletion\nQuestion: What is unusual about this video? Answer:\nThe unusual aspect of this video is that it \nfeatures a giraffe standing on a diving board \nand jumping into a swimming pool. This is \nnot a common sight, as giraffes are not \ntypically found in swimming pools.\nImage Captioning\nImage Question Answering\nIn-context Completion\nImage-to-Image\nText-to-Image\nVideo Question Answering\nFigure 1: Emu as a generalist interface for diverse vision-language applications, such as image\ncaptioning, image\/video question answering, in-context image-to-text and text-to-image generation,\nand image blending. More examples in Appendix E.\n2\nPublished as a conference paper at ICLR 2024\ndocuments. Furthermore, public videos (especially user-generated clips) possess richer content\ndiversity than Common Crawl1, from which current training datasets mainly originate.\nIn this work, we introduce Emu, a large multimodal model that learns from both video and image\ndata interleaved with text, under a unified objective of predicting the next visual or text token in\nan autoregressive fashion. To take advantage of rich web-scale data with an omnivore capacity, we\nformulate diverse sources of multimodal data (e.g., videos with subtitles, webpages with images and\ntext) into a unified format of interleaved image embeddings and text tokens (videos are converted\ninto an interleaved sequence of randomly-selected frames and subtitles). Specifically, visual signals\nare first encoded into embeddings via a visual representation model EVA-CLIP (Sun et al., 2023),\ninstead of being converted into discrete tokens. These visual embeddings together with text tokens\nconstitute an interleaved multimodal input sequence, which will be fed into Emu for training.\nWe pretrain Emu on these multimodal data sequences under a simple unified objective: predicting\nthe next element in a multimodal sequence. Different from existing LMMs that compute the predict-\nthe-next loss on text tokens only, in training Emu, all input elements including both discrete text\ntokens and continuous image embeddings are accounted for loss computation. We adopt the cross-\nentropy classification loss for discrete text tokens, and the ℓ2 regression loss for continuous visual\nembeddings. As raw images typically lack the left-to-right causal dependency as in language, Emu\ndoes not perform image generative pretraining in the original pixel space. Instead, visual embeddings\nare transformed into a causal latent space via Causal Transformer, which accepts the image encodings\ngenerated by EVA-CLIP as input, and outputs N tokens that capture the causal dependency of the\ngiven image (as illustrated in Figure 2).\nPretrained with the unified objective and diverse data forms, Emu can serve as a generalist interface\nfor both image-to-text and text-to-image tasks by performing various types of completion in a\nmultimodal sequence. As illustrated in Figure 1, Emu accepts multimodal prompts (e.g., text, image,\nvideo, or their interleaved sequence) and generates multimodal response (for image generation,\nvisual embeddings are decoded by a fine-tuned diffusion model). Further, Emu demonstrates\nimpressive capabilities such as in-context text and image generation (the 2nd block of Figure 1),\nimage blending (the 5th row of Figure 1 that combines a cat and a tiger into a real-looking tiger-cat),\nvideo understanding (the last block of Figure 1), and real-world knowledge grounding (Section 5.4).\nWe evaluate Emu on a broad range of zero-shot and few-shot tasks including image captioning,\nvisual question answering, video question answering, and text-to-image generation. For qualitative\ndemonstration, we also build an effective multimodal assistant via instruction tuning on multimodal\nconversational data. The instruction-tuned Emu assistant can effectively follow human instructions\nand interact with users via multimodal response.\n2\nEMU: PREDICT THE NEXT IN MULTIMODALITY\n2.1\nARCHITECTURE\nEmu is a large-scale multimodal model that performs completion in multimodality, i.e., perceiving\ninterleaved multimodal input and generating outputs varying in modalities. As illustrated in Figure 2,\nEmu consists of four parts: Visual Encoder, Causal Transformer, Multimodal Modeling, and Visual\nDecoder. We leverage pretrained EVA-CLIP (Sun et al., 2023), LLaMA (Touvron et al., 2023) and\nStable Diffusion (Rombach et al., 2022) to initialize the Visual Encoder, the Multimodal Modeling\nLLM and the Visual Decoder, respectively.\nGiven any sequence of interleaved image, text and video, we first encode the image into dense\nvisual features via EVA-CLIP, then transform the encodings into a fixed number of N visual causal\nembeddings via Causal Transformer. Similarly, we encode a video of T frames into T × N visual\ncausal embeddings. Two special image tokens [IMG] and [\/IMG] are prepended and appended for\neach image or frame, respectively, to represent the beginning and end of the encoded image\/frame\nembeddings. The visual causal embeddings are combined with text tokens to form multimodal\nsequences that are fed into the Multimodal Modeling LLM for unified autoregressive modeling. We\nappend <s> and <\/s> tokens to the start and the end of each sequence. In inference, we fine-tune\nthe Visual Decoder to decode the visual embeddings into a realistic image.\n1https:\/\/commoncrawl.org\/\n3\nPublished as a conference paper at ICLR 2024\nEncoder\n（EVA-CLIP）\nMultimodal Modeling with LLM\n(LLaMA)\n…\nClassification\nAn\n[\/IMG]\n[IMG]\negg that will\n<\/s>\n<s>\nemu\nhatch into a\nemu\nbaby\n[\/IMG]\n[IMG]\n…\negg that will\nemu\nhatch into a\nemu\nbaby\nAn\n<\/s>\n[IMG]\n[IMG]\nRegression\n…\n…\nDecoder\n(Stable Diffusion)\nCausal Transformer\nFigure 2: Emu unifies the modeling of different modalities in an auto-regressive manner. Visual\nsignals are first encoded into embeddings, and together with text tokens form an interleaved sequence.\nThe training objective is to either classify the next text token or regress the next visual embedding.\nIn inference, regressed visual embeddings are decoded into a realistic image via a fine-tuned latent\ndiffusion model.\nCausal Transformer. Auto-regressively modeling images in raster order is counter-intuitive and has\nnot demonstrated satisfactory performance, which may be attributed to the fact that images naturally\npossess 2D structures and are not perceived as sequential signals like text. To better capture the\ncharacteristics of images and achieve unified modeling of different modalities, we propose a Causal\nTransformer module to transform 2D spatial visual signals to 1D causal sequences in a latent space\nZ. Specifically, given an image I with its encodings g(I) from EVA-CLIP as condition, Causal\nTransformer accepts randomly initialized embeddings {e1, e2, . . . , eN} as input, and outputs N\nembeddings {z1, z2, . . . , zN} that capture the causal dependency of the given image. The architecture\nof Causal Transformer is similar to the decoder of Transformer (Vaswani et al., 2017), with each\nblock consisting of a causal self-attention layer, a cross-attention layer, and a feed-forward layer.\nThe cross-attention layer aggregates visual information from the image embeddings extracted from\nEVA-CLIP, where the visual embeddings are treated as keys and values, and the outputs from the\nprevious causal attention layer serve as queries.\nVisual Decoder. We use a latent diffusion model to decode visual embeddings into images, and adopt\nthe weights of Stable Diffusion (Rombach et al., 2022) for initialization. Specifically, we feed N\nvisual embeddings generated by Emu into the diffusion model as conditions for image decoding.\nWe replace the linear projections of the cross-attention modules in Stable Diffusion with new linear\nlayers that accommodate the dimension of Emu and Stable Diffusion.\n2.2\nTRAINING OBJECTIVE\nGiven an unlabeled web-scale corpora D consisting of interleaved multimodal sequences x =\n(x1, x2, . . . , xn), where x can be vision-language sequences of various forms, such as image-text\npairs, image-text interleaved documents, or videos with subtitles. xi can be a signal unit (text or\nimage token) from any arbitrary modality. We first convert all continuous 2D signals (images and\nvideo frames) into 1D causal latent embedding sequences using Causal Transformer, then insert\nthem back into the corresponding places in the sequence x. The resulting sequence is represented\nas u = (u1, u2, . . . , um), where ui can be either a discrete text token, or a visual embedding that\ncaptures causal dependency with neighboring visual embeddings.\nWe approximate the likelihood of the web-scale corpora p(x) with p(u), and maximize the likelihood\nin a unified auto-regressive manner as follows:\nmax\nθ\nX\nu∈D\n|u|\nX\ni=1\nlog P(ui|u1, . . . , ui−1; θ) ≈p(x)\n(1)\n4\nPublished as a conference paper at ICLR 2024\nTwo types of losses are adopted to optimize this objective. For discrete text tokens, cross-entropy\nloss is used to supervise classification in the predefined vocabulary with a language modeling head.\nFor continuous visual embeddings, ℓ2 regression loss is adopted with a separate regression head.\n2.3\nGENERALIST INTERFACE\nThe unified auto-regressive modeling of different modalities endows Emu with a powerful ability to\nserve as a multimodal generalist that can perform any types of completion in a multimodal sequence,\ni.e., accepting multimodal sequence as input, and outputting signals across vision and language\nmodalities. For example, given two examples as the prompt, Emu automatically infers and completes\nthe corresponding task given a new input, as shown in the second block of Figure 1.\nSpecifically, given a multimodal context, if the expected output format is text, Emu will use the\nlanguage modeling head to generate discrete text tokens. If the desired output is image, we will\nappend a [IMG] token at the end of the input sequence, then Emu will autoregressively generate N\nvisual embeddings that will then be sent to the visual decoder for decoding into a real-world image.\n3\nEMU TRAINING\nWe pretrain Emu with web-scale data across modalities in various forms, including image-text\npairs (LAION-2B (Schuhmann et al., 2022), LAION-COCO (lai, b)), interleaved images-text data\n(MMC4 (Zhu et al., 2023b)), video-text pairs (WebVid-10M (Bain et al., 2021)), and our collected in-\nterleaved video-text data (YT-Storyboard-1B). All these data are formulated as multimodal sequences,\nfrom which Emu learns under the objective of predict-the-next-element in a unified auto-regressive\nmanner. After pretraining, we finetune a decoder to transform visual embeddings into realistic images.\n3.1\nDATA\nImage-text Pairs. We use the image-text pairs from LAION-2B (Schuhmann et al., 2022) and\nLAION-COCO (lai, b) for pretraining. LAION-2B provides images paired with noisy alt-texts from\nthe web, and LAION-COCO is its 600M subset that is captioned by BLIP (Li et al., 2022).\nVideo-text Pairs. WebVid-10M (Bain et al., 2021) is an extensive dataset consisting of a large\ncollection of short videos with textual descriptions. These videos are sourced from materials websites\nwith diverse contents and a strong correlation between text and video. We use heuristic rules to\nremove irrelevant metadata (e.g.resolution of the original video, camera parameters).\nInterleaved Image and Text. Large-scale image-text interleaved data plays a crucial role in unlocking\nthe in-context learning ability of multimodal models. We leverage the Multimodal-C4 (MMC4)\ndataset (Zhu et al., 2023b), an expanded version of the text-only C4 (Raffel et al., 2020). MMC4\ncomprises a collection of approximately 75 million image-text-interleaved documents, with 400\nmillion images and 38 billion tokens in total. From each document, we sample a random subsequence\nof L = 1024 take up to the first N = 5 images. Additionally, we randomly sample N = 5 images along\nwith their corresponding sentences to construct a subsequence of L = 512.,\nInterleaved Video and Text. Videos with subtitles also present a promising and scalable source\nof interleaved multimodal data. We introduce the YT-Storyboard-1B dataset which collects 18\nmillion videos and their corresponding subtitles from YouTube2 using the video-ids provided by the\nYT-Temporal-1B dataset (Zellers et al., 2022). Instead of raw videos, we collect storyboard images\n(about 1.8 billion images in total), a set of thumbnails provided by the YouTube website for quick\nvideo viewing. The combination of storyboard thumbnails and subtitles creates a natural interleaved\nsequence of video and text ordered by timestamps, as in Figure 3. More details are in Appendix A.1.1.\n3.2\nPRETRAINING\nWe initialize Emu’s Visual Encoder with the 1B version of EVA-01-CLIP (Sun et al., 2023), and\nMultimodal Modeling LLM with the 13B version of LLaMA (Touvron et al., 2023). LLaMA is a\ndecoder-only Transformer (Vaswani et al., 2017) and EVA-01-CLIP is a 40-layer ViT (Dosovitskiy\n2https:\/\/www.youtube.com\n5\nPublished as a conference paper at ICLR 2024\n1:15\n1:20\nthe female puts the \negg in a shallow pit \nthe male bird\n1:25\n1:30\n1:35\n1:21\ncovers it with \nleaves for \nprotection\nthe male even \nsafeguards the chicks till\nthey attained maturity\n1:19\nthe female puts \nthe egg in a \nshallow pit the \nmale bird\ncovers it with \nleaves for \nprotection\nincubation of eggs is the job of \nthe male bird in nature.\nthe male even  safeguards the \nchicks till they attained maturity\nInterleaved Video-text Data\nVideo Storyboard Images with Subtitles \nSort by timestamp\n1:26\nincubation of eggs \nis the job of the \nmale bird in nature\n1:40\n1:38\nthis is an emu egg \nwhich weighs 600 to\n800 grams\nthis is an emu \negg which \nweighs 600 to\n800 grams\nFigure 3: Interleaved video-text data. The combination of storyboard thumbnails and subtitles\ncaptions creates a natural interleaved sequence of video and text that is ordered by the timestamps.\net al., 2021). The Causal Transformer comprises 12 blocks, each of which consists of a causal\nself-attention layer, a cross-attention layer, and a feed-forward layer. Random initialization is used\nfor Causal Transformer. The total number of parameters of Emu is 14B and is trained end-to-end.\nWe use a batch size of 128 for image-text pair data, 64 for interleaved image-text data, 16 for video-\ntext pair and interleaved video-text data. Detailed pertaining hyperparameters are in Appendix A.1.1.\nFor each video, we randomly sample 8 frames for pretraining, and all images\/frames are resized into\n224×224 resolution. For image-text pair and interleaved data, we randomly put each image before or\nafter its corresponding sentence. We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps\nwith around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.\n3.3\nVISUAL DECODING\nAfter pretraining, we tune the visual decoder with both LAION-COCO (lai, b) and LAION-\nAesthetics (lai, a) (a high-aesthetics quality subset of LAION-5B (Schuhmann et al., 2022)) image-text\npair datasets under text-to-image task. Specifically, We initialize the diffusion model with Stable\nDiffusion v1.5. We freeze the Visual Encoder, Multimodal Modeling LLM in Emu, and the VAE\nin diffusion model during training, with only the parameters of U-Net updated. For each training\nsample, we append the [IMG] token to the end of the input text and feed it into the Multimodal\nModeling LLM, which will then generate N visual embeddings in an auto-regressive manner. These\nvisual causal embeddings are fed into Image Decoder as the condition for image generation training.\nWe follow the model setups of Stable Diffusion v1.5. We train the diffusion model with 32 A100-40G\nGPUs for 15k iterations. Detailed hyperparameters are in Appendix A.2. To further improve sample\nquality, we randomly drop image embeddings condition by 10% of the time during training to enable\nclassifier-free guidance (Ho & Salimans, 2022).\n4\nINSTRUCTION TUNING\nLanguage instruction tuning has helped pretrained language models to align with user inten-\ntions (Ouyang et al., 2022; Wang et al., 2022c; Taori et al., 2023; Zheng et al., 2023) and generalize\nto unseen tasks (Wei et al., 2022; Chung et al., 2022). We apply multimodal instruction tuning on\nEmu to align it with human instructions through supervised finetuning on publicly available datasets,\nincluding language instructions from ShareGPT (Zheng et al., 2023) and Alpaca (Taori et al., 2023),\nimage-text instructions from LLaVA (Liu et al., 2023b), and video instructions from VideoChat (Li\net al., 2023c) and Video-ChatGPT (Maaz et al., 2023). Dataset details can be found in Appendix B.1.\nIn instruction tuning, we freeze all parameters of pretrained Emu, and fine-tune a low-rank adaption\n(LoRA) module (Hu et al., 2022). The main focus of instruction tuning is to align the model with\nnatural language instructions, which are less relevant to vision features. Thus, we attach LoRA\nmodules only to the self-attention layers of the Multimodal Modeling LLM, and add no adaptation to\nthe Vision Encoder. Training details can be found in Appendix B.1.\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Zero-shot comparison, * indicates that the zero-shot prompt is built by using two examples\nfrom the task, where their corresponding images have been removed. Emu-I is the instruction-tuned\nEmu model. The best results are bold and the second best are underlined.\nModels\nImage-Text Tasks\nVideo-Text Tasks\nCOCO\nNoCaps\nFlickr30K\nVQAv2\nOKVQA\nVizWiz\nVisDial\nMSVDQA\nMSRVTTQA\nNExTQA\nPer-task Finetuning\nPALI-X-55B\n149.2\n126.3\n-\n86.0\n66.1\n70.9\n-\n-\n47.1\n38.3\nMetaLM\n82.2\n58.7\n43.3\n41.1\n11.4\n-\n-\n-\n-\n-\nKosmos-1\n84.7\n-\n67.1\n51.0\n-\n29.2\n-\n-\n-\n-\nFlamingo-9B *\n79.4\n-\n61.5\n51.8\n44.7\n28.8\n48.0\n30.2\n13.7\n23.0\nEmu\n112.4\n96.5\n72.0\n52.0\n38.2\n34.2\n47.4\n18.8\n8.3\n19.6\nEmu *\n-\n-\n-\n52.9\n42.8\n34.4\n47.8\n34.3\n17.8\n23.4\nEmu-I\n120.4\n108.8\n77.4\n57.2\n43.4\n32.2\n43.0\n34.6\n16.8\n5.8\nEmu-I *\n-\n-\n-\n62.0\n49.2\n38.3\n51.1\n37.0\n21.2\n19.9\nAll instruction-tuning data are packed with this template:\n<System Message>\n[USER]:\n<Instruction> [ASSISTANT]:\n<Answer>,\n(2)\nwhere [USER] and [ASSISTANT] are special tokens initialized from the embeddings of words\n‘user’ and ‘assistant’, respectively. <System Message> varies depending on the specific task\n(Appendix B.2). <Instruction> and <Answer> are actual slots for human instructions and\nassistant answers, and only <Answer> is accounted for loss computation.\n5\nEVALUATION\nWe evaluate Emu on a broad range of vision-language tasks including image captioning (MS-\nCOCO (Chen et al., 2015)), image question answering (VQAv2 (Goyal et al., 2017), OKVQA (Marino\net al., 2019), VizWiz (Gurari et al., 2018)), visual dialog (VisDial (Das et al., 2017)), video question\nanswering (MSRVTTQA (Xu et al., 2017), MSVDQA (Xu et al., 2017), NextQA (Xiao et al., 2021))\nand text2image generation(MS-COCO (Lin et al., 2014)). Details are described in Appendix C.1. We\nevaluate our pretrained and instruction-tuned models in zero-shot and few-shot settings.\n5.1\nZERO-SHOT EVALUATION\nIn the zero-shot setting, the model is tested on tasks and datasets never encountered during training.\nTask-specific prompts are used to indicate different tasks to perform, without any additional tuning\nfor model parameters.\nMultimodal Understanding. Table 1 presents the zero-shot multimodal understanding performance\nof Emu and Emu-I (the instruction-tuned model). For zero-shot evaluation of Emu, we adopt the\nmultimodal Chain-of-Thought prompting technique following Huang et al. (2023), which first asks\nthe model to generate a caption for visual content before outputting the final result. Additionally,\nwe evaluate using the same strategy following Flamingo (Alayrac et al., 2022), where two text-only\nexamples from the task are used as prompts (results indicated by an *). For more detailed information\nregarding the evaluation, please refer to Appendix C.2.\nOn COCO captioning task, Emu achieves impressive zero-shot CIDEr score (Vedantam et al., 2015)\nof 112.4, which outperforms other LMMs by a large margin. In a wide range of image and video\nquestion answering tasks, Emu consistently surpasses LMMs like Kosmos-1 and Flamingo-9B.\nNotably, Emu achieves an accuracy of 34.4% on the complex VizWiz VQA dataset, versus Kosmos-\n1’s 29.2% and Flamingo-9B’s 28.8%. Emu-I is the instruction-tuned Emu model that achieves\nnotable improvements. Remarkably, even with only 14B parameters, Emu-I can outperform much\nlarger-scale Flamingo-80B model in several tasks such as VQAv2 (62.0% vs. 56.3%), VizWiz (38.3%\nvs. 31.6%), and MSVDQA (37.0% vs. 35.6%).\nText2image Generation.\nWe evaluate the zero-shot image generation ability on the validation set\nof MS-COCO (Lin et al., 2014). Following (Ramesh et al., 2021), we randomly sample 30k prompts\nfrom the validation set and calculate the zero-shot FID (Heusel et al., 2017). The results are shown in\nTable 2. For the generation of both Emu and SDv1.5, we use PNDM (Liu et al., 2022) scheduler with\n7\nPublished as a conference paper at ICLR 2024\nTable 3: Few-shot comparison. k is the number of in-context examples, and we used the same\nexample selection approach (i.e. RICES (Yang et al., 2022b) ) as Flamingo (Alayrac et al., 2022).\nModels\nVQAv2\nVizWiz\nMSVDQA\nMSRVTTQA\nk=2\nk=4\nk=8\nk=2\nk=4\nk=8\nk=2\nk=4\nk=8\nk=2\nk=4\nk=8\nKosmos-1\n51.4\n51.8\n51.4\n31.4\n35.3\n39.0\n-\n-\n-\n-\n-\n-\nFlamingo-9B\n-\n56.3\n58.0\n-\n34.9\n39.4\n-\n36.2\n40.8\n-\n18.2\n23.9\nPALI-X\n-\n56.9\n57.1\nEmu\n56.4\n58.4\n59.0\n37.8\n41.3\n43.9\n36.0\n37.1\n39.8\n21.2\n21.8\n24.1\nTable 4: Zero-shot evaluation regarding each core VL capability of MM-Vet (Yu et al., 2023b).\nModel\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nLLaMA-Adapter v2-7B (Gao et al., 2023)\n16.8\n7.8\n2.5\n3.0\n16.6\n4.4\n13.6±0.2\nMiniGPT-4-14B (Zhu et al., 2023a)\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4±0.4\nInstructBLIP-14B (Dai et al., 2023)\n30.8\n16.0\n9.8\n9.0\n21.1\n10.5\n25.6±0.3\nDreamLLM-7B (Dong et al., 2023)\n41.8\n26.4\n33.4\n33.0\n31.0\n11.5\n35.9±0.1\nLLaVA-65B (Lu et al., 2023)\n39.2\n28.2\n26.2\n28.3\n33.0\n15.0\n35.5±0.3\nEmu-I -14B\n45.5\n19.2\n36.7\n35.9\n25.2\n3.8\n36.3±0.3\n50 steps. We also adopt classifier-free guidance (Ho & Salimans, 2022) for better generation quality.\nThe scaling factor is set to 5.0 and 3.0 for Emu and SDv1.5 respectively, as these settings yield the\nbest performance for both models. Emu achieves better performance compared to a concurrent work\nGILL (Koh et al., 2023a), which also generates images with LLMs. However, our model is inferior to\nSDv1.5 in terms of FID. This is probably because the condition space (image embeddings) of our\nvisual decoder deviates a lot from the condition space (text embeddings) of the diffusion model used\nas initialization, and our model is trained for a relatively short 15k steps.\n5.2\nFEW-SHOT EVALUATION\nTable 2: Zero-shot text-to-image results on\nMS-COCO validation set. 30k samples are\nrandomly sampled for evaluation.\nModels\nFID(↓)\nunimodal generation models\nGLIDE (Nichol et al., 2021)\n12.24\nMake-A-Scene (Gafni et al., 2022)\n11.84\nDALL-E 2 (Ramesh et al., 2022)\n10.39\nSDv1.5 (Rombach et al., 2022)\n9.93\nImagen (Saharia et al., 2022)\n7.27\nParti (Yu et al., 2022b)\n7.23\nmultimodal generation models\nGILL (Koh et al., 2023a)\n12.20\nEmu (ours)\n11.66\nIn few-shot evaluation, the model is prompted with\ntask-specific prompts and several examples collected\nfrom the training data to evaluate its in-context learn-\ning ability. Evaluation details can be found in Ap-\npendix C.3. Table 3 presents the performance of the\npretraining model Emu in image and video question\nanswering tasks under the few-shot (k = 2, 4, 8)\nevaluation setting. We use the Retrieval In-Context\nExample Selection (Yang et al., 2022b) approach\nfollowing Flamingo (Alayrac et al., 2022). Emu\ndemonstrates superior performance to Flamingo-9B\nand Kosmos-1 under almost all scenarios. For ex-\nample, Emu achieves a VQAv2 accuracy of 58.4%\nand VizWiz 41.3% under the 4-shot setting, surpass-\ning Flamingo-9B by +2.1% and +6.4%, respectively.\nFor video-text tasks, Emu demonstrates strong performance as well, such as 4-shot 21.8% v.s.\nFlamingo’s 18.2% on the MSRVTTQA benchmark. Additionally, we can observe a positive corre-\nlation between the number of shots k (k = 0, 2, 4, 8) and the performance of Emu. These results\ndemonstrate Emu’s remarkable in-context learning ability.\n5.3\nIN-THE-WILD EVALUATION\nTable 4 presents zero-shot evaluation results on the in-the-wild benchmark MM-Vet (Yu et al.,\n2023b). We report the mean and std of 5 evaluation runs following Yu et al. (2023b). For each core\ncapability, the average score is reported. Emu-I exhibits state-of-the-art in-the-wild capability, and\neven outperforms LLaVA-65B (Lu et al., 2023) in Rec, Know, Gen abilities and the total score.\n5.4\nQUALITATIVE EVALUATION\nBeyond quantitative benchmarks, we conduct adequate qualitative evaluation of Emu. Emu demon-\nstrates impressive capabilities that cannot be evaluated on standard benchmarks, including real-world\n8\nPublished as a conference paper at ICLR 2024\nknowledge grounding (upper right of Figure 4), interleaved multi-image understanding (left side of\nFigure 4), detailed video understanding (lower right of Figure 4), multimodal assistant (Figure 5),\nmulti-turn dialogue (Figure 6), image blending (Figure 7), and (in-context) text-to-image genera-\ntion. For in-context text-to-image generation, Emu can generate context-related images (in the first\ntwo rows of Figure 8, the generated images share the oil painting style in context, compared with\nthe corresponding images generated without context in the first two rows of Figure 9), and follow\ncontext-related instructions, as shown in the 4th row of Figure 1. The multimodal in-context ability\nof Emu is responsible for this brand-new ability of image generation.\nWe also compare Emu with other state-of-the-art multimodal assistants in terms of the ability to\nperform typical image captioning tasks (Figure 10) and follow human instructions (Figure 11). In\nFigure 11, we test a slightly difficult instruction, and only Emu response properly to list 8 books\nwritten by Agatha Christie and then recommend one.\n6\nRELATED WORK\nMultimodal pretraining (Radford et al., 2021; Jia et al., 2021; Sun et al., 2023; Chen et al., 2020;\nKim et al., 2021; Wang et al., 2022d;a;b; Cho et al., 2021; Li et al., 2021; Yu et al., 2022a; Chen\net al., 2023c; Lu et al., 2022) learns cross-modal interactions from large-scale multimodal data.\nFlamingo (Alayrac et al., 2022) bridges powerful yet private pretrained vision and large language\nmodels and first demonstrates remarkable multimodal zero-shot and few-shot behaviors. With the\nincreasing impact (Schulman et al., 2022) and accessability (Touvron et al., 2023) of LLMs, recent\nwork has also considered building multimodal models based on LLMs (Li et al., 2023b; Driess et al.,\n2023; Huang et al., 2023; Dai et al., 2023; Ye et al., 2023; Zeng et al., 2023; Koh et al., 2023b), such\nas BLIP-series (Li et al., 2023b; Dai et al., 2023) that connect frozen vision and language pretrained\nmodels with a Q-Former to bridge the modality gap. These LMMs commonly use predicting the\nnext text token as the training objective and exert no supervision for vision data (Hao et al., 2022;\nHuang et al., 2023; Zhu et al., 2023a; Liu et al., 2023b; Ye et al., 2023). Instead, Emu unifies the\nmodeling of vision and language with the objective of predicting the next visual or text token in\nan autoregressive manner, and further explores videos as a new source of interleaved image-text\ndata. This unified modeling leads to a generalist interface for diverse multimodal tasks that output\neither image or text. Emerging recent studies (Zhu et al., 2023a; Liu et al., 2023b; Maaz et al.,\n2023; Li et al., 2023c; Liu et al., 2023a; Li et al., 2023a; Chen et al., 2023b;a) attempt to build\npowerful visual multimodal assistants based on LMMs through constructed conversation data. We\nalso instruction-tune Emu using publicly available datasets and build a multimodal assistant that\naligns well with human instructions on both images and videos.\n7\nLIMITATIONS AND FUTURE TOPICS\nEmu shares the well-acknowledged constraints inherent in other LLMs and LMMs, including suscep-\ntibility to both visual and language hallucinations, slow auto-regressive inference speed, a cessation of\nknowledge updates after pretraining, and a potential for generating non-factual content. Besides, Emu\npredominantly focused on English-language data. As a result, the model’s proficiency in languages\nother than English is currently delicate, and users should exercise caution when applying it in such\ncontexts. Addressing challenges related to hallucination, enhancing inference speed, and expanding\nmultilingual capabilities are crucial areas for future exploration and improvement.\n8\nCONCLUSION\nIn this work, we present Emu, a Large Multimodal Model trained with a unified autoregressive\nobjective of predicting the next element, including both visual and textual tokens. Apart from\ncommonly used image-text pairs and interleaved documents, we explore another scalable data source\nof image-text interleaved data, i.e., video. Emu trained under such unified objective and diverse\ndata can serve as a generalist interface that is capable of performing diverse multimodal tasks,\nsuch as image captioning, image\/video question answering, and text-to-image generation, together\nwith new abilities like in-context text and image generation, and image blending. We also build a\nmultimodal assistant instruction-tuned on Emu, which exhibits excellent human-aligned abilities\nsuch as multi-turn dialogue. We hope that our work will inspire the community to continue exploring\nthe potential of diverse multimodal data at web-scale and also the generative pretraining beyond\nvision and language.\n9\nPublished as a conference paper at ICLR 2024\nETHICS STATEMENTS\nEmu is currently in a preliminary stage and has been developed solely for research purposes. Its\nusage in specific applications is not recommended until comprehensive risk analyses have been\nconducted and corresponding mitigation strategies thoroughly explored. The ensuing discussion\noutlines potential risks and corresponding mitigation strategies of Emu, acknowledging the necessity\nfor further research efforts to comprehensively assess associated risks.\nPOTENTIAL RISKS\nThe ethical considerations associated with Emu primarily stem from two key aspects: 1) model\ninitialization: the Multimodal Modeling module of Emu is initialized from an open-sourced large\nlanguage model LLaMA (Touvron et al., 2023), the Visual Decoder module is initialized from\nStable Diffusion (Rombach et al., 2022), and the Vision Encoder is initialized from EVA-CLIP (Sun\net al., 2023). Consequently, Emu inherits the potential risks of generating harmful and biased\ninformation, including offensive language, propagation of social biases and stereotypes, and the\ngeneration of inappropriate content such as pornography and child abuse. 2) Pretraining data. The\npretraining data of Emu are publicly available and they are sourced from the Internet, where bias and\nharmful information are prevalent. Besides, the datasets sourced from the Internet (such as Common\nCrawl) may include links to images with personal information, potentially compromising privacy and\ncontaining sensitive content like faces, medical images, or other personal data.\nMITIGATION STRATEGIES\nIt is crucial to reiterate that Emu is designed exclusively for preliminary academic research and\nshould not be deployed in specific applications without rigorous risk analyses and mitigation strategy\nexploration. Deployment in production environments warrants a more thorough investigation into\nmodel behavior and potential biases.\nGiven the extensive size of pre-training datasets and the associated training costs, curating datasets\nand developing models for widespread use exceeds the scope of a single research paper. However,\nwe are open to discussing mitigation strategies to help address ethical concerns.\nShort-term approaches include: 1) relying on prompting to mitigate any biases and harmful outputs,\n2) implementing rule-based filtering, human oversight, and evaluation to identify and block harmful\ninformation, 3) employing a discriminator model capable of classifying harmful information for\nenhanced blocking, 4) Emu itself can be finetuned to become a multimodal discriminator.\nIn the long term, strategies involve: 1) social or public policy interventions, such as regulatory frame-\nworks and guidelines; 2) thoughtful product design, especially regarding user interface decisions; 3)\nadvancements in AI Ethics of powerful large models, including the development of better benchmarks\nand improved mitigation strategies.\nAdditionally, to address privacy concerns, methods exist for obfuscating or generating personal\nhuman attributes like faces (Yang et al., 2022a; Maximov et al., 2020), ensuring anonymity without\ncompromising the quality of learned representations. While this avenue is worth exploring, it is\ncurrently beyond the scope of this paper.\nIn conclusion, Emu is presently a model intended for preliminary research purposes only, and\ndeployment should be deferred until the aforementioned issues are thoroughly considered and\naddressed. Caution must be exercised before transitioning to production environments.\nREFERENCES\nLaion-aesthetics. https:\/\/laion.ai\/blog\/laion-aesthetics\/, a.\nLaion coco: 600m synthetic captions from laion2b-en. https:\/\/laion.ai\/blog\/laion-coco\/, b.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,\nAndy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\n10\nPublished as a conference paper at ICLR 2024\nAndrew Zisserman, and Karén Simonyan.\nFlamingo: a visual language model for few-shot learn-\ning. In NeurIPS, 2022. URL http:\/\/papers.nips.cc\/paper_files\/paper\/2022\/hash\/\n960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html.\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and\nLudwig Schmidt. Openflamingo, 2023. URL https:\/\/doi.org\/10.5281\/zenodo.7733589.\nMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder\nfor end-to-end retrieval. In Proceedings of the IEEE\/CVF International Conference on Computer Vision, pp.\n1728–1738, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\nDelong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan Wang. Visual instruction tuning with polite flamingo.\narXiv preprint arXiv:2307.01003, 2023a.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm’s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023b.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language\nmodel. arXiv preprint arXiv:2305.18565, 2023c.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325,\n2015.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing\nLiu. UNITER: universal image-text representation learning. In Andrea Vedaldi, Horst Bischof, Thomas\nBrox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 - 16th European Conference, Glasgow,\nUK, August 23-28, 2020, Proceedings, Part XXX, volume 12375 of Lecture Notes in Computer Science, pp.\n104–120. Springer, 2020. doi: 10.1007\/978-3-030-58577-8\\_7. URL https:\/\/doi.org\/10.1007\/\n978-3-030-58577-8_7.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In\nMarina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp.\n1931–1942. PMLR, 2021. URL http:\/\/proceedings.mlr.press\/v139\/cho21a.html.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models\nwith instruction tuning. CoRR, abs\/2305.06500, 2023. doi: 10.48550\/arXiv.2305.06500. URL https:\n\/\/doi.org\/10.48550\/arXiv.2305.06500.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv\nBatra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n326–335, 2017.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\nZhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint\narXiv:2309.11499, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL\nhttps:\/\/openreview.net\/forum?id=YicbFdNTTy.\n11\nPublished as a conference paper at ICLR 2024\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv\npreprint arXiv:2303.03378, 2023.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on Computer Vision, pp.\n89–106. Springer, 2022.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint\narXiv:2304.15010, 2023.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,\nPing Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv\npreprint arXiv:2305.04790, 2023.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 6904–6913, 2017.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3608–3617, 2018.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by\na two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing\nsystems, 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL\nhttps:\/\/openreview.net\/forum?id=nZeVKeeFYf9.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language\nmodels. arXiv preprint arXiv:2302.14045, 2023.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In\nMarina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp.\n4904–4916. PMLR, 2021. URL http:\/\/proceedings.mlr.press\/v139\/jia21b.html.\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or\nregion supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine\nLearning Research, pp. 5583–5594. PMLR, 2021. URL http:\/\/proceedings.mlr.press\/v139\/\nkim21k.html.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models.\narXiv preprint arXiv:2305.17216, 2023a.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal\ninputs and outputs. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nand Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 17283–17300.\nPMLR, 2023b. URL https:\/\/proceedings.mlr.press\/v202\/koh23a.html.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\n12\nPublished as a conference paper at ICLR 2024\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq R. Joty, Caiming Xiong, and Steven Chu-\nHong Hoi.\nAlign before fuse:\nVision and language representation learning with momentum dis-\ntillation.\nIn Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jen-\nnifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Con-\nference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, vir-\ntual, pp. 9694–9705, 2021.\nURL https:\/\/proceedings.neurips.cc\/paper\/2021\/hash\/\n505259756244493872b7709a8a01b536-Abstract.html.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine Learning, pp.\n12888–12900. PMLR, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023b.\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023c.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th\nEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740–755.\nSpringer, 2014.\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal\nmodel with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023b.\nLuping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds.\narXiv preprint arXiv:2202.09778, 2022.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\nunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\nYadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jianfeng Gao, and Yelong Shen. An empirical study of\nscaling instruct-tuned large multimodal models. arXiv preprint arXiv:2309.09958, 2023.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed\nvideo understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In Proceedings of the IEEE\/cvf conference on computer\nvision and pattern recognition, pp. 3195–3204, 2019.\nMaxim Maximov, Ismail Elezi, and Laura Leal-Taixé. Ciagan: Conditional identity anonymization generative\nadversarial networks. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition,\npp. 5447–5456, 2020.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu\nYeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and scalable any-modality\naugmented language model. arXiv preprint arXiv:2309.16058, 2023.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741, 2021.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),\nProceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learning Research, pp. 8748–8763. PMLR, 2021. URL\nhttp:\/\/proceedings.mlr.press\/v139\/radford21a.html.\n13\nPublished as a conference paper at ICLR 2024\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The\nJournal of Machine Learning Research, 21(1):5485–5551, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.\n8821–8831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 10684–10695, June 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–\n36494, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe,\nLiam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI\nblog, 2022.\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for\nclip at scale. arXiv preprint arXiv:2303.15389, 2023.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https:\/\/github.\ncom\/tatsu-lab\/stanford_alpaca, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566–4575,\n2015.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan\nWang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100,\n2022a.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence\nlearning framework. In International Conference on Machine Learning, pp. 23318–23340. PMLR, 2022b.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint\narXiv:2212.10560, 2022c.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\nlanguage model pretraining with weak supervision. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022d. URL https:\n\/\/openreview.net\/forum?id=GUrhfTuf_3.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\nand Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL\nhttps:\/\/openreview.net\/forum?id=gEZrGCozdqR.\n14\nPublished as a conference paper at ICLR 2024\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to\nexplaining temporal actions. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 9777–9786, June 2021.\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question\nanswering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM\ninternational conference on Multimedia, pp. 1645–1653, 2017.\nKaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in\nimagenet. In International Conference on Machine Learning, pp. 25313–25330. PMLR, 2022a.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on\nArtificial Intelligence, 2022b.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv\npreprint arXiv:2304.14178, 2023.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022a.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image\ngeneration. arXiv preprint arXiv:2206.10789, 2022b.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. arXiv preprint arXiv:2309.02591, 2023a.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Li-\njuan Wang.\nMm-vet: Evaluating large multimodal models for integrated capabilities.\narXiv preprint\narXiv:2308.02490, 2023b.\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati,\nJack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language\nand sound. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp.\n16375–16387, 2022.\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao\nKong. What matters in training a gpt4-style language model with multimodal inputs?\narXiv preprint\narXiv:2307.02469, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with\nmt-bench and chatbot arena, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023a.\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of\nimages interleaved with text. arXiv preprint arXiv:2304.06939, 2023b.\n15\nPublished as a conference paper at ICLR 2024\nA\nEMU TRAINING\nA.1\nPRETRAINING\nA.1.1\nDATASET DETAILS\nImage-text Pairs. The LAION-2B dataset is the english subset of Laion-5B (Schuhmann et al.,\n2022) and contains large-scale image-text pairs data. LAION-COCO (lai, b) is captioned 600M\nimages from LAION-2B with an ensemble of BLIP (Li et al., 2022) and CLIP (Radford et al., 2021)\nmodels. Whereas the text in LAION-COCO (lai, b) exhibits enhanced fluency and relevance to\nthe associated images, it has insufficient text diversity and a potential loss of high-level semantic\ninformation, including world knowledge contents presented in the original LAION-2B dataset. Thus,\nwe employ both the LAION-2B and LAION-COCO (lai, b) datasets during Emu pretraining.\nVideo-text Pairs. Webvid-10M (Bain et al., 2021) dataset contains a diversity of content with strong\ncorrelation between text and video. However, we found that a certain amount of the data contained\nirrelevant metadata information (e.g.resolution of the original video, camera parameters). To prevent\nthe model from being influenced by these irrelevant details, we use heuristic rules to remove these\ncontent. Firstly, we build a word list consisting of irrelevant information. This word list is then utilized\nas a filtering mechanism to process the raw video text descriptions obtained from the original dataset.\nAs a result, approximately 1 million datasets requiring cleaning are identified. Subsequently, specific\nrules are designed based on this list to identify and eliminate any words of irrelevant information\nwithin the text. Finally, the cleaned texts are subjected to rewriting using the Vicuna-13B (Zheng\net al., 2023), thereby ensuring its fluency and enhancing the overall quality.\nInterleaved Image and Text. Multimodal-C4 (Zhu et al., 2023b) is used as interleaved image-text\ndata in pretraining. Following OpenFlamingo (Awadalla et al., 2023), we filter images based on CLIP\nsimilarity score to ensure the relevance of the images and text in each document. Specifically, any\nimage with a CLIP similarity score below the threshold of 0.32 for all text in the same document is\ndiscarded. From each document, we sample a random subsequence of L = 1024 and take up to the first\nN = 5 images included in the sampled sequence. This process results in long text with the inclusion\nof multiple images. Additionally, we randomly sample N = 5 images along with their corresponding\nsentences to construct a subsequence of L = 512. This approach yields N = 5 image-text pairs.\nInterleaved Video and Text. Videos with interleaved subtitles text represent a valuable and scalable\nsource of multimodal data that has received limited attention thus far. In our study, we introduced\nYT-Storyboard-1B dataset, which collected storyboard images from YouTube, utilizing the video-ids\nprovided by the YT-Temporal-1B dataset, which encompasses a vast collection of 18 million videos,\nequating to a total of 1.8 billion storyboard images. Specifically, for each video, we crawl the\nstoryboard images and subtitles files directly. Where the sampling time between storyboard images is\nfixed, so the start time of each image can be determined by the order. Subtitle files record the content\nof each subtitle, as well as the start and end times. Therefore, storyboard images and subtitles can\nbe sorted according to their timestamps and adjacent subtitles can be merged to form an interleaved\nvideo-text sequence. By opting to collect storyboard images instead of raw video data, we eliminate\nthe necessity of video decoding. Moreover, this approach leads to a substantial 20-fold reduction in\ndata storage costs, resulting in increased download efficiency.\nA.1.2\nTRAINING DETAILS\nWe report the detailed training hyperparameter settings of Emu during the pretraining in Table 5.\nA.1.3\nABLATIONS ON INTERLEAVED VIDEO-TEXT DATA\nWe conduct ablation experiments to study the effectiveness of the YT-Storyboard-1B dataset. Smaller\nmodel size, batch size and shorter training schedule are used compared to the final Emu model\nto reduce the training cost. The model consists of a large-sized EVA-CLIP (Sun et al., 2023) as\nvisual encoder and LLaMA-7B (Touvron et al., 2023) as multimodal modeling module. Image-\ntext and video-text pairs are used as common datasets, including LAION-COCO (lai, b), LAION-\n2B (Schuhmann et al., 2022) and WebVid-10M (Bain et al., 2021), with batch size of 256, 256, 32,\nrespectively. YT-Storyboard-1B batch size of 32 is used. Models are trained for 7.2k gradient steps\non image-text and video-text pairs and additional 2.4k gradient steps on YT-Storyboard-1B.\n16\nPublished as a conference paper at ICLR 2024\nTable 5: Summary of pretraining hyperparameters of Emu.\nConfiguration\nEmu Pretraining\nVision encoder weight init.\nEVA-CLIP (Sun et al., 2023)\nLarge language model weight init.\nLLaMA-13B (Touvron et al., 2023)\nCausal transformer weight init.\nrandom init.\nVision encoder peak learning rate\n4e-5\nLarge language model peak learning rate\n3e-5\nCausal transformer peak learning rate\n1e-4\nWarmup ratio\n0.2\nLearning rate schedule\ncosine decay\nOptimizer\nAdamW (Loshchilov & Hutter, 2017)\nOptimizer hyper-parameters\nβ1, β2, ϵ = 0.9, 0.98, 1e-6\nWeight decay\n0.05\nInput image resolution\n224×224\nIterations\n10k\nData\nLAION-2B (Schuhmann et al., 2022), LAION-\nCOCO (lai, b), MMC4 (Zhu et al., 2023b),\nWebvid-10M (Bain et al., 2021), YT-Storyboard-\n1B (Zellers et al., 2022)\nBatch size per dataset\n128, 128, 64, 16, 16\nTable 6: Quantitative comparison of w\/wo interleaved video and text data during pre-training. k is the\nnumber of in-context examples, and we used the same example selection approach (i.e. RICES (Yang\net al., 2022b) ) as Flamingo (Alayrac et al., 2022). * indicates that the zero-shot prompt is built by\nusing two examples from the task, where their corresponding images have been removed.\nSetting\nCOCO\nOKVQA\nMSVDQA\nMSRVTTQA\nk=0\nk=0 *\nk=4\nk=0 *\nk=4\nk=0 *\nk=4\nEmu-7B w\/o YT-Storyboard-1B\n110.8\n43.9\n44.6\n30.2\n31.1\n16.9\n19.9\nEmu-7B w\/ YT-Storyboard-1B\n112.9\n42.3\n45.7\n30.8\n34.9\n17.9\n20.8\nWith YT-Storyboard-1B incorporated in the pretraining stage, Emu-7B achieves better zero-shot\nperformance on MS-COCO (Chen et al., 2015), MSVDQA (Xu et al., 2017) and MSRVTTQA (Xu\net al., 2017). Besides, YT-Storyboard-1B also brings stronger in-context learning capability under\n4-shot evaluation.\nA.2\nVISUAL DECODING\nA.2.1\nDATASET DETAILS\nLAION-Aesthetics (lai, a) is the subset of LAION-5B (Schuhmann et al., 2022) which have relatively\nhigh aesthetics quality while LAION-COCO (lai, b) has relatively high image-text correlation. To\nempower the visual decoder to possess the ability of decoding visual embeddings with both high\nquality and high relevance to text prompts, we use both LAION-COCO and LAION-Aesthetics for\nvisual decoding training. More specifically, we filter all text prompts with length greater than 150 to\npreserve a large enough batch size and prevent the GPU memory overflow. This rule discards about\n8% of LAION-Aesthetics and 0.01% of LAION-COCO data, which has little effect on data diversity.\nA.2.2\nTRAINING DETAILS\nThe detailed training setups are listed in Table 7.\n17\nPublished as a conference paper at ICLR 2024\nTable 7: Summary of Emu visual decoder training hyperparameters.\nConfiguration\nVisual Decoder\nWeight init\nStable Diffusion v1.5\nBatch size\n50 × 4 × 8\nIterations\n15k\nLearning rate\nwarm up to 1e-4 for the first 5k, decrease to\n5e-5 and 1e-5 at 10k and 14k\nInput image resolution\n512 × 512\nObjective\nϵ-prediction\nOptimizer\nAdamW (Loshchilov & Hutter, 2017)\nOptimizer hyper-parameters\nβ1, β2, ϵ = 0.9, 0.999, 1e-8\nWeight decay\n1e-2\nData\nLAION-COCO\n(lai,\nb),\nLAION-\nAesthetics (lai, a)\nData ratio\n7:2\nB\nINSTRUCTION TUNING\nB.1\nDATASET AND TRAINING DETAILS\nWe collect publicly available language, image and video instruction datasets for instruction tuning.\n• Language instructions: ShareGPT contains about 70K user dialogues with ChatGPT or\nGPT-4, and Alpaca (Taori et al., 2023) dataset contains 52K instruction-following data\ngenerated using self-instruct (Wang et al., 2022c) from OpenAI’s text-davinci-003.\n• Image instructions: we use LLaVA (Liu et al., 2023b) dataset consisting of three types\nof visual instructions, conversation, detailed description, and complex reasoning, with\na total number of 158K image-text instruction-following samples. In our preliminary\nexperiments, we found the instruction-tuned model often generates instruction-irrelevant\ndetailed descriptions of the image. Thus, we remove the detailed description subset of\nLLaVA. We also find a bad pattern ’on top of the back of’ in the model’s response, and\nwe filter all data that contains this pattern. The resulting 130K LLaVA subset is used for\ninstruction tuning.\n• Video instructions: we use VideoChat-11K (Li et al., 2023c) and a subset of Video-ChatGPT-\n100k (Maaz et al., 2023) as our video-instruction dataset. VideoChat-11K dataset is built\nfrom WebVid-10M consisting of 7K detailed video descriptions and 4K video conversations.\nVideo-ChatGPT-100k is built from ActivityNet, and we sample an around 30K subset that\nincludes only videos under one minute.\nWe use a batch size of 128 and train for 10K steps, with 3 epoches for ShareGPT, Alpaca and LLaVA\ndatasets, and 60K samples for video-instruction data. The learning rate linearly warms up to 1e-5 in\nthe first 500 steps, then decays to zero with a cosine schedule. The overall instruction tuning stage\ntakes around 16 hours with 16 A100-80G GPUs. We attach LoRAs (Hu et al., 2022) on all linear\nprojections of the self-attention layer, with the LoRA rank and alpha being 16.\nB.2\nSYSTEM MESSAGES\nWe use different system messages for language-instruction, image-instruction and video-instruction\ndatasets, as shown in Table 8.\n18\nPublished as a conference paper at ICLR 2024\nTable 8: Summary of the prompting template.\nTask Type\n<System Message>\nLanguage Instruction Datasets\nNone\nImage Instruction Datasets\nYou are a helpful assistant and you\nwill be presented with an image:\n[IMG]ImageContent[\/IMG]. You will be able\nto see the image after I provide it to\nyou.\nPlease answer my questions based on\nthe given image.\nVideo Instruction Datasets\nYou are a helpful assistant and you will\nbe presented with a video consisting\nof multiple chronological images:\n[IMG]ImageContent[\/IMG]. You will be able\nto see the video after I provide it to\nyou.\nPlease answer my questions based on\nthe given video.\nTable 9: Summary of the evaluation benchmarks.\nDataset\nTask\nSplit\nMetric\nImage\nCOCO Text2Image\nText-to-Image Generation\nVal\nFID(↓)\nCOCO Caption\nScene description\nTest\nCIDEr(↑)\nVQAv2\nScene understanding QA\nTest-dev\nVQA acc.(↑)\nOKVQA\nExternal knowledge QA\nVal\nVQA acc.(↑)\nVizWiz\nScene understanding QA\nTest-dev\nVQA acc.(↑)\nVisDial\nImage Dialogue\nVal\nNDCG(↑)\nVideo\nMSVDQA\nEvent understanding QA\nTest\nTop-1 acc.(↑)\nMSRVTTQA\nEvent understanding QA\nTest\nTop-1 acc.(↑)\nNextQA\nTemporal\/Causal QA\nTest\nWUPS(↑)\nC\nEVALUATION\nC.1\nBENCHMARKS\nEmu excels at performing diverse types of completion in multimodal sequences by accepting multi-\nmodal prompts, including text, images, videos, or their combinations, and generating comprehensive\nmultimodal responses. To evaluate the capabilities of Emu, we conduct extensive benchmark tests\ncovering various tasks, which are summarized in Table 9. Specifically, we meticulously select 9\nbenchmarks that encompass multimodal image\/video and language tasks, including text-to-image\ngeneration, visual question answering for both images and videos, and image-based visual dialogue.\nWhen benchmarking OKVQA, we use VQAv2 evaluation code3 and stem the answers using Porter\nstemming to consolidate answers following Marino et al. (2019). For other tasks, we either submit\nour results for evaluation on the official website or use standard evaluation code.\nC.2\nZERO-SHOT EVALUATION\nPrompt Template. To ensure that the model outputs answers in the required style for the benchmark\ntests, we prompt Emu and Emu-I with task-specific templates, as shown in Table 10. For each type\nof task, we have developed dedicated templates to structure the model’s output. In these templates,\n“{question}” will be replaced with the question from the question-answering task, “{history\n3https:\/\/github.com\/GT-Vision-Lab\/VQA\n19\nPublished as a conference paper at ICLR 2024\nquestion}” will be replaced with the historical question from the multi-turn visual dialogues, and\nsimilarly “history answer” will be replaced with the historical annotated answer from the multi-\nturn visual dialogues. Then, the image\/video will be added before the text as input to the model.\nAdditionally, we implement post-processing techniques to filter out commonly occurring redundant\nphrases such as “it is”, “it’s”, “a”, “an”, and “the”. Furthermore, the model is required to output\n“unanswerable” for questions that cannot be answered in the VizWiz dataset. To achieve this, we\naugment the template by adding the phrase “is the answer known?” and prompt the model to\nrespond with either “yes” or “no” by constraining the model generation. If the model responds with\n“no”, we immediately return the answer as “unanswerable”. On the other hand, if the model responds\nwith “yes”, we proceed to prompt the model to provide a valid answer.\nMultimodal Chain-of-Thought Prompting. To enhance the capabilities of the pretrained model,\nwe utilize the Multimodal Chain-of-Thought prompting technique. Initially, when presented with\nan image or video, we employ a prompt to guide the model in generating a descriptive caption.\nSubsequently, the model is given both the caption and a task-specific prompt to generate the final\nresult. The complete prompt template is shown in Table 10, where the “{caption}” tag in template\nwill be replaced with the descriptive text generated by Emu. The experimental results demonstrate\nthat this test-time technique effectively improves the model’s performance without any additional\ndata, leveraging the inherent capability of the model itself.\nText-only Examples Prompting. To ensure a fair comparison with Flamingo, we include results\nobtained through text-only examples prompting, denoted by an asterisk (*) in Table 1. We adopt the\nsame approach as Flamingo in selecting examples (i.e., RICES). This involves utilizing two text-only\nexamples from the task as prompts, without any accompanying images (similar to the few-shot text\nprompts). During the evaluation process, we observed that this approach effectively formats the\nmodel’s output, regardless of the label format of the datasets and the evaluation metrics employed,\nenabling a more accurate reflection of its true performance.\nC.3\nFEW-SHOT EVALUATION\nIn the few-shot evaluation settings, we incorporate a few example samples as prefixes in the template\nand connected the few-shot examples using “.\n”. Additionally, like Flamingo, we employ the\nRetrieval In-Context Example Selection (RICES) approach to select the few-shot examples.\nTo implement RICES, we begin by randomly selecting 5000 training set samples for each dataset.\nThen, using the pretrained EVA-CLIP model, we extract features from both the training set im-\nages\/videos and the test set images\/videos. For each test set sample, we select examples from the\ntraining set based on the highest cosine similarity using the extracted features, including them in the\nprompt. For the video-text task, we retrieve similar videos from the training set by comparing the\nmean of frame-level visual features extracted from our pretrained EVA-CLIP model.\nFurthermore, we discover that the support video examples didn’t require too many frames, which\ncould exceed the LLM’s context length limit. Therefore, we sample 8 frames for the given video and\nonly 2 frames for the corresponding support video examples.\nD\nCOMPARISON WITH RELATED WORK\nThe results are presented in Table 11, where Emu achieves state-of-the-art results on 5 out of 6\nbenchmarks evaluated.\nCM3Leon Yu et al. (2023a) has similar motivation with us to train a unified image-to-text and\ntext-to-image model. The most significant difference lies in that CM3Leon discretizes images, but\nwe directly input and output image continuous features. We can find that CM3Leon performs much\nworse than Emu on image-to-text tasks.\nAnyMAL Moon et al. (2023) is a large-scale Multimodal LLM that can process any modality, but\nhave relatively weak performance on image to text tasks.\n20\nPublished as a conference paper at ICLR 2024\nTable 10: Summary of the prompting template.\nModel\nType\nTemplate\nEmu\nImage Captioning\ndescribing the image in detail.\nthe image\nshows\nImage QA\na picture of {caption}.\nbased on the\npicture, {question} short answer:\nImage Dialog\na picture of {caption}.\nbased on the\npicture, {history question} short answer:\n{history answer}.\n· · · based on the\npicture, {question} short answer:\nVideo Event understanding QA\na video of {caption}.\na question about\nthe video:\n{question} answer:\nVideo Temporal\/Causal QA\na video of {caption}.\na question about\nthe video:\n{question} short answer:\nEmu-I\nImage Captioning\n[USER]: please provide an accurate and\nconcise description of the given image.\n[ASSISTANT]: the image depicts a photo of\nImage QA\n[USER]: based on the content of the\nimage and common sense, please provide\nan accurate answer consisting of only one\nword or phrase.\n[ASSISTANT]: the answer\nis:\nImage Dialog\n[USER]: {history question} [ASSISTANT]:\n{history answer}.<\\s> · · · [USER]:\n{question} [ASSISTANT]:\nVideo Event understanding QA\n[USER]: based on the content of the video\nand common sense, please provide an\naccurate answer consisting of only one\nword or phrase.\n{question} [ASSISTANT]:\nthe answer is:\nVideo Temporal\/Causal QA\n[USER]: based on the content of the\nvideo and common sense, please provide\nan accurate short answer.\n{question}\n[ASSISTANT]: the answer is:\nTable 11: Zero-shot comparison with concurrent work.\nModels\nCOCO (Image2Text)\nCOCO (Text2Image)\nVQAv2\nOKVQA\nVizWiz\nVisDial\nCM3Leon\n61.6\n10.82\n47.6\n23.8\n37.6\n22.6\nAnyMAL-13B\n99.5\n-\n59.6\n33.1\n24.4\n-\nEmu\n112.4\n11.66\n52.0\n38.2\n34.2\n47.4\nEmu *\n-\n-\n52.9\n42.8\n34.4\n47.8\nEmu-I\n120.4\n-\n57.2\n43.4\n32.2\n43.0\nEmu-I *\n-\n-\n62.0\n49.2\n38.3\n51.1\nE\nQUALITATIVE CASES\n21\nPublished as a conference paper at ICLR 2024\nThis is the first image.\nThe difference between the two images is that in the \nfirst image, there is an alpaca standing in front of a \nfence, while in the second image, there is a llama \nstanding in front of a fence. Alpacas and llamas are \nboth members of the camelid family, but they are \ndifferent species. Alpacas are native to South \nAmerica, while llamas are native to Central and \nSouth America.\nThis is the second image.\nWhat's the difference?\nThe video shows a step-by-step guide on how to \nmake a chocolate milkshake. First, the person in the \nvideo adds ice cubes to a blender. Then, they add two \nscoops of chocolate ice cream and one scoop of \nvanilla ice cream to the blender. After that, they add \nthree tablespoons of chocolate syrup and two \ntablespoons of cocoa powder to the blender. Finally, \nthey blend all the ingredients together until the \nmixture is smooth and creamy. The video ends with \nthe person pouring the chocolate milkshake into a \nglass and garnishing it with whipped cream and \nchocolate sprinkles. Overall, the video provides a \nsimple and easy recipe for making a delicious \nchocolate milkshake.\nStep by step describe the cooking recipe shown in \nthe video?\nThis is the first image.\nThis is the second image.\nThis is the third image.\nThis is the fourth image.\nDescribe all images.\nThe first image is the cover of the book \"The Way \nof the Nameless\". The second image is the cover of \nthe book \"Lost Stars\". The third image is the cover \nof the book \"A Land of My Dreams\". The fourth \nimage is the cover of the book \"The Key to \nMindfulness\".\nWhat's the name of this character?\nWho is she?\nTell me the characters in the image.\nThe character's name is Mewtwo.\nThe woman in the image is Saber, a character from \nthe anime series Fate\/Stay Night: Unlimited Blade \nWorks.\nThe characters in the image are SpongeBob \nSquarePants, Patrick Star, and Squidward Tentacles.\nFigure 4: Examples of interleaved multi-image understanding(left side), real-world knowledge\ngrounding(upper right), detailed video understanding(lower right).\n22\nPublished as a conference paper at ICLR 2024\nThe image shows a black and white photograph of Mahatma Gandhi spinning yarn in the late 1920s. Gandhi is \nsitting on the ground with a spinning wheel in front of him. He is wearing a white shawl and appears to be focused \non spinning the yarn. There are no other figures or objects visible in the image.\nMahatma Gandhi Spinning Yarn in the Late 1920s\nWhat does the image show? Describe all details including the figures, texts and numbers.\nThe image shows the Twelve Apostles on the Great Ocean Road in Victoria, Australia.\nThis meme features a cat with its mouth open and the caption \"This is your face when a teacher gives you a \nsurprise test.\" The meme is meant to be humorous and is often used to express frustration or annoyance with \nunexpected situations.\nThe red ball is heavier than the blue ball.\nExplain this meme to me.\nWhich ball is heavier?\nWhere is it in the image?\nFigure 5: Examples of multimodal assistant.\n23\nPublished as a conference paper at ICLR 2024\nWhat are the things I should be cautious about when I visit here?\n    There are a few things you should be cautious about when visiting the lake:\n    1. Weather: The weather at the lake can be unpredictable, with sudden changes in temperature, wind, and  \n        precipitation. Make sure to check the weather forecast before your visit and pack appropriate clothing and gear.\n    2. Wildlife: The lake is home to a variety of wildlife, including bears, wolves, and moose. Make sure to stay on \n        designated trails and keep your distance from any wildlife you encounter.\n    3. Water quality: The lake's water quality can vary depending on the time of year and the weather conditions. Make  \n        sure to check with local authorities before swimming or boating in the lake to ensure it is safe to do so.\n    4. Navigation: The lake is large and can be difficult to navigate, especially for inexperienced boaters. Make sure to \n        familiarize yourself with the lake's layout and any potential hazards before venturing out on the water.\n    5. Safety equipment: Make sure to bring any necessary safety equipment, such as life jackets, first aid kits, and \n        communication devices, in case of emergencies.\n    By taking these precautions, you can ensure a safe and enjoyable visit to the lake.\nDo you have any suggestions about safety equipment?\nCan you write a poem about the image?\n    Sure, here's a poem about the image:\n    The serene lake stretches out before me,\n    A vast expanse of crystal-clear water.\n    The sun shines down, casting a golden glow,\n    As I take in the breathtaking view.\n    The gentle waves lap against the shore,\n    Creating a soothing symphony of sound.\n    I feel a sense of peace and tranquility,\n    As I bask in the beauty of this place.\n    Yes, here are a few suggestions for safety equipment:\n    1. Life jackets: Make sure to bring a life jacket for each person in your group, especially if you plan on boating or \n        swimming in the lake.\n    2. First aid kit: A well-stocked first aid kit can be a lifesaver in case of emergencies. Make sure to include items such \n        as bandages, antiseptic wipes, and pain relievers.\n    3. Communication devices: Bring a reliable communication device, such as a cell phone or walkie-talkie, in case of  \n        emergencies.\n    4. Navigation equipment: If you plan on boating or kayaking in the lake, make sure to bring a map, compass, and \n        GPS device to help you navigate the waters.\n    By bringing these safety equipment, you can ensure a safe and enjoyable visit to the lake.\nFigure 6: Examples of multi-turn dialogue.\n24\nPublished as a conference paper at ICLR 2024\nInput Prompt\nCompletion\nFigure 7: Examples of image blending.\nInput Prompt\nCompletion\nThis is an image of a cute \npuppy. The puppy is \nsitting and resting on the \nlawn, surrounded by \nmany flowers and plants.\nThis is an image of a big \nhouse.\nA portrait of a old man. He \nhas a bushy beard and wears \na straw hat. His face is \ntowards the left of the frame, \nbut looking towards the \ncamera.\nThis is an oil painting \ndepicting sunflowers. \nThere are many sunflowers \nin a vase. And there are red \nand yellow sunflowers.\nThere is an cute animal \nsitting in front of a white \nbackground. The animal's \nbody has two colors and \nlooks very cute. Its body is \nhidden behind its head and \nit's looking at camera.\nAn image of the animal in \nthe first image is sitting \nand resting on the lawn, \nsurrounded by many \nflowers and plants.\nFigure 8: Examples of in-context text-to-image generation.\n25\nPublished as a conference paper at ICLR 2024\nCompletion\nThis is an image of a cute \npuppy. The puppy is \nsitting and resting on the \nlawn, surrounded by many \nflowers and plants.\nThis is an image of a big \nhouse\nA photo of stalk pink\nroses used for bedroom.\nA Mercedes C-Class \nspeeds down the highway.\nA large cabin on top \nof a sunny mountain in  \nthe style of Dreamworks\nAn image of a dog\nwearing a pair of glasses. \nInput Prompt\nFigure 9: Examples of text-to-image generation.\n26\nPublished as a conference paper at ICLR 2024\nEmu\nLLaVA\nmPLUG-Owl\nInput Image\nInput Text\nDescribe the image in detail.\nModel Output\nThe image is a painting by Claude Monet titled \"Impression, Sunrise\". It depicts a serene \nscene of the sun rising over the water. The painting is done in a loose, impressionistic \nstyle, with visible brushstrokes and a focus on capturing the essence of the scene rather \nthan the details. The colors are muted and soft, with shades of blue, pink, and yellow \ndominating the canvas.\nIn the foreground of the painting, there is a small boat bobbing up and down on the water. \nThe sun is just peeking over the horizon, casting a warm, golden light over the scene. \nThe water is calm and still, reflecting the colors of the sky above.\nIn the background of the painting, there are a few buildings visible, but they are mostly \nobscured by the mist and haze of the early morning. The sky is a mix of pink and blue, \nwith a few wispy clouds drifting lazily across the sky.\nOverall, the painting is a beautiful and serene depiction of a peaceful morning on the \nwater. The soft colors and loose brushstrokes evoke a sense of calm and tranquility, \ninviting the viewer to take a moment to appreciate the beauty of nature.\nThe image is a captivating scene of a small boat floating on a body of water, possibly a \nbay or a river, during a beautiful sunset. The boat is positioned towards the center of the \npicture, surrounded by a serene atmosphere.\nThere are several other boats scattered throughout the image, some closer to the edges of \nthe frame, while others are positioned more towards the center. The presence of multiple \nboats adds a sense of depth and interest to the scene.\nThe sunset casts a warm glow on the water and the boats, creating a picturesque and \ntranquil environment for those on the small boats to enjoy the view.\nThe painting depicts a beautiful sunrise over the ocean, creating a stunning and serene \natmosphere. The scene is set against a backdrop of a blue-gray sky, with a bright yellow \nsun peeking out from behind a cloud.\nThe water in the foreground of the painting is calm and peaceful, gently reflecting the \nsun's rays. Several sailboats can be seen floating in various positions, adding to the \ntranquil atmosphere of this early morning moment. In addition to boats, there are several \nbirds flying in different directions, enjoying the beautiful view.\nInstructBLIP\nIn this painting, Claude Monet's famous impressionist artwork titled \"Sunrise\" takes \ncenter stage. The painting depicts the beautiful sunrise over the horizon, with its warm \norange and yellow hues spreading across the sky. The water is also a significant part of \nthe painting, with several boats sailing along it. The sunrise is captured in the middle of \nthe painting, with boats scattered across the scene. In the background, the boats and the \nwater are surrounded by various colors, with some areas featuring a lighter, brighter tone. \nThe sunrise creates a serene and peaceful atmosphere, while the boats add a touch of \nactivity to the scene.\nFigure 10: Comparison of Emu with other methods on image captioning task.\n27\nPublished as a conference paper at ICLR 2024\nEmu\nLLaVA\nmPLUG-Owl\nInput Image\nInput Text\nTell me 8 books written by the woman and recommend one to me.\nModel Output\nHere are 8 books written by Agatha Christie:\n1. \"The Mysterious Affair at Styles\" (1920)\n2. \"The Secret Adversary\" (1922)\n3. \"The Murder on the Links\" (1923)\n4. \"The Man in the Brown Suit\" (1924)\n5. \"The Secret of Chimneys\" (1925)\n6. \"The Murder of Roger Ackroyd\" (1926)\n7. \"The Big Four\" (1927)\n8. \"The Murder at the Vicarage\" (1930)\nRecommendation: \"The Murder of Roger Ackroyd\" (1926)\n\"The Murder of Roger Ackroyd\" is considered by many to be Agatha Christie's greatest \nmystery novel. It is a classic whodunit with a twist ending that will keep you guessing \nuntil the very end.\nThe woman in the image, Agatha Christie, has written numerous books, totaling up to 66 \nnovels, including mysteries and thrillers. One of her most famous and highly \nrecommended books is \"And Then There Were None.\" This book is a classic whodunit \nnovel that tells the story of ten strangers who are lured to an isolated island and then \nmurdered one by one, according to the rules of the famous nursery rhyme. The intricate \nplot, unique setting, and engaging characters make it an excellent read for anyone who \nenjoys mysteries and suspense.\nAgatha Christie was a prolific British author who wrote over 80 novels, short stories, and \nplays. Some of her most famous works include:\n1. Murder on the Orient Express (1934): A classic mystery novel about the investigation \nof a murder on a luxury train.\n2. And Then There Were None (also known as Ten Little Indians) ( 10 Little Indian, 2015)\n3. The Murders on The Rue Morgue (first published in 90s) - A detective story about a \nseries of murders in Paris.\n4. A MurDER IS ANNOUNCED (published in the 40's). A play that was adapted for a \nmovie and later became a TV mini-series. It'll give you a glimpse into her writing style.\nInstructBLIP\nthe spiderwick chronicles\nFigure 11: Comparison of Emu with other methods in term of following human instructions.\n28\nPublished as a conference paper at ICLR 2024\nVideo-ChatGPT\nImageBind-LLM\nInput Video\nInput Text\nWhat's the woman doing in the video?\nModel Output\nThe woman in the video is wearing a pair of Apple VR goggles while sitting in an airplane. \nShe appears to be using the goggles to watch something, possibly a video or a 360-degree \nview of the airplane's surroundings.\nThe woman is sitting in a plane and looking out of the window.\nThe woman is looking at her cell phone, possibly using it to take a picture or view \nsomething on the screen.\nEmu\nFigure 12: Comparison of Emu with other methods in term of following human instructions.\n29\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Emu: Generative Pretraining in Multimodality.pdf"}
{"title":"Any-to-Any Generation via Composable Diffusion","authors":"Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal","summary":"We present Composable Diffusion (CoDi), a novel generative model capable of\ngenerating any combination of output modalities, such as language, image,\nvideo, or audio, from any combination of input modalities. Unlike existing\ngenerative AI systems, CoDi can generate multiple modalities in parallel and\nits input is not limited to a subset of modalities like text or image. Despite\nthe absence of training datasets for many combinations of modalities, we\npropose to align modalities in both the input and output space. This allows\nCoDi to freely condition on any input combination and generate any group of\nmodalities, even if they are not present in the training data. CoDi employs a\nnovel composable generation strategy which involves building a shared\nmultimodal space by bridging alignment in the diffusion process, enabling the\nsynchronized generation of intertwined modalities, such as temporally aligned\nvideo and audio. Highly customizable and flexible, CoDi achieves strong\njoint-modality generation quality, and outperforms or is on par with the\nunimodal state-of-the-art for single-modality synthesis. The project page with\ndemonstrations and code is at https:\/\/codi-gen.github.io","url":"http:\/\/arxiv.org\/abs\/2305.11846v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2305.11846v1","published":1684517912000,"comment":"Project Page: https:\/\/codi-gen.github.io","pdf_text":"Any-to-Any Generation via Composable Diffusion\nZineng Tang1∗\nZiyi Yang2†\nChenguang Zhu2\nMichael Zeng2\nMohit Bansal1†\n1University of North Carolina at Chapel Hill\n2Microsoft Azure Cognitive Services Research\nhttps:\/\/codi-gen.github.io\nAbstract\nWe present Composable Diffusion (CoDi), a novel generative model capable of\ngenerating any combination of output modalities, such as language, image, video,\nor audio, from any combination of input modalities. Unlike existing generative\nAI systems, CoDi can generate multiple modalities in parallel and its input is not\nlimited to a subset of modalities like text or image. Despite the absence of training\ndatasets for many combinations of modalities, we propose to align modalities in\nboth the input and output space. This allows CoDi to freely condition on any input\ncombination and generate any group of modalities, even if they are not present in\nthe training data. CoDi employs a novel composable generation strategy which\ninvolves building a shared multimodal space by bridging alignment in the diffusion\nprocess, enabling the synchronized generation of intertwined modalities, such\nas temporally aligned video and audio. Highly customizable and ﬂexible, CoDi\nachieves strong joint-modality generation quality, and outperforms or is on par\nwith the unimodal state-of-the-art for single-modality synthesis. The project page\nwith demonstrations and code is at https:\/\/codi-gen.github.io\/\n\"Raining, rain,\nmoderate\"\n(Raining ambience)\n\"Teddy bear on a\nskateboard, 4k\"\n(Raining street\nambience)\nCoDi\n\"A toy on the\nstreet sitting on\na board\"\n(Rain ambience, street noise, skateboard sound)\nFigure 1: CoDi can generate various (joint) combinations of output modalities from diverse (joint)\nsets of inputs: video, image, audio, and text (example combinations depicted by the colored arrows).\n1\nIntroduction\nRecent years have seen the rise of powerful cross-modal models that can generate one modality\nfrom another, e.g. text-to-text [6, 37], text-to-image [13, 19, 22, 41, 44], or text-to-audio [23, 33].\n∗Work done at Microsoft internship and UNC.\n†Corresponding authors: ziyiyang@microsoft.com, mbansal@cs.unc.edu\nPreprint. Under review.\narXiv:2305.11846v1  [cs.CV]  19 May 2023\nHowever, these models are restricted in their real-world applicability where multiple modalities\ncoexist and interact. While one can chain together modality-speciﬁc generative models in a multi-step\ngeneration setting, the generation power of each step remains inherently limited, and a serial, multi-\nstep process can be cumbersome and slow. Moreover, independently generated unimodal streams\nwill not be consistent and aligned when stitched together in a post-processing way (e.g., synchronized\nvideo and audio). The development of a comprehensive and versatile model that can generate any\ncombination of modalities from any set of input conditions has been eagerly anticipated, as it would\nmore accurately capture the multimodal nature of the world and human comprehension, seamlessly\nconsolidate information from a wide range of sources, and enable strong immersion in human-AI\ninteractions (for example, by generating coherent video, audio, and text description at the same time).\nIn pursuit of this goal, we propose Composable Diffusion, or CoDi, the ﬁrst model capable of\nsimultaneously processing and generating arbitrary combinations of modalities as shown in Fig. 1.\nTraining a model to take any mixture of input modalities and ﬂexibly generate any mixture of outputs\npresents signiﬁcant computational and data requirements, as the number of combinations for the\ninput and output modalities scales exponentially. Also aligned training data for many groups of\nmodalities is scarce or even non-existent, making it infeasible to train with all possible input-output\ncombinations. To address this challenge, we propose to align multiple modalities in both the input\nconditioning (Section 3.2) and generation diffusion step (Section 3.4). Furthermore, a proposed\n“Bridging Alignment” strategy for contrastive learning (Section 3.2) allows us efﬁciently model the\nexponential number of input-output combinations with a linear number of training objectives.\nBuilding a model with any-to-any generation capacity with exceptional generation quality requires\ncomprehensive model design and training on diverse data resources. Therefore, we build CoDi in an\nintegrative way. First, we train a latent diffusion model (LDM) for each modality, e.g., text, image,\nvideo, and audio. These models can be trained in parallel independently, ensuring exceptional single-\nmodality generation quality using widely available modality-speciﬁc training data (i.e., data with one\nor more modalities as input and one modality as output). For conditional cross-modality generation,\nsuch as generating images using audio+language prompts, the input modalities are projected into a\nshared feature space (Section 3.2), and the output LDM attends to the combination of input features.\nThis multimodal conditioning mechanism prepares the diffusion model to condition on any modality\nor combination of modalities without directly training for such settings.\nThe second stage of training enables the model to handle many-to-many generation strategies that\ninvolve simultaneously generating arbitrary combinations of output modalities. To the best of our\nknowledge, CoDi is the ﬁrst AI model with this capability. This is achieved by adding a cross-\nattention module to each diffuser, and an environment encoder V to project the latent variable of\ndifferent LDMs into a shared latent space (Section 3.4). Next, we freeze the parameters of the LDM,\ntraining only the cross-attention parameters and V . Since the environment encoder of different\nmodalities are aligned, an LDM can cross-attend with any group of co-generated modalities by\ninterpolating the representation’s output by V . This enables CoDi to seamlessly generate any group\nof modalities, without training on all possible generation combinations. This reduces the number of\ntraining objectives from exponential to linear.\nWe demonstrate the any-to-any generation capability of CoDi, including single-to-single modality\ngeneration, multi-condition generation, and the novel capacity of joint generation of multiple modali-\nties. For example, generating synchronized video and audio given the text input prompt; or generating\nvideo given a prompt image and audio. We also provide a quantitative evaluation of CoDi using eight\nmultimodal datasets. CoDi exhibits exceptional generation quality across assorted scenarios, with\nsynthesis quality on par or even better than single to single modality SOTA, e.g., audio generation\nand audio captioning.\n2\nRelated Works\nDiffusion models (DMs) learn the data distribution by denoising and recovering the original data.\nDeep Diffusion Process (DDP) [45] adopts a sequence of reversible diffusion steps to model image\nprobability distribution. It uses a reversible encoder to map the input image to a latent space and\na decoder to map the latent variables to an output image. Denoising diffusion probabilistic model\n(DDPM) [20] uses a cascade of diffusion processes to gradually increase the complexity of the\nprobability density function model. At each step, the model adds noise to the input image and\n2\nText\nEncoder\nVision\nEncoder\nAudio\nEncoder\n(2) Diffusion Step(s)\nText UNet\nVision UNet\nAudio UNet\n(1) Conditioning\nL2\nLoss\nL2\nLoss\nVision\nEnvironment\nEncoder\nAudio\nEnvironment\nEncoder\nVision UNet\nAudio UNet\nText\nEnvironment\nEncoder\nText UNet\nBridging\nAlignment\nVision UNet\nAudio UNet\nDecode\nDecode\nInference\nStage 2\nJoint\nGeneration\nVision\nEncoder\nText\nEncoder\nn diffusion steps\nA panda enjoying\nher breakfast at\nthe table \n(b)\n(c)\n(3) Latent Alignment at Step t\nStage 1\nComposable\nConditioning\n(a)\nText\nEncoder\nText\nEncoder\nVision\nEncoder\nAudio\nEncoder\nText UNet\nVision UNet\nAudio UNet\nL2\nLoss\n(1) Conditioning Alignment\n(2) Single Diffuser Training\nL2\nLoss\nBridging Alignment\nVideo\nEncoder\nAudio\nEncoder\nImage\nEncoder\nFigure 2: CoDi model architecture: (a) We ﬁrst train individual diffusion model with aligned prompt\nencoder by “Bridging Alignment”; (b) Diffusion models learn to attend with each other via “Latent\nAlignment”; (c) CoDi achieves any-to-any generation with a linear number of training objectives.\nestimates the corresponding noise level using an autoregressive model. This allows the model to\ncapture the dependencies between adjacent pixels and generate high-quality images. Score-based\ngenerative models (SOG) [46] use the score function to model the diffusion process. [40] generates\nhigh-ﬁdelity images conditioned on CLIP representations of text prompts. Latent diffusion model\n(LDM) [41] uses a VAE to encode inputs into latent space to reduce modeling dimension and\nimproves efﬁciency. The motivation is that image compression can be separated into semantic space\nby a diffusion model and perceptual space by an autoencoder. By incorporating temporal modeling\nmodules and cascading model architectures, video diffusion models have been built upon image\ndiffusers to generate temporally consistent and inherent frames[14, 19, 21, 44]. Diffusion models have\nalso been applied to other domains, such as generating audio from text and vision prompts[23, 33].\nMultimodal modeling has experienced rapid advancement recently, with researchers striving to build\nuniform representations of multiple modalities using a single model to achieve more comprehensive\ncross-modal understanding. Vision transformers [11], featuring diverse model architectures and\ntraining techniques, have been applied to various downstream tasks such as vision Q&A and image\ncaptioning. Multimodal encoders have also proven successful in vision-language [1, 8, 57], video-\naudio [47] and video-speech-language [55, 56] domains. Aligning data from different modalities is\nan active research area [12, 38], with promising applications in cross-modality retrieval and building\nuniform multimodal representations [33, 35, 41].\n3\nMethodology\n3.1\nPreliminary: Latent Diffusion Model\nDiffusion models (DM) represent a class of generative models that learn data distributions p(x) by\nsimulating the diffusion of information over time. During training, random noise is iteratively added\nto x, while the model learns to denoise the examples. For inference, the model denoises data points\nsampled from simple distributions such as Gaussian. Latent diffusion models (LDM) [41] learn the\ndistribution of the latent variable z corresponding to x, signiﬁcantly reducing computational cost by\ndecreasing the data dimension.\n3\nIn LDM, an autoencoder is ﬁrst trained to reconstruct x, i.e., ˆx = D(E(x)), where E and D denote\nthe encoder and decoder, respectively. The latent variable z = E(x) is iteratively diffused over\ntime steps t based on a variance schedule β1, . . . , βT , i.e., q(zt|zt−1) = N(zt; √1 −βtzt−1, βtI)\n[20, 45].\nThe forward process allows the random sampling of zt at any timestep in a closed form [20, 45]:\nzt = αtz + σtϵ, where ϵ ∼N(0, I), αt := 1 −βt and σt := 1 −Qt\ns=1 αs. The diffuser learns how\nto denoise from {zt} to recover z. Following the reparameterization method proposed in [20], the\ndenoising training objective can be expressed as [41]:\nLD = Ez,ϵ,t∥ϵ −ϵθ(zt, t, C(y))∥2\n2.\n(1)\nIn data generation, the denoising process can be realized through reparameterized Gaussian sampling:\np(zt−1|zt) = N\n\u0012\nzt−1;\n1\n√αt\n\u0012\nzt −βt\n√σt\nϵθ\n\u0013\n, βtI\n\u0013\n.\n(2)\nIn LD, the diffusion time step t ∼U[1, T]; ϵθ is a denoising model with UNet backbone parame-\nterized by θ; y represents the conditional variable that can be used to control generation; C is the\nprompt encoder. The conditioning mechanism is implemented by ﬁrst featurizing y into C(y), then\nthe UNet ϵθ conditions on C(y) via cross-attention, as described in [41]. Distinct from previous\nworks, our model can condition on any combinations of modalities of text, image, video and audio.\nDetails are presented in the following section.\n3.2\nComposable Multimodal Conditioning\nTo enable our model to condition on any combination of input\/prompt modalities, we align the prompt\nencoder of text, image, video and audio (denoted by Ct, Ci, Cv, and Ca, respectively) to project the\ninput from any modality into the same space. Multimodal conditioning can then be conveniently\nachieved by interpolating the representations of each modality m: C(xt, xi, xv, xa) = P\nm αmC(m)\nfor m ∈xt, xi, xv, xa, with P\nm αm = 1. Through simple weighted interpolation of aligned\nembeddings, we enable models trained with single-conditioning (i.e., with only one input) to perform\nzero-shot multi-conditioning (i.e., with multiple inputs). This process is illustrated in Fig. 2 (a)(2).\nOptimizing all four prompt encoders simultaneously in a combinatorial manner is computationally\nheavy, with O(n2) pairs. Additionally, for certain dual modalities, well-aligned paired datasets are\nlimited or unavailable e.g., image-audio pairs. To address this challenge, we propose a simple and\neffective technique called \"Bridging Alignment\" to efﬁciently align conditional encoders. As shown\nin Fig. 2 (a)(1), we choose the text modality as the \"bridging\" modality due to its ubiquitous presence\nin paired data, such as text-image, text-video, and text-audio pairs. We begin with a pretrained\ntext-image paired encoder, i.e., CLIP [38]. We then train audio and video prompt encoders on\naudio-text and video-text paired datasets using contrastive learning, with text and image encoder\nweights frozen.\nIn this way, all four modalities are aligned in the feature space. As shown in Section 5.2, CoDi\ncan effectively leverage and combine the complementary information present in any combination\nof modalities to generate more accurate and comprehensive outputs. The high generation quality\nremains unaffected with respect to the number of prompt modalities. As we will discuss in subsequent\nsections, we continue to apply Bridging Alignment to align the latent space of LDMs with different\nmodalities to achieve joint multimodal generation.\n3.3\nComposable Diffusion\nTraining an end-to-end anything-to-anything model requires extensive learning on various data\nresources. The model also needs to maintain generation quality for all synthesis ﬂows. To address\nthese challenges, CoDi is designed to be composable and integrative, allowing individual modality-\nspeciﬁc models to be built independently and then smoothly integrated later. Speciﬁcally, we start by\nindependently training image, video, audio, and text LDMs. These diffusion models then efﬁciently\nlearn to attend across modalities for joint multimodal generation (Section 3.4) by a novel mechanism\nnamed “latent alignment”.\n4\nImage Diffusion Model.\nThe image LDM follows the same structure as Stable Diffusion 1.5 [41]\nand is initialized with the same weights. Reusing the weights transfers the knowledge and exceptional\ngeneration ﬁdelity of Stable Diffusion trained on large-scale high-quality image datasets to CoDi.\nVideo Diffusion Model.\nTo model the temporal properties of videos and simultaneously maintain\nvision generation quality, we construct the video diffuser by extending the image diffuser with\ntemporal modules. Speciﬁcally, we insert pseudo-temporal attention before the residual block [13].\nHowever, we argue that pseudo-temporal attention only enables video frames to globally attend\nto each other by ﬂattening the pixels (height, width dimension) to batch dimension, resulting in\na lack of cross-frame interaction between local pixels. We argue that this results in the common\ntemporal-inconsistency issue in video generation that locations, shapes, colors, etc. of objects can\nbe inconsistent across generated frames. To address this problem, we propose adapting the latent\nshift method [2] that performs temporal-spatial shifts on latent features in accordance with temporal\nattention. We divide the video by the hidden dimension into k = 8 chunks, and for each chunk\ni = 0 to 7, we shift the temporal dimension forward by i positions. Further details will be provided\nin the appendix.\nAudio Diffusion Model.\nTo enable ﬂexible cross-modality attention in joint generation, the audio\ndiffuser is designed to have a similar architecture to vision diffusers, where the mel-spectrogram\ncan be naturally viewed as an image with 1 channel. We use a VAE encoder to encode the mel-\nspectrogram of audio to a compressed latent space. In audio synthesis, a VAE decoder maps the latent\nvariable to the mel-spectrogram, and a vocoder generates the audio sample from the mel-spectrogram.\nWe employ the audio VAE from [33] and the vocoder from [27].\nText Diffusion Model.\nThe VAE of the text LDM is OPTIMUS [29], and its encoder and decoder\nare [9] and GPT-2 [39], respectively. For the denoising UNet, unlike the one in image diffusion, the\n2D convolution in residual blocks is replaced with 1D convolution [53].\n3.4\nJoint Multimodal Generation by Latent Alignment\nThe ﬁnal step is to enable cross-attention between diffusion ﬂows in joint generation, i.e., generating\ntwo or more modalities simultaneously. This is achieved by adding cross-modal attention sublayers to\nthe UNet ϵθ (Fig. 2 (b)(2)). Speciﬁcally, consider a diffusion model of modality A that cross-attends\nwith another modality B. Let the latent variables of modalities mA and mB at diffusion step t be\ndenoted as zA\nt and zB\nt , respectively. The proposed “Latent Alignment” technique is such that a\nmodality-speciﬁc environment encoder VB ﬁrst projects zB\nt into a shared latent space for different\nmodalities. Then, in each layer of the UNet for modality A, a cross-attention sublayer attends to\nVB(zB\nt ). For the diffusion model of modality A, the training objective in Eq. (1) now becomes:\nLA\nCross = Ez,ϵ,t∥ϵ −ϵθc(zA\nt , VB(zB\nt ), t, C(y))∥2\n2,\n(3)\nwhere θc denotes the weights of cross-attention modules in the UNet.\nThe training objective of A + B joint generation is LA\nCross + LB\nCross. V (·) of different modalities are\ntrained to be aligned with contrastive learning. Since zA\nt and zB\nt at any time step can be sampled with\nclosed form in the diffusion process Section 3.1, one can conveniently train the contrastive learning\ntogether with LCross. The purpose of V is to achieve the generation of any combination of modalities\n(in polynomial) by training on a linear number of joint-generation tasks. For example, if we have\ntrained the joint generation of modalities A, B, and B, C independently, then we have VA(zA\nt ),\nVB(zB\nt ), and VC(zC\nt ) aligned. Therefore, CoDi can seamlessly achieve joint generation of modalities\nA and C without any additional training. Moreover, such design automatically effortlessly enables\njoint generation of modalities A, B, and C concurrently. Speciﬁcally, UNet of A can cross-attend\nwith the interpolation of VB(zB\nt ), and VC(zC\nt ), although CoDi has not been trained with such task.\nAs shown in Fig. 2(b)(3), we follow similar designs to the \"Bridging Alignment\" in training joint\ngeneration: (1) We ﬁrst train the cross-attention weights in the image and text diffusers, as well\nas their environment encoders V , on text-image paired data. (2) We freeze the weights of the text\ndiffuser and train the environment encoder and cross-attention weights of the audio diffuser on\ntext-audio paired data. (3) Finally we freeze the audio diffuser and its environment encoder, and\ntrain the joint generation of the video modality on audio-video paired data. As demonstrated in\n5\nTable 1: Training tasks (CT stands for “contrastive learning” to align prompt encoders) and datasets\nwith corresponding statistics. * denotes the number of accessible examples in the original datasets.\nCategories\nTasks\nDatasets\n# of samples\nDomain\nImage + Text\nImage→Text, Text→Image\nLaion400M [42]\n400M\nOpen\nText→Image+Text\nAudio + Text\nText→Audio, Audio→Text,\nText→Audio+Text, Audio-Text CT\nAudioSet [16]\n900K*\nYouTube\nAudioCaps [24]\n46K\nYouTube\nFreesound 500K\n2.5M\nPublic audio samples\nBBC Sound Effect\n30K\nAuthentic natural sound\nAudiovisual\nImage→Audio, Image→Video+Audio\nAudioSet\n900K*\nYouTube\nSoundNet [3]\n1.0M*\nFlickr, natural sound\nVideo\nText→Video, Image→Video,\nVideo-Text CT\nWebvid10M [4]\n10.7M\nShort videos\nHD-Villa-100M [54]\n100M\nYouTube\n“Concept art by Sylvain \nSarrailh of a haunted \nJapan temple in a forest”\n“mountain view, \nsunset.”\n(Subway ambient sound) \n“A beautiful ballet \ndancer spinning, \nview from top.”\nFigure 3: Single-to-single modality generation. Clockwise from top left: text→image, image→text,\nimage→video, audio→image.\nSection 5.3, although only trained on three paired joint generation tasks (i.e, Text+Audio, Text+Image,\nand Video+Audio), CoDi is capable of generating assorted combinations of modalities simultaneously\nthat are unseen in training, e.g., joint image-text-audio generation in Fig. 5.\n4\nExperiments\n4.1\nTraining Objectives and Datasets\nWe list training tasks of CoDi in Table 1, including single modality synthesis, joint multimodal\ngeneration, and contrastive learning to align prompt encoders. Table 1 provides an overview of the\ndatasets, tasks, number of samples, and domain. Datasets are from the following domains: image\n+ text (e.g. image with caption), audio + text (e.g. audio with description), audio + video (e.g.\nvideo with sound), and video + text (e.g. video with description). As one may have noticed, the\nlanguage modality appears in most datasets and domains. This echos the idea of using text as the\nbridge modality to be able to extrapolate and generate new unseen combinations such as audio\nand image bridged by text, as mentioned in Section 3.2 and Section 3.4. Due to space limit, more\ndetails on training datasets and can be found in Appendix C, model architecture details in Appendix\nAppendix A.1, and training details in Appendix B.\nImage + Text. We use a recently developed large-scale image caption dataset, Laion400M [42].\nThis image-text paired data allows us to train with tasks text→image, image→text, and the joint\ngeneration of image and text. For the joint generation task, we propose to train with text→image+text,\nwhere the prompt text is the truncated image caption, and the output text is the original caption. Since\nthe condition information is incomplete, the text and image diffuser will need to learn to attend with\neach other through the joint generation process.\n6\nTable 2: FID scores comparing different text\nto image models on the validation set of\nCOCO-caption [32].\nMethod\nFID ↓\nCogView [10]\n27.10\nGLIDE [36]\n12.24\nMake-a-Scene [15]\n11.84\nLDM [33]\n12.63\nStable Diffusion-1.4\n11.21\nVersatile Diffusion [53]\n11.10\nCoDi (Ours)\n11.26\nTable 3: MSR-VTT text-to-video generation perfor-\nmance.\nMethod\nZero-Shot\nCLIPSIM ↑\nGODIVA [50]\nNo\n0.2402\nNÜWA [51]\nNo\n0.2439\nCogVideo [22]\nYes\n0.2631\nMake-A-Video [44]\nYes\n0.3049\nVideo LDM [5]\nYes\n0.2929\nCoDi (Ours)\nYes\n0.2890\nTable 4: The comparison between our audio diffuser and baseline TTA generation models. Evaluation\nis conducted on AudioCaps test set. AS, AC, FSD, BBC, and SDN stand for AudioSet, AudioCaps,\nFreesound, BBC Sound Effect, and Soundnet.\nModel\nDatasets\nFD ↓\nIS ↑\nKL ↓\nFAD ↓\nOVL ↑\nREL ↑\nGround truth\n-\n-\n-\n-\n-\n83.61\n80.11\nDiffSound\nAS + AC\n47.68\n4.01\n2.52\n7.75\n45.00\n43.83\nAudioGen\nAS + AC + 8 others\n-\n-\n2.09\n3.13\n-\n-\nAudioLDM-L-Full\nAS + AC + FSD + BBC\n23.31\n8.13\n1.59\n1.96\n65.91\n65.97\nCoDi (Ours)\nAS + AC + FSD + BBC + SDN\n22.90\n8.77\n1.40\n1.80\n66.87\n67.60\nTable 5: COCO image caption-\ning scores comparison.\nModel\nB@4\nMETEOR\nCIDEr\nAutoregressive Model\nOscar [31]\n36.58\n30.4\n124.12\nClipCap [35]\n32.15\n27.1\n108.35\nOFA [49]\n44.9\n32.5\n154.9\nBLIP2 [30]\n43.7\n-\n145.8\nDiffusion Model\nDDCap [59]\n35.0\n28.2\n117.8\nSCD-Net [34]\n39.4\n29.2\n131.6\nCoDi (Ours)\n40.2\n31.0\n149.9\nTable 6: AudioCaps audio cap-\ntioning scores comparison.\nModel\nSPIDEr\nCIDEr\nSPICE\nAudioCaps [24]\n0.369\n0.593\n0.144\nBART-Finetune [17]\n0.465\n0.753\n0.176\nVALOR [7]\n-\n0.741\n-\nAL-MixGen [25]\n0.466\n0.755\n0.177\nCoDi (Ours)\n0.480\n0.789\n0.182\nTable 7: MSRVTT video cap-\ntioning scores comparison.\nModel\nB@4\nMETEOR\nCIDEr\nORG-TRL [58]\n43.6\n28.8\n50.9\nMV-GPT [43]\n48.9\n38.7\n60.0\nGIT [48]\n54.8\n33.1\n75.9\nmPLUG-2 [52]\n57.8\n34.9\n80.3\nCoDi (Ours)\n52.1\n32.5\n74.4\nAudio + Text. We curated a new dataset, Freesound 500K, by crawling 500K audio samples together\nwith tags and descriptions from the Freesound website. We also use AudioSet [42] with 2 million\nhuman-labeled 10-second sound clips from YouTube videos and AudioCaps [24] with 46K audio-\ntext pairs derived from the AudioSet dataset. Audio samples are clipped into 10-second segments\nfor training purposes. The paired audio + text data enables us to train text→audio, audio→text,\ntext→audio + text generation, and audio-text contrastive learning. Similar to image + text joint\ngeneration, in text→audio + text, text prompt is the truncated text, and the output is the original text.\nVideo. We use the following diverse and high-quality video datasets to train video generation and\nvideo prompt encoder. WebVid [4], a large-scale dataset of web videos together with descriptions;\nHD-Villa-100M [54] with high resolution YouTube videos of at least 720P. We perform text→video\nand video-text contrastive learning task with WebVid. We use HD-Villa-100M for image→video\ngeneration where the middle frame is the input image.\nAudiovisual. Web videos are a natural aligned audio-video data resource. However, many existing\ndatasets, e.g., ACAV100M [28], feature heavily on videos of human speech rather than natural sounds.\nTherefore, we leverage sound-oriented datasets AudioSet and SoundNet [3] for joint audio-video\ngeneration. For image→audio + video, we use the middle frame of the target video as the input\nprompt image. We also use the middle frame as the prompt input to train the model to generate the\naudio, i.e., image→audio.\n5\nEvaluation Results\nIn this section, we will evaluate the model generation quality in different settings including single\nmodality generation, multi-condition generation, and multi-output joint generation. We provide both\nquantitative benchmarking on evaluation datasets as well as qualitative visualization demonstrations.\n7\n(Pirate ship sailing ambience)\n“Oil painting, cosmic horror painting,\nelegant intricate ArtStation concept \nart by Craig Mullins detailed”\n“Panda eating bamboo, \npeople laughing.”\n(A video with sound with people watching panda and laughing)\n(Video of waterfall with camera moving forward)\n“Forward moving \ncamera view”\n(Sound of \nflowing water)\nFigure 4: Generation with multiple input modality conditions. Top to bottom: text+audio→image,\ntext+audio→video, video+audio→text.\n5.1\nSingle Modality Generation Results\nWe ﬁrst show example demo in Fig. 3, where we present various single to single modality generation.\nThen, we evaluate the synthesis quality of the unimodal generation on text, image, video, and\naudio. CoDi achieves SOTA on audio captions and audio generation, as shown in Table 6 and\nTable 4. Notably for the ﬁrst time in the ﬁeld, CoDi, a diffusion-base model, exhibits comparable\nperformance on image captioning with autoregressive transformer-based SOTA (Table 5). CoDi is\nthe ﬁrst diffusion-model based for video captioning Table 7. On image and video generation, CoDi\nperforms competitively with state-of-the-art (Tables 2 and 3). This gives us strong starting points for\nmulti-condition and multi-output generation that will be presented next in Section 5.2 and Section 5.3.\nWe demonstrate in Section 3.2 that CoDi is capable of integrating representation from different\nmodalities in the generation. Thus, we ﬁrst show multi-condition generation demo as shown in Fig. 4.\n5.2\nMulti-Condition Generation Results\nTable 8: CoDi is capable of generating high quality\noutput (image in this case) from various combina-\ntions of prompt modalities.\nInputs\nFID ↓\nSingle-modality Prompt\nText\n14.2\nAudio\n14.3\nDual-modality Prompt\nText + Audio\n14.9\nTable 9: MSR-VTT text-to-video generation per-\nformance.\nInputs\nCLIPSIM ↑\nSingle-modality Prompt\nText\n0.2890\nDual-modality Prompt\nText+Audio\n0.2912\nText+Image\n0.2891\nText+Audio+Image\n0.2923\n8\n“Sea shore sound \nambience.\"\n“Fireworks in the sky.”\n(Sound of firework \nsynchronized with \nvideo) \nGenerated Video\n(Sample frames)\n“Wave crashes the \nshore, seagulls”\n(Sound of seagulls \nand wave crashing \nthe shore) \n“Teddy bear on a\nskateboard, 4k.”\n(Raining ambience) \n(Skateboarding sound, street noise, raining ambience) \nFigure 5:\nJoint generation of multiple output modalities by CoDi.\nFrom top to bottom:\ntext→video+audio, text→image+text+audio, text+audio+image→video+audio.\nFor quantitative evaluation, we focus on multiple inputs to image synthesis output since the evaluation\nmetric for this case (FID) does not require speciﬁc modality inputs like text. We test with several\ninput combinations including text + image, text + audio, image + audio, text + video, as well as three\ninputs text + audio + image. We test on the validation set of AudioCaps [24] since all four modalities\nare present in this dataset. The prompt image input is the middle frame of the video. As shown in\nTable 8, CoDi achieves high image generation quality given assorted groups of input modalities. We\nalso test with several input combinations with video as output including text, text + audio, image +\nimage, as well as text + audio + image. We also test on MSRVTT [24] since all four modalities are\npresent in this dataset. Similarly, the prompt image input is the middle frame of the video. As shown\nin Table 9, CoDi achieves high video and ground truth text similarity given assorted groups of input\nmodalities. Again our model does not need to train on multi-condition generation like text + audio or\ntext + image. Through bridging alignment and composable multimodal conditioning as proposed in\nSection 3.2, our model trained on single condition can zero-shot infer on multiple conditions.\n5.3\nMulti-Output Joint Generation Results\nFor joint multimodal generation, we ﬁrst demonstrate high-quality multimodal output joint generation\ndemo as shown in Fig. 5. For quantitative evaluation, there is no existing evaluation metric since we\nare the ﬁrst model that can simultaneously generate across all 4 modalities. Therefore, we propose\nthe following metric SIM that quantiﬁes the coherence and consistency between the two generated\nmodalities by cosine similarity of embeddings:\nSIM(A, B) = cos (CA(A), CB(B))\n(4)\nwhere A, B are the generated modalities, and CA and CB are aligned encoders that project A and\nB to the same space. We use the prompt encoder as described in Section 3.2. This metric aims to\ncompute the cosine similarity of the embedding of two modalities using contrastive learned prompt\nencoders. Thus, the higher the metric, the more aligned and similar the generated modalities are.\nTo demonstrate the effectiveness of joint generation, assume the prompt modality is P, we compare\nSIM(A, B) of A and B generated separately vs. jointly, i.e., {P →A, P →B} vs. {P →\n9\nTable 10: Similarity scores between generated modalities. The number on the left of “\/” represents\nthe similarity score of independent generation, and the right it represents the case of joint generation.\nJointly generated outputs consistently show stronger coherence.\nInputs\nSIM-IT\nSIM-AT\nSIM-VT\nSIM-VA\nTwo Joint Outputs\nAudio →Image+Text\n0.251 \/ 0.260\n-\n-\n-\nImage →Audio+Text\n-\n0.244 \/ 0.256\n-\n-\nText →Video+Audio\n-\n-\n-\n0.240 \/ 0.255\nAudio →Video+Text\n-\n-\n0.256 \/ 0.261\n-\nThree Joint Outputs\nText →Video+Image+Audio\n0.256 \/ 0.270\n0.240 \/ 0.257\n-\n0.240 \/ 0.257\nMulti-Inputs-Outputs\nText+Image →Video+Audio\n-\n-\n-\n0.247 \/ 0.259\nA + B}. The benchmark is the validation set of AudioCaps [24]. We test on the following settings,\naudio →image+text, image →audio+text, and text→video+audio, image →video+audio. audio→\nvideo+text, audio→text+video+image, text →video+image+audio, where the image prompt is the\nmiddle frame of the video clip. As shown in Table 10, joint generation (similarity shown on the right\nside of “\/”) consistently outperforms independent generation (on the left side of “\/”).\n6\nConclusion\nIn this paper, we present Composable Diffusion (CoDi), a groundbreaking model in multimodal\ngeneration that is capable of processing and simultaneously generating modalities across text, image,\nvideo, and audio. Our approach enables the synergistic generation of high-quality and coherent\noutputs spanning various modalities, from assorted combinations of input modalities. Through\nextensive experiments, we demonstrate CoDi’s remarkable capabilities in ﬂexibly generating single\nor multiple modalities from a wide range of inputs. Our work marks a signiﬁcant step towards\nmore engaging and holistic human-computer interactions, establishing a solid foundation for future\ninvestigations in generative artiﬁcial intelligence. Limitations & Broader Impacts. See Appendix D\nfor limitations and broader impacts discussion.\nAcknowledgement\nWe would like to thank Bei Liu for HD-VILA-100M data support. We also thank Shi Dong, Mahmoud\nKhademi, Junheng Hao, Yuwei Fang, Yichong Xu and Azure Cognitive Services Research team\nmembers for their feedback.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022. 3\n[2] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift:\nLatent diffusion with temporal shift for efﬁcient text-to-video generation. arXiv preprint arXiv:2304.08477,\n2023. 5, 15, 16\n[3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from\nunlabeled video. Advances in neural information processing systems, 29, 2016. 6, 7\n[4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In Proceedings of the IEEE\/CVF International Conference on Computer\nVision, pages 1728–1738, 2021. 6, 7, 16\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. arXiv\npreprint arXiv:2304.08818, 2023. 7\n10\n[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artiﬁcial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 1\n[7] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu. Valor:\nVision-audio-language omni-perception pretraining model and dataset. arXiv preprint arXiv:2304.08345,\n2023. 7\n[8] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\nIn International Conference on Machine Learning, pages 1931–1942. PMLR, 2021. 3\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 5\n[10] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. arXiv\npreprint arXiv:2105.13290, 2021. 7\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 10687–10696, 2021. 3\n[12] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap: Learning audio\nconcepts from natural language supervision. arXiv preprint arXiv:2206.04769, 2022. 3\n[13] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.\nStructure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,\n2023. 1, 5\n[14] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.\nStructure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,\n2023. 3\n[15] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV, pages 89–106. Springer, 2022. 7\n[16] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore,\nManoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In\n2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776–780.\nIEEE, 2017. 6\n[17] Félix Gontier, Romain Serizel, and Christophe Cerisara. Automated audio captioning by ﬁne-tuning bart\nwith audioset tags. In Detection and Classiﬁcation of Acoustic Scenes and Events-DCASE 2021, 2021. 7\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n15\n[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High deﬁnition video\ngeneration with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840–6851, 2020. 2, 4\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\nVideo diffusion models. arXiv preprint arXiv:2204.03458, 2022. 3, 15\n[22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for\ntext-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1, 7\n[23] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu,\nXiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion\nmodels. arXiv preprint arXiv:2301.12661, 2023. 1, 3\n11\n[24] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions\nfor audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 119–132, 2019. 6, 7, 9, 10\n[25] Eungbeom Kim, Jinhee Kim, Yoori Oh, Kyungsu Kim, Minju Park, Jaeheon Sim, Jinwoo Lee, and Kyogu\nLee. Improving audio-language learning with mixgen and multi-level test-time augmentation. arXiv\npreprint arXiv:2210.17143, 2022. 7\n[26] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014. 15\n[27] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hiﬁ-gan: Generative adversarial networks for efﬁcient\nand high ﬁdelity speech synthesis. Advances in Neural Information Processing Systems, 33:17022–17033,\n2020. 5\n[28] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song.\nAcav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision, pages 10274–10284, 2021. 7\n[29] Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and Jianfeng Gao. Optimus:\nOrganizing sentences via pre-trained modeling of a latent space. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 4678–4699, 2020. 5\n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 7\n[31] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In\nComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart XXX 16, pages 121–137. Springer, 2020. 7\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740–755. Springer, 2014. 7\n[33] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint\narXiv:2301.12503, 2023. 1, 3, 5, 7\n[34] Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, and Tao Mei. Semantic-\nconditional diffusion networks for image captioning. arXiv preprint arXiv:2212.03099, 2022. 7\n[35] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip preﬁx for image captioning. arXiv preprint\narXiv:2111.09734, 2021. 3, 7\n[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021. 7\n[37] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023. 1\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748–8763. PMLR,\n2021. 3, 4\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 5\n[40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 3\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684–10695, 2022. 1, 3, 4, 5\n12\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An\nopen large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, 2022. 6, 7\n[43] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining\nfor multimodal video captioning. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 17959–17968, 2022. 7\n[44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv\npreprint arXiv:2209.14792, 2022. 1, 3, 7\n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages\n2256–2265. PMLR, 2015. 2, 4\n[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In International Conference on\nLearning Representations, 2021. 3\n[47] Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal. TVLT: Textless vision-language transformer.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural\nInformation Processing Systems, 2022. 3\n[48] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022. 7\n[49] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. arXiv preprint arXiv:2202.03052, 2022. 7\n[50] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.\nGodiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806,\n2021. 7\n[51] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nüwa: Visual\nsynthesis pre-training for neural visual world creation. In Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XVI, pages 720–736. Springer, 2022.\n7\n[52] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian,\nWei Wang, et al. mplug-2: A modularized multi-modal foundation model across text, image and video.\narXiv preprint arXiv:2302.00402, 2023. 7\n[53] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text,\nimages and variations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022. 5, 7\n[54] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining\nGuo. Advancing high-resolution video-language representation with large-scale video transcriptions. In\nProceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 5036–5045,\n2022. 6, 7, 16\n[55] Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian,\nMei Gao, Yi-Ling Chen, et al. i-code: An integrative and composable multimodal learning framework.\narXiv preprint arXiv:2205.01818, 2022. 3\n[56] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through\nvision and language and sound. In Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 16375–16387, 2022. 3\n[57] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin\nChoi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing\nSystems, 34:23634–23651, 2021. 3\n13\n[58] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha. Object\nrelational graph with teacher-recommended learning for video captioning. In Proceedings of the IEEE\/CVF\nconference on computer vision and pattern recognition, pages 13278–13288, 2020. 7\n[59] Zixin Zhu, Yixuan Wei, Jianfeng Wang, Zhe Gan, Zheng Zhang, Le Wang, Gang Hua, Lijuan Wang,\nZicheng Liu, and Han Hu. Exploring discrete diffusion models for image captioning. arXiv preprint\narXiv:2211.11694, 2022. 7\n14\nA\nModel Architecture and Conﬁguration\nA.1\nOverview\nIn this section, we provide more details on the model architecture as shown in Table 11, where each\nmodality speciﬁc diffuser is based on UNet architecture with different variations detailed in the table.\nAnother notable difference is the video architecture where we add temporal attention and temporal\nshift as discussed in Section 3.3 and we will discuss its detail in the next section.\nTable 11: Hyperparameters for our diffusion models. Note the video and image generation uses the\nsame diffuser.\nModality\nVideo (Image) LDM\nAudio LDM\nText LDM\nHyperparameter\nArchitecture\nLDM\nLDM\nLDM\nz-shape\n4 × #frames × 64 × 64\n8 × 256 × 16\n768 × 1 × 1\nChannels\n320\n320\n320\nDepth\n4\n2\n2\nChannel multiplier\n1,2,4,4\n1,2,4,4\n1,2,4,4\nAttention resolutions\n64,32,16\n64,32,16\n64,32,16\nHead channels\n32\n32\n32\nNumber of heads\n8\n8\n8\nCA embed dim\n768\n768\n768\nCA resolutions\n64,32,16\n64,32,16\n64,32,16\nAutoencoders\nAutoKL\nAudioLDM\nOptimus\nWeight initialization\nStable Diffusion-1.4\n-\nVersatile Diffusion\nParameterization\nϵ\nϵ\nϵ\nLearning rate\n2e −5\n5e −6\n5e −5\nTotal batch size\n256\n1024\n1024\nDiffusion Setup\nDiffusion steps\n1000\n1000\n1000\nNoise schedule\nLinear\nLinear\nLinear\nβ0\n0.00085\n0.00085\n0.00085\nβT\n0.0120\n0.0120\n0.0120\nSampling Parameters\nSampler\nDDIM\nDDIM\nDDIM\nSteps\n50\n50\n50\nη\n1.0\n1.0\n1.0\nGuidance scale\n2.0\n7.5\n2.0\nA.2\nVideo LDM Architecture\nExcept for the base image UNet architecture, we also add temporal attention and temporal shift [2]\nbefore each residual block. Following VDM [21], the temporal attention is a transformer attention\nmodule where we ﬂatten the height and width dimension to batch size dimension and the self-attention\nis performed on the time dimension. The temporal shift is illustrated in Fig. 6 where we ﬁrst split\nchannels into k chunks. Then, we shift the channel dimension numbered 0 to k −1 by temporal\ndimension from 0 to k −1 times respectively. Eventually, we concatenate the shifted chunks by\nthe hidden dimension. Note that we use k = 3 in the illustration for simplicity but k = 8 in our\nimplementation. We then add a convolution layer before the temporal shift module. Finally, we use\nresidual connection [18] and add the output to the input before the convolution layer.\nB\nModel Training\nPrompt Encoders Training.\nAs discussed in Section 3.2, we use bridging alignment to perform\ncontrastive learning between all prompt encoders. We use Adam [26] optimizer with learning rate\n1e-4 and weight decay 1e-4.\nDiffusion Model Training.\nWe train diffusion model with training objectives and hyperparameters\ndetailed in Table 1 and Table 11. For video LDM, we adopt a more speciﬁc training curriculum.\n15\nt-1\nt\nt+1\nC\nW\nH\nC\nW\nH\nC\nW\nH\nC\/3\nW\nC\/3 C\/3\nC\/3\nW\nH\nC\/3 C\/3\nC\/3\nW\nH\nC\/3 C\/3\nH\nC\/3\nW\nC\/3 C\/3\nC\/3\nW\nH\nC\/3 C\/3\nC\/3\nW\nH\nC\/3 C\/3\nH\nFigure 6: Temporal shift [2] illustration. C, H, W represent channel, height, width, respectively. The\nvertical line represents time steps from t −1, t, and t + 1. The grey blocks denote “padding tensors”.\nWe adopt curriculum learning on frame resolution and frames-per-second (FPS). First, the diffuser\nis trained on the WebVid dataset of a 256-frame resolution, with the training objective being text-\nconditioned video generation. The training clips are sampled from 2-second video chunks with 4 FPS.\nSecond, the model is further trained on HDVILLA and ACAV datasets, with a 512-frame resolution\nand 8 FPS, and the training objective is image-conditioned video generation (the image is a randomly\nsampled frame of the clip). Each training clip contains 16 frames sampled from a 2-second video\nchunk with 8 FPS.\nJoint Generation Training.\nAs discussed in Section 3.2, we train joint generation by aligning\nenvironment encoders and optimize cross-attention layers only in the diffusion models. We use Adam\noptimizer with learning rate 1e-5 and weight decay 1e-4.\nC\nTraining Datasets\nIn this section, we introduce more details about the video and audiovisual training datasets.\nVideo.\nWebVid [4] is a large-scale dataset of web videos with diverse content, spanning over 40\ncategories such as sports, cooking, and travel. It contains over 1.2 million video clips (all without\nsound) that are all at least 30 seconds in duration with video descriptions. We perform text→video\nand video-text contrastive learning task with this dataset. HD-Villa-100M [54] is a large-scale video\ndataset with over 100 million video clips sourced from YouTube. The dataset covers a wide range of\nvideo categories and includes high-quality videos with a resolution of at least 720P. Since it lacks\ncurated video description and we use the middle frame as image input to perform image→video\ngeneration.\nAudiovisual.\nSoundNet originally contains over two million sounds and spans a wide range of\ncategories including music, animal sounds, natural sounds, and environmental sounds. We collected\nall currently accessible 1M videos.\nD\nLimitations & Broader Impacts\nWhile the paper primarily focuses on the technical advancements and potential applications of\nCoDi, we also consider potential negative social impacts that could arise from the development and\ndeployment of such technology. These impacts can include:\nDeepfakes and Misinformation.\nAs part of a common issue for generative AI models, the ability\nof CoDi to generate realistic and synchronized multimodal outputs also raises concerns about the\n16\ncreation and dissemination of deepfakes. Malicious actors could exploit this technology to create\nhighly convincing fake content, such as fabricated videos or audio clips, which can be used for\nmisinformation, fraud, or other harmful purposes.\nBias and Stereotyping.\nIf the training data used for CoDi is biased or contains stereotypes, the\ngenerated multimodal outputs may also reﬂect these.\n17\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Any-to-Any Generation via Composable Diffusion.pdf"}
{"title":"Multimodal Unified Attention Networks for Vision-and-Language Interactions","authors":"Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian","summary":"Learning an effective attention mechanism for multimodal data is important in\nmany vision-and-language tasks that require a synergic understanding of both\nthe visual and textual contents. Existing state-of-the-art approaches use\nco-attention models to associate each visual object (e.g., image region) with\neach textual object (e.g., query word). Despite the success of these\nco-attention models, they only model inter-modal interactions while neglecting\nintra-modal interactions. Here we propose a general `unified attention' model\nthat simultaneously captures the intra- and inter-modal interactions of\nmultimodal features and outputs their corresponding attended representations.\nBy stacking such unified attention blocks in depth, we obtain the deep\nMultimodal Unified Attention Network (MUAN), which can seamlessly be applied to\nthe visual question answering (VQA) and visual grounding tasks. We evaluate our\nMUAN models on two VQA datasets and three visual grounding datasets, and the\nresults show that MUAN achieves top-level performance on both tasks without\nbells and whistles.","url":"http:\/\/arxiv.org\/abs\/1908.04107v2","pdf_url":"http:\/\/arxiv.org\/pdf\/1908.04107v2","published":1565611937000,"comment":"11 pages, 7 figures","pdf_text":"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nMultimodal Uniﬁed Attention Networks for\nVision-and-Language Interactions\nZhou Yu, Member, IEEE, Yuhao Cui, Jun Yu, Member, IEEE, Dacheng Tao, Fellow, IEEE,\nQi Tian Fellow, IEEE\nAbstract—Learning an effective attention mechanism for mul-\ntimodal data is important in many vision-and-language tasks that\nrequire a synergic understanding of both the visual and textual\ncontents. Existing state-of-the-art approaches use co-attention\nmodels to associate each visual object (e.g., image region) with\neach textual object (e.g., query word). Despite the success of these\nco-attention models, they only model inter-modal interactions\nwhile neglecting intra-modal interactions. Here we propose a\ngeneral ‘uniﬁed attention’ model that simultaneously captures\nthe intra- and inter-modal interactions of multimodal features\nand outputs their corresponding attended representations. By\nstacking such uniﬁed attention blocks in depth, we obtain the\ndeep Multimodal Uniﬁed Attention Network (MUAN), which can\nseamlessly be applied to the visual question answering (VQA)\nand visual grounding tasks. We evaluate our MUAN models on\ntwo VQA datasets and three visual grounding datasets, and the\nresults show that MUAN achieves top level performance on both\ntasks without bells and whistles.\nIndex Terms—Multimodal learning, visual question answering\n(VQA), visual grounding, uniﬁed attention, deep learning.\nI. INTRODUCTION\nDeep learning in computer vision and natural language pro-\ncessing has facilitated recent advances in artiﬁcial intelligence.\nSuch advances drive research interest in multimodal learning\ntasks lying at the intersection of vision and language such\nas multimodal embedding learning [1][2][3], visual caption-\ning [4][5], visual question answering (VQA) [6] and visual\ngrounding [7], etc. In these tasks, learning a ﬁne-grained\nsemantic understanding of both visual and textual content is\nkey to their performance.\nThe attention mechanism is a predominant focus of recent\ndeep learning research. It aims to focus on certain data\nelements, and aggregate essential information to obtain a more\ndiscriminative local representation [8], [4]. This mechanism\nhas improved the performance of a wide range of unimodal\nlearning tasks (e.g., vision [9], [10], [11], language [12], [13],\nThis work was supported in part by National Natural Science Foundation\nof China under Grant 61702143 and Grant 61836002, and in part by the\nAustralian Research Council Projects under Grant FL-170100117. (Zhou Yu\nand Yuhao Cui contribute equally to this work. Jun Yu is the corresponding\nauthor.)\nZ. Yu, Y. Cui and J. Yu are with Key Laboratory of Complex Systems Mod-\neling and Simulation, School of Computer Science and Technology, Hangzhou\nDianzi University, P. R. China (e-mail: yuz@hdu.edu.cn; yujun@hdu.edu.cn;\ncuiyh@hdu.edu.cn\nD. Tao is with the UBTECH Sydney AI Centre, School of Computer\nScience, Faculty of Engineering, The University of Sydney, Australia (e-mail:\ndacheng.tao@sydney.edu.au).\nQ. Tian is with the Noah’s Ark Lab, Huawei, P. R. China (e-mail:\ntian.qi1@huawei.com).\nhow\nmany\npeople\nare\ncatching\nthe\nfrisbee\nhow\nmany\npeople\nare\ncatching\nthe\nfrisbee\nhow\nmany\npeople\nare\ncatching\nthe\nfrisbee\nhow\nmany\npeople\nare\ncatching\nthe\nfrisbee\nHow many people are \ncatching the frisbee?\nV\nT\nV \nT \nUnified Attention\nFig. 1: Schematic of the proposed uniﬁed attention, which\nsimultaneously models inter- and intra-modal interactions in a\nsingle framework. Given multimodal inputs V and T, AV V ,\nAT T denote the intra-modal interactions within each modality,\nwhile AV T and AT V\ndenote the inter-modal interactions\nacross different modalities. V ′ and T ′ are the attended features\nfor V and T respectively.\n[14]) in conjunction with deep convolutional neural networks\n(CNNs) and recurrent neural networks (RNNs).\nFor the multimodal learning tasks described above, attention\nlearning considers the inputs from both the visual and textual\nmodalities. Taking the VQA problem in Fig. 1 as an example,\nto correctly answer a question like ‘How many people are\ncatching the frisbee’ for an image, the attention model should\nideally learn to focus on particular image regions (i.e., the\nperson near the frisbee). Such visual attention based models\nhave become an integral component in many multimodal tasks\nthat require ﬁne-grained visual understanding [4][15][16]. Be-\nyond the visual attention models, recent studies have intro-\nduced co-attention models, which simultaneously learn the\nvisual attention and textual attention to beneﬁt from ﬁne-\ngrained representations for both modalities. Early approaches\nlearned separate attention distributions for each modality in\nan iterative manner, neglecting the dense interaction between\neach question word and image region [17][18]. To address\nthis problem, dense co-attention models have been proposed\nto capture complete interactions between word-region pairs,\nwhich are further extended to form deep co-attention models\n[19][20].\nDespite the success of the co-attention models in multi-\narXiv:1908.04107v2  [cs.CV]  19 Aug 2019\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nmodal learning tasks, these models only consider inter-modal\ninteractions (i.e., AT V or AT V in Fig. 1) while neglecting\nintra-modal ones (i.e., AT T and AV V ). On the other hand,\nmodeling intra-modal interactions has been proved to be\nbeneﬁcial for many unimodal learning tasks [21][22][23][24].\nWe argue that intra-modal interactions within each modality\nprovide complementary and important information to the inter-\nmodal interactions.\nInspired by the famous self-attention model [21] in the\nNLP community, we naturally extend its idea for multimodal\ndata and propose a uniﬁed attention accordingly. Our uniﬁed\nattention model characterizes the intra- and inter-modal in-\nteractions jointly in a uniﬁed framework which we call the\nuniﬁed attention (UA) block (see Fig. 1). The attention map\nlearned from the UA block includes four relationships: the\ninter-modal interactions (AV T and AT V ) to build co-attention\nacross different modalities, and the intra-modal interactions\n(AV V and AT T ) to build self-attention within each modality.\nThe learned uniﬁed attention is further used to obtain the\nattended output features for multimodal inputs. By stacking\nsuch UA block in depth, we obtain the Multimodal Uniﬁed\nAttention Network (MUAN), which can be trained in an end-\nto-end manner to perform deep multimodal reasoning.\nTo evaluate the effectiveness of our proposed MUAN model,\nwe apply it to for VQA and visual grounding. The quantitative\nand qualitative results on two VQA dataset VQA-v2 [25] and\nCLEVR [26], and three visual grounding datasets RefCOCO\n[27], RefCOCO+ [27] and RefCOCOg [28] show that MUAN\nachieves top level performance on both tasks without using\nany dataset speciﬁc model tuning.\nIn summary, we have made the following contributions in\nthis study:\n• We extend the self-attention model for single modality\nto a uniﬁed attention model, which can characterize\nintra- and inter-modal interactions of multimodal data. By\nstacking such uniﬁed attention model (i.e., UA block) in\ndepth, we obtain a neat multimodal uniﬁed attention net-\nwork (MUAN), which can perform accurate multimodal\nreasoning.\n• We modify the original self-attention model to a gated\nself-attention (GSA) model as the basic component for\nthe UA block, which facilities more accurate and robust\nattention learning and leads to more discriminative fea-\ntures for speciﬁc tasks.\n• We apply MUAN to two multimodal learning tasks,\nnamely VQA and visual grounding. The results on ﬁve\nbenchmark datasets show the superiority of MUAN over\nexisting state-of-the-art approaches.\nII. RELATED WORK\nWe brieﬂy review existing studies on VQA and visual\ngrounding, and establish a connection between these two tasks\nby attention learning.\nVisual Question Answering (VQA). VQA aims to answer a\nquestion in natural language with respect to a given image,\nso requires multimodal reasoning over multimodal inputs.\nSince Antol et al. presented a large-scale VQA benchmark\ndataset with free-form questions [6], multimodal fusion and\nattention learning have become two major research focuses for\nVQA. For multimodal fusion, early methods used simple con-\ncatenation or element-wise multiplication between multimodal\nfeatures [29][6]. Fukui et al. [16], Kim et al. [30], Yu et al.\n[18] and Ben et al. [31] proposed different approximated bi-\nlinear pooling methods to effectively integrate the multimodal\nfeatures with second-order feature interactions. For attention\nlearning, question-guided visual attention on image regions\nhas become the de-facto component in many VQA approaches\n[15][32]. Chen et al. proposed a question-guided attention map\nthat projects the question embeddings to the visual space and\nformulates a conﬁgurable convolutional kernel to search the\nimage attention region [32]. Yang et al. proposed a stacked\nattention network to learn the attention iteratively [15]. Some\napproaches introduce off-the-shelf object detectors [33] or\nobject proposals [34] as the candidates of the attention regions\nand then use the question to identify the relevant ones. Taken\nfurther, co-attention models that consider both textual and vi-\nsual attentions have been proposed [17][18]. Lu et al. proposed\na co-attention learning framework to alternately learn the\nimage attention and question attention [17]. Yu et al. reduced\nthe co-attention method into two steps, self-attention for a\nquestion embedding and the question-conditioned attention for\na visual embedding [35]. The learned co-attentions by these\napproaches are coarse, in that they neglect the interaction\nbetween question words and image regions. To address this\nissue, Nguyen et al. [20] and Kim et al. [19] introduced dense\nco-attention models that established the complete interaction\nbetween each question word and each image region.\nVisual Grounding. Visual grounding (a.k.a., referring expres-\nsion comprehension) aims to localize an object in an image\nreferred to in query text. Most previous approaches follow a\ntwo-stage pipeline [7][36][16]: 1) use an off-the-shelf object\ndetector, such as Edgebox [37] or Faster R-CNN [38] to\ngenerate a set of region proposals along with the proposal\nfeatures for the input image; and 2) compute a matching score\nbetween each proposal feature and query feature and adopt the\nproposal (or its reﬁned bounding box [39]) with the highest\nscore as the referent. From the attention learning point of view,\nvisual grounding represents a task of learning query-guided\nattention on the image region proposals. The aforementioned\ntwo-stage approaches are analogous to the visual attention\nmodels in VQA. Yu et al. [40], Zhang et al. [41] and Deng\net al. [42] also modeled the attention on question words\nalong with visual attention, providing a connection to the co-\nattention model in VQA.\nJoint Modeling of Self- and Co-Attention. Although ex-\ntensive studies on self-attention and co-attention have been\nmade by existing multimodal learning methods, the two kinds\nof attentions are usually considered solely. To the best of\nour knowledge, only a few attempts have modeled intra-\nand inter-modal interactions jointly. Li et al. introduced a\nvideoQA approach which used self-attention to learn intra-\nmodal interactions of video and question modalities respec-\ntively, and then fed them through a co-attention block to\nmodel inter-modal interactions [43]. Gao et al. presented a\ndynamic fusion framework for VQA with modeling intra- and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\ninter-modal attention blocks. [44]. Yu et al applied a modular\nco-attention network for VQA which stacked multiple self-\nattention and guided-attention blocks in depth to perform deep\nvisual reasoning. In summary, all these methods models the\nself-attention and co-attention in two sequential stages, which\nis sub-optimal and may result in serious information lose. This\ninspires us to design a general uniﬁed attention framework to\nsimultaneously model the two attentions in one stage.\nIII. MULTIMODAL UNIFIED ATTENTION\nIn this section, we introduce the multimodal uniﬁed at-\ntention, which is the basic component of our Multimodal\nUniﬁed Attention Network (MUAN). Taking the multimodal\ninput features X from the image modality and Y from the\ntext modality, the uniﬁed attention outputs their correspond-\ning attended features. In contrast to existing visual attention\nmethods, which model unidirectional inter-modal interactions\n(i.e., X →Y ) [16][30], or the co-attention methods, which\nmodel bidirectional inter-modal interactions (i.e., X ↔Y )\n[19][20], our uniﬁed attention models the intra-modal and\ninter-modal interactions simultaneously (i.e., X →X, Y →Y\nand X ↔Y ) in a general framework.\nInspired by the self-attention model which has achieved\nremarkable\nperformance\nin\nnatural\nlanguage\nprocessing\n[21][45][22], we design a uniﬁed attention model for mul-\ntimodal data. Furthermore, to obtain more accurate attention\nmap in the uniﬁed attention learning, we introduce a bilinear\npooling based gating model to reweight the importance of in-\nput features, which can to some extent eliminate the irrelevant\nor noisy features.\nA. Gated Self-Attention\nThe self-attention model proposed in [21] takes a group of\ninput features X = [x1; ...; xm] ∈Rm×dx and outputs a group\nof attended features F = [f1, ..., fm] ∈Rm×d, where m is\nthe number of samples, dx and d are the dimensionalities of\ninput and output features, respectively. To achieve this goal,\nX is ﬁrst fed into three independent fully-connected layers.\nQ = FCq(X), K = FCk(X), V = FCv(X)\n(1)\nwhere Q, K, V\n∈Rm×d are three feature matrices of the\nsame shape, corresponding to the queries, keys, and values,\nrespectively.\nGiven a query q ∈Q and all keys K, we calculate the dot-\nproducts of q with K, divide each by a scaling factor\n√\nd and\napply the softmax function to obtain the attention weights on\nthe values. In practice, the attention function can be computed\non all queries Q simultaneously, and in doing so we obtain\nthe output features F as follows:\nA = softmax(QKT\n√\nd\n)\n(2)\nF = AV\n(3)\nwhere A ∈Rm×m is the attention map containing the attention\nweights for all query-key pairs, and the output features F are\nthe weighted summation of the values V determined by A.\nFC\nFC\n Gated \nDot-product\nFC\nFC\nσ\nFC\nFC\nMatMul\nQ\nV\nK\nK\nQ\nMatMul\n(a) Gated Self-Attention (GSA)\nAdd & Norm\nFFN\nAdd & Norm\nGSA\nX\nY\nFC\nFC\nZ\nZ’\n(b) UA Block\nFig. 2: Flowcharts of the Gated Self-Attention (GSA) model\nand uniﬁed attention (UA) block for multimodal data\nLearning an accurate attention map A is crucial for self-\nattention learning. The scaled dot-product attention in Eq.(2)\nmodels the relationship between feature pairs. However, the\nimportance of each individual features is not explicitly con-\nsidered during attention learning. Consequently, irrelevant or\nnoisy features may have a negative impact on the attention\nmap, resulting in inaccurate output features. To address this\nproblem, we introduce a novel gating model into Eq.(2) to\nimprove the quality of the learned attention. Inspired by the\nbilinear pooling models which have been in ﬁne-grained visual\nrecognition [46] and multi-modal fusion [30], we design a\ngating model based on low-rank bilinear pooling to reweight\nthe features of Q and K before their scaled dot-products:\nM = σ\n\u0000FCg \u0000FCg\nq(Q) ⊙FCg\nk(K)\n\u0001\u0001\n(4)\nwhere FCg\nq, FCg\nk ∈Rd×dg, FCg ∈Rdg×2 are three indepen-\ndent fully-connected layers, and dg is the dimensionality of the\nprojected space. ⊙denotes the element-wise product function\nand σ(·) the sigmoid function. M ∈Rm×2 corresponds to the\ntwo masks Mq ∈Rm and Mk ∈Rm for the features Q and\nV , respectively.\nThe learned two masks Mq and Mk are tiled to ˜\nMq, ˜\nMk ∈\nRm×d and then used to formulate a gated self-attention (GSA)\nmodel as follows:\nAg = softmax((Q ⊙˜\nMq)(K ⊙˜\nMk)T\n√\nd\n)\n(5)\nF = AgV\n(6)\nFig. 2a illustrates the ﬂowchart of our gated self-attention\nmodel. Similar to [21], the multi-head strategy is introduced\nin our model to attain more diverse attention.\nB. Uniﬁed Attention Block\nBased on the gated self-attention model above, we introduce\nthe multimodal uniﬁed attention block, which simultaneously\nmodels intra- and inter-modal interactions.\nGiven a group of textual features (e.g., question words) X ∈\nRm×dx and a group of visual features (e.g., image regions)\nY ∈Rn×dy, we ﬁrst learn two fully-connected layers FCx and\nFCy to embed X and Y into a dz-dimensional common space,\nand then concatenate the two groups of embedded features on\nrows to form a uniﬁed feature matrix Z:\nZ = [FCx(X); FCy(Y )]\n(7)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nwhere Z = [z1, ..., zs] ∈Rs×dz with s = m + n1.\nThe UA block (see Fig. 2b) consists of a gated self-\nattention (GSA) module and a feed-forward network (FFN)\nmodule. Taking the uniﬁed feature matrix Z as input, the GSA\nmodule learns the pairwise interactions between the sample\npairs < zi, zj > within Z. Since zi and zj may come from\ndifferent (or the same) modalities, the intra- and inter-modal\nrelationships are represented at the same time. Compared to\nexisting co-attention models, which only model the inter-\nmodal relationships [19][20], the intra-modal relationships\n(e.g., word-to-word or region-to-region) are also important\nfor understanding the intrinsic structure within each modality,\nthus facilitating more accurate visual reasoning. The FFN\nmodule takes the output features of the GSA module as input,\nand then performs transformation through two consecutive\nfully-connected layers (FC(4d)-ReLU-Drop(0.1)-FC(d)). To\nsimplify optimization, shortcut connection [38] and layer nor-\nmalization [47] are applied after the GSA and FFN modules.\nIt is worth noting that the ﬁnal output features Z′ of the UA\nblock are of the same shape as the input features Z, making\nit possible to stack multiple UA blocks in depth2.\nIV. MULTIMODAL UNIFIED ATTENTION NETWORKS\nIn this section, we describe the MUAN architectures for\nVQA and visual grounding (see Fig. 3). The core component\nof both models is the deep MUAN-L model, which consists\nof L UA blocks stacked in depth to perform deep multimodal\nreasoning and attentional feature transformation. The proposed\nVQA model and the visual grounding model are very similar\nto each other, except for the input feature representations and\nthe loss functions used during model training. We therefore\nhighlight these two parts in each model.\nA. Architecture for VQA\nImage and Question Representations. The inputs for VQA\nconsist of an images and a question, and the goal is to\npredict an answer to the question. Our model ﬁrst extracts\nrepresentations for the image and the question and then feeds\nthe multimodal features into the MUAN model to output their\ncorresponding output features with uniﬁed attention learning.\nFinally, one of the attended feature is fed to a multi-label\nclassiﬁer to predict the correct answer.\nThe input question is ﬁrst tokenized into a sequence of\nwords, and then trimmed (or zero padded) to a maximum\nlength of m. Similar to [22], we add a dummy token [ans]\nat the beginning of the question, and the attended feature of\nthis token will be used to predict the answer. These words are\nﬁrstly represented as one-hot vectors and then transformed to\n300-D word embeddings using the pre-trained GloVe model\n[48]. Finally, the word embeddings are fed into a one-layer\nLSTM network [49] with dx hidden units, resulting in the\nﬁnal question feature X ∈R(m+1)×dx. The input image\nis represented as a group of dy-dimensional visual features\n1In our implementation, we let dx = dz = d and omit FCx(·) for\nsimplicity, and rewrite Eq.(7) as Z = [X; FCy(Y )]\n2For multiple UA blocks stacked in depth, only the ﬁrst block needs to\nhandle multimodal inputs. Eq.(7) is omitted in the other blocks.\nextracted from a pre-trained CNN model [38] or a pre-\ntrained object detector [50]. This results in the image feature\nY ∈Rn×dy, where n is the number of extracted features.\nNote that we mask the zero-padded features during attention\nlearning to make their attention weights all zero.\nMUAN-L. The multimodal features X\nand Y\nare fed\ninto a deep MUAN-L model consisting of L UA blocks\n[UA(1), UA(2), ..., UA(L)]. For UA(1), X and Y are integrated\nby Eq.(7) to obtain the initialized uniﬁed features Z(0), which\nare further fed to the remaining UA blocks in a recursive\nmanner.\nZ(l+1) = UA(l+1)(Z(l))\n(8)\nwhere l ∈[0, L −1]. Note that the ﬁnal output features Z(L)\nare the same shape as the input features Z(0), and each paired\n< z(0)\ni\n, z(L)\ni\n> has a one-to-one correspondence.\nAnswer Prediction. Using the attended features Z(L) from\nMUAN-L, we project the ﬁrst feature z(L)\n1\n(the [ans] token)\ninto a vector p ∈Rk, where k corresponds to the size of the\nanswer vocabulary.\nFor the datasets that have multiple answers to each question,\nwe following the strategy in [51] and use the binary cross-\nentropy (BCE) loss to train an k-way classiﬁer with respect\nto the ground-truth label y ∈Rk:\nL =\nk\nX\ni=1\n(yilog(σ(pi)) + (1 −yi)log(1 −σ(pi)))\n(9)\nwhere σ(·) is the sigmoid activation function.\nFor the datasets that have exactly one answer to each ques-\ntion, we use the softmax cross-entropy loss to train the model\nwith respect to the one-hot ground-truth label y ∈{0, 1}k:\nL = −yT log softmax(p)\n(10)\nB. Architecture for Visual Grounding\nThe inputs for visual grounding consist of an image and a\nquery. Similar to the VQA architecture above, we extract the\nquery features X ∈Rm×dx using GloVe embeddings followed\nby a LSTM network, and extract the region-based proposal\nfeatures Y ∈Rn×dy for the image using an pre-trained object\ndetector. Note that we do not use the dummy token for visual\ngrounding which is specially designed for VQA.\nThe multimodal input features are integrated and trans-\nformed by MUAN-L to output their attended representations.\nOn top of the attended feature for each region proposal, we\nappend two fully-connected layers to project each attended\nfeature z(L) ∈Z(L) into a score s ∈R and a 4-D vector\nt ∈R4 to regress the reﬁned bounding box coordinates for\nthe proposal, respectively.\ns = FC(z(L));\nt = FC(z(L))\n(11)\nAccordingly, a ranking loss Lrank and a regression loss Lreg\nare designed to optimize the model in a multitask learning\nmanner. Following the strategy in [39], KL-divergence is used\nas the ranking loss:\nLrank = 1\nn\nn\nX\ni=1\ns∗\ni log(s∗\ni\nsi\n)\n(12)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nUA Block\nUA Block\nUA Block\nL\nQ : How many \nblack sheep on \nthe grass?\nGloVe+LSTM\nPre-trained CNN \/\nObject Detector\n[ans]\n Q: black sheep on \nthe grass\nGloVe+LSTM\n...\n...\nx\nVisual Question Answering\nVisual Grounding\nMUAN-L\nReg Loss\nRank Loss\nMUAN-L\nBCE Loss\nA : 1\n[ans]\nObject Detector \nFig. 3: Architectures of the Multimodal Uniﬁed Attention Networks (MUAN) for visual question answer (left) and visual\ngrounding (right), respectively. Both architectures contain the a MUAN-L model which consists of L stacked UA blocks to\noutput the features with uniﬁed attention learning. For VQA, we add a dummy token [ans] at the beginning of the question,\nand use its attended feature to predict the answer. For visual grounding, the attended features of the region proposals are used\nto predict their ranking scores and reﬁned bounding boxes.\nwhere S = [s1, s2, ..., sn] ∈Rn are the predicted scores for\nn proposals. The ground-truth label S∗= [s∗\n1, s∗\n2, ..., s∗\nn] ∈\nRn is obtained by calculating the IoU scores of all proposals\nw.r.t. the unique ground-truth bounding box and assign the\nIoU score of the i-th proposal to s∗\ni if the IoU score is larger\nthan a threshold η and 0 otherwise. Softmax normalizations\nare respectively applied to S and S∗to make them form a\nscore distribution.\nThe smoothed ℓ1 loss [52] is used as the regression loss\nto penalize the differences between the reﬁned bounding box\nand the ground-truth bounding box:\nLreg = 1\nn\nn\nX\ni=1\nsmoothL1(t∗\ni , ti)\n(13)\nwhere ti ∈R4 and t∗\ni ∈R4 correspond to the coordinates\nof the predicted bounding box and the ground-truth bounding\nbox for i-th proposal, respectively.\nBy combining the two terms, we obtain the overall loss\nfunction Lall as follows:\nLall = Lrank + λLreg\n(14)\nwhere λ is a hyper-parameter to balance the two terms.\nV. EXPERIMENTS\nIn this section, we conduct experiments to evaluate the per-\nformance of the MUAN models in VQA and visual grounding\ntasks. We conduct extensive ablation experiments to explore\nthe effect of different hyper-parameters in MUAN. Finally,\nwe compare the best MUAN models to current state-of-the-\nart methods on ﬁve benchmark datasets (two VQA datasets\nand three visual grounding datasets).\nA. Datasets\nVQA-v2 is a commonly-used benchmark dataset for open-\nended VQA [25]. It contains human annotated question-answer\npairs for MS-COCO images [53]. The dataset is split into\nthree subsets: train (80k images with 444k questions); val\n(40k images with 214k questions); and test (80k images with\n448k questions). The test subset is further split into test-\ndev and test-std sets that are evaluated online with limited\nattempts. For each questions, multiple answer are provided by\ndifferent annotators. To evaluate the performance of a model\nwith respect to such multi-label answers, an accuracy-based\nevaluation metric is deﬁned as follows which is robust to inter-\nhuman variability in phrasing the answer a [6]:\nAccuracy(a) = min\n\u001acount(a)\n3\n, 1\n\u001b\n(15)\nwhere count(a) is a function that count the answer a voted\nby different annotators.\nCLEVR is a synthesized dataset containing 100k images and\n853k questions [26]. Each image contains 3D-rendered objects\nand is associated with a number of questions that test various\naspects of visual reasoning including attribute identiﬁcation,\nobject counting, and logical operations. The whole dataset is\nsplit into three subsets: train (70k images with 700k questions),\nval (15k images with 150k questions) and test (15k images\nwith 15k questions). Each question is associated with exactly\none answer and standard accuracy metric is used to evaluate\nmodel performance.\nRefCOCO, RefCOCO+, and RefCOCOg are three datasets to\nevaluate visual grounding performance. All three datasets are\ncollected from MS-COCO images [53], but the queries are\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nQ: Are the giraffes eating?\nA: Yes\nQ: What color are the motorcycles?\nA: White and black\nQ: How many oranges in the bowl?\nA: 1\n(a) VQA-v2\nQ: Do the brown object and the small \npurple cylinder have the same material?\nA: No\nQ: How many purple things are made of \nthe same material as the gray cube?\nA: 0\nQ: What is the color of the large shiny \nblock?\nA: purple\n(b) CLEVR\nQ: full pizza in center\nQ: girl on left with green jacket\n(c) RefCOCO\nQ: man with glasses facing us\nQ: bear full glass\n(d) RefCOCO+\nQ: a catcher sitting in a baseball \nfield\nQ: a green bench with no one \nsitting on it\n(e) RefCOCOg\nFig. 4: Typical examples from VQA-v2, CLEVR, RefCOCO, RefCOCO+, and RefCOCOg.\ndifferent in three respects: 1) RefCOCO [27] and RefCOCO+\n[27] contains short queries (3.6 words on average) while\nRefCOCOg [28] contains relatively long queries (8.4 words on\naverage); 2) RefCOCO and RefCOCO+ contain 3.9 same-type\nobjects on average, while in RefCOCOg this number is 1.6;\nand 3) RefCOCO+ does not contain any location word, while\nthe counterparts do not have this constraint. RefCOCO and\nRefCOCO+ are split into four subsets: train (120k queries),\nval (11k queries), testA (6k queries about people), and testB\n(5k queries about objects). RefCOCOg is split into three\nsubsets: train (81k queries), val (5k queries), and test (10k\nqueries). For all the three datasets, accuracy is adopted as\nthe evaluation metric, which is deﬁned as the percentage in\nwhich the predicted bounding box overlaps with the ground-\ntruth bounding box by IoU>0.5.\nFig. 4 shows some typical examples from these datasets.\nB. Experimental Setup\nUniversal Setup. We use the following hyper-parameters as\nthe default settings for MUAN unless otherwise noted. In each\nUA block, the latent dimensionality d is 768 and the number of\nheads h is 8, so the dimensionality of each head is d\/h = 96.\nThe latent dimensionality in the gating model dg is 96. The\nnumber of UA blocks L ranges from 2 to 12.\nAll the models are optimized using the Adam solver [54]\nwith β1 = 0.9 and β2 = 0.99. The models (except those for\nCLEVR) are trained up to 13 epochs with a batch size 64 and\na base learning rate α set to 1.5e−2\/\n√\ndL. Similar to [19],\nthe learning rate is warmed-up for 3 epochs and decays by\n1\/5 every 2 epochs after 10 epochs. We report the best results\nevaluated on the validation set. For CLEVR, a smaller base\nlearning rate α = 3.5e−3\/\n√\ndL is used to train up to 20 epochs\nand decay by 1\/5 at the 16th and 18th epochs, respectively.\nVQA Setup. For VQA-v2, we follow the strategy in [51] and\nextract the pool5 feature for each object from a Faster R-\nCNN model (with a ResNet-101 backbone) [55] pre-trained on\nthe Visual Genome dataset [56], resulting in the input visual\nfeatures Y ∈Rn×2048, where n ∈[10, 100] is the number of\nextracted objects with a conﬁdence threshold. The maximum\nnumber of question words m = 14, and the size of the answer\nvocabulary k = 3129, which corresponds to answers appearing\nmore than 8 times in the training set. For CLEVR, we follow\nthe strategy in [57] and extract the res4b22 features from a\nResNet-101 model pre-trained on ImageNet [38], resulting in\nthe image features Y ∈R196×1024. The maximum number of\nquestion words m = 43, and the size of the answer vocabulary\nk = 28.\nVisual Grounding Setup. We use the same settings for the\nthree evaluated datasets. To detect proposals and extract their\nvisual features for each image, we use two pre-trained proposal\ndetectors as previous works did: 1) a Faster R-CNN model [55]\npre-trained on the Visual Genome dataset [39]; and 2) a Mask\nR-CNN model [58] pre-trained on MS-COCO dataset [40].\nDuring the training data preparation for the proposal detectors,\nwe exclude the images in the training, validation and testing\nsets of RefCOCO, RefCOCO+ and RefCOCOg to avoid con-\ntamination of the used visual grounding datasets. Each of the\nobtained proposal visual features is further concatenated with a\nspatial feature containing the bounding-box coordinates of the\nproposal3. This results in the image features Y ∈R100×4096.\nThe maximum number of question words m is 15 and the loss\nweight λ is 0.5.\nC. Ablation Studies\nWe run a number of ablation experiments on VQA-v2 to\nexplore the effectiveness of MUAN.\nFirst, we explore the effectiveness of the gating mechanism\nfor the UA block with respect to different number of block L.\nIn Fig. 5a, we report the overall accuracies of the MUAN-L\nmodels (L ranges from 2 to 12) with the gating mechanism\n(i.e., Eq.(5)) or without the gating mechanism (i.e., Eq.(2)) for\nthe UA block. From the results, we can see that MUAN with\nthe gating model steadily outperforms counterpart without the\ngating model. Furthermore, increasing L consistently improves\nthe accuracies of both models, which ﬁnally saturate at L =\n10. We think the saturation is caused by over-ﬁtting. To train\na deeper model we may require more training data [22].\n3For\neach\nproposal,\nwe\nﬁrst\nextract\na\n5-D\nspatial\nfeature\n[xtl\/W, ytl\/H, xbr\/W, ybr\/H, wh\/WH]\nproposed\nin\n[59],\nand\nthen\nlinearly transform it to a 2048-D feature with a fully-connected layer to\nmatch the dimensionality of a 2048-D proposal visual feature.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\n(a) Effect of gating mechanism\n(b) Effect of self- and co-attention\nFig. 5: Ablation of the MUAN models with the number of UA\nblocks L ranges from 2 to 12. All results are evaluated on the\nval split of VQA-v2. (a) Results of MUAN-L variants with or\nwithout the gating mechanism. (b) Results of reference MUAN\nmodel along with the variants without modeling self-attention\n(AT T and AV V ) or co-attention (AT V and AV T ).\nTABLE I: Ablation of the MUAN-10 models with different\nhyper-parameters. All results are reported on the val split\nof VQA-v2. Unlisted hyper-parameter values are identical to\nthose of the reference model.\nd\nh\nd\/h\ndg\nAcc. (%)\n#Param (×106)\nref.\n768\n8\n96\n96\n67.28\n83.0\n(A)\n32\n67.16\n82.9\n64\n67.27\n82.9\n128\n67.17\n83.1\n(B)\n6\n128\n67.11\n83.1\n12\n64\n67.23\n82.9\n16\n48\n67.25\n82.9\n(C)\n256\n32\n66.30\n14.5\n512\n64\n66.92\n40.6\n1024\n128\n67.30\n141.6\nNext, we conduct the ablation studies to explore the effects\nof self-attention and co-attention in MUAN. By masking the\nvalues in the self-attention part (i.e., AT T and AV V ) or the\nco-attention part (i.e., AT T and AV V ) to −∞, we obtain two\ndegraded variants of MUAN. We compare the two MUAN\nvariants to its reference model in Fig. 5b with L ∈{2, 6, 10}.\nThe results shows that: 1) both the self-attention and co-\nattention in MUAN contribute to the performance of VQA; and\n2) co-attention plays a more important role than self-attention\nin MUAN, especially when the model is relatively shallow.\nFinally, we investigate MUAN-10 model performance with\ndifferent hyper-parameters for the UA block in Table I. In\nrow (A), we vary the dimensionality dg in the gating model.\nThe results suggest that the reference model results in a 0.12\npoint improvement over the worst counterpart. Further, the\nmodel sizes of these variants are almost identical, indicating\nthat the computational cost of the gating model can be more\nor less ignored. In row (B), we vary the number of parallel\nheads h with a ﬁxed output dimensionality d, keeping the\ncomputational cost constant. The results suggest that h = 8\nis the best choice for MUAN. Too few or too many heads\nreduces the quality of learned attention. In row (C), we ﬁx\nthe number of heads to h = 8 and vary the dimensionality d,\nresulting in much smaller and larger models with the model\nTABLE II: Accuracies (%) of the single-model on the test-dev\nand test-std splits of VQA-v2 to compare with the state-of-\nthe-art methods. All models use the same bottom-up attention\nvisual features [50] and are trained on the train+val+vg splits,\nwhere vg indicates the augmented training samples from\nVisual Genome [56].\nMethod\nTest-dev\nTest-std\nAll\nY\/N\nNum\nOther\nAll\nBottom-Up [51]\n65.32\n81.82\n44.21\n56.05\n65.67\nCounter [60]\n68.09\n83.14\n51.62\n58.97\n68.41\nMFH+CoAtt [35]\n68.76\n84.27\n49.56\n59.89\n-\nBAN [19]\n69.52\n85.31\n50.93\n60.26\n-\nBAN+Counter [19]\n70.04\n85.42\n54.04\n60.52\n70.35\nDFAF [44]\n70.22\n86.09\n53.32\n60.49\n70.34\nMCAN [61]\n70.63\n86.82\n53.26\n60.72\n70.90\nMUAN (ours)\n70.82\n86.77\n54.40\n60.89\n71.10\ncomplexity proportional to O(d2). From the results, we can\nsee that d is a key hyper-parameter to the performance. Too\nsmall d may restrict the model capacity, leading to inferior\nperformance. The model with d = 1024 slightly surpasses the\nreference model at the expense of much higher computational\ncomplexity and greater risk of over-ﬁtting.\nThe hyper-parameters in the reference model is a trade-off\nbetween efﬁciency and efﬁcacy. Therefore, we adopt the refer-\nence MUAN-10 model (abbreviated to MUAN for simplicity)\nin all the following experiments.\nD. Results on VQA-v2\nTaking the ablation studies into account, we compare our\nbest MUAN model to the state-of-the-art methods on VQA-v2\nin Table II. With the same bottom-up-attention visual features\n[50], MUAN signiﬁcantly outperforms current state-of-the-art\nmethods BAN [19] by 1.3 points in terms of overall accuracy\non the test-dev split. Furthermore, for the Num-type questions,\nwhich verify object counting performance, BAN+Counter [19]\nreports the best result by utilizing an elaborate object counting\nmodule [60]. In contrast, MUAN achieves slightly higher\naccuracy than BAN+Counter, and in doing so does not use the\nauxiliary bounding-box coordinates of each object [60]. This\nsuggests that MUAN can perform accurate object counting\nbased on the visual features alone. As far as we know, MUAN\nis the ﬁrst single model that achieves 71%+ accuracy on\nthe test-std split with the standard bottom-up-attention visual\nfeatures provided by [50].\nE. Results on CLEVR\nWe also conduct experiments to compare MUAN with\nexisting state-of-the-art approaches, and human performance\non CLEVR, which is a synthesized dataset for evaluating com-\npositional visual reasoning. Compared to VQA-v2, CLEVR\nrequires a model not only to focus on query-speciﬁc objects,\nbut only to reason the relations among the related objects,\nwhich is much more challenging. In the meantime, since the\nimage contents are completely synthesized by the algorithm,\nit is possible for a model to fully understand the semantic,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\nTABLE III: Overall accuracies (%) on the test split of CLEVR to compare with the state-of-the-art methods. (*) denotes use\nof extra program labels. (†) denotes use of data augmentation.\nMethod\nHuman\n[26]\nQ-type Prior\n[26]\nLSTM\n[26]\nCNN+LSTM\n[26]\nN2NMN*\n[62]\nRN†\n[63]\nPG+EE*\n[64]\nFiLM\n[65]\nMAC\n[57]\nMUAN\n(ours)\nAccuracy\n92.6\n41.8\n46.8\n52.3\n83.7\n95.5\n96.9\n97.7\n98.9\n98.7\nTABLE IV: Accuracies (%) on RefCOCO, RefCOCO+ and RefCOCOg to compare with the state-of-the-art methods. All\nmethods use the detected proposals rather than the ground-truth bounding-boxes. COCO [53] and Genome [56] denote two\ndatasets for training the proposal detectors. SSD [66], FRCN [55] and MRCN [58] denote the used detection models with\nVGG-16 [67] or ResNet-101 [38] backbones.\nMethod\nProposal Generator\nRefCOCO\nRefCOCO+\nRefCOCOg\nDataset\nDetector\nBackbone\nTestA\nTestB\nVal\nTestA\nTestB\nVal\nTest\nVal\nAttr [68]\nCOCO\nFRCN\nVGG-16\n72.0\n57.3\n-\n58.0\n46.2\n-\n-\n-\nCMN [69]\nCOCO\nFRCN\nVGG-16\n71.0\n65.8\n-\n54.3\n47.8\n-\n-\n-\nVC [70]\nCOCO\nFRCN\nVGG-16\n73.3\n67.4\n-\n58.4\n53.2\n-\n-\n-\nSpe.+Lis.+Rein.+MMI [36]\nCOCO\nSSD\nVGG-16\n73.7\n65.0\n69.5\n60.7\n48.8\n55.7\n59.6\n60.2\nSpe.+Lis.+Rein.+MMI [36]\nCOCO\nSSD\nVGG-16\n73.1\n64.9\n69.0\n60.0\n49.6\n54.9\n59.2\n59.3\nDDPN [39]\nGenome\nFRCN\nVGG-16\n76.9\n67.5\n73.4\n67.0\n50.2\n60.1\n-\n-\nDDPN [39]\nGenome\nFRCN\nResNet-101\n80.1\n72.4\n76.8\n70.5\n54.1\n64.8\n67.0\n66.7\nMAttNet [40]\nCOCO\nFRCN\nResNet-101\n80.4\n69.3\n76.4\n70.3\n56.0\n64.9\n67.0\n66.7\nMAttNet [40]\nCOCO\nMRCN\nResNet-101\n81.1\n70.0\n76.7\n71.6\n56.0\n65.3\n67.3\n66.6\nMUAN (ours)\nCOCO\nMRCN\nResNet-101\n82.8\n78.6\n81.4\n70.5\n62.9\n68.9\n71.5\n71.0\nMUAN (ours)\nGenome\nFRCN\nResNet-101\n86.5\n78.7\n82.8\n79.5\n64.3\n73.2\n74.3\n74.2\nresulting in relatively higher performance of existing state-of-\nthe-arts compared to those on VQA-v2.\nFrom the results shown in Table III, we can see that MUAN\nis at least comparable to the state-of-the-art, even if the model\nis not speciﬁcally designed for this dataset. While some prior\napproaches used extra supervisory program labels [64][62] or\naugmented dataset [63] to guide training, MUAN is able to\nlearn to infer the correct answers directly from the image and\nquestion features.\nF. Results on RefCOCO, RefCOCO+, and RefCOCOg\nWe report the comparative results on RefCOCO, Ref-\nCOCO+, and RefCOCOg in Table IV. We use the common\nevaluation criterion accuracy, which is deﬁned as the percent-\nage of predicted bounding box overlaps with the groundtruth\nof IoU > 0.5. From the results, we can see that: 1) with\nthe standard proposal features extracted from the detector\npre-trained on MSCOCO, MUAN reports a remarkable im-\nprovement over MAttNet, the state-of-the-art visual grounding\nmodel; 2) with the powerful proposal features extracted from\nthe detector pre-trained on Visual Genome, MUAN reports\n∼9% improvement over a strong baseline DDPN [39], which\nuses the same visual features. These results reveal the fact\nthat MUAN outperforms existing state-of-the-arts steadily\nregardless of the used proposal features. Compared with ex-\nisting approaches, MUAN additionally models the intra-modal\ninteractions within each modality, which provide contextual\ninformation to facilitate visual grounding performance.\nG. Qualitative Analysis\nIn Fig. 6, we show one VQA example and visualize four\nattention maps (obtained by Eq.(5)) from the 1st, 3rd, 6th\nand 9th UA blocks, respectively. Since only the feature of\nthe [ans] token is used to predict the answer, we focus on its\nrelated attention weights (i.e., the ﬁrst row of each attention\nmap). In the 1st attention map, the word ‘many’ obtains\nthe largest weight while the other words and visual objects\nare almost abandoned. This suggests that the 1st block acts\nas a question-type classiﬁer. In the 3rd attention map, the\nword ‘street’ is highlighted, which is a contextual word to\nunderstand the question. The key word ‘buses’ is highlighted\nin the 6th attention map, and the two buses (i.e., the 22th\nand 31th objects) are highlighted in the 9th attention map.\nThis visual reasoning process explains the information of the\nhighlighted words and objects is gradually aggregated into the\n[ans] feature. For the 9th UA block, we split its attention map\ninto four parts (i.e., AT T , AV T , AT V and AV V ). In AT T ,\nthe largest values reﬂect the relationships between the key\nword and its context, providing a structured and ﬁne-grained\nunderstanding of the question semantics (i.e., bus is on the\nstreet). In AT V , some words on the rows attend to the key\nobjects, suggesting that these words aggregate the information\nfrom the key objects to improve their representations. Similar\nobservations can be observed from AV V and AV T .\nIn Fig. 7, we demonstrate one visual grounding example\nand visualize the prediction and the learned uniﬁed attention.\nIn the ﬁrst image, we can see that MUAN accurately localize\nthe most relevant object proposal, and then output the reﬁned\nbounding boxes as the ﬁnal prediction. We visualize the\nlearned textual and visual attentions of the 1st, 3rd, 6th and\n9th UA blocks, respectively. By performing columnwise max-\npooling over the uniﬁed attention map, we obtain the attention\nweights for the words and objects. For better visualization\neffect, we only visualize three representative objects with the\nlargest attention weights. From the results, we can see that: 1)\nthe keywords are highlighted only in the 1st block, indicating\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\nQ: How many buses are on \nthe street?\nA: 2\nP: 2\n#1\n#3\n#6\n#9\n31\n22\n12\n14\n6\n17\n4\nFig. 6: Visualizations of the learned uniﬁed attention maps (Eq.(5)) for VQA. The attention maps come from the 1st, 3rd, 6th\nand 9th UA block, respectively. The index within [0-32] on the axes of the attention maps corresponds to the object in the\nimage (33 objects in total). For better visualization effect, we highlight the objects in the image that are related to the answer.\nFurthermore, we split the last attention map into four parts (i.e., AT T , AV T , AT V and AV V ) to carry out detailed analysis.\nProp.\nPred.\nG.T.\nQ: Green hat with baby\nGreen hat with baby\nGreen hat with baby\nGreen hat with baby\nGreen hat with baby\n1st\n1st\n1st\n1st\n1st\nPrediction\n#1\n#3\n#6\n#9\nFig. 7: Visualizations of the prediction and the learned visual attention for visual grounding. The groundtruth (red), top-ranked\nproposal (blue) and reﬁned prediction (yellow) are shown in the ﬁrst image. Next four images illustrate the learned visual\nattentions from the 1st, 3rd, 6th and 9th UA blocks, respectively. The visual attention is represented by three representative\nobjects with the largest attention values. The brightness of objects and darkness of words represent their importance in the\nattention weights.\nthat this information has been successfully transferred to the\nattended visual features in the following blocks; and 2) the\nlearned visual attention in the 1st block is meaningless. After\nreceiving the textual information, the visual attention tends to\nfocus on the contextual objects in the 3rd and 6th blocks (i.e.,\nthe hat and the baby), and ﬁnally focuses on the correct target\nobject (i.e., the woman) in the 9th block.\nVI. CONCLUSION AND FUTURE WORK\nIn this work, we present a novel uniﬁed attention model that\ncaptures intra- and inter-modal interactions simultaneously for\nmultimodal data. By stacking such uniﬁed attention blocks\nin depth, we obtain a Multimodal Uniﬁed Attention Network\n(MUAN), that is suitable for both VQA and visual ground-\ning tasks. Our approach is simple and highly effective. We\nverify the effectiveness of MUAN on ﬁve datasets, and the\nexperimental results show that our approach achieves top level\nperformance on all the benchmarks without using any dataset\nspeciﬁc model tuning.\nSince MUAN is a general framework that can be applied\nto many multimodal learning tasks, there remains signiﬁcant\nroom for improvement, for example by introducing multitask\nlearning with sharing the same backbone model or introducing\nweakly-supervised model pre-training with large-scale multi-\nmodal data in the wild.\nREFERENCES\n[1] C. Zheng, L. Pan, and P. Wu, “Multimodal deep network embedding\nwith integrated structure and attribute information,” IEEE transactions\non neural networks and learning systems, 2019.\n[2] Z. Yu, F. Wu, Y. Yang, Q. Tian, J. Luo, and Y. Zhuang, “Discrim-\ninative coupled dictionary hashing for fast cross-media retrieval,” in\nInternational ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR), 2014, pp. 395–404.\n[3] E. Yang, C. Deng, C. Li, W. Liu, J. Li, and D. Tao, “Shared predictive\ncross-modal deep quantization,” IEEE transactions on neural networks\nand learning systems, vol. 29, no. 11, pp. 5292–5303, 2018.\n[4] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov,\nR. S. Zemel, and Y. Bengio, “Show, attend and tell: Neural image\ncaption generation with visual attention.” in International Conference\non Machine Learning (ICML), vol. 14, 2015, pp. 77–81.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\n[5] J. Song, Y. Guo, L. Gao, X. Li, A. Hanjalic, and H. T. Shen, “From\ndeterministic to generative: multi-modal stochastic rnns for video cap-\ntioning,” IEEE transactions on neural networks and learning systems,\n2018.\n[6] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,\nand D. Parikh, “Vqa: Visual question answering,” in IEEE International\nConference on Computer Vision (ICCV), 2015, pp. 2425–2433.\n[7] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele, “Ground-\ning of textual phrases in images by reconstruction,” in European\nConference on Computer Vision (ECCV), 2016, pp. 817–834.\n[8] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\n2014.\n[9] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual\nattention,” in NIPS, 2014, pp. 2204–2212.\n[10] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,\n“Draw: A recurrent neural network for image generation,” in Interna-\ntional Conference on Machine Learning (ICML), 2015, pp. 1462–1471.\n[11] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to\nscale: Scale-aware semantic image segmentation,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2016, pp. 3640–\n3649.\n[12] M.-T.\nLuong,\nH.\nPham,\nand\nC.\nD.\nManning,\n“Effective\nap-\nproaches to attention-based neural machine translation,” arXiv preprint\narXiv:1508.04025, 2015.\n[13] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\ndependency\nparsing,”\nin\nInternational\nConference\non\nLearning\nRepresentations (ICLR), 2017. [Online]. Available: https:\/\/nlp.stanford.\nedu\/pubs\/dozat2017deep.pdf\n[14] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for\nabstractive sentence summarization,” arXiv preprint arXiv:1509.00685,\n2015.\n[15] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention net-\nworks for image question answering,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016, pp. 21–29.\n[16] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and\nM. Rohrbach, “Multimodal compact bilinear pooling for visual question\nanswering and visual grounding,” Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2016.\n[17] J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image\nco-attention for visual question answering,” in NIPS, 2016, pp. 289–297.\n[18] Z. Yu, J. Yu, J. Fan, and D. Tao, “Multi-modal factorized bilinear\npooling with co-attention learning for visual question answering,” IEEE\nInternational Conference on Computer Vision (ICCV), pp. 1839–1848,\n2017.\n[19] J.-H. Kim, J. Jun, and B.-T. Zhang, “Bilinear attention networks,” NIPS,\n2018.\n[20] D.-K. Nguyen and T. Okatani, “Improved fusion of visual and language\nrepresentations by dense symmetric co-attention for visual question an-\nswering,” IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, 2017, pp. 6000–6010.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[23] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in IEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018, pp. 7794–7803.\n[24] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks for\nobject detection,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n[25] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making\nthe v in vqa matter: Elevating the role of image understanding in visual\nquestion answering,” IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[26] J.\nJohnson,\nB.\nHariharan,\nL.\nvan\nder\nMaaten,\nL.\nFei-Fei,\nC. Lawrence Zitnick, and R. Girshick, “Clevr: A diagnostic dataset\nfor compositional language and elementary visual reasoning,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR),\n2017, pp. 2901–2910.\n[27] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame:\nReferring to objects in photographs of natural scenes,” in Conference\non Empirical Methods in Natural Language Processing (EMNLP), 2014,\npp. 787–798.\n[28] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy,\n“Generation and comprehension of unambiguous object descriptions,” in\nIEEE International Conference on Computer Vision (ICCV), 2016, pp.\n11–20.\n[29] B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus, “Simple base-\nline for visual question answering,” arXiv preprint arXiv:1512.02167,\n2015.\n[30] J.-H. Kim, K. W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang,\n“Hadamard Product for Low-rank Bilinear Pooling,” in International\nConference on Learning Representation (ICLR), 2017.\n[31] H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, “Mutan: Multi-\nmodal tucker fusion for visual question answering,” in IEEE Interna-\ntional Conference on Computer Vision (ICCV), 2017.\n[32] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia, “Abc-\ncnn: An attention based convolutional neural network for visual question\nanswering,” arXiv preprint arXiv:1511.05960, 2015.\n[33] I. Ilievski, S. Yan, and J. Feng, “A focused dynamic attention model for\nvisual question answering,” arXiv preprint arXiv:1604.01485, 2016.\n[34] K. J. Shih, S. Singh, and D. Hoiem, “Where to look: Focus regions for\nvisual question answering,” in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016, pp. 4613–4621.\n[35] Z. Yu, J. Yu, C. Xiang, J. Fan, and D. Tao, “Beyond bilinear: Generalized\nmulti-modal factorized high-order pooling for visual question answer-\ning,” IEEE Transactions on Neural Networks and Learning Systems,\n2018.\n[36] L. Yu, H. Tan, M. Bansal, and T. L. Berg, “A joint speaker-listener-\nreinforcer model for referring expressions,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2017, pp.\n7282–7290.\n[37] C. L. Zitnick and P. Doll´ar, “Edge boxes: Locating object proposals from\nedges,” in European Conference on Computer Vision (ECCV), 2014, pp.\n391–405.\n[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016.\n[39] Z. Yu, J. Yu, C. Xiang, Z. Zhao, Q. Tian, and D. Tao, “Rethinking\ndiversiﬁed and discriminative proposal generation for visual grounding,”\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI), 2018.\n[40] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.\nBerg, “Mattnet: Modular attention network for referring expression\ncomprehension,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2018, pp. 1307–1315.\n[41] B. Zhuang, Q. Wu, C. Shen, I. Reid, and A. van den Hengel, “Parallel\nattention: A uniﬁed framework for visual object discovery through\ndialogs and queries,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018, pp. 4252–4261.\n[42] C. Deng, Q. Wu, Q. Wu, F. Hu, F. Lyu, and M. Tan, “Visual grounding\nvia accumulated attention,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018, pp. 7746–7755.\n[43] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, “Be-\nyond rnns: Positional self-attention with co-attention for video question\nanswering,” in AAAI, 2019.\n[44] G. Peng, H. Li, H. You, Z. Jiang, P. Lu, S. Hoi, and X. Wang, “Dynamic\nfusion with intra-and inter-modality attention ﬂow for visual question\nanswering,” arXiv preprint arXiv:1812.05252, 2018.\n[45] A.\nRadford,\nK.\nNarasimhan,\nT.\nSalimans,\nand\nI.\nSutskever,\n“Improving\nlanguage\nunderstanding\nby\ngenerative\npre-training,”\nURL\nhttps:\/\/s3-us-west-2.\namazonaws.\ncom\/openai-assets\/research-\ncovers\/languageunsupervised\/language understanding paper. pdf, 2018.\n[46] Y. Li, N. Wang, J. Liu, and X. Hou, “Factorized bilinear models for\nimage recognition,” IEEE International Conference on Computer Vision\n(ICCV), 2017.\n[47] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[48] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for\nword representation.” in Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), vol. 14, 2014, pp. 1532–1543.\n[49] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[50] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\nL. Zhang, “Bottom-up and top-down attention for image captioning and\nvisual question answering,” IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018.\n[51] D. Teney, P. Anderson, X. He, and A. v. d. Hengel, “Tips and tricks for\nvisual question answering: Learnings from the 2017 challenge,” arXiv\npreprint arXiv:1708.02711, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\n[52] R. Girshick, “Fast r-cnn,” in IEEE International Conference on Com-\nputer Vision (ICCV), 2015, pp. 1440–1448.\n[53] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European Conference on Computer Vision (ECCV), 2014,\npp. 740–755.\n[54] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[55] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\ntime object detection with region proposal networks,” in NIPS, 2015,\npp. 91–99.\n[56] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY. Kalantidis, L.-J. Li, D. A. Shamma et al., “Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations,”\narXiv preprint arXiv:1602.07332, 2016.\n[57] D. A. Hudson and C. D. Manning, “Compositional attention networks\nfor machine reasoning,” arXiv preprint arXiv:1803.03067, 2018.\n[58] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2017,\npp. 2961–2969.\n[59] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling\ncontext in referring expressions,” in European Conference on Computer\nVision (ECCV).\nSpringer, 2016, pp. 69–85.\n[60] Y. Zhang, J. Hare, and A. Pr¨ugel-Bennett, “Learning to count objects in\nnatural images for visual question answering,” International Conference\non Learning Representation (ICLR), 2018.\n[61] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, “Deep modular co-\nattention networks for visual question answering,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2019, pp. 6281–\n6290.\n[62] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko, “Learning to\nreason: End-to-end module networks for visual question answering,” in\nProceedings of the IEEE International Conference on Computer Vision\n(ICCV), 2017.\n[63] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu,\nP. Battaglia, and T. Lillicrap, “A simple neural network module for\nrelational reasoning,” in Advances in neural information processing\nsystems, 2017, pp. 4967–4976.\n[64] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, L. Fei-Fei,\nC. Lawrence Zitnick, and R. Girshick, “Inferring and executing programs\nfor visual reasoning,” in IEEE International Conference on Computer\nVision (ICCV), 2017, pp. 2989–2998.\n[65] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, “Film:\nVisual reasoning with a general conditioning layer,” in Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, 2018.\n[66] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.\nBerg, “Ssd: Single shot multibox detector,” in European Conference on\nComputer Vision (ECCV).\nSpringer, 2016, pp. 21–37.\n[67] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[68] J. Liu, L. Wang, and M.-H. Yang, “Referring expression generation and\ncomprehension via attributes,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 4856–4864.\n[69] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko, “Modeling\nrelationships in referential expressions with compositional modular\nnetworks,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017, pp. 1115–1124.\n[70] H. Zhang, Y. Niu, and S.-F. Chang, “Grounding referring expressions in\nimages by variational context,” in The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2018.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Multimodal Unified Attention Networks for Vision-and-Language Interactions.pdf"}
{"title":"UniMuMo: Unified Text, Music and Motion Generation","authors":"Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, Chuang Gan","summary":"We introduce UniMuMo, a unified multimodal model capable of taking arbitrary\ntext, music, and motion data as input conditions to generate outputs across all\nthree modalities. To address the lack of time-synchronized data, we align\nunpaired music and motion data based on rhythmic patterns to leverage existing\nlarge-scale music-only and motion-only datasets. By converting music, motion,\nand text into token-based representation, our model bridges these modalities\nthrough a unified encoder-decoder transformer architecture. To support multiple\ngeneration tasks within a single framework, we introduce several architectural\nimprovements. We propose encoding motion with a music codebook, mapping motion\ninto the same feature space as music. We introduce a music-motion parallel\ngeneration scheme that unifies all music and motion generation tasks into a\nsingle transformer decoder architecture with a single training task of\nmusic-motion joint generation. Moreover, the model is designed by fine-tuning\nexisting pre-trained single-modality models, significantly reducing\ncomputational demands. Extensive experiments demonstrate that UniMuMo achieves\ncompetitive results on all unidirectional generation benchmarks across music,\nmotion, and text modalities. Quantitative results are available in the\n\\href{https:\/\/hanyangclarence.github.io\/unimumo_demo\/}{project page}.","url":"http:\/\/arxiv.org\/abs\/2410.04534v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.04534v1","published":1728230645000,"comment":null,"pdf_text":"UniMuMo: Unified Text, Music and Motion Generation\nHan Yang1\nKun Su2\nYutong Zhang3\nJiaben Chen4\nKaizhi Qian5\nGaowen Liu6\nChuang Gan4,5\n1The Chinese University of Hong Kong\n2University of Washington\n3The University of British Columbia\n4UMass Amherst\n5MIT-IBM Watson AI Lab\n6Cisco Research\n{email, addresses}@inst.edu\nMusic\nMotion\nAligned\nMusic & Motion\nMotion Aligned\nwith Music\nMusic Aligned\nwith Motion\nText\nDescription\n“The audio is a rap song  with keyboard \naccompaniment. This is a la style \nhip-hop style dance.”\nMusic & Motion\nCaption\n“The music is a rock song with a strong\nemphasis on drum and guitar. The dance is that\na man wiggles his shoulders.”\nUniMuMo\nFigure 1. UniMuMo is able to perform generation tasks on any combination of music, motion, and text. The tasks shown in the figure include\ntext-to-aligned-music-motion, music-to-motion, motion-to-music, music-captioning, and motion-captioning.\nAbstract\nWe introduce UniMuMo, a unified multimodal model ca-\npable of taking arbitrary text, music, and motion data as\ninput conditions to generate outputs across all three modal-\nities. To address the lack of time-synchronized data, we\nalign unpaired music and motion data based on rhythmic\npatterns to leverage existing large-scale music-only and\nmotion-only datasets. By converting music, motion, and\ntext into token-based representation, our model bridges\nthese modalities through a unified encoder-decoder trans-\nformer architecture. To support multiple generation tasks\nwithin a single framework, we introduce several architec-\ntural improvements. We propose encoding motion with a\nmusic codebook, mapping motion into the same feature\nspace as music. We introduce a music-motion parallel gen-\neration scheme that unifies all music and motion genera-\ntion tasks into a single transformer decoder architecture\nwith a single training task of music-motion joint genera-\ntion. Moreover, the model is designed by fine-tuning exist-\ning pre-trained single-modality models, significantly reduc-\ning computational demands. Extensive experiments demon-\nstrate that UniMuMo achieves competitive results on all\nunidirectional generation benchmarks across music, motion,\nand text modalities. Quantitative results are available in\nhttps:\/\/hanyangclarence.github.io\/unimumo demo\/.\n1. Introduction\nMusic and body movements are synchronized and insepa-\nrable. The beat and metrical structures in rhythm encour-\nage the spontaneous coordination of body motion with\nmusic [29], activating the motor-related areas of human\nbrains [26]. Dance particularly exemplifies this connection\nthrough choreography that aligns with the music’s rhythm,\nmelody and emotion. Meanwhile, even though most people\nare not professional musicians or dancers, they often inter-\npret music and dance using simple, natural language. This\ndescriptive text serves as a vital bridge between understand-\nable ideas and abstract concepts in music and motion.\nThe synergy between music, motion, and text provides\na natural motivation to create a model capable of under-\nstanding and creating contents across all these modalities.\nMoreover, building a framework that can flexibly generate\nmusic, motion, and text in arbitrary combinations is crucial\nfor real-world applications, even though existing models\nalready achieve impressive results in unidirectional genera-\ntion tasks such as text-to-music [7], music-to-motion [50],\nmotion-to-music [55] and motion-to-text [24]. In the real\nworld, there is a demand for diverse generative abilities, and\n1\narXiv:2410.04534v1  [cs.SD]  6 Oct 2024\nmore complex generation tasks may be necessary, such as\ncreating dance sequences based on both music and textual de-\nscriptions. Training individual models for each unique com-\nbination, although potentially yielding better output quality,\nwould significantly increase training costs, deployment ef-\nforts and storage requirements. Thus, a unified model that\nsupports all combinations of conditioning and generation\ntasks, rather than a collection of separate models or training\nadapters to incorporate individual models, offers a more cost-\neffective solution. To this end, we introduce a novel task of\ndynamically generating music, motion, and text in a multi-\ntude of combinations unifiedly. As demonstrated in Fig. 1,\nthis task is designed to handle diverse generative scenarios,\nranging from text-to-music, text-to-motion, to more complex\ncombinations like text-to-music-plus-motion or music-plus-\ntext-to-motion.\nHowever, the task could be challenging, especially in two\naspects: i) the lack of comprehensive datasets that include\nall three modalities - music, motion, and text - limits the\ndevelopment of a general and unified model. While there are\nindividual datasets for music-only [44], motion-only [37],\nmusic to motion [32] and text to motion [20], a holistic\nand large-scale dataset that encompasses all three modalities\nstill remains absent; ii) designing a unified architecture that\nsupports both the conditioning and generation of all three\nmodalities is challenging, mainly due to the significant dif-\nferences between the neural representations for the three\nmodalities and the multiplicity of desired generation tasks.\nTo address the first challenge of lacking paired data, we\npropose to align unpaired music and motion sequences based\non their rhythmic patterns. Specifically, we extract both mu-\nsic beats and motion visual beats, then employ dynamic time\nwarping to find the alignment and warp the motion sequence\nto adjust the motion visual beats to match the music beats.\nWe found that such augmentation is accurate and efficient.\nWith the augmented synchronized music-motion data, we\ncan utilize existing music and motion datasets to train our\nunified generative model. Additionally, we construct text\ndescriptions from music and motion metadata using a mix-\nture of template filling, large language model generation\nand music-based language model generation, striking a bal-\nance between diversity, language fluency and description\naccuracy.\nTo overcome the second challenge, we propose a novel\nframework, UniMuMo, to unify the generation of different\nmodalities. Our pipeline consists of three main stages: a\nmusic-motion joint tokenizer that encodes music and mo-\ntion sequences into discrete representations within the same\nspace, a music-motion transformer-decoder model trained\non the task of music-motion joint generation, and a music-\nmotion captioner that generates text descriptions from music\nand motion features. In the first stage, we bridge the modality\ngap between music and motion by mapping motion into the\nmusic feature space. Specifically, instead of using separate\nVector-Quantized Variational Autoencoders (VQ-VAE) to\nquantize music and motion sequences, we encode motion\nwith the codebook of a pre-trained music VQ-VAE, namely\nEncodec [10]. This design facilitates the unification of mu-\nsic and motion within the same generative framework in\nthe subsequent stage. In the second stage, we train a uni-\nfied music and motion generative model with a novel task\nof music-motion joint generation from text conditions. To\nenable the mutual conditioning of music and motion, and\nunlock the music-to-motion and motion-to-music generation\ncapabilities, we introduce a novel music-motion parallel gen-\neration scheme, where we perform two mutually conditioned\nstreams of autoregressive generation of aligned music and\nmotion simultaneously. With the reuse of Encodec and joint\nencoding of motion in the previous stage, the current stage\ncan be effectively achieved by fine-tuning the pre-trained\ntext-to-music model associated with Encodec, namely Mu-\nsicGen [7], equipping it with additional motion conditioning\nand generation capabilities while maintaining its music gen-\neration capabilities. In the third stage, we fine-tune a T5\ndecoder for music and motion captioning tasks, using the\nfeatures extracted by the music-motion decoder trained in\nstage 2. To transform the decoder into an effective feature\nextractor, we replace its causal self-attention layers with\ntrainable full self-attention layers, and fine-tune them to-\ngether with the T5 decoder on music and motion captioning\ntasks. Extensive experiments demonstrate that UniMuMo\nachieves competitive performance across all unidirectional\ngeneration tasks in music, motion, and text when compared\nwith existing state-of-the-art models, demonstrating the ef-\nfectiveness and versatility of our approach.\nOur work offers significant advancements in multimodal\ngenerative research, summarized as follows:\n• To the best of our knowledge, this is the first unified frame-\nwork capable of arbitrarily generating content across mu-\nsic, motion, and text.\n• To address the shortage of paired multimodal data, we aug-\nment and enrich existing large-scale datasets with music-\nmotion data alignment and text augmentations.\n• We propose a novel joint codebook for encoding music\nand motion sequences, along with a music-motion parallel\ngeneration scheme, facilitating multiple generation tasks\nwithin a single architecture.\n• Our framework achieves results comparable to SOTAs\nacross all generation tasks in music, motion, and text.\n2. Related Work\nText to Music. Text-conditioned music generation has been\nwidely studied in recent years. There are two main branches:\ndiffusion-based and transformer-based. For diffusion-based\nmodels, Riffusion [15] uses a latent text-to-image diffu-\nsion model to generate spectrograms, which are then con-\n2\nverted into audio clips; Mousai [45] proposes training a\ndiffusion model in the latent space of a diffusion autoen-\ncoder; Noise2Music [22] introduces a cascade of diffusion\nmodels that first generates the audio in a coarse form and\nthen progressively refine it. AudioLDM [34] proposes to\ntrain a latent diffusion model using CLAP [51] embeddings,\na language-audio joint representation, for text conditioning.\nFor transformer-based models, MusicLM [2] proposes to en-\ncode music into high-level ”semantic tokens” and low-level\n”acoustic tokens”, and use a cascade of transformer decoders\nto generate the two levels stage by stage. MusicGen [7]\nleverages a single-stage transformer decoder to model the\nhierarchical music tokens directly.\nMusic to Text. Several models have been proposed for au-\ndio captioning. WAC [25] proposes to transfer a pre-trained\nspeech-to-text Whisper model to the music captioning task.\nLTU [18] takes the concatenated music embeddings and\ntext embeddings as input to a large language model and di-\nrectly trains caption generation using language modeling\nobjectives. LP-MusicCaps [12] uses a transformer encoder-\ndecoder structure, where the music spectrogram is first en-\ncoded by the encoder and then cross-attended by the decoder\nfor text generation. MU-LLaMA [35] leverages a frozen\nLLaMA [49] and fine-tunes a Music Understanding Adapter\nto fuse music features into the LLaMA model.\nMusic to Motion. Most of the works on music-conditioned\ndance generation are based on transformers. Several ap-\nproaches [14, 31, 41] adopt similar structures that first use\na music transformer encoder and a motion transformer en-\ncoder to encode music and initial motion into representations\nseparately, and then employ a transformer decoder for cross-\nmodal fusion and motion generation. Bailando [46] proposes\nto train a transformer on motion features encoded by a chore-\nographic memory module, which is the codebook of a motion\nVQ-VAE. Besides autoregressive transformers, EDGE [50]\nadopts a transformer-based diffusion model capable of both\ndance generation and editing.\nMotion to Music. Most of the relevant works focus on gen-\nerating corresponding music from video input. Foley Mu-\nsic [16] focuses on generating music for videos of people\nplaying instruments, and uses Musical Instrument Digital\nInterface (MIDI) to bridge the gap between body key points\nand the final music. Similarly, RhythmicNet [47] extends\nthe scenarios to arbitrary motion videos by first estimating\nvisual rhythm and conditionally generating drum and piano\nmusic. Dance2Music [1] encodes a dance similarity matrix\nwith CNN and predicts the next note with an LSTM autore-\ngressively. CDCD [56] proposes a single-stage method that\nuses a discrete latent diffusion model to generate music spec-\ntrograms conditioned on video features. D2M-GAN [55]\nproposes a GAN-based model to generate the music tokens\nbased on video and pose features.\nText to Motion. Text-to-motion approaches can be mainly\ncategorized into transformer-based and diffusion-based.\nTransformer-based models mainly work with motion tokens\ngenerated by a motion VQ-VAE. TM2T [21] regards text-\nto-motion and motion-to-text as machine translation tasks\nand trains a transformer encoder-decoder to perform bidirec-\ntional translation uniformly. T2M-GPT [53] adopts a GPT-\nbased model to directly generate motion tokens from text\nconditions. MotionGPT [24] proposes incorporating motion\ntokens into natural language tokens and performing language\nmodeling on both text and motion. For the diffusion-based\nmodel, MotionDiffuse [54] and MDM [48] directly adopt\ndiffusion model while MLD [5] adopt latent diffusion on\nmotion generation.\nMotion to Text. Motion-to-text generation is usually re-\ngarded as a sequence-to-sequence translation task. Various\nmethods adopts different sequence-to-sequence models, such\nas RNN [40], recurrent autoencoder [52], sequence genera-\ntive adversarial nets [19] and transformer [21, 24].\n3. Text-Music-Motion Aligned Data Generation\nTo model arbitrary generation across music, motion, and text,\nwe propose to expand existing music and motion datasets\nby aligning motion with music and synthesizing textual de-\nscriptions. The data generation pipeline includes four major\nsteps: 1) music beat detection, 2) visual beat detection, 3)\nmusic-motion alignment, and 4) text description synthesis.\nMusic Beat Detection. We estimate music beats from a\nmusic waveform Y ∈RTw, where Tw represents the number\nof samples, using a Bidirectional-LSTM-based model from\n[6]. This model performs beat tracking on extracted drum\nfeatures and non-drum features separately, then aggregates\nthe results with a learnable fuser. We manually evaluate\nthe accuracy of this beat tracking model and find that it\nperforms well in most test cases, outperforming the beat\ntracking methods in the Librosa API [38]. The resulting\nmusic beats are represented as a binary sequence Bm ∈RTw,\nwhere each frame is marked as ‘beat’ or ‘non-beat.’\nVisual Beats Detection. Given a 3D motion sequence\nM ∈RTm×J×3 where Tm represents the number of frames,\nJ the number of joints, and the last dimension indicates\nx, y, z coordinates, we obtain visual beats in three steps. In\nthe first stage, we calculate the motion directogram [8], a 2D\nmatrix that factors motion into different motion angles, simi-\nlar to how an audio spectrogram factors sound amplitude into\ndifferent frequencies. Specifically, we first compute the first-\norder difference of the motion sequence ∆Mt = Mt−Mt−1.\nBased on its motion angle, we assign the motion magnitude\nof every joint into one of the bins in 2π\/Nbins. The motion di-\nrectogram Md(t, θ) is obtained by summing the motion mag-\nnitudes of each bin: Md(t, θ) = P\nj ∆Mt(j)1θ(∠Mt(j)),\nwhere 1θ(ϕ) = 1 if |θ −ϕ| ≤2π\/Nbins else 0. In the second\nstage, we convert the motion directogram to the kinematic\noffset Mk, which represents the motion changes, similar to\n3\nFrozen\nEncodec\nTrainable Motion \nEncoder-Decoder\nRVQ\nReconstruction\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\n0\n1\nM-2\nM-1\n...\nx K\nShared  Codebook\nStage 1: Music-Motion Joint Tokenization\nStage 2: Music-Motion Parallel Generation\nThe module is not used in this stage\nCross-modal\nCausal-attn \nMask\n“The audio is a rap \nsong  with keyboard \naccompaniment. \nThis is a la style hip-\nhop style dance.”\nT5\nEncoder\nStart\n1\n1\n2\n2\n3\n3\nStart\n1\n1\n2\n2\n3\n4\n3\n4\nx N\nCross-Attention\nCross-modal\nCausal\nAttention\nFull\nSelf\nAttention\nFull\nSelf\nAttention\nMusic\nFFN\nMotion\nFFN\nFull Self-\nattn Mask\nThe module is not used in this stage\nNull\nT5 \nEncoder\nStart\n1\n1\n2\n2\n3\n3\nStart\nN x\nCross-Attention\nCross-modal\nCausal\nAttention\nFull\nSelf\nAttention\nFull\nSelf\nAttention\nMusic\nFFN\nMotion\nFFN\nStage 3: Music-Motion Conditioned \nCaption Generation\nT5\nDecoder\nGenerated Text\nFigure 2. Overview: The training of UniMuMo consists of three stages: In stage 1, we train a motion RVQ-VAE using the frozen codebook\nfrom a pre-trained music RVQ-VAE to encode motion into the same space as music. In stage 2, we fine-tune a pre-trained music transformer\ndecoder model on the text-to-music-motion task using the music-motion parallel generation scheme. In stage 3, we fine-tune a T5 decoder\nfor music-motion captioning using the previous music-motion decoder as a feature extractor.\nthe onset envelope in an audio spectrogram. We first obtain\nmotion flux Mf, which represents the deceleration in various\ndirections, by computing the negative first-order difference\nof the directogram ∆Md. We then average each frame of\nMf and filter the top 1% peaks to obtain kinematic offset\nMk. In the last stage, we use dynamic programming to com-\npute the visual beats by designing an objective function that\nselects strong visual changes from kinematic offsets and en-\ncourages equal-spacing beats. More details can be found in\nAppendix A. The final visual beats are also represented as a\nbinary sequence Bv ∈RTm, where each frame is marked as\n‘beat’ or ‘non-beat’.\nMusic-Motion Alignment. We apply dynamic time warping\nto determine the optimal matching between music beats Bm\nand visual beats Bv, finding the alignment even though the\nduration of these two binary sequences could be different. Fi-\nnally, we warp motion sequences by interpolating according\nto the warping curve to obtain aligned music-motion pairs.\nThe reason for warping motion to match music, rather than\nthe reverse, is that music beats tend to be steady, so warp-\ning music could result in perceptually unacceptable changes.\nMore details can be found in Appendix B.\nText Description Synthesis. To compensate for the absence\nof text descriptions in our used datasets, we employ two\nmethods for captions synthesis: (1) using Music Understand-\ning Language Model to generate caption directly from audio;\nand (2) using Large Language Model to synthesize captions\nfrom metadata (genre, tempo, etc.), striking a balance be-\ntween musical accuracy and diversity. Examples and more\ndetails are shown in Appendix C.\n4. UniMuMo Framework\nUniMuMo consists of three training stages to enable arbi-\ntrary generation between music, motion, and text. In stage\n1, we encode aligned music and motion data into discrete\ntokens. To efficiently bridge the gap between the two modal-\nities, we propose to use a frozen pre-trained audio tokenizer\nEncodec [10] and train a motion tokenizer that reuses the\nsame residual codebooks of the audio tokenizer. In stage\n2, we fine-tune a state-of-the-art text-to-music transformer\ndecoder [7] by conducting the task of generating music and\nmotion tokens simultaneously with music and motion text\ndescriptions. At the inference stage, we can perform paral-\nlel generation to unlock applications of music and motion\ngeneration. In stage 3, we treat the pre-trained music-motion\ndecoder model in stage two as a feature extractor and fine-\ntune a T5 decoder on language modeling task for music and\nmotion captioning. An overview of the UniMuMo frame-\nwork is shown in Figure 2.\n4.1. Stage 1. Music and Motion Joint Tokenization\nWhile existing tokenization approaches can faithfully recon-\nstruct the music or motion individually, the correlations be-\ntween the two modalities become intricate in distinct spaces.\nTherefore, directly applying them in the unified generation\nframework poses challenges. Besides, a music tokenizer usu-\nally requires more training resources and time to achieve\nhigh-quality reconstruction than a motion tokenizer. Inspired\nby these facts, we introduce an efficient and effective way to\nencode music and motion into a joint latent space. We pro-\npose using a pre-trained audio tokenizer, Encodec [10], and\ntraining a new motion encoder-decoder. The motion encoder\nencodes the motion into the same embedding space as the\n4\nmusic and reuses the frozen music Residual Vector Quantiz-\ners (RVQ) to discretize the motion into tokens. From these\ntokens, the motion decoder can decode to reconstruct the\nmotion. Given the higher complexity and richer information\nin music compared to motion, the learned music codebook\nis theoretically capable of encoding motion.\nSpecifically, given a waveform Y ∈RT ·fw with T the au-\ndio duration and fw the sample rate, Encodec first encodes\nit into a continuous tensor of Xmusic ∈Rd×T ·fr, where\nfr ≪fw is the frame rate of the residual codebook and d is\nthe dimension of codebook entries. Xmusic is then quantized\nby the RVQ into music tokens Qmusic ∈{1, . . . , M}K×T ·fr,\nwhere K is the number of RVQ and M is the number\nof codebook entries. For an aligned motion sequence of\nthe same duration M ∈Rdm×T ·fm with frame rate fm\nand feature dimension dm, our motion encoder encodes it\ninto Xmotion ∈Rd×T ·fr, the same shape as Xmusic, which\nis then tokenized by the same RVQ into motion tokens\nQmotion ∈{1, . . . , M}K×T ·fr. The motion decoder decodes\nthe motion feature after RVQ, resulting in ˆ\nM. The motion\nencoder-decoder is trained by minimizing the motion recon-\nstruction loss together with a commitment loss Lcommit from\nthe codebook:\nLtotal =\n1\n|D|\nX\nM∈D\n(∥M −ˆ\nM∥2 + λLcommit)\n(1)\nwhere D is the motion dataset and λ controls the strength of\nthe commitment loss. Empirically, λ is set to 0.02.\nWith this design, the music-motion joint tokenization can\neffectively learn multimodal correlations by mapping mo-\ntion features into the same space as music, without the need\nto train another computationally heavy music autoencoder.\nMoreover, it enables direct use the text-to-music model as-\nsociated with Encodec as an initialization for the following\nmusic-motion decoder model, significantly reducing training\ncosts and enhancing the performance. Experimentally, such\nfeature alignment is crucial to learning the joint generation\nof music and motion within a single transformer model.\n4.2. Stage 2. Music and Motion Generation from\nText\nIn this stage, we modify and fine-tune an existing state-of-the-\nart text-to-music model with the music and motion tokens\nextracted from Stage 1, enabling it to handle all tasks re-\nlated to music and motion generation, such as text-to-music-\nmotion and motion-to-music. In particular, we employ Mu-\nsicGen [7], an open-source, single-stage transformer decoder\nmodel that can generate multi-level music tokens with a spe-\ncific codebook interleaving pattern. Following their practice,\nwe apply the delay pattern for both music and motion tokens,\nutilize a T5 encoder for encoding text descriptions, and adopt\ncross-attention to incorporate text conditioning features into\nthe transformer decoder.\nTo enable the autoregressive generation of music and mo-\ntion within a unified framework, we propose training on the\ntask of music-motion joint generation, together with a novel\nparallel generation scheme, where two streams (i.e., music\nand motion) of predict-next-token generation are conducted\nsimultaneously, with each stream conditioned on each other.\nSpecifically, given the music tokens Qmusic and motion to-\nkens Qmotion with the same shape K × S where S = T · fr\nis the sequence length, we first transform them with delay\npattern [7] into Q′\nmusic and Q′\nmotion respectively, resulting\nshape K × S′, where S′ = S + K −1. We then concatenate\nthem in time dimension into Qinput of the shape K × 2S′\nas the input to the transformer decoder. The model’s output\nis transformed back to the normal pattern for loss calcula-\ntion. Training on music-motion joint generation, we adopt\nthe predict-next-token objectives for both music and motion\ntokens in each forward pass:\nL = −\n1\n|D|\nX\nQ∈D\n(\nµ ·\nS\nX\nt=1\nlog P\nh\nQmusic\nt\n|Qmusic\n<t , Qmotion\n<t\ni\n+(1 −µ) ·\nS\nX\nt=1\nlog P\nh\nQmotion\nt\n|Qmusic\n<t , Qmotion\n<t\ni)\n(2)\nwhere µ balances between music loss and motion loss, and P\ndenotes predict-next-token probability of the model. Empiri-\ncally, µ is set to 0.85. To enable the parallel autoregressive\ngeneration, we apply a cross-modal causal attention mask, as\nshown in Stage 2 of Figure 2. The causal attention mask is of\nshape 2S′ × 2S′, each quarter of which is an S′ × S′ lower\ntriangular matrix, allowing music and motion tokens to have\nboth cross-modal and uni-modal causal attention. A further\nillustration of the strategy can be found in Appendix D.\nWith the above construction, the model can perform par-\nallel sampling during inference, enabling the prediction of\nthe next token for both music and motion concurrently:\nˆQmusic\nt\n= argmax\ni∈M\nP[Qmusic\nt,i\n| ˆQmusic\n<t , ˆQmotion\n<t\n]\n(3)\nˆQmotion\nt\n= argmax\ni∈M\nP[Qmotion\nt,i\n| ˆQmusic\n<t , ˆQmotion\n<t\n]\n(4)\nwhere M is the codebook size. With this sampling strategy,\nwe can conduct the joint generation of music and motion\nunder text conditions. Additionally, it facilitates zero-shot\nmusic-to-motion and motion-to-music generation. For ex-\nample, given a music sequence Qmusic\n1:S , an aligned motion\nsequence can be autoregressively sampled by\nˆQmotion\nt\n= argmax\ni∈M\nP[Qmotion\nt,i\n|Qmusic\n<t\n, ˆQmotion\n<t\n]\n(5)\nAn illustration of the sampling process can also be found in\nAppendix D.\nConsidering the inherent differences between music and\nmotion, we further introduce the following changes to the\npre-trained MusicGen to alleviate the mutual interference\n5\nbetween the two modalities. First, we add another train-\nable embedder for motion tokens, with which the model\ncan learn to differentiate the two modalities. Second, to en-\nsure the temporal parallelism, we add positional encodings\n{E1, E2, . . . , ES′} to music and motion separately, instead\nof using a holistic positional encoding of length 2S′. Third,\ninspired by the idea of Mixture of Experts (MoE), we intro-\nduce an additional feed-forward network (FFN) for motion\nin each transformer layer. As shown in Fig. 2, in each for-\nward pass, the first half of the feature (i.e., music features)\nis processed by the music FFN, and the second half (i.e.,\nmotion features) by the motion FFN. Fourth, we add a new\nmotion classification head at the end of the network to dis-\ntinguish motion code prediction from music code prediction.\nNote that for the new modules introduced above, we initial-\nize the motion embedder and FFNs with the corresponding\ncomponents from the pre-trained MusicGen. With a joint\nmotion VQ-VAE trained in Stage 1, such initialization en-\nsures that music features are not confused by uninitialized\nmotion features at the beginning of training, allowing the\nmusic generation capability to be better preserved.\nFollowing MusicGen, text conditioning is added with\ncross-attention. In the framework of music-motion joint gen-\neration, we add the text condition of two modalities inde-\npendently. We first encode music descriptions and motion\ndescriptions separately into features and apply classifier-\nfree guidance dropout independently. Then, during cross-\nattention on text conditions, we specialize the attention mask\nto allow music features to attend only to music conditions\nand motion features to attend only to motion conditions.\nBy fine-tuning the model on the music-motion dataset\nwith the above settings, we find that the model learns to\ngenerate motion in parallel with music quickly while still\nkeeping its music-generation ability. With a single training\ntask of music-motion joint generation, various applications\ncould be achieved in a zero-shot fashion, including text-to-\nmusic, text-to-motion, music-to-motion, motion-to-music,\nmotion-text-to-music, etc.\n4.3. Stage 3. Music and Motion Captioning\nThe final stage is for caption generation, where we treat the\nfine-tuned music-motion decoder in the previous stage as a\nfeature encoder for music and motion, and fine-tune another\nT5 decoder to generate captions for music and motion.\nHowever, using the music-motion decoder directly as a\nfeature extractor brings challenges. Firstly, the self-attention\nin the decoder is done causally, which is inadequate for\ncapturing rich music and motion features. Secondly, since the\ninput of the model is the concatenation of music and motion,\nwe are limited to input music-motion pairs for captioning,\nwhich is inflexible.\nTo address these issues, we introduce a trainable full self-\nattention module, initialized with the trained cross-modal\ncausal attention module, as shown in Fig. 2, Stage 3. In-\nspired by BLIP [30], which claims that the major differ-\nence between transformer encoders and decoders lies in\nthe self-attention layers, with embedding layers and FFNs\nfunctioning similarly, we therefore fine-tune only the newly\nintroduced full self-attention modules together with the T5\ndecoder on caption generation task, keeping the rest of the\nmusic-motion decoder unchanged. Considering that captions\nof music and motion are independent, we remove the cross-\nattention areas on the attention mask.\nIn practice, we first randomly mask the entire music or\nmotion tokens as empty, and concatenate them together as\ninput Qinput. This allows us to conduct music or motion\ncaptioning independently. Next, we forward it through the\nmusic-motion decoder with a null condition, where full self-\nattention is applied. We then take the output of the last hidden\nlayer of the model as the feature, which is cross-attended\nby the T5 text decoder. We fine-tune the model with the\nlanguage modeling task, and the generation target is either\nmusic caption or motion caption, depending on the input\nmasking.\n5. Experiment\n5.1. Effectiveness of Music-Motion Alignment\nTo quantitatively evaluate the beat alignment results, we cal-\nculate the mean L1 distance between each music beat and its\nnearest visual beat before and after the alignment. Specifi-\ncally, we randomly sample 300 music clips, each 10 seconds\nlong, pair each with a random motion sequence, and then\ncalculate the score before and after aligning them using the\nalgorithms we introduced. The mean L1 distance decreases\nfrom 6.34 to 1.78, demonstrating the overall effectiveness of\nour alignment algorithms. We further conduct a user study,\nthe results of which are in Appendix E.\n5.2. Evaluations\nWe conduct extensive evaluations of our model across\nvarious tasks and metrics. More implementation details\nabout hyperparameter choices, dataset, metrics and train-\ning\/evaluation setups are in Appendix F.\nModels\nFADVGG ↓\nKL↓\nCLAP↑\nRiffusion [15]\n14.8\n2.06\n0.19\nMubert [39]\n9.6\n1.58\n-\nMousai [45]\n7.5\n1.59\n0.23\nMusicLM (860M) [2]\n4.0\n1.31\n-\nMusicGen (300M) [7]\n4.9\n1.42\n0.27\nAudioLDM 2-Full (346M) [34]\n3.13\n1.17\n0.38\nOurs (300M)\n5.93\n1.99\n0.27\nMusicGen (fine-tuned on our data)\n5.81\n1.97\n0.28\nOurs (trained on data with vocals)\n4.11\n1.95\n0.29\nTable 1. Comparison of text-to-music generation on MusicCaps.\nBold and underlined results are the best and second-best results.\n6\nMethods\nR-Precision↑\nMMDist↓\nBleu↑\nROUGE-L↑\nCider↑\nBertScore↑\nTop1\nTop3\n@1\n@4\nReal\n0.506\n0.800\n2.986\n-\n-\n-\n-\n-\nMotionGPT [24]\n0.534\n0.803\n2.978\n42.61\n6.04\n34.47\n7.92\n31.57\nTM2T [21]\n0.525\n0.814\n2.995\n61.76\n21.98\n47.40\n71.12\n37.27\nOurs\n0.520\n0.806\n2.958\n52.84\n9.27\n40.11\n6.22\n40.90\nTable 5. Comparison of motion captioning on HumanML3D dataset.\nModels\nBeats Coverage↑\nBeats Hit↑\nDance2Music [1]\n83.5\n82.4\nFoley Music [16]\n74.1\n69.4\nCMT [11]\n85.5\n83.5\nD2M-GAN [55]\n88.2\n84.7\nCDCD [56]\n93.9\n90.7\nOurs\n93.0\n88.4\nTable 2. Comparison of motion-conditioned music generation on\nAIST++.\nModels\nDistk →\nDistg →\nBeat Align.↑\nReal\n10.61\n7.48\n0.24\nBailando [46]\n7.92\n7.72\n0.23\nFACT [31]\n10.85\n6.14\n0.22\nEDGE [50]\n10.58\n7.62\n0.27\nOurs (music conditioned)\n10.68\n10.35\n0.24\nOurs (text conditioned)\n9.14\n9.37\n0.25\nTable 3. Comparison of music-conditioned and text-conditioned\ndance generation.\nModels\nBleu↑\nMeteor↑\nRouge↑\nBertScore↑\nLTU [18]\n0.238\n0.250\n0.332\n0.876\nLP-MusicCaps [12]\n0.165\n0.202\n0.281\n0.879\nMU-LLaMA [35]\n0.238\n0.354\n0.475\n0.913\nOurs\n0.261\n0.291\n0.369\n0.892\nTable 4. Comparison of music captioning on MusicQA dataset.\nText-to-Music. In Table 1, we compare our UniMuMo with\nRiffusion [15], Mubert [39], Mousai [45], MusicLM [2], Mu-\nsicGen [7] and AudioLDM 2 [34]. We evaluate the perfor-\nmance on MusicCaps, with results of SOTAs directly sourced\nfrom their respective papers. We employ three metrics:\nFrechet Audio Distance (FADVGG) [27], Kullback-Leibler\nDivergence (KL) [28] and CLAP similarity (CLAP) [23, 51].\nThe first two metrics measure the audio quality, while the\nlast one measures the correspondence between generated au-\ndio and text descriptions. Note that the audio quality of our\nmodel does not match with SOTA models. We argue that this\nmight be due to the poor audio quality of our training data.\nFollowing MusicGen, we also use vocal-free training data.\nTo achieve this, we use Demucs [9, 43] to remove the vocal\npart of the music in Music4All dataset. Nonetheless, we ob-\nserve that many of the processed audio are of bad quality.\nThis is testified by the experiment of fine-tuning MusicGen\non our dataset for the same number of epochs while keeping\nall other settings the same (e.g., sequence length, batch size).\nAs shown in Table 1, the audio quality of the tuned model\nalso degrades. We also tried training the model on the orig-\ninal dataset with vocals, resulting in improved quantitative\nscores. However, the generated music is not perceptually\ngood, often filled with weird and meaningless vocals. This\nphenomenon, where training on music with vocals yields\nbetter quantitative scores, is also reported in MusicGen.\nDance-to-Music. In Table 2, we compare UniMuMo with\nDance2Music [1], Foley Music [16], CMT [11], D2M-\nGAN [55] and CDCD [56] on dance-conditioned music gen-\neration. For evaluation, we adopt Beats Coverage and Beats\nHit [55], both of which measure the alignment of generated\nmusic with motion.\nMusic\/Text-to-Dance. In Table 3, we compare UniMuMo’s\ndance-generation capabilities with Bailando [46], FACT [31]\nand EDGE [50] on AIST++ dataset. We evaluate UniMuMo\non both music-conditioned and text-conditioned dance gen-\neration tasks. Although there is currently no established\nbenchmark for the text-to-dance task, we can also apply the\nsame evaluation metrics to measure and compare the qual-\nity of generated dance. For evaluation metrics, we adopt\nkinetic distribution spread (Distk) and geometric distribu-\ntion spread (Distg) to measure the diversity. Additionally, we\nemploy the beat alignment score to measure the alignment\nbetween conditioning audio and generated dance. Follow-\ning EDGE, we evaluate the motion sequences on 5-second\nclip. For text-to-dance, we directly evaluate the dance that is\njointly generated with music, conditioned on both music and\nmotion captions, and we calculate the beat alignment score\nbetween the generated dance and music. The quantitative\nscores show that UniMuMo achieves competitive results on\nmusic-conditioned dance generation, even though it hasn’t\nbeen fine-tuned on AIST++ music. For text-conditioned gen-\neration, it achieves inferior dance quality since there is no\nground truth music for reference, but also gains a higher beat\nalignment score due to the joint generation.\nMusic-to-Text. In Table 4, we compare UniMuMo against\nSOTA music captioning models including LTU [18], LP-\nMusicCaps [12] and MU-LLaMA [35]. The evaluation is\nconducted on the MusicQA dataset released by [35], which\nis a music-related question-answering dataset. We take the\nanswers to the question ”Describe the audio” together with\nthe corresponding music as evaluation data, totaling 552\nmusic-caption pairs. Following MU-LLaMa, the metrics we\nuse includes Bleu, Meteor, RougeL and BertScore, which\nare all common evaluation metrics in natural language pro-\ncessing.\nMotion-to-Text. In Table 5, we compare UniMuMo with\nTM2T [21] and MotionGPT [24] for motion captioning us-\ning the HumanML3D test set. Following MotionGPT, we\nadopt the motion-retrieval precision (R-Precision) to mea-\nsure the accuracy of motion-text matching using top-1 and\ntop-3 retrieval accuracy, multi-modal distance (MM Dist)\n7\nto measure the distance between motion and text, and other\npopular natural language processing metrics, including Blue,\nRouge, Cider and BertScore, to assess the linguistic quality.\nSince we source only 50% of our training motion data from\nHumanML3D, and the motion is augmented to align with\nmusic beats, UniMuMo still lags behind the best SOTA in\ncertain metrics for HumanML3D motion captioning task.\nBased on the quantitative results presented above, Uni-\nMuMo achieves competitive performance compared to the\nSOTA benchmarks across various single-modal generation\ntasks. Specifically, in the motion-to-music, music-to-motion,\nmusic captioning and motion captioning tasks, UniMuMo\ngenerally ranks second among the SOTAs. However, in the\ntext-to-music task, UniMuMo’s performance is not as com-\npetitive, which we argue may be attributed to the limitations\nin our training data.\n5.3. Ablation Studies\nFADVGG ↓KL↓CLAP↑Distk →Distg →Beat Align.↑\nFull\n5.93\n1.99\n0.27\n10.54\n8.15\n0.24\nAblation 1\n6.75\n2.13\n0.26\n6.03\n8.28\n0.22\nAblation 2\n6.79\n2.06\n0.26\n6.73\n7.35\n0.23\nAblation 3\n6.29\n2.11\n0.24\n6.03\n8.28\n0.22\nAblation 4\n6.41\n2.06\n0.27\n9.22\n6.58\n0.22\nAblation 5\n7.10\n2.22\n0.23\n9.61\n9.01\n0.23\nTable 6. Comparisons of our full model with different ablation\nstudies on MusicCaps for music generation and our Music4All for\ndance generation. Ablation 1-2 show the results of using an inde-\npendent motion VQVAE for encoding motion sequences. Ablation\n3 shows the results of model without the key structures of separate\nembedder and MoE. Ablation 4 shows the results of using a mixture\nof training tasks during training. Ablation 5 shows the result of\ntraining our model from scratch.\nIn the ablation study, we first evaluate the effectiveness of\nthe proposed joint codebook encoding by training our model\nwith motion encoded by an independent motion VQ-VAE\n(ablation 1 and 2). We then assess the impact of the addi-\ntional separate embedder and FFN introduced to MusicGen\nby training the model without them (ablation 3). We also\ninvestigate training the model with multiple tasks, rather\nthan the single proposed music-motion joint generation (ab-\nlation 4). Finally, we evaluate the effectiveness of using a\npre-trained model for initialization by training the model\nfrom scratch (ablation 5). All ablations are compared with\nour benchmarks on MusicCaps for music generation and\nMusic4All for motion generation, as shown in Table 6. More\ndetails and analysis are in Appendix G.\n6. Conclusion\nIn this paper, we introduce UniMuMo, the first unified frame-\nwork for arbitrary generation across music, motion, and text.\nTo address the limitations of paired multimodal data, we\nexpand existing datasets with rhythm-based music-motion\nalignment and text augmentation, thus creating a compre-\nhensive new dataset. To build a unified model, we propose\nnovel architectural designs, including a music-motion joint\ntokenizer for bridging modality gaps and a music-motion\nparallel generation scheme for synchronized music and mo-\ntion generation. Extensive experiments show that UniMuMo\nachieves competitive performance in all unidirectional gen-\nerative tasks. We believe our framework will not only open\nup new avenues for multimodal generation but also inspire\nfuture advancements in this rapidly evolving field.\nReferences\n[1] Gunjan Aggarwal and Devi Parikh.\nDance2music: Au-\ntomatic dance-driven music generation.\narXiv preprint\narXiv:2107.06252, 2021. 3, 7\n[2] Andrea Agostinelli, Timo I. Denk, Zal´an Borsos, Jesse En-\ngel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren\nJansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi,\nNeil Zeghidour, and C. Frank. Musiclm: Generating music\nfrom text. ArXiv, abs\/2301.11325, 2023. 3, 6, 7, 13\n[3] Sebastian B¨ock and Gerhard Widmer. Maximum filter vibrato\nsuppression for onset detection. In Proc. of the 16th Int. Conf.\non Digital Audio Effects (DAFx). Maynooth, Ireland (Sept\n2013), page 4, 2013. 11\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877–1901, 2020. 11\n[5] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000–18010, 2023. 3\n[6] Ching-Yu Chiu, Alvin Wen-Yu Su, and Yi-Hsuan Yang. Drum-\naware ensemble architecture for improved joint musical beat\nand downbeat tracking. IEEE Signal Processing Letters, 28:\n1100–1104, 2021. 3\n[7] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant,\nGabriel Synnaeve, Yossi Adi, and Alexandre D´efossez. Sim-\nple and controllable music generation.\narXiv preprint\narXiv:2306.05284, 2023. 1, 2, 3, 4, 5, 6, 7, 13\n[8] Abe Davis and Maneesh Agrawala. Visual rhythm and beat.\nACM Transactions on Graphics (TOG), 37(4):1–11, 2018. 3,\n11\n[9] Alexandre D´efossez.\nHybrid spectrogram and waveform\nsource separation. In Proceedings of the ISMIR 2021 Work-\nshop on Music Source Separation, 2021. 7, 14\n[10] Alexandre D´efossez, Jade Copet, Gabriel Synnaeve, and Yossi\nAdi. High fidelity neural audio compression. arXiv preprint\narXiv:2210.13438, 2022. 2, 4\n[11] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu,\nZexin He, Hongming Liu, and Shuicheng Yan. Video back-\nground music generation with controllable music transformer.\nIn Proceedings of the 29th ACM International Conference on\nMultimedia, pages 2037–2045, 2021. 7\n8\n[12] SeungHeon Doh, Keunwoo Choi, Jongpil Lee, and Juhan\nNam. Lp-musiccaps: Llm-based pseudo music captioning.\narXiv preprint arXiv:2307.16372, 2023. 3, 7\n[13] Daniel PW Ellis. Beat tracking by dynamic programming.\nJournal of New Music Research, 36(1):51–60, 2007. 11\n[14] Di Fan, Lili Wan, Wanru Xu, and Shenghui Wang. A bi-\ndirectional attention guided cross-modal network for music\nbased dance generation. Computers and Electrical Engineer-\ning, 103:108310, 2022. 3\n[15] Seth* Forsgren and Hayk* Martiros. Riffusion - Stable diffu-\nsion for real-time music generation. 2022. 2, 6, 7\n[16] Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenen-\nbaum, and Antonio Torralba. Foley music: Learning to gener-\nate music from videos. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part XI 16, pages 758–775. Springer, 2020. 3, 7\n[17] Toni Giorgino. Computing and visualizing dynamic time\nwarping alignments in r: the dtw package. Journal of statisti-\ncal Software, 31:1–24, 2009. 11\n[18] Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlin-\nsky, and James Glass. Listen, think, and understand. arXiv\npreprint arXiv:2305.10790, 2023. 3, 7\n[19] Yusuke Goutsu and Tetsunari Inamura. Linguistic descrip-\ntions of human motion with generative adversarial seq2seq\nlearning. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA), pages 4281–4287. IEEE, 2021. 3\n[20] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 5152–5161, 2022. 2, 14\n[21] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:\nStochastic and tokenized modeling for the reciprocal genera-\ntion of 3d human motions and texts. In European Conference\non Computer Vision, pages 580–597. Springer, 2022. 3, 7, 15\n[22] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,\nAndy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,\nJiahui Yu, Christian Frank, et al.\nNoise2music: Text-\nconditioned music generation with diffusion models. arXiv\npreprint arXiv:2302.03917, 2023. 3\n[23] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Lup-\ning Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin,\nand Zhou Zhao. Make-an-audio: Text-to-audio generation\nwith prompt-enhanced diffusion models.\narXiv preprint\narXiv:2301.12661, 2023. 7\n[24] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and\nTao Chen. Motiongpt: Human motion as a foreign language.\narXiv preprint arXiv:2306.14795, 2023. 1, 3, 7, 15\n[25] Marek Kadlˇc´ık, Adam H´ajek, J¨urgen Kieslich, and Radosław\nWiniecki. A whisper transformer for audio captioning trained\nwith synthetic captions and transfer learning. arXiv preprint\narXiv:2305.09690, 2023. 3\n[26] Peter E Keller and Martina Rieger. Musical movement and\nsynchronization. Music Perception, 26(5):397–400, 2009. 1\n[27] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi.\nFr\\’echet audio distance: A metric for\nevaluating music enhancement algorithms. arXiv preprint\narXiv:1812.08466, 2018. 7\n[28] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,\nAlexandre D´efossez, Jade Copet, Devi Parikh, Yaniv Taigman,\nand Yossi Adi. Audiogen: Textually guided audio generation.\narXiv preprint arXiv:2209.15352, 2022. 7\n[29] Edward W Large. On synchronizing movements to music.\nHuman movement science, 19(4):527–566, 2000. 1\n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\nBootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In International Con-\nference on Machine Learning, pages 12888–12900. PMLR,\n2022. 6\n[31] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa.\nAi choreographer: Music conditioned 3d dance generation\nwith aist++. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision, pages 13401–13412, 2021.\n3, 7\n[32] Ruilong Li, Shan Yang, David A. Ross, and Angjoo\nKanazawa. Learn to dance with aist++: Music conditioned\n3d dance generation, 2021. 2, 12, 14\n[33] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-\noldm: Text-to-audio generation with latent diffusion models.\narXiv preprint arXiv:2301.12503, 2023. 15\n[34] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-\nuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang,\nand Mark D. Plumbley. AudioLDM 2: Learning holistic audio\ngeneration with self-supervised pretraining. arXiv preprint\narXiv:2308.05734, 2023. 3, 6, 7\n[35] Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and\nYing Shan. Music understanding llama: Advancing text-to-\nmusic generation with question answering and captioning.\narXiv preprint arXiv:2308.11276, 2023. 3, 7, 11, 13, 15\n[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 14\n[37] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. AMASS: Archive of\nmotion capture as surface shapes. In International Conference\non Computer Vision, pages 5442–5451, 2019. 2, 12, 14\n[38] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis,\nMatt McVicar, Eric Battenberg, and Oriol Nieto. librosa:\nAudio and music signal analysis in python. In Proceedings of\nthe 14th python in science conference, 2015. 3, 15\n[39] Mubert-Inc.\nMubert.\nhttps:\/\/mubert.\ncom\/,\nhttps:\/\/github.com\/mubertai\/ mubert-text-to-music. 2022. 6,\n7\n[40] Matthias Plappert, Christian Mandery, and Tamim Asfour.\nLearning a bidirectional mapping between human whole-body\nmotion and natural language using deep recurrent neural net-\nworks. Robotics and Autonomous Systems, 109:13–26, 2018.\n3\n[41] Junfu Pu and Ying Shan.\nMusic-driven dance regenera-\ntion with controllable key pose constraints. arXiv preprint\narXiv:2207.03682, 2022. 3\n[42] Lawrence R Rabiner and Biing-Hwang Juang. Fundamentals\nof speech recognition. Tsinghua University Press, 1999. 11\n[43] Simon Rouard, Francisco Massa, and Alexandre D´efossez.\nHybrid transformers for music source separation. In ICASSP\n23, 2023. 7, 14\n9\n[44] Igor Andr´e Pegoraro Santana, Fabio Pinhelli, Juliano Donini,\nLeonardo Catharin, Rafael Biazus Mangolin, Val´eria Delisan-\ndra Feltrim, Marcos Aur´elio Domingues, et al. Music4all:\nA new music database and its applications. In 2020 Interna-\ntional Conference on Systems, Signals and Image Processing\n(IWSSIP), pages 399–404. IEEE, 2020. 2, 11, 13\n[45] Flavio Schneider, Zhijing Jin, and Bernhard Sch¨olkopf. Mo\\ˆ\nusai: Text-to-music generation with long-context latent diffu-\nsion. arXiv preprint arXiv:2301.11757, 2023. 3, 6, 7\n[46] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,\nChen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d\ndance generation by actor-critic gpt with choreographic mem-\nory. In Proceedings of the IEEE\/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11050–11059,\n2022. 3, 7, 15\n[47] Kun Su, Xiulong Liu, and Eli Shlizerman.\nHow does it\nsound? Advances in Neural Information Processing Systems,\n34:29258–29273, 2021. 3\n[48] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel\nCohen-Or, and Amit H Bermano. Human motion diffusion\nmodel. arXiv preprint arXiv:2209.14916, 2022. 3\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language models.\ncorr, abs\/2302.13971, 2023. doi: 10.48550. arXiv preprint\narXiv.2302.13971. 3\n[50] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 448–458, 2023. 1, 3, 7, 15\n[51] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor\nBerg-Kirkpatrick, and Shlomo Dubnov.\nLarge-scale con-\ntrastive language-audio pretraining with feature fusion and\nkeyword-to-caption augmentation. In ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 1–5. IEEE, 2023. 3, 7\n[52] Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata.\nPaired recurrent autoencoders for bidirectional translation\nbetween robot actions and linguistic descriptions.\nIEEE\nRobotics and Automation Letters, 3(4):3441–3448, 2018. 3\n[53] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli\nHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi\nShen.\nT2m-gpt: Generating human motion from textual\ndescriptions with discrete representations. arXiv preprint\narXiv:2301.06052, 2023. 3\n[54] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong,\nXinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-\ndriven human motion generation with diffusion model. arXiv\npreprint arXiv:2208.15001, 2022. 3\n[55] Ye Zhu, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei\nChai, Yan Yan, and Sergey Tulyakov. Quantized gan for\ncomplex music generation from dance videos. In European\nConference on Computer Vision, pages 182–199. Springer,\n2022. 1, 3, 7, 15\n[56] Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov,\nand Yan Yan. Discrete contrastive diffusion for cross-modal\nmusic and image generation. In The Eleventh International\nConference on Learning Representations, 2022. 3, 7, 15\n10\nAppendix\nA. Visual Beats Detection Details\nAs discussed in the main paper, there are three steps to ob-\ntain visual beats from 3D motion sequence M ∈RTm×J×3\nwhere Tm is the number of frames, J is the number of\njoints, and the third dimension represents x, y, z coordi-\nnates. Inspired by the idea used to find visual impacts in\nraw videos [8], we adapt this approach to find local saliency\nin motion sequences. The main idea is to use the sudden\nvisible deceleration of the motion sequence as the basis of\nthe heuristic to find “visual beats”. The three steps have very\nsimilar physical meanings to traditional music beat detection\ntechniques [3, 13].\nIn the first step, we compute the motion directogram Md,\na 2D matrix that factors motion into different motion angles,\nas described in the main paper. The motion directogram is\nsimilar to the audio spectrogram, which could offer spectral\nflux to measure the change in amplitude of different frequen-\ncies over time. Therefore, in the second step, we compute the\npre-direction deceleration of Md as an analogue for spectral\nflux to obtain the motion flux Mf. In audio, the onset enve-\nlope could be inferred from the spectral flux. In motion, we\nobtain the visual impact envelope, called kinematic offsets\nMk, by averaging each frame of Mf and filtering the top 1%\npeaks. With the onset envelope, usually, an onset detection\nfollowed by dynamic programming-based beat tracking algo-\nrithms [3, 13] are used to find the most likely periodic music\nbeats. In motion sequence, we also use dynamic program-\nming to compute the visual beats by designing an objective\nfunction that selects strong visual changes from kinematic\noffsets and encourages equal-spacing beats. We optimize the\nobjective function:\nV (m) =\nn\nX\nj=1\nu(mj) + α\nn−1\nX\nj=1\nVT (mj, mj+1)\nVT (mj, mj+1) = T[bin(mj+1 −mj)]\nTmax\n−1.0\nwhere u is the kinematic offset value of the candidate beat to\nencourage strong visual impacts, α is the weight to balance\nthe two terms, T is the autocorrelation mean over the local\ntime window, and {mj}n\nj=1 ∈m is a subset of candidate\nbeats. The VT (mj, mj+1) regularizes the estimated tempos\nwithin a local window to encourage equal-spacing beats.\nWe measure the deviation by computing the time-dependent\nautocorrelation function T on kinematic offsets.\nB. Music Motion Alignment Details\nWe apply dynamic time warping (DTW) to compute the\noptimal alignment between music beats Bm and visual beats\nBv. The optimal alignment minimizes the sum of distances\nbetween aligned elements, even though the lengths of Bm\nand Bv may differ. The local distance between elements of\nBm and Bv is computed by Euclidean distance. Regarding\nthe transitions allowed while searching for the minimum-\ndistance path, we use the Rabiner-Juang step pattern [42]. We\nuse python-dtw package [17] to find the alignment. Finally,\nwe warp motion sequences according to the warping curve.\nC. Text Description Construction Details\nC.1. Music Caption Generation with Music Lan-\nguage Model\nWe employed MU-LLaMa [35], a specialized large language\nmodel for music-related Q&A tasks, to generate music de-\nscriptions using genre metadata from the Music4All dataset.\nOur experiments show that MU-LLaMa generally under-\nstands music effectively, accurately assesses tags, and inte-\ngrates them into cohesive descriptions. However, we also\nnotice that the generated descriptions often lack diversity\nboth in sentence structure and content. Repeated results are\nalso observed.\nC.2. Music Caption Generation with ChatGPT and\nTemplate Filling\nWe adopt energy, tempo, genres, and tags from the metadata\nof Music4All [44]. Energy, indicating musical intensity and\nactivity, ranges from 0.0 to 1.0. Tempo, measured in beats per\nminute (BPM), reflects the music’s speed. The dataset cate-\ngorizes music into 853 unique genres and includes 19,541\ntags.\nFirst, we convert energy and tempo into descriptive tags\nwith the criteria shown in Table 8 and 9. Adverbs and adjec-\ntives are randomly selected and paired, and the thresholds are\nmanually determined by listening to samples and comparing\nvalues. Then, we construct phrases from tags using random\ntemplates. We construct a tempo phrase from tempo descrip-\ntion, an energy phrase from energy description and a tag\nphrase from genres and tags. The choices of templates are\nshown in Table 10. Next, we randomly shuffle, dropout and\nconcatenate the phrases to construct raw music text descrip-\ntions. Finally, we refine the descriptions with ChatGPT [4].\nWe set the ChatGPT content as “You are an expert in music,\nskilled in writing music comments and descriptions.” and\nuse the prompt “Here are {n} music descriptions, please\npolish them separately into fluent and meaningful sentences\nwith details. Please return the polished results in the format\nof “1: content... 2: content... ...”” to polish n descriptions on\neach request.\nThis approach ensures that our synthesized descriptions\nare more natural than direct tag concatenation and more di-\nverse than using full descriptive templates. Moreover, it has\nmore control on the synthesized results than directly asking\nChatGPT to write descriptions from tags. However, since the\n11\nDescription Type\nExamples\nChatGPT Generated\nBlending elements of underground hip-hop, ekip, and rap, this music exudes a\ngentle yet quick energy that immerses the listener in its unique category.\nThis high-energy song combines the raw power of indie rock with the smooth and expressive\nelements of jazz, resulting in a unique and captivating musical experience.\nThis high-tempo track seamlessly combines German pop and alternative styles, boasting a\nspirited and lively atmosphere that will keep listeners engaged from start to finish.\nMU-LLaMa Generated\nThe music is a blend of neofolk, martial industrial, and dark ambient.\nThe audio is a progressive rock\/metal song with a fast tempo, steady drumming, and a bass guitar rhythm.\nThe music is a bossa nova\/jazz\/MPB\/soul fusion with a touch of Brazilian rhythms.\nMotion Desctiption\nThe style of the dance is ballet jazz.\nThis is a break style dance.\nThe genre of the dance is LA style hip-hop.\nTable 7. Examples of three kinds of synthesized text descriptions.\nRange\nAdverb\nAdjective\ntempo < 60\nextremely,\nvery\nslow, languid,\nlethargic, relaxed,\nleisure, chilled\n60 ≤tempo < 75\nslow, languid,\nlethargic, relaxed,\nleisure, chilled\n75 ≤tempo < 110\nmoderate, easy-going,\nlaid-back,medium,\nbalanced, neutral\n110 ≤tempo < 150\nfast, upbeat, high,\nbrisk, quick, rapid,\nswift\ntempo >= 150\nextremely,\nvery, highly\nfast, upbeat, high,\nbrisk, quick, rapid,\nswift\nTable 8. Choices of descriptive tags for tempo. Adverbs and adjec-\ntives are randomly chosen and paired.\ngenerated captions source the musical information only from\ngenre, tag, intensity, and tempo, they often cannot provide\nspecific details such as the instrument composition or the\nemotional tone. Additionally, the descriptive tags of energy\nand tempo can sometimes be imprecise due to the inaccura-\ncies of metadata. Therefore, during training we leverage a\nmixture of the above two methods for music captioning to\nstrike a balance between musical accuracy and diversity.\nC.3. Motion Caption Generation\nThe only metadata that is available for AIST++ [32] and\nDancedDB [37] is a genre tag. Therefore, we directly con-\nstruct the motion descriptions using the following templates:\n“The genre of the dance is <>”, “The style of the dance is\n<>.”, “The is a <> style dance.”, where <> is filled with\nthe genre tag.\nFinally, as mentioned in the paper, we apply text condi-\ntioning separately to music and motion, adding classifier-free\nRange\nAdverb\nAdjective\nenergy < 0.1\nextremely,\nvery\nsoft, calm, peaceful,\nserene, gentle, light,\ntranquil, mild, mellow\n0.1 ≤energy < 0.4\nsoft, calm, peaceful,\nserene, gentle, light,\ntranquil, mild, mellow\n0.4 ≤energy < 0.7\nmoderate, comfortable,\nbalanced, relaxing\n0.7 ≤energy < 0.9\nintense, powerful, strong,\nvigorous, fierce,\npotent, energetic\nenergy >0.9\nextremely,\nvery, highly\nintense, powerful, strong,\nvigorous, fierce,\npotent, energetic\nTable 9. Choices of descriptive tags for energy.\nPhrase Type\nPhrase Choices\nTempo Phrase\nwith a <> tempo,\nwhose speed is <>,\na <> music,\nset in a <> pace\nEnergy Phrase\nwhich is <>,\nwith <> intensity,\na <> music,\nwhose energy is <>\nTag Phrase\nThis is atrack which is <>,\nThis song has the style of <>,\nThe music is <>,\nThe genre of the music is <>\nTable 10. Choices of phrase template for tempo, energy and tags.\nInside <>, we fill in tempo tag, energy tag or a list of genres and\ntags.\nguidance dropout independently. Examples of the several\ntypes of descriptions mentioned above are shown in Table 7.\n12\nD. Illustration of Music Motion Parallel Gener-\nation\nDuring the training of UniMuMo, we introduce a music\nmotion parallel generation scheme, which not only allows\nfor the joint generation of music and motion, but also enables\nzero-shot music-to-motion and motion-to-music generation.\nIn this section, we provide illustrations to further explain the\ntraining and inference processes.\nDuring training, as illustrated in Figure 3a, music and\nmotion are trained on language modeling task separately.\nIn each forward pass, the system is trained to predict the\nnext music token for music and the next motion token for\nmotion. The predict-next-token losses are calculated sepa-\nrately for each modality and then summed up with different\nweights. The customized cross-modal causal-attention mask,\neach quarter of which is a lower triangular matrix, ensures\nthat each modality can causally attend to itself while also\nallowing both modalities can causally attend to each other.\nNote that in our implementation, music and motion are\nencoded using a four-layer residual codebook, resulting four\ntokens at each timestep. As mentioned in the paper, we fol-\nlow the approach of MusicGen [7], adopting a delay pattern\nto transform the input tokens of shape K ×S, where K is the\nnumber of codebooks, chosen to be 4 in our implementation,\nand S is the length of the sequence of tokens. Figure 3b\nillustrates this pattern, with more details available in their\noriginal paper. During implementation, we first transform\nthe music tokens and motion tokens separately according to\nthis pattern, and then concatenate them to form the model\ninput.\nDuring inference, as shown in Figure 4a and 4b, one mu-\nsic token and one motion token are sampled on each forward\npass. For joint music-motion generation, the sampled next\nmusic token and next motion token are placed in the corre-\nsponding positions of the input for the next timestep. When\ngenerating music conditioned on motion, the conditioning\nmotion token at the same timestep is used instead of the\npredicted next motion token.\nE. User Study on Music-Motion Alignment\nSince no ground truth for measuring the accuracy of the\nmusic-motion alignment, we further conduct a user study,\nwhere users are required to rate the audio-visual alignment\nfrom 1 (not aligned) to 5 (aligned) based on their perception.\nAccording to the responses of 8 users, each presented with\n20 videos (half aligned, half randomly paired), the average\nscore is 3.95 for aligned results and 3.26 for random pairs,\ndemonstrating the algorithm’s effectiveness. A screenshot of\nthe survey form is in Figure 5.\n(a) An illustration of one forward pass during UniMuMo’s training.\nMusic tokens (including the music start token) are denoted in blue\nblocks and motion tokens (including the motion start token) are denoted\nin orange blocks. The numbers in the block denote the timestep of each\ntoken.\nOriginal Pattern: K x S\nDelay Pattern: (K+3) x S\n...\n11\n12\n13\n1\nS-2\n1\nS-1\n1S\n...\n11\n12\n13\n1\nS-2\n1\nS-1\n1S\n...\n11\n12\n13\n1\nS-2\n1\nS-1\n1S\n...\n11\n12\n13\n1\nS-2\n1\nS-1\n1S\n...\n11\n12\n13\n14\n1\n1S\n1\n1\n...\n1\n11\n12\n13\n1S\n1\nS-1\n1\n1\n...\n1\n1\n11\n12\n1\nS-1\n1\nS-2\n1S\n1\n...\n1\n1\n1\n11\n1\nS-2\n1\nS-3\n1\nS-1\n1S\n(b) An illustration of the delay pattern in MusicGen. Each color repre-\nsents a different layer of the residual codebook, and the numbers on\nthe blocks indicate the timestep. After applying the delay pattern, the\ntokens denoted in grey are padded with a special empty token.\nFigure 3. Illustrations on the technical details in our training pro-\ncess.\nF. Implementation Details\nF.1. Training\nDatasets. For the music training dataset, we mainly use Mu-\nsic4All [44], which consists of 109K 30-second soundtracks\nwith corresponding metadata, such as genre, energy, and\ntempo. For evaluation and comparison with prior work on\nmusic generation, we use the MusicCaps benchmark [2],\nwhich is composed of 5.5K ten-second samples with expert-\nprepared text descriptions. For evaluation on music caption-\ning, we leverage the MusicQA Dataset [35], which contains\n560 music tracks with textual descriptions.\n13\n(a) An illustration of how UniMuMo conduct music motion parallel\ngeneration. In each timestep T, one forward pass is performed.\n(b) An illustration of UniMuMo’s motion-to-music generation. The\nmusic-to-motion generation is similar to this process.\nFigure 4. Illustrations on the technical details in the inference\nprocess.\nFor motion dataset, we use a mixture of 3D dance dataset\nAIST++ [32], DancedDB [37] and 3D motion dataset Hu-\nmanML3D [20]. AIST++ contains 311 minutes of dance\nacross 30 subjects and 10 genres, while DanceDB contains\n203.38 minutes of dance across 20 subjects. Text descriptions\nfor both datasets are synthesized by filling templates with\nmetadata. HumanML3D, a much larger dataset of general\nmotion, contains 14616 motions with 44970 text descrip-\ntions, in total 28.59 hours long. We upsample its motion\ndata from 20 Hz to 60 Hz to align with the frame rate of the\nFigure 5. A screen shot of the user study form for evaluating our\nmusic-motion alignment algorithm.\nAIST++ dataset.\nData Preparations. We remove the vocal part of all the\ntraining audios using Demucs [9, 43] and keep only the\ninstrumental part. We randomly pair each soundtrack with 5\nmotion sequences, with approximately 50% drawn from 3D\ndance datasets AIST++ and DanceDB, and the remainder\nfrom HumanML3D. The pairing and tokenization of music\nand motion are done in advance to save training time.\nStage 1: Music and Motion Tokenization Model. We adopt\nthe default Encodec from the MusicGen model, which com-\npresses 32K Hz waveform into 50 Hz. The RVQ has 4 code-\nbooks, each with 2048 entries of dimension 128. The motion\nencoder encodes 60 Hz, 263-dimensional motion features\ninto 50 Hz, 128-dimensional features suitable for quantiza-\ntion by the RVQ. The motion encoder-decoder, together with\nthe frozen RVQ, is trained on 2-second motion data, with a\nbatch size of 336 and a learning rate of 2e-4.\nStage 2 Music-Motion Decoder Model. In stage 2, the\nmodel we fine-tune is MusicGen-small, a 300M transformer\ndecoder model together with a 120M T5 text encoder. We\ntrain the model on 10-second aligned music-motion pairs\nwith a batch size 144. We train the model for 15K steps with\nAdamW optimizer [36], β1 = 0.9, β2 = 0.95, and a learning\n14\nrate of 5e-5. The training takes around 6 hours on 48 Tesla\nV-100 32G GPUs.\nStage 3: Music-Motion Captioner Model. In stage 3, we\nchoose the T5-base model as the text decoder. We train the\nmodel on 10-second unpaired music and motion sequences.\nMotion sequences shorter than this duration are zero-padded\nto 10 seconds. The other settings remain the same as in stage\n2.\nF.2. Evaluation\nText-to-Music. The evaluation scripts for FAD and KL are\ndirectly from the evaluation repository of AudioLDM1 [33].\nWe use the default pre-trained model in the official webpage\nof CLAP2 to calculate the CLAP score. All the scores for\nSOTAs are directly borrowed from the corresponding papers,\nexcept for MusicGen. As explained on their Hugging Face\npages, their publicly released models are trained on another\nset of vocal-free music, resulting in slightly lower quantita-\ntive scores. Since we directly fine-tune their released model,\nwe report the scores as presented on the Hugging Face page\ninstead.\nMotion-to-Music. The evaluation scripts for Beats Coverage\nand Beats Hit are directly sourced from D2M-GAN 3 [55].\nThe test music-motion pairs are 2 seconds long, and the\nsegmentation is also from D2M-GAN [55]. All scores for\nother models are directly borrowed from CDCD [56].\nMusic\/Text-to-Dance. We directly use the script in Bai-\nlando 4 [46] for evaluating feature distributions (Distk and\nDistg). For the beat alignment score, we modify the script\nfrom [46] to use the Librosa [38] API for music beat detec-\ntion. All scores for other models are sourced from [50]. Re-\ngarding the choice of evaluation metrics, we did not include\nthe Physical Foot Contact score (PFC) proposed in [50], as\ntheir provided script does not yield correct scores on our data,\neven when evaluating the ground truth. The discrepancy may\nbe due to the slight differences in motion representation (e.g.,\nfps, number of joint). According to [50], the Frechet Incep-\ntion Distances on kinetic features and geometric features\nare also not employed, as they are not considered reliable\nfor measuring dance quality. For testing data, we use the\noriginal test set split from the AIST++ dataset, and slice the\ndance-music pairs into 5-second segments using the script\nfrom [50].\nMusic-to-Text. We use the evaluation script from MU-\nLLaMa 5 [35] for the testing metrics. The testing dataset,\nMusicQA, is also publicly available in [35]. As we only take\nthe subset of the dataset related to music captioning as our\ntest set, we re-evaluate all previous SOTAs and report their\n1https:\/\/github.com\/haoheliu\/audioldm eval\n2https:\/\/github.com\/LAION-AI\/CLAP\n3https:\/\/github.com\/L-YeZhu\/D2M-GAN\n4https:\/\/github.com\/lisiyao21\/Bailando\n5https:\/\/github.com\/shansongliu\/MU-LLaMA\nresults.\nMotion-to-Text. We adopt the evaluation script from the\nopen-sourced code of MotionGPT 6 [24]. Since they an-\nnounced that they used a different package for NLP-related\nmetrics calculation due to package conflict, we re-evaluated\ntheir model and TM2T [21] on our HumanML3D test set.\nThis accounts for the differences between our reported re-\nsults and theirs.\nG. More Analysis on Ablation Study\nAblation 1-2: Effect of joint codebook encoding. In this\nanalysis, we examine the effectiveness of using a shared\ncodebook that maps motion into the same feature space as\nmusic. Specifically, we train the model on motion codes ex-\ntracted by an independent motion VQ-VAE. In ablation 1,\nwe don’t initialize the motion embedder with corresponding\npre-trained weights in MusicGen, while in ablation 2, we\nperform such initialization, which is also the practice in our\nfull model. The results of both ablation studies yield inferior\nscores, demonstrate the effectiveness of our method. Impor-\ntantly, ablation 2 proves that it is not merely the initialization\nof the motion embedders that facilitates training, but it is\nonly when the initialized motion embedders are input with\nmeaningful motion tokens that training is facilitated.\nAblation 3: Effect of additional architectures. We also\nexamine the effectiveness of the additional components we\nintroduce to the pre-trained MusicGen, specifically the sepa-\nrate motion embedder and Mixture of Expert (MoE) struc-\nture. In ablation 3, we train the model without MoE and the\nseparate motion embedder, allowing music and motion to\nshare most of the MusicGen model, except the linear clas-\nsifier at the end. Experimentally, this results in degraded\nperformance, especially in motion generation.\nAblation 4: An alternative multi-task training scheme. In\nstage two, we employ a single training task of music-motion\njoint generation. However, within this parallel transformer\nstructure, it is also intuitive to adopt a mixture of three train-\ning tasks: music-motion generation, music-to-motion and\nmotion-to-music. We explore this idea in ablation 4, where\nwe randomly select among the three tasks during training and\napply the corresponding cross-modal self-attention masks\n(e.g., in music-to-motion, we allow music to only causally\nattend to itself, and motion to causally attend to itself and\nfully attend to music). However, the results are not satisfac-\ntory. We hypothesize that the reasons could be 1) a mixture\ntraining tasks might result in gradient conflicts; 2) the dif-\nficulty level of the three tasks varies, so simply randomly\nselecting the task might not be sufficient; and 3) the rhythms\nof music and dance have regular patters, so they might not\nnecessarily need to be fully attended to when performing\ncross attention.\n6https:\/\/github.com\/OpenMotionLab\/MotionGPT\n15\nAblation 5: Effect of using pre-trained model. In ablation\n5, we train the same model from scratch, keeping other\nsettings unchanged. The best results shown in the table are\nachieved after around 30K iterations of training, compared\nwith only 15K iterations of training if the pre-trained model\nis loaded.\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/UniMuMo: Unified Text, Music and Motion Generation.pdf"}
{"title":"MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation","authors":"Lijian Xu, Hao Sun, Ziyu Ni, Hongsheng Li, Shaoting Zhang","summary":"Medicine is inherently multimodal and multitask, with diverse data modalities\nspanning text, imaging. However, most models in medical field are unimodal\nsingle tasks and lack good generalizability and explainability. In this study,\nwe introduce MedViLaM, a unified vision-language model towards a generalist\nmodel for medical data that can flexibly encode and interpret various forms of\nmedical data, including clinical language and imaging, all using the same set\nof model weights. To facilitate the creation of such multi-task model, we have\ncurated MultiMedBench, a comprehensive pretaining dataset and benchmark\nconsisting of several distinct tasks, i.e., continuous question-answering,\nmulti-label disease classification, disease localization, generation and\nsummarization of radiology reports. MedViLaM demonstrates strong performance\nacross all MultiMedBench tasks, frequently outpacing other generalist models by\na significant margin. Additionally, we present instances of zero-shot\ngeneralization to new medical concepts and tasks, effective transfer learning\nacross different tasks, and the emergence of zero-shot medical reasoning.","url":"http:\/\/arxiv.org\/abs\/2409.19684v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2409.19684v1","published":1727612590000,"comment":null,"pdf_text":"MEDVILAM: A MULTIMODAL LARGE LANGUAGE MODEL WITH\nADVANCED GENERALIZABILITY AND EXPLAINABILITY FOR\nMEDICAL DATA UNDERSTANDING AND GENERATION\nA PREPRINT\nLijian Xu ∗1,2, Hao Sun1, Ziyu Ni3, Hongsheng Li1,4 and Shaoting Zhang2\n1Centre for Perceptual and Interactive Intelligence, the Chinese University of Hong Kong, Hongkong\n2Shanghai Artificial Intelligence Laboratory, Shanghai\n3SenseTime Research, Shanghai\n4Department of Electronic Engineering, the Chinese University of Hong Kong, Hongkong\nAugust 10, 2023\nABSTRACT\nMedicine is inherently multimodal and multitask, with diverse data modalities spanning text, imaging.\nHowever, most models in medical field are unimodal single tasks and lack good generalizability and\nexplainability. In this study, we introduce MedViLaM, a unified vision-language model towards a\ngeneralist model for medical data that can flexibly encode and interpret various forms of medical\ndata, including clinical language and imaging, all using the same set of model weights. To facilitate\nthe creation of such multi-task model, we have curated MultiMedBench, a comprehensive pretraining\ndataset and benchmark consisting of several distinct tasks, i.e., continuous question-answering, multi-\nlabel disease classification, disease localization, generation and summarization of radiology reports.\nMedViLaM demonstrates strong performance across all MultiMedBench tasks, frequently outpacing\nother generalist models by a significant margin. Additionally, we present instances of zero-shot\ngeneralization to new medical concepts and tasks, effective transfer learning across different tasks,\nand the emergence of zero-shot medical reasoning. Experiments on various medical image datasets\ndemonstrate MedViLaM’s superior generalization performance over existing methods, suggesting its\npotential for clinical applications in future.\nKeywords Multi-modality · Multi-task · Disease Classification · Visual Grounding · Repeort Generation\n1\nIntroduction\nDeep learning has achieved remarkable progress for the screening and diagnosis of medical images due to automated\nfeature discovery and superior results. However, the application of the proposed methods in clinic is still challenged by\nthe limited generalizability and interpretability of the proposed methods. Medical images were acquired with different\nacquisition parameters or modalities that have very different characteristics. Furthermore, deep learning models are\nessentially black boxes that lack explainability of their decision-making process. The poor explainability leads to\ndistrust from clinicians who are trained to make explainable clinical inferences. Consequently, there is an urgent need\nfor innovative methodologies to improve the generalizability and explainability of deep learning methods that will\nenable them to be used routinely in clinical practice.\nLarge Language Models (LLMs) have acquired great advancement in various language tasks and provides a new\nparadigm for human-computer interaction based on vast amounts of text data. Models like ChatGPT have demonstrated\nthe powerful reasoning capabilities of language models in complex scenarios like medical diagnosis to assist profes-\nsionals in delivering care. Recently, multimodal models, such as GPT-4o [1] and LLaMA-3 [2], shown impressive\n∗corresponding author: bmexlj@gmail.com\narXiv:2409.19684v1  [cs.CV]  29 Sep 2024\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nadvancement in generalizability and interpretability in the language and vision tasks of nature datasets with the integra-\ntion of large language models. Preliminary experiments show that GPT-4o achieves progress in the multi-tasks of the\nmedical field, yet is still constrained by the intricate nature of medical tasks. In the comprehensive scene, such as visual\ngrounding and report generation tasks, GPT-4o can engage in continuous diagnosis conversations, but failed to provide\nthe visual explanations [3].\nWe previously presented the first model for conducting multi-task analysis of chest X-ray images, which exhibits\ncompetitive performance for the interpretability of generated reports. However, it still faces challenges when it comes\nto generalizing to unseen disease categories and undefined instructions (tasks). These limitations highlight the need\nfor further research and development to improve the model’s ability to handle novel diseases and adapt to unfamiliar\ninstructions. Given the wider array of modalities and tasks in instruction tuning, more generalized models like LLMs\nare expected to understand the tasks better and render modality-wise and task-wise emergent capability. On the other\nhand, the small benchmark for single task is another limitation of the previous study. The verification of the model’s\ngeneralizability should be performed on a more comprehensive benchmark.\nFor advancement and verification of the generalization performance, we herein introduced MedViLaM, a unified\nvision-language model supported by LLM, and further developed comprehensive benchmark to address these challenges.\nMedViLaM incorporated instruction tuning to fully activate LLM’ knowledge and reasoning for medical images.\nWe reorganized a multi-task training dataset comprising 20.5M multi-task-oriented instruction pairs (associated with\n1.8M medical images) and clinical ground-truth pairs for building customized instructions of multi-tasks: visual\nquestion answering, disease classification, disease localization, and report generation. In such a manner, we unified the\nvarious individual vision-focused tasks in a single training framework with homogeneous model inputs and outputs.\nFurthermore, we established a comprehension benchmark for medical images, comprising various public and private\ndatasets, aiming to evaluate of the generalizability of large-scale foundation models. Our model demonstrates strong\nperformance for all tasks in both direct inference and few-shot fine-tuning experimental settings compared to prior\nmethods, which are usually developed and tuned for specific tasks. To summarize:\n(1) We proposed a vision-language model incorporating instruction tuning to enhance medical visual understanding in\nLLMs. The proposed model support multi-task analysis of various medical modalities and has achieved competitive\nperformance of generalization and interpretability.\n(2) We’ve established a novel framework for developing pretraining datasets and benchmark intended for custom\ninstruction tuning. Our method deviates from the conventional system of pair-wise supervision (an image and\nits corresponding label), utilizing instead cross-task training supervision for each specimen, which amplifies the\ncomprehension of correlations amongst tasks. We introduced a thorough benchmark for assessing the generalizability\nof large-scale foundation models when applied downstream to a variety of real-world clinical tasks.\n(3) The proposed model achieved competitive results on various medical benchmarks, showcasing few-shot general-\nization ability. Radiologist evaluation are also performed on the chest X-ray diagnoses generated by our model. In a\nblinded side-by-side ranking on 200 retrospective chest X-rays, three clinicians expressed a pairwise preference for\nMedViLaM results over those produced by radiologists in up to 80.50% of cases.\n2\nResults\n2.1\nMedViLaM provides accurate Classification and Localization of Disease\nWe evaluated the disease classification task on an assembled comprehensive benchmark from five private datasets\nand five publicly available datasets with both direct inference and fine-tuning settings. As illustrated in Figure 3, our\nmethod achieves competitive performance on 12 classes of the chest X-ray benchmark. Three radiologists independently\nevaluated the 12 classes of disease labels. Detailed AUC and F1 score of each disease on 10 test sets from 5 public\nlarge datasets and five private datasets of Chest X-ray images are described in the supplementary materials.\nWe further evaluated the performance of our model on 3D medical image inputs using both a classification task and a\nvisual grounding task shown in Figure 4. In the classification task, we determined whether there were plaques in the\nspecified branches (LM, LAD, LCX, RCA, etc.) of the coronary volume. The prompt for the classification task was\n\"Is there a plaque on LM?\" with \"yes\/no\" as the answer. We further confirmed the location of the plaques through the\nvisual grounding task. The prompt for the visual grounding task was \"Where is the plaque on LM?\" with the answer\ndescribed in the format of \"center at [x, y, z], box length is [a, b, c]\" using a 3D bounding box format. Additionally,\nwe trained the model for visual grounding tasks on cardiac structures to enable it to answer the positions of the left\nand right ventricles and atria. The 3D medical images were pre-trained on private data and fine-tuned on publicly\navailable datasets, namely ImageCAS, ASOCA, and CCA200. The overall accuracy (ACC) for plaque classification task\nwas 30.1%\/32.6%\/34.5%, showing variations in performance across different coronary artery branches, likely due to\n2\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nFigure 1: Illustration of MedViLaM. The joint training of multi-tasks from multi-modality is presented to improve the\nvisual grounding at various levels of granularity. During training, we only train the linear projection matrix (as indicated\nwith dashed boxes) while keeping the parameters of Vit and Vicuna frozen.\nFigure 2: Four modalities (image, video, volume, audio) and their associated datasets and multi-tasks: classification\n(CLS), segmentation (SEG), visual grounding (VG), VQA. The datasets and task objects share the same color.\n3\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nFigure 3: Model performance of Multi-label Classification task on our chest X-ray benchmark. Three radiologists\nindependently evaluated the 12 classes of disease labels. Detailed AUC and F1 score of each disease on 10 test sets from\n5 public large datasets and five private datasets of Chest X-ray images are described in the supplementary materials.\ndifferences in their morphological characteristics. For the visual grounding task of plaque localization, the overall ACC\nwas 70.1%\/73.2%\/75.1%. For the visual grounding task of cardiac structures, the overall ACC was 90.1%\/92.1%\/93.5%.\n2.2\nMedViLaM Supports Video and Audio Analysis\nWe chose endoscopic data for the application of our model in video analysis. We designed classification and visual\ngrounding tasks based on publicly available datasets to comprehensively diagnose videos (refer to Figure 5 Datasets).\nThe classification task involves identifying abnormalities, presence of polyps, instruments, specific human tissues, as\nwell as related collective names or categories. The visual grounding task focuses on determining the specific locations\nof polyps, instruments, tissues, etc., using bounding boxes to annotate a frame selected from the video. Due to the\nrichness of the polyp dataset, we primarily validated the model’s visual grounding performance on LPPolypVideo\/SUN-\nSEG\/CVC-12k datasets related to polyps, with ACC scores of 80.5%. For the relevant metrics of each task, please refer\nto the Supplementary Table.\n2.3\nGeneralizability in Unseen Disease Diagnosis and Foreign Object Detection\nTo further examine the generalization and scalability of our model, we conducted preliminary experiments of the\nreferring bbox detection task on 12 typical datasets across 6 modalities in the medical domain. The proposed model is\nfine-tuned with 20-shot labels for each disease of non-radiology and radiology datasets, respectively. We conducted\ncomparative experiments with VGTR and OFA(Large) as representatives of specialist and generalist models, respectively,\nto evaluate their performance and versatility in the referring bbox detection task.\n2.3.1\nNon-radiology Images\nEndoscopy We evaluated the generalization performance of instrument and disease localization on two typical\nendoscopy datasets, namely, EndoVis18 and LDPolyVideo. As depicted in Table. 2, ViLaM consistently outperforms\n4\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nFigure 4: The performance of our model on the 3D coronary artery visual grounding task is as follows: (a) A demo of\nthe visual grounding task dialogue; (b) Localization performance of different cardiac structures and plaques on different\ncoronary artery branches across various datasets, using ACC and mIoU as evaluation metrics.\nother approaches. Table. 3 further demonstrates that the proposed method achieves superior performance in multiple\nsurgical instrument categories. However, there is still room for improvement in some categories, which may be attributed\nto the issue of data imbalance.\nPhotography We evaluated the visual grounding performance of three methods on two datasets, ISIC16 and HAM10000,\nand found that all three methods achieved an accuracy of over 60% on both datasets. This is likely due to the fact that\nskin disease images and their corresponding features share similar characteristics.\nUltrasound We compared the visual grounding performance of three methods on two ultrasound datasets, TN3K and\nBUID, and our method achieved competitive results compared to the advanced specialist model. Specifically, our\nmethod achieved an accuracy of 16.50% on the TN3K validation set and 38.63% on the BUID breast cancer dataset.\n2.3.2\nRadiology Images\nCT We compared the visual grounding performance of three methods on two CT datasets, Luna16 and DeepLesion,\nand found that all three methods achieved nearly 0% accuracy in the 20-shot finetuning experiment on both datasets.\nThis is likely due to the fact that the features of CT images and general images are quite different, and the lesions, such\nas lung nodules, are too small, as shown in Fig.6 (d).\nMRI Two MRI datasets, the ADNI dataset and the LGG dataset, verify the visual grounding performance of three\nmethods. For Gliomas with larger contrast in the LGG dataset, we have better performance than VGTR. Our result of\nhippocampus detection is poor due to the low contrast of ADNI, as illustrated in Fig.6 (f).\n2.3.3\nMedical Foreign Object Detection\nWe evaluated ViLaM’s generalizability on the Object-CXR dataset [6] for foreign object detection in chest X-rays.\nVicuna-7b is utilized as the large language model in our framework. Without fine-tuning, ViLaM accurately answers\nquestions about foreign objects in sample images as presented in Fig.7a. It correctly states no foreign objects are\n5\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nFigure 5: Performance of our model on classification and visual grounding tasks (using box or points) for endoscopic\nvideos. The model is capable of classifying and localizing specific foreign objects and surgical instruments appearing in\nthe videos.\npresent in Fig.7a, while providing possible explanations. In Fig.7b, it localizes the foreign object and deduces it could\nbe metal\/plastic debris.\nIn Fig.7a, we pose the question, \"Is there anything foreign in this x-ray that is not part of the patient’s body?\". Our\nmodel accurately identifies the absence of any foreign objects, responding with, \"No, there is nothing visible in the\nx-ray that is not part of the patient’s body.\". Particularly, leveraging the expansive generalizability of the large language\nmodel, detailed descriptions explain why no foreign objects were found, and also point out other abnormalities that\nare not foreign objects, as highlighted in red. Furthermore, Fig.7b shows a case with foreign objects, by inquiring\nabout their presence. Our model accurately identifies the foreign object and provides its localization coordinates,\nthus demonstrating the model’s ability to visually detect foreign objects and generalize in a zero-shot setting. Further\ndemonstrating ViLaM’s generalization capabilities, the model can recognize the detected foreign object upon inquiry. It\ngoes beyond mere recognition by deducing that the object is likely made of metal or plastic debris. ViLaM also exhibits\nan ability to infer the potential origin or source of the debris, leveraging its extensive language understanding capacity.\nSubsequently, we fine-tuned our model on the object-CXR dataset, which further demonstrates its generalization\ncapabilities. We employed 8,000 training samples of chest X-ray images for fine-tuning, half of which contain foreign\nobjects. Our generalist model achieves an AUC of 93.1%, surpassing the JF Healthcare baseline [6] of 92.1%. This\nresult investigates the scalability and generalizability of our approach, which extends well to medically relevant tasks\nthrough large language models. Notably, when compared with classical and dedicated object detection methods, such\nas Fast-RCNN [7] of 95.7% and YOLO-v3 of 89.7% [8], our generalist vision-language model achieves a similar\nperformance. This provides further validation of our model’s generalizability, showing that a generalist model can\nachieve comparable results to a specialized model in certain tasks.\n3\nDiscussion\nWe aim to develop a single and unified model for various medical image modalities, with the goal of improving\nthe generalization and clinical interpretation. Our algorithm shows promising results in predicting all tasks through\n6\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nTable 1: Dataset overview for multi-task joint training and fine-tuning. The study consists of 14 individual tasks across\nfive task types and 35 datasets spanning seven modalities. We strictly follow the official train\/validation\/test split. For\nthe datasets without official split ratio, we randomly split them into train\/validation\/test sets by 7:1:2.\nTask\nDataset\nModality\nTarget\nTrain\nTest\nOfficial Split\nMulti-Task\nJoint Train\nClassification\nMIMIC-CXR\nChest X-ray\n26-class\n242.3k 5.1k\nY\nPadchest\nChest X-ray\n193-class\n128.5k 16.0k\nN\nVinDr_BodyPart\nX-ray\n5-class\n11.2k\n2.4k\nY\nMURA\nmusculoskeletal X-ray 7-class\n30.6k\n550\nY\nRSNA_BoneAge\nHand X-ray\nsingle class\n12.6k\n200\nY\nLocalization\nVinDr-CXR\nChest X-ray\n28-class\n15.0k\n3.0k\nY\nChestX-Det\nChest X-ray\n13-class\n2.7k\n500\nY\nVinDr_Mammo\nbreast X-ray\n10-class\n4.0k\n1.0k\nY\nVinDr_SpineXR\nX-ray\nSpinal lesions\n8.39k\n2.78k\nY\nASOCA\nCardiac CTA\nCoronary artery 30\n10\nY\nCCA200\nCardiac CTA\nCoronary artery 20\n10\nY\nSegmentation\nCheXmask\nChest X-ray\n14-class\n219.3k 10.0k\nN\nFracAtlas\nX-ray\n4-class\n570\n60\nY\nCholecSeg8k\nEndoscopy\nInstrument\n7\n2\nN\nHyper-Kvasir\nEndoscopy\nPolyp\n224\n37\nN\nReport Generation MIMIC-CXR\nChest X-ray\n14-class\n242.3k 5.1k\nY\nTask-Specific\nFine-tune\nClassification\nChestXray14\nChest X-ray\n14-class\n77.8k\n25.5k\nY\nCheXpert\nChest X-ray\n14-class\n223.4k 4.9k\nY\nRSNA\nChest X-ray\n14-class\n25.1k\n3.0k\nY\nLocalization\nChestXray14\nChest X-ray\n11-class\n690\n190\nY\nMS-CXR\nChest X-ray\n11-class\n800\n200\nN\nTBX11K\nChest X-ray\nTuberculosis\n20-shot 1.0k\nN\nRSNA Pneumonia Chest CT\nPneumonia\n20-shot 1.0k\nY\nISIC16\nPhotography\nSkin Lesions\n20-shot 379\nY\nHAM10000\nPhotography\nSkin Lesions\n20-shot 2.0k\nN\nTN3K\nUltrasound\nThyroid Nodule 20-shot 614\nY\nBUID\nUltrasound\nBreast Cancer\n20-shot 320\nN\nLuna16\nChest CT\nLung Nodule\n20-shot 125\nY\nDeepLesion\nChest CT\nLesion\n20-shot 660\nN\nADNI\nMR\nHippocampus\n20-shot 1.7k\nY\nLGG\nMR\nGliomas\n20-shot 680\nN\nSegmentation\nEndoVis18\nEndoscopy\nInstrument\n8.4k\n1.2k\nY\nLDPolypVideo\nEndoscopy\nPolyp\n7.0k\n1.0k\nY\nJSRT\nChest X-ray\n14-class\n170\n50\nN\nCheXmask\nChest X-ray\n14-class\n219.3k 10.0k\nN\nexperiments on various medical datasets and benchmarks. Additionally, we conducted a controlled trial and evaluation\nof the generated diagnosis results by three radiologists.\nMedViLaM enhances the generalizability and verify it in the comprehensive benchmarsk\nDuring data collection, medical images were acquired with different acquisition parameters or modalities have very\ndifferent characteristics. Besides,most AI algorithms for medical images have been developed with a very broad range\nof applications, data collection procedures and performance assessment metrics. The generalizability problem becomes\neven more conspicuous when a deep learning model trained on data from a given medical center is deployed to other\nmedical centers whose data have significant variations or there is a domain shift from the training set. Consequently,\nthose trained models depending on the variability of pretraining datasets and the bias in small benchmark often fail\nwhen deployed to real-world clinical scenarios.\nBy leveraging a carefully designed instruct tuning framework and incorporating diverse training strategies, our method\ncan effectively extract relevant features and make predictions for multiple tasks in medical images across various\n7\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nTable 2: Evaluation results of visual grounding task on 12 typical medical datasets of six modalities. 20-shot fine-tuning\nexperiments were performed for non-radiology (Endoscopy, Photography and Ultrasound) and radiology datasets (DR,\nCT and MRI). Acc@0.5 is applied to evaluate methods.\nDatasets\nEndoscopy\nPhotography\nUltrasound\nDR\nCT\nMRI\nEndoVis18 LDPolyp ISIC16 HAM10000 TN3K BUID TBX11K RSNA Luna16 DeepLesion ADNI LGG\nVGTR [4]\n3.87\n7.30\n64.12\n63.20\n12.70 31.46\n1.99\n4.67\n0.00\n0.36\n2.46\n3.67\nOFA [5]\n7.32\n0.30\n63.85\n61.20\n6.81\n19.62\n20.40\n14.67\n0.00\n2.08\n26.26 26.77\nOurs\n12.53\n9.86\n67.66\n86.00\n16.50 38.63\n30.84\n28.00\n0.00\n5.23\n4.52 18.85\nTable 3: Evaluation results of visual grounding task with 20-shot setting on seven labels from the EndoVis18 dataset.\nAcc@0.5 is applied to evaluate methods. Seven surgical instruments contain Bipolar Forceps (BF), Prograsp Forceps\n(PF), Large Needle Driver (LND), Monopolar Curved Scissors (MCS), Ultrasound Probe (UP), Suction Instrument (SI)\nand Clip Applier (CA).\nLabel\nMean\nBF\nPF\nLND MCS\nUP\nSI\nCA\nVGTR [4]\n3.87\n12.29\n0.00\n0.00\n14.78 0.00 0.00\n0.00\nOFA [5]\n7.32\n22.49 19.73\n0.94\n4.85\n0.00 0.00\n3.22\nOurs\n12.53 16.88\n4.17\n3.33\n13.55 0.00 0.00 16.67\nFigure 6: Typical medical datasets for referring bbox detection task, containing 6 modalities: (a) Endoscopy: EndoVis18,\nLDPolypVideo; (b) DR: TBX11k, RSNA Pneumonia; (c) Photography: ISIC16, HAM10000; (d) CT: Luna16,\nDeeplesion; (f) MRI: ADNI, LGG.\n8\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nNo, there is nothing visible in the x-ray that is not part of the pa-\ntient's body. The chest appears to be healthy, with no abnormalities \nvisible in the x-ray. The lungs look clear, and there is no evidence of \nany disease or inflammation in the chest area. The heart appears to \nbe normal in size and shape, and there is no evidence of any abnor-\nmalities in its structure or function. The ribs appear to be properly \naligned, and there is no evidence of any fractures or other injuries in \nthe chest area. There is also no evidence of any tumors or other ab-\nnormal growths in the chest area. The only abnormality that is visi-\nble in the x-ray is a small scar on the right side of the lung, near the \ntop of the image. This scar appears to be a healed fracture that oc-\ncurred at some point in the past, and it does not appear to be caus-\ning any problems or discomfort for the patient. The only other ab-\nnormality that is visible in the x-ray is a small nodule on the left side \nof the lung, near the bottom of the image.\nThis is a picture of a person's chest x-ray. Is there any-\nthing foreign in this x-ray that is not in the patient's body?\n(a) VQA in chest X-ray without foreign ob-\njects.\nCan you help me recognize the foreign object?\nThis is a picture of a person's chest x-ray. Is there any-\nthing foreign in this x-ray that is not in the patient's body?\nfind out by looking at the x-ray image in the region of \n[129, 0, 746, 753]\nYes, I can help you recognize the foreign object in the x-ray image. \nIt looks like there is a small piece of metal or plastic in the pa-\ntient's chest that is not part of the patient's body. It could be a \nmedical implant or a piece of debris that was left over from a surgi-\ncal procedure. The foreign object in the x-ray image appears to be a \nsmall piece of metal or plastic that is located in the patient's chest. \nIt could be a medical implant or a piece of debris that was left over \nfrom a surgical procedure. It is difficult to determine the exact \nnature of the object without more information about the patient's \nmedical history and the purpose of the x-ray.It is important to note \nthat the presence of a foreign object in an x-ray image does not nec-\nessarily indicate a problem or a medical issue. There are many rea-\nsons why a foreign object might appear in an x-ray image, and not \nall of them are necessarily cause for concern. If you have any ques-\ntions or concerns about a foreign object in an x-ray image, it is im-\nportant to consult with a healthcare professional for further infor-\nmation.\n(b) VQA in chest X-ray with foreign objects.\nThis is a picture of a person's \nchest x-ray. Where is the \ntuberculosis in the image?\nIt is in the center of the image, \nin front of the woman's hand \non the right side of the image. \nin the region of [620, 149, 803, \n457]\nR: [615, 156, 838, 455]\nY: [620, 149, 803, 457]\n(c) VQA in chest X-ray for the tuberculosis\nlocalization in the TBX11K dataset, without\nreferring expressions.\nThis is a picture of a person's \nchest x-ray. Where is the \npneumonia on the middle \nright side in the image?\n[496, 306, 697, 616] in this \nchest x-ray image\nR: [496, 302, 687, 596]\nY: [496, 306, 697, 616]\n(d) VQA in chest X-ray for the pneumonia\nlocalization in the RSNA dataset, with the\norientation-related referring expression.\nFigure 7: The zero-shot results of visual question answering for foreign objects detection in chest X-ray images. (a)\nChest X-ray image without foreign objects. Our model accurately states that there are no visible external foreign objects\nand points out the possible abnormality. (b) Chest X-ray image with foreign objects. The presence of foreign objects\nis accurately detected by giving its coordinates. Particularly, the model can deduce that the foreign object is metal or\nplastic by asking to recognize the foreign object.\nmodalities . This allows for a comprehensive analysis and diagnosis of various diseases and abnormalities in the\nimages. With unseen evaluation datasets, the proposed unified model has exhibited impressive performance in disease\nclassification and grounding tasks (via direct inference), surpassing the capabilities of existing state-of-the-art methods\nwith a few-shot fine-tune setting.\nMedViLaM provides reliable evidence for a better explainability\nOur previous study introduces an explainable approach for identifying diverse diseases in patients using chest radio-\ngraphs. It enhances the interpretability of chest X-ray reporting by generating more detailed information on disease\nattributes. This includes disease size, location, severity, and contour, providing stronger evidence for diagnosis and\ntreatment. Three radiologists further evaluate the generated reports against the recorded ones, which also exhibit the\nenhanced explainability of our multi-task model.\nIn this study, we further imporve the interpretability via large language model. The large language model could provide\nthe interpretability of the diagnosis result via detailed descriptions of the disease description and the accurate location of\nthe lesions. For instance, the disease category, severity level, and approximate location of the lesion could be preliminary\nverified with the disease entity classification and attribute classification task. The disease localization task could further\nprovide a more accurate bounding box of the lesion. For Pneumothorax and Cardiomegaly, the segmentation function\ncould provide an accurate assessment of disease degree by postprocessing the contours of the pneumothorax\/lung\/heart\nmask. These results together contribute to a better verification of the generated reports. In the blinded comparison from\nfour centers, three radiologists perceived the quality of 59% of the generated diagnoses to be equivalent to or even better\n9\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nthan the original physician reports. The proposed model can potentially serve as an appealing alternative solution to\nassist doctors and the research community in expediting the screening process for clinical cases.\nLimitation and Future Work\nOur proposed model exhibits competent performance across multiple tasks and enhances the generalizability and\nexplainability of the generated results. On the other hand, the established benchmark comes with several constraints,\nsuch as the limited size of the individual datasets and restricted modality and task diversity. Another significant hurdle\nin the development of models applicable across a broader variety of biomedical data types is the absence of large-scale\nmultimodal datasets. These would allow for joint learning and alignment of the modality-specific encoders with the\ndecoder.\n4\nMaterial and Method\n4.1\nModel Overview\nThe workflow is shown in Figure 1. Inspired by the advanced multi-modal models, MedViLaM leverages frozen\npretrained visual encoders and LLMs, encoding and aligning features for both images and text, and performs unified\nmodeling and joint training on downstream visual and language tasks. This enables MedViLaM to robustly perform\nvarious language and vision tasks based on instructions, providing diverse and complex output results. Benefiting\nfrom pre-trained language models and mutual guidance between tasks, MedViLaM can engage in continuous question-\nanswering and provide visual explanations of answers during conversations, which is particularly crucial in safety-critical\ndomains like medical diagnosis.\nImage Encoder: With an input image xi ∈RH×W , visual features are extracted by image encoder and further projected\nto feature dimension:\nvi = Pimg(Eimg(xi)) ∈R(hf ×wf )×d\n(1)\nwhere hf and wf are the output size of visual features, and d represents the feature dimension. Eimg can be any\ncommon visual backbones and\nMulti-modality Align: This module follows an encoder-decoder architecture format. Given the input visual features\nvi and text features li, we first generate fused multi-modal representations by combining the image and text embeddings.\nThese fused features serve as the keys and values in the cross-attention blocks in decoder. By conditioning on the partial\nsequence yi,<j predicted so far, the decoder recursively makes predictions for token at position j, effectively generating\naligned descriptions across modalities.\nyi,j = Dmm(Emm(concat(vi, li)), yi,<j) ∈R1×d\n(2)\nLarge Language Model Decoder: With the alignment tokens yi,j, a frozen LLM is used as the decoder to output final\nresults. If not specified, Vicuna-7B [9] is utilized as our large language model.\nWe employ ViT-L14 as our visual encoder, which consists of 14 transformer encoder layers and an FFN intermediate\nsize of 4,096. The input image size is set to 896 × 896, with a patch size of 64×64. The hidden dimensions of the\nViT-L14 are 1,024, with 16 attention heads. Meanwhile, we utilize Vicuna-7B, a large language model fine-tuned with\ninstructions, as our text encoder. The Vicuna-7B model boasts 12 transformer layers, with 768 hidden dimensions,\n12 attention heads, and an FFN intermediate size of 3,072. The vocabulary size is 30,522, and the maximum input\nsequence length is 512. To align the text encoder and visual encoder, we employ a Q-former with 12 transformer layers.\nThis Q-former has 768 hidden dimensions, 12 attention heads, and query, key, and value dimensions of 256 each.\nIn terms of the training progress, the hyperparameters are presented in Table.??. We utilize the AdamW optimizer,\nwhich is configured with a cosine annealing schedule as the learning policy. The initial learning rate is set to 2 × 10−5,\nand the AdamW optimizer is employed with hyperparameters β = (0.9, 0.98). Additionally, we set the weight decay to\n0.05 and the dropout rate to 0.1. During the first 1,000 warm-up steps, the learning rate increases to 2 × 10−5, and\nsubsequently decays to 10−7. Unless otherwise specified, our training protocol consists of 70,000 steps, executed on\n4 × 8 NVIDIA V100 GPUs, which takes approximately two days to complete.\nFor the annotation, We normalize all coordinates to a uniform range of 0 to 1000, ensuring that all images have a\nconsistent coordinate system. For the polygon representation, we select the point closest to the origin as the starting\npoint and employ a 25-point labelling scheme to describe the polygon sequence in a clockwise direction. To demarcate\nthe beginning and end of the sequence, we utilize <BOS> and <EOS> tags, respectively. For the sampling rule for\n10\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\npolygons, we employ isometric sampling, wherein we initially calculate the perimeter of the polygon and subsequently\ndivide it into 25 equal segments to sample the polygon.\n4.2\nBuilding Dataset for Customized Instruction Tuning\nIn this work, we constructed a multi-task dataset for joint training of disease classification, localization, segmentation,\nand report generation. In general, we unify the input and output labels of all sub-tasks into a uniform format for\nconsistent modeling and joint training, i.e., a set of image-instruction-label triplets as samples shown in Figure 1(c)\nand more in the supplementary materials. We further built a subset including the attributes and phrases for chest X-ray\nimages like \"small base effusion, normal cardiac silhouette,\" which can be used as instruction for the report generation\ntask. Additionally, the dataset underwent quality assurance by radiologists to ensure its accuracy and reliability.\nWe utilized public datasets for training and testing our proposed transformer model, e.g., MIMIC-CXR, VinDr-CXR,\nand ChestX-Det. We first sort out the aligned lesion categories of each dataset and the associated radiology report data\nand bounding box (BBox) data. We exclude the image datasets that are included in the test and validation datasets of\ndownstream tasks to avoid data leakage. Each dataset is described in detail as follows:\n• MIMIC-CXR [10] contains more than 377,110 radiograph images from over 227,835 radiographic studies.\nEach radiograph is paired with lesion classification and associated radiology report. We employ this dataset\nfor multi-label classification and report generation tasks.\n• Padchest [11] includes 160,868 images obtained from 67,625 patients, covering six different position views.\nIt has 174 different radiographic findings and 19 differential diagnosis, totaling 193 classes. They are used for\nthe classification task.\n• VinDr-CXR [12] includes chest radiographs with annotations for the classification of 28 common chest\ndiseases. The dataset contains 15,000 CXR scans in the training set. We select eight diseases from the dataset\nalong with their corresponding BBox for the disease localization task.\n• ChestX-Det [13] consists of 3,578 images from NIH ChestXray14[14] for 13 common disease. We select\nseven diseases from the dataset along with BBox for the disease localization task.\n• CheXpert [15] is a multi-label classification chest X-ray dataset with 224,316 images collected from 65,240\npatients. We extract 1% of the dataset to conduct a finetuning experiment for multi-diseases classification. We\nfollow MRM to focus on 5 diseases: Atelectasis, Cardiomegaly, Consolidation, Edema and Pleural Effusion.\nWe sample training\/test sets from the official training set and they constitutes 21,84\/5,000 images of the whole\ndataset.\n• MS-CXR [16] is sourced from MIMIC-CXR, and consists of 1,153 samples with BBOX and concise radiology\nreport, which is good for visual grounding finetuning. We randomly split it into training\/validation\/test sets by\n7:1:2 based on the patients, and evaluate the average performance of the model in all eight diseases.\n• ChestX-ray14 [14] is an available dataset for diagnosing 8 common lung diseases and localization of key\nfindings, with 984 radiograph images and hand-labelled BBOX. We directly conduct zero-shot experiment\nwith the entire dataset as the test set.\n• COVIDx CXR-4 [17] is an open-source dataset of COVID-19 chest images, which contains 84,818 images\nfrom 45,342 subjects. Six subsets are included in this dataset, i.e., Cohen [18], SIRM [19; 20], BIMCV [21],\nStonyBrook [22], and RICORD [23].\n• VinDr-Mammo [24] is a large-scale benchmark dataset of full-field digital mammography, called VinDr-\nMammo, which consists of 5,000 four-view exams with breast-level assessment and finding annotations. Each\nof these exams was independently double read, with discordance (if any) being resolved by arbitration by a\nthird radiologist.\n• VinDr-SpineXR [25] is a large-scale X-ray dataset for spinal lesions detection and classification. The\nVinDr-SpineXR contains 10,469 images from 5,000 studies that are manually annotated with 13 types of\nabnormalities, each scan was annotated by an expert radiologist.\n• VinDr-BodyPartXR [26] is currently the largest open dataset to date that provides annotations for developing\nsupervised-learning classification algorithms. of Body Parts from DICOM X-ray Scans. It includes 16,093\nX-ray images that are collected and manually annotated.\n• FracAtlas [27] includes 4,083 images that have been manually annotated for bone fracture classifcation,\nlocalization, and segmentation with the help of 2 expert radiologists and an orthopedist using the open-source\nlabeling platform, makesense.ai. There are 717 images with 922 instances of fractures. Each of the fracture\ninstances has its own mask and bounding box, whereas the scans also have global labels for classifcation tasks.\n11\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\n• MURA [28] consists of 14,863 studies from 12,173 patients, with a total of 40,561 multi-view radiographic\nimages,where each study is manually labeled by radiologists as either normal or abnormal. Each belongs\nto one of seven standard upper extremity radiographic study types: elbow, finger, forearm, hand, humerus,\nshoulder, and wrist.\n• MURA [29] consists of 14236 images with 0 labeled objects. There are 3 splits in the dataset: training (12611\nimages), validation (1425 images), and test (200 images). Alternatively, the dataset could be split into 2 image\nsplits: male (7706 images) and female (6530 images). Additionally, the images are tagged with boneage\n(months).\n• TN3K [30] dataset consists of 2D ultrasound images of thyroid nodules with a resolution of 512×512 for\nthyroid nodules detection.\n• BUID [31] dataset consists of 780 images with an average image size of 500×500 pixels from 600 female\npatients for breast cancer detection.\nDatasets for 3D Volume\n• ASOCA [32] has a training set of 40 Cardiac Computed Tomography Angiography (CCTA) with contrast\nagent showing the coronary arteries, comprising of 20 healthy patients and 20 patients with confirmed coronary\nartery disease.\n• CCA200 [33] contains 200 cases with coronary artery disease are collected named CCA-200 dataset. To\ndemonstrate the robustness of our model in small-scale data, comparative experiments are designed: 20 cases\nare used for training, and 180 cases for testing. The collected images are acquired with an isotropic resolution\nof 0.5 mm. Ground truths of 200 cases are coronary artery internal diameter annotations labeled by four\nradiologists.\n• ISIC16 [34] is a collection of dermoscopic images of skin lesions, annotated by dermatologists and skin\ncancer experts. It consists of 1,267 dermoscopic images of skin lesions, including melanomas and benign\nlesions, with a resolution of 1024×768 pixels.\n• Luna16 [35] dataset is a publicly available dataset for lung nodule analysis, specifically designed for lung\nnodule detection in CT scans.\n• DeepLesion [36] dataset is a large-scale, publicly available dataset for lesion detection and segmentation in\nCT, with a resolution of 512x512 pixels.\n• ADNI [37] is a large, publicly available dataset for Alzheimer’s disease research from magnetic resonance\nimaging (MRI) scans, specifically designed for the development and evaluation of algorithms for early detection\nand diagnosis of Alzheimer’s disease.\n• LGG [38] (Low-Grade Glioma) dataset is a publicly available dataset for brain tumor segmentation, specifically\nfor detecting low-grade gliomas from MRI scans.\nDatasets for Video\n• EndoVis18 [39] is a publicly available dataset for endoscopy image analysis. We follow ISINet’s annotation\nand data set division of surgical instrument categories[40].\n• CholecSeg8k [41] is a publicly available dataset for laparoscopic cholecystectomy (gallbladder removal) video\nanalysis, specifically designed for surgical instrument tracking tasks. It consists of 8,000 frames from 10\nlaparoscopic cholecystectomy videos, with a resolution of 640×480 pixels.\n• Cholecscopic [42] contains 76 colonoscopy videos recorded following a very simple protocol (identical to the\nusual protocol followed by the clinicians in their daily practice): the clinician has to record the lesion from\ndifferent viewpoints using both NBI and WL. The length of the video does not need to be larger than 30 seconds.\nThe main idea is to orbit around the lesion, recording it from different angles in order to allow us to apply the\nSfM algorithm. Every video is associated with ground truth from histopathology, the human operators’ opinion\n(including 4 experts and 3 beginners), and the calibration of every recording system (Olympus ExeraCV180\nand Olympus Exera-CV190) necessary for the 3D shape reconstruction. The dataset includes 15 serrated\nadenomas, 21 hyperplastic lesions and 40 adenoma.\n• Hyper-Kvasir [43] contains a total of 373 videos containing different findings and landmarks. This corresponds\nto approximately 11.62 hours of videos and 1,059,519 video frames that can be converted to images if needed.\nEach video has been manually assessed by a medical professional working in the field of gastroenterology and\nresulted in a total of 171 annotated findings.\n12\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\n• Kvasir-Capsule [44] contains 47,238 labeled images and 117 videos, where it captures anatomical landmarks\nand pathological and normal findings. The results is more than 4,741,621 images and video frames all together.\n• LPPolypVideo [45] consists of 44 colonoscopy videos for polyp detection, with a total of 18,142 frames, and\na resolution of 512×512 pixels.\n• Nerthus [46] consists of 21 videos with a total number of 5, 525 frames, annotated and verified by medical\ndoctors (ex- perienced endoscopists), including 4 classes showing four-score BBPS-defined bowel-preparation\nquality. The number of videos per class varies from 1 to 10. The number of frames per class varies from 500\nto 2, 700. The number of videos and frames is sufficient to be used for different tasks, e.g., image retrieval,\nmachine learning, deep learning and transfer learning, etc.. The dataset consists of videos with resolution\n720x576 and is organized by sorting the videos into separate folders named according to their BBPS-bowel\npreparation quality score.\n• PolypDiag [47] collected colonoscopy videos from two widely used public datasets: Hyper-Kvasir[43] and\nLDPolypVideo [45]. The new dataset contains 61 normal videos without polyps and 102 abnormal videos\nwith polyps for training, and 30 normal videos and 60 abnormal videos for testing. The videos in the training\nset have video-level labels and the videos in testing set contain frame-level labels. This dataset contains over\none million frames and has diverse polyps with various sizes and shapes.\n• SUN-SEG [48] is a high-quality per-frame annotated VPS dataset, which includes 158,690 frames elected\nfrom the famous SUN dataset.\nDatasets for Audio\n• Coswara1 [49] has 6507 clean, 1117 noisy, and remaining highly degraded audio files corresponding to\nrespiratory sound samples from 941 participants. The audio samples are recorded at a sampling frequency of\n48 kHz. All sound files were manually curated. A web interface was designed allowing the annotator (human)\nto listen to every sound file and answer some questions. These questions helped verify the category label, and\nthe quality of the audio file. The annotator was also provide an option to provide any additional comments for\neach audio file.\n4.3\nClinical Evaluation of Report Generation\nTo further validate the clinical applicability of reports generated by our model, we conducted a comprehensive evaluation\nwith two experienced radiologists. We selected a total of 200 cases for evaluation, including 150 cases from three\ndifferent medical facilities and 50 cases from the MIMIC test set. To match the intended inputs of our model, we\nexcluded cases that mentioned multiple imaging views or comparisons to prior test results in the generate reports.\nOur study involved two distinct yet complementary human evaluations: (a) a parallel evaluation, where raters compared\nand ranked alternative reports based on their quality, and (b) an independent evaluation conducted to assess the quality\nof each individual reports.\nParallel evaluation Each of the 200 cases was evaluated by one radiologist randomly chosen from a pool of two. Both\ncenter reports and generated reports were available for each case. The radiologists, who were unaware of the source of\nthe reports, reviewed them in a randomized sequence.\nIndependent evaluation Raters were provided with a single chest X-ray, the disease findings, and a reference report\nfrom different centers. They were tasked with assessing the quality of the report produced by MedViLaM. We followed\nevaluation methodology proposed by Yu et al. [50]. The raters needed to determine whether there were discrepancies\n(errors), any missing elements (omissions), or inaccurate descriptions (e.g., location and severity) in the generated\nreport and evaluate their clinical significance.\nCode and Data Availability\nCode for training and evaluation is available at https:\/\/github.com\/MedHK23\/MedViLaM The new dataset released\nin this study can be found at https:\/\/huggingface.co\/datasets\/MedHK23\/MedViLaM.\nAcknowledgements\nThis research was partially supported by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the\nInnovation and Technology Commission (ITC)’s InnoHK (L.X., H.L. and S.Z.). H.L. and S.Z. are PI and co-PI of the\nCPII. Thanks to Xiaoyu Yang, Xinglong Liu and Xiaosong Wang for their work in this study.\n13\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\nAuthor contributions\nAll authors have contributed fully to the concept and design of the study. LX and ZN collected the clinical data,\nperformed the experiments, and analyzed the experiment results. LX performed the comparative experiments with other\nmethods and drafted the manuscript. SZ, and HL supervised the projects and gave final approval of the manuscript. All\nauthors have carefully read and approved the final manuscript.\nCompeting interests\nThe authors declare no competing interests.\nReferences\n[1] OpenAI. Gpt-4o(ision) system card, 2024.\n[2] Meta. Llama3.2, 2024.\n[3] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.\n[4] Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang. Visual grounding with transformers. In 2022 IEEE\nInternational Conference on Multimedia and Expo (ICME), pages 1–6. IEEE, 2022.\n[5] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence\nlearning framework, 2022.\n[6] JF Healthcare. Object-cxr - automatic detection of foreign objects on chest x-rays.\n[7] Ross Girshick. Fast R-CNN. pages 1440–1448.\n[8] Joseph Redmon and Ali Farhadi. YOLOv3: An Incremental Improvement.\n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, March 2023.\n[10] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-\nying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest\nradiographs with free-text reports. Scientific data, 6(1):317, 2019.\n[11] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A large chest x-ray\nimage dataset with multi-label annotated reports. Medical Image Analysis, 66, 2020.\n[12] Ha Q Nguyen, Khanh Lam, Linh T Le, Hieu H Pham, Dat Q Tran, Dung B Nguyen, Dung D Le, Chi M Pham,\nHang TT Tong, Diep H Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist’s annotations.\nScientific Data, 9(1):429, 2022.\n[13] Jie Lian, Jingyu Liu, Shu Zhang, Kai Gao, Xiaoqing Liu, Dingwen Zhang, and Yizhou Yu. A structure-aware\nrelation network for thoracic diseases detection and segmentation. IEEE Transactions on Medical Imaging,\n40(8):2042–2052, 2021.\n[14] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8:\nHospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of\ncommon thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 2097–2106, 2017.\n[15] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\nBehzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty\nlabels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages\n590–597, 2019.\n[16] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer, Stephanie Hyland,\nMaria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics\nto improve biomedical vision–language processing. In European conference on computer vision, pages 1–21.\nSpringer, 2022.\n[17] Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-net: a tailored deep convolutional neural network\ndesign for detection of covid-19 cases from chest x-ray images. Scientific Reports, 10(1):19549, Nov 2020.\n14\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\n[18] Joseph Paul Cohen, Paul Morrison, and Lan Dao. Covid-19 image data collection. arXiv 2003.11597, 2020.\n[19] Muhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Abdul Kadir,\nZaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al Emadi, Mamun\nBin Ibne Reaz, and Mohammad Tariqul Islam. Can ai help in screening viral and covid-19 pneumonia? IEEE\nAccess, 8:132665–132676, 2020.\n[20] Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin Abul Kashem,\nMohammad Tariqul Islam, Somaya Al Maadeed, Susu M. Zughaier, Muhammad Salman Khan, and Muham-\nmad E.H. Chowdhury. Exploring the effect of image enhancement techniques on covid-19 detection using chest\nx-ray images. Computers in Biology and Medicine, 132:104319, 2021.\n[21] Maria de la Iglesia Vayá, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel\nCazorla, Joaquin Galant, Xavier Barber, Domingo Orozco-Beltrán, Francisco García-García, Marisa Caparrós,\nGermán González, and Jose María Salinas. Bimcv covid-19+: a large annotated dataset of rx and ct images from\ncovid-19 patients, 2020.\n[22] Joel Saltz, Mary Saltz, Prateek Prasanna, Richard Moffitt, Janos Hajagos, Erich Bremer, Joseph Balsamo, and\nTahsin Kurc. Stony brook university covid-19 positive cases. the cancer imaging archive, 4, 2021.\n[23] Emily B Tsai, Scott Simpson, Matthew P Lungren, Michelle Hershman, Leonid Roshkovan, Errol Colak, Bradley J\nErickson, George Shih, Anouk Stein, Jayashree Kalpathy-Cramer, et al. The rsna international covid-19 open\nradiology database (ricord). Radiology, 299(1):E204–E213, 2021.\n[24] Hieu T. Nguyen, Ha Q. Nguyen, Hieu H. Pham, Khanh Lam, Linh T. Le, Minh Dao, and Van Vu. Vindr-mammo:\nA large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. medRxiv, 2022.\n[25] Hieu T Nguyen, Hieu H Pham, Nghia T Nguyen, Ha Q Nguyen, Thang Q Huynh, Minh Dao, and Van Vu.\nVindr-spinexr: A deep learning framework for spinal lesions detection and classification from radiographs. In\nMedical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference,\nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 291–301. Springer, 2021.\n[26] Hieu H Pham, Dung V Do, and Ha Q Nguyen. Dicom imaging router: An open deep learning framework for\nclassification of body parts from dicom x-ray scans. arXiv preprint arXiv:2108.06490, 2021.\n[27] Iftekharul Abedeen, Md Ashiqur Rahman, Fatema Zohra Prottyasha, Tasnim Ahmed, Tareque Mohmud Chowd-\nhury, and Swakkhar Shatabda. Fracatlas: A dataset for fracture classification, localization and segmentation of\nmusculoskeletal radiographs. Scientific Data, 10(1):521, 2023.\n[28] Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang, Kaylie Zhu,\nDillon Laird, Robyn L Ball, et al. Mura: Large dataset for abnormality detection in musculoskeletal radiographs.\narXiv preprint arXiv:1712.06957, 2017.\n[29] Safwan S Halabi, Luciano M Prevedello, Jayashree Kalpathy-Cramer, Artem B Mamonov, Alexander Bilbily,\nMark Cicero, Ian Pan, Lucas Araújo Pereira, Rafael Teixeira Sousa, Nitamar Abdala, et al. The rsna pediatric\nbone age machine learning challenge. Radiology, 290(2):498–503, 2019.\n[30] Haifan Gong, Jiaxin Chen, Guanqi Chen, Haofeng Li, Guanbin Li, and Fei Chen. Thyroid region prior guided\nattention for ultrasound segmentation of thyroid nodules. Computers in biology and medicine, 155:106389, 2023.\n[31] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images.\n28:104863.\n[32] Ramtin Gharleghi, Dona Adikari, Katy Ellenberger, Sze-Yuan Ooi, Chris Ellis, Chung-Ming Chen, Ruochen\nGao, Yuting He, Raabid Hussain, Chia-Yen Lee, et al. Automated segmentation of normal and diseased coronary\narteries–the asoca challenge. Computerized Medical Imaging and Graphics, 97:102049, 2022.\n[33] Xiaoyu Yang, Lijian Xu, Simon Yu, Qing Xia, Hongsheng Li, and Shaoting Zhang. Segmentation and vascular\nvectorization for coronary artery by geometry-based cascaded neural network. IEEE Transactions on Medical\nImaging, pages 1–1, 2024.\n[34] David Gutman, Noel CF Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, and Allan Halpern.\nSkin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical\nimaging (isbi) 2016, hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1605.01397,\n2016.\n[35] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard,\nPiergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and\ncombination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the\nluna16 challenge. Medical image analysis, 42:1–13, 2017.\n15\nMedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data\nunderstanding and generation\nA PREPRINT\n[36] Ke Yan, Xiaosong Wang, Le Lu, and Ronald M Summers. Deeplesion: automated mining of large-scale lesion\nannotations and universal lesion detection with deep learning. Journal of medical imaging, 5(3):036501–036501,\n2018.\n[37] Susanne G. Mueller, Michael W. Weiner, Leon J. Thal, Ronald C. Petersen, Clifford R. Jack, William Jagust,\nJohn Q. Trojanowski, Arthur W. Toga, and Laurel Beckett. Ways toward an early diagnosis in Alzheimer’s disease:\nThe Alzheimer’s Disease Neuroimaging Initiative (ADNI). 1(1):55–66.\n[38] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B\nFreymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer genome atlas glioma mri collections\nwith expert segmentation labels and radiomic features. Scientific data, 4(1):1–13, 2017.\n[39] Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi, Imanol Luengo,\nFelix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, et al. 2018 robotic scene segmentation\nchallenge. arXiv preprint arXiv:2001.11190, 2020.\n[40] Cristina González, Laura Bravo-Sánchez, and Pablo Arbelaez. Isinet: an instance-based approach for surgical\ninstrument segmentation. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pages 595–605. Springer, 2020.\n[41] Andru P Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel De Mathelin, and Nicolas Padoy.\nEndonet: a deep architecture for recognition tasks on laparoscopic videos. IEEE transactions on medical imaging,\n36(1):86–97, 2016.\n[42] Pablo Mesejo, Daniel Pizarro, Armand Abergel, Olivier Rouquette, Sylvain Beorchia, Laurent Poincloux, and\nAdrien Bartoli. Computer-aided classification of gastrointestinal lesions in regular colonoscopy. IEEE transactions\non medical imaging, 35(9):2051–2063, 2016.\n[43] Hanna Borgli, Vajira Thambawita, Pia H Smedsrud, Steven Hicks, Debesh Jha, Sigrun L Eskeland, Kristin Ranheim\nRandel, Konstantin Pogorelov, Mathias Lux, Duc Tien Dang Nguyen, et al. Hyperkvasir, a comprehensive multi-\nclass image and video dataset for gastrointestinal endoscopy. Scientific data, 7(1):1–14, 2020.\n[44] Pia H Smedsrud, Vajira Thambawita, Steven A Hicks, Henrik Gjestang, Oda Olsen Nedrejord, Espen Næss, Hanna\nBorgli, Debesh Jha, Tor Jan Derek Berstad, Sigrun L Eskeland, Mathias Lux, Håvard Espeland, Andreas Petlund,\nDuc Tien Dang Nguyen, Enrique Garcia-Ceja, Dag Johansen, Peter T Schmidt, Ervin Toth, Hugo L Hammer,\nThomas de Lange, Michael A Riegler, and Pål Halvorsen. Kvasir-Capsule, a video capsule endoscopy dataset.\nScientific Data, 8(1):142, 2021.\n[45] Yiting Ma, Xuejin Chen, Kai Cheng, Yang Li, and Bin Sun. Ldpolypvideo benchmark: a large-scale colonoscopy\nvideo dataset of diverse polyps. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021:\n24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages\n387–396. Springer, 2021.\n[46] Konstantin Pogorelov, Kristin Ranheim Randel, Thomas de Lange, Sigrun Losada Eskeland, Carsten Griwodz,\nDag Johansen, Concetto Spampinato, Mario Taschwer, Mathias Lux, Peter Thelin Schmidt, Michael Riegler, and\nPål Halvorsen. Nerthus: A bowel preparation quality video dataset. In Proceedings of the 8th ACM on Multimedia\nSystems Conference, MMSys’17, pages 170–174, New York, NY, USA, 2017. ACM.\n[47] Yu Tian, Guansong Pang, Fengbei Liu, Yuyuan Liu, Chong Wang, Yuanhong Chen, Johan Verjans, and Gustavo\nCarneiro. Contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 88–98.\nSpringer, 2022.\n[48] Ge-Peng Ji, Guobao Xiao, Yu-Cheng Chou, Deng-Ping Fan, Kai Zhao, Geng Chen, and Luc Van Gool. Video\npolyp segmentation: A deep learning perspective. Machine Intelligence Research, 19(6):531–549, 2022.\n[49] Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Prasanta Kumar\nGhosh, Sriram Ganapathy, et al. Coswara–a database of breathing, cough, and voice sounds for covid-19 diagnosis.\narXiv preprint arXiv:2005.10548, 2020.\n[50] Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Uru-\nrahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al. Evaluating\nprogress in automatic chest x-ray radiology report generation. medRxiv, pages 2022–08, 2022.\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation.pdf"}
{"title":"Generative Visual Instruction Tuning","authors":"Jefferson Hernandez, Ruben Villegas, Vicente Ordonez","summary":"We propose to use automatically generated instruction-following data to\nimprove the zero-shot capabilities of a large multimodal model with additional\nsupport for generative and image editing tasks. We achieve this by curating a\nnew multimodal instruction-following set using GPT-4V and existing datasets for\nimage generation and editing. Using this instruction set and the existing\nLLaVA-Finetune instruction set for visual understanding tasks, we produce\nGenLLaVA, a Generative Large Language and Visual Assistant. GenLLaVA is built\nthrough a strategy that combines three types of large pretrained models through\ninstruction finetuning: Mistral for language modeling, SigLIP for image-text\nmatching, and StableDiffusion for text-to-image generation. Our model\ndemonstrates visual understanding capabilities superior to LLaVA and\nadditionally demonstrates competitive results with native multimodal models\nsuch as Unified-IO 2, paving the way for building advanced general-purpose\nvisual assistants by effectively re-using existing multimodal models. We\nopen-source our dataset, codebase, and model checkpoints to foster further\nresearch and application in this domain.","url":"http:\/\/arxiv.org\/abs\/2406.11262v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2406.11262v2","published":1718608018000,"comment":"Add more results using task tokens, expand the introduction and\n  related work FIX: error in LLM-as-judge evaluation that was over-inflating\n  the results","pdf_text":"GENERATIVE VISUAL INSTRUCTION TUNING\nJefferson Hernandez1, Ruben Villegas2, Vicente Ordonez1\n1Rice University\n2Google DeepMind\nABSTRACT\nWe propose to use automatically generated instruction-following data to improve the zero-shot\ncapabilities of a large multimodal model with additional support for generative and image editing\ntasks. We achieve this by curating a new multimodal instruction-following set using GPT-4V and\nexisting datasets for image generation and editing. Using this instruction set and the existing LLaVA-\nFinetune instruction set for visual understanding tasks, we produce GenLLaVA, a Generative Large\nLanguage and Visual Assistant. GenLLaVA is built through a strategy that combines three types of\nlarge pretrained models through instruction finetuning: Mistral for language modeling, SigLIP for\nimage-text matching, and StableDiffusion for text-to-image generation. Our model demonstrates\nvisual understanding capabilities superior to LLaVA and additionally demonstrates competitive results\nwith native multimodal models such as Unified-IO 2, paving the way for building advanced general-\npurpose visual assistants by effectively re-using existing multimodal models. We open-source our\ndataset, codebase, and model checkpoints to foster further research and application in this domain1.\n1\nIntroduction\nThe field of multimodal models has become increasingly popular in the research community as they are one of the key\nbuilding blocks for general-purpose assistants (Achiam et al., 2023; Gemini Team et al., 2023; Bai et al., 2023). One\nof the main directions researchers have pursued is to combine Large Language Models (LLMs) with Vision Models\nfor multimodal tasks i.e. creating LVLMs. The recently proposed LLaVA model (Liu et al., 2023b) is among the\nlatest wave of works that have demonstrated the effectiveness of instruction tuning for multimodal models (Liu et al.,\n2024a; Zhao et al., 2023; Wang et al., 2023a; Karamcheti et al., 2024). In these works, a two-stage pipeline is followed:\n(1) multimodal pre-training where the unimodal models are combined and trained on a large corpus of captioning\ndata Schuhmann et al. (2022); Ordonez et al. (2011); and (2) a supervised fine-tuning (SFT) Liu et al. (2024a,b) stage\nwhere the model is trained on domain-specific data and enables it to better perform various downstream tasks of interest.\nVisual generation is another research direction that combines visual and language modalities. There are two common\napproaches for text-to-image generation. One approach employs diffusion models (Rombach et al., 2022), which\nhave shown unprecedented performance in image synthesis, becoming the de facto method for visual generation. The\nother line of work converts visual content into discrete tokens using vector quantization (VQ) and then leverages an\nautoregressive transformer for high-quality image synthesis (Chang et al., 2022; Lee et al., 2022).\nAs visual understanding and generation capabilities advance rapidly and independently, a growing trend is to combine\nthese into a unified Large Multimodal Model (LMM). There are two main approaches to achieving such unification.\nMany LMMs (Koh et al., 2024; Sun et al., 2024b; Fu et al., 2024) produce conditional embeddings to be used by a\npretrained diffusion model for image generation. On the other hand, there are LMMs (Lu et al., 2024a; Chameleon\nTeam, 2024; Yu et al., 2023) that adopt VQ encoders to project visual inputs into discrete tokens and use the same\nnext-token prediction paradigm as Language Models.\nThere has been a considerable amount of work building on top of the LLaVA model ranging from image generation (Koh\net al., 2024; Sun et al., 2024b,a), grounding (You et al., 2024), image editing (Fu et al., 2024) to video understanding (Lin\net al., 2024). These works share the same principles; they extend the ideas of visual instruction tuning to one or more\ncapabilities. However, after adding a new capability (i.e., image generation), the resulting models often lose some, if\n1https:\/\/github.com\/jeffhernandez1995\/GenLlaVA.git\narXiv:2406.11262v2  [cs.CV]  2 Oct 2024\nGenerative Visual Instruction Tuning\nFigure 1: Comparison of GenLLaVA against recent architectures. Unlike BLIP-2 (Li et al., 2023), we use a Linear\nprojector similar to the LlaVA architecture (Liu et al., 2023b). Generation capabilities are added using a diffusion\nmodel, but unlike GILL (Koh et al., 2024), we use a Q-former as the generation head. Finally, our model benefits from\nusing a stronger visual encoder, namely SigLIP(Zhai et al., 2023); a stronger LLM, namely Mistral-7b (Jiang et al.,\n2023); and a stronger diffuser, namely SDv1.4 (Rombach et al., 2022). ∗L stands for Linear projection, and Q stands for\nQ-former resampler.\nnot most, of their visual and language understanding capabilities. We propose a strategy that leads to a model that can\nperform generative tasks while retaining multimodal understanding capabilities.\nIn this paper, we present generative visual instruction tuning, an approach in which we teach a Large Multimodal Model\n(LMM) image understanding, image generation, and image editing tasks without diminishing the performance of each\nindividual capability. (See Fig. 1 for an overview of our method.) To our knowledge, this is the first time such capability\nhas been achieved, and our findings pave the way for building a general-purpose visual assistant. Our contributions are\nthe following:\n• Generative multimodal instruction-following data. Inspired by Liu et al. (2023b), which curated an instruction\nset for image understanding tasks, we curate a multimodal instruction tuning set that combines image\nunderstanding, image generation, and image editing data.\n• A single composite model, i.e. GenLLaVA, which unifies visual understanding and generation using open-\nsource models. GenLLaVA is trained using a single-stage training recipe, unlike its predecessor LLaVA Liu\net al. (2024a).\n• Open source. We will publicly release our generated multimodal instruction data, code to replicate our results,\nmodel checkpoints, and a visual chat demo.\n2\nRelated Work\nLarge Multimodal Models (LMMs).\nLarge Multimodal Models (LMMs) refer to large language models that can\nunderstand various modalities beyond human language. Some research efforts are focused on combining image, audio,\nvideo, and other modalities with language (Zhan et al., 2024; Lu et al., 2024a), while others aim to enhance the fusion\nof vision knowledge and language. For example, BLIP-2 (Li et al., 2023) created a large-scale image captioning dataset\nand paired a language model with a vision encoder to produce a robust multimodal model. Following this, LLaVA (Liu\net al., 2023b) developed a cost-effective approach to train an advanced LLM through visual instruction tuning. Although\nLLaVA-NeXT (Liu et al., 2024b) improved performance for single-image tasks, it required over 2,000 tokens per image,\nwhich is about four times more than the original LLaVA. More recent models such as QwenVL (Bai et al., 2023),\nCogVLM (Wang et al., 2023b), and Yi-VL (Young et al., 2024) follow architectures similar to those of LLaVA. Our\nproposed method not only focuses on models for multimodal understanding but also on adding generative capabilities\nto such models.\nDiffusion-based LMMs for visual generation\nWe review works that combine diffusion with autoregressive prediction\nto create LMMs for generative tasks. For instance, GILL (Koh et al., 2024) translates the hidden representations of\nan LLM into embeddings that correspond to a text-to-image model by learning a neural network to perform efficient\nmapping using the text encoder of the diffusion model. MGIE (Fu et al., 2024) adapts the text embedder, image input\n2\nGenerative Visual Instruction Tuning\nIn: Can you change this image so that the \ncolor of the fur in the rabbit is yellow?\nOut: Here’s the image of the rabbit with \nyellow fur.\nGPT4 + DALL·E \nGPT4o + Code\nIn: Can you change this image so that the \ncolor of the fur in the rabbit is yellow?\nOut: I have changed the rabbit’s fur color to \nyellow.\nGenLLaVA\nIn: Can you change this image so that the \ncolor of the fur in the rabbit is yellow?\nOut: An image of a rabbit with yellow fur.\nFigure 2: Editing capabilities of our model. GPT4 currently uses a version of the DALLE text-to-image model as a tool\nand, hence, is not directly able to edit images. GPT4o instead uses tools through Python-generated code to accomplish\nthe requested action. Our model, GenLLaVA, connects input features obtained from CLIP to a language model that also\nproduces output embeddings for a text-to-image StableDiffusion model, achieving an end-to-end editing task with a\nmultimodal model.\nadapter, and LM head output parameters of an LMM jointly with a diffusion model for image editing from instructions.\nDreamLLM (Dong et al., 2024) uses the same paradigm as GILL and MGIE but instead trains on interleaves documents\nfor visual generation and understanding synergy. Transfusion (Zhou et al., 2024) combines the next-token production\nobjective with the using a bidirectional casual mask on the visual tokens in a single unified model to understand and\ngenerate both discrete and continuous modalities. Show-O (Xie et al., 2024) is concurrent to Transfusion and similarly\nuses a bidirectional casual mask on the visual tokens but uses an extra masking loss similar to MaskGIT (Chang et al.,\n2022).\nToken-based LMMs for visual generation\nWe review works that project visual features into discrete tokens and\nuse next-token prediction for generative tasks. AnyGPT (Zhan et al., 2024) discretizes data from multiple modalities,\nextends the existing LLM vocabulary to add the extra modalities, and incorporates new randomly initialized parameters\nthat enable additional input embeddings and prediction outputs. Codi-2 (Tang et al., 2024) follows a similar line and\nadds multiple decoders for image, audio, and video generation to condition an LLM to generate interleaved outputs\nfrom different modalities. CM3leon (Yu et al., 2023) proposes an early-fusion token-based decoder-only mixed modal\nmodel based on the CM3 architecture that is capable of both text and image generation and editing. Unified-IO 2 (Lu\net al., 2024a), Chameleon (Chameleon Team, 2024) and GPT-4o OpenAI (2024) take the early-fusion fully multimodal\napproach training the model from scratch to be able to expand the number of supported tasks and modalities.\n3\nMethod\n3.1\nBackground: Large Multimodal Models\nLarge language models (LLMs) excel in natural language generation, while Large Multimodal Models enhance LLMs\nwith the ability to interpret images and respond accordingly. Built upon a pre-trained LLM, the LMM incorporates a\nvisual encoder (e.g., CLIP (Radford et al., 2021)) to derive visual features f, along with an adapter W that maps f into\nthe language domain. Following the training methodology of LLaVA (Liu et al., 2023b), this process is encapsulated in\nthe equation:\nC = {x1, x2, . . . , xl},\nf = Encvis(V),\nxt = LMM({x1, . . . , xt−1} | W(f)),\n(1)\n3\nGenerative Visual Instruction Tuning\nwhere l represents the number of tokens within C. The set C can represent an image caption (Features Alignment) or\nmultimodal instruction-following data (Instruction Tuning). The LMM employs the standard autoregressive method for\nnext-token prediction, allowing it to function as a visual assistant across diverse tasks such as visual question answering\nand complex reasoning. Despite gaining visual perceptive abilities through this training, its responses are currently\nconstrained to text.\n3.2\nVisual Generation in Large Multimodal Models\nWe append N visual tokens [IMG] after the instruction E, with their word embeddings being trainable. The LMM\nlearns to generate these tokens through its language modeling (LM) head. These visual tokens represent visual-related\ninstruction comprehension within E and form a bridge between the language and vision modalities. We follow the same\nvisual generation framework of GILL (Koh et al., 2024) and MGIE (Fu et al., 2024) in extracting visual features, which\nwe summarize here for succinctness.\nWe employ a generation head T to convert [IMG] into concrete visual guidance. The model T is a sequence-to-\nsequence model that translates the sequential visual tokens from the LMM into the semantically meaningful latent set\nU = {u1, u2, . . . , uL} for visual guidance:\nut = T ({u1, . . . , ut−1} | {e[IMG] + h[IMG]}),\n(2)\nwhere e denotes the word embedding and h is the hidden state (from the final layer of the LMM before the LM head)\nof [IMG]. Specifically, the transformation applied to e serves as a broad visual representation, while h provides an\ninstance-specific visual latent that reflects both the original image and the text conditioning the generation.\nTo guide image generation with the visual latent information U, we employ a latent diffusion model (Rombach\net al., 2022), incorporating a variational autoencoder (VAE) for handling denoising diffusion in the latent space.\nFirst, we encode the desired visual output via the diffusion model encoder o = EncVAE(O); this output may be\nintended for image generation or editing tasks. The diffusion process progressively introduces noise into o as zt,\nincreasing the noise level over timesteps t. We then train the UNet ϵθ to predict the added noise (Ho et al., 2020).\nThe diffusion process is conditioned on the visual latent information U through a cross-attention layer, defined as\nAttention(Q, K, V ) = softmax( QKT\n√\ndim ) · V , where:\nQ = W (i)\nQ · φi(zt), K = W (i)\nK · U, V = W (i)\nV\n· U,\n(3)\nwith φ representing the flattening operation, and W (i)\nQ , W (i)\nK , and W (i)\nV\nbeing learnable attention matrices. We apply\nclassifier-free guidance (Ho & Salimans, 2021), where the score estimation sθ is extrapolated to deviate from the\nunconditional ∅, following standard practices in diffusion models.\n4\nExperiment Settings\n4.1\nGenerative Visual Instruction Data\nMultimodal instruction tuning is a crucial process that equips the model with a wide range of skills and capabilities\nacross different modalities while also enabling it to adapt to novel and unique instructions. We build the multimodal\ninstruction tuning dataset by aggregating a diverse set of supervised datasets and tasks. Each task is provided with a\nclear prompt, either by using existing prompts or crafting new ones using GPT4-V.\nNatural Language. [1.93%]\nWe use the publicly available ShareGPT (ShareGPT, 2023) dataset, which was used to\ntrain the Vicuna LLM (Chiang et al., 2023). This dataset contains mostly English natural language conversations but\nalso contains code and markdown. We filter inappropriate or low-quality entries following the same methodology of\nChiang et al. (2023). As a final preprocessing step, entries that surpass 2048 tokens are truncated rather than split into\nmultiple conversations. This results in ∼40K conversations.\nImage Editing. [9.63%]\nWe create a subset of the Instruction Prompt-to-Prompt dataset (IPr2Pr) (Brooks et al., 2023)\nfor editing our editing data. We use ∼200K from the CLIP-filtered data version of IPr2Pr, where editing instructions\nare generated by GPT-3, and images are synthesized by the Prompt-to-Prompt model (Hertz et al., 2023).\n4\nGenerative Visual Instruction Tuning\nIn: What is happening in this \nimage?\nOut: A monkey is holding a \nknife and a banana.\nIn: Can you generate a \nsimilar image?\nOut: \nIn: Can you make the fur of \nthe monkey blue?\nOut: \nGILL\nMGIE\nIn: What is happening in this \nimage?\nOut: A monkey is holding two \nswords in its hands, standing \nin front of a group of \npeople.The monkey appears \nto be the main focus of the \nscene, and it seems to be \nengaging with the people \naround it.\nIn: Can you generate a \nsimilar image?\nIn: Can you make the fur of \nthe monkey blue?\nOut: \nUnified-IO 2\nIn: What is happening in this \nimage?\nOut: The image features a \nsmall, brown monkey holding \na large pair of scissors in its \nmouth. The monkey is \npositioned in the center of the \nscene, with the scissors in its \nhand. \nIn: Can you generate a \nsimilar image?\nOut: \nIn: Can you make the fur of \nthe monkey blue?\nGenLLaVA\nIn: What is happening in this \nimage?\nOut: A monkey is holding two \nknives in its hands and has \nanother knife hanging from its \nmouth. The monkey appears \nto be performing or \ndemonstrating some sort of \nact or trick with the knives.\nIn: Can you generate a \nsimilar image?\nOut: \nIn: Can you make the fur of \nthe monkey blue?\nFigure 3: Qualitative conversational example of our model. The dashed line indicates that the conversation has to be\nrestarted from the beginning due to the model losing track of it.\nImage Generation. [26.88%]\nFor text-to-image generation, we use the same image & text pairs that were used\nto pre-train the LLaVA model. This dataset, named LLaVA-Pretrain, is inverted and presented to our model in the\nformat (caption, image) with a dynamically pre-generated e.g. “Please generate an image of caption”. These\nprefixes are created using the GPT4 language model. This dataset contains ∼558K data points originally sourced from\nthe LAION (Schuhmann et al., 2022), SBU (Ordonez et al., 2011), and CC3M (Changpinyo et al., 2021) datasets and\ncaptioned by the BLIP-2 model (Li et al., 2023).\nImage Understanding.[61.56%]\nFor image understanding, we combine the dataset used to fine-tune the LLaVA\nmodel. This dataset, named LLaVA-Finetune, contains ∼665K samples. We also add the LVIS-INSTRUCT4V (Wang\net al., 2023a) dataset—a new visual instruction tuning dataset constructed in the same way as the original LlaVA dataset\nbut using GPT4-V (Achiam et al., 2023) as the captioner instead of BLIP-2 (Li et al., 2023). We remove duplicates\nfrom the resulting dataset; this results in ∼880K samples. We additionally add the following instruction datasets:\n• LRV-Instruction (Liu et al., 2023a) (∼80K) a diagram undestanding and hallucination reduction dataset.\n• laion-gpt4v-dataset (∼15K) a subset of the LAION (Schuhmann et al., 2022) dataset with high-quality\ncaptions created using GPT-4V (Achiam et al., 2023).\n• ShareGPT4V (Chen et al., 2024a) (∼100K) a conversational dataset created using publicly available conversa-\ntions that users had with the GPT-4V model.\n• Datasets for documents, chart and OCR understanding such as DocVQA (Mathew et al., 2021)(∼50K),\nSynDog-EN (Kim et al., 2022)(∼65K), ChartQA (Masry et al., 2022)(∼23K), DVQA (Kafle et al., 2018)\n(∼50K) and AI2D (Kembhavi et al., 2016) (∼15K).\n5\nGenerative Visual Instruction Tuning\nTable 1: Main result. Comparison of various models across advanced knowledge and general understanding. ⋆MGIE\nwas not originally designed for these tasks, as it is purely an editing model. For VQA, we take the generated caption as\nthe answer, and when asking it to generate entirely new images, we provide a blank image as the prompt. We intend to\nshow that models lose previous capabilities when we add a new one.\nModel name\nAdvanced Knowledge General Understanding Editing Generation\nMathVista\nMMMU\nMMVet SEED-B MMB\nEVR\nCC3M COCO\nGILL (Koh et al., 2024)\n18.6\n26.8\n13.0\n29.4\n38.2\n30.4\n15.3\n0.67\nAnyGPT (Zhan et al., 2024)\n24.4\n24.0\n14.8\n28.0\n36.0\n40.3\n14.3\n0.65\nMGIE⋆(Fu et al., 2024)\n15.5\n25.6\n13.0\n28.8\n6.6\n71.5\n13.6\n0.66\nChameleon (Chameleon Team, 2024)\n22.3\n22.4\n10.3\n30.5\n15.4\n-\n10.2\n0.78\nUnified-IO 2 (Lu et al., 2024a)\n28.3\n35.5\n36.6\n61.6\n57.9\n50.2\n13.4\n0.72\nGenLLaVA (Ours)\n30.5\n37.1\n35.8\n64.5\n66.8\n66.9\n12.5\n0.73\n4.2\nTraining details.\nIn this section, we evaluate our model on a broad range of tasks that require visual understanding and generation. We\ndo not perform task-specific finetuning in any experiments. The Supplementary section details additional results on\nGenLLaVA ’s instruction capabilities.\nWe adopt LLaVA-v1.5-7B (Liu et al., 2024a) architecture, then tune it on the constructed GVIT-mix-2076K. We named\nthis model GenLLaVA and it is made of the following components:\n• Image Processing & Visual Representations. We implement all image processing logic using the default\nimage transforms provided by torchvision and the TIMM library (Wightman, 2019). We normalize pixel\nvalues using the default ImageNet values. The default backbone employed by all visual representations Encvis\nthat we evaluate in this work is a Vision Transformer (Dosovitskiy et al., 2021); we extract patch features from\nthe penultimate layer, following LLaVA (Liu et al., 2023b).\n• Vision-Language Projector. We use a simple 2-layer GELU MLP as the projector W, which projects each\npatch independently into the embedding space of the language model.\n• Language Model. We choose the Mistral-7B LLM (Jiang et al., 2023). In order to combine the projected\nvisual patch embeddings, we perform simple sequence-wise concatenation, placing the patch embeddings\nbefore the text embeddings.\n• Visual Generation Head. The generation head T is a lightweight 4-layer encoder-decoder Transformer, which\ntakes word embeddings e and hidden states h from the [IMG] tokens, as well as L learnable query tokens as\nthe input and generates the visual latent U, we use the L = 77, and the dimension of each ut ∈U is 768.\n• Diffusion Image Decoder. We adopt Stable Diffusion v1.4 (SDv1.4) (Rombach et al., 2022) trained on\n512×512 resolution. Similar to the visual encoder, the SD model is frozen without any modifications or\ntraining throughout the whole process.\nWe implement our training codebase in PyTorch. We train all models in BF16 mixed precision. For a fair comparison,\nthe rest of the model training protocol is kept unchanged from the original LLaVA. Generative Visual Instruction tuning\ntakes about 48 hours for both full-parameter tuning and LoRA tuning on 8 NVIDIA Tesla A100 GPUs, each with 48GB\nmemory, with DeepSpeed ZeRO Stage 3 (Rajbhandari et al., 2020) to distribute training across GPUs.\nSingle-stage training.\nUnlike its predecessor LLaVA (Liu et al., 2023b), our model does not use a two-stage training\npipeline and instead directly finetunes the Vision-Language projector, the Language model, and the Visual Generation\nHead. We found that the logical extension of the original pipeline—a three-stage training pipeline—consisting of (1)\nmultimodal alignment, (2) instruction tuning, and (3) image generation tuning to teach the models progressively simply\ndoes not work, and the model performance on visual understanding tasks decreases significantly. We instead choose\na single-stage pipeline as it has been shown to work previously in the work of Karamcheti et al. (2024). This comes\nwith other unintentional advantages: training cost is reduced by 20%, and we can use the LLaVA-Pretrain data as\nimage-to-text data instead of having to collect more.\nTask Tokens.\nTo perform unified learning on multimodal understanding and generation, we introduce special tokens,\nwhich we name task tokens, to format the data. Specifically, we create the tokens [T2I] and [I2T] to indicate the\nlearning (generation or understanding) task for the input sequence. We keep the original special token from the language\n6\nGenerative Visual Instruction Tuning\nIn: What is unusual about this image?\nOut: The unique feature of the image is that it \nfeatures two watermelon wheels on a bicycle, \nwhich makes it stand out from other bicycles \nand adds an element of creativity to its design.\nIn: Can you edit the image to have yellow \nwatermelon wheels?\nOut:\nVQAv2\nGQA\nVizWiz\nTextVQA\nScienceQA\n10 20 30 40 50 60 70 80\n34.1\n30.9\n55.7\n20.1\n26.3\n50.4\n65\n41\n38.5\n42.7\n61\n86.2\n79.3\n62.9\n58.6\n66.1\n69.3\nGILL 7B\nMGIE 2 7B\nAnyGPT 7B\nUnified-IO 2 7B\nGenLLaVA 7B (Ours)\nFigure 4: (Left) Results on selected Visual Question answering datasets. (Right) A qualitative example of our model.\nmodel that indicates the start and end of the text. Similarly, [SOI] and [EOI] are pre-defined special tokens marking\nthe start and end of visual tokens for generation. Without these task tokens, the model has trouble inferring the user\nintention and would generate an image when it is not necessary. We remark that this methodology is not new and has\nbeen used before by others (Lu et al., 2024a; Xie et al., 2024).\n4.3\nEvaluation Details.\nVisual Understanding\nWe evaluate vision-language performance and compare it against other generalist models,\ni.e., models that can do visual generation and understating. Results on a collection of 5 vision\/language benchmarks\nare shown in Table 1. These benchmarks are designed to probe advanced knowledge and general understanding.\nMMBench (Liu et al., 2024c) evaluates answer robustness through comprehensive shuffling of multiple-choice options.\nSEED-Bench (Li et al., 2024) tests model performance on images and videos with multiple-choice questions. MM-\nVet (Yu et al., 2024) evaluates the ability to engage in visual conversations and assess the accuracy and helpfulness\nof responses. Mathvista (Lu et al., 2024b) consolidates mathematical reasoning benchmarks, focusing on logical\nand algebraic reasoning with puzzle tests. MMMU (Yue et al., 2024) covers 57 subjects across STEM, humanities,\nand social sciences, ranging from elementary to advanced professional levels, and tests both world knowledge and\nproblem-solving skills.\nVisual generation.\nFor visual generation evaluation, we measured the Fréchet Inception Distance (FID) (Heusel\net al., 2017) on the CC3M validation set (Changpinyo et al., 2021) (which is a measure of image realism) and CLIP\nSimilarity on the MS-COCO dataset (Lin et al., 2014) (which is a measure of the alignment between the text prompt\nand the generated image). For image editing, we measured the DINOScore on the 5.7K validation examples of the EVR\ndataset (Tan et al., 2019) following the same protocol as Fu et al. (2024).\n4.4\nMain Result\nWe evaluate GenLLaVA against similar models across various tasks, covering advanced knowledge, general understand-\ning, and visual generation tasks. As shown in Table 1, GenLLaVA demonstrates consistent improvements, surpassing\nmodels like GILL, AnyGPT, and MGIE in most cases.\nFor advanced knowledge tasks such as MathVista and MMMU, GenLLaVA performs exceptionally well, achieving the\ntop scores compared to other models. In general understanding, GenLLaVA excels, particularly on SEED-B and MMB,\nsignificantly outperforming its competitors. Although MGIE specializes in image editing and maintains a slight edge in\nthis area, GenLLaVA holds its own and outperforms other general models on the EVR dataset.\nIn visual generation tasks, GenLLaVA shows competitive performance, with results that closely match those of\nUnified-IO 2 on CC3M and CLIP Similarity while consistently maintaining an edge over other models. These results\ndemonstrate GenLLaVA ’s versatility and ability to achieve superior outcomes across multiple modalities.\n7\nGenerative Visual Instruction Tuning\nDescribe this image in \ndetail.\nGenLLaVA\nThe image captures a serene moment \nbetween a dog and a sunflower. The dog, \nwhich appears to be an Australian \nShepherd, is the central figure in the \nimage. Its coat is predominantly white \nwith shades of brown and tan, and it has \na distinctive pattern on its face. The dog's \neyes are gently closed, suggesting a \nmoment of tranquility or contentment. \nThe sunflower, with its bright yellow petals \nand dark brown center, is held close to the \ndog's nose.\nThe image captures the intricate details of \nthe dog's fur, the sunflower's texture, and \nthe surrounding grass. The overall \ncomposition emphasizes the connection \nbetween the dog and its natural \nenvironment, highlighting the beauty of \nthe moment and the bond between the \npet and its owner.\nThe scene features a brown and white dog \nstanding on grass, holding an orange \nsunflower in its mouth. It appears that the \nlarge canine is enjoying itself while \nplaying or exploring outdoors near some \ngreenery.\nMGIE\nGILL\nThe image features a brown and white \ndog standing in a grassy field, sniffing a \nlarge yellow sunflower. The dog appears \nto be enjoying the flower's scent, as it is \nclose to the sunflower, possibly even \nnibbling on it. The field is filled with \nseveral sunflowers, creating a vibrant and \nlively atmosphere. \nUnified-IO 2\nDo you think the image \nis unusual or not?\nGenLLaVA\nYes, the image is unusual because it \ndepicts a living room setting with furniture \nand decor typically found indoors placed \non a body of water.\nThe image shows the living room with a \ncouch, chair, and table placed in the \nmiddle of a flood-like ocean, it is an \nunusual and intriguing scene.\nYes, I believe that this scene with a couch \nand chairs floating on water in an ocean \nsetting appears quite peculiar.\nMGIE\nGILL\nYes, the image is unusual because it \nshows a living room set, including a \ncouch, chairs, and a coffee table, floating \nin the middle of the ocean. This is not a \ntypical scene, as living room furniture is \nusually found indoors, in a home or \napartment, and not in a body of water.\nUnified-IO 2\nFigure 5: Comparisons of VQA capabilities among GenLLaVA, Unified-IO 2, MGIE, and GILL. One can observe\nthat GenLLaVA is able to describe the image in detail and respond to commonly asked questions, even addressing the\nunusual aspects within an image. Hallucinations made by the models are highlighted in red.\nGenLLaVA surpasses Unified-IO 2 in visual understanding performance. It shows strong and balanced performance\nacross visual generation tasks, highlighting its versatility and robustness as a generalist vision-language model with\nsignificantly less training time.\n4.5\nResults on selected Visual Question Answering datasets.\nWe evaluate various models’ performance across diverse visual question-answering datasets, including VQAv2, GQA,\nVizWiz, TextVQA, and ScienceQA. Our results show that Unified-IO 2 and GenLLaVA consistently perform well across\nmost datasets. Specifically, Unified-IO 2 achieves the highest scores on the ScienceQA (86.2%) and TextVQA (67%),\nwhile GenLLaVA demonstrates strong performance on VQAv2 (79.3%) and a competitive score on GQA (62.9%).\nIn contrast, GILL and MGIE exhibit generally lower performance across all datasets, with MGIE notably struggling\non VizWiz (20.1%) and TextVQA (26.3%). AnyGPT shows moderate effectiveness, with its best performance on\nScienceQA (61%). We used VLMEvalKit from Duan et al. (2024) to get the results for these datasets, which perform a\ngeneration-based evaluation using the LLM-as-a-judge protocol.2 The results can be seen in Fig. 4 and Fig. 5.\n4.6\nAblations\nWe investigate the effect of scaling the data used to train GenLlaVA, the effect of using different image backbones, and\nthe number of visual tokens used for image generation.\n2We used GPT-4 (0409) as the judge.\n8\nGenerative Visual Instruction Tuning\nTable 2: Ablation experiments on different datasets. We present evaluation results across various ablation types.\n(a) Data Ablation. We study the effect of progressively adding data to the model, starting with visual understanding-\nonly data and then incorporating a mix of both understanding and generation tasks.\nModel Variation Advanced Knowledge General Understanding Editing Generation\nMathVista\nMMMU\nMM-Vet SEED-B MMB\nEVR\nCC3M COCO\nLLaVA-Finetune\n24.7\n28.7\n30.1\n54.7\n65.6\n-\n-\n-\nGeneration\n16.8\n27.5\n27.8\n52.1\n59.8\n30.2\n13.9\n0.73\nExtra Knowledge\n28.2\n31.8\n32.4\n59.7\n64.1\n28.5\n14.0\n0.72\nIPr2Pr-200K\n24.9\n29.7\n33.1\n63.5\n65.0\n64.7\n14.3\n0.71\nTask Tokens\n30.5\n37.1\n35.8\n64.5\n66.8\n66.9\n12.5\n0.73\n(b) Vision Encoder Ablation. We study the performance of using different vision encoders across several visual\nunderstanding and generation benchmarks. We do not condition on the task tokens for this experiment.\nModel Variation Advanced Knowledge General Understanding Editing Generation\nMathVista\nMMMU\nMM-Vet SEED-B MMB\nEVR\nCC3M COCO\nCLIP\/B-224px\n23.6\n28.6\n29.1\n53.4\n60.3\n62.1\n15.0\n0.68\nCLIP\/L-336px\n24.6\n29.2\n32.4\n59.7\n64.1\n64.5\n14.4\n0.70\nSigLIP\/L-384px\n24.9\n29.7\n33.1\n63.5\n65.0\n64.7\n14.3\n0.71\n(c) Number of Generation Tokens. We study the performance across several visual understanding and generation\nbenchmarks when varying the number of visual generation tokens (denoted as N). We do not condition on the task\ntokens for this experiment.\nModel Variation Advanced Knowledge General Understanding Editing Generation\nMathVista\nMMMU\nMM-Vet SEED-B MMB\nEVR\nCC3M COCO\nN = 4\n30.7\n35.0\n30.5\n58.2\n64.7\n40.3\n17.0\n0.62\nN = 8\n28.7\n32.8\n32.4\n61.2\n65.3\n53.3\n15.6\n0.67\nN = 16\n24.9\n29.7\n33.1\n63.5\n65.0\n64.7\n14.3\n0.71\nInstruction data.\nWe start with the original instruction tuning dataset from LlaVA-1.5, basically reproducing the\noriginal results using a one-stage training recipe. We add the LLaVA-Pretrain and generation head to our model, and\nwe notice that adding generation capabilities significantly affects the visual understanding capabilities, with all metrics\ndegrading between 2.3% (MM-Vet) and 7.9% (MathVista). To compensate for this loss in performance, we modify\nthe ratio of image generation to image understanding data in our dataset from ∼50%-50% to ∼70%-30%, by adding\nmore image instruction data from LVIS-INSTRUCT4V, LRV-Instruction and other chart understanding datasets. This\nresults in a model with significant generation capabilities that maintain its image-understanding capabilities. We finally\nadd image-generation capabilities using our selected subset of the IPr2Pr dataset. This reduces the image understanding\ncapabilities, but we consider this a small enough change that balances the three tasks while maintaining commendable\nperformance. Finally, task tokens are added to the resulting instruction set to condition the model on the desired task.\nThe results can be seen in Table 2a.\nChoice of Visual encoder.\nThe quality of the vision encoder can also have big effects on the final LMM performance.\nWe start by using a CLIP\/B, which has the lowest performance, then we compare against a stronger visual encoder\nand create versions of GenLLaVA, which is trained with CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023),\nrespectively. We can see in Table 2b that the SigLIP encoder generally achieves better performance than the CLIP\nencoder. This shows that SigLIP is a better vision encoder for LMM development.\nNumber of visual generation tokens.\nWe experiment with varying the number of visual generation tokens, N, to\ndetermine the optimal number required for balancing image generation and understanding tasks. We noticed that we\nneed more visual generation tokens than GILL (N = 4) and MGIE (N = 8) to achieve the best performance. We find\nthat N = 16 is the best choice for our model. We hypothesize that this is because our model has to balance image\ngeneration and editing in the same head and thus needs more visual tokens to capture the complexity of the tasks. The\nresults can be seen in Table 2c.\n9\nGenerative Visual Instruction Tuning\n4.7\nComparison with SOTA.\nWhen compared with state-of-the-art models, GenLLaVA maintains similar performance to models of the LLaVA family\nwhen evaluated using the average of the scores on the MathVista, MMMU, MMVet, SEED-B, and MMB datasets.\nHowever, it lags behind larger and more specialized models. However, some of these models lack the generative\ncapabilities present in GenLLaVA.\nWhen compared with models of similar size and setup (∼7b parameters), our model surpasses the original LlaVAv1(Liu\net al., 2023b) model by 9% points (37.5% vs. 46.9%), and LlaVA-1.5 (Liu et al., 2024a) by ∼1% points (45.3% vs.\n46.9%). It is surpassed by the LlaVA-Next (Liu et al., 2024b) family of models by ∼4% points; by Idefics2 (Laurençon\net al., 2024) by ∼8% points (55.7% vs. 46.9%) and by the newly released MiniCPM-Llama3 (Xu et al., 2024) model\nby ∼13% points (60.6% vs. 46.9%). Compared with the absolute state-of-the-art open-source models, GenLLaVA\nlags behind Yi-VL (Young et al., 2024) by ∼2% points; and surpasses Emu2 (Sun et al., 2024a) by ∼1% points. It\nlags behind LlaVA-Next (Liu et al., 2024b) (34b) by ∼12% points, and InternVL 1.5 (Chen et al., 2024b) by ∼15%\npoints. Compared with the absolute state-of-the-art closed models, GenLLaVA lags behind GPT-4o by ∼26% points,\nGPT-4V (Achiam et al., 2023) by ∼20% points; and the Gemini family (Gemini Team et al., 2023) of models by 13%\npoints.\n5\nConclusion\nIn this paper, we have introduced the Generative Large Language and Visual Assistant (GenLLaVA), a comprehensive\nframework for enabling Large Multimodal Models (LMMs) to excel simultaneously in image understanding, generation,\nand editing, while maintaining competitive performance. By balancing multimodal capabilities within a single model\nthrough the curation of a diverse multimodal instruction dataset and the development of an innovative single-phase\ntraining methodology, GenLLaVA sets a new benchmark in the development of multimodal systems. Our results show\nthat unifying generation and understanding under a single framework is possible without compromising their strengths.\nOur work sets a new standard for building visual assistants with extra capabilities, and we hope our open-source\ncontributions, including datasets, codebase, and model checkpoints, will serve as valuable resources for the research\ncommunity, driving further advancements in the field of multimodal AI. GenLLaVA’s capabilities can be extended to\nvideo understanding, audio-visual tasks, and more advanced real-time multimodal interactions.\nAcknowledgements\nThe authors would like to thank Google Cloud and the TPU Research Cloud program from Google for providing\nfunding for this research effort. We are also thankful for support from the Department of Computer Science at Rice\nUniversity.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,\n2023.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\net al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat\nLee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712, 2023.\nChameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer.\nIn 2022 IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11305–11315. IEEE, 2022.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In CVPR, 2021.\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. In ECCV, 2024a.\n10\nGenerative Visual Instruction Tuning\nZhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source\nsuites. arXiv preprint arXiv:2404.16821, 2024b.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with\n90%* chatgpt quality, March 2023. URL https:\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\nZhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal\ncomprehension and creation. In The Twelfth International Conference on Learning Representations, 2024. URL\nhttps:\/\/openreview.net\/forum?id=y01KGvd9Bw.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\nis worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\nHaodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang,\nJiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality\nmodels. arXiv preprint arXiv:2407.11691, 2024.\nTsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image\nediting via multimodal large language models. In ICLR, 2024.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv\npreprint arXiv:2312.11805, 2023.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. In ICLR, 2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium. NeurIPS, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS Workshop on Deep Generative Models and\nDownstream Applications, 2021.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nKushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan. Dvqa: Understanding data visualizations via question\nanswering. In CVPR, 2018.\nSiddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic\nvlms: Investigating the design space of visually-conditioned language models. In ICML, 2024.\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In ECCV, 2016.\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In ECCV, 2022.\nJing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. In\nNeurIPS, 2024.\nHugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?\narXiv preprint arXiv:2405.02246, 2024.\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using\nresidual quantization. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp.\n11523–11532, 2022.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal\nllms with generative comprehension. In CVPR, 2024.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. In ICML, 2023.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by\nalignment before projection. In EMNLP, 2024.\n11\nGenerative Visual Instruction Tuning\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large\nmulti-modal models via robust instruction tuning. In ICLR, 2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023b.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR,\n2024a.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024b.\nURL https:\/\/llava-vl.github.io\/blog\/\n2024-01-30-llava-next\/.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\nHe, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024c.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha\nKembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. In\nCVPR, 2024a.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In\nICLR, 2024b.\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering\nabout charts with visual and logical reasoning. In ACL, 2022.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In WACV,\n2021.\nOpenAI. Gpt-4o system card, 2024. URL https:\/\/openai.com\/index\/gpt-4o-system-card\/. Accessed:\n2024-09-30.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs.\nIn NeurIPS, 2011.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\nICML, 2021.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training\ntrillion parameter models. In SC, 2020.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for\ntraining next generation image-text models. In NeurIPS, Datasets and Benchmarks Track, 2022.\nTeams ShareGPT. Sharegpt: Share your wildest chatgpt conversations with one click, 2023.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao,\nJingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. In CVPR, 2024a.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multimodality. In ICLR, 2024b.\nHao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit Bansal. Expressing visual relationships via language. In\nACL, 2019.\nZineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context,\ninterleaved, and interactive any-to-any generation. In CVPR, 2024.\nJunke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023a.\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023b.\nRoss Wightman. Pytorch image models. https:\/\/github.com\/rwightman\/pytorch-image-models, 2019.\n12\nGenerative Visual Instruction Tuning\nJinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie\nChen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding\nand generation. arXiv preprint arXiv:2408.12528, 2024.\nRuyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang.\nLLaVA-UHD: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703,\n2024.\nHaoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2024.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen,\nJing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,\nBrian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes\nTamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman,\nMaryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive\nmulti-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for\nexpert agi. In CVPR, 2024.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training.\nIn ICCV, 2023.\nJun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang,\nLinyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. In ACL, 2024.\nBo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023.\nChunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma,\nLuke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal\nmodel. arXiv preprint arXiv:2408.11039, 2024.\n13\nGenerative Visual Instruction Tuning\nSystem Prompt:\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and politely answers to the user's questions.\n(a) VQA (Short)\nUSER: Based on the image, please answer the question. <IMAGE> <QUESTION> Please provide an accurate answer within one word.\nASSISTANT: The answer is: <ANSWER>\n(b) VQA (Long)\nUSER: This is an exam, please answer according to the image and question. <IMAGE> <QUESTION> \nASSISTANT: The answer is: <ANSWER>\n(c) ADVANCED KNOWLEDGE \nUSER: This is a hard exam, please answer according to the image and the question. <IMAGE> <QUESTION> Please think step by step.\nASSISTANT: The answer is: <ANSWER>\n(d) GENERATION \nUSER: Generate an image with the following description. <DESCRIPTION>\nASSISTANT: <GENERATED IMAGE>\n(e) EDITING\nUSER: Based on the image, please follow the instruction. <IMAGE> Edit the image according to the description. <DESCRIPTION>\nASSISTANT: <GENERATED IMAGE>\nFigure A1: Prompt templates. (a) Short VQA includes VQAv2, VizWiz, GQA, and TextVQA and ScienceQA. (b) Long\nVQA includes MMB, SEED-B, and MM-Vet. (c) Advanced knowledge includes MathVista and MMMU. (d) Generation\nincludes CC3M and MS-COCO. (e) Editing includes EVR. <IMAGE> is the image representation, <QUESTION> denotes\neach specific question, <ANSWER> is the generated answer, <DESCRIPTION> is an image description for generation or\nediting, and <GENERATED IMAGE> is the output of the generation head..\nA\nAppendix\nA.1\nImplementation details\nWe adopt the hyperparameters in Table A1 for all our experiments.\nTable A1: Training Hyperparameters\nHyperparameter\nValue\nBatch Size\n128\nMax Gradient Norm\n1.0\nWeight Decay\n0.1\nLearning Rate\n2e-5\nOptimizer\nAdamW\nScheduler\nWarmup & Cosine Decay\nWarmup Ratio\n0.03\nA.2\nEvaluation Details\nDuring the evaluation with VLMEvalKit (Duan et al., 2024), we used the prompt templates listed in Fig A1\nA.3\nAdditional Qualitative Examples\nIn Tables A2 and A3, we present a qualitative comparative analysis of VQA results between our model, GenLLaVA, and\nother state-of-the-art models: GPT-4 (Achiam et al., 2023), LLaVA (Liu et al., 2024a), GPT-4o (OpenAI, 2024), and\nInternVL-1.5 (Chen et al., 2024b). Our model, while smaller than state-of-the-art models, is still able to give detailed\nand precise responses to given questions, avoiding the introduction of hallucinations, unlike LLaVA-1.5.\n14\nGenerative Visual Instruction Tuning\nVisual input example, Extreme Ironing:\nSource: https:\/\/www.barnorama.com\/wp-content\/uploads\/2016\/12\/03-Confusing-Pictures.jpg\nUser\nWhat is unusual about this image?\nGenLLaVA (Ours)\nThe unusual aspect of this image is that a man is ironing clothes on the back of a yellow\nSUV in the middle of a city street. This is not a common sight, as ironing is typically\ndone indoors, in a private setting. The presence of a taxi cab in the background further\nemphasizes the unexpected nature of the scene, as it juxtaposes the mundane task of ironing\nwith the hustle and bustle of urban life.\nUser\nWhat is unusual about this image?\nLLaVA (Liu et al., 2024a)\nThe unusual aspect of this image is that a man is ironing clothes while standing on the\nback of a moving car. This is not a typical scene, as ironing clothes is usually done indoors,\nin a stationary position, and not while the vehicle is in motion. The man’s actions are\nboth unconventional and potentially dangerous, as ironing clothes on a moving car can be\nhazardous and may cause accidents or injuries.\nUser\nWhat is unusual about this image?\nGPT-4V (Achiam et al., 2023)\nThe unusual aspect of this image is the man ironing on a portable ironing board mounted\nto the back of a yellow taxi in the middle of a bustling city street. This activity is not\ntypically associated with such a setting, making it an extraordinary and humorous sight.\nThis represents an extreme form of “extreme ironing,” a performance sport where people\niron clothes in unexpected and often difficult locations.\nUser\nWhat is unusual about this image?\nGPT-4o (OpenAI, 2024)\nThe image shows a man ironing clothes on an ironing board that is attached to the back\nof a moving vehicle, which appears to be a yellow taxi. This is highly unusual because\nironing is typically done indoors, on stable surfaces, and definitely not while attached to a\nmoving car. This extreme scenario might be an example of “extreme ironing,” a humorous\nand unconventional sport where people iron clothes in extreme or unusual situations. The\nimage itself blends an ordinary chore with a risky and absurd setting, making it quite\nstriking and humorous.\nUser\nWhat is unusual about this image?\nInternVL-1.5 (Chen et al., 2024b)\nThis image depicts an unusual scene in which a person is ironing clothes on an ironing\nboard attached to the back of a taxi while it is parked on a city street. Several elements\nmake this situation peculiar: (1) Location: Ironing is typically done indoors at home or\nin a laundry room where there are proper facilities for such tasks. (2) Practicality: It’s\nimpractical to set up an ironing board and use an iron outdoors, especially in a busy urban\nenvironment where distractions and interruptions could occur frequently.\nThe juxtaposition of a mundane domestic activity with the bustling cityscape creates a\nhumorous and unexpected scenario.\nTable A2: Visual question answering example comparison of GenLLaVA to LLaVA-1.5-7B(Liu et al., 2024a),\nGPT-4V (Achiam et al., 2023), GPT-4o (OpenAI, 2024), and InternVL-1.5 (Chen et al., 2024b). This table follows the\nformat from Bubeck et al. (2023).\n15\nGenerative Visual Instruction Tuning\nVisual input example, Chicken Nugget Map:\nUser\nCan you explain this meme in detail?\nGenLLaVA (Ours)\nThe meme is a play on the phrase \"beauty is in the eye of the beholder,\" which means that\nwhat one person finds beautiful may not be the same for another. In this case, the person\ncreating the meme finds beauty in the shape of the Earth formed by pieces of fried chicken\narranged on a baking tray. It’s a humorous take on appreciating simple things and finding\nartistic or aesthetic value in everyday objects or food items.\nUser\nCan you explain this meme in detail?\nLLaVA (Liu et al., 2024a)\nThe meme in the image is a creative and humorous take on food, with a focus on chicken\nnuggets, as the center of the universe. The meme begins with the phrase “Sometimes I just\nlook at pictures of the Earth from space and I marvel at how beautiful it all is...” The main\nimage features a tray of cooked chicken nuggets, with the chicken pieces positioned to\nresemble the continents and islands on a world map. The meme playfully suggests that the\nchicken nuggets represent the Earth, and the various locations depicted in the photo are\nactually chicken nugget versions of different places. The punchline of the meme appears\nin smaller text at the bottom, reading “I mean, it’s not the real Earth, but how beautiful it\nis all is.” This lighthearted and entertaining meme combines food and humor, using the\nimage of chicken nuggets to create a funny and imaginative representation of the world.\nUser\nCan you explain this meme?\nGPT-4V (Achiam et al., 2023)\nThis meme cleverly uses chicken nuggets shaped like continents to make a humorous\ncomparison. The caption suggests that the person admires pictures of Earth from space\nfor their beauty, but the image shows chicken nuggets on a baking tray arranged to look\nlike a world map. This juxtaposition creates a playful contrast between the expected\nawe-inspiring images of our planet and the mundane, yet amusing arrangement of food.\nThe humor lies in the unexpected substitution and the notion that even chicken nuggets\ncan evoke a sense of wonder if seen from the right perspective.\nUser\nCan you explain this meme in detail?\nGPT-4o (OpenAI, 2024)\nThis meme humorously compares the shapes of fried chicken nuggets to a map of the\nworld, as though the person is marveling at the beauty of the Earth from space. The chicken\nnuggets are arranged in a way that resembles the continents, and the caption plays off the\nidea of someone gazing at these shapes with awe, as if they’re looking at Earth, rather than\njust pieces of food.\nThe humor comes from the exaggerated comparison of something mundane (fried chicken\nnuggets) to something grand and majestic (the Earth viewed from space). It’s a playful and\nlighthearted take on how our brains often find patterns in random objects that resemble\nfamiliar things.\nUser\nCan you explain this meme in detail?\nInternVL-1.5 (Chen et al., 2024b)\nThis meme humorously contrasts the profound and contemplative act of admiring the\nbeauty of Earth from space with a more mundane, everyday activity. The top part of the\nimage includes text that says: \"Sometimes I just look at pictures of the earth from space\nand I marvel at how beautiful it all is.\" Following this statement, instead of showing an\nimage of Earth from space, there is a picture of several pieces of fried chicken arranged on\na baking sheet.\nThe joke lies in the unexpected and humorous twist, where the \"beautiful sight\" referenced\nin the text turns out to be something as ordinary and relatable as fried chicken. This creates\na light-hearted contrast between grandeur (Earth viewed from space) and simplicity (fried\nchicken), playing on the idea that both can evoke similar feelings of wonder or satisfaction.\nTable A3: Visual question answering example comparison of GenLLaVA to LLaVA-1.5-7B(Liu et al., 2024a),\nGPT-4V (Achiam et al., 2023), GPT-4o (OpenAI, 2024), and InternVL-1.5 (Chen et al., 2024b). This table follows the\nformat from Bubeck et al. (2023).\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Generative Visual Instruction Tuning.pdf"}
