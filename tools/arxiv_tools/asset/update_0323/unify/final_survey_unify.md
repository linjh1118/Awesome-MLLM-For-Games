# Paper List of unify.md
- [25/03] **OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models**  
[[Paper](http://arxiv.org/pdf/2503.08686v1)] [[Code/Page](https://github.com/hustvl/OmniMamba)] [[TLDR/Notes](#omnimamba--efficient-and-unified-multimodal-understanding-and-generation-via-state-space-models)]

- [25/01] **Dual Diffusion for Unified Image Generation and Understanding**  
[[Paper](http://arxiv.org/pdf/2501.00289v1)] [[Code/Page]()] [[TLDR/Notes](#dual-diffusion-for-unified-image-generation-and-understanding)]

- [24/12] **LMFusion: Adapting Pretrained Language Models for Multimodal Generation**  
[[Paper](http://arxiv.org/pdf/2412.15188v4)] [[Code/Page]()] [[TLDR/Notes](#lmfusion--adapting-pretrained-language-models-for-multimodal-generation)]

- [24/12] **MetaMorph: Multimodal Understanding and Generation via Instruction Tuning**  
[[Paper](http://arxiv.org/pdf/2412.14164v1)] [[Code/Page]()] [[TLDR/Notes](#metamorph--multimodal-understanding-and-generation-via-instruction-tuning)]

- [24/12] **SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**  
[[Paper](http://arxiv.org/pdf/2412.09604v1)] [[Code/Page]()] [[TLDR/Notes](#synergen-vl--towards-synergistic-image-understanding-and-generation-with-vision-experts-and-token-folding)]

- [24/02] **Diffusion Language Models Are Versatile Protein Learners**  
[[Paper](http://arxiv.org/pdf/2402.18567v2)] [[Code/Page](https://github.com/bytedance/dplm}.)] [[TLDR/Notes](#diffusion-language-models-are-versatile-protein-learners)]

- [24/12] **TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2412.03069v1)] [[Code/Page]()] [[TLDR/Notes](#tokenflow--unified-image-tokenizer-for-multimodal-understanding-and-generation)]

- [25/01] **LoRaFlow: High-Quality Signal Reconstruction using Rectified Flow**  
[[Paper](http://arxiv.org/pdf/2501.00024v1)] [[Code/Page]()] [[TLDR/Notes](#loraflow--high-quality-signal-reconstruction-using-rectified-flow)]

- [24/12] **Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads**  
[[Paper](http://arxiv.org/pdf/2412.00127v1)] [[Code/Page]()] [[TLDR/Notes](#orthus--autoregressive-interleaved-image-text-generation-with-modality-specific-heads)]

- [24/11] **JetFormer: An Autoregressive Generative Model of Raw Images and Text**  
[[Paper](http://arxiv.org/pdf/2411.19722v1)] [[Code/Page]()] [[TLDR/Notes](#jetformer--an-autoregressive-generative-model-of-raw-images-and-text)]

- [24/11] **MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding**  
[[Paper](http://arxiv.org/pdf/2411.17762v3)] [[Code/Page]()] [[TLDR/Notes](#muse-vl--modeling-unified-vlm-through-semantic-discrete-encoding)]

- [24/11] **JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2411.07975v2)] [[Code/Page]()] [[TLDR/Notes](#janusflow--harmonizing-autoregression-and-rectified-flow-for-unified-multimodal-understanding-and-generation)]

- [23/10] **On the Performance of Multimodal Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03211v2)] [[Code/Page]()] [[TLDR/Notes](#on-the-performance-of-multimodal-language-models)]

- [24/10] **MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding**  
[[Paper](http://arxiv.org/pdf/2410.21747v1)] [[Code/Page]()] [[TLDR/Notes](#motiongpt-2--a-general-purpose-motion-language-model-for-motion-generation-and-understanding)]

- [24/10] **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2410.13848v1)] [[Code/Page]()] [[TLDR/Notes](#janus--decoupling-visual-encoding-for-unified-multimodal-understanding-and-generation)]

- [24/10] **PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**  
[[Paper](http://arxiv.org/pdf/2410.13861v2)] [[Code/Page](https://github.com/rongyaofang/PUMA.)] [[TLDR/Notes](#puma--empowering-unified-mllm-with-multi-granular-visual-generation)]

- [24/10] **MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling**  
[[Paper](http://arxiv.org/pdf/2410.10798v2)] [[Code/Page]()] [[TLDR/Notes](#mmar--towards-lossless-multi-modal-auto-regressive-probabilistic-modeling)]

- [21/05] **Looking at CTR Prediction Again: Is Attention All You Need?**  
[[Paper](http://arxiv.org/pdf/2105.05563v1)] [[Code/Page]()] [[TLDR/Notes](#looking-at-ctr-prediction-again--is-attention-all-you-need-)]

- [24/09] **MIO: A Foundation Model on Multimodal Tokens**  
[[Paper](http://arxiv.org/pdf/2409.17692v3)] [[Code/Page]()] [[TLDR/Notes](#mio--a-foundation-model-on-multimodal-tokens)]

- [24/09] **MonoFormer: One Transformer for Both Diffusion and Autoregression**  
[[Paper](http://arxiv.org/pdf/2409.16280v1)] [[Code/Page](https://monoformer.github.io/.)] [[TLDR/Notes](#monoformer--one-transformer-for-both-diffusion-and-autoregression)]

- [24/09] **VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2409.04429v3)] [[Code/Page]()] [[TLDR/Notes](#vila-u--a-unified-foundation-model-integrating-visual-understanding-and-generation)]

- [24/08] **Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2408.12528v6)] [[Code/Page](https://github.com/showlab/Show-o.)] [[TLDR/Notes](#show-o--one-single-transformer-to-unify-multimodal-understanding-and-generation)]

- [24/08] **Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model**  
[[Paper](http://arxiv.org/pdf/2408.11039v1)] [[Code/Page]()] [[TLDR/Notes](#transfusion--predict-the-next-token-and-diffuse-images-with-one-multi-modal-model)]

- [24/07] **ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation**  
[[Paper](http://arxiv.org/pdf/2407.06135v1)] [[Code/Page]()] [[TLDR/Notes](#anole--an-open--autoregressive--native-large-multimodal-models-for-interleaved-image-text-generation)]

- [24/06] **Hybrid Alignment Training for Large Language Models**  
[[Paper](http://arxiv.org/pdf/2406.15178v1)] [[Code/Page]()] [[TLDR/Notes](#hybrid-alignment-training-for-large-language-models)]

- [24/12] **Relational Programming with Foundation Models**  
[[Paper](http://arxiv.org/pdf/2412.14515v1)] [[Code/Page]()] [[TLDR/Notes](#relational-programming-with-foundation-models)]

- [24/04] **SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation**  
[[Paper](http://arxiv.org/pdf/2404.14396v2)] [[Code/Page](https://github.com/AILab-CVC/SEED-X.)] [[TLDR/Notes](#seed-x--multimodal-models-with-unified-multi-granularity-comprehension-and-generation)]

- [24/03] **Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models**  
[[Paper](http://arxiv.org/pdf/2403.18814v1)] [[Code/Page](https://github.com/dvlab-research/MiniGemini.)] [[TLDR/Notes](#mini-gemini--mining-the-potential-of-multi-modality-vision-language-models)]

- [24/02] **AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**  
[[Paper](http://arxiv.org/pdf/2402.12226v3)] [[Code/Page](https://junzhan2000.github.io/AnyGPT.github.io/)] [[TLDR/Notes](#anygpt--unified-multimodal-llm-with-discrete-sequence-modeling)]

- [24/02] **World Model on Million-Length Video And Language With Blockwise RingAttention**  
[[Paper](http://arxiv.org/pdf/2402.08268v4)] [[Code/Page]()] [[TLDR/Notes](#world-model-on-million-length-video-and-language-with-blockwise-ringattention)]

- [24/02] **Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization**  
[[Paper](http://arxiv.org/pdf/2402.03161v3)] [[Code/Page](https://video-lavit.github.io.)] [[TLDR/Notes](#video-lavit--unified-video-language-pre-training-with-decoupled-visual-motional-tokenization)]

- [24/01] **MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer**  
[[Paper](http://arxiv.org/pdf/2401.10208v2)] [[Code/Page](https://github.com/OpenGVLab/MM-Interleaved}.)] [[TLDR/Notes](#mm-interleaved--interleaved-image-text-generative-modeling-via-multi-modal-feature-synchronizer)]

- [23/12] **Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action**  
[[Paper](http://arxiv.org/pdf/2312.17172v1)] [[Code/Page]()] [[TLDR/Notes](#unified-io-2--scaling-autoregressive-multimodal-models-with-vision--language--audio--and-action)]

- [23/12] **Generative Multimodal Models are In-Context Learners**  
[[Paper](http://arxiv.org/pdf/2312.13286v2)] [[Code/Page]()] [[TLDR/Notes](#generative-multimodal-models-are-in-context-learners)]

- [24/08] **xGen-MM (BLIP-3): A Family of Open Large Multimodal Models**  
[[Paper](http://arxiv.org/pdf/2408.08872v2)] [[Code/Page]()] [[TLDR/Notes](#xgen-mm-(blip-3)--a-family-of-open-large-multimodal-models)]

- [23/12] **VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2312.09251v1)] [[Code/Page]()] [[TLDR/Notes](#vl-gpt--a-generative-pre-trained-transformer-for-vision-and-language-understanding-and-generation)]

- [23/09] **DreamLLM: Synergistic Multimodal Comprehension and Creation**  
[[Paper](http://arxiv.org/pdf/2309.11499v2)] [[Code/Page](https://dreamllm.github.io.)] [[TLDR/Notes](#dreamllm--synergistic-multimodal-comprehension-and-creation)]

- [23/10] **Making LLaMA SEE and Draw with SEED Tokenizer**  
[[Paper](http://arxiv.org/pdf/2310.01218v1)] [[Code/Page]()] [[TLDR/Notes](#making-llama-see-and-draw-with-seed-tokenizer)]

- [23/10] **On the Performance of Multimodal Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03211v2)] [[Code/Page]()] [[TLDR/Notes](#on-the-performance-of-multimodal-language-models)]

- [23/09] **Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**  
[[Paper](http://arxiv.org/pdf/2309.04669v3)] [[Code/Page](https://github.com/jy0205/LaVIT.)] [[TLDR/Notes](#unified-language-vision-pretraining-in-llm-with-dynamic-discrete-visual-tokenization)]

- [23/07] **Planting a SEED of Vision in Large Language Model**  
[[Paper](http://arxiv.org/pdf/2307.08041v2)] [[Code/Page]()] [[TLDR/Notes](#planting-a-seed-of-vision-in-large-language-model)]

- [23/07] **Emu: Generative Pretraining in Multimodality**  
[[Paper](http://arxiv.org/pdf/2307.05222v2)] [[Code/Page]()] [[TLDR/Notes](#emu--generative-pretraining-in-multimodality)]

- [23/05] **Any-to-Any Generation via Composable Diffusion**  
[[Paper](http://arxiv.org/pdf/2305.11846v1)] [[Code/Page](https://codi-gen.github.io)] [[TLDR/Notes](#any-to-any-generation-via-composable-diffusion)]

- [19/08] **Multimodal Unified Attention Networks for Vision-and-Language Interactions**  
[[Paper](http://arxiv.org/pdf/1908.04107v2)] [[Code/Page]()] [[TLDR/Notes](#multimodal-unified-attention-networks-for-vision-and-language-interactions)]

- [24/10] **UniMuMo: Unified Text, Music and Motion Generation**  
[[Paper](http://arxiv.org/pdf/2410.04534v1)] [[Code/Page](https://hanyangclarence.github.io/unimumo_demo/}{project)] [[TLDR/Notes](#unimumo--unified-text--music-and-motion-generation)]

- [24/09] **MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation**  
[[Paper](http://arxiv.org/pdf/2409.19684v1)] [[Code/Page]()] [[TLDR/Notes](#medvilam--a-multimodal-large-language-model-with-advanced-generalizability-and-explainability-for-medical-data-understanding-and-generation)]

- [24/06] **Generative Visual Instruction Tuning**  
[[Paper](http://arxiv.org/pdf/2406.11262v2)] [[Code/Page]()] [[TLDR/Notes](#generative-visual-instruction-tuning)]



# TLDR/Notes
## omnimamba--efficient-and-unified-multimodal-understanding-and-generation-via-state-space-models
### Abstract
Recent advancements in unified multimodal understanding and visual generation
(or multimodal generation) models have been hindered by their quadratic
computational complexity and dependence on large-scale training data. We
present OmniMamba, the first linear-architecture-based multimodal generation
model that generates both text and images through a unified next-token
prediction paradigm. The model fully leverages Mamba-2's high computational and
memory efficiency, extending its capabilities from text generation to
multimodal generation. To address the data inefficiency of existing unified
models, we propose two key innovations: (1) decoupled vocabularies to guide
modality-specific generation, and (2) task-specific LoRA for
parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage
training strategy to mitigate data imbalance between two tasks. Equipped with
these techniques, OmniMamba achieves competitive performance with JanusFlow
while surpassing Show-o across benchmarks, despite being trained on merely 2M
image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba
stands out with outstanding inference efficiency, achieving up to a 119.2 times
speedup and 63% GPU memory reduction for long-sequence generation compared to
Transformer-based counterparts. Code and models are released at
https://github.com/hustvl/OmniMamba
### 🌟 论文解读 | OmniMamba：高效且统一的跨模态理解和生成模型

### 📌 背景痛点/本文动机
近年来，跨模态理解和视觉生成模型取得了显著进展，但它们通常需要大量的训练数据和复杂的计算，限制了它们的实际应用。此外，现有的统一模型通常依赖于Transformer架构，这导致了二次计算复杂性和较慢的推理速度。

### 🚀 核心方法
💡 创新点1：解耦词汇表
OmniMamba采用了解耦词汇表的设计，为文本和图像模态分别使用独立的词汇表。这种设计使得模型能够更有效地学习特定模态的生成，并提高了训练效率。

💡 创新点2：任务特定LoRA
为了提高模型对特定任务的适应性，OmniMamba引入了任务特定的LoRA模块。这些模块被应用于Mamba-2层的输入投影中，并在执行特定任务时激活相应的LoRA路径，从而提高了数据效率。

💡 创新点3：解耦训练策略
OmniMamba采用了两阶段的解耦训练策略，以解决理解和生成任务之间的数据不平衡问题。第一阶段是针对特定任务的预训练，用于模块的初始化和模态对齐。第二阶段是统一的微调，用于多任务训练。

### 📈 实验结果
在跨模态理解和视觉生成任务上，OmniMamba取得了与JanusFlow相当的性能，并超过了Show-o。此外，OmniMamba在推理效率方面表现出色，与基于Transformer的模型相比，实现了高达119.2倍的加速和63%的GPU内存减少。

### 💬 可借鉴之处
OmniMamba的设计和训练策略为跨模态理解和视觉生成模型的开发提供了新的思路。解耦词汇表和任务特定LoRA的设计可以提高模型的数据效率和适应性，而解耦训练策略可以解决数据不平衡问题。这些创新点对于开发高效且统一的跨模态模型具有重要的参考价值。

## dual-diffusion-for-unified-image-generation-and-understanding
### Abstract
Diffusion models have gained tremendous success in text-to-image generation,
yet still lag behind with visual understanding tasks, an area dominated by
autoregressive vision-language models. We propose a large-scale and fully
end-to-end diffusion model for multi-modal understanding and generation that
significantly improves on existing diffusion-based multimodal models, and is
the first of its kind to support the full suite of vision-language modeling
capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and
recent advances in discrete diffusion language modeling, we leverage a
cross-modal maximum likelihood estimation framework that simultaneously trains
the conditional likelihoods of both images and text jointly under a single loss
function, which is back-propagated through both branches of the diffusion
transformer. The resulting model is highly flexible and capable of a wide range
of tasks including image generation, captioning, and visual question answering.
Our model attained competitive performance compared to recent unified image
understanding and generation models, demonstrating the potential of multimodal
diffusion modeling as a promising alternative to autoregressive next-token
prediction models.
### 🌟 论文解读 | 双向扩散模型：统一图像生成与理解

### 📌 背景痛点/本文动机
扩散模型在文本到图像生成方面取得了巨大成功，但在视觉理解任务方面仍然落后于自回归视觉语言模型。本文旨在构建一个大规模、端到端的扩散模型，以实现多模态理解和生成，并显著提升现有扩散模型在多模态任务上的表现。

### 🚀 核心方法
💡 创新点1：双向扩散模型
本文提出了一种名为 Dual Diffusion Transformer (D-DiT) 的模型，该模型基于多模态扩散 Transformer (MM-DiT) 架构，并进行了修改，使其能够在图像和文本两种模态上输出扩散目标。D-DiT 模型通过联合训练图像和文本的生成任务，实现了图像生成、图像描述和视觉问答等多种功能。

💡 创新点2：联合损失函数
本文提出了一种简单、优雅且易于实现的联合损失函数，该函数同时训练图像和文本的生成任务。该损失函数通过最小化图像和文本的生成误差，实现了两种模态的联合建模。

### 📈 实验结果
本文在多个多模态任务上进行了实验，包括图像生成、图像描述和视觉问答。实验结果表明，D-DiT 模型在图像生成任务上取得了与现有模型相当的性能，并在图像描述和视觉问答任务上取得了显著的提升。

### 💬 可借鉴之处
本文提出的双向扩散模型为多模态理解和生成提供了一种新的思路，具有以下可借鉴之处：

*   **联合建模**：通过联合训练图像和文本的生成任务，可以实现更全面的多模态理解和生成。
*   **双向扩散**：双向扩散模型可以有效地处理图像和文本两种模态，并实现更灵活的采样方式。
*   **联合损失函数**：联合损失函数可以有效地指导模型的训练，并提高模型在多模态任务上的性能。

### 🌟 总结
本文提出的双向扩散模型为多模态理解和生成提供了一种新的思路，并取得了显著的成果。该模型有望在图像生成、图像描述和视觉问答等领域得到广泛应用。

## lmfusion--adapting-pretrained-language-models-for-multimodal-generation
### Abstract
We present LMFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LMFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LMFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LMFusion improves image understanding by 20% and image generation by 3.6% using
only 50% of the FLOPs while maintaining Llama-3's language capabilities. We
also demonstrate that this framework can adapt existing vision-language models
with multimodal generation ability. Overall, this framework not only leverages
existing computational investments in text-only LLMs but also enables the
parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
### 🌟 论文解读 | LMFusion：赋予预训练语言模型多模态生成能力

### 📌 背景痛点/本文动机
近年来，多模态生成模型在理解和生成文本和图像方面取得了显著进展。然而，这些模型通常需要从头开始训练，这需要大量的计算资源。此外，直接在多模态数据上微调预训练的文本语言模型会导致其语言处理能力的显著下降。

### 🚀 核心方法
LMFusion 是一个框架，旨在通过在预训练的文本语言模型 Llama-3 上引入额外的并行 Transformer 模块来赋予其多模态生成能力。LMFusion 的关键创新点包括：

💡 创新点1：计算复用
LMFusion 利用现有计算资源，无需在文本数据上重新训练，从而显著降低计算需求。

💡 创新点2：性能保留和迁移
LMFusion 通过冻结文本模块并仅微调图像模块，在保留预训练语言模型的语言能力的同时，使其能够理解和生成图像。

### 📈 实验结果
与从头开始训练的多模态生成模型相比，LMFusion 在图像理解和图像生成方面分别提高了 20% 和 3.6%，同时仅使用了 50% 的 FLOPs，并保持了 Llama-3 的语言能力。

### 💬 可借鉴之处
LMFusion 的框架不仅适用于文本语言模型，还可以扩展到视觉语言模型，从而提高其多模态生成能力。此外，LMFusion 的设计理念可以应用于其他多模态任务，例如视频生成和音频生成。

## metamorph--multimodal-understanding-and-generation-via-instruction-tuning
### Abstract
In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a
simple and effective extension to visual instruction tuning that enables a
pretrained LLM to quickly morph into an unified autoregressive model capable of
generating both text and visual tokens. VPiT teaches an LLM to predict discrete
text tokens and continuous visual tokens from any input sequence of image and
text data curated in an instruction-following format. Our empirical
investigation reveals several intriguing properties of VPiT: (1) visual
generation ability emerges as a natural byproduct of improved visual
understanding, and can be unlocked efficiently with a small amount of
generation data; (2) while we find understanding and generation to be mutually
beneficial, understanding data contributes to both capabilities more
effectively than generation data. Building upon these findings, we train our
MetaMorph model and achieve competitive performance on both visual
understanding and generation. In visual generation, MetaMorph can leverage the
world knowledge and reasoning abilities gained from LLM pretraining, and
overcome common failure modes exhibited by other generation models. Our results
suggest that LLMs may have strong "prior" vision capabilities that can be
efficiently adapted to both visual understanding and generation with a
relatively simple instruction tuning process.
### 🌟 论文解读 | MetaMorph：通过指令微调实现多模态理解和生成

### 📌 背景痛点/本文动机
随着多模态大型语言模型（MLLMs）在视觉理解方面的进步，从基本的图像描述到复杂的视觉推理，这些模型已经能够处理多模态输入（主要是图像和语言）并生成文本标记。然而，目前的多模态模型往往将视觉生成视为与视觉理解正交的能力，需要大量的数据和时间进行训练。本文旨在探索一种简单而有效的方法，使预训练的大型语言模型（LLM）能够快速转变为一个统一的自回归模型，能够生成文本和视觉标记。

### 🚀 核心方法
本文提出了视觉预测指令微调（VPiT）方法，这是一种简单而有效的视觉指令微调扩展，它使预训练的LLM能够快速转变为一个统一的自回归模型，能够生成文本和视觉标记。VPiT 教导 LLM 从任何以指令遵循格式编写的图像和文本数据输入序列中预测离散的文本标记和连续的视觉标记。通过实验，本文发现 VPiT 具有以下有趣特性：
1. 视觉生成能力是改进视觉理解的副产品，并且可以用少量的生成数据进行高效解锁。
2. 虽然我们发现理解和生成是相互有益的，但理解数据比生成数据更有效地贡献于这两种能力。

### 📈 实验结果
基于以上发现，本文训练了一个名为 MetaMorph 的统一模型，并在视觉理解和生成方面取得了有竞争力的性能。在视觉生成方面，MetaMorph 可以利用从 LLM 预训练中获得的世界知识和推理能力，并克服其他生成模型中常见的失败模式。

### 💬 可借鉴之处
本文提出的 VPiT 方法为多模态理解和生成提供了一种简单而有效的方法，可以有效地利用预训练的 LLM 的能力。此外，本文还发现理解和生成是相互有益的，并且理解数据比生成数据更有效地贡献于这两种能力。这些发现为开发更强大的多模态模型提供了新的思路。

## synergen-vl--towards-synergistic-image-understanding-and-generation-with-vision-experts-and-token-folding
### Abstract
The remarkable success of Large Language Models (LLMs) has extended to the
multimodal domain, achieving outstanding performance in image understanding and
generation. Recent efforts to develop unified Multimodal Large Language Models
(MLLMs) that integrate these capabilities have shown promising results.
However, existing approaches often involve complex designs in model
architecture or training pipeline, increasing the difficulty of model training
and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful
encoder-free MLLM capable of both image understanding and generation. To
address challenges identified in existing encoder-free unified MLLMs, we
introduce the token folding mechanism and the vision-expert-based progressive
alignment pretraining strategy, which effectively support high-resolution image
understanding while reducing training complexity. After being trained on
large-scale mixed image-text data with a unified next-token prediction
objective, SynerGen-VL achieves or surpasses the performance of existing
encoder-free unified MLLMs with comparable or smaller parameter sizes, and
narrows the gap with task-specific state-of-the-art models, highlighting a
promising path toward future unified MLLMs. Our code and models shall be
released.
### 🌟 论文解读 | SynerGen-VL：基于视觉专家和Token折叠的协同图像理解和生成模型

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在多模态领域的成功应用，统一的图像理解和生成模型（MLLMs）成为研究热点。然而，现有的MLLMs模型架构或训练流程复杂，增加了模型训练和扩展的难度。

### 🚀 核心方法
💡 创新点1：Token折叠机制
为了支持高分辨率图像理解，同时减少训练复杂性，SynerGen-VL引入了Token折叠机制。该机制通过将输入的视觉Token序列压缩，减少了序列长度，从而提高了模型处理高分辨率图像的能力。

💡 创新点2：视觉专家和渐进式对齐预训练策略
为了将视觉能力整合到预训练的LLM中，同时最小化对LLM预训练知识的干扰，SynerGen-VL引入了视觉专家和渐进式对齐预训练策略。视觉专家是针对图像表示的额外参数，而渐进式对齐预训练策略则通过两个阶段的预训练，逐步将视觉特征与LLM的表示空间对齐。

### 📈 实验结果
SynerGen-VL在图像理解和生成任务上取得了与现有编码器无关的统一MLLMs相当或更好的性能，并且缩小了与特定任务模型之间的差距。在图像理解方面，SynerGen-VL在需要高分辨率图像理解的基准测试中取得了优异的成绩。在图像生成方面，SynerGen-VL也取得了与现有模型相当的性能。

### 💬 可借鉴之处
SynerGen-VL的Token折叠机制和视觉专家策略为构建统一的图像理解和生成模型提供了一种简单而有效的方法。此外，SynerGen-VL的渐进式对齐预训练策略也为如何在保持LLM预训练知识的同时整合视觉能力提供了新的思路。

## diffusion-language-models-are-versatile-protein-learners
### Abstract
This paper introduces diffusion protein language model (DPLM), a versatile
protein language model that demonstrates strong generative and predictive
capabilities for protein sequences. We first pre-train scalable DPLMs from
evolutionary-scale protein sequences within a generative self-supervised
discrete diffusion probabilistic framework, which generalizes language modeling
for proteins in a principled way. After pre-training, DPLM exhibits the ability
to generate structurally plausible, novel, and diverse protein sequences for
unconditional generation. We further demonstrate the proposed diffusion
generative pre-training makes DPLM possess a better understanding of proteins,
making it a superior representation learner, which can be fine-tuned for
various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).
Moreover, DPLM can be tailored for various needs, which showcases its prowess
of conditional generation in several ways: (1) conditioning on partial peptide
sequences, e.g., generating scaffolds for functional motifs with high success
rate; (2) incorporating other modalities as conditioner, e.g.,
structure-conditioned generation for inverse folding; and (3) steering sequence
generation towards desired properties, e.g., satisfying specified secondary
structures, through a plug-and-play classifier guidance. Code is released at
\url{https://github.com/bytedance/dplm}.
### 🌟 论文解读 | 蛋白质学习的新篇章：扩散语言模型的多功能应用

### 📌 背景痛点/本文动机
蛋白质作为生命活动的基础，其序列和结构的研究对于理解生物功能和设计新型蛋白质至关重要。然而，现有的蛋白质语言模型在生成和预测能力上存在局限性，例如，基于掩码预测的模型难以进行序列生成，而基于自回归的模型则在序列理解上有所欠缺。

### 🚀 核心方法
💡 创新点1：扩散蛋白质语言模型（DPLM）
本文提出了DPLM，一种基于离散扩散概率框架的蛋白质语言模型。DPLM通过在进化尺度上的蛋白质序列上进行预训练，能够生成结构合理、新颖且多样的蛋白质序列，并展现出强大的生成和预测能力。

💡 创新点2：条件生成策略
DPLM支持多种条件生成策略，包括：
1. 基于部分肽序列的条件生成，例如，以高成功率生成功能基序的支架。
2. 结合其他模态作为条件，例如，结构条件生成用于逆折叠。
3. 通过即插即用的分类器引导，将序列生成引导到所需的属性，例如，满足指定的二级结构。

### 📈 实验结果
实验结果表明，DPLM在无条件生成、蛋白质表示学习和条件生成任务上均表现出色。DPLM生成的蛋白质序列具有高结构合理性、新颖性和多样性，并且在各种预测任务上优于现有的蛋白质序列编码器模型，如ESM-2。

### 💬 可借鉴之处
DPLM为蛋白质学习和设计提供了新的思路和方法，其强大的生成和预测能力以及灵活的条件生成策略使其在药物发现、蛋白质工程等领域具有广泛的应用前景。此外，DPLM的设计理念和方法也可以为其他序列建模任务提供借鉴。

## tokenflow--unified-image-tokenizer-for-multimodal-understanding-and-generation
### Abstract
We present TokenFlow, a novel unified image tokenizer that bridges the
long-standing gap between multimodal understanding and generation. Prior
research attempt to employ a single reconstruction-targeted Vector Quantization
(VQ) encoder for unifying these two tasks. We observe that understanding and
generation require fundamentally different granularities of visual information.
This leads to a critical trade-off, particularly compromising performance in
multimodal understanding tasks. TokenFlow addresses this challenge through an
innovative dual-codebook architecture that decouples semantic and pixel-level
feature learning while maintaining their alignment via a shared mapping
mechanism. This design enables direct access to both high-level semantic
representations crucial for understanding tasks and fine-grained visual
features essential for generation through shared indices. Our extensive
experiments demonstrate TokenFlow's superiority across multiple dimensions.
Leveraging TokenFlow, we demonstrate for the first time that discrete visual
input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\%
average improvement. For image reconstruction, we achieve a strong FID score of
0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art
performance in autoregressive image generation with a GenEval score of 0.55 at
256*256 resolution, achieving comparable results to SDXL.
### 🌟 论文解读 | TokenFlow：统一图像分词器，跨越多模态理解和生成的鸿沟

### 📌 背景痛点/本文动机
在多模态领域，视觉理解和生成任务通常需要不同的视觉信息粒度。视觉理解需要丰富的语义表示来支持复杂的推理，而视觉生成则需要精确的空间结构和纹理细节编码。现有的方法往往采用单一的重建目标向量量化（VQ）编码器来统一这两个任务，这导致了一个关键的权衡，尤其是在多模态理解任务中性能的妥协。

### 🚀 核心方法
TokenFlow 通过其独特的双流设计解决了这一挑战。TokenFlow 的关键洞察是解耦语义和像素级特征的学习，同时通过共享索引映射保持它们的对齐。通过将具有语义和像素级相似性的补丁映射到相同的索引，量化特征可以直接应用于自回归视觉生成和多模态理解任务。

TokenFlow 采用双编码器架构，包括语义编码器 Esem 和像素编码器 Epix。语义编码器从预训练的文本对齐视觉编码器（例如 CLIP ViT-B/14）中学习，而像素编码器捕获详细的视觉信息。提取的特征然后通过最小化语义和像素级距离的加权求和来量化，创建一个联合表示空间。

TokenFlow 的双代码本设计允许专业化学习，同时通过共享索引保持跨级别相关性。这种创新允许同时访问语义和像素级表示，而不会损害任何方面。TokenFlow 还展示了显著的扩展性，即使在超过 130K 条目的超大规模代码本中，也能保持异常高的代码本利用率（95%+）。

### 📈 实验结果
TokenFlow 在多个维度上表现出优越性。利用 TokenFlow，我们首次证明离散视觉输入可以超越 LLaVA-1.5 13B 在理解性能方面，平均提高了 7.2%。对于图像重建，我们在 384*384 分辨率下实现了 0.63 的 FID 分数。此外，TokenFlow 在自回归图像生成方面建立了最先进的性能，在 256*256 分辨率下实现了 0.55 的 GenEval 分数，实现了与 SDXL 相当的结果。

### 💬 可借鉴之处
TokenFlow 的双代码本架构和共享映射机制为多模态理解和生成任务提供了一种有效且高效的统一视觉编码机制。TokenFlow 的设计理念和方法可以启发未来多模态模型的研究和开发，推动多模态领域的发展。

## loraflow--high-quality-signal-reconstruction-using-rectified-flow
### Abstract
LoRa technology, crucial for low-power wide-area networks, faces significant
performance degradation at extremely low signal-to-noise ratios (SNRs). We
present LoRaFlow, a novel approach using rectified flow to reconstruct
high-quality LoRa signals in challenging noise conditions. Unlike existing
neural-enhanced methods focused on classification, LoRaFlow recovers the signal
itself, maintaining compatibility with standard dechirp algorithms. Our method
combines a hybrid neural network architecture, synthetic data generation, and
robust augmentation strategies. This minimally invasive enhancement to LoRa
infrastructure potentially extends operational range and reliability without
overhauling existing systems. LoRaFlow opens new possibilities for robust IoT
communications in harsh environments and its core methodology can be
generalized to support various communication technologies.
### 🌟 论文解读 | LoRaFlow：基于校正流的LoRa信号高质量重建

### 📌 背景痛点/本文动机
LoRa技术是低功耗广域网络（LPWAN）的关键技术，在物联网（IoT）应用中发挥着重要作用。然而，LoRa信号在极低信噪比（SNR）条件下性能显著下降，限制了其应用范围和可靠性。现有的基于神经网络的LoRa信号增强方法主要集中在分类任务上，而LoRaFlow则专注于信号本身的重建，以恢复高质量信号。

### 🚀 核心方法
💡 创新点1：校正流技术
LoRaFlow采用校正流技术，这是一种基于扩散的生成模型，能够有效地从噪声输入中重建高质量的LoRa信号。与传统的扩散模型相比，校正流通过学习直线轨迹的映射，减少了重建信号所需的步骤，从而在低SNR条件下表现出更高的效率。

💡 创新点2：混合神经网络架构
LoRaFlow的模型架构结合了扩散变换器（DiT）和卷积层，充分利用了DiT的扩展性和灵活性，以及卷积层在处理多尺度空间信息方面的优势。这种混合架构能够有效地捕捉LoRa信号中的长距离依赖关系和全局上下文信息。

💡 创新点3：合成数据生成和增强策略
为了训练模型，LoRaFlow使用合成数据生成技术，模拟了不同扩频因子（SF）和带宽（BW）的LoRa信号。此外，LoRaFlow还采用了多种数据增强策略，如频率域掩蔽、时域平移、信号反转和频谱图滚动，以提高模型在现实世界场景中的泛化能力。

💡 创新点4：最小侵入式集成
LoRaFlow的设计旨在与现有的LoRa基础设施无缝集成，无需对现有系统进行重大修改。它通过在标准解调算法之前对信号进行去噪，从而提高LoRa网络的性能，而不会影响现有的LoRa硬件和软件。

### 📈 实验结果
LoRaFlow在多个方面都取得了显著的性能提升。与传统的解调方法相比，LoRaFlow在低SNR条件下实现了更高的信号重建精度，尤其是在相位和幅度方面。此外，LoRaFlow在符号错误率（SER）方面也优于现有的基于神经网络的方法，如NELoRa。

### 💬 可借鉴之处
LoRaFlow的核心方法，包括校正流技术、混合神经网络架构、合成数据生成和增强策略，以及最小侵入式集成，为LoRa信号增强领域提供了新的思路。此外，LoRaFlow的模型架构和训练方法也可以为其他通信技术的信号重建任务提供参考。

## orthus--autoregressive-interleaved-image-text-generation-with-modality-specific-heads
### Abstract
We introduce Orthus, an autoregressive (AR) transformer that excels in
generating images given textual prompts, answering questions based on visual
inputs, and even crafting lengthy image-text interleaved contents. Unlike prior
arts on unified multimodal modeling, Orthus simultaneously copes with discrete
text tokens and continuous image features under the AR modeling principle. The
continuous treatment of visual signals minimizes the information loss for both
image understanding and generation while the fully AR formulation renders the
characterization of the correlation between modalities straightforward. The key
mechanism enabling Orthus to leverage these advantages lies in its
modality-specific heads -- one regular language modeling (LM) head predicts
discrete text tokens and one diffusion head generates continuous image features
conditioning on the output of the backbone. We devise an efficient strategy for
building Orthus -- by substituting the Vector Quantization (VQ) operation in
the existing unified AR model with a soft alternative, introducing a diffusion
head, and tuning the added modules to reconstruct images, we can create an
Orthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).
Orthus-base can further embrace post-training to better model interleaved
images and texts. Empirically, Orthus surpasses competing baselines including
Show-o and Chameleon across standard benchmarks, achieving a GenEval score of
0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows
exceptional mixed-modality generation capabilities, reflecting the potential
for handling intricate practical generation tasks.
### 🌟 论文解读 | Orthus：基于自回归的图像-文本生成模型

### 📌 背景痛点/本文动机
多模态模型在图像到文本和文本到图像的生成方面展现出巨大潜力，但现有方法往往存在建模冗余和信息损失的问题。例如，将图像和文本映射到离散令牌进行自回归建模会导致信息损失，而将自回归建模与扩散建模结合则难以同时处理图像理解和生成任务。

### 🚀 核心方法
Orthus 是一种基于自回归的图像-文本生成模型，它通过以下创新点解决了现有方法的痛点：

💡 创新点1：连续处理视觉信号
Orthus 直接处理连续的图像特征，避免了图像令牌化过程中的信息损失，从而提高了图像理解和生成的质量。

💡 创新点2：统一的自回归建模
Orthus 使用统一的自回归模型来处理离散的文本令牌和连续的图像特征，简化了模态之间的相关性建模，并提高了模型的效率。

💡 创新点3：模态特定头部
Orthus 使用两个模态特定头部：一个语言模型头部预测离散的文本令牌，一个扩散头部生成连续的图像特征。

💡 创新点4：高效的构建策略
Orthus 可以通过替换现有统一自回归模型中的 VQ 操作并引入扩散头部来高效构建，从而显著降低训练成本。

### 📈 实验结果
在多模态理解和生成任务的标准基准测试中，Orthus 超越了包括 Show-o 和 Chameleon 在内的竞争基线，实现了 0.58 的 GenEval 分数和 1265.8 的 MME-P 分数。此外，Orthus 还展示了出色的混合模态生成能力，证明了其在处理复杂生成任务方面的潜力。

### 💬 可借鉴之处
Orthus 的创新方法为多模态建模提供了新的思路，其连续处理视觉信号和统一的自回归建模策略值得借鉴。此外，Orthus 的高效构建策略也为多模态模型的开发提供了宝贵的经验。

## jetformer--an-autoregressive-generative-model-of-raw-images-and-text
### Abstract
Removing modeling constraints and unifying architectures across domains has
been a key driver of the recent progress in training large multimodal models.
However, most of these models still rely on many separately trained components
such as modality-specific encoders and decoders. In this work, we further
streamline joint generative modeling of images and text. We propose an
autoregressive decoder-only transformer - JetFormer - which is trained to
directly maximize the likelihood of raw data, without relying on any separately
pretrained components, and can understand and generate both text and images.
Specifically, we leverage a normalizing flow model to obtain a soft-token image
representation that is jointly trained with an autoregressive multimodal
transformer. The normalizing flow model serves as both an image encoder for
perception tasks and an image decoder for image generation tasks during
inference. JetFormer achieves text-to-image generation quality competitive with
recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
image autoencoders, which are trained with a complex mixture of losses,
including perceptual ones. At the same time, JetFormer demonstrates robust
image understanding capabilities. To the best of our knowledge, JetFormer is
the first model that is capable of generating high-fidelity images and
producing strong log-likelihood bounds.
### 🌟 论文解读 | JetFormer：一种用于原始图像和文本的自回归生成模型

### 📌 背景痛点/本文动机
近年来，大型多模态模型在训练方面取得了显著进展，其中一个关键驱动力是去除建模约束并统一跨领域的架构。然而，大多数这些模型仍然依赖于许多单独训练的组件，例如特定于模态的编码器和解码器。本文旨在进一步简化图像和文本的联合生成建模。

### 🚀 核心方法
本文提出了JetFormer，一种自回归解码器-仅Transformer，它可以直接最大化原始数据的似然性，而无需依赖任何单独预训练的组件，并能够理解和生成文本和图像。具体来说，我们利用正则化流模型来获得软令牌图像表示，该表示与自回归多模态Transformer联合训练。正则化流模型在感知任务中作为图像编码器，在推理时作为图像解码器。JetFormer在文本到图像生成质量方面与最近的VQ-VAE和VAE基线相当。这些基线依赖于预训练的图像自动编码器，这些自动编码器是用复杂的损失混合物训练的，包括感知损失。同时，JetFormer展示了强大的图像理解能力。据我们所知，JetFormer是第一个能够生成高保真图像并产生强大的对数似然界限的模型。

### 📈 实验结果
JetFormer在ImageNet类条件图像生成和Web-scale多模态生成方面进行了实验，结果表明JetFormer在文本到图像生成和视觉语言理解方面具有竞争力。

### 💬 可借鉴之处
JetFormer的设计简单，易于扩展，并且能够生成高质量的图像和文本。此外，JetFormer能够计算对数似然，这对于比较不同的生成模型或进行hill-climbing非常有用。

## muse-vl--modeling-unified-vlm-through-semantic-discrete-encoding
### Abstract
We introduce MUSE-VL, a Unified Vision-Language Model through Semantic
discrete Encoding for multimodal understanding and generation. Recently, the
research community has begun exploring unified models for visual generation and
understanding. However, existing vision tokenizers (e.g., VQGAN) only consider
low-level information, which makes it difficult to align with language tokens.
This results in high training complexity and necessitates a large amount of
training data to achieve optimal performance. Additionally, their performance
is still far from dedicated understanding models. This paper proposes Semantic
Discrete Encoding (SDE), which effectively aligns the information of visual
tokens and language tokens by adding semantic constraints to the visual
tokenizer. This greatly reduces the amount of training data and improves the
performance of the unified model. With the same LLM size, our method improved
the understanding performance by 4.8% compared to the previous SOTA Emu3 and
surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model
also surpasses the existing unified models on visual generation benchmarks.
### 🌟 论文解读 | MUSE-VL：基于语义离散编码的统一视觉语言模型

### 📌 背景痛点/本文动机
近年来，多模态大语言模型（MLLMs）的研究领域日益兴起，研究者们致力于将视觉理解和生成任务整合到自回归的下一个词预测模型中。然而，如何将视觉输入转换为离散的标记，以便与文本标记对齐，成为实现统一MLLMs的关键挑战之一。现有的视觉标记化方法（如VQGAN）仅考虑低级信息，难以与语言标记对齐，导致训练复杂度高，需要大量训练数据才能达到最佳性能，且其性能仍远低于专门的视觉理解模型。

### 🚀 核心方法
本文提出了语义离散编码（SDE）方法，通过在视觉标记化过程中添加语义约束，有效地将视觉标记和语言标记的信息对齐。SDE方法包括一个语义编码器和一个语义解码器，用于从离散代码中提取和重建语义特征。此外，SDE方法还包含一个图像解码器，用于从量化特征中重建原始图像。通过这种方式，SDE方法能够在图像离散化过程中考虑语义信息，满足视觉理解和生成任务的需求。

基于SDE标记器，本文提出了MUSE-VL，一个用于多模态理解和生成的统一自回归变换器。MUSE-VL将视觉和语言数据建模为统一的离散标记，并在各种视觉语言基准测试中取得了最先进的性能。

### 📈 实验结果
实验结果表明，与之前的SOTA Emu3相比，MUSE-VL在理解性能上提高了4.8%，并超过了专门的视觉理解模型LLaVA-NeXT 34B。此外，MUSE-VL在视觉生成基准测试中也超过了现有的统一模型。

### 💬 可借鉴之处
本文提出的SDE方法和MUSE-VL模型为多模态理解和生成任务提供了一种新的解决方案。SDE方法有效地将视觉标记和语言标记的信息对齐，而MUSE-VL模型则将视觉和语言数据建模为统一的离散标记，并在各种视觉语言基准测试中取得了最先进的性能。这些方法和模型为多模态大语言模型的研究和应用提供了有价值的参考。

## janusflow--harmonizing-autoregression-and-rectified-flow-for-unified-multimodal-understanding-and-generation
### Abstract
We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.
### 🌟 论文解读 | JanusFlow：统一多模态理解和生成的新框架

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在学习和泛化方面的显著能力，研究人员开发了专门用于图像理解和文本到图像生成的复杂模型。然而，这些模型通常需要单独训练和集成，导致架构复杂且效率低下。为了解决这个问题，本文提出了JanusFlow，一个统一的多模态模型，旨在同时处理图像理解和生成任务。

### 🚀 核心方法
💡 创新点1：融合自回归语言模型和修正流
JanusFlow的核心创新在于将自回归语言模型与修正流相结合。修正流是一种先进的生成建模方法，能够在LLM框架内直接训练，无需复杂的架构修改。

💡 创新点2：解耦理解和生成编码器
为了进一步提高模型的性能，JanusFlow采用了两种关键策略：解耦理解和生成编码器，以及在对齐训练过程中对齐它们的表示。这种设计有助于防止任务之间的干扰，并增强模型的语义一致性。

### 📈 实验结果
JanusFlow在多模态理解和图像生成方面取得了最先进的性能，甚至在某些基准测试中超过了专门的模型。在图像生成基准测试中，JanusFlow在MJHQ FID-30k、GenEval和DPG-Bench上分别取得了9.51、0.63和80.09%的分数，超过了包括SDv1.5和SDXL在内的现有文本到图像模型。在多模态理解基准测试中，JanusFlow在MMBench、SeedBench和GQA上分别取得了74.9、70.5和60.3的分数，超过了LLaVA-v1.5和Qwen-VL-Chat等专门模型。

### 💬 可借鉴之处
JanusFlow的成功表明，将自回归语言模型和修正流相结合可以有效地解决多模态学习和生成任务中的挑战。此外，解耦理解和生成编码器以及表示对齐正则化等策略也为未来研究提供了有价值的参考。

## on-the-performance-of-multimodal-language-models
### Abstract
Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
### 🌟 论文解读 | 多模态语言模型性能探究

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理任务中展现出强大的能力，如何将视觉信息与语言模型相结合，以处理现实世界中的多模态数据，成为了一个重要的研究方向。本文旨在探究不同多模态指令微调方法在各类下游任务中的性能，并分析其优缺点。

### 🚀 核心方法
本文比较了五种公开可用的多模态LLMs微调方法：BLIP-2、InstructBLIP、LLaVA、MiniGPT4和mPLUG-Owl。这些方法涵盖了不同的架构选择，包括视觉编码器、视觉头部和数据使用方式。通过在多个任务上进行基准测试和消融实验，本文揭示了不同方法的优缺点，并分析了影响多模态LLMs性能的关键因素。

### 📈 实验结果
实验结果表明，InstructBLIP在所有任务上表现最佳，这归功于其丰富的指令微调数据。其他方法在未训练过的任务上表现较差，这可能是由于过拟合到特定任务类型。此外，实验还发现：

* 使用更大的视觉编码器（ViT-g）可以显著提高所有任务的性能。
* 训练视觉头部（如Q-Former）可以更好地提取与任务相关的视觉特征，从而提高性能。
* 在指令微调阶段训练语言模型可以带来额外的性能提升。
* 数据量在达到一定规模后，其对性能的提升作用逐渐减弱，但数据多样性仍然至关重要。

### 💬 可借鉴之处
本文为构建有效的多模态LLMs提供了有价值的见解，并强调了数据多样性和任务多样性在提升模型性能方面的重要性。未来研究可以关注以下几个方面：

* 探索更有效的多模态数据集，以增强模型的泛化能力。
* 研究如何减轻多模态LLMs的幻觉问题，使其更可靠。
* 开发更先进的架构和方法，以更好地融合视觉和语言信息。

## motiongpt-2--a-general-purpose-motion-language-model-for-motion-generation-and-understanding
### Abstract
Generating lifelike human motions from descriptive texts has experienced
remarkable research focus in the recent years, propelled by the emerging
requirements of digital humans.Despite impressive advances, existing approaches
are often constrained by limited control modalities, task specificity, and
focus solely on body motion representations.In this paper, we present
MotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these
limitations. MotionGPT-2 accommodates multiple motion-relevant tasks and
supporting multimodal control conditions through pre-trained Large Language
Models (LLMs). It quantizes multimodal inputs-such as text and single-frame
poses-into discrete, LLM-interpretable tokens, seamlessly integrating them into
the LLM's vocabulary. These tokens are then organized into unified prompts,
guiding the LLM to generate motion outputs through a
pretraining-then-finetuning paradigm. We also show that the proposed
MotionGPT-2 is highly adaptable to the challenging 3D holistic motion
generation task, enabled by the innovative motion discretization framework,
Part-Aware VQVAE, which ensures fine-grained representations of body and hand
movements. Extensive experiments and visualizations validate the effectiveness
of our method, demonstrating the adaptability of MotionGPT-2 across motion
generation, motion captioning, and generalized motion completion tasks.
### 🌟 论文解读 | MotionGPT-2：通用运动-语言模型，实现运动生成与理解

### 📌 背景痛点/本文动机
近年来，从描述性文本生成逼真的人类运动引起了广泛关注，这主要得益于数字人类的需求日益增长。然而，现有的方法通常受到以下限制：
1. **控制方式的局限性**：现有的方法通常只针对单一类型的控制条件，例如文本描述或多帧姿态，限制了其在需要同时生成基于文本描述和多关键帧人体姿态的运动序列场景中的应用。
2. **缺乏通用世界知识的任务特定框架**：现有的模型往往是任务特定的，例如基于扩散和GPT的框架，缺乏适应多种任务的能力，并且无法充分利用大型语言模型（LLMs）中嵌入的世界知识。
3. **仅关注身体运动表示**：现有的基于文本的运动生成解决方案主要关注生成仅身体运动，而不是整体运动，这在某些场景下（例如体育活动和演奏乐器）的合理性和表现力仍然不尽如人意。

### 🚀 核心方法
为了克服这些限制，本文提出了MotionGPT-2，这是一个统一的通用运动-语言模型（LMLM），能够处理多种控制信号，执行各种与运动相关的任务，并生成整体人类运动。其关键创新点包括：
1. **将多模态控制信号转化为统一表示**：MotionGPT-2设计了一个通用的框架和任务感知提示，用于人类运动合成。该框架允许在多模态控制条件下生成人类运动，并通过任务感知提示适应特定的运动相关任务。
2. **构建具有强大泛化能力的任务无关框架**：MotionGPT-2通过使用LLMs来共同表示运动和语言，改进了MotionGPT。首先，将人类运动嵌入到离散的运动标记中，然后使用这些运动标记扩展LLM的词汇表，创建一个丰富的运动-语言词汇表。通过将人类运动和语言纳入统一的词汇表，运动和语言之间的复杂关系变得透明。进一步地，MotionGPT-2将来自语言和运动提示的标记结合起来生成指令，并实现了一种多模态预训练和指令微调方法来有效地训练MotionGPT-2。
3. **实现整体人类运动的精确离散表示**：为了解决整体人类运动生成任务，本文引入了运动离散化框架Part-Aware VQVAE，该框架利用两个级别的离散码本和运动编码器来学习身体和手的表示。这种两级的离散化框架捕获了细微的手部运动，同时保持了整体的身体动力学。

### 📈 实验结果
本文在HumanML3D、KIT-ML和Motion-X数据集上进行了广泛的实验，结果表明，MotionGPT-2在多个运动相关任务中具有强大的适应性和效率。与从头开始训练的具有特定框架的模型相比，MotionGPT-2仅使用1%的额外参数就实现了具有竞争力的性能，并将训练时间显著减少到其他方法的10%。

### 💬 可借鉴之处
MotionGPT-2为运动生成和理解领域提供了一个新的视角，其多模态控制、任务无关框架和精确的离散表示方法为未来的研究提供了有价值的参考。此外，该模型在多个数据集上的出色表现表明，LLMs在运动生成和理解方面具有巨大的潜力。

## janus--decoupling-visual-encoding-for-unified-multimodal-understanding-and-generation
### Abstract
In this paper, we introduce Janus, an autoregressive framework that unifies
multimodal understanding and generation. Prior research often relies on a
single visual encoder for both tasks, such as Chameleon. However, due to the
differing levels of information granularity required by multimodal
understanding and generation, this approach can lead to suboptimal performance,
particularly in multimodal understanding. To address this issue, we decouple
visual encoding into separate pathways, while still leveraging a single,
unified transformer architecture for processing. The decoupling not only
alleviates the conflict between the visual encoder's roles in understanding and
generation, but also enhances the framework's flexibility. For instance, both
the multimodal understanding and generation components can independently select
their most suitable encoding methods. Experiments show that Janus surpasses
previous unified model and matches or exceeds the performance of task-specific
models. The simplicity, high flexibility, and effectiveness of Janus make it a
strong candidate for next-generation unified multimodal models.
### 🌟 论文解读 | Janus：解耦视觉编码，统一多模态理解和生成

### 📌 背景痛点/本文动机
多模态模型在理解和生成领域取得了显著进展，但现有模型往往使用单一的视觉编码器来处理理解和生成任务，这导致模型在多模态理解方面表现不佳。本文提出Janus，一个解耦视觉编码的统一多模态框架，旨在解决这一问题。

### 🚀 核心方法
💡 创新点1：解耦视觉编码
Janus引入两个独立的视觉编码路径：一个用于多模态理解，一个用于多模态生成，并通过统一的Transformer架构进行处理。这种解耦设计可以缓解视觉编码器在理解和生成任务中的冲突，并提高框架的灵活性。

💡 创新点2：灵活性和可扩展性
Janus允许理解和生成任务独立选择最合适的编码方法，并可以轻松扩展以支持更多输入模态，如点云、脑电图信号或音频数据。

### 📈 实验结果
Janus在多模态理解和生成任务上取得了最先进的性能，超越了现有的统一模型，并在某些方面甚至超过了特定任务模型。在多模态理解基准测试中，Janus在MMBench、SEED-Bench和POPE上分别取得了69.4、63.7和87.0的分数，超过了LLaVA-v1.5和Qwen-VL-Chat等模型。在视觉生成基准测试中，Janus在MSCOCO-30K和GenEval上分别取得了8.53的FID分数和61%的准确率，超过了DALL-E 2和SDXL等模型。

### 💬 可借鉴之处
Janus的设计理念为开发下一代统一多模态模型提供了新的思路。其解耦视觉编码的方法可以有效地解决多模态理解和生成任务之间的冲突，并提高模型的性能和灵活性。此外，Janus的架构简单、易于扩展，可以轻松地集成更多输入模态，使其成为一个强大的多模态通用模型。

## puma--empowering-unified-mllm-with-multi-granular-visual-generation
### Abstract
Recent advancements in multimodal foundation models have yielded significant
progress in vision-language understanding. Initial attempts have also explored
the potential of multimodal large language models (MLLMs) for visual content
generation. However, existing works have insufficiently addressed the varying
granularity demands of different image generation tasks within a unified MLLM
paradigm - from the diversity required in text-to-image generation to the
precise controllability needed in image manipulation. In this work, we propose
PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA
unifies multi-granular visual features as both inputs and outputs of MLLMs,
elegantly addressing the different granularity requirements of various image
generation tasks within a unified MLLM framework. Following multimodal
pretraining and task-specific instruction tuning, PUMA demonstrates proficiency
in a wide range of multimodal tasks. This work represents a significant step
towards a truly unified MLLM capable of adapting to the granularity demands of
various visual tasks. The code and model will be released in
https://github.com/rongyaofang/PUMA.
### 🌟 论文解读 | PUMA：多粒度视觉生成赋能统一多模态大语言模型

### 📌 背景痛点/本文动机
随着多模态基础模型在视觉-语言理解方面的显著进步，多模态大语言模型（MLLMs）在视觉内容生成方面的潜力也逐渐被探索。然而，现有的工作在统一MLLM框架内处理不同图像生成任务时，对粒度需求的差异考虑不足。例如，文本到图像生成需要多样性，而图像编辑则需要精确的可控性。现有的方法大多依赖于从视觉编码器中提取的单粒度特征，忽略了不同任务对粒度需求的差异。

### 🚀 核心方法
为了解决上述问题，本文提出了PUMA（emPowering Unified MLLM with Multi-grAnular visual generation），一个统一的多粒度MLLM，它将多粒度视觉特征作为MLLM的输入和输出，优雅地解决了不同图像生成任务在统一MLLM框架内的粒度需求差异。

PUMA的主要创新点包括：

💡 创新点1：多粒度视觉编码和解码
PUMA使用语义图像编码器提取多尺度特征，并使用一组专门的扩散模型作为解码器，以处理不同粒度的特征。这种设计使得PUMA能够在保持语义准确性的同时，实现生成多样性，从而更好地适应各种视觉任务的需求。

💡 创新点2：多粒度自回归MLLM
PUMA设计了一个自回归MLLM，它可以处理和生成文本标记以及多粒度图像特征。该模型通过逐步预测每个粒度级别的每个标记，并从最粗粒度级别到最细粒度级别进行预测，从而能够捕获不同尺度之间的依赖关系。

### 📈 实验结果
PUMA在多种视觉任务上取得了优异的性能，包括：

* **多样化的文本到图像生成**：PUMA能够生成与文本条件相对应的多样化图像。
* **图像编辑**：PUMA能够进行精确的图像编辑，例如添加、删除或替换图像中的元素。
* **条件图像生成**：PUMA能够根据特定条件生成图像，例如将Canny边缘图像转换为自然图像。
* **图像理解**：PUMA在图像理解任务上表现出色，例如视觉问答和视觉推理。

### 💬 可借鉴之处
PUMA的多粒度视觉生成框架为MLLMs在视觉内容生成方面的应用提供了新的思路。其多粒度特征编码和解码的设计，以及多粒度自回归MLLM的架构，都值得进一步研究和探索。此外，PUMA在多种视觉任务上的优异性能也表明，多粒度方法在视觉内容生成方面具有巨大的潜力。

## mmar--towards-lossless-multi-modal-auto-regressive-probabilistic-modeling
### Abstract
Recent advancements in multi-modal large language models have propelled the
development of joint probabilistic models capable of both image understanding
and generation. However, we have identified that recent methods inevitably
suffer from loss of image information during understanding task, due to either
image discretization or diffusion denoising steps. To address this issue, we
propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling
framework. Unlike discretization line of method, MMAR takes in
continuous-valued image tokens to avoid information loss. Differing from
diffusion-based approaches, we disentangle the diffusion process from
auto-regressive backbone model by employing a light-weight diffusion head on
top each auto-regressed image patch embedding. In this way, when the model
transits from image generation to understanding through text generation, the
backbone model's hidden representation of the image is not limited to the last
denoising step. To successfully train our method, we also propose a
theoretically proven technique that addresses the numerical stability issue and
a training strategy that balances the generation and understanding task goals.
Through extensive evaluations on 18 image understanding benchmarks, MMAR
demonstrates much more superior performance than other joint multi-modal
models, matching the method that employs pretrained CLIP vision encoder,
meanwhile being able to generate high quality images at the same time. We also
showed that our method is scalable with larger data and model size.
### 🌟 论文解读 | MMAR：迈向无损多模态自回归概率建模

### 📌 背景痛点/本文动机
近年来，多模态大型语言模型在图像理解和生成方面取得了显著进展。然而，现有的多模态概率模型在图像理解任务中不可避免地会丢失图像信息，这主要是因为图像离散化或扩散去噪步骤导致的。为了解决这个问题，本文提出了一个新颖的多模态自回归（MMAR）概率建模框架。

### 🚀 核心方法
💡 创新点1：连续图像表示
与离散化方法不同，MMAR采用连续值图像标记，避免了信息丢失。通过引入一个轻量级的扩散头，将扩散过程与自回归骨干模型解耦，从而在图像生成和理解任务中充分利用图像建模能力。

💡 创新点2：低精度训练下的数值稳定性
针对低精度训练设置下扩散模型训练和推理过程中数值精度敏感的问题，本文提出了一种理论上证明的技术，通过优化扩散模型参数化来最小化数值误差。

💡 创新点3：平衡生成和理解任务的训练策略
为了平衡图像生成和理解任务的目标，本文提出了一种两阶段训练方法。第一阶段使用大规模、中等质量的数据来增强模型的数据分布多样性，从而提高模型的理解能力。第二阶段使用少量高质量数据来进一步提高图像生成能力并细化模型对图像的理解。

### 📈 实验结果
在18个图像理解基准测试中，MMAR表现出比其他多模态模型更优越的性能，与使用预训练CLIP视觉编码器的方法相当，同时能够生成高质量的图像。此外，实验结果表明，MMAR方法具有可扩展性，随着数据量和模型规模的增加，性能也会得到提升。

### 💬 可借鉴之处
MMAR框架为多模态概率建模提供了一种新的思路，通过连续图像表示和低精度训练下的数值稳定性技术，实现了无损的多模态自回归概率建模。此外，两阶段训练策略为平衡生成和理解任务提供了有效的解决方案。这些创新点对于多模态模型的研究和应用具有重要的参考价值。

## looking-at-ctr-prediction-again--is-attention-all-you-need-
### Abstract
Click-through rate (CTR) prediction is a critical problem in web search,
recommendation systems and online advertisement displaying. Learning good
feature interactions is essential to reflect user's preferences to items. Many
CTR prediction models based on deep learning have been proposed, but
researchers usually only pay attention to whether state-of-the-art performance
is achieved, and ignore whether the entire framework is reasonable. In this
work, we use the discrete choice model in economics to redefine the CTR
prediction problem, and propose a general neural network framework built on
self-attention mechanism. It is found that most existing CTR prediction models
align with our proposed general framework. We also examine the expressive power
and model complexity of our proposed framework, along with potential extensions
to some existing models. And finally we demonstrate and verify our insights
through some experimental results on public datasets.
### 🌟 论文解读 | 重新审视点击率预测：注意力机制真的足够吗？

### 📌 背景痛点/本文动机
点击率（CTR）预测是网络搜索、推荐系统和在线广告显示中的关键问题。学习良好的特征交互对于反映用户对物品的偏好至关重要。许多基于深度学习的CTR预测模型已被提出，但研究人员通常只关注是否达到了最先进的性能，而忽略了整个框架是否合理。本文使用经济学中的离散选择模型重新定义了CTR预测问题，并提出了一种基于自注意力机制的通用神经网络框架。

### 🚀 核心方法
💡 创新点1：本文使用经济学中的离散选择模型重新定义了CTR预测问题，并提出了一种基于自注意力机制的通用神经网络框架。该框架包括嵌入层、特征交互层、聚合层和空间变换层，可以涵盖大多数现有的CTR预测模型。

💡 创新点2：本文提出了一种基于自注意力机制的特征交互形式，可以包含大多数现有CTR预测模型的特征处理功能。此外，本文还分析了该框架中特征交互算子的表达能力，并提出了扩展先前模型的方法。

### 📈 实验结果
本文在两个公开数据集上进行了实验，结果表明，本文提出的模型在性能上与大多数现有的CTR模型相当，甚至在某些情况下优于它们。

### 💬 可借鉴之处
本文提出的通用神经网络框架为CTR预测问题提供了一种新的视角，并为未来的研究提供了有价值的参考。此外，本文提出的基于自注意力机制的特征交互形式可以应用于其他领域，例如推荐系统、自然语言处理等。

## mio--a-foundation-model-on-multimodal-tokens
### Abstract
In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.
### 🌟 论文解读 | MIO：基于多模态token的通用模型

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）和多模态大型语言模型（MM-LLMs）的兴起，人工智能在通用智能（AGI）方面取得了巨大进步。然而，现有的MM-LLMs仍然缺乏真正的“任意到任意”的理解和生成能力，即无法在多种模态之间进行无缝转换和生成。例如，GPT-4o虽然展示了强大的“任意到任意”LLMs潜力，但它不支持多模态交错序列的生成。为了解决这一难题，本文提出了MIO，一个基于多模态token的通用模型，能够理解和生成语音、文本、图像和视频，并支持多模态交错序列的生成。

### 🚀 核心方法
💡 创新点1：多模态tokenization
MIO使用SEED-Tokenizer对图像进行tokenization，使用SpeechTokenizer对语音进行tokenization，并将这些token添加到LLM的词汇表中。由于图像token和语音token都包含因果语义，因此可以像文本token一样进行自回归训练。

💡 创新点2：因果多模态建模
MIO使用因果语言模型（CausalLM）进行训练，将多模态token的生成目标统一为下一个token的预测，并使用交叉熵损失进行优化。

💡 创新点3：多模态交错序列生成
MIO支持多模态交错序列的生成，例如视频-文本交错序列，这使得MIO能够进行视觉故事讲述、视觉思维链推理等高级任务。

💡 创新点4：四阶段训练过程
MIO采用四阶段训练过程，包括：
1. 对齐预训练：学习与语言空间更对齐的多模态表示。
2. 交错预训练：获得具有更丰富上下文语义的多模态表示。
3. 语音增强预训练：增强模型的语音相关能力。
4. 综合监督微调：在多种文本、视觉和语音任务上进行微调。

### 📈 实验结果
MIO在图像理解、图像生成、语音理解和语音生成、视频理解和视频生成等任务上取得了与现有双模态模型和“任意到任意”模型相当甚至更优的性能。

### 💬 可借鉴之处
MIO的设计和训练过程为构建通用多模态模型提供了新的思路，其多模态交错序列生成能力也为视觉故事讲述、视觉思维链推理等高级任务提供了新的可能性。

## monoformer--one-transformer-for-both-diffusion-and-autoregression
### Abstract
Most existing multimodality methods use separate backbones for
autoregression-based discrete text generation and diffusion-based continuous
visual generation, or the same backbone by discretizing the visual data to use
autoregression for both text and visual generation. In this paper, we propose
to study a simple idea: share one transformer for both autoregression and
diffusion. The feasibility comes from two main aspects: (i) Transformer is
successfully applied to diffusion for visual generation, and (ii) transformer
training for autoregression and diffusion is very similar, and the difference
merely lies in that diffusion uses bidirectional attention mask and
autoregression uses causal attention mask. Experimental results show that our
approach achieves comparable image generation performance to current
state-of-the-art methods as well as maintains the text generation capability.
The project is publicly available at https://monoformer.github.io/.
### 🌟 论文解读 | MonoFormer：一箭双雕，Transformer同时处理扩散和自回归

### 📌 背景痛点/本文动机
现有的多模态方法通常为基于自回归的离散文本生成和基于扩散的连续视觉生成使用独立的骨干网络，或者通过将视觉数据离散化来使用自回归进行文本和视觉生成。然而，这种方法存在一些局限性，例如需要训练多个模型，导致计算成本高，且难以实现模型之间的知识共享。

### 🚀 核心方法
MonoFormer 提出了一个简单而有效的想法：共享一个 Transformer 进行自回归和扩散。其可行性主要基于以下两点：
1. Transformer 已成功应用于扩散模型进行视觉生成。
2. Transformer 在自回归和扩散训练中的过程非常相似，主要区别在于扩散使用双向注意力掩码，而自回归使用因果注意力掩码。

MonoFormer 使用一个共享的 Transformer 进行文本生成和图像生成。对于文本生成，Transformer 接收文本 token 嵌入作为输入，并逐个预测输出 token。对于图像生成，Transformer 接收噪声化的潜在嵌入作为输入，并预测噪声，然后通过 VAE 解码器生成图像。

### 📈 实验结果
在 ImageNet 数据集上进行的实验表明，MonoFormer 在图像生成方面取得了与当前最先进方法相当的性能，同时保持了文本生成能力。在文本生成方面，MonoFormer 的性能略低于 TinyLlama 基线模型，这可能是由于混合训练图像生成数据集导致的。未来可以通过增加更多语言数据来进一步提高性能。

### 💬 可借鉴之处
MonoFormer 的主要创新点在于共享一个 Transformer 进行自回归和扩散，从而实现了模型之间的知识共享，降低了计算成本。此外，MonoFormer 还采用了预训练的 LLM 进行 Transformer 初始化，进一步提高了模型性能。这些方法可以为多模态理解和生成模型的构建提供新的思路。

## vila-u--a-unified-foundation-model-integrating-visual-understanding-and-generation
### Abstract
VILA-U is a Unified foundation model that integrates Video, Image, Language
understanding and generation. Traditional visual language models (VLMs) use
separate modules for understanding and generating visual content, which can
lead to misalignment and increased complexity. In contrast, VILA-U employs a
single autoregressive next-token prediction framework for both tasks,
eliminating the need for additional components like diffusion models. This
approach not only simplifies the model but also achieves near state-of-the-art
performance in visual language understanding and generation. The success of
VILA-U is attributed to two main factors: the unified vision tower that aligns
discrete visual tokens with textual inputs during pretraining, which enhances
visual perception, and autoregressive image generation can achieve similar
quality as diffusion models with high-quality dataset. This allows VILA-U to
perform comparably to more complex models using a fully token-based
autoregressive framework.
### 🌟 论文解读 | VILA-U：统一视觉理解和生成的基础模型

### 📌 背景痛点/本文动机
传统的视觉语言模型（VLMs）通常使用独立的模块来理解和生成视觉内容，这可能导致模块之间的不匹配和模型复杂度的增加。为了解决这个问题，本文提出了VILA-U，一个统一的视觉语言模型，它将视频、图像和语言的理解和生成任务整合到一个自回归的下一个标记预测框架中。

### 🚀 核心方法
💡 创新点1：统一的视觉塔
VILA-U采用统一的视觉塔，通过向量量化将视觉输入转换为离散的标记，并与文本输入进行对齐。这种对齐在预训练过程中进行，可以增强视觉感知能力。

💡 创新点2：自回归图像生成
VILA-U使用自回归模型进行图像生成，无需额外的扩散模型。通过在高质量数据集上进行训练，自回归图像生成可以达到与扩散模型相似的质量。

### 📈 实验结果
VILA-U在视觉语言理解和生成任务上取得了接近最先进的性能。在视觉理解方面，VILA-U的性能接近于使用连续视觉标记的领先VLMs，甚至在相同的LLM大小下，性能超过了许多方法。在视觉生成方面，VILA-U的FID得分优于其他自回归方法，并且与一些基于扩散的方法具有可比的性能。

### 💬 可借鉴之处
VILA-U的统一框架可以作为一个通用的视觉语言模型，用于各种视觉语言任务。此外，VILA-U的视觉塔和自回归图像生成方法可以应用于其他多模态任务，例如音频和文本的理解和生成。

## show-o--one-single-transformer-to-unify-multimodal-understanding-and-generation
### Abstract
We present a unified transformer, i.e., Show-o, that unifies multimodal
understanding and generation. Unlike fully autoregressive models, Show-o
unifies autoregressive and (discrete) diffusion modeling to adaptively handle
inputs and outputs of various and mixed modalities. The unified model flexibly
supports a wide range of vision-language tasks including visual
question-answering, text-to-image generation, text-guided
inpainting/extrapolation, and mixed-modality generation. Across various
benchmarks, it demonstrates comparable or superior performance to existing
individual models with an equivalent or larger number of parameters tailored
for understanding or generation. This significantly highlights its potential as
a next-generation foundation model. Code and models are released at
https://github.com/showlab/Show-o.
### 🌟 论文解读 | Show-o：一个Transformer统一多模态理解和生成

### 📌 背景痛点/本文动机
近年来，多模态智能的两个关键支柱——理解和生成取得了显著进展。多模态理解方面，如LLaVA等模型在视觉问答等任务中表现出色；视觉生成方面，如DDPMs等模型在文本到图像/视频生成方面取得了突破性进展。然而，现有的多模态理解和生成模型大多将每个领域独立处理，缺乏一个统一的模型来同时处理这两个任务。

### 🚀 核心方法
💡 创新点1：统一Transformer架构
Show-o提出了一种统一的Transformer架构，将自回归和扩散建模相结合，以适应不同和混合模态的输入和输出。这使得Show-o能够灵活地支持各种视觉语言任务，包括视觉问答、文本到图像生成、文本引导的修复/外推和混合模态生成。

💡 创新点2：Omni-Attention机制
Show-o引入了Omni-Attention机制，这是一种具有因果和全注意力机制的全面注意力机制，能够根据输入序列的格式自适应地混合和变化。这使得Show-o能够以不同的方式处理文本和图像信号，从而提高模型效率和下游应用的性能。

💡 创新点3：离散扩散建模
Show-o采用离散扩散建模来处理图像生成，而不是连续扩散。这使得Show-o能够以更少的采样步骤生成图像，从而提高生成效率。

### 📈 实验结果
在多个基准测试中，Show-o在多模态理解和生成方面表现出与现有模型相当甚至更好的性能。例如，在MSCOCO数据集上，Show-o的FID分数为9.24，优于许多参数规模更大的生成模型。在GenEval基准测试中，Show-o在所有六个指标上都取得了显著优于LDM等模型的性能。

### 💬 可借鉴之处
Show-o的成功表明，将自回归和扩散建模相结合的统一Transformer架构具有巨大的潜力。Omni-Attention机制和离散扩散建模等技术也为多模态理解和生成领域提供了新的思路。此外，Show-o的实验结果也为未来统一模型的设计提供了有价值的见解。

### 📚 参考资料
[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528)

## transfusion--predict-the-next-token-and-diffuse-images-with-one-multi-modal-model
### Abstract
We introduce Transfusion, a recipe for training a multi-modal model over
discrete and continuous data. Transfusion combines the language modeling loss
function (next token prediction) with diffusion to train a single transformer
over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B
parameters from scratch on a mixture of text and image data, establishing
scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our
experiments show that Transfusion scales significantly better than quantizing
images and training a language model over discrete image tokens. By introducing
modality-specific encoding and decoding layers, we can further improve the
performance of Transfusion models, and even compress each image to just 16
patches. We further demonstrate that scaling our Transfusion recipe to 7B
parameters and 2T multi-modal tokens produces a model that can generate images
and text on a par with similar scale diffusion models and language models,
reaping the benefits of both worlds.
### 🌟 论文解读 | Transfusion：一种预测下一个标记并扩散图像的多模态模型

### 📌 背景痛点/本文动机
多模态生成模型需要能够感知、处理和生成离散元素（如文本或代码）和连续元素（如图像、音频和视频数据）。虽然基于下一个标记预测目标训练的语言模型在离散模态中占据主导地位，但扩散模型及其泛化形式在生成连续模态方面处于最前沿。许多努力旨在结合这些方法，包括将语言模型扩展为使用扩散模型作为工具，或者将预训练的扩散模型嫁接到语言模型上。另一种方法是将连续模态量化，并在离散标记上训练标准语言模型，从而简化模型架构，但会丢失信息。在这项工作中，我们展示了通过训练单个模型来预测离散文本标记和扩散连续图像，从而完全整合这两种模态，并且没有信息损失的可能性。

### 🚀 核心方法
Transfusion 是一种训练模型的方法，可以无缝地生成离散和连续模态。我们通过在 50% 文本和 50% 图像数据上预训练一个 transformer 模型来演示 Transfusion，每个模态使用不同的目标：文本使用下一个标记预测，图像使用扩散。模型在每个训练步骤中都暴露于两种模态和损失函数。标准嵌入层将文本标记转换为向量，而分块层将每个图像表示为分块向量。我们对文本标记应用因果注意力，并对图像块应用双向注意力。对于推理，我们引入了一种解码算法，该算法结合了从语言模型生成文本的标准做法和从扩散模型生成图像的标准做法。

### 📈 实验结果
与 Chameleon 的离散化方法相比，Transfusion 模型在每种模态组合中都表现出更好的扩展性。在文本到图像生成中，我们发现 Transfusion 在不到 Chameleon 计算量的三分之一的情况下超过了 Chameleon 方法，无论是 FID 还是 CLIP 分数。当控制 FLOPs 时，Transfusion 实现了比 Chameleon 模型低约 2 倍的 FID 分数。我们在图像到文本生成中观察到类似的趋势，其中 Transfusion 在 21.8% 的 FLOPs 下与 Chameleon 相匹配。令人惊讶的是，Transfusion 在学习文本到文本预测方面也更有效率，在文本任务上实现了困惑度平衡，大约是 Chameleon 的 FLOPs 的 50% 到 60%。

### 💬 可借鉴之处
Transfusion 是一种很有前景的训练真正多模态模型的方法。它通过训练单个模型来预测离散文本标记和扩散连续图像，从而完全整合了这两种模态，并且没有信息损失。Transfusion 在各种模态组合中都表现出更好的扩展性，并且可以生成高质量的文本和图像。此外，Transfusion 还可以生成基于其他图像的图像，这表明它可以适应和泛化到新的模态组合。

## anole--an-open--autoregressive--native-large-multimodal-models-for-interleaved-image-text-generation
### Abstract
Previous open-source large multimodal models (LMMs) have faced several
limitations: (1) they often lack native integration, requiring adapters to
align visual representations with pre-trained large language models (LLMs); (2)
many are restricted to single-modal generation; (3) while some support
multimodal generation, they rely on separate diffusion models for visual
modeling and generation. To mitigate these limitations, we present Anole, an
open, autoregressive, native large multimodal model for interleaved image-text
generation. We build Anole from Meta AI's Chameleon, adopting an innovative
fine-tuning strategy that is both data-efficient and parameter-efficient. Anole
demonstrates high-quality, coherent multimodal generation capabilities. We have
open-sourced our model, training framework, and instruction tuning data.
### 🌟 论文解读 | ANOLE：开启多模态生成新纪元

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，多模态模型在图像和文本生成方面取得了显著进展。然而，现有的开源多模态模型（LMMs）存在一些局限性，例如缺乏原生集成、单模态生成限制以及依赖额外的扩散模型进行视觉建模和生成。为了解决这些问题，本文提出了ANOLE，一个开放、自回归、原生的多模态模型，用于交错图像-文本生成。

### 🚀 核心方法
💡 创新点1：原生多模态集成
ANOLE基于Meta AI的Chameleon模型，采用创新的微调策略，实现了视觉表示与预训练大型语言模型（LLMs）的原生集成，无需适配器进行对齐。

💡 创新点2：高效微调
ANOLE采用数据高效和参数高效的微调策略，仅需约6000个样本和不到4000万个参数，即可有效实现视觉和多媒体生成能力。

💡 创新点3：自回归生成
ANOLE采用自回归方法进行图像和文本生成，能够生成高质量、连贯的多模态序列。

💡 创新点4：开放资源
ANOLE开源了模型、训练框架和指令微调数据，为研究人员和开发者提供了丰富的资源和支持。

### 📈 实验结果
ANOLE在图像生成和交错图像-文本生成方面表现出色。生成的图像质量高，与给定指令紧密相关，并能够生成多样化的图像类型。此外，ANOLE能够生成组织良好、提供全面细节的文本，并与图像无缝集成，实现完美的视觉和文本元素互补。

### 💬 可借鉴之处
ANOLE为多模态生成领域带来了新的突破，其原生多模态集成、高效微调、自回归生成和开放资源等特点为研究人员和开发者提供了宝贵的借鉴和启示。未来，ANOLE有望在精确指令遵循、上下文长度扩展、多模态理解能力提升以及下游任务应用等方面取得进一步发展。

## hybrid-alignment-training-for-large-language-models
### Abstract
Alignment training is crucial for enabling large language models (LLMs) to
cater to human intentions and preferences. It is typically performed based on
two stages with different objectives: instruction-following alignment and
human-preference alignment. However, aligning LLMs with these objectives in
sequence suffers from an inherent problem: the objectives may conflict, and the
LLMs cannot guarantee to simultaneously align with the instructions and human
preferences well. To response to these, in this work, we propose a Hybrid
Alignment Training (Hbat) approach, based on alternating alignment and modified
elastic weight consolidation methods. The basic idea is to alternate between
different objectives during alignment training, so that better collaboration
can be achieved between the two alignment tasks.We experiment with Hbat on
summarization and dialogue tasks. Experimental results show that the proposed
\textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields
consistent performance gains over the traditional two-stage alignment training
when using both proximal policy optimization and direct preference
optimization.
### 🌟 论文解读 | 混合对齐训练：提升大型语言模型性能的新方法

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在理解和执行人类指令方面存在挑战，因为它们缺乏对人类意图和偏好的理解。为了解决这个问题，通常采用两阶段的对齐训练：指令遵循对齐和人类偏好对齐。然而，这种顺序对齐训练存在固有的问题：两个目标可能冲突，LLMs无法保证同时很好地对齐指令和人类偏好。

### 🚀 核心方法
本文提出了一种混合对齐训练（HBAT）方法，通过交替对齐和改进的弹性权重整合方法来解决上述问题。

💡 创新点1：交替对齐
HBAT 采用交替对齐方法，在训练过程中交替学习指令遵循对齐和人类偏好对齐，从而更好地协调这两个对齐任务。

💡 创新点2：改进的弹性权重整合
HBAT 引入改进的弹性权重整合（EWC）方法，动态地对每个参数施加适当的约束，从而缓解与先前目标的优化冲突。

### 📈 实验结果
在摘要和全文中，HBAT 在摘要和对话任务上进行了实验，结果表明 HBAT 可以显著优于所有基线方法。例如，基于 LLaMA2-13B 模型，HBAT 在摘要任务上比传统的 RLHF 方法提高了 2.26 个 ROUGE-L 分数，在对话任务上提高了 21.01 个 GPT-4 胜率分数。

### 💬 可借鉴之处
HBAT 方法为解决 LLM 对齐训练中的优化冲突问题提供了一种新的思路，可以应用于其他 NLP 任务和偏好对齐方法。此外，HBAT 方法还可以与其他优化对齐方法相结合，进一步提升 LLM 的性能。

## relational-programming-with-foundation-models
### Abstract
Foundation models have vast potential to enable diverse AI applications. The
powerful yet incomplete nature of these models has spurred a wide range of
mechanisms to augment them with capabilities such as in-context learning,
information retrieval, and code interpreting. We propose Vieira, a declarative
framework that unifies these mechanisms in a general solution for programming
with foundation models. Vieira follows a probabilistic relational paradigm and
treats foundation models as stateless functions with relational inputs and
outputs. It supports neuro-symbolic applications by enabling the seamless
combination of such models with logic programs, as well as complex, multi-modal
applications by streamlining the composition of diverse sub-models. We
implement Vieira by extending the Scallop compiler with a foreign interface
that supports foundation models as plugins. We implement plugins for 12
foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9
challenging tasks that span language, vision, and structured and vector
databases. Our evaluation shows that programs in Vieira are concise, can
incorporate modern foundation models, and have comparable or better accuracy
than competitive baselines.
### 🌟 论文解读 | 基于基础模型的关联编程框架：Vieira

### 📌 背景痛点/本文动机
随着深度学习的发展，基础模型（如GPT、CLIP等）在各个领域展现出强大的能力。然而，这些模型在编程AI应用时存在局限性，例如容易产生幻觉、缺乏结构化数据处理能力、难以组合不同模态的数据等。为了克服这些局限性，本文提出了Vieira，一个基于关联编程的框架，旨在为编程基础模型提供一种通用的解决方案。

### 🚀 核心方法
💡 创新点1：关联编程范式
Vieira采用关联编程范式，将基础模型视为具有关联输入和输出的无状态函数。这种范式使得基础模型可以与逻辑程序无缝结合，支持神经符号应用，并简化了多模态子模型的组合。

💡 创新点2：插件式框架
Vieira通过扩展Scallop编译器，实现了一个支持基础模型作为插件的框架。该框架提供了一个可定制和可扩展的插件库，包括GPT、CLIP、SAM等12个基础模型。这使得Vieira能够支持各种应用，并具有减少幻觉、增强检索和多模态组合性等优势。

### 📈 实验结果
Vieira在9个具有挑战性的任务上进行了评估，涵盖了语言、视觉、结构化和向量数据库等领域。结果表明，Vieira的程序简洁，能够整合现代基础模型，并且在准确率方面与竞争基线相当或更好。

### 💬 可借鉴之处
Vieira为编程基础模型提供了一种新的思路，其关联编程范式和插件式框架具有以下可借鉴之处：

* **关联编程范式**：将基础模型视为关联函数，可以方便地与逻辑程序结合，支持神经符号应用和多模态组合。
* **插件式框架**：通过插件库支持多种基础模型，使得Vieira能够适应不同的应用场景。
* **概率逻辑推理**：Vieira支持概率逻辑推理，可以处理不确定性信息，并提高模型的鲁棒性。

### 总结
Vieira是一个基于关联编程的框架，为编程基础模型提供了一种通用的解决方案。该框架具有简洁、易用、可扩展等优点，在多个任务上取得了优异的性能。Vieira为开发基于基础模型的AI应用提供了新的思路和方法，具有广泛的应用前景。

## seed-x--multimodal-models-with-unified-multi-granularity-comprehension-and-generation
### Abstract
The rapid evolution of multimodal foundation model has demonstrated
significant progresses in vision-language understanding and generation, e.g.,
our previous work SEED-LLaMA. However, there remains a gap between its
capability and the real-world applicability, primarily due to the model's
limited capacity to effectively respond to various user instructions and
interact with diverse visual data. In this work, we focus on bridging this gap
through integrating two enhanced features: (1) comprehending images of
arbitrary sizes and ratios, and (2) enabling multi-granularity image
generation. We present a unified and versatile foundation model, namely,
SEED-X, which is able to model multi-granularity visual semantics for
comprehension and generation tasks. Besides the competitive results on public
benchmarks, SEED-X demonstrates its effectiveness in handling real-world
applications across various domains after instruction tuning. We hope that our
work will inspire future research into what can be achieved by versatile
multimodal foundation models in real-world applications. The models, codes, and
datasets are released in https://github.com/AILab-CVC/SEED-X.
### 🌟 论文解读 | SEED-X：多模态模型的统一多粒度理解和生成

### 📌 背景痛点/本文动机
随着多模态基础模型在视觉语言理解和生成方面的快速发展，例如SEED-LLaMA，这些模型在学术基准测试中表现出色。然而，它们在实际应用中的能力仍然有限，主要原因是模型难以有效响应各种用户指令和与多样化的视觉数据进行交互。本文旨在通过集成两个增强功能来弥合这一差距：（1）理解任意大小和比例的图像，以及（2）实现多粒度图像生成。

### 🚀 核心方法
💡 创新点1：视觉分词和解码
SEED-X采用视觉分词器来统一图像理解和生成，并预训练一个多粒度解码器来促进图像生成和高精度图像操作。首先，使用预训练的ViT作为视觉分词器，并预训练一个视觉解码器，通过接收ViT特征作为输入来解码逼真的图像。然后，进一步微调视觉解码器，使其能够接收额外的条件图像作为输入，以保留输入图像的细粒度细节。

💡 创新点2：动态分辨率图像编码
为了使多模态语言模型能够处理任意大小和纵横比的图像，本文提出了动态分辨率图像编码。该方法将输入图像划分为子图像，并为每个子图像的ViT特征添加可扩展的二维位置嵌入，从而使模型能够处理训练过程中未遇到的图像分辨率。

### 📈 实验结果
SEED-X在公共基准测试中取得了有竞争力的结果，并在经过指令调整后，在各个领域的实际应用中表现出有效性。实验结果表明，SEED-X在多模态理解和生成方面具有出色的性能，能够生成与指令高度一致的图像，并保留输入图像的细粒度细节。

### 💬 可借鉴之处
SEED-X的研究结果表明，通过统一多粒度理解和生成，多模态基础模型在现实世界场景中具有巨大的潜力。本文提出的视觉分词和解码方法以及动态分辨率图像编码技术为多模态模型的实际应用提供了新的思路和方法。

## mini-gemini--mining-the-potential-of-multi-modality-vision-language-models
### Abstract
In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
### 🌟 论文解读 | Mini-Gemini：挖掘多模态视觉语言模型的潜力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，赋予多模态输入能力已成为当前视觉语言模型（VLMs）的重要组成部分。尽管VLMs在视觉对话和推理方面取得了进展，但与GPT-4和Gemini等先进模型相比，仍然存在性能差距。本文旨在通过挖掘VLMs的潜力，缩小这一差距，并提高VLMs的性能和灵活性。

### 🚀 核心方法
💡 创新点1：高分辨率视觉标记
为了增强视觉标记，本文提出了一种双编码器系统，其中一个用于高分辨率图像，另一个用于低分辨率视觉嵌入。这种设计可以在不增加视觉标记数量的情况下，提高视觉细节。

💡 创新点2：高质量数据
为了提高数据质量，本文收集和制作了更多基于公共资源的数据，包括高质量响应、任务导向指令和生成相关数据。这些数据有助于提高模型的性能和扩展其能力。

💡 创新点3：VLM引导生成
本文将VLM与先进的生成模型相结合，以支持图像和文本的并发生成。VLM通过提供LLMs生成的文本，来指导图像生成。

### 📈 实验结果
实验结果表明，Mini-Gemini在多个零样本基准测试中取得了领先性能，甚至超过了Gemini Pro、Qwen-VL-Plus和GPT 4V等私有模型。

### 💬 可借鉴之处
Mini-Gemini框架为VLMs的研究和应用提供了新的思路。其双编码器系统、高质量数据集和VLM引导生成方法，都可以为其他VLMs的设计和应用提供参考。

## anygpt--unified-multimodal-llm-with-discrete-sequence-modeling
### Abstract
We introduce AnyGPT, an any-to-any multimodal language model that utilizes
discrete representations for the unified processing of various modalities,
including speech, text, images, and music. AnyGPT can be trained stably without
any alterations to the current large language model (LLM) architecture or
training paradigms. Instead, it relies exclusively on data-level preprocessing,
facilitating the seamless integration of new modalities into LLMs, akin to the
incorporation of new languages. We build a multimodal text-centric dataset for
multimodal alignment pre-training. Utilizing generative models, we synthesize
the first large-scale any-to-any multimodal instruction dataset. It consists of
108k samples of multi-turn conversations that intricately interweave various
modalities, thus equipping the model to handle arbitrary combinations of
multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is
capable of facilitating any-to-any multimodal conversation while achieving
performance comparable to specialized models across all modalities, proving
that discrete representations can effectively and conveniently unify multiple
modalities within a language model. Demos are shown in
https://junzhan2000.github.io/AnyGPT.github.io/
### 🌟 论文解读 | AnyGPT：统一多模态语言模型，实现任意模态间的无缝转换

### 📌 背景痛点/本文动机
随着人工智能的发展，大型语言模型（LLM）在理解和生成人类语言方面取得了显著进展。然而，这些模型的能力仅限于文本处理，而现实世界是 inherently 多模态的，包括视觉、语言、声音和触觉等多种信息渠道。因此，将 LLM 扩展到多模态感知成为了一个重要的研究方向。

### 🚀 核心方法
💡 创新点1：AnyGPT，一个基于 token 的任意模态到任意模态的多模态语言模型
AnyGPT 能够理解和生成包括语音、文本、图像和音乐在内的多种模态。它通过使用多模态分词器将原始多模态数据（如图像和音频）压缩成离散的语义 token 序列，然后使用 LLM 对这些 token 序列进行自回归训练。在推理过程中，多模态 token 被解码回原始模态表示。

💡 创新点2：构建 AnyInstruct-108k 数据集，实现任意模态间的对话
为了解决多模态对齐数据稀缺的问题，AnyGPT 构建了一个文本为中心的多模态对齐数据集 AnyInstruct-108k，包含 108k 个多轮对话样本，这些对话样本交织了多种模态，使模型能够处理任意组合的多模态输入和输出。

### 📈 实验结果
实验结果表明，AnyGPT 在各种跨模态任务中取得了有希望的结果，并证明了离散表示可以有效地将多个模态统一到一个统一的 LLM 中。

### 💬 可借鉴之处
AnyGPT 的创新之处在于其使用离散表示来统一处理多种模态，并通过构建大规模的多模态对话数据集来训练模型。这种方法为多模态 LLM 的发展提供了新的思路，并为实现任意模态间的无缝转换提供了可能。

## world-model-on-million-length-video-and-language-with-blockwise-ringattention
### Abstract
Enabling long-context understanding remains a key challenge in scaling
existing sequence models -- a crucial component in developing generally
intelligent models that can process and operate over long temporal horizons
that potentially consist of millions of tokens. In this paper, we aim to
address these challenges by providing a comprehensive exploration of the full
development process for producing 1M context language models and video-language
models, setting new benchmarks in language retrieval and new capabilities in
long video understanding. We detail our long context data curation process,
progressive context extension from 4K to 1M tokens, and present an efficient
open-source implementation for scalable training on long sequences.
Additionally, we open-source a family of 7B parameter models capable of
processing long text documents and videos exceeding 1M tokens.
### 🌟 论文解读 | 基于百万长度视频和语言的块状环注意力世界模型

### 📌 背景痛点/本文动机
随着人工智能的发展，序列模型在处理和理解长文本、图像和视频方面面临着巨大挑战。现有的模型大多局限于处理短序列，难以应对复杂、长篇幅的语言和视觉上下文。此外，训练能够处理数百万个token的模型需要克服高内存和计算成本，以及缺乏长上下文数据的难题。

### 🚀 核心方法
本文提出了一个名为“大型世界模型”（LWM）的模型，旨在解决上述挑战。LWM采用块状环注意力（Blockwise RingAttention）技术，无需近似或开销即可扩展上下文大小，从而实现高效的长序列训练。主要创新点包括：

💡 创新点1：渐进式上下文扩展
LWM通过逐步增加序列长度，从4K token扩展到1M token，有效地解决了长上下文建模的计算成本问题。同时，模型采用RoPE（旋转位置编码）技术，通过调整θ参数来适应更长的上下文长度。

💡 创新点2：模型生成问答数据
为了增强模型在长序列对话中的能力，LWM采用了一种基于模型的问答技术。该技术利用短上下文模型从书籍中生成训练数据，从而显著提升模型在长序列对话中的表现。

💡 创新点3：多模态训练
LWM将视觉模态（图像和视频）融入语言模型，通过优化图像和视频的下一个token预测损失，实现了对视觉信息的有效处理。此外，LWM还采用了掩码序列打包策略和精细的损失平衡，以更好地处理不同长度的序列。

### 📈 实验结果
LWM在长视频理解和长上下文事实检索方面取得了具有竞争力的结果。在语言任务、检索任务和LOFT数据集上的评估结果表明，LWM在处理长序列方面具有显著优势。此外，LWM还展示了图像和视频生成能力，能够根据文本生成静态图像和动态视频。

### 💬 可借鉴之处
LWM的研究成果为长上下文序列模型的开发提供了重要参考。其渐进式上下文扩展、模型生成问答数据和多模态训练等技术，为解决长序列建模的挑战提供了有效途径。此外，LWM的开源实现和模型也为研究人员提供了宝贵的资源。

## video-lavit--unified-video-language-pre-training-with-decoupled-visual-motional-tokenization
### Abstract
In light of recent advances in multimodal Large Language Models (LLMs), there
is increasing attention to scaling them from image-text data to more
informative real-world videos. Compared to static images, video poses unique
challenges for effective large-scale pre-training due to the modeling of its
spatiotemporal dynamics. In this paper, we address such limitations in
video-language pre-training with an efficient video decomposition that
represents each video as keyframes and temporal motions. These are then adapted
to an LLM using well-designed tokenizers that discretize visual and temporal
information as a few tokens, thus enabling unified generative pre-training of
videos, images, and text. At inference, the generated tokens from the LLM are
carefully recovered to the original continuous pixel space to create various
video content. Our proposed framework is both capable of comprehending and
generating image and video content, as demonstrated by its competitive
performance across 13 multimodal benchmarks in image and video understanding
and generation. Our code and models are available at
https://video-lavit.github.io.
### 🌟 论文解读 | Video-LaVIT：基于解耦视觉-运动标记的视频-语言预训练

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在多模态领域的突破，将它们从图像-文本数据扩展到更丰富的真实世界视频数据成为了一个重要的研究方向。然而，与静态图像相比，视频数据由于其时空动态特性，在有效的大规模预训练方面面临着独特的挑战。现有的方法要么无法有效地编码视频的时空动态信息，要么在处理长视频时计算成本过高。

### 🚀 核心方法
💡 创新点1：视频分解
Video-LaVIT 提出了一种高效的视频分解方法，将每个视频表示为关键帧和运动向量。关键帧用于捕捉视频的主要视觉语义，而运动向量则描述了关键帧随时间的动态变化。这种分解方法有效地减少了视频表示所需的标记数量，从而提高了大规模预训练的效率。

💡 创新点2：解耦视觉-运动标记化
Video-LaVIT 设计了两种标记化器来处理视频模态：图像标记化器和运动标记化器。图像标记化器利用现有的图像标记化器（如 LaVIT）来处理关键帧，而运动标记化器则基于 VQ-VAE 架构开发，用于将运动向量转换为离散的标记序列。这种解耦的标记化方法使得模型能够有效地学习视频的视觉和运动信息。

💡 创新点3：视频去标记化
Video-LaVIT 还设计了一个视频去标记化器，用于将 LLM 生成的离散标记序列恢复到原始的连续像素空间，从而生成各种视频内容。视频去标记化器采用条件去噪 U-Net 架构，并通过运动向量进行增强的运动条件，从而提高了视频生成的质量。

💡 创新点4：统一生成建模
Video-LaVIT 将所有模态（视频、图像和文本）都视为 1D 离散标记，并使用 LLM 进行统一的生成预训练。这种统一的建模方法使得模型能够同时理解和生成各种多模态内容。

### 📈 实验结果
Video-LaVIT 在 13 个多模态基准测试中取得了非常出色的性能，包括图像和视频的理解和生成任务。在图像理解方面，Video-LaVIT 在大多数基准测试中取得了最先进的性能。在视频理解方面，Video-LaVIT 在视频问答和视频理解任务中取得了最先进的准确率。在视频生成方面，Video-LaVIT 在文本到视频生成和长视频生成任务中取得了具有竞争力的性能。

### 💬 可借鉴之处
Video-LaVIT 的视频分解、解耦视觉-运动标记化、视频去标记化和统一生成建模等创新方法为视频-语言预训练提供了新的思路，并为构建更强大的多模态 AI 助手奠定了基础。

## mm-interleaved--interleaved-image-text-generative-modeling-via-multi-modal-feature-synchronizer
### Abstract
Developing generative models for interleaved image-text data has both
research and practical value. It requires models to understand the interleaved
sequences and subsequently generate images and text. However, existing attempts
are limited by the issue that the fixed number of visual tokens cannot
efficiently capture image details, which is particularly problematic in the
multi-image scenarios. To address this, this paper presents MM-Interleaved, an
end-to-end generative model for interleaved image-text data. It introduces a
multi-scale and multi-image feature synchronizer module, allowing direct access
to fine-grained image features in the previous context during the generation
process. MM-Interleaved is end-to-end pre-trained on both paired and
interleaved image-text corpora. It is further enhanced through a supervised
fine-tuning phase, wherein the model improves its ability to follow complex
multi-modal instructions. Experiments demonstrate the versatility of
MM-Interleaved in recognizing visual details following multi-modal instructions
and generating consistent images following both textual and visual conditions.
Code and models are available at
\url{https://github.com/OpenGVLab/MM-Interleaved}.
### 🌟 论文解读 | MM-Interleaved：基于多模态特征同步器的交错图像-文本生成模型

### 📌 背景痛点/本文动机
随着互联网上交错图像-文本数据（如新闻、博客）的普及，开发能够理解和生成此类数据的生成模型具有重要的研究价值和实际应用潜力。然而，现有的多模态模型在处理交错图像-文本数据时存在局限性，主要问题在于固定数量的视觉标记无法有效地捕捉图像细节，尤其是在多图像场景中。为了解决这个问题，本文提出了MM-Interleaved，一个端到端的生成模型，用于处理交错图像-文本数据。

### 🚀 核心方法
💡 创新点1：多模态特征同步器（MMFS）
MM-Interleaved引入了一个多尺度、多图像特征同步器模块（MMFS），允许在生成过程中直接访问来自先前上下文的细粒度图像特征。MMFS基于可变形稀疏注意力机制，能够有效地从多尺度图像特征图中提取相关信息，从而弥补了视觉标记数量有限导致的图像细节丢失问题。

💡 创新点2：端到端预训练和监督微调
MM-Interleaved在配对和交错的图像-文本语料库上进行端到端预训练，并通过监督微调阶段进一步增强了模型能力，使其能够更好地遵循复杂的模态指令。

### 📈 实验结果
实验结果表明，MM-Interleaved在识别多模态指令后的视觉细节和生成一致图像方面表现出色。在多个多模态理解和生成基准测试中，MM-Interleaved取得了最先进的性能，并且与现有的图像到文本和文本到图像方法相比，具有更高的标记效率。

### 💬 可借鉴之处
MM-Interleaved的提出为多模态生成模型的发展提供了新的思路，其核心方法MMFS可以有效解决视觉标记数量有限导致的图像细节丢失问题，从而提高模型的生成质量。此外，MM-Interleaved的端到端预训练和监督微调策略也为多模态模型的训练提供了参考。

## unified-io-2--scaling-autoregressive-multimodal-models-with-vision--language--audio--and-action
### Abstract
We present Unified-IO 2, the first autoregressive multimodal model that is
capable of understanding and generating image, text, audio, and action. To
unify different modalities, we tokenize inputs and outputs -- images, text,
audio, action, bounding boxes, etc., into a shared semantic space and then
process them with a single encoder-decoder transformer model. Since training
with such diverse modalities is challenging, we propose various architectural
improvements to stabilize model training. We train our model from scratch on a
large multimodal pre-training corpus from diverse sources with a multimodal
mixture of denoisers objective. To learn an expansive set of skills, such as
following multimodal instructions, we construct and finetune on an ensemble of
120 datasets with prompts and augmentations. With a single unified model,
Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and
strong results in more than 35 benchmarks, including image generation and
understanding, natural language understanding, video and audio understanding,
and robotic manipulation. We release all our models to the research community.
### 🌟 论文解读 | 统一输入输出2：扩展自回归多模态模型，涵盖视觉、语言、音频和动作

### 📌 背景痛点/本文动机
随着人工智能的发展，构建能够理解并生成多种模态（如图像、文本、音频和动作）的模型变得越来越重要。然而，训练这样的模型面临着巨大的挑战，包括数据集的多样性、模型架构的设计、训练过程的稳定性等。现有的多模态模型通常只能处理有限的模态，或者需要依赖预训练的语言模型，限制了其通用性和灵活性。

### 🚀 核心方法
Unified-IO 2 是一个自回归多模态模型，能够理解和生成图像、文本、音频和动作。为了统一不同的模态，该模型将输入和输出（包括图像、文本、音频、动作、边界框等）编码到一个共享的语义空间中，并使用单个编码器-解码器Transformer模型进行处理。为了稳定训练过程，该模型采用了多种架构改进，包括二维旋转嵌入、QK归一化和缩放余弦注意力机制。此外，该模型还使用了一种新的多模态混合去噪器目标，并结合了超过120个数据集进行指令微调，以学习广泛的技能。

### 📈 实验结果
Unified-IO 2 在多个基准测试中取得了最先进的性能，包括图像生成和理解、自然语言理解、视频和音频理解以及机器人操作等。该模型在GRIT基准测试中取得了最先进的性能，并在超过35个基准测试中取得了强大的结果。

### 💬 可借鉴之处
Unified-IO 2 的设计为构建通用多模态模型提供了新的思路。其创新点包括：
* **统一任务表示**：将不同模态的输入和输出编码到一个共享的语义空间中，使用单个模型进行处理。
* **架构改进**：采用二维旋转嵌入、QK归一化和缩放余弦注意力机制等改进，以稳定训练过程。
* **多模态混合去噪器目标**：结合去噪和生成任务，有效地利用多模态信号。
* **大规模指令微调**：使用超过120个数据集进行指令微调，以学习广泛的技能。

### 📚 参考文献
* [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172)

## generative-multimodal-models-are-in-context-learners
### Abstract
The human ability to easily solve multimodal tasks in context (i.e., with
only a few demonstrations or simple instructions), is what current multimodal
systems have largely struggled to imitate. In this work, we demonstrate that
the task-agnostic in-context learning capabilities of large multimodal models
can be significantly enhanced by effective scaling-up. We introduce Emu2, a
generative multimodal model with 37 billion parameters, trained on large-scale
multimodal sequences with a unified autoregressive objective. Emu2 exhibits
strong multimodal in-context learning abilities, even emerging to solve tasks
that require on-the-fly reasoning, such as visual prompting and object-grounded
generation. The model sets a new record on multiple multimodal understanding
tasks in few-shot settings. When instruction-tuned to follow specific
instructions, Emu2 further achieves new state-of-the-art on challenging tasks
such as question answering benchmarks for large multimodal models and
open-ended subject-driven generation. These achievements demonstrate that Emu2
can serve as a base model and general-purpose interface for a wide range of
multimodal tasks. Code and models are publicly available to facilitate future
research.
### 🌟 论文解读 | Emu2：大型生成式多模态模型在上下文学习中的突破

### 📌 背景痛点/本文动机
多模态任务涉及理解和生成单模态或多模态信息，例如文本、图像和视频。然而，现有的多模态系统往往需要针对特定任务设计架构和收集大量监督训练数据，这限制了其可扩展性。相比之下，人类能够轻松地在上下文中解决新任务，即通过少量演示或简单指令。本文旨在通过有效扩展大型多模态模型，增强其在上下文中的学习能力，使其能够模仿人类的这种能力。

### 🚀 核心方法
💡 创新点1：Emu2 模型
本文提出了 Emu2，一个具有 370 亿参数的生成式多模态模型，通过在大型多模态序列上进行统一的自回归目标训练。Emu2 在多模态上下文学习中表现出强大的能力，甚至能够解决需要即时推理的任务，例如视觉提示和基于对象的生成。

💡 创新点2：上下文学习
Emu2 在标准多模态数据集上，通过少量示例或指令进行学习的能力得到了评估。具体来说，Emu2 在两种场景下进行了评估：(a) 少样本设置，允许尽可能多的示例适应模型的上下文窗口；(b) 指令调整，模型被调整为遵循特定指令。Emu2 在少样本设置下在广泛的视觉语言任务中取得了有希望的结果，例如在多个视觉问答数据集上表现出最先进的少样本性能。

### 📈 实验结果
Emu2 在多个多模态理解任务中取得了新的记录，并在少样本设置下表现出色。当通过指令调整以遵循特定指令时，Emu2 在具有挑战性的任务上取得了新的最先进成果，例如大型多模态模型问答基准和开放式主题驱动生成。这些成果表明 Emu2 可以作为各种多模态任务的基模型和通用接口。

### 💬 可借鉴之处
本文提出的 Emu2 模型展示了大型生成式多模态模型在上下文学习中的突破。通过有效扩展模型，Emu2 能够模仿人类在上下文中解决新任务的能力。此外，Emu2 还展示了在多模态上下文中可控视觉生成的能力，例如基于主题/文本的生成。这些成果为开发可适应、通用的多模态系统提供了重要的启示。

## xgen-mm-(blip-3)--a-family-of-open-large-multimodal-models
### Abstract
This report introduces xGen-MM (also known as BLIP-3), a framework for
developing Large Multimodal Models (LMMs). The framework comprises meticulously
curated datasets, a training recipe, model architectures, and a resulting suite
of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen
initiative on foundation AI models. Our models undergo rigorous evaluation
across a range of tasks, including both single and multi-image benchmarks. Our
pre-trained base model exhibits strong in-context learning capabilities and the
instruction-tuned model demonstrates competitive performance among open-source
LMMs with similar model sizes. In addition, we introduce a safety-tuned model
with DPO, aiming to mitigate harmful behaviors such as hallucinations and
improve safety. We open-source our models, curated large-scale datasets, and
our fine-tuning codebase to facilitate further advancements in LMM research.
Associated resources will be available on our project page above.
### 🌟 论文解读 | xGen-MM (BLIP-3): 开放式大型多模态模型的家族

### 📌 背景痛点/本文动机
随着人工智能的发展，大型多模态模型（LMMs）因其潜在的应用和涌现能力而备受关注。然而，开源模型与闭源模型之间在开放权重、训练方法和数据集获取方面仍存在差距，这限制了开源社区对LMMs的复制、理解和改进。此外，现有的LMMs在数据规模、质量和多样性方面存在不足，难以达到与闭源模型相媲美的性能。

### 🚀 核心方法
💡 创新点1：数据集的丰富性和规模
xGen-MM (BLIP-3) 框架利用了多种多模态数据集，包括 MINT-1T、OBELICS、BLIP3-KALE、BLIP3-OCR-200M 和 BLIP3-GROUNDING-50M，以提供更丰富、更大规模和更多样化的训练数据。这些数据集涵盖了广泛的领域，包括 HTML、PDF、ArXiv、图像、文本和视觉定位信息，从而提高了模型的泛化能力和鲁棒性。

💡 创新点2：模型架构的改进
xGen-MM (BLIP-3) 框架采用了一种更可扩展的视觉标记采样器（perceiver resampler）来替代 Q-Former 层，并简化了训练目标，使其仅关注多模态上下文中文本标记的自回归损失。这种架构改进使得模型能够更好地处理高分辨率图像，并提高了训练效率。

💡 创新点3：训练方法的优化
xGen-MM (BLIP-3) 框架采用了大规模、多样化的数据集进行预训练，并通过视觉指令微调来提高模型对用户查询的理解和响应能力。此外，框架还引入了直接偏好优化（DPO）和安全微调来提高模型的有用性和安全性，从而减少幻觉和有害行为。

### 📈 实验结果
xGen-MM (BLIP-3) 框架在多个视觉语言任务上取得了显著的性能提升，包括图像描述、视觉问答、OCR 和视觉定位。与现有的开源 LMMs 相比，xGen-MM (BLIP-3) 在单图像和多图像基准测试中均表现出优异的性能。此外，DPO 和安全微调进一步提高了模型的有用性和安全性，使其更适用于实际应用场景。

### 💬 可借鉴之处
xGen-MM (BLIP-3) 框架为 LMMs 的研究和开发提供了宝贵的资源和工具，包括开源模型、数据集和微调代码库。研究人员和从业者可以利用这些资源来探索 LMMs 的潜力和涌现能力，并将其应用于各种实际应用场景。此外，xGen-MM (BLIP-3) 框架的安全微调方法也为构建更安全、更可靠的 AI 模型提供了重要的参考。

## vl-gpt--a-generative-pre-trained-transformer-for-vision-and-language-understanding-and-generation
### Abstract
In this work, we introduce Vision-Language Generative Pre-trained Transformer
(VL-GPT), a transformer model proficient at concurrently perceiving and
generating visual and linguistic data. VL-GPT achieves a unified pre-training
approach for both image and text modalities by employing a straightforward
auto-regressive objective, thereby enabling the model to process image and text
as seamlessly as a language model processes text. To accomplish this, we
initially propose a novel image tokenizer-detokenizer framework for visual
data, specifically designed to transform raw images into a sequence of
continuous embeddings and reconstruct them accordingly. In combination with the
existing text tokenizer and detokenizer, this framework allows for the encoding
of interleaved image-text data into a multimodal sequence, which can
subsequently be fed into the transformer model. Consequently, VL-GPT can
perform large-scale pre-training on multimodal corpora utilizing a unified
auto-regressive objective (i.e., next-token prediction). Upon completion of
pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance
across a diverse range of vision and language understanding and generation
tasks, including image captioning, visual question answering, text-to-image
generation, and more. Additionally, the pre-trained model retrains in-context
learning capabilities when provided with multimodal prompts. We further conduct
instruction tuning on our VL-GPT, highlighting its exceptional potential for
multimodal assistance. The source code and model weights shall be released.
### 🌟 论文解读 | VL-GPT：视觉与语言理解与生成的生成式预训练Transformer

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在自然语言处理（NLP）领域的成功，多模态社区对开发大型视觉-语言（VL）模型产生了浓厚的兴趣。然而，现有的VL模型在图像理解和生成方面存在一些局限性，例如，输入图像和生成图像的图像嵌入不一致，导致模型需要分别对图像理解和生成进行建模，并且难以在图像嵌入上实现自回归训练损失。

### 🚀 核心方法
💡 创新点1：图像分词器-解码器框架
为了实现视觉嵌入和文本标记的统一自回归训练目标，本文提出了一种图像分词器-解码器框架，用于将原始图像转换为连续视觉嵌入并重建它们。该框架由图像分词器和图像解码器组成，分别负责将图像编码为连续视觉嵌入和解码视觉嵌入回像素空间。

💡 创新点2：VL-GPT模型
VL-GPT是一个生成式预训练Transformer模型，用于视觉和语言理解与生成任务。该模型可以统一地自回归预训练大规模多模态语料库，即预测包含连续视觉嵌入和离散文本标记的多模态序列中的下一个标记，而无需任何区分。

### 📈 实验结果
VL-GPT在各种VL理解和生成基准测试中表现出竞争性性能，包括图像描述、视觉问答和文本到图像生成。此外，预训练模型在提供多模态提示时表现出有吸引力的多模态上下文学习能力。

### 💬 可借鉴之处
VL-GPT模型为多模态社区提供了一个强大的基础模型，类似于GPT家族在NLP中的作用。该模型在图像理解和生成方面具有显著的潜力，可以应用于各种任务，例如图像描述、视觉问答和文本到图像生成。此外，VL-GPT的多模态上下文学习能力使其成为一个灵活且有效的多模态通用助手。

## dreamllm--synergistic-multimodal-comprehension-and-creation
### Abstract
This paper presents DreamLLM, a learning framework that first achieves
versatile Multimodal Large Language Models (MLLMs) empowered with frequently
overlooked synergy between multimodal comprehension and creation. DreamLLM
operates on two fundamental principles. The first focuses on the generative
modeling of both language and image posteriors by direct sampling in the raw
multimodal space. This approach circumvents the limitations and information
loss inherent to external feature extractors like CLIP, and a more thorough
multimodal understanding is obtained. Second, DreamLLM fosters the generation
of raw, interleaved documents, modeling both text and image contents, along
with unstructured layouts. This allows DreamLLM to learn all conditional,
marginal, and joint multimodal distributions effectively. As a result, DreamLLM
is the first MLLM capable of generating free-form interleaved content.
Comprehensive experiments highlight DreamLLM's superior performance as a
zero-shot multimodal generalist, reaping from the enhanced learning synergy.
Project page: https://dreamllm.github.io.
### 🌟 论文解读 | DreamLLM：多模态理解与创造的协同学习框架

### 📌 背景痛点/本文动机
多模态理解与创造是机器智能发展的关键方向，而多模态大型语言模型（MLLMs）在视觉领域取得了突破性进展。然而，现有的MLLMs主要关注多模态理解，而多模态创造能力仍然不足。本文旨在探索MLLMs在多模态创造方面的潜力，并实现多模态理解与创造的协同学习。

### 🚀 核心方法
💡 创新点1：端到端生成
不同于现有方法在训练过程中生成中间图像表示（如CLIP嵌入），DreamLLM直接在原始多模态空间中进行采样，生成语言和图像的后验分布。这种方法避免了外部特征提取器（如CLIP）的局限性和信息损失，实现了更全面的多模态理解。

💡 创新点2：交错生成预训练（I-GPT）
DreamLLM通过训练生成交错的多模态语料库，同时编码和解码交错的语言-图像多模态输入。这种独特的训练方式使得DreamLLM能够有效地学习所有条件、边缘和联合多模态分布，从而实现自由形式的交错内容生成。

### 📈 实验结果
DreamLLM在多个视觉-语言理解和内容创造任务中取得了优异的性能，包括图像到文本描述、视觉问答、文本相关问答、文本条件图像生成等。在零样本情况下，DreamLLM在MS-COCO数据集上实现了8.46的FID分数，并在MMBench和MM-Vet评估中分别取得了49.1和35.9的分数，刷新了现有记录。

### 💬 可借鉴之处
DreamLLM的端到端生成和交错生成预训练方法为多模态学习提供了新的思路，并展示了MLLMs在多模态创造方面的巨大潜力。未来可以进一步探索DreamLLM在视频数据、3D内容创造、几何保持任务等方面的应用，并扩展到更多模态，实现统一的零样本多模态通用模型。

## making-llama-see-and-draw-with-seed-tokenizer
### Abstract
The great success of Large Language Models (LLMs) has expanded the potential
of multimodality, contributing to the gradual evolution of General Artificial
Intelligence (AGI). A true AGI agent should not only possess the capability to
perform predefined multi-tasks but also exhibit emergent abilities in an
open-world context. However, despite the considerable advancements made by
recent multimodal LLMs, they still fall short in effectively unifying
comprehension and generation tasks, let alone open-world emergent abilities. We
contend that the key to overcoming the present impasse lies in enabling text
and images to be represented and processed interchangeably within a unified
autoregressive Transformer. To this end, we introduce SEED, an elaborate image
tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.
We identify two crucial design principles: (1) Image tokens should be
independent of 2D physical patch positions and instead be produced with a 1D
causal dependency, exhibiting intrinsic interdependence that aligns with the
left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens
should capture high-level semantics consistent with the degree of semantic
abstraction in words, and be optimized for both discriminativeness and
reconstruction during the tokenizer training phase. With SEED tokens, LLM is
able to perform scalable multimodal autoregression under its original training
recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by
large-scale pretraining and instruction tuning on the interleaved textual and
visual data, demonstrating impressive performance on a broad range of
multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has
exhibited compositional emergent abilities such as multi-turn in-context
multimodal generation, acting like your AI assistant.
### 🌟 论文解读 | 让LLaMA“看”与“画”：SEED Tokenizer的突破

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在理解和生成文本方面取得了巨大成功，推动了多模态技术的发展，为通用人工智能（AGI）的演进奠定了基础。然而，尽管多模态LLMs取得了显著进展，但它们在统一理解和生成任务方面仍存在不足，更不用说在开放世界环境中的涌现能力。本文认为，克服这一困境的关键在于使文本和图像能够在统一的自回归Transformer中相互转换和表示。

### 🚀 核心方法
💡 创新点1：SEED Tokenizer
本文提出了SEED，一种基于VQ的图像分词器，它产生具有1D因果依赖性和必要高级语义的离散视觉代码，以便于视觉理解和生成任务。SEED Tokenizer的设计原则包括：
1. 图像token应独立于2D物理块位置，而是通过1D因果依赖性生成，表现出与LLMs中从左到右的自回归预测机制相一致的内在相互依赖性。
2. 图像token应捕获与词语的语义抽象程度一致的高级语义，并在分词器训练阶段优化区分度和重建能力。

💡 创新点2：SEED-LLaMA
本文通过将预训练的LLM与SEED分词器相结合，构建了SEED-LLaMA模型。SEED-LLaMA在交错文本和视觉数据上进行大规模预训练和指令调整，以实现下一个词预测的训练目标。这种易于实现且统一的代理任务促进了可扩展的多模态预训练。

### 📈 实验结果
SEED-LLaMA在广泛的视觉理解和生成任务上表现出色，包括图像描述、图像/视频问答和文本到图像生成。此外，SEED-LLaMA还展示了多轮上下文多模态生成等涌现能力，类似于人类的AI助手。

### 💬 可借鉴之处
本文提出的SEED Tokenizer和SEED-LLaMA模型为多模态LLMs的训练和应用提供了新的思路和方法。SEED Tokenizer的设计原则和SEED-LLaMA的涌现能力为未来多模态AI的发展提供了重要的参考价值。

## on-the-performance-of-multimodal-language-models
### Abstract
Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
### 🌟 论文解读 | 多模态语言模型性能探究：迈向更强大的AI

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，大型语言模型（LLMs）在自然语言处理任务中展现出强大的能力。然而，现实世界中的许多应用场景涉及多模态数据，例如图像和文本，需要结合视觉和语言信息进行准确和鲁棒的推理。为了应对这一挑战，近年来研究人员开始探索将多模态能力集成到LLMs中，通过整合预训练的视觉编码器来实现。

### 🚀 核心方法
本文对不同的多模态指令微调方法进行了比较分析，并评估了它们在各种任务上的性能，包括复杂推理、对话、图像描述、多项选择题（MCQs）和二元分类。通过严格的基准测试和消融实验，揭示了在将多模态能力集成到LLMs时，架构选择的关键见解。

### 📈 实验结果
实验结果表明，使用更大的视觉编码器（ViT-g）可以显著提高所有任务上的性能，因为它能够捕捉更丰富的图像表示。此外，微调视觉头部（例如Q-Former）也是有效的，因为它使模型能够为下游任务提取更好的表示，并加快训练和推理速度。然而，使用多模态视觉头部并没有显示出比使用仅图像头部更好的优势。

### 💬 可借鉴之处
本文的研究结果表明，在构建有效的多模态LLMs时，应关注以下几个方面：
1. 使用更大的视觉编码器以捕捉更丰富的图像表示。
2. 微调视觉头部以提取更好的表示，并加快训练和推理速度。
3. 在指令微调阶段，关注数据多样性和任务多样性，以提高模型的泛化能力。
4. 探索多样化的数据集，以实现更先进的性能。
5. 关注缓解多模态模型的幻觉问题，使其更可靠和有用。

### 🌟 总结
本文对多模态语言模型的性能进行了深入研究，并揭示了构建有效多模态LLMs的关键组件和策略。研究结果为未来研究提供了有价值的见解，并强调了数据多样性和任务多样性在提高模型能力方面的重要性。

## unified-language-vision-pretraining-in-llm-with-dynamic-discrete-visual-tokenization
### Abstract
Recently, the remarkable advance of the Large Language Model (LLM) has
inspired researchers to transfer its extraordinary reasoning capability to both
vision and language data. However, the prevailing approaches primarily regard
the visual input as a prompt and focus exclusively on optimizing the text
generation process conditioned upon vision content by a frozen LLM. Such an
inequitable treatment of vision and language heavily constrains the model's
potential. In this paper, we break through this limitation by representing both
vision and language in a unified form. Specifically, we introduce a
well-designed visual tokenizer to translate the non-linguistic image into a
sequence of discrete tokens like a foreign language that LLM can read. The
resulting visual tokens encompass high-level semantics worthy of a word and
also support dynamic sequence length varying from the image. Coped with this
tokenizer, the presented foundation model called LaVIT can handle both image
and text indiscriminately under the same generative learning paradigm. This
unification empowers LaVIT to serve as an impressive generalist interface to
understand and generate multi-modal content simultaneously. Extensive
experiments further showcase that it outperforms the existing models by a large
margin on massive vision-language tasks. Our code and models are available at
https://github.com/jy0205/LaVIT.
### 🌟 论文解读 | LaVIT：统一语言-视觉预训练，突破多模态理解的局限

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLM）在自然语言处理领域取得了显著进展，其强大的推理能力激发了研究人员将其应用于视觉和语言数据。然而，现有方法主要将视觉输入视为提示，并通过冻结的LLM优化基于视觉内容的文本生成过程，这种对视觉和语言的不平等处理严重限制了模型的潜力。

### 🚀 核心方法
💡 创新点1：统一表示形式
本文提出了LaVIT（Language-VIsion Transformer），一种新的通用多模态基础模型，它继承了LLM成功的预测下一个图像/文本标记的自回归生成学习范式。为了实现这一目标，LaVIT引入了一种动态视觉标记化机制，将图像转换为LLM可以理解的离散标记序列，从而在统一的生成目标下同时处理图像和文本。

💡 创新点2：动态视觉标记化
LaVIT的视觉标记化机制包括一个选择器和合并器。选择器首先决定哪些视觉块包含信息丰富的语义，并将其选中以编码整个图像。合并器进一步压缩未选中的块到保留的块上，以减少视觉块之间的冗余，从而为不同的图像生成动态长度的序列。保留的视觉标记进一步量化为离散代码，作为预训练期间视觉标记的监督信号。

### 📈 实验结果
LaVIT在广泛的视觉语言任务上取得了最先进的零样本性能，包括图像描述、视觉问答等。与现有模型相比，LaVIT在理解和生成多模态内容方面表现出色，能够接受多种模态组合作为提示，生成高质量的图像，并理解图像内容，生成简洁的文本描述，回答关于图像细节的各种问题。

### 💬 可借鉴之处
LaVIT的成功表明，通过将视觉和语言统一表示为离散标记，并使用统一的生成目标进行训练，可以有效地学习多模态交互和对齐，从而实现强大的多模态理解和生成能力。这种方法为构建更通用、更强大的多模态模型提供了新的思路。

## planting-a-seed-of-vision-in-large-language-model
### Abstract
We present SEED, an elaborate image tokenizer that empowers Large Language
Models (LLMs) with the emergent ability to SEE and Draw at the same time.
Research on image tokenizers has previously reached an impasse, as frameworks
employing quantized visual tokens have lost prominence due to subpar
performance and convergence in multimodal comprehension (compared to BLIP-2,
etc.) or generation (compared to Stable Diffusion, etc.). Despite the
limitations, we remain confident in its natural capacity to unify visual and
textual representations, facilitating scalable multimodal training with LLM's
original recipe. In this study, we identify two crucial principles for the
architecture and training of SEED that effectively ease subsequent alignment
with LLMs. (1) Image tokens should be independent of 2D physical patch
positions and instead be produced with a 1D causal dependency, exhibiting
intrinsic interdependence that aligns with the left-to-right autoregressive
prediction mechanism in LLMs. (2) Image tokens should capture high-level
semantics consistent with the degree of semantic abstraction in words, and be
optimized for both discriminativeness and reconstruction during the tokenizer
training phase. As a result, the off-the-shelf LLM is able to perform both
image-to-text and text-to-image generation by incorporating our SEED through
efficient LoRA tuning. Comprehensive multimodal pretraining and instruction
tuning, which may yield improved results, are reserved for future
investigation. This version of SEED was trained in 5.7 days using only 64 V100
GPUs and 5M publicly available image-text pairs. Our preliminary study
emphasizes the great potential of discrete visual tokens in versatile
multimodal LLMs and the importance of proper image tokenizers in broader
research.
### 🌟 论文解读 | 在大型语言模型中植入视觉种子

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在文本理解和生成任务上的显著成功，研究人员开始探索将视觉信息整合到LLMs中，以实现多模态理解和生成。然而，现有的多模态LLMs在视觉理解和生成任务上尚未达到LLMs在文本任务上的水平。本文提出了一种名为SEED的图像分词器，旨在解决现有图像分词器在多模态理解和生成任务上的局限性，并使LLMs能够同时进行“看”和“画”。

### 🚀 核心方法
💡 创新点1：因果依赖的图像分词器
SEED采用因果Q-Former将2D图像特征转换为具有1D因果依赖关系的语义嵌入，从而与LLMs中的自回归预测机制相匹配。这种设计使得图像分词器能够更好地与LLMs进行对齐，并提高多模态理解和生成任务的性能。

💡 创新点2：高层次的语义表示
SEED的图像分词器不仅捕获了图像的低层次细节，还捕捉了图像的高层次语义信息。这使得图像分词器能够更好地与LLMs中的文本分词器进行对齐，并提高多模态理解和生成任务的性能。

### 📈 实验结果
SEED在图像-文本检索和图像生成任务上取得了与现有模型相当的性能。此外，通过使用SEED对LLMs进行微调，SEED-OPT2.7B在零样本图像描述和视觉问答任务上取得了有希望的结果，并能够生成高质量的图像。

### 💬 可借鉴之处
本文提出的SEED图像分词器为多模态LLMs的研究提供了新的思路。SEED的设计原则和训练方法可以应用于其他多模态任务，以实现更好的性能和更低的训练成本。此外，SEED的研究结果表明，离散视觉分词器在多模态LLMs中具有巨大的潜力，并有望推动多模态LLMs的进一步发展。

## emu--generative-pretraining-in-multimodality
### Abstract
We present Emu, a Transformer-based multimodal foundation model, which can
seamlessly generate images and texts in multimodal context. This omnivore model
can take in any single-modality or multimodal data input indiscriminately
(e.g., interleaved image, text and video) through a one-model-for-all
autoregressive training process. First, visual signals are encoded into
embeddings, and together with text tokens form an interleaved input sequence.
Emu is then end-to-end trained with a unified objective of classifying the next
text token or regressing the next visual embedding in the multimodal sequence.
This versatile multimodality empowers the exploration of diverse pretraining
data sources at scale, such as videos with interleaved frames and text,
webpages with interleaved images and text, as well as web-scale image-text
pairs and video-text pairs. Emu can serve as a generalist multimodal interface
for both image-to-text and text-to-image tasks, and supports in-context image
and text generation. Across a broad range of zero-shot/few-shot tasks including
image captioning, visual question answering, video question answering and
text-to-image generation, Emu demonstrates superb performance compared to
state-of-the-art large multimodal models. Extended capabilities such as
multimodal assistants via instruction tuning are also demonstrated with
impressive performance.
### 🌟 论文解读 | Emu：多模态生成式预训练的突破

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在文本理解和生成方面的突破，多模态模型（LMMs）也逐渐兴起，旨在融合视觉和语言信息，实现更丰富的应用。然而，现有的LMMs大多存在以下问题：

* **视觉信息处理受限**：许多模型仅预测文本token，而忽略了对视觉信息的监督，限制了模型能力。
* **数据来源单一**：主要依赖图像-文本对或文档，而忽略了视频数据作为潜在的可扩展多模态数据源。
* **应用场景有限**：大多针对特定任务进行微调，缺乏通用的多模态接口。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：统一的多模态序列建模
* Emu采用自回归方式，将图像、文本和视频数据统一编码为序列，并进行端到端训练。
* 视觉信号通过EVA-CLIP编码为嵌入，并与文本token形成交错序列。
* Emu的目标是预测序列中的下一个元素，无论是文本token还是视觉嵌入。

💡 创新点2：Causal Transformer
* 为了更好地捕捉图像特征，Emu引入Causal Transformer将2D视觉信号转换为1D因果序列。
* Causal Transformer类似于Transformer解码器，包含因果自注意力层、交叉注意力层和前馈层。
* 通过交叉注意力层，Causal Transformer聚合来自EVA-CLIP的视觉信息，并输出N个嵌入，捕捉图像的因果依赖关系。

💡 创新点3：视觉解码器
* Emu使用潜在扩散模型将视觉嵌入解码为图像。
* 视觉解码器基于Stable Diffusion进行初始化，并通过微调进行优化。

💡 创新点4：多模态指令微调
* Emu通过在多模态对话数据上进行指令微调，使其能够更好地理解并执行人类指令。
* 指令微调采用低秩适配（LoRA）模块，仅对多模态建模LLM的自注意力层进行适配。

### 📈 实验结果
* Emu在图像描述、视觉问答、视频问答和文本到图像生成等任务上取得了优异的性能，超越了现有的LMMs。
* Emu-I（指令微调后的Emu）在零样本和少样本设置下表现出色，甚至超越了参数量更大的模型。
* Emu还展示了在上下文文本和图像生成、图像混合、视频理解和现实世界知识推理等方面的潜力。

### 💬 可借鉴之处
* Emu的多模态序列建模方法为构建通用的多模态接口提供了新的思路。
* Causal Transformer有效地捕捉了图像的因果依赖关系，提高了模型对视觉信息的处理能力。
* 视觉解码器实现了从视觉嵌入到图像的生成，拓展了模型的应用场景。
* 多模态指令微调使模型能够更好地理解并执行人类指令，增强了模型的实用性。

### 📚 总结
Emu作为一款多模态生成式预训练模型，在多模态理解和生成方面取得了突破性进展。其统一的多模态序列建模方法、Causal Transformer、视觉解码器和多模态指令微调等创新点，为构建更强大的多模态模型和应用提供了新的方向。

## any-to-any-generation-via-composable-diffusion
### Abstract
We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at https://codi-gen.github.io
### 🌟 论文解读 | CoDi：任意模态生成的新纪元

### 📌 背景痛点/本文动机
近年来，跨模态模型在生成一个模态从另一个模态的能力方面取得了显著进展，例如文本到文本、文本到图像或文本到音频。然而，这些模型在实际应用中受到限制，因为它们无法处理多个模态共存和交互的情况。虽然可以将模态特定的生成模型在多步生成设置中串联起来，但每一步的生成能力仍然有限，并且串行多步过程可能既繁琐又缓慢。此外，独立生成的单模态流在后期处理中拼接在一起时不会保持一致和对齐（例如，同步的视频和音频）。因此，迫切需要开发一个能够从任何一组输入条件生成任何模态组合的全面且通用的模型，以更准确地捕捉世界的多模态本质和人类的理解，无缝地整合来自广泛来源的信息，并使人类与人工智能的交互更加沉浸式（例如，同时生成连贯的视频、音频和文本描述）。

### 🚀 核心方法
CoDi，即“可组合扩散”，是第一个能够同时处理和生成任意模态组合的模型。为了实现这一目标，CoDi采用了以下创新方法：

💡 创新点1：可组合多模态条件
CoDi通过将文本、图像、视频和音频的提示编码器对齐到同一空间，使其能够对任何输入/提示模态组合进行条件设置。通过简单地对齐嵌入的加权插值，CoDi能够使用单条件训练（即只有一个输入）的模型执行零样本多条件训练（即多个输入）。

💡 创新点2：可组合扩散
CoDi首先独立训练图像、视频、音频和文本的潜在扩散模型（LDM）。然后，通过一种名为“潜在对齐”的新机制，这些扩散模型学习跨模态进行联合多模态生成。具体来说，CoDi在每个扩散器中添加了跨模态注意力子层，并将不同LDM的潜在变量投影到共享潜在空间，从而实现同步生成相互交织的模态，例如时间对齐的视频和音频。

### 📈 实验结果
CoDi在各种场景下表现出卓越的生成质量，其合成质量与单模态到单模态的最先进水平相当甚至更好。例如，在音频生成和音频字幕方面，CoDi的性能与基于自回归变换器的最先进水平相当。此外，CoDi还能够生成各种模态组合，例如文本到视频+音频、文本到图像+文本+音频以及文本+音频+图像到视频+音频。

### 💬 可借鉴之处
CoDi为多模态生成领域带来了突破性的进展，其创新的可组合扩散方法为未来研究提供了新的思路。CoDi的设计和训练策略可以应用于其他多模态任务，例如跨模态检索、多模态表示学习和多模态交互。此外，CoDi的灵活性和可定制性使其能够适应各种应用场景，例如生成式艺术、虚拟现实和增强现实。

## multimodal-unified-attention-networks-for-vision-and-language-interactions
### Abstract
Learning an effective attention mechanism for multimodal data is important in
many vision-and-language tasks that require a synergic understanding of both
the visual and textual contents. Existing state-of-the-art approaches use
co-attention models to associate each visual object (e.g., image region) with
each textual object (e.g., query word). Despite the success of these
co-attention models, they only model inter-modal interactions while neglecting
intra-modal interactions. Here we propose a general `unified attention' model
that simultaneously captures the intra- and inter-modal interactions of
multimodal features and outputs their corresponding attended representations.
By stacking such unified attention blocks in depth, we obtain the deep
Multimodal Unified Attention Network (MUAN), which can seamlessly be applied to
the visual question answering (VQA) and visual grounding tasks. We evaluate our
MUAN models on two VQA datasets and three visual grounding datasets, and the
results show that MUAN achieves top-level performance on both tasks without
bells and whistles.
### 🌟 论文解读 | 多模态统一注意力网络：视觉与语言交互的新突破

### 📌 背景痛点/本文动机
在许多需要视觉和语言内容协同理解的视觉与语言任务中，学习有效的注意力机制对于多模态数据至关重要。现有的最先进方法使用协同注意力模型将每个视觉对象（例如，图像区域）与每个文本对象（例如，查询词）相关联。尽管这些协同注意力模型取得了成功，但它们只模拟了模态间交互，而忽略了模态内交互。本文提出了一种通用的“统一注意力”模型，该模型同时捕获多模态特征的模态内和模态间交互，并输出相应的关注表示。通过在深度上堆叠这样的统一注意力块，我们获得了深度多模态统一注意力网络（MUAN），它可以无缝地应用于视觉问答（VQA）和视觉定位任务。

### 🚀 核心方法
💡 创新点1：统一注意力模型
本文将单模态的自注意力模型扩展为统一注意力模型，该模型可以表征多模态数据的模态内和模态间交互。通过堆叠这样的统一注意力模型（即UA块），我们获得了简洁的多模态统一注意力网络（MUAN），该网络可以进行精确的多模态推理。

💡 创新点2：门控自注意力模型
本文将原始的自注意力模型修改为门控自注意力（GSA）模型，作为UA块的基本组件，这有助于更准确和鲁棒的注意力学习，并为特定任务提供更具区分性的特征。

### 📈 实验结果
本文在两个VQA数据集和三个视觉定位数据集上评估了MUAN模型，结果表明MUAN在两项任务上都取得了顶级性能，无需使用任何数据集特定的模型调整。

### 💬 可借鉴之处
本文提出的MUAN模型为多模态学习任务提供了一种新的思路，其核心思想是将模态内和模态间交互同时建模，并通过深度堆叠统一注意力块进行多模态推理。此外，本文提出的门控自注意力模型也为注意力学习提供了新的思路，有助于提高注意力学习的准确性和鲁棒性。

## unimumo--unified-text--music-and-motion-generation
### Abstract
We introduce UniMuMo, a unified multimodal model capable of taking arbitrary
text, music, and motion data as input conditions to generate outputs across all
three modalities. To address the lack of time-synchronized data, we align
unpaired music and motion data based on rhythmic patterns to leverage existing
large-scale music-only and motion-only datasets. By converting music, motion,
and text into token-based representation, our model bridges these modalities
through a unified encoder-decoder transformer architecture. To support multiple
generation tasks within a single framework, we introduce several architectural
improvements. We propose encoding motion with a music codebook, mapping motion
into the same feature space as music. We introduce a music-motion parallel
generation scheme that unifies all music and motion generation tasks into a
single transformer decoder architecture with a single training task of
music-motion joint generation. Moreover, the model is designed by fine-tuning
existing pre-trained single-modality models, significantly reducing
computational demands. Extensive experiments demonstrate that UniMuMo achieves
competitive results on all unidirectional generation benchmarks across music,
motion, and text modalities. Quantitative results are available in the
\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.
### 🌟 论文解读 | UniMuMo：统一文本、音乐和动作生成

### 📌 背景痛点/本文动机
音乐、动作和文本是人类表达和沟通的重要方式。它们之间存在着紧密的联系，例如舞蹈动作与音乐的节奏同步，文本描述可以传达音乐和动作的情感和内容。然而，现有的模型通常只能处理单向的生成任务，例如从文本生成音乐，或者从音乐生成动作，缺乏一个统一的框架来支持多种模态的生成。

### 🚀 核心方法
💡 创新点1：音乐-动作对齐
为了解决缺乏时间同步数据的问题，UniMuMo 提出了基于节奏模式对齐未配对的音频和动作数据的方法。通过提取音乐节拍和动作视觉节拍，并使用动态时间规整（DTW）找到最佳匹配，将动作序列调整以匹配音乐节拍。

💡 创新点2：音乐-动作并行生成
UniMuMo 引入了一种新的音乐-动作并行生成方案，将所有音乐和动作生成任务统一到一个 Transformer 解码器架构中。通过同时进行两个相互条件化的自回归生成流（音乐和动作），UniMuMo 能够在单个训练任务中实现音乐-动作联合生成。

💡 创新点3：音乐-动作联合分词器
UniMuMo 使用一个预训练的音频分词器 Encodec 和一个新的动作编码器-解码器，将音乐和动作序列编码到相同的特征空间中。这种设计不仅有效地弥合了模态之间的差距，还显著降低了训练成本，并提高了性能。

💡 创新点4：音乐-动作条件描述生成
UniMuMo 使用一个预训练的音乐-动作解码器作为特征提取器，并微调一个 T5 解码器来生成音乐和动作的文本描述。为了更好地捕捉音乐和动作特征，UniMuMo 引入了一个可训练的全自注意力模块，并将其与 T5 解码器一起微调。

### 📈 实验结果
UniMuMo 在各种单向生成任务中取得了与现有最先进模型相当的性能，包括文本到音乐、音乐到动作、动作到音乐、音乐描述和动作描述。实验结果表明，UniMuMo 能够有效地生成高质量的音乐、动作和文本内容，并支持多种模态的组合生成。

### 💬 可借鉴之处
UniMuMo 的创新方法为多模态生成研究开辟了新的途径，并为未来模型的设计和应用提供了有价值的参考。其核心思想和方法可以应用于其他多模态任务，例如图像和文本的生成，或者视频和音频的生成。

## medvilam--a-multimodal-large-language-model-with-advanced-generalizability-and-explainability-for-medical-data-understanding-and-generation
### Abstract
Medicine is inherently multimodal and multitask, with diverse data modalities
spanning text, imaging. However, most models in medical field are unimodal
single tasks and lack good generalizability and explainability. In this study,
we introduce MedViLaM, a unified vision-language model towards a generalist
model for medical data that can flexibly encode and interpret various forms of
medical data, including clinical language and imaging, all using the same set
of model weights. To facilitate the creation of such multi-task model, we have
curated MultiMedBench, a comprehensive pretaining dataset and benchmark
consisting of several distinct tasks, i.e., continuous question-answering,
multi-label disease classification, disease localization, generation and
summarization of radiology reports. MedViLaM demonstrates strong performance
across all MultiMedBench tasks, frequently outpacing other generalist models by
a significant margin. Additionally, we present instances of zero-shot
generalization to new medical concepts and tasks, effective transfer learning
across different tasks, and the emergence of zero-shot medical reasoning.
### 🌟 论文解读 | MedViLaM：多模态大型语言模型，助力医疗数据理解和生成

### 📌 背景痛点/本文动机
医学领域的数据具有多模态和多任务的特点，包括文本、影像等多种数据类型。然而，现有的医学模型大多为单模态、单任务模型，缺乏良好的泛化能力和可解释性。这使得模型在临床应用中面临挑战，例如难以泛化到未见过的疾病类别和未定义的指令（任务）。

### 🚀 核心方法
💡 创新点1：MedViLaM，一个统一的视觉-语言模型，旨在成为医疗数据的通用模型。该模型能够灵活地编码和解释各种形式的医疗数据，包括临床语言和影像，所有这些操作都使用相同的模型权重集。

💡 创新点2：为了促进多任务模型的创建，研究人员构建了MultiMedBench，这是一个全面的预训练数据集和基准，包括多个不同的任务，例如连续问答、多标签疾病分类、疾病定位、放射学报告的生成和摘要。

💡 创新点3：MedViLaM在所有MultiMedBench任务中表现出强大的性能，经常以显著的优势超越其他通用模型。此外，研究人员还展示了模型对新医疗概念和任务的零样本泛化能力、不同任务之间的有效迁移学习，以及零样本医疗推理的出现。

### 📈 实验结果
实验结果表明，MedViLaM在各种医疗图像数据集上的泛化性能优于现有方法，这表明其在未来临床应用中的潜力。

### 💬 可借鉴之处
MedViLaM模型在医疗数据理解和生成方面具有显著的潜力，其多模态、多任务的特点使其能够更好地适应复杂的医疗场景。此外，该模型的可解释性也有助于提高临床医生对模型的信任度。因此，MedViLaM模型为医疗人工智能领域的研究和应用提供了新的思路和方法。

## generative-visual-instruction-tuning
### Abstract
We propose to use automatically generated instruction-following data to
improve the zero-shot capabilities of a large multimodal model with additional
support for generative and image editing tasks. We achieve this by curating a
new multimodal instruction-following set using GPT-4V and existing datasets for
image generation and editing. Using this instruction set and the existing
LLaVA-Finetune instruction set for visual understanding tasks, we produce
GenLLaVA, a Generative Large Language and Visual Assistant. GenLLaVA is built
through a strategy that combines three types of large pretrained models through
instruction finetuning: Mistral for language modeling, SigLIP for image-text
matching, and StableDiffusion for text-to-image generation. Our model
demonstrates visual understanding capabilities superior to LLaVA and
additionally demonstrates competitive results with native multimodal models
such as Unified-IO 2, paving the way for building advanced general-purpose
visual assistants by effectively re-using existing multimodal models. We
open-source our dataset, codebase, and model checkpoints to foster further
research and application in this domain.
### 🌟 论文解读 | GenLLaVA：多模态模型在图像理解、生成和编辑任务上的新突破

### 📌 背景痛点/本文动机
随着多模态模型在人工智能领域的兴起，如何让这些模型在图像理解、生成和编辑任务上同时表现出色，成为了一个重要的研究课题。现有的多模态模型往往在添加新的功能（如图像生成）后，会损失原有的视觉和语言理解能力。本文旨在解决这个问题，提出了一种新的方法，使得多模态模型在保持原有能力的同时，还能进行图像生成和编辑。

### 🚀 核心方法
💡 创新点1：生成多模态指令数据集
本文通过使用GPT-4V和现有的图像生成和编辑数据集，构建了一个新的多模态指令数据集，该数据集包含了图像理解、图像生成和图像编辑的数据。

💡 创新点2：GenLLaVA模型
本文提出了GenLLaVA模型，该模型通过指令微调，将Mistral语言模型、SigLIP图像-文本匹配模型和StableDiffusion文本-图像生成模型结合起来，实现了图像理解、生成和编辑的功能。

💡 创新点3：单阶段训练
与LLaVA模型的两阶段训练流程不同，GenLLaVA模型采用单阶段训练，直接微调视觉-语言投影器、语言模型和视觉生成头，从而降低了训练成本，并提高了训练效率。

### 📈 实验结果
实验结果表明，GenLLaVA模型在图像理解、生成和编辑任务上均表现出色，其性能优于LLaVA模型，并且在许多任务上与Unified-IO 2等原生多模态模型相当。

### 💬 可借鉴之处
本文提出的GenLLaVA模型和训练方法，为构建高级通用视觉助手提供了新的思路。此外，本文开源了数据集、代码和模型检查点，为多模态AI领域的研究和应用提供了宝贵的资源。

