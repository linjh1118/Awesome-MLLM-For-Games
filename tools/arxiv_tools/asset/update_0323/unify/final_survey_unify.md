# Paper List of unify.md
- [25/03] **OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models**  
[[Paper](http://arxiv.org/pdf/2503.08686v1)] [[Code/Page](https://github.com/hustvl/OmniMamba)] [[TLDR/Notes](#omnimamba--efficient-and-unified-multimodal-understanding-and-generation-via-state-space-models)]

- [25/01] **Dual Diffusion for Unified Image Generation and Understanding**  
[[Paper](http://arxiv.org/pdf/2501.00289v1)] [[Code/Page]()] [[TLDR/Notes](#dual-diffusion-for-unified-image-generation-and-understanding)]

- [24/12] **LMFusion: Adapting Pretrained Language Models for Multimodal Generation**  
[[Paper](http://arxiv.org/pdf/2412.15188v4)] [[Code/Page]()] [[TLDR/Notes](#lmfusion--adapting-pretrained-language-models-for-multimodal-generation)]

- [24/12] **MetaMorph: Multimodal Understanding and Generation via Instruction Tuning**  
[[Paper](http://arxiv.org/pdf/2412.14164v1)] [[Code/Page]()] [[TLDR/Notes](#metamorph--multimodal-understanding-and-generation-via-instruction-tuning)]

- [24/12] **SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**  
[[Paper](http://arxiv.org/pdf/2412.09604v1)] [[Code/Page]()] [[TLDR/Notes](#synergen-vl--towards-synergistic-image-understanding-and-generation-with-vision-experts-and-token-folding)]

- [24/02] **Diffusion Language Models Are Versatile Protein Learners**  
[[Paper](http://arxiv.org/pdf/2402.18567v2)] [[Code/Page](https://github.com/bytedance/dplm}.)] [[TLDR/Notes](#diffusion-language-models-are-versatile-protein-learners)]

- [24/12] **TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2412.03069v1)] [[Code/Page]()] [[TLDR/Notes](#tokenflow--unified-image-tokenizer-for-multimodal-understanding-and-generation)]

- [25/01] **LoRaFlow: High-Quality Signal Reconstruction using Rectified Flow**  
[[Paper](http://arxiv.org/pdf/2501.00024v1)] [[Code/Page]()] [[TLDR/Notes](#loraflow--high-quality-signal-reconstruction-using-rectified-flow)]

- [24/12] **Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads**  
[[Paper](http://arxiv.org/pdf/2412.00127v1)] [[Code/Page]()] [[TLDR/Notes](#orthus--autoregressive-interleaved-image-text-generation-with-modality-specific-heads)]

- [24/11] **JetFormer: An Autoregressive Generative Model of Raw Images and Text**  
[[Paper](http://arxiv.org/pdf/2411.19722v1)] [[Code/Page]()] [[TLDR/Notes](#jetformer--an-autoregressive-generative-model-of-raw-images-and-text)]

- [24/11] **MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding**  
[[Paper](http://arxiv.org/pdf/2411.17762v3)] [[Code/Page]()] [[TLDR/Notes](#muse-vl--modeling-unified-vlm-through-semantic-discrete-encoding)]

- [24/11] **JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2411.07975v2)] [[Code/Page]()] [[TLDR/Notes](#janusflow--harmonizing-autoregression-and-rectified-flow-for-unified-multimodal-understanding-and-generation)]

- [23/10] **On the Performance of Multimodal Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03211v2)] [[Code/Page]()] [[TLDR/Notes](#on-the-performance-of-multimodal-language-models)]

- [24/10] **MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding**  
[[Paper](http://arxiv.org/pdf/2410.21747v1)] [[Code/Page]()] [[TLDR/Notes](#motiongpt-2--a-general-purpose-motion-language-model-for-motion-generation-and-understanding)]

- [24/10] **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2410.13848v1)] [[Code/Page]()] [[TLDR/Notes](#janus--decoupling-visual-encoding-for-unified-multimodal-understanding-and-generation)]

- [24/10] **PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**  
[[Paper](http://arxiv.org/pdf/2410.13861v2)] [[Code/Page](https://github.com/rongyaofang/PUMA.)] [[TLDR/Notes](#puma--empowering-unified-mllm-with-multi-granular-visual-generation)]

- [24/10] **MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling**  
[[Paper](http://arxiv.org/pdf/2410.10798v2)] [[Code/Page]()] [[TLDR/Notes](#mmar--towards-lossless-multi-modal-auto-regressive-probabilistic-modeling)]

- [21/05] **Looking at CTR Prediction Again: Is Attention All You Need?**  
[[Paper](http://arxiv.org/pdf/2105.05563v1)] [[Code/Page]()] [[TLDR/Notes](#looking-at-ctr-prediction-again--is-attention-all-you-need-)]

- [24/09] **MIO: A Foundation Model on Multimodal Tokens**  
[[Paper](http://arxiv.org/pdf/2409.17692v3)] [[Code/Page]()] [[TLDR/Notes](#mio--a-foundation-model-on-multimodal-tokens)]

- [24/09] **MonoFormer: One Transformer for Both Diffusion and Autoregression**  
[[Paper](http://arxiv.org/pdf/2409.16280v1)] [[Code/Page](https://monoformer.github.io/.)] [[TLDR/Notes](#monoformer--one-transformer-for-both-diffusion-and-autoregression)]

- [24/09] **VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2409.04429v3)] [[Code/Page]()] [[TLDR/Notes](#vila-u--a-unified-foundation-model-integrating-visual-understanding-and-generation)]

- [24/08] **Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2408.12528v6)] [[Code/Page](https://github.com/showlab/Show-o.)] [[TLDR/Notes](#show-o--one-single-transformer-to-unify-multimodal-understanding-and-generation)]

- [24/08] **Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model**  
[[Paper](http://arxiv.org/pdf/2408.11039v1)] [[Code/Page]()] [[TLDR/Notes](#transfusion--predict-the-next-token-and-diffuse-images-with-one-multi-modal-model)]

- [24/07] **ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation**  
[[Paper](http://arxiv.org/pdf/2407.06135v1)] [[Code/Page]()] [[TLDR/Notes](#anole--an-open--autoregressive--native-large-multimodal-models-for-interleaved-image-text-generation)]

- [24/06] **Hybrid Alignment Training for Large Language Models**  
[[Paper](http://arxiv.org/pdf/2406.15178v1)] [[Code/Page]()] [[TLDR/Notes](#hybrid-alignment-training-for-large-language-models)]

- [24/12] **Relational Programming with Foundation Models**  
[[Paper](http://arxiv.org/pdf/2412.14515v1)] [[Code/Page]()] [[TLDR/Notes](#relational-programming-with-foundation-models)]

- [24/04] **SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation**  
[[Paper](http://arxiv.org/pdf/2404.14396v2)] [[Code/Page](https://github.com/AILab-CVC/SEED-X.)] [[TLDR/Notes](#seed-x--multimodal-models-with-unified-multi-granularity-comprehension-and-generation)]

- [24/03] **Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models**  
[[Paper](http://arxiv.org/pdf/2403.18814v1)] [[Code/Page](https://github.com/dvlab-research/MiniGemini.)] [[TLDR/Notes](#mini-gemini--mining-the-potential-of-multi-modality-vision-language-models)]

- [24/02] **AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**  
[[Paper](http://arxiv.org/pdf/2402.12226v3)] [[Code/Page](https://junzhan2000.github.io/AnyGPT.github.io/)] [[TLDR/Notes](#anygpt--unified-multimodal-llm-with-discrete-sequence-modeling)]

- [24/02] **World Model on Million-Length Video And Language With Blockwise RingAttention**  
[[Paper](http://arxiv.org/pdf/2402.08268v4)] [[Code/Page]()] [[TLDR/Notes](#world-model-on-million-length-video-and-language-with-blockwise-ringattention)]

- [24/02] **Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization**  
[[Paper](http://arxiv.org/pdf/2402.03161v3)] [[Code/Page](https://video-lavit.github.io.)] [[TLDR/Notes](#video-lavit--unified-video-language-pre-training-with-decoupled-visual-motional-tokenization)]

- [24/01] **MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer**  
[[Paper](http://arxiv.org/pdf/2401.10208v2)] [[Code/Page](https://github.com/OpenGVLab/MM-Interleaved}.)] [[TLDR/Notes](#mm-interleaved--interleaved-image-text-generative-modeling-via-multi-modal-feature-synchronizer)]

- [23/12] **Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action**  
[[Paper](http://arxiv.org/pdf/2312.17172v1)] [[Code/Page]()] [[TLDR/Notes](#unified-io-2--scaling-autoregressive-multimodal-models-with-vision--language--audio--and-action)]

- [23/12] **Generative Multimodal Models are In-Context Learners**  
[[Paper](http://arxiv.org/pdf/2312.13286v2)] [[Code/Page]()] [[TLDR/Notes](#generative-multimodal-models-are-in-context-learners)]

- [24/08] **xGen-MM (BLIP-3): A Family of Open Large Multimodal Models**  
[[Paper](http://arxiv.org/pdf/2408.08872v2)] [[Code/Page]()] [[TLDR/Notes](#xgen-mm-(blip-3)--a-family-of-open-large-multimodal-models)]

- [23/12] **VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation**  
[[Paper](http://arxiv.org/pdf/2312.09251v1)] [[Code/Page]()] [[TLDR/Notes](#vl-gpt--a-generative-pre-trained-transformer-for-vision-and-language-understanding-and-generation)]

- [23/09] **DreamLLM: Synergistic Multimodal Comprehension and Creation**  
[[Paper](http://arxiv.org/pdf/2309.11499v2)] [[Code/Page](https://dreamllm.github.io.)] [[TLDR/Notes](#dreamllm--synergistic-multimodal-comprehension-and-creation)]

- [23/10] **Making LLaMA SEE and Draw with SEED Tokenizer**  
[[Paper](http://arxiv.org/pdf/2310.01218v1)] [[Code/Page]()] [[TLDR/Notes](#making-llama-see-and-draw-with-seed-tokenizer)]

- [23/10] **On the Performance of Multimodal Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03211v2)] [[Code/Page]()] [[TLDR/Notes](#on-the-performance-of-multimodal-language-models)]

- [23/09] **Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**  
[[Paper](http://arxiv.org/pdf/2309.04669v3)] [[Code/Page](https://github.com/jy0205/LaVIT.)] [[TLDR/Notes](#unified-language-vision-pretraining-in-llm-with-dynamic-discrete-visual-tokenization)]

- [23/07] **Planting a SEED of Vision in Large Language Model**  
[[Paper](http://arxiv.org/pdf/2307.08041v2)] [[Code/Page]()] [[TLDR/Notes](#planting-a-seed-of-vision-in-large-language-model)]

- [23/07] **Emu: Generative Pretraining in Multimodality**  
[[Paper](http://arxiv.org/pdf/2307.05222v2)] [[Code/Page]()] [[TLDR/Notes](#emu--generative-pretraining-in-multimodality)]

- [23/05] **Any-to-Any Generation via Composable Diffusion**  
[[Paper](http://arxiv.org/pdf/2305.11846v1)] [[Code/Page](https://codi-gen.github.io)] [[TLDR/Notes](#any-to-any-generation-via-composable-diffusion)]

- [19/08] **Multimodal Unified Attention Networks for Vision-and-Language Interactions**  
[[Paper](http://arxiv.org/pdf/1908.04107v2)] [[Code/Page]()] [[TLDR/Notes](#multimodal-unified-attention-networks-for-vision-and-language-interactions)]

- [24/10] **UniMuMo: Unified Text, Music and Motion Generation**  
[[Paper](http://arxiv.org/pdf/2410.04534v1)] [[Code/Page](https://hanyangclarence.github.io/unimumo_demo/}{project)] [[TLDR/Notes](#unimumo--unified-text--music-and-motion-generation)]

- [24/09] **MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation**  
[[Paper](http://arxiv.org/pdf/2409.19684v1)] [[Code/Page]()] [[TLDR/Notes](#medvilam--a-multimodal-large-language-model-with-advanced-generalizability-and-explainability-for-medical-data-understanding-and-generation)]

- [24/06] **Generative Visual Instruction Tuning**  
[[Paper](http://arxiv.org/pdf/2406.11262v2)] [[Code/Page]()] [[TLDR/Notes](#generative-visual-instruction-tuning)]



# TLDR/Notes
## omnimamba--efficient-and-unified-multimodal-understanding-and-generation-via-state-space-models
### Abstract
Recent advancements in unified multimodal understanding and visual generation
(or multimodal generation) models have been hindered by their quadratic
computational complexity and dependence on large-scale training data. We
present OmniMamba, the first linear-architecture-based multimodal generation
model that generates both text and images through a unified next-token
prediction paradigm. The model fully leverages Mamba-2's high computational and
memory efficiency, extending its capabilities from text generation to
multimodal generation. To address the data inefficiency of existing unified
models, we propose two key innovations: (1) decoupled vocabularies to guide
modality-specific generation, and (2) task-specific LoRA for
parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage
training strategy to mitigate data imbalance between two tasks. Equipped with
these techniques, OmniMamba achieves competitive performance with JanusFlow
while surpassing Show-o across benchmarks, despite being trained on merely 2M
image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba
stands out with outstanding inference efficiency, achieving up to a 119.2 times
speedup and 63% GPU memory reduction for long-sequence generation compared to
Transformer-based counterparts. Code and models are released at
https://github.com/hustvl/OmniMamba
### ğŸŒŸ è®ºæ–‡è§£è¯» | OmniMambaï¼šé«˜æ•ˆä¸”ç»Ÿä¸€çš„è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œè·¨æ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œå¤æ‚çš„è®¡ç®—ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹é€šå¸¸ä¾èµ–äºTransformeræ¶æ„ï¼Œè¿™å¯¼è‡´äº†äºŒæ¬¡è®¡ç®—å¤æ‚æ€§å’Œè¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§£è€¦è¯æ±‡è¡¨
OmniMambaé‡‡ç”¨äº†è§£è€¦è¯æ±‡è¡¨çš„è®¾è®¡ï¼Œä¸ºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€åˆ†åˆ«ä½¿ç”¨ç‹¬ç«‹çš„è¯æ±‡è¡¨ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ ç‰¹å®šæ¨¡æ€çš„ç”Ÿæˆï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»»åŠ¡ç‰¹å®šLoRA
ä¸ºäº†æé«˜æ¨¡å‹å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”æ€§ï¼ŒOmniMambaå¼•å…¥äº†ä»»åŠ¡ç‰¹å®šçš„LoRAæ¨¡å—ã€‚è¿™äº›æ¨¡å—è¢«åº”ç”¨äºMamba-2å±‚çš„è¾“å…¥æŠ•å½±ä¸­ï¼Œå¹¶åœ¨æ‰§è¡Œç‰¹å®šä»»åŠ¡æ—¶æ¿€æ´»ç›¸åº”çš„LoRAè·¯å¾„ï¼Œä»è€Œæé«˜äº†æ•°æ®æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§£è€¦è®­ç»ƒç­–ç•¥
OmniMambaé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è§£è€¦è®­ç»ƒç­–ç•¥ï¼Œä»¥è§£å†³ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é¢„è®­ç»ƒï¼Œç”¨äºæ¨¡å—çš„åˆå§‹åŒ–å’Œæ¨¡æ€å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ç»Ÿä¸€çš„å¾®è°ƒï¼Œç”¨äºå¤šä»»åŠ¡è®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è·¨æ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒOmniMambaå–å¾—äº†ä¸JanusFlowç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶è¶…è¿‡äº†Show-oã€‚æ­¤å¤–ï¼ŒOmniMambaåœ¨æ¨ç†æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸åŸºäºTransformerçš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾119.2å€çš„åŠ é€Ÿå’Œ63%çš„GPUå†…å­˜å‡å°‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
OmniMambaçš„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ä¸ºè·¨æ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚è§£è€¦è¯æ±‡è¡¨å’Œä»»åŠ¡ç‰¹å®šLoRAçš„è®¾è®¡å¯ä»¥æé«˜æ¨¡å‹çš„æ•°æ®æ•ˆç‡å’Œé€‚åº”æ€§ï¼Œè€Œè§£è€¦è®­ç»ƒç­–ç•¥å¯ä»¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚è¿™äº›åˆ›æ–°ç‚¹å¯¹äºå¼€å‘é«˜æ•ˆä¸”ç»Ÿä¸€çš„è·¨æ¨¡æ€æ¨¡å‹å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## dual-diffusion-for-unified-image-generation-and-understanding
### Abstract
Diffusion models have gained tremendous success in text-to-image generation,
yet still lag behind with visual understanding tasks, an area dominated by
autoregressive vision-language models. We propose a large-scale and fully
end-to-end diffusion model for multi-modal understanding and generation that
significantly improves on existing diffusion-based multimodal models, and is
the first of its kind to support the full suite of vision-language modeling
capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and
recent advances in discrete diffusion language modeling, we leverage a
cross-modal maximum likelihood estimation framework that simultaneously trains
the conditional likelihoods of both images and text jointly under a single loss
function, which is back-propagated through both branches of the diffusion
transformer. The resulting model is highly flexible and capable of a wide range
of tasks including image generation, captioning, and visual question answering.
Our model attained competitive performance compared to recent unified image
understanding and generation models, demonstrating the potential of multimodal
diffusion modeling as a promising alternative to autoregressive next-token
prediction models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŒå‘æ‰©æ•£æ¨¡å‹ï¼šç»Ÿä¸€å›¾åƒç”Ÿæˆä¸ç†è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨è§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢ä»ç„¶è½åäºè‡ªå›å½’è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡æ—¨åœ¨æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼Œå¹¶æ˜¾è‘—æå‡ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒå‘æ‰©æ•£æ¨¡å‹
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Dual Diffusion Transformer (D-DiT) çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºå¤šæ¨¡æ€æ‰©æ•£ Transformer (MM-DiT) æ¶æ„ï¼Œå¹¶è¿›è¡Œäº†ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å›¾åƒå’Œæ–‡æœ¬ä¸¤ç§æ¨¡æ€ä¸Šè¾“å‡ºæ‰©æ•£ç›®æ ‡ã€‚D-DiT æ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼Œå®ç°äº†å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰å¤šç§åŠŸèƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè”åˆæŸå¤±å‡½æ•°
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ã€ä¼˜é›…ä¸”æ˜“äºå®ç°çš„è”åˆæŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æŸå¤±å‡½æ•°é€šè¿‡æœ€å°åŒ–å›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆè¯¯å·®ï¼Œå®ç°äº†ä¸¤ç§æ¨¡æ€çš„è”åˆå»ºæ¨¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD-DiT æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŒå‘æ‰©æ•£æ¨¡å‹ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

*   **è”åˆå»ºæ¨¡**ï¼šé€šè¿‡è”åˆè®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥å®ç°æ›´å…¨é¢çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚
*   **åŒå‘æ‰©æ•£**ï¼šåŒå‘æ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å›¾åƒå’Œæ–‡æœ¬ä¸¤ç§æ¨¡æ€ï¼Œå¹¶å®ç°æ›´çµæ´»çš„é‡‡æ ·æ–¹å¼ã€‚
*   **è”åˆæŸå¤±å‡½æ•°**ï¼šè”åˆæŸå¤±å‡½æ•°å¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶æé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºçš„åŒå‘æ‰©æ•£æ¨¡å‹ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚è¯¥æ¨¡å‹æœ‰æœ›åœ¨å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚

## lmfusion--adapting-pretrained-language-models-for-multimodal-generation
### Abstract
We present LMFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LMFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LMFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LMFusion improves image understanding by 20% and image generation by 3.6% using
only 50% of the FLOPs while maintaining Llama-3's language capabilities. We
also demonstrate that this framework can adapt existing vision-language models
with multimodal generation ability. Overall, this framework not only leverages
existing computational investments in text-only LLMs but also enables the
parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LMFusionï¼šèµ‹äºˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿™éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼Œç›´æ¥åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬è¯­è¨€æ¨¡å‹ä¼šå¯¼è‡´å…¶è¯­è¨€å¤„ç†èƒ½åŠ›çš„æ˜¾è‘—ä¸‹é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
LMFusion æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬è¯­è¨€æ¨¡å‹ Llama-3 ä¸Šå¼•å…¥é¢å¤–çš„å¹¶è¡Œ Transformer æ¨¡å—æ¥èµ‹äºˆå…¶å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚LMFusion çš„å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¡ç®—å¤ç”¨
LMFusion åˆ©ç”¨ç°æœ‰è®¡ç®—èµ„æºï¼Œæ— éœ€åœ¨æ–‡æœ¬æ•°æ®ä¸Šé‡æ–°è®­ç»ƒï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ€§èƒ½ä¿ç•™å’Œè¿ç§»
LMFusion é€šè¿‡å†»ç»“æ–‡æœ¬æ¨¡å—å¹¶ä»…å¾®è°ƒå›¾åƒæ¨¡å—ï¼Œåœ¨ä¿ç•™é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›çš„åŒæ—¶ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒLMFusion åœ¨å›¾åƒç†è§£å’Œå›¾åƒç”Ÿæˆæ–¹é¢åˆ†åˆ«æé«˜äº† 20% å’Œ 3.6%ï¼ŒåŒæ—¶ä»…ä½¿ç”¨äº† 50% çš„ FLOPsï¼Œå¹¶ä¿æŒäº† Llama-3 çš„è¯­è¨€èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LMFusion çš„æ¡†æ¶ä¸ä»…é€‚ç”¨äºæ–‡æœ¬è¯­è¨€æ¨¡å‹ï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæé«˜å…¶å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLMFusion çš„è®¾è®¡ç†å¿µå¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘ç”Ÿæˆå’ŒéŸ³é¢‘ç”Ÿæˆã€‚

## metamorph--multimodal-understanding-and-generation-via-instruction-tuning
### Abstract
In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a
simple and effective extension to visual instruction tuning that enables a
pretrained LLM to quickly morph into an unified autoregressive model capable of
generating both text and visual tokens. VPiT teaches an LLM to predict discrete
text tokens and continuous visual tokens from any input sequence of image and
text data curated in an instruction-following format. Our empirical
investigation reveals several intriguing properties of VPiT: (1) visual
generation ability emerges as a natural byproduct of improved visual
understanding, and can be unlocked efficiently with a small amount of
generation data; (2) while we find understanding and generation to be mutually
beneficial, understanding data contributes to both capabilities more
effectively than generation data. Building upon these findings, we train our
MetaMorph model and achieve competitive performance on both visual
understanding and generation. In visual generation, MetaMorph can leverage the
world knowledge and reasoning abilities gained from LLM pretraining, and
overcome common failure modes exhibited by other generation models. Our results
suggest that LLMs may have strong "prior" vision capabilities that can be
efficiently adapted to both visual understanding and generation with a
relatively simple instruction tuning process.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaMorphï¼šé€šè¿‡æŒ‡ä»¤å¾®è°ƒå®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢çš„è¿›æ­¥ï¼Œä»åŸºæœ¬çš„å›¾åƒæè¿°åˆ°å¤æ‚çš„è§†è§‰æ¨ç†ï¼Œè¿™äº›æ¨¡å‹å·²ç»èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆä¸»è¦æ˜¯å›¾åƒå’Œè¯­è¨€ï¼‰å¹¶ç”Ÿæˆæ–‡æœ¬æ ‡è®°ã€‚ç„¶è€Œï¼Œç›®å‰çš„å¤šæ¨¡æ€æ¨¡å‹å¾€å¾€å°†è§†è§‰ç”Ÿæˆè§†ä¸ºä¸è§†è§‰ç†è§£æ­£äº¤çš„èƒ½åŠ›ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®å’Œæ—¶é—´è¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿå¿«é€Ÿè½¬å˜ä¸ºä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰æ ‡è®°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†è§†è§‰é¢„æµ‹æŒ‡ä»¤å¾®è°ƒï¼ˆVPiTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ‰©å±•ï¼Œå®ƒä½¿é¢„è®­ç»ƒçš„LLMèƒ½å¤Ÿå¿«é€Ÿè½¬å˜ä¸ºä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰æ ‡è®°ã€‚VPiT æ•™å¯¼ LLM ä»ä»»ä½•ä»¥æŒ‡ä»¤éµå¾ªæ ¼å¼ç¼–å†™çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®è¾“å…¥åºåˆ—ä¸­é¢„æµ‹ç¦»æ•£çš„æ–‡æœ¬æ ‡è®°å’Œè¿ç»­çš„è§†è§‰æ ‡è®°ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡å‘ç° VPiT å…·æœ‰ä»¥ä¸‹æœ‰è¶£ç‰¹æ€§ï¼š
1. è§†è§‰ç”Ÿæˆèƒ½åŠ›æ˜¯æ”¹è¿›è§†è§‰ç†è§£çš„å‰¯äº§å“ï¼Œå¹¶ä¸”å¯ä»¥ç”¨å°‘é‡çš„ç”Ÿæˆæ•°æ®è¿›è¡Œé«˜æ•ˆè§£é”ã€‚
2. è™½ç„¶æˆ‘ä»¬å‘ç°ç†è§£å’Œç”Ÿæˆæ˜¯ç›¸äº’æœ‰ç›Šçš„ï¼Œä½†ç†è§£æ•°æ®æ¯”ç”Ÿæˆæ•°æ®æ›´æœ‰æ•ˆåœ°è´¡çŒ®äºè¿™ä¸¤ç§èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åŸºäºä»¥ä¸Šå‘ç°ï¼Œæœ¬æ–‡è®­ç»ƒäº†ä¸€ä¸ªåä¸º MetaMorph çš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¹¶åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢ï¼ŒMetaMorph å¯ä»¥åˆ©ç”¨ä» LLM é¢„è®­ç»ƒä¸­è·å¾—çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶å…‹æœå…¶ä»–ç”Ÿæˆæ¨¡å‹ä¸­å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ VPiT æ–¹æ³•ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„ LLM çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ç†è§£å’Œç”Ÿæˆæ˜¯ç›¸äº’æœ‰ç›Šçš„ï¼Œå¹¶ä¸”ç†è§£æ•°æ®æ¯”ç”Ÿæˆæ•°æ®æ›´æœ‰æ•ˆåœ°è´¡çŒ®äºè¿™ä¸¤ç§èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚

## synergen-vl--towards-synergistic-image-understanding-and-generation-with-vision-experts-and-token-folding
### Abstract
The remarkable success of Large Language Models (LLMs) has extended to the
multimodal domain, achieving outstanding performance in image understanding and
generation. Recent efforts to develop unified Multimodal Large Language Models
(MLLMs) that integrate these capabilities have shown promising results.
However, existing approaches often involve complex designs in model
architecture or training pipeline, increasing the difficulty of model training
and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful
encoder-free MLLM capable of both image understanding and generation. To
address challenges identified in existing encoder-free unified MLLMs, we
introduce the token folding mechanism and the vision-expert-based progressive
alignment pretraining strategy, which effectively support high-resolution image
understanding while reducing training complexity. After being trained on
large-scale mixed image-text data with a unified next-token prediction
objective, SynerGen-VL achieves or surpasses the performance of existing
encoder-free unified MLLMs with comparable or smaller parameter sizes, and
narrows the gap with task-specific state-of-the-art models, highlighting a
promising path toward future unified MLLMs. Our code and models shall be
released.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SynerGen-VLï¼šåŸºäºè§†è§‰ä¸“å®¶å’ŒTokenæŠ˜å çš„ååŒå›¾åƒç†è§£å’Œç”Ÿæˆæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„æˆåŠŸåº”ç”¨ï¼Œç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼ˆMLLMsï¼‰æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMsæ¨¡å‹æ¶æ„æˆ–è®­ç»ƒæµç¨‹å¤æ‚ï¼Œå¢åŠ äº†æ¨¡å‹è®­ç»ƒå’Œæ‰©å±•çš„éš¾åº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTokenæŠ˜å æœºåˆ¶
ä¸ºäº†æ”¯æŒé«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒå¤æ‚æ€§ï¼ŒSynerGen-VLå¼•å…¥äº†TokenæŠ˜å æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡å°†è¾“å…¥çš„è§†è§‰Tokenåºåˆ—å‹ç¼©ï¼Œå‡å°‘äº†åºåˆ—é•¿åº¦ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§†è§‰ä¸“å®¶å’Œæ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥
ä¸ºäº†å°†è§†è§‰èƒ½åŠ›æ•´åˆåˆ°é¢„è®­ç»ƒçš„LLMä¸­ï¼ŒåŒæ—¶æœ€å°åŒ–å¯¹LLMé¢„è®­ç»ƒçŸ¥è¯†çš„å¹²æ‰°ï¼ŒSynerGen-VLå¼•å…¥äº†è§†è§‰ä¸“å®¶å’Œæ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥ã€‚è§†è§‰ä¸“å®¶æ˜¯é’ˆå¯¹å›¾åƒè¡¨ç¤ºçš„é¢å¤–å‚æ•°ï¼Œè€Œæ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥åˆ™é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒï¼Œé€æ­¥å°†è§†è§‰ç‰¹å¾ä¸LLMçš„è¡¨ç¤ºç©ºé—´å¯¹é½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SynerGen-VLåœ¨å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰ç¼–ç å™¨æ— å…³çš„ç»Ÿä¸€MLLMsç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”ç¼©å°äº†ä¸ç‰¹å®šä»»åŠ¡æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒSynerGen-VLåœ¨éœ€è¦é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒSynerGen-VLä¹Ÿå–å¾—äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SynerGen-VLçš„TokenæŠ˜å æœºåˆ¶å’Œè§†è§‰ä¸“å®¶ç­–ç•¥ä¸ºæ„å»ºç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆæ¨¡å‹æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSynerGen-VLçš„æ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥ä¹Ÿä¸ºå¦‚ä½•åœ¨ä¿æŒLLMé¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶æ•´åˆè§†è§‰èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯ã€‚

## diffusion-language-models-are-versatile-protein-learners
### Abstract
This paper introduces diffusion protein language model (DPLM), a versatile
protein language model that demonstrates strong generative and predictive
capabilities for protein sequences. We first pre-train scalable DPLMs from
evolutionary-scale protein sequences within a generative self-supervised
discrete diffusion probabilistic framework, which generalizes language modeling
for proteins in a principled way. After pre-training, DPLM exhibits the ability
to generate structurally plausible, novel, and diverse protein sequences for
unconditional generation. We further demonstrate the proposed diffusion
generative pre-training makes DPLM possess a better understanding of proteins,
making it a superior representation learner, which can be fine-tuned for
various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).
Moreover, DPLM can be tailored for various needs, which showcases its prowess
of conditional generation in several ways: (1) conditioning on partial peptide
sequences, e.g., generating scaffolds for functional motifs with high success
rate; (2) incorporating other modalities as conditioner, e.g.,
structure-conditioned generation for inverse folding; and (3) steering sequence
generation towards desired properties, e.g., satisfying specified secondary
structures, through a plug-and-play classifier guidance. Code is released at
\url{https://github.com/bytedance/dplm}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è›‹ç™½è´¨å­¦ä¹ çš„æ–°ç¯‡ç« ï¼šæ‰©æ•£è¯­è¨€æ¨¡å‹çš„å¤šåŠŸèƒ½åº”ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è›‹ç™½è´¨ä½œä¸ºç”Ÿå‘½æ´»åŠ¨çš„åŸºç¡€ï¼Œå…¶åºåˆ—å’Œç»“æ„çš„ç ”ç©¶å¯¹äºç†è§£ç”Ÿç‰©åŠŸèƒ½å’Œè®¾è®¡æ–°å‹è›‹ç™½è´¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚ï¼ŒåŸºäºæ©ç é¢„æµ‹çš„æ¨¡å‹éš¾ä»¥è¿›è¡Œåºåˆ—ç”Ÿæˆï¼Œè€ŒåŸºäºè‡ªå›å½’çš„æ¨¡å‹åˆ™åœ¨åºåˆ—ç†è§£ä¸Šæœ‰æ‰€æ¬ ç¼ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ‰©æ•£è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆDPLMï¼‰
æœ¬æ–‡æå‡ºäº†DPLMï¼Œä¸€ç§åŸºäºç¦»æ•£æ‰©æ•£æ¦‚ç‡æ¡†æ¶çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ã€‚DPLMé€šè¿‡åœ¨è¿›åŒ–å°ºåº¦ä¸Šçš„è›‹ç™½è´¨åºåˆ—ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆç»“æ„åˆç†ã€æ–°é¢–ä¸”å¤šæ ·çš„è›‹ç™½è´¨åºåˆ—ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¡ä»¶ç”Ÿæˆç­–ç•¥
DPLMæ”¯æŒå¤šç§æ¡ä»¶ç”Ÿæˆç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºäºéƒ¨åˆ†è‚½åºåˆ—çš„æ¡ä»¶ç”Ÿæˆï¼Œä¾‹å¦‚ï¼Œä»¥é«˜æˆåŠŸç‡ç”ŸæˆåŠŸèƒ½åŸºåºçš„æ”¯æ¶ã€‚
2. ç»“åˆå…¶ä»–æ¨¡æ€ä½œä¸ºæ¡ä»¶ï¼Œä¾‹å¦‚ï¼Œç»“æ„æ¡ä»¶ç”Ÿæˆç”¨äºé€†æŠ˜å ã€‚
3. é€šè¿‡å³æ’å³ç”¨çš„åˆ†ç±»å™¨å¼•å¯¼ï¼Œå°†åºåˆ—ç”Ÿæˆå¼•å¯¼åˆ°æ‰€éœ€çš„å±æ€§ï¼Œä¾‹å¦‚ï¼Œæ»¡è¶³æŒ‡å®šçš„äºŒçº§ç»“æ„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒDPLMåœ¨æ— æ¡ä»¶ç”Ÿæˆã€è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ å’Œæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚DPLMç”Ÿæˆçš„è›‹ç™½è´¨åºåˆ—å…·æœ‰é«˜ç»“æ„åˆç†æ€§ã€æ–°é¢–æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶ä¸”åœ¨å„ç§é¢„æµ‹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„è›‹ç™½è´¨åºåˆ—ç¼–ç å™¨æ¨¡å‹ï¼Œå¦‚ESM-2ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DPLMä¸ºè›‹ç™½è´¨å­¦ä¹ å’Œè®¾è®¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶å¼ºå¤§çš„ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›ä»¥åŠçµæ´»çš„æ¡ä»¶ç”Ÿæˆç­–ç•¥ä½¿å…¶åœ¨è¯ç‰©å‘ç°ã€è›‹ç™½è´¨å·¥ç¨‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æ­¤å¤–ï¼ŒDPLMçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–åºåˆ—å»ºæ¨¡ä»»åŠ¡æä¾›å€Ÿé‰´ã€‚

## tokenflow--unified-image-tokenizer-for-multimodal-understanding-and-generation
### Abstract
We present TokenFlow, a novel unified image tokenizer that bridges the
long-standing gap between multimodal understanding and generation. Prior
research attempt to employ a single reconstruction-targeted Vector Quantization
(VQ) encoder for unifying these two tasks. We observe that understanding and
generation require fundamentally different granularities of visual information.
This leads to a critical trade-off, particularly compromising performance in
multimodal understanding tasks. TokenFlow addresses this challenge through an
innovative dual-codebook architecture that decouples semantic and pixel-level
feature learning while maintaining their alignment via a shared mapping
mechanism. This design enables direct access to both high-level semantic
representations crucial for understanding tasks and fine-grained visual
features essential for generation through shared indices. Our extensive
experiments demonstrate TokenFlow's superiority across multiple dimensions.
Leveraging TokenFlow, we demonstrate for the first time that discrete visual
input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\%
average improvement. For image reconstruction, we achieve a strong FID score of
0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art
performance in autoregressive image generation with a GenEval score of 0.55 at
256*256 resolution, achieving comparable results to SDXL.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TokenFlowï¼šç»Ÿä¸€å›¾åƒåˆ†è¯å™¨ï¼Œè·¨è¶Šå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„é¸¿æ²Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œè§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡é€šå¸¸éœ€è¦ä¸åŒçš„è§†è§‰ä¿¡æ¯ç²’åº¦ã€‚è§†è§‰ç†è§£éœ€è¦ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºæ¥æ”¯æŒå¤æ‚çš„æ¨ç†ï¼Œè€Œè§†è§‰ç”Ÿæˆåˆ™éœ€è¦ç²¾ç¡®çš„ç©ºé—´ç»“æ„å’Œçº¹ç†ç»†èŠ‚ç¼–ç ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€é‡‡ç”¨å•ä¸€çš„é‡å»ºç›®æ ‡å‘é‡é‡åŒ–ï¼ˆVQï¼‰ç¼–ç å™¨æ¥ç»Ÿä¸€è¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œè¿™å¯¼è‡´äº†ä¸€ä¸ªå…³é”®çš„æƒè¡¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­æ€§èƒ½çš„å¦¥åã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
TokenFlow é€šè¿‡å…¶ç‹¬ç‰¹çš„åŒæµè®¾è®¡è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ã€‚TokenFlow çš„å…³é”®æ´å¯Ÿæ˜¯è§£è€¦è¯­ä¹‰å’Œåƒç´ çº§ç‰¹å¾çš„å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«ç´¢å¼•æ˜ å°„ä¿æŒå®ƒä»¬çš„å¯¹é½ã€‚é€šè¿‡å°†å…·æœ‰è¯­ä¹‰å’Œåƒç´ çº§ç›¸ä¼¼æ€§çš„è¡¥ä¸æ˜ å°„åˆ°ç›¸åŒçš„ç´¢å¼•ï¼Œé‡åŒ–ç‰¹å¾å¯ä»¥ç›´æ¥åº”ç”¨äºè‡ªå›å½’è§†è§‰ç”Ÿæˆå’Œå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚

TokenFlow é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼ŒåŒ…æ‹¬è¯­ä¹‰ç¼–ç å™¨ Esem å’Œåƒç´ ç¼–ç å™¨ Epixã€‚è¯­ä¹‰ç¼–ç å™¨ä»é¢„è®­ç»ƒçš„æ–‡æœ¬å¯¹é½è§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚ CLIP ViT-B/14ï¼‰ä¸­å­¦ä¹ ï¼Œè€Œåƒç´ ç¼–ç å™¨æ•è·è¯¦ç»†çš„è§†è§‰ä¿¡æ¯ã€‚æå–çš„ç‰¹å¾ç„¶åé€šè¿‡æœ€å°åŒ–è¯­ä¹‰å’Œåƒç´ çº§è·ç¦»çš„åŠ æƒæ±‚å’Œæ¥é‡åŒ–ï¼Œåˆ›å»ºä¸€ä¸ªè”åˆè¡¨ç¤ºç©ºé—´ã€‚

TokenFlow çš„åŒä»£ç æœ¬è®¾è®¡å…è®¸ä¸“ä¸šåŒ–å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«ç´¢å¼•ä¿æŒè·¨çº§åˆ«ç›¸å…³æ€§ã€‚è¿™ç§åˆ›æ–°å…è®¸åŒæ—¶è®¿é—®è¯­ä¹‰å’Œåƒç´ çº§è¡¨ç¤ºï¼Œè€Œä¸ä¼šæŸå®³ä»»ä½•æ–¹é¢ã€‚TokenFlow è¿˜å±•ç¤ºäº†æ˜¾è‘—çš„æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨è¶…è¿‡ 130K æ¡ç›®çš„è¶…å¤§è§„æ¨¡ä»£ç æœ¬ä¸­ï¼Œä¹Ÿèƒ½ä¿æŒå¼‚å¸¸é«˜çš„ä»£ç æœ¬åˆ©ç”¨ç‡ï¼ˆ95%+ï¼‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
TokenFlow åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚åˆ©ç”¨ TokenFlowï¼Œæˆ‘ä»¬é¦–æ¬¡è¯æ˜ç¦»æ•£è§†è§‰è¾“å…¥å¯ä»¥è¶…è¶Š LLaVA-1.5 13B åœ¨ç†è§£æ€§èƒ½æ–¹é¢ï¼Œå¹³å‡æé«˜äº† 7.2%ã€‚å¯¹äºå›¾åƒé‡å»ºï¼Œæˆ‘ä»¬åœ¨ 384*384 åˆ†è¾¨ç‡ä¸‹å®ç°äº† 0.63 çš„ FID åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒTokenFlow åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹é¢å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ 256*256 åˆ†è¾¨ç‡ä¸‹å®ç°äº† 0.55 çš„ GenEval åˆ†æ•°ï¼Œå®ç°äº†ä¸ SDXL ç›¸å½“çš„ç»“æœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TokenFlow çš„åŒä»£ç æœ¬æ¶æ„å’Œå…±äº«æ˜ å°„æœºåˆ¶ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„ç»Ÿä¸€è§†è§‰ç¼–ç æœºåˆ¶ã€‚TokenFlow çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥å¯å‘æœªæ¥å¤šæ¨¡æ€æ¨¡å‹çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæ¨åŠ¨å¤šæ¨¡æ€é¢†åŸŸçš„å‘å±•ã€‚

## loraflow--high-quality-signal-reconstruction-using-rectified-flow
### Abstract
LoRa technology, crucial for low-power wide-area networks, faces significant
performance degradation at extremely low signal-to-noise ratios (SNRs). We
present LoRaFlow, a novel approach using rectified flow to reconstruct
high-quality LoRa signals in challenging noise conditions. Unlike existing
neural-enhanced methods focused on classification, LoRaFlow recovers the signal
itself, maintaining compatibility with standard dechirp algorithms. Our method
combines a hybrid neural network architecture, synthetic data generation, and
robust augmentation strategies. This minimally invasive enhancement to LoRa
infrastructure potentially extends operational range and reliability without
overhauling existing systems. LoRaFlow opens new possibilities for robust IoT
communications in harsh environments and its core methodology can be
generalized to support various communication technologies.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LoRaFlowï¼šåŸºäºæ ¡æ­£æµçš„LoRaä¿¡å·é«˜è´¨é‡é‡å»º

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
LoRaæŠ€æœ¯æ˜¯ä½åŠŸè€—å¹¿åŸŸç½‘ç»œï¼ˆLPWANï¼‰çš„å…³é”®æŠ€æœ¯ï¼Œåœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼ŒLoRaä¿¡å·åœ¨æä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´å’Œå¯é æ€§ã€‚ç°æœ‰çš„åŸºäºç¥ç»ç½‘ç»œçš„LoRaä¿¡å·å¢å¼ºæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œè€ŒLoRaFlowåˆ™ä¸“æ³¨äºä¿¡å·æœ¬èº«çš„é‡å»ºï¼Œä»¥æ¢å¤é«˜è´¨é‡ä¿¡å·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ¡æ­£æµæŠ€æœ¯
LoRaFlowé‡‡ç”¨æ ¡æ­£æµæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å™ªå£°è¾“å…¥ä¸­é‡å»ºé«˜è´¨é‡çš„LoRaä¿¡å·ã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œæ ¡æ­£æµé€šè¿‡å­¦ä¹ ç›´çº¿è½¨è¿¹çš„æ˜ å°„ï¼Œå‡å°‘äº†é‡å»ºä¿¡å·æ‰€éœ€çš„æ­¥éª¤ï¼Œä»è€Œåœ¨ä½SNRæ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ··åˆç¥ç»ç½‘ç»œæ¶æ„
LoRaFlowçš„æ¨¡å‹æ¶æ„ç»“åˆäº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰å’Œå·ç§¯å±‚ï¼Œå……åˆ†åˆ©ç”¨äº†DiTçš„æ‰©å±•æ€§å’Œçµæ´»æ€§ï¼Œä»¥åŠå·ç§¯å±‚åœ¨å¤„ç†å¤šå°ºåº¦ç©ºé—´ä¿¡æ¯æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™ç§æ··åˆæ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰LoRaä¿¡å·ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆæˆæ•°æ®ç”Ÿæˆå’Œå¢å¼ºç­–ç•¥
ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼ŒLoRaFlowä½¿ç”¨åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œæ¨¡æ‹Ÿäº†ä¸åŒæ‰©é¢‘å› å­ï¼ˆSFï¼‰å’Œå¸¦å®½ï¼ˆBWï¼‰çš„LoRaä¿¡å·ã€‚æ­¤å¤–ï¼ŒLoRaFlowè¿˜é‡‡ç”¨äº†å¤šç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¦‚é¢‘ç‡åŸŸæ©è”½ã€æ—¶åŸŸå¹³ç§»ã€ä¿¡å·åè½¬å’Œé¢‘è°±å›¾æ»šåŠ¨ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæœ€å°ä¾µå…¥å¼é›†æˆ
LoRaFlowçš„è®¾è®¡æ—¨åœ¨ä¸ç°æœ‰çš„LoRaåŸºç¡€è®¾æ–½æ— ç¼é›†æˆï¼Œæ— éœ€å¯¹ç°æœ‰ç³»ç»Ÿè¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚å®ƒé€šè¿‡åœ¨æ ‡å‡†è§£è°ƒç®—æ³•ä¹‹å‰å¯¹ä¿¡å·è¿›è¡Œå»å™ªï¼Œä»è€Œæé«˜LoRaç½‘ç»œçš„æ€§èƒ½ï¼Œè€Œä¸ä¼šå½±å“ç°æœ‰çš„LoRaç¡¬ä»¶å’Œè½¯ä»¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
LoRaFlowåœ¨å¤šä¸ªæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„è§£è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒLoRaFlowåœ¨ä½SNRæ¡ä»¶ä¸‹å®ç°äº†æ›´é«˜çš„ä¿¡å·é‡å»ºç²¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨ç›¸ä½å’Œå¹…åº¦æ–¹é¢ã€‚æ­¤å¤–ï¼ŒLoRaFlowåœ¨ç¬¦å·é”™è¯¯ç‡ï¼ˆSERï¼‰æ–¹é¢ä¹Ÿä¼˜äºç°æœ‰çš„åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œå¦‚NELoRaã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LoRaFlowçš„æ ¸å¿ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬æ ¡æ­£æµæŠ€æœ¯ã€æ··åˆç¥ç»ç½‘ç»œæ¶æ„ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œå¢å¼ºç­–ç•¥ï¼Œä»¥åŠæœ€å°ä¾µå…¥å¼é›†æˆï¼Œä¸ºLoRaä¿¡å·å¢å¼ºé¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒLoRaFlowçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–é€šä¿¡æŠ€æœ¯çš„ä¿¡å·é‡å»ºä»»åŠ¡æä¾›å‚è€ƒã€‚

## orthus--autoregressive-interleaved-image-text-generation-with-modality-specific-heads
### Abstract
We introduce Orthus, an autoregressive (AR) transformer that excels in
generating images given textual prompts, answering questions based on visual
inputs, and even crafting lengthy image-text interleaved contents. Unlike prior
arts on unified multimodal modeling, Orthus simultaneously copes with discrete
text tokens and continuous image features under the AR modeling principle. The
continuous treatment of visual signals minimizes the information loss for both
image understanding and generation while the fully AR formulation renders the
characterization of the correlation between modalities straightforward. The key
mechanism enabling Orthus to leverage these advantages lies in its
modality-specific heads -- one regular language modeling (LM) head predicts
discrete text tokens and one diffusion head generates continuous image features
conditioning on the output of the backbone. We devise an efficient strategy for
building Orthus -- by substituting the Vector Quantization (VQ) operation in
the existing unified AR model with a soft alternative, introducing a diffusion
head, and tuning the added modules to reconstruct images, we can create an
Orthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).
Orthus-base can further embrace post-training to better model interleaved
images and texts. Empirically, Orthus surpasses competing baselines including
Show-o and Chameleon across standard benchmarks, achieving a GenEval score of
0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows
exceptional mixed-modality generation capabilities, reflecting the potential
for handling intricate practical generation tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Orthusï¼šåŸºäºè‡ªå›å½’çš„å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨å»ºæ¨¡å†—ä½™å’Œä¿¡æ¯æŸå¤±çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç¦»æ•£ä»¤ç‰Œè¿›è¡Œè‡ªå›å½’å»ºæ¨¡ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œè€Œå°†è‡ªå›å½’å»ºæ¨¡ä¸æ‰©æ•£å»ºæ¨¡ç»“åˆåˆ™éš¾ä»¥åŒæ—¶å¤„ç†å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
Orthus æ˜¯ä¸€ç§åŸºäºè‡ªå›å½’çš„å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡ä»¥ä¸‹åˆ›æ–°ç‚¹è§£å†³äº†ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¿ç»­å¤„ç†è§†è§‰ä¿¡å·
Orthus ç›´æ¥å¤„ç†è¿ç»­çš„å›¾åƒç‰¹å¾ï¼Œé¿å…äº†å›¾åƒä»¤ç‰ŒåŒ–è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ï¼Œä»è€Œæé«˜äº†å›¾åƒç†è§£å’Œç”Ÿæˆçš„è´¨é‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»Ÿä¸€çš„è‡ªå›å½’å»ºæ¨¡
Orthus ä½¿ç”¨ç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹æ¥å¤„ç†ç¦»æ•£çš„æ–‡æœ¬ä»¤ç‰Œå’Œè¿ç»­çš„å›¾åƒç‰¹å¾ï¼Œç®€åŒ–äº†æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§å»ºæ¨¡ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡æ€ç‰¹å®šå¤´éƒ¨
Orthus ä½¿ç”¨ä¸¤ä¸ªæ¨¡æ€ç‰¹å®šå¤´éƒ¨ï¼šä¸€ä¸ªè¯­è¨€æ¨¡å‹å¤´éƒ¨é¢„æµ‹ç¦»æ•£çš„æ–‡æœ¬ä»¤ç‰Œï¼Œä¸€ä¸ªæ‰©æ•£å¤´éƒ¨ç”Ÿæˆè¿ç»­çš„å›¾åƒç‰¹å¾ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé«˜æ•ˆçš„æ„å»ºç­–ç•¥
Orthus å¯ä»¥é€šè¿‡æ›¿æ¢ç°æœ‰ç»Ÿä¸€è‡ªå›å½’æ¨¡å‹ä¸­çš„ VQ æ“ä½œå¹¶å¼•å…¥æ‰©æ•£å¤´éƒ¨æ¥é«˜æ•ˆæ„å»ºï¼Œä»è€Œæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOrthus è¶…è¶Šäº†åŒ…æ‹¬ Show-o å’Œ Chameleon åœ¨å†…çš„ç«äº‰åŸºçº¿ï¼Œå®ç°äº† 0.58 çš„ GenEval åˆ†æ•°å’Œ 1265.8 çš„ MME-P åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒOrthus è¿˜å±•ç¤ºäº†å‡ºè‰²çš„æ··åˆæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ç”Ÿæˆä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Orthus çš„åˆ›æ–°æ–¹æ³•ä¸ºå¤šæ¨¡æ€å»ºæ¨¡æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶è¿ç»­å¤„ç†è§†è§‰ä¿¡å·å’Œç»Ÿä¸€çš„è‡ªå›å½’å»ºæ¨¡ç­–ç•¥å€¼å¾—å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒOrthus çš„é«˜æ•ˆæ„å»ºç­–ç•¥ä¹Ÿä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å¼€å‘æä¾›äº†å®è´µçš„ç»éªŒã€‚

## jetformer--an-autoregressive-generative-model-of-raw-images-and-text
### Abstract
Removing modeling constraints and unifying architectures across domains has
been a key driver of the recent progress in training large multimodal models.
However, most of these models still rely on many separately trained components
such as modality-specific encoders and decoders. In this work, we further
streamline joint generative modeling of images and text. We propose an
autoregressive decoder-only transformer - JetFormer - which is trained to
directly maximize the likelihood of raw data, without relying on any separately
pretrained components, and can understand and generate both text and images.
Specifically, we leverage a normalizing flow model to obtain a soft-token image
representation that is jointly trained with an autoregressive multimodal
transformer. The normalizing flow model serves as both an image encoder for
perception tasks and an image decoder for image generation tasks during
inference. JetFormer achieves text-to-image generation quality competitive with
recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
image autoencoders, which are trained with a complex mixture of losses,
including perceptual ones. At the same time, JetFormer demonstrates robust
image understanding capabilities. To the best of our knowledge, JetFormer is
the first model that is capable of generating high-fidelity images and
producing strong log-likelihood bounds.
### ğŸŒŸ è®ºæ–‡è§£è¯» | JetFormerï¼šä¸€ç§ç”¨äºåŸå§‹å›¾åƒå’Œæ–‡æœ¬çš„è‡ªå›å½’ç”Ÿæˆæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è®­ç»ƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…¶ä¸­ä¸€ä¸ªå…³é”®é©±åŠ¨åŠ›æ˜¯å»é™¤å»ºæ¨¡çº¦æŸå¹¶ç»Ÿä¸€è·¨é¢†åŸŸçš„æ¶æ„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›æ¨¡å‹ä»ç„¶ä¾èµ–äºè®¸å¤šå•ç‹¬è®­ç»ƒçš„ç»„ä»¶ï¼Œä¾‹å¦‚ç‰¹å®šäºæ¨¡æ€çš„ç¼–ç å™¨å’Œè§£ç å™¨ã€‚æœ¬æ–‡æ—¨åœ¨è¿›ä¸€æ­¥ç®€åŒ–å›¾åƒå’Œæ–‡æœ¬çš„è”åˆç”Ÿæˆå»ºæ¨¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†JetFormerï¼Œä¸€ç§è‡ªå›å½’è§£ç å™¨-ä»…Transformerï¼Œå®ƒå¯ä»¥ç›´æ¥æœ€å¤§åŒ–åŸå§‹æ•°æ®çš„ä¼¼ç„¶æ€§ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å•ç‹¬é¢„è®­ç»ƒçš„ç»„ä»¶ï¼Œå¹¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨æ­£åˆ™åŒ–æµæ¨¡å‹æ¥è·å¾—è½¯ä»¤ç‰Œå›¾åƒè¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºä¸è‡ªå›å½’å¤šæ¨¡æ€Transformerè”åˆè®­ç»ƒã€‚æ­£åˆ™åŒ–æµæ¨¡å‹åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œåœ¨æ¨ç†æ—¶ä½œä¸ºå›¾åƒè§£ç å™¨ã€‚JetFormeråœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢ä¸æœ€è¿‘çš„VQ-VAEå’ŒVAEåŸºçº¿ç›¸å½“ã€‚è¿™äº›åŸºçº¿ä¾èµ–äºé¢„è®­ç»ƒçš„å›¾åƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œè¿™äº›è‡ªåŠ¨ç¼–ç å™¨æ˜¯ç”¨å¤æ‚çš„æŸå¤±æ··åˆç‰©è®­ç»ƒçš„ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥æŸå¤±ã€‚åŒæ—¶ï¼ŒJetFormerå±•ç¤ºäº†å¼ºå¤§çš„å›¾åƒç†è§£èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒJetFormeræ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå›¾åƒå¹¶äº§ç”Ÿå¼ºå¤§çš„å¯¹æ•°ä¼¼ç„¶ç•Œé™çš„æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
JetFormeråœ¨ImageNetç±»æ¡ä»¶å›¾åƒç”Ÿæˆå’ŒWeb-scaleå¤šæ¨¡æ€ç”Ÿæˆæ–¹é¢è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜JetFormeråœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œè§†è§‰è¯­è¨€ç†è§£æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
JetFormerçš„è®¾è®¡ç®€å•ï¼Œæ˜“äºæ‰©å±•ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒå’Œæ–‡æœ¬ã€‚æ­¤å¤–ï¼ŒJetFormerèƒ½å¤Ÿè®¡ç®—å¯¹æ•°ä¼¼ç„¶ï¼Œè¿™å¯¹äºæ¯”è¾ƒä¸åŒçš„ç”Ÿæˆæ¨¡å‹æˆ–è¿›è¡Œhill-climbingéå¸¸æœ‰ç”¨ã€‚

## muse-vl--modeling-unified-vlm-through-semantic-discrete-encoding
### Abstract
We introduce MUSE-VL, a Unified Vision-Language Model through Semantic
discrete Encoding for multimodal understanding and generation. Recently, the
research community has begun exploring unified models for visual generation and
understanding. However, existing vision tokenizers (e.g., VQGAN) only consider
low-level information, which makes it difficult to align with language tokens.
This results in high training complexity and necessitates a large amount of
training data to achieve optimal performance. Additionally, their performance
is still far from dedicated understanding models. This paper proposes Semantic
Discrete Encoding (SDE), which effectively aligns the information of visual
tokens and language tokens by adding semantic constraints to the visual
tokenizer. This greatly reduces the amount of training data and improves the
performance of the unified model. With the same LLM size, our method improved
the understanding performance by 4.8% compared to the previous SOTA Emu3 and
surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model
also surpasses the existing unified models on visual generation benchmarks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MUSE-VLï¼šåŸºäºè¯­ä¹‰ç¦»æ•£ç¼–ç çš„ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç ”ç©¶é¢†åŸŸæ—¥ç›Šå…´èµ·ï¼Œç ”ç©¶è€…ä»¬è‡´åŠ›äºå°†è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ•´åˆåˆ°è‡ªå›å½’çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œå¦‚ä½•å°†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œä»¥ä¾¿ä¸æ–‡æœ¬æ ‡è®°å¯¹é½ï¼Œæˆä¸ºå®ç°ç»Ÿä¸€MLLMsçš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°åŒ–æ–¹æ³•ï¼ˆå¦‚VQGANï¼‰ä»…è€ƒè™‘ä½çº§ä¿¡æ¯ï¼Œéš¾ä»¥ä¸è¯­è¨€æ ‡è®°å¯¹é½ï¼Œå¯¼è‡´è®­ç»ƒå¤æ‚åº¦é«˜ï¼Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¸”å…¶æ€§èƒ½ä»è¿œä½äºä¸“é—¨çš„è§†è§‰ç†è§£æ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†è¯­ä¹‰ç¦»æ•£ç¼–ç ï¼ˆSDEï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨è§†è§‰æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ·»åŠ è¯­ä¹‰çº¦æŸï¼Œæœ‰æ•ˆåœ°å°†è§†è§‰æ ‡è®°å’Œè¯­è¨€æ ‡è®°çš„ä¿¡æ¯å¯¹é½ã€‚SDEæ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªè¯­ä¹‰ç¼–ç å™¨å’Œä¸€ä¸ªè¯­ä¹‰è§£ç å™¨ï¼Œç”¨äºä»ç¦»æ•£ä»£ç ä¸­æå–å’Œé‡å»ºè¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒSDEæ–¹æ³•è¿˜åŒ…å«ä¸€ä¸ªå›¾åƒè§£ç å™¨ï¼Œç”¨äºä»é‡åŒ–ç‰¹å¾ä¸­é‡å»ºåŸå§‹å›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSDEæ–¹æ³•èƒ½å¤Ÿåœ¨å›¾åƒç¦»æ•£åŒ–è¿‡ç¨‹ä¸­è€ƒè™‘è¯­ä¹‰ä¿¡æ¯ï¼Œæ»¡è¶³è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„éœ€æ±‚ã€‚

åŸºäºSDEæ ‡è®°å™¨ï¼Œæœ¬æ–‡æå‡ºäº†MUSE-VLï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€è‡ªå›å½’å˜æ¢å™¨ã€‚MUSE-VLå°†è§†è§‰å’Œè¯­è¨€æ•°æ®å»ºæ¨¡ä¸ºç»Ÿä¸€çš„ç¦»æ•£æ ‡è®°ï¼Œå¹¶åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„SOTA Emu3ç›¸æ¯”ï¼ŒMUSE-VLåœ¨ç†è§£æ€§èƒ½ä¸Šæé«˜äº†4.8%ï¼Œå¹¶è¶…è¿‡äº†ä¸“é—¨çš„è§†è§‰ç†è§£æ¨¡å‹LLaVA-NeXT 34Bã€‚æ­¤å¤–ï¼ŒMUSE-VLåœ¨è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¶…è¿‡äº†ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„SDEæ–¹æ³•å’ŒMUSE-VLæ¨¡å‹ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚SDEæ–¹æ³•æœ‰æ•ˆåœ°å°†è§†è§‰æ ‡è®°å’Œè¯­è¨€æ ‡è®°çš„ä¿¡æ¯å¯¹é½ï¼Œè€ŒMUSE-VLæ¨¡å‹åˆ™å°†è§†è§‰å’Œè¯­è¨€æ•°æ®å»ºæ¨¡ä¸ºç»Ÿä¸€çš„ç¦»æ•£æ ‡è®°ï¼Œå¹¶åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ–¹æ³•å’Œæ¨¡å‹ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## janusflow--harmonizing-autoregression-and-rectified-flow-for-unified-multimodal-understanding-and-generation
### Abstract
We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | JanusFlowï¼šç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„æ–°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­¦ä¹ å’Œæ³›åŒ–æ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸“é—¨ç”¨äºå›¾åƒç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¤æ‚æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å•ç‹¬è®­ç»ƒå’Œé›†æˆï¼Œå¯¼è‡´æ¶æ„å¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†JanusFlowï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šèåˆè‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œä¿®æ­£æµ
JanusFlowçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸ä¿®æ­£æµç›¸ç»“åˆã€‚ä¿®æ­£æµæ˜¯ä¸€ç§å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨LLMæ¡†æ¶å†…ç›´æ¥è®­ç»ƒï¼Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨
ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼ŒJanusFlowé‡‡ç”¨äº†ä¸¤ç§å…³é”®ç­–ç•¥ï¼šè§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ï¼Œä»¥åŠåœ¨å¯¹é½è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é½å®ƒä»¬çš„è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºé˜²æ­¢ä»»åŠ¡ä¹‹é—´çš„å¹²æ‰°ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
JanusFlowåœ¨å¤šæ¨¡æ€ç†è§£å’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä¸“é—¨çš„æ¨¡å‹ã€‚åœ¨å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusFlowåœ¨MJHQ FID-30kã€GenEvalå’ŒDPG-Benchä¸Šåˆ†åˆ«å–å¾—äº†9.51ã€0.63å’Œ80.09%çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†åŒ…æ‹¬SDv1.5å’ŒSDXLåœ¨å†…çš„ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚åœ¨å¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusFlowåœ¨MMBenchã€SeedBenchå’ŒGQAä¸Šåˆ†åˆ«å–å¾—äº†74.9ã€70.5å’Œ60.3çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†LLaVA-v1.5å’ŒQwen-VL-Chatç­‰ä¸“é—¨æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
JanusFlowçš„æˆåŠŸè¡¨æ˜ï¼Œå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œä¿®æ­£æµç›¸ç»“åˆå¯ä»¥æœ‰æ•ˆåœ°è§£å†³å¤šæ¨¡æ€å­¦ä¹ å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ä»¥åŠè¡¨ç¤ºå¯¹é½æ­£åˆ™åŒ–ç­‰ç­–ç•¥ä¹Ÿä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## on-the-performance-of-multimodal-language-models
### Abstract
Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ€§èƒ½æ¢ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚ä½•å°†è§†è§‰ä¿¡æ¯ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥å¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶ä¸åŒå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ–¹æ³•åœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶åˆ†æå…¶ä¼˜ç¼ºç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æ¯”è¾ƒäº†äº”ç§å…¬å¼€å¯ç”¨çš„å¤šæ¨¡æ€LLMså¾®è°ƒæ–¹æ³•ï¼šBLIP-2ã€InstructBLIPã€LLaVAã€MiniGPT4å’ŒmPLUG-Owlã€‚è¿™äº›æ–¹æ³•æ¶µç›–äº†ä¸åŒçš„æ¶æ„é€‰æ‹©ï¼ŒåŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€è§†è§‰å¤´éƒ¨å’Œæ•°æ®ä½¿ç”¨æ–¹å¼ã€‚é€šè¿‡åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ¶ˆèå®éªŒï¼Œæœ¬æ–‡æ­ç¤ºäº†ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶åˆ†æäº†å½±å“å¤šæ¨¡æ€LLMsæ€§èƒ½çš„å…³é”®å› ç´ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructBLIPåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¿™å½’åŠŸäºå…¶ä¸°å¯Œçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚å…¶ä»–æ–¹æ³•åœ¨æœªè®­ç»ƒè¿‡çš„ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¿‡æ‹Ÿåˆåˆ°ç‰¹å®šä»»åŠ¡ç±»å‹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼š

* ä½¿ç”¨æ›´å¤§çš„è§†è§‰ç¼–ç å™¨ï¼ˆViT-gï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ‰€æœ‰ä»»åŠ¡çš„æ€§èƒ½ã€‚
* è®­ç»ƒè§†è§‰å¤´éƒ¨ï¼ˆå¦‚Q-Formerï¼‰å¯ä»¥æ›´å¥½åœ°æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚
* åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µè®­ç»ƒè¯­è¨€æ¨¡å‹å¯ä»¥å¸¦æ¥é¢å¤–çš„æ€§èƒ½æå‡ã€‚
* æ•°æ®é‡åœ¨è¾¾åˆ°ä¸€å®šè§„æ¨¡åï¼Œå…¶å¯¹æ€§èƒ½çš„æå‡ä½œç”¨é€æ¸å‡å¼±ï¼Œä½†æ•°æ®å¤šæ ·æ€§ä»ç„¶è‡³å…³é‡è¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºæ„å»ºæœ‰æ•ˆçš„å¤šæ¨¡æ€LLMsæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œä»»åŠ¡å¤šæ ·æ€§åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

* æ¢ç´¢æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
* ç ”ç©¶å¦‚ä½•å‡è½»å¤šæ¨¡æ€LLMsçš„å¹»è§‰é—®é¢˜ï¼Œä½¿å…¶æ›´å¯é ã€‚
* å¼€å‘æ›´å…ˆè¿›çš„æ¶æ„å’Œæ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚

## motiongpt-2--a-general-purpose-motion-language-model-for-motion-generation-and-understanding
### Abstract
Generating lifelike human motions from descriptive texts has experienced
remarkable research focus in the recent years, propelled by the emerging
requirements of digital humans.Despite impressive advances, existing approaches
are often constrained by limited control modalities, task specificity, and
focus solely on body motion representations.In this paper, we present
MotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these
limitations. MotionGPT-2 accommodates multiple motion-relevant tasks and
supporting multimodal control conditions through pre-trained Large Language
Models (LLMs). It quantizes multimodal inputs-such as text and single-frame
poses-into discrete, LLM-interpretable tokens, seamlessly integrating them into
the LLM's vocabulary. These tokens are then organized into unified prompts,
guiding the LLM to generate motion outputs through a
pretraining-then-finetuning paradigm. We also show that the proposed
MotionGPT-2 is highly adaptable to the challenging 3D holistic motion
generation task, enabled by the innovative motion discretization framework,
Part-Aware VQVAE, which ensures fine-grained representations of body and hand
movements. Extensive experiments and visualizations validate the effectiveness
of our method, demonstrating the adaptability of MotionGPT-2 across motion
generation, motion captioning, and generalized motion completion tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MotionGPT-2ï¼šé€šç”¨è¿åŠ¨-è¯­è¨€æ¨¡å‹ï¼Œå®ç°è¿åŠ¨ç”Ÿæˆä¸ç†è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œä»æè¿°æ€§æ–‡æœ¬ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™ä¸»è¦å¾—ç›Šäºæ•°å­—äººç±»çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸å—åˆ°ä»¥ä¸‹é™åˆ¶ï¼š
1. **æ§åˆ¶æ–¹å¼çš„å±€é™æ€§**ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸åªé’ˆå¯¹å•ä¸€ç±»å‹çš„æ§åˆ¶æ¡ä»¶ï¼Œä¾‹å¦‚æ–‡æœ¬æè¿°æˆ–å¤šå¸§å§¿æ€ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦åŒæ—¶ç”ŸæˆåŸºäºæ–‡æœ¬æè¿°å’Œå¤šå…³é”®å¸§äººä½“å§¿æ€çš„è¿åŠ¨åºåˆ—åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
2. **ç¼ºä¹é€šç”¨ä¸–ç•ŒçŸ¥è¯†çš„ä»»åŠ¡ç‰¹å®šæ¡†æ¶**ï¼šç°æœ‰çš„æ¨¡å‹å¾€å¾€æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¾‹å¦‚åŸºäºæ‰©æ•£å’ŒGPTçš„æ¡†æ¶ï¼Œç¼ºä¹é€‚åº”å¤šç§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æ— æ³•å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­åµŒå…¥çš„ä¸–ç•ŒçŸ¥è¯†ã€‚
3. **ä»…å…³æ³¨èº«ä½“è¿åŠ¨è¡¨ç¤º**ï¼šç°æœ‰çš„åŸºäºæ–‡æœ¬çš„è¿åŠ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨ç”Ÿæˆä»…èº«ä½“è¿åŠ¨ï¼Œè€Œä¸æ˜¯æ•´ä½“è¿åŠ¨ï¼Œè¿™åœ¨æŸäº›åœºæ™¯ä¸‹ï¼ˆä¾‹å¦‚ä½“è‚²æ´»åŠ¨å’Œæ¼”å¥ä¹å™¨ï¼‰çš„åˆç†æ€§å’Œè¡¨ç°åŠ›ä»ç„¶ä¸å°½å¦‚äººæ„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†MotionGPT-2ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é€šç”¨è¿åŠ¨-è¯­è¨€æ¨¡å‹ï¼ˆLMLMï¼‰ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§æ§åˆ¶ä¿¡å·ï¼Œæ‰§è¡Œå„ç§ä¸è¿åŠ¨ç›¸å…³çš„ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆæ•´ä½“äººç±»è¿åŠ¨ã€‚å…¶å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
1. **å°†å¤šæ¨¡æ€æ§åˆ¶ä¿¡å·è½¬åŒ–ä¸ºç»Ÿä¸€è¡¨ç¤º**ï¼šMotionGPT-2è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶å’Œä»»åŠ¡æ„ŸçŸ¥æç¤ºï¼Œç”¨äºäººç±»è¿åŠ¨åˆæˆã€‚è¯¥æ¡†æ¶å…è®¸åœ¨å¤šæ¨¡æ€æ§åˆ¶æ¡ä»¶ä¸‹ç”Ÿæˆäººç±»è¿åŠ¨ï¼Œå¹¶é€šè¿‡ä»»åŠ¡æ„ŸçŸ¥æç¤ºé€‚åº”ç‰¹å®šçš„è¿åŠ¨ç›¸å…³ä»»åŠ¡ã€‚
2. **æ„å»ºå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„ä»»åŠ¡æ— å…³æ¡†æ¶**ï¼šMotionGPT-2é€šè¿‡ä½¿ç”¨LLMsæ¥å…±åŒè¡¨ç¤ºè¿åŠ¨å’Œè¯­è¨€ï¼Œæ”¹è¿›äº†MotionGPTã€‚é¦–å…ˆï¼Œå°†äººç±»è¿åŠ¨åµŒå…¥åˆ°ç¦»æ•£çš„è¿åŠ¨æ ‡è®°ä¸­ï¼Œç„¶åä½¿ç”¨è¿™äº›è¿åŠ¨æ ‡è®°æ‰©å±•LLMçš„è¯æ±‡è¡¨ï¼Œåˆ›å»ºä¸€ä¸ªä¸°å¯Œçš„è¿åŠ¨-è¯­è¨€è¯æ±‡è¡¨ã€‚é€šè¿‡å°†äººç±»è¿åŠ¨å’Œè¯­è¨€çº³å…¥ç»Ÿä¸€çš„è¯æ±‡è¡¨ï¼Œè¿åŠ¨å’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³ç³»å˜å¾—é€æ˜ã€‚è¿›ä¸€æ­¥åœ°ï¼ŒMotionGPT-2å°†æ¥è‡ªè¯­è¨€å’Œè¿åŠ¨æç¤ºçš„æ ‡è®°ç»“åˆèµ·æ¥ç”ŸæˆæŒ‡ä»¤ï¼Œå¹¶å®ç°äº†ä¸€ç§å¤šæ¨¡æ€é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•æ¥æœ‰æ•ˆåœ°è®­ç»ƒMotionGPT-2ã€‚
3. **å®ç°æ•´ä½“äººç±»è¿åŠ¨çš„ç²¾ç¡®ç¦»æ•£è¡¨ç¤º**ï¼šä¸ºäº†è§£å†³æ•´ä½“äººç±»è¿åŠ¨ç”Ÿæˆä»»åŠ¡ï¼Œæœ¬æ–‡å¼•å…¥äº†è¿åŠ¨ç¦»æ•£åŒ–æ¡†æ¶Part-Aware VQVAEï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸¤ä¸ªçº§åˆ«çš„ç¦»æ•£ç æœ¬å’Œè¿åŠ¨ç¼–ç å™¨æ¥å­¦ä¹ èº«ä½“å’Œæ‰‹çš„è¡¨ç¤ºã€‚è¿™ç§ä¸¤çº§çš„ç¦»æ•£åŒ–æ¡†æ¶æ•è·äº†ç»†å¾®çš„æ‰‹éƒ¨è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒäº†æ•´ä½“çš„èº«ä½“åŠ¨åŠ›å­¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨HumanML3Dã€KIT-MLå’ŒMotion-Xæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒMotionGPT-2åœ¨å¤šä¸ªè¿åŠ¨ç›¸å…³ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„å…·æœ‰ç‰¹å®šæ¡†æ¶çš„æ¨¡å‹ç›¸æ¯”ï¼ŒMotionGPT-2ä»…ä½¿ç”¨1%çš„é¢å¤–å‚æ•°å°±å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´æ˜¾è‘—å‡å°‘åˆ°å…¶ä»–æ–¹æ³•çš„10%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MotionGPT-2ä¸ºè¿åŠ¨ç”Ÿæˆå’Œç†è§£é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå…¶å¤šæ¨¡æ€æ§åˆ¶ã€ä»»åŠ¡æ— å…³æ¡†æ¶å’Œç²¾ç¡®çš„ç¦»æ•£è¡¨ç¤ºæ–¹æ³•ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å‡ºè‰²è¡¨ç°è¡¨æ˜ï¼ŒLLMsåœ¨è¿åŠ¨ç”Ÿæˆå’Œç†è§£æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## janus--decoupling-visual-encoding-for-unified-multimodal-understanding-and-generation
### Abstract
In this paper, we introduce Janus, an autoregressive framework that unifies
multimodal understanding and generation. Prior research often relies on a
single visual encoder for both tasks, such as Chameleon. However, due to the
differing levels of information granularity required by multimodal
understanding and generation, this approach can lead to suboptimal performance,
particularly in multimodal understanding. To address this issue, we decouple
visual encoding into separate pathways, while still leveraging a single,
unified transformer architecture for processing. The decoupling not only
alleviates the conflict between the visual encoder's roles in understanding and
generation, but also enhances the framework's flexibility. For instance, both
the multimodal understanding and generation components can independently select
their most suitable encoding methods. Experiments show that Janus surpasses
previous unified model and matches or exceeds the performance of task-specific
models. The simplicity, high flexibility, and effectiveness of Janus make it a
strong candidate for next-generation unified multimodal models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Janusï¼šè§£è€¦è§†è§‰ç¼–ç ï¼Œç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹å¾€å¾€ä½¿ç”¨å•ä¸€çš„è§†è§‰ç¼–ç å™¨æ¥å¤„ç†ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºJanusï¼Œä¸€ä¸ªè§£è€¦è§†è§‰ç¼–ç çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§£è€¦è§†è§‰ç¼–ç 
Januså¼•å…¥ä¸¤ä¸ªç‹¬ç«‹çš„è§†è§‰ç¼–ç è·¯å¾„ï¼šä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£ï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç”Ÿæˆï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„Transformeræ¶æ„è¿›è¡Œå¤„ç†ã€‚è¿™ç§è§£è€¦è®¾è®¡å¯ä»¥ç¼“è§£è§†è§‰ç¼–ç å™¨åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„å†²çªï¼Œå¹¶æé«˜æ¡†æ¶çš„çµæ´»æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçµæ´»æ€§å’Œå¯æ‰©å±•æ€§
Januså…è®¸ç†è§£å’Œç”Ÿæˆä»»åŠ¡ç‹¬ç«‹é€‰æ‹©æœ€åˆé€‚çš„ç¼–ç æ–¹æ³•ï¼Œå¹¶å¯ä»¥è½»æ¾æ‰©å±•ä»¥æ”¯æŒæ›´å¤šè¾“å…¥æ¨¡æ€ï¼Œå¦‚ç‚¹äº‘ã€è„‘ç”µå›¾ä¿¡å·æˆ–éŸ³é¢‘æ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Janusåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¹¶åœ¨æŸäº›æ–¹é¢ç”šè‡³è¶…è¿‡äº†ç‰¹å®šä»»åŠ¡æ¨¡å‹ã€‚åœ¨å¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusåœ¨MMBenchã€SEED-Benchå’ŒPOPEä¸Šåˆ†åˆ«å–å¾—äº†69.4ã€63.7å’Œ87.0çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†LLaVA-v1.5å’ŒQwen-VL-Chatç­‰æ¨¡å‹ã€‚åœ¨è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusåœ¨MSCOCO-30Kå’ŒGenEvalä¸Šåˆ†åˆ«å–å¾—äº†8.53çš„FIDåˆ†æ•°å’Œ61%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†DALL-E 2å’ŒSDXLç­‰æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Janusçš„è®¾è®¡ç†å¿µä¸ºå¼€å‘ä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶è§£è€¦è§†è§‰ç¼–ç çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„å†²çªï¼Œå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒJanusçš„æ¶æ„ç®€å•ã€æ˜“äºæ‰©å±•ï¼Œå¯ä»¥è½»æ¾åœ°é›†æˆæ›´å¤šè¾“å…¥æ¨¡æ€ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„å¤šæ¨¡æ€é€šç”¨æ¨¡å‹ã€‚

## puma--empowering-unified-mllm-with-multi-granular-visual-generation
### Abstract
Recent advancements in multimodal foundation models have yielded significant
progress in vision-language understanding. Initial attempts have also explored
the potential of multimodal large language models (MLLMs) for visual content
generation. However, existing works have insufficiently addressed the varying
granularity demands of different image generation tasks within a unified MLLM
paradigm - from the diversity required in text-to-image generation to the
precise controllability needed in image manipulation. In this work, we propose
PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA
unifies multi-granular visual features as both inputs and outputs of MLLMs,
elegantly addressing the different granularity requirements of various image
generation tasks within a unified MLLM framework. Following multimodal
pretraining and task-specific instruction tuning, PUMA demonstrates proficiency
in a wide range of multimodal tasks. This work represents a significant step
towards a truly unified MLLM capable of adapting to the granularity demands of
various visual tasks. The code and model will be released in
https://github.com/rongyaofang/PUMA.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PUMAï¼šå¤šç²’åº¦è§†è§‰ç”Ÿæˆèµ‹èƒ½ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è§†è§‰-è¯­è¨€ç†è§£æ–¹é¢çš„æ˜¾è‘—è¿›æ­¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ä¹Ÿé€æ¸è¢«æ¢ç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œåœ¨ç»Ÿä¸€MLLMæ¡†æ¶å†…å¤„ç†ä¸åŒå›¾åƒç”Ÿæˆä»»åŠ¡æ—¶ï¼Œå¯¹ç²’åº¦éœ€æ±‚çš„å·®å¼‚è€ƒè™‘ä¸è¶³ã€‚ä¾‹å¦‚ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆéœ€è¦å¤šæ ·æ€§ï¼Œè€Œå›¾åƒç¼–è¾‘åˆ™éœ€è¦ç²¾ç¡®çš„å¯æ§æ€§ã€‚ç°æœ‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºä»è§†è§‰ç¼–ç å™¨ä¸­æå–çš„å•ç²’åº¦ç‰¹å¾ï¼Œå¿½ç•¥äº†ä¸åŒä»»åŠ¡å¯¹ç²’åº¦éœ€æ±‚çš„å·®å¼‚ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PUMAï¼ˆemPowering Unified MLLM with Multi-grAnular visual generationï¼‰ï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šç²’åº¦MLLMï¼Œå®ƒå°†å¤šç²’åº¦è§†è§‰ç‰¹å¾ä½œä¸ºMLLMçš„è¾“å…¥å’Œè¾“å‡ºï¼Œä¼˜é›…åœ°è§£å†³äº†ä¸åŒå›¾åƒç”Ÿæˆä»»åŠ¡åœ¨ç»Ÿä¸€MLLMæ¡†æ¶å†…çš„ç²’åº¦éœ€æ±‚å·®å¼‚ã€‚

PUMAçš„ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç²’åº¦è§†è§‰ç¼–ç å’Œè§£ç 
PUMAä½¿ç”¨è¯­ä¹‰å›¾åƒç¼–ç å™¨æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ä¸€ç»„ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºè§£ç å™¨ï¼Œä»¥å¤„ç†ä¸åŒç²’åº¦çš„ç‰¹å¾ã€‚è¿™ç§è®¾è®¡ä½¿å¾—PUMAèƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°ç”Ÿæˆå¤šæ ·æ€§ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å„ç§è§†è§‰ä»»åŠ¡çš„éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç²’åº¦è‡ªå›å½’MLLM
PUMAè®¾è®¡äº†ä¸€ä¸ªè‡ªå›å½’MLLMï¼Œå®ƒå¯ä»¥å¤„ç†å’Œç”Ÿæˆæ–‡æœ¬æ ‡è®°ä»¥åŠå¤šç²’åº¦å›¾åƒç‰¹å¾ã€‚è¯¥æ¨¡å‹é€šè¿‡é€æ­¥é¢„æµ‹æ¯ä¸ªç²’åº¦çº§åˆ«çš„æ¯ä¸ªæ ‡è®°ï¼Œå¹¶ä»æœ€ç²—ç²’åº¦çº§åˆ«åˆ°æœ€ç»†ç²’åº¦çº§åˆ«è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œèƒ½å¤Ÿæ•è·ä¸åŒå°ºåº¦ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
PUMAåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š

* **å¤šæ ·åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ**ï¼šPUMAèƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æ¡ä»¶ç›¸å¯¹åº”çš„å¤šæ ·åŒ–å›¾åƒã€‚
* **å›¾åƒç¼–è¾‘**ï¼šPUMAèƒ½å¤Ÿè¿›è¡Œç²¾ç¡®çš„å›¾åƒç¼–è¾‘ï¼Œä¾‹å¦‚æ·»åŠ ã€åˆ é™¤æˆ–æ›¿æ¢å›¾åƒä¸­çš„å…ƒç´ ã€‚
* **æ¡ä»¶å›¾åƒç”Ÿæˆ**ï¼šPUMAèƒ½å¤Ÿæ ¹æ®ç‰¹å®šæ¡ä»¶ç”Ÿæˆå›¾åƒï¼Œä¾‹å¦‚å°†Cannyè¾¹ç¼˜å›¾åƒè½¬æ¢ä¸ºè‡ªç„¶å›¾åƒã€‚
* **å›¾åƒç†è§£**ï¼šPUMAåœ¨å›¾åƒç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PUMAçš„å¤šç²’åº¦è§†è§‰ç”Ÿæˆæ¡†æ¶ä¸ºMLLMsåœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶å¤šç²’åº¦ç‰¹å¾ç¼–ç å’Œè§£ç çš„è®¾è®¡ï¼Œä»¥åŠå¤šç²’åº¦è‡ªå›å½’MLLMçš„æ¶æ„ï¼Œéƒ½å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢ã€‚æ­¤å¤–ï¼ŒPUMAåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ä¹Ÿè¡¨æ˜ï¼Œå¤šç²’åº¦æ–¹æ³•åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## mmar--towards-lossless-multi-modal-auto-regressive-probabilistic-modeling
### Abstract
Recent advancements in multi-modal large language models have propelled the
development of joint probabilistic models capable of both image understanding
and generation. However, we have identified that recent methods inevitably
suffer from loss of image information during understanding task, due to either
image discretization or diffusion denoising steps. To address this issue, we
propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling
framework. Unlike discretization line of method, MMAR takes in
continuous-valued image tokens to avoid information loss. Differing from
diffusion-based approaches, we disentangle the diffusion process from
auto-regressive backbone model by employing a light-weight diffusion head on
top each auto-regressed image patch embedding. In this way, when the model
transits from image generation to understanding through text generation, the
backbone model's hidden representation of the image is not limited to the last
denoising step. To successfully train our method, we also propose a
theoretically proven technique that addresses the numerical stability issue and
a training strategy that balances the generation and understanding task goals.
Through extensive evaluations on 18 image understanding benchmarks, MMAR
demonstrates much more superior performance than other joint multi-modal
models, matching the method that employs pretrained CLIP vision encoder,
meanwhile being able to generate high quality images at the same time. We also
showed that our method is scalable with larger data and model size.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MMARï¼šè¿ˆå‘æ— æŸå¤šæ¨¡æ€è‡ªå›å½’æ¦‚ç‡å»ºæ¨¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¦‚ç‡æ¨¡å‹åœ¨å›¾åƒç†è§£ä»»åŠ¡ä¸­ä¸å¯é¿å…åœ°ä¼šä¸¢å¤±å›¾åƒä¿¡æ¯ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå›¾åƒç¦»æ•£åŒ–æˆ–æ‰©æ•£å»å™ªæ­¥éª¤å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€è‡ªå›å½’ï¼ˆMMARï¼‰æ¦‚ç‡å»ºæ¨¡æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¿ç»­å›¾åƒè¡¨ç¤º
ä¸ç¦»æ•£åŒ–æ–¹æ³•ä¸åŒï¼ŒMMARé‡‡ç”¨è¿ç»­å€¼å›¾åƒæ ‡è®°ï¼Œé¿å…äº†ä¿¡æ¯ä¸¢å¤±ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„æ‰©æ•£å¤´ï¼Œå°†æ‰©æ•£è¿‡ç¨‹ä¸è‡ªå›å½’éª¨å¹²æ¨¡å‹è§£è€¦ï¼Œä»è€Œåœ¨å›¾åƒç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸­å……åˆ†åˆ©ç”¨å›¾åƒå»ºæ¨¡èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½ç²¾åº¦è®­ç»ƒä¸‹çš„æ•°å€¼ç¨³å®šæ€§
é’ˆå¯¹ä½ç²¾åº¦è®­ç»ƒè®¾ç½®ä¸‹æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ•°å€¼ç²¾åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç†è®ºä¸Šè¯æ˜çš„æŠ€æœ¯ï¼Œé€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹å‚æ•°åŒ–æ¥æœ€å°åŒ–æ•°å€¼è¯¯å·®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¹³è¡¡ç”Ÿæˆå’Œç†è§£ä»»åŠ¡çš„è®­ç»ƒç­–ç•¥
ä¸ºäº†å¹³è¡¡å›¾åƒç”Ÿæˆå’Œç†è§£ä»»åŠ¡çš„ç›®æ ‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§è§„æ¨¡ã€ä¸­ç­‰è´¨é‡çš„æ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„æ•°æ®åˆ†å¸ƒå¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®æ¥è¿›ä¸€æ­¥æé«˜å›¾åƒç”Ÿæˆèƒ½åŠ›å¹¶ç»†åŒ–æ¨¡å‹å¯¹å›¾åƒçš„ç†è§£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨18ä¸ªå›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMMARè¡¨ç°å‡ºæ¯”å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¸ä½¿ç”¨é¢„è®­ç»ƒCLIPè§†è§‰ç¼–ç å™¨çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒMMARæ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œéšç€æ•°æ®é‡å’Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½ä¹Ÿä¼šå¾—åˆ°æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MMARæ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¦‚ç‡å»ºæ¨¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡è¿ç»­å›¾åƒè¡¨ç¤ºå’Œä½ç²¾åº¦è®­ç»ƒä¸‹çš„æ•°å€¼ç¨³å®šæ€§æŠ€æœ¯ï¼Œå®ç°äº†æ— æŸçš„å¤šæ¨¡æ€è‡ªå›å½’æ¦‚ç‡å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä¸ºå¹³è¡¡ç”Ÿæˆå’Œç†è§£ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›åˆ›æ–°ç‚¹å¯¹äºå¤šæ¨¡æ€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## looking-at-ctr-prediction-again--is-attention-all-you-need-
### Abstract
Click-through rate (CTR) prediction is a critical problem in web search,
recommendation systems and online advertisement displaying. Learning good
feature interactions is essential to reflect user's preferences to items. Many
CTR prediction models based on deep learning have been proposed, but
researchers usually only pay attention to whether state-of-the-art performance
is achieved, and ignore whether the entire framework is reasonable. In this
work, we use the discrete choice model in economics to redefine the CTR
prediction problem, and propose a general neural network framework built on
self-attention mechanism. It is found that most existing CTR prediction models
align with our proposed general framework. We also examine the expressive power
and model complexity of our proposed framework, along with potential extensions
to some existing models. And finally we demonstrate and verify our insights
through some experimental results on public datasets.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é‡æ–°å®¡è§†ç‚¹å‡»ç‡é¢„æµ‹ï¼šæ³¨æ„åŠ›æœºåˆ¶çœŸçš„è¶³å¤Ÿå—ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç‚¹å‡»ç‡ï¼ˆCTRï¼‰é¢„æµ‹æ˜¯ç½‘ç»œæœç´¢ã€æ¨èç³»ç»Ÿå’Œåœ¨çº¿å¹¿å‘Šæ˜¾ç¤ºä¸­çš„å…³é”®é—®é¢˜ã€‚å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾äº¤äº’å¯¹äºåæ˜ ç”¨æˆ·å¯¹ç‰©å“çš„åå¥½è‡³å…³é‡è¦ã€‚è®¸å¤šåŸºäºæ·±åº¦å­¦ä¹ çš„CTRé¢„æµ‹æ¨¡å‹å·²è¢«æå‡ºï¼Œä½†ç ”ç©¶äººå‘˜é€šå¸¸åªå…³æ³¨æ˜¯å¦è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†æ•´ä¸ªæ¡†æ¶æ˜¯å¦åˆç†ã€‚æœ¬æ–‡ä½¿ç”¨ç»æµå­¦ä¸­çš„ç¦»æ•£é€‰æ‹©æ¨¡å‹é‡æ–°å®šä¹‰äº†CTRé¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡ä½¿ç”¨ç»æµå­¦ä¸­çš„ç¦»æ•£é€‰æ‹©æ¨¡å‹é‡æ–°å®šä¹‰äº†CTRé¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åµŒå…¥å±‚ã€ç‰¹å¾äº¤äº’å±‚ã€èšåˆå±‚å’Œç©ºé—´å˜æ¢å±‚ï¼Œå¯ä»¥æ¶µç›–å¤§å¤šæ•°ç°æœ‰çš„CTRé¢„æµ‹æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾äº¤äº’å½¢å¼ï¼Œå¯ä»¥åŒ…å«å¤§å¤šæ•°ç°æœ‰CTRé¢„æµ‹æ¨¡å‹çš„ç‰¹å¾å¤„ç†åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†è¯¥æ¡†æ¶ä¸­ç‰¹å¾äº¤äº’ç®—å­çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†æ‰©å±•å…ˆå‰æ¨¡å‹çš„æ–¹æ³•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸å¤§å¤šæ•°ç°æœ‰çš„CTRæ¨¡å‹ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºå®ƒä»¬ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ä¸ºCTRé¢„æµ‹é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾äº¤äº’å½¢å¼å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æ¨èç³»ç»Ÿã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚

## mio--a-foundation-model-on-multimodal-tokens
### Abstract
In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MIOï¼šåŸºäºå¤šæ¨¡æ€tokençš„é€šç”¨æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMM-LLMsï¼‰çš„å…´èµ·ï¼Œäººå·¥æ™ºèƒ½åœ¨é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MM-LLMsä»ç„¶ç¼ºä¹çœŸæ­£çš„â€œä»»æ„åˆ°ä»»æ„â€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå³æ— æ³•åœ¨å¤šç§æ¨¡æ€ä¹‹é—´è¿›è¡Œæ— ç¼è½¬æ¢å’Œç”Ÿæˆã€‚ä¾‹å¦‚ï¼ŒGPT-4oè™½ç„¶å±•ç¤ºäº†å¼ºå¤§çš„â€œä»»æ„åˆ°ä»»æ„â€LLMsæ½œåŠ›ï¼Œä½†å®ƒä¸æ”¯æŒå¤šæ¨¡æ€äº¤é”™åºåˆ—çš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MIOï¼Œä¸€ä¸ªåŸºäºå¤šæ¨¡æ€tokençš„é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè¯­éŸ³ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€äº¤é”™åºåˆ—çš„ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€tokenization
MIOä½¿ç”¨SEED-Tokenizerå¯¹å›¾åƒè¿›è¡Œtokenizationï¼Œä½¿ç”¨SpeechTokenizerå¯¹è¯­éŸ³è¿›è¡Œtokenizationï¼Œå¹¶å°†è¿™äº›tokenæ·»åŠ åˆ°LLMçš„è¯æ±‡è¡¨ä¸­ã€‚ç”±äºå›¾åƒtokenå’Œè¯­éŸ³tokenéƒ½åŒ…å«å› æœè¯­ä¹‰ï¼Œå› æ­¤å¯ä»¥åƒæ–‡æœ¬tokenä¸€æ ·è¿›è¡Œè‡ªå›å½’è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå› æœå¤šæ¨¡æ€å»ºæ¨¡
MIOä½¿ç”¨å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausalLMï¼‰è¿›è¡Œè®­ç»ƒï¼Œå°†å¤šæ¨¡æ€tokençš„ç”Ÿæˆç›®æ ‡ç»Ÿä¸€ä¸ºä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€äº¤é”™åºåˆ—ç”Ÿæˆ
MIOæ”¯æŒå¤šæ¨¡æ€äº¤é”™åºåˆ—çš„ç”Ÿæˆï¼Œä¾‹å¦‚è§†é¢‘-æ–‡æœ¬äº¤é”™åºåˆ—ï¼Œè¿™ä½¿å¾—MIOèƒ½å¤Ÿè¿›è¡Œè§†è§‰æ•…äº‹è®²è¿°ã€è§†è§‰æ€ç»´é“¾æ¨ç†ç­‰é«˜çº§ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå››é˜¶æ®µè®­ç»ƒè¿‡ç¨‹
MIOé‡‡ç”¨å››é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. å¯¹é½é¢„è®­ç»ƒï¼šå­¦ä¹ ä¸è¯­è¨€ç©ºé—´æ›´å¯¹é½çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚
2. äº¤é”™é¢„è®­ç»ƒï¼šè·å¾—å…·æœ‰æ›´ä¸°å¯Œä¸Šä¸‹æ–‡è¯­ä¹‰çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚
3. è¯­éŸ³å¢å¼ºé¢„è®­ç»ƒï¼šå¢å¼ºæ¨¡å‹çš„è¯­éŸ³ç›¸å…³èƒ½åŠ›ã€‚
4. ç»¼åˆç›‘ç£å¾®è°ƒï¼šåœ¨å¤šç§æ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
MIOåœ¨å›¾åƒç†è§£ã€å›¾åƒç”Ÿæˆã€è¯­éŸ³ç†è§£å’Œè¯­éŸ³ç”Ÿæˆã€è§†é¢‘ç†è§£å’Œè§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰åŒæ¨¡æ€æ¨¡å‹å’Œâ€œä»»æ„åˆ°ä»»æ„â€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MIOçš„è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ä¸ºæ„å»ºé€šç”¨å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶å¤šæ¨¡æ€äº¤é”™åºåˆ—ç”Ÿæˆèƒ½åŠ›ä¹Ÿä¸ºè§†è§‰æ•…äº‹è®²è¿°ã€è§†è§‰æ€ç»´é“¾æ¨ç†ç­‰é«˜çº§ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## monoformer--one-transformer-for-both-diffusion-and-autoregression
### Abstract
Most existing multimodality methods use separate backbones for
autoregression-based discrete text generation and diffusion-based continuous
visual generation, or the same backbone by discretizing the visual data to use
autoregression for both text and visual generation. In this paper, we propose
to study a simple idea: share one transformer for both autoregression and
diffusion. The feasibility comes from two main aspects: (i) Transformer is
successfully applied to diffusion for visual generation, and (ii) transformer
training for autoregression and diffusion is very similar, and the difference
merely lies in that diffusion uses bidirectional attention mask and
autoregression uses causal attention mask. Experimental results show that our
approach achieves comparable image generation performance to current
state-of-the-art methods as well as maintains the text generation capability.
The project is publicly available at https://monoformer.github.io/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MonoFormerï¼šä¸€ç®­åŒé›•ï¼ŒTransformeråŒæ—¶å¤„ç†æ‰©æ•£å’Œè‡ªå›å½’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„å¤šæ¨¡æ€æ–¹æ³•é€šå¸¸ä¸ºåŸºäºè‡ªå›å½’çš„ç¦»æ•£æ–‡æœ¬ç”Ÿæˆå’ŒåŸºäºæ‰©æ•£çš„è¿ç»­è§†è§‰ç”Ÿæˆä½¿ç”¨ç‹¬ç«‹çš„éª¨å¹²ç½‘ç»œï¼Œæˆ–è€…é€šè¿‡å°†è§†è§‰æ•°æ®ç¦»æ•£åŒ–æ¥ä½¿ç”¨è‡ªå›å½’è¿›è¡Œæ–‡æœ¬å’Œè§†è§‰ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”éš¾ä»¥å®ç°æ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å…±äº«ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
MonoFormer æå‡ºäº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æƒ³æ³•ï¼šå…±äº«ä¸€ä¸ª Transformer è¿›è¡Œè‡ªå›å½’å’Œæ‰©æ•£ã€‚å…¶å¯è¡Œæ€§ä¸»è¦åŸºäºä»¥ä¸‹ä¸¤ç‚¹ï¼š
1. Transformer å·²æˆåŠŸåº”ç”¨äºæ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰ç”Ÿæˆã€‚
2. Transformer åœ¨è‡ªå›å½’å’Œæ‰©æ•£è®­ç»ƒä¸­çš„è¿‡ç¨‹éå¸¸ç›¸ä¼¼ï¼Œä¸»è¦åŒºåˆ«åœ¨äºæ‰©æ•£ä½¿ç”¨åŒå‘æ³¨æ„åŠ›æ©ç ï¼Œè€Œè‡ªå›å½’ä½¿ç”¨å› æœæ³¨æ„åŠ›æ©ç ã€‚

MonoFormer ä½¿ç”¨ä¸€ä¸ªå…±äº«çš„ Transformer è¿›è¡Œæ–‡æœ¬ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆï¼ŒTransformer æ¥æ”¶æ–‡æœ¬ token åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€ä¸ªé¢„æµ‹è¾“å‡º tokenã€‚å¯¹äºå›¾åƒç”Ÿæˆï¼ŒTransformer æ¥æ”¶å™ªå£°åŒ–çš„æ½œåœ¨åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹å™ªå£°ï¼Œç„¶åé€šè¿‡ VAE è§£ç å™¨ç”Ÿæˆå›¾åƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ ImageNet æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMonoFormer åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢ï¼ŒMonoFormer çš„æ€§èƒ½ç•¥ä½äº TinyLlama åŸºçº¿æ¨¡å‹ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ··åˆè®­ç»ƒå›¾åƒç”Ÿæˆæ•°æ®é›†å¯¼è‡´çš„ã€‚æœªæ¥å¯ä»¥é€šè¿‡å¢åŠ æ›´å¤šè¯­è¨€æ•°æ®æ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MonoFormer çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºå…±äº«ä¸€ä¸ª Transformer è¿›è¡Œè‡ªå›å½’å’Œæ‰©æ•£ï¼Œä»è€Œå®ç°äº†æ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å…±äº«ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒMonoFormer è¿˜é‡‡ç”¨äº†é¢„è®­ç»ƒçš„ LLM è¿›è¡Œ Transformer åˆå§‹åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›æ–¹æ³•å¯ä»¥ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„æ„å»ºæä¾›æ–°çš„æ€è·¯ã€‚

## vila-u--a-unified-foundation-model-integrating-visual-understanding-and-generation
### Abstract
VILA-U is a Unified foundation model that integrates Video, Image, Language
understanding and generation. Traditional visual language models (VLMs) use
separate modules for understanding and generating visual content, which can
lead to misalignment and increased complexity. In contrast, VILA-U employs a
single autoregressive next-token prediction framework for both tasks,
eliminating the need for additional components like diffusion models. This
approach not only simplifies the model but also achieves near state-of-the-art
performance in visual language understanding and generation. The success of
VILA-U is attributed to two main factors: the unified vision tower that aligns
discrete visual tokens with textual inputs during pretraining, which enhances
visual perception, and autoregressive image generation can achieve similar
quality as diffusion models with high-quality dataset. This allows VILA-U to
perform comparably to more complex models using a fully token-based
autoregressive framework.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VILA-Uï¼šç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆçš„åŸºç¡€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šå¸¸ä½¿ç”¨ç‹¬ç«‹çš„æ¨¡å—æ¥ç†è§£å’Œç”Ÿæˆè§†è§‰å†…å®¹ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å—ä¹‹é—´çš„ä¸åŒ¹é…å’Œæ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VILA-Uï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†è§†é¢‘ã€å›¾åƒå’Œè¯­è¨€çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ•´åˆåˆ°ä¸€ä¸ªè‡ªå›å½’çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¡†æ¶ä¸­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€çš„è§†è§‰å¡”
VILA-Ué‡‡ç”¨ç»Ÿä¸€çš„è§†è§‰å¡”ï¼Œé€šè¿‡å‘é‡é‡åŒ–å°†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œå¹¶ä¸æ–‡æœ¬è¾“å…¥è¿›è¡Œå¯¹é½ã€‚è¿™ç§å¯¹é½åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œï¼Œå¯ä»¥å¢å¼ºè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªå›å½’å›¾åƒç”Ÿæˆ
VILA-Uä½¿ç”¨è‡ªå›å½’æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡åœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè‡ªå›å½’å›¾åƒç”Ÿæˆå¯ä»¥è¾¾åˆ°ä¸æ‰©æ•£æ¨¡å‹ç›¸ä¼¼çš„è´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
VILA-Uåœ¨è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨è§†è§‰ç†è§£æ–¹é¢ï¼ŒVILA-Uçš„æ€§èƒ½æ¥è¿‘äºä½¿ç”¨è¿ç»­è§†è§‰æ ‡è®°çš„é¢†å…ˆVLMsï¼Œç”šè‡³åœ¨ç›¸åŒçš„LLMå¤§å°ä¸‹ï¼Œæ€§èƒ½è¶…è¿‡äº†è®¸å¤šæ–¹æ³•ã€‚åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢ï¼ŒVILA-Uçš„FIDå¾—åˆ†ä¼˜äºå…¶ä»–è‡ªå›å½’æ–¹æ³•ï¼Œå¹¶ä¸”ä¸ä¸€äº›åŸºäºæ‰©æ•£çš„æ–¹æ³•å…·æœ‰å¯æ¯”çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
VILA-Uçš„ç»Ÿä¸€æ¡†æ¶å¯ä»¥ä½œä¸ºä¸€ä¸ªé€šç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒVILA-Uçš„è§†è§‰å¡”å’Œè‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œæ–‡æœ¬çš„ç†è§£å’Œç”Ÿæˆã€‚

## show-o--one-single-transformer-to-unify-multimodal-understanding-and-generation
### Abstract
We present a unified transformer, i.e., Show-o, that unifies multimodal
understanding and generation. Unlike fully autoregressive models, Show-o
unifies autoregressive and (discrete) diffusion modeling to adaptively handle
inputs and outputs of various and mixed modalities. The unified model flexibly
supports a wide range of vision-language tasks including visual
question-answering, text-to-image generation, text-guided
inpainting/extrapolation, and mixed-modality generation. Across various
benchmarks, it demonstrates comparable or superior performance to existing
individual models with an equivalent or larger number of parameters tailored
for understanding or generation. This significantly highlights its potential as
a next-generation foundation model. Code and models are released at
https://github.com/showlab/Show-o.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Show-oï¼šä¸€ä¸ªTransformerç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€æ™ºèƒ½çš„ä¸¤ä¸ªå…³é”®æ”¯æŸ±â€”â€”ç†è§£å’Œç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å¤šæ¨¡æ€ç†è§£æ–¹é¢ï¼Œå¦‚LLaVAç­‰æ¨¡å‹åœ¨è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›è§†è§‰ç”Ÿæˆæ–¹é¢ï¼Œå¦‚DDPMsç­‰æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹å¤§å¤šå°†æ¯ä¸ªé¢†åŸŸç‹¬ç«‹å¤„ç†ï¼Œç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¥åŒæ—¶å¤„ç†è¿™ä¸¤ä¸ªä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€Transformeræ¶æ„
Show-oæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„Transformeræ¶æ„ï¼Œå°†è‡ªå›å½’å’Œæ‰©æ•£å»ºæ¨¡ç›¸ç»“åˆï¼Œä»¥é€‚åº”ä¸åŒå’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚è¿™ä½¿å¾—Show-oèƒ½å¤Ÿçµæ´»åœ°æ”¯æŒå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„ä¿®å¤/å¤–æ¨å’Œæ··åˆæ¨¡æ€ç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šOmni-Attentionæœºåˆ¶
Show-oå¼•å…¥äº†Omni-Attentionæœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å› æœå’Œå…¨æ³¨æ„åŠ›æœºåˆ¶çš„å…¨é¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥åºåˆ—çš„æ ¼å¼è‡ªé€‚åº”åœ°æ··åˆå’Œå˜åŒ–ã€‚è¿™ä½¿å¾—Show-oèƒ½å¤Ÿä»¥ä¸åŒçš„æ–¹å¼å¤„ç†æ–‡æœ¬å’Œå›¾åƒä¿¡å·ï¼Œä»è€Œæé«˜æ¨¡å‹æ•ˆç‡å’Œä¸‹æ¸¸åº”ç”¨çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç¦»æ•£æ‰©æ•£å»ºæ¨¡
Show-oé‡‡ç”¨ç¦»æ•£æ‰©æ•£å»ºæ¨¡æ¥å¤„ç†å›¾åƒç”Ÿæˆï¼Œè€Œä¸æ˜¯è¿ç»­æ‰©æ•£ã€‚è¿™ä½¿å¾—Show-oèƒ½å¤Ÿä»¥æ›´å°‘çš„é‡‡æ ·æ­¥éª¤ç”Ÿæˆå›¾åƒï¼Œä»è€Œæé«˜ç”Ÿæˆæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒShow-oåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºä¸ç°æœ‰æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨MSCOCOæ•°æ®é›†ä¸Šï¼ŒShow-oçš„FIDåˆ†æ•°ä¸º9.24ï¼Œä¼˜äºè®¸å¤šå‚æ•°è§„æ¨¡æ›´å¤§çš„ç”Ÿæˆæ¨¡å‹ã€‚åœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­ï¼ŒShow-oåœ¨æ‰€æœ‰å…­ä¸ªæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—ä¼˜äºLDMç­‰æ¨¡å‹çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Show-oçš„æˆåŠŸè¡¨æ˜ï¼Œå°†è‡ªå›å½’å’Œæ‰©æ•£å»ºæ¨¡ç›¸ç»“åˆçš„ç»Ÿä¸€Transformeræ¶æ„å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚Omni-Attentionæœºåˆ¶å’Œç¦»æ•£æ‰©æ•£å»ºæ¨¡ç­‰æŠ€æœ¯ä¹Ÿä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆé¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒShow-oçš„å®éªŒç»“æœä¹Ÿä¸ºæœªæ¥ç»Ÿä¸€æ¨¡å‹çš„è®¾è®¡æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

### ğŸ“š å‚è€ƒèµ„æ–™
[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528)

## transfusion--predict-the-next-token-and-diffuse-images-with-one-multi-modal-model
### Abstract
We introduce Transfusion, a recipe for training a multi-modal model over
discrete and continuous data. Transfusion combines the language modeling loss
function (next token prediction) with diffusion to train a single transformer
over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B
parameters from scratch on a mixture of text and image data, establishing
scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our
experiments show that Transfusion scales significantly better than quantizing
images and training a language model over discrete image tokens. By introducing
modality-specific encoding and decoding layers, we can further improve the
performance of Transfusion models, and even compress each image to just 16
patches. We further demonstrate that scaling our Transfusion recipe to 7B
parameters and 2T multi-modal tokens produces a model that can generate images
and text on a par with similar scale diffusion models and language models,
reaping the benefits of both worlds.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Transfusionï¼šä¸€ç§é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°å¹¶æ‰©æ•£å›¾åƒçš„å¤šæ¨¡æ€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦èƒ½å¤Ÿæ„ŸçŸ¥ã€å¤„ç†å’Œç”Ÿæˆç¦»æ•£å…ƒç´ ï¼ˆå¦‚æ–‡æœ¬æˆ–ä»£ç ï¼‰å’Œè¿ç»­å…ƒç´ ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘æ•°æ®ï¼‰ã€‚è™½ç„¶åŸºäºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨ç¦»æ•£æ¨¡æ€ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æ‰©æ•£æ¨¡å‹åŠå…¶æ³›åŒ–å½¢å¼åœ¨ç”Ÿæˆè¿ç»­æ¨¡æ€æ–¹é¢å¤„äºæœ€å‰æ²¿ã€‚è®¸å¤šåŠªåŠ›æ—¨åœ¨ç»“åˆè¿™äº›æ–¹æ³•ï¼ŒåŒ…æ‹¬å°†è¯­è¨€æ¨¡å‹æ‰©å±•ä¸ºä½¿ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå·¥å…·ï¼Œæˆ–è€…å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å«æ¥åˆ°è¯­è¨€æ¨¡å‹ä¸Šã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯å°†è¿ç»­æ¨¡æ€é‡åŒ–ï¼Œå¹¶åœ¨ç¦»æ•£æ ‡è®°ä¸Šè®­ç»ƒæ ‡å‡†è¯­è¨€æ¨¡å‹ï¼Œä»è€Œç®€åŒ–æ¨¡å‹æ¶æ„ï¼Œä½†ä¼šä¸¢å¤±ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡è®­ç»ƒå•ä¸ªæ¨¡å‹æ¥é¢„æµ‹ç¦»æ•£æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£è¿ç»­å›¾åƒï¼Œä»è€Œå®Œå…¨æ•´åˆè¿™ä¸¤ç§æ¨¡æ€ï¼Œå¹¶ä¸”æ²¡æœ‰ä¿¡æ¯æŸå¤±çš„å¯èƒ½æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
Transfusion æ˜¯ä¸€ç§è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼åœ°ç”Ÿæˆç¦»æ•£å’Œè¿ç»­æ¨¡æ€ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ 50% æ–‡æœ¬å’Œ 50% å›¾åƒæ•°æ®ä¸Šé¢„è®­ç»ƒä¸€ä¸ª transformer æ¨¡å‹æ¥æ¼”ç¤º Transfusionï¼Œæ¯ä¸ªæ¨¡æ€ä½¿ç”¨ä¸åŒçš„ç›®æ ‡ï¼šæ–‡æœ¬ä½¿ç”¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œå›¾åƒä½¿ç”¨æ‰©æ•£ã€‚æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­éƒ½æš´éœ²äºä¸¤ç§æ¨¡æ€å’ŒæŸå¤±å‡½æ•°ã€‚æ ‡å‡†åµŒå…¥å±‚å°†æ–‡æœ¬æ ‡è®°è½¬æ¢ä¸ºå‘é‡ï¼Œè€Œåˆ†å—å±‚å°†æ¯ä¸ªå›¾åƒè¡¨ç¤ºä¸ºåˆ†å—å‘é‡ã€‚æˆ‘ä»¬å¯¹æ–‡æœ¬æ ‡è®°åº”ç”¨å› æœæ³¨æ„åŠ›ï¼Œå¹¶å¯¹å›¾åƒå—åº”ç”¨åŒå‘æ³¨æ„åŠ›ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§£ç ç®—æ³•ï¼Œè¯¥ç®—æ³•ç»“åˆäº†ä»è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ ‡å‡†åšæ³•å’Œä»æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ ‡å‡†åšæ³•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä¸ Chameleon çš„ç¦»æ•£åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒTransfusion æ¨¡å‹åœ¨æ¯ç§æ¨¡æ€ç»„åˆä¸­éƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ‰©å±•æ€§ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬å‘ç° Transfusion åœ¨ä¸åˆ° Chameleon è®¡ç®—é‡çš„ä¸‰åˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹è¶…è¿‡äº† Chameleon æ–¹æ³•ï¼Œæ— è®ºæ˜¯ FID è¿˜æ˜¯ CLIP åˆ†æ•°ã€‚å½“æ§åˆ¶ FLOPs æ—¶ï¼ŒTransfusion å®ç°äº†æ¯” Chameleon æ¨¡å‹ä½çº¦ 2 å€çš„ FID åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆä¸­è§‚å¯Ÿåˆ°ç±»ä¼¼çš„è¶‹åŠ¿ï¼Œå…¶ä¸­ Transfusion åœ¨ 21.8% çš„ FLOPs ä¸‹ä¸ Chameleon ç›¸åŒ¹é…ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒTransfusion åœ¨å­¦ä¹ æ–‡æœ¬åˆ°æ–‡æœ¬é¢„æµ‹æ–¹é¢ä¹Ÿæ›´æœ‰æ•ˆç‡ï¼Œåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šå®ç°äº†å›°æƒ‘åº¦å¹³è¡¡ï¼Œå¤§çº¦æ˜¯ Chameleon çš„ FLOPs çš„ 50% åˆ° 60%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Transfusion æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„è®­ç»ƒçœŸæ­£å¤šæ¨¡æ€æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡è®­ç»ƒå•ä¸ªæ¨¡å‹æ¥é¢„æµ‹ç¦»æ•£æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£è¿ç»­å›¾åƒï¼Œä»è€Œå®Œå…¨æ•´åˆäº†è¿™ä¸¤ç§æ¨¡æ€ï¼Œå¹¶ä¸”æ²¡æœ‰ä¿¡æ¯æŸå¤±ã€‚Transfusion åœ¨å„ç§æ¨¡æ€ç»„åˆä¸­éƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ‰©å±•æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å’Œå›¾åƒã€‚æ­¤å¤–ï¼ŒTransfusion è¿˜å¯ä»¥ç”ŸæˆåŸºäºå…¶ä»–å›¾åƒçš„å›¾åƒï¼Œè¿™è¡¨æ˜å®ƒå¯ä»¥é€‚åº”å’Œæ³›åŒ–åˆ°æ–°çš„æ¨¡æ€ç»„åˆã€‚

## anole--an-open--autoregressive--native-large-multimodal-models-for-interleaved-image-text-generation
### Abstract
Previous open-source large multimodal models (LMMs) have faced several
limitations: (1) they often lack native integration, requiring adapters to
align visual representations with pre-trained large language models (LLMs); (2)
many are restricted to single-modal generation; (3) while some support
multimodal generation, they rely on separate diffusion models for visual
modeling and generation. To mitigate these limitations, we present Anole, an
open, autoregressive, native large multimodal model for interleaved image-text
generation. We build Anole from Meta AI's Chameleon, adopting an innovative
fine-tuning strategy that is both data-efficient and parameter-efficient. Anole
demonstrates high-quality, coherent multimodal generation capabilities. We have
open-sourced our model, training framework, and instruction tuning data.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ANOLEï¼šå¼€å¯å¤šæ¨¡æ€ç”Ÿæˆæ–°çºªå…ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ç¼ºä¹åŸç”Ÿé›†æˆã€å•æ¨¡æ€ç”Ÿæˆé™åˆ¶ä»¥åŠä¾èµ–é¢å¤–çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰å»ºæ¨¡å’Œç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ANOLEï¼Œä¸€ä¸ªå¼€æ”¾ã€è‡ªå›å½’ã€åŸç”Ÿçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºäº¤é”™å›¾åƒ-æ–‡æœ¬ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸç”Ÿå¤šæ¨¡æ€é›†æˆ
ANOLEåŸºäºMeta AIçš„Chameleonæ¨¡å‹ï¼Œé‡‡ç”¨åˆ›æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œå®ç°äº†è§†è§‰è¡¨ç¤ºä¸é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸç”Ÿé›†æˆï¼Œæ— éœ€é€‚é…å™¨è¿›è¡Œå¯¹é½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆå¾®è°ƒ
ANOLEé‡‡ç”¨æ•°æ®é«˜æ•ˆå’Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä»…éœ€çº¦6000ä¸ªæ ·æœ¬å’Œä¸åˆ°4000ä¸‡ä¸ªå‚æ•°ï¼Œå³å¯æœ‰æ•ˆå®ç°è§†è§‰å’Œå¤šåª’ä½“ç”Ÿæˆèƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªå›å½’ç”Ÿæˆ
ANOLEé‡‡ç”¨è‡ªå›å½’æ–¹æ³•è¿›è¡Œå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯çš„å¤šæ¨¡æ€åºåˆ—ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼€æ”¾èµ„æº
ANOLEå¼€æºäº†æ¨¡å‹ã€è®­ç»ƒæ¡†æ¶å’ŒæŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†ä¸°å¯Œçš„èµ„æºå’Œæ”¯æŒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ANOLEåœ¨å›¾åƒç”Ÿæˆå’Œäº¤é”™å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç”Ÿæˆçš„å›¾åƒè´¨é‡é«˜ï¼Œä¸ç»™å®šæŒ‡ä»¤ç´§å¯†ç›¸å…³ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒç±»å‹ã€‚æ­¤å¤–ï¼ŒANOLEèƒ½å¤Ÿç”Ÿæˆç»„ç»‡è‰¯å¥½ã€æä¾›å…¨é¢ç»†èŠ‚çš„æ–‡æœ¬ï¼Œå¹¶ä¸å›¾åƒæ— ç¼é›†æˆï¼Œå®ç°å®Œç¾çš„è§†è§‰å’Œæ–‡æœ¬å…ƒç´ äº’è¡¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ANOLEä¸ºå¤šæ¨¡æ€ç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†æ–°çš„çªç ´ï¼Œå…¶åŸç”Ÿå¤šæ¨¡æ€é›†æˆã€é«˜æ•ˆå¾®è°ƒã€è‡ªå›å½’ç”Ÿæˆå’Œå¼€æ”¾èµ„æºç­‰ç‰¹ç‚¹ä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†å®è´µçš„å€Ÿé‰´å’Œå¯ç¤ºã€‚æœªæ¥ï¼ŒANOLEæœ‰æœ›åœ¨ç²¾ç¡®æŒ‡ä»¤éµå¾ªã€ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•ã€å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æå‡ä»¥åŠä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ç­‰æ–¹é¢å–å¾—è¿›ä¸€æ­¥å‘å±•ã€‚

## hybrid-alignment-training-for-large-language-models
### Abstract
Alignment training is crucial for enabling large language models (LLMs) to
cater to human intentions and preferences. It is typically performed based on
two stages with different objectives: instruction-following alignment and
human-preference alignment. However, aligning LLMs with these objectives in
sequence suffers from an inherent problem: the objectives may conflict, and the
LLMs cannot guarantee to simultaneously align with the instructions and human
preferences well. To response to these, in this work, we propose a Hybrid
Alignment Training (Hbat) approach, based on alternating alignment and modified
elastic weight consolidation methods. The basic idea is to alternate between
different objectives during alignment training, so that better collaboration
can be achieved between the two alignment tasks.We experiment with Hbat on
summarization and dialogue tasks. Experimental results show that the proposed
\textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields
consistent performance gains over the traditional two-stage alignment training
when using both proximal policy optimization and direct preference
optimization.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ··åˆå¯¹é½è®­ç»ƒï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œæ‰§è¡Œäººç±»æŒ‡ä»¤æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯¹äººç±»æ„å›¾å’Œåå¥½çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µçš„å¯¹é½è®­ç»ƒï¼šæŒ‡ä»¤éµå¾ªå¯¹é½å’Œäººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ç§é¡ºåºå¯¹é½è®­ç»ƒå­˜åœ¨å›ºæœ‰çš„é—®é¢˜ï¼šä¸¤ä¸ªç›®æ ‡å¯èƒ½å†²çªï¼ŒLLMsæ— æ³•ä¿è¯åŒæ—¶å¾ˆå¥½åœ°å¯¹é½æŒ‡ä»¤å’Œäººç±»åå¥½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆå¯¹é½è®­ç»ƒï¼ˆHBATï¼‰æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿å¯¹é½å’Œæ”¹è¿›çš„å¼¹æ€§æƒé‡æ•´åˆæ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤æ›¿å¯¹é½
HBAT é‡‡ç”¨äº¤æ›¿å¯¹é½æ–¹æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº¤æ›¿å­¦ä¹ æŒ‡ä»¤éµå¾ªå¯¹é½å’Œäººç±»åå¥½å¯¹é½ï¼Œä»è€Œæ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªå¯¹é½ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¹è¿›çš„å¼¹æ€§æƒé‡æ•´åˆ
HBAT å¼•å…¥æ”¹è¿›çš„å¼¹æ€§æƒé‡æ•´åˆï¼ˆEWCï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€åœ°å¯¹æ¯ä¸ªå‚æ•°æ–½åŠ é€‚å½“çš„çº¦æŸï¼Œä»è€Œç¼“è§£ä¸å…ˆå‰ç›®æ ‡çš„ä¼˜åŒ–å†²çªã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ‘˜è¦å’Œå…¨æ–‡ä¸­ï¼ŒHBAT åœ¨æ‘˜è¦å’Œå¯¹è¯ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ HBAT å¯ä»¥æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒåŸºäº LLaMA2-13B æ¨¡å‹ï¼ŒHBAT åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šæ¯”ä¼ ç»Ÿçš„ RLHF æ–¹æ³•æé«˜äº† 2.26 ä¸ª ROUGE-L åˆ†æ•°ï¼Œåœ¨å¯¹è¯ä»»åŠ¡ä¸Šæé«˜äº† 21.01 ä¸ª GPT-4 èƒœç‡åˆ†æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
HBAT æ–¹æ³•ä¸ºè§£å†³ LLM å¯¹é½è®­ç»ƒä¸­çš„ä¼˜åŒ–å†²çªé—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¯ä»¥åº”ç”¨äºå…¶ä»– NLP ä»»åŠ¡å’Œåå¥½å¯¹é½æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒHBAT æ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–ä¼˜åŒ–å¯¹é½æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡ LLM çš„æ€§èƒ½ã€‚

## relational-programming-with-foundation-models
### Abstract
Foundation models have vast potential to enable diverse AI applications. The
powerful yet incomplete nature of these models has spurred a wide range of
mechanisms to augment them with capabilities such as in-context learning,
information retrieval, and code interpreting. We propose Vieira, a declarative
framework that unifies these mechanisms in a general solution for programming
with foundation models. Vieira follows a probabilistic relational paradigm and
treats foundation models as stateless functions with relational inputs and
outputs. It supports neuro-symbolic applications by enabling the seamless
combination of such models with logic programs, as well as complex, multi-modal
applications by streamlining the composition of diverse sub-models. We
implement Vieira by extending the Scallop compiler with a foreign interface
that supports foundation models as plugins. We implement plugins for 12
foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9
challenging tasks that span language, vision, and structured and vector
databases. Our evaluation shows that programs in Vieira are concise, can
incorporate modern foundation models, and have comparable or better accuracy
than competitive baselines.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºåŸºç¡€æ¨¡å‹çš„å…³è”ç¼–ç¨‹æ¡†æ¶ï¼šVieira

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPTã€CLIPç­‰ï¼‰åœ¨å„ä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç¼–ç¨‹AIåº”ç”¨æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€ç¼ºä¹ç»“æ„åŒ–æ•°æ®å¤„ç†èƒ½åŠ›ã€éš¾ä»¥ç»„åˆä¸åŒæ¨¡æ€çš„æ•°æ®ç­‰ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Vieiraï¼Œä¸€ä¸ªåŸºäºå…³è”ç¼–ç¨‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºç¼–ç¨‹åŸºç¡€æ¨¡å‹æä¾›ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…³è”ç¼–ç¨‹èŒƒå¼
Vieiraé‡‡ç”¨å…³è”ç¼–ç¨‹èŒƒå¼ï¼Œå°†åŸºç¡€æ¨¡å‹è§†ä¸ºå…·æœ‰å…³è”è¾“å…¥å’Œè¾“å‡ºçš„æ— çŠ¶æ€å‡½æ•°ã€‚è¿™ç§èŒƒå¼ä½¿å¾—åŸºç¡€æ¨¡å‹å¯ä»¥ä¸é€»è¾‘ç¨‹åºæ— ç¼ç»“åˆï¼Œæ”¯æŒç¥ç»ç¬¦å·åº”ç”¨ï¼Œå¹¶ç®€åŒ–äº†å¤šæ¨¡æ€å­æ¨¡å‹çš„ç»„åˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ’ä»¶å¼æ¡†æ¶
Vieiraé€šè¿‡æ‰©å±•Scallopç¼–è¯‘å™¨ï¼Œå®ç°äº†ä¸€ä¸ªæ”¯æŒåŸºç¡€æ¨¡å‹ä½œä¸ºæ’ä»¶çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªå¯å®šåˆ¶å’Œå¯æ‰©å±•çš„æ’ä»¶åº“ï¼ŒåŒ…æ‹¬GPTã€CLIPã€SAMç­‰12ä¸ªåŸºç¡€æ¨¡å‹ã€‚è¿™ä½¿å¾—Vieiraèƒ½å¤Ÿæ”¯æŒå„ç§åº”ç”¨ï¼Œå¹¶å…·æœ‰å‡å°‘å¹»è§‰ã€å¢å¼ºæ£€ç´¢å’Œå¤šæ¨¡æ€ç»„åˆæ€§ç­‰ä¼˜åŠ¿ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Vieiraåœ¨9ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†è¯­è¨€ã€è§†è§‰ã€ç»“æ„åŒ–å’Œå‘é‡æ•°æ®åº“ç­‰é¢†åŸŸã€‚ç»“æœè¡¨æ˜ï¼ŒVieiraçš„ç¨‹åºç®€æ´ï¼Œèƒ½å¤Ÿæ•´åˆç°ä»£åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®ç‡æ–¹é¢ä¸ç«äº‰åŸºçº¿ç›¸å½“æˆ–æ›´å¥½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Vieiraä¸ºç¼–ç¨‹åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶å…³è”ç¼–ç¨‹èŒƒå¼å’Œæ’ä»¶å¼æ¡†æ¶å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **å…³è”ç¼–ç¨‹èŒƒå¼**ï¼šå°†åŸºç¡€æ¨¡å‹è§†ä¸ºå…³è”å‡½æ•°ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä¸é€»è¾‘ç¨‹åºç»“åˆï¼Œæ”¯æŒç¥ç»ç¬¦å·åº”ç”¨å’Œå¤šæ¨¡æ€ç»„åˆã€‚
* **æ’ä»¶å¼æ¡†æ¶**ï¼šé€šè¿‡æ’ä»¶åº“æ”¯æŒå¤šç§åŸºç¡€æ¨¡å‹ï¼Œä½¿å¾—Vieiraèƒ½å¤Ÿé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚
* **æ¦‚ç‡é€»è¾‘æ¨ç†**ï¼šVieiraæ”¯æŒæ¦‚ç‡é€»è¾‘æ¨ç†ï¼Œå¯ä»¥å¤„ç†ä¸ç¡®å®šæ€§ä¿¡æ¯ï¼Œå¹¶æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚

### æ€»ç»“
Vieiraæ˜¯ä¸€ä¸ªåŸºäºå…³è”ç¼–ç¨‹çš„æ¡†æ¶ï¼Œä¸ºç¼–ç¨‹åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶å…·æœ‰ç®€æ´ã€æ˜“ç”¨ã€å¯æ‰©å±•ç­‰ä¼˜ç‚¹ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚Vieiraä¸ºå¼€å‘åŸºäºåŸºç¡€æ¨¡å‹çš„AIåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## seed-x--multimodal-models-with-unified-multi-granularity-comprehension-and-generation
### Abstract
The rapid evolution of multimodal foundation model has demonstrated
significant progresses in vision-language understanding and generation, e.g.,
our previous work SEED-LLaMA. However, there remains a gap between its
capability and the real-world applicability, primarily due to the model's
limited capacity to effectively respond to various user instructions and
interact with diverse visual data. In this work, we focus on bridging this gap
through integrating two enhanced features: (1) comprehending images of
arbitrary sizes and ratios, and (2) enabling multi-granularity image
generation. We present a unified and versatile foundation model, namely,
SEED-X, which is able to model multi-granularity visual semantics for
comprehension and generation tasks. Besides the competitive results on public
benchmarks, SEED-X demonstrates its effectiveness in handling real-world
applications across various domains after instruction tuning. We hope that our
work will inspire future research into what can be achieved by versatile
multimodal foundation models in real-world applications. The models, codes, and
datasets are released in https://github.com/AILab-CVC/SEED-X.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SEED-Xï¼šå¤šæ¨¡æ€æ¨¡å‹çš„ç»Ÿä¸€å¤šç²’åº¦ç†è§£å’Œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œä¾‹å¦‚SEED-LLaMAï¼Œè¿™äº›æ¨¡å‹åœ¨å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œä¸»è¦åŸå› æ˜¯æ¨¡å‹éš¾ä»¥æœ‰æ•ˆå“åº”å„ç§ç”¨æˆ·æŒ‡ä»¤å’Œä¸å¤šæ ·åŒ–çš„è§†è§‰æ•°æ®è¿›è¡Œäº¤äº’ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡é›†æˆä¸¤ä¸ªå¢å¼ºåŠŸèƒ½æ¥å¼¥åˆè¿™ä¸€å·®è·ï¼šï¼ˆ1ï¼‰ç†è§£ä»»æ„å¤§å°å’Œæ¯”ä¾‹çš„å›¾åƒï¼Œä»¥åŠï¼ˆ2ï¼‰å®ç°å¤šç²’åº¦å›¾åƒç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†è§‰åˆ†è¯å’Œè§£ç 
SEED-Xé‡‡ç”¨è§†è§‰åˆ†è¯å™¨æ¥ç»Ÿä¸€å›¾åƒç†è§£å’Œç”Ÿæˆï¼Œå¹¶é¢„è®­ç»ƒä¸€ä¸ªå¤šç²’åº¦è§£ç å™¨æ¥ä¿ƒè¿›å›¾åƒç”Ÿæˆå’Œé«˜ç²¾åº¦å›¾åƒæ“ä½œã€‚é¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ViTä½œä¸ºè§†è§‰åˆ†è¯å™¨ï¼Œå¹¶é¢„è®­ç»ƒä¸€ä¸ªè§†è§‰è§£ç å™¨ï¼Œé€šè¿‡æ¥æ”¶ViTç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è§£ç é€¼çœŸçš„å›¾åƒã€‚ç„¶åï¼Œè¿›ä¸€æ­¥å¾®è°ƒè§†è§‰è§£ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥æ”¶é¢å¤–çš„æ¡ä»¶å›¾åƒä½œä¸ºè¾“å…¥ï¼Œä»¥ä¿ç•™è¾“å…¥å›¾åƒçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€åˆ†è¾¨ç‡å›¾åƒç¼–ç 
ä¸ºäº†ä½¿å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä»»æ„å¤§å°å’Œçºµæ¨ªæ¯”çš„å›¾åƒï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€åˆ†è¾¨ç‡å›¾åƒç¼–ç ã€‚è¯¥æ–¹æ³•å°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºå­å›¾åƒï¼Œå¹¶ä¸ºæ¯ä¸ªå­å›¾åƒçš„ViTç‰¹å¾æ·»åŠ å¯æ‰©å±•çš„äºŒç»´ä½ç½®åµŒå…¥ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­æœªé‡åˆ°çš„å›¾åƒåˆ†è¾¨ç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SEED-Xåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå¹¶åœ¨ç»è¿‡æŒ‡ä»¤è°ƒæ•´åï¼Œåœ¨å„ä¸ªé¢†åŸŸçš„å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEED-Xåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸æŒ‡ä»¤é«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œå¹¶ä¿ç•™è¾“å…¥å›¾åƒçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SEED-Xçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç»Ÿä¸€å¤šç²’åº¦ç†è§£å’Œç”Ÿæˆï¼Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºçš„è§†è§‰åˆ†è¯å’Œè§£ç æ–¹æ³•ä»¥åŠåŠ¨æ€åˆ†è¾¨ç‡å›¾åƒç¼–ç æŠ€æœ¯ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## mini-gemini--mining-the-potential-of-multi-modality-vision-language-models
### Abstract
In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Mini-Geminiï¼šæŒ–æ˜å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œèµ‹äºˆå¤šæ¨¡æ€è¾“å…¥èƒ½åŠ›å·²æˆä¸ºå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡VLMsåœ¨è§†è§‰å¯¹è¯å’Œæ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä¸GPT-4å’ŒGeminiç­‰å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œä»ç„¶å­˜åœ¨æ€§èƒ½å·®è·ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æŒ–æ˜VLMsçš„æ½œåŠ›ï¼Œç¼©å°è¿™ä¸€å·®è·ï¼Œå¹¶æé«˜VLMsçš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜åˆ†è¾¨ç‡è§†è§‰æ ‡è®°
ä¸ºäº†å¢å¼ºè§†è§‰æ ‡è®°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒç¼–ç å™¨ç³»ç»Ÿï¼Œå…¶ä¸­ä¸€ä¸ªç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¦ä¸€ä¸ªç”¨äºä½åˆ†è¾¨ç‡è§†è§‰åµŒå…¥ã€‚è¿™ç§è®¾è®¡å¯ä»¥åœ¨ä¸å¢åŠ è§†è§‰æ ‡è®°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜è§†è§‰ç»†èŠ‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜è´¨é‡æ•°æ®
ä¸ºäº†æé«˜æ•°æ®è´¨é‡ï¼Œæœ¬æ–‡æ”¶é›†å’Œåˆ¶ä½œäº†æ›´å¤šåŸºäºå…¬å…±èµ„æºçš„æ•°æ®ï¼ŒåŒ…æ‹¬é«˜è´¨é‡å“åº”ã€ä»»åŠ¡å¯¼å‘æŒ‡ä»¤å’Œç”Ÿæˆç›¸å…³æ•°æ®ã€‚è¿™äº›æ•°æ®æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ‰©å±•å…¶èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šVLMå¼•å¯¼ç”Ÿæˆ
æœ¬æ–‡å°†VLMä¸å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œä»¥æ”¯æŒå›¾åƒå’Œæ–‡æœ¬çš„å¹¶å‘ç”Ÿæˆã€‚VLMé€šè¿‡æä¾›LLMsç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒMini-Geminiåœ¨å¤šä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†Gemini Proã€Qwen-VL-Pluså’ŒGPT 4Vç­‰ç§æœ‰æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Mini-Geminiæ¡†æ¶ä¸ºVLMsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶åŒç¼–ç å™¨ç³»ç»Ÿã€é«˜è´¨é‡æ•°æ®é›†å’ŒVLMå¼•å¯¼ç”Ÿæˆæ–¹æ³•ï¼Œéƒ½å¯ä»¥ä¸ºå…¶ä»–VLMsçš„è®¾è®¡å’Œåº”ç”¨æä¾›å‚è€ƒã€‚

## anygpt--unified-multimodal-llm-with-discrete-sequence-modeling
### Abstract
We introduce AnyGPT, an any-to-any multimodal language model that utilizes
discrete representations for the unified processing of various modalities,
including speech, text, images, and music. AnyGPT can be trained stably without
any alterations to the current large language model (LLM) architecture or
training paradigms. Instead, it relies exclusively on data-level preprocessing,
facilitating the seamless integration of new modalities into LLMs, akin to the
incorporation of new languages. We build a multimodal text-centric dataset for
multimodal alignment pre-training. Utilizing generative models, we synthesize
the first large-scale any-to-any multimodal instruction dataset. It consists of
108k samples of multi-turn conversations that intricately interweave various
modalities, thus equipping the model to handle arbitrary combinations of
multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is
capable of facilitating any-to-any multimodal conversation while achieving
performance comparable to specialized models across all modalities, proving
that discrete representations can effectively and conveniently unify multiple
modalities within a language model. Demos are shown in
https://junzhan2000.github.io/AnyGPT.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | AnyGPTï¼šç»Ÿä¸€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä»»æ„æ¨¡æ€é—´çš„æ— ç¼è½¬æ¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„èƒ½åŠ›ä»…é™äºæ–‡æœ¬å¤„ç†ï¼Œè€Œç°å®ä¸–ç•Œæ˜¯ inherently å¤šæ¨¡æ€çš„ï¼ŒåŒ…æ‹¬è§†è§‰ã€è¯­è¨€ã€å£°éŸ³å’Œè§¦è§‰ç­‰å¤šç§ä¿¡æ¯æ¸ é“ã€‚å› æ­¤ï¼Œå°† LLM æ‰©å±•åˆ°å¤šæ¨¡æ€æ„ŸçŸ¥æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAnyGPTï¼Œä¸€ä¸ªåŸºäº token çš„ä»»æ„æ¨¡æ€åˆ°ä»»æ„æ¨¡æ€çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹
AnyGPT èƒ½å¤Ÿç†è§£å’Œç”ŸæˆåŒ…æ‹¬è¯­éŸ³ã€æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³ä¹åœ¨å†…çš„å¤šç§æ¨¡æ€ã€‚å®ƒé€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€åˆ†è¯å™¨å°†åŸå§‹å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å‹ç¼©æˆç¦»æ•£çš„è¯­ä¹‰ token åºåˆ—ï¼Œç„¶åä½¿ç”¨ LLM å¯¹è¿™äº› token åºåˆ—è¿›è¡Œè‡ªå›å½’è®­ç»ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¤šæ¨¡æ€ token è¢«è§£ç å›åŸå§‹æ¨¡æ€è¡¨ç¤ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»º AnyInstruct-108k æ•°æ®é›†ï¼Œå®ç°ä»»æ„æ¨¡æ€é—´çš„å¯¹è¯
ä¸ºäº†è§£å†³å¤šæ¨¡æ€å¯¹é½æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼ŒAnyGPT æ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¯¹é½æ•°æ®é›† AnyInstruct-108kï¼ŒåŒ…å« 108k ä¸ªå¤šè½®å¯¹è¯æ ·æœ¬ï¼Œè¿™äº›å¯¹è¯æ ·æœ¬äº¤ç»‡äº†å¤šç§æ¨¡æ€ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä»»æ„ç»„åˆçš„å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒAnyGPT åœ¨å„ç§è·¨æ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå¹¶è¯æ˜äº†ç¦»æ•£è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°å°†å¤šä¸ªæ¨¡æ€ç»Ÿä¸€åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ LLM ä¸­ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AnyGPT çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºæ¥ç»Ÿä¸€å¤„ç†å¤šç§æ¨¡æ€ï¼Œå¹¶é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ºå¤šæ¨¡æ€ LLM çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå®ç°ä»»æ„æ¨¡æ€é—´çš„æ— ç¼è½¬æ¢æä¾›äº†å¯èƒ½ã€‚

## world-model-on-million-length-video-and-language-with-blockwise-ringattention
### Abstract
Enabling long-context understanding remains a key challenge in scaling
existing sequence models -- a crucial component in developing generally
intelligent models that can process and operate over long temporal horizons
that potentially consist of millions of tokens. In this paper, we aim to
address these challenges by providing a comprehensive exploration of the full
development process for producing 1M context language models and video-language
models, setting new benchmarks in language retrieval and new capabilities in
long video understanding. We detail our long context data curation process,
progressive context extension from 4K to 1M tokens, and present an efficient
open-source implementation for scalable training on long sequences.
Additionally, we open-source a family of 7B parameter models capable of
processing long text documents and videos exceeding 1M tokens.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºç™¾ä¸‡é•¿åº¦è§†é¢‘å’Œè¯­è¨€çš„å—çŠ¶ç¯æ³¨æ„åŠ›ä¸–ç•Œæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œåºåˆ—æ¨¡å‹åœ¨å¤„ç†å’Œç†è§£é•¿æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ–¹é¢é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨¡å‹å¤§å¤šå±€é™äºå¤„ç†çŸ­åºåˆ—ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ã€é•¿ç¯‡å¹…çš„è¯­è¨€å’Œè§†è§‰ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè®­ç»ƒèƒ½å¤Ÿå¤„ç†æ•°ç™¾ä¸‡ä¸ªtokençš„æ¨¡å‹éœ€è¦å…‹æœé«˜å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œä»¥åŠç¼ºä¹é•¿ä¸Šä¸‹æ–‡æ•°æ®çš„éš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œå¤§å‹ä¸–ç•Œæ¨¡å‹â€ï¼ˆLWMï¼‰çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚LWMé‡‡ç”¨å—çŠ¶ç¯æ³¨æ„åŠ›ï¼ˆBlockwise RingAttentionï¼‰æŠ€æœ¯ï¼Œæ— éœ€è¿‘ä¼¼æˆ–å¼€é”€å³å¯æ‰©å±•ä¸Šä¸‹æ–‡å¤§å°ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é•¿åºåˆ—è®­ç»ƒã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•
LWMé€šè¿‡é€æ­¥å¢åŠ åºåˆ—é•¿åº¦ï¼Œä»4K tokenæ‰©å±•åˆ°1M tokenï¼Œæœ‰æ•ˆåœ°è§£å†³äº†é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚åŒæ—¶ï¼Œæ¨¡å‹é‡‡ç”¨RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æŠ€æœ¯ï¼Œé€šè¿‡è°ƒæ•´Î¸å‚æ•°æ¥é€‚åº”æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å‹ç”Ÿæˆé—®ç­”æ•°æ®
ä¸ºäº†å¢å¼ºæ¨¡å‹åœ¨é•¿åºåˆ—å¯¹è¯ä¸­çš„èƒ½åŠ›ï¼ŒLWMé‡‡ç”¨äº†ä¸€ç§åŸºäºæ¨¡å‹çš„é—®ç­”æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨çŸ­ä¸Šä¸‹æ–‡æ¨¡å‹ä»ä¹¦ç±ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨é•¿åºåˆ—å¯¹è¯ä¸­çš„è¡¨ç°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€è®­ç»ƒ
LWMå°†è§†è§‰æ¨¡æ€ï¼ˆå›¾åƒå’Œè§†é¢‘ï¼‰èå…¥è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–å›¾åƒå’Œè§†é¢‘çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹æŸå¤±ï¼Œå®ç°äº†å¯¹è§†è§‰ä¿¡æ¯çš„æœ‰æ•ˆå¤„ç†ã€‚æ­¤å¤–ï¼ŒLWMè¿˜é‡‡ç”¨äº†æ©ç åºåˆ—æ‰“åŒ…ç­–ç•¥å’Œç²¾ç»†çš„æŸå¤±å¹³è¡¡ï¼Œä»¥æ›´å¥½åœ°å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
LWMåœ¨é•¿è§†é¢‘ç†è§£å’Œé•¿ä¸Šä¸‹æ–‡äº‹å®æ£€ç´¢æ–¹é¢å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚åœ¨è¯­è¨€ä»»åŠ¡ã€æ£€ç´¢ä»»åŠ¡å’ŒLOFTæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLWMåœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒLWMè¿˜å±•ç¤ºäº†å›¾åƒå’Œè§†é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬ç”Ÿæˆé™æ€å›¾åƒå’ŒåŠ¨æ€è§†é¢‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LWMçš„ç ”ç©¶æˆæœä¸ºé•¿ä¸Šä¸‹æ–‡åºåˆ—æ¨¡å‹çš„å¼€å‘æä¾›äº†é‡è¦å‚è€ƒã€‚å…¶æ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•ã€æ¨¡å‹ç”Ÿæˆé—®ç­”æ•°æ®å’Œå¤šæ¨¡æ€è®­ç»ƒç­‰æŠ€æœ¯ï¼Œä¸ºè§£å†³é•¿åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚æ­¤å¤–ï¼ŒLWMçš„å¼€æºå®ç°å’Œæ¨¡å‹ä¹Ÿä¸ºç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„èµ„æºã€‚

## video-lavit--unified-video-language-pre-training-with-decoupled-visual-motional-tokenization
### Abstract
In light of recent advances in multimodal Large Language Models (LLMs), there
is increasing attention to scaling them from image-text data to more
informative real-world videos. Compared to static images, video poses unique
challenges for effective large-scale pre-training due to the modeling of its
spatiotemporal dynamics. In this paper, we address such limitations in
video-language pre-training with an efficient video decomposition that
represents each video as keyframes and temporal motions. These are then adapted
to an LLM using well-designed tokenizers that discretize visual and temporal
information as a few tokens, thus enabling unified generative pre-training of
videos, images, and text. At inference, the generated tokens from the LLM are
carefully recovered to the original continuous pixel space to create various
video content. Our proposed framework is both capable of comprehending and
generating image and video content, as demonstrated by its competitive
performance across 13 multimodal benchmarks in image and video understanding
and generation. Our code and models are available at
https://video-lavit.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Video-LaVITï¼šåŸºäºè§£è€¦è§†è§‰-è¿åŠ¨æ ‡è®°çš„è§†é¢‘-è¯­è¨€é¢„è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„çªç ´ï¼Œå°†å®ƒä»¬ä»å›¾åƒ-æ–‡æœ¬æ•°æ®æ‰©å±•åˆ°æ›´ä¸°å¯Œçš„çœŸå®ä¸–ç•Œè§†é¢‘æ•°æ®æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œä¸é™æ€å›¾åƒç›¸æ¯”ï¼Œè§†é¢‘æ•°æ®ç”±äºå…¶æ—¶ç©ºåŠ¨æ€ç‰¹æ€§ï¼Œåœ¨æœ‰æ•ˆçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ–¹é¢é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆæ— æ³•æœ‰æ•ˆåœ°ç¼–ç è§†é¢‘çš„æ—¶ç©ºåŠ¨æ€ä¿¡æ¯ï¼Œè¦ä¹ˆåœ¨å¤„ç†é•¿è§†é¢‘æ—¶è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†é¢‘åˆ†è§£
Video-LaVIT æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘åˆ†è§£æ–¹æ³•ï¼Œå°†æ¯ä¸ªè§†é¢‘è¡¨ç¤ºä¸ºå…³é”®å¸§å’Œè¿åŠ¨å‘é‡ã€‚å…³é”®å¸§ç”¨äºæ•æ‰è§†é¢‘çš„ä¸»è¦è§†è§‰è¯­ä¹‰ï¼Œè€Œè¿åŠ¨å‘é‡åˆ™æè¿°äº†å…³é”®å¸§éšæ—¶é—´çš„åŠ¨æ€å˜åŒ–ã€‚è¿™ç§åˆ†è§£æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†è§†é¢‘è¡¨ç¤ºæ‰€éœ€çš„æ ‡è®°æ•°é‡ï¼Œä»è€Œæé«˜äº†å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£è€¦è§†è§‰-è¿åŠ¨æ ‡è®°åŒ–
Video-LaVIT è®¾è®¡äº†ä¸¤ç§æ ‡è®°åŒ–å™¨æ¥å¤„ç†è§†é¢‘æ¨¡æ€ï¼šå›¾åƒæ ‡è®°åŒ–å™¨å’Œè¿åŠ¨æ ‡è®°åŒ–å™¨ã€‚å›¾åƒæ ‡è®°åŒ–å™¨åˆ©ç”¨ç°æœ‰çš„å›¾åƒæ ‡è®°åŒ–å™¨ï¼ˆå¦‚ LaVITï¼‰æ¥å¤„ç†å…³é”®å¸§ï¼Œè€Œè¿åŠ¨æ ‡è®°åŒ–å™¨åˆ™åŸºäº VQ-VAE æ¶æ„å¼€å‘ï¼Œç”¨äºå°†è¿åŠ¨å‘é‡è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°åºåˆ—ã€‚è¿™ç§è§£è€¦çš„æ ‡è®°åŒ–æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ è§†é¢‘çš„è§†è§‰å’Œè¿åŠ¨ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§†é¢‘å»æ ‡è®°åŒ–
Video-LaVIT è¿˜è®¾è®¡äº†ä¸€ä¸ªè§†é¢‘å»æ ‡è®°åŒ–å™¨ï¼Œç”¨äºå°† LLM ç”Ÿæˆçš„ç¦»æ•£æ ‡è®°åºåˆ—æ¢å¤åˆ°åŸå§‹çš„è¿ç»­åƒç´ ç©ºé—´ï¼Œä»è€Œç”Ÿæˆå„ç§è§†é¢‘å†…å®¹ã€‚è§†é¢‘å»æ ‡è®°åŒ–å™¨é‡‡ç”¨æ¡ä»¶å»å™ª U-Net æ¶æ„ï¼Œå¹¶é€šè¿‡è¿åŠ¨å‘é‡è¿›è¡Œå¢å¼ºçš„è¿åŠ¨æ¡ä»¶ï¼Œä»è€Œæé«˜äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šç»Ÿä¸€ç”Ÿæˆå»ºæ¨¡
Video-LaVIT å°†æ‰€æœ‰æ¨¡æ€ï¼ˆè§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬ï¼‰éƒ½è§†ä¸º 1D ç¦»æ•£æ ‡è®°ï¼Œå¹¶ä½¿ç”¨ LLM è¿›è¡Œç»Ÿä¸€çš„ç”Ÿæˆé¢„è®­ç»ƒã€‚è¿™ç§ç»Ÿä¸€çš„å»ºæ¨¡æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶ç†è§£å’Œç”Ÿæˆå„ç§å¤šæ¨¡æ€å†…å®¹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Video-LaVIT åœ¨ 13 ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†éå¸¸å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒVideo-LaVIT åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨è§†é¢‘ç†è§£æ–¹é¢ï¼ŒVideo-LaVIT åœ¨è§†é¢‘é—®ç­”å’Œè§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ã€‚åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢ï¼ŒVideo-LaVIT åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå’Œé•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Video-LaVIT çš„è§†é¢‘åˆ†è§£ã€è§£è€¦è§†è§‰-è¿åŠ¨æ ‡è®°åŒ–ã€è§†é¢‘å»æ ‡è®°åŒ–å’Œç»Ÿä¸€ç”Ÿæˆå»ºæ¨¡ç­‰åˆ›æ–°æ–¹æ³•ä¸ºè§†é¢‘-è¯­è¨€é¢„è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€ AI åŠ©æ‰‹å¥ å®šäº†åŸºç¡€ã€‚

## mm-interleaved--interleaved-image-text-generative-modeling-via-multi-modal-feature-synchronizer
### Abstract
Developing generative models for interleaved image-text data has both
research and practical value. It requires models to understand the interleaved
sequences and subsequently generate images and text. However, existing attempts
are limited by the issue that the fixed number of visual tokens cannot
efficiently capture image details, which is particularly problematic in the
multi-image scenarios. To address this, this paper presents MM-Interleaved, an
end-to-end generative model for interleaved image-text data. It introduces a
multi-scale and multi-image feature synchronizer module, allowing direct access
to fine-grained image features in the previous context during the generation
process. MM-Interleaved is end-to-end pre-trained on both paired and
interleaved image-text corpora. It is further enhanced through a supervised
fine-tuning phase, wherein the model improves its ability to follow complex
multi-modal instructions. Experiments demonstrate the versatility of
MM-Interleaved in recognizing visual details following multi-modal instructions
and generating consistent images following both textual and visual conditions.
Code and models are available at
\url{https://github.com/OpenGVLab/MM-Interleaved}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MM-Interleavedï¼šåŸºäºå¤šæ¨¡æ€ç‰¹å¾åŒæ­¥å™¨çš„äº¤é”™å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äº’è”ç½‘ä¸Šäº¤é”™å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼ˆå¦‚æ–°é—»ã€åšå®¢ï¼‰çš„æ™®åŠï¼Œå¼€å‘èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæ­¤ç±»æ•°æ®çš„ç”Ÿæˆæ¨¡å‹å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†äº¤é”™å›¾åƒ-æ–‡æœ¬æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦é—®é¢˜åœ¨äºå›ºå®šæ•°é‡çš„è§†è§‰æ ‡è®°æ— æ³•æœ‰æ•ˆåœ°æ•æ‰å›¾åƒç»†èŠ‚ï¼Œå°¤å…¶æ˜¯åœ¨å¤šå›¾åƒåœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MM-Interleavedï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå¤„ç†äº¤é”™å›¾åƒ-æ–‡æœ¬æ•°æ®ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€ç‰¹å¾åŒæ­¥å™¨ï¼ˆMMFSï¼‰
MM-Interleavedå¼•å…¥äº†ä¸€ä¸ªå¤šå°ºåº¦ã€å¤šå›¾åƒç‰¹å¾åŒæ­¥å™¨æ¨¡å—ï¼ˆMMFSï¼‰ï¼Œå…è®¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç›´æ¥è®¿é—®æ¥è‡ªå…ˆå‰ä¸Šä¸‹æ–‡çš„ç»†ç²’åº¦å›¾åƒç‰¹å¾ã€‚MMFSåŸºäºå¯å˜å½¢ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å¤šå°ºåº¦å›¾åƒç‰¹å¾å›¾ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¼¥è¡¥äº†è§†è§‰æ ‡è®°æ•°é‡æœ‰é™å¯¼è‡´çš„å›¾åƒç»†èŠ‚ä¸¢å¤±é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç«¯åˆ°ç«¯é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒ
MM-Interleavedåœ¨é…å¯¹å’Œäº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œç«¯åˆ°ç«¯é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒé˜¶æ®µè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªå¤æ‚çš„æ¨¡æ€æŒ‡ä»¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-Interleavedåœ¨è¯†åˆ«å¤šæ¨¡æ€æŒ‡ä»¤åçš„è§†è§‰ç»†èŠ‚å’Œç”Ÿæˆä¸€è‡´å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMM-Interleavedå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ ‡è®°æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MM-Interleavedçš„æå‡ºä¸ºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•MMFSå¯ä»¥æœ‰æ•ˆè§£å†³è§†è§‰æ ‡è®°æ•°é‡æœ‰é™å¯¼è‡´çš„å›¾åƒç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼ŒMM-Interleavedçš„ç«¯åˆ°ç«¯é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒç­–ç•¥ä¹Ÿä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†å‚è€ƒã€‚

## unified-io-2--scaling-autoregressive-multimodal-models-with-vision--language--audio--and-action
### Abstract
We present Unified-IO 2, the first autoregressive multimodal model that is
capable of understanding and generating image, text, audio, and action. To
unify different modalities, we tokenize inputs and outputs -- images, text,
audio, action, bounding boxes, etc., into a shared semantic space and then
process them with a single encoder-decoder transformer model. Since training
with such diverse modalities is challenging, we propose various architectural
improvements to stabilize model training. We train our model from scratch on a
large multimodal pre-training corpus from diverse sources with a multimodal
mixture of denoisers objective. To learn an expansive set of skills, such as
following multimodal instructions, we construct and finetune on an ensemble of
120 datasets with prompts and augmentations. With a single unified model,
Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and
strong results in more than 35 benchmarks, including image generation and
understanding, natural language understanding, video and audio understanding,
and robotic manipulation. We release all our models to the research community.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç»Ÿä¸€è¾“å…¥è¾“å‡º2ï¼šæ‰©å±•è‡ªå›å½’å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¶µç›–è§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘å’ŒåŠ¨ä½œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæ„å»ºèƒ½å¤Ÿç†è§£å¹¶ç”Ÿæˆå¤šç§æ¨¡æ€ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’ŒåŠ¨ä½œï¼‰çš„æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†çš„å¤šæ ·æ€§ã€æ¨¡å‹æ¶æ„çš„è®¾è®¡ã€è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ç­‰ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸åªèƒ½å¤„ç†æœ‰é™çš„æ¨¡æ€ï¼Œæˆ–è€…éœ€è¦ä¾èµ–é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
Unified-IO 2 æ˜¯ä¸€ä¸ªè‡ªå›å½’å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’ŒåŠ¨ä½œã€‚ä¸ºäº†ç»Ÿä¸€ä¸åŒçš„æ¨¡æ€ï¼Œè¯¥æ¨¡å‹å°†è¾“å…¥å’Œè¾“å‡ºï¼ˆåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ã€åŠ¨ä½œã€è¾¹ç•Œæ¡†ç­‰ï¼‰ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œå¹¶ä½¿ç”¨å•ä¸ªç¼–ç å™¨-è§£ç å™¨Transformeræ¨¡å‹è¿›è¡Œå¤„ç†ã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†å¤šç§æ¶æ„æ”¹è¿›ï¼ŒåŒ…æ‹¬äºŒç»´æ—‹è½¬åµŒå…¥ã€QKå½’ä¸€åŒ–å’Œç¼©æ”¾ä½™å¼¦æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ä½¿ç”¨äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ··åˆå»å™ªå™¨ç›®æ ‡ï¼Œå¹¶ç»“åˆäº†è¶…è¿‡120ä¸ªæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Unified-IO 2 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆå’Œç†è§£ã€è‡ªç„¶è¯­è¨€ç†è§£ã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£ä»¥åŠæœºå™¨äººæ“ä½œç­‰ã€‚è¯¥æ¨¡å‹åœ¨GRITåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼ºå¤§çš„ç»“æœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Unified-IO 2 çš„è®¾è®¡ä¸ºæ„å»ºé€šç”¨å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
* **ç»Ÿä¸€ä»»åŠ¡è¡¨ç¤º**ï¼šå°†ä¸åŒæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œä½¿ç”¨å•ä¸ªæ¨¡å‹è¿›è¡Œå¤„ç†ã€‚
* **æ¶æ„æ”¹è¿›**ï¼šé‡‡ç”¨äºŒç»´æ—‹è½¬åµŒå…¥ã€QKå½’ä¸€åŒ–å’Œç¼©æ”¾ä½™å¼¦æ³¨æ„åŠ›æœºåˆ¶ç­‰æ”¹è¿›ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
* **å¤šæ¨¡æ€æ··åˆå»å™ªå™¨ç›®æ ‡**ï¼šç»“åˆå»å™ªå’Œç”Ÿæˆä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡å·ã€‚
* **å¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒ**ï¼šä½¿ç”¨è¶…è¿‡120ä¸ªæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½ã€‚

### ğŸ“š å‚è€ƒæ–‡çŒ®
* [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172)

## generative-multimodal-models-are-in-context-learners
### Abstract
The human ability to easily solve multimodal tasks in context (i.e., with
only a few demonstrations or simple instructions), is what current multimodal
systems have largely struggled to imitate. In this work, we demonstrate that
the task-agnostic in-context learning capabilities of large multimodal models
can be significantly enhanced by effective scaling-up. We introduce Emu2, a
generative multimodal model with 37 billion parameters, trained on large-scale
multimodal sequences with a unified autoregressive objective. Emu2 exhibits
strong multimodal in-context learning abilities, even emerging to solve tasks
that require on-the-fly reasoning, such as visual prompting and object-grounded
generation. The model sets a new record on multiple multimodal understanding
tasks in few-shot settings. When instruction-tuned to follow specific
instructions, Emu2 further achieves new state-of-the-art on challenging tasks
such as question answering benchmarks for large multimodal models and
open-ended subject-driven generation. These achievements demonstrate that Emu2
can serve as a base model and general-purpose interface for a wide range of
multimodal tasks. Code and models are publicly available to facilitate future
research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Emu2ï¼šå¤§å‹ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€ä»»åŠ¡æ¶‰åŠç†è§£å’Œç”Ÿæˆå•æ¨¡æ€æˆ–å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€ç³»ç»Ÿå¾€å¾€éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡æ¶æ„å’Œæ”¶é›†å¤§é‡ç›‘ç£è®­ç»ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»èƒ½å¤Ÿè½»æ¾åœ°åœ¨ä¸Šä¸‹æ–‡ä¸­è§£å†³æ–°ä»»åŠ¡ï¼Œå³é€šè¿‡å°‘é‡æ¼”ç¤ºæˆ–ç®€å•æŒ‡ä»¤ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æœ‰æ•ˆæ‰©å±•å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¢å¼ºå…¶åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¿™ç§èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šEmu2 æ¨¡å‹
æœ¬æ–‡æå‡ºäº† Emu2ï¼Œä¸€ä¸ªå…·æœ‰ 370 äº¿å‚æ•°çš„ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤§å‹å¤šæ¨¡æ€åºåˆ—ä¸Šè¿›è¡Œç»Ÿä¸€çš„è‡ªå›å½’ç›®æ ‡è®­ç»ƒã€‚Emu2 åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç”šè‡³èƒ½å¤Ÿè§£å†³éœ€è¦å³æ—¶æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰æç¤ºå’ŒåŸºäºå¯¹è±¡çš„ç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸Šä¸‹æ–‡å­¦ä¹ 
Emu2 åœ¨æ ‡å‡†å¤šæ¨¡æ€æ•°æ®é›†ä¸Šï¼Œé€šè¿‡å°‘é‡ç¤ºä¾‹æˆ–æŒ‡ä»¤è¿›è¡Œå­¦ä¹ çš„èƒ½åŠ›å¾—åˆ°äº†è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼ŒEmu2 åœ¨ä¸¤ç§åœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼š(a) å°‘æ ·æœ¬è®¾ç½®ï¼Œå…è®¸å°½å¯èƒ½å¤šçš„ç¤ºä¾‹é€‚åº”æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼›(b) æŒ‡ä»¤è°ƒæ•´ï¼Œæ¨¡å‹è¢«è°ƒæ•´ä¸ºéµå¾ªç‰¹å®šæŒ‡ä»¤ã€‚Emu2 åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹åœ¨å¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä¾‹å¦‚åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Emu2 åœ¨å¤šä¸ªå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„è®°å½•ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºè‰²ã€‚å½“é€šè¿‡æŒ‡ä»¤è°ƒæ•´ä»¥éµå¾ªç‰¹å®šæŒ‡ä»¤æ—¶ï¼ŒEmu2 åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆæœï¼Œä¾‹å¦‚å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é—®ç­”åŸºå‡†å’Œå¼€æ”¾å¼ä¸»é¢˜é©±åŠ¨ç”Ÿæˆã€‚è¿™äº›æˆæœè¡¨æ˜ Emu2 å¯ä»¥ä½œä¸ºå„ç§å¤šæ¨¡æ€ä»»åŠ¡çš„åŸºæ¨¡å‹å’Œé€šç”¨æ¥å£ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ Emu2 æ¨¡å‹å±•ç¤ºäº†å¤§å‹ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„çªç ´ã€‚é€šè¿‡æœ‰æ•ˆæ‰©å±•æ¨¡å‹ï¼ŒEmu2 èƒ½å¤Ÿæ¨¡ä»¿äººç±»åœ¨ä¸Šä¸‹æ–‡ä¸­è§£å†³æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒEmu2 è¿˜å±•ç¤ºäº†åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­å¯æ§è§†è§‰ç”Ÿæˆçš„èƒ½åŠ›ï¼Œä¾‹å¦‚åŸºäºä¸»é¢˜/æ–‡æœ¬çš„ç”Ÿæˆã€‚è¿™äº›æˆæœä¸ºå¼€å‘å¯é€‚åº”ã€é€šç”¨çš„å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†é‡è¦çš„å¯ç¤ºã€‚

## xgen-mm-(blip-3)--a-family-of-open-large-multimodal-models
### Abstract
This report introduces xGen-MM (also known as BLIP-3), a framework for
developing Large Multimodal Models (LMMs). The framework comprises meticulously
curated datasets, a training recipe, model architectures, and a resulting suite
of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen
initiative on foundation AI models. Our models undergo rigorous evaluation
across a range of tasks, including both single and multi-image benchmarks. Our
pre-trained base model exhibits strong in-context learning capabilities and the
instruction-tuned model demonstrates competitive performance among open-source
LMMs with similar model sizes. In addition, we introduce a safety-tuned model
with DPO, aiming to mitigate harmful behaviors such as hallucinations and
improve safety. We open-source our models, curated large-scale datasets, and
our fine-tuning codebase to facilitate further advancements in LMM research.
Associated resources will be available on our project page above.
### ğŸŒŸ è®ºæ–‡è§£è¯» | xGen-MM (BLIP-3): å¼€æ”¾å¼å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å®¶æ—

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å› å…¶æ½œåœ¨çš„åº”ç”¨å’Œæ¶Œç°èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå¼€æºæ¨¡å‹ä¸é—­æºæ¨¡å‹ä¹‹é—´åœ¨å¼€æ”¾æƒé‡ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®é›†è·å–æ–¹é¢ä»å­˜åœ¨å·®è·ï¼Œè¿™é™åˆ¶äº†å¼€æºç¤¾åŒºå¯¹LMMsçš„å¤åˆ¶ã€ç†è§£å’Œæ”¹è¿›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„LMMsåœ¨æ•°æ®è§„æ¨¡ã€è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥è¾¾åˆ°ä¸é—­æºæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ•°æ®é›†çš„ä¸°å¯Œæ€§å’Œè§„æ¨¡
xGen-MM (BLIP-3) æ¡†æ¶åˆ©ç”¨äº†å¤šç§å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…æ‹¬ MINT-1Tã€OBELICSã€BLIP3-KALEã€BLIP3-OCR-200M å’Œ BLIP3-GROUNDING-50Mï¼Œä»¥æä¾›æ›´ä¸°å¯Œã€æ›´å¤§è§„æ¨¡å’Œæ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„é¢†åŸŸï¼ŒåŒ…æ‹¬ HTMLã€PDFã€ArXivã€å›¾åƒã€æ–‡æœ¬å’Œè§†è§‰å®šä½ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å‹æ¶æ„çš„æ”¹è¿›
xGen-MM (BLIP-3) æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ›´å¯æ‰©å±•çš„è§†è§‰æ ‡è®°é‡‡æ ·å™¨ï¼ˆperceiver resamplerï¼‰æ¥æ›¿ä»£ Q-Former å±‚ï¼Œå¹¶ç®€åŒ–äº†è®­ç»ƒç›®æ ‡ï¼Œä½¿å…¶ä»…å…³æ³¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­æ–‡æœ¬æ ‡è®°çš„è‡ªå›å½’æŸå¤±ã€‚è¿™ç§æ¶æ„æ”¹è¿›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒæ–¹æ³•çš„ä¼˜åŒ–
xGen-MM (BLIP-3) æ¡†æ¶é‡‡ç”¨äº†å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥æé«˜æ¨¡å‹å¯¹ç”¨æˆ·æŸ¥è¯¢çš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå®‰å…¨å¾®è°ƒæ¥æé«˜æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œä»è€Œå‡å°‘å¹»è§‰å’Œæœ‰å®³è¡Œä¸ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
xGen-MM (BLIP-3) æ¡†æ¶åœ¨å¤šä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€OCR å’Œè§†è§‰å®šä½ã€‚ä¸ç°æœ‰çš„å¼€æº LMMs ç›¸æ¯”ï¼ŒxGen-MM (BLIP-3) åœ¨å•å›¾åƒå’Œå¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDPO å’Œå®‰å…¨å¾®è°ƒè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œä½¿å…¶æ›´é€‚ç”¨äºå®é™…åº”ç”¨åœºæ™¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
xGen-MM (BLIP-3) æ¡†æ¶ä¸º LMMs çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†å®è´µçš„èµ„æºå’Œå·¥å…·ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ã€æ•°æ®é›†å’Œå¾®è°ƒä»£ç åº“ã€‚ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…å¯ä»¥åˆ©ç”¨è¿™äº›èµ„æºæ¥æ¢ç´¢ LMMs çš„æ½œåŠ›å’Œæ¶Œç°èƒ½åŠ›ï¼Œå¹¶å°†å…¶åº”ç”¨äºå„ç§å®é™…åº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼ŒxGen-MM (BLIP-3) æ¡†æ¶çš„å®‰å…¨å¾®è°ƒæ–¹æ³•ä¹Ÿä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å¯é çš„ AI æ¨¡å‹æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## vl-gpt--a-generative-pre-trained-transformer-for-vision-and-language-understanding-and-generation
### Abstract
In this work, we introduce Vision-Language Generative Pre-trained Transformer
(VL-GPT), a transformer model proficient at concurrently perceiving and
generating visual and linguistic data. VL-GPT achieves a unified pre-training
approach for both image and text modalities by employing a straightforward
auto-regressive objective, thereby enabling the model to process image and text
as seamlessly as a language model processes text. To accomplish this, we
initially propose a novel image tokenizer-detokenizer framework for visual
data, specifically designed to transform raw images into a sequence of
continuous embeddings and reconstruct them accordingly. In combination with the
existing text tokenizer and detokenizer, this framework allows for the encoding
of interleaved image-text data into a multimodal sequence, which can
subsequently be fed into the transformer model. Consequently, VL-GPT can
perform large-scale pre-training on multimodal corpora utilizing a unified
auto-regressive objective (i.e., next-token prediction). Upon completion of
pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance
across a diverse range of vision and language understanding and generation
tasks, including image captioning, visual question answering, text-to-image
generation, and more. Additionally, the pre-trained model retrains in-context
learning capabilities when provided with multimodal prompts. We further conduct
instruction tuning on our VL-GPT, highlighting its exceptional potential for
multimodal assistance. The source code and model weights shall be released.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VL-GPTï¼šè§†è§‰ä¸è¯­è¨€ç†è§£ä¸ç”Ÿæˆçš„ç”Ÿæˆå¼é¢„è®­ç»ƒTransformer

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„æˆåŠŸï¼Œå¤šæ¨¡æ€ç¤¾åŒºå¯¹å¼€å‘å¤§å‹è§†è§‰-è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLæ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ï¼Œè¾“å…¥å›¾åƒå’Œç”Ÿæˆå›¾åƒçš„å›¾åƒåµŒå…¥ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ¨¡å‹éœ€è¦åˆ†åˆ«å¯¹å›¾åƒç†è§£å’Œç”Ÿæˆè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸”éš¾ä»¥åœ¨å›¾åƒåµŒå…¥ä¸Šå®ç°è‡ªå›å½’è®­ç»ƒæŸå¤±ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå›¾åƒåˆ†è¯å™¨-è§£ç å™¨æ¡†æ¶
ä¸ºäº†å®ç°è§†è§‰åµŒå…¥å’Œæ–‡æœ¬æ ‡è®°çš„ç»Ÿä¸€è‡ªå›å½’è®­ç»ƒç›®æ ‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å›¾åƒåˆ†è¯å™¨-è§£ç å™¨æ¡†æ¶ï¼Œç”¨äºå°†åŸå§‹å›¾åƒè½¬æ¢ä¸ºè¿ç»­è§†è§‰åµŒå…¥å¹¶é‡å»ºå®ƒä»¬ã€‚è¯¥æ¡†æ¶ç”±å›¾åƒåˆ†è¯å™¨å’Œå›¾åƒè§£ç å™¨ç»„æˆï¼Œåˆ†åˆ«è´Ÿè´£å°†å›¾åƒç¼–ç ä¸ºè¿ç»­è§†è§‰åµŒå…¥å’Œè§£ç è§†è§‰åµŒå…¥å›åƒç´ ç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šVL-GPTæ¨¡å‹
VL-GPTæ˜¯ä¸€ä¸ªç”Ÿæˆå¼é¢„è®­ç»ƒTransformeræ¨¡å‹ï¼Œç”¨äºè§†è§‰å’Œè¯­è¨€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹å¯ä»¥ç»Ÿä¸€åœ°è‡ªå›å½’é¢„è®­ç»ƒå¤§è§„æ¨¡å¤šæ¨¡æ€è¯­æ–™åº“ï¼Œå³é¢„æµ‹åŒ…å«è¿ç»­è§†è§‰åµŒå…¥å’Œç¦»æ•£æ–‡æœ¬æ ‡è®°çš„å¤šæ¨¡æ€åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œè€Œæ— éœ€ä»»ä½•åŒºåˆ†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
VL-GPTåœ¨å„ç§VLç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰æ€§æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨æä¾›å¤šæ¨¡æ€æç¤ºæ—¶è¡¨ç°å‡ºæœ‰å¸å¼•åŠ›çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
VL-GPTæ¨¡å‹ä¸ºå¤šæ¨¡æ€ç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œç±»ä¼¼äºGPTå®¶æ—åœ¨NLPä¸­çš„ä½œç”¨ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›ï¼Œå¯ä»¥åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼ŒVL-GPTçš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä½¿å…¶æˆä¸ºä¸€ä¸ªçµæ´»ä¸”æœ‰æ•ˆçš„å¤šæ¨¡æ€é€šç”¨åŠ©æ‰‹ã€‚

## dreamllm--synergistic-multimodal-comprehension-and-creation
### Abstract
This paper presents DreamLLM, a learning framework that first achieves
versatile Multimodal Large Language Models (MLLMs) empowered with frequently
overlooked synergy between multimodal comprehension and creation. DreamLLM
operates on two fundamental principles. The first focuses on the generative
modeling of both language and image posteriors by direct sampling in the raw
multimodal space. This approach circumvents the limitations and information
loss inherent to external feature extractors like CLIP, and a more thorough
multimodal understanding is obtained. Second, DreamLLM fosters the generation
of raw, interleaved documents, modeling both text and image contents, along
with unstructured layouts. This allows DreamLLM to learn all conditional,
marginal, and joint multimodal distributions effectively. As a result, DreamLLM
is the first MLLM capable of generating free-form interleaved content.
Comprehensive experiments highlight DreamLLM's superior performance as a
zero-shot multimodal generalist, reaping from the enhanced learning synergy.
Project page: https://dreamllm.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DreamLLMï¼šå¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ çš„ååŒå­¦ä¹ æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ æ˜¯æœºå™¨æ™ºèƒ½å‘å±•çš„å…³é”®æ–¹å‘ï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMsä¸»è¦å…³æ³¨å¤šæ¨¡æ€ç†è§£ï¼Œè€Œå¤šæ¨¡æ€åˆ›é€ èƒ½åŠ›ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢MLLMsåœ¨å¤šæ¨¡æ€åˆ›é€ æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å®ç°å¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ çš„ååŒå­¦ä¹ ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«¯åˆ°ç«¯ç”Ÿæˆ
ä¸åŒäºç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆä¸­é—´å›¾åƒè¡¨ç¤ºï¼ˆå¦‚CLIPåµŒå…¥ï¼‰ï¼ŒDreamLLMç›´æ¥åœ¨åŸå§‹å¤šæ¨¡æ€ç©ºé—´ä¸­è¿›è¡Œé‡‡æ ·ï¼Œç”Ÿæˆè¯­è¨€å’Œå›¾åƒçš„åéªŒåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¤–éƒ¨ç‰¹å¾æå–å™¨ï¼ˆå¦‚CLIPï¼‰çš„å±€é™æ€§å’Œä¿¡æ¯æŸå¤±ï¼Œå®ç°äº†æ›´å…¨é¢çš„å¤šæ¨¡æ€ç†è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº¤é”™ç”Ÿæˆé¢„è®­ç»ƒï¼ˆI-GPTï¼‰
DreamLLMé€šè¿‡è®­ç»ƒç”Ÿæˆäº¤é”™çš„å¤šæ¨¡æ€è¯­æ–™åº“ï¼ŒåŒæ—¶ç¼–ç å’Œè§£ç äº¤é”™çš„è¯­è¨€-å›¾åƒå¤šæ¨¡æ€è¾“å…¥ã€‚è¿™ç§ç‹¬ç‰¹çš„è®­ç»ƒæ–¹å¼ä½¿å¾—DreamLLMèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ‰€æœ‰æ¡ä»¶ã€è¾¹ç¼˜å’Œè”åˆå¤šæ¨¡æ€åˆ†å¸ƒï¼Œä»è€Œå®ç°è‡ªç”±å½¢å¼çš„äº¤é”™å†…å®¹ç”Ÿæˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
DreamLLMåœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ç†è§£å’Œå†…å®¹åˆ›é€ ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒåˆ°æ–‡æœ¬æè¿°ã€è§†è§‰é—®ç­”ã€æ–‡æœ¬ç›¸å…³é—®ç­”ã€æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆç­‰ã€‚åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ï¼ŒDreamLLMåœ¨MS-COCOæ•°æ®é›†ä¸Šå®ç°äº†8.46çš„FIDåˆ†æ•°ï¼Œå¹¶åœ¨MMBenchå’ŒMM-Vetè¯„ä¼°ä¸­åˆ†åˆ«å–å¾—äº†49.1å’Œ35.9çš„åˆ†æ•°ï¼Œåˆ·æ–°äº†ç°æœ‰è®°å½•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DreamLLMçš„ç«¯åˆ°ç«¯ç”Ÿæˆå’Œäº¤é”™ç”Ÿæˆé¢„è®­ç»ƒæ–¹æ³•ä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶å±•ç¤ºäº†MLLMsåœ¨å¤šæ¨¡æ€åˆ›é€ æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢DreamLLMåœ¨è§†é¢‘æ•°æ®ã€3Då†…å®¹åˆ›é€ ã€å‡ ä½•ä¿æŒä»»åŠ¡ç­‰æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€ï¼Œå®ç°ç»Ÿä¸€çš„é›¶æ ·æœ¬å¤šæ¨¡æ€é€šç”¨æ¨¡å‹ã€‚

## making-llama-see-and-draw-with-seed-tokenizer
### Abstract
The great success of Large Language Models (LLMs) has expanded the potential
of multimodality, contributing to the gradual evolution of General Artificial
Intelligence (AGI). A true AGI agent should not only possess the capability to
perform predefined multi-tasks but also exhibit emergent abilities in an
open-world context. However, despite the considerable advancements made by
recent multimodal LLMs, they still fall short in effectively unifying
comprehension and generation tasks, let alone open-world emergent abilities. We
contend that the key to overcoming the present impasse lies in enabling text
and images to be represented and processed interchangeably within a unified
autoregressive Transformer. To this end, we introduce SEED, an elaborate image
tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.
We identify two crucial design principles: (1) Image tokens should be
independent of 2D physical patch positions and instead be produced with a 1D
causal dependency, exhibiting intrinsic interdependence that aligns with the
left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens
should capture high-level semantics consistent with the degree of semantic
abstraction in words, and be optimized for both discriminativeness and
reconstruction during the tokenizer training phase. With SEED tokens, LLM is
able to perform scalable multimodal autoregression under its original training
recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by
large-scale pretraining and instruction tuning on the interleaved textual and
visual data, demonstrating impressive performance on a broad range of
multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has
exhibited compositional emergent abilities such as multi-turn in-context
multimodal generation, acting like your AI assistant.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©LLaMAâ€œçœ‹â€ä¸â€œç”»â€ï¼šSEED Tokenizerçš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ¼”è¿›å¥ å®šäº†åŸºç¡€ã€‚ç„¶è€Œï¼Œå°½ç®¡å¤šæ¨¡æ€LLMså–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œæ›´ä¸ç”¨è¯´åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„æ¶Œç°èƒ½åŠ›ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œå…‹æœè¿™ä¸€å›°å¢ƒçš„å…³é”®åœ¨äºä½¿æ–‡æœ¬å’Œå›¾åƒèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„è‡ªå›å½’Transformerä¸­ç›¸äº’è½¬æ¢å’Œè¡¨ç¤ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSEED Tokenizer
æœ¬æ–‡æå‡ºäº†SEEDï¼Œä¸€ç§åŸºäºVQçš„å›¾åƒåˆ†è¯å™¨ï¼Œå®ƒäº§ç”Ÿå…·æœ‰1Då› æœä¾èµ–æ€§å’Œå¿…è¦é«˜çº§è¯­ä¹‰çš„ç¦»æ•£è§†è§‰ä»£ç ï¼Œä»¥ä¾¿äºè§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚SEED Tokenizerçš„è®¾è®¡åŸåˆ™åŒ…æ‹¬ï¼š
1. å›¾åƒtokenåº”ç‹¬ç«‹äº2Dç‰©ç†å—ä½ç½®ï¼Œè€Œæ˜¯é€šè¿‡1Då› æœä¾èµ–æ€§ç”Ÿæˆï¼Œè¡¨ç°å‡ºä¸LLMsä¸­ä»å·¦åˆ°å³çš„è‡ªå›å½’é¢„æµ‹æœºåˆ¶ç›¸ä¸€è‡´çš„å†…åœ¨ç›¸äº’ä¾èµ–æ€§ã€‚
2. å›¾åƒtokenåº”æ•è·ä¸è¯è¯­çš„è¯­ä¹‰æŠ½è±¡ç¨‹åº¦ä¸€è‡´çš„é«˜çº§è¯­ä¹‰ï¼Œå¹¶åœ¨åˆ†è¯å™¨è®­ç»ƒé˜¶æ®µä¼˜åŒ–åŒºåˆ†åº¦å’Œé‡å»ºèƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSEED-LLaMA
æœ¬æ–‡é€šè¿‡å°†é¢„è®­ç»ƒçš„LLMä¸SEEDåˆ†è¯å™¨ç›¸ç»“åˆï¼Œæ„å»ºäº†SEED-LLaMAæ¨¡å‹ã€‚SEED-LLaMAåœ¨äº¤é”™æ–‡æœ¬å’Œè§†è§‰æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥å®ç°ä¸‹ä¸€ä¸ªè¯é¢„æµ‹çš„è®­ç»ƒç›®æ ‡ã€‚è¿™ç§æ˜“äºå®ç°ä¸”ç»Ÿä¸€çš„ä»£ç†ä»»åŠ¡ä¿ƒè¿›äº†å¯æ‰©å±•çš„å¤šæ¨¡æ€é¢„è®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SEED-LLaMAåœ¨å¹¿æ³›çš„è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€å›¾åƒ/è§†é¢‘é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼ŒSEED-LLaMAè¿˜å±•ç¤ºäº†å¤šè½®ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ç”Ÿæˆç­‰æ¶Œç°èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»çš„AIåŠ©æ‰‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„SEED Tokenizerå’ŒSEED-LLaMAæ¨¡å‹ä¸ºå¤šæ¨¡æ€LLMsçš„è®­ç»ƒå’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚SEED Tokenizerçš„è®¾è®¡åŸåˆ™å’ŒSEED-LLaMAçš„æ¶Œç°èƒ½åŠ›ä¸ºæœªæ¥å¤šæ¨¡æ€AIçš„å‘å±•æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## on-the-performance-of-multimodal-language-models
### Abstract
Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ€§èƒ½æ¢ç©¶ï¼šè¿ˆå‘æ›´å¼ºå¤§çš„AI

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„è®¸å¤šåº”ç”¨åœºæ™¯æ¶‰åŠå¤šæ¨¡æ€æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬ï¼Œéœ€è¦ç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯è¿›è¡Œå‡†ç¡®å’Œé²æ£’çš„æ¨ç†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿‘å¹´æ¥ç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å°†å¤šæ¨¡æ€èƒ½åŠ›é›†æˆåˆ°LLMsä¸­ï¼Œé€šè¿‡æ•´åˆé¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ¥å®ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡å¯¹ä¸åŒçš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤æ‚æ¨ç†ã€å¯¹è¯ã€å›¾åƒæè¿°ã€å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰å’ŒäºŒå…ƒåˆ†ç±»ã€‚é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•å’Œæ¶ˆèå®éªŒï¼Œæ­ç¤ºäº†åœ¨å°†å¤šæ¨¡æ€èƒ½åŠ›é›†æˆåˆ°LLMsæ—¶ï¼Œæ¶æ„é€‰æ‹©çš„å…³é”®è§è§£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ›´å¤§çš„è§†è§‰ç¼–ç å™¨ï¼ˆViT-gï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ‰€æœ‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ•æ‰æ›´ä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå¾®è°ƒè§†è§‰å¤´éƒ¨ï¼ˆä¾‹å¦‚Q-Formerï¼‰ä¹Ÿæ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºä¸‹æ¸¸ä»»åŠ¡æå–æ›´å¥½çš„è¡¨ç¤ºï¼Œå¹¶åŠ å¿«è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤šæ¨¡æ€è§†è§‰å¤´éƒ¨å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ¯”ä½¿ç”¨ä»…å›¾åƒå¤´éƒ¨æ›´å¥½çš„ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ„å»ºæœ‰æ•ˆçš„å¤šæ¨¡æ€LLMsæ—¶ï¼Œåº”å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
1. ä½¿ç”¨æ›´å¤§çš„è§†è§‰ç¼–ç å™¨ä»¥æ•æ‰æ›´ä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºã€‚
2. å¾®è°ƒè§†è§‰å¤´éƒ¨ä»¥æå–æ›´å¥½çš„è¡¨ç¤ºï¼Œå¹¶åŠ å¿«è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚
3. åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œå…³æ³¨æ•°æ®å¤šæ ·æ€§å’Œä»»åŠ¡å¤šæ ·æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
4. æ¢ç´¢å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»¥å®ç°æ›´å…ˆè¿›çš„æ€§èƒ½ã€‚
5. å…³æ³¨ç¼“è§£å¤šæ¨¡æ€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ï¼Œä½¿å…¶æ›´å¯é å’Œæœ‰ç”¨ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡å¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶æ­ç¤ºäº†æ„å»ºæœ‰æ•ˆå¤šæ¨¡æ€LLMsçš„å…³é”®ç»„ä»¶å’Œç­–ç•¥ã€‚ç ”ç©¶ç»“æœä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œä»»åŠ¡å¤šæ ·æ€§åœ¨æé«˜æ¨¡å‹èƒ½åŠ›æ–¹é¢çš„é‡è¦æ€§ã€‚

## unified-language-vision-pretraining-in-llm-with-dynamic-discrete-visual-tokenization
### Abstract
Recently, the remarkable advance of the Large Language Model (LLM) has
inspired researchers to transfer its extraordinary reasoning capability to both
vision and language data. However, the prevailing approaches primarily regard
the visual input as a prompt and focus exclusively on optimizing the text
generation process conditioned upon vision content by a frozen LLM. Such an
inequitable treatment of vision and language heavily constrains the model's
potential. In this paper, we break through this limitation by representing both
vision and language in a unified form. Specifically, we introduce a
well-designed visual tokenizer to translate the non-linguistic image into a
sequence of discrete tokens like a foreign language that LLM can read. The
resulting visual tokens encompass high-level semantics worthy of a word and
also support dynamic sequence length varying from the image. Coped with this
tokenizer, the presented foundation model called LaVIT can handle both image
and text indiscriminately under the same generative learning paradigm. This
unification empowers LaVIT to serve as an impressive generalist interface to
understand and generate multi-modal content simultaneously. Extensive
experiments further showcase that it outperforms the existing models by a large
margin on massive vision-language tasks. Our code and models are available at
https://github.com/jy0205/LaVIT.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LaVITï¼šç»Ÿä¸€è¯­è¨€-è§†è§‰é¢„è®­ç»ƒï¼Œçªç ´å¤šæ¨¡æ€ç†è§£çš„å±€é™

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æ¿€å‘äº†ç ”ç©¶äººå‘˜å°†å…¶åº”ç”¨äºè§†è§‰å’Œè¯­è¨€æ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å°†è§†è§‰è¾“å…¥è§†ä¸ºæç¤ºï¼Œå¹¶é€šè¿‡å†»ç»“çš„LLMä¼˜åŒ–åŸºäºè§†è§‰å†…å®¹çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿™ç§å¯¹è§†è§‰å’Œè¯­è¨€çš„ä¸å¹³ç­‰å¤„ç†ä¸¥é‡é™åˆ¶äº†æ¨¡å‹çš„æ½œåŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€è¡¨ç¤ºå½¢å¼
æœ¬æ–‡æå‡ºäº†LaVITï¼ˆLanguage-VIsion Transformerï¼‰ï¼Œä¸€ç§æ–°çš„é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå®ƒç»§æ‰¿äº†LLMæˆåŠŸçš„é¢„æµ‹ä¸‹ä¸€ä¸ªå›¾åƒ/æ–‡æœ¬æ ‡è®°çš„è‡ªå›å½’ç”Ÿæˆå­¦ä¹ èŒƒå¼ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒLaVITå¼•å…¥äº†ä¸€ç§åŠ¨æ€è§†è§‰æ ‡è®°åŒ–æœºåˆ¶ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºLLMå¯ä»¥ç†è§£çš„ç¦»æ•£æ ‡è®°åºåˆ—ï¼Œä»è€Œåœ¨ç»Ÿä¸€çš„ç”Ÿæˆç›®æ ‡ä¸‹åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è§†è§‰æ ‡è®°åŒ–
LaVITçš„è§†è§‰æ ‡è®°åŒ–æœºåˆ¶åŒ…æ‹¬ä¸€ä¸ªé€‰æ‹©å™¨å’Œåˆå¹¶å™¨ã€‚é€‰æ‹©å™¨é¦–å…ˆå†³å®šå“ªäº›è§†è§‰å—åŒ…å«ä¿¡æ¯ä¸°å¯Œçš„è¯­ä¹‰ï¼Œå¹¶å°†å…¶é€‰ä¸­ä»¥ç¼–ç æ•´ä¸ªå›¾åƒã€‚åˆå¹¶å™¨è¿›ä¸€æ­¥å‹ç¼©æœªé€‰ä¸­çš„å—åˆ°ä¿ç•™çš„å—ä¸Šï¼Œä»¥å‡å°‘è§†è§‰å—ä¹‹é—´çš„å†—ä½™ï¼Œä»è€Œä¸ºä¸åŒçš„å›¾åƒç”ŸæˆåŠ¨æ€é•¿åº¦çš„åºåˆ—ã€‚ä¿ç•™çš„è§†è§‰æ ‡è®°è¿›ä¸€æ­¥é‡åŒ–ä¸ºç¦»æ•£ä»£ç ï¼Œä½œä¸ºé¢„è®­ç»ƒæœŸé—´è§†è§‰æ ‡è®°çš„ç›‘ç£ä¿¡å·ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
LaVITåœ¨å¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLaVITåœ¨ç†è§£å’Œç”Ÿæˆå¤šæ¨¡æ€å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ¥å—å¤šç§æ¨¡æ€ç»„åˆä½œä¸ºæç¤ºï¼Œç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶ç†è§£å›¾åƒå†…å®¹ï¼Œç”Ÿæˆç®€æ´çš„æ–‡æœ¬æè¿°ï¼Œå›ç­”å…³äºå›¾åƒç»†èŠ‚çš„å„ç§é—®é¢˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LaVITçš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡å°†è§†è§‰å’Œè¯­è¨€ç»Ÿä¸€è¡¨ç¤ºä¸ºç¦»æ•£æ ‡è®°ï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€çš„ç”Ÿæˆç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ å¤šæ¨¡æ€äº¤äº’å’Œå¯¹é½ï¼Œä»è€Œå®ç°å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ºæ„å»ºæ›´é€šç”¨ã€æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚

## planting-a-seed-of-vision-in-large-language-model
### Abstract
We present SEED, an elaborate image tokenizer that empowers Large Language
Models (LLMs) with the emergent ability to SEE and Draw at the same time.
Research on image tokenizers has previously reached an impasse, as frameworks
employing quantized visual tokens have lost prominence due to subpar
performance and convergence in multimodal comprehension (compared to BLIP-2,
etc.) or generation (compared to Stable Diffusion, etc.). Despite the
limitations, we remain confident in its natural capacity to unify visual and
textual representations, facilitating scalable multimodal training with LLM's
original recipe. In this study, we identify two crucial principles for the
architecture and training of SEED that effectively ease subsequent alignment
with LLMs. (1) Image tokens should be independent of 2D physical patch
positions and instead be produced with a 1D causal dependency, exhibiting
intrinsic interdependence that aligns with the left-to-right autoregressive
prediction mechanism in LLMs. (2) Image tokens should capture high-level
semantics consistent with the degree of semantic abstraction in words, and be
optimized for both discriminativeness and reconstruction during the tokenizer
training phase. As a result, the off-the-shelf LLM is able to perform both
image-to-text and text-to-image generation by incorporating our SEED through
efficient LoRA tuning. Comprehensive multimodal pretraining and instruction
tuning, which may yield improved results, are reserved for future
investigation. This version of SEED was trained in 5.7 days using only 64 V100
GPUs and 5M publicly available image-text pairs. Our preliminary study
emphasizes the great potential of discrete visual tokens in versatile
multimodal LLMs and the importance of proper image tokenizers in broader
research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¤å…¥è§†è§‰ç§å­

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„æ˜¾è‘—æˆåŠŸï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å°†è§†è§‰ä¿¡æ¯æ•´åˆåˆ°LLMsä¸­ï¼Œä»¥å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€LLMsåœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå°šæœªè¾¾åˆ°LLMsåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šçš„æ°´å¹³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSEEDçš„å›¾åƒåˆ†è¯å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåˆ†è¯å™¨åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œå¹¶ä½¿LLMsèƒ½å¤ŸåŒæ—¶è¿›è¡Œâ€œçœ‹â€å’Œâ€œç”»â€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå› æœä¾èµ–çš„å›¾åƒåˆ†è¯å™¨
SEEDé‡‡ç”¨å› æœQ-Formerå°†2Då›¾åƒç‰¹å¾è½¬æ¢ä¸ºå…·æœ‰1Då› æœä¾èµ–å…³ç³»çš„è¯­ä¹‰åµŒå…¥ï¼Œä»è€Œä¸LLMsä¸­çš„è‡ªå›å½’é¢„æµ‹æœºåˆ¶ç›¸åŒ¹é…ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å›¾åƒåˆ†è¯å™¨èƒ½å¤Ÿæ›´å¥½åœ°ä¸LLMsè¿›è¡Œå¯¹é½ï¼Œå¹¶æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜å±‚æ¬¡çš„è¯­ä¹‰è¡¨ç¤º
SEEDçš„å›¾åƒåˆ†è¯å™¨ä¸ä»…æ•è·äº†å›¾åƒçš„ä½å±‚æ¬¡ç»†èŠ‚ï¼Œè¿˜æ•æ‰äº†å›¾åƒçš„é«˜å±‚æ¬¡è¯­ä¹‰ä¿¡æ¯ã€‚è¿™ä½¿å¾—å›¾åƒåˆ†è¯å™¨èƒ½å¤Ÿæ›´å¥½åœ°ä¸LLMsä¸­çš„æ–‡æœ¬åˆ†è¯å™¨è¿›è¡Œå¯¹é½ï¼Œå¹¶æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SEEDåœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨SEEDå¯¹LLMsè¿›è¡Œå¾®è°ƒï¼ŒSEED-OPT2.7Båœ¨é›¶æ ·æœ¬å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„SEEDå›¾åƒåˆ†è¯å™¨ä¸ºå¤šæ¨¡æ€LLMsçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚SEEDçš„è®¾è®¡åŸåˆ™å’Œè®­ç»ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä»¥å®ç°æ›´å¥½çš„æ€§èƒ½å’Œæ›´ä½çš„è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒSEEDçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç¦»æ•£è§†è§‰åˆ†è¯å™¨åœ¨å¤šæ¨¡æ€LLMsä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€LLMsçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## emu--generative-pretraining-in-multimodality
### Abstract
We present Emu, a Transformer-based multimodal foundation model, which can
seamlessly generate images and texts in multimodal context. This omnivore model
can take in any single-modality or multimodal data input indiscriminately
(e.g., interleaved image, text and video) through a one-model-for-all
autoregressive training process. First, visual signals are encoded into
embeddings, and together with text tokens form an interleaved input sequence.
Emu is then end-to-end trained with a unified objective of classifying the next
text token or regressing the next visual embedding in the multimodal sequence.
This versatile multimodality empowers the exploration of diverse pretraining
data sources at scale, such as videos with interleaved frames and text,
webpages with interleaved images and text, as well as web-scale image-text
pairs and video-text pairs. Emu can serve as a generalist multimodal interface
for both image-to-text and text-to-image tasks, and supports in-context image
and text generation. Across a broad range of zero-shot/few-shot tasks including
image captioning, visual question answering, video question answering and
text-to-image generation, Emu demonstrates superb performance compared to
state-of-the-art large multimodal models. Extended capabilities such as
multimodal assistants via instruction tuning are also demonstrated with
impressive performance.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Emuï¼šå¤šæ¨¡æ€ç”Ÿæˆå¼é¢„è®­ç»ƒçš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„çªç ´ï¼Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¹Ÿé€æ¸å…´èµ·ï¼Œæ—¨åœ¨èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œå®ç°æ›´ä¸°å¯Œçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LMMså¤§å¤šå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

* **è§†è§‰ä¿¡æ¯å¤„ç†å—é™**ï¼šè®¸å¤šæ¨¡å‹ä»…é¢„æµ‹æ–‡æœ¬tokenï¼Œè€Œå¿½ç•¥äº†å¯¹è§†è§‰ä¿¡æ¯çš„ç›‘ç£ï¼Œé™åˆ¶äº†æ¨¡å‹èƒ½åŠ›ã€‚
* **æ•°æ®æ¥æºå•ä¸€**ï¼šä¸»è¦ä¾èµ–å›¾åƒ-æ–‡æœ¬å¯¹æˆ–æ–‡æ¡£ï¼Œè€Œå¿½ç•¥äº†è§†é¢‘æ•°æ®ä½œä¸ºæ½œåœ¨çš„å¯æ‰©å±•å¤šæ¨¡æ€æ•°æ®æºã€‚
* **åº”ç”¨åœºæ™¯æœ‰é™**ï¼šå¤§å¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œç¼ºä¹é€šç”¨çš„å¤šæ¨¡æ€æ¥å£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡
* Emué‡‡ç”¨è‡ªå›å½’æ–¹å¼ï¼Œå°†å›¾åƒã€æ–‡æœ¬å’Œè§†é¢‘æ•°æ®ç»Ÿä¸€ç¼–ç ä¸ºåºåˆ—ï¼Œå¹¶è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚
* è§†è§‰ä¿¡å·é€šè¿‡EVA-CLIPç¼–ç ä¸ºåµŒå…¥ï¼Œå¹¶ä¸æ–‡æœ¬tokenå½¢æˆäº¤é”™åºåˆ—ã€‚
* Emuçš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå…ƒç´ ï¼Œæ— è®ºæ˜¯æ–‡æœ¬tokenè¿˜æ˜¯è§†è§‰åµŒå…¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCausal Transformer
* ä¸ºäº†æ›´å¥½åœ°æ•æ‰å›¾åƒç‰¹å¾ï¼ŒEmuå¼•å…¥Causal Transformerå°†2Dè§†è§‰ä¿¡å·è½¬æ¢ä¸º1Då› æœåºåˆ—ã€‚
* Causal Transformerç±»ä¼¼äºTransformerè§£ç å™¨ï¼ŒåŒ…å«å› æœè‡ªæ³¨æ„åŠ›å±‚ã€äº¤å‰æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚ã€‚
* é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚ï¼ŒCausal Transformerèšåˆæ¥è‡ªEVA-CLIPçš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶è¾“å‡ºNä¸ªåµŒå…¥ï¼Œæ•æ‰å›¾åƒçš„å› æœä¾èµ–å…³ç³»ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§†è§‰è§£ç å™¨
* Emuä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å°†è§†è§‰åµŒå…¥è§£ç ä¸ºå›¾åƒã€‚
* è§†è§‰è§£ç å™¨åŸºäºStable Diffusionè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶é€šè¿‡å¾®è°ƒè¿›è¡Œä¼˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ
* Emué€šè¿‡åœ¨å¤šæ¨¡æ€å¯¹è¯æ•°æ®ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¹¶æ‰§è¡Œäººç±»æŒ‡ä»¤ã€‚
* æŒ‡ä»¤å¾®è°ƒé‡‡ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¨¡å—ï¼Œä»…å¯¹å¤šæ¨¡æ€å»ºæ¨¡LLMçš„è‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œé€‚é…ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
* Emuåœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è§†é¢‘é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„LMMsã€‚
* Emu-Iï¼ˆæŒ‡ä»¤å¾®è°ƒåçš„Emuï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†å‚æ•°é‡æ›´å¤§çš„æ¨¡å‹ã€‚
* Emuè¿˜å±•ç¤ºäº†åœ¨ä¸Šä¸‹æ–‡æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆã€å›¾åƒæ··åˆã€è§†é¢‘ç†è§£å’Œç°å®ä¸–ç•ŒçŸ¥è¯†æ¨ç†ç­‰æ–¹é¢çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* Emuçš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ–¹æ³•ä¸ºæ„å»ºé€šç”¨çš„å¤šæ¨¡æ€æ¥å£æä¾›äº†æ–°çš„æ€è·¯ã€‚
* Causal Transformeræœ‰æ•ˆåœ°æ•æ‰äº†å›¾åƒçš„å› æœä¾èµ–å…³ç³»ï¼Œæé«˜äº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„å¤„ç†èƒ½åŠ›ã€‚
* è§†è§‰è§£ç å™¨å®ç°äº†ä»è§†è§‰åµŒå…¥åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œæ‹“å±•äº†æ¨¡å‹çš„åº”ç”¨åœºæ™¯ã€‚
* å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¹¶æ‰§è¡Œäººç±»æŒ‡ä»¤ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚

### ğŸ“š æ€»ç»“
Emuä½œä¸ºä¸€æ¬¾å¤šæ¨¡æ€ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å…¶ç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ–¹æ³•ã€Causal Transformerã€è§†è§‰è§£ç å™¨å’Œå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒç­‰åˆ›æ–°ç‚¹ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹å’Œåº”ç”¨æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## any-to-any-generation-via-composable-diffusion
### Abstract
We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at https://codi-gen.github.io
### ğŸŒŸ è®ºæ–‡è§£è¯» | CoDiï¼šä»»æ„æ¨¡æ€ç”Ÿæˆçš„æ–°çºªå…ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œè·¨æ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆä¸€ä¸ªæ¨¡æ€ä»å¦ä¸€ä¸ªæ¨¡æ€çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°æ–‡æœ¬ã€æ–‡æœ¬åˆ°å›¾åƒæˆ–æ–‡æœ¬åˆ°éŸ³é¢‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•å¤„ç†å¤šä¸ªæ¨¡æ€å…±å­˜å’Œäº¤äº’çš„æƒ…å†µã€‚è™½ç„¶å¯ä»¥å°†æ¨¡æ€ç‰¹å®šçš„ç”Ÿæˆæ¨¡å‹åœ¨å¤šæ­¥ç”Ÿæˆè®¾ç½®ä¸­ä¸²è”èµ·æ¥ï¼Œä½†æ¯ä¸€æ­¥çš„ç”Ÿæˆèƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå¹¶ä¸”ä¸²è¡Œå¤šæ­¥è¿‡ç¨‹å¯èƒ½æ—¢ç¹çåˆç¼“æ…¢ã€‚æ­¤å¤–ï¼Œç‹¬ç«‹ç”Ÿæˆçš„å•æ¨¡æ€æµåœ¨åæœŸå¤„ç†ä¸­æ‹¼æ¥åœ¨ä¸€èµ·æ—¶ä¸ä¼šä¿æŒä¸€è‡´å’Œå¯¹é½ï¼ˆä¾‹å¦‚ï¼ŒåŒæ­¥çš„è§†é¢‘å’ŒéŸ³é¢‘ï¼‰ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿä»ä»»ä½•ä¸€ç»„è¾“å…¥æ¡ä»¶ç”Ÿæˆä»»ä½•æ¨¡æ€ç»„åˆçš„å…¨é¢ä¸”é€šç”¨çš„æ¨¡å‹ï¼Œä»¥æ›´å‡†ç¡®åœ°æ•æ‰ä¸–ç•Œçš„å¤šæ¨¡æ€æœ¬è´¨å’Œäººç±»çš„ç†è§£ï¼Œæ— ç¼åœ°æ•´åˆæ¥è‡ªå¹¿æ³›æ¥æºçš„ä¿¡æ¯ï¼Œå¹¶ä½¿äººç±»ä¸äººå·¥æ™ºèƒ½çš„äº¤äº’æ›´åŠ æ²‰æµ¸å¼ï¼ˆä¾‹å¦‚ï¼ŒåŒæ—¶ç”Ÿæˆè¿è´¯çš„è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬æè¿°ï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
CoDiï¼Œå³â€œå¯ç»„åˆæ‰©æ•£â€ï¼Œæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†å’Œç”Ÿæˆä»»æ„æ¨¡æ€ç»„åˆçš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒCoDié‡‡ç”¨äº†ä»¥ä¸‹åˆ›æ–°æ–¹æ³•ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯ç»„åˆå¤šæ¨¡æ€æ¡ä»¶
CoDié€šè¿‡å°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘çš„æç¤ºç¼–ç å™¨å¯¹é½åˆ°åŒä¸€ç©ºé—´ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹ä»»ä½•è¾“å…¥/æç¤ºæ¨¡æ€ç»„åˆè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚é€šè¿‡ç®€å•åœ°å¯¹é½åµŒå…¥çš„åŠ æƒæ’å€¼ï¼ŒCoDièƒ½å¤Ÿä½¿ç”¨å•æ¡ä»¶è®­ç»ƒï¼ˆå³åªæœ‰ä¸€ä¸ªè¾“å…¥ï¼‰çš„æ¨¡å‹æ‰§è¡Œé›¶æ ·æœ¬å¤šæ¡ä»¶è®­ç»ƒï¼ˆå³å¤šä¸ªè¾“å…¥ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯ç»„åˆæ‰©æ•£
CoDié¦–å…ˆç‹¬ç«‹è®­ç»ƒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ç§åä¸ºâ€œæ½œåœ¨å¯¹é½â€çš„æ–°æœºåˆ¶ï¼Œè¿™äº›æ‰©æ•£æ¨¡å‹å­¦ä¹ è·¨æ¨¡æ€è¿›è¡Œè”åˆå¤šæ¨¡æ€ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒCoDiåœ¨æ¯ä¸ªæ‰©æ•£å™¨ä¸­æ·»åŠ äº†è·¨æ¨¡æ€æ³¨æ„åŠ›å­å±‚ï¼Œå¹¶å°†ä¸åŒLDMçš„æ½œåœ¨å˜é‡æŠ•å½±åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°åŒæ­¥ç”Ÿæˆç›¸äº’äº¤ç»‡çš„æ¨¡æ€ï¼Œä¾‹å¦‚æ—¶é—´å¯¹é½çš„è§†é¢‘å’ŒéŸ³é¢‘ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
CoDiåœ¨å„ç§åœºæ™¯ä¸‹è¡¨ç°å‡ºå“è¶Šçš„ç”Ÿæˆè´¨é‡ï¼Œå…¶åˆæˆè´¨é‡ä¸å•æ¨¡æ€åˆ°å•æ¨¡æ€çš„æœ€å…ˆè¿›æ°´å¹³ç›¸å½“ç”šè‡³æ›´å¥½ã€‚ä¾‹å¦‚ï¼Œåœ¨éŸ³é¢‘ç”Ÿæˆå’ŒéŸ³é¢‘å­—å¹•æ–¹é¢ï¼ŒCoDiçš„æ€§èƒ½ä¸åŸºäºè‡ªå›å½’å˜æ¢å™¨çš„æœ€å…ˆè¿›æ°´å¹³ç›¸å½“ã€‚æ­¤å¤–ï¼ŒCoDiè¿˜èƒ½å¤Ÿç”Ÿæˆå„ç§æ¨¡æ€ç»„åˆï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°è§†é¢‘+éŸ³é¢‘ã€æ–‡æœ¬åˆ°å›¾åƒ+æ–‡æœ¬+éŸ³é¢‘ä»¥åŠæ–‡æœ¬+éŸ³é¢‘+å›¾åƒåˆ°è§†é¢‘+éŸ³é¢‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
CoDiä¸ºå¤šæ¨¡æ€ç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†çªç ´æ€§çš„è¿›å±•ï¼Œå…¶åˆ›æ–°çš„å¯ç»„åˆæ‰©æ•£æ–¹æ³•ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚CoDiçš„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è·¨æ¨¡æ€æ£€ç´¢ã€å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ å’Œå¤šæ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼ŒCoDiçš„çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”å„ç§åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ç”Ÿæˆå¼è‰ºæœ¯ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ã€‚

## multimodal-unified-attention-networks-for-vision-and-language-interactions
### Abstract
Learning an effective attention mechanism for multimodal data is important in
many vision-and-language tasks that require a synergic understanding of both
the visual and textual contents. Existing state-of-the-art approaches use
co-attention models to associate each visual object (e.g., image region) with
each textual object (e.g., query word). Despite the success of these
co-attention models, they only model inter-modal interactions while neglecting
intra-modal interactions. Here we propose a general `unified attention' model
that simultaneously captures the intra- and inter-modal interactions of
multimodal features and outputs their corresponding attended representations.
By stacking such unified attention blocks in depth, we obtain the deep
Multimodal Unified Attention Network (MUAN), which can seamlessly be applied to
the visual question answering (VQA) and visual grounding tasks. We evaluate our
MUAN models on two VQA datasets and three visual grounding datasets, and the
results show that MUAN achieves top-level performance on both tasks without
bells and whistles.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€ç»Ÿä¸€æ³¨æ„åŠ›ç½‘ç»œï¼šè§†è§‰ä¸è¯­è¨€äº¤äº’çš„æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è®¸å¤šéœ€è¦è§†è§‰å’Œè¯­è¨€å†…å®¹ååŒç†è§£çš„è§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸­ï¼Œå­¦ä¹ æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å¯¹äºå¤šæ¨¡æ€æ•°æ®è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ä½¿ç”¨ååŒæ³¨æ„åŠ›æ¨¡å‹å°†æ¯ä¸ªè§†è§‰å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒåŒºåŸŸï¼‰ä¸æ¯ä¸ªæ–‡æœ¬å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼ŒæŸ¥è¯¢è¯ï¼‰ç›¸å…³è”ã€‚å°½ç®¡è¿™äº›ååŒæ³¨æ„åŠ›æ¨¡å‹å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åªæ¨¡æ‹Ÿäº†æ¨¡æ€é—´äº¤äº’ï¼Œè€Œå¿½ç•¥äº†æ¨¡æ€å†…äº¤äº’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„â€œç»Ÿä¸€æ³¨æ„åŠ›â€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒæ—¶æ•è·å¤šæ¨¡æ€ç‰¹å¾çš„æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’ï¼Œå¹¶è¾“å‡ºç›¸åº”çš„å…³æ³¨è¡¨ç¤ºã€‚é€šè¿‡åœ¨æ·±åº¦ä¸Šå †å è¿™æ ·çš„ç»Ÿä¸€æ³¨æ„åŠ›å—ï¼Œæˆ‘ä»¬è·å¾—äº†æ·±åº¦å¤šæ¨¡æ€ç»Ÿä¸€æ³¨æ„åŠ›ç½‘ç»œï¼ˆMUANï¼‰ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°åº”ç”¨äºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§†è§‰å®šä½ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹
æœ¬æ–‡å°†å•æ¨¡æ€çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹æ‰©å±•ä¸ºç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è¡¨å¾å¤šæ¨¡æ€æ•°æ®çš„æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’ã€‚é€šè¿‡å †å è¿™æ ·çš„ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹ï¼ˆå³UAå—ï¼‰ï¼Œæˆ‘ä»¬è·å¾—äº†ç®€æ´çš„å¤šæ¨¡æ€ç»Ÿä¸€æ³¨æ„åŠ›ç½‘ç»œï¼ˆMUANï¼‰ï¼Œè¯¥ç½‘ç»œå¯ä»¥è¿›è¡Œç²¾ç¡®çš„å¤šæ¨¡æ€æ¨ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé—¨æ§è‡ªæ³¨æ„åŠ›æ¨¡å‹
æœ¬æ–‡å°†åŸå§‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹ä¿®æ”¹ä¸ºé—¨æ§è‡ªæ³¨æ„åŠ›ï¼ˆGSAï¼‰æ¨¡å‹ï¼Œä½œä¸ºUAå—çš„åŸºæœ¬ç»„ä»¶ï¼Œè¿™æœ‰åŠ©äºæ›´å‡†ç¡®å’Œé²æ£’çš„æ³¨æ„åŠ›å­¦ä¹ ï¼Œå¹¶ä¸ºç‰¹å®šä»»åŠ¡æä¾›æ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸¤ä¸ªVQAæ•°æ®é›†å’Œä¸‰ä¸ªè§†è§‰å®šä½æ•°æ®é›†ä¸Šè¯„ä¼°äº†MUANæ¨¡å‹ï¼Œç»“æœè¡¨æ˜MUANåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†é¡¶çº§æ€§èƒ½ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æ•°æ®é›†ç‰¹å®šçš„æ¨¡å‹è°ƒæ•´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„MUANæ¨¡å‹ä¸ºå¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’åŒæ—¶å»ºæ¨¡ï¼Œå¹¶é€šè¿‡æ·±åº¦å †å ç»Ÿä¸€æ³¨æ„åŠ›å—è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„é—¨æ§è‡ªæ³¨æ„åŠ›æ¨¡å‹ä¹Ÿä¸ºæ³¨æ„åŠ›å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæé«˜æ³¨æ„åŠ›å­¦ä¹ çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

## unimumo--unified-text--music-and-motion-generation
### Abstract
We introduce UniMuMo, a unified multimodal model capable of taking arbitrary
text, music, and motion data as input conditions to generate outputs across all
three modalities. To address the lack of time-synchronized data, we align
unpaired music and motion data based on rhythmic patterns to leverage existing
large-scale music-only and motion-only datasets. By converting music, motion,
and text into token-based representation, our model bridges these modalities
through a unified encoder-decoder transformer architecture. To support multiple
generation tasks within a single framework, we introduce several architectural
improvements. We propose encoding motion with a music codebook, mapping motion
into the same feature space as music. We introduce a music-motion parallel
generation scheme that unifies all music and motion generation tasks into a
single transformer decoder architecture with a single training task of
music-motion joint generation. Moreover, the model is designed by fine-tuning
existing pre-trained single-modality models, significantly reducing
computational demands. Extensive experiments demonstrate that UniMuMo achieves
competitive results on all unidirectional generation benchmarks across music,
motion, and text modalities. Quantitative results are available in the
\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | UniMuMoï¼šç»Ÿä¸€æ–‡æœ¬ã€éŸ³ä¹å’ŒåŠ¨ä½œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éŸ³ä¹ã€åŠ¨ä½œå’Œæ–‡æœ¬æ˜¯äººç±»è¡¨è¾¾å’Œæ²Ÿé€šçš„é‡è¦æ–¹å¼ã€‚å®ƒä»¬ä¹‹é—´å­˜åœ¨ç€ç´§å¯†çš„è”ç³»ï¼Œä¾‹å¦‚èˆè¹ˆåŠ¨ä½œä¸éŸ³ä¹çš„èŠ‚å¥åŒæ­¥ï¼Œæ–‡æœ¬æè¿°å¯ä»¥ä¼ è¾¾éŸ³ä¹å’ŒåŠ¨ä½œçš„æƒ…æ„Ÿå’Œå†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹é€šå¸¸åªèƒ½å¤„ç†å•å‘çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ä»æ–‡æœ¬ç”ŸæˆéŸ³ä¹ï¼Œæˆ–è€…ä»éŸ³ä¹ç”ŸæˆåŠ¨ä½œï¼Œç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æ”¯æŒå¤šç§æ¨¡æ€çš„ç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šéŸ³ä¹-åŠ¨ä½œå¯¹é½
ä¸ºäº†è§£å†³ç¼ºä¹æ—¶é—´åŒæ­¥æ•°æ®çš„é—®é¢˜ï¼ŒUniMuMo æå‡ºäº†åŸºäºèŠ‚å¥æ¨¡å¼å¯¹é½æœªé…å¯¹çš„éŸ³é¢‘å’ŒåŠ¨ä½œæ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡æå–éŸ³ä¹èŠ‚æ‹å’ŒåŠ¨ä½œè§†è§‰èŠ‚æ‹ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼Œå°†åŠ¨ä½œåºåˆ—è°ƒæ•´ä»¥åŒ¹é…éŸ³ä¹èŠ‚æ‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéŸ³ä¹-åŠ¨ä½œå¹¶è¡Œç”Ÿæˆ
UniMuMo å¼•å…¥äº†ä¸€ç§æ–°çš„éŸ³ä¹-åŠ¨ä½œå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆï¼Œå°†æ‰€æœ‰éŸ³ä¹å’ŒåŠ¨ä½œç”Ÿæˆä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ª Transformer è§£ç å™¨æ¶æ„ä¸­ã€‚é€šè¿‡åŒæ—¶è¿›è¡Œä¸¤ä¸ªç›¸äº’æ¡ä»¶åŒ–çš„è‡ªå›å½’ç”Ÿæˆæµï¼ˆéŸ³ä¹å’ŒåŠ¨ä½œï¼‰ï¼ŒUniMuMo èƒ½å¤Ÿåœ¨å•ä¸ªè®­ç»ƒä»»åŠ¡ä¸­å®ç°éŸ³ä¹-åŠ¨ä½œè”åˆç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šéŸ³ä¹-åŠ¨ä½œè”åˆåˆ†è¯å™¨
UniMuMo ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„éŸ³é¢‘åˆ†è¯å™¨ Encodec å’Œä¸€ä¸ªæ–°çš„åŠ¨ä½œç¼–ç å™¨-è§£ç å™¨ï¼Œå°†éŸ³ä¹å’ŒåŠ¨ä½œåºåˆ—ç¼–ç åˆ°ç›¸åŒçš„ç‰¹å¾ç©ºé—´ä¸­ã€‚è¿™ç§è®¾è®¡ä¸ä»…æœ‰æ•ˆåœ°å¼¥åˆäº†æ¨¡æ€ä¹‹é—´çš„å·®è·ï¼Œè¿˜æ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜äº†æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šéŸ³ä¹-åŠ¨ä½œæ¡ä»¶æè¿°ç”Ÿæˆ
UniMuMo ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„éŸ³ä¹-åŠ¨ä½œè§£ç å™¨ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶å¾®è°ƒä¸€ä¸ª T5 è§£ç å™¨æ¥ç”ŸæˆéŸ³ä¹å’ŒåŠ¨ä½œçš„æ–‡æœ¬æè¿°ã€‚ä¸ºäº†æ›´å¥½åœ°æ•æ‰éŸ³ä¹å’ŒåŠ¨ä½œç‰¹å¾ï¼ŒUniMuMo å¼•å…¥äº†ä¸€ä¸ªå¯è®­ç»ƒçš„å…¨è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶å°†å…¶ä¸ T5 è§£ç å™¨ä¸€èµ·å¾®è°ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
UniMuMo åœ¨å„ç§å•å‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°éŸ³ä¹ã€éŸ³ä¹åˆ°åŠ¨ä½œã€åŠ¨ä½œåˆ°éŸ³ä¹ã€éŸ³ä¹æè¿°å’ŒåŠ¨ä½œæè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMuMo èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„éŸ³ä¹ã€åŠ¨ä½œå’Œæ–‡æœ¬å†…å®¹ï¼Œå¹¶æ”¯æŒå¤šç§æ¨¡æ€çš„ç»„åˆç”Ÿæˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
UniMuMo çš„åˆ›æ–°æ–¹æ³•ä¸ºå¤šæ¨¡æ€ç”Ÿæˆç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ï¼Œå¹¶ä¸ºæœªæ¥æ¨¡å‹çš„è®¾è®¡å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å…¶æ ¸å¿ƒæ€æƒ³å’Œæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆï¼Œæˆ–è€…è§†é¢‘å’ŒéŸ³é¢‘çš„ç”Ÿæˆã€‚

## medvilam--a-multimodal-large-language-model-with-advanced-generalizability-and-explainability-for-medical-data-understanding-and-generation
### Abstract
Medicine is inherently multimodal and multitask, with diverse data modalities
spanning text, imaging. However, most models in medical field are unimodal
single tasks and lack good generalizability and explainability. In this study,
we introduce MedViLaM, a unified vision-language model towards a generalist
model for medical data that can flexibly encode and interpret various forms of
medical data, including clinical language and imaging, all using the same set
of model weights. To facilitate the creation of such multi-task model, we have
curated MultiMedBench, a comprehensive pretaining dataset and benchmark
consisting of several distinct tasks, i.e., continuous question-answering,
multi-label disease classification, disease localization, generation and
summarization of radiology reports. MedViLaM demonstrates strong performance
across all MultiMedBench tasks, frequently outpacing other generalist models by
a significant margin. Additionally, we present instances of zero-shot
generalization to new medical concepts and tasks, effective transfer learning
across different tasks, and the emergence of zero-shot medical reasoning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MedViLaMï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŠ©åŠ›åŒ»ç–—æ•°æ®ç†è§£å’Œç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åŒ»å­¦é¢†åŸŸçš„æ•°æ®å…·æœ‰å¤šæ¨¡æ€å’Œå¤šä»»åŠ¡çš„ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å½±åƒç­‰å¤šç§æ•°æ®ç±»å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒ»å­¦æ¨¡å‹å¤§å¤šä¸ºå•æ¨¡æ€ã€å•ä»»åŠ¡æ¨¡å‹ï¼Œç¼ºä¹è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ç–¾ç—…ç±»åˆ«å’Œæœªå®šä¹‰çš„æŒ‡ä»¤ï¼ˆä»»åŠ¡ï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMedViLaMï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æˆä¸ºåŒ»ç–—æ•°æ®çš„é€šç”¨æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°ç¼–ç å’Œè§£é‡Šå„ç§å½¢å¼çš„åŒ»ç–—æ•°æ®ï¼ŒåŒ…æ‹¬ä¸´åºŠè¯­è¨€å’Œå½±åƒï¼Œæ‰€æœ‰è¿™äº›æ“ä½œéƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹æƒé‡é›†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸ºäº†ä¿ƒè¿›å¤šä»»åŠ¡æ¨¡å‹çš„åˆ›å»ºï¼Œç ”ç©¶äººå‘˜æ„å»ºäº†MultiMedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„é¢„è®­ç»ƒæ•°æ®é›†å’ŒåŸºå‡†ï¼ŒåŒ…æ‹¬å¤šä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œä¾‹å¦‚è¿ç»­é—®ç­”ã€å¤šæ ‡ç­¾ç–¾ç—…åˆ†ç±»ã€ç–¾ç—…å®šä½ã€æ”¾å°„å­¦æŠ¥å‘Šçš„ç”Ÿæˆå’Œæ‘˜è¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šMedViLaMåœ¨æ‰€æœ‰MultiMedBenchä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œç»å¸¸ä»¥æ˜¾è‘—çš„ä¼˜åŠ¿è¶…è¶Šå…¶ä»–é€šç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å±•ç¤ºäº†æ¨¡å‹å¯¹æ–°åŒ»ç–—æ¦‚å¿µå’Œä»»åŠ¡çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€ä¸åŒä»»åŠ¡ä¹‹é—´çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ ï¼Œä»¥åŠé›¶æ ·æœ¬åŒ»ç–—æ¨ç†çš„å‡ºç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒMedViLaMåœ¨å„ç§åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¿™è¡¨æ˜å…¶åœ¨æœªæ¥ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MedViLaMæ¨¡å‹åœ¨åŒ»ç–—æ•°æ®ç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›ï¼Œå…¶å¤šæ¨¡æ€ã€å¤šä»»åŠ¡çš„ç‰¹ç‚¹ä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„åŒ»ç–—åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¹Ÿæœ‰åŠ©äºæé«˜ä¸´åºŠåŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ã€‚å› æ­¤ï¼ŒMedViLaMæ¨¡å‹ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## generative-visual-instruction-tuning
### Abstract
We propose to use automatically generated instruction-following data to
improve the zero-shot capabilities of a large multimodal model with additional
support for generative and image editing tasks. We achieve this by curating a
new multimodal instruction-following set using GPT-4V and existing datasets for
image generation and editing. Using this instruction set and the existing
LLaVA-Finetune instruction set for visual understanding tasks, we produce
GenLLaVA, a Generative Large Language and Visual Assistant. GenLLaVA is built
through a strategy that combines three types of large pretrained models through
instruction finetuning: Mistral for language modeling, SigLIP for image-text
matching, and StableDiffusion for text-to-image generation. Our model
demonstrates visual understanding capabilities superior to LLaVA and
additionally demonstrates competitive results with native multimodal models
such as Unified-IO 2, paving the way for building advanced general-purpose
visual assistants by effectively re-using existing multimodal models. We
open-source our dataset, codebase, and model checkpoints to foster further
research and application in this domain.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GenLLaVAï¼šå¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å…´èµ·ï¼Œå¦‚ä½•è®©è¿™äº›æ¨¡å‹åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸ŠåŒæ—¶è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹å¾€å¾€åœ¨æ·»åŠ æ–°çš„åŠŸèƒ½ï¼ˆå¦‚å›¾åƒç”Ÿæˆï¼‰åï¼Œä¼šæŸå¤±åŸæœ‰çš„è§†è§‰å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿å¾—å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¿æŒåŸæœ‰èƒ½åŠ›çš„åŒæ—¶ï¼Œè¿˜èƒ½è¿›è¡Œå›¾åƒç”Ÿæˆå’Œç¼–è¾‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†
æœ¬æ–‡é€šè¿‡ä½¿ç”¨GPT-4Vå’Œç°æœ‰çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†å›¾åƒç†è§£ã€å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„æ•°æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGenLLaVAæ¨¡å‹
æœ¬æ–‡æå‡ºäº†GenLLaVAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå°†Mistralè¯­è¨€æ¨¡å‹ã€SigLIPå›¾åƒ-æ–‡æœ¬åŒ¹é…æ¨¡å‹å’ŒStableDiffusionæ–‡æœ¬-å›¾åƒç”Ÿæˆæ¨¡å‹ç»“åˆèµ·æ¥ï¼Œå®ç°äº†å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„åŠŸèƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå•é˜¶æ®µè®­ç»ƒ
ä¸LLaVAæ¨¡å‹çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ä¸åŒï¼ŒGenLLaVAæ¨¡å‹é‡‡ç”¨å•é˜¶æ®µè®­ç»ƒï¼Œç›´æ¥å¾®è°ƒè§†è§‰-è¯­è¨€æŠ•å½±å™¨ã€è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç”Ÿæˆå¤´ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGenLLaVAæ¨¡å‹åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¼˜äºLLaVAæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®¸å¤šä»»åŠ¡ä¸Šä¸Unified-IO 2ç­‰åŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GenLLaVAæ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•ï¼Œä¸ºæ„å»ºé«˜çº§é€šç”¨è§†è§‰åŠ©æ‰‹æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼€æºäº†æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä¸ºå¤šæ¨¡æ€AIé¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å®è´µçš„èµ„æºã€‚

