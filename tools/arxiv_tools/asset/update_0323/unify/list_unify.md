OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models
Dual Diffusion for Unified Image Generation and Understanding
LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation
MetaMorph: Multimodal Understanding and Generation via Instruction Tuning
SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding
Liquid: Language Models are Scalable Multi-modal Generators
TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation
OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows
Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads
JetFormer: An Autoregressive Generative Model of Raw Images and Text
MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding
JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation
Spider: Any-to-Many Multimodal LLM
MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding
Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation
PUMA: Empowering Unified MLLM with Multi-granular Visual Generation
MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling
Emu3: Next-Token Prediction is All You Need
MIO: A Foundation Model on Multimodal Tokens
MonoFormer: One Transformer for Both Diffusion and Autoregression
VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation
Show-o: One Single Transformer to Unify Multimodal Understanding and Generation
Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation
X-VILA: Cross-Modality Alignment for Large Language Model
Chameleon: Mixed-Modal Early-Fusion Foundation Models
SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation
Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
World Model on Million-Length Video And Language With Blockwise RingAttention
Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization
MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer
Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action
Emu2: Generative Multimodal Models are In-Context Learners
Gemini: A Family of Highly Capable Multimodal Models
VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation
DreamLLM: Synergistic Multimodal Comprehension and Creation
Making LLaMA SEE and Draw with SEED Tokenizer
NExT-GPT: Any-to-Any Multimodal LLM
LaVIT: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization
Planting a SEED of Vision in Large Language Model
Emu: Generative Pretraining in Multimodality
CoDi: Any-to-Any Generation via Composable Diffusion
Multimodal unified attention networks for vision-and-language interactions
UniMuMo: Unified Text, Music, and Motion Generation
MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation
ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions